2016,Dynamic matrix recovery from incomplete observations under an exact low-rank constraint,Low-rank matrix factorizations arise in a wide variety of applications -- including recommendation systems  topic models  and source separation  to name just a few.  In these and many other applications  it has been widely noted that by incorporating temporal information and allowing for the possibility of time-varying models  significant improvements are possible in practice. However  despite the reported superior empirical performance of these dynamic models over their static counterparts  there is limited theoretical justification for introducing these more complex models. In this paper we aim to address this gap by studying the problem of recovering a dynamically evolving low-rank matrix from incomplete observations. First  we propose the locally weighted matrix smoothing (LOWEMS) framework as one possible approach to dynamic matrix recovery. We then establish error bounds for LOWEMS in both the {\em matrix sensing} and {\em matrix completion} observation models. Our results quantify the potential benefits of exploiting dynamic constraints both in terms of recovery accuracy and sample complexity. To illustrate these benefits we provide both synthetic and real-world experimental results.,Dynamic matrix recovery from incomplete

observations under an exact low-rank constraint

Liangbei Xu Mark A. Davenport

Department of Electrical and Computer Engineering

Georgia Institute of Technology

Atlanta  GA 30318

lxu66@gatech.edu mdav@gatech.edu

Abstract

Low-rank matrix factorizations arise in a wide variety of applications – including
recommendation systems  topic models  and source separation  to name just a few.
In these and many other applications  it has been widely noted that by incorporat-
ing temporal information and allowing for the possibility of time-varying models 
signiﬁcant improvements are possible in practice. However  despite the reported
superior empirical performance of these dynamic models over their static counter-
parts  there is limited theoretical justiﬁcation for introducing these more complex
models. In this paper we aim to address this gap by studying the problem of recov-
ering a dynamically evolving low-rank matrix from incomplete observations. First 
we propose the locally weighted matrix smoothing (LOWEMS) framework as one
possible approach to dynamic matrix recovery. We then establish error bounds for
LOWEMS in both the matrix sensing and matrix completion observation models.
Our results quantify the potential beneﬁts of exploiting dynamic constraints both
in terms of recovery accuracy and sample complexity. To illustrate these beneﬁts
we provide both synthetic and real-world experimental results.

Introduction

1
Suppose that X ∈ Rn1×n2 is a rank-r matrix with r much smaller than n1 and n2. We observe X
through a linear operator A : Rn1×n2 → Rm 

y = A(X) 

y ∈ Rm.

In recent years there has been a signiﬁcant amount of progress in our understanding of how to recover
X from observations of this form even when the number of observations m is much less than the
number of entries in X. (See [8] for an overview of this literature.) When A is a set of weighted linear
combinations of the entries of X  this problem is often referred to as the matrix sensing problem.
In the special case where A samples a subset of entries of X  it is known as the matrix completion
problem. There are a number of ways to establish recovery guarantee in these settings. Perhaps the
most popular approach for theoretical analysis in recent years has focused on the use of nuclear norm
minimization as a convex surrogate for the (nonconvex) rank constraint [1  3  4  5  6  7  15  19  21  22].
An alternative  however is to aim to directly solve the problem under an exact low-rank constraint.
This leads a non-convex optimization problem  but has several computational advantages over most
approaches to minimizing the nuclear norm and is widely used in large-scale applications (such
as recommendation systems) [16]. In general  popular algorithms for solving the rank-constrained
models – e.g.  alternating minimization and alternating gradient descent – do not have as strong of
convergence or recovery error guarantees due to the non-convexity of the rank constraint. However 
there has been signiﬁcant progress on this front in recent years [11  10  12  13  14  23  25]  with many
of these algorithms now having guarantees comparable to those for nuclear norm minimization.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Nearly all of this existing work assumes that the underlying low-rank matrix X remains ﬁxed
throughout the measurement process. In many practical applications  this is a tremendous limitation.
For example  users’ preferences for various items may change (sometimes quite dramatically) over
time. Modelling such drift of user’s preference has been proposed in the context of both music and
movies as a way to achieve higher accuracy in recommendation systems [9  17]. Another example
in signal processing is dynamic non-negative matrix factorization for the blind signal separation
problem [18]. In these and many other applications  explicitly modelling the dynamic structure in the
data has led to superior empirical performance. However  our theoretical understanding of dynamic
low-rank matrix recovery is still very limited.
In this paper we provide the ﬁrst theoretical results on the dynamic low-rank matrix recovery problem.
We determine the sense in which dynamic constraints can help to recover the underlying time-varying
low-rank matrix in a particular dynamic model and quantify this impact through recovery error
bounds. To describe our approach  we consider a simple example where we have two rank-r matrices
X 1 and X 2. Suppose that we have a set of observations for each of X 1 and X 2  given by

yi = Ai(cid:0)X i(cid:1)  

i = 1  2.

The naïve approach is to use y1 to recover X 1 and y2 to recover X 2 separately. In this case the
number of observations required to guarantee successful recovery is roughly mi ≥ C ir max(n1  n2)
for i = 1  2 respectively  where C 1  C 2 are ﬁxed positive constants (see [4]). However  if we know
that X 2 is close to X 1 in some sense (for example  if X 2 is a small perturbation of X 1)  then the
above approach is suboptimal both in terms of recovery accuracy and sample complexity  since in
this setting y1 actually contains information about X 2 (and similarly  y2 contains information about
X 1). There are a variety of possible approaches to incorporating this additional information. The
approach we will take is inspired by the LOWESS (locally weighted scatterplot smoothing) approach
from non-parametric regression. In the case of this simple example  if we look just at the problem of
estimating X 2  our approach reduces to solving a problem of the form

rank(cid:0)X 2(cid:1) ≤ r 

(cid:107)A2(X 2) − y2(cid:107)2

2 + λ(cid:107)A1(X 2) − y1(cid:107)2

2

s.t.

min
X 2

where λ is a parameter that determines how strictly we are enforcing the dynamic constraint (if
X 1 is very close to X 2 we can set λ to be larger  but if X 1 is far from X 2 we will set it to be
comparatively small). This approach generalizes naturally to the locally weighted matrix smoothing
(LOWEMS) program described in Section 2. Note that it has a (simple) convex objective function  but
a non-convex rank constraint. Our analysis in Section 3 shows that the proposed program outperforms
the above naïve recovery strategy both in terms of recovery accuracy and sample complexity.
We should emphasize that the proposed LOWEMS program is non-convex due to the exact low-
rank constraint. Inspired by previous work on matrix factorization  we propose using an efﬁcient
alternating minimization algorithm (described in more detail in Section 4). We explicitly enforce the
low-rank constraint by optimizing over a rank-r factorization and alternately minimize with respect
to one of the factors while holding the other one ﬁxed. This approach is popular in practice since
it is typically less computationally complex than nuclear norm minimization based algorithms. In
addition  thanks to recent work on global convergence guarantees for alternating minimization for
low-rank matrix recovery [10  13  25]  one can reasonably expect similar convergence guarantees to
hold for alternating minimization in the context of LOWEMS  although we leave the pursuit of such
guarantees for future work.
To empirically verify our analysis  we perform both synthetic and real world experiments  described
in Section 5. The synthetic experimental results demonstrate that LOWEMS outperforms the naïve
approach in practice both in terms of recovery accuracy and sample complexity. We also demonstrate
the effectiveness of LOWEMS in the context of recommendation systems.
Before proceeding  we brieﬂy state some of the notation that we will use throughout. For a vector
x ∈ Rn  we let (cid:107)x(cid:107)p denote the standard (cid:96)p norm. Given a matrix X ∈ Rn1×n2  we use Xi: to denote
the ith row of X and X:j to denote the jth column of X. We let (cid:107)X(cid:107)F denote the the Frobenius
norm  (cid:107)X(cid:107)2 the operator norm  (cid:107)X(cid:107)∗ the nuclear norm  and (cid:107)X(cid:107)∞ = maxi j |Xij| the element-
i j XijYij =

wise inﬁnity norm. Given a pair of matrices X  Y ∈ Rn1×n2  we let (cid:104)X  Y (cid:105) = (cid:80)
Tr(cid:0)Y T X(cid:1) denote the standard inner product. Finally  we let nmax and nmin denote max{n1  n2}

and min{n1  n2} respectively.

2

2 Problem formulation

The underlying assumption throughout this paper is that our low-rank matrix is changing over time
during the measurement process. For simplicity we will model this through the following discrete
dynamic process: at time t  we have a low-rank matrix X t ∈ Rn1×n2 with rank r  which we assume
is related to the matrix at previous time-steps via

X t = f (X 1  . . .   X t−1) + t 

 

t=1.

yt  zt ∈ Rmt

yt = At(X t) + zt 
t=1 jointly from {yt}d

where t represents noise. Then we observe each X t through a linear operator At : Rn1×n2 → Rmt 
(1)
where zt is measurement noise. In our problem we will suppose that we observe up to d time steps 
and our goal is to recover {X t}d
The above model is sufﬁciently ﬂexible to incorporate a wide variety of dynamics  but we will
make several simpliﬁcations. First  we note that we can impose the low-rank constraint explicitly
by factorizing X t as X t = U t (V t)T   U t ∈ Rn1×r  V t ∈ Rn2×r. In general both U t and V t may
be changing over time. However  in some applications  it is reasonable to assume that only one set
of factors is changing. For example  in a recommendation system where our matrix represent user
preferences  if the rows correspond to items and the columns correspond to users  then U t contains
the latent properties of the items and V t models the latent preferences of the users. In this context
it is reasonable to assume that only V t changes over time [9  17]  and that there is a ﬁxed matrix
U (which we may assume to be orthonormal) such that we can write X t = U V t for all t. Similar
arguments can be made in a variety of other applications  including personalized learning systems 
blind signal separation  and more.
Second  we assume a Markov property on f  so that X t (or equivalently  V t) only depends on the
previous X t−1 (or V t−1). Furthermore  although other dynamic models could be accommodated  for
the sake of simplicity in our analysis we consider the simple model on V t where

V t = V t−1 + t 

(2)
We will also assume that both t and the measurement noise zt are i.i.d. zero-mean Gaussian noise.
To simplify our discussion  we will assume that our goal is to recover the matrix at the most recent
time-step  i.e.  we wish to estimate X d from {yt}d
t=1. Our general approach can be stated as follows.
The LOWEMS estimator is given by the following optimization program:

t = 2  . . .   d.

ˆX d = arg min
X∈C(r)

L (X) = arg min
X∈C(r)

1
2

assume(cid:80)d

where C(r) = {X ∈ Rn1×n2 : rank(X) ≤ r}  and {wt}d
performance of the LOWEMS estimator for two common choices of operators At.

t=1 are non-negative weights. We further
t=1 wt = 1 to avoid ambiguity. In the following section we provide bounds on the

(3)

(cid:13)(cid:13)At (X) − yt(cid:13)(cid:13)2

2  

wt

d(cid:88)

t=1

3 Recovery error bounds
Given the estimator ˆX d from (3)  we deﬁne the recovery error to be ∆d := ˆX d − X d. Our goal in
this section will be to provide bounds on (cid:107) ˆX d − X d(cid:107)F under two common observation models. Our
analysis builds on the following (deterministic) inequality.
Proposition 3.1. Both the estimator ˆX d by (3) and (9) satisﬁes

wtAt∗(cid:0)ht − zt(cid:1)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
where ht = At(cid:0)X d − X t(cid:1) and At∗ is the adjoint operator of At.

(cid:13)(cid:13)At(cid:0)∆d(cid:1)(cid:13)(cid:13)2

(cid:13)(cid:13)∆d(cid:13)(cid:13)F  

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) d(cid:88)

d(cid:88)

2 ≤ 2

√

(4)

wt

2r

t=1

t=1

This is a deterministic result that holds for any set of {At}. The remaining work is to lower bound the
LHS of (4)  and upper bound the RHS of (4) for concrete choices of {At}. In the following sections
we derive such bounds in the settings of both Gaussian matrix sensing and matrix completion. For
simplicity and without loss of generality  we will assume m1 = . . . = md =: m0  so that the total
number of observations is simply m = dm0.

3

3.1 Matrix sensing setting
For the matrix sensing problem  we will consider the case where all operators At correspond to
Gaussian measurement ensembles  deﬁned as follows.
Deﬁnition 3.2. [4] A linear operator A : Rn1×n2 → Rm is a Gaussian measurement ensemble if
we can express each entry of A (X) as [A (X)]i = (cid:104)Ai  X(cid:105) for a matrix Ai whose entries are i.i.d.
according to N (0  1/m)  and where the matrices A1  . . .   Am are independent from each other.
Also  we deﬁne the matrix restricted isometry property (RIP) for a linear map A.
Deﬁnition 3.3. [4] For each integer r = 1  . . .   nmin  the isometry constant δr of A is the smallest
quantity such that

(1 − δr)(cid:107)X(cid:107)2
holds for all matrices X of rank at most r.

F ≤ (cid:107)A (X)(cid:107)2

2 ≤ (1 + δr)(cid:107)X(cid:107)2

F

An important result (that we use in the proof of Theorem 3.4) is that Gaussian measurement ensembles
satisfy the matrix RIP with high probability provided m ≥ Crnmax. See  for example  [4] for details.
To obtain an error bound in the matrix sensing case we lower bound the LHS of (4) using the matrix
RIP and upper bound the stochastic error (the RHS of (4)) using a covering argument. The following
is our main result in the context of matrix setting.
Theorem 3.4. Suppose that we are given measurements as in (1) where all At’s are Gaussian
measurement ensembles. Assume that X t evolves according to (2) and has rank r. Further assume

(cid:1) for 1 ≤ t ≤ d and that the perturbation noise t is

that the measurement noise zt is i.i.d. N(cid:0)0  σ2
i.i.d. N(cid:0)0  σ2
(cid:40)

(cid:1) for 2 ≤ t ≤ d. If

2

1

m0 ≥ D1 max

nmaxr

w2

t   nmax

where D1 is a ﬁxed positive constant  then the estimator ˆX d from (3) satisﬁes

d(cid:88)
d−1(cid:88)

t=1

(cid:41)
(cid:33)

 

(cid:13)(cid:13)∆d(cid:13)(cid:13)2

F ≤ C0

(cid:32) d(cid:88)

w2

t σ2

1 +

(d − t)w2

t σ2
2

nmaxr

t=1

t=1

(5)

(6)

(7)

(8)

with probability at least P1 = 1 − dC1 exp (−c1n2)  where C0  C1  c1 are positive constants.
If we choose the weights as wd = 1 and wt = 0 for 1 ≤ t ≤ d − 1  the bound in Theorem 3.4
reduces to a bound matching classical (static) matrix recovery results (see  for example  [4] Theorem
2.4). Also note that in this case Theorem 3.4 implies exact recovery when the sample complexity
is O(rn/d). In order to help interpret this result for other choices of the weights  we note that for a
given set of parameters  we can determine the optimal weights that will minimize this bound. Towards
1 and set pt = (d − t)  1 ≤ t ≤ d. Then one can calculate the optimal
this end  we deﬁne κ := σ2
weights by solving the following quadratic program:

2/σ2

Using the method of Lagrange multipliers one can show that (7) has the analytical solution:

{w∗

t }d

t=1 = arg min

(cid:80)
t wt=1; wt≥0
1(cid:80)d

1

1

i=1

1+piκ

w∗
j =

1 + pjκ

d(cid:88)

d−1(cid:88)

w2

t +

ptκw2
t .

t=1

t=1

1 ≤ j ≤ d.

 

A simple special case occurs when σ2 = 0. In this case all V t’s are the same  and the optimal weights
go to wt = 1
d for all t. In contrast  when σ2 grows large the weights eventually converge to wd = 1
and wt = 0 for all t (cid:54)= d. This results in essentially using only yd to recover X d and ignoring the rest
of the measurements. Combining these  we note that when the σ2 is small  we can gain by a factor of
approximately d over the naïve strategy that ignores dynamics and tries to recover X d using only yd.
t when r/d is relatively
large. Thus  when σ2 is small  the required number of measurements can be reduced by a factor of d
compared to what would be required to recover X d using only yd.

Notice also that the minimum sample complexity is proportional to(cid:80)d

t=1 w2

4

3.2 Matrix completion setting

For the matrix completion problem  we consider the following simple uniform sampling ensemble:
Deﬁnition 3.5. A linear operator A : Rn1×n2 → Rm is a uniform sampling ensemble (with
replacement) if all sensing matrices Ai are i.i.d. uniformly distributed on the set

X =(cid:8)ej (n1) eT

k (n2)   1 ≤ j ≤ n1  1 ≤ k ≤ n2

(cid:1)  

where ej (n) are the canonical basis vectors in Rn. We let p = m0/(n1n2) denote the fraction of
sampled entries.

For this observation architecture  our analysis is complicated by the fact that it does not satisfy the
matrix RIP. (A quick problematic example is a rank-1 matrix with only one non-zero entry.) To handle
this we follow the typical approach and restrict our focus to matrices that satisfy certain incoherence
properties.
Deﬁnition 3.6. (Subspace incoherence [10]) Let U ∈ Rn×r be the orthonormal basis for an r-
√
dimensional subspace U  then the incoherence of U is deﬁned as µ(U) := maxi∈[n]
n√
r
where ei denotes the ith standard basis vector. We also simply denote µ(span(U )) as µ(U ).
Deﬁnition 3.7. (Matrix incoherence [13]) A rank-r matrix X ∈ Rn1×n2 with SVD X = U ΣV T is
incoherent with parameter µ if

(cid:13)(cid:13)eT
i U(cid:13)(cid:13)2 

√
(cid:107)U:i(cid:107)2 ≤ µ
r√
n1

for any i ∈ [n1]

and

for any j ∈ [n2] 

√
(cid:107)V:j(cid:107)2 ≤ µ
r√
n2

i.e.  the subspaces spanned by the columns of U and V are both µ-incoherent.

The incoherence assumption guarantees that X is far from sparse  which make it possible to recover
X from incomplete measurements since a measurement contains roughly the same amount of
information for all dimensions.
To proceed we also assume that the matrix X d has “bounded spikiness” in that the maximum entry
optimization constraints C (r) in (3) with C (r  a) :== {X ∈ Rn1×n2 : rank (X) ≤ r (cid:107)X(cid:107)∞ ≤ a}:

of X d is bounded by a  i.e. (cid:13)(cid:13)X d(cid:13)(cid:13)∞ ≤ a. To exploit the spikiness constraint below we replace the

ˆX d = arg min
X∈C(r a)

L (X) = arg min
X∈C(r a)

1
2

(cid:13)(cid:13)At (X) − yt(cid:13)(cid:13)2

2 .

wt

d(cid:88)

t=1

(9)

Note that Proposition 3.1 still holds for (9).
To obtain an error bound in the matrix completion case  we lower bound the LHS of 4 using a
restricted convexity argument (see  for example  [20]) and upper bound the RHS using matrix
Bernstein inequality. The result of this approach is the following theorem.
Theorem 3.8. Suppose that we are given measurements as in (1) where all At’s are uniform sampling
ensembles. Assume that X t evolves according to (2)  has rank r  and is incoherent with parameter

µ0 and(cid:13)(cid:13)X d(cid:13)(cid:13)∞ ≤ a. Further assume that the perturbation noise and the measurement noise satisfy

the same assumptions in Theorem 3.4. If

m0 ≥ D2nmin log2(n1 + n2)φ(cid:48)(w) 
0rσ2
t ((d−t)σ2

t ((d−t)µ2
t=1 w2

  then the estimator ˆX d from (9) satisﬁes

(cid:115)(cid:80)d

1)
2 /n1+σ2
1)
2 +σ2

B1 := C2a2n1n2
(cid:32)(cid:32) d(cid:88)

d−1(cid:88)

t=1 w2

t log(n1 + n2)

m0

  B2

  
d(cid:88)

(cid:33)

(10)

(11)

(cid:33)

 

(12)

where φ(cid:48)(w) =

maxt w2

(cid:80)d
(cid:13)(cid:13)∆d(cid:13)(cid:13)2

F ≤ max

with probability at least P1 = 1 − 5/(n1 + n2) − 5dnmax exp(−nmin)  where

B2 =

C3rn2

1n2

2 log(n1 + n2)
nminm0

t=1
and C2  C3  D2 are absolute positive constants.

w2

t σ2

1 +

(d − t)w2

t σ2
2

+

w2

t a2

t=1

t=1

5

If we choose the weights as wd = 1 and wt = 0 for 1 ≤ t ≤ d − 1  the bound in Theorem 3.8
reduces to a result comparable to classical (static) matrix completion results (see  for example  [15]
Theorem 7). Moreover  from the B2 term in (11)  we obtain the same dependence on m as that of (6) 
i.e.  1/m. However  there are also a few key differences between Theorem 3.4 and our results for
matrix completion. In general the bound is loose in several aspects compared to the matrix sensing
bound. For example  when m0 is small  B1 actually dominates  in which case the dependence on
m is actually 1/
m instead of 1/m. When m0 is sufﬁciently large  then B2 dominates  in which
case we can consider two cases. The ﬁrst case corresponds to when a is relatively large compared to
σ1  σ2 – i.e.  the low-rank matrix is spiky. In this case the term containing a2 in B2 dominates  and
the optimal weights are equal weights of 1/d. This occurs because the term involving a dominates
and there is little improvement to be obtained by exploiting temporal dynamics. In the second case 
when a is relatively small compared to σ1  σ2 (which is usually the case in practice)  the bound can
be simpliﬁed to

√

(cid:32)(cid:32) d(cid:88)

d−1(cid:88)

(cid:33)(cid:33)

(cid:107)∆(cid:107)2

F ≤ c3rn2

1n2

2 log(n1 + n2)
nminm0

w2

t σ2

1 +

t=1

t=1

(d − t)w2

t σ2
2

.

The above bound is much more similar to the bound in (6) from Theorem 3.4. In fact  we can also
obtain the optimal weights by solving the same quadratic program as (7).
When n1 ≈ n2  the sample complexity is Θ(nmin log2(n1 + n2)φ(cid:48)(w)). In this case Theorem 3.8
also implies a similar sample complexity reduction as we observed in the matrix sensing setting.
However  the precise relations between sample complexity and weights wt’s are different in these
two cases (deriving from the fact that the proof uses matrix Bernstein inequalities in the matrix
completion setting rather than concentration inequalities of Chi-squared variables as in the matrix
sensing setting).

4 An algorithm based on alternating minimization
As noted in Section 2  any rank-r matrix can be factorized as X = U V T where U is n1 × r and V is
n2 × r  therefore the LOWEMS estimator in (3) can be reformulated as

d(cid:88)

1
2

wt

(cid:13)(cid:13)At(cid:0)U V T(cid:1) − yt(cid:13)(cid:13)2

2 .

(13)

ˆX d = arg min
X∈C(r)

L (X) = arg min

X=U V T

t=1

The above program can be solved by alternating minimization (see [17])  which alternatively mini-
mizes the objective function over U (or V ) while holding V (or U) ﬁxed until a stopping criterion is
reached. Since the objective function is quadratic  each step in this procedure reduces to conventional
weighted least squares  which can be solved via efﬁcient numerical procedures. Theoretical guar-
antees for global convergence of alternating minimization for the static matrix sensing/completion
problem have recently been established in [10  13  25] by treating the alternating minimization as
a noisy version of the power method. Extending these results to establish convergence guarantees
for (13) would involve analyzing a weighted power method. We leave this analysis for future work 
but expect that similar convergence guarantees should be possible in this setting.

5 Simulations and experiments

5.1 Synthetic simulations

Our synthetic simulations consider both matrix sensing and matrix completion  but with an emphasis
on matrix completion. We set n1 = 100  n2 = 50  d = 4 and r = 5. We consider two baselines:
baseline one is only using yd to recover X d and simply ignoring y1  . . . yd−1; baseline two is using
{yt}d
t=1 with equal weights. Note that both of these can be viewed as special cases of LOWEMS with
weights (0  . . .   0  1) and ( 1
d ) respectively. Recalling the formula for the optimal choice of
1) → ∞
weights in (8)  it is easy to show that baseline one is equivalent to the case where κ = (σ2
and the baseline two equivalent to the case where κ → 0. This also makes intuitive sense since
κ → ∞ means the perturbation is arbitrarily large between time steps  while κ → 0 reduces to the
static setting.

d   . . .   1

2)/(σ2

d   1

6

(a)

(b)

Figure 1: Recovery error under different levels of perturbation noise. (a) matrix sensing. (b) matrix
completion.

Figure 2: Sample complexity under different levels of perturbation noise (matrix completion).

F /(cid:13)(cid:13)X d(cid:13)(cid:13)2

2/σ2

and show the average relative recovery error(cid:13)(cid:13)∆d(cid:13)(cid:13)2

1). Recovery error. In this simulation  we set m0 = 4000 and set the measurement noise level σ1
to 0.05. We vary the perturbation noise level σ2. For every pair of (σ1  σ2) we perform 10 trials 
F . Figure 1 illustrates how LOWEMS
reduces the recovery error compared to our baselines. As one can see  when σ2 is small  the optimal
κ  i.e.  σ2
1  generates nearly equal weights (baseline two)  reducing recovery error approximately
by a factor of 4 over baseline one  which is roughly equal to d as expected. As σ2 grows  the recovery
error of baseline two will increase dramatically due to the perturbation noise. However in this case
the optimal κ of LOWEMS grows with it  leading to a more uneven weighting and to somewhat
diminished performance gains. We also note that  as expected  LOWEMS converges to baseline one
when σ2 is large.
2). Sample complexity. In the interest of conciseness we only provide results here for the matrix
completion setting (matrix sensing yields broadly similar results).
In this simulation we vary
the fraction of observed entries p to empirically ﬁnd the minimum sample complexity required to
guarantee successful recovery (deﬁned as a relative error ≤ 0.08). We compare the sample complexity
of the proposed LOWEMS to baseline one and baseline two under different perturbation noise level
σ2 (σ1 is set as 0.02). For a certain σ2  the relative recovery error is the averaged over 10 trials.
Figure 2 illustrates how LOWEMS reduces the sample complexity required to guarantee successful
recovery. When the perturbation noise is weaker than the measurement noise  the sample complexity
can be reduced approximately by a factor of d compared to baseline one. When the perturbation noise
is much stronger than measurement noise  the recovery error of baseline two will increase due to the
perturbation noise and hence the sample complexity increase rapidly. However in this case proposed
LOWEMS still achieves relatively small sample complexity and its sample complexity converges to
baseline one when σ2 is relatively large.

7

10−210−110000.0050.010.0150.020.0250.03σ2Recovery Error Baseline oneBaseline twoLOWEMS10-210-1100101σ200.010.020.030.040.050.06Recovery ErrorBaseline oneBaseline twoLOWEMS10−310−210−11001010.20.30.40.50.60.70.80.91σ2Sample Complexity p Baseline oneLOWEMSBaseline two(a)

(b)

Figure 3: Experimental results on truncated Netﬂix dataset. (a) Testing RMSE vs. number of time
steps. (b) Validation RMSE vs. κ.

5.2 Real world experiments

We next test the LOWEMS approach in the context of a recommendation system using the (truncated)
Netﬂix dataset. We eliminate those movies with few ratings  and those users rating few movies  and
generate a truncated dataset with 3199 users  1042 movies  2462840 ratings  and hence the fraction of
visible entries in the rating matrix is ≈ 0.74. All the ratings are distributed over a period of 2191 days.
For the sake of robustness  we additionally impose a Frobenius norm penalty on the factor matrices
U and V in (13). We keep the latest (in time) 10% of the ratings as a testing set. The remaining
ratings are split into a validation set and a training set for the purpose of cross validation. We divide
the remaining ratings into d ∈ {1  3  6  8} bins respectively with same time period according to
their timestamps. We use 5-fold cross validation  and we keep 1/5 of the ratings from the dth bin
as a validation set. The number of latent factors r is set to 10. The Frobenius norm regularization
parameter γ is set to 1. We also note that in practice one likely has no prior information on σ1 
σ2 and hence κ. However  we use model selection techniques like cross validation to select the
best κ incorporating the unknown prior information on measurement/perturbation noise. We use
root mean squared error (RMSE) to measure prediction accuracy. Since alternating minimization
uses a random initialization  we generate 10 test RMSE’s (using a boxplot) for the same testing set.
Figure 3(a) shows that the proposed LOWEMS estimator improves the testing RMSE signiﬁcantly
with appropriate κ. Additionally  the performance improvement increases as d gets larger.
To further investigate how the parameter κ affects accuracy  we also show the validation RMSE
compared to κ in Figure 3(b). When κ ≈ 1  LOWEMS achieves the best RMSE on the validation
data. This further demonstrates that imposing an appropriate dynamic constraint should improve
recovery accuracy in practice.

6 Conclusion

In this paper we consider the low-rank matrix recovery problem in a novel setting  where one
of the factor matrices changes over time. We propose the locally weighted matrix smoothing
(LOWEMS) framework  and have established error bounds for LOWEMS in both the matrix sensing
and matrix completion cases. Our analysis quantiﬁes how the proposed estimator improves recovery
accuracy and reduces sample complexity compared to static recovery methods. Finally  we provide
both synthetic and real world experimental results to verify our analysis and demonstrate superior
empirical performance when exploiting dynamic constraints in a recommendation system.
Acknowledge
This work was supported by grants NRL N00173-14-2-C001  AFOSR FA9550-14-1-0342  NSF
CCF-1409406  CCF-1350616  and CMMI-1537261.

8

1368# of bins0.8710.8720.8730.8740.8750.8760.877RMSE10-210-1100101κ0.7450.750.7550.760.7650.770.7750.78RMSEd = 1d = 3d = 6d = 8References
[1] A. Agarwal  S. Negahban  and M. Wainwright. Noisy matrix decomposition via convex relaxation: Optimal

rates in high dimensions. Ann. Stat.  40(2):1171–1197  2012.

[2] P. Bühlmann and S. Van De Geer. Statistics for high-dimensional data: Methods  theory and applications.

Springer-Verlag Berlin Heidelberg  2011.

[3] E. Candès and Y. Plan. Matrix completion with noise. Proc. IEEE  98(6):925–936  2010.
[4] E. Candès and Y. Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of

noisy random measurements. IEEE Trans. Inform. Theory  57(4):2342–2359  2011.

[5] E. Candès and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math. 

9(6):717–772  2009.

[6] E. Candès and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans.

Inform. Theory  56(5):2053–2080  2010.

[7] M. Davenport  Y. Plan  E. van den Berg  and M. Wootters. 1-bit matrix completion.

3(3):189–223  2014.

Inf. Inference 

[8] M. Davenport and J. Romberg. An overview of low-rank matrix recovery from incomplete observations.

IEEE J. Select. Top. Signal Processing  10(4):608–622  2016.

[9] G. Dror  N. Koenigstein  Y. Koren  and M. Weimer. The Yahoo! music dataset and KDD-Cup’11. In Proc.
ACM SIGKDD Int. Conf. on Knowledge  Discovery  and Data Mining (KDD)  San Diego  CA  Aug. 2011.
[10] M. Hardt. Understanding alternating minimization for matrix completion. In Proc. IEEE Symp. Found.

Comp. Science (FOCS)  Philadelphia  PA  Oct. 2014.

[11] M. Hardt and M. Wootters. Fast matrix completion without the condition number. In Proc. Conf. Learning

Theory  Barcelona  Spain  June 2014.

[12] P. Jain and P. Netrapalli. Fast exact matrix completion with ﬁnite samples. In Proc. Conf. Learning Theory 

Paris  France  July 2015.

[13] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimization. In

Proc. ACM Symp. Theory of Comput.  Stanford  CA  June 2013.

[14] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. In Proc. Adv. in Neural

Processing Systems (NIPS)  Vancouver  BC  Dec. 2009.

[15] O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli  20(1):282–303 

2014.

[16] Y. Koren. The Bellkor solution to the Netﬂix grand prize  2009.
[17] Y. Koren. Collaborative ﬁltering with temporal dynamics. Comm. ACM  53(4):89–97  2010.
[18] N. Mohammadiha  P. Smaragdis  G. Panahandeh  and S. Doclo. A state-space approach to dynamic

nonnegative matrix factorization. IEEE Trans. Signal Processing  63(4):949–959  2015.

[19] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional

scaling. Ann. Stat.  39(2):1069–1097  2011.

[20] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal

bounds with noise. J. Machine Learning Research  13(1):1665–1697  2012.

[21] B. Recht  M. Fazel  and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via

nuclear norm minimization. SIAM Rev.  52(3):471–501  2010.

[22] B. Recht  W. Xu  and B. Hassibi. Necessary and sufﬁcient conditions for success of the nuclear norm
heuristic for rank minimization. In Proc. IEEE Conf. on Decision and Control (CDC)  Cancun  Mexico 
Dec. 2008.

[23] R. Sun and Z.-Q. Luo. Guaranteed matrix completion via nonconvex factorization. In Proc. IEEE Symp.

Found. Comp. Science (FOCS)  Berkeley  CA  Oct. 2015.

[24] J. A. Tropp. An introduction to matrix concentration inequalities. Found. Trends Mach. Learning 

8(1–2):1–230  2015.

[25] T. Zhao  Z. Wang  and H. Liu. A nonconvex optimization framework for low rank matrix estimation. In

Proc. Adv. in Neural Processing Systems (NIPS)  Montréal  QC  Dec. 2015.

9

,Liangbei Xu
Mark Davenport
Guolin Ke
Qi Meng
Taifeng Wang
Wei Chen
Weidong Ma
Qiwei Ye
Tie-Yan Liu