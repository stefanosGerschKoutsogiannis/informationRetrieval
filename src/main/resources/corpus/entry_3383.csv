2019,Maximum Mean Discrepancy Gradient Flow,We construct a Wasserstein gradient flow of the maximum mean discrepancy (MMD) and study its convergence properties.
  The MMD is an integral probability metric defined for a reproducing kernel Hilbert space (RKHS)  and serves as a metric on probability measures for a sufficiently rich RKHS.  We obtain conditions for convergence of the gradient flow towards a global optimum  that can be related to particle transport when optimizing neural networks.
  We also propose a way to regularize this MMD flow  based on an injection of noise in the gradient. This algorithmic fix comes with theoretical and empirical evidence.
The practical implementation of the flow is straightforward  since both the MMD and its gradient have simple closed-form expressions  which can be easily estimated with samples.,Maximum Mean Discrepancy Gradient Flow

Michael Arbel

Anna Korba

Gatsby Computational Neuroscience Unit

Gatsby Computational Neuroscience Unit

University College London

michael.n.arbel@gmail.com

University College London

a.korba@ucl.ac.uk

Adil Salim

KAUST

adil.salim@kaust.edu.sa

Visual Computing Center

Gatsby Computational Neuroscience Unit

Arthur Gretton

University College London

arthur.gretton@gmail.com

Abstract

We construct a Wasserstein gradient ﬂow of the maximum mean discrepancy
(MMD) and study its convergence properties. The MMD is an integral probability
metric deﬁned for a reproducing kernel Hilbert space (RKHS)  and serves as a
metric on probability measures for a sufﬁciently rich RKHS. We obtain conditions
for convergence of the gradient ﬂow towards a global optimum  that can be related
to particle transport when optimizing neural networks. We also propose a way to
regularize this MMD ﬂow  based on an injection of noise in the gradient. This
algorithmic ﬁx comes with theoretical and empirical evidence. The practical
implementation of the ﬂow is straightforward  since both the MMD and its gradient
have simple closed-form expressions  which can be easily estimated with samples.

1

Introduction

We address the problem of deﬁning a gradient ﬂow on the space of probability distributions endowed
with the Wasserstein metric  which transports probability mass from a starting distribtion ν to a target
distribution µ. Our ﬂow is deﬁned on the maximum mean discrepancy (MMD) [19]  an integral
probability metric [33] which uses the unit ball in a characteristic RKHS [43] as its witness function
class. Speciﬁcally  we choose the function in the witness class that has the largest difference in
expectation under ν and µ: this difference constitutes the MMD. The idea of descending a gradient
ﬂow over the space of distributions can be traced back to the seminal work of [24]  who revealed
that the Fokker-Planck equation is a gradient ﬂow of the Kullback-Leibler divergence. Its time-
discretization leads to the celebrated Langevin Monte Carlo algorithm  which comes with strong
convergence guarantees (see [14  15])  but requires the knowledge of an analytical form of the target
µ. A more recent gradient ﬂow approach  Stein Variational Gradient Descent (SVGD) [29]  also
leverages this analytical µ.
The study of particle ﬂows deﬁned on the MMD relates to two important topics in modern machine
learning. The ﬁrst is in training Implicit Generative Models  notably generative adversarial networks
[18]. Integral probability metrics have been used extensively as critic functions in this setting: these
include the Wasserstein distance [3  17  22] and maximum mean discrepancy [2  4  5  16  27  28]. In
[32  Section 3.3]  a connection between IGMs and particle transport is proposed  where it is shown
that gradient ﬂow on the witness function of an integral probability metric takes a similar form to the
generator update in a GAN. The critic IPM in this case is the Kernel Sobolev Discrepancy (KSD) 
which has an additional gradient norm constraint on the witness function compared with the MMD. It
is intended as an approximation to the negative Sobolev distance from the optimal transport literature
[35  36  45]. There remain certain differences between gradient ﬂow and GAN training  however.
First  and most obviously  gradient ﬂow can be approximated by representing ν as a set of particles 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

whereas in a GAN ν is the output of a generator network. The requirement that this generator network
be a smooth function of its parameters causes a departure from pure particle ﬂow. Second  in modern
implementations [2  5  27]  the kernel used in computing the critic witness function for an MMD
GAN critic is parametrized by a deep network  and an alternating optimization between the critic
parameters and the generator parameters is performed. Despite these differences  we anticipate that
the theoretical study of MMD ﬂow convergence will provide helpful insights into conditions for GAN
convergence  and ultimately  improvements to GAN training algorithms.
Regarding the second topic  we note that the properties of gradient descent for large neural networks
have been modeled using the convergence towards a global optimum of particle transport in the
population limit  when the number of particles goes to inﬁnity [12  31  38  41]. In particular  [37]
show that gradient descent on the parameters of a neural network can also be seen as a particle
transport problem  which has as its population limit a gradient ﬂow of a functional deﬁned for
probability distributions over the parameters of the network. This functional is in general non-convex 
which makes the convergence analysis challenging. The particular structure of the MMD allows us to
relate its gradient ﬂow to neural network optimization in a well-speciﬁed regression setting similar to
[12  37] (we make this connection explicit in Appendix F).
Our main contribution in this work is to establish conditions for convergence of MMD gradient ﬂow
to its global optimum. We give detailed descriptions of MMD ﬂow for both its continuous-time and
discrete instantiations in Section 2. In particular  the MMD ﬂow may employ a sample approximation
for the target µ: unlike e.g. Langevin Monte Carlo or SVGD  it does not require µ in analytical form.
Global convergence is especially challenging to prove: while for functionals that are displacement
convex  the gradient ﬂow can be shown to converge towards a global optimum [1]  the case of
non-convex functionals  like the MMD  requires different tools. A modiﬁed gradient ﬂow is proposed
in [37] that uses particle birth and death to reach global optimality. Global optimality may also be
achieved simply by teleporting particles from ν to µ  as occurs for the Sobolev Discrepancy ﬂow
absent a kernel regulariser [32  Theorem 4  Appendix D]. Note  however  that the regularised Kernel
Sobolev Discrepancy ﬂow does not rely on teleportation.
Our approach takes inspiration in particular from [7]  where it is shown that although the 1-Wasserstein
distance is non-convex  it can be optimized up to some barrier that depends on the diameter of the
domain of the target distribution. Similarly to [7]  we provide in Section 3 a barrier on the gradient
ﬂow of the MMD  although the tightness of this barrier in terms of the target diameter remains to be
established. We obtain a further condition on the evolution of the ﬂow to ensure global optimality 
and give rates of convergence in that case  however the condition is a strong one: it implies that the
negative Sobolev distance between the target and the current particles remains bounded at all times.
We thus propose a way to regularize the MMD ﬂow  based on a noise injection (Section 4) in
the gradient  with more tractable theoretical conditions for convergence. Encouragingly  the noise
injection is shown in practice to ensure convergence in a simple illustrative case where the original
MMD ﬂow fails. Finally  while our emphasis has been on establishing conditions for convergence 
we note that MMD gradient ﬂow has a simple O(M N + N 2) implementation for N ν-samples and
M µ-samples  and requires only evaluating the gradient of the kernel k on the given samples.

2 Gradient ﬂow of the MMD in W2
2.1 Construction of the gradient ﬂow

In this section we introduce the gradient ﬂow of the Maximum Mean Discrepancy (MMD) and
highlight some of its properties. We start by brieﬂy reviewing the MMD introduced in [19]. We deﬁne
X ⊂ Rd as the closure of a convex open set  and P2(X ) as the set of probability distributions on X
with ﬁnite second moment  equipped with the 2-Wassertein metric denoted W2. For any ν ∈ P2(X ) 
L2(ν) is the set of square integrable functions w.r.t. ν. The reader may ﬁnd a relevant mathematical
background in Appendix A.
Maximum Mean Discrepancy. Given a characteristic kernel k : X × X → R  we denote by H
its corresponding RKHS (see [42]). The space H is a Hilbert space with inner product (cid:104).  .(cid:105)H and
norm (cid:107).(cid:107)H. We will rely on speciﬁc assumptions on the kernel which are given in Appendix B. In
particular  Assumption (A) states that the gradient of the kernel  ∇k  is Lipschitz with constant L.
For such kernels  it is possible to deﬁne the Maximum Mean Discrepancy as a distance on P2(X ).

2

The MMD can be written as the RKHS norm of the unnormalised witness function fµ ν between µ
and ν  which is the difference between the mean embeddings of ν and µ 

M M D(µ  ν) = (cid:107)fµ ν(cid:107)H 

fν µ(z) =

k(x  z) dν(x) −

k(x  z) dµ(x) ∀z ∈ X

(1)

Throughout the paper  µ will be ﬁxed and ν can vary  hence we will only consider the dependence in
ν and denote by F(ν) = 1
2 M M D2(µ  ν). A direct computation [32  Appendix B] shows that for
any ﬁnite measure χ such that ν + χ ∈ P2(X )  we have
−1(F(ν + χ) − F(ν)) =

(2)
This means that fµ ν is the differential of F(ν) . Interestingly  F(ν) admits a free-energy expression:

fµ ν(x)dχ(x).

lim
→0

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

1
2

F(ν) =

V (x) dν(x) +

1
2

W (x  y) dν(x) dν(y) + C.

(cid:90)

where V is a conﬁnement potential  W an interaction potential and C a constant deﬁned by:
k(x  x(cid:48)) dµ(x) dµ(x(cid:48))

k(x  x(cid:48)) dµ(x(cid:48))  W (x  x(cid:48)) = k(x  x(cid:48))  C =

V (x) = −

(3)

(4)

Formulation (3) and the simple expression of the differential in (2) will be key to construct a gradient
ﬂow of F(ν)  to transport particles. In (4)  V reﬂects the potential generated by µ and acting on each
particle  while W reﬂects the potential arising from the interactions between those particles.

Gradient ﬂow of the MMD. We consider now the problem of transporting mass from an initial
distribution ν0 to a target distribution µ  by ﬁnding a continuous path νt starting from ν0 that converges
to µ while decreasing F(νt). Such a path should be physically plausible  in that teleportation
phenomena are not allowed. For instance  the path νt = (1 − e−t)µ + e−tν0 would constantly
teleport mass between µ and ν0 although it decreases F since F(νt) = e−2tF(ν0) [32  Section 3.1 
Case 1]. The physicality of the path is understood in terms of classical statistical physics: given
an initial conﬁguration ν0 of N particles  these can move towards a new conﬁguration µ through
successive small transformations  without jumping from one location to another.
Optimal transport theory provides a way to construct such a continuous path by means of the
continuity equation. Given a vector ﬁeld Vt on X and an initial condition ν0  the continuity equation
is a partial differential equation which deﬁnes a path νt evolving under the action of the vector ﬁeld
Vt  and reads ∂tνt = −div(νtVt) for all t ≥ 0. The reader can ﬁnd more detailed discussions in
Appendix A.2 or [39]. Following [1]  a natural choice is to choose Vt as the negative gradient of the
differential of F(νt) at νt  since it corresponds to a gradient ﬂow of F associated with the W2 metric
(see Appendix A.3). By (2)  we know that the differential of F(νt) at νt is given by fµ νt  hence
Vt(x) = −∇fµ νt(x).1 The gradient ﬂow of F is then deﬁned by the solution (νt)t≥0 of

∂tνt = div(νt∇fµ νt).

(5)
Equation (5) is non-linear in that the vector ﬁeld depends itself on νt. This type of equation is
associated in the probability theory literature to the so-called McKean-Vlasov process [26  30] 

dXt = −∇fµ νt(Xt)dt

X0 ∼ ν0.

(6)

In fact  (6) deﬁnes a process (Xt)t≥0 whose distribution (νt)t≥0 satisﬁes (5)  as shown in Proposi-
tion 1. (Xt)t≥0 can be interpreted as the trajectory of a single particle  starting from an initial random
position X0 drawn from ν0. The trajectory is driven by the velocity ﬁeld −∇fµ νt  and is affected
by other particles. These interactions are captured by the velocity ﬁeld through the dependence on
the current distribution νt of all particles. Existence and uniqueness of a solution to (5) and (6) are
guaranteed in the next proposition  whose proof is given Appendix C.1.
Proposition 1. Let ν0 ∈ P2(X ). Then  under Assumption (A)  there exists a unique process (Xt)t≥0
satisfying the McKean-Vlasov equation in (6) such that X0 ∼ ν0. Moreover  the distribution νt of Xt
is the unique solution of (5) starting from ν0  and deﬁnes a gradient ﬂow of F.

1Also  Vt = ∇V + ∇W (cid:63) νt (see Appendix A.3) where (cid:63) denotes the classical convolution.

3

Besides existence and uniqueness of the gradient ﬂow of F  one expects F to decrease along the path
νt and ideally to converge towards 0. The ﬁrst property  stated in the next proposition  is rather easy
to get and is the object of Proposition 2  similar to the result for KSD ﬂow in [32  Section 3.1].
Proposition 2. Under Assumption (A)  F(νt) is decreasing in time and satisﬁes:

= −

(cid:107)∇fµ νt(x)(cid:107)2 dνt(x).

(7)

(cid:90)

dF(νt)

dt

This property results from (5) and the energy identity in [1  Theorem 11.3.2] and is proved in
Appendix C.1. From (7)  F can be seen as a Lyapunov functional for the dynamics deﬁned by
(5)  since it is decreasing in time. Hence  the continuous-time gradient ﬂow introduced in (5)
allows to formally consider the notion of gradient descent on P2(X ) with F as a cost function. A
time-discretized version of the ﬂow naturally follows  and is provided in the next section.

2.2 Euler scheme
We consider here a forward-Euler scheme of (5). For any T : X → X a measurable map  and
ν ∈ P2(X )  we denote the pushforward measure by T#ν (see Appendix A.2). Starting from
ν0 ∈ P2(X ) and using a step-size γ > 0  a sequence νn ∈ P2(X ) is given by iteratively applying
(8)

νn+1 = (I − γ∇fµ νn )#νn.

For all n ≥ 0  equation (8) is the distribution of the process deﬁned by
X0 ∼ ν0.

Xn+1 = Xn − γ∇fµ νn(Xn)

(9)
The asymptotic behavior of (8) as n → ∞ will be the object of Section 3. For now  we provide a
guarantee that the sequence (νn)n∈N approaches (νt)t≥0 as the step-size γ → 0.
Proposition 3. Let n ≥ 0. Consider νn deﬁned in (8)  and the interpolation path ργ
t = (I − (t − nγ)∇fµ νn )#νn  ∀t ∈ [nγ  (n + 1)γ). Then  under Assumption (A)  ∀ T > 0 
ργ

t deﬁned as:

W2(ργ

t   νt) ≤ γC(T ) ∀t ∈ [0  T ]

(10)

where C(T ) is a constant that depends only on T .

A proof of Proposition 3 is provided in Appendix C.2 and relies on standard techniques to control
the discretization error of a forward-Euler scheme. Proposition 3 means that νn can be linearly
interpolated giving rise to a path ργ
t which gets arbitrarily close to νt on bounded intervals. Note that
as T → ∞ the bound C(T ) it is expected to blow up. However  this result is enough to show that (8)
is indeed a discrete-time ﬂow of F. In fact  provided that γ is small enough  F(νn) is a decreasing
sequence  as shown in Proposition 4.
Proposition 4. Under Assumption (A)  and for γ ≤ 2/3L  the sequence F(νn) is decreasing  and

F(νn+1) − F(νn) ≤ −γ(1 − 3γ
2

L)

(cid:107)∇fµ νn (x)(cid:107)2 dνn(x) 

∀n ≥ 0.

(cid:90)

Proposition 4  whose proof is given in Appendix C.2  is a discrete analog of Proposition 2. In fact 
(8) is intractable in general as it requires the knowledge of ∇fµ νn (and thus of νn) exactly at each
iteration n. Nevertheless  we present in Section 4.2 a practical algorithm using a ﬁnite number of
samples which is provably convergent towards (8) as the sample-size increases. We thus begin by
studying the convergence properties of the time discretized MMD ﬂow (8) in the next section.

3 Convergence properties of the MMD ﬂow
We are interested in analyzing the asymptotic properties of the gradient ﬂow of F. Although we know
from Propositions 2 and 4 that F decreases in time  it can very well converge to local minima. One
way to see this is by looking at the equilibrium condition for (7). As a non-negative and decreasing
function  t (cid:55)→ F(νt) is guaranteed to converge towards a ﬁnite limit l ≥ 0  which implies in turn that
the r.h.s. of (7) converges to 0. If νt happens to converge towards some distribution ν∗  it is possible
to show that the equilibrium condition (11) must hold [31  Prop. 2]  

(cid:90)

(cid:107)∇fµ ν∗ (x)(cid:107)2 dν∗(x) = 0.

(11)

4

Condition (11) does not necessarily imply that ν∗ is a global optimum unless when the loss function
has a particular structure [11]. For instance  this would hold if the kernel is linear in at least one of its
dimensions. However  when a characteristic kernel is required (to ensure the MMD is a distance) 
such a structure can’t be exploited. Similarly  the claim that KSD ﬂow converges globally  [32  Prop.
3  Appendix B.1]  requires an assumption [32  Assump. A] that excludes local minima which are
not global (see Appendix D.1; recall KSD is related to MMD). Global convergence of the ﬂow is
harder to obtain  and will be the topic of this section. The main challenge is the lack of convexity of
F w.r.t. the Wassertein metric. We show that F is merely Λ-convex  and that standard optimization
techniques only provide a loose bound on its asymptotic value. We next exploit a Lojasiewicz type
inequality to prove convergence to the global optimum provided that a particular quantity remains
bounded at all times.

(cid:90) 1

√

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:90)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)

3.1 Optimization in a (W2) non-convex setting
The displacement convexity of a functional F is an important criterion in characterizing the con-
vergence of its Wasserstein gradient ﬂow. Displacement convexity states that t (cid:55)→ F(ρt) is a
convex function whenever (ρt)t∈[0 1] is a path of minimal length between two distributions µ and ν
(see Deﬁnition 2). Displacement convexity should not be confused with mixture convexity  which
corresponds to the usual notion of convexity. As a matter of fact  F is mixture convex in that it
satisﬁes: F(tν + (1 − t)ν(cid:48)) ≤ tF(ν) + (1 − t)F(ν(cid:48)) for all t ∈ [0  1] and ν  ν(cid:48) ∈ P2(X ) (see
Lemma 25). Unfortunately  F is not displacement convex. Instead  F only satisﬁes a weaker notion
of displacement convexity called Λ-displacement convexity  given in Deﬁnition 4 (Appendix A.4).
Proposition 5. Under Assumptions (A) to (C)  F is Λ-displacement convex  and satisﬁes

F(ρt) ≤ (1 − t)F(ν) + tF(ν(cid:48)) −

(12)
for all ν  ν(cid:48) ∈ P2(X ) and any displacement geodesic (ρt)t∈[0 1] from ν to ν(cid:48) with velocity vectors
(vt)t∈[0 1]. The functional Λ is deﬁned for any pair (ρ  v) with ρ ∈ P2(X ) and (cid:107)v(cid:107) ∈ L2(ρ) 

Λ(ρs  vs)G(s  t) ds

0

−

2λdF(ρ)

1
2

(cid:107)v(x)(cid:107)2 dρ(x) 

v(x).∇xk(x  .) dρ(x)

H

Λ(ρ  v) =

terms: thus(cid:82) 1

(13)
where (s  t) (cid:55)→ G(s  t) = s(1 − t)1{s ≤ t} + t(1 − s)1{s ≥ t} and λ is deﬁned in Assumption (C).
Proposition 5 can be obtained by computing the second time derivative of F(ρt)  which is then lower-
bounded by Λ(ρt  vt) (see Appendix D.2). In (13)  the map Λ is a difference of two non-negative
0 Λ(ρs  vs)G(s  t) ds can become negative  and displacement convexity does not hold in
general. [8  Theorem 6.1] provides a convergence when only Λ-displacement convexity holds as long
as either the potential or the interaction term is convex enough. In fact  as mentioned in [8  Remark
6.4]  the convexity of either term could compensate for a lack of convexity of the other. Unfortunately 
this cannot be applied for MMD since both terms involve the same kernel but with opposite signs.
Hence  even under convexity of the kernel  a concave term appears and cancels the effect of the
convex term. Moreover  the requirement that the kernel be positive semi-deﬁnite makes it hard to
construct interesting convex kernels. However  it is still possible to provide an upper bound on the
asymptotic value of F(νn) when (νn)n∈N are obtained using (8). This bound is given in Theorem 6 
s )s∈[0 1] is a constant speed
displacement geodesic from νn to the optimal value µ  with velocity vectors (vn
s )s∈[0 1] of constant
norm.
Theorem 6. Let ¯K be the average of (K(ρj))0≤j≤n. Under Assumptions (A) to (C) and if γ ≤ 1/3L 

and depends on a scalar K(ρn) :=(cid:82) 1

s )(1 − s) ds  where (ρn

0 Λ(ρn

s   vn

F(νn) ≤ W 2

2 (ν0  µ)
2γn

− ¯K.

(14)

Theorem 6 is obtained using techniques from optimal transport and optimization.
It relies on
Proposition 5 and Proposition 4 to prove an extended variational inequality (see Proposition 16)  and
concludes using a suitable Lyapunov function. A full proof is given in Appendix D.3. When ¯K is
non-negative  one recovers the usual convergence rate as O( 1
n ) for the gradient descent algorithm.
However  ¯K can be negative in general  and would therefore act as a barrier on the optimal value

5

that F(νn) can achieve when n → ∞. In that sense  the above result is similar to [7  Theorem 6.9].
Theorem 6 only provides a loose bound  however. In Section 3.2 we show global convergence  under
the boundedness at all times t of a speciﬁc distance between νt and µ.

by the absolute value of its time derivative(cid:82) (cid:107)∇fµ νt(x)(cid:107)2 dνt(x). The latter is the squared weighted

3.2 A condition for global convergence
The lack of convexity of F  as shown in Section 3.1  suggests that a ﬁner analysis of the convergence
should be performed. One strategy is to provide estimates for the dynamics in Proposition 2 using
differential inequalities which can be solved using the Gronwall’s lemma (see [34]). Such inequalities
are known in the optimization literature as Lojasiewicz inequalities (see [6])  and upper-bound F(νt)
Sobolev semi-norm of fµ νt (see Appendix D.4)  also written (cid:107)fµ νt(cid:107) ˙H(νt). Thus one needs to ﬁnd
a relationship between F(νt) = 1
2(cid:107)fµ νt(cid:107)2H and (cid:107)fµ νt(cid:107) ˙H(νt). For this purpose  we consider the
weighted negative Sobolev distance on P2(X )  deﬁned by duality using (cid:107).(cid:107) ˙H(ν) (see also [36]).
Deﬁnition 1. Let ν ∈ P2(x)  with its corresponding weighted Sobolev semi-norm (cid:107).(cid:107) ˙H(ν). The
weighted negative Sobolev distance (cid:107)p − q(cid:107) ˙H−1(ν) between any p and q in P2(x) is deﬁned as

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)

f (x) dp(x) −

f (x) dq(x)

(15)

(cid:107)p − q(cid:107) ˙H−1(ν) =

sup

f∈L2(ν) (cid:107)f(cid:107) ˙H(ν)≤1

with possibly inﬁnite values.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

Equation (59) plays a fundamental role in dynamic optimal transport. It can be seen as the minimum
kinetic energy needed to advect the mass ν to q (see [32]). It is shown in Appendix D.4 that

(cid:107)fµ νt(cid:107)2H ≤ (cid:107)fµ νt(cid:107) ˙H(νt)(cid:107)µ − νt(cid:107) ˙H−1(νt).

(16)
Provided that (cid:107)µ − νt(cid:107) ˙H−1(νt) remains bounded by some positive constant C at all times  (16) leads
to a functional version of Lojasiewicz inequality for F. It is then possible to use the general strategy
explained earlier to prove the convergence of the ﬂow to a global optimum:
Proposition 7. Under Assumption (A) 

(i) If (cid:107)µ − νt(cid:107)2
(ii) If (cid:107)µ − νn(cid:107)2

˙H−1(νt)

˙H−1(νn)

≤ C  for all t ≥ 0  then: F(νt) ≤
≤ C for all n ≥ 0  then: F(νn) ≤

C

CF (ν0)−1+4t  

CF (ν0)−1+4γ(1− 3

C

2 γL)n .

2 turns out to linearize KL(µ(cid:107)νt) 1

Proofs of Proposition 7 (i) and (ii) are direct consequences of Propositions 2 and 4 and the bounded
energy assumption: see Appendix D.4. The fact that (59) appears in the context of Wasserstein ﬂows
of F is not a coincidence. Indeed  (59) is a linearization of the Wasserstein distance (see [35  36] and
Appendix D.6). Gradient ﬂows of F deﬁned under different metrics would involve other kinds of
distances instead of (59). For instance  [37] consider gradient ﬂows under a hybrid metric (a mixture
between the Wasserstein distance and KL divergence)  where convergence rates can then be obtained
provided that the chi-square divergence χ2(µ(cid:107)νt) remains bounded. As shown in Appendix D.6 
χ2(µ(cid:107)νt) 1
2 when µ and νt are close. Hence  we conjecture that
gradient ﬂows of F under a metric d can be shown to converge when the linearization of the metric
remains bounded. This can be veriﬁed on simple examples for (cid:107)µ − νt(cid:107) ˙H−1(νt) as discussed in
Appendix D.5. However  it remains hard to guarantee this condition in general. One possible approach
could be to regularize F using an estimate of (59). Indeed  [32] considers the gradient ﬂow of a
regularized version of the negative Sobolev distance which can be written in closed form  and shows
that this decreases the MMD. Combing both losses could improve the overall convergence properties
of the MMD  albeit at additional computational cost. In the next section  we propose a different
approach to improve the convergence  and a particle-based algorithm to approximate the MMD ﬂow
in practice.

4 A practical algorithm to descend the MMD ﬂow

4.1 A noisy update as a regularization
We showed in Section 3.1 that F is a non-convex functional  and derived a condition in Section 3.2 to
reach the global optimum. We now address the case where such a condition does not necessarily hold 

6

n ≥ 0 

and provide a regularization of the gradient ﬂow to help achieve global optimality in this scenario.
Our starting point will be the equilibrium condition in (11). If an equilibrium ν∗ that satisﬁes (11)
happens to have a positive density  then fµ ν∗ would be constant everywhere. This in turn would
mean that fµ ν∗ = 0 when the RKHS does not contain constant functions  as for a gaussian kernel
[44  Corollary 4.44]. Hence  ν∗ would be a global optimum since F(ν∗) = 0. The limit distribution
ν∗ might be singular  however  and can even be a dirac distribution [31  Theorem 6]. Although the
gradient ∇fµ ν∗ is not identically 0 in that case  (11) only evaluates it on the support ν∗  on which
∇fµ ν∗ = 0 holds. Hence a possible ﬁx would be to make sure that the unnormalised witness gradient
is also evaluated at points outside of the support of ν∗. Here  we propose to regularize the ﬂow by
injecting noise into the gradient during updates of (9) 

Xn+1 = Xn − γ∇fµ νn (Xn + βnUn) 

(17)
where Un is a standard gaussian variable and βn is the noise level at n. Compared to (8)  the sample
here is ﬁrst blurred before evaluating the gradient. Intuitively  if νn approaches a local optimum
ν∗  ∇fµ νn would be small on the support of νn but it might be much larger outside of it  hence
evaluating ∇fµ νn outside the support of νn can help in escaping the local minimum. The stochastic
process (17) is different from adding a diffusion term to (5). The latter case would correspond
to regularizing F using an entropic term as in [31  40] (see also Appendix A.5 on the Langevin
diffusion) and was shown to converge to a global optimum that is in general different from the global
minmum of the un-regularized loss. Eq. (17) is also different from [9  13]  where F (and thus
its associated velocity ﬁeld) is regularized by convolving the interaction potential W in (4) with a
molliﬁer. The optimal solution of a regularized version of the functional F will be generally different
from the non-regularized one  however  which is not desirable in our setting. Eq. (17) is more closely
related to the continuation methods [10  20  21] and graduated optimization [23] used for non-convex
optimization in Euclidian spaces  which inject noise into the gradient of a loss function F at each
iteration. The key difference is the dependence of fµ νn of νn  which is inherently due to functional
optimization. We show in Proposition 8 that (17) attains the global minimum of F provided that the
level of the noise is well controlled  with the proof given in Appendix E.1.
Proposition 8. Let (νn)n∈N be deﬁned by (17) with an initial ν0. Denote Dβn (νn) =
Ex∼νn u∼g[(cid:107)∇fµ νn(x + βnu)(cid:107)2] with g the density of the standard gaussian distribution. Un-
der Assumptions (A) and (D)  and for a choice of βn such that
nF(νn) ≤ Dβn (νn) 
8λ2β2
F(νn+1) − F(νn) ≤ − γ
2
F(νn) ≤ F(ν0)e−4λ2γ(1−3γL)(cid:80)n
i=0 β2
i .
√
i → ∞ holds is when βn decays as 1/

Moreover if(cid:80)n
A particular case where(cid:80)n

where λ and L are deﬁned in Assumptions (A) and (D) and depend only on the choice of the kernel.

i=0 β2

n while still satisfying (18).
In this case  convergence occurs in polynomial time. At each iteration  the level of the noise needs to
be adjusted such that the gradient is not too blurred. This ensures that each step decreases the loss
functional. However  βn does not need to decrease at each iteration: it could increase adaptively
whenever needed. For instance  when the sequence gets closer to a local optimum  it is helpful to
increase the level of the noise to probe the gradient in regions where its value is not ﬂat. Note that for
βn = 0 in (19)   we recover a similar bound to Proposition 4.

(1 − 3γL)Dβn (νn) 

the following inequality holds:

i=0 β2

i → ∞  then

(18)

(19)

(20)

4.2 The sample-based approximate scheme

We now provide a practical algorithm to implement the noisy updates in the previous section  which
employs a discretization in space. The update (17) involves computing expectations of the gradient
of the kernel k w.r.t the target distribution µ and the current distribution νn at each iteration n. This
suggests a simple approximate scheme  based on samples from these two distributions  where at each
iteration n  we model a system of N interacting particles (X i
n)1≤i≤N and their empirical distribution
in order to approximate νn. More precisely  given i.i.d. samples (X i
0)1≤i≤N and (Y m)1≤m≤M from
ν0 and µ and a step-size γ  the approximate scheme iteratively updates the i-th particle as

X i

n+1 = X i

n − γ∇fˆµ ˆνn (X i

n + βnU i

n) 

(21)

7

n are i.i.d standard gaussians and ˆµ  ˆνn denote the empirical distributions of (Y m)1≤m≤M
where U i
n)1≤i≤N   respectively. It is worth noting that for βn = 0  (21) is equivalent to gradient
and (X i
descent over the particles (X i
n) using a sample based version of the MMD. Implementing (21) is
straightforward as it only requires to evaluate the gradient of k on the current particles and target
samples. Pseudocode is provided in Algorithm 1. The overall computational cost of the algorithm
at each iteration is O((M + N )N ) with O(M + N ) memory. The computational cost becomes
O(M + N ) when the kernel is approximated using random features  as is the case for regression with
neural networks (Appendix F). This is in contrast to the cubic cost of the ﬂow of the KSD [32]  which
requires solving a linear system at each iteration. The cost can also be compared to the algorithm in
[40]  which involves computing empirical CDF and quantile functions of random projections of the
particles.
The approximation scheme in (21) is a particle version of (17)  so one would expect it to converge
towards its population version (17) as M and N goes to inﬁnity. This is shown below.
Theorem 9. Let n ≥ 0 and T > 0. Let νn and ˆνn deﬁned by (8) and (21) respectively. Suppose
γ ≥ n:
Assumption (A) holds and that βn < B for all n  for some B > 0. Then for any T
(e4LT − 1)

(cid:18) 1√

2 )e2LT +

(cid:19)

var(µ)

1

2 )

E [W2(ˆνn  νn)] ≤ 1
4

(B + var(ν0)

1

N

1√
M

Theorem 9 controls the propagation of the chaos at each iteration  and uses techniques from [25].
Notice also that these rates remain true when no noise is added to the updates  i.e. for the original
ﬂow when B = 0. A proof is provided in Appendix E.2. The dependence in
M underlines the
fact that our procedure could be interesting as a sampling algorithm when one only has access to M
samples of µ (see Appendix A.5 for a more detailed discussion).
Experiments

√

Figure 1: Comparison between different training methods for student-teacher ReLU networks with
gaussian output non-linearity and synthetic data uniform on a hyper-sphere. In blue  (21) is used
without noise βn = 0 while in red noise is added with the following schedule: β0 > 0 and βn is
decreased by half after every 103 epochs. In green  a diffusion term is added to the particles with
noise level kept ﬁxed during training (βn = β0). In purple  the KSD is used as a cost function instead
of the MMD. In all cases  the kernel is estimated using random features (RF) with a batch size of 102.
Best step-size was selected for each method from {10−3  10−2  10−1} and was used for 104 epochs
on a dataset of 103 samples (RF). Initial parameters of the networks are drawn from i.i.d. gaussians:
N (0  1) for the teacher and N (10−3  1) for the student. Results are averaged over 10 different runs.

Figure 1 illustrates the behavior of the proposed algorithm (21) in a simple setting and compares it
with three other methods: MMD without noise injection (blue traces)  MMD with diffusion (green
traces) and KSD (purple traces  [32]). Here  a student network is trained to produce the outputs of a
teacher network using gradient descent. More details on the experiment are provided in Appendix G.1.
As discussed in Appendix F  this setting can be seen as a stochastic version of the MMD ﬂow since the
kernel is estimated using random features at each iteration ((91) in Appendix G.1). Here  the MMD
ﬂow fails to converge towards the global optimum. Such behavior is consistent with the observations
in [11] when the parameters are initialized from a gaussian noise with relatively high variance (which
is the case here). On the other hand  adding noise to the gradient seems to lead to global convergence.
Indeed  the training error decreases below 10−5 and leads to much better validation error. While
adding a small diffusion term (green) help convergence  the noise-injection (red) still outperforms
it. This also holds for KSD (purple) which leads to a good solution (b) although at a much higher

8

1102104Time (s)105104103102101100MMD2(a) Training error02k4k6k8kEpochs102101(b) Test error105103101101noise level 102101(c) Sensitivity to noiseMMDMMD + noise injectionMMD + diffusionKSDcomputational cost (a). Our noise injection method (red) is also robust to the amount of noise and
achieves best performance over a wide region (c). On the other hand  MMD + diffusion (green)
performs well only for much smaller values of noise that are located in a narrow region. This is
expected since adding a diffusion changes the optimal solution  unlike the injection where the global
optimum of the MMD remains a ﬁxed point of the algorithm.
Another illustrative experiment on a simple ﬂow between Gaussians is given in Appendix G.2.

5 Conclusion

We have introduced MMD ﬂow  a novel ﬂow over the space of distributions  with a practical space-
time discretized implementation and a regularisation scheme to improve convergence. We provide
theoretical results  highlighting intrinsic properties of the regular MMD ﬂow  and guarantees on
convergence based on recent results in optimal transport  probabilistic interpretations of PDEs  and
particle algorithms. Future work will focus on a deeper understanding of regularization for MMD
ﬂow  and its application in sampling and optimization for large neural networks.

References

[1] L. Ambrosio  N. Gigli  and G. Savaré. Gradient ﬂows: in metric spaces and in the space of

probability measures. Springer Science & Business Media  2008.

[2] M. Arbel  D. J. Sutherland  M. Bi´nkowski  and A. Gretton. “On gradient regularizers for MMD

GANs.” In: NIPS (2018).

[3] M. Arjovsky and L. Bottou. “Towards Principled Methods for Training Generative Adversarial

Networks.” In: ICLR. 2017. arXiv: 1701.04862.

[4] M. G. Bellemare  I. Danihelka  W. Dabney  S. Mohamed  B. Lakshminarayanan  S. Hoyer 
and R. Munos. The Cramer Distance as a Solution to Biased Wasserstein Gradients. 2017.
arXiv: 1705.10743.

[5] M. Bi´nkowski  D. J. Sutherland  M. Arbel  and A. Gretton. “Demystifying MMD GANs.” In:

ICLR. 2018.

[6] A. Blanchet and J. Bolte. “A family of functional inequalities: Lojasiewicz inequalities and
displacement convex functions.” In: Journal of Functional Analysis 275.7 (2018)  pp. 1650–
1673.

[8]

[7] L. Bottou  M. Arjovsky  D. Lopez-Paz  and M. Oquab. “Geometrical insights for implicit
generative modeling.” In: Braverman Readings in Machine Learning. Key Ideas from Inception
to Current State. Springer  2018  pp. 229–268.
J. A. Carrillo  R. J. McCann  and C. Villani. “Contractions in the 2-Wasserstein Length Space
and Thermalization of Granular Media.” en. In: Archive for Rational Mechanics and Analysis
179.2 (Feb. 2006)  pp. 217–263. URL: https://doi.org/10.1007/s00205-005-0386-1
(visited on 07/27/2019).
J. A. Carrillo  K. Craig  and F. S. Patacchini. “A blob method for diffusion.” In: Calculus of
Variations and Partial Differential Equations 58.2 (2019)  p. 53.

[9]

[10] P. Chaudhari  A. Oberman  S. Osher  S. Soatto  and G. Carlier. “Deep Relaxation: partial
differential equations for optimizing deep neural networks.” In: arXiv:1704.04932 [cs  math]
(2017). URL: http://arxiv.org/abs/1704.04932.

[11] L. Chizat and F. Bach. “A Note on Lazy Training in Supervised Differentiable Programming.”
In: arXiv:1812.07956 [cs  math] (Dec. 2018). arXiv: 1812.07956. URL: http://arxiv.org/
abs/1812.07956 (visited on 05/05/2019).

[12] L. Chizat and F. Bach. “On the global convergence of gradient descent for over-parameterized

models using optimal transport.” In: NIPS  2018.

[13] K. Craig and A. Bertozzi. “A blob method for the aggregation equation.” In: Mathematics of

computation 85.300 (2016)  pp. 1681–1717.

[14] A. S. Dalalyan and A. Karagulyan. “User-friendly guarantees for the Langevin Monte Carlo

with inaccurate gradient.” In: Stochastic Processes and their Applications (2019).

[15] A. Durmus  S. Majewski  and B. Miasojedow. “Analysis of Langevin Monte Carlo via convex

optimization.” In: arXiv preprint arXiv:1802.09188 (2018).

9

[16] G. K. Dziugaite  D. M. Roy  and Z. Ghahramani. “Training generative neural networks via

Maximum Mean Discrepancy optimization.” In: UAI. 2015.

[17] A. Genevay  G. Peyré  and M. Cuturi. “Learning Generative Models with Sinkhorn Diver-

gences.” In: AISTATS. 2018. arXiv: 1706.00292.
I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville 
and Y. Bengio. “Generative Adversarial Nets.” In: NIPS. 2014. arXiv: 1406.2661.

[18]

[19] A. Gretton  K. M. Borgwardt  M. J. Rasch  B. Schölkopf  and A. Smola. “A kernel two-sample

test.” In: Journal of Machine Learning Research (2012).

[20] C. Gulcehre  M. Moczulski  F. Visin  and Y. Bengio. “Mollifying networks.” In: arXiv preprint

arXiv:1608.04980 (2016).

[21] C. Gulcehre  M. Moczulski  M. Denil  and Y. Bengio. “Noisy activation functions.” In: ICML.

2016.
I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. Courville. “Improved Training of
Wasserstein GANs.” In: NIPS. 2017. arXiv: 1704.00028.

[22]

[23] E. Hazan  K. Y. Levy  and S. Shalev-Shwartz. “On graduated optimization for stochastic

non-convex problems.” In: ICML. 2016.

[24] R. Jordan  D. Kinderlehrer  and F. Otto. “The variational formulation of the Fokker–Planck

equation.” In: SIAM journal on mathematical analysis 29.1 (1998)  pp. 1–17.

[25] B. Jourdain  S. Méléard  and W. Woyczynski. “Nonlinear SDEs driven by Levy proesses and

related PDEs.” In: arXiv preprint arXiv:0707.2723 (2007).

[26] M. Kac. “Foundations of kinetic theory.” In: Proceedings of The third Berkeley symposium on
mathematical statistics and probability. Vol. 3. University of California Press Berkeley and
Los Angeles  California. 1956  pp. 171–197.

[27] C.-L. Li  W.-C. Chang  Y. Cheng  Y. Yang  and B. Póczos. “MMD GAN: Towards Deeper
Understanding of Moment Matching Network.” In: arXiv:1705.08584 [cs  stat] (May 2017).
arXiv: 1705.08584. URL: http://arxiv.org/abs/1705.08584 (visited on 11/13/2018).

[28] Y. Li  K. Swersky  and R. Zemel. “Generative moment matching networks.” In: arXiv preprint

arXiv:1502.02761 (2015).

[29] Q. Liu. “Stein variational gradient descent as gradient ﬂow.” In: Advances in neural information

processing systems. 2017  pp. 3115–3123.

[30] H. McKean Jr. “A class of Markov processes associated with nonlinear parabolic equations.”
In: Proceedings of the National Academy of Sciences of the United States of America 56.6
(1966)  p. 1907.

[31] S. Mei  A. Montanari  and P.-M. Nguyen. “A mean ﬁeld view of the landscape of two-layer
neural networks.” In: Proceedings of the National Academy of Sciences 115.33 (2018)  E7665–
E7671.

[32] Y. Mroueh  T. Sercu  and A. Raj. “Sobolev Descent.” In: AISTATS. 2019.
[33] A. Müller. “Integral Probability Metrics and their Generating Classes of Functions.” In:

Advances in Applied Probability 29.2 (1997)  pp. 429–443.
J. A. Oguntuase. “On an inequality of Gronwall.” In: Journal of Inequalities in Pure and
Applied Mathematics (2001).

[34]

[35] F. Otto and C. Villani. “Generalization of an inequality by Talagrand and links with the
logarithmic Sobolev inequality.” In: Journal of Functional Analysis 173.2 (2000)  pp. 361–
400.
[36] R. Peyre. “Comparison between W2 distance and ˙H−1 norm  and localisation of Wasserstein
distance.” In: ESAIM: Control  Optimisation and Calculus of Variations 24.4 (2018)  pp. 1489–
1501.

[37] G. Rotskoff  S. Jelassi  J. Bruna  and E. Vanden-Eijnden. “Global convergence of neuron

birth-death dynamics.” In: ICML. 2019.

[38] G. M. Rotskoff and E. Vanden-Eijnden. “Neural networks as interacting particle systems:
Asymptotic convexity of the loss landscape and universal scaling of the approximation error.”
In: arXiv preprint arXiv:1805.00915 (2018).

[39] F. Santambrogio. “Optimal transport for applied mathematicians.” In: Birkäuser  NY 55 (2015) 

pp. 58–63.

10

[40] U. ¸Sim¸sekli  A. Liutkus  S. Majewski  and A. Durmus. “Sliced-Wasserstein ﬂows: Nonpara-

metric generative modeling via optimal transport and diffusions.” In: ICML. 2019.
J. Sirignano and K. Spiliopoulos. “Mean ﬁeld analysis of neural networks: A central limit
theorem.” In: arXiv preprint arXiv:1808.09372 (2018).

[41]

[42] A. J. Smola and B. Scholkopf. Learning with kernels. Vol. 4. Citeseer  1998.
[43] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Schölkopf  and G. R. Lanckriet. “Hilbert
space embeddings and metrics on probability measures.” In: Journal of Machine Learning
Research 11.Apr (2010)  pp. 1517–1561.
I. Steinwart and A. Christmann. Support Vector Machines. 1st. Springer Publishing Company 
Incorporated  2008.

[44]

[45] C. Villani. Optimal transport: old and new. Vol. 338. Springer Science & Business Media 

2008.

11

,Michael Arbel
Anna Korba
Adil SALIM
Arthur Gretton