2018,Early Stopping for Nonparametric Testing,Early stopping of iterative algorithms is an algorithmic regularization method to avoid over-fitting in estimation and classification. In this paper  we show that early stopping can also be applied to obtain the minimax optimal testing in a general non-parametric setup. Specifically  a Wald-type test statistic is obtained based on an iterated estimate produced by functional gradient descent algorithms in a reproducing kernel Hilbert space. A notable contribution is to establish a ``sharp'' stopping rule: when the number of iterations achieves an optimal order  testing optimality is achievable; otherwise  testing optimality becomes impossible. As a by-product  a similar sharpness result is also derived for minimax optimal estimation under early stopping. All obtained results hold for various kernel classes  including Sobolev smoothness classes and Gaussian kernel classes.,Early Stopping for Nonparametric Testing

Meimei Liu

Department of Statistical Science

Duke University

Durham  NC 27705

meimei.liu@duke.edu

Guang Cheng

Department of Statistics

Purdue University

West Lafayette  IN 47907

chengg@purdue.edu

Abstract

Early stopping of iterative algorithms is an algorithmic regularization method to
avoid over-ﬁtting in estimation and classiﬁcation. In this paper  we show that
early stopping can also be applied to obtain the minimax optimal testing in a
general non-parametric setup. Speciﬁcally  a Wald-type test statistic is obtained
based on an iterated estimate produced by functional gradient descent algorithms
in a reproducing kernel Hilbert space. A notable contribution is to establish a
“sharp” stopping rule: when the number of iterations achieves an optimal order 
testing optimality is achievable; otherwise  testing optimality becomes impossible.
As a by-product  a similar sharpness result is also derived for minimax optimal
estimation under early stopping. All obtained results hold for various kernel classes 
including Sobolev smoothness classes and Gaussian kernel classes.

1

Introduction

As a computationally efﬁcient approach  early stopping often works by terminating an iterative
algorithm on a pre-speciﬁed number of steps to avoid over-ﬁtting. Recently  various forms of early
stopping have been proposed in estimation and classiﬁcation. Examples include boosting algorithms
(Bühlmann and Yu [2003]  Zhang and Yu [2005]  Wei et al. [2017]); gradient descent over reproducing
kernel Hilbert spaces (Yao et al. [2007]  Raskutti et al. [2014]) and reference therein. However 
statistical inference based on early stopping has largely remained unexplored.
In this paper  we apply the early stopping regularization to nonparametric testing and characterize
its minimax optimality from an algorithmic perspective. Notably  it differs from the traditional
framework of using penalization methods to conduct statistical inference. Recall that classical
nonparametric inference often involves minimizing objective functions in the loss + penalty form to
avoid overﬁtting; examples include the penalized likelihood ratio test  Wald-type test  see Fan and
Jiang [2007]  Shang and Cheng [2013]  Liu et al. [2018] and reference therein. However  solving a
quadratic program in the penalized regularization requires O(n3) basic operations. Additionally  in
practice cross validation method (Golub et al. [1979]) is often used as a tuning procedure which is
known to be optimal for estimation but suboptimal for testing; see Fan et al. [2001]. As far as we are
aware  there is no theoretically justiﬁed tuning procedure for obtaining optimal testing in our setup.
We address this issue by proposing a data-dependent early stopping rule that enjoys both theoretical
support and computational efﬁciency.
To be more speciﬁc  we ﬁrst develop a Wald-type test statistic Dn t based on the iterated estimator
ft with t being the number of iterations. As illustrated in Figure 1 (a) and (b)  the testing power
demonstrates a parabolic pattern. Speciﬁcally  it increases as the iteration grows in the beginning 
and then decreases after reaching its largest value when t = T ⇤  implying how over-ﬁtting affects the
power performance. To precisely quantify T ⇤  we analyze the power performance by characterizing
the strength of the weakest detectable signals (SWDS). We show that SWDS at each iteration is
controlled by the bias of the iterated estimator and the standard derivation of the test statistic. In

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

fact  each iterative step reduces the former but increase the latter. Such a tradeoff in testing is rather
different from the classical “bias-variance” tradeoff in estimation; as shown in Figure 1 (c). Hence 
the early stopping rule to be provided is different from those in the literature such as Raskutti et al.
[2014] and Wei et al. [2017]; also see Figure 1 (a) and (b) in comparison with power and MSE.

Second-order Sobolev kernel

Gaussian kernel

0.961

0.949

r
e
w
o
P

0.937

0.925

0.913

0.036

0.034

M
S
E

0.032

MSE

Power

0.030

0.991

0.989

r
e
w
o
P

0.987

0.985

0.983

100

T ⇤

200

300
Iteration

400

500

0

100

T ⇤

200

Iteration

300

(a)

(b)

MSE

Power

0.012

0.009

M
S
E

0.006

0.003

400

500

Stopping rules

eT

T ⇤
(c)

Variance	of	%$
S.D.	of	!" $
&'()* of	%$

t

Figure 1: (a) and (b) are mean square error (MSE) and power performance of gradient descent update
at each iteration with constant step size ↵ = 1; Power was calculated based on 500 replicates. (a) Data
were generated via yi = 0.5x2
i=1 ⇠ U nif [0  1] 
✏i ⇠ N (0  1). (b) Data were generated by yi = 0.5x2
i + 0.5|xi  0.5| + ✏i with sample size n = 200.
(c) Stopping rules for estimation and testing based on different tradeoff criteria.

i +0.5 sin(4⇡xi)+✏i with sample size n = 200  {xi}n

The above analysis apply to many reproducing kernels  and lead to speciﬁc optimal testing rate 
depending on their eigendecay rate. In the speciﬁc examples of polynomial decaying kernel and
exponential decaying kernel  we further show that the developed stopping rule is indeed “sharp”:
testing optimality is obtained if and only if the number of iterations obtains an optimal order deﬁned
by the stopping rule. As a by-product  we prove that the early stopping rule in Raskutti et al. [2014]
and Wei et al. [2017] is also “sharp” for optimal estimation.

2 Background and Problem Formulation

We begin by introducing some background on reproducing kernel Hilbert space (RKHS)  and func-
tional gradient descent algorithms in the RKHS  together with our nonparametric testing formulation.

2.1 Nonparametric estimation in reproducing kernel Hilbert spaces

Consider the following nonparametric model

yi = f (xi) + ✏i 

i = 1 ···   n 

(2.1)
where xi 2X⇢ Rd for a ﬁxed d  1 are random covariates  and ✏i are Gaussian random noise
with mean zero and variance 2. Throughout we assume that f 2H   where H⇢ L2(PX) is a
reproducing kernel Hilbert space (RKHS) associated with an inner product h· ·iH and a reproducing
kernel function K(· ·) : X⇥X! R. By Mercer’s Theorem  K has the following spectral expansion:
(2.2)

K(x  x0) =

µii(x)i(x0)  x  x0 2X  

1Xi=1

where µ1  µ2 ··· 0 is a sequence of eigenvalues and {i}1i=1 form a basis in L2(PX).
Moreover  for any i  j 2 N 

hi  jiL2(PX ) = ij

and

hi  jiH = ij/µi.

In the literature  e.g.  Guo [2002] and Shang and Cheng [2013]  it is common to assume that j’s are
uniformly bounded. This is also assumed throughout this paper.
Assumption A1. The eigenfunctions {k}1k=0 are uniformly bounded on X   i.e.  there exists a ﬁnite
constant cK > 0 such that

sup

j1 kjksup  cK.

2

Two types of kernel are often considered in the nonparametric literature  depending on how fast
its eigenvalues decay to zero. The ﬁrst is that µi ⇣ i2m  leading to the so-called polynomial
decay kernel (PDK) of order m > 0. For instance  an m-order Sobolev space is an RKHS with a
PDK of order m; see Wahba [1990]  and the trigonometric basis in periodic Sobolev space with
PDK satisﬁes Assumption A1 trivially. The second is that µi ⇣ exp(ip) for some constant
  p > 0  corresponding to the so-called exponential-polynomial decay kernel (EDK) of order
p > 0; see Schölkopf et al. [1999]. In particular  for EDK of order two  an example is K(x1  x2) =
exp((x1  x2)2/2). In the latter case  Assumption A1 holds according to Lu et al. [2016].
By representer theorem  any f 2H can be presented as

f (·) =

1
pn

nXi=1

wiK(xi ·) + ⇠(·) 

where ⇠ 2H and ⇠(·) ? span{K(x1 ·) ···   K(xn ·)}. Given x = (x1 ···   xn)  deﬁne an
n K(xi  xj) and f = (f (x1) ···   f (xn))  then f = pnKw  where
empirical kernel [K]ij = 1
w = (w1 ···   wn)> 2 Rn.
2.2 Gradient Descent Algorithms
Given the samples {(xi  yi)}  consider minimizing the least-square loss function

L(f ) :=

1
2n

(yi  f (xi))2

nXi=1

(2.3)

⌧ =0 ↵⌧ .

f t+1 = f t  ↵tK(f t  y) 

Consider the singular value decomposition K = U ⇤U>  where U U> = In and ⇤=

over a Hilbert space H. Note that by representer theorem  f (x) = hf  K(x ·)iH  then the gradient of
L(f ) is rL(f ) = n1Pn
i=1(f (xi)  yi)K(xi ·). Given x = (x1 ···   xn) and y = (y1 ···   yn) 
deﬁne f t = (ft(x1) ···   ft(xn)) for t = 0  1 ··· . Then straightforward calculation shows that the
functional gradient descent algorithm generates a sequence of vectors {f t}1t=0 via the recursion
where {↵t}1t=0 is the step sizes. Denote the total step size upto the t-th step as ⌘t = Pt1
diag(bµ1 bµ2 ···  bµn) with bµ1  bµ2 ··· bµn  0. We have the following assumption for
the step sizes and ⌘t.
Assumption A2. The step size {↵t}1t=0 is non-increasing; for all ⌧ = 0  1  2 ···   0  ↵⌧ 
min{1  1/bµ1}. The total step size ⌘t =Pt1
⌧ =0 ↵⌧ diverges as t ! 1; for 0  t1 ⌧ t2 as t2 ! 1  
⌘t1 ⌧ ⌘t2.
Assumption A2 supposes the step size {↵t}1t=0 to be bounded and non-increasing  but cannot decrease
too fast as t diverges. Many choices of step sizes satisfy Assumption A2. A trivial example is to
choose a constant step size ↵0 = ··· = ↵t = min{1  1/bµ1}.
Deﬁne t = argmin{j : µj < 1
eigenvalues through t.
Assumption A3. t diverges as t ! 1.
It is easy to check that Assumption A3 is satisﬁed in PDK and EDK introduced in Section 2.1.

⌘t} 1  we have the following assumption on the population

2.3 Nonparametric testing

Our goal is to test whether the nonparametric function in (2.1) is equal to some known function. To
be precise  we consider the nonparametric hypothesis testing problem
H0 : f = f⇤ v.s. H1 : f 2H \ { f⇤} 

where f⇤ is a hypothesized function. For convenience  assume f⇤ = 0  i.e.  we will test

H0 : f = 0 vs. H1 : f 2H \ { 0}.

(2.4)

3

In general  testing f = f⇤ (for an arbitrary known f⇤) is equivalent to testing f⇤ ⌘ f  f⇤ = 0.
So  (2.4) has no loss of generality. Based on the iterated estimator f t  we propose the following
Wald-type test statistic:

where kf tk2
and explicitely show how the stopping time affects minimax optimality of testing.

(2.5)
t (xi). In what follows  we will derive the null limit distribution of Dn t 

Dn t = kf tk2
n 

i=1 f 2

n = 1

nPn

3 Main Results

3.1 Stopping rule for nonparametric testing
Given a sequence of step size {↵t}1t=0 satisfying Assumption A2  we ﬁrst introduce the stopping rule
as follows:

T ⇤ := argmin8<:

<

t 2 N 1

⌘t



nvuut

nXi=1

.

min{1 ⌘ tbµi}9=;
npPn

As will be clariﬁed in Section 3.2  the intuition underlying the stopping rule (3.1) is that 1
⌘t
the bias of the iterated estimator ft  which is a decrease function of t; 1
the standard deviation of the test statistic Dn t as an increasing function of t. The optimal stopping
rule can be achieved by such a bias-standard deviation tradeoff. Recall that such a tradeoff in (3.1)
for testing is different from another type of bias-variance tradeoff in estimation (see Raskutti et al.
[2014]  Wei et al. [2017])  thus leading to different optimal stopping time. In fact  as seen in Figure 1

i=1 min{1 ⌘ tbµi} is
(c)  optimal estimation can be achieved at eT   which is earlier than than T ⇤. This is also empirically

conﬁrmed by Figure 1 (a) and (b) where minimum mean square error (MSE) can always be achieved
earlier than the maximum power. Please see Section 4 for more discussions.

controls

3.2 Minimax optimal testing
In this section  we ﬁrst derive the null limit distribution of (standardized) Dn t as a standard Gaussian
under mild conditions  that is  we only require the total step sizes ⌘t goes to inﬁnity.

⌧ =0(In  ↵⌧ ⇤). As stated in Raskutti
et al. [2014]  the matrix St describes the extent of shrinkage towards the origin. By Assumption A2

Deﬁne a sequence of diagonal shrinkage matrices as St =Qt1
that 0  ↵t  min{1  1/bµ1}  St is positive semideﬁnite.
Theorem 3.1. Suppose Assumption A2  A3 are satisﬁed. Then under H0  as n ! 1 and t ! 1 
we have

Dn t  µn t

n t

d! N (0  1).

Here µn t = EH0[Dn t|x] = 1
Then based on Theorem 3.1  we have the following testing rule at signiﬁcance level ↵:

n t = VarH0[Dn t|x] = 2

n tr((In  St)2) and 2

n2 tr((I  St)4).

(3.1)

n t = I(|Dn t  µn t| z1↵/2n t) 

where z1↵/2 is the 100 ⇥ (1  ↵/2)th percentile of standard normal distribution.
nPn
Lemma 3.2. µn t ⇣ 1
i=1 min{1 ⌘ tbµi}  and 2
Deﬁne the squared separation rate

n t ⇣ 1

n2Pn
nXi=1

i=1 min{1 ⌘ tbµi}.
min{1 ⌘ tbµi}.

1

nvuut

d2
n t =

1
⌘t

+ n t ⇣

1
⌘t

+

The separation rate dn t is used to measure the distance between the null hypothesis and a sequence of
alternative hypotheses. The following Theorem 3.3 shows that  if the alternative signal f is separated
from zero by an order dn t  then the proposed test statistic Dn t asymptotically achieves high power

4

at the total step size ⌘t. To achieve the maximum power  we need to minimize dn t. Under the
stopping rule (3.1)  we can see that when t = T ⇤  the separation rate achieves its minimal value as
d⇤n := dn T ⇤.
Theorem 3.3. (a) Suppose Assumption A2 and A3 are satisﬁed. For any "> 0  there exist positive

constants C"  t" and N" such that with probability greater than 1  ect 

inf
tt"

inf
nN"

inf
f2B

kfknC"dn t

Pf (n t = 1|x)  1  " 

where c is a constant  B = {f 2H : kfkH  C} for a constant C and Pf (·) is the probability
measure under f.

(b) The separation rate dn t achieves its minimal value as d⇤n := dn T ⇤.
The general Theorem 3.3 implies the following concrete stopping rules under various kernel classes.
Corollary 3.4. (PDK of order m) Suppose Assumption A2 holds and m > 3/2. Then at time T ⇤
with ⌘T ⇤ ⇣ n4m/(4m+1)  for any "> 0  there exist constants C" and N" such that  with probability
greater than 1  ecmn(2m3)/(2m1)  ec1n2/(4m+1) 

inf
nN"

inf
f2B

kfknC"n 2m

4m+1

Pf (n T ⇤ = 1|x)  1  " 

where cm is an absolute constant depending on m only  c1 is a constant.

Note that the minimal separation rate n 2m

4m+1 is minimax optimal according to (Ingster [1993]).
Thus  Dn T ⇤ is optimal when ⌘T ⇤ ⇣ n4m/(4m+1). Note that ⌘T ⇤ =PT ⇤1
t=0 ↵t  T ⇤ ⇣ n4m/(4m+1)
when constant step sizes are chosen.
Corollary 3.5. (EDK of order p) Suppose Assumption A2 holds and p  1. Then at time T ⇤ with
⌘T ⇤ ⇣ n(log n)1/(2p)  for any "> 0  there exist constants C" and N" such that  with probability
greater than 1  ec p n(log n)2/p  ec1(log n)1/p 

inf
nN"

inf
f2B
kfknC"n 1

2 (log n)

1
4p

Pf (n T ⇤ = 1|x)  1  " 

where c p is an absolute constant depending on   p.

Note that the minimal separation rate n1/2(log n)1/(4p) is proven to be minimax optimal in Corollary
1 of Wei and Wainwright [2017]. Hence  Dn T ⇤ is optimal at the total step size ⌘T ⇤ ⇣ n(log n)1/(2p).
When the step sizes are chosen as constants  then the corresponding T ⇤ ⇣ n(log n)1/(2p).
3.3 Sharpness of the stopping rule

Theorem 3.3 shows that optimal testing can be achieved when t = T ⇤. In the speciﬁc examples
of PDK and EDK  Theorem 3.6 further shows that when t ⌧ T ⇤ or t  T ⇤  there exists a local
alternative f that is not detectable by Dn t even when it is separated from zero by d⇤n. In this case 
the asymptotic testing power is actually smaller than ↵. Hence  we claim that T ⇤ is sharp in the sense
that testing optimality is obtained if and only if the total step size achieves the order of ⌘T ⇤. Given a
sequence of step size {↵t}1t=0 satisfying Assumption A2  we have the following results.
Theorem 3.6. Suppose Assumption A2 holds  and t ⌧ T ⇤ or t  T ⇤. There exists a positive
constant C1 such that  with probability approaching 1 

lim sup
n!1

inf
f2B

kfknC1d⇤n

Pf (n t = 1|x)  ↵.

In the proof  we construct the alternative function asPn
and (A.9) for the two cases t ⌧ T ⇤ and t  T ⇤  respectively.

i=1 K(xi ·)wi  with wi being deﬁned in (A.8)

5

4 Sharpness of early stopping in nonparametric estimation

In this section  we review the existing early stopping rule for estimation  and further explore its
“sharpness” property. In the literature  Raskutti et al. [2014] and Wei et al. [2017] proposed to use the
ﬁxed point of local empirical Rademacher complexity to deﬁne the stopping rule as follows

eT := argmin(t 2 N 1

⌘t

<


n

nXi=1

min{1 ⌘ tbµi}) .

Given the above stopping rule  the following theorem holds where f⇤ represents truth.

(4.1)

versal positive constants (c1  c2) such that the following events hold with probability at least

Theorem 4.1 (Raskutti et al. [2014]). Given the stopping time eT deﬁned by (4.1)  there are uni-
1  c1 exp(c2n/⌘eT ):
(a) For all iterations t = 1  2 ···  eT : kft  f⇤k2
(b) At the iteration eT   kfeT  f⇤k2
(c) For all t  eT  

n  12 1
⌘eT

n  4

2

2

e⌘t

.

.

.

Ekft  f⇤k2

n 

4  1

n

nXi=1

min{1 bµi⌘t} 

⌘eT

To show the sharpness of eT   it sufﬁces to examine the upper bound in Theorem 4.1 (a). In particular 
we prove a complementary lower bound result. Speciﬁcally  Theorem 4.2 implies that once t ⌧ eT  
the rate optimality will break down for at least one true f 2B with high probability. Denote the
stopping time eT satisfying

⌘eT ⇣⇢n2m/(2m+1)

n/(log n)1/p

K is PDK of order m 
K is EDK of order p.

Theorem 4.2. (a) (PDK of order m) Suppose Assumption A2 holds and m > 3

with probability approaching 1  it holds that

2. For all t ⌧ eT  

sup

f2B kft  f⇤k2
n 

cm2
⌘t 

.

1

⌘eT

sup

f2B kft  f⇤k2

n 

.

⌘eT

(b) (EDK of order p) Suppose Assumption A2 holds and p  1. For all t ⌧ eT   with probability

approaching 1 

1

kft  f⇤k2

At last  we comment brieﬂy that the stopping rule for estimation and Theorem 4.1 (a)  (b) can also be

Combining with Theorem 4.1  we claim that eT is a “sharp” stopping time for estimation.
obtained in our framework as a by-product. Intuitively  the stopping time eT in (4.1) is achieved by
the classical bias-variance tradeoff. Note that kft  f⇤k2
n has a trivial upper bound
+2k E✏ ft  f⇤k2
n  2kft  E✏ ftk2
{z
}
|
}
where the expectation is taken with respect to ✏. The squared bias term is bounded by 1
⌘t
A.3); the variance term is bounded by the mean of Dn t  that is  kft  E ftk2
nPn
Lemma A.1)  where µn t = tr((I  St)2)/n ⇣ 1
i=1 min{1 ⌘ tbµi} as shown in Lemma 3.2.
Obviously  according to (4.1)  when t ⌧ eT   the squared bias will dominate the variance.

(see Lemma
n = OP (µn t) (see

{z

Squared bias

Variance

|

n

n

 

6

5 Numerical Study

In this section  we compare our testing method with an oracle version of stopping rule that uses
knowledge of f⇤  as well as the test based on the penalized regularization. We further conduct the
simulation studies to verify our theoretical results.
An oracle version of early stopping rule The early stopping rule deﬁned in (3.1) involves the bias
of the iterated estimator ft that can be directly calculated as

k E ft  f⇤k2

n = kStU>f⇤k2

n =

1
n

(St

ii)2[U>f⇤(x)]2
i .

nXi=1

And the standard derivation of Dn t is n t = 1
stopping time on the exact in-sample bias of ft and the standard derivation of Dn t  which is deﬁned
as follows:

nq2 tr(I  St)4. An “oracle” method is to base its

T † := argmin(t 2 N 1

n

nXi=1

(St

ii)2[U>f⇤(x)]2

i <

(5.1)

1

nq2 tr(I  St)4) .

i

b=1 based on the pair boostrapped data {x(b)

Our oracle method represents an ideal case that the true function f⇤ is known.
Algorithm based on the early stopping rule (3.1) In the early stopping rule deﬁned in (3.1)  the
bias term is bounded by the order of 1
. To implement the stopping rule in (3.1) practically  we
⌘t
propose a boostrap method to approximate the bias term. Speciﬁcally  we calculate a sequence of
{f (b)
n to approximate
t }B
the bias term  where ftB = PB
  B is a positive integer. On the other hand  the standard

nq2 tr(I  St)4 involves calculating all eigenvalues of the kernel matrix. This

derivation term 1
step can be implemented by many methods on fast computation of kernel eigenvalues; see Stewart
[2002]  Drineas and Mahoney [2005] and reference therein.
Penalization-based test As another reference  we also conduct the penalization-based test by using

i=1  and use kStU>ftBk2

  y(b)
i }n

b=1 ftb
B

nXi=1

(Shawe-Taylor and Cristianini [2004]) deﬁned as

the test statistic as Dn  = kbfn k2
n. Here bfn  is the kernel ridge regression (KRR) estimator
f2H ( 1
bfn  := argmin

H)  
(yi  f (xi))2 + kfk2

where kfk2
H = hf  fiH with h· ·iH the inner product of H. The penalty parameter  plays the same
role of the total step size ⌘t to avoid overﬁtting. Liu et al. [2018] shows that minimax optimal testing

rate can be achieved by choosing the penalty parameter satisfying  ⇣qtr(⇤ + In)1)⇤4/n.

The speciﬁc  varies for different kernel classes. For example  in PDK  the optimal testing can be
achieved with ⇤ ⇣ n4m/(4m+1); in EDK  the corresponding ⇤ ⇣ n1(log n)1/(2p). We discover
an interesting connection that the inverse of these ⇤ share the same order as the stopping rules in
Corollary 3.4 and Corollary 3.5  respectively. Lemma 5.1 provides a theoretical explanation for such
connection.

Lemma 5.1. tr(⇤ + In)1⇤4 ⇣ trI  St4 holds if and only if  ⇣ 1

However  it is still challenging to choose the optimal penalty parameter for testing in practice. A
compromising strategy is to use cross validation (CV) method (Golub et al. [1979])  which was
invented for optimal estimation problems. In the following numerical study  we will show that the
CV-based Dn  performs less satisfactorily than our proposed early stopping method.

(5.2)

n

⌘t

.

5.1 Numerical study I
In this section  we compare our early stopping based test statistics (ES) with two other methods:
the oracle early stopping (Oracle ES) method  and the penalization-based test described above.
Particularity  we consider the hypothesis testing problem H0 : f = 0.

7

iid⇠
Data were generated from the regression model (2.1) with f (xi) = c · cos(4⇡xi)  where xi
Unif[0  1] and c = 0  1 respectively. c = 0 is used for examining the size of the test  and c = 1 is
used for examining the power of the test. The sample size n is ranged from 100 to 1000. We use
Gaussian kernel (i.e.  p = 2 in EDK) to ﬁt the data. Signiﬁcance level was chosen as 0.05. Both size
and power were calculated as the proportions of rejections based on 500 independent replications.
For the ES  we use bootstrap method to approximate the bias with B = 10 and the step size ↵ = 1.
For the penalization-based test  we use 10fold cross validation (10-fold CV) to select the penalty
parameter. For the oracle ES  we follow the stopping rule in (5.1) with constant step size ↵ = 1.
Figure 2 (a) shows that the size of the three testing methods approach the nominal level 0.05 under
various n  demonstrating the testing consistency. Figure 2 (b) displays the power of the three testing
rules. The ES exhibits better power performance than the penalization-based test with 10fold CV
under various sample sizes. Furthermore  as n increases  the power of ES approaches to the Oracle
ES  which serves as the benchmark. As shown in Figure 2 (c)  the ES enjoys great computation
efﬁciency compared with the Wald-test with 10fold CV  and it is reasonable that our proposed ES
takes more time than the oracle ES  due to the extra step for bootstrapping. In Supplementary A.8  we
show additional synthetic experiments with various c based on second-order Sobolev kernel verifying
our theoretical contribution.

0.20

0.15

e
z
S

i

0.10

0.05

0.00

Method

●

10−fold CV

Oracle ES

ES

●

●

●

●

1.00

0.75

r
e
w
o
P

0.50

●

●

●

●

●

●

●

●

0.25

●

●

100

200

300

400

500

n

600

700

800

900

1000

100

200

300

400

(a)

●

●

●

Method

●

10−fold CV

Oracle ES

ES

●

●

●

Method

●

10−fold CV

Oracle ES

ES

1500

1000

500

)
s
(
 

e
m
T

i

 
l

a
n
o

i
t

a

t

u
p
m
o
C

●

●

●

●

●

●

●

700

800

900

1000

0

●

100

●

200

●

300

400

500

600

n
(b)

500

600

700

800

900 1000

n
(c)

Figure 2: (a) is the size with signal strength c = 0; (b) is the power with signal strength c = 1; (c)
is the computational time (in seconds) for the three testing rules.

5.2 Numerical study II

In this section  we show synthetic experiments verifying our sharpness results stated in Corollary
3.4  Corollary 3.5 and Theorem 3.6. Data were generated from the regression model (2.1) with
iid⇠ Unif[0  1]  and c = 0  1  respectively. Other
f (xi) = c(0.8(xi  0.5)2 + 0.2 sin(4⇡xi))  where xi
setting is as the same as in Section 5.1.
In Figure 3 (a) and (b)  we use the second-order Sobolev kernel (i.e.  m = 2 in PDK) to ﬁt the model 
and set the constant step size ↵ = 1. Corollary 3.4 suggests that optimal power can be achieved at
the stopping time T ⇤ ⇣ n8/9. To display the impact of the stopping time on power performance 
we set the total iteration steps T as (n8/9) with  = 2/3  1  4/3 and n ranges from 100 to 1000.
Figure 3 (a) shows that the size approaches the nominal level 0.05 under various choices of (  n) 
demonstrating the testing consistency supported by Theorem 3.1. Figure 3 (b) displays the power of
our testing rule. A key observation is that the power under the theoretically derived stopping rule
( = 1) performs best  compared with other stopping choices ( = 2/3  4/3). In Figure 3 (c) and (d) 
we use Gaussian kernel (i.e.  p = 2 in EDK) to ﬁt the model  and set the constant step size ↵ = 1.
Here we set the total iteration steps as (n/(log n)1/4) with  = 2/3  1  4/3 and n ranges from 100
to 1000. Note that  = 1 corresponds to the optimal stopping time in Corollary 3.5. Overall  the
interpretations are similar to Figure 3 (a) and (b) for PDK.

6 Discussion

The main contribution of this paper is that we apply the early stopping strategy to nonparametric
testing  and propose the ﬁrst “sharp” stopping rule to guarantee minimax optimal testing (to the best
of our knowledge). Our stopping rule depends on the eigenvalues of the kernel matrix  especially the
ﬁrst few leading eigenvalues. There are many efﬁcient methods to compute the top eigenvalues fast 

8

0.20

0.15

e
z
S

i

0.10

●

●

●

●

●

●

●

●

0.05

0.00

100

200

300

400

600
500
Sample Size
(a)

700

800

900

1000

100

200

300

400

γ

●

2/3

1

4/3

●

1.00

0.75

r
e
w
o
P

0.50

●

0.25

0.00

●

●

●

●

●

●

●

●

γ

●

2/3

1

4/3

0.20

0.15

e
z
S

i

0.10

0.05

0.00

γ

●

2/3

1

4/3

●

●

●

●

●

●

●

●

●

●

700

800

900

1000

100

200

300

400

700

800

900

1000

500
600
Sample Size
(c)

1.00

0.75

r
e
w
o
P

0.50

0.25

0.00

●

●

500
600
Sample Size
(b)

●

●

●

γ

●

2/3

1

4/3

700

800

900

1000

●

●

●

●

●

●

●

100

200

300

400

500
600
Sample Size
(d)

Figure 3: (a) is the size of Dn t with signal strength c = 0 under PDK; (b) is the power of Dn t with
signal strength c = 1 under PDK. (c) is the size of Dn t with signal strength c = 0 under EDK; (d)
is the power of Dn t with signal strength c = 1 under EDK.

see Drineas and Mahoney [2005]  Ma and Belkin [2017]. As a future work  we can also introduce the
randomly projected kernel methods to accelerate the computing time.

References
Peter Bühlmann and Bin Yu. Boosting with the l 2 loss: regression and classiﬁcation. Journal of the

American Statistical Association  98(462):324–339  2003.

Petros Drineas and Michael W Mahoney. On the nyström method for approximating a gram matrix
for improved kernel-based learning. journal of machine learning research  6(Dec):2153–2175 
2005.

Jianqing Fan and Jiancheng Jiang. Nonparametric inference with generalized likelihood ratio tests.

TEST  16(3):409–444  Dec 2007. ISSN 1863-8260.

Jianqing Fan  Chunming Zhang  and Jian Zhang. Generalized likelihood ratio statistics and wilks

phenomenon. The Annals of statistics  29(1):153–193  2001.

Gene H Golub  Michael Heath  and Grace Wahba. Generalized cross-validation as a method for

choosing a good ridge parameter. Technometrics  21(2):215–223  1979.

Wensheng Guo. Inference in smoothing spline analysis of variance. Journal of the Royal Statistical

Society: Series B (Statistical Methodology)  64(4):887–898  2002.

Yuri I Ingster. Asymptotically minimax hypothesis testing for nonparametric alternatives. i  ii  iii.

Mathematical Methods of Statistics  2(2):85–114  1993.

Meimei Liu  Zuofeng Shang  and Guang Cheng. Nonparametric testing under random projection.

arXiv preprint arXiv:1802.06308  2018.

Junwei Lu  Guang Cheng  and Han Liu. Nonparametric heterogeneity testing for massive data. arXiv

preprint arXiv:1601.06212  2016.

Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on large-scale
shallow learning. In Advances in Neural Information Processing Systems  pages 3781–3790  2017.

Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Early stopping and non-parametric regression:
an optimal data-dependent stopping rule. Journal of Machine Learning Research  15(1):335–366 
2014.

Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.

Electronic Communications in Probability  18(82):1–9  2013.

Bernhard Schölkopf  Christopher JC Burges  and Alexander J Smola. Advances in kernel methods:

support vector learning. MIT press  1999.

Zuofeng Shang and Guang Cheng. Local and global asymptotic inference in smoothing spline models.

The Annals of Statistics  41(5):2608–2638  2013.

9

John Shawe-Taylor and Nello Cristianini. Kernel methods for pattern analysis. Cambridge university

press  2004.

Gilbert W Stewart. A krylov–schur algorithm for large eigenproblems. SIAM Journal on Matrix

Analysis and Applications  23(3):601–614  2002.

Grace Wahba. Spline models for observational data. SIAM  1990.
Yuting Wei and Martin J Wainwright. The local geometry of testing in ellipses: Tight control via

localized kolomogorov widths. arXiv preprint arXiv:1712.00711  2017.

Yuting Wei  Fanny Yang  and Martin J Wainwright. Early stopping for kernel boosting algorithms:
A general analysis with localized complexities. In Advances in Neural Information Processing
Systems  pages 6067–6077  2017.

Yuan Yao  Lorenzo Rosasco  and Andrea Caponnetto. On early stopping in gradient descent learning.

Constructive Approximation  26(2):289–315  2007.

Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. The Annals of

Statistics  33(4):1538–1579  2005.

10

,Meimei Liu
Guang Cheng