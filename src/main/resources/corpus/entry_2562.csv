2019,Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks,When optimizing over-parameterized models  such as deep neural networks  a large set of parameters can achieve zero training error. In such cases  the choice of the optimization algorithm and its respective hyper-parameters introduces biases that will lead to convergence to specific minimizers of the objective. Consequently  this choice can be considered as an implicit regularization for the training of over-parametrized models. In this work  we push this idea further by studying the discrete gradient dynamics of the training of a two-layer linear network with the least-squares loss. Using a time rescaling  we show that  with a vanishing initialization and a small enough step size  this dynamics sequentially learns the solutions of a reduced-rank regression with a gradually increasing rank.,Implicit Regularization of Discrete Gradient

Dynamics in Linear Neural Networks

Gauthier Gidel
Mila & DIRO

Universit´e de Montr´eal

Francis Bach

INRIA & ´Ecole Normale Sup´erieure

PSL Research University  Paris

Simon Lacoste-Julien∗

Mila & DIRO

Universit´e de Montr´eal

Abstract

When optimizing over-parameterized models  such as deep neural networks  a
large set of parameters can achieve zero training error. In such cases  the choice of
the optimization algorithm and its respective hyper-parameters introduces biases
that will lead to convergence to speciﬁc minimizers of the objective. Consequently 
this choice can be considered as an implicit regularization for the training of
over-parametrized models. In this work  we push this idea further by studying
the discrete gradient dynamics of the training of a two-layer linear network with
the least-squares loss. Using a time rescaling  we show that  with a vanishing
initialization and a small enough step size  this dynamics sequentially learns the
solutions of a reduced-rank regression with a gradually increasing rank.

1

Introduction

When optimizing over-parameterized models  such as deep neural networks  a large set of parameters
leads to a zero training error. However they lead to different values for the test error and thus have
distinct generalization properties. More speciﬁcally  Neyshabur [2017  Part II] argues that the choice
of the optimization algorithm (and its respective hyperparameters) provides an implicit regularization
with respect to its geometry: it biases the training  ﬁnding a particular minimizer of the objective.
In this work  we use the same setting as Saxe et al. [2018]: a regression problem with least-squares
loss on a multi-dimensional output. Our prediction is made either by a linear model or by a two-layer
linear neural network [Saxe et al.  2018]. We extend their work which covered the continuous gradient
dynamics  to weaker assumptions as well as analyze the behavior of the discrete gradient updates
We show that with a vanishing initialization and a small enough step-size  the gradient dynamics of a
two-layer linear neural network sequentially learns components that can be ranked according to a
hierarchical structure whereas the gradient dynamics induced by the same regression problem but
with a linear prediction model instead learns these components simultaneously  missing this notion of
hierarchy between components. The path followed by the two-layer formulation actually corresponds
to successively solving the initial regression problem with a growing low rank constraint which is
also know as reduced-rank regression [Izenman  1975]. Note that this notion of path followed by
the dynamics of a whole network is different from the notion of path introduced by Neyshabur et al.
[2015a] which corresponds to a path followed inside a ﬁxed network  i.e.  one corresponds to training
dynamics whereas the other corresponds to the propagation of information inside a network.
To sum-up  in our framework  the path followed by the gradient dynamics of a two-layer linear network
provides an implicit regularization that may lead to potentially better generalization properties. Our
contributions are the following:

∗CIFAR fellow  Canada CIFAR AI chair

Correspondance to the ﬁrst author: <firstname>.<lastname>@umontreal.ca

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

• Under some assumptions (see Assumption 1)  we prove that both the discrete and continuous
gradient dynamics sequentially learn the solutions of a gradually less regularized version of
reduced-rank regression (Corollary 2 and 3). Among the close related work  such result on
implicit regularization regarding discrete dynamics is novel. For the continuous case  we
weaken the standard commutativity assumption using perturbation analysis.

• We experimentally verify the reasonableness of our assumption and observe improvements in
terms of generalization (matrix reconstruction in our case) using the gradient dynamics of the
two-layer linear network when compared against the linear model.

1.1 Related Work

The implicit regularization provided by the choice of the optimization algorithm has recently become
an active area of research in machine learning  putting lot of interest on the behavior of gradient
descent on deep over-parametrized models [Neyshabur et al.  2015b  2017  Zhang et al.  2017].
Several works show that gradient descent on unregularized problems actually ﬁnds a minimum norm
solution with respect to a particular norm that drastically depends on the problem of interest. Soudry
et al. [2018] look at a logistic regression problem and show that the predictor does converge to the max-
margin solution. A similar idea has been developed in the context of matrix factorization [Gunasekar
et al.  2017]. Under the assumption that the observation matrices commute  they prove that gradient
descent on this non-convex problem ﬁnds the minimum nuclear norm solution of the reconstruction
problem  they also conjecture that this result would still hold without the commutativity assumption.
This conjecture has been later partially solved by Li et al. [2018] under mild assumptions (namely
the restricted isometry property). This work has some similarities with ours  since both focus on a
least-squares regression problem over matrices with a form of matrix factorization that induces a
non convex landscape. Their problem is more general than ours (see Uschmajew and Vandereycken
[2018] for an even more general setting) but they are showing a result of a different kind from ours:
they focus on the properties of the limit solution the continuous dynamics whereas we show some
properties on the whole dynamics (continuous and discrete)  proving that it actually visits points
during the optimization that may provide good generalization. Interestingly  both results actually
share common assumptions such as a commutativity assumption (which is less restrictive in our case
since it is always true in some realistic settings such as linear autoencoders)  vanishing initialization
and a small enough step size.
Nar and Sastry [2018] also analyzed the gradient descent algorithm on a least-squares linear network
model as a discrete time dynamical system  and derived certain necessary (but not sufﬁcient) properties
of the local optima that the algorithm can converge to with a non-vanishing step size. In this work 
instead of looking at the properties of the limit solutions  we focus on the path followed by the
gradient dynamics and precisely caracterize the weights learned along this path.
Combes et al. [2018] studied the continuous dynamics of some non-linear networks under relatively
strong assumptions such as the linear separability of the data. Conversely  in this work  we do not
make such separability assumption on the data but focus on linear networks.
Finally  Gunasekar et al. [2018] compared the implicit regularization provided by gradient descent
in deep linear convolutional and fully connected networks. They show that the solution found by
gradient descent is the minimum norm for both networks but according to a different norm. In this
work  the fact that gradient descent ﬁnds the minimum norm solution is almost straightforward using
standard results on least-squares. But the path followed by the gradient dynamics reveals interesting
properties for generalization. As developed earlier  instead of focusing on the properties of the
solution found by gradient descent  our goal is to study the path followed by the discrete gradient
dynamics in the case of a two-layer linear network.
Prior work [Saxe et al.  2013  2014  Advani and Saxe  2017  Saxe et al.  2018  Lampinen and Ganguli 
2019] studied the gradient dynamics of two-layer linear networks and proved a result similar to our
Thm. 2. We consider Saxe et al. [2018] as the closest related work  we re-use their notion of simple
deep linear neural network  that we call two-layer neural networks  and use some elements of their
proofs to extend their results. However  note that their work comes from a different perspective:
through a mathematical analysis of a simple non-linear dynamics  they intend to highlight continuous
dynamics of learning where one observes the sequential emergence of hierarchically structured
notions to explain the regularities in representation of human semantic knowledge. In this work  we

2

are also considering a two-layer neural network but with an optimization perspective. We are able to
extend Saxe et al. [2018  Eq. 6 and 7] weakening the commutativity assumption considered in Saxe
et al. [2018] using perturbation analysis. In §4.1  we test to what extent our weaker assumption holds.
Our main contribution is to show a similar result on the discrete gradient dynamics  that is important
in our perspective since we aim to study the dynamics of gradient descent. This result cannot be
trivially extended from the result on the continuous dynamics. We provide details on the difﬁculties
of the proof in §3.2.

2 A Simple Deep Linear Model

In this work  we are interested in analyzing a least-squares model with multi-dimensional outputs.
Given a ﬁnite number n of inputs xi ∈ Rd   1 ≤ i ≤ n we want to predict a multi-dimensional
outputs yi ∈ Rp   1 ≤ i ≤ n with a deep linear network [Saxe et al.  2018  Gunasekar et al.  2018] 
(1)

ˆyd(x) := W (cid:62)

Deep linear model:

L ··· W (cid:62)

1 x  

where W1  . . .   WL are learned through a MSE formulation with the least-squares loss f 

(W ∗

1   . . .   W ∗

L) ∈ arg min
Wl∈Rrl−1×rl
1≤l≤L

(cid:107)Y − XW1 ··· WL(cid:107)2

2 =: f (W1  . . .   WL)  

(2)

1
2n

where r0 = d  rl ∈ N   1 ≤ l ≤ L − 1 and rL = p  X ∈ Rn×d and Y ∈ Rn×p are such that 

X(cid:62) := (x1 ··· xn) and Y (cid:62) := (y1 ··· yn)  

(3)
are the design matrices of (xi)1≤i≤n and (yi)1≤i≤n. The deep linear model (1) is a L-layer deep
linear neural network where we see hl := Wl ··· W1x for 1 ≤ l ≤ L − 1 as the lth hidden layer.
At ﬁrst  since this deep linear network cannot represent more than a linear transformation  we could
think that there is no reason to use a deeper representation L = 1. However  in terms of learning ﬂow 
we will see in §3 that for L = 2 this model has a completely different dynamics from L = 1.
Increasing L may induce a low rank constraint when r := min{rl : 1 ≤ l ≤ L − 1} < min(d  p).
In that case  (2) is equivalent to a reduced-rank regression 

W k ∗ ∈ arg min
W ∈Rp×d
rank(W )≤r

1
2n

n(cid:88)

i=1

(cid:107)Y − XW(cid:107)2
2 .

(4)

These problems have explicit solutions depending on X and Y [Reinsel and Velu  1998  Thm. 2.2].
Note that  in this work we are interested in the implicit regularization provided in the context of
over-parametrized models  i.e.  when r > min(p  d). In that case 

{W1 ··· WL : Wl ∈ Rr×l−1 rl   1 ≤ l ≤ L} = Rp×d .

3 Gradient Dynamics as a Regularizer

In this section we would like to study the discrete dynamics of the gradient ﬂow of (2)  i.e. 

l − η∇Wl f(cid:0)W (t)

[L]

(cid:1) W (0)

W (t+1)

l

= W (t)

l ∈ Rrl−1×rl   1 ≤ l ≤ L  

(5)

where we use the notation W (t)
L ). The quantity η is usually called the step-size.
In order to get intuitions on the discrete dynamics we also consider its respective continuous version 

˙Wl(t) = −∇Wl f(cid:0)W[L](t)(cid:1) Wl(0) ∈ Rrl−1×rl   1 ≤ l ≤ L  

[L] := (W (t)

1   . . .   W (t)

(6)

where for 1 ≤ l ≤ L 
˙Wl(t) is the temporal derivative of Wl(t). Note that there is no step-size in
the continuous time dynamics since it actually corresponds to the limit of (5) when η → 0. The
continuous dynamics may be more convenient to study because such differential equations may have
closed form solutions. In §3.1  we will see that under reasonable assumptions it is the case for (6).

3

3.1 Continuous dynamics

Linear model: L = 1. We start with the study of the continuous linear model  its gradient is 

where Σxy := 1

n X(cid:62)Y and Σx := 1

∇f (W ) = ΣxW − Σxy 
(7)
n X(cid:62)X. Thus  W (t) is the solution of the differential equation 
(8)

˙W (t) = Σxy − ΣxW (t)   W (0) = W0 .

Proposition 1. For any W0 ∈ Rd×p   the solution to the linear differential equation (8) is

W (t) = e−tΣx (W0 − Σ†

xΣxy) + Σ†

xΣxy  

(9)

x is the pseudoinverse of Σx.

where Σ†
This standard result on ODE is provided in §B.1. Note that when W0 → 0 we have

W (t) →
W0→0

(Id − e−tΣx )Σ†

(10)
Deep linear network: L ≥ 2. The study of the deep linear model is more challenging since for
L ≥ 2  the landscape of the objective function f is non-convex. The gradient ﬂow of (2) is
∇fWl (W[L]) = W (cid:62)
where we used the convention that W1 0 = Id and WL+1 L = Ip. Thus (6) becomes

l+1:L where Wi:j := Wi ··· Wj   1 ≤ l ≤ L   (11)

1:l−1(ΣxW −Σxy)W (cid:62)

xΣxy .

1 ≤ l ≤ L .

˙Wl(t) = W1:l−1(t)(cid:62)(Σxy − ΣxW (t))Wl+1:L(t)(cid:62)   Wl(0) ∈ Rd×p  

(12)
We obtain a coupled differential equation (12) that is harder to solve than the previous linear
differential equation (8) due  at the same time  to its non-linear components and to the coupling
between Wl   1 ≤ l ≤ L. However  in the case L = 2  Saxe et al. [2018] managed to ﬁnd an explicit
solution to this coupled differential equation under the assumption that “perceptual correlation is
minimal” (Σx = Id).2 In this work we extend Saxe et al. [2018  Eq. 7] (for L = 2) under weaker
assumptions. More precisely  we do not require the covariance matrix Σx to be the identity matrix.
Let (U   V   D) be the SVD of Σxy  our assumption is the following:
Assumption 1. There exist two orthogonal matrices U  V such that we have the joint decomposition 
(13)
where B is such that (cid:107)B(cid:107)2 ≤  and Dx  Dxy are matrices only with diagonal coefﬁcients. We note
σ1 ≥ ··· ≥ σrxy > 0 the singular values of Σxy and λ1  . . .   λrx the diagonal entries of Dx.
Since two matrices commute if and only if they are co-diagonalizable [Horn et al.  1985  Thm. 1.3.21] 
the quantity  represent to what extend Σx and ΣxyΣ(cid:62)
xy do not commute. Before solving (12) under
Assump. 1  we describe some motivating examples where the quantity  is small or zero:

Σx = U (Dx + B)U(cid:62)

Σxy = U DxyV (cid:62)  

and

• Linear autoencoder: If Y is set to X and L = 2  we recover a linear autoencoder:

ˆx(x) = W (cid:62)

2 W (cid:62)

1 x  where h := W (cid:62)

1 x is the encoded representation of x 

xy =(cid:0) 1

n X(cid:62)X(cid:1)2

ΣxyΣ(cid:62)

Thus  B = 0 .

(14)
= Σ2
x .
Note that this linear autoencoder can also be interpreted as a form of principal component
analysis. Actually  if we initialize with W1 = W (cid:62)
2   the gradient dynamics exactly recovers
the PCA of X  which is closely related to the matrix factorization problem of Gunasekar et al.
[2017]. See §A where this derivation is detailed.
• Deep linear multi-class prediction: In that case  p is the number of classes and yi is a
one-hot encoding of the class with  in practice  p (cid:28) d. The intuition on why we may expect
(cid:107)B(cid:107)2 to be small is because rank(Y ) (cid:28) rank(X) and thus the matrices of interest only
have to almost commute on a small space in comparison to the whole space  thus B would be
close to 0. We verify this intuition by computing (cid:107)B(cid:107)2 for several classiﬁcation datasets in
Table 1.
• Minimal inﬂuence of perceptual correlation: Σx ≈ Id. It is the setting discussed by Saxe
et al. [2018]. We compare this assumption for some classiﬁcation datasets with our Assump. 1
in §4.1.

2By a rescaling of the data  their proof is valid for any matrix Σx proportional to the identity matrix.

4

An explicit solution for L = 2. Under Assump. 1 and specifying the initialization  one can solve
the matrix differential equation for  = 0 and then use perturbation analysis to assess how close the
solution of (8) is to the closed form solution derived for  = 0. This result is summarized in the
following theorem proved in §B.2.
Theorem 1. When L = 2  under Assump. 1  if we initialize with W1(0) = U diag(e−δ1  . . .   e−δp )Q
and W2(0) = Q−1 diag(e−δ1   . . .   e−δd )V (cid:62) where Q is an arbitrary invertible matrix  then the
solution of (12) can be decomposed as the sum of the solution for  = 0 and a perturbation term 

(cid:113)
1 (t) := U diag(cid:0)(cid:112)w1(t)  . . .  
wp(t)(cid:1)Q
2 (t) := Q−1 diag(cid:0)(cid:112)w1(t)  . . .  (cid:112)wd(t)(cid:1)V (cid:62)

(15)

W1(t) = W 0

W2(t) = W 0

1 (t) + W 
1 (t) + W 

1 (t) where W 0
2 (t) where W 0

where we have c > 0 such that (cid:107)W 

i (t)(cid:107) ≤  · ect2 and 

e−2δi

σie2σit−2δi

  rxy < i ≤ rx (16)

  1 ≤ i ≤ rxy   wi(t) =

1 + 2e−δiλit

λi(e2σit−2δi − e−2δi) + σi

wi(t) =
where (σi) and (λi) are deﬁned is Assump. 1. Note that rank(Σxy) := rxy ≤ rank(Σx) := rx.
The main difﬁculty in this result is the perturbation analysis for which we use a consequence of
Gr¨onwall’s inequality [Gronwall  1919] (Lemma 4). The proof can be sketched in three parts: ﬁrst
showing the result for  = 0  then showing that in the case  > 0  the matrices W1(t)/t and W2(t)/t
are bounded and ﬁnally use Lemma 4 to get the perturbation bound.
This result is more general than the one provided by Saxe et al. [2018] because it requires a weaker
assumption than Σx = Id and  = 0. In doing so  we obtain a result that takes into account the
inﬂuence of correlations of the input samples. Note that Thm. 1 is only valid if the initialization
W1(0)W2(0) has the same singular vectors as Σxy. However  making such assumptions on the ini-
tialization is standard in the literature and  in practice  we can set the initialization of the optimization
algorithm in order to also ensure that property. For instance  in the case of the linear autoencoder 
one can set W1(0) = W2(0) = e−δId.
In the following subsection we will use Thm. 1 to show that the components [U ]i   1 ≤ i ≤ rxy in
the order deﬁned by the decreasing singular values of Σxy are learned sequentially by the gradient
dynamics.

rxy(cid:88)

σiuiv(cid:62)

i

Sequential learning of components. The sequential learning of the left singular vectors of Σxy
(sorted by the magnitude of its singular values) by the continuous gradient dynamics of deep linear
networks has been highlighted by Saxe et al. [2018]. They note in their Eq. (10) that the ith phase
transition happens approximately after a time Ti deﬁned as (using our notation) 

δi
σi

.

i=1

Ti :=

ln(σi) where Σxy =

(17)
They argue that as δi → ∞  the time Ti is roughly O(1/σi). The intuition is that a vanishing
initialization increases the gap between the phase transition times Ti and thus tends to separate the
learning of each components. However  a vanishing initialization just formally leads to Ti → ∞.
In this work  we introduce a notion of time rescaling in order to formalize this notion of phase
transition and we show that  after this time rescaling  the point visited between two phase transitions
is the solution of a low rank regularized version (4) of the initial problem (2) with the low rank
constraint that loosens sequentially.
The intuition behind time rescaling is that it counterbalances the vanishing initialization in (17): Since
Ti grows as fast as δi we need to multiply the time by δi  in order to grow at the same pace as Ti.
Using this rescaling we can present our theorem  proved in §B.3  which says that a vanishing
initialization tends to force the sequential learning of the component of X associated with the largest
singular value of Σxy. Note that we need to rescale the time uniformly for each component. That is
why in the following we set δi = δ   1 ≤ i ≤ max(p  d).
Theorem 2. Let us denote wi(t)  the values deﬁned in (16). If wi(0) = e−δ   1 ≤ i ≤ r  and
 = e−δ2 ln(δ) then we have that wi(δt) converge to a step function as δ → ∞:

wi(δt) →
δ→∞

σi

λi+σi

1{t = Ti} + σi

1{t > Ti} .

λi

(18)

5

where Ti := 1/σi  1{t ∈ A} = 1 if t ∈ A and 0 otherwise.

Notice how the ith components of W1 and W2 are inactive  i.e.  wi(t) is zero  for small t and
is suddenly learned when t reaches the phase transition time Ti := 1/σi. As shown in Prop. 1
and illustrated in Fig. 1  this sequential learning behavior does not occur for the non-factorized
formulation. Gunasekar et al. [2017] observed similar differences between their factorized and not
factorized formulations of matrix regression. Note that  the time rescaling we introduced is t → δt 
in order to compensate the vanishing initialization  rescaling the time and taking the limit this way
for (8) would lead to a constant function.
Gunasekar et al. [2017] also had to consider a vanishing initialization in order to show that on a
simple matrix factorization problem the continuous dynamics of gradient descent does converge to
the minimum nuclear norm solution. This assumption is necessary in such proofs in order to avoid
to initialize with wrong components. However one cannot consider an initialization with the null
matrix since it is a stationary point of the dynamics  that is why this notion of double limit (vanishing
initialization and t → ∞) is used.
From Thm. 2  two corollaries follow directly. The ﬁrst one regards the nuclear norm of the product
W1(δt)W2(δt). This corollary says that (cid:107)W1(δt)W2(δt)(cid:107)∗ is a step function and that each incre-
ment of this integer value corresponds to the learning of a new component of X. These components
are leaned by order of relevance  i.e.  by order of magnitude of their respective eigenvalues and the
learning of a new component can be easily noticed by an incremental gap in the nuclear norm of the
matrix product W1(δt)W2(δt) 
Corollary 1. Let W1(t) and W2(t) be the matrices solution of (12) deﬁned in (15). The limit of the
squared euclidean norm of W1(t)W2(t) when δ → ∞ is a step function deﬁned as 

rxy(cid:88)

i=1

σ2
i
λ2
i

(19)

2 →
δ→∞

1{Ti < t} + σ2

i

(cid:107)W1(δt)W2(δt)(cid:107)2

(λi+σi)2 1{Ti = t}
where Ti := 1/σi and σ1 > ··· > σrxy > 0 are the positive singular values of Σxy.
It is natural to look at the norm of the prod-
uct W1(δt)W2(δt) since in Thm. 2  (wi(t))
are its singular values. However  since the
rank of W1(δt)W2(δt) is discontinuously
increasing after each phase transition  any
norm would jump with respect to the rank
increments. We illustrate these jumps in
Fig. 1 where we plot
the closed form of
(cid:55)→ W (δt) and
the squared (cid:96)2 norms of t
t (cid:55)→ W1(δt)W2(δt) for vanishing initializa-
tions with Σyx = diag(10−1  10−2  10−3) and
Σx = Id.
From Thm. 2  we can notice that  between
time Tk and Tk+1  the rank of the limit ma-
trix W1W2 is actually equal to k  meaning that
at each phase transition  the rank of W1W2 is
increased by 1. Moreover  this matrix product
contains the k components of X corresponding
to the k largest singular values of Σxy. Thus 
we can show that this matrix product is the so-
lution of the k-low rank constrained version (4)
of the initial problem (2) 
Corollary 2. Let W1(t) and W2(t) be the matrices solution of (12) deﬁned in (15). We have that 

Figure 1: Closed form solution of squared (cid:96)2 norm
of W (δt) and W1(δt)W2(δt) respectively for a linear
model and a two-layer linear autoencoder  depending
on W (0) = W1(0)W2(0) = e−δId. Note that for an
autoencoder λi = σi and thus the trace norm has integer
values. According to Thm. 2  the integer trace norm
increment represents the learning of a new component.

1
σk

< t < 1

σk+1

⇒ W1(δt)W2(δt) →

δ→∞ W k ∗  

1 ≤ k ≤ rxy .

(20)

where W k ∗ is the minimum (cid:96)2 norm solution of the reduced-rank-k regression problem (4) .

6

100101102103Rescaledtime0.00.51.01.52.02.53.0kWk22kW1W2k22 δ=12.0kW1W2k22 δ=7.5kW1W2k22 δ=3.0kWk22 δ=3.0kWk22 δ=7.5kWk22 δ=12.03.2 Discrete dynamics

We are interested in the behavior of optimization methods. Thus  the gradient dynamics of interest
is the discrete one (5). A major contribution of our work is thus contained in this section. The
continuous case studied in § 3.1 provided good intuitions and insights on the behavior of the potential
discrete dynamics that we can use for our analysis.

Why the discrete analysis is challenging. Previous related work [Gunasekar et al.  2017  Saxe
et al.  2018] only provide results on the continuous dynamics. Their proofs use the fact that their
respective continuous dynamics of interest have a closed form solution (e.g.  Thm.1). To our
knowledge  no closed form solution is known for the discrete dynamics (5). Thus its analysis requires
a new proof technique. Moreover  using Euler’s integration methods  one can show that both dynamics
are close but only for a vanishing step size depending on a ﬁnite horizon. Such dependence on the
horizon is problematic since the time rescaling used in Thm. 2 would make any ﬁnite horizon go to
inﬁnity. In this section  we consider realistic conditions on the step-size (namely  it has to be smaller
than the Lipschitz constant and some notion of eigen-gap) without any dependence on the horizon.
Such assumption is relevant since we want to study the dynamics of practical optimization algorithms
(i.e.  with a step size as large as possible and without knowing in advance the horizon).

Single layer linear model.
In this paragraph  we consider the discrete update for the linear model.
Since L = 1  for notational compactness  we call Wt the matrix updated according to (5). Using the
gradient derivation (7)  the discrete update scheme for the linear model is 

Wt+1 = Wt − η(ΣxWt − Σxy) = (Id − ηΣx)Wt + ηΣxy .

Noticing that for 1/λmax(Σx) > η > 0   Id − ηΣx is invertible  this recursion (see §B.4) leads to 
(21)

xΣxy)(Id − ηΣx)t + Σ†

Wt = (W0 − Σ†

xΣxy .

We obtain a similar result as the solution of the differential equation given in Prop. 1. With a vanishing
initialization we reach a function that does not sequentially learn some components.

Two-layer linear model. The discrete update scheme for the two-layer linear network (2) is 

1

W (t+1)

= W (t)

1 − η(ΣxW (t)− Σxy)(W (t)

1 )(cid:62)(ΣxW (t)− Σxy) .
When  = 0  by a change of basis and a proper initialization  we can reduce the study of this matrix
equation to r independant dynamics (see §B.5 for more details)  for 1 ≤ i ≤ r 

2 )(cid:62)   W (t+1)

2 − η(W (t)

= W (t)

2

w(t+1)

i

= w(t)

i + ηw(t)

i (σi − λiw(t)
leading to the following theorem 

i w(t)

i ) .

Thus we can derive a bound on the iterate w(t)
Theorem 3. Under the same assumptions as Thm. 1 and  = 0  we have

i

(22)

(cid:16)(cid:113)

(cid:113)

1 = U diag

W (t)
Moreover  for any 1 ≤ i ≤ rxy  if 1 > w(0)

1   . . .  

w(t)
p

w(t)

(cid:17)
V (cid:62) .
Q and W (t)
i > 0 and 2ησi < 1  then ∀t ≥ 0   1 ≤ i ≤ rx we have 

2 = Q−1 diag

1   . . .  

w(t)
d

(cid:17)

w(t)

(cid:16)(cid:113)

(cid:113)

w(0)

i

(σi − λiw(0)

i

)e(−2ησi+4η2σ2

i )t + w(0)

i λi

≤ w(t)

i ≤

(σi − λiw(0)

i

w(0)

i

)e(−2ησi−η2σ2

i )t + w(0)

i λi

  (23)

and w(t)

i ≤

w(0)
i
1+w(0)
i λiηt

for rxy ≤ i ≤ rx. The differences with the continuous case (16) are in red.

Proof sketch. The solution of the continuous dynamics lets us think directly studying the sequence
i might be quite challenging since the solution of the continuous dynamics wi(t)−1 has a non-
w(t)
linear and non-convex behavior.
The main insight from this proof is that one can treat the discrete case using the right transformation 
to show that some sequence doee converge linearly.

7

Dataset
MNIST
CIFAR-10
ImageNet

∆xy

2.8 × 10−2
3.0 × 10−2
1.7 × 10−1

∆x

.70
.68
.70

Figure 2: Trace norm and reconstruction errors of W (t) for L = 1 and
2 as a function of t.

Table 1: Value of the quantities
∆xy and ∆x deﬁned in (27).

Thm. 2 indicates the quantity wi(t)−1 − λi
wi(t)−1 − σi

is the good candidate to show linear convergence to 0 
= (wi(0)−1 − σi
What we can expect is thus to show that the sequence (w(t)
step of the proof is to show that (w(t)
then to use (22) to get 

has similar properties. The ﬁrst
i ) is an increasing sequence smaller than one. The second step is

i )−1 − σi

)e−2ησit .

(24)

λi

λi

λi

σi

1

w(t+1)

i

σi

= 1
w(t)

− λi
1+x ≤ 1 − x + x2 for any 1 ≤ x ≤ 0 we can derive the upper and lower

1
)+η2(σi−λiw(t)

1+2(σi−λiw(t)

− λi

σi

)2

i

i

i

Then using that 1 − x ≤ 1
bounds on the linear convergence rate. See §B.5 for full proof.

(cid:19)

In order to get a similar interpretation of Thm. 3 in terms of implicit regularization  we use the
intuitions from Thm. 2. The analogy between continuous and discrete time is that the discrete time
dynamics is doing t time-steps of size η  meaning that we have W (ηt) ≈ Wt  the time rescaling in
continuous time consists in multiplying the time by δ thus we get the analog phase transition time 
(25)
i = e−δ. Thus  we show that the ith component is learned

Recall that we assumed that m(0)
around time Ti  and consequently that the components are learned sequentially 
Corollary 3. If η < 1
2σ1

  for 1 ≤ i ≤ rxy − 1  then for 1 ≤ i < rx 

and η < σi−σi+1

  η < 2 σi−σi+1

⇒ Ti := 1

i = n(0)

ηTi := 1
σi

2σ2

ησi

.

σ2
i

i+1

w(δTj )

i

→
δ→∞

if

if

or

i > rxy
i ≤ rxy and j > i .

j < i

(26)

σj η   1 ≤ j ≤ rxy and Tj := +∞ if j > rxy.

where T0 := 0  Tj := 1
This result is proved in §B.5. The quantities σi−σi+1
can be interpreted as relative eigen-
gaps. Note that they are well deﬁned since we assumed that the eigenspaces were unidimensional.
The intuition behind this condition is that the step-size cannot be larger than the eigen-gaps because
otherwise the discrete optimization algorithm would not be able to distinguish some components.

and σi−σi+1

σ2
i

σ2

i+1

(cid:18)

0

σi
λi

4 Experiments

4.1 Assump. 1 for Classiﬁcation Datasets

In this section we verify to what extent Assump. 1 is true on standard classiﬁcation datasets. For
this  we compute the normalized quantities ∆xy and ∆x representing how much Assump. 1 and the
assumption that Σx ≈ Id are respectively broken. We compute B by computing U  the left singular
vector of Σxy and looking at the non-diagonal coefﬁcients of U(cid:62)ΣxU 

(27)

(cid:13)(cid:13) ˆΣx − ˆId

(cid:13)(cid:13)2  

∆xy :=

(cid:107)B(cid:107)2
(cid:107)Σx(cid:107)2

  ∆x := 1
2

8

100101102103104105Numberofiterations012345TraceNormW1W2W100101102103104105Numberofiterations10−410−2100ReconstructionerrorW1W2Wwhere (cid:107)·(cid:107) is the Frobenius norm  the ˆΣ expressions correspond to ˆX := X/(cid:107)X(cid:107) and ˆId := Id/(cid:107)Id(cid:107).
These normalized quantities are between 0 and 1. The closer to 1  the less the assumption hold and
conversely  the closer to 0  the more the assumption approximately holds. We present the results
on three standard classiﬁcation datasets  MNIST [LeCun et al.  2010]  CIFAR10 [Krizhevsky et al. 
2014] and ImageNet [Deng et al.  2009]  a down-sampled version of ImageNet with images of size
64 × 64. In Table 1  we can see that the quantities ∆x and ∆xy do not vary much among the datasets
and that the ∆ associated with our our Assump. 1 is two orders of magnitude smaller than the ∆
associated with Saxe et al. [2018]’s assumption indicating the relevance of our assumption.

4.2 Linear Autoencoder

For an auto-encoder  we have  Y = X. We want to compare the reconstruction properties of W (t)
computed though (21) and of the matrix product W (t)
are computed
though (22). In this experiment  we have p = d = 20  n = 1000  r = 5 and we generated synthetic
data. First we generate a ﬁxed matrix B ∈ Rd×r such that  Bkl ∼ U([0  1])  1 ≤ k  l ≤ n. Then 
for 1 ≤ i ≤ n  we sample xi ∼ Bzi + i where zi ∼ N (0  D := diag(4  2  1  1/2  1/4)) and
i ∼ 10−3N (0  Id). In Fig. 2  we plot the trace norm of W (t) and W (t)
as well as their
respective reconstruction errors as a function of t the number of iterations 

2 where W (t)

and W (t)

1 W (t)

1

2

1 W (t)

2

(cid:107)W (t) − BDB(cid:62)(cid:107)2 .

(28)

We can see that the experimental results are very close to the theoretical behavior predicted with the
continuous dynamics in Figure 1. Contrary to the dynamics induced by the linear model formulation
(L = 1)  the dynamics induced by the two-layer linear network (L = 2) is very close to a step
function: each step corresponds to the learning to a new component: They are learned sequentially.

5 Discussion

There is a growing body of empirical and theoretical evidence that the implicit regularization induced
by gradient methods is key in the training of deep neural networks. Yet  as noted by Zhang et al.
[2017]  even for linear models  our understanding of the origin of generalization is limited. In this
work  we focus on a simple non-convex objective that is parametrized by a two-layer linear network.
In the case of linear regression we show that the discrete gradient dynamics also visits points that are
implicitly regularized solutions of the initial optimization problem. In that sense  in the context of
machine learning  applying gradient descent on the overparametrized model of interest  provides a
form of implicit regularization: it sequentially learns the hierarchical components of our problem
which could help for generalization. Our setting does not pretend to solve generalization in deep
neural networks; many majors components of the standard neural network training are omitted
such as the non-linearities  large values of L and the stochasticity in the learning procedure (SGD).
Nevertheless  it provides useful insights about the source of generalization in deep learning.

Acknowledgments.

This research was partially supported by the Canada CIFAR AI Chair Program  the Canada Excellence
Research Chair in “Data Science for Realtime Decision-making”  by the NSERC Discovery Grant
RGPIN-2017-06936  by a graduate Borealis AI fellowship and by a Google Focused Research award.

References
M. S. Advani and A. M. Saxe. High-dimensional dynamics of generalization error in neural networks.

arXiv preprint arXiv:1710.03667  2017.

N. Berglund. Perturbation theory of dynamical systems. arXiv preprint math/0111178  2001.

E. A. Coddington and N. Levinson. Theory of Ordinary Differential Equations. Tata McGraw-Hill

Education  1955.

R. T. d. Combes  M. Pezeshki  S. Shabanian  A. Courville  and Y. Bengio. On the learning dynamics

of deep neural networks. arXiv preprint arXiv:1809.06848  2018.

9

J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical

image database. In CVPR  2009.

T. H. Gronwall. Note on the derivatives with respect to a parameter of the solutions of a system of

differential equations. Annals of Mathematics  1919.

S. Gunasekar  B. E. Woodworth  S. Bhojanapalli  B. Neyshabur  and N. Srebro. Implicit regularization

in matrix factorization. In NIPS  2017.

S. Gunasekar  J. Lee  D. Soudry  and N. Srebro. Implicit bias of gradient descent on linear convolu-

tional networks. arXiv preprint arXiv:1806.00468  2018.

R. A. Horn  R. A. Horn  and C. R. Johnson. Matrix Analysis. Cambridge University Press  1985.

A. J. Izenman. Reduced-rank regression for the multivariate linear model. Journal of Multivariate

Analysis  1975.

A. Krizhevsky  V. Nair  and G. Hinton. The CIFAR-10 dataset. online: http://www. cs. toronto.

edu/kriz/cifar. html  2014.

A. K. Lampinen and S. Ganguli. An analytic theory of generalization dynamics and transfer learning

in deep linear networks. In ICLR  2019.

Y. LeCun  C. Cortes  and C. Burges. MNIST handwritten digit database. AT&T Labs [Online].

Available: http://yann. lecun. com/exdb/mnist  2010.

Y. Li  T. Ma  and H. Zhang. Algorithmic regularization in over-parameterized matrix sensing and
neural networks with quadratic activations. In Conference On Learning Theory  pages 2–47  2018.

K. Nar and S. Sastry. Step size matters in deep learning. In NeurIPS  2018.

B. Neyshabur. Implicit Regularization in Deep Learning. PhD thesis  TTIC  2017.

B. Neyshabur  R. R. Salakhutdinov  and N. Srebro. Path-SGD: Path-normalized optimization in deep

neural networks. In NIPS  2015a.

B. Neyshabur  R. Tomioka  and N. Srebro. In search of the real inductive bias: On the role of implicit

regularization in deep learning. In ICLR  2015b.

B. Neyshabur  R. Tomioka  R. Salakhutdinov  and N. Srebro. Geometry of optimization and implicit

regularization in deep learning. arXiv preprint arXiv:1705.03071  2017.

G. C. Reinsel and R. Velu. Multivariate Reduced-Rank Regression: Theory and Applications. Springer

Science & Business Media  1998.

A. M. Saxe  J. L. McClellans  and S. Ganguli. Learning hierarchical categories in deep neural

networks. In Proceedings of the Annual Meeting of the Cognitive Science Society  2013.

A. M. Saxe  J. L. McClelland  and S. Ganguli. Exact solutions to the nonlinear dynamics of learning

in deep linear neural networks. In ICLR  2014.

A. M. Saxe  J. L. McClelland  and S. Ganguli. A mathematical theory of semantic development in

deep neural networks. arXiv preprint arXiv:1810.10531  2018.

D. Soudry  E. Hoffer  and N. Srebro. The implicit bias of gradient descent on separable data. In ICLR 

2018.

A. Uschmajew and B. Vandereycken. On critical points of quadratic low-rank matrix optimization

problems. Tech. report (submitted)  July 2018.

C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires

rethinking generalization. 2017.

10

,David Inouye
Pradeep Ravikumar
Inderjit Dhillon
Anastasia Pentina
Ruth Urner
Gauthier Gidel
Francis Bach
Simon Lacoste-Julien