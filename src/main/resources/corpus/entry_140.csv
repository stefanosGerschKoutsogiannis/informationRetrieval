2017,A New Alternating Direction Method for Linear Programming,It is well known that  for a linear program (LP) with constraint matrix $\mathbf{A}\in\mathbb{R}^{m\times n}$  the Alternating Direction Method of Multiplier converges globally and linearly at a rate $O((\|\mathbf{A}\|_F^2+mn)\log(1/\epsilon))$. However  such a rate is related to the problem dimension and the algorithm exhibits a slow and fluctuating ``tail convergence'' in practice. In this paper  we propose a new variable splitting method of LP and prove that our method has a convergence rate of $O(\|\mathbf{A}\|^2\log(1/\epsilon))$. The proof is based on simultaneously estimating the distance from a pair of primal dual iterates to the optimal primal and dual solution set by certain residuals. In practice  we result in a new first-order LP solver that can exploit both the sparsity and the specific structure of matrix $\mathbf{A}$ and a  significant speedup for important problems such as basis pursuit  inverse covariance matrix estimation  L1 SVM and nonnegative matrix factorization problem compared with current fastest LP solvers.,A New Alternating Direction Method for Linear

Programming

Sinong Wang

Department of ECE

The Ohio State University
wang.7691@osu.edu

Ness Shroff

Department of ECE and CSE

The Ohio State University
shroff.11@osu.edu

Abstract

It is well known that  for a linear program (LP) with constraint matrix A ∈ Rm×n 
the Alternating Direction Method of Multiplier converges globally and linearly at a
rate O(((cid:107)A(cid:107)2
F + mn) log(1/)). However  such a rate is related to the problem
dimension and the algorithm exhibits a slow and ﬂuctuating “tail convergence” in
practice. In this paper  we propose a new variable splitting method of LP and prove
that our method has a convergence rate of O((cid:107)A(cid:107)2 log(1/)). The proof is based
on simultaneously estimating the distance from a pair of primal dual iterates to the
optimal primal and dual solution set by certain residuals. In practice  we result
in a new ﬁrst-order LP solver that can exploit both the sparsity and the speciﬁc
structure of matrix A and a signiﬁcant speedup for important problems such as
basis pursuit  inverse covariance matrix estimation  L1 SVM and nonnegative
matrix factorization problem compared with the current fastest LP solvers.

1

Introduction

We are interested in applying the Alternating Direction Method of Multiplier (ADMM) to solve a
linear program (LP) of the form

min
x∈Rn

cT x s.t. Ax = b  xi ≥ 0  i ∈ [nb].

(1)
where c ∈ Rn  A ∈ Rm×n is the constraint matrix  b ∈ Rm and [nb] = {1  . . .   nb}. This problem
plays a major role in numerical optimization  and has been used in a large variety of application areas.
For example  several important machine learning problems including the nonnegative matrix factor-
ization (NMF) [1]  l1-regularized SVM [2]  sparse inverse covariance matrix estimation (SICE) [3]
and the basis pursuit (BP) [4]  and the MAP inference [5] problem can be cast into an LP setting.
The complexity of the traditional LP solver is still at least quadratic in the problem dimension  i.e.  the
Interior Point method (IPM) with a weighted path ﬁnding strategy. However  many recent problems
in machine learning have extremely large-scale targeting data but exhibit a sparse structure  i.e. 
nnz(A) (cid:28) mn  where nnz(A) is the number of non-zero elements in the constraint matrix A. This
characteristic severely limits the ability of the IPM or Simplex technique to solve these problems. On
the other hand  ﬁrst-order methods have received extensive attention recently due to their ability to
deal with large data sets. These methods require a matrix vector multiplication Ax in each iteration
with complexity linear in nnz(A). However  the key challenge in designing a ﬁrst-order algorithm is
that LPs are usually non-smooth and non-strongly convex optimization problems (may not have a
unique solution). Utilizing the standard primal and dual stochastic sub-gradient descent method will
result in an extremely slow convergence rate  i.e.  O(1/2) [6].
The ADMM was ﬁrst developed in 1975 [7]  and since then there have been several LP solvers
based on this technique. Compared with the traditional Augmented Lagrangian Method (ALM)  this

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

method splits the variable into several blocks  and optimizes the augmented Lagrangian (AL) function
in a Gauss-Seidel fashion  which often results in relatively easier subproblems to solve. However 
this method suffers from a slow convergence when the number of blocks increases. Moreover  the
challenge of applying the ADMM to the LP is that the LP problem does not exhibit an explicit
separable structure among variables  which are difﬁcult to split in the traditional sense. The notable
work [8] ﬁrst applies the ADMM to solve the LP by augmenting the original n-dimensional variables
into nm−dimensions  and the resultant Augmented Lagrangian function is separable among n blocks
of variables. They prove that this method converges globally and linearly. However  the rate of this
method is dependent on the problem dimension m  n  and converges quite slowly when m  n are
large. Thus  they leave an open question on whether other efﬁcient splitting methods exist  resulting
in convergence analysis in the space with lower dimension m or n.
In this paper  we propose a new splitting method for LP  which splits the equality and inequality
constraints into two blocks. The resultant subproblems in each iteration are a linear system with a
positive deﬁnite matrix  and n one-dimensional truncation operations. We prove our new method
converges globally and linearly at a faster rate compared with the method in [8]. Speciﬁcally  the main
contributions of this paper can be summarized as follows: (i) We show that the existing ADMM in [8]
exhibits a slow and ﬂuctuating “tail convergence”  and provide a theoretical understanding of why
this phenomenon occurs. (ii) We propose a new ADMM method for LP and provide a new analysis
of the linear convergence rate of this new method  which only involves O(m + n)−dimensional
iterates. This result answers the open question proposed in [8]. (iii) We show that when the matrix A
possesses some speciﬁc structure  the resultant subproblem can be solved in closed form. For the
general constraint matrix A  we design an efﬁciently implemented Accelerated Coordinate Descent
Method (ACDM) to solve the subproblem in O(log(1/)nnz(A)) time. (iv) Practically  we show that
our proposed algorithm signiﬁcantly speeds up solving the basis pursuit  l1-regularized SVM  sparse
inverse covariance matrix estimation  and the nonnegative matrix factorization problem compared
with existing splitting method [8] and the current fastest ﬁrst-order LP solver in [9].

2 Preliminaries

In this section  we ﬁrst review several deﬁnitions that will be used in the sequel. Then we illustrate
some observations from the existing method. We also include several LP-based machine problems
that can be cast into the LP setting in the Appendix.

2.1 Notation
A twice differentiable function f : Rn → R has strong convexity parameter σ if and only if its
Hessian satisﬁes ∇2f (x) (cid:23) σI ∀x. We use (cid:107)·(cid:107) to denote standard l2 norm for vector or spectral norm
for matrix  (cid:107)·(cid:107)1 to denote the l1 norm and (cid:107)·(cid:107)F to denote the Frobenius norm. A twice differentiable
function f : Rn → R has a component-wise Lipschitz continuous gradient with constant Li if
and only if (cid:107)∇if (x) − ∇if (y)(cid:107) ≤ Li(cid:107)x − y(cid:107) ∀x  y. For example  for the quadratic function
2(cid:107)Ax − b(cid:107)2  the gradient ∇F (x) = AT (Ax − b) and the Hessian ∇2F (x) = AT A.
F (x) = 1
Hence the parameter σ and Li satisfy (choose y = x + tei  where t ∈ R  ei ∈ Rn is the unit
vector)  xAT Ax ≥ σ(cid:107)x(cid:107)2 and tAT
i Aei ≤ Li|t| ∀x  t. Thus  the σ is the smallest eigenvalue of
AT A and Li = (cid:107)Ai(cid:107)2  where Ai is the ith column of the matrix A. The projection operator of
point x into convex set S is deﬁned as [x]S = arg minu∈S (cid:107)x − u(cid:107). If S is the non-negative cone 
let [x]+ (cid:44) [x]S. Let Vi = [0 ∞) for i ∈ [nb] and Vi = R for i ∈ [nf ].

2.2 Tail Convergence of the Existing ADMM Method

The existing ADMM in [8] solves the LP (1) by following procedure: in each iteration k  go through
the following two steps:

(cid:16) AT
1. Primal update: xk+1
i + 1(cid:107)Ai(cid:107)2
xk
q (Axk − b).
2. Dual update: zk+1 = zk − λ

i =

(cid:104)

i (b−Axk)

q

(cid:17)(cid:105)

− ci−AT

i zk

λ

  i = 1  . . .   n.

Vi

We plot the solving accuracy versus the number of iterations for solving three kinds of problems (see
Fig.1 in Appendix). We can observe that it converges fast in the initial phase  but exhibits a slow and

2

ﬂuctuating convergence when the iterates approach the optimal set. This method originates from
a speciﬁc splitting method in the standard 2−block ADMM [10]. To provide some understanding
of this phenomenon  we show that this method can be actually recovered by an inexact Uzawa
method [11]. The Augmented Lagrangian function of the problem (1) is denoted by L(x  z) =
2(cid:107)Ax − b − z/ρ(cid:107)2. In each iteration k  the inexact Uzawa method ﬁrst minimizes a local
cT x + ρ
second-order approximation of the quadratic term in L(x  zk) with respect to primal variables x 
speciﬁcally 

1
2

(cid:107)x − xk(cid:107)D 

cT x + (cid:104)ρAT (Axk − b − zk/ρ)  x − xk(cid:105) +

xk+1 = arg min
xi∈Vi

(2)
then update the dual variables by zk+1 = zk − ρ(Axk+1 − b). Let the proximity parameter ρ = λ/q
and matrix D equal to the diagonal matrix diag{. . .   1/q(cid:107)Ai(cid:107)2  . . .}  then we can recover the above
algorithm by the ﬁrst-order optimality condition of (2). This equivalence allows us to illustrate the
main reason for the slow and ﬂuctuating “tail convergence” comes from the inefﬁciency of such a
local approximation of the Augmented Lagrangian function when the iterates approach the optimal
set.
One straightforward idea to resolve this issue is to minimize the Augmented Lagrangian function
exactly instead of its local approximation  which leads to the classic ALM. There exists a line of
works focusing on analyzing the convergence of applying ALM to LP [9  12  13]. This method will
produce a sequence of constrained quadratic programs (QP) that are difﬁcult to solve. The work [9]
proves that the proximal Coordinate Descent method can solve each QPs at a linear rate even when
matrix A is not full column rank. However  there exists several drawbacks in this approach: (i) the
practical solving time of each subproblem is quite long when A is rank-deﬁcient; (ii) the theoretical
performance and complexity of using recent accelerated techniques in proximal optimization [14]
with the ALM is unknown; (iii) it cannot exploit the speciﬁc structure of matrix A when solving
each constrained QP. Therefore  it motivates us to investigate the new and efﬁcient variable splitting
method for such a problem.

3 New Splitting Method in ADMM

We ﬁrst separate the equality and inequality constraints of the above LP (1) by adding another group
of variables y ∈ Rn.

cT x
min
s.t. Ax = b  x = y 
yi ≥ 0  i ∈ [nb].

The dual of problem (3) takes the following form.

min bT zx
s.t. −AT zx − zy = c 

zy i ≤ 0  i ∈ [nb]  zy i = 0  i ∈ [n]\[nb].

(3)

(4)

Let zx  zy be the Lagrange multipliers for constraints Ax = b  x = y respectively. Deﬁne the
indicator function g(y) of the non-negative cone: g(y) = 0 if yi ≥ 0 ∀i ∈ [nb]; otherwise
g(y) = +∞. Then the augmented Lagrangian function of the primal problem (3) is deﬁned as

L(x  y  z) = cT x + g(y) + zT (A1x + A2y − b) +

(cid:107)A1x + A2y − b(cid:107)2 

where z = [zx; zy]. The matrix A1  A2 and vector b are denoted by

(5)

(6)

(cid:20)A

(cid:21)

I

(cid:21)

(cid:20) 0−I

A1 =

  A2 =

  and b =

.

ρ
2

(cid:21)
(cid:20)b

0

In each iteration k  the standard ADMM go through following three steps:

1. Primal update: xk+1 = arg min
x∈Rn
2. Primal update: yk+1 = arg min
y∈Rn

L(x  yk  zk).

L(xk+1  y  zk).

3

Algorithm 1 Alternating Direction Method of Multiplier with Inexact Subproblem Solver

Initialize z0 ∈ Rm+n  choose parameter ρ > 0.
repeat

1. Primal update: ﬁnd xk+1 such that Fk(xk+1) − minx∈Rn Fk(x) ≤ k.
2. Primal update: for each i  let yk+1
3. Dual update: zk+1

i + zk
x + ρ(Axk+1 − b)  zk+1
until (cid:107)Axk+1 − b(cid:107)∞ ≤  and (cid:107)xk+1 − yk+1(cid:107)∞ ≤ 

Vi
y = zk

y + ρ(xk+1 − yk+1).

x = zk

.

i =(cid:2)xk+1

y i/ρ(cid:3)

3. Dual update: zk+1 = zk + ρ(A1xk+1 + A2yk+1 − b).

The ﬁrst step is an unconstrained quadratic program  which can be simpliﬁed as

xk+1 = arg min

x

Fk(x) (cid:44) cT x + (zk)T A1x +

(cid:107)A1x + A2yk − b(cid:107)2.

ρ
2

The gradient of the function Fk(x) can be expressed as
∇Fk(x) = ρ(AT A + I)x + AT

1 [zk + ρ(A2yk − b)] + c 

(7)

(8)

and the Hessian of function Fk(x) is

∇2Fk(x) = ρ(AT A + I).

(9)
Further  based on the ﬁrst-order optimality condition  the ﬁrst step is equivalent to solving a linear
system  which requires inverting the Hessian matrix (9). In practice  the complexity is quite high to
be exactly solved unless the Hessian exhibits some speciﬁc structures. Thus  we relax the ﬁrst step
into the inexact minimization: ﬁnd xk+1 such that
Fk(xk+1) − min
x∈Rn

Fk(x) ≤ k 

(10)

where k is the given accuracy. Transforming the indicator function g(y) back to the constraints  the
second step can be separated into n one−dimensional optimization problems: for each i 

yk+1
i = arg min
yi∈Vi

−zk

y iyi +

(yi − xk+1

i

ρ
2

)2 =(cid:2)xk+1

y i/ρ(cid:3)

i + zk

.

Vi

The resultant algorithm is sketched in Algorithm 1. In some applications such as l1-regularized
SVMs and basis pursuit problem  the objective function contains the l1 norm of the variables.
Transforming to the canonical form (1) will introduce additional n variables and 2n constraints. One
important feature in our method is that we can split the objective function by adding variable y. The
corresponding subproblems are similar with Algorithm 1 and the only difference is that the second
step will be n one−dimensional shrinkage operations. (Details can be seen in Appendix.)

4 Convergence Analysis of New ADMM

In this section  we prove that the Algorithm 1 converges at a global and linear rate  and provide
a roadmap of the main technical development. We can ﬁrst write the primal problem (3) as the
following standard 2−block form.

min
x y

f (x) + g(y)

s.t. A1x + A2y = b 

(11)

where f (x) = cT x and g(y) is the indicator function as deﬁned before. Most works in the literature
prove that the 2-block ADMM converges globally and linearly via assuming that one of the functions
f and g is strongly convex [15  16  17]. Unfortunately  both the linear function f and the indicator
function g in the LP do not satisfy this property  which poses a signiﬁcant challenge on the current
analytical framework. There exists several recent works trying to address this problem in some sense.
In work [18]  they have demonstrated that when the dual step size ρ is sufﬁciently small (impractical) 
the ADMM converges globally linearly  while no implicit rate is given. The work [13] shows that
the ADMM is locally linearly converged when applying to LP. They utilize a unique combination of
iterates and conduct a spectral analysis. However  they still leave an open question whether ADMM
converges globally and linearly when applying to the LP in the above form.

4

In the sequel  we will answer this question positively and provide an accurate analysis of such a
splitting method. The main technical development is based on a geometric argument: we ﬁrst prove
that the set formed by optimal primal and dual solutions of LP (3) is a (3n + m)−dimensional
polyhedron S∗; then we utilize certain global error bound to simultaneously estimate the distance
from iterates xk+1  yk  zk to S∗. All detailed proofs are given in the Appendix.
Lemma 1. (Convergence of 2-block ADMM [10]) Let pk = zk − ρA2yk  we have

(cid:107)pk+1 − [pk+1]G∗(cid:107)2 ≤ (cid:107)pk − [pk]G∗(cid:107)2 − (cid:107)pk+1 − pk(cid:107)2 

where G∗ (cid:44) {p∗ ∈ Rm+n|T (p∗) = p∗}  and the deﬁnition of operator T is given in (54) in
Appendix. Moreover  if the LP (3) has a pair of optimal primal and dual solution  the iterates xk yk
and zk converges to an optimal solution; Otherwise  at least one of the iterates is unbounded.

(cid:107)pk − [pk]G∗(cid:107) ≤ γ(cid:107)pk+1 − pk(cid:107)  γ > 0.

Lemma 1 is tailored from applying the classic Douglas-Rachford splitting method to the LP. This result
guarantees that the sequence pk produced by ADMM globally converges under a mild assumption.
However  to establish the linear convergence rate  the key lies in estimating the other side inequality 
(12)
Then one can combine these two results together to prove that sequence pk converges globally and
linearly with (cid:107)pk+1− [pk+1]G∗(cid:107)2 ≤ (1− 1/γ2)·(cid:107)pk − [pk]G∗(cid:107)2  which further can be used to show
the R−linear convergence of iterates xk  yk and zk. To estimate the constant γ  we ﬁrst describe the
geometry formed by the optimal primal solutions x∗  y∗ and dual solutions z∗ of the LP (3).
Lemma 2. (Geometry of the optimal solution set of LP) The variables (x∗  y∗) are the optimal
primal solutions and z∗ are optimal dual solutions of LP (3) if and only if (i) Ax∗ = b  x∗ = y∗; (ii)
−AT z∗
In Lemma 2  one interesting element is to utilize the strong duality condition (iv) to eliminate the
complementary slackness in the standard KKT condition. Then  the set of optimal primal and dual
solutions is described only by afﬁne constraints  which further implies that the optimal solution set is
an (m + 3n)−dimensional polyhedron. We use S∗ to denote such a polyhedron.
Lemma 3. (Hoffman bound [19  20]) Consider a polyhedron set S = {x ∈ Rd|Ex = t  Cx ≤ d}.
For any point x ∈ Rd  we have

y i = 0  i ∈ [n]\[nb]; (iv) cT x∗ + bT z∗

y i ≤ 0  i ∈ [nb]; z∗

y = c; (iii) y∗

i ≥ 0  z∗

x − z∗

x = 0.

(cid:107)x − [x]S(cid:107) ≤ θS

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) Ex − t

[Cx − d]+

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)  

(13)

where θS is the Hoffman constant that depends on the structure of polyhedron S.
According to the result in Lemma 2  it seems that we can use the Hoffman bound to estimate the
distance between the current iterates (xk  yk  zk) and the solution set S∗ via the their primal and
dual residual. However  to obtain the form of inequality (12)  we need to bound such a residual in
terms of (cid:107)pk − pk+1(cid:107). Indeed  we have these results.
Lemma 4. (Estimation of residual) The sequence (xk+1  yk  zk) produced by Algorithm 1 satisﬁes



A1xk+1 + A2yk − b = (pk+1 − pk)/ρ 
c + AT
1 zk = AT
cT xk+1 + bT zk
i ≥ 0  zk
yk

1 (pk − pk+1) 
x = (A1xk+1 − zk/ρ)T (pk − pk+1) 

y i = 0  i ∈ [n]\[nb].

y i ≤ 0  i ∈ [nb]; zk

One observation from Lemma 4 is that Algorithm 1 automatically preserves the boundness and the
complementary slackness of both primal and dual iterates. Instead  in the previous algorithm in [8] 
the complementary slackness is not preserved during the iteration. Combining the results in Lemma 2 
Lemma 3 and Lemma 4  we are readily to estimate the constant γ.
Lemma 5. (Estimation of linear rate) The sequence pk = zk − ρA2yk produced by Algorithm 1
satisﬁes (cid:107)pk − [pk]G∗(cid:107) ≤ γ(cid:107)pk+1 − pk(cid:107)  where the rate γ is given by

(14)
Rx = supk (cid:107)xk(cid:107) < +∞  Rz = supk (cid:107)zk(cid:107) < +∞ are the maximum radius of iterates xk and zk.

γ = (1 + ρ)

θS∗ .

ρ

+ Rx(cid:107)A1(cid:107) + (cid:107)AT
1 (cid:107)

(cid:20) Rz + 1

(cid:21)

5

Then we can establish the global and linear convergence of Algorithm 1.
Theorem 1. (Linear convergence of Algorithm 1) Denote zk as the primal iterates produced by
Algorithm 1. To guarantee that there exists an optimal dual solution z∗ such that (cid:107)zk − z∗(cid:107) ≤   it
sufﬁces to run Algorithm 1 for number of iterations K = 2γ2 log(2D0/) with the solving accuracy
k satisfying k ≤ 2/8K 2  where D0 = (cid:107)p0 − [p0]G∗(cid:107).
The proof of Theorem 1 consists of two steps: ﬁrst  we establish the global and linear convergence
rate of Algorithm 1 when k = 0 ∀k (exact subproblem solver); then we relax this condition and
prove that when k is less than a speciﬁed threshold  the algorithm still shares a convergence rate of
the same order. The results of primal iterates xk and yk are similar.

5 Efﬁcient Subproblem Solver

In this section  we will show that  due to our speciﬁc splitting method  each subproblem in line 1 of
Algorithm 1 can be either solved in closed-form expression or efﬁciently solved by the Accelerated
Coordinate Descent Method.

5.1 Well-structured Constraint Matrix

(I + AT A)−1 = I − AT (I + AAT )−1A.

xk+1 = ρ−1(I + AT A)−1dk  with dk = −AT

Let the gradient (8) vanish  then the primal iterates xk+1 can be exactly determined by
1 [zk + ρ(A2yk − b)] − c 

(15)
which requires inverting an n × n positive deﬁnite matrix I + AT A  or equivalently  inverting an
m × m positive deﬁnite matrix I + AAT via the following Sherman–Morrison–Woodbury identity 
(16)
One basic fact is that we only need to invert such a matrix once and then use this cached factorization
in subsequent iterations. Therefore  there are several cases for which the above factorization can
be efﬁciently calculated: (i) Factorization has a closed-form expression. For example  in the LP-
based MAP inference [5]  the matrix I + AT A is block diagonal  and each block has been shown
to possess a closed-form factorization. Another important application is that  in the basis pursuit
problem  the encoding matrices such as DFT (discrete Fourier transform) and DWHT (discrete
Walsh-Hadamard transform) matrices have orthonormal rows and satisfy AAT = I. Based on
(15)  each xk+1 = ρ−1(I − 1
2 AT A)dk and can be calculated in O(n log(n)) time by certain
fast transforms. (ii) Factorization has a low-complexity: the dimension m (or n) is small  i.e. 
m = 104. Such a factorization can be calculated in O(m3) and the complexity of each iteration is
only O(nnz(A) + m2). Detailed applications can be viewed in Appendix.
Remark 1. In the traditional Augmented Lagrangian method  the resultant subproblem is a con-
strained and non-strongly convex QP (Hessian is not invertible)  which does not allow the above
close-form expression. Besides  in the ALCD [9]  the coordinate descent (CD) step only picks one
column in each iteration and cannot exploit the nice structure of matrix A. One idea is to modify the
CD step in [9] to the proximal gradient descent. However  it will greatly increase the computation
time due to the large number of inner gradient descent steps.

5.2 General Constraint Matrix

However  in other applications  the constraint matrix A only exhibits the sparsity  which is difﬁcult
to invert. To resolve this issue  we resort to the current fastest accelerated coordinate descent
method [21]. This method has an order improvement up to O(
n) of iteration complexity compared
with previous accelerated coordinate descent methods [22]. However  the naive evaluation of partial
derivative of function Fk(x) in ACDM takes O(nnz(A)) time; second  the time cost of full vector
operation in each iteration of ACDM is O(n). We will show that these difﬁculties can be tackled by a
carefully designed implementation technique1 and the main procedure is listed in Algorithm 2. Here
the iterates st and matrix M in Algorithm 2 is deﬁned as

√

(cid:20)1 − αv

βu

M =

(cid:21)

αv
1 − βu

(cid:20)αv

(cid:21)

βu

with

=

and si

t =

1This technique is motivated by [22].

(cid:34)(cid:16)

(cid:17)∇iFk(ut)eT

i

(cid:35)

ητ

pi(1+ηρ) + 1−τ

Li

pi(1+ηρ)∇iFk(ut)eT

η

i

 

(17)

(cid:20) τ

(cid:21)

1+ηρ

ηρ

1+ηρ

6

Initialize u0  v0  u0 = Au0  v0 = Av0  matrix M  parameter τ  η  S by (17) and distribution

repeat

Algorithm 2 Efﬁciently Subproblem Solver

p = [. . .  (cid:112)1 + (cid:107)Ai(cid:107)2/S  . . . ] and let dk = AT
(cid:21)

[ut  vt]T = Mt−1 · [u  v]T and [ut  vt]T = Mt−1 · [u  v]T .
Sample i from [n] based on probability distribution p.
∇iFk(ut) = ρ(Ai)T ut + ρut i + dk
i   and calculate si
− M−1
Mt = M · Mt−1. Update
t 
t si
=

(cid:20)uT

(cid:20)uT

1 [zk + ρ(A2yk − b)] + c.
(cid:21)

(cid:20)uT

(cid:20)uT

t by (17).
=

(cid:21)

(cid:21)

vT

vT

vT

vT

until Converge
Output xk+1 = (uT − τ vT )/(1 − τ ).

− M−1
t si

tAT  

where η = 1

τ S2   τ =

√

1+

2

4S2/ρ+1

Lemma 6. (Inner complexity) In each iteration of Algorithm 2  if the current picked coordinate
is i  the update can be ﬁnished in O(nnz(Ai)) time  moreover  to guarantee that Fk(xk+1) −
minx Fk(x) ≤ k with probability 1 − p  it sufﬁces to run Algorithm 2 for number of iterations

  S =(cid:80)n

i=1

(cid:112)(cid:107)Ai(cid:107)2 + 1. See more details in Appendix.
(cid:19)

0 = (cid:107)F k(u0) − min

F k(x)(cid:107).

  Dk

x

(cid:18) Dk

0
kp

(18)

Tk ≥ O(1) · n(cid:88)

i=1

(cid:107)Ai(cid:107) log

The above iteration complexity is obtained by choosing parameter β = 0 in [21] and utilizing the
Theorem 1 in [23] to transform the convergence in expectation to the form of probability.
Theorem 2. (Overall complexity) Denote zk as the dual iterates produced by Algorithm 1. To
guarantee that there exists an optimal solution z∗ such that (cid:107)zk − z∗(cid:107) ≤  with probability 1 − p  it
sufﬁces to run Algorithm 1 for k ≥ 2γ2 log(2D0/) outer iterations and solve each sub-problem (7)
for the number of inner iterations

T ≥ O(1) · n(cid:88)

i=1

(cid:32)

(cid:18) 2D0

(cid:19)(cid:33)



(cid:107)Ai(cid:107) log

3 γ2

0 ) 1
ρ(Dk
 2
3 p 1

3

log

.

(19)

The results for the primal iterates xk and yk are similar. In the existing ADMM [8]  each primal and
dual update only requires O(nnz(A)) time to solve. The complexity of this method is

√
O(amµ2(amRx + dmRz)2(

mn + (cid:107)A(cid:107)F )2nnz(A) log(1/)) 

where am = maxi (cid:107)Ai(cid:107)  dm is the largest number of non-zero elements of each row of matrix A 
and µ is the Hoffman constant depends on the optimal solution set of LP. Based on Theorem 2  an
estimation of the worst-case complexity of Algorithm 1 is

O(amθ2

S∗ (Rx(cid:107)A(cid:107) + Rz)2nnz(A) log2(1/)).

Remark that our method has a weak dependence on the problem dimension compared with the
existing ADMM. Since the Frobenius norm of a matrix satisﬁes (cid:107)A(cid:107)2 ≤ (cid:107)A(cid:107)F   our method is faster
than the one in [8].

6 Numerical Results

In this section  we examine the performance of our algorithm and compare it with the state-of-art
of algorithms developed for solving the LP. The ﬁrst is the existing ADMM in [8]. The second is
the ALCD method in [9]  which is reported to be the current fastest ﬁrst-order LP solver. They have
shown that this algorithm can signiﬁcantly speed up solving several important machine learning
problems compared with the Simplex and IPM. We name our Algorithm 1 as LPADMM. In the
experiments  we require that the accuracy of subproblem solver k = 10−3 and the stopping criteria
is that both primal residual (cid:107)A1xk + A2yk − b(cid:107)∞ and dual residual (cid:107)AT
1 zk + c(cid:107)∞ is less than
10−3. All the LP instances are generated from the basis pursuit  L1 SVM  SICE and NMF problems.
The data source and statistics are included in the supplementary material.

7

Figure 1: The duality gap versus the number of iterations. From left to right ﬁgures are the BP  NMF 
the L1 SVM and and the SICE problem.
Table 1: Timing Results for BP  SICE  NMF and L1 SVM Problem (in sec. long means > 60 hours)

LPADMM

Iterations

Data

bp1
bp2
bp3
arcene
real-sim
sonar
colon
w2a

news20

m

n

nnz(A)

17408
34816
69632
50095
176986
80912
217580
12048256
2785205

16384
32768
65536
30097
135072
68224
161040
12146960
2498375

8421376
33619968
134348800
1151775
7609186
2756832
8439626
167299110
53625267

Time
22
79
217
801
955
258
395
19630
7765

ALCD

Iterations

14534
19036
24760
176060
18262
13789
1288
8492
6174

Time
864
2846
12862
1978
1906
659
455
45388
9173

Time
long
long
long
21329
19697
3828
7423
long
long

ADMM

Iterations

long
long
long

2035415
249363
151972
83680
long
long

3155
4657
6287
15198
4274
5446
216
2525
2205

We ﬁrst compare the convergence rate of different algorithms in solving the above problems. We
use the bp1 for BP problem  data set colon cancer for NMF problem  news20 for L1 SVM problem
and real-sim for SICE problem. We set proximity parameter ρ = 1. We adopt the relative duality
x(cid:107)/(cid:107)cT x∗(cid:107)  where x∗ is obtained
gap as the comparison metric  which is deﬁned as (cid:107)cT xk + bT zk
approximately by running our method with a strict stopping condition. In our simulation  one iteration
represents n coordinate descent steps for ALCD and LPADMM  and one dual updating step for
ADMM. As can be seen in the Fig. 1  our new method exhibits a global and linear convergence rate
and matches our theoretical performance bound. Besides  it converges faster than both the ALCD and
existing ADMM method  especially in solving the BP and NMF problem. The sensitivity analysis of
ρ is listed in Appendix.
We next examine the performance of our algorithm from the perspective of time efﬁciency (both
clocking time and number of iterations). We adopt the dynamic step size rule for ALCD to optimize
its performance. Note that  exchanging the role of the primal and dual problem in (3)  we can obtain
the dual version of both ADMM and ACLD  which can be used to tackle the primal or dual sparse
problem. We run both methods and adopt the minimum time. The stopping criterion requires that the
primal and dual residual and the relative duality gap is less than 10−3. The data set bp1 bp2 bp3 is
used for basis pursuit problem  news20 is used for L1 SVM problem; arcene  real-sim are used for
SICE problem; sonar  colon and w2a are used for NMF problem. Among all experiments  we can
observe that our proposed algorithm requires approximately 10% − 40% iterations and 10% − 85%
time of the ALCD method  and become particularly advantageous for basis pursuit problem (50×
speed up) or ill posed problems such as SICE and NMF problem. In particular  for the basis pursuit
problem  the primal iterates xk is updated by closed-form expression (15)  which can be calculated in
O(n log(n)) time by Fast Walsh–Hadamard transform.

7 Conclusions

In this paper  we proposed a new variable splitting method to solve the linear programming problem.
The theoretical contribution of this work is that we prove that 2−block ADMM converges globally and
linearly when applying to the linear program. The obtained convergence rate has a weak dependence
of the problem dimension and is less than the best known result. Compared with the existing LP
solvers  our algorithms not only provides a ﬂexibility to exploit the speciﬁc structure of constraint
matrix A  but also can be naturally combined with the existing acceleration techniques to signiﬁcantly
speed up solving the large-scale machine learning problems. The future work focuses on generalizing
our theoretical framework and exhibiting the global linear convergence rate when applying ADMM
to solve a convex quadratic program.
Acknowledgments: This work is supported by ONR N00014-17-1-2417  N00014-15-1-2166  NSF
CNS-1719371 and ARO W911NF-1-0277.

8

050001000015000Number of iterations10-310-210-1100101102Duality gapADMMLPADMM ALCD10002000300040005000Number of iterations10-310-210-1100101Duality gapADMM LPADMM ALCD01000200030004000500060007000Number of iterations10-310-210-1100101Duality gapADMM LPADMM ALCD050100150200Number of iterations10-310-210-1100101102Duality gapADMM LPADMM ALCDReferences
[1] Ben Recht  Christopher Re  Joel Tropp  and Victor Bittorf. Factoring nonnegative matrices with linear

programs. In Advances in Neural Information Processing Systems  pages 1214–1222  2012.

[2] Ji Zhu  Saharon Rosset  Trevor Hastie  and Robert Tibshirani. 1-norm support vector machines. In NIPS 

volume 15  pages 49–56  2003.

[3] Ming Yuan. High dimensional inverse covariance matrix estimation via linear programming. Journal of

Machine Learning Research  11(Aug):2261–2286  2010.

[4] Junfeng Yang and Yin Zhang. Alternating direction algorithms for l1-problems in compressive sensing.

SIAM journal on scientiﬁc computing  33(1):250–278  2011.

[5] Ofer Meshi and Amir Globerson. An alternating direction method for dual map lp relaxation. Machine

Learning and Knowledge Discovery in Databases  pages 470–483  2011.

[6] Vânia Lúcia Dos Santos Eleutério. Finding approximate solutions for large scale linear programs. PhD

thesis  ETH Zurich  2009.

[7] Roland Glowinski and A Marroco. Sur l’approximation  par éléments ﬁnis d’ordre un  et la résolution  par
pénalisation-dualité d’une classe de problèmes de dirichlet non linéaires. Revue française d’automatique 
informatique  recherche opérationnelle. Analyse numérique  9(2):41–76  1975.

[8] Jonathan Eckstein  Dimitri P Bertsekas  et al. An alternating direction method for linear programming.

1990.

[9] Ian En-Hsu Yen  Kai Zhong  Cho-Jui Hsieh  Pradeep K Ravikumar  and Inderjit S Dhillon. Sparse linear
programming via primal and dual augmented coordinate descent. In Advances in Neural Information
Processing Systems  pages 2368–2376  2015.

[10] Jonathan Eckstein and Dimitri P Bertsekas. On the douglas-rachford splitting method and the proximal

point algorithm for maximal monotone operators. Mathematical Programming  55(1):293–318  1992.

[11] Wotao Yin. Analysis and generalizations of the linearized bregman method. SIAM Journal on Imaging

Sciences  3(4):856–877  2010.

[12] O Güler. Augmented lagrangian algorithms for linear programming. Journal of optimization theory and

applications  75(3):445–470  1992.

[13] Daniel Boley. Local linear convergence of the alternating direction method of multipliers on quadratic or

linear programs. SIAM Journal on Optimization  23(4):2183–2207  2013.

[14] Qihang Lin  Zhaosong Lu  and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances

in Neural Information Processing Systems  pages 3059–3067  2014.

[15] Robert Nishihara  Laurent Lessard  Benjamin Recht  Andrew Packard  and Michael I Jordan. A general

analysis of the convergence of admm. In ICML  pages 343–352  2015.

[16] Tianyi Lin  Shiqian Ma  and Shuzhong Zhang. On the global linear convergence of the admm with

multiblock variables. SIAM Journal on Optimization  25(3):1478–1497  2015.

[17] Wei Deng and Wotao Yin. On the global and linear convergence of the generalized alternating direction

method of multipliers. Journal of Scientiﬁc Computing  66(3):889–916  2016.

[18] Mingyi Hong and Zhi-Quan Luo. On the linear convergence of the alternating direction method of

multipliers. Mathematical Programming  pages 1–35  2012.

[19] Alan J Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the

National Bureau of Standards  49(4)  1952.

[20] Wu Li. Sharp lipschitz constants for basic optimal solutions and basic feasible solutions of linear programs.

SIAM journal on control and optimization  32(1):140–153  1994.

[21] Zeyuan Allen-Zhu  Zheng Qu  Peter Richtarik  and Yang Yuan. Even faster accelerated coordinate descent
using non-uniform sampling. In Proceedings of The 33rd International Conference on Machine Learning 
pages 1110–1119  2016.

[22] Yin Tat Lee and Aaron Sidford. Efﬁcient accelerated coordinate descent methods and faster algorithms for
solving linear systems. In Foundations of Computer Science (FOCS)  2013 IEEE 54th Annual Symposium
on  pages 147–156. IEEE  2013.

[23] Peter Richtárik and Martin Takáˇc. Iteration complexity of randomized block-coordinate descent methods

for minimizing a composite function. Mathematical Programming  144(1-2):1–38  2014.

9

,Sinong Wang
Ness Shroff