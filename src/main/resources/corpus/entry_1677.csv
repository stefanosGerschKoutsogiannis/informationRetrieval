2019,ETNet: Error Transition Network for Arbitrary Style Transfer,Numerous valuable efforts have been devoted to achieving arbitrary style transfer since the seminal work of Gatys et al. However  existing state-of-the-art approaches often generate insufficiently stylized results under challenging cases. We believe a fundamental reason is that these approaches try to generate the stylized result in a single shot and hence fail to fully satisfy the constraints on semantic structures in the content images and style patterns in the style images. Inspired by the works on error-correction  instead  we propose a self-correcting model to predict what is wrong with the current stylization and refine it accordingly in an iterative manner. For each refinement  we transit the error features across both the spatial and scale domain and invert the processed features into a residual image  with a network we call Error Transition Network (ETNet). The proposed model improves over the state-of-the-art methods with better semantic structures and more adaptive style pattern details. Various qualitative and quantitative experiments show that the key concept of both progressive strategy and error-correction leads to better results. Code and models are available at https://github.com/zhijieW94/ETNet.,ETNet: Error Transition Network for Arbitrary

Style Transfer

Chunjin Song∗
Shenzhen University

songchunjin1990@gmail.com

Zhijie Wu∗

Shenzhen University

wzj.micker@gmail.com

Yang Zhou†

Shenzhen University

zhouyangvcc@gmail.com

Minglun Gong

University of Guelph

minglun@uoguelph.ca

Hui Huang†

Shenzhen University
hhzhiyan@gmail.com

Abstract

Numerous valuable efforts have been devoted to achieving arbitrary style transfer
since the seminal work of Gatys et al. However  existing state-of-the-art approaches
often generate insufﬁciently stylized results under challenging cases. We believe a
fundamental reason is that these approaches try to generate the stylized result in a
single shot and hence fail to fully satisfy the constraints on semantic structures in
the content images and style patterns in the style images. Inspired by the works
on error-correction  instead  we propose a self-correcting model to predict what is
wrong with the current stylization and reﬁne it accordingly in an iterative manner.
For each reﬁnement  we transit the error features across both the spatial and scale
domain and invert the processed features into a residual image  with a network we
call Error Transition Network (ETNet). The proposed model improves over the
state-of-the-art methods with better semantic structures and more adaptive style
pattern details. Various qualitative and quantitative experiments show that the key
concept of both progressive strategy and error-correction leads to better results.
Code and models are available at https://github.com/zhijieW94/ETNet.

1

Introduction

Style transfer is a challenging image manipulation task  whose goal is to apply the extracted style
patterns to content images. Starting from the works on neural network based generative models [17 
10  2  3]  the seminal work of Gatys et al. [9] made the ﬁrst effort to achieve stylization using
neural networks with surprising results. The basic assumption they made is that the feature map
output by a deep encoder represents the content information  while the correlations between the
feature channels of multiple deep layers encode the style info. Following Gatys et al. [9]  subsequent
works [8  16  26  5  31] try to address problems in quality  generalization and efﬁciency through
replacing the time-consuming optimization process with feed-forward neural networks.
In this paper  we focus on how to transfer arbitrary styles with one single model. The existing
methods achieve this goal by simple statistic matching [13  21]  local patch exchange [6] and their
combinations [24  28]. Even with their current success  these methods still suffer from artifacts  such
as the distortions to both semantic structures and detailed style patterns. This is because  when there
is large variation between content and style images  it is difﬁcult to use the network to synthesize the
stylized outputs in a single step; see e.g.  Figure 1(a).

∗Equal contribution  in alphabetic order.
†Corresponding authors.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: State of the art methods are all able to achieve good stylization with a simple target style
(top row in (a)). But for a complex style (bottom row in (a))  both WCT and Avatar-Net distort the
spatial structures and fail to preserve texture consistency  our method  however  still performs well.
Different from the existing methods  our proposed model achieves style transfer by a coarse-to-ﬁne
reﬁnement. One can see that  from left to right in (b)  ﬁner details arise more and more along with
the reﬁnements. See the close-up views in our supplementary material for a better visualization.

To address these challenges  we propose an iterative error-correction mechanism [11  22  4  18]
to break the stylization process into multiple reﬁnements; see examples in Figure 1(b). Given an
insufﬁciently stylized image  we compute what is wrong with the current estimate and transit the
error information to the whole image. The simplicity and motivation lie in the following aspect:
the detected errors evaluate the residuals to the ground truth  which thus can guide the reﬁnement
effectively. Both the stylized images and error features encode information from the same content-
style image pairs. Though they do correlate with each other  we cannot simply add the residuals
to images or to deep features  especially both on content and style features simultaneously. Thus 
a novel error transition module is introduced to predict a residual image that is compatible to the
intermediate stylized image. We ﬁrst compute the correlation between the error features and the deep
features extracted from the stylized image. Then errors are diffused to the whole image according to
their similarities [27  14].
Following [9]  we employ a VGG-based encoder to extract deep features and measure the distance of
current stylization to the ground truth deﬁned by a content-style image pair. Meanwhile  a decoder
is designed for error propagation across both the spatial and scale domain and ﬁnally invert the
processed features back to RGB space. Considering the multi-scale property of style features  our
error propagation is also processed in a coarse-to-ﬁne manner. For the coarsest level  we utilize the
non-local block to capture long-range dependency  thus the error can be transited to the whole image.
Then the coarse error is propagated to ﬁner scale to keep consistency across scales. As a cascaded
reﬁnement framework  our successive error-correction procedures can be naturally embedded into a
Laplacian pyramid [1]  which facilitates both the efﬁciency and effectiveness for the training.
In experiments  our model consistently outperforms the existing state-of-the-art models in qualitative
and quantitative evaluations by a large margin. In summary  our contributions are:

errors in stylization results and correcting them iteratively.

• We ﬁrst introduce the concept of error-correction mechanism to style transfer by evaluating
• By explicitly computing the features for perceptual loss in a feed-forward network  each
• The overall pyramid-based style transfer framework can perform arbitrary style transfer and

reﬁnement is formulated as an error diffusion process.

synthesize highly detailed results with favored styles.

2 Related Work

Under the context of deep learning  many works and valuable efforts [9  16  20  26  29  33  31  32]
have approached the problems of style transfer. In this section  we only focus on the related works on
arbitrary style transfer and refer the readers to [15] for a comprehensive survey.
The seminal work of Gatys et al. [9] ﬁrst proposes to transfer arbitrary styles via a back-propagation
optimization. Our approach shares the same iterative error-correction spirit with theirs. The key

2

Figure 2: Framework of our proposed stylization procedure. We start with a zero vector to represent
the initial stylized image  i.e. ˆI 3
cs = 0. Together with downsampled input content-style image pair (I 3
c
and I 3
h. The sum
of I 3
cs. This process
is repeated across two subsequent levels to yield a ﬁnal stylized image I 1

s )  it is fed into the residual image generator ET N et3 to generate a residual image I 3
h and ˆI 3

cs gives us the updated stylized image I 3

cs  which is then upsampled into ˆI 2

cs with full resolution.

difference is that our method achieves error-correction in a feed-forward manner  which allows us
to explicitly perform joint analysis between errors and the synthesized image. With the computed
(especially the long-range) dependencies guiding the error diffusion  a better residual image can
hence be generated.
Since then  several recent papers have proposed novel models to reduce the computation cost
with a feed-forward network. The methods based on  local patch exchange [6]  global statistic
matching [13  21  19] and their combinations [24  28]  all develop a generative decoder to reconstruct
the stylized images from the latent representation. Meanwhile  Shen et al. [23] employ the meta
networks to improve the ﬂexibility for different styles. However  for these methods  they all stylize
images in one step  which limits their capability in transferring arbitrary styles under challenging
cases. Hence when the input content and style images differ a lot  they are hard to preserve spatial
structures and receive style patterns at the same time  leading to poor stylization results.
Different from the above methods  our approach shares the same spirit with networks coupled
with error correction mechanism [11  22  4  18]. Rather than directly learning a mapping from
input to target in one step  these networks [30] resolve the prediction process into multiple steps
to make the model have an error-correction procedure. The error-correction procedures have many
successful applications  including super-resolution [11]  video structure modeling [22]  human pose
estimation [4] and image segmentation [18] and so on. To our knowledge  we make the ﬁrst efforts to
introduce error-correction mechanism to arbitrary style transfer. Coupled with Laplacian pyramid  the
proposed schema enables the networks to gradually bring the results closer to the desired stylization
and synthesize outputs with better structures and ﬁne-grained details.

3 Developed Framework

We formulate style transfer as a successive reﬁnements based on error-correction procedures. As
shown in Figure 2  the overall framework is implemented using a Laplacian pyramid [1  12  7]  which
has 3 levels. Each reﬁnement is performed by an error transition process with an afﬁnity matrix
between pixels [25  14] followed by an error propagation procedure in a coarse-to-ﬁne manner to
compute a residual image. Each element of the afﬁnity matrix is computed on the similarity between
the error feature of one pixel and the feature of stylized result at another pixel  which can be used to
measure the possibility of the error transition between them. See Section 3.1 for more details.

3.1 Error Transition Networks for Residual Images

We develop Error Transition Network to generate a residual image for each level of a pyramid. As
illustrated in Figure 3  it contains two VGG-based encoders and one decoder. Taken a content-style
image pair (Ic and Is) and an intermediate stylized image ( ˆIcs) as inputs  one encoder (Θin(·)) is used
in( ˆIcs) and the other encoder (Θerr(·)) is for the
to extract deep features {f i
error computation. The superscript i represents the feature extracted at the relu_i_1 layer. The deepest
features of both encoders we used are the output of relu_4_1 layer  which means i ∈ {1  2 ···   L}

in} from ˆIcs  i.e. f i

in = Θi

3

Is3Ic3Ics3Ics3Ics2Ics1Is2Ic2Is1Ic1ETNet3ETNet2ETNet1+++^Ics2^Ics1^Ih3Ih2Ih1Figure 3: Error Transition network (a) and the detailed architecture of error propagation block (b).
For ETNet  the input images include a content-style image pair (Ic  Is) and an intermediate stylization
( ˆIcs). The two encoders extract deep features {f i
s} respectively.
After the fusion of ∆E4
in into a
non-local block to compute a global residual feature ∆D4. Then both ∆D4 and ∆E4 are fed to a
series of error propagation blocks to further receive lower-level information until we get the residual
image Ih. Finally  we add Ih to ˆIcs and output the reﬁned image Ics.

s   we input the fused error feature ∆E4  together with f 4

in}  and error features ∆E4

c and {∆Ei

c and ∆E4

(1)

and L = 4. Then a decoder is used to diffuse errors and invert processed features into the ﬁnal
residual image. Following [9]  the error of the stylized image ˆIcs contains two parts  content error
∆Ei

s  which are deﬁned as

c and style error ∆Ei

∆Ei

c( ˆIcs  Ic) = Θi

err(Ic) − Θi

err( ˆIcs) 

∆Ei

s( ˆIcs  Is) = Gi(Is) − Gi( ˆIcs) 

c   ∆EL

s ) = ∆EL

∆EL = ΨL(∆EL

where Gi(·) represents a Gram matrix for features extracted at layer i in the encoder network.
To leverage the correlation between the error features and the deep features of stylized image so as to
determine a compatible residual feature ∆D  we apply the trick of non-local blocks [27] here. To do
that  we ﬁrst fuse the content and style error features as a full error feature:
c · W · ∆EL
s  

(2)
where Ψ(·) and W indicates the fusion operation and a learnable matrix respectively following
[33  31]. Let the f lat(·) and sof tmax(·) denote the ﬂatten operation along the channel dimension
and a softmax operation respectively. Then the residual feature at L-th scale is determined as:

∆DL = f lat(∆EL ⊗ ψh) · sof tmax(f lat(∆EL ⊗ ψu) · f lat(f L

(3)
where the ⊗ represents one convolution operation and {ψh  ψu  ψg} are implemented as learnable
1 × 1 convolutions [27]. The afﬁnity matrix outputted by the softmax operation determines the error
transition within the whole image. And since long-range dependencies are well captured by the
afﬁnity matrix  we are able to better respect the semantic structures  producing superior style pattern
consistency. Note that similar to the previous work [27]  we only apply the non-local blocks at the top
scale (i.e.  L = 4) to better consider the locality of lower-level features and reduce the computation
burden.
Next ∆EL and ∆DL are fed into cascaded residual propagation blocks to receive lower-level
information step by step. As shown in Figure 3(b)  a residual propagation block at the i-th layer
takes four features as input and has two branches which are used to reﬁne ∆Di and ∆Ei respectively.
Conditioned on ∆Ei and ∆Di of last scale  to make the residual feature ∆Di−1 more compatible to
the stylized image ˆIcs and let the ﬁne scale consistent with coarser scales  we take all of them into
account to compute residual feature. Thus we achieve ∆Ei−1 and ∆Di−1 as:

in ⊗ ψg)T ) 

∆Ei−1 = ∆ ¯Ei−1 ⊗ φu 

∆ ¯Ei−1 = Ψi−1(∆Ei ⊗ φt  ∆Ei−1

s

) 

(4)

∆Di−1 = [∆Di ⊗ φv 

in   ∆ ¯Ei−1] ⊗ φw.
f i−1

(5)
Here [· · ·] denotes a concatenation operation to jointly consider all inputs for simplicity and {φs 
φu  φv  φw} represents learnable convolutions for feature adjustment. We further denote ∆ ˆEi ∈
RN×Ci−1 as the processed feature  ∆ ˆEi = ∆Ei ⊗ φt. N and C i−1 indicate the number of pixels
and the channel number of ∆ ˆEi respectively. The learnable matrix Ψi−1 aims to associate ∆ ˆEi and
s ∈ C i−1 × C i−1 for additional feature fusion. Both ∆Ei−1 and ∆Di−1 will be successively
∆Ei−1
upsampled until i = 1. Finally ∆D1 will be directly outputted as a residual image Ih which will be
added to ˆIcs to compute Ics. Then Ics will be inputted to the ﬁner level of a Laplacian pyramid for
further reﬁnement or outputted as the ﬁnal stylization result.

4

fin2fin3fin4Ec4Es4fini-1NBNBnonlocal blockupsampling operationconcatenationconvolution layerVGGVGGFusionFusionPBPBPB+Es3Es2fin1Es1Esi-1CONVConcatCONVConcatpropagation blockPBCONVDiEiDi-1Ei-1CONVCONVIcs^IcIsIcs(cid:269)(cid:66)(cid:270)(cid:9)(cid:67)(cid:10)D 4E4IhEi-1(cid:100)3.2 Style transfer via a Laplacian Pyramid
Figure 2. illustrates a progressive procedure for a pyramid with K = 3 to stylize a 768 × 768 image.
s ) at k-th level of the pyramid
Similar to [7]  except the ﬁnal level  the input images (I k
are downsampled versions of original full resolution images. And correspondingly ˆI k
cs denotes
the intermediate stylized image. With error transition networks  the stylization is performed in a
coarse-to-ﬁne manner. At the begining  we set the initial stylized image ˆI 3
cs to be an all-zero vector
with the same size as I 3
h as:

s   we compute a residual image I k

c . Setting k = 3  together with I k

c and I k

c and I k

c   I k

cs  I k

cs + I k

h = ˆI k

cs = ˆI k
I k

cs + ET N etk( ˆI k

(6)
where ET N etk denotes an error transition network at k-th level. Then we upsample I k
cs to expand it
. We repeat this process until
to be twice the size and we denote the upsampled version of I k
we go back to the full resolution image. Note that we train each error transition network ET N etk(·)
separately due to the limitation in GPU memory. The independent training of each level also offers
beneﬁts on preventing from overﬁtting [7].
The loss functions are deﬁned based on the image structures present in each level of a pyramid as
well. Thus the content and style loss function for I k

cs are respectively deﬁned as:

cs as ˆI k−1

s ) 

cs

K(cid:88)
K(cid:88)

j=k+1

(cid:107)Φvgg(I j

c ) − Φvgg( ˜I j

cs)(cid:107)2 

(cid:107)GL(I j

s ) − GL( ˜I j

cs)(cid:107)2 

(7)

(8)

pc = (cid:107)Φvgg(I k
Lk
L(cid:88)

Lk

ps =

c ) − Φvgg(I k

cs)(cid:107)2 +

(cid:107)Gi(I k

s ) − Gi(I k

cs)(cid:107)2 +

i=1

j=k+1

where the Φvgg denotes a VGG-based encoder and as mentioned  we set L = 4. And ˜I j
cs is computed
as the (j − k) repeated applications of downsampling operation d(·) on I k
cs =
cs)) to capture large patterns that can not be evaluated with Φvgg directly.
d(d(I 1
Moreover  total variation loss LT V (·) is also added to achieve the piece-wise smoothness. Thus the
total loss at k-th level of a pyramid is computed as:

cs  e.g.  k = 1  ˜I 3

Lk

total = λk

pcLk

pc + λk

psLk

ps + λk

tvLT V  

(9)

where the λk
ps is assigned to 1  5  8
respectively to preserve semantic structures and gradually add style details to outputs. We tried to use
4 or more levels in the pyramid  but found only subtle improvement achieved on the visual quality.

tv are always set to 1 and 10−6 while for k = 1  2  3  λk

pc and λk

4 Experimental Results

We ﬁrst evaluate the key ingredients of our method. Then qualitative and quantitative comparisons to
several state of the art methods are presented. Finally  we show some applications using our approach 
demonstrating the ﬂexibility of our framework. Implementation details and more results can be found
in our supplementary document  and code will be made publicly available online.

Ablation Study Our method has three key ingredients: iterative reﬁnements  error measurement
and the joint analysis between the error features and features of the intermediate stylization. Table 1
lists the quantitative metrics of the ablation study on above ingredients. For our full model  we can
see that though content loss increases a little bit  the style loss shrink signiﬁcantly by using more
reﬁnements  proving the efﬁcacy of our iterative strategy on stylization.
For evaluating the role of error computation  we replace the error features with the plain deep features
from the content and style images  i.e. ∆Ei
s = Gi(Is)  before the fusion
and information propagation. From the perceptual loss shown in Table 1  the model without error
information can still somehow improve the stylization a little bit  since the plain deep features also
contain error features  but comparing to our full model of feeding error explicitly  it brings more
difﬁculty for the network to exact the correct residual info. Figure 4 shows a visual comparison. We
can see that without error features  noise and artifacts are introduced  like the unseen stripes in the

err(Ic) and ∆Ei

c = Θi

5

Figure 4: Ablation study on explicit error computation. Our full model is successful in reducing
artifacts and synthesize texture patterns more faithful to the style image. Better zoom for more details.

Figure 5: Ablation study on the effect of current stylized results in computing residual images.

Table 1: Ablation study on multiple reﬁnements  the effect of error computation and the joint analysis
of intermediate stylized results in computing the residual images. All the results are averaged over
100 synthesized images with perceptual metrics. Note that both K and K
represent the number of
reﬁnements  where K denotes a full model and K
represents a simple model that removes the upper
encoder  which means it does not consider the intermediate stylization in computing residual images.

(cid:48)

(cid:48)

Loss K = 1
10.9117
28.3117

Lc
Ls

K = 2
16.8395
12.1404

K = 3
18.3771
7.1984

K = 1
w/o err
10.9332
28.3513

K = 2
w/o err
19.6845
18.3374

K = 3
w/o err K
19.4235
15.2981

10.9424
28.3283

(cid:48)

(cid:48)

= 1 K

= 2 K

(cid:48)

= 3
22.1014
21.3698

19.5554
23.6618

mountain (2nd row  please zoom in to see more details). Our full model yields a much more favorable
result. The synthesized patterns faithfully mimic the provided style image.
Then we evaluate the importance of features from the intermediate stylized result in computing
the residual image Ih. We train a model that removes the upper encoder and directly employ the
computed error features to create Ih. Speciﬁcally  we disable the non-local blocks at the bottleneck
layer of our model and for the error propagation block at i-th scale  it only takes ∆Di and ∆Ei−1
as inputs. From Table 1  we can see that when disabling the fusion with the features from the
intermediate stylization  the ﬁnal stylizations after iterative reﬁnements are worse than our full model
by a large margin. Figure 5 shows an example  where the incomplete model introduces unseen white
bumps and blurs the contents more comparing to our full model  demonstrating the effect of the
fusion with the intermediate stylization in the error transition.

s

Qualitative Comparison Figure 6 presents the qualitative comparison results to state of the art
methods. For baseline methods  codes released by the authors are used with default conﬁgurations for
a fair comparison. Gatys et a. [9] achieves arbitrary style transfer via time-consuming optimization
process but often gets stuck into local minimum with distorted images and fails to capture the
salient style patterns. AdaIN [13] frequently produces instylized issues due to its limitation in style

6

Figure 6: Comparison with results from different methods.

Figure 7: Detail cut-outs. The top row shows close-ups for highlighted areas for a better visualization.
Only our result successfully captures the paint brush patterns in the style image.

representation while WCT [21] improves the generalization ability to unseen styles  but introduces
unappealing distorted patterns and warps content structures. Avatar-Net [24] addresses the style
distortion issue by introducing a feature decorator module. But it blurs the spatial structures and
fails to capture the style patterns with long-range dependency. Meanwhile  AAMS [28] generates
results with worse texture consistency and introduces unseen repetitive style patterns. In contrast 
better transfer results are achieved with our approach. The iterative reﬁnement coupled with error
transition shows a rather stable performance in transferring arbitrary styles. Moreover  the leverage of
Laplacian pyramid further helps the preservation of stylization consistency across scales. The output
style patterns are more faithful to the target style image  without distortion and exhibiting superior
visual detail. Meanwhile  our model better respects the semantic structures in the content images 
making the style pattern be adapted to these structures.
In Figure 7  we show close-up views of transferred results to indicate the superiority in generating
style details. For the compared methods  they either fail to stylize local regions or capture the salient
style patterns. As expected  our approach performs a better style transfer with clearer structures and
good-quality details. Please see the brush strokes and the color distribution present in results.

7

Figure 8: At deployment stage  we can adjust the degree of stylization with paramter α.

Figure 9: A reﬁnement for the outputs of AdaIN and WCT.

Table 2: Quantitative comparison over different models on perceptual (content & style) loss  prefer-
ence score of user study and stylization speed. Note that all the results are averaged over 100 test
images except the preference score.

AdaIN [13] WCT [21] Avatar-Net [24] AAMS [28]

14.2689
40.2651

11.2
1.3102

Ours

18.3771
7.1984
33.3
0.5680

Loss

Content(Lc)
Style(Ls)

Preference/%

Time/sec

11.4325
71.5744

16.1
0.1484

19.6778
26.1967

26.4
0.7590

15.5150
42.8833

13.0
1.1422

Quantitative Comparison To quantitatively evaluate each method  we conduct a comparison
regarding perceptual loss and report the results in the ﬁrst two rows of Table 2. It shows that  the
proposed method achieves a signiﬁcant lower style loss than the baseline models  whereas the content
loss is also lower than WCT but higher than the other three approaches. This indicates that our model
is better at capturing the style patterns presented in style images with good-quality content structures.
It is highly subjective to assess stylization results. Hence  a user study comparison is further designed
for the ﬁve approaches. We randomly pick 30 content images and 30 style images from the test set
and generate 900 stylization results for each method. We randomly select 20 stylized images for each
participant who is asked to vote for the method achieving best results. For each round of evaluation 
the generated images are displayed in a random order. Finally we collect 600 votes from 30 subjects
and detail the preference percentage of each method in the 3rd row of Table 2.
In the 4th row of Table 2  we also compare with the same set of baseline methods [13  21  24] in terms
of running time. AdaIN [13] is the most efﬁcient model as it uses a simple feed-forward scheme. Due
to the requirements for SVD decomposition and patch swapping operations  WCT and Avatar-Net
are much slower. Even though the feedback mechanism makes our method slower than the fastest
AdaIN algorithm  it is noticeably faster than the other three approaches.

Runtime applications Two applications are employed to reveal the ﬂexibility of designed model
at run-time. The same trained model is used for all these tasks without any modiﬁcation.
Our model is able to control the degree of stylization in running time. For each level k in a
pyramid  this can be realized by the interpolation between two kinds of style error features: one
is computed between the intermediate output ˆI k
s denoting as ∆Ec→s  the
other is attained for ˆI k
c as ∆Ec→c. Thus the trade-off can be achieved as
∆Emix = α∆Ec→s + (1 − α)∆Ec→c  which will be fed into the decoder for mixed effect by fusion.
Figure 8 shows a smooth transition when α is changed from 0.25 to 1.
With error-correction mechanism  the proposed model is enabled to further reﬁne the stylized results
from other existing methods. It can be seen in Figure 9 that  both AdaIN and WCT fail to preserve
the global color distribution and introduce unseen patterns. Feeding the output result of these two

cs and content image I k

cs and style image I k

8

ContentAdaINStyleAdaIN_RefinedWCTWCT_Refinedmethods into our model  ETNet is successful in improving the stylization level by making the color
distribution more adaptive to style image and generating more noticeable brushstroke patterns.

5 Conclusions

We present ETNet for arbitrary style transfer by introducing the concept of error-correction mecha-
nism. Our model decomposes the stylization task into a sequence of reﬁnement operations. During
each reﬁnement  error features are computed and then transitted across both the spatial and scale
domain to compute a residual image. Meanwhile long-range dependencies are captured to better
resepect the semantic relationships and facilitate the texture consistency. Experiments show that our
method signiﬁcantly improves the stylization performance over existing methods.

Acknowledgement

We thank the anonymous reviewers for their constructive comments. This work was supported in
parts by NSFC (61861130365  61761146002  61602461)  GD Higher Education Innovation Key
Program (2018KZDXM058)  GD Science and Technology Program (2015A030312015)  Shenzhen
Innovation Program (KQJSCX20170727101233642)  LHTD (20170003)  and Guangdong Laboratory
of Artiﬁcial Intelligence and Digital Economy (SZ).

References
[1] P. Burt and E. Adelson. The laplacian pyramid as a compact image code. IEEE Transactions on

communications  31(4):532–540  1983.

[2] J. Cao  Y. Guo  Q. Wu  C. Shen  J. Huang  and M. Tan. Adversarial learning with local
coordinate coding. In J. Dy and A. Krause  editors  Proceedings of the 35th International
Conference on Machine Learning  volume 80 of Proceedings of Machine Learning Research 
pages 707–715  Stockholmsmässan  Stockholm Sweden  10–15 Jul 2018. PMLR.

[3] J. Cao  L. Mo  Y. Zhang  K. Jia  C. Shen  and M. Tan. Multi-marginal wasserstein gan. In

Advances in Neural Information Processing Systems  pages 1774–1784  2019.

[4] J. Carreira  P. Agrawal  K. Fragkiadaki  and J. Malik. Human pose estimation with iterative error
feedback. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 4733–4742  2016.

[5] D. Chen  L. Yuan  J. Liao  N. Yu  and G. Hua. Stylebank: An explicit representation for neural
image style transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 1897–1906  2017.

[6] T. Q. Chen and M. Schmidt. Fast patch-based style transfer of arbitrary style. CoRR 

[7] E. L. Denton  S. Chintala  A. Szlam  and R. Fergus. Deep generative image models using a

laplacian pyramid of adversarial networks. 2015.

[8] V. Dumoulin  J. Shlens  and M. Kudlur. A learned representation for artistic style. CoRR 

abs/1612.04337  2016.

abs/1610.07629  2016.

[9] L. A. Gatys  A. S. Ecker  and M. Bethge. Image style transfer using convolutional neural
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 2414–2423  2016.

[10] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and
Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems 
pages 2672–2680  2014.

[11] M. Haris  G. Shakhnarovich  and N. Ukita. Deep back-projection networks for super-resolution.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages
1664–1673  2018.

[12] D. J. Heeger and J. R. Bergen. Pyramid-based texture analysis/synthesis. In SIGGRAPH  1995.
[13] X. Huang and S. J. Belongie. Arbitrary style transfer in real-time with adaptive instance
normalization. 2017 IEEE International Conference on Computer Vision (ICCV)  pages 1510–
1519  2017.

[14] P. Jiang  F. Gu  Y. Wang  C. Tu  and B. Chen. Difnet: Semantic segmentation by diffusion

networks. In Advances in Neural Information Processing Systems  pages 1630–1639  2018.

[15] Y. Jing  Y. Yang  Z. Feng  J. Ye  Y. Yu  and M. Song. Neural style transfer: A review. arXiv

preprint arXiv:1705.04058  2017.

9

[16] J. Johnson  A. Alahi  and L. Fei-Fei. Perceptual losses for real-time style transfer and super-

resolution. In European Conference on Computer Vision  pages 694–711. Springer  2016.

[17] D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

2017.

[18] K. Li  B. Hariharan  and J. Malik. Iterative instance segmentation. In Proceedings of the IEEE

conference on computer vision and pattern recognition  pages 3659–3667  2016.

[19] X. Li  S. Liu  J. Kautz  and M.-H. Yang. Learning linear transformations for fast arbitrary style

transfer. In IEEE Conference on Computer Vision and Pattern Recognition  2019.

[20] Y. Li  C. Fang  J. Yang  Z. Wang  X. Lu  and M.-H. Yang. Diversiﬁed texture synthesis with
feed-forward networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition 
pages 266–274  2017.

[21] Y. Li  C. Fang  J. Yang  Z. Wang  X. Lu  and M.-H. Yang. Universal style transfer via feature

transforms. In NIPS  2017.

[22] W. Lotter  G. Kreiman  and D. Cox. Deep predictive coding networks for video prediction and

unsupervised learning. arXiv preprint arXiv:1605.08104  2016.

[23] F. Shen  S. Yan  and G. Zeng. Meta networks for neural style transfer. CoRR  abs/1709.04111 

[24] L. Sheng  Z. Lin  J. Shao  and X. Wang. Avatar-net: Multi-scale zero-shot style transfer by

feature decoration. pages 8242–8250  2018.

[25] Y. Tao  Q. Sun  Q. Du  and W. Liu. Nonlocal neural networks  nonlocal diffusion and nonlocal

modeling. In Advances in Neural Information Processing Systems  pages 496–506  2018.

[26] D. Ulyanov  V. Lebedev  A. Vedaldi  and V. S. Lempitsky. Texture networks: Feed-forward

synthesis of textures and stylized images. In ICML  2016.

[27] X. Wang  R. Girshick  A. Gupta  and K. He. Non-local neural networks. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition  pages 7794–7803  2018.

[28] Y. Yao  J. Ren  X. Xie  W. Liu  Y.-J. Liu  and J. Wang. Attention-aware multi-stroke style

transfer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2019.

[29] Z. Yi  H. Zhang  P. Tan  and M. Gong. Dualgan: Unsupervised dual learning for image-to-image

translation. In IEEE International Conference on Computer Vision  2017.

[30] A. R. Zamir  T.-L. Wu  L. Sun  W. B. Shen  B. E. Shi  J. Malik  and S. Savarese. Feedback
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 
pages 1308–1317  2017.

[31] H. Zhang and K. J. Dana. Multi-style generative network for real-time transfer. CoRR 

abs/1703.06953  2017.

[32] R. Zhang  S. Tang  Y. Li  J. Guo  Y. Zhang  J. Li  and S. Yan. Style separation and synthesis
via generative adversarial networks. In 2018 ACM Multimedia Conference on Multimedia
Conference  pages 183–191. ACM  2018.

[33] Y. Zhang  Y. Zhang  and W. Cai. Separating style and content for generalized style transfer.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages
8447–8455  2018.

10

,Carlos Becker
Christos Christoudias
Pascal Fua
Qibin Hou
PengTao Jiang
Yunchao Wei
Ming-Ming Cheng
Chunjin Song
Zhijie Wu
Yang Zhou
Minglun Gong
Hui Huang