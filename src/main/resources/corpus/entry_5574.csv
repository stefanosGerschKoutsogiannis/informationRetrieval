2014,Tight convex relaxations for sparse matrix factorization,Based on a new atomic norm  we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors  subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension of the suggested norm for rank 1 matrices  showing that its statistical dimension is an order of magnitude smaller than the usual l_1-norm  trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes  we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.,Tight convex relaxations for sparse matrix

factorization

Emile Richard

Electrical Engineering
Stanford University

Guillaume Obozinski
Universit´e Paris-Est

Ecole des Ponts - ParisTech

Jean-Philippe Vert
MINES ParisTech

Institut Curie

Abstract

Based on a new atomic norm  we propose a new convex formulation for sparse
matrix factorization problems in which the number of non-zero elements of the
factors is assumed ﬁxed and known. The formulation counts sparse PCA with
multiple factors  subspace clustering and low-rank sparse bilinear regression as
potential applications. We compute slow rates and an upper bound on the sta-
tistical dimension [1] of the suggested norm for rank 1 matrices  showing that
its statistical dimension is an order of magnitude smaller than the usual (cid:96)1-norm 
trace norm and their combinations. Even though our convex formulation is in the-
ory hard and does not lead to provably polynomial time algorithmic schemes  we
propose an active set algorithm leveraging the structure of the convex problem to
solve it and show promising numerical results.

1

Introduction

A range of machine learning problems such as link prediction in graphs containing community struc-
ture [16]  phase retrieval [5]  subspace clustering [18] or dictionary learning [12] amount to solve
sparse matrix factorization problems  i.e.  to infer a low-rank matrix that can be factorized as the
product of two sparse matrices with few columns (left factor) and few rows (right factor). Such
a factorization allows more efﬁcient storage  faster computation  more interpretable solutions and
especially leads to more accurate estimates in many situations. In the case of interaction networks 
for example  this is related to the assumption that the network is organized as a collection of highly
connected communities which can overlap. More generally  considering sparse low-rank matrices
combines two natural forms of sparsity  in the spectrum and in the support  which can be motivated
by the need to explain systems behaviors by a superposition of latent processes which only involve
a few parameters. Landmark applications of sparse matrix factorization are sparse principal com-
ponents analysis (SPCA) [8  21] or sparse canonical correlation analysis (SCCA)[19]  which are
widely used to analyze high-dimensional data such as genomic data.
In this paper  we propose new convex formulations for the estimation of sparse low-rank matrices. In
particular  we assume that the matrix of interest should be factorized as the sum of rank one factors
that are the product of column and row vectors with respectively k and q non zero-entries  where k
and q are known. We ﬁrst introduce below the (k  q)-rank of a matrix as the minimum number of
left and right factors  having respectively k and q non-zeros  required to reconstruct a matrix. This
index is a more involved complexity measure for matrices than the rank in that it conditions on the
number of non-zero elements of the left and right factors of the matrix. Based on this index  we
propose a new atomic norm for matrices [7] by considering its convex hull restricted to the unit ball
of the operator norm  resulting in convex surrogates to low (k  q)-rank matrix estimation problem.
We analyze the statistical dimension of the new norm and compare it to that of linear combinations
of the (cid:96)1 and trace norms. In the vector case  our atomic norm actually reduces to k-support norm
introduced by [2] and our analysis shows that its statistical power is not better than that of the (cid:96)1-

1

norm. By contrast  in the matrix case  the statistical dimension of our norm is at least one order of
magnitude better than combinations of the (cid:96)1-norm and the trace norm.
However  while in the vector case the computation remains feasible in polynomial time  the norm we
introduce for matrices can not be evaluated in polynomial time. We propose algorithmic schemes
to approximately learn with the new norm. The same norm and meta-algorithms can be used as
a regularizer in supervised problems such as multitask learning or quadratic regression and phase
retrieval  highlighting the fact that our algorithmic contribution does not consist in providing more
efﬁcient solutions to the rank-1 SPCA problem  but to combine atoms found by the rank-1 solvers
in a principled way.

2 Tight convex relaxations of sparse factorization constraints

In this section we propose a new matrix norm allowing to formulate various sparse matrix factor-
ization problems as convex optimization problems. We start by deﬁning the (k  q)-rank of a matrix
in section 2.1  a useful generalization of the rank which also quantiﬁes the sparseness of a matrix
factorization. We then introduce in section 2.2 the (k  q)-trace norm  an atomic norm deﬁned as the
convex relaxations of the (k  q)-rank over the operator norm ball. We discuss further properties and
potential applications of this norm used as a regularizer in section 2.3.

2.1 The (k  q)-rank of a matrix
The rank of a matrix Z ∈ Rm1×m2 is the minimum number of rank-1 matrices needed to express Z
i . The following deﬁnition generalizes this rank

as a linear combination of the form Z =(cid:80)r
(k  q)-sparse decomposition of Z any decomposition of the form Z =(cid:80)r

to incorporate conditions on the sparseness of the rank-1 elements:
Deﬁnition 1 ((k  q)-sparse decomposition and (k  q)-rank) For a matrix Z ∈ Rm1×m2  we call
i where ai (resp.
bi) are unit vectors with at most k (resp. q) non-zero elements  and with minimal r  which we call
the (k  q)-rank of Z.

i=1 ciaib(cid:62)

i=1 aib(cid:62)

∞(cid:88)

i=1

The (k  q)-rank and (k  q)-sparse decomposition of Z can equivalently be deﬁned as the optimal
value and a solution of the optimization problem:

s.t. Z =

ciaib(cid:62)
i  

min(cid:107)c(cid:107)0

(ai  bi  ci) ∈ Am1

k × Am2
j = {a ∈ Rn | (cid:107)a(cid:107)0 ≤ j (cid:107)a(cid:107)2 = 1}. Since An

(1)
where for any 1 ≤ j ≤ n  An
j when i ≤ j  we
have for any k and q rank(Z) ≤ (k  q)-rank(Z) ≤ (cid:107)Z(cid:107)0. The (k  q)-rank is useful to formalize
problems such as sparse matrix factorization  which can be deﬁned as approximating the solution of
a matrix valued problem by a matrix having low (k  q)-rank. For instance the standard rank-1 SPCA
problem consists in ﬁnding the symmetric matrix with (k  k)-rank equal to 1 and providing the best
approximation of the sample covariance matrix [21].

q × R+  
i ⊂ An

2.2 A convex relaxation for the (k  q)-rank

The (k  q)-rank is a discrete  nonconvex index  like the rank or the cardinality  leading to com-
putational difﬁculties if one wants to learn matrices with small (k  q)-rank. We propose a convex
relaxation of the (k  q)-rank aimed at mitigating these difﬁculties. For that purpose  we consider an
atomic norm [7] that provides a convex relaxation of the (k  q)-trace norm  just like the (cid:96)1 norm and
the trace norm are convex relaxations of the (cid:96)0 semi-norm and the rank  respectively. An atomic
norm is a convex function deﬁned based on a small set of elements called atoms which constitute
a basis on which an object of interest can be sparsely decomposed. The function (a norm if the set
is centrally symmetric) is deﬁned as the gauge of the convex hull of atoms. In other terms  its unit
ball or level-set of value 1 is formed by the convex envelope of atoms. In case of atoms of interest 
namely rank-1 factors of given sparsities k and q  we deﬁne

Deﬁnition 2 ((k  q)-trace norm) Let Ak q be a set of atoms Ak q =(cid:8)ab(cid:62) : a ∈ Am1

For a matrix Z ∈ Rm1×m2  the (k  q)-trace norm Ωk q(Z) is the atomic norm induced by Ak q  i.e. 

k   b ∈ Am2

(cid:9) .

q

2

(cid:110) (cid:88)

A∈Ak q

(cid:88)

A∈Ak q

cA : Z =

Ωk q(Z) = inf

cAA  cA ≥ 0  ∀A ∈ Ak q

(cid:111)

.

(2)

In words  Ak q is the set of matrices A ∈ Rm1×m2 such that (k  q)-rank(A) = 1 and (cid:107)A(cid:107)op = 1.
The next lemma provides an explicit formulation for the (k  q)-trace norm and its dual:
Lemma 1 For any Z  K ∈ Rm1×m2  and denoting Gm
(cid:107)Z (I J)(cid:107)∗ : Z =

(cid:111)
(cid:88)
k = {I ⊂ [[1  m]] : |I| = k}  we have
Z (I J)   supp(Z (I J)) ⊂ I × J

(cid:110) (cid:88)

Ωk q(Z) = inf

(3)

 

k q(K) = max(cid:8)(cid:107)KI J(cid:107)op : I ∈ Gm1

(I J)∈Gm1

k ×Gm2

q

k

(cid:9) .

(I J)

  J ∈ Gm2

q

and Ω∗

2.3 Learning matrices with sparse factors

In this section  we brieﬂy discuss how the (k  q)-trace norm norm can be used to formulate various
problems involving the estimation of sparse low-rank matrices. A way to learn a matrix Z with
low empirical risk L(Z) and with low (k  q)-rank is to use Ωk q as a regularizer and minimize an
objective of the form

min

Z∈Rm1×m2

L(Z) + λΩk q(Z).

(4)

(5)

A number of problems can be formulated as variants of (4).
Bilinear regression. In bilinear regression  given two inputs x ∈ Rm1 and x(cid:48) ∈ Rm2 one ob-
serves as output a noisy version of y = x(cid:62)Zx(cid:48). Assuming that Z has low (k  q)-rank means that
the noiseless response is a sum of a small number of terms  each involving only a small number
of features from either of the input vectors. To estimate within such a model from observations
(xi  x(cid:48)

i  yi)i=1 ... n one can consider the following formulation  in which (cid:96) is a convex loss :

(cid:88)

(cid:96)(cid:0)x(cid:62)

i Zx(cid:48)

i  yi

(cid:1) + λΩk q(Z) .

min

Z∈Rm1×m2

i

Subspace clustering. In subspace clustering  one assumes that the data can be clustered in such a
way that the points in each cluster belong to a low dimensional space. If we have a design matrix
X ∈ Rn×p with each row corresponding to an observation  then the previous assumption means
that if X (j) ∈ Rnj×p is a matrix formed by the rows of cluster j  there exist a low rank matrix
Z (j) ∈ Rnj×nj such that Z (j)X (j) = X (j). This means that there exists a block-diagonal matrix Z
such that ZX = X and with low-rank diagonal blocks. This idea  exploited recently by [18] implies
that Z is a sum of low rank sparse matrices; and this property still holds if the clustering is unknown.
We therefore suggest that if all subspaces are of dimension k  Z may be estimated via

min

Z∈Rn×n

Ωk k(Z)

s.t. ZX = X .

Sparse PCA. One possible formulation of sparse PCA with multiple factors is the problem of ap-
proximation of an empirical covariance matrix ˆΣn by a low-rank matrix with sparse factors. This
suggests to formulate sparse PCA as follows:

(cid:8)(cid:107) ˆΣn − Z(cid:107)F : (k  k)-rank(Z) ≤ r and Z (cid:23) 0(cid:9)  

(6)

min

Z

where q is the maximum number of non-zero coefﬁcients allowed in each principal direction. By
contrast to sequential approaches that estimate the principal components one-by-one [11]  this for-
mulation requires to ﬁnd simultaneously a set of complementary factors. If we require the decompo-
sition of Z to be a sum of positive semi-deﬁnite (k  k)-sparse rank one factors (which is a stronger
assumption than assuming that Z is p.s.d.)  the positivity constraint on Z is no longer necessary and
a natural convex relaxation for (6) using another atomic norm (in fact only a gauge here) is

F + λΩk (cid:23)(Z)  
where Ωk (cid:23) is the gauge of the set of atoms Ak (cid:23) := {aa(cid:62)  a ∈ Am
k }.

Z∈Rm×m

min

(cid:107) ˆΣn − Z(cid:107)2

(7)

3

3 Performance of the (k  q)-trace norm for denoising
In this section  we consider the problem of denoising a low-rank matrix Z (cid:63) ∈ Rm1×m2 with sparse
factors corrupted by additive Gaussian noise  that is noisy observations Y ∈ Rm1×m2 of the form
Y = Z (cid:63) + σG   where σ > 0 and G is a random matrix with i.i.d. N (0  1) entries. For a convex
penalty Ω : Rm1×m2 → R  we consider  for any λ > 0  the estimator

ˆZ λ

Ω = arg min
Z

1
2

(cid:107)Z − Y (cid:107)2

F + λΩ(Z) .

(8)

The following result is a straightforward generalization to any norm Ω of the so-called slow rates
that are well know for the (cid:96)1 norms and other norms such as the trace-norm (see e.g. [10]).
Lemma 2 If λ ≥ σΩ∗(G)

then

F ≤ 4λΩ(Z (cid:63)) .

To derive an upper bound in estimation error from these inequalities  and to keep the argument as
Ω where λ = σΩ∗(G). From
simple as possible we consider the oracle1 estimate ˆZOracle
Lemma 2 we immediately get

equal to ˆZ λ

Ω

Ω − Z (cid:63)(cid:13)(cid:13)2
(cid:13)(cid:13) ˆZ λ
E(cid:13)(cid:13) ˆZOracle

− Z (cid:63)(cid:107)2

Ω

F ≤ 4σ Ω(Z (cid:63)) E Ω∗(G) .

  ˆZOracle

kq and Ωk q(ab(cid:62)) = (cid:107)ab(cid:62)(cid:107)∗ = 1 which lead to the corollary:

(9)
This upper bound can be computed for Z (cid:63) = ab(cid:62) ∈ Ak q for different norms. In particular  for
Ω(Z (cid:63))  we have (cid:107)ab(cid:62)(cid:107)1 ≤ √
Corollary 1 When Z (cid:63) = ab(cid:62) ∈ Ak q is an atom  the expected errors of the oracle estimators
ˆZOracle
using respectively the (k  q)-trace norm  the (cid:96)1 norm and the trace norm
Ωk q
are upper bounded as follows:
− Z (cid:63)(cid:107)2
− Z (cid:63)(cid:107)2
− Z (cid:63)(cid:107)2

(cid:19)
(cid:112)2 log(m1m2) ≤ 2σ(cid:112)2kq log(m1m2)  

(cid:18)(cid:114)
F ≤ 8 σ
F ≤ 2σ(cid:107)Z (cid:63)(cid:107)1
√
F ≤ 2σ(
m1 +

E(cid:107) ˆZOracle
E(cid:107) ˆZOracle
E(cid:107) ˆZOracle

and ˆZOracle

(cid:114)

+ 2k +

m1
k

m2
q

m2) .

q log

+ 2q

 

Ωk q

1
∗

1

∗

k log

√

  reaching σ(cid:112)2 log(m1m2) on e1e(cid:62)

1

When the smallest entry in absolute value of a or b is close to 0  then the expected error is smaller for
ˆZOracle
1 while not changing for the two other norms. But under
√
the assumption that the smallest nonzero entries in absolute value of a and b are lower bounded by
kq with c a constant  the upper bound on the rates obtained for the (k  q)-trace norm is at least
c/
an order of magnitude larger than for the other norms. We report the order of magnitude of these
√
upper bounds in Table 1 for m1 = m2 = m and k = q and assuming that nonzeros coefﬁcients are
lower bounded in magnitude by c/
Obviously the comparison of upper bounds is not enough to conclude to the superiority of
(k  q)-trace norm and  admittedly  the problem of denoising considered here is a special instance
of linear regression in which the design matrix is the identity  and  since this is a case in which the
design is trivially incoherent  it is possible to obtain fast rates for decomposable norms such as the
(cid:96)1 or trace norm [13]; however  the slow rates obtained are the same if instead of Y a linear trans-
formation of Z with incoherent design is observed  or when the signal to recover is only weakly
sparse  which is not the case for the fast rates. Moreover  Lemma 2 applies to matrices of any rank
and Corollary 1 generalizes to rank greater than 1. We present in the next section more sophisticated
results  based on bounds on the so-called statistical dimension of different norms [1].

kq.

(10)

4 A bound on the statistical dimension of the (k  q)-trace norm

The squared Gaussian width [7  and ref. therein] and the statistical dimension introduced recently
by Amelunxen et al. [1]  provide quantiﬁed estimation guarantees. The two quantities are equal

1We call it oracle estimate because the choice of λ depends on the unknown noise level. Virtually identi-
cal bounds (up to constants) holding with large probability could be derived for the non-oracle estimator by
controlling the deviations of Ω∗(G) from its expectation.

4

up to an additive term smaller than 1 and we thus present results only in terms of the statistical
dimension. The sample complexity of exact recovery and robust recovery are characterized by this
quantity [7]. It is also equal to the signal to noise ratio necessary for denoising [6] and demixing
[1] (see supplementary section 3). The statistical dimension is deﬁned as follows: if TΩ(A) is the
tangent cone of a matrix norm Ω : Rm1×m2 → R+ at A  then  the statistical dimension of TΩ(A) is

S(Z  Ω) := E(cid:104)(cid:13)(cid:13)ΠTΩ(Z)(G)(cid:13)(cid:13)2

(cid:105)

 

F

where G ∈ Rm1×m2 is a random matrix with i.i.d. standard normal entries and ΠTΩ(Z)(G) is the
orthogonal projection of G onto the cone TΩ(Z). In this section  we compute an upper bound on
the statistical dimension of Ωk q at an atoms A of Ak q  which we will denote by S(A  Ωk q)  and
compare it to results known for linear combinations of the (cid:96)1 and the trace norm of the form Γµ with

∀µ ∈ [0  1]  ∀Z ∈ Rm1×m2   Γµ(Z) :=

µ√
kq

(cid:107)Z(cid:107)1 + (1 − µ)(cid:107)Z(cid:107)(cid:63)  

(11)

j. We deﬁne the strength γ(a  b) ∈ (0  1] as γ(a  b) := (k a2
b2

which are norms that have been used in the literature to infer sparse low-rank matrices [17]. The
ability to recover the support of a sparse vector typically depends on the size of its smallest non-zero
coefﬁcient. For the recovery of a sparse rank 1 matrix  this motivates the following deﬁnition
Deﬁnition 3 Let A = ab(cid:62) ∈ Ak q with I0 = supp(a) and J0 = supp(b). Denote a2
min) ∧ (q b2
b2
min = min
j∈J0
√
The strength of an atom takes the maximal value of 1 when |ai| = 1/
k  i ∈ I and |bj| = 1/
q  j ∈
J where I and J are the supports of a and b. On the contrary  its strength is close to 0 as soon as one
of its nonzero entries is close to zero. We can now present our main result: a bound on the statistical
dimension of Ωk q on Ak q.
Proposition 1 For A = ab(cid:62) ∈ Ak q with strength γ = γ(a  b)  there exist universal constants
c1  c2  independent of m1  m2  k  q such that

min = min
i∈I0

i and
a2

min).

√

S(A  Ωk q) ≤ c1

γ2 (k + q) +

(k + q) log(m1 ∨ m2) .

c2
γ

Our proof  presented in the appendix  follows the scheme proposed in [7] and used for the trace
norm and (cid:96)1 norm. However  Ωk q is not decomposable and requires some work to obtain precise
upper bounds on various quantities.
Note ﬁrst that S must be larger than the number of degrees of freedoms of elements of Ak q which
is k + q − 1. So the bound could not possibly be improved beyond logarithmic factors  besides
the logarithmic dependence on the dimension of the overall space is expected. To appreciate the
result  it should be compared with the statistical dimension for the (cid:96)1-norm which scales as the
product of the size of the support with the logarithm of the dimension of the ambient space  that is
as kq log(m1m2). Using Landau notation  we report in Table 1 the upper and lower bounds known
for the statistical dimension of other norms in the case where k = q and m1 = m2 = m. The
rates are known exactly up to constants for the (cid:96)1 and the trace norm (see e.g. [1]). Of particular
relevance is the comparison with norms of the form Γµ which have been introduced with the aim
of improving over both the (cid:96)1-norm and the trace norm and have been the object of a signiﬁcant
literature [17  15  9]. Using theorem 3.2 in [15]  we prove in appendix 4 a lower bound on the
statistical dimension of Γµ of order kq∧ (m1 + m2) which holds for all values of µ  and which show
that  up to logarithmic factors  Ωk q is an order of magnitude smaller in term of k ∧ q.
In the right column of Table 1 we also report results in the vector case  that is  when m2 = q = 1. In
fact  in that case  Ωk 1 is exactly the k-support norm proposed by [2]. But the statistical dimension
of that norm and the (cid:96)1 norm is the same as it is known that the rate k log p
k cannot be improved
([4]). So  perhaps surprisingly  there improvement in the matrix case but not in the vector case.

5 Algorithm

In this section  we present a working set algorithm that attempts to solve problem (4). Injecting the
variational form (3) of Ωk q in (4) and eliminating the variable Z from the optimization problem

5

Matrix norm
(k  q)-trace

(cid:96)1

trace-norm
(cid:96)1 + trace-n.

S

O(k log m)
Θ(k2 log m
k2 )

Ω(cid:0)k2 ∧ m(cid:1)

Θ(m)

Ω(Z (cid:63))E Ω∗(G)
(k log m
k )1/2
(k2 log m)1/2

O(cid:0)m1/2 ∧ (k2 log m)1/2(cid:1)

m1/2

Vector norm
k-support

(cid:96)1
(cid:96)2

elastic net

S

Θ(k log p
k )
Θ(k log p
k )

p

Θ(k log p
k )

Table 1: Scaling of the statistical dimension S and of the upper bound Ω(Z (cid:63)) EΩ∗(G) in estimation
error (slow-rates) of different matrix norms for elements of Ak q with strength (see Deﬁnition 3)
√
lower bounded by a constant (or equivalently with nonzero coefﬁcient lower bounded by c/
kq
for c a constant). Leftmost columns: scalings for matrices with k = q  m = m1 = m2; rightmost
columns: scalings for vectors with m1 = p and m2 = q = 1. We use the notations Ω and Θ with
f = Ω(g) meaning g = O(f ) and f = Θ(g) to mean that both g = O(f ) and f = O(g).

using Z =(cid:80)

min

Z(IJ)∈Rm1m2

L(cid:16) (cid:88)

Z (IJ)(cid:17)

(I J)∈S

(cid:88)

(cid:107)Z (IJ)(cid:107)∗ 

+ λ

(I J)∈S

(I J)∈S Z (IJ)  one obtains that  when S = Gm1

k × Gm2

q

s.t. Supp(Z (IJ)) ⊂ I × J  (I  J) ∈ S.

  problem (4) is equivalent to
(PS)

k   b ∈ Am2

k q(∇L(Z)) ≤ λ.

k q(K) = max{a(cid:62)Kb | a ∈ Am1

At the optimum of (4) however  most of the variables Z (IJ) are equal to zero  and the solution is
the same as the solution obtained from (PS) in which S is reduced to the set of non-zero matrices
Z (IJ) obtained at optimality  that are often called the active components. We thus propose to solve
problem (4) using a so-called working set algorithm which solves a sequence of problems of the form
(PS) for a growing sequence of working sets S  so as to keep a small number of non-zero matrices
Z (IJ) throughout. Problem (PS) is solved easily using approximate block coordinate descent on
the (Z (IJ))(I J)∈S [3  Chap. 4]   which consists in iterating proximal operators of the trace norm
on blocks I × J. The principle of the working set algorithm is to solve problem (PS) for the
current working set S and to check whether a new component should be added. It can be shown
that a component with support I × J should be added if and only if (cid:107)[∇L(Z)]IJ(cid:107)op > λ for the
current value of Z. If such a component is found  the corresponding (I  J) pair is added in S and
problem (PS) is solved again. Given that for any component in S  we have (cid:107)[∇L(Z)]IJ(cid:107)op ≤ λ at
the optimum of (PS)  the algorithm terminates if Ω∗
q }  which is NP-hard to
The main difﬁculty is that Ω∗
compute  since it reduces in particular to rank 1 sparse PCA when k = q and K is p.s.d.. This
implies that determining when the algorithm should stop and which new component to add is hard.
However  a signiﬁcant amount of research has been carried out on sparse PCA recently  and we
thus propose to leverage some of the recently proposed relaxations and heuristics to solve this rank
In particular  the Truncated Power
1 sparse PCA problem (see [8  20] and references therein).
iteration (TPI) algorithm of [20] can easily be modiﬁed to compute Ω∗
k q which corresponds to a
generalization of the rank 1 sparse PCA in which in general a (cid:54)= b and k (cid:54)= q.
In our numerical experiments  we used a variant of Truncated Power Iteration with multiple restarts 
keeping track of the highest found variance. It should be noted that under RIP conditions on the
matrix  [20] shows that the solution returned by TPI is guaranteed to solve the rank 1 sparse PCA
problem. Also  even if TPI ﬁnds a pair (I  J) which is suboptimal  adding it in S does not hurt as
the algorithm might determine subsequently that it is not necessary. However TPI might fail to ﬁnd
some of the components violating the optimality conditions and terminate the algorithm early.
The proposed algorithm cannot be guaranteed to solve (4) if Ω∗
k q is not computed exactly  but it
exploits as much as possible the structure of the convex optimization problem to ﬁnd a candidate
solution. A similar active set algorithm can be designed to solve problems regularized by Ωk (cid:23).
Formulations regularized by the trace norm require to compute its proximal operator  and thus to
compute an SVD. However  even when m1  m2 are large  solving PS involves the computation of
trace norms of matrices of size only k × q and so the SVDs that need to be computed are fairly
small. This means that the computational bottleneck of the algorithm is clearly in ﬁnding candidate
supports. It has been proved [20] that  under some conditions  the problem can be solved in linear
time. Multiple restarts allow to ﬁnd good candidate supports in practice.

6

Figure 1: Estimates of the statistical dimensions of the (cid:96)1  trace and Ωk q norms at a matrix Z ∈

R1000×1000 in different setting. From left to right: (a): Z is an atom in (cid:101)Ak k for different values of
k. (b) Z is a sum of r atoms in (cid:101)Ak k with non-overlapping support  with k = 10 and varying r 
(c) Z is a sum of 3 atoms in (cid:101)Ak k with non-overlapping support  for varying k. (d) Z is a sum of 3
atoms in (cid:101)Ak k with overlapping support  for varying k.

6 Numerical experiments
In this section we report experimental results to assess the performance of sparse low-rank matrix
estimation using different techniques. We start in Section 6.1 with simulations that conﬁrm and
illustrate the theoretical results on statistical dimension of Ωk q and assess how they generalize to
matrices with (k  q)-rank larger than 1. In Section 6.2 we compare several techniques for sparse
PCA on simulated data.

6.1 Empirical estimates of the statistical dimension.
In order to numerically estimate the statistical dimension S(Z  Ω) of a regularizer Ω at a matrix
Z  we add to Z a random Gaussian noise matrix and observe Y = Z + σG where G has normal
i.i.d. entries following N (0  1). We then denoise Y to form an estimate ˆZ of Z. For small σ  the
normalized mean-squared error (NMSE) deﬁned as NMSE(σ) := E(cid:107) ˆZ−Z(cid:107)2
F/σ2 is a good estimate
of the statistical dimension  since [14] show that S(Z  Ω) = limσ→0 NMSE(σ) . Numerically  we
therefore estimate S(Z  Ω) with the empirical NMSE(σ) for σ = 10−4  averaged over 20 replicates.
We consider square matrices with m1 = m2 = 1000  and estimate the statistical dimension of Ωk q 
the (cid:96)1 and the trace norms at different matrices Z. The constrained denoiser has a simple closed-form
for the (cid:96)1 and the trace norm. For Ωk q  it can be obtained by a sequence of proximal projections
with different parameters λ until Ωk q( ˆZ) has the correct value Ωk q(Z). Since the noise is small 
we found that it was sufﬁcient and faster to perform a (k  q)-SVD of Y by computing a proximal of
Ωk q with a small λ  and then apply the (cid:96)1 constrained denoiser to the set of (k  q)-sparse singular
values.

We ﬁrst estimate the statistical dimensions of the three norms at an atom Z in (cid:101)Ak q for different
values of k = q  where (cid:101)Ak q = {ab(cid:62) ∈ Ak q | (cid:107)ab(cid:62)(cid:107)∞ = 1/

√

kq} is the set of elements of
Ak q with nonzero entries of constant magnitude . Figure 1.a shows the results  which conﬁrm
the theoretical bounds summarized in Table 1. The statistical dimension of the trace norm does
not depend on k  while that of the (cid:96)1 norm increases almost quadratically with k and that of Ωk q
increases linearly with k. The linear versus quadratic dependence of the statistical dimension on
k are reﬂected by the slopes of the curves in the log-log plot in Figure 1.a. As expected  Ωk q
interpolates between the (cid:96)1 norm (for k = 1) and the trace norm (for k = m1)  and outperforms both
norms for intermediate values of k. This experiments therefore conﬁrms that our upper bound (1) on
S(Z  Ωk q) captures the correct order in k  although the constants can certainly be much improved 
and that our algorithm manages  in this simple setting  to correctly approximate the solution of the
convex minimization problem.
Second  we estimate the statistical dimension of Ωk q on matrices with (k  q)-rank larger than 1 
a setting for which we proved no theoretical result. Figure 1.b shows the numerical estimate of

S(Z  Ωk q) for matrices Z which are sums of r atoms in (cid:101)Ak k with non-overlapping support  for
dimensions of the three regularizers on matrices Z which are sums of 3 atoms in (cid:101)Ak k with re-

k = 10 and varying r. We observe that the increase in statistical dimension is roughly linear in
the (k  q)-rank. For a ﬁxed (k  q)-rank of 3  Figures 1.c and 1.d compare the estimated statistical

7

100101102103101102103104105106kNMSE(k k)−rank = 1 l1TraceΩk q11.522.533.544.555.56100200300400500600700800900(k q)−rankNMSE100101102103101102103104105106kNMSENo overlap l1TraceΩk q100101102101102103104105106kNMSE90 % overlap l1TraceΩk qSample covariance

4.20 ± 0.02

Trace

0.98 ± 0.01

(cid:96)1

2.07 ± 0.01

Trace + (cid:96)1
0.96 ± 0.01

Sequential
0.93 ± 0.08

Ωk (cid:23)

0.59 ± 0.03

Table 2: Relative error of covariance estimation with different methods.

spectively non-overlapping or overlapping supports. The shapes of the different curves are overall
similar to the rank 1 case  although the performance of Ωk q degrades when the supports of atoms
overlap. In both cases  Ωk q consistently outperforms the two other norms. Overall these experi-
ments suggest that the statistical dimension of Ωk q at a linear combination of r atoms increases as
Cr (k log m1 + q log m2) where the coefﬁcient C increases with the overlap among the supports of
the atoms.

6.2 Comparison of algorithms for sparse PCA

i=1 xix(cid:62)

(cid:80)n

2 + a3a(cid:62)

√

In this section we compare the performance of different algorithms in estimating a sparsely factored
covariance matrix that we denote Σ(cid:63). The observed sample consists of n i.i.d. random vectors gen-
erated according to N (0  Σ(cid:63) + σ2Idp)  where (k  k)-rank(Σ(cid:63)) = 3. The matrix Σ(cid:63) is formed by
3   having all the same sparsity (cid:107)ai(cid:107)0 = k = 10 
adding 3 blocks of rank 1  Σ(cid:63) = a1a(cid:62)
1 + a2a(cid:62)
3 × 3 overlaps and nonzero entries equal to 1/
k. The noise level σ = 0.8 is set in order to make
the signal to noise ratio below the level σ = 1 where a spectral gap appears and makes the spectral
baseline (penalizing the trace of the PSD matrix) work. In our experiments the number of variables
is p = 200 and n = 80 points are observed. To estimate the true covariance matrix from the noisy
observation  ﬁrst the sample covariance matrix is formed as ˆΣn = 1
i   and given as input
n
to various algorithms which provide a new estimate ˆΣ. The methods we compared are the following:
• Sample covariance. Output ˆΣn as the estimate of the covariance.
• (cid:96)1 penalty. Soft-threshold ˆΣn elementwise.
• Trace penalty on the PSD cone. minZ(cid:23)0
• Trace + (cid:96)1 penalty. minZ(cid:23)0
2(cid:107)Z − ˆΣn(cid:107)2
F + λΩk (cid:23)(Z)   with Ωk (cid:23) the gauge associated with Ak (cid:23)
2(cid:107)Z − ˆΣn(cid:107)2
• Ωk (cid:23) penalty. minZ∈Rp×p
introduced in Section 2.3.
• Sequential sparse PCA. This is the standard way of estimating multiple sparse principal com-
ponents which consists of solving the problem for a single component at each step t = 1 . . . r  and
deﬂate to switch to the next (t + 1)st component. The deﬂation step used in this algorithm is the
orthogonal projection Zt+1 = (Idp − utu(cid:62)
t ) . The tuning parameters for this ap-
proach are the sparsity level k and the number of principal components r. The hyperparameters
were chosen by leaving one portion of the train data off (validation) and selecting the parameter
which allows to build an estimator approximating the best the validation set’s empirical covariance.
We assumed the true value of k known in advance for all algorithms.
We report the relative errors (cid:107) ˆΣ − Σ(cid:63)(cid:107)F/(cid:107)Σ(cid:63)(cid:107)F over 10 runs of our experiments in Table 2. The
results indicate that sparse PCA methods  whether based on Ωk (cid:23) or the sequential method with
deﬂation steps  outperform spectral and (cid:96)1 baselines  and that penalizing Ωk (cid:23) is superior to the
sequential approach. This was to be expected since our algorithm minimizes a loss function close to
the error measure used  whereas the sequential scheme does not optimize a well-deﬁned objective.

2(cid:107)Z − ˆΣn(cid:107)2
F + λΓµ(Z).

t ) Zt (Idp − utu(cid:62)

1

F + λ Tr Z .

1

1

7 Conclusion
We formulated the problem of matrix factorization with sparse factors of known sparsity as the
minimization of an index  the (k  q)-rank which tight convex relaxation is the (k  q)-trace norm
regularizer. This penalty is proved to have near optimal statistical performance. Despite theoretical
computational hardness in the worst-case scenario  exploiting the convex geometry of the problem
allowed us to build an efﬁcient algorithm to minimize it. Future work will consist of relaxing the
constraint on the blocks size  and exploring applications such as ﬁnding small comminuties in large
random graph background.
Acknowlegments This project was partially funded by Agence Nationale de la Recherche grant
ANR-13-MONU-005-10 (CHORUS project) and by ERC grant SMAC-ERC-280032.

8

References
[1] D. Amelunxen  M. Lotz  M. McCoy  and J. Tropp. Living on the edge: a geometric theory of

phase transition in convex optimization. Information and Inference  3(3):224–294  2014.

[2] A. Argyriou  R. Foygel  and N. Srebro. Sparse prediction with the k-support norm. In Advances
[3] F. Bach. Optimization with sparsity-inducing penalties. Foundations and Trends R(cid:13) in Machine

in Neural Information Processing Systems (NIPS)  pages 1466–1474  2012.

Learning  4(1):1–106  2011.

[4] E. J. Candes and M. A. Davenport. How well can we estimate a sparse vector? Applied and

Computational Harmonic Analysis 34  34(2):317–323  2013.

[5] E.J. Cand`es  T Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from

magnitude measurements via convex programming. Comm. Pure Appl. Math.  2011.

[6] V. Chandrasekaran and M. I. Jordan. Computational and statistical tradeoffs via convex relax-

ation. Proc. Natl. Acad. Sci. USA  2013.

[7] V. Chandrasekaran  B. Recht  P.A. Parrilo  and A.S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12  2012.

[8] A. d’Aspremont  L. El Ghaoui  M.I. Jordan  and G.R.G. Lanckriet. A direct formulation for

sparse PCA using semideﬁnite programming. SIAM review  2007.

[9] X.V. Doan and S.A. Vavasis. Finding approximately rank-one submatrices with the nuclear

norm and l1 norm. SIAM Journal on Optimization  23:2502 – 2540  2013.

[10] V. Koltchinskii  K. Lounici  and A. Tsybakov. Nuclear-norm penalization and optimal rates

for noisy low-rank matrix completion. Ann. Stat.  39(5):2302–2329  2011.

[11] L. Mackey. Deﬂation methods for sparse PCA. Advances in Neural Information Processing

Systems (NIPS)  21:1017–1024  2008.

[12] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse

coding. J. Mach. Learn. Res.  11:19–60  2010.

[13] S. N Negahban  P. Ravikumar  M. J Wainwright  and B. Yu. A uniﬁed framework for high-

dimensional analysis of M-estimators. Statistical Science  27(4):538–557  2012.

[14] S. Oymak and B. Hassibi. Sharp MSE bounds for proximal denoising. Technical Report

1305.2714  arXiv  2013.

[15] S. Oymak  A. Jalali  M. Fazel  Y. Eldar  and B. Hassibi. Simultaneously structured models

with application to sparse and low-rank matrices. arXiv preprint 1212.3753  2012.

[16] E. Richard  S. Gaiffas  and N. Vayatis. Link prediction in graphs with autoregressive features.

In Neural Information Processing Systems  volume 15  pages 565–593. MIT Press  2012.

[17] E. Richard  P.-A. Savalle  and N. Vayatis. Estimation of simultaneously sparse and low-rank

matrices. In International Conference on Machine Learning (ICML)  2012.

[18] Y.X. Wang  H. Xu  and C. Leng. Provable subspace clustering: When LRR meets SSC. In

Advances in Neural Information Processing Systems (NIPS)  pages 64–72. 2013.

[19] D Witten  R Tibshirani  and T Hastie. A penalized matrix decomposition  with applications to

sparse PCA and CCA. Biostatistics  10(3):515–534  2009.

[20] X.-T. Yuan and T. Zhang. Truncated power method for sparse eigenvalue problems. J. Mach.

Learn. Res.  14(1):899–925  2013.

[21] H. Zou  T. Hastie  and R. Tibshirani. Sparse principal component analysis. Journal of compu-

tational and graphical statistics  15(2):265–286  2006.

9

,Emile Richard
Guillaume Obozinski
Jean-Philippe Vert