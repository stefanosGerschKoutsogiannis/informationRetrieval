2018,A Structured Prediction Approach for Label Ranking,We propose to solve a label ranking problem as a structured output regression task. In this view  we adopt a least square surrogate loss
approach that solves a supervised learning problem in two steps:
a regression step in a well-chosen feature space and a pre-image (or decoding) step. We use specific feature maps/embeddings for ranking data  which convert any ranking/permutation into a vector representation. These embeddings are all well-tailored for our approach  either by resulting in consistent estimators  or by solving trivially the pre-image problem which is often the bottleneck in structured prediction. Their extension to the case of incomplete or partial rankings is also discussed. Finally  we provide empirical results on synthetic and real-world datasets showing the relevance of our method.,A Structured Prediction Approach for Label Ranking

Anna Korba  Alexandre Garcia  Florence d’Alché-Buc

firstname.lastname@telecom-paristech.fr

LTCI  Télécom ParisTech
Université Paris-Saclay

Paris  France

Abstract

We propose to solve a label ranking problem as a structured output regression task.
In this view  we adopt a least square surrogate loss approach that solves a supervised
learning problem in two steps: a regression step in a well-chosen feature space
and a pre-image (or decoding) step. We use speciﬁc feature maps/embeddings for
ranking data  which convert any ranking/permutation into a vector representation.
These embeddings are all well-tailored for our approach  either by resulting in
consistent estimators  or by solving trivially the pre-image problem which is often
the bottleneck in structured prediction. Their extension to the case of incomplete or
partial rankings is also discussed. Finally  we provide empirical results on synthetic
and real-world datasets showing the relevance of our method.

1

Introduction

Label ranking is a prediction task which aims at mapping input instances to a (total) order over a
given set of labels indexed by {1  . . .   K}. This problem is motivated by applications where the
output reﬂects some preferences  or order of relevance  among a set of objects. Hence there is an
increasing number of practical applications of this problem in the machine learning litterature. In
pattern recognition for instance (Geng and Luo  2014)  label ranking can be used to predict the
different objects which are the more likely to appear in an image among a predeﬁned set. Similarly  in
sentiment analysis  (Wang et al.  2011) where the prediction of the emotions expressed in a document
is cast as a label ranking problem over a set of possible affective expressions. In ad targeting  the
prediction of preferences of a web user over ad categories (Djuric et al.  2014) can be also formalized
as a label ranking problem  and the prediction as a ranking guarantees that each user is qualiﬁed into
several categories  eliminating overexposure. Another application is metalearning  where the goal
is to rank a set of algorithms according to their suitability based on the characteristics of a target
dataset and learning problem (see Brazdil et al. (2003); Aiguzhinov et al. (2010)). Interestingly 
the label ranking problem can also be seen as an extension of several supervised tasks  such as
multiclass classiﬁcation or multi-label ranking (see Dekel et al. (2004); Fürnkranz and Hüllermeier
(2003)). Indeed for these tasks  a prediction can be obtained by postprocessing the output of a label
ranking model in a suitable way. However  label ranking differs from other ranking problems  such as
in information retrieval or recommender systems  where the goal is (generally) to predict a target
variable under the form of a rating or a relevance score (Cao et al.  2007).
More formally  the goal of label ranking is to map a vector x lying in some feature space X to a
ranking y lying in the space of rankings Y. A ranking is an ordered list of items of the set {1  . . .   K}.
These relations linking the components of the y objects induce a structure on the output space
Y. The label ranking task thus naturally enters the framework of structured output prediction for
which an abundant litterature is available (Nowozin and Lampert  2011). In this paper  we adopt
the Surrogate Least Square Loss approach introduced in the context of output kernels (Cortes et al. 
2005; Kadri et al.  2013; Brouard et al.  2016) and recently theoretically studied by Ciliberto et al.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(2016) and Osokin et al. (2017) using Calibration theory (Steinwart and Christmann  2008). This
approach divides the learning task in two steps: the ﬁrst one is a vector regression step in a Hilbert
space where the outputs objects are represented through an embedding  and the second one solves a
pre-image problem to retrieve an output object in the Y space. In this framework  the algorithmic
complexity of the learning and prediction tasks as well as the generalization properties of the resulting
predictor crucially rely on some properties of the embedding. In this work we study and discuss some
embeddings dedicated to ranking data.
Our contribution is three folds: (1) we cast the label ranking problem into the structured prediction
framework and propose embeddings dedicated to ranking representation  (2) for each embedding we
propose a solution to the pre-image problem and study its algorithmic complexity and (3) we provide
theoretical and empirical evidence for the relevance of our method.
The paper is organized as follows. In section 2  deﬁnitions and notations of objects considered through
the paper are introduced  and section 3 is devoted to the statistical setting of the learning problem.
section 4 describes at length the embeddings we propose and section 5 details the theoretical and
computational advantages of our approach. Finally section 6 contains empirical results on benchmark
datasets.

2 Preliminaries

2.1 Mathematical background and notations

Consider a set of items indexed by {1  . . .   K}  that we will denote(cid:74)K(cid:75). Rankings  i.e. ordered lists
of items of(cid:74)K(cid:75)  can be complete (i.e  involving all the items) or incomplete and for both cases  they
ties ranking of the items in(cid:74)K(cid:75). It can be seen as a permutation  i.e a bijection σ :(cid:74)K(cid:75) →(cid:74)K(cid:75) 

can be without-ties (total order) or with-ties (weak order). A full ranking is a complete  and without-

mapping each item i to its rank σ(i). The rank of item i is thus σ(i) and the item ranked at position
j is σ−1(j). We say that i is preferred over j (denoted by i (cid:31) j) according to σ if and only if i is
ranked lower than j: σ(i) < σ(j). The set of all permutations over K items is the symmetric group
which we denote by SK. A partial ranking is a complete ranking including ties  and is also referred
as a weak order or bucket order in the litterature (see Kenkre et al. (2011)). This includes in particular
the top-k rankings  that is to say partial rankings dividing items in two groups  the ﬁrst one being the
k ≤ K most relevant items and the second one including all the rest. These top-k rankings are given
a lot of attention because of their relevance for modern applications  especially search engines or
recommendation systems (see Ailon (2010)). An incomplete ranking is a strict order involving only a
small subset of items  and includes as a particular case pairwise comparisons  another kind of ranking
which is very relevant in large-scale settings when the number of items to be ranked is very large.
We now introduce the main notations used through the paper. For any function f  Im(f ) denotes
the image of f  and f−1 its inverse. The indicator function of any event E is denoted by I{E}. We
will denote by sign the function such that for any x ∈ R  sign(x) = I{x > 0} − I{x < 0}. The
notations (cid:107).(cid:107) and |.| denote respectively the usual l2 and l1 norm in an Euclidean space. Finally  for

any integers a ≤ b (cid:74)a  b(cid:75) denotes the set {a  a + 1  . . .   b}  and for any ﬁnite set C  #C denotes its

cardinality.

2.2 Related work

An overview of label ranking algorithms can be found in Vembu and Gärtner (2010)  Zhou et al.
(2014))  but we recall here the main contributions. One of the ﬁrst proposed approaches  called
pairwise classiﬁcation (see Fürnkranz and Hüllermeier (2003)) transforms the label ranking problem
into K(K − 1)/2 binary classiﬁcation problems. For each possible pair of labels 1 ≤ i < j ≤ K 
the authors learn a model mij that decides for any given example whether i (cid:31) j or j (cid:31) i holds. The
model is trained with all examples for which either i (cid:31) j or j (cid:31) i is known (all examples for which
nothing is known about this pair are ignored). At prediction time  an example is submitted to all
K(K − 1)/2 classiﬁers  and each prediction is interpreted as a vote for a label: if the classiﬁer mij
predicts i (cid:31) j  this counts as a vote for label i. The labels are then ranked according to the number
of votes. Another approach (see Dekel et al. (2004)) consists in learning for each label a linear
utility function from which the ranking is deduced. Then  a large part of the dedicated literature was
devoted to adapting classical partitioning methods such as k-nearest neighbors (see Zhang and Zhou
(2007)  Chiang et al. (2012)) or tree-based methods  in a parametric (Cheng et al. (2010)  Cheng et al.

2

(2009)  Aledo et al. (2017)) or a non-parametric way (see Cheng and Hüllermeier (2013)  Yu et al.
(2010)  Zhou and Qiu (2016)  Clémençon et al. (2017)  Sá et al. (2017)). Finally  some approaches
are rule-based (see Gurrieri et al. (2012)  de Sá et al. (2018)). We will compare our numerical results
with the best performances attained by these methods on a set of benchmark datasets of the label
ranking problem in section 6.

3 Structured prediction for label ranking

that we set to be SK the space of full rankings over the set of items(cid:74)K(cid:75). The quality of a prediction

3.1 Learning problem
Our goal is to learn a function s : X → Y between a feature space X and a structured output space Y 
s(x) is measured using a loss function ∆ : SK × SK → R  where ∆(s(x)  σ) is the cost suffered
by predicting s(x) for the true output σ. We suppose that the input/output pairs (x  σ) come from
some ﬁxed distribution P on X × SK. The label ranking problem is then deﬁned as:

minimizes:X→SKE(s)  with

E(s) =

∆(s(x)  σ)dP (x  σ).

(1)

(cid:90)

X×SK

In this paper  we propose to study how to solve this problem and its empirical counterpart for a
family of loss functions based on some ranking embedding φ : SK → F that maps the permutations
σ ∈ SK into a Hilbert space F:

∆(σ  σ(cid:48)) = (cid:107)φ(σ) − φ(σ(cid:48))(cid:107)2F .

(2)

This loss presents two main advantages: ﬁrst  there exists popular losses for ranking data that can
take this form within a ﬁnite dimensional Hilbert Space F  second  this choice beneﬁts from the
theoretical results on Surrogate Least Square problems for structured prediction using Calibration
Theory of Ciliberto et al. (2016) and of works of Brouard et al. (2016) on Structured Output Prediction
within vector-valued Reproducing Kernel Hilbert Spaces. These works approach Structured Output
Prediction along a common angle by introducing a surrogate problem involving a function g : X → F
(with values in F) and a surrogate loss L(g(x)  σ) to be minimized instead of Eq. 1. The surrogate
loss is said to be calibrated if a minimizer for the surrogate loss is always optimal for the true loss
(Calauzenes et al.  2012). In the context of true risk minimization  the surrogate problem for our case
writes as:

(cid:90)

minimize g:X→FR(g)  with R(g) =

with the following surrogate loss:

X×SK

L(g(x)  φ(σ))dP (x  σ).

(3)

L(g(x)  φ(σ)) = (cid:107)g(x) − φ(σ)(cid:107)2F .

(4)
Problem of Eq. (3) is in general easier to optimize since g has values in F instead of the set of
structured objects Y  here SK. The solution of (3)  denoted as g∗  can be written for any x ∈ X :
g∗(x) = E[φ(σ)|x]. Eventually  a candidate s(x) pre-image for g∗(x) can then be obtained by
solving:

(5)
In the context of Empirical Risk Minimization  a training sample S = {(xi  σi)  i = 1  . . .   N}  with
N i.i.d. copies of the random variable (x  σ) is available. The Surrogate Least Square approach for
Label Ranking Prediction decomposes into two steps:

s(x) = argmin
σ∈SK

L(g∗(x)  φ(σ)).

• Step 1: minimize a regularized empirical risk to provide an estimator of the minimizer of

the regression problem in Eq. (3):

minimize g∈H RS (g)  with RS (g) =

L(g(xi)  φ(σi)) + Ω(g).

(6)

with an appropriate choice of hypothesis space H and complexity term Ω(g). We denote by

(cid:98)g a solution of (6).

N(cid:88)

i=1

1
N

3

original space SK:

• Step 2: solve  for any x in X   the pre-image problem that provides a prediction in the

(cid:98)s(x) = argmin

σ∈SK

(cid:107)φ(σ) −(cid:98)g(x)(cid:107)2F .

The pre-image operation can be written as(cid:98)s(x) = d ◦(cid:98)g(x) with d the decoding function:
applied on(cid:98)g for any x ∈ X .

(cid:107)φ(σ) − h(cid:107)2F for all h ∈ F 

d(h) = argmin
σ∈SK

(8)

(7)

This paper studies how to leverage the choice of the embedding φ to obtain a good compromise
between computational complexity and theoretical guarantees. Typically  the pre-image problem
on the discrete set SK (of cardinality K!) can be eased for appropriate choices of φ as we show in
section 4  leading to efﬁcient solutions. In the same time  one would like to beneﬁt from theoretical

guarantees and control the excess risk of the proposed predictor(cid:98)s.

In the following subsection we exhibit popular losses for ranking data that we will use for the label
ranking problem.

3.2 Losses for ranking

We now present losses ∆ on SK that we will consider for the label ranking task. A natural loss
for full rankings  i.e. permutations in SK  is a distance between permutations. Several distances
on SK are widely used in the literature (Deza and Deza  2009)  one of the most popular being the
Kendall’s τ distance  which counts the number of pairwise disagreements between two permutations
σ  σ(cid:48) ∈ SK:

∆τ (σ  σ(cid:48)) =

I[(σ(i) − σ(j))(σ(cid:48)(i) − σ(cid:48)(j)) < 0].

(9)

The maximal Kendall’s τ distance is thus K(K−1)/2  the total number of pairs. Another well-spread
distance between permutations is the Hamming distance  which counts the number of entries on
which two permutations σ  σ(cid:48) ∈ SK disagree:

(cid:88)

i<j

K(cid:88)

∆H (σ  σ(cid:48)) =

I[σ(i) (cid:54)= σ(cid:48)(i)].

(10)

i=1

The maximal Hamming distance is thus K  the number of labels or items.
The Kendall’s τ distance is a natural discrepancy measure when permutations are interpreted as
rankings and is thus the most widely used in the preference learning literature. In contrast  the
Hamming distance is particularly used when permutations represent matching of bipartite graphs and
is thus also very popular (see Fathony et al. (2018)). In the next section we show how these distances
can be written as Eq. (2) for a well chosen embedding φ.

4 Output embeddings for rankings

In what follows  we study three embeddings tailored to represent full rankings/permutations in SK
and discuss their properties in terms of link with the ranking distances ∆τ and ∆H  and in terms of
algorithmic complexity for the pre-image problem (5) induced.

4.1 The Kemeny embedding

Motivated by the minimization of the Kendall’s τ distance ∆τ   we study the Kemeny embedding 
previously introduced for the ranking aggregation problem (see Jiao et al. (2016)):

φτ : SK → RK(K−1)/2

σ (cid:55)→ (sign(σ(j) − σ(i)))1≤i<j≤K .

which maps any permutation σ ∈ SK into Im(φτ ) (cid:40) {−1  1}K(K−1)/2 (that we have embedded
into the Hilbert space (RK(K−1)/2 (cid:104).  .(cid:105))). One can show that the square of the euclidean distance

4

between the mappings of two permutations σ  σ(cid:48) ∈ SK recovers their Kendall’s τ distance (proving
at the same time that φτ is injective) up to a constant: (cid:107)φτ (σ) − φτ (σ(cid:48))(cid:107)2 = 4∆τ (σ  σ(cid:48)). The
Kemeny embedding then naturally appears to be a good candidate to build a surrogate loss related

to ∆τ . By noticing that φτ has a constant norm (∀σ ∈ SK  (cid:107)φτ (σ)(cid:107) =(cid:112)K(K − 1)/2)  we can

rewrite the pre-image problem (7) under the form:

(cid:98)s(x) = argmin

−(cid:104)φτ (σ) (cid:98)g(x)(cid:105).

(11)

σ∈SK

To compute (11)  one can ﬁrst solve an Integer Linear Program (ILP) to ﬁnd (cid:99)φσ =
τ ((cid:99)φσ). The latter step 
argminφσ∈Im(φτ ) −(cid:104)φσ (cid:98)g(x)(cid:105)  and then ﬁnd the output object σ = φ−1

i.e. inverting φτ   can be performed in O(K 2) by means of the Copeland method (see Merlin and
Saari (1997))  which ranks the items by their number of pairwise victories1. In contrast  the ILP prob-
lem is harder to solve since it involves a minimization over Im(φτ )  a set of structured vectors since
their coordinates are strongly correlated by the transitivity property of rankings. Indeed  consider
a vector v ∈ Im(φτ )  so ∃σ ∈ SK such that v = φτ (σ). Then  for any 1 ≤ i < j < k ≤ K  if its
coordinates corresponding to the pairs (i  j) and (j  k) are equal to one (meaning that σ(i) < σ(j)
and σ(j) < σ(k))  then the coordinate corresponding to the pair (i  k) cannot contradict the others
and must be set to one as well. Since φσ = (φσ)i j ∈ Im(φτ ) is only deﬁned for 1 ≤ i < j ≤ K 
one cannot directly encode the transitivity constraints that take into account the components (φσ)i j
σ)i j ∈ RK(K−1) deﬁned
with j > i. Thus to encode the transitivity constraint we introduce φ(cid:48)
σ)i j = −(φσ)i j else  and write the ILP problem as
by (φ(cid:48)

σ = (φ(cid:48)

(cid:88)

σ)i j = (φσ)i j if 1 ≤ i < j ≤ K and (φ(cid:48)
(cid:98)g(x)i j(φ(cid:48)
φ(cid:48)
σ)i j ∈ {−1  1} ∀ i  j
σ)i j + (φ(cid:48)

follows: (cid:99)φσ = argmin
(φ(cid:48)

σ)j i = 0 ∀ i  j
σ)j k + (φ(cid:48)

(φ(cid:48)
−1 ≤ (φ(cid:48)

σ)i j + (φ(cid:48)

1≤i j≤K

σ)i j 

s.c.

σ

(12)

∀ i  j  k s.t. i (cid:54)= j (cid:54)= k.

σ)k i ≤ 1

Such a problem is NP-Hard. In previous works (see Calauzenes et al. (2012); Ramaswamy et al.
(2013))  the complexity of designing calibrated surrogate losses for the Kendall’s τ distance had
already been investigated. In particular  Calauzenes et al. (2012) proved that there exists no convex
K-dimensional calibrated surrogate loss for Kendall’s τ distance. As a consequence  optimizing this
type of loss has an inherent computational cost. However  in practice  branch and bound based ILP
solvers ﬁnd the solution of (12) in a reasonable time for a reduced number of labels K. We discuss
the computational implications of choosing the Kemeny embedding section 5.2. We now turn to the
study of an embedding devoted to build a surrogate loss for the Hamming distance.

4.2 The Hamming embedding

Another well-spread embedding for permutations  that we will call the Hamming embedding  consists
in mapping σ to its permutation matrix φH (σ):

φH : SK → RK×K

σ (cid:55)→ (I{σ(i) = j})1≤i j≤K  

where we have embedded the set of permutation matrices Im(φH ) (cid:40) {0  1}K×K into the Hilbert
space (RK×K (cid:104).  .(cid:105)) with (cid:104).  .(cid:105) the Froebenius inner product. This embedding shares similar
properties with the Kemeny embedding: ﬁrst  it is also of constant (Froebenius) norm  since
∀σ ∈ SK  (cid:107)φH (σ)(cid:107) =
K. Then  the squared euclidean distance between the mappings of
two permutations σ  σ(cid:48) ∈ SK recovers their Hamming distance (proving that φH is also injective):
(cid:107)φH (σ) − φH (σ(cid:48))(cid:107)2 = ∆H (σ  σ(cid:48)). Once again  the pre-image problem consists in solving the linear
program:

√

−(cid:104)φH (σ) (cid:98)g(x)(cid:105) 
1Copeland method ﬁrstly affects a score si for item i as: si =(cid:80)

(cid:98)s(x) = argmin

σ∈SK

I{σ(i) < σ(j)} and then ranks the

j(cid:54)=i

(13)

items by decreasing score.

5

which is  as for the Kemeny embedding previously  divided in a minimization step  i.e. ﬁnd(cid:99)φσ =
H ((cid:99)φσ). The inversion
step is of complexity O(K 2) since it involves scrolling through all the rows (items i) of the matrix(cid:99)φσ
argminφσ∈Im(φH ) −(cid:104)φσ  g(x)(cid:105)  and an inversion step  i.e. compute σ = φ−1

and all the columns (to ﬁnd their positions σ(i)). The minimization step itself writes as the following
problem:

(cid:98)g(x)i j(φσ)i j 

(cid:99)φσ = argmax
(cid:40)

(cid:88)
(cid:80)
i(φσ)i j =(cid:80)

1≤i j≤K

s.c

φσ

(φσ)i j ∈ {0  1} ∀ i  j

j(φσ)i j = 1 ∀ i  j  

(14)

which can be solved with the Hungarian algorithm (see Kuhn (1955)) in O(K 3) time. Now we turn
to the study of an embedding which presents efﬁcient algorithmic properties.

4.3 Lehmer code
A permutation σ = (σ(1)  . . .   σ(K)) ∈ SK may be uniquely represented via its Lehmer code (also

called the inversion vector)  i.e. a word of the form cσ ∈ CK =∆ {0}×(cid:74)0  1(cid:75)×(cid:74)0  2(cid:75)×···×(cid:74)0  K−1(cid:75) 

where for j = 1  . . .   K:

cσ(j) = #{i ∈(cid:74)K(cid:75) : i < j  σ(i) > σ(j)}.

(15)
The coordinate cσ(j) is thus the number of elements i with index smaller than j that are ranked
higher than j in the permutation σ. By default  cσ(1) = 0 and is typically omitted. For instance  we
have:

e
σ
cσ

1
2
0

2
1
1

3
4
0

4
5
0

5
7
0

6
3
3

7
6
1

8
9
0

9
8
1

It is well known that the Lehmer code is bijective  and that the encoding and decoding algorithms
have linear complexity O(K) (see Mareš and Straka (2007)  Myrvold and Ruskey (2001)). This
embedding has been recently used for ranking aggregation of full or partial rankings (see Li et al.
(2017)). Our idea is thus to consider the following Lehmer mapping for label ranking;

φL : SK → RK

σ (cid:55)→ (cσ(i)))i=1 ... K  

which maps any permutation σ ∈ SK into the space CK (that we have embedded into the Hilbert
space (RK (cid:104).  .(cid:105))). The loss function in the case of the Lehmer embedding is thus the following:

∆L(σ  σ(cid:48)) = (cid:107)φL(σ) − φL(σ(cid:48))(cid:107)2 

(16)

which does not correspond to a known distance over permutations (Deza and Deza  2009). Notice that
|φL(σ)| = dτ (σ  e) where e is the identity permutation  a quantity which is also called the number of
inversions of σ. Therefore  in contrast to the previous mappings  the norm (cid:107)φL(σ)(cid:107) is not constant for
any σ ∈ SK. Hence it is not possible to write the loss ∆L(σ  σ(cid:48)) as −(cid:104)φL(σ)  φL(σ(cid:48))(cid:105)2.Moreover 
K−1 ∆τ (σ  σ(cid:48)) ≤ |φL(σ) −
this mapping is not distance preserving and it can be proven that
φL(σ(cid:48))| ≤ ∆τ (σ  σ(cid:48)) (see Wang et al. (2015)). However  the Lehmer embedding still enjoys great
advantages. Firstly  its coordinates are decoupled  which will enable a trivial solving of the inverse
image step (7). Indeed we can write explicitly its solution as:

1

(cid:98)s(x) = φ−1
(cid:124)
(cid:123)(cid:122)
(cid:125)
L ◦ dL

d

◦(cid:98)g(x) with

dL : RK → CK

(hi)i=1 ... K (cid:55)→ ( argmin

j∈(cid:74)0 i−1(cid:75)(hi − j))i=1 ... K 

(17)

where d is the decoding function deﬁned in (8). Then  there may be repetitions in the coordinates of
the Lehmer embedding  allowing for a compact representation of the vectors.

2The scalar product of two embeddings of two permutations φL(σ)  φL(σ(cid:48)) is not maximized for σ = σ(cid:48).

6

4.4 Extension to partial and incomplete rankings

In many real-world applications  one does not observe full rankings but only partial or incomplete
rankings (see the deﬁnitions section 2.1). We now discuss to what extent the embeddings we propose
for permutations can be adapted to this kind of rankings as input data. Firstly  the Kemeny embedding
can be naturally extended to partial and incomplete rankings since it encodes relative information

about the positions of the items. Indeed  we propose to map any partial ranking(cid:101)σ to the vector:

φ((cid:101)σ) = (sign((cid:101)σ(i) −(cid:101)σ(j))1≤i<j≤K 

(18)
where each coordinate can now take its value in {−1  0  1} (instead of {−1  1} for full rankings).
For any incomplete ranking ¯σ  we also propose to ﬁll the missing entries (missing comparisons) in
the embedding with zeros. This can be interpreted as setting the probability that i (cid:31) j to 1/2 for
a missing comparison between (i  j). In contrast  the Hamming embedding  since it encodes the
absolute positions of the items  is tricky to extend to map partial or incomplete rankings where this
information is missing. Finally  the Lehmer embedding falls between the two latter embeddings. It
also relies on an encoding of relative rankings and thus may be adapted to take into account the partial
ranking information. Indeed  in Li et al. (2017)  the authors propose a generalization of the Lehmer
code for partial rankings. We recall that a tie in a ranking happens when #{i (cid:54)= j  σ(i) = σ(j)} > 0.

The generalized representation c(cid:48) takes into account ties  so that for any partial ranking(cid:101)σ:
Clearly  c(cid:48)(cid:101)σ(j) ≥ c(cid:101)σ(j) for all j ∈(cid:74)K(cid:75). Given a partial ranking(cid:101)σ  it is possible to break its ties to
convert it in a permutation σ as follows: for i  j ∈(cid:74)K(cid:75)2  if(cid:101)σ(i) =(cid:101)σ(j) then σ(i) = σ(j) iff i < j.
The entries j = 1  . . .   K of the Lehmer codes of(cid:101)σ (see (20)) and σ (see (15)) then verify:
where INj = #{i ≤ j (cid:101)σ(i) =(cid:101)σ(j)}. An example illustrating the extension of the Lehmer code to
code cσ(j) for any j ∈(cid:74)K(cid:75) requires to sum over the(cid:74)K(cid:75) items. As an incomplete ranking do not

c(cid:48)(cid:101)σ(j) = #{i ∈(cid:74)K(cid:75) : i < j (cid:101)σ(i) ≥(cid:101)σ(j)}.

partial rankings is given in the Supplementary. However  computing each coordinate of the Lehmer

involve the whole set of items  it is also tricky to extend the Lehmer code to map incomplete rankings.
Taking as input partial or incomplete rankings only modiﬁes Step 1 of our method since it corresponds
to the mapping step of the training data  and in Step 2 we still predict a full ranking. Extending our
method to the task of predicting as output a partial or incomplete ranking raises several mathematical
questions that we did not develop at length here because of space limitations. For instance  to predict
partial rankings  a naive approach would consist in predicting a full ranking and then converting it
to a partial ranking according to some threshold (i.e  keep the top-k items of the full ranking). A
more formal extension of our method to make it able to predict directly partial rankings as outputs
would require to optimize a metric tailored for this data and which could be written as in Eq. (2). A
possibility for future work could be to consider the extension of the Kendall’s τ distance with penalty
parameter p for partial rankings proposed in Fagin et al. (2004).

(19)

c(cid:48)(cid:101)σ(j) = cσ(j) + INj − 1

c(cid:101)σ(j) = cσ(j) 

 

(20)

5 Computational and theoretical analysis

5.1 Theoretical guarantees

In this section  we give some statistical guarantees for the estimators obtained by following the steps
described in section 3. To this end  we build upon recent results in the framework of Surrogate Least
Square by Ciliberto et al. (2016). Consider one of the embeddings φ on permutations presented in the
previous section  which deﬁnes a loss ∆ as in Eq. (2). Let cφ = maxσ∈SK (cid:107)φ(σ)(cid:107). We will denote
function as (8)3. Given an estimator(cid:98)g of g∗ from Step 1  i.e. a minimizer of the empirical surrogate
by s∗ a minimizer of the true risk (1)  g∗ a minimizer of the surrogate risk (3)  and d a decoding
risk (6) we can then consider in Step 2 an estimator(cid:98)s = d ◦(cid:98)g. The following theorem reveals how
the performance of the estimator(cid:98)s we propose can be related to a solution s∗ of (1) for the considered

embeddings.

3Note that d = φ

L ◦ dL for φL and is obtained as the composition of two steps for φτ and φH: solving an
−1

optimization problem and compute the inverse of the embedding.

7

Embedding
φτ
φH
φL

Step 2 (b)
Step 1 (a)
O(K 2N )
NP-hard
O(KN ) O(K 3N )
O(KN )
O(KN )

Regressor
kNN
Ridge

Step 1 (b)
O(1)
O(N 3)

Step 2 (a)
O(N m)
O(N m)

Table 1: Embeddings and regressors complexities.

Theorem 1 The excess risks of the proposed predictors are linked to the excess surrogate risks as:
(i) For the loss (2) deﬁned by the Kemeny and Hamming embedding φτ and φH respectively:

E(d ◦(cid:98)g) − E(s∗) ≤ cφ

(cid:112)R((cid:98)g) − R(g∗)

with cφτ =

(cid:113) K(K−1)
(cid:114)
E(d ◦(cid:98)g) − E(s∗) ≤

2

√

and cφH =

K.

(ii) For the loss (2) deﬁned by the Lehmer embedding φL:

(cid:112)R((cid:98)g) − R(g∗) + E(d ◦ g∗) − E(s∗) + O(K

√

K)

K(K − 1)

2

√

√

2(cid:112)K(K − 1)E(s∗) + O(K

The full proof is given in the Supplementary. Assertion (i) is a direct application of Theorem 2 in
Ciliberto et al. (2016). In particular  it comes from a preliminary consistency result which shows that
E(d ◦ g∗) = E(s∗) for both embeddings. Concerning the Lehmer embedding  it is not possible to
apply their consistency results immediately; however a large part of the arguments of their proof is
used to bound the estimation error for the surrogate risk  and we remain with an approximation error
E(d ◦ g∗) − E(s∗) + O(K
K) resulting in Assertion (ii). In Remark 2 in the Supplementary  we
give several insights about this approximation error. Firstly we show that it can be upper bounded
by 2
K). Then  we explain how this term results from using φL in
the learning procedure. The Lehmer embedding thus have weaker statistical guarantees  but has the
advantage of being more computationnally efﬁcient  as we explain in the next subsection.

Notice that for Step 1  one can choose a consistent regressor with vector values(cid:98)g  i.e such that
R((cid:98)g) → R(g∗) when the number of training points tends to inﬁnity. Examples of such methods that
we use in our experiments to learn(cid:98)g  are the k-nearest neighbors (kNN) or kernel ridge regression
surrogate risk R((cid:98)g) − R(g∗) implies the control of E((cid:98)s) − E(s∗) where(cid:98)s = d ◦(cid:98)g by Theorem 1.

(Micchelli and Pontil  2005) methods whose consistency have been proved (see Chapter 5 in Devroye
et al. (2013) and Caponnetto and De Vito (2007)). In this case the control of the excess of the

√

Remark 1 We clarify that the consistency results of Theorem 1 are established for the task of
predicting full rankings which is adressed in this paper. In the case of predicting partial or incomplete
rankings  these results are not guaranteed to hold. Providing theoretical guarantees for this task is
left for future work.

5.2 Algorithmic complexity

We now discuss the algorithmic complexity of our approach. We recall that K is the number of
items/labels whereas N is the number of samples in the dataset. For a given embedding φ  the
total complexity of our approach for learning decomposes as follows. Step 1 in Section 3 can be
decomposed in two steps: a preprocessing step (Step 1 (a)) consisting in mapping the training sample
{(xi  σi)  i = 1  . . .   N} to {(xi  φ(σi))  i = 1  . . .   N}  and a second step (Step 1 (b)) that consists

in computing the estimator(cid:98)g of the Least squares surrogate empirical minimization (6). Then  at
mapping new inputs to a Hilbert space using(cid:98)g (Step 2 (a))  and then solving the preimage problem (7)

prediction time  Step 2 Section 3 can also be decomposed in two steps: a ﬁrst one consisting in

(Step 2 (b)). The complexity of a predictor corresponds to the worst complexity across all steps. The
complexities resulting from the choice of an embedding and a regressor are summarized Table 1 
where we denoted by m the dimension of the ranking embedded representations. The Lehmer
embedding with kNN regressor thus provides the fastest theoretical complexity of O(KN ) at the
cost of weaker theoretical guarantees. The fastest methods previously proposed in the litterature
typically involved a sorting procedure at prediction Cheng et al. (2010) leading to a O(N Klog(K))
complexity. In the experimental section we compare our approach with the former (denoted as Cheng

8

PL)  but also with the label wise decomposition approach in Cheng and Hüllermeier (2013) (Cheng
LWD) involving a kNN regression followed by a projection on SK computed in O(K 3N )  and the
more recent Random Forest Label Ranking (Zhou RF) Zhou and Qiu (2016). In their analysis  if dX
is the size of input features and Dmax the maximum depth of a tree  then RF have a complexity in
O(DmaxdX K 2N 2).

6 Numerical Experiments

Finally we evaluate the performance of our approach on standard benchmarks. We present the results
obtained with two regressors : Kernel Ridge regression (Ridge) and k-Nearest Neighbors (kNN).
Both regressors were trained with the three embeddings presented in Section 4. We adopt the same
setting as Cheng et al. (2010) and report the results of our predictors in terms of mean Kendall’s τ:

(cid:26)C : number of concordant pairs between 2 rankings

C − D

 

(21)

kτ =

K(K − 1)/2

D : number of discordant pairs between 2 rankings

from ﬁve repetitions of a ten-fold cross-validation (c.v.). Note that kτ is an afﬁne transformation of the
Kendall’s tau distance ∆τ mapping on the [−1  1] interval. We also report the standard deviation of
the resulting scores as in Cheng and Hüllermeier (2013). The parameters of our regressors were tuned
in a ﬁve folds inner c.v. for each training set. We report our parameter grids in the supplementary
materials.

Table 2: Mean Kendall’s τ coefﬁcient on benchmark datasets

kNN Hamming
kNN Kemeny
kNN Lehmer
ridge Hamming
ridge Lehmer
ridge Kemeny
Cheng PL
Cheng LWD
Zhou RF

authorship
0.01±0.02
0.94±0.02
0.93±0.02
-0.00±0.02
0.92±0.02
0.94±0.02
0.94±0.02
0.93±0.02
0.91

glass
0.08±0.04
0.85±0.06
0.85±0.05
0.08±0.05
0.83±0.05
0.86±0.06
0.84±0.07
0.84±0.08
0.89

iris
-0.15±0.13
0.95±0.05
0.95±0.04
-0.10±0.13
0.97±0.03
0.97±0.05
0.96±0.04
0.96±0.04
0.97

vehicle
-0.21±0.04
0.85±0.03
0.84±0.03
-0.21±0.03
0.85±0.02
0.89±0.03
0.86±0.03
0.85±0.03
0.86

vowel
0.24±0.04
0.85±0.02
0.78±0.03
0.26±0.04
0.86±0.01
0.92±0.01
0.85±0.02
0.88±0.02
0.87

wine
-0.36±0.04
0.94±0.05
0.94±0.06
-0.36±0.03
0.84±0.08
0.94±0.05
0.95±0.05
0.94±0.05
0.95

The Kemeny and Lehmer embedding based approaches are competitive with the state of the art
methods on these benchmarks datasets. The Hamming based methods give poor results in terms of
kτ but become the best choice when measuring the mean Hamming distance between predictions and
ground truth (see Table 3 in the Supplementary). In contrast  the fact that the Lehmer embedding
performs well for the optimization of the Kendall’s τ distance highlights its practical relevance for
label ranking. The Supplementary presents additional results (on additional datasets and results in
terms of Hamming distance) which show that our method remains competitive with the state of the
art. The code to reproduce our results is available: https://github.com/akorba/Structured_
Approach_Label_Ranking/

7 Conclusion

This paper introduces a novel framework for label ranking  which is based on the theory of Surrogate
Least Square problem for structured prediction. The structured prediction approach we propose
comes along with theoretical guarantees and efﬁcient algorithms  and its performance has been
shown on real-world datasets. To go forward  extensions of our methodology to predict partial and
incomplete rankings are to be investigated. In particular  the framework of prediction with abstention
should be of interest.

References
Aiguzhinov  A.  Soares  C.  and Serra  A. P. (2010). A similarity-based adaptation of naive bayes for label
ranking: Application to the metalearning problem of algorithm recommendation. In International Conference
on Discovery Science  pages 16–26. Springer.

9

Ailon  N. (2010). Aggregation of partial rankings  p-ratings and top-m lists. Algorithmica  57(2):284–300.

Aledo  J. A.  Gámez  J. A.  and Molina  D. (2017). Tackling the supervised label ranking problem by bagging

weak learners. Information Fusion  35:38–50.

Brazdil  P. B.  Soares  C.  and Da Costa  J. P. (2003). Ranking learning algorithms: Using ibl and meta-learning

on accuracy and time results. Machine Learning  50(3):251–277.

Brouard  C.  Szafranski  M.  and d?Alché Buc  F. (2016). Input output kernel regression: supervised and
semi-supervised structured output prediction with operator-valued kernels. Journal of Machine Learning
Research  17(176):1–48.

Calauzenes  C.  Usunier  N.  and Gallinari  P. (2012). On the (non-) existence of convex  calibrated surrogate

losses for ranking. In Advances in Neural Information Processing Systems  pages 197–205.

Cao  Z.  Qin  T.  Liu  T.-Y.  Tsai  M.-F.  and Li  H. (2007). Learning to rank: from pairwise approach to listwise
approach. In Proceedings of the 24th Annual International Conference on Machine learning (ICML-07) 
pages 129–136. ACM.

Caponnetto  A. and De Vito  E. (2007). Optimal rates for the regularized least-squares algorithm. Foundations

of Computational Mathematics  7(3):331–368.

Cheng  W.  Hühn  J.  and Hüllermeier  E. (2009). Decision tree and instance-based learning for label ranking. In
Proceedings of the 26th Annual International Conference on Machine Learning (ICML-09)  pages 161–168.
ACM.

Cheng  W. and Hüllermeier  E. (2013). A nearest neighbor approach to label ranking based on generalized

labelwise loss minimization.

Cheng  W.  Hüllermeier  E.  and Dembczynski  K. J. (2010). Label ranking methods based on the plackett-luce
model. In Proceedings of the 27th Annual International Conference on Machine Learning (ICML-10)  pages
215–222.

Chiang  T.-H.  Lo  H.-Y.  and Lin  S.-D. (2012). A ranking-based knn approach for multi-label classiﬁcation. In

Asian Conference on Machine Learning  pages 81–96.

Ciliberto  C.  Rosasco  L.  and Rudi  A. (2016). A consistent regularization approach for structured prediction.

In Advances in Neural Information Processing Systems  pages 4412–4420.

Clémençon  S.  Korba  A.  and Sibony  E. (2017). Ranking median regression: Learning to order through local

consensus. arXiv preprint arXiv:1711.00070.

Cortes  C.  Mohri  M.  and Weston  J. (2005). A general regression technique for learning transductions. In
Proceedings of the 22nd Annual International Conference on Machine learning (ICML-05)  pages 153–160.

de Sá  C. R.  Azevedo  P.  Soares  C.  Jorge  A. M.  and Knobbe  A. (2018). Preference rules for label ranking:

Mining patterns in multi-target relations. Information Fusion  40:112–125.

Dekel  O.  Singer  Y.  and Manning  C. D. (2004). Log-linear models for label ranking. In Advances in neural

information processing systems  pages 497–504.

Devroye  L.  Györﬁ  L.  and Lugosi  G. (2013). A probabilistic theory of pattern recognition  volume 31.

Springer Science & Business Media.

Deza  M. and Deza  E. (2009). Encyclopedia of Distances. Springer.

Djuric  N.  Grbovic  M.  Radosavljevic  V.  Bhamidipati  N.  and Vucetic  S. (2014). Non-linear label ranking for

large-scale prediction of long-term user interests. In AAAI  pages 1788–1794.

Fagin  R.  Kumar  R.  Mahdian  M.  Sivakumar  D.  and Vee  E. (2004). Comparing and aggregating rankings
with ties. In Proceedings of the twenty-third ACM SIGMOD-SIGACT-SIGART symposium on Principles of
database systems  pages 47–58. ACM.

Fathony  R.  Behpour  S.  Zhang  X.  and Ziebart  B. (2018). Efﬁcient and consistent adversarial bipartite

matching. In International Conference on Machine Learning  pages 1456–1465.

Fürnkranz  J. and Hüllermeier  E. (2003). Pairwise preference learning and ranking. In European conference on

machine learning  pages 145–156. Springer.

10

Geng  X. and Luo  L. (2014). Multilabel ranking with inconsistent rankers. In Computer Vision and Pattern

Recognition (CVPR)  2014 IEEE Conference on  pages 3742–3747. IEEE.

Gurrieri  M.  Siebert  X.  Fortemps  P.  Greco  S.  and Słowi´nski  R. (2012). Label ranking: A new rule-based
label ranking method. In International Conference on Information Processing and Management of Uncertainty
in Knowledge-Based Systems  pages 613–623. Springer.

Jiao  Y.  Korba  A.  and Sibony  E. (2016). Controlling the distance to a kemeny consensus without computing
it. In Proceedings of the 33rd Annual International Conference on Machine learning (ICML-16)  pages
2971–2980.

Kadri  H.  Ghavamzadeh  M.  and Preux  P. (2013). A generalized kernel approach to structured output learning.
In Proceedings of the 30th Annual International Conference on Machine learning (ICML-13)  pages 471–479.

Kamishima  T.  Kazawa  H.  and Akaho  S. (2010). A survey and empirical comparison of object ranking

methods. In Preference learning  pages 181–201. Springer.

Kenkre  S.  Khan  A.  and Pandit  V. (2011). On discovering bucket orders from preference data. In Proceedings

of the 2011 SIAM International Conference on Data Mining  pages 872–883. SIAM.

Kuhn  H. W. (1955). The hungarian method for the assignment problem. Naval Research Logistics (NRL) 

2(1-2):83–97.

Li  P.  Mazumdar  A.  and Milenkovic  O. (2017). Efﬁcient rank aggregation via lehmer codes. arXiv preprint

arXiv:1701.09083.

Mareš  M. and Straka  M. (2007). Linear-time ranking of permutations. In European Symposium on Algorithms 

pages 187–193. Springer.

Merlin  V. R. and Saari  D. G. (1997). Copeland method ii: Manipulation  monotonicity  and paradoxes. Journal

of Economic Theory  72(1):148–172.

Micchelli  C. A. and Pontil  M. (2005). Learning the kernel function via regularization. Journal of machine

learning research  6(Jul):1099–1125.

Myrvold  W. and Ruskey  F. (2001). Ranking and unranking permutations in linear time. Information Processing

Letters  79(6):281–284.

Nowozin  S. and Lampert  C. H. (2011). Structured learning and prediction in computer vision. Found. Trends.

Comput. Graph. Vis.  6(3:8211;4):185–365.

Osokin  A.  Bach  F. R.  and Lacoste-Julien  S. (2017). On structured prediction theory with calibrated convex

surrogate losses. In Advances in Neural Information Processing Systems (NIPS) 2017  pages 301–312.

Ramaswamy  H. G.  Agarwal  S.  and Tewari  A. (2013). Convex calibrated surrogates for low-rank loss matrices
with applications to subset ranking losses. In Advances in Neural Information Processing Systems  pages
1475–1483.

Sá  C. R.  Soares  C. M.  Knobbe  A.  and Cortez  P. (2017). Label ranking forests.

Steinwart  I. and Christmann  A. (2008). Support Vector Machines. Springer.

Vembu  S. and Gärtner  T. (2010). Label ranking algorithms: A survey. In Preference learning  pages 45–64.

Springer.

Wang  D.  Mazumdar  A.  and Wornell  G. W. (2015). Compression in the space of permutations. IEEE

Transactions on Information Theory  61(12):6417–6431.

Wang  Q.  Wu  O.  Hu  W.  Yang  J.  and Li  W. (2011). Ranking social emotions by learning listwise preference.

In Pattern Recognition (ACPR)  2011 First Asian Conference on  pages 164–168. IEEE.

Yu  P. L. H.  Wan  W. M.  and Lee  P. H. (2010). Preference Learning  chapter Decision tree modelling for

ranking data  pages 83–106. Springer  New York.

Zhang  M.-L. and Zhou  Z.-H. (2007). Ml-knn: A lazy learning approach to multi-label learning. Pattern

recognition  40(7):2038–2048.

Zhou  Y.  Liu  Y.  Yang  J.  He  X.  and Liu  L. (2014). A taxonomy of label ranking algorithms. JCP 

9(3):557–565.

Zhou  Y. and Qiu  G. (2016). Random forest for label ranking. arXiv preprint arXiv:1608.07710.

11

,Anna Korba
Alexandre Garcia
Florence d'Alché-Buc
Matthew Reimherr
Jordan Awan