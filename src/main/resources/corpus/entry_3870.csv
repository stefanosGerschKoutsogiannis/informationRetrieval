2014,On Communication Cost of Distributed Statistical Estimation and Dimensionality,We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean $\vectheta$ of an unknown $d$ dimensional gaussian distribution in the distributed setting. In this problem  the samples from the unknown distribution are distributed among $m$ different machines. The goal is to estimate the mean $\vectheta$ at the optimal minimax rate while communicating as few bits as possible. We show that in this setting  the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting \cite{ZDJW13} and to our improved bounds for the simultaneous setting  we prove new lower bounds of $\Omega(md/\log(m))$ and $\Omega(md)$ for the bits of communication needed to achieve the minimax squared loss  in the interactive and simultaneous settings respectively. To complement  we also demonstrate an interactive protocol achieving the minimax squared loss with $O(md)$ bits of communication  which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting  we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically  when the parameter is promised to be $s$-sparse  we show a simple thresholding based protocol that achieves the same squared loss while saving a $d/s$ factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.,On Communication Cost of Distributed Statistical

Estimation and Dimensionality

Ankit Garg

Department of Computer Science  Princeton University

garg@cs.princeton.edu

Tengyu Ma

Department of Computer Science  Princeton University

tengyu@cs.princeton.edu

Huy L. Nguy˜ˆen

Simons Institute  UC Berkeley

hlnguyen@cs.princeton.edu

Abstract

We explore the connection between dimensionality and communication cost in
distributed learning problems. Speciﬁcally we study the problem of estimating
the mean ~✓ of an unknown d dimensional gaussian distribution in the distributed
setting. In this problem  the samples from the unknown distribution are distributed
among m different machines. The goal is to estimate the mean ~✓ at the optimal
minimax rate while communicating as few bits as possible. We show that in this
setting  the communication cost scales linearly in the number of dimensions i.e.
one needs to deal with different dimensions individually. Applying this result to
previous lower bounds for one dimension in the interactive setting [1] and to our
improved bounds for the simultaneous setting  we prove new lower bounds of
⌦(md/ log(m)) and ⌦(md) for the bits of communication needed to achieve the
minimax squared loss  in the interactive and simultaneous settings respectively.
To complement  we also demonstrate an interactive protocol achieving the mini-
max squared loss with O(md) bits of communication  which improves upon the
simple simultaneous protocol by a logarithmic factor. Given the strong lower
bounds in the general setting  we initiate the study of the distributed parameter
estimation problems with structured parameters. Speciﬁcally  when the param-
eter is promised to be s-sparse  we show a simple thresholding based protocol
that achieves the same squared loss while saving a d/s factor of communication.
We conjecture that the tradeoff between communication and squared loss demon-
strated by this protocol is essentially optimal up to logarithmic factor.

1

Introduction

The last decade has witnessed a tremendous growth in the amount of data involved in machine learn-
ing tasks. In many cases  data volume has outgrown the capacity of memory of a single machine and
it is increasingly common that learning tasks are performed in a distributed fashion on many ma-
chines. Communication has emerged as an important resource and sometimes the bottleneck of the
whole system. A lot of recent works are devoted to understand how to solve problems distributedly
with efﬁcient communication [2  3  4  1  5].
In this paper  we study the relation between the dimensionality and the communication cost of sta-
tistical estimation problems. Most modern statistical problems are characterized by high dimension-
ality. Thus  it is natural to ask the following meta question:
How does the communication cost scale in the dimensionality?

1

We study this question via the problems of estimating parameters of distributions in the distributed
setting. For these problems  we answer the question above by providing two complementary results:

1. Lower bound for general case: If the distribution is a product distribution over the coordi-
nates  then one essentially needs to estimate each dimension of the parameter individually
and the information cost (a proxy for communication cost) scales linearly in the number of
dimensions.

2. Upper bound for sparse case: If the true parameter is promised to have low sparsity  then a
very simple thresholding estimator gives better tradeoff between communication cost and
mean-square loss.

Before getting into the ideas behind these results  we ﬁrst deﬁne the problem more formally. We con-
sider the case when there are m machines  each of which receives n i.i.d samples from an unknown
distribution P (from a family P) over the d-dimensional Euclidean space Rd. These machines need
to estimate a parameter ✓ of the distribution via communicating with each other. Each machine can
do arbitrary computation on its samples and messages it receives from other machines. We regard
communication (the number of bits communicated) as a resource  and therefore we not only want to
optimize over the estimation error of the parameters but also the tradeoff between the estimation er-
ror and communication cost of the whole procedure. For simplicity  here we are typically interested
in achieving the minimax error 1 while communicating as few bits as possible. Our main focus is
the high dimensional setting where d is very large.

Communication Lower Bound via Direct-Sum Theorem The key idea for the lower bound is 
when the unknown distribution P = P1 ⇥ ··· ⇥ Pd is a product distribution over Rd  and each
coordinate of the parameter ✓ only depends on the corresponding component of P   then we can
view the d-dimensional problem as d independent copies of one dimensional problem. We show
that  one unfortunately cannot do anything beyond this trivial decomposition  that is  treating each
dimension independently  and solving d different estimations problems individually. In other words 
the communication cost 2 must be at least d times the cost for one dimensional problem. We call
this theorem “direct-sum” theorem.
To demonstrate our theorem  we focus on the speciﬁc case where P is a d dimensional spherical
3 . The problem is to estimate
Gaussian distribution with an unknown mean and covariance 2Id
the mean of P . The work [1] showed a lower bound on the communication cost for this problem
when d = 1. Our technique when applied to their theorem immediately yields a lower bound
equal to d times the lower bound for the one dimension problem for any choice of d. Note that [5]
independently achieve the same bound by reﬁning the proof in [1].
In the simultaneous communication setting  where all machines send one message to one machine
and this machine needs to ﬁgure out the estimation  the work [1] showed that ⌦(md/ log m) bits
of communication are needed to achieve the minimax squared loss.
In this paper  we improve
this bound to ⌦(md)  by providing an improved lower bound for one-dimensional setting and then
applying our direct-sum theorem.
The direct-sum theorem that we prove heavily uses the idea and tools from the recent developments
in communication complexity and information complexity. There has been a lot of work on the
paradigm of studying communication complexity via the notion of information complexity [6  7  8 
9  10]. Information complexity can be thought of as a proxy for communication complexity that is
especially accurate for solving multiple copies of the same problem simultaneously [8]. Proving so-
called “direct-sum” results has become a standard tool  namely the fact that the amount of resources
required for solving d copies of a problem (with different inputs) in parallel is equal to d times
the amount required for one copy. In other words  there is no saving from solving many copies of
the same problem in batch and the trivial solution of solving each of them separately is optimal.
Note that this generic statement is certainly NOT true for arbitrary type of tasks and arbitrary type
of resources. Actually even for distributed computing tasks  if the measure of resources is the

1by minimax error we mean the minimum possible error that can be achieved when there is no limit on the

communication

2technically  information cost  as discussed below
3where Id denote the d ⇥ d identity matrix

2

communication cost instead of information cost  there exist examples where solving d copies of
a certain problem requires less communication than d times the communication required for one
copy [11]. Therefore  a direct-sum theorem  if true  could indeed capture the features and difﬁculties
of the problems.
Our result can be viewed as a direct sum theorem for communication complexity for statistical es-
timation problems: the amount of communication needed for solving an estimation problem in d
dimensions is at least d times the amount of information needed for the same problem in one di-
mension. The proof technique is directly inspired by the notion of conditional information complex-
ity [7]  which was used to prove direct sum theorems and lower bounds for streaming algorithms.
We believe this is a fruitful connection and can lead to more lower bounds in statistical machine
learning.
To complement the above lower bounds  we also show an interactive protocol that uses a log factor
less communication than the simple protocol  under which each machine sends the sample mean and
the center takes the average as the estimation. Our protocol demonstrates additional power of inter-
active communication and potential complexity of proving lower bound for interactive protocols.

Thresholding Algorithm for Sparse Parameter Estimation In light of the strong lower bounds
in the general case  a question suggests itself as a way to get around the impossibility results:
Can we do better when the data (parameters) have more structure?
We study this questions by considering the sparsity structure on the parameter ✓. Speciﬁcally  we
consider the case when the underlying parameter ✓ is promised to be s-sparse. We provide a simple
protocol that achieves the same squared-loss O(d2/(mn)) as in the general case  while using
˜O(sm) communications  or achieving optimal squared loss O(s2/(mn))  with communication
˜O(dm)  or any tradeoff between these cases. We even conjecture that this is the best tradeoff up to
polylogarithmic factors.

2 Problem Setup  Notations and Preliminaries

Classical Statistical Parameter Estimation We start by reviewing the classical framework of statis-
tical parameter estimation problems. Let P be a family of distributions over X . Let ✓ : P ! ⇥ ⇢ R
denote a function deﬁned on P. We are given samples X 1  . . .   X n from some P 2 P  and are asked
to estimate ✓(P ). Let ˆ✓ : X n ! ⇥ be such an estimator  and ˆ✓(X 1  . . .   X n) is the corresponding
estimate.
Deﬁne the squared loss R of the estimator to be

R(ˆ✓  ✓) = E

ˆ✓ Xhkˆ✓(X 1  . . .   X n)  ✓(P )k2
2i

In the high-dimensional case  let P d := { ~P = P1 ⇥ ··· ⇥ Pd : Pi 2 P} be the family of product
distributions over X d. Let ~✓ : P d ! ⇥d ⇢ Rd be the d-dimensional function obtained by applying
✓ point-wise ~✓ (P1 ⇥ ··· ⇥ Pd) = (✓(P1)  . . .   ✓(Pd)).
Throughout this paper  we consider the case when X = R and P = {N (✓  2) : ✓ 2 [1  1]} is
Gaussian distribution with for some ﬁxed and known . Therefore  in the high-dimensional case 
P d = {N ( ~✓   2Id) : ~✓ 2 [1  1]d} is a collection of spherical Gaussian distributions. We use ˆ~✓ to
denote the d-dimensional estimator. For clarity  in this paper  we always use~· to indicate a vector in
high dimensions.
Distributed Protocols and Parameter Estimation: In this paper  we are interested in the situation
where there are m machines and the jth machine receives n samples ~X (j 1)  . . .   ~X (j n) 2 Rd from
the distribution ~P = N ( ~✓   2Id). The machines communicate via a publicly shown blackboard.
That is  when a machine writes a message on the blackboard  all other machines can see the content
of the message. Following [1]  we usually refer to the blackboard as the fusion center or simply
center. Note that this model captures both point-to-point communication as well as broadcast com-

3

munication. Therefore  our lower bounds in this model apply to both the message passing setting
and the broadcast setting. We will say that a protocol is simultaneous if each machine broadcasts
a single message based on its input independently of the other machine ([1] call such protocols
independent).
We denote the collection of all the messages written on the blackboard by Y . We will refer to Y as
transcript and note that Y 2 {0  1}⇤ is written in bits and the communication cost is deﬁned as the
length of Y   denoted by |Y |. In multi-machine setting  the estimator ˆ~✓ only sees the transcript Y   and
it maps Y to ˆ~✓(Y ) 4  which is the estimation of ~✓ . Let letter j be reserved for index of the machine
and k for the sample and letter i for the dimension. In other words  ~X (j k)
is the ith-coordinate of
kth sample of machine j. We will use ~Xi as a shorthand for the collection of the ith coordinate of
all the samples: ~Xi = { ~X (j k)
: j 2 [m]  k 2 [n]}. Also note that [n] is a shorthand for {1  . . .   n}.
The mean-squared loss of the protocol ⇧ with estimator ˆ~✓ is deﬁned as
R⇣(⇧ 
[kˆ~✓(Y )  ~✓ k2]

ˆ~✓)  ~✓⌘ = sup

E
~X ⇧

~✓

i

i

and the communication cost of ⇧ is deﬁned as

CC(⇧) = sup
~✓

E
~X ⇧

[|Y |]

~X ⇧

RV d((⇧ 

ˆ~✓)  ~✓ ) = E

ˆ~✓)  ~✓ )  R((⇧ 

The main goal of this paper is to study the tradeoff between R⇣(⇧ 
Proving Minimax Lower Bound: We follow the standard way to prove minimax lower bound.
We introduce a (product) distribution V d of ~✓ over the [1  1]d. Let’s deﬁne the mean-squared loss
with respect to distribution V d as

ˆ~✓)  ~✓⌘ and CC(⇧).

[kˆ~✓(Y )  ~✓ k2]#

~✓ ⇠V d" E
ˆ~✓)  ~✓ ) for any distribution V d. Therefore to prove
It is easy to see that RV d((⇧ 
lower bound for the minimax rate  it sufﬁces to prove the lower bound for the mean-squared loss
under any distribution V d. 5
Private/Public Randomness: We allow the protocol to use both private and public randomness.
Private randomness  denoted by Rpriv  refers to the random bits that each machine draws by itself.
Public randomness  denoted by Rpub  is a sequence of random bits that is shared among all parties
before the protocol without being counted toward the total communication. Certainly allowing these
two types of randomness only makes our lower bound stronger  and public randomness is actually
only introduced for convenience.
Furthermore  we will see in the proof of Theorem 3.1  the beneﬁt of allowing private randomness
is that we can hide information using private randomness when doing the reduction from one di-
mension protocol to d-dimensional one. The downside is that we require a stronger theorem (that
tolerates private randomness) for the one dimensional lower bound  which is not a problem in our
case since technique in [1] is general enough to handle private randomness.
Information cost: We deﬁne information cost IC(⇧) of protocol ⇧ as mutual information between
the data and the messages communicated conditioned on the mean ~✓ . 6

4Therefore here ˆ~✓ maps {0  1}⇤ to ⇥
5Standard minimax theorem says that actually the supVd RVd ((⇧ 
compactness condition for the space of ~✓ .
6Note that here we have introduced a distribution for the choice of ~✓   and therefore ~✓ is a random variable.

ˆ~✓)  ~✓ ) under certain

ˆ~✓)  ~✓ ) = R((⇧ 

4

Private randomness doesn’t explicitly appear in the deﬁnition of information cost but it affects it.
Note that the information cost is a lower bound on the communication cost:

ICV d(⇧) = I( ~X; Y | ~✓   Rpub)

ICV d(⇧) = I( ~X; Y | ~✓   Rpub)  H(Y )  CC(⇧)

The ﬁrst inequality uses the fact that I(U ; V | W )  H(V | W )  H(V ) hold for any random
variable U  V  W   and the second inequality uses Shannon’s source coding theorem [13].
We will drop the subscript for the prior V d of ~✓ when it is clear from the context.
3 Main Results

3.1 High Dimensional Lower bound via Direct Sum

Our main theorem roughly states that if one can solves the d-dimensional problem  then one must
be able to solve the one dimensional problem with information cost and square loss reduced by a
factor of d. Therefore  a lower bound for one dimensional problem will imply a lower bound for
high dimensional problem  with information cost and square loss scaled up by a factor of d.
We ﬁrst deﬁne our task formally  and then state the theorem that relates d-dimensional task with
one-dimensional task.
ˆ~✓) solves task T (d  m  n  2 V d) with infor-
Deﬁnition 1. We say a protocol and estimator pair (⇧ 
mation cost C and mean-squared loss R  if for ~✓ randomly chosen from V d  m machines  each of
which takes n samples from N ( ~✓   2Id) as input  can run the protocol ⇧ and get transcript Y so
that the followings are true:

RV d((⇧ 

ˆ~✓)  ~✓ ) = R
(1)
IV d( ~X; Y | ~✓   Rpub) = C
(2)
ˆ~✓) solves the task T (d  m  n  2 V d) with information cost C
Theorem 3.1. [Direct-Sum] If (⇧ 
and squared loss R  then there exists (⇧0  ˆ✓) that solves the task T (1  m  n  2 V) with information
cost at most 4C/d and squared loss at most 4R/d. Furthermore  if the protocol ⇧ is simultaneous 
then the protocol ⇧0 is also simultaneous.
Remark 1. Note that this theorem doesn’t prove directly that communication cost scales linearly
with the dimension  but only information cost. However for many natural problems  communication
cost and information cost are similar for one dimension (e.g. for gaussian mean estimation) and then
this direct sum theorem can be applied. In this sense it is very generic tool and is widely used in
communication complexity and streaming algorithms literature.

Corollary 3.1. Suppose (⇧ 
squared loss R  and communication cost B. Then

ˆ~✓) estimates the mean of N ( ~✓   2Id)  for all ~✓ 2 [1  1]d  with mean-
R  ⌦✓min⇢ d22

nB log m

d2

  d◆
As a corollary  when 2  mn  to achieve the mean-squared loss R = d2
B is at least ⌦⇣ dm
log m⌘.

n log m

 

This lower bound is tight up to polylogarithmic factors. In most of the cases  roughly B/m machines
sending their sample mean to the fusion center and ˆ~✓ simply outputs the mean of the sample means
with O(log m) bits of precision will match the lower bound up to a multiplicative log2 m factor. 7
7When  is very large  when ✓ is known to be in [1  1]  ˆ~✓ = 0 is a better estimator  that is essentially why

the lower bounds not only have the ﬁrst term we desired but also the other two.

mn   the communication cost

5

3.2 Protocol for sparse estimation problem
In this section we consider the class of gaussian distributions with sparse mean: Ps =
{N ( ~✓   2Id) : | ~✓ |0  s  ~✓ 2 Rd}. We provide a protocol that exploits the sparse structure of
~✓ .

Inputs : Machine j gets samples X (j 1)  . . .   X (j n) distributed according to N ( ~✓   2Id)  where
~✓ 2 Rd with | ~✓ |0  s.
For each 1  j  m0 = (Lm log d)/↵  (where L is a sufﬁciently large constant)  machine j sends
its sample mean ¯X (j) = 1
Fusion center calculates the mean of the sample means ¯X = 1
Let ˆ~✓i =⇢ ¯Xi

nX (j 1)  . . .   X (j n) (with precision O(log m)) to the center.
m0⇣ ¯X (1) + ··· + ¯X (m0)⌘.

if | ¯Xi|2  ↵2
otherwise

mn

0

Outputs ˆ~✓

Protocol 1: Protocol for Ps

Theorem 3.2. For any P 2 Ps  for any d/s  ↵  1  Protocol 1 returns ~✓ with mean-squared loss
O( ↵s2

mn ) with communication cost O((dm log m log d)↵).

The proof of the theorem is deferred to supplementary material. Note that when ↵ = 1  we have
a protocol with ˜O(dm) communication cost and mean-squared loss O(s2/(mn))  and when ↵ =
d/s  the communication cost is ˜O(sm) but squared loss O(d2/(mn)). Comparing to the case
where we don’t have sparse structure  basically we either replace the d factor in the communication
cost by the intrinsic dimension s or the d factor in the squared loss by s  but not both.

3.3

Improved upper bound

The lower bound provided in Section 3.1 is only tight up to polylogarithmic factor. To achieve the
centralized minimax rate 2d
mn   the best existing upper bound of O(dm log(m)) bits of communica-
tion is achieved by the simple protocol that ask each machine to send its sample mean with O(log n)
bits precision . We improve the upper bound to O(dm) using the interactive protocols.
Recall that the class of unknown distributions of our model is P d = {N ( ~✓   2Id) : ✓ 2 [1  1]d}.
Theorem 3.3. Then there is an interactive protocol ⇧ with communication O(md) and an estimator
ˆ~✓ based on ⇧ which estimates ~✓ up to a squared loss of O( d2
mn ).

Remark 2. Our protocol is interactive but not simultaneous  and it is a very interesting question
whether the upper bound of O(dm) could be achieved by a simultaneous protocol.

3.4

Improved lower bound for simultaneous protocols

Although we are not able to prove ⌦(dm) lower bound for achieve the centralized minimax rate in
the interactive model  the lower bound for simultaneous case can be improved to ⌦(dm). Again  we
lowerbound the information cost for the one dimensional problem ﬁrst  and applying the direct-sum
theorem in Section 3.1  we got the d-dimensional lower bound.
Theorem 3.4. Suppose simultaneous protocol (⇧ 
~✓ 2 [1  1]d  with mean-squared loss R  and communication cost B  Then

ˆ~✓) estimates the mean of N ( ~✓   2Id)  for all

As a corollary  when 2  mn  to achieve mean-squared loss R = d2
is at least ⌦(dm).

mn   the communication cost B

R  ⌦✓min⇢ d22

nB

  d◆

6

4 Proof sketches

4.1 Proof sketch of theorem 3.1 and corollary 3.1

To prove a lower bound for the d dimensional problem using an existing lower bound for one dimen-
sional problem  we demonstrate a reduction that uses the (hypothetical) protocol ⇧ for d dimensions
to construct a protocol for the one dimensional problem.
For each ﬁxed coordinate i 2 [d]  we design a protocol ⇧i for the one-dimensional problem by
embedding the one-dimensional problem into the ith coordinate of the d-dimensional problem. We
will show essentially that if the machines ﬁrst collectively choose randomly a coordinate i  and run
protocol ⇧i for the one-dimensional problem  then the information cost and mean-squared loss of
this protocol will be only 1/d factor of those of the d-dimensional problem. Therefore  the informa-
tion cost of the d-dimensional problem is at least d times the information cost of one-dimensional
problem.

Inputs : Machine j gets samples X (j 1)  . . .   X (j n) distributed according to N (✓  2)  where ✓ ⇠ V.

1. All machines publicly sample ˘✓i distributed according to V d1.
2. Machine j privately samples ˘X (j 1)

i
  . . .   ˘X (j k)

  . . .   ˘X (j n)
i
i1   X (j k)  ˘X (j k)
i+1   . . .   ˘X (j k)

d

).

Let ˘X (j k) = ( ˘X (j k)

1

distributed according to N (˘✓i  2Id1).

3. All machines run protocol ⇧ on data ˘X and get transcript Yi. The estimator ˆ✓i is ˆ✓i(Yi) =

ˆ~✓(Y )i i.e. the ith coordinate of the d-dimensional estimator.

Protocol 2: ⇧i

In more detail  under protocol ⇧i (described formally in Protocol 2) the machines prepare a d-
dimensional dataset as follows: First they ﬁll the one-dimensional data that they got into the ith
coordinate of the d-dimensional data. Then the machines choose publicly randomly ~✓ i from distri-
bution V d1  and draw independently and privately gaussian random variables from N ( ~✓ i   Id1) 
and ﬁll the data into the other d  1 coordinates. Then machines then simply run the d-dimension
protocol ⇧ on this tailored dataset. Finally the estimator  denoted by ˆ✓i  outputs the ith coordinate
of the d-dimensional estimator ˆ~✓.
We are interested in the mean-squared loss and information cost of the protocol ⇧i’s that we just
designed. The following lemmas relate ⇧i’s with the original protocol ⇧.

Lemma 1. Protocols ⇧i’s satisfyPd
Lemma 2. Protocols ⇧i’s satisfyPd

i=1 RV⇣(⇧i  ˆ✓i)  ✓⌘ = RV d⇣(⇧ 
i=1 ICV (⇧i)  ICV d(⇧)

ˆ~✓)  ~✓⌘

Note that the counterpart of Lemma 2 with communication cost won’t be true  and actually the
communication cost of each ⇧i is the same as that of ⇧. It turns out doing reduction in communi-
cation cost is much harder  and this is part of the reason why we use information cost as a proxy for
communication cost when proving lower bound. Also note that the correctness of Lemma 2 heavily
relies on the fact that ⇧i draws the redundant data privately independently (see Section 2 and the
proof for more discussion on private versus public randomness).
By Lemma 1 and Lemma 2 and a Markov argument  there exists an i 2 {1  . . .   d} such that

R⇣(⇧i  ˆ✓i)  ✓⌘ 

4

d · R⇣(⇧  ~✓ )  ~✓⌘

and

IC(⇧i) 

4
d · IC(⇧)

Then the pair (⇧0  ˆ✓) = (⇧i  ˆ✓i) solves the task T (1  m  n  2 V) with information cost at most
4C/d and squared loss 4R/d  which proves Theorem 3.1.
Corollary 3.1 follows Theorem 3.1 and the following lower bound for one dimensional gaussian
mean estimation proved in [1]. We provide complete proofs in the supplementary.

7

⌘.
Theorem 4.1. [1] Let V be the uniform distribution over {±}  where 2  min⇣1  2 log(m)
If (⇧  ˆ✓) solves the task T (1  m  n  2 V) with information cost C and squared loss R  then either
C  ⌦⇣

2n log(m)⌘ or R  2/10.

2

n

4.2 Proof sketch of theorem 3.3

The protocol is described in protocol 3 in the supplementary. We only describe the d = 1 case 
while for general case we only need to run d protocols individually for each dimension.
The central idea is that we maintain an upper bound U and lower bound L for the target mean  and
iteratively ask the machines to send their sample means to shrink the interval [L  U ]. Initially we
only know that ✓ 2 [1  1]. Therefore we set the upper bound U and lower bound L for ✓ to be
1 and 1. In the ﬁrst iteration the machines try to determine whether ✓ < 0 or  0. This is done
by letting several machines (precisely  O(log m)/2 machines) send whether their sample means
are < 0 or  0. If the majority of the samples are < 0  ✓ is likely to be < 0. However when ✓
is very close to 0  one needs a lot of samples to determine this  but here we only ask O(log m)/2
machines to send their sample means. Therefore we should be more conservative and we only update
the interval in which ✓ might lie to [1  1/2] if the majority of samples are < 0.
We repeat this until the interval (L  U ) become smaller than our target squared loss. Each round 
we ask a number of new machines sending 1 bits of information about whether their sample mean
is large than (U + L)/2. The number of machines participated is carefully set so that the failure
probability p is small. An interesting feature of the protocol is to choose the target error probabil-
ity p differently at each iteration so that we have a better balance between the failure probability
and communication cost. The complete the description of the protocol and proof are given in the
supplementary.

4.3 Proof sketch of theorem 3.4
We use a different prior on the mean N (0  2) instead of uniform over {  } used by [1]. Gaussian
prior allows us to use a strong data processing inequality for jointly gaussian random variables by
[14]. Since we don’t have to truncate the gaussian  we don’t lose the factor of log(m) lost by [1].
Theorem 4.2. ([14]  Theorem 7) Suppose X and V are jointly gaussian random variables with
correlation ⇢. Let Y $ X $ V be a markov chain with I(Y ; X)  R. Then I(Y ; V )  ⇢2R.
Now suppose that each machine gets n samples X 1  . . .   X n ⇠ N (V  2)  where V is the prior
N (0  2) on the mean. By an application of theorem 4.2  we prove that if Y is a B-bit message
depending on X 1  . . .   X n  then Y has only n2
· B bits of information about V . Using some
2
standard information theory arguments  this converts into the statement that if Y is the transcript of
a simultaneous protocol with communication cost  B  then it has at most n2
2 ·B bits of information
about V . Then a lower bound on the communication cost B of a simultaneous protocol estimating
the mean ✓ 2 [1  1] follows from proving that such a protocol must have ⌦(1) bit of information
about V . Complete proof is given in the supplementary.

5 Conclusion

We have lowerbounded the communication cost of estimating the mean of a d-dimensional spherical
gaussian random variables in a distributed fashion. We provided a generic tool called direct-sum for
relating the information cost of d-dimensional problem to one-dimensional problem  which might
be of potential use for other statistical problem than gaussian mean estimation as well.
We also initiated the study of distributed estimation of gaussian mean with sparse structure. We
provide a simple protocol that exploits the sparse structure and conjecture its tradeoff to be optimal:
Conjecture 1. If some protocol estimates the mean for any distribution P 2 Ps with mean-squared
loss R and communication cost C  then C · R & sd2
mn   where we use & to hide log factors and
potential corner cases.

8

References
[1] Yuchen Zhang  John C. Duchi  Michael I. Jordan  and Martin J. Wainwright.

Information-
theoretic lower bounds for distributed statistical estimation with communication constraints.
In NIPS  pages 2328–2336  2013.

[2] Maria-Florina Balcan  Avrim Blum  Shai Fine  and Yishay Mansour. Distributed learning 

communication complexity and privacy. In COLT  pages 26.1–26.22  2012.

[3] Hal Daum´e III  Jeff M. Phillips  Avishek Saha  and Suresh Venkatasubramanian. Protocols for

learning classiﬁers on distributed data. In AISTATS  pages 282–290  2012.

[4] Hal Daum´e III  Jeff M. Phillips  Avishek Saha  and Suresh Venkatasubramanian. Efﬁcient

protocols for distributed classiﬁcation and optimization. In ALT  pages 154–168  2012.

[5] John C. Duchi  Michael I. Jordan  Martin J. Wainwright  and Yuchen Zhang.

Information-
theoretic lower bounds for distributed statistical estimation with communication constraints.
CoRR  abs/1405.0782  2014.

[6] Amit Chakrabarti  Yaoyun Shi  Anthony Wirth  and Andrew Chi-Chih Yao.

Informational
complexity and the direct sum problem for simultaneous message complexity. In FOCS  pages
270–278  2001.

[7] Ziv Bar-Yossef  T. S. Jayram  Ravi Kumar  and D. Sivakumar. An information statistics ap-

proach to data stream and communication complexity. J. Comput. Syst. Sci.  68(4)  2004.

[8] Mark Braverman and Anup Rao. Information equals amortized communication. In FOCS 

pages 748–757  2011.

[9] Boaz Barak  Mark Braverman  Xi Chen  and Anup Rao. How to compress interactive commu-

nication. SIAM J. Comput.  42(3):1327–1363  2013.

[10] Mark Braverman  Faith Ellen  Rotem Oshman  Toniann Pitassi  and Vinod Vaikuntanathan. A
tight bound for set disjointness in the message-passing model. In FOCS  pages 668–677  2013.
[11] Anat Ganor  Gillat Kol  and Ran Raz. Exponential separation of information and communica-

tion. Electronic Colloquium on Computational Complexity (ECCC)  21:49  2014.

[12] Yuchen Zhang  John C. Duchi  and Martin J. Wainwright. Communication-efﬁcient algorithms
for statistical optimization. Journal of Machine Learning Research  14(1):3321–3363  2013.
[13] Claude Shannon. A mathematical theory of communication. Bell System Technical Journal 

27:379–423  623–656  1948.

[14] Elza Erkip and Thomas M. Cover. The efﬁciency of investment information.

Inform. Theory  44  1998.

IEEE Trans.

9

,Ankit Garg
Tengyu Ma
Huy Nguyen