2019,Learning Multiple Markov Chains via Adaptive Allocation,We study the problem of learning the transition matrices of a set of Markov chains from a single stream of observations on each chain. We assume that the Markov chains are ergodic but otherwise unknown. The learner can sample Markov chains sequentially to observe their states. The goal of the learner is to sequentially select various chains to learn transition matrices uniformly well with respect to some loss function. We introduce a notion of loss that naturally extends the squared loss for learning distributions to the case of Markov chains  and further characterize the notion of being \emph{uniformly good} in all problem instances. We present a novel learning algorithm that efficiently balances \emph{exploration} and \emph{exploitation} intrinsic to this problem  without any prior knowledge of the chains. We provide finite-sample PAC-type guarantees on the performance of the algorithm. Further  we show that our algorithm asymptotically attains an optimal loss.,Learning Multiple Markov Chains

via Adaptive Allocation

Mohammad Sadegh Talebi

SequeL Team  Inria Lille – Nord Europe

sadegh.talebi@inria.fr

Odalric-Ambrym Maillard

SequeL Team  Inria Lille – Nord Europe

odalric.maillard@inria.fr

Abstract

We study the problem of learning the transition matrices of a set of Markov chains
from a single stream of observations on each chain. We assume that the Markov
chains are ergodic but otherwise unknown. The learner can sample Markov chains
sequentially to observe their states. The goal of the learner is to sequentially select
various chains to learn transition matrices uniformly well with respect to some
loss function. We introduce a notion of loss that naturally extends the squared loss
for learning distributions to the case of Markov chains  and further characterize
the notion of being uniformly good in all problem instances. We present a novel
learning algorithm that efﬁciently balances exploration and exploitation intrinsic to
this problem  without any prior knowledge of the chains. We provide ﬁnite-sample
PAC-type guarantees on the performance of the algorithm. Further  we show that
our algorithm asymptotically attains an optimal loss.

1

Introduction

t=1

empirical mean estimate ˆµk n built with the Tk n =(cid:80)n

We study a variant of the following sequential adaptive allocation problem: A learner is given a
set of K arms  where to each arm k∈ [K]  an unknown real-valued distributions νk with mean µk
k > 0 is associated. At each round t ∈ N  the learner must select an arm kt ∈ [K] 
and variance σ2
and receives a sample drawn from νk. Given a total budget of n pulls  the objective is to estimate
the expected values (µk)k∈[K] of all distributions uniformly well. The quality of estimation in this
problem is classically measured through the expected quadratic estimation error E[(µk−ˆµk n)2] for the
I{k = kt} many samples received from νk at
time n  and the performance of an allocation strategy is the maximal error  maxk∈[K] E[(µk−ˆµk n)2].
Using ideas from the Multi-Armed Bandit (MAB) literature  previous works (e.g.  [1  2]) have
provided optimistic sampling strategies with near-optimal performance guarantees for this setup.
This generic adaptive allocation problem is related to several application problems arising in optimal
experiment design [3  4]  active learning [5]  or Monte-Carlo methods [6]; we refer to [1  7  2  8] and
references therein for further motivation. We extend this line of work to the case where each process
is a discrete Markov chain  hence introducing the problem of active bandit learning of Markov chains.
More precisely  we no longer assume that (νk)k are real-valued distributions  but we study the case
where each νk is a discrete Markov process over a state space S ⊂ N. The law of the observations
i=2 Pk(Xk i−1  Xk i) 
where pk denotes the initial distribution of states  and Pk is the transition function of the Markov
chain. The goal of the learner is to learn the transition matrices (Pk)k∈[K] uniformly well on the
chains. Note that the chains are not controlled (we only decide which chain to advance  not the states
it transits to).
Before discussing the challenges of the extension to Markov chains  let us give further comments
on the performance measure considered in bandit allocation for real-valued distributions: Using
the expected quadratic estimation error on each arm k makes sense since when Tk n  k ∈ [K] are

(Xk i)i∈N on arm (or chain) k is given by νk(Xk 1  . . . Xk n) = pk(Xk 1)(cid:81)n

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

deterministic  it coincides with σ2
k/Tk n  thus suggesting to pull arm k proportionally to its variance
k. However  for a learning strategy  Tk n typically depends on all past observations. The presented
σ2
analyses in these series of works rely on Wald’s second identity as the technical device  heavily
relying on the use of a quadratic loss criterion  which prevents one from extending the approach
therein to other distances. Another peculiarity arising in working with expectations is the order of
“max” and “expectation” operators. While it makes more sense to control the expected value of the
maximum  the works cited above look at maximum of the expected value  which is more in line with
a pseudo-loss deﬁnition rather than the loss; actually in extensions of these works a pseudo-loss is
considered instead of this performance measure. As we show  all of these difﬁculties can be avoided
by resorting to a high probability setup. Hence  in this paper  we deviate from using an expected loss
criterion  and rather use a high-probability control. We formally deﬁne our performance criterion in
Section 2.3.

1.1 Related Work

On the one hand  our setup can be framed into the line of works on active bandit allocation  considered
for the estimation of reward distributions in MABs as introduced in [1  7]  and further studied in [2  9].
This has been extended to stratiﬁed sampling for Monte-Carlo methods in [10  8]  or to continuous
mean functions in  e.g.  [11]. On the other hand  our extension from real-valued distributions
to Markov chains can be framed into the rich literature on Markov chain estimation; see  e.g. 
[12  13  14  15  16  17]. This stream of works extends a wide range of results from the i.i.d. case to
the Markov case. These include  for instance  the law of large numbers for (functions of) state values
[17]  the central limit theorem for Markov sequences [13] (see also [17  18])  and Chernoff-type or
Bernstein-type concentration inequalities for Markov sequences [19  20]. Note that the majority of
these results are available for ergodic Markov chains.
Another stream of research on Markov chains  which is more relevant to our work  investigates
learning and estimation of the transition matrix (as opposed to its full law); see  e.g.  [16  15  21  22].
Among the recent studies falling into this category  [22] investigates learning of the transition matrix
with respect to a loss function induced by f-divergences in a minimax setup  thus extending [23] to
the case of Markov chains. [21] derives a Probably Approximately Correct (PAC) type bound for
learning the transition matrix of an ergodic Markov chain with respect to the total variation loss. It
further provides a matching lower bound. Among the existing literature on learning Markov chains 
to the best of our knowledge  [21] is the closest to ours. There are however two aspects distinguishing
our work: Firstly  the challenge in our problem resides in dealing with multiple Markov chains  which
is present neither in [21] nor in the other studies cited above. Secondly  our notion of loss does not
coincide with that considered in [21]  and hence  the lower bound of [21] does not apply to our case.
Among the results dealing with multiple chains  we may refer to learning in the Markovian bandits
setup [24  25  26]. Most of these studies address the problem of reward maximization over a ﬁnite
time horizon. We also mention that in a recent study  [27] introduces the so-called active exploration
in Markov decision processes  where the transition kernel is known  and the goal is rather to learn the
mean reward associated to various states. To the best of our knowledge  none of these works address
the problem of learning the transition matrix. Last  as we target high-probability performance bounds
(as opposed to those holding in expectation)  our approach is naturally linked to the PAC analysis.
[28] provides one of the ﬁrst PAC bounds for learning discrete distributions. Since then  the problem
of learning discrete distributions has been well studied; see  e.g.  [29  30  23] and references therein.
We refer to [23] for a rather complete characterization of learning distribution in a minimax setting
under a big class of smooth loss functions. We remark that except for very few studies (e.g.  [29]) 
most of these results are provided for discrete distributions.

1.2 Overview and Contributions

Our contributions are the following: (i) For the problem of learning Markov chains  we consider
a notion of loss function  which appropriately extends the loss function for learning distributions
to the case of Markov chains. Our notion of loss is similar to that considered in [22] (we refer
to Section 2.3 for a comparison between our notion and the one in [22]). In contrast to existing
works on similar bandit allocation problems  our loss function avoids technical difﬁculties faced
when extending the squared loss function to this setup. We further characterize the notion of a
“uniformly good algorithm” under the considered loss function for ergodic chains; (ii) We present
an optimistic algorithm  called BA-MC  for active learning of Markov chains  which is simple to

2

k

n + C0

n )  where (cid:101)O(·) hides

implement and does not require any prior knowledge of the chains. To the best of our knowledge  this
constitutes the ﬁrst algorithm for active bandit allocation for learning Markov chains; (iii) We provide
non-asymptotic PAC-type bounds as well as an asymptotic one on the loss incurred by BA-MC 
indicating three regimes. In the ﬁrst regime  which holds for any learning budget n ≥ 4K  we

present (in Theorem 1) a high-probability bound on the loss scaling as (cid:101)O( KS2
cut-off budget ncutoff (in Theorem 2) so that when n ≥ ncutoff  the loss behaves as (cid:101)O( Λ
where Λ =(cid:80)

log(log(n)) factors. Here  K and S respectively denote the number of chains and the number of
(cid:80)
states in a given chain. This result holds for homogenous Markov chains. We then characterize a
n3/2 ) 
x y Pk(x  y)(1− Pk(x  y)) denotes the sum of variances of all states and all chains 
and where Pk denotes the transition probability of chain k. This latter bound constitutes the second
regime  in view of the fact that Λ
n equals the asymptotically optimal loss (see Section 2.4 for more
details). Thus  this bound indicates that the pseudo-excess loss incurred by the algorithm vanishes
at a rate C0n−3/2 (we refer to Section 4 for a more precise deﬁnition). Furthermore  we carefully
characterize the constant C0. In particular  we discuss that C0 does not deteriorate with mixing times
of the chains  which  we believe  is a strong feature of our algorithm. We also discuss how various
properties of the chains  e.g.  discrepancies between stationary distribution of various states of a given
chain  may impact the learning performance. Finally  we demonstrate a third regime  the asymptotic
one  when the budget n grows large  in which we show (in Theorem 3) that the loss of BA-MC
matches the asymptotically optimal loss Λ
n . All proofs are provided in the supplementary material.
Markov chains have been successfully used for modeling a broad range of practical problems  and
their success makes the studied problem in this paper relevant in practice. There are practical
applications in reinforcement learning (e.g.  active exploration in MDPs [27]) and in rested Markov
bandits (e.g.  channel allocation in wireless communication systems where a given channel’s state
follows a Markov chain1)  for which we believe our contributions could serve as a technical tool.

2 Preliminaries and Problem Statement

2.1 Preliminaries

Before describing our model  we recall some preliminaries on Markov chains; these are standard
deﬁnitions and results  and can be found in  e.g.  [32  33]. Consider a Markov chain deﬁned on a
ﬁnite state space S with cardinality S. Let PS denote the collection of all row-stochastic matrices
over S. The Markov chain is speciﬁed by its transition matrix P ∈ PS and its initial distribution p:
For all x  y ∈ S  P (x  y) denotes the probability of transition to y if the current state is x. In what
follows  we may refer to a chain by just referring to its transition matrix.
We recall that a Markov chain P is ergodic if P m > 0 (entry-wise) for some m ∈ N. If P is ergodic 
then it has a unique stationary distribution π satisfying π = πP . Moreover π := minx∈S π(x) > 0.
A chain P is said to be reversible if its stationary distribution π satisﬁes detailed balance equations:
For all x  y ∈ S  π(x)P (x  y) = π(y)P (y  x). Otherwise  P is called non-reversible. For a Markov
chain P   the largest eigenvalue is λ1(P ) = 1 (with multiplicity one). In a reversible chain P  
all eigenvalues belong to (−1  1]. We deﬁne the absolute spectral gap of a reversible chain P as
γ(P ) = 1 − λ(cid:63)(P )  where λ(cid:63)(P ) denotes the second largest (in absolute value) eigenvalue of
P . If P is reversible  the absolute spectral gap γ(P ) controls the convergence rate of the state
distributions of the chain towards the stationary distribution π. If P is non-reversible  the convergence
rate is determined by the pseudo-spectral gap as introduced in [20] as follows. Deﬁne P (cid:63) as:
P (cid:63)(x  y) := π(y)P (y  x)/π(x) for all x  y ∈ S. Then  the pseudo-spectral gap γps(P ) is deﬁned as:
γps(P ) := max(cid:96)≥1

γ((P (cid:63))(cid:96)P (cid:96))

.

(cid:96)

2.2 Model and Problem Statement

We are now ready to describe our model. We consider a learner interacting with a ﬁnite set of Markov
chains indexed by k ∈ [K] := {1  2  . . .   K}. For ease of presentation  we assume that all Markov
chains are deﬁned on the same state space2 S with cardinality S. The Markov chain k  or for short

1For example  in the Gilbert-Elliott channels [31].
2Our algorithm and results are straightforwardly extended to the case where the Markov chains are deﬁned

on different state spaces.

3

chain k  is speciﬁed by its transition matrix Pk ∈ PS. In this work  we assume that all Markov chains
are ergodic  which implies that any chain k admits a unique stationary distribution  which we denote
by πk. Moreover  the minimal element of πk is bounded away from zero: πk := minx∈S πk(x) > 0.
The initial distributions of the chains are assumed to be arbitrary. Further  we let γk := γ(Pk) denote
the absolute spectral gap of chain k if k is reversible; otherwise  we deﬁne the pseudo-spectral gap of
k by γps k := γps(Pk).
A related quantity in our results is the Gini index of the various states. For a chain k  the Gini index
for state x ∈ S is deﬁned as

Gk(x) :=

Pk(x  y)(1 − Pk(x  y)).

(cid:88)

y∈S

Note that Gk(x) ≤ 1− 1
achieved when Pk(x  y) = 1

S for all y ∈ S (in view of the concavity of z (cid:55)→(cid:80)

In this work  we assume that for all k (cid:80)
sum (over states) of inverse stationary distributions: For a chain k  we deﬁne Hk :=(cid:80)

S . This upper bound is veriﬁed by the fact that the maximal value of Gk(x) is
x∈S z(x)(1 − z(x))).
x∈S Gk(x) > 0.3 Another related quantity in our results is the
x∈S πk(x)−1.
k . The quantity Hk reﬂects the discrepancy between individual elements

Note that S2 ≤ Hk ≤ Sπ−1
of πk.

The online learning problem. The learner wishes to design a sequential allocation strategy to
adaptively sample various Markov chains so that all transition matrices are learnt uniformly well. The
game proceeds as follows: Initially all chains are assumed to be non-stationary with arbitrary initial
distributions chosen by the environment. At each step t ≥ 1  the learner samples a chain kt  based
on the past decisions and the observed states  and observes the state Xkt t. The state of kt evolves
according to Pkt. The state of chains k (cid:54)= kt does not change: Xk t = Xk t−1 for all k (cid:54)= kt.
We introduce the following notations: Let Tk t denote the number of times chain k is selected by the
I{kt(cid:48) = k}  where I{·} denotes the indicator function. Likewise 
we let Tk x t represent the number of observations of chain k  up to time t  when the chain was in state
I{kt(cid:48) = k  Xk t(cid:48) = x}. Further  we note that the learner only controls Tk t (or
t(cid:48)=1
x Tk x t)  but not the number of visits to individual states. At each step t  the learner
maintains empirical estimates of the stationary distributions  and estimates transition probabilities
of various chains based on the observations gathered up to t. We deﬁne the empirical stationary
distribution of chain k at time t as ˆπk t(x) := Tk x t/Tk t for all x ∈ S. For chain k  we maintain the
following smoothed estimation of transition probabilities:

learner up to time t: Tk t :=(cid:80)t
x: Tk x t :=(cid:80)t
equivalently (cid:80)

t(cid:48)=1

t(cid:48)=2

I{Xk t(cid:48)−1 = x  Xk t(cid:48) = y}
αS + Tk x t

 

∀x  y ∈ S 

(1)

α +(cid:80)t

(cid:98)Pk t(x  y) :=

where α is a positive constant. In the literature  the case of α = 1
S is usually referred to as the
Laplace-smoothed estimator. The learner is given a budget of n samples  and her goal is to obtain
an accurate estimation of transition matrices of the Markov chains. The accuracy of the estimation
is determined by some notion of loss  which will be discussed later. The learner adaptively selects
various chains so that the minimal loss is achieved.

2.3 Performance Measures

We are now ready to provide a precise deﬁnition of our notion of loss  which would serve as the
performance measure of a given algorithm. Given n ∈ N  we deﬁne the loss of an adaptive algorithm
A as:

(cid:88)

x∈S

ˆπk n(x)(cid:107)Pk(x ·) − (cid:98)Pk n(x ·)(cid:107)2

2 .

Ln(A) := max
k∈[K]

Lk n  with Lk n :=

The use of the L2-norm in the deﬁnition of loss is quite natural in the context of learning and
estimation of distributions  as it is directly inspired by the quadratic estimation error used in active

3We remark that there exist chains with(cid:80)

x Gk(x) = 0. In view of the deﬁnition of the Gini index  such
chains are necessarily deterministic (or degenerate)  namely their transition matrices belong to {0  1}S×S. One
example is a deterministic cycle with S nodes. We note that such chains may fail to satisfy irreducibility or
aperiodicity.

4

bandit allocation (e.g.  [2]). Given a budget n  the loss Ln(A) of an adaptive algorithm A is a random
variable  due to the evolution of the various chains as well as the possible randomization in the
algorithm. Here  we aim at controlling this random quantity in a high probability setup as follows:
Let δ ∈ (0  1). For a given algorithm A  we wish to ﬁnd ε := ε(n  δ) such that

P (Ln(A) ≥ ε) ≤ δ .

(2)

Remark 1 We remark that the empirical stationary distribution ˆπk t may differ from the stationary

distribution associated to the smoothed estimator (cid:98)Pk t of the transition matrix. Our algorithm and
results  however  do not rely on possible relations between ˆπk t and (cid:98)Pk t  though one could have used

smoothed estimators for πk. The motivation behind using empirical estimate ˆπk t of πk in Ln is that
it naturally corresponds to the occupancy of various states according to a given sample path.

(cid:80)
x∈S (cid:107)Pk(x ·) − (cid:98)Pk n(x ·)(cid:107)2

(cid:80)
xπk(x)(cid:107)Pk(x ·)−(cid:98)Pk n(x ·)(cid:107)2

Comparison with other losses. We now turn our attention to the comparison between our loss
n(A) =
function and some other possible notions. First  we compare ours to the loss function L(cid:48)
2. Such a notion of loss might look more natural or simpler 
maxk
since the weights ˆπk n(x) are replaced simply with 1 (equivalently  uniform weights). However  this
means a strategy may incur a high loss for a part of the state space that is rarely visited  even though
we have absolutely no control on the chain. For instance  in the extreme case when some states x
are reachable with a very small probability  Tk x n may be arbitrarily small thus resulting in a large
loss L(cid:48)
n for all algorithms  while it makes little sense to penalize an allocation strategy for these
“virtual" states. Weighting the loss according to the empirical frequency ˆπk n of visits avoids such a
phenomenon  and is thus more meaningful.
In view of the above discussion  it is also tempting to replace the empirical state distribution
n(A) =
ˆπk n with its expectation πk  namely to deﬁne a pseudo-loss function of the form L(cid:48)(cid:48)
2 (as studied in  e.g.  [22] in a different setup). We recall
maxk
that our aim is to derive performance guarantees on the algorithm’s loss that hold with high probabil-
ity (for 1− δ portions of the sample paths of the algorithm for a given δ). To this end  Ln (which uses
ˆπk n) is more natural and meaningful than L(cid:48)(cid:48)
n as Ln penalizes the algorithm’s performance by the
relative visit counts of various states in a given sample path (through ˆπk n)  and not by the expected
value of these. This matters a lot in the small-budget regime  where ˆπk n could differ signiﬁcantly
from πk — Otherwise when n is large enough  ˆπk n becomes well-concentrated around πk with high
probability. To clarify further  let us consider the small-budget regime  and some state x where πk(x)
is not small. In the case of Ln  using ˆπk n we penalize the performance by the mismatch between
visited x. In contrast  in the case of L(cid:48)(cid:48)
n  weighting the mismatch proportionally to πk(x) does not
seem reasonable since in a given sample path  the algorithm might not have visited x enough even
though πk(x) is not small. We remark that our results in subsequent sections easily apply to the
pseudo-loss L(cid:48)(cid:48)
n  at the expense of an additive second-order term  which might depend on the mixing
times.
Finally  we position the high-probability guarantee on Ln  in the sense of Eq. (2)  against those
holding in expectation. Prior studies on bandit allocation  such as [7  2]  whose objectives involve a
max operator  consider expected squared distance. The presented analyses in these series of works
rely on Wald’s second identity as the technical device. This prevents one to extend the approach
therein to other distances. Another peculiarity arising in working with expectations is the order of
“max” and “expectation” operators. While it makes more sense to control the expected value of the
maximum  the works cited above look at maximum of the expected value  which is more in line with a
pseudo-loss deﬁnition rather than the loss. All of these difﬁculties can be avoided by resorting to a
high probability setup (in the sense of Eq. (2).

(cid:98)Pk n(x ·) and Pk(x ·)  weighted proportionally to the number of rounds the algorithm has actually

Further intuition and example. We now provide an illustrative example to further clarify some
of the above comments. Let us consider the following two-state Markov chain: P =
 
where ε ∈ (0  1). The stationary distribution of this Markov chain is π = [ ε
2+ε ]. Let s1 (resp. s2)
denote the state corresponding to the ﬁrst (resp. second) row of the transition matrix. In view of π 
when ε (cid:28) 1  the chain tends to stay in s2 (the lazy state) most of the time: Out of n observations 
one gets on average only nπ(s1) = nε/(2 + ε) observations of state s1  which means  for ε (cid:28) 1/n 
essentially no observation of state s1. Hence  no algorithm can estimate the transitions from s1 in

1/2
1 − ε

2+ε   2

(cid:20)1/2

ε

(cid:21)

5

such a setup  and all strategies would suffer a huge loss according to L(cid:48)
n  no matter how samples are
allocated to this chain. Thus  L(cid:48)
n has limited interest in order to distinguish between good and base
sampling strategies. On the other hand  using Ln enables to better distinguish between allocation
strategies  since the weight given to s1 would be essentially 0 in this case  thus focusing on the good
estimation of s2 (and other chains) only.

2.4 Static Allocation

The proof of this lemma consists in two steps: First  we provide lower and upper bounds on Lk n in

In this subsection  we investigate the optimal loss asymptotically achievable by an oracle policy that
is aware of some properties of the chains. To this aim  let us consider a non-adaptive strategy where
sampling of various chains is deterministic. Therefore  Tk n  k = 1  . . .   K are not random. The
following lemma is a consequence of the central limit theorem:

Lemma 1 We have for any chain k: Tk nLk n →Tk n→∞(cid:80)
terms of the loss(cid:101)Lk n incurred by the learner had she used an empirical estimator (corresponding to
α = 0 in (1)). Second  we show that by the central limit theorem  Tk n(cid:101)Lk n →Tk n→∞(cid:80)
Now  consider an oracle policy Aoracle  who is aware of(cid:80)
of the above discussion  and taking into account the constraint(cid:80)
(cid:88)
(cid:88)

x Gk(x).
x∈S Gk(x) for various chains. In view
k∈[K] Tk n = n  it would be

asymptotically optimal to allocate Tk n = ηkn samples to chain k  where

(cid:88)

x Gk(x) .

ηk :=

Gk(x)   with Λ :=

Gk(x) .

1
Λ

x∈S

k∈[K]

x∈S

The corresponding loss would satisfy: nLn(Aoracle) →n→∞ Λ . We shall refer to the quantity Λ
n as
the asymptotically optimal loss  which is a problem-dependent quantity. The coefﬁcients ηk  k ∈ [K]
characterize the discrepancy between the transition matrices of the various chains  and indicate that
an algorithm needs to account for such discrepancy in order to achieve the asymptotically optimal
loss. Having characterized the notion of asymptotically optimal loss  we are now ready to deﬁne the
notion of uniformly good algorithm:
Deﬁnition 1 (Uniformly Good Algorithm) An algorithm A is said to be uniformly good if  for
any problem instance  it achieves the asymptotically optimal loss when n grows large; that is 
limn→∞ nLn(A) = Λ for all problem instances.

3 The BA-MC Algorithm

In this section  we introduce an algorithm designed for adaptive bandit allocation of a set of Markov
chains. It is designed based on the optimistic principle  as in MAB problems (e.g.  [34  35])  and
relies on an index function. More precisely  at each time t  the algorithm maintains an index function
bk t+1 for each chain k  which provides an upper conﬁdence bound (UCB) on the loss incurred by k
2 
chain kt ∈ argmaxk∈[K] bk t+1 at time t  we can balance exploration and exploitation by selecting
more the chains with higher estimated losses or those with higher uncertainty in these estimates.
In order to specify the index function bk ·  let us choose α = 1

at t; more precisely  with high probability  bk t+1 ≥ Lk t :=(cid:80)
x∈S ˆπk t(x)(cid:107)Pk(x ·) − (cid:98)Pk t(x ·)(cid:107)2
where (cid:98)Pk t denotes the smoothed estimate of Pk with some α > 0 (see Eq. (1)). Now  by sampling a
later on)  and for each state x ∈ S  deﬁne the estimate of Gini coefﬁcient at time t as (cid:98)Gk t(x) :=
y∈S (cid:98)Pk t(x  y)(cid:0)1 − (cid:98)Pk t(x  y)(cid:1). The index bk t+1 is then deﬁned as
(cid:80)
(cid:88)

3S (we motivate this choice of α

bk t+1 =

2β
Tk t

6.6β3/2

+

(cid:88)
I{Tk x t > 0}(cid:98)Gk t(x) +
(cid:88)
(cid:88)

T 3/2
k x t

x∈S

Tk t

x∈S

(Tk x t + αS)2

y∈S

6

28β2S
Tk t

I{Tk x t > 0}
Tk x t + αS

x∈S

(cid:113)(cid:98)Pk t(x  y)(cid:0)1 − (cid:98)Pk t(x  y)(cid:1)  

(cid:16)(cid:108) log(n)

(cid:109) 6KS2

(cid:17)

δ

log(c)

  with c > 1 being an arbitrary choice. In this paper 

where β := β(n  δ) := c log
we choose c = 1.1.
We remark that the design of the index bk · above comes from the application of empirical Bernstein
concentration for α-smoothed estimators (see Lemma 4 in the supplementary) to the loss function Lk t.
In other words  Lemma 4 guarantees that with high probability  bk t+1 ≥ Lk t. Our concentration
inequality (Lemma 4) is new  to our knowledge  and could be of independent interest.
Having deﬁned the index function bk ·  we are now ready to describe our algorithm  which we call
BA-MC (Bandit Allocation for Markov Chains). BA-MC receives as input a conﬁdence parameter δ 
a budget n  as well as the state space S. It initially samples each chain twice (hence  this phase lasts
for 2K rounds). Then  BA-MC simply consists in sampling the chain with the largest index bk t+1

at each round t. Finally  it returns  after n pulls  an estimate (cid:98)Pk n for each chain k. We provide the

pseudo-code of BA-MC in Algorithm 1. Note that BA-MC does not require any prior knowledge of
the chains (neither the initial distribution nor the mixing time).

Algorithm 1 BA-MC – Bandit Allocation for Markov Chains

Input: Conﬁdence parameter δ  budget n  state space S;
Initialize: Sample each chain twice;
for t = 2K + 1  . . .   n do
Sample chain kt ∈ argmaxk bk t+1;
Observe Xk t  and update Tk x t and Tk t;

end for

(cid:80)

In order to provide more insights into the design of BA-MC  let us remark that (as shown in Lemma 8
in the supplementary) bk t+1 provides a high-probability UCB on the quantity 1
x Gk(x) as well.
Tk t
Now by sampling the chain kt ∈ argmaxk∈[K] bk t+1 at time t  in view of discussions in Section 2.4 

BA-MC would try to mimic an oracle algorithm being aware of(cid:80)

x Gk(x) for various chains.

We remark that our concentration inequality in Lemma 4 (of the supplementary) parallels the one
presented in Lemma 8.3 in [36]. In contrast  our concentration lemma makes appear the terms
Tk x t + αS in the denominator  whereas Lemma 8.3 in [36] makes appear terms Tk x t in the
denominator. This feature plays an important role to deal with situations where some states are not
sampled up to time t  that is for when Tk x t = 0 for some x.

4 Performance Bounds

We are now ready to study the performance bounds on the loss Ln(BA-MC) in both asymptotic and
non-asymptotic regimes. We begin with a generic non-asymptotic bound as follows:
Theorem 1 (BA-MC  Generic Performance) Let δ ∈ (0  1). Then  for any budget n ≥ 4K  with
probability at least 1 − δ  the loss under A = BA-MC satisﬁes

Ln(A) ≤ 287KS2β2

n

+ (cid:101)O(cid:16) K 2S2

(cid:17)

.

n2

The proof of this theorem  provided in Section C in the supplementary  reveals the motivation to
choose α = 1
3S : It veriﬁes that to minimize the dependency of the loss on S  on must choose
α ∝ S−1. In particular  the proof does not rely on the ergodicity assumption:
Remark 2 Theorem 1 is valid even if the Markov chains Pk  k ∈ [K] are reducible or periodic.
In the following theorem  we state another non-asymptotic bound on the performance of BA-MC 
which reﬁnes Theorem 1 for when n ≥ ncutoff  where

ncutoff := ncutoff(δ) := K max

log

where γ(cid:48)

tation Λ :=(cid:80)

k

(cid:80)

and πk := minx∈S πk(x) > 0.

k = γk if k is reversible  and γ(cid:48)

x Gk(x)  and that for any chain k  ηk = 1

k = γps k otherwise. To present the theorem  we recall the no-
x∈S πk(x)−1 

Λ

(cid:18) 300

γ(cid:48)
kπk

k

(cid:17)(cid:19)2

(cid:113)
(cid:16) 2KS
(cid:80)
x∈S Gk(x)  Hk :=(cid:80)

π−1

δ

k

 

7

Theorem 2 Let δ ∈ (0  1)  and assume that n ≥ ncutoff. Then  with probability at least 1 − 2δ 

where C0 := 150K

√

Ln(A) ≤ 2βΛ
n

+

C0β3/2
n3/2

√

SΛ maxk Hk + 3

SΛ maxk

Hk
ηk

.

+ (cid:101)O(n−2)  

√

√

SΛ maxk

Hk
ηk

SΛ maxk Hk +

Recalling the asymptotic loss of the oracle algorithm discussed in Section 2.4 being equal to Λ/n  in
view of the Bernstein concentration  the oracle would incur a loss at most 2βΛ
n for when the budget n
n as the pseudo-excess loss of A (we
is ﬁnite. In this regard  we may look at the quantity Ln(A) − 2βΛ
under BA-MC vanishes at a rate (cid:101)O(n−3/2). In particular  Theorem 2 characterizes the constant C0
refrain from calling this quantity the excess loss as 2βΛ
n is not equal to the high-probability loss of the
oracle). Theorem 2 implies that when n is greater than the cut-off budget ncutoff  the pseudo-excess loss
controlling the main term of the pseudo-excess loss: C0 = O(K
the discrepancy among the(cid:80)
).
This further indicates that the pseudo-excess loss is controlled by the quantity Hk
  which captures (i)
ηk
x Gk(x) values of various chains k  and (ii) the discrepancy between
various stationary probabilities πk(x)  x ∈ S. We emphasize that the dependency of the learning
performance (through C0) on Hk is in alignment with the result obtained in [21] for the estimation of
a single ergodic Markov chain.
The proof of Theorem 2  provided in Section D in the supplementary  shows that to determine the
cut-off budget ncutoff  one needs to determine the value of n such that with high probability  for any
chain k and state x  the term Tk n(Tk x n + αS)−1 approaches πk(x)−1  which is further controlled
by γps k (or γk if k is reversible) as well as the minimal stationary distribution πk. This in turn
allows us to show that  under BA-MC  the number Tk n of samples for any chain k comes close to
the quantity ηkn. Finally  we remark that the proof of Theorem 2 also reveals that the result in the
theorem is indeed valid for any constant α > 0.
In the following theorem  we characterize the asymptotic performance of BA-MC:
Theorem 3 (BA-MC  Asymptotic Regime) Under A =BA-MC  lim supn→∞ nLn(A) = Λ .
The above theorem asserts that  asymptotically  the loss under BA-MC matches the asymptotically
optimal loss Λ/n characterized in Section 2.4. We may thus conclude that BA-MC is uniformly good
(in the sense of Deﬁnition 1). The proof of Theorem 3 (provided in Section E of the supplementary)
proceeds as follows: It divides the estimation problem into two consecutive sub-problems  the
one with the budget n0 =
n of pulls. We then show when
n ≥ ncutoff  the number of samples on each chain k at the end of the ﬁrst sub-problem
n0 =
is lower bounded by Ω(n1/4)  and as a consequence  the index bk would be accurate enough:
bk n0 ∈ 1
the allocation under BA-MC in the course of the second sub-problem to that of the oracle  and further
to show that the difference vanishes as n → ∞.
Below  we provide some further comments about the presented bounds in Theorems 1–3:

n and the other with the rest n − √
x Gk(x) + (cid:101)O(n−1/8)) with high probability. This allows us to relate

((cid:80)

x Gk(x) (cid:80)

Tk n0

√

√

Various regimes. Theorem 1 provides a non-asymptotic bound on the loss valid for any n  while
Theorem 3 establishes the optimality of BA-MC in the asymptotic regime. In view of the inequality
Λ ≤ K(S − 1)  the bound in Theorem 1 is at least off by a factor of S from the asymptotic loss Λ/n.
Theorem 2 bridges between the two results thereby establishing a third regime  in which the algorithm

enjoys the asymptotically optimal loss up to an additive pseudo-excess loss scaling as (cid:101)O(n−3/2).

The effect of mixing.
It is worth emphasizing that the mixing times of the chains do not appear
explicitly in the bounds  and only control (through the pseudo-spectral gap γps k) the cut-off budget
ncutoff that ensures when the pseudo-excess loss vanishes at a rate n−3/2. This is indeed a strong
aspect of our results due to our meaningful deﬁnition of loss  which could be attributed to the fact that
our loss function employs empirical estimates ˆπk n in lieu of πk. Speciﬁcally speaking  as argued in
[36]  given the number of samples of various states (akin to using ˆπk t(x) in the loss deﬁnition)  the
convergence of frequency estimates towards the true values is independent of the mixing time of the
chain. We note that despite the dependence of ncutoff on the mixing times  BA-MC does not need to

8

estimate them as when n ≤ ncutoff  it still enjoys the loss guarantee of Theorem 1. We also mention
that to deﬁne an index function for the loss function maxk
2  one
may have to derive conﬁdence bounds on the mixing time and/or stationary distribution πk as well.

xπk(x)(cid:107)Pk(x ·)−(cid:98)Pk n(x ·)(cid:107)2

(cid:80)

More on the pseudo-excess loss. We stress that the notion of the pseudo-excess loss bears some
similarity with the deﬁnition of regret for active bandit learning of distributions as introduced in [7  2]
(see Section 1). In the latter case  the regret typically decays as n−3/2 similarly to the pseudo-excess
loss in our case. An interesting question is whether the decay rate of the pseudo-excess loss  as a
function of n  can be improved. And more importantly  if a (problem-dependent) lower bound on the
pseudo-excess loss can be established. These questions are open even for the simpler case of active
learning of distributions in the i.i.d. setup; see  e.g.  [37  8  2]. We plan to address these as a future
work.

5 Conclusion

In this paper  we addressed the problem of active bandit allocation in the case of discrete and ergodic
Markov chains. We considered a notion of loss function appropriately extending the loss function
for learning distributions to the case of Markov chains. We further characterized the notion of
a “uniformly good algorithm” under the considered loss function. We presented an algorithm for
learning Markov chains  which we called BA-MC. Our algorithm is simple to implement and does not
require any prior knowledge of the Markov chains. We provided non-asymptotic PAC-type bounds on
the loss incurred by BA-MC  and showed that asymptotically  it incurs an optimal loss. We further
discussed that the (pseudo-excess) loss incurred by BA-MC in our bounds does not deteriorate with
mixing times of the chains. As a future work  we plan to derive a problem-dependent lower bound on
the pseudo-excess loss. Another interesting  and yet very challenging  future direction is to devise
adaptive learning algorithms for restless Markov chains  where the state of various chains evolve at
each round independently of the learner’s decision.

Acknowledgements

This work has been supported by CPER Nord-Pas-de-Calais/FEDER DATA Advanced data science
and technologies 2015-2020  the French Ministry of Higher Education and Research  Inria  and
the French Agence Nationale de la Recherche (ANR)  under grant ANR-16-CE40-0002 (project
BADASS).

References
[1] András Antos  Varun Grover  and Csaba Szepesvári. Active learning in multi-armed bandits. In

International Conference on Algorithmic Learning Theory  pages 287–302. Springer  2008.

[2] Alexandra Carpentier  Alessandro Lazaric  Mohammad Ghavamzadeh  Rémi Munos  and Peter
In

Auer. Upper-conﬁdence-bound algorithms for active learning in multi-armed bandits.
International Conference on Algorithmic Learning Theory  pages 189–203. Springer  2011.

[3] Valerii Vadimovich Fedorov. Theory of optimal experiments. Elsevier  1972.

[4] Hovav A. Dror and David M. Steinberg. Sequential experimental designs for generalized linear

models. Journal of the American Statistical Association  103(481):288–298  2008.

[5] David A. Cohn  Zoubin Ghahramani  and Michael I. Jordan. Active learning with statistical

models. Journal of Artiﬁcial Intelligence Research  4:129–145  1996.

[6] Pierre Etoré and Benjamin Jourdain. Adaptive optimal allocation in stratiﬁed sampling methods.

Methodology and Computing in Applied Probability  12(3):335–360  2010.

[7] András Antos  Varun Grover  and Csaba Szepesvári. Active learning in heteroscedastic noise.

Theoretical Computer Science  411(29-30):2712–2728  2010.

[8] Alexandra Carpentier  Remi Munos  and András Antos. Adaptive strategy for stratiﬁed Monte

Carlo sampling. Journal of Machine Learning Research  16:2231–2271  2015.

9

[9] James Neufeld  András György  Dale Schuurmans  and Csaba Szepesvári. Adaptive Monte
Carlo via bandit allocation. In Proceedings of the 31st International Conference on International
Conference on Machine Learning  pages 1944–1952  2014.

[10] Alexandra Carpentier and Rémi Munos. Adaptive stratiﬁed sampling for Monte-Carlo integra-
tion of differentiable functions. In Advances in Neural Information Processing Systems  pages
251–259  2012.

[11] Alexandra Carpentier and Odalric-Ambrym Maillard. Online allocation and homogeneous
partitioning for piecewise constant mean-approximation. In Advances in Neural Information
Processing Systems  pages 1961–1969  2012.

[12] Patrick Billingsley. Statistical methods in Markov chains. The Annals of Mathematical Statistics 

pages 12–40  1961.

[13] Claude Kipnis and S. R. Srinivasa Varadhan. Central limit theorem for additive functionals
of reversible Markov processes and applications to simple exclusions. Communications in
Mathematical Physics  104(1):1–19  1986.

[14] Moshe Haviv and Ludo Van der Heyden. Perturbation bounds for the stationary probabilities of

a ﬁnite Markov chain. Advances in Applied Probability  16(4):804–818  1984.

[15] Nicky J. Welton and A. E. Ades. Estimation of Markov chain transition probabilities and rates
from fully and partially observed data: uncertainty propagation  evidence synthesis  and model
calibration. Medical Decision Making  25(6):633–645  2005.

[16] Bruce A. Craig and Peter P. Sendi. Estimation of the transition matrix of a discrete-time Markov

chain. Health Economics  11(1):33–42  2002.

[17] Sean P. Meyn and Richard L. Tweedie. Markov chains and stochastic stability. Springer Science

& Business Media  2012.

[18] Emmanuel Rio. Asymptotic theory of weakly dependent random processes. Springer  2017.

[19] Pascal Lezaud. Chernoff-type bound for ﬁnite Markov chains. Annals of Applied Probability 

pages 849–867  1998.

[20] Daniel Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral

methods. Electronic Journal of Probability  20  2015.

[21] Geoffrey Wolfer and Aryeh Kontorovich. Minimax learning of ergodic Markov chains. In

Algorithmic Learning Theory  pages 903–929  2019.

[22] Yi HAO  Alon Orlitsky  and Venkatadheeraj Pichapati. On learning Markov chains. In Advances

in Neural Information Processing Systems  pages 648–657  2018.

[23] Sudeep Kamath  Alon Orlitsky  Dheeraj Pichapati  and Ananda Theertha Suresh. On learning
distributions from their samples. In Conference on Learning Theory  pages 1066–1100  2015.

[24] Ronald Ortner  Daniil Ryabko  Peter Auer  and Rémi Munos. Regret bounds for restless Markov

bandits. Theoretical Computer Science  558:62–76  2014.

[25] Cem Tekin and Mingyan Liu. Online learning of rested and restless bandits. IEEE Transactions

on Information Theory  58(8):5588–5611  2012.

[26] Christopher R. Dance and Tomi Silander. Optimal policies for observing time series and related

restless bandit problems. Journal of Machine Learning Research  20(35):1–93  2019.

[27] Jean Tarbouriech and Alessandro Lazaric. Active exploration in Markov decision processes.
In The 22nd International Conference on Artiﬁcial Intelligence and Statistics  pages 974–982 
2019.

[28] Michael Kearns  Yishay Mansour  Dana Ron  Ronitt Rubinfeld  Robert E. Schapire  and Linda
Sellie. On the learnability of discrete distributions. In Proceedings of the Twenty-Sixth Annual
ACM Symposium on Theory of Computing  pages 273–282. ACM  1994.

10

[29] David Gamarnik. Extension of the PAC framework to ﬁnite and countable Markov chains. IEEE

Transactions on Information Theory  49(1):338–345  2003.

[30] Jiantao Jiao  Kartik Venkat  Yanjun Han  and Tsachy Weissman. Minimax estimation of
functionals of discrete distributions. IEEE Transactions on Information Theory  61(5):2835–
2885  2015.

[31] Mordechai Mushkin and Israel Bar-David. Capacity and coding for the Gilbert-Elliott channels.

IEEE Transactions on Information Theory  35(6):1277–1290  1989.

[32] James R. Norris. Markov chains. Number 2. Cambridge University Press  1998.

[33] David A. Levin  Yuval Peres  and Elizabeth L. Wilmer. Markov chains and mixing times.

American Mathematical Society  Providence  RI  2009.

[34] Tze Leung Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Ad-

vances in Applied Mathematics  6(1):4–22  1985.

[35] Peter Auer  Nicolò Cesa-Bianchi  and Paul Fischer. Finite time analysis of the multiarmed

bandit problem. Machine Learning  47(2-3):235–256  2002.

[36] Daniel J. Hsu  Aryeh Kontorovich  and Csaba Szepesvári. Mixing time estimation in reversible
Markov chains from a single sample path. In Advances in Neural Information Processing
Systems  pages 1459–1467  2015.

[37] Alexandra Carpentier and Rémi Munos. Minimax number of strata for online stratiﬁed sampling:

The case of noisy samples. Theoretical Computer Science  558:77–106  2014.

11

,Mohammad Sadegh Talebi
Odalric-Ambrym Maillard