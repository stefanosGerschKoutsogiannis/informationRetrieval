2014,Generalized Higher-Order Orthogonal Iteration for Tensor Decomposition and Completion,Low-rank tensor estimation has been frequently applied in many real-world problems. Despite successful applications  existing Schatten 1-norm minimization (SNM) methods may become very slow or even not applicable for large-scale problems. To address this difficulty  we therefore propose an efficient and scalable core tensor Schatten 1-norm minimization method for simultaneous tensor decomposition and completion  with a much lower computational complexity. We first induce the equivalence relation of Schatten 1-norm of a low-rank tensor and its core tensor. Then the Schatten 1-norm of the core tensor is used to replace that of the whole tensor  which leads to a much smaller-scale matrix SNM problem. Finally  an efficient algorithm with a rank-increasing scheme is developed to solve the proposed problem with a convergence guarantee. Extensive experimental results show that our method is usually more accurate than the state-of-the-art methods  and is orders of magnitude faster.,Generalized Higher-Order Orthogonal Iteration for

Tensor Decomposition and Completion

Yuanyuan Liu†  Fanhua Shang‡∗  Wei Fan§  James Cheng‡  Hong Cheng†

†Dept. of Systems Engineering and Engineering Management 

‡Dept. of Computer Science and Engineering  The Chinese University of Hong Kong

The Chinese University of Hong Kong
§Huawei Noah′s Ark Lab  Hong Kong

{yyliu  hcheng}@se.cuhk.edu.hk {fhshang  jcheng}@cse.cuhk.edu.hk

david.fanwei@huawei.com

Abstract

Low-rank tensor estimation has been frequently applied in many real-world prob-
lems. Despite successful applications  existing Schatten 1-norm minimization
(SNM) methods may become very slow or even not applicable for large-scale
problems. To address this difﬁculty  we therefore propose an efﬁcient and scal-
able core tensor Schatten 1-norm minimization method for simultaneous tensor
decomposition and completion  with a much lower computational complexity. We
ﬁrst induce the equivalence relation of Schatten 1-norm of a low-rank tensor and
its core tensor. Then the Schatten 1-norm of the core tensor is used to replace
that of the whole tensor  which leads to a much smaller-scale matrix SNM prob-
lem. Finally  an efﬁcient algorithm with a rank-increasing scheme is developed to
solve the proposed problem with a convergence guarantee. Extensive experimen-
tal results show that our method is usually more accurate than the state-of-the-art
methods  and is orders of magnitude faster.

1 Introduction

There are numerous applications of higher-order tensors in machine learning [22  29]  signal pro-
cessing [10  9]  computer vision [16  17]  data mining [1  2]  and numerical linear algebra [14  21].
Especially with the rapid development of modern computing technology in recent years  tensors are
becoming ubiquitous such as multi-channel images and videos  and have become increasingly popu-
lar [10]. Meanwhile  some values of their entries may be missing due to the problems in acquisition
process  loss of information or costly experiments [1]. Low-rank tensor completion (LRTC) has
been successfully applied to a wide range of real-world problems  such as visual data [16  17]  EEG
data [9] and hyperspectral data analysis [9]  and link prediction [29].
Recently  sparse vector recovery and low-rank matrix completion (LRMC) has been intensively
studied [6  5]. Especially  the convex relaxation (the Schatten 1-norm  also known as the trace norm
or the nuclear norm [7]) has been used to approximate the rank of matrices and leads to a convex
optimization problem. Compared with matrices  tensor can be used to express more complicated
intrinsic structures of higher-order data. Liu et al. [16] indicated that LRTC methods utilize all
information along each dimension  while LRMC methods only consider the constraints along two
particular dimensions. As the generalization of LRMC  LRTC problems have drawn lots of attention
from researchers in past several years [10]. To address the observed tensor with missing data  some
weighted least-squares methods [1  8] have been successfully applied to EEG data analysis  nature

(cid:3)

Corresponding author.

1

and hyperspectral images inpainting. However  they are usually sensitive to the given ranks due to
their least-squares formulations [17].
Liu et al. [16] and Signorette et al. [23] ﬁrst extended the Schatten 1-norm regularization for the
estimation of partially observed low-rank tensors. In other words  the LRTC problem is converted
into a convex combination of the Schatten 1-norm minimization (SNM) of the unfolding along
each mode. Some similar algorithms can also be found in [17  22  25]. Besides these approaches
described above  a number of variations [18] and alternatives [20  28] have been discussed in the
literature. In addition  there are some theoretical developments that guarantee the reconstruction of
a low-rank tensor from partial measurements by solving the SNM problem under some reasonable
conditions [24  25  11]. Although those SNM algorithms have been successfully applied in many
real-world applications  them suffer from high computational cost of multiple SVDs as O(N I N +1) 
where the assumed size of an N-th order tensor is I × I × ··· × I.
We focus on two major challenges faced by existing LRTC methods  the robustness of the given
ranks and the computational efﬁciency. We propose an efﬁcient and scalable core tensor Schatten
1-norm minimization method for simultaneous tensor decomposition and completion  which has a
much lower computational complexity than existing SNM methods. In other words  our method
only involves some much smaller unfoldings of the core tensor replacing that of the whole tensor.
Moreover  we design a generalized Higher-order Orthogonal Iteration (gHOI) algorithm with a rank-
increasing scheme to solve our model. Finally  we analyze the convergence of our algorithm and
bound the gap between the resulting solution and the ground truth in terms of root mean square error.

In

2 Notations and Background
The mode-n unfolding of an Nth-order tensor X ∈ RI1×···×IN is a matrix denoted by X(n) ∈
RIn×Πj̸=nIj that is obtained by arranging the mode-n ﬁbers to be the columns of X(n). The Kro-
necker product of two matrices A ∈ Rm×n and B ∈ Rp×q is an mp × nq matrix given by
A⊗ B = [aijB]mp×nq. The mode-n product of a tensor X ∈ RI1×···×IN with a matrix U ∈ RJ×In
is deﬁned as (X ×n U )i1···in−1jin+1···iN =
2.1 Tensor Decompositions and Ranks
The CP decomposition approximates X by
i   where R > 0 is a given integer 
∈ RIn  and ◦ denotes the outer product of vectors. The rank of X is deﬁned as the smallest
an
i
value of R such that the approximation holds with equality. Computing the rank of the given tensor
is NP-hard in general [13]. Fortunately  the n-rank of a tensor X is efﬁcient to compute  and it
consists of the matrix ranks of all mode unfoldings of the tensor. Given the n-rank(X )  the Tucker
decomposition decomposes a tensor X into a core tensor multiplied by a factor matrix along each
mode as follows: X = G ×1 U1 ×2 ··· ×N UN . Since the ranks Rn (n = 1 ···   N ) are in general
much smaller than In  the storage of the Tucker decomposition form can be signiﬁcantly smaller
than that of the original tensor. In [8]  the weighted Tucker decomposition model for LRTC is

in=1 xi1i2···iN ujin.

∑
∑

◦··· ◦ aN

∥W ⊙ (T − G ×1 U1 ×2 ··· ×N UN )∥2
F  

(1)
where the symbol ⊙ denotes the Hadamard (elementwise) product  W is a nonnegative weight tensor
with the same size as T : wi1;i2;··· ;iN = 1 if (i1  i2 ···   iN ) ∈ Ω and wi1;i2;··· ;iN = 0 otherwise 
and the elements of T in the set Ω are given while the remaining entries are missing.

min
G; {Un}

i=1 a1

R

◦ a2

i

i

2.2 Low-Rank Tensor Completion

For the LRTC problem  Liu et al. [16] and Signoretto et al. [23] proposed an extension of LRMC
concept to tensor data as follows:

N∑

(2)
where ∥X(n)∥∗ denotes the Schatten 1-norm of the unfolding X(n)  i.e.  the sum of its singular
values  αn’s are pre-speciﬁed weights  and PΩ keeps the entries in Ω and zeros out others. Gandy

minX

n=1

αn∥X(n)∥∗ 

s.t.  PΩ(X ) = PΩ(T ) 

2

N∑

N∑

et al. [9] presented an unweighted model  i.e.  αn = 1  n = 1  . . .   N. In addition  Tomioka and
Suzuki [24] proposed a latent approach for LRTC problems:

∥(Xn)(n)∥∗ +

∥PΩ(

λ
2

n=1

min{Xn}

(3)
where λ > 0 is a regularization parameter. In fact  each mode-n unfolding X(n) shares the same
entries and cannot be optimized independently. Therefore  we need to apply variable splitting and
introduce a separate variable to each unfolding of X or Xn. However  all algorithms have to be
solved iteratively and involve multiple SVDs of very large matrices in each iteration. Hence  they
suffer from high computational cost and are even not applicable for large-scale problems.

n=1

Xn) − PΩ(T )∥2
F  

3 Core Tensor Schatten 1-Norm Minimization

The existing SNM algorithms for solving the problems (2) and (3) suffer high computational cost 
thus they have a bad scalability. Moreover  current tensor decomposition methods require explicit
knowledge of the rank to gain a reliable performance. Motivated by these  we propose a scalable
model and then achieve a smaller-scale matrix Schatten 1-norm minimization problem.

3.1 Formulation
Deﬁnition 1. The Schatten 1-norm of an Nth-order tensor X ∈ RI1×···×IN is the sum of the Schatten
1-norms of its different unfoldings X(n)  i.e. 

∥X∥∗ =

∥X(n)∥∗ 

(4)

where ∥X(n)∥∗ denotes the Schatten 1-norm of the unfolding X(n).
For the imbalance LRTC problems  the Schatten 1-norm of the tensor can be incorporated by some
pre-speciﬁed weights  αn  n = 1  . . . N. Furthermore  we have the following theorem.
Theorem 1. Let X ∈ RI1×···×IN with n-rank=(R1 ···   RN ) and G ∈ RR1×···×RN satisfy X =
G ×1 U1 ×2 ··· ×N UN   and Un ∈ St(In  Rn)  n = 1  2 ···   N  then
(5)
where ∥X∥∗ denotes the Schatten 1-norm of the tensor X and St(In  Rn) = {U ∈ RIn×Rn :
U T U = IRn
Please see Appendix A of the supplementary material for the detailed proof of the theorem. The core
tensor G with size (R1  R2  ···   RN ) has much smaller size than the observed tensor T (usually
Rn ≪ In  n = 1  2 ···   N). According to Theorem 1  our Schatten 1-norm minimization problem
is formulated into the following form:

} denotes the Stiefel manifold.

∥X∥∗ = ∥G∥∗ 

min

∥G(n)∥∗ +

∥X − G ×1 U1 ··· ×N UN∥2
F  
G;{Un};X
s.t.  PΩ(X ) = PΩ(T )  Un ∈ St(In  Rn)  n = 1 ···   N.

λ
2

n=1

Our tensor decomposition model (6) alleviates the SVD computation burden of much larger unfolded
matrices in (2) and (3). Furthermore  we use the Schatten 1-norm regularization term in (6) to
promote the robustness of the rank while the Tucker decomposition model (1) is usually sensitive to
the given rank-(r1  r2 ···   rN ) [17]. In addition  several works [12  27] have provided some matrix
rank estimation strategies to compute some values (r1  r2 ···   rN ) for the n-rank of the involved
tensor. In this paper  we only set some relatively large integers (R1  R2  ···   RN ) such that Rn ≥ rn
for all n = 1 ···   N. Different from (2) and (3)  some smaller matrices Vn ∈ RRn×Πj̸=nRj (n =
1 ···   N ) are introduced into (6) as the auxiliary variables  and then our model (6) is reformulated
into the following equivalent form:

min

∥Vn∥∗ +

∥X − G ×1 U1 ··· ×N UN∥2
F  

G;{Un};{Vn};X
s.t.  PΩ(X ) = PΩ(T )  Vn = G(n)  Un ∈ St(In  Rn)  n = 1 ···   N.

n=1

λ
2

(6)

(7)

N∑

n=1

3

N∑

N∑

In the following  we will propose an efﬁcient gHOI algorithm based on alternating direction method
of multipliers (ADMM) to solve the problem (7). ADMM decomposes a large problem into a se-
ries of smaller subproblems  and coordinates the solutions of subproblems to compute the optimal
solution. In recent years  it has been shown in [3] that ADMM is very efﬁcient for some convex or
non-convex optimization problems in various applications.

3.2 A gHOI Algorithm with Rank-Increasing Scheme

N∑

n=1

µ
2

∥G(n) − Vn∥2

(∥Vn∥∗ +⟨Yn G(n) − Vn⟩ +

The proposed problem (7) can be solved by ADMM. Its partial augmented Lagrangian function is
L(cid:22) =
F   (8)
where Yn  n = 1 ···   N  are the matrices of Lagrange multipliers  and µ > 0 is a penalty parame-
ter. ADMM solves the proposed problem (7) by successively minimizing the Lagrange function L(cid:22)
over {G  U1 ···   UN   V1 ···   VN  X}  and then updating {Y1 ···   YN}.
 ···   U k+1
Updating {U k+1
N  Gk+1}: The optimization problem with respect to {U1 ···   UN} and
N∑
G is formulated as follows:

∥X −G ×1 U1 ×2 ···×N UN∥2

F ) +

λ
2

n=1

min

n + Y k

G; {Un∈St(In;rn)}

(9)
where rn is an underestimated rank (rn ≤ Rn)  and is dynamically adjusted by using the following
rank-increasing scheme. Different from HOOI in [14]  we will propose a generalized higher-order
orthogonal iteration scheme to solve the problem (9) in Section 3.3.
 ···   V k+1
Updating {V k+1
solving the following problem:

}: With keeping all the other variables ﬁxed  V k+1

is updated by

F +

∥X k − G ×1 U1 ··· ×N UN∥2
F  

∥G(n) − V k

n /µk∥2

µk
2

λ
2

N

n

1

1

∥Vn∥∗ +

µk
2

min
Vn

∥Gk+1

(n)

− Vn + Y k

n /µk∥2
F .

(10)

For solving the problem (10)  the spectral soft-thresholding operation [4] is considered as a shrinkage
operation on the singular values and is deﬁned as follows:

(n) + Y k

n = prox1=(cid:22)k (Mn) := Udiag(max{σ − 1
V k+1

(11)
where Mn = Gk+1
n /µk  max{· ·} should be understood element-wise  and Mn =
∑
Udiag(σ)V T is the SVD of Mn. Here  only some matrices Mn of smaller size in (11) need
to perform SVD. Thus  this updating step has a signiﬁcantly lower computational complexity
× Πj̸=nRj) at worst while the computational complexity of the convex SNM algorithms
O(
for both problems (2) and (3) is O(
Updating X k+1: The optimization problem with respect to X is formulated as follows:

×Πj̸=nIj) at each iteration.

µk   0})V T  

∑

n R2
n

n I 2
n

∥2
F   s.t.  PΩ(X ) = PΩ(T ).
By deriving simply the KKT conditions for (12)  the optimal solution X is given by

∥X − Gk+1 ×1 U k+1

··· ×N U k+1

minX

N

1

X k+1 = PΩ(T ) + PΩc (Gk+1 ×1 U k+1

1

··· ×N U k+1
N ) 

(12)

(13)

where Ωc is the complement of Ω  i.e.  the set of indexes of the unobserved entries.
Rank-increasing scheme: The idea of
interlacing ﬁxed-rank optimization with adaptive
rank-adjusting schemes has appeared recently in the particular context of matrix completion
[27  28].
Let
U k+1 = (U k+1
N )  and Y k+1 =
  U k+1
N ). Considering the fact L(cid:22)k (X k+1 Gk+1  U k+1  V k+1  Y k) + ψk ≤
(Y k+1
L(cid:22)k (X k Gk  U k  V k  Y k)+ψk and ψk =
∥2
F /(2µk)  our rank-increasing scheme starts
rn such that rn ≤ Rn. We increase rn to min(rn + △rn  Rn) at iteration k + 1 if

is here extended to our algorithm for solving the proposed problem.

N )  V k+1 = (V k+1

2
  . . .   Y k+1

  . . .   V k+1

  . . .   U k+1

∑

  V k+1

  Y k+1

It

N
n=1

∥Y k

(cid:12)(cid:12)(cid:12)(cid:12)1 − L(cid:22)k (X k+1 Gk+1  U k+1  V k+1  Y k) + ψk

L(cid:22)k (X k Gk  U k  V k  Y k) + ψk

n

(cid:12)(cid:12)(cid:12)(cid:12) ≤ ϵ 

(14)

1

1

2

2

1

4

n

  Gk+1  V k+1

Algorithm 1 Solving problem (7) via gHOI
Input: PΩ(T )  (R1 ···   RN )  λ and tol.
1: while not converged do
2:
3:
4:
5:
6:
7: end while
Output: X   G  and Un  n = 1 ···   N.

and X k+1 by (18)  (20)  (11) and (13)  respectively.

Update U k+1
Apply the rank-increasing scheme.
n + µk(Gk+1
− V k+1
Update the multiplier Y k+1
(n)
Update the parameter µk+1 by µk+1 = min(ρµk  µmax).
∥2
− V k+1
Check the convergence condition  max(∥Gk+1
F   n = 1  . . .   N ) < tol.

)  n = 1  . . .   N.

n = Y k

by Y k+1

(n)

n

n

n

n

← [U k

n  bUn]
n )T )bHn  and then orthonormal-

which △rn is a positive integer and ϵ is a small constant. Moreover  we augment U k+1

where bHn has △rn randomly generated columns  bUn = (I − U k
ize bUn. Let Vn = refold(V k

n = unfold(Wn) and augment Y k

n ) ∈ Rr1×···×rN   and Wn ∈ R(r1+△r1)×···×(rN +△rN ) be augmented as
follows: (Wn)i1;··· ;iN = (Vn)i1;··· ;iN for all it ≤ rt and t ∈ [1  N ]  and (Wn)i1;··· ;iN = 0 other-
wise  where refold(·) denotes the refolding of the matrix into a tensor and unfold(·) is the unfolding
operator. Hence  we set V k
n by the same way. We then update the
involved variables by (20)  (11) and (13)  respectively.
Summarizing the analysis above  we develop an efﬁcient gHOI algorithm for solving the tensor de-
composition and completion problem (7)  as outlined in Algorithm 1. Our algorithm in essence is
the Gauss-Seidel version of ADMM. The update strategy of Jacobi ADMM can easily be imple-
mented  thus our gHOI algorithm is well suited for parallel and distributed computing and hence is
particularly attractive for solving certain large-scale problems [21]. Algorithm 1 can be accelerated
by adaptively changing µ as in [15].

n (U k

n

3.3 Generalized Higher-Order Orthogonal Iteration

We propose a generalized HOOI scheme for solving the problem (9)  where the conventional HOOI
model in [14] can be seen as a special case of the problem (9) when µk = 0. Therefore  we extend
Theorem 4.2 in [14] to solve the problem (9) as follows.
Theorem 2. Assume a real Nth-order tensor X   then the minimization of the following cost function

f (G  U1  . . .   UN ) =

∥G(n) − V k

n + Y k

µk
2

F +

∥X k − G ×1 U1 ··· ×N UN∥2

F

λ
2

g(U1  U2  . . .   UN ) = ∥λM + µkN∥2
F  
n=1 refold(V k

is equivalent to the maximization  over the matrices U1  U2  . . .   UN having orthonormal columns 
of the function
where M = X k ×1 (U1)T ··· ×N (UN )T and N =
Please see Appendix B of the supplementary material for the detailed proof of the theorem.
Updating {U k+1
}: According to Theorem 2  our generalized HOOI scheme succes-
sively solves Un  n = 1  . . .   N with ﬁxing other variables Uj  j ̸= n. Imagine that the matrices
{U1  . . .   Un−1  Un+1  . . .   UN} are ﬁxed and that the optimization problem (15) is thought of as a
quadratic expression in the components of the matrix Un that is being optimized. Considering that
the matrix has orthonormal columns  we have

 ···   U k+1

− Y k

n /µk).

(15)

N

N

n

1

max

Un∈St(In;rn)

∥λMn ×n U T

n + µkN∥2
F  

(16)

where Mn = X k ×1 (U k+1
This is actually the well-known orthogonal procrustes problem [19]  whose optimal solution is given
by the singular value decomposition of (Mn)(n)N T

n+1)T ··· ×N (U k

)T ··· ×n−1 (U k+1

n−1)T ×n+1 (U k

N )T .

(17)

1

(n)  i.e. 

U k+1

n = U (n)(V (n))T  

(18)

5

N∑

n=1

n /µk∥2
∑

where U (n) and V (n) are obtained by the skinny SVD of (Mn)(n)N T
(n). Repeating the procedure
above for different modes leads to an alternating orthogonal procrustes scheme for solving the max-
imization of the problem (16). For any estimate of those factor matrices Un  n = 1  . . .   N  the
optimal solution to the problem (9) with respect to G is updated in the following.
Updating Gk+1: The optimization problem (9) with respect to G can be rewritten as follows:

N∑

minG

n=1

∥G(n) − V k

n + Y k

n /µk∥2

F +

µk
2

λ
2

··· ×N U k+1

N

∥2
F .

(19)

(19) is a smooth convex optimization problem  thus we can obtain a closed-form solution 
Gk+1 =
− Y k

)T ··· ×N (U k+1

X k ×1 (U k+1

refold(V k
n

N )T +

µk

λ

1

n /µk). (20)

λ + N µk

1

∥X k − G ×1 U k+1
N∑

λ + N µk

n=1

4 Theoretical Analysis

In the following we ﬁrst present the convergence analysis of Algorithm 1.

4.1 Convergence Analysis
} {V k
Theorem 3. Let (Gk {U k
then we have the following conclusions:
} X k) are Cauchy sequences  respectively.
} {V k
(I) (Gk {U k
1   . . .   V k
n ) = 0  n = 1 ···   N  then (Gk {U k
− V k
N
(II) If limk→∞ µk(V k+1
KKT point of the problem (6).

1   . . .   U k
N
n

1   . . .   V k
N

1   . . .   U k
N

1   . . .   U k
N

} X k) be a sequence generated by Algorithm 1 

} X k) converges to a

The proof of the theorem can be found in Appendix C of the supplementary material.

4.2 Recovery Guarantee

We will show that when sufﬁciently many entries are sampled  the KKT point of Algorithm 1 is
stable  i.e.  it recovers a tensor “close to” the ground-truth one. We assume that the observed tensor
T ∈ RI1×I2···×IN can be decomposed as a true tensor D with rank-(r1  r2  . . .   rN ) and a random
gaussian noise E whose entries are independently drawn from N (0  σ2)  i.e.  T = D + E. For
convenience  we suppose I1 = ··· = IN = I and r1 = . . . = rN = r. Let the recovered tensor
A = G×1U1× . . .×N UN   the root mean square error (RMSE) is a frequently used measure of the
difference between the recovered tensor and the true one: RMSE := 1√
[25] analyzes the statistical performance of the convex tensor Schatten 1-norm minimization prob-
lem with the general linear operator X : RI1×:::×IN → Rm. However  our model (6) is non-convex
for the LRTC problem with the operator PΩ. Thus  we follow the sketch of the proof in [26] to
analyze the statistical performance of our model (6).
Deﬁnition 2. The operator PS is deﬁned as follows: PS(X ) = PUN
··· PU1 (X )  where PUn(X ) =
X×n(UnU T
n ).
Theorem 4. Let (G  U1  U2  . . .   UN ) be a KKT point of the problem (6) with given ranks R1 =
··· = RN = R. Then there exists an absolute constant C (please see Supplementary Material) 
such that with probability at least 1 − 2 exp(−I N−1) 

∥D − A∥F .

I N

(

) 1

4

√

√|Ω|  

R

+

N
C1λ

(21)

RMSE ≤ ∥E∥F√
I N
|Ti1;··· ;iN
| and C1 =

+ Cβ

I N−1R log(I N−1)

|Ω|

∥PSPΩ(T −A)∥F
∥PΩ(T −A)∥F

.

where β = maxi1;··· ;iN

The proof of the theorem and the analysis of lower-boundedness of C1 can be found in Appendix
D of the supplementary material. Furthermore  our result can also be extended to the general linear
operator X   e.g.  the identity operator (i.e.  tensor decomposition problems). Similar to [25]  we
assume that the operator satisﬁes the following restricted strong convexity (RSC) condition.

6

Table 1: RSE and running time (seconds) comparison on synthetic tensor data:

(a) Tensor size: 30×30×30×30×30

WTucker

RSE±std.

WCP

RSE±std.

Time

SR
Time
10% 0.4982±2.3e-2 2163.05 0.5003±3.6e-2 4359.23 0.6744±2.7e-2 1575.78 0.6268±5.0e-2 8324.17 0.2537±1.2e-2 159.43
30% 0.1562±1.7e-2 2226.67 0.3364±2.3e-2 3949.57 0.3153±1.4e-2 1779.59 0.2443±1.2e-2 8043.83 0.1206±6.0e-3 143.86
50% 0.0490±9.3e-3 2652.90 0.0769±5.0e-3 3260.86 0.0365±6.2e-4 2024.52 0.0559±7.7e-3 8263.24 0.0159±1.3e-3 135.60

Time

Time

Time

FaLRTC

RSE±std.

Latent

RSE±std.

gHOI
RSE±std.

(b) Tensor size: 60 × 60 × 60 × 60

WTucker

RSE±std.

FaLRTC
SR
10% 0.2319±3.6e-2 1437.61 0.4766±9.4e-2 1586.92 0.4927±1.6e-2
30% 0.0143±2.8e-3 1756.95 0.1994±6.0e-3 1696.27 0.1694±2.5e-3
50% 0.0079±6.2e-4 2534.59 0.1335±4.9e-3 1871.38 0.0602±5.8e-4

RSE±std.

RSE±std.

WCP

Time

Time

Time
562.15
603.49
655.69

Latent

RSE±std.

gHOI
RSE±std.
0.5061±4.4e-2 5075.82 0.1674±3.4e-3
0.1872±7.5e-3 5559.17 0.0076±6.5e-4
0.0583±9.7e-4 6086.63 0.0030±1.7e-4

Time

Time
60.53
57.19
55.62

1
m

Deﬁnition 3 (RSC). We suppose that there is a positive constant κ(X ) such that the operator
X : RI1×:::×IN → Rm satisﬁes the inequality
∥X (△)∥2

≥ κ(X )∥△∥2
F  

2

where △ ∈ RI1×:::×IN is an arbitrary tensor.
Theorem 5. Assume the operator X satisﬁes the RSC condition with a constant κ(X ) and the
observations y = X (D) + ε. Let (G  U1  U2  . . .   UN ) be a KKT point of the following problem
N∑
with given ranks R1 = ··· = RN = R 
∥G(n)∥∗ +
√

∥y − X (G×1U1× ···×N UN )∥2
2.

G; {Un∈St(In;Rn)}

√

Then

(22)

min

√

λ
2

n=1

RMSE ≤

(23)

∥ε∥2
mκ(X )I N

+

C2λ

R

N
mκ(X )I N

 

where C2 =

∥PS X ∗

(y−X (A))∥F

∥y−X (A)∥2

and X ∗ denotes the adjoint operator of X .

The proof of the theorem can be found in Appendix E of the supplementary material.

5 Experiments

5.1 Synthetic Tensor Completion
Following [17]  we generated a low-n-rank tensor T ∈ RI1×I2×···×IN which we used as the ground
truth data. The order of the tensors varies from three to ﬁve  and r is set to 10. Furthermore  we
randomly sample a few entries from T and recover the whole tensor with various sampling ratios
(SRs) by our gHOI method and the state-of-the-art LRTC algorithms including WTucker [8]  WCP
[1]  FaLRTC [17]  and Latent [24]. The relative square error (RSE) of the recovered tensor X for all
these algorithms is deﬁned by RSE := ∥X − T ∥F /∥T ∥F .
The average results (RSE and running time) of 10 independent runs are shown in Table 1  where
the order of tensor data varies from four to ﬁve.
It is clear that our gHOI method consistently
yields much more accurate solutions  and outperforms the other algorithms in terms of both RSE
and efﬁciency. Moreover  we present the running time of our gHOI method and the other methods
with varying sizes of third-order tensors  as shown in Fig. 1(a). We can see that the running time
of WTcuker  WCP  Latent and FaLRTC dramatically grows with the increase of tensor size whereas
that of our gHOI method only increases slightly. This shows that our gHOI method has very good
scalability and can address large-scale problems. To further evaluate the robustness of our gHOI
method with respect to the given tensor rank changes  we conduct some experiments on the synthetic
data of size 100 × 100 × 100  and illustrate the recovery results of all methods with 20% SR  where
the rank parameter of gHOI  WTucker and WCP is chosen from {10  15 ···   40}. The average RSE
results of 10 independent runs are shown in Fig. 1(b)  from which we can see that our gHOI method
performs much more robust than both WTucker and WCP.

7

(a)

(b)

(c)

(d)

Figure 1: Comparison of all these methods in terms of computational time (in seconds and in log-
arithmic scale) and RSE on synthetic third-order tensors by varying tensor sizes (a) or given ranks
(b)  and the BRAINIX data set: running time (c) and RSE (d).

(a) Original

(b) 30% SR

(c) RSE: 0.2693 (d) RSE: 0.3005 (e) RSE: 0.2858 (f) RSE: 0.2187
Figure 2: The recovery results on the BRAINIX data set with 30% SR: (c)-(f) The results of
WTucker  FaLRTC  Latent and gHOI  respectively (Best viewed zoomed in).

5.2 Medical Images Inpainting

In this part  we apply our gHOI method for medical image inpainting problems on the BRAINIX
data set1. The recovery results on one randomly chosen image with 30% SR are illustrated in Fig.
2. Moreover  we also present the recovery accuracy (RSE) and running time (seconds) with varying
SRs  as shown in Fig. 1(c) and (d). From these results  we can observe that our gHOI method
consistently performs better than the other methods in terms of both RSE and efﬁciency. Especially 
gHOI is about 20 times faster than WTucker and FaLRTC  and more than 90 times faster than
Latent  when the sample percentage is 10%. By increasing the sampling rate  the RSE results of
three Schatten 1-norm minimization methods including Latent  FaLRTC and gHOI  dramatically
reduce. In contrast  the RSE of WTucker decreases slightly.

6 Conclusions

We proposed a scalable core tensor Schatten 1-norm minimization method for simultaneous tensor
decomposition and completion. First  we induced the equivalence relation of the Schatten 1-norm of
a low-rank tensor and its core tensor. Then we formulated a tractable Schatten 1-norm regularized
tensor decomposition model with missing data  which is a convex combination of multiple much
smaller-scale matrix SNM. Finally  we developed an efﬁcient gHOI algorithm to solve our problem.
Moreover  we also provided the convergence analysis and recovery guarantee of our algorithm. The
convincing experimental results veriﬁed the efﬁciency and effectiveness of our gHOI algorithm.
gHOI is signiﬁcantly faster than the state-of-the-art LRTC methods. In the future  we will apply
our gHOI algorithm to address a variety of robust tensor recovery and completion problems  e.g. 
higher-order RPCA [10] and robust LRTC.

Acknowledgments

This research is supported in part by SHIAE Grant No. 8115048  MSRA Grant No. 6903555  GRF
No. 411211  CUHK direct grant Nos. 4055015 and 4055017  China 973 Fundamental R&D Pro-
gram  No. 2014CB340304  and Huawei Grant No. 7010255.

1http://www.osirix-viewer.com/datasets/

8

2004006008001000102104Size of tensorsTime (seconds) WTuckerWCPFaLRTCLatentgHOI1020304000.050.10.150.20.25RankRSE WTuckerWCPFaLRTCLatentgHOI0.20.40.60.8102103Sampling ratesTime (seconds) WTuckerFaLRTCLatentgHOI0.20.40.60.800.10.20.30.40.5Sampling ratesRSE WTuckerFaLRTCLatentgHOIReferences
[1] E. Acar  D. Dunlavy  T. Kolda  and M. Mørup. Scalable tensor factorizations with missing data. In SDM 

pages 701–711  2010.

[2] A. Anandkumar  D. Hsu  M. Janzamin  and S. Kakade. When are overcomplete topic models identiﬁable?
uniqueness of tensor Tucker decompositions with structured sparsity. In NIPS  pages 1986–1994  2013.
[3] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning

via the alternating direction method of multipliers. Found. Trends Mach. Learn.  3(1):1–122  2011.

[4] J. Cai  E. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM J.

Optim.  20(4):1956–1982  2010.

[5] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math. 

9(6):717–772  2009.

[6] E. Cand`es  J. Romberg  and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly

incomplete frequency information. IEEE Trans. Inform. Theory  52(2):489–509  2006.

[7] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis  Stanford University  2002.
[8] M. Filipovic and A. Jukic. Tucker factorization with missing data with application to low-n-rank tensor

completion. Multidim. Syst. Sign. Process.  2014.

[9] S. Gandy  B. Recht  and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex opti-

mization. Inverse Problem  27(2)  2011.

[10] D. Goldfarb and Z. Qin. Robust low-rank tesnor recovery: Models and algorithms. SIAM J. Matrix Anal.

Appl.  35(1):225–253  2014.

[11] B. Huang  C. Mu  D. Goldfarb  and J. Wright. Provable low-rank tensor recovery.

Online:4252  2014.

In Optimization-

[12] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries.

Theory  56(6):2980–2998  2010.

IEEE Trans. Inform.

[13] T. Kolda and B. Bader. Tensor decompositions and applications. SIAM Review  51(3):455–500  2009.
[14] L. Lathauwer  B. Moor  and J. Vandewalle. On the best rank-1 and rank-(r1 r2 ... rn) approximation of

high-order tensors. SIAM J. Matrix Anal. Appl.  21(4):1324–1342  2000.

[15] Z. Lin  R. Liu  and Z. Su. Linearized alternating direction method with adaptive penalty for low-rank

representation. In NIPS  pages 612–620  2011.

[16] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual data.

In ICCV  pages 2114–2121  2009.

[17] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in visual data.

IEEE Trans. Pattern Anal. Mach. Intell.  35(1):208–220  2013.

[18] C. Mu  B. Huang  J. Wright  and D. Goldfarb. Square deal: lower bounds and improved relaxations for

tensor recovery. In ICML  pages 73–81  2014.

[19] H. Nick. Matrix procrustes problems. 1995.
[20] B. Romera-Paredes and M. Pontil. A new convex relaxation for tensor completion.

2967–2975  2013.

In NIPS  pages

[21] F. Shang  Y. Liu  and J. Cheng. Generalized higher-order tensor decomposition via parallel ADMM. In

AAAI  pages 1279–1285  2014.

[22] M. Signoretto  Q. Dinh  L. Lathauwer  and J. Suykens. Learning with tensors: A framework based on

covex optimization and spectral regularization. Mach. Learn.  94(3):303–351  2014.

[23] M. Signoretto  L. Lathauwer  and J. Suykens. Nuclear norms for tensors and their use for convex multi-

linear estimation. Technical Report 10-186  ESATSISTA  K. U. Leuven  2010.

[24] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured Schatten norm regularization. In

NIPS  pages 1331–1339  2013.

[25] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor decompo-

sition. In NIPS  pages 972–980  2011.

[26] Y. Wang and H. Xu. Stability of matrix factorization for collaborative ﬁltering. In ICML  2012.
[27] Z. Wen  W. Yin  and Y. Zhang. Solving a low-rank factorization model for matrix completion by a

nonlinear successive over-relaxation algorithm. Math. Prog. Comp.  4(4):333–361  2012.

[28] Y. Xu  R. Hao  W. Yin  and Z. Su. Parallel matrix factorization for low-rank tensor completion.

arXiv:1312.1254  2013.

In

[29] Y. Yilmaz  A. Cemgil  and U. Simsekli. Generalised coupled tensor factorisation. In NIPS  pages 2151–

2159  2011.

9

,Daniel Bartz
Klaus-Robert Müller
Yuanyuan Liu
Fanhua Shang
Wei Fan
James Cheng
Hong Cheng
Kai-Wei Chang
He He
Stephane Ross
Hal Daume III
John Langford