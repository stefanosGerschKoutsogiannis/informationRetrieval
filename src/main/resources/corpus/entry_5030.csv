2017,Simple strategies for recovering inner products from coarsely quantized random projections,Random projections have been increasingly adopted for a diverse set of tasks in machine learning involving dimensionality reduction. One specific line of research on this topic has investigated the use of quantization subsequent to projection with the aim of additional data compression. Motivated by applications in nearest neighbor search and linear learning  we revisit the problem of recovering inner products (respectively cosine similarities) in such setting. We show that even under coarse scalar quantization with 3 to 5 bits per projection  the loss in accuracy tends to range from ``negligible'' to ``moderate''. One implication is that in most scenarios of practical interest  there is no need for a sophisticated recovery approach like maximum likelihood estimation as considered in previous work on the subject. What we propose herein also yields considerable improvements in terms of accuracy over the Hamming distance-based approach in Li et al. (ICML 2014) which is comparable in terms of simplicity,Simple Strategies for Recovering Inner Products from

Coarsely Quantized Random Projections

Ping Li

Baidu Research  and
Rutgers University

pingli98@gmail.com

Martin Slawski

Department of Statistics
George Mason University

mslawsk3@gmu.edu

Abstract

Random projections have been increasingly adopted for a diverse set of tasks in
machine learning involving dimensionality reduction. One speciﬁc line of research
on this topic has investigated the use of quantization subsequent to projection
with the aim of additional data compression. Motivated by applications in nearest
neighbor search and linear learning  we revisit the problem of recovering inner
products (respectively cosine similarities) in such setting. We show that even under
coarse scalar quantization with 3 to 5 bits per projection  the loss in accuracy tends
to range from “negligible” to “moderate”. One implication is that in most scenarios
of practical interest  there is no need for a sophisticated recovery approach like
maximum likelihood estimation as considered in previous work on the subject.
What we propose herein also yields considerable improvements in terms of accuracy
over the Hamming distance-based approach in Li et al. (ICML 2014) which is
comparable in terms of simplicity.

1

Introduction

The method of random projections (RPs) for linear dimensionality reduction has become more
and more popular over the years after the basic theoretical foundation  the celebrated Johnson-
Lindenstrauss (JL) Lemma [12  20  33]  had been laid out. In a nutshell  it states that it is possible
to considerably lower the dimension of a set of data points by means of a linear map in such a way
that squared Euclidean distances and inner products are roughly preserved in the low-dimensional
representation. Conveniently  a linear map of this sort can be realized by a variety of random
matrices [1  2  18]. The scope of applications of RPs has expanded dramatically in the course of
time  and includes dimension reduction in linear classiﬁcation and regression [14  30]  similarity
search [5  17]  compressed sensing [8]  clustering [7  11]  randomized numerical linear algebra and
matrix sketching [29]  and differential privacy [21]  among others.
The idea of achieving further data compression by means of subsequent scalar quantization of the
projected data has been considered for a while. Such setting can be motivated from constraints
concerning data storage and communication  locality-sensitive hashing [13  27]  or the enhancement
of privacy [31]. The extreme case of one-bit quantization can be associated with two seminal works
in computer science  the SDP relaxation of the MAXCUT problem [16] and the simhash [10]. One-bit
compressed sensing is introduced in [6]  and along with its numerous extensions  has meanwhile
developed into a subﬁeld within the compressed sensing literature. A series of recent papers discuss
quantized RPs with a focus on similarity estimation and search. The papers [25  32] discuss quantized
RPs with a focus on image retrieval based on nearest neighbor search. Independent of the speciﬁc
application  [25  32] provide JL-type statements for quantized RPs  and consider the trade-off between
the number of projections and the number of bits per projection under a given budget of bits as it also
appears in the compressed sensing literature [24]. The paper [19] studies approximate JL-type results
for quantized RPs in detail. The approach to quantized RPs taken in the present paper follows [27  28]

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

in which the problem of recovering distances and inner products is recast within the framework of
classical statistical point estimation theory. The paper [28] discusses maximum likelihood estimation
in this context  with an emphasis on the aforementioned trade-off between the number of RPs and the
bit depth per projection. In the present paper we focus on the much simpler and computationally much
more convenient approach in which the presence of the quantizer is ignored  i.e.  quantized data are
treated in the same way as full-precision data. We herein quantify the loss of accuracy of this approach
relative to the full-precision case  which turns out to be insigniﬁcant in many scenarios of practical
interest even under coarse quantization with 3 to 5 bits per projection. Moreover  we show that
the approach compares favorably to the Hamming distance-based (or equivalently collision-based)
scheme in [27] which is of similar simplicity. We argue that both approaches have their merits: the
collision-based scheme performs better in preserving local geometry (the distances of nearby points) 
whereas the one studied in more detail herein yields better preservation globally.
Notation. For a positive integer m  we let [m] = {1  . . .   m}. For l ∈ [m]  v(l) denotes the l-th
component of a vector v ∈ Rm; if there is no danger of confusion with another index  the brackets in
the subscript are omitted. I(P ) denotes the indicator function of expression P .

Supplement: Proofs and additional experimental results can be found in the supplement.
Basic setup. Let X = {x1  . . .   xn} ⊂ Rd be a set of input data with squared Euclidean norms
2  i ∈ [n]. We think of d being large. RPs reduce the dimensionality of the input data
i := (cid:107)xi(cid:107)2
λ2
by means of a linear map A : Rd → Rk  k (cid:28) d. We assume throughout the paper that the map
A is realized by a random matrix with i.i.d. entries from the standard Gaussian distribution  i.e. 
Alj ∼ N (0  1)  l ∈ [k]  j ∈ [d]. One standard goal of RPs is to approximately preserve distances in
X while lowering the dimension  i.e.  (cid:107)Axi − Axj(cid:107)2
2 for all (i  j). This is implied
by approximate inner product preservation (cid:104)xi  xj(cid:105) ≈ (cid:104)Axi  Axj(cid:105) /k for all (i  j).
For the time being  we assume that it is possible to compute and store the squared norms {λ2

and to rescale the input data to unit norm  i.e.  one ﬁrst forms(cid:101)xi ← xi/λi  i ∈ [n]  before applying
i}n
i=1 
= (cid:104)(cid:101)xi (cid:101)xj(cid:105)  i  j ∈ [n]  of
the input data X from their compressed representation Z = {z1  . . .   zn}  zi := A(cid:101)xi  i ∈ [n].

A. In this case  it sufﬁces to recover the (cosine) similarities ρij :=

2/k ≈ (cid:107)xi − xj(cid:107)2

(cid:104)xi xj(cid:105)
λiλj

2 Estimation of cosine similarity based on full-precision RPs

As preparation for later sections  we start by providing background concerning the usual setting
without quantization. Let (Z  Z(cid:48))r be random variables having a bivariate Gaussian distribution with
zero mean  unit variance  and correlation r ∈ (−1  1):

(Z  Z(cid:48))r ∼ N2

0

Let further x  x(cid:48) be a generic pair of points from X   and let z := A(cid:101)x  z(cid:48) := A(cid:101)x(cid:48) be the counterpart in
l=1 of (z  z(cid:48)) are distributed i.i.d. as in (1) with r = ρ =: (cid:104)(cid:101)x (cid:101)x(cid:48)(cid:105).
(l))}k

Z. Then the components {(z(l)  z(cid:48)
Hence the problem of recovering the cosine similarity of x and x(cid:48) can be re-cast as estimating the
correlation from an i.i.d. sample of k bivariate Gaussian random variables. To simplify our exposition 
we henceforth assume that 0 ≤ ρ < 1 as this can easily be achieved by ﬂipping the sign of one of x
or x(cid:48). The standard estimator of ρ is what is called the “linear estimator” herein:

r

(1)

(cid:18)(cid:18)0
(cid:19)

(cid:18)1

 

(cid:19)(cid:19)

.

r
1

(cid:98)ρlin =

(cid:104)z  z(cid:48)(cid:105) =

1
k

1
k

k(cid:88)

l=1

z(l)z(cid:48)

(l).

(2)

(cid:26)

(cid:98)ρMLE = argmax

As pointed out in [26] this estimator can be considerably improved upon by the maximum likelihood
estimator (MLE) given (1):
− 1
2

The estimator(cid:98)ρMLE is not available in closed form  which is potentially a serious concern since it

needs to be evaluated for numerous different pairs of data points. However  this can be addressed

log(1 − r2) − 1
2

(cid:104)z  z(cid:48)(cid:105) 2r

(cid:18) 1

(cid:107)z(cid:48)(cid:107)2

2 − 1
k

(cid:107)z(cid:107)2

2 +

1

1 − r2

(cid:19)(cid:27)

.

(3)

1
k

k

r

2

(cid:110)(cid:16)(cid:107)z(cid:107)2

(cid:17)

(cid:111)

(6)
(7)

(8)

by tabulation of the two statistics

(cid:98)ρMLE over a sufﬁciently ﬁne grid. At processing time  computation of(cid:98)ρMLE can then be reduced to a
One obvious issue of(cid:98)ρlin is that it does not respect the range of the underlying parameter. A natural

look-up in a pre-computed table.

and the corresponding solutions

/k  (cid:104)z  z(cid:48)(cid:105) /k

2 + (cid:107)z(cid:48)(cid:107)2

2

ﬁx is the use of the “normalized linear estimator”

(cid:98)ρnorm = (cid:104)z  z(cid:48)(cid:105) /((cid:107)z(cid:107)2 (cid:107)z(cid:48)(cid:107)2).
ρ((cid:98)ρ) + Varρ((cid:98)ρ) 

(4)
When comparing different estimators of ρ in terms of statistical accuracy  we evaluate the mean
squared error (MSE)  possibly asymptotically as the number of RPs k → ∞. Speciﬁcally  we consider

MSEρ((cid:98)ρ) = Eρ[(ρ −(cid:98)ρ)2] = Bias2

where(cid:98)ρ is some estimator  and the subscript ρ indicates that expectations are taken with respect to a
It turns out that(cid:98)ρnorm and(cid:98)ρMLE can have dramatically lower (asymptotic) MSEs than(cid:98)ρlin for large

sample (z  z(cid:48)) following the bivariate normal distribution in (1) with r = ρ.

values of ρ  i.e.  for points of high cosine similarity. It can be shown that (cf. [4]  p.132  and [26])

(5)

Biasρ((cid:98)ρ) := Eρ[(cid:98)ρ] − ρ 

Biasρ((cid:98)ρlin) = 0 

Varρ((cid:98)ρlin) = (1 + ρ2)/k 
ρ((cid:98)ρnorm) = O(1/k2)  Varρ((cid:98)ρnorm) = (1 − ρ2)2/k + O(1/k2) 
ρ((cid:98)ρMLE) = O(1/k2)  Varρ((cid:98)ρMLE) = (1−ρ2)2

1+ρ2 /k + O(1/k2).

Bias2

Bias2

While for ρ = 0  the (asymptotic) MSEs are the same  we note that the leading terms of the MSEs

of(cid:98)ρnorm and(cid:98)ρMLE decay at rate Θ((1 − ρ)2) as ρ → 1  whereas the MSE of(cid:98)ρlin grows with ρ. The
following table provides the asymptotic MSE ratios of(cid:98)ρlin and(cid:98)ρnorm for selected values of ρ.

ρ

MSEρ((cid:98)ρlin)
MSEρ((cid:98)ρnorm)

0.5

2.2

0.6

3.3

0.7

5.7

0.8

12.6

0.9

50

0.95

0.99

200

5000

In conclusion  if it is possible to pre-compute and store the norms of the data prior to dimensionality
reduction  a simple form of normalization can yield important beneﬁts with regard to the recovery of
inner products and distances for pairs of points having high cosine similarity. The MLE can provide

a further reﬁnement  but the improvement over(cid:98)ρnorm can be at most by a factor of 2.

3 Estimation of cosine similarity based on quantized RPs

The following section contains our main results. After introducing preliminaries regarding quantiza-
tion  we review previous approaches to the problem  before analyzing estimators following a different
paradigm. We conclude with a comparison and some recommendations about what to use in practice.
Quantization. After obtaining the projected data Z  the next step is scalar quantization. Let
t = (t1  . . .   tK−1) with 0 = t0 < t1 < . . . < tK−1 < tK = +∞ be a set of thresholds
inducing a partitioning of the positive real line into K intervals {[ts−1  ts)  s ∈ [K]}  and let
M = {µ1  . . .   µK} be a set of codes with µs representing interval [ts−1  ts)  s ∈ [K]. Given t and
M  the scalar quantizer (or quantization map) is deﬁned by

Q : R → M± := −M ∪ M 

s=1 µsI(|z| ∈ [ts−1  ts)).

i=1 ⊂ (M±)k  qi =(cid:0) Q(zi(l) )(cid:1)k

(9)

The projected and quantized data result as Q = {qi}n
l=1  where zi(l)
denotes the l-th component of zi ∈ Z  l ∈ [k]  i ∈ [n]. The bit depth b of the quantizer is given by
b := 1 + log2(K). For simplicity  we only consider the case where b is an integer. The case b = 1 is
well-studied [10  27] and is hence disregarded in our analysis to keep our exposition compact.
Bin-based vs. code-based approaches. Let q = Q(z) and q(cid:48) = Q(z(cid:48)) be the points resulting from
quantization of the generic pair z  z(cid:48) in the previous section. In this paper  we distinguish between
two basic paradigms for estimating the cosine similarity of the underlying pair x  x(cid:48) from q  q(cid:48). The
ﬁrst paradigm  which we refer to as bin-based estimation  does not make use of the speciﬁc values of

z (cid:55)→ Q(z) = sign(z)(cid:80)K

3

the codes M±  but only of the intervals (“bins”) associated with each code. This is opposite to the
second paradigm  referred to as code-based estimation which only makes use of the values of the
codes. As we elaborate below  an advantage of the bin-based approach is that working with intervals
reﬂects the process of quantization more faithfully and hence can be statistically more accurate; on the
other hand  a code-based approach tends to be more convenient from the point of view computation.
In this paper  we make a case for the code-based approach by showing that the loss in statistical
accuracy can be fairly minor in several regimes of practical interest.

Lloyd-Max (LM) quantizer. With b respectively K being ﬁxed  one needs to choose the thresholds
t and the codes M of the quantizer (the second is crucial only for a code-based approach). In our
setting  with zi(l) ∼ N (0  1)  i ∈ [n]  l ∈ [k]  which is inherited from the distribution of the entries
of A  a standard choice is LM quantization [15] which minimizes the squared distortion error:

(t(cid:63)  µ(cid:63)) = argmin

t µ

Eg∼N (0 1)[{g − Q(g; t  µ)}2].

(10)

Problem (10) can be solved by an iterative scheme that alternates between optimization of t for ﬁxed
µ and vice versa. That scheme can be shown to deliver the global optimum [22]. In the absence of
any prior information about the cosine similarities that we would like to recover  (10) appears as a
reasonable default whose use for bin-based estimation has been justiﬁed in [28]. In the limit of cosine
similarity ρ → 1  it may seem more plausible to use (10) with g replaced by its square  and taking the
root of the resulting optimal thresholds and codes. However  it turns out that empirically this yields
reduced performance more often than improvements  hence we stick to (10) in the sequel.

(l)

l=1 and q(cid:48) = (q(cid:48)

3.1 Bin-based approaches
MLE. Given a pair q = (q(l) )k
l=1 of projected and quantized points  max-
)k
imum likelihood estimation of the underlying cosine similarity ρ is studied in depth in [28].
The associated likelihood function L(r) is based on bivariate normal probabilities of the form
Pr(Z ∈ [ts−1  ts)  Z(cid:48) ∈ [tu−1  tu))  P−r(Z ∈ [ts−1  ts)  Z(cid:48) ∈ [tu−1  tu)) with (Z  Z(cid:48))r as in (1).
It is shown in [28] that the MLE with b ≥ 2 can be more efﬁcient at the bit level than common
single-bit quantization [10  16]; the optimal choice of b increases with ρ. While statistically optimal in
the given setting  the MLE remains computationally cumbersome even when using the approximation
in [28] because it requires cross-tabulation of the empirical frequencies corresponding to the bivariate
normal probabilities above. This makes the use of the MLE unattractive particularly in situations in
which it is not feasible to materialize all O(n2) pairwise similarities estimable from (qi  qj)i<j so
that they would need to be re-evaluated frequently.

as the MLE. The similarity ρ is estimated as(cid:98)ρcol = θ−1(cid:16)(cid:80)k
Collision-based estimator. The collision-based estimator proposed in [27] is a bin-based approach
  where the map
increasing in [27]. Compared to the MLE (cid:98)ρcol uses less information – it only counts “collisions” 
θ : [0  1] → [0  1] is deﬁned by r (cid:55)→ θ(r) = Pr(Q(Z) = Q(Z(cid:48)))  shown to be monotonically
}. The loss in statistical efﬁciency is moderate for b = 2  in particular for ρ
i.e.  events {q(l) = q(cid:48)
the positive side (cid:98)ρcol is convenient to compute given that the evaluation of the function θ−1 can be
close to 1. However  as b increases that loss becomes more and more substantial; cf. Figure 1. On

approximated by employing a look-up table after tabulating θ on a ﬁne grid.

l=1 I(q(l) = q(cid:48)

(l)

)/k

(cid:17)

(l)

Figure 1: (L): Asymptotic MSEs [27] of(cid:98)ρcol (to be divided by k) for 2 ≤ b ≤ 4. (M R): Asymptotic
relative efﬁciencies MSEρ((cid:98)ρcol)/MSEρ((cid:98)ρMLE) for b ∈ {2  4}  where(cid:98)ρMLE is the MLE in [28].

4

00.20.40.60.81-3-2.5-2-1.5-1-0.500.511.5log10(MSE)b = 2b = 3b = 40.20.40.60.8112345Relative Efficiencyb = 200.20.40.60.811102030Relative Efficiencyb = 4b

2
3
4
5
6

bound on bias2

1.2 · 10−1
7.2 · 10−3
4.5 · 10−4
2.8 · 10−5
1.8 · 10−6

obtained from Theorem 1 by setting ρ = 1. (R): Varρ((cid:98)ρlin) (to be divided by k).

ρ((cid:98)ρlin) and the bound of Theorem 1. (M): uniform upper bounds on Bias2

Figure 2: (L): Bias2

ρ((cid:98)ρlin)

3.2 Code-based approaches

In the code-based approach  we simply ignore the fact that the quantized data actually represent
intervals and treat them precisely in the same way as full-precision data. Recovery of cosine similarity
is performed by means of the estimator in §2 with z  z(cid:48) replaced by q  q(cid:48). Perhaps surprisingly  it
turns out that depending on ρ the loss of information incurred by this rather crude approach can be
small already for bit depths between b = 3 and b = 5. That loss increases with ρ  with a fundamental
gap compared to bin-based approaches and to the full precision case in the limit ρ → 1.

Linear estimator. We ﬁrst consider(cid:98)ρlin = (cid:104)q  q(cid:48)(cid:105) /k. We note that(cid:98)ρlin =(cid:98)ρlin b depends on b; b = ∞
corresponds to the estimator(cid:98)ρlin =(cid:98)ρlin ∞ in §2 denoted by the same symbol. A crucial difference
approaches whose bias needs to be analyzed carefully. The exact bias of(cid:98)ρlin in dependence of ρ and
(tu−1  tu)(cid:1) 

between the code-based and the bin-based approaches discussed above is that the latter have vanishing
asymptotic squared bias of the order O(k−2) for any b [27  28]. This is not the case for code-based
b can be evaluated exactly numerically. Numerical evaluations of bias and variance of estimators
discussed in the present section only rely on the computation of coefﬁcients θα β deﬁned by

K(cid:88)
we have Eρ[(cid:98)ρlin] = θ1 1  Varρ((cid:98)ρlin) = (θ2 2 − θ2
provide a bound on the bias of(cid:98)ρlin which quantiﬁes explicitly the rate of decay in dependence b.

(11)
where α  β are non-negative integers and (Z  Z(cid:48)) are bivariate normal (1) with r = ρ. Speciﬁcally 
1 1)/k. In addition to exact numerical evaluation  we

(cid:0)Z ∈ σ(ts−1  ts)  Z

θα β := Eρ[Q(Z)αQ(Z

(cid:88)

σ σ(cid:48)∈{−1 1}

)βµα

s µβ

u Pρ

(cid:48) ∈ σ

(cid:48)

(cid:48)

σα(σ

(cid:48)

)β] =

s u=1

Theorem 1. We have Bias2

b   where Db = 33/22π

12 2−2b ≈ 2.72 · 2−2b.

ρ((cid:98)ρlin) ≤ 4ρ2D2

As shown in Figure 2 (L)  the bound on the squared bias in Theorem 1 constitutes a reasonable
proxy of the exact squared bias. The rate of decay is O(2−4b). Moreover  it can be veriﬁed
numerically that the variance in the full precision case upper bounds the variance for ﬁnite b  i.e. 

Varρ((cid:98)ρlin b) ≤ Varρ((cid:98)ρlin ∞)  ρ ∈ [0  1). Combining bias and variance  we may conclude that
depending on k  the MSE of(cid:98)ρlin based on coarsely quantized data does not tend to be far from what
(i) Suppose k = 100 and b = 3. With full precision  we have MSEρ((cid:98)ρlin ∞) = (1+ρ2)/k ∈ [.01  .02].
From Figure 2 (M) and the observation that Varρ((cid:98)ρlin 3) ≤ Varρ((cid:98)ρlin ∞)  we ﬁnd that the MSE can

is achieved with full precision data. The following two examples illustrate this point.

go up by at most 7.2 · 10−3  i.e.  it can at most double relative to the full precision case.

(ii) Suppose k = 1000 and b = 4. With the same reasoning as in (i)  the MSE under quantization can
increase at most by a factor of 1.45 as compared to full precision data.

Figure 3 shows that these numbers still tend to be conservative. In general  the difference of the
MSEs for b = ∞ on the one hand and b ∈ {3  4  5} on the other hand gets more pronounced for large

values of the similarity ρ and large values of k. This is attributed to the (squared) bias of(cid:98)ρlin. In

particular  it does not pay off to choose k signiﬁcantly larger than the order of the squared bias.

5

00.20.40.60.81-11-10-9-8-7-6-5-4-3-2-1log10(squared Bias)b = 2b = 3b = 4b = 5b = 600.20.40.60.810.811.21.41.61.82varianceFigure 3: MSEs of(cid:98)ρlin for various k and b ∈ {3  4  5} (dotted). The solid (red) lines indicate the
corresponding MSEs for(cid:98)ρlin in the full-precision case (b = ∞).
form (cid:98)ρnorm = (cid:104)z  z(cid:48)(cid:105) /((cid:107)z(cid:107)2 (cid:107)z(cid:48)(cid:107)2) can yield substantial beneﬁts. Interestingly  it turns out that
the counterpart(cid:98)ρnorm = (cid:104)q  q(cid:48)(cid:105) /((cid:107)q(cid:107)2 (cid:107)q(cid:48)(cid:107)2) for quantized data is even more valuable as it helps
reducing the bias of(cid:98)ρlin = (cid:104)q  q(cid:48)(cid:105) /k. This effect can be seen easily in the limit ρ → 1 in which case
Biasρ((cid:98)ρnorm) → 0 by construction. In general  bias and variance can be evaluated as follows.

Normalized estimator. In the full precision case we have seen that simple normalization of the

Proposition 1. In terms of the coefﬁcients θα β deﬁned in (11)  as k → ∞  we have

| Biasρ[(cid:98)ρnorm]| =(cid:12)(cid:12) θ1 1
(cid:16) θ2 2
Var((cid:98)ρnorm) = 1

θ2 0

θ2
2 0

k

− ρ(cid:12)(cid:12) + O(k−1)

− 2θ1 1θ3 1

θ3
2 0

+

(cid:17)

θ2
1 1(θ4 0+θ2 2)

2θ4

2 0

+ O(k−2).

Figure 4 (L M) graphs the above two expressions. In particular  the plots highlight the reduction

in bias compared to (cid:98)ρlin and the fact that the variance is decreasing in ρ as for b = ∞. While

Proposition 1 is asymptotic  we verify a tight agreement in simulations for reasonably small k
(cf. supplement).

Figure 4: (L): Asymptotic Bias2

ρ((cid:98)ρlin). (M): Varρ((cid:98)ρnorm) (asymptotic  to be
divided by k). (R): MSEs of(cid:98)ρlin 4 vs. the MSEs of(cid:98)ρcoll 2 using twice the number of RPs (comparison

ρ((cid:98)ρnorm) relative to Bias2

at the bit level). The stars indicate the values of ρ at which the MSEs of the two estimators are equal.

3.3 Coding-based estimation vs. Collision-based estimation

approaches (for ﬁxed k) intersect are indicated by stars. As k decreases from 104 to 102  these values

Both schemes are comparable in terms of simplicity  but at the level of statistical performance none
of the two dominates the other. The collision-based approach behaves favorably in a high similarity

regime as shows a comparison of MSEρ((cid:98)ρcol) (b = 2) and MSEρ((cid:98)ρnorm) (b = 4) at the bit level
(Figure 4 (R)): since(cid:98)ρcol uses only two bits for each of the k RPs  while(cid:98)ρnorm uses twice as many
bits  we have doubled the number of RPs for(cid:98)ρcol. The values of ρ for which the curves of the two
increase from about ρ = 0.55 to ρ = 0.95. In conclusion (cid:98)ρcol is preferable in applications in which
Figure 1 (L) shows that as b is raised (cid:98)ρcol requires ρ to be increasingly closer to one to achieve lower

high similarities prevail  e.g.  in duplicate detection. On the other hand  for generic high-dimensional
data  one would rather not expect ρ to take high values given that two points drawn uniformly at
random from the sphere are close to orthogonal with high probability.

MSE. By contrast  increasing b for the coding-based schemes yields improvements essentially for the

6

00.20.40.60.81-4-3.5-3-2.5-2-1.5-1log10(MSE)k = 20k = 50k = 100k = 200k = 500k = 1000k = 2000k = 5000k = 10000b = 300.20.40.60.81-4-3.5-3-2.5-2-1.5-1log10(MSE)k = 20k = 50k = 100k = 200k = 500k = 1000k = 2000k = 5000k = 10000b = 400.20.40.60.81-4-3.5-3-2.5-2-1.5-1log10(MSE)k = 20k = 50k = 100k = 200k = 500k = 1000k = 2000k = 5000k = 10000b = 500.20.40.60.81-11-10-9-8-7-6-5-4-3-2-1log10(squared Bias)b = 2b = 3b = 4b = 5b = 600.20.40.60.8100.20.40.60.81varianceb = 3b = 2b = 00.20.40.60.81-5-4.5-4-3.5-3-2.5-2-1.5-1log10(MSE)k = 20k = 50k = 100k = 200k = 500k = 1000k = 2000k = 5000k = 10000whole range of ρ. An interesting phenomenon occurs in the limit ρ → 1. It turns out that the rate of

decay of Varρ((cid:98)ρnorm) is considerably slower than the rate of decay of Varρ((cid:98)ρcol).

Theorem 2. For any ﬁnite b  we have

Varρ((cid:98)ρnorm) = Θ((1 − ρ)1/2) 

Varρ((cid:98)ρcol) = Θ((1 − ρ)3/2) as ρ → 1.

The rate Θ((1 − ρ)3/2) is the same as the MLE [28] which is slower than the rate Θ((1 − ρ)2) in
the full precision case (cf. §2). We conjecture that the rate Θ((1 − ρ)1/2) is intrinsic to code-based
estimation as this rate is also obtained when computing the full precision MLE (3) with quantized
data (i.e.  z  z(cid:48) gets replaced by q  q(cid:48)).

3.4 Quantization of norms
Let us recall that according to our basic setup in §1  we have assumed so far that it is possible to
2  i ∈ [n]  of the original data prior to projection and quantization  and
compute the norms λi = (cid:107)xi(cid:107)2
store them in full precision to approximately recover inner products and squared distances via
where(cid:98)ρij is an estimate of the cosine similarity of xi and xj. Depending on the setting  it may be
tightly bounded in terms of the MSE for estimating cosine similarities and max1≤i≤n |(cid:98)λi− λi|  where
{(cid:98)λi}n

i=1 as well. It turns out that the MSE for estimating distances can be

i=1 denote the quantized versions of {λi}n

(cid:104)xi  xj(cid:105) ≈ λiλj(cid:98)ρij 

i=1; the precise bound is stated in the supplement.

required to quantize the {λi}n

(cid:107)xi − xj(cid:107)2

2 ≈ λ2

i + λ2

j − 2λiλj(cid:98)ρij 

4 Empirical results: linear classiﬁcation using quantized RPs

One traditional application of RPs is dimension reduction in linear regression or classiﬁcation with
high-dimensional predictors [14  30]. The results of §3.2 suggest that as long as the number of RPs
k are no more than a few thousand  subsequent scalar quantization to four bits is not expected to
have much of a negative effect relative to using full precision data. In this section  we verify this
hypothesis for four high-dimensional data sets from the UCI repository: arcene (d = 104)  Dexter
(d = 2 · 104)  farm (d = 5.5 · 104) and PEMS (d = 1.4 · 105).
Setup. All data points are scaled to unit Euclidean norm before dimension reduction and scalar
quantization based on the Lloyd-Max quantizer (10). The number of RPs k is varied according to
{26  27  . . .   212}. For each of these values for k  we consider 20 independent realizations of the
random projection matrix A. Given projected and quantized data {q1  . . .   qn}  we estimate the

underlying cosine similarities ρij as (cid:98)ρij = (cid:98)ρ(qi  qj)  i  j ∈ [n]  where (cid:98)ρ(qi  qj) is a placeholder
for either the collision-based estimator(cid:98)ρcoll based on b = 2 bits or the normalized estimator(cid:98)ρnorm
reference. The {(cid:98)ρij}1≤i j≤n are then used as a kernel matrix fed into LIBSVM [9] to train a binary
for b ∈ {1  2  4 ∞} using data {qi(l)   qj(l)}k
classiﬁer. Prediction on test sets is performed accordingly. LIBSVM is run with 30 different values of
its tuning parameter C ranging from 10−3 to 104.
Results. A subset of the results is depicted in Figure 5 which is composed of three columns (one for
each type of plot) and four rows (one for each data set). All results are averages over 20 independent
sets of random projections. The plots in the left column show the minimum test errors over all 30
choices of the tuning parameter C under consideration in dependency of the number of RPs k. The
plots in the middle column show the test errors in dependency of C for a selected value of k (the full
set of plots can be found in the supplement). The plots in the right column provide a comparison of

the minimum (w.r.t. C) test errors of(cid:98)ρcoll 2 and(cid:98)ρnorm 4 at the bit level  i.e.  with k doubled for(cid:98)ρcoll 2.
(cid:98)ρcoll 2 and(cid:98)ρnorm 4  the latter consistently achieves better performance.

In all plots  classiﬁcation performance improves as b increases. What is more notable though is that
the gap between b = 4 and b = ∞ is indeed minor as anticipated. Regarding the performance of

l=1; one-bit quantization (b = 1) is here included as a

5 Conclusion

In this paper  we have presented theoretical and empirical evidence that it is possible to achieve
additional data compression in the use of random projections by means of coarse scalar quantization.

7

Figure 5: Results of the classiﬁcation experiments. Each row corresponds to one data set. (L):
Accuracy on the test set (optimized over C) in dependence of the number of RPs k (log2 scale). (M):
Accuracy on the test set for a selected value of k in dependence of log10(C). (R): Comparison of the

test accuracies when using the estimators(cid:98)ρnorm 4 respectively(cid:98)ρcoll 2 with twice the number of RPs.

The loss of information incurred at this step tends to be mild even with the naive approach in which
quantized data are treated in the same way as their full precision counterparts. An exception only
arises for cosine similarities close to 1 (Theorem 2). We have also shown that the simple form of

normalization employed in the construction of the estimator(cid:98)ρnorm can be extremely beneﬁcial  even

more so for coarsely quantized data because of a crucial bias reduction.
Regarding future work  it is worthwhile to consider the extension to the case in which the random
projections are not Gaussian but arise from one of the various structured Johnson-Lindenstrauss
transforms  e.g.  those in [2  3  23]. A second direction of interest is to analyze the optimal trade-off
between the number of RPs k and the bit depth b in dependence of the similarity ρ; in the present
work  the choice of b has been driven with the goal of roughly matching the full precision case.

8

6789101112log2(k)0.630.660.690.720.750.780.81accuracy on test setPEMS-4-2024log10(C)0.20.30.40.50.60.7accuracy on test setPEMS  k = 6467891011log2(k)0.40.450.50.550.60.650.70.750.8accuracy on test setPEMS6789101112log2(k)0.60.650.70.750.80.850.9accuracy on test setDexter-4-2024log10(C)0.550.60.650.70.750.8accuracy on test setDexter  k = 51267891011log2(k)0.60.650.70.750.80.850.9accuracy on test setDexter6789101112log2(k)0.70.750.80.850.9accuracy on test setfarm-4-2024log10(C)0.50.550.60.650.70.75accuracy on test setfarm  k = 6467891011log2(k)0.70.750.80.850.9accuracy on test setfarm6789101112log2(k)0.650.70.750.80.85accuracy on test setarcene-4-2024log10(C)0.550.60.650.70.750.80.85accuracy on test setarcene  k = 51267891011log2(k)0.60.650.70.750.80.85accuracy on test setarceneAcknowledgments

The work was partially supported by NSF-Bigdata-1419210  NSF-III-1360971. Ping Li also thanks
Michael Mitzenmacher for helpful discussions.

References
[1] D. Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins. Journal

of Computer and System Sciences  66:671–687  2003.

[2] N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform.

In Proceedings of the Symposium on Theory of Computing (STOC)  pages 557–563  2006.

[3] N. Ailon and E. Liberty. Almost optimal unrestricted fast Johnson–Lindenstrauss transform. ACM

Transactions on Algorithms  9:21  2013.

[4] T. Anderson. An Introduction to Multivariate Statistical Analysis. Wiley  2003.

[5] E. Bingham and H. Mannila. Random projection in dimensionality reduction: applications to image and

text data. In Conference on Knowledge Discovery and Data Mining (KDD)  pages 245–250  2001.

[6] P. Boufounos and R. Baraniuk. 1-bit compressive sensing. In Information Science and Systems  2008.

[7] C. Boutsidis  A. Zouzias  and P. Drineas. Random Projections for k-means Clustering. In Advances in

Neural Information Processing Systems (NIPS)  pages 298–306. 2010.

[8] E. Candes and T. Tao. Near-optimal signal recovery from random projections: Universal encoding

strategies? IEEE Transactions on Information Theory  52:5406–5425  2006.

[9] C-C. Chang and C-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent

Systems and Technology  2:27:1–27:27  2011. http://www.csie.ntu.edu.tw/~cjlin/libsvm.

[10] M. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the Symposium

on Theory of Computing (STOC)  pages 380–388  2002.

[11] S. Dasgupta. Learning mixtures of Gaussians. In Symposium on Foundations of Computer Science (FOCS) 

pages 634–644  1999.

[12] S. Dasgupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Structures and

Algorithms  22:60–65  2003.

[13] M. Datar  N. Immorlica  P. Indyk  and V. Mirrokni. Locality-Sensitive Hashing Scheme Based on p-Stable

Distributions. In Symposium on Computational Geometry (SCG)  pages 253–262  2004.

[14] D. Fradkin and D. Madigan. Experiments with random projections for machine learning. In Conference on

Knowledge Discovery and Data Mining (KDD)  pages 517–522  2003.

[15] A. Gersho and R. Gray. Vector Quantization and Signal Compression. Springer  1991.

[16] M. Goemans and D. Williamson. Improved Approximation Algorithms for Maximum Cut and Satisﬁability

Problems Using Semideﬁnite Programming. Journal of the ACM  42:1115–1145  1995.

[17] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.

In Proceedings of the Symposium on Theory of Computing (STOC)  pages 604–613  1998.

[18] J. Matousek. On variants of the Johnson-Lindenstrauss lemma. Random Structures and Algorithms 

33:142–156  2008.

[19] L. Jacques. A Quantized Johnson-Lindenstrauss Lemma: The Finding of Buffon’s needle. IEEE Transac-

tions on Information Theory  61:5012–5027  2015.

[20] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary

Mathematics  pages 189–206  1984.

[21] K. Kenthapadi  A. Korolova  I. Mironov  and N. Mishra. Privacy via the Johnson-Lindenstrauss Transform.

Journal of Privacy and Conﬁdentiality  5  2013.

[22] J. Kieffer. Uniqueness of locally optimal quantizer for log-concave density and convex error weighting

function. IEEE Transactions on Information Theory  29:42–47  1983.

9

[23] F. Krahmer and R. Ward. New and improved Johnson-Lindenstrauss embeddings via the Restricted

Isometry Property. SIAM Journal on Mathematical Analysis  43:1269–1281  2011.

[24] J. Laska and R. Baraniuk. Regime change: Bit-depth versus measurement-rate in compressive sensing.

IEEE Transactions on Signal Processing  60:3496–3505  2012.

[25] M. Li  S. Rane  and P. Boufounos. Quantized embeddings of scale-invariant image features for mobile
augmented reality. In International Workshop on Multimedia Signal Processing (MMSP)  pages 1–6  2012.

[26] P. Li  T. Hastie  and K. Church. Improving Random Projections Using Marginal Information. In Annual

Conference on Learning Theory (COLT)  pages 635–649  2006.

[27] P. Li  M. Mitzenmacher  and A. Shrivastava. Coding for Random Projections. In Proceedings of the

International Conference on Machine Learning (ICML)  pages 676–678  2014.

[28] P. Li  M. Mitzenmacher  and M. Slawski. Quantized Random Projections and Non-Linear Estimation of
Cosine Similarity. In Advances in Neural Information Processing Systems (NIPS)  pages 2756–2764. 2016.

[29] M. Mahoney. Randomized Algorithms for Matrices and Data. Foundations and Trends in Machine

Learning  3:123–224  2011.

[30] O. Maillard and R. Munos. Compressed least-squares regression. In Advances in Neural Information

Processing Systems (NIPS)  pages 1213–1221. 2009.

[31] S. Rane and P. Boufounos. Privacy-preserving nearest neighbor methods: Comparing signals wihtout

revealing them. IEEE Signal Processing Magazine  30:18–28  2013.

[32] S. Rane  P. Boufounos  and A. Vetro. Quantized embeddings: An efﬁcient and universal nearest neighbor
method for cloud-based image retrieval. In SPIE Optical Engineering and Applications  pages 885609–
885609. International Society for Optics and Photonics  2013.

[33] S. Vempala. The Random Projection Method. American Mathematical Society  2005.

10

,Ping Li
Virag Shah
Jose Blanchet
Ramesh Johari