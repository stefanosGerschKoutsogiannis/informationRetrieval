2007,Efficient Convex Relaxation for Transductive Support Vector Machine,We consider the problem of Support Vector Machine transduction  which involves a combinatorial problem with exponential computational complexity in the number of unlabeled examples. Although several studies are devoted to Transductive SVM  they suffer either from the high computation complexity or from the solutions of local optimum. To address this problem  we propose solving Transductive SVM via a convex relaxation  which converts the NP-hard problem to a semi-definite programming. Compared with the other SDP relaxation for Transductive SVM  the proposed algorithm is computationally more efficient with the number of free parameters reduced from O(n2) to O(n) where n is the number of examples. Empirical study with several benchmark data sets shows the promising performance of the proposed algorithm in comparison with other state-of-the-art implementations of Transductive SVM.,Efﬁcient Convex Relaxation for

Transductive Support Vector Machine

Zenglin Xu

Dept. of Computer Science & Engineering

The Chinese University of Hong Kong

Shatin  N.T.  Hong Kong

Rong Jin

Dept. of Computer Science & Engineering

Michigan State University
East Lansing  MI  48824

zlxu@cse.cuhk.edu.hk

rongjin@cse.msu.edu

Jianke Zhu

Irwin King

Michael R. Lyu

Dept. of Computer Science & Engineering

The Chinese University of Hong Kong

Shatin  N.T.  Hong Kong

fjkzhu king lyug@cse.cuhk.edu.hk

Abstract

We consider the problem of Support Vector Machine transduction  which involves
a combinatorial problem with exponential computational complexity in the num-
ber of unlabeled examples. Although several studies are devoted to Transductive
SVM  they suffer either from the high computation complexity or from the so-
lutions of local optimum. To address this problem  we propose solving Trans-
ductive SVM via a convex relaxation  which converts the NP-hard problem to a
semi-deﬁnite programming. Compared with the other SDP relaxation for Trans-
ductive SVM  the proposed algorithm is computationally more efﬁcient with the
number of free parameters reduced from O(n2) to O(n) where n is the number of
examples. Empirical study with several benchmark data sets shows the promising
performance of the proposed algorithm in comparison with other state-of-the-art
implementations of Transductive SVM.

1 Introduction

Semi-supervised learning has attracted an increasing amount of research interest recently [3  15]. An
important semi-supervised learning paradigm is the Transductive Support Vector Machine (TSVM) 
which maximizes the margin in the presence of unlabeled data and keeps the boundary traversing
through low density regions  while respecting labels in the input space.

Since TSVM requires solving a combinatorial optimization problem  extensive research efforts have
been devoted to efﬁciently ﬁnding the approximate solution to TSVM. The popular version of TSVM
proposed in [8] uses a label-switching-retraining procedure to speed up the computation. In [5]  the
hinge loss in TSVM is replaced by a smooth loss function  and a gradient descent method is used
to ﬁnd the decision boundary in a region of low density. Chapelle et al. [2] employ an iterative
approach for TSVM. It begins with minimizing an easy convex object function  and then gradu-
ally approximates the objective of TSVM with more complicated functions. The solution of the
simple function is used as the initialization for the solution to the complicated function. Other it-
erative methods  such as deterministic annealing [11] and the concave-convex procedure (CCCP)
method [6]  are also employed to solve the optimization problem related to TSVM. The main draw-
back of the approximation methods listed above is that they are susceptible to local optima  and
therefore are sensitive to the initialization of solutions. To address this problem  in [4]  a branch-

Time Comparison

 

CTSVM
RTSVM

2000

1800

1600

1400

1200

1000

800

600

400

200

)
s
d
n
o
c
e
s
(
 

e
m
T

i

0

 

50

100

150

200

250

300

Number of Samples

Figure 1: Computation time of the proposed convex relaxation approach for TSVM (i.e.  CTSVM)
and the semi-deﬁnite relaxation approach for TSVM (i.e.  RTSVM) versus the number of unlabeled
examples. The Course data set is used  and the number of labeled examples is 20.

and-bound search method is developed to ﬁnd the exact solution. In [14]  the authors approximate
TSVM by a semi-deﬁnite programming problem  which leads to a relaxation solution to TSVM
(noted as RTSVM)  to avoid the solution of local optimum. However  both approaches suffer from
the high computational cost and can only be applied to small sized data sets.
To this end  we present the convex relaxation for Transductive SVM (CTSVM). The key idea of our
method is to approximate the non-convex optimization problem of TSVM by its dual problem. The
advantage of doing so is twofold:

† Unlike the semi-deﬁnite relaxation [14] that approximates TSVM by dropping the rank
constraint  the proposed approach approximates TSVM by its dual problem. As the basic
result of convex analysis  the conjugate of conjugate of any function f (x) is the convex en-
velope of f (x)  and therefore provides a tighter convex relaxation for f (x) [7]. Hence  the
proposed approach provides a better convex relaxation than that in [14] for the optimization
problem in TSVM.

† Compared to the semi-deﬁnite relaxation TSVM  the proposed algorithm involves fewer
free parameters and therefore signiﬁcantly improves the efﬁciency by reducing the worst-
case computational complexity from O(n6:5) to O(n4:5). Figure 1 shows the running time
of both the semi-deﬁnite relaxation of TSVM in [14] and the proposed convex relaxation for
TSVM versus increasing number of unlabeled examples. The data set used in this example
is the Course data set (see the experiment section)  and the number of labeled examples
is 20. We clearly see that the proposed convex relaxation approach is considerably more
efﬁcient than the semi-deﬁnition approach.

The rest of this paper is organized as follows. Section 2 reviews the related work on the semi-
deﬁnite relaxation for TSVM. Section 3 presents the convex relaxation approach for Transductive
SVM. Section 4 presents the empirical studies that verify the effectiveness of the proposed relaxation
for TSVM. Section 5 sets out the conclusion.

2 Related Work

In this section  we review the key formulae for Transductive SVM  followed by the semi-deﬁnite
programming relaxation for TSVM.
Let X = (x1; : : : ; xn) denote the entire data set  including both the labeled examples and the
unlabeled ones. We assume that the ﬁrst l examples within X are labeled by y‘ = (y‘
2; : : : ; y‘
l )
where y‘
i 2 f¡1; +1g represents the binary class label assigned to xi. We further denote by y =
(y1; y2; : : : ; yn) 2 f¡1; +1gn the binary class labels predicted for all the data points in X . The goal
of TSVM is to estimate y by using both the labeled examples and the unlabeled ones.

1; y‘

Following the framework of maximum margin  TSVM aims to identify the classiﬁcation model that
will result in the maximum classiﬁcation margin for both labeled and unlabeled examples  which
amounts to solve the following optimization problem:

n

min

w;b;y2f¡1;+1gn;"

kwk2

2 + C

"i

Xi=1

s. t.

yi(w>xi ¡ b) ‚ 1 ¡ "i; "i ‚ 0; i = 1; 2; : : : ; n
yi = y‘

i ; i = 1; 2; : : : ; l;

where C ‚ 0 is the trade-off parameter between the complexity of function w and the margin errors.
The prediction function can be formulated as f (x) = w>x ¡ b.
Evidently  the above problem is a non-convex optimization problem due to the product term yiwj in
the constraint. In order to approximate the above problem into a convex programming problem  we
ﬁrst rewrite the above problem into the following form using the Lagrange Theorem:

min

”;y2f¡1;+1gn;–;‚

1
2

(e + ” ¡ – + ‚y)>D(y)K¡1D(y)(e + ” ¡ – + ‚y) + C–>e

(1)

s. t.

” ‚ 0;

– ‚ 0;

yi = y‘

i ; i = 1; 2; : : : ; l;

where ”  – and ‚ are the dual variables. e is the n-dimensional column vector of all ones and K is
the kernel matrix. D(y) represents a diagonal matrix whose diagonal elements form the vector y.
Detailed derivation can be found in [9  13]. Using the Schur complement  the above formulation can
be further formulated as follows:

min

y2f¡1;+1gn;t;”;–;‚

t

(2)

s. t.

(cid:181)

yy> – K

(e + ” ¡ – + ‚y)>
” ‚ 0; – ‚ 0; yi = y‘

e + ” ¡ – + ‚y

t ¡ 2C–>e ¶ ” 0

i ; i = 1; 2; : : : ; l;

where the operator – represents the element wise product.
To convert the above problem into a convex optimization problem  the key idea is to replace the
quadratic term yy> by a linear variable. Based on the result that the set Sa = fM = yy>jy 2
f¡1; +1gng is equivalent to the set Sb = fMjMi;i = 1; rank(M) = 1g  we can approximate the
problem in (2) as follows:

min

M;t;”;–;‚

t

(3)

s. t.

(cid:181) M – K
(e + ” ¡ –)> t ¡ 2C–>e ¶ ” 0

e + ” ¡ –

” ‚ 0; – ‚ 0;
M ” 0; Mi;i = 1; i = 1; 2; : : : ; n;

where Mij = y‘

i y‘

j for 1 • i; j • l.

Note that the key differences between (2) and (3) are (a) the rank constraint rank(M) = 1 is re-
moved  and (b) the variable ‚ is set to be zero  which is equivalent to setting b = 0. The above
approximation is often referred to as the Semi-Deﬁnite Programming (SDP) relaxation. As re-
vealed by the previous studies [14  1]  the SDP programming problem resulting from the approx-
imation is computationally expensive. More speciﬁcally  there are O(n2) parameters in the SDP
cone and O(n) linear inequality constraints  which implies a worst-case computational complexity
of O(n6:5). To avoid the high computational complexity  we present a different approach for relax-
ing TSVM into a convex problem. Compared to the SDP relaxation approach  it is advantageous
in that (1) it produces the best convex approximation for TSVM  and (2) it is computationally more
efﬁcient than the previous SDP relaxation.

3 Relaxed Transductive Support Vector Machine

In this section  we follow the work of generalized maximum margin clustering [13] by ﬁrst studying
the case of hard margin  and then extending it to the case of soft margin.

3.1 Hard Margin TSVM

In the hard margin case  SVM does not penalize the classiﬁcation error  which corresponds to – = 0
in (1). The resulting formulism of TSVM becomes

min
”;y;‚
s: t:

(e + ” + ‚y)>D(y)K¡1D(y)(e + ” + ‚y)

1
2
” ‚ 0;
yi = y‘
y2
i = 1; i = l + 1; l + 2; : : : ; n:

i ; i = 1; 2; : : : ; l;

(4)

Instead of employing the SDP relaxation as in [14]  we follow the work in [13] and introduce a
variable z = D(y)(e + ”) = y – (e + ”). Given that ” ‚ 0  the constraints in (4) can be written
as y‘
i ‚ 1 for all the unlabeled examples. Hence  z can be
used as the prediction function  i.e.  f ⁄ = z. Using this new notation  the optimization problem in
(4) can be rewritten as follows:

i zi ‚ 1 for the labeled examples  and z2

min
z;‚
s. t.

(z + ‚e)>K¡1(z + ‚e)

1
2
y‘
i zi ‚ 1; i = 1; 2; : : : ; l;
z2
i ‚ 1; i = l + 1; l + 2; : : : ; n:

(5)

One problem with Transductive SVMs is that it is possible to classify all the unlabeled data to one of
the classes with a very large margin due to the high dimension and few labeled data. This will lead
to poor generalization ability. To solve this problem  we introduce the following balance constraint
to ensure that no class takes all the unlabeled examples:

¡† •

1
l

zi ¡

1

n ¡ l

l

Xi=1

n

Xi=l+1

zi • †;

(6)

where † ‚ 0 is a constant. Through the above constraint  we aim to ensure that the difference
between the labeled data and the unlabeled data in their class assignment is small.

To simplify the expression  we further deﬁne w = (z; ‚) 2 Rn+1 and P = (In; e) 2 Rn£(n+1).
Then  the problem in (5) becomes:

min

w
s. t.

w>P>K¡1Pw

y‘
i wi ‚ 1; i = 1; 2; : : : ; l;
w2
i ‚ 1; i = l + 1; l + 2; : : : ; n;

(7)

¡† •

1
l

wi ¡

1

n ¡ l

l

Xi=1

n

Xi=l+1

wi • †:

When this problem is solved  the label vector y can be directly determined by the sign of the pre-
diction function  i.e.  sign(w). This is because wi = (1 + ”)yi for i = l + 1; : : : ; n and ” ‚ 0.
The following theorem shows that the problem in (7) can be relaxed to a semi-deﬁnite programming.
Theorem 1. Given a sample X = fx1; : : : ; xng and a partial set of the labels y‘ = (y‘
2; : : : ; y‘
l )
where 1 • l • n  the variable w that optimizes (7) can be calculated by

1; y‘

w =

1
2

[A ¡ D((cid:176) – b)]¡1 ((cid:176) – a ¡ (ﬁ ¡ ﬂ)c);

(8)

where a = (yl; 0n¡l; 0) 2 Rn+1  b = (0l; 1n¡l; 0) 2 Rn+1  c = ( 1
A = P>K¡1P  and (cid:176) is determined by the following semi-deﬁnite programming:

l 1l; ¡ 1

u 1n¡l; 0) 2 Rn+1 

(cid:176)i ¡ †(ﬁ + ﬂ)

(9)

max
(cid:176);t;ﬁ;ﬂ

s: t:

¡

1
4

t +

n

Xi=1

(cid:181)

A ¡ D((cid:176) – b)

(cid:176) – a ¡ (ﬁ ¡ ﬂ)c;

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)>

t

ﬁ ‚ 0; ﬂ ‚ 0; (cid:176)i ‚ 0; i = 1; 2; : : : ; n:

¶ ” 0

Proof Sketch. We deﬁne the Lagrangian of the minimization problem (7) as follows:

min

w

max

(cid:176)

F(w; (cid:176)) = w>P>K¡1Pw +

(cid:176)i(1 ¡ y‘

i wi) +

l

Xi=1

(cid:176)i(1 ¡ w2
i )

n

Xi=l+1

+ﬁ(c>w ¡ †) + ﬂ(¡c>w ¡ †);

where (cid:176)i ‚ 0 for i = 1; : : : ; n. It can be derived from the duality that minw max(cid:176) F(w; (cid:176)) =
max(cid:176) minw F(w; (cid:176)):
At the optimum  the derivatives of F with respect to the variable w are derived as below:

@F
@w

= 2 [A ¡ D((cid:176) – b)] w ¡ (cid:176) – a + (ﬁ ¡ ﬂ)c = 0;

where A = P>K¡1P. The inverse of A¡D((cid:176)–b) can be computed through adding a regularization
parameter. Therefore  w is able to be calculated by:

w =

1
2

[A ¡ D((cid:176) – b)]¡1 ((cid:176) – a ¡ (ﬁ ¡ ﬂ)c):

Thus  the dual form of the problem becomes:

max

(cid:176)

L((cid:176)) = ¡

1
4

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)> [A ¡ D(b – (cid:176))]¡1 ((cid:176) – a ¡ (ﬁ ¡ ﬂ)c) +

We import a variable t  so that

(cid:176)i ¡ †(ﬁ + ﬂ);

n

Xi=1

¡

1
4

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)>[A ¡ D(b – (cid:176))]¡1((cid:176) – a ¡ (ﬁ ¡ ﬂ)c) ‚ ¡t:

According to the Schur Complement  we obtain a semi-deﬁnite programming cone  from which the
optimization problem (9) can be formulated. ¥
Remark I. The problem in (9) is a convex optimization problem  more speciﬁcally  a semi-deﬁnite
programming problem  and can be efﬁciently solved by the interior-point method [10] implemented
in some optimization packages  such as SeDuMi [12]. Besides  our relaxation algorithm has O(n)
parameters in the SDP cone and O(n) linear equality constraints  which involves a worst-case com-
putational complexity of O(n4:5). However  in the previous relaxation algorithms [1  14]  there
are approximately O(n2) parameters in the SDP cone  which involve a worst-case computational
complexity in the scale of O(n6:5). Therefore  our proposed convex relaxation algorithm is more
efﬁcient. In addition  as analyzed in Section 2  the approximation in [1  14] drops the rank constraint
of the matrix y>y  which does not lead to a tight approximation. On the other hand  our prediction
function f ⁄ implements the conjugate of conjugate of the prediction function f (x)  which is the
convex envelope of f (x) [7]. Thus  our proposed convex approximation method provides a tighter
approximation than the previous method.
Remark II. It is interesting to discuss the connection between the solution of the proposed algorithm
and that of harmonic functions. We consider a special case of (8)  where ‚ = 0 (which implies no
bias term in the primal SVM)  and there is no balance constraint. Then the solution of (9) can be
expressed as follows:

(10)

(11)

It can be further derived as follows:

1

z =

2£K¡1 ¡ D((cid:176) – (0l; 1n¡l))⁄¡1
n!¡1ˆ l
z =ˆIn ¡
Xi=1

Xi=l+1

(cid:176)iKIi

n

((cid:176) – (yl; 0n¡l)):

(cid:176)iy‘

i K(xi; ¢)! ;

where Ii
n is deﬁned as an n £ n matrix with all elements being zero except the i-th diagonal el-
ement which is 1  and K(xi; ¢) is the i-th column of K. Similar to the solution of the harmonic
function  we ﬁrst propagate the class labels from the labeled examples to the unlabeled one by term

Pl

i=1 (cid:176)iy‘

i K(xi; ¢)  and then adjust the prediction labels by the factor ¡In ¡Pn

The key difference in our solution is that (1) different weights (i.e.  (cid:176)i) are assigned to the labeled
examples  and (2) the adjustment factor is different to that in the harmonic function [16].

i=l+1 (cid:176)iKIi

n¢¡1.

3.2 Soft Margin TSVM

We extend TSVM to the case of soft margin by considering the following problem:

min
”;y;–;‚

s: t:

1
2

(e + ” ¡ – + ‚y)>D(y)K¡1D(y)(e + ” ¡ – + ‚y) + C‘

” ‚ 0; – ‚ 0;
yi = y‘
y2
i = 1; l + 1 • i • n;

i ; 1 • i • l;

–2
i + Cu

l

Xi=1

–2
i

n

Xi=l+1

where –i is related to the margin error. Note that we distinguish the labeled examples from the
unlabeled examples by introducing different penalty constants for margin errors  C‘ for labeled
examples and Cu for unlabeled examples.
Similarly  we introduce the slack variable z  and then derive the following dual problem:

(cid:176)i ¡ †(ﬁ + ﬂ)

(12)

max
(cid:176);t;ﬁ;ﬂ

s: t:

¡

1
4

t +

n

Xi=1

(cid:181)

A ¡ D((cid:176) – b)

(cid:176) – a ¡ (ﬁ ¡ ﬂ)c

((cid:176) – a ¡ (ﬁ ¡ ﬂ)c)>

t

¶ ” 0;

0 • (cid:176)i • C‘; i = 1; 2; : : : ; l;
0 • (cid:176)i • Cu; i = l + 1; l + 2; : : : ; n;
ﬁ ‚ 0; ﬂ ‚ 0;

which is also a semi-deﬁnite programming problem and can be solved similarly.

4 Experiments

In this section  we report empirical study of the proposed method on several benchmark data sets.

4.1 Data Sets Description

To make evaluations comprehensive  we have collected four UCI data sets and three text data sets
as our experimental testbeds. The UCI data sets include Iono  sonar  Banana  and Breast  which are
widely used in data classiﬁcation. The WinMac data set consists of the classes  mswindows and
mac  of the Newsgroup20 data set. The IBM data set contains the classes  IBM and non-IBM  of the
Newsgroup20 data set. The course data set is made of the course pages and non-course pages of the
WebKb corpus. For each text data set  we randomly sample the data with the sample size of 60  300
and 1000  respectively. Each resulted sample is noted by the sufﬁx  “-s”  “-m”  or “-l” depending on
whether the sample size is small  medium or large. Table 1 describes the information of these data
sets  where d represents the data dimensionality  l means the number of labeled data points  and n
denotes the total number of examples.

Table 1: Data sets used in the experiments  where d represents the data dimensionality  l means the
number of labeled data points  and n denotes the total number of examples.
l
20
20
20
50
50
50

n
351 WinMac-m 7511
11960
208
IBM-m
400
1800
Course-m
7511
300 WinMac-l
11960
60
60
1800

Data set
Iono
Sonar
Banana
Breast
IBM-s
Course-s

n
300
300
300
1000
1000
1000

l
20
20
20
20
10
10

Data set

d

IBM-l
Course-l

d
34
60
4
9

11960
1800

4.2 Experimental Protocol

To evaluate the effectiveness of the proposed CTSVM method  we choose the conventional SVM
as our baseline method. In our experiments  we also make comparisons with three state-of-the-art

methods: the SVM-light algorithm [8]  the Gradient Decent TSVM (rTSVM) algorithm [5]  and
the Concave Convex Procedure (CCCP) [6]. Since the SDP approximation TSVM [14] has very
high time complexity O(n6:5)  which is difﬁcult to process data sets with hundreds of examples.
Thus  it is only evaluated on the smaller data sets  i.e.  “IBM-s” and “Course-s”.

The experiment setup is described as follows. For each data set  we conduct 10 trials. In each trial 
the training set contains each class of data  and the remaining data are then used as the unlabeled
(test) data. Moreover  the RBF kernel is used for “Iono”  “Sonar” and “Banana”  and the linear
kernel is used for the other data sets. This is because the linear kernel performs better than the RBF
kernel on these data sets. The kernel width of RBF kernel is chosen by 5-cross validation on the
labeled data. The margin parameter C‘ is tuned by using the labeled data in all algorithms. Due to
the small number of labeled examples  for CTSVM and CCCP  the margin parameter for unlabeled
data  Cu  is set equal to C‘. Other parameters in these algorithms are set to the default values
according to the relevant literatures.

4.3 Experimental Results

SVM

SVM-light
78.25§0.36
55.26§5.88

-

Table 2: The classiﬁcation performance of Transductive SVMs on benchmark data sets.
Data Set
CTSVM
78.55§4.83
80.09§2.63
Iono
51.76§5.05
67.39§6.26
Sonar
79.51§3.02
58.45§7.15
Banana
97.79§0.23
96.46§1.18
Breast
75.25§7.49
52.75§15.01
IBM-s
79.75§8.45
Course-s
63.52§5.82
84.82§2.12
WinMac-m 57.64§9.58
73.17§0.89
IBM-m
53.00§6.83
92.92§2.28
80.18§1.27
Course-m
91.25§2.67
60.86§10.10
WinMac-l
73.42§3.23
61.82§7.26
IBM-l
94.62§0.97
Course-l
83.56§3.10

rTSVM
81.72§4.50
69.36§4.69
71.54§7.28
97.17§0.35
65.80§6.56
75.80§12.87
81.03§8.23
64.65§13.38
90.35§3.59
90.19§2.65
73.11§1.99
93.58§2.68

95.68§1.82
67.60§9.29
76.82§4.78
79.42§4.60
67.55§6.74
93.89§1.49
89.81§2.10
75.40§2.26
92.35§3.02

CCCP

82.11§3.83
56.01§6.70
79.33§4.22
96.89§0.67
65.62§14.83
74.20§11.50
84.28§8.84
69.62§11.03
88.78§2.87
91.00§2.42
74.80§1.87
91.32§4.08

Table 2 summarizes the classiﬁcation accuracy and the standard deviations of the proposed algo-
rithm  the baseline method and the state-of-the-art methods. It can be observed that our proposed
algorithm performs signiﬁcantly better than the standard SVM across all the data sets. Moreover  on
the small-size data sets  i.e.  “IBM-s” and “Course-s”  the results of the SDP-relaxation method are
68.57§22.73 and 64.03§7.65  which are worse than the proposed CTSVM method. In addition  the
proposed CTSVM algorithm performs much better than other TSVM methods over “WinMac-m”
and “Course-l”. As shown in Table 2  the SVM-light algorithm achieves the best results on “Course-
m” and “IBM-l”  however  it fails to converge on “Banana”. On the remaining data sets  comparable
results have been obtained for our proposed algorithm. From above  the empirical evaluations in-
dicate that our proposed CTSVM method achieves promising classiﬁcation results comparing with
the state-of-the-art methods.

5 Conclusion and Future Work

This paper presents a novel method for Transductive SVM by relaxing the unknown labels to the
continuous variables. In contrast to the previous relaxation method which involves O(n2) free pa-
rameters in the semi-deﬁnite matrix  our method takes the advantages of reducing the number of
free parameters to O(n)  and can solve the optimization problem more efﬁciently. In addition  the
proposed approach provides a tighter convex relaxation for the optimization problem in TSVM. Em-
pirical studies on benchmark data sets demonstrate that the proposed method is more efﬁcient than
the previous semi-deﬁnite relaxation method and achieves promising classiﬁcation results compar-
ing to the state-of-the-art methods.

As the current model is only designed for a binary-classiﬁcation  we plan to develop a multi-class
Transductive SVM model in the future. Moreover  it is desirable to extend the current model to
classify the new incoming data.

Acknowledgments

The work described in this paper is supported by a CUHK Internal Grant (No. 2050346) and a grant
from the Research Grants Council of the Hong Kong Special Administrative Region  China (Project
No. CUHK4150/07E).

References

[1] T. D. Bie and N. Cristianini. Convex methods for transduction.

In S. Thrun  L. Saul  and
B. Sch¨olkopf  editors  Advances in Neural Information Processing Systems 16. MIT Press 
Cambridge  MA  2004.

[2] O. Chapelle  M. Chi  and A. Zien. A continuation method for semi-supervised SVMs. In ICML
’06: Proceedings of the 23rd international conference on Machine learning  pages 185–192 
New York  NY  USA  2006. ACM Press.

[3] O. Chapelle  B. Sch¨olkopf  and A. Zien. Semi-Supervised Learning. MIT Press  Cambridge 

MA  2006.

[4] O. Chapelle  V. Sindhwani  and S. Keerthi. Branch and bound for semi-supervised support
vector machines. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Infor-
mation Processing Systems 19. MIT Press  Cambridge  MA  2007.

[5] O. Chapelle and A. Zien. Semi-supervised classiﬁcation by low density separation. In Pro-
ceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics  pages
57–64  2005.

[6] R. Collobert  F. Sinz  J. Weston  and L. Bottou. Large scale transductive SVMs. Journal of

Machine Learning Reseaerch  7:1687–1712  2006.

[7] J.-B. Hiriart-Urruty and C. Lemarechal. Convex analysis and minimization algorithms II:

advanced theory and bundle methods. (2nd part edition). Springer-Verlag  New York  1993.

[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In
ICML ’99: Proceedings of the Sixteenth International Conference on Machine Learning  pages
200–209  San Francisco  CA  USA  1999. Morgan Kaufmann Publishers Inc.

[9] G. R. G. Lanckriet  N. Cristianini  P. Bartlett  L. E. Ghaoui  and M. I. Jordan. Learning the
kernel matrix with semideﬁnite programming. Journal of Machine Learning Research  5:27–
72  2004.

[10] Y. Nesterov and A. Nemirovsky. Interior point polynomial methods in convex programming:

Theory and applications. Studies in Applied Mathematics. Philadelphia  1994.

[11] V. Sindhwani  S. S. Keerthi  and O. Chapelle. Deterministic annealing for semi-supervised
kernel machines. In ICML ’06: Proceedings of the 23rd international conference on Machine
learning  pages 841–848  New York  NY  USA  2006. ACM Press.

[12] J. F. Sturm. Using SeDuMi 1.02  a MATLAB toolbox for optimization over symmetric cones.

Optimization Methods and Software  11:625–653  1999.

[13] H. Valizadegan and R. Jin. Generalized maximum margin clustering and unsupervised kernel
learning. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information
Processing Systems 19. MIT Press  Cambridge  MA  2007.

[14] L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector ma-

chines. In AAAI  pages 904–910  2005.

[15] X. Zhu. Semi-supervised learning literature survey. Technical report  Computer Sciences 

University of Wisconsin-Madison  2005.

[16] X. Zhu  Z. Ghahramani  and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds
In Proceedings of Twentith International Conference on Machine

and harmonic functions.
Learning (ICML-2003)  pages 912–919  2003.

,Elad Hazan
Kfir Levy