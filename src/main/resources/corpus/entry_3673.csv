2016,NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization,We study a stochastic and distributed algorithm for nonconvex  problems whose objective consists a sum $N$ nonconvex $L_i/N$-smooth functions  plus a  nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into $N$ subproblems  and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling  a version of NESTT achieves $\epsilon$-stationary solution  using $\mathcal{O}((\sum_{i=1}^N\sqrt{L_i/N})^2/\epsilon)$ gradient evaluations  which can be up to $\mathcal{O}(N)$ times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex $\ell_1$ penalized quadratic problems with polyhedral constraints. Further  we reveal  a fundamental connection between {\it primal-dual} based methods and a few {\it primal only} methods such as IAG/SAG/SAGA.,NESTT: A Nonconvex Primal-Dual Splitting Method

for Distributed and Stochastic Optimization

Davood Hajinezhad  Mingyi Hong ∗

Tuo Zhao†

Zhaoran Wang‡

Abstract

-stationary solution using O(((cid:80)N

We study a stochastic and distributed algorithm for nonconvex problems whose
objective consists of a sum of N nonconvex Li/N-smooth functions  plus a non-
smooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT)
algorithm splits the problem into N subproblems  and utilizes an augmented
Lagrangian based primal-dual scheme to solve it in a distributed and stochastic
manner. With a special non-uniform sampling  a version of NESTT achieves
can be up to O(N ) times better than the (proximal) gradient descent methods.
It also achieves Q-linear convergence rate for nonconvex (cid:96)1 penalized quadratic
problems with polyhedral constraints. Further  we reveal a fundamental connec-
tion between primal-dual based methods and a few primal only methods such as
IAG/SAG/SAGA.

(cid:112)Li/N )2/) gradient evaluations  which

i=1

Introduction

1
Consider the following nonconvex and nonsmooth constrained optimization problem

min
z∈Z

f (z) :=

1
N

N(cid:88)

i=1

gi(z) + g0(z) + p(z) 

(1.1)

(cid:80)N

i=1 gi(z) for notational simplicity.

where Z ⊆ Rd; for each i ∈ {0 ···   N}  gi : Rd → R is a smooth possibly nonconvex function
which has Li-Lipschitz continuous gradient; p(z) : Rd → R is a lower semi-continuous convex but
possibly nonsmooth function. Deﬁne g(z) := 1
N
Problem (1.1) is quite general. It arises frequently in applications such as machine learning and sig-
nal processing; see a recent survey [7]. In particular  each smooth functions {gi}N
i=1 can represent:
1) a mini-batch of loss functions modeling data ﬁdelity  such as the (cid:96)2 loss  the logistic loss  etc;
2) nonconvex activation functions for neural networks  such as the logit or the tanh functions; 3)
nonconvex utility functions used in signal processing and resource allocation  see [4]. The smooth
function g0 can represent smooth nonconvex regularizers such as the non-quadratic penalties [2]  or
the smooth part of the SCAD or MCP regularizers (which is a concave function) [26]. The convex
function p can take the following form: 1) nonsmooth convex regularizers such as (cid:96)1 and (cid:96)2 func-
tions; 2) an indicator function for convex and closed feasible set Z  denoted as ιZ(·); 3) convex
functions without global Lipschitz continuous gradient  such as p(z) = z4 or p(z) = 1/z + ιz≥0(z).
In this work we solve (1.1) in a stochastic and distributed manner. We consider the setting in which
N distributed agents each having the knowledge of one smooth function {gi}N
i=1  and they are
connected to a cluster center which handles g0 and p. At any given time  a randomly selected agent
is activated and performs computation to optimize its local objective. Such distributed computation
model has been popular in large-scale machine learning and signal processing [6]. Such model
is also closely related to the (centralized) stochastic ﬁnite-sum optimization problem [1  9  14  15 
∗Department of Industrial & Manufacturing Systems Engineering and Department of Electrical & Computer
†School
of
‡Department of Operations Research  Princeton University zhaoran@princeton.edu

Engineering  Iowa State University  Ames  IA  {dhaji mingyi}@iastate.edu

tourzhao@gatech.edu

Georgia

Institute

Industrial

and

Systems

Engineering 

Technology

of

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

N(cid:88)

i=1

21  22]  in which each time the iterate is updated based on the gradient information of a random
component function. One of the key differences between these two problem types is that in the
distributed setting there can be disagreement between local copies of the optimization variable z 
while in the centralized setting only one copy of z is maintained.
Our Contributions. We propose a class of NonconvEx primal-dual SpliTTing (NESTT) algorithms
for problem (1.1). We split z ∈ Rd into local copies of xi ∈ Rd  while enforcing the equality
constraints xi = z for all i. That is  we consider the following reformulation of (1.1)

min
x z∈Rd

(cid:96)(x  z) :=

1
N

gi(xi) + g0(z) + h(z) 

s.t. xi = z  i = 1 ···   N 

(1.2)

i=1

i=1 Li/) gradient evaluation to achieve -stationarity 

where h(z) := ιZ(z) + p(z)  x := [x1;··· ; xN ]. Our algorithm uses the Lagrangian relaxation of
the equality constraints  and at each iteration a (possibly non-uniformly) randomly selected primal
variable is optimized  followed by an approximate dual ascent step. Note that such splitting scheme
has been popular in the convex setting [6]  but not so when the problem becomes nonconvex.
The NESTT is one of the ﬁrst stochastic algorithms for distributed nonconvex nonsmooth optimiza-
tion  with provable and nontrivial convergence rates. Our main contribution is given below. First 
in terms of some primal and dual optimality gaps  NESTT converges sublinearly to a point belongs
to stationary solution set of (1.2). Second  NESTT converges Q-linearly for certain nonconvex (cid:96)1
penalized quadratic problems. To the best of our knowledge  this is the ﬁrst time that linear conver-
gence is established for stochastic and distributed optimization of such type of problems. Third  we
show that a gradient-based NESTT with non-uniform sampling achieves an -stationary solution of

(cid:112)Li/N )2/) gradient evaluations. Compared with the classical gradient de-

(1.1) using O(((cid:80)N
scent  which in the worst case requires O((cid:80)N

our obtained rate can be up to O(N ) times better in the case where the Li’s are not equal.
Our work also reveals a fundamental connection between primal-dual based algorithms and the
primal only average-gradient based algorithm such as SAGA/SAG/IAG [5  9  22]. With the key
observation that the dual variables in NESTT serve as the “memory” of the past gradients  one can
specialize NESTT to SAGA/SAG/IAG. Therefore  NESTT naturally generalizes these algorithms to
the nonconvex nonsmooth setting. It is our hope that by bridging the primal-dual splitting algorithms
and primal-only algorithms (in both the convex and nonconvex setting)  there can be signiﬁcant
further research developments beneﬁting both algorithm classes.
Related Work. Many stochastic algorithms have been designed for (1.2) when it is convex. In these
algorithms the component functions gi’s are randomly sampled and optimized. Popular algorithms
include the SAG/SAGA [9  22]  the SDCA [23]  the SVRG [14]  the RPDG [15] and so on. When the
problem becomes nonconvex  the well-known incremental based algorithm can be used [3  24]  but
these methods generally lack convergence rate guarantees. The SGD based method has been studied
in [10]  with O(1/2) convergence rate. Recent works [1] and [21] develop algorithms based on
SVRG and SAGA for a special case of (1.1) where the entire problem is smooth and unconstrained.
To the best of our knowledge there has been no stochastic algorithms with provable  and non-trivial 
convergence rate guarantees for solving problem (1.1). On the other hand  distributed stochastic
algorithms for solving problem (1.1) in the nonconvex setting has been proposed in [13]  in which
each time a randomly picked subset of agents update their local variables. However there has been
no convergence rate analysis for such distributed stochastic scheme. There has been some recent
distributed algorithms designed for (1.1) [17]  but again without global convergence rate guarantee.
Preliminaries. The augmented Lagrangian function for problem (1.1) is given by:

(cid:18) 1

N(cid:88)

(cid:19)

L (x  z; λ) =

gi(xi) + (cid:104)λi  xi − z(cid:105) +

(cid:107)xi − z(cid:107)2
i=1 is the set of dual variables  and η := {ηi > 0}N

where λ := {λi}N
We make the following assumptions about problem (1.1) and the function (1.3).

ηi
2

i=1

N

i=1 are penalty parameters.

+ g0(z) + h(z) 

(1.3)

A-(a) The function f (z) is bounded from below over Z ∩ int(dom f ): f := minz∈Z f (z) > −∞.

p(z) is a convex lower semi-continuous function; Z is a closed convex set.

A-(b) The gi’s and g have Lipschitz continuous gradients  i.e. 

(cid:107)∇g(y) − ∇g(z)(cid:107) ≤ L(cid:107)y − z(cid:107)  and (cid:107)∇gi(y) − ∇gi(z)(cid:107) ≤ Li(cid:107)y − z(cid:107)  ∀ y  z

2

Algorithm 1 NESTT-G Algorithm
1: for r = 1 to R do
2:

Pick ir ∈ {1  2 ···   N} with probability pir and update (x  λ)

xr+1
ir = arg min
xir

Vir (xir   zr  λr

ir ) ;

ir − zr(cid:1) ;
(cid:0)xr+1

ir = λr
λr+1
ir + αir ηir
j = λr
λr+1
j  
zr+1 = arg min
z∈Z

j = zr 
xr+1
L({xr+1

i

∀ j (cid:54)= ir;

}  z; λr).

Update z:

(2.4)

(2.5)
(2.6)
(2.7)

3: end for
4: Output: (zm  xm  λm) where m randomly picked from {1  2 ···   R}.

Clearly L ≤ 1/N(cid:80)N
A-(c) Each ηi in (1.3) satisﬁes ηi > Li/N; if g0 is nonconvex  then(cid:80)N
γi := ηi − Li/N and γz =(cid:80)N

plicity of analysis we will further assume that L0 ≤ 1

i=1 ηi − L0  respectively [27  Theorem 2.1].

(cid:80)N

i=1 Li.

N

Assumption A-(c) implies that L (x  z; λ) is strongly convex w.r.t. each xi and z  with modulus

i=1 ηi > 3L0.

i=1 Li  and the equality can be achieved in the worst case. For sim-

(cid:16)

z − proxγ

We then deﬁne the prox-gradient (pGRAD) for (1.1)  which will serve as a measure of stationarity.
It can be checked that the pGRAD vanishes at the set of stationary solutions of (1.1) [20].
Deﬁnition 1.1. The proximal gradient of problem (1.1) is given by (for any γ > 0)
˜∇fγ(z) := γ
2 The NESTT-G Algorithm
Algorithm Description. We present a primal-dual splitting scheme for the reformulated problem
(1.2). The algorithm is referred to as the NESTT with Gradient step (NESTT-G) since each agent
only requires to know the gradient of each component function. To proceed  let us deﬁne the fol-
lowing function (for some constants {αi > 0}N

[z − 1/γ∇(g(z) + g0(z))]

  with proxγ

[u] := argmin

p(u)+

(cid:17)

u∈Z

p+ιZ

p+ιZ

γ
2

i=1):

(cid:107)z−u(cid:107)2.

Vi(xi  z; λi) =

gi(z) +

1
N

1
N

(cid:104)∇gi(z)  xi − z(cid:105) + (cid:104)λi  xi − z(cid:105) +

αiηi

(cid:107)xi − z(cid:107)2.

2

Note that Vi(·) is related to L(·) in the following way: it is a quadratic approximation (approximated
at the point z) of L(x  y; λ) w.r.t. xi. The parameters α := {αi}N
i=1 give some freedom to the
algorithm design  and they are critical in improving convergence rates as well as in establishing
connection between NESTT-G with a few primal only stochastic optimization schemes.
The algorithm proceeds as follows. Before each iteration begins the cluster center broadcasts z to
everyone. At iteration r + 1 a randomly selected agent ir ∈ {1  2 ··· N} is picked  who minimizes
Vir (·) w.r.t.
its local variable xir  followed by a dual ascent step for λir. The rest of the agents
update their local variables by simply setting them to z. The cluster center then minimizes L(x  z; λ)
with respect to z. See Algorithm 1 for details. We remark that NESTT-G is related to the popular
ADMM method for convex optimization [6]. However our particular update schedule (randomly
picking (xi  λi) plus deterministic updating z)  combined with the special x-step (minimizing an
approximation of L(·) evaluated at a different block variable z) is not known before. These features
are critical in our following rate analysis.
Convergence Analysis. To proceed  let us deﬁne r(j) as the last iteration in which the jth block
is picked before iteration r + 1. i.e. r(j) := max{t | t < r + 1  j = i(t)}. Deﬁne yr
j := zr(j) if
j (cid:54)= ir  and yr
A few important observations are in order. Combining the (x  z) updates (2.4) – (2.7)  we have

= zr. Deﬁne the ﬁltration F r as the σ-ﬁeld generated by {i(t)}r−1
t=1 .

ir

∇gq(zr)) 

q +

(λr

q = zr − 1
1
xr+1
N
αqηq
j = − 1
∇gir (zr)  λr+1
ir = − 1
λr+1
N
N
= zr − 1
1
xr+1
j
αjηj
N

(2.6)
= zr (2.8b)

(λr

j +

∇gq(zr) + λr

q + αqηq(xr+1

q − zr) = 0  with q = ir

1
N

∇gj(zr(j))  ∀ j (cid:54)= ir  ⇒ λr+1
∇gj(zr(j)))  ∀ j (cid:54)= ir.

i = − 1
N

∇gi(yr

i )  ∀ i

3

(2.8a)

(2.8b)

(2.8c)

The key here is that the dual variables serve as the “memory” for the past gradients of gi’s. To
proceed  we ﬁrst construct a potential function using an upper bound of L(x  y; λ). Note that

gj(zr)  ∀ j (cid:54)= ir

1
N

(2.9)

) + (cid:104)λr
) + (cid:104)λr

j   xr+1

ir   xr+1

j − zr(cid:105) +
j − zr(cid:107)2 =
(cid:107)xr+1
ηj
2
ir − zr(cid:105) +
ir − zr(cid:107)2
(cid:107)xr+1
ηi
2
ir − zr(cid:107)2
(cid:107)xr+1

ηir + Lir /N

2

gir (zr) +

gj(xr+1

j

gir (xr+1

ir

1
N
1
N
(i)≤ 1
N
1
N

(ii)
=

(2.10)
where (i) uses (2.8b) and applies the descent lemma on the function 1/N gi(·); in (ii) we have used
(2.5) and (2.8b). Since each i is picked with probability pi  we have

2(αir ηir )2 (cid:107)1/N (∇gir (yr−1

) − ∇gir (zr))(cid:107)2

ηir + Lir /N

gir (zr) +

ir

Eir [L(xr+1  zr; λr) | F r]
N(cid:88)
N(cid:88)

≤ N(cid:88)
≤ N(cid:88)

gi(zr) +

gi(zr) +

1
N

i=1

i=1

1
N

i=1

i=1

pi(ηi + Li/N )

2(αiηi)2

(cid:107)1/N (∇gi(yr−1

i

) − ∇gi(zr))(cid:107)2 + g0(zr) + h(zr)

3piηi

(αiηi)2(cid:107)1/N (∇gi(yr−1

i

) − ∇gi(zr))(cid:107)2 + g0(zr) + h(zr) := Qr 

where in the last inequality we have used Assumption [A-(c)]. In the following  we will use EF r [Qr]
as the potential function  and show that it decreases at each iteration.
Lemma 2.1. Suppose Assumption A holds  and pick

αi = pi = βηi  where β :=

 

and

ηi ≥ 9Li
N pi

 

i = 1 ··· N.

(2.11)

Then the following descent estimate holds true for NESTT-G

Sublinear Convergence. Deﬁne the optimality gap as the following:

1(cid:80)N

i=1 ηi

=

i=1

1
β2

Ezr(cid:107)zr − zr−1(cid:107)2 − N(cid:88)
E(cid:104)(cid:107)zr − prox1/β
(cid:32) N(cid:88)
(cid:112)Li/N
(cid:17)2 E[Q1 − QR+1]
(cid:112)Li/N
(cid:13)(cid:13)xm
i − zm−1(cid:13)(cid:13)2

(cid:35)

i=1

R

≤ 80
3

E[Qr − Qr−1|F r−1] ≤ −

(cid:80)N
E[Gr] := E(cid:104)(cid:107) ˜∇1/βf (zr)(cid:107)2(cid:105)

i=1 ηi
8

αi = pi =

  ηi = 3

i=1

(cid:112)Li/N
(cid:112)Li/N
(cid:80)N
(cid:16) N(cid:88)
(cid:34) N(cid:88)

i=1

1) E[Gm] ≤ 80
3

2) E[Gm] + E

3η2
i

i=1

1
2ηi

(cid:107) 1
N

(∇gi(zr−1) − ∇gi(yr−2

i

))(cid:107)2. (2.12)

h [zr − β∇(g(zr) + g0(zr))](cid:107)2(cid:105)
(cid:33)(cid:112)Li/N   β =

1

3((cid:80)N

(cid:112)Li/N )2

i=1

.

(2.13)

.

(2.14)

;

(cid:32) N(cid:88)

i=1

(cid:112)Li/N

(cid:33)2 E[Q1 − QR+1]

R

.

Note that when h  g0 ≡ 0  E[Gr] reduces to E[(cid:107)∇g(zr)(cid:107)2]. We have the following result.
Theorem 2.1. Suppose Assumption A holds  and pick (for i = 1 ···   N)

Then every limit point generated by NESTT-G is a stationary solution of problem (1.2). Further 

Note that Part (1) is useful in the centralized ﬁnite-sum minimization setting  as it shows the sublin-
ear convergence of NESTT-G  measured only by the primal optimality gap evaluated at zr. Mean-
while  part (2) is useful in the distributed setting  as it also shows that the expected constraint vio-
lation  which measures the consensus among agents  shrinks in the same order. We also comment
that the above result suggests that to achieve an -stationary solution  the NESTT-G requires about
O
ditive N factor for evaluating the gradient of the entire function at the initial step of the algorithm).

number of gradient evaluations (for simplicity we have ignored an ad-

(cid:32)(cid:18)(cid:80)N

(cid:112)Li/N

(cid:19)2

(cid:33)

i=1

/

4

Algorithm 2 NESTT-E Algorithm
1: for r = 1 to R do
2:

Update z by minimizing the augmented Lagrangian:

3:

z

L(xr  z; λr).

zr+1 = arg min
Randomly pick ir ∈ {1  2 ··· N} with probability pir :
xr+1
ir = argmin
xir

Uir (xir   zr+1; λr

ir );

(cid:0)xr+1
ir − zr+1(cid:1) ;

ir + αir ηir
j   λr+1

ir = λr
λr+1
j = xr
xr+1
4: end for
5: Output: (zm  xm  λm) where m randomly picked from {1  2 ···   R}.

j ∀ j (cid:54)= ir.

j = λr

(3.15)

(3.16)

(3.17)
(3.18)

It is interesting to observe that our choice of pi is proportional to the square root of the Lipschitz
constant of each component function  rather than to Li. Because of such choice of the sampling
probability  the derived convergence rate has a mild dependency on N and Li’s. Compared with the
conventional gradient-based methods  our scaling can be up to N times better. Detailed discussion
and comparison will be given in Section 4.
Note that similar sublinear convergence rates can be obtained for the case αi = 1 for all i (with
different scaling constants). However due to space limitation  we will not present those results here.
Linear Convergence. In this section we show that the NESTT-G is capable of linear convergence
for a family of nonconvex quadratic problems  which has important applications  for example in
high-dimensional statistical learning [16]. To proceed  we will assume the following.
B-(a) Each function gi(z) is a quadratic function of the form gi(z) = 1/2zT Aiz + (cid:104)b  z(cid:105)  where

Ai is a symmetric matrix but not necessarily positive semideﬁnite;

B-(b) The feasible set Z is a closed compact polyhedral set;
B-(c) The nonsmooth function p(z) = µ(cid:107)z(cid:107)1  for some µ ≥ 0.

Our linear convergence result is based upon certain error bound condition around the stationary
solutions set  which has been shown in [18] for smooth quadratic problems and has been extended
to including (cid:96)1 penalty in [25  Theorem 4]. Due to space limitation the statement of the condition
will be given in the supplemental material  along with the proof of the following result.
Theorem 2.2. Suppose that Assumptions A  B are satisﬁed. Then the sequence {E[Qr+1]}∞
converges Q-linearly 4 to some Q∗ = f (z∗)  where z∗ is a stationary solution for problem (1.1).
That is  there exists a ﬁnite ¯r > 0  ρ ∈ (0  1) such that for all r ≥ ¯r  E[Qr+1 − Q∗]≤ ρE[Qr − Q∗].
Linear convergence of this type for problems satisfying Assumption B has been shown for (deter-
ministic) proximal gradient based methods [25  Theorem 2  3]. To the best of our knowledge  this is
the ﬁrst result that shows the same linear convergence for a stochastic and distributed algorithm.

r=1

3 The NESTT-E Algorithm

Algorithm Description. In this section  we present a variant of NESTT-G  which is named NESTT
with Exact minimization (NESTT-E). Our motivation is the following. First  in NESTT-G every
agent should update its local variable at every iteration [cf. (2.4) or (2.6)]. In practice this may not
be possible  for example at any given time a few agents can be in the sleeping mode so they cannot
perform (2.6). Second  in the distributed setting it has been generally observed (e.g.  see [8  Section
V]) that performing exact minimization (whenever possible) instead of taking the gradient steps for
local problems can signiﬁcantly speed up the algorithm. The NESTT-E algorithm to be presented in
this section is designed to address these issues. To proceed  let us deﬁne a new function as follows:

U (x  z; λ) :=

Ui(xi  z; λi) :=

gi(xi) + (cid:104)λi  xi − z(cid:105) +

(cid:107)xi − z(cid:107)2

αiηi

2

(cid:19)

.

N(cid:88)

i=1

(cid:18) 1

N(cid:88)

N

i=1

4A sequence {xr} is said to converge Q-linearly to some ¯x if lim supr (cid:107)xr+1 − ¯x(cid:107)/(cid:107)xr − ¯x(cid:107) ≤ ρ  where

ρ ∈ (0  1) is some constant; cf [25] and references therein.

5

Note that if αi = 1 for all i  then the L(x  z; λ) = U (x  z; λ) + p(z) + h(z). The algorithm details
are presented in Algorithm 2.
Convergence Analysis. We begin analyzing NESTT-E. The proof technique is quite different from
that for NESTT-G  and it is based upon using the expected value of the Augmented Lagrangian func-
tion as the potential function; see [11  12  13]. For the ease of description we deﬁne the following
quantities:

L2
i

αiηiN 2 − γi

2

+

1 − αi
αi

Li
N

  α := {αi}N

i=1.

w := (x  z  λ) 

β :=

 

ci :=

i=1 ηi

(cid:20)
(cid:21)
(z − proxh[z − ∇z(L(w) − h(z))]);∇x1L(w);··· ;∇xN L(w)

To measure the optimality of NESTT-E  deﬁne the prox-gradient of L(x  z; λ) as:
˜∇L(w) =
We deﬁne the optimality gap by adding to (cid:107) ˜∇L(w)(cid:107)2 the size of the constraint violation [13]:

∈ R(N +1)d.

(3.19)

1(cid:80)N

H(wr) := (cid:107) ˜∇L(wr)(cid:107)2 +

L2
i

N 2 (cid:107)xr

i − zr(cid:107)2.

N(cid:88)

i=1

It can be veriﬁed that H(wr) → 0 implies that wr reaches a stationary solution for problem (1.2).
We have the following theorem regarding the convergence properties of NESTT-E.
Theorem 3.1. Suppose Assumption A holds  and that (ηi  αi) are chosen such that ci < 0 . Then
for some constant f  we have

E[L(wr)] ≥ E[L(wr+1)] ≥ f > −∞ 

∀ r ≥ 0.

Further  almost surely every limit point of {wr} is a stationary solution of problem (1.2). Finally 
for some function of α denoted as C(α) = σ1(α)/σ2(α)  we have the following:

where σ1 := max(ˆσ1(α)  ˜σ1) and σ2 := max(ˆσ2(α)  ˜σ2)  and these constants are given by

ˆσ1(α) = max

4

(cid:40)

(cid:26)

i

N(cid:88)

i=1

E[H(wm)] ≤ C(α)E[L(w1) − L(wR+1)]
(cid:32)
(cid:18) L4

(cid:33)

R

 

αi

i
N 2

− 1

(cid:18) 1

(cid:19)2 L2
N(cid:88)
ηi + L0)2 + 3
− 1 − αi

i=1

Li
N

αi

i +

L2
N 2 + η2
i
N(cid:88)

(cid:18) γi

2

i=1

− L2

i

N 2αiηi

+ 3

i
αiη2

L2
i
N 2  

(cid:19)(cid:27)

 

˜σ2 =

(cid:19)(cid:41)

L2
i
N 2

i N 4 +
(cid:80)N
i=1 ηi − L0

 

.

2

˜σ1 =

4η2

i + (2 +

ˆσ2(α) = max

i

pi

(3.20)

We remark that the above result shows the sublinear convergence of NESTT-E to the set of stationary
solutions. Note that γi = ηi − Li/N  to satisfy ci < 0  a simple derivation yields

(cid:16)
(2 − αi) +(cid:112)(αi − 2)2 + 8αi

(cid:17)

2N αi

.

Li

ηi >

Further  the above result characterizes the dependency of the rates on various parameters of the
algorithm. For example  to see the effect of α on the convergence rate  let us set pi =
 

Li(cid:80)N
and ηi = 3Li/N  and assume L0 = 0  then consider two different choices of α: (cid:98)αi = 1  ∀ i and
(cid:101)αi = 4  ∀ i. One can easily check that applying these different choices leads to following results:

i=1 Li

N(cid:88)

C((cid:98)α) = 49

N(cid:88)

C((cid:101)α) = 28

Li/N 

Li/N.

i=1

i=1

The key observation is that increasing αi’s reduces the constant in front of the rate. Hence  we
expect that in practice larger αi’s will yield faster convergence.
4 Connections and Comparisons with Existing Works
In this section we compare NESTT-G/E with a few existing algorithms in the literature. First  we
present a somewhat surprising observation  that NESTT-G takes the same form as some well-known
algorithms for convex ﬁnite-sum problems. To formally state such relation  we show in the following
result that NESTT-G in fact admits a compact primal-only characterization.

6

Table 1: Comparison of # of gradient evaluations for NESTT-G and GD in the worst case

# of Gradient Evaluations
Case I: Li = 1  ∀i
√
Case II : O(
N ) terms with Li = N
the rest with Li = 1
Case III : O(1) terms with Li = N 2
the rest with Li = 1

O(cid:16)

((cid:80)N

NESTT-G

(cid:112)Li/N )2/

(cid:17) O(cid:16)(cid:80)N

GD
i=1 Li/

(cid:17)

i=1

O(N/)
O(N/)
O(N/)

O(N/)
O(N 3/2/)
O(N 2/)

Proposition 4.1. The NESTT-G can be written into the following compact form:

zr+1 = arg min

h(z) + g0(z) +

(cid:107)z − ur+1(cid:107)2

1
2β

z

(cid:16) 1

with ur+1 := zr − β

(∇gir (zr) − ∇gir (yr−1

ir

)) +

1
N

N αir

i=1

N(cid:88)

(cid:17)

)

.

∇gi(yr−1

i

(4.21a)

(4.21b)

Based on this observation  the following comments are in order.
(1) Suppose h ≡ 0  g0 ≡ 0 and αi = 1  pi = 1/N for all i. Then (4.21) takes the same form as the
SAG presented in [22]. Further  when the component functions gi’s are picked cyclically in a
Gauss-Seidel manner  the iteration (4.21) takes the same form as the IAG algorithm [5].
(2) Suppose h (cid:54)= 0 and g0 (cid:54)= 0  and αi = pi = 1/N for all i. Then (4.21) is the same as the SAGA

algorithm [9]  which is design for optimizing convex nonsmooth ﬁnite sum problems.

thors show that SAGA achieves -stationarity using O(N 2/3((cid:80)N
Compared with GD  which achieves -stationarity using O((cid:80)N
worse case (in the sense that(cid:80)N

Note that SAG/SAGA/IAG are all designed for convex problems. Through the lens of primal-dual
splitting  our work shows that they can be generalized to nonconvex nonsmooth problems as well.
Secondly  NESTT-E is related to the proximal version of the nonconvex ADMM [13  Algorithm 2].
However  the introduction of αi’s is new  which can signiﬁcantly improve the practical performance
but complicates the analysis. Further  there has been no counterpart of the sublinear and linear
convergence rate analysis for the stochastic version of [13  Algorithm 2].
Thirdly  we note that a recent paper [21] has shown that SAGA works for smooth and unconstrained
nonconvex problem. Suppose that h ≡ 0  g0 (cid:54)= 0  Li = Lj  ∀ i  j and αi = pi = 1/N  the au-
i=1 Li/N )/) gradient evaluations.
i=1 Li/) gradient evaluations in the
i=1 Li/N = L)  the rate in [21] is O(N 1/3) times better. How-
ever  the algorithm in [21] is different from NESTT-G in two aspects: 1) it does not generalize to
the nonsmooth constrained problem (1.1); 2) it samples two component functions at each iteration 
while NESTT-G only samples once. Further  the analysis and the scaling are derived for the case of
uniform Li’s  therefore it is not clear how the algorithm and the rates can be adapted for the non-
uniform case. On the other hand  our NESTT works for the general nonsmooth constrained setting.
The non-uniform sampling used in NESTT-G is well-suited for problems with non-uniform Li’s 
and our scaling can be up to N times better than GD (or its proximal version) in the worst case.
Note that problems with non-uniform Li’s for the component functions are common in applications
such as sparse optimization and signal processing. For example in LASSO problem the data matrix
is often normalized by feature (or “column-normalized” [19])  therefore the (cid:96)2 norm of each row of
the data matrix (which corresponds to the Lipschitz constant for each component function) can be
dramatically different.
In Table 1 we list the comparison of the number of gradient evaluations for NESTT-G and GD  in
i=1 Li/N = L). For simplicity  we omitted an additive constant
of O(N ) for computing the initial gradients.
5 Numerical Results
In this section we evaluate the performance of NESTT. Consider the high dimensional regression
problem with noisy observation [16]  where M observations are generated by y = Xν + . Here
y ∈ RM is the observed data sample; X ∈ RM×P is the covariate matrix; ν ∈ RP is the ground
truth  and  ∈ RM is the noise. Suppose that the covariate matrix is not perfectly known  i.e.  we
observe A = X + W where W ∈ RM×P is the noise matrix with known covariance matrix ΣW .
Let us deﬁne ˆΓ := 1/M (A(cid:62)A) − ΣW   and ˆγ := 1/M (A(cid:62)y). To estimate the ground truth ν  let

the worst case (in the sense that(cid:80)N

7

Figure 1: Comparison of NESTT-G/E  SAGA  SGD on problem (5.22). The x-axis denotes the number of
passes of the dataset. Left: Uniform Sampling pi = 1/N; Right: Non-uniform Sampling (pi =
).
Table 2: Optimality gap (cid:107) ˜∇1/βf (zr)(cid:107)2 for different algorithms  with 100 passes of the datasets.

√
(cid:80)N

Li/N

Li/N

√

i=1

SAGA

SGD

NESTT-E (α = 10)

NESTT-G

N
10
20
30
40
50

Uniform Non-Uni Uniform Non-Uni Uniform Non-Uni Uniform Non-Uni
3.4054
0.6370
0.2260
0.0574
0.0154

6.16E-19
5.9E-9
2.7E-6
8.1E-5
7.1E-4

2.8022
11.3435
0.1253
0.7385
3.3187

2.3E-21
1.2E-10
4.5E-7
1.8E-5
1.2E-4

2.6E-16
2.4E-9
3.2E-6
5.8E-4
8.3E.-4

6.1E-24
2.9E-11
1.4E-7
3.1E-5
2.7E-4

2.7E-17
7.7E-7
2.5E-5
4.1E-5
2.5E-4

0.2265
6.9087
0.1639
0.3193
0.0409

us consider the following (nonconvex) optimization problem posed in [16  problem (2.4)] (where
R > 0 controls sparsity):

z(cid:62) ˆΓz − ˆγz

s.t. (cid:107)z(cid:107)1 ≤ R.

min

z

(5.22)

N

N

i=1

i Wi

(cid:1) z.

(cid:80)N

as [16]. Let X = (X1;···   XN ) ∈ RM×P with(cid:80)

Due to the existence of noise  ˆΓ is not positive semideﬁnite hence the problem is not convex. Note
that this problem satisﬁes Assumption A– B  then by Theorem 2.2 NESTT-G converges Q-linearly.
To test the performance of the proposed algorithm  we generate the problem following similar setups
i Ni = M and each Xi ∈ RNi×P corresponds
to Ni data points  and it is generated from i.i.d Gaussian. Here Ni represents the size of each mini-
batch of samples. Generate the observations yi = Xi×ν∗ +i ∈ RNi  where ν∗ is a K-sparse vector
to be estimated  and i ∈ RNi is the random noise. Let W = [W1;··· ; WN ]  with Wi ∈ RNi×P
generated with i.i.d Gaussian. Therefore we have z(cid:62) ˆΓz = 1
We set M = 100  000  P = 5000  N = 50  K = 22 ≈ √
P  and R = (cid:107)ν∗(cid:107)1. We implement

M z(cid:62)(cid:0)X(cid:62)

i Xi − W (cid:62)

NESTT-G/E  the SGD  and the nonconvex SAGA proposed in [21] with stepsize β =
3LmaxN 2/3
(with Lmax := maxi Li). Note that the SAGA proposed in [21] only works for the unconstrained
problems with uniform Li  therefore when applied to (5.22) it is not guaranteed to converge. Here
we only include it for comparison purposes.
In Fig. 1 we compare different algorithms in terms of the gap (cid:107) ˜∇1/βf (zr)(cid:107)2.
In the left ﬁgure
we consider the problem with Ni = Nj for all i  j  and we show performance of the proposed
algorithms with uniform sampling (i.e.  the probability of picking ith block is pi = 1/N). On the
right one we consider problems in which approximately half of the component functions have twice

the size of Li’s as the rest  and consider the non-uniform sampling (pi =(cid:112)Li/N /(cid:80)N

(cid:112)Li/N).

Clearly in both cases the proposed algorithms perform quite well. Furthermore  it is clear that the
NESTT-E performs well with large α := {αi}N
i=1  which conﬁrms our theoretical rate analysis. Also
it is worth mentioning that when the Ni’s are non-uniform  the proposed algorithms [NESTT-G and
NESTT-E (with α = 10)] signiﬁcantly outperform SAGA and SGD. In Table 2 we further compare
different algorithms when changing the number of component functions (i.e.  the number of mini-
batches N) while the rest of the setup is as above. We run each algorithm with 100 passes over
the dataset. Similarly as before  our algorithms perform well  while SAGA seems to be sensitive to
the uniformity of the size of the mini-batch [note that there is no convergence guarantee for SAGA
applied to the nonconvex constrained problem (5.22)].

i=1

1

8

0100200300400500# Grad/N10-1510-1010-5100105Optimality gapUniform SamplingSGDNESTT-E( = 10)NESTT-E( = 1)NESTT-GSAGA0100200300400500# Grad/N10-2010-1510-1010-5100105Optimality gapNon-Uniform SamplingSGDNESTT-E( = 10)NESTT-E( = 1)NESTT-GSAGAReferences
[1] Z. A.-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. 2016.

Preprint  available on arXiv  arXiv:1603.05643.

[2] A. Antoniadis  I. Gijbels  and M. Nikolova. Penalized likelihood regression for generalized
linear models with non-quadratic penalties. Annals of the Institute of Statistical Mathematics 
63(3):585–615  2009.

[3] D. Bertsekas. Incremental gradient  subgradient  and proximal methods f or convex optimiza-

tion: A survey. 2000. LIDS Report 2848.

[4] E. Bjornson and E. Jorswieck. Optimal resource allocation in coordinated multi-cell systems.

Foundations and Trends in Communications and Information Theory  9  2013.

[5] D. Blatt  A. O. Hero  and H. Gauchman. A convergent incremental gradient method with a

constant step size. SIAM Journal on Optimization  18(1):29–51  2007.

[6] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. Foundations and Trends in
Machine Learning  3(1):1–122  2011.

[7] V. Cevher  S. Becker  and M. Schmidt. Convex optimization for big data: Scalable  ran-
domized  and parallel algorithms for big data analytics. IEEE Signal Processing Magazine 
31(5):32–43  Sept 2014.

[8] T.-H. Chang  M. Hong  and X. Wang. Multi-agent distributed optimization via inexact consen-

sus admm. IEEE Transactions on Signal Processing  63(2):482–497  Jan 2015.

[9] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with

support for non-strongly convex composite objectives. In The Proceeding of NIPS  2014.

[10] S. Ghadimi and G. Lan. Stochastic ﬁrst- and zeroth-order methods for nonconvx stochastic

programming. SIAM Journal on Optimizatnoi  23(4):2341–2368  2013.

[11] D. Hajinezhad  T. H. Chang  X. Wang  Q. Shi  and M. Hong. Nonnegative matrix factorization
using admm: Algorithm and convergence analysis. In 2016 IEEE International Conference on
Acoustics  Speech and Signal Processing (ICASSP)  pages 4742–4746  March 2016.

[12] D. Hajinezhad and M. Hong. Nonconvex alternating direction method of multipliers for dis-

tributed sparse principal component analysis. In the Proceedings of GlobalSIPT  2015.

[13] M. Hong  Z.-Q. Luo  and M. Razaviyayn. Convergence analysis of alternating direction
method of multipliers for a family of nonconvex problems. SIAM Journal On Optimization 
26(1):337–364  2016.

[14] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In the Proceedings of the Neural Information Processing (NIPS). 2013.

[15] G. Lan. An optimal randomized incremental gradient method. 2015. Preprint.
[16] P.-L. Loh and M. Wainwright. High-dimensional regression with noisy and missing data:

Provable guarantees with nonconvexity. The Annals of Statistics  40(3):1637–1664  2012.
[17] P. D. Lorenzo and G. Scutari. Next: In-network nonconvex optimization. 2016. Preprint.
[18] Z.-Q. Luo and P. Tseng. On the linear convergence of descent methods for convex essentially

smooth minimization. SIAM Journal on Control and Optimization  30(2):408–425  1992.

[19] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-
dimensional analysis of m-estimators with decomposable regularizers. Statist. Sci.  27(4):538–
557  11 2012.

[20] M. Razaviyayn  M. Hong  Z.-Q. Luo  and J. S. Pang. Parallel successive convex approximation

for nonsmooth nonconvex optimization. In the Proceedings of NIPS  2014.

[21] S. J. Reddi  S. Sra  B. Poczos  and A. Smola. Fast incremental method for nonconvex opti-

mization. 2016. Preprint  available on arXiv: arXiv:1603.06159.

[22] M. Schmidt  N. L. Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. 2013. Technical report  INRIA.

[23] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent methods for

regularzied loss minimization. Journal of Machine Learning Rsearch  14:567–599  2013.

[24] S. Sra. Scalable nonconvex inexact proximal splitting.

In Advances in Neural Information

Processing Systems (NIPS)  2012.

[25] P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable mini-

mization. Mathematical Programming  117:387–423  2009.

[26] Z. Wang  H. Liu  and T. Zhang. Optimal computational and statistical rates of convergence for

sparse nonconvex learning problems. Annals of Statistics  42(6):2164–2201  2014.

[27] S. Zlobec. On the Liu - Floudas convexiﬁcation of smooth programs. Journal of Global

Optimization  32:401 – 407  2005.

9

,Davood Hajinezhad
Mingyi Hong
Tuo Zhao
Zhaoran Wang