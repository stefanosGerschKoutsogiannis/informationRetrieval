2015,GP Kernels for Cross-Spectrum Analysis,Multi-output Gaussian processes provide a convenient framework for multi-task problems.  An illustrative and motivating example of a multi-task problem is multi-region electrophysiological time-series data  where experimentalists are interested in both power and phase coherence between channels.  Recently  Wilson and Adams (2013) proposed the spectral mixture (SM) kernel to model the spectral density of a single task in a Gaussian process framework.  In this paper  we develop a novel covariance kernel for multiple outputs  called the cross-spectral mixture (CSM) kernel.  This new  flexible kernel represents both the power and phase relationship between multiple observation channels.  We demonstrate the expressive capabilities of the CSM kernel through implementation of a Bayesian hidden Markov model  where the emission distribution is a multi-output Gaussian process with a CSM covariance kernel.  Results are presented for measured multi-region electrophysiological data.,GP Kernels for Cross-Spectrum Analysis

1Kyle Ulrich  3David E. Carlson  2Kafui Dzirasa  1Lawrence Carin
1Department of Electrical and Computer Engineering  Duke University
2Department of Psychiatry and Behavioral Sciences  Duke University
{kyle.ulrich  kafui.dzirasa  lcarin}@duke.edu

3Department of Statistics  Columbia University

david.edwin.carlson@gmail.com

Abstract

Multi-output Gaussian processes provide a convenient framework for multi-task
problems. An illustrative and motivating example of a multi-task problem is
multi-region electrophysiological time-series data  where experimentalists are in-
terested in both power and phase coherence between channels. Recently  Wilson
and Adams (2013) proposed the spectral mixture (SM) kernel to model the spec-
tral density of a single task in a Gaussian process framework. In this paper  we
develop a novel covariance kernel for multiple outputs  called the cross-spectral
mixture (CSM) kernel. This new  ﬂexible kernel represents both the power and
phase relationship between multiple observation channels. We demonstrate the
expressive capabilities of the CSM kernel through implementation of a Bayesian
hidden Markov model  where the emission distribution is a multi-output Gaus-
sian process with a CSM covariance kernel. Results are presented for measured
multi-region electrophysiological data.

Introduction

1
Gaussian process (GP) models have become an important component of the machine learning liter-
ature. They have provided a basis for non-linear multivariate regression and classiﬁcation tasks  and
have enjoyed much success in a wide variety of applications [16].
A GP places a prior distribution over latent functions  rather than model parameters. In the sense
that these functions are deﬁned for any number of sample points and sample positions  as well
as any general functional form  GPs are nonparametric. The properties of the latent functions are
deﬁned by a positive deﬁnite covariance kernel that controls the covariance between the function
at any two sample points. Recently  the spectral mixture (SM) kernel was proposed by Wilson and
Adams [24] to model a spectral density with a scale-location mixture of Gaussians. This ﬂexible
and interpretable class of kernels is capable of recovering any composition of stationary kernels
[27  9  13]. The SM kernel has been used for GP regression of a scalar output (i.e.  single function  or
observation “task”)  achieving impressive results in extrapolating atmospheric CO2 concentrations
[24]; image inpainting [25]; and feature extraction from electrophysiological signals [21].
However  the SM kernel is not deﬁned for multiple outputs (multiple correlated functions). Multi-
output GPs intersect with the ﬁeld of multi-task learning [4]  where solving similar problems jointly
allows for the transfer of statistical strength between problems  improving learning performance
when compared to learning all tasks individually. In this paper  we consider neuroscience appli-
cations where low-frequency (< 200 Hz) extracellular potentials are simultaneously recorded from
implanted electrodes in multiple brain regions of a mouse [6]. These signals are known as local ﬁeld
potentials (LFPs) and are often highly correlated between channels. Inferring and understanding
that interdependence is biologically signiﬁcant.

1

A multi-output GP can be thought of as a standard GP (all observations are jointly normal) where the
covariance kernel is a function of both the input space and the output space (see [2] and references
therein for a comprehensive review); here “input space” means the points at which the functions are
sampled (e.g.  time)  and the “output space” may correspond to different brain regions. A particular
positive deﬁnite form of this multi-output covariance kernel is the sum of separable (SoS) kernels 
or the linear model of coregionalization (LMC) in the geostatistics literature [10]  where a separable
kernel is represented by the product of separate kernels for the input and output spaces.
While extending the SM kernel to the multi-output setting via the LMC framework (i.e.  the SM-
LMC kernel) provides a powerful modeling framework  the SM-LMC kernel does not intuitively
represent the data. Speciﬁcally  the SM-LMC kernel encodes the cross-amplitude spectrum (square
root of the cross power spectral density) between every pair of channels  but provides no cross-
phase information. Together  the cross-amplitude and cross-phase spectra form the cross-spectrum 
deﬁned as the Fourier transform of the cross-covariance between the pair of channels.
Motivated by the desire to encode the full cross-spectra into the covariance kernel  we design a novel
kernel termed the cross-spectral mixture (CSM) kernel  which provides an intuitive representation
of the power and phase dependencies between multiple outputs. The need for embedding the full
cross-spectrum into the covariance kernel is illustrated by a recent surge in neuroscience research
discovering that LFP interdependencies between regions exhibit phase synchrony patterns that are
dependent on frequency band [11  17  18].
The remainder of the paper is organized as follows. Section 2 provides a summary of GP regression
models for vector-valued data  and Section 3 introduces the SM  SM-LMC  and novel CSM covari-
ance kernels. In Section 4  the CSM kernel is incorporated in a Bayesian hidden Markov model
(HMM) [14] with a GP emission distribution as a demonstration of its utility in hierarchical model-
ing. Section 5 provides details on inverting the Bayesian HMM with variational inference  as well
as details on a fast  novel GP ﬁtting process that approximates the CSM kernel by its representation
in the spectral domain. Section 6 analyzes the performance of this approximation and presents re-
sults for the CSM kernel in the neuroscience application  considering measured multi-region LFP
data from the brain of a mouse. We conclude in Section 7 by discussing how this novel kernel can
trivially be extended to any time-series application where GPs and the cross-spectrum are of interest.

2 Review of Multi-Output Gaussian Process Regression

A multi-output regression task estimates samples from C output channels  yn = [yn1  . . .   ynC]T
corresponding to the n-th input point xn (e.g.  the n-th temporal sample). An unobserved latent
function f (x) = [f1(x)  . . .   fC(x)]T is responsible for generating the observations  such that yn ∼
N (f (xn)  H−1)  where H = diag(η1  . . .   ηC) is the precision of additive Gaussian noise.
A GP prior on the latent function is formalized by f (x) ∼ GP(m(x)  K(x  x(cid:48))) for arbitrary
input x  where the mean function m(x) ∈ RC is set to equal 0 without loss of generality  and
the covariance function (K(x  x(cid:48)))c c(cid:48) = kc c(cid:48)
(x  x(cid:48)) = cov(fc(x)  fc(cid:48)(x(cid:48))) creates dependencies
between observations at input points x and x(cid:48)  as observed on channels c and c(cid:48). In general  the input
space x could be vector valued  but for simplicity we here assume it to be scalar  consistent with our
motivating neuroscience application in which x corresponds to time.
A convenient representation for multi-output kernel functions is to separate the kernel into the prod-
uct of a kernel for the input space and a kernel for the interactions between the outputs. This is
known as a separable kernel. A sum of separable kernels (SoS) representation [2] is given by

Q(cid:88)

Q(cid:88)

kc c(cid:48)

(x  x(cid:48)) =

bq(c  c(cid:48))kq(x  x(cid:48)) 

or

K(x  x(cid:48)) =

Bqkq(x  x(cid:48)) 

(1)

q=1

q=1

where kq(x  x(cid:48)) is the input space kernel for component q  bq(c  c(cid:48)) is the q-th output interaction
kernel  and Bq ∈ RC×C is a positive semi-deﬁnite output kernel matrix. Note that we have a dis-
crete set of C output spaces  c ∈ {1  . . .   C}  where the input space x is continuous  and discretely
sampled arbitrarily in experiments. The SoS formulation is also known as the linear model of core-
gionalization (LMC) [10] and Bq is termed the coregionalization matrix. When Q = 1  the LMC
reduces to the intrinsic coregionalization model (ICM) [2]  and when rank(Bq) is restricted to equal
1  the LMC reduces to the semiparametric latent factor model (SLFM) [19].

2

Any ﬁnite number of latent functional evaluations f = [f1(x)  . . .   fC(x)]T at locations x =
[x1  . . .   xN ]T has a multivariate normal distribution N (f ; 0  K)  such that K is formed through
the block partitioning

Bq ⊗ kq(x  x) 

(2)

 k1 1(x  x)

...

kC 1(x  x)

K =

 =

Q(cid:88)

q=1

k1 C(x  x)

···
...
··· kC C(x  x)

...

(x  x) is an N × N matrix and ⊗ symbolizes the Kronecker product.

where each kc c(cid:48)
A vector-valued dataset consists of observations y = vec([y1  . . .   yN ]T ) ∈ RCN at the respective
locations x = [x1  . . .   xN ]T such that the ﬁrst N elements of y are from channel 1 up to the last N
elements belonging to channel C. Since both the likelihood p(y|f   x) and distribution over latent
functions p(f|x) are Gaussian  the marginal likelihood is conveniently represented by

p(y|x) =

p(y|f   x)p(f|x)df = N (0  Γ) 

Γ = K + H−1 ⊗ I N  

(3)

(cid:90)

where all possible functions f have been marginalized out.
Each input-space covariance kernel is deﬁned by a set of hyperparameters  θ. This conditioning was
removed for notational simplicity  but will henceforth be included in the notation. For example  if
the squared exponential kernel is used  then kSE(x  x(cid:48); θ) = exp(− 1
2||x − x(cid:48)||2/(cid:96)2)  deﬁned by a
single hyperparameter θ = {(cid:96)}. To ﬁt a GP to the dataset  the hyperparameters are typically chosen
to maximize the marginal likelihood in (3) via gradient ascent.

3 Expressive Kernels in the Spectral Domain
This section ﬁrst introduces the spectral mixture (SM) kernel [24] as well as a multi-output extension
of the SM kernel within the LMC framework. While the SM-LMC model is capable of represent-
ing complex spectral relationships between channels  it does not intuitively model the cross-phase
spectrum between channels. We propose a novel kernel known as the cross-spectral mixture (CSM)
kernel that provides both the cross-amplitude and cross-phase spectra of multi-channel observations.
Detailed derivations of each of these kernels is found in the Supplemental Material.
3.1 The Spectral Mixture Kernel
A spectral Gaussian (SG) kernel is deﬁned by an amplitude spectrum with a single Gaussian distri-
bution reﬂected about the origin 

[N (ω;−µ  ν) + N (ω; µ  ν)]  

SSG(ω; θ) =

(4)
where θ = {µ  ν} are the kernel hyperparameters  µ represents the peak frequency  and the variance
ν is a scale parameter that controls the spread of the spectrum around µ. This spectrum is a function
of angular frequency. The Fourier transform of (4) results in the stationary  positive deﬁnite auto-
covariance function

1
2

kSG(τ ; θ) = exp(− 1
2

√

ντ 2) cos(µτ ) 

(5)
where stationarity implies dependence on input domain differences k(τ ; θ) = k(x  x(cid:48); θ) with τ =
x− x(cid:48). The SG kernel may also be derived by considering a latent signal f (x) =
2 cos(ω(x + φ))
with frequency uncertainty ω ∼ N (µ  ν) and phase offset ωφ. The kernel is the auto-covariance
function for f (x)  such that kSG(τ ; θ) = cov(f (x)  f (x+τ )). When computing the auto-covariance 
the frequency ω is marginalized out  providing the kernel in (5) that includes all frequencies in the
spectral domain with probability 1.
A weighted  linear combination of SG kernels gives the spectral mixture (SM) kernel [24] 

(6)
where θq = {aq  νq  µq} and θ = {θq} has 3Q degrees of freedom. The SM kernel may be derived
as the Fourier transform of the spectral density SSM(ω; θ) or as the auto-covariance of latent func-

Q(cid:88)
(cid:112)2aq cos(ωq(x + φq)) with uncertainty in angular frequency ωq ∼ N (µq  νq).

tions f (x) =(cid:80)Q

aqSSG(ω; θq) 

aqkSG(τ ; θq) 

SSM(ω; θ) =

kSM(τ ; θ) =

Q(cid:88)

q=1

q=1

q=1

3

Figure 1: Latent functions drawn for two channels f1(x) (blue) and f2(x) (red) using the CSM kernel (left)
and rank-1 SM-LMC kernel (center). The functions are comprised of two SG components centered at 4 and 5
Hz. For the CSM kernel  we set the phase shift ψc(cid:48) 2 = π. Right: the cross-amplitude (purple) and cross-phase
(green) spectra between f1(x) and f2(x) are shown for the CSM kernel (solid) and SM-LMC kernel (dashed).
The ability to tune phase relationships is beneﬁcial for kernel design and interpretation.

The moniker for the SM kernel in (6) reﬂects the mixture of Gaussian components that deﬁne the
spectral density of the kernel. The SM kernel is able to represent any stationary covariance kernel
given large enough Q; to name a few  this includes any combination of squared exponential  Mat`ern 
rational quadratic  or periodic kernels [9  16  24].

3.2 The Cross-Spectral Mixture Kernel
A multi-output version of the SM kernel uses the SG kernel directly within the LMC framework:

KSM-LMC(τ ; θ) =

BqkSG(τ ; θq) 

(7)

phase spectrum. Speciﬁcally  each channel is merely a weighted sum of(cid:80)

where Q SG kernels are shared among the outputs via the coregionalization matrices {Bq}Q
q=1. A
generalized  non-stationary version of this SM-LMC kernel was proposed in [23] using the Gaussian
process regression network (GPRN) [26]. The marginal distribution for any single channel is simply
a Gaussian process with a SM covariance kernel. While this formulation is capable of providing
a full cross-amplitude spectrum between two channels  it contains no information about a cross-
q Rq latent functions
where Rq = rank(Bq). Whereas these functions are shared exactly across channels  our novel CSM
kernel shares phase-shifted versions of these latent functions across channels.
Deﬁnition 3.1. The cross-spectral mixture (CSM) kernel takes the form

Q(cid:88)

q=1

(cid:1)(cid:1)  

c(cid:48)q − φr

cq

(8)

q=1 Rq(2C − 1) degrees of freedom 
cq respectively represent the amplitude and shift in the input space for latent functions

(cid:44) 0}Rq

(cid:113)

r=1

Rq(cid:88)

Q(cid:88)

q  φr

cq and φr

q=1
q  φr
1q

(cid:0)τ + φr

νqτ 2) cos(cid:0)µq
c(cid:48)q exp(− 1
q=1 has 2Q +(cid:80)Q
cqar
ar
2
r=1}Q
(cid:41)
Bq(cid:101)kSG(τ ; θq)

kc c(cid:48)
CSM(τ ; θ) =
where θ = {νq  µq {ar
and ar
associated with channel c. In the LMC framework  the CSM kernel is

(cid:40) Q(cid:88)
(cid:101)kSG(τ ; θq) = exp(− 1

Rq(cid:88)
cq =(cid:112)ar
where(cid:101)kSG(τ  θq) is phasor notation of the SG kernel  Bq is rank-Rq  {βr

KCSM(τ ; θ) = Re

νqτ 2 + jµqτ ) 

Bq =

βr

r=1

q=1

2

 

cq} are complex scalar
cq is an alternative phase representation.
√−1  Re{·} returns the real component of its argument  and

coefﬁcients encoding amplitude and phase  and ψr
cq
We use complex notation where j =
† represents the complex conjugate of β.
β
Both the CSM and SM-LMC kernels force the marginal distribution of data from a single chan-
nel to be a Gaussian process with a SM covariance kernel. The CSM kernel is derived in the
Supplemental Material by considering functions represented by phase-shifted sinusoidal signals 
iid∼ N (µq  νq). Computing the

fc(x) = (cid:80)Q

(cid:112)2ar

cq))  where each ωr
q

(cid:80)Rq

(cid:44) µqφr

q (x + φr

cq cos(ωr

cross-covariance function cov(fc(x)  fc(cid:48)(x + τ )) provides the CSM kernel.
A comparison between draws from Gaussian processes with CSM and SM-LMC kernels is shown
in Figure 1. The utility of the CSM kernel is clearly illustrated by its ability to encode phase

r=1

q=1

q(βr
βr

q)† 

cq exp(−jψr

cq) 

4

Time00.20.40.60.81-4-2024f1(x)f2(x)Time00.20.40.60.81-4-2024f1(x)f2(x)Frequency33.544.555.56Amplitude00123Phase-3.14-1.570 1.57 3.14 resenting the cross-spectrum in phasor notation  i.e.  Γc c(cid:48)(ω; Θ) = (cid:80)

information  as well as its powerful functional form of the full cross-spectrum (both amplitude
and phase). The amplitude function Ac c(cid:48)(ω) and phase function Φc c(cid:48)(ω) are obtained by rep-
q(Bq)c c(cid:48)SSG(ω; θq) =
Ac c(cid:48)(ω) exp(jΦc c(cid:48)(ω)).
Interestingly  while the CSM and SM-LMC kernels have identical
marginal amplitude spectra for shared {µq  νq  aq}  their cross-amplitude spectra differ due to the
inherent destructive interference of the CSM kernel (see Figure 1  right).

4 Multi-Channel HMM Analysis
Neuroscientists are interested in examining how the network structure of the brain changes as an-
imals undergo a task  or various levels of arousal [15]. The LFP signal is a modality that allows
researchers to explore this network structure. In the model provided in this section  we cluster seg-
ments of the LFP signal into discrete “brain states” [21]. Each brain state is represented by a unique
cross-spectrum provided by the CSM kernel. The use of the full cross-spectrum to deﬁne brain states
is supported by previous work discovering that 1) the power spectral density of LFP signals indicate
various levels of arousal states in mice [7  21]  and 2) frequency-dependent phase synchrony patterns
change as animals undergo different conditions in a task [11  17  18] (see Figure 2).
The vector-valued observations from C channels are segmented into W contiguous  non-overlapping
windows. The windows are common across channels  such that the C-channel data for window
w ∈ {1  . . .   W} are represented by yw
n . Given data  each
window consists of Nw temporal samples  but the model is deﬁned for any set of sample locations.
n} as emissions from a hidden Markov model (HMM) with L hidden 
We model the observations {yw
discrete states. State assignments are represented by latent variables ζw ∈ {1  . . .   L} for each win-
dow w ∈ {1  . . .   W}. In general  L is a set upper bound of the number of states (brain states [21] 
or “clusters”)  but the model can shrink down and infer the number of states needed to ﬁt the data.
This is achieved by deﬁning the dynamics of the latent states according to a Bayesian HMM [14]:

nC]T at sample location xw

n1  . . .   yw

n = [yw

ζ1 ∼ Categorical(ρ0) 

ζw ∼ Categorical(ρζw−1 ) ∀w ≥ 2 

ρ0  ρ(cid:96) ∼ Dirichlet(ν) 

where the initial state assignment is drawn from a categorical distribution with probability vector ρ0
and all subsequent states assignments are drawn from the transition vector ρζw−1. Here  ρ(cid:96)h is the
probability of transitioning from state (cid:96) to state h. The vectors {ρ0  ρ1  . . .   ρL} are independently
drawn from symmetric Dirichlet distributions centered around ν = [1/L  . . .   1/L] to impose spar-
sity on transition probabilities. In effect  this allows the model to learn the number of states needed
for the data (i.e.  fewer than L) [3].
Each cluster (cid:96) ∈ {1  . . .   L} is assigned GP parameters θ(cid:96). The latent cluster assignment ζw for
window w indicates which set of GP parameters control the emission distribution of the HMM:

n ∼ N (f w(xw
n )  H−1
yw
where (K(x  x(cid:48); θ(cid:96)))c c(cid:48) = kc c(cid:48)
CSM(x  x(cid:48); θ(cid:96)) is the CSM kernel  and the cluster-dependent precision
H ζw = diag(ηζw ) generates independent Gaussian observation noise. In this way  each window w
is modeled as a stochastic process with a multi-channel cross-spectrum deﬁned by θζw.

f w(x) ∼ GP(0  K(x  x(cid:48); θζw )) 

(9)

) 

ζw

DELTA Waves

THETA Waves

ALPHA Waves

BETA Waves

Figure 2: A short segment of LFP data recorded from the basolateral amygdala and infralimbic cortex is
shown on the left. The cross-amplitude and phase spectra are produced using Welch’s averaged periodogram
method [22] for several consecutive 5 second windows of LFP data. Frequency dependent phase synchrony lags
are consistently present in the cross-phase spectrum  motivating the CSM kernel. This frequency dependency
aligns with preconcieved notions of bands  or brain waves (e.g.  8-12 Hz alpha waves).

5

Time (sec)0.10.20.30.40.50.60.70.8PotentialRawLFPDataBLAIL CortexFrequency (Hz)0246810121416AmplitudeCross-AmplitudeSpectrumFrequency (Hz)0246810121416Lag (rad)-1.5-1-0.500.51Cross-PhaseSpectrum5

Inference

1   . . .   yw
Nw

(cid:96)=0 {ζw}W

(cid:96)=1 and model variables Ω = {{ρ(cid:96)}L

A convenient notation vectorizes all observations within a window  yw = vec([yw
]T ) 
where vec(A) is the vectorization of matrix A; i.e.  the ﬁrst Nw elements of yw are observations
from channel 1  up to the last Nw elements of yw belonging to channel C. Because samples are
obtained on an evenly spaced temporal grid  we ﬁx Nw = N and align relative sample locations
within a window to an oracle xw = x = [x1  . . .   xN ]T for all w.
The model in Section 4 generates the set of observations Y = {yw}W
w=1 at aligned sample locations
x given kernel hyperparameters Θ = {θ(cid:96)  η(cid:96)}L
w=1}.
mate posterior distribution q(Ω) = q(ζ1:W )(cid:81)L
The latent variables Ω are inverted using mean-ﬁeld variational inference [3]  obtaining an approxi-
(cid:96)=0 Dir(ρ(cid:96); α(cid:96)). The approximate posterior is chosen
to minimize the KL divergence to the true posterior distribution p(Ω|Y   Θ  x) using the standard
variational EM method detailed in Chapter 3 of [3].
maximize the expected marginal log-likelihood Q =(cid:80)W
(cid:80)L
During each iteration of the variational EM algorithm  the kernel hyperparameters Θ are chosen to
(cid:96)=1 q(ζw = (cid:96)) log N (yw; 0  Γ(cid:96))via
to brain state (cid:96)  and Γ(cid:96) = Re{(cid:101)Γ(cid:96)} is the CSM kernel matrix for state (cid:96) with the complex form
(cid:101)Γ(cid:96) = (cid:80)
gradient ascent  where q(ζw = (cid:96)) is the marginal posterior probability that window w is assigned
(cid:96) ⊗ I N . Performing gradient ascent requires the derivatives
∂Q
(cid:96) yw [16]. A na¨ıve implementation of
∂Θj
this gradient requires the inversion of Γ(cid:96)  which has complexity O(N 3C 3) and storage requirements
O(N 2C 2) since a simple method to invert a sum of Kronecker products does not exist.
A common trick for GPs with evenly spaced samples (e.g.  a temporal grid) is to use the discrete
Fourier transform (DFT) to approximate the inverse of Γ(cid:96) by viewing this as an approximately
circulant matrix [5  12]. These methods can speed up inference because circulant matrices are diag-
onalizable by the DFT coefﬁcient matrix. Adjusting these methods to the multi-output formulation 
we show how the DFT of the marginal covariance matrices retains the cross-spectrum information.
Proposition 5.1. Let yw ∼ N (0  Γζw ) represent the marginal likelihood of circularly-symmetric
[8] real-valued observations in window w  and denote the concatenation of the DFT of each channel
as zw = (I C ⊗ U )†yw where U is the N × N unitary DFT matrix. Then  zw is shown in the
Supplemental Material to have the complex normal distribution [8]:

q ⊗(cid:101)kSG(x  x; θ(cid:96)) + H−1
(cid:96)w − Γ−1

) where α(cid:96)w = Γ−1

w (cid:96) tr((α(cid:96)wαT

(cid:96) ) ∂Γ(cid:96)
∂Θj

= 1
2

(cid:80)

q B(cid:96)

w=1

Q(cid:88)

zw ∼ CN (0  2Sζw ) 

S(cid:96) = δ−1

q ⊗ W (cid:96)
B(cid:96)

q + H−1

(cid:96) ⊗ I N  

(10)

q=1

where δ = xi+1 − xi for all i = 2  . . .   N  and W (cid:96)
diagonal. The spectral density SSG(ω; θ) = [SSG(ω1; θ)  . . .   SSG(ω(cid:98) N +1
angular frequencies ω = 2π
N δ

(cid:5)(cid:3)  and 0 = [0  . . .   0] is a row vector of(cid:4) N−1

q ≈ diag([SSG(ω; θ(cid:96)q)  0]) is approximately
2 (cid:99); θ)] is found via (4) at

(cid:2)0  1  . . .  (cid:4) N

(cid:5) zeros.

2

2

The hyperparameters of the CSM kernels Θ may now be optimized from the expected marginal
log-likelihood of Z = {zw}W
w=1 instead of Y . Conceptually  the only difference during the ﬁtting
process is that  with the latter  derivatives of the covariance kernel are used  while  with the for-
mer  derivatives of the power spectral density are used. Computationally  this method improves the
na¨ıve O(N 3C 3) complexity of ﬁtting the standard CSM kernel to O(N C 3) complexity. Memory
requirements are also reduced from O(N 2C 2) to O(N C 2). The reason for this improvement is that
S(cid:96) is now represented as N independent C × C blocks  reducing the inversion of S(cid:96) to inverting a
permuted block-diagonal matrix.

6 Experiments
Section 6.1 demonstrates the performance of the CSM kernel and the accuracy of the DFT approx-
imation In Section 6.2  the DFT approximation for the CSM kernel is used in a Bayesian HMM
framework to cluster time-varying multi-channel LFP data based on the full cross-spectrum; the
HMM states here correspond to states of the brain during LFP recording.

6

0 (0)

CSM

CSM

SE-LMC
SM-LMC

5550 (1240)
412 (184)
204 (71.7)

Figure 3: Time-series data is drawn from a Gaussian process with
a known CSM covariance kernel  where the domain restricted to a
ﬁxed number of seconds. A Gaussian process is then ﬁtted to this
data using the DFT approximation. The KL-divergence of the ﬁtted
marginal likelihood from the true marginal likelihood is shown.
6.1 Performance and Inference Analysis
The performance of the CSM kernel is compared to the SM-LMC kernel and SE-LMC (squared
exponential) kernel. Each of these models allow Q=20  and the rank of the coregionalization ma-
trices is varied from rank-1 to rank-3. For a given rank  the CSM kernel always obtains the largest
marginal likelihood for a window of LFP data  and the marginal likelihood always increases for
increasing rank. To penalize the number of kernel parameters (e.g.  a rank-3  Q=20 CSM kernel for
7 channels has 827 free parameters to optimize)  the Akaike information criterion (AIC) is used for
model selection [1]. For this reason  we do not test rank greater than 3. Table 1 shows that a rank-2
CSM kernel is selected using this criterion  followed by a rank-1 CSM kernel. To show the rank-2
CSM kernel is consistently selected as the preferred model we report means and standard deviations
of AIC value differences across 30 different randomly selected 3-second windows of LFP data.
Next  we provide numerical results for the conditions required when using the DFT approximation
in (10). This allows for one to deﬁne details of a particular application in order to determine if the
DFT approximation to the CSM kernel is appropriate. A CSM kernel is deﬁned for two outputs
with a single Gaussian component  Q = 1. The mean frequency and variance for this component
of interest  namely greater than 1 Hz; therefore  we test values of(cid:101)µ1 ∈ { 1
are set to push the limits of the application. For example  with LFP data  low frequency content is
variances at these frequencies to be around(cid:101)ν1 = 1 Hz2. A conversion to angular frequency gives
2   1  3} Hz. We anticipate
µ1 = 2π(cid:101)µ1 and ν1 = 4π2(cid:101)ν1. The covariance matrix Γ in (3) is formed using these parameters  a

ﬁxed noise variance  and N observations on a time grid with sampling rate of 200 Hz. Data y are
drawn from the marginal likelihood with covariance Γ.
A new CSM kernel is ﬁt to y using the DFT approximation  providing an estimate ˆΓ. The KL
divergence of the ﬁtted marginal likelihood from the true marginal likelihood is

KL(p(y|ˆΓ)||p(y|Γ)) =

(cid:34)

1
2

log

(cid:35)
|Γ|
|ˆΓ| − N + tr(Γ−1 ˆΓ)

 

Table 1: The mean and standard de-
viation of the difference between the
AIC value of a given model and the
AIC value of the rank-2 CSM model.
Lower values are better.

Model
SE-LMC
SM-LMC

CSM

SE-LMC
SM-LMC

∆ AIC

4770 (993)
512 (190)
109 (110)
5180 (1120)
325 (167)

Rank

1
1
1
2
2
2
3
3
3

where | · | and tr(·) are the determinant and trace operators 

N KL(p(y|ˆΓ)||p(y|Γ)) for various values of(cid:101)µ1 and N provides the results in Figure 3. This plot

1
shows that the DFT approximation struggles to resolve low frequency components unless the se-
ries length is sufﬁciently long. Due to the approximation error  when using the DFT approximation
on LFP data we a priori ﬁlter out frequencies below 1.5 Hz and perform analyses with a series
length of 3 seconds. This ensures the DFT approximation represents the true covariance matrix. The
following application of the CSM kernel uses these settings.

respectively.

Computing

Including the CSM Kernel in a Bayesian Hierarchical Model

6.2
We analyze 12 hours of LFP data of a mouse transitioning between different stages of sleep [7  21].
Observations were recorded simultaneously from 4 channels [6]  high-pass ﬁltered at 1.5 Hz  and
subsampled to 200 Hz. Using 3 second windows provides N = 600 and W = 14  400. The HMM
was implemented with the number of kernel components Q = 15 and the number of states L = 7.

7

Series Length (seconds)012345678KL Divergence00.0050.010.0150.020.0250.030.0357 = 0.5 Hz7 = 1 Hz7 = 3 HzFigure 4: A subset of results from the Bayesian HMM analysis of brain states. In the upper left  the full cross-
spectrum for an arbitrary state (state 7) is plotted. In the upper right  the amplitude (top) and phase (bottom)
functions for the cross-spectrum between the Dorsomedial Striatum (DMS) and Hippocampus (DHipp) are
shown for all seven states. On the bottom  the maximum likelihood state assignments are shown and compared
to the state assignments from [7]. The same colors between the CSM state assignments and the phase and
amplitude functions correspond to the same state. These colors are alligned to the [7] states  but there is no
explicit relationship between the colors of the two state sequences.

This was chosen because sleep staging tasks categorize as many as seven states: various levels of
rapid eye movement  slow wave sleep  and wake [20]. Although rigorous model selection on L
is necessary to draw scientiﬁc conclusions from the results  the purpose of this experiment is to
illustrate the utility of the CSM kernel in this application.
An illustrative subset of the results are shown in Figure 4. The full cross-spectrum is shown for
a single state (state 7)  and the cross-spectrum between the Dorsomedial Striatum and the Dorsal
Hippocampus are shown for all states. Furthermore  we show the progression of these brain state
assignments over 3 hours and compare them to states from the method of [7]  where statistics of the
Hippocampus spectral density were clustered in an ad hoc fashion. To the best of our knowledge 
this method represents the most relevant and accurate results for sleep staging from LFP signals in
the neuroscience literature. From these results  it is apparent that our clusters pick up sub-states of
[7]. For instance  states 3  6  and 7 all appear with high probability when the method from [7] infers
state 3. Observing the cross-phase function of sub-state 7 reveals striking differences from other
states in the theta wave (4-7 Hz) and the alpha wave (8-15 Hz). This cross-phase function is nearly
identical for states 2 and 5  implying that signiﬁcant differences in the cross-amplitude spectrum
may have played a role in identifying the difference between these two brain states.
Many more of these interesting details exist due to the expressive nature of the CSM kernel. As a
full interpretation of the cross-spectrum results is not the focus of this work  we contend that the
CSM kernel has the potential to have a tremendous impact in ﬁelds such as neuroscience  where the
dynamics of cross-spectrum relationships of LFP signals are of great interest.

7 Conclusion
This work introduces the cross-spectral mixture kernel as an expressive kernel capable of extracting
patterns for multi-channel observations. Combined with the powerful nonparametric representation
of a Gaussian process  the CSM kernel expresses a functional form for every pairwise cross-spectrum
between channels. This is a novel approach that merges Gaussian processes in the machine learning
community to standard signal processing techniques. We believe the CSM kernel has the potential
to impact a broad array of disciplines since the kernel can trivially be extended to any time-series
application where Gaussian processes and the cross-spectrum are of interest.
Acknowledgments
The research reported here was funded in part by ARO  DARPA  DOE  NGA and ONR.

8

01356AmplitudeBasalAmy01356AmplitudeDLS01356AmplitudeDMS05101501356AmplitudeFrequency051015Frequency051015Frequency051015DHippFrequency051015051015051015−3.14−1.5701.573.14Phase−3.14−1.5701.573.14Phase−3.14−1.5701.573.14Phase051015−3.14−1.5701.573.14Phase05101500.511.522.5Frequency(Hz)Amplitude051015−3−2−10123Frequency(Hz)PhaseMinutes 020406080100120140160Dzirasa et al.CSM KernelState 1State 2State 3State 4State 5State 6State 7References
[1] H. Akaike. A new look at the statistical model identiﬁcation. IEEE Transactions on Automatic Control 

19(6):716–723  1974.

[2] M. A. Alvarez  L. Rosasco  and N. D. Lawrence. Kernels for vector-valued functions: a review. Founda-

tions and Trends in Machine Learning  4(3):195–266  2012.

[3] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis  University College

London.

[4] R. Caruana. Multitask learning. Machine Learning  28(1):41–75  1997.
[5] C. R. Dietrich and G. N. Newsam. Fast and exact simulation of stationary Gaussian processes through
circulant embedding of the covariance matrix. SIAM Journal on Scientiﬁc Computing  18(4):1088–1107 
1997.

[6] K. Dzirasa  R. Fuentes  S. Kumar  J. M. Potes  and M. A. L. Nicolelis. Chronic in vivo multi-circuit

neurophysiological recordings in mice. Journal of Neuroscience Methods  195(1):36–46  2011.

[7] K. Dzirasa  S. Ribeiro  R. Costa  L. M. Santos  S. C. Lin  A. Grosmark  T. D. Sotnikova  R. R. Gainet-
dinov  M. G. Caron  and M. A. L. Nicolelis. Dopaminergic control of sleep–wake states. The Journal of
Neuroscience  26(41):10577–10589  2006.

[8] R. G. Gallager. Principles of digital communication. pages 229–232  2008.
[9] M. G¨onen and E. Alpaydn. Multiple kernel learning algorithms. JMLR  12:2211–2268  2011.
[10] P. Goovaerts. Geostatistics for Natural Resources Evaluation. Oxford University Press  1997.
[11] G. G. Gregoriou  S. J. Gotts  H. Zhou  and R. Desimone. High-frequency  long-range coupling between

prefrontal and visual cortex during attention. Science  324(5931):1207–1210  2009.

[12] M. L´azaro-Gredilla  J. Qui˜nonero Candela  C. E. Rasmussen  and A. R. Figueiras-Vidal. Sparse spectrum

Gaussian process regression. JMLR  (11):1865–1881  2010.

[13] J. R. Lloyd  D. Duvenaud  R. Grosse  J. B. Tenenbaum  and Z. Ghahramani. Automatic construction and

natural-language description of nonparametric regression models. AAAI  2014.

[14] D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report  1997.
[15] D. Pfaff  A. Ribeiro  J. Matthews  and L. Kow. Concepts and mechanisms of generalized central nervous

system arousal. ANYAS  2008.

[16] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. 2006.
[17] P. Sauseng and W. Klimesch. What does phase information of oscillatory brain activity tell us about

cognitive processes? Neuroscience and Biobehavioral Reviews  32:1001–1013  2008.

[18] C. M. Sweeney-Reed  T. Zaehle  J. Voges  F. C. Schmitt  L. Buentjen  K. Kopitzki  C. Esslinger  H. Hin-
richs  H. J. Heinze  R. T. Knight  and A. Richardson-Klavehn. Corticothalamic phase synchrony and
cross-frequency coupling predict human memory formation. eLIFE  2014.

[19] Y. W. Teh  M. Seeger  and M. I. Jordan. Semiparametric latent factor models. AISTATS  10:333–340 

2005.

[20] M. A. Tucker  Y. Hirota  E. J. Wamsley  H. Lau  A. Chaklader  and W. Fishbein. A daytime nap containing
solely non-REM sleep enhances declarative but not procedural memory. Neurobiology of Learning and
Memory  86(2):241–7  2006.

[21] K. Ulrich  D. E. Carlson  W. Lian  J. S. Borg  K. Dzirasa  and L. Carin. Analysis of brain states from

multi-region LFP time-series. NIPS  2014.

[22] P. D. Welch. The use of fast Fourier transform for the estimation of power spectra: A method based on
time averaging over short  modiﬁed periodograms. IEEE Transactions on Audio and Electroacoustics 
15(2):70–73  1967.

[23] A. G. Wilson. Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian

processes. PhD thesis  University of Cambridge  2014.

[24] A. G. Wilson and R. P. Adams. Gaussian process kernels for pattern discovery and extrapolation. ICML 

2013.

[25] A. G. Wilson  E. Gilboa  A. Nehorai  and J. P. Cunningham. Fast kernel learning for multidimensional

pattern extrapolation. NIPS  2014.

[26] A. G. Wilson and D. A. Knowles. Gaussian process regression networks. ICML  2012.
[27] Z. Yang  A. J. Smola  L. Song  and A. G. Wilson. ´A la carte – learning fast kernels. AISTATS  2015.

9

,Tofigh Naghibi Mohamadpoor
Beat Pfister
Kyle Ulrich
Kafui Dzirasa
Lawrence Carin