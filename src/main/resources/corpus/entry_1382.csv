2019,Deep ReLU Networks Have Surprisingly Few Activation Patterns,The success of deep networks has been attributed in part to their expressivity: per parameter  deep networks can approximate a richer class of functions than shallow networks. In ReLU networks  the number of activation patterns is one measure of expressivity; and the maximum number of patterns grows exponentially with the depth. However  recent work has showed that the practical expressivity of deep networks - the functions they can learn rather than express - is often far from the theoretical maximum. In this paper  we show that the average number of activation patterns for ReLU networks at initialization is bounded by the total number of neurons raised to the input dimension. We show empirically that this bound  which is independent of the depth  is tight both at initialization and during training  even on memorization tasks that should maximize the number of activation patterns. Our work suggests that realizing the full expressivity of deep networks may not be possible in practice  at least with current methods.,Deep ReLU Networks Have

Surprisingly Few Activation Patterns

Boris Hanin

Facebook AI Research
Texas A&M University
bhanin@math.tamu.edu

David Rolnick

drolnick@seas.upenn.edu

University of Pennsylvania

Philadelphia  PA USA

Abstract

The success of deep networks has been attributed in part to their expressivity: per
parameter  deep networks can approximate a richer class of functions than shallow
networks. In ReLU networks  the number of activation patterns is one measure of
expressivity; and the maximum number of patterns grows exponentially with the
depth. However  recent work has showed that the practical expressivity of deep
networks – the functions they can learn rather than express – is often far from the
theoretical maximum. In this paper  we show that the average number of activation
patterns for ReLU networks at initialization is bounded by the total number of
neurons raised to the input dimension. We show empirically that this bound  which
is independent of the depth  is tight both at initialization and during training  even
on memorization tasks that should maximize the number of activation patterns.
Our work suggests that realizing the full expressivity of deep networks may not be
possible in practice  at least with current methods.

Introduction

1
A fundamental question in the theory of deep learning is why deeper networks often work better in
practice than shallow ones. One proposed explanation is that  while even shallow neural networks
are universal approximators [3  7  9  16  21]  there are functions for which increased depth allows
exponentially more efﬁcient representations. This phenomenon has been quantiﬁed for various
complexity measures [4  5  6  10  18  19  22  23  24  25]. However  authors such as Ba and Caruana
have called into question this point of view [2]  observing that shallow networks can often be trained
to imitate deep networks and thus that functions learned in practice by deep networks may not achieve
the full expressive power of depth.
In this article  we attempt to capture the difference between the maximum complexity of deep
networks and the complexity of functions that are actually learned (see Figure 1). We provide
theoretical and empirical analyses of the typical complexity of the function computed by a ReLU
network N . Given a vector ✓ of its trainable parameters  N computes a continuous and piecewise
linear function x 7! N (x; ✓). Each ✓ thus is associated with a partition of input space Rnin into
activation regions  polytopes on which N (x; ✓) computes a single linear function corresponding to a
ﬁxed activation pattern in the neurons of N .
We aim to count the number of such activation regions. This number has been the subject of previous
work (see §1.1)  with the majority concerning large lower bounds on the maximum over all ✓ of
the number of regions for a given network architecture. In contrast  we are interested in the typical
behavior of ReLU nets as they are used in practice. We therefore focus on small upper bounds for the
average number of activation regions present for a typical value of ✓. Our main contributions are:
• We give precise deﬁnitions and prove several fundamental properties of both linear and
• We prove in Theorem 5 an upper bound for the expected number of activation regions in a
ReLU net N . Roughly  we show that if nin is the input dimension and C is a cube in input

activation regions  two concepts that are often conﬂated in the literature (see §2).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Schematic illustration of the space of functions f : Rnin ! Rnout. For a given neural
network architecture  there is a set Fexpress of functions expressible by that architecture. Within
this set  the functions corresponding to networks at initialization are concentrated within a set Finit.
Intermediate between Finit and Fexpress is a set Flearn containing the functions which the network has
non-vanishing probability of learning using gradient descent. (None of these is of course a formal
deﬁnition.) This paper seeks to demonstrate the gap between Fexpress and Flearn and that  at least for
certain measures of complexity  there is a surprisingly small gap between Finit and Flearn.
space Rnin  then  under reasonable assumptions on network gradients and biases 

#activation regions of N that intersect C

(T #neurons)nin

nin!



  T > 0.

(1)

vol(C)

• This bound holds in particular for deep ReLU nets at initialization  and is in sharp contrast to
the maximum possible number of activation patterns  which is exponential in depth [23  28].
• Theorem 5 also strongly suggests that the bounds on number of activation regions continue
to hold approximately throughout training. We empirically verify that this behavior holds 
even for networks trained on memorization-based tasks (see §4 and Figures 3-6).

Figure 2: Function deﬁned by a ReLU network of depth 5 and width 8 at initialization. Left: Partition
of the input space into regions  on each of which the activation pattern of neurons is constant. Right:
the function computed by the network  which is linear on each activation region.
It may seem counterintuitive that the number of activation patterns in a ReLU net is effectively
capped far below its theoretical maximum during training  even for tasks where a higher number of
regions would be advantageous (see §4). We provide in §3.2-3.3 two intuitive explanations for this
phenomenon. The essence of both is that many activation patterns can be created only when a typical
neuron z in N turns on/off repeatedly  forcing the value of its pre-activation z(x) to cross the level of
its bias bz many times. This requires (i) signiﬁcant overlap between the range of z(x) on the different
activation regions of x 7! z(x) and (ii) the bias bz to be picked within this overlap. Intuitively  (i)
and (ii) require either large or highly coordinated gradients. In the former case  z(x) oscillates over a
large range of outputs and bz can be random  while in the latter z(x) may oscillate only over a small
range of outputs and bz is carefully chosen. Neither is likely to happen with a proper initialization.
Moreover  both appear to be difﬁcult to learn with gradient-based optimization.
The rest of this article is structured as follows. Section 2 gives formal deﬁnitions and some important
properties of both activation regions and the closely related notion of linear regions (see Deﬁnitions
1 and 2).Section 3 contains our main technical result  Theorem 5  stated in §3.1. Sections 3.2 and
3.3 provide heuristics for understanding Theorem 5 and its implications. Finally  §4 is devoted to
experiments that push the limits of how many activation regions a ReLU network can learn in practice.
1.1 Relation to Prior Work
We consider the typical number of activation regions in ReLU nets. Interesting bounds on the
maximum number of regions are given in [4  6  19  22  23  25  26  28]. Our main theoretical result 

2

Theorem 5  is related to [14]  which conjectured that our Theorem 5 should hold and proved bounds
for other notions of average complexity of activation regions. Theorem 5 is also related in spirit to
[8]  which uses a mean ﬁeld analysis of wide ReLU nets to show that they are biased towards simple
functions. Our empirical work (e.g. §4) is related both to the experiments of [20] and to those of
[1  29]. The last two observe that neural networks are capable of ﬁtting noisy or completely random
data. Theorem 5 and experiments in §4 give a counterpoint  suggesting limitations on the complexity
of random functions that ReLU nets can ﬁt in practice (see Figures 4-6).

Figure 3: The average number of activation regions in a 2D cross-section of input space  for fully
connected networks of various architectures training on MNIST. Left: a closeup of 0.5 epochs of
training. Right: 20 epochs of training. The notation [20  20  20] indicates a network with three layers 
each of width 20. The number of activation regions starts at approximately (#neurons)2/2  as
predicted by Theorem 5 (see Remark 1). This value changes little during training  ﬁrst decreasing
slightly and then rebounding  but never increasing exponentially. Each curve is averaged over 10
independent training runs  and for each run the number of regions is averaged over 5 different 2D
cross-sections  where for each cross-section we count the number of regions in the (inﬁnite) plane
passing through the origin and two random training examples. Standard deviations between different
runs are shown for each curve. See Appendix A for more details.
2 How to Think about Activation Regions
Before stating our main results on counting activation regions in §3  we provide a formal deﬁnition
and contrast them with linear regions in §2.1. We also note in §2.1 some simple properties of
activation regions that are useful both for understanding how they are built up layer by layer in a deep
ReLU net and for visualizing them. Then  in §2.2  we explain the relationship between activation
regions and arrangements of bent hyperplanes (see Lemma 4).
2.1 Activation Regions vs. Linear Regions
Our main objects of study in this article are activation regions  which we now deﬁne.
Deﬁnition 1 (Activation Patterns/Regions). Let N be a ReLU net with input dimension nin. An
activation pattern for N is an assignment to each neuron of a sign:

A := {az  z a neuron in N} 2 {1  1}#neurons.

Fix ✓  a vector of trainable parameters in N   and an activation pattern A. The activation region
corresponding to A  ✓ is

R(A; ✓) := {x 2 Rnin

z a neuron in N} 

|

(1)az (z(x; ✓)  bz) > 0 

where neuron z has pre-activation z(x; ✓)  bias bz  and post-activation max{0  z(x; ✓)  bz}. We
say the activation regions of N at ✓ are the non-empty activation regions R(A  ✓).
Perhaps the most fundamental property of activation regions is their convexity.
Lemma 1 (Convexity of Activation Regions). Let N be a ReLU net. Then for every activation
pattern A and any vector ✓ of trainable parameters for N each activation region R(A; ✓) is convex.
We note that Lemma 1 has been observed before (e.g. Theorem 2 in [23])  but in much of the
literature the difference between linear regions (deﬁned below)  which are not necessarily convex 
and activation regions  which are  is ignored. It turns out that Lemma 1 holds for any piecewise
linear activation  such as leaky ReLU and hard hyperbolic tangent/sigmoid. This fact seems to be less

3

well-known (see Appendix B.1 for a proof). To provide a useful alternative description of activation
regions for a ReLU net N   a ﬁxed vector ✓ of trainable parameters and neuron z of N   deﬁne

Hz(✓) := {x 2 Rnin | z(x; ✓) = bz}.

(2)
The sets Hz(✓) can be thought of as “bent hyperplanes” (see Lemma 4). The non-empty activation
regions of N at ✓ are the connected components of Rnin with all the bent hyperplanes Hz(✓) removed:
Lemma 2 (Activation Regions as Connected Components). For any ReLU net N and any vector ✓
of trainable parameters

activation regions (N   ✓) = connected componentsRnin✏ [neurons z

Hz(✓).

We prove Lemma 2 in Appendix B.2. We may compare activation regions with linear regions  which
are the regions of input space on which the network deﬁnes different linear functions.
Deﬁnition 2 (Linear Regions). Let N be a ReLU net with input dimension nin  and ﬁx ✓  a vector of
trainable parameters for N . Deﬁne

(3)

BN (✓) := {x 2 Rnin | rN (· ; ✓) is discontinuous at x}.

The linear regions of N at ✓ are the connected components of input space with BN removed:

linear regions (N   ✓) = connected components (Rnin\BN (✓)) .

Linear regions have often been conﬂated with activation regions  but in some cases they are different.
This can  for example  happen when an entire layer of the network is zeroed out by ReLUs  leading
many distinct activation regions to coalesce into a single linear region. However  the number of
activation regions is always at least as large as the number of linear regions.
Lemma 3 (More Activation Regions than Linear Regions). Let N be a ReLU net. For any parameter
vector ✓ for N   the number of linear regions in N at ✓ is always bounded above by the number of
activation regions in N at ✓. In fact  the closure of every linear region is the closure of the union of
some number of activation regions.
Lemma 3 is proved in Appendix B.3. We prove moreover in Appendix B.4 that generically  the
gradient of rN is different in the interior of most activation regions and hence that most activation
regions lie in different linear regions. In particular  this means that the number of linear regions is
generically very similar to the number of activation regions.
2.2 Activation Regions and Hyperplane Arrangements
Activation regions in depth 1 ReLU nets are given by hyperplane arrangements in Rnin (see [27]).
Indeed  if N is a ReLU net with one hidden layer  then the sets Hz(✓) from (2) are simply hyper-
planes  giving the well-known observation that the activation regions in a depth 1 ReLU net are the
connected components of Rnin with the hyperplanes Hz(✓) removed. The study of regions induced
by hyperplane arrangements in Rn is a classical subject in combinatorics [27]. A basic result is
that for hyperplanes in general position (e.g. chosen at random)  the total number of connected
components coming from an arrangement of m hyperplanes in Rn is constant:

#connected components =

nXi=0✓m

i◆ ' ⇢ mn

n!  
2m 

m  n
m  n

.

(4)

Hence  for random wj  bj drawn from any reasonable distributions the number of activation regions
in a ReLU net with input dimension nin and one hidden layer of size m is given by (4). The situation
is more subtle for deeper networks. By Lemma 2  activation regions are connected components for an
arrangement of “bent” hyperplanes Hz(✓) from (2)  which are only locally described by hyperplanes.
To understand their structure more carefully  ﬁx a ReLU net N with d hidden layers and a vector ✓ of
trainable parameters for N . Write Nj for the network obtained by keeping only the ﬁrst j layers of N
and ✓j for the corresponding parameter vector. The following lemma makes precise the observation
that the hyperplane Hz(✓) can only bend only when it meets a bent hyperplane Hbz(✓) corresponding
to some neuronbz in an earlier layer.
Lemma 4 (Hz(✓) as Bent Hyperplanes). Except on a set of ✓ 2 R#params of measure 0 with respect
to Lebesgue measure  the sets Hz(✓1) corresponding to neurons from the ﬁrst hidden layer are
hyperplanes in Rnin. Moreover  ﬁx 2  j  d. Then  for each neuron z in layer j  the set Hz(✓j)
coincides with a single hyperplane in the interior of each activation region of Nj1.

4

Lemma 4  which follows immediately from the proof of Lemma 7 in Appendix B.1  ensures that

in a small ball near any point that does not belong toSz Hz(✓)  the collection of bent hyperplanes

Hz(✓) look like an ordinary hyperplane arrangement. Globally  however  Hz(✓) can deﬁne many
more regions than ordinary hyperplane arrangements. This reﬂects the fact that deep ReLU nets may
have many more activation regions than shallow networks with the same number of neurons.
Despite their different extremal behaviors  we show in Theorem 5 that the average number of
activation regions in a random ReLU net enjoys depth-independent upper bounds at initialization. We
show experimentally that this holds throughout training as well (see §4). On the other hand  although
we do not prove this here  we believe that the effect of depth can be seen through the ﬂuctuations
(e.g. the variance)  rather than the mean  of the number of activation regions. For instance  for depth
1 ReLU nets  the variance is 0 since for a generic conﬁguration of weights/biases  the number of
activation regions is constant (see (4)). The variance is strictly positive  however  for deeper networks.
3 Main Result
3.1 Formal Statement
Theorem 5 gives upper bounds on the average number of activation regions per unit volume of input
space for a feed-forward ReLU net with random weights/biases. Note that it applies even to highly
correlated weight/bias distributions and hence holds throughout training. Also note that although we
require no tied weights  there are no further constraints on the connectivity between adjacent layers.
Theorem 5 (Counting Activation Regions). Let N be a feed-forward ReLU network with no tied
weights  input dimension nin  output dimension 1  and random weights/biases satisfying:

1. The distribution of all weights has a density with respect to Lebesgue measure on R#weights.
2. Every collection of biases has a density with respect to Lebesgue measure conditional on

the values of all weights and other biases (for identically zero biases  see Appendix D).

3. There exists Cgrad > 0 so that for every neuron z and each m  1  we have

sup
x2Rnin

E [krz(x)km]  Cm

grad.

4. There exists Cbias > 0 so that for any neurons z1  . . .   zk  the conditional distribution of the

biases ⇢bz1  ... bzk

of these neurons given all the other weights and biases in N satisﬁes

sup

b1 ... bk2R

⇢bz1  ... bzk

(b1  . . .   bk)  Ck

bias.

2#neurons

vol(C)

.
#neurons  nin
(5)

Then  there exists 0  T > 0 depending on Cgrad  Cbias with the following property. Suppose that
 > 0. Then  for all cubes C with side length   we have
E [#non-empty activation regions of N in C]

 ⇢(T #neurons)nin /nin! #neurons  nin
Here  the average is with respect to the distribution of weights and biases in N .
Remark 1. The heuristic of §3.3 suggests the average number of activation patterns in N over all of
Rnin is at most (#neurons)nin /nin!  its value for depth 1 networks (see (4)). This is conﬁrmed in
our experiments (see Figures 3-6).
We state and prove a generalization of Theorem 5 in Appendix C. Note that by Theorem 1 (and
Proposition 2) in [12]  Condition 3 is automatically satisﬁed by a fully connected depth d ReLU
net N with independent weights and biases whose marginals are symmetric around 0 and satisfy
Var[weights] = 2/fan-in with the constant Cgrad in 3 depending only on an upper bound for the
sumPd
j=1 1/nj of the reciprocals of the hidden layer widths of N . For example  if the layers of N
have constant width n  then Cgrad depends on the depth and width only via the aspect ratio d/n of
N   which is small for wide networks. Also  at initialization when all biases are independent  the
constant Cbias can be taken simply to be the maximum of the density of the bias distribution.
Below are two heuristics for the second (5). First  in §3.2 we derive the upper bound (5) via an
intuitive geometric argument. Then in §3.3  we explain why  at initialization  we expect the upper
bounds (5) to have matching  depth-independent  lower bounds (to leading order in the number of
neurons). This suggests that the average total number of activation regions at initialization should be
the same for any two ReLU nets with the same number of neurons (see (4) and Figure 3).

5

3.2 Geometric Intuition
We give an intuitive explanation for the upper bounds in Theorem 5  beginning with the simplest case
of a ReLU net N with nin = 1. Activation regions for N are intervals  and at an endpoint x of such
an interval the pre-activation of some neuron z in N equals its bias: i.e. z(x) = bz. Thus 
#{x 2 [a  b] | z(x) = bz}.

#activation regions of N in [a  b]  1 + Xneurons z

Geometrically  the number of solutions to z(x) = bz for inputs x 2 I is the number of times the
horizontal line y = bz intersects the graph y = z(x) over x 2 I. A large number of intersections at a
given bias bz may only occur if the graph of z(x) has many oscillations around that level. Hence 
since bz is random  the graph of z(x) must oscillate many times over a large range on the y axis. This
can happen only if the total variationRx2I |z0(x)| of z(x) over I is large. Thus  if |z0(x)| is typically

of moderate size  we expect only O(1) solutions to z(x) = bz per unit input length  suggesting

E [#activation regions of N in [a  b]] = O ((b  a) · #neurons)  

Is Theorem 5 Sharp?

in accordance with Theorem 5 (cf. Theorems 1 3 in [14]). When nin > 1  the preceding argument 
shows that density of 1-dimensional regions per unit length along any 1-dimensional line segment in
input space is bounded above by the number of neurons in N . A unit-counting argument therefore
suggests that the density of nin-dimensional regions per unit nin-dimensional volume is bounded
above by #neurons raised to the input dimension  which is precisely the upper bound in Theorem 5
in the non-trivial regime where #neurons  nin.
3.3
Theorem 5 shows that  on average  depth does not increase the local density of activation regions.
We give here an intuitive explanation of why this should be the case in wide networks on any ﬁxed
subset of input space Rnin. Consider a ReLU net N with random weights/biases  and ﬁx a layer
index `  1. Note that the map x 7! x(`1) from inputs x to the post-activations of layer `  1 is
itself a ReLU net. Note also that in wide networks  the gradients rz(x) for different neurons in
the same layer are only weakly correlated (cf. e.g. [17]). Hence  for the purpose of this heuristic 
we will assume that the bent hyperplanes Hz(✓) for neurons z in layer ` are independent. Consider
an activation region R for x(`1)(x). By deﬁnition  in the interior of R  the gradient rz(x) for
neurons z in layer ` are constant and hence the corresponding bent hyperplane from (2) inside R
is the hyperplane {x 2 R | hrz  xi = bz}. This in keeping with Lemma 4. The 2/fan-in weight
normalization ensures that for each x

E⇥@xi@xj z(x)⇤ = 2 · i j ) Cov[rz(x)] = 2 Id.

See  for example  equation (17) in [11]. Thus  the covariance matrix of the normal vectors rz of the
hyperplanes Hz(✓) \ R for neurons in layer ` are independent of `! This suggests that  per neuron 
the average contribution to the number of activation regions is the same in every layer. In particular 
deep and shallow ReLU nets with the same number of neurons should have the same average number
of activation regions (see (4)  Remark 1  and Figures 3-6).
4 Maximizing the Number of Activation Regions
While we have seen in Figure 3 that the number of regions does not strongly increase during training
on a simple task  such experiments leave open the possibility that the number of regions would go
up markedly if the task were more complicated. Will the number of regions grow to achieve the
theoretical upper bound (exponential in the depth) if the task is designed so that having more regions
is advantageous? We now investigate this possibility. See Appendix A for experimental details.
4.1 Memorization
Memorization tasks on large datasets require learning highly oscillatory functions with large numbers
of activation regions. Inspired by the work of Arpit et. al. in [1]  we train on several tasks interpolating
between memorization and generalization (see Figure 4) in a certain fraction of MNIST labels have
been randomized. We ﬁnd that the maximum number of activation regions learned does increase with
the amount of noise to be memorized  but only slightly. In no case does the number of activation
regions change by more than a small constant factor from its initial value. Next  we train a network to
memorize binary labels for random 2D points (see Figure 5). Again  the number of activation regions
after training increases slightly with increasing memorization  until the task becomes too hard for the
network and training fails altogether. Varying the learning rate yields similar results (see Figure 6(a)) 
suggesting the small increase in activation regions is probably not a result of hyperparameter choice.

6

Figure 4: Depth 3  width 32 network trained on MNIST with varying levels of label corruption.
Activation regions are counted along lines through input space (lines are selected to pass through
both the origin and randomly selected MNIST examples)  with counts averaged across 100 such lines.
Theorem 5 and [14] predict the expected number of regions should be approximately the number
of neurons (in this case  96). Left: average number of regions plotted against epoch. Curves are
averaged over 40 independent training runs  with standard deviations shown. Right: average number
of regions plotted against average training accuracy. Throughout training the number of regions is
well-predicted by our result. There are slightly  but not exponentially  more regions when memorizing
more datapoints. See Appendix A for more details.

Figure 5: Depth 3  width 32 fully connected ReLU net trained for 2000 epochs to memorize random
2D points with binary labels. The number of regions predicted by Theorem 5 for such a network
is 962/2! = 4608. Left: number of regions plotted against epoch. Curves are averaged over 40
independent training runs  with standard deviations shown. Right: #regions plotted against training
accuracy. The number of regions increased during training  and increased more for greater amounts
of memorization. The exception was for the maximum amount of memorization  where the network
essentially failed to learn  perhaps because of insufﬁcient capacity. See Appendix A for more details.

4.2 The Effect of Initialization
We explore here whether varying the scale of biases and weights at initialization affects the number
of activation regions in a ReLU net. Note that scaling the biases changes the maximum density of the
bias  and thus affects the upper bound on the density of activation regions given in Theorem 5 by
increasing Tbias. Larger  more diffuse biases reduce the upper bound  while smaller  more tightly
concentrated biases increase it. However  Theorem 5 counts only the local rather than global number
of regions. The latter are independent of scaling the biases:
Lemma 6. Let N be a deep ReLU network  and for c > 0 let N bias
multiplying all biases in N by c. Then  N (x) = N bias
constant therefore does not change the total number of activation regions.
In the extreme case of biases initialized to zero  Theorem 5 does not apply. However  as we explain
in Appendix D  zero biases only create fewer activation regions (see Figure 7). We now consider
changing the scale of weights at initialization. In [23]  it was suggested that initializing the weights
of a network with greater variance should increase the number of activation regions. Likewise  the
upper bound in Theorem 5 on the density of activation regions increases as gradient norms increase 
and it has been shown that increased weight variance increases gradient norms [12]. However  this is
again a property of the local  rather than global  number of regions.
Indeed  for a network N of depth d  write N weight
all its weights by c  and let N bias
1/c⇤

for the network obtained from N by multiplying
be obtained from N by dividing the biases in the kth layer by ck.

c

be the network obtained by
(cx)/c. Rescaling all biases by the same

c

c

7

Figure 6: Depth 3  width 32 network trained to memorize 5000 random 2D points with independent
binary labels  for various learning rates and weight scales at initialization. All networks start with
⇡ 4608 regions  as predicted by Theorem 5. Left: None of the learning rates gives a number of
regions larger than a small constant times the initial value. Learning rate 103  which gives the
maximum number of regions  is the learning rate in all other experiments  while 102 is too large
and causes learning to fail. Center: Different weight scales at initialization do not strongly affect
the number of regions. All weight scales are given relative to variance 2/fan-in. Right: For a given
accuracy  the number of regions learned grows with the weight scale at initialization. However  poor
initialization impedes high accuracy. See Appendix A for details.

Figure 7: Activation regions within input space  for a network of depth 3 and width 64 training on
MNIST. (a) Cross-section through the origin  shown at initialization  after one epoch  and after twenty
epochs. The plane is chosen to pass through two sample points from MNIST  shown as black dots.
(b) Cross-section not through the origin  shown at initialization. The plane is chosen to pass through
three sample points from MNIST. For discussion of activation regions at zero bias  see Appendix D.

c

and N bias
1/c⇤

c

(x) = cdN bias
1/c⇤

(x). We therefore conclude that the activation
A scaling argument shows that N weight
are the same. Thus  scaling the weights uniformly is equivalent to
regions of N weight
scaling the biases differently for every layer. We have seen from Lemma 6 that scaling the biases
uniformly by any amount does not affect the global number of activation regions. Therefore  it makes
sense (though we do not prove it) that scaling the weights uniformly should approximately preserve
the global number of activation regions. We test this intuition empirically by attempting to memorize
points randomly drawn from a 2D input space with arbitrary binary labels for various initializations
(see Figure 6). We ﬁnd that neither at initialization nor during training is the number of activation
regions strongly dependent on the weight scaling used for initialization.

5 Conclusion
We have presented theoretical and empirical evidence that the number of activation regions learned in
practice by a ReLU network is far from the maximum possible and depends mainly on the number
of neurons in the network  rather than its depth. This surprising result implies that  at least when
network gradients and biases are well-behaved (see conditions 3 4 in the statement of Theorem 5)  the
partition of input space learned by a deep ReLU network is not signiﬁcantly more complex than that
of a shallow network with the same number of neurons. We found that this is true even after training
on memorization-based tasks  in which we expect a large number of regions to be advantageous for
ﬁtting many randomly labeled inputs. Our results are stated for ReLU nets with no tied weights and
biases (and arbitrary connectivity). We believe that analogous results and proofs hold for residual and
convolutional networks but have not veriﬁed the technical details.

8

References
[1] Devansh Arpit  Stanislaw Jastrzebski  Nicolas Ballas  David Krueger  Emmanuel Bengio 
Maxinder S Kanwal  Tegan Maharaj  Asja Fischer  Aaron Courville  Yoshua Bengio  et al. A
closer look at memorization in deep networks. In ICML  2017.

[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NeurIPS  2014.

[3] Andrew R Barron. Approximation and estimation bounds for artiﬁcial neural networks. Machine

learning  14(1):115–133  1994.

[4] Monica Bianchini and Franco Scarselli. On the complexity of neural network classiﬁers: A
comparison between shallow and deep architectures. IEEE Transactions on Neural Networks
and Learning Systems  25(8):1553–1565  2014.

[5] Nadav Cohen  Or Sharir  and Amnon Shashua. On the expressive power of deep learning: A

tensor analysis. In COLT  pages 698–728  2016.

[6] Francesco Croce  Maksym Andriushchenko  and Matthias Hein. Provable robustness of ReLU

networks via maximization of linear regions. In AISTATS  2018.

[7] George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of

control  signals and systems  2(4):303–314  1989.

[8] Giacomo De Palma  Bobak Toussi Kiani  and Seth Lloyd. Deep neural networks are biased

towards simple functions. Preprint arXiv:1812.10156  2018.

[9] Ken-Ichi Funahashi. On the approximate realization of continuous mappings by neural networks.

Neural networks  2(3):183–192  1989.

[10] Boris Hanin. Universal function approximation by deep neural nets with bounded width and

ReLU activations. Preprint arXiv:1708.02691  2017.

[11] Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In

NeurIPS  2018.

[12] Boris Hanin and Mihai Nica. Products of many large random matrices and gradients in deep

neural networks. Preprint arXiv:1812.05994  2018.

[13] Boris Hanin and David Rolnick. How to start training: The effect of initialization and architec-

ture. In NeurIPS  2018.

[14] Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In ICML  2019.

[15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers:

Surpassing human-level performance on ImageNet classiﬁcation. In ICCV  2015.

[16] Kurt Hornik  Maxwell Stinchcombe  and Halbert White. Multilayer feedforward networks are

universal approximators. Neural networks  2(5):359–366  1989.

[17] Jaehoon Lee  Yasaman Bahri  Roman Novak  Samuel S Schoenholz  Jeffrey Pennington  and

Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In ICLR  2018.

[18] Henry W Lin  Max Tegmark  and David Rolnick. Why does deep and cheap learning work so

well? Journal of Statistical Physics  168(6):1223–1247  2017.

[19] Guido F Montufar  Razvan Pascanu  Kyunghyun Cho  and Yoshua Bengio. On the number of

linear regions of deep neural networks. In NeurIPS  2014.

[20] Roman Novak  Yasaman Bahri  Daniel A Abolaﬁa  Jeffrey Pennington  and Jascha Sohl-
Dickstein. Sensitivity and generalization in neural networks: an empirical study. In ICLR 
2018.

[21] Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta numerica 

8:143–195  1999.

9

[22] Ben Poole  Subhaneil Lahiri  Maithra Raghu  Jascha Sohl-Dickstein  and Surya Ganguli.
Exponential expressivity in deep neural networks through transient chaos. In NeurIPS  2016.
[23] Maithra Raghu  Ben Poole  Jon Kleinberg  Surya Ganguli  and Jascha Sohl-Dickstein. On the

expressive power of deep neural networks. In ICML  pages 2847–2854  2017.

[24] David Rolnick and Max Tegmark. The power of deeper networks for expressing natural

functions. In ICLR  2018.

[25] Thiago Serra and Srikumar Ramalingam. Empirical bounds on linear regions of deep rectiﬁer

networks. Preprint arXiv:1810.03370  2018.

[26] Thiago Serra  Christian Tjandraatmadja  and Srikumar Ramalingam. Bounding and counting

linear regions of deep neural networks. In ICML  2018.

[27] Richard P Stanley et al. An introduction to hyperplane arrangements. Geometric combinatorics 

13:389–496  2004.
[28] Matus Telgarsky.

arXiv:1509.08101  2015.

Representation beneﬁts of deep feedforward networks.

Preprint

[29] Chiyuan Zhang  Samy Bengio  Moritz Hardt  Benjamin Recht  and Oriol Vinyals. Understanding

deep learning requires rethinking generalization. In ICLR  2017.

10

,Chong Wang
Xi Chen
Alexander Smola
Eric Xing
Boris Hanin
David Rolnick