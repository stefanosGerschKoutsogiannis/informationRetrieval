2015,Bayesian Manifold Learning: The Locally Linear Latent Variable Model (LL-LVM),We introduce the Locally Linear Latent Variable Model (LL-LVM)  a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations  their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus  the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships  select the intrinsic dimensionality of the manifold  construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.,Bayesian Manifold Learning:

The Locally Linear Latent Variable Model

Mijung Park  Wittawat Jitkrittum  Ahmad Qamar∗ 

Zolt´an Szab´o  Lars Buesing†  Maneesh Sahani

Gatsby Computational Neuroscience Unit

University College London

{mijung  wittawat  zoltan.szabo}@gatsby.ucl.ac.uk

atqamar@gmail.com  lbuesing@google.com  maneesh@gatsby.ucl.ac.uk

Abstract

We introduce the Locally Linear Latent Variable Model (LL-LVM)  a probabilistic
model for non-linear manifold discovery that describes a joint distribution over ob-
servations  their manifold coordinates and locally linear maps conditioned on a set
of neighbourhood relationships. The model allows straightforward variational op-
timisation of the posterior distribution on coordinates and locally linear maps from
the latent space to the observation space given the data. Thus  the LL-LVM en-
capsulates the local-geometry preserving intuitions that underlie non-probabilistic
methods such as locally linear embedding (LLE). Its probabilistic semantics make
it easy to evaluate the quality of hypothesised neighbourhood relationships  select
the intrinsic dimensionality of the manifold  construct out-of-sample extensions
and to combine the manifold model with additional probabilistic models that cap-
ture the structure of coordinates within the manifold.

1

Introduction

Many high-dimensional datasets comprise points derived from a smooth  lower-dimensional mani-
fold embedded within the high-dimensional space of measurements and possibly corrupted by noise.
For instance  biological or medical imaging data might reﬂect the interplay of a small number of la-
tent processes that all affect measurements non-linearly. Linear multivariate analyses such as princi-
pal component analysis (PCA) or multidimensional scaling (MDS) have long been used to estimate
such underlying processes  but cannot always reveal low-dimensional structure when the mapping is
non-linear (or  equivalently  the manifold is curved). Thus  there has been substantial recent interest
in algorithms to identify non-linear manifolds in data.
Many more-or-less heuristic methods for non-linear manifold discovery are based on the idea of
preserving the geometric properties of local neighbourhoods within the data  while embedding  un-
folding or otherwise transforming the data to occupy fewer dimensions. Thus  algorithms such as
locally-linear embedding (LLE) and Laplacian eigenmap attempt to preserve local linear relation-
ships or to minimise the distortion of local derivatives [1  2]. Others  like Isometric feature mapping
(Isomap) or maximum variance unfolding (MVU) preserve local distances  estimating global man-
ifold properties by continuation across neighbourhoods before embedding to lower dimensions by
classical methods such as PCA or MDS [3]. While generally hewing to this same intuitive path  the
range of available algorithms has grown very substantially in recent years [4  5].

∗Current afﬁliation: Thread Genius
†Current afﬁliation: Google DeepMind

1

However  these approaches do not deﬁne distributions over the data or over the manifold properties.
Thus  they provide no measures of uncertainty on manifold structure or on the low-dimensional
locations of the embedded points; they cannot be combined with a structured probabilistic model
within the manifold to deﬁne a full likelihood relative to the high-dimensional observations; and they
provide only heuristic methods to evaluate the manifold dimensionality. As others have pointed out 
they also make it difﬁcult to extend the manifold deﬁnition to out-of-sample points in a principled
way [6].
An established alternative is to construct an explicit probabilistic model of the functional relationship
between low-dimensional manifold coordinates and each measured dimension of the data  assuming
that the functions instantiate draws from Gaussian-process priors. The original Gaussian process
latent variable model (GP-LVM) required optimisation of the low-dimensional coordinates  and thus
still did not provide uncertainties on these locations or allow evaluation of the likelihood of a model
over them [7]; however a recent extension exploits an auxiliary variable approach to optimise a
more general variational bound  thus retaining approximate probabilistic semantics within the latent
space [8]. The stochastic process model for the mapping functions also makes it straightforward
to estimate the function at previously unobserved points  thus generalising out-of-sample with ease.
However  the GP-LVM gives up on the intuitive preservation of local neighbourhood properties that
underpin the non-probabilistic methods reviewed above. Instead  the expected smoothness or other
structure of the manifold must be deﬁned by the Gaussian process covariance function  chosen a
priori.
Here  we introduce a new probabilistic model over high-dimensional observations  low-dimensional
embedded locations and locally-linear mappings between high and low-dimensional linear maps
within each neighbourhood  such that each group of variables is Gaussian distributed given the
other two. This locally linear latent variable model (LL-LVM) thus respects the same intuitions
as the common non-probabilistic manifold discovery algorithms  while still deﬁning a full-ﬂedged
probabilistic model. Indeed  variational inference in this model follows more directly and with fewer
separate bounding operations than the sparse auxiliary-variable approach used with the GP-LVM.
Thus  uncertainty in the low-dimensional coordinates and in the manifold shape (deﬁned by the local
maps) is captured naturally. A lower bound on the marginal likelihood of the model makes it possible
to select between different latent dimensionalities and  perhaps most crucially  between different
deﬁnitions of neighbourhood  thus addressing an important unsolved issue with neighbourhood-
deﬁned algorithms. Unlike existing probabilistic frameworks with locally linear models such as
mixtures of factor analysers (MFA)-based and local tangent space analysis (LTSA)-based methods
[9  10  11]  LL-LVM does not require an additional step to obtain the globally consistent alignment
of low-dimensional local coordinates.1
This paper is organised as follows. In section 2  we introduce our generative model  LL-LVM  for
which we derive the variational inference method in section 3. We brieﬂy describe out-of-sample
extension for LL-LVM and mathematically describe the dissimilarity between LL-LVM and GP-
LVM at the end of section 3.
In section 4  we demonstrate the approach on several real world
problems.
Notation: In the following  a diagonal matrix with entries taken from the vector v is written diag(v).
The vector of n ones is 1n and the n × n identity matrix is In. The Euclidean norm of a vector is
(cid:107)v(cid:107)  the Frobenius norm of a matrix is (cid:107)M(cid:107)F . The Kronecker delta is denoted by δij (= 1 if i = j 
and 0 otherwise). The Kronecker product of matrices M and N is M ⊗ N. For a random vector w 
we denote the normalisation constant in its probability density function by Zw. The expectation of
a random vector w with respect to a density q is (cid:104)w(cid:105)q.

2 The model: LL-LVM
Suppose we have n data points {y1  . . .   yn} ⊂ Rdy  and a graph G on nodes {1 . . . n} with edge
set EG = {(i  j) | yi and yj are neighbours}. We assume that there is a low-dimensional (latent)
representation of the high-dimensional data  with coordinates {x1  . . .   xn} ⊂ Rdx  dx < dy. It will
be helpful to concatenate the vectors to form y = [y1

(cid:62)](cid:62) and x = [x1

(cid:62)  . . .   yn

(cid:62)  . . .   xn

(cid:62)](cid:62).

1This is also true of one previous MFA-based method [12] which ﬁnds model parameters and global coor-

dinates by variational methods similar to our own.

2

Figure 1: Locally linear mapping Ci
transforms the tan-
for ith data point
gent space  TxiMx at xi
in the low-
dimensional space to the tangent space 
TyiMy at the corresponding data point
in the high-dimensional space. A
yi
neighbouring data point is denoted by yj
and the corresponding latent variable by
xj.

Our key assumption is that the mapping between high-dimensional data and low-dimensional co-
ordinates is locally linear (Fig. 1). The tangent spaces are approximated by {yj − yi}(i j)∈EG and
{xj − xi}(i j)∈EG   the pairwise differences between the ith point and neighbouring points j. The
matrix Ci ∈ Rdy×dx at the ith point linearly maps those tangent spaces as

(1)
Under this assumption  we aim to ﬁnd the distribution over the linear maps C = [C1 ···   Cn] ∈
Rdy×ndx and the latent variables x that best describe the data likelihood given the graph G:

yj − yi ≈ Ci(xj − xi).

log p(y|G) = log

p(y  C  x|G) dx dC.

(2)

(cid:90)(cid:90)

n(cid:88)

n(cid:88)

The joint distribution can be written in terms of priors on C  x and the likelihood of y as

p(y  C  x|G) = p(y|C  x G)p(C|G)p(x|G).

(3)
In the following  we highlight the essential components the Locally Linear Latent Variable Model
(LL-LVM). Detailed derivations are given in the Appendix.
Adjacency matrix and Laplacian matrix The edge set of G for n data points speciﬁes a n × n
symmetric adjacency matrix G. We write ηij for the i  jth element of G  which is 1 if yj and
yi are neighbours and 0 if not (including on the diagonal). The graph Laplacian matrix is then
L = diag(G 1n) − G.

Prior on x We assume that the latent variables are zero-centered with a bounded expected scale 
and that latent variables corresponding to neighbouring high-dimensional points are close (in Eu-
clidean distance). Formally  the log prior on the coordinates is then

log p({x1 . . . xn}|G  α) = − 1

2

(α(cid:107)xi(cid:107)2 +

ηij(cid:107)xi − xj(cid:107)2) − log Zx 

i=1

j=1

where the parameter α controls the expected scale (α > 0). This prior can be written as multivariate
normal distribution on the concatenated x:

p(x|G  α) = N (0  Π)  where Ω−1 = 2L ⊗ Idx  Π−1 = αIndx + Ω−1.

log p({C1 . . . Cn}|G) = − 
2
= − 1
2

Prior on C We assume that the linear maps corresponding to neighbouring points are similar in
terms of Frobenius norm (thus favouring a smooth manifold of low curvature). This gives
F − log Zc

ηij(cid:107)Ci − Cj(cid:107)2

n(cid:88)

Ci

(cid:13)(cid:13)(cid:13)2

n(cid:88)

(cid:13)(cid:13)(cid:13) n(cid:88)
Tr(cid:2)(JJ(cid:62) + Ω−1)C(cid:62)C(cid:3) − log Zc 

− 1
2

i=1

i=1

j=1

F

(4)
where J := 1n ⊗ Idx. The second line corresponds to the matrix normal density  giving p(C|G) =
MN (C|0  Idy   (JJ(cid:62) + Ω−1)−1) as the prior on C. In our implementation  we ﬁx  to a small
value2  since the magnitude of the product Ci(xi − xj) is determined by optimising the hyper-
parameter α above.

2 sets the scale of the average linear map  ensuring the prior precision matrix is invertible.

3

high-dimensional spacelow-dimensional spaceyiyjTxiMxixjxCiTMyiyC

G

y

x

α

V

Figure 2: Graphical representation of generative process in LL-
LVM. Given a dataset  we construct a neighbourhood graph G. The
distribution over the latent variable x is controlled by the graph G
as well as the parameter α. The distribution over the linear map
C is also governed by the graph G. The latent variable x and the
linear map C together determine the data likelihood.

(cid:107) n(cid:88)

n(cid:88)

n(cid:88)

2

Likelihood Under the local-linearity assumption  we penalise the approximation error of Eq. (1) 
which yields the log likelihood

yi(cid:107)2− 1

log p(y|C  x  V  G) = − 
2

−1(∆yj i − Ci∆xj i )− log Zy 
(5)
where ∆yj i = yj − yi and ∆xj i = xj − xi.3 Thus  y is drawn from a multivariate normal
distribution given by

ηij(∆yj i − Ci∆xj i )

j=1

V

i=1

i=1

(cid:62)

with Σ−1

ei = −(cid:80)n

y = (1n1n

(cid:62)](cid:62) ∈ Rndy;
j=1 ηjiV−1(Cj + Ci)∆xj i . For computational simplicity  we assume V−1 = γIdy.
The graphical representation of the generative process underlying the LL-LVM is given in Fig. 2.

(cid:62)) ⊗ Idy + 2L ⊗ V−1  µy = Σye  and e = [e1

(cid:62) ···   en

p(y|C  x  V  G) = N (µy  Σy) 

3 Variational inference
Our goal is to infer the latent variables (x  C) as well as the parameters θ = {α  γ} in LL-LVM. We
infer them by maximising the lower bound L of the marginal likelihood of the observations
dxdC := L(q(C  x)  θ).

log p(y|G  θ) ≥

p(y  C  x|G  θ)

q(C  x) log

(cid:90)(cid:90)

(6)

q(C  x)

Following the common treatment for computational tractability  we assume the posterior over (C  x)
factorises as q(C  x) = q(x)q(C) [13]. We maximise the lower bound w.r.t. q(C  x) and θ by the
variational expectation maximization algorithm [14]  which consists of (1) the variational expecta-
tion step for computing q(C  x) by

q(x) ∝ exp

q(C) ∝ exp

q(C) log p(y  C  x|G  θ)dC

q(x) log p(y  C  x|G  θ)dx

 

(7)

(8)

(cid:21)
(cid:21)

 

(cid:20)(cid:90)
(cid:20)(cid:90)

then (2) the maximization step for estimating θ by ˆθ = arg maxθ L(q(C  x)  θ).
Variational-E step Computing q(x) from Eq. (7) requires rewriting the likelihood in Eq. (5) as a
quadratic function in x

2 (x(cid:62)Ax − 2x(cid:62)b)(cid:3)  
where the normaliser ˜Zx has all the terms that do not depend on x from Eq. (5). Let ˜L := (1n1(cid:62)
n +
i j=1 ∈ Rndx×ndx where the i  jth
2γL)−1. The matrix A is given by A := A(cid:62)
(cid:2)(cid:80)
k ηikV−1(Ck + Ci)(cid:3) . The
˜L(p  q)AE(p  i)(cid:62)AE(q  j) and each i  jth (dy × dx) block of
(cid:62)](cid:62) ∈ Rndx with the component dx-dimensional vectors
(cid:62)V−1(yj − yi)). The likelihood combined with

dx × dx block is Aij =(cid:80)n
given by bi =(cid:80)n

AE ∈ Rndy×ndx is given by AE(i  j) = −ηijV−1(Cj + Ci) + δij
vector b is deﬁned as b = [b1

p(y|C  x  θ  G) = 1
˜Zx

(cid:62)V−1(yi − yj) − Ci

exp(cid:2)− 1

EΣyAE = [Aij]n

(cid:62) ···   bn

(cid:80)n

p=1

q=1

j=1 ηij(Cj

the prior on x gives us the Gaussian posterior over x (i.e.  solving Eq. (7))

q(x) = N (x|µx  Σx)  where Σ−1

x = (cid:104)A(cid:105)q(C) + Π−1  µx = Σx(cid:104)b(cid:105)q(C).

(9)

3The  term centers the data and ensures the distribution can be normalised. It applies in a subspace orthog-

onal to that modelled by x and C and so its value does not affect the resulting manifold model.

4

Figure 3: A simulated example. A: 400 data points drawn from Swiss Roll. B: true latent points (x)
in 2D used for generating the data. C: Posterior mean of C and D: posterior mean of x after 50 EM
iterations given k = 9  which was chosen by maximising the lower bound across different k’s. E:
Average lower bounds as a function of k. Each point is an average across 10 random seeds.

p(y|C  x  G  θ) = 1
˜ZC

exp[− 1

2Tr(ΓC(cid:62)C − 2C(cid:62)V−1H)] 

Similarly  computing q(C) from Eq. (8) requires rewriting the likelihood in Eq. (5) as a quadratic
function in C

(10)
where the normaliser ˜ZC has all the terms that do not depend on C from Eq. (5)  and Γ := Q˜LQ(cid:62).
The matrix Q = [q1 q2 ··· qn] ∈ Rndx×n where the jth subvector of the ith column is qi(j) =
ηijV−1(xi − xj) + δij

(cid:2)(cid:80)
k ηikV−1(xi − xk)(cid:3) ∈ Rdx. We deﬁne H = [H1 ···   Hn] ∈ Rdy×ndx
whose ith block is Hi =(cid:80)n

j=1 ηij(yj − yi)(xj − xi)(cid:62).

The likelihood combined with the prior on C gives us the Gaussian posterior over C (i.e.  solving
Eq. (8))

q(C) = MN (µC  I  ΣC)  where Σ

C := (cid:104)Γ(cid:105)q(x) + JJ
−1

(cid:62)

+ Ω

−1 and µC = V

−1(cid:104)H(cid:105)q(x)Σ

(cid:62)
C.

(11)

The expected values of A  b  Γ and H are given in the Appendix.
Variational-M step We set the parameters by maximising L(q(C  x)  θ) w.r.t. θ which is split
into two terms based on dependence on each parameter: (1) expected log-likelihood for updating
V by arg maxV Eq(x)q(C)[log p(y|C  x  V  G)]; and (2) negative KL divergence between the prior
and the posterior on x for updating α by arg maxα Eq(x)q(C)[log p(x|G  α)− log q(x)]. The update
rules for each hyperparameter are given in the Appendix.
The full EM algorithm4 starts with an initial value of θ. In the E-step  given q(C)  compute q(x)
as in Eq. (9). Likewise  given q(x)  compute q(C) as in Eq. (11). The parameters θ are updated
in the M-step by maximising Eq. (6). The two steps are repeated until the variational lower bound
in Eq. (6) saturates. To give a sense of how the algorithm works  we visualise ﬁtting results for
a simulated example in Fig. 3. Using the graph constructed from 3D observations given different
k  we run our EM algorithm. The posterior means of x and C given the optimal k chosen by the
maximum lower bound resemble the true manifolds in 2D and 3D spaces  respectively.

Out-of-sample extension In the LL-LVM model one can formulate a computationally efﬁcient
out-of-sample extension technique as follows. Given n data points denoted by D = {y1 ···   yn} 
the variational EM algorithm derived in the previous section converts D into the posterior q(x  C):
D (cid:55)→ q(x)q(C). Now  given a new high-dimensional data point y∗  one can ﬁrst ﬁnd
the neighbourhood of y∗ without changing the current neighbourhood graph. Then  it is pos-
sible to compute the distributions over the corresponding locally linear map and latent variable
q(C∗  x∗) via simply performing the E-step given q(x)q(C) (freezing all other quantities the same)
as D ∪ {y∗} (cid:55)→ q(x)q(C)q(x∗)q(C∗).

4An implementation is available from http://www.gatsby.ucl.ac.uk/resources/lllvm.

5

post mean of x678910119001000average lwbsktrue xAECBDposterior mean of C400 datapoints Figure 4: Resolving short-circuiting problems using variational lower bound. A: Visualization of
400 samples drawn from a Swiss Roll in 3D space. Points 28 (red) and 29 (blue) are close to each
other (dotted grey) in 3D. B: Visualization of the 400 samples on the latent 2D manifold. The
distance between points 28 and 29 is seen to be large. C: Posterior mean of x with/without short-
circuiting the 28th and the 29th data points in the graph construction. LLLVM achieves a higher
lower bound when the shortcut is absent. The red and blue parts are mixed in the resulting estimate
in 2D space (right) when there is a shortcut. The lower bound is obtained after 50 EM iterations.

X = [x1  . . .   xdx ] ∈ Rn×dx is deﬁned by p(Y|X) = (cid:81)dy

Comparison to GP-LVM A closely related probabilistic dimensionality reduction algorithm to
LL-LVM is GP-LVM [7]. GP-LVM deﬁnes the mapping from the latent space to data space us-
ing Gaussian processes. The likelihood of the observations Y = [y1  . . .   ydy ] ∈ Rn×dy (yk
is the vector formed by the kth element of all n high dimensional vectors) given latent variables
k=1 N (yk|0  Knn + β−1In)  where
the i  jth element of the covariance matrix is of the exponentiated quadratic form: k(xi  xj) =
with smoothness-scale parameters {αq} [8]. In LL-LVM  once
σ2
f exp
we integrate out C from Eq. (5)  we also obtain the Gaussian likelihood given x 

q=1 αq(xi q − xj q)2(cid:105)
(cid:80)dx

(cid:104)− 1

2

(cid:90)

exp(cid:2)− 1

LL y(cid:3) .

2 y(cid:62) K−1

p(y|x  G  θ) =

p(y|C  x  G  θ)p(C|G  θ) dC = 1

ZYy

LL = (2L ⊗ V−1) − (W ⊗ V−1) Λ (W(cid:62) ⊗
In contrast to GP-LVM  the precision matrix K−1
V−1) depends on the graph Laplacian matrix through W and Λ. Therefore  in LL-LVM  the graph
structure directly determines the functional form of the conditional precision.

4 Experiments

4.1 Mitigating the short-circuit problem

Like other neighbour-based methods  LL-LVM is sensitive to misspeciﬁed neighbourhoods; the
prior  likelihood  and posterior all depend on the assumed graph. Unlike other methods  LL-
LVM provides a natural way to evaluate possible short-circuits using the variational lower bound
of Eq. (6). Fig. 4 shows 400 samples drawn from a Swiss Roll in 3D space (Fig. 4A). Two points 
labelled 28 and 29  happen to fall close to each other in 3D  but are actually far apart on the la-
tent (2D) surface (Fig. 4B). A k-nearest-neighbour graph might link these  distorting the recovered
coordinates. However  evaluating the model without this edge (the correct graph) yields a higher
variational bound (Fig. 4C). Although it is prohibitive to evaluate every possible graph in this way 
the availability of a principled criterion to test speciﬁc hypotheses is of obvious value.
In the following  we demonstrate LL-LVM on two real datasets: handwritten digits and climate data.

4.2 Modelling USPS handwritten digits

As a ﬁrst real-data example  we test our method on a subset of 80 samples each of the digits
0  1  2  3  4 from the USPS digit dataset  where each digit is of size 16×16 (i.e.  n = 400  dy = 256).
We follow [7]  and represent the low-dimensional latent variables in 2D.

6

400 samples (in 3D)2D representationposterior mean of x in 2D space ABC29282829G without shortcutG with shortcutLB: 1119.4LB: 1151.5Figure 5: USPS handwritten digit dataset described in section 4.2. A: Mean (in solid) and variance
(1 standard n deviation shading) of the variational lower bound across 10 different random starts of
EM algorithm with different k’s. The highest lower bound is achieved when k = n/80. B: The
posterior mean of x in 2D. Each digit is colour coded. On the right side are reconstructions of y∗ for
randomly chosen query points x∗. Using neighbouring y and posterior means of C we can recover
y∗ successfully (see text). C: Fitting results by GP-LVM using the same data. D: ISOMAP (k = 30)
and E: LLE (k=40). Using the extracted features (in 2D)  we evaluated a 1-NN classiﬁer for digit
identity with 10-fold cross-validation (the same data divided into 10 training and test sets). The
classiﬁcation error is shown in F. LL-LVM features yield the comparably low error with GP-LVM
and ISOMAP.

Fig. 5A shows variational lower bounds for different values of k  using 9 different EM initialisations.
The posterior mean of x obtained from LL-LVM using the best k is illustrated in Fig. 5B. Fig. 5B
also shows reconstructions of one randomly-selected example of each digit  using its 2D coordinates
x∗ as well as the posterior mean coordinates ˆxi  tangent spaces ˆCi and actual images yi of its
k = n/80 closest neighbours. The reconstruction is based on the assumed tangent-space structure
of the generative model (Eq. (5))  that is: ˆy∗ = 1
. A similar process
could be used to reconstruct digits at out-of-sample locations. Finally  we quantify the relevance
of the recovered subspace by computing the error incurred using a simple classiﬁer to report digit
identity using the 2D features obtained by LL-LVM and various competing methods (Fig. 5C-F).
Classiﬁcation with LL-LVM coordinates performs similarly to GP-LVM and ISOMAP (k = 30) 
and outperforms LLE (k = 40).

yi + ˆCi(x∗ − ˆxi)

(cid:80)k

(cid:105)

(cid:104)

k

i=1

4.3 Mapping climate data

In this experiment  we attempted to recover 2D geographical relationships between weather stations
from recorded monthly precipitation patterns. Data were obtained by averaging month-by-month
annual precipitation records from 2005–2014 at 400 weather stations scattered across the US (see
Fig. 6) 5. Thus  the data set comprised 400 12-dimensional vectors. The goal of the experiment is to
recover the two-dimensional topology of the weather stations (as given by their latitude and longi-

5The dataset is made available by the National Climatic Data Center at http://www.ncdc.noaa.

gov/oa/climate/research/ushcn/. We use version 2.5 monthly data [15].

7

30345x 104variational lower boundA# EM iterations0true Y*estimatek=n/80posterior mean of x (k=n/80)Bdigit 1digit 2digit 3digit 0digit 4query (0)query (1)GP-LVMCISOMAPDLLEEClassi(cid:31)cation errorFLLEISOMAPGPLVMLLLVMk=n/100k=n/50k=n/40query (2)query (3)00.20.4query (4)(a) 400 weather stations

(b) LLE

(c) LTSA

(d) ISOMAP

(e) GP-LVM

(f) LL-LVM

Figure 6: Climate modelling problem as described in section 4.3. Each example corresponding to
a weather station is a 12-dimensional vector of monthly precipitation measurements. Using only
the measurements  the projection obtained from the proposed LL-LVM recovers the topological
arrangement of the stations to a large degree.

tude) using only these 12-dimensional climatic measurements. As before  we compare the projected
points obtained by LL-LVM with several widely used dimensionality reduction techniques. For the
graph-based methods LL-LVM  LTSA  ISOMAP  and LLE  we used 12-NN with Euclidean distance
to construct the neighbourhood graph.
The results are presented in Fig. 6. LL-LVM identiﬁed a more geographically-accurate arrangement
for the weather stations than the other algorithms. The fully probabilistic nature of LL-LVM and
GPLVM allowed these algorithms to handle the noise present in the measurements in a principled
way. This contrasts with ISOMAP which can be topologically unstable [16] i.e. vulnerable to short-
circuit errors if the neighbourhood is too large. Perhaps coincidentally  LL-LVM also seems to
respect local geography more fully in places than does GP-LVM.

5 Conclusion

We have demonstrated a new probabilistic approach to non-linear manifold discovery that embod-
ies the central notion that local geometries are mapped linearly between manifold coordinates and
high-dimensional observations. The approach offers a natural variational algorithm for learning 
quantiﬁes local uncertainty in the manifold  and permits evaluation of hypothetical neighbourhood
relationships.
In the present study  we have described the LL-LVM model conditioned on a neighbourhood graph.
In principle  it is also possible to extend LL-LVM so as to construct a distance matrix as in [17]  by
maximising the data likelihood. We leave this as a direction for future work.

Acknowledgments

The authors were funded by the Gatsby Charitable Foundation.

8

−120−110−100−90−80−7030354045LongitudeLatitudeReferences
[1] S. T. Roweis and L. K. Saul. Nonlinear Dimensionality Reduction by Locally Linear Embed-

ding. Science  290(5500):2323–2326  2000.

[2] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and

clustering. In NIPS  pages 585–591  2002.

[3] J. B. Tenenbaum  V. Silva  and J. C. Langford. A Global Geometric Framework for Nonlinear

Dimensionality Reduction. Science  290(5500):2319–2323  2000.

[4] L.J.P. van der Maaten  E. O. Postma  and H. J. van den Herik. Dimensionality re-
http://www.iai.uni-bonn.de/˜jz/

duction: A comparative review  2008.
dimensionality_reduction_a_comparative_review.pdf.

[5] L. Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep 

pages 1–17  2005. http://www.lcayton.com/resexam.pdf.

[6] J. Platt. Fastmap  metricmap  and landmark MDS are all Nystr¨om algorithms. In Proceedings
of 10th International Workshop on Artiﬁcial Intelligence and Statistics  pages 261–268  2005.
[7] N. Lawrence. Gaussian process latent variable models for visualisation of high dimensional

data. In NIPS  pages 329–336  2003.

[8] M. K. Titsias and N. D. Lawrence. Bayesian Gaussian process latent variable model.

AISTATS  pages 844–851  2010.

In

[9] S. Roweis  L. Saul  and G. Hinton. Global coordination of local linear models. In NIPS  pages

889–896  2002.

[10] M. Brand. Charting a manifold. In NIPS  pages 961–968  2003.
[11] Y. Zhan and J. Yin. Robust local tangent space alignment. In NIPS  pages 293–301. 2009.
[12] J. Verbeek. Learning nonlinear image manifolds by global alignment of local linear models.

IEEE Transactions on Pattern Analysis and Machine Intelligence  28(8):1236–1250  2006.

[13] C. Bishop. Pattern recognition and machine learning. Springer New York  2006.
[14] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis  Gatsby

Unit  University College London  2003.

[15] M. Menne  C. Williams  and R. Vose. The U.S. historical climatology network monthly tem-
perature data  version 2.5. Bulletin of the American Meteorological Society  90(7):993–1007 
July 2009.

[16] Mukund Balasubramanian and Eric L. Schwartz. The isomap algorithm and topological sta-

bility. Science  295(5552):7–7  January 2002.

[17] N. Lawrence. Spectral dimensionality reduction via maximum entropy. In AISTATS  pages

51–59  2011.

9

,Mijung Park
Wittawat Jitkrittum
Zoltan Szabo