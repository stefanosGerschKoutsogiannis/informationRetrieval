2014,Kernel Mean Estimation via Spectral Filtering,The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g.  when centering a kernel PCA matrix)  and it also forms the core inference step of modern kernel methods (e.g.  kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing “better” estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1]  and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis  we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.,Kernel Mean Estimation via Spectral Filtering

Krikamol Muandet
MPI-IS  T¨ubingen

krikamol@tue.mpg.de

Bharath Sriperumbudur
Dept. of Statistics  PSU

bks18@psu.edu

Bernhard Sch¨olkopf

MPI-IS  T¨ubingen
bs@tue.mpg.de

Abstract

The problem of estimating the kernel mean in a reproducing kernel Hilbert space
(RKHS) is central to kernel methods in that it is used by classical approaches (e.g. 
when centering a kernel PCA matrix)  and it also forms the core inference step of
modern kernel methods (e.g.  kernel-based non-parametric tests) that rely on em-
bedding probability distributions in RKHSs. Previous work [1] has shown that
shrinkage can help in constructing “better” estimators of the kernel mean than the
empirical estimator. The present paper studies the consistency and admissibility
of the estimators in [1]  and proposes a wider class of shrinkage estimators that
improve upon the empirical estimator by considering appropriate basis functions.
Using the kernel PCA basis  we show that some of these estimators can be con-
structed using spectral ﬁltering algorithms which are shown to be consistent under
some technical assumptions. Our theoretical analysis also reveals a fundamental
connection to the kernel-based supervised learning framework. The proposed es-
timators are simple to implement and perform well in practice.

1

Introduction

The kernel mean or the mean element  which corresponds to the mean of the kernel function in a
reproducing kernel Hilbert space (RKHS) computed w.r.t. some distribution P  has played a fun-
damental role as a basic building block of many kernel-based learning algorithms [2–4]  and has
recently gained increasing attention through the notion of embedding distributions in an RKHS [5–
13]. Estimating the kernel mean remains an important problem as the underlying distribution P is
usually unknown and we must rely entirely on the sample drawn according to P.

from P  the most common
Given a random sample drawn independently and identically (i.i.d.)
way to estimate the kernel mean is by replacing P by the empirical measure  Pn := 1
i=1 δXi
where δx is a Dirac measure at x [5  6]. Without any prior knowledge about P  the empirical
estimator is possibly the best one can do. However  [1] showed that this estimator can be “improved”
by constructing a shrinkage estimator which is a combination of a model with low bias and high
variance  and a model with high bias but low variance. Interestingly  signiﬁcant improvement is
in fact possible if the trade-off between these two models is chosen appropriately. The shrinkage
estimator proposed in [1]  which is motivated from the classical James-Stein shrinkage estimator
[14] for the estimation of the mean of a normal distribution  is shown to have a smaller mean-squared
error than that of the empirical estimator. These ﬁndings provide some support for the conceptual
premise that we might be somewhat pessimistic in using the empirical estimator of the kernel mean
and there is abundant room for further progress.

nPn

In this work  we adopt a spectral ﬁltering approach to obtain shrinkage estimators of kernel mean
that improve on the empirical estimator. The motivation behind our approach stems from the idea
presented in [1] where the kernel mean estimation is reformulated as an empirical risk minimization
(ERM) problem  with the shrinkage estimator being then obtained through penalized ERM. It is
important to note that this motivation differs fundamentally from the typical supervised learning as
the goal of regularization here is to get the James-Stein-like shrinkage estimators [14] rather than

1

to prevent overﬁtting. By looking at regularization from a ﬁlter function perspective  in this paper 
we show that a wide class of shrinkage estimators for kernel mean can be obtained and that these
estimators are consistent for an appropriate choice of the regularization/shrinkage parameter.

Unlike in earlier works [15–18] where the spectral ﬁltering approach has been used in supervised
learning problems  we here deal with unsupervised setting and only leverage spectral ﬁltering as a
way to construct a shrinkage estimator of the kernel mean. One of the advantages of this approach
is that it allows us to incorporate meaningful prior knowledge. The resultant estimators are char-
acterized by the ﬁlter function  which can be chosen according to the relevant prior knowledge.
Moreover  the spectral ﬁltering gives rise to a broader interpretation of shrinkage through  for exam-
ple  the notion of early stopping and dimension reduction. Our estimators not only outperform the
empirical estimator  but are also simple to implement and computationally efﬁcient.

The paper is organized as follows. In Section 2  we introduce the problem of shrinkage estimation
and present a new result that theoretically justiﬁes the shrinkage estimator over the empirical esti-
mator for kernel mean  which improves on the work of [1] while removing some of its drawbacks.
Motivated by this result  we consider a general class of shrinkage estimators obtained via spectral
ﬁltering in Section 3 whose theoretical properties are presented in Section 4. The empirical perfor-
mance of the proposed estimators are presented in Section 5. The missing proofs of the results are
given in the supplementary material.

2 Kernel mean shrinkage estimator

In this section  we present preliminaries on the problem of shrinkage estimation in the context of esti-
mating the kernel mean [1] and then present a theoretical justiﬁcation (see Theorem 1) for shrinkage
estimators that improves our understanding of the kernel mean estimation problem  while alleviating
some of the issues inherent in the estimator proposed in [1].
Preliminaries: Let H be an RKHS of functions on a separable topological space X . The space H
is endowed with inner product h· ·i  associated norm k · k  and reproducing kernel k : X × X → R 
which we assume to be continuous and bounded  i.e.  κ := supx∈X pk(x  x) < ∞. The kernel
mean of some unknown distribution P on X and its empirical estimate—we refer to this as kernel
mean estimator (KME)—from i.i.d. sample x1  . . .   xn are given by

µP :=ZX

k(x ·) dP(x)

and

ˆµP :=

1
n

nXi=1

k(xi ·) 

(1)

respectively. As mentioned before  ˆµP is the “best” possible estimator to estimate µP if nothing is
known about P. However  depending on the information that is available about P  one can construct
various estimators of µP that perform “better” than µP. Usually  the performance measure that is
used for comparison is the mean-squared error though alternate measures can be used. Therefore 
our main objective is to improve upon KME in terms of the mean-squared error  i.e.  construct ˜µP
such that EPk˜µP− µPk2 ≤ EPkˆµP− µPk2 for all P ∈ P with strict inequality holding for at least one
element in P where P is a suitably large class of Borel probability measures on X . Such an estimator
˜µP is said to be admissible w.r.t P. If P = M 1
+(X ) is the set of all Borel probability measures on
X   then ˜µP satisfying the above conditions may not exist and in that sense  ˆµP is possibly the best
estimator of µP that one can have.
Admissibility of shrinkage estimator: To improve upon KME  motivated by the James-Stein esti-
mator  ˜θ  [1] proposed a shrinkage estimator ˆµα := αf ∗ + (1 − α)ˆµP where α ∈ R is the shrinkage
parameter that balances the low-bias  high-variance model (ˆµP) with the high-bias  low-variance
model (f ∗ ∈ H). Assuming for simplicity f ∗ = 0  [1] showed that EPkˆµα − µPk2 < EPkˆµP − µPk2
if and only if α ∈ (0  2∆/(∆ + kµPk2)) where ∆ := EPkˆµP − µPk2. While this is an interesting
result  the resultant estimator ˆµα is strictly not a “statistical estimator” as it depends on quantities
that need to be estimated  i.e.  it depends on α whose choice requires the knowledge of µP  which
is the quantity to be estimated. We would like to mention that [1] handles the general case with f ∗
being not necessarily zero  wherein the range for α then depends on f ∗ as well. But for the purposes
of simplicity and ease of understanding  for the rest of this paper we assume f ∗ = 0. Since ˆµα is
not practically interesting  [1] resorted to the following representation of µP and ˆµP as solutions to
the minimization problems [1  19]:

2

P

21/β β

λ+1

λ+1

µP = arg inf

λ+1

λ+1

ˆµP = arg inf
g∈H

1
n

from these viewpoints.

kk(xi ·) − gk2 

g∈HZX kk(x ·) − gk2 dP(x) 
nXi=1

ˇµλ = arg inf
g∈H
λ+1   i.e.  ˇµλ = ˆµ λ

1
n

λ+1

(2)
using which ˆµα is shown to be the solution to the regularized empirical risk minimization problem:
(3)

nXi=1
kk(xi ·) − gk2 + λkgk2 
where λ > 0 and α := λ
. It is interesting to note that unlike in supervised
learning (e.g.  least squares regression)  the empirical minimization problem in (2) is not ill-posed
and therefore does not require a regularization term although it is used in (3) to obtain a shrinkage
estimator of µP. [1] then obtained a value for λ through cross-validation and used it to construct
as an estimator of µP  which is then shown to perform empirically better than ˆµP. However 
ˆµ λ
no theoretical guarantees including the basic requirement of ˆµ λ
being consistent are provided. In
fact  because λ is data-dependent  the above mentioned result about the improved performance of
ˆµα over a range of α does not hold as such a result is proved assuming α is a constant and does not
depend on the data. While it is clear that the regularizer in (3) is not needed to make (2) well-posed 
the role of λ is not clear from the point of view of ˆµ λ
being consistent and better than ˆµP. The
following result provides a theoretical understanding of ˆµ λ
Theorem 1. Let ˇµλ be constructed as in (3). Then the following hold.
(i) kˇµλ − µPk
kˇµλ − µPk = OP(n− min{β 1/2}).
+(X ) : kµPk2 <
(ii) For λ = cn−β with c > 0 and β > 1  deﬁne Pc β := {P ∈ M 1
AR k(x  x) dP(x)} where A :=
21/β β+c1/β (β−1)(β−1)/β . Then ∀ n and ∀ P ∈ Pc β  we have
EPkˇµλ − µPk2 < EPkˆµP − µPk2.
Remark. (i) Theorem 1(i) shows that ˇµλ is a consistent estimator of µP as long as λ → 0 and the
convergence rate in probability of kˇµλ − µPk is determined by the rate of convergence of λ to zero 
with the best possible convergence rate being n−1/2. Therefore to attain a fast rate of convergence 
it is instructive to choose λ such that λ√n → 0 as λ → 0 and n → ∞.

→ 0 as λ → 0 and n → ∞.

In addition  if λ = n−β for some β > 0  then

(ii) Suppose for some c > 0 and β > 1  we choose λ = cn−β  which means the resultant estimator
ˇµλ is a proper estimator as it does not depend on any unknown quantities. Theorem 1(ii) shows
that for any n and P ∈ Pc β  ˇµλ is a “better” estimator than ˆµP. Note that for any P ∈ M 1
+(X ) 
kµPk2 = R R k(x  y) dP(x) dP(y) ≤ (R pk(x  x) dP(x))2 ≤ R k(x  x) dP(x). This means ˇµλ
+(X ) to Pc β which considers only those distributions for which
is admissible if we restrict M 1
kµPk2/R k(x  x) dP(x) is strictly less than a constant  A < 1. It is obvious to note that if c is
very small or β is very large  then A gets closer to one and ˇµλ behaves almost like ˆµP  thereby
matching with our intuition.
(iii) A nice interpretation for Pc β can be obtained as in Theorem 1(ii) when k is a translation in-
variant kernel on Rd. It can be shown that Pc β contains the class of all probability measures whose
characteristic function has an L2 norm (and therefore is the set of square integrable probability den-
sities if P has a density w.r.t. the Lebesgue measure) bounded by a constant that depends on c  β and
k (see §2 in the supplementary material).
(cid:4)
3 Spectral kernel mean shrinkage estimator

Let us return to the shrinkage estimator ˆµα considered in [1]  i.e.  ˆµα = αf ∗ + (1 − α)ˆµP =
αPihf ∗  eiiei + (1 − α)PihˆµP  eiiei  where (ei)i∈N are the countable orthonormal basis (ONB)
of H—countable ONB exist since H is separable which follows from X being separable and k
being continuous [20  Lemma 4.33]. This estimator can be generalized by considering the shrinkage
estimator ˆµα := Pi αihf ∗  eiiei + Pi(1 − αi)hˆµP  eiiei where α := (α1  α2  . . .) ∈ R∞ is
a sequence of shrinkage parameters. If ∆α := EPkˆµα − µPk2 is the risk of this estimator  the
following theorem gives an optimality condition on α for which ∆α < ∆.
Theorem 2. For some ONB (ei)i  ∆α − ∆ =Pi(∆α i − ∆i) where ∆α i and ∆i denote the risk
of the ith component of ˆµα and ˆµP  respectively. Then  ∆α i − ∆i < 0 if

(4)

0 < αi <

2∆i
∆i + (f ∗

i − µi)2  

3

uncorrelated isotropic Gaussian

correlated anisotropic Gaussian

ˆθM L = X

.

θ

ˆθM L = X

.

θ

target

target

X ∼ N (θ  I)
Figure 1: Geometric explanation of a shrinkage estimator when estimating a mean of a Gaussian
distribution. For isotropic Gaussian  the level sets of the joint density of ˆθM L = X are hyperspheres.
In this case  shrinkage has the same effect regardless of the direction. Shaded area represents those
estimates that get closer to θ after shrinkage. For anisotropic Gaussian  the level sets are concentric
ellipsoids  which makes the effect dependent on the direction of shrinkage.

X ∼ N (θ  Σ)

where f ∗

i and µi denote the Fourier coefﬁcients of f ∗ and µP  respectively.

The condition in (4) is a component-wise version of the condition given in [1  Theorem 1] for a class
of estimators ˆµα := αf ∗ + (1 − α)ˆµP which may be expressed here by assuming that we have a
constant shrinkage parameter αi = α for all i. Clearly  as the optimal range of αi may vary across
coordinates  the class of estimators in [1] does not allow us to adjust αi accordingly. To understand
why this property is important  let us consider the problem of estimating the mean of Gaussian
distribution illustrated in Figure 1. For correlated random variable X ∼ N (θ  Σ)  a natural choice
of basis is the set of orthonormal eigenvectors which diagonalize the covariance matrix Σ of X.
Clearly  the optimal range of αi depends on the corresponding eigenvalues. Allowing for different
basis (ei)i and shrinkage parameter αi opens up a wide range of strategies that can be used to
construct “better” estimators.

A natural strategy under this representation is as follows:
i) we specify the ONB (ei)i and project
ˆµP onto this basis. ii) we shrink each ˆµi independently according to a pre-deﬁned shrinkage rule.
iii) the shrinkage estimate is reconstructed as a superposition of the resulting components. In other
words  an ideal shrinkage estimator can be deﬁned formally as a non-linear mapping:

ˆµP −→Xi

h(αi)hf ∗  eiiei +Xi

(1 − h(αi))hˆµP  eiiei

(5)

nPn

where h : R → R is a shrinkage rule. Since we make no reference to any particular basis (ei)i  nor to
any particular shrinkage rule h  a wide range of strategies can be adopted here. For example  we can
i=1 xi and 1− h(αi) = 1/√αi
view whitening as a special case in which f ∗ is the data average 1
where αi and ei are the ith eigenvalue and eigenvector of the covariance matrix  respectively.
Inspired by Theorem 2  we adopt the spectral ﬁltering approach as one of the strategies to construct
the estimators of the form (5). To this end  owing to the regularization interpretation in (3)  we
consider estimators of the formPn
i=1 βik(xi ·) for some β ∈ Rn—looking for such an estimator
is equivalent to learning a signed measure that is supported on (xi)n
i=1 βik(xi ·)
is a minimizer of (3)  β should satisfy Kβ = K1n where K is an n × n Gram matrix and 1n =
[1/n. . . .   1/n]⊤. Here the solution is trivially β = 1n  i.e.  the coefﬁcients of the standard estimator
ˆµP if K is invertible. Since K−1 may not exist and even if it exists  the computation of it can be
numerically unstable  the idea of spectral ﬁltering—this is quite popular in the theory of inverse
problems [15] and has been used in kernel least squares [17]—is to replace K−1 by some regularized
matrices gλ(K) that approximates K−1 as λ goes to zero. Note that unlike in (3)  the regularization
i=1 βik(xi ·)) without which the

is quite important here (i.e.  the case of estimators of the formPn

the linear system is under determined. Therefore  we propose the following class of estimators:

i=1. Since Pn

ˆµλ :=

nXi=1

βik(xi ·) with β(λ) := gλ(K)K1n 

(6)

where gλ(·) is a ﬁlter function and λ is referred to as a shrinkage parameter. The matrix-valued
function gλ(K) can be described by a scalar function gλ : [0  κ2] → R on the spectrum of K.
That is  if K = UDU⊤ is the eigen-decomposition of K where D = diag(˜γ1  . . .   ˜γn)  we have
gλ(D) = diag(gλ(˜γ1)  . . .   gλ(˜γn)) and gλ(K) = Ugλ(D)U⊤. For example  the scalar ﬁlter
function of Tikhonov regularization is gλ(γ) = 1/(γ + λ).
In the sequel  we call this class of
estimators a spectral kernel mean shrinkage estimator (Spectral-KMSE).

4

Table 1: Update equations for β and corresponding ﬁlter functions.

Algorithm

L2 Boosting
Acc. L2 Boosting βt ← βt−1 + ωt(βt−1 − βt−2) + κt
Iterated Tikhonov
Truncated SVD

Update Equation (a := K1n − Kβt−1)
βt ← βt−1 + ηa
(K + nλI)βi = 1n + nλβi−1
None

a

n

i=1(1 − ηγ)i

Filter Function

g(γ) = ηPt−1

g(γ) = pt(γ)
g(γ) = (γ+λ)t−γt
λ(γ+λ)t
g(γ) = γ−11{γ≥λ}

Tikhonov
L2 Boosting

ν−method
Iterated Tikhonov

TSVD

 

1.5

1

0.5

γ

)

γ

(
g

 

0
0

0.2

0.4

0.6

0.8

1

γ

Figure 2: Plot of g(γ)γ.

nPn

i=1 k(·  xi) ⊗ k(·  xi).

Proposition 3. The Spectral-KMSE satisﬁes ˆµλ = Pn
i=1 gλ(˜γi)˜γihˆµ  ˜vii˜vi  where (˜γi  ˜vi) are
eigenvalue and eigenfunction pairs of the empirical covariance operator bCk : H → H deﬁned as
bCk = 1
By virtue of Proposition 3  if we choose 1 − h(˜γ) := gλ(˜γ)˜γ  the Spectral-KMSE is indeed in
the form of (5) when f ∗ = 0 and (ei)i is the kernel PCA (KPCA) basis  with the ﬁlter function
gλ determining the shrinkage rule. Since by deﬁnition gλ(˜γi) approaches the function 1/˜γi as λ
goes to 0  the function gλ(˜γi)˜γi approaches 1 (no shrinkage). As the value of λ increases  we have
more shrinkage because the value of gλ(˜γi)˜γi deviates from 1  and the behavior of this deviation
depends on the ﬁlter function gλ. For example  we can see that Proposition 3 generalizes Theorem
2 in [1] where the ﬁlter function is gλ(K) = (K + nλI)−1  i.e.  g(γ) = 1/(γ + λ). That is  we
have gλ(˜γi)˜γi = ˜γi/(˜γi + λ)  implying that the effect of shrinkage is relatively larger in the low-
variance direction. In the following  we discuss well-known examples of spectral ﬁltering algorithms
obtained by various choices of gλ. Update equations for β(λ) and corresponding ﬁlter functions are
summarized in Table 1. Figure 2 illustrates the behavior of these ﬁlter functions.

L2 Boosting. This algorithm  also known as gradient descent or Landweber iteration  ﬁnds a
weight β by performing a gradient descent iteratively. Thus  we can interpret early stopping as
shrinkage and the reciprocal of iteration number as shrinkage parameter  i.e.  λ ≈ 1/t. The step-size
η does not play any role for shrinkage [16]  so we use the ﬁxed step-size η = 1/κ2 throughout.

Accelerated L2 Boosting. This algorithm  also known as ν-method  uses an accelerated gradient
descent step  which is faster than L2 Boosting because we only need √t iterations to get the same
solution as the L2 Boosting would get after t iterations. Consequently  we have λ ≈ 1/t2.
Iterated Tikhonov. This algorithm can be viewed as a combination of Tikhonov regularization
and gradient descent. Both parameters λ and t play the role of shrinkage parameter.

Truncated Singular Value Decomposition. This algorithm can be interpreted as a projection onto
the ﬁrst principal components of the KPCA basis. Hence  we may interpret dimensionality reduction
as shrinkage and the size of reduced dimension as shrinkage parameter. This approach has been used
in [21] to improve the kernel mean estimation under the low-rank assumption.

Most of the above spectral ﬁltering algorithms allow to compute the coefﬁcients β without explicitly
computing the eigen-decomposition of K  as we can see in Table 1  and some of which may have
no natural interpretation in terms of regularized risk minimization. Lastly  an initialization of β
corresponds to the target of shrinkage. In this work  we assume that β0 = 0 throughout.

4 Theoretical properties of Spectral-KMSE

This section presents some theoretical properties for the proposed Spectral-KMSE in (6). To this
end  we ﬁrst present a regularization interpretation that is different from the one in (3) which involves
learning a smooth operator from H to H [22]. This will be helpful to investigate the consistency of
the Spectral-KMSE. Let us consider the following regularized risk minimization problem 

arg minF∈H⊗H

EX kk(X ·) − F[k(X ·)]k2

H + λkFk2

HS

(7)

where F is a Hilbert-Schmidt operator from H to H. Essentially  we are seeking a smooth operator
F that maps k(x ·) to itself  where (7) is an instance of the regression framework in [22]. The
formulation of shrinkage as the solution of a smooth operator regression  and the empirical solution
(8) and in the lines below  were given in a personal communication by Arthur Gretton. It can be

5

γi

i=1

shown that the solution to (7) is given by F = Ck(Ck + λI)−1 where Ck : H → H is a covariance
operator in H deﬁned as Ck = R k(·  x) ⊗ k(·  x) dP(x) (see §5 of the supplement for a proof).
Deﬁne µλ := FµP = Ck(Ck + λI)−1µP. Since k is bounded  it is easy to verify that Ck is Hilbert-
Schmidt and therefore compact. Hence by the Hilbert-Schmidt theorem  Ck =Pi γih·  ψiiψi where
(γi)i∈N are the positive eigenvalues and (ψi)i∈N are the corresponding eigenvectors that form an
ONB for the range space of Ck denoted as R(Ck). This implies µλ can be decomposed as µλ =
P∞
γi+λhµP  ψiiψi. We can observe that the ﬁlter function corresponding to the problem (7)
is gλ(γ) = 1/(γ + λ). By extending this approach to other ﬁlter functions  we obtain µλ =
P∞
i=1 γigλ(γi)hµP  ψiiψi which is equivalent to µλ = Ckgλ(Ck)µP.
Since Ck is a compact operator  the role of ﬁlter function gλ is to regularize the inverse of Ck.
In standard supervised setting  the explicit form of the solution is fλ = gλ(Lk)Lkfρ where Lk
is the integral operator of kernel k acting in L2(X   ρX ) and fρ is the expected solution given by
fρ(x) =RY y dρ(y|x) [16]. It is interesting to see that µλ admits a similar form to that of fλ  but it is
written in term of covariance operator Ck instead of the integral operator Lk. Moreover  the solution
to (7) is also in a similar form to the regularized conditional embedding µY |X = CY X (Ck + λI)−1
[9]. This connection implies that the spectral ﬁltering may be applied more broadly to improve the
estimation of conditional mean embedding  i.e.  µY |X = CY X gλ(Ck).
The empirical counterpart of (7) is given by

arg min

F

1
n

nXi=1

kk(xi ·) − F[k(xi ·)]k2

H + λkFk2

HS 

(8)

1

i=1 k(xi ·) ⊗ k(xi ·) and ˆµP := 1

K(K + λI)−1Φ where Φ = [k(x1 ·)  . . .   k(xn ·)]⊤  which matches
resulting in ˆµλ = FˆµP = 1⊤
n
with the one in (6) with gλ(K) = (K + λI)−1. Note that this is exactly the F-KMSE proposed in
[1]. Based on µλ which depends on P  an empirical version of it can be obtained by replacing Ck
and µP with their empirical estimators leading to ˜µλ = bCkgλ(bCk)ˆµP. The following result shows
that ˆµλ = ˜µλ  which means the Spectral-KMSE proposed in (6) is equivalent to solving (8).
Proposition 4. Let bCk and ˆµP be the sample counterparts of Ck and µP given by bCk
:=
nPn
nPn
i=1 k(xi ·)  respectively. Then  we have that ˜µλ :=
bCkgλ(bCk)ˆµP = ˆµλ  where ˆµλ is deﬁned in (6).

Having established a regularization interpretation for ˆµλ  it is of interest to study the consistency and
convergence rate of ˆµλ similar to KMSE in Theorem 1. Our main goal here is to derive convergence
rates for a broad class of algorithms given a set of sufﬁcient conditions on the ﬁlter function  gλ. We
believe that for some algorithms it is possible to derive the best achievable bounds  which requires
ad-hoc proofs for each algorithm. To this end  we provide a set of conditions any admissible ﬁlter
function  gλ must satisfy.
Deﬁnition 1. A family of ﬁlter functions gλ : [0  κ2] → R  0 < λ ≤ κ2 is said to be admis-
sible if there exists ﬁnite positive constants B  C  D  and η0 (all independent of λ) such that
(C1) supγ∈[0 κ2] |γgλ(γ)| ≤ B  (C2) supγ∈[0 κ2] |rλ(γ)| ≤ C and (C3) supγ∈[0 κ2] |rλ(γ)|γη ≤
Dλη  ∀ η ∈ (0  η0] hold  where rλ(γ) := 1 − γgλ(γ).
These conditions are quite standard in the theory of inverse problems [15  23]. The constant η0
is called the qualiﬁcation of gλ and is a crucial factor that determines the rate of convergence in
inverse problems. As we will see below  that the rate of convergence of ˆµλ depends on two factors:
(a) smoothness of µP which is usually unknown as it depends on the unknown P and (b) qualiﬁcation
of gλ which determines how well the smoothness of µP is captured by the spectral ﬁlter  gλ.

Theorem 5. Suppose gλ is admissible in the sense of Deﬁnition 1. Let κ = supx∈X pk(x  x). If
µP ∈ R(Cβ

k ) for some β > 0  then for any δ > 0  with probability at least 1 − 3e−δ 
(2√2κ2√δ)min{1 β}

2κB + κB√2δ

kˆµλ − µPk ≤

√n

+ Dλmin{β η0}kC−β

k µPk + Cτ

nmin{1/2 β/2}

kC−β

k µPk 

where R(A) denotes the range space of A and τ is some universal constant that does not depend on
λ and n. Therefore  kˆµλ − µPk = OP(n− min{1/2 β/2}) with λ = o(n− min{1/2 β/2}
min{β η0 } ).
Theorem 5 shows that the convergence rate depends on the smoothness of µP which is imposed
through the range space condition that µP ∈ R(Cβ
k ) for some β > 0. Note that this is in contrast

6

to the estimator in Theorem 1 which does not require any smoothness assumptions on µP. It can
be shown that the smoothness of µP increases with increase in β. This means  irrespective of the
smoothness of µP for β > 1  the best possible convergence rate is n−1/2 which matches with that of
KMSE in Theorem 1. While the qualiﬁcation η0 does not seem to directly affect the rates  it controls
the rate at which λ converges to zero. For example  if gλ(γ) = 1/(γ + λ) which corresponds to
Tikhonov regularization  it can be shown that η0 = 1 which means for β > 1  λ = o(n−1/2)
implying that λ cannot decay to zero slower than n−1/2. Ideally  one would require a larger η0
(preferably inﬁnity which is the case with truncated SVD) so that the convergence of λ to zero can
be made arbitrarily slow if β is large. This way  both β and η0 control the behavior of the estimator.
In fact  Theorem 5 provides a choice for λ—which is what we used in Theorem 1 to study the
admissibility of ˇµλ to Pc β—to construct the Spectral-KMSE. However  this choice of λ depends
on β which is not known in practice (although η0 is known as it is determined by the choice of gλ).
Therefore  λ is usually learnt from data through cross-validation or through Lepski’s method [24] for
which guarantees similar to the one presented in Theorem 5 can be provided. However  irrespective
of the data-dependent/independent choice for λ  checking for the admissibility of Spectral-KMSE
(similar to the one in Theorem 1) is very difﬁcult and we intend to consider it in future work.

5 Empirical studies

Synthetic data. Given the i.i.d. sample X = {x1  x2  . . .   xn} from P where xi ∈ Rd  we evaluate
different estimators using the loss function L(β  X  P) := kPn
i=1 βik(xi ·) − Ex∼P[k(x ·)]k2
H.
The risk of the estimator is subsequently approximated by averaging over m independent copies of
X. In this experiment  we set n = 50  d = 20  and m = 1000. Throughout  we use the Gaussian
RBF kernel k(x  x′) = exp(−kx − x′k2/2σ2) whose bandwidth parameter is calculated using the
median heuristic  i.e.  σ2 = median{kxi − xjk2}. To allow for an analytic calculation of the
loss L(β  X  P)  we assume that the distribution P is a d-dimensional mixture of Gaussians [1  8].
Speciﬁcally  the data are generated as follows: x ∼P4
i=1 πiN (θi  Σi)+ε  θij ∼ U (−10  10)  Σi ∼
W(3 × Id  7)  ε ∼ N (0  0.2 × Id) where U (a  b) and W(Σ0  df ) are the uniform distribution and
Wishart distribution  respectively. As in [1]  we set π = [0.05  0.3  0.4  0.25].
A natural approach for choosing λ is cross-validation procedure  which can be performed efﬁciently
for the iterative methods such as Landweber and accelerated Landweber. For these two algorithms 
we evaluate the leave-one-out score and select βt at the iteration t that minimizes this score (see 
e.g.  Figure 3(a)). Note that these methods have the built-in property of computing the whole regu-
larization path efﬁciently. Since each iteration of the iterated Tikhonov is in fact equivalent to the
F-KMSE  we assume t = 3 for simplicity and use the efﬁcient LOOCV procedure proposed in [1]
to ﬁnd λ at each iteration. Lastly  the truncation limit of TSVD can be identiﬁed efﬁciently by mean
of generalized cross-validation (GCV) procedure [25]. To allow for an efﬁcient calculation of GCV
2.
score  we resort to the alternative loss function L(β) := kKβ − K1nk2
Figure 3 reveals interesting aspects of the Spectral-KMSE. Firstly  as we can see in Figure 3(a)  the
number of iterations acts as shrinkage parameter whose optimal value can be attained within just
a few iterations. Moreover  these methods do not suffer from “over-shrinking” because λ → 0 as
t → ∞. In other words  if the chosen t happens to be too large  the worst we can get is the stan-
dard empirical estimator. Secondly  Figure 3(b) demonstrates that both Landweber and accelerated
Landweber are more computationally efﬁcient than the F-KMSE. Lastly  Figure 3(c) suggests that
the improvement of shrinkage estimators becomes increasingly remarkable in a high-dimensional
setting.
Interestingly  we can observe that most Spectral-KMSE algorithms outperform the S-
KMSE  which supports our hypothesis on the importance of the geometric information of RKHS
mentioned in Section 3. In addition  although the TSVD still gain from shrinkage  the improvement
is smaller than other algorithms. This highlights the importance of ﬁlter functions and associated
parameters.

Real data. We apply Spectral-KMSE to the density estimation problem via kernel mean matching
[1  26]. The datasets were taken from the UCI repository1 and pre-processed by standardizing
I) to the pre-processed dataset

each feature. Then  we ﬁt a mixture model Q = Pr

1http://archive.ics.uci.edu/ml/

j=1 πjN (θj  σ2

j

7

10−1.2

10−1.4

10−1.6

)
s
n
o
i
t
a
r
e
t
i
 
0
0
0
1
(
 
k
s
R

i

10−1.8
 
0

 

KME
S−KMSE
F−KMSE
Landweber
Acc. Landweber
Iterated Tikhonov (λ=0.01)

KME
S−KMSE
F−KMSE
Landweber
Acc Landweber
Iterated Tikhonov
Truncated SVD

102

101

100

10−1

10−2

10−3

10−4

10−5

60

50

40

30

20

10

 

S−KMSE
F−KMSE
Landweber
Acc Landweber
Iter. Tikhonov
Truncated SVD

 

)
s
n
o

i
t

a
r
e

t
i
 

0
0
0
1
(
 
t

n
e
m
e
v
o
r
p
m

I
 
f

t

 

o
e
g
a
n
e
c
r
e
P

)
s
n
o
i
t
a
r
e
t
i
 
0
0
0
1
(
 
e
m
T
 
d
e
s
p
a
E

i

l

10

20

Iterations

30

40

10−6

 
101

102

Sample Size

103

0

 

20

40
Dimensionality

60

80

100

(a) risk vs. iteration

(b) runtime vs. sample size

(c) risk vs. dimension

Figure 3: (a) For iterative algorithms  the number of iterations acts as shrinkage parameter. (b) The
iterative algorithms such as Landweber and accelerated Landweber are more efﬁcient than the F-
KMSE. (c) A percentage of improvement w.r.t. the KME  i.e.  100 × (R − Rλ)/R where R and Rλ
denote the approximated risk of KME and KMSE  respectively. Most Spectral-KMSE algorithms
outperform S-KMSE which does not take into account the geometric information of the RKHS.

i=1 by minimizing kµQ − ˆµXk2 subject to the constraintPr

j=1 πj = 1. Here µQ is the
X := {xi}n
mean embedding of the mixture model Q and ˆµX is the empirical mean embedding obtained from
X. Based on different estimators of µX  we evaluate the resultant model Q by the negative log-
likelihood score on the test data. The parameters (πj  θj  σ2
j ) are initialized by the best one obtained
from the K-means algorithm with 50 initializations. Throughout  we set r = 5 and use 25% of each
dataset as a test set.

Dataset

Table 2: The average negative log-likelihood evaluated on the test set. The results are obtained from
30 repetitions of the experiment. The boldface represents the statistically signiﬁcant results.
Iter Tik TSVD
36.1334
10.9078
18.1267
14.2868
13.8633
28.0417
18.4128
16.6954
35.1881

S-KMSE F-KMSE Landweber Acc Land
36.1402
10.7403
18.1158
14.2195
13.8426
28.0546
18.3693
16.7548
35.1814

ionosphere
glass
bodyfat
housing
vowel
svmguide2
vehicle
wine
wdbc

KME
36.1769
10.7855
18.1964
14.3016
13.9253
28.1091
18.5295
16.7668
35.1916

36.1554
10.7541
18.1941
14.1983
14.1368
27.9693
18.3124
16.6790
35.1366

36.1622
10.7448
18.1810
14.0409
13.8817
27.9640
18.2547
16.7457
35.0023

36.1204
10.7099
18.1607
14.2499
13.8337
28.1052
18.4873
16.7596
35.1402

36.1442
10.7791
18.1061
14.3129
13.8375
28.1128
18.3910
16.5719
35.1850

Table 2 reports the results on real data. In general  the mixture model Q obtained from the proposed
shrinkage estimators tend to achieve lower negative log-likelihood score than that obtained from the
standard empirical estimator. Moreover  we can observe that the relative performance of different
ﬁlter functions vary across datasets  suggesting that  in addition to potential gain from shrinkage  in-
corporating prior knowledge through the choice of ﬁlter function could lead to further improvement.

6 Conclusion

We shows that several shrinkage strategies can be adopted to improve the kernel mean estimation.
This paper considers the spectral ﬁltering approach as one of such strategies. Compared to previous
work [1]  our estimators take into account the speciﬁcs of kernel methods and meaningful prior
knowledge through the choice of ﬁlter functions  resulting in a wider class of shrinkage estimators.
The theoretical analysis also reveals a fundamental similarity to standard supervised setting. Our
estimators are simple to implement and work well in practice  as evidenced by the empirical results.

Acknowledgments

The ﬁrst author thanks Ingo Steinwart for pointing out existing works along the line of spectral ﬁlter-
ing  and Arthur Gretton for suggesting the connection of shrinkage to smooth operator framework.
This work was carried out when the second author was a Research Fellow in the Statistical Labora-
tory  Department of Pure Mathematics and Mathematical Statistics at the University of Cambridge.

8

References

[1] K. Muandet  K. Fukumizu  B. Sriperumbudur  A. Gretton  and B. Sch¨olkopf. “Kernel Mean Estimation

and Stein Effect”. In: ICML. 2014  pp. 10–18.

[2] B. Sch¨olkopf  A. Smola  and K.-R. M¨uller. “Nonlinear Component Analysis as a Kernel Eigenvalue

[3]

Problem”. In: Neural Computation 10.5 (July 1998)  pp. 1299–1319.
J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge  UK: Cambridge
University Press  2004.

[4] B. Sch¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines  Regularization  Opti-

mization  and Beyond. Cambridge  MA  USA: MIT Press  2001.

[5] A. Berlinet and T. C. Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer

Academic Publishers  2004.

[6] A. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. “A Hilbert Space Embedding for Distributions”. In:

ALT. Springer-Verlag  2007  pp. 13–31.

[7] A. Gretton  K. M. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. J. Smola. “A kernel method for the

two-sample-problem”. In: NIPS. 2007.

[8] K. Muandet  K. Fukumizu  F. Dinuzzo  and B. Sch¨olkopf. “Learning from Distributions via Support

Measure Machines”. In: NIPS. 2012  pp. 10–18.

[9] L. Song  J. Huang  A. Smola  and K. Fukumizu. “Hilbert Space Embeddings of Conditional Distributions

with Applications to Dynamical Systems”. In: ICML. 2009.

[10] K. Muandet  D. Balduzzi  and B. Sch¨olkopf. “Domain Generalization via Invariant Feature Representa-

tion”. In: ICML. 2013  pp. 10–18.

[11] K. Muandet and B. Sch¨olkopf. “One-Class Support Measure Machines for Group Anomaly Detection”.

In: UAI. AUAI Press  2013  pp. 449–458.

[12] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Sch¨olkopf  and G. R. G. Lanckriet. “Hilbert Space

Embeddings and Metrics on Probability Measures”. In: JMLR 99 (2010)  pp. 1517–1561.

[13] K. Fukumizu  L. Song  and A. Gretton. “Kernel Bayes’ Rule: Bayesian Inference with Positive Deﬁnite

Kernels”. In: JMLR 14 (2013)  pp. 3753–3783.

[14] C. M. Stein. “Estimation of the Mean of a Multivariate Normal Distribution”. In: The Annals of Statistics

9.6 (1981)  pp. 1135–1151.

[15] H. W. Engl  M. Hanke  and A. Neubauer. Regularization of Inverse Problems. Vol. 375. Mathematics

and its Applications. Kluwer Academic Publishers Group  Dordrecht  1996.

[16] E. D. Vito  L. Rosasco  and R. Verri. Spectral Methods for Regularization in Learning Theory. 2006.
[17] E. D. Vito  L. Rosasco  A. Caponnetto  U. D. Giovannini  and F. Odone. “Learning from Examples as

an Inverse Problem.” In: JMLR 6 (2005)  pp. 883–904.

[18] L. Baldassarre  L. Rosasco  A. Barla  and A. Verri. “Vector Field Learning via Spectral Filtering.” In:

ECML/PKDD (1). Vol. 6321. Lecture Notes in Computer Science. Springer  2010  pp. 56–71.
J. Kim and C. D. Scott. “Robust Kernel Density Estimation”. In: JMLR 13 (2012)  2529–2565.
I. Steinwart and A. Christmann. Support Vector Machines. New York: Springer  2008.

[19]
[20]
[21] L. Song and B. Dai. “Robust Low Rank Kernel Embeddings of Multivariate Distributions”. In: NIPS.

2013  pp. 3228–3236.

[22] S. Gr¨unew¨alder  G. Arthur  and J. Shawe-Taylor. “Smooth Operators”. In: ICML. Vol. 28. 2013 

pp. 1184–1192.

[23] L. L. Gerfo  L. Rosasco  F. Odone  E. D. Vito  and A. Verri. “Spectral Algorithms for Supervised Learn-

ing.” In: Neural Computation 20.7 (2008)  pp. 1873–1897.

[24] O. V. Lepski  E. Mammen  and V. G. Spokoiny. “Optimal Spatial Adaptation to Inhomogeneous Smooth-
ness: An Approach based on Kernel Estimates with Variable Bandwith Selectors”. In: Annals of Statistics
25 (1997)  pp. 929–947.

[25] G. Golub  M. Heath  and G. Wahba. “Generalized Cross-Validation as a Method for Choosing a Good

Ridge Parameter”. In: Technometrics 21 (1979)  pp. 215–223.

[26] L. Song  X. Zhang  A. Smola  A. Gretton  and B. Sch¨olkopf. “Tailoring Density Estimation via Repro-

ducing Kernel Moment Matching”. In: ICML. 2008  pp. 992–999.

9

,Krikamol Muandet
Bharath Sriperumbudur
Bernhard Schölkopf
Nihar Bhadresh Shah
Dengyong Zhou