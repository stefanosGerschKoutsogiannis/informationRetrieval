2018,A Convex Duality Framework for GANs,Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function  this game reduces to finding the generative model minimizing a divergence measure  e.g. the Jensen-Shannon (JS) divergence  to the data distribution. However  in practice the discriminator is constrained to be in a smaller class F such as neural nets. Then  a natural question is how the divergence minimization interpretation changes as we constrain F. In this work  we address this question by developing a convex duality framework for analyzing GANs. For a convex set F  this duality framework interprets the original GAN formulation as finding the generative model with minimum JS-divergence to the distributions penalized to match the moments of the data distribution  with the moments specified by the discriminators in F. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. As a byproduct  we apply the duality framework to a hybrid of f-divergence and Wasserstein distance. Unlike the f-divergence  we prove that the proposed hybrid divergence changes continuously with the generative model  which suggests regularizing the discriminator's Lipschitz constant in f-GAN and vanilla GAN. We numerically evaluate the power of the suggested regularization schemes for improving GAN's training performance.,A Convex Duality Framework for GANs

Farzan Farnia∗

farnia@stanford.edu

David Tse∗

dntse@stanford.edu

Abstract

Generative adversarial network (GAN) is a minimax game between a generator
mimicking the true model and a discriminator distinguishing the samples produced
by the generator from the real training samples. Given an unconstrained discrimi-
nator able to approximate any function  this game reduces to ﬁnding the generative
model minimizing a divergence score  e.g. the Jensen-Shannon (JS) divergence  to
the data distribution. However  in practice the discriminator is constrained to be
in a smaller class F such as convolutional neural nets. Then  a natural question is
how the divergence minimization interpretation will change as we constrain F. In
this work  we address this question by developing a convex duality framework for
analyzing GAN minimax problems. For a convex set F  this duality framework
interprets the original vanilla GAN problem as ﬁnding the generative model with
the minimum JS-divergence to the distributions penalized to match the moments
of the data distribution  with the moments speciﬁed by the discriminators in F.
We show that this interpretation more generally holds for f-GAN and Wasserstein
GAN. We further apply the convex duality framework to explain why regularizing
the discriminator’s Lipschitz constant  e.g. via spectral normalization or gradi-
ent penalty  can greatly improve the training performance in a general f-GAN
problem including the vanilla GAN formulation. We prove that Lipschitz regu-
larization can be interpreted as convolving the original divergence score with the
ﬁrst-order Wasserstein distance  which results in a continuously-behaving target
divergence measure. We numerically explore the power of Lipschitz regularization
for improving the continuity behavior and training performance in GAN problems.

1

Introduction

Learning a probability model from data samples is a fundamental task in unsupervised learning. The
recently developed generative adversarial network (GAN) [1] leverages the power of deep neural
networks to successfully address this task across various domains [2]. In contrast to traditional
methods of parameter ﬁtting like maximum likelihood estimation  the GAN approach views the
problem as a game between a generator G whose goal is to generate fake samples that are close to
the real data training samples and a discriminator D whose goal is to distinguish between the real
and fake samples. The generator creates the fake samples by mapping from random noise input.
The following minimax problem is the original GAN problem  also called vanilla GAN  introduced in
[1]

E(cid:2)log D(X)(cid:3) + E(cid:2)log(cid:0)1 − D(G(Z))(cid:1)(cid:3).

G∈G max
min
D∈F

(1)

Here Z denotes the generator’s noise input  X represents the random vector for the real data distributed
as PX  and G and F respectively represent the generator and discriminator function sets. Implement-
ing this minimax game using deep neural network classes G and F has lead to the state-of-the-art
generative model for many different tasks.

∗Department of Electrical Engineering  Stanford University.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Figure 1: (a) Divergence minimization in vanilla GAN with D unconstrained  between the generative
models and PX  (b) Divergence minimization in vanilla GAN with D constrained to a linear space F 
between the generative models and the discriminator moment matching models formed around PX.

To shed light on the probabilistic meaning of vanilla GAN  [1] shows that given an unconstrained
discriminator D  i.e. if F contains all possible functions  the minimax problem (1) will reduce to

min
G∈G JSD(PX  PG(Z)) 

(2)

where JSD denotes the Jensen-Shannon (JS) divergence. The optimization problem (2) can be
interpreted as ﬁnding the closest generative model to the data distribution PX (Figure 1a)  where
distance is measured using the JS-divergence. Various GAN formulations were later proposed by
changing the divergence measure in (2). f-GAN [3] generalizes vanilla GAN by minimizing a general
f-divergence. Wasserstein GAN (WGAN) [4] is based on the ﬁrst-order Wasserstein (the earth-
mover’s) distance. MMD-GAN [5  6  7] considers the maximum mean discrepancy. Energy-based
GAN [8] uses the total variation distance. Quadratic GAN [9] ﬁnds the distribution minimizing the
second-order Wasserstein distance.
However  GANs trained in practice differ from this minimum divergence formulation  since their
discriminator is not optimized over an unconstrained set and is constrained to smaller classes such as
convolutional neural nets. As shown in [9  10]  constraining the discriminator is in fact necessary to
guarantee good generalization properties for a GAN’s learned model. Then  how does the minimum
divergence interpretation illustarted in Figure 1a change after we constrain the discrminator? An
existing approach used in [10  11] is to view the maximum discriminator objective as a discriminator
class F-based distance between probability distributions. For unconstrained F  the F-based distance
reduces to the original divergence measure  e.g. the JS-divergence in vanilla GAN.
While [10] demonstrates a useful application of F-based distances in analyzing GANs’ generalization
properties  the connection between F-based distances and the original divergence score remains
unclear for a constrained F. Then  what is the probabilistic interpretation of GAN minimax game
in practice where a constrained discriminator is used? In this work  we address this question by
interpreting the dual problem to the discriminator maximization problem. To analyze the dual problem 
we develop a convex duality framework for divergence minimization problems with generalized
moment matching constraints. We apply this convex duality framework to the f-divergence and
Wasserstein distance families  providing interpretation for f-GAN  including vanilla GAN minimizing
the JS-divergence  and Wasserstein GAN.
Speciﬁcally  we generalize [1]’s interpretation of the vanilla GAN problem (1)  which only holds
for an unconstrained discriminator set  to the more general case with linear space discriminator
sets. Under this assumption  we interpret vanilla GAN as the following JS-divergence minimization
between two sets of probability distributions (Figure 1b)  the generative models and the discriminator
moment-matching models 

(3)
Here PF (PX) denotes the set of discriminator moment matching models that contains any distribution
Q satisfying moment matching constraints EQ[D(X)] = EP [D(X)] for any discriminator D ∈ F.

JSD(PG(Z)  Q).

min
G∈G

min

Q∈PF (PX)

2

More generally  we show that a similar interpretation holds for GANs trained over convex discrimina-
tor sets. We also discuss the application of our duality framework to neural net discriminators with
bounded Lipschitz constants. While a set of neural network functions is not necessarily convex  we
prove that a convex combination of Lipschitz-bounded neural nets can be approximated by uniformly
combining boundedly-many neural net functions. This result applied to our duality framework shows
that the convex duality interpretation approximately holds for neural net discriminators.
As a byproduct  we apply the duality framework to the inﬁmal convolution hybrid of f-divergence
and the ﬁrst-order Wasserstein (W1) distance  e.g. the following hybrid of JS-divergence and W1
distance:

dJSD W1 (P1  P2) := min
Q

W1(P1  Q) + JSD(Q  P2).

(4)

We prove that unlike the JS-divergence this hybrid divergence changes continuously and remedies the
undesired discontinuous behavior of JS-divergence in optimizing generator parameters for vanilla
GAN. [4] observes this issue with minimzing the JS-divergence in vanilla GAN and proposes
to instead minimize the continuously-changing W1 distance in WGAN. However  as empirically
demonstrated in [12] vanilla GAN with a Lipschitz-bounded discriminator results in superior and
state-of-the-art generative models over multiple benchmark tasks. In this paper  we leverage the
convex duality framework to prove that the inﬁmal convolution hybrid dJSD W1  possessing the
same desired continuity property as in the W1-distance  is in fact the divergence score minimized in
vanilla GAN with a Lipschitz-bounded discriminator. Hence  our analysis provides an explanation
for why regularizing the discriminator’s Lipschitz constant via gradient penalty [13] or spectral
normalization [12] greatly improves the training performance in vanilla GAN. We then extend our
focus to the inﬁmal convolution hybrid between the f-divergence and the second-order Wasserstein
(W2) distance. In this case  we derive the f-GAN (e.g. vanilla GAN) problem with its discriminator
being adversarially trained over the generator’s samples. We numerically evaluate the power of these
hybrid divergences and their implied regularization schemes for training GANs.

2 Divergence Measures

2.1

Jensen-Shannon divergence

The Jensen-Shannon divergence is deﬁned in terms of the KL-divergence (denoted by KL) as

JSD(P  Q) :=

KL(P(cid:107)M ) +

1
2

KL(Q(cid:107)M )

1
2

is the mid-distribution between P and Q. Unlike the KL-divergence  the JS-

where M = P +Q
divergence is symmetric JSD(P  Q) = JSD(Q  P ) and bounded 0 ≤ JSD(P  Q) ≤ log 2.
2.2

f-divergence

2

The f-divergence family [14] generalizes the KL and JS divergence measures. Given a convex lower
semicontinuous function f with f (1) = 0  the f-divergence df is deﬁned as

df (P  Q) := EP

(5)
Here EP denotes expectation over distribution P and p  q denote the density functions for distributions
P  Q  respectively. The KL-divergence and the JS-divergence are members of the f-divergence family 
corresponding to respectively fKL(t) = t log t and fJSD(t) = t

2 log t − t+1

2 .
2 log t+1

p(x)

(cid:90)

(cid:2)f(cid:0) q(X)

p(X)

(cid:1)(cid:3) =

p(x)f(cid:0) q(x)

(cid:1) dx.

2.3 Optimal transport cost  Wasserstein distance
The optimal transport cost for cost function c(x  x(cid:48))  which we denote by Wc  is deﬁned as

D c-concave

3

Wc(P  Q) :=

inf

M∈Π(P Q)

(6)

where Π(P  Q) contains all couplings with marginals P  Q. The Kantorovich duality [15] shows that
for a non-negative lower semi-continuous cost c 
EP

(cid:2)Dc(X)(cid:3) 

Wc(P  Q) = max

(7)

E(cid:2)c(X  X(cid:48))(cid:3) 
(cid:2)D(X)(cid:3) − EQ

where we use Dc to denote D’s c-transform deﬁned as Dc(x) := supx(cid:48) D(x(cid:48)) − c(x  x(cid:48)) and call
D c-concave if D is the c-transform of a valid function. An important special case is the ﬁrst-order
Wasserstein (W1) distance corresponding to the norm cost c(x  x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)  i.e.

For the norm cost function  a function D is c-concave if and only if D is 1-Lipschitz  and the
c-transform Dc = D holds for a 1-Lipschitz D. Therefore  the Kantorovich duality result (7) implies

(8)

(9)

(10)

W1(P  Q) :=

inf

M∈Π(P Q)

W1(P  Q) = max

D 1-Lipschitz

EP

E(cid:2)(cid:107)X − X(cid:48)(cid:107)(cid:3).
(cid:2)D(X)(cid:3) − EQ
(cid:2)D(X)(cid:3).
E(cid:2)(cid:107)X − X(cid:48)(cid:107)2(cid:3)1/2

.

In this paper  we also consider and analyze the second-order Wasserstein (W2) distance  corresponding
to the norm-squared cost c(x  x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2  deﬁned as

W2(P  Q) :=

inf

M∈Π(P Q)

3 Divergence minimization in GANs: a convex duality framework

In this section  we develop a convex duality framework for analyzing divergence minimization
problems conditioned to moment-matching constraints. Our framework generalizes the duality
framework developed in [16] for the f-divergence family.
For a general divergence measure d(P  Q)  we deﬁne d’s convex conjugate for distribution P   which
P   as the following operator mapping a real-valued function with domain X to a real
we denote by d∗
number
(11)
Here the supremum is over all distributions on the support set X . The following theorem connects this
operation to divergence minimization problems under moment matching constraints. Next section 
we discuss the application of this theorem in deriving several well-known GAN formulations for
divergence measures discussed in Section 2.
Theorem 1. Suppose divergence d(P  Q) is non-negative  lower semicontinuous and convex in
distribution Q. Consider a convex set of continuous functions F and assume support set X is
compact. Then 

EQ[D(X)] − d(P  Q).

d∗
P (D) := sup
Q

G∈G max
min
D∈F
G∈G min

Q

= min

EPX [D(X)] − d∗

(cid:8)d(PG(Z)  Q) + max

D∈F { EPX[D(X)] − EQ[D(X)]}(cid:9).

(D)

PG(Z)

(12)

Proof. We defer the proof to the Appendix.

Theorem 1 interprets the LHS minimax problem in (12) as ﬁnding the closest generative model to
a set of distributions penalized to share the same generalized moments speciﬁed by discriminators
in F with PX. The following corollary of Theorem 1 shows if we further assume that F is a linear
space  then the additive penalty term penalizing the worst-case moment mismatch will turn to hard
constraints in the discriminator optimization problem. This result reveals a divergence minimization
problem between the generative models and the following set PF (P ) which we call the discriminator
moment matching models 

PF (P ) :=(cid:8)Q : ∀D ∈ F  EQ[D(X)] = EP [D(X)](cid:9).

(13)
Corollary 1. In Theorem 1 suppose F is also a linear space  i.e. for any D1  D2 ∈ F  λ ∈ R we
have D1 + D2 ∈ F and λD1 ∈ F. Then 
EPX [D(X)] − d∗

d(PG(Z)  Q).

(14)

min

PG(Z)

(D) = min
G∈G

Q∈PF (PX)

G∈G max
min
D∈F

In next section  we apply this duality framework to divergence measures discussed in Section 2 and
show how to derive various GAN problems through this convex duality framework.

4

4 Duality framework applied to different divergence measures

f-divergence: f-GAN and vanilla GAN

4.1
Theorem 2 shows the application of Theorem 1 to an f-divergence. Here we use f∗ to denote
f’s convex-conjugate [17]  deﬁned as f∗(u) := supt ut − f (t). Theorem 2 applies to a general
f-divergence df as long as the convex-conjugate f∗ is a non-deacreasing function  a condition met by
all f-divergence examples discussed in [3] with the only exception of Pearson χ2-divergence.
Theorem 2. Consider f-divergence df where the corresponding f has a non-decreasing convex-
conjugate f∗. In addition to the assumptions in Theorem 1  suppose that F is closed to adding
constants  i.e. D + λ ∈ F for any D ∈ F  λ ∈ R. Then  the minimax problem in the LHS of (12)
and (14)  reduces to

E[D(X)] − E(cid:2)f∗(cid:0)D(G(Z))(cid:1)(cid:3).

(15)

G∈G max
min
D∈F

Proof. We defer the proof to the Appendix.

The minimax problem (15) is the f-GAN problem introduced and discussed in [3]. Therefore 
Theorem 2 reveals that f-GAN searches for the generative model minimizing the f-divergence to the
discriminator moment matching models speciﬁed by discriminator set F. The following example
shows the application of this result to the vanilla GAN introduced in the original GAN work [1].
2 log t −
Example 1. Consider the JS-divergence  i.e. f-divergence corresponding to fJSD(t) = t
t+1
2 log t+1

2 . Then  (15) up to additive and multiplicative constants reduces to

E[D(X)] + E(cid:2)log(cid:0)1 − exp(D(G(Z))(cid:1)(cid:3).

G∈G max
min
D∈F

(16)
Moreover  if for function set ˜F the corresponding F = {D : D(x) = − log(1 + exp( ˜D(x)))  ˜D ∈
˜F} is a convex set  then (16) will reduce to the following minimax game which is the vanilla GAN
problem (1) with sigmoid activation applied to the discriminator output 

E(cid:2) log

G∈G max
min
˜D∈ ˜F

1

1 + exp( ˜D(X))

(cid:3) + E(cid:2) log

(cid:3).

exp( ˜D(X))

1 + exp( ˜D(X))

Therefore the minimax game between G and D in (18) can be viewed as minimizing the optimal
transport cost between generative models and the distributions matching moments over F with PX’s
moments. The following example applies this result to the ﬁrst-order Wasserstein distance and
recovers the WGAN problem [4] with a constrained 1-Lipschitz discriminator.
Example 2. Let the optimal transport cost in (18) be the W1 distance  and suppose F is a convex
subset of 1-Lipschitz functions. Then  the minimax problem (18) will reduce to

E[D(X)] − E(cid:2)D(G(Z))(cid:3).

G∈G max
min
D∈F

Therefore  the moment-matching interpretation also holds for WGAN: for a convex set F of 1-
Lipschitz functions WGAN ﬁnds the generative model with minimum W1 distance to the distributions
penalized to share the same moments over F with the data distribution. We discuss two more
examples in the Appendix: 1) for the indicator cost cI (x  x(cid:48)) = I(x (cid:54)= x(cid:48)) corresponding to the total
variation distance we draw the connection to the energy-based GAN [8]  2) for the second-order
cost c2(x  x(cid:48)) = (cid:107)x − x(cid:48)(cid:107)2 we recover [9]’s quadratic GAN formulation under the LQG setting
assumptions  i.e. linear generator  quadratic discriminator and Gaussian input data.

5

4.2 Optimal Transport Cost: Wasserstein GAN

Theorem 3. Let divergence d be optimal transport cost Wc where c is a non-negative lower semi-
continuous cost function. Then  the minimax problem in the LHS of (12) and (14) reduces to

E[D(X)] − E(cid:2)Dc(G(Z))(cid:3).

G∈G max
min
D∈F

Proof. We defer the proof to the Appendix.

(17)

(18)

(19)

5 Duality framework applied to neural net discriminators

We applied the duality framework to analyze GAN problems with convex discriminator sets. However 
a neural net set Fnn = {fw : w ∈ W}  where fw denotes a neural net function with ﬁxed
architecture and weights w in feasible set W  does not generally satisfy this convexity assumption.
Note that a linear combination of several neural net functions in Fnn may not remain in Fnn.
Therefore  we apply the duality framework to Fnn’s convex hull  which we denote by conv(Fnn) 
containing any convex combination of neural net functions in Fnn. However  a convex combination
of inﬁnitely-many neural nets from Fnn is characterized by inﬁnitely-many parameters  which makes
optimizing the discriminator over conv(Fnn) computationally intractable. In the following theorem 
we show that although a function in conv(Fnn) is a combination of inﬁnitely-many neural nets  that
function can be approximated by uniformly combining boundedly-many neural nets in Fnn.
Theorem 4. Suppose any function fw ∈ Fnn is L-Lipschitz and bounded as |fw(x)| ≤ M. Also 
assume that the k-dimensional random input X is norm-bounded as (cid:107)X(cid:107)2 ≤ R. Then  any function
in conv(Fnn) can be uniformly approximated over the ball (cid:107)x(cid:107)2 ≤ R within -error by a uniform
combination ˆf (x) = 1
m

(cid:80)m
i=1 fwi(x) of m = O( M 2k log(LR/)

) functions (fwi)m

i=1 ∈ Fnn.

2

Proof. We defer the proof to the Appendix.

The above theorem suggests using a uniform combination of multiple discriminator nets to ﬁnd a
better approximation of the solution to the divergence minimization problem in Theorem 1 solved
over conv(Fnn). Note that this approach is different from MIX-GAN [10] proposed for achieving
equilibrium in GAN minimiax game. While our approach considers a uniform combination of
multiple neural nets as the discriminator  MIX-GAN considers a randomized combination of the
minimax game over multiple neural net discriminators and generators.

6

Inﬁmal Convolution hybrid of f-divergence and Wasserstein distance:
GAN with Lipschitz or adversarially-trained discriminator

Here we apply the convex duality framework to a novel class of divergence measures. For an f-
divergence df   we deﬁne the divergence score df W1  which we call the inﬁmal convolution hybrid of
df and W1 divergence measures  as follows

W1(P1  Q) + df (Q  P2).

df W1 (P1  P2) := inf
Q

(20)
The above inﬁmum is taken over all distributions on the support set X   ﬁnding the distribution Q∗
minimizing the sum of the Wasserstein distance between P1 and Q and the f-divergence from Q to P2.
Earlier in the introduction  we mentioned and discussed a special case of the above deﬁnition for the
hybrid between the JS-divergence and W1-distance. While f-divergence in f-GAN  e.g. JS-divergence
in vanilla GAN  does not change continuously with the generator parameters  the following theorem
proves that similar to the continuous behavior of W1-distance shown in [18  4] the inﬁmal convolution
hybrid divergence changes continuously with the generative model.
Theorem 5. Suppose Gθ ∈ G is continuously changing with parameters θ. Then  for any Q and
Z  df W1(PGθ (Z)  Q) will behave continuously as a function of θ. Moreover  if Gθ is assumed to be
locally Lipschitz  then df W1(PGθ (Z)  Q) will be differentiable w.r.t. θ almost everywhere.

Proof. We defer the proof to the Appendix.

Our next result reveals the minimax problem dual to minimizing this hybrid divergence with symmet-
ric f-divergence component. We note that this symmetricity condition is met by the JS-divergence
and the squared Hellinger divergence among the f-divergence examples discussed in [3].
Theorem 6. Consider df W1 with a symmetric f-divergence df   i.e. df (P  Q) = df (Q  P )  satisfying
the assumptions in Theorem 2. If the composition f∗ ◦ D is 1-Lipschitz for all D ∈ F  the minimax
problem in Theorem 1 for the hybrid df W1 reduces to the f-GAN problem  i.e.

E[D(X)] − E(cid:2) f∗(cid:0)D(G(Z)(cid:1)(cid:3).

(21)

G∈G max
min
D∈F

Proof. We defer the proof to the Appendix.

6

The above theorem reveals that if the discriminator’s Lipschitz constant in f-GAN is properly
regularized  then solving the f-GAN problem over the regularized discriminator in fact minimizes
the continuously-changing divergence df W1. As a special case  in vanilla GAN (17) we only need
to constrain the discriminator ˜D to be 1-Lipschitz  which can be done via the gradient penalty
regularization [13] or the spectral normalization of ˜D’s weight matrices [12]. Therefore  using these
techniques we indeed minimize the continuously-behaving divergence score dJSD W1. These results
are consistent with [12]’s empirical results indicating that regularizing the discriminator’s Lipschitz
constant improves the training performance in vanilla GAN.
Our discussion has so far focused on convolving f-divergence with the ﬁrst order Wasserstein distance 
which translates into training f-GAN with a Lipschitz-bounded discriminator. As another solution 
we show that the desired continuity property can also be achieved through the following inﬁmal
convolution with the second-order Wasserstein (W2) distance-squared:

df W2(P1  P2) := inf
Q

(22)
Theorem 7. Suppose Gθ ∈ G continuously changes with parameters θ ∈ Rk. Then  for any
distribution Q and random vector Z  df W2 (PGθ (Z)  Q) will be continuous in θ. Also  if we further
assume Gθ is bounded and locally-Lipschitz w.r.t. θ  then the divergence df W2(PGθ (Z)  Q) is almost
everywhere differentiable w.r.t. θ.

W 2

2 (P1  Q) + df (Q  P2).

Proof. We defer the proof to the Appendix.

The following result shows that minimizing df W2 reduces to f-GAN problem where the discriminator
is being adversarially trained.
Theorem 8. Assume df and F satisfy the assumptions in Theorem 6. Then  the minimax problem in
Theorem 1 corresponding to the hybrid df W2 divergence reduces to

E[D(X)] + E(cid:2) min

−f∗(cid:0)D( G(Z) + u )(cid:1) + (cid:107)u(cid:107)2(cid:3).

G∈G max
min
D∈F

u

(23)

Proof. We defer the proof to the Appendix.

The above result reduces minimizing the hybrid df W2 divergence to an f-GAN minimax game with
an additional third player. Here  the third player assists the generator by perturbing the generated fake
samples in order to make them harder to be distinguished from the real samples by the discriminator.
The cost for perturbing a fake sample G(Z) to G(Z) + u will be proportional to (cid:107)u(cid:107)2  constraining
the power of the third player who plays adversarially against the discriminator. To implement the
game between the three players  we can adversarially learn the discriminator while we are training
GAN  via the Wasserstein risk minimization (WRM) adversarial learning scheme discussed in [19].

7 Numerical Experiments

To evaluate our theoretical results  we used the CelebA [20] and LSUN-bedroom [21] datasets.
Furthermore  in the Appendix we include the results of our experiments over the MNIST [22]
dataset. We considered vanilla GAN [1] with the minimax formulation in (17) and DCGAN [23]
convolutional architecture for the neural net discriminator and generator. We used the code provided
by [13] and trained DCGAN via Adam optimizer [24] for 200 000 generator iterations. We applied 5
discriminator updates per generator update.
Figure 2 shows how the discriminator loss evaluated over 2000 validation samples  which is an
estimate of the divergence measure  changed as we trained the DCGAN over LSUN samples. Using
standard DCGAN regularizied by only batch normalization (BN) [25]  we observed (Figure 2- top
left) that the JS-divergence estimate always remained close to its maximum value log2 2 = 1 and also
correlated poorly with the visual quality of the generated samples. In this experiment  the vanilla GAN
training failed and led to mode collapse starting at about the 110 000th iteration. On the other hand 
after replacing BN with two different Lipschitz regularization tecniques  spectral normalization (SN)
[12] and gradient penalty (GP) [13]  to ensure that the discriminator is 1-Lipschitz  the discriminator
loss decreased in a continuous monotonic fashion (Figures 2-top right and 2-bottom left).
These observations are consistent with Theorems 5 and 6 showing that the discriminator loss will
become an estimate for the inﬁmal convolution hybrid dJSD W1 divergence which is behaving contin-
uously with generator parameters. Also  the samples generated by the Lipschitz-regularized DCGAN

7

Figure 2: Divergence estimate in DCGAN trained over LSUN samples  (top-left) JS-divergence in
DCGAN regularized with batch normalization (BN)  (top-right) hybrid dJSD W1 in DCGAN with a
1-Lipschitz spectrally-normalized (SN) discriminator  (bottom-left) hybrid dJSD W1 in DCGAN with
a 1-Lipschitz discriminator regularized via the gradient penalty (GP)  (bottom-right) hybrid dJSD W2
in DCGAN with discriminator being adversarially-trained using WRM.

looked qualitatively better and correlated well with the estimate of dJSD W1 divergence. Figure
2-bottom right shows that a similar desired behavior with nice monotonic decrease in discriminator’s
loss can also be achieved through minimizing the second-order hybrid divergence dJSD W2. In this
experiment  we trained the discriminator in vanilla GAN via the Wasserstein risk minimization
(WRM) adversarial learning scheme [19].
Figure 3 shows the results of similar experiments over the CelebA dataset. Again  we observed
(Figure 3-top left) that the JS-divergence estimate remains close to 1 while training DCGAN with
BN. However  after applying two different Lipschitz regularization methods  SN and GP in Figures
3-top right and bottom left  we observed that the hybrid dJSD W1 changed nicely and monotonically 
and correlated well with the quality of samples generated. Figure 3-bottom right shows that a similar
desired behavior can also be obtained after minimizing the second-order inﬁmal convolution hybrid
dJSD W2 divergence. We defer the presentation of some random samples generated by the generators
trained in these experiments to the Appendix.

8 Related Work

Theoretical studies of GAN have focused on three different aspects: approximation  generalization 
and optimization. Regarding the approximation properties of GANs  [11] studies GANs’ approxima-
tion power through a moment-matching approach. The authors view the maximized discriminator
objective as an F-based adversarial divergence  showing that the adversarial divergence between two
distributions will be at its minimum value if the two distributions have the same generalized moments
speciﬁed by F. Our convex duality framework provides a dual interpretation for their results and
draws the connection between the adversarial diveregnce and the original divergence scores. [26]
studies the f-GAN problem through an information geometric approach and the connection between
the Bregman divergence and the f-divergence.

8

Figure 3: Divergence estimate in DCGAN trained over CelebA samples  (top-left) JS-divergence
in DCGAN regularized with batch normalization  (top-right) hybrid dJSD W1 in DCGAN with a
1-Lipschitz spectrally-normalized discriminator  (bottom-left) hybrid dJSD W1 in DCGAN with a
1-Lipschitz discriminator regularized via the gradient penalty  (bottom-right) hybrid dJSD W2 in
DCGAN with its discriminator being adversarially-trained using WRM.

Analyzing the generalization performance in GANs has been another problem of interest in the
machine learning literature. [10] proves generalization guarantee results for GANs in terms of the
F-based distance measures. [27] uses an elegant approach based on birthday paradox to empirically
study the generalizibility of a GAN’s learned models. [28] develops a quantitative approach for
examining diversity and generalization for a GAN’s learned distribution. [29] studies approximation-
generalization trade-offs in GANs by analyzing the discriminative power in F-based distances.
Regarding the optimization aspects of GANs  [30  31] propose duality-based methods for improving
optimization performance in training deep generative models. [32] suggests convolving the data
distribution with a Gassian distribution for regularizing the learning problem in f-GANs. Moreover 
several other works including [33  34  35  9  36] explore the optimization and stability properties of
GANs. We also note that the same convex analysis approach used in this paper for studying GANs
has also provided several powerful frameworks for analyzing other supervised and unsupervised
learning problems [37  38  39  40  41].

Acknowledgments: We are grateful for support under a Stanford Graduate Fellowship  the National
Science Foundation grant under CCF-1563098  and the Center for Science of Information (CSoI)  an
NSF Science and Technology Center under grant agreement CCF-0939370.

References

[1] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[2] Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks.

arXiv:1701.00160  2016.

arXiv preprint

9

[3] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  pages 271–279  2016.

[4] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial

networks. International Conference on Machine Learning  2017.

[5] Gintare Karolina Dziugaite  Daniel M Roy  and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906 
2015.

[6] Yujia Li  Kevin Swersky  and Rich Zemel. Generative moment matching networks. In Interna-

tional Conference on Machine Learning  pages 1718–1727  2015.

[7] Chun-Liang Li  Wei-Cheng Chang  Yu Cheng  Yiming Yang  and Barnabás Póczos. Mmd
gan: Towards deeper understanding of moment matching network. In Advances in Neural
Information Processing Systems  pages 2200–2210  2017.

[8] Junbo Zhao  Michael Mathieu  and Yann LeCun. Energy-based generative adversarial network.

arXiv preprint arXiv:1609.03126  2016.

[9] Soheil Feizi  Farzan Farnia  Tony Ginart  and David Tse. Understanding gans: the lqg setting.

arXiv preprint arXiv:1710.10793  2017.

[10] Sanjeev Arora  Rong Ge  Yingyu Liang  Tengyu Ma  and Yi Zhang. Generalization and

equilibrium in generative adversarial nets (gans). arXiv preprint arXiv:1703.00573  2017.

[11] Shuang Liu  Olivier Bousquet  and Kamalika Chaudhuri. Approximation and convergence
properties of generative adversarial learning. In Advances in Neural Information Processing
Systems  pages 5551–5559  2017.

[12] Takeru Miyato  Toshiki Kataoka  Masanori Koyama  and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. International Conference on Learning Representations 
2018.

[13] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron C Courville.
Improved training of wasserstein gans. In Advances in Neural Information Processing Systems 
pages 5769–5779  2017.

[14] Imre Csiszár  Paul C Shields  et al. Information theory and statistics: A tutorial. Foundations

and Trends R(cid:13) in Communications and Information Theory  1(4):417–528  2004.

[15] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business

Media  2008.

[16] Yasemin Altun and Alex Smola. Unifying divergence minimization and statistical inference
via convex duality. In International Conference on Computational Learning Theory  pages
139–153  2006.

[17] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[18] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-

sarial networks. arXiv preprint arXiv:1701.04862  2017.

[19] Aman Sinha  Hongseok Namkoong  and John Duchi. Certiﬁable distributional robustness with
principled adversarial training. In International Conference on Learning Representations  2018.

[20] Ziwei Liu  Ping Luo  Xiaogang Wang  and Xiaoou Tang. Deep learning face attributes in the

wild. In Proceedings of International Conference on Computer Vision (ICCV)  2015.

[21] Fisher Yu  Ari Seff  Yinda Zhang  Shuran Song  Thomas Funkhouser  and Jianxiong Xiao. Lsun:
Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv
preprint arXiv:1506.03365  2015.

10

[22] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.com/exdb/mnist/ 

1998.

[23] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[25] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training

by reducing internal covariate shift. arXiv preprint arXiv:1502.03167  2015.

[26] Richard Nock  Zac Cranko  Aditya K Menon  Lizhen Qu  and Robert C Williamson. f-gans
in an information geometric nutshell. In Advances in Neural Information Processing Systems 
pages 456–464  2017.

[27] Sanjeev Arora and Yi Zhang. Do gans actually learn the distribution? an empirical study. arXiv

preprint arXiv:1706.08224  2017.

[28] Shibani Santurkar  Ludwig Schmidt  and Aleksander Madry. A classiﬁcation-based perspective

on gan distributions. arXiv preprint arXiv:1711.00970  2017.

[29] Pengchuan Zhang  Qiang Liu  Dengyong Zhou  Tao Xu  and Xiaodong He. On the
discrimination-generalization tradeoff in GANs. International Conference on Learning Repre-
sentations  2018.

[30] Xu Chen  Jiang Wang  and Hao Ge. Training generative adversarial networks via primal-dual
subgradient methods: a Lagrangian perspective on GAN. In International Conference on
Learning Representations  2018.

[31] Shengjia Zhao  Jiaming Song  and Stefano Ermon. The information autoencoding family: A
lagrangian perspective on latent variable generative models. arXiv preprint arXiv:1806.06514 
2018.

[32] Kevin Roth  Aurelien Lucchi  Sebastian Nowozin  and Thomas Hofmann. Stabilizing training
of generative adversarial networks through regularization. In Advances in Neural Information
Processing Systems  pages 2015–2025  2017.

[33] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In

Advances in Neural Information Processing Systems  pages 5591–5600  2017.

[34] Lars Mescheder  Sebastian Nowozin  and Andreas Geiger. The numerics of gans. In Advances

in Neural Information Processing Systems  pages 1823–1833  2017.

[35] Constantinos Daskalakis  Andrew Ilyas  Vasilis Syrgkanis  and Haoyang Zeng. Training gans

with optimism. arXiv preprint arXiv:1711.00141  2017.

[36] Maziar Sanjabi  Jimmy Ba  Meisam Razaviyayn  and Jason D Lee. Solving approximate

wasserstein gans to stationarity. arXiv preprint arXiv:1802.08249  2018.

[37] Miroslav Dudík  Steven J Phillips  and Robert E Schapire. Maximum entropy density estimation
with generalized regularization and an application to species distribution modeling. Journal of
Machine Learning Research  8(Jun):1217–1260  2007.

[38] Meisam Razaviyayn  Farzan Farnia  and David Tse. Discrete rényi classiﬁers. In Advances in

Neural Information Processing Systems  pages 3276–3284  2015.

[39] Farzan Farnia and David Tse. A minimax approach to supervised learning. In Advances in

Neural Information Processing Systems  pages 4240–4248  2016.

[40] Rizal Fathony  Anqi Liu  Kaiser Asif  and Brian Ziebart. Adversarial multiclass classiﬁcation:
A risk minimization perspective. In Advances in Neural Information Processing Systems  pages
559–567  2016.

[41] Rizal Fathony  Mohammad Ali Bashiri  and Brian Ziebart. Adversarial surrogate losses for
ordinal regression. In Advances in Neural Information Processing Systems  pages 563–573 
2017.

11

,Anastasiia Mishchuk
Dmytro Mishkin
Filip Radenovic
Jiri Matas
Farzan Farnia
David Tse