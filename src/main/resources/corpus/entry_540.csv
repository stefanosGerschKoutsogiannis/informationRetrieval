2016,An urn model for majority voting in classification ensembles,In this work we analyze the class prediction of parallel randomized ensembles by majority voting as an urn model. For a given test instance  the ensemble can be viewed as an urn of marbles of different colors. A marble represents an individual classifier. Its color represents the class label prediction of the corresponding classifier. The sequential querying of classifiers in the ensemble can be seen as draws without replacement from the urn. An analysis of this classical urn model based on the hypergeometric distribution makes it possible to estimate the confidence on the outcome of majority voting when only a fraction of the individual predictions is known. These estimates can be used to speed up the prediction by the ensemble. Specifically  the aggregation of votes can be halted when the confidence in the final prediction is sufficiently high. If one assumes a uniform prior for the distribution of possible votes the analysis is shown to be equivalent to a previous one based on Dirichlet distributions. The advantage of the current approach is that prior knowledge on the possible vote outcomes can be readily incorporated in a Bayesian framework. We show how incorporating this type of problem-specific knowledge into the statistical analysis of majority voting leads to faster classification by the ensemble and allows us to estimate the expected average speed-up beforehand.,An urn model for majority voting

in classiﬁcation ensembles

Victor Soto

Computer Science Department

Columbia University
New York  NY  USA

vsoto@cs.columbia.edu

{gonzalo.martinez alberto.suarez}@uam.es

Alberto Suárez and Gonzalo Martínez-Muñoz

Computer Science Department

Universidad Autónoma de Madrid

Madrid  Spain

Abstract

In this work we analyze the class prediction of parallel randomized ensembles by
majority voting as an urn model. For a given test instance  the ensemble can be
viewed as an urn of marbles of different colors. A marble represents an individual
classiﬁer.
Its color represents the class label prediction of the corresponding
classiﬁer. The sequential querying of classiﬁers in the ensemble can be seen
as draws without replacement from the urn. An analysis of this classical urn
model based on the hypergeometric distribution makes it possible to estimate
the conﬁdence on the outcome of majority voting when only a fraction of the
individual predictions is known. These estimates can be used to speed up the
prediction by the ensemble. Speciﬁcally  the aggregation of votes can be halted
when the conﬁdence in the ﬁnal prediction is sufﬁciently high. If one assumes
a uniform prior for the distribution of possible votes the analysis is shown to be
equivalent to a previous one based on Dirichlet distributions. The advantage of
the current approach is that prior knowledge on the possible vote outcomes can be
readily incorporated in a Bayesian framework. We show how incorporating this
type of problem-speciﬁc knowledge into the statistical analysis of majority voting
leads to faster classiﬁcation by the ensemble and allows us to estimate the expected
average speed-up beforehand.

1

Introduction

Combining the outputs of multiple predictors is in many cases of interest a successful strategy to
improve the capabilities of artiﬁcial intelligence systems  ranging from agent architectures [19]  to
committee learning [13  15  8  9]. A common approach is to build a collection of individual subsys-
tems and then integrate their outputs into a ﬁnal decision by means of a voting process. Speciﬁcally 
in the machine learning literature  there is extensive empirical evidence on the improvements in
generalization capacity that can be obtained using ensembles of learners [7  11]. However  one of the
drawbacks of these types of systems is the linear memory and time costs incurred in the computation
of the ﬁnal ensemble prediction by combination of the individual predictions. There are various
strategies that alleviate these shortcomings. These techniques are grouped into static (or off-line)
and dynamic (or online). In static pruning techniques  only a subset of complementary predictors
from the original ensemble is kept [16  21  6]. By contrast  in dynamic pruning  the whole ensemble
is retained. The prediction of the class label of a particular instance is accelerated by halting the
sequential querying process when it is unlikely that the remaining (unknown) votes would change
the output prediction [10  20  14  12  2  3  17]. These techniques are online in the sense that  as new
individual predictions become known  the algorithm dynamically updated the estimated probability
of having a stable prediction; i.d. a prediction that coincides with that by the complete ensemble.
This is the basis of the Statistical Instance-Based Algorithm (SIBA) proposed in [14]. In a similar

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

approach  albeit with a different objective  Reyzin proposes to randomly sample hypotheses from
the original AdaBoost ensemble. The goal is to minimize the number of features that are used for
prediction  with a limited loss of accuracy [18]. This feature-efﬁcient prediction is beneﬁcial when
access to the features of a new instance at test time is costly (e.g.  in some medical problems). A
different approach is followed in [3]. In this work  a policy is learned to decide which classiﬁers
should be queried and which discarded in the prediction of the class label of a given instance.
The dynamic ensemble pruning method proposed in this work is closely related to SIBA [14]. In
SIBA  the members of a committee are queried sequentially. At each step in the querying process  the
votes recorded are used to estimate the probability that the majority decision of the classiﬁers queried
up to that moment coincides with the complete ensemble. If this probability exceeds a speciﬁed
conﬁdence level  α  the voting process is halted. To compute this estimate  the probability that a
single predictor outputs a given decision for the particular instance considered is modeled as a random
variable. Starting from a uniform prior  Bayes’ theorem is used to update the distribution of this
variable with the information provided by the actual votes  as they become known. In most of the
problems analyzed in [14]  the assumption that the prior is uniform leads to conservative estimates
of the conﬁdence on the stability of the predictions when only a fraction of the classiﬁers have
been queried. Analyzing the results of those experiments  it is apparent that the actual disagreement
percentages between the dynamic decision output and the decision made by the complete committee
are signiﬁcantly lower than the speciﬁed target α. As a consequence  more queries are made than the
ones that are actually needed.
The present work has two objectives. First  we propose an intuitive mathematical modeling of
the voting process in ensembles of classiﬁers based on the hypergeometric distribution. Under the
assumption that the distribution of possible vote outcomes is uniform  we prove that this derivation is
equivalent to the one presented in [14]. However  the vote distribution is  in general  not uniform.
Its shape depends on the classiﬁcation task considered and on the base learning algorithm used to
generate the predictors. Second  to take into account this dependence  we propose to approximate
this distribution using a non-parametric prior. The use of this problem-speciﬁc prior knowledge
leads to more accurate estimations of the disagreement rates between the dynamic sub-committee
prediction and the complete committee  which are closer to the speciﬁed target α. In this manner 
faster classiﬁcation can be achieved with minimal loss of accuracy. In addition  the use of priors allow
us to estimate quite precisely the expected average number of trees that would be necessary to query.

2 Modeling ensemble voting processes as a classical urn problem

Consider the following process modeled as a classical urn model. Let us suppose we have marbles
of l different colors in an urn. The number of marbles of color yk in the urn is Tk  with k = 1 . . . l.
k=1 Tk. The contents of the urn can therefore be
described by vector T = (cid:104)T1  T2 . . . Tl(cid:105). Assume that t < T marbles are extracted from the urn
without replacement. This extraction process can be characterized by vector t = (cid:104)t1  t2 . . . tl(cid:105) where
k=1 tk. The probability of extracting a
color distribution of marbles t  given the initial color distribution of the urn T is described by the
multivariate hypergeometric distribution

The total number of marbles in the urn is T =(cid:80)l
tk is the number of marbles of color yk extracted  with t =(cid:80)l
(cid:81)l
(cid:0)T

(cid:1) . . .(cid:0)Tl
(cid:0)T
(cid:1)

.

(1)

P(t|T) =

(cid:0)Ti
(cid:1)

ti

(cid:0)T1

(cid:1)

(cid:1)

tl

=

t1

t

i=1

t

Consider the case in which the total number of marbles in the urn  T   is known but that the color
distribution  T  is unknown. In this case  the color distribution of the extracted marbles  t  can be
used to estimate the content of the urn applying Bayes Theorem

(cid:0)T1

(cid:1)P(T)
(cid:1) . . .(cid:0)Tl
(cid:1) . . .(cid:0)T ∗
(cid:0)T ∗
(cid:1)P(T∗)

tl

l
tl

(2)

P(T|t) =

P(t|T)P(T)

P(t)

=

(cid:80)

P(t|T)P(T)

P(t|T∗)P(T∗)

(cid:80)
t1
i ≥ ti ∀i and(cid:80)l
T∗∈Ωt
i=1 T ∗

=

1
t1
i = T .

T∗∈Ωt

where Ωt is the set of vectors T∗  such that T ∗
This problem is equivalent to the voting process in an ensemble of classiﬁers: Suppose we want
to predict the class label of an instance by combining the individual predictions of the ensemble
classiﬁers (marbles). Assuming that the individual predictions are deterministic  the class (color) that

2

each classiﬁer (marble) would output if queried is ﬁxed  but unknown before the query. Therefore 
for each instance considered we have a different "bag of colored marbles" with an unknown class
distribution. After a partial count of votes of the ensemble is known  Eq. 2 provides an estimate of the
distribution of votes for the complete ensemble. This estimate can be used to compute the probability
that the decision obtained using only a partial tally of votes  t  of size t < T and by the ﬁnal decision
using all T votes  coincide

(cid:88)

T∈Tt

(cid:80)

(cid:0)T1

(cid:1)P(T)
(cid:1) . . .(cid:0)Tl
(cid:1) . . .(cid:0)T ∗
(cid:0)T ∗
(cid:1)P(T∗)

tl

1
t1

l
tl

t1
T∗∈Ωt

P∗(t  T ) =

 

(3)

i=1 Ti = T .

where Tt is the set of vectors of votes for the complete ensemble T = {T1  T2 . . . Tl} such that
the class predicted by the subensemble of size t and the class predicted by the complete committee

coincide  with Ti ≥ ti  and(cid:80)l

If P∗(t  T ) = 1  then the classiﬁcation given by the partial ensemble and the full ensemble coincide.
This case happens when the difference between the number of votes for the ﬁrst and second class
in t is greater than the remaining votes in the urn. In such case  the voting process can be halted
with full conﬁdence that the decision of the partial ensemble will not change when the predictions of
the remaining classiﬁers are considered. In addition  if it is acceptable that  with a small probability
1 − α  the prediction of the partially polled ensemble and that of the complete ensemble disagree 
then the voting process can be stopped when the P∗(t  T ) exceeds the speciﬁed conﬁdence level α.
The ﬁnal classiﬁcation would be given as the combined decisions of the classiﬁers that have been
polled up to that point only.

2.1 Uniform prior
Assuming a uniform prior for the distribution of possible T vectors P(T) = 1/(cid:107)T(cid:107)  where (cid:107)T(cid:107)
stands for the number of possible T vectors  this derivation is equivalent to the one presented in
[14]. That formulation assumes that the base classiﬁers of the ensemble are independent realizations
from a pool of all possible classiﬁers given the training dataset. Assuming that an unlimited number
of realizations can be performed  the distribution of class votes in the ensemble converges to a
Dirichlet distribution in the limit of inﬁnite ensemble size. Then  assuming a partial tally of t votes 
the probability that the ensemble’s decision will change if the precictions of the remaining T − t
classiﬁers are considered  can be estimated.
In order to prove the equivalence between both formulations  we ﬁrst need to introduce three results 
presented in the theorem and propositions below.
Theorem. Chu-Vandermonde Identity. Let s  t  r ∈ N then

Proposition 1. Upper negation. Let r ∈ C and k ∈ Z  then

(cid:18)s + t
(cid:19)
(cid:19)
(cid:18)r

r

k

r(cid:88)

k=0

=

(cid:19)(cid:18) t
(cid:18)s
(cid:18)k − r − 1

k

r − k

(cid:19)
(cid:19)

k

= (−1)k

(4)

(5)

The previous theorem and proposition are used in the following proposition  which is the key to prove
the equivalence between the two formulations:
Proposition 2. Let n1 and n2 be positive integers such that n1 + n2 = n and n ≤ N. Then

(cid:18) i

N−n2(cid:88)

(cid:19)(cid:18)N − i
(cid:19)

(cid:18)N + 1
(cid:19)
(cid:1) =(cid:0) n
Proof. First the symmetry property of the binomial (i.e. (cid:0)n
(cid:19)(cid:18) N − i
(cid:18) i

(cid:19)(cid:18)N − i
(cid:19)

(cid:18) i

N − n

N−n2(cid:88)

N−n2(cid:88)

indices

n−k

n1

n2

i=n1

=

k

n1

n2

i=n1

=

i − n1

N − i − n2

(cid:19)

(6)

(cid:1)) is used to bring down the

i=n1

3

The upper indices are removed by applying the upper negation property of proposition 1.

Now  the Chu-Vandermonde identity can be applied with r = N − n1 − n2 and k = i − n1

N−n2(cid:88)

(cid:18) i

i − n1

i=n1

N−n2(cid:88)

i=n1

(cid:19)(cid:18) N − i
(cid:18)−n1 − 1

N − i − n2

(cid:19)

=

i=n1

N−n2(cid:88)
(cid:19)
(cid:19)(cid:18) −n2 − 1
(cid:18)−n − 2
(cid:19)

N − i − n2

i − n1

Finally the upper negation is applied again

(cid:18)−n1 − 1

(cid:19)(cid:18) −n2 − 1

(cid:19)

i − n1

N − i − n2

(−1)i−n1 (−1)N−i−n2

(−1)i−n1(−1)N−i−n2 =

(−1)N−n

(cid:18)−n − 2
(cid:19)

N − n

(cid:19)

(cid:18)N + 1

N − n

(−1)N−n =

N − n

t1
T∗∈Ωt

=

(cid:80)
(cid:0)T1
(cid:1) . . .(cid:0)Tl
···(cid:80) ˆTl−1

t1

tl

(cid:1)

Proposition 3 Following the hypergeometric reformulation given by Equation 2 and assuming that
P(T) follows a uniform distribution 1/(cid:107)T(cid:107)  where (cid:107)T(cid:107) stands for the number of possible T vectors
then

(cid:81)l

P(T|t) =

(cid:81)l
(T − t)!
i=1 (Ti − ti)!

i=1 (ti + 1)Ti−ti

(t + l)T−t

where (x)n = x(x + 1) . . . (x + n − 1) is the Pochhammer symbol. This formulation is equivalent
to the one proposed in [14].
Proof. Equation 2 can be simpliﬁed by taking into account the uniform prior P(T) = 1/(cid:107)T(cid:107) as

P(T|t) =

P(t|T)P(T)

The indices of the summation  Ωt  is the set of vectors T such that Ti ≥ ti ∀i and(cid:80)l

1
t1

P(t)

l
tl

(7)

i=1 Ti = T .

They can be changed for l classes to

(cid:0)T1

(cid:1) . . .(cid:0)Tl
(cid:1)
(cid:1) . . .(cid:0)T ∗
(cid:0)T ∗

tl

(cid:1)

P(T|t) =

(cid:80) ˆT1

(cid:80) ˆT2

(cid:0)T ∗

1
t1

(cid:1) . . .(cid:0)T ∗

l
tl

(cid:1)

(8)

l

l becomes ﬁxed once the values of T ∗

k in the summations. Note that the
1 . . . T ∗
l−1
i for i < k as
k = 1  . . .   (l − 1). The summations in the denominator of

i = T . In this sense  the values for ˆTk have a dependency on T ∗
i − ti) 
i=1 (T ∗
(cid:19)

(cid:19)

(cid:19)

(cid:19)(cid:18)T ∗

(cid:18)T ∗

(cid:18)T ∗

ˆTl−1(cid:88)

T ∗
2 =t2

T ∗
1 =t1

Eq. 8 can be rearranged

T ∗
l−1=tl−1
where ˆTk for k = 1  . . .   (l − 1) are the maximum values for T ∗
summation over T ∗

are ﬁxed since(cid:80)l
is unnecessary since the value of T ∗
ˆTk = T − t + tk −(cid:80)k−1
i=1 T ∗
(cid:19) ˆT2(cid:88)
(cid:18)T ∗
ˆT1(cid:88)
ˆTl−1(cid:88)
Proposition 2 (Eq. 6) can be used  together with N = T −(cid:80)l−2
(cid:18)T ∗
T−t+tl−1−(cid:80)l−2
(cid:88)

T ∗
l−1 in closed form

(cid:19)(cid:18)T ∗

(cid:18)T ∗

(cid:18)T ∗

(cid:18)T ∗

ˆT2(cid:88)

ˆT1(cid:88)

T ∗
l−1=tl−1

i=1(T ∗

T ∗
1 =t1

T ∗
1 =t1

T ∗
2 =t2

i −ti)

···

1
t1

1
t1

l
tl

. . .

=

i   to express the summation over

T ∗
2 =t2
i=1 T ∗

T ∗
l−1=tl−1

l−1
tl−1

i − T ∗
l−1

···

2
t2

l
tl

(cid:19)

.

=

i=1 T ∗

(cid:19)
T−(cid:80)l−2
i −tl(cid:88)
T −(cid:80)l−2
(cid:18)
T −(cid:80)l−2

T ∗
l−1=tl−1

i=1 T ∗

i + 1

i=1 T ∗
i − tl−1 − tl

l−1
tl−1

(cid:19)(cid:18)T −(cid:80)l−2
(cid:19)

(cid:18)T −(cid:80)l−2

i=1 T ∗

tl

=

i=1 T ∗
tl−1 + tl + 1

i + 1

 

(cid:19)
(cid:19)

=

T ∗
l−1=tl−1

l−1
tl−1

l
tl

4

where the symmetry property of the binomial has been used in the last step. The subsequent
summations are carried out in the same manner. The summation over T ∗
k requires the application of

i + (l − k − 1)  n1 = tk and n2 =(cid:80)l
i=1 T ∗
(cid:19)
ˆTl−2(cid:88)

(cid:19)(cid:18)T −(cid:80)l−2

(cid:18)T ∗

i + 1

i=1 T ∗
tl−1 + tl + 1

Eq. 6 with N = T −(cid:80)k−1
(cid:18)T ∗
(cid:19)
ˆT1(cid:88)
(cid:0)T1
(cid:1) . . .(cid:0)Tl
(cid:1)
(cid:1) =
(cid:0)T +l−1

P(T|t) =

l−2
tl−2
Employing this result in Eq. 8  one obtains

T ∗
l−2=tl−2

T ∗
1 =t1

1
t1

...

t1

tl

t+l−1

= ··· =

i=k+1 ti + (l − k − 1)
(cid:18)T + l − 1
(cid:19)
(cid:81)l

t + l − 1

T1!

t1!(T1−t1)! . . .

Tl!

tl!(Tl−tl)!

(T +l−1)!

(t+l−1)!(T−t)!

(cid:81)l
(T − t)!
i=1 (Ti − ti)!

=

i=1 (ti + 1)Ti−ti

(t + l)T−t

.

1 + . . . + ˜T n

2.2 Non-uniform prior
The distribution P(T) can be modeled using a non-parametric non-uniform prior. The values of this
prior can be obtained from the training data by some form of validation; e.g.  out-of-bag or cross
validation. Out-of-bag validation is faster because it does not require multiple generations of the
ensemble. Therefore  it will be the validation method used in our implementation of the method. To
compute the out-of-bag error  each training instance  xn  is classiﬁed by the ensemble predictors
that do not have that particular instance in their training set. Let ˜T n = ˜T n
l   be the
number of such classiﬁers  where ˜T n
is the number of out-of-bag votes for class i  where i = 1  . . .   l 
i
assigned to instance xn. The number of votes for each class for an ensemble of size T is estimated as
i ≈ round(T ˜T n
i / ˜T n). To mitigate the inﬂuence of the random ﬂuctuations that appear because
T n
of the ﬁnite size of the training set and to avoid spurious numeric artifacts  the prior is subsequently
smoothed using a sliding window of size 5 over the vote distribution.
As shown in Section 2  the response time of the ensemble can be reduced by using Eq. 3  if we allow
that a small fraction  1 − α  of the predictions given by ensembles of size t and T do not coincide.
Assuming this tolerance  when P∗(t  T ) > α  the voting process can be halted and the ensemble
will output the decision given by the t ≤ T queried classiﬁers. However  the computation of Eq. 3
is costly and should be performed off-line. In the SIBA formulation  a lookup table indexed by the
number of votes of the minority class (for binary problems) and whose values are the minimum
number of votes of the majority class such that P∗(t  T ) > α  is used. Using a precomputed lookup
table to halt the voting process does not entail a signiﬁcant overhead during classiﬁcation: a single
lookup operation in the table is needed for each vote. The consequence of using a uniform prior is
that all classes are considered equivalent. Hence  it is sufﬁcient to compute one lookup table and use
the minority class for indexing.
When prior knowledge is taken into account  the probability P∗(t1 = n  t2 = m  T ) is not necessarily
equal to P∗(t1 = m  t2 = n  T ) for n (cid:54)= m. Therefore  a different lookup table per class will be
necessary. In addition  it is necessary to compute a different set of tables for each dataset. In the
original formulation  the lookup table values depend only on T and α. Therefore  they are independent
of the particular classiﬁcation problem considered. In our case  the prior distribution is estimated
from the training data: Hence  it is problem dependent. However  the querying process is similar to
SIBA. For instance  if we have 1 vote for class 1 and 7 for class 2  one determines whether the value
in position 1 (minority class at this moment) of the lookup table for class 1 is greater or equal to 7. If
it is  the querying process stops. As a side effect  for the experimental comparison  it is necessary to
recompute the lookup tables for each realization of the data. Notwithstanding  in a real setting  these
tables need to be computed only once. This can be done ofﬂine. Therefore  the speed improvements
in the classiﬁcation phase are independent of the size of the training set.
The lookup table and the estimated non-parametric prior can be used to estimate also the average
number of classiﬁers that are expected to be queried during test. This estimation can be made using
Monte Carlo simulation. To this end one would perform the following experiment repeatedly and
compute the average number of queries: extract a random vector T from the prior distribution;
generate a vector of votes of size T that contains exactly Ti votes for class i with i = 1 . . . l; ﬁnally 
query a random permutation of this vector of votes until the process can be halted as given by the
lookup table and keep the number of queries.

5

3 Experiments

In this section we present the results of an extensive empirical evaluation of the dynamical ensemble
pruning method described in the previous section. The experiments are performed in a series of
benchmark classiﬁcation problems from the UCI Repository [1] and synthetic data [4] using Random
Forests [5]. The code is available at: https://github.com/vsoto/majority-ibp-prior.
The protocol for the experiments is as follows: for each problem  100 partitions are created by
10 × 10-fold cross-validation for real datasets and by random sampling in the synthetic datasets. All
the classiﬁcation tasks considered are binary  except for New-thyroid  Waveform and Wine  which have
three classes. For each partition  the following steps are carried out: (i) a Random Forest ensemble
of size T = 101 is built; (ii) we compute the generalization error rate of the complete ensemble in
the test set and record the mean number of trees that are queried to determine the ﬁnal prediction.
Note that this number need not be T : the voting process can be halted when the remaining votes (i.e.
the predictions of classiﬁers that have not been queried up to that point) cannot modify the partial
ensemble decision. This is the case when the number of remaining votes is below the difference
between the majority class and the second most voted class; (iii) The SIBA algorithm [14] is applied
to dynamically select the number of classiﬁers that are needed for each instance in the test set to
achieve a level of conﬁdence in the prediction above α = 0.99. We use SIBA as the benchmark for
comparison since in previous studies it has been shown to provide the best overall results  especially
for T < 500 [2]; (iv) The process is repeated using the proposed method with non-uniform priors
for the class vote distribution  with the same conﬁdence threshold  α = 0.99. The prior distribution
P(T) is estimated in the training set using out-of-bag data. This prior is also used to estimate the
expected number of trees to be queried in the testing phase. In addition  for steps (iii) and (iv) we
compute the test error rate  the average number of queried trees  and the disagreement rates between
the predictions of the partially queried ensembles and the complete ones.

Table 1: Error rates (left) and disagreement % (right). The statistical signiﬁcant differences  using
paired t-tests at a signiﬁcance level α = 0.05  are highlighted in boldface.

RF

Problem
13.00±3.7
Australian
3.22±2.1
Breast
24.34±4.2
Diabetes
Echocardiogram 22.18±14.3
23.43±3.5
German
18.30±6.9
Heart
15.47±5.6
Horse-colic
6.44±4.1
Ionosphere
6.33±8.9
Labor
27.10±6.7
Liver
0.00±0.0
Mushroom
4.29±4.0
New-thyroid
7.60±1.3
Ringnorm
16.25±8.7
Sonar
4.59±1.5
Spam
17.85±1.1
Threenorm
1.05±1.1
Tic-tac-toe
4.66±0.6
Twonorm
4.05±2.9
Votes
17.30±0.9
Waveform
1.69±2.8
Wine

Error rates
SIBA
13.09±3.7
3.23±2.1
24.25±4.1
22.05±14.7
23.65±3.3
18.37±7.0
15.44±5.4
6.44±4.1
6.17±8.8
27.09±7.0
0.00±0.0
4.38±4.0
7.72±1.2
16.45±8.7
4.63±1.5
18.04±1.1
1.16±1.1
4.77±0.6
4.12±2.9
17.36±0.8
1.74±2.8

HYPER
13.25±3.8
3.76±2.3
24.23±4.0
22.18±14.1
23.62±3.3
18.37±7.2
15.44±5.4
6.52±3.9
6.43±9.1
27.01±6.9
0.08±0.2
4.66±4.2
7.82±1.2
16.45±8.8
4.86±1.4
17.97±1.1
1.72±1.5
4.90±0.6
4.30±2.9
17.45±0.8
2.30±3.5

Disagreement %
SIBA
HYPER
0.3±0.6
0.9±1.1
1.0±1.1
0.1±0.4
0.8±1.0
0.6±0.9
0.7±3.1
1.4±4.6
0.8±0.8
0.8±0.9
0.8±1.8
1.0±2.1
0.7±1.3
0.4±0.9
0.1±0.6
0.7±1.3
0.2±1.7
1.2±4.5
1.0±1.7
0.9±1.5
0.1±0.2
0.0±0.0
0.7±2.0
0.1±0.7
0.5±0.2
0.8±0.3
0.9±2.0
0.8±1.9
0.1±0.2
0.7±0.4
0.8±0.2
1.0±0.2
0.1±0.4
0.7±1.0
0.4±0.1
0.7±0.2
0.1±0.4
1.0±1.8
1.0±0.3
0.6±0.1
0.1±0.6
1.1±2.5

In Table 1  we compare the error rates of Random Forest (RF) and of the dynamically pruned
ensembles using the halting rule derived from assuming uniform priors (SIBA) and using non-
uniform priors (HYPER)  and the disagreement rates. The values displayed are averages over 100
realizations of the datasets The standard deviation is given after the ± symbol.

6

Figure 1: Vote distribution  P(T)  and disagreement rates for Sonar (left) and Votes (right)

From Table 1  one observes that the mean error rates of the pruned ensembles using SIBA and HYPER
are only slightly worse than the rates obtained by the complete ensemble (RF). These differences
should be expected since we are allowing a small disagreement of 1 − α = 1% between the decisions
of the partial and the complete ensemble. In any case  the differences in generalization error can
be made arbitrarily small by increasing α. By design  the disagreement rates are expected to be
below  but close to 1%. From Table 1  one observes that the disagreement % of the proposed method
(HYPER) are closer to the speciﬁed threshold (1 − α = 1%) than those of SIBA  except for Liver 
Sonar and Threenorm  where the differences are small. In these problems (and in general in the
problems where SIBA obtains disagreement rates closer to 1 − α)  the distribution of T is closer to a
uniform distribution (see Figure 1  left histogram). In consequence  the assumption of uniform prior
taken by SIBA is closer to the real one. However  when P(T) differs from the uniform distribution
(see for instance Votes in Figure 1 right histogram) the results of SIBA are rather different from the
expected disagreement rates.

Table 2: Number of queried trees and speed-up rate with respect to the full ensemble of 101 trees.
The statistical signiﬁcant differences between SIBA and HYPER  using paired t-tests at a signiﬁcance
level α = 0.05  are highlighted in boldface.

# of trees

RF∗

Problem
62.2±1.4
Australian
54.2±0.9
Breast
68.8±1.8
Diabetes
Echocardiogram 68.0±4.6
71.8±1.3
German
67.2±2.5
Heart
66.2±2.1
Horse-colic
57.9±1.5
Ionosphere
61.6±4.0
Labor
74.5±2.3
Liver
51.0±0.0
Mushroom
55.2±1.8
New-thyroid
68.6±0.8
Ringnorm
73.9±3.0
Sonar
57.1±0.3
Spam
76.6±0.5
Threenorm
60.7±0.9
Tic-tac-toe
67.2±0.2
Twonorm
54.5±1.2
Votes
72.3±0.7
Waveform
57.3±2.1
Wine

SIBA
16.1±2.1
8.9±1.4
24.9±3.2
22.6±8.2
28.4±2.8
22.5±4.2
20.2±3.5
11.9±2.3
14.1±6.0
31.8±4.5
6.0±0.0
10.7±2.6
22.9±1.1
32.1±6.6
11.1±0.5
34.8±1.0
12.8±1.4
21.0±0.5
8.8±1.8
29.3±1.1
11.4±2.7

HYPER MC Estim RF∗
12.8±2.3
1.6
4.0±1.0
1.9
24.0±3.2
1.5
20.0±8.0
1.5
27.7±2.9
1.4
20.9±4.2
1.5
17.5±3.7
1.5
7.8±2.1
1.7
9.7±5.3
1.6
31.7±4.5
1.4
1.0±0.0
2.0
6.0±2.3
1.8
20.4±1.5
1.5
32.6±6.8
1.4
7.2±0.6
1.8
35.8±1.6
1.3
7.8±1.2
1.7
18.4±0.9
1.5
4.1±1.4
1.9
27.8±1.7
1.4
5.8±1.8
1.8

12.9±0.9
4.0±0.4
23.8±1.1
21.6±3.2
30.1±1.0
20.7±1.7
18.6±1.5
7.8±0.6
10.2±2.0
31.6±2.0
1.0±0.0
6.2±1.4
19.7±2.3
31.8±2.4
7.1±0.5
33.4±2.5
8.6±0.7
18.8±1.7
4.0±0.7
28.6±2.8
6.7±1.4

Speed-up rate

SIBA HYPER
6.3
11.3
4.1
4.5
3.6
4.5
5.0
8.5
7.2
3.2
16.8
9.4
4.4
3.1
9.1
2.9
7.9
4.8
11.5
3.4
8.9

7.9
25.3
4.2
5.1
3.6
4.8
5.8
12.9
10.4
3.2
101.0
16.8
5.0
3.1
14.0
2.8
12.9
5.5
24.6
3.6
17.5

In order to analyze this aspect in more detail  we have computed the disagreement rates for different
values of alpha (α = 0.999  0.995  0.99  0.95). In Figure 1 the relation between the target 1 − α and
the actual disagreement rate is presented. A diagonal solid line marks the expected upper limit for
the disagreement. The results for SIBA  HYPER and for the case of using a ﬁxed number of trees
for all instances (FIXED) (and equal to the average number of trees used by HYPER in those tasks)

7

010203040506070809010000.0050.010.0150.020.025t1P(t1) 0 0.02 0.04 0.06 0.08 0.1 0.12 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05disagreement1-alpha sonar HYPERSIBAFIXED010203040506070809010000.050.10.150.20.250.3t1P(t1) 0 0.01 0.02 0.03 0.04 0.05 0.06 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05disagreement1-alpha votes HYPERSIBAFIXEDare presented in these plots. This last case (FIXED) can be seen as a stochastic approximation to
the prediction of the whole ensemble. From these plots  we observe that the results for HYPER are
very close to the expected disagreement rates for cases in which the prior is approximately uniform
(Sonar)  and for cases in which the prior is non-uniform (Votes). As expected  the results of SIBA are
close to the target only for the case of approximately uniform prior (Sonar). Finally  when a stochastic
approximation is used (FIXED) the disagreement rates are clearly above the target threshold given by
α. From these results we conclude that the proposed model provides a more accurate description
of the voting process used to compute the prediction of the ensemble. This means that taking into
account the prior distribution of possible vote outcomes  P(T)  is important to obtain disagreement
rates that are closer to the threshold established.
Finally  in Table 2  we present the average number of trees used by Random Forest (RF∗)  the SIBA
method the proposed method using non-parametric priors (HYPER)  and the expected average of
the number of trees to be queried in HYPER using Monte Carlo sampling (MC Estim). Note that
the number of trees used by RF∗ is not necessary T = 101: the voting process is halted when the
remaining (unknown) predictions cannot alter the decision of the ensemble. The number of trees
given in RF∗ is the same as the trees that SIBA or HYPER would use when α = 100%. Finally  the
last three columns of Table 2 display the speed-up rate of the partial ensembles with respect to the
full ensemble of size T = 101. From this table it is clear that HYPER reduces the number of queried
classiﬁers with respect to SIBA in most of the tasks investigated. In addition  using only training data 
the Monte Carlo estimations of the average number of trees are very precise. The largest average
difference between this estimation and HYPER is 2.4 trees for German and Threenorm. The speed-up
rate of HYPER with respect to the full ensemble is remarkable: from 2.8 times faster for Threenorm
to 101 times faster in Mushroom. This dataset can be used to illustrate the beneﬁts of using the prior
distribution. For this problem  most classiﬁers agree in their predictions. HYPER takes advantage of
this prior knowledge and queries only one classiﬁer to cast the ﬁnal decision. In this problem  the
chances that the prediction of a single classiﬁer  and the prediction of the complete ensemble are
different  are below 1%. Similar behavior (but not as extreme) is observed in Breast and Votes.

4 Conclusions

In this work  we present an intuitive  rigorous mathematical description of the voting process in an
ensemble of classiﬁers: For a given an instance  the process is equivalent to extracting marbles (the
individual classiﬁers)  without replacement  from a bag that contains a known number of marbles  but
whose color (class label prediction) distribution is unknown. In addition  we show that for the speciﬁc
case of a uniform prior distribution of class votes this process is equivalent to the one developed in
[14]. In the current description  which does not assume a uniform distribution prior for the class
votes  the hypergeometric distribution plays a central role.
The results of this statistical description are then used to design a dynamic ensemble pruning method 
with the goal of speeding up predictions in the test phase. For a given instance  it is possible to
compute the probability that the the partial decision made on the basis of the known votes (i.e.  the
class label predictions of the subset of classiﬁers that have been queried) and the ﬁnal ensemble
decision coincide. If this probability is above a speciﬁed threshold  sufﬁciently close to 1  a reliable
estimate of the class label that the complete ensemble would predict can be made on the basis of
the known votes. The effectiveness of this dynamic ensemble pruning method is illustrated using
random forests. The prior distribution of class votes is estimated using out-of-bag data. As a result of
incorporating this problem-speciﬁc knowledge in the statistical analysis of the voting process  the
differences between the predictions of the dynamically pruned ensemble and the complete ensemble
are closer to the speciﬁed threshold than when a uniform distribution is assumed  as in SIBA [14]. In
the empirical evaluation performed  this dynamic ensemble pruning algorithm consistently yields
improvements of classiﬁcation speed over SIBA without a signiﬁcant deterioration of accuracy.
Finally  the statistical model proposed is used to provide an accurate estimate of the average number
of individual classiﬁer predictions that are needed to reach a stable ensemble prediction.

Acknowledgments

The authors acknowledge ﬁnancial support from the Comunidad de Madrid (project CASI-CAM-
CM S2013/ICE-2845)  and from the Spanish Ministerio de Economía y Competitividad (projects
TIN2013-42351-P and TIN2015-70308-REDT).

8

References
[1] A. Asuncion and D. Newman. UCI machine learning repository  2007.

[2] J. Basilico  M. Munson  T. Kolda  K. Dixon  and W. Kegelmeyer. Comet: A recipe for learning and using
large ensembles on massive data. In Proceedings - IEEE International Conference on Data Mining  ICDM 
pages 41–50  2011.

[3] D. Benbouzid  R. Busa-Fekete  and B. Kégl. Fast classiﬁcation using sparse decision dags. In Proceedings
of the 29th International Conference on Machine Learning  ICML 2012  volume 1  pages 951–958  2012.

[4] L. Breiman. Bias  variance  and arcing classiﬁers. Technical Report 460  Statistics Department  University

of California  1996.

[5] L. Breiman. Random forests. Machine Learning  45(1):5–32  2001.

[6] R. Caruana and A. Niculescu-Mizil. Ensemble selection from libraries of models. In Proc. of the 21st

International Conference on Machine Learning (ICML’04)  2004.

[7] R. Caruana and A. Niculescu-Mizil. An empirical comparison of supervised learning algorithms. In Proc.
of the 23rd International Conference on Machine Learning  pages 161–168  New York  NY  USA  2006.
ACM Press.

[8] T. G. Dietterich. Ensemble methods in machine learning. In Multiple Classiﬁer Systems: First International

Workshop  pages 1–15  2000.

[9] T. G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision

trees: Bagging  boosting  and randomization. Machine Learning  40(2):139–157  2000.

[10] W. Fan  F. Chu  H. Wang  and P. S. Yu. Pruning and dynamic scheduling of cost-sensitive ensembles. In
Proc. of the 18th National Conference on Artiﬁcial Intelligence  pages 146–151. American Association for
Artiﬁcial Intelligence  2002.

[11] M. Fernández-Delgado  E. Cernadas  S. Barro  and D. Amorim. Do we need hundreds of classiﬁers to
solve real world classiﬁcation problems? Journal of Machine Learning Research  15:3133–3181  2014.

[12] T. Gao and D. Koller. Active classiﬁcation based on value of classiﬁer. In NIPS  2011.

[13] L. K. Hansen and P. Salamon. Neural network ensembles. IEEE Transactions on Pattern Analysis and

Machine Intelligence  12:993–1001  1990.

[14] D. Hernández-Lobato  G. Martínez-Muñoz  and A. Suárez. Statistical instance-based pruning in ensembles
of independent classiﬁers. IEEE Transactions on Pattern Analysis and Machine Intelligence  31(2):364–369 
2009.

[15] T. K. Ho  J. J. Hull  and S. N. Srihari. Decision combination in multiple classiﬁer systems.

Transactions on Pattern Analysis Machine Intelligence  16(1):66–75  1994.

IEEE

[16] D. D. Margineantu and T. G. Dietterich. Pruning adaptive boosting. In Proc. of the 14th International

Conference on Machine Learning  pages 211–218. Morgan Kaufmann  1997.

[17] F. Markatopoulou  G. Tsoumakas  and I. Vlahavas. Dynamic ensemble pruning based on multi-label

classiﬁcation. Neurocomputing  150(PB):501–512  2015.

[18] L. Reyzin. Boosting on a budget: Sampling for feature-efﬁcient prediction. In L. Getoor and T. Scheffer 
editors  Proceedings of the 28th International Conference on Machine Learning (ICML-11)  ICML ’11 
pages 529–536  New York  NY  USA  June 2011. ACM.

[19] R. S. Sutton  J. Modayil  M. Delp  T. Degris  P. M. Pilarski  A. White  and D. Precup. Horde: A scalable
real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In S. Tumer 
Yolum and Stone  editors  Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS
2011)  pages 761–768  Taipei  Taiwan  2011.

[20] H. Wang  W. Fan  P. S. Yu  and J. Han. Mining concept-drifting data streams using ensemble classiﬁers. In
KDD ’03: Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and
data mining  pages 226–235  New York  NY  USA  2003. ACM Press.

[21] Y. Zhang  S. Burer  and W. N. Street. Ensemble pruning via semi-deﬁnite programming. Journal of

Machine Learning Research  7:1315–1338  2006.

9

,Victor Soto
Alberto Suárez
Gonzalo Martinez-Muñoz