2016,A Constant-Factor Bi-Criteria Approximation Guarantee for k-means++,This paper studies the $k$-means++ algorithm for clustering as well as the class of $D^\ell$ sampling algorithms to which $k$-means++ belongs.  It is shown that for any constant factor $\beta > 1$  selecting $\beta k$ cluster centers by $D^\ell$ sampling yields a constant-factor approximation to the optimal clustering with $k$ centers  in expectation and without conditions on the dataset.  This result extends the previously known $O(\log k)$ guarantee for the case $\beta = 1$ to the constant-factor bi-criteria regime.  It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.,A Constant-Factor Bi-Criteria Approximation

Guarantee for k-means++

Dennis Wei
IBM Research

Yorktown Heights  NY 10598  USA

dwei@us.ibm.com

Abstract

This paper studies the k-means++ algorithm for clustering as well as the class of D(cid:96)
sampling algorithms to which k-means++ belongs. It is shown that for any constant
factor β > 1  selecting βk cluster centers by D(cid:96) sampling yields a constant-factor
approximation to the optimal clustering with k centers  in expectation and without
conditions on the dataset. This result extends the previously known O(log k)
guarantee for the case β = 1 to the constant-factor bi-criteria regime. It also
improves upon an existing constant-factor bi-criteria result that holds only with
constant probability.

1

Introduction

The k-means problem and its variants constitute one of the most popular paradigms for clustering
[15]. Given a set of n data points  the task is to group them into k clusters  each deﬁned by a cluster
center  such that the sum of distances from points to cluster centers (raised to a power (cid:96)) is minimized.
Optimal clustering in this sense is known to be NP-hard [11  3  20  6]. In practice  the most widely
used algorithm remains Lloyd’s [19] (often referred to as the k-means algorithm)  which alternates
between updating centers given cluster assignments and re-assigning points to clusters.
In this paper  we study an enhancement to Lloyd’s algorithm known as k-means++ [4] and the more
general class of D(cid:96) sampling algorithms to which k-means++ belongs. These algorithms select
cluster centers randomly from the given data points with probabilities proportional to their current
costs. The clustering can then be reﬁned using Lloyd’s algorithm. D(cid:96) sampling is attractive for
two reasons: First  it is guaranteed to yield an expected O(log k) approximation to the optimal
clustering with k centers [4]. Second  it is as simple as Lloyd’s algorithm  both conceptually as well
as computationally with O(nkd) running time in d dimensions.
The particular focus of this paper is on the setting where an optimal k-clustering remains the
benchmark but more than k cluster centers can be sampled to improve the approximation. Speciﬁcally 
it is shown that for any constant factor β > 1  if βk centers are chosen by D(cid:96) sampling  then a
constant-factor approximation to the optimal k-clustering is obtained. This guarantee holds in
expectation and for all datasets  like the one in [4]  and improves upon the O(log k) factor therein.
Such a result is known as a constant-factor bi-criteria approximation since both the optimal cost and
the relevant degrees of freedom (k in this case) are exceeded but only by constant factors.
In the context of clustering  bi-criteria approximation guarantees can be valuable because an ap-
propriate number of clusters k is almost never known or pre-speciﬁed in practice. Approaches to
determining k from the data are ideally based on knowing how the optimal cost decreases as k
increases  but obtaining this optimal trade-off between cost and k is NP-hard as mentioned earlier.
Alternatively  a simpler algorithm (like k-means++) that has a constant-factor bi-criteria guarantee
would ensure that the trade-off curve generated by this algorithm deviates by no more than constant
factors along both axes from the optimal curve. This may be more appealing than a deviation along

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

the cost axis that grows as O(log k). Furthermore  if a solution with a speciﬁed number of clusters k
is truly required  then linear programming techniques can be used to select a k-subset from the βk
cluster centers while still maintaining a constant-factor approximation [1  8].
The next section reviews existing work on D(cid:96) sampling and other clustering approximations. Section 2
formally states the problem  the D(cid:96) sampling algorithm  and existing lemmas regarding the algorithm.
Section 3 states the main results and compares them to previous results. Proofs are presented in
Section 4 with more algebraic proofs deferred to the supplementary material.

1.1 Related Work

Approximation algorithms for k-means ((cid:96) = 2)  k-medians ((cid:96) = 1)  and related problems span a
wide range in the trade-off between tighter approximation factors and lower algorithm complexity.
At one end  while exact algorithms [14] and polynomial-time approximation schemes (PTAS)
(see [22  18  9  12  13  10] and references therein) may have polynomial running times in n  the
dependence on k and/or the dimension d is exponential or worse. Simpler local search [17  5] and
linear programming [8  16] algorithms offer constant-factor approximations but still with high-order
polynomial running times in n  and some rely on dense discretizations of size O(n−d log(1/)).
In contrast to the above  this paper focuses on highly practical algorithms in the D(cid:96) sampling class 
including k-means++. As mentioned  it was proved in [4] that D(cid:96) sampling results in an O(log k)
approximation  in expectation and for all datasets. The current work extends this guarantee to the
constant-factor bi-criteria regime  also for all datasets. The authors of [4] also provided a matching
lower bound  exhibiting a dataset on which k-means++ achieves an expected Ω(log k) approximation.
Improved O(1) approximation factors have been shown for sampling algorithms like k-means++
provided that the dataset satisﬁes certain conditions. Such results were established in [24] for k-
means++ and other variants of Lloyd’s algorithm under the condition that the dataset is well-suited
in a sense to partitioning into k clusters  and for an algorithm called successive sampling [23] with
O(n(k + log n) + k2 log2 n) running time subject to a bound on the dispersion of the points.
In a similar direction to the one pursued in the present work  [1] showed that if the number of cluster
centers is increased to a constant factor times k  then k-means++ can achieve a constant-factor
approximation  albeit only with constant probability. An O(1) factor was also obtained independently
by [2] using more centers  of order O(k log k). It is important to note that the constant-probability
result of [1] in no way implies the main results herein  which are true in expectation and are therefore
stronger guarantees. Furthermore  Section 3.1 shows that a constant-probability corollary of Theorem
1 improves upon [1] by more than a factor of 2.
Recently  [21  7] have also established constant-factor bi-criteria results for the k-means problem.
These works differ from the present paper in studying more complex local search and linear program-
ming algorithms applied to large discretizations  of size nO(log(1/)/2) (a high-order polynomial)
in [21] and O(n−d log(1/)) in [7]  the latter the same as in [17]. Moreover  [7] employs search
neighborhoods that are also of exponential size in d (requiring doubly exponential running time).

2 Preliminaries

2.1 Problem Deﬁnition
We are given n points x1  . . .   xn in a real metric space X with metric D(x  y). The objective is to
choose t cluster centers c1  . . .   ct in X and assign points to the nearest cluster center to minimize the
potential function

n(cid:88)

φ =

min

j=1 ... t

D(xi  cj)(cid:96).

i=1

are broken arbitrarily. For a subset of points S  deﬁne φ(S) =(cid:80)

A cluster is thus deﬁned by the points xi assigned to a center cj  where ties (multiple closest centers)
xi∈S minj=1 ... t D(xi  cj)(cid:96) to be
the contribution to the potential from S; φ(xi) is the contribution from a single point xi.
The exponent (cid:96) ≥ 1 in (1) is regarded as a problem parameter. Letting (cid:96) = 2 and D be Euclidean
distance  we have what is usually known as the k-means problem  so-called because the optimal

(1)

2

Algorithm 1 D(cid:96) Sampling

Input: Data points x1  . . .   xn  number of clusters t. Initialize φ(xi) = 1 for i = 1  . . .   n.
for j = 1 to t do

Select jth center cj = xi with probability φ(xi)/φ.
Update φ(xi) for i = 1  . . .   n.

cluster centers are means of the points assigned to them. The choice (cid:96) = 1 is also popular and
corresponds to the k-medians problem.
Throughout this paper  an optimal clustering will always refer to one that minimizes (1) over solutions
with t = k clusters  where k ≥ 2 is given. Likewise  the term optimal cluster and symbol A will refer
to one of the k clusters from this optimal solution. The goal is to approximate the potential φ∗ of this
optimal k-clustering using t = βk cluster centers for β ≥ 1.

2.2 D(cid:96) Sampling Algorithm

The D(cid:96) sampling algorithm chooses cluster centers randomly from x1  . . .   xn with probabilities
proportional to their current contributions to the potential  as detailed in Algorithm 1. Following [4] 
the case (cid:96) = 2 is referred to as the k-means++ algorithm and the non-uniform probabilities used after
the ﬁrst iteration are referred to as D2 weighting (hence D(cid:96) in general). For t cluster centers  the
running time of D(cid:96) sampling is O(ntd) in d dimensions.
In practice  Algorithm 1 is used as an initialization to Lloyd’s algorithm  which usually produces
further decreases in the potential. The analysis herein pertains only to Algorithm 1 and not to the
subsequent improvement due to Lloyd’s algorithm.

2.3 Existing Lemmas Regarding D(cid:96) Sampling

The following lemmas synthesize useful results from [4] that bound the expected potential within a
single optimal cluster due to selecting a center from that cluster with uniform or D(cid:96) weighting.
Lemma 1. [4  Lemmas 3.1 and 5.1] Given an optimal cluster A  let φ be the potential resulting from
u φ∗(A)
selecting a ﬁrst cluster center randomly from A with uniform weighting. Then E[φ(A)] ≤ r((cid:96))
for any A  where

Lemma 2. [4  Lemma 3.2] Given an optimal cluster A and an initial potential φ  let φ(cid:48) be the
potential resulting from adding a cluster center selected randomly from A with D(cid:96) weighting. Then
E[φ(cid:48)(A)] ≤ r((cid:96))

D φ∗(A) for any A  where r((cid:96))

D = 2(cid:96)r((cid:96))
u .

The factor of 2(cid:96) between r((cid:96))

u and r((cid:96))

D for general (cid:96) is explained just before Theorem 5.1 in [4].

3 Main Results

The main results of this paper are stated below in terms of the single-cluster approximation ratio r((cid:96))
D
deﬁned by Lemma 2. Subsequently in Section 3.1  the results are discussed in the context of previous
work.
(cid:19)
Theorem 1. Let φ be the potential resulting from selecting βk cluster centers according to Algo-
rithm 1  where β ≥ 1. The expected approximation ratio is then bounded as
− Θ
k ∼ log k is the kth
2 + ··· + 1

E[φ]
φ∗ ≤ r((cid:96))
√
where ϕ = (1 +
harmonic number.

.
= 1.618 is the golden ratio and Hk = 1 + 1

(cid:26) ϕ(k − 2)

(β − 1)k + ϕ

(cid:27)(cid:19)

(cid:18) 1

n

1 + min

D

(cid:18)

  Hk−1

 

5)/2

In the proof of Theorem 1 in Section 4.2  it is shown that the 1/n term is indeed non-positive and can
therefore be omitted  with negligible loss for large n.

3

(cid:26)2 

(cid:96) = 2 and D is Euclidean 

r((cid:96))
u =

2(cid:96)  otherwise.

The approximation ratio bound in Theorem 1 is stated as a function of k. The following corollary
conﬁrms that the theorem also implies a constant-factor bi-criteria approximation.
Corollary 1. With the same deﬁnitions as in Theorem 1  the expected approximation ratio is bounded
as

(cid:18)

E[φ]
φ∗ ≤ r((cid:96))

D

1 +

ϕ
β − 1

(cid:19)

.

Proof. The minimum in Theorem 1 is bounded by its ﬁrst term. This term is in turn increasing in k
with asymptote ϕ/(β − 1)  which can therefore be taken as a k-independent bound.

It follows from Corollary 1 that a constant “oversampling” ratio β > 1 leads to a constant-factor
approximation. Theorem 1 offers a further reﬁnement for ﬁnite k.
The bounds in Theorem 1 and Corollary 1 consist of two factors. As β increases  the second 
parenthesized factor decreases to 1 either exactly or approximately as 1/(β − 1). The ﬁrst factor
of r((cid:96))
D however is no smaller than 4  and is a direct consequence of Lemma 2. Any future work on
improving Lemma 2 would therefore strengthen the approximation factors above.

3.1 Comparisons to Existing Results

A comparison of Theorem 1 to results in [4] is implicit in its statement since the Hk−1 term in the
minimum comes directly from [4  Theorems 3.1 and 5.1]. For k = 2  3  the ﬁrst term in the minimum
is smaller than Hk−1 for any β ≥ 1  and hence Theorem 1 is always an improvement. For k > 3 
Theorem 1 improves upon [4] for β greater than the critical value
φ(k − 2 − Hk−1)

βc = 1 +

kHk−1

.

Numerical evaluation of βc shows that it reaches a maximum value of 1.204 at k = 22 and then
decreases back toward 1 roughly as 1/Hk−1. It can be concluded that for any k  at most 20%
oversampling is required for Theorem 1 to guarantee a better approximation than [4].
The most closely related result to Theorem 1 and Corollary 1 is found in [1  Theorem 1]. The latter
establishes a constant-factor bi-criteria approximation that holds only with constant probability  as
opposed to in expectation. Since a bound on the expectation implies a bound with constant probability
via Markov’s inequality (but not the other way around)  a direct comparison with [1] is possible.
Speciﬁcally  for (cid:96) = 2 and the t = (cid:100)16(k +
k)(cid:101) cluster centers assumed in [1]  Theorem 1 in the
present work implies that

√

(cid:18)

(cid:26)

(cid:27)(cid:19)

(cid:16)

≤ 8

1 +

ϕ
15

(cid:17)

 

ϕ(k − 2)
√
after taking k → ∞. Then by Markov’s inequality 

E[φ]
φ∗ ≤ 8

(cid:100)15k + 16

1 + min

k(cid:101) + ϕ

  Hk−1

(cid:16)

(cid:17) .

ϕ
15

1 +

= 9.137

φ

φ∗ ≤ 8

0.97

with probability at least 1 − 0.97 = 0.03 as in [1]. This 9.137 approximation factor is less than half
the factor of 20 in [1].
Corollary 1 may also be compared to the results in [21]  which are obtained through more complex
algorithms applied to a large discretization  of size nO(log(1/)/2) for reasonably small . The main
difference between Corollary 1 and the bounds in [21] is the extra factor of r((cid:96))
D . As discussed above 
this factor is due to Lemma 2 and is unlikely to be intrinsic to the D(cid:96) sampling algorithm.

4 Proofs

The overall strategy used to prove Theorem 1 is similar to that in [4]. The key intermediate result is
Lemma 3 below  which relates the potential at a later iteration in Algorithm 1 to the potential at an
earlier iteration. Section 4.1 is devoted to proving Lemma 3. Subsequently in Section 4.2  Theorem 1
is proven by an application of Lemma 3.

4

In the sequel  we say that an optimal cluster A is covered by a set of cluster centers if at least one of
the centers lies in A. Otherwise A is uncovered. Also deﬁne ρ = r((cid:96))
Lemma 3. For an initial set of centers leaving u optimal clusters uncovered  let φ denote the
potential  U the union of uncovered clusters  and V the union of covered clusters. Let φ(cid:48) denote
the potential resulting from adding t ≥ u centers  each selected randomly with D(cid:96) weighting as in
Algorithm 1. Then the new potential is bounded in expectation as

D φ∗ as an abbreviation.

E[φ(cid:48) | φ] ≤ cV (t  u)φ(V) + cU (t  u)ρ(U)

for coefﬁcients cV (t  u) and cU (t  u) that depend only on t  u. This holds in particular for

cV (t  u) =

t + au + b
t − u + b

(cid:26)cV (t − 1  u − 1)  u > 0 

(a + 1)u
t − u + b

= 1 +

 

(2a)

(2b)
where the parameters a and b satisfy a + 1 ≥ b > 0 and ab ≥ 1. The choice of a  b that minimizes
cV (t  u) in (2a) is a + 1 = b = ϕ.

cU (t  u) =

u = 0 

0 

4.1 Proof of Lemma 3

Lemma 3 is proven using induction  showing that if it holds for (t  u) and (t  u + 1)  then it also
holds for (t + 1  u + 1)  similar to the proof of [4  Lemma 3.3]. The proof is organized into three
parts. Section 4.1.1 provides base cases. In Section 4.1.2  sufﬁcient conditions on the coefﬁcients
cV (t  u)  cU (t  u) are derived that allow the inductive step to be completed. In Section 4.1.3  it is
shown that the closed-form expressions in (2) are consistent with the base cases in Section 4.1.1 and
satisfy the sufﬁcient conditions from Section 4.1.2  thus completing the proof.

4.1.1 Base cases

This subsection exhibits two base cases of Lemma 3. The ﬁrst case corresponds to u = 0  for which
we have φ(V) = φ. Since adding centers cannot increase the potential  i.e. φ(cid:48) ≤ φ deterministically 
Lemma 3 holds with

cV (t  0) = 1 

cU (t  0) = 0 

(3)
The second base case occurs for t = u  u ≥ 1. For this purpose  a slightly strengthened version of [4 
Lemma 3.3] is used  as given next.
Lemma 4. With the same deﬁnitions as in Lemma 3 except with t ≤ u  we have
φ(U) 

E[φ(cid:48) | φ] ≤ (1 + Ht)φ(V) + (1 + Ht−1)ρ(U) +

t ≥ 0.

where we deﬁne H0 = 0 and H−1 = −1 for convenience.
The improvement is in the coefﬁcient in front of ρ(U)  from (1 + Ht) to (1 + Ht−1). The proof
follows that of [4  Lemma 3.3] with some differences and is deferred to the supplementary material.
Specializing to the case t = u  Lemma 4 coincides with Lemma 3 with coefﬁcients

u − t
u

cV (u  u) = 1 + Hu 

cU (u  u) = 1 + Hu−1.

(4)

4.1.2 Sufﬁcient conditions on coefﬁcients

We now assume inductively that Lemma 3 holds for (t  u) and (t  u + 1). The induction to the case
(t + 1  u + 1) is then completed under the following sufﬁcient conditions on the coefﬁcients:

cV (t  u + 1) ≥ 1 

(cV (t  u + 1) − cU (t  u + 1))cV (t  u)2 ≥ (cU (t  u + 1) − cV (t  u))2 

cV (t  u) +(cid:0)cV (t  u)2 + 4 max{cV (t  u + 1) − cV (t  u)  0}(cid:1)1/2(cid:105)

(cid:104)

and

cV (t + 1  u + 1) ≥ 1
2
cU (t + 1  u + 1) ≥ cV (t  u).

(5a)
(5b)

(6a)

(6b)

 

5

The ﬁrst pair of conditions (5) applies to the coefﬁcients involved in the inductive hypothesis for (t  u)
and (t  u + 1). The second pair (6) can be seen as a recursive speciﬁcation of the new coefﬁcients
for (t + 1  u + 1). This inductive step together with base cases (3) and (4) are sufﬁcient to extend
Lemma 3 to all t > u  starting with (t  u) = (1  0) and (t  u + 1) = (1  1).
The inductive step is broken down into a series of three lemmas  each building upon the last. The ﬁrst
lemma applies the inductive hypothesis to derive a bound on the potential that depends not only on
φ(V) and ρ(U) but also on φ(U).
Lemma 5. Assume that Lemma 3 holds for (t  u) and (t  u + 1). Then for the case (t + 1  u + 1) 
i.e. φ corresponding to u + 1 uncovered clusters and φ(cid:48) resulting after adding t + 1 centers 

(cid:26) cV (t  u)φ(U) + cV (t  u + 1)φ(V)

φ(U) + φ(V)

φ(V)

E[φ(cid:48) | φ] ≤ min

cV (t  u)φ(U) + cU (t  u + 1)φ(V)

φ(U) + φ(V)

+

(cid:27)

ρ(U)  φ(U) + φ(V)

.

Proof. We consider the two cases in which the ﬁrst of the t + 1 new centers is chosen from either the
covered set V or the uncovered set U. Denote by φ1 the potential after adding the ﬁrst new center.
Covered case: This case occurs with probability φ(V)/φ and leaves the covered and uncovered sets
unchanged. We then invoke Lemma 3 with (t  u + 1) (one fewer center to add) and φ1 playing the
role of φ. The contribution to E[φ(cid:48) | φ] from this case is then bounded by
φ(V)
φ

(cid:0)cV (t  u + 1)φ1(V) + cU (t  u + 1)ρ(U)(cid:1) ≤ φ(V)

(cV (t  u + 1)φ(V) + cU (t  u + 1)ρ(U))  
(7)

noting that φ1(S) ≤ φ(S) for any set S.
Uncovered case: We consider each uncovered cluster A ⊆ U separately. With probability φ(A)/φ 
the ﬁrst new center is selected from A  moving A from the uncovered to the covered set and reducing
the number of uncovered clusters by one. Applying Lemma 3 for (t  u)  the contribution to E[φ(cid:48) | φ]
is bounded by

φ

(cid:2)cV (t  u)(cid:0)φ1(V) + φ1(A)(cid:1) + cU (t  u)(ρ(U) − ρ(A))(cid:3) .

φ(A)
φ

Taking the expectation with respect to possible centers in A and using Lemma 2 and φ1(V) ≤ φ(V) 
we obtain the further bound
φ(A)
φ

[cV (t  u)(φ(V) + ρ(A)) + cU (t  u)(ρ(U) − ρ(A))] .

Summing over A ⊆ U yields

φ(U)
φ

(cV (t  u)φ(V) + cU (t  u)ρ(U)) +

cV (t  u) − cU (t  u)

(cid:88)

φ

A⊆U

≤ φ(U)

cV (t  u)(φ(V) + ρ(U)) 

using the inner product bound(cid:80)A⊆U φ(A)ρ(A) ≤ φ(U)ρ(U).

φ

φ(A)ρ(A)

(8)

The result follows from summing (7) and (8) and combining with the trivial bound E[φ(cid:48) | φ] ≤ φ =
φ(U) + φ(V).
The bound in Lemma 5 depends on φ(U)  the potential over uncovered clusters  which can be
arbitrarily large or small. In the next lemma  φ(U) is eliminated by maximizing with respect to it.
Lemma 6. Assume that Lemma 3 holds for (t  u) and (t  u + 1) with cV (t  u + 1) ≥ 1. Then for the
case (t + 1  u + 1) in the sense of Lemma 5 

(cid:110)
(cid:111)
cV (t  u)(φ(V) + ρ(U)) (cid:112)Q

 

E[φ(cid:48) | φ] ≤ 1
2

cV (t  u)(φ(V) + ρ(U)) +

1
2

max

6

where

Q =(cid:0)cV (t  u)2 − 4cV (t  u) + 4cV (t  u + 1)(cid:1) φ(V)2

+ 2(cid:0)cV (t  u)2 − 2cV (t  u) + 2cU (t  u + 1)(cid:1) φ(V)ρ(U) + cV (t  u)2ρ(U)2.

Proof. Let B1(φ(U)) and B2(φ(U)) denote the two terms inside the minimum in Lemma 5 (i.e.
B2(φ(U)) = φ(U) + φ(V)). The derivative of B1(φ(U)) with respect to φ(U) is given by
1(φ(U)) =
B(cid:48)

(cid:2)(cV (t  u) − cV (t  u + 1))φ(V) + (cV (t  u) − cU (t  u + 1))ρ(U)(cid:3) 

φ(V)

(φ(U) + φ(V))2

which does not change sign as a function of φ(U). The two cases B(cid:48)
1(φ(U)) < 0
are considered separately below. Taking the maximum of the resulting bounds (9)  (10) establishes
the lemma.
Case B(cid:48)
former has the ﬁnite supremum

1(φ(U)) ≥ 0: Both B1(φ(U)) and B2(φ(U)) are non-decreasing functions of φ(U). The

1(φ(U)) ≥ 0 and B(cid:48)

cV (t  u)(φ(V) + ρ(U)) 

(9)
whereas the latter increases without bound. Therefore B1(φ(U)) eventually becomes the smaller of
the two and (9) can be taken as an upper bound on min{B1(φ(U))  B2(φ(U))}.
1(φ(U)) < 0: At φ(U) = 0  we have B1(0) = cV (t  u + 1)φ(V) + cU (t  u + 1)ρ(U) and
Case B(cid:48)
B2(0) = φ(V). The assumption cV (t  u + 1) ≥ 1 implies that B1(0) ≥ B2(0). Since B1(φ(U))
is now a decreasing function  the two functions must intersect and the point of intersection then
provides an upper bound on min{B1(φ(U))  B2(φ(U))}. The supplementary material provides some
algebraic details on solving for the intersection. The resulting bound is

cV (t  u)(φ(V) + ρ(U)) +

1
2

1
2

(cid:112)Q.

(10)

The bound in Lemma 6 is a nonlinear function of φ(V) and ρ(U)  in contrast to the desired form in
Lemma 3. The next step is to linearize the bound by imposing additional conditions (5).
Lemma 7. Assume that Lemma 3 holds for (t  u) and (t  u + 1) with coefﬁcients satisfying (5). Then
for the case (t + 1  u + 1) in the sense of Lemma 5 
E[φ(cid:48) | φ] ≤ 1
2

cV (t  u) +(cid:0)cV (t  u)2 + 4 max{cV (t  u + 1) − cV (t  u)  0}(cid:1)1/2(cid:105)

φ(V) + cV (t  u)ρ(U).

(cid:104)

Proof. It sufﬁces to linearize the

bρ(U))2 for all φ(V)  ρ(U) with a =(cid:2)cV (t  u)2 + 4(cV (t  u + 1) − cV (t  u))(cid:3)1/2 and b = cV (t  u).

Q term in Lemma 6  speciﬁcally by showing that Q ≤ (aφ(V) +

Proof of this inequality is provided in the supplementary material. Incorporating the inequality into
Lemma 6 proves the result.

√

Given conditions (5) and Lemma 7  the inductive step for Lemma 3 can be completed by deﬁning
cV (t + 1  u + 1) and cU (t + 1  u + 1) recursively as in (6).

4.1.3 Proof with speciﬁc form for coefﬁcients
We now prove that Lemma 3 holds for coefﬁcients cV (t  u)  cU (t  u) given by (2) with a + 1 ≥ b > 0
and ab ≥ 1. Given the inductive approach and the results established in Sections 4.1.1 and 4.1.2 
the proof requires the remaining steps below. First  it is shown that the base cases (3)  (4) from
Section 4.1.1 imply that Lemma 3 is true for the same base cases but with cV (t  u)  cU (t  u) given
by (2) instead. Second  (2) is shown to satisfy conditions (5) for all t > u  thus permitting Lemma
7 to be used. Third  (2) is also shown to satisfy (6)  which combined with Lemma 7 completes the
induction.

7

(cid:18)

Considering the base cases  for u = 0  (3) and (2) coincide so there is nothing to prove. For the case
t = u  u ≥ 1  Lemma 3 with coefﬁcients given by (4) implies the same with coefﬁcients given by (2)
provided that

(cid:19)

b

(cid:18)

(cid:19)

ρ(U)

(a + 1)(u − 1)

b

(1 + Hu)φ(V) + (1 + Hu−1)ρ(U) ≤

1 +

(a + 1)u

φ(V) +

1 +

for all φ(V)  ρ(U). This in turn is ensured if the coefﬁcients satisfy Hu ≤ (a + 1)u/b for all u ≥ 1.
The most stringent case is u = 1 and corresponds to the assumption a + 1 ≥ b.
For the second step of establishing (5)  it is clear that (5a) is satisﬁed by (2a). A direct calculation
presented in the supplementary material shows that (5b) is also true.
Lemma 8. Condition (5b) is satisﬁed for all t > u if cV (t  u)  cU (t  u) are given by (2) and ab ≥ 1.
Similarly for the third step  it sufﬁces to show that (2a) satisﬁes recursion (6a) since (2b) automatically
satisﬁes (6b). A proof is provided in the supplementary material.
Lemma 9. Recursion (6a) is satisﬁed for all t > u if cV (t  u) is given by (2a) and ab ≥ 1.
Lastly  we minimize cV (t  u) in (2a) with respect to a  b  subject to a + 1 ≥ b > 0 and ab ≥ 1. For
ﬁxed a  minimizing with respect to b yields b = a + 1 and cV (t  u) = 1 + ((a + 1)u)/(t− u + a + 1).
Minimizing with respect to a then results in setting ab = a(a + 1) = 1. The solution satisfying
a + 1 > 0 is a = ϕ − 1 and b = ϕ.

4.2 Proof of Theorem 1
Denote by nA the number of points in optimal cluster A. In the ﬁrst iteration of Algorithm 1  the ﬁrst
cluster center is selected from some A with probability nA/n. Conditioned on this event  Lemma 3
is applied with covered set V = A  u = k − 1 uncovered clusters  and t = βk − 1 remaining cluster
centers. This bounds the ﬁnal potential φ(cid:48) as

E[φ(cid:48) | φ] ≤ cV (βk − 1  k − 1)φ(A) + cU (βk − 1  k − 1)(ρ − ρ(A))

where cV (t  u)  cU (t  u) are given by (2) with a + 1 = b = ϕ. Taking the expectation over possible
centers in A and using Lemma 1 

E[φ(cid:48) | A] ≤ r((cid:96))

u cV (βk − 1  k − 1)φ∗(A) + cU (βk − 1  k − 1)(ρ − ρ(A)).

Taking the expectation over clusters A and recalling that ρ = r((cid:96))

E[φ(cid:48)] ≤ r((cid:96))

D cU (βk − 1  k − 1)φ∗ − C

φ∗(A) 

(11)

(cid:88)
D φ∗ 
nA
n

A

where C = r((cid:96))
2 

D cU (βk − 1  k − 1)− r((cid:96))

u cV (βk − 1  k − 1). Using (2) and r((cid:96))
2(cid:96) ((β − 1)k + ϕ(k − 1)) − (β − 1 + ϕ)k

C = r((cid:96))
u

(β − 1)k + ϕ

(2(cid:96) − 1)(β − 1)k + ϕ((2(cid:96) − 1)(k − 1) − 1)

(β − 1)k + ϕ
The last expression for C is seen to be non-negative for β ≥ 1  k ≥ 2  and (cid:96) ≥ 1. Furthermore  since
nA = 1 (a singleton cluster) implies that φ∗(A) = 0  we have

.

= r((cid:96))
u

D = 2(cid:96)r((cid:96))

u from Lemma

(cid:88)

A

nAφ∗(A) =

nAφ∗(A) ≥ 2φ∗.

(cid:88)

A:nA≥2

(cid:18)

(cid:19)

Substituting (2) and (12) into (11)  we obtain

E[φ(cid:48)]
φ∗ ≤ r((cid:96))

D

ϕ(k − 2)

(β − 1)k + ϕ

1 +

− 2C
n

.

(12)

(13)

The last step is to recall [4  Theorems 3.1 and 5.1]  which together state that

(14)
for φ(cid:48) resulting from selecting exactly k cluster centers. In fact  (14) also holds for βk centers  β ≥ 1 
since adding centers cannot increase the potential. The proof is completed by taking the minimum of
(13) and (14).

D (1 + Hk−1)

E[φ(cid:48)]
φ∗ ≤ r((cid:96))

8

References
[1] A. Aggarwal  A. Deshpande  and R. Kannan. Adaptive sampling for k-means clustering. In Proc. 12th Int.
Workshop and 13th Int. Workshop on Approximation  Randomization  and Combinatorial Optimization.
Algorithms and Techniques  pages 15–28  August 2009.

[2] N. Ailon  R. Jaiswal  and C. Monteleoni. Streaming k-means approximation. In Adv. Neural Information

Processing Systems 22  pages 10–18  December 2009.

[3] D. Aloise  A. Deshpande  P. Hansen  and P. Popat. NP-hardness of Euclidean sum-of-squares clustering.

Mach. Learn.  75(2):245–248  May 2009.

[4] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proc. 18th ACM-SIAM

Symp. Discrete Algorithms  pages 1027–1035  January 2007.

[5] V. Arya  N. Garg  R. Khandekar  A. Meyerson  K. Munagala  and V. Pandit. Local search heuristics for

k-median and facility location problems. SIAM J. Comput.  33(3):544–562  March 2004.

[6] P. Awasthi  M. Charikar  R. Krishnaswamy  and A. K. Sinop. The hardness of approximation of Euclidean

k-means. In Proc. 31st Int. Symp. Computational Geometry  pages 754–767  June 2015.

[7] S. Bandyapadhyay and K. Varadarajan. On variants of k-means clustering. Technical Report

arXiv:1512.02985  December 2015.

[8] M. Charikar  S. Guha  E. Tardos  and D. B. Shmoys. A constant-factor approximation algorithm for the

k-median problem. J. Comput. Syst. Sci.  65(1):129–149  August 2002.

[9] K. Chen. On coresets for k-median and k-means clustering in metric and Euclidean spaces and their

applications. SIAM J. Comput.  39(3):923–947  September 2009.

[10] V. Cohen-Addad  P. N. Klein  and C. Mathieu. Local search yields approximation schemes for k-means

and k-median in Euclidean and minor-free metrics. Technical Report arXiv:1603.09535  March 2016.

[11] S. Dasgupta. The hardness of k-means clustering. Technical Report CS2008-0916  Department of

Computer Science and Engineering  University of California  San Diego  2008.

[12] D. Feldman  M. Monemizadeh  and C. Sohler. A PTAS for k-means clustering based on weak coresets. In

Proc. 23rd Int. Symp. Computational Geometry  pages 11–18  June 2007.

[13] Z. Friggstad  M. Rezapour  and M. R. Salavatipour. Local search yields a PTAS for k-means in doubling

metrics. Technical Report arXiv:1603.08976  March 2016.

[14] M. Inaba  N. Katoh  and H. Imai. Applications of weighted Voronoi diagrams and randomization to

variance-based k-clustering. In Proc. 10th Int. Symp. Computational Geometry  pages 332–339  1994.

[15] A. K. Jain. Data clustering: 50 years beyond k-means. Pattern Recogn. Lett.  31(8):651–666  June 2010.

[16] K. Jain and V. V. Vazirani. Approximation algorithms for metric facility location and k-median problems

using the primal-dual schema and Lagrangian relaxation. J. ACM  48(2):274–296  March 2001.

[17] T. Kanungo  D. M. Mount  N. S. Netanyahu  C. D. Piatko  R. Silverman  and A. Y. Wu. A local search

approximation algorithm for k-means clustering. Comput. Geom.  28(2–3):89–112  June 2004.

[18] A. Kumar  Y. Sabharwal  and S. Sen. Linear-time approximation schemes for clustering problems in any

dimensions. J. ACM  57(2):5:1–5:32  January 2010.

[19] S. Lloyd. Least squares quantization in PCM. Technical report  Bell Laboratories  1957.

[20] M. Mahajan  P. Nimbhorkar  and K. Varadarajan. The planar k-means problem is NP-hard. In Proc. 3rd

Int. Workshop Algorithms and Computation  pages 274–285  February 2009.

[21] K. Makarychev  Y. Makarychev  M. Sviridenko  and J. Ward. A bi-criteria approximation algorithm for k

means. Technical Report arXiv:1507.04227  August 2015.

[22] J. Matoušek. On approximate geometric k-clustering. Discrete & Comput. Geom.  24(1):61–84  January

2000.

[23] R. R. Mettu and C. G. Plaxton. Optimal time bounds for approximate clustering. Mach. Learn.  56(1–

3):35–60  June 2004.

[24] R. Ostrovsky  Y. Rabani  L. J. Schulman  and C. Swamy. The effectiveness of Lloyd-type methods for the

k-means problem. J. ACM  59(6):28  December 2012.

9

,Dennis Wei