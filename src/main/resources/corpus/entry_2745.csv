2017,Joint distribution optimal transportation for domain adaptation,This paper deals with the unsupervised domain adaptation problem  where one wants to estimate a prediction function $f$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a  non-linear transformation between the joint feature/label space distributions of the two domain $\ps$ and $\pt$. We propose a solution of this problem with optimal transport  that allows to recover an estimated target $\pt^f=(X f(X))$ by optimizing simultaneously the optimal coupling and $f$. We show that our method corresponds to the minimization of a bound on the target error  and provide an efficient algorithmic solution  for which convergence is proved. The versatility of our approach  both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems  for which we reach or surpass state-of-the-art results.,Joint distribution optimal transportation for domain

adaptation

Nicolas Courty∗

Université de Bretagne Sud 
IRISA  UMR 6074  CNRS 

courty@univ-ubs.fr

Rémi Flamary∗

Université Côte d’Azur 

Lagrange  UMR 7293   CNRS  OCA

remi.flamary@unice.fr

Amaury Habrard

Univ Lyon  UJM-Saint-Etienne  CNRS 
Lab. Hubert Curien UMR 5516  F-42023
amaury.habrard@univ-st-etienne.fr

Alain Rakotomamonjy
Normandie Universite

Université de Rouen  LITIS EA 4108

alain.rakoto@insa-rouen.fr

Abstract

This paper deals with the unsupervised domain adaptation problem  where one
wants to estimate a prediction function f in a given target domain without any
labeled sample by exploiting the knowledge available from a source domain where
labels are known. Our work makes the following assumption: there exists a non-
linear transformation between the joint feature/label space distributions of the two
domain Ps and Pt that can be estimated with optimal transport. We propose a
solution of this problem that allows to recover an estimated target P f
t = (X  f (X))
by optimizing simultaneously the optimal coupling and f. We show that our method
corresponds to the minimization of a bound on the target error  and provide an
efﬁcient algorithmic solution  for which convergence is proved. The versatility of
our approach  both in terms of class of hypothesis or loss functions is demonstrated
with real world classiﬁcation and regression problems  for which we reach or
surpass state-of-the-art results.

1

Introduction

In the context of supervised learning  one generally assumes that the test data is a realization of the
same process that generated the learning set. Yet  in many practical applications it is often not the
case  since several factors can slightly alter this process. The particular case of visual adaptation [1]
in computer vision is a good example: given a new dataset of images without any label  one may want
to exploit a different annotated dataset  provided that they share sufﬁcient common information and
labels. However  the generating process can be different in several aspects  such as the conditions and
devices used for acquisition  different pre-processing  different compressions  etc. Domain adaptation
techniques aim at alleviating this issue by transferring knowledge between domains [2]. We propose
in this paper a principled and theoretically founded way of tackling this problem.
The domain adaptation (DA) problem is not new and has received a lot of attention during the past ten
years. State-of-the-art methods are mainly differing by the assumptions made over the change in data
distributions. In the covariate shift assumption  the differences between the domains are characterized
by a change in the feature distributions P(X)  while the conditional distributions P(Y |X) remain
unchanged (X and Y being respectively the instance and label spaces). Importance re-weighting can
be used to learn a new classiﬁer (e.g. [3])  provided that the overlapping of the distributions is large

∗Both authors contributed equally.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

enough. Kernel alignment [4] has also been considered for the same purpose. Other types of method 
denoted as Invariant Components by Gong and co-authors [5]  are looking for a transformation
T such that the new representations of input data are matching  i.e. Ps(T (X)) = Pt(T (X)).
Methods are then differing by: i) The considered class of transformation  that are generally deﬁned
as projections (e.g. [6  7  8  9  5])  afﬁne transform [4] or non-linear transformation as expressed by
neural networks [10  11] ii) The types of divergences used to compare Ps(T (X)) and Pt(T (X)) 
such as Kullback Leibler [12] or Maximum Mean Discrepancy [9  5]. Those divergences usually
require that the distributions share a common support to be deﬁned. A particular case is found in
the use of optimal transport  introduced for domain adaptation by [13  14]. T is then deﬁned to be
a push-forward operator such that Ps(X) = Pt(T (X)) and that minimizes a global transportation
effort or cost between distributions. The associated divergence is the so-called Wasserstein metric 
that has a natural Lagrangian formulation and avoids the estimation of continuous distribution by
means of kernel. As such  it also alleviates the need for a shared support.
The methods discussed above implicitly assume that the conditional distributions are unchanged by
T   i.e. Ps(Y |T (X)) ≈ Pt(Y |T (X)) but there is no clear reason for this assumption to hold. A
more general approach is to adapt both marginal feature and conditional distributions by minimizing
a global divergence between them. However  this task is usually hard since no label is available in
the target domain and therefore no empirical version Pt(Y |X) can be used. This was achieved by
restricting to speciﬁc class of transformation such as projection [9  5].
Contributions and outline. In this work we propose a novel framework for unsupervised domain
adaptation between joint distributions. We propose to ﬁnd a function f that predicts an output
value given an input x ∈ X   and that minimizes the optimal transport loss between the joint source
distribution Ps and an estimated target joint distribution P f
t = (X  f (X)) depending on f (detailed
in Section 2). The method is denoted as JDOT for “Joint Distribution Optimal Transport" in the
remainder. We show that the resulting optimization problem stands for a minimization of a bound
on the target error of f (Section 3) and propose an efﬁcient algorithm to solve it (Section 4). Our
approach is very general and does not require to learn explicitly a transformation  as it directly solves
for the best function. We show that it can handle both regression and classiﬁcation problems with a
large class of functions f including kernel machines and neural networks. We ﬁnally provide several
numerical experiments on real regression and classiﬁcation problems that show the performances of
JDOT over the state-of-the-art (Section 5).

2 Joint distribution Optimal Transport
Let Ω ∈ Rd be a compact input measurable space of dimension d and C the set of labels. P(Ω)
denotes the set of all the probability measures over Ω. The standard learning paradigm assumes
classically the existence of a set of data Xs = {xs
i}Ns
i=1 associated with a set of class label information
Ys = {ys
i}Nt
i=1 (the
testing set). In order to determine the set of labels Yt associated with Xt   one usually relies on an
empirical estimate of the joint probability distribution P(X  Y ) ∈ P(Ω × C) from (Xs  Ys)  and
the assumption that Xs and Xt are drawn from the same distribution µ ∈ P(Ω). In the considered
adaptation problem  one assumes the existence of two distinct joint probability distributions Ps(X  Y )
and Pt(X  Y ) which correspond respectively to two different source and target domains. We will
write µs and µt their respective marginal distributions over X.

i ∈ C (the learning set)  and a data set with unknown labels Xt = {xt

i}Ns
i=1  ys

2.1 Optimal transport in domain adaptation
The Monge problem is seeking for a map T0 : Ω → Ω that pushes µs toward µt deﬁned as:

(cid:90)

T

Ω

d(x T (x))dµs(x) 
where T #µs the image measure of µs by T   verifying:

T0 = argmin

s.t. T #µs = µt 

T #µs(A) = µt(T −1(A))  ∀ Borel subset A ⊂ Ω 

(1)
and d : Ω × Ω → R+ is a metric. In the remainder  we will always consider without further
notiﬁcation the case where d is the squared Euclidean metric. When T0 exists  it is called an optimal
transport map  but it is not always the case (e.g. assume that µs is deﬁned by one Dirac measure and

2

(cid:90)

µt by two). A relaxed version of this problem has been proposed by Kantorovitch [15]  who rather
seeks for a transport plan (or equivalently a joint probability distribution) γ ∈ P(Ω × Ω) such that:

γ0 = argmin
γ∈Π(µs µt)

Ω×Ω

d(x1  x2)dγ(x1  x2) 

(2)

where Π(µs  µt) = {γ ∈ P(Ω × Ω)|p+#γ = µs  p−#γ = µt} and p+ and p− denotes the two
marginal projections of Ω × Ω to Ω. Minimizers of this problem are called optimal transport plans.
Should γ0 be of the form (id × T )#µs  then the solution to Kantorovich and Monge problems
coincide. As such the Kantorovich relaxation can be seen as a generalization of the Monge problem 
with less constraints on the existence and uniqueness of solutions [16].
Optimal transport has been used in DA as a principled way to bring the source and target distribution
closer [13  14  17]  by seeking for a transport plan between the empirical distributions of Xs and
Xt and interpolating Xs thanks to a barycentric mapping [14]  or by estimating a mapping which is
not the solution of Monge problem but allows to map unseen samples [17]. Moreover  they show
that better constraining the structure of γ through entropic or classwise regularization terms helps in
achieving better empirical results.

2.2

Joint distribution optimal transport loss

The main idea of this work is is to handle a change in both marginal and conditional distributions.
As such  we are looking for a transformation T that will align directly the joint distributions Ps and
Pt. Following the Kantovorich formulation of (2)  T will be implicitly expressed through a coupling
between both joint distributions as:

(cid:90)

γ0 = argmin

γ∈Π(Ps Pt)

(Ω×C)2

D(x1  y1; x2  y2)dγ(x1  y1; x2  y2) 

(3)

where D(x1  y1; x2  y2) = αd(x1  x2) + L(y1  y2) is a joint cost measure combining both the
distances between the samples and a loss function L measuring the discrepancy between y1 and y2.
While this joint cost is speciﬁc (separable)  we leave for future work the analysis of generic joint cost
function. Putting it in words  matching close source and target samples with similar labels costs few.
α is a positive parameter which balances the metric in the feature space and the loss. As such  when
α → +∞  this cost is dominated by the metric in the input feature space  and the solution of the
coupling problem is the same as in [14]. It can be shown that a minimizer to (3) always exists and is
unique provided that D(·) is lower semi-continuous (see [18]  Theorem 4.1)  which is the case when
d(·) is a norm and for every usual loss functions [19].
In the unsupervised DA problem  one does not have access to labels in the target domain  and as such
it is not possible to ﬁnd the optimal coupling. Since our goal is to ﬁnd a function on the target domain
f : Ω → C  we suggest to replace y2 by a proxy f (x2). This leads to the deﬁnition of the following
joint distribution that uses a given function f as a proxy for y:

(cid:80)Nt

P f
t = (x  f (x))x∼µt
In practice we consider empirical versions of Ps and P f

i f (xt

i=1 δxt

t =
i). γ is then a matrix which belongs to ∆   i.e.the transportation polytope of non-
1
Nt
negative matrices between uniform distributions. Since our goal is to estimate a prediction f on
the target domain  we propose to ﬁnd the one that produces predictions that match optimally source
labels to the aligned target instances in the transport plan. For this purpose  we propose to solve the
following problem for JDOT:

i=1 δxs

i  ys
i

t   i.e. ˆPs = 1

Ns

(cid:80)Ns

(4)

and ˆP f

min
f γ∈∆

D(xs

i   ys

i ; xt

j  f (xt

j))γij ≡ min

W1( ˆPs 

ˆP f
t )

(5)

f

(cid:88)

ij

where W1 is the 1-Wasserstein distance for the loss D(x1  y1; x2  y2) = αd(x1  x2) + L(y1  y2).
We will make clear in the next section that the function f we retrieve is theoretically sound with
respect to the target error. Note that in practice we add a regularization term for function f in order
to avoid overﬁtting as discussed in Section 4. An illustration of JDOT for a regression problem is
given in Figure 1. In this ﬁgure  we have very different joint and marginal distributions but we want

3

Figure 1: Illustration of JDOT on a 1D regression problem. (left) Source and target empirical
distributions and marginals (middle left) Source and target models (middle right) OT matrix on
empirical joint distributions and with JDOT proxy joint distribution (right) estimated prediction
function f.

i   xt

i   xt

t which leads to a very good model for JDOT.

to illustrate that the OT matrix γ obtained using the true empirical distribution Pt is very similar to
the one obtained with the proxy P f
Choice of α. This is an important parameter balancing the alignment of feature space and labels. A
natural choice of the α parameter is obtained by normalizing the range of values of d(xs
j) with
α = 1/ maxi j d(xs
j). In the numerical experiment section  we show that this setting is very good
in two out of three experiments. However  in some cases  better performances are obtained with a
cross-validation of this parameter. Also note that α is strongly linked to the smoothness of the loss
L and of the optimal labelling functions and can be seen as a Lipschitz constant in the bound of
Theorem 3.1.
Relation to other optimal transport based DA methods. Previous DA methods based on optimal
transport [14  17] do not not only differ by the nature of the considered distributions  but also in the
way the optimal plan is used to ﬁnd f. They learn a complex mapping between the source and target
distributions when the objective is only to estimate a prediction function f on target. To do so  they
rely on a barycentric mapping that minimizes only approximately the Wasserstein distance between
the distributions. As discussed in Section 4  JDOT uses the optimal plan to propagate and fuse the
labels from the source to target. Not only are the performances enhanced  but we also show how this
approach is more theoretically well grounded in next section 3.
Relation to Transport Lp distances. Recently  Thorpe and co-authors introduced the Transportation
Lp distance [20]. Their objective is to compute a meaningful distance between multi-dimensional
signals. Interestingly their distance can be seen as optimal transport between two distributions of
the form (4) where the functions are known and the label loss L is chosen as a Lp distance. While
their approach is inspirational  JDOT is different both in its formulation  where we introduce a more
general class of loss L  and in its objective  as our goal is to estimate the target function f which is
not known a priori. Finally we show theoretically and empirically that our formulation addresses
successfully the problem of domain adaptation.

3 A Bound on the Target Error

def

Let f be an hypothesis function from a given class of hypothesis H. We deﬁne the expected loss in
= E(x y)∼Pt L(y  f (x)). We deﬁne similarly errS(f ) for the
the target domain errT (f ) as errT (f )
source domain. We assume the loss function L to be bounded  symmetric  k-lipschitz and satisfying
the triangle inequality.
To provide some guarantees on our method  we consider an adaptation of the notion probabilistic
Lipschitzness introduced in [21  22] which assumes that two close instances must have the same
labels with high probability. It corresponds to a relaxation of the classic Lipschitzness allowing one to
model the marginal-label relatedness such as in Nearest-Neighbor classiﬁcation  linear classiﬁcation
or cluster assumption. We propose an extension of this notion in a domain adaptation context by
assuming that a labeling function must comply with two close instances of each domain w.r.t. a
coupling Π.

4

505x2.01.51.00.50.00.51.01.5yToy regression distributions2.50.02.55.0x1.00.50.00.51.0Toy regression modelsSource modelTarget modelSource samplesTarget samples2.50.02.55.0x1.00.50.00.51.0Joint OT matricesJDOT matrix linkOT matrix link2.50.02.55.0x1.00.50.00.51.0Model estimated with JDOTSource modelTarget modelJDOT modelDeﬁnition (Probabilistic Transfer Lipschitzness) Let µs and µt be respectively the source and
target distributions. Let φ : R → [0  1]. A labeling function f : Ω → R and a joint distribution
Π(µs  µt) over µs and µt are φ-Lipschitz transferable if for all λ > 0:

P r(x1 x2)∼Π(µs µt) [|f (x1) − f (x2)| > λd(x1  x2)] ≤ φ(λ).

(cid:82)

f

Intuitively  given a deterministic labeling functions f and a coupling Π  it bounds the probability of
ﬁnding pairs of source-target instances labelled differently in a (1/λ)-ball with respect to Π.
We can now give our main result (simpliﬁed version):

of ∈

H.

Let Π∗

be

any

function

labeling

(Ω×C)2 αd(xs  xt) + L(ys  yt)dΠ(xs  ys; xt  yt) and W1( ˆPs 

Theorem 3.1 Let
=
ˆP f
t ) the as-
argminΠ∈Π(Ps P f
t )
sociated 1-Wasserstein distance. Let f∗ ∈ H be a Lipschitz labeling function that veriﬁes the
φ-probabilistic transfer Lipschitzness (PTL) assumption w.r.t. Π∗ and that minimizes the joint error
errS(f∗) + errT (f∗) w.r.t all PTL functions compatible with Π∗. We assume the input instances
are bounded s.t. |f∗(x1) − f∗(x2)| ≤ M for all x1  x2. Let L be any symmetric loss function 
k-Lipschitz and satisfying the triangle inequality. Consider a sample of Ns labeled source instances
drawn from Ps and Nt unlabeled instances drawn from µt  and then for all λ > 0  with α = kλ  we
have with probability at least 1 − δ that:

errT (f ) ≤ W1( ˆPs 

ˆP f
t ) +

c(cid:48) log(

2
δ

)

+

1√
NT

NS

+ errS(f∗) + errT (f∗) + kM φ(λ).

(cid:114) 2

(cid:18) 1√

(cid:19)

The detailed proof of Theorem 3.1 is given in the supplementary material. The previous bound on the
target error above is interesting to interpret. The ﬁrst two terms correspond to the objective function
(5) we propose to minimize accompanied with a sampling bound. The last term φ(λ) assesses the
probability under which the probabilistic Lipschitzness does not hold. The remaining two terms
involving f∗ correspond to the joint error minimizer illustrating that domain adaptation can work
only if we can predict well in both domains  similarly to existing results in the literature [23  24].
If the last terms are small enough  adaptation is possible if we are able to align well Ps and P f
t  
provided that f∗ and Π∗ verify the PTL. Finally  note that α = kλ and tuning this parameter is thus
actually related to ﬁnding the Lipschitz constants of the problem.

4 Learning with Joint Distribution OT

In this section  we provide some details about the JDOT’s optimization problem given in Equation
(5) and discuss algorithms for its resolution. We will assume that the function space H to which
f belongs is either a RKHS or a function space parametrized by some parameters w ∈ Rp. This
framework encompasses linear models  neural networks  and kernel methods. Accordingly  we
are going to deﬁne a regularization term Ω(f ) on f. Depending on how H is deﬁned  Ω(f ) is
either a non-decreasing function of the squared-norm induced by the RKHS (so that the representer
theorem is applicable) or a squared-norm on the vector parameter. We will further assume that Ω(f )
is continuously differentiable. As discussed above  f is to be learned according to the following
optimization problem

(cid:88)

i j

(cid:0)αd(xs

min

f∈H γ∈∆

γi j

j) + L(ys

i   xt

i   f (xt

j))(cid:1) + λΩ(f )

where the loss function L is continuous and differentiable with respects to its second variable. Note
that while the above problem does not involve any regularization term on the coupling matrix γ  it is
essentially for the sake of simplicity and readability. Regularizers like entropic regularization [25] 
which is relevant when the number of samples is very large  can still be used without signiﬁcant
change to the algorithmic framework.
Optimization procedure. According to the above hypotheses on f and L  Problem (6) is smooth
and the constraints are separable according to f and γ. Hence  a natural way to solve the problem (6)
is to rely on alternate optimization w.r.t. both parameters γ and f. This algorithm well-known as
Block Coordinate Descent (BCD) or Gauss-Seidel method (the pseudo code of the algorithm is given
in appendix). Block optimization steps are discussed with further details in the following.

(6)

5

i   xt

j) + L(ys

Solving with ﬁxed f boils down to a classical OT problem with a loss matrix C such that Ci j =
j)). We can use classical OT solvers such as the network simplex algorithm 
αd(xs
but other strategies can be considered  such as regularized OT [25] or stochastic versions [26].
The optimization problem with ﬁxed γ leads to a new learning problem expressed as

i   f (xt

γi jL(ys

i   f (xt

j)) + λΩ(f )

(7)

(cid:88)

i j

min
f∈H

Note how the data ﬁtting term elegantly and naturally encodes the transfer of source labels ys
i through
estimated labels of test samples with a weighting depending on the optimal transport matrix. However 
this comes at the price of having a quadratic number NsNt of terms  which can be considered as
computationally expensive. We will see in the sequel that we can beneﬁt from the structure of the
chosen loss to greatly reduce its complexity. In addition  we emphasize that when H is a RKHS 
owing to kernel trick and the representer theorem  problem (7) can be re-expressed as an optimization
problem with Nt number of parameters all belonging to R.
Let us now discuss brieﬂy the convergence of the proposed algorithm. Owing to the 2-block coordinate
descent structure  to the differentiability of the objective function in Problem (6) and constraints on f
(or its kernel trick parameters) and γ are closed  non-empty and convex  convergence result of Grippo
et al. [27] on 2-block Gauss-Seidel methods directly applies. It states that if the sequence {γk  f k}
produced by the algorithm has limit points then every limit point of the sequence is a critical point of
Problem (6).
Estimating f for least square regression problems. We detail the use of JDOT for transfer least-
square regression problem i.e when L is the squared-loss. In this context  when the optimal transport
matrix γ is ﬁxed the learning problem boils down to

min
f∈H

(cid:107)ˆyj − f (xt

j)(cid:107)2 + λ(cid:107)f(cid:107)2

1
nt

(8)

(cid:88)

j

(cid:80)

j γi jys

where the ˆyj = nt
i is a weighted average of the source target values. Note that this
simpliﬁcation results from the properties of the quadratic loss and that it may not occur for more
complex regression loss.
Estimating f for hinge loss classiﬁcation problems. We now aim at estimating a multiclass
classiﬁer with a one-against-all strategy. We suppose that the data ﬁtting is the binary squared hinge
loss of the form L(y  f (x)) = max(0  1 − yf (x))2. In a One-Against-All strategy we often use the
i k = 0. Denote as fk ∈ H the
binary matrices P such that P s
decision function related to the k-vs-all problem. The learning problem (7) can now be expressed as

i k = 1 if sample i is of class k else P s

ˆPj kL(1  fk(xt

j)) + (1 − ˆPj k)L(−1  fk(xt

j)) + λ

(cid:107)fk(cid:107)2

(9)

j k

k

(cid:88)

min
fk∈H

(cid:88)

γ(cid:62)Ps. Interestingly this formulation
where ˆP is the transported class proportion matrix ˆP = 1
Nt
illustrates that for each target sample  the data ﬁtting term is a convex sum of hinge loss for a negative
and positive label with weights in γ.

5 Numerical experiments

In this section we evaluate the performance of our method (JDOT) on two different transfer tasks of
classiﬁcation and regression on real datasets 2.
Caltech-Ofﬁce classiﬁcation dataset. This dataset [28] is dedicated to visual adaptation. It contains
images from four different domains: Amazon  the Caltech-256 image collection  Webcam and DSLR.
Several features  such as presence/absence of background  lightning conditions  image quality  etc.)
induce a distribution shift between the domains  and it is therefore relevant to consider a domain
adaptation task to perform the classiﬁcation. Following [14]  we choose deep learning features
to represent the images  extracted as the weights of the fully connected 6th layer of the DECAF
convolutional neural network [29]  pre-trained on ImageNet. The ﬁnal feature vector is a sparse 4096
dimensional vector.

2Open Source Python implementation of JDOT: https://github.com/rflamary/JDOT

6

Table 1: Accuracy on the Caltech-Ofﬁce Dataset. Best value in bold.

Domains

caltech→amazon
caltech→webcam
caltech→dslr
amazon→caltech
amazon→webcam
amazon→dslr
webcam→caltech
webcam→amazon
webcam→dslr
dslr→caltech
dslr→amazon
dslr→webcam

Mean

Mean rank

p-value

Base

92.07
76.27
84.08
84.77
79.32
86.62
71.77
79.44
96.18
77.03
83.19
96.27
83.92
5.33

SurK

91.65
77.97
82.80
84.95
81.36
87.26
71.86
78.18
95.54
76.94
82.15
92.88
83.63
5.58

< 0.01 < 0.01

SA

ARTL

OT-IT OT-MM JDOT

90.50
81.02
85.99
85.13
85.42
89.17
75.78
81.42
94.90
81.75
83.19
88.47
85.23
4.00
0.01

92.17
80.00
88.54
85.04
79.32
85.99
72.75
79.85
100.00
78.45
83.82
98.98
85.41
3.75
0.04

89.98
80.34
78.34
85.93
74.24
77.71
84.06
89.56
99.36
85.57
90.50
96.61
86.02
3.50
0.25

92.59
78.98
76.43
87.36
85.08
79.62
82.99
90.50
99.36
83.35
90.50
96.61
86.95
2.83
0.86

91.54
88.81
89.81
85.22
84.75
87.90
82.64
90.71
98.09
84.33
88.10
96.61
89.04
2.50
−

Table 2: Accuracy on the Amazon review experiment. Maximum value in bold font.

Domains
books→dvd
books→kitchen
books→electronics
dvd→books
dvd→kitchen
dvd→electronics
kitchen→books
kitchen→dvd

kitchen→electronics
electronics→books
electronics→dvd
electronics→kitchen

Mean
p-value

NN

0.805
0.768
0.746
0.725
0.760
0.732
0.704
0.723
0.847
0.713
0.726
0.855

0.759
0.004

DANN JDOT (mse)
0.806
0.767
0.747
0.747
0.765
0.738
0.718
0.730
0.846
0.718
0.726
0.850

0.794
0.791
0.778
0.761
0.811
0.778
0.732
0.764
0.844
0.740
0.738
0.868

0.763
0.006

0.783
0.025

JDOT (Hinge)

0.795
0.794
0.781
0.763
0.821
0.788
0.728
0.765
0.845
0.749
0.737
0.872

0.787

−

We compare our method with four other methods: the surrogate kernel approach ([4]  denoted
SurK)  subspace adaptation for its simplicity and good performances on visual adaptation ([8]  SA) 
Adaptation Regularization based Transfer Learning ([30]  ARTL)  and the two variants of regularized
optimal transport [14]: entropy-regularized OT-IT and classwise regularization implemented with the
Majoration-Minimization algorithm OT-MM  that showed to give better results in practice than its
group-lasso counterpart. The classiﬁcation is conducted with a SVM together with a linear kernel for
every method. Its results when learned on the source domain and tested on the target domain are also
reported to serve as baseline (Base). All the methods have hyper-parameters  that are selected using
the reverse cross-validation of Zhong and colleagues [31].The dimension d for SA is chosen from
{1  4  7  . . .   31}. The entropy regularization for OT-IT and OT-MM is taken from {102  . . .   105} 
102 being the minimum value for the Sinkhorn algorithm to prevent numerical errors. Finally the η
parameter of OT-MM is selected from {1  . . .   105} and the α in JDOT from {10−5  10−4  . . .   1}.
The classiﬁcation accuracy for all the methods is reported in Table 1. We can see that JDOT
is consistently outperforming the baseline (5 points in average)  indicating that the adaptation is
successful in every cases. Its mean accuracy is the best as well as its average ranking. We conducted
a Wilcoxon signed-rank test to test if JDOT was statistically better than the other methods  and
report the p-value in the tables. This test shows that JDOT is statistically better than the considered
methods  except for OT based ones that where state of the art on this dataset [14].
Amazon review classiﬁcation dataset We now consider the Amazon review dataset [32] which
contains online reviews of different products collected on the Amazon website. Reviews are encoded
with bag-of-word unigram and bigram features as input. The problem is to predict positive (higher
than 3 stars) or negative (3 stars or less) notation of reviews (binary classiﬁcation). Since different

7

Table 3: Comparison of different methods on the Wiﬁ localization dataset. Maximum value in bold.

Domains
t1 → t2
t1 → t3
t2 → t3
hallway1
hallway2
hallway3

KRR

80.84±1.14
76.44±2.66
67.12±1.28
60.02 ±2.60
49.38 ± 2.30
48.42 ±1.32

SurK

90.36±1.22
94.97±1.29
85.83 ± 1.31
76.36 ± 2.44
64.69 ±0.77
65.73 ± 1.57

DIP

87.98±2.33
84.20±4.29
80.58 ± 2.10
77.48 ± 2.68
78.54 ± 1.66
75.10± 3.39

DIP-CC
91.30±3.24
84.32±4.57
81.22 ± 4.31
76.24± 5.14
77.8± 2.70
73.40± 4.06

GeTarS

86.76 ± 1.91
90.62±2.25
82.68 ± 3.71
84.38 ± 1.98
77.38 ± 2.09
80.64 ± 1.76

CTC

89.36±1.78
94.80±0.87
87.92 ± 1.87
86.98 ± 2.02
87.74 ± 1.89
82.02± 2.34

CTC-TIP
89.22±1.66
92.60 ± 4.50
89.52 ± 1.14
86.78 ± 2.31
87.94 ± 2.07
81.72 ± 2.25

JDOT

93.03 ± 1.24
90.06 ± 2.01
86.76 ± 1.72
98.83±0.58
98.45±0.67
99.27±0.41

i   xt

words are employed to qualify the different categories of products  a domain adaptation task can be
formulated if one wants to predict positive reviews of a product from labelled reviews of a different
product. Following [33  11]  we consider only a subset of four different types of product: books 
DVDs  electronics and kitchens. This yields 12 possible adaptation tasks. Each domain contains
2000 labelled samples and approximately 4000 unlabelled ones. We therefore use these unlabelled
samples to perform the transfer  and test on the 2000 labelled data.
The goal of this experiment is to compare to the state-of-the-art method on this subset  namely
Domain adversarial neural network ([11]  denoted DANN)  and to show the versatility of our method
that can adapt to any type of classiﬁer. The neural network used for all methods in this experiment is
a simple 2-layer model with sigmoid activation function in the hidden layer to promote non-linearity.
50 neurons are used in this hidden layer. For DANN  hyper-parameters are set through the reverse
cross-validation proposed in [11]  and following the recommendation of authors the learning rate
is set to 10−3. In the case of JDOT  we used the heuristic setting of α = 1/ maxi j d(xs
j)  and
as such we do not need any cross-validation. The squared Euclidean norm is used for both metric
in feature space and we test as loss functions both mean squared errors (mse) and Hinge losses. 10
iterations of the block coordinate descent are realized. For each method  we stop the learning process
of the network after 5 epochs. Classiﬁcation accuracies are presented in table 2. The neural network
(NN)  trained on source and tested on target  is also presented as a baseline. JDOT surpasses DANN
in 11 out of 12 tasks (except on books→dvd). The Hinge loss is better in than mse in 10 out of 12
cases  which is expected given the superiority of the Hinge loss on classiﬁcation tasks [19].
Wiﬁ localization regression dataset For the regression task  we use the cross-domain indoor Wiﬁ
localization dataset that was proposed by Zhang and co-authors [4]  and recently studied in [5]. From
a multi-dimensional signal (collection of signal strength perceived from several access points)  the
goal is to locate the device in a hallway  discretized into a grid of 119 squares  by learning a mapping
from the signal to the grid element. This translates as a regression problem. As the signals were
acquired at different time periods by different devices  a shift can be encountered and calls for an
adaptation. In the remaining  we follow the exact same experimental protocol as in [4  5] for ease of
comparison. Two cases of adaptation are considered: transfer across periods  for which three time
periods t1  t2 and t3 are considered  and transfer across devices  where three different devices are
used to collect the signals in the same straight-line hallways (hallway1-3)  leading to three different
adaptation tasks in both cases.
We compare the result of our method with several state-of-the-art methods: kernel ridge regression
with RBF kernel (KRR)  surrogate kernel ([4]  denoted SurK)  domain-invariant projection and its
cluster regularized version ([7]  denoted respectively DIP and DIP-CC)  generalized target shift ([34] 
denoted GeTarS)  and conditional transferable components  with its target information preservation
regularization ([5]  denoted respectively CTC and CTC-TIP). As in [4  5]  the hyper-parameters of
the competing methods are cross-validated on a small subset of the target domain. In the case of
JDOT  we simply set the α to the heuristic value of α = 1/ maxi j d(xs
j) as discussed previously 
and f is estimated with kernel ridge regression.
Following [4]  the accuracy is measured in the following way: the prediction is said to be correct if it
falls within a range of three meters in the transfer across periods  and six meters in the transfer across
devices. For each experiment  we randomly sample sixty percent of the source and target domain  and
report the mean and standard deviation of ten repetitions accuracies in Table 3. For transfer across
periods  JDOT performs best in one out of three tasks. For transfer across devices  the superiority of
JDOT is clearly assessed  for it reaches an average score > 98%  which is at least ten points ahead
of the best competing method for every task. Those extremely good results could be explained by the
fact that using optimal transport allows to consider large shifts of distribution  for which divergences
(such as maximum mean discrepancy used in CTC) or reweighting strategies can not cope with.

i   xt

8

6 Discussion and conclusion

We have presented in this paper the Joint Distribution Optimal Transport for domain adaptation 
which is a principled way of performing domain adaptation with optimal transport. JDOT assumes
the existence of a transfer map that transforms a source domain joint distribution Ps(X  Y ) into
a target domain equivalent version Pt(X  Y ). Through this transformation  the alignment of both
feature space and conditional distributions is operated  allowing to devise an efﬁcient algorithm that
simultaneously optimizes for a coupling between Ps and Pt and a prediction function that solves the
transfer problem. We also proved that learning with JDOT is equivalent to minimizing a bound on
the target distribution. We have demonstrated through experiments on classical real-world benchmark
datasets the superiority of our approach w.r.t. several state-of-the-art methods  including previous
work on optimal transport based domain adaptation  domain adversarial neural networks or transfer
components  on a variety of task including classiﬁcation and regression. We have also showed the
versatility of our method  that can accommodate with several types of loss functions (mse  hinge) or
class of hypothesis (including kernel machines or neural networks). Potential follow-ups of this work
include a semi-supervised extension (using unlabelled examples in source domain) and investigating
stochastic techniques for solving efﬁciently the adaptation. From a theoretical standpoint  future
works include a deeper study of probabilistic transfer lipschitzness and the development of guarantees
able to take into the complexity of the hypothesis class and the space of possible transport plans.

Acknowledgements

This work beneﬁted from the support of the project OATMIL ANR-17-CE23-0012 of the French
National Research Agency (ANR)  the Normandie Projet GRR-DAISI  European funding FEDER
DAISI and CNRS funding from the Déﬁ Imag’In. The authors also wish to thank Kai Zhang and
Qiaojun Wang for providing the Wiﬁ localization dataset.

References
[1] V. M. Patel  R. Gopalan  R. Li  and R. Chellappa. Visual domain adaptation: an overview of recent

advances. IEEE Signal Processing Magazine  32(3)  2015.

[2] S. J. Pan and Q. Yang. A survey on transfer learning.

Engineering  22(10):1345–1359  2010.

IEEE Transactions on Knowledge and Data

[3] M. Sugiyama  S. Nakajima  H. Kashima  P.V. Buenau  and M. Kawanabe. Direct importance estimation

with model selection and its application to covariate shift adaptation. In NIPS  2008.

[4] K. Zhang  V. W. Zheng  Q. Wang  J. T. Kwok  Q. Yang  and I. Marsic. Covariate shift in Hilbert space: A

solution via surrogate kernels. In ICML  2013.

[5] M. Gong  K. Zhang  T. Liu  D. Tao  C. Glymour  and B. Schölkopf. Domain adaptation with conditional

transferable components. In ICML  volume 48  pages 2839–2848  2016.

[6] B. Gong  Y. Shi  F. Sha  and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In

CVPR  2012.

[7] M. Baktashmotlagh  M. Harandi  B. Lovell  and M. Salzmann. Unsupervised domain adaptation by domain

invariant projection. In ICCV  pages 769–776  2013.

[8] B. Fernando  A. Habrard  M. Sebban  and T. Tuytelaars. Unsupervised visual domain adaptation using

subspace alignment. In ICCV  2013.

[9] M. Long  J. Wang  G. Ding  J. Sun  and P. Yu. Transfer joint matching for unsupervised domain adaptation.

In CVPR  pages 1410–1417  2014.

[10] Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation.

1180–1189  2015.

In ICML  pages

[11] Y. Ganin  E. Ustinova  H. Ajakan  P. Germain  H. Larochelle  F. Laviolette  M. Marchand  and V. Lempitsky.
Domain-adversarial training of neural networks. Journal of Machine Learning Research  17(59):1–35 
2016.

9

[12] S. Si  D. Tao  and B. Geng. Bregman divergence-based regularization for transfer subspace learning. IEEE

Transactions on Knowledge and Data Engineering  22(7):929–942  July 2010.

[13] N. Courty  R. Flamary  and D. Tuia. Domain adaptation with regularized optimal transport. In ECML/PKDD 

2014.

[14] N. Courty  R. Flamary  D. Tuia  and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE

Transactions on Pattern Analysis and Machine Intelligence  2016.

[15] L. Kantorovich. On the translocation of masses. C.R. (Doklady) Acad. Sci. URSS (N.S.)  37:199–201 

1942.

[16] F. Santambrogio. Optimal transport for applied mathematicians. Birkäuser  NY  2015.

[17] M. Perrot  N. Courty  R. Flamary  and A. Habrard. Mapping estimation for discrete optimal transport. In

NIPS  pages 4197–4205  2016.

[18] C. Villani. Optimal transport: old and new. Grund. der mathematischen Wissenschaften. Springer  2009.

[19] Lorenzo Rosasco  Ernesto De Vito  Andrea Caponnetto  Michele Piana  and Alessandro Verri. Are loss

functions all the same? Neural Computation  16(5):1063–1076  2004.

[20] M. Thorpe  S. Park  S. Kolouri  G. Rohde  and D. Slepcev. A transportation lp distance for signal analysis.

CoRR  abs/1609.08669  2016.

[21] R. Urner  S. Shalev-Shwartz  and S. Ben-David. Access to unlabeled data can speed up prediction time. In

Proceedings of ICML  pages 641–648  2011.

[22] S. Ben-David  S. Shalev-Shwartz  and R. Urner. Domain adaptation–can quantity compensate for quality?

In Proc of ISAIM  2012.

[23] Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In

Proc. of COLT  2009.

[24] S. Ben-David  J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman Vaughan. A theory of

learning from different domains. Machine Learning  79(1-2):151–175  2010.

[25] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS  2013.

[26] A. Genevay  M. Cuturi  G. Peyré  and F. Bach. Stochastic optimization for large-scale optimal transport. In

NIPS  pages 3432–3440  2016.

[27] Luigi Grippo and Marco Sciandrone. On the convergence of the block nonlinear gauss–seidel method

under convex constraints. Operations research letters  26(3):127–136  2000.

[28] K. Saenko  B. Kulis  M. Fritz  and T. Darrell. Adapting visual category models to new domains. In ECCV 

LNCS  pages 213–226  2010.

[29] J. Donahue  Y. Jia  O. Vinyals  J. Hoffman  N. Zhang  E. Tzeng  and T. Darrell. Decaf: A deep convolutional

activation feature for generic visual recognition. In ICML  2014.

[30] M. Long  J. Wang  G. Ding  S. Jialin Pan  and P.S. Yu. Adaptation regularization: A general framework for

transfer learning. IEEE TKDE  26(7):1076–1089  2014.

[31] E. Zhong  W. Fan  Q. Yang  O. Verscheure  and J. Ren. Cross validation framework to choose amongst

models and datasets for transfer learning. In ECML/PKDD  2010.

[32] J. Blitzer  R. McDonald  and F. Pereira. Domain adaptation with structural correspondence learning. In
Proc. of the 2006 conference on empirical methods in natural language processing  pages 120–128  2006.

[33] M. Chen  Z. Xu  K. Weinberger  and F. Sha. Marginalized denoising autoencoders for domain adaptation.

In ICML  2012.

[34] K. Zhang  M. Gong  and B. Schölkopf. Multi-source domain adaptation: A causal view.

Conference on Artiﬁcial Intelligence  pages 3150–3157  2015.

In AAAI

10

,Nicolas Courty
Rémi Flamary
Amaury Habrard
Alain Rakotomamonjy