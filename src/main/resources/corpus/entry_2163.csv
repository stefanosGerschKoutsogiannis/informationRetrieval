2014,From MAP to Marginals: Variational Inference in Bayesian Submodular Models,Submodular optimization has found many applications in machine learning and beyond. We carry out the first systematic investigation of inference in probabilistic models defined through submodular functions  generalizing regular pairwise MRFs and Determinantal Point Processes. In particular  we present L-Field  a variational approach to general log-submodular and log-supermodular distributions based on sub- and supergradients. We obtain both lower and upper bounds on the log-partition function  which enables us to compute probability intervals for marginals  conditionals and marginal likelihoods. We also obtain fully factorized approximate posteriors  at the same computational cost as ordinary submodular optimization. Our framework results in convex problems for optimizing over differentials of submodular functions  which we show how to optimally solve. We provide theoretical guarantees of the approximation quality with respect to the curvature of the function. We further establish natural relations between our variational approach and the classical mean-field method. Lastly  we empirically demonstrate the accuracy of our inference scheme on several submodular models.,From MAP to Marginals: Variational Inference in

Bayesian Submodular Models

Department of Computer Science

Department of Computer Science

Andreas Krause

ETH Z¨urich

krausea@ethz.ch

Josip Djolonga

ETH Z¨urich

josipd@inf.ethz.ch

Abstract

Submodular optimization has found many applications in machine learning and
beyond. We carry out the ﬁrst systematic investigation of inference in probabilis-
tic models deﬁned through submodular functions  generalizing regular pairwise
MRFs and Determinantal Point Processes. In particular  we present L-FIELD  a
variational approach to general log-submodular and log-supermodular distribu-
tions based on sub- and supergradients. We obtain both lower and upper bounds
on the log-partition function  which enables us to compute probability intervals
for marginals  conditionals and marginal likelihoods. We also obtain fully factor-
ized approximate posteriors  at the same computational cost as ordinary submod-
ular optimization. Our framework results in convex problems for optimizing over
differentials of submodular functions  which we show how to optimally solve.
We provide theoretical guarantees of the approximation quality with respect to
the curvature of the function. We further establish natural relations between our
variational approach and the classical mean-ﬁeld method. Lastly  we empirically
demonstrate the accuracy of our inference scheme on several submodular models.

Introduction

1
Submodular functions [1] are a rich class of set functions F : 2V → R  investigated originally
in game theory and combinatorial optimization. They capture natural notions such as diminishing
returns and economies of scale. In recent years  submodular optimization has seen many important
applications in machine learning  including active learning [2]  recommender systems [3]  document
summarization [4]  representation learning [5]  clustering [6]  the design of structured norms [7] etc.
In this work  instead of using submodular functions to obtain point estimates through optimiza-
tion  we take a Bayesian approach and deﬁne probabilistic models over sets (so called point pro-
cesses) using submodular functions. Many of the aforementioned applications can be understood
as performing MAP inference in such models. We develop L-FIELD  a general variational infer-
ence scheme for reasoning about log-supermodular (P (A) ∝ exp(−F (A))) and log-submodular
(P (A) ∝ exp(F (A))) distributions  where F is a submodular set function.
Previous work. There has been extensive work on submodular optimization (both approximate and
exact minimization and maximization  see  e.g.  [8  9  10  11]). In contrast  we are unaware of pre-
vious work that addresses the general problem of probabilistic inference in Bayesian submodular
models. There are two important special cases that have received signiﬁcant interest. The most
prominent examples are undirected pairwise Markov Random Fields (MRFs) with binary variables 
also called the Ising model [12]  due to their importance in statistical physics  and applications  e.g. 
in computer vision. While MAP inference is efﬁcient for regular (log-supermodular) MRFs  com-
puting the partition function is known to be #P-hard [13]  and the approximation problem has been
also shown to be hard [14]. Also  there is no FPRAS in the log-submodular case unless RP=NP [13].
An important case of log-submodular distributions is the Determinantal Point Process (DPP)  used

1

in machine learning as a principled way of modeling diversity. Its partition function can be com-
puted efﬁciently  and a 1
4-approximation scheme for ﬁnding the (NP-hard) MAP [15] is known. In
this paper  we propose a variational inference scheme for general Bayesian submodular models  that
encompasses these two and many other distributions  and has instance-dependent quality guaran-
tees. A hallmark of the models is that they capture high-order interactions between many random
variables. Existing variational approaches [16] cannot efﬁciently cope with such high-order interac-
tions — they generally have to sum over all variables in a factor  scaling exponentially in the size of
the factor. We discuss this prototypically for mean-ﬁeld in Sec. 5.
Our contributions. In summary  our main contributions are

log-supermodular distributions  that can capture high-order variable interactions.

• We provide the ﬁrst general treatment of probabilistic inference with log-submodular and
• We develop L-FIELD  a novel variational inference scheme that optimizes over sub- and
supergradients of submodular functions. Our scheme yields both upper and lower bounds
on the partition function  which imply rigorous probability intervals for marginals. We can
also obtain factorial approximations of the distribution at no larger computational cost than
performing MAP inference in the model (for which a plethora of algorithms are available).
• We identify a natural link between our scheme and the well-known mean-ﬁeld method.
• We establish theoretical guarantees about the accuracy of our bounds  dependent on the
• We demonstrate the accuracy of L-FIELD on several Bayesian submodular models.

curvature of the underlying submodular function.

2 Submodular functions and optimization

Submodular functions are set functions satisfying a diminishing returns condition. Formally  let V
be some ﬁnite ground set  w.l.o.g. V = {1  . . .   n}  and consider a set function F : 2V → R. The
marginal gain of adding item i ∈ V to the set A ⊆ V w.r.t. F is deﬁned as F (i|A) = F (A ∪ {i}) −
F (A). Then  a function F : 2V → R is said to be submodular if for all A ⊆ B ⊆ V and i ∈ V − B
it holds that F (i|A) ≥ F (i|B). A function F is called supermodular if −F is submodular. Without
loss of generality1  we will also make the assumption that F is normalized so that F (∅) = 0.
The problem of submodular function optimization has received signiﬁcant attention. The (uncon-
strained) minimization of submodular functions  minA F (A)  can be done in polynomial time.
While general purpose algorithms [8] can be impractical due to their high order  several classes
of functions admit faster  specialized algorithms  e.g. [17  18  19]. Many important problems can
be cast as the minimization of a submodular objective  ranging from image segmentation [20  12] to
clustering [6]. Submodular maximization has also found numerous applications  e.g. experimental
design [21]  document summarization [4] or representation learning [5]. While this problem is in
general NP-hard  effective constant-factor approximation algorithms exist (e.g. [22  11]).
In this paper we lift results from submodular optimization to probabilistic inference  which lets us
quantify uncertainty about the solutions of the problem  instead of binding us to a single one. Our
approach allows us to obtain (approximate) marginals at the same cost as traditional MAP inference.

normalizing quantity Z = (cid:80)

3 Probabilistic inference in Bayesian submodular models
Which Bayesian models are associated with submodular functions? Suppose F : 2V → R is a sub-
modular set function. We consider distributions over subsets2 A ⊆ V of the form P (A) = 1Z e+F (A)
and P (A) = 1Z e−F (A)  which we call log-submodular and log-supermodular  respectively. The
S⊆V e±F (S) is called the partition function  and − log Z is also
known as free energy in the statistical physics literature. Note that distributions over subsets of V
are isomorphic to distributions of |V | = n binary random variables X1  . . .   Xn ∈ {0  1} — we
simply identify Xi as the indicator function of the event i ∈ A  or formally Xi = [i ∈ A].
Examples of log-supermodular distributions. There are many distributions that ﬁt this frame-
work. As a prominent example  consider binary pairwise Markov random ﬁelds (MRFs) 

1The functions F (A) and F (A) + c encode the same distributions by virtue of normalization.
2In the appendix we also consider cardinality constraints  i.e.  distributions over sets A that satisfy |A| ≤ k.

2

(cid:81)

are equivalent to distributions P (A) ∝ exp(−F (A))  where F (A) =(cid:80)

tion j. Then  we consider F (A) =(cid:80)

concave. Then  the function F (A) =(cid:80)k

i j φi j(Xi  Xj). Assuming the potentials φi j are positive  such MRFs
P (X1  . . .   Xn) = 1Z
i j Fi j(A)  and Fi j(A) =
− log φi j([i ∈ A]  [j ∈ A]). An MRF is called regular iff each Fi j is submodular (and con-
sequently P (A) is log-supermodular). Such models are extensively used in applications  e.g. in
computer vision [12]. More generally  a rich class of distributions can be deﬁned using decompos-
able submodular functions  which can be written as sums of (usually simpler) submodular functions.
As an example  let G1  . . .   Gk ⊆ V be groups of elements and let φ1  . . .   φk : [0 ∞) → R be
i=1 φi(|Gi ∩ A|) is submodular. Models using these types
of functions strictly generalize pairwise MRFs  and can capture higher-order variable interactions 
which can be crucial in computer vision applications such as semantic segmentation (e.g. [23]).
Examples of log-submodular distributions. A prominent example of log-submodular distributions
are Determinantal Point Processes (DPPs) [24]. A DPP is a distribution over sets A of the form
P (A) = 1Z exp(F (A))  where F (A) = log |KA|. Here  K ∈ RV ×V is a positive semi-deﬁnite
matrix  KA is the square submatrix indexed by A  and | · | denotes the determinant. Because K is
positive semi-deﬁnite  F (A) is known to be submodular  and hence DPPs are log-submodular. An-
other natural model is that of facility location. Assume that we have a set of locations V where we
can open shops  and a set N of customers that we would like to serve. For each customer i ∈ N and
location j ∈ V we have a non-negative number Ci j quantifying how much service i gets from loca-
i∈N maxj∈A Ci j. We can also penalize the number of open
shops and use a distribution P (A) ∝ exp(F (A) − λ|A|) for λ > 0. Such objectives have been used
for optimization in many applications  ranging from clustering [25] to recommender systems [26].
The Inference Challenge. Having introduced the models that we consider  we now show how to do
inference in them3. Let us introduce the following operations that preserve submodularity.
Deﬁnition 1. Let F : 2V → R be submodular and let X  Y ⊆ V . Deﬁne the submodular functions
F X as the restriction of F to 2X  and FX : 2V −X → R as FX (A) = F (A ∪ X) − F (X).
First  let us see how to compute marginals. The probability that the random subset S distributed as
P (S = A) ∝ exp(−F (A)) is in some non-empty lattice [X  Y ] = {A | X ⊆ A ⊆ Y } is equal to
P (S ∈ [X  Y ]) =
(1)
where Z Y
A⊆Y −X e−(F (X∪A)−F (X)) is the partition function of (FX )Y . Marginals P (i ∈ S)
of any i ∈ V can be obtained using [{i}  V ]. We also obtain conditionals — if  for example  we
X if A ∈ [X  Y ] 
condition on the event on (1)  we have P (S = A|S ∈ [X  Y ]) = exp(−F (A))/Z Y
0 otherwise. Note that log-supermodular distributions are conjugate with each other: for a log-
supermodular prior P (A) ∝ exp(−F (A)) and a likelihood function4 P (E | A) ∝ exp(−L(E; A)) 
for which L is submodular w.r.t. A for each evidence E  the posterior P (A | E) ∝ exp(−(F (A) +
L(E; A))) is log-supermodular as well. The same holds for log-submodular distributions.
4 The variational approach
In Section 3 we have seen that due to the closure properties of submodular functions  important in-
ference tasks (e.g.  marginals  conditioning) in Bayesian submodular models require computing par-
tition functions of suitably deﬁned/restricted submodular functions. Given that the general problem
is #P hard  we seek approximate methods. The main idea is to exploit the peculiar property of sub-
modular functions that they can be both lower- and upper-bounded using simple additive functions
i∈A s({i}).
We will also treat modular functions s(·) as vectors s ∈ RV with coordinates si = s({i}). Because
modular functions have tractable log-partition functions  we obtain the following bounds.
Lemma 1. If ∀A ⊆ V : sl(A) + cl ≤ F (A) ≤ su(A) + cu for modular su  sl  and cl  cu ∈ R  then

of the form s(A) + c  where c ∈ R and s : 2V → R is modular  i.e. it satisﬁes s(A) =(cid:80)

exp(−F (X ∪ A)) = e−F (X)Z Y
XZ  

exp(−F (A)) =

1
Z

X =(cid:80)

(cid:88)

1
Z

X⊆A⊆Y

(cid:88)

A⊆Y −X

log Z +(sl  cl) ≤ log(cid:80)
log Z−(su  cu) ≤ log(cid:80)

where log Z +(s  c) = c +(cid:80)

A⊆V exp(+F (A)) ≤ log Z +(su  cu) and
A⊆V exp(−F (A)) ≤ log Z−(sl  cl) 

i∈V log(1 + esi ) and log Z−(s  c) = −c +(cid:80)

i∈V log(1 + e−si).

3We consider log-supermodular distributions  as the log-submodular case is analogous.
4Such submodular loss functions L have been considered  e.g.  in document summarization [4].

3

We can use any modular (upper or lower) bound s(A) + c to deﬁne a completely factorized
distribution that can be used as a proxy to approximate values of interest of the original distribution.
For example  the marginal of i ∈ A under Q(A) ∝ exp(−s(A) + c) is easily seen to be 1/(1 + esi).
Instead of optimizing over all possible bounds of the above form  we consider for each X ⊆ V two
sets of modular functions  which are exact at X and lower- or upper-bound F respectively. Similarly
as for convex functions  we deﬁne [8][§6.2] the subdifferential of F at X as

∂F (X) = {s ∈ Rn | ∀Y ⊆ V : F (Y ) ≥ F (X) + s(Y ) − s(X)}.

(2)

The superdifferential ∂F (X) is deﬁned analogously by inverting the inequality sign [27]. For each
subgradient s ∈ ∂F (X)  the function gX (Y ) = s(Y ) + F (X) − s(X) is lower bounding F .
Similarly  for a supergradient s ∈ ∂F (X)  hX (Y ) = s(Y ) + F (X)− s(X) is an upper bound of F .
Note that both hX and gX are of the form that we considered (modular plus constant) and are tight at
X  i.e. hX (X) = gX (X) = F (X). Because we will be optimizing over differentials  we deﬁne for
X (s) = Z−(s  F (X) − s(X)).
any X ⊆ V the shorthands Z +

X (s) = Z +(s  F (X) − s(X)) and Z−

4.1 Optimizing over subgradients
To analyze the problem of minimizing log Z−
X (s) subject to s ∈ ∂F (X)  we introduce the base
polyhedron of F   deﬁned as B(F ) = {s ∈ RV | s(V ) = F (V ) and ∀A ⊆ V : s(A) ≤ F (A)}  i.e.
the set of modular lower bounds that are exact at V . As the following lemma shows  we do not have
to consider log Z−
Lemma 2. For all X ⊆ V we have mins∈∂F (∅) Z−
problem is equivalent to

X for all X and we can restrict our attention to the case X = ∅.

∅ (s) ≤ mins∈∂F (X) Z−

X (s). Moreover  the former

log(1 + e−si)

subject to

s ∈ B(F ).

(3)

(cid:88)

i∈V

minimize

s

Thus  we have to optimize a convex function over B(F )  a problem that has been already con-
sidered [8  9]. For example  we can use the Frank-Wolfe algorithm [28  29]  which is easy to
implement and has a convergence rate of O( 1
It requires the optimization of linear functions
g(s) = (cid:104)w  s(cid:105) = wT s over the domain  which  as shown by Edmonds [1]  can be done greedily in
O(|V | log |V |) time. More precisely  to compute a maximizer s∗ ∈ B(F ) of g(s)  pick a bijection
σ : {1  . . .  |V |} → V that orders w  i.e. wσ(1) ≥ wσ(2) ≥ ··· ≥ wσ(|V |). Then  set s∗
σ(i) =
F (σ(i)|{σ(1)  . . .   σ(i − 1)}). Alternatively  if we can efﬁciently minimize the sum of the function
plus a modular term  e.g. for the family of graph-cut representable functions [10]  we can apply the
divide-and-conquer algorithm [9][§9.1]  which needs the minimization of O(|V |) problems.

k ).

Deﬁne f (x) = log(1 + e−x) (cid:46) Elementwise.
for k ← 1  2  . . .   T do

1: procedure FRANK-WOLFE(F   x1  )
2:
3:
4:
5:
6:
7:
8:

Pick s ∈ argminx∈B(F )(cid:104)x ∇f (xk)(cid:105)
if (cid:104)xk − s ∇f (xk)(cid:105) ≤  then

return xk
(cid:46) Small duality gap.
xk+1 = (1− γk)xk + γks; γk = 2

else

k+2

|V | 1; A∗ ← minimizer of F (·) − s(·)

s ← F (V )
if F (A∗) = s(A∗) then

1: procedure DIVIDE-CONQUER(F )
2:
3:
4:
5:
6:
7:
8:

return s
sA ←DIVIDE-CONQUER(F A)
sV −A ←DIVIDE-CONQUER(FA)
return (sA  sV −A)

else

The entropy viewpoint and the Fenchel dual. Interestingly  (3) can be interpreted as a maximum
entropy problem. Recall that  for s ∈ B(F ) we use the distribution P (A) ∝ exp(−s(A))  whose
entropy is exactly the negative of our objective. Hence  we can consider Problem (3) as that of
maximizing the entropy over the set of factorized distributions with parameters in −B(F ). We can
go back to the standard representation using the marginals p via pi = 1/(1+exp(si)). This becomes
obvious if we consider the Fenchel dual of the problem  which  as discussed in §5  allows us to make
connections with the classical mean-ﬁeld approach. To this end  we introduce the Lov`asz extension 
deﬁned for any F : 2V → R as the support function over B(F )  i.e. f (p) = sups∈B(F ) sT p [30].
Let us also deﬁne for p ∈ [0  1]V by H[p] the Shannon entropy of a vector of |V | independent
Bernoulli random variables with success probabilities p.

4

Lemma 3. The Fenchel dual problem of Problem (3) is

(cid:16)

maximize
p∈[0 1]V

H[p] − f (p).

(cid:17)

(4)

(5)

Moreover  there is zero duality gap  and the pair (s∗  p∗) is primal-dual optimal if and only if

p∗ =

1

1 + exp(s∗
i )

  . . .  

1

1 + exp(s∗
n)

and

f (p∗) = p∗T s∗.

From the discussion above  it can be easily seen that the Fenchel dual reparameterizes the prob-
lem from the parameters −s to the marginals p. Note that the dual lets us provide a certiﬁcate of
optimality  as the Lov´asz extension can be computed with Edmonds’ greedy algorithm.

4.2 Optimizing over supergradients
To optimize over subgradients  we pick for each set X ⊆ V a representative supergradient and
optimize over all X. As in [27]  we consider the following supergradients  elements of ∂F (X).

Grow supergradient ˆsX
i ∈ X ˆsX ({i}) = F (i|V − {i})
i /∈ X ˆsX ({i}) = F (i|X)

Shrink supergradient ˇsX
ˇsX ({i}) = F (i|X − {i})
ˇsX ({i}) = F ({i})

Bar supergradient sX
sX ({i}) = F (i|V − {i})
sX ({i}) = F ({i})

Optimizing the bound over bar supergradients requires the minimization of the original function
plus a modular term. As already mentioned for the divide-and-conquer strategy above  we can do
this efﬁciently for several problems. The exact formulation of the problem is presented below.
Lemma 4. Deﬁne the modular functions m1({i}) = log(1 + e−F (i|V −i)) − log(1 + eF (i))  and
m2({i}) = log(1 + eF (i|V −i)) − log(1 + e−F (i)). The following pairs of problems are equivalent.

minimizeX log Z +
maximizeX log Z−

X (sX ) ≡ minimizeX F (X) + m1(X)
X (sX ) ≡ minimizeX F (X) − m2(X)

Even though we cannot optimize over grow and shrink supergradients  we can evaluate all three at
the optimum for the problems above and pick the one that gives the best bound.

5 Mean-ﬁeld methods and the multi-linear extension
Is there a relation to traditional variational methods? If Q(·) is a distribution over subsets of V   then
0 ≤ KL(Q || P ) = EQ
= log Z − H[Q] + EQ[F ] 
which yields the bound log Z ≥ H[Q] − EQ[F ]. The mean-ﬁeld method restricts Q to be a com-
pletely factorized distribution  so that elements are picked independently and Q can be described by
the vector of marginals q ∈ [0  1]V   over which it is then optimized. Compare this with our approach.

= log Z + EQ

exp(−F (S))

Q(S)
P (S)

Q(S)

(cid:105)

(cid:104)

log

(cid:104)

log

(cid:105)

Mean-Field Objective
maximizeq∈[0 1]V H[q] − Eq[F ]
(cid:46) Non-concave  can be hard to evaluate.

Our Objective: L-FIELD
maximizeq∈[0 1]V H[q] − f (q)
(cid:46) Concave  efﬁcient to evaluate.

(cid:81)
i q[i∈A]

˜f (q) = Eq[F ] =(cid:80)

Both the Lov´asz extension f (q) and the multi-linear extension ˜f (q) = Eq[F ] are continuous
extensions of F   introduced for submodular minimization [30] and maximization [31]  respec-
tively. The former agrees with the convex envelope of F and can be efﬁciently evaluated (in
O(|V |) evaluations of F ) using Edmonds’ greedy algorithm (cf.  §4.1  [1]). In contrast  evaluating
(1 − qi)[i /∈A]F (A) in general requires summing over exponen-
tially many terms – a problem potentially as hard as the original inference problem! Even if ˜f (q)
is approximated by sampling  it is neither convex nor concave. Moreover  computing the coordinate
ascent updates of mean-ﬁeld can be intractable for general F . Hence  our approach can be motivated
as follows: instead of using the multi-linear extension ˜f  we use the Lov´asz extension f of F   which
makes the problem convex and tractable. This analogy motivated the name L-FIELD (L for Lov´asz).

A⊆V

i

5

6 Curvature-dependent approximation bounds

How accurate are the bounds obtained via our variational approach? We now provide theoreti-
cal guarantees on the approximation quality as a function of the curvature of F   which quantiﬁes
how far the function is from modularity. Curvature is deﬁned for polymatroid functions  which
are normalized non-decreasing submodular functions  i.e.  a submodular function F : 2V → R is
polymatroid if for all A ⊆ B ⊆ V it holds that F (A) ≤ F (B).
Deﬁnition 2 (From [32]). Let G : 2V → R be a polymatroid function. The curvature κ of G is
deﬁned as 5 κ = 1 − mini∈V : G({i})>0

G(i|V −{i})

.

G({i})

The curvature is always between 0 and 1 and is equal to 0 if and only if the function is modular.
Although the curvature is a notion for polymatroid functions  we can still show results for the general
case as any submodular function F can be decomposed [33] as the sum of a modular term m(·)
deﬁned as m({i}) = F (i|V − {i}) and G = F − m  which is a polymatroid function. Our bounds

below depend on the curvature of G and GMAX = G(V ) = F (V ) −(cid:80)

i∈V F (i|V − i).

Theorem 1. Let F = G + m  where G is polymatroid with curvature κ and m is modular deﬁned as
above. Pick any bijection σ : V → {1  2  . . .  |V |} and deﬁne sets Sσ
i = {σ(1)  . . .   σ(i)}.
i−1)  then s + m ∈ ∂F (∅) and the following inequalities hold.
If we deﬁne s : sσ(i) = G(Sσ
(6)

log Z−(s + m  0) − log

exp(−F (A)) ≤ κGMAX

0 = ∅  Sσ

(cid:88)

A⊆V

exp(+F (A)) − log Z +(s + m  0) ≤ κGMAX

(7)

i ) − G(Sσ
(cid:88)

log

A⊆V

by s(A) =(cid:80)
(cid:88)

log

A⊆V

Theorem 2. Under the same assumptions as in Theorem 1  if we deﬁne the modular function s(·)

i∈A G({i})  then s + m ∈ ∂F (∅) and the following inequalities hold.
exp(−F (A)) − log Z−(s + m  0) ≤
GMAX ≤ κ
1 − κ
GMAX ≤ κ
1 − κ

1 + (n − 1)(1 − κ)

1 + (n − 1)(1 − κ)

exp(+F (A)) ≤

κ(n − 1)

κ(n − 1)

(cid:88)

A⊆V

log Z +(s + m  0) − log

GMAX

GMAX

(8)

(9)

Note that we establish bounds for speciﬁc sub-/supergradients. Since our variational scheme con-
siders these in the optimization as well  the same quality guarantees hold for the optimized bounds.
Further  note that we get a dependence on the range of the function via GMAX. However  if we con-
sider αF for large α > 1  most of the mass will be concentrated at the MAP (assuming it is unique).
In this case  L-FIELD also performs well  as it can always choose gradients that are tight at the MAP.
When we optimize over supergradients  all possible tight sets are considered. Similarly  the subgra-
dients are optimized over B(F )  and for any X ⊆ V there exists some sX ∈ B(F ) tight at X.

7 Experiments

Our experiments6 aim to address four main questions: (1) How large is the gap between the upper-
and lower-bounds for the log-partition function and the marginals? (2) How accurate are the fac-
torized approximations obtained from a single MAP-like optimization problem? (3) How does the
accuracy depend on the amount of evidence (i.e.  concentration of the posterior)  the curvature of the
function  and the type of Bayesian submodular model considered? (4) How does L-FIELD compare
to mean-ﬁeld on problems where the latter can be applied?
We consider approximate marginals obtained from the following methods: lower/upper: obtained
from the factorized distributions associated with the modular lower/upper bounds; lower-/upper-
bound: the lower/upper bound of the estimated probability interval. All of the functions we consider
are graph-representable [17]  which allows us to perform the optimization over superdifferentials
using a single graph cut and use the exact divide-and-conquer algorithm. We used the min-cut
5We differ from the convention to remove i ∈ V s.t. G({i}) = 0. Please see the appendix for a discussion.
6The code will be made available at http://las.ethz.ch.

6

(cid:0)|Nv∩A|

|Nv|

(cid:1)µ(cid:17)

(cid:16)−(cid:80)

implementation from [34]. Since the update equations are easily computable  we have also
implemented mean-ﬁeld for the ﬁrst experiment. For the other two experiments computing the
updates requires exhaustive enumeration and is intractable. The results are shown on Figure 1 and
the experiments are explained below. We plot the averages of several repetitions of the experiments.
Note that computing intervals for marginals requires two MAP-like optimizations per variable;
hence we focus on small problems with |V | = 100. We point out that obtaining a single factorized
approximation (as produced  e.g.  by mean-ﬁeld)  only requires a single MAP-like optimization 
which can be done for more than 270 000 variables [19].
Log-supermodular: Cuts / Pairwise MRFs. Our ﬁrst experiment evaluates L-FIELD on a se-
quence of distributions that are increasingly more concentrated. Motivated by applications in semi-
supervised learning  we sampled data from a 2-dimensional Gaussian mixture model with 2 clus-
ters. The centers were sampled from N ([3  3]  I) and N ([−3 −3]  I) respectively. For each cluster 
we sampled n = 50 points from a bivariate normal. These 2n points were then used as nodes
to create a graph with weight between points x and x(cid:48) equal to e−||x−x(cid:48)||. As prior we chose
P (A) ∝ exp(−F (A))  where F is the cut function in this graph  hence P (A) is a regular MRF.
Then  for k = 1  . . .   n we consider the conditional distribution on the event that k points from the
ﬁrst cluster are on one side of the cut and k points from the other cluster are on the other side. As we
provide more evidence  the posterior concentrates  and the intervals for both the log-partition func-
tion and marginals shrink. Compared with ground truth  the estimates of the marginal probabilities
improve as well. Due to non-convexity  mean-ﬁeld occasionally gets stuck in local optima  resulting
in very poor marginals. To prevent this  we chose the best run out of 20 random restarts. These best
runs produced slightly better marginals than L-FIELD for this model  at the cost of less robustness.
Log-supermodular: Decomposable functions. Our second experiment assesses the performance
as a function of the curvature of F . It is motivated by a problem in outbreak detection on networks.
Assume that we have a graph G = (V  E) and some of its nodes E ⊆ V have been infected by
some contagious process. Instead of E  we observe a noisy set N ⊆ V   corrupted with a false
positive rate of 0.1 and a false negative rate of 0.2. We used a log-supermodular prior P (A) ∝
  where µ ∈ [0  1] and Nv is the union of v and its neighbors. This prior
exp
prefers smaller sets and sets that are more clustered on the graph. Note that µ controls the preference
of clustered nodes and affects the curvature. We sampled random graphs with 100 nodes from a
Watts-Strogatz model and obtained E by running an independent cascade starting from 2 random
nodes. Then  for varying µ  we consider the posterior  which is log-supermodular  as the noise model
results in a modular likelihood. As the curvature increases  the intervals for both the log-partition
function and marginals decrease as expected. Surprisingly  the marginals are very accurate (< 0.1
average error) even for very large curvature. This suggests that our curvature dependent bounds are
very conservative  and much better performance can be expected in practice.
Log-submodular: Facility location modeling. Our last experiment evaluates how accurate L-
FIELD is when quantifying uncertainty in submodular maximization tasks. Concretely  we consider
the problem of sensor placement in water distribution networks  which can be modeled as sub-
modular maximization [35]. More speciﬁcally  we have a water distribution network and there are
some junctions V where we can put sensors that can detect contaminated water. We also have a
set I of contamination scenarios. For each i ∈ I and j ∈ V we have a utility Ci j ∈ [0  1]  that
comes from real data [35]. Moreover  as the sensors are expensive  we would like to use as few
as possible. We use the facility-location model  more precisely P (S = A) ∝ exp(F (A) − 2|A|) 
i∈N maxj∈A Ci j. Instead of optimizing for a ﬁxed placement  here we consider
the problem of sampling from P in order to quantify the uncertainty in the optimization task. We
used the following sampling strategy. We consider nodes v ∈ V in some order. We then sample a
Bernoulli Z with probability P (Z = 1) = qv based on the factorized distribution q from the modu-
lar upper bound. We then condition on v ∈ S if Z = 1  or v /∈ S if Z = 0. In the computation of the
lower bound we used the subgradient sg computed from the greedy order of V — the i-th element
in this order v1  . . .   vn is the one that gives the highest improvement when added to the set formed
by the previous i − 1 elements. Then  sg ∈ ∂F (∅) : sg
i = F (vi|{v0  . . .   vi−1}). We repeated the
experiment several times using randomly sampled 500 contamination scenarios and 100 locations
from a larger dataset. Note that our approximations get better as we condition on more information
(i.e.  proceed through the iterations of the sampling procedure above). Also note that even from the
very beginning  the marginals are very accurate (< 0.1 average error).

with F (A) = (cid:80)

v∈V

7

Lower
Upper

Mean-Field

d
n
u
o
B

r
e
w
o
L
-
r
e
p
p
U
—

p
a
G
e
g
a
r
e
v
A

1

0.8

0.6

0.4

0.2

0

0

30

20

10
40
Number of Conditioned Pairs
(a) [CT] — Logp. Bounds

50

0

20

10
40
Number of Conditioned Pairs

30

(b) [CT] — Prob. Interval Gap

n
o
i
t
c
n
u
F
n
o
i
t
i
t
r
a
P
-
g
o
L

n
o
i
t
c
n
u
F
n
o
i
t
i
t
r
a
P
-
g
o
L

n
o
i
t
c
n
u
F
n
o
i
t
i
t
r
a
P
-
g
o
L

150

100

50

0

30

20

10

0

100

80

60

40

20

0

Lower
Upper

0

0.2

0.6
0.4
1-Curvature

0.8

1

(d) [NW] — Logp. Bounds

Lower
Upper

d
n
u
o
B

r
e
w
o
L
-
r
e
p
p
U
—

p
a
G
e
g
a
r
e
v
A

d
n
u
o
B

r
e
w
o
L
-
r
e
p
p
U
—
p
a
G
e
g
a
r
e
v
A

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

50

s
l
a
n
i
g
r
a

M

f
o

r
o
r
r
E
e
t
u
l
o
s
b
A
n
a
e

M

s
l
a
n
i
g
r
a

M

f
o

r
o
r
r
E
e
t
u
l
o
s
b
A
n
a
e

M

Lower
Upper

Lower-Bound
Upper-Bound
Mean-Field

0.6

0.4

0.2

0

4

2
8
Number of Conditioned Pairs

6

(c) [CT] — Mean Error on Marginals

Lower
Upper

Lower-Bound
Upper-Bound

0.6

0.4

0.2

0

0

0.2

0.6
0.4
1-Curvature

0.8

1

0

0.2

0.6
0.4
1-Curvature

0.8

1

(e) [NW] — Prob. Interval Gap

(f) [NW] — Mean Error on Marginals

Lower
Upper

Lower-Bound
Upper-Bound

s
l
a
n
i
g
r
a

M

f
o
r
o
r
r
E
e
t
u
l
o
s
b
A
n
a
e

M

0.6

0.4

0.2

0

0

20

40
60
Iteration

80

100

0

20

40
60
Iteration

80

100

0

5

10

Iteration

15

20

(g) [SP] — Logp. Bounds

(h) [SP] — Prob. Interval Gap

(i) [SP] — Mean Error on Marginals

Figure 1: Experiments on [CT] Cuts (a-c)  [NW] network detection (d-f)  [SP] sensor placement (g-i). Note
that to generate (c f i) we had to compute the exact marginals by exhaustive enumeration. Hence  these three
graphs were created using a smaller ground set of size 20. The error bars capture 3 standard errors.
8 Conclusion

We proposed L-FIELD  the ﬁrst variational method for approximate inference in general Bayesian
submodular and supermodular models. Our approach has several attractive properties: It produces
rigorous upper and lower bounds on the log-partition function and on marginal probabilities. These
bounds can be optimized efﬁciently via convex and submodular optimization. Accurate factorial
approximations can be obtained at the same computational cost as performing MAP inference in the
underlying model  a problem for which a vast array of scalable methods are available. Furthermore 
we identiﬁed a natural connection to the traditional mean-ﬁeld method and bounded the quality of
our approximations with the curvature of the function. Our experiments demonstrate the accuracy
of our inference scheme on several natural examples of Bayesian submodular models. We believe
that our results present a signiﬁcant step in understanding the role of submodularity – so far mainly
considered for optimization – in approximate Bayesian inference. Furthermore  L-FIELD presents a
signiﬁcant advance in our ability to perform probabilistic inference in models with complex  high-
order dependencies  which present a major challenge for classical techniques.
Acknowledgments. This research was supported in part by SNSF grant 200021 137528  ERC StG
307036 and a Microsoft Research Faculty Fellowship.
References

[1]

J. Edmonds. “Submodular functions  matroids  and certain polyhedra”. In: Combinatorial structures and
their applications (1970)  pp. 69–87.

[2] D. Golovin and A. Krause. “Adaptive Submodularity: Theory and Applications in Active Learning and
Stochastic Optimization”. In: Journal of Artiﬁcial Intelligence Research (JAIR) 42 (2011)  pp. 427–486.

8

[3] Y. Yue and C. Guestrin. “Linear Submodular Bandits and its Application to Diversiﬁed Retrieval”. In:

Neural Information Processing Systems (NIPS). 2011.

[4] H. Lin and J. Bilmes. “A class of submodular functions for document summarization”. In: 49th Annual

Meeting of the Association for Computational Linguistics: HLT. 2011  pp. 510–520.

[5] V. Cevher and A. Krause. “Greedy Dictionary Selection for Sparse Representation”. In: IEEE Journal

of Selected Topics in Signal Processing 99.5 (2011)  pp. 979–988.

[6] M. Narasimhan  N. Jojic  and J. Bilmes. “Q-clustering”. In: NIPS. Vol. 5. 10.10. 2005  p. 5.
[7] F. Bach. “Structured sparsity-inducing norms through submodular functions.” In: NIPS. 2010.
[8] S. Fujishige. Submodular functions and optimization. Vol. 58. Annals of Discrete Mathematics. 2005.
[9] F. Bach. “Learning with submodular functions: a convex optimization perspective”. In: Foundations and

Trends R(cid:13) in Machine Learning 6.2-3 (2013)  pp. 145–373. ISSN: 1935-8237.

[10] S. Jegelka  H. Lin  and J. A. Bilmes. “On fast approximate submodular minimization.” In: NIPS. 2011.
[11] N. Buchbinder  M. Feldman  J. Naor  and R. Schwartz. “A tight linear time (1/2)-approximation for

unconstrained submodular maximization”. In: Foundations of Computer Science (FOCS). 2012.

[12] Y. Boykov  O. Veksler  and R. Zabih. “Fast approximate energy minimization via graph cuts”. In: Pattern

Analysis and Machine Intelligence  IEEE Transactions on 23.11 (2001)  pp. 1222–1239.

[13] M. Jerrum and A. Sinclair. “Polynomial-time approximation algorithms for the Ising model”. In: SIAM

Journal on computing 22.5 (1993)  pp. 1087–1116.

[14] L. A. Goldberg and M. Jerrum. “The complexity of ferromagnetic Ising with local ﬁelds”. In: Combina-

torics  Probability and Computing 16.01 (2007)  pp. 43–61.
J. Gillenwater  A. Kulesza  and B. Taskar. “Near-Optimal MAP Inference for Determinantal Point Pro-
cesses”. In: Proc. Neural Information Processing Systems (NIPS). 2012.

[16] M. J. Wainwright and M. I. Jordan. “Graphical Models  Exponential Families  and Variational Infer-

[15]

ence”. In: Found. Trends Mach. Learn. 1.1-2 (2008)  pp. 1–305.

[17] V. Kolmogorov and R. Zabin. “What energy functions can be minimized via graph cuts?” In: Pattern

Analysis and Machine Intelligence  IEEE Transactions on 26.2 (2004)  pp. 147–159.

[18] P. Stobbe and A. Krause. “Efﬁcient Minimization of Decomposable Submodular Functions”. In: Proc.

Neural Information Processing Systems (NIPS). 2010.

[19] S. Jegelka  F. Bach  and S. Sra. “Reﬂection methods for user-friendly submodular optimization”. In:

Advances in Neural Information Processing Systems. 2013  pp. 1313–1321.

[20] S. Jegelka and J. Bilmes. “Submodularity beyond submodular energies: coupling edges in graph cuts”.
In: Computer Vision and Pattern Recognition (CVPR)  2011 IEEE Conference on. 2011  pp. 1897–1904.
[21] A. Krause and C. Guestrin. “Near-optimal Nonmyopic Value of Information in Graphical Models”. In:

Conference on Uncertainty in Artiﬁcial Intelligence (UAI). 2005.

[22] A. Krause and D. Golovin. “Submodular Function Maximization”. In: Tractability: Practical Ap-

proaches to Hard Problems (to appear). Cambridge University Press  2014.

[23] P. Kohli  L. Ladick´y  and P. H. Torr. “Robust higher order potentials for enforcing label consistency”.

In: International Journal of Computer Vision 82.3 (2009)  pp. 302–324.

[24] A. Kulesza and B. Taskar. “Determinantal Point Processes for Machine Learning”. In: Foundations and

Trends in Machine Learning 5.2–3 (2012).

[25] R. Gomes and A. Krause. “Budgeted Nonparametric Learning from Data Streams”. In: ICML. 2010.
[26] K. El-Arini  G. Veda  D. Shahaf  and C. Guestrin. “Turning down the noise in the blogosphere”. In: Proc.

ACM SIGKDD International Conference on Knowledge Discovery and Data mining. 2009.

[27] R. Iyer  S. Jegelka  and J. Bilmes. “Fast Semidifferential-based Submodular Function Optimization”. In:

ICML (3). 2013  pp. 855–863.

[28] M. Frank and P. Wolfe. “An algorithm for quadratic programming”. In: Naval Research Logistics Quar-

terly 3.1-2 (1956)  pp. 95–110. ISSN: 1931-9193.

[29] M. Jaggi. “Revisiting Frank-Wolfe: Projection-free sparse convex optimization”. In: 30th International

Conference on Machine Learning (ICML-13). 2013  pp. 427–435.

[30] L. Lov´asz. “Submodular functions and convexity”. In: Mathematical Programming The State of the Art.

Springer  1983  pp. 235–257.

[31] G. Calinescu  C. Chekuri  M. P´al  and J. Vondr´ak. “Maximizing a submodular set function subject to a

matroid constraint”. In: Integer programming and combinatorial optimization. Springer  2007.

[32] M. Conforti and G. Cornuejols. “Submodular set functions  matroids and the greedy algorithm: tight
worst-case bounds and some generalizations of the Rado-Edmonds theorem”. In: Discrete applied math-
ematics 7.3 (1984)  pp. 251–274.

[33] W. H. Cunningham. “Decomposition of submodular functions”. In: Combinatorica 3.1 (1983).
[34] Y. Boykov and V. Kolmogorov. “An experimental comparison of min-cut/max-ﬂow algorithms for en-
ergy minimization in vision”. In: Pattern Analysis and Machine Intelligence  IEEE Trans. on 26.9 (2004).
[35] A. Krause  J. Leskovec  C. Guestrin  J. VanBriesen  and C. Faloutsos. “Efﬁcient Sensor Placement Op-
timization for Securing Large Water Distribution Networks”. In: Journal of Water Resources Planning
and Management 134.6 (2008)  pp. 516–526.

9

,Josip Djolonga
Andreas Krause
Muhammad Bilal Zafar
Isabel Valera
Manuel Rodriguez
Krishna Gummadi
Adrian Weller
Siyuan Huang
Siyuan Qi
Yinxue Xiao
Yixin Zhu
Ying Nian Wu
Song-Chun Zhu
Peter Anderson
Ayush Shrivastava
Devi Parikh
Dhruv Batra
Stefan Lee