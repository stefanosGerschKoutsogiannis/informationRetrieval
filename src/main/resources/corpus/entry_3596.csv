2017,Multi-output Polynomial Networks and Factorization Machines,Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting  i.e.  for learning vector-valued functions  with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence  despite the fact that it involves a non-convex basis selection step. On classification tasks  we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks  we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.,Multi-output Polynomial Networks

and Factorization Machines

Mathieu Blondel

NTT Communication Science Laboratories

Kyoto  Japan

mathieu@mblondel.org

Vlad Niculae∗
Cornell University

Ithaca  NY

vlad@cs.cornell.edu

Takuma Otsuka

Naonori Ueda

NTT Communication Science Laboratories

NTT Communication Science Laboratories

Kyoto  Japan

otsuka.takuma@lab.ntt.co.jp

RIKEN

Kyoto  Japan

ueda.naonori@lab.ntt.co.jp

Abstract

Factorization machines and polynomial networks are supervised polynomial mod-
els based on an efﬁcient low-rank decomposition. We extend these models to the
multi-output setting  i.e.  for learning vector-valued functions  with application to
multi-class or multi-task problems. We cast this as the problem of learning a 3-way
tensor whose slices share a common basis and propose a convex formulation of that
problem. We then develop an efﬁcient conditional gradient algorithm and prove
its global convergence  despite the fact that it involves a non-convex basis selec-
tion step. On classiﬁcation tasks  we show that our algorithm achieves excellent
accuracy with much sparser models than existing methods. On recommendation
system tasks  we show how to combine our algorithm with a reduction from ordinal
regression to multi-output classiﬁcation and show that the resulting algorithm
outperforms simple baselines in terms of ranking accuracy.

1

Introduction

Interactions between features play an important role in many classiﬁcation and regression tasks.
Classically  such interactions have been leveraged either explicitly  by mapping features to their
products (as in polynomial regression)  or implicitly  through the use of the kernel trick. While fast
linear model solvers have been engineered for the explicit approach [9  28]  they are typically limited
to small numbers of features or low-order feature interactions  due to the fact that the number of
parameters that they need to learn scales as O(dt)  where d is the number of features and t is the order
of interactions considered. Models kernelized with the polynomial kernel do not suffer from this
problem; however  the cost of storing and evaluating these models grows linearly with the number of
training instances  a problem sometimes referred to as the curse of kernelization [30].

Factorization machines (FMs) [25] are a more recent approach that can use pairwise feature interac-
tions efﬁciently even in very high-dimensional data. The key idea of FMs is to model the weights
of feature interactions using a low-rank matrix. Not only this idea offers clear beneﬁts in terms of
model compression compared to the aforementioned approaches  it has also proved instrumental
in modeling interactions between categorical variables  converted to binary features via a one-hot
encoding. Such binary features are usually so sparse that many interactions are never observed in the

∗Work performed during an internship at NTT Commmunication Science Laboratories  Kyoto.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

training set  preventing classical approaches from capturing their relative importance. By imposing
a low rank on the feature interaction weight matrix  FMs encourage shared parameters between
interactions  allowing to estimate their weights even if they never occurred in the training set. This
property has been used in recommender systems to model interactions between user variables and
item variables  and is the basis of several industrial successes of FMs [32  17].

Originally motivated as neural networks with a polynomial activation (instead of the classical
sigmoidal or rectiﬁer activations)  polynomial networks (PNs) [20] have been shown to be intimately
related to FMs and to only subtly differ in the non-linearity they use [5]. PNs achieve better
performance than rectiﬁer networks on pedestrian detection [20] and on dependency parsing [10] 
and outperform kernel approximations such as the Nyström method [5]. However  existing PN and
FM works have been limited to single-output models  i.e.  they are designed to learn scalar-valued
functions  which restricts them to regression or binary classiﬁcation problems.

Our contributions. In this paper  we generalize FMs and PNs to multi-output models  i.e.  for
learning vector-valued functions  with application to multi-class or multi-task problems.

1) We cast learning multi-output FMs and PNs as learning a 3-way tensor  whose slices share a
common basis (each slice corresponds to one output). To obtain a convex formulation of that
problem  we propose to cast it as learning an inﬁnite-dimensional but row-wise sparse matrix. This
can be achieved by using group-sparsity inducing penalties. (§3)

2) To solve the obtained optimization problem  we develop a variant of the conditional gradient
(a.k.a. Frank-Wolfe) algorithm [11  15]  which repeats the following two steps: i) select a new basis
vector to add to the model and ii) reﬁt the model over the current basis vectors. (§4) We prove the
global convergence of this algorithm (Theorem 1)  despite the fact that the basis selection step is
non-convex and more challenging in the shared basis setting. (§5)

3) On multi-class classiﬁcation tasks  we show that our algorithm achieves comparable accuracy to
kernel SVMs but with much more compressed models than the Nyström method. On recommender
system tasks  where kernelized models cannot be used (since they do not generalize to unseen
user-item pairs)  we demonstrate how our algorithm can be combined with a reduction from ordinal
regression to multi-output classiﬁcation and show that the resulting algorithm outperforms single-
output PNs and FMs both in terms of root mean squared error (RMSE) and ranking accuracy  as
measured by nDCG (normalized discounted cumulative gain) scores. (§6)

2 Background and related work

Notation. We denote the set {1  . . .   m} by [m]. Given a vector v ∈ Rk  we denote its elements
by vr ∈ R ∀r ∈ [k]. Given a matrix V ∈ Rk×m  we denote its rows by vr ∈ Rm ∀r ∈ [k] and its
columns by v: c ∀c ∈ [m]. We denote the lp norm of V by kV kp := k vec(V )kp and its lp/lq norm
by kV kp q :=(cid:16)Pk
Factorization machines (FMs). Given an input vector x ∈ Rd  FMs predict a scalar output by

. The number of non-zero rows of V is denoted by kV k0 ∞.

q(cid:17)
r=1 kvrkp

1
p

ˆyFM := wTx +Xi<j

wi jxixj 

feature interaction weights. To obtain a low-rank W   [25] originally proposed to use a change of

where w ∈ Rd contains feature weights and W ∈ Rd×d is a low-rank matrix that contains pairwise
variable W = H TH  where H ∈ Rk×d (with k ∈ N+ a rank parameter) and to learn H instead.

Noting that this quadratic model results in a non-convex problem in H  [4  31] proposed to convexify
the problem by learning W directly but to encourage low rank using a nuclear norm on W . For
learning  [4] proposed a conditional gradient like approach with global convergence guarantees.

Polynomial networks (PNs). PNs are a recently-proposed form of neural network where the usual
activation function is replaced with a squared activation. Formally  PNs predict a scalar output by

k

ˆyPN := wTx + vTσ(Hx) = wTx +

vr σ(hT

r x) 

where σ(a) = a2 (evaluated element-wise) is the squared activation  v ∈ Rk is the output layer
vector  H ∈ Rk×d is the hidden layer matrix and k is the number of hidden units. Because the

Xr=1

2

hT

1

hT
k

m

Wm

. . .

v1 m

W2

W1

h1

hT

1

+··· + vk m

hk

hT
k

d

d

v1 1

h1

+··· + vk 1

hk

Figure 1: Our multi-output PNs / FMs learn a tensor whose slices share a common basis {hr}k
r.h.s term can be rewritten as xTW x = Pd

i j=1 wi jxixj if we set W = H T diag(v)H  we see
that PNs are clearly a slight variation of FMs and that learning (v  H) can be recast as learning a
low-rank matrix W . Based on this observation  [20] proposed to use GECO [26]  a greedy algorithm
for convex optimization with a low-rank constraint  similar to the conditional gradient algorithm. [13]
proposed a learning algorithm for PNs with global optimality guarantees but their theory imposes
non-negativity on the network parameters and they need one distinct hyper-parameter per hidden unit
to avoid trivial models. Other low-rank polynomial models were recently introduced in [29  23] but
using a tensor network (a.k.a. tensor train) instead of the canonical polyadic (CP) decomposition.

r=1.

3 A convex formulation of multi-output PNs and FMs

In this section  we generalize PNs and FMs to multi-output problems. For the sake of concreteness 
we focus on PNs for multi-class classiﬁcation. The extension to FMs is straightforward and simply

requires to replace σ(hTx) = (hTx)2 by σANOVA(h  x) :=Pi<j xihixjhj   as noted in [5].
The predictions of multi-class PNs can be naturally deﬁned as ˆyMPN := argmaxc∈[m] wT
c x+xTWcx 
where m is the number of classes  wc ∈ Rd and Wc ∈ Rd×d is low-rank. Following [5]  we can
model the linear term directly in the quadratic term if we augment all data points with an extra feature
of value 1  i.e.  xT ← [1  xT]. We will therefore simply assume ˆyMPN = argmaxc∈[m] xTWcx
henceforth. Our main proposal in this paper is to decompose W1  . . .   Wm using a shared basis:

Wc = H T diag(v: c)H =Pk

r=1 vr chrhT
r

∀c ∈ [m] 

(1)

where  in neural network terminology  H ∈ Rk×d can be interpreted as a hidden layer matrix and
V ∈ Rk×m as an output layer matrix. Compared to the naive approach of decomposing each Wc as
Wc = H T

c diag(v: c)Hc  this reduces the number of parameters from m(dk + k) to dk + mk.

While a nuclear norm could be used to promote a low rank on each Wc  similarly as in [4  31]  this is
clearly not sufﬁcient to impose a shared basis. A naive approach would be to use non-orthogonal
joint diagonalization as a post-processing. However  because this is a non-convex problem for which
no globally convergent algorithm is known [24]  this would result in a loss of accuracy. Our key
idea is to cast the problem of learning a multi-output PN as that of learning an inﬁnite but row-wise
sparse matrix. Without loss of generality  we assume that basis vectors (hidden units) lie in the unit

ball. We therefore denote the set of basis vectors by H := {h ∈ Rd : khk2 ≤ 1}. Let us denote this
inﬁnite matrix by U ∈ R|H|×m (we use a discrete notation for simplicity). We can then write

ˆyMPN = argmax

c∈[m]

o(x; U )c where o(x; U ) := Xh∈H

σ(hTx)uh ∈ Rm and

uh ∈ Rm denotes the weights of basis h across all classes (outputs). In this formulation  we have
uh chhT and sharing a common basis (hidden units) amounts to encouraging the rows
Wc =Ph∈H
of U   uh  to be either dense or entirely sparse. This can be naturally achieved using group-sparsity
inducing penalties. Intuitively  V in (1) can be thought as U restricted to its row support. Deﬁne the
training set by X ∈ Rn×d and y ∈ [m]n. We then propose to solve the convex problem

n

ℓ (yi  o(xi; U ))  

(2)

F (U ) :=

min
Ω(U )≤τ

Xi=1

3

Table 1: Sparsity-inducing penalties considered in this paper. With some abuse of notation  we denote
by eh and ec standard basis vectors of dimension |H| and m  respectively. Selecting an optimal
basis vector h⋆ to add is a non-convex optimization problem. The constant ǫ ∈ (0  1) is the tolerance
parameter used for the power method and ν is the multiplicative approximation we guarantee.

l1 (lasso)

Ω(U )

kUk1

Ω∗(G)

kGk∞

∆⋆ ∈ τ · ∂Ω∗(G)
τ sign(gh⋆  c⋆ )eh⋆ eT
c⋆

l1/l2 (group lasso)

kUk1 2

kGk∞ 2

l1/l∞

kUk1 ∞ kGk∞ 1

τ eh⋆ gT

h⋆ /kgh⋆k2
τ eh⋆ sign(gh⋆ )T

Subproblem

h⋆  c⋆ ∈ argmax

h∈H c∈[m]|gh c|
h∈H kghk2
h∈H kghk1

h⋆ ∈ argmax
h⋆ ∈ argmax

ν

1 − ǫ

1−ǫ√m

1−ǫ
m

where ℓ is a smooth and convex multi-class loss function (cf. Appendix A for three common examples) 
Ω is a sparsity-inducing penalty and τ > 0 is a hyper-parameter. In this paper  we focus on the l1
(lasso)  l1/l2 (group lasso) and l1/l∞ penalties for Ω  cf. Table 1. However  as we shall see  solving
(2) is more challenging with the l1/l2 and l1/l∞ penalties than with the l1 penalty. Although our
formulation is based on an inﬁnite view  we next show that U ⋆ has ﬁnite row support.

Proposition 1 Finite row support of U ⋆ for multi-output PNs and FMs
Let U ⋆ be an optimal solution of (2)  where Ω is one of the penalties in Table 1. Then 
kU ⋆k0 ∞ ≤ nm + 1. If Ω(·) = k · k1  we can tighten this bound to kU ⋆k0 ∞ ≤ min(nm + 1  dm).
Proof is in Appendix B.1. It is open whether we can tighten this result when Ω = k · k1 2 or k · k1 ∞.

4 A conditional gradient algorithm with approximate basis vector selection

At ﬁrst glance  learning with an inﬁnite number of basis vectors seems impossible. In this section 
we show how the well-known conditional gradient algorithm [11  15] combined with group-sparsity
inducing penalties naturally leads to a greedy algorithm that selects and adds basis vectors that are
useful across all outputs. On every iteration  the conditional gradient algorithm performs updates

of the form U (t+1) = (1 − γ)U (t) + γ∆⋆  where γ ∈ [0  1] is a step size and ∆⋆ is obtained by
solving a linear approximation of the objective around the current iterate U (t):

∆⋆ ∈ argmin

Ω(∆)≤τh∆ ∇F (U (t))i = τ · argmax

Ω(∆)≤1h∆ −∇F (U (t))i.

(3)

Let us denote the negative gradient −∇F (U ) by G ∈ R|H|×m for short. Its elements are deﬁned by

n

gh c = −

σ(hTxi)∇ℓ (yi  o(xi; U ))c  

Xi=1

where ∇ℓ(y  o) ∈ Rm is the gradient of ℓ w.r.t. o (cf. Appendix A). For ReLu activations  solving
(3) is known to be NP-hard [1]. Here  we focus on quadratic activations  for which we will be able to
provide approximation guarantees. Plugging the expression of σ  we get

n

1

2(cid:16)X TDcX − Dc

diag(xi)2(cid:17) (FM)
gh c = −hTΓch where Γc := X TDcX (PN) or Γc :=
and Dc ∈ Rn×n is a diagonal matrix such that (Dc)i i := ∇ℓ(yi  o(xi; U ))c. Let us recall the
deﬁnition of the dual norm of Ω: Ω∗(G) := maxΩ(∆)≤1h∆  Gi. By comparing this equation to (3) 
we see that ∆⋆ is the argument that achieves the maximum in the dual norm Ω∗(G)  up to a constant
factor τ . It is easy to verify that any element in the subdifferential of Ω∗(G)  which we denote by
∂Ω∗(G) ⊆ R|H|×m  achieves that maximum  i.e.  ∆⋆ ∈ τ · ∂Ω∗(G).
Basis selection. As shown in Table 1  elements of ∂Ω∗(G) (subgradients) are |H|× m matrices with
a single non-zero row indexed by h⋆  where h⋆ is an optimal basis (hidden unit) selected by

Xi=1

h⋆ ∈ argmax

h∈H kghkp 

4

(4)

and where p = ∞ when Ω = k · k1  p = 2 when Ω = k.k1 2 and p = 1 when Ω = k · k1 ∞. We
call (4) a basis vector selection criterion. Although this selection criterion was derived from the
linearization of the objective  it is fairly natural: it chooses the basis vector with largest “violation” 
as measured by the lp norm of the negative gradient row gh.

Multiplicative approximations. The key challenge in solving (3) or equivalently (4) arises from the
fact that G has inﬁnitely many rows gh. We therefore cast basis vector selection as a continuous
optimization problem w.r.t. h. Surprisingly  although the entire objective (2) is convex  (4) is not.

Instead of the exact maximum  we will therefore only require to ﬁnd a ˆ∆ ∈ R|H|×m that satisﬁes

Ω( ˆ∆) ≤ τ

and

h ˆ∆  Gi ≥ νh∆⋆  Gi 

where ν ∈ (0  1] is a multiplicative approximation (higher is better). It is easy to verify that this is
equivalent to replacing the optimal h⋆ by an approximate ˆh ∈ H that satisﬁes kgˆhkp ≥ νkgh⋆kp.
Sparse case. When Ω(·) = k · k1  we need to solve

max

h∈H kghk∞ = max
h∈H

max

c∈[m]|hTΓch| = max
c∈[m]

max

h∈H |hTΓch|.

It is well known that the optimal solution of maxh∈H |hTΓch| is the dominant eigenvector of Γc.
Therefore  we simply need to ﬁnd the dominant eigenvector hc of each Γc and select ˆh as the hc
with largest singular value |hT

Γchc|. Using the power method  we can ﬁnd an hc that satisﬁes
|hT

Γchc| ≥ (1 − ǫ) max

(5)

c

c

h∈H |hTΓch| 

for some tolerance parameter ǫ ∈ (0  1). The procedure takes O(Nc log(d)/ǫ) time  where Nc is
the number of non-zero elements in Γc [26]. Taking the maximum w.r.t. c ∈ [m] on both sides of
(5) leads to kgˆhk∞ ≥ νkgh⋆k∞  where ν = 1 − ǫ. However  using Ω = k · k1 does not encourage
selecting an ˆh that is useful for all outputs. In fact  when Ω = k · k1  our approach is equivalent to
imposing independent nuclear norms on W1  . . .   Wm.
Group-sparse cases. When Ω(·) = k.k1 2 or Ω(·) = k.k1 ∞  we need to solve
h∈H kghk2

h∈H kghk1 = max
h∈H

2 = max
h∈H

|hTΓch| 

(hTΓch)2

f2(h) :=

f1(h) :=

or max

max

respectively. Unlike the l1-constrained case  we are clearly selecting a basis vector with largest viola-
tion across all outputs. However  we are now faced with a more difﬁcult non-convex optimization
problem. Our strategy is to ﬁrst choose an initialization h(0) which guarantees a certain multiplicative
approximation ν  then reﬁne the solution using a monotonically non-increasing iterative procedure.
Initialization. We simply choose h(0) as the approximate solution of the Ω = k·k1 case  i.e.  we have

Now  using √mkxk∞ ≥ kxk2 ≥ kxk∞ and mkxk∞ ≥ kxk1 ≥ kxk∞  this immediately implies

kgh(0)k∞ ≥ (1 − ǫ) max

h∈H kghk∞.

kgh(0)kp ≥ ν max

h∈H kghkp 

with ν = 1−ǫ√m if p = 2 and ν = 1−ǫ
Reﬁning the solution. We now apply another instance of the conditional gradient algorithm to solve
the subproblem maxkhk2≤1 fp(h) itself  leading to the following iterates:

m if p = 1.

h(t+1) = (1 − ηt)h(t) + ηt ∇fp(h(t))
k∇fp(h(t))k2

 

(6)

where ηt ∈ [0  1]. Following [3  Section 2.2.2]  if we use the Armijo rule to select ηt  every limit
point of the sequence {h(t)} is a stationary point of fp. In practice  we observe that ηt = 1 is almost
always selected. Note that when ηt = 1 and m = 1 (i.e.  single-output case)  our reﬁning algorithm
recovers the power method. Generalized power methods were also studied for structured matrix
factorization [16  21]  but with different objectives and constraints. Since the conditional gradient

5

m

Xc=1

m

Xc=1

Algorithm 1 Multi-output PN/FM training

Input: X  y  k  τ
H ← [ ]  V ← [ ]
for t := 1  . . .   k do

r xi)vr ∀i ∈ [n]

r=1 σ(hT

Compute oi := Pt−1
Let gh := [−hTΓ1h  . . .   −hTΓmh]T
Find ˆh ≈ argmaxh∈H kghkp
Append ˆh to H and 0 to V
V ← argmin
Ω(V )≤τ

Ft(V   H)

Optional: V   H ← argmin
Ω(V )≤τ

Ft(V   H)

hr ∈H ∀r∈[t]

end for
Output: V   H (equivalent to U = Pk

t=1 eht vT
t )

2 otherwise.

2 x2 if |x| ≤ 1  |x| − 1

algorithm assumes a differentiable function  in the case p = 1  we replace the absolute function with
the Huber function |x| ≈ 1
Corrective reﬁtting step. After t iterations  U (t) contains at most t non-zero rows. We can therefore
always store U (t) as V (t) ∈ Rt×m (the output layer matrix) and H (t) ∈ Rt×d (the basis vectors /
hidden units added so far). In order to improve accuracy  on iteration t  we can then reﬁt the objective
r xi)vr(cid:17). We consider two kinds of corrective steps  a convex
Ft(V   H) :=Pn
one that minimizes Ft(V   H (t)) w.r.t. V ∈ Rt×m and an optional non-convex one that minimizes
Ft(V   H) w.r.t. both V ∈ Rt×m and H ∈ Rt×d. Reﬁtting allows to remove previously-added

bad basis vectors  thanks to the use of sparsity-inducing penalties. Similar reﬁtting procedures are
commonly used in matching pursuit [22]. The entire procedure is summarized in Algorithm 1 and
implementation details are given in Appendix D.

i=1 ℓ(cid:16)yi Pt

r=1 σ(hT

5 Analysis of Algorithm 1

The main difﬁculty in analyzing the convergence of Algorithm 1 stems from the fact that we cannot
solve the basis vector selection subproblem globally when Ω = k · k1 2 or k · k1 ∞. Therefore  we
need to develop an analysis that can cope with the multiplicative approximation ν. Multiplicative
approximations were also considered in [18] but the condition they require is too stringent (cf.
Appendix B.2 for a detailed discussion). The next theorem guarantees the number of iterations needed
to output a multi-output network that achieves as small objective value as an optimal solution of (2).

Theorem 1 Convergence of Algorithm 1
Assume F is smooth with constant β. Let U (t) be the output after t iterations of Algorithm 1 run with

constraint parameter τ

ν . Then  F (U (t)) − min
Ω(U )≤τ

F (U ) ≤ ǫ ∀t ≥

8τ 2β
ǫν2 − 2.

In [20]  single-output PNs were trained using GECO [26]  a greedy algorithm with similar O(cid:0) τ 2β
ǫν2(cid:1)

guarantees. However  GECO is limited to learning inﬁnite vectors (not matrices) and it does not
constrain its iterates like we do. Hence GECO cannot remove bad basis vectors. The proof of
Theorem 1 and a detailed comparison with GECO are given in Appendix B.2. Finally  we note
that the inﬁnite dimensional view is also key to convex neural networks [2  1]. However  to our
knowledge  we are the ﬁrst to give an explicit multiplicative approximation guarantee for a non-linear
multi-output network.

6 Experimental results

6.1 Experimental setup

Datasets. For our multi-class experiments  we use four publicly-available datasets: segment (7
classes)  vowel (11 classes)  satimage (6 classes) and letter (26 classes) [12]. Quadratic models sub-

6

stantially improve over linear models on these datasets. For our recommendation system experiments 
we use the MovieLens 100k and 1M datasets [14]. See Appendix E for complete details.

Model validation. The greedy nature of Algorithm 1 allows us to easily interleave training with
model validation. Concretely  we use an outer loop (embarrassingly parallel) for iterating over the
range of possible regularization parameters  and an inner loop (Algorithm 1  sequential) for increasing
the number of basis vectors. Throughout our experiments  we use 50% of the data for training  25%
for validation  and 25% for evaluation. Unless otherwise speciﬁed  we use a multi-class logistic loss.

6.2 Method comparison for the basis vector (hidden unit) selection subproblem

As we mentioned previously  the linearized subprob-
lem (basis vector selection) for the l1/l2 and l1/l∞
constrained cases involves a signiﬁcantly more chal-
lenging non-convex optimization problem. In this
section  we compare different methods for obtaining
an approximate solution ˆh to (4). We focus on the
ℓ1/ℓ∞ case  since we have a method for computing
the true global solution h⋆  albeit with exponential
complexity in m (cf. Appendix C). This allows us
to report the empirically observed multiplicative
approximation factor ˆν := f1(ˆh)/f1(h⋆).
Compared methods. We compare l1 init + reﬁne (proposed)  random init + reﬁne  l1 init (without
reﬁne)  random init and best data: ˆh = xi⋆ /kxi⋆k2 where i⋆ = argmax
i∈[n]
Results. We report ˆν in Figure 2. l1 init + reﬁne achieves nearly the global maximum on both
datasets and outperforms random init + reﬁne  showing the effectiveness of the proposed initialization
and that the iterative update (6) can get stuck in a bad local minimum if initialized badly. On the
other hand  l1 init + reﬁne outperforms l1 init alone  showing the importance of iteratively reﬁning
the solution. Best data  a heuristic similar to that of approximate kernel SVMs [7]  is not competitive.

Figure 2: Empirically observed multiplicative
approximation factor ˆν = f1(ˆh)/f1(h⋆).

f1(xi/kxik2).

6.3 Sparsity-inducing penalty comparison

In this section  we compare the l1  l1/l2 and l1/l∞ penalties for the
choice of Ω  when varying the maximum number of basis vectors
(hidden units). Figure 3 indicates test set accuracy when using
output layer reﬁtting. We also include linear logistic regression 
kernel SVMs and the Nyström method as baselines. For the latter
i xj + 1)2. Hyper-parameters
two  we use the quadratic kernel (xT
are chosen so as to maximize validation set accuracy.

Results. On the vowel (11 classes) and letter (26 classes) datasets 
l1/l2 and l1/l∞ penalties outperform l1 norm starting from 20 and
75 hidden units  respectively. On satimage (6 classes) and segment
(7 classes)  we observed that the three penalties are mostly similar
(not shown). We hypothesize that l1/l2 and l1/l∞ penalties make
a bigger difference when the number of classes is large. Multi-
output PNs substantially outperform the Nyström method with
comparable number of basis vectors (hidden units). Multi-output
PNs reach the same test accuracy as kernel SVMs with very few
basis vectors on vowel and satimage but appear to require at least
100 basis vectors to reach good performance on letter. This is not
surprising  since kernel SVMs require 3 208 support vectors on
letter  as indicated in Table 2 below.

6.4 Multi-class benchmark comparison

Figure 3: Penalty comparison.

Compared methods. We compare the proposed conditional gradient algorithm with output layer
reﬁtting only and with both output and hidden layer reﬁtting; projected gradient descent (FISTA)

7

0.000.250.500.751.00best datarandom initl1 initrandom init+refinel1 init + refine(proposed)satimage0.000.250.500.751.00vowel0.860.880.900.920.94letter050100150Max. hidden units0.500.70Test multi-class accuracyTable 2: Muli-class test accuracy and number of basis vectors / support vectors.

segment

vowel

satimage

letter

Conditional gradient (full reﬁtting  proposed)

96.71 (41)

l1
l1/l2
96.71 (40)
l1/l∞ 96.71 (24)

87.83 (12)

89.80 (25)

92.29 (150)

89.57 (15)

89.08 (18)

91.81 (106)

86.96 (15)

88.99 (20)

92.35 (149)

Conditional gradient (output-layer reﬁtting  proposed)

97.05 (20)

l1
l1/l2
96.36 (21)
l1/l∞ 96.19 (16)

80.00 (21)

89.71 (40)

91.01 (139)

85.22 (15)

89.71 (50)

92.24 (150)

86.96 (41)

89.35 (41)

91.68 (128)

Projected gradient descent (random init)

96.88 (50)

l1
l1/l2
96.88 (50)
l1/l∞ 96.71 (50)

79.13 (50)

89.53 (50)

88.45 (150)

80.00 (48)

89.80 (50)

88.45 (150)

83.48 (50)

89.08 (50)

88.45 (150)

l2

2

Baselines
Linear

96.88 (50)

81.74 (50)

89.98 (50)

88.45 (150)

92.55

60.00

83.03

71.17

Kernelized

96.71 (238)

85.22 (189)

89.53 (688)

93.73 (3208)

OvR PN

94.63

73.91

89.44

75.36

with random initialization; linear and kernelized models; one-vs-rest PNs (i.e.  ﬁt one PN per class).
We focus on PNs rather than FMs since they are known to work better on classiﬁcation tasks [5].

Results are included in Table 2. From these results  we can make the following observations and
conclusions. When using output-layer reﬁtting on vowel and letter (two datasets with more than 10
classes)  group-sparsity inducing penalties lead to better test accuracy. This is to be expected  since
these penalties select basis vectors that are useful across all classes. When using full hidden layer and
output layer reﬁtting  l1 catches up with l1/l2 and l1/l∞ on the vowel and letter datasets. Intuitively 
the basis vector selection becomes less important if we make more effort at every iteration by reﬁtting
the basis vectors themselves. However  on vowel  l1/l2 is still substantially better than l1 (89.57 vs.
87.83).

Compared to projected gradient descent with random initialization  our algorithm (for both output
and full reﬁtting) is better on 3/4 (l1)  2/4 (l1/l2) and 3/4 (l1/l∞) of the datasets. In addition  with our
algorithm  the best model (chosen against the validation set) is substantially sparser. Multi-output
PNs substantially outperform OvR PNs. This is to be expected  since multi-output PNs learn to share
basis vectors across different classes.

6.5 Recommender system experiments using ordinal regression

A straightforward way to implement recommender systems consists in training a single-output model
to regress ratings from one-hot encoded user and item indices [25]. Instead of a single-output PN
or FM  we propose to use ordinal McRank  a reduction from ordinal regression to multi-output
binary classiﬁcation  which is known to achieve good nDCG (normalized discounted cumulative
gain) scores [19]. This reduction involves training a probabilistic binary classiﬁer for each of the m
relevance levels (for instance  m = 5 in the MovieLens datasets). The expected relevance of x (e.g.
the concatenation of the one-hot encoded user and item indices) is then computed by

ˆy =

m

X

c=1

c p(y = c | x) =

m

X

c=1

chp(y ≤ c | x) − p(y ≤ c − 1 | x)i 

where we use the convention p(y ≤ 0 | x) = 0. Thus  all we need to do to use ordinal McRank is to
train a probabilistic binary classiﬁer p(y ≤ c | x) for all c ∈ [m].
Our key proposal is to use a multi-output model to learn all m classiﬁers simultaneously  i.e.  in a
multi-task fashion. Let xi be a vector representing a user-item pair with corresponding rating yi  for

8

Figure 4: Recommender system experiment: RMSE (lower is better) and nDCG (higher is better).

i ∈ [n]. We form a n × m matrix Y such that yi c = +1 if yi ≤ c and −1 otherwise  and solve

min
Ω(U )≤τ

n

m

Xi=1

Xc=1

ℓ yi c  Xh∈H

σANOVA(h  xi)uh c!  

where ℓ is set to the binary logistic loss  in order to be able to produce probabilities. After running
Algorithm 1 on that objective for k iterations  we obtain H ∈ Rk×d and V ∈ Rk×m. Because H is
to a single-output regression model  therefore comes from learning V ∈ Rk×m instead of v ∈ Rk.

shared across all outputs  the only small overhead of using the ordinal McRank reduction  compared

In this experiment  we focus on multi-output factorization machines (FMs)  since FMs usually work
better than PNs for one-hot encoded data [5]. We show in Figure 4 the RMSE and nDCG (truncated
at 1 and 5) achieved when varying k (the maximum number of basis vectors / hidden units).
Results. When combined with the ordinal McRank reduction  we found that l1/l2 and l1/l∞–
constrained multi-output FMs substantially outperform single-output FMs and PNs on both RMSE
and nDCG measures. For instance  on MovieLens 100k and 1M  l1/l∞–constrained multi-output
FMs achieve an nDCG@1 of 0.75 and 0.76  respectively  while single-output FMs only achieve 0.71
and 0.75. Similar trends are observed with nDCG@5. We believe that this reduction is more robust
to ranking performance measures such as nDCG thanks to its modelling of the expected relevance.

7 Conclusion and future directions

We deﬁned the problem of learning multi-output PNs and FMs as that of learning a 3-way tensor
whose slices share a common basis. To obtain a convex optimization objective  we reformulated that
problem as that of learning an inﬁnite but row-wise sparse matrix. To learn that matrix  we developed
a conditional gradient algorithm with corrective reﬁtting  and were able to provide convergence
guarantees  despite the non-convexity of the basis vector (hidden unit) selection step.

Although not considered in this paper  our algorithm and its analysis can be modiﬁed to make
use of stochastic gradients. An open question remains whether a conditional gradient algorithm
with provable guarantees can be developed for training deep polynomial networks or factorization
machines. Such deep models could potentially represent high-degree polynomials with few basis
vectors. However  this would require the introduction of a new functional analysis framework.

9

010203040500.940.960.981.00Movielens 100kRMSE010203040500.680.700.720.740.76nDCG@1010203040500.730.740.750.760.77nDCG@501020304050Max. hidden units0.900.920.940.960.981.00Movielens 1M01020304050Max. hidden units0.720.730.740.750.7601020304050Max. hidden units0.750.760.77Single-output PNSingle-output FMOrdinal McRank FM l1/l2Ordinal McRank FM l1/lReferences

[1] F. Bach. Breaking the curse of dimensionality with convex neural networks. JMLR  2017.

[2] Y. Bengio  N. Le Roux  P. Vincent  O. Delalleau  and P. Marcotte. Convex neural networks. In

NIPS  2005.

[3] D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont  1999.

[4] M. Blondel  A. Fujino  and N. Ueda. Convex factorization machines. In ECML/PKDD  2015.

[5] M. Blondel  M. Ishihata  A. Fujino  and N. Ueda. Polynomial networks and factorization

machines: New insights and efﬁcient training algorithms. In ICML  2016.

[6] M. Blondel  K. Seki  and K. Uehara. Block coordinate descent algorithms for large-scale sparse

multiclass classiﬁcation. Machine Learning  93(1):31–52  2013.

[7] A. Bordes  S. Ertekin  J. Weston  and L. Bottou. Fast kernel classiﬁers with online and active

learning. JMLR  6(Sep):1579–1619  2005.

[8] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[9] Y.-W. Chang  C.-J. Hsieh  K.-W. Chang  M. Ringgaard  and C.-J. Lin. Training and testing
low-degree polynomial data mappings via linear svm. Journal of Machine Learning Research 
11:1471–1490  2010.

[10] D. Chen and C. D. Manning. A fast and accurate dependency parser using neural networks. In

EMNLP  2014.

[11] J. C. Dunn and S. A. Harshbarger. Conditional gradient algorithms with open loop step size

rules. Journal of Mathematical Analysis and Applications  62(2):432–444  1978.

[12] R.-E. Fan and C.-J. Lin.

http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/

datasets/  2011.

[13] A. Gautier  Q. N. Nguyen  and M. Hein. Globally optimal training of generalized polynomial

neural networks with nonlinear spectral methods. In NIPS  2016.

[14] GroupLens. http://grouplens.org/datasets/movielens/  1998.

[15] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML  2013.

[16] M. Journée  Y. Nesterov  P. Richtárik  and R. Sepulchre. Generalized power method for sparse

principal component analysis. Journal of Machine Learning Research  11:517–553  2010.

[17] Y. Juan  Y. Zhuang  W.-S. Chin  and C.-J. Lin. Field-aware factorization machines for CTR

prediction. In ACM Recsys  2016.

[18] S. Lacoste-Julien  M. Jaggi  M. Schmidt  and P. Pletscher. Block-coordinate Frank-Wolfe

optimization for structural SVMs. In ICML  2012.

[19] P. Li  C. J. Burges  and Q. Wu. McRank: Learning to rank using multiple classiﬁcation and

gradient boosting. In NIPS  2007.

[20] R. Livni  S. Shalev-Shwartz  and O. Shamir. On the computational efﬁciency of training neural

networks. In NIPS  2014.

[21] R. Luss and M. Teboulle. Conditional gradient algorithms for rank-one matrix approximations

with a sparsity constraint. SIAM Review  55(1):65–98  2013.

[22] S. G. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transac-

tions on Signal Processing  41(12):3397–3415  1993.

[23] A. Novikov  M. Troﬁmov  and I. Oseledets. Exponential machines.

arXiv preprint

arXiv:1605.03795  2016.

10

[24] A. Podosinnikova  F. Bach  and S. Lacoste-Julien. Beyond CCA: Moment matching for multi-

view models. In ICML  2016.

[25] S. Rendle. Factorization machines. In ICDM  2010.

[26] S. Shalev-Shwartz  A. Gonen  and O. Shamir. Large-scale convex minimization with a low-rank

constraint. In ICML  2011.

[27] S. Shalev-Shwartz  Y. Wexler  and A. Shashua. ShareBoost: Efﬁcient multiclass learning with

feature sharing. In NIPS  2011.

[28] S. Sonnenburg and V. Franc. Cofﬁn: A computational framework for linear SVMs. In ICML 

2010.

[29] E. Stoudenmire and D. J. Schwab. Supervised learning with tensor networks. In NIPS  2016.

[30] Z. Wang  K. Crammer  and S. Vucetic. Multi-class Pegasos on a budget. In ICML  2010.

[31] M. Yamada  W. Lian  A. Goyal  J. Chen  K. Wimalawarne  S. A. Khan  S. Kaski  H. M.
Mamitsuka  and Y. Chang. Convex factorization machine for toxicogenomics prediction. In
KDD  2017.

[32] E. Zhong  Y. Shi  N. Liu  and S. Rajan. Scaling factorization machines with parameter server.

In CIKM  2016.

11

,Mathieu Blondel
Vlad Niculae
Takuma Otsuka
Naonori Ueda
Kaiyi Ji
Yingbin Liang