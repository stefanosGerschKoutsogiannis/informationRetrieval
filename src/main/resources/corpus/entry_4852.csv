2014,Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature,We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration  to average over ensembles of models or unknown (hyper-)parameters (for example to compute marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference  but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems  but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative  permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.,Sampling for Inference in Probabilistic Models with

Fast Bayesian Quadrature

Tom Gunter  Michael A. Osborne

Engineering Science
University of Oxford

{tgunter mosb}@robots.ox.ac.uk

Roman Garnett

Knowledge Discovery and Machine Learning

University of Bonn

rgarnett@uni-bonn.de

Philipp Hennig

MPI for Intelligent Systems

T¨ubingen  Germany

phennig@tuebingen.mpg.de

Stephen J. Roberts
Engineering Science
University of Oxford

sjrob@robots.ox.ac.uk

Abstract

We propose a novel sampling framework for inference in probabilistic models: an
active learning approach that converges more quickly (in wall-clock time) than
Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in proba-
bilistic inference is numerical integration  to average over ensembles of models or
unknown (hyper-)parameters (for example to compute the marginal likelihood or
a partition function). MCMC has provided approaches to numerical integration that
deliver state-of-the-art inference  but can suffer from sample inefﬁciency and poor
convergence diagnostics. Bayesian quadrature techniques offer a model-based
solution to such problems  but their uptake has been hindered by prohibitive com-
putation costs. We introduce a warped model for probabilistic integrands (like-
lihoods) that are known to be non-negative  permitting a cheap active learning
scheme to optimally select sample locations. Our algorithm is demonstrated to
offer faster convergence (in seconds) relative to simple Monte Carlo and annealed
importance sampling on both synthetic and real-world examples.

1

Introduction

Bayesian approaches to machine learning problems inevitably call for the frequent approximation
of computationally intractable integrals of the form

(cid:90)

Z = (cid:104)(cid:96)(cid:105) =

(cid:96)(x) π(x) dx 

(1)

where both the likelihood (cid:96)(x) and prior π(x) are non-negative. Such integrals arise when marginal-
ising over model parameters or variables  calculating predictive test likelihoods and computing
model evidences. In all cases the function to be integrated—the integrand—is naturally constrained
to be non-negative  as the functions being considered deﬁne probabilities.
In what follows we will primarily consider the computation of model evidence  Z. In this case
(cid:96)(x) deﬁnes the unnormalised likelihood over a D-dimensional parameter set  x1  ...  xD  and π(x)
deﬁnes a prior density over x. Many techniques exist for estimating Z  such as annealed impor-
tance sampling (AIS) [1]  nested sampling [2]  and bridge sampling [3]. These approaches are based
around a core Monte Carlo estimator for the integral  and make minimal effort to exploit prior in-
formation about the likelihood surface. Monte Carlo convergence diagnostics are also unreliable for
partition function estimates [4  5  6]. More advanced methods—e.g.  AIS—also require parameter
tuning  and will yield poor estimates with misspeciﬁed parameters.

1

The Bayesian quadrature (BQ) [7  8  9  10] approach to estimating model evidence is inherently
model based. That is  it involves specifying a prior distribution over likelihood functions in the form
of a Gaussian process (GP) [11]. This prior may be used to encode beliefs about the likelihood
surface  such as smoothness or periodicity. Given a set of samples from (cid:96)(x)  posteriors over both
the integrand and the integral may in some cases be computed analytically (see below for discussion
on other generalisations). Active sampling [12] can then be used to select function evaluations so as
to maximise the reduction in entropy of either the integrand or integral. Such an approach has been
demonstrated to improve sample efﬁciency  relative to na¨ıve randomised sampling [12].
In a big-data setting  where likelihood function evaluations are prohibitively expensive  BQ is
demonstrably better than Monte Carlo approaches [10  12]. As the cost of the likelihood decreases 
however  BQ no longer achieves a higher effective sample rate per second  because the computa-
tional cost of maintaining the GP model and active sampling becomes relevant  and many Monte
Carlo samples may be generated for each new BQ sample. Our goal was to develop a cheap and
accurate BQ model alongside an efﬁcient active sampling scheme  such that even for low cost likeli-
hoods BQ would be the scheme of choice. Our contributions extend existing work in two ways:
Square-root GP: Foundational work [7  8  9  10] on BQ employed a GP prior directly on the likeli-
hood function  making no attempt to enforce non-negativity a priori. [12] introduced an approximate
means of modelling the logarithm of the integrand with a GP. This involved making a ﬁrst-order ap-
proximation to the exponential function  so as to maintain tractability of inference in the integrand
model. In this work  we choose another classical transformation to preserve non-negativity—the
square-root. By placing a GP prior on the square-root of the integrand  we arrive at a model which
both goes some way towards dealing with the high dynamic range of most likelihoods  and enforces
non-negativity without the approximations resorted to in [12].
Fast Active Sampling: Whereas most approaches to BQ use either a randomised or ﬁxed sampling
scheme  [12] targeted the reduction in the expected variance of Z. Here  we sample where the
expected posterior variance of the integrand after the quadratic transform is at a maximum. This is
a cheap way of balancing exploitation of known probability mass and exploration of the space in
order to approximately minimise the entropy of the integral.
We compare our approach  termed warped sequential active Bayesian integration (WSABI)  to non-
negative integration with standard Monte Carlo techniques on simulated and real examples. Cru-
cially  we make comparisons of error against ground truth given a ﬁxed compute budget.

(cid:17)

(x−x(cid:48))2

σ2

(cid:16)− 1

2

2 Bayesian Quadrature

Given a non analytic integral (cid:104)(cid:96)(cid:105) := (cid:82) (cid:96)(x)π(x) dx on a domain X = RD  Bayesian quadrature

is a model based approach of inferring both the functional form of the integrand and the value of
the integral conditioned on a set of sample points. Typically the prior density is assumed to be a
Gaussian  π(x) := N (x; ν  Λ); however  via the use of an importance re-weighting trick  q(x) =
(q(x)/π(x)) π(x)  any prior density q(x) may be integrated against. For clarity we will henceforth
notationally consider only the X = R case  although all results trivially extend to X = Rd.
Typically a GP prior is chosen for (cid:96)(x)  although it may also be directly speciﬁed on
(cid:96)(x)π(x). This is parameterised by a mean µ(x) and scaled Gaussian covariance K(x  x(cid:48)) :=
. The output length-scale λ and input length-scale σ control the standard devi-
λ2 exp
ation of the output and the autocorrelation range of each function evaluation respectively  and will
be jointly denoted as θ = {λ  σ}. Conditioned on samples xd = {x1  ...  xN} and associated func-
and the posterior covariance is CD(x  x(cid:48)) := K(x  x) − K(x  xd)K(xd  xd)−1K(xd  x)  where

tion values (cid:96)(xd)  the posterior mean is mD(x) := µ(x) + K(x  xd)K−1(xd  xd)(cid:0)(cid:96)(xd) − µ(xd)(cid:1) 
D :=(cid:8)xd  (cid:96)(xd)  θ(cid:9). For an extensive review of the GP literature and associated identities  see [11].
variance estimate of the integral are given as follows: E(cid:96)|D(cid:2)(cid:104)(cid:96)(cid:105)(cid:3) = (cid:82) mD(x) π(x) dx (2)  and

When a GP prior is placed directly on the integrand in this manner  the posterior mean and vari-
ance of the integral can be derived analytically through the use of Gaussian identities  as in
[10]. This is because the integration is a linear projection of the function posterior onto π(x) 
and joint Gaussianity is preserved through any arbitrary afﬁne transformation. The mean and

2

V(cid:96)|D(cid:2)(cid:104)(cid:96)(cid:105)(cid:3) =(cid:82)(cid:82) CD(x  x(cid:48)) π(x) dx π(x(cid:48)) dx(cid:48) (3). Both mean and variance are analytic when π(x)

is Gaussian  a mixture of Gaussians  or a polynomial (amongst other functional forms).
If the GP prior is placed directly on the likelihood in the style of traditional Bayes–Hermite quadra-
ture  the optimal point to add a sample (from an information gain perspective) is dependent only on
xd—the locations of the previously sampled points. This means that given a budget of N samples 
the most informative set of function evaluations is a design that can be pre-computed  completely un-
inﬂuenced by any information gleaned from function values [13]. In [12]  where the log-likelihood
is modelled by a GP  a dependency is introduced between the uncertainty over the function at any
point and the function value at that point. This means that the optimal sample placement is now
directly inﬂuenced by the obtained function values.

(a) Traditional Bayes–Hermite quadrature.

(b) Square-root moment-matched Bayesian quadrature.

Figure 1: Figure 1a depicts the integrand as modelled directly by a GP  conditioned on 15 samples
selected on a grid over the domain. Figure 1b shows the moment matched approximation—note the
larger relative posterior variance in areas where the function is high. The linearised square-root GP
performed identically on this example  and is not shown.

An illustration of Bayes–Hermite quadrature is given in Figure 1a. Conditioned on a grid of 15
samples  it is visible that any sample located equidistant from two others is equally informative in
reducing our uncertainty about (cid:96)(x). As the dimensionality of the space increases  exploration can
be increasingly difﬁcult due to the curse of dimensionality. A better designed BQ strategy would
create a dependency structure between function value and informativeness of sample  in such a way
as to appropriately express prior bias towards exploitation of existing probability mass.

3 Square-Root Bayesian Quadrature

Crucially  likelihoods are non-negative  a fact neglected by traditional Bayes–Hermite quadrature. In
[12] the logarithm of the likelihood was modelled  and approximate the posterior of the integral  via
a linearisation trick. We choose a different member of the power transform family—the square-root.
The square-root transform halves the dynamic range of the function we model. This helps deal with
the large variations in likelihood observed in a typical model  and has the added beneﬁt of extending
the autocorrelation range (or the input length-scale) of the GP  yielding improved predictive power
when extrapolating away from existing sample points.

2(cid:0)(cid:96)(x) − α(cid:1)  such that (cid:96)(x) = α + 1/2 ˜(cid:96)(x)2  where α is a small positive scalar.1 We

Let ˜(cid:96)(x) :=
then take a GP prior on ˜(cid:96)(x): ˜(cid:96) ∼ GP(0  K). We can then write the posterior for ˜(cid:96) as

(cid:113)

p(˜(cid:96) | D) = GP(cid:0)˜(cid:96); ˜mD(·)  ˜CD(· ·)(cid:1);

˜mD(x) := K(x  xd)K(xd  xd)−1 ˜(cid:96)(xd);

(4)
(5)
(6)
The square-root transformation renders analysis intractable with this GP: we arrive at a process
whose marginal distribution for any (cid:96)(x) is a non-central χ2 (with one degree of freedom). Given
this process  the posterior for our integral is not closed-form. We now describe two alternative
approximation schemes to resolve this problem.

˜CD(x  x(cid:48)) := K(x  x(cid:48)) − K(x  xd)K(xd  xd)−1K(xd  x(cid:48)).

1α was taken as 0.8 × min (cid:96)(xd) in all experiments; our investigations found that performance was insen-

sitive to the choice of this parameter.

3

 TruefunctionGPposteriormean95%conﬁdenceintervalℓ(x)X TruefunctionWSABI-Mposteriormean95%conﬁdenceintervalℓ(x)X3.1 Linearisation
We ﬁrstly consider a local linearisation of the transform f : ˜(cid:96) (cid:55)→ (cid:96) = α + 1/2 ˜(cid:96)2. As GPs are closed
under linear transformations  this linearisation will ensure that we arrive at a GP for (cid:96) given our
existing GP on ˜(cid:96). Generically  if we linearise around ˜(cid:96)0  we have (cid:96) (cid:39) f (˜(cid:96)0) + f(cid:48)(˜(cid:96)0)(˜(cid:96) − ˜(cid:96)0). Note
that f(cid:48)(˜(cid:96)) = ˜(cid:96): this simple gradient is a further motivation for our transform  as described further in
Section 3.3. We choose ˜(cid:96)0 = ˜mD; this represents the mode of p(˜(cid:96) | D). Hence we arrive at

(cid:96)(x) (cid:39)(cid:0)α + 1/2 ˜mD(x)2(cid:1) + ˜mD(x)(cid:0)˜(cid:96)(x) − ˜mD(x)(cid:1) = α − 1/2 ˜mD(x)2 + ˜mD(x) ˜(cid:96)(x).

Under this approximation  in which (cid:96) is a simple afﬁne transformation of ˜(cid:96)  we have

p((cid:96) | D) (cid:39) GP(cid:0)(cid:96); mL

D(·)  CL
D(x) := α + 1/2 ˜mD(x)2;

mL
CL
D(x  x(cid:48)) := ˜mD(x) ˜CD(x  x(cid:48)) ˜mD(x(cid:48)).

D(· ·)(cid:1);

(7)

(8)
(9)
(10)

(11)
(12)

3.2 Moment Matching
Alternatively  we consider a moment-matching approximation: p((cid:96) | D) is approximated as a GP
with mean and covariance equal to those of the true χ2 (process) posterior. This gives p((cid:96) | D) :=

GP(cid:0)(cid:96); mM

D (· ·)(cid:1)  where

D (·)  CM

D (x) := α + 1/2(cid:0) ˜m2D(x) + ˜CD(x  x)(cid:1);

mM
CM
D (x  x(cid:48)) := 1/2 ˜CD(x  x(cid:48))2 + ˜mD(x) ˜CD(x  x(cid:48)) ˜mD(x(cid:48)).

We will call these two approximations WSABI-L (for “linear”) and WSABI-M (for “moment
matched”)  respectively. Figure 2 shows a comparison of the approximations on synthetic data.
The likelihood function  (cid:96)(x)  was deﬁned to be (cid:96)(x) = exp(−x2)  and is plotted in red. We placed
a GP prior on ˜(cid:96)  and conditioned this on seven observations spanning the interval [−2  2]. We then
drew 50 000 samples from the true χ2 posterior on ˜(cid:96) along a dense grid on the interval [−5  5] and
used these to estimate the true density of (cid:96)(x)  shown in blue shading. Finally  we plot the means and
95% conﬁdence intervals for the approximate posterior. Notice that the moment matching results in
a higher mean and variance far from observations  but otherwise the approximations largely agree
with each other and the true density.

3.3 Quadrature

D and CL

D and CM

˜mD and ˜CD are both mixtures of un-normalised Gaussians K. As such  the expressions for poste-
rior mean and covariance under either the linearisation (mL
D  respectively) or the moment-
matching approximations (mM
D   respectively) are also mixtures of un-normalised Gaus-
sians. Substituting these expressions (under either approximation) into (2) and (3) yields closed-
form expressions (omitted due to their length) for the mean and variance of the integral (cid:104)(cid:96)(cid:105). This
result motivated our initial choice of transform: for linearisation  for example  it was only the fact
that the gradient f(cid:48)(˜(cid:96)) = ˜(cid:96) that rendered the covariance in (10) a mixture of un-normalised Gaus-
sians. The discussion that follows is equally applicable to either approximation.
It is clear that the posterior variance of the likelihood model is now a function of both the expected
value of the likelihood at that point  and the distance of that sample location from the rest of xd.
This is visualised in Figure 1b.
Comparing Figures 1a and 1b we see that conditioned on an identical set of samples  WSABI both
achieves a closer ﬁt to the true underlying function  and associates minimal probability mass with
negative function values. These are desirable properties when modelling likelihood functions—both
arising from the use of the square-root transform.

4 Active Sampling

Given a full Bayesian model of the likelihood surface  it is natural to call on the framework of
Bayesian decision theory  selecting the next function evaluation so as to optimally reduce our uncer-

4

Figure 2: The χ2 process  alongside moment matched (WSABI-M) and linearised approxi-
mations (WSABI-L). Notice that the WSABI-L mean is nearly identical to the ground truth.

tainty about either the total integrand surface or the integral. Let us deﬁne this next sample location
to be x∗  and the associated likelihood to be (cid:96)∗ := (cid:96)(x∗). Two utility functions immediately present
themselves as natural choices  which we consider below. Both options are appropriate for either of
the approximations to p((cid:96)) described above.

4.1 Minimizing expected entropy

4.2 Uncertainty sampling

(13)

(14)

(15)

One possibility would be to follow [12] in minimising the expected entropy of the integral  by
selecting x∗ = arg min

x

(cid:68)V(cid:96)|D (cid:96)(x)

(cid:2)(cid:104)(cid:96)(cid:105)(cid:3)(cid:11)   where
(cid:90)

(cid:10)V(cid:96)|D (cid:96)(x)
(cid:2)(cid:104)(cid:96)(cid:105)(cid:3)(cid:69)
(cid:2)(cid:104)(cid:96)(cid:105)(cid:3)N(cid:0)(cid:96)(x); mD(x)  CD(x  x)(cid:1)d(cid:96)(x).
V(cid:96)|D(cid:2)(cid:96)(x)π(x)(cid:3) (this is known as uncertainty sampling)  where
(cid:2)(cid:96)(x)π(x)(cid:3) = π(x)CD(x  x)π(x) = π(x)2 ˜CD(x  x)(cid:0)1/2 ˜CD(x  x) + ˜mD(x)2(cid:1) 

V(cid:96)|D (cid:96)(x)

=

x

VM
(cid:96)|D

Alternatively  we can target the reduction in entropy of the total integrand (cid:96)(x)π(x) instead  by
targeting x∗ = arg max

in the case of our moment matched approximation  and  under the linearisation approximation 

(cid:2)(cid:96)(x)π(x)(cid:3) = π(x)2 ˜CD(x  x) ˜mD(x)2.

VL
(cid:96)|D

The uncertainty sampling option reduces the entropy of our GP approximation to p((cid:96)) rather than
the true (intractable) distribution. The computation of either (14) or (15) is considerably cheaper
and more numerically stable than that of (13). Notice that as our model builds in greater uncertainty
in the likelihood where it is high  it will naturally balance sampling in entirely unexplored regions
against sampling in regions where the likelihood is expected to be high. Our model (the square-
root transform) is more suited to the use of uncertainty sampling than the model taken in [12].
This is because the approximation to the posterior variance is typically poorer for the extreme log-
transform than for the milder square-root transform. This means that  although the log-transform
would achieve greater reduction in dynamic range than any power transform  it would also introduce
the most error in approximating the posterior predictive variance of (cid:96)(x). Hence  on balance  we
consider the square-root transform superior for our sampling scheme.
Figures 3–4 illustrate the result of square-root Bayesian quadrature  conditioned on 15 samples
selected sequentially under utility functions (14) and (15) respectively. In both cases the posterior
mean has not been scaled by the prior π(x) (but the variance has). This is intended to exaggerate the
contributions to the mean made by WSABI-M.
A good posterior estimate of the integral has been achieved  and this set of samples is more infor-
mative than a grid under the utility function of minimising the integral error. In all active-learning

5

 95%CI(WSABI-L)Mean(WSABI-L)95%CI(WSABI-M)Mean(WSABI-M)Mean(groundtruth)χ2processℓ(x)XFigure 3: Square-root Bayesian quadrature
with active sampling according to utility
function (14) and corresponding moment-
matched model. Note the non-zero expected
mean everywhere.

Figure 4: Square-root Bayesian quadrature
with active sampling according to utility
function (15) and corresponding linearised
model. Note the zero expected mean away
from samples.

examples a covariance matrix adaptive evolution strategy (CMA-ES) [14] global optimiser was used
to explore the utility function surface before selecting the next sample.

5 Results

Given this new model and fast active sampling scheme for likelihood surfaces  we now test for speed
against standard Monte Carlo techniques on a variety of problems.

5.1 Synthetic Likelihoods

We generated 16 likelihoods in four-dimensional space by selecting K normal distributions with
K drawn uniformly at random over the integers 5–14. The means were drawn uniformly at random
over the inner quarter of the domain (by area)  and the covariances for each were produced by scaling
each axis of an isotropic Gaussian by an integer drawn uniformly at random between 21 and 29. The
overall likelihood surface was then given as a mixture of these distributions  with weights given by
partitioning the unit interval into K segments drawn uniformly at random—‘stick-breaking’. This
procedure was chosen in order to generate ‘lumpy’ surfaces. We budgeted 500 samples for our new
method per likelihood  allocating the same amount of time to simple Monte Carlo (SMC).
Naturally the computational cost per evaluation of this likelihood is effectively zero  which afforded
SMC just under 86 000 samples per likelihood on average. WSABI was on average faster to converge
to 10−3 error (Figure 5)  and it is visible in Figure 6 that the likelihood of the ground truth is larger
under this model than with SMC. This concurs with the fact that a tighter bound was achieved.

5.2 Marginal Likelihood of GP Regression

As an initial exploration into the performance of our approach on real data  we ﬁtted a Gaussian
process regression model to the yacht hydrodynamics benchmark dataset [15]. This has a six-
dimensional input space corresponding to different properties of a boat hull  and a one-dimensional
output corresponding to drag coefﬁcient. The dataset has 308 examples  and using a squared ex-
ponential ARD covariance function a single evaluation of the likelihood takes approximately 0.003
seconds.
Marginalising over the hyperparameters of this model is an eight-dimensional non-analytic integral.
Speciﬁcally  the hyperparameters were: an output length-scale  six input length-scales  and an output
noise variance. We used a zero-mean isotropic Gaussian prior over the hyperparameters in log space
with variance of 4. We obtained ground truth through exhaustive SMC sampling  and budgeted 1 250
samples for WSABI. The same amount of compute-time was then afforded to SMC  AIS (which
was implemented with a Metropolis–Hastings sampler)  and Bayesian Monte Carlo (BMC). SMC
achieved approximately 375 000 samples in the same amount of time. We ran AIS in 10 steps 
spaced on a log-scale over the number of iterations  hence the AIS plot is more granular than the
others (and does not begin at 0). The ‘hottest’ proposal distribution for AIS was a Gaussian centered
on the prior mean  with variance tuned down from a maximum of the prior variance.

6

 OptimalnextsampleTruefunctionWSABI-Mposteriormean95%ConﬁdenceintervalPriormassℓ(x)X OptimalnextsampleTruefunctionWSABI-Lposteriormean95%ConﬁdenceintervalPriormassℓ(x)XFigure 5: Time in seconds vs. average frac-
tional error compared to the ground truth in-
tegral  as well as empirical standard error
bounds  derived from the variance over the
16 runs. WSABI-M performed slightly better.

Figure 6: Time in seconds versus average
likelihood of the ground truth integral over
16 runs. WSABI-M has a signiﬁcantly larger
variance estimate for the integral as com-
pared to WSABI-L.

Figure 7: Log-marginal likelihood of GP regression on the yacht hydrodynamics dataset.

Figure 7 shows the speed with which WSABI converges to a value very near ground truth compared
to the rest. AIS performs rather disappointingly on this problem  despite our best attempts to tune
the proposal distribution to achieve higher acceptance rates.
Although the ﬁrst datapoint (after 10 000 samples) is the second best performer after WSABI  further
compute budget did very little to improve the ﬁnal AIS estimate. BMC is by far the worst performer.
This is because it has relatively few samples compared to SMC  and those samples were selected
completely at random over the domain. It also uses a GP prior directly on the likelihood  which due
to the large dynamic range will have a poor predictive performance.

5.3 Marginal Likelihood of GP Classiﬁcation

We ﬁtted a Gaussian process classiﬁcation model to both a one dimensional synthetic dataset  as
well as real-world binary classiﬁcation problem deﬁned on the nodes of a citation network [16].
The latter had a four-dimensional input space and 500 examples. We use a probit likelihood model 
inferring the function values using a Laplace approximation. Once again we marginalised out the
hyperparameters.

7

 SMC±1std.errorSMCWSABI-L±1std.errorWSABI-LFractionalerrorvs.groundtruthTimeinseconds02040608010012014016018020010−310−210−1100 SMCWSABI-LAveragelikelihoodofgroundtruthTimeinseconds050100150200×105012345 BMCAISSMCWSABI-MWSABI-LGroundtruthlogZTimeinseconds0200400600800100012001400×104−1.5−1−0.500.515.4 Synthetic Binary Classiﬁcation Problem

We generate 500 binary class samples using a 1D input space. The GP classiﬁcation scheme im-
plemented in Gaussian Processes for Machine Learning Matlab Toolbox (GPML) [17] is employed
using the inference and likelihood framework described above. We marginalised over the three-
dimensional hyperparameter space of: an output length-scale  an input length-scale and a ‘jitter’
parameter. We again tested against BMC  AIS  SMC and  additionally  Doubly-Bayesian Quadrature
(BBQ) [12]. Ground truth was found through 100 000 SMC samples.
This time the acceptance rate for AIS was signiﬁcantly higher  and it is visibly converging to the
ground truth in Figure 8  albeit in a more noisy fashion than the rest. WSABI-L performed partic-
ularly well  almost immediately converging to the ground truth  and reaching a tighter bound than
SMC in the long run. BMC performed well on this particular example  suggesting that the active sam-
pling approach did not buy many gains on this occasion. Despite this  the square-root approaches
both converged to a more accurate solution with lower variance than BMC. This suggests that the
square-root transform model generates signiﬁcant added value  even without an active sampling
scheme. The computational cost of selecting samples under BBQ prevents rapid convergence.

5.5 Real Binary Classiﬁcation Problem

For our next experiment  we again used our method to calculate the model evidence of a GP model
with a probit likelihood  this time on a real dataset.
The dataset  ﬁrst described in [16]  was a graph from a subset of the CiteSeerx citation network.
Papers in the database were grouped based on their venue of publication  and papers from the 48
venues with the most associated publications were retained. The graph was deﬁned by having these
papers as its nodes and undirected citation relations as its edges. We designated all papers appear-
ing in NIPS proceedings as positive observations. To generate Euclidean input vectors  the authors
performed “graph principal component analysis” on this network [18]; here  we used the ﬁrst four
graph principal components as inputs to a GP classiﬁer. The dataset was subsampled down to a set
of 500 examples in order to generate a cheap likelihood  half of which were positive.

Figure 8: Log-marginal likelihood for GP
classiﬁcation—synthetic dataset.

Figure 9: Log-marginal likelihood for GP
classiﬁcation—graph dataset.

Across all our results  it is noticeable that WSABI-M typically performs worse relative to WSABI-L as
the dimensionality of the problem increases. This is due to an increased propensity for exploration
as compared to WSABI-L. WSABI-L is the fastest method to converge on all test cases  apart from the
synthetic mixture model surfaces where WSABI-M performed slightly better (although this was not
shown in Figure 5). These results suggest that an active-sampling policy which aggressively exploits
areas of probability mass before exploring further aﬁeld may be the most appropriate approach to
Bayesian quadrature for real likelihoods.

6 Conclusions

We introduced the ﬁrst fast Bayesian quadrature scheme  using a novel warped likelihood model
and a novel active sampling scheme. Our method  WSABI  demonstrates faster convergence (in
wall-clock time) for regression and classiﬁcation benchmarks than the Monte Carlo state-of-the-art.

8

 BBQBMCAISSMCWSABI-MWSABI-LGroundtruthlogZTimeinseconds050100150200250300350400450−158−156−154−152−150−148−146−144 BBQBMCAISSMCWSABI-MWSABI-LGroundtruthlogZTimeinseconds020040060080010001200140016001800−310−300−290−280−270−260−250−240−230−220References
[1] R.M. Neal. Annealed importance sampling. Statistics and Computing  11(2):125–139  2001.
[2] J. Skilling. Nested sampling. Bayesian inference and maximum entropy methods in science

and engineering  735:395–405  2004.

[3] X. Meng and W. H. Wong. Simulating ratios of normalizing constants via a simple identity: a

theoretical exploration. Statistica Sinica  6(4):831–860  1996.

[4] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical

Report CRG-TR-93-1  University of Toronto  1993.

[5] S.P. Brooks and G.O. Roberts. Convergence assessment techniques for Markov chain Monte

Carlo. Statistics and Computing  8(4):319–335  1998.

[6] M.K. Cowles  G.O. Roberts  and J.S. Rosenthal. Possible biases induced by MCMC conver-

gence diagnostics. Journal of Statistical Computation and Simulation  64(1):87  1999.

[7] P. Diaconis. Bayesian numerical analysis. In S. Gupta J. Berger  editor  Statistical Decision

Theory and Related Topics IV  volume 1  pages 163–175. Springer-Verlag  New York  1988.

[8] A. O’Hagan. Bayes-Hermite quadrature.

29:245–260  1991.

Journal of Statistical Planning and Inference 

[9] M. Kennedy. Bayesian quadrature with non-normal approximating functions. Statistics and

Computing  8(4):365–375  1998.

[10] C. E. Rasmussen and Z. Ghahramani. Bayesian Monte Carlo.

In S. Becker and K. Ober-
mayer  editors  Advances in Neural Information Processing Systems  volume 15. MIT Press 
Cambridge  MA  2003.

[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press 

2006.

[12] M. A. Osborne  D. K. Duvenaud  R. Garnett  C. E. Rasmussen  S. J. Roberts  and Z. Ghahra-
mani. Active learning of model evidence using Bayesian quadrature. In P. Bartlett  F. C. N.
Pereira  C. J. C. Burges  L. Bottou  and K. Q. Weinberger  editors  Advances in Neural Infor-
mation Processing Systems. MIT Press  Cambridge  MA  2012.

[13] T. P. Minka. Deriving quadrature rules from Gaussian processes. Technical report  Statistics

Department  Carnegie Mellon University  2000.

[14] N. Hansen  S. D. M¨uller  and P. Koumoutsakos. Reducing the time complexity of the de-
randomized evolution strategy with covariance matrix adaptation (CMA-ES). Evolutionary
Computation  11(1):1–18  2003.

[15] J Gerritsma  R Onnink  and A Versluis. Geometry  resistance and stability of the delft system-

atic yacht hull series. International shipbuilding progress  28(328)  1981.

[16] R. Garnett  Y. Krishnamurthy  X. Xiong  J. Schneider  and R. P. Mann. Bayesian optimal
active search and surveying. In J. Langford and J. Pineau  editors  Proceedings of the 29th
International Conference on Machine Learning (ICML 2012). Omnipress  Madison  WI  USA 
2012.

[17] C. E. Rasmussen and H. Nickisch. Gaussian processes for machine learning (GPML) toolbox.

The Journal of Machine Learning Research  11(2010):3011–03015.

[18] F. Fouss  A. Pirotte  J-M Renders  and M. Saerens. Random-walk computation of similarities
IEEE Transac-

between nodes of a graph with application to collaborative recommendation.
tions on Knowledge and Data Engineering  19(3):355–369  2007.

9

,Tom Gunter
Michael Osborne
Roman Garnett
Philipp Hennig
Stephen Roberts
Qi Li
Zhenan Sun
Ran He
Tieniu Tan