2019,Model Selection for Contextual Bandits,We introduce the problem of model selection for contextual bandits  where a
learner must adapt to the complexity of the optimal policy while balancing exploration and exploitation. Our main result is a new model selection guarantee for linear contextual bandits. We work in the stochastic realizable setting with a sequence of nested linear policy classes of dimension $d_1 < d_2 < \ldots$ 
where the $m^\star$-th class contains the optimal policy  and we design an
algorithm that achieves $\tilde{O}l(T^{2/3}d^{1/3}_{m^\star})$
regret with no prior knowledge of the optimal dimension
$d_{m^\star}$. The algorithm also achieves regret $\tilde{O}(T^{3/4} + \sqrt{Td_{m^\star}})$ 
which is optimal for $d_{m^{\star}}\geq{}\sqrt{T}$. This is the first model selection result for contextual bandits with non-vacuous regret for
all values of $d_{m^\star}$  and to the best of our knowledge is the first positive result of this type for any online learning setting with partial information. The core of the algorithm is a new estimator for the gap in the best loss
achievable by two linear policy classes  which we show admits a
convergence rate faster than the rate required to learn the parameters for either class.,Model selection for contextual bandits

Massachusetts Institute of Technology

Dylan J. Foster

dylanf@mit.edu

Haipeng Luo

University of Southern California

haipengl@usc.edu

Akshay Krishnamurthy
Microsoft Research NYC
akshay@cs.umass.edu

Abstract

We introduce the problem of model selection for contextual bandits  where a learner
must adapt to the complexity of the optimal policy while balancing exploration
and exploitation. Our main result is a new model selection guarantee for linear
contextual bandits. We work in the stochastic realizable setting with a sequence

of nested linear policy classes of dimension d1< d2< . . .  where the m￿-th class
contains the optimal policy  and we design an algorithm that achieves ˜O(T 2￿3d1￿3
m￿)
also achieves regret ˜O￿T 3￿4+√T dm￿￿  which is optimal for dm￿ ≥√T . This
regret with no prior knowledge of the optimal dimension dm￿. The algorithm
is the ﬁrst model selection result for contextual bandits with non-vacuous regret
for all values of dm￿  and to the best of our knowledge is the ﬁrst positive result
of this type for any online learning setting with partial information. The core of
the algorithm is a new estimator for the gap in the best loss achievable by two
linear policy classes  which we show admits a convergence rate faster than the rate
required to learn the parameters for either class.

1

Introduction

Model selection is the fundamental statistical task of choosing a hypothesis class using data. The
choice of hypothesis class modulates a tradeoff between approximation error and estimation error  as
a small class can be learned with less data  but may have worse asymptotic performance than a richer
class. In the classical statistical learning setting  model selection algorithms provide the following

the sample complexity of the algorithm scales with the statistical complexity of the smallest subclass

luckiness guarantee: If the class of models decomposes as a nested sequenceF1⊂F2⊂￿Fm⊂F 
Fm￿ containing the true model  even though m￿ is not known in advance. Such guarantees date back

to Vapnik’s structural risk minimization principle and are by now well-known (Vapnik  1982  1992;
Devroye et al.  1996; Birgé and Massart  1998; Shawe-Taylor et al.  1998; Lugosi and Nobel  1999;
Koltchinskii  2001; Bartlett et al.  2002; Massart  2007). In practice  one may use cross-validation—
the de-facto model selection procedure—to decide whether to use  for example  a linear model  a
decision tree  or a neural network. That cross-validation appears in virtually every machine learning
pipeline highlights the necessity of model selection for successful ML deployments.
We investigate model selection in contextual bandits  a simple interactive learning setting. Our main
question is: Can model selection guarantees be achieved in contextual bandit learning  where a
learner must balance exploration and exploitation to make decisions online?
Contextual bandit learning is more challenging than statistical learning on two fronts: First  decisions
must be made online without seeing the entire dataset  and second  the learner’s actions inﬂuence what
data is observed (“bandit feedback”). Between these extremes is full-information online learning 
where the learner does not have to deal with bandit feedback  but still makes decisions online. Even
in this simpler setting  model selection is challenging  since the learner must attempt to identify
the appropriate model class while making irrevocable decisions and incurring regret. Nevertheless 
several prior works on so-called parameter-free online learning (McMahan and Abernethy  2013;

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Orabona  2014; Koolen and Van Erven  2015; Luo and Schapire  2015; Foster et al.  2017; Cutkosky
and Boahen  2017) provide algorithms for online model selection with guarantees analogous to those
in statistical learning. With bandit feedback  however  the learner must carefully balance exploration
and exploitation  which presents a substantial challenge for model selection. At an intuitive level  the
reason is that different hypothesis classes require different amounts of exploration  but either over-
or under-exploring can incur signiﬁcant regret (A detailed discussion requires a formal setup and is
deferred to Section 2). At this point  it sufﬁces to say that prior to this work  we are not aware of any
adequate model selection guarantee that adapts results from statistical learning to any online learning
setting with partial information.
We provide a new model selection guarantee for the linear stochastic contextual bandit setting (Chu

et al.  2011; Abbasi-Yadkori et al.  2011). We consider a sequence of feature maps into d1< d2<
. . .< dM dimensions and assume that the losses are linearly related to the contexts via the m￿-th
feature map  so that the optimal policy is a dm￿-dimensional linear policy. We design an algorithm
that achieves ˜O(T 2￿3d1￿3
m￿) regret to this optimal policy over T rounds  with no prior knowledge
of dm￿. As this bound has no dependence on the maximum dimensionality dM  we say that the
algorithm adapts to the complexity of the optimal policy. All prior approaches suffer linear regret
that the problem is learnable. Our algorithm can also be tuned to achieve ˜O￿T 3￿4+√T dm￿￿ regret 
for non-trivial values of dm￿  whereas the regret of our algorithm is sublinear whenever dm￿ is such
which matches the optimal rate when dm￿≥√T .
least ⌦(d) labeled examples  we can estimate the improvement in value of the optimal loss using
only O(√d) examples  analogous to so-called variance estimation results in statistics (Dicker  2014;

At a technical level  we design a sequential test to determine whether the optimal square loss for a
large linear class is substantially better than that of a smaller linear class. We show that this test has
sublinear sample complexity: while learning a near-optimal predictor in d dimensions requires at

Kong and Valiant  2018). Crucially  this implies that we can test whether or not to use the larger class
without over-exploring for the smaller class.

2 Preliminaries

We work in the stochastic contextual bandit setting (Langford and Zhang  2008; Beygelzimer et al. 

loss vector. The learner interacts with nature for T rounds  where in round t: (1) nature samples

goal of the learner is to choose actions to minimize the cumulative loss.
Following several prior works (Chu et al.  2011; Abbasi-Yadkori et al.  2011; Agarwal et al.  2012;
Russo and Van Roy  2013; Li et al.  2017)  we study a variant of the contextual bandit setting where

2011; Agarwal et al.  2014). The setting is deﬁned by a context spaceX   a ﬁnite action space
A∶={1  . . .   K} and a distributionD supported over(x  `) pairs  where x∈X and `∈ RA is a
(xt ` t)∼D  (2) the learner observes xt and chooses action at  (3) the learner observes `t(at). The
the learner has access to a class of regression functionsF∶X×A→ R containing the Bayes optimal
We refer to this assumption (f￿∈F) as realizability. For each regression function f we deﬁne the
induced policy ⇡f(x)∶= argmina f(x  a). Note that ⇡￿∶= ⇡f￿ is the globally optimal policy  and
chooses the best action on every context. We measure performance via regret to ⇡￿:

f￿(x  a)∶= E[`(a)￿ x] ∀x  a.

regressor

(1)

Reg∶= T￿t=1

`t(at)− T￿t=1

`t(⇡￿(xt)).

Low regret is tractable here due to the realizability assumption  and it is well known that the optimal

regret is ˜⇥￿￿T⋅ comp(F)￿  where comp(F) measures the statistical complexity ofF. For example 
comp(F)= log￿F￿ for ﬁnite classes  and comp(F)= d for d-dimensional linear classes (Agarwal
plexity of the optimal regressor f￿ rather than the worst-case complexity of the classF. To this end 

et al.  2012; Chu et al.  2011).1
Model selection for contextual bandits. We aim for reﬁned guarantees that scale with the com-

1We suppress dependence on K and logarithmic dependence on T for this discussion.

2

Fm∶=￿(x  a)￿￿  m(x  a)￿￿ ∈ Rdm￿  

rather than the less favorable ˜O(￿T⋅ comp(F))?

precisely whenever the optimal model class is learnable. This implies that the bound  in spite of
having worse dependence on T   adapts to the complexity of the optimal class with no prior knowledge.
We achieve this type of guarantee for linear contextual bandits. We assume that each regressor class

we assume thatF is structured as a nested sequence of classesF1⊂F2⊂ . . .⊂FM =F  and we
deﬁne m￿∶= min{m∶ f￿∈Fm}. The model selection problem for contextual bandits asks:
Given that m￿ is not known in advance  can we achieve regret scaling as ˜O(￿T⋅ comp(Fm￿)) 
A slightly weaker model selection problem is to achieve ˜O￿T ↵⋅ comp(Fm￿)1−↵￿ for some ↵∈
[1￿2  1)  again without knowing m￿. Crucially  the exponents on T and comp(Fm￿) sum to one 
implying that we can achieve sublinear regret whenever comp(Fm￿) is sublinear in T   which is
Fm consists of linear functions of the form
where m∶X×A→ Rdm is a ﬁxed feature map. To obtain a nested sequence of classes  and to
ensure the complexity is monotonically increasing  we assume that d1< d2< . . .   dM= d and that
for each m  the feature map m contains the map m−1 as its ﬁrst dm−1 coordinates.2 If m￿ is the
where ￿∈ Rdm￿ is the optimal coefﬁcient vector. In this setup  the optimal rate if m￿ is known is
˜O(√T dm￿)  obtained by LinUCB (Chu et al.  2011).3 Our main result achieves both ˜O(T 2￿3d1￿3
m￿)
regret (i.e.  ↵= 2￿3) and ˜O￿T 3￿4+√T dm￿￿ regret without knowing m￿ in advance.
learner for each sub-classFm and aggregate these base learners with a master Hedge instance (Freund
methods do not depend on the so-called “local norms”  which are essential for achieving√T -regret

and Schapire  1997). Other strategies include parameter-free methods like AdaNormalHedge (Luo
and Schapire  2015) and Squint (Koolen and Van Erven  2015). Unfortunately  none of these methods
appear to adequately handle bandit feedback. For example  the regret bounds of parameter-free

Related work. The model selection guarantee we seek is straightforward for full information online
learning and statistical learning. A simple strategy for the former setting is to use a low-regret online

f￿(x  a)=￿￿  m￿(x  a)￿ 

smallest feature map that realizes the optimal regressor  we can write

in the bandit setting via the usual importance weighting approach (Auer et al.  2002). See Appendix B
for further discussion.
In the bandit setting  two approaches we are aware of also fail: the Corral algorithm of Agarwal
et al. (2017b)  and an adaptive version of the classical ✏-greedy strategy (Langford and Zhang  2008).

Unfortunately  both algorithms require tuning parameters in terms of the unknown index m￿  and
naive tuning gives a guarantee of the form ˜O(T ↵comp(Fm￿)) where ↵+ > 1. For example  for
ﬁnite classes Corral gives regret√T log￿Fm￿￿. This guarantee is quite weak  since it is vacuous
when log￿Fm￿￿= ⇥(√T) even though such a class admits sublinear regret if m￿ were known in
O(T ↵comp(Fm￿)1−↵)-type rates.

advance (see Appendix B). The conceptual takeaway from these examples is that model selection
for contextual bandits appears to require new algorithmic ideas  even when we are satisﬁed with

Several recent papers have developed adaptive guarantees for various contextual bandit settings.
These include: (1) adaptivity to easy data  where the optimal policy achieves low loss (Allenberg
et al.  2006; Agarwal et al.  2017a; Lykouris et al.  2018; Allen-Zhu et al.  2018)  (2) adaptivity to
smoothness in settings with continuous action spaces (Locatelli and Carpentier  2018; Krishnamurthy
et al.  2019)  and (3) adaptivity in non-stationary environments  where distribution drift parameters
are unknown (Luo et al.  2018; Cheung et al.  2019; Auer et al.  2018; Chen et al.  2019). The latter
results can be cast as model selection with appropriate nested classes of time-dependent policies  but
these results are incomparable to our own  since they are specialized to the non-stationary setting.

2This is without loss of generality in a certain quantitative sense  since we can concatenate features without

signiﬁcantly increasing the complexity ofFm. See Corollary 5.
3Regret scaling as ˜O(√dT) is optimal for the ﬁnite action setting we work in. Results for the inﬁnite action
case  where regret scales as ˜⇥(d√T)  are incomparable to ours.

3

Interestingly  for multi-armed (non-contextual) bandits  several lower bounds demonstrate that model
selection is not possible. The simplest of these results is Lattimore’s pareto frontier (Lattimore  2015) 

which states that for multi-armed bandits  if we want to ensure O(√T) regret against a single ﬁxed
arm instead of the usual O(√KT) rate  we must incur ⌦(K√T) regret to the remaining K− 1
arms. This precludes a model selection guarantee of the form￿T⋅ comp(A) since for bandits  the

statistical complexity is simply the number of arms. Related lower bounds are known for Lipschitz
bandits (Locatelli and Carpentier  2018; Krishnamurthy et al.  2019). Our results show that model
selection is possible for contextual bandits  and thus highlight an important gap between the two
settings.
In concurrent work  Chatterji et al. (2019) studied a similar model selection problem with two classes 
where the ﬁrst class consists of all K constant policies and the second is a d-dimensional linear

assumptions on the context distribution are strictly stronger than our own. A detailed discussion is
deferred to the end of the section.
Technical preliminaries and assumptions. For a matrix A  A† denotes the pseudoinverse and

class. They obtain logarithmic regret to the ﬁrst class and O(√T d) regret to the second  but their
￿A￿2 denotes the spectral norm. Id denotes the identity matrix in Rd×d and￿⋅￿p denotes the `p norm.

We use non-asymptotic big-O notation  and use ˜O to hide terms logarithmic in K  dM  M  and T .
For a real-valued random variable z  we use the following notation to indicate if z is subgaussian or
subexponential  following Vershynin (2012):

z∼ subE()⇔ sup

p≥1{p−1(E￿z￿p)1￿p}≤ .

(2)

p≥1{p−1￿2(E￿z￿p)1￿p}≤  

familiar deﬁnitions for subgaussian/subexponential random variables; see Appendix C.1.

We require a lower bound on the eigenvalues of the second moment matrices for the feature vectors.

z∼ subG(2)⇔ sup
When z is a random variable in Rd  we write z∼ subGd(2) if￿✓  z￿∼ subG(2) for all￿✓￿2= 1
and z∼ subEd() if￿✓  z￿∼ subE() for all￿✓￿2= 1. These deﬁnitions are equivalent to many other
m) under x∼D. Nestedness implies that
We assume that for each m and a∈A  m(x  a)∼ subG(⌧ 2
⌧1≤ ⌧2≤ . . .  and we deﬁne ⌧ = ⌧M. We also assume that `(a)− E[`(a)￿ x]∼ subG(2) for all
x∈X and a∈A. Finally  we assume that￿￿￿≤ B. To keep notation clean  we use the convention
that ≤ ⌧ and B≤ 1  which ensures that `(a)∼ subG(4⌧ 2).
m ∶= min(⌃m)  where
For each m  deﬁne ⌃m ∶= 1
min(⋅) denotes the smallest eigenvalue; nestedness implies 1≥ 2≥ . . .. We assume m≥ > 0
requires a lower bound on min(E[(x  a)(x  a)￿]) for all actions. Previous work suggests that

for all m  and our regret bounds scale inversely proportional to .
Note that prior linear contextual bandit algorithms (Chu et al.  2011; Abbasi-Yadkori et al.  2011) do
not require lower bounds on the second moment matrices. As discussed earlier  the work of Chatterji
et al. (2019) obtains stronger model selection guarantees in the case of two classes  but their result

advanced exploration is not needed under such assumptions (Bastani et al.  2017; Kannan et al.  2018;
Raghavan et al.  2018)  which considerably simpliﬁes the problem.4 As such  the result should be
seen as complementary to our own. Whether model selection can be achieved without some type of
eigenvalue condition is an important open question.

K∑a∈A Ex∼D[m(x  a)m(x  a)￿]. We let 2

3 Model selection for linear contextual bandits

We now present our algorithm for model selection in linear contextual bandits  ModCB (“Model
Selection for Contextual Bandits”). Pseudocode is displayed in Algorithm 1. The algorithm maintains

an “active” policy class index̂m∈[M]  which it updates over the T rounds starting from̂m= 1. The
algorithm updateŝm only when it can prove that̂m≠ m￿  which is achieved through a statistical test
called EstimateResidual (Algorithm 2). When̂m is not being updated  the algorithm operates as if
assumption. Consider the case d= 2 and ￿=(1￿2  1). Suppose there are four actions  and that at the ﬁrst
round  (x ⋅)={e1 −e1  e2 −e2}. We can ensure that with probability 1￿2  the ﬁrst action played will be one

of the ﬁrst two. At this point a greedy strategy will always choose e1  but the average context distribution has
minimum eigenvalue 1.

4It appears that exploration is still required for linear contextual bandits under our average eigenvalue

4

Algorithm 1 ModCB (Model Selection for Contextual Bandits)

input:

t

1

Receive xt.

otherwise

deﬁnitions:

• Deﬁne T min

initialization:

• Feature maps{m(⋅ ⋅)}m∈[M]  where m(x  a)∈ Rdm  and time T∈ N.
• Subgaussian parameter ⌧> 0  second moment parameter > 0.
• Failure probability  ∈ (0  1)  exploration parameter  ∈ (0  1)  conﬁdence parameters.
C1  C2> 0.
• Deﬁne 0= ￿10M 2T 2 and µt=(K￿t)∧ 1.
m log2(2dm￿0)
4⋅ d1￿2
8 ⋅ dm log(2￿0)
+ ⌧ 10
• Deﬁne ↵m t= C1⋅￿ ⌧ 6
￿.
Kt1−
1−(2￿0)+ K￿+ 1.
2⋅ dm log(2￿0)+ log
m = C2⋅￿ ⌧ 4
• ̂m← 1. // Index of candidate policy class.
• Exp4-IX1← Exp4-IX(⇧1  T  0).
• S←{￿}. // Times at which uniform exploration takes place.
for t= 1  . . .   T do
with probability 1− µt
Feed xt into Exp4-IX̂m and take at to be the predicted action.
Update Exp4-IX̂m with(xt  at ` t(at)).
Sample at uniformly fromA and let S← S∪{t}.
̂⌃i← 1
K∑a∈A∑t
Hi←{(i(xs  as) `(as))}s∈S for each i>̂m.
̂Êm i← EstimateResidual￿Hi ̂⌃̂m ̂⌃i￿ for each i>̂m. // Gap estimate.
if there exists i>̂m such that̂Êm i≥ 2↵i t and t≥ T min
Let̂m be the smallest such i. Re-initialize Exp4-IX̂m← Exp4-IX(⇧̂m  T  0).
̂m= m￿ by running a low-regret contextual bandit algorithm with the policies induced byF̂m; we
call this policy class ⇧m∶={x￿ argmina∈A￿  m(x  a)￿￿￿￿2≤ ⌧￿}.5 Note that the low-regret
algorithm we run for ⇧̂m cannot based on realizability  sinceF̂m will not contain the true regressor
f￿ until we reach m￿. This rules out the usual linear contextual bandit algorithms such as LinUCB.

s=1 i(xs  a)i(xs  a)￿ for each i≥̂m. // Empirical second moment.

Instead we use a variant of Exp4-IX (Neu  2015)  which is an agnostic contextual bandit algorithm
and does not depend on realizability. To deal with inﬁnite classes  unbounded losses  and other
technical issues  we require some simple modiﬁcations to Exp4-IX; pseudocode and analysis are
deferred to Appendix C.3.

/* Test whether we should move on to a larger policy class. */

then

i

3.1 Key idea: Estimating prediction error with sublinear sample complexity

Before stating the main result  we elaborate on the statistical test (EstimateResidual) used in Algo-
rithm 1. EstimateResidual estimates an upper bound on the gap between the best-in-class loss for two

policy classes ⇧i and ⇧j  which we deﬁne as i j∶= L￿i− L￿j   where L￿i ∶= min⇡∈⇧i L(⇡). At each
round  Algorithm 1 uses EstimateResidual to estimate the gap ̂m i for all i>̂m. If the estimated gap
is sufﬁciently large for some i  the algorithm setŝm to the smallest such i for the next round. This
approach is based on the following observation: For all m≥ m￿  L￿m= L￿m￿. Hence  if ̂m i> 0  it
must be the case that̂m≠ m￿  and we should move on to a larger class.
The key challenge is to estimate ̂m i while ensuring low regret. Naively  we could use uniform
exploration and ﬁnd a policy in ⇧i that has minimal empirical loss  which gives an estimate of L￿i .
the regret if̂m= m￿. Similar tuning issues arise with other approaches and are discussed further in
5The norm constraint ⌧￿ guarantees that ⇧m contains parameter vectors arising from a certain square loss
minimization problem under our assumption that￿￿￿2≤ 1; see Proposition 20.

Unfortunately  this requires tuning the exploration parameter in terms of di and would compromise

Appendix B.

5

Algorithm 2 EstimateResidual

input: Examples{(xs  ys)}n
Deﬁne d2= d− d1 and

Return estimator

s=1  second moment matrix estimateŝ⌃∈ Rd×d and̂⌃1∈ Rd1×d1.
̂R= ̂D†−̂⌃†  where ̂D=￿ ̂⌃1
0d2×d2 ￿ .
0d1×d2
0d2×d1
2￿￿s<t￿̂⌃1￿2̂Rxsys ̂⌃1￿2̂Rxtyt￿.
̂E= 1
￿n

We do not estimate the gaps i j directly  but instead estimate an upper bound motivated by the
realizability assumption. For each m  deﬁne

and deﬁne6

1

￿m∶= argmin
∈Rdm
Ei j∶= 1
K ￿a∈A

Ex∼D(￿  m(x  a)￿− `(a))2 
K ￿a∈A
Ex∼D￿￿￿i   i(x  a)￿−￿￿j   j(x  a)￿￿2

(3)

(4)

.

is that the square loss gap upper bounds the policy gap.

With realizability  the square loss gap behaves similar to the policy gap: it is non-zero whenever

We callEi j the square loss gap and we call i j the policy gap. A key lemma driving these deﬁnitions
Lemma 1. For all i∈[M] and j≥ m￿  i j≤￿4KEi j. Furthermore  if i  j≥ m￿ thenEi j= 0.
the latter is non-zero  and m￿ has zero gap to all m≥ m￿. Therefore  we seek estimators for the
square loss gapÊm i for i>̂m. Observe thatÊm i depends on the optimal predictors ￿̂m ￿i in the
two classes. A natural approach to estimateÊm i is to solve regression problems over both classes to
estimate the predictors  then plug them into the expression forE; we call this the plug-in approach.
As this relies on linear regression  it gives an O(di￿n) error rate for estimatingÊm i from n uniform
As a key technical contribution  we design more efﬁcient estimators for the square loss gapÊm i. We
work in the following slightly more general gap estimation setup: we receive pairs{(xs  ys)}n
s=1 i.i.d.
from a distributionD∈ (Rd× R)  where x∼ subG(⌧ 2) and y∼ subG(2). We partition the feature
space into x=(x(1)  x(2))  where x(1)∈ Rd1  and deﬁne

exploration samples. Unfortunately  since the error scales linearly with the size of the larger class  we
must over-explore to ensure low error  and this compromises the regret if the smaller class is optimal.

￿∶= argmin
∈Rd

E(￿  x￿− y)2  

￿1∶= argmin
∈Rd1

E￿￿  x(1)￿− y￿2

.

These are  respectively  the optimal linear predictor and the optimal linear predictor restricted

The pseudocode for our estimator EstimateResidual is displayed in Algorithm 2. In addition to the

to the ﬁrst d1 dimensions. The square loss gap for the two predictors is deﬁned as E ∶=
E￿￿￿  x￿−￿￿1   x(1)￿￿2. Our problem of estimatingÊm i clearly falls into this general setup if
we uniformly explore the actions for n rounds  then set{xs}n
s=1 to be the features obtained through
the feature map i and{ys}n
s=1 to be the observed losses.
n labeled samples  it takes as input two empirical second moment matriceŝ⌃ and̂⌃1 constructed
via an extra set of m i.i.d. unlabeled samples; these serve as estimates for ⌃ ∶= E[xx￿] and
⌃1∶= E￿x(1)x(1)￿￿. The intuition is that one can write the square loss gap asE=￿⌃1￿2R E[xy]￿2
where R∈ Rd×d is a certain function of ⌃ and ⌃1. EstimateResidual simply replaces the second
weighted norm of E[xy] through a U-statistic. The main guarantee for the estimator is as follows.
6In Appendix D we show that ￿m and consequentlyEi j are always uniquely deﬁned.

moment matrices with their empirical counterparts and then uses the labeled examples to estimate the

2

6

Theorem 2. Suppose we takê⌃ and̂⌃1 to be the empirical second moment matrices formed from m
i.i.d. unlabeled samples. Then when m≥ C(d+ log(2￿))⌧ 4￿min(⌃)  EstimateResidual  given n
labeled samples  guarantees that with probability at least 1−  

⌧ 6

￿̂E−E￿≤ 1

2E+ O￿ 2⌧ 4

min(⌃)⋅ d1￿2 log2(2d￿)

2

n

+

4

min(⌃)⋅ d log(2￿)m

⋅￿E[xy]￿2
2￿.

(5)

To compare with the plug-in approach  we focus on the dependence between d and n. When
EstimateResidual is applied within ModCB we have plenty of unlabeled data  so the dependence

estimator has sublinear sample complexity. This property is crucial for our model selection result.
The result generalizes and is inspired by the variance estimation method of Dicker (Dicker  2014;

3.2 Main result
Equipped with EstimateResidual  we can now sketch the approach behind ModCB in a bit more detail.

on m is less important. The dominant term in Theorem 2 is ˜O(√d￿n)  a signiﬁcant improvement
over the ˜O(d￿n) rate for the plug-in estimator. In particular  the dependence on the larger ambient
dimension is much milder: we can achieve constant error with n ￿ √d  or in other words the
Kong and Valiant  2018)  which obtains a rate of O￿√d￿n+ 1￿√n￿ to estimate the optimal square loss
min∈Rd E(￿  x￿− y)2 when the second moments are known. By estimating the square loss gap
instead of the loss itself  we avoid the 1￿√n term  which is critical for achieving ˜O(T 2￿3d1￿3
m￿) regret.
Recall that the algorithm maintains an index̂m denoting the current guess for m￿. We run Exp4-IX
i>̂m such that the estimated gap satisﬁeŝÊm i≥ 2↵i t and t≥ T min
bound in Theorem 2—implies thatÊm i> 0 and thuŝm≠ m￿. If this is the case  we advancêm to the
Theorem 3. When C1 and C2 are sufﬁciently large absolute constants and = 1￿3  ModCB guaran-
tees that with probability at least 1−  
Reg≤ ˜O￿ ⌧ 4
3⋅(T m￿)2￿3(Kdm￿)1￿3 log(2￿)￿.
When = 1￿4  ModCB guarantees that with probability at least 1−  
2⋅ K1￿4(T m￿)3￿4 log(2￿)+ ⌧ 5

over the induced policy class ⇧m  mixing in some additional uniform exploration (with probability
µt at round t). We use all of the data to estimate the second moment matrices of all classes  and we
pass only the exploration data into the subroutine EstimateResidual. We check if there exists some
which—based on the deviation

4⋅￿K(T m￿)dm￿ log(2￿)￿.

smallest such i  and if not  we continue with our current guess. This leads to the following guarantee.

Reg≤ ˜O￿ ⌧ 3

(6)

(7)

i

A few remarks are in order

• The two stated bounds are incomparable in general. Tracking only dependence on T and

m￿) while the latter is ˜O(T 3￿4+√T dm￿). The former is better
dm￿  the ﬁrst is ˜O(T 2￿3d1￿3
when dm￿≤ T 1￿4. There is a more general Pareto frontier that can be explored by choosing
∈[1￿3  1￿4]  but no choice for  dominates the others for all values of dm￿.
˜O(√T dm￿) regret. The bound (7) matches this oracle rate when dm￿>√T   but otherwise
• Recall that if had we known dm￿ in advance  we have could simply run LinUCB to achieve
our guarantee is slightly worse than the oracle rate. Nevertheless  both bounds are o(T)
whenever the oracle rate is o(T) (that is  when dm￿= o(T))  so the algorithm has sublinear
regret whenever the optimal model class is learnable. It remains open whether there is a
model selection algorithm that can match the oracle rate for all values of dm￿ simultaneously.
• We have not optimized dependence on the condition number ⌧￿ or the logarithmic factors.
• If the individual distribution parameters{⌧m}m∈[M] and{m}m∈[M] are known  the algo-
rithm can be modiﬁed slightly so that regret scales in terms of ⌧m￿ and −1
m￿. However the
current model  in which we assume access only to uniform upper and lower bounds on these
parameters  is more realistic.

7

Additionally  Appendix A contains a validation experiment  where we demonstrate that ModCB
compares favorably to LinUCB in simulations.

this preserves the relevant details but simpliﬁes the argument. The analysis has two cases depending

guarantee for Exp4-IX using policy class ⇧1  and by accounting for uniform exploration.

algorithm never advances. We can bound regret as

the following improved regret bounds.

Non-nested feature maps. As a ﬁnal variant  we note that the algorithm easily extends to the case

Corollary 5. For non-nested feature maps  ModCB with preprocessing guarantees that with proba-

˜O￿ ⌧ 4
3⋅ T 2￿3(Kdm￿)1￿3 log(2￿)￿ 
˜O￿ ⌧ 3
2⋅ K1￿4T 3￿4 log(2￿)+ ⌧ 5

4⋅√KT dm￿ log(2￿)￿ 

Improving the dependence on m￿. Theorem 3 obtains the desired model selection guarantee for
linear classes  but the bound includes a polynomial dependence on the optimal index m￿. This
contrasts the logarithmic dependence on m￿ that can be obtained through structural risk minimization
in statistical learning (Vapnik  1992). However  this poly(m￿) dependence can be replaced by a single
log(T) factor with a simple preprocessing step: Given feature maps{m(⋅ ⋅)}m∈[M] we construct a
new collection of maps￿ ¯m(⋅ ⋅)￿m∈￿ ¯M￿  where ¯M≤ log T   as follows. First  for i= 1  . . .   log T  
take ¯i to be the largest feature map m for which dm≤ ei. Second  remove any duplicates. This
preprocessing reduces the number of feature maps to at most log(T) while ensuring that a map of
dimension O(dm￿) that contains m￿ is always available. Speciﬁcally  the preprocessing step yields
Theorem 4. ModCB with preprocessing guarantees that with probability at least 1−  
Reg≤￿￿￿￿￿￿￿￿￿￿￿
= 1￿3.
= 1￿4.
where feature maps are not nested. Suppose we have non-nested feature maps{m}m∈[M]  where
d1≤ d2≤ . . .≤ dM; note that the inequality is no longer strict. In this case  we can obtain a nested
collection by concatenating 1  . . .   m−1 to the map m for each m. This process increases the
dimension of the optimal map from dm￿ to at most dm￿m￿  so we have the following corollary.
bility at least 1−  
Reg≤￿￿￿￿￿￿￿￿￿￿￿
˜O￿ ⌧ 4
= 1￿3.
3⋅ T 2￿3(Kdm￿m￿)1￿3 log(2￿)￿ 
= 1￿4.
˜O￿ ⌧ 3
2⋅ K1￿4T 3￿4 log(2￿)+ ⌧ 5
on the case where there are just two classes  so M= 2. We only track dependence on T and dm  as
on whether f￿ belongs toF1 orF2.
First  if f￿∈F1 then by Lemma 1 we have thatE1 2= 0. Further  via Theorem 2  we can guarantee
that we never advance to ̂m= 2 with high probability. The result then follows from the regret
The more challenging case is when f￿∈F2. Let̂T denote the ﬁrst round wherêm= 2  or T if the
The four terms correspond to: (1) uniform exploration with probability µt￿ t− in round t  (2) the
Exp4-IX regret bound for class ⇧1 until timêT   (3) the policy gap between the best policy in ⇧1 and
the optimal policy ⇡￿∈ ⇧2  and (4) the Exp4-IX bound over class ⇧2 until time T . The two regret
bounds (the second and fourth terms) clearly contribute O(√T d2) to the overall regret  and the ﬁrst
term is controlled by our choice of ∈{1￿4  1￿3}. We are left to bound the third term. For this
term  observe that in round̂T− 1  since the algorithm did not advance  we must havêE1 2≤ 2↵2 ̂T−1.
Appealing to Theorem 2  this implies thatE1 2≤ O(̂E1 2+ ↵2 ̂T−1)≤ O(↵2 ̂T−1). Plugging in the

Reg≤ O￿T 1−￿+ ˜O￿￿̂T d1￿+̂T 1 2+ ˜O￿￿(T−̂T)d2￿ .

3.3 Proof sketch
We now sketch the proof of the main theorem  with the full proof deferred to Appendix D. We focus

4⋅√KT dm￿m￿ log(2￿)￿ 

deﬁnition of ↵2 t and applying Lemma 1  this gives

̂T 1 2≤ O￿̂T￿E1 2￿≤ O￿̂T￿↵2 ̂T−1￿= O￿T

2 d1￿4
2 ￿ .
1+

8

(8)

The sublinear estimation rate for EstimateResidual (Theorem 2) plays a critical role in this argument.

2 ) regret  and with = 1￿4 we obtain ˜O(T 3￿4+√T d2+ T 5￿8d1￿4

This establishes the result. In particular  with = 1￿3 we obtain ˜O(T 2￿3+√T d2+ T 2￿3d1￿4
2 )≤
2 )= ˜O(T 3￿4+√T d2).7
˜O(T 2￿3d1￿3
With the ˜O(d￿n) rate for the naïve plug-in estimator  we could at best set ↵t 2 = d2￿t1−  but this
to√d2. Unfortunately  this results in
would degrade the dimension dependence in (8) from d1￿4
T 1−+√d2T 1+
for ∈(0  1) for which the exponents on d2 and T sum to one for both terms.

regret  which is not a meaningful model selection result  since there is no choice

2

2

4 Discussion

model selection is negligible.

This paper initiates the study of model selection tradeoffs in contextual bandits. We provide the
ﬁrst model selection algorithm for the linear contextual bandit setting  which we show achieves

2. Is it possible to generalize our results beyond linear classes? Speciﬁcally  given regressor

tual bandit algorithm that adapts the complexity of the optimal policy class with no prior knowledge 
and raises a number of intriguing questions:

˜O￿T 2￿3d1￿3
m￿￿ when the optimal model is a dm￿-dimensional linear function. This is the ﬁrst contex-
1. Is it possible to achieve ˜O￿√T dm￿￿ regret in our setup? This would show that the price for
classesF1⊂F2⊂ . . .⊂FM and assuming the optimal model f￿ belongs toFm￿ for some
unknown m￿  is there a contextual bandit algorithm that achieve ˜O￿T ↵⋅ comp1−↵(Fm￿)￿
regret for some ↵∈[1￿2  1)? More ambitiously  can we achieve ˜O￿￿T⋅ comp(Fm￿)￿?

3. We have conducted a validation experiment with ModCB (see Appendix A)  demonstrating
that the algorithm performs favorably in simulations. While this synthetic experiment is
encouraging  ModCB may not be immediately useful for practical deployments for several
reasons  including its reliance on linear realizability and its computational complexity. Are
there more robust algorithmic principles for theoretically-sound and practically-effective
model selection in contextual bandits?

4. For what classesF can we estimate the loss at a sublinear rate  and is this necessary

for contextual bandit model selection? Any sublinear guarantee will lead to non-trivial
model selection guarantees through a strategy similar to ModCB. Interestingly  it is already
known that for certain (e.g.  sparse linear) classes  sublinear loss estimation is not possible
(Verzelen and Gassiat  2018). On the other hand  positive results are available for certain
nonparametric classes (Brown et al.  2007; Wang et al.  2008).

Model selection is instrumental to the success of ML deployments  yet few positive results exist
for partial feedback settings. We believe these questions are technically challenging and practically
important  and we are hopeful that positive results of the type we provide here will extend to
interactive learning settings beyond contextual bandits.

Acknowledgements

We thank Ruihao Zhu for working with us at the early stages of this project  and for many helpful
discussions. AK is supported by NSF IIS-1763618. HL is supported by NSF IIS-1755781.

References
Yasin Abbasi-Yadkori  Dávid Pál  and Csaba Szepesvári. Improved algorithms for linear stochastic

bandits. In Advances in Neural Information Processing Systems  2011.

Alekh Agarwal  Miroslav Dudík  Satyen Kale  John Langford  and Robert E Schapire. Contextual
bandit learning with predictable rewards. In International Conference on Artiﬁcial Intelligence
and Statistics  2012.

7Note that if d2≤√T then T 5￿8d1￿4

2 ≤ T 3￿4  but if d2≥√T then T 5￿8d1￿4

2 ≤√T d2.

9

Alekh Agarwal  Daniel Hsu  Satyen Kale  John Langford  Lihon Li  and Robert E. Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
Machine Learning  2014.

Alekh Agarwal  Akshay Krishnamurthy  John Langford  Haipeng Luo  and Robert E. Schapire. Open
problem: First-order regret bounds for contextual bandits. In Conference on Learning Theory 
2017a.

Alekh Agarwal  Haipeng Luo  Behnam Neyshabur  and Robert E Schapire. Corralling a band of

bandit algorithms. Conference on Learning Theory  2017b.

Zeyuan Allen-Zhu  Sébastien Bubeck  and Yuanzhi Li. Make the minority great again: First-order

regret bound for contextual bandits. International Conference on Machine Learning  2018.

Chamy Allenberg  Peter Auer  László Györﬁ  and György Ottucsák. Hannan consistency in on-line
learning in case of unbounded losses under partial monitoring. In International Conference on
Algorithmic Learning Theory. Springer  2006.

Peter Auer  Nicolo Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. The nonstochastic multiarmed

bandit problem. SIAM Journal on Computing  2002.

Peter Auer  Pratik Gajane  and Ronald Ortner. Adaptively tracking the best arm with an unknown

number of distribution changes. In European Workshop on Reinforcement Learning  2018.

Peter L Bartlett  Stéphane Boucheron  and Gábor Lugosi. Model selection and error estimation.

Machine Learning  2002.

Hamsa Bastani  Mohsen Bayati  and Khashayar Khosravi. Mostly exploration-free algorithms for

contextual bandits. arXiv:1704.09011  2017.

Shai Ben-David  Nicolo Cesa-Bianchi  David Haussler  and Philip M Long. Characterizations of
learnability for classes of (0 ...  n)-valued functions. Journal of Computer and System Sciences 
1995.

Alina Beygelzimer  John Langford  Lihong Li  Lev Reyzin  and Robert Schapire. Contextual
bandit algorithms with supervised learning guarantees. In International Conference on Artiﬁcial
Intelligence and Statistics  2011.

Lucien Birgé and Pascal Massart. Minimum contrast estimators on sieves: exponential bounds and

rates of convergence. Bernoulli  1998.

Lawrence D Brown  Michael Levine  et al. Variance estimation in nonparametric regression via the

difference sequence method. The Annals of Statistics  2007.

Niladri S. Chatterji  Vidya Muthukumar  and Peter L. Bartlett. Osom: A simultaneously optimal

algorithm for multi-armed and linear contextual bandits. arXiv:1905.10040  2019.

Yifang Chen  Chung-Wei Lee  Haipeng Luo  and Chen-Yu Wei. A new algorithm for non-stationary
contextual bandits: Efﬁcient  optimal  and parameter-free. Conference on Learning Theory  2019.

Wang Chi Cheung  David Simchi-Levi  and Ruihao Zhu. Learning to optimize under non-stationarity.

International Conference on Artiﬁcial Intelligence and Statistics  2019.

Wei Chu  Lihong Li  Lev Reyzin  and Robert Schapire. Contextual bandits with linear payoff

functions. In International Conference on Artiﬁcial Intelligence and Statistics  2011.

Ashok Cutkosky and Kwabena A. Boahen. Online learning without prior information. Conference

on Learning Theory  2017.

Amit Daniely  Sivan Sabato  Shai Ben-David  and Shai Shalev-Shwartz. Multiclass learnability and

the ERM principle. The Journal of Machine Learning Research  2015.

Victor de la Peña and Evarist Giné. Decoupling: From Dependence to Independence. Springer  1998.

10

Luc Devroye  Lázló Györﬁ  and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition.

Springer  1996.

Lee H Dicker. Variance estimation in high-dimensional linear models. Biometrika  2014.

Dylan J. Foster  Satyen Kale  Mehryar Mohri  and Karthik Sridharan. Parameter-free online learning

via model selection. In Advances in Neural Information Processing Systems  2017.

Dylan J. Foster  Alekh Agarwal  Miroslav Dudik  Haipeng Luo  and Robert Schapire. Practical
contextual bandits with regression oracles. In International Conference on Machine Learning 
2018.

Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an

application to boosting. Journal of Computer and System Sciences  1997.

Pierre Gaillard  Gilles Stoltz  and Tim Van Erven. A second-order bound with excess losses. In

Conference on Learning Theory  2014.

David Haussler and Philip M Long. A generalization of Sauer’s lemma. Journal of Combinatorial

Theory  Series A  1995.

Sampath Kannan  Jamie H Morgenstern  Aaron Roth  Bo Waggoner  and Zhiwei Steven Wu. A
smoothed analysis of the greedy algorithm for the linear contextual bandit problem. In Advances
in Neural Information Processing Systems  2018.

Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions

on Information Theory  2001.

Weihao Kong and Gregory Valiant. Estimating learnability in the sublinear data regime. In Advances

in Neural Information Processing Systems  2018.

Wouter M Koolen and Tim Van Erven. Second-order quantile methods for experts and combinatorial

games. In Conference on Learning Theory  2015.

Akshay Krishnamurthy  Alekh Agarwal  and Miro Dudik. Contextual semibandits via supervised

learning oracles. In Advances In Neural Information Processing Systems  2016.

Akshay Krishnamurthy  Zhiwei Steven Wu  and Vasilis Syrgkanis. Semiparametric contextual bandits.

In International Conference on Machine Learning  2018.

Akshay Krishnamurthy  John Langford  Aleksandrs Slivkins  and Chicheng Zhang. Contextual
bandits with continuous actions: Smoothing  zooming  and adapting. Conference on Learning
Theory  2019.

John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. In Advances in neural information processing systems  2008.

Tor Lattimore. The pareto regret frontier for bandits. In Advances in Neural Information Processing

Systems  2015.

Lihong Li  Yu Lu  and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual

bandits. In International Conference on Machine Learning  2017.

Andrea Locatelli and Alexandra Carpentier. Adaptivity to smoothness in x-armed bandits.

Conference on Learning Theory  2018.

In

Gábor Lugosi and Andrew B Nobel. Adaptive model selection using empirical complexities. Annals

of Statistics  1999.

Haipeng Luo and Robert E Schapire. Achieving all with no parameters: Adanormalhedge. In

Conference on Learning Theory  2015.

Haipeng Luo  Chen-Yu Wei  Alekh Agarwal  and John Langford. Efﬁcient contextual bandits in

non-stationary worlds. Conference on Learning Theory  2018.

11

Thodoris Lykouris  Karthik Sridharan  and Éva Tardos. Small-loss bounds for online learning with

partial information. Conference on Learning Theory  2018.

Pascal Massart. Concentration inequalities and model selection. Springer  2007.

Brendan McMahan and Jacob Abernethy. Minimax optimal algorithms for unconstrained linear

optimization. In Advances in Neural Information Processing Systems  2013.

Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits.

In Advances in Neural Information Processing Systems  2015.

Francesco Orabona. Simultaneous model selection and optimization through parameter-free stochastic

learning. In Advances in Neural Information Processing Systems  2014.

Manish Raghavan  Aleksandrs Slivkins  Jennifer Wortman Vaughan  and Zhiwei Steven Wu. The
externalities of exploration and how data diversity helps exploitation. Conference on Learning
Theory  2018.

Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic

exploration. In Advances in Neural Information Processing Systems  2013.

John Shawe-Taylor  Peter L. Bartlett  Robert C Williamson  and Martin Anthony. Structural risk
minimization over data-dependent hierarchies. IEEE Transactions on Information Theory  1998.

Vladimir Vapnik. Estimation of dependences based on empirical data. Springer-Verlag  1982.

Vladimir Vapnik. Principles of risk minimization for learning theory.

Information Processing Systems  1992.

In Advances in Neural

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. Cambridge

University Press  2012.

Nicolas Verzelen and Elisabeth Gassiat. Adaptive estimation of high-dimensional signal-to-noise

ratios. Bernoulli  2018.

Lie Wang  Lawrence D Brown  T Tony Cai  Michael Levine  et al. Effect of mean on variance

function estimation in nonparametric regression. The Annals of Statistics  2008.

12

,Dylan Foster
Akshay Krishnamurthy
Haipeng Luo