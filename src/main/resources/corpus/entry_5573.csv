2019,Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks,We study the sample complexity of learning one-hidden-layer convolutional neural networks (CNNs) with non-overlapping filters. We propose a novel algorithm called approximate gradient descent for training CNNs  and show that  with high probability  the proposed algorithm with random initialization grants a linear convergence to the ground-truth parameters up to statistical precision. Compared with existing work  our result applies to general non-trivial  monotonic and Lipschitz continuous activation functions including ReLU  Leaky ReLU  Sigmod and Softplus etc. Moreover  our sample complexity  beats existing results in the dependency of the number of hidden nodes and filter size. In fact  our result matches the information-theoretic lower bound for learning one-hidden-layer CNNs with linear activation functions  suggesting that our sample complexity is tight. Our theoretical analysis is backed up by numerical experiments.,Tight Sample Complexity of Learning

One-hidden-layer Convolutional Neural Networks

Yuan Cao

Quanquan Gu

Department of Computer Science

University of California  Los Angeles

Department of Computer Science

University of California  Los Angeles

CA 90095  USA

yuancao@cs.ucla.edu

CA 90095  USA

qgu@cs.ucla.edu

Abstract

We study the sample complexity of learning one-hidden-layer convolutional neural
networks (CNNs) with non-overlapping ﬁlters. We propose a novel algorithm
called approximate gradient descent for training CNNs  and show that  with high
probability  the proposed algorithm with random initialization grants a linear con-
vergence to the ground-truth parameters up to statistical precision. Compared with
existing work  our result applies to general non-trivial  monotonic and Lipschitz
continuous activation functions including ReLU  Leaky ReLU  Sigmod and Soft-
plus etc. Moreover  our sample complexity beats existing results in the dependency
of the number of hidden nodes and ﬁlter size. In fact  our result matches the
information-theoretic lower bound for learning one-hidden-layer CNNs with linear
activation functions  suggesting that our sample complexity is tight. Our theoretical
analysis is backed up by numerical experiments.

1

Introduction

Deep learning is one of the key research areas in modern artiﬁcial intelligence. Deep neural networks
have been successfully applied to various ﬁelds including image processing [25]  speech recognition
[20] and reinforcement learning [33]. Despite the remarkable success in a broad range of applications 
theoretical understandings of neural network models remain largely incomplete:
the high non-
convexity of neural networks makes convergence analysis of learning algorithms very difﬁcult;
numerous practically successful choices of the activation function  twists of the training process and
variants of the network structure make neural networks even more mysterious.
One of the fundamental problems in learning neural networks is parameter recovery  where we
assume the data are generated from a “teacher” network  and the task is to estimate the ground-
truth parameters of the teacher network based on the generated data. Recently  a line of research
[41  16  38] gives parameter recovery guarantees for gradient descent based on the analysis of local
convexity and smoothness properties of the square loss function. The results of Zhong et al. [41] and
Fu et al. [16] hold for various activation functions except ReLU activation function  while Zhang et al.
[38] prove the corresponding result for ReLU. Their results are for fully connected neural networks
and their analysis requires accurate knowledge of second-layer parameters. For instance  Fu et al.
[16] and Zhang et al. [38] directly assume that the second-layer parameters are known  while Zhong
et al. [41] reduce the second-layer parameters to be ±1’s with the homogeneity assumption  and
then exactly recovers them with a tensor initialization algorithm. Moreover  it may not be easy to
generalize the local convexity and smoothness argument to other algorithms that are not based on the
exact gradient of the loss function. Another line of research [6  13  18  10] focuses on convolutional
neural networks with ReLU activation functions. Brutzkus and Globerson [6]  Du et al. [13] provide
convergence analysis for gradient descent on parameters of both layers  while Goel et al. [18]  Du
and Goel [10] proposed new algorithms to learn single-hidden-layer CNNs. However  these results

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Comparison with related work [13  14  18  10]. Note that Du et al. [14] did not study any
speciﬁc learning algorithms. All sample complexity results are calculated for standard Gaussian
inputs and non-overlapping ﬁlters. The D. Convotron stands for Double Convotron  which is an
algorithm proposed by Du and Goel [10].

Du et al. [13]
Du et al. [14]
Convotron [18]

D. Convotron [10] sublinear (cid:101)O(poly(k  r  −1)) (leaky) ReLU symmetric

sub-Gaussian
(leaky) ReLU symmetric

(sub)linear1

-

This paper

linear

general

Gaussian

no
yes
yes
yes
no

yes
-
no
yes
yes

Conv. rate

linear

-

Sample comp.

(cid:101)O((k + r) · −2)
(cid:101)O(k2r · −2)
(cid:101)O((k + r) · −2)

Act. fun.
ReLU
Linear

Data input Overlap Sec. layer
Gaussian

heavily rely on the exact calculation of the population gradient for ReLU networks  and do not
provide tight sample complexity guarantees.
In this paper  we study the parameter recovery problem for non-overlapping convolutional neural
networks. We aim to develop a new convergence analysis framework for neural networks that: (i)
works for a class of general activation functions  (ii) does not rely on ad hoc initialization  (iii) can be
potentially applied to different variants of the gradient descent algorithm. The main contributions of
this paper is as follows:
• We propose an approximate gradient descent algorithm that learns the parameters of both layers in
a non-overlapping convolutional neural network. With weak requirements on initialization that can
be easily satisﬁed  the proposed algorithm converges to the ground-truth parameters linearly up to
statistical precision.

• Our convergence result holds for all non-trivial  monotonic and Lipschitz continuous activation
functions. Compared with the results in Brutzkus and Globerson [6]  Du et al. [13]  Goel et al.
[18]  Du and Goel [10]  our analysis does not rely on any analytic calculation related to the
activation function. We also do not require the activation function to be smooth  which is assumed
in the work of Zhong et al. [41] and Fu et al. [16].

• We consider the empirical version of the problem where the estimation of parameters is based on
n independent samples. We avoid the usual analysis with sample splitting by proving uniform
concentration results. Our method outperforms the state-of-the-art results in terms of sample
complexity. In fact  our result for general non-trivial  monotonic and Lipschitz continuous activation
functions matches the lower bound given for linear activation functions in Du et al. [14]  which
implies the statistical optimality of our algorithm.

Detailed comparison between our results and the state-of-the-art on learning one-hidden-layer CNNs is
given in Table 1. We compare the convergence rates and sample complexities obtained by recent work
with our result. We also summarize the applicable activation functions and data input distributions 
and whether overlapping/non-overlapping ﬁlters and second layer training are considered in each of
the work.
Notation: Let A = [Aij] ∈ Rd×d be a matrix and x = (x1  ...  xd)(cid:62) ∈ Rd be a vector. We use
i=1 |xi|q)1/q to denote (cid:96)q vector norm for 0 < q < +∞. The spectral and Frobenius
norms of A are denoted by (cid:107)A(cid:107)2 and (cid:107)A(cid:107)F . For a symmetric matrix A  we denote by λmax(A) 
λmin(A) and λi(A) the maximum  minimum and i-th largest eigenvalues of A. We denote by A (cid:23) 0
that A is positive semideﬁnite (PSD). Given two sequences {an} and {bn}  we write an = O(bn)
if there exists a constant 0 < C < +∞ such that an ≤ C bn  and an = Ω(bn) if an ≤ C bn for
a ∧ b = min{a  b}  a ∨ b = max{a  b}.

(cid:107)x(cid:107)q = ((cid:80)d
some constant C. We use notations (cid:101)O(·) (cid:101)Ω(·) to hide the logarithmic factors. Finally  we denote

1[18] provided a general sublinear convergence result as well as a linear convergence rate for the noiseless

case. We only list their sample complexity result of the noisy case in the table for proper comparison.

2

2 Related Work

There has been a vast body of literature on the theory of deep learning. We will review in this section
the most relevant work to ours.
It is well known that neural networks have remarkable expressive power due to the universal ap-
proximation theorem [22]. However  even learning a one-hidden-layer neural network with a sign
activation can be NP-hard [5] in the realizable case. In order to explain the success of deep learning
in various applications  additional assumptions on the data generating distribution have been explored
such as symmetric distributions [4] and log-concave distributions [24]. More recently  a line of
research has focused on Gaussian distributed input for one-hidden-layer or two-layer networks with
different structures [23  35  6  27  41  40  17  38  16]. Compared with these results  our work aims at
providing tighter sample complexity for more general activation functions.
A recent line of work [28  32  29  26  15  2  11  42  1  3  7] studies the training of neural networks in
the over-parameterized regime. Mei et al. [28]  Shamir [32]  Mei et al. [29] studied the optimization
landscape of over-parameterized neural networks. Li and Liang [26]  Du et al. [15]  Allen-Zhu
et al. [2]  Du et al. [11]  Zou et al. [42] proved that gradient descent can ﬁnd the global minima
of over-parameterized neural networks. Generalization bounds under the same setting are studied
in Allen-Zhu et al. [1]  Arora et al. [3]  Cao and Gu [7]. Compared with these results in the over-
parameterized setting  the parameter recovery problem studied in this paper is in the classical setting 
and therefore different approaches need to be taken for the theoretical analysis.
This paper studies convolutional neural networks (CNNs). There are not much theoretical literature
speciﬁcally for CNNs. The expressive power of CNNs is shown in Cohen and Shashua [8]. Nguyen
and Hein [30] study the loss landscape in CNNs and Brutzkus and Globerson [6] show the global
convergence of gradient descent on one-hidden-layer CNNs. Du et al. [12] extend the result to
non-Gaussian input distributions with ReLU activation. Zhang et al. [39] relax the class of CNN
ﬁlters to a reproducing kernel Hilbert space and prove the generalization error bound for the relaxation.
Gunasekar et al. [19] show that there is an implicit bias in gradient descent on training linear CNNs.

3 The One-hidden-layer Convolutional Neural Network

In this section we formalize the one-hidden-layer convolutional neural network model. In a convo-
lutional network with neuron number k and ﬁlter size r  a ﬁlter w ∈ Rr interacts with the input x
at k different locations I1  . . .  Ik  where I1  . . .  Ik ⊆ {1  2  . . .   d} are index sets of cardinality r.
Let Ij = {pj1  . . .   pjr}  j = 1  . . .   k  then the corresponding selection matrices P1  . . .   Pk are
deﬁned as Pj = (epj1  . . .   epjr )(cid:62)  j = 1  . . .   k.
We consider a convolutional neural network of the form

y =

vjσ(w(cid:62)Pjxi) 

k(cid:88)

j=1

k(cid:88)

where σ(·) is the activation function  and w ∈ Rr  v ∈ Rk are the ﬁrst and second layer parameters
respectively. Suppose that we have n samples {(xi  yi)}n
i=1  where x1  . . .   xn ∈ Rd are generated
independently from standard Gaussian distribution  and the corresponding output y1  . . .   yn ∈ R are
generated from the teacher network with true parameters w∗ and v∗ as follows.

yi =

v∗
j σ(w∗(cid:62)Pjxi) + i 

j=1

where k is the number of hidden neurons  and 1  . . .   n are independent sub-Gaussian white noises
with ψ2 norm ν. Through out this paper  we assume that (cid:107)w∗(cid:107)2 = 1.
The choice of activation function σ(·) determines the landscape of the neural network. In this paper 
we assume that σ(·) is a non-trivial  Lipschitz continuous increasing function.
Assumption 3.1. σ is 1-Lipschitz continuous: |σ(z1) − σ(z2)| ≤ |z1 − z2| for all z1  z2 ∈ R.
Assumption 3.2. σ is a non-trivial (not a constant) increasing function.
Remark 3.3. Assumptions 3.1 and 3.2 are fairly weak assumptions satisﬁed by most practically
used activation functions including the rectiﬁed linear unit (ReLU) function σ(z) = max(z  0)  the

3

and the erf function σ(z) =(cid:82) z

sigmoid function σ(z) = 1/(1 + ez)  the hyperbolic tangent function σ(z) = (ez − e−z)/(ez + e−z) 
0 e−t2/2dt. Since we do not make any assumptions on the second layer
true parameter v∗  our assumptions can be easily relaxed to any non-trivial  L-Lipschitz continuous
and monotonic functions for arbitrary ﬁxed positive constant L.

4 Approximate Gradient Descent
In this section we present a new algorithm for the estimation of w∗ and v∗.
4.1 Algorithm Description
Let y = (y1  . . .   yn)(cid:62)  Σ(w) = [σ(w(cid:62)Pjxi)]n×k and ξ = Ez∼N (0 1)[σ(z)z]. The algorithm
is given in Algorithm 1. We call it approximate gradient descent because it is derived by simply
replacing the σ(cid:48)(·) terms in the gradient of the empirical square loss function by the constant ξ−1. It
is easy to see that under Assumption 3.1 and 3.2  we have ξ > 0. Therefore  replacing σ(cid:48)(·) > 0 with
ξ−1 will not drastically change the gradient direction  and gives us an approximate gradient.
Algorithm 1 is also related to  but different from the Convotron algorithm proposed by Goel et al.
[18] and the Double Convotron algorithm proposed by Du and Goel [10]. The Approximate Gradi-
ent Descent algorithm can be seen a generalized version of the Convotron algorithm  which only
considers optimizing over the ﬁrst layer parameters of the convolutional neural network. Compared
to the Double Convotron  Algorithm 1 implements a simpler update rule based on iterative weight
normalization for the ﬁrst layer parameters  and uses a different update rule for the second layer
parameters.

Algorithm 1 Approximate Gradient Descent for Non-overlapping CNN
Require: Training data {(xi  yi)}n

i=1  number of iterations T   step size α  initialization w0 ∈ Sr−1 

v0.
for t = 0  1  2  . . .   T − 1 do

(cid:2)yi −(cid:80)k

(cid:80)n
j=1 vt
n Σ(cid:62)(wt)[y − Σ(wt)vt]

w = − 1
gt
v = − 1
gt
ut+1 = wt − αgt

i=1

n

j(cid:48)=1 ξ−1vt
w  wt+1 = ut+1/(cid:107)ut+1(cid:107)2  vt+1 = vt − αgt

v

jσ(wt(cid:62)Pjxi)(cid:3) ·(cid:80)k

j(cid:48)Pj(cid:48)xi

end for

Ensure: wT   vT

4.2 Convergence Analysis of Algorithm 1

In this section we give the main convergence result of Algorithm 1. We ﬁrst introduce some notations.
The following quantities are determined purely by the activation function:

κ := Ez∼N (0 1)[σ(z)]  ∆ := Varz∼N (0 1)[σ(z)]  L := 1 + |σ(0)|  Γ := 1 + |σ(0) − κ| 
φ(w  w(cid:48)) := Covz∼N (0 I)[σ(w(cid:62)z)  σ(w(cid:48)(cid:62)z)].

The following lemma shows that the function φ(w  w(cid:48)) can in fact be written as a function of w(cid:62)w(cid:48) 
which we denote as ψ(w(cid:62)w(cid:48)). The lemma also reveals that ψ(·) is an increasing function.
Lemma 4.1. Under Assumption 3.2  there exists an increasing function ψ(τ ) such that ψ(w(cid:62)w(cid:48)) =
φ(w  w(cid:48))  and ∆ ≥ ψ(τ ) > 0 for all τ > 0.
We further deﬁne the following quantities.
√

(cid:27)

k)

 |κ1(cid:62)(v0 − v∗)|

 

(cid:26)|κ|(2L|1(cid:62)v∗| +
(cid:115)
(cid:40)
(cid:26) 1

(cid:107)v0 − v∗(cid:107)2 

(cid:18) 1

∆ + κ2k

ψ

2 + ∆

2

(cid:19)

M = max

D = max

ρ = min

4(1 + 4α∆)L2(cid:107)v∗(cid:107)2

2 + 4α∆(κ2M 2k + 1) + 2

∆2(1 − 4α∆)

w∗(cid:62)w0

(cid:107)v∗(cid:107)2

2  v∗(cid:62)v0

(cid:27)

.

(cid:41)

 

4

Let D0 = D + (cid:107)v∗(cid:107)2. Note that in our problem setting the number of ﬁlters k can scale with n.
However  although k is used in the deﬁnition of M  D  ρ and D0  it is not difﬁcult to check that all
these quantities can be upper bounded  as is stated in the following lemma.
Lemma 4.2. If α ≤ 1/(8∆)  then M  D  D0 have upper bounds  and ρ has a lower bound  that only
depend on the activation function σ(·)  the ground-truth (w∗  v∗) and the initialization (w0  v0).
We now present our main result  which states that the iterates wt and vt in Algorithm 1 converge
linearly towards w∗ and v∗ respectively up to statistical accuracy.
Theorem 4.3. Let δ ∈ (0  1)  γ1 = (1 + αρ)−1/2  γ2 =
κ2k). Suppose that the initialization (w0  v0) satisﬁes

1 − α∆ + 4α2∆2  and γ3 = 1 − α(∆ +

√

w∗(cid:62)w0 > 0  v∗(cid:62)v0 > 0  κ2(1(cid:62)v∗)1(cid:62)(v0 − v∗) ≤ ρ 

(4.1)

and the step size α is chosen such that

1

2(∆ + κ2k)

∧ 1
8∆

∧

∆2
(24L2 + 2∆2)(cid:107)v∗(cid:107)2

α ≤

If

(cid:114)

c1

(r + k) log(c2nk/δ)

n

√
≤ 1 ∧ ρ

2 + 2M 2k + 10

|1(cid:62)v∗| ∧ Γ(cid:112)k(r + k)
(cid:18)

1 + L + ξ
1 ∧

k

ξ

D0(D0Γ + M + ν)

√
(Γ + κ

1

k)(D0Γ + M + ν)

∧

∧

.

2 + (cid:107)v∗(cid:107)2
2)
√
k)Γ

r + k
L2 + |κ| + ∆ + L

1
2((cid:107)v0 − v∗(cid:107)2

∧
∧ (Γ + |κ|√
(cid:18)
w∗(cid:62)w0

1 + αρ
1 ∧

ρ

ρ

(cid:107)v∗(cid:107)2

(cid:19)
(cid:19)

for some large enough absolute constants c1 and c2  then there exists absolute constants C and C(cid:48)
such that  with probability at least 1 − δ we have

(cid:107)wt − w∗(cid:107)2 ≤ γt
(cid:107)vt − v∗(cid:107)2 ≤ R1t3/2(γ1 ∨ γ2 ∨ γ3)t + (R2 + R3|κ|

1(cid:107)w0 − w∗(cid:107)2 + 8ρ−1γ−2

1 ηw 

√

k)(ηw + ηv) 

(4.2)

(4.3)
(4.4)

(4.5)

for all t = 0  . . .   T   where

ηw = Cξ−1D0(D0Γ + M + ν) ·

(cid:114)

(r + k) log(120nk/δ)

 

n

(cid:114)

√
ηv = C(cid:48)(Γ + κ

(r + k) log(120nk/δ)

k)(D0Γ + M + ν) ·

 

n

√

√
k)

be interpreted as an assumption that n ≥ (cid:101)Ω((1 + κ

(4.6)
and R1  R2  R3 are constants that only depend on the choice of activation function σ(·)  the ground-
truth parameters (w∗  v∗) and the initialization (w0  v0).
Equation (4.2) is an assumption on the sample size n. Although this assumption looks complicated 
essentially except k and r  all quantities in this condition can be treated as constants  and (4.2) can
r + k)  which is by no means a strong
assumption. The second and third lines of (4.2) are to guarantee ηw ≤ 1 ∧ [ρ/(1 + αρ)w∗(cid:62)w0] and
ηv ≤ 1 ∧ (ρ/(cid:107)v∗(cid:107)2) respectively  while the ﬁrst line is for technical purposes to ensure convergence.
Remark 4.4. Theorem 4.3 shows that with initialization satisfying (4.1)  Algorithm 1 linearly
converges to true parameters up to statistical error. This condition for initialization can be easily
satisﬁed with random initialization. In Section 4.3  we will give a detailed initialization algorithm
inspired by a random initialization method proposed in Du et al. [13].
Remark 4.5. Compared with the most relevant convergence results in literature given by Du et al.
[13]  our result is based on optimizing the empirical loss function instead of the population loss
function. In particular  when κ = 0  Theorem 4.3 proves that Algorithm 1 eventually gives es-
timation of parameters with statistical error of order O
. This rate matches
the information-theoretic lower bound for one-hidden-layer convolutional neural networks with
linear activation functions. Note that our result holds for general activation functions. Matching the

(cid:16)(cid:113) (r+k) log(120nk/δ)

(cid:17)

n

5

lower bound of the linear case implies the optimality of our algorithm. Compared with two recent
results  namely the Convotron algorithm proposed by Goel et al. [18] and the Double Convotron
algorithm proposed by Du and Goel [10]  which work for ReLU activation and generic symmetric
input distributions  our theoretical guarantee for Algorithm 1 gives a tighter sample complexity for
more general activation functions  but requires the data inputs to be Gaussian. We remark that if
restricting to ReLU activation function  our analysis can be extended to generic symmetric input
distributions as well  and can still provide tight sample complexity.
Remark 4.6. A recent result by Du et al. [13] discussed a speed-up in convergence when training
non-overlapping CNNs with ReLU activation function. This phenomenon also exists in Algorithm 1.
To show it  we ﬁrst note that in Theorem 4.3  the convergence rate of wt and vt are essentially
determined by γ1 = (1 + αρ)−1/2. For appropriately chosen α  ρ being too small (i.e. w∗(cid:62)w0 or
v∗(cid:62)v0 being too small  by Lemma 4.1) is the only possible reason of slow convergence. Now by
the iterative nature of Algorithm 1  for any T1 > 0  we can analyze the convergence behavior after
T1 by treating wT1 and vT1 as new initialization and applying Theorem 4.3 again. By Theorem 4.3 
even if the initialization gives small w∗(cid:62)w0 and v∗(cid:62)v0  after certain number of iterations  w∗(cid:62)wt
and v∗(cid:62)vt become much larger and therefore the convergence afterwards gets much faster. This
phenomenon is comparable with the two-phase convergence result of Du et al. [13]. However  while
Du et al. [13] only show the convergence of second layer parameters for phase II of their algorithm 
our result shows that linear convergence of Algorithm 1 starts at the ﬁrst iteration.

4.3

Initialization

To complete the theoretical analysis of Algorithm 1  it remains to show that initialization condition
(4.1) can be achieved with practical algorithms. The following theorem is inspired by a similar
method proposed by Du et al. [13]. It gives a simple random initialization method that satisfy (4.1).
Theorem 4.7. Suppose that w ∈ Rr and v ∈ Rk be vectors generated by Pw and
Pv with support Sr−1 and B(0  k−1/2|1(cid:62)v∗|) respectively. Then there exists (w0  v0) ∈
{(w  v)  (−w  v)  (w −v)  (−w −v)} that satisﬁes (4.1).
Remark 4.8. The proof of Theorem 4.7 is fairly straightforward–the vector v generated by pro-
posed initialization method in fact satisﬁes that κ2(1(cid:62)v∗)1(cid:62)(v − v∗) ≤ 0. Moreover  it is worth
noting that for activation functions with κ = Ez∼N (0 1)[σ(z)] = 0  the initialization condition
κ2(1(cid:62)v∗)1(cid:62)(v0 − v∗) ≤ ρ is automatically satisﬁed. Therefore for any vector w ∈ Sr−1 and
v ∈ Rk  one of (w  v)  (−w  v)  (w −v)  (−w −v) satisﬁes the initialization condition  making
initialization for Algorithm 1 extremely easy.

5 Proof of the Main Theory

In this section we give the proof of Theorem 4.3. The proof consists of three steps: (i) prove uniform
concentration inequalities for approximate gradients  (ii) give recursive upper bounds for (cid:107)wt− w∗(cid:107)2
and (cid:107)vt − v∗(cid:107)2  (iii) derive the ﬁnal convergence result (4.3) and (4.4).
We ﬁrst analyze how well the approximate gradients gt
Instead of using the classic analysis on gt
we consider uniform concentration over a parameter set W0 × V0 deﬁned as follows.

v conditioning on all previous iterations {ws  vs}t

v concentrate around their expectations.
s=1 

w and gt

w and gt

W0 := Sr−1 = {w : (cid:107)w(cid:107)2 = 1}  V0 := {v : (cid:107)v − v∗(cid:107)2 ≤ D  |κ1(cid:62)(v − v∗)| ≤ M}.

Deﬁne

(cid:35)

· k(cid:88)

ξ−1vj(cid:48)Pj(cid:48)xi 

(cid:34)

yi − k(cid:88)

n(cid:88)

i=1

vjσ(w(cid:62)Pjxi)

gw(w  v) = − 1
n
gv(w  v) = − 1
n
gw(w  v) = (cid:107)v(cid:107)2
gv(w  v) = (∆I + κ211(cid:62))v − [φ(w  w∗)I + κ211(cid:62)]v∗.

Σ(cid:62)(w)[y − Σ(w)v] 
2w − (v∗(cid:62)v)w∗ 

j(cid:48)=1

j=1

The following claim follows by direct calculation.

6

Claim 5.1. For any ﬁxed w  v  it holds that E[gw(w  v)] = gw(w  v) and E[gv(w  v)] = gv(w  v) 
where the expectation is taken over the randomness of data.
Our goal is to bound sup(w v)∈W0×V0 (cid:107)gw(w  v) − gw(w  v)(cid:107)2 and sup(w v)∈W0×V0 (cid:107)gv(w  v) −
gv(w  v)(cid:107)2. A key step for proving such uniform bounds is to show thee uniform Lipschitz continuity
of gw(w  v) and gv(w  v)  which is given in the following lemma.
Lemma 5.2. For any δ > 0  if n ≥ (r + k) log(324/δ)  then with probability at least 1 − δ  the
following inequalities hold uniformly over all w  w(cid:48) ∈ W0 and v  v(cid:48) ∈ V0:
(cid:107)gw(w  v) − gw(w(cid:48)  v)(cid:107)2 ≤ Cξ−1D2
k · (cid:107)w − w(cid:48)(cid:107)2 
√
k) · (cid:107)v − v(cid:48)(cid:107)2 
(cid:107)gw(w  v) − gw(w  v(cid:48))(cid:107)2 ≤ Cξ−1(ν + D0L
√
√
(cid:107)gv(w  v) − gv(w(cid:48)  v)(cid:107)2 ≤ C(ν + D0L
k · (cid:107)w − w(cid:48)(cid:107)2 
k)
(cid:107)gv(w  v) − gv(w  v(cid:48))(cid:107)2 ≤ CL2k · (cid:107)v − v(cid:48)(cid:107)2 

(5.1)
(5.2)
(5.3)
(5.4)

√

0

where C is an absolute constant.

If gw and gv are gradients of some objective function f  then Lemma 5.2 essentially proves the
uniform smoothness of f. However  in our algorithm  gw is not the exact gradient  and therefore the
results are stated in the form of Lipschitz continuity of gw. Lemma 5.2 enables us to use a covering
number argument together with point-wise concentration inequalities to prove uniform concentration 
which is given as Lemma 5.3.
Lemma 5.3. Assume that n ≥ (r + k) log(972/δ)  and

ξ−1D0(D0Γ + M + ν)(cid:112)n(r + k) log(90nk/δ) ≥ D0(D0 + 1) ∨ ξ−1(ν + D2

k)[D0Γ + M + ν](cid:112)n(r + k) log(90nk/δ) ≥ (∆ + κ + D0L) ∨ (ν + D0L + L2).

√
(Γ + κ

0 + D0L) 

Then with probability at least 1 − δ we have

sup

(w v)∈W0×V0

sup

(w v)∈W0×V0

(cid:107)gw(w  v) − gw(w  v)(cid:107)2 ≤ ηw 
(cid:107)gv(w  v) − gv(w  v)(cid:107)2 ≤ ηv 

(5.5)

(5.6)

where ηw and ηv are deﬁned in (4.5) and (4.6) respectively with large enough constants C and C(cid:48).
We now proceed to study the recursive properties of {wt} and {vt}. Deﬁne

W := {w : (cid:107)w(cid:107)2 = 1  w∗(cid:62)w ≥ w∗(cid:62)w0/2} 
V := {v : (cid:107)v − v∗(cid:107)2 ≤ D  |κ1(cid:62)(v − v∗)| ≤ M  v∗(cid:62)v ≥ ρ  κ2(1(cid:62)v∗)1(cid:62)(v − v∗) ≤ ρ}.

Then clearly W × V ⊆ W0 × V0  and therefore the results of Lemma 5.3 hold for (w  v) ∈ W × V.
Lemma 5.4. Suppose that (5.5) and (5.6) hold. Under the assumptions of Theorem 4.3  in Algo-
rithm 1  if (wt  vt) ∈ W × V  then
[(cid:107)wt − w∗(cid:107)2 − 8ρ−1(1 + αρ)ηw] 
(cid:107)wt+1 − w∗(cid:107)2 − 8ρ−1(1 + αρ)ηw ≤
|1(cid:62)(vt+1 − v∗)| ≤ [1 − α(∆ + κ2k)]|1(cid:62)(vt − v∗)| + αL(cid:107)wt − w∗(cid:107)2|1(cid:62)v∗| + α
(cid:107)vt+1 − v∗(cid:107)2

2 ≤ (1 − α∆ + 4α2∆2)(cid:107)vt − v∗(cid:107)2

2(cid:107)wt − w∗(cid:107)2

(cid:18) αL2

1√
1 + αρ

kηv  (5.8)

(cid:107)v∗(cid:107)2

+ 4α2L2

(cid:19)

(5.7)

√

2

+ 4α2κ4k[1(cid:62)(vt − v∗)]2 +

(wt+1  vt+1) ∈ W × V.

2 +

(cid:18) 2α

∆

(cid:19)

∆

+ 4α2

η2
v 

(5.9)

(5.10)

It is not difﬁcult to see that the results of Lemma 5.4 imply convergence of wt and vt up to statistical
error. To obtain the ﬁnal convergence result of Theorem 4.3  it sufﬁces to rewrite the recursive bounds
into explicit bounds  which is mainly tedious calculation. We therefore summarize the result as the
following lemma  and defer the detailed calculation to appendix.

7

Lemma 5.5. Suppose that (5.7)  (5.8) and (5.9) hold for all t = 0  . . .   T . Then

(cid:107)wt − w∗(cid:107)2 ≤ γt
(cid:107)vt − v∗(cid:107)2 ≤ R1t3(γ1 ∨ γ2 ∨ γ3)t + (R2 + R3|κ|

1(cid:107)w0 − w∗(cid:107)2 + 8ρ−1γ−2

1 ηw 

√

k)(ηw + ηv)

√

1 − α∆ + 4α2∆2  and γ3 = 1−α(∆+κ2k) 
for all t = 0  . . .   T   where γ1 = (1+αρ)−1/2  γ2 =
and R1  R2  R3 are constants that only depend on the choice of activation function σ(·)  the ground-
truth parameters (w∗  v∗) and the initialization (w0  v0).

We are now ready to present the ﬁnal proof of Theorem 4.3  which is a straightforward combination
of the results of Lemma 5.4 and Lemma 5.5.
Proof of Theorem 4.3. By Lemma 5.4  as long as (w0  v0) ∈ W × V  (5.7)  (5.8) and (5.9) hold for
all t = 0  . . .   T . Therefore by Lemma 5.5  we have

(cid:107)wt − w∗(cid:107)2 ≤ γt
(cid:107)vt − v∗(cid:107)2 ≤ R1t3(γ1 ∨ γ2 ∨ γ3)t + (R2 + R3|κ|

1(cid:107)w0 − w∗(cid:107)2 + 8ρ−1γ−2

1 ηw 

√

k)(ηw + ηv)

for all t = 0  . . .   T . This completes the proof of Theorem 4.3.

6 Experiments

We perform numerical experiments to backup our theoretical analysis. We test Algorithm 1 together
with the initialization method given in Theorem 4.7 for ReLU  sigmoid and hyperbolic tangent
networks  and compare its performance with the Double Convotron algorithm proposed by Du and
Goel [10]. To give a reasonable comparison  we use a batch version of Double Convotron without the
additional noises on unit sphere  which gives the best performance for Double Convotron  and makes
it directly comparable with our algorithm. The detailed parameter choices are given as follows:
• For all experiments  we set the number of iterations T = 100  sample size n = 1000.
• We tune the step size α to maximize performance. Speciﬁcally  we set α = 0.04 for ReLU 
α = 0.25 for sigmoid  and α = 0.1 for hyperbolic tangent networks. Note that for sigmoid and
hyperbolic tangent networks  an inappropriate step size can easily lead to blown up errors for
Double Convotron.

• We uniformly generate w∗ from unit sphere  and generate v∗ as a standard Gaussian vector.

• We consider two settings: (i) k = 15  r = 5 (cid:101)ν = 0.08  (ii) k = 30  r = 9 (cid:101)ν = 0.04  where(cid:101)ν is

the standard deviation of white Gaussian noises.

The random initialization is performed as follows: we generate w uniformly over the unit sphere.
We then generate a standard Gaussian vector v. If (cid:107)v(cid:107)2 ≥ k−1/2|1(cid:62)v∗|/2  then v is projected onto
the ball B(0  k−1/2|1(cid:62)v∗|/2). We then run the approximate gradient descent algorithm and Double
Convotron algorithm starting with each of (w  v)  (−w  v)  (w −v)  (−w −v)  and present the
results corresponding to the starting point that gives the smallest (cid:107)wT − w∗(cid:107)2.
Figure 1 gives the experimental results in semi-log plots. We summarize the results as follows.

1. For all the six cases  the approximate gradient descent algorithm eventually reaches a stable state

of linear convergence  until reaching very small error.

2. For ReLU networks  both algorithms converges. The convergence of approximated gradient
descent algorithm is slower compared with Double Convotron  but it eventually reaches smaller
statistical error  indicating a better sample complexity.

3. For sigmoid and hyperbolic tangent networks  not surprisingly  Double Convotron does not

converge. In contrast  approximated gradient descent still converges in a linear rate.

The experimental results discussed above clearly demonstrates the validity of our theoretical analysis.
In Appendix F  we also present some additional experiments on non-Gaussian inputs  and demonstrate
that although this setting is not the focus of our theoretical results  approximate gradient descent still
has promising performance on symmetric data distributions.

8

(a) ReLU

(b) Sigmod

(c) Hyperbolic tangent

(d) ReLU

(e) Sigmod

(f) Hyperbolic tangent

Figure 1: Numerical simulation for Algorithm 1 and the Double Convotron algorithm proposed by
Du and Goel [10] with different activation functions  number of hidden nodes and ﬁlter sizes. The
results for the case k = 15  r = 5 and k = 30  r = 9 are shown in (a)-(c) and (d)-(f) respectively. (a) 
(d) show the results for ReLU networks; (b)  (e) give the results for sigmoid networks; and ﬁnally the
results for hyperbolic tangent activation function are in (c) and (f). All plots are semi-log plots.

7 Conclusions and Future Work

We propose a new algorithm namely approximate gradient descent for training CNNs  and show that 
with high probability  the proposed algorithm with random initialization can recover the ground-truth
parameters up to statistical precision at a linear convergence rate . Compared with previous results  our
result applies to a class of monotonic and Lipschitz continuous activation functions including ReLU 
Leaky ReLU  Sigmod and Softplus etc. Moreover  our algorithm achieves better sample complexity
in the dependency of the number of hidden nodes and ﬁlter size. In particular  our result matches
the information-theoretic lower bound for learning one-hidden-layer CNNs with linear activation
functions  suggesting that our sample complexity is tight. Numerical experiments on synthetic data
corroborate our theory. Our algorithms and theory can be extended to learn one-hidden-layer CNNs
with overlapping ﬁlters. We leave it as a future work. It is also of great importance to extend the
current result to deeper CNNs with multiple convolution ﬁlters.

Acknowledgement

We thank the anonymous reviewers and area chair for their helpful comments. This research was
sponsored in part by the National Science Foundation CAREER Award IIS-1906169  IIS-1903202 
and Salesforce Deep Learning Research Award. The views and conclusions contained in this paper
are those of the authors and should not be interpreted as representing any funding agencies.

References

[1] ALLEN-ZHU  Z.  LI  Y. and LIANG  Y. (2018). Learning and generalization in overparameter-

ized neural networks  going beyond two layers. arXiv preprint arXiv:1811.04918 .

[2] ALLEN-ZHU  Z.  LI  Y. and SONG  Z. (2018). A convergence theory for deep learning via

over-parameterization. arXiv preprint arXiv:1811.03962 .

[3] ARORA  S.  DU  S. S.  HU  W.  LI  Z. and WANG  R. (2019). Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584 .

9

02040608010010−310−210−1100101epochl2 error ApproxGD: || w − w* ||2ApproxGD: || v − v* ||2DoubleConvotron: || w − w* ||2DoubleConvotron: || v − v* ||202040608010010−210−1100101epochl2 error ApproxGD: || w − w* ||2ApproxGD: || v − v* ||2DoubleConvotron: || w − w* ||2DoubleConvotron: || v − v* ||202040608010010−310−210−1100101102epochl2 error ApproxGD: || w − w* ||2ApproxGD: || v − v* ||2DoubleConvotron: || w − w* ||2DoubleConvotron: || v − v* ||202040608010010−410−310−210−1100101epochl2 error ApproxGD: || w − w* ||2ApproxGD: || v − v* ||2DoubleConvotron: || w − w* ||2DoubleConvotron: || v − v* ||202040608010010−210−1100101102epochl2 error ApproxGD: || w − w* ||2ApproxGD: || v − v* ||2DoubleConvotron: || w − w* ||2DoubleConvotron: || v − v* ||202040608010010−310−210−1100101102epochl2 error ApproxGD: || w − w* ||2ApproxGD: || v − v* ||2DoubleConvotron: || w − w* ||2DoubleConvotron: || v − v* ||2[4] BAUM  E. B. (1990). A polynomial time algorithm that learns two hidden unit nets. Neural

Computation 2 510–522.

[5] BLUM  A. and RIVEST  R. L. (1989). Training a 3-node neural network is np-complete. In

Advances in neural information processing systems.

[6] BRUTZKUS  A. and GLOBERSON  A. (2017). Globally optimal gradient descent for a convnet

with gaussian inputs. arXiv preprint arXiv:1702.07966 .

[7] CAO  Y. and GU  Q. (2019). A generalization theory of gradient descent for learning over-

parameterized deep relu networks. arXiv preprint arXiv:1902.01384 .

[8] COHEN  N. and SHASHUA  A. (2016). Convolutional rectiﬁer networks as generalized tensor

decompositions. In International Conference on Machine Learning.

[9] CUADRAS  C. M. (2002). On the covariance between functions. Journal of Multivariate

Analysis 81 19–27.

[10] DU  S. S. and GOEL  S. (2018). Improved learning of one-hidden-layer convolutional neural

networks with overlaps. arXiv preprint arXiv:1805.07798 .

[11] DU  S. S.  LEE  J. D.  LI  H.  WANG  L. and ZHAI  X. (2018). Gradient descent ﬁnds global

minima of deep neural networks. arXiv preprint arXiv:1811.03804 .

[12] DU  S. S.  LEE  J. D. and TIAN  Y. (2017). When is a convolutional ﬁlter easy to learn? arXiv

preprint arXiv:1709.06129 .

[13] DU  S. S.  LEE  J. D.  TIAN  Y.  POCZOS  B. and SINGH  A. (2017). Gradient descent
learns one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint
arXiv:1712.00779 .

[14] DU  S. S.  WANG  Y.  ZHAI  X.  BALAKRISHNAN  S.  SALAKHUTDINOV  R. and SINGH  A.
(2018). How many samples are needed to learn a convolutional neural network? arXiv preprint
arXiv:1805.07883 .

[15] DU  S. S.  ZHAI  X.  POCZOS  B. and SINGH  A. (2018). Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054 .

[16] FU  H.  CHI  Y. and LIANG  Y. (2018). Local geometry of one-hidden-layer neural networks

for logistic regression. arXiv preprint arXiv:1802.06463 .

[17] GE  R.  LEE  J. D. and MA  T. (2017). Learning one-hidden-layer neural networks with

landscape design. arXiv preprint arXiv:1711.00501 .

[18] GOEL  S.  KLIVANS  A. and MEKA  R. (2018). Learning one convolutional layer with

overlapping patches. arXiv preprint arXiv:1802.02547 .

[19] GUNASEKAR  S.  LEE  J.  SOUDRY  D. and SREBRO  N. (2018). Implicit bias of gradient

descent on linear convolutional networks. arXiv preprint arXiv:1806.00468 .

[20] HINTON  G.  DENG  L.  YU  D.  DAHL  G. E.  MOHAMED  A.-R.  JAITLY  N.  SENIOR 
A.  VANHOUCKE  V.  NGUYEN  P.  SAINATH  T. N. ET AL. (2012). Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal Processing Magazine 29 82–97.

[21] HOEFFDING  W. (1940). Masstabinvariante korrelationtheorie  schriften des mathematis
chen instituts und des instituts für angewandte mathematik der universität berlin 5  181#
233.(translated in ﬁsher  ni and pk sen (1994). the collected works of wassily hoeffding  new
york.

[22] HORNIK  K. (1991). Approximation capabilities of multilayer feedforward networks. Neural

networks 4 251–257.

10

[23] JANZAMIN  M.  SEDGHI  H. and ANANDKUMAR  A. (2015). Beating the perils of non-
convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint
arXiv:1506.08473 .

[24] KLIVANS  A. R.  LONG  P. M. and TANG  A. K. (2009). Baum’s algorithm learns intersections
of halfspaces with respect to log-concave distributions. In Approximation  Randomization  and
Combinatorial Optimization. Algorithms and Techniques. Springer  588–600.

[25] KRIZHEVSKY  A.  SUTSKEVER  I. and HINTON  G. E. (2012). Imagenet classiﬁcation with

deep convolutional neural networks. In Advances in neural information processing systems.

[26] LI  Y. and LIANG  Y. (2018). Learning overparameterized neural networks via stochastic

gradient descent on structured data. arXiv preprint arXiv:1808.01204 .

[27] LI  Y. and YUAN  Y. (2017). Convergence analysis of two-layer neural networks with relu

activation. arXiv preprint arXiv:1705.09886 .

[28] MEI  S.  BAI  Y. and MONTANARI  A. (2016). The landscape of empirical risk for non-convex

losses. arXiv preprint arXiv:1607.06534 .

[29] MEI  S.  MONTANARI  A. and NGUYEN  P.-M. (2018). A mean ﬁeld view of the landscape of

two-layers neural networks. arXiv preprint arXiv:1804.06561 .

[30] NGUYEN  Q. and HEIN  M. (2017). The loss surface and expressivity of deep convolutional

neural networks. arXiv preprint arXiv:1710.10928 .

[31] SEN  P. K. (1994). The impact of wassily hoeffding’s research on nonparametrics. In The

Collected Works of Wassily Hoeffding. Springer  29–55.

[32] SHAMIR  O. (2016). Distribution-speciﬁc hardness of learning neural networks. arXiv preprint

arXiv:1609.01037 .

[33] SILVER  D.  HUANG  A.  MADDISON  C. J.  GUEZ  A.  SIFRE  L.  VAN DEN DRIESSCHE 
G.  SCHRITTWIESER  J.  ANTONOGLOU  I.  PANNEERSHELVAM  V.  LANCTOT  M. ET AL.
(2016). Mastering the game of go with deep neural networks and tree search. Nature 529
484–489.

[34] SLEPIAN  D. (1962). The one-sided barrier problem for gaussian noise. Bell Labs Technical

Journal 41 463–501.

[35] TIAN  Y. (2016). Symmetry-breaking convergence analysis of certain two-layered neural

networks with relu nonlinearity .

[36] VERSHYNIN  R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027 .

[37] YI  X. and CARAMANIS  C. (2015). Regularized em algorithms: A uniﬁed framework and

statistical guarantees. In Advances in Neural Information Processing Systems.

[38] ZHANG  X.  YU  Y.  WANG  L. and GU  Q. (2018). Learning one-hidden-layer relu networks

via gradient descent. arXiv preprint arXiv:1806.07808 .

[39] ZHANG  Y.  LIANG  P. and WAINWRIGHT  M. J. (2016). Convexiﬁed convolutional neural

networks. arXiv preprint arXiv:1609.01000 .

[40] ZHONG  K.  SONG  Z. and DHILLON  I. S. (2017). Learning non-overlapping convolutional

neural networks with multiple kernels. arXiv preprint arXiv:1711.03440 .

[41] ZHONG  K.  SONG  Z.  JAIN  P.  BARTLETT  P. L. and DHILLON  I. S. (2017). Recovery

guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175 .

[42] ZOU  D.  CAO  Y.  ZHOU  D. and GU  Q. (2018). Stochastic gradient descent optimizes

over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888 .

11

,Yuan Cao
Quanquan Gu