2016,Statistical Inference for Pairwise Graphical Models Using Score Matching,Probabilistic graphical models have been widely used to model complex systems and aid scientific discoveries. As a result  there is a large body of literature focused on consistent model selection. However  scientists are often interested in understanding uncertainty associated with the estimated parameters  which current literature has not addressed thoroughly. In this paper  we propose a novel estimator for edge parameters for pairwise graphical models based on Hyv\"arinen scoring rule. Hyv\"arinen scoring rule is especially useful in cases where the normalizing constant cannot be  obtained efficiently in a closed form. We prove that the estimator is $\sqrt{n}$-consistent and asymptotically Normal. This result allows us to construct confidence intervals for edge parameters  as well as  hypothesis tests.  We establish our results under conditions that are typically assumed in the literature for consistent estimation. However  we do not require that the estimator consistently recovers the graph structure. In particular  we prove that the asymptotic distribution of the estimator is robust to model selection mistakes and uniformly valid for a large number of data-generating processes. We illustrate validity of our estimator through extensive simulation studies.,Statistical Inference for Pairwise Graphical Models

Using Score Matching

Ming Yu

mingyu@chicagobooth.edu

Varun Gupta

varun.gupta@chicagobooth.edu

mladen.kolar@chicagobooth.edu

University of Chicago Booth School of Business

Mladen Kolar⇤

Chicago  IL 60637

Abstract

Probabilistic graphical models have been widely used to model complex systems
and aid scientiﬁc discoveries. As a result  there is a large body of literature
focused on consistent model selection. However  scientists are often interested in
understanding uncertainty associated with the estimated parameters  which current
literature has not addressed thoroughly. In this paper  we propose a novel estimator
for edge parameters for pairwise graphical models based on Hyvärinen scoring
rule. Hyvärinen scoring rule is especially useful in cases where the normalizing
constant cannot be obtained efﬁciently in a closed form. We prove that the estimator
is pn-consistent and asymptotically Normal. This result allows us to construct
conﬁdence intervals for edge parameters  as well as  hypothesis tests. We establish
our results under conditions that are typically assumed in the literature for consistent
estimation. However  we do not require that the estimator consistently recovers
the graph structure. In particular  we prove that the asymptotic distribution of the
estimator is robust to model selection mistakes and uniformly valid for a large
number of data-generating processes. We illustrate validity of our estimator through
extensive simulation studies.

1

Introduction

Undirected probabilistic graphical models are widely used to explore and represent dependencies
between random variables. They have been used in areas ranging from computational biology to
neuroscience and ﬁnance. See [7] for a recent review. An undirected probabilistic graphical model
consists of an undirected graph G = (V  E)  where V = {1  . . .   p} is the vertex set and E ⇢ V ⇥ V
is the edge set  and a random vector X = (X1  . . .   Xp) 2X p ✓ RP . Each coordinate of the random
vector X is associated with a vertex in V and the graph structure encodes the conditional independence
assumptions underlying the distribution of X. In particular  Xa and Xb are conditionally independent
given all the other variables if and only if (a  b) 62 E  that is  the nodes a and b are not adjacent in G.
One of the fundamental problems in statistics is that of learning the structure of G from i.i.d. samples
from X and quantifying uncertainty of the estimated structure.

⇤This work is supported by an IBM Corporation Faculty Research Fund at the University of Chicago Booth
School of Business. This work was completed in part with resources provided by the University of Chicago
Research Computing Center.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

✓(l)
ab t(l)

ha(xa) 

x 2X✓ Rp.

a   t(l)

✓(k)
a t(k)

a (xa)+ X(a b)2E Xl2[L]

We consider a basic class of pairwise interaction graphical models with densities belonging to an
exponential family P = {p✓(x) | ✓ 2 ⇥} with natural parameter space ⇥ and
log p✓(x) =Xa2V Xk2[K]
ab (xa  xb) (✓)+Xa2V
(1)
The functions t(k)
ab are sufﬁcient statistics and (✓) is the log-partition function. In this paper
the support of the densities is either X = RP or X = RP
+ and P is dominated by Lebesgue
measure on Rp. To simplify the notation  we will write log p✓(x) = ✓Tt(x)  (✓) + h(x) where
✓ 2 Rs and t(x) : Rp 7! Rs with s = p
2 · L + p · K. The natural parameter space has the
form ⇥= {✓ 2 Rs | (x) = logRX
exp(✓Tt(x)dx) < 1}. Under the model in (1)  there
is no edge between a and b in the corresponding conditional independence graph if and only if
✓(1)
ab = ··· = ✓(L)
ab = 0. The model in (1) encompasses a large number of graphical models studied in
the literature (see  for example  [7  15] and referenced there in).
The main focus of the paper is on construction of an asymptotically normal estimator for parameters
in (1) and performing (asymptotic) inference for them. We illustrate a procedure for construction of
valid conﬁdence intervals that have the nominal coverage and propose a statistical test for existence
of edges in the graphical model with nominal size. Our inference results are robust to model selection
mistakes  which commonly occur in ultra-high dimensional setting. Results in the paper complement
existing literature  which is focused on consistent model selection and parameter recovery  as we
review in the next section.
We use Hyvärinen scoring rule to estimate ✓  as in [15]. However  rather than focusing on consistent
model selection we modify the regularized score matching procedure to construct a regular estimator
that is robust to model selection mistakes and show how to use its asymptotic distribution for
statistical inference. Compared to previous work on high-dimensional inference in graphical models
[23  2  29  11]  this is the ﬁrst work on inference in models where computing the normalizing constant
is intractable.
Related work. Our work straddles two areas of statistical learning which have attracted signiﬁcant
research of late: model selection and estimation in high-dimensional graphical models  and high-
dimensional inference. Our approach to inference for high-dimensional graphical models is based on
regularized score matching. We brieﬂy review the literature most relevant to our work  and refer the
reader to a recent review article for a comprehensive overview [7].
Graphical model selection: Much of the research effort on graphical model selection has been done
under the assumption that the data obeys the law X ⇠ N (0  ⌃) (Gaussian graphical models)  in
which case the edge set E of the graph G is encoded by the non-zero elements of the precision matrix
⌦=⌃ 1. More recently  [31] studied estimation of graphical models under the assumption that the
node conditional distributions belong to an exponential family distribution (including  for example 
Bernoulli  Gaussian  Poisson and exponential) via regularized likelihood (see also [13  6  30] and
references therein). In our paper  we construct a novel pn-consistent estimator of a parameter
corresponding to a particular edge in (1). As we mentioned earlier  this is the ﬁrst procedure that can
obtain a parametric rate of convergence for an edge parameter in a graphical model where computing
the normalizing constant is intractable.
High-dimensional inference: Methods for construction of conﬁdence intervals for low dimensional
parameters in high-dimensional linear and generalized linear models  as well as hypothesis tests 
have been developed in [32  4  28  12]. These methods construct honest  uniformly valid conﬁdence
intervals and hypothesis test based on a ﬁrst stage `1 penalized estimator. [16  23  5] construct
pn-consistent estimators for elements of the precision matrix ⌦ under a Gaussian assumption.
We contribute to the literature on high dimensional inference by demonstrating how to construct
estimators that are robust and uniformly valid under more general distributional assumptions than
Gaussian.
Score Matching estimators: Score matching estimators were ﬁrst proposed in [9  10]. Score
matching offers a computational advantage when the normalization constant is not available in
closed-form making likelihood based approaches intractable. Despite its power  there have not been
any results on inference in high-dimensional models using score matching. In [8]  the authors use
score matching for inference of Gaussian linear models (and hence for Gaussian graphical models) in
low-dimensional setting. In [15]  the authors use `1 regularized score matching to develop consistent

2

estimators for graphical models in high-dimensional setting. We present the ﬁrst high-dimensional
inference results using score matching.

Q2P

En [S(xi  Q)] .

2 Score Matching
Let X be a random variable with values in X   and let P be a family of distributions over X . A
scoring rule S(x  Q) is a real valued function that quantiﬁes accuracy of Q 2P upon observing a
realized value of X  x 2X . There are a large number of scoring rules that correspond to different
decision problems [20]. Given n independent realizations of X  {xi}i2[n]  one ﬁnds optimal score
estimator bQ 2P that minimizes the empirical score
bQ = arg min
When X = Rp and P consists of twice differentiable densities with respect to Lebesgue measure  the
Hyvärinen scoring rule [9] is given as
(3)
S(x  Q) = (1/2)||r log q(x)||2
where q is the density of Q with respect to Lebesgue measure on X   rf (x) = {@/(@xj)f (x)}j2[p]
denotes the gradient  and f (x) = Pj2[p] @2/(@x2
j )f (x) the Laplacian operator on Rp. This
scoring rule is convenient for learning models that are speciﬁed in an unnormalized fashion or whose
normalizing constant is difﬁcult to compute. The score matching rule is proper  that is  EX⇠P S(X  Q)
is minimized over P at Q = P . Under suitable regularity conditions  the Fisher divergence between
P  Q 2P   D(P  Q) =R p(x)||r log q(x)r log p(x)||2
2dx  where p is the density of P   is induced
by the score matching rule [9]. For a parametric exponential family P = {p✓ | ✓ 2 ⇥} with densities
given in (1)  minimizing (2) can be done in a closed form [9  8]. An estimatorb✓ obtained in this way
can be shown to be asymptotically consistent [9]  however  in general it will not be efﬁcient [8].
Hyvärinen [10] proposed a generalization of the score matching approach to the case of non-negative
data. When X = Rp

+ the scoring rule is given as

2 +  log q(x)

(2)

S+(x  Q) =Xa2V"2xa

@ log q(x)

@xa

+ x2
a

@2 log q(x)

@x2
a

+

1
2

a✓ @ log q(x)

@xa ◆2#.

x2

(4)

For exponential families  the non-negative score matching loss again can be obtained in a closed form
and the estimator is consistent and asymptotically normal under suitable conditions [10].
In the context of probabilistic graphical models  [8] studied score matching to learn Gaussian graphical
models with symmetry constraints. [15] proposed a regularized score matching procedure to learn
conditional independence graph in a high-dimensional setting by minimizing En [`(xi ✓ )] + ||✓||1 
where the loss `(xi ✓ ) is either S(xi  Q✓) or S+(xi  Q✓). For Gaussian models  `1-norm regularized
score matching is a simple but state-of-the-art method  which coincides with the method in [17].
Extending the work on estimation of inﬁnite-dimensional exponential families [26]  [27] study
learning structure of nonparametric probabilistic graphical models using a score matching estimator.
In the next section  we present a new estimator for components of ✓ in (1) that is consistent and
asymptotically normal  building on [15] and [4].

3 Methodology
In this section  we propose a procedure that constructs a pn-consistent estimator of an element ✓ab
of ✓. Our procedure is based on the three steps that we describe after introducing some additional
notation. We start by describing the procedure for the case where X = Rp.
For ﬁxed indices a  b 2 [p]  let qab
given Xab = xab. In particular 

✓ (xa  xb | xab) be the conditional density of (Xa  Xb)

✓ (x) := qab

log qab

✓ (x) = h✓ab ' (x)i  ab(✓  xab) + hab(x)

where ✓ab 2 Rs0 is a part of the vector ✓ corresponding to {✓(k)
bc }l2[L] c2ab
and '(x) = 'ab(x) 2 Rs0 is the corresponding vector of sufﬁcient statistics with the dimension

b }k2[K]  {✓(l)

a  ✓ (k)

ac  ✓ (l)

3

s0 = 2K + 2(p 2)L. Here ab(✓  xab) is the log-partition function for the conditional distribution
and hab(x) = ha(xa) + hb(xb). Let rabf (x) = ((@/@xa)f (x)  (@/@xb)f (x))T 2 R2 be the
gradient with respect to xa and xb and abf (x) =(@2/@x2
With this notation  we introduce the following scoring rule
2 + ab log qab

✓ (x) = (1/2)✓T (x)✓ + ✓T g(x) 

b) f (x).

a) + (@2/@x2

Sab(x  ✓) = (1/2)||rab log qab

✓ (x)||2

(5)

where
(x) = '1(x)'1(x)T + '2(x)'2(x)T

and

g(x) = '1(x)hab

1 (x) + '2(x)hab

1 = (@/@xa)hab  and hab

2 (x) + ab'(x)
with '1 = (@/@xa)'  '2 = (@/@xb)'  hab
2 = (@/@xb)hab. This scoring
rule is related to the one in (3)  however  rather than using the density q✓ in evaluating the parameter
vector  we only consider the conditional density qab
✓ . We will use this conditional scoring rule to
create an asymptotically normal estimator of an element ✓ab. Our motivation for using this estimator
comes from the fact that the parameter ✓ab can be identiﬁed from the conditional distribution of
(Xa  Xb) | XMab where Mab := {c | (a  c) 2 E or (b  c) 2 E} is the Markov blanket of (Xa  Xb).
Furthermore  the optimization problems arising in steps 1-3 below can be solved much more efﬁciently 
as the problems are of much smaller dimension.
We are now ready to describe our procedure for estimating ✓ab  which proceeds in three steps.
Step 1: We ﬁnd a pilot estimator of ✓ab by solving the following program

(6)

✓2Rs0

b✓ab = arg min

distribution cannot be estimated [14  22]. Therefore  we proceed to create a regular estimator of ✓ab

Since we are after an asymptotically normal estimator of ✓ab  one may think that it is sufﬁcient to ﬁnd

En⇥Sab(xi ✓ )⇤ + 1||✓||1
where 1 is a tuning parameter. LetcM1 = M (b✓ab) := {(c  d) |b✓ab
cd 6= 0}.
e✓ab = arg min{En⇥Sab(xi ✓ )⇤ | M (✓) ✓ cM1} and appeal to results of [21]. Unfortunately  this is
not the case. Sincee✓ is obtained via a model selection procedure  it is irregular and its asymptotic
in steps 2 and 3. The idea is to create an estimatore✓ab that is insensitive to ﬁrst order perturbations
of other components ofe✓ab  which we consider as nuisance components. The idea of creating an
Step 2: Letbab be a minimizer of
The vector (1 bab T)T approximately computes a row of the inverse of the Hessian in (6).
Step 3: LetfM = {(a  b)}[ cM1 [ M (bab). We obtain our estimator as a solution to the following

estimator that is robust to perturbations of nuisance have been recently used in [4]  however  the
approach goes back to the work of [19].

(1/2)En[('1 ab(xi)  '1 ab(xi)T )2 + ('2 ab(xi)  '2 ab(xi)T )2] + 2||||1.

program

(8)

(7)

e✓ab = arg min En⇥Sab(xi ✓ )⇤

s.t. M (✓) ✓ fM .

Motivation for this procedure will be clear from the proof of Theorem 1 given in the next section.
Extension to non-negative data. For non-negative data  the procedure is slightly different. Instead of
(5)  as shown in [15]  we instead deﬁne a different scoring rule Sab
2 ✓T +(x)✓ + ✓T g+(x)
with +(x) = x2
1 (x) + '2(x)hab
2 (x) +
b)'.
x2
a'11(x) + x2

a · '1(x)'1(x)T + x2
b'22(x) + 2xa'1(x) + 2xb'2(x). Here '11 = (@2/@x2

+ (x  ✓) = 1
b · '2(x)'2(x)T and g+(x) = '1(x)hab

a)'  and '22 = (@2/@x2

Now we can deﬁne e'1 = xa'1 and e'2 = xb'2. Then +(x) = e'1(x)e'1(x)T + e'2(x)e'2(x)T  
which is of the same form as (5) with e'1 and e'2 replacing '1 and '2  respectively. Thus our three

step procedure for non-negative data follows as before.

4 Asymptotic Normality of the Estimator

In this section  we outline main theoretical properties of our procedure. We start by providing
high-level conditions that allow us to establish properties of each step in our procedure.

4

Assumption M. We are given n i.i.d. samples {xi}i2[n] from p✓⇤ of the form in (1). The parameter
vector ✓⇤ is sparse  with |M (✓ab ⇤)|⌧ n. Let

ab ⇤ = arg min E[('1 ab(xi)  '1 ab(xi)T )2 + ('2 ab(xi)  '2 ab(xi)T )2]

(9)
and ⌘1i = '1 ab(xi)  '1 ab(xi)T ab ⇤ and ⌘2i = '2 ab(xi)  '2 ab(xi)T ab ⇤ for i 2 [n]. The
vector ab ⇤ is sparse with |M (ab ⇤)|⌧ n. Let m = |M (✓ab ⇤)|_| M (ab ⇤)|.
The assumption M supposes that the parameter to be estimated is sparse  which makes estimation
in high-dimensional setting feasible. An extension to approximately sparse parameter is possible 
but technical. One of the beneﬁts of using the conditional score to learn parameters of the model is
that the sample size will only depend on the size of M (✓ab ⇤) and not on the sparsity of the whole
vector ✓⇤ as in [15]. The second part of the assumption states that the inverse of population Hessian
is approximately sparse  which is a reasonable assumption since the Markov blanket of (Xa  Xb) is
small under the sparsity assumption on ✓ab ⇤.
Our next condition assumes that the Hessian in (6) and (7) is well conditioned. Let (s  A) =
minimal and maximal s-sparse eigenvalues of a semi-deﬁnite matrix A  respectively.
Assumption SE. The event

2 | 1 || ||0  s and +(s  A) = supT A/||||2

2 | 1 || ||0  s denote the

infT A/||||2

ESE = {min  (m · log n  En [(xi)])  +(m · log n  En [(xi)])  max}

holds with probability 1  SE where 0 < min  max < 1.
We choose to impose the sparse eigenvalue condition directly on En [(xi)] rather that on the
population quantity E [(xi)]. It is well known that the condition SE holds for a large number of
models. See for example [24] and speciﬁcally [31] for exponential family graphical models.

Let rj✓ = ||b✓ab  ✓ab ⇤||j and rj = ||bab  ab ⇤||j  for j 2{ 1  2}  be the rates of estimation in
steps 1 and 2. Under the assumption SE  on the event E✓ = {||En [(xi)✓ + g(xi)]||1  1/2}
we have that r1✓  c1m/ and r2✓  c2pm/.
Similarly  on the event E =
{||En [⌘1i'1 ab(xi) + ⌘2i'2 ab(xi)]||1  2/2} we have that r1  c1m/min and r2 
c2pm/min using results of [18]. Again  one needs to verify the two events hold with high-
probability for the model at hand. However  this is a routine calculation under suitable tail assumptions.
See for example Lemma 9 in [31].

max4

(10)

ab =

n · pnEn⇥w⇤ T(xi)✓ab ⇤ + g(xi)⇤ + O2

The following result establishes a Bahadur representation fore✓ab.
Theorem 1. Suppose that assumptions M and SE holds. Deﬁne w⇤ with w⇤ab = 1 and w⇤
ab ⇤  where ab ⇤ is given in the assumption M. On the event E \E ✓  we have that
pn ·⇣e✓ab  ✓⇤ab⌘ = b1

min · pn2m  
where  = 1 _ 2 and n = En [⌘1i'1 ab(xi) + ⌘2i'2 ab(xi)].
Theorem 1 is deterministic in nature. It establishes a representation that holds on the event E \E ✓ \
ESE  which in many cases holds with overwhelming probability. We will show that under suitable
conditions the ﬁrst term converges to a normal distribution. The following is a regularity condition
needed even in a low dimensional setting for asymptotic normality [8].
Assumption R. Eqab⇥||(Xa  Xb  xab)✓ab ⇤||2⇤ and Eqab⇥||g(Xa  Xb  xab)||2⇤ are ﬁnite for all
R holds  (m log p)2/n = o(1) and P (E \E ✓ \E SE) ! 1. Then pn(e✓ab  ✓⇤ab) !D N (0  V ) +
op(1)  where V = (E [n])2 · Varw⇤ T(xi)✓ab + g(xi) and n is as in Theorem 1.
V using the following consistent estimatorbV  
fMi⌘En [(xi)]fM1 eab 
abEn [(xi)]fM1⇣Enh⇣(xi)e✓ab + g(xi)⌘fM⇣(xi)e✓ab + g(xi)⌘T

values of xab in the domain.
Theorem 1 and Lemma 9 together give the following corollary:
Corollary 2. Suppose that the conditions of Theorem 1 hold. In addition  suppose the assumption

We see that the variance V depend on true ✓ab and ab  which are unknown. In practice  we estimate

eT

5

where eab is a canonical vector with 1 in position of element ab. Using this estimate  we can construct
a conﬁdence interval with asymptotically nominal coverage. In particular 

lim
n!1

sup
✓⇤2⇥

P✓⇤✓✓⇤ab 2e✓ab ± z↵/2 ·qbV /n◆ = ↵ + o(1).

In the next section  we outline the proof of Theorem 1. Proofs of other technical results are relegated
to appendix.

4.1 Proof of Theorem 1

(11)

problem

We ﬁrst introduce some auxiliary estimates. Leteab be a minimizer of the following constrained
+'2 ab(xi)  '2 ab(xi)T 2i s.t. M () ✓ fM
min Enh'1 ab(xi)  '1 ab(xi)T 2
wherefM is deﬁned in step 3 of the procedure. Essentially eab is the reﬁtted estimator from step 2
constrained to have the support onfM. Let ew 2 Rs0 with ewab = 1  ewfM = efM and zero elsewhere.
The solutione✓ab satisﬁes the ﬁrst order optimality condition⇣En [(xi)]e✓ab + En[g(xi)]⌘fM
Multiplying by ew  it follows that
ewT⇣En [(xi)]e✓ab + En[g(xi)]⌘
= (ew  w⇤)T En [(xi)]⇣e✓ab  ✓ab ⇤⌘ + (ew  w⇤)TEn⇥(xi)✓ab ⇤ + g(xi)⇤ +
w⇤ TEn [(xi)]⇣e✓ab  ✓ab ⇤⌘ + w⇤ TEn⇥(xi)✓ab ⇤ + g(xi)⇤   L1 + L2 + L3 + L4 = 0.
(12)
min · 2m. Using Lemma 8  the
term L3 can be written as En [⌘1i'1 ab(xi) + ⌘2i'2 ab(xi)]⇣e✓ab  ✓ab ⇤ab ⌘ + O⇣1/2
min · 2m⌘.

From Lemma 6 and Lemma 7  we have that |L1 + L2| C · 2

Putting all the pieces together completes the proof.

max4

max2

= 0.

5 Synthetic Datasets

In this section we illustrate ﬁnite sample properties of our inference procedure on data simulated
from three different Exponential family distributions. The ﬁrst two examples involve Gaussian
node-conditional distributions  for which we use regularized score matching. For the third setting
where the node-conditional distributions follow an Exponential distribution  we use regularized
non-negative score matching procedure. In each example  we report the mean coverage rate of 95%
conﬁdence intervals for several coefﬁcients averaged over 500 independent simulation runs.
Gaussian Graphical Model. We ﬁrst consider the simplest case of a Gaussian graphical model. The
data is generated according to X ⇠ N (0  ⌃). We denote the precision matrix by ⌦=⌃ 1 = (wab)
(the inverse of covariance matrix).
For the experiment  we set diagonal entries of ⌦ as wjj = 1  and we set the coefﬁcients of the 4
nearest neighbor lattice graph according to wj j1 = wj1 j = 0.5 and wj j2 = wj2 j = 0.3. We
set the sample size n = 300. Table 1 shows the empirical coverage rate for different choices of the
number of nodes p for four chosen coefﬁcients. As is evident  our inference procedure performs
remarkably well for the Gaussian graphical model studied.
Normal Conditionals. Our second synthetic dataset is sampled from the following exponential
j=1 jxj}  where
b = (1  . . .   p) and b(2) = ((2)
p ) are p dimensional vectors  and B = {jk} is a
symmetric interaction matrix with diagonal entries set to 0. The above distribution is a special case
of a class of exponential family distributions with normal conditionals  and densities that need not be
unimodal [1]. This family is intriguing from the perspective of graphical modeling as  in contrast to
the Gaussian case  conditional dependence may also express itself in the variances.

family distribution: q(x|B  b  b(2)) / exp{Pj6=k jkx2

k +Pp

j +Pp

1   . . .   (2)

j=1 (2)

j x2

j x2

6

Table 1: Empirical Coverage for Gaussian Graphical Model

p = 50
p = 200
p = 400

w1 3

w1 4

w1 2
w1 10
95.4% 92.4% 93.8% 93.2%
94.6% 92.4% 92.6% 94.0%
94.6% 94.8% 92.6% 93.8%

Table 2: Empirical Coverage for Normal Conditionals

p = 100
p = 300

1 3

1 2
1 10
93.2% 93.4% 94.6% 95.0%
93.2% 93.0% 92.6% 93.0%

1 4

Table 3: Empirical Coverage for Exponential Graphical Model

p = 100
p = 300

✓1 3

✓1 10
✓1 2
92.0% 90.0% 90.0% 92.4%
92.6% 92.0% 92.2% 92.4%

✓1 4

For our experiment we set j = 0.4  (2)
j = 2  and we use a 4 nearest neighbor lattice dependence
graph with interaction matrix: j j1 = j1 j = 0.2 and j j2 = j2 j = 0.2. Since the
univariate marginal distributions are all Gaussian  we generate the data by Gibbs sampling. The ﬁrst
500 samples were discarded as ‘burn in’ step  and of the remaining samples  we keep one in three.
We set the number of samples n = 500. Table 2 shows the empirical coverage rate for p = 100
and p = 300 nodes. Again  we see that our inference algorithm behaves well on the above Normal
Conditionals Model.
Exponential Graphical Model. Our ﬁnal synthetic simulated example illustrates non-negative
score matching for Exponential Graphical Model. Here the node-conditional distributions obey an
exponential distribution  and therefore the variables take only non-negative values. Such exponential
distributions are typically used for data describing inter-arrival times between events  among other

j=1 ✓jXj Pj6=k ✓jkXjXk}. To
applications. The density function is given by q(x|✓) / exp{Pp
ensure that the distribution is valid and normalizable  we require ✓j > 0  and ✓jk  0. Therefore 
we can only model negative dependencies via the Exponential graphical model. For the experiment
we choose ✓j = 2  and a 2 nearest neighbor dependence graph with ✓j j1 = ✓j1 j = 0.3. We set
n = 1000 and again use Gibbs sampling to generate data. The empirical coverage rate and histograms
of estimates of four selected coefﬁcients are presented in Table 3 and Figures 1 for p = 100 and
p = 300  respectively.
We should point out that  in general  non-negative score matching is harder than regular score
matching. For example  as shown in [15]  to recover the structure from a regular Gaussian distribution

y
t
i
s
n
e
D

y
t
i
s
n
e
D

5

4

3

2

1

0

5

4

3

2

1

0

6

5

4

3

2

1

y
t
i
s
n
e
D

6

5

4

3

2

1

y
t
i
s
n
e
D

6

5

4

3

2

1

y
t
i
s
n
e
D

0

0.2

θ

1 2

0.4

0.6

0
-0.4

-0.2

θ

0

1 3

0.2

0.4

0
-0.4

-0.2

θ

0

1 4

0.2

0.4

0
-0.4

-0.2

θ

0

0.2

0.4

1 10

6

5

4

3

2

1

y
t
i
s
n
e
D

6

5

4

3

2

1

y
t
i
s
n
e
D

6

5

4

3

2

1

y
t
i
s
n
e
D

0

0.2

θ

1 2

0.4

0.6

0
-0.3 -0.2 -0.1

θ

0

0.1

0.2

1 3

0
-0.3 -0.2 -0.1

θ

0

0.1

0.2

1 4

0
-0.4

-0.2

θ

0

0.2

0.4

1 10

Figure 1: Histograms for ✓: the ﬁrst row is for p = 100 and the second row is for p = 300

7

with high probability  a sample size about O(m2 log p) sufﬁces  while to recover from non-negative
Gaussian distribution  we need O(m2(log p)8)  which is signiﬁcantly larger. Therefore  we expect
that conﬁdence intervals for non-negative score matching would require more samples to give accurate
inference. We can see this from Table 3  where the empirical coverage rate tends to be about 92% 
rather than the designed 95% – still impressive for the not so large sample size. The histograms in
Figures 1 show that the ﬁtting is quite good  but to get a better estimation and hence better coverage 
we would need more samples.

6 Protein Signaling Dataset

In this section we apply our algorithm to a protein signaling ﬂow cytometry dataset. The dataset
contains the presence of p = 11 proteins in n = 7466 cells. It was ﬁrst analyzed using Bayesian
Networks in [25] who ﬁt a directed acyclic graph to the data  while [31] ﬁt their proposed M-estimators
for exponential and Gaussian graphical models to the data set.
Figure 2 shows the network structure after applying our method to the data using an Exponential
Graphical Model. Since the data is non-negative and skewed  it can also be analyzed after log
transformation as was done by [31] for ﬁtting Gaussian graphical model. We instead learn the
structure directly from the data without such a transformation. To infer the network structure  we
calculate the p-value for each pair of nodes  and keep the edges with p-values smaller than 0.01.
Estimated negative conditional dependencies are shown via red edges in the ﬁgure. Recall that
the exponential graphical model restricts the edge weights to be non-negative  hence only negative
dependencies can be estimated. From the ﬁgure we see that PKA is a major protein inhibitor in cell
signaling networks. This result is consistent with the estimated graph structure in [31]  as well as in
the Bayesian network of [25]. In addition  we ﬁnd signiﬁcant dependency between PKC and PIP3.

Jnk

P38

Raf

PKC

PKA

Mek

Plcg

Akt

PIP2

Erk

PIP3

Figure 2: Estimated Structure of Protein Signaling Dataset

7 Conclusion

Driven by applications in Biology and Social Networks  there has been a surge in statistical learning
models and methods for networks with large number of nodes. Graphical models provide a very
ﬂexible modeling framework for such networks  leading to much work in estimation and inference
algorithms for Gaussian graphical models  and more generally for graphical models with node-
conditional densities lying in Exponential family  in high dimensional setting. Most of this work is
based on regularized likelihood loss minimization  which has the disadvantage of being computation-
ally intractable when the normalization constant (partition function) of the conditional densities is
not available in closed form. Score matching estimators provide a way around this issue  but so far
there has been no work which provides inference guarantees for score matching based estimators for
high-dimensional graphical models. In this paper we ﬁll this gap for the case where score matching
is used to estimate the parameter corresponding to a single edge at a time. An interesting future
extension would be to perform inference on the entire model instead of one edge at a time as in the
current paper. Another extension would be to extend our results to discrete valued data.

8

References
[1] B. C. Arnold  E. Castillo  and J. M. Sarabia. Conditional speciﬁcation of statistical models. Springer Series

in Statistics. Springer-Verlag  New York  1999. ISBN 0-387-98761-4.

[2] R. F. Barber and M. Kolar. Rocket: Robust conﬁdence intervals via kendall’s tau for transelliptical graphical

models. ArXiv e-prints  arXiv:1502.07641  Feb. 2015.

[3] A. Belloni and V. Chernozhukov. Least squares after model selection in high-dimensional sparse models.

Bernoulli‘  19(2):521–547  May 2013.

[4] A. Belloni  V. Chernozhukov  and C. B. Hansen. Inference on treatment effects after selection amongst

high-dimensional controls. Rev. Econ. Stud.  81(2):608–650  Nov 2013.

[5] M. Chen  Z. Ren  H. Zhao  and H. H. Zhou. Asymptotically normal and efﬁcient estimation of covariate-

adjusted gaussian graphical model. Journal of the American Statistical Association  0(ja):00–00  2015.

[6] S. Chen  D. M. Witten  and A. Shojaie. Selection and estimation for mixed graphical models. ArXiv

e-prints  arXiv:1311.0085  Nov. 2013.

Statistics and Its Application  3  2016.

[7] M. Drton and M. H. Maathuis. Structure learning in graphical modeling. To appear in Annual Review of

[8] P. G. M. Forbes and S. L. Lauritzen. Linear estimating equations for exponential families with application

to Gaussian linear concentration models. Linear Algebra Appl.  473:261–283  2015.

[9] A. Hyvärinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res.  6:

695–709  2005.

[10] A. Hyvärinen. Some extensions of score matching. Comput. Stat. Data Anal.  51(5):2499–2512  2007.
[11] J. Jankova and S. A. van de Geer. Conﬁdence intervals for high-dimensional inverse covariance estimation.

ArXiv e-prints  arXiv:1403.6752  Mar. 2014.

[12] A. Javanmard and A. Montanari. Conﬁdence intervals and hypothesis testing for high-dimensional

regression. J. Mach. Learn. Res.  15(Oct):2869–2909  2014.

[13] J. D. Lee and T. J. Hastie. Learning the structure of mixed graphical models. J. Comput. Graph. Statist. 

24(1):230–253  2015.

[14] H. Leeb and B. M. Pötscher. Can one estimate the unconditional distribution of post-model-selection

estimators? Econ. Theory  24(02):338–376  Nov 2007.

[15] L. Lin  M. Drton  and A. Shojaie. Estimation of high-dimensional graphical models using regularized

score matching. ArXiv e-prints  arXiv:1507.00433  July 2015.

[16] W. Liu. Gaussian graphical model estimation with false discovery rate control. Ann. Stat.  41(6):2948–2978 

[17] W. Liu and X. Luo. Fast and adaptive sparse precision matrix estimation in high dimensions. J. Multivar.

2013.

Anal.  135:153–162  2015.

[18] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. Stat. Sci.  27(4):538–557  2012.

[19] J. Neyman. Optimal asymptotic tests of composite statistical hypotheses. Probability and statistics  57:

213  1959.

2012.

1–18  2009.

[20] M. Parry  A. P. Dawid  and S. L. Lauritzen. Proper local scoring rules. Ann. Stat.  40(1):561–592  Feb

[21] S. L. Portnoy. Asymptotic behavior of likelihood methods for exponential families when the number of

parameters tends to inﬁnity. Ann. Stat.  16(1):356–366  1988.

[22] B. M. Pötscher. Conﬁdence sets based on sparse estimators are necessarily large. Sankhy¯a  71(1  Ser. A):

[23] Z. Ren  T. Sun  C.-H. Zhang  and H. H. Zhou. Asymptotic normality and optimalities in estimation of large

Gaussian graphical models. Ann. Stat.  43(3):991–1026  2015.

[24] M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements. 2011.
[25] K. Sachs  O. Perez  D. Pe’er  D. A. Lauffenburger  and G. P. Nolan. Causal protein-signaling networks

derived from multiparameter single-cell data. Science  308(5721):523–529  2005.

[26] B. Sriperumbudur  K. Fukumizu  A. Gretton  and A. Hyvärinen. Density estimation in inﬁnite dimensional

exponential families. ArXiv e-prints  arXiv:1312.3516  Dec. 2013.

[27] S. Sun  M. Kolar  and J. Xu. Learning structured densities via inﬁnite dimensional exponential families.
In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett  editors  Advances in Neural
Information Processing Systems 28  pages 2287–2295. Curran Associates  Inc.  2015.

[28] S. A. van de Geer  P. Bühlmann  Y. Ritov  and R. Dezeure. On asymptotically optimal conﬁdence regions

and tests for high-dimensional models. Ann. Stat.  42(3):1166–1202  Jun 2014.

[29] J. Wang and M. Kolar. Inference for high-dimensional exponential family graphical models. In A. Gretton

and C. C. Robert  editors  Proc. of AISTATS  volume 51  pages 751–760  2016.

[30] E. Yang  Y. Baker  P. Ravikumar  G. I. Allen  and Z. Liu. Mixed graphical models via exponential families.

In Proc. 17th Int. Conf  Artif. Intel. Stat.  pages 1042–1050  2014.

[31] E. Yang  P. Ravikumar  G. I. Allen  and Z. Liu. On graphical models via univariate exponential family

distributions. J. Mach. Learn. Res.  16:3813–3847  2015.

[32] C.-H. Zhang and S. S. Zhang. Conﬁdence intervals for low dimensional parameters in high dimensional

linear models. J. R. Stat. Soc. B  76(1):217–242  Jul 2013.

9

,Ming Yu
Mladen Kolar
Varun Gupta
Marek Śmieja
Łukasz Struski
Jacek Tabor
Bartosz Zieliński
Przemysław Spurek
Xiuyuan Lu
Benjamin Van Roy