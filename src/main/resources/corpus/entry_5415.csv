2017,Tensor Biclustering,Consider a dataset where data is collected on multiple features of multiple individuals over multiple times. This type of data can be represented as a three dimensional individual/feature/time tensor and has become increasingly prominent in various areas of science. The tensor biclustering problem computes a subset of individuals and a subset of features whose signal trajectories over time lie in a low-dimensional subspace  modeling similarity among the signal trajectories while allowing different scalings across different individuals or different features. We study the information-theoretic limit of this problem under a generative model. Moreover  we propose an efficient spectral algorithm to solve the tensor biclustering problem and analyze its achievability bound in an asymptotic regime. Finally  we show the efficiency of our proposed method in several synthetic and real datasets.,Tensor Biclustering

Soheil Feizi

Stanford University

sfeizi@stanford.edu

Hamid Javadi

Stanford University

hrhakim@stanford.edu

David Tse

Stanford University

dntse@stanford.edu

Abstract

Consider a dataset where data is collected on multiple features of multiple individu-
als over multiple times. This type of data can be represented as a three dimensional
individual/feature/time tensor and has become increasingly prominent in various
areas of science. The tensor biclustering problem computes a subset of individuals
and a subset of features whose signal trajectories over time lie in a low-dimensional
subspace  modeling similarity among the signal trajectories while allowing dif-
ferent scalings across different individuals or different features. We study the
information-theoretic limit of this problem under a generative model. Moreover 
we propose an efﬁcient spectral algorithm to solve the tensor biclustering problem
and analyze its achievability bound in an asymptotic regime. Finally  we show the
efﬁciency of our proposed method in several synthetic and real datasets.

Introduction

1
Let T ∈ Rn1×n2 be a data matrix whose rows and columns represent individuals and features 
respectively. Given T  the matrix biclustering problem aims to ﬁnd a subset of individuals (i.e. 
J1 ⊂ {1  2  ...  n1}) which exhibit similar values across a subset of features (i.e.  J2 ⊂ {1  2  ...  n2})
(Figure 1-a). The matrix biclustering problem has been studied extensively in machine learning and
statistics and is closely related to problems of sub-matrix localization  planted clique and community
detection [1  2  3].
In modern datasets  however  instead of collecting data on every individual-feature pair at a single
time  we may collect data at multiple times. One can visualize a trajectory over time for each
individual-feature pair. This type of datasets has become increasingly prominent in different areas of
science. For example  the roadmap epigenomics dataset [4] provides multiple histon modiﬁcation
marks for genome-tissue pairs  the genotype-tissue expression dataset [5] provides expression data
on multiple genes for individual-tissue pairs  while there have been recent efforts to collect various
omics data in individuals at different times [6].
Suppose we have n1 individuals  n2 features  and we collect data for every individual-feature pair
at m different times. This data can be represented as a three dimensional tensor T ∈ Rn1×n2×m
(Figure 1-b). The tensor biclustering problem aims to compute a subset of individuals and a subset
of features whose trajectories are highly similar. Similarity is modeled as the trajectories as lying
in a low-dimensional (say one-dimensional) subspace (Figure 1-d). This deﬁnition allows different
scalings across different individuals or different features  and is important in many applications
such as in omics datasets [6] because individual-feature trajectories often have their own intrinsic
scalings. In particular  at each time the individual-feature data matrix may not exhibit a matrix
bicluster separately. This means that repeated applications of matrix biclustering cannot solve the
tensor biclustering problem. Moreover  owing to the same reason  trajectories in a bicluster can
have large distances among themselves (Figure 1-d). Thus  a distance-based clustering of signal
trajectories is likely to fail as well.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: (a) The matrix biclustering problem. (b) The tensor biclustering problem. (c) The tensor
triclustering problem. (d) A visualization of a bicluster in a three dimensional tensor. Trajectories in
the bicluster (red points) form a low dimensional subspace.

This problem formulation has two main differences with tensor triclustering  which is a natural
generalization of matrix biclustering to a three dimensional tensor (Figure 1-c). Firstly  unlike tensor
triclustering  tensor biclustering has an asymmetric structure along tensor dimensions inspired by
aforementioned applications. That is  since a tensor bicluster is deﬁned as a subset of individuals
and a subset of features with similar trajectories  the third dimension of the tensor (i.e.  the time
dimension) plays a different role compared to the other two dimensions. This is in contrast with
tensor triclustering where there is not such a difference between roles of tensor dimensions in deﬁning
the cluster. Secondly  in tensor biclustering  the notion of a cluster is deﬁned regarding to trajectories
lying in a low-dimensional subspace while in tensor triclustering  a cluster is deﬁned as a sub-cube
with similar entries.
Finding statistically signiﬁcant patterns in multi-dimensional data tensors has been studied in di-
mensionality reduction [7  8  9  10  11  12  13  14]  topic modeling [15  16  17]  among others. One
related model is the spiked tensor model [7]. Unlike the tensor biclustering model that is asymmetric
along tensor dimensions  the spiked tensor model has a symmetric structure. Computational and
statistical limits for the spiked tensor model have been studied in [8  9  10  14]  among others. For
more details  see Supplementary Materials (SM) Section 1.3.
In this paper  we study information-theoretic and computational limits for the tensor biclustering
problem under a statistical model described in Section 2. From a computational perspective  we
present four polynomial time methods and analyze their asymptotic achievability bounds. In particular 
one of our proposed methods  namely tensor folding+spectral  outperforms other methods both
theoretically (under realistic model parameters) and numerically in several synthetic and real data
experiments. Moreover  we characterize a fundamental limit under which no algorithm can solve the
tensor biclustering problem reliably in a minimax sense. We show that above this limit  a maximum
likelihood estimator (MLE) which has an exponential computational complexity can solve this
problem with vanishing error probability.

1.1 Notation
We use T   X   and Z to represent input  signal  and noise tensors  respectively. For any set J  |J|
denotes its cardinality. [n] represents the set {1  2  ...  n}. ¯J = [n] − J. (cid:107)x(cid:107)2 = (xtx)1/2 is the
second norm of the vector x. x ⊗ y is the Kronecker product of two vectors x and y. The asymptotic
notation a(n) = O(b(n)) means that  there exists a universal constant c such that for sufﬁciently

2

-24-1024122030-2-2-4-4-34-2-1240120230-2-2-4-4All TrajectoriesTrajectories in the Bicluster(d)(c)(b)(a)Tensor TriclusteringfeaturestimesindividualsTensor BiclusteringfeaturesindividualsOur ModelTensor BiclusteringMatrix Biclusteringfeaturestimesindividualslarge n  we have |a(n)| < cb(n). If there exists c > 0 such that a(n) = O(b(n) log(n)c)  we use
the notation a(n) = ˜O(b(n)). The asymptotic notation a(n) = Ω(b(n)) and a(n) = ˜Ω(b(n)) is the
same as b(n) = O(a(n)) and b(n) = ˜O(a(n))  respectively. Moreover  we write a(n) = Θ(b(n))
iff a(n) = Ω(b(n)) and b(n) = Ω(a(n)). Similarly  we write a(n) = ˜Θ(b(n)) iff a(n) = ˜Ω(b(n))
and b(n) = ˜Ω(a(n)).

2 Problem Formulation
Let T = X + Z where X is the signal tensor and Z is the noise tensor. Consider

T = X + Z =

σru(J1)

r ⊗ w(J2)

r ⊗ vr + Z 

(1)

q(cid:88)

r=1

r

r

1

1

and w(J2)

where u(J1)
have zero entries outside of J1 and J2 index sets  respectively. We assume
σ1 ≥ σ2 ≥ ... ≥ σq > 0. Under this model  trajectories X (J1  J2  :) form an at most q dimensional
subspace. We assume q (cid:28) min(m |J1| × |J2|).
Deﬁnition 1 (Tensor Biclustering). The problem of tensor biclustering aims to compute bicluster
index sets (J1  J2) given T according to (1).
In this paper  we make the following simplifying assumptions: we assume q = 1  n = |n1| = |n2| 
and k = |J1| = |J2|. To simplify notation  we drop superscripts (J1) and (J2) from u(J1)
and
  respectively. Without loss of generality  we normalize signal vectors such that (cid:107)u1(cid:107) =
w(J2)
(cid:107)w1(cid:107) = (cid:107)v1(cid:107) = 1. Moreover  we assume that for every (j1  j2) ∈ J1 × J2  ∆ ≤ u1(j1) ≤ c∆
and ∆ ≤ w1(j2) ≤ c∆  where c is a constant. Under these assumptions  a signal trajectory can be
written as X (j1  j2  :) = u1(j1)w1(j2)v1. The scaling of this trajectory depends on row and column
speciﬁc parameters u1(j1) and w1(j2). Note that our analysis can be extended naturally to a more
general setup of having multiple embedded biclusters with q > 1. We discuss this in Section 7.
Next we describe the noise model. If (j1  j2) /∈ J1 × J2  we assume that entries of the noise trajectory
Z(j1  j2  :) are i.i.d. and each entry has a standard normal distribution. If (j1  j2) ∈ J1 × J2  we
assume that entries of Z(j1  j2  :) are i.i.d. and each entry has a Gaussian distribution with zero mean
and σ2
- Noise Model I: In this model  we assume σ2

z variance. We analyze the tensor biclustering problem under two noise models for σ2
z:

z = 1  i.e.  the variance of the noise within and outside
of the bicluster is assumed to be the same. This is the noise model often considered in analysis
of sub-matrix localization [2  3] and tensor PCA [7  8  9  10  11  12  14]. Although this model
simpliﬁes the analysis  it has the following drawback: under this noise model  for every value
of σ1  the average trajectory lengths in the bicluster is larger than the average trajectory lengths
outside of the bicluster. See SM Section 1.2 for more details.
z = max(0  1− σ2

z is modeled to minimize
If
the difference between the average trajectory lengths within and outside of the bicluster.
1 < mk2  noise is added to make the average trajectory lengths within and outside of the bicluster
σ2
comparable. See SM Section 1.2 for more details.

- Noise Model II: In this model  we assume σ2

mk2 )  i.e.  σ2

1

3 Computational Limits of the Tensor Biclustering Problem

3.1 Tensor Folding+Spectral

Recall the formulation of the tensor biclustering problem (1). Let

T(j1 1) (cid:44) T (j1  :  :)

(2)
be horizontal (the ﬁrst mode) and lateral (the second mode) matrix slices of the tensor T   respectively.
One way to learn the embedded bicluster in the tensor is to compute row and column indices whose
trajectories are highly correlated with each other. To do that  we compute

and T(j2 2) (cid:44) T (:  j2  :) 

Tt

(j2 2)T(j2 2)

Tt

(j1 1)T(j1 1).

(3)

C1 (cid:44) n(cid:88)

and C2 (cid:44) n(cid:88)

j2=1

j1=1

3

Figure 2: A visualization of the tensor folding+spectral algorithm 1 to compute the bicluster index
set J2. The bicluster index set J1 can be computed similarly.

Algorithm 1 Tensor Folding+Spectral

Input: T   k
Compute ˆu1  the top eigenvector of C1
Compute ˆw1  the top eigenvector of C2
Compute ˆJ1  indices of the k largest values of | ˆw1|
Compute ˆJ2  indices of the k largest values of |ˆu1|
Output: ˆJ1 and ˆJ2

C1 represents a combined covariance matrix along the tensor columns (Figure 2). We refer to it as
the folded tensor over the columns. If there was no noise  this matrix would be equal to σ2
1.
1u1ut
Thus  its eigenvector corresponding to the largest eigenvalue would be equal to u1. On the other
hand  we have u1(j1) = 0 if j1 /∈ J1 and |u1(j1)| > ∆  otherwise. Therefore  selecting k indices of
the top eigenvector with largest magnitudes would recover the index set J1. However  with added
noise  the top eigenvector of the folded tensor would be a perturbed version of u1. Nevertheless one
can estimate J1 similarly (Algorithm 1). A similar argument holds for C2.
Theorem 1. Let ˆu1 and ˆw1 be top eigenvectors of C1 and C2  respectively. Under both noise models
I and II 

√
- for m < ˜O(
√
- for m = ˜Ω(

n)  if σ2

n)  if σ2

1 = ˜Ω(n) 
√
1 = ˜Ω(

n max(n  m)) 

1 ∈ ¯J1  j2 ∈ J2 and j(cid:48)

2 ∈ ¯J2.

as n → ∞  with high probability  we have |ˆu1(j1)| > |ˆu1(j(cid:48)
j1 ∈ J1  j(cid:48)
In the proof of Theorem 1  following the result of [18] for a Wigner noise matrix  we have proved an
l∞ version of the Davis-Kahan Lemma for a Wishart noise matrix. This lemma can be of independent
interest for the readers.

1)| and | ˆw1(j2)| > | ˆw1(j(cid:48)

2)| for every

3.2 Tensor Unfolding+Spectral
Let Tunf olded ∈ Rm×n2 be the unfolded tensor T such that Tunf olded(:  (j1 − 1)n + j2) =
T (j1  j2  :) for 1 ≤ j1  j2 ≤ n. Without noise  the right singular vector of this matrix is u1 ⊗ w1
which corresponds to the singular value σ1. Therefore  selecting k2 indices of this singular vector
with largest magnitudes would recover the index set J1 × J2. With added noise  however  the top
singular vector of the unfolded tensor will be perturbed. Nevertheless one can estimate J1 × J2
similarly (SM Section 2).

4

n2n2. . .. . .n2mn2mn1Matrix SlicesTT(n1  :   : )T(k+1  :   : )T(k  :   : )T(1  :   : ). . .. . .T(n1  :   : ) T(n1  :   : )tT(1  :   : ) T(1  :   : )tn2n2Combined CovarianceSpectralDecomposition Bicluster Index Set(J2)Input Tensor Theorem 2. Let ˆx be the top right singular vector of Tunf olded. Under both noise models I and II 
1 = ˜Ω(max(n2  m))  as n → ∞  with high probability  we have |ˆx(j(cid:48))| < |ˆx(j)| for every j in
if σ2
the bicluster and j(cid:48) outside of the bicluster.

3.3 Thresholding Sum of Squared and Individual Trajectory Lengths

If the average trajectory lengths in the bicluster is larger than the one outside of the bicluster  methods
based on trajectory length statistics can be successful in solving the tensor biclustering problem. One
such method is thresholding individual trajectory lengths. In this method  we select k2 indices (j1  j2)
with the largest trajectory length (cid:107)T (j1  j2  :)(cid:107) (SM Section 2).
Theorem 3. As n → ∞  with high probability  ˆJ1 = J1 and ˆJ2 = J2

√

- if σ2

- if σ2

mk2)  under noise model I.

1 = ˜Ω(
1 = ˜Ω(mk2)  under noise model II.

Another method to solve the tensor biclustering problem is thresholding sum of squared trajectory
lengths. In this method  we select k row indices with the largest sum of squared trajectory lengths
along the columns as an estimation of J1. We estimate J2 similarly (SM Section 2).
Theorem 4. As n → ∞  with high probability  ˆJ1 = J1 and ˆJ2 = J2

√

- if σ2

- if σ2

1 = ˜Ω(k
1 = ˜Ω(mk2 + k

√

nm)  under noise model I.

nm)  under noise model II.

4 Statistical (Information-Theoretic) Limits of the Tensor Biclustering

Problem

4.1 Coherent Case

√

In this section  we study a statistical (information theoretic) boundary for the tensor biclustering
k for j1 ∈ J1. Similarly 
√
problem under the following statistical model: We assume u1(j1) = 1/
k for j2 ∈ J2. Moreover  we assume v1 is a ﬁxed given vector with
we assume w1(j2) = 1/
(cid:107)v1(cid:107) = 1. In the next section  we consider a non-coherent model where v1 is random and unknown.
Let T be an observed tensor from the tensor biclustering model (J1  J2). Let Jall be the set of

all possible (J1  J2). Thus  |Jall| =(cid:0)n

(cid:1)2. A maximum likelihood estimator (MLE) for the tensor
T (j1  j2  :) − k(1 − σ2

(cid:107)T (j1  j2  :)(cid:107)2

(cid:88)

k
biclustering problem can be written as:

(cid:88)

(4)

z )

vt
1

2σ1

(j1 j2)∈ ˆJ1× ˆJ2

max
ˆJ∈Jall

(j1 j2)∈ ˆJ1× ˆJ2
( ˆJ1  ˆJ2) ∈ Jall.

to compute the likelihood function for(cid:0)n

(cid:1)2 possible bicluster indices. Thus  the computational

Note that under the noise model I  the second term is zero. To solve this optimization  one needs

k
complexity of the MLE is exponential in n.
1 = ˜Ω(k)  as n → ∞  with high probability  (J1  J2) is
Theorem 5. Under noise model I  if σ2
the optimal solution of optimization (4). A similar result holds under noise model II if mk =
Ω(log(n/k)).

Next  we establish an upper bound on σ2
1 under which no computational method can solve the tensor
biclustering problem with vanishing probability of error. This upper bound indeed matches with the
MLE achievability bound of Theorem 5 indicating its tightness.
Theorem 6. Let T be an observed tensor from the tensor biclustering model with bicluster indices
(J1  J2). Let A be an algorithm that uses T and computes ( ˆJ1  ˆJ2). Under noise model I  for any

5

ﬁxed 0 < α < 1  if σ2

(cid:105)
1 < cαk log(n/k)  as n → ∞  we have

P(cid:104) ˆJ1 (cid:54)= J1 or ˆJ2 (cid:54)= J2

sup

inf

A∈AllAlg

(J1 J2)∈Jall

> 1 − α −

log(2)

2k log(ne/k)

.

(5)

A similar result holds under noise model II if mk = Ω(log(n/k)).

4.2 Non-coherent Case

In this section we consider a similar setup to the one of Section 4.1 with the difference that v1 is
assumed to be uniformly distributed over a unit sphere. For simplicity  in this section we only consider
noise model I. The ML optimization in this setup can be written as follows:

max
ˆJ∈Jall

(cid:107) (cid:88)

(j1 j2)∈ ˆJ1× ˆJ2
( ˆJ1  ˆJ2) ∈ Jall.
1 = ˜Ω(max(k 

T (j1  j2  :)(cid:107)2

(6)

√

km))  as n → ∞  with high probability 

Theorem 7. Under noise model I  if σ2
(J1  J2) is the optimal solution of optimization (6).

If k > Ω(m)  the achievability bound of Theorem 7 simpliﬁes to the one of Theorem 5. In this case 
using the result of Theorem 6  this bound is tight. If k < O(m)  the achievability bound of Theorem
√
7 simpliﬁes to ˜Ω(
mk) which is larger than the one of Theorem 5 (this is the price we pay for not
knowing v1). In the following  we show that this bound is also tight.
To show the converse of Theorem 7  we consider the detection task which is presumably easier than
the estimation task. Consider two probability distributions: (1) Pσ1 under which the observed tensor
is T = σ1u1 ⊗ w1 ⊗ v1 + Z where J1 and J2 have uniform distributions over k subsets of [n] and
v1 is uniform over a unit sphere. (2) P0 under which the observed tensor is T = Z. Noise entries are
i.i.d. normal. We need the following deﬁnition of contiguous distributions ([8]):
Deﬁnition 2. For every n ∈ N  let P0 n and P1 n be two probability measures on the same measure
space. We say that the sequence (P1 n) is contiguous with respect to (P0 n)  if  for any sequence of
events An  we have

Theorem 8. If σ2

1 < ˜O(

√

P0 n(An) = 0 ⇒ lim
n→∞

lim
n→∞
mk)  Pσ1 is contiguous with respect to P0.

P1 n(An) = 0.

(7)

This theorem with Lemma 2 of [8] establishes the converse of Theorem 7. The proof is based on
bounding the second moment of the Radon-Nikodym derivative of Pσ1 with respect to P0 (SM
Section 4.9).

5 Summary of Asymptotic Results

√
Table 1 summarizes asymptotic bounds for the case of ∆ = 1/
k and m = Θ(n). For the MLE we
consider the coherent model of Section 4.1. Also in Table 1 we summarize computational complexity
of different tensor biclustering methods. We discuss analytical and empirical running time of these
methods in SM Section 2.2.

√
Table 1: Comparative analysis of tensor biclustering methods. Results have been simpliﬁed for the
case of m = Θ(n) and ∆ = 1/

k.

Methods

1  noise model I
σ2

1  noise model II Comp. Complexity
σ2

Tensor Folding+Spectral
Tensor Unfolding+Spectral

Th. Sum of Squared Trajectory Lengths

Th. Individual Trajectory Lengths

Maximum Likelihood
Statistical Lower Bound

˜Ω(n3/2)
˜Ω(n2)
˜Ω(nk2)
˜Ω(nk2)
˜Ω(k)
˜O(k)

˜Ω(n3/2)
˜Ω(n2)
˜Ω(nk)
˜Ω(k2√
˜Ω(k)
˜O(k)

n)

6

O(n4)
O(n3)
O(n3)
O(n3)
exp(n)

-

Figure 3: Performance of different tensor biclustering methods in various values of σ1 (i.e.  the signal
strength)  under both noise models I and II. We consider n = 200  m = 50  k = 40. Experiments
have been repeated 10 times for each point.

√

In both noise models  the maximum likelihood estimator which has an exponential computational
complexity leads to the best achievability bound compared to other methods. Below this bound  the
inference is statistically impossible. Tensor folding+spectral method outperforms other methods
with polynomial computational complexity if k >
n under noise model I  and k > n1/4 under
noise model II. For smaller values of k  thresholding individual trajectory lengths lead to a better
achievability bound. This case is a part of the high-SNR regime where the average trajectory lengths
within the bicluster is signiﬁcantly larger than the one outside of the bicluster. Unlike thresholding
individual trajectory lengths  other methods use the entire tensor to solve the tensor biclustering
problem. Thus  when k is very small  the accumulated noise can dominate the signal strength.
Moreover  the performance of the tensor unfolding method is always worst than the one of the tensor
folding method. The reason is that  the tensor unfolding method merely infers a low dimensional
subspace of trajectories  ignoring the block structure that true low dimensional trajectories form.

6 Numerical Results

6.1 Synthetic Data

In this section we evaluate the performance of different tensor biclustering methods in synthetic
datasets. We use the statistical model described in Section 4.1 to generate the input tensor T . Let
( ˆJ1  ˆJ2) be estimated bicluster indices (J1  J2) where | ˆJ1| = | ˆJ2| = k. To evaluate the inference
quality we compute the fraction of correctly recovered bicluster indices (SM Section 3.1).
In our simulations we consider n = 200  m = 50  k = 40. Figure 3 shows the performance of
four tensor biclustering methods in different values of σ1 (i.e.  the signal strength)  under both noise
models I and II. Tensor folding+spectral algorithm outperforms other methods in both noise models.
The gain is larger in the setup of noise model II compared to the one of noise model I.

6.2 Real Data

In this section we apply tensor biclustering methods to the roadmap epigenomics dataset [4] which
provides histon mark signal strengths in different segments of human genome in various tissues and
cell types. In this dataset  ﬁnding a subset of genome segments and a subset of tissues (cell-types)
with highly correlated histon mark values can provide insight on tissue-speciﬁc functional roles
of genome segments [4]. After pre-processing the data (SM Section 3.2)  we obtain a data tensor
T ∈ Rn1×n2×m where n1 = 49 is the number of tissues (cell-types)  n2 = 1457 is the number of

7

(b)(a)noise model IInoise model ISignal StrengthSignal StrengthBicluster Recovery RateBicluster Recovery RateTh. individual trajectory lengthsTh. sum of squared trajectory lengthsTensor unfolding+SpectralTensor folding+Spectral0501001502002503003504000.10.20.30.40.50.60.70.80.9105010015020025030035040000.20.40.60.81Figure 4: An application of tensor biclustering methods to the the roadmap epigenomics data.

genome segments  and m = 7 is the number of histon marks. Note that although in our analytical
results for simplicity we assume n1 = n2  our proposed methods can be used in a more general case
such as the one considered in this section.
We form two combined covariance matrices C1 ∈ Rn1×n1 and C2 ∈ Rn2×n2 according to (3).
Figure 4-(a b) shows largest eigenvalues of C1 and C2  respectively. As illustrated in these ﬁgures 
spectral gaps (i.e.  λ1 − λ2) of these matrices are large  indicating the existence of a low dimensional
signal tensor in the input tensor. We also form an unfolded tensor Tunf olded ∈ Rm×n1n2. Similarly 
there is a large gap between the ﬁrst and second largest singular values of Tunf olded (Figure 4-c).
We use the tensor folding+spectral algorithm 1 with |J1| = 10 and |J2| = 400 (we consider other
values for the bicluster size in SM Section 3.2). The output of the algorithm ( ˆJ1  ˆJ2) is illustrated in
Figure 4-d (note that for visualization purposes  we re-order rows and columns to have the bicluster
appear in the corner). Figure 4-e illustrates the unfolded subspace {T (j1  j2  :) : (j1  j2) ∈ ˆJ1 × ˆJ2}.
In this inferred bicluster  Histon marks H3K4me3  H3K9ac  and H3K27ac have relatively high values.
Reference [4] shows that these histon marks indicate a promoter region with an increased activation
in the genome.
To evaluate the quality of the inferred bicluster  we compute total absolute pairwise correlations
among vectors in the inferred bicluster. As illustrated in Figure 4-f  the quality of inferred bicluster
by tensor folding+spectral algorithm is larger than the one of other methods. Next  we compute the
bicluster quality by choosing bicluster indices uniformly at random with the same cardinality. We
repeat this experiment 100 times. There is a signiﬁcant gap between the quality of these random
biclusters and the ones inferred by tensor biclustering methods indicating the signiﬁcance of our
inferred biclusters. For more details on these experiment  see SM Section 3.2.

7 Discussion

In this paper  we introduced and analyzed the tensor biclustering problem. The goal is to compute a
subset of tensor rows and columns whose corresponding trajectories form a low dimensional subspace.
To solve this problem  we proposed a method called tensor folding+spectral which demonstrated
improved analytical and empirical performance compared to other considered methods. Moreover  we
characterized computational and statistical (information theoretic) limits for the tensor biclustering
problem in an asymptotic regime  under both coherent and non-coherent statistical models.
Our results consider the case when the rank of the subspace is equal to one (i.e.  q = 1). When q > 1 
in both tensor folding+spectral and tensor unfolding+spectral methods  the embedded subspace in
the signal matrix will have a rank of q > 1  with singular values σ1 ≥ σ2 ≥ ... ≥ σq > 0. In this

8

12345670123456i-th largest singular valuei-th largest eigenvaluei-th largest eigenvalue10612345670123456106123456705001000150020002500iii(c)(f)(e)(b)(d)(a)2004006008001000120014005101520253035404500.10.20.30.40.50.60.70.80.91tissuesgenome segments(chromosome 20)1000200030004000051015202530tissues x genome segmentsH3K27me3H3K27acH3K36me3H3K4me1H3K4me3H3K9acH3K9me30.450.50.550.60.650.70.750.80.85inferred bicluster qualityrandomTensorunfoldingTensorfoldingTh. individualTLTh. sum ofsquared TL setup  we need the spectral radius of the noise matrix to be smaller than σq in order to guarantee
the recovery of the subspace. The procedure to characterize asymptotic achievability bounds would
follow from similar steps of the rank one case with some technical differences. For example  we
would need to extend Lemma 6 to the case where the signal matrix has rank q > 1. Moreover  in our
problem setup  we assumed that the size of the bicluster k and the rank of its subspace q are know
parameters. In practice  these parameters can be learned approximately from the data. For example 
in the tensor folding+spectral method  a good choice for the q parameter would be the index where
eigenvalues of the folded matrix decrease signiﬁcantly. Knowing q  one can determine the size of
the bicluster similarly as the number of indices in top eigenvectors with signiﬁcantly larger absolute
values. Another practical approach to estimate model parameters would be trial and error plus cross
validations.
Some of the developed proof techniques may be of independent interest as well. For example  we
proved an l∞ version of the Davis-Kahan lemma for a Wishart noise matrix. Solving the tensor
biclustering problem for the case of having multiple overlapping biclusters  for the case of having
incomplete tensor  and for the case of a priori unknown bicluster sizes are among future directions.

8 Code

We provide code for tensor biclustering methods in the following link: https://github.com/
SoheilFeizi/Tensor-Biclustering.

9 Acknowledgment

We thank Prof. Ofer Zeitouni for the helpful discussion on detectably proof techniques of probability
measures.

References
[1] Amos Tanay  Roded Sharan  and Ron Shamir. Biclustering algorithms: A survey. Handbook of

computational molecular biology  9(1-20):122–124  2005.

[2] Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and
submatrix localization with a growing number of clusters and submatrices. arXiv preprint
arXiv:1402.1267  2014.

[3] T Tony Cai  Tengyuan Liang  and Alexander Rakhlin. Computational and statistical boundaries

for submatrix localization in a large noisy matrix. arXiv preprint arXiv:1502.01988  2015.

[4] Anshul Kundaje  Wouter Meuleman  Jason Ernst  Misha Bilenky  Angela Yen  Alireza Heravi-
Moussavi  Pouya Kheradpour  Zhizhuo Zhang  Jianrong Wang  Michael J Ziller  et al. Integrative
analysis of 111 reference human epigenomes. Nature  518(7539):317–330  2015.

[5] GTEx Consortium et al. The genotype-tissue expression (gtex) pilot analysis: Multitissue gene

regulation in humans. Science  348(6235):648–660  2015.

[6] Rui Chen  George I Mias  Jennifer Li-Pook-Than  Lihua Jiang  Hugo YK Lam  Rong Chen 
Elana Miriami  Konrad J Karczewski  Manoj Hariharan  Frederick E Dewey  et al. Personal
omics proﬁling reveals dynamic molecular and medical phenotypes. Cell  148(6):1293–1307 
2012.

[7] Emile Richard and Andrea Montanari. A statistical model for tensor pca. In Advances in Neural

Information Processing Systems  pages 2897–2905  2014.

[8] Andrea Montanari  Daniel Reichman  and Ofer Zeitouni. On the limitation of spectral methods:
From the gaussian hidden clique problem to rank-one perturbations of gaussian tensors. In
Advances in Neural Information Processing Systems  pages 217–225  2015.

[9] Samuel B Hopkins  Tselil Schramm  Jonathan Shi  and David Steurer. Fast spectral algorithms
from sum-of-squares proofs: tensor decomposition and planted sparse vectors. arXiv preprint
arXiv:1512.02337  2015.

9

[10] Samuel B Hopkins  Jonathan Shi  and David Steurer. Tensor principal component analysis via

sum-of-square proofs. In COLT  pages 956–1006  2015.

[11] Amelia Perry  Alexander S Wein  and Afonso S Bandeira. Statistical limits of spiked tensor

models. arXiv preprint arXiv:1612.07728  2016.

[12] Thibault Lesieur  Léo Miolane  Marc Lelarge  Florent Krzakala  and Lenka Zdeborová. Sta-
tistical and computational phase transitions in spiked tensor estimation. arXiv preprint
arXiv:1701.08010  2017.

[13] Animashree Anandkumar  Rong Ge  and Majid Janzamin. Guaranteed non-orthogonal tensor

decomposition via alternating rank-1 updates. arXiv preprint arXiv:1402.5180  2014.

[14] Anru Zhang and Dong Xia. Guaranteed tensor pca with optimality in statistics and computation.

arXiv preprint arXiv:1703.02724  2017.

[15] Animashree Anandkumar  Rong Ge  Daniel J Hsu  and Sham M Kakade. A tensor approach
to learning mixed membership community models. Journal of Machine Learning Research 
15(1):2239–2312  2014.

[16] Animashree Anandkumar  Rong Ge  Daniel J Hsu  Sham M Kakade  and Matus Telgarsky.
Tensor decompositions for learning latent variable models. Journal of Machine Learning
Research  15(1):2773–2832  2014.

[17] Victoria Hore  Ana Viñuela  Alfonso Buil  Julian Knight  Mark I McCarthy  Kerrin Small  and
Jonathan Marchini. Tensor decomposition for multiple-tissue gene expression experiments.
Nature Genetics  48(9):1094–1100  2016.

[18] Yiqiao Zhong and Nicolas Boumal. Near-optimal bounds for phase synchronization. arXiv

preprint arXiv:1703.06605  2017.

10

,Aaron Defazio
Francis Bach
Simon Lacoste-Julien
Siddhartha Banerjee
Peter Lofgren
Soheil Feizi
Hamid Javadi
David Tse
Jingwei Xu
Bingbing Ni
Xiaokang Yang