2011,See the Tree Through the Lines: The Shazoo Algorithm,Predicting the nodes of a given graph is a fascinating   theoretical problem with applications in several domains.   Since graph sparsification via spanning trees   retains enough information while making the task much easier    trees are an important special case of this problem.   Although it is known how to predict the nodes of an unweighted tree   in a nearly optimal way  in the weighted case a fully satisfactory   algorithm is not available yet. We fill this hole and introduce an efficient node predictor    Shazoo  which is nearly optimal on any weighted tree. Moreover  we show that Shazoo can   be viewed as a common nontrivial generalization of both previous approaches for   unweighted trees and weighted lines.   Experiments on real-world datasets confirm that Shazoo performs well in that   it fully exploits the structure of the input tree    and gets very close to (and sometimes better than)   less scalable energy minimization methods.,See the Tree Through the Lines:

The Shazoo Algorithm∗

Fabio Vitale

DSI  University of Milan  Italy
fabio.vitale@unimi.it

Nicol`o Cesa-Bianchi

DSI  University of Milan  Italy

nicolo.cesa-bianchi@unimi.it

Claudio Gentile

DICOM  University of Insubria  Italy

claudio.gentile@uninsubria.it

Giovanni Zappella

Dept. of Mathematics  Univ. of Milan  Italy

giovanni.zappella@unimi.it

Abstract

Predicting the nodes of a given graph is a fascinating theoretical problem with ap-
plications in several domains. Since graph sparsiﬁcation via spanning trees retains
enough information while making the task much easier  trees are an important
special case of this problem. Although it is known how to predict the nodes of an
unweighted tree in a nearly optimal way  in the weighted case a fully satisfactory
algorithm is not available yet. We ﬁll this hole and introduce an efﬁcient node
predictor  SHAZOO  which is nearly optimal on any weighted tree. Moreover  we
show that SHAZOO can be viewed as a common nontrivial generalization of both
previous approaches for unweighted trees and weighted lines. Experiments on
real-world datasets conﬁrm that SHAZOO performs well in that it fully exploits
the structure of the input tree  and gets very close to (and sometimes better than)
less scalable energy minimization methods.

Introduction

1
Predictive analysis of networked data is a fast-growing research area whose application domains
include document networks  online social networks  and biological networks. In this work we view
networked data as weighted graphs  and focus on the task of node classiﬁcation in the transductive
setting  i.e.  when the unlabeled graph is available beforehand. Standard transductive classiﬁcation
methods  such as label propagation [2  3  18]  work by optimizing a cost or energy function deﬁned
on the graph  which includes the training information as labels assigned to training nodes. Although
these methods perform well in practice  they are often computationally expensive  and have perfor-
mance guarantees that require statistical assumptions on the selection of the training nodes.
A general approach to sidestep the above computational issues is to sparsify the graph to the largest
possible extent  while retaining much of its spectral properties —see  e.g.  [5  6  12  16]. Inspired
by [5  6]  this paper reduces the problem of node classiﬁcation from graphs to trees by extracting
suitable spanning trees of the graph  which can be done quickly in many cases. The advantage
of performing this reduction is that node prediction is much easier on trees than on graphs. This
fact has recently led to the design of very scalable algorithms with nearly optimal performance
guarantees in the online transductive model  which comes with no statistical assumptions. Yet  the
current results in node classiﬁcation on trees are not satisfactory. The TREEOPT strategy of [5] is
optimal to within constant factors  but only on unweighted trees. No equivalent optimality results
are available for general weighted trees. To the best of our knowledge  the only other comparable
result is WTA by [6]  which is optimal (within log factors) only on weighted lines. In fact  WTA can
still be applied to weighted trees by exploiting an idea contained in [9]. This is based on linearizing
the tree via a depth-ﬁrst visit. Since linearization loses most of the structural information of the tree 
∗This work was supported in part by Google Inc. through a Google Research Award  and by the PASCAL2

Network of Excellence under EC grant 216886. This publication only reﬂects the authors’ views.

1

this approach yields suboptimal mistake bounds. This theoretical drawback is also conﬁrmed by
empirical performance: throwing away the tree structure negatively affects the practical behavior of
the algorithm on real-world weighted graphs.
The importance of weighted graphs  as opposed to unweighted ones  is suggested by many practical
scenarios where the nodes carry more information than just labels  e.g.  vectors of feature values. A
natural way of leveraging this side information is to set the weight on the edge linking two nodes to
be some function of the similariy between the vectors associated with these nodes. In this work  we
bridge the gap between the weighted and unweighted cases by proposing a new prediction strategy 
called SHAZOO  achieving a mistake bound that depends on the detailed structure of the weighted
tree. We carry out the analysis using a notion of learning bias different from the one used in [6] and
more appropriate for weighted graphs. More precisely  we measure the regularity of the unknown
node labeling via the weighted cutsize induced by the labeling on the tree (see Section 3 for a precise
deﬁnition). This replaces the unweighted cutsize that was used in the analysis of WTA. When the
weighted cutsize is used  a cut edge violates this inductive bias in proportion to its weight. This
modiﬁed bias does not prevent a fair comparison between the old algorithms and the new one:
SHAZOO specializes to TREEOPT in the unweighted case  and to WTA when the input tree is a
weighted line. By specializing SHAZOO’s analysis to the unweighted case we recover TREEOPT’s
optimal mistake bound. When the input tree is a weighted line  we recover WTA’s mistake bound
expressed through the weighted cutsize instead of the unweighted one. The effectiveness of SHAZOO
on any tree is guaranteed by a corresponding lower bound (see Section 3).
SHAZOO can be viewed as a common nontrivial generalization of both TREEOPT and WTA. Obtain-
ing this generalization while retaining and extending the optimality properties of the two algorithms
is far from being trivial from a conceptual and technical standpoint. Since SHAZOO works in the
online transductive model  it can easily be applied to the more standard train/test (or “batch”) trans-
ductive setting: one simply runs the algorithm on an arbitrary permutation of the training nodes  and
obtains a predictive model for all test nodes. However  the implementation might take advantage
of knowing the set of training nodes beforehand. For this reason  we present two implementations
of SHAZOO  one for the online and one for the batch setting. Both implementations result in fast
algorithms. In particular  the batch one is linear in |V |. This is achieved by a fast algorithm for
weighted cut minimization on trees  a procedure which lies at the heart of SHAZOO.
Finally  we test SHAZOO against WTA  label propagation  and other competitors on real-world
weighted graphs. In almost all cases (as expected)  we report improvements over WTA due to the
better sensitivity to the graph structure. In some cases  we see that SHAZOO even outperforms stan-
dard label propagation methods. Recall that label propagation has a running time per prediction
which is proportional to |E|  where E is the graph edge set. On the contrary  SHAZOO can typically
be run in constant amortized time per prediction by using Wilson’s algorithm for sampling random
spanning trees [17]. By disregarding edge weights in the initial sampling phase  this algorithm is
able to draw a random (unweighted) spanning tree in time proportional to |V | on most graphs. Our
experiments reveal that using the edge weights only in the subsequent prediction phase causes in
practice only a minor performance degradation.
2 Preliminaries and basic notation
Let T = (V  E  W ) be an undirected and weighted tree with |V | = n nodes  positive edge weights
Wi j > 0 for (i  j) ∈ E  and Wi j = 0 for (i  j) /∈ E. A binary labeling of T is any assignment
y = (y1  . . .   yn) ∈ {−1  +1}n of binary labels to its nodes. We use (T  y) to denote the resulting
labeled weighted tree. The online learning protocol for predicting (T  y) is deﬁned as follows. The
learner is given T while y is kept hidden. The nodes of T are presented to the learner one by one 
according to an unknown and arbitrary permutation i1  . . .   in of V . At each time step t = 1  . . .   n

node it is presented and the learner must issue a prediction(cid:98)yit ∈ {−1  +1} for the label yit. Then
irregularity in a labeled tree (T  y) is the weighted cutsize ΦW =(cid:80)

yit is revealed and the learner knows whether a mistake occurred. The learner’s goal is to minimize
the total number of prediction mistakes.
Following previous works [10  9  5  6]  we measure the regularity of a labeling y of T in terms of
φ-edges  where a φ-edge for (T  y) is any (i  j) ∈ E such that yi (cid:54)= yj. The overall amount of
(i j)∈Eφ Wi j  where Eφ ⊆ E
is the subset of φ-edges in the tree. We use the weighted cutsize as our learning bias  that is  we
want to design algorithms whose predictive performance scales with ΦW . Unlike the φ-edge count
Φ = |Eφ|  which is a good measure of regularity for unweighted graphs  the weighted cutsize takes

2

(r s)∈π(i j)

Wr s

the resistance distance metric d  that is  d(i  j) = (cid:80)

the edge weight Wi j into account when measuring the irregularity of a φ-edge (i  j). In the sequel 
when we measure the distance between any pair of nodes i and j on the input tree T we always use
  where π(i  j) is the unique

1

path connecting i to j.
3 A lower bound for weighted trees
In this section we show that the weighted cutsize can be used as a lower bound on the number of
online mistakes made by any algorithm on any tree. In order to do so (and unlike previous papers
on this speciﬁc subject —see  e.g.  [6])  we need to introduce a more reﬁned notion of adversarial
“budget”. Given T = (V  E  W )  let ξ(M) be the maximum number of edges of T such that the
(i j)∈E(cid:48) wi j ≤ M
sum of their weights does not exceed M  ξ(M) = max
.
We have the following simple lower bound (all proofs are omitted from this extended abstract).

(cid:110)|E(cid:48)| : E(cid:48) ⊆ E  (cid:80)

(cid:111)

Theorem 1 For any weighted tree T = (V  E  W ) there exists a randomized label assignment to
V such that any algorithm can be forced to make at least ξ(M)/2 online mistakes in expectation 
while ΦW ≤ M.
Specializing [6  Theorem 1] to trees gives the lower bound K/2 under the constraint Φ ≤ K ≤ |V |.
The main difference between the two bounds is the measure of label regularity being used: Whereas
Theorem 1 uses ΦW   which depends on the weights  [6  Theorem 1] uses the weight-independent
quantity Φ. This dependence of the lower bound on the edge weights is consistent with our learning
bias  stating that a heavy φ-edge violates the bias more than a light one. Since ξ is nondecreasing 
the lower bound implies a number of mistakes of at least ξ(ΦW )/2. Note that ξ(ΦW ) ≥ Φ for any
labeled tree (T  y). Hence  whereas a constraint K on Φ implies forcing at least K/2 mistakes  a
constraint M on ΦW allows the adversary to force a potentially larger number of mistakes.
In the next section we describe an algorithm whose mistake bound nearly matches the above lower
bound on any weighted tree when using ξ(ΦW ) as the measure of label regularity.

4 The Shazoo algorithm
In this section we introduce the SHAZOO algorithm  and relate it to previously proposed methods
for online prediction on unweighted trees (TREEOPT from [5]) and weighted line graphs (WTA from
[6]). In fact  SHAZOO is optimal on any weighted tree  and reduces to TREEOPT on unweighted trees
and to WTA on weighted line graphs. Since TREEOPT and WTA are optimal on any unweighted tree
and any weighted line graph  respectively  SHAZOO necessarily contains elements of both of these
algorithms.
In order to understand our algorithm  we now deﬁne some relevant structures of the input tree T . See
Figure 1 (left) for an example. These structures evolve over time according to the set of observed
labels. First  we call revealed a node whose label has already been observed by the online learner;
otherwise  a node is unrevealed. A fork is any unrevealed node connected to at least three different
revealed nodes by edge-disjoint paths. A hinge node is either a revealed node or a fork. A hinge
tree is any component of the forest obtained by removing from T all edges incident to hinge nodes;
hence any fork or labeled node forms a 1-node hinge tree. When a hinge tree H contains only one
hinge node  a connection node for H is the node contained in H. In all other cases  we call a
connection node for H any node outside H which is adjacent to a node in H. A connection fork is
a connection node which is also a fork. Finally  a hinge line is any path connecting two hinge nodes
such that no internal node is a hinge node.
Given an unrevealed node i and a label value y ∈ {−1  +1}  the cut function cut(i  y) is the value
of the minimum weighted cutsize of T over all labelings y ∈ {−1  +1}n consistent with the labels
seen so far and such that yi = y. Deﬁne ∆(i) = cut(i −1) − cut(i  +1) if i is unrevealed  and
∆(i) = yi  otherwise. The algorithm’s pseudocode is given in Algorithm 1. At time t  in order
to predict the label yit of node it  SHAZOO calculates ∆(i) for all connection nodes i of H(it) 
where H(it) is the hinge tree containing it. Then the algorithm predicts yit using the label of the
connection node i of H(it) which is closest to it and such that ∆(i) (cid:54)= 0 (recall from Section 2
that all distances/lengths are measured using the resistance metric). Ties are broken arbitrarily. If
∆(i) = 0 for all connection nodes i in H(it) then SHAZOO predicts a default value (−1 in the

3

Figure 1: Left: An input tree. Revealed nodes are dark grey  forks are doubly circled  and hinge
lines have thick black edges. The hinge trees not containing hinge nodes (i.e.  the ones that are not
singletons) are enclosed by dotted lines. The dotted arrows point to the connection node(s) of such
hinge trees. Middle: The predictions of SHAZOO on the nodes of a hinge tree. The numbers on the
edges denote edge weights. At a given time t  SHAZOO uses the value of ∆ on the two hinge nodes
(the doubly circled ones  which are also forks in this case)  and is required to issue a prediction on
node it (the black node in this ﬁgure). Since it is between a positive ∆ hinge node and a negative
∆ hinge node  SHAZOO goes with the one which is closer in resistance distance  hence predicting

(cid:98)yit = −1. Right: A simple example where the mincut prediction strategy does not work well in the

weighted case. In this example  mincut mispredicts all labels  yet Φ = 1  and the ratio of ΦW to the
total weight of all edges is about 1/|V |. The labels to be predicted are presented according to the
numbers on the left of each node. Edge weights are also displayed  where a is a very small constant.
pseudocode). If it is a fork (which is also a hinge node)  then H(it) = {it}. In this case  it is
a connection node of H(it)  and obviously the one closest to itself. Hence  in this case SHAZOO

predicts yt simply by(cid:98)yit = sgn(cid:0)∆(it)(cid:1). See Figure 1 (middle) for an example. On unweighted
Let C(cid:0)H(it)(cid:1) be the set of the connection nodes i of H(it) for which ∆(i) (cid:54)= 0
if C(cid:0)H(it)(cid:1) (cid:54)≡ ∅
Let j be the node of C(cid:0)H(it)(cid:1) closest to it
Set(cid:98)yit = sgn(cid:0)∆(j)(cid:1)
else Set(cid:98)yit = −1 (default value)

Algorithm 1: SHAZOO
for t = 1 . . . n

trees  computing ∆(i) for a connection node i reduces to the Fork Label Estimation Procedure in
[5  Lemma 13]. On the other hand  predicting with the label of the connection node closest to it
in resistance distance is reminiscent of the nearest-neighbor prediction of WTA on weighted line
graphs [6]. In fact  as in WTA  this enables to take advantage of labelings whose φ-edges are light
weighted. An important limitation of WTA is that this algorithm linearizes the input tree. On the
one hand  this greatly simpliﬁes the analysis of nearest-neighbor prediction; on the other hand  this
prevents exploiting the structure of T   thereby causing logaritmic slacks in the upper bound of WTA.
The TREEOPT algorithm  instead  performs better when the unweighted input tree is very different
from a line graph (more precisely  when the input tree cannot be decomposed into long edge-disjoint
paths  e.g.  a star graph). Indeed  TREEOPT’s upper bound does not suffer from logaritmic slacks 
and is tight up to constant factors on any unweighted tree. Similar to TREEOPT  SHAZOO does
not linearize the input tree and extends to the weighted case TREEOPT’s superior performance  also
conﬁrmed by the experimental comparison reported in Section 6.
In Figure 1 (right) we show an example that highlights the importance of using the ∆ function to
compute the fork labels. Since ∆ predicts a fork it with the label that minimizes the weighted cutsize
of T consistent with the revealed labels  one may wonder whether computing ∆ through mincut
based on the number of φ-edges (rather than their weighted sum) could be an effective prediction
strategy. Figure 1 (right) illustrates an example of a simple tree where such a ∆ mispredicts the
labels of all nodes  when both ΦW and Φ are small.
Remark 1 We would like to stress that SHAZOO can also be used to predict the nodes of an arbi-
trary graph by ﬁrst drawing a random spanning tree T of the graph  and then predicting optimally
on T —see  e.g.  [5  6]. The resulting mistake bound is simply the expected value of SHAZOO’s
mistake bound over the random draw of T . By using a fast spanning tree sampler [17]  the involved
computational overhead amounts to constant amortized time per node prediction on “most” graphs.

4

121324211121>0>0<0+++++2436151+a1+2a1+(V-1)a1+3aRemark 2 In certain real-world input graphs  the presence of an edge linking two nodes may also
carry information about the extent to which the two nodes are dissimilar  rather than similar. This
information can be encoded by the sign of the weight  and the resulting network is called a signed
graph. The regularity measure is naturally extended to signed graphs by counting the weight of
frustrated edges (e.g. [7])  where (i  j) is frustrated if yiyj (cid:54)= sgn(wi j). Many of the existing
algorithms for node classiﬁcation [18  9  10  5  8  6] can in principle be run on signed graphs.
However  the computational cost may not always be preserved. For example  mincut [4] is in general
NP-hard when the graph is signed [13]. Since our algorithm sparsiﬁes the graph using trees  it can
be run efﬁciently even in the signed case. We just need to re-deﬁne the ∆ function as ∆(i) =
fcut(i −1) − fcut(i  +1)  where fcut is the minimum total weight of frustrated edges consistent
with the labels seen so far. The argument contained in Section 5 for the positive edge weights (see 
e.g.  Eq. (1) therein) allows us to show that also this version of ∆ can be computed efﬁciently. The
prediction rule has to be re-deﬁned as well: We count the parity of the number z of negative-weighted

edges along the path connecting it to the closest node j ∈ C(cid:0)H(it)(cid:1)  i.e. (cid:98)yit = (−1)zsgn(cid:0)∆(j)(cid:1).

Remark 3 In [5] the authors note that TREEOPT approximates a version space (Halving) algo-
rithm on the set of tree labelings. Interestingly  SHAZOO is also an approximation to a more general
Halving algorithm for weighted trees. This generalized Halving gives a weight to each labeling
consistent with the labels seen so far and with the sign of ∆(f) for each fork f. These weighted
labelings  which depend on the weights of the φ-edges generated by each labeling  are used for com-
puting the predictions. One can show (details omitted due to space limitations) that this generalized
Halving algorithm has a mistake bound within a constant factor of SHAZOO’s.

L =(cid:80)
(cid:88)

5 Mistake bound analysis and implementation
We now show that SHAZOO is nearly optimal on every weighted tree T . We obtain an upper bound
in terms of ΦW and the structure of T   nearly matching the lower bound of Theorem 1. We now
give some auxiliary notation that is strictly needed for stating the mistake bound.
Given a labeled tree (T  y)  a cluster is any maximal subtree whose nodes have the same label. An
in-cluster line graph is any line graph that is entirely contained in a single cluster. Finally  given a
line graph L  we set RW
  i.e.  the (resistance) distance between its terminal nodes.

1

disjoint in-cluster line graphs such that the number of mistakes made by SHAZOO is at most of the
order of

Theorem 2 For any labeled and weighted tree (T  y)  there exists a set LT of O(cid:0)ξ(ΦW )(cid:1) edge-
The above mistake bound depends on the tree structure through LT . The sum contains O(cid:0)ξ(ΦW )(cid:1)
by the same key quantity ξ(cid:0)ΦW(cid:1) occurring in the lower bound of Theorem 1. However  Theorem 2
Hence  the factor multiplying ξ(cid:0)ΦW(cid:1) may be of the order of log(cid:0)1 + ΦW RW
(cid:1). If  instead  T has
nodes  and the number of mistakes can never exceed O(cid:0)ξ(ΦW )(cid:1). This is a log factor improvement

also shows that SHAZOO can take advantage of trees that cannot be covered by long line graphs. For
example  if the input tree T is a weighted line graph  then it is likely to contain long in-cluster lines.

(cid:110)|L|  1 +(cid:4)log(cid:0)1 + ΦW RW

(cid:1)(cid:5)(cid:111)

constant diameter (e.g.  a star graph)  then the in-cluster lines can only contain a constant number of

min

L∈LT

terms  each one being at most logarithmic in the scale-free products ΦW RW

L . The bound is governed

(i j)∈L

Wi j

L

.

L

over WTA which  by its very nature  cannot exploit the structure of the tree it operates on.1
As for the implementation  we start by describing a method for calculating cut(v  y) for any unla-
beled node v and label value y. Let T v be the maximal subtree of T rooted at v  such that no internal
i (y) be the
node is revealed. For any node i of T v  let T v
i consistent with the revealed nodes and such that yi = y. Since
minimum weighted cutsize of T v

i be the subtree of T v rooted at i. Let Φv

1One might wonder whether an arbitrarily large gap between upper (Theorem 2) and lower (Theorem 1)
L . One way to get around this is to follow the
bounds exists due to the extra factors depending on ΦW RW
analysis of WTA in [6]. Speciﬁcally  we can adapt here the more general analysis from that paper (see Lemma
2 therein) that allows us to drop  for any integer K  the resistance contribution of K arbitrary non-φ edges of
the line graphs in LT (thereby reducing RW
L for any L containing any of these edges) at the cost of increasing
the mistake bound by K. The details will be given in the full version of this paper.

5

∆(v) = cut(v −1) − cut(v  +1) = Φv
i (y) can be recursively deﬁned as follows  where C v
see by induction that the quantity Φv
of all children of i in T v  and Yj ≡ {yj} if yj is revealed  and Yj ≡ {−1  +1}  otherwise:2

v(+1)  our goal is to compute Φv

v(−1) − Φv

v(y). It is easy to
i is the set

(cid:17)

(cid:16)

(cid:88)

j∈Cv

i



Φv

i (y) =

j (y(cid:48)) + I{y(cid:48) (cid:54)= y} wi j
Φv

min
y(cid:48)∈Yj

if i is an internal node of T v

(1)

0

otherwise.

i (y) for each node i  the values Φv

case running time is proportional to(cid:80)

v(y) can be computed through a simple depth-ﬁrst visit of T v. In all backtracking steps of
Now  Φv
this visit the algorithm uses (1) to compute Φv
j (y) for all children
j of i being calculated during the previous backtracking steps. The total running time is therefore
linear in the number of nodes of T v.
Next  we describe the basic implementation of SHAZOO for the on-line setting. A batch learning
implementation will be given at the end of this section. The online implementation is made up of
three steps.
1. Find the hinge nodes of subtree T it. Recall that a hinge-node is either a fork or a revealed
node. Observe that a fork is incident to at least three nodes lying on different hinge lines. Hence  in
this step we perform a depth-ﬁrst visit of T it  marking each node lying on a hinge line. In order to
accomplish this task  it sufﬁces to single out all forks marking each labeled node and  recursively 
each parent of a marked node of T it. At the end of this process we are able to single out the forks
by counting the number of edges (i  j) of each marked node i such that j has been marked  too. The
remaining hinge nodes are the leaves of T it whose labels have currently been revealed.
2. Compute sgn(∆(i)) for all connection forks of H(it). From the previous step we can easily
ﬁnd the connection node(s) of H(it). Then  we simply exploit the above-described technique for
computing the cut function  obtaining sgn(∆(i)) for all connection forks i of H(it).
3. Propagate the labels of the nodes of C(H(it)) (only if it is not a fork). We perform a visit of
H(it) starting from every node r ∈ C(H(it)). During these visits  we mark each node j of H(it)
with the label of r computed in the previous step  together with the length of π(r  j)  which is what
we need for predicting any label of H(it) at the current time step.
The overall running time is dominated by the ﬁrst step and the calculation of ∆(i). Hence the worst
t≤|V | |V (T it)|. This quantity can be quadratic in |V |  though
this is rarely encountered in practice if the node presentation order is not adversarial. For example 
it is easy to show that in a line graph  if the node presentation order is random  then the total time is
of the order of |V | log |V |. For a star graph the total time complexity is always linear in |V |  even
on adversarial orders.
In many real-world scenarios  one is interested in the more standard problem of predicting the labels
of a given subset of test nodes based on the available labels of another subset of training nodes.
Building on the above on-line implementation  we now derive an implementation of SHAZOO for
this train/test (or “batch learning”) setting. We ﬁrst show that computing |Φi
i(−1)| for
all unlabeled nodes i in T takes O(|V |) time. This allows us to compute sgn(∆(v)) for all forks v
in O(|V |) time  and then use the ﬁrst and the third steps of the on-line implementation. Overall  we
show that predicting all labels in the test set takes O(|V |) time.
Consider tree T i as rooted at i. Given any unlabeled node i  we perform a visit of T i starting at
i. During the backtracking steps of this visit we use (1) to calculate Φi
j(y) for each node j in T i
and label y ∈ {−1  +1}. Observe now that for any pair i  j of adjacent unlabeled nodes and any
label y ∈ {−1  +1}  once we have obtained Φi
i (y) in
constant time  as Φj
of j in T i are descendants of i  while the children of i in T i (but j) are descendants of j in T j.
SHAZOO computes Φi
i (y) for all child nodes j of i in T i 
and use this value for computing Φj
j(y). Generalizing this argument  it is easy to see that in the next
phase we can compute Φk
k(y) in constant time for all nodes k of T i such that for all ancestors u of
k and all y ∈ {−1  +1}  the values of Φu

i(y) − miny(cid:48)∈{−1 +1}(cid:0)Φi

(cid:1). In fact  all children

j(+1) and Φi
j(y(cid:48)) + I{y(cid:48) (cid:54)= y} wi j

i(y)  we can compute in constant time Φj

j(−1)  we can compute Φj

u(y) have previously been computed.

i(+1)| and |Φi

i (y) = Φi

i(y)  Φi

2The recursive computations contained in this section are reminiscent of the sum-product algorithm [11].

6

OMV predicts yit through the sign of(cid:80)

The time for computing Φs
s(y) for all nodes s of T i and any label y is therefore linear in the time
of performing a breadth-ﬁrst (or depth-ﬁrst) visit of T i  i.e.  linear in the number of nodes of T i.
Since each labeled node with degree d is part of at most d trees T i for some i  we have that the total
number of nodes of all distinct (edge-disjoint) trees T i across i ∈ V is linear in |V |.
Finally  we need to propagate the connection node labels of each hinge tree as in the third step of
the online implementation. Since also this last step takes linear time  we conclude that the total time
for predicting all labels is linear in |V |.
6 Experiments
We tested our algorithm on a number of real-world weighted graphs from different domains (char-
acter recognition  text categorization  bioinformatics  Web spam detection) against the following
baselines:
Online Majority Vote (OMV). This is an intuitive and fast algorithm for sequentially predicting the
node labels is via a weighted majority vote over the labels of the adjacent nodes seen so far. Namely 
s yiswis it  where s ranges over s < t such that (is  it) ∈ E.
Both the total time and space required by OMV are Θ(|E|).
Label Propagation (LABPROP). LABPROP [18  2  3] is a batch transductive learning method com-
puted by solving a system of linear equations which requires total time of the order of |E|×|V |. This
relatively high computational cost should be taken into account when comparing LABPROP to faster
online algorithms. Recall that OMV can be viewed as a fast “online approximation” to LABPROP.
Weighted Tree Algorithm (WTA). As explained in the introductory section  WTA can be viewed
as a special case of SHAZOO. When the input graph is not a line  WTA turns it into a line by ﬁrst
extracting a spanning tree of the graph  and then linearizing it. The implementation described in
[6] runs in constant amortized time per prediction whenever the spanning tree sampler runs in time
Θ(|V |).
The Graph Perceptron algorithm [10] is another readily available baseline. This algorithm has been
excluded from our comparison because it does not seem to be very competitive in terms of perfor-
mance (see  e.g.  [6])  and is also computationally expensive.
In our experiments  we combined SHAZOO and WTA with spanning trees generated in different ways
(note that OMV and LABPROP do not need to extract spanning trees from the input graph).
Random Spanning Tree (RST). Following Ch. 4 of [12]  we draw a weighted spanning tree with
probability proportional to the product of its edge weights. We also tested our algorithms combined
with random spanning trees generated uniformly at random ignoring the edge weights (i.e.  the
weights were only used to compute predictions on the randomly generated tree) —we call these
spanning trees NWRST (no-weight RST). On most graphs  this procedure can be run in time linear
in the number of nodes [17]. Hence  the combinations SHAZOO+NWRST and WTA+NWRST run in
O(|V |) time on most graphs.
Minimum Spanning Tree (MST). This is just the minimal weight spanning tree  where the weight
of a spanning tree is the sum of its edge weights. This is the tree that best approximates the original
graph i.t.o. trace norm distance of the corresponding Laplacian matrices.
Following [10  6]  we also ran SHAZOO and WTA using committees of spanning trees  and then
aggregating predictions via a majority vote. The resulting algorithms are denoted by k*SHAZOO
and k*WTA  where k is the number of spanning trees in the aggregation. We used either k = 7  11
or k = 3  7  depending on the dataset size.
For our experiments  we used ﬁve datasets: RCV1  USPS  KROGAN  COMBINED  and WEB-
SPAM. WEBSPAM is a big dataset (110 900 nodes and 1 836 136 edges) of inter-host links created
for the Web Spam Challenge 2008 [15].3 KROGAN (2 169 nodes and 6 102 edges) and COM-
BINED (2 871 nodes and 6 407 edges) are high-throughput protein-protein interaction networks of
budding yeast taken from [14] —see [6] for a more complete description. Finally  USPS and RCV1
are graphs obtained from the USPS handwritten characters dataset (all ten categories) and the ﬁrst
10 000 documents in chronological order of Reuters Corpus Vol. 1 (the four most frequent cate-
gories)  respectively. In both cases  we used Euclidean 10-Nearest Neighbor to create edges  each

3We do not compare our results to those obtained within the challenge since we are only exploiting the

graph (weighted) topology here  disregarding content features.

7

(cid:0)σ2

i j = 1

2

(cid:1)  where σ2

i + σ2

j

i j . We set σ2

i is the average

weight wi j being equal to e−(cid:107)xi−xj(cid:107)2/σ2
squared distance between i and its 10 nearest neighbours.
Following previous experimental settings [6]  we associate binary classiﬁcation tasks with the ﬁve
datasets/graphs via a standard one-vs-all reduction. Each error rate is obtained by averaging over ten
randomly chosen training sets (and ten different trees in the case of RST and NWRST). WEBSPAM
is natively a binary classiﬁcation problem  and we used the same train/test split provided with the
dataset: 3 897 training nodes and 1 993 test nodes (the remaining nodes being unlabeled).
In the below table  we show the macro-averaged classiﬁcation error rates (percentages) achieved by
the various algorithms on the ﬁrst four datasets mentioned in the main text. For each dataset we
trained ten times over a random subset of 5%  10% and 25% of the total number of nodes and tested
on the remaining ones. In boldface are the lowest error rates on each column  excluding LABPROP
which is used as a “yardstick” comparison. Standard deviations averaged over the binary problems
are small: most of the times less than 0.5%.

Datasets

Predictors
SHAZOO+RST
SHAZOO+NWRST
SHAZOO+MST
WTA+RST
WTA+NWRST
WTA+MST
7*SHAZOO+RST
7*SHAZOO+NWRST
7*WTA+RST
7*WTA+NWRST
11*SHAZOO+RST
11*SHAZOO+NWRST
11*WTA+RST
11*WTA+NWRST
OMV
LABPROP

5%
3.62
3.88
1.07
5.34
5.74
1.81
1.68
1.89
2.10
2.33
1.52
1.70
1.84
2.04
24.79
1.95

USPS
10%
2.82
3.03
0.96
4.23
4.45
1.60
1.28
1.38
1.56
1.73
1.17
1.27
1.36
1.51
12.34
1.11

25%
2.02
2.18
0.80
3.02
3.26
1.21
0.97
1.06
1.14
1.24
0.89
0.98
1.01
1.12
2.10
0.82

5%
21.72
21.97
17.71
25.53
25.50
21.07
16.33
16.49
17.44
17.69
15.82
15.95
16.40
16.70
31.65
16.28

RCV1
10%
18.70
19.21
14.87
22.66
22.70
17.94
13.52
13.98
14.74
15.18
13.04
13.42
13.95
14.28
22.35
12.99

25%
15.68
15.95
11.73
19.05
19.24
13.92
11.07
11.37
12.15
12.53
10.59
10.93
11.42
11.68
11.79
10.00

KROGAN

10%
17.68
18.14
16.92
21.05
21.28
20.63
15.58
15.62
16.64
16.60
15.40
15.33
16.15
16.05
38.75
14.98

5%
18.11
18.11
17.46
21.82
21.90
21.41
15.54
15.61
16.75
16.71
15.36
15.40
16.20
16.22
43.13
15.56

25%
17.10
17.32
16.30
20.08
20.18
19.61
15.46
15.50
15.88
16.00
15.29
15.32
15.53
15.50
29.84
15.23

5%
17.77
17.22
16.79
21.76
21.58
21.74
15.12
15.02
16.42
16.24
14.91
14.87
15.90
15.74
44.72
14.79

COMBINED

10%
17.24
17.21
16.64
21.38
21.42
21.20
15.24
15.12
16.09
16.13
15.06
14.99
15.58
15.57
40.86
14.93

25%
17.34
17.53
17.15
20.26
20.64
20.32
15.84
15.80
15.72
15.79
15.61
15.67
15.30
15.33
33.24
15.18

Next  we extract from the above table a speciﬁc comparison among SHAZOO  WTA  and LABPROP.
SHAZOO and WTA use a single minimum spanning tree (the best performing tree type for both
algorithms). Note that SHAZOO consistently outperforms WTA.

We then report the results on WEBSPAM. SHAZOO and WTA use only non-weighted random span-
ning trees (NWRST) to optimize scalability. Since this dataset is extremely unbalanced (5.4% positive
labels) we use the average test set F-measure instead of the error rate.

SHAZOO WTA
0.947

0.954

OMV
0.706

LABPROP

0.931

3*WTA
0.967

3*SHAZOO

0.964

7*WTA
0.968

7*SHAZOO

0.968

Our empirical results can be brieﬂy summarized as follows:
1. Without using committees  SHAZOO outperforms WTA on all datasets  irrespective to the type
of spanning tree being used. With committees  SHAZOO works better than WTA almost always 
although the gap between the two reduces.
2. The predictive performance of SHAZOO+MST is comparable to  and sometimes better than  that
of LABPROP  though the latter algorithm is slower.
3. k*SHAZOO  with k = 11 (or k = 7 on WEBSPAM) seems to be especially effective  outper-
forming LABPROP  with a small (e.g.  5%) training set size.
4. NWRST does not offer the same theoretical guarantees as RST  but it is extremely fast to generate
(linear in |V | on most graphs — e.g.  [1])  and in our experiments is only slightly inferior to RST.

8

References
[1] N. Alon  C. Avin  M. Kouck´y  G. Kozma  Z. Lotker  and M.R. Tuttle. Many random walks
are faster than one. In Proc. 20th Symp. on Parallel Algo. and Architectures  pages 119–128.
Springer  2008.

[2] M. Belkin  I. Matveeva  and P. Niyogi. Regularization and semi-supervised learning on large
graphs. In Proceedings of the 17th Annual Conference on Learning Theory  pages 624–638.
Springer  2004.

[3] Y. Bengio  O. Delalleau  and N. Le Roux. Label propagation and quadratic criterion. In Semi-

Supervised Learning  pages 193–216. MIT Press  2006.

[4] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In
Proceedings of the 18th International Conference on Machine Learning. Morgan Kaufmann 
2001.

[5] N. Cesa-Bianchi  C. Gentile  and F. Vitale. Fast and optimal prediction of a labeled tree. In

Proceedings of the 22nd Annual Conference on Learning Theory  2009.

[6] N. Cesa-Bianchi  C. Gentile  F. Vitale  and G. Zappella. Random spanning trees and the pre-
diction of weighted graphs. In Proceedings of the 27th International Conference on Machine
Learning  2010.

[7] C. Altaﬁni G. Iacono. Monotonicity  frustration  and ordered response: an analysis of the
energy landscape of perturbed large-scale biological networks. BMC Systems Biology  4(83) 
2010.

[8] M. Herbster and G. Lever. Predicting the labelling of a graph via minimum p-seminorm in-
terpolation. In Proceedings of the 22nd Annual Conference on Learning Theory. Omnipress 
2009.

[9] M. Herbster  G. Lever  and M. Pontil. Online prediction on large diameter graphs. In Advances

in Neural Information Processing Systems 22. MIT Press  2009.

[10] M. Herbster  M. Pontil  and S. Rojas-Galeano. Fast prediction on a tree. In Advances in Neural

Information Processing Systems 22. MIT Press  2009.

[11] F.R. Kschischang  B.J. Frey  and H.A. Loeliger. Factor graphs and the sum-product algorithm.

IEEE Transactions on Information Theory  47(2):498–519  2001.

[12] R. Lyons and Y. Peres. Probability on trees and networks. Manuscript  2008.
[13] S.T. McCormick  M.R. Rao  and G. Rinaldi. Easy and difﬁcult objective functions for max cut.

Math. Program.  94(2-3):459–466  2003.

[14] G. Pandey  M. Steinbach  R. Gupta  T. Garg  and V. Kumar. Association analysis-based trans-
formations for protein interaction networks: a function prediction case study. In Proceedings of
the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
pages 540–549. ACM Press  2007.

[15] Yahoo! Research (Barcelona) and Laboratory of Web Algorithmics (Univ. of Milan). Web

spam collection. URL: barcelona.research.yahoo.net/webspam/datasets/.

[16] D. A. Spielman and N. Srivastava. Graph sparsiﬁcation by effective resistances. In Proc. of

the 40th annual ACM symposium on Theory of computing (STOC 2008). ACM Press  2008.

[17] D.B. Wilson. Generating random spanning trees more quickly than the cover time. In Proceed-
ings of the 28th ACM Symposium on the Theory of Computing  pages 296–303. ACM Press 
1996.

[18] X. Zhu  Z. Ghahramani  and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and
harmonic functions. In Proceedings of the 20th International Conference on Machine Learn-
ing  2003.

9

,Xianjie Chen
Alan Yuille
Yuan Deng
Jon Schneider
Balasubramanian Sivan