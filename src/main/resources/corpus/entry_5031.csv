2014,Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning,Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance  thanks to their scalability. While various methods have been proposed to speed up their convergence  the model selection phase is often ignored. In fact  in theoretical works most of the time assumptions are made  for example  on the prior knowledge of the norm of the optimal solution  while in the practical world validation methods remain the only viable approach. In this paper  we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training  with no parameters to tune  nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings  to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.,Simultaneous Model Selection and Optimization

through Parameter-free Stochastic Learning

Francesco Orabona∗

Yahoo! Labs

New York  USA

francesco@orabona.com

Abstract

Stochastic gradient descent algorithms for training linear and kernel predictors
are gaining more and more importance  thanks to their scalability. While various
methods have been proposed to speed up their convergence  the model selection
phase is often ignored.
In fact  in theoretical works most of the time assump-
tions are made  for example  on the prior knowledge of the norm of the optimal
solution  while in the practical world validation methods remain the only viable
approach. In this paper  we propose a new kernel-based stochastic gradient de-
scent algorithm that performs model selection while training  with no parameters
to tune  nor any form of cross-validation. The algorithm builds on recent advance-
ment in online learning theory for unconstrained settings  to estimate over time
the right regularization in a data-dependent way. Optimal rates of convergence are
proved under standard smoothness assumptions on the target function as well as
preliminary empirical results.

1

Introduction

Stochastic Gradient Descent (SGD) algorithms are gaining more and more importance in the Ma-
chine Learning community as efﬁcient and scalable machine learning tools. There are two possible
ways to use a SGD algorithm: to optimize a batch objective function  e.g. [23]  or to directly opti-
mize the generalization performance of a learning algorithm  in a stochastic approximation way [20].
The second use is the one we will consider in this paper. It allows learning over streams of data 
coming Independent and Identically Distributed (IID) from a stochastic source. Moreover  it has
been advocated that SGD theoretically yields the best generalization performance in a given amount
of time compared to other more sophisticated optimization algorithms [6].
Yet  both in theory and in practice  the convergence rate of SGD for any ﬁnite training set critically
depends on the step sizes used during training. In fact  often theoretical analysis assumes the use
of optimal step sizes  rarely known in reality  and in practical applications wrong step sizes can
result in arbitrarily bad performance. While in ﬁnite dimensional hypothesis spaces simple optimal
strategies are known [2]  in inﬁnite dimensional spaces the only attempts to solve this problem
achieve convergence only in the realizable case  e.g. [25]  or assume prior knowledge of intrinsic
(and unknown) characteristic of the problem [24  29  31  33  34]. The only known practical and
theoretical way to achieve optimal rates in inﬁnite Reproducing Kernel Hilbert Space (RKHS) is
to use some form of cross-validation to select the step size that corresponds to a form of model
selection [26  Chapter 7.4]. However  cross-validation techniques would result in a slower training
procedure partially neglecting the advantage of the stochastic training. A notable exception is the
algorithm in [21]  that keeps the step size constant and the number of epochs on the training set acts
as a regularizer. Yet  the number of epochs is decided through the use of a validation set [21].

∗Work done mainly while at Toyota Technological Institute at Chicago.

1

Note that the situation is exactly the same in the batch setting where the regularization takes the role
of the step size. Even in this case  optimal rates can be achieved only when the regularization is
chosen in a problem dependent way [12  17  27  32].
On a parallel route  the Online Convex Optimization (OCO) literature studies the possibility to
learn in a scenario where the data are not IID [9  36]. It turns out that this setting is strictly more
difﬁcult than the IID one and OCO algorithms can also be used to solve the corresponding stochastic
problems [8]. The literature on OCO focuses on the adversarial nature of the problem and on various
ways to achieve adaptivity to its unknown characteristics [1  11  14  15].
This paper is in between these two different worlds: We extend tools from OCO to design a novel
stochastic parameter-free algorithm able to obtain optimal ﬁnite sample convergence bounds in inﬁ-
nite dimensional RKHS. This new algorithm  called Parameter-free STOchastic Learning (PiSTOL) 
has the same complexity as the plain stochastic gradient descent procedure and implicitly achieves
the model selection while training  with no parameters to tune nor the need for cross-validation. The
core idea is to change the step sizes over time in a data-dependent way. As far as we know  this is
the ﬁrst algorithm of this kind to have provable optimal convergence rates.
The rest of the paper is organized as follows. After introducing some basic notations (Sec. 2)  we
will explain the basic intuition of the proposed method (Sec. 3). Next  in Sec. 4 we will describe
the PiSTOL algorithm and its regret bounds in the adversarial setting and in Sec. 5 we will show its
convergence results in the stochastic setting. The detailed discussion of related work is deferred to
Sec. 6. Finally  we show some empirical results and draw the conclusions in Sec. 7.

2 Problem Setting and Deﬁnitions

|  ∀x  x(cid:48)

Let X ⊂ Rd a compact set and HK the RKHS associated to a Mercer kernel K : X × X → R
implementing the inner product (cid:104)·   ·(cid:105)K that satisﬁes the reproducing property  (cid:104)K(x ·)   f (·)(cid:105)K =
f (x). Without loss of generality  in the following we will always assume (cid:107)k(xt ·)(cid:107)K ≤ 1.
Performance is measured w.r.t. a loss function (cid:96) : R → R+. We will consider L-Lipschitz losses 
that is |(cid:96)(x) − (cid:96)(x(cid:48))| ≤ L|x − x(cid:48)
∈ R  and H-smooth losses  that is differentiable losses
with the ﬁrst derivative H-Lipschitz. Note that a loss can be both Lipschitz and smooth. A vector x
is a subgradient of a convex function (cid:96) at v if (cid:96)(u) − (cid:96)(v) ≥ (cid:104)u − v  x(cid:105) for any u in the domain of
(cid:96). The differential set of (cid:96) at v  denoted by ∂(cid:96)(v)  is the set of all the subgradients of (cid:96) at v. 1(Φ)
will denote the indicator function of a Boolean predicate Φ.
In the OCO framework  at each round t the algorithm receives a vector xt ∈ X   picks a ft ∈ HK 
and pays (cid:96)t(ft(xt))  where (cid:96)t is a loss function. The aim of the algorithm is to minimize the
t=1 (cid:96)t(ft(xt))  and the

regret  that is the difference between the cumulative loss of the algorithm (cid:80)T
cumulative loss of an arbitrary and ﬁxed competitor h ∈ HK (cid:80)T

t=1 (cid:96)t(h(xt)).

ρ(x) := arg mint∈R(cid:82)

t=1 will consist of samples drawn IID from ρ. Denote by fρ(x) :=(cid:82)
(cid:113)(cid:82)
ρX . Deﬁne the (cid:96)-risk of f  as E (cid:96)(f ) := (cid:82)

For the statistical setting  let ρ a ﬁxed but unknown distribution on X × Y  where Y = [−1  1]. A
training set {xt  yt}T
Y ydρ(y|x)
the regression function  where ρ(·|x) is the conditional probability measure at x induced by ρ.
Denote by ρX the marginal probability measure on X and let L2
ρX be the space of square in-
tegrable functions with respect to ρX   whose norm is denoted by (cid:107)f(cid:107)L2
X f 2(x)dρX .
X×Y (cid:96)(yf (x))dρ. Also  deﬁne
Note that fρ ∈ L2
ρX E (cid:96)(f ). In
f (cid:96)
the binary classiﬁcation case  deﬁne the misclassiﬁcation risk of f as R(f ) := P (y (cid:54)= sign(f (x))).
The inﬁmum of the misclassiﬁcation risk over all measurable f will be called Bayes risk and
fc := sign(fρ)  called the Bayes classiﬁer  is such that R(fc) = inf f∈L2
ρX R(f ).
X K(x  x(cid:48))f (x(cid:48))dρX (x(cid:48)).
Let LK : L2
ρX consisting of eigenfunctions of LK with
There exists an orthonormal basis {Φ1  Φ2 ···} of L2
corresponding non-negative eigenvalues {λ1  λ2 ···} and the set {λi} is ﬁnite or λk → 0 when
k → ∞ [13  Theorem 4.7]. Since K is a Mercer kernel  LK is compact and positive. Therefore 
the fractional power operator Lβ
K is well deﬁned for any β ≥ 0. We indicate its range space by

ρX → HK the integral operator deﬁned by (LKf )(x) = (cid:82)

Y (cid:96)(yt)dρ(y|x)  that gives the optimal (cid:96)-risk  E (cid:96)(f (cid:96)

:=

ρX

ρ) = inf f∈L2

2

Algorithm 1 Averaged SGD.

Algorithm 2 The Kernel Perceptron.

Parameters: η > 0
Initialize: f1 = 0 ∈ HK
for t = 1  2  . . . do
Receive input vector xt ∈ X
Predict with ˆyt = ft(xt)
Update ft+1 = ft + ηyt(cid:96)(cid:48)(yt ˆyt)k(xt ·)
end for
Return ¯fT = 1
T

(cid:80)T

t=1 ft

Lβ
K(L2

ρX ) :=

Parameters: None
Initialize: f1 = 0 ∈ HK
for t = 1  2  . . . do
Receive input vector xt ∈ X
Predict with ˆyt = sign(ft(xt))
Suffer loss 1(ˆyt (cid:54)= yt)
Update ft+1 = ft + yt1(ˆyt (cid:54)= yt)k(xt ·)
(cid:27)

end for

(cid:26)

(cid:88)

f =

aiΦi

:

−2β
a2
i λ
i

< ∞

.

(1)

∞(cid:88)

i=1

i:ai(cid:54)=0

1
2

ρX ) = HK  that
By the Mercer’s theorem  we have that L
K(L2
is every function f ∈ HK can be written as L
Kg for some g ∈
ρX   with (cid:107)f(cid:107)K = (cid:107)g(cid:107)L2
. On the other hand  by deﬁnition
L2
ρX
ρX . Thus  the smaller
of the orthonormal basis  L0
K(L2
β is  the bigger this space of the functions will be 1 see Fig. 1.
This space has a key role in our analysis. In particular  we will
assume that f (cid:96)

ρX ) for β > 0  that is

ρX ) = L2

1
2

ρX : f (cid:96)

ρ = Lβ

Kg.

(2)

ρ ∈ Lβ

K(L2
∃g ∈ L2

Figure 1: L2
spaces  with 0 < β1 < 1

ρX   HK  and Lβ
2 < β2.

K (L2

ρX )

3 A Gentle Start: ASGD  Optimal Step Sizes  and the Perceptron

Consider the square loss  (cid:96)(x) = (1 − x)2. We want to investigate the problem of training a pre-
dictor  ¯fT   on the training set {xt  yt}T
t=1 in a stochastic way  using each sample only once  to have
E (cid:96)( ¯fT ) converge to E (cid:96)(f (cid:96)
ρ). The Averaged Stochastic Gradient Descent (ASGD) in Algorithm 1 has
been proposed as a fast stochastic algorithm to train predictors [35]. ASGD simply goes over all
the samples once  updates the predictor with the gradients of the losses  and returns the averaged
solution. For ASGD with constant step size 0 < η ≤ 1

4  it is immediate to show2 that

E[E (cid:96)( ¯fT )] ≤ inf

h∈HK E (cid:96)(h) + (cid:107)h(cid:107)2

K (ηT )

−1 + 4η.

(3)

ρ) = inf h∈HK E (cid:96)(h) but there is no guarantee that the inﬁmum is attained [26].

This result shows the link between step size and regularization: In expectation  the (cid:96)-risk of the
averaged predictor will be close to the (cid:96)-risk of the best regularized function in HK. Moreover 
the amount of regularization depends on the step size used. From (3)  one might be tempted to
choose η = O(T − 1
2 ). With this choice  when the number of samples goes to inﬁnity  ASGD would
converge to the performance of the best predictor in HK at a rate of O(T − 1
2 )  only if the inﬁmum
inf h∈HK E (cid:96)(h) is attained by a function in HK. Note that even with a universal kernel we only have
E (cid:96)(f (cid:96)
On the other hand  there is a vast  and often ignored  literature examining the general case when (2)
holds [4  7  12  17  24  27  29  31–34]. Under this assumption  this inﬁmum is attained only when
2  yet it is possible to prove convergence for β > 0. In fact  when (2) holds it is known that
β ≥ 1
(cid:17)
ρ) = O((ηT )−2β) [13  Proposition 8.5]. Hence  it was
minh∈HK
observed in [33] that setting η = O(T
 
1The case that β < 1 implicitly assumes that HK is inﬁnite dimensional. If HK has ﬁnite dimension  β is

− E (cid:96)(f (cid:96)
− 2β
2β+1 ) in (3)  we obtain E[E (cid:96)( ¯fT )]−E (cid:96)(f (cid:96)

K (ηT )−1(cid:105)

E (cid:96)(h) + (cid:107)h(cid:107)2

ρ) = O

− 2β

(cid:16)

(cid:104)

2β+1

T

0 or 1. See also the discussion in [27].

2The proofs of this statement and of all other presented results are in [19] .

3

T(cid:88)

t=1

(cid:118)(cid:117)(cid:117)(cid:116) T(cid:88)

t=1

that is the optimal rate [27  33]. Hence  the setting η = O(T − 1
2  that is
ρ ∈ HK. In all the other cases  the convergence rate of ASGD to the optimal (cid:96)-risk is suboptimal.
f (cid:96)
Unfortunately  β is typically unknown to the learner.
On the other hand  using the tools to design self-tuning algorithms  e.g. [1  14]  it may be possible
to design an ASGD-like algorithm  able to self-tune its step size in a data-dependent way. Indeed 
we would like an algorithm able to select the optimal step size in (3)  that is

2 ) is optimal only when β = 1

E[E (cid:96)( ¯fT )] ≤ inf

h∈HK E (cid:96)(h) + min

η>0 (cid:107)h(cid:107)2

K (ηT )

−1 + 4η = inf

h∈HK E (cid:96)(h) + 4(cid:107)h(cid:107)K T

− 1
2 .

(4)

2 ). An algorithm
In the OCO setting  this would correspond to a regret bound of the form O((cid:107)h(cid:107)K T
that has this kind of guarantee is the Perceptron algorithm [22]  see Algorithm 2. In fact  for the
Perceptron it is possible to prove the following mistake bound [9]:

1

Number of Mistakes ≤ inf
h∈HK

(cid:96)h(yth(xt)) + (cid:107)h(cid:107)2

K + (cid:107)h(cid:107)K

(cid:96)h(yth(xt)) 

(5)

where (cid:96)h is the hinge loss  (cid:96)h(x) = max(1 − x  0). The Perceptron algorithm is similar to SGD
but its behavior is independent of the step size  hence  it can be thought as always using the optimal
one. Unfortunately  we are not done yet: While (5) has the right form of the bound  it is not a regret
bound  rather only a mistake bound  speciﬁc for binary classiﬁcation. In fact  the performance of the
competitor h is measured with a different loss (hinge loss) than the performance of the algorithm
2 cannot be proved. In-
(misclassiﬁcation loss). For this asymmetry  the convergence when β < 1
2 )  returns the averaged
stead  we need an online algorithm whose regret bound scales as O((cid:107)h(cid:107)K T
solution  and  thanks to the equality in (4)  obtains a convergence rate which would depend on

1

min

η>0 (cid:107)h(cid:107)2

K (ηT )

−1 + η.

(6)

Note that (6) has the same form of the expression in (3)  but with a minimum over η. Hence  we can
expect such algorithm to always have the optimal rate of convergence. In the next section  we will
present an algorithm that has this guarantee.

4 PiSTOL: Parameter-free STOchastic Learning

In this section we describe the PiSTOL algorithm. The pseudo-code is in Algorithm 3. The algo-
rithm builds on recent advancement in unconstrained online learning [16  18  28]. It is very similar
to a SGD algorithm [35]  the main difference being the computation of the solution based on the
past gradients  in line 4. Note that the calculation of (cid:107)gt(cid:107)2
K can be done incrementally  hence  the
computational complexity is the same as ASGD in a RKHS  Algorithm 1  that is O(d) in Rd and
O(t) in a RKHS. For the PiSTOL algorithm we have the following regret bound.
Theorem 1. Assume that the losses (cid:96)t are convex and L-Lipschitz. Let a > 0 such that a ≥ 2.25L.
Then  for any h ∈ HK  the following bound on the regret holds for the PiSTOL algorithm

T(cid:88)

t=1

[(cid:96)t(ft(xt)) − (cid:96)t(h(xt))] ≤(cid:107)h(cid:107)K

(cid:18) exp( x

where φ(x) := x

2 exp

2 )(x+1)+2
2 )−x

1−x exp( x

(cid:33)

(cid:107)h(cid:107)K

√aLT
b

+ 1

log

(cid:32)

(cid:118)(cid:117)(cid:117)(cid:116)2a
(cid:33)
(cid:32)
T−1(cid:88)
−1L(cid:1) log (1 + T )  
+ bφ(cid:0)a
t=1 |st|
(cid:19)(cid:0)exp(cid:0) x
(cid:1) (x + 1) + 2(cid:1).

L +

2

This theorem shows that PiSTOL has the right dependency on (cid:107)h(cid:107)K and T that was outlined in
Sec. 3 and its regret bound is also optimal up to √log log T terms [18]. Moreover  Theorem 1
improves on the results in [16  18]  obtaining an almost optimal regret that depends on the sum of
the absolute values of the gradients  rather than on the time T . This is critical to obtain a tighter
bound when the losses are H-smooth  as shown in the next Corollary.

4

(cid:17)

Algorithm 3 PiSTOL: Parameter-free STOchastic Learning.
1: Parameters: a  b  L > 0
(cid:16)(cid:107)gt−1(cid:107)2
2: Initialize: g0 = 0 ∈ HK  α0 = aL
3: for t = 1  2  . . . do
Set ft = gt−1
b
4:
Receive input vector xt ∈ X
5:
Adversarial setting: Suffer loss (cid:96)t(ft(xt))
6:
Receive subgradient st ∈ partial(cid:96)t(ft(xt))
7:
Update gt = gt−1 − stk(xt ·) and αt = αt−1 + a|st|(cid:107)k(xt ·)(cid:107)K
8:
9: end for
10: Statistical setting: Return ¯fT = 1
T

t=1 ft

2αt−1

exp

αt−1

K

(cid:80)T
max

(cid:107)h(cid:107)

T(cid:88)

t=1

Corollary 1. Under the same assumptions of Theorem 1  if the losses (cid:96)t are also H-smooth  then3

4
 .
(cid:33) 1

(cid:32) T(cid:88)
(cid:17)

t=1

[(cid:96)t(ft(xt)) − (cid:96)t(h(xt))] = ˜O

4
3

K T

1

3  (cid:107)h(cid:107)K T

1
4

(cid:96)t(h(xt)) + 1

This bound shows that  if the cumulative loss of the competitor is small  the regret can grow slower
than √T . It is worse than the regret bounds for smooth losses in [9  25] because when the cumulative
loss of the competitor is equal to 0  the regret still grows as ˜O
instead of being constant.
However  the PiSTOL algorithm does not require the prior knowledge of the norm of the competitor
function h  as all the ones in [9  25] do.
In [19]   we also show a variant of PiSTOL for linear kernels with almost optimal learning rate for
each coordinate. Contrary to other similar algorithms  e.g. [14]  it is a truly parameter-free one.

(cid:107)f(cid:107)

K T

1
3

4
3

(cid:16)

5 Convergence Results for PiSTOL

In this section we will use the online-to-batch conversion to study the (cid:96)-risk and the misclassiﬁcation
risk of the averaged solution of PiSTOL. We will also use the following deﬁnition: ρ has Tsybakov
noise exponent q ≥ 0 [30] iff there exist cq > 0 such that

PX ({x ∈ X : −s ≤ fρ(x) ≤ s}) ≤ cqsq 

∀s ∈ [0  1].

Setting α = q

q+1 ∈ [0  1]  and cα = cq + 1  condition (7) is equivalent [32  Lemma 6.1] to:

PX (sign(f (x)) (cid:54)= fc(x)) ≤ cα(R(f ) − R(fρ))α 

(8)
These conditions allow for faster rates in relating the expected excess misclassiﬁcation risk to the
expected (cid:96)-risk  as detailed in the following Lemma that is a special case of [3  Theorem 10].
Lemma 1. Let (cid:96) : R → R+ be a convex loss function  twice differentiable at 0  with (cid:96)(cid:48)(0) < 0 
(cid:96)(cid:48)(cid:48)(0) > 0  and with the smallest zero in 1. Assume condition (8) is veriﬁed. Then for the averaged
solution ¯fT returned by PiSTOL it holds

∀f ∈ L2
ρX .

(7)

E[R( ¯fT )] − R(fc) ≤

32

cα
C

(cid:16)

(cid:0)E[E (cid:96)( ¯fT )] − E (cid:96)(f (cid:96)

ρ)(cid:1)(cid:17) 1

2−α

(cid:26)

  C = min

(cid:48)
−(cid:96)

(0) 

((cid:96)(cid:48)(0))2
(cid:96)(cid:48)(cid:48)(0)

(cid:27)

.

The results in Sec. 4 give regret bounds over arbitrary sequences. We now assume to have a se-
t=1 IID from ρ. We want to train a predictor from this data  that
quence of training samples (xt  yt)T
minimizes the (cid:96)-risk. To obtain such predictor we employ a so-called online-to-batch conversion [8].
t=1 
For a convex loss (cid:96)  we just need to run an online algorithm over the sequence of data (xt  yt)T
using the losses (cid:96)t(x) = (cid:96)(ytx)  ∀t = 1 ···   T . The online algorithm will generate a sequence
of solutions ft and the online-to-batch conversion can be obtained with a simple averaging of all
the solutions  ¯fT = 1
t=1 ft  as for ASGD. The average regret bound of the online algorithm
T
becomes a convergence guarantee for the averaged solution [8]. Hence  for the averaged solution of
PiSTOL  we have the following Corollary that is immediate from Corollary 1 and the results in [8].

(cid:80)T

3For brevity  the ˜O notation hides polylogarithmic terms.

5

(cid:16)

(cid:110)

4(cid:111)(cid:17)
4(cid:0)TE (cid:96)(h) + 1(cid:1) 1

Corollary 2. Assume that the samples (xt  yt)T
the assumptions of Corollary 1  the averaged solution of PiSTOL satisﬁes

t=1 are IID from ρ  and (cid:96)t(x) = (cid:96)(ytx). Then  under

.

− 3

4
3

K T

max

(cid:107)h(cid:107)

− 2
3  (cid:107)h(cid:107)K T

h∈HK E (cid:96)(h) + ˜O

E[E (cid:96)( ¯fT )] ≤ inf
Hence  we have a ˜O(T − 2
3 ) convergence rate to the φ-risk of the best predictor in HK  if the best
predictor has φ-risk equal to zero  and ˜O(T − 1
2 ) otherwise. Contrary to similar results in literature 
e.g. [25]  we do not have to restrict the inﬁmum over a ball of ﬁxed radius in HK and our bounds
depends on ˜O((cid:107)h(cid:107)K) rather than O((cid:107)h(cid:107)2
K)  e.g. [35]. The advantage of not restricting the competi-
tor in a ball is clear: The performance is always close to the best function in HK  regardless of its
norm. The logarithmic terms are exactly the price we pay for not knowing in advance the norm of
2(2−α) )
the optimal solution. For binary classiﬁcation using Lemma 1  we can also prove a ˜O(T
bound on the excess misclassiﬁcation risk in the realizable setting  that is if f (cid:96)
It would be possible to obtain similar results with other algorithms  as the one in [25]  using a
doubling-trick approach [9]. However  this would result most likely in an algorithm not useful in
any practical application. Moreover  the doubling-trick itself would not be trivial  for example the
one used in [28] achieves a suboptimal regret and requires to start from scratch the learning over two
different variables  further reducing its applicability in any real-world application.
As anticipated in Sec. 3  we now show that the dependency on ˜O((cid:107)h(cid:107)K) rather than on O((cid:107)h(cid:107)2
K)
ρX )  without the need
gives us the optimal rates of convergence in the general case that f (cid:96)
to tune any parameter. This is our main result.
Theorem 2. Assume that the samples (xt  yt)T
(cid:96)(ytx). Then  under the assumptions of Corollary 1  the averaged solution of PiSTOL satisﬁes
− 2β

t=1 are IID from ρ  (2) holds for β ≤ 1
(cid:16)

2   and (cid:96)t(x) =

ρ ∈ HK.

ρ ∈ Lβ

(cid:111)(cid:17)

K(L2

(cid:110)

− 1

− 2β

β

• If β ≤ 1
• If 1

3 then E[E (cid:96)( ¯fT )] − E (cid:96)(f (cid:96)
(cid:16)
3 < β ≤ 1
≤ ˜O
max

ρ) ≤ ˜O
(cid:110)
2   then E[E (cid:96)( ¯fT )] − E (cid:96)(f (cid:96)
ρ)
(E (cid:96)(f (cid:96)

ρ) + 1/T )

2β+1 T

β

max

(E (cid:96)(f (cid:96)

ρ) + 1/T )

2β+1 T

2β+1   T

− 2β
2β+1   (E (cid:96)(f (cid:96)

ρ) + 1/T )

3β−1
4β T

− 1

2   T

− 2β

β+1

.

β+1

(cid:111)(cid:17)

.

2β ).

− 2β

ρ) = 0  and ˜O(T
− β+1

− 3β
2β+1 )  if E (cid:96)(f (cid:96)
− 2β
β+1 ) for any T = O(E (cid:96)(f (cid:96)
ρ)

This theorem guarantees consistency w.r.t. the (cid:96)-risk. We
have that the rate of convergence to the optimal (cid:96)-risk
is ˜O(T
2β+1 ) other-
wise. However  for any ﬁnite T the rate of convergence
is ˜O(T
In other
words  we can expect a ﬁrst regime at faster convergence 
that saturates when the number of samples becomes big
enough  see Fig. 2. This is particularly important because
often in practical applications the features and the kernel
are chosen to have good performance  meaning low opti-
mal (cid:96)-risk. Using Lemma 1  we have that the excess mis-
(2β+1)(2−α) ) if E (cid:96)(f (cid:96)
classiﬁcation risk is ˜O(T
ρ) (cid:54)= 0  and
(β+1)(2−α) ) if E (cid:96)(f (cid:96)
˜O(T
ρ) = 0. It is also worth noting
that  being the algorithm designed to work in the adver-
sarial setting  we expect its performance to be robust to
small deviations from the IID scenario.

−

−

2β

2β

Figure 2: Upper bound on the excess
(cid:96)-risk of PiSTOL for β = 1
2 .

Also  note that the guarantees of Corollary 2 and Theorem 2 hold simultaneously. Hence  the theo-
retical performance of PiSTOL is always better than both the ones of SGD with the step sizes tuned
with the knowledge of β or with the agnostic choice η = O(T − 1
2 ). In [19]   we also show another
convergence result assuming a different smoothness condition.
Regarding the optimality of our results  lower bounds for the square loss are known [27] under
assumption (2) and further assuming that the eigenvalues of LK have a polynomial decay  that is

(9)

(λi)i∈N ∼ i

−b  b ≥ 1.

6

10110210310410510610710−310−210−1100101TBoundExcessℓ-riskbound Eℓ(fℓρ)=0Eℓ(fℓρ)=0.1Eℓ(fℓρ)=1Condition (9) can be interpreted as an effective dimension of the space. It always holds for b =
1 [27] and this is the condition we consider that is usually denoted as capacity independent  see
the discussion in [21  33]. In the capacity independent setting  the lower bound is O(T
2β+1 ) 
that matches the asymptotic rates in Theorem 2  up to logarithmic terms. Even if we require the
loss function to be Lipschitz and smooth  it is unlikely that different lower bounds can be proved
in our setting. Note that the lower bounds are worst case w.r.t. E (cid:96)(f (cid:96)
ρ)  hence they do not cover
ρ) = 0  where we get even better rates. Hence  the optimal regret bound of PiSTOL
the case E (cid:96)(f (cid:96)
in Theorem 1 translates to an optimal convergence rate for its averaged solution  up to logarithmic
terms  establishing a novel link between these two areas.

− 2β

6 Related Work

2 ≤ β ≤ 1. Note that  while in the range β ≥ 1

The approach of stochastically minimizing the (cid:96)-risk of the square loss in a RKHS has been pio-
neered by [24]. The rates were improved  but still suboptimal  in [34]  with a general approach for
locally Lipschitz loss functions in the origin. The optimal bounds  matching the ones we obtain for
ρ) (cid:54)= 0  were obtained for β > 0 in expectation by [33]. Their rates also hold for β > 1
2 
E (cid:96)(f (cid:96)
while our rates  as the ones in [27]  saturate at β = 1
2. In [29]  high probability bounds were proved
2  that implies fρ ∈ HK  it is
in the case that 1
2 considered in this
possible to prove high probability bounds [4  7  27  29]  the range 0 < β < 1
paper is very tricky  see the discussion in [27]. In this range no high probability bounds are known
without additional assumptions. All the previous approaches require the knowledge of β  while our
ρ) = 0.
algorithm is parameter-free. Also  we obtain faster rates for the excess (cid:96)-risk  when E (cid:96)(f (cid:96)
Another important difference is that we can use any smooth and Lipschitz loss  useful for example
to generate sparse solutions  while the optimal results in [29  33] are speciﬁc for the square loss.
For ﬁnite dimensional spaces and self-concordant losses  an optimal parameter-free stochastic algo-
rithm has been proposed in [2]. However  the convergence result seems speciﬁc to ﬁnite dimension.
The guarantees obtained from worst-case online algorithms  for example [25]  have typically opti-
mal convergence only w.r.t. the performance of the best in HK  see the discussion in [33]. Instead 
all the guarantees on the misclassiﬁcation loss w.r.t. a convex (cid:96)-risk of a competitor  e.g. the Per-
ceptron’s guarantee  are inherently weaker than the presented ones. To see why  assume that the
classiﬁer returned by the algorithm after seeing T samples is fT   these bounds are of the form of
R(fT ) ≤ E (cid:96)(h) +O(T − 1
K + 1)). For simplicity  assume the use of the hinge loss so that easy
ρ) = 2R(fc). Hence  even in the easy case that fc ∈ HK 
calculations show that f (cid:96)
we have R(fT ) ≤ 2R(fc) + O(T − 1
In the batch setting  the same optimal rates were obtained by [4  7] for the square loss  in high
probability  for β > 1
2. In [27]  using an additional assumption on the inﬁnity norm of the functions
2. The optimal tuning of the
in HK  they give high probability bounds also in the range 0 < β ≤ 1
regularization parameter is achieved by cross-validation. Hence  we match the optimal rates of a
batch algorithm  without the need to use validation methods.
In Sec. 3 we saw that the core idea to have the optimal rate was to have a classiﬁer whose per-
formance is close to the best regularized solution  where the regularizer is (cid:107)h(cid:107)K. Changing the
regularization term from the standard (cid:107)h(cid:107)2
K with q ≥ 1 is not new in the batch learning
literature. It has been ﬁrst proposed for classiﬁcation by [5]  and for regression by [17]. Note that  in
both cases no computational methods to solve the optimization problem were proposed. Moreover 
in [27] it was proved that all the regularizers of the form (cid:107)h(cid:107)q
K with q ≥ 1 gives optimal conver-
gence rates bound for the square loss  given an appropriate setting of the regularization weight. In
particular  [27  Corollary 6] proves that  using the square loss and under assumptions (2) and (9) 
the optimal weight for the regularizer (cid:107)h(cid:107)q
. This implies a very important conse-
quence  not mentioned in that paper: In the the capacity independent setting  that is b = 1  if we
use the regularizer (cid:107)h(cid:107)K  the optimal regularization weight is T − 1
2   independent of the exponent of
the range space (1) where fρ belongs. Moreover  in the same paper it was argued that “From an
algorithmic point of view however  q = 2 is currently the only feasible case  which in turn makes
SVMs the method of choice”. Indeed  in this paper we give a parameter-free efﬁcient procedure to

2 ((cid:107)h(cid:107)2
ρ = fc and E (cid:96)(f (cid:96)
2 ((cid:107)fc(cid:107)2

K + 1))  i.e. no convergence to the Bayes risk.

K to (cid:107)h(cid:107)q

− 2β+q(1−β)

2β+2/b

K is T

7

Figure 3: Average test errors and standard deviations of PiSTOL and SVM w.r.t.
samples over 5 random permutations  on a9a  SensIT Vehicle  and news20.binary.

the number of training

train predictors with smooth losses  that implicitly uses the (cid:107)h(cid:107)K regularizer. Thanks to this  the
regularization parameter does not need to be set using prior knowledge of the problem.

7 Discussion

Borrowing from OCO and statistical learning theory tools  we have presented the ﬁrst parameter-
free stochastic learning algorithm that achieves optimal rates of convergence w.r.t. the smoothness
of the optimal predictor. In particular  the algorithm does not require any validation method for the
model selection  rather it automatically self-tunes in an online and data-dependent way.
Even if this is mainly a theoretical work  we believe that it might also have a big potential in the
applied world. Hence  as a proof of concept on the potentiality of this method we have also run
a few preliminary experiments  to compare the performance of PiSTOL to an SVM using 5-folds
cross-validation to select the regularization weight parameter. The experiments were repeated with 5
random shufﬂes  showing the average and standard deviations over three datasets.4 The latest version
of LIBSVM was used to train the SVM [10]. We have that PiSTOL closely tracks the performance
of the tuned SVM when a Gaussian kernel is used. Also  contrary to the common intuition  the
stochastic approach of PiSTOL seems to have an advantage over the tuned SVM when the number
of samples is small. Probably  cross-validation is a poor approximation of the generalization perfor-
mance in that regime  while the small sample regime does not affect at all the analysis of PiSTOL.
Note that in the case of News20  a linear kernel is used over the vectors of size 1355192. The ﬁnite
dimensional case is not covered by our theorems  still we see that PiSTOL seems to converge at the
same rate of SVM  just with a worse constant. It is important to note that the total time the 5-folds
cross-validation plus the training with the selected parameter for the SVM on 58000 samples of
SensIT Vehicle takes ∼ 6.5 hours  while our unoptimized Matlab implementation of PiSTOL less
than 1 hour  ∼ 7 times faster. The gains in speed are similar on the other two datasets.
This is the ﬁrst work we know of in this line of research of stochastic adaptive algorithms for sta-
tistical learning  hence many questions are still open. In particular  it is not clear if high probability
bounds can be obtained  as the empirical results hint  without additional hypothesis. Also  we only
proved convergence w.r.t. the (cid:96)-risk  however for β ≥ 1
ρ ∈ HK  hence it would be
  e.g. [29]. Probably this would
require a major change in the proof techniques used. Finally  it is not clear if the regret bound in
Theorem 1 can be improved to depend on the squared gradients. This would result in a ˜O(T −1)
bound for the excess (cid:96)-risk for smooth losses when E (cid:96)(f (cid:96)
Acknowledgments

possible to prove the stronger convergence results on(cid:13)(cid:13)fT − f (cid:96)

2.
ρ) = 0 and β = 1

2 we know that f (cid:96)

(cid:13)(cid:13)K

ρ

I am thankful to Lorenzo Rosasco for introducing me to the beauty of the operator Lβ
Brendan McMahan for fruitful discussions.

K and to

4Datasets available at http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/.

The precise details to replicate the experiments are in [19] .

8

1011021031040.150.160.170.180.190.20.210.220.230.24Number of Training SamplesPercentage of Errors on the Test Seta9a  Gaussian Kernel SVM  5−folds CVPiSTOL  averaged solution1021031040.10.110.120.130.140.150.160.170.180.190.2Number of Training SamplesPercentage of Errors on the Test SetSensIT Vehicle  Gaussian Kernel SVM  5−folds CVPiSTOL  averaged solution10210310400.10.20.30.40.50.60.7Number of Training SamplesPercentage of Errors on the Test Setnews20.binary  Linear Kernel SVM  5−folds CVPiSTOL  averaged solutionReferences
[1] P. Auer  N. Cesa-Bianchi  and C. Gentile. Adaptive and self-conﬁdent on-line learning algorithms. J.

Comput. Syst. Sci.  64(1):48–75  2002.

O(1/n). In NIPS  pages 773–781  2013.

[2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate

[3] P. L. Bartlett  M. I. Jordan  and J. D. McAuliffe. Convexity  Classiﬁcation  and Risk Bounds. Journal of

the American Statistical Association  101(473):138–156  March 2006.

[4] F. Bauer  S. Pereverzev  and L. Rosasco. On regularization algorithms in learning theory. J. Complexity 

23(1):52–72  February 2007.

Statist.  36(2):489–531  04 2008.

[5] G. Blanchard  O. Bousquet  and P. Massart. Statistical performance of support vector machines. Ann.

[6] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Advances in Neural Information

Processing Systems  volume 20  pages 161–168. NIPS Foundation  2008.

[7] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of

[8] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning algorithms.

Computational Mathematics  7(3):331–368  2007.

IEEE Trans. Inf. Theory  50(9):2050–2057  2004.

[9] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press  2006.
[10] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines  2001. Software available at

http://www.csie.ntu.edu.tw/˜cjlin/libsvm.

[11] K. Chaudhuri  Y. Freund  and D. J. Hsu. A parameter-free hedging algorithm. In Advances in neural

information processing systems  pages 297–305  2009.

[12] D.-R. Chen  Q. Wu  Y. Ying  and D.-X. Zhou. Support vector machine soft margin classiﬁers: Error

analysis. Journal of Machine Learning Research  5:1143–1175  2004.

[13] F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge University

Press  New York  NY  USA  2007.

[14] J. C. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[15] H. Luo and R. E. Schapire. A drifting-games analysis for online learning and applications to boosting. In

Advances in Neural Information Processing Systems  2014.

[16] H. B. McMahan and F. Orabona. Unconstrained online linear learning in Hilbert spaces: Minimax algo-

rithms and normal approximations. In COLT  2014.

[17] S. Mendelson and J. Neeman. Regularization in kernel learning. Ann. Statist.  38(1):526–565  02 2010.
In Advances in Neural Information Processing
[18] F. Orabona. Dimension-free exponentiated gradient.

Systems 26  pages 1806–1814. Curran Associates  Inc.  2013.

[19] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning 

[20] H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Stat.  22:400–407  1951.
[21] L. Rosasco  A. Tacchetti  and S. Villa. Regularization by early stopping for online learning algorithms 

2014. arXiv:1406.3816.

2014. arXiv:1405.0042.

[22] F. Rosenblatt. The Perceptron: A probabilistic model for information storage and organization in the

[23] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM.

brain. Psychological Review  65:386–407  1958.

In Proc. of ICML  pages 807–814  2007.

[24] S. Smale and Y. Yao. Online learning algorithms. Found. Comp. Math  6:145–170  2005.
[25] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates. In Advances in Neural

Information Processing Systems 23  pages 2199–2207. Curran Associates  Inc.  2010.

[26] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.
[27] I. Steinwart  D. R. Hush  and C. Scovel. Optimal rates for regularized least squares regression. In COLT 

[28] M. Streeter and B. McMahan. No-regret algorithms for unconstrained online convex optimization. In
Advances in Neural Information Processing Systems 25  pages 2402–2410. Curran Associates  Inc.  2012.
[29] P. Tarr`es and Y. Yao. Online learning as stochastic approximation of regularization paths  2013.

[30] A. B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist.  32:135–166  2004.
[31] Y. Yao. On complexity issues of online learning algorithms. IEEE Trans. Inf. Theory  56(12):6470–6481 

[32] Y. Yao  L. Rosasco  and A. Caponnetto. On early stopping in gradient descent learning. Constr. Approx. 

2009.

arXiv:1103.5538.

2010.

26:289–315  2007.

[33] Y. Ying and M. Pontil. Online gradient descent learning algorithms. Foundations of Computational

[34] Y. Ying and D.-X. Zhou. Online regularized classiﬁcation algorithms.

IEEE Trans. Inf. Theory 

Mathematics  8(5):561–596  2008.

52(11):4775–4788  2006.

[35] T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In

Proc. of ICML  pages 919–926  New York  NY  USA  2004. ACM.

[36] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In Proc. of

ICML  pages 928–936  2003.

9

,Karthika Mohan
Judea Pearl
Jin Tian
Francesco Orabona