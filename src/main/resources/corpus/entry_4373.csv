2017,Predicting Scene Parsing and Motion Dynamics in the Future,It is important for intelligent systems  e.g. autonomous vehicles and robotics to anticipate the future in order to plan early and make decisions accordingly. Predicting the future scene parsing and motion dynamics helps the agents better understand the visual environment better as the former provides dense semantic segmentations  i.e. what objects will be present and where they will appear  while the latter provides dense motion information  i.e. how the objects move in the future. In this paper  we propose a novel model to predict the scene parsing and motion dynamics in unobserved future video frames simultaneously. Using history information (preceding frames and corresponding scene parsing results) as input  our model is able to predict the scene parsing and motion for arbitrary time steps ahead. More importantly  our model is superior compared to other methods that predict parsing and motion separately  as the complementary relationship between the two tasks are fully utilized in our model through joint learning. To our best knowledge  this is the first attempt in jointly predicting scene parsing and motion dynamics in the future frames. On the large-scale Cityscapes dataset  it is demonstrated that our model produces significantly better parsing and motion prediction results compared to well established baselines. In addition  we also show our model can be used to predict the steering angle of the vehicles  which further verifies the ability of our model to learn underlying latent parameters.,Predicting Scene Parsing and Motion Dynamics

in the Future

Xiaojie Jin1  Huaxin Xiao2  Xiaohui Shen3  Jimei Yang3  Zhe Lin3

Yunpeng Chen2  Zequn Jie4  Jiashi Feng2  Shuicheng Yan5 2

1NUS Graduate School for Integrative Science and Engineering (NGS)  NUS

2Department of ECE  NUS

3Adobe Research

4Tencent AI Lab

5Qihoo 360 AI Institute

Abstract

The ability of predicting the future is important for intelligent systems  e.g. au-
tonomous vehicles and robots to plan early and make decisions accordingly. Future
scene parsing and optical ﬂow estimation are two key tasks that help agents better
understand their environments as the former provides dense semantic information 
i.e. what objects will be present and where they will appear  while the latter pro-
vides dense motion information  i.e. how the objects will move. In this paper  we
propose a novel model to simultaneously predict scene parsing and optical ﬂow in
unobserved future video frames. To our best knowledge  this is the ﬁrst attempt in
jointly predicting scene parsing and motion dynamics. In particular  scene parsing
enables structured motion prediction by decomposing optical ﬂow into different
groups while optical ﬂow estimation brings reliable pixel-wise correspondence
to scene parsing. By exploiting this mutually beneﬁcial relationship  our model
shows signiﬁcantly better parsing and motion prediction results when compared
to well-established baselines and individual prediction models on the large-scale
Cityscapes dataset. In addition  we also demonstrate that our model can be used to
predict the steering angle of the vehicles  which further veriﬁes the ability of our
model to learn latent representations of scene dynamics.

1

Introduction

Future prediction is an important problem for artiﬁcial intelligence. To enable intelligent systems like
autonomous vehicles and robots to react to their environments  it is necessary to endow them with the
ability of predicting what will happen in the near future and plan accordingly  which still remains an
open challenge for modern artiﬁcial vision systems.
In a practical visual navigation system  scene parsing and dense motion estimation are two essential
components for understanding the scene environment. The former provides pixel-wise prediction
of semantic categories (thus the system understands what and where the objects are) and the latter
describes dense motion trajectories (thus the system learns how the objects move). The visual
system becomes “smarter” by leveraging the prediction of these two types of information  e.g.
predicting how the car coming from the opposite direction moves to plan the path ahead of time
and predict/control the steering angle of the vehicle. Despite numerous models have been proposed
on scene parsing [4  7  17  26  28  30  15] and motion estimation [2  9  21]  most of them focus on
processing observed images  rather than predicting in unobserved future scenes. Recently  a few
works [22  16  3] explore how to anticipate the scene parsing or motion dynamics  but they all tackle
these two tasks separately and fail to utilize the beneﬁts that one task brings to the other.
In this paper  we try to close this research gap by presenting a novel model for jointly predicting scene
parsing and motion dynamics (in terms of the dense optical ﬂow) for future frames. More importantly 
we leverage one task as the auxiliary of the other in a mutually boosting way. See Figure 1 for

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Our task. The proposed model jointly predicts scene parsing and optical ﬂow in the future. Top: Future
ﬂow (highlighted in red) anticipated using preceding frames. Bottom: Future scene parsing (highlighted in red)
anticipated using preceding scene parsing results. We use the ﬂow ﬁeld color coding from [2].

an illustration of our task. For the task of predictive scene parsing  we use the discriminative and
temporally consistent features learned in motion prediction to produce parsing prediction with more
ﬁne details. For the motion prediction task  we utilize the semantic segmentations produced by
predictive parsing to separately estimate motion for pixels with different categories. In order to
perform the results for multiple time steps  we take the predictions as input and iterate the model
to predict subsequent frames. The proposed model has a generic framework which is agnostic to
backbone deep networks and can be conveniently trained in an end-to-end manner.
Taking Cityscapes [5] as testbed  we conduct extensive experiments to verify the effectiveness of
our model in future prediction. Our model signiﬁcantly improves mIoU of parsing predictions and
reduces the endpoint error (EPE) of ﬂow predictions compared to strongly competitive baselines
including a warping method based on optical ﬂow  standalone parsing prediction or ﬂow prediction
and other state-of-the-arts methods [22]. We also present how to predict steering angles using the
proposed model.

2 Related work

For the general ﬁeld of classic ﬂow (motion) estimation and image semantic segmentation  which is
out of this paper’s scope  we refer the readers to comprehensive review articles [2  10]. Below we
mainly review existing works that focus on predictive tasks.

Flow and scene parsing prediction The research on predictive scene parsing or motion prediction
is still relatively under-explored. All existing works in this direction tackle the parsing prediction and
ﬂow prediction as independent tasks. With regards to motion prediction  Luo et al. [19] employed a
convolutional LSTM architecture to predict sequences of 3D optical ﬂow. Walker et al. [35] made
long-term motion and appearance prediction via a transition and context model. [31] trained CNN for
predicting motion of handwritten characters in a synthetic dataset. [36] predicted future optical ﬂow
given a static image. Different from above works  our model not only predicts the ﬂow but also scene
parsing at the same time  which deﬁnitely provides richer information to visual systems.
There are also only a handful number of works exploring the prediction of scene parsing in future
frames. Jin et al. [16] trained a deep model to predict the segmentations of the next frame from
preceding input frames  which is shown to be beneﬁcial for still-image parsing task. Based on the
network proposed in [20]  Natalia et al. [22] predicted longer-term parsing maps for future frames
using the preceding frames’ parsing maps. Different from [22]  we simultaneously predict optical
ﬂows for future frames. Beneﬁted from the discriminative local features learned from ﬂow prediction 
the model produces more accurate parsing results. Another related work to ours is [24] which
employed an RNN to predict the optical ﬂow and used the ﬂow to warp preceding segmentations.
Rather than simply producing the future parsing map through warping  our model predicts ﬂow and
scene parsing jointly using learning methods. More importantly  we leverage the beneﬁt that each
task brings to the other to produce better results for both ﬂow prediction and parsing prediction.

Predictive learning While there are few works speciﬁcally on predictive scene parsing or dense
motion prediction  learning to prediction in general has received a signiﬁcant attention from the

2

4tXInput3tXInput2tXInput1tXInputtOutputtSOutput1tSInput2tSInput3tSInput4tSInput……dtOutputdtSOutputFigure 2: The framework of our model for predicting future scene parsing and optical ﬂow for one time step
ahead. Our model is motivated by the assumption that ﬂow and parsing prediction are mutually beneﬁcial. We
design the architecture to promote such mutual beneﬁts. The model consists of two module networks  i.e. the
ﬂow anticipating network (blue) which takes preceding frames: Xt−4:t−1 as input and predicts future ﬂow and
the parsing anticipating network (yellow) which takes the preceding parsing results: St−4:t−1 as input and
predicts future scene parsing. By providing pixel-level class information (i.e. St−1)  the parsing anticipating
network beneﬁts the ﬂow anticipating network to enable the latter to semantically distinguish different pixels
(i.e. moving/static/other objects) and predict their ﬂows more accurately in the corresponding branch. Through
the transform layer  the discriminative local features learned by the ﬂow anticipating network are combined
with the parsing anticipating network to facilitate parsing over small objects and avoid over-smooth in parsing
predictions. When predicting multiple time-steps ahead  the prediction of the parsing network in a time-step is
used as the input in the next time-step.

research community in recent years. Research in this area has explored different aspects of this
problem. [37] focused on predicting the trajectory of objects given input image. [13] predicted
the action class in the future frames. Generative adversarial networks (GAN) are ﬁrstly introduced
in [11] to generate natural images from random noise  and have been widely used in many ﬁelds
including image synthesis [11]  future prediction [18  20  34  36  32  33] and semantic inpainting [23].
Different from above methods  our model explores a new predictive task  i.e. predicting the scene
parsing and motion dynamics in the future simultaneously.

Multi-task learning Multi-task learning [1  6] aims to solve multiple tasks jointly by taking
advantage of the shared domain knowledge in related tasks. Our work is partially related to multi-task
learning in that both the parsing results and motion dynamics are predicted jointly in a single model.
However  we note that predicting parsing and motion “in the future” is a novel and challenging task
which cannot be straightforwardly tackled by conventional multi-task learning methods. To our best
knowledge  our work is the ﬁrst solution to this challenging task.

3 Predicting scene parsing and motion dynamics in the future

In this section  we ﬁrst propose our model for predicting semantics and motion dynamics one time
step ahead  and then extend our model to perform predictions for multiple time steps.
Due to high cost of acquiring dense human annotations of optical ﬂow and scene parsing for
natural scene videos  only subset of frames are labeled for scene parsing in the current datasets.
Following [22]  to circumvent the need for datasets with dense annotations  we train an adapted
Res101 model (denoted as Res101-FCN  more details are given in Sec. 4.1) for scene parsing to
produce the target semantic segmentations for frames without human annotations. Similarly  to obtain
the dense ﬂow map for each frame  we use the output of the state-of-the-art epicﬂow [25] as our target
optical ﬂow. Note that our model is orthogonal to speciﬁc ﬂow methods since they are only used to
produce the target ﬂow for training the ﬂow anticipating network. Notations used in the following text
are as follows. Xi denotes the i-th frame of a video and Xt−k:t−1 denotes the sequence of frames
with length k from Xt−k to Xt−1. The semantic segmentation of Xt is denoted as St  which is the

3

4tX1tX…CNN12 Res. BlocksUp-samplingConvTransformLayer4tS1tS…CNN2OBJ-OTHflowLsegLOBJ-STAflowLOBJ-MOVflowLFlow Anticipating NetworkParsing Anticipating Networkoutput of the penultimate layer of Res101-FCN. St has the same spatial size as Xt and is a vector of
length C at each location  where C is the number of semantic classes. We denote Ot as the pixel-wise
optical ﬂow map from Xt−1 to Xt  which is estimated via epicﬂow [25]. Correspondingly  ˆSt and
ˆOt denote the predicted semantic segmentation and optical ﬂow.

3.1 Prediction for one time step ahead

Model overview The key idea of our approach is to model ﬂow prediction and parsing prediction
jointly  which are potentially mutually beneﬁcial. As illustrated in Figure 2  the proposed model
consists of two module networks that are trained jointly  i.e. the ﬂow anticipating network that takes
preceding frames Xt−k:t−1 as input to output the pixelwise ﬂow prediction for Ot (from Xt−1 to
Xt)  and the parsing anticipating network that takes the segmentation of preceding frames St−k:t−1
as input to output pixelwise semantic prediction for an unobserved frame Xt. The mutual inﬂuences
of each network on the other are exploited in two aspects. First  the last segmentations St−1 produced
by the parsing anticipating network convey pixel-wise class labels  which are used by the ﬂow
anticipating network to predict optical ﬂow values for each pixel according to its belonging object
group  e.g. moving objects or static objects. Second  the parsing anticipating network combines the
discriminative local feature learned by the ﬂow anticipating network to produce sharper and more
accurate parsing predictions.
Since both parsing prediction and ﬂow prediction are essentially both the dense classiﬁcation problem 
we use the same deep architecture (Res101-FCN) for predicting parsing results and optical ﬂow. Note
the Res101-FCN used in this paper can be replaced by any CNNs. We adjust the input/output layers
of these two networks according to the different channels of their input/output. The features extracted
by feature encoders (CNN1 and CNN2) are spatially enlarged via up-sampling layers and ﬁnally fed
to a convolutional layer to produce pixel-wise predictions which have the same spatial size as input.

Flow anticipating network In videos captured for autonomous driving or navigation  regions
with different class labels have different motion patterns. For example  the motion of static
objects like road is only caused by the motion of the camera while the motion of moving ob-
jects is a combination of motions from both the camera and objects themselves. Therefore
compared to methods that predict all pixels’ optical ﬂow in a single output layer  it would
largely reduce the difﬁculty of feature learning by separately modeling the motion of regions
with different classes. Following [29]  we assign each class into one of three pre-deﬁned object
groups  i.e. G = {moving objects (MOV-OBJ)  static objects (STA-OBJ)  other objects (OTH-OBJ)}
in which MOV-OBJ includes pedestrians  truck  etc.  STA-OBJ includes sky  road  etc.  and OTH-OBJ
includes vegetation and buildings  etc. which have diverse motion patterns and shapes. We append a
small network (consisting of two residual blocks) to the feature encoder (CNN1) for each object group
to learn speciﬁed motion representations. During training  the loss for each pixel is only generated at
the branch that corresponds to the object group to which the pixel belongs. Similarly  in testing  the
ﬂow prediction for each pixel is generated by the corresponding branch. The loss function between
the model output ˆOt and target output Ot is

(cid:88)

g∈G

Lﬂow( ˆOt  Ot) =

Lg
ﬂow; Lg

ﬂow =

1
|Ng|

(i j)∈Ng

(1)

(cid:88)

(cid:13)(cid:13)(cid:13)Oi j

t − ˆOi j

t

(cid:13)(cid:13)(cid:13)2

where (i  j) index the pixel in the region Ng.

Parsing anticipating network The input of the parsing anticipating network is a sequence of
preceding segmentations St−k:t−1. We also explore other input space alternatives  including preced-
ing frames Xt−k:t−1  and the combination of preceding frames and corresponding segmentations
Xt−k:t−1St−k:t−1  and we observe that the input St−k:t−1 achieves the best prediction performance.
We conjecture it is easier to learn the mapping between variables in the same domain (i.e. both
are semantic segmentations). However  there are two drawbacks brought by this strategy. Firstly 
St−k:t−1 lose the discriminative local features e.g. color  texture and shape etc.  leading to the missing
of small objects in predictions  as illustrated in Figure 3 (see yellow boxes). The ﬂow prediction
network may learn such features from the input frames. Secondly  due to the lack of local features
in St−k:t−1  it is difﬁcult to learn accurate pixel-wise correspondence in the parsing anticipating

4

network  which causes the predicted labeling maps to be over-smooth  as shown in Figure 3. The
ﬂow prediction network can provide reliable dense pixel-wise correspondence by regressing to the
target optical ﬂow. Therefore  we integrate the features learned by the ﬂow anticipating network with
the parsing prediction network through a transform layer (a shallow CNN) to improve the quality of
predicted labeling maps. Depending on whether human annotations are available  the loss function is
deﬁned as

Lseg( ˆS  S) =

(i j)∈Xt

L(cid:96)1 ( ˆS  S) + Lgdl( ˆS  S)  otherwise

log( ˆSi j

t (c))  Xt has human annotation 

(2)

where c is the ground truth class for the pixel at location (i  j). It is a conventional pixel-wise
cross-entropy loss when Xt has human annotations. L(cid:96)1 and Lgdl are (cid:96)1 loss and gradient difference
loss [20] which are deﬁned as

− (cid:80)
(cid:12)(cid:12)(cid:12) 

t − ˆSi j

t

(cid:12)(cid:12)(cid:12)Si j
(cid:16)(cid:12)(cid:12)(cid:12)|Si j

t − Si−1 j

t

L(cid:96)1 ( ˆS  S) =

Lgdl =

(cid:88)
(cid:88)

(i j)∈Xt

(i j)∈Xt

| − | ˆSi j

t − ˆSi−1 j

t

|(cid:12)(cid:12)(cid:12) +

(cid:12)(cid:12)(cid:12)|Si j−1

t

− Si j

t

| − | ˆSi j−1

t

− ˆSi j

t

|(cid:12)(cid:12)(cid:12)(cid:17)

.

The (cid:96)1 loss encourages predictions to regress to the target values while the gradient difference loss
produces large errors in the gradients of the target and predictions.
The reason for using different losses for human and non-human annotated frames in Eq. 2 is that
the automatically produced parsing ground-truth (by the pre-trained Res101-FCN) of the latter may
contain wrong annotations. The cross-entropy loss using one-hot vectors as labels is sensitive to the
wrong annotations. Comparatively  the ground-truth labels used in the combined loss (L(cid:96)1 + Lgdl) are
inputs of the softmax layer (ref. Sec. 3) which allow for non-zero values in more than one category 
thus our model can learn useful information from the correct category even if the annotation is wrong.
We ﬁnd replacing L(cid:96)1 + Lgdl with the cross-entropy loss reduces the mIoU of the baseline S2S (i.e.
the parsing participating network) by 1.5 from 66.1 when predicting the results one time-step ahead.
Now we proceed to explain the role of the transform layer which transforms the features of CNN1
before combining them with those of CNN2. Compared with naively combining the features from two
networks (e.g.  concatenation)  the transform layer brings the following two advantages: 1) naturally
normalize the feature maps to proper scales; 2) align the features of semantic meaning such that the
integrated features are more powerful for parsing prediction. Effectiveness of this transform layer is
clearly validated in the ablation study in Sec. 4.2.1.
The ﬁnal objective of our model is to minimize the combination of losses from the ﬂow anticipating
network and the parsing anticipating network as follows

L(Xt−k:t−1  St−k:t−1  ˆXt  ˆSt) = Lﬂow( ˆOt  Ot) + Lseg( ˆS  S).

3.2 Prediction for multiple time steps ahead

Based on the above model which predicts scene parsing and ﬂow for the single future time step  we
explore two ways to predict further into the future. Firstly  we iteratively apply the model to predict
one more time step into the future by treating the prediction as input in a recursive way. Speciﬁcally 
for predicting multiple time steps in the ﬂow anticipating network  we warp the most recent frame
Xt−1 using the output prediction ˆOt to get the ˆXt which is then combined with Xt−k−1:t−1 to feed
the ﬂow anticipating network to generate ˆOt+1  and so forth. For the parsing anticipating network  we
combine the predicted parsing map ˆSt with St−k−1:t−1 as the input to generate the parsing prediction
at t + 1. This scheme is easy to implement and allows us to predict arbitrarily far into the future
without increasing training complexity w.r.t. with the number of time-steps we want to predict.
Secondly  we ﬁne-tune our model by taking into account the inﬂuence that the recurrence has on
prediction for multiple time steps. We apply our model recurrently as described above to predict two
time steps ahead and apply the back propagation through time (BPTT) [14] to update the weight. We
have veriﬁed through experiments that the ﬁne-tuning approach can further improve the performance
as it models longer temporal dynamics during training.

5

Figure 3: Two examples of prediction results for predicting one time step ahead. Odd row: The images from
left to right are Xt−2  Xt−1  the target optical ﬂow map Ot  the ﬂow predictions from PredFlow and the ﬂow
predictions from our model. Even row: The images from left to right are St−2  St−1  the ground truth semantic
annotations at the time t  the parsing prediction from S2S and the parsing prediction from our model. The ﬂow
predictions from our model show clearer object boundaries and predict more accurate values for moving objects
(see black boxes) compared to PredFlow. Our model is superior to S2S by being more discriminative to the
small objects in parsing predictions (see yellow boxes).

Figure 4: An example of prediction results for predicting ten time steps ahead. Top (from left to right): Xt−11 
Xt−10  the target optical ﬂow map Ot  the ﬂow prediction from PredFlow and the ﬂow prediction from our
model. Bottom (from left to right): St−11  St−10  the ground truth semantic annotation at the time t  the parsing
prediction from S2S and the parsing prediction from our model. Our model outputs better prediction compared
to PredFlow (see black boxes) and S2S (see yellow boxes).

4 Experiment

4.1 Experimental settings

Datasets We verify our model on the large scale Cityscapes [5] dataset which contains 2 975/500
train/val video sequences with 19 semantic classes. Each video sequence lasts for 1.8s and contains
30 frames  among which the 20th frame has ﬁne human annotations. Every frame in Cityscapes has a
resolution of 1 024 × 2 048 pixels.

Evaluation criteria We use the mean IoU (mIoU) for evaluating the performance of predicted
parsing results on those 500 frames in the val set with human annotations. For evaluating the
performance of ﬂow prediction  we use the average endpoint error (EPE) [2] following conventions [8]
which is deﬁned as 1
N
and v are the components of optical ﬂow along x and y directions  respectively. To be consistent with
mIoU  EPEs are also reported on the 20th frame in each val sequence.

(cid:112)(u − uGT)2 + (v − vGT)2 where N is the number of pixels per-frame  and u

Baselines To fully demonstrate the advantages of our model on producing better predictions  we
compare our model against the following baseline methods:

6

Table 1: The performance of parsing prediction on
Cityscapes val set. For each competing model  we list
the mIoU/EPE when predicting one time step ahead.
Best results in bold.

Model
Copy last input
Warp last input
PredFlow
S2S [22]
ours (w/o Trans. layer)
ours

mIoU EPE
3.03
59.7
3.03
61.3
61.3
2.71
62.6
64.7
66.1

2.42
2.30

-

Table 2: The performance of motion prediction on
Cityscapes val set. For each model  we list the
mIoU/EPE when predicting one time step ahead. Best
results in bold.
Model
Copy last input
Warp last input
PredFlow
S2S [22]
ours (w/o Recur. FT)
ours

mIoU EPE
9.40
41.3
9.40
42.0
43.6
8.10
50.8
52.6
53.9

6.63
6.31

-

• Copy last input Copy the last optical ﬂow (Ot−1) and parsing map (St−1) at time t − 1 as
predictions at time t.

• Warp last input Warp the last segmentation St−1 using Ot−1 to get the parsing prediction
at the next time step. In order to make ﬂow applicable to the correct locations  we also warp
the ﬂow ﬁeld using the optical ﬂow in each time step.

• PredFlow Perform ﬂow prediction without the object masks generated from segmentations.
The architecture is the same as the ﬂow prediction net in Figure 2 which generates pixel-wise
ﬂow prediction in a single layer  instead of multiple branches. For fair comparison with our
joint model  in the following we report the average result of two independent PredFlow with
different random initializations. When predicting the segmentations at time t  we use the
ﬂow prediction output by PredFlow at time t to warp the segmentations at time t − 1. This
baseline aims to verify the advantages brought by parsing prediction when predicting ﬂow.
• S2S [22] Use only parsing anticipating network. The difference is that the former does not
leverage features learned by the ﬂow anticipating network to produce parsing predictions.
We replace the backbone network in the original S2S as the same one of ours  i.e. Res101-
FCN and retrain S2S with the same conﬁgurations as those of ours. Similar to the PredFlow 
the average performance of two randomly initialized S2S is reported. This baseline aims to
verify the advantages brought by ﬂow prediction when predicting parsing.

Implementation details Throughout the experiments  we set the length of the input sequence
as 4 frames  i.e. k = 4 in Xt−k:t−1 and St−k:t−1 (ref. Sec. 3). The original frames are ﬁrstly
downsampled to the resolution of 256 × 512 to accelerate training. In the ﬂow anticipating network 
we assign 19 semantic classes into three object groups which are deﬁned as follows: MOV-OBJ
including person  rider  car  truck  bus  train  motorcycle and bicycle  STA-OBJ including road 
sidewalk  sky  pole  trafﬁc light and trafﬁc sign and OTH-OBJ including building  wall  fence  terrain
and vegetation. For data augmentation  we randomly crop a patch with the size of 256 × 256 and
perform random mirror for all networks. All results of our model are based on single-model single-
scale testing. For other hyperparameters including weight decay  learning rate  batch size and epoch
number etc.  please refer to the supplementary material. All of our experiments are carried out on
NVIDIA Titan X GPUs using the Caffe library.

4.2 Results and analysis

Examples of the ﬂow predictions and parsing predictions output by our model for one-time step and
ten-time steps are illustrated in Figure 3 and Figure 4 respectively. Compared to baseline models  our
model produces more visually convincing prediction results.

4.2.1 One-time step anticipation

Table 1 lists the performance of parsing and ﬂow prediction on the 20th frame in the val set which has
ground truth semantic annotations. It can be observed that our model achieves the best performance on
both tasks  demonstrating the effectiveness on learning the latent representations for future prediction.
Based on the results  we analyze the effect of each component in our model as follows.

7

The effect of ﬂow prediction on parsing prediction Compared with S2S which does not leverage
ﬂow predictions  our model improves the mIoU with a large margin (3.5%). As shown in Figure 3 
compared to S2S  our model performs better on localizing the small objects in the predictions e.g.
pedestrian and trafﬁc sign  because it combines the discriminative local features learned in the ﬂow
anticipating network. These results clearly demonstrate the beneﬁt of ﬂow prediction for parsing
prediction.
The effect of parsing prediction on ﬂow prediction Compared with the baseline PredFlow which
has no access to the semantic information when predicting the ﬂow  our model reduces the average
EPE from 2.71 to 2.30 (a 15% improvement)  which demonstrates parsing prediction is beneﬁcial to
ﬂow prediction. As illustrated in Figure 3  the improvement our model makes upon PredFlow comes
from two aspects. First  since the segmentations provide boundary information of objects  the ﬂow
map predicted by our model has clearer object boundaries while the ﬂow map predicted by PredFlow
is mostly blurry. Second  our model shows more accurate ﬂow predictions on the moving objects
(ref. Sec. 4.1 for the list of moving objects). We calculate the average EPE for only the moving
objects  which is 2.45 for our model and 3.06 for PredFlow. By modeling the motion of different
objects separately  our model learns better representation for each motion mode. If all motions are
predicted in one layer as in PredFlow  then the moving objects which have large displacement than
other regions are prone to smoothness.
Beneﬁts of the transform layer As introduced in Sec. 3.1  the transform layer improves the perfor-
mance of our model by learning the latent feature space transformations from CNN1 to CNN2. In our
experiments  the transform layer contains one residual block [12] which has been widely used due to
its good performance and easy optimization. Details of the residual block used in our experiments
are included in the supplementary material. Compared to the variant of our model w/o the transform
layer  adding the transform layer improves the mIoU by 1.4 and reduces EPE by 0.12. We observe
that stacking more residual blocks only leads to marginal improvements at larger computational costs.

4.2.2 Longer duration prediction

The comparison of the prediction performance among all methods for ten time steps ahead is listed in
Table 2  from which one can observe that our model performs the best in this challenging task. The
effect of each component in our model is also veriﬁed in this experiment. Speciﬁcally  compared with
S2S  our model improves the mIoU by 3.1% due to the synergy with the ﬂow anticipating network.
The parsing prediction helps reducing the EPE of PredFlow by 1.79. Qualitative results are illustrated
in Figure 4.
The effect of recurrent ﬁne-tuning As explained in Sec. 3.2  it helps our model to capture long term
video dynamics by ﬁne-tuning the weights when recurrently applying the model to predict the next
time step in the future. As shown in Table 2  compared to the variant w/o recurrent ft  our model w/
recurrent ﬁne-tuning improves the mIoU by 1.3% and reduces the EPE by 0.32  therefore verifying
the effect of recurrent ﬁne-tuning.

4.3 Application for predicting the steering angle of a vehicle

With the parsing prediction and ﬂow prediction
available  one can enable the moving agent to be
more alert about the environments and get “smarter”.
Here  we investigate one application: predicting the
steering angle of the vehicle. The intuition is it is
convenient to infer the steering angle given the pre-
dicted ﬂow of static objects  e.g. road and sky  the
motion of which is only caused by ego-motion of
the camera mounted on the vehicle. Speciﬁcally  we
append a fully connected layer to take the features
learned in the STA-OBJ branch in the ﬂow antici-
pating network as input and perform regression to
steering angles. We test our model on the dataset
from Comma.ai [27] which consists of 11 videos

1https://github.com/commaai/research

8

Table 3: Comparison results of steering angle predic-
tion on a dataset from Comma.ai [27]. The criteria
is the mean square error (MSE  in degree2) between
the prediction and groud truth.

Model
Copy last prediction
Comma.ai1 [27]
ours

MSE (degrees2)

4.81
∼ 4
2.96

amounting to about 7 hours. The data of steering angles have been recorded for each frame captured
at 20Hz with the resolution of 160 × 320. We randomly sample 50K/5K frames from the train set for
training and validation purpose. Since there are videos captured at night  we normalize all training
frames to [0  255]. Similar to Cityscapes  we use epicﬂow and Res101-FCN to produce the target
output for ﬂow prediction and parsing prediction  respectively. We ﬁrst train our model following
Sec. 3 and then ﬁne-tune the whole model with the MSE loss after adding the fully connected layer
for steering angle prediction. During training  random crop with the size of 160 × 160 and random
mirror are employed and other hyperparameter settings follow Sec. 4.1. The testing results are listed
in Table 3. Compared to the model from Comma.ai which uses a ﬁve-layer CNN to estimate the
steering angle from a single frame and is trained end-to-end on all the training frames (396K)  our
model achieves much better performance (2.84 versus ∼4 in degrees2). Although we do not push the
performance by using more training data and more complex prediction models (only a fully connected
layer is used in our model for output steering angle)  this preliminary experiment still veriﬁes the
advantage of our model in learning the underlying latent parameters. We think it is just an initial
attempt in validating the dense prediction results through applications  which hopefully can stimulate
researchers to explore other interesting ways to utilize the parsing prediction and ﬂow prediction.

5 Conclusion

In this paper  we proposed a novel model to predict the future scene parsing and motion dynamics.
To our best knowledge  this is the ﬁrst research attempt to anticipate visual dynamics for building
intelligent agents. The model consists of two networks: the ﬂow anticipating network and the
parsing anticipating network which are jointly trained and beneﬁt each other. On the large scale
Cityscapes dataset  the experimental results demonstrate that the proposed model generates more
accurate prediction than well-established baselines both on single time step prediction and multiple
time prediction. In addition  we also presented a method to predict the steering angle of a vehicle
using our model and achieve promising preliminary results on the task.

Acknowledgements The work of Jiashi Feng was partially supported by National University of
Singapore startup grant R-263-000-C08-133  Ministry of Education of Singapore AcRF Tier One
grant R-263-000-C21-112 and NUS IDS grant R-263-000-C67-646.

References
[1] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Multi-task feature learning. In NIPS 

2007.

[2] Simon Baker  Daniel Scharstein  JP Lewis  Stefan Roth  Michael J Black  and Richard Szeliski. A database
and evaluation methodology for optical ﬂow. International Journal of Computer Vision  92(1):1–31  2011.
[3] Yu-Wei Chao  Jimei Yang  Brian Price  Scott Cohen  and Jia Deng. Forecasting human dynamics from

static images. In CVPR  2017.

[4] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille. Semantic

image segmentation with deep convolutional nets and fully connected crfs. In ICLR  2015.

[5] Marius Cordts  Mohamed Omran  Sebastian Ramos  Timo Rehfeld  Markus Enzweiler  Rodrigo Benenson 
Uwe Franke  Stefan Roth  and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.
arXiv preprint arXiv:1604.01685  2016.

[6] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi–task learning. In SIGKDD  2004.
[7] Clement Farabet  Camille Couprie  Laurent Najman  and Yann LeCun. Learning hierarchical features for
scene labeling. Pattern Analysis and Machine Intelligence  IEEE Transactions on  35(8):1915–1929  2013.
[8] Philipp Fischer  Alexey Dosovitskiy  Eddy Ilg  Philip Häusser  Caner Hazırba¸s  Vladimir Golkov  Patrick
van der Smagt  Daniel Cremers  and Thomas Brox. Flownet: Learning optical ﬂow with convolutional
networks. arXiv preprint arXiv:1504.06852  2015.

[9] David F Fouhey and C Lawrence Zitnick. Predicting object dynamics in scenes. In CVPR  2014.
[10] Alberto Garcia-Garcia  Sergio Orts-Escolano  Sergiu Oprea  Victor Villena-Martinez  and Jose Garcia-
Rodriguez. A review on deep learning techniques applied to semantic segmentation. arXiv preprint
arXiv:1704.06857  2017.

[11] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair  Aaron

Courville  and Yoshua Bengio. Generative adversarial nets. In NIPS  2014.

[12] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In CVPR  2016.

9

[13] Minh Hoai and Fernando De la Torre. Max-margin early event detectors. International Journal of Computer

Vision  107(2):191–202  2014.

[14] Herbert Jaeger. Tutorial on training recurrent neural networks  covering BPPT  RTRL  EKF and the" echo

state network" approach  volume 5. GMD-Forschungszentrum Informationstechnik  2002.

[15] Xiaojie Jin  Yunpeng Chen  Jiashi Feng  Zequn Jie  and Shuicheng Yan. Multi-path feedback recurrent

neural network for scene parsing. In AAAI  2017.

[16] Xiaojie Jin  Xin Li  Huaxin Xiao  Xiaohui Shen  Zhe Lin  Jimei Yang  Yunpeng Chen  Jian Dong  Luoqi

Liu  Zequn Jie  et al. Video scene parsing with predictive feature learning. In ICCV  2017.

[17] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic segmenta-

tion. In CVPR  2015.

[18] William Lotter  Gabriel Kreiman  and David Cox. Unsupervised learning of visual structure using predictive

generative networks. arXiv preprint arXiv:1511.06380  2015.

[19] Zelun Luo  Boya Peng  De-An Huang  Alexandre Alahi  and Li Fei-Fei. Unsupervised learning of long-term

motion dynamics for videos. arXiv preprint arXiv:1701.01821  2017.

[20] Michael Mathieu  Camille Couprie  and Yann LeCun. Deep multi-scale video prediction beyond mean

square error. arXiv preprint arXiv:1511.05440  2015.

[21] Rudolf Mester. Motion estimation revisited: an estimation-theoretic approach. In Image Analysis and

Interpretation (SSIAI)  2014 IEEE Southwest Symposium on  pages 113–116. IEEE  2014.

[22] Natalia Neverova  Pauline Luc  Camille Couprie  Jakob Verbeek  and Yann LeCun. Predicting deeper into

the future of semantic segmentation. arXiv preprint arXiv:1703.07684  2017.

[23] Deepak Pathak  Philipp Krahenbuhl  Jeff Donahue  Trevor Darrell  and Alexei A Efros. Context encoders:

Feature learning by inpainting. In CVPR  2016.

[24] Viorica Patraucean  Ankur Handa  and Roberto Cipolla. Spatio-temporal video autoencoder with differen-

tiable memory. arXiv preprint arXiv:1511.06309  2015.

[25] Jerome Revaud  Philippe Weinzaepfel  Zaid Harchaoui  and Cordelia Schmid. Epicﬂow: Edge-preserving

interpolation of correspondences for optical ﬂow. In CVPR  2015.

[26] Anirban Roy and Sinisa Todorovic. Scene labeling using beam search under mutex constraints. In CVPR 

2014.

[27] Eder Santana and George Hotz. Learning a driving simulator. CoRR  abs/1608.01230  2016.
[28] Alexander G Schwing and Raquel Urtasun. Fully connected deep structured networks. arXiv preprint

arXiv:1503.02351  2015.

[29] Laura Sevilla-Lara  Deqing Sun  Varun Jampani  and Michael J Black. Optical ﬂow with semantic

segmentation and localized layers. In CVPR  2016.

[30] Richard Socher  Cliff C Lin  Chris Manning  and Andrew Y Ng. Parsing natural scenes and natural

language with recursive neural networks. In ICML  2011.

[31] Nitish Srivastava  Elman Mansimov  and Ruslan Salakhudinov. Unsupervised learning of video representa-

tions using lstms. In ICML  2015.

[32] Ruben Villegas  Jimei Yang  Seunghoon Hong  Xunyu Lin  and Honglak Lee. Decomposing motion and

content for natural video sequence prediction. In ICLR  2017.

[33] Ruben Villegas  Jimei Yang  Yuliang Zou  Sungryull Sohn  Xunyu Lin  and Honglak Lee. Learning to

generate long-term future via hierarchical prediction. In ICML  2017.

[34] Carl Vondrick  Hamed Pirsiavash  and Antonio Torralba. Generating videos with scene dynamics. In NIPS 

2016.

[35] Jacob Walker  Abhinav Gupta  and Martial Hebert. Patch to the future: Unsupervised visual prediction. In

CVPR  2014.

[36] Jacob Walker  Abhinav Gupta  and Martial Hebert. Dense optical ﬂow prediction from a static image. In

ICCV  2015.

[37] Jenny Yuen and Antonio Torralba. A data-driven approach for event prediction. In ECCV  2010.

10

,Xiaojie Jin
Huaxin Xiao
Xiaohui Shen
Jimei Yang
Zhe Lin
Yunpeng Chen
Zequn Jie
Jiashi Feng
Shuicheng Yan
Jovana Mitrovic
Dino Sejdinovic
Yee Whye Teh
Matteo Togninalli
Elisabetta Ghisu
Felipe Llinares-López
Bastian Rieck
Karsten Borgwardt