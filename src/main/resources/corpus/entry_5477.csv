2018,The Lingering of Gradients: How to Reuse Gradients Over Time,Classically  the time complexity of a first-order method is estimated by its number of gradient computations. In this paper  we study a more refined complexity by taking into account the ``lingering'' of gradients: once a gradient is computed at $x_k$  the additional time to compute gradients at $x_{k+1} x_{k+2} \dots$ may be reduced.

We show how this improves the running time of gradient descent and SVRG. For instance  if the "additional time'' scales linearly with respect to the traveled distance  then the "convergence rate'' of gradient descent can be improved from $1/T$ to $\exp(-T^{1/3})$. On the empirical side  we solve a hypothetical revenue management problem on the Yahoo! Front Page Today Module application with 4.6m users to $10^{-6}$ error (or $10^{-12}$ dual error) using 6 passes of the dataset.,The Lingering of Gradients:

How to Reuse Gradients over Time

Zeyuan Allen-Zhu∗
Microsoft Research AI
Redmond  WA 98052

David Simchi-Levi∗

MIT

Cambridge  MA 02139

zeyuan@csail.mit.edu

dslevi@mit.edu

Xinshang Wang∗

MIT

Cambridge  MA 02139
xinshang@mit.edu

Abstract

Classically  the time complexity of a ﬁrst-order method is estimated by its number
of gradient computations. In this paper  we study a more reﬁned complexity by
taking into account the “lingering” of gradients: once a gradient is computed at
xk  the additional time to compute gradients at xk+1  xk+2  . . . may be reduced.
We show how this improves the running time of gradient descent and SVRG. For
instance  if the “additional time” scales linearly with respect to the traveled dis-
tance  then the “convergence rate” of gradient descent can be improved from 1/T
to exp(−T 1/3). On the empirical side  we solve a hypothetical revenue manage-
ment problem on the Yahoo! Front Page Today Module application with 4.6m
users to 10−6 error (or 10−12 dual error) using 6 passes of the dataset.

1

Introduction

(cid:110)

(cid:80)n

(cid:111)

First-order methods play a fundamental role in large-scale machine learning and optimization tasks.
In most scenarios  the performance of a ﬁrst-order method is represented by its convergence rate:
the relationship between ε (the optimization error) versus T (the number of gradient computations).
This is meaningful because in most applications  the time complexities for evaluating gradients at
different points are of the same magnitude. In other words  the worse-case time complexities of
ﬁrst-order methods are usually proportional to a ﬁxed parameter times T .
In large-scale settings  however  if we have already spent time computing the (full) gradient at x 
perhaps we can use such information to reduce the time complexity to compute full gradients at
other points near x. We call this the “lingering” of gradients  because the gradient at x may be
partially reused for future consideration  but will eventually fade away once we are far from x.
Formally  consider the (ﬁnite-sum) stochastic convex minimization problem:

minx∈Rd

f (x) def= 1
n

i=1 fi(x)

.

(1.1)

Then  could it be possible that whenever x is sufﬁciently close to y  for at least a large fraction of
indices i ∈ [n]  we have ∇fi(x) ≈ ∇fi(y)? In other words  if ∇f1(x)  . . .  ∇fn(x) are already
calculated at some point x  can we reuse a large fraction of them to approximate ∇f (y)?
Example 1.
In classiﬁcation problems  fi(x) represents the loss value for “how well training
sample i is classiﬁed under predictor x”. For any sample i that has a large margin under predictor x 
its gradient ∇fi(x) may stay close to ∇fi(y) whenever x is close to y.
Formally  let fi(x) = max{0  1−(cid:104)x  ai(cid:105)} be the hinge loss (or its smoothed variant if needed) with
respect to the i-th sample ai ∈ Rd. If the margin |1 − (cid:104)x  ai(cid:105)| is sufﬁciently large  then moving
∗Authors sorted in alphabetical order. Full version of this paper (containing additional theoretical results 

additional experiments  and missing proofs) available at https://arxiv.org/abs/1901.02871.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

from x to a nearby point y should not affect the sign of 1−(cid:104)x  ai(cid:105)  and thus not change the gradient.
Therefore  if samples a1  . . .   an are sufﬁciently spread out in the space  then a large fraction of them
should incur large margins  and thus have the same gradients when x changes by a small amount.

In revenue management problems  fi(x) represents the marginal proﬁt of the i-th
+ over d items. In many applications (see Section 2.2) 

Example 2.
customer under bid-price strategy x ∈ Rd
∇fi(x) only depends on customer i’s preferences under x.
If the bid-price vector x ∈ Rd
+ changes by a small amount to y  then for a large fraction of customers
i  their most proﬁtable items may not change  and thus ∇fi(x) ≈ ∇fi(y). (Indeed  imagine if one
of the items is Xbox  and its price drops by 5%  perhaps 90% of the customers will not change their
minds about buying or not. We shall demonstrate this using real-life data.)

1.1 Our Results
We assume in this paper that  given any point x ∈ Rd and index i ∈ [n]  one can efﬁciently evaluate
a “lingering radius” δ(x  i). The radius satisﬁes the condition that for every point y that is within
distance δ(x  i) from x  the stochastic gradient ∇fi(y) is equal to ∇fi(x). We make two remarks:
• We use “equal to” for the purpose of proving theoretical results. In practice and in our experi-
ments  it sufﬁces to use approximate equality such as (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤ 10−10.
• By “efﬁcient” we mean δ(x  i) is computable in the same complexity as evaluating ∇fi(x). This
is reasonable because when ∇fi(x) is an explicit function of x  it is usually easy to tell how
sensitive it is to the input x. (We shall include such an example in our experiments.)

If we denote by B(x  r) the set of indices j satisfying δ(x  j) < r  and if we travel to some point y
that is at most distance r from x  then we only need to re-evaluate the (stochastic) gradients ∇fj(y)
for j ∈ B(x  r). Intuitively  one should expect |B(x  r)| to grow as a function of r  and this is indeed
the case – for instance  for the revenue management problem (see Section 5).

Theory. To present the simplest theoretical result  we modify gradient descent (GD) to take into
account the lingering of gradients. At a high level  we run GD  but during its execution  we maintain
a decomposition of the indices Λ0 ∪ ··· ∪ Λt = {1  2  . . .   n} where t is logarithmic in n. Now 
whenever we need ∇fi(xk) for some i ∈ Λp  we approximate it by ∇fi(xk(cid:48)) for a point k(cid:48) that was
visited at most 2p steps ago. Our algorithm makes sure that such ∇fi(xk(cid:48)) is available in memory.
We prove that the performance of our algorithm depends on how |B(x  r)| grows in r. Formally  let
T be the total number of stochastic gradient computations divided by n  and suppose |B(x  r)| ≤
O(r)  i.e.  it linearly scales in the radius r. Then  our algorithm ﬁnds a point x with f (x)− f (x∗) ≤
2−Ω(T 1/3). In contrast  traditional GD satisﬁes f (x) − f (x∗) ≤ O(T −1).
In the full version of this paper  we also study the case when |B(x  r)| ≤ O(rβ) for an arbitrary
constant β ∈ (0  1].
Practice. We also design an algorithm that practically maximizes the use of gradient lingering. We
take the SVRG method [19  36] as the prototype because it is widely applied in large-scale settings.

Recall that SVRG uses gradient estimator ∇f ((cid:101)x) − ∇fi((cid:101)x) + ∇fi(xk) to estimate the full gradient
∇f (xk)  where(cid:101)x is the so-called snapshot point (which was visited at most n steps ago) and i is a
whose stochastic gradients need to be recomputed  and ignore those such that ∇fi(xk) = ∇fi((cid:101)x).

random index. At a high level  we modify SVRG so that the index i is only generated from those

This can further reduce the variance of the gradient estimator  and improve the running time.

1.2 Related Work

Variance Reduction. The SVRG method was independently proposed by Johnson and Zhang
[19]  Zhang et al. [36]  and belong to the class of stochastic methods using the so-called variance-
reduction technique [4  8  19  23  27–30  35  36]. The common idea behind these methods is to use
some full gradient of the past to approximate future  but they do not distinguish which ∇fi(x) can
“linger longer in time” among all indices i ∈ [n] for different x.

2

Arguably the two most widely applied variance-reduction methods are SVRG and SAGA [8]. They
have complementary performance depending on the internal structural of the dataset [5]  so we
compare to both in our experiments.
Reuse Gradients. Some researchers have exploited the internal structure of the dataset to speed up
ﬁrst-order methods. That is  they use ∇fi(x) to approximate ∇fj(x) when the two data samples i
and j are sufﬁciently close. This is orthogonal to our setting because we use ∇fi(x) to approximate
∇fi(y) when x and y are sufﬁciently close.
In the extreme case when all the data samples are
identical  they have ∇fi(x) = ∇fj(x) for every i  j and thus stochastic gradient methods converge
as fast as full gradient ones. For this problem  Hofmann et al. [16] introduced a variant of SAGA 
Allen-Zhu et al. [5] introduced a variant of SVRG and a variant of accelerated coordinate descent.
Other authors study how to reduce gradient computations at the snapshot points of SVRG [15  20].
This is also orthogonal to the idea of this paper  and can be added to our algorithms for even better
performance (see Section 5).

2 Notions and Problem Formulation
We denote by (cid:107) · (cid:107) the Euclidean norm  and (cid:107) · (cid:107)∞ the inﬁnity norm. Recall the notion of Lipschitz
smoothness (it has other equivalent deﬁnitions  see textbook [25]).
Deﬁnition 2.1. A function f : Rd → R is L-Lipschitz smooth (or L-smooth for short) if

∀x  y ∈ Rd : (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107) .
We also introduce the notion of “lowbit sequence” for a positive integer.
Deﬁnition 2.2. For positive integer k  let lowbit(k) def= 2i where i ≥ 0 is the maximum integer such
that k is integral multiple of 2i. For instance  lowbit(34) = 2  lowbit(12) = 4  and lowbit(8) = 8.
Given positive integer k  let the lowbit sequence of k be (k0  k1  . . .   kt) where

0 = k0 < k1 < ··· < kt = k and ki−1 = ki − lowbit(ki) .

For instance  the lowbit sequence of 45 is (0  32  40  44  45).

2.1 Our Model
We propose the following model to capture the lingering of gradients. For every x ∈ Rd and index
i ∈ [n]  let δ(x  i) ≥ 0 be the lingering radius of ∇fi(x)  meaning that2

∀y ∈ Rd with (cid:107)y − x(cid:107) ≤ δ(x  i)

it satisﬁes ∇fi(x) = ∇fi(y) .

In other words  as long as we travel within distance δ(x  i) from x  the gradient ∇fi(x) can be
reused to represent ∇fi(y). Accordingly  for every x ∈ Rd and r ≥ 0  we denote by B(x  r) the set

of indices j satisfying δ(x  j) < r. That is  B(x  r) def=(cid:8)j ∈ [n](cid:12)(cid:12) δ(x  j) < r(cid:9) .

Our main assumption of this paper is that
Assumption 1. Each δ(x  i) can be computed in the same time complexity as ∇fi(x).
Under Assumption 1  if at some point x we have already computed ∇fi(x) for all i ∈ [n]  then we
can compute δ(x  i) as well for every i ∈ [n]  and sort the indices i ∈ [n] in the increasing order of
δ(x  i). In the future  if we arrive at any point y  we can calculate r = (cid:107)x − y(cid:107) and use

(cid:17)

(cid:16)(cid:80)

i(cid:54)∈B(x r) ∇fi(x) +(cid:80)

∇(cid:48) = 1

n

i∈B(x r) ∇fi(y)

to represent ∇f (y). We stress that the time to compute ∇(cid:48) is only proportional to |B(x  r)|.
We denote by Ttime the gradient complexity  which equals how many times ∇fi(x) and δ(x  i) are
calculated  divided by n. In computing ∇(cid:48) above  the gradient complexity is |B(x  r)|/n. If we
always set δ(x  i) = 0 then |B(x  r)| = n and the gradient complexity for computing ∇(cid:48) remains
1. However  if the underlying Problem (1.1) is nice enough so that |B(x  r)| becomes an increasing
function of r (see Figure 2)  then we can hope to design faster algorithms.

2Recall that  in practice  one should replace the exact equality with  for instance  (cid:107)∇fi(x) − ∇fi(y)(cid:107) ≤

10−10. To present the simplest statements  we do not introduce such an extra parameter.

3

2.2 Revenue Management Problem

As a motivating example  consider a canonical revenue management problem of selling d resources
to n customers. Let bj ≥ 0 be the capacity of resource j ∈ [d]; let pi j ∈ [0  1] be the probability that
customer i ∈ [n] will purchase a unit of resource j if offered resource j; and let rj be the revenue
for each unit of resource j. We want to offer each customer one and only one candidate resource 
and let yi j be the probability we offer customer i resource j. The following is an LP relaxation for
this problem:

(cid:26) (cid:88)

max
y≥0

i∈[n] j∈[d]

rjpi jyi j

(cid:12)(cid:12)(cid:12)(cid:12)∀j ∈ [d] 

(cid:88)

i∈[n]

(cid:94)∀i ∈ [n] 

(cid:88)

j∈[d]

(cid:27)

pi jyi j ≤ bj

yi j = 1

(2.1)

This LP (2.1) and its variants have repeatedly found many applications  including adwords/ad al-
location problems [3  10  11  14  17  24  34  37]  and revenue management for airline and service
industries [7  13  18  26  31  33]. Some authors also study the online version of solving such LPs
[1  2  9  12].
A standard way to reduce (2.1) to convex optimization is by regularization (cf. [37]). Let us subtract
def= maxi∈[n] pi j

the maximization objective by R(x) def= µ(cid:80)
(cid:111)

and µ > 0 is some small regularization weight. Then  after transforming to the dual  we have

(cid:16) (rj − xj)pi j

j∈[d] yi j log yi j  where pi

(cid:80)

n(cid:88)

i∈[n] pi

(cid:110)

(cid:17)

exp

piµ

.

(2.2)

pi · log Zi +

min
x≥0

µ

i=1

where Zi =

xjbj

d(cid:88)
yi j = exp(cid:0) (rj−xj )pi j

j=1

d(cid:88)
(cid:1)/Zi.

j=1

Any solution x (usually known as the bid price in operations management [32]) to (2.2) naturally
gives back a solution y for the primal (2.1)  by setting

(2.3)
If we let fi(x) def= µnpi · log Zi + (cid:104)x  b(cid:105)  then (2.2) reduces to Problem (1.1). We conduct empirical
studies on this revenue management problem in Section 5.

piµ

3 Our Modiﬁcation to Gradient Descent

(cid:80)n
then we can arrive at a point x with f (x) − f (x∗) ≤ O(cid:0)(cid:107)x0−x∗(cid:107)2

In this section  we consider a convex function f (x) = 1
i=1 fi(x) that is L-smooth. Recall from
textbooks (e.g.  [25]) that if gradient descent (GD) is applied for T iterations  starting at x0 ∈ Rd 
n
T convergence rate.

(cid:1). This is the 1

T

To improve on this theoretical rate  we make the following assumption on B(x  r):
Assumption 2. There exists α ∈ [0  1]  C > 0 such that 

∀x ∈ Rd  r ≥ 0 :

≤ ψ(r) def= max{α  r/C} .

|B(x  r)|

n

It says that |B(n  r)| is a growing function in r  and the growth rate is ∝ r. (In the full version we
investigate the more general case where the growth rate is ∝ rβ for arbitrary β ∈ (0  1].) We also
allow an additive term α to cover the case that an α fraction of the stochastic gradients always need
to be recalculated  regardless of the distance. We illustrate the meaningfulness of Assumption 2 in
Figure 2.
Our result of this section can be summarized as follows. Hiding (cid:107)x0−x∗(cid:107)  L  C in the big-O notion 
and letting Ttime be the gradient complexity  we can modify GD so that it ﬁnds a point x with

f (x) − f (x∗) ≤ O(cid:0) α

+ 2−Ω(Ttime)1/3(cid:1) .

Ttime

We emphasize that our modiﬁed algorithm does not need to know α or C.

3.1 Algorithm Description
In classical gradient descent (GD)  starting from x0 ∈ Rd  one iteratively updates xk+1 ← xk −
L∇f (xk). We propose GDlin (see Algorithm 1) which  at a high level  differs from GD in two ways:

1

4

(cid:80)n
i=1 fi(x) convex and L-smooth  starting vector x(0) ∈ Rd  number of epochs

(cid:5) it satisﬁes g = ∇f (xk)

(cid:1)s(cid:101); and ξ ← C

m.

S ≥ 1  parameters C  D > 0.

x0 ← x(s−1); m ← (cid:100)(cid:0)1 + C2

Algorithm 1 GDlin(f  x(0)  S  C  D)
Input: f (x) = 1
n
Output: vector x ∈ Rd.
1: for s ← 1 to S do
2:
3:
4:
5:
6:
7:
8:
x(s) ← xm;
9:
10: return x = x(S).

xk+1 ← xk − min(cid:8) ξ(cid:107)g(cid:107)   1

g ← (cid:126)0 and gi ← (cid:126)0 for each i ∈ [n].
for k ← 0 to m − 1 do

(cid:9)g

16D2

L

n

Calculate Λk ⊆ [n] from x0  . . .   xk according to Deﬁnition 3.1.
for i ∈ Λk do
g ← g +

and gi ← ∇fi(xk).

∇fi(xk)−gi

(a)
Figure 1: Illustration of index sets Λk

(b)

• It performs a truncated gradient descent with travel distance (cid:107)xk − xk+1(cid:107) ≤ ξ per step.
• It speeds up the process of calculating ∇f (xk) by using the lingering of past gradients.

Formally  GDlin consists of S epochs s = 1  2  . . .   S of growing length m = (cid:100)(cid:0)1 + C2

each epoch  it starts with x0 ∈ Rd and performs m truncated gradient descent steps

16D2

(cid:1)s(cid:7). In

xk+1 ← xk − min(cid:8)

(cid:9) · ∇f (xk) .

ξ

(cid:107)∇f (xk)(cid:107)   1

L

We choose ξ = C/m to ensure that the worst-case travel distance (cid:107)xm − x0(cid:107) is at most mξ = C.
In each iteration k = 0  1  . . .   m− 1 of this epoch s  in order to calculate ∇f (xk)  GDlin constructs
index sets Λ0  Λ1  . . .   Λm−1 ⊆ [n] and recalculates only ∇fi(xk) for those i ∈ Λk. We formally
introduce index sets below  and illustrate them in Figure 1.
Deﬁnition 3.1. Given x0  x1  . . .   xm−1 ∈ Rd  we deﬁne index subsets Λ0  . . . Λm−1 ⊆ [n] as
follows. Let Λ0 = [n]. For each k ∈ {1  2  . . .   m − 1}  if (k0  . . .   kt) is k’s lowbit sequence from
Deﬁnition 2.2  then (recalling k = kt)

(cid:0)Bki(k − ki) \ Bki (kt−1 − ki)(cid:1) where Bk(r) def= Λk ∩ B(xk  r · ξ) .

def=(cid:83)t−1

Λk

i=0

3.2

Intuitions & Properties of Index Sets

We show in this paper that our construction of index sets satisfy the following three properties.
Lemma 3.2. The construction of Λ0  . . .   Λm−1 ensures that g = ∇f (xk) in each iteration k.

5

158096072809304𝚲𝟎Λ1Λ2Λ3Λ4Λ5Λ6Λ7𝚲𝟖Λ9Λ10Λ11𝚲𝟏𝟐Λ13𝚲𝟏𝟒𝚲𝟏𝟓Λ16𝟏=𝐵08𝟐=𝐵012∖𝐵0(8)𝟑=𝐵014∖𝐵0(12)𝟒=𝐵015∖𝐵0(14)𝟓=𝐵84𝟔=𝐵86∖𝐵84𝟕=𝐵87∖𝐵86𝟖=𝐵122𝟗=𝐵123∖𝐵12(2)𝟎=𝐵141158096072809304𝚲𝟎𝚲𝟖𝚲𝟏𝟐𝚲𝟏𝟒𝚲𝟏𝟓⊕⊕⊕⊕⊗⊗⊖⊙Claim 3.3. The gradient complexity to construct Λ0  . . .   Λm−1 is O(cid:0) 1

k=0 |Λk|(cid:1) under
(cid:80)m−1
(cid:80)m−1
k=0 |Λk| ≤ O(αm + log2 m) .

n

(cid:3).

Assumption 1. The space complexity is O(n log n).
Lemma 3.4. Under Assumption 2  we have 1
n
Claim 3.3 is easy to verify. Indeed  for each Λ(cid:96) that is calculated  we can sort its indices j ∈ Λ(cid:96) in
the increasing order of δ(xk  j).3 Now  whenever we calculate Bki (k−ki)\Bki(kt−1−ki)  we have

already sorted the indices in Λki  so can directly retrieve those j with δ(xki  j) ∈(cid:0)kt−1−ki  k−ki

As for the space complexity  in any iteration k  we only need to store (cid:100)log2 k(cid:101) index sets Λ(cid:96) for
(cid:96) < k. For instance  when calculating Λ15 (see Figure 1(b))  we only need to use Λ0  Λ8  Λ12  Λ14;
and from k = 16 onwards  we no longer need to store Λ1  . . .   Λ15.
Lemma 3.2 is technically involved to prove (see full version)  but we give a sketched proof by
picture. Take k = 15 as an example. As illustrated by Figure 1(b)  for every j ∈ [n] 
• If j belongs to Λ15 —i.e.  boxes 4  0  9  7 of Figure 1—
• If j belongs to Λ14 \ B14(1) —i.e.  ⊕ region of Figure 1(b)—

We have calculated ∇fj(xk) so are ﬁne.
We have ∇fj(x15) = ∇fj(x14) because (cid:107)x15 − x14(cid:107) ≤ ξ and j (cid:54)∈ B14(1). Therefore  we can
safely retrieve gj = ∇fj(x14) to represent ∇fj(x15).
We have ∇fj(x15) = ∇fj(x12) for similar reason above. Also  the most recent update of gj
was at iteration 12  so we can safely retrieve gj to represent ∇fj(x15).

• If j belongs to Λ12 \ B12(3) —i.e.  ⊗ region of Figure 1(b)—

• And so on.
In sum  for all indices j ∈ [n]  we have gj = ∇fj(xk) so g = g1+···+gn
Lemma 3.4 is also involved to prove (see full version)  but again should be intuitive from the picture.
The indices in boxes 1  2  3  4 of Figure 1 are disjoint  and belong to B(x0  15ξ)  totaling at most
|B(x0  15ξ)| ≤ nψ(15ξ). The indices in boxes 5  6  7 of Figure 1 are also disjoint  and belong to
B(x8  7ξ)  totaling at most |B(x8  7ξ)| ≤ nψ(7ξ). If we sum up the cardinality of these boxes by
carefully grouping them in this manner  then we can prove Lemma 3.4 using Assumption 2.

equals ∇f (xk).

n

3.3 Convergence Theorem

So far  Lemma 3.4 shows we can reduce the gradient complexity from O(m) to (cid:101)O(1) for every m

steps of gradient descent. Therefore  we wish to set m as large as possible  or equivalently ξ = C/m
as small as possible. Unfortunately  when ξ is too small  it will impact the performance of truncated
gradient descent (see full version). This motivates us to start with small value of m and increase
it epoch by epoch. Indeed  as the number of epoch grows  f (x0) becomes closer to the minimum
f (x∗)  and thus we can choose smaller values of ξ.
Formally  we have
Theorem 3.5. Given any x(0) ∈ Rd and D > 0 that is an upper bound on (cid:107)x(0) − x∗(cid:107). Suppose
Assumption 1 and 2 are satisﬁed with parameters C ∈ (0  D]  α ∈ [0  1]. Then  denoting by ms =

(cid:1)s(cid:101)  we have that GDlin(f  x0  S  C  D) outputs a point x ∈ Rd satisfying f (x)−f (x∗) ≤
with gradient complexity Ttime = O(cid:0)(cid:80)S

(cid:100)(cid:0)1+ C2

s=1 αms + log2 ms

(cid:1).

16D2

4LD2
mS

As simple corollaries  we have
Theorem 3.6. In the setting of Theorem 3.5  given any T ≥ 1  one can choose S so that GDlin ﬁnds
a point x in gradient complexity Ttime = O(T ) s.t.

f (x) − f (x∗) ≤ O(cid:0) LD4

C2

(cid:1) +

· α

LD2

and in this case GDlin gives back the convergence f (x) − f (x∗) ≤ O(cid:0) LD2

We remark here if ψ(r) = 1 (so there is no lingering effect for gradients)  we can choose C = D

(cid:1) of GD.

2Ω(C2T /D2)1/3

T

3Calculating those lingering radii δ(xk  j) require gradient complexity |Λ(cid:96)| according to Assumption 1  and

the time for sorting is negligible.

6

.

T

4 Our Modiﬁcation to SVRG

In this section  we use Assumption 1 to improve the running time of SVRG [19  36]  one of the most
widely applied stochastic gradient methods in large-scale settings. The purpose of this section is
to construct an algorithm that works well in practice: to (1) work for any possible lingering radii
δ(x  i)  (2) be identical to SVRG if δ(x  i) ≡ 0  and (3) be faster than SVRG when δ(x  i) is large.
Recall how the SVRG method works. Each epoch of SVRG consists of m iterations (m = 2n
in practice). Each epoch starts with a point x0 (known as the snapshot) where the full gradient
∇f (x0) is computed exactly. In each iteration k = 0  1  . . .   m − 1 of this epoch  SVRG updates
xk+1 ← xk − ηg where η > 0 is the learning rate and g is the gradient estimator g = ∇f (x0) +
∇fi(xk) − ∇fi(x0) for some i randomly drawn from [n]. Note that it satisﬁes Ei[g] = ∇f (xk) so
g is an unbiased estimator of the gradient. In the next epoch  SVRG starts with xm of the previous
epoch.4 We denote by x(s) the value of x0 at the beginning of epoch s = 0  1  2  . . .   S − 1.
Our Algorithm. Our algorithm SVRGlin (pseudocode in full version) maintains disjoint subsets
Hs ⊆ [n]  where each Hs includes the set of the indices i whose gradients ∇fi(x(s)) from epoch s
can still be safely reused at present.
At the starting point x0 of an epoch s  we let Hs = [n]\(H0∪···∪Hs−1) and re-calculate gradients
∇fi(x0) only for i ∈ Hs; the remaining ones can be loaded from the memory. This computes the
full gradient ∇f (x0). Then  we denote by m = 2|Hs| and perform only m iterations within epoch
s. We next discuss how to perform update xk → xk+1 and maintain {Hs}s during each iteration.
• In each iteration k of this epoch  we claim that ∇fi(xk) = ∇fi(x0) for every i ∈ H0∪···∪Hs.5

Thus  we can uniformly sample i from [n]\(cid:0)H0∪···∪Hs
(cid:17)

(cid:1)  and construct an unbiased estimator

(cid:16)

g ← ∇f (x0) +

1 −

[∇fi(xk) − ∇fi(x0)]

(cid:80)s
s(cid:48)=0 |Hs(cid:48)|

n

of the true gradient ∇f (xk). Then  we update xk+1 ← xk − ηg the same way as SVRG.
We emphasize that the above choice of g reduces its variance (because there are fewer random
choices)  and it is known that reducing variance leads to faster running time [19].
• As for how to maintain {Hs}s  in each iteration k after xk+1 is computed  for every s(cid:48) ≤ s  we
wish to remove those indices i ∈ Hs(cid:48) such that the current position x lies outside of the lingering
radius of i  i.e.  δ(x(s)  i) < (cid:107)x − x(s)(cid:107). To efﬁciently implement this  we need to make sure
that whenever Hs(cid:48) is constructed (at the beginning of epoch s(cid:48))  the algorithm sort all the indices
i ∈ Hs(cid:48) by the increasing order of δ(x(s(cid:48))  i). We include implementation details in full version.

5 Preliminary Empirical Evaluation

In this section  we construct a revenue maximization
LP (2.1) using the publicly accessible dataset of Yahoo!
Front Page Today Module [6  22]. We describe details of
the experimental setup in full version. Based on this real-
life dataset  we validate Assumption 2 and our motivation
behind lingering gradients. We also test the performance
of SVRGlin from Section 4 on optimizing this LP.
Illustration of Lingering Radius. We calculate linger-
ing radii on the dual problem (2.2). Let θ > 0 be a pa-
rameter large enough so that e−θ can be viewed as zero.
(For instance  θ = 20 gives e−20 ≈ 2 × 10−9.) Then  for
each point x ∈ R≥0 and index i ∈ [n]  we let

Figure 2: |B(x  r)|/n as a function of r. We
choose θ = 5. Dashed curve is
when x = (cid:126)0  and solid curve is
when x is near the optimum.

(cid:8)(rj−xj)pi j

(cid:9) .

(rj∗ − xj∗ )pi j∗ − (rj − xj)pi j − θpiµ

δ(x  i) = min

j∈[n] j(cid:54)=j∗

pi j∗ + pi j

where

j∗ = arg max
j∈[n]

4Some authors use the average of x1  . . .   xm to start the next epoch  but we choose this simpler version.
5This is because for every i ∈ Hs  by deﬁnition of Hs we have ∇fi(xk) = ∇fi(x(s)) = ∇fi(x0); for
every i ∈ Hs(cid:48) where s(cid:48) < s  we know ∇fi(xk) = ∇fi(x(s(cid:48))) but we also have ∇fi(x0) = ∇fi(x(s(cid:48)))
(because otherwise i would have been removed from Hs(cid:48)).

7

00.20.40.60.8100.010.020.030.04B(x r)/nr(a) SVRGlin vs. SVRG and SAGA.

(b) SVRGlin vs. SVRG and SAGA

Figure 3: Comparison of (a) dual objective optimality (for which different learning rates are presented) and (b)

primal objective optimality (for which the learning rates are best tuned).

∇fi(y) ≈ (b1  . . .   bd) + npi j∗ ej∗

It is now a simple exercise to verify that  denoting by ej the j-th basis unit vector  then6
for every (cid:107)y − x(cid:107)∞ ≤ δ(x  i) .

In Figure 2  we plot |B(x  r)| =(cid:12)(cid:12)(cid:8)j ∈ [n](cid:12)(cid:12) δ(x  j) < r(cid:9)(cid:12)(cid:12) as an increasing function of r. We see

that for practical data  |B(x  r)|/n is indeed bounded above by some increasing function ψ(·). We
provide more justiﬁcation on why this happens in the full paper.
Remark 5.1. This δ(x  i) differs from our deﬁnition in Section 2 in two ways. First  it ensures
∇fi(y) ≈ ∇fi(x) as opposed to exact equality; for practical purposes this is no big issue  and we
choose θ = 5 in our experiments. Second  (cid:107)y − x(cid:107)∞ ≤ δ(x  i) gives a bigger “safe region” than
(cid:107)y − x(cid:107) ≤ δ(x  i); thus  when implementing SVRGlin  we adopt (cid:107) · (cid:107)∞ as the norm of choice.

Numerical Experiments. We consider solving the dual problem (2.2). In Figure 3(a)  we plot the
optimization error of (2.2) as a function #grad/n  the number of stochastic gradient computations
divided by n  also known as #passes of dataset.
Figure 3(a) compares our SVRGlin to SVRG and SAGA (each for 3 best tuned step lengths).7 We can
see SVRGlin is close to SVRG or SAGA during the ﬁrst 5-7 passes of the data. This is because initially 
x moves fast and cannot usually stay in the lingering radii for most indices i. After that period 
SVRGlin requires a dramatically smaller number of gradient computations  as x moves slower and
slower  becoming more easily to stay in the lingering radii. It is interesting to note that SVRGlin does
not signiﬁcantly improve the optimization error as a function of number of epochs; the improvement
primarily lies in improving the number of gradient computations per epoch. The comparison is

More Plots.
In Figure 3(b)  we also compare the primal objective value for the LP (2.1). (We
explain how to get feasible primal solutions from the dual in the full version.) It is perhaps worth
noting that we have chosen µ = 10−5 as the regularization error  and the primal objective error
indeed reaches to 10−6 which is roughly µ. In the full version  we also compare the running time of
the algorithms. Those plots are almost identical to Figure 3(b).

6 Conclusion

In this paper  we study convex problems where the stochastic gradients ∇fi(x) can be reused when
we move away from x. In our theoretical result  we model the number of stochastic gradients that can
be changed (and thus cannot be reused) as a function of how much distance we travel away from x 
and show faster convergence for gradient descent (in terms of the number of gradient computations).
On the empirical side  we show how to modify the SVRG method to use reuse stochastic gradients
efﬁciently. Figure 3(a) and Figure 3(b) summarize our ﬁndings on a hypothetic experiment.

6For any other coordinate j (cid:54)= j∗  it satisﬁes
7Each epoch of SVRG consists of a full gradient computation and 2n iterations  totaling 3n computations of

e(rj−yj )pi j /(piµ)
(rj∗ −yj∗ )pi j∗ /(piµ) ≤ e−θ and hence is negligible.

(new) stochastic gradients. (We do not count the computation of ∇fi(0) at x = 0.)

e

8

5E-125E-115E-105E-095E-085E-075E-065E-055E-04051015202530Optimization Error#grad/nSVRG:0.0001SVRG:0.0003SVRG:0.0005SAGA:0.0001SAGA:0.0003SAGA:0.0005SVRG_lin:0.00051E-61E-51E-41E-31E-21E-1051015202530Primal Error#grad/nSVRGSAGASVRG_linAcknowledgements

We would like to thank Greg Yang  Ilya Razenshteyn and S´ebastien Bubeck for discussing the
motivation of this problem.

References
[1] Shipra Agrawal and Nikhil R Devanur. Fast Algorithms for Online Stochastic Convex Pro-

gramming. In SODA  pages 1405–1424  2015.

[2] Shipra Agrawal  Zizhuo Wang  and Yinyu Ye. A dynamic near-optimal algorithm for online

linear programming. Operations Research  62(4):876–890  2014.

[3] Saeed Alaei  MohammadTaghi Hajiaghayi  and Vahid Liaghat. Online prophet-inequality
In Proceedings of the 13th ACM Conference

matching with applications to ad allocation.
on Electronic Commerce  pages 18–35. ACM  2012.

[4] Zeyuan Allen-Zhu and Yang Yuan.

Improved SVRG for Non-Strongly-Convex or Sum-of-

Non-Convex Objectives. In ICML  2016.

[5] Zeyuan Allen-Zhu  Yang Yuan  and Karthik Sridharan. Exploiting the Structure: Stochastic

Gradient Methods Using Raw Clusters. In NeurIPS  2016.

[6] Wei Chu  Seung-Taek Park  Todd Beaupre  Nitin Motgi  Amit Phadke  Seinjuti Chakraborty 
and Joe Zachariah. A Case Study of Behavior-driven Conjoint Analysis on Yahoo!: Front Page
Today Module. In KDD  pages 1097–1104  2009.

[7] Dragos Florin Ciocan and Vivek Farias. Model predictive control for dynamic resource allo-

cation. Mathematics of Operations Research  37(3):501–525  2012.

[8] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A Fast Incremental Gradient

Method With Support for Non-Strongly Convex Composite Objectives. In NeurIPS  2014.

[9] Nikhil R Devanur and Thomas P Hayes. The adwords problem: online keyword matching with
budgeted bidders under random permutations. In Proceedings of the 10th ACM conference on
Electronic commerce  pages 71–78. ACM  2009.

[10] Nikhil R Devanur  Balasubramanian Sivan  and Yossi Azar. Asymptotically optimal algorithm
for stochastic adwords. In Proceedings of the 13th ACM Conference on Electronic Commerce 
pages 388–404. ACM  2012.

[11] Jon Feldman  Aranyak Mehta  Vahab Mirrokni  and S Muthukrishnan. Online Stochastic

Matching: Beating 1-1/e. In FOCS  pages 117–126  2009.

[12] Jon Feldman  Monika Henzinger  Nitish Korula  Vahab Mirrokni  and Cliff Stein. Online
stochastic packing applied to display ad allocation. Algorithms–ESA 2010  pages 182–194 
2010.

[13] Kris Johnson Ferreira  David Simchi-Levi  and He Wang. Online network revenue manage-

ment using Thompson sampling. manuscript on SSRN  2016.

[14] Bernhard Haeupler  Vahab S Mirrokni  and Morteza Zadimoghaddam. Online Stochastic
Weighted Matching: Improved Approximation Algorithms. In WINE  volume 11  pages 170–
181. Springer  2011.

[15] Reza Harikandeh  Mohamed Osama Ahmed  Alim Virani  Mark Schmidt  Jakub Koneˇcn`y  and
Scott Sallinen. Stop wasting my gradients: Practical SVRG. In NeurIPS  pages 2251–2259 
2015.

[16] Thomas Hofmann  Aurelien Lucchi  Simon Lacoste-Julien  and Brian McWilliams. Variance
reduced stochastic gradient descent with neighbors. In NeurIPS 2015  pages 2296–2304  2015.

[17] Patrick Jaillet and Xin Lu. Online Stochastic Matching: New Algorithms with Better Bounds.

Mathematics of Operations Research  39(3):624–646  2014.

9

[18] Stefanus Jasin and Sunil Kumar. A re-solving heuristic with bounded revenue loss for network
revenue management with customer choice. Mathematics of Operations Research  37(2):313–
345  2012.

[19] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems  NeurIPS 2013  pages
315–323  2013.

[20] Lihua Lei and Michael I. Jordan. Less than a single pass: Stochastically controlled stochastic

gradient method. In AISTATS  pages 148–156  2017.

[21] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Nonconvex Finite-Sum Optimization

Via SCSG Methods. In NeurIPS  2017.

[22] Lihong Li  Wei Chu  John Langford  and Robert E. Schapire. A Contextual-bandit Approach
to Personalized News Article Recommendation. In WWW  pages 661–670  New York  NY 
USA  2010.

[23] Mehrdad Mahdavi  Lijun Zhang  and Rong Jin. Mixed optimization for smooth functions. In

Advances in Neural Information Processing Systems  pages 674–682  2013.

[24] Vahideh H Manshadi  Shayan Oveis Gharan  and Amin Saberi. Online stochastic matching:
Online actions based on ofﬂine statistics. Mathematics of Operations Research  37(4):559–
573  2012.

[25] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course  vol-

ume I. Kluwer Academic Publishers  2004. ISBN 1402075537.

[26] Martin I Reiman and Qiong Wang. An asymptotically optimal policy for a quantity-based
network revenue management problem. Mathematics of Operations Research  33(2):257–282 
2008.

[27] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic

average gradient. ArXiv e-prints  abs/1309.2388  September 2013.

[28] Shai Shalev-Shwartz. SDCA without Duality  Regularization  and Individual Convexity. In

ICML  2016.

[29] Shai Shalev-Shwartz and Tong Zhang. Proximal Stochastic Dual Coordinate Ascent. ArXiv

e-prints  abs/1211.2717  2012.

[30] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regular-

ized loss minimization. Journal of Machine Learning Research  14:567–599  2013.

[31] Clifford Stein  Van-Anh Truong  and Xinshang Wang. Advance Service Reservations with

Heterogeneous Customers. ArXiv e-prints  abs/1805.05554  2016.

[32] Kalyan Talluri and Garrett van Ryzin. An Analysis of Bid-Price Controls for Network Revenue

Management. Management Science  44(11-part-1):1577–1593  1998.

[33] Xinshang Wang  Van-Anh Truong  and David Bank. Online advance admission scheduling for

services  with customer preferences. ArXiv e-prints  abs/1805.10412  2015.

[34] Xinshang Wang  Van-Anh Truong  Shenghuo Zhu  and Qiong Zhang. Dynamic Optimization

of Mobile Push Campaigns. Working paper  2016.

[35] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance

Reduction. SIAM Journal on Optimization  24(4):2057—-2075  2014.

[36] Lijun Zhang  Mehrdad Mahdavi  and Rong Jin. Linear convergence with condition number
independent access of full gradients. In Advances in Neural Information Processing Systems 
pages 980–988  2013.

[37] Wenliang Zhong  Rong Jin  Cheng Yang  Xiaowei Yan  Qi Zhang  and Qiang Li. Stock Con-

strained Recommendation in Tmall. KDD  pages 2287–2296  2015.

10

,Finnian Lattimore
Tor Lattimore
Mark Reid
Zeyuan Allen-Zhu
David Simchi-Levi
Xinshang Wang