2018,The Lingering of Gradients: How to Reuse Gradients Over Time,Classically  the time complexity of a first-order method is estimated by its number of gradient computations. In this paper  we study a more refined complexity by taking into account the ``lingering'' of gradients: once a gradient is computed at $x_k$  the additional time to compute gradients at $x_{k+1} x_{k+2} \dots$ may be reduced.

We show how this improves the running time of gradient descent and SVRG. For instance  if the "additional time'' scales linearly with respect to the traveled distance  then the "convergence rate'' of gradient descent can be improved from $1/T$ to $\exp(-T^{1/3})$. On the empirical side  we solve a hypothetical revenue management problem on the Yahoo! Front Page Today Module application with 4.6m users to $10^{-6}$ error (or $10^{-12}$ dual error) using 6 passes of the dataset.,The Lingering of Gradients:

How to Reuse Gradients over Time

Zeyuan Allen-Zhuâˆ—
Microsoft Research AI
Redmond  WA 98052

David Simchi-Leviâˆ—

MIT

Cambridge  MA 02139

zeyuan@csail.mit.edu

dslevi@mit.edu

Xinshang Wangâˆ—

MIT

Cambridge  MA 02139
xinshang@mit.edu

Abstract

Classically  the time complexity of a ï¬rst-order method is estimated by its number
of gradient computations. In this paper  we study a more reï¬ned complexity by
taking into account the â€œlingeringâ€ of gradients: once a gradient is computed at
xk  the additional time to compute gradients at xk+1  xk+2  . . . may be reduced.
We show how this improves the running time of gradient descent and SVRG. For
instance  if the â€œadditional timeâ€ scales linearly with respect to the traveled dis-
tance  then the â€œconvergence rateâ€ of gradient descent can be improved from 1/T
to exp(âˆ’T 1/3). On the empirical side  we solve a hypothetical revenue manage-
ment problem on the Yahoo! Front Page Today Module application with 4.6m
users to 10âˆ’6 error (or 10âˆ’12 dual error) using 6 passes of the dataset.

1

Introduction

(cid:110)

(cid:80)n

(cid:111)

First-order methods play a fundamental role in large-scale machine learning and optimization tasks.
In most scenarios  the performance of a ï¬rst-order method is represented by its convergence rate:
the relationship between Îµ (the optimization error) versus T (the number of gradient computations).
This is meaningful because in most applications  the time complexities for evaluating gradients at
different points are of the same magnitude. In other words  the worse-case time complexities of
ï¬rst-order methods are usually proportional to a ï¬xed parameter times T .
In large-scale settings  however  if we have already spent time computing the (full) gradient at x 
perhaps we can use such information to reduce the time complexity to compute full gradients at
other points near x. We call this the â€œlingeringâ€ of gradients  because the gradient at x may be
partially reused for future consideration  but will eventually fade away once we are far from x.
Formally  consider the (ï¬nite-sum) stochastic convex minimization problem:

minxâˆˆRd

f (x) def= 1
n

i=1 fi(x)

.

(1.1)

Then  could it be possible that whenever x is sufï¬ciently close to y  for at least a large fraction of
indices i âˆˆ [n]  we have âˆ‡fi(x) â‰ˆ âˆ‡fi(y)? In other words  if âˆ‡f1(x)  . . .  âˆ‡fn(x) are already
calculated at some point x  can we reuse a large fraction of them to approximate âˆ‡f (y)?
Example 1.
In classiï¬cation problems  fi(x) represents the loss value for â€œhow well training
sample i is classiï¬ed under predictor xâ€. For any sample i that has a large margin under predictor x 
its gradient âˆ‡fi(x) may stay close to âˆ‡fi(y) whenever x is close to y.
Formally  let fi(x) = max{0  1âˆ’(cid:104)x  ai(cid:105)} be the hinge loss (or its smoothed variant if needed) with
respect to the i-th sample ai âˆˆ Rd. If the margin |1 âˆ’ (cid:104)x  ai(cid:105)| is sufï¬ciently large  then moving
âˆ—Authors sorted in alphabetical order. Full version of this paper (containing additional theoretical results 

additional experiments  and missing proofs) available at https://arxiv.org/abs/1901.02871.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  MontrÂ´eal  Canada.

from x to a nearby point y should not affect the sign of 1âˆ’(cid:104)x  ai(cid:105)  and thus not change the gradient.
Therefore  if samples a1  . . .   an are sufï¬ciently spread out in the space  then a large fraction of them
should incur large margins  and thus have the same gradients when x changes by a small amount.

In revenue management problems  fi(x) represents the marginal proï¬t of the i-th
+ over d items. In many applications (see Section 2.2) 

Example 2.
customer under bid-price strategy x âˆˆ Rd
âˆ‡fi(x) only depends on customer iâ€™s preferences under x.
If the bid-price vector x âˆˆ Rd
+ changes by a small amount to y  then for a large fraction of customers
i  their most proï¬table items may not change  and thus âˆ‡fi(x) â‰ˆ âˆ‡fi(y). (Indeed  imagine if one
of the items is Xbox  and its price drops by 5%  perhaps 90% of the customers will not change their
minds about buying or not. We shall demonstrate this using real-life data.)

1.1 Our Results
We assume in this paper that  given any point x âˆˆ Rd and index i âˆˆ [n]  one can efï¬ciently evaluate
a â€œlingering radiusâ€ Î´(x  i). The radius satisï¬es the condition that for every point y that is within
distance Î´(x  i) from x  the stochastic gradient âˆ‡fi(y) is equal to âˆ‡fi(x). We make two remarks:
â€¢ We use â€œequal toâ€ for the purpose of proving theoretical results. In practice and in our experi-
ments  it sufï¬ces to use approximate equality such as (cid:107)âˆ‡fi(x) âˆ’ âˆ‡fi(y)(cid:107) â‰¤ 10âˆ’10.
â€¢ By â€œefï¬cientâ€ we mean Î´(x  i) is computable in the same complexity as evaluating âˆ‡fi(x). This
is reasonable because when âˆ‡fi(x) is an explicit function of x  it is usually easy to tell how
sensitive it is to the input x. (We shall include such an example in our experiments.)

If we denote by B(x  r) the set of indices j satisfying Î´(x  j) < r  and if we travel to some point y
that is at most distance r from x  then we only need to re-evaluate the (stochastic) gradients âˆ‡fj(y)
for j âˆˆ B(x  r). Intuitively  one should expect |B(x  r)| to grow as a function of r  and this is indeed
the case â€“ for instance  for the revenue management problem (see Section 5).

Theory. To present the simplest theoretical result  we modify gradient descent (GD) to take into
account the lingering of gradients. At a high level  we run GD  but during its execution  we maintain
a decomposition of the indices Î›0 âˆª Â·Â·Â· âˆª Î›t = {1  2  . . .   n} where t is logarithmic in n. Now 
whenever we need âˆ‡fi(xk) for some i âˆˆ Î›p  we approximate it by âˆ‡fi(xk(cid:48)) for a point k(cid:48) that was
visited at most 2p steps ago. Our algorithm makes sure that such âˆ‡fi(xk(cid:48)) is available in memory.
We prove that the performance of our algorithm depends on how |B(x  r)| grows in r. Formally  let
T be the total number of stochastic gradient computations divided by n  and suppose |B(x  r)| â‰¤
O(r)  i.e.  it linearly scales in the radius r. Then  our algorithm ï¬nds a point x with f (x)âˆ’ f (xâˆ—) â‰¤
2âˆ’â„¦(T 1/3). In contrast  traditional GD satisï¬es f (x) âˆ’ f (xâˆ—) â‰¤ O(T âˆ’1).
In the full version of this paper  we also study the case when |B(x  r)| â‰¤ O(rÎ²) for an arbitrary
constant Î² âˆˆ (0  1].
Practice. We also design an algorithm that practically maximizes the use of gradient lingering. We
take the SVRG method [19  36] as the prototype because it is widely applied in large-scale settings.

Recall that SVRG uses gradient estimator âˆ‡f ((cid:101)x) âˆ’ âˆ‡fi((cid:101)x) + âˆ‡fi(xk) to estimate the full gradient
âˆ‡f (xk)  where(cid:101)x is the so-called snapshot point (which was visited at most n steps ago) and i is a
whose stochastic gradients need to be recomputed  and ignore those such that âˆ‡fi(xk) = âˆ‡fi((cid:101)x).

random index. At a high level  we modify SVRG so that the index i is only generated from those

This can further reduce the variance of the gradient estimator  and improve the running time.

1.2 Related Work

Variance Reduction. The SVRG method was independently proposed by Johnson and Zhang
[19]  Zhang et al. [36]  and belong to the class of stochastic methods using the so-called variance-
reduction technique [4  8  19  23  27â€“30  35  36]. The common idea behind these methods is to use
some full gradient of the past to approximate future  but they do not distinguish which âˆ‡fi(x) can
â€œlinger longer in timeâ€ among all indices i âˆˆ [n] for different x.

2

Arguably the two most widely applied variance-reduction methods are SVRG and SAGA [8]. They
have complementary performance depending on the internal structural of the dataset [5]  so we
compare to both in our experiments.
Reuse Gradients. Some researchers have exploited the internal structure of the dataset to speed up
ï¬rst-order methods. That is  they use âˆ‡fi(x) to approximate âˆ‡fj(x) when the two data samples i
and j are sufï¬ciently close. This is orthogonal to our setting because we use âˆ‡fi(x) to approximate
âˆ‡fi(y) when x and y are sufï¬ciently close.
In the extreme case when all the data samples are
identical  they have âˆ‡fi(x) = âˆ‡fj(x) for every i  j and thus stochastic gradient methods converge
as fast as full gradient ones. For this problem  Hofmann et al. [16] introduced a variant of SAGA 
Allen-Zhu et al. [5] introduced a variant of SVRG and a variant of accelerated coordinate descent.
Other authors study how to reduce gradient computations at the snapshot points of SVRG [15  20].
This is also orthogonal to the idea of this paper  and can be added to our algorithms for even better
performance (see Section 5).

2 Notions and Problem Formulation
We denote by (cid:107) Â· (cid:107) the Euclidean norm  and (cid:107) Â· (cid:107)âˆ the inï¬nity norm. Recall the notion of Lipschitz
smoothness (it has other equivalent deï¬nitions  see textbook [25]).
Deï¬nition 2.1. A function f : Rd â†’ R is L-Lipschitz smooth (or L-smooth for short) if

âˆ€x  y âˆˆ Rd : (cid:107)âˆ‡f (x) âˆ’ âˆ‡f (y)(cid:107) â‰¤ L(cid:107)x âˆ’ y(cid:107) .
We also introduce the notion of â€œlowbit sequenceâ€ for a positive integer.
Deï¬nition 2.2. For positive integer k  let lowbit(k) def= 2i where i â‰¥ 0 is the maximum integer such
that k is integral multiple of 2i. For instance  lowbit(34) = 2  lowbit(12) = 4  and lowbit(8) = 8.
Given positive integer k  let the lowbit sequence of k be (k0  k1  . . .   kt) where

0 = k0 < k1 < Â·Â·Â· < kt = k and kiâˆ’1 = ki âˆ’ lowbit(ki) .

For instance  the lowbit sequence of 45 is (0  32  40  44  45).

2.1 Our Model
We propose the following model to capture the lingering of gradients. For every x âˆˆ Rd and index
i âˆˆ [n]  let Î´(x  i) â‰¥ 0 be the lingering radius of âˆ‡fi(x)  meaning that2

âˆ€y âˆˆ Rd with (cid:107)y âˆ’ x(cid:107) â‰¤ Î´(x  i)

it satisï¬es âˆ‡fi(x) = âˆ‡fi(y) .

In other words  as long as we travel within distance Î´(x  i) from x  the gradient âˆ‡fi(x) can be
reused to represent âˆ‡fi(y). Accordingly  for every x âˆˆ Rd and r â‰¥ 0  we denote by B(x  r) the set

of indices j satisfying Î´(x  j) < r. That is  B(x  r) def=(cid:8)j âˆˆ [n](cid:12)(cid:12) Î´(x  j) < r(cid:9) .

Our main assumption of this paper is that
Assumption 1. Each Î´(x  i) can be computed in the same time complexity as âˆ‡fi(x).
Under Assumption 1  if at some point x we have already computed âˆ‡fi(x) for all i âˆˆ [n]  then we
can compute Î´(x  i) as well for every i âˆˆ [n]  and sort the indices i âˆˆ [n] in the increasing order of
Î´(x  i). In the future  if we arrive at any point y  we can calculate r = (cid:107)x âˆ’ y(cid:107) and use

(cid:17)

(cid:16)(cid:80)

i(cid:54)âˆˆB(x r) âˆ‡fi(x) +(cid:80)

âˆ‡(cid:48) = 1

n

iâˆˆB(x r) âˆ‡fi(y)

to represent âˆ‡f (y). We stress that the time to compute âˆ‡(cid:48) is only proportional to |B(x  r)|.
We denote by Ttime the gradient complexity  which equals how many times âˆ‡fi(x) and Î´(x  i) are
calculated  divided by n. In computing âˆ‡(cid:48) above  the gradient complexity is |B(x  r)|/n. If we
always set Î´(x  i) = 0 then |B(x  r)| = n and the gradient complexity for computing âˆ‡(cid:48) remains
1. However  if the underlying Problem (1.1) is nice enough so that |B(x  r)| becomes an increasing
function of r (see Figure 2)  then we can hope to design faster algorithms.

2Recall that  in practice  one should replace the exact equality with  for instance  (cid:107)âˆ‡fi(x) âˆ’ âˆ‡fi(y)(cid:107) â‰¤

10âˆ’10. To present the simplest statements  we do not introduce such an extra parameter.

3

2.2 Revenue Management Problem

As a motivating example  consider a canonical revenue management problem of selling d resources
to n customers. Let bj â‰¥ 0 be the capacity of resource j âˆˆ [d]; let pi j âˆˆ [0  1] be the probability that
customer i âˆˆ [n] will purchase a unit of resource j if offered resource j; and let rj be the revenue
for each unit of resource j. We want to offer each customer one and only one candidate resource 
and let yi j be the probability we offer customer i resource j. The following is an LP relaxation for
this problem:

(cid:26) (cid:88)

max
yâ‰¥0

iâˆˆ[n] jâˆˆ[d]

rjpi jyi j

(cid:12)(cid:12)(cid:12)(cid:12)âˆ€j âˆˆ [d] 

(cid:88)

iâˆˆ[n]

(cid:94)âˆ€i âˆˆ [n] 

(cid:88)

jâˆˆ[d]

(cid:27)

pi jyi j â‰¤ bj

yi j = 1

(2.1)

This LP (2.1) and its variants have repeatedly found many applications  including adwords/ad al-
location problems [3  10  11  14  17  24  34  37]  and revenue management for airline and service
industries [7  13  18  26  31  33]. Some authors also study the online version of solving such LPs
[1  2  9  12].
A standard way to reduce (2.1) to convex optimization is by regularization (cf. [37]). Let us subtract
def= maxiâˆˆ[n] pi j

the maximization objective by R(x) def= Âµ(cid:80)
(cid:111)

and Âµ > 0 is some small regularization weight. Then  after transforming to the dual  we have

(cid:16) (rj âˆ’ xj)pi j

jâˆˆ[d] yi j log yi j  where pi

(cid:80)

n(cid:88)

iâˆˆ[n] pi

(cid:110)

(cid:17)

exp

piÂµ

.

(2.2)

pi Â· log Zi +

min
xâ‰¥0

Âµ

i=1

where Zi =

xjbj

d(cid:88)
yi j = exp(cid:0) (rjâˆ’xj )pi j

j=1

d(cid:88)
(cid:1)/Zi.

j=1

Any solution x (usually known as the bid price in operations management [32]) to (2.2) naturally
gives back a solution y for the primal (2.1)  by setting

(2.3)
If we let fi(x) def= Âµnpi Â· log Zi + (cid:104)x  b(cid:105)  then (2.2) reduces to Problem (1.1). We conduct empirical
studies on this revenue management problem in Section 5.

piÂµ

3 Our Modiï¬cation to Gradient Descent

(cid:80)n
then we can arrive at a point x with f (x) âˆ’ f (xâˆ—) â‰¤ O(cid:0)(cid:107)x0âˆ’xâˆ—(cid:107)2

In this section  we consider a convex function f (x) = 1
i=1 fi(x) that is L-smooth. Recall from
textbooks (e.g.  [25]) that if gradient descent (GD) is applied for T iterations  starting at x0 âˆˆ Rd 
n
T convergence rate.

(cid:1). This is the 1

T

To improve on this theoretical rate  we make the following assumption on B(x  r):
Assumption 2. There exists Î± âˆˆ [0  1]  C > 0 such that 

âˆ€x âˆˆ Rd  r â‰¥ 0 :

â‰¤ Ïˆ(r) def= max{Î±  r/C} .

|B(x  r)|

n

It says that |B(n  r)| is a growing function in r  and the growth rate is âˆ r. (In the full version we
investigate the more general case where the growth rate is âˆ rÎ² for arbitrary Î² âˆˆ (0  1].) We also
allow an additive term Î± to cover the case that an Î± fraction of the stochastic gradients always need
to be recalculated  regardless of the distance. We illustrate the meaningfulness of Assumption 2 in
Figure 2.
Our result of this section can be summarized as follows. Hiding (cid:107)x0âˆ’xâˆ—(cid:107)  L  C in the big-O notion 
and letting Ttime be the gradient complexity  we can modify GD so that it ï¬nds a point x with

f (x) âˆ’ f (xâˆ—) â‰¤ O(cid:0) Î±

+ 2âˆ’â„¦(Ttime)1/3(cid:1) .

Ttime

We emphasize that our modiï¬ed algorithm does not need to know Î± or C.

3.1 Algorithm Description
In classical gradient descent (GD)  starting from x0 âˆˆ Rd  one iteratively updates xk+1 â† xk âˆ’
Lâˆ‡f (xk). We propose GDlin (see Algorithm 1) which  at a high level  differs from GD in two ways:

1

4

(cid:80)n
i=1 fi(x) convex and L-smooth  starting vector x(0) âˆˆ Rd  number of epochs

(cid:5) it satisï¬es g = âˆ‡f (xk)

(cid:1)s(cid:101); and Î¾ â† C

m.

S â‰¥ 1  parameters C  D > 0.

x0 â† x(sâˆ’1); m â† (cid:100)(cid:0)1 + C2

Algorithm 1 GDlin(f  x(0)  S  C  D)
Input: f (x) = 1
n
Output: vector x âˆˆ Rd.
1: for s â† 1 to S do
2:
3:
4:
5:
6:
7:
8:
x(s) â† xm;
9:
10: return x = x(S).

xk+1 â† xk âˆ’ min(cid:8) Î¾(cid:107)g(cid:107)   1

g â† (cid:126)0 and gi â† (cid:126)0 for each i âˆˆ [n].
for k â† 0 to m âˆ’ 1 do

(cid:9)g

16D2

L

n

Calculate Î›k âŠ† [n] from x0  . . .   xk according to Deï¬nition 3.1.
for i âˆˆ Î›k do
g â† g +

and gi â† âˆ‡fi(xk).

âˆ‡fi(xk)âˆ’gi

(a)
Figure 1: Illustration of index sets Î›k

(b)

â€¢ It performs a truncated gradient descent with travel distance (cid:107)xk âˆ’ xk+1(cid:107) â‰¤ Î¾ per step.
â€¢ It speeds up the process of calculating âˆ‡f (xk) by using the lingering of past gradients.

Formally  GDlin consists of S epochs s = 1  2  . . .   S of growing length m = (cid:100)(cid:0)1 + C2

each epoch  it starts with x0 âˆˆ Rd and performs m truncated gradient descent steps

16D2

(cid:1)s(cid:7). In

xk+1 â† xk âˆ’ min(cid:8)

(cid:9) Â· âˆ‡f (xk) .

Î¾

(cid:107)âˆ‡f (xk)(cid:107)   1

L

We choose Î¾ = C/m to ensure that the worst-case travel distance (cid:107)xm âˆ’ x0(cid:107) is at most mÎ¾ = C.
In each iteration k = 0  1  . . .   mâˆ’ 1 of this epoch s  in order to calculate âˆ‡f (xk)  GDlin constructs
index sets Î›0  Î›1  . . .   Î›mâˆ’1 âŠ† [n] and recalculates only âˆ‡fi(xk) for those i âˆˆ Î›k. We formally
introduce index sets below  and illustrate them in Figure 1.
Deï¬nition 3.1. Given x0  x1  . . .   xmâˆ’1 âˆˆ Rd  we deï¬ne index subsets Î›0  . . . Î›mâˆ’1 âŠ† [n] as
follows. Let Î›0 = [n]. For each k âˆˆ {1  2  . . .   m âˆ’ 1}  if (k0  . . .   kt) is kâ€™s lowbit sequence from
Deï¬nition 2.2  then (recalling k = kt)

(cid:0)Bki(k âˆ’ ki) \ Bki (ktâˆ’1 âˆ’ ki)(cid:1) where Bk(r) def= Î›k âˆ© B(xk  r Â· Î¾) .

def=(cid:83)tâˆ’1

Î›k

i=0

3.2

Intuitions & Properties of Index Sets

We show in this paper that our construction of index sets satisfy the following three properties.
Lemma 3.2. The construction of Î›0  . . .   Î›mâˆ’1 ensures that g = âˆ‡f (xk) in each iteration k.

5

158096072809304ğš²ğŸÎ›1Î›2Î›3Î›4Î›5Î›6Î›7ğš²ğŸ–Î›9Î›10Î›11ğš²ğŸğŸÎ›13ğš²ğŸğŸ’ğš²ğŸğŸ“Î›16ğŸ=ğµ08ğŸ=ğµ012âˆ–ğµ0(8)ğŸ‘=ğµ014âˆ–ğµ0(12)ğŸ’=ğµ015âˆ–ğµ0(14)ğŸ“=ğµ84ğŸ”=ğµ86âˆ–ğµ84ğŸ•=ğµ87âˆ–ğµ86ğŸ–=ğµ122ğŸ—=ğµ123âˆ–ğµ12(2)ğŸ=ğµ141158096072809304ğš²ğŸğš²ğŸ–ğš²ğŸğŸğš²ğŸğŸ’ğš²ğŸğŸ“âŠ•âŠ•âŠ•âŠ•âŠ—âŠ—âŠ–âŠ™Claim 3.3. The gradient complexity to construct Î›0  . . .   Î›mâˆ’1 is O(cid:0) 1

k=0 |Î›k|(cid:1) under
(cid:80)mâˆ’1
(cid:80)mâˆ’1
k=0 |Î›k| â‰¤ O(Î±m + log2 m) .

n

(cid:3).

Assumption 1. The space complexity is O(n log n).
Lemma 3.4. Under Assumption 2  we have 1
n
Claim 3.3 is easy to verify. Indeed  for each Î›(cid:96) that is calculated  we can sort its indices j âˆˆ Î›(cid:96) in
the increasing order of Î´(xk  j).3 Now  whenever we calculate Bki (kâˆ’ki)\Bki(ktâˆ’1âˆ’ki)  we have

already sorted the indices in Î›ki  so can directly retrieve those j with Î´(xki  j) âˆˆ(cid:0)ktâˆ’1âˆ’ki  kâˆ’ki

As for the space complexity  in any iteration k  we only need to store (cid:100)log2 k(cid:101) index sets Î›(cid:96) for
(cid:96) < k. For instance  when calculating Î›15 (see Figure 1(b))  we only need to use Î›0  Î›8  Î›12  Î›14;
and from k = 16 onwards  we no longer need to store Î›1  . . .   Î›15.
Lemma 3.2 is technically involved to prove (see full version)  but we give a sketched proof by
picture. Take k = 15 as an example. As illustrated by Figure 1(b)  for every j âˆˆ [n] 
â€¢ If j belongs to Î›15 â€”i.e.  boxes 4  0  9  7 of Figure 1â€”
â€¢ If j belongs to Î›14 \ B14(1) â€”i.e.  âŠ• region of Figure 1(b)â€”

We have calculated âˆ‡fj(xk) so are ï¬ne.
We have âˆ‡fj(x15) = âˆ‡fj(x14) because (cid:107)x15 âˆ’ x14(cid:107) â‰¤ Î¾ and j (cid:54)âˆˆ B14(1). Therefore  we can
safely retrieve gj = âˆ‡fj(x14) to represent âˆ‡fj(x15).
We have âˆ‡fj(x15) = âˆ‡fj(x12) for similar reason above. Also  the most recent update of gj
was at iteration 12  so we can safely retrieve gj to represent âˆ‡fj(x15).

â€¢ If j belongs to Î›12 \ B12(3) â€”i.e.  âŠ— region of Figure 1(b)â€”

â€¢ And so on.
In sum  for all indices j âˆˆ [n]  we have gj = âˆ‡fj(xk) so g = g1+Â·Â·Â·+gn
Lemma 3.4 is also involved to prove (see full version)  but again should be intuitive from the picture.
The indices in boxes 1  2  3  4 of Figure 1 are disjoint  and belong to B(x0  15Î¾)  totaling at most
|B(x0  15Î¾)| â‰¤ nÏˆ(15Î¾). The indices in boxes 5  6  7 of Figure 1 are also disjoint  and belong to
B(x8  7Î¾)  totaling at most |B(x8  7Î¾)| â‰¤ nÏˆ(7Î¾). If we sum up the cardinality of these boxes by
carefully grouping them in this manner  then we can prove Lemma 3.4 using Assumption 2.

equals âˆ‡f (xk).

n

3.3 Convergence Theorem

So far  Lemma 3.4 shows we can reduce the gradient complexity from O(m) to (cid:101)O(1) for every m

steps of gradient descent. Therefore  we wish to set m as large as possible  or equivalently Î¾ = C/m
as small as possible. Unfortunately  when Î¾ is too small  it will impact the performance of truncated
gradient descent (see full version). This motivates us to start with small value of m and increase
it epoch by epoch. Indeed  as the number of epoch grows  f (x0) becomes closer to the minimum
f (xâˆ—)  and thus we can choose smaller values of Î¾.
Formally  we have
Theorem 3.5. Given any x(0) âˆˆ Rd and D > 0 that is an upper bound on (cid:107)x(0) âˆ’ xâˆ—(cid:107). Suppose
Assumption 1 and 2 are satisï¬ed with parameters C âˆˆ (0  D]  Î± âˆˆ [0  1]. Then  denoting by ms =

(cid:1)s(cid:101)  we have that GDlin(f  x0  S  C  D) outputs a point x âˆˆ Rd satisfying f (x)âˆ’f (xâˆ—) â‰¤
with gradient complexity Ttime = O(cid:0)(cid:80)S

(cid:100)(cid:0)1+ C2

s=1 Î±ms + log2 ms

(cid:1).

16D2

4LD2
mS

As simple corollaries  we have
Theorem 3.6. In the setting of Theorem 3.5  given any T â‰¥ 1  one can choose S so that GDlin ï¬nds
a point x in gradient complexity Ttime = O(T ) s.t.

f (x) âˆ’ f (xâˆ—) â‰¤ O(cid:0) LD4

C2

(cid:1) +

Â· Î±

LD2

and in this case GDlin gives back the convergence f (x) âˆ’ f (xâˆ—) â‰¤ O(cid:0) LD2

We remark here if Ïˆ(r) = 1 (so there is no lingering effect for gradients)  we can choose C = D

(cid:1) of GD.

2â„¦(C2T /D2)1/3

T

3Calculating those lingering radii Î´(xk  j) require gradient complexity |Î›(cid:96)| according to Assumption 1  and

the time for sorting is negligible.

6

.

T

4 Our Modiï¬cation to SVRG

In this section  we use Assumption 1 to improve the running time of SVRG [19  36]  one of the most
widely applied stochastic gradient methods in large-scale settings. The purpose of this section is
to construct an algorithm that works well in practice: to (1) work for any possible lingering radii
Î´(x  i)  (2) be identical to SVRG if Î´(x  i) â‰¡ 0  and (3) be faster than SVRG when Î´(x  i) is large.
Recall how the SVRG method works. Each epoch of SVRG consists of m iterations (m = 2n
in practice). Each epoch starts with a point x0 (known as the snapshot) where the full gradient
âˆ‡f (x0) is computed exactly. In each iteration k = 0  1  . . .   m âˆ’ 1 of this epoch  SVRG updates
xk+1 â† xk âˆ’ Î·g where Î· > 0 is the learning rate and g is the gradient estimator g = âˆ‡f (x0) +
âˆ‡fi(xk) âˆ’ âˆ‡fi(x0) for some i randomly drawn from [n]. Note that it satisï¬es Ei[g] = âˆ‡f (xk) so
g is an unbiased estimator of the gradient. In the next epoch  SVRG starts with xm of the previous
epoch.4 We denote by x(s) the value of x0 at the beginning of epoch s = 0  1  2  . . .   S âˆ’ 1.
Our Algorithm. Our algorithm SVRGlin (pseudocode in full version) maintains disjoint subsets
Hs âŠ† [n]  where each Hs includes the set of the indices i whose gradients âˆ‡fi(x(s)) from epoch s
can still be safely reused at present.
At the starting point x0 of an epoch s  we let Hs = [n]\(H0âˆªÂ·Â·Â·âˆªHsâˆ’1) and re-calculate gradients
âˆ‡fi(x0) only for i âˆˆ Hs; the remaining ones can be loaded from the memory. This computes the
full gradient âˆ‡f (x0). Then  we denote by m = 2|Hs| and perform only m iterations within epoch
s. We next discuss how to perform update xk â†’ xk+1 and maintain {Hs}s during each iteration.
â€¢ In each iteration k of this epoch  we claim that âˆ‡fi(xk) = âˆ‡fi(x0) for every i âˆˆ H0âˆªÂ·Â·Â·âˆªHs.5

Thus  we can uniformly sample i from [n]\(cid:0)H0âˆªÂ·Â·Â·âˆªHs
(cid:17)

(cid:1)  and construct an unbiased estimator

(cid:16)

g â† âˆ‡f (x0) +

1 âˆ’

[âˆ‡fi(xk) âˆ’ âˆ‡fi(x0)]

(cid:80)s
s(cid:48)=0 |Hs(cid:48)|

n

of the true gradient âˆ‡f (xk). Then  we update xk+1 â† xk âˆ’ Î·g the same way as SVRG.
We emphasize that the above choice of g reduces its variance (because there are fewer random
choices)  and it is known that reducing variance leads to faster running time [19].
â€¢ As for how to maintain {Hs}s  in each iteration k after xk+1 is computed  for every s(cid:48) â‰¤ s  we
wish to remove those indices i âˆˆ Hs(cid:48) such that the current position x lies outside of the lingering
radius of i  i.e.  Î´(x(s)  i) < (cid:107)x âˆ’ x(s)(cid:107). To efï¬ciently implement this  we need to make sure
that whenever Hs(cid:48) is constructed (at the beginning of epoch s(cid:48))  the algorithm sort all the indices
i âˆˆ Hs(cid:48) by the increasing order of Î´(x(s(cid:48))  i). We include implementation details in full version.

5 Preliminary Empirical Evaluation

In this section  we construct a revenue maximization
LP (2.1) using the publicly accessible dataset of Yahoo!
Front Page Today Module [6  22]. We describe details of
the experimental setup in full version. Based on this real-
life dataset  we validate Assumption 2 and our motivation
behind lingering gradients. We also test the performance
of SVRGlin from Section 4 on optimizing this LP.
Illustration of Lingering Radius. We calculate linger-
ing radii on the dual problem (2.2). Let Î¸ > 0 be a pa-
rameter large enough so that eâˆ’Î¸ can be viewed as zero.
(For instance  Î¸ = 20 gives eâˆ’20 â‰ˆ 2 Ã— 10âˆ’9.) Then  for
each point x âˆˆ Râ‰¥0 and index i âˆˆ [n]  we let

Figure 2: |B(x  r)|/n as a function of r. We
choose Î¸ = 5. Dashed curve is
when x = (cid:126)0  and solid curve is
when x is near the optimum.

(cid:8)(rjâˆ’xj)pi j

(cid:9) .

(rjâˆ— âˆ’ xjâˆ— )pi jâˆ— âˆ’ (rj âˆ’ xj)pi j âˆ’ Î¸piÂµ

Î´(x  i) = min

jâˆˆ[n] j(cid:54)=jâˆ—

pi jâˆ— + pi j

where

jâˆ— = arg max
jâˆˆ[n]

4Some authors use the average of x1  . . .   xm to start the next epoch  but we choose this simpler version.
5This is because for every i âˆˆ Hs  by deï¬nition of Hs we have âˆ‡fi(xk) = âˆ‡fi(x(s)) = âˆ‡fi(x0); for
every i âˆˆ Hs(cid:48) where s(cid:48) < s  we know âˆ‡fi(xk) = âˆ‡fi(x(s(cid:48))) but we also have âˆ‡fi(x0) = âˆ‡fi(x(s(cid:48)))
(because otherwise i would have been removed from Hs(cid:48)).

7

00.20.40.60.8100.010.020.030.04B(x r)/nr(a) SVRGlin vs. SVRG and SAGA.

(b) SVRGlin vs. SVRG and SAGA

Figure 3: Comparison of (a) dual objective optimality (for which different learning rates are presented) and (b)

primal objective optimality (for which the learning rates are best tuned).

âˆ‡fi(y) â‰ˆ (b1  . . .   bd) + npi jâˆ— ejâˆ—

It is now a simple exercise to verify that  denoting by ej the j-th basis unit vector  then6
for every (cid:107)y âˆ’ x(cid:107)âˆ â‰¤ Î´(x  i) .

In Figure 2  we plot |B(x  r)| =(cid:12)(cid:12)(cid:8)j âˆˆ [n](cid:12)(cid:12) Î´(x  j) < r(cid:9)(cid:12)(cid:12) as an increasing function of r. We see

that for practical data  |B(x  r)|/n is indeed bounded above by some increasing function Ïˆ(Â·). We
provide more justiï¬cation on why this happens in the full paper.
Remark 5.1. This Î´(x  i) differs from our deï¬nition in Section 2 in two ways. First  it ensures
âˆ‡fi(y) â‰ˆ âˆ‡fi(x) as opposed to exact equality; for practical purposes this is no big issue  and we
choose Î¸ = 5 in our experiments. Second  (cid:107)y âˆ’ x(cid:107)âˆ â‰¤ Î´(x  i) gives a bigger â€œsafe regionâ€ than
(cid:107)y âˆ’ x(cid:107) â‰¤ Î´(x  i); thus  when implementing SVRGlin  we adopt (cid:107) Â· (cid:107)âˆ as the norm of choice.

Numerical Experiments. We consider solving the dual problem (2.2). In Figure 3(a)  we plot the
optimization error of (2.2) as a function #grad/n  the number of stochastic gradient computations
divided by n  also known as #passes of dataset.
Figure 3(a) compares our SVRGlin to SVRG and SAGA (each for 3 best tuned step lengths).7 We can
see SVRGlin is close to SVRG or SAGA during the ï¬rst 5-7 passes of the data. This is because initially 
x moves fast and cannot usually stay in the lingering radii for most indices i. After that period 
SVRGlin requires a dramatically smaller number of gradient computations  as x moves slower and
slower  becoming more easily to stay in the lingering radii. It is interesting to note that SVRGlin does
not signiï¬cantly improve the optimization error as a function of number of epochs; the improvement
primarily lies in improving the number of gradient computations per epoch. The comparison is

More Plots.
In Figure 3(b)  we also compare the primal objective value for the LP (2.1). (We
explain how to get feasible primal solutions from the dual in the full version.) It is perhaps worth
noting that we have chosen Âµ = 10âˆ’5 as the regularization error  and the primal objective error
indeed reaches to 10âˆ’6 which is roughly Âµ. In the full version  we also compare the running time of
the algorithms. Those plots are almost identical to Figure 3(b).

6 Conclusion

In this paper  we study convex problems where the stochastic gradients âˆ‡fi(x) can be reused when
we move away from x. In our theoretical result  we model the number of stochastic gradients that can
be changed (and thus cannot be reused) as a function of how much distance we travel away from x 
and show faster convergence for gradient descent (in terms of the number of gradient computations).
On the empirical side  we show how to modify the SVRG method to use reuse stochastic gradients
efï¬ciently. Figure 3(a) and Figure 3(b) summarize our ï¬ndings on a hypothetic experiment.

6For any other coordinate j (cid:54)= jâˆ—  it satisï¬es
7Each epoch of SVRG consists of a full gradient computation and 2n iterations  totaling 3n computations of

e(rjâˆ’yj )pi j /(piÂµ)
(rjâˆ— âˆ’yjâˆ— )pi jâˆ— /(piÂµ) â‰¤ eâˆ’Î¸ and hence is negligible.

(new) stochastic gradients. (We do not count the computation of âˆ‡fi(0) at x = 0.)

e

8

5E-125E-115E-105E-095E-085E-075E-065E-055E-04051015202530Optimization Error#grad/nSVRG:0.0001SVRG:0.0003SVRG:0.0005SAGA:0.0001SAGA:0.0003SAGA:0.0005SVRG_lin:0.00051E-61E-51E-41E-31E-21E-1051015202530Primal Error#grad/nSVRGSAGASVRG_linAcknowledgements

We would like to thank Greg Yang  Ilya Razenshteyn and SÂ´ebastien Bubeck for discussing the
motivation of this problem.

References
[1] Shipra Agrawal and Nikhil R Devanur. Fast Algorithms for Online Stochastic Convex Pro-

gramming. In SODA  pages 1405â€“1424  2015.

[2] Shipra Agrawal  Zizhuo Wang  and Yinyu Ye. A dynamic near-optimal algorithm for online

linear programming. Operations Research  62(4):876â€“890  2014.

[3] Saeed Alaei  MohammadTaghi Hajiaghayi  and Vahid Liaghat. Online prophet-inequality
In Proceedings of the 13th ACM Conference

matching with applications to ad allocation.
on Electronic Commerce  pages 18â€“35. ACM  2012.

[4] Zeyuan Allen-Zhu and Yang Yuan.

Improved SVRG for Non-Strongly-Convex or Sum-of-

Non-Convex Objectives. In ICML  2016.

[5] Zeyuan Allen-Zhu  Yang Yuan  and Karthik Sridharan. Exploiting the Structure: Stochastic

Gradient Methods Using Raw Clusters. In NeurIPS  2016.

[6] Wei Chu  Seung-Taek Park  Todd Beaupre  Nitin Motgi  Amit Phadke  Seinjuti Chakraborty 
and Joe Zachariah. A Case Study of Behavior-driven Conjoint Analysis on Yahoo!: Front Page
Today Module. In KDD  pages 1097â€“1104  2009.

[7] Dragos Florin Ciocan and Vivek Farias. Model predictive control for dynamic resource allo-

cation. Mathematics of Operations Research  37(3):501â€“525  2012.

[8] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A Fast Incremental Gradient

Method With Support for Non-Strongly Convex Composite Objectives. In NeurIPS  2014.

[9] Nikhil R Devanur and Thomas P Hayes. The adwords problem: online keyword matching with
budgeted bidders under random permutations. In Proceedings of the 10th ACM conference on
Electronic commerce  pages 71â€“78. ACM  2009.

[10] Nikhil R Devanur  Balasubramanian Sivan  and Yossi Azar. Asymptotically optimal algorithm
for stochastic adwords. In Proceedings of the 13th ACM Conference on Electronic Commerce 
pages 388â€“404. ACM  2012.

[11] Jon Feldman  Aranyak Mehta  Vahab Mirrokni  and S Muthukrishnan. Online Stochastic

Matching: Beating 1-1/e. In FOCS  pages 117â€“126  2009.

[12] Jon Feldman  Monika Henzinger  Nitish Korula  Vahab Mirrokni  and Cliff Stein. Online
stochastic packing applied to display ad allocation. Algorithmsâ€“ESA 2010  pages 182â€“194 
2010.

[13] Kris Johnson Ferreira  David Simchi-Levi  and He Wang. Online network revenue manage-

ment using Thompson sampling. manuscript on SSRN  2016.

[14] Bernhard Haeupler  Vahab S Mirrokni  and Morteza Zadimoghaddam. Online Stochastic
Weighted Matching: Improved Approximation Algorithms. In WINE  volume 11  pages 170â€“
181. Springer  2011.

[15] Reza Harikandeh  Mohamed Osama Ahmed  Alim Virani  Mark Schmidt  Jakub KoneË‡cn`y  and
Scott Sallinen. Stop wasting my gradients: Practical SVRG. In NeurIPS  pages 2251â€“2259 
2015.

[16] Thomas Hofmann  Aurelien Lucchi  Simon Lacoste-Julien  and Brian McWilliams. Variance
reduced stochastic gradient descent with neighbors. In NeurIPS 2015  pages 2296â€“2304  2015.

[17] Patrick Jaillet and Xin Lu. Online Stochastic Matching: New Algorithms with Better Bounds.

Mathematics of Operations Research  39(3):624â€“646  2014.

9

[18] Stefanus Jasin and Sunil Kumar. A re-solving heuristic with bounded revenue loss for network
revenue management with customer choice. Mathematics of Operations Research  37(2):313â€“
345  2012.

[19] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive vari-
ance reduction. In Advances in Neural Information Processing Systems  NeurIPS 2013  pages
315â€“323  2013.

[20] Lihua Lei and Michael I. Jordan. Less than a single pass: Stochastically controlled stochastic

gradient method. In AISTATS  pages 148â€“156  2017.

[21] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Nonconvex Finite-Sum Optimization

Via SCSG Methods. In NeurIPS  2017.

[22] Lihong Li  Wei Chu  John Langford  and Robert E. Schapire. A Contextual-bandit Approach
to Personalized News Article Recommendation. In WWW  pages 661â€“670  New York  NY 
USA  2010.

[23] Mehrdad Mahdavi  Lijun Zhang  and Rong Jin. Mixed optimization for smooth functions. In

Advances in Neural Information Processing Systems  pages 674â€“682  2013.

[24] Vahideh H Manshadi  Shayan Oveis Gharan  and Amin Saberi. Online stochastic matching:
Online actions based on ofï¬‚ine statistics. Mathematics of Operations Research  37(4):559â€“
573  2012.

[25] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course  vol-

ume I. Kluwer Academic Publishers  2004. ISBN 1402075537.

[26] Martin I Reiman and Qiong Wang. An asymptotically optimal policy for a quantity-based
network revenue management problem. Mathematics of Operations Research  33(2):257â€“282 
2008.

[27] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ï¬nite sums with the stochastic

average gradient. ArXiv e-prints  abs/1309.2388  September 2013.

[28] Shai Shalev-Shwartz. SDCA without Duality  Regularization  and Individual Convexity. In

ICML  2016.

[29] Shai Shalev-Shwartz and Tong Zhang. Proximal Stochastic Dual Coordinate Ascent. ArXiv

e-prints  abs/1211.2717  2012.

[30] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regular-

ized loss minimization. Journal of Machine Learning Research  14:567â€“599  2013.

[31] Clifford Stein  Van-Anh Truong  and Xinshang Wang. Advance Service Reservations with

Heterogeneous Customers. ArXiv e-prints  abs/1805.05554  2016.

[32] Kalyan Talluri and Garrett van Ryzin. An Analysis of Bid-Price Controls for Network Revenue

Management. Management Science  44(11-part-1):1577â€“1593  1998.

[33] Xinshang Wang  Van-Anh Truong  and David Bank. Online advance admission scheduling for

services  with customer preferences. ArXiv e-prints  abs/1805.10412  2015.

[34] Xinshang Wang  Van-Anh Truong  Shenghuo Zhu  and Qiong Zhang. Dynamic Optimization

of Mobile Push Campaigns. Working paper  2016.

[35] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance

Reduction. SIAM Journal on Optimization  24(4):2057â€”-2075  2014.

[36] Lijun Zhang  Mehrdad Mahdavi  and Rong Jin. Linear convergence with condition number
independent access of full gradients. In Advances in Neural Information Processing Systems 
pages 980â€“988  2013.

[37] Wenliang Zhong  Rong Jin  Cheng Yang  Xiaowei Yan  Qi Zhang  and Qiang Li. Stock Con-

strained Recommendation in Tmall. KDD  pages 2287â€“2296  2015.

10

,Finnian Lattimore
Tor Lattimore
Mark Reid
Zeyuan Allen-Zhu
David Simchi-Levi
Xinshang Wang