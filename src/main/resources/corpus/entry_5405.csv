2017,Teaching Machines to Describe Images with Natural Language Feedback,Robots will eventually be part of every household.  It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper  we bring a human in the loop  and enable a human teacher to give feedback to a learning agent in the form of natural language. A descriptive sentence can provide a stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts.  We propose a phrase-based captioning model trained with policy gradients  and design a critic that provides reward to the learner by conditioning on the human-provided feedback. We show  that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.,Teaching Machines to Describe Images via Natural

Language Feedback

Huan Ling1  Sanja Fidler1 2

University of Toronto1  Vector Institute2
{linghuan fidler}@cs.toronto.edu

Abstract

Robots will eventually be part of every household. It is thus critical to enable
algorithms to learn from and be guided by non-expert users. In this paper  we
bring a human in the loop  and enable a human teacher to give feedback to a
learning agent in the form of natural language. We argue that a descriptive sentence
can provide a much stronger learning signal than a numeric reward in that it can
easily point to where the mistakes are and how to correct them. We focus on
the problem of image captioning in which the quality of the output can easily be
judged by non-experts. In particular  we ﬁrst train a captioning model on a subset
of images paired with human written captions. We then let the model describe new
images and collect human feedback on the generated descriptions. We propose a
hierarchical phrase-based captioning model  and design a feedback network that
provides reward to the learner by conditioning on the human-provided feedback.
We show that by exploiting descriptive feedback on new images our model learns
to perform better than when given human written captions on these images.

1

Introduction

In the era where A.I. is slowly ﬁnding its way into everyone’s lives  be in the form of social bots [36  2] 
personal assistants [24  13  32]  or household robots [1]  it becomes critical to allow non-expert users
to teach and guide their robots [37  18]. For example  if a household robot keeps bringing food served
on an ashtray thinking it’s a plate  one should ideally be able to educate the robot about its mistakes 
possibly without needing to dig into the underlying software.
Reinforcement learning has become a standard way of training artiﬁcial agents that interact with an
environment. There have been signiﬁcant advances in a variety of domains such as games [31  25] 
robotics [17]  and even ﬁelds like vision and NLP [30  19]. RL agents optimize their action policies
so as to maximize the expected reward received from the environment. Training typically requires a
large number of episodes  particularly in environments with large action spaces and sparse rewards.
Several works explored the idea of incorporating humans in the learning process  in order to help
the reinforcement learning agent to learn faster [35  12  11  6  5]. In most cases  a human teacher
observes the agent act in an environment  and is allowed to give additional guidance to the learner.
This feedback typically comes in the form of a simple numerical (or “good”/“bad”) reward which is
used to either shape the MDP reward [35] or directly shape the policy of the learner [5].
In this paper  we aim to exploit natural language as a way to guide an RL agent. We argue that a
sentence provides a much stronger learning signal than a numeric reward in that it can easily point
to where the mistakes occur and suggests how to correct them. Such descriptive feedback can thus
naturally facilitate solving the credit assignment problem as well as to help guide exploration. Despite
its clear beneﬁts  very few approaches aimed at incorporating language in Reinforcement Learning.
In pioneering work  [22] translated natural language advice into a short program which was used to
bias action selection. While this is possible in limited domains such as in navigating a maze [22] or
learning to play a soccer game [15]  it can hardly scale to the real scenarios with large action spaces
requiring versatile language feedback.

Machine
( a cat ) ( sitting ) ( on a sidewalk ) ( next to a street . )

Human Teacher
Feedback: There is a dog on a sidewalk  not a cat.
Type of mistake: wrong object
Select the mistake area:
( a cat ) ( sitting ) ( on a sidewalk ) ( next to a street . )
Correct the mistake:
( a dog ) ( sitting ) ( on a sidewalk ) ( next to a street . )

Figure 1: Our model accepts feedback from a human teacher in the form of natural language. We generate
captions using the current snapshot of the model and collect feedback via AMT. The annotators are requested to
focus their feedback on a single word/phrase at a time. Phrases  indicated with brackets in the example  are part
or our captioning model’s output. We also collect information about which word the feedback applies to and its
suggested correction. This information is used to train our feedback network.

Here our goal is to allow a non-expert human teacher to give feedback to an RL agent in the form of
natural language  just as one would to a learning child. We focus on the problem of image captioning
in which the quality of the output can easily be judged by non-experts.
Towards this goal  we make several contributions. We propose a hierarchical phrase-based RNN as
our image captioning model  as it can be naturally integrated with human feedback. We design a web
interface which allows us to collect natural language feedback from human “teachers” for a snapshot of
our model  as in Fig. 1. We show how to incorporate this information in Policy Gradient RL [30]  and
show that we can improve over RL that has access to the same amount of ground-truth captions. Our
code and data will be released (http://www.cs.toronto.edu/~linghuan/feedbackImageCaption/)
to facilitate more human-like training of captioning models.

2 Related Work

Several works incorporate human feedback to help an RL agent learn faster. [35] exploits humans
in the loop to teach an agent to cook in a virtual kitchen. The users watch the agent learn and
may intervene at any time to give a scalar reward. Reward shaping [26] is used to incorporate this
information in the MDP. [6] iterates between “practice”  during which the agent interacts with the real
environment  and a critique session where a human labels any subset of the chosen actions as good or
bad. In [12]  the authors compare different ways of incorporating human feedback  including reward
shaping  Q augmentation  action biasing  and control sharing. The same authors implement their
TAMER framework on a real robotic platform [11]. [5] proposes policy shaping which incorporates
right/wrong feedback by utilizing it as direct policy labels. These approaches mostly assume that
humans provide a numeric reward  unlike in our work where the feedback is given in natural language.
A few attempts have been made to advise an RL agent using language. [22]’s pioneering work
translated advice to a short program which was then implemented as a neural network. The units in
this network represent Boolean concepts  which recognize whether the observed state satisﬁes the
constraints given by the program. In such a case  the advice network will encourage the policy to
take the suggested action. [15] incorporated natural language advice for a RoboCup simulated soccer
task. They too translate the advice in a formal language which is then used to bias action selection.
Parallel to our work  [7] exploits textual advice to improve training time of the A3C algorithm in
playing an Atari game. Recently  [37  18] incorporates human feedback to improve a text-based QA
agent. Our work shares similar ideas  but applies them to the problem of image captioning. In [27] 
the authors incorporate human feedback in an active learning scenario  however not in an RL setting.
Captioning represents a natural way of showing that our algorithm understands a photograph to a
non-expert observer. This domain has received signiﬁcant attention [8  39  10]  achieving impressive
performance on standard benchmarks. Our phrase model shares the most similarity with [16] 
but differs in that exploits attention [39]  linguistic information  and RL to train. Several recent
approaches trained the captioning model with policy gradients in order to directly optimize for the
desired performance metrics [21  30  3]. We follow this line of work. However  to the best of our
knowledge  our work is the ﬁrst to incorporate natural language feedback into a captioning model.

2

Figure 2: Our hierarchical phrase-based cap-
tioning model  composed of a phrase-RNN at
the top level  and a word-level RNN which out-
puts a sequence of words for each phrase. The
useful property of this model is that it directly
produces an output sentence segmented into
linguistic phrases. We exploit this information
while collecting and incorporating human feed-
back into the model. Our model also exploits
attention  and linguistic information (phrase
labels such as noun  preposition  verb  and con-
junction phrase). Please see text for details.

Related to our efforts is also work on dialogue based visual representation learning [40  41]  however
this work tackles a simpler scenario  and employs a slightly more engineered approach.
We stress that our work differs from the recent efforts in conversation modeling [19] or visual
dialog [4] using Reinforcement Learning. These models aim to mimic human-to-human conversations
while in our work the human converses with and guides an artiﬁcial learning agent.

3 Our Approach

Our framework consists of a new phrase-based captioning model trained with Policy Gradients that
incorporates natural language feedback provided by a human teacher. While a number of captioning
methods exist  we design our own which is phrase-based  allowing for natural guidance by a non-
expert. In particular  we argue that the strongest learning signal is provided when the feedback
describes one mistake at a time  e.g. a single wrong word or a phrase in a caption. An example can
be seen in Fig. 1. This is also how one most effectively teaches a learning child. To avoid parsing the
generated sentences at test time  we aim to predict phrases directly with our captioning model. We
ﬁrst describe our phrase-based captioner  then describe our feedback collection process  and ﬁnally
propose how to exploit feedback as a guiding signal in policy gradient optimization.

3.1 Phrase-based Image Captioning

Our captioning model  forming the base of our approach  uses a hierarchical Recurrent Neural
Network  similar to [34  14]. In [14]  the authors use a two-level LSTM to generate paragraphs 
while [34] uses it to generate sentences as a sequence of phrases. The latter model shares a similar
overall structure as ours  however  our model additionally reasons about the type of phrases and
exploits the attention mechanism over the image.
The structure of our model is best explained through Fig. 2. The model receives an image as input and
outputs a caption. It is composed of a phrase RNN at the top level  and a word RNN that generates a
sequence of words for each phrase. One can think of the phrase RNN as providing a “topic” at each
time step  which instructs the word RNN what to talk about.
Following [39]  we use a convolutional neural network in order to extract a set of feature vectors
a = (a1  . . .   an)  with aj a feature in location j in the input image. We denote the hidden state of
the phrase RNN at time step t with ht  and ht i to denote the i-th hidden state of the word RNN for
the t-th phrase. Computation in our model can be expressed with the following equations:

ht = fphrase(ht−1  lt−1  ct−1  et−1)
lt = softmax(fphrase−label(ht))
ct = fatt(ht  lt  a)

N(cid:124)
N
R
-
e
s
a
r
h
p

(cid:123)
(cid:122)

(cid:125)

et = fword−phrase(wt 1  . . .   wt end)

ht 0 = fphrase−word(ht  lt  ct)
ht i = fword(ht i−1  ct  wt i)
wt i = fout(ht i  ct  wt i−1)

N(cid:124)
N
R
-
d
r
o
w

(cid:123)
(cid:122)

(cid:125)

fphrase
fphrase−label
fatt
fphrase−word

LSTM  dim 256

3-layer MLP

2-layer MLP with ReLu
3-layer MLP with ReLu

fword
fout
fword−phrase mean+3-lay. MLP with ReLu

LSTM  dim 256
deep decoder [28]

3

Image

Ref. caption

Feedback

Corr. caption

Image

Ref. caption

Feedback

Corr. caption

( a woman ) ( is sit-
ting ) ( on a bench
) ( with a plate ) (
of food . )

the
What
woman
is
sitting on is
not visible.

( a woman ) ( is
sitting ) ( with a
plate ) ( of food .
)

( a man ) ( rid-
ing a motorcy-
cle ) ( on a city
street . )

is

There
a
man and a
woman.

( a man and a
woman ) ( riding a
motorcycle ) ( on
a city street . )

( a horse ) ( is
standing ) ( in a
barn ) ( in a ﬁeld
. )

There is no
barn. There
is a fence.

( a horse ) ( is
standing ) ( in
a fence ) ( in a
ﬁeld . )

(
( a man )
is swinging a
baseball bat ) (
on a ﬁeld . )

The baseball
player is not
swinging
a
bate.

( a man ) ( is play-
ing baseball ) ( on
a ﬁeld . )

Table 1: Examples of collected feedback. Reference caption comes from the MLE model.

Table 2: Statistics for our collected feedback information. The table on the right shows how many times the
feedback sentences mention words to be corrected and suggest correction.

Num. of evaluated examples (annot. round 1)

Evaluated as containing errors

To ask for feedback (annot. round 2)

Avg. num. of feedback rounds per image

Avg. num. of words in feedback sent.
Avg. num. of words needing correction

Avg. num. of modiﬁed words

9000
5150
4174
2.22
8.04
1.52
1.46

Something should be replaced

mistake word is in description
correct word is in description

Something is missing

missing word is in description

Something should be removed

removed word is in description

2999
2664
2674
334
246
841
779

feedback round: number of correction rounds for the same example  description: natural language feedback

Figure 3: Caption quality evalua-
tion by the human annotators. Plot on
the left shows evaluation for captions
generated with our reference model
(MLE). The right plot shows evalua-
tion of the human-corrected captions
(after completing at least one round
of feedback).

As in [39]  ct denotes a context vector obtained by applying the attention mechanism to the image.
This context vector essentially represents the image area that the model “looks at” in order to generate
the t-th phrase. This information is passed to both the word-RNN as well as to the next hidden state
of the phrase-RNN. We found that computing two different context vectors  one passed to the phrase
and one to the word RNN  improves generation by 0.6 points (in weighted metric  see Table 4) mainly
helping the model to avoid repetition of words. Furthermore  we noticed that the quality of attention
signiﬁcantly improves (1.5 points  Table 4) if we provide it with additional linguistic information. In
particular  at each time step t our phrase RNN also predicts a phrase label lt  following the standard
deﬁnition from the Penn Tree Bank. For each phrase  we predict one out of four possible phrase labels 
i.e.  a noun (NP)  preposition (PP)  verb (VP)  and a conjunction phrase (CP). We use additional
<EOS> token to indicate the end of the sentence. By conditioning on the NP label  we help the model
look at the objects in the image  while VP may focus on more global image information.
Above  wt i denotes the i-th word output of the word-RNN in the t-th phrase  encoded with a one-hot
vector. Note that we use an additional <EOP> token in word-RNN’s vocabulary  which signals the
end-of-phrase. Further  et encodes the generated phrase via simple mean-pooling over the words 
which provides additional word-level context to the next phrase. Details about the choices of the
functions are given in the table. Following [39]  we use a deep output layer [28] in the LSTM and
double stochastic attention.
Implementation details.
To train our hierarchical model  we ﬁrst process MS-COCO image
caption data [20] using the Stanford Core NLP toolkit [23]. We ﬂatten each parse tree  separate a
sentence into parts  and label each part with a phrase label (<NP>  <PP>  <CP>  <VP>). To simplify
the phrase structure  we merge some NPs to its previous phrase label if it is not another NP.
Pre-training. We pre-train our model using the standard cross-entropy loss. We use the ADAM
optimizer [9] with learning rate 0.001. We discuss Policy Gradient optimization in Subsec. 3.4.

3.2 Crowd-sourcing Human Feedback

We aim to bring a human in the loop when training the captioning model. Towards this  we create a
web interface that allows us to collect feedback information on a larger scale via AMT. Our interface

4

perfectaccecptablegrammarminor_errormajor_error050010001500200025003000evaluation after correctionFigure 4: The architecture of our feedback network (FBN) that classiﬁes each phrase (bottom left) in a sampled
sentence (top left) as either correct  wrong or not relevant  by conditioning on the feedback sentence.

is akin to that depicted in Fig. 1  and we provide further visualizations in the Appendix. We also
provide it online on our project page. In particular  we take a snapshot of our model and generate
captions for a subset of MS-COCO images [20] using greedy decoding. In our experiments  we take
the model trained with the MLE objective.
We do two rounds of annotation. In the ﬁrst round  the annotator is shown a captioned image and
is asked to assess the quality of the caption  by choosing between: perfect  acceptable  grammar
mistakes only  minor or major errors. We asked the annotators to choose minor and major error if the
caption contained errors in semantics  i.e.  indicating that the “robot” is not understanding the photo
correctly. We advised them to choose minor for small errors such as wrong or missing attributes or
awkward prepositions  and go with major errors whenever any object or action naming is wrong.
For the next (more detailed  and thus more costly) round of annotation  we only select captions which
are not marked as either perfect or acceptable in the ﬁrst round. Since these captions contain errors 
the new annotator is required to provide detailed feedback about the mistakes. We found that some of
the annotators did not ﬁnd errors in some of these captions  pointing to the annotator noise in the
process. The annotator is shown the generated caption  delineating different phrases with the “(” and
“)” tokens. We ask the annotator to 1) choose the type of required correction  2) write feedback in
natural language  3) mark the type of mistake  4) highlight the word/phrase that contains the mistake 
5) correct the chosen word/phrase  6) evaluate the quality of the caption after correction. We allow
the annotator to submit the HIT after one correction even if her/his evaluation still points to errors.
However  we plea to the good will of the annotators to continue in providing feedback. In the latter
case  we reset the webpage  and replace the generated caption with their current correction.
The annotator ﬁrst chooses the type of error  i.e.  something “ should be replaced”  “is missing”  or
“should be deleted”. (S)he then writes a sentence providing feedback about the mistake and how
it should be corrected. We require that the feedback is provided sequentially  describing a single
mistake at a time. We do this by restricting the annotator to only select mistaken words within a single
phrase (in step 4). In 3)  the annotator marks further details about the mistake  indicating whether it
corresponds to an error in object  action  attribute  preposition  counting  or grammar. For 4) and 5)
we let the annotator highlight the area of mistake in the caption  and replace it with a correction.
The statistics of the data is provided in Table 2  with examples shown in Table 1. An interesting fact
is that the feedback sentences in most cases mention both the wrong word from the caption  as well
as the correction word. Fig. 3 (left) shows evaluation of the caption quality of the reference (MLE)
model. Out of 9000 captions  5150 are marked as containing errors (either semantic or grammar) 
and we randomly choose 4174 for the second round of annotation (detailed feedback). Fig. 3 (left)
shows the quality of all the captions after correction  i.e. good reference captions as well as 4174
corrected captions as submitted by the annotators. Note that we only paid for one round of feedback 
thus some of the captions still contained errors even after correction. Interestingly  on average the
annotators still did 2.2 rounds of feedback per image (Table 2).
3.3 Feedback Network
Our goal is to incorporate natural language feedback into the learning process. The collected feedback
contains rich information of how the caption can be improved: it conveys the location of the mistake
and typically suggests how to correct it  as seen in Table 2. This provides strong supervisory signal
which we want to exploit in our RL framework. In particular  we design a neural network which will
provide additional reward based on the feedback sentence. We refer to it as the feedback network
(FBN). We ﬁrst explain our feedback network  and show how to integrate its output in RL.

5

Sampled caption
A cat on a sidewalk.
A dog on a sidewalk.
A cat on a sidewalk.

Feedback

There is a dog on a sidewalk not a cat.

Phrase
A cat
A dog
on a sidewalk

Prediction
wrong
correct
not relevant

Table 3: Example classif. of each phrase in a newly sampled caption into correct/wrong/not-relevant conditioned
on the feedback sentence. Notice that we do not need the image to judge the correctness/relevance of a phrase.

Note that RL training will require us to generate samples (captions) from the model. Thus  during
training  the sampled captions for each training image will change (will differ from the reference
MLE caption for which we obtained feedback for). The goal of the feedback network is to read a
newly sampled caption  and judge the correctness of each phrase conditioned on the feedback. We
make our FBN to only depend on text (and not on the image)  making its learning task easier. In
particular  our FBN performs the following computation:

(1)
(2)
(3)
(4)

fsent
fphrase
ff bn

hcaption
t
hf eedback
t

t and wf

3-layer MLP with dropout

+3-way softmax

LSTM  dim 256
linear+mean pool

= fsent(hcaption
= fsent(hf eedback

  wc
t )
  wf
t )
i 1  . . .   wc
i N )
T (cid:48)  qi  m)

t−1
t−1
qi = fphrase(wc
T   hf
oi = ff bn(hc
t denote the one-hot encoding of words in the sampled caption and feedback sentence 
Here  wc
respectively. By wc
i · we denote words in the i-th phrase of the sampled caption. FBN thus encodes
both the caption and feedback using an LSTM (with shared parameters)  performs mean pooling over
the words in a phrase to represent the phrase i  and passes this information through a 3-layer MLP.
The MLP additionally accepts information about the mistake type (e.g.  wrong object/action) encoded
as a one hot vector m (denoted as “extra information” in Fig. 4). The output layer of the MLP is a
3-way classiﬁcation layer that predicts whether the phrase i is correct  wrong  or not relevant (wrt
feedback sentence). An example output from FBN is shown in Table 3.
Implementation details. We train our FBN with the ground-truth data that we collected.
In
particular  we use (reference  feedback  marked phrase in reference caption) as an example of a wrong
phrase  (corrected sentence  feedback  marked phrase in corrected caption) as an example of the
correct phrase  and treat the rest as the not relevant label. Reference here means the generated caption
that we collected feedback for  and marked phrase means the phrase that the annotator highlighted
in either the reference or the corrected caption. We use the standard cross-entropy loss to train our
model. We use ADAM [9] with learning rate 0.001  and a batch size of 256. When a reference
caption has several feedback sentences  we treat each one as independent training data.

3.4 Policy Gradient Optimization using Natural Language Feedback

We follow [30  29] to directly optimize for the desired image captioning metrics using the Policy
Gradient technique. For completeness  we brieﬂy summarize it here [30].
One can think of an caption decoder as an agent following a parameterized policy pθ that selects an
action at each time step. An “action” in our case requires choosing a word from the vocabulary (for
the word RNN)  or a phrase label (for the phrase RNN). An “agent” (our captioning model) then
receives the reward after generating the full caption  i.e.  the reward can be any of the automatic
metrics  their weighted sum [30  21]  or in our case will also include the reward from feedback.
The objective for learning the parameters of the model is the expected reward received when com-
pleting the caption ws = (ws
t is the word sampled from the model at time step t):

1  . . .   ws

T ) (ws
L(θ) = −Ews∼pθ [r(ws)]

(5)
To optimize this objective  we follow the reinforce algorithm [38]  as also used in [30  29]. The
gradient of (5) can be computed as

∇θL(θ) = −Ews∼pθ [r(ws)∇θ log pθ(ws)] 

which is typically estimated by using a single Monte-Carlo sample:
∇θL(θ) ≈ −r(ws)∇θ log pθ(ws)

6

(6)

(7)

We follow [30] to deﬁne the baseline b as the reward obtained by performing greedy decoding:

ˆwt = arg max p(wt|ht)

b = r( ˆw) 
∇θL(θ) ≈ −(r(ws) − r( ˆw))∇θ log pθ(ws)

(8)

Note that the baseline does not change the expected gradient but can drastically reduce its variance.
Reward. We deﬁne two different rewards  one at the sentence level (optimizing for a performance
metrics)  and one at the phrase level. We use human feedback information in both. We ﬁrst deﬁne the
sentence reward wrt to a reference caption as a weighted sum of the BLEU scores:

(cid:88)

r(ws) = β

λi · BLEUi(ws  ref )

(9)

i

In particular  we choose λ1 = λ2 = 0.5  λ3 = λ4 = 1  λ5 = 0.3. As reference captions to compute
the reward  we either use the reference captions generated by a snapshot of our model which were
evaluated as not having minor and major errors  or ground-truth captions. The details are given in the
experimental section. We weigh the reward by the caption quality as provided by the annotators. In
particular  β = 1 for perfect (or GT)  0.8 for acceptable  and 0.6 for grammar/ﬂuency issues only.
We further incorporate the reward provided by the feedback network. In particular  our FBN allows
us to deﬁne the reward at the phrase level (thus helping with the credit assignment problem). Since
our generated sentence is segmented into phrases  i.e.  ws = wp
t denotes the
(sequence of words in the) t-th phrase  we deﬁne the combined phrase reward as:

P   where wp

2 . . . wp

1wp

r(wp

(10)
Note that FBN produces a classiﬁcation of each phrase. We convert this into reward  by assigning
correct to 1  wrong to −1  and 0 to not relevant. We do not weigh the reward by the conﬁdence of the
network  which might be worth exploring in the future. Our ﬁnal gradient takes the following form:

t ) = r(ws) + λf ff bn(ws  f eedback  wp
t )

∇θL(θ) = − P(cid:88)

p=1

(r(wp) − r( ˆwp))∇θ log pθ(wp)

(11)

Implementation details. We use Adam with learning rate 1e−6 and batch size 50. As in [29]  we
follow an annealing schedule. We ﬁrst optimize the cross entropy loss for the ﬁrst K epochs  then
for the following t = 1  . . .   T epochs  we use cross entropy loss for the ﬁrst (P − f loor(t/m))
phrases (where P denotes the number of phrases)  and the policy gradient algorithm for the remaining
f loor(t/m) phrases. We choose m = 5. When a caption has multiple feedback sentences  we take
the sum of the FBN’s outputs (converted to rewards) as the reward for each phrase. When a sentence
does not have any feedback  we assign it a zero reward.
4 Experimental Results
To validate our approach we use the MS-COCO dataset [20]. We use 82K images for training  2K for
validation  and 4K for testing. In particular  we randomly chose 2K val and 4K test images from the
ofﬁcial validation split. To collect feedback  we randomly chose 7K images from the training set  as
well as all 2K images from our validation. In all experiments  we report the performance on our (held
out) test set. For all the models (including baselines) we used a pre-trained VGG [33] network to
extract image features. We use a word vocabulary size of 23 115.
Phrase-based captioning model. We analyze different instantiations of our phrase-based caption-
ing in Table 4  showing the importance of predicting phrase labels. To sanity check our model we
compare it to a ﬂat approach (word-RNN only) [39]. Overall  our model performs slightly worse
than [39] (0.66 points). However  the main strength of our model is that it allows a more natural
integration with feedback. Note that these results are reported for the models trained with MLE.
Feedback network. As reported in Table 2  our dataset which contains detailed feedback (descrip-
tions) contains 4173 images. We randomly select 9/10 of them to serve as a training set for our
feedback network  and use 1/10 of them to be our test set. The classiﬁcation performance of our FBN
is reported in Table 5. We tried exploiting additional information in the network. The second line
reports the result for FBN which also exploits the reference caption (for which the feedback was
written) as input  represented with a LSTM. The model in the third line uses the type of error  i.e.
the phrase is “missing”  “wrong”  or “redundant”. We found that by using information about what
kind of mistake the reference caption had (e.g.  corresponding to misnaming an object  action  etc)
achieves the best performance. We use this model as our FBN used in the following experiments.

7

ﬂat (word level) with att

phrase with att.

BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L Weighted metric
65.36
64.69
65.46
65.37

phrase with att +phrase label
phrase with 2 att +phrase label
Table 4: Comparing performance of the ﬂat captioning model [39]  and different instantiations of our phrase-
based captioning model. All these models were trained using the cross-entropy loss.

104.78
102.14
103.64
104.12

51.04
50.80
51.40
50.90

44.03
43.37
44.59
44.02

29.68
28.80
29.36
29.51

20.40
19.31
19.25
19.91

Feedback network
no extra information
use reference caption

use "missing"/"wrong"/"redundant"
use "action"/"object"/"preposition"/etc

Accuracy

73.30
73.24
72.92
74.66

Table 5: Classiﬁcation results of our feedback
network (FBN) on a held-out feedback data. The
FBN predicts correct/wrong/not relevant for each
phrase in a caption. See text for details.

RL with Natural Language Feedback.
In Table 6 we report the performance for several instan-
tiations of the RL models. All models have been pre-trained using cross-entropy loss (MLE) on
the full MS-COCO training set. For the next rounds of training  all the models are trained only
on the 9K images that comprise our full evaluation+feedback dataset from Table 2. In particular 
we separate two cases. In the ﬁrst  standard case  the “agent” has access to 5 captions for each
image. We experiment with different types of captions  e.g. ground-truth captions (provided by
MS-COCO)  as well as feedback data. For a fair comparison  we ensure that each model has access
to (roughly) the same amount of data. This means that we count a feedback sentence as one source
of information  and a human-corrected reference caption as yet another source. We also exploit
reference (MLE) captions which were evaluated as correct  as well as corrected captions obtained
from the annotators. In particular  we tried two types of experiments. We deﬁne “C” captions as all
captions that were corrected by the annotators and were not evaluated as containing minor or major
error  and ground-truth captions for the rest of the images. For “A”  we use all captions (including
reference MLE captions) that did not have minor or major errors  and GT for the rest. A detailed
break-down of these captions is reported in Table 7.
We ﬁrst test a model using the standard cross-entropy loss  but which now also has access to the
corrected captions in addition to the 5GT captions. This model (MLEC) is able to improve over the
original MLE model by 1.4 points. We then test the RL model by optimizing the metric wrt the 5GT
captions (as in [30]). This brings an additional point  achieving 2.4 over the MLE model. Our RL
agent with feedback is given access to 3GT captions  the “C" captions and feedback sentences. We
show that this model outperforms the no-feedback baseline by 0.5 points. Interestingly  with “A”
captions we get an additional 0.3 boost. If our RL agent has access to 4GT captions and feedback
descriptions  we achieve a total of 1.1 points over the baseline RL model and 3.5 over the MLE
model. Examples of generated captions are shown in Fig. 6.
We also conducted a human evaluation using AMT. In particular  Turkers are shown an image
captioned by the baseline RL and our method  and are asked to choose the better caption. As shown
in Fig. 5  our RL with feedback is 4.7 percent higher than the RL baseline. We additionally count
how much human interaction is required for either the baseline RL and our approach. In particular 
we count every interaction with the keyboard as 1 click. In evaluation  choosing the quality of the
caption counts as 1 click  and for captions/feedback  every letter counts as a click. The main save
comes from the ﬁrst evaluation round  in which we only as for the quality of captions. Overall  there
is almost half clicks saved in our setting.
We also test a more realistic scenario  in which the models have access to either a single GT caption 
or in our case “C" (or “A”) and feedback. This mimics a scenario in which the human teacher observes
the agent and either gives feedback about the agent’s mistakes  or  if the agent’s caption is completely
wrong  the teacher writes a new caption. Interestingly  RL when provided with the corrected captions
performs better than when given GT captions. Overall  our model outperforms the base RL (no
feedback) by 1.2 points. We note that our RL agents are trained (not counting pre-training) only on a
small (9K) subset of the full MS-COCO training set. Further improvements are thus possible.
Discussion. These experiments make an important point. Instead of giving the RL agent a com-
pletely new target (caption)  a better strategy is to “teach” the agent about the mistakes it is doing
and suggest a correction. Natural language thus offers itself as a rich modality for providing such
guidance not only to humans but also to artiﬁcial agents.

8

Table 6: Comparison of our RL with feedback information to baseline RL and MLE models.

MLE (5 GT)

MLEC (5 GT + C)
MLEC (5 GT + A)

RLB (5 GT)

RLF (3GT+FB+C)
RLF (3GT+FB+A)
RLF (4GT + FB)

RLB (1 GT)

RLB (C)
RLB (A)

RLF (C + FB)
RLF (A + FB)

.
t
n
e
s
5

.
t
n
e
s
1

BLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE-L Weighted metric
65.37
66.85
66.14
66.90
66.52
66.98
67.10
65.68
65.84
65.81
65.76
66.23

104.12
105.58
105.47
106.55
107.02
107.31
107.67
104.93
105.50
105.31
106.03
106.12

50.90
51.20
51.32
51.10
51.41
51.54
51.30
51.07
51.06
51.28
51.35
51.58

44.02
45.19
44.87
45.10
45.23
45.54
45.50
44.58
44.64
44.58
44.65
45.00

19.91
19.79
20.27
20.30
20.66
20.53
20.30
19.97
20.23
20.24
20.62
20.34

29.51
29.89
30.17
30.10
30.48
30.52
30.60
29.81
30.01
29.87
30.20
30.15

GT: ground truth captions; FB: feedback; MLE(A)(C): MLE model using ﬁve GT sentences + either C or A
captions (see text and Table 7); RLB: baseline RL (no feedback network); RLF: RL with feedback (here we

also use C or A captions as well as FBN);

ground-truth perfect acceptable grammar error only

A
C

3107
6326

2661
1502

2790
1502

442
234

Table 7: Detailed break-down of what
captions were used as “A” or “C” in Table 6
for computing additional rewards in RL.

(a)

(b)

Figure 5: (a) Human preferences: RL baseline vs RL with feedback (our approach)  (b) Number of human
“clicks” required for MLE/baseline RL  and ours. A click is counted when an annotator hits the keyboard: in
evaluation  choosing the quality of the caption counts as 1 click  and for captions/feedback  every letter counts as
a click. The main save comes from the ﬁrst evaluation round  in which we only as for the quality of captions.

MLE: ( a man ) ( walking ) ( in front of a
building ) ( with a cell phone . )
RLB: ( a man ) ( is standing ) ( on a sidewalk )
( with a cell phone . )
RLF: ( a man ) ( wearing a black suit ) ( and
tie ) ( on a sidewalk . )

MLE: ( a clock tower ) ( with a clock ) ( on
top . )
RLB: ( a clock tower ) ( with a clock ) ( on top
of it . )
RLF: ( a clock tower ) ( with a clock ) ( on the
front . )

MLE: ( two giraffes ) ( are standing ) ( in a
ﬁeld ) ( in a ﬁeld . )
RLB: ( a giraffe ) ( is standing ) ( in front of a
large building . )
RLF: ( a giraffe ) ( is ) ( in a green ﬁeld ) ( in a
zoo . )

MLE: ( two birds ) ( are standing ) ( on the
beach ) ( on a beach . )
RLB: ( a group ) ( of birds ) ( are ) ( on the
beach . )
RLF: ( two birds ) ( are standing ) ( on a beach
) ( in front of water . )

Figure 6: Qualitative examples of captions from the MLE and RLB models (baselines)  and our RBF model.

5 Conclusion
In this paper  we enable a human teacher to provide feedback to the learning agent in the form of
natural language. We focused on the problem of image captioning. We proposed a hierarchical
phrase-based RNN as our captioning model  which allowed natural integration with human feedback.
We crowd-sourced feedback for a snapshot of our model  and showed how to incorporate it in Policy
Gradient optimization. We showed that by exploiting descriptive feedback our model learns to
perform better than when given independently written captions.

Acknowledgment
We gratefully acknowledge the support from NVIDIA for their donation of the GPUs used for this research. This
work was partially supported by NSERC. We also thank Relu Patrascu for infrastructure support.

9

47.7 52.3 0 10 20 30 40 50 60 RLB RLF Human preferences 0 50000 100000 150000 200000 250000 300000 350000 400000 RLB RLF # of clicks References
[1] Cmu’s herb robotic platform  http://www.cmu.edu/herb-robot/.
[2] Microsoft’s tay  https://twitter.com/tayandyou.
[3] Bo Dai  Dahua Lin  Raquel Urtasun  and Sanja Fidler. Towards diverse and natural image descriptions via

a conditional gan. In arXiv:1703.06029  2017.

[4] A. Das  S. Kottur  K. Gupta  A. Singh  D. Yadav  J. M. Moura  D. Parikh  and D. Batra. Visual dialog. In

arXiv:1611.08669  2016.

[5] Shane Grifﬁth  Kaushik Subramanian  Jonathan Scholz  Charles L. Isbell  and Andrea Lockerd Thomaz.

Policy shaping: Integrating human feedback with reinforcement learning. In NIPS  2013.

[6] K. Judah  S. Roy  A. Fern  and T. Dietterich. Reinforcement learning via practice and critique advice. In

AAAI  2010.

[7] Russell Kaplan  Christopher Sauer  and Alexander Sosa. Beating atari with natural language guided

reinforcement learning. In arXiv:1704.05539  2017.

[8] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR 

2015.

[9] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[10] Ryan Kiros  Ruslan Salakhutdinov  and Richard S. Zemel. Unifying visual-semantic embeddings with

multimodal neural language models. CoRR  abs/1411.2539  2014.

[11] W. Bradley Knox  Cynthia Breazeal    and Peter Stone. Training a robot via human feedback: A case study.

In International Conference on Social Robotics  2013.

[12] W. Bradley Knox and Peter Stone. Reinforcement learning from simultaneous human and mdp reward. In

Intl. Conf. on Autonomous Agents and Multiagent Systems  2012.

[13] Jacqueline Kory Westlund  Jin Joo Lee  Luke Plummer  Fardad Faridi  Jesse Gray  Matt Berlin  Harald
Quintus-Bosz  Robert Hartmann  Mike Hess  Stacy Dyer  Kristopher dos Santos  Sigurdhur Örn Ad-
halgeirsson  Goren Gordon  Samuel Spaulding  Marayna Martinez  Madhurima Das  Maryam Archie 
Sooyeon Jeong  and Cynthia Breazeal. Tega: A social robot. In International Conference on Human-Robot
Interaction  2016.

[14] Jonathan Krause  Justin Johnson  Ranjay Krishna  and Li Fei-Fei. A hierarchical approach for generating

descriptive image paragraphs. In CVPR  2017.

[15] G. Kuhlmann  P. Stone  R. Mooney  and J. Shavlik. Guiding a reinforcement learner with natural language
advice: Initial results in robocup soccer. In AAAI Workshop on Supervisory Control of Learning and
Adaptive Systems  2004.

[16] Remi Lebret  Pedro O. Pinheiro  and Ronan Collobert.

arXiv:1502.03671  2015.

Phrase-based image captioning.

In

[17] Sergey Levine  Chelsea Finn  Trevor Darrell  and Pieter Abbeel. End-to-end training of deep visuomotor

policies. J. Mach. Learn. Res.  17(1):1334–1373  2016.

[18] Jiwei Li  Alexander H. Miller  Sumit Chopra  Marc’Aurelio Ranzato  and Jason Weston. Dialogue learning

with human-in-the-loop. In arXiv:1611.09823  2016.

[19] Jiwei Li  Will Monroe  Alan Ritter  Michel Galley  Jianfeng Gao  and Dan Jurafsky. Deep reinforcement

learning for dialogue generation. In arXiv:1606.01541  2016.

[20] Tsung-Yi Lin  Michael Maire  Serge Belongie  James Hays  Pietro Perona  Deva Ramanan  Piotr Dollár 
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV  pages 740–755. 2014.
[21] Siqi Liu  Zhenhai Zhu  Ning Ye  Sergio Guadarrama  and Kevin Murphy. Improved image captioning via

policy gradient optimization of spider. In arXiv:1612.00370  2016.

[22] Richard Maclin and Jude W. Shavlik. Incorporating advice into agents that learn from reinforcements. In

National Conference on Artiﬁcial Intelligence  pages 694–699  1994.

[23] Christopher D. Manning  Mihai Surdeanu  John Bauer  Jenny Finkel  Steven J. Bethard  and David

McClosky. The Stanford CoreNLP natural language processing toolkit. In ICLR  2014.

[24] Maja J. Matariˇc. Socially assistive robotics: Human augmentation vs. automation. Science Robotics  2(4) 

2017.

[25] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A. Rusu  Joel Veness  Marc G. Bellemare 
Alex Graves  Martin Riedmiller  Andreas K. Fidjeland  Georg Ostrovski  Stig Petersen  Charles Beattie 
Amir Sadik  Ioannis Antonoglou  Helen King  Dharshan Kumaran  Daan Wierstra  Shane Legg  and Demis
Hassabis. Human-level control through deep reinforcement learning. Nature  518(7540):529–533  2015.

10

[26] Andrew Y. Ng  Daishi Harada  and Stuart J. Russell. Policy invariance under reward transformations:

Theory and application to reward shaping. In ICML  pages 278–287  1999.

[27] Amar Parkash and Devi Parikh. Attributes for classiﬁer feedback. In European Conference on Computer

Vision (ECCV)  2012.

[28] Razvan Pascanu  Caglar Gulcehre  Kyunghyun Cho  and Yoshua Bengio. How to construct deep recurrent
neural networks. In Association for Computational Linguistics (ACL) System Demonstrations  pages 55–60 
2014.

[29] Marc’Aurelio Ranzato  Sumit Chopra  Michael Auli  and Wojciech Zaremba. Sequence level training with

recurrent neural networks. In arXiv:1511.06732  2015.

[30] Steven J. Rennie  Etienne Marcheret  Youssef Mroueh  Jarret Ross  and Vaibhava Goel. Self-critical

sequence training for image captioning. In arXiv:1612.00563  2016.

[31] David Silver  Aja Huang  Chris J. Maddison  Arthur Guez  Laurent Sifre  George van den Driessche 
Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanctot  Sander Dieleman  Dominik
Grewe  John Nham  Nal Kalchbrenner  Ilya Sutskever  Timothy Lillicrap  Madeleine Leach  Koray
Kavukcuoglu  Thore Graepel  and Demis Hassabis. Mastering the game of Go with deep neural networks
and tree search. Nature  529(7587):484–489  2016.

[32] Edgar Simo-Serra  Sanja Fidler  Francesc Moreno-Noguer  and Raquel Urtasun. Neuroaesthetics in fashion:

Modeling the perception of beauty. In CVPR  2015.

[33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. 23  2015.

[34] Ying Hua Tan and Chee Seng Chan. phi-lstm: A phrase-based hierarchical lstm model for image captioning.

In ACCV  2016.

[35] A. Thomaz and C. Breazeal. Reinforcement learning with human teachers: Evidence of feedback and

guidance. In AAAI  2006.

[36] Oriol Vinyals and Quoc Le. A neural conversational model. In arXiv:1506.05869  2015.
[37] Jason Weston. Dialog-based language learning. In arXiv:1604.06045  2016.
[38] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement

learning. In Machine Learning  1992.

[39] Kelvin Xu  Jimmy Ba  Ryan Kiros  Kyunghyun Cho  Aaron Courville  Ruslan Salakhutdinov  Richard
Zemel  and Yoshua Bengio. Show  attend and tell: Neural image caption generation with visual attention.
In ICML  2015.

[40] Yanchao Yu  Arash Eshghi  and Oliver Lemon. Training an adaptive dialogue policy for interactive learning

of visually grounded word meanings. In Proc. of SIGDIAL  2016.

[41] Yanchao Yu  Arash Eshghi  Gregory Mills  and Oliver Lemon. The burchak corpus: a challenge data set
for interactive learning of visually grounded word meanings. In Workshop on Vision and Language  2017.

11

,huan ling
Sanja Fidler