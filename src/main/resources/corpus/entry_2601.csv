2011,Improving Topic Coherence with Regularized Topic Models,Topic models have the potential to improve search and browsing by extracting useful semantic themes from web pages and other text documents. When learned topics are coherent and interpretable  they can be valuable for faceted browsing  results set diversity analysis  and document retrieval. However  when dealing with small collections or noisy text (e.g. web search result snippets or blog posts)  learned topics can be less coherent  less interpretable  and less useful. To overcome this  we propose two methods to regularize the learning of topic models. Our regularizers work by creating a structured prior over words that reflect broad patterns in the external data. Using thirteen datasets we show that both regularizers improve topic coherence and interpretability while learning a faithful representation of the collection of interest. Overall  this work makes topic models more useful across a broader range of text data.,Improving Topic Coherence with

Regularized Topic Models

David Newman

University of California  Irvine

newman@uci.edu

Edwin V. Bonilla

Wray Buntine

NICTA & Australian National University

{edwin.bonilla  wray.buntine}@nicta.com.au

Abstract

Topic models have the potential to improve search and browsing by extracting
useful semantic themes from web pages and other text documents. When learned
topics are coherent and interpretable  they can be valuable for faceted browsing 
results set diversity analysis  and document retrieval. However  when dealing with
small collections or noisy text (e.g. web search result snippets or blog posts) 
learned topics can be less coherent  less interpretable  and less useful. To over-
come this  we propose two methods to regularize the learning of topic models.
Our regularizers work by creating a structured prior over words that reﬂect broad
patterns in the external data. Using thirteen datasets we show that both regularizers
improve topic coherence and interpretability while learning a faithful representa-
tion of the collection of interest. Overall  this work makes topic models more
useful across a broader range of text data.

1

Introduction

Topic modeling holds much promise for improving the ways users search  discover  and organize
online content by automatically extracting semantic themes from collections of text documents.
Learned topics can be useful in user interfaces for ad-hoc document retrieval [18]; driving faceted
browsing [14]; clustering search results [19]; or improving display of search results by increasing
result diversity [10]. When the text being modeled is plentiful  clear and well written (e.g. large
collections of abstracts from scientiﬁc literature)  learned topics are usually coherent  easily under-
stood  and ﬁt for use in user interfaces. However  topics are not always consistently coherent  and
even with relatively well written text  one can learn topics that are a mix of concepts or hard to
understand [1  6]. This problem is exacerbated for content that is sparse or noisy  such as blog posts 
tweets  or web search result snippets. Take for instance the task of learning categories in clustering
search engine results. A few searches with Carrot2  Yippee  or WebClust quickly demonstrate that
consistently learning meaningful topic facets is a difﬁcult task [5].
Our goal in this paper is to improve the coherence  interpretability and ultimate usability of learned
topics. To achieve this we propose QUAD-REG and CONV-REG  two new methods for regularizing
topic models  which produce more coherent and interpretable topics. Our work is predicated on
recent evidence that a pointwise mutual information-based score (PMI-Score) is highly correlated
with human-judged topic coherence [15  16]. We develop two Bayesian regularization formula-
tions that are designed to improve PMI-Score. We experiment with ﬁve search result datasets from
7M Blog posts  four search result datasets from 1M News articles  and four datasets of Google
search results. Using these thirteen datasets  our experiments demonstrate that both regularizers
consistently improve topic coherence and interpretability  as measured separately by PMI-Score and
human judgements. To the best of our knowledge  our models are the ﬁrst to address the problem
of learning topics when dealing with limited and/or noisy text content. This work opens up new
application areas for topic modeling.

1

2 Topic Coherence and PMI-Score

Topics learned from a statistical topic model are formally a multinomial distribution over words 
and are often displayed by printing the 10 most probable words in the topic. These top-10 words
usually provide sufﬁcient information to determine the subject area and interpretation of a topic 
and distinguish one topic from another. However  topics learned on sparse or noisy text data are
often less coherent  difﬁcult to interpret  and not particularly useful. Some of these noisy topics
can be vaguely interpretable  but contain (in the top-10 words) one or two unrelated words – while
other topics can be practically incoherent. In this paper we wish to improve topic models learned on
document collections where the text data is sparse and/or noisy. We postulate that using additional
(possibly external) data will regularize the learning of the topic models.
Therefore  our goal is to improve topic coherence. Topic coherence – meaning semantic coherence
– is a human judged quality that depends on the semantics of the words  and cannot be measured
by model-based statistical measures that treat the words as exchangeable tokens. Fortunately  recent
work has demonstrated that it is possible to automatically measure topic coherence with near-human
accuracy [16  15] using a score based on pointwise mutual information (PMI). In that work they
showed (using 6000 human evaluations) that the PMI-Score broadly agrees with human-judged
topic coherence. The PMI-Score is motivated by measuring word association between all pairs of
words in the top-10 topic words. PMI-Score is deﬁned as follows:

(cid:88)

i<j

PMI-Score(w) =

1
45

PMI(wi  wj)  ij ∈ {1 . . . 10}

where PMI(wi  wj) = log

P (wi  wj)

P (wi)P (wj)

 

(1)

(2)

and 45 is the number of PMI scores over the set of distinct word pairs in the top-10 words. A key
aspect of this score is that it uses external data – that is data not used during topic modeling. This
data could come from a variety of sources  for example the corpus of 3M English Wikipedia articles.
For this paper  we will use both PMI-Score and human judgements to measure topic coherence.
Note that we can measure the PMI-Score of an individual topic  or for a topic model of T topics (in
that case PMI-Score will refer to the average of T PMI-Scores). This PMI-Score – and the idea of
using external data to measure it – forms the foundation of our idea for regularization.

3 Regularized Topic Models

In this section we describe our approach to regularization in topic models by proposing two dif-
ferent methods: (a) a quadratic regularizer (QUAD-REG) and (b) a convolved Dirichlet regularizer
(CONV-REG). We start by introducing the standard notation in topic modeling and the baseline
latent Dirichlet allocation method (LDA  [4  9]).

3.1 Topic Modeling and LDA

Topic models are a Bayesian version of probabilistic latent semantic analysis [11].
In standard
LDA topic modeling each of D documents in the corpus is modeled as a discrete distribution over T
latent topics  and each topic is a discrete distribution over the vocabulary of W words. For document
d  the distribution over topics  θt|d  is drawn from a Dirichlet distribution Dir[α]. Likewise  each
distribution over words  φw|t  is drawn from a Dirichlet distribution  Dir[β].
For the ith token in a document  a topic assignment  zid  is drawn from θt|d and the word  xid  is
drawn from the corresponding topic  φw|zid. Hence  the generative process in LDA is given by:

θt|d ∼ Dirichlet[α]
zid ∼ Mult[θt|d]

(3)
(4)
We can compute the posterior distribution of the topic assignments via Gibbs sampling by writ-
ing down the joint probability  integrating out θ and φ  and following a few simple mathematical
manipulations to obtain the standard Gibbs sampling update:
p(zid = t|xid = w  z¬i) ∝ N¬i
wt + β
N¬i
t + W β

φw|t ∼ Dirichlet[β]
xid ∼ Mult[φw|zid ].

(N¬i

td + α) .

(5)

2

where z¬i denotes the set of topic assignment variables except the ith variable; Nwt is the number
of times word w has been assigned to topic t; Ntd is the number of times topic t has been assigned

to document d  and Nt =(cid:80)W

w=1 Nwt.

Given samples from the posterior distribution we can compute point estimates of the document-topic
proportions θt|d and the word-topic probabilities φw|t. We will denote henceforth φt as the vector
of word probabilities for a given topic t and analogously for other variables.

3.2 Regularization via Structured Priors

To learn better topic models for small or noisy collections we introduce structured priors on φt based
upon external data  which has a regularization effect on the standard LDA model. More speciﬁcally 
our priors on φt will depend on the structural relations of the words in the vocabulary as given by
external data  which will be characterized by the W × W “covariance” matrix C. Intuitively  C
is a matrix that captures the short-range dependencies between words in the external data. More
importantly  we are only interested in relatively frequent terms from the vocabulary  so C will be a
sparse matrix and hence computations are still feasible for our methods to be used in practice.

3.3 Quadratic Regularizer (QUAD-REG)

Here we use a standard quadratic form with a trade-off factor. Therefore  given a matrix of word
dependencies C  we can use the prior:

p(φt|C) ∝ (cid:16)
W(cid:88)

φT

t Cφt

(cid:17)ν
(cid:16)

(cid:17)

for some power ν. Note we do not know the normalization factor but for our purposes of MAP
estimation we do not need it. The log posterior (omitting irrelevant constants) is given by:

LMAP =

Nit log φi|t + ν log

φT

t Cφt

.

i=1

Optimizing Equation (7) with respect to φw|t subject to the constraints(cid:80)W
(cid:33)

the following ﬁxed point update:
φw|t ←

(cid:80)W

Nwt + 2ν

(cid:32)

φw|t

1

i=1 Ciwφi|t
t Cφt

φT

Nt + 2ν

i=1 φi|t = 1  we obtain

.

(8)

(6)

(7)

We note that unlike other topic models in which a covariance or correlation structure is used (as
in the correlated topic model  [3]) in the context of correlated priors for θt|d  our method does not
require the inversion of C  which would be impractical for even modest vocabulary sizes.
By using the update in Equation (8) we obtain the values for φw|t. This means we no longer have
neat conjugate priors for φw|t and thus the sampling in Equation (5) does not hold. Instead  at the
end of each major Gibbs cycle  φw|t is re-estimated and the corresponding Gibbs update becomes:
(9)

p(zid = t|xid = w  z¬i  φw|t) ∝ φw|t(N¬i

td + α) .

3.4 Convolved Dirichlet Regularizer (CONV-REG)

Another approach to leveraging information on word dependencies from external data is to consider
that each φt is a mixture of word probabilities ψt  where the coefﬁcients are constrained by the
word-pair dependency matrix C:

(10)
Each topic has a different ψt drawn from a Dirichlet  thus the model is a convolved Dirichlet. This
means that we convolve the supplied topic to include a spread of related words. Then we have that:

where ψt ∼ Dirichlet(γ1).

φt ∝ Cψt

p(w|z = t  C  ψt) =

Cijψj|t

.

(11)

Nit

W(cid:89)

 W(cid:88)

i=1

j=1

3

Table 1: Search result datasets came from a collection of 7M Blogs  a collection of 1M News articles 
and the web. The ﬁrst two collections were indexed with Lucene. The queries below were issued to
create ﬁve Blog datasets  four News datasets  and four Web datasets. Search result set sizes ranged
from 1000 to 18 590. For Blogs and News  half of each dataset was set aside for Test  and Train was
sampled from the remaining half. For Web  Train was the top-40 search results.

Blogs

News

Web

Name
beijing
climate
obama
palin
vista

baseball
drama
health
legal

depression
migraine
america

south africa

Query
beijing olympic ceremony
climate change
obama debate
palin interview
vista problem
major league baseball game team player
television show series drama
health medicine insurance
law legal crime court
depression
migraine
america
south africa

# Results DTest DTrain
39
58
72
40
32
29
23
25
23
40
40
40
40

5024
14 932
18 590
10 478
4214
3774
3024
1655
2976
1000
1000
1000
1000

2512
7466
9295
5239
2107
1887
1512
828
1488
1000
1000
1000
1000

We obtain the MAP solution to ψt by optimizing:

W(cid:88)

LMAP =

Solving for ψw|t we obtain:

i=1

Nit log

Cijψj|t +

(γ − 1) log ψj|t

W(cid:88)
ψw|t ∝ W(cid:88)

j=1

i=1

W(cid:88)

j=1

(cid:80)W

NitCiw
j=1 Cijψj|t

W(cid:88)

j=1

s.t.

ψj|t = 1.

(12)

ψw|t + γ.

(13)

We follow the same semi-collapsed inference procedure used for QUAD-REG  with the updates in
Equations (13) and (10) producing the values for φw|t to be used in the semi-collapsed sampler (9).

4 Search Result Datasets

Text datasets came from a collection of 7M Blogs (from ICWSM 2009)  a collection of 1M News
articles (LDC Gigaword)  and the Web (using Google’s search). Table 1 shows a summary of the
datasets used. These datasets provide a diverse range of content for topic modeling. Blogs are often
written in a chatty and informal style  which tends to produce topics that are difﬁcult to interpret.
News articles are edited to a higher standard  so learned topics are often fairly interpretable when
one models  say  thousands of articles. However  our experiments use 23-29 articles  limiting the
data for topic learning. Snippets from web search result present perhaps the most sparse data. For
each dataset we created the standard bag-of-words representation and performed fairly standard
tokenization. We created a vocabulary of terms that occurred at least ﬁve times (or two times  for the
Web datasets)  after excluding stopwords. We learned the topic models on the Train data set  setting
T = 15 for Blogs datasets  T = 10 for News datasets  and T = 8 for the Web datasets.
Construction of C: The word co-occurrence data for regularization was obtained from the entire
LDC dataset of 1M articles (for News)  a subset of the 7M blog posts (for Blogs)  and using all 3M
English Wikipedia articles (for Web). Word co-occurrence was computed using a sliding window
of ten words to emphasize short-range dependency. Note that we only kept positive PMI values.
For each dataset we created a W × W matrix of co-occurrence counts using the 2000-most frequent
terms in the vocabulary for that dataset  thereby maintaining reasonably good sparsity for these data.
Selecting most-frequent terms makes sense because our objective is to improve PMI-Score  which
is deﬁned over the top-10 topic words  which tend to involve relatively high-frequency terms. Using
high-frequency terms also avoids potential numerical problems of large PMI values arising from
co-occurrence of rare terms.

4

Figure 1: PMI-Score and test perplexity of regularized methods vs. LDA on Blogs  T = 15. Both
regularization methods improve PMI-Score and perplexity for all datasets  with the exception of
‘vista’ where QUAD-REG has slightly higher perplexity.

5 Experiments

In this section we evaluate our regularized topic models by reporting the average PMI-Score over 10
different runs  each computed using Equations (1) and (2) (and then in Section 5.4  we use human
judgements). Additionally  we report the average test data perplexity over 10 samples from the
posterior across ten independent chains  where each perplexity is calculated using:

(cid:18)

(cid:19)
− 1
N test log p(xtest)

Perp(xtest) = exp

θt|d =

α + Ntd
T α + Nd

(cid:88)

(cid:88)

log p(xtest) =

N test

dw log

φw|t =

t

dw
β + Nwt
W β + Nt

.

φw|tθt|d

(14)

(15)

The document mixture θt|d is learned from test data  and the log probability of the test words is com-
puted using this mixture. Each φw|t is computed by Equation (15) for the baseline LDA model  and
it is used directly for the QUAD-REG and CONV-REG methods. For the Gibbs sampling algorithms
we set α = 0.05 N
DT and β = 0.01 (initially). This setting of α allocates 5% of the probability mass
for smoothing. We run the sampling for 300 iterations; applied the ﬁxed point iterations (on the
regularized models) 10 times every 20 Gibbs iterations and ran 10 different random initializations
(computing average over these runs). We used T = 10 for the News datasets  T = 15 for the Blogs
datasets and T = 8 for the Web datasets. Note that test perplexity is computed on DTest (Table 1) that
is at least an order of magnitude larger than the training data. After some preliminary experiments 
T .
we ﬁxed QUAD-REG’s regularization parameter to ν = 0.5 N

5.1 Results

Figures 1 and 2 show the average PMI-Scores and average test perplexities for the Blogs and News
datasets. For Blogs (Figure 1) we see that our regularized models consistently improve PMI-Score
and test perplexity on all datasets with the exception of the ‘vista’ dataset where QUAD-REG has
slightly higher perplexity. For News (Figure 2) we see that both regularization methods improve
PMI-Score and perplexity for all datasets. Hence  we can conclude that our regularized models not
only provide a good characterization of the collections but also improve the coherence of the learned
topics as measured by the PMI-Score. It is reasonable to expect both PMI-Score and perplexity to
improve as semantically related words should be expected in topic models  so with little data  our
regularizers push both measures in a positive direction.

5.2 Coherence of Learned Topics

Table 2 shows selected topics learned by LDA and our QUAD-REG model. To obtain correspon-
dence of topics (for this experiment)  we initialized the QUAD-REG model with the converged LDA
model. Overall  our regularized model tends to learn topics that are more focused on a particu-
lar subject  contain fewer spurious words  and therefore are easier to interpret. The following list
explains how the regularized version of the topic is more useful:

5

beijingclimateobamapalinvista1.822.22.4PMI−Score  LDAQuad−RegConv−Regbeijingclimateobamapalinvista02000400060008000Test Perplexity  LDAQuad−RegConv−RegFigure 2: PMI-Score and test perplexity of regularized methods vs. LDA on News  T = 10. Both
regularization methods improve PMI-Score and perplexity for all datasets.

Table 2: Selected topics improved by regularization. Each pair ﬁrst shows an LDA topic and the
corresponding topic produced by QUAD-REG (initialized from the converged LDA model). QUAD-
REG’s PMI-Scores were always better than LDA’s on these examples. The regularized versions tend
to be more focused on a particular subject and easier to interpret.

Name Model Topic
beijing

obama

drama

legal

LDA
REG
LDA
REG
LDA
REG
LDA
REG

girl phony world yang ﬁreworks interest maybe miaoke peiyi young
girl yang peiyi miaoke lin voice real lip music sync
palin biden sarah running mccain media hilton stein paris john
palin sarah mate running biden vice governor selection alaska choice
wire david place police robert baltimore corner friends com simon
drama episode characters series cop cast character actors detective emmy
saddam american iraqi iraq judge against charges minister thursday told
iraqi saddam iraq military crimes tribunal against troops accused ofﬁcials

beijing QUAD-REG has better focus on the names and issues involved in the controversy over the
Chinese replacing the young girl doing the actual singing at the Olympic opening ceremony
with the girl who lip-synched.

obama QUAD-REG focuses on Sarah Palin’s selection as a GOP Vice Presidential candidate  while
LDA has a less clear theme including the story of Paris Hilton giving Palin fashion advice.
drama QUAD-REG learns a topic related to television police dramas  while LDA narrowly focuses

legal

on David Simon’s The Wire along with other scattered terms: robert and friends.
LDA topic is somewhat related to Saddam Hussein’s appearance in court  but includes
uninteresting terms such as: thursday  and told. The QUAD-REG topic is an overall better
category relating to the tribunal and charges against Saddam Hussein.

5.3 Modeling of Google Search Results

Are our regularized topic models useful for building facets in a clustering-web-search-results type
of application? Figure 3 (top) shows the average PMI-Score (mean +/− two standard errors over
10 runs) for the four searches described in Table 1 (Web dataset) and the average perplexity using
top-1000 results as test data (bottom). In all cases QUAD-REG and CONV-REG learn better topics 
as measured by PMI-Score  compared to those learned by LDA. Additionally  whereas QUAD-REG
exhibits slightly higher values of perplexity compared to LDA  CONV-REG consistently improved
perplexity on all four search datasets. This level of improvement in PMI-Score through regulariza-
tion was not seen in News or Blogs likely because of the greater sparsity in these data.

5.4 Human Evaluation of Regularized Topic Models

So far we have evaluated our regularized topic models by assessing (a) how faithful their represen-
tation is to the collection of interest  as measured by test perplexity  and (b) how coherent they are 

6

baseballdramahealthlegal22.533.5PMI−Score  LDAQuad−RegConv−Regbaseballdramahealthlegal0200040006000800010000Test Perplexity  LDAQuad−RegConv−RegFigure 3: PMI-Score and test perplexity of regularized methods vs. LDA on Google search results.
Both methods improve PMI-Score and CONV-REG also improves test perplexity  which is computed
using top-1000 results as test data (therefore top-1000 test perplexity is not reported).

as given by the PMI-Score. Ultimately  we have hypothesized that humans will ﬁnd our regularized
topic models more semantically coherent than baseline LDA and therefore more useful for tasks
such as document clustering  search and browsing. To test this hypothesis we performed further ex-
periments where we asked humans to directly compare our regularized topics with LDA topics and
choose which is more coherent. As our experimental results in this section show  our regularized
topic models signiﬁcantly outperform LDA based on actual human judgements.
To evaluate our models with human judgments we used Amazon Mechanical Turk (AMT  https:
//www.mturk.com) where we asked workers to compare topic pairs (one topic given by one
of our regularized models and the other topic given by LDA) and to answer explicitly which topic
was more coherent according to how clearly they represented a single theme/idea/concept. To keep
the cognitive load low (while still having a fair and sound evaluation of the topics) we described
each topic by its top-10 words. We provided an additional option “...Can’t decide...” indicating
that the user could not ﬁnd a qualitative difference between the topics presented. We also included
control comparisons to ﬁlter out bad workers. These control comparisons were done by replacing
a randomly-selected topic word with an intruder word. To have aligned (matched) pairs of topics 
the sampling procedure of our regularized topic models was initialized with LDA’s topic assignment
obtained after convergence of Gibbs sampling. These experiments produced a total of 3650 topic-
comparison human evaluations and the results can be seen in Figure 4.

6 Related Work

Several authors have investigated the use of domain knowledge from external sources in topic model-
ing. For example  [7  8] propose a method for combining topic models with ontological knowledge
to tag web pages. They constrain the topics in an LDA-based model to be amongst those in the given
ontology. [20] also use statistical topic models with a predeﬁned set of topics to address the task of
query classiﬁcation. Our goal is different to theirs in that we are not interested in constraining the
learned topics to those in the external data but rather in improving the topics in small or noisy collec-
tions by means of regularization. Along a similar vein  [2] incorporate domain knowledge into topic
models by encouraging some word pairs to have similar probability within a topic. Their method 
as ours  is based on replacing the standard Dirichlet prior over word-topic probabilities. However 
unlike our approach that is entirely data-driven  it appears that their method relies on interactive
feedback from the user or on the careful selection of words within an ontological concept.
The effect of structured priors in LDA has been investigated by [17] who showed that learning
hierarchical Dirichlet priors over the document-topic distribution can provide better performance
than using a symmetric prior. Our work is motivated by the fact that priors matter but is focused on a
rather different use case of topic models  i.e. when we are dealing with small or noisy collections and
want to improve the coherence of the topics by re-deﬁning the prior on the word-topic distributions.
Priors that introduce correlations in topic models have been investigated by [3]. Unlike our work
that considers priors on the word-topic distributions (φw|t)  they introduce a correlated prior on the

7

depressionmigraineamericasouth africa1.822.22.42.62.833.23.4PMI−Score  LDA top−40Quad−Reg top−40Conv−Reg top−40LDA top−1000depressionmigraineamericasouth africa02004006008001000Test Perplexity  LDA top−40Quad−Reg top−40Conv−Reg top−40Figure 4: The proportion of times workers in Amazon Mechanical Turk selected each topic model as
showing better coherence. In nearly all cases our regularized models outperform LDA. CONV-REG
outperforms LDA in 11 of 13 datasets. QUAD-REG never performs worse than LDA (at the dataset
level). On average (from 3650 topic comparisons) workers selected QUAD-REG as more coherent
57% of the time while they selected LDA as more coherent only 37% of the time. Similarly  they
chose CONV-REG’s topics as more coherent 56% of the time  and LDA as more coherent only 39%
of the time. These results are statistically signiﬁcant at 5% level of signiﬁcance when performing
a paired t-test on the total values across all datasets. Note that the two bars corresponding to each
dataset do not add up to 100% as the remaining mass corresponds to “...Can’t decide...” responses.

topic proportions (θt|d). In our approach  considering similar priors for φw|t to those studied by [3]
would be unfeasible as they would require the inverse of a W × W covariance matrix.
Network structures associated with a collection of documents are used in [12] in order to “smooth”
the topic distributions of the PLSA model [11]. Our methods are different in that they do not require
the collection under study to have an associated network structure as we aim at addressing the
different problem of regularizing topic models on small or noisy collections. Additionally  their work
is focused on regularizing the document-topic distributions instead of the word-topic distributions.
Finally  the work in [13]  contemporary to ours  also addresses the problem of improving the quality
of topic models. However  our approach focuses on exploiting the knowledge provided by external
data given the noisy and/or small nature of the collection of interest.

7 Discussion & Conclusions

In this paper we have proposed two methods for regularization of LDA topic models based upon
the direct inclusion of word dependencies in our word-topic prior distributions. We have shown that
our regularized models can improve the coherence of learned topics signiﬁcantly compared to the
baseline LDA method  as measured by the PMI-Score and assessed by human workers in Amazon
Mechanical Turk. While our focus in this paper has been on small  and small and noisy datasets  we
would expect our regularization methods also to be effective on large and noisy datasets. Note that
mixing and rate of convergence may be more of an issue with larger datasets  since our regularizers
use a semi-collapsed Gibbs sampler. We will address these large noisy collections in future work.

Acknowledgments

NICTA is funded by the Australian Government as represented by the Department of Broadband 
Communications and the Digital Economy and the Australian Research Council through the ICT
Centre of Excellence program. DN was also supported by an NSF EAGER Award  an IMLS Re-
search Grant  and a Google Research Award.

8

baseballdramahealthlegalbeijingclimateobamapalinvistadepressionmigraineamericasouthafrica020406080% Time Method is Better  QuadRegLDAbaseballdramahealthlegalbeijingclimateobamapalinvistadepressionmigraineamericasouthafrica020406080% Time Method is Better  ConvRegLDAReferences
[1] L. AlSumait  D. Barbar´a  J. Gentle  and C. Domeniconi. Topic signiﬁcance ranking of LDA generative

models. In ECML/PKDD  2009.

[2] D. Andrzejewski  X. Zhu  and M. Craven.

Dirichlet forest priors. In ICML  2009.

Incorporating domain knowledge into topic modeling via

[3] David M. Blei and John D. Lafferty. Correlated topic models. In NIPS  2005.
[4] D.M. Blei  A.Y. Ng  and M.I. Jordan. Latent Dirichlet allocation. JMLR  3:993–1022  2003.
[5] Claudio Carpineto  Stanislaw Osinski  Giovanni Romano  and Dawid Weiss. A survey of web clustering

engines. ACM Comput. Surv.  41(3)  2009.

[6] J. Chang  J. Boyd-Graber  S. Gerrish  C. Wang  and D. Blei. Reading tea leaves: How humans interpret

topic models. In NIPS  2009.

[7] Chaitanya Chemudugunta  America Holloway  Padhraic Smyth  and Mark Steyvers. Modeling documents

by combining semantic concepts with unsupervised statistical learning. In ISWC  2008.

[8] Chaitanya Chemudugunta  Padhraic Smyth  and Mark Steyvers. Combining concept hierarchies and

statistical topic models. In CIKM  2008.

[9] T. Grifﬁths and M. Steyvers. Probabilistic topic models. In Latent Semantic Analysis: A Road to Meaning 

2006.

[10] Shengbo Guo and Scott Sanner. Probabilistic latent maximal marginal relevance. In SIGIR  2010.
[11] Thomas Hofmann. Probabilistic latent semantic indexing. In SIGIR  1999.
[12] Qiaozhu Mei  Deng Cai  Duo Zhang  and ChengXiang Zhai. Topic modeling with network regularization.

In WWW  2008.

[13] David Mimno  Hanna Wallach  Edmund Talley  Miriam Leenders  and Andrew McCallum. Optimizing

semantic coherence in topic models. In EMNLP  2011.

[14] D.M. Mimno and A. McCallum. Organizing the OCA: learning faceted subjects from a library of digital

books. In JCDL  2007.

[15] D. Newman  J.H. Lau  K. Grieser  and T. Baldwin. Automatic evaluation of topic coherence. In NAACL

HLT  2010.

[16] D. Newman  Y. Noh  E. Talley  S. Karimi  and T. Baldwin. Evaluating topic models for digital libraries.

In JCDL  2010.

[17] H. Wallach  D. Mimno  and A. McCallum. Rethinking LDA: Why priors matter. In NIPS  2009.
[18] Xing Wei and W. Bruce Croft. LDA-based document models for ad-hoc retrieval. In SIGIR  2006.
[19] Hua-Jun Zeng  Qi-Cai He  Zheng Chen  Wei-Ying Ma  and Jinwen Ma. Learning to cluster web search

results. In SIGIR  2004.

[20] Haijun Zhai  Jiafeng Guo  Qiong Wu  Xueqi Cheng  Huawei Sheng  and Jin Zhang. Query classiﬁcation
based on regularized correlated topic model. In Proceedings of the International Joint Conference on Web
Intelligence and Intelligent Agent Technology  2009.

9

,Ryan Tibshirani