2019,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation,Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However  the different data distributions in the two domains  or \emph{domain shift/discrepancy}  inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains  the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper  we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation  which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First  the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then  we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart  respectively. Finally  we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5$\rightarrow $Cityscapes and SYNTHIA$\rightarrow $Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at \url{https://github.com/RogerZhangzz/CAG\_UDA}.,Category Anchor-Guided Unsupervised Domain

Adaptation for Semantic Segmentation

Qiming Zhang∗1

Jing Zhang∗1 Wei Liu2 Dacheng Tao1

1UBTECH Sydney AI Centre  School of Computer Science  Faculty of Engineering

The University of Sydney  Darlington  NSW 2008  Australia

2Tencent AI Lab  China

qzha2506@uni.sydney.edu.au  jing.zhang1@sydney.edu.au

wl2223@columbia.edu  dacheng.tao@sydney.edu.au

Abstract

Unsupervised domain adaptation (UDA) aims to enhance the generalization ca-
pability of a certain model from a source domain to a target domain. UDA is of
particular signiﬁcance since no extra effort is devoted to annotating target domain
samples. However  the different data distributions in the two domains  or domain
shift/discrepancy  inevitably compromise the UDA performance. Although there
has been a progress in matching the marginal distributions between two domains 
the classiﬁer favors the source domain features and makes incorrect predictions
on the target domain due to category-agnostic feature alignment. In this paper  we
propose a novel category anchor-guided (CAG) UDA model for semantic segmen-
tation  which explicitly enforces category-aware feature alignment to learn shared
discriminative features and classiﬁers simultaneously. First  the category-wise
centroids of the source domain features are used as guided anchors to identify the
active features in the target domain and also assign them pseudo-labels. Then 
we leverage an anchor-based pixel-level distance loss and a discriminative loss
to drive the intra-category features closer and the inter-category features further
apart  respectively. Finally  we devise a stagewise training mechanism to reduce the
error accumulation and adapt the proposed model progressively. Experiments on
both the GTA5→Cityscapes and SYNTHIA→Cityscapes scenarios demonstrate
the superiority of our CAG-UDA model over the state-of-the-art methods. The
code is available at https://github.com/RogerZhangzz/CAG_UDA.

1

Introduction

Semantic segmentation is a classical computer vision task that refers to assigning pixel-wise category
labels to a given image to facilitate downstream applications such as autonomous driving  video
surveillance  and image editing. The recent progress in semantic segmentation has been dominated
by deep neural networks trained on large datasets. Despite their success  annotating labels at the pixel
level is prohibitively expensive and time-consuming  e.g.  about 90 minutes for a single image in
the Cityscapes dataset [8]. One economical alternative is to exploit computer graphics techniques to
simulate a virtual 3D environment and automatically generate images and labels  e.g.  GTA5 [31] and
SYNTHIA [32]. Although synthetic images have similar appearances to real images  there still exist
subtle differences in textures  layouts  colors  and illumination conditions [11  42–44]  which result
in different data distributions  or domain discrepancy. Consequently  the performance of a certain
model trained on synthetic datasets degrades drastically when applied to realistic scenes. To address
this issue  one promising approach is domain adaptation [1  45  15  34  36  27  33  40  47  13] to

∗indicates equal contributions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

reduce the domain shift and learn a shared discriminative model for both domains. In this paper  we
tackle the more challenging unsupervised domain adaptation (UDA) situation  where no labels are
available in the target domain during training.
Previous methods have tried to learn domain-invariant representations by matching the distributions
between source and target domains at the appearance level [27  34  40  13  21]  feature level [14  27 
3  13]  or output level [45  36  26]. However  even though matching the global marginal distributions
can bring the two domains closer  e.g.  reaching a lower maximum mean discrepancy (MMD) [25]
or a saddle point in the minimax game via adversarial learning [13]  it does not guarantee that
samples from different categories in the target domain are properly separated  hence compromising
the generalization ability. To tackle this issue  one could instead consider category-aware feature
alignment by matching the local joint distributions of features and categories [7  19  33]. Other
approaches adopt the idea of self-training by generating pseudo-labels for samples in the target
domain and providing extra supervision to the classiﬁer [47  21  3]. Together with supervision from
the source domain  this enforces the network to simultaneously learn domain-invariant discriminative
feature representations and shared decision boundaries through back-propagation. The ideas of
minimizing the entropy (uncertainty) of the output [39] or discrepancies between the outputs of two
classiﬁers (voters) [26] have also been exploited to implicitly enforce category-level alignment.
Although category-level alignment and self-training methods have produced some promising results 
there are still some outstanding issues that need to be addressed to further improve the adaptation
performance. For example  error-prone pseudo-labels will mislead the classiﬁer and accumulate
errors. Meanwhile  implicit category-level alignment may be affected by category imbalance. To deal
with these issues and take advantage of both approaches  here we propose a novel idea of category
anchors  which facilitate both category-wise feature alignment and self-training. It is motivated by
the observation that features from the same category tend to be clustered together. Moreover  the
centroids of source domain features in each category can serve as explicit anchors to guide adaptation.
Speciﬁcally  we propose a novel category anchor-guided unsupervised domain adaptation model
(CAG-UDA) for semantic segmentation. This model explicitly enforces category-wise feature
alignment to learn shared feature representations and classiﬁers for both domains simultaneously.
First  the centroids of category-wise features in the source domain are used as anchors to identify
the active features in the target domain. Then  we assign pseudo-labels to these active features
according to the category of the closest anchor. Lastly  two loss functions are proposed: the ﬁrst is a
pixel-level distance loss between the guiding anchors and active features  which pushes them closer
and explicitly minimizes the intra-category feature variance; the other is a pixel-level discriminative
loss to supervise the classiﬁer and maximize the inter-category feature variance. To reduce the error
accumulation of incorrect pseudo-labels  we propose a stagewise training mechanism to adapt the
model progressively.
The main contributions of this paper can be summarized as follows. First  we propose a novel category
anchor idea to tackle the challenging UDA problem in semantic segmentation. Second  we propose a
simple yet effective category anchor-based method to identify active features in the target domain 
further enabling category-wise feature alignment. Finally  the proposed CAG-UDA model achieves
new state-of-the-art performance in both GTA5→Cityscapes and SYNTHIA→Cityscapes scenarios.

2 Related Work

Many recent advances in computer vision [20  12  30  11  24  46  5] have been based on deep neural
networks trained on large-scale labeled datasets such as ImageNet [9]  Pascal VOC [10]  MS COCO
[22]  and Cityscapes [8]. However  a domain shift between training data and testing data impairs
model performance [29  17  18]. To overcome this issue  a variety of domain adaptation methods for
classiﬁcation [6  23  37  28  41  3  19]  detection [38  16]  and segmentation [7  14  13  27  34  40  21 
47] have been proposed. In this paper  we focus on the challenging semantic segmentation problem.
The current mainstream approaches include style transfer [27  34  40  13  21]  feature alignment
[7  14  13]  and self-training [47  21]. As our work is most related to the latter two approaches  we
brieﬂy review and discuss their characteristics.
Feature distribution alignment: Previous methods that match the global marginal distributions be-
tween two domains [14  13  27] do not distinguish local category-wise feature distribution shifts.
Consequently  error-prone predictions are made for misaligned features with shared decision bound-

2

aries. In contrast to these methods  we propose a category-wise feature alignment method to explicitly
reduce category-level mismatches and learn discriminative domain-invariant features. The idea of
category-level feature alignment was also exploited in [26  33] for semantic segmentation. Luo
et al. proposed a weighted adversarial learning method to align the category-level feature distri-
butions implicitly [26]. Saito et al. tried to align the feature distributions and learn discriminative
domain-invariant features by utilizing task-speciﬁc classiﬁers as a discriminator [33]. In contrast
to the implicit feature alignment in the aforementioned methods  we propose a novel category
anchor-guided method  which directly aligns category-wise features in both domains.
Pseudo-label assignment: Assigning pseudo-labels to target domain samples based on the trained
classiﬁers helps adapt the feature extractor and classiﬁer to the target domain. Zou et al. [47] proposed
an iterative self-training UDA model by alternatively generating pseudo-labels and retraining the
model. They also dealt with the category imbalance issue by controlling the proportion of selected
pseudo-labels in each category [47]. Li et al. [21] proposed a bidirectional learning domain adaptation
model that alternately trains the image translation model and the self-supervised segmentation
adaptation model. In contrast to these methods  where pseudo-labels were determined according
to the predicted category probability  we propose a category anchor-based method to generate
trustable pseudo-labels. Compared with selected samples that have been “correctly” classiﬁed with
high conﬁdence  our selected samples are not determined by the decision boundaries so are more
informative for the classiﬁer to further adapt to the target domain.
The idea of assigning pseudo-labels based on category centers has also been utilized in domain
adaptation for classiﬁcation  e.g.  category centroids in [41]  prototypes in [3]  and cluster centers
in [19]. The former two methods minimize the distance loss against category centroids  while the
third minimizes contrastive domain discrepancies. Our method differs from these methods in several
ways. First  we tackle the more challenging task of image semantic segmentation rather than image
classiﬁcation  where dense pixel-wise labels need to be predicted as not just single labels for entire
images. Second  we ﬁx the category centroids (hence called category anchors) instead of updating
them at each iteration. On one hand  the mini-batch size used for segmentation (e.g.  1) in this paper
is much smaller than that used for classiﬁcation. On the other hand  pixels are spatially coherent
in an image  so the category centroids calculated at each iteration will be biased and unreliable due
to the dominance of homogeneous features. Third  the pseudo-labels of target domain samples are
determined by their distance against the category centroids from the source domain instead of the
target domain. This is reasonable since: 1) the source domain category centroids are calculated from
all training samples based on ground-truth labels  which are reliable; 2) driving the target domain
features towards the source domain category centroids can effectively reduce the domain discrepancy.
Fourth  together with the category anchor-based distance loss  we also add the segmentation loss
based on the pseudo-labeled target samples to learn discriminative feature representations and adapt
the decision boundaries simultaneously.

3 A category anchor-guided UDA model for semantic segmentation

3.1 Problem Formulation

Supervised semantic segmentation: A semantic segmentation model M can be formulated as a
mapping function from the image domain X to the output label domain Y :

(1)
which predicts a pixel-wise category label ˆy close to the ground-truth annotation y ∈ Y for a given
image x ∈ X. Usually  the segmentation model M is trained in a supervised manner by minimizing
the difference between the prediction ˆy and its ground-truth y for every training sample x. The
cross-entropy (CE) loss is widely used as a measurement  which is deﬁned as:

M : X → Y 

LCE = − N(cid:88)

H×W(cid:88)

C(cid:88)

yijclog (pijc)  

(2)

i=1

j=1

c=1

the ground-truth label  i.e.  ∀i  j (cid:80)

where N is the number of training images  H and W denote the image size  j is the pixel index  C is
the number of categories  c is the category index  yijc ∈ {0  1} is the one-hot vector representation of
c yijc = 1  and pijc is the predicted category probability by M.
UDA for semantic segmentation: Generally  a segmentation model trained on a source domain Xs
has a limited generalization capability to a target domain Xt  when the distributions between Xs and

3

Figure 1: An illustration of the proposed category anchor-guided UDA model for semantic seg-
mentation. (a) The architecture of the proposed CAG-UDA model consists of an encoder  a feature
transformer (fD)  and a classiﬁer. The green part denotes the source domain ﬂow while the orange
parts represent the target domain ﬂow. (b) The illustration of the process of active target sample
identiﬁcation and pseudo label assignment described in Section 3.2. (c) The illustration of the
proposed category-wise feature alignment with the anchor-based pixel-level distance loss Ldis and
cross-entropy loss LCE described in Section 3.3. Best viewed in color.

Xt are different  i.e.  there is a domain shift/discrepancy. Several unsupervised domain adaptation
models have been proposed  which can be formulated as the following mapping function:

Muda : Xs ∪ Xt → Ys ∪ Yt 

(3)
where Muda is trained on the labeled training samples (Xs  Ys) in the source domain together with
the training unlabeled samples Xt in the target domain. Typically  the aforementioned CE loss and
some domain-adaptation losses are used to align the distributions of both domains (e.g.  p (Xs) and
p (Ys)) and to learn domain-invariant discriminative feature representations.
Model components: The main semantic segmentation approaches have been based on fully con-
volutional neural networks (CNNs) since the seminal work in [24]. Usually  a DCNN-based model
has two parts: an encoder Enc and a decoder Dec  where the encoder maps the input image into a
low-dimensional feature space and then the decoder decodes it to the label space. The decoder can be
further divided into a feature transformation net fD and a classiﬁer Cls  where Cls denotes the last
classiﬁcation layer and fD denotes the remaining part in Dec. Typical encoders are the classiﬁcation
networks pretrained on ImageNet [9]  e.g.  VGGNet [35] and ResNet [12]. The decoder consists of
convolutional layers responsible for context modeling  multi-scale feature fusion  etc. UDA methods
typically employ a segmentation model with carefully designed modules for domain adaptation.

3.2 Network Architecture

The network architecture of our proposed CAG-UDA model is shown in Figure 1(a). The CAG-UDA
model employs Deeplab v2 [4] as the base segmentation model  where ResNet-101 is used as the
encoder Enc and the ASPP module is used in the decoder Dec. To reduce the domain shift  we
devise a category anchor-guided alignment module on the features from fD  consisting of category
anchor construction (CAC)  active target sample identiﬁcation (ATI)  and pseudo-label assignment
(PLA) as shown in Figure 1(b). The details are as follows.
Category anchor construction (CAC): Based on the observation that pixels in the same category
cluster in the feature space  we propose to calculate the centroids of the features of each category in
the source domain as a representative of the feature distribution  i.e.  the mean. Considering that the
features fed into the classiﬁer directly relate to the decision boundaries  we choose the features from
fD to calculate these centroids. Mathematically  this can be written as:
i ))|j)  

ys
ijc (fD (Enc (xs

H×W(cid:88)

N(cid:88)

f s
c =

(4)

1
|Λs
c|

i=1

j

where Λs
to the cth category  i.e.  Λs

c is the index set of all pixels on the training images in the source domain Xs belonging
c  i.e. 

c| denotes the number of pixels in Λs

c = {(i  j)|ys

ijc = 1}  |Λs

4

𝑆𝑇classifierencoder𝑓𝐷𝑆𝑇𝑆𝑇PredictionGroundtruthPseudolabelsPredictionImagesImagesLoss𝐿𝑑𝑖𝑠𝑆CategoryanchorSourcesampleLoss𝐿𝑑𝑖𝑠𝑇CategoryanchorActivesampleCAG architectureATIandPLACategoryanchors……1219active(CA1)FeaturespaceTargetsamplesinactiveactive(CA19)(CA19)(CA1)∆𝑑hyperbola:𝑑1−𝑑19=Δ𝑑DomainadaptationLoss𝐿𝐶𝐸𝑇Loss𝐿𝐶𝐸𝑆Global marginal distributionsCAG category-wise alignment(b)(a)(c)𝐿;<𝐿𝑑𝑖𝑠TargetsampleSourcesampleDecisionboundaryc| =(cid:80)N

(cid:80)H×W

j

i=1

yijc  and fD (xs

i )|j is the feature vector at index j on the feature map fD (xs
|Λs
i ).
It is noteworthy that we calculate the category centroids at the beginning of each training stage
and then keep them ﬁxed during training (we propose a stagewise training mechanism in Section
3.4.). Therefore  we call these centroids category anchors (CAs) in this paper  i.e.  CA = {f s
c   c =
1  ...  C}.
Active target sample identiﬁcation (ATI): To align the category-wise feature distributions between
two domains  we expect that the category centroids from the target domain get closer to the category
anchors during training. However  on one hand  target sample labels are unavailable. On the other
hand  the calculated centroids on target samples are very unstable at each iteration since the mini-
batch size is very small (i.e.  1) in this paper and image pixels are spatially coherent. To tackle
these issues  we propose identifying active target samples and assigning them pseudo-labels for the
subsequent feature alignment. The term “active target samples” refers to target samples near one
category anchor and far from the other anchors  i.e.  being activated by one speciﬁc category anchor.
Mathematically  this can be formulated as follows. We ﬁrst deﬁne the distance between a target
feature fD (Enc (xt

(cid:0)Enc(cid:0)xt
i))|j and the cth category anchor as
where (cid:107)·(cid:107)2 is the L2 norm of a vector. Then  we sort {dt
compare the shortest distance dt
predeﬁned threshold (cid:52)d  we identify this target sample as active one  i.e. 

(cid:1)(cid:1)|j
(5)
ijc  c = 1  ...  C} in an ascending order and
(cid:48) . If their difference is larger than a

ijc∗ with the second shortest dt

ijc =(cid:13)(cid:13)f s

c − fD

(cid:13)(cid:13)2

dt

ijc

 

i

(cid:48) − dt
dt
ijc
otherwise 

ijc∗ > (cid:52)d 

0 

at
ij =

i according to its closest category anchor f s

ij denotes the active state of the target feature fD (Enc (xt

(6)
i))|j. Like the category anchors 
where at
we calculate the active states at the beginning of each training stage and keep them ﬁxed during
subsequent stages. This is explained in Section 3.4  where we introduce a stagewise training
mechanism.
Pseudo-label assignment (PLA): After we obtain the active state according to Eq. (6)  a pseudo
c∗ with a reliable margin (cid:52)d:
label c∗ can be assigned to xt
(7)
Due to the lack of the target domain labels  the classiﬁer layer is biased to the source domain and
does not generalize well to the target domain  as shown in Figure 1(c). Consequently  some of the
pseudo-labels from predicted probabilities may be error-prone. However  based on the observation
of the intra-category clustering characteristics  the generated pseudo-labels via category anchors
are independent of the biased classiﬁer and are thus more reliable than those assigned by predicted
category probabilities. Further  considering that high-probability samples have been “correctly”
classiﬁed by the classiﬁer layer with high conﬁdence  these samples provide only weak supervision
signals. In contrast  active samples are more informative for adapting the classiﬁer to the target
domain as the classiﬁer layer may not predict these active samples with high probabilities.

ijc − (cid:52)d ∀c (cid:54)= c

ijc∗ = 1  if dt
ˆyt

ijc∗ < dt

∗

.

(cid:26) 1 

3.3 Objective Functions

When training the CAG-UDA model  we leverage a CE loss Ls
propose a category-wise distance loss Ls
losses on the active target samples  i.e.  a CE loss Lt
on the pseudo-labels  to guide the adaptation process. These are deﬁned as:

CE as deﬁned in Eq. (2). We also
dis on the source domain samples and two domain adaptation
dis based

CE and a category-wise distance loss Lt

(8)

(9)

(10)

Ls

dis =

ijc (cid:107)f s
ys

c − fD (Enc (xs

i ))|j(cid:107)2  

at
ij

ˆyt

C(cid:88)
(cid:13)(cid:13)f s

c=1

ijclog(cid:0)pt
(cid:1)  
(cid:1)(cid:1)|j
(cid:0)Enc(cid:0)xt

ijc

i

(cid:13)(cid:13)2

.

Lt

dis =

at
ij

ˆyt
ijc

c − fD

i=1

c=1

j=1

N(cid:88)
H×W(cid:88)
C(cid:88)
H×W(cid:88)
CE = − M(cid:88)
C(cid:88)
H×W(cid:88)
M(cid:88)

j=1

i=1

Lt

i=1

j=1

c=1

5

Although only the active samples are directly driven towards the category anchors by Lt
dis  other
inactive target samples within each category may also follow the active samples due to being
clustered. Therefore  minimizing Lt
dis indeed reduces the intra-category variances in the target
domain. Meanwhile  Lt
CE leverages the pseudo-labels to update the network weights together with
the source domain CE loss  prompting the encoder  decoder  and classiﬁer to adapt to the target
domain and therefore reducing the intra- and inter-category variances simultaneously. The illustration
is show in Figure 1(c). To leverage the complementarity between the proposed category anchor-based
PLA and category probability-based PLA in [47]  we also identify active target samples based on the
predicted category probability and add an extra CE loss LtP

CE similar to Eq. (9).

(11)

CE = − M(cid:88)

LtP

H×W(cid:88)

C(cid:88)

atP
ij

ˆytP

i=1

j=1

c=1

(cid:0)Ls

(cid:1) + λ2

(cid:1)  

ijc

ijclog(cid:0)pt
(cid:16)

(cid:17)

where atP
Then the ﬁnal objective function is as follows:

ij   ytP

ijc refer to the probability-based active state and assigned pseudo-labels respectively.

dis + Lt

dis

Lt

CE + LtP
CE

 

(12)

L = Ls

CE + λ1

where λ1 and λ2 are loss weights.

3.4 Stagewise Training Procedure

We tried to train the CAG-UDA model in a single stage and update the pseudo-labels at each iteration.
However  it is not stable because there are some error-prone pseudo-labels  which may produce
incorrect supervision signals  lead to more erroneous pseudo-labels iteratively and trap the network
to a local minimum with poor performance eventually  e.g. less than 30 mIoU. To address this issue 
we propose a stagewise training mechanism as summarized in Algorithm 1. First  we pretrain the
segmentation model on the source domain. Then  we leverage the global feature alignment method
in [14] to warm up the training process and obtain a well-initialized model. Next  we train the
CAG-UDA model with the proposed losses for several stages. At the beginning of each stage  we
calculate the CAs  identify the active target samples  and assign pseudo-labels to them. By using this
stagewise delayed updating mechanism  we avoid updating the pseudo-labels at each iteration and
CE serve as two regularizations on the network.
reduce the error accumulation. Hence  Lt

dis and Lt

Algorithm 1 Stagewise training the CAG-UDA model
Input: training dataset: (Xs  Ys  Xt)  maximum stages: K  maximum iterations: L  distance

threshold: (cid:52)d.

0 according to [14];

0 ← (Xs  Ys) according to [4];

Output: MK and ( ˆYs  ˆYt).
1: Pretraining: M p
2: Warm-up: M0 ← (Xs  Ys) and M p
3: for k ← 1 to K do
c } ← Mk−1 and (Xs  Ys) according to Eq. (4);
CAC: {f s
4:
ATI: {dt
ijc} {at
5:
ijc∗} ← {dt
PLA: {ˆyt
6:
for n ← 1 to L do
7:
8:
9:
10:
11: end for
12: Prediction: ( ˆYs  ˆYt) ← (Xs  Xt) and MK.

ij} ← Mk−1  (Xs  Ys  Xt)  {f s
ijc} (cid:52)d according to Eq. (7);

SGD: training Mk−1 on (Xs  Ys  Xt  {ˆyt

end for
Mk ← Mk−1

c } and (cid:52)d according to Eq. (5) and Eq. (6);

ijc∗}  {f s

c }  {at

ij}) according to Eq.(12);

4 Experiments

4.1 Experimental Settings

Datasets and evaluation metrics: Following [21]  we evaluate the CAG-UDA model in two com-
mon scenarios  GTA5[31]→Cityscapes[8] and SYNTHIA[32]→Cityscapes[8]. GTA5 contains

6

Table 1: Results of the CAG-UDA model and SOTA methods ( GTA5→Cityscapes).

k
l
a
w
e
d
i
s

16.8
25.9
22.3
30.8
16.8
27.1
33.1
47.5
35.6
12.7
44.7

25.4
51.6

g
n
i
d
l
i
u
b

77.2
79.8
75.6
81.3
77.2
79.6
81.0
82.5
80.1
69.5
84.2
74.7
83.8

d
a
o
r

75.8
86.5
69.9
85.0
75.8
87.0
89.4
91.5
86.7
69.0
91.0

69.8
90.4

l
l
a
w

12.5
22.1
15.8
25.8
12.5
27.3
26.6
31.3
19.8
9.9
34.6
11.3
34.2

e
c
n
e
f

21.0
20.0
20.1
21.2
21.0
23.3
26.8
25.6
17.5
19.5
27.6

18.3
27.8

e
l
o
p

25.5
23.6
18.8
22.2
25.5
28.3
27.2
33.0
38.0
22.8
30.2

24.2
38.4

t
h
g
i
l

30.1
33.1
28.2
25.4
30.1
35.5
33.5
33.7
39.9
31.7
36.0

35.6
25.3

n
g
i
s

20.1
21.8
17.1
26.6
20.1
24.2
24.7
25.8
41.5
15.3
36.0

23.3
48.4

.
e
g
e
v

81.3
81.8
75.6
83.4
81.3
83.6
83.9
82.7
82.7
73.9
85.0

72.0
85.4

e
c
a
r
r
e
t

24.6
25.9
8.00
36.7
24.6
27.4
36.7
28.8
27.9
11.3
43.6
14.4
38.2

n
o
s
r
e
p

53.8
57.3
55.0
58.9
53.8
58.6
58.7
62.4
64.9
54.7
58.6

58.7
58.6

y
k
s

70.3
75.9
73.5
76.2
70.3
74.2
78.8
82.7
73.6
67.2
83.0
65.3
78.1

r
e
d
i
r

26.4
26.2
2.9
24.9
26.4
28.0
30.5
30.8
19.0
23.9
31.6

29.0
34.6

r
a
c

49.9
76.3
66.9
80.7
49.9
76.2
84.8
85.2
65.0
53.4
83.3

53.1
84.7

k
c
u
r
t

17.2
29.8
34.4
29.5
17.2
33.1
38.5
27.7
12.0
29.7
35.3

14.3
21.9

s
u
b

25.9
32.1
30.8
42.9
25.9
36.7
44.5
34.5
28.6
4.6
49.7
19.2
42.7

n
i
a
r
t

6.5
7.2
0.00
2.50
6.5
6.7
1.7
6.4
4.5
11.6
3.3

7.9
41.1

r
o
t
o
m

25.3
29.5
18.4
26.9
25.3
31.9
31.6
25.2
31.1
26.1
28.8

15.1
29.3

e
k
i
b

36.0
32.5
0.00
11.6
36.0
31.4
32.4
24.4
42.0
32.5
35.6

16.3
37.2

mIoU
36.6
41.4
33.3
41.7
36.6
43.2
45.5
45.4
42.7
33.6
48.5

34.1
50.2

Source only

AdaptSegNet[36]

Source only
DCAN[40]
Source only
CLAN[26]
AdvEnt[39]

DISE[2]

Cycada[13  21]

Source only

BLF[21]

Source only
CAG-UDA

Table 2: Results of the CAG-UDA model on the testing set ( GTA5→Cityscapes).

k
l
a
w
e
d
i
s

g
n
i
d
l
i
u
b

d
a
o
r

l
l
a
w

e
c
n
e
f

e
l
o
p

t
h
g
i
l

n
g
i
s

.
e
g
e
v

e
c
a
r
r
e
t

n
o
s
r
e
p

y
k
s

r
e
d
i
r

r
a
c

k
c
u
r
t

s
u
b

n
i
a
r
t

r
o
t
o
m

e
k
i
b

CAG-UDA 93.2

57.0

85.6

35.7

25.1

37.5

30.8

45.3

87.1

50.1

89.4

62.7

40.8

87.8

18.0

32.4

34.5

34.4

35.4

mIoU
51.7

24 966 1914×1052-pixel images and has the same 19 category annotations as Cityscapes. SYN-
THIA contains 9 400 1914×1052-pixel images and only has 16 common category annotations.
Cityscapes is divided into a training set  a validation set  and a testing set. The training set con-
sists of 2 957 2048×1024-pixel images and the validation set contains 500 images at the same
resolution. Following common practice  we report the results on the Cityscapes validation set 
speciﬁcally  the category-wise intersection over union (IoU). Moreover  we also report the mean IoU
(mIoU) of all 19 categories in the GTA5→Cityscapes scenario and the 16 common categories in the
SYNTHIA→Cityscapes scenario. Some methods [36  26  21] only reported mIoU for 13 common
categories in the SYNTHIA→Cityscapes scenario  denoted as mIoU* in this paper.
Implementation details: In our experiments  training images were randomly cropped to 1280×640
pixels after being randomly resized by ×1 ∼ ×1.5. Due to GPU memory limitations  the batch size
was set to 1 and the weights of all batch normalization layers were frozen. In the warm-up phase 
we used a CNN-based domain discriminator comprising 5 convolutional layers of kernel size 3×3 
ﬁlter numbers [64  128  256  512  1]  and stride 2. The ﬁrst three convolutional layers are followed by
a ReLU layer  while the fourth layer is followed by a leaky ReLU layer parameterized by 0.2. We
used a CE loss and an adversarial loss to train the model for 20 epochs. The adversarial loss weights
were set to 1e-2. In the stagewise training phase  we trained the CAG-UDA mode for 20 epochs
with the SGD optimizer. The initial learning rate was 2.5e-4  which decayed by the poly policy with
power 0.9. The weight decay  momentum  λ1  and λ2 were set to 1e-4  0.9  0.3  and 0.7  respectively.
(cid:52)d was set to 2.5. We also assigned pseudo-labels based on predicted category probabilities  and
the threshold P0 was set to 0.95. Experiments were conducted on a TITAN Tesla V100 GPU with
PyTorch implementation. Code will be made publicly available.

4.2 Main Results
Quantitative results: The results of the GTA5→Cityscapes scenario are presented in Table 1 with
the best results highlighted in bold. All the models adopted ResNet-101 as a backbone network for
fair comparison. Overall  our CAG-UDA model strikingly outperforms all other models with a 50.2
mIoU  surpassing the model trained on the source domain by a signiﬁcant gain of 16.1. Compared
with CLAN [26] and DISE [2]  which implicitly align category-level features  our model achieves an
extra gain of 4.5 and outperforms them on fence  trafﬁc sign  rider  train  and bike by large margins.
This is due to the proposed category anchor-guided alignment method  which explicitly uses category
centroids as representatives of feature distributions  reducing the side effect of category imbalance.
Like [40  13]  BLF in [21] also involves a style-transfer module but combines it with self-training
in a bidirectional learning framework. It achieved the second-best mIoU of 48.5. BLF achieves
better results than the CAG-UDA model on stuff categories such as road  building  wall  terrace 
and sky but is inferior to the CAG-UDA model for small objects. This is because BLF includes a

7

Table 3: Results of the CAG-UDA model and SOTA methods ( SYNTHIA→Cityscapes).

k
l
a
w
e
d
i
s

37.2
37.0
46.7
41.7

36.4
53.5
42.2
40.8

g
n
i
d
l
i
u
b

78.8
80.1
80.3
85.5
75.7
77.1
79.7
81.7

d
a
o
r

79.2
81.3
86.0
84.8

82.8
91.7
85.6
84.7

l
l
a
w

e
c
n
e
f

e
l
o
p

-
-
-
-

5.1
2.5
8.7
7.8

-
-
-
-

0.1
0.2
0.4
0.0

-
-
-
-

25.8
27.1
25.9
35.1

t
h
g
i
l

9.9
16.1
14.1
13.7

8.0
6.2
5.4
13.3

n
g
i
s

10.5
13.7
11.6
23.0
18.7
7.6
8.1
22.7

e
l
b
a
t
e
g
e
v

78.2
78.2
79.2
86.5
74.7
78.4
80.4
84.5

n
o
s
r
e
p

53.5
53.4
54.1
66.3
51.1
55.8
57.9
64.2

r
e
d
i
r

19.6
21.2
27.9
28.1
15.9
19.2
23.8
27.8

y
k
s

80.5
81.5
81.3
78.1

76.9
81.2
84.1
77.6

r
a
c

67.0
73.0
73.7
81.8

77.7
82.3
73.3
80.9

s
u
b

29.5
32.9
42.2
21.8

24.8
30.3
36.4
19.7

r
o
t
o
m

21.6
22.6
25.7
22.9

4.1
17.1
14.2
22.7

e
k
i
b

31.3
30.7
45.3
49.0
37.3
34.3
33.0
48.3

mIoU mIoU*
45.9
47.8
51.4
52.6

-
-
-
-

38.4
41.5
41.2
44.5

-
-
-
-

AdaptSegNet[36]

CLAN[26]
BLF[21]

CAG-UDA(13)

DCAN[40]
DISE[2]

AdvEnt[39]

CAG-UDA(16)

Figure 2: (a) Subjective evaluation of the CAG-UDA model on some images from the Cityscapes
validation set. (b) Comparison between probability-based PLA and the proposed CAs-based PLA on
an image from the Cityscapes training set. Best viewed in color and zoom-in.

style-transfer module that beneﬁts from the texture clues in the stuff categories and assigns reliable
pseudo-labels accordingly. In contrast  CAG-UDA uses a category-anchor guided method that can
tackle the category imbalance and generate more informative pseudo-labels  leading to better results
on more categories.
We also present the result on the testing set of the Cityscapes dataset in Table 2. The CAG-UDA
model reaches 51.7 mIoU  proving the good generalization of our method.
Results in the SYNTHIA→Cityscapes scenario are listed in Table 3. Same as the previous work  we
report the performance of the CAG-UDA model in two mIoU metrics: 13 categories (mIoU*) and 16
categories (mIoU) for fair comparisons. Since the domain shift is much larger than the above scenario 
the performance is slightly worse. The CAG-UDA model still achieves better results than all previous
SOTA methods  including CLAN  BLF  etc. Similar to the above discussions with the GTA5 dataset 
the superiority of the CAG-UDA model remains in small objects like pole  sign  person  and bike.
Qualitative results: Some qualitative segmentation examples are given in Figure 2(a). Training
merely on the source domain dataset leads to a limited generalization ability  e.g.  the road and person
were incorrectly predicted as sidewalk and building in the ﬁrst row. Beneﬁted from the category
anchor-guided adaptation  the proposed CAG-UDA model achieves better results  especially for small
objects  e.g.  pole  sign  and person. Besides  we also attribute it to the proposed CAs-based pseudo
label assignment  which successfully activated small objects and assigned them trustable pseudo-
labels  as highlighted in red circles in Figure 2(b). More results can be found in the supplement.

8

Table 4: Results of ablation study (GTA5→Cityscapes).

d
a
o
r

69.8
88.4
88.8
88.3
89.4
88.9
88.1
88.9
88.8
90.4
90.4

.
e
d
i
s

25.4
45.2
45.5
46.9
40.1
41.7
46.6
47.1
47.5
50.6
51.6

.
l
i
u
b

74.7
82.0
83.7
81.5
81.8
82.0
82.1
83.0
83.6
84.0
83.8

l
l
a
w

11.3
30.1
33.2
28.7
31.0
31.7
30.2
31.0
31.7
33.5
34.2

.
c
n
e
f

18.3
22.0
21.4
27.7
22.6
22.5
28.4
27.3
29.1
28.3
27.8

e
l
o
p

24.2
35.4
39.5
38.9
39.9
39.7
39.7
39.7
39.7
39.9
38.4

t
h
g
i
l

35.6
36.7
40.0
27.0
41.2
41.2
31.3
31.0
34.4
31.6
25.3

n
g
i
s

23.3
23.7
25.9
40.4
23.2
23.5
38.8
36.0
35.6
42.4
48.4

.
e
g
e
v

72.0
82.7
83.9
83.7
83.0
82.7
83.6
84.3
84.4
85.1
85.4

.
r
r
e
t

14.4
27.6
33.8
31.2
28.3
27.0
30.7
32.6
33.0
35.2
38.2

y
k
s

65.3
70.8
74.3
74.9
68.5
70.0
75.1
75.1
76.8
77.3
78.1

n
o
s
r
e
p

58.7
51.4
58.2
61.8
54.5
57.8
61.9
62.0
62.1
61.5
58.6

r
e
d
i
r

29.0
26.9
24.9
30.2
23.8
25.7
28.5
29.4
28.2
34.2
34.6

r
a
c

53.1
81.5
84.8
84.0
85.7
85.8
84.3
84.6
84.5
84.9
84.7

k
c
u
r
t

14.3
14.5
19.3
15.9
21.5
21.9
16.3
16.6
17.2
19.4
21.9

s
u
b

19.2
25.0
32.8
36.7
25.6
27.7
36.3
35.7
35.2
41.7
42.7

n
i
a
r
t

7.9
21.4
22.6
23.4
0.7
1.1
29.1
27.2
32.0
41.0
41.1

r
o
t
o
m

15.1
13.0
15.0
23.3
13.9
18.0
25.0
19.2
25.8
27.3
29.3

e
k
i
b

16.3
7.9
14.7
31.7
8.5
11.1
29.4
28.4
27.6
32.0
37.2

mIoU gain
34.1
41.4
44.3
46.1
41.2
42.1
46.6
46.3
47.2
49.5
50.2

-
7.3
10.2
12.0
7.1
8.0
12.5
12.2
13.1
15.4
16.1

Source only
Warm-up
+LtP
CE
+Lt
dis + LtP
dis
dis + Lt
dis
dis + Lt
CE + LtP
CE

+LsP
+Ls
dis + Lt
+Lt

CE

+Ls

CE

CAG-UDA (Stage 1)
CAG-UDA (Stage 2)
CAG-UDA (Stage 3)

Ablation studies: The ablation study results are listed in Table 4. We add a superscript P to the
symbols of losses to denote that the active target samples are identiﬁed by category probabilities
as described in Section 3.3. Several models were trained by combining Lt
CE with different losses.
As can be seen from the 2nd and 3rd rows  the proposed category anchor-guided PLA is more
effective than the predicted category probability-based one. More detailed comparisons of different
hyper-parameters can be found in the supplement. In addition  the CE loss is more effective than the
distance loss. The results in the 4th row demonstrate the complementarity between the CE loss and
distance loss  as well as between the category anchor-based and probability-based PLA. We combine
them as in Eq. (12) to train the CAG-UDA model and obtain a better result as listed in the bottom row.
Finally  the stagewise trained CAG-UDA model obtains an mIoU of 50.2  outperforming the SOTA
models. Besides  the CAG-UDA model has been trained for an extra stage  e.g.  Stage 4. However  it
is saturated at 50.2 mIoU with no improvement.

4.3 Limitations

The proposed CAG-UDA model relies on reliable pseudo-labels to guarantee a correct supervision
imposed on the network to be trained. To this end  we adopt a warm-up strategy to roughly align
two domains together and increase the reliability of the generated pseudo-labels by the CAs  as
described in Section 3.4. In contrast  we also conducted an experiment by removing the warm-up
stage and observed a signiﬁcant drop of 6.3 mIoU. Some techniques can also be used to obtain reliable
pseudo-labels such as enforcing local smoothness on the probability map  utilizing a normalized
threshold during assigning pseudo-labels  and reducing the appearance bias through a style transfer
module. We leave it as the future work to build a stage-free and end-to-end CAG-UDA model.

5 Conclusion

In this paper  we proposed a novel category anchor-guided (CAG) unsupervised domain adaptation
(UDA) model for semantic segmentation. The CAG-UDA model successfully adapts the segmentation
model to the target domain through category-wise feature alignment guided by category anchors.
Speciﬁcally  we proposed a category anchor construction module  an active target sample identiﬁcation
module  and a pseudo-label assignment module. We utilized a distance loss and a CE loss based on
the identiﬁed active target samples  which complementarily enhance the adaptation performance. We
also proposed a stagewise training mechanism to reduce the error accumulation and adapt the CAG-
UDA model progressively. The experiments on the GTA5 and SYNTHIA datasets demonstrate the
superiority of the CAG-UDA model over representative methods on generalization to the Cityscapes
dataset.

Acknowledgements

This work is supported by the Australian Research Council Project FL-170100117 and the National
Natural Science Foundation of China Project 61806062.

9

References
[1] K. Bousmalis  N. Silberman  D. Dohan  D. Erhan  and D. Krishnan. Unsupervised pixel-level domain
adaptation with generative adversarial networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  pages 3722–3731  2017.

[2] W. Chang  H. Wang  W. Peng  and W. Chiu. All about structure: Adapting structural information across

domains for boosting semantic segmentation. CoRR  abs/1903.12212  2019.

[3] C. Chen  W. Xie  T. Xu  W. Huang  Y. Rong  X. Ding  Y. Huang  and J. Huang. Progressive feature

alignment for unsupervised domain adaptation. arXiv preprint arXiv:1811.08585  2018.

[4] L.-C. Chen  G. Papandreou  I. Kokkinos  K. Murphy  and A. L. Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets  atrous convolution  and fully connected crfs. IEEE transactions
on pattern analysis and machine intelligence  40(4):834–848  2017.

[5] L.-C. Chen  Y. Zhu  G. Papandreou  F. Schroff  and H. Adam. Encoder-decoder with atrous separable
convolution for semantic image segmentation. In Proceedings of the European Conference on Computer
Vision (ECCV)  pages 801–818  2018.

[6] M. Chen  K. Q. Weinberger  and J. Blitzer. Co-training for domain adaptation. In Advances in Neural

Information Processing Systems (Neurips)  pages 2456–2464  2011.

[7] Y.-H. Chen  W.-Y. Chen  Y.-T. Chen  B.-C. Tsai  Y.-C. Frank Wang  and M. Sun. No more discrimination:
Cross city adaptation of road scene segmenters. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV)  pages 1992–2001  2017.

[8] M. Cordts  M. Omran  S. Ramos  T. Rehfeld  M. Enzweiler  R. Benenson  U. Franke  S. Roth  and
B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3213–3223  2016.

[9] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 248–255. Ieee  2009.

[10] M. Everingham  L. Van Gool  C. K. Williams  J. Winn  and A. Zisserman. The pascal visual object classes

(voc) challenge. International journal of computer vision  88(2):303–338  2010.

[11] K. He  G. Gkioxari  P. Dollár  and R. Girshick. Mask r-cnn. In Proceedings of the IEEE International

Conference on Computer Vision (ICCV)  pages 2961–2969  2017.

[12] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 770–778  2016.

[13] J. Hoffman  E. Tzeng  T. Park  J.-Y. Zhu  P. Isola  K. Saenko  A. Efros  and T. Darrell. Cycada: Cycle-
consistent adversarial domain adaptation. In International Conference on Machine Learning (ICML) 
2018.

[14] J. Hoffman  D. Wang  F. Yu  and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based

adaptation. arXiv preprint arXiv:1612.02649  2016.

[15] W. Hong  Z. Wang  M. Yang  and J. Yuan. Conditional generative adversarial network for structured
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 1335–1344  2018.

[16] N. Inoue  R. Furuta  T. Yamasaki  and K. Aizawa. Cross-domain weakly-supervised object detection
through progressive domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  pages 5001–5009  2018.

[17] W. Jiang  H. Gao  W. Lu  W. Liu  F.-L. Chung  and H. Huang. Stacked robust adaptively regularized auto-
regressions for domain adaptation. IEEE Transactions on Knowledge and Data Engineering  31(3):561–
574  2018.

[18] W. Jiang  W. Liu  and F.-l. Chung. Knowledge transfer for spectral clustering. Pattern Recognition 

81:484–496  2018.

[19] G. Kang  L. Jiang  Y. Yang  and A. G. Hauptmann. Contrastive adaptation network for unsupervised

domain adaptation. arXiv preprint arXiv:1901.00976  2019.

[20] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems (Neurips)  pages 1097–1105  2012.

[21] Y. Li  L. Yuan  and N. Vasconcelos. Bidirectional learning for domain adaptation of semantic segmentation.

arXiv preprint arXiv:1904.10620  2019.

[22] T.-Y. Lin  M. Maire  S. Belongie  J. Hays  P. Perona  D. Ramanan  P. Dollár  and C. L. Zitnick. Microsoft
coco: Common objects in context. In Proceedings of the European Conference on Computer Vision
(ECCV)  pages 740–755. Springer  2014.

[23] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information

Processing Systems (Neurips)  pages 469–477  2016.

[24] J. Long  E. Shelhamer  and T. Darrell. Fully convolutional networks for semantic segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3431–
3440  2015.

[25] M. Long  Y. Cao  J. Wang  and M. Jordan. Learning transferable features with deep adaptation networks.

In International Conference on Machine Learning (ICML)  pages 97–105  2015.

[26] Y. Luo  L. Zheng  T. Guan  J. Yu  and Y. Yang. Taking a closer look at domain shift: Category-level

adversaries for semantics consistent domain adaptation. arXiv preprint arXiv:1809.09478  2018.

[27] Z. Murez  S. Kolouri  D. Kriegman  R. Ramamoorthi  and K. Kim. Image to image translation for domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 

10

pages 4500–4509  2018.

[28] P. O. Pinheiro. Unsupervised domain adaptation with similarity learning. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 8004–8013  2018.

[29] G.-J. Qi  W. Liu  C. Aggarwal  and T. Huang. Joint intermodal and intramodal label transfers for extremely
rare or unseen classes. IEEE transactions on pattern analysis and machine intelligence  39(7):1360–1373 
2016.

[30] S. Ren  K. He  R. Girshick  and J. Sun. Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in Neural Information Processing Systems (Neurips)  pages 91–99  2015.
[31] S. R. Richter  V. Vineet  S. Roth  and V. Koltun. Playing for data: Ground truth from computer games. In

Proceedings of the European Conference on Computer Vision (ECCV)  pages 102–118. Springer  2016.

[32] G. Ros  L. Sellart  J. Materzynska  D. Vazquez  and A. M. Lopez. The synthia dataset: A large collection
of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  pages 3234–3243  2016.

[33] K. Saito  K. Watanabe  Y. Ushiku  and T. Harada. Maximum classiﬁer discrepancy for unsupervised
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 3723–3732  2018.

[34] S. Sankaranarayanan  Y. Balaji  A. Jain  S. Nam Lim  and R. Chellappa. Learning from synthetic data:
Addressing domain shift for semantic segmentation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  pages 3752–3761  2018.

[35] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

International Conference on Learning Representations (ICLR)  2015.

[36] Y.-H. Tsai  W.-C. Hung  S. Schulter  K. Sohn  M.-H. Yang  and M. Chandraker. Learning to adapt structured
output space for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  pages 7472–7481  2018.

[37] E. Tzeng  J. Hoffman  K. Saenko  and T. Darrell. Adversarial discriminative domain adaptation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 7167–
7176  2017.

[38] D. Vazquez  A. M. Lopez  J. Marin  D. Ponsa  and D. Geronimo. Virtual and real world adaptation for
pedestrian detection. IEEE transactions on pattern analysis and machine intelligence  36(4):797–809 
2014.

[39] T.-H. Vu  H. Jain  M. Bucher  M. Cord  and P. Pérez. Advent: Adversarial entropy minimization for domain

adaptation in semantic segmentation. arXiv preprint arXiv:1811.12833  2018.

[40] Z. Wu  X. Han  Y.-L. Lin  M. Gokhan Uzunbas  T. Goldstein  S. Nam Lim  and L. S. Davis. Dcan: Dual
channel-wise alignment networks for unsupervised scene adaptation. In Proceedings of the European
Conference on Computer Vision (ECCV)  pages 518–534  2018.

[41] S. Xie  Z. Zheng  L. Chen  and C. Chen. Learning semantic representations for unsupervised domain

adaptation. In International Conference on Machine Learning (ICML)  pages 5419–5428  2018.

[42] J. Zhang  Y. Cao  S. Fang  Y. Kang  and C. Wen Chen. Fast haze removal for nighttime image using
maximum reﬂectance prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 7418–7426  2017.

[43] J. Zhang  Y. Cao  Y. Wang  C. Wen  and C. W. Chen. Fully point-wise convolutional neural network for
modeling statistical regularities in natural images. In 2018 ACM Multimedia Conference on Multimedia
Conference  pages 984–992. ACM  2018.

[44] J. Zhang and D. Tao. Famed-net: A fast and accurate multi-scale end-to-end dehazing network. IEEE

Transactions on Image Processing  29:72–84  2020.

[45] Y. Zhang  P. David  and B. Gong. Curriculum domain adaptation for semantic segmentation of urban scenes.
In Proceedings of the IEEE International Conference on Computer Vision (ICCV)  pages 2020–2030  2017.
[46] H. Zhao  J. Shi  X. Qi  X. Wang  and J. Jia. Pyramid scene parsing network. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2881–2890  2017.

[47] Y. Zou  Z. Yu  B. Vijaya Kumar  and J. Wang. Unsupervised domain adaptation for semantic segmentation
via class-balanced self-training. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 289–305  2018.

11

,Zhuo Wang
Alan Stocker
Daniel Lee
Qiming ZHANG
Jing Zhang
Wei Liu
Dacheng Tao