2019,Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation,Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However  the different data distributions in the two domains  or \emph{domain shift/discrepancy}  inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains  the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper  we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation  which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First  the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then  we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart  respectively. Finally  we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5$\rightarrow $Cityscapes and SYNTHIA$\rightarrow $Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at \url{https://github.com/RogerZhangzz/CAG\_UDA}.,Category Anchor-Guided Unsupervised Domain

Adaptation for Semantic Segmentation

Qiming Zhangâˆ—1

Jing Zhangâˆ—1 Wei Liu2 Dacheng Tao1

1UBTECH Sydney AI Centre  School of Computer Science  Faculty of Engineering

The University of Sydney  Darlington  NSW 2008  Australia

2Tencent AI Lab  China

qzha2506@uni.sydney.edu.au  jing.zhang1@sydney.edu.au

wl2223@columbia.edu  dacheng.tao@sydney.edu.au

Abstract

Unsupervised domain adaptation (UDA) aims to enhance the generalization ca-
pability of a certain model from a source domain to a target domain. UDA is of
particular signiï¬cance since no extra effort is devoted to annotating target domain
samples. However  the different data distributions in the two domains  or domain
shift/discrepancy  inevitably compromise the UDA performance. Although there
has been a progress in matching the marginal distributions between two domains 
the classiï¬er favors the source domain features and makes incorrect predictions
on the target domain due to category-agnostic feature alignment. In this paper  we
propose a novel category anchor-guided (CAG) UDA model for semantic segmen-
tation  which explicitly enforces category-aware feature alignment to learn shared
discriminative features and classiï¬ers simultaneously. First  the category-wise
centroids of the source domain features are used as guided anchors to identify the
active features in the target domain and also assign them pseudo-labels. Then 
we leverage an anchor-based pixel-level distance loss and a discriminative loss
to drive the intra-category features closer and the inter-category features further
apart  respectively. Finally  we devise a stagewise training mechanism to reduce the
error accumulation and adapt the proposed model progressively. Experiments on
both the GTA5â†’Cityscapes and SYNTHIAâ†’Cityscapes scenarios demonstrate
the superiority of our CAG-UDA model over the state-of-the-art methods. The
code is available at https://github.com/RogerZhangzz/CAG_UDA.

1

Introduction

Semantic segmentation is a classical computer vision task that refers to assigning pixel-wise category
labels to a given image to facilitate downstream applications such as autonomous driving  video
surveillance  and image editing. The recent progress in semantic segmentation has been dominated
by deep neural networks trained on large datasets. Despite their success  annotating labels at the pixel
level is prohibitively expensive and time-consuming  e.g.  about 90 minutes for a single image in
the Cityscapes dataset [8]. One economical alternative is to exploit computer graphics techniques to
simulate a virtual 3D environment and automatically generate images and labels  e.g.  GTA5 [31] and
SYNTHIA [32]. Although synthetic images have similar appearances to real images  there still exist
subtle differences in textures  layouts  colors  and illumination conditions [11  42â€“44]  which result
in different data distributions  or domain discrepancy. Consequently  the performance of a certain
model trained on synthetic datasets degrades drastically when applied to realistic scenes. To address
this issue  one promising approach is domain adaptation [1  45  15  34  36  27  33  40  47  13] to

âˆ—indicates equal contributions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

reduce the domain shift and learn a shared discriminative model for both domains. In this paper  we
tackle the more challenging unsupervised domain adaptation (UDA) situation  where no labels are
available in the target domain during training.
Previous methods have tried to learn domain-invariant representations by matching the distributions
between source and target domains at the appearance level [27  34  40  13  21]  feature level [14  27 
3  13]  or output level [45  36  26]. However  even though matching the global marginal distributions
can bring the two domains closer  e.g.  reaching a lower maximum mean discrepancy (MMD) [25]
or a saddle point in the minimax game via adversarial learning [13]  it does not guarantee that
samples from different categories in the target domain are properly separated  hence compromising
the generalization ability. To tackle this issue  one could instead consider category-aware feature
alignment by matching the local joint distributions of features and categories [7  19  33]. Other
approaches adopt the idea of self-training by generating pseudo-labels for samples in the target
domain and providing extra supervision to the classiï¬er [47  21  3]. Together with supervision from
the source domain  this enforces the network to simultaneously learn domain-invariant discriminative
feature representations and shared decision boundaries through back-propagation. The ideas of
minimizing the entropy (uncertainty) of the output [39] or discrepancies between the outputs of two
classiï¬ers (voters) [26] have also been exploited to implicitly enforce category-level alignment.
Although category-level alignment and self-training methods have produced some promising results 
there are still some outstanding issues that need to be addressed to further improve the adaptation
performance. For example  error-prone pseudo-labels will mislead the classiï¬er and accumulate
errors. Meanwhile  implicit category-level alignment may be affected by category imbalance. To deal
with these issues and take advantage of both approaches  here we propose a novel idea of category
anchors  which facilitate both category-wise feature alignment and self-training. It is motivated by
the observation that features from the same category tend to be clustered together. Moreover  the
centroids of source domain features in each category can serve as explicit anchors to guide adaptation.
Speciï¬cally  we propose a novel category anchor-guided unsupervised domain adaptation model
(CAG-UDA) for semantic segmentation. This model explicitly enforces category-wise feature
alignment to learn shared feature representations and classiï¬ers for both domains simultaneously.
First  the centroids of category-wise features in the source domain are used as anchors to identify
the active features in the target domain. Then  we assign pseudo-labels to these active features
according to the category of the closest anchor. Lastly  two loss functions are proposed: the ï¬rst is a
pixel-level distance loss between the guiding anchors and active features  which pushes them closer
and explicitly minimizes the intra-category feature variance; the other is a pixel-level discriminative
loss to supervise the classiï¬er and maximize the inter-category feature variance. To reduce the error
accumulation of incorrect pseudo-labels  we propose a stagewise training mechanism to adapt the
model progressively.
The main contributions of this paper can be summarized as follows. First  we propose a novel category
anchor idea to tackle the challenging UDA problem in semantic segmentation. Second  we propose a
simple yet effective category anchor-based method to identify active features in the target domain 
further enabling category-wise feature alignment. Finally  the proposed CAG-UDA model achieves
new state-of-the-art performance in both GTA5â†’Cityscapes and SYNTHIAâ†’Cityscapes scenarios.

2 Related Work

Many recent advances in computer vision [20  12  30  11  24  46  5] have been based on deep neural
networks trained on large-scale labeled datasets such as ImageNet [9]  Pascal VOC [10]  MS COCO
[22]  and Cityscapes [8]. However  a domain shift between training data and testing data impairs
model performance [29  17  18]. To overcome this issue  a variety of domain adaptation methods for
classiï¬cation [6  23  37  28  41  3  19]  detection [38  16]  and segmentation [7  14  13  27  34  40  21 
47] have been proposed. In this paper  we focus on the challenging semantic segmentation problem.
The current mainstream approaches include style transfer [27  34  40  13  21]  feature alignment
[7  14  13]  and self-training [47  21]. As our work is most related to the latter two approaches  we
brieï¬‚y review and discuss their characteristics.
Feature distribution alignment: Previous methods that match the global marginal distributions be-
tween two domains [14  13  27] do not distinguish local category-wise feature distribution shifts.
Consequently  error-prone predictions are made for misaligned features with shared decision bound-

2

aries. In contrast to these methods  we propose a category-wise feature alignment method to explicitly
reduce category-level mismatches and learn discriminative domain-invariant features. The idea of
category-level feature alignment was also exploited in [26  33] for semantic segmentation. Luo
et al. proposed a weighted adversarial learning method to align the category-level feature distri-
butions implicitly [26]. Saito et al. tried to align the feature distributions and learn discriminative
domain-invariant features by utilizing task-speciï¬c classiï¬ers as a discriminator [33]. In contrast
to the implicit feature alignment in the aforementioned methods  we propose a novel category
anchor-guided method  which directly aligns category-wise features in both domains.
Pseudo-label assignment: Assigning pseudo-labels to target domain samples based on the trained
classiï¬ers helps adapt the feature extractor and classiï¬er to the target domain. Zou et al. [47] proposed
an iterative self-training UDA model by alternatively generating pseudo-labels and retraining the
model. They also dealt with the category imbalance issue by controlling the proportion of selected
pseudo-labels in each category [47]. Li et al. [21] proposed a bidirectional learning domain adaptation
model that alternately trains the image translation model and the self-supervised segmentation
adaptation model. In contrast to these methods  where pseudo-labels were determined according
to the predicted category probability  we propose a category anchor-based method to generate
trustable pseudo-labels. Compared with selected samples that have been â€œcorrectlyâ€ classiï¬ed with
high conï¬dence  our selected samples are not determined by the decision boundaries so are more
informative for the classiï¬er to further adapt to the target domain.
The idea of assigning pseudo-labels based on category centers has also been utilized in domain
adaptation for classiï¬cation  e.g.  category centroids in [41]  prototypes in [3]  and cluster centers
in [19]. The former two methods minimize the distance loss against category centroids  while the
third minimizes contrastive domain discrepancies. Our method differs from these methods in several
ways. First  we tackle the more challenging task of image semantic segmentation rather than image
classiï¬cation  where dense pixel-wise labels need to be predicted as not just single labels for entire
images. Second  we ï¬x the category centroids (hence called category anchors) instead of updating
them at each iteration. On one hand  the mini-batch size used for segmentation (e.g.  1) in this paper
is much smaller than that used for classiï¬cation. On the other hand  pixels are spatially coherent
in an image  so the category centroids calculated at each iteration will be biased and unreliable due
to the dominance of homogeneous features. Third  the pseudo-labels of target domain samples are
determined by their distance against the category centroids from the source domain instead of the
target domain. This is reasonable since: 1) the source domain category centroids are calculated from
all training samples based on ground-truth labels  which are reliable; 2) driving the target domain
features towards the source domain category centroids can effectively reduce the domain discrepancy.
Fourth  together with the category anchor-based distance loss  we also add the segmentation loss
based on the pseudo-labeled target samples to learn discriminative feature representations and adapt
the decision boundaries simultaneously.

3 A category anchor-guided UDA model for semantic segmentation

3.1 Problem Formulation

Supervised semantic segmentation: A semantic segmentation model M can be formulated as a
mapping function from the image domain X to the output label domain Y :

(1)
which predicts a pixel-wise category label Ë†y close to the ground-truth annotation y âˆˆ Y for a given
image x âˆˆ X. Usually  the segmentation model M is trained in a supervised manner by minimizing
the difference between the prediction Ë†y and its ground-truth y for every training sample x. The
cross-entropy (CE) loss is widely used as a measurement  which is deï¬ned as:

M : X â†’ Y 

LCE = âˆ’ N(cid:88)

HÃ—W(cid:88)

C(cid:88)

yijclog (pijc)  

(2)

i=1

j=1

c=1

the ground-truth label  i.e.  âˆ€i  j (cid:80)

where N is the number of training images  H and W denote the image size  j is the pixel index  C is
the number of categories  c is the category index  yijc âˆˆ {0  1} is the one-hot vector representation of
c yijc = 1  and pijc is the predicted category probability by M.
UDA for semantic segmentation: Generally  a segmentation model trained on a source domain Xs
has a limited generalization capability to a target domain Xt  when the distributions between Xs and

3

Figure 1: An illustration of the proposed category anchor-guided UDA model for semantic seg-
mentation. (a) The architecture of the proposed CAG-UDA model consists of an encoder  a feature
transformer (fD)  and a classiï¬er. The green part denotes the source domain ï¬‚ow while the orange
parts represent the target domain ï¬‚ow. (b) The illustration of the process of active target sample
identiï¬cation and pseudo label assignment described in Section 3.2. (c) The illustration of the
proposed category-wise feature alignment with the anchor-based pixel-level distance loss Ldis and
cross-entropy loss LCE described in Section 3.3. Best viewed in color.

Xt are different  i.e.  there is a domain shift/discrepancy. Several unsupervised domain adaptation
models have been proposed  which can be formulated as the following mapping function:

Muda : Xs âˆª Xt â†’ Ys âˆª Yt 

(3)
where Muda is trained on the labeled training samples (Xs  Ys) in the source domain together with
the training unlabeled samples Xt in the target domain. Typically  the aforementioned CE loss and
some domain-adaptation losses are used to align the distributions of both domains (e.g.  p (Xs) and
p (Ys)) and to learn domain-invariant discriminative feature representations.
Model components: The main semantic segmentation approaches have been based on fully con-
volutional neural networks (CNNs) since the seminal work in [24]. Usually  a DCNN-based model
has two parts: an encoder Enc and a decoder Dec  where the encoder maps the input image into a
low-dimensional feature space and then the decoder decodes it to the label space. The decoder can be
further divided into a feature transformation net fD and a classiï¬er Cls  where Cls denotes the last
classiï¬cation layer and fD denotes the remaining part in Dec. Typical encoders are the classiï¬cation
networks pretrained on ImageNet [9]  e.g.  VGGNet [35] and ResNet [12]. The decoder consists of
convolutional layers responsible for context modeling  multi-scale feature fusion  etc. UDA methods
typically employ a segmentation model with carefully designed modules for domain adaptation.

3.2 Network Architecture

The network architecture of our proposed CAG-UDA model is shown in Figure 1(a). The CAG-UDA
model employs Deeplab v2 [4] as the base segmentation model  where ResNet-101 is used as the
encoder Enc and the ASPP module is used in the decoder Dec. To reduce the domain shift  we
devise a category anchor-guided alignment module on the features from fD  consisting of category
anchor construction (CAC)  active target sample identiï¬cation (ATI)  and pseudo-label assignment
(PLA) as shown in Figure 1(b). The details are as follows.
Category anchor construction (CAC): Based on the observation that pixels in the same category
cluster in the feature space  we propose to calculate the centroids of the features of each category in
the source domain as a representative of the feature distribution  i.e.  the mean. Considering that the
features fed into the classiï¬er directly relate to the decision boundaries  we choose the features from
fD to calculate these centroids. Mathematically  this can be written as:
i ))|j)  

ys
ijc (fD (Enc (xs

HÃ—W(cid:88)

N(cid:88)

f s
c =

(4)

1
|Î›s
c|

i=1

j

where Î›s
to the cth category  i.e.  Î›s

c is the index set of all pixels on the training images in the source domain Xs belonging
c  i.e. 

c| denotes the number of pixels in Î›s

c = {(i  j)|ys

ijc = 1}  |Î›s

4

ğ‘†ğ‘‡classifierencoderğ‘“ğ·ğ‘†ğ‘‡ğ‘†ğ‘‡PredictionGroundtruthPseudolabelsPredictionImagesImagesLossğ¿ğ‘‘ğ‘–ğ‘ ğ‘†CategoryanchorSourcesampleLossğ¿ğ‘‘ğ‘–ğ‘ ğ‘‡CategoryanchorActivesampleCAG architectureATIandPLACategoryanchorsâ€¦â€¦1219active(CA1)FeaturespaceTargetsamplesinactiveactive(CA19)(CA19)(CA1)âˆ†ğ‘‘hyperbola:ğ‘‘1âˆ’ğ‘‘19=Î”ğ‘‘DomainadaptationLossğ¿ğ¶ğ¸ğ‘‡Lossğ¿ğ¶ğ¸ğ‘†Global marginal distributionsCAG category-wise alignment(b)(a)(c)ğ¿;<ğ¿ğ‘‘ğ‘–ğ‘ TargetsampleSourcesampleDecisionboundaryc| =(cid:80)N

(cid:80)HÃ—W

j

i=1

yijc  and fD (xs

i )|j is the feature vector at index j on the feature map fD (xs
|Î›s
i ).
It is noteworthy that we calculate the category centroids at the beginning of each training stage
and then keep them ï¬xed during training (we propose a stagewise training mechanism in Section
3.4.). Therefore  we call these centroids category anchors (CAs) in this paper  i.e.  CA = {f s
c   c =
1  ...  C}.
Active target sample identiï¬cation (ATI): To align the category-wise feature distributions between
two domains  we expect that the category centroids from the target domain get closer to the category
anchors during training. However  on one hand  target sample labels are unavailable. On the other
hand  the calculated centroids on target samples are very unstable at each iteration since the mini-
batch size is very small (i.e.  1) in this paper and image pixels are spatially coherent. To tackle
these issues  we propose identifying active target samples and assigning them pseudo-labels for the
subsequent feature alignment. The term â€œactive target samplesâ€ refers to target samples near one
category anchor and far from the other anchors  i.e.  being activated by one speciï¬c category anchor.
Mathematically  this can be formulated as follows. We ï¬rst deï¬ne the distance between a target
feature fD (Enc (xt

(cid:0)Enc(cid:0)xt
i))|j and the cth category anchor as
where (cid:107)Â·(cid:107)2 is the L2 norm of a vector. Then  we sort {dt
compare the shortest distance dt
predeï¬ned threshold (cid:52)d  we identify this target sample as active one  i.e. 

(cid:1)(cid:1)|j
(5)
ijc  c = 1  ...  C} in an ascending order and
(cid:48) . If their difference is larger than a

ijcâˆ— with the second shortest dt

ijc =(cid:13)(cid:13)f s

c âˆ’ fD

(cid:13)(cid:13)2

dt

ijc

 

i

(cid:48) âˆ’ dt
dt
ijc
otherwise 

ijcâˆ— > (cid:52)d 

0 

at
ij =

i according to its closest category anchor f s

ij denotes the active state of the target feature fD (Enc (xt

(6)
i))|j. Like the category anchors 
where at
we calculate the active states at the beginning of each training stage and keep them ï¬xed during
subsequent stages. This is explained in Section 3.4  where we introduce a stagewise training
mechanism.
Pseudo-label assignment (PLA): After we obtain the active state according to Eq. (6)  a pseudo
câˆ— with a reliable margin (cid:52)d:
label câˆ— can be assigned to xt
(7)
Due to the lack of the target domain labels  the classiï¬er layer is biased to the source domain and
does not generalize well to the target domain  as shown in Figure 1(c). Consequently  some of the
pseudo-labels from predicted probabilities may be error-prone. However  based on the observation
of the intra-category clustering characteristics  the generated pseudo-labels via category anchors
are independent of the biased classiï¬er and are thus more reliable than those assigned by predicted
category probabilities. Further  considering that high-probability samples have been â€œcorrectlyâ€
classiï¬ed by the classiï¬er layer with high conï¬dence  these samples provide only weak supervision
signals. In contrast  active samples are more informative for adapting the classiï¬er to the target
domain as the classiï¬er layer may not predict these active samples with high probabilities.

ijc âˆ’ (cid:52)d âˆ€c (cid:54)= c

ijcâˆ— = 1  if dt
Ë†yt

ijcâˆ— < dt

âˆ—

.

(cid:26) 1 

3.3 Objective Functions

When training the CAG-UDA model  we leverage a CE loss Ls
propose a category-wise distance loss Ls
losses on the active target samples  i.e.  a CE loss Lt
on the pseudo-labels  to guide the adaptation process. These are deï¬ned as:

CE as deï¬ned in Eq. (2). We also
dis on the source domain samples and two domain adaptation
dis based

CE and a category-wise distance loss Lt

(8)

(9)

(10)

Ls

dis =

ijc (cid:107)f s
ys

c âˆ’ fD (Enc (xs

i ))|j(cid:107)2  

at
ij

Ë†yt

C(cid:88)
(cid:13)(cid:13)f s

c=1

ijclog(cid:0)pt
(cid:1)  
(cid:1)(cid:1)|j
(cid:0)Enc(cid:0)xt

ijc

i

(cid:13)(cid:13)2

.

Lt

dis =

at
ij

Ë†yt
ijc

c âˆ’ fD

i=1

c=1

j=1

N(cid:88)
HÃ—W(cid:88)
C(cid:88)
HÃ—W(cid:88)
CE = âˆ’ M(cid:88)
C(cid:88)
HÃ—W(cid:88)
M(cid:88)

j=1

i=1

Lt

i=1

j=1

c=1

5

Although only the active samples are directly driven towards the category anchors by Lt
dis  other
inactive target samples within each category may also follow the active samples due to being
clustered. Therefore  minimizing Lt
dis indeed reduces the intra-category variances in the target
domain. Meanwhile  Lt
CE leverages the pseudo-labels to update the network weights together with
the source domain CE loss  prompting the encoder  decoder  and classiï¬er to adapt to the target
domain and therefore reducing the intra- and inter-category variances simultaneously. The illustration
is show in Figure 1(c). To leverage the complementarity between the proposed category anchor-based
PLA and category probability-based PLA in [47]  we also identify active target samples based on the
predicted category probability and add an extra CE loss LtP

CE similar to Eq. (9).

(11)

CE = âˆ’ M(cid:88)

LtP

HÃ—W(cid:88)

C(cid:88)

atP
ij

Ë†ytP

i=1

j=1

c=1

(cid:0)Ls

(cid:1) + Î»2

(cid:1)  

ijc

ijclog(cid:0)pt
(cid:16)

(cid:17)

where atP
Then the ï¬nal objective function is as follows:

ij   ytP

ijc refer to the probability-based active state and assigned pseudo-labels respectively.

dis + Lt

dis

Lt

CE + LtP
CE

 

(12)

L = Ls

CE + Î»1

where Î»1 and Î»2 are loss weights.

3.4 Stagewise Training Procedure

We tried to train the CAG-UDA model in a single stage and update the pseudo-labels at each iteration.
However  it is not stable because there are some error-prone pseudo-labels  which may produce
incorrect supervision signals  lead to more erroneous pseudo-labels iteratively and trap the network
to a local minimum with poor performance eventually  e.g. less than 30 mIoU. To address this issue 
we propose a stagewise training mechanism as summarized in Algorithm 1. First  we pretrain the
segmentation model on the source domain. Then  we leverage the global feature alignment method
in [14] to warm up the training process and obtain a well-initialized model. Next  we train the
CAG-UDA model with the proposed losses for several stages. At the beginning of each stage  we
calculate the CAs  identify the active target samples  and assign pseudo-labels to them. By using this
stagewise delayed updating mechanism  we avoid updating the pseudo-labels at each iteration and
CE serve as two regularizations on the network.
reduce the error accumulation. Hence  Lt

dis and Lt

Algorithm 1 Stagewise training the CAG-UDA model
Input: training dataset: (Xs  Ys  Xt)  maximum stages: K  maximum iterations: L  distance

threshold: (cid:52)d.

0 according to [14];

0 â† (Xs  Ys) according to [4];

Output: MK and ( Ë†Ys  Ë†Yt).
1: Pretraining: M p
2: Warm-up: M0 â† (Xs  Ys) and M p
3: for k â† 1 to K do
c } â† Mkâˆ’1 and (Xs  Ys) according to Eq. (4);
CAC: {f s
4:
ATI: {dt
ijc} {at
5:
ijcâˆ—} â† {dt
PLA: {Ë†yt
6:
for n â† 1 to L do
7:
8:
9:
10:
11: end for
12: Prediction: ( Ë†Ys  Ë†Yt) â† (Xs  Xt) and MK.

ij} â† Mkâˆ’1  (Xs  Ys  Xt)  {f s
ijc} (cid:52)d according to Eq. (7);

SGD: training Mkâˆ’1 on (Xs  Ys  Xt  {Ë†yt

end for
Mk â† Mkâˆ’1

c } and (cid:52)d according to Eq. (5) and Eq. (6);

ijcâˆ—}  {f s

c }  {at

ij}) according to Eq.(12);

4 Experiments

4.1 Experimental Settings

Datasets and evaluation metrics: Following [21]  we evaluate the CAG-UDA model in two com-
mon scenarios  GTA5[31]â†’Cityscapes[8] and SYNTHIA[32]â†’Cityscapes[8]. GTA5 contains

6

Table 1: Results of the CAG-UDA model and SOTA methods ( GTA5â†’Cityscapes).

k
l
a
w
e
d
i
s

16.8
25.9
22.3
30.8
16.8
27.1
33.1
47.5
35.6
12.7
44.7

25.4
51.6

g
n
i
d
l
i
u
b

77.2
79.8
75.6
81.3
77.2
79.6
81.0
82.5
80.1
69.5
84.2
74.7
83.8

d
a
o
r

75.8
86.5
69.9
85.0
75.8
87.0
89.4
91.5
86.7
69.0
91.0

69.8
90.4

l
l
a
w

12.5
22.1
15.8
25.8
12.5
27.3
26.6
31.3
19.8
9.9
34.6
11.3
34.2

e
c
n
e
f

21.0
20.0
20.1
21.2
21.0
23.3
26.8
25.6
17.5
19.5
27.6

18.3
27.8

e
l
o
p

25.5
23.6
18.8
22.2
25.5
28.3
27.2
33.0
38.0
22.8
30.2

24.2
38.4

t
h
g
i
l

30.1
33.1
28.2
25.4
30.1
35.5
33.5
33.7
39.9
31.7
36.0

35.6
25.3

n
g
i
s

20.1
21.8
17.1
26.6
20.1
24.2
24.7
25.8
41.5
15.3
36.0

23.3
48.4

.
e
g
e
v

81.3
81.8
75.6
83.4
81.3
83.6
83.9
82.7
82.7
73.9
85.0

72.0
85.4

e
c
a
r
r
e
t

24.6
25.9
8.00
36.7
24.6
27.4
36.7
28.8
27.9
11.3
43.6
14.4
38.2

n
o
s
r
e
p

53.8
57.3
55.0
58.9
53.8
58.6
58.7
62.4
64.9
54.7
58.6

58.7
58.6

y
k
s

70.3
75.9
73.5
76.2
70.3
74.2
78.8
82.7
73.6
67.2
83.0
65.3
78.1

r
e
d
i
r

26.4
26.2
2.9
24.9
26.4
28.0
30.5
30.8
19.0
23.9
31.6

29.0
34.6

r
a
c

49.9
76.3
66.9
80.7
49.9
76.2
84.8
85.2
65.0
53.4
83.3

53.1
84.7

k
c
u
r
t

17.2
29.8
34.4
29.5
17.2
33.1
38.5
27.7
12.0
29.7
35.3

14.3
21.9

s
u
b

25.9
32.1
30.8
42.9
25.9
36.7
44.5
34.5
28.6
4.6
49.7
19.2
42.7

n
i
a
r
t

6.5
7.2
0.00
2.50
6.5
6.7
1.7
6.4
4.5
11.6
3.3

7.9
41.1

r
o
t
o
m

25.3
29.5
18.4
26.9
25.3
31.9
31.6
25.2
31.1
26.1
28.8

15.1
29.3

e
k
i
b

36.0
32.5
0.00
11.6
36.0
31.4
32.4
24.4
42.0
32.5
35.6

16.3
37.2

mIoU
36.6
41.4
33.3
41.7
36.6
43.2
45.5
45.4
42.7
33.6
48.5

34.1
50.2

Source only

AdaptSegNet[36]

Source only
DCAN[40]
Source only
CLAN[26]
AdvEnt[39]

DISE[2]

Cycada[13  21]

Source only

BLF[21]

Source only
CAG-UDA

Table 2: Results of the CAG-UDA model on the testing set ( GTA5â†’Cityscapes).

k
l
a
w
e
d
i
s

g
n
i
d
l
i
u
b

d
a
o
r

l
l
a
w

e
c
n
e
f

e
l
o
p

t
h
g
i
l

n
g
i
s

.
e
g
e
v

e
c
a
r
r
e
t

n
o
s
r
e
p

y
k
s

r
e
d
i
r

r
a
c

k
c
u
r
t

s
u
b

n
i
a
r
t

r
o
t
o
m

e
k
i
b

CAG-UDA 93.2

57.0

85.6

35.7

25.1

37.5

30.8

45.3

87.1

50.1

89.4

62.7

40.8

87.8

18.0

32.4

34.5

34.4

35.4

mIoU
51.7

24 966 1914Ã—1052-pixel images and has the same 19 category annotations as Cityscapes. SYN-
THIA contains 9 400 1914Ã—1052-pixel images and only has 16 common category annotations.
Cityscapes is divided into a training set  a validation set  and a testing set. The training set con-
sists of 2 957 2048Ã—1024-pixel images and the validation set contains 500 images at the same
resolution. Following common practice  we report the results on the Cityscapes validation set 
speciï¬cally  the category-wise intersection over union (IoU). Moreover  we also report the mean IoU
(mIoU) of all 19 categories in the GTA5â†’Cityscapes scenario and the 16 common categories in the
SYNTHIAâ†’Cityscapes scenario. Some methods [36  26  21] only reported mIoU for 13 common
categories in the SYNTHIAâ†’Cityscapes scenario  denoted as mIoU* in this paper.
Implementation details: In our experiments  training images were randomly cropped to 1280Ã—640
pixels after being randomly resized by Ã—1 âˆ¼ Ã—1.5. Due to GPU memory limitations  the batch size
was set to 1 and the weights of all batch normalization layers were frozen. In the warm-up phase 
we used a CNN-based domain discriminator comprising 5 convolutional layers of kernel size 3Ã—3 
ï¬lter numbers [64  128  256  512  1]  and stride 2. The ï¬rst three convolutional layers are followed by
a ReLU layer  while the fourth layer is followed by a leaky ReLU layer parameterized by 0.2. We
used a CE loss and an adversarial loss to train the model for 20 epochs. The adversarial loss weights
were set to 1e-2. In the stagewise training phase  we trained the CAG-UDA mode for 20 epochs
with the SGD optimizer. The initial learning rate was 2.5e-4  which decayed by the poly policy with
power 0.9. The weight decay  momentum  Î»1  and Î»2 were set to 1e-4  0.9  0.3  and 0.7  respectively.
(cid:52)d was set to 2.5. We also assigned pseudo-labels based on predicted category probabilities  and
the threshold P0 was set to 0.95. Experiments were conducted on a TITAN Tesla V100 GPU with
PyTorch implementation. Code will be made publicly available.

4.2 Main Results
Quantitative results: The results of the GTA5â†’Cityscapes scenario are presented in Table 1 with
the best results highlighted in bold. All the models adopted ResNet-101 as a backbone network for
fair comparison. Overall  our CAG-UDA model strikingly outperforms all other models with a 50.2
mIoU  surpassing the model trained on the source domain by a signiï¬cant gain of 16.1. Compared
with CLAN [26] and DISE [2]  which implicitly align category-level features  our model achieves an
extra gain of 4.5 and outperforms them on fence  trafï¬c sign  rider  train  and bike by large margins.
This is due to the proposed category anchor-guided alignment method  which explicitly uses category
centroids as representatives of feature distributions  reducing the side effect of category imbalance.
Like [40  13]  BLF in [21] also involves a style-transfer module but combines it with self-training
in a bidirectional learning framework. It achieved the second-best mIoU of 48.5. BLF achieves
better results than the CAG-UDA model on stuff categories such as road  building  wall  terrace 
and sky but is inferior to the CAG-UDA model for small objects. This is because BLF includes a

7

Table 3: Results of the CAG-UDA model and SOTA methods ( SYNTHIAâ†’Cityscapes).

k
l
a
w
e
d
i
s

37.2
37.0
46.7
41.7

36.4
53.5
42.2
40.8

g
n
i
d
l
i
u
b

78.8
80.1
80.3
85.5
75.7
77.1
79.7
81.7

d
a
o
r

79.2
81.3
86.0
84.8

82.8
91.7
85.6
84.7

l
l
a
w

e
c
n
e
f

e
l
o
p

-
-
-
-

5.1
2.5
8.7
7.8

-
-
-
-

0.1
0.2
0.4
0.0

-
-
-
-

25.8
27.1
25.9
35.1

t
h
g
i
l

9.9
16.1
14.1
13.7

8.0
6.2
5.4
13.3

n
g
i
s

10.5
13.7
11.6
23.0
18.7
7.6
8.1
22.7

e
l
b
a
t
e
g
e
v

78.2
78.2
79.2
86.5
74.7
78.4
80.4
84.5

n
o
s
r
e
p

53.5
53.4
54.1
66.3
51.1
55.8
57.9
64.2

r
e
d
i
r

19.6
21.2
27.9
28.1
15.9
19.2
23.8
27.8

y
k
s

80.5
81.5
81.3
78.1

76.9
81.2
84.1
77.6

r
a
c

67.0
73.0
73.7
81.8

77.7
82.3
73.3
80.9

s
u
b

29.5
32.9
42.2
21.8

24.8
30.3
36.4
19.7

r
o
t
o
m

21.6
22.6
25.7
22.9

4.1
17.1
14.2
22.7

e
k
i
b

31.3
30.7
45.3
49.0
37.3
34.3
33.0
48.3

mIoU mIoU*
45.9
47.8
51.4
52.6

-
-
-
-

38.4
41.5
41.2
44.5

-
-
-
-

AdaptSegNet[36]

CLAN[26]
BLF[21]

CAG-UDA(13)

DCAN[40]
DISE[2]

AdvEnt[39]

CAG-UDA(16)

Figure 2: (a) Subjective evaluation of the CAG-UDA model on some images from the Cityscapes
validation set. (b) Comparison between probability-based PLA and the proposed CAs-based PLA on
an image from the Cityscapes training set. Best viewed in color and zoom-in.

style-transfer module that beneï¬ts from the texture clues in the stuff categories and assigns reliable
pseudo-labels accordingly. In contrast  CAG-UDA uses a category-anchor guided method that can
tackle the category imbalance and generate more informative pseudo-labels  leading to better results
on more categories.
We also present the result on the testing set of the Cityscapes dataset in Table 2. The CAG-UDA
model reaches 51.7 mIoU  proving the good generalization of our method.
Results in the SYNTHIAâ†’Cityscapes scenario are listed in Table 3. Same as the previous work  we
report the performance of the CAG-UDA model in two mIoU metrics: 13 categories (mIoU*) and 16
categories (mIoU) for fair comparisons. Since the domain shift is much larger than the above scenario 
the performance is slightly worse. The CAG-UDA model still achieves better results than all previous
SOTA methods  including CLAN  BLF  etc. Similar to the above discussions with the GTA5 dataset 
the superiority of the CAG-UDA model remains in small objects like pole  sign  person  and bike.
Qualitative results: Some qualitative segmentation examples are given in Figure 2(a). Training
merely on the source domain dataset leads to a limited generalization ability  e.g.  the road and person
were incorrectly predicted as sidewalk and building in the ï¬rst row. Beneï¬ted from the category
anchor-guided adaptation  the proposed CAG-UDA model achieves better results  especially for small
objects  e.g.  pole  sign  and person. Besides  we also attribute it to the proposed CAs-based pseudo
label assignment  which successfully activated small objects and assigned them trustable pseudo-
labels  as highlighted in red circles in Figure 2(b). More results can be found in the supplement.

8

Table 4: Results of ablation study (GTA5â†’Cityscapes).

d
a
o
r

69.8
88.4
88.8
88.3
89.4
88.9
88.1
88.9
88.8
90.4
90.4

.
e
d
i
s

25.4
45.2
45.5
46.9
40.1
41.7
46.6
47.1
47.5
50.6
51.6

.
l
i
u
b

74.7
82.0
83.7
81.5
81.8
82.0
82.1
83.0
83.6
84.0
83.8

l
l
a
w

11.3
30.1
33.2
28.7
31.0
31.7
30.2
31.0
31.7
33.5
34.2

.
c
n
e
f

18.3
22.0
21.4
27.7
22.6
22.5
28.4
27.3
29.1
28.3
27.8

e
l
o
p

24.2
35.4
39.5
38.9
39.9
39.7
39.7
39.7
39.7
39.9
38.4

t
h
g
i
l

35.6
36.7
40.0
27.0
41.2
41.2
31.3
31.0
34.4
31.6
25.3

n
g
i
s

23.3
23.7
25.9
40.4
23.2
23.5
38.8
36.0
35.6
42.4
48.4

.
e
g
e
v

72.0
82.7
83.9
83.7
83.0
82.7
83.6
84.3
84.4
85.1
85.4

.
r
r
e
t

14.4
27.6
33.8
31.2
28.3
27.0
30.7
32.6
33.0
35.2
38.2

y
k
s

65.3
70.8
74.3
74.9
68.5
70.0
75.1
75.1
76.8
77.3
78.1

n
o
s
r
e
p

58.7
51.4
58.2
61.8
54.5
57.8
61.9
62.0
62.1
61.5
58.6

r
e
d
i
r

29.0
26.9
24.9
30.2
23.8
25.7
28.5
29.4
28.2
34.2
34.6

r
a
c

53.1
81.5
84.8
84.0
85.7
85.8
84.3
84.6
84.5
84.9
84.7

k
c
u
r
t

14.3
14.5
19.3
15.9
21.5
21.9
16.3
16.6
17.2
19.4
21.9

s
u
b

19.2
25.0
32.8
36.7
25.6
27.7
36.3
35.7
35.2
41.7
42.7

n
i
a
r
t

7.9
21.4
22.6
23.4
0.7
1.1
29.1
27.2
32.0
41.0
41.1

r
o
t
o
m

15.1
13.0
15.0
23.3
13.9
18.0
25.0
19.2
25.8
27.3
29.3

e
k
i
b

16.3
7.9
14.7
31.7
8.5
11.1
29.4
28.4
27.6
32.0
37.2

mIoU gain
34.1
41.4
44.3
46.1
41.2
42.1
46.6
46.3
47.2
49.5
50.2

-
7.3
10.2
12.0
7.1
8.0
12.5
12.2
13.1
15.4
16.1

Source only
Warm-up
+LtP
CE
+Lt
dis + LtP
dis
dis + Lt
dis
dis + Lt
CE + LtP
CE

+LsP
+Ls
dis + Lt
+Lt

CE

+Ls

CE

CAG-UDA (Stage 1)
CAG-UDA (Stage 2)
CAG-UDA (Stage 3)

Ablation studies: The ablation study results are listed in Table 4. We add a superscript P to the
symbols of losses to denote that the active target samples are identiï¬ed by category probabilities
as described in Section 3.3. Several models were trained by combining Lt
CE with different losses.
As can be seen from the 2nd and 3rd rows  the proposed category anchor-guided PLA is more
effective than the predicted category probability-based one. More detailed comparisons of different
hyper-parameters can be found in the supplement. In addition  the CE loss is more effective than the
distance loss. The results in the 4th row demonstrate the complementarity between the CE loss and
distance loss  as well as between the category anchor-based and probability-based PLA. We combine
them as in Eq. (12) to train the CAG-UDA model and obtain a better result as listed in the bottom row.
Finally  the stagewise trained CAG-UDA model obtains an mIoU of 50.2  outperforming the SOTA
models. Besides  the CAG-UDA model has been trained for an extra stage  e.g.  Stage 4. However  it
is saturated at 50.2 mIoU with no improvement.

4.3 Limitations

The proposed CAG-UDA model relies on reliable pseudo-labels to guarantee a correct supervision
imposed on the network to be trained. To this end  we adopt a warm-up strategy to roughly align
two domains together and increase the reliability of the generated pseudo-labels by the CAs  as
described in Section 3.4. In contrast  we also conducted an experiment by removing the warm-up
stage and observed a signiï¬cant drop of 6.3 mIoU. Some techniques can also be used to obtain reliable
pseudo-labels such as enforcing local smoothness on the probability map  utilizing a normalized
threshold during assigning pseudo-labels  and reducing the appearance bias through a style transfer
module. We leave it as the future work to build a stage-free and end-to-end CAG-UDA model.

5 Conclusion

In this paper  we proposed a novel category anchor-guided (CAG) unsupervised domain adaptation
(UDA) model for semantic segmentation. The CAG-UDA model successfully adapts the segmentation
model to the target domain through category-wise feature alignment guided by category anchors.
Speciï¬cally  we proposed a category anchor construction module  an active target sample identiï¬cation
module  and a pseudo-label assignment module. We utilized a distance loss and a CE loss based on
the identiï¬ed active target samples  which complementarily enhance the adaptation performance. We
also proposed a stagewise training mechanism to reduce the error accumulation and adapt the CAG-
UDA model progressively. The experiments on the GTA5 and SYNTHIA datasets demonstrate the
superiority of the CAG-UDA model over representative methods on generalization to the Cityscapes
dataset.

Acknowledgements

This work is supported by the Australian Research Council Project FL-170100117 and the National
Natural Science Foundation of China Project 61806062.

9

References
[1] K. Bousmalis  N. Silberman  D. Dohan  D. Erhan  and D. Krishnan. Unsupervised pixel-level domain
adaptation with generative adversarial networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  pages 3722â€“3731  2017.

[2] W. Chang  H. Wang  W. Peng  and W. Chiu. All about structure: Adapting structural information across

domains for boosting semantic segmentation. CoRR  abs/1903.12212  2019.

[3] C. Chen  W. Xie  T. Xu  W. Huang  Y. Rong  X. Ding  Y. Huang  and J. Huang. Progressive feature

alignment for unsupervised domain adaptation. arXiv preprint arXiv:1811.08585  2018.

[4] L.-C. Chen  G. Papandreou  I. Kokkinos  K. Murphy  and A. L. Yuille. Deeplab: Semantic image
segmentation with deep convolutional nets  atrous convolution  and fully connected crfs. IEEE transactions
on pattern analysis and machine intelligence  40(4):834â€“848  2017.

[5] L.-C. Chen  Y. Zhu  G. Papandreou  F. Schroff  and H. Adam. Encoder-decoder with atrous separable
convolution for semantic image segmentation. In Proceedings of the European Conference on Computer
Vision (ECCV)  pages 801â€“818  2018.

[6] M. Chen  K. Q. Weinberger  and J. Blitzer. Co-training for domain adaptation. In Advances in Neural

Information Processing Systems (Neurips)  pages 2456â€“2464  2011.

[7] Y.-H. Chen  W.-Y. Chen  Y.-T. Chen  B.-C. Tsai  Y.-C. Frank Wang  and M. Sun. No more discrimination:
Cross city adaptation of road scene segmenters. In Proceedings of the IEEE International Conference on
Computer Vision (ICCV)  pages 1992â€“2001  2017.

[8] M. Cordts  M. Omran  S. Ramos  T. Rehfeld  M. Enzweiler  R. Benenson  U. Franke  S. Roth  and
B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3213â€“3223  2016.

[9] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 
pages 248â€“255. Ieee  2009.

[10] M. Everingham  L. Van Gool  C. K. Williams  J. Winn  and A. Zisserman. The pascal visual object classes

(voc) challenge. International journal of computer vision  88(2):303â€“338  2010.

[11] K. He  G. Gkioxari  P. DollÃ¡r  and R. Girshick. Mask r-cnn. In Proceedings of the IEEE International

Conference on Computer Vision (ICCV)  pages 2961â€“2969  2017.

[12] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 770â€“778  2016.

[13] J. Hoffman  E. Tzeng  T. Park  J.-Y. Zhu  P. Isola  K. Saenko  A. Efros  and T. Darrell. Cycada: Cycle-
consistent adversarial domain adaptation. In International Conference on Machine Learning (ICML) 
2018.

[14] J. Hoffman  D. Wang  F. Yu  and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based

adaptation. arXiv preprint arXiv:1612.02649  2016.

[15] W. Hong  Z. Wang  M. Yang  and J. Yuan. Conditional generative adversarial network for structured
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 1335â€“1344  2018.

[16] N. Inoue  R. Furuta  T. Yamasaki  and K. Aizawa. Cross-domain weakly-supervised object detection
through progressive domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  pages 5001â€“5009  2018.

[17] W. Jiang  H. Gao  W. Lu  W. Liu  F.-L. Chung  and H. Huang. Stacked robust adaptively regularized auto-
regressions for domain adaptation. IEEE Transactions on Knowledge and Data Engineering  31(3):561â€“
574  2018.

[18] W. Jiang  W. Liu  and F.-l. Chung. Knowledge transfer for spectral clustering. Pattern Recognition 

81:484â€“496  2018.

[19] G. Kang  L. Jiang  Y. Yang  and A. G. Hauptmann. Contrastive adaptation network for unsupervised

domain adaptation. arXiv preprint arXiv:1901.00976  2019.

[20] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiï¬cation with deep convolutional neural

networks. In Advances in Neural Information Processing Systems (Neurips)  pages 1097â€“1105  2012.

[21] Y. Li  L. Yuan  and N. Vasconcelos. Bidirectional learning for domain adaptation of semantic segmentation.

arXiv preprint arXiv:1904.10620  2019.

[22] T.-Y. Lin  M. Maire  S. Belongie  J. Hays  P. Perona  D. Ramanan  P. DollÃ¡r  and C. L. Zitnick. Microsoft
coco: Common objects in context. In Proceedings of the European Conference on Computer Vision
(ECCV)  pages 740â€“755. Springer  2014.

[23] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information

Processing Systems (Neurips)  pages 469â€“477  2016.

[24] J. Long  E. Shelhamer  and T. Darrell. Fully convolutional networks for semantic segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 3431â€“
3440  2015.

[25] M. Long  Y. Cao  J. Wang  and M. Jordan. Learning transferable features with deep adaptation networks.

In International Conference on Machine Learning (ICML)  pages 97â€“105  2015.

[26] Y. Luo  L. Zheng  T. Guan  J. Yu  and Y. Yang. Taking a closer look at domain shift: Category-level

adversaries for semantics consistent domain adaptation. arXiv preprint arXiv:1809.09478  2018.

[27] Z. Murez  S. Kolouri  D. Kriegman  R. Ramamoorthi  and K. Kim. Image to image translation for domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 

10

pages 4500â€“4509  2018.

[28] P. O. Pinheiro. Unsupervised domain adaptation with similarity learning. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 8004â€“8013  2018.

[29] G.-J. Qi  W. Liu  C. Aggarwal  and T. Huang. Joint intermodal and intramodal label transfers for extremely
rare or unseen classes. IEEE transactions on pattern analysis and machine intelligence  39(7):1360â€“1373 
2016.

[30] S. Ren  K. He  R. Girshick  and J. Sun. Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in Neural Information Processing Systems (Neurips)  pages 91â€“99  2015.
[31] S. R. Richter  V. Vineet  S. Roth  and V. Koltun. Playing for data: Ground truth from computer games. In

Proceedings of the European Conference on Computer Vision (ECCV)  pages 102â€“118. Springer  2016.

[32] G. Ros  L. Sellart  J. Materzynska  D. Vazquez  and A. M. Lopez. The synthia dataset: A large collection
of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  pages 3234â€“3243  2016.

[33] K. Saito  K. Watanabe  Y. Ushiku  and T. Harada. Maximum classiï¬er discrepancy for unsupervised
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)  pages 3723â€“3732  2018.

[34] S. Sankaranarayanan  Y. Balaji  A. Jain  S. Nam Lim  and R. Chellappa. Learning from synthetic data:
Addressing domain shift for semantic segmentation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  pages 3752â€“3761  2018.

[35] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

International Conference on Learning Representations (ICLR)  2015.

[36] Y.-H. Tsai  W.-C. Hung  S. Schulter  K. Sohn  M.-H. Yang  and M. Chandraker. Learning to adapt structured
output space for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  pages 7472â€“7481  2018.

[37] E. Tzeng  J. Hoffman  K. Saenko  and T. Darrell. Adversarial discriminative domain adaptation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  pages 7167â€“
7176  2017.

[38] D. Vazquez  A. M. Lopez  J. Marin  D. Ponsa  and D. Geronimo. Virtual and real world adaptation for
pedestrian detection. IEEE transactions on pattern analysis and machine intelligence  36(4):797â€“809 
2014.

[39] T.-H. Vu  H. Jain  M. Bucher  M. Cord  and P. PÃ©rez. Advent: Adversarial entropy minimization for domain

adaptation in semantic segmentation. arXiv preprint arXiv:1811.12833  2018.

[40] Z. Wu  X. Han  Y.-L. Lin  M. Gokhan Uzunbas  T. Goldstein  S. Nam Lim  and L. S. Davis. Dcan: Dual
channel-wise alignment networks for unsupervised scene adaptation. In Proceedings of the European
Conference on Computer Vision (ECCV)  pages 518â€“534  2018.

[41] S. Xie  Z. Zheng  L. Chen  and C. Chen. Learning semantic representations for unsupervised domain

adaptation. In International Conference on Machine Learning (ICML)  pages 5419â€“5428  2018.

[42] J. Zhang  Y. Cao  S. Fang  Y. Kang  and C. Wen Chen. Fast haze removal for nighttime image using
maximum reï¬‚ectance prior. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 7418â€“7426  2017.

[43] J. Zhang  Y. Cao  Y. Wang  C. Wen  and C. W. Chen. Fully point-wise convolutional neural network for
modeling statistical regularities in natural images. In 2018 ACM Multimedia Conference on Multimedia
Conference  pages 984â€“992. ACM  2018.

[44] J. Zhang and D. Tao. Famed-net: A fast and accurate multi-scale end-to-end dehazing network. IEEE

Transactions on Image Processing  29:72â€“84  2020.

[45] Y. Zhang  P. David  and B. Gong. Curriculum domain adaptation for semantic segmentation of urban scenes.
In Proceedings of the IEEE International Conference on Computer Vision (ICCV)  pages 2020â€“2030  2017.
[46] H. Zhao  J. Shi  X. Qi  X. Wang  and J. Jia. Pyramid scene parsing network. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  pages 2881â€“2890  2017.

[47] Y. Zou  Z. Yu  B. Vijaya Kumar  and J. Wang. Unsupervised domain adaptation for semantic segmentation
via class-balanced self-training. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 289â€“305  2018.

11

,Zhuo Wang
Alan Stocker
Daniel Lee
Qiming ZHANG
Jing Zhang
Wei Liu
Dacheng Tao