2015,Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with Sub-Exponential Designs,We consider the problem of high-dimensional structured estimation with norm-regularized estimators  such as Lasso  when the design matrix and noise are drawn from sub-exponential distributions.Existing results only consider sub-Gaussian designs and noise  and both the sample complexity and non-asymptotic estimation error have been shown to depend on the Gaussian width of suitable sets. In contrast  for the sub-exponential setting  we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets  and the analysis holds for any norm. Further  using generic chaining  we show that the exponential width for any set will be at most $\sqrt{\log p}$ times the Gaussian width of the set  yielding Gaussian width based results even for the sub-exponential case. Further  for certain popular estimators  viz Lasso and Group Lasso  using a VC-dimension based analysis  we show that the sample complexity will in fact be the same order as Gaussian designs. Our general analysis and results are the first in the sub-exponential setting  and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions.,Beyond Sub-Gaussian Measurements:

High-Dimensional Structured Estimation with

Sub-Exponential Designs

Vidyashankar Sivakumar

Arindam Banerjee

Department of Computer Science & Engineering

University of Minnesota  Twin Cities

{sivakuma banerjee}@cs.umn.edu

Pradeep Ravikumar

Department of Computer Science

University of Texas  Austin

pradeepr@cs.utexas.edu

Abstract

We consider the problem of high-dimensional structured estimation with norm-
regularized estimators  such as Lasso  when the design matrix and noise are drawn
from sub-exponential distributions. Existing results only consider sub-Gaussian
designs and noise  and both the sample complexity and non-asymptotic estimation
error have been shown to depend on the Gaussian width of suitable sets. In con-
trast  for the sub-exponential setting  we show that the sample complexity and the
estimation error will depend on the exponential width of the corresponding sets 
and the analysis holds for any norm. Further  using generic chaining  we show that
the exponential width for any set will be at most
log p times the Gaussian width
of the set  yielding Gaussian width based results even for the sub-exponential case.
Further  for certain popular estimators  viz Lasso and Group Lasso  using a VC-
dimension based analysis  we show that the sample complexity will in fact be the
same order as Gaussian designs. Our general analysis and results are the ﬁrst in
the sub-exponential setting  and are readily applicable to special sub-exponential
families such as log-concave and extreme-value distributions.

√

1

Introduction

We consider the following problem of high dimensional linear regression:

y = Xθ∗ + ω  

(1)
where y ∈ Rn is the response vector  X ∈ Rn×p has independent isotropic sub-exponential ran-
dom rows  ω ∈ Rn has i.i.d sub-exponential entries and the number of covariates p is much larger
compared to the number of samples n. Given y  X and assuming that θ∗ is ‘structured’  usually char-
acterized as having a small value according to some norm R(·)  the problem is to recover ˆθ close
to θ∗. Considerable progress has been made over the past decade on high-dimensional structured
estimation using suitable M-estimators or norm-regularized regression [16  2] of the form:

ˆθλn = argmin
θ∈Rp

1
2n

(cid:107)y − Xθ(cid:107)2

2 + λnR(θ)  

(2)

where R(θ) is a suitable norm  and λn > 0 is the regularization parameter. Early work focused
on high-dimensional estimation of sparse vectors using the Lasso and related estimators  where
R(θ) = (cid:107)θ(cid:107)1 [13  22  23]. Sample complexity of such estimators have been rigorously established
based on the RIP(restricted isometry property) [4  5] and the more general RE(restricted eigenvalue)
conditions [3  16  2]. Several subsequent advances have considered structures beyond (cid:96)1  using more
general norms such as (overlapping) group sparse norms  k-support norm  nuclear norm  and so on
[16  8  7]. In recent years  much of the literature has been uniﬁed and nonasymptotic estimation
error bound analysis techniques have been developed for regularized estimation with any norm [2].

1

In spite of such advances  most of the existing literature relies on the assumption that entries in
the design matrix X ∈ Rn×p are sub-Gaussian. In particular  recent uniﬁed treatments based on
decomposable norms  atomic norms  or general norms all rely on concentration properties of sub-
Gaussian distributions [16  7  2]. Certain estimators  such as the Dantzig selector and variants 
consider a constrained problem rather than a regularized problem as in (2) but the analysis again
relies on entries of X being sub-Gaussian [6  8]. For the setting of constrained estimation  building
on prior work by [10]  [20] outlines a possible strategy for such analysis which can work for any
distribution  but works out details only for the sub-Gaussian case. In recent work [9] considered
sub-Gaussian design matrices but with heavy-tailed noise  and suggested modifying the estimator in
(1) via a median-of-means type estimator based on multiple estimates of ˆθ from sub-samples.
In this paper  we establish results for the norm-regularized estimation problem as in (2) for any
norm R(θ) under the assumption that elements Xij of the design matrix X ∈ Rn×p follow a sub-
exponential distribution  whose tails are dominated by scaled versions of the (symmetric) exponen-
tial distribution  i.e.  P (|Xij| > t) ≤ c1 exp(−t/c2) for all t ≥ 0 and for suitable constants c1  c2
[12  21]. To understand the motivation of our work  note that in most of machine learning and
statistics  unlike in compressed sensing  the design matrix cannot be chosen but gets determined
by the problem. In many application domains like ﬁnance  climate science  ecology  social net-
work analysis  etc.  variables with heavier tails than sub-Gaussians are frequently encountered. For
example in climate science  to understand the relationship between extreme value phenomena like
heavy precipitation variables from the extreme-value distributions are used. While high dimensional
statistical techniques have been used in practice for such applications  currently lacking is the the-
oretical guarantees on their performance. Note that the class of sub-exponential distributions have
heavier tails compared to sub-Gaussians but have all moments. To the best of our knowledge  this
is the ﬁrst paper to analyze regularized high-dimensional estimation problems of the form (2) with
sub-exponential design matrices and noise.
In our main result  we obtain bounds on the estimation error (cid:107) ˆ∆n(cid:107)2 = (cid:107)ˆθλn − θ∗(cid:107)2  where θ∗ is
the optimal structured parameter. The sample complexity bounds are log p worse compared to the
sub-Gaussian case. For example for the (cid:96)1 norm  we obtain n = O(s log2 p) sample complexity
bound instead of O(s log p) for the sub-Gaussian case. The analysis depends on two key ingredients
which have been discussed in previous work [16  2]: 1. The satisfaction of the RE condition on a set
A which is the error set associated with the norm  and 2. The design matrix-noise interaction man-
ifested in the form of lower bounds on the regularization parameter. Speciﬁcally  the RE condition
depends on the properties of the design matrix. We outline two different approaches for obtaining
the sample complexity  to satisfy the RE condition: one based on the ‘exponential width’ of A and
another based on the VC-dimension of linear predictors drawn from A [10  20  11]. For two widely
used cases  Lasso and group-lasso  we show that the VC-dimension based analysis leads to a sharp
bound on the sample complexity  which is exactly the same order as that for sub-Gaussian design
matrices! In particular  for Lasso with s-sparsity  O(s log p) samples are sufﬁcient to satisfy the RE
condition for sub-exponential designs. Further  we show that the bound on the regularization param-
eter depends on the ‘exponential width’ we(ΩR) of the unit norm ball ΩR = {u ∈ Rp|R(u) ≤ 1}.
Through a careful argument based on generic chaining [19]  we show that for any set T ⊂ Rp  the
exponential width we(T ) ≤ cwg(T )
log p  where wg(T ) is the Gaussian width of the set T and c
is an absolute constant. Recent advances on computing or bounding wg(T ) for various structured
sets can then be used to bound we(T ). Again  for the case of Lasso  we(ΩR) ≤ c log p.
The rest of the paper is organized as follows. In Section 2 we describe various aspects of the problem
and highlight our contributions. In Section 3 we establish a key result on the relationship between
Gaussian and exponential widths of sets which will be used for our subsequent analysis. In Section
4 we establish results on the regularization parameter λn  RE constant κ and the non-asymptotic
estimation error (cid:107) ˆ∆n(cid:107)2. We show some experimental results before concluding in Section 6.

√

2 Background and Preliminaries

In this section  we describe various aspects of the problem  introducing notations along the way  and
highlight our contributions. Throughout the paper values of constants change from line to line.

2

2.1 Problem setup

We consider the problem deﬁned in (2). The goal of this paper is to establish conditions for consis-
tent estimation and derive bounds on (cid:107) ˆ∆n(cid:107)2 = (cid:107)ˆθ − θ∗(cid:107)2.
n X T (y−Xθ∗))  β > 1  the error vector ˆ∆n = ˆθ−θ∗
Error set: Under the assumption λn ≥ βR∗( 1
lies in a cone A ⊆ Sp−1 [3  16  2].
n X T (y − Xθ)) following analysis in [16  2].
Regularization parameter: For β > 1  λn ≥ βR∗( 1
Restricted Eigenvalue (RE) conditions: For consistent estimation  the design matrix X should
n(cid:107)Xu(cid:107)2 ≥ κ on the error set A for some constant
1√
satisfy the following RE condition inf u∈A
κ > 0 [3  16  2  20  18]. The RE sample complexity is the number of samples n required to satisfy
the RE condition and has been shown to be related to the Gaussian width of the error set. [7  2  20].
Deterministic recovery bounds: If X satisﬁes the RE condition on the error set A and λn satisﬁes
the assumptions stated earlier  [2] show the error bound (cid:107) ˆ∆n(cid:107)2 ≤ cΨ(A) λn
κ with high probability
(w.h.p)  for some constant c  where Ψ(A) = supu∈A
(cid:96)1 norm regularization: One example for R(·) we will consider throughout the paper is the (cid:96)1
norm regularization. In particular we will always consider (cid:107)θ∗(cid:107)0 = s.
Group-sparse norms: Another popular example we consider is the group-sparse norm. Let G =
{G1 G2  . . .  GNG} denote a collection of groups  which are blocks of any vector θ ∈ Rp. For any
vector θ ∈ Rp  let θNG denote a vector with coordinates θNG
i = 0.
Let m = maxi∈[1 ···  NG ] |Gi| be the maximum size of any group. In the group sparse setting for
any subset SG ⊆ {1  2  . . .   NG} with cardinality |SG| = sG  we assume that the parameter vector
θ∗ ∈ Rp satisﬁes θ∗NG = (cid:126)0  ∀NG (cid:54)∈ SG. Such a vector is called SG-group sparse. We will focus on

i = θi if i ∈ GNG   else θNG

the case when R(θ) =(cid:80)NG

R(u)
(cid:107)u(cid:107)2

is the norm compatibility constant.

i=1 (cid:107)θi(cid:107)2.

2.2 Contributions

One of our major results is the relationship between the Gaussian and exponential width of sets
using arguments from generic chaining [19]. Existing analysis frameworks for our problem for
sub-Gaussian X and ω obtain results in terms of Gaussian widths of suitable sets associated with
the norm [2  20]. For sub-exponential X and ω this dependency  in some cases  is replaced by the
exponential width of the set. By establishing a precise relationship between the two quantities  we
leverage existing results on the computation of Gaussian widths for our scenario. Another contribu-
tion is obtaining the same order of the RE sample complexity bound as for the sub-Gaussian case
for (cid:96)1 and group-sparse norms. While this strong result has already been explored in [11] for (cid:96)1 
we adapt it for our analysis framework and also extend it to the group-sparse setting. As for the
application of our work  the results are applicable to all log-concave distributions which by deﬁ-
nition are distributions admitting a log-concave density f i.e. a density of the form f = eΨ with
Ψ any concave function. This covers many practically used distributions including extreme value
distributions.

3 Relationship between Gaussian and Exponential Widths
In this section we introduce a complexity parameter of a set we(·)  which we call the exponential
√
width of the set  and establish a sharp upper bound for it in terms of the Gaussian width of the set
wg(·). In particular  we prove the inequality: we(A) ≤ c · wg(A)
log p for some ﬁxed constant c.
To see the connection with the rest of the paper  remember that our subsequent results for λn and κ
are expressed in terms of the Gaussian width and exponential width of speciﬁc sets associated with
the norm. With this result  we establish precise sample complexity bounds by leveraging a body of
literature on the computation of Gaussian widths for various structured sets [7  20]. We note that
while the exponential width has been deﬁned and used earlier  see for e.g. [19  15]  to the best of
our knowledge this is the ﬁrst result establishing the relation between the Gaussian and exponential
widths of sets. Our result relies on generic chaining [19].

3

3.1 Generic Chaining  Gaussian Width and Exponential Widths
Consider a process {Xt}t∈T = (cid:104)h  t(cid:105) indexed by a set T ⊆ Rp  where each element hi has mean
0. It follows from the deﬁnition that the process is centered  i.e.  E(Xt) = 0 ∀t ∈ T . We will
also assume for convenience w.l.o.g that set T is ﬁnite. Also  for any s  t ∈ T   consider a canonical
distance metric d(s  t). We are interested in computing the quantity E supt∈T Xt. Now  for reasons
detailed in the supplement  consider that we split T into a sequence of subsets T0 ⊆ T1 ⊆ . . . ⊆ T  
with T0 = {t0}  |Tn| ≤ 22n for n ≥ 1 and Tm = T for some large m. Let function πn : T → Tn 
deﬁned as πn(t) = {s : d(s  t) ≤ d(s1  t) ∀s  s1 ∈ Tn}  maps each point t ∈ T to some point
s ∈ Tn closest according to d. The set Tn and the associated function πn deﬁne a partition An
of the set T . Each element of the partition An has some element s ∈ Tn and all t ∈ T closest to
it according to the map πn. Also the size of the partition |An| ≤ 22n. An are called admissible
sequences in generic chaining. Note that there are multiple admissible sequences corresponding to
multiple ways of deﬁning the sets T0  T1  . . .   Tm. We will denote by ∆(An(t)) the diameter of the
element An(t) w.r.t distance metric d deﬁned as ∆(An(t)) = sups t∈An(t) d(s  t).
Deﬁnition 1 γ-functionals: [19] Given α > 0  and a metric space (T  d) we deﬁne

γα(T  d) = inf sup

t

2n/α∆(An(t))  

(3)

(cid:88)

n≥0

1
L

(cid:16)− u2

(cid:17)

where the inf is taken over all possible admissible sequences of the set T .
Gaussian width: Let {Xt}t∈T = (cid:104)g  t(cid:105) where each element gi is i.i.d N (0  1). The quantity
wg(T ) = E supt∈T Xt is called the Gaussian width of the set T . Deﬁne the distance metric
d2(s  t) = (cid:107)s − t(cid:107)2. The relation between Gaussian width and the γ-functionals is seen from
the following result from [Theorem 2.1.1] of [19] stated below:

γ2(T  d2) ≤ wg(T ) ≤ Lγ2(T  d2) .

(4)

Note that  following [Theorem 2.1.5] in [19] any process which satisﬁes the concentration bound
P (|Xs − Xt| ≥ u) ≤ 2 exp
Exponential width: Let {Xt}t∈T = (cid:104)e  t(cid:105) where each element ei is is a centered i.i.d exponential
random variable satisfying P (|ei| ≥ u) = exp(−u). Deﬁne the distance metrics d2(s  t) = (cid:107)s− t(cid:107)2
and d∞(s  t) = (cid:107)s − t(cid:107)∞. The quantity we(T ) = E supt∈T Xt is called the exponential width of
the set T . By [Theorem 1.2.7] and [Theorem 5.2.7] in [19]  for some universal constant L  we(T )
satisﬁes:

satisﬁes the upper bound in (4).

d2(s t)2

(γ2(T  d2) + γ1(T  d∞)) ≤ we(T ) ≤ L(γ2(T  d2) + γ1(T  d∞))

(5)
Note that any process which satisﬁes the sub-exponential concentration bound P (|Xs− Xt| ≥ u) ≤
2 exp

satisﬁes the upper bound in the above inequality [15  19].

(cid:16)−K min

(cid:16) u2

(cid:17)(cid:17)

1
L

d2(s t)2  

u

d∞(s t)

3.2 An Upper Bound for the Exponential Width

In this section we prove the following relationship between the exponential and Gaussian widths:
Theorem 1 For any set T ⊂ Rp  for some constant c the following holds:

we(T ) ≤ c · wg(T )(cid:112)log p .

Proof: The result depends on geometric results [Lemma 2.6.1] and [Theorem 2.6.2] in [19].
Theorem 2 [19] Consider a countable set T ⊂ Rp  and a number u > 0. Assume that the
Gaussian width is bounded i.e. S = γ2(T  d2) ≤ ∞. Then there is a decomposition T ⊂ T1 + T2
where T1 + T2 = {t1 + t2 : t1 ∈ T1  t2 ∈ T2}  such that

γ2(T1  d2) ≤ LS  
γ2(T2  d2) ≤ LS  

γ1(T1  d∞) ≤ LSu
T2 ⊂ LS
u

B1  

where L is some universal constant and B1 is the unit (cid:96)1 norm ball in Rp.

(6)

(7)

(8)

4

We ﬁrst examine the exponential widths of the sets T1 and T2. For the set T1:

we(T1) ≤ L[γ2(T1  d2) + γ1(T1  d∞)] ≤ L[S + Su] = L(wg(T ) + wg(T )u)  

(9)
where the ﬁrst inequality follows from (5) and the second inequality follows from (7). We will
need the following result on bounding the exponential width of an unit (cid:96)1-norm ball in p dimensions
to compute the exponential width of T2. The proof  given in the supplement  is based on the fact
supt∈B1(cid:104)e  t(cid:105) = (cid:107)e(cid:107)∞ and then using a simple union bound argument to bound (cid:107)e(cid:107)∞.
Lemma 1 Consider the set B1 = {t ∈ Rp : (cid:107)t(cid:107)1 ≤ 1}. Then for some universal constant L:

(cid:20)

(cid:21)

we(B1) = E

(cid:104)e  t(cid:105)

sup
t∈B1

≤ L log p .

(10)

The exponential width of T2 is:
we(T2) = we((LS/u)B1) = (LS/u)we(B1) = (L/u)wg(T )we(B1) ≤ (L/u)wg(T ) log p . (11)
The ﬁrst equality follows from (8) as T2 is a subset of a (LS/u)-scaled (cid:96)1 norm ball  the second
inequality follows from elementary properties of widths of sets and the last inequality follows from
Lemma 1. Now as stated in Theorem 2  u in (9) and (11) is any number greater than 0. We choose
u =

log p for some constant L yields:

log p and noting that (1 +

√

√

√
log p) ≤ L

we(T1) ≤ Lwg(T )(cid:112)log p 

(12)
The ﬁnal step  following arguments as [Theorem 2.1.6] [19]  is to bound exponential width of set T .

we(T2) ≤ Lwg(T )(cid:112)log p
(cid:104)h  t2(cid:105)] ≤ we(T1) + we(T2) ≤ Lwg(T )(cid:112)log p .

we(T ) = E[sup
t∈T

(cid:104)h  t(cid:105)] ≤ E[ sup
t1∈T1

(cid:104)h  t1(cid:105)] + E[ sup
t2∈T2

This proves Theorem 1.

4 Recovery Bounds
We obtain bounds on the error vector ˆ∆n = ˆθ − θ∗.
βR∗( 1
stant κ  then [2  16] obtain the following error bound w.h.p for some constant c:

If the regularization parameter λn ≥
n X T (y − Xθ∗))  β > 1 and the RE condition is satisﬁed on the error set A with RE con-

(cid:107) ˆ∆n(cid:107)2 ≤ c · λn
κ

Ψ(A)  

(13)

where Ψ(A) is the norm compatibility constant given by supu∈A(R(u)/(cid:107)u(cid:107)2).
4.1 Regularization Parameter

As discussed earlier 
βR∗( 1
implying that λn ≥ βR∗( 1
entries 

for our analysis the regularization parameter should satisfy λn ≥
n X T (y − Xθ∗))  β > 1. Observe that for the linear model (1)  ω = y − Xθ∗ is the noise 
(cid:20)
R∗(cid:18) 1
n X T ω). With e denoting a sub-exponential random vector with i.i.d

(cid:28) 1

(cid:29)(cid:21)

(cid:19)(cid:21)

E[(cid:107)ω(cid:107)2]E

(cid:104)e  u(cid:105)

(cid:107)ω(cid:107)2

.

(14)

  u

=

(cid:20)

(cid:20)

(cid:21)

E

X T ω

= E

n

sup
u∈ΩR

sup
u∈ΩR

X T ω
(cid:107)ω(cid:107)2

n

1
n

The ﬁrst equality follows from the deﬁnition of dual norm. The second inequality follows from
the fact that X and ω are independent of each other. Also by elementary arguments [21] 
e = X T (ω/|ω(cid:107)2) has i.i.d sub-exponential entries with sub-exponential norm bounded by
supω∈Rn (cid:107)(cid:104)X T
i   ω/(cid:107)ω(cid:107)2(cid:105)(cid:107)ψ1. The above argument was ﬁrst proposed for the sub-Gaussian case
in [2]. For sub-exponential design and noise  the difference compared to the sub-Gaussian case is
the dependence on the exponential width instead of the Gaussian width of the unit norm ball. Us-
ing known results on the Gaussian widths of unit (cid:96)1 and group-sparse norms  corollaries below are
derived using the relationship between Gaussian and exponential widths derived in Section 3:

5

Corollary 1 If R(·) is the (cid:96)1 norm  for sub-exponential design matrix X and noise ω 

(cid:20)
R∗(cid:18) 1

n

E

(cid:19)(cid:21)

X T (y − Xθ∗)

≤ η0√
n

log p .

(15)

Corollary 2 If R(·) is the group-sparse norm  for sub-exponential design matrix X and noise ω 

(cid:20)
R∗(cid:18) 1

n

E

(cid:19)(cid:21)

(cid:112)(m + log NG) log p .

X T (y − Xθ∗)

≤ η0√
n

(16)

( 1√

( 1√

n )(cid:107)Xu(cid:107)2 ≤ sup
u∈A

4.2 The RE condition
For Gaussian and sub-Gaussian X  previous work has established RIP bounds of the form κ1 ≤
n )(cid:107)Xu(cid:107)2 ≤ κ2. In particular  RIP is satisﬁed w.h.p if the number of
inf
u∈A
samples is of the order of square of the Gaussian width of the error set  i.e.  O(w2
g(A))  which we
will call the sub-Gaussian RE sample complexity bound. As we move to heavier tails  establishing
such two-sided bounds requires assumptions on the boundedness of the Euclidean norm of the rows
of X [15  17  10]. On the other hand  analysis of only the lower bound requires very few assumptions
on X. In particular  (cid:107)Xu(cid:107)2 being the sum of random non-negative quantities the lower bound should
be satisﬁed even with very weak moment assumptions on X. Making these observations  [10  17]
develop arguments obtaining sub-Gaussian RE sample complexity bounds when set A is the unit
sphere Sp−1 even for design matrices having only bounded fourth moments. Note that with such
weak moment assumptions  a non-trivial non-asymptotic upper bound cannot be established. Our
analysis for the RE condition essentially follow this premise and arguments from [10].

4.2.1 A Bound Based on Exponential Width

We obtain a sample complexity bound which depends on the exponential width of the error set A.
The result we state below follows along similar arguments made in [20]  which in turn are based on
arguments from [10  14].
Theorem 3 Let X ∈ Rn×p have independent isotropic sub-exponential rows. Let A ⊆ Sp−1 
0 < ξ < 1  and c is a constant that depends on the sub-exponential norm K = supu∈A (cid:107)|(cid:104)X  u(cid:105)|(cid:107)ψ1.
Let we(A) denote the exponential width of the set. Then for some τ > 0 with probability atleast
(1 − exp(−τ 2/2)) 

(cid:107)Xu(cid:107)2 ≥ cξ(1 − ξ2)2√

n − 4we(A) − ξτ .

(17)

inf
u∈A

Contrasting the result (17) with previous results for the sub-Gaussian case [2  20] the dependence
on wg(A) on the r.h.s is replaced by we(A)  thus leading to a log p worse sample complexity bound.
The corollary below applies the result for the (cid:96)1 norm. Note that results from [1] for (cid:96)1 norm show
RIP bounds w.h.p for the same number of samples.
Corollary 3 For an s-sparse θ∗ and (cid:96)1 norm regularization  if n ≥ c · s log2 p then with probability
atleast (1 − exp(−τ 2/2)) and constants c  κ depending on ξ and τ 

(cid:107)Xu(cid:107)2 ≥ κ .

inf
u∈A

(18)

4.2.2 A Bound Based on VC-Dimensions

In this section  we show a stronger sub-Gaussian RE sample complexity result for sub-exponential
X and (cid:96)1  group-sparse regularization. The arguments follow along similar lines to [11  10].
Theorem 4 Let X ∈ Rn×p be a random matrix with isotropic random sub-exponential rows Xi ∈
Rp. Let A ⊆ Sp−1  0 < ξ < 1  c is a constant that depends on the sub-exponential norm K =
supu∈A (cid:107)|(cid:104)X  u(cid:105)|(cid:107)ψ1 and deﬁne β = c(1 − ξ2)2. Let we(A) denote the exponential width of the set

6

A. Let Cξ = {I[|(cid:104)Xi  u(cid:105)| > ξ]  u ∈ A} be a VC-class with VC-dimension V C(Cξ) ≤ d. For some
suitable constant c1  if n ≥ c1(d/β2)  then with probability atleast 1 − exp(−η0β2n):

1√
n

inf
u∈A

(cid:107)Xu(cid:107)2 ≥ cξ(1 − ξ2)2

2

.

(19)

s1

Consider the case of (cid:96)1 norm. A consequence of the above result is that the RE condition is satisﬁed
on the set B = {u|(cid:107)u(cid:107)0 = s1} ∩ Sp−1 for some s1 ≥ c · s where c is a constant that will depend
on the RE constant κ when n is O(s1 log p). The argument follows from the fact that B ∩ Sp−1 is a

(cid:1) spheres. Thus the result is obtained by applying Theorem 4 to each sphere and using

union of(cid:0) p

a union bound argument. The ﬁnal step involves showing that the RE condition is satisﬁed on the
error set A if it is satisﬁed on B using Maurey’s empirical approximation argument [17  18  11].
Corollary 4 For set A ⊆ Sp−1  which is the error set for the (cid:96)1 norm  if n ≥ c2s log(ep/s)/β2
for some suitable constant c2  then with probability atleast 1 − exp(−η0nβ2) −
wη1 pη1−1   where
1
η0  η1  w > 1 are constants  the following result holds for κ depending on the constant ξ:

1√
n

inf
u∈A

(cid:107)Xu(cid:107)2 ≥ κ .

(20)

Essentially the same arguments for the group-sparse norm lead to the following result:
Corollary 5 For set A ⊆ Sp−1  which is the error set for the group-sparse norm  if n ≥ (c(msG +
sG log(eNG/sG)))/β2  then with probability atleast 1 − exp(−η0nβ2) −
G mη1−1 where
η0  η1  w > 1 are constants and κ depending on constant ξ 
(cid:107)Xu(cid:107)2 ≥ κ .

1
wη1 N η1−1

(21)

1√
n

inf
u∈A

4.3 Recovery Bounds for (cid:96)1 and Group-Sparse Norms

We combine result (13) with results obtained for λn and κ previously for (cid:96)1 and group-sparse norms.

Corollary 6 For the (cid:96)1 norm  when n ≥ cs log p for some constant c  with high probability:

(22)
Corollary 7 For the group-sparse norm  when n ≥ c(msG + sG log(NG))  for some constant c 
with high probability:

s log p/

n) .

√
(cid:107) ˆ∆n(cid:107)2 ≤ O(

√

(cid:107) ˆ∆n(cid:107)2 ≤ O

sG log p(m + log NG)

n

.

(23)

(cid:32)(cid:114)

(cid:33)

√

Both bounds are
log p worse compared to corresponding bounds for the sub-Gaussian case. In
terms of sample complexity  n should scale as O(s log2 p)  instead of O(s log p) for sub-Gaussian 
for (cid:96)1 norm and O(sG log p(m + log NG))  instead of O(sG(m + log NG)) for the sub-Gaussian
case  for group-sparse lasso to get upto a constant order error bound.

5 Experiments

We perform experiments on synthetic data to compare estimation errors for Gaussian and sub-
exponential design matrices and noise for both (cid:96)1 and group sparse norms. For (cid:96)1 we run exper-
iments with dimensionality p = 300 and sparsity level s = 10. For group sparse norms we run
experiments with dimensionality p = 300  max. group size m = 6  number of groups NG = 50
groups each of size 6 and 4 non-zero groups. For the design matrix X  for the Gaussian case we
sample rows randomly from an isotropic Gaussian distribution  while for sub-exponential design

7

Figure 1: Probability of recovery
in noiseless case with increasing
sample size. There is a sharp
phase transition and the curves
overlap for Gaussian and sub-
exponential designs.

Figure 2: Estimation error (cid:107) ˆ∆n(cid:107)2 vs sample size for (cid:96)1 (left) and group-sparse norms (right). The curve for
sub-exponential designs and noise decays slower than Gaussians.

matrices we sample each row of X randomly from an isotropic extreme-value distribution. The
number of samples n in X is incremented in steps of 10 with an initial starting value of 5. For the
noise ω  it is sampled i.i.d from the Gaussian and extreme-value distributions with variance 1 for
the Gaussian and sub-exponential cases respectively. For each sample size n  we repeat the proce-
dure above 100 times and all results reported in the plots are average values over the 100 runs. We
report two sets of results. Figure 1 shows percentage of success vs sample size for the noiseless
case when y = Xθ∗. A success in the noiseless case denotes exact recovery which is possible when
the RE condition is satisﬁed. Hence we expect the sample complexity for recovery to be order of
square of Gaussian width for Gaussian and extreme-value distributions as validated by the plots in
Figure 1. Figure 2 shows average estimation error vs number of samples for the noisy case when
y = Xθ∗ +ω. The noise is added only for runs in which exact recovery was possible in the noiseless
case. For example when n = 5 we do not have any results in Figure 2 as even noiseless recovery is
not possible. For each n  the estimation errors are average values over 100 runs. As seen in Figure
2  the error decay is slower for extreme-value distributions compared to the Gaussian case.

6 Conclusions

This paper presents a uniﬁed framework for analysis of non-asymptotic error and structured recovery
in norm regularized regression problems when the design matrix and noise are sub-exponential 
essentially generalizing the corresponding analysis and results for the sub-Gaussian case. The main
observation is that the dependence on Gaussian width is replaced by the exponential width of suitable
sets associated with the norm. Together with the result on the relationship between exponential and
Gaussian widths  previous analysis techniques essentially carry over to the sub-exponential case. We
also show that a stronger result exists for the RE condition for the Lasso and group-lasso problems.
As future work we will consider extending the stronger result for the RE condition for all norms.
Acknowledgements: This work was supported by NSF grants IIS-1447566 
IIS-1447574 
IIS-1422557  CCF-1451986  CNS-1314560  IIS-0953274  IIS-1029711  and by NASA grant
NNX12AQ39A.

8

02040608010012014016018020000.20.40.60.81Number of samplesProbability of success Basis pursuit with Gaussian designBasis pursuit with sub−exponential designGroup sparse with Gaussian designGroup sparse with sub−exponential design60801001201401601802000.550.60.650.70.750.80.850.90.951Estimation error Lasso with Gaussian design and noiseLasso with sub−exponential design and noise1201301401501601701800.650.70.750.80.850.9Number of samplesEstimation error Group sparse lasso withGaussian design and noiseGroup sparse lasso withsub−exponential design and noiseReferences

[1] R. Adamczak  A. E. Litvak  A. Pajor  and N. Tomczak-Jaegermann. Restricted isometry prop-
erty of matrices with independent columns and neighborly polytopes by random sampling.
Constructive Approximation  34(1):61–88  2011.

[2] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with Norm Regularization.

In NIPS  2014.

[3] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

Annals of Statistics  37(4):1705–1732  2009.

[4] E. J. Candes  J. Romberg  and T. Tao. Robust Uncertainty Principles : Exact Signal Recon-
struction from Highly Incomplete Frequency Information. IEEE Transactions on Information
Theory  52(2):489–509  2006.

[5] E. J. Candes and T. Tao. Decoding by Linear Programming. IEEE Transactions on Information

Theory  51(12):4203–4215  2005.

[6] E. J. Candes and T. Tao. The Dantzig selector : statistical estimation when p is much larger

than n. Annals of Statistics  35(6):2313–2351  2007.

[7] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The Convex Geometry of Linear

Inverse Problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[8] S. Chatterjee  S. Chen  and A. Banerjee. Generalized Dantzig Selector: Application to the

k-support norm. In NIPS  2014.

[9] D. Hsu and S. Sabato. Heavy-tailed regression with a generalized median-of-means. In ICML 

2014.

[10] V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix

without concentration. arXiv:1312.3580  2013.

[11] G. Lecu´e and S. Mendelson.

arXiv:1401.2188  2014.

Sparse recovery under weak moment assumptions.

[12] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes.

Springer Berlin  1991.

[13] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional

data. Annals of Statistics  37(1):246–270  2009.

[14] S. Mendelson. Learning without concentration. Journal of the ACM  To appear  2015.
[15] S. Mendelson and G. Paouris. On generic chaining and the smallest singular value of random

matrices with heavy tails. Journal of Functional Analysis  262(9):3775–3811  2012.

[16] S. N. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A Uniﬁed Framework for High-
Dimensional Analysis of $M$-Estimators with Decomposable Regularizers. Statistical Sci-
ence  27(4):538–557  2012.

[17] R. I. Oliveira. The lower tail of random quadratic forms  with applications to ordinary least

squares and restricted eigenvalue properties. arXiv:1312.2903  2013.

[18] M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements.

Transaction on Information Theory  59(6):3434–3447  2013.
[19] M. Talagrand. The Generic Chaining. Springer Berlin  2005.
[20] J. A. Tropp. Convex recovery of a structured signal from independent random linear measure-

IEEE

ments. In Sampling Theory - a Renaissance. To appear  2015.

[21] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y Eldar
and G. Kutyniok  editors  Compressed Sensing  pages 210–268. Cambridge University Press 
Cambridge  2012.

[22] M. J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using L1
-constrained quadratic programmming ( Lasso ). IEEE Transaction on Information Theory 
55(5):2183–2201  2009.

[23] P. Zhao and B. Yu. On Model Selection Consistency of Lasso. Journal of Machine Learning

Research  7:2541–2563  2006.

9

,Vidyashankar Sivakumar
Arindam Banerjee
Pradeep Ravikumar