2018,A Bayesian Approach to Generative Adversarial Imitation Learning,Generative adversarial training for imitation learning has shown promising results on high-dimensional and continuous control tasks. This paradigm is based on reducing the imitation learning problem to the density matching problem  where the agent iteratively refines the policy to match the empirical state-action visitation frequency of the expert demonstration. Although this approach has shown to robustly learn to imitate even with scarce demonstration  one must still address the inherent challenge that collecting trajectory samples in each iteration is a costly operation. To address this issue  we first propose a Bayesian formulation of generative adversarial imitation learning (GAIL)  where the imitation policy and the cost function are represented as stochastic neural networks. Then  we show that we can significantly enhance the sample efficiency of GAIL leveraging the predictive density of the cost  on an extensive set of imitation learning tasks with high-dimensional states and actions.,A Bayesian Approach to Generative Adversarial

Imitation Learning

Wonseok Jeon1  Seokin Seo1  Kee-Eung Kim1 2
1 School of Computing  KAIST  Republic of Korea

2 PROWLER.io

{wsjeon  siseo}@ai.kaist.ac.kr  kekim@cs.kaist.ac.kr

Abstract

Generative adversarial training for imitation learning has shown promising results
on high-dimensional and continuous control tasks. This paradigm is based on
reducing the imitation learning problem to the density matching problem  where
the agent iteratively reﬁnes the policy to match the empirical state-action visitation
frequency of the expert demonstration. Although this approach can robustly learn
to imitate even with scarce demonstration  one must still address the inherent
challenge that collecting trajectory samples in each iteration is a costly operation. To
address this issue  we ﬁrst propose a Bayesian formulation of generative adversarial
imitation learning (GAIL)  where the imitation policy and the cost function are
represented as stochastic neural networks. Then  we show that we can signiﬁcantly
enhance the sample efﬁciency of GAIL leveraging the predictive density of the
cost  on an extensive set of imitation learning tasks with high-dimensional states
and actions.

1

Introduction

Imitation learning is the problem where an agent learns to mimic the demonstration provided by the
expert  in an environment with unknown cost function. Imitation learning with policy gradients [Ho
et al.  2016] is a recently proposed approach that uses gradient-based stochastic optimizers. Along
with trust-region policy optimization (TRPO) [Schulman et al.  2015] as the optimizer  it is shown
to be one of the most practical approaches that scales well to large-scale environments  i.e. high-
dimensional state and action spaces. Generative adversarial imitation learning (GAIL) [Ho and Ermon 
2016]  which is of our primary interest  is a recent instance of imitation learning algorithms with
policy gradients. GAIL reformulates the imitation learning problem as a density matching problem 
and makes use of generative adversarial networks (GANs) [Goodfellow et al.  2014]. This is achieved
by generalizing the representation of the underlying cost function using neural networks  instead
of restricting it to the class of linear functions for the sake of simpler optimization. As a result  the
policy being learned becomes the generator  and the cost function becomes the discriminator. Based
on the promising results from GAIL  a number of improvements appeared in the literature [Wang
et al.  2017  Li et al.  2017].
Yet  one of the fundamental challenges lies in the fact that obtaining trajectory samples from the
environment is often very costly  e.g.  physical robots situated in real-world. Among a number of
improved variants of GAIL  we remark that generative moment matching imitation learning (GM-
MIL) [Kim and Park  2018]  which uses kernel mean embedding to improve the discriminator training
just as in generative moment matching networks (GMMNs) [Li et al.  2015]  was experimentally
shown to converge much faster and more stable compared to GAIL. This gives us a hint that a robust
discriminator is an important factor in improving the sample efﬁciency of generative-adversarial
approaches to imitation learning.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this work  we also aim to enhance the sample efﬁciency of the generative-adversarial approach to
imitation learning. Our main idea is to use a Bayesian discriminator in GAIL  e.g. using a Bayesian
neural network  thus referring to our algorithm as Bayes-GAIL (BGAIL). To achieve this  we ﬁrst
reformulate GAIL in the Bayesian framework. As a result  we show that GAIL can be seen as
optimizing a surrogate objective in our approach  with iterative updates being maximum-likelihood
(ML) point estimations. In our work  instead of using the ML point estimate  we propose to use the
predictive density of the cost. This gives more informative cost signals for the policy training and
makes BGAIL signiﬁcantly more sample-efﬁcient compared to the original GAIL.

2 Preliminaries

2.1 Reinforcement Learning (RL) and Notations

We ﬁrst deﬁne the basic notions from RL. The RL problem considers an agent that chooses an action
after observing an environment state and the environment that reacts with a cost and a successor
state to the agent’s action. The agent-environment interaction is modeled by using a Markov decision
process (MDP) M := (cid:104)S  A  c  PT   ν  γ(cid:105); S is a state space  A is an action space  c(s  a) is a cost
function  PT (s(cid:48)|s  a) is the state transition distribution  ν(s) is the initial state distribution  γ ∈ [0  1]
is a discount factor. M− denotes an MDP M without the cost function (MDP\C)  i.e.  (cid:104)S  A  P  ν  γ(cid:105).
The (stochastic) policy π(a|s) is deﬁned as the probability of choosing action a in state s.
Given the cost function c  the objective of RL is to ﬁnd the policy π that minimizes the expected
t=0 γtc(ssst  aaat)]  where the subscript π in the expectation im-
plies that the trajectory (sss0  aaa0  sss1  aaa1  ...) is generated from the policy π with the transition dis-
tribution of M−. The state value function V c
π are deﬁned as
t=0 c(ssst  aaat)|sss0 = s  aaa0 = a]  respec-
V c
tively. The optimal value functions V c∗   Qc∗ for c are the value functions for the optimal policy
πc∗ := arg minπη(π  c) under the cost function c. The γ-discounted state visitation occupancy
t=0 γtδ(s − ssst)] for Dirac delta function δ when
the state space S is assumed to be continuous. For convenience  we denote the γ-discounted
state-action visitation occupancy for π as ρπ(s  a) := ρπ(s)π(a|s). It can be simply shown that
s a ρπ(s  a)c(s  a). Throughout this paper  bold-math letters are

long-term cost η(π  c) := Eπ [(cid:80)∞
π (s) := Eπ [(cid:80)∞
ρπ for policy π is deﬁned as ρπ(s) := Eπ [(cid:80)∞
η(π  c) = E(sss aaa)∼ρπ [c(sss  aaa)] :=(cid:80)

π(s  a) := Eπ [(cid:80)∞

t=0 c(ssst  aaat)|sss0 = s] and Qc

π and the action value function Qc

used to indicate random variables  and their realizations are written as non-bold letters.

2.2

Imitation Learning

Historically  behavioral cloning (BC) [Pomerleau  1991] is one of the simplest approach to imitation
learning  which learns to map the states to demonstrated actions using supervised learning. However 
BC is susceptible to compounding error  which refers to small prediction error accumulated over time
to a catastrophic level [Bagnell  2015]. Inverse reinforcement learning (IRL) [Russell  1998  Ng and
Russell  2000  Ziebart et al.  2008] is a more modern approach  where the objective is to learn the
underlying unknown cost function that makes the expert optimal. Although this is a more principled
approach to imitation learning  IRL algorithms usually involve planning as an inner loop  which
usually requires the knowledge of transition distribution and mainly increases the computational
complexity of IRL. In addition  IRL is fundamentally an ill-posed problem  i.e.  there exist inﬁnitely
many cost functions that can describe identical policies  and thus requires some form of preferences
on the choice of cost functions [Ng and Russell  2000]. The Bayesian approach to IRL [Ramachandran
and Amir  2007  Choi and Kim  2011] is one way of encoding the cost function preferences  which
will be introduced in the following section.
Finally  imitation learning with policy gradients [Ho et al.  2016] is one of the most recent approaches 
which replaces the costly planning inner loop with the policy gradient update in RL  making the
algorithm practical and scalable. Generative adversarial imitation learning (GAIL) [Ho and Ermon 
2016] is an instance of this approach  based on the adversarial training objective

(cid:111)
[log D(sss  aaa)] + E(sss aaa)∼ρπ [log(1 − D(sss  aaa))]

arg min

D∈(0 1)S×A

(1)
for a set (0  1)S×A of functions D : S × A → (0  1). This is essentially the training objective of
GAN  where the generator is the policy π  and the discriminator D is the intermediate cost function
to be used in policy gradient update to match ρπ to ρπE .

(cid:110)E(sss aaa)∼ρπE

max

π

 

2

Figure 1: Graphical model for GAIL. The state-action pairs are denoted by zzz := (sss  aaa). Note that
p(z1) = ν(s1)πθ(a1|s1) and p(zt+1|zt) = πθ(at+1|st+1)PT (st+1|st  at). Also  the discriminator
parameter φφφ and the policy parameter θθθ are regarded as random variables.

2.3 Bayesian Inverse Reinforcement Learning (BIRL)

  a(n)

t

t

(cid:81)T (n)

tion Qc∗  i.e. p(D|c) := (cid:81)N

The Bayesian framework for IRL was proposed by Ramachandran and Amir [2007]  where the
cost function c is regarded as a random function. For the expert demonstration set D := {τn =
t=1 |n = 1  ...  N} collected under M−  the cost function preference and the optimality
(s(n)
)T (n)
conﬁdence on the expert’s trajectories D are encoded as prior p(c) and likelihood p(D|c)  respectively.
As for the likelihood  the samples in D are assumed independent Gibbs distribution with potential func-
)/β)
with the temperature parameter β. Under this model  reward inference and imitation learning using
the posterior mean reward were suggested. Choi and Kim [2011] suggested a BIRL approach using
maximum a posterior (MAP) inference. Based on the reward optimality region [Ng and Russell 
2000]  the authors found that there are cases where the posterior mean reward exists outside the
optimality region  whereas MAP reward is posed inside the region. In addition  it was shown that
the existing works on IRL [Ng and Russell  2000  Ratliff et al.  2006  Syed et al.  2008  Neu and
Szepesvári  2007  Ziebart et al.  2008] can be viewed as special cases of MAP inference if we choose
the likelihood and a prior properly.

  c) ∝ exp(Qc∗(s(n)

  c) for p(a(n)

t=1 p(a(n)

t

|s(n)

t

|s(n)

t

  a(n)

t

n=1

t

t

3 Bayesian Generative Adversarial Imitation Learning

In order to formally present our approach  let us denote the agent’s policy as πA and the expert’s
policy as πE. In addition  let us denote sets DA and DE of trajectories generated by πA and πE 
respectively  under M− for

DA :=

τ (n)
A = (s(n)

A t  a(n)

A t)T

t=1

 

(2)

(cid:26)

(cid:12)(cid:12)(cid:12)(cid:12)n = 1  ...  NA

(cid:27)

where the quantities for expert are deﬁned in a similar way. In the remainder of this work  we drop
the subscripts A and E if there is no confusion. Also  note that DE will be given as input to the
imitation learning algorithm  whereas DA will be generated in each iteration of optimization. It is
natural to assume that the agent’s and the expert’s trajectories τA and τE are independently generated 
t=2 PT (st|st−1  at−1)π(at|st). In this

i.e.  p(τA  τE) = p(τA)p(τE)  with p(τ ) := ν(s1)π(a1|s1)(cid:81)T

work  we reformulate GAIL [Ho and Ermon  2016] in the Bayesian framework as follows.

3.1 Bayesian Framework for Adversarial Imitation Learning

Agent-expert discrimination Suppose πA is ﬁxed for simplicity  which will be later parameterized
for learning. Let us consider binary auxiliary random variables oooA t  oooE t for all t  where ooot becomes
1 if given state-action pair (ssst  aaat) is generated by the expert  and becomes 0 otherwise. Then  the

3

joint distribution of (τττ A  τττ E  oooA  oooE) can be written as

p(τA  τE  oA  oE) = p(τA)p(τE)

p(oA t|sA t  aE t)

(cid:35)

p(oE t|sA t  aE t)

(cid:35)(cid:34) T(cid:89)

t=1

.

(3)

(cid:34) T(cid:89)

t=1

T(cid:89)

t=1

for ooo := (ooot)T
t=1 := (ooo1  ...  oooT )  where ot is a realization of a random variable ooot. Although
p(ot|st  at) cannot be the same for both agent and expert and all t  we can simplify the problem by
applying a single approximate discriminator Dφ(s  a) with parameter φ such that
p(ot|st  at) ≈ ˆp(ot|st  at; φ) := (1 − Dφ(st  at))otDφ(st  at)1−ot =

(cid:26)1 − Dφ(st  at)  if ot = 1 

Dφ(st  at) 

otherwise.
(4)

Using the approximation in (4)  the distribution in (3) is given by

p(τA  τE  oA  oE) ≈ ˆp(τA  τE  oA  oE; φ)

:= p(τA)p(τE)

ˆp(oA t|sA t  aA t; φ)

T(cid:89)

t=1

ˆp(oE t|sE t  aE t; φ).

(5)

(6)

It should be noted that the distribution in (6) works for the arbitrary choice of τA  τE  oA  oE. Also 
the graphical model for those random variables is shown in Figure 1 to clarify the dependencies
between random variables.
Now  suppose a discrimination optimality event oooA = 0   oooE = 1 is observed for some ﬁxed
trajectories τA  τE  where 1 := (1)T
t=1 := (1  ...  1) and 0 is deﬁned in a similar way. Intuitively  the
discrimination optimality event is an event such that the discriminator perfectly recognizes the policy
that generates given state-action pairs. By introducing a prior p(φ) on the discriminator parameter φ
and the agent policy πA(·|·; θ) parameterized with θ  we obtain the following posterior distribution
conditioned on the discrimination optimality event and θ:

p(φ  τA  τE|0A  1E; θ) ∝ p(φ)p(τA; θ)p(τE)p(0A|τA; φ)p(1E|τE; φ).

(7)
Here  0A and 1E is deﬁned as the events oooA = 0 and oooE = 1   respectively. By using the posterior
p(φ|0A  1E; θ) which marginalizes out τA and τE in (7)  we can consider the full distribution of φ or
select an appropriate point estimate for φ that maximizes the posterior.

Discrimination-based imitation Suppose we want to ﬁnd the parameter θ of πA that well approx-
imates πE based on the discrimination results. By considering parameters (θ  φ) as random variables
(θθθ  φφφ)  the distribution for (τττ A  τττ E  oooA  oooE  θθθ  φφφ) is

(8)

(9)

p(τA  τE  oA  oE  θ  φ) = p(θ)p(φ)p(τA  τE  oA  oE; θ  φ)

T(cid:89)

T(cid:89)

= p(θ)p(φ)p(τA; θ)p(τE)

ˆp(oA t|sA t  aA t; φ)

ˆp(oE t|sE t  aE t; φ) 

t=1

t=1

where φφφ is assumed to be independent with θθθ. Similar to the optimism for the agent-expert discrimi-
nation  suppose we observe the imitation optimality event oooA (cid:54)= 0 that is irrespective of oooE. Note
that the imitation optimality event implies preventing the occurrence of discrimination optimality
events. To get the optimal policy parameter by using the discriminator  we can consider the following
(conditional) posterior:

p(θ  τA|˜0A; φ) ∝ p(θ)p(τA; θ)p(˜0A|τA; φ).

(10)
Here  ˜0A is deﬁned as an probabilistic event oooA (cid:54)= 0 . Finally by using p(θ|˜0A; φ) that comes from
the marginalization of τA in (10)  either the full distribution of θ or corresponding point estimate can
be used.

3.2 GAIL as an Iterative Point Estimator

Under our Bayesian framework  GAIL can be regarded as an algorithm that iteratively uses (7) and
(10) for updating θ and φ using their point estimates. For the discriminator update  the objective of

4

GAIL is to maximize the expected log-likelihood with θprev given from the previous iteration and τττ A
generated by using πA(a|s; θprev):

Eτττ A τττ E|θθθ=θprev [log p(0A|τττ A  φ)p(1E|τττ E  φ)]

arg max

φ

= arg max

φ

Eτττ A τττ E|θθθ=θprev

log Dφ(sssA t  aaaA t) +

(cid:34) T(cid:88)

t=1

(cid:35)

(11)

.

(12)

log(1 − Dφ(sssE t  aaaE t))

T(cid:88)

t=1

This can be regarded as a surrogate objective with an uninformative prior p(φ) since

log p(0A  1E|φ  θprev) = log Eτττ A τττ E|θθθ=θprev [p(0A|τττ A  φ)p(1E|τττ E  φ)] + constant
≥ Eτττ A τττ E|θθθ=θprev [log p(0A|τττ A  φ)p(1E|τττ E  φ)] + constant 

(13)
(14)

where the inequality in (14) follows from the Jensen’s inequality. For the policy update  the objective
of GAIL is

(cid:2)log p(˜0A|τττ A  φprev)(cid:3) = arg min

Eτττ A|θθθ=θ

θ

arg max

θ

Eτττ A|θθθ=θ

log Dφprev (sssA t  aaaA t)

.

(15)

(cid:35)

(cid:34) T(cid:88)

t=1

Similarly  for the uninformative prior p(θ)  we can show that

log p(˜0A|θ  φprev) = log Eτττ A|θθθ=θ

≥ Eτττ A|θθθ=θ

(cid:2)p(˜0A|τττ A  φprev)(cid:3) + constant
(cid:2)log p(˜0A|τττ A  φprev)(cid:3) + constant 

(16)
(17)

and thus  the objective in (15) can be regarded as a surrogate objective. In addition  since the form
of the objective in (15) is the same as the policy optimization with an immediate cost function
log Dφprev (· ·)  GAIL uses TRPO  a state-of-the-art policy gradient algorithm  for updating θ.
Note that our approach shares the same insight behind the probabilistic inference formulation of
reinforcement learning  in which the reinforcement learning problem is casted into the probabilistic
inference problem by introducing the auxiliary return optimality event [Toussaint  2009  Neumann 
2011  Abdolmaleki et al.  2018]. Also  if we consider the maximization of log p(1A|θ  φprev)  which
result from deﬁning the imitation optimality event as oooA = 1   it can be shown that the corresponding
surrogate objective becomes the policy optimization with an immediate reward function log(1 −
Dφprev (· ·)). This is in line with speeding up GAN training by either maximizing log(1 − D(·)) or
minimizing log D(·)  suggested in Goodfellow et al. [2014]. Some recent work on adversarial inverse
reinforcement learning also support the use of such reward function [Finn et al.  2016  Fu et al. 
2018].

3.3 Sample-efﬁcient Imitation Learning with Predictive Cost Function

Since model-free imitation learning algorithms (e.g. GAIL) require experience samples obtained
from the environment  improving the sample-efﬁciency is critical. From the Bayesian formulation in
the previous section  GAIL can be seen as maximizing (minimizing) the expected log-likelihood in a
point-wise manner for discriminator (policy) updates  and this makes the algorithm quite inefﬁcient
compared to using the full predictive distribution.
We thus propose to use the posterior of the discriminator parameter so that more robust cost signals
are available for policy training. Formally  let us consider the iterative updates for the policy parameter
θ and the discriminator parameter φ  where the point estimate of θ is obtained using the distribution
over φ in each iteration. In other words  given θprev from the previous iteration  we want to utilize
pposterior(φ) := p(φ|0A  1E  θprev) that satisﬁes

log pposterior(φ) = log(cid:8)p(φ)Eτττ A|θθθ=θprev [p(0A|τττ A  φ)] Eτττ E [p(1E|τττ E  φ)](cid:9) + constant.

(18)

By using Monte-Carlo estimations for the expectations over trajectories in (18)  the log posterior in
(18) can be approximated as

log p(φ) + log

exp(F (n)

A φ) + log

exp(F (n)

E φ) + constant 

(19)

N(cid:88)

N(cid:88)

n=1

n=1

5

(cid:40)
K(cid:88)
(cid:32)
(cid:40) T(cid:88)

1
K

k=1

T(cid:88)
K(cid:88)

t=1

1
K

(cid:41)
(cid:33)(cid:41)

A φ :=(cid:80)T

E φ :=(cid:80)T

t=1 log(1 − Dφ(s(n)

A t  a(n)

E t  a(n)

A t) and F (n)

t=1 log Dφ(s(n)

where F (n)
E t)). Note that we
can also use the surrogate objective of GAIL in (14) with prior on p(φ)  which might be suitable for
the inﬁnite-horizon problems.
At each iteration of our algorithm  we try to ﬁnd policy parameter θ that maximizes the log of the
posterior log p(θ|˜0A). For an uninformative prior on θ  the objective can be written as
arg maxθ log p(θ|˜0A) = arg minθ log p(0A|θ) = arg minθ log Eτττ A|θθθ=θ φφφ∼pposterior[p(0A|τττ A  φφφ)].
(20)
By applying the Jensen’s inequality to (20)  we have Eτττ A|θθθ=θ φφφ∼pposterior[log p(0A|τττ A  φφφ)]  which
can be minimized by policy optimization. In contrast to GAIL that uses the single point estimate
for the maximization of pposterior  multiple parameters φ1  ...  φK that are randomly sampled from
pposterior are used to estimate the objective:

Eτττ A|θθθ=θ

log p(0A|τττ A  φk)

= Eτττ A|θθθ=θ

log Dφk (sssA t  aaaA t)

(21)

(cid:40)

K(cid:88)

k=1

1
K

(cid:41)

= Eτττ A|θθθ=θ

log Dφk (sssA t  aaaA t)

.

(22)

(cid:80)K
(cid:80)K
Note that (22) implies we can perform RL policy optimization with the predictive cost function
k=1 log Dφk (s  a). In addition  if we consider p(1A|τττ A  φk) rather than p(˜0A|τττ A  φk)  the
1
K
k=1(1 − log Dφk (s  a)).
optimization problem becomes RL with the predictive reward function 1
K
The remaining question is how to get the samples from the posterior  and this will be discussed in the
next section.

k=1

t=1

4 Posterior Sampling Based on Stein Variational Gradient Descent (SVGD)

SVGD [Liu and Wang  2016] is a recently proposed Bayesian inference algorithm based on the
particle updates  which we brieﬂy review as follows: suppose that random variable xxx follows the
distribution q(0)  and target distribution p is known up to the normalization constant. Also  consider a
sequence of transformations T (0)  T (1)  ...  where

T (i)(x) := x + (i)ψq(i) p(x)  ψq p(x(cid:48)) := Exxx∼q[k(xxx  x(cid:48))∇xxx log p(xxx) + ∇xxxk(xxx  x(cid:48))]

(23)
with sufﬁciently small step size (i)  probability distribution q(i) of (T (i−1) ◦ ··· T (0))(xxx) and some
positive deﬁnite kernel k(· ·). Interestingly  the deterministic transformation (23) turns out to be an
iterative update to the probability distribution towards the target distribution p  and ψq(i) p can be
interpreted as the functional gradient in the reproducing kernel Hilbert space (RKHS) deﬁned by the
kernel k(· ·). SVGD was shown to minimize the kernelized Stein discrepancy S(q(i)  p) between
q(i) and p [Liu et al.  2016] in each iteration. In practice  SVGD uses a ﬁnite number of particles.
More formally  for K particles {x(0)
k=1 that are initially sampled  SVGD iteratively updates those
particles by the following transformation that approximates (23):
k   x)∇x(i)

T (i)(x) := x + (i) ˆψ(i)

k ) + ∇x(i)

K(cid:88)

log p(x(i)

k }K

p (x) :=

p (x) 

k(x(i)

k(x(i)

k   x)

(cid:16)

(cid:17)

ˆψ(i)

.

k

k

1
K

k=1

(24)
Even with the approximate deterministic transform and a few particles  SVGD was experimentally
shown to signiﬁcantly outperform common Bayesian inference algorithms. In the extreme case  if a
single particle is used  SVGD is equivalent to MAP inference.
In our work  we use SVGD to draw the samples of the discriminator parameters from the posterior (19).
Speciﬁcally  we ﬁrst choose a set of K initial particles (discriminator parameters) {φ(0)
k=1. Then 
we use the gradient of (19) for those particles and apply the update rule in (24) to get the particles
generated from the posterior distribution in (19). Finally  by using those particles  the predictive
cost function is derived. The complete BGAIL algorithm leveraging SVGD and the predictive cost
function is summarized in Algorithm 1.

k }K

6

{φk}K

k=1  p(φ) for preference of φ

Algorithm 1 Bayesian Generative Adversarial Imitation Learning (BGAIL)
1: Input: Expert trajectories DE  initial policy parameter θ  a set of initial discriminator parameters
2: for each iteration do
3:
4:
5:
6:
7:
8:
9:
10:

Sample trajectories by using policy πθ.
Update θ using policy optimization  e.g.  TRPO  with cost function 1
K
Sample trajectories from DE.
for k = 1  ...  K do

Update φk ← φk + α ˆψ(φk) for a step size parameter α  where

Calculate gradient δk of either (19) or its surrogate objective (17) for φk.

end for
for k = 1  ...  K do

k=1 log Dφ(i)

k

(s  a).

(cid:80)K

(cid:0)k(φj  φ)δj + ∇φj k(φj  φ)(cid:1) .

K(cid:88)

j=1

(cid:46) SVGD

ˆψ(φ) :=

1
K

end for

11:
12: end for

5 Experiments

We evaluated our BGAIL on ﬁve continuous control tasks (Hopper-v1  Walker2d-v1  HalfCheetah-
v1  Ant-v1  Humanoid-v1) from OpenAI Gym  implemented with the MuJoCo physics simula-
tor [Todorov et al.  2012]. We summarize our experimental setting as follows. For all tasks  neural
networks with 2 hidden layers were used for all policy and discriminator networks  where 100 hidden
units for each hidden layer and tanh activations are used. Before training  expert’s trajectories were
collected from the expert policy released by the authors of the original GAIL1  but our code was
built on the GAIL implementation in OpenAI Baselines [Dhariwal et al.  2017] which uses Tensor-
Flow [Abadi et al.  2016]. For the policy  Gaussian policy was used with both mean and variance
dependent on the observation. For the discriminator  the number of particles K was chosen to be 5.
All discriminator parameters φ1  ...  φK were initialized independently and randomly. For training  we
used uninformative prior and SVGD along with the Adam optimizer [Kingma and Ba  2014]  whereas
Adagrad was used in the SVGD paper [Liu and Wang  2016]. Our SVGD was implemented using
the code released by the authors2  with the radial basis function (RBF) kernel (squared-exponential
kernel) k(· ·) and the median heuristic for choosing the bandwidth parameter. In addition  5 inner
loops were used for updating discriminator parameters  which corresponds to the inner loop from
line 6 to line 11 in Algorithm 1.
First  we compare BGAIL to two different settings for GAIL. The ﬁrst setting is the same as in
the authors’ code  where the variance of the Gaussian policy is learnable constant parameter and
a single discriminator update is performed in each iteration. Also  the state-action pairs of the
expert demonstration were subsampled from complete trajectories. In the second setting  we made
changes to the original setting to improve sample efﬁciency by (1) state-dependent variance and
(2) 5 disciminator updates per iteration  and (3) use the whole trajectories without sub-sampling.
In the remainder of this paper  these two settings shall be referred to as vanilla GAIL and tuned
GAIL  respectively. In all settings of our experiments  the maximum number of expert trajectories
was chosen as in Ho and Ermon [2016]  i.e. 240 for Humanoid and 25 for all other tasks  and 50000
state-action pairs were used for each iteration in the ﬁrst experiment. The number of training iterations
were also also chosen as the same as written in GAIL paper. The imitation performances of vanilla
GAIL  tuned GAIL and our algorithm are summarized in Table 1. Note that the evaluation in Table 1
was done in the exactly same setting as the original GAIL paper. In that paper  the imitation learner
was evaluated over 50 independent trajectories using the single trained policy  and the mean and the
standard deviation of those 50 trajectories were given. Similarly  we evaluated each of the 5 trained
policies over 50 independent trajectories  and we reported the mean and the standard deviation over 50
trajectories of the 3rd best policy in terms of the mean score for fair comparison. As we can see  tuned

1https://github.com/openai/imitation
2https://github.com/DartML/Stein-Variational-Gradient-Descent

7

Figure 2: Comparison with GAIL when either 1000 (Hopper-v1  Walker-v1  HalfCheetah-v1) or 5000
(Ant-v1  Humanoid-v1) state-action pairs are used for each training iteration. The numbers inside the
bracket on the titles indicate (from left to right) the state dimension and the action dimension of the
task  respectively. The tasks are ordered by following the ascending order of the state dimension.

Task
Hopper-v1
Walker2d-v1
HalfCheetah-v1
Ant-v1
Humanoid-v1

Dataset size

25
25
25
25
240

GAIL (released)
3560.85 ± 3.09
6832.01 ± 254.64
4840.07 ± 95.36
4132.90 ± 878.67
10361.94 ± 61.28

GAIL (tuned)
3595.30 ± 5.89
7011.02 ± 25.18
5022.93 ± 81.46
4759.12 ± 416.15
10329.66 ± 59.37

BGAIL

3613.94 ± 10.25
7017.46 ± 33.32
4970.77 ± 363.48
4808.90 ± 78.10
10388.34 ± 99.03

Table 1: Imitation Performances for vanilla GAIL  tuned GAIL and BGAIL

GAIL and BGAIL perform slightly better than vanilla GAIL for most of the tasks and hugely better
at Ant-v1. We think this is due to (1) the expressive power by using the policy with state-dependent
variance  (2) the stabilization of the algorithm due to the multiple iteration for discriminator training
and (3) the efﬁcient use of expert’s trajectories without sub-sampling procedure.
Second  we checked the sample efﬁciency of our algorithm by reducing the number of state-action
pairs used for each training iteration from 50000 to 1000 for Hopper-v1  Walker2d-v1  HalfCheetah-
v1 and to 5000 for other much high-dimensional tasks. Note that the vanilla GAIL in this experiment
used 50000 state-action pairs to see the sample efﬁciency of the original work  whereas the tuned
GAIL was trained with either 1000 or 5000 state-action pairs per each iteration to compare its sample
efﬁciency with our algorithm. Compared to vanilla GAIL  the performances of both tuned GAIL and
BGAIL converge to the optimal (expert’s performance) much faster as depicted in Figure 2. Note
that 5 different policies were trained for both BGAIL and tuned GAIL  whereas a single policy was
trained for vanilla GAIL. The shades in Figure 2 indicate the standard deviation of scores over these
5 policies. Also  it can be shown that the performances of tuned GAIL and BGAIL are almost similar
at Hopper-v1 that is relatively a low-dimensional task. On the other hand  as the dimension of tasks
increases  BGAIL becomes much more sample-efﬁcient compared to tuned GAIL.

8

0500100015002000iter01000200030004000averaged scoreHopper-v1 (11  3)BGAILGAIL (tuned)GAIL0500100015002000iter010002000300040005000600070008000averaged scoreWalker2d-v1 (17  6)BGAILGAIL (tuned)GAIL01000200030004000iter20001000010002000300040005000averaged scoreHalfCheetah-v1 (17  6)BGAILGAIL (tuned)GAIL0500100015002000iter20001000010002000300040005000averaged scoreAnt-v1 (111  8)BGAILGAIL (tuned)GAIL050010001500200025003000iter0200040006000800010000averaged scoreHumanoid-v1 (376  17)BGAILGAIL (tuned)GAIL6 Discussion

In this work  GAIL is analyzed in the Bayesian approach  and such approach can lead to highly sample-
efﬁcient model-free imitation learning. Our Bayesian approach is related to Bayesian GAN [Saatci
and Wilson  2017] that considered the posterior distributions of both generator and discriminator
parameters in the generative adversarial training. Similarly in our work  the posterior for the agent-
expert discriminator was used for the predictive density of the cost during training  whereas only
a point estimate for the policy parameter was used for simplicity. We think our algorithm can be
simply extended to the multi-policy imitation learning  and the sample efﬁciency of our algorithm
may be enhanced by utilizing the posterior of the policy parameter as shown in Stein variational
policy gradient (SVPG) [Liu et al.  2017]. Also for the theoretical analysis  ours slightly differs from
the analysis in Bayesian GAN due to the inter-trajectory correlation from MDP formulation in our
work. This makes the objective of original GAIL regarded as the surrogate objective in our Bayesian
approach  whereas the objective of Bayesian GAN is exactly reduced into that of original GAN for ML
point estimation. In addition  we think our analysis ﬁlls the gap between theory and experiments in
GAIL since GAIL was theoretically analyzed based on the discounted occupancy measure  which can
be deﬁned in the inﬁnite-horizon setting  but their experiments were only done on the ﬁnite-horizon
tasks in MuJoCo simulator. Finally  while BGAIL effectively works with uninformative prior in our
experiments  the proper choice of the prior such as Gaussian prior with Fisher information covariance
in [Abdolmaleki et al.  2018]. may also enhance the sample efﬁciency.

Acknowledgement

This work was supported by the ICT R&D program of MSIT/IITP. (No. 2017-0-01778  Development
of Explainable Human-level Deep Machine Learning Inference Framework) and the Ministry of Trade 
Industry & Energy (MOTIE  Korea) under Industrial Technology Innovation Program (No.10063424 
Development of Distant Speech Recognition and Multi-task Dialog Processing Technologies for
In-door Conversational Robots).

References
Jonathan Ho  Jayesh Gupta  and Stefano Ermon. Model-free imitation learning with policy optimiza-
tion. In Proceedings of the 33rd International Conference on Machine Learning (ICML)  pages
2760–2769  2016.

John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML)  pages 1889–1897  2015.

Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural

Information Processing Systems (NIPS) 29  pages 4565–4573  2016.

Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair 
In Advances in Neural

Aaron Courville  and Yoshua Bengio. Generative adversarial nets.
Information Processing Systems (NIPS) 27  pages 2672–2680  2014.

Ziyu Wang  Josh S Merel  Scott E Reed  Nando de Freitas  Gregory Wayne  and Nicolas Heess.
Robust imitation of diverse behaviors. In Advances in Neural Information Processing Systems
(NIPS) 30  pages 5326–5335  2017.

Yunzhu Li  Jiaming Song  and Stefano Ermon. InfoGAIL: Interpretable imitation learning from
visual demonstrations. In Advances in Neural Information Processing Systems (NIPS) 30  pages
3815–3825  2017.

Kee-Eung Kim and Hyun Soo Park. Imitation learning via kernel mean embedding. In Proceedings

of the 33rd AAAI Conference on Artiﬁcial Intelligence  2018.

Yujia Li  Kevin Swersky  and Rich Zemel. Generative moment matching networks. In Proceedings

of the 32nd International Conference on Machine Learning (ICML)  pages 1718–1727  2015.

9

Dean A Pomerleau. Efﬁcient training of artiﬁcial neural networks for autonomous navigation. Neural

Computation  3(1):88–97  1991.

J Andrew Bagnell. An invitation to imitation. Technical report  Carnegie Mellon University  2015.

Stuart Russell. Learning agents for uncertain environments. In Proceedings of the 11st Annual

Conference on Computational Learning Theory (COLT)  pages 101–103  1998.

Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of

the 17th International Conference on Machine Learning (ICML)  pages 663–670  2000.

Brian D Ziebart  Andrew L Maas  J Andrew Bagnell  and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artiﬁcial Intelligence 
pages 1433–1438  2008.

Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of
the 20th International Joint Conference on Artiﬁcal Intelligence (IJCAI)  pages 2586–2591  2007.

Jaedeug Choi and Kee-Eung Kim. MAP inference for Bayesian inverse reinforcement learning. In

Advances in Neural Information Processing Systems (NIPS) 24  pages 1989–1997  2011.

Nathan D Ratliff  J Andrew Bagnell  and Martin A Zinkevich. Maximum margin planning. In

Proceedings of the 23rd International Conference on Machine Learning  pages 729–736  2006.

Umar Syed  Michael Bowling  and Robert E Schapire. Apprenticeship learning using linear program-
ming. In Proceedings of the 25th International Conference on Machine Learning (ICML)  pages
1032–1039  2008.

Gergely Neu and Csaba Szepesvári. Apprenticeship learning using inverse reinforcement learning and
gradient methods. In Proceedings of the 23rd Conference on Uncertainty in Artiﬁcial Intelligence
(UAI)  pages 295–302  2007.

Marc Toussaint. Robot trajectory optimization using approximate inference. In Proceedings of the

26th International Conference on Machine Learning (ICML)  pages 1049–1056. ACM  2009.

Gerhard Neumann. Variational inference for policy search in changing situations. In Proceedings of

the 28th International Conference on Machine Learning (ICML)  pages 817–824  2011.

Abbas Abdolmaleki  Jost Tobias Springenberg  Yuval Tassa  Remi Munos  Nicolas Heess  and Martin
Riedmiller. Maximum a posteriori policy optimization. In Proceedings of the 6th International
Conference on Learning Representations (ICLR)  2018.

Chelsea Finn  Paul Christiano  Pieter Abbeel  and Sergey Levine. A connection between generative
adversarial networks  inverse reinforcement learning  and energy-based models. arXiv preprint
arXiv:1611.03852  2016.

Justin Fu  Katie Luo  and Sergey Levine. Learning robust rewards with adversarial inverse reinforce-
ment learning. In Proceedings of the 6th International Conference on Learning Representations
(ICLR)  2018.

Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. In Advances in Neural Information Processing Systems (NIPS) 29  pages 2378–2386 
2016.

Qiang Liu  Jason Lee  and Michael Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests.
In Proceedings of the 33rd International Conference on Machine Learning (ICML)  pages 276–284 
2016.

Emanuel Todorov  Tom Erez  and Yuval Tassa. MuJoCo: A physics engine for model-based control.
In Proceedings of the 25th IEEE/RSH International Conference on Intelligent Robots and Systems
(IROS)  pages 5026–5033  2012.

Prafulla Dhariwal  Christopher Hesse  Oleg Klimov  Alex Nichol  Matthias Plappert  Alec Radford 
John Schulman  Szymon Sidor  and Yuhuai Wu. OpenAI Baselines. https://github.com/
openai/baselines  2017.

10

Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro  Greg S
Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  et al. TensorFlow: Large-scale machine
learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467  2016.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

Yunus Saatci and Andrew G Wilson. Bayesian GAN. In Advances in Neural Information Processing

Systems (NIPS) 30  pages 3622–3631  2017.

Yang Liu  Prajit Ramachandran  Qiang Liu  and Jian Peng. Stein variational policy gradient. arXiv

preprint arXiv:1704.02399  2017.

11

,Wonseok Jeon
Seokin Seo
Kee-Eung Kim