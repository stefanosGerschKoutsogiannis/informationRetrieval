2018,Optimal Algorithms for Non-Smooth Distributed Optimization in Networks,In this work  we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the Lipschitz continuity of the global objective function  and (2) the Lipschitz continuity of local individual functions. Under the local regularity assumption  we provide the first optimal first-order decentralized algorithm called multi-step primal-dual (MSPD) and its corresponding optimal convergence rate. A notable aspect of this result is that  for non-smooth functions  while the dominant term of the error is in $O(1/\sqrt{t})$  the structure of the communication network only impacts a second-order term in $O(1/t)$  where $t$ is time. In other words  the error due to limits in communication resources decreases at a fast rate even in the case of non-strongly-convex objective functions. Under the global regularity assumption  we provide a simple yet efficient algorithm called distributed randomized smoothing (DRS) based on a local smoothing of the objective function  and show that DRS is within a $d^{1/4}$ multiplicative factor of the optimal convergence rate  where $d$ is the underlying dimension.,Optimal Algorithms for Non-Smooth
Distributed Optimization in Networks

Kevin Scaman1 Francis Bach2 Sébastien Bubeck3 Yin Tat Lee3 4 Laurent Massoulié2 5

1 Huawei Noah’s Ark Lab  2 INRIA  Ecole Normale Supérieure  PSL Research University 

3 Microsoft Research  4 University of Washington  5 MSR-INRIA Joint Centre

Abstract

In this work  we consider the distributed optimization of non-smooth convex func-
tions using a network of computing units. We investigate this problem under two
regularity assumptions: (1) the Lipschitz continuity of the global objective function 
and (2) the Lipschitz continuity of local individual functions. Under the local regu-
larity assumption  we provide the ﬁrst optimal ﬁrst-order decentralized algorithm
called multi-step primal-dual (MSPD) and its corresponding optimal convergence
rate. A notable aspect of this result is that  for non-smooth functions  while the
dominant term of the error is in O(1/
t)  the structure of the communication
network only impacts a second-order term in O(1/t)  where t is time. In other
words  the error due to limits in communication resources decreases at a fast rate
even in the case of non-strongly-convex objective functions. Under the global reg-
ularity assumption  we provide a simple yet efﬁcient algorithm called distributed
randomized smoothing (DRS) based on a local smoothing of the objective function 
and show that DRS is within a d1/4 multiplicative factor of the optimal convergence
rate  where d is the underlying dimension.

√

1

Introduction

(cid:80)n

Distributed optimization ﬁnds many applications in machine learning  for example when the dataset
is large and training is achieved using a cluster of computing units. As a result  many algorithms were
recently introduced to minimize the average ¯f = 1
i=1 fi of functions fi which are respectively
n
accessible by separate nodes in a network [1  2  3  4]. Most often  these algorithms alternate between
local and incremental improvement steps (such as gradient steps) with communication steps between
nodes in the network  and come with a variety of convergence rates (see for example [5  4  6  7]).
Recently  a theoretical analysis of ﬁrst-order distributed methods provided optimal convergence rates
for strongly-convex and smooth optimization in networks [8]. In this paper  we extend this analysis to
the more challenging case of non-smooth convex optimization. The main contribution of this paper is
to provide optimal convergence rates and their corresponding optimal algorithms for this class of
distributed problems under two regularity assumptions: (1) the Lipschitz continuity of the global
objective function ¯f  and (2) a bound on the average of Lipschitz constants of local functions fi.
Under the local regularity assumption  we provide in Section 4 matching upper and lower bounds
of complexity in a decentralized setting in which communication is performed using the gossip
algorithm [9]. Moreover  we propose the ﬁrst optimal algorithm for non-smooth decentralized
optimization  called multi-step primal-dual (MSPD). Under the more challenging global regularity
assumption  we show in Section 3 that distributing the simple smoothing approach introduced in [10]
yields fast convergence rates with respect to communication. This algorithm  called distributed
randomized smoothing (DRS)  achieves a convergence rate matching the lower bound up to a d1/4
multiplicative factor  where d is the dimensionality of the problem.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Our analysis differs from the smooth and strongly-convex setting in two major aspects: (1) the naïve
master/slave distributed algorithm is in this case not optimal  and (2) the convergence rates differ
between communication and local computations. More speciﬁcally  error due to limits in communica-
tion resources enjoys fast convergence rates  as we establish by formulating the optimization problem
as a composite saddle-point problem with a smooth term for communication and non-smooth term
for the optimization of the local functions (see Section 4 and Eq. (21) for more details).
Related work. Many algorithms were proposed to solve the decentralized optimization of an average
of functions (see for example [1  11  3  4  12  2  13  5])  and a sheer amount of work was devoted to
improving the convergence rate of these algorithms [5  6]. In the case of non-smooth optimization 
fast communication schemes were developed in [14  15]  although precise optimal convergence rates
were not obtained. Our decentralized algorithm is closely related to the recent primal-dual algorithm
of [14] which enjoys fast communication rates in a decentralized and stochastic setting. Unfortunately 
their algorithm lacks gossip acceleration to reach optimality with respect to communication time.
Finally  optimal convergence rates for distributed algorithms were investigated in [8] for smooth and
strongly-convex objective functions  and [16  17] for totally connected networks.

2 Distributed optimization setting
Optimization problem. Let G = (V E) be a strongly connected directed graph of n computing
units and diameter ∆  each having access to a convex function fi over a convex set K ⊂ Rd. We
consider minimizing the average of the local functions

n(cid:88)

i=1

¯f (θ) =

min
θ∈K

1
n

fi(θ)  

(1)

(cid:113) 1

n

(cid:80)n

in a distributed setting. More speciﬁcally  we assume that each computing unit can compute a
subgradient ∇fi(θ) of its own function in one unit of time  and communicate values (i.e. vectors
in Rd) to its neighbors in G. A direct communication along the edge (i  j) ∈ E requires a time τ ≥ 0.
These actions may be performed asynchronously and in parallel  and each machine i possesses a
local version of the parameter  which we refer to as θi ∈ K.
Regularity assumptions. Optimal convergence rates depend on the precise set of assumptions
applied to the objective function. In our case  we will consider two different constraints on the
regularity of the functions:

(A1) Global regularity: the (global) function ¯f is convex and Lg-Lipschitz continuous  in the

sense that  for all θ  θ(cid:48) ∈ K 

| ¯f (θ) − ¯f (θ(cid:48))| ≤ Lg(cid:107)θ − θ(cid:48)(cid:107)2 .

(2)

(A2) Local regularity: Each local function is convex and Li-Lipschitz continuous  and we

denote as L(cid:96) =

i=1 L2

i the (cid:96)2-average of the local Lipschitz constants.

Assumption (A1) is weaker than (A2)  as we always have Lg ≤ L(cid:96). Moreover  we may have Lg = 0
and L(cid:96) arbitrarily large  for example with two linear functions f1(x) = −f2(x) = ax and a → +∞.
We will see in the following sections that the local regularity assumption is easier to analyze and
leads to matching upper and lower bounds. For the global regularity assumption  we only provide an
algorithm with a d1/4 competitive ratio  where d is the dimension of the problem. Finding an optimal
distributed algorithm for global regularity is  to our understanding  a much more challenging task and
is left for future work.
Finally  we assume that the feasible region K is convex and bounded  and denote by R the radius of a
ball containing K  i.e.

(3)
where θ0 ∈ K is the initial value of the algorithm  that we set to θ0 = 0 without loss of generality.
Black-box optimization procedure. The lower complexity bounds in Theorem 2 and Theorem 3
depend on the notion of black-box optimization procedures of [8] that we now recall. A black-box
optimization procedure is a distributed algorithm verifying the following constraints:

∀θ ∈ K  (cid:107)θ − θ0(cid:107)2 ≤ R  

2

1. Local memory: each node i can store past values in a (ﬁnite) internal memory Mi t ⊂ Rd
at time t ≥ 0. These values can be accessed and used at time t by the algorithm run by node
i  and are updated either by local computation or by communication (deﬁned below)  that is 
for all i ∈ {1  ...  n} 

(4)
2. Local computation: each node i can  at time t  compute a subgradient of its local function

∇fi(θ) for a value θ ∈ Mi t−1 in the node’s internal memory before the computation.

i t ∪ Mcomm

(5)
3. Local communication: each node i can  at time t  share a value to all or part of its neighbors 

i t = Span ({θ ∇fi(θ) : θ ∈ Mi t−1}) .

Mcomp

Mi t ⊂ Mcomp

i t

.

that is  for all i ∈ {1  ...  n} 

Mcomm

i t

= Span

Mj t−τ

.

(6)

(cid:18) (cid:91)

(j i)∈E

(cid:19)

4. Output value: each node i must  at time t  specify one vector in its memory as local output

of the algorithm  that is  for all i ∈ {1  ...  n} 

θi t ∈ Mi t .

(7)

(9)

Hence  a black-box procedure will return n output values—one for each computing unit—and our
analysis will focus on ensuring that all local output values are converging to the optimal parameter of
Eq. (1). For simplicity  we assume that all nodes start with the simple internal memory Mi 0 = {0}.
Note that communications and local computations may be performed in parallel and asynchronously.

3 Distributed optimization under global regularity

n(cid:88)

The most standard approach for distributing a ﬁrst-order optimization method consists in computing
a subgradient of the average function

∇ ¯f (θ) =

1
n

∇fi(θ)  

(8)
where ∇fi(θ) is any subgradient of fi at θ  by sending the current parameter θt to all nodes 
performing the computation of all local subgradients in parallel and averaging them on a master node.
Since each iteration requires communicating twice to the whole network (once for θt and once for
sending the local subgradients to the master node  which both take a time ∆τ where ∆ is the diameter
of the network) and one subgradient computation (on each node and performed in parallel)  the time
to reach a precision ε with such a distributed subgradient descent is upper-bounded by

i=1

(cid:18)(cid:16) RLg

(cid:17)2

O

ε

(cid:19)

(∆τ + 1)

.

Note that this convergence rate depends on the global Lipschitz constant Lg  and is thus applicable
under the global regularity assumption. The number of subgradient computations in Eq. (9) (i.e. the
term not proportional to τ) cannot be improved  since it is already optimal for objective functions
deﬁned on only one machine (see for example Theorem 3.13 p. 280 in [18]). However  quite surpris-
ingly  the error due to communication time may beneﬁt from fast convergence rates in O(RLg/ε).
This result is already known under the local regularity assumption (i.e. replacing Lg with L(cid:96) or
even maxi Li) in the case of decentralized optimization [14] or distributed optimization on a totally
connected network [17]. To our knowledge  the case of global regularity has not been investigated by
prior work.

3.1 A simple algorithm with fast communication rates

We now show that the simple smoothing approach introduced in [10] can lead to fast rates for error
due to communication time. Let γ > 0 and f : Rd → R be a real function. We denote as smoothed
version of f the following function:

(10)
where X ∼ N (0  I) is a standard Gaussian random variable. The following lemma shows that f γ is
both smooth and a good approximation of f.

f γ(θ) = E [f (θ + γX)]  

3

Algorithm 1 distributed randomized smoothing

Input: approximation error ε > 0  communication graph G  α0 = 1  αt+1 = 2/(1 +(cid:112)1 + 4/α2

t )

(cid:108) 20RLgd1/4

(cid:109)

ε

T =

  K =

(cid:108) 5RLgd−1/4

(cid:109)

ε

  γt = Rd−1/4αt  ηt =

√

Rαt
2Lg(d1/4+

.

t+1
K )

Output: optimizer θT
1: Compute a spanning tree T on G.
2: Send a random seed s to every node in T .
3: Initialize the random number generator of each node using s.
4: x0 = 0  z0 = 0  G0 = 0
5: for t = 0 to T − 1 do
yt = (1 − αt)xt + αtzt
(cid:80)K
6:
Send yt to every node in T .
7:
k=1 ∇fi(yt + γtXt k)  where Xt k ∼ N (0  I)
Each node i computes gi = 1
8:
K
9: Gt+1 = Gt + 1
nαt
zt+1 = argminx∈K (cid:107)x + ηt+1Gt+1(cid:107)2
10:
xt+1 = (1 − αt)xt + αtzt+1
11:
12: end for
13: return θT = xT

(cid:80)

i gi

2

Lemma 1 (Lemma E.3 of [10]). If γ > 0  then f γ is Lg

f (θ) ≤ f γ(θ) ≤ f (θ) + γLg

d .

(11)

γ -smooth and  for all θ ∈ Rd 

√

(cid:80)n
i=1 ∇f γ

n

Hence  smoothing the objective function allows the use of accelerated optimization algorithms and
provides faster convergence rates. Of course  the price to pay is that each computation of the smoothed
gradient ∇ ¯f γ(θ) = 1
i (θ) now requires  at each iteration m  to sample a sufﬁcient amount
of subgradients ∇fi(θ + γXm k) to approximate Eq. (10)  where Xm k are K i.i.d. Gaussian random
variables. At ﬁrst glance  this algorithm requires all computing units to synchronize on the choice
of Xm k  which would require to send to all nodes each Xm k and thus incur a communication cost
proportional to the number of samples. Fortunately  computing units only need to share one random
seed s ∈ R and then use a random number generator initialized with the provided seed to generate
the same random variables Xm k without the need to communicate any vector. The overall algorithm 
denoted distributed randomized smoothing (DRS)  uses the randomized smoothing optimization
algorithm of [10] adapted to a distributed setting  and is summarized in Alg. 1. The computation of a
spanning tree T in step 1 allows efﬁcient communication to the whole network in time at most ∆τ.
Most of the algorithm (i.e. steps 2  4  6  7  9  10 and 11) are performed on the root of the spanning
subtree T   while the rest of the computing units are responsible for computing the smoothed gradient
(step 8). The seed s of step 2 is used to ensure that every Xm k  although random  is the same on
every node. Finally  step 10 is a simple orthogonal projection of the gradient step on the convex set K.
We now show that the DRS algorithm converges to the optimal parameter under the global regularity
assumption.

Theorem 1. Under global regularity (A1)  Alg. 1 achieves an approximation error E(cid:2) ¯f (θT )(cid:3)− ¯f (θ∗)

of at most ε > 0 in a time Tε upper-bounded by

O

(cid:18) RLg
(cid:24) RLgd1/4

ε

(cid:25)

ε

Tε ≤ 40

.

ε

(cid:17)2(cid:19)
(cid:16) RLg
(cid:25)(cid:24) RLgd−1/4
(cid:24) RLgd1/4
(cid:17)4

ε

ε

(cid:25)

.

(12)

(13)

More speciﬁcally  Alg. 1 completes its T iterations by time

Comparing Eq. (13) to Eq. (9)  we can see that our algorithm improves on the standard method when
the dimension is not too large  and more speciﬁcally

(14)
In practice  this condition is easily met  as ε ≤ 10−2 already leads to the condition d ≤ 108 (assuming
that R and Lg have values around 1). Moreover  for problems of moderate dimension  the term

ε

.

(∆τ + 1)d1/4 +

∆τ + 100

d ≤(cid:16) RLg

4

d1/4 remains a small multiplicative factor (e.g. for d = 1000  d1/4 ≈ 6). Finally  note that DRS
achieves a linear speedup when communication through the whole network requires a constant time 
i.e.  ∆τ = O(1)  and the convexity of each local function fi is not necessary for Theorem 1 to hold.
Remark 1. Several other smoothing methods exist in the literature  notably the Moreau envelope [19]
enjoying a dimension-free approximation guarantee. However  the Moreau envelope of an average
of functions is difﬁcult to compute (requires a different oracle than computing a subgradient)  and
unfortunately leads to convergence rates with respect to local Lipschitz characteristics instead of Lg.

3.2 Optimal convergence rate

The following result provides oracle complexity lower bounds under the global regularity assumption 
and is proved in the supplemental material. This lower bound extends the communication complexity
lower bound for totally connected communication networks from [17].
Theorem 2. Let G be a network of computing units of size n > 0  and Lg  R > 0. There exists n
functions fi : Rd → R such that (A1) holds and  for any t < (d−2)
2 min{∆τ  1} and any black-box
procedure one has  for all i ∈ {1  ...  n} 

(cid:115)

¯f (θi t) − min
θ∈B2(R)

¯f (θ) ≥ RLg
36

1
(1 + t
2∆τ )2

+

1

1 + t

.

(15)

Assuming that the dimension d is large compared to the characteristic values of the problem (a
standard set-up for lower bounds in non-smooth optimization [20  Theorem 3.2.1])  Theorem 2
implies that  under the global regularity assumption (A1)  the time to reach a precision ε > 0 with
any black-box procedure is lower bounded by

(cid:18) RLg

(cid:16) RLg

(cid:17)2(cid:19)

 

ε

ε

Ω

∆τ +

(16)
where the notation g(ε) = Ω(f (ε)) stands for ∃C > 0 s.t. ∀ε > 0  g(ε) ≥ Cf (ε). This lower bound
proves that the convergence rate of DRS in Eq. (13) is optimal with respect to computation time and
within a d1/4 multiplicative factor of the optimal convergence rate with respect to communication.
The proof of Theorem 2 relies on the use of two objective functions: ﬁrst  the standard worst case
function used for single machine convex optimization (see e.g. [18]) is used to obtain a lower bound
on the local computation time of individual machines. Then  a second function ﬁrst introduced in [17]
is split on the two most distant machines to obtain worst case communication times. By aggregating
these two functions  a third one is obtained with the desired lower bound on the convergence rate. The
complete proof is available as supplementary material. Finally  note that  due to its random nature 
Alg. 1 is not per se a black-box procedure  and Theorem 2 does not apply to it. Lower bounds for
random algorithms are more challenging and left for future work.
Remark 2. The lower bound also holds for the average of local parameters 1
i=1 θi  and more
n
generally any parameter that can be computed using any vector of the local memories at time t: in
Theorem 2  θi t may be replaced by any θt such that

(cid:80)n

θt ∈ Span

Mi t

.

(17)

(cid:19)

(cid:18)(cid:91)

i∈V

4 Decentralized optimization under local regularity

In many practical scenarios  the network may be unknown or changing through time  and a local
communication scheme is preferable to the master/slave approach of Alg. 1. Decentralized algo-
rithms tackle this problem by replacing targeted communication by local averaging of the values of
neighboring nodes [9]. More speciﬁcally  we now consider that  during a communication step  each
machine i broadcasts a vector xi ∈ Rd to its neighbors  then performs a weighted average of the
values received from its neighbors:

node i sends xi to his neighbors and receives (cid:80)

(18)

j Wjixj .

In order to ensure the efﬁciency of this communication scheme  we impose standard assumptions on
the matrix W ∈ Rn×n  called the gossip matrix [9  8]:

5

1. W is symmetric and positive semi-deﬁnite 
2. The kernel of W is the set of constant vectors: Ker(W ) = Span(1)  where 1 = (1  ...  1)(cid:62) 
3. W is deﬁned on the edges of the network: Wij (cid:54)= 0 only if i = j or (i  j) ∈ E.

Note that these assumptions are implied by symmetry  stochasticity and positive eigengap on I − W .

4.1 Optimal convergence rate

vergence rate is obtained by replacing the diameter of the network with 1/(cid:112)γ(W )  where

Similarly to the smooth and strongly-convex case of [8]  the lower bound on the optimal con-

γ(W ) = λn−1(W )/λ1(W ) is the ratio between smallest non-zero and largest eigenvalues of W  
also known as the normalized eigengap.
Theorem 3. Let L(cid:96)  R > 0 and γ ∈ (0  1]. There exists a matrix W of eigengap γ(W ) = γ  and n
functions fi satisfying (A2)  where n is the size of W   such that for all t < d−2
γ  1) and
all i ∈ {1  ...  n} 

2 min(τ /

√

(cid:115)

¯f (θi t) − min
θ∈B2(R)

¯f (θ) ≥ RL(cid:96)
108

1
(1 + 2t
τ

√

γ

)2

+

1

1 + t

.

(19)

Assuming that the dimension d is large compared to the characteristic values of the problem  The-
orem 3 implies that  under the local regularity assumption (A2) and for a gossip matrix W with
eigengap γ(W )  the time to reach a precision ε > 0 with any decentralized black-box procedure is
lower-bounded by

Ω

The proof of Theorem 3 relies on linear graphs (whose diameter is proportional to 1/(cid:112)γ(L) where

L is the Laplacian matrix) and Theorem 2. More speciﬁcally  a technical aspect of the proof consists
in splitting the functions used in Theorem 2 on multiple nodes to obtain a dependency in L(cid:96) instead
of Lg. The complete derivation is available as supplementary material.

+

ε

ε

.

(20)

(cid:18) RL(cid:96)

τ(cid:112)γ(W )

(cid:16) RL(cid:96)

(cid:17)2(cid:19)

4.2 Optimal decentralized algorithm

We now provide an optimal decentralized optimization algorithm under (A2). This algorithm is
closely related to the primal-dual algorithm proposed by [14]  which we modify by the use of
(cid:80)n
accelerated gossip using Chebyshev polynomials as in [8].
First  we formulate our optimization problem in Eq. (1) as the saddle-point problem in Eq. (21) below 
i=1 fi(θi) over Θ = (θ1  . . .   θn) ∈ Kn
by considering the equivalent problem of minimizing 1
with the constraint that θ1 = ··· = θn  or equivalently ΘA = 0  where A is a square root of the
n
symmetric matrix W . Through Lagrangian duality  we therefore get the equivalent problem:

min
Θ∈Kn

max
Λ∈Rd×n

1
n

fi(θi) − tr Λ(cid:62)ΘA .

(21)

We solve it by applying Algorithm 1 in Chambolle-Pock [21] (we could alternatively apply composite
Mirror-Prox [22])  which is both simple and well tailored to our problem: (a) it is an accelerated
method for saddle-point problems  (b) it allows for composite problems with a sum of non-smooth and
smooth terms  (c) it provides a primal-dual gap that can easily be extended to the case of approximate
proximal operators. At each iteration t  with initialization Λ0 = 0 and Θ0 = Θ−1 = (θ0  . . .   θ0):

(a) Λt+1 = Λt − σ(2Θt+1 − Θt)A
(b) Θt+1 = argmin
Θ∈Kn

1
n

n(cid:88)

i=1

fi(θi) − tr Θ(cid:62)Λt+1A(cid:62) +

tr(Θ − Θt)(cid:62)(Θ − Θt)  

(22)

1
2η

where the gain parameters η  σ are required to satisfy σηλ1(W ) ≤ 1. We implement the algorithm
n) ∈ Rd×n  for which all updates can be made
with the variables Θt and Y t = ΛtA(cid:62) = (yt

1  . . .   yt

n(cid:88)

i=1

6

Algorithm 2 multi-step primal-dual algorithm
Input: approximation error ε > 0  gossip matrix W ∈ Rn×n 

K = (cid:98)1/(cid:112)γ(W )(cid:99)  M = T = (cid:100) 4RL(cid:96)

ε (cid:101)  c1 =

√
1−
√
1+

γ(W )

γ(W )

Output: optimizer ¯θT
1: Θ0 = 0  Θ−1 = 0  Y0 = 0
2: for t = 0 to T − 1 do
3:
4:
5:
6:
7:
8: Θt+1 = ˜ΘM
9: end for
10: return ¯θT = 1
T

Y t+1 = Y t − σ ACCELERATEDGOSSIP(2Θt − Θt−1  W   K)
˜Θ0 = Θt
for m = 0 to M − 1 do
i − 2
˜θm
(cid:80)T
(cid:80)n

(cid:2) η
n∇fi(˜θm

i ) − ηyt+1

˜θm+1
i
end for

i − θt

= m
m+2

i=1 θt
i

m+2

t=1

1
n

i

  η = nR
L(cid:96)

1−cK
1
1+cK
1

  σ = 1+c2K
τ (1−cK

1

1 )2 .

// see [8  Alg. 2]

(cid:3)  ∀i ∈ {1  . . .   n}

locally: Since AA(cid:62) = W   they now become

(a(cid:48)) Y t+1 = Y t − σ(2Θt+1 − Θt)W
fi(θi) − θ(cid:62)
(b(cid:48))

θt+1
i

= argmin
θi∈K

1
n

i yt+1

i +

(cid:107)θi − θt

i(cid:107)2 ∀i ∈ {1  . . .   n}  

1
2η

(23)

The step (b(cid:48)) still requires a proximal step for each function fi. We approximate it by the outcome of
the subgradient method run for M steps  with a step-size proportional to 2/(m + 2) as suggested
in [23]. That is  initialized with ˜θ0
i − 2
˜θm

(cid:3)  m = 0  . . .   M − 1.

i  it performs the iterations
i − θt

i ) − ηyt+1

∇fi(˜θm

(cid:2) η

i = θt

˜θm+1
i

(24)

m

=

i

m + 2

m + 2

n

We thus replace the step (b(cid:48)) by running M steps of the subgradient method to obtain ˜θM
Theorem 4. Under local regularity (A2)  the approximation error with the iterative algorithm of
Eq. (23) and (24) after T iterations and using M subgradient steps per iteration is bounded by

.

i

Theorem 4 implies that the proposed algorithm achieves an error of at most ε in a time no larger than

(cid:16) 1

T

(cid:17)

+

1
M

(cid:19)2(cid:19)

¯f (θ) ≤ RL(cid:96)(cid:112)γ(W )
(cid:18) RL(cid:96)
τ(cid:112)γ(W )

+

ε

1(cid:112)γ(W )

¯f (¯θT ) − min
θ∈K

(cid:18) RL(cid:96)

ε

O

.

(25)

.

(26)

While the ﬁrst term (associated to communication) is optimal  the second does not match the lower
bound of Theorem 3. This situation is similar to that of strongly-convex and smooth decentralized
optimization [8]  when the number of communication steps is taken equal to the number of overall
iterations.
By using Chebyshev acceleration [24  25] with an increased number of communication steps  the
algorithm reaches the optimal convergence rate. More precisely  since one communication step is a
multiplication (of Θ e.g.) by the gossip matrix W   performing K communication steps is equivalent
to multiplying by a power of W . More generally  multiplication by any polynomial PK(W ) of
degree K can be achieved in K steps. Since our algorithm depends on the eigengap of the gossip
matrix  a good choice of polynomial consists in maximizing this eigengap γ(PK(W )). This is the
approach followed by [8] and leads to the choice PK(x) = 1 − TK(c2(1 − x))/TK(c2)  where
c2 = (1 + γ(W ))/(1 − γ(W )) and TK are the Chebyshev polynomials [24] deﬁned as T0(x) = 1 
T1(x) = x  and  for all k ≥ 1  Tk+1(x) = 2xTk(x) − Tk−1(x). We refer the reader to [8] for more
γ(PK(W )) ≥ 1/4 and the optimal convergence rate.

details on the method. Finally  as mentioned in [8]  chosing K = (cid:98)1/(cid:112)γ(W )(cid:99) leads to an eigengap

7

We denote the resulting algorithm as multi-step primal-dual (MSPD) and describe it in Alg. 2. The
procedure ACCELERATEDGOSSIP is extracted from [8  Algorithm 2] and performs one step of
Chebyshev accelerated gossip  while steps 4 to 8 compute the approximation of the minimization
problem (b’) of Eq. (23). Our performance guarantee for the MSPD algorithm is then the following:
Theorem 5. Under local regularity (A2)  Alg. 2 achieves an approximation error ¯f (¯θT ) − ¯f (θ∗) of
at most ε > 0 in a time Tε upper-bounded by

 

(27)

(cid:18) RL(cid:96)

O

ε

τ(cid:112)γ(W )
(cid:25)
(cid:24) 4RL(cid:96)

ε

(cid:16) RL(cid:96)

(cid:17)2(cid:19)

+

ε

(cid:25)2

(cid:24) 4RL(cid:96)
(cid:80)n

ε

+

τ(cid:112)γ(W )
(cid:80)T

which matches the lower complexity bound of Theorem 3. Alg. 2 is therefore optimal under the the
local regularity assumption (A2).
Remark 3. It is clear from the algorithm’s description that it completes its T iterations by time

Tε ≤

.

(28)

ε

t=1

To obtain the average of local parameters ¯θT = 1
i=1 θi  one can then rely on the gossip
algorithm [9] to average over the network the individual nodes’ time averages. Let W (cid:48) = I −
nT
1 )2. Since W (cid:48) is bi-stochastic  semi-deﬁnite positive
c3PK(W ) where c3 = (1 + c2K
and λ2(W (cid:48)) = 1 − γ(PK(W )) ≤ 3/4  using it for gossiping the time averages leads to a time

(cid:1)(cid:1) to ensure that each node reaches a precision ε on the objective function (see [9] for

O(cid:0) τ√
γ ln(cid:0) RL(cid:96)

1 )/(1 − cK

more details on the linear convergence of gossip)  which is negligible compared to Eq. (27).
Remark 4. A stochastic version of the algorithm is also possible by considering stochastic oracles
on each fi and using stochastic subgradient descent instead of the subgradient method.
Remark 5. In the more general context where node compute times ρi are not necessarily all equal
to 1  we may still apply Alg. 2  where now the number of subgradient iterations performed by node i
is M/ρi rather than M. The proof of Theorem 5 also applies  and now yields the modiﬁed upper
bound on time to reach precision ε:

(cid:18) RL(cid:96)

ε

τ(cid:112)γ(W )

(cid:16) RLc

(cid:17)2(cid:19)

+

ε

 

(29)

O

(cid:80)n

i .
i=1 ρiL2

where L2

c = 1
n

5 Conclusion

In this paper  we provide optimal convergence rates for non-smooth and convex distributed optimiza-
tion in two settings: Lipschitz continuity of the global objective function  and Lipschitz continuity of
local individual functions. Under the local regularity assumption  we provide optimal convergence
rates that depend on the (cid:96)2-average of the local Lipschitz constants and the (normalized) eigengap
of the gossip matrix. Moreover  we also provide the ﬁrst optimal decentralized algorithm  called
multi-step primal-dual (MSPD).
Under the global regularity assumption  we provide a lower complexity bound that depends on the
Lipschitz constant of the (global) objective function  as well as a distributed version of the smoothing
approach of [10] and show that this algorithm is within a d1/4 multiplicative factor of the optimal
convergence rate.
√
In both settings  the optimal convergence rate exhibits two different speeds: a slow rate in Θ(1/
t)
with respect to local computations and a fast rate in Θ(1/t) due to communication. Intuitively 
communication is the limiting factor in the initial phase of optimization. However  its impact
decreases with time and  for the ﬁnal phase of optimization  local computation time is the main
limiting factor.
The analysis presented in this paper allows several natural extensions  including time-varying com-
munication networks  asynchronous algorithms  stochastic settings  and an analysis of unequal node
compute speeds going beyond Remark 5. Moreover  despite the efﬁciency of DRS  ﬁnding an optimal
algorithm under the global regularity assumption remains an open problem and would make a notable
addition to this work.

8

References

[1] Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent opti-

mization. IEEE Transactions on Automatic Control  54(1):48–61  2009.

[2] Stephen Boyd  Neal Parikh  Eric Chu  Borja Peleato  and Jonathan Eckstein. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and Trends in Machine Learning  3(1):1–122  2011.

[3] John C. Duchi  Alekh Agarwal  and Martin J. Wainwright. Dual averaging for distributed
optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic
control  57(3):592–606  2012.

[4] Wei Shi  Qing Ling  Gang Wu  and Wotao Yin. EXTRA: An exact ﬁrst-order algorithm for
decentralized consensus optimization. SIAM Journal on Optimization  25(2):944–966  2015.
[5] Wei Shi  Qing Ling  Kun Yuan  Gang Wu  and Wotao Yin. On the linear convergence of the
ADMM in decentralized consensus optimization. IEEE Transactions on Signal Processing 
62(7):1750–1761  2014.

[6] Dušan Jakoveti´c  José M. F. Moura  and Joao Xavier. Linear convergence rate of a class
of distributed augmented lagrangian algorithms. IEEE Transactions on Automatic Control 
60(4):922–936  2015.

[7] Angelia Nedic  Alex Olshevsky  and Wei Shi. Achieving geometric convergence for distributed
optimization over time-varying graphs. SIAM Journal on Optimization  27(4):2597–2633  2017.
[8] Kevin Scaman  Francis Bach  Sébastien Bubeck  Yin Tat Lee  and Laurent Massoulié. Optimal
algorithms for smooth and strongly convex distributed optimization in networks. In Proceedings
of the 34th International Conference on Machine Learning ICML  pages 3027–3036  2017.

[9] Stephen Boyd  Arpita Ghosh  Balaji Prabhakar  and Devavrat Shah. Randomized gossip

algorithms. IEEE/ACM Transactions on Networking (TON)  14(SI):2508–2530  2006.

[10] John C. Duchi  Peter L. Bartlett  and Martin J. Wainwright. Randomized smoothing for

stochastic optimization. SIAM Journal on Optimization  22(2):674–701  2012.

[11] Dušan Jakoveti´c  Joao Xavier  and José M. F. Moura. Fast distributed gradient methods. IEEE

Transactions on Automatic Control  59(5):1131–1146  2014.

[12] Aryan Mokhtari and Alejandro Ribeiro. DSA: Decentralized double stochastic averaging

gradient algorithm. Journal of Machine Learning Research  17(1):2165–2199  2016.

[13] Ermin Wei and Asuman Ozdaglar. Distributed alternating direction method of multipliers. In

51st Annual Conference on Decision and Control (CDC)  pages 5445–5450. IEEE  2012.

[14] Guanghui Lan  Soomin Lee  and Yi Zhou. Communication-efﬁcient algorithms for decentralized

and stochastic optimization. arXiv preprint arXiv:1701.03961  2017.

[15] Martin Jaggi  Virginia Smith  Martin Takác  Jonathan Terhorst  Sanjay Krishnan  Thomas
Hofmann  and Michael I Jordan. Communication-efﬁcient distributed dual coordinate ascent.
In Advances in Neural Information Processing Systems 27  pages 3068–3076  2014.

[16] Ohad Shamir. Fundamental limits of online and distributed algorithms for statistical learning
and estimation. In Advances in Neural Information Processing Systems 27  pages 163–171 
2014.

[17] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning
and optimization. In Advances in Neural Information Processing Systems 28  pages 1756–1764 
2015.

[18] Sébastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends

in Machine Learning  8(3-4):231–357  2015.

[19] J. J. Moreau. Proximité et dualité dans un espace hilbertien. Bulletin de la Société Mathématique

de France  93:273–299  1965.

[20] Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Kluwer Academic

Publishers  2004.

[21] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems
with applications to imaging. Journal of Mathematical Imaging and Vision  40(1):120–145 
May 2011.

9

[22] Niao He  Anatoli Juditsky  and Arkadi Nemirovski. Mirror prox algorithm for multi-term
composite minimization and semi-separable problems. Computational Optimization and Appli-
cations  61(2):275–319  2015.

[23] Simon Lacoste-Julien  Mark Schmidt  and Francis Bach. A simpler approach to obtaining an
O(1/t) convergence rate for the projected stochastic subgradient method. Technical Report
1212.2002  arXiv  2012.

[24] W. Auzinger. Iterative Solution of Large Linear Systems. Lecture notes  TU Wien  2011.
[25] M. Arioli and J. Scott. Chebyshev acceleration of iterative reﬁnement. Numerical Algorithms 

66(3):591–608  2014.

10

,Nan Du
Le Song
Manuel Gomez Rodriguez
Hongyuan Zha
Sainbayar Sukhbaatar
arthur szlam
Jason Weston
Rob Fergus
Kevin Scaman
Francis Bach
Sebastien Bubeck
Laurent Massoulié
Yin Tat Lee
Ryan Tibshirani
Rina Foygel Barber
Emmanuel Candes
Aaditya Ramdas