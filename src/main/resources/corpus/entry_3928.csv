2017,On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm,Tensor train (TT) decomposition provides a space-efficient representation for higher-order tensors. Despite its advantage  we face two crucial limitations when we apply the TT decomposition to machine learning problems: the lack of statistical theory and of scalable algorithms. In this paper  we address the limitations. First  we introduce a convex relaxation of the TT decomposition problem and derive its error bound for the tensor completion task. Next  we develop a randomized optimization method  in which the time complexity is as efficient as the space complexity is. In experiments  we numerically confirm the derived bounds and empirically demonstrate the performance of our method with a real higher-order tensor.,On Tensor Train Rank Minimization: Statistical

Efﬁciency and Scalable Algorithm

Masaaki Imaizumi

Institute of Statistical Mathematics

RIKEN Center for Advanced Intelligence Project

imaizumi@ism.ac.jp

Takanori Maehara

RIKEN Center for Advanced Intelligence Project

takanori.maehara@riken.jp

Kohei Hayashi

National Institute of Advanced Industrial Science and Technology

RIKEN Center for Advanced Intelligence Project

hayashi.kohei@gmail.com

Abstract

Tensor train (TT) decomposition provides a space-efﬁcient representation for
higher-order tensors. Despite its advantage  we face two crucial limitations when
we apply the TT decomposition to machine learning problems: the lack of statistical
theory and of scalable algorithms. In this paper  we address the limitations. First 
we introduce a convex relaxation of the TT decomposition problem and derive
its error bound for the tensor completion task. Next  we develop a randomized
optimization method  in which the time complexity is as efﬁcient as the space
complexity is. In experiments  we numerically conﬁrm the derived bounds and
empirically demonstrate the performance of our method with a real higher-order
tensor.

1

Introduction

Tensor decomposition is an essential tool for dealing with data represented as multidimensional arrays 
or simply  tensors. Through tensor decomposition  we can determine latent factors of an input tensor
in a low-dimensional multilinear space  which saves the storage cost and enables predicting missing
elements. Note that  a different multilinear interaction among latent factors deﬁnes a different tensor
decomposition model  which yields several variations of tensor decomposition. For general purposes 
however  either Tucker decomposition [29] or CANDECOMP/PARAFAC (CP) decomposition [8]
model is commonly used.
In the past three years  an alternative tensor decomposition model  called tensor train (TT) decompo-
sition [21] has actively been studied in machine learning communities for such as approximating the
inference on a Markov random ﬁeld [18]  modeling supervised learning [19  24]  analyzing restricted
Boltzmann machine [4]  and compressing deep neural networks [17]. A key property is that  for
higher-order tensors  TT decomposition provides more space-saving representation called TT format
while preserving the representation power. Given an order-K tensor (i.e.  a K-dimensional tensor) 
the space complexity of Tucker decomposition is exponential in K  whereas that of TT decomposition

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

is linear in K. Further  on TT format  several mathematical operations including the basic linear
algebra operations can be performed efﬁciently [21].
Despite its potential importance  we face two crucial limitations when applying this decomposition
to a much wider class of machine learning problems. First  its statistical performance is unknown.
In Tucker decomposition and its variants  many authors addressed the generalization error and
derived statistical bounds (e.g. [28  27]). For example  Tomioka et al.[28] clarify the way in which
using the convex relaxation of Tucker decomposition  the generalization error is affected by the
rank (i.e.  the dimensionalities of latent factors)  dimension of an input  and number of observed
elements. In contrast  such a relationship has not been studied for TT decomposition yet. Second 
standard TT decomposition algorithms  such as alternating least squares (ALS) [6  30]   require a
huge computational cost. The main bottleneck arises from the singular value decomposition (SVD)
operation to an “unfolding” matrix  which is reshaped from the input tensor. The size of the unfolding
matrix is huge and the computational cost grows exponentially in K.
In this paper  we tackle the above issues and present a scalable yet statistically-guaranteed TT
decomposition method. We ﬁrst introduce a convex relaxation of the TT decomposition problem and
its optimization algorithm via the alternating direction method of multipliers (ADMM). Based on
this  a statistical error bound for tensor completion is derived  which achieves the same statistical
efﬁciency as the convex version of Tucker decomposition does. Next  because the ADMM algorithm
is not sufﬁciently scalable  we develop an alternative method by using a randomization technique. At
the expense of losing the global convergence property  the dependency of K on the time complexity
is reduced from exponential to quadratic. In addition  we show that a similar error bound is still
guaranteed. In experiments  we numerically conﬁrm the derived bounds and empirically demonstrate
the performance of our method using a real higher-order tensor.

2 Preliminaries

2.1 Notation
Let X⇢ RI1⇥···⇥IK be the space of order-K tensors  where Ik denotes the dimensionality of the
k-th mode for k = 1  . . .   K. For brevity  we deﬁne I<k := Qk0<k Ik0; similarly  Ik  Ik< and
Ik are deﬁned. For a vector Y 2 Rd  [Y ]i denotes the i-th element of Y . Similarly  [X]i1 ... iK
denotes the (i1  . . .   iK) elements of a tensor X 2X . Let [X]i1 ... ik1 : ik+1 ... iK denote an Ik-
dimensional vector (Xi1 ... ik1 j ik+1 ... iK )Ik
j=1 called the mode-k ﬁber. For a vector Y 2 Rd 
kY k = (Y T Y )1/2 denotes the `2-norm and kY k1 = maxi |[Y ]i| denotes the max norm. For tensors
X  X0 2X   an inner product is deﬁned as hX  X0i :=PI1...IK
i1 ... iK =1 X(i1  . . .   iK)X0(i1  . . .   iK)
and kXkF = hX  Xi1/2 denotes the Frobenius norm. For a matrix Z  kZks :=Pj j(Z) denotes
the Schatten-1 norm  where j(·) is a j-th singular value of Z.

2.2 Tensor Train Decomposition
Let us deﬁne a tuple of positive integers (R1  . . .   RK1) and an order-3 tensor Gk 2 RIk⇥Rk1⇥Rk
for each k = 1  . . .   K. Here  we set R0 = RK = 1. Then  TT decomposition represents each
element of X as follows:

Xi1 ... iK = [G1]i1 : :[G2]i2 : : ··· [GK]iK  : :.
Note that [Gk]ik : : is an Rk1 ⇥ Rk matrix. We deﬁne G := {Gk}K
k=1 as a set of the tensors 
and let X(G) be a tensor whose elements are represented by G as (1). The tuple (R1  . . .   RK1)
controls the complexity of TT decomposition  and it is called a Tensor Train (TT) rank. Note that TT
decomposition is universal  i.e.  any tensor can be represented by TT decomposition with sufﬁciently
large TT rank [20].
When we evaluate the computational complexity  we assume the shape of G is roughly symmetric.
That is  we assume there exist I  R 2 N such that Ik = O(I) for k = 1  . . .   K and Rk = O(R) for
k = 1  . . .   K  1.

(1)

2

2.3 Tensor Completion Problem
Suppose there exists a true tensor X⇤ 2X that is unknown  and a part of the elements of X⇤ is
observed with some noise. Let S ⇢{ (j1  j2  . . .   jK)}I1 ... IK
j1 ... jK =1 be a set of indexes of the observed
elements and n := |S| QK
k=1 Ik be the number of observations. Let j(i) be an i-th element of
S for i = 1  . . .   n  and yi denote i-th observation from X⇤ with noise. We consider the following
observation model:

yi = [X⇤]j(i) + ✏i 

(2)

where ✏i is i.i.d. noise with zero mean and variance 2. For simplicity  we introduce observation
vector Y := (y1  . . .   yn)  noise vector E := (✏1  . . .  ✏ n)  and rearranging operator X : X! Rn
that randomly picks the elements of X. Then  the model (2) is rewritten as follows:

Y = X(X⇤) + E.

The goal of tensor completion is to estimate the true tensor X⇤ from the observation vector Y .
Because the estimation problem is ill-posed  we need to restrict the degree of freedom of X⇤  such
as rank. Because the direct optimization of rank is difﬁcult  its convex surrogation is alternatively
used [2  3  11  31  22]. For tensor completion  the convex surrogation yields the following optimization
problem [5  14  23  26]:

X2⇥ 1

2nkY  X(X)k2 + nkXks⇤  

min

(3)

where ⇥ ⇢X is a convex subset of X   n  0 is a regularization coefﬁcient  and k·k s⇤ is the
k=1 keX(k)ks. Here  eX(k) is the k-unfolding
overlapped Schatten norm deﬁned as kXks⇤ := 1
matrix deﬁned by concatenating the mode-k ﬁbers of X. The overlapped Schatten norm regularizes
the rank of X in terms of Tucker decomposition [16  28]. Although the Tucker rank of X⇤ is unknown
in general  the convex optimization adjusts the rank depending on n.
To solve the convex problem (3)  the ADMM algorithm is often employed [1  26  28]. Since the
overlapped Schatten norm is not differentiable  the ADMM algorithm avoids the differentiation of
the regularization term by alternatively minimizing the augmented Lagrangian function iteratively.

KPK

3 Convex Formulation of TT Rank Minimization

To adopt TT decomposition to the convex optimization problem as (3)  we need the convex surrogation
of TT rank. For that purpose  we introduce the Schatten TT norm [22] as follows:

kXks T :=

1

K  1

K1Xk=1

kQk(X)ks :=

1

K  1

K1Xk=1Xj

j(Qk(X)) 

(4)

where Qk : X! RIk⇥Ik< is a reshaping operator that converts a tensor to a large matrix where the
ﬁrst k modes are combined into the rows and the rest K  k modes are combined into the columns.
Oseledets et al.[21] shows that the matrix rank of Qk(X) can bound the k-th TT rank of X  implying
that the Schatten TT norm surrogates the sum of the TT rank. Putting the Schatten TT norm into (3) 
we obtain the following optimization problem:

X2X 1

2nkY  X(X)k2 + nkXks T .

min

(5)

3.1 ADMM Algorithm
To solve (5)  we consider the augmented Lagrangian function L(x {Zk}K1
k=1 )  where
x 2 RQk Ik is the vectorization of X  Zk is a reshaped matrices with size Ik⇥Ik<  and ↵k 2 RQk Ik
is a coefﬁcient for constraints. Given initial points (x(0) {Z(0)
k }k)  the `-th step of ADMM

k=1  {↵k}K1

k }k {↵(0)

3

is written as follows:

1

x(`+1) = e⌦T Y + n⌘

K1Xk=1
K  1
(x(`+1) + ↵(`)
k + (x(`+1)  Vk(Z(`+1)

= proxn/⌘(V 1
= ↵(`)

Z(`+1)
↵(`+1)
k

k

k

k ))  k = 1  . . .   K 
))  k = 1  . . .   K.

k

(Vk(Z(`)

k )  ↵(`)

k )! /(1 + n⌘K) 

Here  e⌦ is an n ⇥QIk

k=1 matrix that works as the inversion mapping of X; Vk is a vectorizing
operator of an Ik ⇥ Ik< matrix; prox(·) is the shrinkage operation of the singular values as
proxb(W ) = U max{S  bI  0}V T   where U SV T is the singular value decomposition of W ; ⌘> 0
is a hyperparameter for a step size. We stop the iteration when the convergence criterion is satisﬁed
(e.g. as suggested by Tomioka et al.[28]). Since the Schatten TT norm (4) is convex  the sequence of
the variables of ADMM is guaranteed to converge to the optimal solution ([5  Theorem 5.1]). We
refer to this algorithm as TT-ADMM.
TT-ADMM requires huge resources in terms of both time and space. For the time complexity  the
proximal operation of the Schatten TT norm  namely the SVD thresholding of V 1
  yields the
dominant complexity  which is O(I 3K/2) time. For the space complexity  we have O(K) variables
of size O(I K)  which requires O(KI K) space.

k

4 Alternating Minimization with Randomization

In this section  we consider reducing the space complexity for handling higher order tensors. The
idea is simple: we only maintain the TT format of the input tensor rather than the input tensor itself.
This leads the following optimization problem:

min

G  1

2nkY  X(X(G))k2 + nkX(G)ks T .

(6)

Remember that G = {Gk}k is the set of TT components and X(G) is the tensor given by the TT
format with G. Now we only need to store the TT components G  which drastically improves the
space efﬁciency.

4.1 Randomized Schatten TT norm

We approximate the optimization of the Schatten TT norm. To avoid the computation of exponentially
large-scale SVDs in the Schatten TT norm  we employ a technique called the “very sparse random
projection” [12]. The main idea is that  if the size of a matrix is sufﬁciently larger than its rank  then
its singular values (and vectors) are well preserved even after the projection by a sparse random
matrix. This motivates us to use the Schatten TT norm over the random projection.
Preliminary  we introduce tensors for the random projection. Let D1  D2 2 N be the size of the
matrix after projection. For each k = 1  . . .   K  1 and parameters  let ⇧k 1 2 RD1⇥I1⇥···⇥Ik be a
tensor whose elements are independently and identically distributed as follows:

[⇧k 1]d1 i1 ... ik =8<:

0

+ps/d1
ps/d1

with probability 1/2s 
with probability 1  1/s 
with probability 1/2s 

(7)

for i1  . . .   ik and d1 = 1  . . .   D1. Here  s > 0 is a hyperparameter controlling sparsity. Similarly 
we introduce a tensor ⇧k 2 2 RD2⇥Ik+1⇥···⇥IK1 that is deﬁned in the same way as ⇧k 1. With
⇧k 1 and ⇧k 2  let Pk : X! RD1⇥D2 be a random projection operator whose element is deﬁned as
follows:

[Pk(X)]d1 d2 =

I1Xj1=1

···

IKXjK =1

[⇧k 1]d1 j1 ... jk [X]j1 ... jK [⇧k 2]d2 jk+1 ... jK .

(8)

4

Note that we can compute the above projection by using the facts that X has the TT format and the
projection matrices are sparse. Let ⇡(k)
be a set of indexes of non-zero elements of ⇧k j. Then  using
the TT representation of X  (8) is rewritten as

j

[Pk(X(G))]d1 d2 = X(j1 ... jk)2⇡(k)
X(jk+1 ... jK )2⇡(k)

2

1

[⇧k 1]d1 j1 ... jk [G1]j1 ··· [Gk]jk

[Gk]jk+1 ··· [GK]jK [⇧k 2]d2 jk+1 ... jK  

j

j

| = |⇡(2)

|)  the computational

If the projection matrices have only S nonzero elements (i.e.  S = |⇡(1)
cost of the above equation is O(D1D2SKR3).
The next theorem guarantees that the Schatten-1 norm of Pk(X) approximates the original one.
Theorem 1. Suppose X 2X has TT rank (R1  . . .   Rk). Consider the reshaping operator Qk in
(4)  and the random operator Pk as (8) with tensors ⇧k 1 and ⇧k 2 deﬁned as (7). If D1  D2 
max{Rk  4(log(6Rk) + log(1/✏))/✏2}  and all the singular vectors u of Q(X)k are well-spread as
Pj |uj|3  ✏/(1.6kps)  we have
with probability at least 1  ✏.
Note that the well-spread condition can be seen as a stronger version of the incoherence assumption
which will be discussed later.

1  ✏
Rk kQk(X)ks  kPk(X)ks  (1 + ✏)kQk(X)ks 

4.2 Alternating Minimization
Note that the new problem (6) is non-convex because X(G) does not form a convex set on X .
However  if we ﬁx G except for Gk  it becomes convex with respect to Gk. Combining with the
random projection  we obtain the following minimization problem:

Gk " 1

min

2nkY  X(X(G))k2 +

n
K  1

K1Xk0=1

kPk0(X(G))ks# .

(9)

We solve this by the ADMM method for each k = 1  . . .   K. Let gk 2 RIkRk1Rk be the vector-
ization of Gk  and Wk0 2 RD1⇥D2 be a matrix for the randomly projected matrix. The augmented
Lagrangian function is then given by Lk(gk {Wk0}K1
k0=1)  where {k0 2 RD1D2}K1
k0=1
are the Lagrange multipliers. Starting from initial points (g(0)
k0=1)  the `-th
ADMM step is written as follows:

k0=1 {(0)

k0 }K1

k0=1 {k0}K1
k  {W (0)
k0 }K1
K1Xk0=1
k0 )⌘   k0 = 1  . . .   K  1 
))  k0 = 1  . . .   K  1.

K  1

T

1

k0 )!  
k0 )  (`)

k0(⌘eVk(W (`)

g(`+1)
k

W (`+1)

k0

(`+1)
k0

k0k0!1 ⌦T Y /n +

T

= ⌦T ⌦/n + ⌘
K1Xk0=1
= proxn/⌘⇣eV 1
eVk(W (`+1)

k0 + ( k0g(`+1)

(k0g(`+1)

= (`)

k0

k

k

k

+ (`)

Here  (k) 2 RD1D2⇥IkRk1Rk is the matrix imitating the mapping Gk 7! Pk(X(Gk;G\{Gk})) 
eVk is a vectorizing operator of D1 ⇥ D2 matrix  and ⌦ is an n ⇥ IkRk1Rk matrix of the operator
X  X(·;G\{Gk}) with respect to gk. Similarly to the convex approach  we iterate the ADMM steps
until convergence. We refer to this algorithm as TT-RAM  where RAM stands for randomized least
square.
The time complexity of TT-RAM at the `-th iteration is O((n + KD2)KI 2R4); the details are
deferred to Supplementary material. The space complexity is O(n + KI 2R4)  where O(n) is for Y
and O(KI 2R4) is for the parameters.

5

5 Theoretical Analysis

In this section  we investigate how the TT rank and the number of observations affect to the estimation
error. Note that all the proofs of this section are deferred to Supplementary material.

5.1 Convex Solution

To analyze the statistical error of the convex problem (5)  we assume the incoherence of the reshaped
version of X⇤.
Assumption 2. (Incoherence Assumption) There exists k 2{ 1  . . .   K} such that a matrix Qk(X⇤)
has orthogonal singular vectors {ur 2 RIk   vr 2 RIk<}Rk
max

r=1 satisfying

max

and

1
2

1iI<k kPU eik  (µRk/Ik)

1
2

1iI<k kPV eik  (µRk/Ik<)

with some 0  µ < 1. Here  PU and PV are linear projections onto spaces spanned by {ur}r and
{vr}r; {ei}i is the natural basis.
Intuitively  the incoherence assumption requires that the singular vectors for the matrix Qk(X⇤) are
well separated. This type of assumption is commonly used in the matrix and tensor completion
studies [2  3  31]. Under the incoherence assumption  the error rate of the solution of (5) is derived.

Theorem 3. Let X⇤ 2X be a true tensor with TT rank (R1  . . .   RK1)  and let bX 2X be
the minimizer of (3). Suppose that n  kX⇤(E)k1/n and that Assumption 2 for some k0 2
{1  2  . . .   K} is satisﬁed. If

n  Cm0µ2

k0 max{Ik0  Ik0<}Rk0 log3 max{Ik0  Ik0<}

with a constant Cm0  then with probability at least 1  (max{Ik0  Ik0<})3 and with a constant
CX 

kbX  X⇤kF  CX

n
K

K1Xk=1pRk.

Theorem 3 states that the bound for the statistical error gets larger as the TT rank increases. In other
words  completing a tensor is relatively easy as long as the tensor has small TT rank. Also  when we
set n ! 0 as n increases  we can state the consistency of the minimizer.
The result of Theorem 3 is similar to that obtained from the studies on matrix completion [3  16] and
tensor completion with the Tucker decomposition or SVD [28  31]. Note that  although Theorem 3 is
for tensor completion  the result can easily be generalized to other settings such as the tensor recovery
or the compressed sensing problems.

5.2 TT-RAM Solution
Prior to the analysis  let G⇤ be the true TT components such that X⇤ = X(G⇤). For simpliﬁcation 
we assume that the elements of G⇤ are normalized  i.e.  kGkk = 1 8k  and an Rk ⇥ Ik1Ik matrix
reshaped from G⇤k has a Rk row rank.
In addition to the incoherence property (Assumption 2)  we introduce an additional assumption on
the initial point of the ALS iteration.
Assumption 4. (Initial Point Assumption) Let Ginit := {Ginit
iteration procedure. Then  there exists a ﬁnite constant C that satisﬁes

k=1 be the initial point of the ALS

k }K

max

k2{1 ... K}kGinit

k  G⇤kkF  C.

Assumption 4 requires that the initial point is sufﬁciently close to the true solutions G⇤. Although
the ALS method is not guaranteed to converge to the global optimum in general  Assumption 4
guarantees the convergence to the true solutions [25]. Now we can evaluate the error rate of the
solution obtained by TT-RAM.

6

Theorem 5. Let X(G⇤) be the true tensor generated by G⇤ with TT rank (R1  . . .   RK1)  and
bG = Gt be the solution of TT-RAM at the t-th iteration. Further  suppose that Assumption 2 for some
k0 2{ 1  2  . . .   K} and Assumption 4 are satisﬁed  and suppose that Theorem (1) holds with ✏> 0
for k = 1  . . .   K. Let Cm  CA  CB > 0 be 0 << 1 be some constants. If
k0Rk0 max{Ik0  Ik0<} log3 max{Ik0  Ik0<} 
and the number of iterations t satisﬁes t  (log )1{log(CBnK1(1 + ✏)Pk pRk)  log C} 
then with probability at least 1  ✏(max{Ik0  Ik0<})3 and for n  kX⇤(E)k1/n 

n  Cmµ2

kX(bG)  X(G⇤)kF  CA(1 + ✏)n

K1Xk=1pRk.

(10)

Again  we can obtain the consistency of TT-RAM by setting n ! 0 as n increases. Since the setting
of n corresponds to that of Theorem 3  the speed of convergence of TT-RAM in terms of n is
equivalent to the speed of TT-ADMM.
By comparing with the convex approach (Theorem 3)  the error rate becomes slightly worse. Here 
k=1 pRk in (10) comes from the estimation by the alternating minimization  which
linearly increases by K. This is because there are K optimization problems and their errors are
accumulated to the ﬁnal solution. The term (1 + ✏) in (10) comes from the random projection. The
size of the error ✏ can be arbitrary small by controlling the parameters of the random projection
D1  D2 and s.

the term nPK1

6 Related Work

To solve the tensor completion problem with TT decomposition  Wang et al.[30] and Grasedyck et
al.[6] developed algorithms that iteratively solve minimization problems with respect to Gk for each
k = 1  . . .   K. Unfortunately  the adaptivity of the TT rank is not well discussed. [30] assumed that
the TT rank is given. Grasedyck et al.[6] proposed a grid search method. However  the TT rank is
determined by a single parameter (i.e.  R1 = ··· = RK1) and the search method lacks its generality.
Furthermore  the scalability problem remains in both methods—they require more than O(I K) space.
Phien et al. [22] proposed a convex optimization method using the Schatten TT norm. However 
because they employed an alternating-type optimization method  the global convergence of their
method is not guaranteed. Moreover  since they maintain X directly and perform the reshape of X
several times  their method requires O(I K) time.
Table 1 highlights the difference between the existing and our methods. We emphasize that our study
is the ﬁrst attempt to analyze the statistical performance of TT decomposition. In addition  TT-RAM
is only the method that both time and space complexities do not grow exponentially in K.

Method
TCAM-TT[30]
ADF for TT[6]
SiLRTC-TT[22]
TT-ADMM
TT-RAM

Global

Convergence

Rank

Adaptivity

X

(search)

X
X
X

Time

Complexity
O(nIKR4)

O(KIR3 + nKR2)

O(I 3K/2)
O(KI 3K/2)

O((n + KD2)KI 2R4) O(n + KI 2R4)

X
X

Space

Complexity

Statistical
Bounds

O(I K)
O(I K)
O(KI K)
O(I K)

Table 1: Comparison of TT completion algorithms  with R is a parameter for the TT rank such that
R = R1 = ··· = RK1  I = I1 = ··· = IK is dimension  K is the number of modes  n is the
number of observed elements  and D is the dimension of random projection.

7 Experiments

7.1 Validation of Statistical Efﬁciency
Using synthetic data  we verify the theoretical bounds derived in Theorems 3 and 5. We ﬁrst
generate TT components G⇤; each component G⇤k is generated as G⇤k = G†k/kG†kkF where each

7

Figure 1: Synthetic data: the estimation error kbX  X⇤kF against SRRPk pRk with the order-4

tensor (K = 4) and the order-5 tensor (K = 5). For each rank and n  we measure the error by 10
trials with different random seeds  which affect both the missing pattern and the initial points.

Table 2: Electricity data: the prediction error and the runtime (in seconds).

K = 5

K = 7

K = 8

K = 10

Method
Tucker

Error
0.219
TCAM-TT
0.219
ADF for TT 0.998
SiLRTC-TT 0.339
TT-ADMM 0.221
TT-RAM 0.219

Time
7.125
2.174
1.221
1.478
0.289
4.644

Error
0.371
0.928
1.160
0.928
1.019
0.928

Time
610.61
27.497
23.211
206.708
154.991
4.726

Error
N/A
0.928
1.180
N/A
1.061
0.928

Time
N/A

146.651
278.712

N/A

2418.00
7.654

Error
N/A
N/A
N/A
N/A
N/A
1.173

Time
N/A
N/A
N/A
N/A
N/A
7.968

element of G†k is sampled from the i.i.d. standard normal distribution. Then we generate Y by
following the generative model (2) with the observation ratio n/Qk Ik = 0.5 and the noise variance
0.01. We prepare two tensors of different size: an order-4 tensor of size 8 ⇥ 8 ⇥ 10 ⇥ 10 and an
order-5 tensor of size 5 ⇥ 5 ⇥ 7 ⇥ 7 ⇥ 7. At the order-4 tensor  the TT rank is set as (R1  R2  R3)
where R1  R2  R3 2{ 3  5  7}. At the order-5 tensor  the TT rank is set as (R1  R2  R3  R4) where
R1  R2  R3  R4 2{ 2  4}. For estimation  we set the size of Gk and ⇧k as 10  which is larger than
the true TT rank. The regularization coefﬁcient n is selected from {1  3  5}. The parameters for
random projection are set as s = 20 and D1 = D2 = 10.
Figure 1 shows the relation between the estimation error and the sum of root rank (SRR)Pk pRk.

The result of TT-ADMM shows that the empirical errors are linearly related to SSR which is shown
by the theoretical result. The result of TT-RAM roughly replicates the theoretical relationship.

7.2 Higher-Order Markov Chain for Electricity Data

We apply the proposed tensor completion methods for analyzing the electricity consumption data [13].
The dataset contains time series measurements of household electric power consumption for every
minutes from December 2006 to November 2010 and it contains over 200  000 observations.
The higher-order Markov chain is a suitable method to represent long-term dependency  and it is a com-
mon tool of time-series analysis [7] and natural language processing [9]. Let {Wt}t be discrete-time
random variables take values in a ﬁnite set B  and the order-K Markov chain describes the conditional
distribution of Wt with given {W⌧}⌧<t as P (Wt|{W⌧}⌧<t ) = P (Wt|Wt1  . . .   WtK). As K
increases  the conditional distribution of Wt can include more information from the past observations.
We complete the missing values of K-th Markov transition of the electricity dataset. We discretize
the value of the dataset into 10 values and set K 2{ 5  7  8  10}. Next  we empirically estimate the
conditional distribution of size 10K using 200  000 observations. Then  we create X by randomly
selecting n = 10  000 elements from the the conditional distribution and regarding the other elements
as missing. After completion  the prediction error is measured. We select hyperparameters using a
grid search with cross-validation.

8

Figure 2 compares the prediction error and the runtime by the related studies with TT decomposition.
For reference  we also report those values by Tucker decomposition without TT. When K = 5  the
rank adaptive methods achieve low estimation errors. As K increases  however  all the methods
except for TT-RAM suffers from the scalability issue. Indeed  at K = 10  only TT-RAM works and
the others does not due to exhausting memory.

8 Conclusion

In this paper  we investigated TT decomposition from the statistical and computational viewpoints.
To analyze its statistical performance  we formulated the convex tensor completion problem via the
low-rank TT decomposition using the TT Schatten norm. In addition  because the optimization of the
convex problem is infeasible  we developed an alternative algorithm called TT-RAM by combining
with the ideas of random projection and alternating minimization. Based on this  we derived the error
bounds of estimation for both the convex minimizer and the solution obtained by TT-RAM. The
experiments supported our theoretical results and demonstrated the scalability of TT-RAM.

Acknowledgement

We thank Prof. Taiji Suzuki for comments that greatly improved the manuscript. M. Imaizumi is
supported by Grant-in-Aid for JSPS Research Fellow (15J10206) from the JSPS. K. Hayashi is
supported by ONR N62909-17-1-2138.

References
[1] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Foundations and Trends R in
Machine Learning  3(1):1–122  2011.

[2] E. Candes and B. Recht. Exact matrix completion via convex optimization. Communications of

the ACM  55(6):111–119  2012.

[3] E. J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925–

936  2010.

[4] J. Chen  S. Cheng  H. Xie  L. Wang  and T. Xiang. On the equivalence of restricted boltzmann

machines and tensor network states. arXiv preprint arXiv:1701.04831  2017.

[5] S. Gandy  B. Recht  and I. Yamada. Tensor completion and low-n-rank tensor recovery via

convex optimization. Inverse Problems  27(2):025010  2011.

[6] L. Grasedyck  M. Kluge  and S. Krämer. Alternating least squares tensor completion in the

TT-format. arXiv preprint arXiv:1509.00311  2015.

[7] J. D. Hamilton. Time series analysis  volume 2. Princeton University Press  Princeton  1994.
[8] R. A. Harshman. Foundations of the parafac procedure: Models and conditions for an “explana-

tory” multi-modal factor analysis. UCLA Working Papers in Phonetics  16:1–84  1970.

[9] D. Jurafsky and J. H. Martin. Speech and language processing  volume 3. Pearson  2014.
[10] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review  51(3):455–

500  2009.

[11] A. Krishnamurthy and A. Singh. Low-rank matrix and tensor completion via adaptive sampling.

In Advances in Neural Information Processing Systems  pages 836–844  2013.

[12] P. Li  T. J. Hastie  and K. W. Church. Very sparse random projections. In Proceedings of
the 12th International Conference on Knowledge Discovery and Data Mining  pages 287–296.
ACM  2006.

[13] M. Lichman. UCI machine learning repository  2013.

9

[14] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in
visual data. Transactions on Pattern Analysis and Machine Intelligence  35(1):208–220  2013.
[15] Y. Mu  J. Dong  X. Yuan  and S. Yan. Accelerated low-rank visual recovery by random
projection. In IEEE Conference on Computer Vision and Pattern Recognition  pages 2609–2616 
2011.

[16] S. Negahban  M. J. Wainwright  et al. Estimation of (near) low-rank matrices with noise and

high-dimensional scaling. The Annals of Statistics  39(2):1069–1097  2011.

[17] A. Novikov  D. Podoprikhin  A. Osokin  and D. P. Vetrov. Tensorizing neural networks. In

Advances in Neural Information Processing Systems  pages 442–450  2015.

[18] A. Novikov  A. Rodomanov  A. Osokin  and D. Vetrov. Putting mrfs on a tensor train. In

International Conference on Machine Learning  pages 811–819. JMLR W&CP  2014.

[19] A. Novikov  M. Troﬁmov  and I. Oseledets. Exponential machines.

arXiv:1605.03795  2016.

arXiv preprint

[20] I. Oseledets and E. Tyrtyshnikov. TT-cross approximation for multidimensional arrays. Linear

Algebra and its Applications  432(1):70–88  2010.

[21] I. V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc Computing  33(5):2295–

2317  2011.

[22] H. N. Phien  H. D. Tuan  J. A. Bengua  and M. N. Do. Efﬁcient tensor completion: Low-rank

tensor train. arXiv preprint arXiv:1601.01083  2016.

[23] M. Signoretto  R. Van de Plas  B. De Moor  and J. A. Suykens. Tensor versus matrix completion:
a comparison with application to spectral data. Signal Processing Letters  18(7):403–406  2011.
[24] E. Stoudenmire and D. J. Schwab. Supervised learning with tensor networks. In Advances in

Neural Information Processing Systems  pages 4799–4807  2016.

[25] T. Suzuki  H. Kanagawa  H. Kobayashi  N. Shimizu  and Y. Tagami. Minimax optimal alternat-
ing minimization for kernel nonparametric tensor learning. In Advances in Neural Information
Processing Systems  pages 3783–3791  2016.

[26] R. Tomioka  K. Hayashi  and H. Kashima. Estimation of low-rank tensors via convex optimiza-

tion. arXiv preprint arXiv:1010.0789  2010.

[27] R. Tomioka and T. Suzuki. Convex tensor decomposition via structured schatten norm regular-

ization. In Advances in Neural Information Processing Systems  pages 1331–1339  2013.

[28] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor
decomposition. In Advances in Neural Information Processing Systems (NIPS. Citeseer  2011.
[29] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika 

31(3):279–311  1966.

[30] W. Wang  V. Aggarwal  and S. Aeron. Tensor completion by alternating minimization under the

tensor train (TT) model. arXiv preprint arXiv:1609.05587  2016.

[31] Z. Zhang and S. Aeron. Exact tensor completion using t-svd. Transactions on Signal Processing 

2016.

10

,Masaaki Imaizumi
Takanori Maehara
Kohei Hayashi