2014,Dependent nonparametric trees for dynamic hierarchical clustering,Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However  the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy  and the parameters of the clusters  to evolve with time. In this paper  we present a distribution over collections of time-dependent  infinite-dimensional trees that can be used to model evolving hierarchies  and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.,Dependent nonparametric trees for dynamic

hierarchical clustering

Avinava Dubey∗†  Qirong Ho∗‡ 

Sinead Williamson£  Eric P. Xing†

† Machine Learning Department  Carnegie Mellon University

‡ Institute for Infocomm Research  A*STAR

£ McCombs School of Business  University of Texas at Austin

akdubey@cs.cmu.edu  hoqirong@gmail.com

sinead.williamson@mccombs.utexas.edu  epxing@cs.cmu.edu

Abstract

Hierarchical clustering methods offer an intuitive and powerful way to model a
wide variety of data sets. However  the assumption of a ﬁxed hierarchy is of-
ten overly restrictive when working with data generated over a period of time:
We expect both the structure of our hierarchy  and the parameters of the clus-
ters  to evolve with time. In this paper  we present a distribution over collections
of time-dependent  inﬁnite-dimensional trees that can be used to model evolving
hierarchies  and present an efﬁcient and scalable algorithm for performing approx-
imate inference in such a model. We demonstrate the efﬁcacy of our model and
inference algorithm on both synthetic data and real-world document corpora.

Introduction

1
Hierarchically structured clustering models offer a natural representation for many forms of data.
For example  we may wish to hierarchically cluster animals  where “dog” and “cat” are subcategories
of “mammal”  and “poodle” and “dachshund” are subcategories of “dog”. When modeling scientiﬁc
articles  articles about machine learning and programming languages may be subcategories under
computer science. Representing clusters in a tree structure allows us to explicitly capture these
relationships  and allow clusters that are closer in tree-distance to have more similar parameters.
Since hierarchical structures occur commonly  there exists a rich literature on statistical models for
trees. We are interested in nonparametric distributions over trees – that is  distributions over trees
with inﬁnitely many leaves and inﬁnitely many internal nodes. We can model any ﬁnite data set
using a ﬁnite subset of such a tree  marginalizing over the inﬁnitely many unoccupied branches. The
advantage of such an approach is that we do not have to specify the tree dimensionality in advance 
and can grow our representation in a consistent manner if we observe more data.
In many settings  our data points are associated with a point in time – for example the date when
a photograph was taken or an article was written. A stationary clustering model is inappropriate in
such a context: The number of clusters may change over time; the relative popularities of clusters
may vary; and the location of each cluster in parameter space may change. As an example  consider
a topic model for scientiﬁc articles over the twentieth century. The ﬁeld of computer science – and
therefore topics related to it – did not exist in the ﬁrst half of the century. The proportion of scientiﬁc
articles devoted to genetics has likely increased over the century  and the terminology used in such
articles has changed with the development of new sequencing technology.
Despite this  to the best of our knowledge  there are no nonparametric distributions over time-
evolving trees in the literature. There exist a variety of distributions over stationary trees
[1  14  5  13  10]  and time-evolving non-hierarchical clustering models [16  7  11  2  4  12] – but
no models that combine time evolution and hierarchical structure. The reason for this is likely to
be practical: Inference in trees is typically very computationally intensive  and adding temporal
variation will  in general  increase the computational requirements. Designing such a model must 
therefore  proceed hand in hand with developing efﬁcient and scalable inference schemes.

1

(a) Inﬁnite tree

(b) Changing popularity

(c) Cluster/topic drift

Figure 1: Our dependent tree-structured stick breaking process can model trees of arbitrary size and shape 
and captures popularity and parameter changes through time. a) Model any number of nodes (clusters  topics) 
of any branching factor  and up to any depth b) Nodes can change in probability mass  or new nodes can be
created c) Node parameters can evolve over time.
In this paper  we deﬁne a distribution over temporally varying trees with inﬁnitely many nodes that
captures this form of variation  and describe how this model can cluster both real-valued observa-
tions and text data. Further  we propose a scalable approximate inference scheme that can be run in
parallel  and demonstrate its efﬁcacy on synthetic data where ground-truth clustering is available  as
well as demonstrate qualitative and quantitative performance on three text corpora.
2 Background
The model proposed in this paper is a dependent nonparametric process with tree-structured
marginals. A dependent nonparametric process [12] is a distribution over collections of random
measures indexed by values in some covariate space  such that at each covariate value  the marginal
distribution is given by some known nonparametric distribution. For example  a dependent Dirichlet
process [12  7  11] is a distribution over collections of probability measures with Dirichlet process-
distributed marginals; a dependent Pitman-Yor process [15] is a distribution over collections of
probability measures with Pitman-Yor process-distributed marginals; a dependent Indian buffet
process [17] is a distribution over collections of matrices with Indian buffet process-distributed
marginals; etc.
If our covariate space is time  such distributions can be used to construct non-
parametric  time-varying models.
There are two main methods of inducing dependency: Allowing the sizes of the atoms composing
the measure to vary across covariate space  and allowing the parameter values associated with the
atoms to vary across covariate space. In the context of a time-dependent topic model  these methods
correspond to allowing the popularity of a topic to change over time  and allowing the words used
to express a topic to change over time (topic drift). Our proposed model incorporates both forms
of dependency. In the supplement  we discuss some speciﬁc dependent nonparametric models that
share properties with our model.
The key difference between our proposed model and existing dependent nonparametric models is
that ours has tree-distributed marginals. There are a number of options for the marginal distribution
over trees  as we discuss in the supplement. We choose a distribution over inﬁnite-dimensional trees
known as the tree-structured stick breaking process [TSSBP  1]  described in Section 2.1.
2.1 The tree-structured stick-breaking process
The tree-structured stick-breaking process (TSSBP) is a distribution over trees with inﬁnitely many
leaves and inﬁnitely many internal nodes. Each node  within the tree is associated with a mass π
 π = 1  and each data point is assigned to a node in the tree according to p(zn = ) =
π  where zn is the node assignment of the nth data point. The TSSBP is unique among the current
toolbox of random inﬁnite-dimensional trees in that data can be assigned to an internal node  rather
than a leaf  of the tree. This property is often desirable; for example in a topic modeling context 
a document could be assigned to a general topic such as “science” that lives toward the root of the
tree  or to a more speciﬁc topic such as “genetics” that is a descendant of the science topic.
The TSSBP can be represented using two interleaving stick-breaking processes – one (parametrized
by α) that determines the size of a node and another (parametrized by γ) that determines the branch-
ing probabilities. Index the root node as node ∅ and let π∅ be the mass assigned to it. Index its
(countably inﬁnite) child nodes as node 1  node 2  . . . and let π1  π2  . . . be the masses assigned to
them; index the child nodes of node 1 as nodes 1 · 1  1 · 2  . . . and let π1·1  π1·2  . . . be the masses
assigned to nodes 1 · 1  1 · 2 . . . ; etc. Then we can sample the inﬁnite-dimensional tree as:

such that(cid:80)

(cid:81)i−1
ν ∼ Beta(1  α(||))  ψ ∼ Beta(1  γ) 
j=1(1 − ψ·j) π = νφ
φ·i = ψ·i

(cid:81)
(cid:48)≺(1 − ν(cid:48))φ(cid:48) 

π∅ = ν∅  φ∅ = 1

(1)

2

where || indicates the depth of node   and (cid:48) ≺  indicates that (cid:48) is an ancestor node of . We refer
to the resulting inﬁnite-dimensional weighted tree as Π = ((π)  (φi)).
3 Dependent tree-structured stick-breaking processes
We now describe a dependent tree-structured stick-breaking process where both atom sizes and their
locations vary with time. We ﬁrst describe a distribution over atom sizes  and then use this distribu-
tion over collections of trees as the basis for time-varying clustering models and topic models.
3.1 A distribution over time-varying trees
We start with the basic TSSBP model [1] (described in Section 2.1 and the left of Figure 1)  and
modify it so that the latent variables ν  ψ and π are replaced with sequences ν(t)
  ψ(t)
and π(t)



indexed by discrete time t ∈ T (the middle of Figure 1). The forms of ν(t)
and ψ(t)
are chosen so

that the marginal distribution over the π(t)


is as described in Equation 1.

 = (cid:80)Nt
 = (cid:80)Nt
Let N (t) be the number of observations at time t  and let z(t)
n be the node allocation of the nth
observation at time t. For each node  at time t  let X (t)
n = ) be the number
of observations assigned to node  at time t  and Y (t)
n ) be the number of
observations assigned to descendants of node . Introduce a “window” parameter h ∈ N. We can
(cid:1)
then deﬁne a prior predictive distribution over the tree at time t  as
(cid:80)t
t(cid:48)=t−h Y (t(cid:48))
t(cid:48)=t−h(X (t(cid:48))

 ∼ Beta(cid:0)1 +(cid:80)t−1
·i ∼ Beta(cid:0)1 +(cid:80)t−1

  α(||) +(cid:80)t−1
·i ) γ +(cid:80)

t(cid:48)=t−h X (t(cid:48))
t(cid:48)=t−h(X (t(cid:48))

·j )(cid:1).

·j + Y (t(cid:48))

I(z(t)

n=1

I( ≺ z(t)



·i + Y (t(cid:48))

ν(t)
ψ(t)

(2)



n=1



j>i

 )  (φ(t)

i ))  t ∈ T ).

Following [1]  we let α(d) = λdα0  for α0 > 0 and λ ∈ (0  1). This deﬁnes a sequence of trees
(Π(t) = ((π(t)
Intuitively  the prior distribution over a tree at time t is given by the posterior distribution of the (sta-
tionary) TSSBP  conditioned on the observations in some window t − h  . . .   t − 1. The following
theorem gives the equivalence of dynamic TSSBP (dTSSBP) and TSSBP
Theorem 1. The marginal posterior distribution of the dTSSBP  at time t  follows a TSSBP.

The proof is a straightforward extension of that for the generalized P´olya urn dependent Dirichlet
process [7] and is given in the supplimentary. The above theorem implies that Equation 2 deﬁnes a
dependent tree-structured stick-breaking process.
We note that an alternative choice for inducing dependency would be to down-weight the contri-
bution of observations for previous time-steps. For example  we could exponentially decay the
contributions of observations from previous time-steps  inducing a similar form of dependency as
that found in the recurrent Chinese restaurant process [2]. However  unlike the method described in
Equation 2  such an approach would not yield stationary TSSBP-distributed marginals.
3.2 Dependent hierarchical clustering
The construction above gives a distribution over inﬁnite-dimensional trees  which in turn have a
probability distribution over their nodes. In order to use this distribution in a hierarchical Bayesian
model for data  we must associate each node with a parameter value θ(t)
. We let Θ(t) denote the set

of all parameters θ(t)
 associated with a tree Π(t). We wish to capture two properties: 1) Within a tree
Π(t)  nodes have similar values to their parents; and 2) Between trees Π(t) and Π(t+1)  corresponding
parameters θ(t)
have similar values. This form of variation is shown in the right of

Figure 1. In this subsection  we present two models that exhibit these properties: One appropriate
for real-valued data  and one appropriate for multinomial data.
3.2.1 A time-varying  tree-structured mixture of Gaussians
An inﬁnite mixture of Gaussians is a ﬂexible choice for density estimation and clustering real-valued
observations. Here  we suggest a time-varying hierarchical clustering model that is similar to the
generalized Gaussian model of [1]. The model assumes Gaussian-distributed data at each node  and
allows the means of clusters to evolve in an auto-regressive model  as below:

and θ(t+1)



θ(t)∅ |θ(t−1)

∅

∼ N (θ(t−1)

∅

  σ0σa

1 I) 

·i|θ(t)
θ(t)



  θ(t−1)
·i ∼ N (m  s2I) 

(3)

3

(cid:18)

(cid:19)−1

(cid:18)

(cid:19)





.

)2

+

σ0σ

σ0σ

+ ηθ(t−1)

1
|·i|+a
1

and θ(t)

and θ(t)


·i
|·i|+a
1

1
|·i|
σ0σ
1

|·i|+a
1

θ(t)

|·i|
(σ0σ
1

  m = s2·

||
1   and the factor potential associated with the link between θ(t)

  σ0 > 0  σ1 ∈ (0  1) 
where  s2 =
η ∈ [0  1)  and a ≥ 1. Due to the self-conjugacy of the Gaussian distribution  this corresponds to
a Markov network with factor potentials given by unnormalized Gaussian distributions: Up to a
normalizing constant  the factor potential associated with the link between θ(t−1)
is Gaus-
sian with variance σ0σ
·i is
Gaussian with variance σ0σ
For a single time point  this allows for fractal-like behavior  where the distance between child and
parent decreases down the tree. This behavior  which is not used in the generalized Gaussian model
of [1]  makes it easier to identify the root node  and guarantees that the marginal distribution over
the location of the leaf nodes has ﬁnite variance. The a parameter enforces the idea that the amount
of variation between θ(t)
·i   while η ensures the

variance of node parameters remains ﬁnite across time. We chose spherical Gaussian distributions
to ensure that structural variation is captured by the tree rather than by node parameters.
3.3 A time-varying model for hierarchically clustering documents
Given a dictionary of V words  a document can be represented using a V -dimensional term fre-
quency vector  that corresponds to a location on the surface of the (V − 1)-dimensional unit sphere.
The von Mises-Fisher distribution  with mean direction µ and concentration parameter τ  provides
a distribution on this space. A mixture of von Mises-Fisher distributions can  therefore  be used to
cluster documents [3  8]. Following the terminology of topic modeling [6]  the mean direction µk
associated with the kth cluster can be interpreted as the topic associated with that cluster.
We construct a time-dependent hierarchical clustering model appropriate for documents by associ-
ating nodes of our dependent nonparametric tree with topics. Let x(t)
n be the vector associated with
the nth document at time t. We assign a mean parameter θ(t)


is smaller than that between θ(t)


and θ(t+1)

and θ(t)



(cid:113)

to each node  in each tree Π(t) as
  θ(t−1)
·i ∼ vMF(τ (t)

(4)

·i   ρ(t)
·i ) 
1 θ(t−1)

∅

κ0θ(t)−1+κ0κa

(cid:113)
θ(t)∅ |θ(t−1)

∅

∼ vMF(τ (t)∅   ρ(t)∅ ) 



·i|θ(t)
θ(t)
1(θ(t)−1 · θ(t−1)
·i = κ0κ
τ (t)

) 
|·i|
1

∅





) 

·i

θ(t−1)
·i

= κ0

1(θ(t)

1 + κ2a

1 + κ2a

1 + 2κa

ρ(t)
·i =

where  ρ(t)∅

n according to z(t)

1 + 2κa
· θ(t−1)

 )  we sample each document x(t)

ρ(t)
∅
  κ0 > 0  κ1 > 1  and

τ (t)∅
=
|·i|+a
|·i|
θ(t)
 +κ0κ
κ0κ
1
1
ρ(t)
·i
θ(t)−1 is a probability vector of the same dimension as the θ(t)
that can be interpreted as the parent of
the root node at time t.1 This yields similar dependency behavior to that described in Section 3.2.1.
n ∼
Conditioned on Π(t) and Θ(t) = (θ(t)
Discrete(Π(t)) and xn ∼ vMF(θ(t)  β). This is a hierarchical extension of the temporal vMF mix-
ture proposed by [8].
4 Online Learning
In many time-evolving applications  we observe data points in an online setting. We are typically
interested in obtaining predictions for future data points  or characterizing the clustering structure of
current data  rather than improving predictive performance on historic data. We therefore propose
a sequential online learning algorithm  where at each time t we infer the parameter settings for the
tree Π(t) conditioned on the previous trees  which we do not re-learn. This allows us to focus our
computational efforts on the most recent (and likely relevant) data. This has the added advantage of
reducing the computational demands of the algorithm  as we do not incorporate a backwards pass
through the data  and are only ever considering a fraction of the data at a time.
In developing an inference scheme  there is always a trade-off between estimate quality and com-
putational requirements. MCMC samplers are often the “gold standard” of inference techniques 
because they have the true posterior distribution as the stationary distribution of their Markov Chain.
However  they can be very slow  particularly in complex models. Estimating the parameter setting
that maximizes the data likelihood is a much cheaper  but cannot capture the full posterior.

1In our experiments  we set θ(t)−1 to be the average over all data points at time t. This ensures that the root

node is close to the centroid of the data  rather than the periphery.

4



{X (0)

only depend on {z(0)

 ). The resulting algorithm has the following desirable properties:

In order to develop an inference algorithm that is parallelizable  runs in reasonable time  but still
obtains good predictive performance  we combine Gibbs sampling steps for learning the tree
parameters (Π(t)) and the topic indicators (z(t)
n ) with a MAP method for estimating the location
parameters (θ(t)
1. The priors for ν(t)

  ψ(t)

 } . . .{X (t−1)
n }. Hence we
2. The posteriors for ν(t)

n } (or more
can Gibbs sample ν(t)
precisely  their sufﬁcient statistics {X  Y}). Similarly  we can Gibbs sample the cluster/topic
assignments {z(t)
 } and the data  as well as infer
the MAP estimate of {θ(t)
 } in parallel given the data and the cluster/topic assignments. Because
of the online assumption  we do not consider evidence from times u > t.

are conditionally independent given {z(1)
in parallel given the cluster assignments {z(1)

} can be updated in amortized constant time.

n } in parallel given the parameters {ν(t)

}  whose sufﬁcient statistics

  Y (t−1)

  ψ(t)

  ψ(t)


n } . . .{z(t−1)

n } . . .{z(t)

n } . . .{z(t)

  Y (0)

  ψ(t)


  θ(t)

n









Due to the conjugacy between the beta and binomial distributions  we can

Sampling ν(t)
easily Gibbs sample the stick-breaking parameters



  ψ(t)


|X  Y ∼ Beta(cid:0)1 +(cid:80)t
·i|X·i  Y·i ∼ Beta(cid:0)1 +(cid:80)t

ν(t)


ψ(t)

 α(||) +(cid:80)t
·i ) γ +(cid:80)

(cid:1)
(cid:80)t
t(cid:48)=t−h Y (t(cid:48))
t(cid:48)=t−h(X (t(cid:48))

·i + Y (t(cid:48))

j>i



t(cid:48)=t−h X (t(cid:48))
t(cid:48)=t−h(X (t(cid:48))



·j )(cid:1).

·j + Y (t(cid:48))

The ν(t)
so the sampler can be parallelized. We only explicitly store π(t)


 distributions for each node are conditionally independent given the counts X  Y   and
for nodes  with nonzero

   θ(t)


  φ(t)



  ψ(t)

counts  i.e.(cid:80)t

t(cid:48)=t−h X (t(cid:48))

 + Y (t(cid:48))

 > 0.
Conditioned on the ν(t)
 } {ψ(t)

 }  x(t)



and ψ(t)


n | {ν(t)

Sampling z(t)
  the distribution over the cluster assignments z(t)
n
n
is just given by the TSSBP. We therefore use the slice sampling method described in [1] to Gibbs
sample z(t)
n   θ. Since the cluster assignments are conditionally independent
given the tree  this step can be performed in parallel.
Learning θ
It is possible to Gibbs sample the cluster parameters θ; however  in the document clus-
tering case described in Section 3.3  this requires far more time than sampling all other parameters.
To improve the speed of our algorithm  we instead use maximum a posteriori (MAP) estimates for
θ  obtained using a parallel coordinate ascent algorithm. Notably  conditioned on the trees at time
for odd-numbered tree depths || are conditionally independent given the
t − 1 and t + 1  the θ(t)
(cid:48) s at even-numbered tree depths |(cid:48)|  and vice versa. Hence  our algorithm alternates between
θ(t)
parallel optimization of odd-depth θ(t)


  and parallel optimization of even-depth θ(t)


.







  its postdecessor θ(t+1)

In general  the conditional distribution of a cluster parameter θ(t)
 depends on the values of its prede-
cessor θ(t−1)
  its parent at time t  and its children at time t. In some cases 
not all of these values will be available – for example if a node was unoccupied at previous time
steps. In this case  the distribution now depends on the full history of the parent node. For computa-
tional reasons  and because we do not wish to store the full history  we approximate the distribution
as being dependent only on observed members of the node’s Markov blanket.
5 Experimental evaluation
We evaluate the performance of our model on both synthetic and real-world data sets. Evaluation
on synthetic data sets allows us to verify that our inference algorithm allows us to recover the “true”
evolving hierarchical structure underlying our data. Evaluation on real-world data allows us to
evaluate whether our modeling assumptions are useful in practice.
5.1 Synthetic data
We manually created a time-evolving tree  as shown in Figure 2  with Gaussian-distributed data
at each node. This synthetic time-evolving tree features temporal variation in node probabilities 
temporal variation in node parameters  and addition and deletion of nodes. Using the Gaussian
model described in Equation 3  we inferred the structure of the tree at each time period as described
in Section 4. Figure 3 shows the recovered tree structure  demonstrating the ability of our inference
algorithm to recover the expected evolving hierarchical structure. Note that it accurately captures
evolution in node probabilities and location  and the addition and deletion of new nodes.

5

Figure 2: Ground truth tree  evolving over three time steps

Figure 3: Recovered tree structure  over three consecutive time periods. Each color indicates a node in the
tree and each arrow indicates a branch connecting parent to child; nodes are consistently colored across time.

Depth limit
TWITTER
SOU
PNAS

TWITTER
SOU
PNAS

dTSSBP

4

522 ± 4.35
2708 ± 32.0
4562 ± 116

3

249 ± 0.98
1320 ± 33.6
3217 ± 195

o-TSSBP

T-TSSBP

4

414 ± 3.31
1455 ± 44.5
2672 ± 357

3

199 ± 2.19
583 ± 16.4
1163 ± 196

4

335 ± 54.8
1687 ± 329
4333 ± 647

3

182 ± 24.1
1089 ± 143
2962 ± 685

dDP

o-DP

204 ± 8.82
834 ± 51.2
2374 ± 51.7
Table 1: Test set average log-likelihood on three datasets.

136 ± 0.42
633 ± 18.8
1061 ± 10.5

T-DP

112 ± 10.9
890 ± 70.5
2174 ± 134

5.2 Real-world data
In Section 3.3  we described how the dependent TSSBP can be combined with a von Mises-Fisher
likelihood to cluster documents. To evaluate this model  we looked at three corpora:
• TWITTER: 673 102 tweets containing hashtags relevant to the NFL  collected over 18 weeks in 2011 and
• PNAS: 79 800 paper titles from the Proceedings of the National Academy of Sciences between 1915 and
2005  containing 36 901 unique words (after stopwording). We grouped the titles into 10 ten-year epochs.
• STATE OF THE UNION (SOU): Presidential SoU addresses from 1790 through 2002  containing 56 352
sentences and 21 505 unique words (after stopwording). We grouped the sentences into 21 ten-year epochs.

containing 2 636 unique words (after stopwording). We grouped the tweets into 9 two-week epochs.

In each case  documents were represented using their vector of term frequencies.
Our hypothesis is that the topical structure of language is hierarchically structured and time-
evolving  and that a model that captures these properties will achieve better performance than models
that ignore hierarchical structure and/or temporal evolution. To test these hypotheses  we compare
our dependent tree-structured stick-breaking process (dTSSBP) against several online nonparamet-
ric models for document clustering:

1. Multiple tree-structured stick-breaking process (T-TSSBP): We modeled the entire corpus using the sta-
tionary TSSBP model  with each node modeled using an independent von Mises-Fisher distribution. Each
time period is modeled with a separate tree  using a similar implementation to our time-dependent TSSBP.
2. “Online” tree-structured stick-breaking processes (o-TSSBP): This simulates online learning of a single 
stationary tree over the entire corpus. We used our dTSSBP implementation with an inﬁnite window h =
∞  and once a node is created at time t  we prevent its vMF mean θ(t)
from changing in future time points.
3. Dependent Dirichlet process (dDP): We modeled the entire corpus using an h-order Markov generalized
P´olya urn DDP [7]. This model was implemented by modifying our dTSSBP code to have a single level.
Node parameters were evolved as θ(t)

k ∼ vMF(θ(t)

k   ξ).

4. Multiple Dirichlet process (T-DP): We modeled the entire corpus using DP mixtures of von Mises-Fisher
distributions  one DP per time period. Each node was modeled using an independent von Mises-Fisher
distribution. We used our own implementation.



6

Figure 4: PNAS dataset: Birth  growth  and death of tree-structured topics in our dTSSBP model. This
illustration captures some trends in American scientiﬁc research throughout the 20th century  by focusing on
the evolution of parent and child topics in two major scientiﬁc areas: Chemistry and Immunology (the rest of
the tree has been omitted for clarity). At each epoch  we show the number of documents assigned to each topic 
as well as it’s most popular words (according to the vMF mean θ).

5. “Online” Dirichlet process (o-DP): This simulates online learning of a single DP over the entire corpus.
We used our dDP implementation with an inﬁnite window h = ∞  and once a cluster is instantiated at time
t  we prevent its vMF mean θ(t) from changing in future time points.

Evaluation scheme: We divide each dataset into two parts: the ﬁrst 50%  and last 50% of time
points. We use the ﬁrst 50% to tune model parameters and select a good random restart (by training
on 90% and testing on 10% of the data at each time point)  and then use the last 50% to evaluate
the performance of the best parameters/restart (again  by training on 90% and testing on 10% data).
When training the 3 TSSBP-based models  we grid-searched κ0 ∈ {1  10  100  1000  10000}  and
ﬁxed κ1 = 1  a = 0 for simplicity. Each value of κ0 was run 5 times to get different random
restarts  and we took the best κ0-restart pair for evaluation on the last 50% of time points. For the 3
DP-based models  there is no κ0 parameter  so we simply took 5 random restarts and used the best
one for evaluation. For all TSSBP- and DP-based models  we repeated the evaluation phase 5 times
to get error bars. Every dTSSBP trial completed in < 20 minutes on a single processor core  while
we observed moderate (though not perfectly linear) speedups with 2-4 processors.
Parameter settings: For all models  we estimated each node/cluster’s vMF concentration param-
eter β from the data. For the TSSBP-based models  we used stick breaking parameters γ = 0.5 and
α(d) = 0.5d  and set θ(t)−1 to the average document term frequency vector at time t. In order to keep
running times reasonable  we limit the TSSBP-based models to a maximum depth of either 3 or 4
(we report results for both)2. For the DP-based models  we used a Dirichlet process concentration
parameter of 1. The dDP’s inter-epoch vMF concentration parameter was set to ξ = 0.001.
Results: Table 1 shows the average log (unnormalized) likelihoods on the test sets (from the last
50% of time points). The tree-based models uniformly out-perform the non-hierarchical models 
while the max-depth-4 tree models outperform the max-depth-3 ones. On all 3 datasets  the max-
depth-4 dTSSBP uniformly outperforms all models  conﬁrming our initial hypothesis.
5.3 Qualitative results
In addition to high-quality quantitative results  we ﬁnd that the time-dependent tree model gives
good qualitative performance. Figure 4 shows two time-evolving sub-trees obtained from the PNAS
data set. The top level shows a sub-tree concerned with Chemistry; the bottom level shows a sub-tree

2One justiﬁcation is that shallow hierarchies are easier to interpret than deep ones; see [5  9].

7

9 mobilities  ions  air  electrons  presence  resistance  function  electric  molecules  disease 36 pressure  ions  solutions  salts  osmotic  molecules  mobilities  gas  effect  influence 3 pressure  acoustic  exhibit  excitation  telephonic  variation  heat  specific  liquids  chiefly Chemistry 1915 - 1924 19 electrons  mobilities  ions  air  presence  metals  electric  resistance  function  conductivity 3 pressure  ions  solutions  salts  osmotic  molecules  mobilities  gas  effect  influence 9 pressure  acoustic  liquids  telephonic  exhibit  excitation  variation  heat  specific  reservoirs 3 solutions  liquids  non  salts  fields  electrolytes  dielectric  fused  squares  intensive Chemistry 1925 - 1934 0 3 pressure  acoustic  liquids  telephonic  exhibit  excitation  variation heat  specific  reservoirs 24 solutions  equations  finite  field  liquids  salts  non  electrolytes  conductance  certain Chemistry 1945 - 1954 0 11 pressure  acoustic  liquids  telephonic  exhibit  excitation  variation  heat  specific  reservoirs 11 solutions  equations  finite  field  liquids  non  salts  electrolytes  conductance  certain Chemistry 1965 - 1974 … … 30 virus  murine  leukemia  cells  sarcoma  antibody  herpes  induced  simian  type Immunology 1965 - 1974 209 virus  simian  rna  cells  vesicular  stomatitis  influenza  sequence  antigen  viral 97 virus  leukemia  murine  sarcoma  cells  induced  mice  herpes  antigens  simplex 93 virus  sarcoma  avian  gene  transforming  genome  protein  sequences  murine  myeloblastosis 63 virus  cells  epstein  barr  murine  antibody  sarcoma  leukemia  vitro  antibodies Immunology 1975 - 1984 133 virus  simian  rna  cells  vesicular  stomatitis  influenza  sequence  antigen  viral 97 virus  leukemia  murine  sarcoma  cells  induced  mice  herpes  antigens  simplex 65 virus  cells  epstein  barr  murine  antibody  sarcoma  leukemia  vitro  antibodies Immunology 1985 - 1994 Figure 5: State of the Union dataset: Birth  growth  and death of tree-structured topics in our dTSSBP
model. This illustration captures some key events in American history. At each epoch  we show the number of
documents assigned to each topic  as well as it’s most popular words (according to the vMF mean θ).

concerned with Immunology. Our dynamic tree model discovers closely-related topics and groups
them under a sub-tree  and creates  grows and destroys individual sub-topics as needed to ﬁt the data.
For instance  our model captures the sudden surge in Immunology-related research from 1975-1984 
which happened right after the structure of the antibody molecule was identiﬁed a few years prior.
In the Chemistry topic  the study of mechanical properties of materials (pressure  acoustic properties 
speciﬁc heat  etc) is a constant presence throughout the century. The study of electrical properties
of materials starts off with a topic (in purple) that seems devoted to Physical Chemistry. However 
following the development of Quantum Mechanics in the 30s  this line of research became more
closely aligned with Physics than Chemistry  and it disappears from the sub-tree. In its wake  we
see the growth of a topic more concerned with electrolytes  solutions and salts  which remained the
within the sphere of Chemistry.
Figure 5 shows time-evolving sub-trees obtained from the State of the Union dataset. We see a
sub-tree tracking the development of the Cold War. The parent node contains general terms relevant
to the Cold War; starting from the 1970s  a child node (shown in purple) contains terms relevant
to nuclear arms control  in light of the Strategic Arms Limitation Talks of that decade. The same
decade also sees the birth of a child node focused on Asia (shown in cyan)  contemporaneous with
President Richard Nixon’s historic visit to China in 1972. In addition to the Cold War  we also
see topics corresponding to events such as the Mexican War  the Civil War and the Indian Wars 
demonstrating our model’s ability to detect events in a timeline.

6 Discussion

In this paper  we have proposed a ﬂexible nonparametric model for dynamically-evolving  hierar-
chically structured data. This model can be applied to multiple types of data using appropriate
choices of likelihood; we present an application in document clustering that combines high-quality
quantitative performance with intuitively interpretable results. One of the signiﬁcant challenges in
constructing nonparametric dependent tree models is the need for efﬁcient inference algorithms. We
make judicious use of approximations and combine MCMC and MAP approximation techniques to
develop an inference algorithm that can be applied in an online setting  while being parallelizable.
Acknowledgements: This research was supported by NSF Big data IIS1447676  DARPA XDATA
FA87501220324 and NIH GWAS R01GM087694.

8

40 world  peace  free  nation  nations  america  war  dream  american  communist Cold War 1960 - 1970 10 world  security  strength  relations  peace  people  fourth  nations  nuclear  continue 144 world  peace  free  nation  nations  america  war  dream  american  communist 6 world  major  peace  asia  force  exist  security  america  natural  nation Cold War 1970 - 1980 3 world  major  peace  asia  force  exist  security  america  natural  nation 3 world  power  defenses  years  leadership  restore  alliances  trusts  peace  requires Cold War 1980 - 1990 87 world  peace  free  nation  nations  america  war  dream  american  communist 3 world  security  strength  relations  peace  people  fourth  nations  nuclear  continue Cold War 1990 - 2000 10 world  peace  free  nation  nations  america  war  dream  american  communist 5 world  security  strength  relations  peace  people  fourth  nations  nuclear  continue 19 general  army  command  war  proper  summer  secretary  operations  time  mexico Mexican War 1840 - 1850 10 slavery  constitution  senate  van  buren  war  existed  rebellion  time  act Civil War 1860 - 1870 Indian Wars 1790 - 1800 1 indian  tribes  overtures  friendship  spared  source  lands  commissioners  extinguished  title Indian Wars 1800 - 1810 11 indian  tribes  friendship  overtures  spared  lands  source  demarcation  practicable  imposition Indian Wars 1810 - 1820 Indian Wars 1830 - 1840 1 indian  tribes  overtures  friendship  spared  source  lands  commissioners  title  demarcation 2 indian  tribes  overtures  friendship  spared  source  lands  imposition  war  mode … 6 indian  tribes  friendship  overtures  spared  lands  source  demarcation  practicable  imposition 5 indian  tribes  friendship  overtures  spared  lands  source  demarcation  practicable  imposition References
[1] R. Adams  Z. Ghahramani  and M. Jordan. Tree-structured stick breaking for hierarchical data.

In Advances in Neural Information Processing Systems  2010.

[2] A. Ahmed and E. Xing. Dynamic non-parametric mixture models and the recurrent Chinese

restaurant process: with applications to evolutionary clustering. In SDM  2008.

[3] A. Banerjee  I. Dhillon  J. Ghosh  and S. Sra. Clustering on the unit hypersphere using von

Mises-Fisher distributions. Journal of Machine Learning Research  6:1345–1382  1995.

[4] D. Blei and P. Frazier. Distance dependent Chinese restaurant processes. Journal of Machine

Learning Research  12(2461–2488)  2011.

[5] D. Blei  T. Grifﬁths  M. Jordan  and J. Tenenbaum. Hierarchical topic models and the nested

Chinese restaurant process. In Advances in Neural Information Processing Systems  2004.

[6] D. Blei  A. Ng  and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[7] F. Caron  M. Davy  and A. Doucet. Generalized Polya urn for time-varying Dirichlet processes.

In uai  2007.

[8] S. Gopal and Y. Yang. Von Mises-Fisher clustering models. In International Conference on

Machine Learning  2014.

[9] Q. Ho  J. Eisenstein  and E. Xing. Document hierarchies from text and links. In Proceedings

of the 21st international conference on World Wide Web  pages 739–748. ACM  2012.

[10] J. Kingman. On the genealogy of large populations. Journal of Applied Probability  19:27–43 

1982.

[11] D. Lin  E. Grimson  and J. Fisher. Construction of dependent Dirichlet processes based on

Poisson processes. In Advances in Neural Information Processing Systems  2010.

[12] S. N. MacEachern. Dependent nonparametric processes. In Bayesian Statistical Science  1999.
[13] R. M. Neal. Density modeling and clustering using Dirichlet diffusion trees. Bayesian Statis-

tics  7:619–629  2003.

[14] A. Rodriguez  D. Dunson  and A. Gelfand. The nested Dirichlet process. Journal of the

American Statistical Association  103(483)  2008.

[15] E. Sudderth and M. Jordan. Shared segmentation of natural scenes using dependent Pitman-

Yor processes. In Advances in Neural Information Processing Systems  2008.

[16] X. Wang and A. McCallum. Topics over time: a non-Markov continuous-time model of topical

trends. In Knowledge Discovery and Data Mining  2006.

[17] S. Williamson  P. Orbanz  and Z. Ghahramani. Dependent Indian buffet processes. In Artiﬁcial

Intelligence and Statistics  2010.

9

,Kumar Avinava Dubey
Qirong Ho
Sinead Williamson
Eric Xing