2008,Performance analysis for L\_2 kernel classification,We provide statistical performance guarantees for a recently introduced kernel classifier that optimizes the $L_2$ or integrated squared error (ISE) of a difference of densities. The classifier is similar to a support vector machine (SVM) in that it is the solution of a quadratic program and yields a sparse classifier. Unlike SVMs  however  the $L_2$ kernel classifier does not involve a regularization parameter. We prove a distribution free concentration inequality for a cross-validation based estimate of the ISE  and apply this result to deduce an oracle inequality and consistency of the classifier on the sense of both ISE and probability of error. Our results can also be specialized to give performance guarantees for an existing method of $L_2$ kernel density estimation.,Performance analysis for L2 kernel classiﬁcation

JooSeuk Kim

Department of EECS
University of Michigan
Ann Arbor  MI  USA

stannum@umich.edu

Clayton D. Scott∗
Department of EECS
University of Michigan
Ann Arbor  MI  USA

clayscot@umich.edu

Abstract

We provide statistical performance guarantees for a recently introduced kernel
classiﬁer that optimizes the L2 or integrated squared error (ISE) of a difference
of densities. The classiﬁer is similar to a support vector machine (SVM) in that
it is the solution of a quadratic program and yields a sparse classiﬁer. Unlike
SVMs  however  the L2 kernel classiﬁer does not involve a regularization param-
eter. We prove a distribution free concentration inequality for a cross-validation
based estimate of the ISE  and apply this result to deduce an oracle inequality and
consistency of the classiﬁer on the sense of both ISE and probability of error. Our
results also specialize to give performance guarantees for an existing method of
L2 kernel density estimation.

1 Introduction

In the binary classiﬁcation problem we are given realizations (x1  y1)  . . .   (xn  yn) of a jointly
distributed pair (X  Y )  where X ∈ Rd is a pattern and Y ∈ {−1  +1} is a class label. The goal
of classiﬁcation is to build a classiﬁer  i.e.  a function taking X as input and outputting a label  such
that some measure of performance is optimized. Kernel classiﬁers [1] are an important family of
classiﬁers that have drawn much recent attention for their ability to represent nonlinear decision
boundaries and to scale well with increasing dimension d. A kernel classiﬁer (without offset) has
the form

(cid:41)

(cid:40)

n(cid:88)

g(x) = sign

αiyik(x  xi)

 

i=1

where αi are parameters and k is a kernel function. For example  support vector machines (SVMs)
without offset have this form [2]  as does the standard kernel density estimate (KDE) plug-in rule.
Recently Kim and Scott [3] introduced an L2 or integrated squared error (ISE) criterion to design the
coefﬁcients αi of a kernel classiﬁer with Gaussian kernel. Their L2 classiﬁer performs comparably
to existing kernel methods while possesing a number of desirable properties. Like the SVM  L2
kernel classiﬁers are the solutions of convex quadratic programs that can be solved efﬁciently using
standard decomposition algorithms.
In addition  the classiﬁers are sparse  meaning most of the
coefﬁcients αi = 0  which has advantages for representation and evaluation efﬁciency. Unlike
the SVM  however  there are no free parameters to be set by the user except the kernel bandwidth
parameter.
In this paper we develop statistical performance guarantees for the L2 kernel classiﬁer introduced
in [3]. The linchpin of our analysis is a new concentration inequality bounding the deviation of a
cross-validation based ISE estimate from the true ISE. This bound is then applied to prove an oracle
inequality and consistency in both ISE and probability of error. In addition  as a special case of

∗Both authors supported in part by NSF Grant CCF-0830490

1

our analysis  we are able to deduce performance guarantees for the method of L2 kernel density
estimation described in [4  5].
The ISE criterion has a long history in the literature on bandwidth selection for kernel density esti-
mation [6] and more recently in parametric estimation [7]. The use of ISE for optimizing the weights
of a KDE via quadratic programming was ﬁrst described in [4] and later rediscovered in [5]. In [8] 
an (cid:96)1 penalized ISE criterion was used to aggregate a ﬁnite number of pre-determined densities.
Linear and convex aggregation of densities  based on an L2 criterion  are studied in [9]  where the
densities are based on a ﬁnite dictionary or an independent sample. In contrast  our proposed method
allows data-adaptive kernels  and does not require and independent (holdout) sample.
In classiﬁcation  some connections relating SVMs and ISE are made in [10]  although no new algo-
rithms are proposed. Finally  the “difference of densities” perspective has been applied to classiﬁca-
tion in other settings by [11]  [12]  and [13]. In [11] and [13]  a difference of densities are used to
ﬁnd smoothing parameters or kernel bandwidths. In [12]  conditional densities are chosen among a
parameterized set of densities to maximize the average (bounded) density differences.
Section 2 reviews the L2 kernel classiﬁer  and presents a slight modiﬁcation needed for our analysis.
Our results are presented in Section 3. Conclusions are offered in the ﬁnal section  and proofs are
gathered in an appendix.

2 L2 Kernel Classiﬁcation

We review the previous work of Kim & Scott [3] and introduce an important modiﬁcation. For
convenience  we relabel Y so that it belongs to {1 −γ} and denote I+ = {i | Yi = +1} and I− =
{i | Yi = −γ}. Let f−(x) and f+(x) denote the class-conditional densities of the pattern given the
label. From decision theory  the optimal classiﬁer has the form

g∗(x) = sign{f+(x) − γf−(x)}  

(1)
where γ incorporates prior class probabilities and class-conditional error costs (in the Bayesian
setting) or a desired tradeoff between false positives and false negatives [14]. Denote the “difference
of densities” dγ(x) := f+(x) − γf−(x).
The class-conditional densities are modelled using the Gaussian kernel as

(cid:88)
(cid:98)f+ (x; α) =
α |

A =

i∈I+

with constraints α = (α1  . . .   αn) ∈ A where

The Gaussian kernel is deﬁned as

kσ (x  Xi) =

The ISE associated with α is

ISE (α) = (cid:107)(cid:98)dγ (x; α) − dγ (x)(cid:107)2

(cid:90) (cid:98)d2

=

γ (x; α) dx − 2

αikσ (x  Xi)  

αikσ (x  Xi)

(cid:98)f− (x; α) =

(cid:88)

i∈I−

(cid:88)
(cid:161)

i∈I+

αi =

2πσ2(cid:162)−d/2 exp

.

2σ2

i∈I−

(cid:88)

−(cid:107)x − Xi(cid:107)2

αi = 1  αi ≥ 0 ∀i

 .
(cid:190)
(cid:189)
(cid:90) (cid:179)(cid:98)dγ (x; α) − dγ (x)
(cid:90) (cid:98)dγ (x; α) dγ (x) dx +
(cid:90)
(cid:90) (cid:98)dγ (x; α) dγ (x) dx
(cid:90)

L2 =

γ (x; α) dx − 2Hn (α) +

γ (x) dx.
d2

(cid:180)2

dx

γ (x) dx.
d2

Since we do not know the true dγ (x)  we need to estimate the second term in the above equation

by Hn (α) which will be explained in detail in Section 2.1. Then  the empirical ISE is

(2)

(3)

H (α) (cid:44)

(cid:90) (cid:98)d2

(cid:100)ISE (α) =

2

Now (cid:98)α is deﬁned as

and the ﬁnal classiﬁer will be

2.1 Estimation of H (α)

(cid:98)α = arg min
(cid:40)

α∈A

(cid:100)ISE (α)
(cid:98)dγ (x;(cid:98)α) ≥ 0
(cid:98)dγ (x;(cid:98)α) < 0.

g (x) =

+1 
−γ 

(4)

In this section  we propose a method of estimating H (α) in (2). The basic idea is to view H (α) as
an expectation and estimate it using a sample average. In [3]  the resubstitution estimator for H (α)
was used. However  since this estimator is biased  we use a leave-one-out cross-validation (LOOCV)
estimator  which is unbiased and facilitates our theoretical analysis. Note that the difference of
densities can be expressed as

n(cid:88)

(cid:98)dγ (x; α) = (cid:98)f+ (x) − γ(cid:98)f− (x) =
(cid:90) (cid:98)dγ (x; α) dγ (x) dx =
(cid:90) n(cid:88)
(cid:90) n(cid:88)
n(cid:88)

αiYikσ (x  Xi) f+ (x) dx − γ

(cid:90) (cid:98)dγ (x; α) f+ (x) dx − γ

i=1

i=1

i=1

αiYikσ (x  Xi) .

(cid:90) (cid:98)dγ (x; α) f− (x) dx

αiYikσ (x  Xi) f− (x) dx

Then 

H (α) =

=

=

where

i=1

αiYih (Xi)

(cid:90)

h (Xi) (cid:44)

kσ (x  Xi) f+ (x) dx − γ

kσ (x  Xi) f− (x) dx.

(cid:88)



(cid:98)hi (cid:44)

(cid:88)

N+ − 1
1
N+

j∈I+

j∈I+ j(cid:54)=i
kσ (Xj  Xi) −

We estimate each h (Xi) in (5) for i = 1  . . .   n using leave-one-out cross-validation
i ∈ I+

kσ (Xj  Xi)  

1

kσ (Xj  Xi) − γ
N−

where N+ = |I+|   N− = |I−|. Then  the estimate of H (α) is Hn (α) =

i=1 αiYi

γ

N− − 1

j∈I− j(cid:54)=i

kσ (Xj  Xi)  

i ∈ I−

(cid:80)n

(5)

(cid:98)hi.

(cid:90)
(cid:88)
(cid:88)

j∈I−

The optimization problem (4) can be formulated as a quadratic program. The ﬁrst term in (3) is

2.2 Optimization

(cid:90) (cid:98)d2

γ (x; α) dx =

n(cid:88)

n(cid:88)

i=1

j=1

(cid:90) (cid:195)

n(cid:88)
(cid:90)

i=1

(cid:33)2

αiYikσ (x  Xi)

dx

=

αiαjYiYj

kσ (x  Xi) kσ (x  Xj) dx =

αiαjYiYjk√

n(cid:88)

n(cid:88)

i=1

j=1

(cid:80)n
2σ (Xi  Xj) − n(cid:88)

2σ (Xi  Xj)

(cid:98)hi. Finally  since

n(cid:88)

n(cid:88)

(cid:98)α = arg min

α∈A

1
2

by the convolution theorem for Gaussian kernels [15]. As we have seen in Section 2.1  the second
term Hn (α) in (3) is linear in α and can be expressed as
the third term does not depend on α  the optimization problem (4) becomes the following quadratic
program (QP)

i=1 αici where ci = Yi

αiαjYiYjk√

ciαi.

(6)

i=1

j=1

i=1

The QP (6) is similar to the dual QP of the 2-norm SVM with hinge loss [2] and can be solved by a
variant of the Sequential Minimal Optimization (SMO) algorithm [3].

3

3 Statistical performance analysis

In this section  we give theoretical performance analysis on our proposed method. We assume that
{Xi}i∈I+ and {Xi}i∈I− are i.i.d samples from f+ (x) and f− (x)  respectively  and treat N+ and
N− as deterministic variables n+ and n− such that n+ → ∞ and n− → ∞ as n → ∞.

3.1 Concentration inequality for Hn (α)

Lemma 1. Conditioned on Xi (cid:98)hi is an unbiased estimator of h (Xi)  i.e 

E

= h (Xi) .

(cid:105)

(cid:104)(cid:98)hi | Xi
(cid:190)

Furthermore  for any  > 0 

|Hn (α) − H (α)| > 

(cid:179)

e−c(n+−1)2 + e−c(n−−1)2(cid:180)

≤ 2n

(cid:189)

P

(cid:161)√

sup
α∈A

(cid:162)2d

2πσ

/ (1 + γ)4.

where c = 2
Lemma 1 implies that Hn (α) → H (α) almost surely for all α ∈ A simultaneously  provided that
σ  n+  and n− evolve as functions of n such that n+σ2d/ ln n → ∞ and n−σ2d/ ln n → ∞.

3.2 Oracle Inequality

Next  we establish on oracle inequality  which relates the performance of our estimator to that of the
best possible kernel classiﬁer.

(cid:179)

(cid:180)

(cid:161)√

(cid:162)2d

Theorem 1. Let  > 0 and set δ = δ () = 2n

2

2πσ

/ (1 + γ)4. Then  with probability at least 1 − δ

e−c(n+−1)2 + e−c(n−−1)2

where c =

ISE (α) + 4.

α∈A

ISE ((cid:98)α) ≤ inf
(cid:175)(cid:175)(cid:175)ISE (α) − (cid:100)ISE (α)

(cid:175)(cid:175)(cid:175) ≤ 2 

Proof. From Lemma 1  with probability at least 1 − δ

∀α ∈ A

for all α ∈ A  we have

by using the fact ISE (α)− (cid:100)ISE (α) = 2 (Hn (α) − H (α)). Then  with probability at least 1− δ 
ISE ((cid:98)α) ≤ (cid:100)ISE ((cid:98)α) + 2 ≤ (cid:100)ISE (α) + 2 ≤ ISE (α) + 4
where the second inequality holds from the deﬁnition of(cid:98)α. This proves the theorem.
Next  we have a theorem stating that ISE ((cid:98)α) converges to zero in probability.
σ → 0  n+σ2d/ ln n → ∞  and n−σ2d/ ln n → ∞  then ISE ((cid:98)α) → 0 in probability as n → ∞

Theorem 2. Suppose that for f = f+ and f−  the Hessian Hf (x) exists and each entry of Hf (x)
is piecewise continuous and square integrable. If σ  n+  and n− evolve as functions of n such that

ISE consistency

3.3

This result intuitively follows from the oracle inequality since the standard Parzen window density
estimate is consistent and uniform weights are among the simplex A. The rigorous proof is omitted
due to space limitations.

4

3.4 Bayes Error Consistency

In classiﬁcation  we are ultimately interested in minimizing the probability of error. Let us now
assume {Xi}n
i=1 is an i.i.d sample from f (x) = pf+ (x) + (1 − p)f− (x)  where 0 < p < 1 is
the prior probability of the positive class. The consistency with respect to the probability of error
could be easily shown if we set γ to γ∗ = 1−p
and apply Theorem 3 in [17]. However  since p is
unknown  we must estimate γ∗. Note that N+ and N− are binomial random variables  and we may
estimate γ∗ as γ = N−
. The next theorem says the L2 kernel classiﬁer is consistent with respect to
N+
the probability of error.
Theorem 3. Suppose that the assumptions in Theorem 2 are satisﬁed. In addition  suppose that
f− ∈ L2 (R)  i.e. (cid:107)f−(cid:107)L2 < ∞. Let γ = N−/N+ be an estimate of γ∗ = 1−p
p . If σ evolves as
a function of n such that σ → 0 and nσ2d/ ln n → ∞ as n → ∞  then the L2 kernel classiﬁer is
consistent. In other words  given training data Dn = ((X1  Y1)   . . .   (Xn  Yn))  the classiﬁcation
error

p

(cid:110)

(cid:110)(cid:98)dγ (X;(cid:98)α)

(cid:111)

Ln = P

sgn

(cid:111)

(cid:54)= Y | Dn

converges to the Bayes error L∗ in probability as n → ∞.

The proof is given in Appendix A.2.

3.5 Application to density estimation

By setting γ = 0  our goal becomes estimating f+ and we recover the L2 kernel density estimate
of [4  5] using leave-one-out cross-validation. Given an i.i.d sample X1  . . .   Xn from f (x)  the L2
kernel density estimate of f (x) is deﬁned as

n(cid:88)

(cid:98)f (x;(cid:98)α) =
(cid:98)αikσ (x  Xi)
 1
2σ (Xi  Xj) − n(cid:88)

αi

i=1

αiαjk√

n − 1

i=1

j=1

i=1

with(cid:98)αi’s optimized such that
n(cid:88)
n(cid:88)

(cid:98)α = arg min(cid:80)

1
2

αi=1
αi≥0

 .

kσ (Xi  Xj)

(cid:88)

j(cid:54)=i

Our concentration inequality  oracle inequality  and L2 consistency result immediately extend to
provide the same performance guarantees for this method. For example  we state the following
corollary.
Corollary 1. Suppose that the Hessian Hf (x) of a density function f (x) exists and each entry of
Hf (x) is piecewise continuous and square integrable. If σ → 0 and nσ2d/ ln n → ∞ as n → ∞ 
then

(cid:90) (cid:179)(cid:98)f (x;(cid:98)α) − f (x)

(cid:180)2

dx → 0

in probability.

4 Conclusion

Through the development of a novel concentration inequality  we have established statistical per-
formance guarantees on a recently introduced L2 kernel classiﬁer. We view the relatively clean
analysis of this classiﬁer as an attractive feature relative to other kernel methods. In future work  we
hope to invoke the full power of the oracle inequality to obtain adaptive rates of convergence  and
consistency for σ not necessarily tending to zero.

5

A Appendix

A.1 Proof of Lemma 1

Note that for any given i  (kσ (Xj  Xi))j(cid:54)=i are independent and bounded by M = 1/
For random vectors Z ∼ f+ (x) and W ∼ f− (x)  h (Xi) in (5) can be expressed as

(cid:161)√

2πσ

(cid:162)d.

h (Xi) = E [kσ (Z  Xi) | Xi] − γE [kσ (W  Xi) | Xi] .

Since Xi ∼ f+ (x) for i ∈ I+ and Xi ∼ f− (x) for i ∈ I−  it can be easily shown that

(cid:105)

(cid:104)(cid:98)hi | Xi

E

= h (Xi) .

(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:190)
(cid:88)

For i ∈ I+ 

P

(cid:189)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175) > 
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) γ
(cid:88)

n+ − 1

≤ P

+ P

1

n−

j∈I−

kσ (Xj  Xi) − E [kσ (Z  Xi) | Xi]

j∈I+ j(cid:54)=i
kσ (Xj  Xi) − γE [kσ (W  Xi) | Xi]

By Hoeffding’s inequality [16]  the ﬁrst term in (7) is

kσ (Xj  Xi) − (n+ − 1)E [kσ (Z  Xi) | Xi]

(cid:175)(cid:175)(cid:175)(cid:175) >

γ
1 + γ

(cid:175)(cid:175)(cid:175)(cid:175) >

(cid:190)

1 + γ



(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:190)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x

(cid:175)(cid:175)(cid:175)(cid:175) >

(7)

(cid:190)
(cid:190)

(cid:190)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x

kσ (Xj  Xi) − E

kσ (Xj  Xi) − E

kσ (Xj  Xi) | Xi

kσ (Xj  Xi) | Xi

(n+ − 1) 

1 + γ

(n+ − 1) 

1 + γ

(cid:183) (cid:88)
(cid:183) (cid:88)

j∈I+ j(cid:54)=i

j∈I+ j(cid:54)=i

(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)

P

j∈I+ j(cid:54)=i

= P

= P

j∈I+ j(cid:54)=i

j∈I+ j(cid:54)=i

(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175) (cid:88)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:175)(cid:88)

= P

j∈I−

j∈I−

(cid:189)(cid:175)(cid:175)(cid:175)(cid:175)(cid:88)
(cid:110)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)

≤ 2e−2(n+−1)2/(1+γ)2M 2

.

The second term in (7) is

P

kσ (Xj  Xi) − n−E [kσ (W  Xi) | Xi]

kσ (Xj  Xi) − E

(cid:175)(cid:175)(cid:175)(cid:175) >

n−
1 + γ
kσ (Xj  Xi) | Xi

(cid:183)(cid:88)

j∈I−

(cid:190)
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x

(cid:190)

≤ 2e−2n−2/(1+γ)2M 2 ≤ 2e−2(n−−1)2/(1+γ)2M 2

.

(cid:175)(cid:175)(cid:175) ≥ 
(cid:111)

(cid:183)

(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) ≥ 

Therefore 

P

P

= E
≤ 2e−2(n+−1)2/(1+γ)2M 2 + 2e−2(n−−1)2/(1+γ)2M 2

.

In a similar way  it can be shown that for i ∈ I− 

(cid:110)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)

(cid:175)(cid:175)(cid:175) > 

(cid:111)

P

≤ 2e−2(n+−1)2/(1+γ)2M 2 + 2e−2(n−−1)2/(1+γ)2M 2

.

6

1 + γ

(n+ − 1) 

(cid:184)(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:184)(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175)(cid:175) Xi = x
(cid:184)(cid:175)(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175)(cid:175) Xi = X

n−
1 + γ

(cid:190)(cid:184)

(cid:189)

Then 

P

sup
α∈A
≤ P

= P

≤ P

= P

= P

≤

i∈I+
≤ n+

= n



αiγ

αiYi

i=1

αiγ

i=1

i∈I+





i∈I+

+ P

+ P

i∈I−

i∈I−

αi

αi

= P

1 + γ

(cid:41)

max
i∈I−

max
i∈I+

sup
α∈A

sup
α∈A

sup
α∈A

sup
α∈A

sup
α∈A

αi |Yi|

1 + γ


|Hn (α) − H (α)| > 

(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) n(cid:88)
(cid:180)(cid:175)(cid:175)(cid:175)(cid:175)(cid:175) > 
(cid:40)
(cid:190)
(cid:179)(cid:98)hi − h (Xi)
(cid:40)
(cid:41)
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) > 
n(cid:88)
(cid:189)
(cid:190)
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) +
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) > 
n(cid:88)
n(cid:88)
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)
(cid:189)
(cid:189)
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) >
n(cid:88)
n(cid:88)
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:189)
(cid:190)
(cid:190)
(cid:189)
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >
(cid:190)(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:190)
(cid:189)(cid:91)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:189)(cid:91)
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:175)(cid:175)(cid:175)(cid:175) B
(cid:190)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:190)
(cid:189)(cid:175)(cid:175)(cid:175)(cid:98)hi − h (Xi)
(cid:175)(cid:175)(cid:175) >
(cid:175)(cid:175)(cid:175) >
(cid:88)
(cid:88)
2e−2(n+−1)2/(1+γ)4M 2 + 2e−2(n−−1)2/(1+γ)4M 2(cid:180)
(cid:179)
(cid:179)
2e−2(n+−1)2/(1+γ)4M 2 + 2e−2(n−−1)2/(1+γ)4M 2(cid:180)
2e−2(n+−1)2/(1+γ)4M 2 + 2e−2(n−−1)2/(1+γ)4M 2(cid:180)
(cid:180)2

(cid:107)(cid:98)dγ (x;(cid:98)α) − dγ∗ (x)(cid:107)L2 = (cid:107)(cid:98)dγ (x;(cid:98)α) − dγ (x) + (γ − γ∗) f− (x)(cid:107)L2

(cid:90) (cid:179)(cid:98)dγ (x;(cid:98)α) − dγ∗ (x)
≤ (cid:107)(cid:98)dγ (x;(cid:98)α) − dγ (x)(cid:107)L2 + (cid:107) (γ − γ∗) f− (x)(cid:107)L2
(cid:112)
ISE ((cid:98)α) + |γ − γ∗| · (cid:107)f− (x)(cid:107)L2 

dx → 0

+ n−

1 + γ

1 + γ

1 + γ

1 + γ

1 + γ

(cid:179)

i∈I−

i∈I−

+ P

+

P

i∈I+

P







.

A.2 Proof of Theorem 3

From Theorem 3 in [17]  it sufﬁces to show that

in probability. Since from the triangle inequality

(cid:190)

(cid:175)(cid:175)(cid:175)(cid:175) B

γ
1 + γ

(cid:190)

(cid:169)

=

we need to show that ISE ((cid:98)α) and γ converges in probability to 0 and γ∗  respectively. The con-
In the previous analyses  we have shown the convergence of ISE ((cid:98)α) by treating N+  N− and γ

vergence of γ to γ∗ can be easily shown from the strong law of large numbers.

as deterministic variables but now we turn to the case where these variables are random. Deﬁne an
event D =

. For any  > 0 

  γ ≤ 2γ∗

(cid:169)

2

(cid:169)

+ P

N+ ≥ np

(cid:110)
2   N− ≥ n(1−p)
P{ISE ((cid:98)α) > } ≤ P
(cid:175)(cid:175) n+ ≥ np
(cid:170)
ISE ((cid:98)α) >   D
2   n− ≥ n(1−p)
(cid:88)
(cid:169)
ISE ((cid:98)α) >   D
(cid:88)
(cid:169)
ISE ((cid:98)α) > 
(cid:169)
ISE ((cid:98)α) > 

(cid:111)
Dc(cid:170)
(cid:169)
≤ 2γ∗(cid:170)
(cid:175)(cid:175) N+ = n+  N− = n−
(cid:175)(cid:175) N+ = n+  N− = n−
(cid:175)(cid:175) N+ = n+  N− = n−

(n+ n−)∈S

  n−
n+

max

P

P

P

2

=

=

≤

.

ISE ((cid:98)α) >   D
(cid:170) · P{N+ = n+  N− = n−}
(cid:170) · P{N+ = n+  N− = n−}
(cid:170)

The ﬁrst term converges to 0 from the strong law of large numbers. Let deﬁne a set S =
(n+  n−)

. Then 

(cid:170)

(8)

P

.

(n+ n−)∈S

7

Provided that σ → 0 and nσ2d/ ln n → ∞  any pair (n+  n−) ∈ S satisﬁes σ → 0  n+σ2d/ ln n →
∞  and n−σ2d/ ln n → ∞ as n → ∞ and thus the term in (8) converges to 0 from Theorem 2. This
proves the theorem.

References
[1] B. Sch¨olkopf and A. J. Smola  Learning with Kernels  MIT Press  Cambridge  MA  2002.
[2] C. Cortes and V. Vapnik  “Support-vector networks ” Machine Learning  vol. 20  no. 3  pp. 273–297 

1995.

[3] J. Kim and C. Scott  “Kernel classiﬁcation via integrated squared error ” IEEE Workshop on Statistical

Signal Processing  August 2007.

[4] D. Kim  Least Squares Mixture Decomposition Estimation  unpublished doctoral dissertation  Dept. of

Statistics  Virginia Polytechnic Inst. and State Univ.  1995.

[5] Mark Girolami and Chao He  “Probability density estimation from optimally condensed data samples ”
IEEE Transactions on Pattern Analysis and Machine Intelligence  vol. 25  no. 10  pp. 1253–1264  OCT
2003.

[6] B.A. Turlach  “Bandwidth selection in kernel density estimation: A review ” Technical Report 9317 

C.O.R.E. and Institut de Statistique  Universit´e Catholique de Louvain  1993.

[7] David W.Scott  “Parametric statistical modeling by minimum integrated square error ” Technometrics 43 

pp. 274–285  2001.

[8] A.B. Tsybakov F. Bunea and M.H. Wegkamp  “Sparse density estimation with l1 penalties ” Proceedings
of 20th Annual Conference on Learning Theory  COLT 2007  Lecture Notes in Artiﬁcial Intelligence 
v4539  pp. 530– 543  2007.

[9] Ph. Rigollet and A.B. Tsybakov  “Linear and convex aggregation of density estimators ” https://

hal.ccsd.cnrs.fr/ccsd-00068216  2004.

[10] Robert Jenssen  Deniz Erdogmus  Jose C.Principe  and Torbjørn Eltoft  “Towards a uniﬁcation of infor-
mation theoretic learning and kernel method ” in Proc. IEEE Workshop on Machine Learning for Signal
Processing (MLSP2004)  Sao Luis  Brazil.

[11] Peter Hall and Matthew P.Wand  “On nonparametric discrimination using density differeces ” Biometrika 

vol. 75  no. 3  pp. 541–547  Sept 1988.

[12] P. Meinicke  T. Twellmann  and H. Ritter  “Discriminative densities from maximum contrast estimation ”

in Advances in Neural Information Proceeding Systems 15  Vancouver  Canada  2002  pp. 985–992.

[13] M. Di Marzio and C.C. Taylor  “Kernel density classiﬁcation and boosting: an l2 analysis ” Statistics and

Computing  vol. 15  pp. 113–123(11)  April 2005.

[14] E. Lehmann  Testing statistical hypotheses  Wiley  New York  1986.
[15] M.P. Wand and M.C. Jones  Kernel Smoothing  Chapman & Hall  1995.
[16] L. Devroye and G. Lugosi  “Combinatorial methods in density estimation ” 2001.
[17] Charles T. Wolverton and Terry J. Wagner  “Asymptotically optimal discriminant fucntions for pattern

classiﬁcation ” IEEE Trans. Info. Theory  vol. 15  no. 2  pp. 258–265  Mar 1969.

8

,Hastagiri Vanchinathan
Gábor Bartók
Andreas Krause
Ming Lin
Jieping Ye
Isabel Valera
Adish Singla
Manuel Gomez Rodriguez