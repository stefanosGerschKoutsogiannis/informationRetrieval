2013,Understanding Dropout,Dropout is a relatively new algorithm for training neural networks which relies on stochastically dropping out'' neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections  with arbitrary probability values  and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks  the averaging properties of dropout are characterized by three recursive equations  including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. We also show in simple cases how dropout performs stochastic gradient descent on a regularized error function.",Understanding Dropout

Pierre Baldi

Department of Computer Science
University of California  Irvine

Irvine  CA 92697

pfbaldi@uci.edu

Peter Sadowski

Department of Computer Science
University of California  Irvine

Irvine  CA 92697

pjsadows@ics.uci.edu

Abstract

Dropout is a relatively new algorithm for training neural networks which relies
on stochastically “dropping out” neurons during training in order to avoid the
co-adaptation of feature detectors. We introduce a general formalism for study-
ing dropout on either units or connections  with arbitrary probability values  and
use it to analyze the averaging and regularizing properties of dropout in both lin-
ear and non-linear networks. For deep neural networks  the averaging properties
of dropout are characterized by three recursive equations  including the approx-
imation of expectations by normalized weighted geometric means. We provide
estimates and bounds for these approximations and corroborate the results with
simulations. Among other results  we also show how dropout performs stochastic
gradient descent on a regularized error function.

1

Introduction

Dropout is an algorithm for training neural networks that was described at NIPS 2012 [7]. In its
most simple form  during training  at each example presentation  feature detectors are deleted with
probability q = 1 − p = 0.5 and the remaining weights are trained by backpropagation. All weights
are shared across all example presentations. During prediction  the weights are divided by two.
The main motivation behind the algorithm is to prevent the co-adaptation of feature detectors  or
overﬁtting  by forcing neurons to be robust and rely on population behavior  rather than on the
activity of other speciﬁc units. In [7]  dropout is reported to achieve state-of-the-art performance on
several benchmark datasets. It is also noted that for a single logistic unit dropout performs a kind of
“geometric averaging” over the ensemble of possible subnetworks  and conjectured that something
similar may occur also in multilayer networks leading to the view that dropout may be an economical
approximation to training and using a very large ensemble of networks.
In spite of the impressive results that have been reported  little is known about dropout from a
theoretical standpoint  in particular about its averaging  regularization  and convergence properties.
Likewise little is known about the importance of using q = 0.5  whether different values of q can
be used including different values for different layers or different units  and whether dropout can be
applied to the connections rather than the units. Here we address these questions.

2 Dropout in Linear Networks

It is instructive to ﬁrst look at some of the properties of dropout in linear networks  since these can
be studied exactly in the most general setting of a multilayer feedforward network described by an
underlying acyclic graph. The activity in unit i of layer h can be expressed as:

Sh

i (I) =

whl

ij Sl

j with S0

j = Ij

(1)

(cid:88)

(cid:88)

l<h

j

1

where the variables w denote the weights and I the input vector. Dropout applied to the units can be
expressed in the form

Sh

i =

whl

ij δl

jSl

j with S0

j = Ij

(2)

(cid:88)

(cid:88)

l<h

j

(cid:88)

(cid:88)

l<h

j

(cid:88)

(cid:88)

j is a gating 0-1 Bernoulli variable  with P (δl

j . Throughout this paper we assume
where δl
that the variables δl
j are independent of each other  independent of the weights  and independent of
the activity of the units. Similarly  dropout applied to the connections leads to the random variables

j = 1) = pl

Sh

i =

δhl
ij whl

ij Sl

j with S0

j = Ij

(3)

For brevity in the rest of this paper  we focus exclusively on dropout applied to the units  but all the
results remain true for the case of dropout applied to the connections with minor adjustments.
For a ﬁxed input vector  the expectation of the activity of all the units  taken over all possible real-
izations of the gating variables hence all possible subnetworks  is given by:

E(Sh

i ) =

whl

ij pl

jE(Sl
j)

for h > 0

(4)

with E(S0
feedforward propagation in the original network  simply replacing the weights whl

j ) = Ij in the input layer. In short  the ensemble average can easily be computed by

ij by whl

j.
ij pl

l<h

j

3 Dropout in Neural Networks

3.1 Dropout in Shallow Neural Networks

1 wjIj.
To achieve the greatest level of generality  we assume that the unit produces different outputs
O1  . . .   Om  corresponding to different sums S1 . . .   Sm with different probabilities P1  . . .   Pm
In the most relevant case  these outputs and these sums are associated with the
m = 2n possible subnetworks of the unit. The probabilities P1  . . .   Pm could be generated  for
instance  by using Bernoulli gating variables  although this is not necessary for this derivation. It is

Consider ﬁrst a single logistic unit with n inputs O = σ(S) = 1/(1 + ce−λS) and S =(cid:80)n
((cid:80) Pm = 1).
useful to deﬁne the following four quantities: the mean E =(cid:80) PiOi; the mean of the complements
E(cid:48) = (cid:80) Pi(1 − Oi) = 1 − E; the weighted geometric mean (W GM) G = (cid:81)
weighted geometric mean of the complements G(cid:48) =(cid:81)

; and the
i(1 − Oi)Pi. We also deﬁne the normalized
weighted geometric mean N W GM = G/(G + G(cid:48)). We can now prove the key averaging theorem
for logistic functions:

i OPi

i

N W GM (O1  . . .   Om) =

1

1 + ce−λE(S)

= σ(E(S))

To prove this result  we write

1

1

=

1 +

N W GM (O1  . . .   Om) =

The logistic function satisﬁes the identity [1 − σ(x)]/σ(x) = ce−λx and thus

(cid:81)(1−σ(Si))Pi
(cid:81) σ(Si)Pi
1 + ce−λ(cid:80) PiSi
dropout conﬁgurations by simple forward propagation by: N W GM = σ((cid:80)n

(cid:81)(1−Oi)Pi
(cid:81) OPi
1 +(cid:81)[ce−λSi]Pi

N W GM (O1  . . .   Om) =

Thus in the case of Bernoulli gating variables  we can compute the N W GM over all possible
1 wjpjIj). A similar
result is true also for normalized exponential transfer functions. Finally  one can also show that
the only class of functions f that satisfy N W GM (f ) = f (E) are the constant functions and the
logistic functions [1].

= σ(E(S))

1 +

=

1

1

i

(5)

(6)

(7)

2

3.2 Dropout in Deep Neural Networks

We can now deal with the most interesting case of deep feedforward networks of sigmoidal units 1 
described by a set of equations of the form

(cid:88)

(cid:88)

l<h

j

(cid:88)

(cid:88)

l<h

j

(8)

(9)

Oh

i = σ(Sh

i ) = σ(

whl

ij Ol

j) with O0

j = Ij

where Oh

i is the output of unit i in layer h. Dropout on the units can be described by

Oh

i = σ(Sh

i ) = σ(

whl

ij δl

jOl

j) with O0

j = Ij

using the Bernoulli selector variables δl

j. For each sigmoidal unit

(cid:81)N (Oh
i )P (N ) +(cid:81)N (1 − Oh

i )P (N )

(cid:81)N (Oh

i ) =

N W GM (Oh

(10)
where N ranges over all possible subnetworks. Assume for now that the N W GM provides a
good approximation to the expectation (this point will be analyzed in the next section). Then the
averaging properties of dropout are described by the following three recursive equations. First the
approximation of means by NWGMs:

i )P (N )

E(Oh

i ) ≈ N W GM (Oh
i )

Second  using the result of the previous section  the propagation of expectation symbols:

(cid:2)E(Sh
i )(cid:3)

N W GM (Oh

i ) = σh
i

(11)

(12)

And third  using the linearity of the expectation with respect to sums  and to products of independent
random variables:

E(Sh

i ) =

whl

ij pl

jE(Ol
j)

(13)

l<h

j

Equations 11  12  and 13 are the fundamental equations explaining the averaging properties of the
dropout procedure. The only approximation is of course Equation 11 which is analyzed in the next
section. If the network contains linear units  then Equation 11 is not necessary for those units and
their average can be computed exactly. In the case of regression with linear units in the top layers 
this allows one to shave off one layer of approximations. The same is true in binary classiﬁcation
by requiring the output layer to compute directly the N W GM of the ensemble rather than the
expectation. It can be shown that for any error function that is convex up (∪)  the error of the mean 
weighted geometric mean  and normalized weighted geometric mean of an ensemble is always less
than the expected error of the models [1].
Equation 11 is exact if and only if the numbers Oh
Thus it is useful to measure the consistency C(Oh

i (I)(cid:3) taken over all subnetworks N and their distribution when the input I is

i are identical over all possible subnetworks N .
i   I) of neuron i in layer h for input I by using

the variance V ar(cid:2)Oh

ﬁxed. The larger the variance is  the less consistent the neuron is  and the worse we can expect
the approximation in Equation 11 to be. Note that for a random variable O in [0 1] the variance
cannot exceed 1/4 anyway. This is because V ar(O) = E(O2) − (E(O))2 ≤ E(O) − (E(O))2 =
E(O)(1 − E(O)) ≤ 1/4. This measure can also be averaged over a training set or a test set.

1Given the results of the previous sections  the network can also include linear units or normalized expo-

nential units.

3

(cid:88)

(cid:88)

4 The Dropout Approximation

Given a set of numbers O1  . . .   Om between 0 and 1  with probabilities P1  . . .   PM (corresponding
to the outputs of a sigmoidal neuron for a ﬁxed input and different subnetworks)  we are primarily
interested in the approximation of E by N W GM. The N W GM provides a good approximation
because we show below that to a ﬁrst order of approximation: E ≈ N W GM and E ≈ G. Further-
more  there are formulae in the literature for bounding the error E − G in terms of the consistency
(e.g. the Cartwright and Field inequality [6]). However  one can suspect that the N W GM provides
even a better approximation to E than the geometric mean. For instance  if the numbers Oi satisfy
0 < Oi ≤ 0.5 (consistently low)  then

G

G(cid:48) ≤ E
E(cid:48)

and therefore G ≤ G

(14)
This is proven by applying Jensen’s inequality to the function ln x − ln(1 − x) for x ∈ (0  0.5]. It is
also known as the Ky Fan inequality [2  8  9].
To get even better results  one must consider a second order approximation. For this  we write
Oi = 0.5 + i with 0 ≤ |i| ≤ 0.5. Thus we have E(O) = 0.5 + E() and V ar(O) = V ar().
Using a Taylor expansion:

G + G(cid:48) ≤ E

(cid:89)

∞(cid:88)

i

n=0

1
2

G =

(cid:18)pi

(cid:19)

n

(2i)n =

1
2

1 +

(cid:88)

i

pi2i +

(cid:88)

i

pi(pi − 1)

2

(2i)2 +

(cid:88)

i<j



4pipjij + R3(i)

(cid:18)pi

(cid:19)

(2i)3

(1 + ui)3−pi

(15)

(16)

pi2

i +R3() =

1
2

+E()−V ar()+R3() = E(O)−V ar(O)+R3()

where R3(i) is the remainder and

3
where |ui| ≤ 2|i|. Expanding the product gives

R3(i) =

(cid:88)

G =

1
2

+

pii+(

i

i

By symmetry  we have

(cid:88)

i)2−(cid:88)
(cid:89)

(17)

(18)

G(cid:48) =

(1 − Oi)pi = 1 − E(O) − V ar(O) + R3()

i

where R3() is the higher order remainder. Neglecting the remainder and writing E = E(O) and
V = V ar(O) we have

G

G + G(cid:48) ≈ E − V
1 − 2V

G + G(cid:48) ≈ 1 − E − V
G(cid:48)
1 − 2V

and

(19)

Thus  to a second order  the differences between the mean and the geometric mean and the normal-
ized geometric means satisfy

E − G ≈ V

and E − G

G + G(cid:48) ≈ V (1 − 2E)
1 − 2V

(20)

and

(21)
Finally it is easy to check that the factor (1− 2E)/(1− 2V ) is always less or equal to 1. In addition
we always have V ≤ E(1 − E)  with equality achieved only for 0-1 Bernoulli variables. Thus

and (1 − E) − G(cid:48)

G + G(cid:48) ≈ V (1 − 2E)
1 − 2V

1 − E − G(cid:48) ≈ V

4

|E − G

G + G(cid:48)| ≈ V |1 − 2E|
1 − 2V

≤ E(1 − E)|1 − 2E|

1 − 2V

≤ 2E(1 − E)|1 − 2E|

(22)

The ﬁrst inequality is optimal in the sense that it is attained in the case of a Bernoulli variable
with expectation E and  intuitively  the second inequality shows that the approximation error is
always small  regardless of whether E is close to 0  0.5  or 1. In short  the NWGM provides a
very good approximation to E  better than the geometric mean G. The property is always true to
a second order of approximation and it is exact when the activities are consistently low  or when
N W GM ≤ E  since the latter implies G ≤ N W GM ≤ E. Several additional properties of the
dropout approximation  including the extension to rectiﬁed linear units and other transfer functions 
are studied in [1].

5 Dropout Dynamics

Dropout performs gradient descent on-line with respect to both the training examples and the en-
semble of all possible subnetworks. As such  and with the appropriately decreasing learning rates 
it is almost surely convergent like other forms of stochastic gradient descent [11  4  5]. To further
understand the properties of dropout  it is again instructive to look at the properties of the gradient
in the linear case.

5.1 Single Linear Unit

In the case of a single linear unit  consider the two error functions EEN S and ED associated with
the ensemble of all possible subnetworks and the network with dropout. For a single input I  these
are deﬁned by:

EEN S =

ED =

1
2

1
2

(t − OEN S)2 =

piwiIi)2

(t − OD)2 =

1
2

δiwiIi)2

1
2

(t − n(cid:88)
(t − n(cid:88)

i=1

i=1

(23)

(24)

We use a single training input I for notational simplicity  otherwise the errors of each training
example can be combined additively. The learning gradient is given by

∂EEN S

∂wi

= −(t − OEN S)

∂OEN S

∂wi

= −(t − OEN S)piIi

∂ED
∂wi

= −(t − OD)

∂OD
∂wi

= −(t − OD)δiIi = −tλiIi + wiδ2

i I 2

i +

(25)

wjδiδjIiIj

(26)

(cid:88)

j(cid:54)=i

(cid:18) ∂ED

(cid:19)

∂wi

E

The dropout gradient is a random variable and we can take its expectation. A short calculation yields

=

∂EEN S

∂wi

+ wipi(1 − pi)I 2

i

∂EEN S

∂wi

+ wiI 2

i V ar(δi)

(27)

Thus  remarkably  in this case the expectation of the gradient with dropout is the gradient of the
regularized ensemble error

E = EEN S +

1
2

n(cid:88)

i=1

w2

i I 2

i V ar(δi)

(28)

The regularization term is the usual weight decay or Gaussian prior term based on the square of the
weights to prevent overﬁtting. Dropout provides immediately the magnitude of the regularization
term which is adaptively scaled by the inputs and by the variance of the dropout variables. Note that
pi = 0.5 is the value that provides the highest level of regularization.

5

5.2 Single Sigmoidal Unit
The previous result generalizes to a sigmoidal unit O = σ(S) = 1/(1 + ce−λS) trained to minimize
the relative entropy error E = −(t log O + (1 − t) log(1 − O)). In this case 

(29)
The terms O and Ii are not independent but using a Taylor expansion with the N W GM approxi-
mation gives

= −λ(t − O)

= −λ(t − O)δiIi

∂ED
∂wi

∂S
∂wi

(cid:18) ∂ED

(cid:19)

∂wi

E

with SEN S =(cid:80)

≈ ∂EEN S
∂wi

+ λσ(cid:48)(SEN S)wiI 2

i V ar(δi)

(30)

j wjpjIj. Thus  as in the linear case  the expectation of the dropout gradient is ap-
proximately the gradient of the ensemble network regularized by weight decay terms with the proper
adaptive coefﬁcients. A similar analysis  can be carried also for a set of normalized exponential
units and for deeper networks [1].

5.3 Learning Phases and Sparse Coding

(cid:88)

During dropout learning  we can expect three learning phases: (1) At the beginning of learning  when
the weights are typically small and random  the total input to each unit is close to 0 for all the units
and the consistency is high: the output of the units remains roughly constant across subnetworks
(and equal to 0.5 with c = 1). (2) As learning progresses  activities tend to move towards 0 or 1
and the consistency decreases  i.e. for a given input the variance of the units across subnetworks
increases. (3) As the stochastic gradient learning procedure converges  the consistency of the units
converges to a stable value.
Finally  for simplicity  assume that dropout is applied only in layer h where the units have an output
of the form Oh
j is a constant since dropout
is not applied to layer l. Thus

i =(cid:80)

j. For a ﬁxed input  Ol

i ) and Sh

i = σ(Sh

l<h whl

ij δl

jOl

V ar(Sh

i ) =

(whl

ij )2(Ol

j)2pl

j(1 − pl
j)

(31)

l<h

under the usual assumption that the selector variables δl
j are independent of each other. Thus
i ) depends on three factors. Everything else being equal  it is reduced by: (1) Small weights
V ar(Sh
which goes together with the regularizing effect of dropout; (2) Small activities  which shows that
dropout is not symmetric with respect to small or large activities. Overall  dropout tends to favor
small activities and thus sparse coding; and (3) Small (close to 0) or large (close to 1) values of the
dropout probabilities pl
j = 0.5 maximize the regularization effect but may also lead
to slower convergence to the consistent state. Additional results and simulations are given in [1].

j. Thus values pl

6 Simulation Results

We use Monte Carlo simulation to partially investigate the approximation framework embodied by
the three fundamental dropout equations 11  12  and 13  the accuracy of the second-order approxi-
mation and bounds in Equations 20 and 22  and the dynamics of dropout learning. We experiment
with an MNIST classiﬁer of four hidden layers (784-1200-1200-1200-1200-10) that replicates the
results in [7] using the Pylearn2 and Theano software libraries[12  3]. The network is trained with
a dropout probability of 0.8 in the input  and 0.5 in the four hidden layers. For ﬁxed weights and
a ﬁxed input  10 000 Monte Carlo simulations are used to estimate the distribution of activity O
in each neuron. Let O∗ be the activation under the deterministic setting with the weights scaled
appropriately.
The left column of Figure 1 conﬁrms empirically that the second-order approximation in Equation
20 and the bound in Equation 22 are accurate. The right column of Figure 1 shows the difference be-
tween the true ensemble average E(O) and the prediction-time neuron activity O∗. This difference
grows very slowly in the higher layers  and only for active neurons.

6

Figure 1: Left: The difference E(O) − N W GM (O)  it’s second-order approximation in Equation
20  and the bound from Equation 22  plotted for four hidden layers and a typical ﬁxed input. Right:
The difference between the true ensemble average E(O) and the ﬁnal neuron prediction O∗.

Next  we examine the neuron consistency during dropout training. Figure 2a shows the three phases
of learning for a typical neuron. In Figure 2b  we observe that the consistency does not decline in
higher layers of the network.
One clue into how this happens is the distribution of neuron activity. As noted in [10] and section 5
above  dropout training results in sparse activity in the hidden layers (Figure 3). This increases the
consistency of neurons in the next layer.

7

(a) The three phases of learning. For a particu-
lar input  a typical active neuron (red) starts out
with low variance  experiences a large increase in
variance during learning  and eventually settles to
some steady constant value. In contrast  a typical
inactive neuron (blue) quickly learns to stay silent.
Shown are the mean with 5% and 95% percentiles.

(b) Consistency does not noticeably decline in the up-
per layers. Shown here are the mean Std(O) for active
neurons (0.1 < O after training) in each layer  along
with the 5% and 95% percentiles.

Figure 2

Figure 3: In every hidden layer of a dropout trained network  the distribution of neuron activations
O∗ is sparse and not symmetric. These histograms were totalled over a set of 100 random inputs.

8

References
[1] P. Baldi and P. Sadowski. The Dropout Learning Algorithm. Artiﬁcial Intelligence  2014. In

press.

[2] E. F. Beckenbach and R. Bellman. Inequalities. Springer-Verlag Berlin  1965.
[3] J. Bergstra  O. Breuleux  F. Bastien  P. Lamblin  R. Pascanu  G. Desjardins  J. Turian 
D. Warde-Farley  and Y. Bengio. Theano: a CPU and GPU math expression compiler.
In
Proceedings of the Python for Scientiﬁc Computing Conference (SciPy)  Austin  TX  June
2010. Oral Presentation.

[4] L. Bottou. Online algorithms and stochastic approximations. In D. Saad  editor  Online Learn-

ing and Neural Networks. Cambridge University Press  Cambridge  UK  1998.

[5] L. Bottou. Stochastic learning. In O. Bousquet and U. von Luxburg  editors  Advanced Lectures
on Machine Learning  Lecture Notes in Artiﬁcial Intelligence  LNAI 3176  pages 146–168.
Springer Verlag  Berlin  2004.

[6] D. Cartwright and M. Field. A reﬁnement of the arithmetic mean-geometric mean inequality.

Proceedings of the American Mathematical Society  pages 36–38  1978.

[7] G. Hinton  N. Srivastava  A. Krizhevsky  I. Sutskever  and R. R. Salakhutdinov. Improving neu-
ral networks by preventing co-adaptation of feature detectors. http://arxiv.org/abs/1207.0580 
2012.

[8] E. Neuman and J. S´andor. On the Ky Fan inequality and related inequalities i. MATHEMATI-

CAL INEQUALITIES AND APPLICATIONS  5:49–56  2002.

[9] E. Neuman and J. Sandor. On the Ky Fan inequality and related inequalities ii. Bulletin of the

Australian Mathematical Society  72(1):87–108  2005.

[10] S. Nitish.

Improving Neural Networks with Dropout. PhD thesis  University of Toronto 

Toronto  Canada  2013.

[11] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartin-

gales and some applications. Optimizing methods in statistics  pages 233–257  1971.

[12] D. Warde-Farley  I. Goodfellow  P. Lamblin  G. Desjardins  F. Bastien  and Y. Bengio.

pylearn2. 2011. http://deeplearning.net/software/pylearn2.

9

,Pierre Baldi
Peter Sadowski
Jie Hu
Rongrong Ji
ShengChuan Zhang
Xiaoshuai Sun
Qixiang Ye
Chia-Wen Lin
Qi Tian