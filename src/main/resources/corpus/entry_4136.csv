2011,Optimal learning rates for least squares SVMs using Gaussian kernels,We prove a new oracle inequality for support vector machines with Gaussian RBF kernels solving the regularized least squares regression problem. To this end  we apply the modulus of smoothness. With the help of the new oracle inequality we then derive learning rates that can also be achieved by a simple data-dependent parameter selection method. Finally  it turns out that our learning rates are asymptotically optimal for regression functions satisfying certain standard smoothness conditions.,Optimal learning rates for least squares SVMs using

Gaussian kernels

M. Eberts  I. Steinwart

Institute for Stochastics and Applications

University of Stuttgart

D-70569 Stuttgart

{eberts ingo.steinwart}@mathematik.uni-stuttgart.de

Abstract

We prove a new oracle inequality for support vector machines with Gaussian RBF
kernels solving the regularized least squares regression problem. To this end  we
apply the modulus of smoothness. With the help of the new oracle inequality we
then derive learning rates that can also be achieved by a simple data-dependent
parameter selection method. Finally  it turns out that our learning rates are asymp-
totically optimal for regression functions satisfying certain standard smoothness
conditions.

1

Introduction

On the basis of i.i.d. observations D := ((x1  y1)   . . .   (xn  yn)) of input/output observations drawn
from an unknown distribution P on X ⇥ Y   where Y ⇢ R  the goal of non-parametric least squares
regression is to ﬁnd a function fD : X ! R such that  for the least squares loss L : Y ⇥R ! [0 1)
deﬁned by L (y  t) = (y  t)2  the risk

RL P (fD) :=ZX⇥Y

L (y  fD (x)) dP (x  y) =ZX⇥Y
is small. This means RL P (fD) has to be close to the optimal risk

(y  fD (x))2 dP (x  y)

R⇤L P := inf {RL P (f) | f : X ! R measureable}  

called the Bayes risk with respect to P and L. It is well known that the function f⇤L P : X ! R
deﬁned by f⇤L P (x) = EP (Y |x)  x 2 X  is the only function for which the Bayes risk is attained.
Furthermore  some simple transformations show

(1)

RL P (f) R ⇤L P =ZXf  f⇤L P2 dPX =f  f⇤L P2

L2(PX )  

where PX is the marginal distribution of P on X.
In this paper  we assume that X ⇢ Rd is a non-empty  open and bounded set such that its boundary
@X has Lebesgue measure 0  Y := [M  M] for some M > 0 and P is a probability measure on
X⇥Y such that PX is the uniform distribution on X. In Section 2 we also discuss that this condition
can easily be generalized by assuming that PX on X is absolutely continuous with respect to the
Lebesgue measure on X such that the corresponding density of PX is bounded away from 0 and 1.
Recall that because of the ﬁrst assumption  it sufﬁces to restrict considerations to decision functions

f : X ! [M  M]. To be more precise  if  we denote the clipped value of some t 2 R byÛt  that is

M if t < M
t
M if t > M  

if t 2 [M  M]

Ût :=8<:

1

then it is easy to check that

for all f : X ! R.
The non-parametric least squares problem can be solved in many ways. Several of them are e.g. de-
scribed in [1]. In this paper  we use SVMs to ﬁnd a solution for the non-parametric least squares
problem by solving the regularized problem

RL P(Ûf ) R L P (f)  

fD  = arg min
f2H

kfk2

H + RL D (f) .

(2)

Here  > 0 is a ﬁxed real number  H is a reproducing kernel Hilbert space (RKHS) over X  and
RL D (f) is the empirical risk of f  that is

In this work we restrict our considerations to Gaussian RBF kernels k on X  which are deﬁned by

L (yi  f (xi)) .

1
n

RL D (f) =

nXi=1
k (x  x0) = exp kx  x0k2

2

2

!  

x  x0 2 X  

for some width  2 (0  1]. Our goal is to deduce asymptotically optimal learning rates for the SVMs
(2) using the RKHS H of k. To this end  we ﬁrst establish a general oracle inequality. Based on
this oracle inequality  we then derive learning rates if the regression function is contained in some
Besov space. It will turn out  that these learning rates are asymptotically optimal. Finally  we show
that these rates can be achieved by a simple data-dependent parameter selection method based on a
hold-out set.
The rest of this paper is organized as follows: The next section presents the main theorems and as a
consequence of these theorems some corollaries inducing asymptotically optimal learning rates for
regression functions contained in Sobolev or Besov spaces. Section 3 states some  for the proof of
the main statement necessary  lemmata and a version of [2  Theorem 7.23] applied to our special
case as well as the proof of the main theorem. Some further proofs and additional technical results
can be found in the appendix.

2 Results

In this section we present our main results including the optimal rates for LS-SVMs using Gaussian
kernels. To this end  we ﬁrst need to introduce some function spaces  which are later assumed to
contain the regression function.
Let us begin by recalling from  e.g. [3  p. 44]  [4  p. 398]  and [5  p. 360]  the modulus of smooth-
ness:
Deﬁnition 1. Let ⌦ ⇢ Rd with non-empty interior  ⌫ be an arbitrary measure on ⌦  and f :⌦ ! Rd
be a function with f 2 Lp (⌫) for some p 2 (0 1). For r 2 N  the r-th modulus of smoothness of
f is deﬁned by

!r Lp(⌫) (f  t) = sup

h (f  · )kLp(⌫)  
where k·k 2 denotes the Euclidean norm and the r-th difference 4r
j (1)rj f (x + jh)

khk2tk4r
j=0r

h (f  x) =(Pr

4r

0

t  0  

h (f ·) is deﬁned by
if x 2 ⌦r h
if x /2 ⌦r h

for h = (h1  . . .   hd) 2 Rd with hi  0 and ⌦r h := {x 2 ⌦: x + sh 2 ⌦ 8 s 2 [0  r]}.
It is well-known that the modulus of smoothness with respect to Lp (⌫) is a nondecreasing function
of t and for the Lebesgue measure on ⌦ it satisﬁes

!r Lp(⌦) (f  s)  

(3)

!r Lp(⌦) (f  t) ✓1 + t
s◆r

2

for all f 2 Lp (⌦) and all s > 0  see e.g. [6  (2.1)]. Moreover  the modulus of smoothness can be
used to deﬁne the scale of Besov spaces. Namely  for 1  p  q  1  ↵> 0  r := b↵c + 1  and an
arbitrary measure ⌫  the Besov space B↵

p q (⌫) is

B↵

p q(⌫) is deﬁned by

|f|B↵
and  for q = 1  it is deﬁned by

p q (⌫) :=nf 2 Lp (⌫) : |f|B↵
where  for 1  q < 1  the seminorm |·|B↵
p q(⌫) :=✓Z 1
p 1(⌫) := sup

p q(⌫) < 1o  
t ◆ 1
t↵!r Lp(⌫) (f  t)q dt
t>0t↵!r Lp(⌫) (f  t) .
p q(⌫)  see
In both cases the norm of B↵
(⌫) = Lip⇤ (↵  Lp (⌫))
e.g. [3  pp. 54/55] and [4  p. 398]. Finally  for q = 1  we often write B↵
p 1
and call Lip⇤ (↵  Lp (⌫)) the generalized Lipschitz space of order ↵. In addition  it is well-known 
p (Rd) fall into the scale of Besov spaces 
see e.g. [7  p. 25 and p. 44]  that the Sobolev spaces W ↵
namely

|f|B↵
p q (⌫) can be deﬁned by kfkB↵

p q(⌫) := kfkLp(⌫) + |f|B↵

0

 

q

p (Rd) ⇢ B↵
W ↵

p q(Rd)

(4)

for ↵ 2 N  p 2 (1 1)  and max{p  2} q  1 and especially W ↵
For our results we need to extend functions f :⌦ ! R to functions ˆf : Rd ! R such that the
smoothness properties of f described by some Sobolev or Besov space are preserved by ˆf. Recall
that Stein’s Extension Theorem guarantees the existence of such an extension  whenever ⌦ is a
bounded Lipschitz domain. To be more precise  in this case there exists a linear operator E mapping
functions f :⌦ ! R to functions Ef : Rd ! R with the properties:

2 (Rd) = B↵

2 2(Rd).

(a) E (f)
(b) E continuously maps W m

|⌦ = f  that is  E is an extension operator.

p (⌦) into W m

That is  there exist constants am p  0  such that  for every f 2 W m

p (⌦)  we have

p Rd for all p 2 [1 1] and all integer m  0.
p qRd for all p 2 (1 1)  q 2 (0 1] and all ↵> 0.

p q (⌦)  we have

p (Rd)  am p kfkW m

p (⌦) .

(5)

kEfkW m
p q (⌦) into B↵

(c) E continuously maps B↵

That is  there exist constants a↵ p q  0  such that  for every f 2 B↵

kEfkB↵

p q(Rd)  a↵ p q kfkB↵

p q(⌦) .

p

p

and W m1

For detailed conditions on ⌦ ensuring the existence of E  we refer to [8  p. 181] and [9  p. 83].
Property (c) follows by some interpolation argument since B↵
p q can be interpreted as interpolation
for q 2 [1 1]  p 2 (1 1)  ✓ 2 (0  1) and m0  m1 2 N0
space of the Sobolev spaces W m0
with m0 6= m1 and ↵ = m0(1  ✓) + m1✓  see [10  pp. 65/66] for more details. In the following 
we always assume that we do have such an extension operator E. Moreover  if µ is the Lebesgue
measure on ⌦  such that @⌦ has Lebesgue measure 0  the canonical extension of µ to Rd is given

by eµ(A) := µ(A \ ⌦) for all measurable A ⇢ Rd. However  in a slight abuse of notation  we
often write µ instead ofeµ  since this simpliﬁes the presentation. Analogously  we proceed for the

uniform distribution on ⌦ and its canonical extension to Rd and the same convention will be applied
to measures PX on ⌦ that are absolutely continuous w.r.t. the Lebesgue measure.
Finally  in order to state our main results  we denote the closed unit ball of the d-dimensional Eu-
clidean space by B`d
2 .
Theorem 1. Let X ⇢ B`d
2 be a domain such that we have an extension operator E in the above
sense. Furthermore  let M > 0  Y := [M  M]  and P be a distribution on X ⇥ Y such that
PX is the uniform distribution on X. Assume that we have ﬁxed a version f⇤L P of the regression

3

function such that f⇤L P (x) = EP (Y |x) 2 [M  M] for all x 2 X. Assume that  for ↵  1 and
r := b↵c + 1  there exists a constant c > 0 such that  for all t 2 (0  1]  we have

(6)
Then  for all "> 0 and p 2 (0  1) there exists a constant K > 0 such that for all n  1  ⌧  1  and
> 0  the SVM using the RKHS H satisﬁes

kfD k2

H

(1p)(1+")d

pn

+ K⌧
n

with probability Pn not less than 1  e⌧ .
With this oracle inequality we can derive learning rates for the learning method (2).
Corollary 1. Under the assumptions of Theorem 1 and for "> 0  p 2 (0  1)  and ⌧  1 ﬁxed  we
have  for all n  1 

2↵

2↵+2↵p+dp+(1p)(1+")d

!r L2(Rd)Ef⇤L P  t  ct↵ .
+ RL P(ÛfD ) R ⇤L P  K d + Kc22↵ + K
+ RL P(ÛfD n) R ⇤L P  Cn

n = c1n
n = c2n

2↵+2↵p+dp+(1p)(1+")d  

2↵+2↵p+dp+(1p)(1+")d .

Hn

2↵+d

1

n kfD nk2

with probability Pn not less than 1  e⌧ and with

Here  c1 > 0 and c2 > 0 are user-speciﬁed constants and C > 0 is a constant independent of n.
Note that for every ⇢> 0 we can ﬁnd "  p 2 (0  1) sufﬁciently close to 0 such that the learning rate
in Corollary 1 is at least as fast as

n 2↵

2↵+d +⇢ .

To achieve these rates  however  we need to set n and n as in Corollary 1  which in turn requires
us to know ↵. Since in practice we usually do not know this value  we now show that a standard
training/validation approach  see e.g. [2  Chapters 6.5  7.4  8.2]  achieves the same rates adaptively 
i.e. without knowing ↵. To this end  let ⇤:= (⇤ n) and := ( n) be sequences of ﬁnite subsets
⇤n  n ⇢ (0  1]. For a data set D := ((x1  y1)   . . .   (xn  yn))  we deﬁne

D1 := ((x1  y1)   . . .   (xm  ym))
D2 := ((xm+1  ym+1)   . . .   (xn  yn))

where m :=⌅ n

functions

2⇧ + 1 and n  4. We will use D1 as a training set by computing the SVM decision

fD1   := arg min
f2H

kfk2

H

+ RL D1 (f)  

(  ) 2 ⇤n ⇥ n

and use D2 to determine (  ) by choosing a (D2  D2) 2 ⇤n ⇥ n such that
( )2⇤n⇥n RL D2 (fD1  ) .

RL D2fD1 D2  D2 =

min

Theorem 2. Under the assumptions of Theorem 1 we ﬁx sequences ⇤:= (⇤ n) and := ( n)
of ﬁnite subsets ⇤n  n ⇢ (0  1] such that ⇤n is an ✏n-net of (0  1] and n is an n-net of (0  1]
with ✏n  n1 and n  n 1
2+d . Furthermore  assume that the cardinalities |⇤n| and |n| grow
polynomially in n. Then  for all ⇢> 0  the TV-SVM producing the decision functions fD1 D2  D2
learns with the rate

n 2↵

2↵+d +⇢

(7)

with probability Pn not less than 1  e⌧ .
What is left to do is to relate Assumption (6) with the function spaces introduced earlier  such
that we can show that the learning rates deduced earlier are asymptotically optimal under some
circumstances.

4

Corollary 2. Let X ⇢ B`d
2 be a domain such that we have an extension operator E of the form
described in front of Theorem 1. Furthermore  let M > 0  Y := [M  M]  and P be a distribution
on X ⇥ Y such that PX is the uniform distribution on X. If  for some ↵ 2 N  we have f⇤L P 2
2 (PX)  then  for all ⇢> 0  both the SVM considered in Corollary 1 and the TV-SVM considered
W ↵
in Theorem 2 learn with the rate

n 2↵

2↵+d +⇢

with probability Pn not less than 1  e⌧ . Moreover  if ↵> d/ 2  then this rate is asymptotically
optimal in a minmax sense.

Similar to Corollary 2 we can show assumption (6) and asymptotically optimal learning rates if the
regression function is contained in a Besov space.
Corollary 3. Let X ⇢ B`d
2 be a domain such that we have an extension operator E of the form
described in front of Theorem 1. Furthermore  let M > 0  Y := [M  M]  and P be a distribution
on X ⇥ Y such that PX is the uniform distribution on X. If  for some ↵  1  we have f⇤L P 2
(PX)  then  for all ⇢> 0  both the SVM considered in Corollary 1 and the TV-SVM considered
B↵
2 1
in Theorem 2 learn with the rate

n 2↵

2↵+d +⇢

(PX) = B↵

2 1

2 1

2 1

(PX) ! L2(PX)) ⇠ i ↵

with probability Pn not less than 1  e⌧ .
Since for the entropy numbers ei( id : B↵
d holds (cf. [7  p. 151])
(X) is continuously embedded into the space `1(X) of all bounded
and since B↵
functions on X  we obtain by [11  Theorem 2.2] that n 2↵
2↵+d is the optimal learning rate in a
minimax sense for ↵> d (cf. [12  Theorem 13]). Therefore  for ↵> d   the learning rates obtained
in Corollary 3 are asymptotically optimal.
So far  we always assumed that PX is the uniform distribution on X. This can be generalized by as-
suming that PX is absolutely continuous w.r.t. the Lebesgue measure µ such that the corresponding
density is bounded away from zero and from inﬁnity. Then we have L2(PX) = L2(µ) with equiva-
lent norms and the results for µ hold for PX as well. Moreover  to derive learning rates  we actually
only need that the Lebesgue density of PX is upper bounded. The assumption that the density is
bounded away from zero is only needed to derive the lower bounds in Corollaries 2 and 3.
Furthermore  we assumed  2 (0  1] in Theorem 1  and hence in Corollary 1 and Theorem 2 as
well. Note that  does not need to be restricted by one. Instead  only needs to be bounded from
above by some constant such that estimates on the entropy numbers for Gaussian kernels as used in
the proofs can be applied. For the sake of simplicity we have chosen one as upper bound  another
upper bound would only have inﬂuence on the constants.

There have already been made several investigations on learning rates for SVMs using the least
squares loss  see e.g. [13  14  15  16  17] and the references therein. In particular  optimal rates
have been established in [16]  if f⇤P 2 H  and the eigenvalue behavior of the integral operator
associated to H is known. Moreover  if f⇤P 62 H [17] and [12] establish both learning rates of
the form n/(+p)  where  is a parameter describing the approximation properties of H and
p is a parameter describing the eigenvalue decay. Furthermore  in the introduction of [17] it is
mentioned that the assumption on the eigenvalues and eigenfunctions also hold for Gaussian kernels
with ﬁxed width  but this case as well as the more interesting case of Gaussian kernels with variable
widths are not further investigated. In the ﬁrst case  where Gaussian kernels with ﬁxed width are
considered  the approximation error behaves very badly as shown in [18] and fast rates cannot be
expected as we discuss below. In the second case  where variable widths are considered as in our
paper  it is crucial to carefully control the inﬂuence of  on all arising constants which unfortunately
has not been worked out in [17]  either. In [17] and [12]  however  additional assumptions on the
interplay between H and L2(PX) are required  and [17] actually considers a different exponent
in the regularization term of (2). On the other hand  [12] shows that the rate n/(+p) is often
asymptotically optimal in a minmax sense. In particular  the latter is the case for H = W m
2 (X) 
2 (X)  and s 2 (d/2  m]  that is  when using a Sobolev space as the underlying RKHS H 
f 2 W s

5

then all target functions contained in a Sobolev of lower smoothness s > d/2 can be learned with the
asymptotically optimal rate n 2s
2s+d . Here we note that the condition s > d/2 ensures by Sobolev’s
2 (X) consists of bounded functions  and hence Y = [M  M] does not
embedding theorem that W s
impose an additional assumption on f⇤L P. If s 2 (0  d/2]  then the results of [12] still yield the
above mentioned rates  but we no longer know whether they are optimal in a minmax sense  since
Y = [M  M] does impose an additional assumption. In addition  note that for Sobolev spaces this
result  modulo an extra log factor  has already been proved by [1]. This result suggests that by using
a C1-kernel such as the Gaussian RBF kernel  one could actually learn the entire scale of Sobolev
spaces with the above mentioned rates. Unfortunately  however  there are good reasons to believe
that this is not the case. Indeed  [18] shows that for many analytic kernels the approximation error
can only have polynomial decay if f⇤L P is analytic  too. In particular  for Gaussian kernels with
ﬁxed width  and f⇤L P 62 C1 the approximation error does not decay polynomially fast  see [18 
2 (X)  then  in general  the approximation error function only
Proposition 1.1.]  and if f⇤L P 2 W m
has a logarithmic decay. Since it seems rather unlikely that these poor approximation properties can
be balanced by superior bounds on the estimation error  the above-mentioned results indicate that
Gaussian kernels with ﬁxed width may have a poor performance. This conjecture is backed-up by
many empirical experience gained throughout the last decade. Beginning with [19]  research has thus
focused on the learning performance of SVMs with varying widths. The result that is probably the
closest to ours is [20]. Although these authors actually consider binary classiﬁcation using convex
loss functions including the least squares loss  formulated it is relatively straightforward to translate
their ﬁnding to our least squares regression scenario. The result is the learning rate n m
m+2d+2   again
2 (X) for some m > 0. Furthermore  [21] treats the case  where X
under the assumption f⇤L P 2 W m
is isometrically embedded into a t-dimensional  connected and compact C1-submanifold of Rd. In
this case  it turns out that the resulting learning rate does not depend on the dimension d  but on the
intrinsic dimension t of the data. Namely the authors show the rate n s
8s+4t modulo a logarithmic
factor  where s 2 (0  1] and f⇤L P 2 Lip (s). Another direction of research that can be applied to
Gaussian kernels with varying widths are multi-kernel regularization schemes  see [22  23  24] for
some results in this direction. For example  [22] establishes learning rates of the form n 2md
4(4md) +⇢
2 (X) for some m 2 (d/2  d/2 + 2)  where again ⇢> 0 can be chosen to be
whenever f⇤L P 2 W m
arbitrarily close to 0. Clearly  all these results provide rates that are far from being optimal  so that
it seems fair to say that our results represent a signiﬁcant advance. Furthermore  we can conclude
that  in terms of asymptotical minmax rates  multi-kernel approaches applied Gaussian RBFs cannot
provide any signiﬁcant improvement over a simple training/validation approach for determining the
kernel width and the regularization parameter  since the latter already leads to rates that are optimal
modulo an arbitrarily small ⇢ in the exponent.

3 Proof of the main result

To prove Theorem 1 we deduce an oracle inequality for the least squares loss by specializing [2 
Theorem 7.23] (cf. Theorem 3). To be ﬁnally able to show Theorem 1 originating from Theorem 3 
we have to estimate the approximation error.
Lemma 1. Let X ⇢ Rd be a domain such that we have an extension operator E of the form de-
scribed in front of Theorem 1  PX be the uniform distribution on X and f 2 L1 (X). Furthermore 
let ˜f be deﬁned by

2 Ef (x)

˜f (x) :=p⇡ d
for all x 2 Rd and  for r 2 N and > 0  K : Rd ! R be deﬁned by
rXj=1✓r
j◆ (1)1j 1
jd✓ 2
p⇡◆ d
K (·) := exp k·k2
2 ! .

K (·) :=

with

(8)

(9)

(·)

2

K jp2

2

6

Then  for r 2 N  > 0  and q 2 [1 1)  we have Ef 2 Lq(ePX) and

q

Lq(PX )  Cr q !q

r Lq(Rd) (Ef  /2)  

where Cr q is a constant only depending on r  q and µ(X).

K ⇤ ˜f  f

In order to use the conclusion of Lemma 1 in the proof of Theorem 1 it is necessary to know some
properties of K ⇤ ˜f. Therefore  we need the next two lemmata.
Lemma 2. Let g 2 L2Rd  H be the RKHS of the Gaussian RBF kernel k over X ⇢ Rd and
p⇡◆ d

j◆ (1)1j 1

j22 !
2kxk2

jd✓ 2

exp 

K (x) :=

2

2

rXj=1✓r

for x 2 Rd and a ﬁxed r 2 N. Then we have
K ⇤ g 2 H  
kK ⇤ gkH  (2r  1)kgkL2(Rd) .

Lemma 3. Let g 2 L1Rd  H be the RKHS of the Gaussian RBF kernel k over X ⇢ Rd and

K be as in Lemma 2. Then

|K ⇤ g (x)| p⇡ d

2 (2r  1)kgkL1(Rd)

holds for all x 2 X. Additionally  we assume that X is a domain in Rd such that we have an
extension operator E of the form described in front of Theorem 1  Y := [M  M] and  for all x 2
2 Ef⇤L P (x)  where f⇤L P denotes a version of the conditional expectation
Rd  ˜f (x) := (p⇡) d
such that f⇤L P (x) = EP (Y |x) 2 [M  M] for all x 2 X. Then we have ˜f 2 L1Rd and
for all x 2 X  which implies

|K ⇤ ˜f (x)| a0 1 (2r  1) M

L(y  K ⇤ ˜f (x))  4ra2M 2

H

for the least squares loss L and all (x  y) 2 X ⇥ Y .
Next  we modify [2  Theorem 7.23]  so that the proof of Theorem 1 can be build upon it.
2   Y := [M  M] ⇢ R be a closed subset with M > 0 and P be a
Theorem 3. Let X ⇢ B`d
distribution on X ⇥ Y . Furthermore  let L : Y ⇥ R ! [0 1) be the least squares loss  k be
the Gaussian RBF kernel over X with width  2 (0  1] and H be the associated RKHS. Fix an
f0 2 H and a constant B0  4M 2 such that kL  f0k1  B0. Then  for all ﬁxed ⌧  1  > 0 
"> 0 and p 2 (0  1)  the SVM using H and L satisﬁes
kfD k2
+3456M 2 + 15B0 (ln(3) + 1)⌧
 9⇣kf0k2
with probability Pn not less than 1  e⌧   where C" p is a constant only depending on "  p and M.
With the previous results we are ﬁnally able to prove the oracle inequality declared by Theorem 1.
Proof of Theorem 1. First of all  we want to apply Theorem 3 for f0 := K ⇤ ˜f with
j22 !
2kxk2

+ RL P⇣ÛfD ⌘ R ⇤L P

+ RL P (f0) R ⇤L P⌘ + C" p

exp 

j◆ (1)1j 1

p⇡◆ d

jd✓ 2

(1p)(1+")d

K (x) :=

pn

H

n

2

2

rXj=1✓r

7

(10)

k ˜fkL2(Rd) =p⇡ d
p⇡ d
p⇡◆ d
✓ 2
i.e. ˜f 2 L2Rd. Because of this and Lemma 2

2

f0 = K ⇤ ˜f 2 H

2 kEf⇤L PkL2(Rd)
2 a0 2kf⇤L PkL2(X)
a0 2M  

is satisﬁed and with Lemma 3 we have

kL  f0k1

=

sup

(x y)2X⇥Y |L (y  f0 (x))| =

Furthermore  (1) and Lemma 1 yield

sup

(x y)2X⇥YL⇣y  K ⇤ ˜f (x)⌘  4ra2M 2 =: B0 .
=K ⇤ ˜f  f⇤L P

RL P (f0) R ⇤L P = RL P⇣K ⇤ ˜f⌘ R ⇤L P
r L2(Rd)⇣Ef⇤L P 

2⌘

L2(PX )



2

 Cr 2 !2
 Cr 2 c22↵  

and

˜f (x) :=p⇡ d

2 Ef⇤L P (x)

for all x 2 Rd. The choice f⇤L P (x) 2 [M  M] for all x 2 X implies f⇤L P 2 L2 (X) and the latter
together with X ⇢ B`d

2 and (5) yields

where we used the assumption

!r L2(Rd)⇣Ef⇤L P 



2⌘  c↵

for  2 (0  1]  ↵  1  r = b↵c + 1 and a constant c > 0 in the last step. By Lemma 2 and (10) we
know

kf0kH

= kK ⇤ ˜fkH  (2r  1)k ˜fkL2(Rd)  (2r  1)✓ 2

p⇡◆ d

2

a0 2M .

Therefore  Theorem 3 and the above choice of f0 yield  for all ﬁxed ⌧  1  > 0  "> 0 and
p 2 (0  1)  that the SVM using H and L satisﬁes

H

kfD k2

+ RL P⇣ÛfD ⌘ R ⇤L P

 9  (2r  1)2✓ 2
p⇡◆d
+3456 + 15 · 4ra2 M 2(ln(3) + 1)⌧

0 2M 2 + Cr 2c22↵!

(1p)(1+")d

+ C" p

pn

a2

n
(1p)(1+")d

pn

+ C2⌧
n

 C1d + 9 Crc22↵ + C" p

with probability Pn not less than 1  e⌧ and with constants C1 := 9 (2r  1)2 2d⇡ d
C2 := (ln(3) + 1)3456 + 15 · 4ra2 M 2  a := max{a0 1  1}  Cr := Cr 2 only depending on r
and µ(X) and C" p as in Theorem 3.

0 2M 2 

2 a2

8

References
[1] L. Gy¨orﬁ  M. Kohler  A. Krzy˙zak  and H. Walk. A Distribution-Free Theory of Nonparametric

Regression. Springer-Verlag New York  2002.

[2] I. Steinwart and A. Christmann. Support Vector Machines. Springer-Verlag  New York  2008.
[3] R.A. DeVore and G.G. Lorentz. Constructive Approximation. Springer-Verlag Berlin Heidel-

berg  1993.

[4] R.A. DeVore and V.A. Popov. Interpolation of Besov Spaces. AMS  Volume 305  1988.
[5] H. Berens and R.A. DeVore. Quantitative Korovin theorems for positive linear operators on

Lp-spaces. AMS  Volume 245  1978.

[6] H. Johnen and K. Scherer. On the equivalence of the K-functional and moduli of continuity and
some applications. In Lecture Notes in Math.  volume 571  pages 119–140. Springer-Verlag
Berlin  1976.

[7] D.E. Edmunds and H. Triebel. Function Spaces  Entropy Numbers  Differential 0perators.

Cambridge University Press  1996.

[8] E.M. Stein. Singular Integrals and Differentiability Properties of Functions. Princeton Univ.

Press  1970.

[9] R.A. Adams and J.J.F. Fournier. Sobolev Spaces. Academic Press  2nd edition  2003.
[10] H. Triebel. Theory of Function Spaces III. Birkh¨auser Verlag  2006.
[11] V. Temlyakov. Optimal estimators in learning theory. Banach Center Publications  Inst. Math.

Polish Academy of Sciences  72:341–366  2006.

[12] I. Steinwart  D. Hush  and C. Scovel. Optimal rates for regularized least squares regression.

Proceedings of the 22nd Annual Conference on Learning Theory  2009.

[13] F. Cucker and S. Smale. On the mathematical foundations of learning. Bull. Amer. Math. Soc. 

39:1–49  2002.

[14] E. De Vito  A. Caponnetto  and L. Rosasco. Model selection for regularized least-squares

algorithm in learning theory. Found. Comput. Math.  5:59–85  2005.

[15] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approxi-

mations. Constr. Approx.  26:153–172  2007.

[16] A. Caponnetto and E. De Vito. Optimal rates for regularized least squares algorithm. Found.

Comput. Math.  7:331–368  2007.

[17] S. Mendelson and J. Neeman. Regularization in kernel learning. Ann. Statist.  38:526–565 

2010.

[18] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Anal. Appl. 

Volume 1  2003.

[19] I. Steinwart and C. Scovel. Fast rates for support vector machines using Gaussian kernels. Ann.

Statist.  35:575–607  2007.

[20] D.-H. Xiang and D.-X. Zhou. Classiﬁcation with Gaussians and convex loss. J. Mach. Learn.

Res.  10:1447–1468  2009.

[21] G.-B. Ye and D.-X. Zhou. Learning and approximation by Gaussians on Riemannian mani-

folds. Adv. Comput. Math.  Volume 29  2008.

[22] Y. Ying and D.-X. Zhou. Learnability of Gaussians with ﬂexible variances. J. Mach. Learn.

Res. 8  2007.

[23] C.A. Micchelli  M. Pontil  Q. Wu  and D.-X. Zhou. Error bounds for learning the kernel. 2005.
[24] Y. Ying and C. Campbell. Generalization bounds for learning the kernel. In S. Dasgupta and

A. Klivans  editors  Proceedings of the 22nd Annual Conference on Learning Theory  2009.

9

,Tamara Broderick
Nicholas Boyd
Andre Wibisono
Ashia Wilson
Michael Jordan
Paul Lagrée
Claire Vernade
Yi Ouyang
Mukul Gagrani
Ashutosh Nayyar
Rahul Jain