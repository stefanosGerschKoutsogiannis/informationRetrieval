2019,Beyond the Single Neuron Convex Barrier for Neural Network Certification,We propose a new parametric framework  called k-ReLU  for computing precise
and scalable convex relaxations used to certify neural networks. The key idea is to
approximate the output of multiple ReLUs in a layer jointly instead of separately.
This joint relaxation captures dependencies between the inputs to different ReLUs
in a layer and thus overcomes the convex barrier imposed by the single neuron
triangle relaxation and its approximations. The framework is parametric in the
number of k ReLUs it considers jointly and can be combined with existing verifiers
in order to improve their precision. Our experimental results show that k-ReLU en-
ables significantly more precise certification than existing state-of-the-art verifiers
while maintaining scalability.,Beyond the Single Neuron Convex Barrier

for Neural Network Certiﬁcation

Gagandeep Singh1  Rupanshu Ganvir2  Markus Püschel1  Martin Vechev1

Department of Computer Science

ETH Zurich  Switzerland

1{gsingh pueschel martin.vechev}@inf.ethz.ch

2rganvir@student.ethz.ch

Abstract

We propose a new parametric framework  called k-ReLU  for computing precise
and scalable convex relaxations used to certify neural networks. The key idea is to
approximate the output of multiple ReLUs in a layer jointly instead of separately.
This joint relaxation captures dependencies between the inputs to different ReLUs
in a layer and thus overcomes the convex barrier imposed by the single neuron
triangle relaxation and its approximations. The framework is parametric in the
number of k ReLUs it considers jointly and can be combined with existing veriﬁers
in order to improve their precision. Our experimental results show that k-ReLU en-
ables signiﬁcantly more precise certiﬁcation than existing state-of-the-art veriﬁers
while maintaining scalability.

1

Introduction

Neural networks are being increasingly used in many safety critical domains including autonomous
driving  medical devices  and face recognition. Thus  it is important to ensure they are provably
robust and cannot be fooled by adversarial examples [1]: small changes to a given image that can
fool the network into making a wrong classiﬁcation. To address this challenge  a range of veriﬁcation
techniques were introduced recently ranging from exact but expensive methods based on SMT solvers
[2–4]  mixed integer linear programming [5]  and Lipschitz optimization [6] to approximate and
incomplete  but more scalable methods based on abstract interpretation [7–9]  duality [10  11]  semi
deﬁnite [12  13] and linear relaxations [14–17]. Recently  combinations of approximate methods with
solvers have been used for producing more precise results than approximate methods alone while
also being more scalable than exact methods [18  19].
The key challenge any veriﬁcation method must address is computing the output of ReLU assignments
where the input can take both positive and negative values. Exact computation must consider two paths
per neuron  which quickly becomes infeasible due to a combinatorial explosion while approximate
methods trade precision for scalability via a convex relaxation of ReLU outputs.
The most precise convex relaxation of ReLU output is based on the convex hull of Polyhedra [20]
which is practically infeasible as it requires an exponential number of convex hull computations 
each with a worst-case exponential complexity in the number of neurons. The most common convex
relaxation of y1:=ReLU(x1) used in practice [17  5] is the triangle relaxation from [3]. We note
that other works such as [8  9  14–16  11] approximate this relaxation. The triangle relaxation
creates constraints only between y1 and x1 and is optimal in the x1y1-plane. Because of this
optimality  recent work [17] refers to the triangle relaxation as the convex barrier  meaning the best
convex approximation one can obtain when processing each ReLU separately. However  the triangle
relaxation is not optimal when one considers multiple neurons at a time as it ignores all dependencies
between x1 and any other neuron x2 in the same layer  and thus loses precision.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

x2

(0  2)

(−2  0)

(2  0)
x1

(0  −2)

z = x1 + x2

(0  2  −2)

(1  2  −2)

y2

(2  1  −2)

(2  0  −2)

y2

(0  2  2)

(0  0  −2)

y1

y1

(0  0  2)

z = x1 + x2

(2  0  2)

(a) Input shape

(b) 1-ReLU

(c) 2-ReLU

Figure 1: The input space for the ReLU assignments y1 := ReLU (x1)  y2 := ReLU (x2) is shown
on the left in blue. Shapes of the relaxations projected to 3D are shown on the right in red.

This work: beyond the single neuron convex barrier In this work  we address this issue by
proposing a novel parameterized framework  called k-ReLU  for generating convex approximations
that consider multiple ReLUs jointly. Here  the parameter k determines how many ReLUs are
considered jointly with large k resulting in more precise output. For example  unlike prior work  our
framework can generate a convex relaxation for y1:=ReLU(x1) and y2:=ReLU(x2) that is optimal
in the x1x2y1y2-space. We next illustrate this point with an example.
Precision gain with k-ReLU on an example Consider the input space of x1x2 as deﬁned by the
blue area in Fig. 1 and the ReLU operations y1:=ReLU(x1) and y2:=ReLU(x2). The input space is
bounded by the relational constraints x2 − x1 ≤ 2  x1 − x2 ≤ 2  x1 + x2 ≤ 2 and −x1 − x2 ≤ 2.
The relaxations produced are in a four dimensional space of x1x2y1y2. For simplicity of presentation 
we show the feasible shape of y1y2 as a function of z = x1 + x2.
The triangle relaxation from [3] is in fact a special case of our framework with k = 1  that is 
1-ReLU. 1-ReLU independently computes two relaxations - one in the x1y1 space and the other
in the x2y2 space. The ﬁnal relaxation is the cartesian product of the feasible sets of the two
individually computed relaxations and is oblivious to any correlations between x1 and x2. The
relaxation adds triangle constraints {y1 ≥ 0  y1 ≥ x1  y1 ≤ 0.5 · x1 + 1} between x1 and y1 as well
as {y2 ≥ 0  y2 ≥ x2  y2 ≤ 0.5 · x2 + 1} between x2 and y2.
In contrast  2-ReLU considers the two ReLU’s jointly and captures the relational constraints between
x1 and x2. 2-ReLU computes the following relaxation:

{y1 ≥ 0  y1 ≥ x1  y2 ≥ 0  y2 ≥ x2  2 · y1 + 2 · y2 − x1 − x2 ≤ 2}

The polytope produced is shown in Fig. 1c. Note that in this case the shape of y1y2 is not independent
of x1 + x2 as opposed to the triangle relaxation. At the same time  it is more precise than Fig. 1b for
all values of z.

Main contributions Our main contributions are:

• A novel framework  called k-ReLU  that computes optimal convex relaxations for the output
of k ReLU operations jointly. k-ReLU is generic and can be combined with existing veriﬁers
for improved precision while maintaining scalability. Further  k-ReLU is also adaptive and
can be tuned to balance precision and scalability by varying k.

• A method for computing approximations of the optimal relaxations for larger k  which is

more precise than simply using l < k.

• An instantiation of k-ReLU with the recent DeepPoly convex relaxation [9] resulting in a

veriﬁer called kPoly.

• An evaluation showing kPoly is more precise and scalable than the state-of-the-art veriﬁers
[9  19] on the task of certifying neural networks of up to 100K neurons against challenging
adversarial perturbations (e.g.  L∞ balls with  = 0.3).

We note that the work of [12] computes semi deﬁnite relaxations that consider multiple ReLUs jointly 
however these are not optimal and do not scale to the large networks used in our experiments.

2

Table 1: Volume of the output bounding box computed by kPoly on a 9 × 200 network.

k
Volume

1-ReLU

4.5272 · 1014

2-ReLU
5.1252 · 107

3-ReLU
2.9679 · 105

Precision gain in practice Table 1 quantitatively compares the precision of kPoly instantiated with
three relaxations: k = 1  k = 2  and k = 3. We measure the volume of the output bounding box
computed after propagating an L∞-ball of radius  = 0.015 through a deep  fully connected MNIST
network with 9 layers containing 200 neurons each. We can observe that the volume of the output
from 3-ReLU and 2-ReLU is respectively 9 and 7 orders of magnitude smaller than from 1-ReLU.
We note that the networks we consider  as for example the 9 × 200 network above  are especially
challenging for state-of-the-art veriﬁers as these methods either lose unnecessary precision [8  9  14 
15  19  16  17] or simply do not scale [5  18  12  4  13  10].
Finally  we remark that while we consider robustness certiﬁcation against norm based perturbations in
our evaluation  our framework can also be used for precise and scalable veriﬁcation of other network
safety properties such as stability [21] or robustness against geometric and semantic perturbations
[9  22  23].

2 Overview of k-ReLU

We now show  on a simple example  that the k-ReLU concept can be used to improve the results of
state-of-the-art veriﬁers. In particular  we illustrate how the output of our veriﬁer kPoly instantiated
with 1-ReLU is reﬁned by instantiating it with 2-ReLU. This is possible as the 2-ReLU relaxation
can capture extra relationships between neurons that 1-ReLU inherently cannot.
Consider the simple feedforward neural network with ReLU activations shown in Fig. 2. The network
has two inputs each taking values independently in the range [−1  1]  one hidden layer and one output
layer each containing two neurons. For simplicity  we split each layer into two parts: one for the
afﬁne transformation and the other for the ReLU. The weights of the afﬁne transformation are shown
on the arrows and the biases are above or below the respective neuron. The goal is to verify that
x9 ≤ 4 holds for the output x9 with respect to all inputs.
We ﬁrst show that 1-ReLU instantiated with the state-of-the-art DeepPoly [9] relaxation fails to verify
the property. DeepPoly  described formally in Section 4  associates two pairs of lower and upper
≤
j aj · xj + c
bounds with each neuron xi: (a
i   a
where c  li  ui ∈ R ∪ {−∞  +∞} and aj ∈ R. The bounds computed by the veriﬁer using this
instantiation are shown as annotations in Fig. 2.
First Layer The veriﬁer starts by computing the bounds for x1 and x2 which are simply taken from
the input speciﬁcation resulting in:

i have the form(cid:80)

≥
≤
≥
i ) and (li  ui). Here  a
i and a

x1 ≥ −1  x1 ≤ 1  l1 = −1  u1 = 1  and x2 ≥ −1  x2 ≤ 1  l2 = −1  u2 = 1.

Second Layer Next  the afﬁne assignments x3 := x1 +x2 and x4 := x1−x2 are handled. DeepPoly
handles afﬁne transformations exactly and thus no precision is lost. The afﬁne transformation results
in the following bounds for x3 and x4:

x3 ≥ x1 + x2  x3 ≤ x1 + x2  l3 = −2  u3 = 2 
x4 ≥ x1 − x2  x4 ≤ x1 − x2  l4 = −2  u4 = 2.

DeepPoly can precisely handle ReLU assignments when the input neuron takes only positive or
negative values  otherwise it loses precision. Since x3 and x4 can take both positive and negative
values  an approximation based on the triangle relaxation is applied which for x5 yields:

(1)
Note that DeepPoly discards the other lower bound x5 ≥ x3 from the triangle relaxation. The lower
bound l5 is set to 0 and the relation x3 ≤ x1 + x2 is substituted for x3 in (1) for computing the upper
bound which yields l5 = 0  u5 = 2. Analogously  for x6 we obtain:

x5 ≥ 0 

x5 ≤ 1 + 0.5 · x3.

x6 ≥ 0 

x6 ≤ 1 + 0.5 · x4 

l6 = 0 

u6 = 2.

(2)

3

x1 ≥ −1
x1 ≤ 1
l1 = −1
u1 = 1

[-1 1]

x1

x2

[-1 1]

x2 ≥ −1
x2 ≤ 1
l2 = −1
u2 = 1

1

1

1

-1

1-ReLU

2-ReLU

max(0  x3)

max(0  x4)

x5

x6

0

2

1

1

x3

x4

0

0

x7

x8

1.5

max(0  x7)

max(0  x8)

x3 ≥ x1 + x2
x3 ≤ x1 + x2
l3 = −2
u3 = 2
0

x5 ≥ 0
x5 ≤ 1 + 0.5 · x3
l5 = 0
u5 = 2

x7 ≥ x5 + 2 · x6
x7 ≤ x5 + 2 · x6
l7 = 0
u7 = 5

x4 ≥ x1 − x2
x4 ≤ x1 − x2
l4 = −2
u4 = 2

x6 ≥ 0
x6 ≤ 1 + 0.5 · x4
l6 = 0
u6 = 2

x8 ≥ x6 + 1.5
x8 ≤ x6 + 1.5
l8 = 1.5
u8 = 3.5

2 · x5 + 2 · x6 − x3 − x4 ≤ 2

x7 ≤ 4 

x3 + x4 ≤ 2 
x3 − x4 ≤ 2 
x4 − x3 ≤ 2 
−x3 − x4 ≤ 2

x9 ≥ x7
x9 ≤ x7
l9 = 0
u9 = 5

x9

x10

x10 ≥ x8
x10 ≤ x8
l10 = 1.5
u10 = 3.5

x9 ≤ 4

Figure 2: Veriﬁcation of property x9 ≤ 2. Reﬁning DeepPoly with 1-ReLU fails to prove the property
whereas 2-ReLU adds extra constraints (in green) that help in verifying the property.

Third Layer Next  the afﬁne assignments x7 := x5 + 2x6 and x8 := x6 + 1.5 are handled.
DeepPoly adds the constraints:

x7 ≥ x5 + 2 · x6 

x7 ≤ x5 + 2 · x6 

x8 ≥ x6 + 1.5 

x8 ≤ x6 + 1.5 

(3)

To compute the upper and lower bounds for x7 and x8  DeepPoly substitutes the polyhedral constraints
for x5 and x6 from (1) and (2) in (3). It again substitutes for the constraints for x5 and x6 in terms of
x3 and x4 and iterates until it reaches the input layer where it substitutes the concrete bounds for x1
and x2. Doing so yields l7 = 0  u7 = 5 and l8 = 1.5  u8 = 3.5.
Reﬁnement with 1-ReLU fails Because DeepPoly discards one of the lower bounds from the
triangle relaxations for the ReLU assignments in the previous layer  it is possible to reﬁne lower and
upper bounds for x7 and x8 by encoding the network upto the ﬁnal afﬁne transformation using the
relatively tighter ReLU relaxations based on the triangle formulation and then computing bounds for
x7 and x8 with respect to this formulation. However  this does not improve bounds and still yields
l7 = 0  u7 = 5  l8 = 1.5  u8 = 3.5.
As the lower bounds for both x7 and x8 are non-negative  the DeepPoly ReLU approximation simply
propagates x7 and x8 to the output layer. The ﬁnal output is thus:

x9 ≥ x7 
x10 ≥ x8 

x9 ≤ x7 
x10 ≤ x8 

l9 = 0 
l10 = 1.5 

u9 = 5 
u10 = 3.5.

Because the upper bound is u9 = 5  the veriﬁer fails to prove the property that x9 ≤ 4.

Reﬁnement with 2-ReLU veriﬁes the property Now we consider reﬁnement with our 2-ReLU
relaxation which considers the two ReLU assignments x5 := ReLU (x3) and x6 := ReLU (x4)
jointly. Besides the box constraints for x3 and x4  it also considers the constraints x3 + x4 ≤
2  x3 − x4 ≤ −2 −x3 − x4 ≤ 2  x4 − x3 ≤ 2 for computing the output of ReLU. The ReLU output
contains the extra constraint 2 · x5 + 2 · x6 − x3 − x4 ≤ 2 that 1-ReLU cannot capture. We again
encode the network upto the ﬁnal afﬁne transformation with the tighter ReLU relaxations obtained
using 2-ReLU and reﬁne the bounds for x7  x8. Now  we obtain better upper bounds as u7 = 4. The
better bound for u7 is then propagated to u9 and is sufﬁcient for proving the desired property.
We remark that while in this work we instantiate the k-ReLU concept with the DeepPoly relaxation 
the idea can be applied to other relaxation [11  7–10  12  14  15  17  18].

4

3 k-ReLU relaxation framework

In this section we formally describe our k-ReLU framework for generating optimal convex relax-
ations in the input-output space for k ReLU operations jointly. In the next section  we discuss the
instantiation of our framework with existing veriﬁers which enables more precise results.
We consider a ReLU based feedforward  convolutional or residual neural network with h neurons
from a set H (that is h = |H|) and a bounded input region I ⊆ Rm where m < h is the number
of neural network inputs. In our exposition  we treat the afﬁne transformation and the ReLUs as
separate layers. We consider a convex approximation method M that processes network layers in
sequence from the input to the output layer passing the output of predecessor layers as input to the
successor layers. Let S ⊆ Rh be a convex set computed via M approximating the set of values that
neurons upto layer l-1 can take with respect to I and B ⊇ S be the smallest bounding box around S.
We use Conv(S1 S2) and S1 ∩ S2 to denote the convex hull and the intersection of convex sets S1
and S2  respectively.
Let X  Y ⊆ H be respectively the set of input and output neurons in the l-th layer consisting of n
ReLU assignments of the form yi:=ReLU(xi) where xi ∈ X and yi ∈ Y. In the general case  each
input neuron xi takes on both positive and negative values in S. We deﬁne the polyhedra induced by
the two branches of each ReLU assignment yi:=ReLU(xi) as C+
i = {xi ≥ 0  yi = xi} ⊆ Rh and
C−
| s ∈ J → {−  +}} (where J ⊆ [n]}) be
i∈J C s(i)
the set of polyhedra Q ⊆ Rh constructed by the intersection of polyhedra Ci ⊆ Rh for neurons xi  yi
indexed by the set J such that each Ci ∈ {C+
i }. We next formulate the best convex relaxation of
the output after n ReLU assignments.

i = {xi ≤ 0  yi = 0} ⊆ Rh. Let QJ = {(cid:84)

i  C−

i

3.1 Best convex relaxation

The best convex relaxation after the n ReLU assignments is given by

(4)
Sbest considers all n assignments jointly. Computing it is practically infeasible as it involves computing
2n convex hulls each of which has exponential cost in the number of neurons h [24].

Sbest = ConvQ∈Q[n](Q ∩ S).

3.2

1-ReLU

We now describe the prior convex relaxation [3] through triangles (here called 1-ReLU) that handles
the n ReLU assignments separately. Here  the input to the i-th assignment yi:=ReLU(xi) is the
polyhedron P1-ReLU ⊇ S where for each xi ∈ X   P1-ReLU i contains only an interval constraint [li  ui]
that bounds xi  that is  li ≤ xi ≤ ui. Here  the interval bounds are simply obtained from the bounding
box B of S. The output of this method after n assignments is
Conv(P1-ReLU i ∩ C+

S1-ReLU = S ∩ n(cid:92)

i   P1-ReLU i ∩ C−
i ).

(5)

i=1

i   P1-ReLU i ∩ C−

The projection of Conv(P1-ReLU i ∩ C+
i ) onto the xiyi-plane is a triangle minimizing
the area and is the optimal convex relaxation in this plane. However  because the input polyhedron
P1-ReLU is a hyperrectangle (when projected to X )  it does not capture relational constraints between
different xi’s in X (meaning it typically has to substantially over-approximate the set S). Thus  as
expected  the computed result S1-ReLU of the 1-ReLU method will incur signiﬁcant imprecision when
compared with the Sbest result.

3.3 k-ReLU relaxations

We now describe our k-ReLU framework for computing a convex relaxation of the output of n ReLUs
in one layer by considering groups of k ReLUs jointly with k > 1. For simplicity  we assume that
n > k and k divides n. Let J be a partition of the set of indices [n] such that each block Ji ∈ J
contains exactly k indices. Let Pk-ReLU i ⊆ Rh be a polyhedron containing interval and relational
constraints over the neurons from X indexed by Ji. In our framework  Pk-ReLU i is derived via B and
S and satisﬁes S ⊆ Pk-ReLU i.

5

(S ∩(cid:84)n/k

i=1 Ki) as per (6)

Pk-ReLU i ∩ Q

for each
Q ∈ QJi

convex

hull

for each
Pk-ReLU i

Pk-ReLU i

{Pk-ReLU i}
{Pk-ReLU i}

Convex hull for Ji
Denoted by Ki
2x1 + x2 + x3 − y1 ≤ 0
y2 + x2 − x3 ≤ −1
y3 − x1 + x3 ≤ 1

...
...

Figure 3: Steps to instantiating the k-ReLU framework.

Convex set S via M=

SDP [12  13]

Abstract

Interpretation[7–9]
Linear relaxations
[14  15  17  18]
Duality [10  11]
Partition J of [n]

n/k(cid:92)

Our k-ReLU framework produces the following convex relaxation of the output:

Sk-ReLU = S ∩

ConvQ∈QJi

(Pk-ReLU i ∩ Q).

(6)

i=1

The result of (6) is the optimal convex relaxation for the output of n ReLUs for the given choice of
S  k J   and Pk-ReLU i.
Theorem 3.1. For k > 1 and a partition J of indices  if there exists a Ji for which Pk-ReLU i (cid:36)
u∈Ji

P1-ReLU u holds  then Sk-ReLU (cid:36) S1-ReLU.

(cid:84)

The proof of Theorem 3.1 is given in appendix. Note that P1-ReLU only contains interval constraints
whereas Pk-ReLU contains both  the same interval constraints and extra relational constraints. Thus 
any convex relaxation obtained using k-ReLU is typically strictly more precise than a 1-ReLU one.
Precise and scalable relaxations for large k For each Ji  the optimal convex relaxation Ki =
(Pk-ReLU i ∩ Q) from (6) requires computing the convex hull of 2k convex sets each of
ConvQ∈QJi
which has a worst-case exponential cost in terms of k. Thus  computing Ki via (6) can become
computationally expensive for large values of k. We propose an efﬁcient relaxation K(cid:48)
i for each block
Ji ∈ J (where |Ji|= k as described earlier) based on computing relaxations for all subsets of Ji
that are of size 2 ≤ l < k. Let Ri = {{j1  . . .   jl} | j1  . . .   jl ∈ Ji} be the set containing all subsets
of Ji containing l indices. For each R ∈ Ri  let P (cid:48)
l-ReLU R ⊆ Rh be a polyhedron containing interval
and relational constraints between the neurons from X indexed by R with S ⊆ P (cid:48)
The relaxation K(cid:48)

(cid:1) times as:

l-ReLU R.

i is computed by applying l-ReLU(cid:0)k
(cid:92)
k-ReLU = S ∩(cid:84)n/k

ConvQ∈QR (P (cid:48)

K(cid:48)
i =

R∈Ri

l

l-ReLU R ∩ Q).

(7)

i=1 K(cid:48)
The layerwise convex relaxation S(cid:48)
i via (7) is tighter than computing relaxation
i ∈ J (cid:48) there exists Rj corresponding to a
Sl-ReLU via (6) with a partition J (cid:48) where for each block J (cid:48)
block of J such that J (cid:48)
⊆ Pl-ReLU J (cid:48)
is the polyhedron in (6)
where Pl-ReLU J (cid:48)
for computing Sl-ReLU. In our instantiations  we ensure that this condition holds for gaining precision.

i ∈ Rj and P (cid:48)

l-ReLU J (cid:48)

i

i

i

4

Instantiating the k-ReLU framework

Our k-ReLU framework from Section 3 can be instantiated to produce different relaxations depending
on the parameters S  k  J   and Pk-ReLU i. Fig. 3 shows the steps to instantiating our framework. The
inputs to the framework is the convex set S and the partition J based on k. These inputs are ﬁrst used
to produce a set containing n/k polyhedra {Pk-ReLU i}. Each polyhedron Pk-ReLU i is then intersected
with polyhedra from the set QJi producing 2k polyhedra which are then combined via the convex
hull (each called Ki). The Ki’s are then combined with S to produce the ﬁnal relaxation that captures
the values which neurons can take after the ReLU assignments. This relaxation is tighter than that
produced by applying M directly on the ReLU layer enabling precision gains.

6

yi

yi

li

l(cid:48)

i

u(cid:48)

i

ui

xi

li

l(cid:48)

i

u(cid:48)

i

ui

xi

(a)

(b)

Figure 4: DeepPoly relaxations for yi:=ReLU(xi) using the original bounds li  ui (in blue) and the
reﬁned bounds l(cid:48)
i (in green) for xi. The reﬁned relaxations have smaller area in the xiyi-plane.

i  u(cid:48)

4.1 Computing key parameters
We next describe the choice of the key parameters S  k J   Pk-ReLU i in our framework.
Input convex set Examples of convex approximation method M for computing S include [11  7–
10  12  14  15  17  18]. In this paper  we use the DeepPoly [9] relaxation for computing S which is a
state-of-the-art precise and scalable veriﬁer for neural networks.
k and partition J We use (6) to compute the output relaxation when k ∈ {2  3}. For larger k  we
compute the output based on (7). To maximize precision gain  we group those indices i together into
a block where the triangle relaxation for yi:=ReLU(xi) has the larger area in the xiyi-plane.
Computing Pkrelu i We note that for a ﬁxed block Ji  several polyhedron Pk-ReLU i are possible that
produce convex relaxations with varying degree of precision. Ideally  one would like Pk-ReLU i to be
the projection of S onto the variables in the set X indexed by the block Ji. However  computing this
projection exactly is expensive and therefore we compute a relaxation of it.
We use the method M to compute Pk-ReLU i by computing the upper bounds for linear relational
u=1 au · xu with respect to S. In our experiments  we found that setting
au ∈ {−1  0  1} yields maximum precision (except the case where all possible au are zero). Thus
Pk-ReLU i ⊇ S contains 3k − 1 constraints which include the interval constraints for all xu.

expressions of the form(cid:80)k

4.2 Veriﬁcation with k-ReLU framework
Let ψ ⊆ Rh be a convex set deﬁning a safe region for the outputs with respect to the input region I
and SO ⊆ Rh be the output convex relaxation obtained after processing afﬁne layers with the convex
approximation method M and ReLU layers with our k-ReLU framework. ψ holds if ψ ⊆ SO.

4.3

Instantiation with DeepPoly

≥

≤
i   a

i are of the form(cid:80)

We now show to instantiate the k-ReLU framework with DeepPoly [9]. DeepPoly is a type of a
restricted Polyhedra abstraction which balances scalability and precision of the analysis. It associates
≤
four constraints per neuron hi ∈ H: (a) a lower polyhedral constraint of the form a
i ≤ hi  (b) an
≥
i   (c) a lower bound constraint li ≤ hi  and (d) an upper bound
upper polyhedral constraint hi ≤ a
constraint hi ≤ ui. The polyhedral expressions a
j aj ·hj +c where aj  c ∈ R
and capture relational information ensuring that DeepPoly is exact for afﬁne transformations. The
analysis proceeds layer by layer and thus the polyhedral constraints for a neuron in layer l contain
only the neurons upto layer l-1. S here is the set of points satisfying DeepPoly constraints for all
neurons. We next discuss how the k-ReLU framework can be used for improving the precision of the
ReLU transformer for DeepPoly and also that of the overall veriﬁcation procedure.
Improving the precision of DeepPoly ReLU relaxation DeepPoly loses precision for ReLU as-
signments yi:=ReLU(xi) where xi can take both positive and negative values. It computes convex
relaxations shown in Fig. 4 (a) and (b). It keeps the one with smaller area in the xiyi-plane.We
note that both of these relaxations depend only on the interval bounds li  ui for xi. DeepPoly uses
backsubstitution (see [9] for details) for obtaining precise bounds li  ui. We note that DeepPoly

7

Table 2: Neural network architectures and parameters used in our experiments.

Dataset

Model

Type

#Neurons

#Layers

Defense Reﬁne
ReLU

k

MNIST

6 × 100
9 × 100
6 × 200
9 × 200
ConvSmall
ConvBig

CIFAR10 ConvSmall

ConvBig
ResNet

fully connected
fully connected
fully connected
fully connected
convolutional
convolutional

convolutional
convolutional
Residual

610
910
1 210
1 810
3 604
34 688

4 852
62 464
107 496

None
6
None
9
None
6
None
9
None
3
6 DiffAI [29]

PGD [30]
3
6
PGD [30]
13 Wong [11]

3

2

2

2

 Adaptive
5


 Adaptive
5

 Adaptive

i  u(cid:48)

ReLU relaxations are weaker than the 1-ReLU relaxation (as they contain one constraint less than the
triangle). This precision loss accumulates as the analysis proceeds deeper in the network. We now
show how the k-ReLU framework can recover precision for DeepPoly.
We compute reﬁned bounds l(cid:48)
i for those neurons xi that are inputs to a ReLU and can take positive
values. We maximize and minimize xi with respect to the convex relaxation produced by replacing
the DeepPoly ReLU relaxation (Fig. 4 (a) and (b)) with our k-ReLU relaxations based on (6). Since
the constraints from both DeepPoly and k-ReLU are linear  we use a LP solver for maximizing and
minimizing. l(cid:48)
i facilitate the two DeepPoly ReLU relaxations shown in green in Fig. 4 (a) and
(b). These relaxations are tighter than the original ones and improve the precision of DeepPoly.
k-ReLU for improving robustness certiﬁcation When DeepPoly alone cannot prove the target
property  we instead check if ψ holds with the tighter ReLU relaxations from k-ReLU via LP solver.

i and u(cid:48)

5 Evaluation

We instantiated our k-ReLU framework with DeepPoly in the form of a veriﬁer called kPoly. kPoly
is written in Python and uses cdd [25  26] for computing convex hulls  and Gurobi [27] for reﬁning
DeepPoly ReLU relaxations and proving that ψ holds with k-ReLU relaxations. We made kPoly
publicly available as part of the ERAN [28] framework for neural network veriﬁcation. We evaluated
kPoly for the task of robustness certiﬁcation of challenging deep neural networks. We compare kPoly
against two state-of-the-art veriﬁers: DeepPoly [9] and ReﬁneZono [19]. DeepPoly has the same
precision as [15  16] whereas ReﬁneZono reﬁnes the results of DeepZ [8] and is more precise than
[8  11  14]. Both  DeepPoly and ReﬁneZono are more scalable than [5  18  12  4  13  10]  however we
show that kPoly is more precise than DeepPoly and ReﬁneZono while also scaling to large networks.
We next describe the neural networks  benchmarks and parameters used in our experiments.
Neural networks We used 9 MNIST [31] and CIFAR10 [32] fully connected (FNNs)  convolu-
tional (CNNs)  and residual networks with ReLU activations shown in Table 2. The ﬁrst 8 net-
works in Table 2 are available at https://github.com/eth-sri/eran; the residual network is taken from
https://github.com/locuslab/convex_adversarial. Five of the networks do not use adversarial training
while the rest use different variants of it. The MNIST ConvBig network is trained with DiffAI [29] 
the two CIFAR10 convolutional networks are trained with PGD [30] and the residual network is
trained via [11]. The largest network in our experiments contains > 100K neurons and has 13 layers.
Robustness property We consider the L∞-norm [33] based adversarial region around a correctly
classiﬁed image from the test set parameterized by the radius  ∈ R. Our goal is to certify that the
network classiﬁes all images in the adversarial region correctly.
Machines The runtimes of all experiments for the MNIST FNNs were measured on a 3.3 GHz 10
Core Intel i9-7900X Skylake CPU with a main memory of 64 GB whereas the experiments for the
rest were run on a 2.6 GHz 14 core Intel Xeon CPU E5-2690 with 512 GB of main memory.

8

Table 3: Number of veriﬁed adversarial regions and runtime of kPoly vs. DeepPoly and ReﬁneZono.

Dataset

Model

#correct



DeepPoly[9]

ReﬁneZono [19]

kPoly

MNIST

6 × 100
9 × 100
6 × 200
9 × 200
ConvSmall
ConvBig

CIFAR10 ConvSmall

ConvBig
ResNet

960
947
972
950
980
929

630
631
290

0.026
0.026
0.015
0.015
0.12
0.3

2/255
2/255
8/255

veriﬁed(#)

time(s)

veriﬁed(#)

time(s)

veriﬁed(#)

time(s)

160
182
292
259
158
711

359
421
243

0.3
0.4
0.5
0.9
3
21

4
43
12

312
304
341
316
179
648

347
305
243

310
411
570
860
707
285

716
592
27

441
369
574
506
347
736

399
459
245

307
171
187
464
477
40

86
346
91

Benchmarks For each MNIST and CIFAR10 network  we selected the ﬁrst 1000 images from the
respective test set and ﬁltered out incorrectly classiﬁed images. The number of correctly classiﬁed
images by each network are shown in Table 3. We chose challenging  values for deﬁning the
adversarial region for each network. We note that our benchmarks (e.g.  the 9 × 200 network with
 = 0.015) are quite challenging to handle for state-of-the-art veriﬁers (as we will see below).
k-ReLU parameters for the experiments We reﬁne both the DeepPoly ReLU relaxation and the
veriﬁcation results for the MNIST FNNs whereas only the veriﬁcation results are reﬁned for the rest.
All neurons that can take positive values after the afﬁne transformation are selected for reﬁnement.
As an optimization  we use the MILP ReLU encoding from [5] when reﬁning the ReLU relaxation
for the second ReLU layer. The last column of Table 2 shows the value of k for all networks. For the
MNIST and CIFAR10 ConvBig networks  we encode the ﬁrst 3 ReLU layers with 1-ReLU while the
remaining are encoded with 5-ReLU. We use l = 3 in (7) for encoding 5-ReLU. For the remaining
3 networks  we encode the ﬁrst ReLU layer with 1-ReLU while the remaining layers are encoded
adaptively. Here  we choose a value of k for which the total number of calls to 3-ReLU is ≤ 500. We
next discuss our experimental results shown in Table 3.
kPoly vs DeepPoly and ReﬁneZono Table 3 compares the precision in number of adversarial regions
veriﬁed and the average runtime per image in seconds for kPoly  DeepPoly and ReﬁneZono. We
reﬁne the veriﬁcation results with ReﬁneZono and kPoly only when DeepZ and DeepPoly fails to
verify. kPoly is more precise than both DeepPoly and ReﬁneZono on all networks. ReﬁneZono is
more precise than DeepPoly on the networks trained without adversarial training. On the 9 × 200 and
MNIST ConvSmall networks  kPoly veriﬁes 506 and 347 regions respectively whereas ReﬁneZono
veriﬁes 316 and 179 regions respectively. The precision gain is less on networks with adversarial
training and kPoly veriﬁes 25  40  38 and 2 regions more than DeepPoly on the last 4 networks in
Table 3. kPoly is faster than ReﬁneZono on all networks and has an average runtime of < 8 minutes.
The largest runtimes are on the MNIST 9 × 200 and ConvSmall networks. These are quite small
compared to the CIFAR10 ResNet network where kPoly has an average runtime of only 91 seconds.
1-ReLU vs k-ReLU We consider the ﬁrst 100 regions for the MNIST ConvSmall network and
compare the number of regions veriﬁed by kPoly when run with k-ReLU and 1-ReLU. We note that
kPoly run with 1-ReLU is equivalent to [17]. kPoly with 1-ReLU veriﬁes 20 regions whereas with
k-ReLU it veriﬁes 35. kPoly with 1-ReLU has an average runtime of 9 seconds.
Effect of heuristic for J We ran kPoly based on k-ReLU with random partitioning Jr using the
same setup as for 1-ReLU. We observed that kPoly produced worse bounds and veriﬁed 34 regions.

6 Conclusion

We presented k-ReLU  a novel parametric framework which produces more precise results than the
single neuron triangle convex relaxation. The key idea of k-ReLU is to consider multiple ReLUs
jointly. We showed k-ReLU leads to signiﬁcantly improved precision  enabling us to prove properties
beyond the reach of prior work  while preserving scalability.

9

References
[1] C. Szegedy  W. Zaremba  I. Sutskever  J. Bruna  D. Erhan  I. Goodfellow  and R. Fergus 

“Intriguing properties of neural networks ” arXiv preprint arXiv:1312.6199  2013.

[2] G. Katz  C. W. Barrett  D. L. Dill  K. Julian  and M. J. Kochenderfer  “Reluplex: An efﬁcient
SMT solver for verifying deep neural networks ” in Computer Aided Veriﬁcation - 29th Inter-
national Conference  CAV 2017  Heidelberg  Germany  July 24-28  2017  Proceedings  Part I 
2017.

[3] R. Ehlers  “Formal veriﬁcation of piece-wise linear feed-forward neural networks ” in Automated

Technology for Veriﬁcation and Analysis (ATVA)  2017.

[4] R. Bunel  I. Turkaslan  P. H. Torr  P. Kohli  and M. P. Kumar  “A uniﬁed view of piecewise
linear neural network veriﬁcation ” in Proc. Advances in Neural Information Processing Systems
(NeurIPS)  2018  pp. 4795–4804.

[5] V. Tjeng  K. Y. Xiao  and R. Tedrake  “Evaluating robustness of neural networks with mixed
integer programming ” in International Conference on Learning Representations  (ICLR)  2019.

[6] W. Ruan  X. Huang  and M. Kwiatkowska  “Reachability analysis of deep neural networks with
provable guarantees ” in Proc. International Joint Conference on Artiﬁcial Intelligence  (IJCAI) 
2018.

[7] T. Gehr  M. Mirman  D. Drachsler-Cohen  P. Tsankov  S. Chaudhuri  and M. Vechev  “AI2:
Safety and robustness certiﬁcation of neural networks with abstract interpretation ” in Proc.
IEEE Symposium on Security and Privacy (SP)  vol. 00  2018  pp. 948–963.

[8] G. Singh  T. Gehr  M. Mirman  M. Püschel  and M. Vechev  “Fast and effective robustness
certiﬁcation ” in Proc. Advances in Neural Information Processing Systems (NeurIPS)  2018 
pp. 10 825–10 836.

[9] G. Singh  T. Gehr  M. Püschel  and M. Vechev  “An abstract domain for certifying neural

networks ” Proc. ACM Program. Lang.  vol. 3  no. POPL  pp. 41:1–41:30  2019.

[10] K. Dvijotham  R. Stanforth  S. Gowal  T. Mann  and P. Kohli  “A dual approach to scalable
veriﬁcation of deep networks ” in Proc. Uncertainty in Artiﬁcial Intelligence (UAI)  2018  pp.
162–171.

[11] E. Wong and J. Z. Kolter  “Provable defenses against adversarial examples via the convex outer

adversarial polytope ” arXiv preprint arXiv:1711.00851  2017.

[12] A. Raghunathan  J. Steinhardt  and P. S. Liang  “Semideﬁnite relaxations for certifying robust-
ness to adversarial examples ” in Advances in Neural Information Processing Systems (NeurIPS) 
2018  pp. 10 877–10 887.

[13] K. D. Dvijotham  R. Stanforth  S. Gowal  C. Qin  S. De  and P. Kohli  “Efﬁcient neural network
veriﬁcation with exactness characterization ” in Proc. Uncertainty in Artiﬁcial Intelligence  UAI 
2019  p. 164.

[14] L. Weng  H. Zhang  H. Chen  Z. Song  C.-J. Hsieh  L. Daniel  D. Boning  and I. Dhillon 
“Towards fast computation of certiﬁed robustness for ReLU networks ” in Proc. International
Conference on Machine Learning (ICML)  vol. 80  2018  pp. 5276–5285.

[15] H. Zhang  T.-W. Weng  P.-Y. Chen  C.-J. Hsieh  and L. Daniel  “Efﬁcient neural network robust-
ness certiﬁcation with general activation functions ” in Proc. Advances in Neural Information
Processing Systems (NeurIPS)  2018.

[16] A. Boopathy  T.-W. Weng  P.-Y. Chen  S. Liu  and L. Daniel  “Cnn-cert: An efﬁcient framework
for certifying robustness of convolutional neural networks ” in AAAI Conference on Artiﬁcial
Intelligence (AAAI)  Jan 2019.

[17] H. Salman  G. Yang  H. Zhang  C. Hsieh  and P. Zhang  “A convex relaxation barrier to tight

robustness veriﬁcation of neural networks ” CoRR  vol. abs/1902.08722  2019.

10

[18] S. Wang  K. Pei  J. Whitehouse  J. Yang  and S. Jana  “Efﬁcient formal safety analysis of neural
networks ” in Proc. Advances in Neural Information Processing Systems (NeurIPS)  2018  pp.
6369–6379.

[19] G. Singh  T. Gehr  M. Püschel  and M. Vechev  “Boosting robustness certiﬁcation of neural

networks ” in International Conference on Learning Representations  2019.

[20] P. Cousot and N. Halbwachs  “Automatic discovery of linear restraints among variables of a

program ” in Proc. Principles of Programming Languages (POPL)  1978  pp. 84–96.

[21] Y. Yang and M. Rinard  “Correctness veriﬁcation of neural networks ” 2019.

[22] M. Balunovic  M. Baader  G. Singh  T. Gehr  and M. Vechev  “Certifying geometric robustness
of neural networks ” in Advances in Neural Information Processing Systems (NeurIPS)  2019 
pp. 15 287–15 297.

[23] J. Mohapatra  Tsui-Wei  Weng  P.-Y. Chen  S. Liu  and L. Daniel  “Towards verifying robustness

of neural networks against semantic perturbations ” 2019.

[24] G. Singh  M. Püschel  and M. Vechev  “Fast polyhedra abstract domain ” in ACM SIGPLAN

Notices. ACM  2017.

[25] “Extended convex hull ” Computational Geometry  vol. 20  no. 1  pp. 13 – 23  2001.

[26] “pycddlib ” 2018. [Online]. Available: https://pypi.org/project/pycddlib/

[27] Gurobi Optimization  LLC  “Gurobi optimizer reference manual ” 2018. [Online]. Available:

http://www.gurobi.com

[28] “ERAN: ETH Robustness Analyzer for Neural Networks ” 2018. [Online]. Available:

https://github.com/eth-sri/eran

[29] M. Mirman  T. Gehr  and M. Vechev  “Differentiable abstract interpretation for provably robust
neural networks ” in Proc. International Conference on Machine Learning (ICML)  2018  pp.
3575–3583.

[30] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu  “Towards deep learning models
resistant to adversarial attacks ” in Proc. International Conference on Learning Representations
(ICLR)  2018.

[31] Y. Lecun  L. Bottou  Y. Bengio  and P. Haffner  “Gradient-based learning applied to document

recognition ” in Proc. of the IEEE  1998  pp. 2278–2324.

[32] A. Krizhevsky  “Learning multiple layers of features from tiny images ” Tech. Rep.  2009.

[33] N. Carlini and D. A. Wagner  “Towards evaluating the robustness of neural networks ” in Proc.

IEEE Symposium on Security and Privacy (SP)  2017  pp. 39–57.

11

A Appendix

A.1 Proof of Theorem 3.1

Proof. Since Pk-ReLU i (cid:36)(cid:84)

P1-ReLU u for Ji  by monotonicity of intersection and convex hull 

(cid:92)

ConvQ∈QJi

u∈Ji
(Pk-ReLU i ∩ Q) (cid:36) ConvQ∈QJi
For any Q ∈ QJi  we have that either Q ⊆ C+
u or Q ⊆ C−
on the right hand side of (8) with either C+
u or C−
u such that for all u ∈ Ji both C+
(cid:92)
at least in one substitution and obtain by monotonicity 
u   P1-ReLU u ∩ C−
u )

⊆ Convu∈Ji(P1-ReLU u ∩ C+

P1-ReLU u) ∩ C+
u   (

P1-ReLU u) ∩ C−
u )

⊆ Convu∈Ji((

P1-ReLU u) ∩ Q)

u∈Ji

(cid:92)

u∈Ji

((

u∈Ji

(8)
u for u ∈ Ji. Thus  we can replace all Q
u are used

u and C−

(cid:92)

(
u∈Ji

P1-ReLU u ⊆ P1-ReLU u).

For remaining i  similarly ConvQ∈Qi(Pk-ReLU i ∩ Q) ⊆ Convu∈Ji(P1-ReLU u ∩ C+
holds. Since (cid:36) relation holds for at least one i and ⊆ holds for others  Sk-ReLU (cid:36) S1-ReLU holds.

u   P1-ReLU u ∩ C−
u )

12

,Gagandeep Singh
Rupanshu Ganvir
Markus Püschel
Martin Vechev