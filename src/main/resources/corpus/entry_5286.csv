2013,Streaming Variational Bayes,We present SDA-Bayes  a framework for (S)treaming  (D)istributed  (A)synchronous computation of a Bayesian posterior. The framework makes streaming updates to the estimated posterior according to a user-specified approximation primitive function.  We demonstrate the usefulness of our framework  with variational Bayes (VB) as the primitive  by fitting the latent Dirichlet allocation model to two large-scale document collections.  We demonstrate the advantages of our algorithm over stochastic variational inference (SVI)  both in the single-pass setting SVI was designed for and in the streaming setting  to which SVI does not apply.,Streaming Variational Bayes

Tamara Broderick  Nicholas Boyd  Andre Wibisono  Ashia C. Wilson

University of California  Berkeley

{tab@stat  nickboyd@eecs  wibisono@eecs  ashia@stat}.berkeley.edu

Michael I. Jordan

University of California  Berkeley
jordan@cs.berkeley.edu

Abstract

a

(S)treaming 

framework for

We present SDA-Bayes 
(D)istributed 
(A)synchronous computation of a Bayesian posterior. The framework makes
streaming updates to the estimated posterior according to a user-speciﬁed approx-
imation batch primitive. We demonstrate the usefulness of our framework  with
variational Bayes (VB) as the primitive  by ﬁtting the latent Dirichlet allocation
model to two large-scale document collections. We demonstrate the advantages
of our algorithm over stochastic variational inference (SVI) by comparing the two
after a single pass through a known amount of data—a case where SVI may be
applied—and in the streaming setting  where SVI does not apply.

1

Introduction

Large  streaming data sets are increasingly the norm in science and technology. Simple descriptive
statistics can often be readily computed with a constant number of operations for each data point in
the streaming setting  without the need to revisit past data or have advance knowledge of future data.
But these time and memory restrictions are not generally available for the complex  hierarchical
models that practitioners often have in mind when they collect large data sets. Signiﬁcant progress
on scalable learning procedures has been made in recent years [e.g.  1  2]. But the underlying
models remain simple  and the inferential framework is generally non-Bayesian. The advantages
of the Bayesian paradigm (e.g.  hierarchical modeling  coherent treatment of uncertainty) currently
seem out of reach in the Big Data setting.
An exception to this statement is provided by [3–5]  who have shown that a class of approxima-
tion methods known as variational Bayes (VB) [6] can be usefully deployed for large-scale data
sets. They have applied their approach  referred to as stochastic variational inference (SVI)  to the
domain of topic modeling of document collections  an area with a major need for scalable infer-
ence algorithms. VB traditionally uses the variational lower bound on the marginal likelihood as an
objective function  and the idea of SVI is to apply a variant of stochastic gradient descent to this
objective. Notably  this objective is based on the conceptual existence of a full data set involving D
data points (i.e.  documents in the topic model setting)  for a ﬁxed value of D. Although the stochas-
tic gradient is computed for a single  small subset of data points (documents) at a time  the posterior
being targeted is a posterior for D data points. This value of D must be speciﬁed in advance and is
used by the algorithm at each step. Posteriors for D! data points  for D! != D  are not obtained as
part of the analysis.
We view this lack of a link between the number of documents that have been processed thus far
and the posterior that is being targeted as undesirable in many settings involving streaming data.
In this paper we aim at an approximate Bayesian inference algorithm that is scalable like SVI but

1

is also truly a streaming procedure  in that it yields an approximate posterior for each processed
collection of D! data points—and not just a pre-speciﬁed “ﬁnal” number of data points D. To that
end  we return to the classical perspective of Bayesian updating  where the recursive application
of Bayes theorem provides a sequence of posteriors  not a sequence of approximations to a ﬁxed
posterior. To this classical recursive perspective we bring the VB framework; our updates need
not be exact Bayesian updates but rather may be approximations such as VB. This approach is
similar in spirit to assumed density ﬁltering or expectation propagation [7–9]  but each step of those
methods involves a moment-matching step that can be computationally costly for models such as
topic models. We are able to avoid the moment-matching step via the use of VB. We also note other
related work in this general vein: MCMC approximations have been explored by [10]  and VB or
VB-like approximations have also been explored by [11  12].
Although the empirical success of SVI is the main motivation for our work  we are also motivated by
recent developments in computer architectures  which permit distributed and asynchronous compu-
tations in addition to streaming computations. As we will show  a streaming VB algorithm naturally
lends itself to distributed and asynchronous implementations.

2 Streaming  distributed  asynchronous Bayesian updating

Streaming Bayesian updating. Consider data x1  x2  . . . generated iid according to a distribution
p(x | Θ) given parameter(s) Θ. Assume that a prior p(Θ) has also been speciﬁed. Then Bayes theo-
rem gives us the posterior distribution of Θ given a collection of S data points  C1 := (x1  . . .   xS):

p(Θ | C1) = p(C1)−1 p(C1 | Θ) p(Θ) 

s=1 p(xs | Θ).

p(Θ | C1  . . .   Cb) ∝ p(Cb | Θ) p(Θ | C1  . . .   Cb−1).

where p(C1 | Θ) = p(x1  . . .   xS | Θ) =!S
Suppose we have seen and processed b−1 collections  sometimes called minibatches  of data. Given
the posterior p(Θ | C1  . . .   Cb−1)  we can calculate the posterior after the bth minibatch:
(1)
That is  we treat the posterior after b − 1 minibatches as the new prior for the incoming data points.
If we can save the posterior from b − 1 minibatches and calculate the normalizing constant for the
bth posterior  repeated application of Eq. (1) is streaming; it automatically gives us the new posterior
without needing to revisit old data points.
In complex models  it is often infeasible to calculate the posterior exactly  and an approximation
must be used. Suppose that  given a prior p(Θ) and data minibatch C  we have an approximation
algorithm A that calculates an approximate posterior q: q(Θ) = A(C  p(Θ)). Then  setting q0(Θ) =
p(Θ)  one way to recursively calculate an approximation to the posterior is
p(Θ | C1  . . .   Cb) ≈ qb(Θ) = A (Cb  qb−1(Θ)) .
(2)
When A yields the posterior from Bayes theorem  this calculation is exact. This approach already
differs from that of [3–5]  which we will see (Sec. 3.2) directly approximates p(Θ | C1  . . .   CB)
for ﬁxed B without making intermediate approximations for b strictly between 1 and B.
Distributed Bayesian updating. The sequential updates in Eq. (2) handle streaming data in theory 
but in practice  the A calculation might take longer than the time interval between minibatch arrivals
or simply take longer than desired. Parallelizing computations increases algorithm throughput. And
posterior calculations need not be sequential. Indeed  Bayes theorem yields

p(Θ | C1  . . .   CB) ∝" B#b=1

p(Cb | Θ)$ p(Θ) ∝" B#b=1

p(Θ | Cb) p(Θ)−1$ p(Θ).

That is  we can calculate the individual minibatch posteriors p(Θ | Cb)  perhaps in parallel  and then
combine them to ﬁnd the full posterior p(Θ | C1  . . .   CB).
Given an approximating algorithm A as above  the corresponding approximate update would be

p(Θ | C1  . . .   CB) ≈ q(Θ) ∝" B#b=1

A(Cb  p(Θ)) p(Θ)−1$ p(Θ) 

(3)

(4)

2

for some approximating distribution q  provided the normalizing constant for the right-hand side of
Eq. (4) can be computed.
Variational inference methods are generally based on exponential family representations [6]  and we
will make that assumption here. In particular  we suppose p(Θ) ∝ exp{ξ0 · T (Θ)}; that is  p(Θ) is
an exponential family distribution for Θ with sufﬁcient statistic T (Θ) and natural parameter ξ0. We
suppose further that A always returns a distribution in the same exponential family; in particular  we
suppose that there exists some parameter ξb such that
(5)
for

qb(Θ) ∝ exp{ξb · T (Θ)}

When we make these two assumptions  the update in Eq. (4) becomes

p(Θ | C1  . . .   CB) ≈ q(Θ) ∝ exp%"ξ0 +

qb(Θ) = A(Cb  p(Θ)).
B&b=1

(ξb − ξ0)$ · T (Θ)’  

(6)

where the normalizing constant is readily obtained from the exponential family form. In what fol-
lows we use the shorthand ξ ←A (C  ξ0) to denote that A takes as input a minibatch C and a prior
with exponential family parameter ξ0 and that it returns a distribution in the same exponential family
with parameter ξ.
So  to approximate p(Θ | C1  . . .   CB)  we ﬁrst calculate ξb via the approximation primitive A for
each minibatch Cb; note that these calculations may be performed in parallel. Then we sum together
the quantities ξb − ξ0 across b  along with the initial ξ0 from the prior  to ﬁnd the ﬁnal exponential
family parameter to the full posterior approximation q. We previously saw that the general Bayes
sequential update can be made streaming by iterating with the old posterior as the new prior (Eq. (2)).
Similarly  here we see that the full posterior approximation q is in the same exponential family as
the prior  so one may iterate these parallel computations to arrive at a parallelized algorithm for
streaming posterior computation.
We emphasize that while these updates are reminiscent of prior-posterior conjugacy  it is actually
the approximate posteriors and single  original prior that we assume belong to the same exponential
family. It is not necessary to assume any conjugacy in the generative model itself nor that any true
intermediate or ﬁnal posterior take any particular limited form.
Asynchronous Bayesian updating. Performing B computations in parallel can in theory speed up
algorithm running time by a factor of B  but in practice it is often the case that a single computation
thread takes longer than the rest. Waiting for this thread to ﬁnish diminishes potential gains from
distributing the computations. This problem can be ameliorated by making computations asyn-
chronous. In this case  processors known as workers each solve a subproblem. When a worker
ﬁnishes  it reports its solution to a single master processor. If the master gives the worker a new
subproblem without waiting for the other workers to ﬁnish  it can decrease downtime in the system.
Our asynchronous algorithm is in the spirit of Hogwild! [1]. To present the algorithm we ﬁrst
describe an asynchronous computation that we will not use in practice  but which will serve as a
conceptual stepping stone. Note in particular that the following scheme makes the computations
in Eq. (6) asynchronous. Have each worker continuously iterate between three steps: (1) collect
a new minibatch C  (2) compute the local approximate posterior ξ ←A (C  ξ0)  and (3) return
∆ξ := ξ − ξ0 to the master. The master  in turn  starts by assigning the posterior to equal the prior:
ξ(post) ← ξ0. Each time the master receives a quantity ∆ξ from any worker  it updates the posterior
synchronously: ξ(post) ← ξ(post) +∆ ξ. If A returns the exponential family parameter of the true
posterior (rather than an approximation)  then the posterior at the master is exact by Eq. (4).
A preferred asynchronous computation works as follows. The master initializes its posterior estimate
to the prior: ξ(post) ← ξ0. Each worker continuously iterates between four steps: (1) collect a new
minibatch C  (2) copy the master posterior value locally ξ(local) ← ξ(post)  (3) compute the local
approximate posterior ξ ←A (C  ξ(local))  and (4) return ∆ξ := ξ − ξ(local) to the master. Each
time the master receives a quantity ∆ξ from any worker  it updates the posterior synchronously:
ξ(post) ← ξ(post) +∆ ξ.
The key difference between the ﬁrst and second frameworks proposed above is that  in the second 
the latest posterior is used as a prior. This latter framework is more in line with the streaming update
of Eq. (2) but introduces a new layer of approximation. Since ξ(post) might change at the master

3

while the worker is computing ∆ξ  it is no longer the case that the posterior at the master is exact
when A returns the exponential family parameter of the true posterior. Nonetheless we ﬁnd that the
latter framework performs better in practice  so we focus on it exclusively in what follows.
We refer to our overall framework as SDA-Bayes  which stands for (S)treaming  (D)istributed 
(A)synchronous Bayes. The framework is intended to be general enough to allow a variety of local
approximations A. Indeed  SDA-Bayes works out of the box once an implementation of A—and a
prior on the global parameter(s) Θ—is provided. In the current paper our preferred local approxi-
mation will be VB.

3 Case study: latent Dirichlet allocation
In what follows  we consider examples of the choices for the Θ prior and primitive A in the context
of latent Dirichlet allocation (LDA) [13]. LDA models the content of D documents in a corpus.
Themes potentially shared by multiple documents are described by topics. The unsupervised learn-
ing problem is to learn the topics as well as discover which topics occur in which documents.
More formally  each topic (of K total topics) is a distribution over the V words in the vocabulary:
βk = (βkv)V
v=1. Each document is an admixture of topics. The words in document d are assumed
to be exchangeable. Each word wdn belongs to a latent topic zdn chosen according to a document-
speciﬁc distribution of topics θd = (θdk)K
k=1. The full generative model  with Dirichlet priors for
βk and θd conditioned on respective parameters ηk and α  appears in [13].
To see that this model ﬁts our speciﬁcation in Sec. 2  consider the set of global parameters Θ=
β. Each document wd = (wdn)Nd
n=1 is distributed iid conditioned on the global topics. The full
collection of data is a corpus C = w = (wd)D
d=1 of documents. The posterior for LDA  p(β  θ  z |
C  η  α)  is equal to the following expression up to proportionality:

∝" K#k=1

Dirichlet(βk | ηk)$ ·" D#d=1

Dirichlet(θd | α)$ ·" D#d=1

Nd#n=1

θdzdnβzdn wdn$ .

(7)

The posterior for just the global parameters p(β| C  η  α) can be obtained from p(β  θ  z| C  η  α) by
integrating out the local  document-speciﬁc parameters θ  z. As is common in complex models  the
normalizing constant for Eq. (7) is intractable to compute  so the posterior must be approximated.

3.1 Posterior-approximation algorithms

To apply SDA-Bayes to LDA  we use the prior speciﬁed by the generative model. It remains to
choose a posterior-approximation algorithm A. We consider two possibilities here: variational
Bayes (VB) and expectation propagation (EP). Both primitives take Dirichlet distributions as priors
for β and both return Dirichlet distributions for the approximate posterior of the topic parameters β;
thus the prior and approximate posterior are in the same exponential family. Hence both VB and EP
can be utilized as a choice for A in the SDA-Bayes framework.
Mean-ﬁeld variational Bayes. We use the shorthand pD for Eq. (7)  the posterior given D docu-
ments. We assume the approximating distribution  written qD for shorthand  takes the form

Nd#n=1

qD(θd | γd)$ ·" D#d=1

qD(βk | λk)$ ·" D#d=1

qD(β  θ  z | λ  γ  φ) =" K#k=1

qD(zdn | φdwdn)$ (8)
for parameters (λkv)  (γdk)  (φdvk) with k ∈{ 1  . . .   K}  v ∈{ 1  . . .   V }  d ∈{ 1  . . .   D}.
Moreover  we set qD(βk | λk) = DirichletV (βk | λk)  qD(θd | γd) = DirichletK(θd | γd)  and
qD(zdn | φdwdn) = CategoricalK(zdn | φdwdn). The subscripts on Dirichlet and Categorical indicate
the dimensions of the distributions (and of the parameters).
The problem of VB is to ﬁnd the best approximating qD  deﬁned as the collection of variational
parameters λ  γ  φ that minimize the KL divergence from the true posterior: KL (qD ’ pD). Even
ﬁnding the minimizing parameters is a difﬁcult optimization problem. Typically the solution is
approximated by coordinate descent in each parameter [6  13] as in Alg. 1. The derivation of VB for
LDA can be found in [4  13] and Sup. Mat. A.1.

4

d=1; hyperparameters η  α

Algorithm 1: VB for LDA
Input: Data (nd)D
Output: λ
Initialize λ
while (λ  γ  φ) not converged do

for d = 1  . . .   D do

(γd φ d) ← LocalVB(d  λ)
∀(k  v)  λkv ← ηkv +(D

d=1 φdvkndv

Algorithm 2: SVI for LDA
Input: Hyperparameters η  α  D  (ρt)T
Output: λ
Initialize λ
for t = 1  . . .   T do

t=1

(γd φ d) ← LocalVB(d  λ)

Collect new data minibatch C
foreach document indexed d in C do
∀(k  v)  ˜λkv ← ηkv + D
∀(k  v)  λkv ← (1 − ρt)λkv + ρt˜λkv

|C|(d in C φdvkndv

Subroutine LocalVB(d  λ)

Output: (γd φ d)
Initialize γd
while (γd φ d) not converged do

∀(k  v)  set φdvk ∝
exp (Eq[log θdk] + Eq[log βkv])
(normalized across k)
∀k  γdk ← αk +(V

v=1 φdvkndv

Algorithm 3: SSU for LDA
Input: Hyperparameters η  α
Output: A sequence λ(1) λ (2)  . . .
Initialize ∀(k  v)  λ(0)
for b = 1  2  . . . do

kv ← ηkv

Collect new data minibatch C
foreach document indexed d in C do

(γd φ d) ← LocalVB(d  λ)

∀(k  v)  λ(b)

kv ← λ(b−1)

kv +(d in C φdvkndv

Figure 1: Algorithms for calculating λ  the parameters for the topic posteriors in LDA. VB iter-
ates multiple times through the data  SVI makes a single pass  and SSU is streaming. Here  ndv
represents the number of words v in document d.
Expectation propagation. An EP [7] algorithm for approximating the LDA posterior appears in
Alg. 6 of Sup. Mat. B. Alg. 6 differs from [14]  which does not provide an approximate posterior for
the topic parameters  and is instead our own derivation. Our version of EP  like VB  learns factorized
Dirichlet distributions over topics.

3.2 Other single-pass algorithms for approximate LDA posteriors

The algorithms in Sec. 3.1 pass through the data multiple times and require storing the data set in
memory—but are useful as primitives for SDA-Bayes in the context of the processing of minibatches
of data. Next  we consider two algorithms that can pass through a data set just one time (single pass)
and to which we compare in the evaluations (Sec. 4).
Stochastic variational inference. VB uses coordinate descent to ﬁnd a value of qD  Eq. (8)  that
locally minimizes the KL divergence  KL (qD ’ pD). Stochastic variational inference (SVI) [3  4]
is exactly the application of a particular version of stochastic gradient descent to the same optimiza-
tion problem. While stochastic gradient descent can often be viewed as a streaming algorithm  the
optimization problem itself here depends on D via pD  the posterior on D data points. We see that 
as a result  D must be speciﬁed in advance  appears in each step of SVI (see Alg. 2)  and is indepen-
dent of the number of data points actually processed by the algorithm. Nonetheless  while one may
choose to visit D! != D data points or revisit data points when using SVI to estimate pD [3  4]  SVI
can be made single-pass by visiting each of D data points exactly once and then has constant mem-
ory requirements. We also note that two new parameters  τ0 > 0 and κ ∈ (0.5  1]  appear in SVI 
beyond those in VB  to determine a learning rate ρt as a function of iteration t: ρt := (τ0 + t)−κ.
Sufﬁcient statistics. On each round of VB (Alg. 1)  we update the local parameters for all doc-
uments and then compute λkv ← ηkv +(D
d=1 φdvkndv. An alternative single-pass (and indeed
streaming) option would be to update the local parameters for each minibatch of documents as they
arrive and then add the corresponding terms φdvkndv to the current estimate of λ for each document
d in the minibatch. This essential idea has been proposed previously for models other than LDA by
[11  12] and forms the basis of what we call the sufﬁcient statistics update algorithm (SSU): Alg. 3.
This algorithm is equivalent to SDA-Bayes with A chosen to be a single iteration over the global
variable λ of VB (i.e.  updating λ exactly once instead of iterating until convergence).

5

Wikipedia

Nature

Log pred prob
Time (hours)

32-SDA 1-SDA
−7.31
2.09

SSU
−7.43 −7.32 −7.91
8.28
43.93

7.87

SVI

32-SDA 1-SDA
−7.11
0.55

SSU
−7.19 −7.08 −7.82
1.27
10.02

1.22

SVI

Table 1: A comparison of (1) log predictive probability of held-out data and (2) running time of
four algorithms: SDA-Bayes with 32 threads  SDA-Bayes with 1 thread  SVI  and SSU.

4 Evaluation

We follow [4] (and further [15  16]) in evaluating our algorithms by computing (approximate) pre-
dictive probability. Under this metric  a higher score is better  as a better model will assign a higher
probability to the held-out words.
We calculate predictive probability by ﬁrst setting aside held-out testing documents C(test) from the
full corpus and then further setting aside a subset of held-out testing words Wd test in each testing
document d. The remaining (training) documents C(train) are used to estimate the global parameter
posterior q(β)  and the remaining (training) words Wd train within the dth testing document are used
to estimate the document-speciﬁc parameter posterior q(θd).1 To calculate predictive probability 
an approximation is necessary since we do not know the predictive distribution—just as we seek to
learn the posterior distribution. Speciﬁcally  we calculate the normalized predictive distribution and
report “log predictive probability” as

where we use the approximation

(d∈C(test) |Wd test|

(d∈C(test) log p(Wd test | C(train)  Wd train)
p(wtest | C(train)  Wd train) =)β)θd* K&k=1
≈)β)θd* K&k=1

 

(d∈C(test) |Wd test|

= (d∈C(test)(wtest∈Wd test log p(wtest | C(train)  Wd train)
θdkβkwtest+ p(θd | Wd train β ) p(β | C(train)) dθd dβ
θdkβkwtest+ q(θd) q(β) dθd dβ =

Eq[θdk] Eq[βkwtest].

K&k=1

To facilitate comparison with SVI  we use the Wikipedia and Nature corpora of [3  5] in our exper-
iments. These two corpora represent a range of sizes (3 611 558 training documents for Wikipedia
and 351 525 for Nature) as well as different types of topics. We expect words in Wikipedia to rep-
resent an extremely broad range of topics whereas we expect words in Nature to focus more on the
sciences. We further use the vocabularies of [3  5] and SVI code available online at [17]. We hold
out 10 000 Wikipedia documents and 1 024 Nature documents (not included in the counts above)
for testing.
In the results presented in the main text  we follow [3  4] in ﬁtting an LDA model
with K = 100 topics and hyperparameters chosen as: ∀k  αk = 1/K  ∀(k  v) η kv = 0.01. For
both Wikipedia and Nature  we set the parameters in SVI according to the optimal values of the
parameters described in Table 1 of [3] (number of documents D correctly set in advance  step size
parameters κ = 0.5 and τ0 = 64).
Figs. 3(a) and 3(d) demonstrate that both SVI and SDA are sensitive to minibatch size when
ηkv = 0.01  with generally superior performance at larger batch sizes.
Interestingly  both SVI
and SDA performance improve and are steady across batch size when ηkv = 1 (Figs. 3(a) and 3(d)).
Nonetheless  we use ηkv = 0.01 in what follows in the interest of consistency with [3  4]. Moreover 
in the remaining experiments  we use a large minibatch size of 215 = 32 768. This size is the largest
before SVI performance degrades in the Nature data set (Fig. 3(d)).
Performance and timing results are shown in Table 1. One would expect that with additional stream-
ing capabilities  SDA-Bayes should show a performance loss relative to SVI. We see from Table 1
1 In all cases  we estimate q(θd) for evaluative purposes using VB since direct EP estimation takes pro-

hibitively long.

6

y
t
i
l
i

b
a
b
o
r
p
 
e
v
i
t
c
d
e
r
p
 
g
o

i

l

−7.3

−7.35

−7.4

−7.45

 

 sync
 async

8

1

4

2
16
number of threads
(a) Wikipedia

 

32

y
t
i
l
i

b
a
b
o
r
p
 
e
v
i
t
c
d
e
r
p
 
g
o

i

l

−7.1

−7.15

−7.2

 

1

 sync
 async

 

40

30

20

10

)
s
r
u
o
h
(
 
e
m

i
t
 
n
u
r

 

 sync
 async

32

0

 

1

8

4

2
16
number of threads
(b) Nature

8

4

2
16
number of threads
(c) Wikipedia

32

)
s
r
u
o
h
(
 
e
m

i
t
 
n
u
r

10
8
6
4
2
0

 

1

 

 sync
 async

32

8

4

2
16
number of threads
(d) Nature

Figure 2: SDA-Bayes log predictive probability (two left plots) and run time (two right plots) as a
function of number of threads.

that such loss is small in the single-thread case  while SSU performs much worse. SVI is faster than
single-thread SDA-Bayes in this single-pass setting.
Full SDA-Bayes improves run time with no performance cost. We handicap SDA-Bayes in the
above comparisons by utilizing just a single thread. In Table 1  we also report performance of SDA-
Bayes with 32 threads and the same minibatch size. In the synchronous case  we consider minibatch
size to equal the total number of data points processed per round; therefore  the minibatch size equals
the number of data points sent to each thread per round times the total number of threads. In the
asynchronous case  we analogously report minibatch size as this product.
Fig. 2 shows the performance of SDA-Bayes when we run with {1  2  4  8  16  32} threads while
keeping the minibatch size constant. The goal in such a distributed context is to improve run time
while not hurting performance. Indeed  we see dramatic run time improvement as the number of
threads grows and in fact some slight performance improvement as well. We tried both a paral-
lel version and a full distributed  asynchronous version of the algorithm; Fig. 2 indicates that the
speedup and performance improvements we see here come from parallelizing—which is theoreti-
cally justiﬁed by Eq. (3) when A is Bayes rule. Our experiments indicate that our Hogwild!-style
asynchrony does not hurt performance. In our experiments  the processing time at each thread seems
to be approximately equal across threads and dominate any communication time at the master  so
synchronous and asynchronous performance and running time are essentially identical. In general 
a practitioner might prefer asynchrony since it is more robust to node failures.
SVI is sensitive to the choice of total data size D. The evaluations above are for a single posterior
over D data points. Of greater concern to us in this work is the evaluation of algorithms in the
streaming setting. We have seen that SVI is designed to ﬁnd the posterior for a particular  pre-
chosen number of data points D. In practice  when we run SVI on the full data set but change the
input value of D in the algorithm  we can see degradations in performance. In particular  we try
values of D equal to {0.01  0.1  1  10  100} times the true D in Fig. 3(b) for the Wikipedia data set
and in Fig. 3(e) for the Nature data set.
A practitioner in the streaming setting will typically not know D in advance  or multiple values of
D may be of interest. Figs. 3(b) and 3(e) illustrate that an estimate may not be sufﬁcient. Even in
the case where D is known in advance  it is reasonable to imagine a new inﬂux of further data. One
might need to run SVI again from the start (and  in so doing  revisit the ﬁrst data set) to obtain the
desired performance.
SVI is sensitive to learning step size.
[3  5] use cross-validation to tune step-size parameters
(τ0 κ ) in the stochastic gradient descent component of the SVI algorithm. This cross-validation
requires multiple runs over the data and thus is not suited to the streaming setting. Figs. 3(c) and 3(f)
demonstrate that the parameter choice does indeed affect algorithm performance. In these ﬁgures 
we keep D at the true training data size.
[3] have observed that the optimal (τ0 κ ) may interact with minibatch size  and we further observe
that the optimal values may vary with D as well. We also note that recent work has suggested a way
to update (τ0 κ ) adaptively during an SVI run [18].
EP is not suited to LDA. Earlier attempts to apply EP to the LDA model in the non-streaming
setting have had mixed success  with [19] in particular ﬁnding that EP performance can be poor for
LDA and  moreover  that EP requires “unrealistic intermediate storage requirements.” We found

7

y
t
i
l
i

 

b
a
b
o
r
p
e
v
i
t
c
d
e
r
p
g
o

 

i

l

−7
−7.3
−7.6
−7.9
−8.2
−8.5

 

5

 

SVI  η = 1.0
SVI  η = 0.01
SDA  η = 1.0
SDA  η = 0.01

15
log batch size (base 2)

10

y
t
i
l
i

 

b
a
b
o
r
p
e
v
i
t
c
d
e
r
p
g
o

 

i

l

−7.3
−7.35
−7.4
−7.45
−7.5
0

 

 

y
t
i
l
i

 

b
a
b
o
r
p
e
v
i
t
c
d
e
r
p
g
o

 

i

l

−7.3

−7.35

−7.4

−7.45

−7.5
0

 

D = 361155800
D = 36115580
D = 3611558
D = 361155
D = 36115
2e6

3e6

1e6

number of examples seen

 

5

0

 

τ0 = 16
τ0 = 256
τ0 = 64
τ0 = 256
τ0 = 64
τ0 = 16

 

 

 

.

 

.

κ = 1

κ = 1

0
κ = 0
0
κ = 1
5
5

κ = 0
κ = 0

.

 

.

.

.

1e6

2e6

3e6

number of examples seen

(a) Sensitivity to minibatch size on
Wikipedia

(b) SVI
Wikipedia

sensitivity to D on

(c) SVI sensitivity to stepsize pa-
rameters on Wikipedia

y
t
i
l
i

 

b
a
b
o
r
p
e
v
i
t
c
d
e
r
p

i

 
g
o

l

−7
−7.3
−7.6
−7.9
−8.2
−8.5

 

5

SDA  η = 1.0
SDA  η = 0.01
SVI  η = 0.01
SVI  η = 1.0

15
log batch size (base 2)

10

 

y
t
i
l
i

 

b
a
b
o
r
p
e
v
i
t
c
d
e
r
p
g
o

 

i

l

−7
−7.2
−7.4
−7.6
−7.8
−8
0

 

D = 3515250
D = 35152500
D = 351525
D = 35152
D = 3515
3e5
number of examples seen

2e5

1e5

 

y
t
i
l
i

 

b
a
b
o
r
p
e
v
i
t
c
d
e
r
p
g
o

 

i

l

−7
−7.2
−7.4
−7.6
−7.8
−8
0

 

 

0

5

 

 

τ0 = 16
τ0 = 64
τ0 = 256
τ0 = 16
τ0 = 64
τ0 = 256

 

 

.

.

 

κ = 0
κ = 0

5
5
κ = 1
0
0
κ = 0

κ = 1
κ = 1

.

.

.

 

.

1e5

3e5
number of examples seen

2e5

(d) Sensitivity to minibatch size on
Nature

(e) SVI sensitivity to D on Nature

(f) SVI sensitivity to stepsize pa-
rameters on Nature

Figure 3: Sensitivity of SVI and SDA-Bayes to some respective parameters. Legends have the same
top-to-bottom order as the rightmost curve points.

this to also be true in the streaming setting. We were not able to obtain competitive results with EP;
based on an 8-thread implementation of SDA-Bayes with an EP primitive2  after over 91 hours on
Wikipedia (and 6.7×104 data points)  log predictive probability had stabilized at around −7.95 and 
after over 97 hours on Nature (and 9.7×104 data points)  log predictive probability had stabilized at
around −8.02. Although SDA-Bayes with the EP primitive is not effective for LDA  it remains to be
seen whether this combination may be useful in other domains where EP is known to be effective.

5 Discussion

We have introduced SDA-Bayes  a framework for streaming  distributed  asynchronous computa-
tion of an approximate Bayesian posterior. Our framework makes streaming updates to the esti-
mated posterior according to a user-speciﬁed approximation primitive. We have demonstrated the
usefulness of our framework  with variational Bayes as the primitive  by ﬁtting the latent Dirichlet
allocation topic model to the Wikipedia and Nature corpora. We have demonstrated the advantages
of our algorithm over stochastic variational inference and the sufﬁcient statistics update algorithm 
particularly with respect to the key issue of obtaining approximations to posterior probabilities based
on the number of documents seen thus far  not posterior probabilities for a ﬁxed number of docu-
ments.

Acknowledgments

We thank M. Hoffman  C. Wang  and J. Paisley for discussions  code  and data and our reviewers
for helpful comments. TB is supported by the Berkeley Fellowship  NB by a Hertz Foundation
Fellowship  and ACW by the Chancellor’s Fellowship at UC Berkeley. This research is supported in
part by NSF award CCF-1139158  DARPA Award FA8750-12-2-0331  AMPLab sponsor donations 
and the ONR under grant number N00014-11-1-0688.

2We chose 8 threads since any fewer was too slow to get results and anything larger created too high of a

memory demand on our system.

8

References
[1] F. Niu  B. Recht  C. R´e  and S. J. Wright. Hogwild!: A lock-free approach to parallelizing stochastic

gradient descent. In Neural Information Processing Systems  2011.

[2] A. Kleiner  A. Talwalkar  P. Sarkar  and M. Jordan. The big data bootstrap. In International Conference

on Machine Learning  2012.

[3] M. Hoffman  D. M. Blei  and F. Bach. Online learning for latent Dirichlet allocation. In Neural Informa-

tion Processing Systems  volume 23  pages 856–864  2010.

[4] M. Hoffman  D. M. Blei  J. Paisley  and C. Wang. Stochastic variational inference. Journal of Machine

Learning Research  14:1303–1347.

[5] C. Wang  J. Paisley  and D. M. Blei. Online variational inference for the hierarchical Dirichlet process. In

Artiﬁcial Intelligence and Statistics  2011.

[6] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational inference.

Foundations and Trends in Machine Learning  1(1-2):1–305  2008.

[7] T. P. Minka. Expectation propagation for approximate Bayesian inference. In Uncertainty in Artiﬁcial

Intelligence  pages 362–369. Morgan Kaufmann  2001.

[8] T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis  Massachusetts

Institute of Technology  2001.

[9] M. Opper. A Bayesian approach to on-line learning.
[10] K. R Canini  L. Shi  and T. L Grifﬁths. Online inference of topics with latent Dirichlet allocation. In

Artiﬁcial Intelligence and Statistics  volume 5  2009.

[11] A. Honkela and H. Valpola. On-line variational Bayesian learning. In International Symposium on Inde-

pendent Component Analysis and Blind Signal Separation  pages 803–808  2003.

[12] J. Luts  T. Broderick  and M. P. Wand. Real-time semiparametric regression. Journal of Computational

and Graphical Statistics  to appear. Preprint arXiv:1209.3550.

[13] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[14] T. Minka and J. Lafferty. Expectation-propagation for the generative aspect model. In Uncertainty in

Artiﬁcial Intelligence  pages 352–359. Morgan Kaufmann  2002.

[15] Y. Teh  D. Newman  and M. Welling. A collapsed variational Bayesian inference algorithm for latent

Dirichlet allocation. In Neural Information Processing Systems  2006.

[16] A. Asuncion  M. Welling  P. Smyth  and Y. Teh. On smoothing and inference for topic models.

Uncertainty in Artiﬁcial Intelligence  2009.

In

[17] M. Hoffman. Online inference for LDA (Python code) at

http://www.cs.princeton.edu/˜blei/downloads/onlineldavb.tar  2010.

[18] R. Ranganath  C. Wang  D. M. Blei  and E. P. Xing. An adaptive learning rate for stochastic variational

inference. In International Conference on Machine Learning  2013.

[19] W. L. Buntine and A. Jakulin. Applying discrete PCA in data analysis.

Intelligence.

In Uncertainty in Artiﬁcial

[20] M. Seeger. Expectation propagation for exponential families. Technical report  University of California

at Berkeley  2005.

9

,Tamara Broderick
Nicholas Boyd
Andre Wibisono
Ashia Wilson
Michael Jordan
Paul Lagrée
Claire Vernade
Yi Ouyang
Mukul Gagrani
Ashutosh Nayyar
Rahul Jain