2016,Multiple-Play Bandits in the Position-Based Model,Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the multiple-play semi-bandit setting. However  a major concern in this context is when the system cannot decide whether the user feedback for each item is actually exploitable. Indeed  much of the content may have been simply ignored by the user. The present work proposes to exploit available information regarding the display position bias under the so-called Position-based click model (PBM). We first discuss how this model differs from the Cascade model and its variants considered in several recent works on multiple-play bandits. We then provide a novel regret lower bound for this model as well as computationally efficient algorithms that display good empirical and theoretical performance.,Multiple-Play Bandits in the Position-Based Model

Paul Lagrée∗

LRI  Université Paris Sud
Université Paris Saclay

paul.lagree@u-psud.fr

Claire Vernade∗

LTCI  CNRS  Télécom ParisTech

Université Paris Saclay

vernade@enst.fr

Olivier Cappé
LTCI  CNRS

Télécom ParisTech

Université Paris Saclay

Abstract

Sequentially learning to place items in multi-position displays or lists is a task that
can be cast into the multiple-play semi-bandit setting. However  a major concern in
this context is when the system cannot decide whether the user feedback for each
item is actually exploitable. Indeed  much of the content may have been simply
ignored by the user. The present work proposes to exploit available information
regarding the display position bias under the so-called Position-based click model
(PBM). We ﬁrst discuss how this model differs from the Cascade model and its
variants considered in several recent works on multiple-play bandits. We then
provide a novel regret lower bound for this model as well as computationally
efﬁcient algorithms that display good empirical and theoretical performance.

1

Introduction

During their browsing experience  users are constantly provided – without having asked for it – with
clickable content spread over web pages. While users interact on a website  they send clicks to the
system for a very limited selection of the clickable content. Hence  they let every unclicked item with
an equivocal answer: the system does not know whether the content was really deemed irrelevant
or simply ignored. In contrast  in traditional multi-armed bandit (MAB) models  the learner makes
actions and observes at each round the reward corresponding to the chosen action. In the so-called
multiple play semi-bandit setting  when users are presented with L items  they are assumed to provide
feedback for each of those items.
Several variants of this basic setting have been considered in the bandit literature. The necessity
for the user to provide feedback for each item has been called into question in the context of the
so-called Cascade Model [8  14  6] and its extensions such as the Dependent Click Model (DCM)
[20]. Both models are particularly suited for search contexts  where the user is assumed to be looking
for something relative to a query. Consequently  the learner expects explicit feedback: in the Cascade
Model each valid observation sequence must be either all zeros or terminated by a one  such that no
ambiguity is left on the evaluation of the presented items  while multiple clicks are allowed in the
DCM thus leaving some ambiguity on the last zeros of a sequence.
In the Cascade Model  the positions of the items are not taken into account in the reward process
because the learner is assumed to obtain a click as long as the interesting item belongs to the list.
Indeed  there are even clear indications that the optimal strategy in a learning context consists in
showing the most relevant items at the end of the list in order to maximize the amount of observed
feedback [14] – which is counter-intuitive in recommendation tasks.
To overcome these limitations  [6] introduces weights – to be deﬁned by the learner – that are
attributed to positions in the list  with a click on position l ∈ {1  . . .   L} providing a reward wl 
where the sequence (wl)l is decreasing to enforce the ranking behavior. However  no rule is given for

∗The two authors contributed equally.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

setting the weights (wl)l that control the order of importance of the positions. The authors propose an
algorithm based on KL-UCB [10] and prove a lower bound on the regret as well as an asymptotically
optimal upper bound.
Another way to address the limitations of the Cascade Model is to consider the DCM as in [20]. Here 
examination probabilities vl are introduced for each position l: conditionally on the event that the
user effectively scanned the list up to position l  he/she can choose to leave with probability vl and in
that case  the learner is aware of his/her departure. This framework naturally induces the necessity to
rank the items in the optimal order.
All previous models assume that a portion of the recommendation list is explicitly examined by the
user and hence that the learning algorithm eventually has access to rewards corresponding to the
unbiased user’s evaluation of each item. In contrast  we propose to analyze multiple-play bandits in
the Position-based model (PBM) [5]. In the PBM  each position in the list is also endowed with a
binary Examination variable [8  19] which is equal to one only when the user paid attention to the
corresponding item. But this variable  that is independent of the user’s evaluation of the item  is not
observable. It allows to model situations where the user is not explicitly looking for speciﬁc content 
as in typical recommendation scenarios.
Compared to variants of the Cascade model  the PBM is challenging due to the censoring induced by
the examination variables: the learning algorithm observes actual clicks but non-clicks are always
ambiguous. Thus  combining observations made at different positions becomes a non-trivial statistical
task. Some preliminary ideas on how to address this issue appear in the supplementary material of
[13]. In this work  we provide a complete statistical study of stochastic multiple-play bandits with
semi-bandit feedback in the PBM.
We introduce the model and notations in Section 2 and provide the lower bound on the regret in
Section 3. In Section 4  we present two optimistic algorithms as well as a theoretical analysis of
their regret. In the last section dedicated to experiments  those policies are compared to several
benchmarks on both synthetic and realistic data.

2 Setting and Parameter Estimation

We consider the binary stochastic bandit model with K Bernoulli-distributed arms. The model
parameters are the arm expectations θ = (θ1  θ2  . . .   θK)  which lie in Θ = (0  1)K. We will
denote by B(θ) the Bernoulli distribution with parameter θ and by d(p  q) := p log(p/q) + (1 −
p) log((1 − p)/(1 − q)) the Kullback-Leibler divergence from B(p) to B(q). At each round t  the
learner selects a list of L arms – referred to as an action – chosen among the K arms which are
indexed by k ∈ {1  . . .   K}. The set of actions is denoted by A and thus contains K!/(K − L)!
ordered lists; the action selected at time t will be denoted A(t) = (A1(t)  . . .   AL(t)).
The PBM is characterized by examination parameters (κl)1≤l≤L  where κl is the probability that the
user effectively observes the item in position l [5]. At round t  the selection A(t) is shown to the user
and the learner observes the complete feedback – as in semi-bandit models – but the observation at
position l  Zl(t)  is censored being the product of two independent Bernoulli variables Yl(t) and Xl(t) 
where Yl(t) ∼ B(κl) is non null when the user considered the item in position l – which is unknown to
the learner – and Xl(t) ∼ B(θAl(t)) represents the actual user feedback to the item shown in position
l=1 Zl(t)  where Z(t) = (X1(t)Y1(t)  . . .   XL(t)YL(t))
denotes the vector of censored observations at step t.
In the following  we will assume  without loss of generality  that θ1 > ··· > θK and κ1 > ··· >
κL > 0  in order to simplify the notations. The fact that the sequences (θl)l and (κl)l are decreasing
t=1 ra∗ − rA(t) the regret
L(cid:88)
T(cid:88)
(cid:88)
where µa =(cid:80)L
average  ∆a = µ∗ − µa the expected gap to optimality  and  Na(T ) =(cid:80)T
l=1 κlθal is the expected reward of action a  µ∗ = µa∗ is the best possible reward in
1{A(t) = a} is the

l. The learner receives a reward rA(t) =(cid:80)L
implies that the optimal list is a∗ = (1  . . .   L). Denoting by R(T ) =(cid:80)T

(µ∗ − µa) E[Na(T )] =

∆aE[Na(T )] 

incurred by the learner up to time T   one has
− E[θAl(t)]) =

E[R(T )] =

κl(θa∗

t=1

l=1

(cid:88)

a∈A

a∈A

t=1

l

(1)

number of times action a has been chosen up to time T .

2

In the following  we assume that the examination parameters (κl)1≤l≤L are known to the learner.
These can be estimated from historical data [5]  using  for instance  the EM algorithm [9] (see also
Section 5). In most scenarios  it is realistic to assume that the content (e.g.  ads in on-line advertising)
is changing much more frequently than the layout (web page design for instance) making it possible
to have a good knowledge of the click-through biases associated with the display positions.
The main statistical challenge associated with the PBM is that one needs to obtain estimates and
conﬁdence bounds for the components θk of θ from the available B(κlθk)-distributed draws cor-
responding to occurrences of arm k at various positions l = 1  . . .   L in the list. To this aim 
l=1 Sk l(t) 
l=1 Nk l(t). We further require bias-corrected versions

s=1 Zl(s)1{Al(s) = k}  Sk(t) = (cid:80)L

1{Al(s) = k}  Nk(t) =(cid:80)L

we deﬁne the following statistics: Sk l(t) = (cid:80)t−1
Nk l(t) =(cid:80)t−1
of the counts ˜Nk l(t) =(cid:80)t−1
given by I(θk) =(cid:80)L

A time t  and conditionally on the past actions A(1) up to A(t − 1)  the Fisher information for θk is
l=1 Nk l(t)κl/(θk(1 − κlθk)) (see Appendix A). We cannot however estimate
θk using the maximum likelihood estimator since it has no closed form expression. Interestingly
though  the simple pooled linear estimator

s=1 κl1{Al(s) = k} and ˜Nk(t) =(cid:80)L

˜Nk l(t).

l=1

s=1

ˆθk(t) = Sk(t)/ ˜Nk(t) 

υ(θk) = ((cid:80)L

l=1 Nk l(t)κlθk(1 − κlθk))/((cid:80)L

(2)
considered in the supplementary material to [13]  is unbiased and has a (conditional) variance of
l=1 Nk l(t)κl)2  which is close to optimal given the
Cramér-Rao lower bound. Indeed  υ(θk)I(θk) is recognized as a ratio of a weighted arithmetic mean
to the corresponding weighted harmonic mean  which is known to be larger than one  but is upper
bounded by 1/(1− θk)  irrespectively of the values of the κl’s. Hence  if  for instance  we can assume
that all θk’s are smaller than one half  the loss with respect to the best unbiased estimator is no more
than a factor of two for the variance. Note that despite its simplicity  ˆθk(t) cannot be written as a
simple sum of conditionally independent increments divided by the number of terms and will thus
require speciﬁc concentration results.
It can be checked that when θk gets very close to one  ˆθk(t) is no longer close to optimal. This
observation also has a Bayesian counterpart that will be discussed in Section 5. Nevertheless  it is
l=1 Sk l(t)/κl)/Nk l(t) which gets very

always preferable to the “position-debiased” estimator ((cid:80)L

unreliable as soon as one of the κl’s gets very small.

3 Lower Bound on the Regret

In this section  we consider the fundamental asymptotic limits of learning performance for online
algorithms under the PBM. These cannot be deduced from earlier general results  such as those of
[11  7]  due to the censoring in the feedback associated to each action. We detail a simple and general
proof scheme – using the results of [12] – that applies to the PBM  as well as to more general models.
Lower bounds on the regret rely on changes of measure: the question is how much can we mistake
the true parameters of the problem for others  when observing successive arms? With this in mind 
we will subscript all expectations and probabilities by the parameter value and indicate explicitly
that the quantities µa  a∗  µ∗  ∆a  introduced in Section 2  also depend on the parameter. For ease of
notation  we will still assume that θ is such that a∗(θ) = (1  . . .   L).

3.1 Existing results for multiple-play bandit problems

Lower bounds on the regret will be proved for uniformly efﬁcient algorithms  in the sense of [16]:
Deﬁnition 1. An algorithm is said to be uniformly efﬁcient if for any bandit model parameterized by
θ and for all α ∈ (0  1]  its expected regret after T rounds is such that EθR(T ) = o(T α).
For the multiple-play MAB  [2] obtained the following bound

lim inf
T→∞

EθR(T )
log(T )

θL − θk
d(θk  θL)

.

(3)

≥ K(cid:88)

k=L+1

3

For the “learning to rank” problem where rewards follow the weighted Cascade Model with decreasing
weights (wl)l=1 ... L  [6] derived the following bound

lim inf
T→∞

EθR(T )

log T

≥ wL

K(cid:88)

k=L+1

θL − θk
d(θk  θL)

.

Perhaps surprisingly  this lower bound does not show any additional term corresponding to the
complexity of ranking the L optimal arms. Indeed  the errors are still asymptotically dominated by
the need to discriminate irrelevant arms (θk)k>L from the worst of the relevant arms  that is  θL.

3.2 Lower bound step by step
Step 1: Computing the expected log-likelihood ratio. Denoting by Fs−1 the σ-algebra generated
by the past actions and observations  we deﬁne the log-likelihood ratio for the two values θ and λ of
the parameters by

(cid:96)(t) :=

log

p(Z(s); θ | Fs−1)
p(Z(s); λ | Fs−1)

.

Lemma 2. For each position l and each item k  deﬁne the local amount of information by

and its cumulated sum over the L positions by Ia(θ  λ) :=(cid:80)L

Il(θk  λk) := Eθ

p(Zl(t); θ)
p(Zl(t); λ)

log

expected log-likelihood ratio is given by

 
1{al = k}Il(θk  λk). The

(cid:21)

(cid:12)(cid:12)(cid:12)(cid:12) Al(t) = k
(cid:80)K

l=1

k=1

(4)

Eθ[(cid:96)(t)] =

Ia(θ  λ)Eθ[Na(t)].

(5)

t(cid:88)
(cid:20)

s=1

(cid:88)

a∈A

(cid:80)

The next proposition is adapted from Theorem 17 in Appendix B of [12] and provides a lower bound
on the expected log-likelihood ratio.
Proposition 3. Let B(θ) := {λ ∈ Θ|∀l ≤ L  θl = λl and µ∗(θ) < µ∗(λ)} be the set of changes of
measure that improve over θ without modifying the optimal arms. Assuming that the expectation of
the log-likelihood ratio may be written as in (5)  for any uniformly efﬁcient algorithm one has

∀λ ∈ B(θ) 

lim inf
T→∞

a∈A Ia(θ  λ)Eθ[Na(T )]

≥ 1.

log(T )

Step 2: Variational form of the lower bound. We are now ready to obtain the lower bound in a
form similar to that originally given by [11].
Theorem 4. The expected regret of any uniformly efﬁcient algorithm satisﬁes

lim inf
T→∞

EθR(T )

log T

≥ f (θ)   where f (θ) = inf
c(cid:23)0

Ia(θ  λ)ca ≥ 1.

∆a(θ)ca  

(cid:88)
(cid:88)
+   that satisﬁes the inequality(cid:80)

λ∈B(θ)

a∈A

s.t.

a∈A

inf

Theorem 4 is a straightforward consequence of Proposition 3  combined with the expression of the
expected regret given in (1). The vector c ∈ R|A|
a∈A Ia(θ  λ)ca ≥ 1 
represents the feasible values of Eθ[Na(T )]/ log(T ).

Step 3: Relaxing the constraints. The bounds mentioned in Section 3.1 may be recovered from
Theorem 4 by considering only the changes of measure that affect a single suboptimal arm.
Corollary 5.

(cid:88)

a∈A

f (θ) ≥ inf
c(cid:23)0

∆a(θ)ca  

s.t. (cid:88)

L(cid:88)

a∈A

l=1

1{al = k}Il(θk  θL)ca ≥ 1  

∀k ∈ {L + 1  . . .   K}.

Corollary 5 is obtained by restricting the constraint set B(θ) of Theorem 4 to ∪K
Bk(θ) := {λ ∈ Θ|∀j (cid:54)= k  θj = λj and µ∗(θ) < µ∗(λ)} .

k=L+1Bk(θ)  where

4

3.3 Lower bound for the PBM

Theorem 6. For the PBM  the following lower bound holds for any uniformly efﬁcient algorithm:

≥ K(cid:88)

k=L+1

lim inf
T→∞

EθR(T )

log T

min

l∈{1 ... L}

∆vk l (θ)

d(κlθk  κlθL)

 

(6)

where vk l := (1  . . .   l − 1  k  l  . . .   L − 1).

Proof. First  note that for the PBM one has Il(θk  λk) = d(κlθk  κlλk). To get the expression given
in Theorem 6 from Corollary 5  we proceed as in [6] showing that the optimal coefﬁcients (ca)a∈A
can be non-zero only for the K − L actions that put the suboptimal arm k in the position l that reaches
the minimum of ∆vk l (θ)/d(κlθk  κlθL). Nevertheless  this position does not always coincide with
L  the end of the displayed list  contrary to the case of [6] (see Appendix B for details).

The discrete minimization that appears in the r.h.s. of Theorem 6 corresponds to a fundamental
trade-off in the PBM. When trying to discriminate a suboptimal arm k from the L optimal ones  it
is desirable to put it higher in the list to obtain more information  as d(κlθk  κlθL) is an increasing
function of κl. On the other hand  the gap ∆vk l (θ) is also increasing as l gets closer to the top
of the list. The fact that d(κlθk  κlθL) is not linear in κl (it is a strictly convex function of κl)
renders the trade-off non trivial. It is easily checked that when (θ1 − θL) is very small  i.e. when all
optimal arms are equivalent  the optimal exploratory position is l = 1. In contrast  it is equal to L
when the gap (θL − θL+1) becomes very small. Note that by using that for any suboptimal a ∈ A 
1{al = k}κl(θL − θk)  one can lower bound the r.h.s. of Theorem 6 by

∆a(θ) ≥(cid:80)K
(cid:80)K
k=L+1(θL − θk)/d(κLθk  κLθL)  which is not tight in general.
of Ia(θ  λ) is simpler: it is equal to(cid:80)L

(cid:80)L

k=L+1

l=1

κL
Remark 7. In the uncensored version of the PBM – i.e.  if the Yl(t) were observed –  the expression
1{Al(t) = k}κld(θk  λk) and leads to a lower
bound that coincides with (3). The uncensored PBM is actually statistically very close to the weighted
Cascade model and can be addressed by algorithms that do not assume knowledge of the (κl)l but
only of their ordering.

(cid:80)K

k=1

l=1

4 Algorithms

In this section we introduce two algorithms for the PBM. The ﬁrst one uses the CUCB strategy of [4]
and requires an simple upper conﬁdence bound for θk based on the estimator ˆθk(t) deﬁned in (2).
The second algorithm is based on the Parsimonious Item Exploration – PIE(L) – scheme proposed
in [6] and aims at reaching asymptotically optimal performance. For this second algorithm  termed
PBM-PIE  it is also necessary to use a multi-position analog of the well-known KL-UCB index [10]
that is inspired by a result of [17]. The analysis of PBM-PIE provided below conﬁrms the relevance
of the lower bound derived in Section 3.

PBM-UCB The ﬁrst algorithm simply consists in sorting optimistic indices in decreasing order
and pulling the corresponding ﬁrst L arms [4]. To derive the expression of the required “exploration
bonus” we use an upper conﬁdence for ˆθk(t) based on Hoeffding’s inequality:

(cid:115)

(cid:115)

U U CB

k

(t  δ) =

Sk(t)
˜Nk(t)

+

Nk(t)
˜Nk(t)

δ

2 ˜Nk(t)

 

for which a coverage bound is given by the next proposition  proven in Appendix C.
Proposition 8. Let k be any arm in {1  . . .   K}  then for any δ > 0 

P(cid:0)U U CB

k

(t  δ) ≤ θk

(cid:1) ≤ eδ log(t)e−δ.

Following the ideas of [7]  it is possible to obtain a logarithmic regret upper bound for this algorithm.
The proof is given in Appendix D.

5

Theorem 9. Let C(κ) = min1≤l≤L[((cid:80)L

L and ∆ = mina∈σ(a∗)\a∗ ∆a 
where σ(a∗) denotes the permutations of the optimal action. Using PBM-UCB with δ = (1 +
) log(t) for some  > 0  there exists a constant C0() independent from the model parameters such
that the regret of PBM-UCB is bounded from above by
E[R(T )] ≤ C0() + 16(1 + )C(κ) log T

j=1 κj)2]/κ2

(cid:88)

(cid:33)

+

1

.

L
∆

κL(θL − θk)

k /∈a∗

j=1 κj)2/l + ((cid:80)l
(cid:32)

The presence of the term L/∆ in the above expression is attributable to limitations of the mathematical
analysis. On the other hand  the absence of the KL-divergence terms appearing in the lower bound (6)
is due to the use of an upper conﬁdence bound based on Hoeffding’s inequality.

PBM-PIE We adapt the PIE(l) algorithm introduced by [6] for the Cascade Model to the PBM in
Algorithm 1 below. At each round  the learner potentially explores at position L with probability 1/2
using the following upper-conﬁdence bound for each arm k

(cid:40)

(cid:18) Sk l(t)
(t) is the minimum of the convex function Φ : q (cid:55)→(cid:80)L

sup
q∈[θmin

Uk(t  δ) =

Nk l(t)d

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) L(cid:88)

Nk l(t)

(t) 1]

l=1

q

k

(cid:19)

(cid:41)

  κlq

≤ δ

 

(7)

k

where θmin
l=1 Nk l(t)d(Sk l(t)/Nk l(t)  κlq).
In other positions  l = 1  . . .   L − 1  PBM-PIE selects the arms with the largest estimates ˆθk(t).
The resulting algorithm is presented as Algorithm 1 below  denoting by L(t) the L-largest empirical
estimates  referred to as the “leaders” at round t.

Algorithm 1 – PBM-PIE
Require: K  L  observation probabilities κ   > 0

Initialization: ﬁrst K rounds  play each arm at every position
for t = K + 1  . . .   T do
Compute ˆθk(t) for all k
L(t) ← top-L ordered arms by decreasing ˆθk(t)
Al(t) ← Ll(t) for each position l < L
B(t) ← {k|k /∈ L(t)  Uk(t  (1 + ) log(T )) ≥ ˆθLL(t)(t)
if B(t) = ∅ then

AL(t) ← LL(t)
With probability 1/2  select AL(t) uniformly at random from B(t)  else AL(t) ← LL(t)

else

end if
Play action A(t) and observe feedback Z(t); Update Nk l(t + 1) and Sk l(t + 1).

end for

The Uk(t  δ) index deﬁned in (7) aggregates observations from all positions – as in PBM-UCB – but
allows to build tighter conﬁdence regions as shown by the next proposition proved in Appendix E.
Proposition 10. For all δ ≥ L + 1 

P (Uk(t  δ) < θk) ≤ eL+1

(cid:18)(cid:100)δ log(t)(cid:101) δ

(cid:19)L

L

e−δ.

We may now state the main result of this section that provides an upper bound on the regret of
PBM-PIE.
Theorem 11. Using PBM-PIE with δ = (1 + ) log(t) and  > 0  for any η < mink<K(θk −
θk+1)/2  there exist problem-dependent constants C1(η)  C2(  η)  C3() and β(  η) such that

E[R(T )] ≤ (1 + )2 log(T )

κL(θL − θk)

d(κLθk  κL(θL − η))

+ C1(η) +

C2(  η)
T β( η)

+ C3().

K(cid:88)

k=L+1

The proof of this result is provided in Appendix E. Comparing to the expression in (6)  Theorem 11
shows that PBM-PIE reaches asymptotically optimal performance when the optimal exploring

6

position is indeed located at index L. In other case  there is a gap that is caused by the fact the
exploring position is ﬁxed beforehand and not adapted from the data.
We conclude this section by a quick description of two other algorithms that will be used in the
experimental section to benchmark our results.

Ranked Bandits (RBA-KL-UCB) The state-of-the-art algorithm for the sequential “learning to
rank” problem was proposed by [18]. It runs one bandit algorithm per position  each one being
entitled to choose the best suited arm at its rank. The underlying bandit algorithm that runs in each
position is left to the choice of the user  the better the policy the lower the regret can be. If the bandit
algorithm at position l selects an arm already chosen at a higher position  it receives a reward of zero.
Consequently  the bandit algorithm operating at position l tends to focus on the estimation of l-th
best arm. In the next section  we use as benchmark the Ranked Bandits strategy using the KL-UCB
algorithm [10] as the per-position bandit.

PBM-TS The observations Zl(t) are censored Bernoulli which results in a posterior that does not
belong to a standard family of distribution. [13] suggest a version of Thompson Sampling called
“Bias Corrected Multiple Play TS” (or BC-MP-TS) that approximates the true posterior by a Beta
distribution. We observed in experiments that for parameter values close to one  this algorithm does
not explore enough. In Figure 1(a)  we show this phenomenon for θ = (0.95  0.85  0.75  0.65  0.55).
The true posterior for the parameter θk at time t may be written as a product of truncated scaled beta
distributions

πt(θk) ∝(cid:89)

θαk l(t)
k

(1 − κlθk)βk l(t) 

where αk l(t) = Sk l(t) and βk l(t) = Nk l(t) − Sk l(t). To draw from this exact poste-
rior  we use rejection sampling with proposal distribution Beta(αk m(t)  βk m(t))/κm  where
m = arg max1≤l≤L(αk l(t) + βk l(t)).

l

5 Experiments

5.1 Simulations

In order to evaluate our strategies  a simple problem is considered in which K = 5  L = 3 
κ = (0.9  0.6  0.3) and θ = (0.45  0.35  0.25  0.15  0.05). The arm expectations are chosen such that
the asymptotic behavior can be observed after reasonable time horizon. All results are averaged based
on 10  000 independent runs of the algorithm. We present the results in Figure 1(b) where PBM-UCB 
PBM-PIE and PBM-TS are compared to RBA-KL-UCB. The performance of PBM-PIE and
PBM-TS are comparable  the latter even being under the lower bound (it is a common observation 
e.g. see [13]  and is due to the asymptotic nature of the lower bound). The curves conﬁrm our analysis

(a) Average regret of PBM-TS and BC-MP-
TS compared for high parameters. Shaded
areas: ﬁrst and last deciles.

(b) Average regret of various algorithms
on synthetic data under the PBM.

Figure 1: Simulation results for the suggested strategies.

7

102103104105Roundt0200400600800100012001400RegretR(T)LowerBoundBC-MP-TSPBM-TS100101102103104105Roundt050100150200250300RegretR(T)LowerBoundPBM-TSPBM-UCBPBM-PIERBA-KLUCB#ads (K) #records min θ max θ
0.077
0.050
0.067
0.069
0.148
0.146
0.149
0.084

216  565
68  179
435  951
110  071
147  214
122  218
1  228  004
391  951

5
5
6
6
6
8
11
11

0.016
0.031
0.025
0.023
0.004
0.108
0.022
0.022

Table 1: Statistics on the queries: each line corresponds
to the sub-dataset associated with a query.

Figure 2: Performance of the proposed
algorithms under the PBM on real data.

for PBM-PIE and lets us conjecture that the true Thompson Sampling policy might be asymptotically
optimal. As expected  PBM-PIE shows asymptotically optimal performance  matching the lower
bound after a large enough horizon.

5.2 Real data experiments: search advertising

The dataset was provided for KDD Cup 2012 track 2[1] and involves session logs of soso.com  a
search engine owned by Tencent. It consists of ads that were inserted among search results. Each of
the 150M lines from the log contains the user ID  the query typed  an ad  a position (1  2 or 3) at
which it was displayed and a binary reward (click/no-click). First  for every query  we excluded ads
that were not displayed at least 1  000 times at every position. We also ﬁltered queries that had less
than 5 ads satisfying the previous constraints. As a result  we obtained 8 queries with at least 5 and
up to 11 ads. For each query q  we computed the matrix of the average click-through rates (CTR):
Mq ∈ RK×L  where K is the number of ads for the query q and L = 3 the number of positions. It is
noticeable that the SVD of each Mq matrix has a highly dominating ﬁrst singular value  therefore
validating the low-rank assumption underlying in the PBM. In order to estimate the parameters of the
problem  we used the EM algorithm suggested by [5  9]. Table 1 reports some statistics about the
bandit models reconstructed for each query: number of arms K  amount of data used to compute the
parameters  minimum and maximum values of the θ’s for each model.
We conducted a series of 2  000 simulations over this dataset. At the beginning of each run  a query
was randomly selected together with corresponding probabilities of scanning positions and arm
expectations. Even if rewards were still simulated  this scenario is more realistic since the values of
the parameters were extracted from a real-world dataset. We show results for the different algorithms
in Figure 2. It is remarkable that RBA-KL-UCB performs slightly better than PBM-UCB. One can
imagine that PBM-UCB does not beneﬁt enough from position aggregations – only 3 positions are
considered – to beat RBA-KL-UCB. Both of them are outperformed by PBM-TS and PBM-PIE.

Conclusion

This work provides the ﬁrst analysis of the PBM in an online context. The proof scheme used to
obtain the lower bound on the regret is interesting on its own  as it can be generalized to various other
settings. The tightness of the lower bound is validated by our analysis of PBM-PIE but it would be
an interesting future contribution to provide such guarantees for more straightforward algorithms
such as PBM-TS or a ‘PBM-KLUCB’ using the conﬁdence regions of PBM-PIE. In practice  the
algorithms are robust to small variations of the values of the (κl)l  but it would be preferable to obtain
some control over the regret under uncertainty on these examination parameters.

Acknowledgements

This work was partially supported by the French research project ALICIA (grant ANR-13-CORD-
0020) and by the Machine Learning for Big Data Chair at Télécom ParisTech.

8

100101102103104105Roundt0100200300400500600700800RegretR(T)PBM-TSPBM-UCBPBM-PIERBA-KLUCBReferences
[1] Kdd cup 2012 track 2. http://www.kddcup2012.org/.

[2] V. Anantharam  P. Varaiya  and J. Walrand. Asymptotically efﬁcient allocation rules for the
multiarmed bandit problem with multiple plays - part I: IID rewards. Automatic Control  IEEE
Transactions on  32(11):968–976  1987.

[3] S. Boucheron  G. Lugosi  and P. Massart. Concentration Inequalities: A Nonasymptotic Theory

of Independence. OUP Oxford  2013.

[4] W. Chen  Y. Wang  and Y. Yuan. Combinatorial multi-armed bandit: General framework and

applications. In Proc. of the 30th Int. Conf. on Machine Learning  2013.

[5] A. Chuklin  I. Markov  and M. d. Rijke. Click models for web search. Synthesis Lectures on

Information Concepts  Retrieval  and Services  7(3):1–115  2015.

[6] R. Combes  S. Magureanu  A. Proutière  and C. Laroche. Learning to rank: Regret lower bounds
and efﬁcient algorithms. In Proc. of the 2015 ACM SIGMETRICS Int. Conf. on Measurement
and Modeling of Computer Systems  2015.

[7] R. Combes  M. S. T. M. Shahi  A. Proutière  et al. Combinatorial bandits revisited. In Advances

in Neural Information Processing Systems  2015.

[8] N. Craswell  O. Zoeter  M. Taylor  and B. Ramsey. An experimental comparison of click
position-bias models. In Proc. of the Int. Conf. on Web Search and Data Mining. ACM  2008.

[9] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data via

the EM algorithm. Journal of the royal statistical society. Series B  pages 1–38  1977.

[10] A. Garivier and O. Cappé. The KL-UCB algorithm for bounded stochastic bandits and beyond.

In Proc. of the Conf. on Learning Theory  2011.

[11] T. L. Graves and T. L. Lai. Asymptotically efﬁcient adaptive choice of control laws in controlled

markov chains. SIAM journal on control and optimization  35(3):715–743  1997.

[12] E. Kaufmann  O. Cappé  and A. Garivier. On the complexity of best arm identiﬁcation in

multi-armed bandit models. Journal of Machine Learning Research  2015.

[13] J. Komiyama  J. Honda  and H. Nakagawa. Optimal regret analysis of thompson sampling in
stochastic multi-armed bandit problem with multiple plays. In Proc. of the 32nd Int. Conf. on
Machine Learning  2015.

[14] B. Kveton  C. Szepesvári  Z. Wen  and A. Ashkan. Cascading bandits : Learning to rank in the

cascade model. In Proc. of the 32nd Int. Conf. on Machine Learning  2015.

[15] B. Kveton  Z. Wen  A. Ashkan  and C. Szepesvári. Tight regret bounds for stochastic combi-
natorial semi-bandits. In Proc. of the 18th Int. Conf. on Artiﬁcial Intelligence and Statistics 
2015.

[16] T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

applied mathematics  6(1):4–22  1985.

[17] S. Magureanu  R. Combes  and A. Proutière. Lipschitz bandits: Regret lower bounds and

optimal algorithms. In Proc. of the Conf. on Learning Theory  2014.

[18] F. Radlinski  R. Kleinberg  and T. Joachims. Learning diverse rankings with multi-armed

bandits. In Proc. of the 25th Int. Conf. on Machine learning. ACM  2008.

[19] M. Richardson  E. Dominowska  and R. Ragno. Predicting clicks: estimating the click-through

rate for new ads. In Proc. of the 16th Int. Conf. on World Wide Web. ACM  2007.

[20] K. Sumeet  B. Kveton  C. Szepesvári  and Z. Wen. DCM bandits: Learning to rank with multiple

clicks. In Proc. of the 33rd Int. Conf. on Machine Learning  2016.

9

,Tamara Broderick
Nicholas Boyd
Andre Wibisono
Ashia Wilson
Michael Jordan
Paul Lagrée
Claire Vernade
Yi Ouyang
Mukul Gagrani
Ashutosh Nayyar
Rahul Jain