2012,Relax and Randomize : From Value to Algorithms,We show a principled way of deriving online learning algorithms from a minimax analysis. Various upper bounds on the minimax value  previously thought to be non-constructive  are shown to yield algorithms. This allows us to seamlessly recover known methods and to derive new ones  also capturing such ''unorthodox'' methods as Follow the Perturbed Leader and the R^2 forecaster. Understanding the inherent complexity of the learning problem thus leads to the development of algorithms. To illustrate our approach  we present several new algorithms  including a family of randomized methods that use the idea of a ''random play out''. New versions of the Follow-the-Perturbed-Leader algorithms are presented  as well as methods based on the Littlestone's dimension  efficient methods for matrix completion with trace norm  and algorithms for the problems of transductive learning and prediction with static experts.,Relax and Randomize: From Value to Algorithms

Alexander Rakhlin

University of Pennsylvania

Ohad Shamir

Microsoft Research

Karthik Sridharan

University of Pennsylvania

Abstract

We show a principled way of deriving online learning algorithms from a minimax
analysis. Various upper bounds on the minimax value  previously thought to be
non-constructive  are shown to yield algorithms. This allows us to seamlessly re-
cover known methods and to derive new ones  also capturing such “unorthodox”
methods as Follow the Perturbed Leader and the R2 forecaster. Understanding
the inherent complexity of the learning problem thus leads to the development of
algorithms. To illustrate our approach  we present several new algorithms  includ-
ing a family of randomized methods that use the idea of a “random playout”. New
versions of the Follow-the-Perturbed-Leader algorithms are presented  as well as
methods based on the Littlestone’s dimension  efﬁcient methods for matrix com-
pletion with trace norm  and algorithms for the problems of transductive learning
and prediction with static experts.

1

Introduction

This paper studies the online learning framework  where the goal of the player is to incur small
regret while observing a sequence of data on which we place no distributional assumptions. Within
this framework  many algorithms have been developed over the past two decades [6]. More recently 
a non-algorithmic minimax approach has been developed to study the inherent complexities of se-
quential problems [2  1  15  20]. It was shown that a theory in parallel to Statistical Learning can be
developed  with random averages  combinatorial parameters  covering numbers  and other measures
of complexity. Just as the classical learning theory is concerned with the study of the supremum of
empirical or Rademacher process  online learning is concerned with the study of the supremum of
martingale processes. While the tools introduced in [15  17  16] provide ways of studying the mini-
max value  no algorithms have been exhibited to achieve these non-constructive bounds in general.
In this paper  we show that algorithms can  in fact  be extracted from the minimax analysis. This
observation leads to a unifying view of many of the methods known in the literature  and also gives
a general recipe for developing new algorithms. We show that the potential method  which has
been studied in various forms  naturally arises from the study of the minimax value as a certain
relaxation. We further show that the sequential complexity tools introduced in [15] are  in fact 
relaxations and can be used for constructing algorithms that enjoy the corresponding bounds. By
choosing appropriate relaxations  we recover many known methods  improved variants of some
known methods  and new algorithms. One can view our framework as one for converting a non-
constructive proof of an upper bound on the value of the game into an algorithm. Surprisingly 
this allows us to also study such “unorthodox” methods as Follow the Perturbed Leader [10]  and
the recent method of [7] under the same umbrella with others. We show that the idea of a random
playout has a solid theoretical basis  and that Follow the Perturbed Leader algorithm is an example
of such a method. Based on these developments  we exhibit an efﬁcient method for the trace norm
matrix completion problem  novel Follow the Perturbed Leader algorithms  and efﬁcient methods
for the problems of online transductive learning. The framework of this paper gives a recipe for
developing algorithms. Throughout the paper  we stress that the notion of a relaxation  introduced
below  is not appearing out of thin air but rather as an upper bound on the sequential Rademacher
complexity. The understanding of inherent complexity thus leads to the development of algorithms.

1

2 Value  The Minimax Algorithm  and Relaxations

Let us introduce some notation. The sequence x1  . . .   xt is often denoted by x1∶t   and the set of all
distributions on some setA by (A). Unless speciﬁed otherwise  ✏ denotes a vector(✏1  . . .  ✏ T)
of i.i.d. Rademacher random variables. AnX -valued tree x of depth d is deﬁned as a sequence
(x1  . . .   xd) of mappings xt∶{±1}t−1￿X (see [15]). We often write xt(✏) instead of xt(✏1∶t−1).
LetF be the set of learner’s moves andX the set of moves of Nature. The online protocol dictates
that on every round t = 1  . . .   T the learner and Nature simultaneously choose ft ∈F  xt ∈X  
and observe each other’s actions. The learner aims to minimize regret RegT ￿∑T
t=1 `(ft  xt)−
t=1 `(f  xt) where ` ∶ F×X → R is a known loss function. Our aim is to study this
inf f∈F∑T
` F andX . We do assume  however  that ` F  andX are such that the minimax theorem in the
space of distributions overF andX holds. By studying the abstract setting  we are able to develop

general algorithmic and non-algorithmic ideas that are common across various application areas.
The starting point of our development is the minimax value of the associated online learning game:

online learning problem at an abstract level without assuming convexity or other such properties of

inf

E

xt

E

. . .

inf

(1)

sup

sup

qt+1

sup
xT

argmin

E
ft+1

sup
xt+1

. . . inf
qT

qT∈(F)

xT∈X E

the optimal algorithm that solves the minimax expression at every round t and returns

fT∼qT￿ T￿t=1
fT￿ T￿i=t+1

Henceforth  if the quantiﬁcation in inf and sup is omitted  it will be understood that xt  ft  pt  qt

x1∈X E
f1∼q1
q1∈(F)
ft∼q￿`(ft  xt)+ inf

T￿t=1
VT(F)=
`(f  xt)￿
`(ft  xt)− inf
f∈F
where (F) is the set of distributions onF. The minimax formulation immediately gives rise to
T￿i=1
`(f  xi)￿￿￿
`(fi  xi)− inf
q∈(F) ￿sup
f∈F
range overX  F  (X)  (F)  respectively. Moreover  Ext is with respect to pt while Eft is with
respect to qt. We now notice a recursive form for the value of the game. Deﬁne for any t∈[T− 1]
and any given preﬁx x1  . . .   xt∈X the conditional value
VT(F￿x1  . . .   xt)￿ inf
x∈X￿ E
q∈(F)
t=1 `(f  xt) andVT(F)=VT(F￿{}). The minimax optimal
withVT(F￿x1  . . .   xT)￿− inf f∈F∑T
f∼q[`(f  x)]+VT(F￿x1  . . .   xt−1  x)￿ .
x∈X￿ E

f∼q[`(f  x)]+VT(F￿x1  . . .   xt  x)￿

algorithm specifying the mixed strategy of the player can be written succinctly as

Similar recursive formulations have appeared in the literature [8  13  19  3]  but now we have
tools to study the conditional value of the game. We will show that various upper bounds on

VT(F￿x1  . . .   xt−1  x) yield an array of algorithms. In this way  the non-constructive approaches of

[15  16  17] to upper bound the value of the game directly translate into algorithms. We note that
the minimax algorithm in (2) can be interpreted as choosing the best decision that takes into account
the present loss and the worst-case future. The ﬁrst step in our analysis is to appeal to the minimax
theorem and perform the same manipulation as in [1  15]  but only on the conditional values:

qt= argmin
q∈(F)

sup

sup

(2)

E

E

inf

(3)

pt+1

E
xt+1

T￿i=1

. . . sup
pT

xi∼pi

`(f  xi)￿ .

`(fi  xi)− inf
f∈F

VT(F￿x1  . . .   xt)= sup

The idea now is to come up with more manageable  yet tight  upper bounds on the conditional value.

xT￿ T￿i=t+1
fi∈F
A relaxation RelT is a sequence of real-valued functions RelT(F￿x1  . . .   xt) for each t∈[T]. We
call a relaxation admissible if for any x1  . . .   xT∈X  
f∼q[`(f  x)]+ RelT(F￿x1  . . .   xt  x)￿
x∈X￿ E
for all t ∈ [T − 1]  and
t=1 `(f  xt). We use the notation
RelT(F) for RelT(F￿{}). A strategy q that minimizes the expression in (4) deﬁnes an optimal
f∼q[`(f  x)]+ RelT(F￿x1  . . .   xt−1  x)￿   (5)

RelT(F￿x1  . . .   xT) ≥ − inf f∈F∑T
qt= arg min

RelT(F￿x1  . . .   xt)≥ inf
q∈(F)

Meta-Algorithm for an admissible relaxation RelT :

x∈X￿ E
q∈(F) sup

on round t  compute

sup

(4)

2

play ft∼ qt and receive xt from the opponent. Importantly  minimization need not be exact: any qt

that satisﬁes the admissibility condition (4) is a valid method  and we will say that such an algorithm
is admissible with respect to the relaxation RelT .
Proposition 1. Let RelT be an admissible relaxation. For any admissible algorithm with respect
to RelT   (including the Meta-Algorithm)  irrespective of the strategy of the adversary 

(6)

T￿t=1

Eft∼qt`(ft  xt)− inf
f∈F

T￿t=1

`(f  xt)≤ RelT(F)  

and therefore  E[RegT]≤ RelT(F) . If `(⋅ ⋅) is bounded  the Hoeffding-Azuma inequality yields a
high-probability bound on RegT . We also have thatVT(F)≤ RelT(F) . Further  if for all t∈[T] 
the admissible strategies qt are deterministic  RegT≤ RelT(F) .

The reader might recognize RelT as a potential function. It is known that one can derive regret
bounds by coming up with a potential such that the current loss of the player is related to the differ-
ence in the potentials at successive steps  and that the regret can be extracted from the ﬁnal potential.
The origin/recipe for “good” potential functions has always been a mystery (at least to the authors).
One of the key contributions of this paper is to show that they naturally arise as relaxations on the
conditional value  and the conditional value is itself the tightest possible relaxation. In particular 
for many problems a tight relaxation is achieved through symmetrization applied to the expression
in (3). Deﬁne the conditional Sequential Rademacher complexity

(7)

(8)

Here the supremum is over allX -valued binary trees of depth T− t. One may view this complexity

as a partially symmetrized version of the sequential Rademacher complexity

x

RT(F￿x1  . . .   xt)= sup

f∈F￿2
E✏t+1∶T sup
RT(F)￿ RT(F￿{}) = sup

x

T￿s=t+1

✏s`(f  xs−t(✏t+1∶s−1))− t￿s=1
✏s`(f  xs(✏1∶s−1))￿
f∈F￿2
E✏1∶T sup

T￿s=1

`(f  xs)￿ .

deﬁned in [15]. We shall refer to the term involving the tree x as the “future” and the term being
subtracted off in (7) – as the “past”. This indeed corresponds to the fact that the quantity is con-
ditioned on the already observed x1  . . .   xt  while for the future we have the worst possible binary
tree.1
Proposition 2. The conditional Sequential Rademacher complexity is admissible.
We now show that several well-known methods arise as further relaxations on RT .

1


(9)

log￿￿￿f∈F

upper bound on sequential Rademacher complexity leads to the following relaxation:

Furthermore  it leads to a parameter-free version of the Exponential Weights algorithm  deﬁned on

>0￿￿￿￿￿￿￿
RelT(F￿x1  . . .   xt)= inf

Exponential Weights [12  21] SupposeF is a ﬁnite class and￿`(f  x)￿≤ 1. In this case  a (tight)

`(f  xi)￿￿￿+ 2(T− t)￿￿￿￿￿￿￿
exp￿−
t￿i=1
Proposition 3. The relaxation (9) is admissible and RT(F￿x1  . . .   xt) ≤ RelT(F￿x1  . . .   xt) .
round t+ 1 by the mixed strategy qt+1(f)∝ exp￿−∗t∑t
s=1 `(f  xs)￿ with ∗t the optimal value in
(9). The algorithm’s regret is bounded by RelT(F)≤ 2￿2T log￿F￿ .
free algorithm. The learning rate ∗ can be optimized (via 1D line search) at each iteration.
In the setting of online linear optimization [22]  the loss is `(f  x)=￿f  x￿.
SupposeF is a unit ball in some Banach space andX is the dual. Let￿⋅￿ be some(2  C)-smooth
norm onX (in the Euclidean case  C = 2). Using the notation ˜xt−1=∑t−1
s=1 xs  a straightforward
RelT(F￿x1  . . .   xt)=￿￿˜xt−1￿2+￿∇￿˜xt−1￿2   xt￿+ C(T− t+ 1)
1It is cumbersome to write out the indices on xs−t(✏t+1∶s−1) in (7)  so we will instead use xs(✏) whenever

We point out that the exponential-weights algorithm arising from the relaxation (9) is a parameter-

upper bound on sequential Rademacher complexity is the following relaxation:

Mirror Descent [4  14]

this doesn’t cause confusion.

(10)

3

inf

sup

inf

sup

E

sup

inf

sup

Proposition 4. The relaxation (10) is admissible and RT(F￿x1  . . .   xt)≤ RelT(F￿x1  . . .   xt) .
It yields the update ft=

2￿￿˜xt−1￿2+C(T−t+1) with regret bound RelT(F)≤√2CT .
−∇￿˜xt−1￿2

We would like to remark that the traditional mirror descent update can be shown to arise out of an
appropriate relaxation. The algorithms proposed are parameter free as the step size is tuned automat-
ically. We chose the popular methods of Exponential Weights and Mirror Descent for illustration.
In the remainder of the paper  we develop new algorithms to show universality of our approach.
3 Classiﬁcation

improper version  we may write the value in (1) as

between the learner and Nature  so let us outline them. The “proper” version of supervised learning

We start by considering the problem of supervised learning  whereX is the space of instances and
Y the space of responses (labels). There are two closely related protocols for the online interaction
follows the protocol presented in Section 2: at time t  the learner selects ft∈F  Nature simultane-
ously selects(xt  yt)∈X×Y  and the learner suffers the loss `(f(xt)  yt). The “improper” version
is as follows: at time t  Nature chooses xt∈X and presents it to the learner as “side information” 
the learner then picks ˆyt∈Y and Nature simultaneously chooses yt∈Y. In the improper version  the
loss of the learner is `(ˆyt  yt)  and it is easy to see that we may equivalently state this protocol as the
learner choosing any function ft∈YX (not necessarily inF)  and Nature simultaneously choosing
(xt  yt). We mostly focus on the “improper” version of supervised learning in this section. For the
ˆyT∼qT￿ T￿t=1
VT(F)= sup
`(f(xt)  yt)￿
`(ˆyt  yt)− inf
E
x1∈X
yT∈X
y1∈X
f∈F
ˆy1∼q1
q1∈(Y)
qT∈(Y)
and a relaxation RelT is admissible if for any(x1  y1) . . .  (xT   yT)∈X×Y 
i=1 (x  y)￿￿≤ RelT￿F￿{(xi  yi)}t
i=1￿
`(ˆy  y)+ RelT￿F￿{(xi  yi)}t
y∈Y￿ E
ˆy∼q
x∈X
q∈(Y)
Let us now focus on binary prediction  i.e. Y ={±1}. In this case  the supremum over y in (11)
becomes a maximum over two values. Let us now take the absolute loss `(ˆy  y)=￿ˆy− y￿= 1− ˆyy.
i=1 (x −1)￿￿
qt= argmin
q∈(Y)
i=1 (x −1)￿￿
or equivalently as : qt= 1
We now assume thatF has a ﬁnite Littlestone’s dimension Ldim(F) [11  5]. Suppose the loss
function is `(ˆy  y)=￿ˆy− y￿  and consider the “mixed” conditional Rademacher complexity

i=1 (x  1)￿   1+ q+ RelT￿F￿{(xi  yi)}t
i=1 (x  1)￿− RelT￿F￿{(xi  yi)}t
✏if(xi(✏))− t￿i=1￿f(xi)− yi￿￿

We can see2 that the optimal randomized strategy  given the side information x  is given by (11) as

as a possible relaxation. The admissibility condition (11) with the conditional sequential
Rademacher (13) as a relaxation would require us to upper bound

xT∈X

T￿t=1

. . . sup

(11)

(12)

max￿1− q+ RelT￿F￿{(xi  yi)}t
2￿RelT￿F￿{(xi  yi)}t
T−t￿i=1
ˆyt∼qt￿ˆyt− yt￿+ sup

yt∈{±1}￿ E

f∈F￿2

E✏ sup

sup
x

max

inf

x

However  the supremum over x is preventing us from obtaining a concise algorithm. We need to

then proceed as in the Exponential Weights example for a ﬁnite collection of experts. This leads to
an upper bound on (13) and gives rise to algorithms similar in spirit to those developed in [5]  but
with more attractive computational properties and deﬁned more concisely.

further “relax” this supremum  and the idea is to pass to a ﬁnite cover ofF on the given tree x and
i=0￿t
i￿  which is shown in [15] to be the maximum size of
Deﬁne the function g(d  t) = ∑d
an exact (zero) cover for a function class with the Littlestone’s dimension Ldim = d. Given
{(x1  yt)  . . .  (xt  yt)} and  = (1  . . .   t) ∈ {±1}t  letFt() = {f ∈ F ∶ f(xi) = i ∀i ≤
t}  the subset of functions that agree with the signs given by  on the “past” data and let
F￿x1 ... xt ￿ F￿xt ￿ {(f(x1)  . . .   f(xt)) ∶ f ∈ F} be the projection ofF onto x1  . . .   xt. De-
note Lt(f)=∑t
i=1￿i− yi￿ for ∈{±1}t. The following proposition
gives a relaxation and an algorithm which achieves the O(￿Ldim(F)T log T) regret bound. Un-

like the algorithm of [5]  we do not need to run an exponential number of experts in parallel and
only require access to an oracle that computes the Littlestone’s dimension.

i=1￿f(xi)− yi￿ and Lt()=∑t

✏if(xi(✏))− t￿i=1￿f(xi)− yi￿￿￿

E✏ sup

f∈F￿2

qt∈[−1 1]

(13)

(14)

sup
xt

T−t￿i=1

2The extension to k-class prediction is immediate.

4

Proposition 5. The relaxation



RelT￿F￿(xt  yt)￿= 1

g(Ldim(Ft())  T− t) exp{−Lt()}￿￿+ 2(T− t) .
is admissible and leads to an admissible algorithm which uses weights qt(−1)= 1− qt(+1) and

log￿￿ ￿∈F￿xt
qt(+1)=∑( +1)∈F￿xt g(Ldim(Ft( +1))  T− t) exp{−Lt−1()}
∑( t)∈F￿xt g(Ldim(Ft(  t))  T− t) exp{−Lt−1()}  

There is a very close correspondence between the proof of Proposition 5 and the proof of the com-
binatorial lemma of [15]  the analogue of the Vapnik-Chervonenkis-Sauer-Shelah result.

(15)

4 Randomized Algorithms and Follow the Perturbed Leader

We now develop a class of admissible randomized methods that arise through sampling. Consider
the objective (5) given by a relaxation RelT . If RelT is the sequential (or classical) Rademacher
complexity  it involves an expectation over sequences of coin ﬂips  and this computation (coupled
with optimization for each sequence) can be prohibitively expensive. More generally  RelT might
involve an expectation over possible ways in which the future might be realized. In such cases 
we may consider a rather simple “random playout” strategy: draw the random sequence and solve
only one optimization problem for that random sequence. The ideas of random playout have been
discussed in previous literature for estimating the utility of a move in a game (see also [3]). We show
that random playout strategy has a solid basis: for the examples we consider  it satisﬁes admissibility.
In many learning problems the sequential and the classical Rademacher complexities are within a
constant factor of each other. This holds true  for instance  for linear functions in ﬁnite-dimensional
spaces. In such cases  the relaxation RelT does not involve the supremum over a tree  and the
randomized method only needs to draw a sequence of coin ﬂips and compute a solution to an opti-
mization problem slightly more complicated than ERM. We show that Follow the Perturbed Leader
(FPL) algorithms [10] arise in this way. We note that FPL has been previously considered as a rather
unorthodox algorithm providing some kind of regularization via randomization. Our analysis shows
that it arises through a natural relaxation based on the sequential (and thus the classical) Rademacher
complexity  coupled with the random playout idea. As a new algorithmic contribution  we provide
a version of the FPL algorithm for the case of the decision sets being `2 balls  with a regret bound
that is independent of the dimension. We also provide an FPL-style method for the combination of

Under the above assumption one can use the following relaxation

The assumption below implies that the sequential and classical Rademacher complexities are within
constant factor C of each other. We later verify that it holds in the examples we consider.

`1 and `∞ balls. To the best of our knowledge  these results are novel.
Assumption 1. There exists a distribution D∈ (X) and constant C≥ 2 such that for any t∈[T]
and given any x1  . . .   xt−1  xt+1  . . .   xT∈X and any ✏t+1  . . .  ✏ T∈{±1} 
f∈F[ CAt(f)− Lt−1(f)]
E
✏t xt∼D
p∈(X)
i=t ✏i`(f  xi).
i=1 `(f  xi)  and At(f)=∑T
where ✏t’s are i.i.d. Rademacher  Lt−1(f)=∑t−1
`(f  xi)￿
✏i`(f  xi)− t￿i=1
f∈F￿C
xt+1 ...xT∼D

f∈F￿ CAt+1(f)− Lt−1(f)+ E

x∼p[`(f  x)]− `(f  xt)￿≤

which is a partially symmetrized version of the classical Rademacher averages.
The proof of admissibility for the randomized methods is quite curious – the forecaster can be seen as
mimicking the sequential Rademacher complexity by sampling from the “equivalently bad” classical
Rademacher complexity under the speciﬁc distribution D speciﬁed by the above assumption.
Lemma 6. Under Assumption 1  the relaxation in Eq. (16) is admissible and a randomized strategy

that ensures admissibility is given by: at time t  draw xt+1  . . .   xT∼ D and ✏t+1  . . .  ✏ T and then:
(a) In the case the loss ` is convex in its ﬁrst argument and setF is convex and compact  deﬁne

RelT(F￿x1  . . .   xt)=

T￿i=t+1

E

E✏ sup

(16)

sup

sup

E
xt∼p

sup

5

(b) In the case of non-convex loss  sample ft from the distribution

ft= argmin
g∈F
ˆqt= argmin
ˆq∈(F)

sup

sup

✏i`(f  xi)− t−1￿i=1
x∈X￿`(g  x)+ sup
f∈F￿C
T￿i=t+1
✏i`(f  xi)− t−1￿i=1
f∼ˆq[`(f  x)]+ sup
x∈X￿ E
f∈F￿C
T￿i=t+1
✏t`(f  xt)￿  
✏￿sup
E[RegT]≤ C Ex1∶T∼D E
T￿t=1
f∈F

`(f  xi)− `(f  x)￿￿
`(f  xi)− `(f  x)￿￿

The expected regret for the method is bounded by the classical Rademacher complexity:

(17)

(18)

(19)

(20)

xi− x￿￿

sup

E

E

Here  we consider the setting similar to that

Of particular interest are the settings of static experts and transductive learning  which we consider
in Section 5. In the transductive case  the xt’s are pre-speciﬁed before the game  and in the static
expert case – effectively absent. In these cases  as we show below  there is no explicit distribution
D and we only need to sample the random signs ✏’s. We easily see that in these cases  the expected
regret bound is simply two times the transductive Rademacher complexity.
The idea of sampling from a ﬁxed distribution is particularly appealing in the case of linear loss 

At round t  the generic algorithm speciﬁed by Lemma 18 draws fresh Rademacher random variables

The above lemma is especially attractive with Gaussian perturbations as sum of normal random

any symmetric distribution satisﬁes Assumption 2.
Lemma 7. If D is any symmetric distribution over R  then Assumption 2 is satisﬁed by using the

`(f  x)=￿f  x￿. SupposeX is a unit ball in some norm￿⋅￿ in a vector space B  andF is a unit ball
in the dual norm￿⋅￿∗. A sufﬁcient condition implying Assumption 1 is then
Assumption 2. There exists a distribution D∈ (X) and constant C≥ 2 such that for any w∈ B 
xt∼p￿w+ 2✏txt￿≤ E
✏t￿w+ C✏ txt￿
x∈X
xt∼D
✏ and xt+1  . . .   xT∼ D and picks
✏ixi− t−1￿i=1
ft= argmin
x∈X￿￿f  x￿+￿C
T￿i=t+1
f∈F
We now look at `2￿`2 and `1￿`∞ cases and provide corresponding randomized algorithms.
Example : `1￿`∞ Follow the Perturbed Leader
in [10]. LetF ⊂ RN be the `1 unit ball andX the (dual) `∞ unit ball in RN. In [10] F is the
probability simplex andX =[0  1]N but these are subsumed by the `1￿`∞ case. Next we show that
product distribution DN and any C ≥ 6￿Ex∼D￿x￿. In particular  Assumption 2 is satisﬁed with a
distribution D that is uniform on the vertices of the cube{±1}N and C= 6.
variables is again normal. Hence  instead of drawing xt+1  . . .   xT ∼ N(0  1) on round t  one can
simply draw one vector Xt∼ N(0  T− t) as the perturbation. In this case  C≤ 8.
Lemma 8. Suppose F is the `N
1 unit ball and X is the dual `N∞ unit ball  and let D be
draws Rademacher random variables ✏t+1  . . .  ✏ T and xt+1  . . .   xT ∼ DN and picks ft =
i=t+1 ✏ixi￿ where C = 6￿Ex∼D￿x￿. The expected regret is bounded as :
f∈F
E✏￿ T￿t=1
Pyt+1∶T∼D￿C￿ T￿i=t+1
✏txt￿∞+ 4
T￿t=1
E
x1∶T∼DN
For instance  for the case of coin ﬂips (with C= 6) or the Gaussian distribution (with C= 3√2⇡)
the bound above is 4C√T log N  as the second term is bounded by a constant.
We now consider the case whenF andX
Example : `2￿`2 Follow the Perturbed Leader
Lemma 9. LetX andF be unit balls in Euclidean norm. Then Assumption 2 is satisﬁed with a
uniform distribution D on the surface of the unit sphere with constant C= 4√2.

are both the unit `2 ball. We can use as perturbation the uniform distribution on the surface of unit
sphere  as the following lemma shows. This result was hinted at in [2]  as in high dimensional case 
the random draw from the unit sphere is likely to produce orthogonal directions. However  we do
not require dimensionality to be high for our result.

The form of update in Equation (20)  however  is not in a convenient form  and the following
lemma shows a simple Follow the Perturbed Leader type algorithm with the associated regret bound.

any symmetric distribution. Consider the randomized algorithm that at each round t  freshly

￿f ∑t−1

i=1 xi− C∑T
E[RegT]≤ C

yi￿≤ 4￿

argmin

sup

6

i=t+1 ✏ixi￿2

. The randomized algorithm enjoys a

As in the previous example the update in (20) is not in a convenient form and this is addressed below.

i=1 xi+ C∑T

the surface of the unit sphere. Consider the randomized algorithm that at each round (say round

Lemma 10. LetX andF be unit balls in Euclidean norm  and D be the uniform distribution on
i=t+1 xi￿￿L where C = 4√2
t) freshly draws xt+1  . . .   xT ∼ D and picks ft =￿−∑t−1
i=1 xi+ C∑T
2+ 1￿1￿2
and scaling factor L=￿￿−∑t−1
t=1 xt￿2≤ 4√2T .
bound on the expected regret given by E[RegT]≤ C Ex1 ... xT∼D￿∑T

Importantly  the bound does not depend on the dimensionality of the space. To the best of our
knowledge  this is the ﬁrst such result for Follow the Perturbed Leader style algorithms. Further 
unlike [10  6]  we directly deal with the adaptive adversary.
5 Static Experts with Convex Losses and Transductive Online Learning
We show how to recover a variant of the R2 forecaster of [7]  for static experts and transductive

set of static experts. The transductive setting is equivalent to this: the sequence of xt’s is known

online learning. At each round  the learner makes a prediction qt∈[−1  1]  observes the outcome
yt∈[−1  1]  and suffers convex L-Lipschitz loss `(qt  yt). Regret is deﬁned as the difference be-
t=1 `(f[t]  yt)  where F ⊂[−1  1]T can be seen as a
tween learner’s cumulative loss and inf f∈F∑T
before the game starts  and hence the effective function class is once again a subset of[−1  1]T . It

turns out that in these cases  sequential Rademacher complexity becomes the classical Rademacher
complexity (see [17])  which can thus be taken as a relaxation. This is also the reason that an ef-
ﬁcient implementation by sampling is possible. For general convex loss  one possible admissible
relaxation is just a conditional version of the classical Rademacher averages:

(22)

(21)

T￿t=1

T￿t=1

T￿s=t+1

@`(ˆyt  yt)⋅ f[t]

✏sf[s]− Lt(f)￿

RelT(F￿y1  . . .   yt)= E✏t+1∶T sup
f∈F￿2L
where Lt(f)=∑t
s=1 `(f[s]  ys). If (21) is used as a relaxation  the calculation of prediction ˆyt
involves a supremum over f ∈ F with (potentially nonlinear) loss functions of instances seen so
`(ˆyt  yt)− inf
T￿t=1
f∈F

far. In some cases this optimization might be hard and it might be preferable if the supremum only
involves terms linear in f. To this end we start by noting that by convexity

One can now consider an alternative online learning problem which  if we solve  also solves the

original problem. More precisely  the new loss is `′(ˆy  r)= r⋅ ˆy; we ﬁrst pick prediction ˆyt (de-
terministically) and the adversary picks rt (corresponding to rt= @`(ˆyt  yt) for choice of yt picked
by adversary). Now note that `′ is indeed convex in its ﬁrst argument and is L Lipschitz because
￿@`(ˆyt  yt)￿≤ L. This is a one dimensional convex learning game where we pick ˆyt and regret is

given by the right hand side of (22). Hence  we can consider the relaxation

@`(ˆyt  yt)⋅ ˆyt− inf
f∈F

`(f(xt)  yt)≤ T￿t=1

as a linearized form of (21). At round t  the prediction of the algorithm is then

@`(ˆyi  yi)⋅ f[i]￿
✏if[t]− t￿i=1
RelT(F￿@`(ˆy1  y1)  . . .  @`(ˆyt  yt))= E✏t+1∶T sup
f∈F￿2L
T￿i=t+1
t−1￿i=1
@`(ˆyi  yi)f[i]− 1
ˆyt= E
2 f[t]￿￿
✏if[i]− 1
f∈F￿ T￿i=t+1
2 f[t]￿− sup
✏￿sup
f∈F￿ T￿i=t+1
t=1 ✏tf[t]￿ .
tion (24). Further the regret of the strategy is bounded as RegT≤ 2L E✏￿supf∈F∑T

(24)
Lemma 11. The relaxation in Eq. (23) is admissible w.r.t. the prediction strategy speciﬁed in Equa-

This algorithm is similar to R2  with the main difference that R2 computes the inﬁma over a sum
of absolute losses  while here we have a more manageable linearized objective. While we need
to evaluate the expectation over ✏’s on each round  we can estimate ˆyt by sampling ✏’s and using
McDiarmid’s inequality argue that the estimate is close to ˆyt with high probability. The randomized

@`(ˆyi  yi)f[i]+ 1

✏if[i]− 1

t−1￿i=1

(23)

2L

2L

prediction is now given simply as: on round t  draw ✏t+1  . . .  ✏ T and predict
ˆyt(✏)= inf
✏if[i]+ 1

`(f[i]  yi)+ 1

2 f[t]￿− inf

✏if[i]+ 1

f∈F￿− T￿i=t+1

f∈F￿− T￿i=t+1

t−1￿i=1

2L

2L

t−1￿i=1

`(f[i]  yi)− 1

2 f[t]￿(25)

We now show that this predictor enjoys regret bound of the transductive Rademacher complexity :

7

Lemma 12. The relaxation speciﬁed in Equation (21) is admissible w.r.t. the randomized prediction

6 Matrix Completion
Consider the problem of predicting unknown entries in a matrix  in an online fashion. At each round

strategy speciﬁed in Equation (25)  and enjoys bound E[RegT]≤ 2L E✏￿supf∈F∑T
t=1 ✏tf[t]￿ .
t the adversary picks an entry in an m× n matrix and a value yt for that entry. The learner then
chooses a predicted value ˆyt  and suffers loss `(yt  ˆyt)  assumed to be ⇢-Lipschitz. We deﬁne our
regret with respect to the classF of all matrices whose trace-norm is at most B (namely  we can
B = ⇥(√mn). Consider a transductive version  where we know in advance the location of all
(transductive) Rademacher complexity ofF  which by Theorem 6 of [18]  is O(B√n) independent

of T . Moreover  in [7]  it was shown how one can convert algorithms with such guarantees to obtain
the same regret even in a “fully” online case  where the set of entry locations is unknown in advance.
In this section we use the two alternatives provided for transductive learning problem in the previous
subsection  and provide two alternatives for the matrix completion problem. Both variants proposed
here improve on the one provided by the R2 forecaster in [7]  since that algorithm competes against

use any such matrix to predict just by returning its relevant entry at each round). Usually  one has

entries we need to predict. We show how to develop an algorithm whose regret is bounded by the

variants are also computationally more efﬁcient. Our ﬁrst variant also improves on the recently
proposed method in [9] in terms of memory requirements  and each iteration is simpler: Whereas

case). This can be done very efﬁciently  e.g. with power iterations or the Lanczos method.
Our ﬁrst algorithm follows from Eq. (24)  which for our setting gives the following prediction rule:

the smaller classF′ of matrices with bounded trace-norm and bounded individual entries  and our
that method requires storing and optimizing full m× n matrices every iteration  our algorithm only
requires computing spectral norms of sparse matrices (assuming T ￿ mn  which is usually the
ˆyt= B E
2 xt￿￿￿ (26)
In the above￿⋅￿ stands for the spectral norm and each xi is a matrix with a 1 at some speciﬁc posi-

tion and 0 elsewhere. Notice that the algorithm only involves calculation of spectral norms on each
round  which can be done efﬁciently. As mentioned in previous subsection  one can approximately
evaluate the expectation by sampling several ✏’s on each round and averaging. The second algorithm
follows (25)  and is given by ﬁrst drawing ✏ at random and then predicting

@`(ˆyi  yi)xi− 1

@`(ˆyi  yi)xi+ 1

2 xt￿−￿ T￿i=t+1

✏￿￿￿ T￿i=t+1

✏ixi− 1

✏ixi− 1

t−1￿i=1

t−1￿i=1

2⇢

2⇢

t−1￿i=1

t−1￿i=1

2⇢

2⇢

✏if[xi]− 1

✏if[xi]− 1

￿f￿⌃≤B￿ T￿i=t+1

2 f[xt]￿− sup

`(f[xi]  yi)+ 1

ˆyt(✏)= sup
`(f[xi]  yi)− 1
2 f[xt]￿
￿f￿⌃≤B￿ T￿i=t+1
where￿f￿⌃ is the trace norm of the m×n f  and f[xi] is the entry of the matrix f at the position xi.
mentioned earlier  we get that the expected regret of either variant is O￿B⇢ (√m+√n)￿.

Notice that the above involves solving two trace norm constrained convex optimization problems per
round. As a simple corollary of Lemma 12  together with the bound on the Rademacher complexity

7 Conclusion
In [2  1  15  20] the minimax value of the online learning game has been analyzed and non-
constructive bounds on the value have been provided.
In this paper  we provide a general con-
structive recipe for deriving new (and old) online learning algorithms  using techniques from the
apparently non-constructive minimax analysis. The recipe is rather simple: we start with the notion
of conditional sequential Rademacher complexity  and ﬁnd an “admissible” relaxation which upper
bounds it. This relaxation immediately leads to an online learning algorithm  as well as to an as-
sociated regret guarantee. In addition to the development of a uniﬁed algorithmic framework  our
contributions include (1) a new algorithm for online binary classiﬁcation whenever the Littlestone
dimension of the class is ﬁnite; (2) a family of randomized online learning algorithms based on the
idea of a random playout  with new Follow the Perturbed Leader style algorithms arising as special
cases; and (3) efﬁcient algorithms for trace norm based online matrix completion problem which
improve over currently known methods.

Acknowledgements: We gratefully acknowledge the support of NSF under grants CAREER DMS-
0954737 and CCF-1116928.

8

References
[1] J. Abernethy  A. Agarwal  P. L. Bartlett  and A. Rakhlin. A stochastic view of optimal regret

through minimax duality. In COLT  2009.

[2] J. Abernethy  P. L. Bartlett  A. Rakhlin  and A. Tewari. Optimal strategies and minimax lower

bounds for online convex games. In COLT  2008.

[3] J. Abernethy  M.K. Warmuth  and J. Yellin. Optimal strategies from random walks. In COLT 

pages 437–445  2008.

[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31(3):167–175  2003.

[5] S. Ben-David  D. P´al  and S. Shalev-Shwartz. Agnostic online learning. In COLT  2009.
[6] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University

Press  2006.

[7] N. Cesa-Bianchi and O. Shamir. Efﬁcient online learning via randomized rounding. In NIPS 

2011.

[8] J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the Theory of

Games  3:97–139  1957.

[9] E. Hazan  S. Kale  and S. Shalev-Shwartz. Near-optimal algorithms for online matrix predic-

tion. In COLT  2012.

[10] A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. J. Comput. Syst.

Sci.  71(3):291–307  2005.

[11] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold

algorithm. Machine Learning  2(4):285–318  04 1988.

[12] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Com-

putation  108(2):212–261  1994.

[13] J.F. Mertens  S. Sorin  and S. Zamir. Repeated games. Univ. Catholique de Louvain  Center

for Operations Research & Econometrics  1994.

[14] A.S. Nemirovsky and D.B. Yudin. Problem complexity and method efﬁciency in optimization.

1983.

[15] A. Rakhlin  K. Sridharan  and A. Tewari. Online learning: Random averages  combinatorial

parameters  and learnability. In NIPS  2010. Available at http://arxiv.org/abs/1006.1138.

[16] A. Rakhlin  K. Sridharan  and A. Tewari. Online learning: Beyond regret. In COLT  2011.

Available at http://arxiv.org/abs/1011.3168.

[17] A. Rakhlin  K. Sridharan  and A. Tewari. Online learning: Stochastic  constrained  and

smoothed adversaries. In NIPS  2011. Available at http://arxiv.org/abs/1104.5070.

[18] O. Shamir and S. Shalev-Shwartz. Collaborative ﬁltering with the trace norm: Learning 

bounding  and transducing. In COLT  2011.

[19] S. Sorin. The operator approach to zero-sum stochastic games. Stochastic Games and Appli-

cations  NATO Science Series C  Mathematical and Physical Sciences  570:417–426  2003.

[20] K. Sridharan and A. Tewari. Convex games in banach spaces. In COLT  2010.
[21] V.G. Vovk. Aggregating strategies.

In Proc. Third Workshop on Computational Learning

Theory  pages 371–383. Morgan Kaufmann  1990.

[22] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

2003.

9

,Benigno Uria
Iain Murray
Hugo Larochelle
Felix Xinnan Yu
Ananda Theertha Suresh
Krzysztof Choromanski
Daniel Holtmann-Rice
Sanjiv Kumar