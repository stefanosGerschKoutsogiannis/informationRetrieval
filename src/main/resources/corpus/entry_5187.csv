2019,Splitting Steepest Descent for Growing Neural Architectures,We develop a progressive training approach for neural networks which adaptively grows the network structure by splitting existing neurons to multiple off-springs. By leveraging a functional steepest descent idea   we derive a simple criterion for deciding the best subset of neurons to split and a \emph{splitting gradient} for optimally updating the off-springs. Theoretically  our splitting strategy is a second order functional steepest descent for escaping saddle points in an $\Linfty$-Wasserstein metric space  on which the standard parametric gradient descent is a first-order steepest descent.  Our method provides a new computationally efficient approach for optimizing neural network structures   especially for learning lightweight neural architectures in resource-constrained settings.,Splitting Steepest Descent for Growing Neural

Architectures

Qiang Liu
UT Austin

lqiang@cs.utexas.edu

Lemeng Wu *
UT Austin

lmwu@cs.utexas.edu

Dilin Wang ⇤
UT Austin

dilin@cs.utexas.edu

Abstract

We develop a progressive training approach for neural networks which adaptively
grows the network structure by splitting existing neurons to multiple off-springs.
By leveraging a functional steepest descent idea  we derive a simple criterion for
deciding the best subset of neurons to split and a splitting gradient for optimally
updating the off-springs. Theoretically  our splitting strategy is a second-order
functional steepest descent for escaping saddle points in an 1-Wasserstein metric
space  on which the standard parametric gradient descent is a ﬁrst-order steepest
descent. Our method provides a new practical approach for optimizing neural
network structures  especially for learning lightweight neural architectures in
resource-constrained settings.

1

Introduction

Deep neural networks (DNNs) have achieved remarkable empirical successes recently. However 
efﬁcient and automatic optimization of model architectures remains to be a key challenge. Compared
with parameter optimization which has been well addressed by gradient-based methods (a.k.a.
back-propagation)  optimizing model structures involves signiﬁcantly more challenging discrete
optimization with large search spaces and high evaluation cost. Although there have been rapid
progresses recently  designing the best architectures still requires a lot of expert knowledge and
trial-and-errors for most practical tasks.
This work targets extending the power of gradient descent to the domain of model structure optimiza-
tion of neural networks. In particular  we consider the problem of progressively growing a neural
network by “splitting” existing neurons into several “off-springs”  and develop a simple and practical
approach for deciding the best subset of neurons to split and how to split them  adaptively based on
the existing model structure. We derive the optimal splitting strategies by considering the steepest
descent of the loss when the off-springs are inﬁnitesimally close to the original neurons  yielding a
splitting steepest descent that monotonically decrease the loss in the space of model structures.
Our main method  shown in Algorithm 1  alternates between a standard parametric descent phase
which updates the parameters to minimize the loss with a ﬁxed model structure  and a splitting phase
which updates the model structures by splitting neurons. The splitting phase is triggered when no
further improvement can be made by only updating parameters  and allow us to escape the parametric
local optima by augmenting the neural network in a locally optimal fashion. Theoretically  these two
phases can be viewed as performing functional steepest descent on an 1-Wasserstein metric space  in
which the splitting phase is a second-order descent for escaping saddle points in the functional space 
while the parametric gradient descent corresponds to a ﬁrst-order descent. Empirically  our algorithm
is simple and practical  and provides a promising tool for many challenging problems  including
progressive training of interpretable neural networks  learning lightweight and energy-efﬁcient neural
architectures for resource-constrained settings  and transfer learning  etc.

⇤Equal contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Related Works The idea of progressively growing neural networks by node splitting is not new 
but previous works are mostly based on heuristic or purely random splitting strategies (e.g.  Wynne-
Jones  1992; Chen et al.  2016). A different approach for progressive training is the Frank-Wolfe or
gradient boosting based strategies (e.g.  Schwenk & Bengio  2000; Bengio et al.  2006; Bach  2017) 
which iteratively add new neurons derived from functional conditional gradient  while keeping the
previous neurons ﬁxed. However  these methods are not suitable for large scale settings  because
adding each neuron requires to solve a difﬁcult non-convex optimization problem  and keeping the
previous neurons ﬁxed prevents us from correcting the mistakes made in earlier iterations. A practical
alternative of Frank-Wolfe is to simply add new randomly initialized neurons and co-optimize the
new and old neurons together. However  random initialization does not allow us to leverage the
information of the existing model and takes more time to converge. In contrast  splitting neurons
from the existing network allows us to inherent the knowledge from the existing model (see Chen
et al. (2016))  and is faster to converge in settings like continual learning  when the previous model is
not far away from the optimal solution.
An opposite direction of progressive training is to prune large pre-trained networks (e.g.  Han et al. 
2016; Li et al.  2017; Liu et al.  2017). In comparison  our splitting method requires no large pre-
trained models and can outperform existing pruning methods in terms of learning ultra-small neural
architectures  which is of critical importance for resource-constrained settings like mobile devices
and Internet of things. More broadly  there has been a series of recent works on neural architecture
search  based on various strategies from combinatorial optimization  including reinforcement learning
(RL) (e.g.  Pham et al.  2018; Cai et al.  2018; Zoph & Le  2017)  evolutionary algorithms (EA) (e.g. 
Stanley & Miikkulainen  2002; Real et al.  2018)  and continuous relaxation (e.g.  Liu et al.  2019a;
Xie et al.  2018). However  these general-purpose black-box optimization methods do not leverage
the inherent geometric structure of the loss landscape  and are highly computationally expensive due
to the need of evaluating the candidate architectures based on inner training loops.

Background: Steepest Descent and Saddle Points Stochastic gradient descent is the driving
horse for solving large scale optimization in machine learning and deep learning. Gradient descent
can be viewed as a steepest descent procedure that iteratively improves the solution by following the
direction that maximally decreases the loss function within a small neighborhood of the previous
solution. Speciﬁcally  for minimizing a loss function L(✓)  each iteration of steepest descent updates
the parameter via ✓ ✓ + ✏  where ✏ is a small step size and  is an update direction chosen to
maximally decrease the loss L(✓ + ✏) of the updated parameter under a norm constraint kk  1 
where k·k denotes the Euclidean norm. When rL(✓) 6= 0 and ✏ is inﬁnitesimal  the optimal descent
direction  equals the negative gradient direction  that is   = rL(✓)/krL(✓)k  yielding a descent
of L(✓ + ✏)  L(✓) ⇡ ✏krL(✓)k. At a critical point with a zero gradient (rL(✓) = 0)  the
steepest descent direction depends on the spectrum of the Hessian matrix r2L(✓). Denote by min
the minimum eigenvalue of r2L(✓) and vmin its associated eigenvector. When min > 0  the point ✓
is a stable local minimum and no further improvement can be made in the inﬁnitesimal neighborhood.
When min < 0  the point ✓ is a saddle point or local maximum  and the steepest descent direction
equals the eigenvector ±vmin  which yields an ✏2min/2 decrease on the loss.2 In practice  it has
been shown that there is no need to explicitly calculate the negative eigenvalue direction  because
saddle points and local maxima are unstable and can be escaped by using gradient descent with
random initialization or stochastic noise (e.g.  Lee et al.  2016; Jin et al.  2017).

2 Splitting Neurons Using Steepest Descent

We introduce our main method in this section. We ﬁrst illustrate the idea with the simple case of
splitting a single neuron in Section 2.1  and then consider the more general case of simultaneously
splitting multiple neurons in deep networks in Section 2.2  which yields our main progressive training
algorithm (Algorithm 1). Section 2.3 draws a theoretical discussion and interpret our procedure as a
functional steepest descent of the distribution of the neuron weights under the 1-Wasserstein metric.

2The property of the case when min = 0 depends on higher order information.

2

2.1 Splitting a Single Neuron
Let (✓  x) be a neuron inside a neural network that we want to learn from data  where ✓ is the
parameter of the neuron and x its input variable. Assume the loss of ✓ has a general form of

L(✓) := Ex⇠D[((✓  x))] 

(1)
where D is a data distribution  and  is a map determined by the overall loss function. The parameters
of the other parts of the network are assumed to be ﬁxed or optimized using standard procedures and
are omitted for notation convenience.
Standard gradient descent can only yield parametric updates of ✓. We introduce a generalized steepest
descent procedure that allows us to incrementally grow the neural network by gradually introducing
new neurons  achieved by “splitting” the existing neurons into multiple copies in a (locally) optimal
fashion derived using ideas from steepest descent idea.
In particular  we split ✓ into m off-springs ✓ := {✓i}m
i=1 
and replace the neuron (✓  x) with a weighted sum of
the off-spring neurons Pm
i=1 wi(✓i  x)  where w :=
i=1 is a set of positive weights assigned on the off-
springs  and satisﬁesPm
i=1 wi = 1  wi > 0. This yields

{wi}m
an augmented loss function on ✓ and w:

w/2 w/2

w

...
a

b

...
a a

b b

L(✓  w) := Ex⇠D" mXi=1

wi(✓i  x)!# .

(2)

A key property of this construction is that it introduces a smooth change on the loss function when
the off-springs {✓i}m
i=1 are close to the original parameter ✓: when ✓i = ✓  8i = 1  . . .   m  the
augmented network and loss are equivalent to the original ones  that is  L(✓1m  w) = L(✓)  where
1m denotes the m ⇥ 1 vector consisting of all ones; when all the {✓i} are within an inﬁnitesimal
neighborhood of ✓  it yields an inﬁnitesimal change on the loss  with which a steepest descent can be
derive.
Formally  consider the set of splitting schemes (m  ✓  w) whose off-springs are ✏-close to the original
neuron:

mXi=1

{(m  ✓  w) : m 2 N+  k✓i  ✓k  ✏ 

wi = 1  wi > 0  8i = 1  . . .   m}.

We want to decide the optimal (m  ✓  w) to maximize the decrease of loss L(✓  w)  L(✓)  when the
step size ✏ is inﬁnitesimal. Although this appears to be an inﬁnite dimensional optimization because
m is allowed to be arbitrarily large  we show that the optimal choice is achieved with either m = 1
(no splitting) or m = 2 (splitting into two off-springs)  with uniform weights wi = 1/m. Whether a
neuron should be split (m = 1 or 2) and the optimal values of the off-springs {✓i} are decided by
the minimum eigenvalue and eigenvector of a splitting matrix  which plays a role similar to Hessian
matrix for deciding saddle points.

Deﬁnition 2.1 (Splitting Matrix). For L(✓) in (1)  its splitting matrix S(✓) is deﬁned as

S(✓) = Ex⇠D[0((✓  x))r2
(3)
We call the minimum eigenvalue min(S(✓)) of S(✓) the splitting index of ✓  and the eigenvector
vmin(S(✓)) related to min(S(✓)) the splitting gradient of ✓.

✓✓(✓  x)].

The splitting matrix S(✓) is a Rd⇥d symmetric “semi-Hessian” matrix that involves the ﬁrst derivative
0(·)  and the second derivative of (✓  x). It is useful to compare it with the typical gradient and
Hessian matrix of L(✓):
r✓L(✓) = Ex⇠D[0((✓  x))r✓(✓  x)] 
✓✓L(✓) = S(✓) + E[00((✓  x))r✓(✓  x)⌦2]
r2
}
where v⌦2 := vv> is the outer product. The splitting matrix S(✓) differs from the gradient r✓L(✓)
in replacing r✓(✓  x) with the second-order derivative r2
✓✓(✓  x)  and differs from the Hessian

{z

T (✓)

|

 

3

✓✓L(✓) in missing an extra term T (✓). We should point out that S(✓) is the “easier part”
matrix r2
of the Hessian matrix  because the second-order derivative r2
✓✓(✓  x) of the individual neuron 
is much simpler than the second-order derivative 00(·) of “everything else”  which appears in the
extra term T (✓). In addition  as we show in Section 2.2  S(✓) is block diagonal in terms of multiple
neurons  which is crucial for enabling practical computational algorithm.
It is useful to decompose each ✓i into ✓i = ✓ + ✏(µ + i)  where µ is an average displacement vector

into two terms that reﬂect the effects of the average displacement and splitting  respectively.

shared by all copies  and i is the splitting vector associated with ✓i  and satisﬁesPi wii = 0 (which
impliesPi wi✓i = ✓ + ✏µ). It turns out that the change of loss L(✓  w) L(✓) naturally decomposes
Theorem 2.2. Assume ✓i = ✓ + ✏(µ + i) withPi wii = 0 andPi wi = 1. For L(✓) and L(✓  w)
in (1) and (2)  assume L(✓  w) has bounded third order derivatives w.r.t. ✓. We have
L(✓  w)  L(✓) = ✏rL(✓)>µ +

+ O(✏3) 

wi>i S(✓)i

(4)

+

mXi=1

✏2
2

|

II (  w; ✓)

{z

}

I(µ; ✓) = L(✓ + ✏µ)  L(✓) + O(✏3)

✏2
2

{z

µ>r2L(✓)µ
}

|

where the change of loss is decomposed into two terms: the ﬁrst term I(µ; ✓) is the effect of the
average displacement µ  and it is equivalent to applying the standard parametric update ✓ ✓ + ✏µ
on L(✓). The second term II (  w; ✓) is the change of the loss caused by the splitting vectors
 := {i}. It depends on L(✓) only through the splitting matrix S(✓).
Therefore  the optimal average displacement µ should be decided by standard parametric steepest
(gradient) descent  which yields a typical O(✏) decrease of loss at non-stationary points. In compari-
son  the splitting term II(  w; ✓) is always O(✏2)  which is much smaller. Given that introducing
new neurons increases model size  splitting should not be preferred unless it is impossible to achieve
an O(✏2) gain with pure parametric updates that do not increase the model size. Therefore  it is
motivated to introduce splitting only at stable local minima  when the optimal µ equals zero and no
further improvement is possible with (inﬁnitesimal) regular parametric descent on L(✓). In this case 
we only need to minimize the splitting term II(  w; ✓) to decide the optimal splitting strategy  which
is shown in the following theorem.

Theorem 2.3. a) If the splitting matrix is positive deﬁnite  that is  min(S(✓)) > 0  we have
II(  w; ✓) > 0 for any w > 0 and  6= 0  and hence no inﬁnitesimal splitting can decrease the loss.
We call that ✓ is splitting stable in this case.
b) If min(S(✓)) < 0  an optimal splitting strategy that minimizes II(  w; ✓) subject to kik  1 is

m = 2 

w1 = w2 = 1/2 

and

1 = vmin(S(✓)) 

2 = vmin(S(✓)) 

where vmin(S(✓))  called the splitting gradient  is the eigenvector related to min(S(✓)). Here we
split the neuron into two copies of equal weights  and update each copy with the splitting gradient.
The change of loss obtained in this case is II({1 1} {1/2  1/2}; ✓) = ✏2min(S(✓))/2 < 0.
Remark The splitting stability (S(✓)  0) does not necessarily ensure the standard parametric
stability of L(✓) (i.e.  r2L(✓) = S(✓) + T (✓)  0)  except when (·) is convex which ensures
T (✓) ⌫ 0 (see Deﬁnition 2.1). If both S(✓)  0 and r2L(✓)  0 hold  the loss can not be improved
by any local update or splitting  no matter how many off-springs are allowed. Since stochastic
gradient descent guarantees to escape unstable stationary points (Lee et al.  2016; Jin et al.  2017)  we
only need to calculate S(✓) to decide the splitting stability in practice.

2.2 Splitting Deep Neural Networks

In practice  we need to split multiple neurons simultaneously  which may be of different types  or
locate in different layers of a deep neural network. The key questions are if the optimal splitting
strategies of different neurons inﬂuence each other in some way  and how to compare the gain of
splitting different neurons and select the best subset of neurons to split under a budget constraint.
It turns out the answers are simple. We show that the change of loss caused by splitting a set of
neurons is simply the sum of the splitting terms II(  w; ✓) of the individual neurons. Therefore  we

4

Algorithm 1 Splitting Steepest Descent for Optimizing Neural Architectures

Initialize a neural network with a set of neurons ✓[1:n] = {✓[`]}n
`=1 that can be split  whose loss
satisﬁes (5). Decide a maximum number m⇤ of neurons to split at each iteration  and a threshold
⇤  0 of the splitting index. A stepsize ✏.
1. Update the parameters using standard optimizers (e.g.  stochastic gradient descent) until no
further improvement can be made by only updating parameters.
2. Calculate the splitting matrices {S[`]} of the neurons following (7)  as well as their minimum
eigenvalues {[`]
3. Select the set of neurons to split by picking the top m⇤ neurons with the smallest eigenvalues
{[`]
4. Split each of the selected neurons into two off-springs with equal weights  and update the
neuron network by replacing each selected neuron `(✓[`]  ·) with
✓[`]
1 ✓[`] + ✏v[`]

min} and the associated eigenvectors {v[`]

min} and satisﬁes [`]

2 ✓[`]  ✏v[`]
min.

1   ·) + `(✓[`]

min  ⇤.

2   ·)) 

min}.

(`(✓[`]

where

min ✓

1
2

[`]

Update the list of neurons. Go back to Step 1 or stop when a stopping criterion is met.

can calculate the splitting matrix of each neuron independently without considering the other neurons 
and compare the “splitting desirability” of the different neurons by their minimum eigenvalues
(splitting indexes). This motivates our main algorithm (Algorithm 1)  in which we progressively split
the neurons with the most negative splitting indexes following their own splitting gradients. Since the
neurons can be in different layers and of different types  this provides an adaptive way to grow neural
network structures to ﬁt best with data.
To set up the notation  let ✓[1:n] = {✓[1]  . . .✓ [n]} be the parameters of a set of neurons (or any
duplicable sub-structures) in a large neural network  where ✓[`] is the parameter of the `-th neuron.
Assume we split ✓[`] into m` copies ✓[`] := {✓[`]
i=1 satisfying
Pm`
i  0  8i = 1  . . .   m`. Denote by L(✓[1:n]) and L(✓[1:n]  w[1:n]) the
loss function of the original and augmented networks  respectively. It is hard to specify the actual
expression of the loss functions in general cases  but it is sufﬁcient to know that L(✓[1:n]) depends on
each ✓[`] only through the output of its related neuron 

i=1  with weights w[`] = {w[`]

i = 1 and w[`]

i=1 w[`]

i }m`

i }m`

L(✓[1:n]) = Ex⇠Dh`⇣`⇣✓[`]  h[`]⌘ ; ✓[¬`]⌘i  

h[`] = g`(x; ✓[¬`]) 

(5)

where ` denotes the activation function of neuron `  and g` and ` denote the parts of the loss
that connect to the input and output of neuron `  respectively  both of which depend on the other
parameters ✓[¬`] in some complex way. Similarly  the augmented loss L(✓[1:n]  w[1:n]) satisﬁes

L(✓[1:n]  w[1:n]) = Ex⇠D"` m`Xi=1

wi`⇣✓[`]

i

  h[`]⌘ ; ✓[¬`]  w[¬`]!#  

where h[`] = g`(x; ✓[¬`]  w[¬`])  and g`  ` are the augmented variants of g`  `  respectively.
Interestingly  although each equation in (5) and (6) only provides a partial speciﬁcation of the loss
function of deep neural nets  they together are sufﬁcient to establish the following key extension of
Theorem 2.2 to the case of multiple neurons.

(6)

Theorem 2.4. Under the setting above  assume ✓[`]
µ[`] denotes the average displacement vector on ✓[`]  and [`]
i

i ) for 8` 2 [1 : n]  where
is the i-th splitting vector of ✓[`]  with
i = 0. Assume L(✓[1:n]  w[1:n]) has bounded third order derivatives w.r.t. ✓[1:n]. We have

i = ✓[`] + ✏(µ[`] + [`]

i=1 wi[`]

L(✓[1:n]  w[1:n]) = L(✓[1:n] + ✏µ[1:n]) +

w[`]
i [`]

i

>S[`](✓[1:n])[`]

i

+O(✏3) 

Pm`

m`Xi=1

nX`=1

✏2
2

|

5

II`([`]  w[`]; ✓[1:n])

{z

}

where the effect of average displacement is again equivalent to that of the corresponding parametric
update ✓[1:n] ✓[1:n] + ✏µ[1:n]; the splitting effect equals the sum of the individual splitting terms
II`([`]  w[`]; ✓[1:n])  which depends on the splitting matrix S[`](✓[1:n]) of neuron ` 

S[`](✓[1:n]) = Ex⇠Dhr``⇣`⇣✓[`]  h[`]⌘ ; ✓[¬`]⌘r2

✓✓`⇣✓[`]  h[`]⌘i .

(7)

The important implication of Theorem 2.4 is that there is no crossing term in the splitting matrix 
unlike the standard Hessian matrix. Therefore  the splitting effect of an individual neuron only
depends on its own splitting matrix and can be evaluated individually; the splitting effects of different
neurons can be compared using their splitting indexes  allowing us to decide the best subset of neurons
to split when a maximum number constraint is imposed. As shown in Algorithm 1  we decide a
maximum number m⇤ of neurons to split at each iteration  and a threshold ⇤  0 of splitting index 
and split the neurons whose splitting indexes are ranked in top m⇤ and smaller than ⇤.
Computational Efﬁciency The computational cost of exactly evaluating all the splitting indexes
and gradients on a data instance is O(nd3)  where n is the number of neurons and d is the number
of the parameters of each neuron. Note that this is much better than evaluating the Hessian matrix 
which costs O(N 3)  where N is the total number of parameters (e.g.  N  nd). In practice  d
is not excessively large or can be controlled by identifying a subset of important neurons to split.
Further computational speedup can be obtained by using efﬁcient gradient-based large scale eigen-
computation methods  which we investigate in future work.

2.3 Splitting as 1-Wasserstein Steepest Descent
We present a functional aspect of our approach  in which we frame the co-optimization of the neural
parameters and structures into a functional optimization in the space of distributions of the neuron
weights  and show that our splitting strategy can be viewed as a second-order descent for escaping
saddle points in the 1-Wasserstein space of distributions  while the standard parametric gradient
descent corresponds to a ﬁrst-order descent in the same space.
We illustrate our theory using the single neuron case in Section 2.1. Consider the augmented loss
L(✓  w) in (2). Because the off-springs of the neuron are exchangeable  we can equivalently represent
L(✓  w) as a functional of the empirical measure of the off-springs 

L[⇢] = Ex⇠D [ (E✓⇠⇢[(✓  x)])]  ⇢

=

wi✓i 

(8)

where ✓i denotes the delta measure on ✓i and L[⇢] is the functional representation of L(✓  w). The
idea is to optimize L[⇢] in the space of probability distributions (or measures) using a functional
steepest descent. To do so  a notion of distance on the space of distributions need to be decided. We
consider the p-Wasserstein metric 

mXi=1

Dp(⇢  ⇢0) = inf

2⇧(⇢ ⇢0)E(✓ ✓0)⇠[k✓  ✓0kp]1/p  

for p > 0 

(9)

where ⇧(⇢  ⇢0) denotes the set of probability measures whose ﬁrst and second marginals are ⇢ and
⇢0  respectively  and  can be viewed as describing a transport plan from ⇢ to ⇢0. We obtain the
1-Wasserstein metric D1(⇢  ⇢0) in the limit when p ! +1  in which case the p-norm reduces to
an esssup norm  that is 

D1(⇢  ⇢0) = inf

2⇧(⇢ ⇢0)

esssup
(✓ ✓0)⇠

[k✓  ✓0k] 

where the esssup notation denotes the smallest number c such that the set {(✓  ✓0) : k✓  ✓0k > c}
has zero probability under . See more discussion in Villani (2008) and Appendix A.2.
The 1-Wasserstein metric yields a natural connection to node splitting. For each ✓  the conditional
distribution (✓0 | ✓) represents the distribution of points ✓0 transported from ✓  which can be viewed
as the off-springs of ✓ in the context of node splitting. If D1(⇢  ⇢0)  ✏  it means that ⇢0 can be
obtained from splitting ✓ ⇠ ⇢ such that all the off-springs are ✏-close  i.e.  k✓0  ✓k  ✏. This is
consistent with the augmented neighborhood introduced in Section 2.1  except that  here can be an
absolutely continuous distribution  representing a continuously inﬁnite number of off-springs; but this

6

yields no practical difference because any distribution  can be approximated arbitrarily close using a
countable number of particles. Note that p-Wasserstein metrics with ﬁnite p are not suitable for our
purpose because Dp(⇢  ⇢0)  ✏ with p < 1 does not ensure k✓0  ✓k  ✏ for all ✓ ⇠ ⇢ and ✓0 ⇠ ⇢0.
Similar to the steepest descent on the Euclidean space  the 1-Wasserstein steepest descent on L[⇢]
should iteratively ﬁnd new points that maximize the decrease of loss in an ✏-ball of the current points.
Deﬁne

⇢⇤ = arg min

⇢0

{L[⇢0] L [⇢] : D1(⇢  ⇢0)  ✏} 

⇤(⇢  ✏) = L[⇢⇤] L [⇢].
We are ready to show the connection of Algorithm 1 to the 1-Wasserstein steepest descent.
Theorem 2.5. Consider the L(✓  w) and L[⇢] in (2) and (8)  connected with ⇢ =Pi wi✓i. Deﬁne
✓✓(✓  x)⇤ with f⇢(x) =
G⇢(✓) = Ex⇠D [0(f⇢(x))r✓(✓  x)] and S⇢(✓) = Ex⇠D⇥0(f⇢(x))r2
E✓⇠⇢[(✓  x)]  which are related to the gradient and splitting matrices of L(✓  w)  respectively.
Assume L(✓  w) has bounded third order derivatives w.r.t. ✓.
a) If L(✓  w) is on a non-stationary point w.r.t. ✓  then the steepest descent of L[⇢] is achieved by
moving all the particles of ⇢ with gradient descent on L(✓  w)  that is 

L[(I  ✏G⇢)]⇢] L [⇢] = ⇤(⇢  ✏) + O(✏2) = ✏E✓⇠⇢[kG⇢(✓)k] + O(✏2) 
where (I  ✏G⇢)]⇢ denotes the distribution of ✓0 = ✓  ✏G⇢(✓)/kG⇢(✓)k when ✓ ⇠ ⇢.
b) If L(✓  w) reaches a stable local optima w.r.t. ✓  the steepest descent on L[⇢] is splitting each
neuron with min(S⇢(✓)) < 0 into two copies of equal weights following their minimum eigenvectors 
while keeping the remaining neurons to be unchanged. Precisely  denote by (I ± ✏vmin(S⇢(✓))+)]⇢
the distribution obtained in this way  we have

L[(I ± ✏vmin(S⇢(✓))+)]⇢] L [⇢] = ⇤(⇢  ✏) + O(✏3) 

where we have ⇤(⇢  ✏) = ✏2E✓⇠⇢[min(min(S⇢(✓))  0)]/2.
Remark There has been a line of theoretical works on analyzing gradient-based learning of neural
networks via 2-Wasserstein gradient ﬂow by considering the mean ﬁeld limit when the number of
neurons m goes to inﬁnite (m ! 1) (e.g.  Mei et al.  2018; Chizat & Bach  2018). These analysis
focus on the ﬁrst-order descent on the 2-Wasserstein space as a theoretical tool for understanding the
behavior of gradient descent on overparameterized neural networks. Our framework is signiﬁcant
different  since we mainly consider the second-order descent on the 1-Wasserstein space  and the
case of ﬁnite number of neurons m in order to derive practical algorithms.

3 Experiments

We test our method on both toy and realistic tasks  including learning interpretable neural networks 
architecture search for image classiﬁcation and energy-efﬁcient keyword spotting. Due to limited
space  many of the detailed settings are shown in Appendix  in which we also include additional
results on distribution approximation (Appendix C.1)  transfer learning (Appendix C.2).

s
e
u
l
a
v
n
e
g
i
E

e
s
a
e
r
c
e
d

s
s
o
L

s
s
o
L
g
n
i
n
i
a
r
T

(a) x

(b)

(c) Angle

(d) #Iteration

Figure 1: Results on a one-dimensional RBF network. (a) The true and estimated functions. (b) The eigenvalue
vs. loss decrease. (c) The loss decrease vs. the angle of the splitting direction with the minimum eigenvector. (d)
The training loss vs. the iteration (of gradient descent); the splittings happen at the cliff points.

7

012340.0−0.2−0.4−0.6(Lgenvalue0.0000.0010.0020.003Loss decrease (one step)0200N400N0.00.20.40.60.82StimaO 6SOit (Rurs)RanGRm 6SOitNew InitiaOizatiRnGraGient BRRstingBaseOine (scratch)Toy RBF Neural Networks We apply our method to learn a one-dimensional RBF neural network
shown in Figure 1a. See Appendix B.1 for details of the setting. We start with a small neural network
with m = 1 neuron and gradually increase the model size by splitting neurons. Figure 1a shows
that we almost recover the true function as we split up to m = 8 neurons. Figure 1b shows the top
ﬁve eigenvalues and the decrease of loss when we split m = 7 neurons to m = 8 neurons; we can
see that the eigenvalue and loss decrease correlate linearly  conﬁrming our results in Theorem 2.4.
Figure 1c shows the decrease of the loss when we split the top one neuron following the direction
with different angles from the minimum eigenvector at m = 7. We can see that the decrease of the
loss is maximized when the splitting direction aligns with the eigenvector  consistent with our theory.
In Figure 1d  we compare with different baselines of progressive training  including Random Split 
splitting a randomly chosen neuron with a random direction; New Initialization  adding a new
neuron with randomly initialized weights and co-optimization it with previous neurons; Gradient
Boosting  adding new neurons with Frank-Wolfe algorithm while ﬁxing the previous neurons;
Baseline (scratch)  training a network of size m = 8 from scratch. Figure 1d shows our method
yields the best result.

Learning Interpretable Neural Networks To visualize the dynamics of the splitting process  we
apply our method to incrementally train an interpretable neural network designed by Li et al. (2018) 
which contains a “prototype layer” whose weights are enforced to be similar to realistic images to
encourage interpretablity. See Appendix B.2 and Li et al. (2018) for more detailed settings. We apply
our method to split the prototype layer starting from a single neuron on MNIST  and show in Figure 2
the evolutionary tree of the neurons in our splitting process. We can see that the blurry (and hence
less interpretable) prototypes tend to be selected and split into two off-springs that are similar yet
more interpretable. Figure 2 (b) shows the decrease of loss when we split each of the ﬁve neurons at
the 5-th step (with the decrease of loss measured at the local optima reached dafter splitting); we ﬁnd
that the eigenvalue correlates well with the decrease of loss and the interpretablity of the neurons. The
complete evolutionary tree and quantitative comparison with baselines are shown in Appendix B.2.

Eigenvalue

Loss decay (splitting + ﬁnetune) 

s
e
u
l
a
v
n
e
g
i
E

e
s
a
e
r
c
e
d

s
s
o
L

(a)

(b)

Figure 2: Progressive learning of the interpretable prototype network in Li et al. (2018) on MNIST. (a) The
evolutionary tree of our splitting process  in which the least interpretable  or most ambiguous prototypes tend to
be split ﬁrst. (b) The eigenvalue and resulting loss decay when splitting the different neurons at the 5-th step.

Lightweight Neural Architectures for Image Classiﬁcation We investigate the effectiveness of
our methods in learning small and efﬁcient network structures for image classiﬁcation. We experiment
with two popular deep neural architectures  MobileNet (Howard et al.  2017) and VGG19 (Simonyan
& Zisserman  2015). In both cases  we start with a relatively small network and gradually grow
the network by splitting the convolution ﬁlters following Algorithm 1. See Appendix B.3 for more
details of the setting. Because there is no other off-the-shelf progressive growing algorithm that
can adaptively decide the neural architectures like our method  we compare with pruning methods 
which follow the opposite direction of gradually removing neurons starting from a large pre-trained
network. We test two state-of-the-art pruning methods  including batch-normalization-based pruning
(Bn-prune) (Liu et al.  2017) and L1-based pruning (L1-prune) (Li et al.  2017). As shown in
Figure 3a-b  our splitting method yields higher accuracy with similar model sizes. This is surprising
and signiﬁcant  because the pruning methods leverage the knowledge from a large pre-train model 
while our method does not.
To further test the effect of architecture learning in both splitting and pruning methods  we test
another setting in which we discard the weights of the neurons and retain the whole network starting
from a random initialization  under the structure obtained from splitting or pruning at each iteration.

8

MobileNet (ﬁnetune)

VGG19 (ﬁnetune)

MobileNet (retrain)

VGG19 (retrain)

y
c
a
r
u
c
c
A

t
s
e
T

(a)

Ratio

(b)

Ratio

(c)

Ratio

(d) Ratio

Figure 3: Results on CIFAR-10. (a)-(b) Results of Algorithm 1 and pruning methods (which successively
ﬁnetune the neurons after pruning). (c)-(d) Results of Algorithm 1 and prunning methods with retrainning  in
which we retrain all the weights starting from random initialization after each splitting or pruning step. The
x-axis represents the ratio between the number parameters of the learned models and a full size baseline network.

As shown in Figure 3c-d  the results of retraining is comparable with (or better than) the result of
successive ﬁnetuning in Figure 3a-b  which is consistent with the ﬁndings in Liu et al. (2019b).
Meanwhile  our splitting method still outperforms both Bn-prune and L1-prune.

Resource-Efﬁcient Keyword Spotting on Edge Devices Keyword spotting systems aim to detect
a particular keyword from a continuous stream of audio. It is typically deployed on energy-constrained
edge devices and requires real-time response and high accuracy for good user experience. This casts
a key challenge of constructing efﬁcient and lightweight neural architectures. We apply our method
to solve this problem  by splitting a small model (a compact version of DS-CNN) obtained from
Zhang et al. (2017). See Appendix B.4 for detailed settings.
Table 1 shows the results on the Google speech commands benchmark dataset (Warden  2018)  in
which our method achieves signiﬁcantly higher accuracy than the best model (DS-CNN) found by
Zhang et al. (2017)  while having 31% less parameters and Flops. Figure 4 shows further comparison
with Bn-prune (Liu et al.  2017)  which is again inferior to our method.

Acc Params (K) Ops (M)
Method
86.94
DNN
CNN
92.64
BasicLSTM 93.62
94.11
LSTM
GRU
94.72
94.21
CRNN
94.85
DS-CNN
95.36
Ours
Table 1: Results on keyword spotting. All
results are averaged over 5 rounds.

495.7
476.7
492.6
495.8
498.0
485.0
413.7
282.6

1.0
25.3
47.9
48.4
48.4
19.3
56.9
39.2

y
c
a
r
u
c
c
A

t
s
e
T

#Params

#Ops

Figure 4: Comparison of accuracy vs. model size (#Params)
and number of ﬂops (#Ops) on keyword spotting.

4 Conclusion

We present a simple approach for progressively training neural networks via neuron splitting. Our ap-
proach highlights a novel view of neural structure optimization as continuous functional optimization 
and yields a practical procedure with broad applications. For future work  we will further investigate
fast gradient descent based approximation of large scale eigen-computation and more theoretical
analysis  extensions and applications of our approach.

Acknowledgement

This work is supported in part by NSF CRII 1830161 and NSF CAREER 1846421. We would like to
acknowledge Google Cloud and Amazon Web Services (AWS) for their support.

9

OursBn-pruneL1-pruneBaseOLne0.200.400.9092949694.85002004006009092949694.85OursPruneDS-CNNReferences

Bach  Francis. Breaking the curse of dimensionality with convex neural networks. Journal of Machine

Learning Research  18(19):1–53  2017.

Bengio  Yoshua  Roux  Nicolas L  Vincent  Pascal  Delalleau  Olivier  and Marcotte  Patrice. Convex

neural networks. In Advances in neural information processing systems  pp. 123–130  2006.

Cai  Han  Zhu  Ligeng  and Han  Song. Proxylessnas: Direct neural architecture search on target task

and hardware. In International Conference on Learning Representation  2018.

Chen  Tianqi  Goodfellow  Ian  and Shlens  Jonathon. Net2net: Accelerating learning via knowledge

transfer. In International Conference on Learning Representations  2016.

Chen  Yutian  Welling  Max  and Smola  Alex. Super-samples from kernel herding. In Conference on

Uncertainty in Artiﬁcial Intelligence (UAI)  2010.

Chizat  Lenaic and Bach  Francis. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In Advances in neural information processing
systems  pp. 3036–3046  2018.

Gretton  Arthur  Borgwardt  Karsten M  Rasch  Malte J  Schölkopf  Bernhard  and Smola  Alexander.

A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773  2012.

Han  Song  Mao  Huizi  and Dally  William J. Deep compression: Compressing deep neural networks
with pruning  trained quantization and huffman coding. International Conference on Learning
Representations  2016.

Howard  Andrew G  Zhu  Menglong  Chen  Bo  Kalenichenko  Dmitry  Wang  Weijun  Weyand 
Tobias  Andreetto  Marco  and Adam  Hartwig. Mobilenets: Efﬁcient convolutional neural
networks for mobile vision applications. arXiv preprint arXiv:1704.04861  2017.

Jin  Chi  Ge  Rong  Netrapalli  Praneeth  Kakade  Sham M  and Jordan  Michael I. How to escape
saddle points efﬁciently. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70  pp. 1724–1732. JMLR. org  2017.

Lee  Jason D  Simchowitz  Max  Jordan  Michael I  and Recht  Benjamin. Gradient descent only

converges to minimizers. In Conference on learning theory  pp. 1246–1257  2016.

Li  Hao  Kadav  Asim  Durdanovic  Igor  Samet  Hanan  and Graf  Hans Peter. Pruning ﬁlters for

efﬁcient convnets. International Conference on Learning Representations  2017.

Li  Oscar  Liu  Hao  Chen  Chaofan  and Rudin  Cynthia. Deep learning for case-based reasoning
In Thirty-Second AAAI

through prototypes: A neural network that explains its predictions.
Conference on Artiﬁcial Intelligence  2018.

Liu  Hanxiao  Simonyan  Karen  and Yang  Yiming. Darts: Differentiable architecture search.

International Conference on Learning Representations  2019a.

Liu  Zhuang  Li  Jianguo  Shen  Zhiqiang  Huang  Gao  Yan  Shoumeng  and Zhang  Changshui.
Learning efﬁcient convolutional networks through network slimming. In Proceedings of the IEEE
International Conference on Computer Vision  pp. 2736–2744  2017.

Liu  Zhuang  Sun  Mingjie  Zhou  Tinghui  Huang  Gao  and Darrell  Trevor. Rethinking the value of

network pruning. International Conference on Learning Representations  2019b.

Mei  Song  Montanari  Andrea  and Nguyen  Phan-Minh. A mean ﬁeld view of the landscape of

two-layers neural networks. Proceedings of the National Academy of Sciences of USA  2018.

Pham  Hieu  Guan  Melody  Zoph  Barret  Le  Quoc  and Dean  Jeff. Efﬁcient neural architecture
search via parameter sharing. In International Conference on Machine Learning  pp. 4092–4101 
2018.

Rahimi  Ali and Recht  Benjamin. Random features for large-scale kernel machines. In Advances in

Neural Information Processing Systems  pp. 1177–1184  2007.

Real  Esteban  Aggarwal  Alok  Huang  Yanping  and Le  Quoc V. Regularized evolution for image

classiﬁer architecture search. ICML AutoML Workshop  2018.

Schwenk  Holger and Bengio  Yoshua. Boosting neural networks. Neural computation  12(8):

1869–1887  2000.

Simonyan  Karen and Zisserman  Andrew. Very deep convolutional networks for large-scale image

recognition. International Conference on Learning Representations  2015.

10

Stanley  Kenneth O and Miikkulainen  Risto. Evolving neural networks through augmenting topolo-

gies. Evolutionary computation  10(2):99–127  2002.

Villani  Cédric. Optimal transport: old and new  volume 338. Springer Science & Business Media 

2008.

Warden  Pete. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv

preprint arXiv:1804.03209  2018.

Wynne-Jones  Mike. Node splitting: A constructive algorithm for feed-forward neural networks. In

Advances in neural information processing systems  pp. 1072–1079  1992.

Xie  Sirui  Zheng  Hehui  Liu  Chunxiao  and Lin  Liang. SNAS: stochastic neural architecture search.

International Conference on Learning Representations  2018.

Zhang  Yundong  Suda  Naveen  Lai  Liangzhen  and Chandra  Vikas. Hello edge: Keyword spotting

on microcontrollers. arXiv preprint arXiv:1711.07128  2017.

Zoph  Barret and Le  Quoc V. Neural architecture search with reinforcement learning. International

Conference on Learning Representations  2017.

11

,Lemeng Wu
Dilin Wang
Qiang Liu