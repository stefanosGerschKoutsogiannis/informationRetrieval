2018,Computing Kantorovich-Wasserstein Distances on $d$-dimensional histograms using $(d+1)$-partite graphs,This paper presents a novel method to compute the exact Kantorovich-Wasserstein distance between a pair of $d$-dimensional histograms having $n$ bins each. We prove that this problem is equivalent to an uncapacitated minimum cost flow problem on a $(d+1)$-partite graph with $(d+1)n$ nodes and $dn^{\frac{d+1}{d}}$ arcs  whenever the cost is separable along the principal $d$-dimensional directions. We show numerically the benefits of our approach by computing the Kantorovich-Wasserstein distance of order 2 among two sets of instances: gray scale images and $d$-dimensional biomedical histograms. On these types of instances  our approach is competitive with state-of-the-art optimal transport algorithms.,Computing Kantorovich-Wasserstein Distances on

d-dimensional histograms using (d + 1)-partite graphs

Gennaro Auricchio  Stefano Gualandi  Marco Veneroni

Università degli Studi di Pavia  Dipartimento di Matematica “F. Casorati"

gennaro.auricchio01@universitadipavia.it 

stefano.gualandi@unipv.it  marco.veneroni@unipv.it

Federico Bassetti

Politecnico di Milano  Dipartimento di Matematica

federico.bassetti@polimi.it

Abstract

This paper presents a novel method to compute the exact Kantorovich-Wasserstein
distance between a pair of d-dimensional histograms having n bins each. We prove
that this problem is equivalent to an uncapacitated minimum cost ﬂow problem on
a (d + 1)-partite graph with (d + 1)n nodes and dn
d arcs  whenever the cost
is separable along the principal d-dimensional directions. We show numerically
the beneﬁts of our approach by computing the Kantorovich-Wasserstein distance
of order 2 among two sets of instances: gray scale images and d-dimensional bio
medical histograms. On these types of instances  our approach is competitive with
state-of-the-art optimal transport algorithms.

d+1

1

Introduction

The computation of a measure of similarity (or dissimilarity) between pairs of objects is a crucial
subproblem in several applications in Computer Vision [24  25  22]  Computational Statistic [17] 
Probability [6  8]  and Machine Learning [29  12  14  5]. In mathematical terms  in order to compute
the similarity between a pair of objects  we want to compute a distance. If the distance is equal to
zero the two objects are considered to be equal; the more the two objects are different  the greater is
their distance value. For instance  the Euclidean norm is the most used distance function to compare a
pair of points in Rd. Note that the Euclidean distance requires only O(d) operations to be computed.
When computing the distance between complex discrete objects  such as for instance a pair of discrete
measures  a pair of images  a pair of d-dimensional histograms  or a pair of clouds of points  the
Kantorovich-Wasserstein distance [31  30] has proved to be a relevant distance function [24]  which
has both nice mathematical properties and useful practical implications. Unfortunately  computing
the Kantorovich-Wasserstein distance requires the solution of an optimization problem. Even if
the optimization problem is polynomially solvable  the size of practical instances to be solved is
very large  and hence the computation of Kantorovich-Wasserstein distances implies an important
computational burden.
The optimization problem that yields the Kantorovich-Wasserstein distance can be solved with
different methods. Nowadays  the most popular methods are based on (i) the Sinkhorn’s algorithm
[11  28  3]  which solves (heuristically) a regularized version of the basic optimal transport problem 
and (ii) Linear Programming-based algorithms [13  15  20]  which exactly solve the basic optimal
transport problem by formulating and solving an equivalent uncapacitated minimum cost ﬂow
problem. For a nice overview of both computational approaches  we refer the reader to Chapters 2
and 3 in [23]  and the references therein contained.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this paper  we propose a Linear Programming-based method to speed up the computation of
Kantorovich-Wasserstein distances of order 2  which exploits the structure of the ground distance
to formulate an uncapacitated minimum cost ﬂow problem. The ﬂow problem is then solved with a
state-of-the-art implementation of the well-known Network Simplex algorithm [16].
Our approach is along the line of research initiated in [19]  where the authors proposed a very efﬁcient
method to compute Kantorovich-Wasserstein distances of order 1 (i.e.  the so–called Earth Mover
Distance)  whenever the ground distance between a pair of points is the (cid:96)1 norm. In [19]  the structure
of the (cid:96)1 ground distance and of regular d-dimensional histograms is exploited to deﬁne a very small
ﬂow network. More recently  this approach has been successfully generalized in [7] to the case of
(cid:96)∞ and (cid:96)2 norms  providing both exact and approximations algorithms  which are able to compute
distances between pairs of 512 × 512 gray scale images. The idea of speeding up the computation
of Kantorovich-Wasserstein distances by deﬁning a minimum cost ﬂow on smaller structured ﬂow
networks is also used in [22]  where a truncated distance is used as ground distance in place of a (cid:96)p
norm.
The outline of this paper is as follows. Section 2 reviews the basic notion of discrete optimal transport
and ﬁxes the notation. Section 3 contains our main contribution  that is  Theorem 1 and Corollary 2 
which permits to speed-up the computation of Kantorovich-Wasserstein distances of order 2 under
quite general assumptions. Section 4 presents numerical results of our approaches  compared with
the Sinkhorn’s algorithm as implemented in [11] and a standard Linear Programming formulation on
a complete bipartite graph [24]. Finally  Section 5 concludes the paper.

2 Discrete Optimal Transport: an Overview

Let X and Y be two discrete spaces. Given two probability vectors µ and ν deﬁned on X and Y  
respectively  and a cost c : X × Y → R+  the Kantorovich-Rubinshtein functional between µ and ν
is deﬁned as

(cid:88)

Wc(µ  ν) = inf

π∈Π(µ ν)

c(x  y)π(x  y)

(1)

(x y)∈X×Y

y∈Y π(x  y) = µ(x) and(cid:80)

probability measures π such that(cid:80)

where Π(µ  ν) is the set of all the probability measures on X × Y with marginals µ and ν  i.e. the
x∈X π(x  y) = ν(y)  for every (x  y)
in X × Y . Such probability measures are sometimes called transport plans or couplings for µ and ν.
An important special case is when X = Y and the cost function c is a distance on X. In this case
Wc is a distance on the simplex of probability vectors on X  also known as Kantorovich-Wasserstein
distance of order 1.
We remark that the Kantorovich-Wasserstein distance of order p can be deﬁned  more in general 
for arbitrary probability measures on a metric space (X  δ) by

(cid:18)

(cid:90)

(cid:19)min(1/p 1)

inf

X×X

π∈Π(µ ν)

Wp(µ  ν) :=

δp(x  y)π(dxdy)

(2)
where now Π(µ  ν) is the set of all probability measures on the Borel sets of X × X that have
marginals µ and ν  see  e.g.  [4]. The inﬁmum in (2) is attained  and any probability π which realizes
the minimum is called an optimal transport plan.
The Kantorovich-Rubinshtein transport problem in the discrete setting can be seen as a special case of
the following Linear Programming problem  where we assume now that µ and ν are generic vectors
of dimension n  with positive components 

(P ) min

c(x  y)π(x  y)

(cid:88)

(cid:88)
s.t. (cid:88)
(cid:88)

y∈Y

x∈X

y∈Y
π(x  y) ≤ µ(x)

π(x  y) ≥ ν(y)

x∈X
π(x  y) ≥ 0.

2

∀x ∈ X

∀y ∈ Y

(3)

(4)

(5)

(6)

If(cid:80)
x µ(x) = (cid:80)

(a)

(b)

(c)

Figure 1: (a) Two given 2-dimensional histograms of size N × N  with N = 3; (b) Complete bipartite
graph with N 4 arcs; (c): 3-partite graph with (d + 1)N 3 arcs.

y ν(y) we have the so-called balanced transportation problem  otherwise the
transportation problem is said to be unbalanced [18  10]. For balanced optimal transport problems 
constraints (4) and (5) must be satisﬁed with equality  and the problem reduces to the Kantorovich
transport problem (up to normalization of the vectors µ and ν).
Problem (P) is related to the so-called Earth Mover’s distance. In this case  X  Y ⊂ Rd  x and y are
the centers of two data clusters  and µ(x) and ν(y) give the number of points in the respective cluster.
Finally  c(x  y) is some measure of dissimilarity between the two clusters x and y. Once the optimal
transport π∗ is determined  the Earth Mover’s distance between µ and ν is deﬁned as (e.g.  see [24])

(cid:80)

(cid:80)

x∈X

EM D(µ  ν) =

(cid:80)
(cid:80)
y∈Y c(x  y)π∗(x  y)

y∈Y π∗(x  y)

x∈X

.

Problem (P) can be formulated as an uncapacitated minimum cost ﬂow problem on a bipartite graph
deﬁned as follows [2]. The bipartite graph has two partitions of nodes: the ﬁrst partition has a node
for each point x of X  and the second partition has a node for each point y of Y . Each node x of the
ﬁrst partition has a supply of mass equal to µ(x)  each node of the second partition has a demand of
ν(y) units of mass. The bipartite graph has an (uncapacitated) arc for each element in the Cartesian
product X × Y having cost equal to c(x  y). The minimum cost ﬂow problem deﬁned on this graph
yields the optimal transport plan π∗(x  y)  which indeed is an optimal solution of problem (3)–(6).
For instance  in case of a regular 2D dimensional histogram of size N × N  that is  having n = N 2
bins  we get a bipartite graph with 2N 2 nodes and N 4 arcs (or 2n nodes and n2 arcs). Figure 1–(a)
shows an example for a 3 × 3 histogram  and Figure 1–(b) gives the corresponding complete bipartite
graph.
In this paper  we focus on the case p = 2 in equation (2) and the ground distance function δ is the
Euclidean norm (cid:96)2  that is the Kantorovich-Wasserstein distance of order 2  which is denoted by W2.
We provide  in the next section  an equivalent formulation on a smaller (d + 1)-partite graph.

3 Formulation on (d + 1)-partite Graphs

For the sake of clarity  but without loss of generality  we present ﬁrst our construction considering 2-
dimensional histograms and the (cid:96)2 Euclidean ground distance. Then  we discuss how our construction
can be generalized to any pair of d-dimensional histograms.
Let us consider the following ﬂow problem: let µ and ν be two probability measures over a N × N
regular grid denoted by G. In the following paragraphs  we use the notation sketched in Figure 2. In
addition  we deﬁne the set U := {1  . . .   N}.

3

Figure 2: Basic notation used in Section 3: in order to
send a unit of ﬂow from point (a  j) to point (i  b)  we ei-
ther send a unit of ﬂow directly along arc ((a  j)  (i  b))
of cost c((a  j)  (i  b)) = (a − i)2 + (j − b)2  or  we
ﬁrst send a unit of ﬂow from (a  j) to (i  j)  and then
from (i  j) to (i  b)  having total cost c((a  j)  (i  j)) +
c((i  j)  (i  b)) = (a−i)2+(j−j)2+(i−i)2+(j−b)2 =
(a − i)2 + (j − b)2 = c((a  j)  (i  b)). Indeed  the cost
of the two different path is exactly the same.

R : (F1  F2) → N(cid:88)

(cid:34) N(cid:88)

N(cid:88)

(cid:35)

Since we are considering the (cid:96)2 norm as ground distance  we minimize the functional

(a − i)2f (1)

a i j +

(j − b)2f (2)

i j b

(7)

i j=1

a=1

b=1

among all Fi = {f (i)
following constraints

f (1)
a i j = µa j 

a b c}  with a  b  c ∈ {1  ...  N} real numbers (i.e.  ﬂow variables) satisfying the
N(cid:88)
N(cid:88)
(cid:88)

∀a  j ∈ U × U

∀i  b ∈ U × U

f (2)
i j b = νi b 

(8)

(9)

(cid:88)

∀i  j ∈ U × U  a ∈ U  b ∈ U.

i=1

j=1

(10)

f (1)
a i j =

f (2)
i j b 

a

b

Constraints (8) impose that the mass µa j at the point (a  j) is moved to the points (k  j)k=1 ... N .
Constraints (9) force the point (i  b) to receive from the points (i  l)l=1 ... N a total mass of νi b.
Constraints (10) require that all the mass that goes from the points (a  j)a=1 ... N to the point (i  j)
is moved to the points (i  b)b=1 ... N . We call a pair (F1  F2) satisfying the constraints (8)–(10) a
feasible ﬂow between µ and ν. We denote by F(µ  ν) the set of all feasible ﬂows between µ and ν.
Indeed  we can formulate the minimization problem deﬁned by (7)–(10) as an uncapacitated minimum
cost ﬂow problem on a tripartite graph T = (V  A). The set of nodes of T is V := V (1)∪ V (2)∪ V (3) 
where V (1)  V (2) and V (3) are the nodes corresponding to three N × N regular grids. We denote by
(i  j)(l) the node of coordinates (i  j) in the grid V (l). We deﬁne the two disjoint set of arcs between
the successive pairs of node partitions as

A(1)
A(2)

:= {((a  j)(1)  (i  j)(2)) | i  a  j ∈ U} 
:= {((i  j)(2)  (i  b)(3)) | i  b  j ∈ U} 

(11)
(12)
and  hence  the arcs of T are A := A(1) ∪ A(2). Note that in this case the graph T has 3N 2 nodes
and 2N 3 arcs. Whenever (F1  F2) is a feasible ﬂow between µ and ν  we can think of the values
f (1)
a i j as the quantity of mass that travels from (a  j) to (i  j) or  equivalently  that moves along the
arc ((a  j)  (i  j)) of the tripartite graph  while the values f (2)
i j b are the mass moving along the arc
((i  j)  (i  b)) (e.g.  see Figures 1–(c) and 2).
Now we can give an idea of the roles of the sets V (1)  V (2) and V (3): V (1) is the node set where is
drawn the initial distribution µ  while on V (3) it is drawn the ﬁnal conﬁguration of the mass ν. The
node set V (2) is an auxiliary grid that hosts an intermediate conﬁguration between µ and ν.
We are now ready to state our main contribution.
Theorem 1. For each measure π on G × G that transports µ into ν  we can ﬁnd a feasible ﬂow
(F1  F2) such that

R(F1  F2) =

((a − i)2 + (b − j)2)π(a j) (i b)).

(13)

(cid:88)

((a j) (i b))

4

((a j) (i b))

(cid:88)
(cid:88)
(cid:88)
i j b =(cid:80)
n(cid:88)

a b

j i

=

(cid:34) n(cid:88)

(a − i)2π((a j) (i b)) +

(j − b)2π((a j) (i b))

 .

(cid:88)

a b

n(cid:88)

(j − b)2f (2)

i j b

(cid:35)

.

Proof. (Sketch). We will only show how to build a feasible ﬂow starting from a transport plan  the
inverse building uses a more technical lemma (the so–called gluing lemma [4  31]) and can be found
in the Additional Material. Let π be a transport plan  if we write explicitly the ground distance
(cid:96)2((a  j)  (i  b)) we ﬁnd that

(cid:96)2((a  j)  (i  b))π((a j) (i b)) =

((a − i)2 + (j − b)2)π((a j) (i b))

(cid:88)

((a j) (i b))

a i j =(cid:80)
(cid:88)

If we set f (1)

b π((a j) (i b)) and f (2)

a π((a j) (i b)) we ﬁnd

(cid:96)2((a  j)  (i  b))π((a j) (i b)) =

(a − i)2f (1)

a i j +

((a j) (i b))

i j

a

b

In order to conclude we have to prove that those f (1)
By deﬁnition we have

a i j and f (2)

i j b satisfy the constraints (8)–(10).

f (1)
a i j =

π((a j) (i b)) = µa j 

thus proving (8); similarly  it is possible to check constraint (9). The constraint (10) also follows
easily since

(cid:88)
(cid:88)

i

(cid:88)
(cid:88)
(cid:88)
(cid:88)

b

i

(cid:88)

f (1)
a i j =

π((a j) (i b)) =

f (2)
i j b.

a

a

b

b

As a straightforward  yet fundamental  consequence we have the following result.
Corollary 1. If we set c((a  j)  (i  b)) = (a − i)2 + (j − b)2 then  for any discrete measures µ and
ν  we have that

W 2

2 (µ  ν) = min
F(µ ν)

R(F1  F2).

(14)

Indeed  we can compute the Kantorovich-Wasserstein distance of order 2 between a pair of discrete
measures µ  ν  by solving an uncapacitated minimum cost ﬂow problem on the given tripartite graph
T := (V (1) ∪ V (2) ∪ V (3)  A(1) ∪ A(2)).
We remark that our approach is very general and it can be directly extended to deal with the following
generalizations.

More general cost functions. The structure that we have exploited of the Euclidean distance (cid:96)2 is
present in any cost function c : G × G → [0 ∞] that is separable  i.e.  has the form

c(x  y) = c(1)(x1  y1) + c(2)(x2  y2) 

where both c(1) and c(2) are positive real valued functions deﬁned over G. We remark that the whole
class of costs cp(x  y) = (x1 − y1)p + (x2 − y2)p is of that kind  so we can compute any of the
Kantorovich-Wasserstein distances related to each cp.

Higher dimensional grids. Our approach can handle discrete measures in spaces of any dimension
d  that is  for instance  any d-dimensional histogram. In dimension d = 2  we get a tripartite
graph because we decomposed the transport along the two main directions. If we have a problem
in dimension d  we need a (d + 1)-plet of grids connected by arcs oriented as the d fundamental
directions  yielding a (d + 1)-partite graph. As the dimension d grows  our approach gets faster and
more memory efﬁcient than the standard formulation given on a bipartite graph.
In the Additional Material  we present a generalization of Theorem 1 to any dimension d and to
separable cost functions c(x  y).

5

Figure 3: DOTmark benchmark: Classic  Microscopy  and Shapes images.

4 Computational Results

In this section  we report the results obtained on two different set of instances. The goal of our
experiments is to show how our approach scales with the size of the histogram N and with the
dimension of the histogram d. As cost distance c(x  y)  with x  y ∈ Rd  we use the squared (cid:96)2 norm.
As problem instances  we use the gray scale images (i.e.  2-dimensional histograms) proposed by
the DOTMark benchmark [26]  and a set of d-dimensional histograms obtained by bio medical data
measured by ﬂow cytometer [9].

Implementation details. We run our experiments using the Network Simplex as implemented in
the Lemon C++ graph library1  since it provides the fastest implementation of the Network Simplex
algorithm to solve uncapacitated minimum cost ﬂow problems [16]. We did try other state-of-the-art
implementations of combinatorial algorithm for solving min cost ﬂow problems  but the Network
Simplex of the Lemon graph library was the fastest by a large margin. The tests are executed on a
gaming laptop with Windows 10 (64 bit)  equipped with an Intel i7-6700HQ CPU and 16 GB of Ram.
The code was compiled with MS Visual Studio 2017  using the ANSI standard C++17. The code
execution is single threaded. The Matlab implementation of the Sinkhorn’s algorithm [11] runs in
parallel on the CPU cores  but we do not use any GPU in our test. The C++ and Matlab code we used
for this paper is freely available at http://stegua.github.io/dpartion-nips2018.

Results for the DOTmark benchmark. The DOTmark benchmark contains 10 classes of gray
scale images related to randomly generated images  classical images  and real data from microscopy
images of mitochondria [26]. In each class there are 10 different images. Every image is given in the
data set at the following pixel resolutions: 32 × 32  64 × 64  128 × 128  256 × 256  and 512 × 512.
The images in Figure 3 are respectively the ClassicImages  Microscopy  and Shapes images (one
class for each row)  shown at highest resolution.
In our test  we ﬁrst compared ﬁve approaches to compute the Kantorovich-Wasserstein distances on
images of size 32 × 32:

1. EMD: The implementation of Transportation Simplex provided by [24]  known in the
literature as EMD code  that is an exact general method to solve optimal transport problem.
We used the implementation in the programming language C  as provided by the authors 
and compiled with all the compiler optimization ﬂags active.

2. Sinkhorn: The Matlab implementation of the Sinkhorn’s algorithm2 [11]  that is an approx-
imate approach whose performance in terms of speed and numerical accuracy depends on
a parameter λ: for smaller values of λ  the algorithm is faster  but the solution value has a
large gap with respect to the optimal value of the transportation problem; for larger values
of λ  the algorithm is more accurate (i.e.  smaller gap)  but it becomes slower. Unfortunately 
for very large value of λ the method becomes numerically unstable. The best value of λ
is very problem dependent. In our tests  we used λ = 1 and λ = 1.5. The second value 

1http://lemon.cs.elte.hu (last visited on October  26th  2018)
2http://marcocuturi.net/SI.html (last visited on October  26th  2018)

6

λ = 1.5  is the largest value we found for which the algorithm computes the distances for
all the instances considered without facing numerical issues.

3. Improved Sinkhorn: We implemented in Matlab an improved version of the Sinkhorn’s
algorithm  specialized to compute distances over regular 2-dimensional grids [28  27].
The main idea is to improve the matrix-vector operations that are the true computational
bottleneck of Sinkhorn’s algorithm  by exploiting the structure of the cost matrix. Indeed 
there is a parallelism with our approach to the method presented in [28]  since both exploits
the geometric cost structure. In [28]  the authors proposes a general method that exploits a
heat kernel to speed up the matrix-vector products. When the discrete measures are deﬁned
over a regular 2-dimensional grid  the cost matrix used by the Sinkhorn’s algorithm can be
obtained using a Kronecker product of two smaller matrices. Hence  instead of performing
a matrix-vector product using a matrix of dimension N × N  we perform two matrix-matrix
products over matrices of dimension
N  yielding a signiﬁcant runtime improvement.
In addition  since the smaller matrices are Toeplitz matrices  they can be embedded into
circulant matrices  and  as consequence  it is possible to employ a Fast Fourier Transform
approach to further speed up the computation. Unfortunately  the Fast Fourier Transform
makes the approach still more numerical unstable  and we did not used it in our ﬁnal
implementation.

N ×√

√

4. Bipartite: The bipartite formulation presented in Figure 1–(b)  which is the same as [24] 

but it is solved with the Network Simplex implemented in the Lemon Graph library [16].

5. 3-partite: The 3-partite formulation proposed in this paper  which for 2-dimensional his-
tograms is represented in 1–(c). Again  we use the Network Simplex of the Lemon Graph
Library to solve the corresponding uncapacitated minimum cost ﬂow problem.

opt

Tables 1(a) and 1(b) report the averages of our computational results over different classes of images
of the DOTMark benchmark. Each class of gray scale image contains 10 instances  and we compute
the distance between every possible pair of images within the same class: the ﬁrst image plays the
role of the source distribution µ  and the second image gives the target distribution ν. Considering
all pairs within a class  it gives 45 instances for each class. We report the means and the standard
deviations (between brackets) of the runtime  measured in seconds. Table 1(a) shows in the second
column the runtime for EMD [24]. The third and fourth columns gives the runtime and the optimality
gap for the Sinkhorn’s algorithm with λ = 1; the 6-th and 7-th columns for λ = 1.5. The percentage
· 100  where U B is the upper bound computed by the Sinkhorn’s
gap is computed as Gap = U B−opt
algorithm  and opt is the optimal value computed by EMD. The last two columns report the runtime
for the bipartite and 3-partite approaches presented in this paper.
Table 1(b) compares our 3-partite formulation with the Improved Sinkhorn’s algorithm [28  27] 
reporting the same statistics of the previous table. In this case  we run the Improved Sinkhorn using
three values of the parameter λ  that are  1.0  1.25  and 1.5. While the Improved Sinkhorn is indeed
much faster that the general algorithm as presented in [11]  it does suffer of the same numerical
stability issues  and  it can yield very poor percentage gap to the optimal solution  as it happens for
the GRFrough and the WhiteNoise classes  where the optimality gaps are on average 31.0% and
39.2%  respectively.
As shown in Tables 1(a) and 1(b)  the 3-partite approach is clearly faster than any of the alternatives
considered here  despite being an exact method. In addition  we remark that  even on the bipartite
formulation  the Network Simplex implementation of the Lemon Graph library is order of magnitude
faster than EMD  and hence it should be the best choice in this particular type of instances. We
remark that it might be unfair to compare an algorithm implemented in C++ with an algorithm
implemented in Matlab  but still  the true comparison is on the solution quality more than on the
runtime. Moreover  when implemented on modern GPU that can fully exploit parallel matrix-vector
operations  the Sinkhorn’s algorithm can run much faster  but they cannot improve the optimality gap.
In order to evaluate how our approach scale with the size of the images  we run additional tests using
images of size 64 × 64 and 128 × 128. Table 2 reports the results for the bipartite and 3-partite
approaches for increasing size of the 2-dimensional histograms. The table report for each of the
two approaches  the number of vertices |V | and of arcs |A|  and the means and standard deviations
of the runtime. As before  each row gives the averages over 45 instances. Table 2 shows that the
3-partite approach is clearly better (i) in terms of memory  since the 3-partite graph has a fraction of
the number of arcs  and (ii) of runtime  since it is at least an order of magnitude faster in computation

7

EMD [24]

Sinkhorn [11]

Bipartite

3-partite

λ = 1

λ = 1.5

Image Class
Classic
Microscopy
Shapes

Runtime
24.0 (3.3)
35.0 (3.3)
25.2 (5.3)

Runtime
6.0 (0.5)
3.5 (1.0)
1.6 (1.1)

Gap Runtime
17.3% 8.9 (0.7)
2.4% 5.3 (1.4)
5.6% 2.5 (1.6)

Gap
Runtime
9.1% 0.54 (0.05)
1.2% 0.55 (0.03)
3.0% 0.50 (0.07)

Runtime
0.07 (0.01)
0.08 (0.01)
0.05 (0.01)

(a)

Improved Sinkhorn [28  27]

3-partite

λ = 1

λ = 1.25

λ = 1.5

Image Class
CauchyDensity
Classic
GRFmoderate
GRFrough
GRFsmooth
LogGRF
LogitGRF
Microscopy
Shapes
WhiteNoise

Runtime
0.22 (0.15)
0.20 (0.01)
0.19 (0.01)
0.19 (0.01)
0.20 (0.02)
0.22 (0.05)
0.22 (0.02)
0.18 (0.03)
0.11 (0.04)
0.18 (0.01)

Gap
Runtime
2.8% 0.33 (0.23)
17.3% 0.31 (0.02)
12.6% 0.29 (0.02)
58.7% 0.29 (0.01)
4.3% 0.30 (0.04)
1.3% 0.32 (0.08)
4.7% 0.33 (0.03)
2.4% 0.27 (0.04)
5.6% 0.16 (0.06)
76.3% 0.28 (0.01)

(b)

Gap
Runtime
2.0% 0.41 (0.28)
12.4% 0.39 (0.03)
9.0% 0.37 (0.03)
42.1% 0.38 (0.02)
3.1% 0.38 (0.04)
0.9% 0.40 (0.13)
3.3% 0.42 (0.04)
1.7% 0.34 (0.05)
4.0% 0.20 (0.07)
53.8% 0.37 (0.02)

Gap
Runtime
1.5% 0.07 (0.01)
9.1% 0.07 (0.01)
6.6% 0.07 (0.01)
31.0% 0.05 (0.01)
2.2% 0.08 (0.01)
0.7% 0.08 (0.01)
2.5% 0.07 (0.02)
1.2% 0.08 (0.02)
3.0% 0.05 (0.01)
39.2% 0.04 (0.00)

Table 1: Comparison of different approaches on 32× 32 images. The runtime (in seconds) is given as
· 100  where U B is the
“Mean (StdDev)”. The gap to the optimal value opt is computed as U B−opt
upper bound computed by Sinkhorn’s algorithm. Each row reports the averages over 45 instances.

opt

Size
64 × 64

Image Class
Classic
Microscopy
Shape
128 × 128 Classic

Microscopy
Shape

|V |
8 193

Bipartite
|A|
16 777 216

32 768 268 435 456

Runtime
16.3 (3.6)
11.7 (1.4)
13.0 (3.9)
1 368 (545)
959 (181)
983 (230)

|V |
12 288

3-partite
|A|
524 288

49 152

4 194 304

Runtime
2.2 (0.2)
1.0 (0.2)
1.1 (0.3)
36.2 (5.4)
23.0 (4.8)
17.8 (5.2)

Table 2: Comparison of the bipartite and the 3-partite approaches on 2-dimensional histograms.

time. Indeed  the 3-partite formulation is better essentially because it exploits the structure of the
ground distance c(x  y) used  that is  the squared (cid:96)2 norm.

Flow Cytometry biomedical data. Flow cytometry is a laser-based biophysical technology used
to study human health disorders. Flow cytometry experiments produce huge set of data  which
are very hard to analyze with standard statistics methods and algorithms [9]. Currently  such data
is used to study the correlations of only two factors (e.g.  biomarkers) at the time  by visualizing
2-dimensional histograms and by measuring the (dis-)similarity between pairs of histograms [21].
However  during a ﬂow cytometry experiment up to hundreds of factors (biomarkers) are measured
and stored in digital format. Hence  we can use such data to build d-dimensional histograms that
consider up to d biomarkers at the time  and then comparing the similarity among different individuals
by measuring the distance between the corresponding histograms. In this work  we used the ﬂow
cytometry data related to Acute Myeloid Leukemia (AML)  available at http://flowrepository.

8

N
16

32

d
2
3
4
2
3

n
256
4 096
65 536
1 024
32 768

|V |
512
8 192

2 048

Bipartite Graph

|A|
65 536
16 777 216
out-of-memory
1 048 756
out-of-memory

Runtime
0.024 (0.01)
38.2 (14.0)

0.71 (0.14)

|V |
768
16 384
327 680
3072
131 072

|A|
8 192
196 608
4 194 304
65 536
3 145 728

(d + 1)-partite Graph

Runtime
0.003 (0.00)
0.12 (0.02)
4.8 (0.84)
0.04 (0.01)
5.23 (0.69)

Table 3: Comparison between the bipartite and the (d + 1)-partite approaches on Flow Cytometry
data.

org/id/FR-FCM-ZZYA  which contains cytometry data for 359 patients  classiﬁed as “normal” or
affected by AML. This dataset has been used by the bioinformatics community to run clustering
algorithms  which should predict whether a new patient is affected by AML [1].
Table 3 reports the results of computing the distance between pairs of d-dimensional histograms  with
d ranging in the set {2  3  4}  obtained using the AML biomedical data. Again  the ﬁrst d-dimensional
histogram plays the role of the source distribution µ  while the second histogram gives the target
distribution ν. For simplicity  we considered regular histograms of size n = N d (i.e.  n is the total
number of bins)  using N = 16 and N = 32. Table 3 compares the results obtained by the bipartite
and (d + 1)-partite approach  in terms of graph size and runtime. Again  the (d + 1)-partite approach 
by exploiting the structure of the ground distance  outperforms the standard formulation of the optimal
transport problem. We remark that for N = 32 and d = 3  we pass for going out-of-memory with the
bipartite formulation  to compute the distance in around 5 seconds with the 4-partite formulation.

5 Conclusions

In this paper  we have presented a new network ﬂow formulation on (d + 1)-partite graphs that can
speed up the optimal solution of transportation problems whenever the ground cost function c(x  y)
(see objective function (3)) has a separable structure along the main d directions  such as  for instance 
the squared (cid:96)2 norm used in the computation of the Kantorovich-Wasserstein distance of order 2.
Our computational results on two different datasets show how our approach scales with the size of the
histograms N and with the dimension of the histograms d. Indeed  by exploiting the cost structure 
the proposed approach is better in term of memory consumption  since it has only dn
d arcs instead
of n2. In addition  it is much faster since it has to solve an uncapacitated minimum cost ﬂow problem
on a much smaller ﬂow network.

d+1

Acknowledgments

We are deeply indebted to Giuseppe Savaré  for introducing us to optimal transport and for many
stimulating discussions and suggestions. We thanks Mattia Tani for a useful discussion concerning
the Improved Sinkhorn’s algorithm.
This research was partially supported by the Italian Ministry of Education  University and Research
(MIUR): Dipartimenti di Eccellenza Program (2018–2022) - Dept. of Mathematics “F. Casorati” 
University of Pavia.
The last author’s research is partially supported by “PRIN 2015. 2015SNS29B-002. Modern Bayesian
nonparametric methods”.

References
[1] Nima Aghaeepour  Greg Finak  Holger Hoos  Tim R Mosmann  Ryan Brinkman  Raphael Gottardo 
Richard H Scheuermann  FlowCAP Consortium  DREAM Consortium  et al. Critical assessment of
automated ﬂow cytometry data analysis techniques. Nature methods  10(3):228  2013.

9

[2] Ravindra K Ahuja  Thomas L Magnanti  and James B Orlin. Network ﬂows: Theory  Algorithms  and
Applications. Cambridge  Mass.: Alfred P. Sloan School of Management  Massachusetts Institute of
Technology  1988.

[3] Jason Altschuler  Jonathan Weed  and Philippe Rigollet. Near-linear time approximation algorithms for
optimal transport via Sinkhoirn iteration. In Advances in Neural Information Processing Systems  pages
1961–1971  2017.

[4] Luigi Ambrosio  Nicola Gigli  and Giuseppe Savaré. Gradient ﬂows: in metric spaces and in the space of

probability measures. Springer Science & Business Media  2008.

[5] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein GAN. arXiv preprint arXiv:1701.07875 

2017.

[6] Federico Bassetti  Antonella Bodini  and Eugenio Regazzini. On minimum Kantorovich distance estimators.

Statistics & probability letters  76(12):1298–1302  2006.

[7] Federico Bassetti  Stefano Gualandi  and Marco Veneroni. On the computation of Kantorovich-Wasserstein
distances between 2D-histograms by uncapacitated minimum cost ﬂows. arXiv preprint arXiv:1804.00445 
2018.

[8] Federico Bassetti and Eugenio Regazzini. Asymptotic properties and robustness of minimum dissimilarity
estimators of location-scale parameters. Theory of Probability & Its Applications  50(2):171–186  2006.

[9] Tytus Bernas  Elikplimi K Asem  J Paul Robinson  and Bartek Rajwa. Quadratic form: a robust metric for

quantitative comparison of ﬂow cytometric histograms. Cytometry Part A  73(8):715–726  2008.

[10] Lenaic Chizat  Gabriel Peyré  Bernhard Schmitzer  and François-Xavier Vialard. Scaling algorithms for

unbalanced transport problems. arXiv preprint arXiv:1607.05816  2016.

[11] Marco Cuturi. Sinkhoirn distances: Lightspeed computation of optimal transport. In Advances in Neural

Information Processing Systems  pages 2292–2300  2013.

[12] Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters.

Conference on Machine Learning  pages 685–693  2014.

In International

[13] Merrill M Flood. On the Hitchcock distribution problem. Paciﬁc Journal of Mathematics  3(2):369–386 

1953.

[14] Charlie Frogner  Chiyuan Zhang  Hossein Mobahi  Mauricio Araya  and Tomaso A Poggio. Learning with

a Wasserstein loss. In Advances in Neural Information Processing Systems  pages 2053–2061  2015.

[15] Andrew V Goldberg  Éva Tardos  and Robert Tarjan. Network ﬂow algorithm. Technical report  Cornell

University Operations Research and Industrial Engineering  1989.

[16] Péter Kovács. Minimum-cost ﬂow algorithms: an experimental evaluation. Optimization Methods and

Software  30(1):94–127  2015.

[17] Elizaveta Levina and Peter Bickel. The Earth Mover ’s distance is the Mallows distance: Some insights
from statistics. In Computer Vision  2001. ICCV 2001. Proceedings. Eighth IEEE International Conference
on  volume 2  pages 251–256. IEEE  2001.

[18] Matthias Liero  Alexander Mielke  and Giuseppe Savaré. Optimal entropy-transport problems and a new
Hellinger–Kantorovich distance between positive measures. Inventiones mathematicae  211(3):969–1117 
2018.

[19] Haibin Ling and Kazunori Okada. An efﬁcient Earth Mover ’s distance algorithm for robust histogram

comparison. IEEE Transactions on Pattern Analysis and Machine Intelligence  29(5):840–853  2007.

[20] James B Orlin. A faster strongly polynomial minimum cost ﬂow algorithm. Operations research 

41(2):338–350  1993.

[21] Darya Y Orlova  Noah Zimmerman  Stephen Meehan  Connor Meehan  Jeffrey Waters  Eliver EB Ghosn 
Alexander Filatenkov  Gleb A Kolyagin  Yael Gernez  Shanel Tsuda  et al. Earth Mover ’s distance (EMD):
a true metric for comparing biomarker expression levels in cell populations. PloS one  11(3):1–14  2016.

[22] Oﬁr Pele and Michael Werman. Fast and robust Earth Mover ’s distances. In Computer vision  2009 IEEE

12th international conference on  pages 460–467. IEEE  2009.

10

[23] Gabriel Peyré  Marco Cuturi  et al. Computational optimal transport. Technical Report 1803.00567  ArXiv 

2018.

[24] Yossi Rubner  Carlo Tomasi  and Leonidas J Guibas. A metric for distributions with applications to image

databases. In Computer Vision  1998. Sixth International Conference on  pages 59–66. IEEE  1998.

[25] Yossi Rubner  Carlo Tomasi  and Leonidas J Guibas. The Earth Mover ’s distance as a metric for image

retrieval. International Journal of Computer Vision  40(2):99–121  2000.

[26] Jörn Schrieber  Dominic Schuhmacher  and Carsten Gottschlich. Dotmark–a benchmark for discrete

optimal transport. IEEE Access  5:271–282  2017.

[27] Justin Solomon. Optimal transport on discrete domains. Technical Report 1801.07745  arXiv  2018.

[28] Justin Solomon  Fernando De Goes  Gabriel Peyré  Marco Cuturi  Adrian Butscher  Andy Nguyen  Tao Du 
and Leonidas Guibas. Convolutional Wasserstein distances: Efﬁcient optimal transportation on geometric
domains. ACM Transactions on Graphics (TOG)  34(4):66  2015.

[29] Justin Solomon  Raif Rustamov  Leonidas Guibas  and Adrian Butscher. Wasserstein propagation for

semi-supervised learning. In International Conference on Machine Learning  pages 306–314  2014.

[30] Anatoly Moiseevich Vershik. Long history of the Monge-Kantorovich transportation problem. The

Mathematical Intelligencer  35(4):1–9  2013.

[31] Cédric Villani. Optimal transport: old and new  volume 338. Springer Science & Business Media  2008.

11

,Chao Qin
Diego Klabjan
Daniel Russo
Gennaro Auricchio
Federico Bassetti
Stefano Gualandi
Marco Veneroni