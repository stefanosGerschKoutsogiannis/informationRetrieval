2009,A Fast  Consistent Kernel Two-Sample Test,A kernel embedding of probability distributions into reproducing kernel Hilbert spaces (RKHS) has recently been proposed  which  allows the comparison of two probability measures P and Q based on the distance between their respective embeddings: for a sufficiently rich RKHS  this distance is zero if and only if P and Q coincide. In using this distance as a statistic for a  test of whether two samples are from different distributions  a major difficulty arises in computing the significance threshold  since the empirical statistic has as its null distribution (where P=Q)  an infinite weighted sum of $\chi^2$ random variables. The main result of the present work is a  novel  consistent estimate of this null distribution  computed from  the eigenspectrum of the Gram matrix on the aggregate sample from P and Q. This estimate may be computed faster than a previous consistent estimate based on the bootstrap.  Another prior approach was to compute the null distribution based on fitting a parametric family with the low order moments of the test statistic: unlike the present work  this heuristic has no guarantee of being accurate or consistent. We verify the performance of our null distribution estimate on both   an artificial example and on high dimensional multivariate data.,A Fast  Consistent Kernel Two-Sample Test

Arthur Gretton

Carnegie Mellon University

MPI for Biological Cybernetics

arthur.gretton@gmail.com

Kenji Fukumizu

Inst. of Statistical Mathematics

Tokyo Japan

fukumizu@ism.ac.jp

Zaid Harchaoui

Carnegie Mellon University

Pittsburgh  PA  USA

zaid.harchaoui@gmail.com

Bharath K. Sriperumbudur

Dept. of ECE  UCSD
La Jolla  CA 92037
bharathsv@ucsd.edu

Abstract

A kernel embedding of probability distributions into reproducing kernel Hilbert
spaces (RKHS) has recently been proposed  which allows the comparison of two
probability measures P and Q based on the distance between their respective em-
beddings: for a sufﬁciently rich RKHS  this distance is zero if and only if P and
Q coincide. In using this distance as a statistic for a test of whether two samples
are from different distributions  a major difﬁculty arises in computing the signif-
icance threshold  since the empirical statistic has as its null distribution (where
P = Q) an inﬁnite weighted sum of χ2 random variables. Prior ﬁnite sample
approximations to the null distribution include using bootstrap resampling  which
yields a consistent estimate but is computationally costly; and ﬁtting a parametric
model with the low order moments of the test statistic  which can work well in
practice but has no consistency or accuracy guarantees. The main result of the
present work is a novel estimate of the null distribution  computed from the eigen-
spectrum of the Gram matrix on the aggregate sample from P and Q  and having
lower computational cost than the bootstrap. A proof of consistency of this esti-
mate is provided. The performance of the null distribution estimate is compared
with the bootstrap and parametric approaches on an artiﬁcial example  high di-
mensional multivariate data  and text.

1 Introduction
Learning algorithms based on kernel methods have enjoyed considerable success in a wide range of
supervised learning tasks  such as regression and classiﬁcation [25]. One reason for the popularity of
these approaches is that they solve difﬁcult non-parametric problems by representing the data points
in high dimensional spaces of features  speciﬁcally reproducing kernel Hilbert spaces (RKHSs)  in
which linear algorithms can be brought to bear. While classical kernel methods have addressed the
mapping of individual points to feature space  more recent developments [14  29  28] have focused
on the embedding of probability distributions in RKHSs. When the embedding is injective  the
RKHS is said to be characteristic [11  29  12]  and the distance between feature mappings constitutes
a metric on distributions. This distance is known as the maximum mean discrepancy (MMD).

One well-deﬁned application of the MMD is in testing whether two samples are drawn from two
different distributions (i.e.  a two-sample or homogeneity test). For instance  we might wish to ﬁnd
whether DNA microarrays obtained on the same tissue type by different labs are distributed iden-
tically  or whether differences in lab procedure are such that the data have dissimilar distributions
(and cannot be aggregated) [8]. Other applications include schema matching in databases  where
tests of distribution similarity can be used to determine which ﬁelds correspond [14]  and speaker

1

veriﬁcation  where MMD can be used to identify whether a speech sample corresponds to a person
for whom previously recorded speech is available [18].

A major challenge when using the MMD in two-sample testing is in obtaining a signiﬁcance thresh-
old  which the MMD should exceed with small probability when the null hypothesis (that the sam-
ples share the same generating distribution) is satisﬁed. Following [14  Section 4]  we deﬁne this
threshold as an upper quantile of the asymptotic distribution of the MMD under the null hypothesis.
Unfortunately this null distribution takes the form of an inﬁnite weighted sum of χ2 random vari-
ables. Thus  obtaining a consistent ﬁnite sample estimate of this threshold — that is  an estimate
that converges to the true threshold in the inﬁnite sample limit — is a signiﬁcant challenge. Three
approaches have previously been applied: distribution-free large deviation bounds [14  Section 3] 
which are generally too loose for practical settings; ﬁtting to the Pearson family of densities [14] 
a simple heuristic that performs well in practice  but has no guarantees of accuracy or consistency;
and a bootstrap approach  which is guaranteed to be consistent  but has a high computational cost.

The main contribution of the present study is a consistent ﬁnite sample estimate of the null distribu-
tion (not based on bootstrap)  and a proof that this estimate converges to the true null distribution in
the inﬁnite sample limit. Brieﬂy  the inﬁnite sequence of weights that deﬁnes the null distribution is
identical to the sequence of normalized eigenvalues obtained in kernel PCA [26  27  7]. Thus  we
show that the null distribution deﬁned using ﬁnite sample estimates of these eigenvalues converges
to the population distribution  using only convergence results on certain statistics of the eigenvalues.
In experiments  our new estimate of the test threshold has a smaller computational cost than that
of resampling-based approaches such as the bootstrap  while providing performance as good as the
alternatives for larger sample sizes.

We begin our presentation in Section 2 by describing how probability distributions may be embedded
in an RKHS. We also review the maximum mean discrepancy as our chosen distance measure on
these embeddings  and recall the asymptotic behaviour of its ﬁnite sample estimate. In Section 3 
we present both moment-based approximations to the null distribution of the MMD (which have
no consistency guarantees); and our novel  consistent estimate of the null distribution  based on the
spectrum of the kernel matrix over the aggregate sample. Our experiments in Section 4 compare the
different approaches on an artiﬁcial dataset  and on high-dimensional microarray and neuroscience
data. We also demonstrate the generality of a kernel-based approach by testing whether two samples
of text are on the same topic  or on different topics.

2 Background
In testing whether two samples are generated from the same distribution  we require both a measure
of distance between probabilities  and a notion of whether this distance is statistically signiﬁcant. For
the former  we deﬁne an embedding of probability distributions in a reproducing kernel Hilbert space
(RKHS)  such that the distance between these embeddings is our test statistic. For the latter  we give
an expression for the asymptotic distribution of this distance measure  from which a signiﬁcance
threshold may be obtained.

Let F be an RKHS on the separable metric space X  with a continuous feature mapping φ(x) ∈ F
for each x ∈ X. The inner product between feature mappings is given by the positive deﬁnite kernel
function k(x  x′) := hφ(x)  φ(x′)iF. We assume in the following that the kernel k is bounded. Let
P be the set of Borel probability measures on X. Following [4  10  14]  we deﬁne the mapping to F
of P ∈ P as the expectation of φ(x) with respect to P   or

µP : P → F

P 7→ ZX

φ(x)dP.

The maximum mean discrepancy (MMD) [14  Lemma 7] is deﬁned as the distance between two
such mappings 

MMD(P  Q)

:= kµP − µQkF
= (Ex x′(k(x  x′)) + Ey y ′k(y  y′) − 2Ex yk(x  y))1/2  

where x and x′ are independent random variables drawn according to P   y and y′ are independent
and drawn according to Q  and x is independent of y. This quantity is a pseudo-metric on distribu-
tions: that is  it satisﬁes all the qualities of a metric besides MMD(P  Q) = 0 iff P = Q. For MMD

2

to be a metric  we require that the kernel be characteristic [11  29  12].1 This criterion is satisﬁed for
many common kernels  such as the Gaussian kernel (both on compact domains and on Rd) and the
B2l+1 spline kernel on Rd.
samples
We now consider two possible empirical estimates of the MMD  based on i.i.d.
(x1  . . .   xm) from P and (y1  . . .   ym) from Q (we assume an equal number of samples for sim-
plicity). An unbiased estimate of MMD is the one-sample U-statistic

MMD2

u :=

1

m(m − 1)

mXi6=j

h(zi  zj) 

(1)

where zi := (xi  yi) and h(zi  zj) := k(xi  xj)+k(yi  yj)−k(xi  yj)−k(xj  yi). We also deﬁne the
biased estimate MMD2
b by replacing the U-statistic in (1) with a V-statistic (the sum then includes
terms i = j).
Our goal is to determine whether P and Q differ  based on m samples from each. To this end  we
require a measure of whether MMD2
u differs signiﬁcantly from zero; or  if the biased statistic MMD2
b
is used  whether this value is signiﬁcantly greater than its expectation when P = Q. In other words
we conduct a hypothesis test with null hypothesis H0 deﬁned as P = Q  and alternative hypothesis
H1 as P 6= Q. We must therefore specify a threshold that the empirical MMD will exceed with
small probability  when P = Q. For an asymptotic false alarm probability (Type I error) of α  an
appropriate threshold is the 1 − α quantile of the asymptotic distribution of the empirical MMD
assuming P = Q. According to [14  Theorem 8]  this distribution takes the form

mMMD2

u →
D

λl(z2

l − 2) 

(2)

∞Xl=1

ZX

denotes convergence in distribution  zl ∼ N(0  2) i.i.d.  λi are the solutions to the eigen-

where →
D
value equation

˜k(xi  xj)ψl(xi)dP := λlψl(xj ) 

(3)

and ˜k(xi  xj) := k(xi  xj ) − Exk(xi  x) − Exk(x  xi) + Ex x′k(x  x′). Consistency in power of
the resulting hypothesis test (that is  the convergence of its Type II error to zero for increasing m) is
shown in [14].

The eigenvalue problem (3) has been studied extensively in the context of kernel PCA [26  27  7]:
this connection will be used in obtaining a ﬁnite sample estimate of the null distribution in (2) 
and we summarize certain important results. Following [3  10]  we deﬁne the covariance operator
C : F → F as

hf  Cf iF := var(f (x))

(4)
The eigenvalues λl of C are the solutions to the eigenvalue problem in (3) [19  Proposition 2].
Following e.g. [27  p.2511]  empirical estimates of these eigenvalues are

= Exf 2(x) − [Exf (x)]2 .

ˆλl =

1
m

νl

(5)

where νl are the eigenvalues of the centered Gram matrix

Ki j := k(xi  xj)  and H = I − 1
mMMD2

b  we observe that these differ by a quantity with expectation tr(C) =P∞

m 11⊤ is a centering matrix. Finally  by subtracting mMMD2

u from

l=1 λl  and thus

eK := HKH 
∞Xl=1

b →
D

mMMD2

λlz2
l .

1Other interpretations of the MMD are also possible  for particular kernel choices. The most closely related
is the L2 distance between probability density estimates [1]  although this requires the kernel bandwidth to
decrease with increasing sample size. See [1  14] for more detail. Yet another interpretation is given in [32].

3

3 Theory

In the present section  we describe three approaches for approximating the null distribution of MMD.
We ﬁrst present the Pearson curve and Gamma-based approximations  which consist of parametrized
families of distributions that we ﬁt by matching the low order moments of the empirical MMD. Such
approximations can be accurate in practice  although they remain heuristics with no consistency
guarantees. Second  we describe a null distribution estimate based on substituting the empirical
estimates (5) of the eigenvalues into (2). We prove that this estimate converges to its population
counterpart in the large sample limit.

3.1 Moment-based null distribution estimates

The Pearson curves and the Gamma approximation are both based on the low order moments of the
empirical MMD. The second and third moments for MMD are obtained in [14]:

E(cid:16)(cid:2)MMD2
E(cid:16)(cid:2)MMD2

u(cid:3)2(cid:17) =
u(cid:3)3(cid:17) =

Ez z′(cid:2)h2(z  z′)(cid:3) and

2

m(m − 1)
8(m − 2)

m2(m − 1)2

Ez z′ [h(z  z′)Ez′′ (h(z  z′′)h(z′  z′′))] + O(m−4).

(6)

(7)

Pearson curves take as arguments the variance  skewness and kurtosis As in [14]  we replace the
+ 1. An alternative 
more computationally efﬁcient approach is to use a two-parameter Gamma approximation [20  p.
343  p. 359] 

kurtosis with a lower bound due to [31]  kurt(cid:0)MMD2

u(cid:1)(cid:1)2
u(cid:1) ≥(cid:0)skew(cid:0)MMD2

mMMDb(Z) ∼

xα−1e−x/β

βαΓ(α)

where α =

(E(MMDb(Z)))2
var(MMDb(Z))

 

β =

mvar(MMDb(Z))

E(MMDb(Z))

 

(8)

and we use the biased statistic MMD2
b. Although the Gamma approximation is necessarily less
accurate than the Pearson approach  it has a substantially lower computational cost (O(m2) for
the Gamma approximation  as opposed to O(m3) for Pearson). Moreover  we will observe in our
experiments that it performs remarkably well  at a substantial cost saving over the Pearson curves.

3.2 Null distribution estimates using Gram matrix spectrum

In [14  Theorem 8]  it was established that for large sample sizes  the null distribution of MMD
approaches an inﬁnite weighted sum of independent χ2
1 random variables  the weights being the
population eigenvalues of the covariance operator C. Hence  an efﬁcient and theoretically grounded
way to calibrate the test is to compute the quantiles by replacing the population eigenvalues of C
with their empirical counterparts  as computed from the Gram matrix (see also [18]  where a similar
strategy is proposed for the KFDA test with ﬁxed regularization).

The following result shows that this empirical estimate of the null distribution converges in distribu-
tion to its population counterpart. In other words  a test using the MMD statistic  with the threshold
computed from quantiles of the null distribution estimate  is asymptotically consistent in level.

Theorem 1 Let z1  . . .   zl  . . . be an inﬁnite sequence of i.i.d. random variables  with z1 ∼ N(0  2).

AssumeP∞

l=1 λ1/2

l < ∞. Then  as m → ∞

Furthermore  as m → ∞

∞Xl=1

ˆλl(z2

l − 2) →
D

∞Xl=1
u > t(cid:1) − P ∞Xl=1

4

sup

t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
P(cid:0)mMMD2

λl(z2

l − 2) .

ˆλl(z2

l − 2) > t!(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

→ 0 .

Proof (sketch) We begin with a proof of conditions under which the sumP∞

l − 2) is ﬁnite
w.p. 1. According to [16  Exercise 30  p. 358]  we may use Kolmogorov’s inequality to determine
that this sum converges a.s. if

l=1 λl(z2

Ez[λ2

l (z2

l − 2)2] < ∞ 

∞Xl=1

l=1 λ1/2

from which it follows that the covariance operator must be Hilbert-Schmidt: this is guaranteed by
l < ∞ (see also [7]). We now proceed to the convergence result. Let C

the assumptionP∞
and bC be the covariance operator and its empirical estimator. Let λl andbλl (l = 1  2  . . .) be the
eigenvalues of C and bC  respectively  in descending order. We want to prove

(9)

l → 0

∞Xp=1

(bλl − λl)Z 2

in probability as n → ∞  where Zp ∼ N (0  2) are i.i.d. random variables. The constant −2 in
Z 2

p − 2 can be neglected as Tr[bC] → Tr[C]  where the proof is given in the online supplement. Thus
(cid:12)(cid:12)(cid:12)Xl

(bλl − λl)Z 2

l − λ1/2

l(cid:12)(cid:12)(cid:12)

l Z 2

)λ1/2

)Z 2

l

l − λ1/2

l

l(cid:12)(cid:12)(cid:12) +(cid:12)(cid:12)(cid:12)Xl (cid:0)bλ1/2
(cid:12)(cid:12)2o1/2
(cid:12)(cid:12)2o1/2

l − λ1/2

l

l

l

l − λ1/2

l(cid:12)(cid:12)(cid:12) ≤(cid:12)(cid:12)(cid:12)Xl bλ1/2
(cid:0)bλ1/2
≤nXl bλlZ 4
lo1/2nXl (cid:12)(cid:12)bλ1/2
lo1/2nXl (cid:12)(cid:12)bλ1/2
+nXl
l and PlbλlZ 4
i =Xi
EXi

ˆλiZ 4

λlZ 4

E[ˆλi]E[Z 4

i ] = κE[tr( ˆC)] 

We now establish Pl λlZ 4

inequality. To prove the latter  we use that since ˆλi and Zi are independent 

l are of Op(1). The former follows from Chebyshev’s

(Cauchy-Schwarz).

(10)

where κ = E[Z 4]. Since E[tr( ˆC)] is bounded when the kernel has bounded expectation  we again
have the desired result by Chebyshev’s inequality. The proof is complete if we show

(11)

(12)

(13)

From

we have

l − λ1/2

l

(cid:12)(cid:12)(cid:12)bλ1/2

It is known as an extension of the Hoffmann-Wielandt inequality that

l

l

l

2

)2 = op(1).

l − λ1/2

l − λ1/2

l + λ1/2

Xl (cid:0)bλ1/2
(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)bλ1/2
(cid:12)(cid:12)(cid:12) (bλ1/2
(cid:12)(cid:12)(cid:12)bλ1/2
Xl
≤Xl
|bλl − λl|.
(cid:12)(cid:12)(cid:12)bλl − λl(cid:12)(cid:12)(cid:12) ≤ kbC − Ck1 
Xl

l − λ1/2

(cid:12)(cid:12)(cid:12)

2

l

) =(cid:12)(cid:12)(cid:12)bλl − λl(cid:12)(cid:12)(cid:12)  

where k · k1 is the trace norm (see [23]  also shown in [5  p. 490]). Using [18  Prop. 12]  which

second statement follows immediately from the Polya theorem [21]  as in [18].

gives kbC − Ck1 → 0 in probability  the proof of the ﬁrst statement is completed. The proof of the

3.3 Discussion

We now have several ways to calibrate the MMD test statistic  ranked in order of increasing com-
putational cost: 1) the Gamma approximation  2) the “empirical null distribution”: that is  the null
distribution estimate using the empirical Gram matrix spectrum  and 3) the Pearson curves  and

5

the resampling procedures (subsampling or bootstrap with replacement). We include the ﬁnal two
approaches in the same cost category since even though the Pearson approach scales worse with
m than the bootstrap (O(m3) vs O(m2))  the bootstrap has a higher cost for sample sizes less than
about 103 due the requirement to repeatedly re-compute the test statistic. We also note that our result
of large-sample consistency in level holds under a restrictive condition on the decay of the spectrum
of the covariance operator  whereas the Gamma approximation calculations are straightforward and
remain possible for any spectrum decay behaviour. The Gamma approximation remains a heuristic 
however  and we give an example of a distribution and kernel for which it performs less accurately
than the spectrum-based estimate in the upper tail  which is of most interest for testing purposes.

4 Experiments

In this section  we compare the four approaches to obtaining the null distribution  both in terms of
the approximation error computed with respect to simulations from the true null  and when used
in homogeneity testing. Our approaches are denoted Gamma (the two-parameter Gamma approx-
imation)  Pears (the Pearson curves based on the ﬁrst three moments  using a lower bound for the
kurtosis)  Spec (our new approximation to the null distribution  using the Gram matrix eigenspec-
trum)  and Boot (the bootstrap approach).
Artiﬁcial data: We ﬁrst provide an example of a distribution P for which the heuristics Gamma
and Pears have difﬁculty in approximating the null distribution  whereas Spec converges. We chose
P to be a mixture of normals P = 0.5 ∗ N(−1  0.44) + 0.5 ∗ N(+1  0.44)  and k as a Gaussian
kernel with bandwidth ranging over σ = 2−4  2−3  2−2  2−1  20  21  22. The sample sizes were set
to m = 5000  the total sample size hence being 10  000  and the results were averaged over 50  000
replications. The eigenvalues of the Gram matrix were estimated in this experiment using [13] 
which is slower but more accurate than standard Matlab routines. The true quantiles of the MMD
null distribution  referred to as the oracle quantiles  were estimated by Monte Carlo simulations with
50  000 runs. We report the empirical performance of Spec compared to the oracle in terms of ∆q =
maxtr :q<r<1 |P(mM M D2
u >

u > tr)|  where tq is such that P(mM M D2

u > tr) −bPm(mM M D2

tq) = q for q = 0.6  0.7  0.8  0.9  and bPm is the Spec null distribution estimate obtained with m

samples from each of P and Q. We also use this performance measure for the Gamma and Pears
approximations. This focuses the performance comparison on the quantiles corresponding to the
upper tail of the null distribution  while still addressing uniform accuracy over a range of thresholds
so as to ensure reliable p-values. The results are shown in Figure 1  and demonstrate that for this
combination of distribution and kernel  Spec performs almost uniformly better than both Gamma and
Pears. We emphasize that the performance advantage of Spec is greatest when we restrict ourselves
to higher quantiles  which are of most interest in testing.

6
.
0

∆

0.08

0.07

0.06

0.05

0.04

0.03

 

0.02
−4

∆
0.6 vs σ

Gam
Spec
Pears

−2

0

(σ)
log
2

 

2

∆
0.7 vs σ

0.08

0.06

Gam
Spec
Pears

7
.
0

∆

0.04

0.02

 

0
−4

−2

0

(σ)
log
2

 

2

8
.
0

∆

0.06

0.05

0.04

0.03

0.02

0.01

 

0
−4

∆
0.8 vs σ

Gam
Spec
Pears

−2

0

(σ)
log
2

 

2

∆
0.9 vs σ

0.05

0.04

Gam
Spec
Pears

9
.
0

∆

0.03

0.02

 

0.01
−4

−2

0

(σ)
log
2

 

2

Figure 1: Evolution of ∆
q for resp. the Gamma (Gam)  Spectrum (Spec)  and Pearson (Pears) approximations
to the null distribution  as the Gaussian kernel bandwidth parameter varies. From left to right  plots of ∆
versus σ = 2−4  2−3  . . .   22 for q = 0.6  0.7  0.8  0.9.

q

Benchmark data: We next demonstrate the performance of the MMD tests on a number of mul-
tivariate datasets  taken from [14  Table 1]. We compared microarray data from normal and tumor
tissues (Health status)  microarray data from different subtypes of cancer (Subtype)  and local ﬁeld
potential (LFP) electrode recordings from the Macaque primary visual cortex (V1) with and with-
out spike events (Neural Data I and II  described in [24]). In all cases  we were provided with two
samples having different statistical properties  where the detection of these differences was made
difﬁcult by the high data dimensionality (for the microarray data  density estimation is impossi-

6

ble given the small sample size and high data dimensionality  and a successful test cannot rely on
accurate density estimates as an intermediate step).

In computing the null distributions for both the Spec and Pears cases  we drew 500 samples from the
associated null distribution estimates  and computed the test thresholds using the resulting empirical
quantiles. For the Spec case  we computed the eigenspectrum on the gram matrix of the aggregate
data from P and Q  retaining in all circumstances the maximum number 2m − 1 of nonzero eigen-
values of the empirical Gram matrix. This is a conservative approach  given that the Gram matrix
spectrum may decay rapidly [2  Appendix C]  in which case it might be possible to safely discard the
smallest eigenvalues. For the bootstrap approach Boot  we aggregated points from the two samples 
then assigned these randomly without replacement to P and Q. In our experiments  we performed
500 such iterations  and used the resulting histogram of MMD values as our null distribution. We
used a Gaussian kernel in all cases  with the bandwidth set to the median distance between points in
the aggregation of samples from P and Q.
We applied our tests to the benchmark data as follows: Given datasets A and B  we either drew one
sample with replacement from A and the other from B (in which case a Type II error was made
when the null hypothesis H0 was accepted); or we drew both samples with replacement from a
single pool consisting of A and B combined (in which case a Type I error was made when H0
was rejected: this should happen a fraction 1 − α of the time). This procedure was repeated 1000
times to obtain average performance ﬁgures. We summarize our results in Table 1. Note that an
extensive benchmark of the MMD Boot and Pears tests against other nonparametric approaches to
two-sample testing is provided in [14]: these include the the Friedman-Rafsky generalisation of the
Kolmogorov-Smirnov and Wald-Wolfowitz tests [9]  the Biau-Gy¨orﬁ test [6]  and the Hall-Tajvidi
test [17]. See [14] for details.

We observe that the kernel tests perform extremely well on these data: the Type I error is in the
great majority of cases close to its design value of 1 − α  and the Type II error is very low (and
often zero). The Spec test is occasionally slightly conservative  and has a lower Type I error than
required: this is most pronounced in the Health Status dataset  for which the sample size m is low.
The computational cost shows the expected trend  with Gamma being least costly  followed by Spec 
Pears  and ﬁnally Boot (this trend is only visible for the larger m = 500 datasets). Note that for yet
larger sample sizes  however  we expect the cost of Pears to exceed that of the remaining methods 
due to its O(m3) cost requirement (vs O(m2) for the other approaches).

Dataset
Neural Data I

Neural Data II

Health status

Subtype

Attribute
Type I/Type II
Time (sec)
Type I/Type II
Time (sec)
Type I/Type II
Time (sec)
Type I/Type II
Time (sec)

Gamma
0.95 / 0.00
0.06
0.96 / 0.00
0.08
0.96 / 0.00
0.01
0.95 / 0.02
0.05

Pears
0.96 / 0.00
3.92
0.96 / 0.00
3.97
0.96 / 0.00
0.01
0.95 / 0.01
0.05

Spec
0.96 / 0.00
2.79
0.97 / 0.00
2.91
0.98 / 0.00
0.01
0.96 / 0.01
0.05

Boot
0.96 / 0.00
5.79
0.96 / 0.00
8.08
0.95 / 0.00
0.03
0.94 / 0.01
0.07

Table 1: Benchmarks for the kernel two-sample tests on high dimensional multivariate data. Type I and Type
II errors are provided  as are average run times. Sample size (dimension): Neural I 500 (63) ; Neural II 500
(100); Health Status 25 (12 600); Subtype 25 (2 118).

the
are

of

from the

test

on

we

data.

Our

data

demonstrate

performance
taken

the
Canadian Hansard

structured
Finally 
(text)
corpus
(http : //www.isi.edu/natural − language/download/hansard/). As in the earlier work on
dependence testing presented in [15]  debate transcripts on the three topics of agriculture  ﬁsheries 
and immigration were used. Transcripts were in English and French  however we conﬁne ourselves
to reporting results on the English data (the results on the French data were similar). Our goal was to
distinguish samples on different topics  for instance P being drawn from transcripts on agriculture
and Q from transcripts on immigration (in the null case  both samples were from the same topic).
The data were processed following the same procedures as in [15]. We investigated two different
kernels on text: the k-substring kernel of [22  30] with k = 10  and a bag-of-words kernel. In
both cases  we computed kernels between ﬁve-line extracts  ignoring lines shorter than ﬁve words
long. Results are presented in Figure 2  and represent an average over all three combinations of

7

1

0.8

0.6

0.4

0.2

r
o
r
r
e
 
I
I
 
e
p
y
T

0
 
10

Test performance bow

Test performance spec

Gram matrix spectrum  bow

 

Gamma
Pears
Spec
Boot

1

0.8

0.6

0.4

0.2

r
o
r
r
e
 
I
I
 
e
p
y
T

0.08

0.06

0.04

0.02

e
d
u
t
i
l

p
m
a
 
.
g
E

i

20
40
Sample size m

30

50

0
10

20
40
Sample size m

30

50

0
0

5

10

Eig. index

15

20

Figure 2: Canadian Hansard data. Left: Average Type II error over all of agriculture-ﬁsheries  agriculture-
immigration  and ﬁsheries-immigration  for the bag-of-words kernel. Center: Average Type II error for the
k-substring kernel. Right: Eigenspectrum of a centered Gram matrix obtained by drawing m = 10 points from
each of P and Q  where P 6= Q  for the bag-of-words kernel.

different topic pairs: agriculture-ﬁsheries  agriculture-immigration  and ﬁsheries-immigration. For
each topic pairing  results are averaged over 300 repetitions.

We observe that in general  the MMD is very effective at distinguishing distributions of text frag-
ments on different topics: for sample sizes above 30  all the test procedures are able to detect differ-
ences in distribution with zero Type II error  for both kernels. When the k-substring kernel is used 
the Boot  Gamma  and Pears approximations can distinguish the distributions for sample sizes as low
as 10: this indicates that a more sophisticated encoding of the text than provided by bag-of-words
results in tests of greater sensitivity (consistent with the independence testing observations of [15]).

We now investigate the fact that for sample sizes below m = 30 on the Hansard data  the Spec
test has a much higher Type II error the alternatives. The k-substring and bag-of-words kernels are
diagonally dominant: thus for small sample sizes  the empirical estimate of the kernel spectrum
is effectively truncated at a point where the eigenvalues remain large  introducing a bias (Figure
2). This effect vanishes on the Hansard benchmark once the number of samples reaches 25-30.
By contrast  for the Neural data using a Gaussian kernel  this small sample bias is not observed 
and the Spec test has equivalent Type II performance to the other three tests (see Figure 1 in the
online supplement). In this case  for sample sizes of interest (i.e.  where there are sufﬁcient samples
to obtain a Type II error of less than 50%)  the bias in the Spec test due to spectral truncation is
negligible. We emphasize that the speed advantage of the Spec test becomes important only for
larger sample sizes (and the consistency guarantee is only meaningful in this regime).

5 Conclusion

We have presented a novel method for estimating the null distribution of the RKHS distance be-
tween probability distribution embeddings  for use in a nonparametric test of homogeneity. Unlike
previous parametric heuristics based on moment matching  our new distribution estimate is consis-
tent; moreover  it is computationally less costly than the bootstrap  which is the only alternative
consistent approach. We have demonstrated in experiments that our method performs well on high
dimensional multivariate data and text  as well as for distributions where the parametric heuristics
show inaccuracies. We anticipate that our approach may also be generalized to kernel independence
tests [15]  and to homogeneity tests based on the kernel Fisher discriminant [18].
Acknowledgments: The ordering of the second through fourth authors is alphabetical. We thank Choon-Hui
Teo for generating the Gram matrices for the text data  Malte Rasch for his assistance in the experimental
evaluation  and Karsten Borgwardt for his assistance with the microarray data. A. G. was supported by grants
DARPA IPTO FA8750-09-1-0141  ONR MURI N000140710747  and ARO MURI W911NF0810242. Z. H.
was supported by grants from the Technical Support Working Group through funding from the Investigative
Support and Forensics subgroup and NIMH 51435  and from Agence Nationale de la Recherche under contract
ANR-06-BLAN-0078 KERNSIG. B. K. S. was supported by the MPI for Biological Cybernetics  NSF (grant
DMS-MSPA 0625409)  the Fair Isaac Corporation and the University of California MICRO program.

References
[1] N. Anderson  P. Hall  and D. Titterington. Two-sample test statistics for measuring discrepancies be-
tween two multivariate probability density functions using kernel-based density estimates. Journal of

8

Multivariate Analysis  50:41–54  1994.

[2] F. R. Bach and M. I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res.  3:1–48  2002.
[3] C. Baker. Joint measures and cross-covariance operators. Transactions of the American Mathematical

[4] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.

Society  186:273–289  1973.

Springer-Verlag  Berlin  2003.

[5] Rajendra Bhatia and Ludwig Elsner. The Hoffman-Wielandt inequality in inﬁnite dimensions. Proceed-

ings of Indian Academy of Science (Mathematical Sciences)  104(3):483–494  1994.

[6] G. Biau and L. Gyorﬁ. On the asymptotic properties of a nonparametric l1-test statistic of homogeneity.

IEEE Transactions on Information Theory  51(11):3965–3973  2005.

[7] G. Blanchard  O. Bousquet  and L. Zwald. Statistical properties of kernel principal component analysis.

Machine Learning  66:259–294  2007.

[8] K. M. Borgwardt  A. Gretton  M. J. Rasch  H.-P. Kriegel  B. Sch¨olkopf  and A. J. Smola. Integrating
structured biological data by kernel maximum mean discrepancy. Bioinformatics (ISMB)  22(14):e49–
e57  2006.

[9] J. Friedman and L. Rafsky. Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample

tests. The Annals of Statistics  7(4):697–717  1979.

[10] K. Fukumizu  F. R. Bach  and M. I. Jordan. Dimensionality reduction for supervised learning with repro-

ducing kernel Hilbert spaces. J. Mach. Learn. Res.  5:73–99  2004.

[11] K. Fukumizu  A. Gretton  X. Sun  and B. Sch¨olkopf. Kernel measures of conditional dependence.

In

NIPS 20  pages 489–496  2008.

[12] K. Fukumizu  B. Sriperumbudur  A. Gretton  and B. Sch¨olkopf. Characteristic kernels on groups and

semigroups. In NIPS 21  pages 473–480  2009.

[13] G. Golub and Q. Ye. An inverse free preconditioned krylov subspace method for symmetric generalized

eigenvalue problems. SIAM Journal on Scientiﬁc Computing  24:312–334  2002.

[14] A. Gretton  K. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. Smola. A kernel method for the two-sample-

[15] A. Gretton  K. Fukumizu  C.-H. Teo  L. Song  B. Sch¨olkopf  and A. Smola. A kernel statistical test of

problem. In NIPS 19  pages 513–520  2007.

independence. In NIPS 20  pages 585–592  2008.

[16] G. R. Grimmet and D. R. Stirzaker. Probability and Random Processes. Oxford University Press  Oxford 

[17] P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.

third edition  2001.

Biometrika  89(2):359–374  2002.

[18] Z. Harchaoui  F. Bach  and E. Moulines. Testing for homogeneity with kernel ﬁsher discriminant analysis.

In NIPS 20  pages 609–616. 2008. (long version: arXiv:0804.1026v1).

[19] M. Hein and O. Bousquet. Kernels  associated structures  and generalizations. Technical Report 127 

Max Planck Institute for Biological Cybernetics  2004.

[20] N. L. Johnson  S. Kotz  and N. Balakrishnan. Continuous Univariate Distributions. Volume 1 (Second

Edition). John Wiley and Sons  1994.

[21] E. Lehmann and J. Romano. Testing Statistical Hypothesis (3rd ed.). Wiley  New York  2005.
[22] C. Leslie  E. Eskin  and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classiﬁcation.

In Proceedings of the Paciﬁc Symposium on Biocomputing  pages 564–575  2002.

[23] A. S. Markus. The eigen- and singular values of the sum and product of linear operators. Russian

Mathematical Surveys  19(4):93–123  1964.

[24] M. Rasch  A. Gretton  Y. Murayama  W. Maass  and N. K. Logothetis. Predicting spiking activity from

local ﬁeld potentials. Journal of Neurophysiology  99:1461–1476  2008.

[25] B. Sch¨olkopf and A. Smola. Learning with Kernels. MIT Press  Cambridge  MA  2002.
[26] B. Sch¨olkopf  A. J. Smola  and K.-R. M¨uller. Nonlinear component analysis as a kernel eigenvalue

problem. Neural Computation  10:1299–1319  1998.

[27] J. Shawe-Taylor  C. Williams  N. Cristianini  and J. Kandola. On the eigenspectrum of the Gram matrix

and the generalisation error of kernel PCA. IEEE Trans. Inf. Theory  51(7):2510–2522  2005.

[28] A. J. Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A Hilbert space embedding for distributions. In ALT

[29] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf.

Injective hilbert space

embeddings of probability measures. In COLT 21  pages 111–122  2008.

[30] C. H. Teo and S. V. N. Vishwanathan. Fast and space efﬁcient string kernels using sufﬁx arrays. In ICML 

18  pages 13–31  2007.

pages 929–936  2006.

[31] J. E. Wilkins. A note on skewness and kurtosis. Ann. Math. Stat.  15(3):333–335  1944.
[32] G. Zech and B. Aslan. A multivariate two-sample test based on the concept of minimum energy.

In

PHYSTAT  pages 97–100  2003.

9

,Cem Keskin
Shahram Izadi