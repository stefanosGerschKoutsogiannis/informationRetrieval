2015,Variance Reduced Stochastic Gradient Descent with Neighbors,Stochastic Gradient Descent (SGD) is a workhorse in machine learning  yet it is also known to be slow relative to steepest descent.  Recently  variance reduction techniques such as SVRG and SAGA have been proposed to overcome this weakness. With asymptotically vanishing variance  a constant step size can be maintained  resulting in geometric convergence rates. However  these methods are either based on occasional computations of full gradients at pivot points (SVRG)  or on keeping per data point corrections in memory (SAGA). This has the disadvantage that one cannot employ these methods in a streaming setting and that speed-ups relative to SGD may need a certain number of epochs in order to materialize. This paper investigates a new class of algorithms that can exploit neighborhood structure in the training data to share and re-use information about past stochastic gradients across data points. While not meant to be offering advantages in an asymptotic setting  there are significant benefits in the transient optimization phase  in particular in a streaming or single-epoch setting. We investigate this family of algorithms in a thorough analysis and show supporting experimental results. As a side-product we provide a simple and unified proof technique for a broad class of variance reduction algorithms.,Variance Reduced Stochastic Gradient Descent

with Neighbors

Thomas Hofmann

Department of Computer Science

ETH Zurich  Switzerland

Aurelien Lucchi

Department of Computer Science

ETH Zurich  Switzerland

Simon Lacoste-Julien

INRIA - Sierra Project-Team

´Ecole Normale Sup´erieure  Paris  France

Brian McWilliams

Department of Computer Science

ETH Zurich  Switzerland

Abstract

Stochastic Gradient Descent (SGD) is a workhorse in machine learning  yet its
slow convergence can be a computational bottleneck. Variance reduction tech-
niques such as SAG  SVRG and SAGA have been proposed to overcome this
weakness  achieving linear convergence. However  these methods are either based
on computations of full gradients at pivot points  or on keeping per data point cor-
rections in memory. Therefore speed-ups relative to SGD may need a minimal
number of epochs in order to materialize. This paper investigates algorithms that
can exploit neighborhood structure in the training data to share and re-use infor-
mation about past stochastic gradients across data points  which offers advantages
in the transient optimization phase. As a side-product we provide a uniﬁed con-
vergence analysis for a family of variance reduction algorithms  which we call
memorization algorithms. We provide experimental results supporting our theory.

1

Introduction

We consider a general problem that is pervasive in machine learning  namely optimization of an em-
pirical or regularized convex risk function. Given a convex loss l and a µ-strongly convex regularizer
Ω  one aims at ﬁnding a parameter vector w which minimizes the (empirical) expectation:

n(cid:88)

i=1

w∗ = argmin

w

f (w) 

f (w) =

1
n

fi(w) 

fi(w) := l(w  (xi  yi)) + Ω(w) .

(1)

We assume throughout that each fi has L-Lipschitz-continuous gradients. Steepest descent can
ﬁnd the minimizer w∗  but requires repeated computations of full gradients f(cid:48)(w)  which becomes
prohibitive for massive data sets. Stochastic gradient descent (SGD) is a popular alternative  in
particular in the context of large-scale learning [2  10]. SGD updates only involve f(cid:48)
i (w) for an index
i chosen uniformly at random  providing an unbiased gradient estimate  since Ef(cid:48)
i (w) = f(cid:48)(w).
It is a surprising recent ﬁnding [11  5  9  6] that the ﬁnite sum structure of f allows for signiﬁcantly
faster convergence in expectation. Instead of the standard O(1/t) rate of SGD for strongly-convex
functions  it is possible to obtain linear convergence with geometric rates. While SGD requires
asymptotically vanishing learning rates  often chosen to be O(1/t) [7]  these more recent methods
introduce corrections that ensure convergence for constant learning rates.
Based on the work mentioned above  the contributions of our paper are as follows: First  we de-
ﬁne a family of variance reducing SGD algorithms  called memorization algorithms  which includes
SAGA and SVRG as special cases  and develop a unifying analysis technique for it. Second  we

1

show geometric rates for all step sizes γ < 1
4L  including a universal (µ-independent) step size
choice  providing the ﬁrst µ-adaptive convergence proof for SVRG. Third  based on the above anal-
ysis  we present new insights into the trade-offs between freshness and biasedness of the corrections
computed from previous stochastic gradients. Fourth  we propose a new class of algorithms that
resolves this trade-off by computing corrections based on stochastic gradients at neighboring points.
We experimentally show its beneﬁts in the regime of learning with a small number of epochs.

2 Memorization Algorithms

2.1 Algorithms

(cid:80)n

Variance Reduced SGD Given an optimization problem as in (1)  we investigate a class of
stochastic gradient descent algorithms that generates an iterate sequence wt (t ≥ 0) with updates
taking the form:

gi(w) = f(cid:48)

i (w) − ¯αi with

w+ = w − γgi(w) 
where ¯α := 1
j=1 αj. Here w is the current and w+ the new parameter vector  γ is the step size 
n
and i is an index selected uniformly at random. ¯αi are variance correction terms such that E¯αi = 0 
which guarantees unbiasedness Egi(w) = f(cid:48)(w). The aim is to deﬁne updates of asymptotically
vanishing variance  i.e. gi(w) → 0 as w → w∗  which requires ¯αi → f(cid:48)
i (w∗). This implies that
corrections need to be designed in a way to exactly cancel out the stochasticity of f(cid:48)
i (w∗) at the
optimum. How the memory αj is updated distinguishes the different algorithms that we consider.

¯αi := αi − ¯α 

(2)

i = f(cid:48)

i (w) for the selected i  and α+

SAGA The SAGA algorithm [4] maintains variance corrections αi by memorizing stochastic gra-
j = αj  for j (cid:54)= i. Note that
dients. The update rule is α+
these corrections will be used the next time the same index i gets sampled. Setting ¯αi := αi − ¯α
guarantees unbiasedness. Obviously  ¯α can be updated incrementally. SAGA reuses the stochastic
gradient f(cid:48)
q-SAGA We also consider q-SAGA  a method that updates q ≥ 1 randomly chosen αj variables
at each iteration. This is a convenient reference point to investigate the advantages of “fresher”
corrections. Note that in SAGA the corrections will be on average n iterations “old”. In q-SAGA
this can be controlled to be n/q at the expense of additional gradient computations.

i (w) computed at step t to update w as well as ¯αi.

SVRG We reformulate a variant of SVRG [5] in our framework using a randomization argument
similar to (but simpler than) the one suggested in [6]. Fix q > 0 and draw in each iteration r ∼
j(w) (∀j) is performed  otherwise they are
Uniform[0; 1). If r < q/n  a complete update  α+
left unchanged. While q-SAGA updates exactly q variables in each iteration  SVRG occasionally
updates all α variables by triggering an additional sweep through the data. There is an option to not
maintain α variables explicitly and to save on space by storing only ¯α = f(cid:48)(w) and w.

j = f(cid:48)

(cid:26)f(cid:48)

j(w)
αj

if j ∈ J
otherwise 

Uniform Memorization Algorithms Motivated by SAGA and SVRG  we deﬁne a class of algo-
rithms  which we call uniform memorization algorithms.
Deﬁnition 1. A uniform q-memorization algorithm evolves iterates w according to Eq. (2) and
selects in each iteration a random index set J of memory locations to update according to

such that any j has the same probability of q/n of being updated  i.e. ∀j (cid:80)
Note that q-SAGA and the above SVRG are special cases. For q-SAGA: P{J} = 1/(cid:0)n

j :=

α+

J(cid:51)j P{J} = q
n .

(cid:1) if |J| = q

(3)

P{J} = 0 otherwise. For SVRG: P{∅} = 1 − q/n  P{[1 : n]} = q/n  P{J} = 0  otherwise.
N -SAGA Because we need it in Section 3  we will also deﬁne an algorithm  which we call N -
SAGA  which makes use of a neighborhood system Ni ⊆ {1  . . .   n} and which selects neighbor-
hoods uniformly  i.e. P{Ni} = 1

n. Note that Deﬁnition 1 requires |{i : j ∈ Ni}| = q (∀j).

q

2

i (w) = ξ(cid:48)

Finally  note that for generalized linear models where fi depends on xi only through (cid:104)w  xi(cid:105)  we
get f(cid:48)
i(w)xi  i.e. the update direction is determined by xi  whereas the effective step length
depends on the derivative of a scalar function ξi(w). As used in [9]  this leads to signiﬁcant memory
savings as one only needs to store the scalars ξ(cid:48)
i(w) as xi is always given when performing an update.

2.2 Analysis

2(cid:107)w − w∗(cid:107)2  

Recurrence of Iterates The evolution equation (2) in expectation implies the recurrence (by cru-
cially using the unbiasedness condition Egi(w) = f(cid:48)(w)):

E(cid:107)w+−w∗(cid:107)2 = (cid:107)w − w∗(cid:107)2 − 2γ(cid:104)f(cid:48)(w)  w − w∗(cid:105) + γ2E(cid:107)gi(w)(cid:107)2 .

(4)
Here and in the rest of this paper  expectations are always taken only with respect to i (conditioned
on the past). We utilize a number of bounds (see [4])  which exploit strong convexity of f (wherever
µ appears) as well as Lipschitz continuity of the fi-gradients (wherever L appears):

(cid:104)f(cid:48)(w)  w − w∗(cid:105) ≥ f (w) − f (w∗) + µ

i (w∗)(cid:105)  

i (w∗)(cid:107)2.

(cid:107)f(cid:48)
E(cid:107)f(cid:48)

i (w∗)(cid:107)2  

i (w) − f(cid:48)

E(cid:107)gi(w)(cid:107)2 ≤ 2E(cid:107)f(cid:48)

i (w) − f(cid:48)
i (w)−f(cid:48)
E(cid:107)¯αi − f(cid:48)

i (w∗)(cid:107)2 ≤ 2Lhi(w) 
i (w∗)(cid:107)2 ≤ 2Lf δ(w) 
i (w∗)(cid:107)2 = E(cid:107)αi − f(cid:48)

i (w∗)(cid:107)2 + 2E(cid:107)¯αi − f(cid:48)
hi(w) := fi(w) − fi(w∗) − (cid:104)w − w∗  f(cid:48)
f δ(w) := f (w) − f (w∗)  
i (w∗)(cid:107)2 − (cid:107)¯α(cid:107)2 ≤ E(cid:107)αi − f(cid:48)

(5)
(6)
(7)
(8)
(9)
Eq. (6) can be generalized [4] using (cid:107)x±y(cid:107)2 ≤ (1+β)(cid:107)x(cid:107)2 +(1+β−1)(cid:107)y(cid:107)2 with β > 0. However
for the sake of simplicity  we sacriﬁce tightness and choose β = 1. Applying all of the above yields:
Lemma 1. For the iterate sequence of any algorithm that evolves solutions according to Eq. (2)  the
following holds for a single update step  in expectation over the choice of i:
(cid:107)w − w∗(cid:107)2 − E(cid:107)w+ − w∗(cid:107)2 ≥ γµ(cid:107)w − w∗(cid:107)2 − 2γ2E(cid:107)αi − f(cid:48)
All proofs are deferred to the Appendix.
Ideal and Approximate Variance Correction Note that in the ideal case of αi = f(cid:48)
would immediately get a condition for a contraction by choosing γ = 1
with ρ = γµ = µ
How can we further bound E(cid:107)αi − f(cid:48)
key insight is that for memorization algorithms  we can apply the smoothness bound in Eq. (7)

i (w∗)  we
2L  yielding a rate of 1 − ρ
i (w∗)(cid:107)2 in the case of “non-ideal” variance-reducing SGD? A

i (w∗)(cid:107)2 +(cid:0)2γ − 4γ2L(cid:1) f δ(w) .

2L  which is half the inverse of the condition number κ := L/µ.

i (w∗)(cid:107)2 = (cid:107)f(cid:48)

i (wτi) − f(cid:48)

i (w∗)(cid:107)2 ≤ 2Lhi(wτi) 

(10)
Note that if we only had approximations βi in the sense that (cid:107)βi − αi(cid:107)2 ≤ i (see Section 3)  then
we can use (cid:107)x − y(cid:107) ≤ 2(cid:107)x(cid:107) + 2(cid:107)y(cid:107) to get the somewhat worse bound:

(where wτi is old w) .

(cid:107)αi − f(cid:48)

(cid:107)βi − f(cid:48)

i (w∗)(cid:107)2 ≤ 2(cid:107)αi − f(cid:48)

i (w∗)(cid:107)2 + 2(cid:107)βi − αi(cid:107)2 ≤ 4Lhi(wτi) + 2i.

(11)

Lyapunov Function Ideally  we would like to show that for a suitable choice of γ  each iteration
results in a contraction E(cid:107)w+ − w∗(cid:107)2 ≤ (1 − ρ)(cid:107)w − w∗(cid:107)2  where 0 < ρ ≤ 1. However  the main
challenge arises from the fact that the quantities αi represent stochastic gradients from previous iter-
ations. This requires a somewhat more complex proof technique. Adapting the Lyapunov function
i (w∗)(cid:107)2 such that Hi → 0 as w → w∗. We
method from [4]  we deﬁne upper bounds Hi ≥ (cid:107)αi − f(cid:48)
i = 0 and (conceptually) initialize Hi = (cid:107)f(cid:48)
i (w∗)(cid:107)2  and then update Hi in sync with αi 
start with α0
if αi is updated
(12)
otherwise
(cid:80)n
i (w∗)(cid:107)2 ≤ ¯H with
i=1 Hi. The Hi are quantities showing up in the analysis  but need not be computed. We

so that we always maintain valid bounds (cid:107)αi − f(cid:48)
¯H := 1
n
now deﬁne a σ-parameterized family of Lyapunov functions1
Lσ(w  H) := (cid:107)w − w∗(cid:107)2 + Sσ ¯H  with S :=

i (w∗)(cid:107)2 ≤ Hi and E(cid:107)αi − f(cid:48)

and 0 ≤ σ ≤ 1 .

(13)

(cid:26)2L hi(w)

(cid:18) γn

(cid:19)

H +
i

Hi

:=

Lq

1This is a simpliﬁed version of the one appearing in [4]  as we assume f(cid:48)(w∗) = 0 (unconstrained regime).

3

In expectation under a random update  the Lyapunov function Lσ changes as ELσ(w+  H +) =
E(cid:107)w+ − w∗(cid:107)2 + Sσ E ¯H +. We can readily apply Lemma 1 to bound the ﬁrst part. The second part
is due to (12)  which mirrors the update of the α variables. By crucially using the property that any
αj has the same probability of being updated in (3)  we get the following result:
Lemma 2. For a uniform q-memorization algorithm  it holds that

E ¯H + =

¯H +

2Lq
n

f δ(w).

(14)

(cid:18) n − q

(cid:19)

n

Note that in expectation the shrinkage does not depend on the location of previous iterates wτ and
the new increment is proportional to the sub-optimality of the current iterate w. Technically  this is
how the possibly complicated dependency on previous iterates is dealt with in an effective manner.

Convergence Analysis We ﬁrst state our main Lemma about Lyapunov function contractions:
Lemma 3. Fix c ∈ (0; 1] and σ ∈ [0; 1] arbitrarily. For any uniform q-memorization algorithm with
sufﬁciently small step size γ such that

(cid:26) Kσ

(cid:27)

we have that

min

γ ≤ 1
2L

4qL
nµ
ELσ(w+  H +) ≤ (1 − ρ)Lσ(w  H)  with ρ := cµγ.

and K :=

  1 − σ

K + 2cσ

 

 

Note that γ < 1

2L maxσ∈[0 1] min{σ  1 − σ} = 1

4L (in the c → 0 limit).

(15)

(16)

(17)

(18)

ρ(γ) =

q
n

where

· 1 − a
1 − a/2
a∗(K)
4L

By maximizing the bounds in Lemma 3 over the choices of c and σ  we obtain our main result that
provides guaranteed geometric rates for all step sizes up to 1
4L.
Theorem 1. Consider a uniform q-memorization algorithm. For any step size γ = a
the algorithm converges at a geometric rate of at least (1 − ρ(γ)) with

4L with a < 1 

=

µ
4L

· K(1 − a)
1 − a/2

if γ ≥ γ∗(K)  otherwise ρ(γ) = µγ

 

(cid:21)

γ∗(K) :=

a∗(K) :=

 

√
2K

  K :=

4qL
nµ

=

κ .

4q
n

(cid:20)

a∗(K)

1 + K +
We would like to provide more insights into this result.
Corollary 1. In Theorem 1  ρ is maximized for γ = γ∗(K). We can write ρ∗(K) = ρ(γ∗) as

1 + K 2

√
2

µ

=

K

q
n

q
n

√

1 + K 2

1 + K +

ρ∗(K) =

2 K−1 + O(K−3)).

a∗(K) =
n (1 − 1

2); 1] ≈ [0.585; 1]. So for q ≤ n µ

(19)
2 K + O(K 3))  whereas in the ill-conditioned case ρ∗ =

4L in the regime where the condition number dominates n (large
n in the opposite regime of large data (small K). Note that if K ≤ 1  we have ρ∗ = ζ q
4L  it pays off to increase freshness as it affects

µ
4L
In the big data regime ρ∗ = q
4L (1 − 1
The guaranteed rate is bounded by µ
K) and by q
with ζ ∈ [2/(2 +
the rate proportionally. In the ill-conditioned regime (κ > n)  the inﬂuence of q vanishes.
Note that for γ ≥ γ∗(K)  γ → 1
4L the rate decreases monotonically  yet the decrease is only minor.
With the exception of a small neighborhood around 1
4L ) results in
very similar rates. Underestimating γ∗ however leads to a (signiﬁcant) slow-down by a factor γ/γ∗.
As the optimal choice of γ depends on K  i.e. µ  we would prefer step sizes that are µ-independent 
thus giving rates that adapt to the local curvature (see [9]). It turns out that by choosing a step size
that maximizes minK ρ(γ)/ρ∗(K)  we obtain a K-agnostic step size with rate off by at most 1/2:
Corollary 2. Choosing γ = 2−√
To gain more insights into the trade-offs for these ﬁxed large universal step sizes  the following
corollary details the range of rates obtained:
L}. In particular  we have
Corollary 3. Choosing γ = a
L} (roughly matching the rate given in [4] for q = 1).
for the choice γ = 1

4L with a < 1 yields ρ = min{ 1−a
1− 1
2 a

4L   leads to ρ(γ) ≥ (2 − √

4L  the entire range of γ ∈ [γ∗; 1

5L that ρ = min{ 1

2 ρ∗(K) for all K.

2)ρ∗(K) > 1

q

n   1

5

µ

q

n   a

4

n

µ

3

2

4

3 Sharing Gradient Memory

3.1

-Approximation Analysis

stochastic gradients in the current update and by assuring(cid:80)

As we have seen  fresher gradient memory  i.e. a larger choice for q  affects the guaranteed conver-
gence rate as ρ ∼ q/n. However  as long as one step of a q-memorization algorithm is as expensive
as q steps of a 1-memorization algorithm  this insight does not lead to practical improvements per
se. Yet  it raises the question  whether we can accelerate these methods  in particular N -SAGA 
by approximating gradients stored in the αi variables. Note that we are always using the correct
i ¯αi = 0  we will not introduce any bias
in the update direction. Rather  we lose the guarantee of asymptotically vanishing variance at w∗.
However  as we will show  it is possible to retain geometric rates up to a δ-ball around w∗.
We will focus on SAGA-style updates for concreteness and investigate an algorithm that mirrors N -
SAGA with the only difference that it maintains approximations βi to the true αi variables. We aim
to guarantee E(cid:107)αi − βi(cid:107)2 ≤  and will use Eq. (11) to modify the right-hand-side of Lemma 1. We
see that approximation errors i are multiplied with γ2  which implies that we should aim for small
learning rates  ideally without compromising the N -SAGA rate. From Theorem 1 and Corollary 1
we can see that we can choose γ (cid:46) q/µn for n sufﬁciently large  which indicates that there is hope
to dampen the effects of the approximations. We now make this argument more precise.
Theorem 2. Consider a uniform q-memorization algorithm with α-updates that are on average -
accurate (i.e. E(cid:107)αi − βi(cid:107)2 ≤ ). For any step size γ ≤ ˜γ(K)  where ˜γ is given by Corollary 5 in
the appendix (note that ˜γ(K) ≥ 2
EL(wt  H t) ≤ (1 − µγ)tL0 +

(20)
where E denote the (unconditional) expectation over histories (in contrast to E which is conditional) 
and s(γ) := 4γ
Corollary 4. With γ = min{µ  ˜γ(K)} we have

3 γ∗(K) and ˜γ(K) → γ∗(K) as K → 0)  we get

with L0 := (cid:107)w0 − w∗(cid:107)2 + s(γ)E(cid:107)fi(w∗)(cid:107)2 

Kµ (1 − 2Lγ).

4γ
µ

 

4γ
µ

n  we thus converge towards some

with a rate ρ = min{µ2  µ˜γ} .

≤ 4 
(21)
√
In the relevant case of µ ∼ 1/
-ball around w∗ at a similar
rate as for the exact method. For µ ∼ n−1  we have to reduce the step size signiﬁcantly to com-
-ball  resulting in the slower rate ρ ∼ n−2 
pensate the extra variance and to still converge to an
instead of ρ ∼ n−1.
We also note that the geometric convergence of SGD with a constant step size to a neighborhood
of the solution (also proven in [8]) can arise as a special case in our analysis. By setting αi = 0 in
Lemma 1  we can take  = E(cid:107)f(cid:48)
i (w∗)(cid:107)2 for SGD. An approximate q-memorization algorithm can
thus be interpreted as making  an algorithmic parameter  rather than a ﬁxed value as in SGD.

√

√

3.2 Algorithms

Sharing Gradient Memory We now discuss our proposal of using neighborhoods for sharing
gradient information between close-by data points. Thereby we avoid an increase in gradient com-
putations relative to q- or N -SAGA at the expense of suffering an approximation bias. This leads
to a new tradeoff between freshness and approximation quality  which can be resolved in non-trivial
ways  depending on the desired ﬁnal optimization accuracy.
We distinguish two types of quantities. First  the gradient memory αi as deﬁned by the reference
algorithm N -SAGA. Second  the shared gradient memory state βi  which is used in a modiﬁed
update rule in Eq. (2)  i.e. w+ = w − γ(f(cid:48)
i (w) − βi + ¯β). Assume that we select an index i for the
weight update  then we generalize Eq. (3) as follows
if j ∈ Ni
otherwise  

¯βi := βi − ¯β .

(cid:26)f(cid:48)

n(cid:88)

i (w)
βj

¯β :=

(22)

β+
j

:=

1
n

βi 

i=1

In the important case of generalized linear models  where one has f(cid:48)
the relevant case in Eq. (22) by β+
j
direction  while reducing storage requirements.

i(w)xi  we can modify
i(w)xj. This has the advantages of using the correct

i (w) = ξ(cid:48)

:= ξ(cid:48)

5

i(w)xi + λw with ξ(cid:48)
j (cid:107) = |ξ(cid:48)
(cid:107)α+

Approximation Bounds For our analysis  we need to control the error (cid:107)αi − βi(cid:107)2 ≤ i. This
obviously requires problem-speciﬁc investigations.
2(cid:107)w(cid:107)2 and thus
Let us ﬁrst look at the case of ridge regression. fi(w) := 1
i(w) := (cid:104)xi  w(cid:105) − yi. Considering j ∈ Ni being updated  we have
f(cid:48)
i (w) = ξ(cid:48)
j(w) − ξ(cid:48)

(23)
where δij := (cid:107)xi − xj(cid:107). Note that this can be pre-computed with the exception of the norm (cid:107)w(cid:107)
that we only know at the time of an update.
Similarly  for regularized logistic regression with y ∈ {−1  1}  we have ξ(cid:48)
With the requirement on neighbors that yi = yj we get

i(w)|(cid:107)xj(cid:107) ≤ (δij(cid:107)w(cid:107) + |yj − yi|)(cid:107)xj(cid:107) =: ij(w)

i(w) = yi/(1 + eyi(cid:104)xi w(cid:105)).

2 ((cid:104)xi  w(cid:105) − yi)2 + λ

j − β+

(cid:107)α+

j − β+

j (cid:107) ≤ eδij(cid:107)w(cid:107) − 1
Again  we can pre-compute δij and (cid:107)xj(cid:107). In addition to ξ(cid:48)
N -SAGA We can use these bounds in two ways. First  assuming that the iterates stay within a
norm-ball (e.g. L2-ball)  we can derive upper bounds

1 + e−(cid:104)xi w(cid:105)(cid:107)xj(cid:107) =: ij(w)

i(w) we can also store (cid:104)xi  w(cid:105).

(24)

j(r) ≥ max{ij(w) : j ∈ Ni  (cid:107)w(cid:107) ≤ r} 

(r) =

1
n

j(r) .

(25)

(cid:88)

j

Obviously  the more compact the neighborhoods are  the smaller (r). This is most useful for the
analysis. Second  we can specify a target accuracy  and then prune neighborhoods dynamically.
This approach is more practically relevant as it allows us to directly control . However  a dynam-
ically varying neighborhood violates Deﬁnition 1. We ﬁx this in a sound manner by modifying the
memory updates as follows:

f(cid:48)

i (w)
f(cid:48)
j(w)
βj

β+
j

:=

if j ∈ Ni and ij(w) ≤ 
if j ∈ Ni and ij(w) > 
otherwise

(26)

This allows us to interpolate between sharing more aggressively (saving computation) and perform-
ing more computations in an exact manner. In the limit of  → 0  we recover N -SAGA  as  → max
we recover the ﬁrst variant mentioned.

Computing Neighborhoods Note that the pairwise Euclidean distances show up in the bounds in
Eq. (23) and (24). In the classiﬁcation case we also require yi = yj  whereas in the ridge regression
case  we also want |yi − yj| to be small. Thus modulo ﬁltering  this suggests the use of Euclidean
distances as the metric for deﬁning neighborhoods. Standard approximation techniques for ﬁnding
near(est) neighbors can be used. This comes with a computational overhead  yet the additional costs
will amortize over multiple runs or multiple data analysis tasks.

4 Experimental Results

Algorithms We present experimental results on the performance of the different variants of mem-
orization algorithms for variance reduced SGD as discussed in this paper. SAGA has been uniformly
superior to SVRG in our experiments  so we compare SAGA and N -SAGA (from Eq. (26))  along-
side with SGD as a straw man and q-SAGA as a point of reference for speed-ups. We have chosen
q = 20 for q-SAGA and N -SAGA. The same setting was used across all data sets and experiments.

Data Sets As special cases for the choice of the loss function and regularizer in Eq. (1)  we con-
sider two commonly occurring problems in machine learning  namely least-square regression and
(cid:96)2-regularized logistic regression. We apply least-square regression on the million song year regres-
sion from the UCI repository. This dataset contains n = 515  345 data points  each described by
d = 90 input features. We apply logistic regression on the cov and ijcnn1 datasets obtained from
the libsvm website 2. The cov dataset contains n = 581  012 data points  each described by d = 54
input features. The ijcnn1 dataset contains n = 49  990 data points  each described by d = 22 input
features. We added an (cid:96)2-regularizer Ω(w) = µ(cid:107)w(cid:107)2

2 to ensure the objective is strongly convex.

2http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets

6

(a) Cov

(b) Ijcnn1

(c) Year

µ = 10−1  gradient evaluation

µ = 10−3  gradient evaluation

µ = 10−1  datapoint evaluation

µ = 10−3  datapoint evaluation

Figure 1: Comparison of N -SAGA  q-SAGA  SAGA and SGD (with decreasing and constant step
size) on three datasets. The top two rows show the suboptimality as a function of the number
of gradient evaluations for two different values of µ = 10−1  10−3. The bottom two rows show
the suboptimality as a function of the number of datapoint evaluations (i.e. number of stochastic
updates) for two different values of µ = 10−1  10−3.

7

epochs24681012141618Suboptimality10-810-610-410-2100SGDcstSGDSAGAq-SAGA0N-SAGA0=10N-SAGA0=0.10N-SAGA0=0.01epochs246810Suboptimality10-1010-810-610-410-2100SGDcstSGDSAGAq-SAGA0N-SAGA0=0.10N-SAGA0=0.050N-SAGA0=0.01epochs24681012141618Suboptimality10-810-610-410-2100SGDcstSGDSAGAq-SAGA0N-SAGA0=20N-SAGA0=10N-SAGA0=0.5epochs24681012141618Suboptimality10-810-610-410-2100epochs246810Suboptimality10-810-610-410-2100epochs24681012141618Suboptimality10-810-610-410-2100epochs24681012141618Suboptimality10-1010-810-610-410-2100epochs246810Suboptimality10-1010-5100epochs24681012141618Suboptimality10-1010-5100epochs24681012141618Suboptimality10-1010-5100epochs246810Suboptimality10-1010-5100epochs24681012141618Suboptimality10-1010-810-610-410-2100Experimental Protocol We have run the algorithms in question in an i.i.d. sampling setting and
averaged the results over 5 runs. Figure 1 shows the evolution of the suboptimality f δ of the ob-
jective as a function of two different metrics: (1) in terms of the number of update steps performed
(“datapoint evaluation”)  and (2) in terms of the number of gradient computations (“gradient evalua-
tion”). Note that SGD and SAGA compute one stochastic gradient per update step unlike q-SAGA 
which is included here not as a practically relevant algorithm  but as an indication of potential im-
provements that could be achieved by fresher corrections. A step size γ = q
µn was used everywhere 
except for “plain SGD”. Note that as K (cid:28) 1 in all cases  this is close to the optimal value suggested
by our analysis; moreover  using a step size of ∼ 1
L for SAGA as suggested in previous work [9]
did not appear to give better results. For plain SGD  we used a schedule of the form γt = γ0/t with
constants optimized coarsely via cross-validation. The x-axis is expressed in units of n (suggestively
called ”epochs”).
SAGA vs. SGD cst As we can see  if we run SGD with the same constant step size as SAGA 
it takes several epochs until SAGA really shows a signiﬁcant gain. The constant step-size variant
of SGD is faster in the early stages until it converges to a neighborhood of the optimum  where
individual runs start showing a very noisy behavior.
SAGA vs. q-SAGA q-SAGA outperforms plain SAGA quite consistently when counting stochas-
tic update steps. This establishes optimistic reference curves of what we can expect to achieve with
N -SAGA. The actual speed-up is somewhat data set dependent.
N -SAGA vs. SAGA and q-SAGA N -SAGA with sufﬁciently small  can realize much of the
possible freshness gains of q-SAGA and performs very similar for a few (2-10) epochs  where it
traces nicely between the SAGA and q-SAGA curves. We see solid speed-ups on all three datasets
for both µ = 0.1 and µ = 0.001.
It should be clearly stated that running N -SAGA at a ﬁxed  for longer will not
Asymptotics
result in good asymptotics on the empirical risk. This is because  as theory predicts  N -SAGA
can not drive the suboptimality to zero  but rather levels-off at a point determined by .
In our
experiments  the cross-over point with SAGA was typically after 5 − 15 epochs. Note that the gains
in the ﬁrst epochs can be signiﬁcant  though. In practice  one will either deﬁne a desired accuracy
level and choose  accordingly or one will switch to SAGA for accurate convergence.

5 Conclusion

n) and large (∼ 1

We have generalized variance reduced SGD methods under the name of memorization algorithms
and presented a corresponding analysis  which commonly applies to all such methods. We have
investigated in detail the range of safe step sizes with their corresponding geometric rates as guar-
anteed by our theory. This has delivered a number of new insights  for instance about the trade-offs
between small (∼ 1
4L ) step sizes in different regimes as well as about the role of
the freshness of stochastic gradients evaluated at past iterates.
We have also investigated and quantiﬁed the effect of additional errors in the variance correction
terms on the convergence behavior. Dependent on how µ scales with n  we have shown that such
errors can be tolerated  yet  for small µ  may have a negative effect on the convergence rate as much
smaller step sizes are needed to still guarantee convergence to a small region. We believe this result
to be relevant for a number of approximation techniques in the context of variance reduced SGD.
Motivated by these insights and results of our analysis  we have proposed N -SAGA  a modiﬁcation
of SAGA that exploits similarities between training data points by deﬁning a neighborhood system.
Approximate versions of per-data point gradients are then computed by sharing information among
neighbors. This opens-up the possibility of variance-reduction in a streaming data setting  where
each data point is only seen once. We believe this to be a promising direction for future work.
Empirically  we have been able to achieve consistent speed-ups for the initial phase of regularized
risk minimization. This shows that approximate computations of variance correction terms consti-
tutes a promising approach of trading-off computation with solution accuracy.

Acknowledgments We would like to thank Yannic Kilcher  Martin Jaggi  R´emi Leblond and the
anonymous reviewers for helpful suggestions and corrections.

8

References
[1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in

high dimensions. Commun. ACM  51(1):117–122  2008.

[2] L. Bottou. Large-scale machine learning with stochastic gradient descent.

pages 177–186. Springer  2010.

In COMPSTAT 

[3] S. Dasgupta and K. Sinha. Randomized partition trees for nearest neighbor search. Algorith-

mica  72(1):237–263  2015.

[4] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with
In Advances in Neural Information

support for non-strongly convex composite objectives.
Processing Systems  pages 1646–1654  2014.

[5] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  pages 315–323  2013.

[6] J. Koneˇcn`y and P. Richt´arik. Semi-stochastic gradient descent methods. arXiv preprint

arXiv:1312.1666  2013.

[7] H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical

statistics  pages 400–407  1951.

[8] M. Schmidt. Convergence rate of stochastic gradient with constant step size. UBC Technical

Report  2014.

[9] M. Schmidt  N. L. Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average

gradient. arXiv preprint arXiv:1309.2388  2013.

[10] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient

solver for SVM. Mathematical programming  127(1):3–30  2011.

[11] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. The Journal of Machine Learning Research  14(1):567–599  2013.

9

,Thomas Hofmann
Aurelien Lucchi
Simon Lacoste-Julien
Brian McWilliams