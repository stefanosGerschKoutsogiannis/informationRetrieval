2015,A Dual Augmented Block Minimization Framework for Learning with Limited Memory,In past few years  several techniques have been proposed for training of linear Support Vector Machine (SVM) in limited-memory setting  where a dual block-coordinate descent (dual-BCD) method was used to balance cost spent on I/O and computation. In this paper  we consider the more general setting of regularized \emph{Empirical Risk Minimization (ERM)} when data cannot fit into memory. In particular  we generalize the existing block minimization framework based on strong duality and \emph{Augmented Lagrangian} technique to achieve global convergence for ERM with arbitrary convex loss function and regularizer. The block minimization framework is flexible in the sense that  given a solver working under sufficient memory  one can integrate it with the framework to obtain a solver globally convergent under limited-memory condition. We conduct experiments on L1-regularized classification and regression problems to corroborate our convergence theory and compare the proposed framework to  algorithms adopted from online and distributed settings  which shows superiority of the proposed approach on data of size ten times larger than the memory capacity.,A Dual-Augmented Block Minimization Framework

for Learning with Limited Memory

Ian E.H. Yen ∗
∗ ianyen@cs.utexas.edu

∗ University of Texas at Austin

Shan-Wei Lin †
{r03922067 sdlin}@csie.ntu.edu.tw

† National Taiwan University

Shou-De Lin †

Abstract

In past few years  several techniques have been proposed for training of linear
Support Vector Machine (SVM) in limited-memory setting  where a dual block-
coordinate descent (dual-BCD) method was used to balance cost spent on I/O and
computation. In this paper  we consider the more general setting of regularized
Empirical Risk Minimization (ERM) when data cannot ﬁt into memory. In par-
ticular  we generalize the existing block minimization framework based on strong
duality and Augmented Lagrangian technique to achieve global convergence for
general convex ERM. The block minimization framework is ﬂexible in the sense
that  given a solver working under sufﬁcient memory  one can integrate it with
the framework to obtain a solver globally convergent under limited-memory con-
dition. We conduct experiments on L1-regularized classiﬁcation and regression
problems to corroborate our convergence theory and compare the proposed frame-
work to algorithms adopted from online and distributed settings  which shows su-
periority of the proposed approach on data of size ten times larger than the memory
capacity.

1

Introduction

Nowadays data of huge scale are prevalent in many applications of statistical learning and data
mining.
It has been argued that model performance can be boosted by increasing both number
of samples and features  and through crowdsourcing technology  annotated samples of terabytes
storage size can be generated [3]. As a result  the performance of model is no longer limited by the
sample size but the amount of available computational resources. In other words  the data size can
easily go beyond the size of physical memory of available machines. Under this setting  most of
learning algorithms become slow due to expensive I/O from secondary storage device [26].
When it comes to huge-scale data  two settings are often considered — online and distributed learn-
ing. In the online setting  each sample is processed only once without storage  while in the dis-
tributed setting  one has several machines that can jointly ﬁt the data into memory. However  the
real cases are often not as extreme as these two — there are usually machines that can ﬁt part of the
data  but not all of them. In this setting  an algorithm can only process a block of data at a time.
Therefore  balancing the time spent on I/O and computation becomes the key issue [26]. Although
one can employ an online-fashioned learning algorithm in this setting  it has been observed that on-
line method requires large number of epoches to achieve comparable performance to batch method 
and at each epoch it spends most of time on I/O instead of computation [2  21  26]. The situation
for online method could become worse for problem of non-smooth  non-strongly convex objective
function  where a qualitatively slower convergence of online method is exhibited [15  16] than that
proved for strongly-convex problem like SVM [14].
In the past few years  several algorithms have been proposed to solve large-scale linear Support Vec-
tor Machine (SVM) in the limited memory setting [2  21  26]. These approaches are based on a dual

1

Block Coordinate Descent (dual-BCD) algorithim  which decomposes the original problem into a
series of block sub-problems  each of them requires only a block of data loaded into memory. The
approach was proved linearly convergent to the global optimum  and demonstrated fast convergence
empirically. However  the convergence of the algorithm relies on the assumption of a smooth dual
problem  which  as we show  does not hold generally for other regularized Empirical Risk Mini-
mizaton (ERM) problem. As a result  although the dual-BCD approach can be extended to the more
general setting  it is not globally convergent except for a class of problems with L2-regularizer.
In this paper  we ﬁrst show how to adapt the dual block-coordinate descnet method of [2  26] to
the general setting of regularized Empirical Risk Mimization (ERM)  which subsumes most of su-
pervised learning problems ranging from classiﬁcation  regression to ranking and recommendation.
Then we discuss the convergence issue arises when the underlying ERM is not strongly-convex. A
Primal Proximal Point ( or Dual Augmented Lagrangian ) method is then proposed to address this
issue  which as we show  results in a block minimization algorithm with global convergence to op-
timum for convex regularized ERM problems. The framework is ﬂexible in the sense that  given a
solver working under sufﬁcient-memory condition  it can be integrated into the block minimization
framework to obtain a solver globally convergent under limited-memory condition.
We conduct experiments on L1-regularized classiﬁcation and regression problems to corroborate
our convergence theory  which shows that the proposed simple dual-augmented technique changes
the convergence behavior dramatically. We also compare the proposed framework to algorithms
adopted from online and distributed settings. In particular  we describe how to adapt a distributed op-
timization framework — Alternating Direction Method of Multiplier (ADMM) [1] — to the limited-
memory setting  and show that  although the adapted algorithm is effective  it is not as efﬁcient as the
proposed framework specially designed for limited-memory setting. Note our experiment does not
adapt into comparison some recently proposed distributed learning algorithms (CoCoA etc.) [7  10]
that only apply to ERM with L2-regularizer or some other distributed method designed for some
speciﬁc loss function [19].

2 Problem Setup

In this work  we consider the regularized Empirical Risk Minimization problem  which given a data
set D = {(Φn  yn)}N

n=1  estimates a model through

N(cid:88)
F (w  ξ) = R(w) +
Φnw = ξn  n ∈ [N ]

n=1

min

w∈Rd ξn∈Rp
s.t.

Ln(ξn)

(1)

as the logistic loss Ln(ξ) = log((cid:80)

where w ∈ Rd is the model parameter to be estimated  Φn is a p by d design matrix that encodes
features of the n-th data sample  Ln(ξn) is a convex loss function that penalizes the discrepancy
between ground truth and prediction vector ξn ∈ Rp  and R(w) is a convex regularization term
penalizing model complexity.
The formulation (1) subsumes a large class of statistical learning problems ranging from classiﬁ-
cation [27]  regression [17]  ranking [8]  and convex clustering [24]. For example  in classiﬁcation
problem  we have p = |Y| where Y consists of the set of all possible labels and Ln(ξ) can be deﬁned
k∈Y exp(ξk)) − ξyn as in logistic regression or the hinge loss
Ln(ξ) = maxk∈Y (1− δk yn + ξk − ξyn) as used in support vector machine; in a (multi-task) regres-
sion problem  the target variable consists of K real values Y = RK  the prediction vector has p = K
dimensions  and a square loss Ln(ξ) = 1
2 is often used. There are also a variety of regular-
2(cid:107)w(cid:107)2
izers R(w) employed in different applications  which includes the L2-regularizer R(w) = λ
in ridge regression  L1-regularizer R(w) = λ(cid:107)w(cid:107)1 in Lasso  nuclear-norm R(w) = λ(cid:107)w(cid:107)∗ in
matrix completion  and a family of structured group norms R(w) = λ(cid:107)w(cid:107)G [11]. Although the
speciﬁc form of Ln(ξ)  R(w) does not affect the implementation of the limited-memory training
procedure  two properties of the functions — strong convexity and smoothness — have key effects
on the behavior of the block minimization algorithm.

2(cid:107)ξ−yn(cid:107)2

2

N(cid:88)

R∗(−µ) +

L∗
n(αn)

min

µ∈Rd αn∈Rp

s.t.

N(cid:88)

n=1

n=1

ΦT

n αn = µ

(4)

m
2

M
2

Deﬁnition 1 (Strong Convexity). A function f (x) is strongly convex iff it is lower bounded by a
simple quadratic function

f (y) ≥ f (x) + ∇f (x)T (y − x) +

(cid:107)x − y(cid:107)2

(2)

for some constant m > 0 and ∀x  y ∈ dom(f ).
Deﬁnition 2 (Smoothness). A function f (x) is smooth iff it is upper bounded by a simple quadratic
function

f (y) ≤ f (x) + ∇f (x)T (y − x) +

(cid:107)x − y(cid:107)2

(3)

for some constant 0 ≤ M < ∞ and ∀x  y ∈ dom(f ).
For instance  the square loss and logistic loss are both smooth and strongly convex 1  while the hinge-
loss satisﬁes neither of them. On the other hand  most of regularizers such as L1-norm  structured
group norm  and nuclear norm are neither smooth nor strongly convex  except for the L2-regularizer 
which satiﬁes both. In the following we will demonstrate the effects of these properties to Block
Minimization algorithms.
Throughout this paper  we will assume that a solver for (1) that works in sufﬁcient-memory condition
is given  and our task is to design an algorithmic framework that integrates with the solver to efﬁ-
ciently solve (1) when data cannot ﬁt into memory. We will assume  however  that the d-dimensional
parameter vector w can be ﬁt into memory.

3 Dual Block Minimization

In this section  we extend the block minimization framework of [26] from linear SVM to the general
setting of regularized ERM (1).The dual of (1) can be expressed as

where R∗(−µ) is the convex conjugate of R(w) and L∗
n(αn) is the convex conjugate of Ln(ξn).
The block minimization algorithm of [26] basically performs a dual Block-Coordinate Descent
(dual-BCD) over (4) by dividing the whole data set D into K blocks DB1  ... DBK   and op-
timizing a block of dual variables (αBk   µ) at a time  where DBk = {(Φn  yn)}n∈Bk and
αBk = {αn|n ∈ Bk}.
In [26]  the dual problem (4) is derived explicitly in order to perform the algorithm. However 
for many sparsity-inducing regularizer such as L1-norm and nuclear norm  it is more efﬁcient and
convenient to solve (1) in the primal [6  28]. Therefore  here instead of explicitly forming the dual
problem  we express it implicitly as

L(α  w  ξ) 

(cid:27)

(cid:26)

(cid:26)

(5)
where L(α  w  ξ) is the Lagrangian function of (1)  and maximize (5) w.r.t. a block of variables
αBk from the primal instead of dual by strong duality

G(α) = min
w ξ

L(α  w  ξ)

L(α  w  ξ)

max
αBk

(6)
with other dual variables {αBj = αt
}j(cid:54)=k ﬁxed. The maximization of dual variables αBk in (6)
then enforces the primal equalities Φnw = ξn  n ∈ Bk  which results in the block minimization
problem

= min
w ξ

max
αBk

min
w ξ

Bj

(cid:27)

(cid:88)

min

w∈Rd ξn∈Rp

s.t.

R(w) +
Φnw = ξn  n ∈ Bk 

n∈Bk

Ln(ξn) + µtT
Bk

w

(7)

1The logistic loss is strongly convex when its input ξ are within a bounded range  which is true as long as

we have a non-zero regularizer R(w).

3

=(cid:80)

Bk

ΦT

n αt

n /∈Bk

n. Note that  in (7)  variables {ξn}n /∈Bk have been dropped since they
where µt
 
are not relevant to the block of dual variables αBk  and thus given the d dimensional vector µt
one can solve (7) without accessing data {(Φn  yn)}n /∈Bk outside the block Bk. Throughout the
B via

dual-BCD algorithm  we maintain d-dimensional vector µt =(cid:80)N

n and compute µt

n=1 ΦT

n αt

Bk

ΦT

n αt
n

(8)

B = µt − (cid:88)

µt

n∈Bk

in the beginning of solving each block subproblem (7). Since subproblem (7) is of the same form to
the original problem (1) except for one additional linear augmented term µT
w  one can adapt the
Bk
solver of (1) to solve (7) easily by providing an augmented version of the gradient

∇w ¯F (w  ξ) = ∇wF (w  ξ) + µt

Bk

to the solver  where ¯F (.) denotes the function with augmented terms and F (.) denotes the function
is constant and separable w.r.t. coordi-
without augmented terms. Note the augmented term µt
nates  so it adds little overhead to the solver. After obtaining solution (w∗  ξ
) from (7)  we can
derive the corresponding optimal dual variables αBk for (6) according to the KKT condition and
maintain µ subsequently by

∗
Bk

Bk

(cid:88)
n = ∇ξn Ln(ξ
n)  n ∈ Bk
∗
αt+1
n αt+1
n .

µt+1 = µt

ΦT

+

Bk

n∈Bk

(9)
(10)

The procedure is summarized in Algorithm 1  which requires a total memory capacity of O(d +
|DBk| + p|Bk|). The factor d comes from the storage of µt  wt  factor |DBk| comes from the
storage of data block  and the factor p|Bk| comes from the storage of αBk. Note this requires the
same space complexity as that required in the original algorithm proposed for linear SVM [26] 
where p = 1 for the binary classiﬁcation setting.

4 Dual-Augmented Block Minimization

dual

objective

R∗(−(cid:80)N

n αn) + (cid:80)N

The Block Minimization Algorithm 1  though can be applied to the general regularized ERM prob-
lem (1)  it is not guaranteed that the sequence {αt}∞
t=0 produced by Algorithm 1 converges to global
optimum of (1). In fact  the global convergence of Algorithm 1 only happens for some special cases.
One sufﬁcient condition for the global convergence of a Block-Coordinate Descent algorithm is that
the terms in objective function that are not separable w.r.t. blocks must be smooth (Deﬁnition 2).
The
terms
n=1 
and thus is also separable w.r.t. {αBk}K
k=1  while the ﬁrst term couples variables αB1  ...  αBK
involving all the blocks. As a result  if R∗(−µ) is a smooth function according to Deﬁnition 2  then
Algorithm 1 has global convergence to the optimum. However  the following theorem states this is
true only when R(w) is strongly convex.
Theorem 1 (Strong/Smooth Duality). Assume f (.) is closed and convex. Then f (.) is smooth with
parameter M if and only if its convex conjugate f∗(.) is strongly convex with parameter m = 1
M .

n(αn)  where second term is separable w.r.t.

two
to {αn}N

function
n=1 L∗

(expressed

comprises

only α)

n=1 ΦT

using

(4)

A proof of above theorem can be found in [9]. According to Theorem 1  the Block Minimization
Algorithm 1 is not globally convergent if R(w) is not strongly convex  which however  is the case
for most of regularizers other than the L2-norm R(w) = 1
In this section  we propose a remedy to this problem  which by a Dual-Augmented Lagrangian
method (or equivalently  Primal Proximal Point method)  creates a dual objective function of desired
property that iteratively approaches the original objective (1)  and results in fast global convergence
of the dual-BCD approach.

2(cid:107)w(cid:107)2  as discussed in Section 2.

4

Algorithm 1 Dual Block Minimization

1. Split data D into blocks B1  B2  ...  BK.
2. Initialize µ0 = 0.
for t = 0  1  ... do

Algorithm 2 Dual-Aug. Block Minimization
1. Split data D into blocks B1  B2  ...  BK.
2. Initialize w0 = 0  µ0 = 0.
for t = 0  1  ... (outer iteration) do

into memory.

Bk

3.1. Draw k uniformly from [K].
3.2. Load DBk and αt
3.3. Compute µt
from (8).
3.4. Solve (7) to obtain (w∗  ξ
3.5. Compute αt+1
Bk
3.6. Maintain µt+1 through (10).
3.7. Save αt+1
Bk

out of memory.

by (9).

∗
Bk

Bk

).

end for

4.1 Algorithm

into memory.

for s = 0  1  ...  S do
3.1.1. Draw k uniformly from [K].
3.1.2. Load DBk  αs
Bk
3.1.3. Compute µs
from (15).
3.1.4. Solve (14) to obtain (w∗  ξ
3.1.5. Compute αs+1
Bk
3.1.6. Maintain µs+1 through (17).
3.1.7. Save αs+1
Bk

out of memory.

by (16).

∗
Bk

Bk

).

end for
3.2. wt+1 = w∗(αS).

end for

The Dual Augmented Lagrangian (DAL) method (or equivalently  Proximal Point Method) modiﬁes
the original problem by introducing a sequence of Proximal Maps

wt+1 = arg min

F (w) +

w

(cid:107)w − wt(cid:107)2 

1
2ηt

(11)

where F (w) denotes the ERM problem (1) Under this simple modiﬁcation  instead of doing Block-
Coordinate Descent in the dual of original problem (1)  we perform Dual-BCD on the proximal sub-
problem (11). As we show in next section  the dual formulation of (11) has the required property
for global convergence of the Dual BCD algorithm — all terms involving more than one block of
variables αBk are smooth. Given the current iterate wt  the Dual-Augmented Block Minimization
algorithm optimizes the dual of proximal-point problem (11) w.r.t. one block of variables αBk at a
time  keeping others ﬁxed {αBj = α(t s)

}j(cid:54)=k:

Bj

L(w  ξ  α) = min

max
αBk

w ξ

L(w  ξ  α)

(12)

min
w ξ
where L(.) is the Lagrangian of (11)

max
αBk

L(w  ξ  α) = F (w  ξ) +

(13)
Once again  the maximization w.r.t. αBk in (12) enforces the equalities Φnw = ξn  n ∈ Bk and
thus leads to a primal sub-problem involving only data in block Bk:

n=1

n (Φnw − ξn) +
αT

(cid:107)w − wt(cid:107)2.

1
2ηt

N(cid:88)
(cid:88)

min

w∈Rd ξn∈Rp

s.t.

= (cid:80)

Ln(ξn) + µ(t s)T

Bk

R(w) +
Φnw = ξn  n ∈ Bk 

n∈Bk

where µ(t s)
. Note that (14) is almost the same as (7) except that it has a
Bk
proximal-point augmented term. Therefore  one can follow the same procedure as in Algorithm 1 to

n α(t s)
ΦT

n

maintain the vector µ(t s) =(cid:80)N

n /∈Bk

w +

1
2ηt

(cid:107)w − wt(cid:107)2

(14)

before solving each block subproblem (14). After obtaining solution (w∗  ξ
dual variables αBk as

and maintain µ subsequently as

n α(t s)

= µ(t s) − (cid:88)

n

and computes

n=1 ΦT
µ(t s)
Bk

ΦT

n α(t s)

n

n∈Bk

α(t s+1)

n

= ∇ξn Ln(ξ
n)  n ∈ Bk.
∗
(cid:88)

µ(t s+1) = µ(t s)
Bk

+

ΦT

n α(t s+1)

n

.

n∈Bk

5

(15)

∗
Bk

) from (14)  we update

(16)

(17)

The sub-problem (14) is of similar form to the original ERM problem (1). Since the augmented
term is a simple quadratic function separable w.r.t. each coordinate  given a solver for (1) working
in sufﬁcient-memory condition  one can easily adapt it by modifying

∇w ¯F (w  ξ) = ∇wF (w  ξ) + µt
∇2

Bk
wF (w  ξ) + I/ηt 

¯F (w  ξ) = ∇2

w

+ (w − wt)/ηt

where ¯F (.) denotes the function with augmented terms and F (.) denotes the function without aug-
mented terms. The Block Minimization procedure is repeated until every sub-problem (14) reaches
a tolerance in. Then the proximal point method update wt+1 = w∗(α(t s)) is performed  where
w∗(α(t s)) is the solution of (14) for the latest dual iterate α(t s). The resulting algorithm is sum-
marized in Algorithm 2.

4.2 Analysis

In this section  we analyze the convergence rate of Algorithm 2 to the optimum of (1). First  we
show that the proximal-point formulation (11) has a dual problem with desired property for the
global convergence of Block-Coordinate Descent. In particular  since the dual of (11) takes the form

˜R∗(− N(cid:88)

n=1

min
αn∈Rp

N(cid:88)

n=1

ΦT

n αn) +

L∗
n(αn)

(18)

2ηt

(cid:107)w−wt(cid:107)2  and since ˜R(w) is strongly
where ˜R∗(.) is the convex conjugate of ˜R(w) = R(w)+ 1
convex with parameter m = 1/ηt  the convex conjugate ˜R∗(.) is smooth with parameter M = ηt
according to Theorem 1. Therefore  (18) is in the composite form of a convex  smooth function plus
a convex  block-separable function. This type of function has been widely studied in the literature
of Block-Coordinate Descent [13]. In particular  one can show that a Block-Coordinate Descent
applied on (18) has global convergence to optimum with a fast rate by the following theorem.
Theorem 2 (BCD Convergence). Let the sequence {αs}∞
s=1 be the iterates produced by Block
Coordinate Descent in the inner loop of Algorithm 2  and K be the number of blocks. Denote ˜F ∗(α)
opt the optimal value of (18). Then with probability 1−ρ 
as the dual objective function of (18) and ˜F ∗

˜F ∗(αs) − ˜F ∗

opt ≤  

for s ≥ βK log(

˜F ∗(α0) − ˜F ∗

opt

ρ

)

(19)

for some constant β > 0 if (i) Ln(.) is smooth  or (ii) Ln(.) is polyhedral function and R(.) is also
polyhedral or smooth. Otherwise  for any convex Ln(.)  R(.) we have

˜F ∗(αs) − ˜F ∗

opt ≤  

for s ≥ cK


log(

˜F ∗(α0) − ˜F ∗

opt

ρ

)

(20)

for some constant c > 0.

Note the above analysis (in appendix) does not assume exact solution of each block subproblem.
Instead  it only assumes each block minimization step leads to a dual ascent amount proportional to
that produced by a single (dual) proximal gradient ascent step on the block of dual variables. For
the outer loop of Primal Proximal-Point (or Dual Augmented Lagrangian) iterates (11)  we show the
following convergence theorem.
Theorem 3 (Proximal Point Convergence). Let F (w) be objective of the regularized ERM problem
(1)  and R = maxv maxw{(cid:107)v − w(cid:107) : F (w) ≤ F (w0)  F (v) ≤ F (w0)} be the radius of initial
level set. The sequence {wt}∞

t=1 produced by the Proximal-Point update (11) with ηt = η has
F (wt+1) − Fopt ≤  

for t ≥ τ log(

).

(21)

ω


for some constant τ  ω > 0 if both Ln(.) and R(.) are (i) strictly convex and smooth or (ii) polyhe-
dral. Otherwise  for any convex F (w) we have

F (wt+1) − Fopt ≤ R2/(2ηt).

6

The following theorem further shows that solving sub-problem (11) inexactly with tolerance /t
sufﬁces for convergence to  overall precision  where t is the number of outer iterations required.
Theorem 4 (Inexact Proximal Map). Suppose  for a given dual iterate wt  each sub-problem (11)
is solved inexactly s.t. the solution ˆwt+1 has

(22)
t=1 be the sequence of iterates produced by inexact proximal updates and {wt}∞

Then let { ˆwt}∞
as that generated by exact updates. After t iterations  we have

(cid:107) ˆwt+1 − proxηtF (wt)(cid:107) ≤ 0.

(cid:107) ˆwt − wt(cid:107) ≤ t0.

t=1

(23)

Note for Ln(.)  R(.) being strictly convex and smooth  or polyhedral  t is of order O(log(1/)) 
and thus it only requires O(K log(1/) log(t/)) = O(K log2(1/)) overall number of block min-
imization steps to achieve  suboptimality. Otherwise  as long as Ln(.) is smooth  for any convex
regularizer R(.)  t is of order O(1/)  so it requires O(K(1/) log(t/)) = O( K log(1/)
) total
number of block minimization steps.
4.3 Practical Issues



(cid:80)

4.3.1 Solving Sub-Problem Inexactly
While the analysis in Section 4.2 assumes exact solution of subproblems  in practice  the Block
Minimization framework does not require solving subproblem (11)  (14) exactly. In our experiments 
it sufﬁces for the fast convergence of proximal-point update (11) to solve subproblem (14) for only a
single pass of all blocks of variables αB1 ...  αBK   and limit the number of iterations the designated
solver spends on each subproblem (7)  (14) to be no more than some parameter Tmax.
4.3.2 Random Selection w/o Replacement
In Algorithm 1 and 2  the block to be optimized is chosen uniformly at random from k ∈ {1  ...  K} 
which eases the analysis for proving a better convergence rate [13]. However  in practice  to avoid
unbalanced update frequency among blocks  we do random sampling without replacement for both
Algorithm 1 and 2  that is  for every K iterations  we generate a random permutation π1  ...  πK of
block index 1  ..  K and optimize block subproblems (7)  (14) according to the order π1  ..  πK. This
also eases the checking of inner-loop stopping condition.
4.3.3 Storage of Dual Variables
Both the algorithms 1 and 2 need to store the dual variables αBk into memory and load/save them
from/to some secondary storage units  which requires a time linear to p|Bk|. For some problems 
such as multi-label classiﬁcation with large number of labels or structured prediction with large
number of factors  this can be very expensive. In this situation  one can instead maintain µ ¯Bk
=
n αn = µ − µBk directly. Note µ ¯Bk has I/O and storage cost linear to d  which can be
ΦT
much smaller than p|Bk| in a low-dimensional problem.
5 Experiment
In this section  we compare the proposed Dual Augmented Block Minimization framework (Algo-
rithm 2) to the vanilla Dual Block Coordinate Descent algorithm [26] and methods adopted from
Online and Distributed Learning. The experiments are conducted on the problem of L1-regularized
L2-loss SVM [27] and the (Lasso) (L1-regularized Regression) problem [17] in the limited-memory
setting with data size 10 times larger than the available memory. For both problems  we use state-
of-the-art randomized coordinate descent method [13  27] as the solver for solving sub-problems
(7)  (14)  (59)  (63)  and we set parameter ηt = 1  λ = 1 (of L1-regularizer) for all experiments.
Four public benchmark data sets are used— webspam  rcv1-binary for classiﬁcation and year-pred 
E2006 for regression  which can be obtained from the LIBSVM data set collections. For year-pred
and E2006  the features are generated from Random Fourier Features [12  23] that approximate the
effect of Gaussian RBF kernel. Table 1 summarizes the data statistics. The algorithms in compar-
ison and their shorthands are listed below  where all solvers are implemented in C/C++ and run on
64-bit machine with 2.83GHz Intel(R) Xeon(R) CPU. We constrained the process to use no more
than 1/10 of memory required to store the whole data.
• OnlineMD: Stochastic Mirror Descent method specially designed for L1-regularized prob-

lem proposed in [15] with step size chosen from 10−2-102 for best performance.

n∈Bk

7

Table 1: Data Statistics: Summary of data statistics when stored using sparse format. The last two
columns specify memory consumption in (MB) of the whole data and that of a block when data is
split into K = 10 partitions.

#test
31 500
20 242
51 630
3 308

dimension
680 714
7 951 176

2 000
30 000

Data

rcv1

#train
webspam 315 000
202 420
463 715
16 087

year-pred
E2006

#non-zeros Memory Block
2 068
1 174 704 031
1 201
656 977 694
1 370
927 893 715
8 088 636
809

20 679
12 009
13 702
8 088

Figure 1: Relative function value difference to the optimum and Testing RMSE (Accuracy) on
LASSO (top) and L1-regularized L2-SVM (bottom). (RMSE best for year-pred: 9.1320; for E2006:
0.4430)  (Accuracy best for for webspam: 0.4761%; best for rcv1: 2.213%).

• D-BCD2: Dual Block-Coordinate Descent method (Algorithm 1).
• DA-BCD: Dual-Augmented Block Minimization (Algorithm 2).
• ADMM: ADMM for limited-memory learning (Algorithm 3 in appendix-B).
• BC-ADMM: Block-Coordinate ADMM that updates a randomly chosen block of dual vari-

ables at a time for limited-memory learning (Algorithm 4 in appendix-B) .

We use wall clock time that includes both I/O and computation as measure for training time in all
experiments. In Figure 5  three measures are plotted versus the training time: Relative objective
function difference to the optimum  Testing RMSE and Accuracy. Figure 5 shows the results  where
as expected  the dual Block Coordinate Descent (D-BCD) method without augmentation cannot im-
prove the objective after certain number of iterations. However  with extremely simple modiﬁcation 
the Dual-Augmented Block Minimization (DA-BCD) algorithm becomes not only globally conver-
gent but with a rate several times faster than other approaches. Among all methods  the convergence
of Online Mirror Descent (SMIDAS) is signiﬁcantly slower  which is expected since (i) the online
Mirror Descent on a non-smooth  non-strongly convex function converges at a rate qualitatively
slower than the linear convergence rate of DA-BCD and ADMM [15  16]  and (ii) Online method
does not utilize the available memory capacity and thus spends unbalanced time on I/O and com-
putation. For methods adopted from distributed optimization  the experiment shows BC-ADMM
consistently  but only slightly  improves ADMM  and both of them converge much slower than the
DA-BCD approach  presumably due to the conservative updates on the dual variables.
Acknowledgement We thank to the support of Telecommunication Lab.  Chunghwa Telecom Co. 
Ltd via TL-103-8201  AOARD via No. FA2386-13-1-4045  Ministry of Science and Technology 
National Taiwan University and Intel Co. via MOST102-2911-I-002-001  NTU103R7501  102-
2923-E-002-007-MY2  102-2221-E-002-170  103-2221-E-002-104-MY2.

2The objective value obtained from D-BCD ﬂuctuates a lot  in ﬁgures we plot the lowest values achieved by

D-BCD from the beginning to time t.

8

10002000300040005000600010−310−2timeobjectiveyear−pred−obj ADMMBC−ADMMDA−BCDD−BCDonlineMD10002000300040005000600010−2timermseyear−rmse ADMMBC−ADMMDA−BCDD−BCDonlineMD10002000300040005000600010−310−210−1100timeobj(cid:13)e2006−obj ADMMBC−ADMMDA−BCDD−BCDonlineMD10002000300040005000600010−1100timeRMSE(cid:13)e2006−rmse ADMMBC−ADMMDA−BCDD−BCDonlineMD2000400060008000100001200010−210−1100101timeobj(cid:13)webspam−obj ADMMBC−ADMMDA−BCDD−BCDonlineMD2000400060008000100001200010−1100101timeerror(cid:13)webspam−error ADMMBC−ADMMDA−BCDD−BCDonlineMD2000400060008000100001200010−210−1100101timeobj(cid:13)rcv1−obj ADMMBC−ADMMDA−BCDD−BCDonlineMD20004000600080001000012000100timeerror(cid:13)rcv1−error ADMMBC−ADMMDA−BCDD−BCDonlineMDReferences
[1] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning  2011.
[2] K. Chang and D. Roth. Selective block minimization for faster convergence of limited memory large-scale

linear models. In SIGKDD. ACM  2011.

[3] J. Deng  W. Dong  R. Socher  L. J. Li  K. Li  and L. F. Fei. Imagenet: A large-scale hierarchical image

database. In CVPR  2009.

[4] A. Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the

National Bureau of Standards  1952.

[5] M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers  2012.
[6] C. Hsieh  I. Dhillon  P. Ravikumar  S. Becker  and P. Olsen. Quic & dirty: A quadratic approximation

approach for dirty statistical models. In NIPS  2014.

[7] M. Jaggi  V. Smith  M. Tak´ac  J. Terhorst  S. Krishnan  T. Hofmann  and M. Jordan. Communication-

efﬁcient distributed dual coordinate ascent. In NIPS  2014.

[8] T. Joachims. A support vector method for multivariate performance measures. In ICML  2005.
[9] S. Kakade  S. Shalev-Shwartz  and A. Tewari. Applications of strong convexity–strong smoothness dual-

ity to learning with matrices. CoRR  2009.

[10] C. Ma  V. Smith  M. Jaggi  M. Jordan  P. Richt´arik  and M. Tak´aˇc. Adding vs. averaging in distributed

primal-dual optimization. ICML  2015.

[11] G. Obozinski  L. Jacob  and J. Vert. Group lasso with overlaps: the latent group lasso approach. arXiv

preprint  2011.

[12] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  2007.
[13] P. Richt´arik and M. Tak´aˇc.

Iteration complexity of randomized block-coordinate descent methods for

minimizing a composite function. Mathematical Programming  2014.

[14] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

svm. Mathematical programming  2011.

[15] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1-regularized loss minimization. JMLR  2011.
[16] N. Srebro  K. Sridharan  and A. Tewari. On the universality of online mirror descent. In NIPS  2011.
[17] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

1996.

[18] R. Tomioka  T. Suzuki  and M. Sugiyama. Super-linear convergence of dual augmented lagrangian algo-

rithm for sparsity regularized estimation. JMLR  2011.

[19] I. Troﬁmov and A. Genkin. Distributed coordinate descent for l1-regularized logistic regression. arXiv

preprint  2014.

[20] P. Wang and C. Lin. Iteration complexity of feasible descent methods for convex optimization. JMLR 

2014.

[21] I. Yen  C. Chang  T. Lin  S.  and S. Lin. Indexed block coordinate descent for large-scale linear classiﬁ-

cation with limited memory. In SIGKDD. ACM  2013.

[22] I. Yen  C. Hsieh  P. Ravikumar  and I. Dhillon. Constant nullspace strong convexity and fast convergence

of proximal methods under high-dimensional settings. In NIPS  2014.

[23] I. Yen  T. Lin  S. Lin  P. Ravikumar  and I. Dhillon. Sparse random feature algorithm as coordinate descent

in hilbert space. In NIPS  2014.

[24] I. Yen  X. Lin  K. Zhong  P. Ravikumar  and I. Dhillon. A convex exemplar-based approach to MAD-

Bayes dirichlet process mixture models. In ICML  2015.

[25] I. Yen  K. Zhong  C. Hsieh  P. Ravikumar  and I. Dhillon. Sparse linear programming via primal and dual

augmented coordinate descent. In NIPS  2015.

[26] H. Yu  C. Hsieh  . Chang  and C. Lin. Large linear classiﬁcation when data cannot ﬁt in memory. SIGKDD 

2010.

[27] G. Yuan  K. Chang  C. Hsieh  and C. Lin. A comparison of optimization methods and software for

large-scale L1-regularized linear classiﬁcation. JMLR  2010.

[28] K. Zhong  I. Yen  I. Dhillon  and P. Ravikumar. Proximal quasi-Newton for computationally intensive

l1-regularized m-estimators. In NIPS  2014.

9

,Ian En-Hsu Yen
Shan-Wei Lin
Shou-De Lin
Laurence Aitchison
Lloyd Russell
Adam Packer
Jinyao Yan
Philippe Castonguay
Michael Hausser
Srinivas Turaga