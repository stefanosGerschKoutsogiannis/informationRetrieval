2015,Gradient Estimation Using Stochastic Computation Graphs,In a variety of problems originating in supervised  unsupervised  and reinforcement learning  the loss function is defined by an expectation over a collection of random variables  which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function  using samples  lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs--directed acyclic graphs that include both deterministic functions and conditional probability distributions and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work  along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations  enabling  for example  attention  memory  and control actions.,Gradient Estimation Using

Stochastic Computation Graphs

John Schulman1 2

joschu@eecs.berkeley.edu

Nicolas Heess1

heess@google.com

Theophane Weber1

theophane@google.com

Pieter Abbeel2

pabbeel@eecs.berkeley.edu

1 Google DeepMind

2 University of California  Berkeley  EECS Department

Abstract

In a variety of problems originating in supervised  unsupervised  and reinforce-
ment learning  the loss function is deﬁned by an expectation over a collection
of random variables  which might be part of a probabilistic model or the exter-
nal world. Estimating the gradient of this loss function  using samples  lies at
the core of gradient-based learning algorithms for these problems. We introduce
the formalism of stochastic computation graphs—directed acyclic graphs that in-
clude both deterministic functions and conditional probability distributions—and
describe how to easily and automatically derive an unbiased estimator of the loss
function’s gradient. The resulting algorithm for computing the gradient estimator
is a simple modiﬁcation of the standard backpropagation algorithm. The generic
scheme we propose uniﬁes estimators derived in variety of prior work  along with
variance-reduction techniques therein. It could assist researchers in developing in-
tricate models involving a combination of stochastic and deterministic operations 
enabling  for example  attention  memory  and control actions.

Introduction

1
The great success of neural networks is due in part to the simplicity of the backpropagation al-
gorithm  which allows one to efﬁciently compute the gradient of any loss function deﬁned as a
composition of differentiable functions. This simplicity has allowed researchers to search in the
space of architectures for those that are both highly expressive and conducive to optimization; yield-
ing  for example  convolutional neural networks in vision [12] and LSTMs for sequence data [9].
However  the backpropagation algorithm is only sufﬁcient when the loss function is a deterministic 
differentiable function of the parameter vector.
A rich class of problems arising throughout machine learning requires optimizing loss functions
that involve an expectation over random variables. Two broad categories of these problems are (1)
likelihood maximization in probabilistic models with latent variables [17  18]  and (2) policy gradi-
ents in reinforcement learning [5  23  26]. Combining ideas from from those two perennial topics 
recent models of attention [15] and memory [29] have used networks that involve a combination of
stochastic and deterministic operations.
In most of these problems  from probabilistic modeling to reinforcement learning  the loss functions
and their gradients are intractable  as they involve either a sum over an exponential number of latent
variable conﬁgurations  or high-dimensional integrals that have no analytic solution. Prior work (see
Section 6) has provided problem-speciﬁc derivations of Monte-Carlo gradient estimators  however 
to our knowledge  no previous work addresses the general case.
Appendix C recalls several classic and recent techniques in variational inference [14  10  21] and re-
inforcement learning [23  25  15]  where the loss functions can be straightforwardly described using

1

the formalism of stochastic computation graphs that we introduce. For these examples  the variance-
reduced gradient estimators derived in prior work are special cases of the results in Sections 3 and 4.
The contributions of this work are as follows:
• We introduce a formalism of stochastic computation graphs  and in this general setting  we derive
• We show how this estimator can be computed as the gradient of a certain differentiable function
(which we call the surrogate loss)  hence  it can be computed efﬁciently using the backpropaga-
tion algorithm. This observation enables a practitioner to write an efﬁcient implementation using
automatic differentiation software.

unbiased estimators for the gradient of the expected loss.

tation graphs  generalizing prior work from reinforcement learning and variational inference.

• We describe variance reduction techniques that can be applied to the setting of stochastic compu-
• We brieﬂy describe how to generalize some other optimization techniques to this setting:
majorization-minimization algorithms  by constructing an expression that bounds the loss func-
tion; and quasi-Newton / Hessian-free methods [13]  by computing estimates of Hessian-vector
products.

The main practical result of this article is that to compute the gradient estimator  one just needs
to make a simple modiﬁcation to the backpropagation algorithm  where extra gradient signals are
introduced at the stochastic nodes. Equivalently  the resulting algorithm is just the backpropagation
algorithm  applied to the surrogate loss function  which has extra terms introduced at the stochastic
nodes. The modiﬁed backpropagation algorithm is presented in Section 5.

2 Preliminaries
2.1 Gradient Estimators for a Single Random Variable

This section will discuss computing the gradient of an expectation taken over a single random
variable—the estimators described here will be the building blocks for more complex cases with
multiple variables. Suppose that x is a random variable  f is a function (say  the cost)  and we are
interested in computing @
@✓ Ex [f (x)]. There are a few different ways that the process for generating
x could be parameterized in terms of ✓  which lead to different gradient estimators.
• We might be given a parameterized probability distribution x ⇠ p(·; ✓). In this case  we can use

the score function (SF) estimator [3]:

This classic equation is derived as follows:

@
@✓ Ex [f (x)] =

@

@
@✓

@✓ Ex [f (x)] = Exf (x)
@✓Z dx p(x; ✓)f (x) =Z dx
=Z dx p(x; ✓)

@
@✓

@

log p(x; ✓) .

@
@✓

p(x; ✓)f (x)

log p(x; ✓)f (x) = Exf (x)

@
@✓

log p(x; ✓)

(1)

(2)

(3)

This equation is valid if and only if p(x; ✓) is a continuous function of ✓; however  it does not
need to be a continuous function of x [4].

• x may be a deterministic  differentiable function of ✓ and another random variable z  i.e.  we can

write x(z  ✓). Then  we can use the pathwise derivative (PD) estimator  deﬁned as follows.

@

@✓ Ez [f (x(z  ✓))] = Ez @

@✓

f (x(z  ✓)) .

This equation  which merely swaps the derivative and expectation  is valid if and only if f (x(z  ✓))
is a continuous function of ✓ for all z [4]. 1 That is not true if  for example  f is a step function.
1 Note that for the pathwise derivative estimator  f (x(z  ✓)) merely needs to be a continuous function of
✓—it is sufﬁcient that this function is almost-everywhere differentiable. A similar statement can be made
about p(x; ✓) and the score function estimator. See Glasserman [4] for a detailed discussion of the technical
requirements for these gradient estimators to be valid.

2

• Finally ✓ might appear both in the probability distribution and inside the expectation  e.g.  in

@✓ Ez⇠p(·; ✓) [f (x(z  ✓))]. Then the gradient estimator has two terms:

@

log p(z; ✓)◆f (x(z  ✓)) .

(4)

@

@✓ Ez⇠p(·; ✓) [f (x(z  ✓))] = Ez⇠p(·; ✓) @

@✓

f (x(z  ✓)) +✓ @

@✓

This formula can be derived by writing the expectation as an integral and differentiating  as in
Equation (2).

In some cases  it is possible to reparameterize a probabilistic model—moving ✓ from the distribution
to inside the expectation or vice versa. See [3] for a general discussion  and see [10  21] for a recent
application of this idea to variational inference.
The SF and PD estimators are applicable in different scenarios and have different properties.

1. SF is valid under more permissive mathematical conditions than PD. SF can be used if f is

discontinuous  or if x is a discrete random variable.

2. SF only requires sample values f (x)  whereas PD requires the derivatives f0(x). In the context
of control (reinforcement learning)  SF can be used to obtain unbiased policy gradient estimators
in the “model-free” setting where we have no model of the dynamics  we only have access to
sample trajectories.

3. SF tends to have higher variance than PD  when both estimators are applicable (see for instance
[3  21]). The variance of SF increases (often linearly) with the dimensionality of the sampled
variables. Hence  PD is usually preferable when x is high-dimensional. On the other hand  PD
has high variance if the function f is rough  which occurs in many time-series problems due to
an “exploding gradient problem” / “butterﬂy effect”.

4. PD allows for a deterministic limit  SF does not. This idea is exploited by the deterministic policy

gradient algorithm [22].

Nomenclature. The methods of estimating gradients of expectations have been independently pro-
posed in several different ﬁelds  which use differing terminology. What we call the score function
estimator (via [3]) is alternatively called the likelihood ratio estimator [5] and REINFORCE [26].
We chose this term because the score function is a well-known object in statistics. What we call
the pathwise derivative estimator (from the mathematical ﬁnance literature [4] and reinforcement
learning [16]) is alternatively called inﬁnitesimal perturbation analysis and stochastic backpropa-
gation [21]. We chose this term because pathwise derivative is evocative of propagating a derivative
through a sample path.

2.2 Stochastic Computation Graphs

The results of this article will apply to stochastic computation graphs  which are deﬁned as follows:
Deﬁnition 1 (Stochastic Computation Graph). A directed  acyclic graph  with three types of
nodes:

1. Input nodes  which are set externally  including the parameters we differentiate with

respect to.

2. Deterministic nodes  which are functions of their parents.

3. Stochastic nodes  which are distributed conditionally on their parents.

Each parent v of a non-input node w is connected to it by a directed edge (v  w).

In the subsequent diagrams of this article  we will use circles to denote stochastic nodes and squares
to denote deterministic nodes  as illustrated below. The structure of the graph fully speciﬁes what
estimator we will use: SF  PD  or a combination thereof. This graphical notation is shown below 
along with the single-variable estimators from Section 2.1.

3

✓

Input node

Deterministic node

✓

x

f

✓

z

x

f

Stochastic node

Gives SF estimator

Gives PD estimator

2.3 Simple Examples

Several simple examples that illustrate the stochastic computation graph formalism are shown below.
The gradient estimators can be described by writing the expectations as integrals and differentiating 
as with the simpler estimators from Section 2.1. However  they are also implied by the general
results that we will present in Section 3.

Stochastic Computation Graph

(1)

(2)

(3)

✓

✓

✓

(4)

✓

(5)

✓

x0

x

x

x

x

y

f1

x1

y

y

y

f

f2

x2

f

f

f

Objective

Ey [f (y)]

Ex [f (y(x))]

Ex y [f (y)]

Ex [f (x  y(✓))]

Gradient Estimator

@x
@✓

@
@x

log p(y | x)f (y)

@
@✓

@
@✓

@
@✓

log p(x | ✓)f (y(x))

log p(x | ✓)f (y)

log p(x | ✓)f (x  y(✓)) +

@y
@✓

@f
@y

Ex1 x2 [f1(x1) + f2(x2)]

@
log p(x1 | ✓  x0)(f1(x1) + f2(x2))
@✓
@
log p(x2 | ✓  x1)f2(x2)
@✓

+

Figure 1: Simple stochastic computation graphs

These simple examples illustrate several important motifs  where stochastic and deterministic nodes
are arranged in series or in parallel. For example  note that in (2) the derivative of y does not appear
in the estimator  since the path from ✓ to f is “blocked” by x. Similarly  in (3)  p(y | x) does not
appear (this type of behavior is particularly useful if we only have access to a simulator of a system 
but not access to the actual likelihood function). On the other hand  (4) has a direct path from ✓ to
f  which contributes a term to the gradient estimator. (5) resembles a parameterized Markov reward
process  and it illustrates that we’ll obtain score function terms of the form grad log-probability ⇥
future costs.
The examples above all have one input ✓  but the formal-
ism accommodates models with multiple inputs  for ex-
ample a stochastic neural network with multiple layers of
weights and biases  which may inﬂuence different sub-
sets of the stochastic and cost nodes. See Appendix C
for nontrivial examples with stochastic nodes and multi-
ple inputs. The ﬁgure on the right shows a deterministic
computation graph representing classiﬁcation loss for a two-layer neural network  which has four
parameters (W1  b1  W2  b2) (weights and biases). Of course  this deterministic computation graph
is a special type of stochastic computation graph.

cross-
entropy
loss

soft-
max

y=label

h1

W1

b1

W2

b2

x

h2

4

3 Main Results on Stochastic Computation Graphs
3.1 Gradient Estimators

This section will consider a general stochastic computation graph  in which a certain set of nodes
are designated as costs  and we would like to compute the gradient of the sum of costs with respect
to some input node ✓.
In brief  the main results of this section are as follows:

1. We derive a gradient estimator for an expected sum of costs in a stochastic computation graph.
This estimator contains two parts (1) a score function part  which is a sum of terms grad log-
prob of variable ⇥ sum of costs inﬂuenced by variable; and (2) a pathwise derivative term  that
propagates the dependence through differentiable functions.
2. This gradient estimator can be computed efﬁciently by differentiating an appropriate “surrogate”

objective function.

Let ⇥ denote the set of input nodes  D the set of deterministic nodes  and S the set of stochastic
nodes. Further  we will designate a set of cost nodes C  which are scalar-valued and deterministic.
(Note that there is no loss of generality in assuming that the costs are deterministic—if a cost is
stochastic  we can simply append a deterministic node that applies the identity function to it.) We
will use ✓ to denote an input node (✓ 2 ⇥) that we differentiate with respect to. In the context of
machine learning  we will usually be most concerned with differentiating with respect to a parameter
vector (or tensor)  however  the theory we present does not make any assumptions about what ✓
represents.
For the results that follow  we need to deﬁne the
notion of “inﬂuence”  for which we will introduce
two relations  and D. The relation v  w
(“v inﬂuences w”) means that there exists a se-
quence of nodes a1  a2  . . .   aK  with K  0  such
that (v  a1)  (a1  a2)  . . .   (aK1  aK)  (aK  w) are
edges in the graph. The relation v D w (“v deter-
ministically inﬂuences w”) is deﬁned similarly  ex-
cept that now we require that each ak is a determin-
istic node. For example  in Figure 1  diagram (5)
above  ✓ inﬂuences {x1  x2  f1  f2}  but it only de-
terministically inﬂuences {x1  x2}.
Next  we will establish a condition that is sufﬁcient
for the existence of the gradient. Namely  we will
stipulate that every edge (v  w) with w lying in the
“inﬂuenced” set of ✓ corresponds to a differentiable
dependency: if w is deterministic  then the Jacobian
@v must exist; if w is stochastic  then the probability mass function p(w | v  . . . ) must be differen-
@w
tiable with respect to v.
More formally:

Notation Glossary
⇥: Input nodes
D: Deterministic nodes
S: Stochastic nodes
C: Cost nodes
v  w: v inﬂuences w
v D w: v deterministically inﬂuences w
DEPSv: “dependencies” 
{w 2 ⇥ [ S | w D v}
ˆQv: sum of cost nodes inﬂuenced by v.
ˆv: denotes the sampled value of the node v.

Condition 1 (Differentiability Requirements). Given input node ✓ 2 ⇥  for all edges (v  w)
which satisfy ✓ D v and ✓ D w  then the following condition holds: if w is deterministic 
Jacobian @w
@v exists  and if w is stochastic  then the derivative of the probability mass function
@v p(w | PARENTSw) exists.
@

Note that Condition 1 does not require that all the functions in the graph are differentiable.
If
the path from an input ✓ to deterministic node v is blocked by stochastic nodes  then v may be a
nondifferentiable function of its parents. If a path from input ✓ to stochastic node v is blocked by
other stochastic nodes  the likelihood of v given its parents need not be differentiable; in fact  it does
not need to be known2.

2This fact is particularly important for reinforcement learning  allowing us to compute policy gradient esti-

mates despite having a discontinuous dynamics function or reward function.

5

We need a few more deﬁnitions to state the main theorems. Let DEPSv := {w 2 ⇥ [ S | w D v} 
the “dependencies” of node v  i.e.  the set of nodes that deterministically inﬂuence it. Note the
following:
• If v 2 S  the probability mass function of v is a function of DEPSv  i.e.  we can write p(v| DEPSv).
• If v 2 D  v is a deterministic function of DEPSv  so we can write v(DEPSv).
Let ˆQv := Pcv 

ˆc  i.e.  the sum of costs downstream of node v. These costs will be treated as
constant  ﬁxed to the values obtained during sampling. In general  we will use the hat symbol ˆv to
denote a sample value of variable v  which will be treated as constant in the gradient formulae.
Now we can write down a general expression for the gradient of the expected sum of costs in a
stochastic computation graph:

c2C

Theorem 1.
tions hold:

Suppose that ✓ 2 ⇥ satisﬁes Condition 1. Then the following two equivalent equa-

@

@✓ E"Xc2C

@✓

✓ @

c# = E2664 Xw2S 
= E2664Xc2C
ˆc Xwc 

✓Dw

✓Dw

@
@✓

log p(w | DEPSw)◆ ˆQw + Xc2C✓Dc
log p(w | DEPSw) + Xc2C 

@
@✓

@
@✓

✓Dc

c(DEPSc)3775
c(DEPSc)3775

(5)

(6)

Proof: See Appendix A.
The estimator expressions above have two terms. The ﬁrst term is due to the inﬂuence of ✓ on proba-
bility distributions. The second term is due to the inﬂuence of ✓ on the cost variables through a chain
of differentiable functions. The distribution term involves a sum of gradients times “downstream”
costs. The ﬁrst term in Equation (5) involves a sum of gradients times “downstream” costs  whereas
the ﬁrst term in Equation (6) has a sum of costs times “upstream” gradients.

3.2 Surrogate Loss Functions

The next corollary lets us write down a “surrogate” objective L 
which is a function of the inputs that we can differentiate to obtain
an unbiased gradient estimator.
:= Pw log p(w | DEPSw) ˆQw +
Corollary 1. Let L(⇥ S)
Pc2C c(DEPSc). Then differentiation of L gives us an unbiased gra-
dient estimate: @

@✓ E⇥Pc2C c⇤ = E⇥ @

@✓ L(⇥ S)⇤ .

One practical consequence of this result is that we can apply a stan-
dard automatic differentiation procedure to L to obtain an unbiased
gradient estimator. In other words  we convert the stochastic com-
putation graph into a deterministic computation graph  to which we
can apply the backpropagation algorithm.
There are several alternative ways to deﬁne the surrogate objective
function that give the same gradient as L from Corollary 1. We

Surrogate Loss Computation Graph

(1)

(2)

(3)

(4)

(5)

✓

✓

✓

✓

✓

x

log p(y|x) ˆf

log p(x; ✓) ˆf

log p(x; ✓) ˆf

log p(x; ✓) ˆf

f

y

x0

log p(x1|x0; ✓)
( ˆf1 + ˆf2)

log p(x2|x1; ✓) ˆf2

ˆPv

p( ˆw | DEPSw)

ˆQw+Pc2C c(DEPSc) 
could also write L(⇥ S) :=Pw
where ˆPw is the probability p(w|DEPSw) obtained during sampling 
which is viewed as a constant.
The surrogate objective from Corollary 1 is actually an upper bound
on the true objective in the case that (1) all costs c 2 C are negative 
(2) the the costs are not deterministically inﬂuenced by the parameters ⇥. This construction al-
lows from majorization-minimization algorithms (similar to EM) to be applied to general stochastic
computation graphs. See Appendix B for details.

Figure 2: Deterministic compu-
tation graphs obtained as surro-
gate loss functions of stochas-
tic computation graphs from Fig-
ure 1.

6

3.3 Higher-Order Derivatives.

The gradient estimator for a stochastic computation graph is itself a stochastic computation graph.
Hence  it is possible to compute the gradient yet again (for each component of the gradient vector) 
and get an estimator of the Hessian. For most problems of interest  it is not efﬁcient to compute
this dense Hessian. On the other hand  one can also differentiate the gradient-vector product to get
a Hessian-vector product—this computation is usually not much more expensive than the gradient
computation itself. The Hessian-vector product can be used to implement a quasi-Newton algo-
rithm via the conjugate gradient algorithm [28]. A variant of this technique  called Hessian-free
optimization [13]  has been used to train large neural networks.

@✓ Ex⇠p(·; ✓) [f (x)] = Ex⇠p(·; ✓)⇥ @

4 Variance Reduction
Consider estimating @
@✓ Ex⇠p(·; ✓) [f (x)]. Clearly this expectation is unaffected by subtracting a con-
@✓ Ex⇠p(·; ✓) [f (x)  b]. Taking the score function estimator 
stant b from the integrand  which gives @
@✓ log p(x; ✓)(f (x)  b)⇤. Taking b = Ex [f (x)] gener-
we get @
ally leads to substantial variance reduction—b is often called a baseline3 (see [6] for a more thorough
discussion of baselines and their variance reduction properties).
We can make a general statement for the case of stochastic computation graphs—that we can
add a baseline to every stochastic node  which depends all of the nodes it doesn’t inﬂuence. Let
NONINFLUENCED(v) := {w | v ⌃ w}.
Theorem 2.

✓ @

@✓

log p(v | PARENTSv)◆( ˆQv  b(NONINFLUENCED(v)) + Xc2C⌫✓

@
@✓

c375

@

@✓ E"Xc2C

c# = E264Xv2Sv✓

Proof: See Appendix A.

5 Algorithms
As shown in Section 3  the gradient estimator can be obtained by differentiating a surrogate objective
function L. Hence  this derivative can be computed by performing the backpropagation algorithm
on L. That is likely to be the most practical and efﬁcient method  and can be facilitated by automatic
differentiation software.
Algorithm 1 shows explicitly how to compute the gradient estimator in a backwards pass through
the stochastic computation graph. The algorithm will recursively compute gv := @
every deterministic and input node v.

@v EPc2Cvc

c at

6 Related Work
As discussed in Section 2  the score function and pathwise derivative estimators have been used in a
variety of different ﬁelds  under different names. See [3] for a review of gradient estimation  mostly
from the simulation optimization literature. Glasserman’s textbook provides an extensive treatment
of various gradient estimators and Monte Carlo estimators in general. Griewank and Walther’s
textbook [8] is a comprehensive reference on computation graphs and automatic differentiation (of
deterministic programs.) The notation and nomenclature we use is inspired by Bayes nets and
inﬂuence diagrams [19]. (In fact  a stochastic computation graph is a type of Bayes network; where
the deterministic nodes correspond to degenerate probability distributions.)
The topic of gradient estimation has drawn signiﬁcant recent interest in machine learning. Gradients
for networks with stochastic units was investigated in Bengio et al. [2]  though they are concerned

3The optimal baseline for scalar ✓ is in fact the weighted expectation Ex[f (x)s(x)2]
Ex[s(x)2]

@

@✓ log p(x; ✓).

where s(x) =

7

Algorithm 1 Compute Gradient Estimator for Stochastic Computation Graph

for v 2 Graph do
gv =⇢1dim v
0dim v
end for
Compute ˆQw for all nodes w 2 Graph
for v in REVERSETOPOLOGICALSORT(NONINPUTS) do

if v 2 C
otherwise

. Initialization at output nodes

. Reverse traversal

for w 2 PARENTSv do

if not ISSTOCHASTIC(w) then
if ISSTOCHASTIC(v) then

gw += ( @

gw += ( @v

@w log p(v | PARENTSv)) ˆQw
@w )T gv

else

end if

end if

end for

end for
return [g✓]✓2⇥

with differentiating through individual units and layers; not how to deal with arbitrarily structured
models and loss functions. Kingma and Welling [11] consider a similar framework  although only
with continuous latent variables  and point out that reparameterization can be used to to convert
hierarchical Bayesian models into neural networks  which can then be trained by backpropagation.
The score function method is used to perform variational inference in general models (in the context
of probabilistic programming) in Wingate and Weber [27]  and similarly in Ranganath et al. [20];
both papers mostly focus on mean-ﬁeld approximations without amortized inference. It is used to
train generative models using neural networks with discrete stochastic units in Mnih and Gregor [14]
and Gregor et al. in [7]; both amortize inference by using an inference network.
Generative models with continuous valued latent variables networks are trained (again using an
inference network) with the reparametrization method by Rezende  Mohamed  and Wierstra [21] and
by Kingma and Welling [10]. Rezende et al. also provide a detailed discussion of reparameterization 
including a discussion comparing the variance of the SF and PD estimators.
Bengio  Leonard  and Courville [2] have recently written a paper about gradient estimation in neural
networks with stochastic units or non-differentiable activation functions—including Monte Carlo
estimators and heuristic approximations. The notion that policy gradients can be computed in mul-
tiple ways was pointed out in early work on policy gradients by Williams [26]. However  all of this
prior work deals with speciﬁc structures of the stochastic computation graph and does not address
the general case.

7 Conclusion
We have developed a framework for describing a computation with stochastic and deterministic
operations  called a stochastic computation graph. Given a stochastic computation graph  we can
automatically obtain a gradient estimator  given that the graph satisﬁes the appropriate conditions
on differentiability of the functions at its nodes. The gradient can be computed efﬁciently in a
backwards traversal through the graph: one approach is to apply the standard backpropagation al-
gorithm to one of the surrogate loss functions from Section 3; another approach (which is roughly
equivalent) is to apply a modiﬁed backpropagation procedure shown in Algorithm 1. The results we
have presented are sufﬁciently general to automatically reproduce a variety of gradient estimators
that have been derived in prior work in reinforcement learning and probabilistic modeling  as we
show in Appendix C. We hope that this work will facilitate further development of interesting and
expressive models.

8 Acknowledgements
We would like to thank Shakir Mohamed  Dave Silver  Yuval Tassa  Andriy Mnih  and others at
DeepMind for insightful comments.

8

References
[1] J. Baxter and P. L. Bartlett. Inﬁnite-horizon policy-gradient estimation. Journal of Artiﬁcial Intelligence

Research  pages 319–350  2001.

[2] Y. Bengio  N. L´eonard  and A. Courville. Estimating or propagating gradients through stochastic neurons

for conditional computation. arXiv preprint arXiv:1308.3432  2013.

[3] M. C. Fu. Gradient estimation. Handbooks in operations research and management science  13:575–616 

2006.

[4] P. Glasserman. Monte Carlo methods in ﬁnancial engineering  volume 53. Springer Science & Business

Media  2003.

[5] P. W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM 

33(10):75–84  1990.

[6] E. Greensmith  P. L. Bartlett  and J. Baxter. Variance reduction techniques for gradient estimates in

reinforcement learning. The Journal of Machine Learning Research  5:1471–1530  2004.

[7] K. Gregor  I. Danihelka  A. Mnih  C. Blundell  and D. Wierstra. Deep autoregressive networks. arXiv

preprint arXiv:1310.8499  2013.

[8] A. Griewank and A. Walther. Evaluating derivatives: principles and techniques of algorithmic differenti-

ation. Siam  2008.

[9] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation  9(8):1735–1780  1997.
[10] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv:1312.6114  2013.
[11] D. P. Kingma and M. Welling. Efﬁcient gradient-based inference through transformations between bayes

nets and neural nets. arXiv preprint arXiv:1402.0480  2014.

[12] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86(11):2278–2324  1998.

[13] J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International Con-

ference on Machine Learning (ICML-10)  pages 735–742  2010.

[14] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. arXiv:1402.0030 

2014.

[15] V. Mnih  N. Heess  A. Graves  and K. Kavukcuoglu. Recurrent models of visual attention. In Advances

in Neural Information Processing Systems  pages 2204–2212  2014.

[16] R. Munos. Policy gradient in continuous time. The Journal of Machine Learning Research  7:771–791 

2006.

[17] R. M. Neal. Learning stochastic feedforward networks. Department of Computer Science  University of

Toronto  1990.

[18] R. M. Neal and G. E. Hinton. A view of the em algorithm that justiﬁes incremental  sparse  and other

variants. In Learning in graphical models  pages 355–368. Springer  1998.

[19] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kauf-

mann  2014.

[20] R. Ranganath  S. Gerrish  and D. M. Blei.

arXiv:1401.0118  2013.

Black box variational

inference.

arXiv preprint

[21] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in

deep generative models. arXiv:1401.4082  2014.

[22] D. Silver  G. Lever  N. Heess  T. Degris  D. Wierstra  and M. Riedmiller. Deterministic policy gradient

algorithms. In ICML  2014.

[23] R. S. Sutton  D. A. McAllester  S. P. Singh  Y. Mansour  et al. Policy gradient methods for reinforcement

learning with function approximation. In NIPS  volume 99  pages 1057–1063. Citeseer  1999.

[24] N. Vlassis  M. Toussaint  G. Kontes  and S. Piperidis. Learning model-free robot control by a Monte

Carlo EM algorithm. Autonomous Robots  27(2):123–130  2009.

[25] D. Wierstra  A. F¨orster  J. Peters  and J. Schmidhuber. Recurrent policy gradients. Logic Journal of IGPL 

18(5):620–634  2010.

[26] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.

Machine learning  8(3-4):229–256  1992.

[27] D. Wingate and T. Weber. Automated variational inference in probabilistic programming. arXiv preprint

arXiv:1301.1299  2013.

[28] S. J. Wright and J. Nocedal. Numerical optimization  volume 2. Springer New York  1999.
[29] W. Zaremba and I. Sutskever. Reinforcement learning neural Turing machines.

arXiv preprint

arXiv:1505.00521  2015.

9

,Cho-Jui Hsieh
Si Si
Inderjit Dhillon
John Schulman
Nicolas Heess
Theophane Weber
Pieter Abbeel
Abdul-Saboor Sheikh
Jörg Lücke
Tor Lattimore
Branislav Kveton
Shuai Li
Csaba Szepesvari