2019,Rethinking Kernel Methods for Node Representation Learning on Graphs,Graph kernels are kernel methods measuring graph similarity and serve as a standard tool for graph classification. However  the use of kernel methods for node classification  which is a related problem to graph representation learning  is still ill-posed and the state-of-the-art methods are heavily based on heuristics. Here  we present a novel theoretical kernel-based framework for node classification that can bridge the gap between these two representation learning problems on graphs. Our approach is motivated by graph kernel methodology but extended to learn the node representations capturing the structural information in a graph. We theoretically show that our formulation is as powerful as any positive semidefinite kernels. To efficiently learn the kernel  we propose a novel mechanism for node feature aggregation and a data-driven similarity metric employed during the training phase. More importantly  our framework is flexible and complementary to other graph-based deep learning models  e.g.  Graph  Convolutional Networks (GCNs). We empirically evaluate our approach on a number of standard node classification benchmarks  and demonstrate that our model sets the new state of the art.,Rethinking Kernel Methods for

Node Representation Learning on Graphs

Yu Tian‚àó

Rutgers University

Long Zhao‚àó

Rutgers University

yt219@cs.rutgers.edu

lz311@cs.rutgers.edu

Xi Peng

University of Delaware

xipeng@udel.edu

Dimitris N. Metaxas
Rutgers University

dnm@cs.rutgers.edu

Abstract

Graph kernels are kernel methods measuring graph similarity and serve as a stan-
dard tool for graph classiÔ¨Åcation. However  the use of kernel methods for node
classiÔ¨Åcation  which is a related problem to graph representation learning  is still
ill-posed and the state-of-the-art methods are heavily based on heuristics. Here  we
present a novel theoretical kernel-based framework for node classiÔ¨Åcation that can
bridge the gap between these two representation learning problems on graphs. Our
approach is motivated by graph kernel methodology but extended to learn the node
representations capturing the structural information in a graph. We theoretically
show that our formulation is as powerful as any positive semideÔ¨Ånite kernels. To
efÔ¨Åciently learn the kernel  we propose a novel mechanism for node feature aggrega-
tion and a data-driven similarity metric employed during the training phase. More
importantly  our framework is Ô¨Çexible and complementary to other graph-based
deep learning models  e.g.  Graph Convolutional Networks (GCNs). We empirically
evaluate our approach on a number of standard node classiÔ¨Åcation benchmarks 
and demonstrate that our model sets the new state of the art. The source code is
publicly available at https://github.com/bluer555/KernelGCN.

1

Introduction

Graph structured data  such as citation networks [11  22  30]  biological models [12  45]  grid-like
data [36  37  51] and skeleton-based motion systems [6  42  49  50]  are abundant in the real world.
Therefore  learning to understand graphs is a crucial problem in machine learning. Previous studies
in the literature generally fall into two main categories: (1) graph classiÔ¨Åcation [8  19  40  47  48] 
where the whole structure of graphs is captured for similarity comparison; (2) node classiÔ¨Åcation [1 
19  38  41  46]  where the structural identity of nodes is determined for representation learning.
For graph classiÔ¨Åcation  kernel methods  i.e.  graph kernels  have become a standard tool [20]. Given
a large collection of graphs  possibly with node and edge attributes  such algorithms aim to learn a
kernel function that best captures the similarity between any two graphs. The graph kernel function
can be utilized to classify graphs via standard kernel methods such as support vector machines or
k-nearest neighbors. Moreover  recent studies [40  47] also demonstrate that there has been a close
connection between Graph Neural Networks (GNNs) and the Weisfeiler-Lehman graph kernel [32] 
and relate GNNs to the classic graph kernel methods for graph classiÔ¨Åcation.

‚àóindicates equal contributions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Overview of our kernel-based framework.

Node classiÔ¨Åcation  on the other hand  is still an ill-posed problem in representation learning on
graphs. Although identiÔ¨Åcation of node classes often leverages their features  a more challenging
and important scenario is to incorporate the graph structure for classiÔ¨Åcation. Recent efforts in
Graph Convolutional Networks (GCNs) [19] have made great progress on node classiÔ¨Åcation. In
particular  these efforts broadly follow a recursive neighborhood aggregation scheme to capture
structural information  where each node aggregates feature vectors of its neighbors to compute its
new features [1  41  46]. Empirically  these GCNs have achieved the state-of-the-art performance
on node classiÔ¨Åcation. However  the design of new GCNs is mostly based on empirical intuition 
heuristics  and experimental trial-and-error.
In this paper  we propose a novel theoretical framework leveraging kernel methods for node classiÔ¨Å-
cation. Motivated by graph kernels  our key idea is to decouple the kernel function so that it can be
learned driven by the node class labels on the graph. Meanwhile  its validity and expressive power
are guaranteed. To be speciÔ¨Åc  this paper makes the following contributions:
‚Ä¢ We propose a learnable kernel-based framework for node classiÔ¨Åcation. The kernel function is
decoupled into a feature mapping function and a base kernel to ensure that it is valid as well as
learnable. Then we present a data-driven similarity metric and its corresponding learning criteria
for efÔ¨Åcient kernel training. The implementation of each component is extensively discussed. An
overview of our framework is shown in Fig. 1.

‚Ä¢ We demonstrate the validity of our learnable kernel function. More importantly  we theoretically
show that our formulation is powerful enough to express any valid positive semideÔ¨Ånite kernels.
‚Ä¢ A novel feature aggregation mechanism for learning node representations is derived from the per-
spective of kernel smoothing. Compared with GCNs  our model captures the structural information
of a node by aggregation in a single step  other than a recursive manner  thus is more efÔ¨Åcient.

‚Ä¢ We discuss the close connection between the proposed approach and GCNs. We also show that our
method is Ô¨Çexible and complementary to GCNs and their variants but more powerful  and can be
leveraged as a general framework for future work.

2 Related Work

Graph Kernels. Graph kernels are kernels deÔ¨Åned on graphs to capture the graph similarity  which
can be used in kernel methods for graph classiÔ¨Åcation. Many graph kernels are instances of the
family of convolutional kernels [15]. Some of them measure the similarity between walks or paths on
graphs [4  39]. Other popular kernels are designed based on limited-sized substructures [18  33  31 
32]. Most graph kernels are employed in models which have learnable components  but the kernels
themselves are hand-crafted and motivated by graph theory. Some learnable graph kernels have been
proposed recently  such as Deep Graph Kernels [43] and Graph Matching Networks [21]. Compared
to these approaches  our method targets at learning kernels for node representation learning.
Node Representation Learning. Conventional methods for learning node representations largely
focus on matrix factorization. They directly adopt classic techniques for dimension reduction [2  3].
Other methods are derived from the random walk algorithm [23  26] or sub-graph structures [13 
35  44  28]. Recently  Graph Convolutional Networks (GCNs) have emerged as an effective class of

2

InputNodesFeatureMappingùëî"InnerProductSpaceLoss‚ÑíInferenceLearnableKernelùêæ"Base	ùêäùêûùê´ùêßùêûùê•	ùëò -./‚äïmodels for learning representations of graph structured data. They were introduced in [19]  which
consist of an iterative process aggregating and transforming representation vectors of its neighboring
nodes to capture structural information. Recently  several variants have been proposed  which employ
self-attention mechanism [38] or improve network architectures [41  46] to boost the performance.
However  most of them are based on empirical intuition and heuristics.

3 Preliminaries

We begin by summarizing some of the most important concepts about kernel methods as well as
representation learning on graphs and  along the way  introduce our notations.
Kernel Concepts. A kernel K : X √ó X (cid:55)‚Üí R is a function of two arguments: K(x  y) for x  y ‚àà X .
The kernel function K is symmetric  i.e.  K(x  y) = K(y  x)  which means it can be interpreted as
a measure of similarity. If the Gram matrix K ‚àà RN√óN deÔ¨Åned by K(i  j) = K(xi  xj) for any
{xi}N
i=1 is positive semideÔ¨Ånite (p.s.d.)  then K is a p.s.d. kernel [24]. If K(x  y) can be represented
as (cid:104)Œ®(x)  Œ®(y)(cid:105)  where Œ® : X (cid:55)‚Üí RD is a feature mapping function  then K is a valid kernel.
Graph Kernels. In the graph space G  we denote a graph as G = (V  E)  where V is the set of nodes
and E is the edge set of G. Given two graphs Gi = (Vi  Ei) and Gj = (Vj  Ej) in G  the graph
kernel KG(Gi  Gj) measures the similarity between them. According to the deÔ¨Ånition in [29]  the
kernel KG must be p.s.d. and symmetric. The graph kernel KG between Gi and Gj is deÔ¨Åned as:

KG(Gi  Gj) =

kbase(f (vi)  f (vj)) 

(1)
where kbase is the base kernel for any pair of nodes in Gi and Gj  and f : V (cid:55)‚Üí ‚Ñ¶ is a function
to compute the feature vector associated with each node. However  deriving a new p.s.d. graph
kernel is a non-trivial task. Previous methods often implement kbase and f as the dot product between
hand-crafted graph heuristics [25  31  4]. There are little learnable parameters in these approaches.
Representation Learning on Graphs. Although graph kernels have been applied to a wide range
of applications  most of them depend on hand-crafted heuristics. In contrast  representation learning
aims to automatically learn to encode graph structures into low-dimensional embeddings. Formally 
given a graph G = (V  E)  we follow [14] to deÔ¨Åne representation learning as an encoder-decoder
framework  where we minimize the empirical loss L over a set of training node pairs D ‚äÜ V √ó V :
(2)

(cid:96)(ENC-DEC(vi  vj)  sG(vi  vj)).

(cid:88)

L =

(cid:88)

(cid:88)

vi‚ààVi

vj‚ààVj

(vi vj )‚ààD

Equation (2) has three methodological components: ENC-DEC  sG and (cid:96). Most of the previous
methods on representation learning can be distinguished by how these components are deÔ¨Åned. The
detailed meaning of each component is explained as follows.
‚Ä¢ ENC-DEC : V √ó V (cid:55)‚Üí R is an encoder-decoder function. It contains an encoder which projects
each node into a M-dimensional vector to generate the node embedding. This function contains
a number of trainable parameters to be optimized during the training phase. It also includes a
decoder function  which reconstructs pairwise similarity measurements from the node embeddings
generated by the encoder.
‚Ä¢ sG is a pairwise similarity function deÔ¨Åned over the graph G. This function is user-speciÔ¨Åed  and
‚Ä¢ (cid:96) : R √ó R (cid:55)‚Üí R is a loss function  which is leveraged to train the model. This function evaluates
the quality of the pairwise reconstruction between the estimated value ENC-DEC(vi  vj) and the
true value sG(vi  vj).

it is used for measuring the similarity between nodes in G.

4 Proposed Method: Learning Kernels for Node Representation

Given a graph G  as we can see from Eq. (2)  the encoder-decoder ENC-DEC aims to approximate
the pairwise similarity function sG  which leads to a natural intuition: we can replace ENC-DEC
with a kernel function KŒ∏ parameterized by Œ∏ to measure the similarity between nodes in G  i.e. 

(cid:88)

L =

(vi vj )‚ààD

(cid:96)(KŒ∏(vi  vj)  sG(vi  vj)).

(3)

3

However  there exist two technical challenges: (1) designing a valid p.s.d. kernel which captures the
node feature is non-trivial; (2) it is impossible to handcraft a uniÔ¨Åed kernel to handle all possible
graphs with different characteristics [27]. To tackle these issues  we introduce a novel formulation to
replace KŒ∏. Inspired by the graph kernel as deÔ¨Åned in Eq. (1) and the mapping kernel framework [34] 
our key idea is to decouple KŒ∏ into two components: a base kernel kbase which is p.s.d. to maintain
the validity  and a learnable feature mapping function gŒ∏ to ensure the Ô¨Çexibility of the resulting
kernel. Therefore  we rewrite Eq. (3) by KŒ∏(vi  vj) = kbase(gŒ∏(vi)  gŒ∏(vj)) for vi  vj ‚àà V of the
graph G to optimize the following objective:

(cid:96)(kbase(gŒ∏(vi)  gŒ∏(vj))  sG(vi  vj)).

(4)

(cid:88)

L =

(vi vj )‚ààD

Theorem 1 demonstrates that the proposed formulation  i.e.  KŒ∏(vi  vj) = kbase(gŒ∏(vi)  gŒ∏(vj))  is
still a valid p.s.d. kernel for any feature mapping function gŒ∏ parameterized by Œ∏.
Theorem 1. Let gŒ∏ : V (cid:55)‚Üí RM be a function which maps nodes (or their corresponding features)
to a M-dimensional Euclidean space. Let kbase : RM √ó RM (cid:55)‚Üí R be any valid p.s.d. kernel. Then 
KŒ∏(vi  vj) = kbase(gŒ∏(vi)  gŒ∏(vj)) is a valid p.s.d. kernel.

Proof. Let Œ¶ be the corresponding feature mapping function of the p.s.d. kernel kbase. Then  we
have kbase(zi  zj) = (cid:104)Œ¶(zi)  Œ¶(zj)(cid:105)  where zi  zj ‚àà RM . Substitute gŒ∏(vi)  gŒ∏(vj) for zi  zj  and
we have kbase(gŒ∏(vi)  gŒ∏(vj)) = (cid:104)Œ¶(gŒ∏(vi))  Œ¶(gŒ∏(vj))(cid:105). Write the new feature mapping Œ®(v) as
Œ®(v) = Œ¶(gŒ∏(v))  and we immediately have that kbase(gŒ∏(vi)  gŒ∏(vj)) = (cid:104)Œ®(vi)  Œ®(vj)(cid:105). Hence 
kbase(gŒ∏(vi)  gŒ∏(vj)) is a valid p.s.d. kernel.

A natural follow-up question is whether our proposed formulation  in principle  is powerful enough to
express any valid p.s.d. kernels? Our answer  in Theorem 2  is yes: if the base kernel has an invertible
feature mapping function  then the resulting kernel is able to model any valid p.s.d. kernels.
Theorem 2. Let K(vi  vj) be any valid p.s.d. kernel for node pairs (vi  vj) ‚àà V √ó V . Let kbase :
RM √ó RM (cid:55)‚Üí R be a p.s.d. kernel which has an invertible feature mapping function Œ¶. Then there
exists a feature mapping function gŒ∏ : V (cid:55)‚Üí RM   such that K(vi  vj) = kbase(gŒ∏(vi)  gŒ∏(vj)).

Proof. Let Œ® be the corresponding feature mapping function of the p.s.d. kernel K  and then we have
K(vi  vj) = (cid:104)Œ®(vi)  Œ®(vj)(cid:105). Similarly  for zi  zj ‚àà RM   we have kbase(zi  zj) = (cid:104)Œ¶(zi)  Œ¶(zj)(cid:105).
Substitute gŒ∏(v) for z  and then it is easy to see that gŒ∏(v) = (Œ¶‚àí1 ‚ó¶ Œ®)(v) is the desired feature
mapping function when Œ¶‚àí1 exists.

4.1

Implementation and Learning Criteria

Theorems 1 and 2 have demonstrated the validity and power of the proposed formulation in Eq. (4).
In this section  we discuss how to implement and learn gŒ∏  kbase  sG and (cid:96)  respectively.
Implementation of the Feature Mapping Function gŒ∏. The function gŒ∏ aims to project the feature
vector xv of each node v into a better space for similarity measurement. Our key idea is that in a
graph  connected nodes usually share some similar characteristics  and thus changes between nearby
nodes in the latent space of nodes should be smooth. Inspired by the concept of kernel smoothing 
we consider gŒ∏ as a feature smoother which maps xv into a smoothed latent space according to the
graph structure. The kernel smoother estimates a function as the weighted average of neighboring
observed data. To be speciÔ¨Åc  given a node v ‚àà V   according to Nadaraya-Watson kernel-weighted
average [10]  a feature smoothing function is deÔ¨Åned as:

(cid:80)
(cid:80)

g(v) =

u‚ààV k(u  v)p(u)

u‚ààV k(u  v)

 

(5)

where p is a mapping function to compute the feature vector of each node  and here we let p(v) = xv;
k is a pre-deÔ¨Åned kernel function to capture pairwise relations between nodes. Note that we omit Œ∏
for g here since there are no learnable parameters in Eq. (5). In the context of graphs  the natural
choice of computing k is to follow the graph structure  i.e.  the structural information within the
node‚Äôs h-hop neighborhood.

4

(cid:80)

2 AD‚àí 1

To compute g  we let A be the adjacent matrix of the given graph G and I be the identity matrix
with the same size. We notice that I + D‚àí 1
2 is a valid p.s.d. matrix  where D(i  i) =
j A(i  j). Thus we can employ this matrix to deÔ¨Åne the kernel function k. However  in practice 
this matrix would lead to numerical instabilities and exploding or vanishing gradients when used for
training deep neural networks. To alleviate this problem  we adopt the renormalization trick [19]:
I + D‚àí 1
ÀúA(i  j). Then the
h-hop neighborhood can be computed directly from the h power of ¬ØA  i.e.  ¬ØAh. And the kernel k for
node pairs vi  vj ‚àà V is computed as k(vi  vj) = ¬ØAh(i  j). After collecting the feature vector xv of
each node v ‚àà V into a matrix XV   we rewrite Eq. (5) approximately into its matrix form:

2   where ÀúA = A + I and ÀúD(i  i) =(cid:80)

2 ‚Üí ¬ØA = ÀúD‚àí 1

2 AD‚àí 1

2 ÀúA ÀúD‚àí 1

j

g(V ) ‚âà ¬ØAhXV .

(6)

Next  we enhance the expressive power of Eq. (6) to model any valid p.s.d. kernels by implementing
it with deep neural networks based on the following two aspects. First  we make use of multi-layer
perceptrons (MLPs) to model and learn the composite function Œ¶‚àí1 ‚ó¶ Œ® in Theorem 2  thanks to the
universal approximation theorem [16  17]. Second  we add learnable weights to different hops of
node neighbors. As a result  our Ô¨Ånal feature mapping function gŒ∏ is deÔ¨Åned as:

(cid:32)(cid:88)

œâh ¬∑(cid:16) ¬ØAh (cid:12) M(h)(cid:17)(cid:33)

gŒ∏(V ) =

h

¬∑ MLP(l)(XV ) 

(7)

where Œ∏ means the set of parameters in gŒ∏; œâh is a learnable parameter for the h-hop neighborhood
of each node v; (cid:12) is the Hadamard (element-wise) product; M(h) is an indicator matrix where
M(h)(i  j) equals to 1 if vj is a h-th hop neighbor of vi and 0 otherwise. The hyperparameter l
controls the number of layers in the MLP.
Equation (7) can be interpreted as a weighted feature aggregation schema around the given node v
and its neighbors  which is employed to compute the node representation. It has a close connection
with Graph Neural Networks. We leave it in Section 5 for a more detailed discussion.
Implementation of the Base Kernel kbase. As we have shown in Theorem 2  in order to model
an arbitrary p.s.d. kernel  we require that the corresponding feature mapping function Œ¶ of the
base kernel kbase must be invertible  i.e.  Œ¶‚àí1 exists. An obvious choice would let Œ¶ be an identity
function  then kbase will reduce to the dot product between nodes in the latent space. Since gŒ∏ maps
node representations to a Ô¨Ånite dimensional space  the identity function makes our model directly
measure the node similarity in this space. On the other hand  an alternative choice of kbase is the RBF
kernel which additionally projects node representations to an inÔ¨Ånite dimensional latent space before
comparison. We compare both implementations in the experiments for further evaluation.
Data-Driven Similarity Metric sG and Criteria (cid:96). In node classiÔ¨Åcation  each node vi ‚àà V is
associated with a class label yi ‚àà Y . We aim to measure node similarity with respect to their class
labels other than hand-designed metrics. Naturally  we deÔ¨Åne the pairwise similarity sG as:

sG(vi  vj) =

‚àí1

if yi = yj
o/w

(8)

However  in practice  it is hard to directly minimize the loss between KŒ∏ and sG in Eq. (8). Instead 
we consider a ‚Äúsoft‚Äù version of sG  where we require that the similarity of node pairs with the
same label is greater than those with distinct labels by a margin. Therefore  we train the kernel
KŒ∏(vi  vj) = kbase(gŒ∏(vi)  gŒ∏(vj)) to minimize the following objective function on triplets:

(cid:96)(KŒ∏(vi  vj)  KŒ∏(vi  vk)) 

(9)

(cid:88)

LK =

(vi vj  vk)‚ààT

where T ‚äÜ V √ó V √ó V is a set of node triplets: vi is an anchor  and vj is a positive of the same class
as the anchor while vk is a negative of a different class. The loss function (cid:96) is deÔ¨Åned as:

(cid:96)(KŒ∏(vi  vj)  KŒ∏(vi  vk)) = [KŒ∏(vi  vk) ‚àí KŒ∏(vi  vj) + Œ±]+.

(10)
It ensures that given two positive nodes of the same class and one negative node  the kernel value of
the negative should be farther away than the one of the positive by the margin Œ±. Here  we present
Theorem 3 and its proof to show that minimizing Eq. (9) leads to KŒ∏ = sG.

(cid:26)1

5

Theorem 3. If |KŒ∏(vi  vj)| ‚â§ 1 for any vi  vj ‚àà V   minimizing Eq. (9) with Œ± = 2 yields KŒ∏ = sG.
Proof. Let (vi  vj  vk) be all triplets satisfying yi = yj  yi (cid:54)= yk. Suppose that for Œ± = 2  Eq. (10)
holds for all (vi  vj  vk). It means KŒ∏(vi  vk) + 2 ‚â§ KŒ∏(vi  vj) for all (vi  vj  vk). As |KŒ∏(vi  vj)| ‚â§
1  we have KŒ∏(vi  vk) = ‚àí1 for all (vi  vk) and KŒ∏(vi  vj) = 1 for all (vi  vj). Hence  KŒ∏ = sG.
We note that |KŒ∏(vi  vj)| ‚â§ 1 can be simply achieved by letting kbase be the dot product and
normalizing all gŒ∏ to the norm ball. In the following sections  the normalized KŒ∏ is denoted by ¬ØKŒ∏.

4.2

Inference for Node ClassiÔ¨Åcation

Once the kernel function KŒ∏(vi  vj) = kbase(gŒ∏(vi)  gŒ∏(vj)) has learned how to measure the simi-
larity between nodes  we can leverage the output of the feature mapping function gŒ∏ as the node
representation for node classiÔ¨Åcation. In this paper  we introduce the following two classiÔ¨Åers.
Nearest Centroid ClassiÔ¨Åer. The nearest centroid classiÔ¨Åer extends the k-nearest neighbors algo-
rithm by assigning to observations the label of the class of training samples whose centroid is closest
(cid:80)
to the observation. It does not require additional parameters. To be speciÔ¨Åc  given a testing node
u  for all nodes vi with class label yi ‚àà Y in the training set  we compute the per-class average
¬ØKŒ∏(u  vi)  where Vy is the set of nodes belonging to
similarity between u and vi: ¬µy = 1|Vy|
class y ‚àà Y . Then the class assigned to the testing node u:
y‚àó = arg maxy‚ààY ¬µy.

vi‚ààVy

(11)

Softmax ClassiÔ¨Åer. The idea of the softmax classiÔ¨Åer is to reuse the ground truth labels of nodes
for training the classiÔ¨Åer  so that it can be directly employed for inference. To do this  we add the
softmax activation œÉ after gŒ∏(vi) to minimize the following objective:

q(yi) log(œÉ(gŒ∏(vi))) 

(12)

LY = ‚àí (cid:88)

vi‚ààV

where q(yi) is the one-hot ground truth vector. Note that Eq. (12) is optimized together with Eq. (9)
in an end-to-end manner. Let Œ® denote the corresponding feature mapping function of KŒ∏  then
we have KŒ∏(vi  vj) = (cid:104)Œ®(vi)  Œ®(vj)(cid:105) = kbase(gŒ∏(vi)  gŒ∏(vj)). In this case  we use the node feature
produced by Œ® for classiÔ¨Åcation since Œ® projects node features into the dot-product space which is a
natural metric for similarity comparison. To this end  kbase is Ô¨Åxed to be the identity function for the
softmax classiÔ¨Åer  so that we have (cid:104)Œ®(vi)  Œ®(vj)(cid:105) = (cid:104)gŒ∏(vi)  gŒ∏(vj)(cid:105) and thus Œ®(vi) = gŒ∏(vi).

5 Discussion

(cid:16) ¬ØAH(l)W(l)(cid:17)

Our feature mapping function gŒ∏ proposed in Eq. (7) has a close connection with Graph Convolutional
Networks (GCNs) [19] in the way of capturing node latent representations. In GCNs and most of
their variants  each layer leverages the following aggregation rule:

 

H(l+1) = œÅ

(13)
where W(l) is a layer-speciÔ¨Åc trainable weighting matrix; œÅ denotes an activation function; H(l) ‚àà
RN√óD denotes the node features in the l-th layer  and H0 = X. Through stacking multiple layers 
GCNs aggregate the features for each node from its L-hop neighbors recursively  where L is the
network depth. Compared with the proposed gŒ∏  GCNs actually interleave two basic operations of gŒ∏:
feature transformation and Nadaraya-Watson kernel-weighted average  and repeat them recursively.
We contrast our approach with GCNs in terms of the following aspects. First  our aggregation function
is derived from the kernel perspective  which is novel. Second  we show that aggregating features
in a recursive manner is inessential. Powerful h-hop node representations can be obtained by our
model where aggregation is performed only once. As a result  our approach is more efÔ¨Åcient both in
storage and time when handling very large graphs  since no intermediate states of the network have
to be kept. Third  our model is Ô¨Çexible and complementary to GCNs: our function gŒ∏ can be directly
replaced by GCNs and other variants  which can be exploited for future work.

6

Time and Space Complexity. We assume the number of features F is Ô¨Åxed for all layers and both
GCNs and our method have L ‚â• 2 layers. We count matrix multiplications as in [7]. GCN‚Äôs time
complexity is O(L(cid:107) ¬ØA(cid:107)0F + L|V |F 2)  where (cid:107) ¬ØA(cid:107)0 is the number of nonzeros of ¬ØA and |V | is the
number of nodes in the graph. While ours is O((cid:107) ¬ØAh(cid:107)0F + L|V |F 2)  since we do not aggregate
features recursively. Obviously  (cid:107) ¬ØAh(cid:107)0 is constant but L(cid:107) ¬ØA(cid:107)0 is linear to L. For space complexity 
GCNs have to store all the feature matrices for recursive aggregation which needs O(L|V |F + LF 2)
space  where LF 2 is for storing trainable parameters of all layers  and thus the Ô¨Årst term is linear
to L. Instead  ours is O(|V |F + LF 2) where the Ô¨Årst term is again constant to L. Our experiments
indicate that we save 20% (0.3 ms) time and 15% space on Cora dataset [22] than GCNs.

6 Experiments

We evaluate the proposed kernel-based approach on three benchmark datasets: Cora [22]  Citeseer [11]
and Pubmed [30]. They are citation networks  where the task of node classiÔ¨Åcation is to classify
academic papers of the network (graph) into different subjects. These datasets contain bag-of-words
features for each document (node) and citation links between documents.
We compare our approach to Ô¨Åve state-of-the-art methods: GCN [19]  GAT [38]  FastGCN [5] 
JK [41] and KLED [9]. KLED is a kernel-based method  while the others are based on deep neural
networks. We test all methods in the supervised learning scenario  where all data in the training
set are used for training. We evaluate the proposed method in two different experimental settings
according to FastGCN [5] and JK [41]  respectively. The statistics of the datasets together with
their data split settings (i.e.  the number of samples contained in the training  validation and testing
sets  respectively) are summarized in Table 1. Note that there are more training samples in the data
split of JK [41] than FastGCN [5]. We report the average means and standard deviations of node
classiÔ¨Åcation accuracy which are computed from ten runs as the evaluation metrics.

Table 1: Overview of the three evaluation datasets under two different data split settings.

Nodes Edges Classes Features Data split of FastGCN [5] Data split of JK [41]
Dataset
Cora [22]
2 708 5 429
Citeseer [11] 3 327 4 732
Pubmed [30] 19 717 44 338

1 208 / 500 / 1 000
1 827 / 500 / 1 000
18 217 / 500 / 1 000

1 433
3 703
500

7
6
3

1 624 / 542 / 542
1 997 / 665 / 665

-

6.1 Variants of the Proposed Method

As we have shown in Section 4.1  there are alternative choices to implement each component of our
framework. In this section  we summarize all the variants of our method employed for evaluation.
Choices of the Feature Mapping Function g. We implement the feature mapping function gŒ∏
according to Eq. (7). In addition  we also choose GCN and GAT as the alternative implementations
of gŒ∏ for comparison  and denote them by gGCN and gGAT  respectively.
Choices of the Base Kernel kbase. The base kernel kbase has two different implementations: the dot
product which is denoted by k(cid:104)¬∑ ¬∑(cid:105)  and the RBF kernel which is denoted by kRBF. Note that when the
softmax classiÔ¨Åer is employed  we set the base kernel to be k(cid:104)¬∑ ¬∑(cid:105).
Choices of the Loss L and ClassiÔ¨Åer C. We consider the following three combinations of the loss
function and classiÔ¨Åer. (1) LK in Eq. (9) is optimized  and the nearest-centroid classiÔ¨Åer CK is
employed for classiÔ¨Åcation. This combination aims to evaluate the effectiveness of the learned kernel.
(2) LY in Eq. (12) is optimized  and the softmax classiÔ¨Åer CY is employed for classiÔ¨Åcation. This
combination is used in a baseline without kernel methods. (3) Both Eq. (9) and Eq. (12) are optimized 
and we denote this loss by LK+Y . The softmax classiÔ¨Åer CY is employed for classiÔ¨Åcation. This
combination aims to evaluate how the learned kernel improves the baseline method.
In the experiments  we use K to denote kernel-based variants and N to denote ones without the kernel
function. All these variants are implemented by MLPs with two layers. Due to the space limitation 
we ask the readers to refer to the supplementary material for implementation details.

7

6.2 Results of Node ClassiÔ¨Åcation

The means and standard deviations of node classiÔ¨Åcation accuracy (%) following the setting of
FastGCN [5] are organized in Table 2. Our variant of K3 sets the new state of the art on all datasets.
And on Pubmed dataset  all our variants improve previous methods by a large margin. It proves the
effectiveness of employing kernel methods for node classiÔ¨Åcation  especially on datasets with large
graphs. Interestingly  our non-kernel baseline N1 even achieves the state-of-the-art performance 
which shows that our feature mapping function can capture more Ô¨Çexible structural information than
previous GCN-based approaches. For the choice of the base kernel  we can Ô¨Ånd that K2 outperforms
K1 on two large datasets: Citeseer and Pubmed. We conjecture that when handling complex datasets 
the non-linear kernel  e.g.  the RBF kernel  is a better choice than the liner kernel.
To evaluate the performance of our feature mapping function  we report the results of two variants
K‚àó
1 and K‚àó
2 in Table 2. They utilize GCN and GAT as the feature mapping function respectively. As
expected  our K1 outperforms K‚àó
2 among most datasets. This demonstrates that the recursive
aggregation schema of GCNs is inessential  since the proposed gŒ∏ aggregates features only in a single
step  which is still powerful enough for node classiÔ¨Åcation. On the other hand  it is also observed
that both K‚àó
2 outperform their original non-kernel based implementations  which shows that
learning with kernels yields better node representations.
Table 3 shows the results following the setting of JK [41]. Note that we do not evaluate on Pubmed in
this setup since its corresponding data split for training and evaluation is not provided by [41]. As
expected  our method achieves the best performance among all datasets  which is consistent with the
results in Table 2. For Cora  the improvement of our method is not so signiÔ¨Åcant. We conjecture
that the results in Table 3 involve more training data due to different data splits  which narrows the
performance gap between different methods on datasets with small graphs  such as Cora.

1 and K‚àó

1 and K‚àó

Table 2: Accuracy (%) of node classiÔ¨Åcation following the setting of FastGCN [5].

Method
KLED [9]
GCN [19]
GAT [38]
FastGCN [5]
K1 = {k(cid:104)¬∑ ¬∑(cid:105)  gŒ∏ LK CK}
K2 = {kRBF  gŒ∏ LK CK}
K3 = {k(cid:104)¬∑ ¬∑(cid:105)  gŒ∏ LK+Y  CY }
N1 = {gŒ∏ LY  CY }
K‚àó
1 = {k(cid:104)¬∑ ¬∑(cid:105)  gGCN LK CK}
K‚àó
2 = {k(cid:104)¬∑ ¬∑(cid:105)  gGAT LK CK}

Cora [22]

Citeseer [11]

Pubmed [30]

82.3
86.0
85.6
85.0

86.68 ¬± 0.17
86.12 ¬± 0.05
88.40 ¬± 0.24
87.56 ¬± 0.14
87.04 ¬± 0.09
86.10 ¬± 0.33

-

77.2
76.9
77.6

77.92 ¬± 0.25
78.68 ¬± 0.38
80.28 ¬± 0.03
79.80 ¬± 0.03
77.12 ¬± 0.23
77.92 ¬± 0.19

82.3
86.5
86.2
88.0

89.22 ¬± 0.17
89.36 ¬± 0.21
89.42 ¬± 0.01
89.24 ¬± 0.14
87.84 ¬± 0.12

-

Table 3: Accuracy (%) of node classiÔ¨Åcation following the setting of JK [41].

Method
GCN [19]
GAT [38]
JK-Concat [41]
K3 = {k(cid:104)¬∑ ¬∑(cid:105)  gŒ∏ LK+Y  CY }

Cora [22]
88.20 ¬± 0.70
87.70 ¬± 0.30
89.10 ¬± 1.10
89.24 ¬± 0.31

Citeseer [11]
77.30 ¬± 1.30
76.20 ¬± 0.80
78.30 ¬± 0.80
80.78 ¬± 0.28

6.3 Ablation Study on Node Feature Aggregation Schema
In Table 4  we implement three variants of K3 (2-hop and 2-layer with œâh by default) to evaluate
the proposed node feature aggregation schema. We answer the following three questions. (1) How
does performance change with fewer (or more) hops? We change the number of hops from 1 to 3 
and the performance improves if it is larger  which shows capturing long-range structures of nodes is
important. (2) How many layers of MLP are needed? We show results with different layers ranging
from 1 to 3. The best performance is obtained with two layers  while networks overÔ¨Åt the data when

8

more layers are employed. (3) Is it necessary to have a trainable parameter œâh? We replace œâh with
a Ô¨Åxed constant ch  where c ‚àà (0  1]. We can see larger c improves the performance. However  all
results are worse than learning a weighting parameter œâh  which shows the importance of it.
Table 4: Results of accuracy (%) with different settings of the aggregation schema.

Variants of K3
Default
1-hop
3-hop
1-layer
3-layer
c = 0.25
c = 0.50
c = 0.75
c = 1.00

Cora [22]
88.40 ¬± 0.24
85.56 ¬± 0.02
88.25 ¬± 0.01
82.60 ¬± 0.01
86.33 ¬± 0.04
69.33 ¬± 0.09
76.98 ¬± 0.10
84.25 ¬± 0.01
87.31 ¬± 0.01

Citeseer [11]
80.28 ¬± 0.03
77.73 ¬± 0.02
80.13 ¬± 0.01
77.63 ¬± 0.01
78.53 ¬± 0.20
74.48 ¬± 0.03
77.47 ¬± 0.04
77.99 ¬± 0.01
78.57 ¬± 0.01

Pubmed [30]
89.42 ¬± 0.01
88.98 ¬± 0.01
89.53 ¬± 0.01
85.80 ¬± 0.01
89.46 ¬± 0.05
84.68 ¬± 0.02
86.45 ¬± 0.01
87.45 ¬± 0.01
88.68 ¬± 0.01

6.4

t-SNE Visualization of Node Embeddings

We visualize the node embeddings of GCN  GAT and our method on Citeseer with t-SNE. For
our method  we use the embedding of K3 which obtains the best performance. Figure 2 illustrates
the results. Compared with other methods  our method produces a more compact clustering result.
SpeciÔ¨Åcally our method clusters the ‚Äúred‚Äù points tightly  while in the results of GCN and GAT  they
are loosely scattered into other clusters. This is caused by the fact that both GCN and GAT minimize
the classiÔ¨Åcation loss LY   only targeting at accuracy. They tend to learn node embeddings driven
by those classes with the majority of nodes. In contrast  K3 are trained with both LK and LY . Our
kernel-based similarity loss LK encourages data within the same class to be close to each other. As a
result  the learned feature mapping function gŒ∏ encourages geometrically compact clusters.

Figure 2: t-SNE visualization of node embeddings on Citeseer dataset.

Due to the space limitation  we ask the readers to refer to the supplementary material for more
experiment results  such as the results of link prediction and visualization on other datasets.

7 Conclusions

In this paper  we introduce a kernel-based framework for node classiÔ¨Åcation. Motivated by the
design of graph kernels  we learn the kernel from ground truth labels by decoupling the kernel
function into a base kernel and a learnable feature mapping function. More importantly  we show
that our formulation is valid as well as powerful enough to express any p.s.d. kernels. Then the
implementation of each component in our approach is extensively discussed. From the perspective
of kernel smoothing  we also derive a novel feature mapping function to aggregate features from a
node‚Äôs neighborhood. Furthermore  we show that our formulation is closely connected with GCNs but
more powerful. Experiments on standard node classiÔ¨Åcation benchmarks are conducted to evaluated
our approach. The results show that our method outperforms the state of the art.

9

(a) GCN(c) Ours(b) GATAcknowledgments

This work is funded by ARO-MURI-68985NSMUR and NSF 1763523  1747778  1733843  1703883.

References
[1] S. Abu-El-Haija  A. Kapoor  B. Perozzi  and J. Lee. N-GCN: Multi-scale graph convolution for semi-

supervised node classiÔ¨Åcation. arXiv preprint arXiv:1802.08888  2018.

[2] A. Ahmed  N. Shervashidze  S. Narayanamurthy  V. Josifovski  and A. J. Smola. Distributed large-scale
natural graph factorization. In Proceedings of the International Conference on World Wide Web (WWW) 
pages 37‚Äì48  2013.

[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In

Advances in Neural Information Processing Systems (NeurIPS)  pages 585‚Äì591  2002.

[4] K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs.

International Conference on Data Mining (ICDM)  2005.

In Proceedings of the IEEE

[5] J. Chen  T. Ma  and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance

sampling. arXiv preprint arXiv:1801.10247  2018.

[6] Y. Chen  L. Zhao  X. Peng  J. Yuan  and D. N. Metaxas. Construct dynamic graphs for hand gesture
recognition via spatial-temporal attention. In Proceedings of the British Machine Vision Conference
(BMVC)  2019.

[7] W.-L. Chiang  X. Liu  S. Si  Y. Li  S. Bengio  and C.-J. Hsieh. Cluster-GCN: An efÔ¨Åcient algorithm for
training deep and large graph convolutional networks. In Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD)  pages 257‚Äì266  2019.

[8] M. Draief  K. Kutzkov  K. Scaman  and M. Vojnovic. KONG: Kernels for ordered-neighborhood graphs.

In Advances in Neural Information Processing Systems (NeurIPS)  pages 4051‚Äì4060  2018.

[9] F. Fouss  L. Yen  A. Pirotte  and M. Saerens. An experimental investigation of graph kernels on a
collaborative recommendation task. In Proceedings of the International Conference on Data Mining
(ICDM)  pages 863‚Äì868  2006.

[10] J. Friedman  T. Hastie  and R. Tibshirani. The elements of statistical learning. Springer series in statistics

New York  2001.

[11] C. L. Giles  K. D. Bollacker  and S. Lawrence. Citeseer: An automatic citation indexing system. In

Proceedings of the Third ACM Conference on Digital Libraries  pages 89‚Äì98  1998.

[12] J. Gilmer  S. S. Schoenholz  P. F. Riley  O. Vinyals  and G. E. Dahl. Neural message passing for quantum
chemistry. In Proceedings of the International Conference on Machine Learning (ICML)  pages 1263‚Äì1272 
2017.

[13] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)  pages 855‚Äì864 
2016.

[14] W. L. Hamilton  R. Ying  and J. Leskovec. Representation learning on graphs: Methods and applications.

arXiv preprint arXiv:1709.05584  2017.

[15] D. Haussler. Convolution kernels on discrete structures. Technical report  Department of Computer Science 

University of California at Santa Cruz  1999.

[16] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks  4(2):251‚Äì257 

1991.

[17] K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal approximators.

Neural networks  2(5):359‚Äì366  1989.

[18] T. Horv√°th  T. G√§rtner  and S. Wrobel. Cyclic pattern kernels for predictive graph mining. In Proceedings
of the ACM SIGKDD International Conference on Knowledge discovery and Data Mining (KDD)  pages
158‚Äì167  2004.

[19] T. N. Kipf and M. Welling. Semi-supervised classiÔ¨Åcation with graph convolutional networks.

Proceedings of the International Conference on Learning Representations (ICLR)  2017.

In

10

[20] N. M. Kriege  M. Neumann  C. Morris  K. Kersting  and P. Mutzel. A unifying view of explicit and implicit
feature maps for structured data: systematic studies of graph kernels. arXiv preprint arXiv:1703.00676 
2017.

[21] Y. Li  C. Gu  T. Dullien  O. Vinyals  and P. Kohli. Graph matching networks for learning the similarity of
graph structured objects. In Proceedings of the International Conference on Machine Learning (ICML) 
2019.

[22] A. K. McCallum  K. Nigam  J. Rennie  and K. Seymore. Automating the construction of internet portals

with machine learning. Information Retrieval  3(2):127‚Äì163  2000.

[23] T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in Neural Information Processing Systems (NeurIPS) 
pages 3111‚Äì3119  2013.

[24] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press  2012.

[25] M. Neuhaus and H. Bunke. Self-organizing maps for learning the edit costs in graph matching. IEEE

Transactions on Systems  Man  and Cybernetics  Part B (Cybernetics)  35(3):503‚Äì514  2005.

[26] B. Perozzi  R. Al-Rfou  and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)  pages
701‚Äì710  2014.

[27] J. Ramon and T. G√§rtner. Expressivity versus efÔ¨Åciency of graph kernels. In Proceedings of the International

Workshop on Mining Graphs  Trees and Sequences  pages 65‚Äì74  2003.

[28] L. F. Ribeiro  P. H. Saverese  and D. R. Figueiredo. struc2vec: Learning node representations from
structural identity. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (KDD)  pages 385‚Äì394  2017.

[29] B. Scholkopf and A. J. Smola. Learning with kernels: support vector machines  regularization  optimization 

and beyond. MIT press  2001.

[30] P. Sen  G. Namata  M. Bilgic  L. Getoor  B. Galligher  and T. Eliassi-Rad. Collective classiÔ¨Åcation in

network data. AI magazine  29(3):93‚Äì93  2008.

[31] N. Shervashidze and K. Borgwardt. Fast subtree kernels on graphs. In Advances in Neural Information

Processing Systems (NeurIPS)  pages 1660‚Äì1668  2009.

[32] N. Shervashidze  P. Schweitzer  E. J. v. Leeuwen  K. Mehlhorn  and K. M. Borgwardt. Weisfeiler-lehman

graph kernels. Journal of Machine Learning Research  12:2539‚Äì2561  2011.

[33] N. Shervashidze  S. Vishwanathan  T. Petri  K. Mehlhorn  and K. Borgwardt. EfÔ¨Åcient graphlet kernels
for large graph comparison. In Proceedings of the International Conference on ArtiÔ¨Åcial Intelligence and
Statistics (AISTATS)  pages 488‚Äì495  2009.

[34] K. Shin and T. Kuboyama. A generalization of haussler‚Äôs convolution kernel: mapping kernel.

Proceedings of the International Conference on Machine Learning (ICML)  pages 944‚Äì951  2008.

In

[35] J. Tang  M. Qu  M. Wang  M. Zhang  J. Yan  and Q. Mei. Line: Large-scale information network embedding.

In Proceedings of the International Conference on World Wide Web (WWW)  pages 1067‚Äì1077  2015.

[36] Z. Tang  X. Peng  S. Geng  L. Wu  S. Zhang  and D. Metaxas. Quantized Densely Connected U-Nets for
EfÔ¨Åcient Landmark Localization. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 339‚Äì354  2018.

[37] Y. Tian  X. Peng  L. Zhao  S. Zhang  and D. N. Metaxas. CR-GAN: learning complete representations
for multi-view generation. In Proceedings of the International Joint Conference on ArtiÔ¨Åcial Intelligence
(IJCAI)  pages 942‚Äì948  2018.

[38] P. VeliÀáckovi¬¥c  G. Cucurull  A. Casanova  A. Romero  P. Lio  and Y. Bengio. Graph attention networks. In

Proceedings of the International Conference on Learning Representations (ICLR)  2018.

[39] S. V. N. Vishwanathan  N. N. Schraudolph  R. Kondor  and K. M. Borgwardt. Graph kernels. Journal of

Machine Learning Research  11(Apr):1201‚Äì1242  2010.

[40] K. Xu  W. Hu  J. Leskovec  and S. Jegelka. How powerful are graph neural networks? In Proceedings of

the International Conference on Learning Representations (ICLR)  2019.

11

[41] K. Xu  C. Li  Y. Tian  T. Sonobe  K.-i. Kawarabayashi  and S. Jegelka. Representation learning on graphs
with jumping knowledge networks. In Proceedings of the 34th International Conference on Machine
Learning (ICML)  2018.

[42] S. Yan  Y. Xiong  and D. Lin. Spatial temporal graph convolutional networks for skeleton-based action

recognition. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI)  2018.

[43] P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proceedings of the ACM SIGKDD International

Conference on Knowledge Discovery and Data Mining (KDD)  pages 1365‚Äì1374  2015.

[44] Z. Yang  W. W. Cohen  and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings.

In Proceedings of the International Conference on Machine Learning (ICML)  pages 40‚Äì48  2016.

[45] J. You  B. Liu  Z. Ying  V. Pande  and J. Leskovec. Graph convolutional policy network for goal-directed
molecular graph generation. In Advances in Neural Information Processing Systems (NeurIPS)  pages
6410‚Äì6421  2018.

[46] L. Zhang  H. Song  and H. Lu. Graph node-feature convolution for representation learning. arXiv preprint

arXiv:1812.00086  2018.

[47] M. Zhang  Z. Cui  M. Neumann  and Y. Chen. An end-to-end deep learning architecture for graph

classiÔ¨Åcation. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI)  2018.

[48] Z. Zhang  M. Wang  Y. Xiang  Y. Huang  and A. Nehorai. Retgk: Graph kernels based on return probabilities
of random walks. In Advances in Neural Information Processing Systems (NeurIPS)  pages 3964‚Äì3974 
2018.

[49] L. Zhao  X. Peng  Y. Tian  M. Kapadia  and D. Metaxas. Learning to forecast and reÔ¨Åne residual motion
for image-to-video generation. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 387‚Äì403  2018.

[50] L. Zhao  X. Peng  Y. Tian  M. Kapadia  and D. N. Metaxas. Semantic graph convolutional networks for
3D human pose regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 3425‚Äì3435  2019.

[51] Y. Zhu  M. Elhoseiny  B. Liu  X. Peng  and A. Elgammal. A generative adversarial approach for zero-shot
In Proceedings of the IEEE Conference on Computer Vision and Pattern

learning from noisy texts.
Recognition (CVPR)  2018.

12

,Yu Tian
Long Zhao
Xi Peng
Dimitris Metaxas