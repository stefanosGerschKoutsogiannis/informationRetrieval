2019,Rethinking Kernel Methods for Node Representation Learning on Graphs,Graph kernels are kernel methods measuring graph similarity and serve as a standard tool for graph classification. However  the use of kernel methods for node classification  which is a related problem to graph representation learning  is still ill-posed and the state-of-the-art methods are heavily based on heuristics. Here  we present a novel theoretical kernel-based framework for node classification that can bridge the gap between these two representation learning problems on graphs. Our approach is motivated by graph kernel methodology but extended to learn the node representations capturing the structural information in a graph. We theoretically show that our formulation is as powerful as any positive semidefinite kernels. To efficiently learn the kernel  we propose a novel mechanism for node feature aggregation and a data-driven similarity metric employed during the training phase. More importantly  our framework is flexible and complementary to other graph-based deep learning models  e.g.  Graph  Convolutional Networks (GCNs). We empirically evaluate our approach on a number of standard node classification benchmarks  and demonstrate that our model sets the new state of the art.,Rethinking Kernel Methods for

Node Representation Learning on Graphs

Yu Tianâˆ—

Rutgers University

Long Zhaoâˆ—

Rutgers University

yt219@cs.rutgers.edu

lz311@cs.rutgers.edu

Xi Peng

University of Delaware

xipeng@udel.edu

Dimitris N. Metaxas
Rutgers University

dnm@cs.rutgers.edu

Abstract

Graph kernels are kernel methods measuring graph similarity and serve as a stan-
dard tool for graph classiï¬cation. However  the use of kernel methods for node
classiï¬cation  which is a related problem to graph representation learning  is still
ill-posed and the state-of-the-art methods are heavily based on heuristics. Here  we
present a novel theoretical kernel-based framework for node classiï¬cation that can
bridge the gap between these two representation learning problems on graphs. Our
approach is motivated by graph kernel methodology but extended to learn the node
representations capturing the structural information in a graph. We theoretically
show that our formulation is as powerful as any positive semideï¬nite kernels. To
efï¬ciently learn the kernel  we propose a novel mechanism for node feature aggrega-
tion and a data-driven similarity metric employed during the training phase. More
importantly  our framework is ï¬‚exible and complementary to other graph-based
deep learning models  e.g.  Graph Convolutional Networks (GCNs). We empirically
evaluate our approach on a number of standard node classiï¬cation benchmarks 
and demonstrate that our model sets the new state of the art. The source code is
publicly available at https://github.com/bluer555/KernelGCN.

1

Introduction

Graph structured data  such as citation networks [11  22  30]  biological models [12  45]  grid-like
data [36  37  51] and skeleton-based motion systems [6  42  49  50]  are abundant in the real world.
Therefore  learning to understand graphs is a crucial problem in machine learning. Previous studies
in the literature generally fall into two main categories: (1) graph classiï¬cation [8  19  40  47  48] 
where the whole structure of graphs is captured for similarity comparison; (2) node classiï¬cation [1 
19  38  41  46]  where the structural identity of nodes is determined for representation learning.
For graph classiï¬cation  kernel methods  i.e.  graph kernels  have become a standard tool [20]. Given
a large collection of graphs  possibly with node and edge attributes  such algorithms aim to learn a
kernel function that best captures the similarity between any two graphs. The graph kernel function
can be utilized to classify graphs via standard kernel methods such as support vector machines or
k-nearest neighbors. Moreover  recent studies [40  47] also demonstrate that there has been a close
connection between Graph Neural Networks (GNNs) and the Weisfeiler-Lehman graph kernel [32] 
and relate GNNs to the classic graph kernel methods for graph classiï¬cation.

âˆ—indicates equal contributions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Overview of our kernel-based framework.

Node classiï¬cation  on the other hand  is still an ill-posed problem in representation learning on
graphs. Although identiï¬cation of node classes often leverages their features  a more challenging
and important scenario is to incorporate the graph structure for classiï¬cation. Recent efforts in
Graph Convolutional Networks (GCNs) [19] have made great progress on node classiï¬cation. In
particular  these efforts broadly follow a recursive neighborhood aggregation scheme to capture
structural information  where each node aggregates feature vectors of its neighbors to compute its
new features [1  41  46]. Empirically  these GCNs have achieved the state-of-the-art performance
on node classiï¬cation. However  the design of new GCNs is mostly based on empirical intuition 
heuristics  and experimental trial-and-error.
In this paper  we propose a novel theoretical framework leveraging kernel methods for node classiï¬-
cation. Motivated by graph kernels  our key idea is to decouple the kernel function so that it can be
learned driven by the node class labels on the graph. Meanwhile  its validity and expressive power
are guaranteed. To be speciï¬c  this paper makes the following contributions:
â€¢ We propose a learnable kernel-based framework for node classiï¬cation. The kernel function is
decoupled into a feature mapping function and a base kernel to ensure that it is valid as well as
learnable. Then we present a data-driven similarity metric and its corresponding learning criteria
for efï¬cient kernel training. The implementation of each component is extensively discussed. An
overview of our framework is shown in Fig. 1.

â€¢ We demonstrate the validity of our learnable kernel function. More importantly  we theoretically
show that our formulation is powerful enough to express any valid positive semideï¬nite kernels.
â€¢ A novel feature aggregation mechanism for learning node representations is derived from the per-
spective of kernel smoothing. Compared with GCNs  our model captures the structural information
of a node by aggregation in a single step  other than a recursive manner  thus is more efï¬cient.

â€¢ We discuss the close connection between the proposed approach and GCNs. We also show that our
method is ï¬‚exible and complementary to GCNs and their variants but more powerful  and can be
leveraged as a general framework for future work.

2 Related Work

Graph Kernels. Graph kernels are kernels deï¬ned on graphs to capture the graph similarity  which
can be used in kernel methods for graph classiï¬cation. Many graph kernels are instances of the
family of convolutional kernels [15]. Some of them measure the similarity between walks or paths on
graphs [4  39]. Other popular kernels are designed based on limited-sized substructures [18  33  31 
32]. Most graph kernels are employed in models which have learnable components  but the kernels
themselves are hand-crafted and motivated by graph theory. Some learnable graph kernels have been
proposed recently  such as Deep Graph Kernels [43] and Graph Matching Networks [21]. Compared
to these approaches  our method targets at learning kernels for node representation learning.
Node Representation Learning. Conventional methods for learning node representations largely
focus on matrix factorization. They directly adopt classic techniques for dimension reduction [2  3].
Other methods are derived from the random walk algorithm [23  26] or sub-graph structures [13 
35  44  28]. Recently  Graph Convolutional Networks (GCNs) have emerged as an effective class of

2

InputNodesFeatureMappingğ‘”"InnerProductSpaceLossâ„’InferenceLearnableKernelğ¾"Base	ğŠğğ«ğ§ğğ¥	ğ‘˜ -./âŠ•models for learning representations of graph structured data. They were introduced in [19]  which
consist of an iterative process aggregating and transforming representation vectors of its neighboring
nodes to capture structural information. Recently  several variants have been proposed  which employ
self-attention mechanism [38] or improve network architectures [41  46] to boost the performance.
However  most of them are based on empirical intuition and heuristics.

3 Preliminaries

We begin by summarizing some of the most important concepts about kernel methods as well as
representation learning on graphs and  along the way  introduce our notations.
Kernel Concepts. A kernel K : X Ã— X (cid:55)â†’ R is a function of two arguments: K(x  y) for x  y âˆˆ X .
The kernel function K is symmetric  i.e.  K(x  y) = K(y  x)  which means it can be interpreted as
a measure of similarity. If the Gram matrix K âˆˆ RNÃ—N deï¬ned by K(i  j) = K(xi  xj) for any
{xi}N
i=1 is positive semideï¬nite (p.s.d.)  then K is a p.s.d. kernel [24]. If K(x  y) can be represented
as (cid:104)Î¨(x)  Î¨(y)(cid:105)  where Î¨ : X (cid:55)â†’ RD is a feature mapping function  then K is a valid kernel.
Graph Kernels. In the graph space G  we denote a graph as G = (V  E)  where V is the set of nodes
and E is the edge set of G. Given two graphs Gi = (Vi  Ei) and Gj = (Vj  Ej) in G  the graph
kernel KG(Gi  Gj) measures the similarity between them. According to the deï¬nition in [29]  the
kernel KG must be p.s.d. and symmetric. The graph kernel KG between Gi and Gj is deï¬ned as:

KG(Gi  Gj) =

kbase(f (vi)  f (vj)) 

(1)
where kbase is the base kernel for any pair of nodes in Gi and Gj  and f : V (cid:55)â†’ â„¦ is a function
to compute the feature vector associated with each node. However  deriving a new p.s.d. graph
kernel is a non-trivial task. Previous methods often implement kbase and f as the dot product between
hand-crafted graph heuristics [25  31  4]. There are little learnable parameters in these approaches.
Representation Learning on Graphs. Although graph kernels have been applied to a wide range
of applications  most of them depend on hand-crafted heuristics. In contrast  representation learning
aims to automatically learn to encode graph structures into low-dimensional embeddings. Formally 
given a graph G = (V  E)  we follow [14] to deï¬ne representation learning as an encoder-decoder
framework  where we minimize the empirical loss L over a set of training node pairs D âŠ† V Ã— V :
(2)

(cid:96)(ENC-DEC(vi  vj)  sG(vi  vj)).

(cid:88)

L =

(cid:88)

(cid:88)

viâˆˆVi

vjâˆˆVj

(vi vj )âˆˆD

Equation (2) has three methodological components: ENC-DEC  sG and (cid:96). Most of the previous
methods on representation learning can be distinguished by how these components are deï¬ned. The
detailed meaning of each component is explained as follows.
â€¢ ENC-DEC : V Ã— V (cid:55)â†’ R is an encoder-decoder function. It contains an encoder which projects
each node into a M-dimensional vector to generate the node embedding. This function contains
a number of trainable parameters to be optimized during the training phase. It also includes a
decoder function  which reconstructs pairwise similarity measurements from the node embeddings
generated by the encoder.
â€¢ sG is a pairwise similarity function deï¬ned over the graph G. This function is user-speciï¬ed  and
â€¢ (cid:96) : R Ã— R (cid:55)â†’ R is a loss function  which is leveraged to train the model. This function evaluates
the quality of the pairwise reconstruction between the estimated value ENC-DEC(vi  vj) and the
true value sG(vi  vj).

it is used for measuring the similarity between nodes in G.

4 Proposed Method: Learning Kernels for Node Representation

Given a graph G  as we can see from Eq. (2)  the encoder-decoder ENC-DEC aims to approximate
the pairwise similarity function sG  which leads to a natural intuition: we can replace ENC-DEC
with a kernel function KÎ¸ parameterized by Î¸ to measure the similarity between nodes in G  i.e. 

(cid:88)

L =

(vi vj )âˆˆD

(cid:96)(KÎ¸(vi  vj)  sG(vi  vj)).

(3)

3

However  there exist two technical challenges: (1) designing a valid p.s.d. kernel which captures the
node feature is non-trivial; (2) it is impossible to handcraft a uniï¬ed kernel to handle all possible
graphs with different characteristics [27]. To tackle these issues  we introduce a novel formulation to
replace KÎ¸. Inspired by the graph kernel as deï¬ned in Eq. (1) and the mapping kernel framework [34] 
our key idea is to decouple KÎ¸ into two components: a base kernel kbase which is p.s.d. to maintain
the validity  and a learnable feature mapping function gÎ¸ to ensure the ï¬‚exibility of the resulting
kernel. Therefore  we rewrite Eq. (3) by KÎ¸(vi  vj) = kbase(gÎ¸(vi)  gÎ¸(vj)) for vi  vj âˆˆ V of the
graph G to optimize the following objective:

(cid:96)(kbase(gÎ¸(vi)  gÎ¸(vj))  sG(vi  vj)).

(4)

(cid:88)

L =

(vi vj )âˆˆD

Theorem 1 demonstrates that the proposed formulation  i.e.  KÎ¸(vi  vj) = kbase(gÎ¸(vi)  gÎ¸(vj))  is
still a valid p.s.d. kernel for any feature mapping function gÎ¸ parameterized by Î¸.
Theorem 1. Let gÎ¸ : V (cid:55)â†’ RM be a function which maps nodes (or their corresponding features)
to a M-dimensional Euclidean space. Let kbase : RM Ã— RM (cid:55)â†’ R be any valid p.s.d. kernel. Then 
KÎ¸(vi  vj) = kbase(gÎ¸(vi)  gÎ¸(vj)) is a valid p.s.d. kernel.

Proof. Let Î¦ be the corresponding feature mapping function of the p.s.d. kernel kbase. Then  we
have kbase(zi  zj) = (cid:104)Î¦(zi)  Î¦(zj)(cid:105)  where zi  zj âˆˆ RM . Substitute gÎ¸(vi)  gÎ¸(vj) for zi  zj  and
we have kbase(gÎ¸(vi)  gÎ¸(vj)) = (cid:104)Î¦(gÎ¸(vi))  Î¦(gÎ¸(vj))(cid:105). Write the new feature mapping Î¨(v) as
Î¨(v) = Î¦(gÎ¸(v))  and we immediately have that kbase(gÎ¸(vi)  gÎ¸(vj)) = (cid:104)Î¨(vi)  Î¨(vj)(cid:105). Hence 
kbase(gÎ¸(vi)  gÎ¸(vj)) is a valid p.s.d. kernel.

A natural follow-up question is whether our proposed formulation  in principle  is powerful enough to
express any valid p.s.d. kernels? Our answer  in Theorem 2  is yes: if the base kernel has an invertible
feature mapping function  then the resulting kernel is able to model any valid p.s.d. kernels.
Theorem 2. Let K(vi  vj) be any valid p.s.d. kernel for node pairs (vi  vj) âˆˆ V Ã— V . Let kbase :
RM Ã— RM (cid:55)â†’ R be a p.s.d. kernel which has an invertible feature mapping function Î¦. Then there
exists a feature mapping function gÎ¸ : V (cid:55)â†’ RM   such that K(vi  vj) = kbase(gÎ¸(vi)  gÎ¸(vj)).

Proof. Let Î¨ be the corresponding feature mapping function of the p.s.d. kernel K  and then we have
K(vi  vj) = (cid:104)Î¨(vi)  Î¨(vj)(cid:105). Similarly  for zi  zj âˆˆ RM   we have kbase(zi  zj) = (cid:104)Î¦(zi)  Î¦(zj)(cid:105).
Substitute gÎ¸(v) for z  and then it is easy to see that gÎ¸(v) = (Î¦âˆ’1 â—¦ Î¨)(v) is the desired feature
mapping function when Î¦âˆ’1 exists.

4.1

Implementation and Learning Criteria

Theorems 1 and 2 have demonstrated the validity and power of the proposed formulation in Eq. (4).
In this section  we discuss how to implement and learn gÎ¸  kbase  sG and (cid:96)  respectively.
Implementation of the Feature Mapping Function gÎ¸. The function gÎ¸ aims to project the feature
vector xv of each node v into a better space for similarity measurement. Our key idea is that in a
graph  connected nodes usually share some similar characteristics  and thus changes between nearby
nodes in the latent space of nodes should be smooth. Inspired by the concept of kernel smoothing 
we consider gÎ¸ as a feature smoother which maps xv into a smoothed latent space according to the
graph structure. The kernel smoother estimates a function as the weighted average of neighboring
observed data. To be speciï¬c  given a node v âˆˆ V   according to Nadaraya-Watson kernel-weighted
average [10]  a feature smoothing function is deï¬ned as:

(cid:80)
(cid:80)

g(v) =

uâˆˆV k(u  v)p(u)

uâˆˆV k(u  v)

 

(5)

where p is a mapping function to compute the feature vector of each node  and here we let p(v) = xv;
k is a pre-deï¬ned kernel function to capture pairwise relations between nodes. Note that we omit Î¸
for g here since there are no learnable parameters in Eq. (5). In the context of graphs  the natural
choice of computing k is to follow the graph structure  i.e.  the structural information within the
nodeâ€™s h-hop neighborhood.

4

(cid:80)

2 ADâˆ’ 1

To compute g  we let A be the adjacent matrix of the given graph G and I be the identity matrix
with the same size. We notice that I + Dâˆ’ 1
2 is a valid p.s.d. matrix  where D(i  i) =
j A(i  j). Thus we can employ this matrix to deï¬ne the kernel function k. However  in practice 
this matrix would lead to numerical instabilities and exploding or vanishing gradients when used for
training deep neural networks. To alleviate this problem  we adopt the renormalization trick [19]:
I + Dâˆ’ 1
ËœA(i  j). Then the
h-hop neighborhood can be computed directly from the h power of Â¯A  i.e.  Â¯Ah. And the kernel k for
node pairs vi  vj âˆˆ V is computed as k(vi  vj) = Â¯Ah(i  j). After collecting the feature vector xv of
each node v âˆˆ V into a matrix XV   we rewrite Eq. (5) approximately into its matrix form:

2   where ËœA = A + I and ËœD(i  i) =(cid:80)

2 â†’ Â¯A = ËœDâˆ’ 1

2 ADâˆ’ 1

2 ËœA ËœDâˆ’ 1

j

g(V ) â‰ˆ Â¯AhXV .

(6)

Next  we enhance the expressive power of Eq. (6) to model any valid p.s.d. kernels by implementing
it with deep neural networks based on the following two aspects. First  we make use of multi-layer
perceptrons (MLPs) to model and learn the composite function Î¦âˆ’1 â—¦ Î¨ in Theorem 2  thanks to the
universal approximation theorem [16  17]. Second  we add learnable weights to different hops of
node neighbors. As a result  our ï¬nal feature mapping function gÎ¸ is deï¬ned as:

(cid:32)(cid:88)

Ï‰h Â·(cid:16) Â¯Ah (cid:12) M(h)(cid:17)(cid:33)

gÎ¸(V ) =

h

Â· MLP(l)(XV ) 

(7)

where Î¸ means the set of parameters in gÎ¸; Ï‰h is a learnable parameter for the h-hop neighborhood
of each node v; (cid:12) is the Hadamard (element-wise) product; M(h) is an indicator matrix where
M(h)(i  j) equals to 1 if vj is a h-th hop neighbor of vi and 0 otherwise. The hyperparameter l
controls the number of layers in the MLP.
Equation (7) can be interpreted as a weighted feature aggregation schema around the given node v
and its neighbors  which is employed to compute the node representation. It has a close connection
with Graph Neural Networks. We leave it in Section 5 for a more detailed discussion.
Implementation of the Base Kernel kbase. As we have shown in Theorem 2  in order to model
an arbitrary p.s.d. kernel  we require that the corresponding feature mapping function Î¦ of the
base kernel kbase must be invertible  i.e.  Î¦âˆ’1 exists. An obvious choice would let Î¦ be an identity
function  then kbase will reduce to the dot product between nodes in the latent space. Since gÎ¸ maps
node representations to a ï¬nite dimensional space  the identity function makes our model directly
measure the node similarity in this space. On the other hand  an alternative choice of kbase is the RBF
kernel which additionally projects node representations to an inï¬nite dimensional latent space before
comparison. We compare both implementations in the experiments for further evaluation.
Data-Driven Similarity Metric sG and Criteria (cid:96). In node classiï¬cation  each node vi âˆˆ V is
associated with a class label yi âˆˆ Y . We aim to measure node similarity with respect to their class
labels other than hand-designed metrics. Naturally  we deï¬ne the pairwise similarity sG as:

sG(vi  vj) =

âˆ’1

if yi = yj
o/w

(8)

However  in practice  it is hard to directly minimize the loss between KÎ¸ and sG in Eq. (8). Instead 
we consider a â€œsoftâ€ version of sG  where we require that the similarity of node pairs with the
same label is greater than those with distinct labels by a margin. Therefore  we train the kernel
KÎ¸(vi  vj) = kbase(gÎ¸(vi)  gÎ¸(vj)) to minimize the following objective function on triplets:

(cid:96)(KÎ¸(vi  vj)  KÎ¸(vi  vk)) 

(9)

(cid:88)

LK =

(vi vj  vk)âˆˆT

where T âŠ† V Ã— V Ã— V is a set of node triplets: vi is an anchor  and vj is a positive of the same class
as the anchor while vk is a negative of a different class. The loss function (cid:96) is deï¬ned as:

(cid:96)(KÎ¸(vi  vj)  KÎ¸(vi  vk)) = [KÎ¸(vi  vk) âˆ’ KÎ¸(vi  vj) + Î±]+.

(10)
It ensures that given two positive nodes of the same class and one negative node  the kernel value of
the negative should be farther away than the one of the positive by the margin Î±. Here  we present
Theorem 3 and its proof to show that minimizing Eq. (9) leads to KÎ¸ = sG.

(cid:26)1

5

Theorem 3. If |KÎ¸(vi  vj)| â‰¤ 1 for any vi  vj âˆˆ V   minimizing Eq. (9) with Î± = 2 yields KÎ¸ = sG.
Proof. Let (vi  vj  vk) be all triplets satisfying yi = yj  yi (cid:54)= yk. Suppose that for Î± = 2  Eq. (10)
holds for all (vi  vj  vk). It means KÎ¸(vi  vk) + 2 â‰¤ KÎ¸(vi  vj) for all (vi  vj  vk). As |KÎ¸(vi  vj)| â‰¤
1  we have KÎ¸(vi  vk) = âˆ’1 for all (vi  vk) and KÎ¸(vi  vj) = 1 for all (vi  vj). Hence  KÎ¸ = sG.
We note that |KÎ¸(vi  vj)| â‰¤ 1 can be simply achieved by letting kbase be the dot product and
normalizing all gÎ¸ to the norm ball. In the following sections  the normalized KÎ¸ is denoted by Â¯KÎ¸.

4.2

Inference for Node Classiï¬cation

Once the kernel function KÎ¸(vi  vj) = kbase(gÎ¸(vi)  gÎ¸(vj)) has learned how to measure the simi-
larity between nodes  we can leverage the output of the feature mapping function gÎ¸ as the node
representation for node classiï¬cation. In this paper  we introduce the following two classiï¬ers.
Nearest Centroid Classiï¬er. The nearest centroid classiï¬er extends the k-nearest neighbors algo-
rithm by assigning to observations the label of the class of training samples whose centroid is closest
(cid:80)
to the observation. It does not require additional parameters. To be speciï¬c  given a testing node
u  for all nodes vi with class label yi âˆˆ Y in the training set  we compute the per-class average
Â¯KÎ¸(u  vi)  where Vy is the set of nodes belonging to
similarity between u and vi: Âµy = 1|Vy|
class y âˆˆ Y . Then the class assigned to the testing node u:
yâˆ— = arg maxyâˆˆY Âµy.

viâˆˆVy

(11)

Softmax Classiï¬er. The idea of the softmax classiï¬er is to reuse the ground truth labels of nodes
for training the classiï¬er  so that it can be directly employed for inference. To do this  we add the
softmax activation Ïƒ after gÎ¸(vi) to minimize the following objective:

q(yi) log(Ïƒ(gÎ¸(vi))) 

(12)

LY = âˆ’ (cid:88)

viâˆˆV

where q(yi) is the one-hot ground truth vector. Note that Eq. (12) is optimized together with Eq. (9)
in an end-to-end manner. Let Î¨ denote the corresponding feature mapping function of KÎ¸  then
we have KÎ¸(vi  vj) = (cid:104)Î¨(vi)  Î¨(vj)(cid:105) = kbase(gÎ¸(vi)  gÎ¸(vj)). In this case  we use the node feature
produced by Î¨ for classiï¬cation since Î¨ projects node features into the dot-product space which is a
natural metric for similarity comparison. To this end  kbase is ï¬xed to be the identity function for the
softmax classiï¬er  so that we have (cid:104)Î¨(vi)  Î¨(vj)(cid:105) = (cid:104)gÎ¸(vi)  gÎ¸(vj)(cid:105) and thus Î¨(vi) = gÎ¸(vi).

5 Discussion

(cid:16) Â¯AH(l)W(l)(cid:17)

Our feature mapping function gÎ¸ proposed in Eq. (7) has a close connection with Graph Convolutional
Networks (GCNs) [19] in the way of capturing node latent representations. In GCNs and most of
their variants  each layer leverages the following aggregation rule:

 

H(l+1) = Ï

(13)
where W(l) is a layer-speciï¬c trainable weighting matrix; Ï denotes an activation function; H(l) âˆˆ
RNÃ—D denotes the node features in the l-th layer  and H0 = X. Through stacking multiple layers 
GCNs aggregate the features for each node from its L-hop neighbors recursively  where L is the
network depth. Compared with the proposed gÎ¸  GCNs actually interleave two basic operations of gÎ¸:
feature transformation and Nadaraya-Watson kernel-weighted average  and repeat them recursively.
We contrast our approach with GCNs in terms of the following aspects. First  our aggregation function
is derived from the kernel perspective  which is novel. Second  we show that aggregating features
in a recursive manner is inessential. Powerful h-hop node representations can be obtained by our
model where aggregation is performed only once. As a result  our approach is more efï¬cient both in
storage and time when handling very large graphs  since no intermediate states of the network have
to be kept. Third  our model is ï¬‚exible and complementary to GCNs: our function gÎ¸ can be directly
replaced by GCNs and other variants  which can be exploited for future work.

6

Time and Space Complexity. We assume the number of features F is ï¬xed for all layers and both
GCNs and our method have L â‰¥ 2 layers. We count matrix multiplications as in [7]. GCNâ€™s time
complexity is O(L(cid:107) Â¯A(cid:107)0F + L|V |F 2)  where (cid:107) Â¯A(cid:107)0 is the number of nonzeros of Â¯A and |V | is the
number of nodes in the graph. While ours is O((cid:107) Â¯Ah(cid:107)0F + L|V |F 2)  since we do not aggregate
features recursively. Obviously  (cid:107) Â¯Ah(cid:107)0 is constant but L(cid:107) Â¯A(cid:107)0 is linear to L. For space complexity 
GCNs have to store all the feature matrices for recursive aggregation which needs O(L|V |F + LF 2)
space  where LF 2 is for storing trainable parameters of all layers  and thus the ï¬rst term is linear
to L. Instead  ours is O(|V |F + LF 2) where the ï¬rst term is again constant to L. Our experiments
indicate that we save 20% (0.3 ms) time and 15% space on Cora dataset [22] than GCNs.

6 Experiments

We evaluate the proposed kernel-based approach on three benchmark datasets: Cora [22]  Citeseer [11]
and Pubmed [30]. They are citation networks  where the task of node classiï¬cation is to classify
academic papers of the network (graph) into different subjects. These datasets contain bag-of-words
features for each document (node) and citation links between documents.
We compare our approach to ï¬ve state-of-the-art methods: GCN [19]  GAT [38]  FastGCN [5] 
JK [41] and KLED [9]. KLED is a kernel-based method  while the others are based on deep neural
networks. We test all methods in the supervised learning scenario  where all data in the training
set are used for training. We evaluate the proposed method in two different experimental settings
according to FastGCN [5] and JK [41]  respectively. The statistics of the datasets together with
their data split settings (i.e.  the number of samples contained in the training  validation and testing
sets  respectively) are summarized in Table 1. Note that there are more training samples in the data
split of JK [41] than FastGCN [5]. We report the average means and standard deviations of node
classiï¬cation accuracy which are computed from ten runs as the evaluation metrics.

Table 1: Overview of the three evaluation datasets under two different data split settings.

Nodes Edges Classes Features Data split of FastGCN [5] Data split of JK [41]
Dataset
Cora [22]
2 708 5 429
Citeseer [11] 3 327 4 732
Pubmed [30] 19 717 44 338

1 208 / 500 / 1 000
1 827 / 500 / 1 000
18 217 / 500 / 1 000

1 433
3 703
500

7
6
3

1 624 / 542 / 542
1 997 / 665 / 665

-

6.1 Variants of the Proposed Method

As we have shown in Section 4.1  there are alternative choices to implement each component of our
framework. In this section  we summarize all the variants of our method employed for evaluation.
Choices of the Feature Mapping Function g. We implement the feature mapping function gÎ¸
according to Eq. (7). In addition  we also choose GCN and GAT as the alternative implementations
of gÎ¸ for comparison  and denote them by gGCN and gGAT  respectively.
Choices of the Base Kernel kbase. The base kernel kbase has two different implementations: the dot
product which is denoted by k(cid:104)Â· Â·(cid:105)  and the RBF kernel which is denoted by kRBF. Note that when the
softmax classiï¬er is employed  we set the base kernel to be k(cid:104)Â· Â·(cid:105).
Choices of the Loss L and Classiï¬er C. We consider the following three combinations of the loss
function and classiï¬er. (1) LK in Eq. (9) is optimized  and the nearest-centroid classiï¬er CK is
employed for classiï¬cation. This combination aims to evaluate the effectiveness of the learned kernel.
(2) LY in Eq. (12) is optimized  and the softmax classiï¬er CY is employed for classiï¬cation. This
combination is used in a baseline without kernel methods. (3) Both Eq. (9) and Eq. (12) are optimized 
and we denote this loss by LK+Y . The softmax classiï¬er CY is employed for classiï¬cation. This
combination aims to evaluate how the learned kernel improves the baseline method.
In the experiments  we use K to denote kernel-based variants and N to denote ones without the kernel
function. All these variants are implemented by MLPs with two layers. Due to the space limitation 
we ask the readers to refer to the supplementary material for implementation details.

7

6.2 Results of Node Classiï¬cation

The means and standard deviations of node classiï¬cation accuracy (%) following the setting of
FastGCN [5] are organized in Table 2. Our variant of K3 sets the new state of the art on all datasets.
And on Pubmed dataset  all our variants improve previous methods by a large margin. It proves the
effectiveness of employing kernel methods for node classiï¬cation  especially on datasets with large
graphs. Interestingly  our non-kernel baseline N1 even achieves the state-of-the-art performance 
which shows that our feature mapping function can capture more ï¬‚exible structural information than
previous GCN-based approaches. For the choice of the base kernel  we can ï¬nd that K2 outperforms
K1 on two large datasets: Citeseer and Pubmed. We conjecture that when handling complex datasets 
the non-linear kernel  e.g.  the RBF kernel  is a better choice than the liner kernel.
To evaluate the performance of our feature mapping function  we report the results of two variants
Kâˆ—
1 and Kâˆ—
2 in Table 2. They utilize GCN and GAT as the feature mapping function respectively. As
expected  our K1 outperforms Kâˆ—
2 among most datasets. This demonstrates that the recursive
aggregation schema of GCNs is inessential  since the proposed gÎ¸ aggregates features only in a single
step  which is still powerful enough for node classiï¬cation. On the other hand  it is also observed
that both Kâˆ—
2 outperform their original non-kernel based implementations  which shows that
learning with kernels yields better node representations.
Table 3 shows the results following the setting of JK [41]. Note that we do not evaluate on Pubmed in
this setup since its corresponding data split for training and evaluation is not provided by [41]. As
expected  our method achieves the best performance among all datasets  which is consistent with the
results in Table 2. For Cora  the improvement of our method is not so signiï¬cant. We conjecture
that the results in Table 3 involve more training data due to different data splits  which narrows the
performance gap between different methods on datasets with small graphs  such as Cora.

1 and Kâˆ—

1 and Kâˆ—

Table 2: Accuracy (%) of node classiï¬cation following the setting of FastGCN [5].

Method
KLED [9]
GCN [19]
GAT [38]
FastGCN [5]
K1 = {k(cid:104)Â· Â·(cid:105)  gÎ¸ LK CK}
K2 = {kRBF  gÎ¸ LK CK}
K3 = {k(cid:104)Â· Â·(cid:105)  gÎ¸ LK+Y  CY }
N1 = {gÎ¸ LY  CY }
Kâˆ—
1 = {k(cid:104)Â· Â·(cid:105)  gGCN LK CK}
Kâˆ—
2 = {k(cid:104)Â· Â·(cid:105)  gGAT LK CK}

Cora [22]

Citeseer [11]

Pubmed [30]

82.3
86.0
85.6
85.0

86.68 Â± 0.17
86.12 Â± 0.05
88.40 Â± 0.24
87.56 Â± 0.14
87.04 Â± 0.09
86.10 Â± 0.33

-

77.2
76.9
77.6

77.92 Â± 0.25
78.68 Â± 0.38
80.28 Â± 0.03
79.80 Â± 0.03
77.12 Â± 0.23
77.92 Â± 0.19

82.3
86.5
86.2
88.0

89.22 Â± 0.17
89.36 Â± 0.21
89.42 Â± 0.01
89.24 Â± 0.14
87.84 Â± 0.12

-

Table 3: Accuracy (%) of node classiï¬cation following the setting of JK [41].

Method
GCN [19]
GAT [38]
JK-Concat [41]
K3 = {k(cid:104)Â· Â·(cid:105)  gÎ¸ LK+Y  CY }

Cora [22]
88.20 Â± 0.70
87.70 Â± 0.30
89.10 Â± 1.10
89.24 Â± 0.31

Citeseer [11]
77.30 Â± 1.30
76.20 Â± 0.80
78.30 Â± 0.80
80.78 Â± 0.28

6.3 Ablation Study on Node Feature Aggregation Schema
In Table 4  we implement three variants of K3 (2-hop and 2-layer with Ï‰h by default) to evaluate
the proposed node feature aggregation schema. We answer the following three questions. (1) How
does performance change with fewer (or more) hops? We change the number of hops from 1 to 3 
and the performance improves if it is larger  which shows capturing long-range structures of nodes is
important. (2) How many layers of MLP are needed? We show results with different layers ranging
from 1 to 3. The best performance is obtained with two layers  while networks overï¬t the data when

8

more layers are employed. (3) Is it necessary to have a trainable parameter Ï‰h? We replace Ï‰h with
a ï¬xed constant ch  where c âˆˆ (0  1]. We can see larger c improves the performance. However  all
results are worse than learning a weighting parameter Ï‰h  which shows the importance of it.
Table 4: Results of accuracy (%) with different settings of the aggregation schema.

Variants of K3
Default
1-hop
3-hop
1-layer
3-layer
c = 0.25
c = 0.50
c = 0.75
c = 1.00

Cora [22]
88.40 Â± 0.24
85.56 Â± 0.02
88.25 Â± 0.01
82.60 Â± 0.01
86.33 Â± 0.04
69.33 Â± 0.09
76.98 Â± 0.10
84.25 Â± 0.01
87.31 Â± 0.01

Citeseer [11]
80.28 Â± 0.03
77.73 Â± 0.02
80.13 Â± 0.01
77.63 Â± 0.01
78.53 Â± 0.20
74.48 Â± 0.03
77.47 Â± 0.04
77.99 Â± 0.01
78.57 Â± 0.01

Pubmed [30]
89.42 Â± 0.01
88.98 Â± 0.01
89.53 Â± 0.01
85.80 Â± 0.01
89.46 Â± 0.05
84.68 Â± 0.02
86.45 Â± 0.01
87.45 Â± 0.01
88.68 Â± 0.01

6.4

t-SNE Visualization of Node Embeddings

We visualize the node embeddings of GCN  GAT and our method on Citeseer with t-SNE. For
our method  we use the embedding of K3 which obtains the best performance. Figure 2 illustrates
the results. Compared with other methods  our method produces a more compact clustering result.
Speciï¬cally our method clusters the â€œredâ€ points tightly  while in the results of GCN and GAT  they
are loosely scattered into other clusters. This is caused by the fact that both GCN and GAT minimize
the classiï¬cation loss LY   only targeting at accuracy. They tend to learn node embeddings driven
by those classes with the majority of nodes. In contrast  K3 are trained with both LK and LY . Our
kernel-based similarity loss LK encourages data within the same class to be close to each other. As a
result  the learned feature mapping function gÎ¸ encourages geometrically compact clusters.

Figure 2: t-SNE visualization of node embeddings on Citeseer dataset.

Due to the space limitation  we ask the readers to refer to the supplementary material for more
experiment results  such as the results of link prediction and visualization on other datasets.

7 Conclusions

In this paper  we introduce a kernel-based framework for node classiï¬cation. Motivated by the
design of graph kernels  we learn the kernel from ground truth labels by decoupling the kernel
function into a base kernel and a learnable feature mapping function. More importantly  we show
that our formulation is valid as well as powerful enough to express any p.s.d. kernels. Then the
implementation of each component in our approach is extensively discussed. From the perspective
of kernel smoothing  we also derive a novel feature mapping function to aggregate features from a
nodeâ€™s neighborhood. Furthermore  we show that our formulation is closely connected with GCNs but
more powerful. Experiments on standard node classiï¬cation benchmarks are conducted to evaluated
our approach. The results show that our method outperforms the state of the art.

9

(a) GCN(c) Ours(b) GATAcknowledgments

This work is funded by ARO-MURI-68985NSMUR and NSF 1763523  1747778  1733843  1703883.

References
[1] S. Abu-El-Haija  A. Kapoor  B. Perozzi  and J. Lee. N-GCN: Multi-scale graph convolution for semi-

supervised node classiï¬cation. arXiv preprint arXiv:1802.08888  2018.

[2] A. Ahmed  N. Shervashidze  S. Narayanamurthy  V. Josifovski  and A. J. Smola. Distributed large-scale
natural graph factorization. In Proceedings of the International Conference on World Wide Web (WWW) 
pages 37â€“48  2013.

[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In

Advances in Neural Information Processing Systems (NeurIPS)  pages 585â€“591  2002.

[4] K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs.

International Conference on Data Mining (ICDM)  2005.

In Proceedings of the IEEE

[5] J. Chen  T. Ma  and C. Xiao. Fastgcn: fast learning with graph convolutional networks via importance

sampling. arXiv preprint arXiv:1801.10247  2018.

[6] Y. Chen  L. Zhao  X. Peng  J. Yuan  and D. N. Metaxas. Construct dynamic graphs for hand gesture
recognition via spatial-temporal attention. In Proceedings of the British Machine Vision Conference
(BMVC)  2019.

[7] W.-L. Chiang  X. Liu  S. Si  Y. Li  S. Bengio  and C.-J. Hsieh. Cluster-GCN: An efï¬cient algorithm for
training deep and large graph convolutional networks. In Proceedings of the ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD)  pages 257â€“266  2019.

[8] M. Draief  K. Kutzkov  K. Scaman  and M. Vojnovic. KONG: Kernels for ordered-neighborhood graphs.

In Advances in Neural Information Processing Systems (NeurIPS)  pages 4051â€“4060  2018.

[9] F. Fouss  L. Yen  A. Pirotte  and M. Saerens. An experimental investigation of graph kernels on a
collaborative recommendation task. In Proceedings of the International Conference on Data Mining
(ICDM)  pages 863â€“868  2006.

[10] J. Friedman  T. Hastie  and R. Tibshirani. The elements of statistical learning. Springer series in statistics

New York  2001.

[11] C. L. Giles  K. D. Bollacker  and S. Lawrence. Citeseer: An automatic citation indexing system. In

Proceedings of the Third ACM Conference on Digital Libraries  pages 89â€“98  1998.

[12] J. Gilmer  S. S. Schoenholz  P. F. Riley  O. Vinyals  and G. E. Dahl. Neural message passing for quantum
chemistry. In Proceedings of the International Conference on Machine Learning (ICML)  pages 1263â€“1272 
2017.

[13] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)  pages 855â€“864 
2016.

[14] W. L. Hamilton  R. Ying  and J. Leskovec. Representation learning on graphs: Methods and applications.

arXiv preprint arXiv:1709.05584  2017.

[15] D. Haussler. Convolution kernels on discrete structures. Technical report  Department of Computer Science 

University of California at Santa Cruz  1999.

[16] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks  4(2):251â€“257 

1991.

[17] K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal approximators.

Neural networks  2(5):359â€“366  1989.

[18] T. HorvÃ¡th  T. GÃ¤rtner  and S. Wrobel. Cyclic pattern kernels for predictive graph mining. In Proceedings
of the ACM SIGKDD International Conference on Knowledge discovery and Data Mining (KDD)  pages
158â€“167  2004.

[19] T. N. Kipf and M. Welling. Semi-supervised classiï¬cation with graph convolutional networks.

Proceedings of the International Conference on Learning Representations (ICLR)  2017.

In

10

[20] N. M. Kriege  M. Neumann  C. Morris  K. Kersting  and P. Mutzel. A unifying view of explicit and implicit
feature maps for structured data: systematic studies of graph kernels. arXiv preprint arXiv:1703.00676 
2017.

[21] Y. Li  C. Gu  T. Dullien  O. Vinyals  and P. Kohli. Graph matching networks for learning the similarity of
graph structured objects. In Proceedings of the International Conference on Machine Learning (ICML) 
2019.

[22] A. K. McCallum  K. Nigam  J. Rennie  and K. Seymore. Automating the construction of internet portals

with machine learning. Information Retrieval  3(2):127â€“163  2000.

[23] T. Mikolov  I. Sutskever  K. Chen  G. S. Corrado  and J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in Neural Information Processing Systems (NeurIPS) 
pages 3111â€“3119  2013.

[24] K. P. Murphy. Machine learning: a probabilistic perspective. MIT press  2012.

[25] M. Neuhaus and H. Bunke. Self-organizing maps for learning the edit costs in graph matching. IEEE

Transactions on Systems  Man  and Cybernetics  Part B (Cybernetics)  35(3):503â€“514  2005.

[26] B. Perozzi  R. Al-Rfou  and S. Skiena. Deepwalk: Online learning of social representations. In Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)  pages
701â€“710  2014.

[27] J. Ramon and T. GÃ¤rtner. Expressivity versus efï¬ciency of graph kernels. In Proceedings of the International

Workshop on Mining Graphs  Trees and Sequences  pages 65â€“74  2003.

[28] L. F. Ribeiro  P. H. Saverese  and D. R. Figueiredo. struc2vec: Learning node representations from
structural identity. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (KDD)  pages 385â€“394  2017.

[29] B. Scholkopf and A. J. Smola. Learning with kernels: support vector machines  regularization  optimization 

and beyond. MIT press  2001.

[30] P. Sen  G. Namata  M. Bilgic  L. Getoor  B. Galligher  and T. Eliassi-Rad. Collective classiï¬cation in

network data. AI magazine  29(3):93â€“93  2008.

[31] N. Shervashidze and K. Borgwardt. Fast subtree kernels on graphs. In Advances in Neural Information

Processing Systems (NeurIPS)  pages 1660â€“1668  2009.

[32] N. Shervashidze  P. Schweitzer  E. J. v. Leeuwen  K. Mehlhorn  and K. M. Borgwardt. Weisfeiler-lehman

graph kernels. Journal of Machine Learning Research  12:2539â€“2561  2011.

[33] N. Shervashidze  S. Vishwanathan  T. Petri  K. Mehlhorn  and K. Borgwardt. Efï¬cient graphlet kernels
for large graph comparison. In Proceedings of the International Conference on Artiï¬cial Intelligence and
Statistics (AISTATS)  pages 488â€“495  2009.

[34] K. Shin and T. Kuboyama. A generalization of hausslerâ€™s convolution kernel: mapping kernel.

Proceedings of the International Conference on Machine Learning (ICML)  pages 944â€“951  2008.

In

[35] J. Tang  M. Qu  M. Wang  M. Zhang  J. Yan  and Q. Mei. Line: Large-scale information network embedding.

In Proceedings of the International Conference on World Wide Web (WWW)  pages 1067â€“1077  2015.

[36] Z. Tang  X. Peng  S. Geng  L. Wu  S. Zhang  and D. Metaxas. Quantized Densely Connected U-Nets for
Efï¬cient Landmark Localization. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 339â€“354  2018.

[37] Y. Tian  X. Peng  L. Zhao  S. Zhang  and D. N. Metaxas. CR-GAN: learning complete representations
for multi-view generation. In Proceedings of the International Joint Conference on Artiï¬cial Intelligence
(IJCAI)  pages 942â€“948  2018.

[38] P. VeliË‡ckoviÂ´c  G. Cucurull  A. Casanova  A. Romero  P. Lio  and Y. Bengio. Graph attention networks. In

Proceedings of the International Conference on Learning Representations (ICLR)  2018.

[39] S. V. N. Vishwanathan  N. N. Schraudolph  R. Kondor  and K. M. Borgwardt. Graph kernels. Journal of

Machine Learning Research  11(Apr):1201â€“1242  2010.

[40] K. Xu  W. Hu  J. Leskovec  and S. Jegelka. How powerful are graph neural networks? In Proceedings of

the International Conference on Learning Representations (ICLR)  2019.

11

[41] K. Xu  C. Li  Y. Tian  T. Sonobe  K.-i. Kawarabayashi  and S. Jegelka. Representation learning on graphs
with jumping knowledge networks. In Proceedings of the 34th International Conference on Machine
Learning (ICML)  2018.

[42] S. Yan  Y. Xiong  and D. Lin. Spatial temporal graph convolutional networks for skeleton-based action

recognition. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence (AAAI)  2018.

[43] P. Yanardag and S. Vishwanathan. Deep graph kernels. In Proceedings of the ACM SIGKDD International

Conference on Knowledge Discovery and Data Mining (KDD)  pages 1365â€“1374  2015.

[44] Z. Yang  W. W. Cohen  and R. Salakhutdinov. Revisiting semi-supervised learning with graph embeddings.

In Proceedings of the International Conference on Machine Learning (ICML)  pages 40â€“48  2016.

[45] J. You  B. Liu  Z. Ying  V. Pande  and J. Leskovec. Graph convolutional policy network for goal-directed
molecular graph generation. In Advances in Neural Information Processing Systems (NeurIPS)  pages
6410â€“6421  2018.

[46] L. Zhang  H. Song  and H. Lu. Graph node-feature convolution for representation learning. arXiv preprint

arXiv:1812.00086  2018.

[47] M. Zhang  Z. Cui  M. Neumann  and Y. Chen. An end-to-end deep learning architecture for graph

classiï¬cation. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence (AAAI)  2018.

[48] Z. Zhang  M. Wang  Y. Xiang  Y. Huang  and A. Nehorai. Retgk: Graph kernels based on return probabilities
of random walks. In Advances in Neural Information Processing Systems (NeurIPS)  pages 3964â€“3974 
2018.

[49] L. Zhao  X. Peng  Y. Tian  M. Kapadia  and D. Metaxas. Learning to forecast and reï¬ne residual motion
for image-to-video generation. In Proceedings of the European Conference on Computer Vision (ECCV) 
pages 387â€“403  2018.

[50] L. Zhao  X. Peng  Y. Tian  M. Kapadia  and D. N. Metaxas. Semantic graph convolutional networks for
3D human pose regression. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  pages 3425â€“3435  2019.

[51] Y. Zhu  M. Elhoseiny  B. Liu  X. Peng  and A. Elgammal. A generative adversarial approach for zero-shot
In Proceedings of the IEEE Conference on Computer Vision and Pattern

learning from noisy texts.
Recognition (CVPR)  2018.

12

,Yu Tian
Long Zhao
Xi Peng
Dimitris Metaxas