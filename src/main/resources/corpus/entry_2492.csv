2014,How hard is my MDP?" The distribution-norm to the rescue",In Reinforcement Learning (RL)  state-of-the-art algorithms require a large number of samples per state-action pair to estimate the transition kernel $p$. In many problems  a good approximation of $p$ is not needed. For instance  if from one state-action pair $(s a)$  one can only transit to states with the same value  learning $p(\cdot|s a)$ accurately is irrelevant (only its support matters). This paper aims at capturing such behavior by defining a novel hardness measure for Markov Decision Processes (MDPs) we call the {\em distribution-norm}. The distribution-norm w.r.t.~a measure $\nu$ is defined on zero $\nu$-mean functions $f$ by the standard variation of $f$ with respect to $\nu$. We first provide a concentration inequality for the dual of the distribution-norm. This allows us to replace the generic but loose $||\cdot||_1$ concentration inequalities used in most previous analysis of RL algorithms  to benefit from this new hardness measure. We then show that several common RL benchmarks have low hardness when measured using the new norm. The distribution-norm captures finer properties than the number of states or the diameter and can be used to assess the difficulty of MDPs.,“How hard is my MDP?”

The distribution-norm to the rescue

Odalric-Ambrym Maillard
The Technion  Haifa  Israel

Timothy A. Mann

The Technion  Haifa  Israel

odalric-ambrym.maillard@ens-cachan.org

mann.timothy@gmail.com

Shie Mannor

The Technion  Haifa  Israel

shie@ee.technion.ac.il

Abstract

In Reinforcement Learning (RL)  state-of-the-art algorithms require a large num-
ber of samples per state-action pair to estimate the transition kernel p. In many
problems  a good approximation of p is not needed. For instance  if from one
state-action pair (s  a)  one can only transit to states with the same value  learning
p(·|s  a) accurately is irrelevant (only its support matters). This paper aims at cap-
turing such behavior by deﬁning a novel hardness measure for Markov Decision
Processes (MDPs) based on what we call the distribution-norm. The distribution-
norm w.r.t. a measure ν is deﬁned on zero ν-mean functions f by the standard
variation of f with respect to ν. We ﬁrst provide a concentration inequality for the
dual of the distribution-norm. This allows us to replace the problem-free  loose
|| · ||1 concentration inequalities used in most previous analysis of RL algorithms 
with a tighter problem-dependent hardness measure. We then show that several
common RL benchmarks have low hardness when measured using the new norm.
The distribution-norm captures ﬁner properties than the number of states or the
diameter and can be used to assess the difﬁculty of MDPs.

Introduction

1
The motivation for this paper started with a question: Why are the number of samples needed for Re-
inforcement Learning (RL) in practice so much smaller than those given by theory? Can we improve
this? In Markov Decision Processes (MDPs  Puterman (1994))  when the performance is measured
by (1) the sample complexity (Kearns and Singh  2002; Kakade  2003; Strehl and Littman  2008;
Szita and Szepesv´ari  2010) or (2) the regret (Bartlett and Tewari  2009; Jaksch  2010; Ortner  2012) 
algorithms have been developed that achieve provably near-optimal performance. Despite this  one
can often solve MDPs in practice with far less samples than required by current theory. One possible
reason for this disconnect between theory and practice is because the analysis of RL algorithms has
focused on bounds that hold for the most difﬁcult MDPs. While it is interesting to know how an
RL algorithm will perform for the hardest MDPs  most MDPs we want to solve in practice are far
from pathological. Thus  we want algorithms (and analysis) that perform appropriately with respect
to the hardness of the MDP it is facing.
A natural way to ﬁll this gap is to formalize a “hardness” metric for MDPs and show that MDPs
from the literature that were solved with few samples are not “hard” according to this metric. For
ﬁnite-state MDPs  usual metrics appearing in performance bounds of MDPs include the number of
states and actions  the maximum of the value function in the discounted setting  and the diameter
or sometimes the span of the bias function in the undiscounted setting. They only capture limited
properties of the MDP. Our goal in this paper is to propose a more reﬁned notion of hardness.

1

Previous work Despite the rich literature on MDPs  there has been surprisingly little work on met-
rics capturing the difﬁculty of learning MDPs. In Jaksch (2010)  the authors introduce the UCRL
algorithm for undiscounted MDPs  whose regret scales with the diameter D of the MDP  a quan-
tity that captures the time to reach any state from any other.
In Bartlett and Tewari (2009)  the
authors modify UCRL to achieve regret that scales with the span of the bias function  which can be
arbitrarily smaller than D. The resulting algorithm  REGAL achieves smaller regret  but it is an
open question whether the algorithm can be implemented. Closely related to our proposed solution 
in Filippi et al. (2010) the authors provide a modiﬁed version of UCRL  called KL-UCRL that
uses modiﬁed conﬁdence intervals on the transition kernel based on Kullback-Leibler divergence
rather than || · ||1 control on the error. The resulting algorithm is reported to work better in prac-
tice  although this is not reﬂected in the theoretical bounds. Farahmand (2011) introduced a metric
for MDPs called the action-gap. This work is the closest in spirit to our approach. The action-
gap captures the difﬁculty of distinguishing the optimal policy from near-optimal policies  and is
complementary to the notion of hardness proposed here. However  the action-gap has mainly been
used for planning  instead of learning  which is our main focus. In the discounted setting  several
works have improved the bounds with respect to the number of states (Szita and Szepesv´ari  2010)
and the discount factor (Lattimore and Hutter  2012). However  these analyses focus on worst case
bounds that do not scale with the hardness of the MDP  missing an opportunity to help bridge the
gap between theory and practice.
Contributions Our main contribution is a reﬁned metric for the hardness of MDPs  that captures the
observed “easiness” of common benchmark MDPs. To accomplish this we ﬁrst introduce a norm in-
duced by a distribution ν  aka the distribution-norm. For functions f with zero ν-expectation  ||f||ν
is the variance of f. We deﬁne the dual of this norm in Lemma 1  and then study its concentration
properties in Theorem 1. This central result is of independent interest beyond its application in RL.

More precisely  for a discrete probability measure p and its empirical version�pn built from n i.i.d
samples  we control ||p −�pn||� p in O((np0)−1/2)  where p0 is the minimum mass of p on its sup-

port. Second  we deﬁne a hardness measure for MDPs based on the distribution-norm. This measure
captures stochasticity along the value function. This quantity is naturally small in MDPs that are
nearly deterministic  but it can also be small in MDPs with highly stochastic transition kernels. For
instance  this is the case when all states reachable from a state have the same value. We show that
some common benchmark MDPs have small hardness measure. This illustrates that our proposed
norm is a useful tool for the analysis and design of existing and future RL algorithms.
Outline In Section 2  we formalize the distribution-norm  and give intuition about the interplay with
its dual. We compare to distribution-independent norms. Theorem 1 provides a concentration in-
equality for the dual of this norm  that is of independent interest beyond the MDP setting. Section 3
uses these insights to deﬁne a problem-dependent hardness metric for both undiscounted and dis-
counted MDPs (Deﬁnition 2  Deﬁnition 1)  that we call the environmental norm. Importantly  we
show in section 3.2 that common benchmark MDPs have small environmental norm C in this sense 
and compare our bound to approaches bounding the problem-free || · ||1 norm.
2 The distribution-norm and its dual
In Machine Learning (ML)  norms often play a crucial role in obtaining performance bounds. One
typical example is the following. Let X be a measurable space equipped with an unknown prob-
ability measure ν ∈ M1(X ) with density p. Based on some procedure  an algorithm produces a
candidate measure ˜ν ∈ M1(X ) with density ˜p. One is then interested in the loss with respect to a
continuous function f. It is natural to look at the mismatch between ν and ˜ν on f. That is

(ν − ˜ν  f ) =�X

f (x)(ν − ˜ν)(dx) =�X

f (x)(p(x) − ˜p(x))dx .

A typical bound on this quantity is obtained by applying a H¨older inequality to f and p − ˜p  which
gives (ν − ˜ν  f ) � ||p − ˜p||1||f||∞ . Assuming a bound is known for ||f||∞  this inequality can
be controlled with a bound on ||p − ˜p||1. When X is ﬁnite and ˜p is the empirical distribution �pn
estimated from n i.i.d. samples of p  results such as Weissman et al. (2003) can be applied to bound
this term with high probability.
However  in this learning problem  what matters is not f but the way f behaves with respect to ν.
Thus  trying to capture the properties of f via the distribution-free ||f||∞ bound is not satisfactory.
So we propose  instead  a norm || · ||ν driven by ν. Well-behaving f will have small norm ||f||ν 
whereas badly-behaving f will have large norm ||f||ν. Every distribution has a natural norm asso-

2

ciated with it that measures the quadratic variations of f with respect to ν. This quantity is at the
heart of many key results in mathematical statistics  and is formally deﬁned by

||f||ν =��X�f (x) − Eνf�2

ν(dx) .

(1)

To get a norm  we restrict C(X ) to the space of continuous functions Eν = {f ∈ C(X ) : ||f||ν <
∞  supp(ν) ⊂ supp(f )  Eνf = 0} . We then deﬁne the corresponding dual space in a standard way
by E �

ν = {µ : ||µ||� ν < ∞} where

||µ||� ν = sup

f∈Eν�x f (x)µ(dx)

||f||ν

.

Note that for f ∈ Eν  using the fact the ν(X ) = ˜ν(X ) = 1 and that x → f (x)− Eνf is a zero mean
function  we immediately have

(ν − ˜ν  f ) = (ν − ˜ν  f − Eνf )

� ||p − ˜p||� ν||f − Eνf||ν .

(2)
The key difference with the generic H¨older inequality is that || · ||ν is now capturing the behavior of
f with respect to ν  as opposed to || · ||∞. Conceptually  using a quadratic norm instead of an L1
norm  as we do here  is analogous to moving from Hoeffding’s inequality to Bernstein’s inequality
in the framework of concentration inequalities.
We are interested in situations where ||f||ν is much smaller than ||f||∞. That is  f is well-behaving
with respect to ν. In such cases  we can get an improved bound ||p − ˜p||� ν||f − Eνf||ν instead of
the best possible generic bound inf c∈R ||p − ˜p||1||f − c||∞.
Simply controlling either ||p − ˜p||� ν (respectively ||p − ˜p||1) or ||f||ν (respectively ||f||∞) is not
enough. What matters is the product of these quantities. For our choice of norm  we show that
||p − ˜p||� ν concentrates at essentially the same speed as ||p − ˜p||1  but ||f||∞ is typically much
larger than ||f||ν for the typical functions met in the analysis of MDPs. We do not claim that the
norm deﬁned in equation (1) is the best norm that leads to a minimal ||p − ˜p||� ν||f − Eνf||ν  but
we show that it is an interesting candidate.

We proceed in two steps. First  we design in Section 2 a concentration bound for ||p−�pn||� ν that is
not much larger than the Weissman et al. (2003) bound on ||p −�pn||1. (Note that ||p −�pn||� ν must
be larger than ||p −�pn||1 as it captures a reﬁned property). Second  in Section 3  we consider RL in

an MDP where p represents the transition kernel of a station-action pair and f represents the value
function of the MDP for a policy. The value function and p are strongly linked by construction 
and the distribution-norm helps us capture their interplay. We show in Section 3.2 that common
benchmark MDPs have optimal value functions with small || · ||ν norm. This naturally introduces a
new way to capture the hardness of MDPs  besides the diameter (Jaksch  2010) or the span (Bartlett
and Tewari  2009). Our formal notion of MDP hardness is summarized in Deﬁnitions 1 and 2  for
discounted and undiscounted MDPs  respectively.

term on the right hand side of (2)  which corresponds to the dual norm when ˜p =�pn is the empirical

2.1 A dual-norm concentration inequality
For convenience we consider a ﬁnite space X = {1  . . .   S} with S points. We focus on the ﬁrst
mean built from n i.i.d. samples from the distribution ν. We denote by p the probability vector
corresponding to ν. The following lemma  whose proof is in the supplementary material  provides a
convenient way to compute the dual norm.
Lemma 1 Assume that X = {1  . . .   S}  and  without loss of generality 
{1  . . .   K}  with K � S. Then the following equality holds true
n s − p2

that supp(p) =

s

.

Now we provide a ﬁnite-sample bound on our proposed norm.

||�pn − p||� p = ����

K�s=1�p2

ps

3

Theorem 1 (Main result) Assume that supp(p) = {1  . . .   K}  with K � S. Then for all δ ∈
(0  1)  with probability higher than 1 − δ 
p(1)��  (3)

+ 2� (2n − 1) ln(1/δ)

p(K) − 1 � K − 1

where p(K) is the smallest non zero component of p = (p1  . . .   pS)  and p(1) the largest one.

||�pn − p||� p � min�� 1

� 1

p(K) −

n2

n

1

The proof follows an adaptation of Maurer and Pontil (2009) for empirical Bernstein bounds  and
uses results for self-bounded functions from the same paper. This gives tighter bounds than naive
concentration inequalities (Hoeffding  Bernstein  etc.). We indeed get a O(n−1/2) scaling  whereas
using simpler techniques would lead to a weak O(n−1/4) scaling.
Proof We will apply Theorem 7 of Maurer and Pontil (2009). Using the notation of this theorem 
we denote the sample by X = (X1  . . .   Xn) and the function we want to control by

We now introduce  for any s ∈ S the modiﬁed sample Xi0 s = (X1  . . .   Xi0−1  s  Xi0+1  . . .   Xn).
We are interested in the quantity V(X)−V(Xi0 s). To apply Theorem 7 of Maurer and Pontil (2009) 
we need to identify constants a  b such that

V(X) = ||�pn − p||2

� p .

�∀i ∈ [n]  V(X) − inf s∈S V(Xi s) � b
i=1�V(X) − inf s∈S V(Xi s)�2
�n

� aV(X) .

The two following lemmas enable us to identify a and b. They follow from simple algebra and are
proved in Appendix A in the supplementary material.

Lemma 2 V(X) satisﬁes Ep�V(X)� = K−1

n . Moreover  for all i ∈ {1  . . .   n} we have that

V(X) − inf

s∈S V(Xi s) � b   where b =

2n − 1

n2 � 1

p(K) −

1

p(1)� .

� p satisﬁes

Lemma 3 V(X) = ||�pn − p||2
Thus  we can choose a = 2b. By application of Theorem 7 of Maurer and Pontil (2009) to ˜V(X) =
V(X)/b  we deduce that for all ε > 0 

n�i=1�V(X) − inf

s∈S V(Xi s)�2

� 2bV(X) .

After inverting this bound in ε and using the fact that √a + b � √a + √b for non-negative a  b  we
deduce that for all δ ∈ (0  1)  with probability higher than 1 − δ  then

Plugging back in the deﬁnition of ˜V(X)  we obtain

n

ε2

ε2/b

� p >

4 K−1

K − 1

4E ˜V(X) + 2ε� .
n + 2ε� .

P� ˜V(X) − E ˜V(X) > ε� � exp�−
+ ε� � exp�−
P�||�pn − p||2
||�pn − p||2
||�pn − p||� p � �EV(X) + 2�b ln(1/δ)

� p � EV(X) + 2�EV(X)b ln(1/δ) + 2b log(1/δ)
= ��EV(X) +�b ln(1/δ)�2
+ 2� (2n − 1) ln(1/δ)
= � K − 1

+ b log(1/δ) .

p(K) −

� 1

n2

n

1

p(1)�  

Thus  we deduce from this inequality that

which concludes the proof. We recover here a O(n−1/2) behavior  more precisely a O(p−1
scaling where p(K) is the smallest non zero probability mass of p.

(K)n−1/2)
�

4

3 Hardness measure in Reinforcement Learning using the distribution-norm
In this section  we apply the insights from Section 2 for the distribution-norm to learning in Markov
Decision Processes (MDPs). We start by deﬁning a formal notion of hardness C for discounted
MDPs and undiscounted MDPs with average reward  that we call the environmental norm. Then  we
show in Section 3.2 that several benchmark MDPs have small environmental norm. In Section 3.1 
we present a regret bound for a modiﬁcation of UCRL whose regret scales with C  without having
to know C in advance.
Deﬁnition 1 (Discounted MDP) Let M =< S A  r  p  γ > be a γ-discounted MDP  with reward
function r and transition kernel p. We denote V π the value function corresponding to a policy π
(Puterman  1994). We deﬁne the environmental-value norm of policy π in MDP M by

C π

M = max

s a∈S×A||V π||p(·|s a) .

Deﬁnition 2 (Undiscounted MDP) Let M =< S A  r  p > be an undiscounted MDP  with reward
function r and transition kernel p. We denote by hπ the bias function for policy π (Puterman  1994;
Jaksch  2010). We deﬁne the environmental-value norm of policy π in MDP M by the quantity

C π

M = max

s a∈S×A||hπ||p(·|s a) .

M � 1

In the discounted setting with bounded rewards in [0  1]  V π � 1
In the undiscounted setting  then ||hπ||p(·|s a) � span(hπ)  and thus C π
the class of C-“hard” MDPs by MC =�M : C π∗

1−γ as well.
M � span(hπ). We deﬁne
M � C� . That is  the class of MDPs with optimal

1−γ and thus C π

“Easy” MDPs and algorithms

policy having a low environmental-value norm  or for short  MDPs with low environmental norm.
Important note It may be tempting to think that  since the above deﬁnition captures a notion of
variance  an MDP that is very noisy will have a high environmental norm. However this reasoning
is incorrect. The environmental norm of an MDP is not the variance of a roll-out trajectory  but
rather captures the variations of the value (or the bias value) function with respect to the transition
kernel. For example  consider a fully connected MDP with transition kernel that transits to every
state uniformly at random  but with a constant reward function. In this trivial MDP  C π
M = 0 for
all policies π  even though the MDP is extremely noisy because the value function is constant. In
general MDPs  the environmental norm depends on how varied the value function is at the possible
next states and on the distribution over next states. Note also that we use the term hardness rather
than complexity to avoid confusion with such concepts as Rademacher or VC complexity.
3.1
In this section  we demonstrate how the dual norm (instead of the usual || · ||1 norm) can lead to
improved bounds for learning in MDPs with small environmental norm.
Discounted MDPs Due to space constraints  we only report one proposition that illustrates the kind
of achievable results. Indeed  our goal is not to derive a modiﬁed version of each existing algorithm
for the discounted scenario  but rather to instill the key idea of using a reﬁned hardness measure
when deriving the core lemmas underlying the analysis of previous (and future) algorithms.
The analysis of most RL algorithms for the discounted case uses a “simulation lemma” (Kearns
and Singh  2002); see also Strehl and Littman (2008) for a reﬁned version. A simulation lemma
bounds the error in the value function of running a policy planned on an estimated MDP in the MDP
where the samples were taken from. This effectively controls the number of samples needed from
each state-action pair to derive a near-optimal policy. The following result is a simulation lemma
exploiting our proposed notion of hardness (the environmental norm).
Proposition 1 Let M be a γ-discounted MDP with deterministic rewards. For a policy π  let us
denote its corresponding value V π. We denote by p the transition kernel of M  and for convenience

use the notation pπ(s�|s) for p(s�|s  π(s)). Now  let�p be an estimate of the transition kernel such
that maxs∈S ||pπ(·|s) −�pπ(·|s)||� pπ(·|s) � ε and let us denote �V π its corresponding value in the
MDP with kernel�p. Then  the maximal expected error between the two values is bounded by
where C π = maxs a∈S×A ||V π||p(·|s a). In particular  for the optimal policy π�  then C π� � C.

s0∈S�Epπ(·|s0)�V π� − E�pπ(·|s0)��V π�� � εC π

def= max

1 − γ

E π

rr

 

5

To understand when this lemma results in smaller sample sizes  we need to compare to what
one would get using the standard || · ||1 decomposition  for an MDP with rewards in [0  1].
If

maxs∈S ||pπ(·|s) −�pπ(·|s)||1 � ε�  then one would get
� ε�V ∗MAX
1 − γ �

rr � εspan(V π)
E π

1 − γ

ε�

(1 − γ)2 .

When  for example  C is a bound with respect to all policies  this simulation lemma can be plugged
directly into the analysis of R-MAX (Kakade  2003) or MBIE (Strehl and Littman  2008) to obtain
a hardness-sensitive bound on the sample complexity. Now  in most analyses  one only needs to
bound the hardness with respect to the optimal policy and to the optimistic/greedy policies actually
used by the algorithm. For an optimal policy ˜π computed from an (ε  ε�)-approximate model (see
Lemma 4 for details)  it is not difﬁcult to show that C ˜π � C π�
+ ε)/(1 − γ)  which thus
allows for a tighter analysis. We do not report further results here  to avoid distracting the reader
from the main message of the paper  which is the introduction of a distribution-dependent hardness
metric for MDPs. Likewise  we do not detail the steps that lead from this result to the various
sample-complexity bounds one can ﬁnd in the abundant literature on the topic  as it would not be
more illuminating than Proposition 1.
Undiscounted MDPs In the undiscounted setting  with average reward criterion  it is natural to
consider the UCRL algorithm from Jaksch (2010). We modify the deﬁnition of plausible MDPs
used in the algorithm as follows: Using the same notations as that of Jaksch (2010)  we replace the
admissibility condition for a candidate transition kernel ˜p at the beginning of episode k at time tk

+ (ε�C π�

� 1

˜p(K) −

with the following condition involving the result of Theorem 1

||�pk(·|s  a) − ˜p(·|s  a)||1 �� 14S log(2Atk/δ)
+ 2� (2Nk(s  a) − 1) ln(tkSA/δ)

max{1  Nk(s  a)}

K − 1

max{1  Nk(s  a)}2

max{1  Nk(s  a)}

p0 − 1 �

||�pk(·|s  a) − ˜p(·|s  a)||�  ˜p(·|s a) � Bk(s  a) def=
min�� 1

˜p(1)��   (4)
where ˜p(K) is the smallest non zero component of ˜p(·|s  a)  and ˜p(1) the largest one  and K is the
size of the support of ˜p(·|s  a). We here assume for simplicity that the transition kernel p of the MDP
always puts at least p0 mass on each point of its support  and thus constraint an admissible kernel ˜p
to satisfy the same condition. One restriction of the current (simple) analysis is that the algorithm
needs to know a bound on p0 in advance. We believe it is possible to remove such an assumption by
estimating p0 and taking care of the additional low probability event corresponding to the estimation
error. As this comes at the price of a more complicated algorithm and analysis  we do not report
this extension here for clarity. Note that the optimization problem corresponding to Extended Value
Iteration with (4) can still be solved by optimizing over the simplex. We refer to Jaksch (2010) for
implementation details. Naturally  similar modiﬁcations apply also to REGAL and other UCRL
variants introduced in the MDP literature.
In order to assess the performance of the policy chosen by UCRL it is useful to show the following:
Lemma 4 Let M and ˜M be two communicating MDPs over the same state-action space such that
one is an (ε  ε�)-approximation of the other in the sense that for all s  a |r(s  a) − ˜r(s  a)| � ε and
||˜p(·|s  a) − p(·|s  a)||� p(·|s a) � ε�. Let ρ�(M ) denotes the average value function of M. Then
Lemma 4 is a simple adaptation from Ortner et al. (2014). We now provide a bound on the regret of
this modiﬁed UCRL algorithm. The regret bound turns out to be a bit better than UCRL in the case
of an MDP M ∈ MC with a small C.
Proposition 2 Let us consider a ﬁnite-state MDP with S state  low environmental norm (M ∈ MC)
and diameter D. Assume moreover that the transition kernel that always puts at least p0 mass on
each point of its support. Then  the modiﬁed UCRL algorithm run with condition (4) is such that
for all δ  with probability higher than 1 − δ  for all T   the regret after T steps is bounded by
log(T SA/δ)� .

RT = O��DC√SA�� log(T SA/δ)

||ρ�(M ) − ρ�( ˜M )||p � ε� min{CM   C ˜M} + ε .

+ √S� + D�� T

p0

1

 

p0

6

Since we used some crude upper bounds in parts of the proof of Proposition 2  we believe the

The regret bound for the original UCRL from Jaksch (2010) scales as O�DS�AT log(T SA/δ)�.
log(T SA/δ)�. The cruder factors
right scaling for the bound of Proposition 2 is O�C� T SA

come from some second order terms that we controlled trivially to avoid technical and not very
illuminating considerations. What matters here is that C appears as a factor of the leading term.
Indeed proposition 2 is mostly here for illustration purpose of what one can achieve  and improving
on the other terms is technical and goes beyond the scope of this paper. Comparing the two regret
bounds  the result of Proposition 2 provides a qualitative improvement over the result of Jaksch
(2010) whenever C < D√Sp0 (respectively C < √Sp0) for the conjectured (resp. current) result.
Note. The modiﬁed UCRL algorithm does not need to know the environmental norm C of the MDP
in advance. It only appears in the analysis and in the ﬁnal regret bound. This property is similar to
that of UCRL with respect to the diameter D.

p0

3.2 The hardness of benchmarks MDPs
In this section  we consider the hardness of a set of MDPs that have appeared in past literature.
Table 3.2 summarizes the results for six MDPs that were chosen to be both representative of typ-
ical ﬁnite-states MDPs but also cover a diverse range of tasks. These MDPs are also signiﬁcant
in the sense that good solutions for them have been learned with far fewer samples then sug-
gested by existing theoretical bounds. The metrics we report include the number of states S 
the number of actions A  the maximum of V � (denoted V ∗MAX)  the span of V ∗  the C π∗
M   and
p(s�|s  a)  that is the minimum non-zero probability mass given by the
p0 = min
transition kernel of the MDP. While we cannot compute the hardness for all policies  the hardness
with respect to π∗ is signiﬁcant because it indicates how hard it is to learn the value function V ∗
of the optimal policy. Notice that C π∗
M is signiﬁcantly smaller than both V ∗MAX and span(V ∗) in
all the MDPs. This suggests that a model accurately representing the optimal value function can be
derived with a small number of samples (and a bound based on � · �1V ∗MAX is overly conservative).

s�∈supp(p(·|s a)

s∈S a∈A

min

MDP
bottleneck McGovern and Barto (2001)
red herring Hester and Stone (2009)
taxi † Dietterich (1998)
inventory † Mankowitz et al. (2014)
mountain car † � � Sutton and Barto (1998)
pinball † � � Konidaris and Barto (2009)

S
231
121
500
101
150
2304

A V ∗MAX
19.999
4
17.999
4
7.333
6
19.266
2
3
19.999
19.999
5

Span(V ∗)

19.999
17.999
0.885
0.963
19.999
19.991

C π∗
p0
M
0.1
0.526
0.1
4.707
0.055
0.043
0.263 < 10−3
1.296
0.322
0.059 < 10−3

Table 1: MDPs marked with a † indicate that the true MDP was not available and so it was
estimated from samples. We estimated these MDPs with 10  000 samples from each state-
action pair. MDPs marked with a � indicate that the original MDP is deterministic and there-
fore we added noise to the transition dynamics. For the Mountain Car problem  we added a
small amount of noise to the vehicle’s velocity during each step (post+1 = post + velt(1 +
X) where X is a random variable with equally probable events {−velM AX   0  velM AX}). For the
pinball domain we added noise similar to Tamar et al. (2013). MDPs marked with a � were dis-
cretized to create a ﬁnite state MDP. The rewards of all MDPs were normalized to [0  1] and discount
factor γ = 0.95 was used.

To understand the environmental-value norm of near-optimal policies π in an MDP  we ran policy
iteration on each of the benchmark MDPs from Table 3.2 for 100 iterations (see supplementary
material for further details). We computed the environmental-value norm of all encountered policies
and selected the policy π with maximal norm and its corresponding worst case distribution. Figure 1
M as the
compares the Weissman et al. (2003) bound ×VMAX to the bound given by Theorem 1 ×C π
number of samples increases. This is indeed the comparison of this products that matters for the
learning regret  rather than that of one or the other factor only. In each MDP  we see an order of
magnitude improvement by exploiting the distribution-norm. This is particularly signiﬁcant because
the Weissman et al. (2003) bound is quite close to the behavior observed in experiments. The result
in Figure 1 strengthens support for our theoretical ﬁndings  suggesting that bounds based on the
distribution-norm scale with the MDP’s hardness.

7

Figure 1: Comparison of the Weissman et al. (2003) bound times VMAX to (3) of Theorem 1 times
M in the benchmark MDPs. In each MDP  we selected the policy π (from the policies encountered
C π
during policy iteration) that gave the largest C π and the worst next state distribution for our bound.
In each MDP  the improvement with the distribution-norm is an order of magnitude (or more) better
than using the distribution-free Weissman et al. (2003) bound.

4 Discussion and conclusion
In the early days of learning theory  sample independent quantities such as the VC-dimension and
later the Rademacher complexity were used to derive generalization bounds for supervised learning.
Later on  data dependent bounds (empirical VC or empirical Rademacher) replaced these quantities
to obtain better bounds. In a similar spirit  we proposed the ﬁrst analysis in RL where instead of
considering generic a-priori bounds one can use stronger MDP-speciﬁc bounds. Similarly to the su-
pervised learning  where generalization bounds have been used to drive model selection algorithms
and structural risk minimization  our proposed distribution dependent norm suggests a similar ap-
proach in solving RL problems. Although we do not claim to close the gap between theoretical
and empirical bounds  this paper opens an interesting direction of research towards this goal  and
achieves a signiﬁcant ﬁrst step. It inspires at least a modiﬁcation of the whole family of UCRL-
based algorithms  and could potentially beneﬁt also to others fundamental problems in RL such as
basis-function adaptation or model selection  but efﬁcient implementation should not be overlooked.
We choose a natural weighted L2 norm induced by a distribution  due to its simplicity of interpre-
tation and showed several benchmark MDPs have low hardness. A natural question is how much
beneﬁt can be obtained by studying other Lp or Orlicz distribution-norms? Further  one may wish
to create other distribution dependent norms that emphasize certain areas of the state space in order
to better capture desired (or undesired) phenomena. This is left for future work.
In the analysis we basically showed how to adapt existing algorithms to use the new distribution
dependent hardness measure. We believe this is only the beginning of what is possible  and that new
algorithms will be developed to best utilize distribution dependent norms in MDPs.
Acknowledgements This work was supported by the European Community’s Seventh Framework
Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL) and the Technion.

References
Bartlett  P. L. and Tewari  A. (2009). Regal: A regularization based algorithm for reinforcement
In Proceedings of the Twenty-Fifth Conference on

learning in weakly communicating mdps.
Uncertainty in Artiﬁcial Intelligence  pages 35–42.

Dietterich  T. G. (1998). The MAXQ method for hierarchical reinforcement learning. In Interna-

tional Conference on Machine Learning  pages 118–126.

8

02004006008001000Samples100101102103Error(log-scale)BottleneckWeissman×VMAXTheorem1×Cπ02004006008001000Samples100101102103Error(log-scale)RedHerringWeissman×VMAXTheorem1×Cπ02004006008001000Samples100101102103Error(log-scale)TaxiWeissman×VMAXTheorem1×Cπ02004006008001000Samples10−1100101102103Error(log-scale)InventoryManagementWeissman×VMAXTheorem1×Cπ02004006008001000Samples100101102103Error(log-scale)MountainCarWeissman×VMAXTheorem1×Cπ02004006008001000Samples10−1100101102103104Error(log-scale)PinballWeissman×VMAXTheorem1×CπFarahmand  A. M. (2011). Action-gap phenomenon in reinforcement learning. In Shawe-Taylor  J. 
Zemel  R. S.  Bartlett  P. L.  Pereira  F. C. N.  and Weinberger  K. Q.  editors  Proceedings of the
25th Annual Conference on Neural Information Processing Systems  pages 172–180  Granada 
Spain.

Filippi  S.  Capp´e  O.  and Garivier  A. (2010). Optimism in reinforcement learning and kullback-
In Communication  Control  and Computing (Allerton)  2010 48th Annual

leibler divergence.
Allerton Conference on  pages 115–122. IEEE.

Hester  T. and Stone  P. (2009). Generalized model learning for reinforcement learning in factored
domains. In The Eighth International Conference on Autonomous Agents and Multiagent Systems
(AAMAS).

Jaksch  T. (2010). Near-optimal regret bounds for reinforcement learning. Journal of Machine

Learning Research  11:1563–1600.

Kakade  S. M. (2003). On the Sample Complexity of Reinforcement Learning. PhD thesis  University

College London.

Kearns  M. and Singh  S. (2002). Near-optimal reinforcement learning in polynomial time. Machine

Learning  49:209–232.

Konidaris  G. and Barto  A. (2009). Skill discovery in continuous reinforcement learning domains
using skill chaining. In Bengio  Y.  Schuurmans  D.  Lafferty  J.  Williams  C. K. I.  and Culotta 
A.  editors  Advances in Neural Information Processing Systems 22  pages 1015–1023.

Lattimore  T. and Hutter  M. (2012). PAC bounds for discounted MDPs. In Algorithmic learning

theory  pages 320–334. Springer.

Mankowitz  D. J.  Mann  T. A.  and Mannor  S. (2014). Time-regularized interrupting options

(TRIO). In Proceedings of the 31st International Conference on Machine Learning.

Maurer  A. and Pontil  M. (2009). Empirical Bernstein bounds and sample-variance penalization. In

Conference On Learning Theory (COLT).

McGovern  A. and Barto  A. G. (2001). Automatic discovery of subgoals in reinforcement learning
using diverse density. In Proceedings of the 18th International Conference on Machine Learning 
pages 361 – 368  San Fransisco  USA.

Ortner  R. (2012). Online regret bounds for undiscounted continuous reinforcement learning. In

Neural Information Processing Systems 25  pages 1772—-1780.

Ortner  R.  Maillard  O.-A.  and Ryabko  D. (2014). Selecting near-optimal approximate state rep-

resentations in reinforcement learning. Technical report  Montanuniversitaet Leoben.

Puterman  M. L. (1994). Markov Decision Processes - Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.

Strehl  A. L. and Littman  M. L. (2008). An analysis of model-based interval estimation for markov

decision processes. Journal of Computer and System Sciences  74(8):1309–1331.

Sutton  R. and Barto  A. (1998). Reinforcement Learning: An Introduction. MIT Press.
Szita  I. and Szepesv´ari  C. (2010). Model-based reinforcement learning with nearly tight explo-
In Proceedings of the 27th International Conference on Machine

ration complexity bounds.
Learning.

Tamar  A.  Castro  D. D.  and Mannor  S. (2013). TD methods for the variance of the reward-to-go.

In Proceedings of the 30 th International Conference on Machine Learning.

Weissman  T.  Ordentlich  E.  Seroussi  G.  Verdu  S.  and Weinberger  M. J. (2003). Inequalities for

the l1 deviation of the empirical distribution. Technical report  Hewlett-Packard Labs.

9

,Odalric-Ambrym Maillard
Timothy Mann
Shie Mannor