2017,ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching,We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy  we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further  we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.,ALICE: Towards Understanding Adversarial

Learning for Joint Distribution Matching

Chunyuan Li1  Hao Liu2  Changyou Chen3  Yunchen Pu1  Liqun Chen1 

cl319@duke.edu

Ricardo Henao1 and Lawrence Carin1

1Duke University 2Nanjing University 3University at Buffalo

Abstract

We investigate the non-identiﬁability issues associated with bidirectional adver-
sarial training for joint distribution matching. Within a framework of conditional
entropy  we propose both adversarial and non-adversarial approaches to learn
desirable matched joint distributions for unsupervised and supervised tasks. We
unify a broad family of adversarial models as joint distribution matching problems.
Our approach stabilizes learning of unsupervised bidirectional adversarial learning
methods. Further  we introduce an extension for semi-supervised learning tasks.
Theoretical results are validated in synthetic data and real-world applications.

Introduction

1
Deep directed generative models are a powerful framework for modeling complex data distributions.
Generative Adversarial Networks (GANs) [1] can implicitly learn the data generating distribution;
more speciﬁcally  GAN can learn to sample from it. In order to do this  GAN trains a generator to
mimic real samples  by learning a mapping from a latent space (where the samples are easily drawn)
to the data space. Concurrently  a discriminator is trained to distinguish between generated and real
samples. The key idea behind GAN is that if the discriminator ﬁnds it difﬁcult to distinguish real from
artiﬁcial samples  then the generator is likely to be a good approximation to the true data distribution.
In its standard form  GAN only yields a one-way mapping  i.e.  it lacks an inverse mapping mechanism
(from data to latent space)  preventing GAN from being able to do inference. The ability to compute
a posterior distribution of the latent variable conditioned on a given observation may be important
for data interpretation and for downstream applications (e.g.  classiﬁcation from the latent variable)
[2  3  4  5  6  7]. Efforts have been made to simultaneously learn an efﬁcient bidirectional model
that can produce high-quality samples for both the latent and data spaces [3  4  8  9  10  11]. Among
them  the recently proposed Adversarially Learned Inference (ALI) [4  10] casts the learning of such
a bidirectional model in a GAN-like adversarial framework. Speciﬁcally  a discriminator is trained to
distinguish between two joint distributions: that of the real data sample and its inferred latent code 
and that of the real latent code and its generated data sample.
While ALI is an inspiring and elegant approach  it tends to produce reconstructions that are not
necessarily faithful reproductions of the inputs [4]. This is because ALI only seeks to match two
joint distributions  but the dependency structure (correlation) between the two random variables
(conditionals) within each joint is not speciﬁed or constrained. In practice  this results in solutions
that satisfy ALI’s objective and that are able to produce real-looking samples  but have difﬁculties
reconstructing observed data [4]. ALI also has difﬁculty discovering the correct pairing relationship
in domain transformation tasks [12  13  14].
In this paper  (i) we ﬁrst describe the non-identiﬁability issue of ALI. To solve this problem  we
propose to regularize ALI using the framework of Conditional Entropy (CE)  hence we call the
proposed approach ALICE. (ii) Adversarial learning schemes are proposed to estimate the conditional

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

entropy  for both unsupervised and supervised learning paradigms. (iii) We provide a uniﬁed view
for a family of recently proposed GAN models from the perspective of joint distribution matching 
including ALI [4  10]  CycleGAN [12  13  14] and Conditional GAN [15]. (iv) Extensive experiments
on synthetic and real data demonstrate that ALICE is signiﬁcantly more stable to train than ALI  in
that it consistently yields more viable solutions (good generation and good reconstruction)  without
being too sensitive to perturbations of the model architecture  i.e.  hyperparameters. We also show
that ALICE results in more faithful image reconstructions. (v) Further  our framework can leverage
paired data (when available) for semi-supervised tasks. This is empirically demonstrated on the
discovery of relationships for cross domain tasks based on image data.
2 Background
Consider two general marginal distributions q(x) and p(z) over x ∈ X and z ∈ Z. One domain
can be inferred based on the other using conditional distributions  q(z|x) and p(x|z). Further  the
combined structure of both domains is characterized by joint distributions q(x  z) = q(x)q(z|x) and
p(x  z) = p(z)p(x|z).
To generate samples from these random variables  adversarial methods [1] provide a sampling
mechanism that only requires gradient backpropagation  without the need to specify the conditional
densities. Speciﬁcally  instead of sampling directly from the desired conditional distribution  the
random variable is generated as a deterministic transformation of two inputs  the variable in the source
domain  and an independent noise  e.g.  a Gaussian distribution. Without loss of generality  we use an
universal distribution approximator speciﬁcation [9]  i.e.  the sampling procedure for conditionals
˜x ∼ pθ(x|z) and ˜z ∼ qφ(z|x) is carried out through the following two generating processes:
(1)
˜x = gθ(z  )  z ∼ p(z)   ∼ N (0  I)  and ˜z = gφ(x  ζ)  x ∼ q(x)  ζ ∼ N (0  I) 
where gθ(·) and gφ(·) are two generators  speciﬁed as neural networks with parameters θ and
φ  respectively. In practice  the inputs of gθ(·) and gφ(·) are simple concatenations  [z ] and
[x ζ]  respectively. Note that (1) implies that pθ(x|z) and qφ(z|x) are parameterized by θ and φ
respectively  hence the subscripts.

The goal of GAN [1] is to match the marginal pθ(x) =(cid:82) pθ(x|z)p(z)dz to q(x). Note that q(x)

denotes the true distribution of the data (from which we have samples) and p(z) is speciﬁed as a
simple parametric distribution  e.g.  isotropic Gaussian. In order to do the matching  GAN trains
a ω-parameterized adversarial discriminator network  fω(x)  to distinguish between samples from
pθ(x) and q(x). Formally  the minimax objective of GAN is given by the following expression:
ω LGAN(θ  ω) = Ex∼q(x)[log σ(fω(x))] + E˜x∼pθ (x|z) z∼p(z)[log(1 − σ(fω(˜x)))]  (2)
min
where σ(·) is the sigmoid function. The following lemma characterizes the solutions of (2) in terms
of marginals pθ(x) and q(x).
Lemma 1 ([1]) The optimal decoder and discriminator  parameterized by {θ
a saddle point of the objective in (2)  if and only if pθ∗ (x) = q(x).
Alternatively  ALI [4] matches the joint distributions pθ(x  z) = pθ(x|z)p(z) and qφ(x  z) =
q(x)qφ(z|x)  using an adversarial discriminator network similar to (2)  fω(x  z)  parameterized by
ω. The minimax objective of ALI can be then written as

}  correspond to

  ω∗

max

∗

θ

min
θ φ

max

ω LALI(θ  φ  ω) = Ex∼q(x) ˜z∼qφ(z|x)[log σ(fω(x  ˜z))]

+ E˜x∼pθ (x|z) z∼p(z)[log(1−σ(fω(˜x  z)))].

(3)

∗

  ω∗

∗
  φ

} form a saddle point of the objective in (3)  if and only if pθ∗ (x  z) = qφ∗ (x  z).

Lemma 2 ([4]) The optimum of the two generators and the discriminator with parameters
{θ
From Lemma 2  if a solution of (3) is achieved  it is guaranteed that all marginals and conditional
distributions of the pair {x  z} match. Note that this implies that qφ(z|x) and pθ(z|x) match;
however  (3) imposes no restrictions on these two conditionals. This is key for the identiﬁability
issues of ALI described below.
3 Adversarial Learning with Information Measures
The relationship (mapping) between random variables x and z is not speciﬁed or constrained by
ALI. As a result  it is possible that the matched distribution π(x  z) (cid:44) pθ∗ (x  z) = qφ∗ (x  z) is
undesirable for a given application.

2

To illustrate this issue  Figure 1 shows all solutions
(saddle points) to the ALI objective on a simple toy
problem. The data and latent random variables can take
two possible values  X = {x1  x2} and Z = {z1  z2} 
respectively. In this case  their marginals q(x) and p(z)
are known  i.e.  q(x = x1) = 0.5 and p(z = z1) = 0.5.
The matched joint distribution  π(x  z)  can be repre-
sented as a 2 × 2 contingency table. Figure 1(a) repre-
sents all possible solutions of the ALI objective in (3) 
for any δ ∈ [0  1]. Figures 1(b) and 1(c) represent oppo-
Figure 1: Illustration of possible solutions to
site extreme solutions when δ = 1 and δ = 0  respec-
the ALI objective. The ﬁrst row shows the map-
tively. Note that although we can generate “realistic”
pings between two domains  The second row
values of x from any sample of p(z)  for 0 < δ < 1  we
shows matched joint distribution  π(x  z)  as
will have poor reconstruction ability since the sequence
contingency tables parameterized by δ = [0  1].
x ∼ q(x)  ˜z ∼ qφ(z|x)  ˜x ∼ pθ(x|˜z)  can easily
result in ˜x (cid:54)= x. The two (trivial) exceptions where the model can achieve perfect reconstruction
correspond to δ = {1  0}  and are illustrated in Figures 1(b) and 1(c)  respectively. From this simple
example  we see that due to the ﬂexibility of the joint distribution  π(x  z)  it is quite likely to obtain
an undesirable solution to the ALI objective. For instance  i) one with poor reconstruction ability or
ii) one where a single instance of z can potentially map to any possible value in X   e.g.  in Figure 1(a)
with δ = 0.5  z1 can generate either x1 or x2 with equal probability.
Many applications require meaningful mappings. Consider two scenarios:
• A1: In unsupervised learning  one desirable property is cycle-consistency [12]  meaning that the
inferred z of a corresponding x  can reconstruct x itself with high probability. In Figure 1 this
corresponds to either δ → 1 or δ → 0  as in Figures 1(b) and 1(c).
• A2: In supervised learning  the pre-speciﬁed correspondence between samples imposes restrictions
on the mapping between x and z  e.g.  in image tagging  x are images and z are tags. In this case 
paired samples from the desired joint distribution are usually available  thus we can leverage this
supervised information to resolve the ambiguity between Figure 1(b) and (c).

From our simple example in Figure 1  we see that in order to alleviate the identiﬁability issues
associated with the solutions to the ALI objective  we have to impose constraints on the conditionals
qφ(z|x) and pθ(z|x). Furthermore  to fully mitigate the identiﬁability issues we require supervision 
i.e.  paired samples from domains X and Z.
To deal with the problem of undesirable but matched joint distributions  below we propose to use
an information-theoretic measure to regularize ALI. This is done by controlling the “uncertainty”
between pairs of random variables  i.e.  x and z  using conditional entropies.
3.1 Conditional Entropy
Conditional Entropy (CE) is an information-theoretic measure that quantiﬁes the uncertainty of
random variable x when conditioned on z (or the other way around)  under joint distribution π(x  z):

H π(x|z) (cid:44) −E

π(x z)[log π(x|z)]  and H π(z|x) (cid:44) −E

(4)
The uncertainty of x given z is linked with H π(x|z); in fact  H π(x|z) = 0 if only if x is a
deterministic mapping of z. Intuitively  by controlling the uncertainty of qφ(z|x) and pθ(z|x)  we
can restrict the solutions of the ALI objective to joint distributions whose mappings result in better
reconstruction ability. Therefore  we propose to use the CE in (4)  denoted as Lπ
CE(θ  φ) = H π(x|z)
or H π(z|x) (depending on the task; see below)  as a regularization term in our framework  termed
ALI with Conditional Entropy (ALICE)  and deﬁned as the following minimax objective:
(5)

π(x z)[log π(z|x)].

min
θ φ

ω LALICE(θ  φ  ω) = LALI(θ  φ  ω) + Lπ
max

CE(θ  φ).

CE(θ  φ) is dependent on the underlying distributions for the random variables  parametrized by
Lπ
(θ  φ)  as made clearer below. Ideally  we could select the desirable solutions of (5) by evaluating
their CE  once all the saddle points of the ALI objective have been identiﬁed. However  in practice 
CE(θ  φ) is intractable because we do not have access to the saddle points beforehand. Below  we
Lπ
propose to approximate the CE in (5) during training for both unsupervised and supervised tasks.
Since x and z are symmetric in terms of CE according to (4)  we use x to derive our theoretical
results. Similar arguments hold for z  as discussed in the Supplementary Material (SM).

3

x2z1z2x100z1z2x2x1x2z1z2x100z1z2x2x1x2z1z2x1z1z2x2x1(a)(b)(c)/2/21/21/21/21/2(1)/2(1)/23.2 Unsupervised Learning
In the absence of explicit probability distributions needed for computing the CE  we can bound
the CE using the criterion of cycle-consistency [12]. We denote the reconstruction of x as ˆx  via
generating procedure (cycle) ˆx ∼ pθ(ˆx|z)  z ∼ qφ(z|x)  x ∼ q(x). We desire that pθ(ˆx|z) have
high likelihood for ˆx = x  for the x ∼ q(x) that begins the cycle x → z → ˆx  and hence that ˆx
be similar to the original x. Lemma 3 below shows that cycle-consistency is an upper bound of the
conditional entropy in (4).
Lemma 3 For joint distributions pθ(x  z) or qφ(x  z)  we have
H qφ(x|z) (cid:44) −E
qφ(x z)[log pθ(x|z)] − E
where qφ(z) = (cid:82) dxqφ(x  z). The proof is in the SM. Note that latent z is implicitly involved
qφ(x z)[log pθ(x|z)] (cid:44) LCycle(θ  φ).
in LCycle(θ  φ) via E
following upper bound of (5):

qφ(x z)[·]. For the unsupervised case we want to leverage (6) to optimize the
(7)

qφ(x z)[log qφ(x|z)] = −E
≤ −E

qφ(z)[KL(qφ(x|z)(cid:107)pθ(x|z))]
(6)

min
θ φ

ω LALI(θ  φ  ω) + LCycle(θ  φ) .
max

Note that as ALI reaches its optimum  pθ(x  z) and qφ(x  z) reach saddle point π(x  z)  then
LCycle(θ  φ) → H qφ(x|z) → H π(x|z) in (4) accordingly  thus (7) effectively approaches (5)
(ALICE). Unlike Lπ
CE(θ  φ) in (4)  its upper bound  LCycle(θ  φ)  can be easily approximated via
Monte Carlo simulation. Importantly  (7) can be readily added to ALI’s objective without additional
changes to the original training procedure.
The cycle-consistency property has been previously leveraged in CycleGAN [12]  DiscoGAN [13]
and DualGAN [14]. However  in [12  13  14]  cycle-consistency  LCycle(θ  φ)  is implemented via (cid:96)k
losses  for k = 1  2  and real-valued data such as images. As a consequence of an (cid:96)2-based pixel-wise
loss  the generated samples tend to be blurry [8]. Recognizing this limitation  we further suggest
to enforce cycle-consistency (for better reconstruction) using fully adversarial training (for better
generation)  as an alternative to LCycle(θ  φ) in (7). Speciﬁcally  to reconstruct x  we specify an
η-parameterized discriminator fη(x  ˆx) to distinguish between x and its reconstruction ˆx:

min
θ φ

η LA
max

Cycle(θ  φ  η) = Ex∼q(x)[log σ(fη(x  x))]

+ Eˆx∼pθ (ˆx|z) z∼qφ(z|x) log(1 − σ(fη(x  ˆx)))].

(8)
Finally  the fully adversarial training algorithm for unsupervised learning using the ALICE framework
Cycle(θ  φ  η) in (7); thus  for ﬁxed (θ  φ)  we maximize
is the result of replacing LCycle(θ  φ) with LA
wrt {ω  η}.
The use of paired samples {x  ˆx} in (8) is critical.
It encourages the generators to mimic the
reconstruction relationship implied in the ﬁrst joint; on the contrary  the model may reduce to the
basic GAN discussed in Section 3  and generate any realistic sample in X . The objective in (8)
enjoys many theoretical properties of GAN. Particularly  Proposition 1 guarantees the existence of
the optimal generator and discriminator.
Proposition 1 The optimal generators and discriminator {θ
achieved  if and only if E

} of the objective in (8) is

∗
  φ

  η∗

∗

qφ∗ (z|x)pθ∗ (ˆx|z) = δ(x − ˆx).

The proof is provided in the SM. Together with Lemma 2 and 3  we can also show that:

Corollary 1 When cycle-consistency is satisﬁed (the optimum in (8) is achieved)  (i) a determin-
istic mapping enforces E
qφ(z)[KL(qφ(x|z)(cid:107)pθ(x|z))] = 0  which indicates the conditionals are
matched. (ii) On the contrary  the matched conditionals enforce H qφ(x|z) = 0  which indicates the
corresponding mapping becomes deterministic.
3.3 Semi-supervised Learning
When the objective in (7) is optimized in an unsupervised way  the identiﬁability issues associated
with ALI are largely reduced due to the cycle-consistency-enforcing bound in Lemma 3. This
means that samples in the training data have been probabilistically “paired” with high certainty 
by conditionals pθ(x|z) and pφ(z|x)  though perhaps not in the desired conﬁguration. In real-
world applications  obtaining correctly paired data samples for the entire dataset is expensive or

4

even impossible. However  in some situations obtaining paired data for a very small subset of the
observations may be feasible. In such a case  we can leverage the small set of empirically paired
samples  to further provide guidance on selecting the correct conﬁguration. This suggests that ALICE
is suitable for semi-supervised classiﬁcation.
For a paired sample drawn from empirical distribution ˜π(x  z)  its desirable joint distribution is well
speciﬁed. Thus  one can directly approximate the CE as

H ˜π(x|z) ≈ E

(9)
where the approximation (≈) arises from the fact that pθ(x|z) is an approximation to ˜π(x|z). For
the supervised case we leverage (9) to approximate (5) using the following minimax objective:
(10)

˜π(x z)[log pθ(x|z)] (cid:44) LMap(θ)  

min
θ φ

ω LALI(θ  φ  ω) + LMap(θ).
max

Note that as ALI reaches its optimum  pθ(x  z) and qφ(x  z) reach saddle point π(x  z)  then
LMap(θ) → H ˜π(x|z) → H π(x|z) in (4) accordingly  thus (10) approaches (5) (ALICE).
We can employ standard losses for supervised learning objectives to approximate LMap(θ) in (10) 
such as cross-entropy or (cid:96)k loss in (9). Alternatively  to also improve generation ability  we propose
an adversarial learning scheme to directly match pθ(x|z) to the paired empirical conditional ˜π(x|z) 
using conditional GAN [15] as an alternative to LMap(θ) in (10). The χ-parameterized discriminator
fχ is used to distinguish the true pair {x  z} from the artiﬁcially generated one {ˆx  z} (conditioned
on z)  using
(11)
χ LA
max

Map(θ  χ) = Ex z∼˜π(x z)[log σ(fχ(x  z)) + Eˆx∼pθ (ˆx|z) log(1 − σ(fχ(ˆx  z)))].

min

θ

Map(θ  χ) in (10)  thus for ﬁxed (θ  φ) we maximize wrt {ω  χ}.

The fully adversarial training algorithm for supervised learning using the ALICE in (11) is the result
of replacing LMap(θ) with LA
  χ∗
Proposition 2 The optimum of generators and discriminator {θ
} form saddle points of objective
in (11)  if and only if ˜π(x|z) = pθ∗ (x|z) and ˜π(x  z) = pθ∗ (x  z).
The proof is provided in the SM. Proposition 2 enforces that the generator will map to the correctly
paired sample in the other space. Together with the theoretical result for ALI in Lemma 2  we have

∗

Corollary 2 When the optimum in (10) is achieved  ˜π(x  z) = pθ∗ (x  z) = qφ∗ (x  z).
Corollary 2 indicates that ALI’s drawbacks associated with identiﬁability issues can be alleviated for
the fully supervised learning scenario. Two conditional GANs can be used to boost the perfomance 
each for one direction mapping. When tying the weights of discriminators of two conditional GANs 
ALICE recovers Triangle GAN [16]. In practice  samples from the paired set ˜π(x  z) often contain
enough information to readily approximate the sufﬁcient statistics of the entire dataset. In such case 
we may use the following objective for semi-supervised learning:

min
θ φ

ω LALI(θ  φ  ω) + LCycle(θ  φ) + LMap(θ) .
max

(12)

The ﬁrst two terms operate on the entire set  while the last term only applies to the paired subset. Note
that we can train (12) fully adversarially by replacing LCycle(θ  φ) and LMap(θ) with LA
Cycle(θ  φ  η)
Map(θ  χ) in (8) and (11)  respectively. In (12) each of the three terms are treated with equal
and LA
weighting in the experiments if not speciﬁcially mentioned  but of course one may introduce additional
hyperparameters to adjust the relative emphasis of each term.
4 Related Work: A Uniﬁed Perspective for Joint Distribution Matching
Connecting ALI and CycleGAN. We provide an information theoretical interpretation for cycle-
consistency  and show that it is equivalent to controlling conditional entropies and matching con-
ditional distributions. When cycle-consistency is satisﬁed  Corollary 1 shows that the conditionals
are matched in CycleGAN. They also train additional discriminators to guarantee the matching of
marginals for x and z using the original GAN objective in (2). This reveals the equivalence between
ALI and CycleGAN  as the latter can also guarantee the matching of joint distributions pθ(x  z) and
qφ(x  z). In practice  CycleGAN is easier to train  as it decomposes the joint distribution matching
objective (as in ALI) into four subproblems. Our approach leverages a similar idea  and further
improves it with adversarially learned cycle-consistency  when high quality samples are of interest.

5

(a) True x

(b) True z

(c) Inception Score

(d) MSE

Figure 2: Quantitative evaluation of generation (c) and reconstruction (d) results on toy data (a b).

Stochastic Mapping vs. Deterministic Mapping. We propose to enforce the cycle-consistency in
ALI for the case when two stochastic mappings are speciﬁed as in (1). When cycle-consistency is
achieved  Corollary 1 shows that the bounded conditional entropy vanishes  and thus the corresponding
mapping reduces to be deterministic. In the literture  one deterministic mapping has been empirically
tested in ALI’s framework [4]  without explicitly specifying cycle-consistency. BiGAN [10] uses
two deterministic mappings. In theory  deterministic mappings guarantee cycle-consistency in ALI’s
framework. However  to achieve this  the model has to ﬁt a delta distribution (deterministic mapping)
to another distribution in the sense of KL divergence (see Lemma 3). Due to the asymmetry of
KL  the cost function will pay extremely low cost for generating fake-looking samples [17]. This
explains the underﬁtting reasoning in [4] behind the subpar reconstruction ability of ALI. Therefore 
in ALICE  we explicitly add a cycle-consistency regularization to accelerate and stabilize training.
Conditional GANs as Joint Distribution Matching. Conditional GAN and its variants [15  18  19 
20] have been widely used in supervised tasks. Our scheme to learn conditional entropy borrows the
formulation of conditional GAN [15]. To the authors’ knowledge  this is the ﬁrst attempt to study the
conditional GAN formulation as joint distribution matching problem. Moreover  we add the potential
to leverage the well-deﬁned distribution implied by paired data  to resolve the ambiguity issues of
unsupervised ALI variants [4  10  12  13  14].
5 Experimental Results
The code to reproduce these experiments is at https://github.com/ChunyuanLI/ALICE
5.1 Effectiveness and Stability of Cycle-Consistency
To highlight the role of the CE regularization for unsupervised learning  we perform an experiment
on a toy dataset. q(x) is a 2D Gaussian Mixture Model (GMM) with 5 mixture components  and
p(z) is chosen as a standard Gaussian  N (0  I). Following [4]  the covariance matrices and centroids
are chosen such that the distribution exhibits severely separated modes  which makes it a relatively
hard task despite its 2D nature. Following [21]  to study stability  we run an exhaustive grid search
over a set of architectural choices and hyper-parameters  576 experiments for each method. We report
Mean Squared Error (MSE) and inception score (denoted as ICP) [22] to quantitatively evaluate the
performance of generative models. MSE is a proxy for reconstruction quality  while ICP reﬂects the
plausibility and variety of sample generation. Lower MSE and higher ICP indicate better results. See
SM for the details of the grid search and the calculation of ICP.
We train on 2048 samples  and test on 1024 samples. The ground-truth test samples for x and z are
shown in Figure 2(a) and (b)  respectively. We compare ALICE  ALI and Denoising Auto-Encoders
(DAEs) [23]  and report the distribution of ICP and MSE values  for all (576) experiments in Figure 2
(c) and (d)  respectively. For reference  samples drawn from the “oracle” (ground-truth) GMM yield
ICP=4.977±0.016. ALICE yields an ICP larger than 4.5 in 77% of experiments  while ALI’s ICP
wildly varies across different runs. These results demonstrate that ALICE is more consistent and
quantitatively reliable than ALI. The DAE yields the lowest MSE  as expected  but it also results in
the weakest generation ability. The comparatively low MSE of ALICE demonstrates its acceptable
reconstruction ability compared to DAE  though a very signiﬁcantly improvement over ALI.
Figure 3 shows the qualitative results on the test set. Since ALI’s results vary largely from trial to
trial  we present the one with highest ICP. In the ﬁgure  we color samples from different mixture
components to highlight their correspondance between the ground truth  in Figure 2(a)  and their
reconstructions  in Figure 3 (ﬁrst row  columns 2  4 and 6  for ALICE  ALI and DAE  respectively).
Importantly  though the reconstruction of ALI can recover the shape of manifold in x (Gaussian
mixture)  each individual reconstructed sample can be substantially far away from its “original”
mixture component (note the highly mixed coloring)  hence the poor MSE. This occurs because the
adversarial training in ALI only requires that the generated samples look realistic  i.e.  to be located

6

(a) ALICE

(b) ALI

(c) DAEs

Figure 3: Qualitative results on toy data. Two-column blocks represent the results of each method  with left for
z and right for x. For the ﬁrst row  left is sampling of z  and right is reconstruction of x. Colors indicate mixture
component membership. The second row shows reconstructions  x  from linearly interpolated samples in z.
near true samples in X   but the mapping between observed and latent spaces (x → z and z → x) is
not speciﬁed. In the SM we also consider ALI with various combinations of stochastic/deterministic
mappings  and conclude that models with deterministic mappings tend to have lower reconstruction
ability but higher generation ability. In terms of the estimated latent space  z  in Figure 3 (ﬁrst row 
columns 1  3 and 5  for ALICE  ALI and DAE  respectively)  we see that ALICE results in a better
latent representation  in the sense of mapping consistency (samples from different mixture components
remain clustered) and distribution consistency (samples approximate a Gaussian distribution). The
results for reconstruction of z and sampling of x are shown in the SM.
In Figure 3 (second row)  we also investigate latent space interpolation between a pair of test set
examples. We use x1 = [−2.2 −2.2] and x9 = [2.2  2.2]  map them into z1 and z9  linearly
interpolate between z1 and z9 to get intermediate points z2  . . .   z8  and then map them back to the
original space as x2  . . .   x8. We only show the index of the samples for better visualization. Figure 3
shows that ALICE’s interpolation is smooth and consistent with the ground-truth distributions.
Interpolation using ALI results in realistic samples (within mixture components)  but the transition is
not order-wise consistent. DAEs provides smooth transitions  but the samples in the original space
look unrealistic as some of them are located in low probability density regions of the true model.
We investigate the impact of different amount of regularization on three datasets  including the toy
dataset  MNIST and CIFAR-10 in SM Section D. The results show that our regularizer can improve
image generation and reconstruction of ALI for a large range of weighting hyperparameter values.
5.2 Reconstruction and Cross-Domain Transformation on Real Datasets
Two image-to-image translation tasks are considered. (i) Car-to-Car [24]: each domain (x and z)
includes car images in 11 different angles  on which we seek to demonstrate the power of adversarially
learned reconstruction and weak supervision. (ii) Edge-to-Shoe [25]: x domain consists of shoe
photos and z domain consists of edge images  on which we report extensive quantitative comparisons.
Cycle-consistency is applied on both domains. The goal is to discover the cross-domain relationship
(i.e.  cross-domain prediction)  while maintaining reconstruction ability on each domain.
Adversarially learned reconstruction To demonstrate the effectiveness of our fully adversarial
scheme in (8) (Joint A.) on real datasets  we use it in place of the (cid:96)2 losses in DiscoGAN [13]. In
practice  feature matching [22] is used to help the adversarial objective in (8) to reach its optimum.
We also compared with a baseline scheme (Marginal A.) in [12]  which adversarially discriminates
between x and its reconstruction ˆx.
The results are shown in Figure 4 (a). From
top to bottom  each row shows ground-truth
images  DiscoGAN (with Joint A.  (cid:96)2 loss
and Marginal A. schemes  respectively) and
BiGAN [10]. Note that BiGAN is the best
ALI variant in our grid search compasion.
The proposed Joint A. scheme can retain the
same crispness characteristic to adversarially-
trained models  while (cid:96)2 tends to be blurry.
Marginal A. provides realistic car images  but not faithful reproductions of the inputs. This explains

Figure 4: Results on Car-to-Car task.

(a) Reconstruction

(b) Prediction

7

Inputs`2BiGANJointA.MarginalA.loss246810Number of Paired Angles20406080Classification Accuracy (%)ALICE (10% sup.)ALICE (1% sup.)DiscoGANBiGAN(a) Cross-domain transformation

(b) Reconstruction

Figure 5: SSIM and generated images on Edge-to-Shoe dataset.

(c) Generated edges

the observations in [12] in terms of no performance gain. The BiGAN learns the shapes of cars  but
misses the textures. This is a sign of underﬁtting  thus indicating BiGAN is not easy to train.
Weak supervision The DiscoGAN and BiGAN are unsupervised methods  and exhibit very different
cross-domain pairing conﬁgurations during different training epochs  which is indicative of non-
identiﬁability issues. We leverage very weak supervision to help with convergence and guide the
pairing. The results on shown in Figure 4 (b). We run each methods 5 times  the width of the
colored lines reﬂect the standard deviation. We start with 1% true pairs for supervision  which yields
signiﬁcantly higher accuracy than DiscoGAN/BiGAN. We then provided 10% supervison in only 2
or 6 angles (of 11 total angles)  which yields comparable angle prediction accuracy with full angle
supervison in testing. This shows ALICE’s ability in terms of zero-shot learning  i.e.  predicting
unseen pairs. In the SM  we show that enforcing different weak supervision strategies affects the ﬁnal
pairing conﬁgurations  i.e.  we can leverage supervision to obtain the desirable joint distribution.
Quantitative comparison To quantitatively assess the generated images  we use structural similarity
(SSIM) [26]  which is an established image quality metric that correlates well with human visual
perception. SSIM values are between [0  1]; higher is better. The SSIM of ALICE on prediction and
reconstruction is shown in Figure 5 (a)(b) for the edge-to-shoe task. As a baseline  we set DiscoGAN
with (cid:96)2-based supervision ((cid:96)2-sup). BiGAN/ALI  highlighted with a circle is outperformed by
ALICE in two aspects: (i) In the unpaired setting (0% supervision)  cycle-consistency regularization
(LCycle) shows signiﬁcant performance gains  particularly on reconstruction. (ii) When supervision
is leveraged (10%)  SSIM is signiﬁcantly increased on prediction. The adversarial-based supervision
((cid:96)A-sup) shows higher prediction than (cid:96)2-sup. ALICE achieves very similar performance with the
50% and full supervision setup  indicating its advantage of in semi-supervised learning. Several
generated edge images (with 50% supervision) are shown in Figure 5(c)  (cid:96)A-sup tends to provide
more details than (cid:96)2-sup. Both methods generate correct paired edges  and quality is higher than
BiGAN and DiscoGAN. In the SM  we also report MSE metrics  and results on edge domain only 
which are consistent with the results presented here.
One-side cycle-consistency When uncertainty in one domain is desirable  we consider one-side
cycle-consistency. This is demonstrated on the CelebA face dataset [27]. Each face is associated
with a 40-dimensional attribute vector. The results are in the Figure 8 of SM. In the ﬁrst task  we
consider the images x are generated from a 128-dimensional Gaussian latent space z  and apply
LCycle on x. We compare ALICE and ALI on reconstruction in Figure 8 (a)(b). ALICE shows more
faithful reproduction of the input subjects. In the second task  we consider z as the attribute space 
from which the images x are generated. The mapping from x to z is then attribute classiﬁcation. We
Map on both domains. When 10% paired samples
only apply LCycle on the attribute domain  and LA
are considered  the predicted attributes still reach 86% accuracy  which is comparable with the fully
supervised case. To test the diversity on x  we ﬁrst predict the attributes of a true face image  and
then generated multiple images conditioned on the predicted attributes. Four examples are shown in
Figure 8 (c).
6 Conclusion
We have studied the problem of non-identiﬁability in bidirectional adversarial networks. A uniﬁed
perspective of understanding various GAN models as joint matching is provided to tackle this problem.
This insight enables us to propose ALICE (with both adversarial and non-adversarial solutions) to
reduce the ambiguity and control the conditionals in unsupervised and semi-supervised learning. For
future work  the proposed view can provide opportunities to leverage the advantages of each model 
to advance joint-distribution modeling.

8

BiGANBiGAN(cid:11)(cid:20)(cid:23)(cid:14)(cid:1)(cid:14)(cid:13)(cid:15)(cid:14)(cid:21)(cid:8)(cid:17)(cid:19)(cid:23)(cid:22)(cid:21)(cid:3)(cid:16)(cid:7)(cid:2)(cid:10)(cid:5)(cid:16)(cid:21)(cid:12)(cid:18)(cid:7)(cid:2)(cid:10)Lcycle+`2Lcycle+`A(cid:2)(cid:9)(cid:8)(cid:4)(cid:6)(cid:2)(cid:9)(cid:8)(cid:4)(cid:6)Acknowledgements We acknowledge Shuyang Dai  Chenyang Tao and Zihang Dai for helpful
feedback/editing. This research was supported in part by ARO  DARPA  DOE  NGA  ONR and NSF.

References
[1] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In NIPS  2014.

[2] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR  2014.
[3] X. Chen  Y. Duan  R. Houthooft  J. Schulman  I. Sutskever  and P. Abbeel. InfoGAN: Interpretable

representation learning by information maximizing generative adversarial nets. In NIPS  2016.

[4] V. Dumoulin  I. Belghazi  B. Poole  A. Lamb  M. A.  O. Mastropietro  and A. Courville. Adversarially

learned inference. ICLR  2017.

[5] L. Mescheder  S. Nowozin  and A. Geiger. Adversarial variational bayes: Unifying variational autoencoders

and generative adversarial networks. ICML  2017.

[6] Y. Pu  Z. Gan  R. Henao  X. Yuan  C. Li  A. Stevens  and L. Carin. Variational autoencoder for deep

learning of images  labels and captions. In NIPS  2016.

[7] Y. Pu  Z. Gan  R. Henao  C. Li  S. Han  and L. Carin. Vae learning via Stein variational gradient descent.

NIPS  2017.

[8] A. B. L. Larsen  S. K. Sønderby  H. Larochelle  and O. Winther. Autoencoding beyond pixels using a

learned similarity metric. ICML  2016.

[9] A. Makhzani  J. Shlens  N. Jaitly  I. Goodfellow  and B. Frey. Adversarial autoencoders. arXiv preprint

arXiv:1511.05644  2015.

[10] J. Donahue  K. Philipp  and T. Darrell. Adversarial feature learning. ICLR  2017.
[11] Y. Pu  W. Wang  R. Henao  L. Chen  Z. Gan  C. Li  and L. Carin. Adversarial symmetric variational

autoencoder. NIPS  2017.

[12] J. Zhu  T. Park  P. Isola  and A. Efros. Unpaired image-to-image translation using cycle-consistent

adversarial networks. ICCV  2017.

[13] T. Kim  M. Cha  H. Kim  J. Lee  and J. Kim. Learning to discover cross-domain relations with generative

adversarial networks. ICML  2017.

[14] Z. Yi  H. Zhang  and P. Tan. DualGAN: Unsupervised dual learning for image-to-image translation. ICCV 

2017.

[15] M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784  2014.
[16] Z. Gan  L. Chen  W. Wang  Y. Pu  Y. Zhang  H. Liu  C. Li  and L. Carin. Triangle generative adversarial

networks. NIPS  2017.

[17] M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks. In

ICLR  2017.

[18] S. Reed  Z. Akata  X. Yan  L. Logeswaran  B. Schiele  and H. Lee. Generative adversarial text to image

synthesis. In ICML  2016.

[19] P. Isola  J. Zhu  T. Zhou  and A. Efros. Image-to-image translation with conditional adversarial networks.

CVPR  2017.

[20] C. Li  K. Xu  J. Zhu  and B. Zhang. Triple generative adversarial nets. NIPS  2017.
[21] J. Zhao  M. Mathieu  and Y. LeCun. Energy-based generative adversarial network. ICLR  2017.
[22] T. Salimans  I. Goodfellow  W. Zaremba  V. Cheung  A. Radford  and X. Chen. Improved techniques for

training GANs. In NIPS  2016.

[23] P. Vincent  H. Larochelle  Y. Bengio  and P. Manzagol. Extracting and composing robust features with

denoising autoencoders. In ICML  2008.

[24] S. Fidler  S. Dickinson  and R. Urtasun. 3D object detection and viewpoint estimation with a deformable

3D cuboid model. In NIPS  2012.

[25] A. Yu and K. Grauman. Fine-grained visual comparisons with local learning. In CVPR  2014.
[26] Z. Wang  A. C Bovik  H. R Sheikh  and E. P Simoncelli. Image quality assessment: from error visibility to

structural similarity. IEEE trans. on Image Processing  2004.

[27] Z. Liu  P. Luo  X. Wang  and X. Tang. Deep learning face attributes in the wild. In ICCV  2015.

9

,Chunyuan Li
Hao Liu
Changyou Chen
Yuchen Pu
Liqun Chen
Ricardo Henao
Lawrence Carin
Stepan Tulyakov
Anton Ivanov
François Fleuret