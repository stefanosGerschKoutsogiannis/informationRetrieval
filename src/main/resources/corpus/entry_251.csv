2014,A Wild Bootstrap for Degenerate Kernel Tests,A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes  for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics  which are degenerate under the null hypothesis  and non-degenerate elsewhere. To illustrate this approach  we construct a two-sample test  an instantaneous independence test and a multiple lag independence test for time series. In experiments  the wild bootstrap gives strong performance on synthetic examples  on audio data  and in performance benchmarking for the Gibbs sampler. The code is available at https://github.com/kacperChwialkowski/wildBootstrap.,A Wild Bootstrap for Degenerate Kernel Tests

Kacper Chwialkowski

Department of Computer Science

University College London

London  Gower Street  WC1E 6BT

kacper.chwialkowski@gmail.com

Dino Sejdinovic

Gatsby Computational Neuroscience Unit  UCL

17 Queen Square  London WC1N 3AR
dino.sejdinovic@gmail.com

Arthur Gretton

Gatsby Computational Neuroscience Unit  UCL

17 Queen Square  London WC1N 3AR

arthur.gretton@gmail.com

Abstract

A wild bootstrap method for nonparametric hypothesis tests based on kernel dis-
tribution embeddings is proposed. This bootstrap method is used to construct
provably consistent tests that apply to random processes  for which the naive
permutation-based bootstrap fails.
It applies to a large group of kernel tests
based on V-statistics  which are degenerate under the null hypothesis  and non-
degenerate elsewhere. To illustrate this approach  we construct a two-sample test 
an instantaneous independence test and a multiple lag independence test for time
series. In experiments  the wild bootstrap gives strong performance on synthetic
examples  on audio data  and in performance benchmarking for the Gibbs sampler.
The code is available at https://github.com/kacperChwialkowski/
wildBootstrap.

Introduction

1
Statistical tests based on distribution embeddings into reproducing kernel Hilbert spaces have been
applied in many contexts  including two sample testing [19  15  31]  tests of independence [17  32 
4]  tests of conditional independence [14  32]  and tests for higher order (Lancaster) interactions
[24]. For these tests  consistency is guaranteed if and only if the observations are independent and
identically distributed. Much real-world data fails to satisfy the i.i.d. assumption: audio signals 
EEG recordings  text documents  ﬁnancial time series  and samples obtained when running Markov
Chain Monte Carlo  all show signiﬁcant temporal dependence patterns.
The asymptotic behaviour of kernel test statistics becomes quite different when temporal dependen-
cies exist within the samples. In recent work on independence testing using the Hilbert-Schmidt
Independence Criterion (HSIC) [8]  the asymptotic distribution of the statistic under the null hy-
pothesis is obtained for a pair of independent time series  which satisfy an absolute regularity or
a φ-mixing assumption. In this case  the null distribution is shown to be an inﬁnite weighted sum
of dependent χ2-variables  as opposed to the sum of independent χ2-variables obtained in the i.i.d.
setting [17]. The difference in the asymptotic null distributions has important implications in prac-
tice: under the i.i.d. assumption  an empirical estimate of the null distribution can be obtained by
repeatedly permuting the time indices of one of the signals. This breaks the temporal dependence
within the permuted signal  which causes the test to return an elevated number of false positives 
when used for testing time series. To address this problem  an alternative estimate of the null distri-
bution is proposed in [8]  where the null distribution is simulated by repeatedly shifting one signal
relative to the other. This preserves the temporal structure within each signal  while breaking the
cross-signal dependence.

1

1

nm−1

(cid:80)

A serious limitation of the shift procedure in [8] is that it is speciﬁc to the problem of independence
testing: there is no obvious way to generalise it to other testing contexts. For instance  we might
have two time series  with the goal of comparing their marginal distributions - this is a generalization
of the two-sample setting to which the shift approach does not apply.
We note  however  that many kernel tests have a test statistic with a particular structure: the Maxi-
mum Mean Discrepancy (MMD)  HSIC  and the Lancaster interaction statistic  each have empirical
estimates which can be cast as normalized V -statistics 
1≤i1 ... im≤n h(Zi1  ...  Zim)  where
Zi1  ...  Zim are samples from a random process at the time points {i1  . . .   im}. We show that a
method of external randomization known as the wild bootstrap may be applied [22  28] to simulate
from the null distribution. In brief  the arguments of the above sum are repeatedly multiplied by
random  user-deﬁned time series. For a test of level α  the 1 − α quantile of the empirical distri-
bution obtained using these perturbed statistics serves as the test threshold. This approach has the
important advantage over [8] that it may be applied to all kernel-based tests for which V -statistics
are employed  and not just for independence tests.
The main result of this paper is to show that the wild bootstrap procedure yields consistent tests
for time series  i.e.  tests based on the wild bootstrap have a Type I error rate (of wrongly rejecting
the null hypothesis) approaching the design parameter α  and a Type II error (of wrongly accepting
the null) approaching zero  as the number of samples increases. We use this result to construct a
two-sample test using MMD  and an independence test using HSIC. The latter procedure is applied
both to testing for instantaneous independence  and to testing for independence across multiple time
lags  for which the earlier shift procedure of [8] cannot be applied.
We begin our presentation in Section 2  with a review of the τ-mixing assumption required of the
time series  as well as of V -statistics (of which MMD and HSIC are instances). We also introduce
the form taken by the wild bootstrap. In Section 3  we establish a general consistency result for
the wild bootstrap procedure on V -statistics  which we apply to MMD and to HSIC in Section 4.
Finally  in Section 5  we present a number of empirical comparisons: in the two sample case  we test
for differences in audio signals with the same underlying pitch  and present a performance diagnostic
for the output of a Gibbs sampler (the MCMC M.D.); in the independence case  we test for inde-
pendence of two time series sharing a common variance (a characteristic of econometric models) 
and compare against the test of [4] in the case where dependence may occur at multiple  potentially
unknown lags. Our tests outperform both the naive approach which neglects the dependence struc-
ture within the samples  and the approach of [4]  when testing across multiple lags. Detailed proofs
are found in the appendices of an accompanying technical report [9]  which we reference from the
present document as needed.

2 Background

The main results of the paper are based around two concepts: τ-mixing [10]  which describes the
dependence within the time series  and V -statistics [27]  which constitute our test statistics. In this
section  we review these topics  and introduce the concept of wild bootstrapped V -statistics  which
will be the key ingredient in our test construction.

τ-mixing. The notion of τ-mixing is used to characterise weak dependence. It is a less restrictive
alternative to classical mixing coefﬁcients  and is covered in depth in [10]. Let {Zt Ft}t∈N be a sta-
tionary sequence of integrable random variables  deﬁned on a probability space Ω with a probability
measure P and a natural ﬁltration Ft. The process is called τ-dependent if

τ (r) = sup
l∈N
τ (M  X) = E

(cid:18)

1
l

sup

r≤i1≤...≤il

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

sup
g∈Λ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:19)

τ (F0  (Zi1  ...  Zil )) r→∞−→ 0  where

(cid:90)

g(t)PX|M(dt) −

g(t)PX (dt)

and Λ is the set of all one-Lipschitz continuous real-valued functions on the domain of X. τ (M  X)
can be interpreted as the minimal L1 distance between X and X∗ such that X d= X∗ and X∗
is independent of M ⊂ F. Furthermore  if F is rich enough  this X∗ can be constructed (see
Proposition 4 in the Appendix). More information is provided in the Appendix B.

2

(cid:88)

1
nm

V -statistics. The test statistics considered in this paper are always V -statistics. Given the ob-
servations Z = {Zt}n
t=1  a V -statistic of a symmetric function h taking m arguments is given by

i∈N m

V (h  Z) =

h(Zi1  ...  Zim ) 

j+1  . . .   Z∗

m) = 0  where Z∗

(1)
where N m is a Cartesian power of a set N = {1  ...  n}. For simplicity  we will often drop the
second argument and write simply V (h).
We will refer to the function h as to the core of the V -statistic V (h). While such functions
are usually called kernels in the literature  in this paper we reserve the term kernel for positive-
deﬁnite functions taking two arguments. A core h is said to be j-degenerate if for each z1  . . .   zj
Eh(z1  . . .   zj  Z∗
m are independent copies of Z1. If h is
j-degenerate for all j ≤ m − 1  we will say that it is canonical. For a one-degenerate core
h  we deﬁne an auxiliary function h2  called the second component of the core  and given by
h2(z1  z2) = Eh(z1  z2  Z∗
m). Finally we say that nV (h) is a normalized V -statistic  and
that a V -statistic with a one-degenerate core is a degenerate V -statistic. This degeneracy is common
to many kernel statistics when the null hypothesis holds [15  17  24].
Our main results will rely on the fact that h2 governs the asymptotic behaviour of normalized degen-
erate V -statistics. Unfortunately  the limiting distribution of such V -statistics is quite complicated
- it is an inﬁnite sum of dependent χ2-distributed random variables  with a dependence determined
by the temporal dependence structure within the process {Zt} and by the eigenfunctions of a certain
integral operator associated with h2 [5  8]. Therefore  we propose a bootstrapped version of the
V -statistics which will allow a consistent approximation of this difﬁcult limiting distribution.

j+1  . . .   Z∗

3   . . .   Z∗

Bootstrapped V -statistic. We will study two versions of the bootstrapped V -statistics

(cid:88)
(cid:88)

1
nm
1
nm

Vb1(h  Z) =

Vb2(h  Z) =

i∈N m

i∈N m

Wi1 nWi2 nh(Zi1   ...  Zim ) 

˜Wi1 n ˜Wi2 nh(Zi1   ...  Zim ) 

(2)

(3)

(cid:80)n

n

j=1 Wj n. This

and(cid:80)n−1

where {Wt n}1≤t≤n is an auxiliary wild bootstrap process and ˜Wt n = Wt n − 1
auxiliary process  proposed by [28  22]  satisﬁes the following assumption:
Bootstrap assumption: {Wt n}1≤t≤n is a row-wise strictly stationary triangular array independent
of all Zt such that EWt n = 0 and supn E|W 2+σ
t n | < ∞ for some σ > 0. The autocovariance of the
process is given by EWs nWt n = ρ(|s − t|/ln) for some function ρ  such that limu→0 ρ(u) = 1
r=1 ρ(|r|/ln) = O(ln). The sequence {ln} is taken such that ln = o(n) but limn→∞ ln =
∞. The variables Wt n are τ-weakly dependent with coefﬁcients τ (r) ≤ Cζ
ln for r = 1  ...  n 
ζ ∈ (0  1) and C ∈ R.
As noted in in [22  Remark 2]  a simple realization of a process that satisﬁes this assumption is
1 − e−2/ln t where W0 n and 1  . . .   n are independent standard nor-
Wt n = e−1/ln Wt−1 n +
mal random variables. For simplicity  we will drop the index n and write Wt instead of Wt n. A
process that fulﬁls the bootstrap assumption will be called bootstrap process. Further discussion of
the wild bootstrap is provided in the Appendix A. The versions of the bootstrapped V -statistics in
(2) and (3) were previously studied in [22] for the case of canonical cores of degree m = 2. We
extend their results to higher degree cores (common within the kernel testing framework)  which are
not necessarily one-degenerate. When stating a fact that applies to both Vb1 and Vb2  we will simply
write Vb  and the argument Z will be dropped when there is no ambiguity.

√

r

3 Asymptotics of wild bootstrapped V -statistics

In this section  we present main Theorems that describe asymptotic behaviour of V -statistics. In
the next section  these results will be used to construct kernel-based statistical tests applicable to
dependent observations. Tests are constructed so that the V -statistic is degenerate under the null
hypothesis and non-degenerate under the alternative. Theorem 1 guarantees that the bootstrapped
V -statistic will converge to the same limiting null distribution as the simple V -statistic. Following
[22]  we will establish the convergence of the bootstrapped distribution to the desired asymptotic

3

distribution in the Prokhorov metric ϕ [13  Section 11.3])  and ensure that this distance approaches
zero in probability as n → ∞. This two-part convergence statement is needed due to the additional
randomness introduced by the Wj n.
Theorem 1. Assume that the stationary process {Zt} is τ-dependent with τ (r) = O(r−6−) for
some  > 0. If the core h is a Lipschitz continuous  one-degenerate  and bounded function of m
in probability as n → ∞  where ϕ is Prokhorov metric.

arguments and its h2-component is a positive deﬁnite kernel  then ϕ(n(cid:0)m
Proof. By Lemma 3 and Lemma 2 respectively  ϕ(nVb(h)  nVb(h2)) and ϕ(nV (h)  n(cid:0)m

(cid:1)Vb(h  Z)  nV (h  Z)) → 0
(cid:1)V (h2))

2

2

converge to zero. By [22  Theorem 3.1]  nVb(h2) and nV (h2  Z) have the same limiting distribution 
i.e.  ϕ(nVb(h2)  nV (h2  Z)) → 0 in probability under certain assumptions. Thus  it sufﬁces to check
these assumptions hold: Assumption A2. (i) h2 is one-degenerate and symmetric - this follows from
Lemma 1; (ii) h2 is a kernel - is one of the assumptions of this Theorem; (iii) Eh2(Z1  Z1) ≤ ∞ - by
Lemma 7  h2 is bounded and therefore has a ﬁnite expected value; (iv) h2 is Lipschitz continuous
r=1 r−1−/2 ≤ ∞. Assumption B2. This assumption about the auxiliary

- follows from Lemma 7. Assumption B1. (cid:80)n
r=1 r2(cid:112)τ (r) ≤ C(cid:80)n
(cid:80)n

r=1 r2(cid:112)τ (r) < ∞. Since τ (r) = O(r−6−) then

process {Wt} is the same as our Bootstrap assumption.

On the other hand  if the V -statistic is not degenerate  which is usually true under the alternative  it
converges to some non-zero constant. In this setting  Theorem 2 guarantees that the bootstrapped
V -statistic will converge to zero in probability. This property is necessary in testing  as it implies
that the test thresholds computed using the bootstrapped V -statistics will also converge to zero  and
so will the corresponding Type II error. The following theorem is due to Lemmas 4 and 5.
Theorem 2. Assume that the process {Zt} is τ-dependent with a coefﬁcient τ (r) = O(r−6−).
If the core h is a Lipschitz continuous  symmetric and bounded function of m arguments  then
nVb2(h) converges in distribution to some non-zero random variable with ﬁnite variance  and Vb1(h)
converges to zero in probability.

Although both Vb2 and Vb1 converge to zero  the rate and the type of convergence are not the same:
nVb2 converges in law to some random variable while the behaviour of nVb1 is unspeciﬁed. As a
consequence  tests that utilize Vb2 usually give lower Type II error then the ones that use Vb1. On the
other hand  Vb1 seems to better approximate V -statistic distribution under the null hypothesis. This
agrees with our experiments in Section 5 as well as with those in [22  Section 5]).

4 Applications to Kernel Tests

on X is an element µk(P ) ∈ Hk  given by µk(P ) =(cid:82) k(·  x) dP (x) [3  29]. If a measurable kernel

In this section  we describe how the wild bootstrap for V -statistics can be used to construct ker-
nel tests for independence and the two-sample problem  which are applicable to weakly dependent
observations. We start by reviewing the main concepts underpinning the kernel testing framework.
For every symmetric  positive deﬁnite function  i.e.  kernel k : X × X → R  there is an associated
reproducing kernel Hilbert space Hk [3  p. 19]. The kernel embedding of a probability measure P
k is bounded  the mean embedding µk(P ) exists for all probability measures on X   and for many
interesting bounded kernels k  including the Gaussian  Laplacian and inverse multi-quadratics  the
kernel embedding P (cid:55)→ µk(P ) is injective. Such kernels are said to be characteristic [30]. The
RKHS-distance (cid:107)µk(Px) − µk(Py)(cid:107)2Hk
between embeddings of two probability measures Px and
Py is termed the Maximum Mean Discrepancy (MMD)  and its empirical version serves as a popular
statistic for non-parametric two-sample testing [15]. Similarly  given a sample of paired observations
{(Xi  Yi)}n
i=1 ∼ Pxy  and kernels k and l respectively on X and Y domains  the RKHS-distance
(cid:107)µκ(Pxy) − µκ(PxPy)(cid:107)2Hκ
between embeddings of the joint distribution and of the product of the
marginals  measures dependence between X and Y . Here  κ((x  y)  (x(cid:48)  y(cid:48))) = k(x  x(cid:48))l(y  y(cid:48))
is the kernel on the product space of X and Y domains. This quantity is called Hilbert-Schmidt
Independence Criterion (HSIC) [16  17]. When characteristic RKHSs are used  the HSIC is zero
iff X|=Y : this follows from [18]. The empirical statistic is written (cid:91)HSICκ = 1
n2 Tr(KHLH) for
kernel matrices K and L and the centering matrix H = I − 1

n 11(cid:62).

4

nx(cid:88)

i=1

i

j=1

nx(cid:88)
nx(cid:88)
(cid:80)nx

i=1

− 2
nxny

ny(cid:88)

j=1

ny(cid:88)

ny(cid:88)

i=1

j=1

(cid:80)ny

i=1 ∼ Px  and {Yj}ny

4.1 Wild Bootstrap For MMD
j=1 ∼ Py. Our goal is to test the null hypothe-
Denote the observations by {Xi}nx
sis H0 : Px = Py vs. the alternative H1 : Px (cid:54)= Py. In the case where samples have equal sizes  i.e. 
nx = ny  application of the wild bootstrap to MMD-based tests on dependent samples is straight-
forward: the empirical MMD can be written as a V -statistic with the core of degree two on pairs
zi = (xi  yi) given by h(z1  z2) = k(x1  x2)−k(x1  y2)−k(x2  y1)+k(y1  y2). It is clear that when-
ever k is Lipschitz continuous and bounded  so is h. Moreover  h is a valid positive deﬁnite kernel 
since it can be represented as an RKHS inner product (cid:104)k(·  x1) − k(·  y1)  k(·  x2) − k(·  y2)(cid:105)Hk
.
Under the null hypothesis  h is also one-degenerate  i.e.  Eh ((x1  y1)  (X2  Y2)) = 0. Therefore 
we can use the bootstrapped statistics in (2) and (3) to approximate the null distribution and attain a
desired test level.
When nx (cid:54)= ny  however  it is no longer possible to write the empirical MMD as a one-sample
V -statistic. We will therefore require the following bootstrapped version of MMD

(cid:92)MMDk b =

1
n2
x

˜W (x)

j k(xi  xj) − 1
˜W (x)
n2
x

˜W (y)

i

˜W (y)

j k(yi  yj)

(4)

˜W (x)

i

˜W (y)

j k(xi  yj) 

ny

nx

i=1 W (x)

i

j=1 W (y)

j

; {W (x)

t − 1

t − 1

  ˜W (y)

t = W (y)

t = W (x)

t } and {W (y)
t }
where ˜W (x)
are two auxiliary wild bootstrap processes that are independent of {Xt} and {Yt} and also indepen-
dent of each other  both satisfying the bootstrap assumption in Section 2. The following Proposi-
tion shows that the bootstrapped statistic has the same asymptotic null distribution as the empirical
MMD. The proof follows that of [22  Theorem 3.1]  and is given in the Appendix.
Proposition 1. Let k be bounded and Lipschitz continuous  and let {Xt} and {Yt} both be
τ-dependent with coefﬁcients τ (r) = O(r−6−)  but independent of each other. Further  let
nx = ρxn and ny = ρyn where n = nx + ny. Then  under the null hypothesis Px = Py 
ϕ
and (cid:92)MMDk is the MMD between empirical measures.

(cid:17) → 0 in probability as n → ∞  where ϕ is the Prokhorov metric

ρxρyn(cid:92)MMDk  ρxρyn(cid:92)MMDk b

(cid:16)

4.2 Wild Bootstrap For HSIC

Using HSIC in the context of random processes is not new in the machine learning literature. For
a 1-approximating functional of an absolutely regular process [6]  convergence in probability of
the empirical HSIC to its population value was shown in [33]. No asymptotic distributions were
obtained  however  nor was a statistical test constructed. The asymptotics of a normalized V -statistic
were obtained in [8] for absolutely regular and φ-mixing processes [12]. Due to the intractability
of the null distribution for the test statistic  the authors propose a procedure to approximate its null
distribution using circular shifts of the observations leading to tests of instantaneous independence 
i.e.  of Xt|=Yt  ∀t. This was shown to be consistent under the null (i.e.  leading to the correct
Type I error)  however consistency of the shift procedure under the alternative is a challenging open
question (see [8  Section A.2] for further discussion). In contrast  as shown below in Propositions 2
and 3 (which are direct consequences of the Theorems 1 and 2)  the wild bootstrap guarantees test
consistency under both hypotheses: null and alternative  which is a major advantage. In addition  the
wild bootstrap can be used in constructing a test for the harder problem of determining independence
across multiple lags simultaneously  similar to the one in [4].
Following symmetrisation  it is shown in [17  8] that the empirical HSIC can be written as a degree
four V -statistic with core given by

h(z1  z2  z3  z4) =

1
4!

k(xπ(1)  xπ(2))[l(yπ(1)  yπ(2)) + l(yπ(3)  yπ(4)) − 2l(yπ(2)  yπ(3))] 

(cid:88)

π∈S4

where we denote by Sn the group of permutations over n elements. Thus  we can directly apply
the theory developed for higher-order V -statistics in Section 3. We consider two types of tests:
instantaneous independence and independence at multiple time lags.

5

Table 1: Rejection rates for two-sample experiments. MCMC: sample size=500; a Gaussian kernel
with bandwidth σ = 1.7 is used; every second Gibbs sample is kept (i.e.  after a pass through both
dimensions). Audio: sample sizes are (nx  ny) = {(300  200)  (600  400)  (900  600)}; a Gaussian
kernel with bandwidth σ = 14 is used. Both: wild bootstrap uses blocksize of ln = 20; averaged
over at least 200 trials. The Type II error for all tests was zero

MCMC

Audio

experiment \ method
i.i.d. vs i.i.d. (H0)
i.i.d. vs Gibbs (H0)
Gibbs vs Gibbs (H0)

H0
H1

permutation (cid:92)MMDk b
.025
.100
.110

{.970 .965 .995}

.040
.528
.680
{1 1 1}

{.145 .120 .114}
{.600 .898 .995}

Vb1
.012
.052
.060

Vb2
.070
.105
.100

Test of instantaneous independence Here  the null hypothesis H0 is that Xt and Yt are indepen-
dent at all times t  and the alternative hypothesis H1 is that they are dependent.
Proposition 2. Under the null hypothesis  if the stationary process Zt = (Xt  Yt) is τ-dependent

with a coefﬁcient τ (r) = O(cid:0)r−6−(cid:1) for some  > 0  then ϕ(6nVb(h)  nV (h)) → 0 in probability 

where ϕ is the Prokhorov metric.

Proof. Since k and l are bounded and Lipschitz continuous  the core h is bounded and Lipschitz
continuous. One-degeneracy under the null hypothesis was stated in [17  Theorem 2]  and that h2 is
a kernel is shown in [17  section A.2  following eq. (11)]. The result follows from Theorem 1.

The following proposition holds by the Theorem 2  since the core h is Lipschitz continuous  sym-
metric and bounded.

Proposition 3. If the stationary process Zt is τ-dependent with a coefﬁcient τ (r) = O(cid:0)r−6−(cid:1)

for some  > 0  then under the alternative hypothesis nVb2(h) converges in distribution to some
random variable with a ﬁnite variance and Vb1 converges to zero in probability.

Lag-HSIC Propositions 2 and 3 also allow us to construct a test of time series independence that
is similar to one designed by [4]. Here  we will be testing against a broader null hypothesis: Xt and
Yt(cid:48) are independent for |t − t(cid:48)| < M for an arbitrary large but ﬁxed M. In the Appendix  we show
how to construct a test when M → ∞  although this requires an additional assumption about the
uniform convergence of cumulative distribution functions.
Since the time series Zt = (Xt  Yt) is stationary  it sufﬁces to check whether there exists a de-
pendency between Xt and Yt+m for −M ≤ m ≤ M. Since each lag corresponds to an indi-
vidual hypothesis  we will require a Bonferroni correction to attain a desired test level α. We
therefore deﬁne q = 1 − α
t = (Xt  Yt+m).
Let Sm n = nV (h  Z m) denote the value of the normalized HSIC statistic calculated on the
shifted process Z m
t . Let Fb n denote the empirical cumulative distribution function obtained by
the bootstrap procedure using nVb(h  Z). The test will then reject the null hypothesis if the event
An =
occurs. By a simple application of the union bound 
it is clear that the asymptotic probability of the Type I error will be limn→∞ P H0 (An) ≤ α.
On the other hand  if the alternative holds  there exists some m with |m| ≤ M for which
V (h  Z m) = n−1Sm n converges to a non-zero constant. In this case

2M +1. The shifted time series will be denoted Z m

max−M≤m≤M Sm n > F −1

b n (q)

(cid:110)

(cid:111)

P H1 (An) ≥ P H1 (Sm n > F −1

b n (q)) = P H1(n−1Sm n > n−1F −1

(5)
b n (q) → 0  which follows from the convergence of Vb to zero in probability shown
as long as n−1F −1
in Proposition 3. Therefore  the Type II error of the multiple lag test is guaranteed to converge to
zero as the sample size increases. Our experiments in the next Section demonstrate that while this
procedure is deﬁned over a ﬁnite range of lags  it results in tests more powerful than the procedure
for an inﬁnite number of lags proposed in [4]. We note that a procedure that works for an inﬁnite
number of lags  although possible to construct  does not add much practical value under the present
assumptions. Indeed  since the τ-mixing assumption applies to the joint sequence Zt = (Xt  Yt) 

b n (q)) → 1

6

Figure 1: Comparison of Shift-HSIC and tests based on Vb1 and Vb2. The left panel shows the
performance under the null hypothesis  where a larger AR coefﬁcient implies a stronger temporal
dependence. The right panel show the performance under the alternative hypothesis  where a larger
extinction rate implies a greater dependence between processes.

Figure 2: In both panel Type II error is plotted. The left panel presents the error of the lag-HSIC
and KCSD algorithms for a process following dynamics given by the equation (6). The errors for a
process with dynamics given by equations (7) and (8) are shown in the right panel. The X axis is
indexed by the time series length  i.e.  sample size. The Type I error was around 5%.

dependence between Xt and Yt+m is bound to disappear at a rate of o(m−6)  i.e.  the variables both
within and across the two series are assumed to become gradually independent at large lags.

5 Experiments
The MCMC M.D. We employ MMD in order to diagnose how far an MCMC chain is from its
stationary distribution [26  Section 5]  by comparing the MCMC sample to a benchmark sample.
A hypothesis test of whether the sampler has converged based on the standard permutation-based
bootstrap leads to too many rejections of the null hypothesis  due to dependence within the chain.
Thus  one would require heavily thinned chains  which is wasteful of samples and computationally
burdensome. Our experiments indicate that the wild bootstrap approach allows consistent tests di-
rectly on the chains  as it attains a desired number of false positives.
To assess performance of the wild bootstrap in determining MCMC convergence  we consider the
situation where samples {Xi} and {Yi} are bivariate  and both have the identical marginal distri-
bution given by an elongated normal P = N
. However  they could
have arisen either as independent samples  or as outputs of the Gibbs sampler with stationary distri-
bution P . Table 1 shows the rejection rates under the signiﬁcance level α = 0.05. It is clear that in
the case where at least one of the samples is a Gibbs chain  the permutation-based test has a Type I
error much larger than α. The wild bootstrap using Vb1 (without artiﬁcial degeneration) yields the
correct Type I error control in these cases. Consistent with ﬁndings in [22  Section 5]  Vb1 mimics
the null distribution better than Vb2. The bootstrapped statistic (cid:92)MMDk b in (4) which also relies on
the artiﬁcially degenerated bootstrap processes  behaves similarly to Vb2. In the alternative scenario
where {Yi} was taken from a distribution with the same covariance structure but with the mean set
to µ = [ 2.5 0 ]  the Type II error for all tests was zero.
Pitch-evoking sounds Our second experiment is a two sample test on sounds studied in the ﬁeld
of pitch perception [20]. We synthesise the sounds with the fundamental frequency parameter of
treble C  subsampled at 10.46kHz. Each i-th period of length Ω contains d = 20 audio samples

(cid:20) 15.5

(cid:21)(cid:19)

[ 0

0 ]  

14.5
15.5

14.5

(cid:18)

7

0.20.40.60.8−0.0500.050.10.150.2type I errorAR coeffcient0.20.40.60.8100.20.40.60.81type II errorExtinction rate Vb1Vb2Shift10015020025030000.20.40.60.81type II error ratesample size20025030000.20.40.60.81sample size KCSDHSICj

2σ2

√

(cid:17)

(cid:80)d

s=1 aj s exp

(cid:16)− (tr−ts−(j−i)Ω)2

1 − λ2i  where a0  i ∼ N (0  Id)  with Xi r =(cid:80)

at times 0 = t1 < . . . < td < Ω – we treat this whole vector as a single observation Xi or Yi 
i.e.  we are comparing distributions on R20. Sounds are generated based on the AR process ai =
.
λai−1 +
Thus  a given pattern – a smoothed version of a0 – slowly varies  and hence the sound deviates from
periodicity  but still evokes a pitch. We take X with σ = 0.1Ω and λ = 0.8  and Y is either an
independent copy of X (null scenario)  or has σ = 0.05Ω (alternative scenario) (Variation in the
smoothness parameter changes the width of the spectral envelope  i.e.  the brightness of the sound).
nx is taken to be different from ny. Results in Table 1 demonstrate that the approach using the wild
bootstrapped statistic in (4) allows control of the Type I error and reduction of the Type II error with
increasing sample size  while the permutation test virtually always rejects the null hypothesis. As
in [22] and the MCMC example  the artiﬁcial degeneration of the wild bootstrap process causes the
Type I error to remain above the design parameter of 0.05  although it can be observed to drop with
increasing sample size.
Instantaneous independence To examine instantaneous independence test performance  we com-
pare it with the Shift-HSIC procedure [8] on the ’Extinct Gaussian’ autoregressive process proposed
in the [8  Section 4.1]. Using exactly the same setting we compute type I error as a function of the
temporal dependence and type II error as a function of extinction rate. Figure 1 shows that all three
tests (Shift-HSIC and tests based on Vb1 and Vb2) perform similarly.
Lag-HSIC The KCSD [4] is  to our knowledge  the only test procedure to reject the null hypoth-
esis if there exist t t(cid:48) such that Zt and Zt(cid:48) are dependent. In the experiments  we compare lag-HSIC
with KCSD on two kinds of processes: one inspired by econometrics and one from [4].
In lag-HSIC  the number of lags under examination was equal to max{10  log n}  where n is the
sample size. We used Gaussian kernels with widths estimated by the median heuristic. The cumu-
lative distribution of the V -statistics was approximated by samples from nVb2. To model the tail of
this distribution  we have ﬁtted the generalized Pareto distribution to the bootstrapped samples ([23]
shows that for a large class of underlying distribution functions such an approximation is valid).
The ﬁrst process is a pair of two time series which share a common variance 

t   σ2

t−1 + Y 2

Yt = 2 tσ2

t = 1 + 0.45(X 2

i ∈ {1  2}. (6)
Xt = 1 tσ2
t  
The above set of equations is an instance of the VEC dynamics [2] used in econometrics to model
market volatility. The left panel of the Figure 2 presents the Type II error rate: for KCSD it remains
at 90% while for lag-HSIC it gradually drops to zero. The Type I error  which we calculated by
sampling two independent copies (X (1)
) of the process and performing the
tests on the pair (X (1)
Our next experiment is a process sampled according to the dynamics proposed by [4] 

)  was around 5% for both of the tests.

i.i.d.∼ N (0  1) 

) and (X (2)

  Y (2)

  Y (1)

  Y (2)

t−1) 

t

t

i t

t

t

t

t

1 t

Xt = cos(φt 1) 

φt 1 = φt−1 1 + 0.11 t + 2πf1Ts 

Yt = [2 + C sin(φt 1)] cos(φt 2)  φt 2 = φt−1 2 + 0.12 t + 2πf2Ts 

i.i.d.∼ N (0  1)  (7)
i.i.d.∼ N (0  1)  (8)
with parameters C = .4  f1 = 4Hz f2 = 20Hz  and frequency 1
= 100Hz. We compared
Ts
performance of the KCSD algorithm  with parameters set to vales recommended in [4]  and the
lag-HSIC algorithm. The Type II error of lag-HSIC  presented in the right panel of the Figure 2 
is substantially lower than that of KCSD. The Type I error (C = 0) is equal or lower than 5% for
both procedures. Most oddly  KCSD error seems to converge to zero in steps. This may be due
to the method relying on a spectral decomposition of the signals across a ﬁxed set of bands. As
the number of samples increases  the quality of the spectrogram will improve  and dependence will
become apparent in bands where it was undetectable at shorter signal lengths.

2 t

References
[1] M.A. Arcones. The law of large numbers for U-statistics under absolute regularity. Electron. Comm.

Probab  3:13–19  1998.

[2] L. Bauwens  S. Laurent  and J.V.K. Rombouts. Multivariate GARCH models: a survey. J. Appl. Econ. 

21(1):79–109  January 2006.

[3] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.

Kluwer  2004.

8

[4] M. Besserve  N.K. Logothetis  and B. Schlkopf. Statistical analysis of coupled time series with kernel

cross-spectral density operators. In NIPS  pages 2535–2543. 2013.

[5] I.S. Borisov and N.V. Volodko. Orthogonal series and limit theorems for canonical U- and V-statistics of

stationary connected observations. Siberian Adv. Math.  18(4):242–257  2008.

[6] S. Borovkova  R. Burton  and H. Dehling. Limit theorems for functionals of mixing processes with
applications to U-statistics and dimension estimation. Trans. Amer. Math. Soc.  353(11):4261–4318  2001.
[7] R. Bradley et al. Basic properties of strong mixing conditions. a survey and some open questions. Prob-

ability surveys  2(107-44):37  2005.

[8] K. Chwialkowski and A. Gretton. A kernel independence test for random processes. In ICML  2014.
[9] Kacper Chwialkowski  Dino Sejdinovic  and Arthur Gretton. A wild bootstrap for degenerate kernel tests.

tech. report. arXiv preprint arXiv:1408.5404  2014.

[10] J. Dedecker  P. Doukhan  G. Lang  S. Louhichi  and C. Prieur. Weak dependence: with examples and

applications  volume 190. Springer  2007.

[11] J´erˆome Dedecker and Cl´ementine Prieur. New dependence coefﬁcients. examples and applications to

statistics. Probability Theory and Related Fields  132(2):203–236  2005.

[12] P. Doukhan. Mixing. Springer  1994.
[13] R.M. Dudley. Real analysis and probability  volume 74. Cambridge University Press  2002.
[14] K. Fukumizu  A. Gretton  X. Sun  and B. Sch¨olkopf. Kernel measures of conditional dependence. In

NIPS  volume 20  pages 489–496  2007.

[15] A. Gretton  K.M. Borgwardt  M.J. Rasch  B. Sch¨olkopf  and A. Smola. A kernel two-sample test. J.

Mach. Learn. Res.  13:723–773  2012.

[16] A. Gretton  O. Bousquet  A. Smola  and B. Sch¨olkopf. Measuring statistical dependence with Hilbert-

Schmidt norms. In Algorithmic learning theory  pages 63–77. Springer  2005.

[17] A. Gretton  K. Fukumizu  C Teo  L. Song  B. Sch¨olkopf  and A. Smola. A kernel statistical test of

independence. In NIPS  volume 20  pages 585–592  2007.

[18] Arthur Gretton. A simpler condition for consistency of a kernel independence test. arXiv:1501.06103 

2015.

[19] Z. Harchaoui  F. Bach  and E. Moulines. Testing for homogeneity with kernel Fisher discriminant analysis.

In NIPS. 2008.

[20] P. Hehrmann. Pitch Perception as Probabilistic Inference. PhD thesis  Gatsby Computational Neuro-

science Unit  University College London  2011.

[21] A. Leucht. Degenerate U- and V-statistics under weak dependence: Asymptotic theory and bootstrap

consistency. Bernoulli  18(2):552–585  2012.

[22] A. Leucht and M.H. Neumann. Dependent wild bootstrap for degenerate U- and V-statistics. Journal of

Multivariate Analysis  117:257–280  2013.

[23] J. Pickands III. Statistical inference using extreme order statistics. Ann. Statist.  pages 119–131  1975.
[24] D. Sejdinovic  A. Gretton  and W. Bergsma. A kernel test for three-variable interactions. In NIPS  pages

1124–1132  2013.

[25] D. Sejdinovic  B. Sriperumbudur  A. Gretton  and K. Fukumizu. Equivalence of distance-based and

RKHS-based statistics in hypothesis testing. Ann. Statist.  41(5):2263–2702  2013.

[26] D. Sejdinovic  H. Strathmann  M. Lomeli Garcia  C. Andrieu  and A. Gretton. Kernel Adaptive

Metropolis-Hastings. In ICML  2014.

[27] R. Serﬂing. Approximation Theorems of Mathematical Statistics. Wiley  New York  1980.
[28] X. Shao. The dependent wild bootstrap. J. Amer. Statist. Assoc.  105(489):218–235  2010.
[29] A. J Smola  A. Gretton  L. Song  and B. Sch¨olkopf. A Hilbert space embedding for distributions. In Al-
gorithmic Learning Theory  volume LNAI4754  pages 13–31  Berlin/Heidelberg  2007. Springer-Verlag.
[30] B. Sriperumbudur  A. Gretton  K. Fukumizu  G. Lanckriet  and B. Sch¨olkopf. Hilbert space embeddings

and metrics on probability measures. J. Mach. Learn. Res.  11:1517–1561  2010.

[31] M. Sugiyama  T. Suzuki  Y. Itoh  T. Kanamori  and M. Kimura. Least-squares two-sample test. Neural

Networks  24(7):735–751  2011.

[32] K. Zhang  J. Peters  D. Janzing  B.  and B. Sch¨olkopf. Kernel-based conditional independence test and

application in causal discovery. In UAI  pages 804–813  2011.

[33] X. Zhang  L. Song  A. Gretton  and A. Smola. Kernel measures of independence for non-iid data. In

NIPS  volume 22  2008.

9

,Kacper Chwialkowski
Dino Sejdinovic
Arthur Gretton
Ankit Shah
Pritish Kamath
Julie Shah
Shen Li