2007,Computational Equivalence of Fixed Points and No Regret Algorithms  and Convergence to Equilibria,We study the relation between notions of game-theoretic equilibria which are based on stability under a set of deviations  and empirical equilibria which are reached by rational players. Rational players are modelled by players using no regret algorithms  which guarantee that their payoff in the long run is almost as much as the most they could hope to achieve by consistently deviating from the algorithm's suggested action. We show that for a given set of deviations over the strategy set of a player  it is possible to efficiently approximate fixed points of a given deviation if and only if there exist efficient no regret algorithms resistant to the deviations. Further  we show that if all players use a no regret algorithm  then the empirical distribution of their plays converges to an equilibrium.,Computational Equivalence of Fixed Points and No
Regret Algorithms  and Convergence to Equilibria

IBM Almaden Research Center

Computer Science Department 

Elad Hazan

650 Harry Road

San Jose  CA 95120

hazan@us.ibm.com

Satyen Kale

Princeton University

35 Olden St.

Princeton  NJ 08540

satyen@cs.princeton.edu

Abstract

We study the relation between notions of game-theoretic equilibria which are
based on stability under a set of deviations  and empirical equilibria which are
reached by rational players. Rational players are modeled by players using no
regret algorithms  which guarantee that their payoff in the long run is close to
the maximum they could hope to achieve by consistently deviating from the algo-
rithm’s suggested action.
We show that for a given set of deviations over the strategy set of a player  it is
possible to efﬁciently approximate ﬁxed points of a given deviation if and only if
there exist efﬁcient no regret algorithms resistant to the deviations. Further  we
show that if all players use a no regret algorithm  then the empirical distribution
of their plays converges to an equilibrium.

1 Introduction

We consider a setting where a number of agents need to repeatedly make decisions in the face of
uncertainty. In each round  the agent obtains a payoff based on the decision she chose. Each agent
would like to be able to maximize her payoff. While this might seem like a natural objective  it
may be impossible to achieve without placing restrictions on the kind of payoffs that can arise. For
instance  if the payoffs were adversarially chosen  then the agent’s task would become essentially
hopeless.
In such a situation  one way for the agent to cope with the uncertainty is to aim for a relative
benchmark rather an absolute one. The notion of regret minimization captures this intuition. We
imagine that the agent has a choice of several well-deﬁned ways to change her decision  and now
the agent aims to maximize her payoff relative to what she could have obtained had she changed her
decisions in a consistent manner. As an example of what we mean by consistent changes  a possible
objective could be to maximize her payoff relative to the most she could have achieved by choosing
some ﬁxed decision in all the rounds. The difference between these payoffs is known as external
regret in the game theory literature. Another notion is that of internal regret  which arises when the
possible ways to change are the ones that switch from some decision i to another  j  whenever the
agent chose decision i  leaving all other decisions unchanged.
A learning algorithm for an agent is said to have no regret with respect to an associated set of decision
modiﬁers (also called deviations) Φ if the average payoff of an agent using the algorithm converges
to the largest average payoff she would have achieved had she changed her decisions using a ﬁxed
decision modiﬁer in all the rounds. Based on what set of decision modiﬁers are under consideration 
various no regret algorithms are known (for e.g. Hannan [10] gave algorithms to minimize external
regret  and Hart and Mas-Collel [11] give algorithms to minimize internal regret).

1

The reason no regret algorithms are so appealing  apart from the fact that they model rational behav-
ior of agents in the face of uncertainty  is that in various cases it can be shown that using no regret
algorithms guides the overall play towards a game theoretic equilibrium. For example  Freund and
Schapire [7] show that in a zero-sum game  if all agents use a no external regret algorithm  then
the empirical distribution of the play converges to the set of minimax equilibria. Similarly  Hart
and Mas-Collel [11] show that if all agents use a no internal regret algorithm  then the empirical
distribution of the play converges to the set of correlated equilibria.
In general  given a set of decision modiﬁers Φ  we can deﬁne a notion of game theoretic equilibrium
that is based on the property of being stable under deviations speciﬁed by Φ. This is a joint distri-
bution on the agents’ decisions that ensures that the expected payoff to any agent is no less than the
most she could achieve if she decided to unilaterally (and consistently) decided to deviate from her
suggested action using any decision modiﬁer in Φ. One can then show that if all agents use a Φ-no
regret algorithm  then the empirical distribution of the play converges to the set of Φ-equilibria.
This brings us to the question of whether it is possible to design no regret algorithms for various sets
of decision modiﬁers Φ. In this paper  we design algorithms which achieve no regret with respect to
Φ for a very general setting of arbitrary convex compact decision spaces  arbitrary concave payoff
functions  and arbitrary continuous decision modiﬁers. Our method works as long as it is possible
to compute approximate ﬁxed points for (convex combinations) of decision modiﬁers in Φ. Our
algorithms are based on a connection to the framework of Online Convex Optimization (see  e.g.
[18]) and we show how to apply known learning algorithms to obtain Φ-no regret algorithms. The
generality of our connection allows us to use various sophisticated Online Convex Optimization
algorithms which can exploit various structural properties of the utility functions and guarantee a
faster rate of convergence to the equilibrium.
Previous work by Greenwald and Jafari [9] gave algorithms for the case when the decision space is
the simplex of probability distributions over the agents’ decisions  the payoff functions are linear 
and the decision modiﬁers are also linear. Their algorithm  based on the work of Hart and Mas-
Collel [11]  uses a version of Blackwell’s Approachability Theorem  and also needs to computes
ﬁxed points of the decision modiﬁers. Since these modiﬁers are linear  it is possible to compute
ﬁxed points for them by computing the stationary distribution of an appropriate stochastic matrix
(say  by computing its top eigenvector).
Computing Brouwer ﬁxed points of continuous functions is in general a very hard problem (it is
PPAD-complete  as shown by Papadimitriou [15]). Fixed points are ubiquitous in game theory.
Most common notions of equilibria in game theory are deﬁned as the set of ﬁxed points of a certain
mapping. For example  Nash Equilibria (NE) are the set of ﬁxed points of the best response mapping
(appropriately deﬁned to avoid ambiguity). The fact that Brouwer ﬁxed points are hard to compute in
general is no reason why computing speciﬁc ﬁxed points should be hard (for instance  as mentioned
earlier  computing ﬁxed points of linear functions is easy via eigenvector computations). More
speciﬁcally  could it be the case that the NE  being a ﬁxed point of some well-speciﬁed mapping 
is easy to compute? These hopes were dashed by the work of [6  3] who showed that computing
NE is as computationally difﬁcult as ﬁnding ﬁxed points in a general mapping:
they show that
computing NE in a two-player game is PPAD-complete. Further work showed that even computing
an approximate NE is PPAD-complete [4].
Since our algorithms (and all previous ones as well) depend on computing (approximate) ﬁxed points
of various decision modiﬁers  the above discussion leads us to question whether this is necessary.
We show in this paper that indeed it is: a Φ-no-regret algorithm can be efﬁciently used to compute
approximate ﬁxed points of any convex combination of decision modiﬁers. This establishes an
equivalence theorem  which is the main contribution of this paper: there exist efﬁcient Φ-no-regret
algorithms if and only it is possible to efﬁciently compute ﬁxed points of convex combinations
of decision modiﬁers in Φ. This equivalence theorem allows us to translate complexity theoretic
lower bounds on computing ﬁxed points to designing no regret algorithms. For instance  a Nash
equilibrium can be obtained by applying Brouwer’s ﬁxed point theorem to an appropriately deﬁned
continuous mapping from the compact convex set of pairs of the players’ mixed strategies to itself.
Thus  if Φ contains this mapping  then it is PPAD-hard to design Φ-no-regret algorithms.
It was recently brought to our attention that Stolz and Lugosi [17]  building on the work of Hart and
Schmeidler [12]  have also considered Φ-no-regret algorithms. They also show how to design them

2

from ﬁxed-point oracles  and proved convergence to equilibria under even more general conditions
than we consider. Gordon  Greenwald  Marks  and Zinkevich [8] have also considered similar no-
tions of regret and showed convergence to equilibria  in the special case when the deviations in Φ
can be represented as the composition of a ﬁxed embedding into a higher dimensional space and
an adjustable linear transformation. The focus of our results is on the computational aspect of such
reductions  and the equivalence of ﬁxed-points computation and no-regret algorithms.
2 Preliminaries
2.1 Games and Equilibria

We consider the following kinds of games. First  the set of strategies for the players of the game is
a convex compact set. Second  the utility functions for the players are concave over their strategy
sets. To avoid cumbersome notation  we restrict ourselves to two player games  although all of our
results naturally extend to multi-player games.
Formally  for i = 1  2  player i plays points from a convex compact set Ki ⊆ Rni. Her payoff
is given by function ui : K1 × K2 → R  i.e. if x1  x2 is the pair of strategies played by the two
players  then the payoff to player i is given by ui(x1  x2). We assume that u1 is a concave function
of x1 for any ﬁxed x2  and similarly u2 is a concave function of x2 for any ﬁxed x1.
We now deﬁne a notion of game theoretic equilibrium based on the property of being stable with
respect to consistent deviations. By this  we mean an online game-playing strategy for the players
that will guarantee that neither stands to gain if they decided to unilaterally  and consistently  deviate
from their suggested moves.
To model this  assume that each player i has a set of possible deviations Φi which is a ﬁnite1 set of
continuous mappings φi : Ki → Ki. Let Φ = (Φ1  Φ2). Let Ψ be a joint distribution on K1 × K2.
If it is the case that for any deviation φ1 ∈ Φ1  player 1’s expected payoff obtained by sampling x1
using Ψ is always larger than her expected payoff obtained by deviating to φ1(x1)  then we call Ψ
stable under deviations in Φ1. The distribution Ψ is said to be a Φ-equilibrium if Ψ is stable under
deviations in Φ1 and Φ2. A similar deﬁnition appears in [12] and [17].
Deﬁnition 1 (Φ-equilibrium). A joint distribution Ψ over K1 × K2 is called a Φ-equilibrium if the
following holds  for any φ1 ∈ Φ1  and for any φ2 ∈ Φ2:

Z
Z

u1(x1  x2)Ψ(x1  x2) ≥
u2(x1  x2)Ψ(x1  x2) ≥

u1(φ1(x1)  x2)Ψ(x1  x2)

u2(x1  φ2(x2))Ψ(x1  x2)

Z
Z

0

We say that Ψ is a ε-approximate Φ-equilibrium if the inequalities above are satisﬁed up to an
additive error of ε.

Intuitively  we imagine a repeated game between the two players  where at equilibrium  the players’
moves are correlated by a signal  which could be the past history of the play  and various external
factors. This signal samples a pair of moves from an equilibrium joint distribution over all pairs
of moves  and suggests to each player individually only the move she is supposed to play. If no
player stands to gain if she unilaterally  but consistently  used a deviation from her suggested move 
then the distribution of the correlating signal is stable under the set of deviations  and is hence an
equilibrium.
Example 1: Correlated Equilibria. A standard 2-player game is obtained when the Ki are the
simplices of distributions over some base sets of actions Ai and the utility functions ui are bilinear
in x1  x2. If the sets Φi consist of the maps φa b : Ki → Ki for every pair a  b ∈ Ai deﬁned as

φa b(x)[c] =

xa + xb
xc

if c = a
if c = b
otherwise

(1)

1It is highly plausible that the results in this paper extend to the case where Φ is inﬁnite – indeed  our results
hold for any set of mappings Φ which is obtained by taking all convex combinations of ﬁnitely many mappings
– but we restrict to ﬁnite Φ in this paper for simplicity.

3

then it can be shown that any Φ-equilibrium can be equivalently viewed as a correlated equilibrium
of the game  and vice-versa.

Example 2: The Stock Market game. Consider the following setting: there are two investors
(the generalization to many investors is straightforward)  who invest their wealth in n stocks. In
each period  they choose portfolios x1 and x2 over the n stocks  and observe the stock returns. We
model the stock returns as a function r of the portfolios x1  x2 chosen by the investors  and it maps
the portfolios to the vector of stock returns. We make the assumption that each player has a small
inﬂuence on the market  and thus the function r is insensitive to the small perturbations in the input.
The wealth gain for each investor i is r(x1  x2) · xi. The standard way to measure performance of
an investment strategy is the logarithmic growth rate  viz. log(r(x1  x2) · xi). We can now deﬁne
the utility functions as ui(x1  x2) = log(r(x1  x2) · xi). Intuitively  this game models the setting in
which the market prices are affected by the investments of the players.
A natural goal for a good investment strategy would be to compare the wealth gain to that of the
best ﬁxed portfolio  i.e. Φi is the set of all constant maps. This was considered by Cover in his
Universal Portfolio Framework [5]. Another possible goal would be to compare the wealth gained
to that achievable by modifying the portfolios using the φa b maps above  as considered by [16]. In
Section 3  we show that the stock market game admits algorithms that converge to an ε-equilibrium
in O( 1

ε ) rounds  whereas all previous algorithms need O( 1

ε2 ) rounds.

ε log 1

2.2 No regret algorithms

The online learning framework we consider is called online convex optimization [18]  in which there
is a ﬁxed convex compact feasible set K ⊂ Rn and an arbitrary  unknown sequence of concave
payoff functions f (1)  f (2)  . . . : K → R. The decision maker must make a sequence of decisions 
where the tth decision is a selection of a point x(t) ∈ K and obtains a payoff of f (t)(x(t)) on period
t. The decision maker can only use the previous points x(1)  . . .   x(t−1)  and the previous payoff
functions f (1)  . . .   f (t−1) to choose the point x(t).
The performance measure we use to evaluate online algorithms is regret  deﬁned as follows. The
decision maker has a ﬁnite set of N decision modiﬁers Φ which  as before  is a set of continuous
mappings from K → K. Then the regret for not using some deviation φ ∈ Φ is the excess payoff
the decision maker could have obtained if she had changed her points in each round by applying φ.
Deﬁnition 2 (Φ-Regret). Let Φ be a set of continuous functions from K → K. Given a set of T
concave utility functions f1  ...  fT   deﬁne the Φ-regret as

RegretΦ(T ) = max
φ∈Φ

f (t)(x(t)).

TX

f (t)(φ(x(t))) − TX

t=1

t=1

Two speciﬁc examples of Φ-regret deserve mention. The ﬁrst one is “external regret”  which is
deﬁned when Φ is the set of all constant mappings from K to itself. The second one is “internal
regret”  which is deﬁned when K is the simplex of distributions over some base set of actions A 
and Φ is the set of the φa b functions (deﬁned in (1)) for all pairs a  b ∈ A.
A desirable property of an algorithm for Online Convex Optimization is Hannan consistency: the
regret  as a function of the number of rounds T   is sublinear. This implies that the average per
iteration payoff of the algorithm converges to the average payoff of a clairvoyant algorithm that uses
the best deviation in hindsight to change the point in every round. For the purpose of this paper  we
require a slightly stronger property for an algorithm  viz. that the regret is polynomially sublinear as
a function of T .
Deﬁnition 3 (No Φ-regret algorithm). A no Φ-regret algorithm is one which  given any sequence of
concave payoff functions f (1)  f (2)  . . .  generates a sequence of points x(1)  x(2)  . . . ∈ K such that
for all T = 1  2  . . .  RegretΦ(T ) = O(T 1−c) for some constant c > 0. Such an algorithm will be
called efﬁcient if it computes x(t) in poly(n  N  t  L) time.

In the above deﬁnition  L is a description length parameter for K  deﬁned appropriately depending
on how the set K is represented. For instance  if K is the n-dimensional probability simplex  then

4

L = n. If K is speciﬁed by means of a separation oracle and inner and outer radii r and R  then
L = log(R/r)  and we allow poly(n  N  t  L) calls to the separation oracle in each iteration.
The relatively new framework of Online Convex Optimization (OCO) has received much attention
recently in the machine learning community. Our no Φ-regret algorithms can use any of wide variety
of algorithms for OCO. In this paper  we will use Exponentiated Gradient (EG) algorithm ([14]  [1]) 
which has the following (external) regret bound:
Theorem 1. Let the domain K be the simplex of distributions over a base set of size n. Let
G∞ be an upper bound on the L∞ norm of the gradients of the payoff functions  i.e. G∞ ≥
supx∈K k∇f (t)(x)k∞. Then the EG algorithm generates points x(1)  . . .   x(T ) such that

TX

f (t)(x) − TX

f (t)(x(t)) ≤ O(G∞plog(n)T )

max
x∈K

t=1

t=1

√

If the utility functions are strictly concave rather than linear  even stronger regret bounds  which
depend on log(T ) rather than
While most of the literature on online convex optimization focuses on external regret  it was ob-
served that any Online Convex Optimization algorithm for external regret can be converted to an
internal regret algorithm (for example  see [2]  [16]).

T   are known [13].

2.3 Fixed Points

As mentioned in the introduction  our no regret algorithms depend on computing ﬁxed points of
the relevant mappings. For a given set of deviations Φ  denote by CH(Φ) the set of all convex
combinations of deviations in Φ  i.e.

nP
φ∈Φαφφ : αφ ≥ 0 andP

CH(Φ) =

o

φ∈Φαφ = 1

.

Since each map φ ∈ CH(Φ) is a continuous function from K → K  and K is a convex compact
there exists a point x ∈ K
domain  by Brouwer’s ﬁxed theorem  φ has a ﬁxed point in K  i.e.
such that φ(x) = x. We consider algorithms which approximate ﬁxed points for a given map in the
following sense.
Deﬁnition 4 (FPTAS for ﬁxed points of deviations). Let Φ be a set of N continuous functions
from K → K. A fully polynomial time approximation scheme (FPTAS) for ﬁxed points of Φ is an
algorithm  which  given any function φ ∈ CH(Φ) and an error parameter ε > 0  computes a point
x ∈ K such that kφ(x) − xk ≤ ε in poly(n  N  L  1

ε ) time.

3 Convergence of no Φ-regret algorithms to Φ-equilibria

In this section we prove that if the players use no Φ-regret algorithms  then the empirical distribu-
tion of the moves converges to a Φ-equilibrium. [11] shows that if players use no internal regret
algorithms  then the empirical distribution of the moves converges to a correlated equilibrium. This
was generalized by [9] to any set of linear transformations Φ. The more general setting of this paper
also follows easily from the deﬁnitions. A similar theorem was also proved in [17].
The advantage of this general setting is that the connection to online convex optimization allows for
faster rates of convergence using recent online learning techniques. We give an example of a natural
game theoretic setting with faster convergence rate below.
Theorem 2. If each player i chooses moves using a no Φi-regret algorithms  then the empirical
game distribution of the players’ moves converges to a Φ-equilibrium. Further  an ε-approximate
Φ-equilibrium is reached after T iterations for the ﬁrst T which satisﬁes 1

T RegretΦ(T ) ≤ ε.

Proof. Consider the ﬁrst player. In each game iteration t  let (x1
(t)) be the pair of moves
played by the two players. From player 1’s point of view  the payoff function she obtains  f (t)  is
the following:

(t)  x2

∀x ∈ K1 :

f (t)(x)   u1(x  x2

(t)).

5

Note that this function is concave by assumption. Then we have  by deﬁnition 3 

RegretΦ1(T ) = max
φ∈Φ

f (t)(φ(x1

t

t

f (t)(x1

(t)).

(t))) −X

X

TX

t=1

Z

TX

t=1

1
T

Z

Rewriting this in terms of the original utility function  and scaling by the number of iterations we
get

u1(x1

(t)  x2

(t)) ≥ 1
T

u1(φ(x1

(t))  x2

(t)) − 1
T

RegretΦ1(T ).

Denote by Ψ(T ) the empirical distribution of the played strategies till iteration T   i.e. the distribution
(t)) for t = 1  2  . . .   T . Then  the above
which puts a probability mass of 1
inequality can be rewritten as

T on all pairs (x1

(t)  x2

u1(x1  x2)Ψ(T )(x1  x2) ≥

u1(φ(x1)  x2)Ψ(T )(x1  x2) − 1
T

RegretΦ1(T ).

A similar inequality holds for player 2 as well. Now assume that both players use no regret algo-
(T ) ≤ O(T 1−c) for some constant c > 0. Hence as T → ∞  we
rithms  which ensure that RegretΦi
(T ) → 0. Thus Ψ(T ) converges to a Φ-equilibrium. Also  Ψ(T ) is a ε-approximate
T RegretΦi
have 1
T RegretΦ2(T ) are less than ε 
equilibrium as soon as T is large enough so that 1
i.e. T ≥ Ω( 1

T RegretΦ1(T ) and 1

ε1/c ).

A corollary of Theorem 2 is that we can obtain faster rates of convergence using recent online
learning techniques  when the payoff functions are non-linear. This is natural in many situations 
since risk aversion is associated with the concavity of utility functions.
Corollary 3. For the stock market game as deﬁned in section 2.1  there exists no regret algorithms
which guarantee convergence to an ε-equilibrium in O( 1

ε log 1

ε ) iterations.

Proof sketch. The utility functions observed by the investor i in the stock market game are of the
form ui(x1  x2) = log(r(x1  x2) · xi). This logarithmic utility function is exp-concave  by the
assumption on the insensitivity of the function r to small perturbations in the input. Thus the online
algorithm of [5]  or the more efﬁcient algorithms of [13] can be applied. In the full version of this
(T ) = O(log T ).
paper  we show that Lemma 6 can be modiﬁed to obtain algorithms with RegretΦi
By the Theorem 2 above  the investors reach ε-equilibrium in O( 1
ε ) iterations.

ε log 1

4 Computational Equivalence of Fixed Points and No Regret algorithms

In this section we prove our main result on the computational equivalence of computing ﬁxed points
and designing no regret algorithms. By the result of the previous section  players using no regret
algorithms converge to equilibria.
We assume that the payoff functions f (t) are scaled so that the (L2) norm of their gradients is
bounded by 1  i.e. k∇f (t)k ≤ 1. Our main theorem is the following:
Theorem 4. Let Φ be a given ﬁnite set of deviations. Then there is a FPTAS for ﬁxed points of Φ if
and only if there exists an efﬁcient no Φ-regret algorithm.

The ﬁrst direction of the theorem is proved by designing utility functions for which the no regret
property will imply convergence to an approximate ﬁxed point of the corresponding transformations.
The proof crucially depends on the fact that no regret algorithms have the stringent requirement that
their worst case regret  against arbitrary adversarially chosen payoff functions  is sublinear as a
function of the number of the rounds.
Lemma 5. If there exists a no Φ-regret algorithm then there exists an FPTAS for ﬁxed points of Φ.
Proof. Let φ0 ∈ CH(Φ) be a given mapping whose ﬁxed point we wish to compute. Let ε be a given
error parameter.

6

At iteration t  let x(t) be the point chosen by A. If kφ0(x(t)) − x(t)k ≤ ε  we can stop  because we
have found an approximate ﬁxed point. Else  supply A with the following payoff function:

f (t)(x)   (φ0(x(t)) − x(t))>

kφ0(x(t)) − x(t)k (x − x(t))

This is a linear function  with k∇f (t)(x)k = 1. Also  f (t)(x(t)) = 0  and f (t)(φ0(x(t))) =
kφ0(x(t)) − x(t)k ≥ ε. After T iterations  since φ0 is a convex combination of functions in Φ 
and since all the f (t) are linear functions  we have

max
φ∈Φ

Thus 

f (t)(φ(x(t))) ≥ TX

TX

t=1

X

t=1

f (t)(φ(x(t))) −X

f (t)(φ0(x(t))) ≥ εT.

RegretΦ(T ) = max
φ∈Φ

(2)
Since A is a no-regret algorithm  assume that A ensures that RegretΦ(T ) = O(T 1−c) for some
constant c > 0. Thus  when T = Ω( 1
ε1/c ) the lower bound (2) on the regret cannot hold unless we
have already found an ε-approximate ﬁxed point of φ0.

t

t

f (t)(x(t)) ≥ εT.

The second direction is on the lines of the algorithms of [2] and [16] which use ﬁxed point compu-
tations to obtain no internal regret algorithms.
√
Lemma 6. If there is an FPTAS for ﬁxed points of Φ  then there is an efﬁcient no Φ-regret algorithm.
In fact  the algorithm guarantees that RegretΦ(T ) = O(

T ). 2

Proof. We reduce the given OCO problem to an “inner” OCO problem. The “outer” OCO problem
is the original one. We use a no external regret algorithm for the inner OCO problem to generate
points in K for the outer one  and use the payoff functions obtained in the outer OCO problem to
generate appropriate payoff functions for the inner one.
Let Φ = {φ1  φ2  . . .   φN}. The domain for the inner OCO problem is the simplex of all distribu-
tions on Φ  denoted ∆N . For a distribution α ∈ ∆N   let αi be the probability measure assigned to
φi in the distribution α. There is a natural mapping from ∆N → CH(Φ): for any α ∈ ∆N   denote

by φα the functionPN

i=1 αiφi ∈ CH(Φ).

Let x(t) ∈ K be the point used in the outer OCO problem in the tth round  and let f (t) be the
obtained payoff function. Then the payoff functions for the inner OCO problem is the function
g(t) : ∆N → R deﬁned as follows:

∀α ∈ ∆N :

g(t)(α)   f (t)(φα(x(t))).

i αi(φi(x(t)) − x0))  becauseP

We can rewrite g(t) as g(t)(α) = f (t)(x0 +P

We now apply the Exponentiated Gradient (EG) algorithm (see Section 2.2) to the inner OCO prob-
lem. To analyze the algorithm  we bound k∇g(t)k∞ as follows. Let x0 be an arbitrary point in K.
i αi = 1. Then 
∇g(t) = X(t)∇f (t)(φα(x(t)))  where X(t) is an N × n matrix whose ith row is (φi(x(t)) − x0)>.
Thus 
|(φi(x(t))−x0)>∇f (t)(φα(x(t)))| ≤ kφi(x(t))−x0kk∇f (t)(φα(x(t)))k ≤ 1.
k∇g(t)k∞ = max
The last inequality follows because we assumed that the diameter of K is bounded by 1  and the
norm of the gradient of f (t) is also bounded by 1.
Let α(t) be the distribution on Φ produced by the EG algorithm at time t. Now  the point x(t) is
computed by running the FPTAS for computing an 1√
-approximate ﬁxed point of the function φα(t) 
i.e. we have kφα(t)(x(t)) − x(t)k ≤ 1√

.

t

i

t

2In the full version of the paper  we improve the regret bound to O(log T ) under some stronger concavity

assumptions on the payoff functions.

7

Now  using the deﬁnition of the g(t) functions  and by the regret bound for the EG algorithm  we
have that for any ﬁxed distribution α ∈ ∆N  

TX

g(t)(α)− TX

g(t)(α(t)) ≤ O(plog(N)T ). (3)

TX

f (t)(φα(x(t)))− TX

f (t)(φα(t)(x(t))) =

t=1

t=1

Since k∇f (t)k ≤ 1 

t=1

t=1

f (t)(φα(t)(x(t))) − f (t)(x(t)) ≤ kφα(t)(x(t)) − x(t)k ≤ 1√

t

.

(4)

Summing (4) from t = 1 to T   and adding to (3)  we get that for any distribution α over Φ 

TX
f (t)(φα(x(t))) −X
PT
t=1 f (t)(φi(x(t))) −PT

t=1

t

f (t)(x(t)) ≤ O(plog(N)T ) +
t=1 f (t)(x(t)) ≤ O(plog(N)T )  and thus we have a no Φ-regret al-

= O(plog(N)T ).

the above inequality implies that

TX

In particular  by concentrating α on any given φi 

1√
t

t=1

gorithm.

References
[1] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta algorithm and

applications. Manuscript  2005.

[2] A. Blum and Y. Mansour. From external to internal regret. In COLT  pages 621–636  2005.
[3] X. Chen and X. Deng. Settling the complexity of two-player nash equilibrium. In 47th FOCS  pages

261–272  2006.

[4] X. Chen  X. Deng  and S-H. Teng. Computing nash equilibria: Approximation and smoothed complexity.

focs  0:603–612  2006.

[5] T. Cover. Universal portfolios. Math. Finance  1:1–19  1991.
[6] C. Daskalakis  P. W. Goldberg  and C. H. Papadimitriou. The complexity of computing a nash equilibrium.

In 38th STOC  pages 71–78  2006.

[7] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic

Behavior  29:79–103  1999.

[8] G. Gordon  A. Greenwald  C. Marks  and M. Zinkevich. No-regret learning in convex games. Brown

University Tech Report CS-07-10  2007.

[9] A. Greenwald and A. Jafari. A general class of no-regret learning algorithms and game-theoretic equilib-

ria  2003.

[10] J. Hannan. Approximation to bayes risk in repeated play. In M. Dresher  A. W. Tucker  and P. Wolfe 

editors  Contributions to the Theory of Games  volume III  pages 97–139  1957.

[11] S. Hart and A. Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica 

68(5):1127–1150  2000.

[12] S. Hart and D. Schmeidler. Existence of correlated equilibria. Mathematics of Operations Research 

14(1):18–25  1989.

[13] E. Hazan  A. Kalai  S. Kale  and A. Agarwal. Logarithmic regret algorithms for online convex optimiza-

tion. In 19’th COLT  2006.

[14] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Inf.

Comput.  132(1):1–63  1997.

[15] C. H. Papadimitriou. On the complexity of the parity argument and other inefﬁcient proofs of existence.

J. Comput. Syst. Sci.  48(3):498–532  1994.

[16] G. Stoltz and G. Lugosi. Internal regret in on-line portfolio selection. Machine Learning  59:125–159 

2005.

[17] G. Stoltz and G. Lugosi. Learning correlated equilibria in games with compact sets of strategies. Games

and Economic Behavior  59:187–208  2007.

[18] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In 20th ICML 

pages 928–936  2003.

8

,Harshil Shah
David Barber