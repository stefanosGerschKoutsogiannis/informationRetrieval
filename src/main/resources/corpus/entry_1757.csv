2019,Efficient Smooth Non-Convex Stochastic Compositional Optimization via Stochastic Recursive Gradient Descent,Stochastic compositional optimization arises in many important machine learning tasks such as reinforcement learning and portfolio management. The objective function is the composition of two expectations of stochastic functions  and is more challenging to optimize than vanilla stochastic optimization problems. In this paper  we investigate the stochastic compositional optimization in the general smooth non-convex setting. We employ a recently developed idea of \textit{Stochastic Recursive Gradient Descent} to design a novel algorithm named SARAH-Compositional  and prove a sharp Incremental First-order Oracle (IFO) complexity upper bound for stochastic compositional optimization: $\mathcal{O}((n+m)^{1/2} \varepsilon^{-2})$ in the finite-sum case and $\mathcal{O}(\varepsilon^{-3})$ in the online case. Such a complexity is known to be the best one among IFO complexity results for non-convex stochastic compositional optimization. Numerical experiments validate the superior performance of our algorithm and theory.,Efﬁcient Smooth Non-Convex Stochastic

Compositional Optimization via Stochastic Recursive

Gradient Descent

Missouri University of Science and Techology

Wenqing Hu

huwen@mst.edu

Chris Junchi Li 
Tencent AI Lab

junchi.li.duke@gmail.com

Xiangru Lian

University of Rochester

admin@mail.xrlian.com

Ji Liu

University of Rochester & Kwai Inc.

ji.liu.uwisc@gmail.com

Huizhuo Yuan∗
Peking University

hzyuan@pku.edu.cn

Abstract

Stochastic compositional optimization arises in many important machine learning
applications. The objective function is the composition of two expectations of
stochastic functions  and is more challenging to optimize than vanilla stochastic
optimization problems. In this paper  we investigate the stochastic compositional
optimization in the general smooth non-convex setting. We employ a recently
developed idea of Stochastic Recursive Gradient Descent to design a novel algo-
rithm named SARAH-Compositional  and prove a sharp Incremental First-order
Oracle (IFO) complexity upper bound for stochastic compositional optimization:
O((n + m)1/2ε−2) in the ﬁnite-sum case and O(ε−3) in the online case. Such
a complexity is known to be the best one among IFO complexity results for
non-convex stochastic compositional optimization. Numerical experiments on risk-
adverse portfolio management validate the superiority of SARAH-Compositional
over a few rival algorithms.

1

Introduction

We consider the general smooth  non-convex compositional optimization problem of minimizing the
composition of two expectations of stochastic functions:

{Φ(x) ≡ (f ◦ g)(x)}  

min
x∈Rd

(1)
where the outer and inner functions f : Rl → R  g : Rd → Rl are deﬁned as f (y) := Ev[fv(y)] 
g(x) := Ew[gw(y)]  v and w are index random variables  and each component fv  gw are smooth
but not necessarily convex. Compositional optimization can be used to formulate many important
machine learning problems  e.g. reinforcement learning (Sutton and Barto  1998)  risk management
(Dentcheva et al.  2017)  multi-stage stochastic programming (Shapiro et al.  2009)  deep neural net
(Yang et al.  2019)  etc. We list a speciﬁc application instance that can be written in the stochastic
compositional form of (1)  namely the risk-adverse portfolio management problem  formulated as

min
x∈RN

− 1
T

(cid:104)rt  x(cid:105) +

1
T

(cid:104)rt  x(cid:105) − 1
T

(cid:104)rs  x(cid:105)

 

(2)

∗Partial work was performed when the author was an intern at Tencent AI Lab. Full version of this paper is

available at: http://arxiv.org/abs/1912.13515

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

T(cid:88)

t=1

(cid:32)

T(cid:88)

t=1

(cid:33)2

T(cid:88)

s=1

Algorithm
SCGD (Wang et al.  2017a)
Acc-SCGD (Wang et al.  2017a)
SCGD (Wang et al.  2017b)
SCVR / SC-SCSG (Liu et al.  2017)
VRSC-PG (Huo et al.  2018)
SARAH-Compositional (this work)

Finite-sum
unknown
unknown
unknown

(n + m)4/5ε−2
(n + m)2/3ε−2
(n + m)1/2ε−2

Online
ε−8
ε−7
ε−4.5
ε−3.6
unknown

ε−3

Table 1: Comparison of IFO complexities with different algorithms for general non-convex problem.

where rt ∈ RN denotes the returns of N assets at time t  and x ∈ RN denotes the investment quantity
corresponding to N assets. The goal is to maximize the return while controlling the variance of the
portfolio. (2) can be written as a compositional optimization problem with two functions

(cid:34)

T(cid:88)

t=1

T(cid:88)

s=1

1
T

(cid:104)rs  x(cid:105)

(cid:35)(cid:62)
T(cid:88)
(cid:0)(cid:104)rt  w\(N +1)(cid:105) − wN +1

 

(cid:1)2

 

(3)

(4)

g(x) =

x1  x2  . . .   xN  

f (w) = − 1
T

(cid:104)rt  w\(N +1)(cid:105) +

1
T

t=1

where w\(N +1) denotes the (column) subvector consisting of the ﬁrst N coordinates of w  and wN +1
denotes the (N + 1)-th coordinate of w.
Compared with vanilla stochastic optimization problem where the optimizer is allowed to access
the stochastic gradients  stochastic compositional problem (1) is more difﬁcult to solve. Classical
algorithms for solving (1) are often more computationally challenging. This is mainly due to the
nonlinear structure of the composition function with respect to the random index pair (v  w). Treating
the objective function as an expectation Evfv(g(x))  computing each iterate of the gradient estimation
involves recalculating g(x) = Ewgw(x)  which is either time-consuming or impractical. To tackle
such weakness in practice  Wang et al. (2017a) ﬁrstly introduce a two-time-scale algorithm called
Stochastic Compositional Gradient Descent (SCGD) along with its accelerated (in Nesterov’s sense)
variant Acc-SCGD  and provide a ﬁrst convergence rate analysis to that problem. Subsequently 
Wang et al. (2017b) proposed accelerated stochastic compositional proximal gradient algorithm
(ASC-PG) which improves over the upper bound complexities in Wang et al. (2017a). Furthermore 
variance reduced gradient methods designed speciﬁcally for compositional optimization on non-
convex settings arises from Liu et al. (2017) and later generalized to the nonsmooth setting (Huo et al. 
2018). These approaches aim at getting variance reduced estimators of g  ∂g and ∂g(x)∇f (g(x)) 
respectively. Such success signals the necessity and possibility of designing a special algorithm for
non-convex objectives with better convergence rates.
In this paper  we propose an efﬁcient algorithm called SARAH-Compositional for the stochastic
compositional optimization problem (1). For notational simplicity  we let n  m ≥ 1 and the index
pair (v  w) be uniformly distributed over the product set [1  n] × [1  m]  i.e.

 1

m

m(cid:88)

j=1

n(cid:88)

i=1

fi

 .

gj(x)

Φ(x) =

1
n

(5)

We use the same notation for the online case  in which case either n or m can be inﬁnite.
A fundamental theoretical question for stochastic compositional optimization is the Incremental
First-order Oracle (IFO) (the number of individual gradient and function evaluations; see Deﬁnition 1
in §2 for a precise deﬁnition) complexity bounds for stochastic compositional optimization. Our new
SARAH-Compositional algorithm is developed by integrating the iteration of Stochastic Recursive
Gradient Descent (Nguyen et al.  2017)  shortened as SARAH 2 with the stochastic compositional
optimization formulation (Wang et al.  2017a). The motivation of this approach is that SARAH

2This is also referred to as stochastic recursive variance reduction method  incremental variance reduction
method or SPIDER-BOOST in various recent literatures. We stick to name the algorithm after SARAH to
respect to our best knowledge the earliest discovery of that algorithm.

2

with speciﬁc choice of stepsizes is known to be optimal in stochastic optimization and regarded as
a cutting-edge variance reduction technique  with signiﬁcantly reduced oracle access complexities
than earlier variance reduction method (Fang et al.  2018). We prove that SARAH-Compositional

can reach an IFO computational complexity of O(min(cid:0)(n + m)1/2ε−2  ε−3(cid:1))  improving the best
known result of O(min(cid:0)(n + m)2/3ε−2  ε−3.6(cid:1)) in non-convex compositional optimization. See

Table 1 for detailed comparison.
Related Works Classical ﬁrst-order methods such as gradient descent (GD)  accelerated gradient
descent (AGD) and stochastic gradient descent (SGD) have received intensive attetions in both
convex and non-convex optimization (Nesterov  2004; Ghadimi and Lan  2016; Li and Lin  2015).
When the objective can be written in a ﬁnite-sum or online/expectation structure  variance-reduced
gradient (a.k.a. variance reduction) techniques including SAG (Schmidt et al.  2017)  SVRG (Xiao
and Zhang  2014; Allen-Zhu and Hazan  2016; Reddi et al.  2016)  SDCA (Shalev-Shwartz and
Zhang  2013  2014)  SAGA (Defazio et al.  2014)  SCSG (Lei et al.  2017)  SNVRG (Zhou et al. 
2018)  SARAH/SPIDER (Nguyen et al.  2017; Fang et al.  2018; Wang et al.  2019; Nguyen et al. 
2019)  etc.  can be employed to improve the theoretical convergence properties of classical ﬁrst-order
algorithms. Notably in the smooth nonconvex setting  Fang et al. (2018) recently proposed the
SPIDER-SFO algorithm which non-trivially hybrids the iteration of stochastic recursive gradient
descent (SARAH) (Nguyen et al.  2017) with the normalized gradient descent. In the representative
case of batch-size 1  SPIDER-SFO adopts a small step-length that is proportional to ε2 ∧ εn−1/2
where ε is the squared targeted accuracy  and (by rebooting the SPIDER tracking iteration once every
n ∧ O(ε−2) iterates) the variance of the stochastic estimator can be constantly controlled by O(ε2).
For ﬁnding ε-accurate solution purposes  recent works Wang et al. (2019); Nguyen et al. (2019)
discovered two variants of the SARAH algorithm that achieve the same complexity as SPIDER-
SFO (Fang et al.  2018) and SNVRG (Zhou et al.  2018).3 The theoretical convergence property of
SARAH/SPIDER methods in the smooth non-convex case outperforms that of SVRG  and is provably
optimal under a set of mild assumptions (Arjevani et al.  2019; Fang et al.  2018; Nguyen et al.  2019;
Wang et al.  2019).
It turns out that when solving compositional optimization problem (1)  classical ﬁrst-order methods
for optimizing a single objective function can either be non-applicable or it brings at least O(m)
queries to calculate the inner function g. To remedy this issue  Wang et al. (2017a b) considered the
stochastic setting and proposed the SCGD algorithm to calculate or estimate the inner ﬁnite-sum
more efﬁciently  achieving a polynomial rate that is independent of m. Later on  Lian et al. (2017);
Liu et al. (2017); Huo et al. (2018) and Lin et al. (2018) merged SVRG method into the compositional
optimization framework to do variance reduction on all three steps of the estimation. In stark contrast 
our work adopts the SARAH/SPIDER method which is theoretically more efﬁcient than the SVRG
method in the non-convex compositional optimization setting.
Contributions This work makes two contributions as follows. First  we propose a new algo-
rithm for stochastic compositional optimization called SARAH-Compositional  which operates
SARAH/SPIDER-type recursive variance reduction to estimate relevant quantities. Second  we
conduct theoretical analysis for both online and ﬁnite-sum cases  which veriﬁes the superiority of
SARAH-Compositional over the best known previous results. In the ﬁnite-sum case  we obtain
a complexity of (n + m)1/2ε−2 which improves over the best known complexity (n + m)2/3ε−2
achieved by Huo et al. (2018). In the online case we obtain a complexity of ε−3 which improves the
best known complexity ε−3.6 obtained in Liu et al. (2017).
Notational Conventions Throughout the paper  we treat the parameters Lg  Lf   LΦ  Mg  Mf   ∆ and
σ as global constants. Let (cid:107)•(cid:107) denote the Euclidean norm of a vector or the operator norm of a matrix
induced by Euclidean norm  and let (cid:107) • (cid:107)F denotes the Frobenius norm. For ﬁxed T ≥ t ≥ 0 let xt:T
denote the sequence {xt  ...  xT}. Let Et[•] denote the conditional expectation E[•|x0  x1  ...  xt].
Let [1  n] = {1  ...  n} and S denote the cardinality of a multi-set S ⊆ [1  n] of samples (a generic
set that permits repeated instances). The averaged sub-sampled stochastic estimator is denoted as
Ai where the summation counts repeated instances. We denote pn = O(qn) if
there exist some constants 0 < c < C < ∞ such that cqn ≤ pn ≤ Cqn as n becomes large. Other
notations are explained at their ﬁrst appearances.

AS = (1/S)(cid:80)

i∈S

3Wang et al. (2019) names their algorithm SPIDER-BOOST since it can be seen as the SPIDER-SFO

algorithm with relaxed step-length restrictions.

3

Organization The rest of our paper is organized as follows. §2 formally poses our algorithm and
assumptions. §3 presents the convergence rate theorem and §4 presents numerical experiments that
apply our algorithm to the task of portfolio management. We conclude our paper in §5. Proofs of
convergence results for ﬁnite-sum and online cases and auxiliary lemmas are deferred to §A and §B
in the supplementary material.

2 SARAH for Stochastic Compositional Optimization

Recall our goal is to solve the compositional optimization problem (1)  i.e. to minimize Φ(x) =
f (g(x)) where

n(cid:88)

i=1

m(cid:88)

j=1

f (y) :=

1
n

fi(y) 

g(x) :=

1
m

gj(x).

Here for each j ∈ [1  m] and i ∈ [1  n] the functions gj : Rd → Rl and fi : Rl → R. We can
formally take the derivative to the function Φ(x) and obtain (via the chain rule) the gradient descent
iteration

xt+1 = xt − η[∂g(xt)](cid:62)∇f (g(xt))  

(6)
where the ∂ operator computes the Jacobian matrix of the smooth mapping  and the gradient operator
∇ is only taken with respect to the ﬁrst-level variable. As discussed in §1  it can be either impossible
(online case) or time-consuming (ﬁnite-sum case) to estimate the terms ∂g(xt) =

m(cid:88)

∂gj(xt)

1
m

j=1

gj(xt) in the iteration scheme (6). In this paper  we design a novel algorithm

and g(xt) =

1
m

m(cid:88)

j=1

(SARAH-Compositional) based on Stochastic Compositional Variance Reduced Gradient method (see
Lin et al. (2018)) yet hybriding with the stochastic recursive gradient method Nguyen et al. (2017).
As the readers see later  our SARAH-Compositional is more efﬁcient than all existing algorithms for
non-convex compositional optimization.
We introduce some deﬁnitions and assumptions. First  we assume the algorithm has accesses to
an Incremental First-order Oracle (IFO) in our black-box environment (Lin et al.  2018); also see
(Agarwal and Bottou  2015; Woodworth and Srebro  2016) for vanilla optimization case:
Deﬁnition 1 (IFO). (Lin et al.  2018) The Incremental First-order Oracle (IFO) returns  when some
x ∈ Rd and j ∈ [1  m] are inputted  the vector-matrix pair [gj(x)  ∂gj(x)] or when some y ∈ Rl and
i ∈ [1  n] are inputted  the scalar-vector pair [fi(y) ∇fi(y)].
Second  our goal in this work is to ﬁnd an ε-accurate solution  deﬁned as
Deﬁnition 2 (ε-accurate solution). We call x ∈ Rd an ε-accurate solution to problem (1)  if

(7)
It is worth remarking here that the inequality (7) can be modiﬁed to (cid:107)∇Φ(x)(cid:107) ≤ Cε for some global
constant C > 0 without hurting the magnitude of IFO complexity bounds.
Let us ﬁrst make some assumptions regarding to each component of the (compositional) objective
function. Analogous to Assumption 1(i) in Fang et al. (2018)  we make the following ﬁnite gap
assumption:
Assumption 1 (Finite gap). We assume that the algorithm is initialized at x0 ∈ Rd with

(cid:107)∇Φ(x)(cid:107) ≤ ε.

∆ := Φ(x0) − Φ∗ < ∞  

where Φ∗ denotes the global minimum value of Φ(x).
We make the following smoothness and boundedness assumptions  which are standard in recent
compositional optimization literatures (e.g. Lian et al. (2017); Huo et al. (2018); Lin et al. (2018)).
Assumption 2 (Smoothness). There exist Lipschitz constants Lg  Lf   LΦ > 0 such that for i ∈ [1  n] 
j ∈ [1  m] we have

(cid:107)∂gj(x) − ∂gj(x(cid:48))(cid:107)F
(cid:107)∇fi(y) − ∇fi(y(cid:48))(cid:107)

(cid:13)(cid:13)[∂gj(x)](cid:62)∇fi(g(x)) − [∂gj(x(cid:48))](cid:62)∇fi(g(x(cid:48)))(cid:13)(cid:13) ≤ LΦ(cid:107)x − x(cid:48)(cid:107)

≤ Lg(cid:107)x − x(cid:48)(cid:107)
≤ Lf(cid:107)y − y(cid:48)(cid:107)

for x  x(cid:48) ∈ Rd 
for y  y(cid:48) ∈ Rl 
for x  x(cid:48) ∈ Rd.

(8)

(9)

4

Algorithm 1 SARAH-Compositional  Online Case (resp. Finite-Sum Case)

Input: T  q  x0  η  SL
for t = 0 to T − 1 do

1   SL

2   SL
3

if mod (t  q) = 0 then

Draw SL

1 indices with replacement S L

1 t ⊆ [1  m] and let gt =

Draw SL

2 indices with replacement S L

2 t ⊆ [1  m] and let Gt =

(resp. gt = g (xt) in ﬁnite-sum case)

1
SL
1

1
SL
2

(cid:80)
(cid:80)
(cid:34)

gj(xt)

j∈S L

1 t

∂gj(xt)

j∈S L

2 t

(cid:80)

1
SL
3

i∈S L

3 t

(cid:62)
3 t ⊆ [1  n] and let Ft = (Gt)

(resp. Gt = ∂g (xt) in ﬁnite-sum case)
∇fi(gt)

(resp. Ft = (Gt)

(cid:62) ∇f (gt) in ﬁnite-sum case)

(cid:35)

Draw SL

3 indices with replacement S L

else

Draw one index jt ∈ [1  m] and let gt = gjt(xt) − gjt(xt−1) + gt−1 and

Gt = ∂gjt(xt) − ∂gjt(xt−1) + Gt−1

Draw one index it ∈ [1  n] and let

Ft = (Gt)

(cid:62) ∇fit (gt) − (Gt−1)
return Output(cid:101)x chosen uniformly at random from {xt}T−1

end if
Update xt+1 = xt − ηFt

end for

t=0

(cid:62) ∇fit (gt−1) + Ft−1

Here for the purpose of using stochastic recursive estimation of ∂g(x)  we slightly strengthen the
smoothness assumption by adopting the Frobenius norm on the left hand of the ﬁrst line of (9).
Assumption 3 (Boundedness). There exist boundedness constants Mg  Mf > 0 such that for
i ∈ [1  n]  j ∈ [1  m] we have

(cid:107)∂gj(x)(cid:107) ≤ Mg
(cid:107)∇fi(y)(cid:107) ≤ Mf

for x ∈ Rd 
for y ∈ Rl.

(10)

Notice that applying mean-value theorem for vector-valued functions to (10) gives another Lipschitz
condition

(cid:107)gj(x) − gj(x(cid:48))(cid:107) ≤ Mg(cid:107)x − x(cid:48)(cid:107)

(11)
and analogously for fi(y). It turns out that under the above two assumptions  a choice of LΦ in (9)
can be expressed as a polynomial of Lf   Lg  Mf   Mg. For clarity purposes in the rest of this paper 
we adopt the following typical choice of LΦ

for x  x(cid:48) ∈ Rd  

LΦ ≡ Mf Lg + M 2

g Lf  

(12)

whose applicability can be veriﬁed via a simple application of the chain rule. We integrate both
ﬁnite-sum and online cases into one algorithm SARAH-Compositional and write it in Algorithm 1.

3 Convergence Rate Analysis

In this section  we aim to justify that our proposed SARAH-Compositional algorithm provides IFO
complexities of O((n + m)1/2ε−2) in the ﬁnite-sum case and O(ε−3) in the online case  which
supersedes the concurrent and comparative algorithms (see more in Table 1).
Let us ﬁrst analyze the convergence in the ﬁnite-sum case. In this case we have S L
S L
2 = [1  m]  S L

3 = [1  n]. Involved analysis leads us to conclude

1 = [1  m] 

5

Theorem 1 (Finite-sum case). Suppose Assumptions 1  2 and 3 in §2 hold  let S L
S L
3 = [1  n]  q = (2m + n)/3  and set the stepsize

1 = S L

2 = [1  m] 

(cid:114)

η =

(cid:16)

1

(cid:17) .

Then for the ﬁnite-sum case  SARAH-Compositional Algorithm 1 outputs an (cid:101)x satisfying
E(cid:107)∇Φ((cid:101)x)(cid:107)2 ≤ ε2 in

6(2m + n)

f + M 2

g L2

f L2
g

M 4

iterates. The IFO complexity to achieve an ε-accurate solution is bounded by

√

2m + n ·(cid:113)
2m + n ·(cid:113)

M 4

√

2m + n +

√

24[Φ(x0) − Φ∗]

g L2

f + M 2

g ·
f L2

ε2

√

M 4

g L2

f + M 2

g ·
f L2

1944[Φ(x0) − Φ∗]

ε2

.

(13)

(14)

(15)

Theorem 1 allows us to achieve an ε-accurate solution  and a simple application of Markov’s
inequality allows us to derive high-probability results for achieving ε-accurate solutions. Compared
with Fang et al. (2018)  one observes that Theorem 1 indicates an IFO complexity upper bound of
O(m + n + (m + n)1/2ε−2) to achieve an ε-accurate solution  sharing a similar form with that of
SARAH/SPIDER for non-convex stochastic optimization when m + n is regarded as the number of
individual functions.4 SPIDER-SFO (as a SARAH variant) is optimal in both ﬁnite-sum and online
cases  in the sense that it matches the theoretical lower bound (Fang et al.  2018; Arjevani et al. 
2019)  which makes it tempting to claim that our proposed SARAH-Compositional as its extension is
also optimal. We emphasize that the set of assumptions for compositional optimization is different
from vanilla optimization  and claiming optimality of the IFO complexity requires a corresponding
lower bound result  left as a future direction to explore.
Let us then analyze the convergence in the online case  where we sample minibatches S L
3 of
relevant quantities instead of the ground truth once every q iterates. To characterize the estimation
error  we put in one additional ﬁnite variance assumption:
Assumption 4 (Finite Variance). We assume that there exists H1  H2 and H3 as the upper bounds
on the variance of the functions f (y)  ∂g(x)  and g(x)  respectively  such that

1  S L

2  S L

E(cid:107)gi(x) − g(x)(cid:107)2 ≤ H1
E(cid:107)∂gi(x) − ∂g(x)(cid:107)2 ≤ H2
E(cid:107)∇fi(y) − ∇f (y)(cid:107)2 ≤ H3

for x ∈ Rd 
for x ∈ Rd 
for y ∈ Rl.

(16)

From Assumptions 2 and 3 we can easily verify  via triangle inequality and convexity of norm  that
f . On the contrary  H1 cannot be represented
H2 can be chosen as 4M 2
as a function of boundedness and smoothness constants. We conclude the following theorem for the
online case:

g and H3 can be chosen as 4M 2

Theorem 2 (Online case). Suppose Assumptions 1  2  3 and 4 in §2 hold  let SL

1 =

SL

2 =

3H2M 2
f

ε2

  SL

3 =

and set the stepsize

3H3M 2
g

ε2

  let q =

D0 := 3(cid:0)H1M 2
(cid:114)

f + H2M 2

D0
3ε2 where we denote the noise-relevant parameter
(cid:16)

f + H3M 2
g

(cid:1)  

(cid:17) .

g L2

η =

ε

(17)

(18)

Then for the online case  SARAH-Compositional Algorithm 1 outputs an(cid:101)x satisfying E(cid:107)∇Φ((cid:101)x)(cid:107)2 ≤

f + M 2

g L2

f L2
g

6D0

M 4

g L2
f

3H1M 2
ε2

 

2ε2 in

(19)
4Here and in below  the smoothness and boundedness parameters and Φ(x0) − Φ∗ are treated as constants.

f + M 2

g L2

M 4

ε3

(cid:112)

D0 ·(cid:113)

√

g ·
f L2

24[Φ(x0) − Φ∗]

6

Figure 1: Experiment on the portfolio management. The x-axis is the number of gradients calculations
divided by the number of samples  the y-axis is the function value gap.

iterates. The IFO complexity to achieve an ε-accurate solution is bounded by
1944[Φ(x0) − Φ∗]

√

M 4

g L2

f + M 2

g ·
f L2

(cid:112)

D0 ·(cid:113)

D0
ε2 +

ε3

.

(20)

We see that in the online case  the IFO complexity to achieve an ε-accurate solution is upper bounded
by O(ε−3). Due to space limits  the detailed proofs of Theorems 1 and 2 are deferred to the
supplementary material.

4 Experiments

In this section  we study performance of our algorithm to risk-adverse portfolio management problem
and conduct numerical experiments to support our theory.5 We follow the setups in Huo et al. (2018);
Liu et al. (2017) and compare with existing algorithms for compositional optimization. Readers are
referred to Wang et al. (2017a) for more tasks our algorithm can be potentially applied for.
Recall that in §1  we formulate our portfolio management problem as a mean-variance optimization
problem (2)  which can be formulated as a compositional optimization problem (1). As it satisﬁes
Assumptions 1–4 in a bounded domain of optimization  it serves as a good example to validate our
theory. For convenience we repeat the display here:

T(cid:88)

t=1

(cid:32)

T(cid:88)

t=1

(cid:33)2

T(cid:88)

s=1

min
x∈RN

− 1
T

(cid:104)rt  x(cid:105) +

1
T

(cid:104)rt  x(cid:105) − 1
T

(cid:104)rs  x(cid:105)

 

(2)

where x = {x1  x2  . . .   xN} ∈ RN denotes the quantities invested at every asset i = 1  . . .   N.
When applying SARAH-Compositional we adopt the online case where we pick SL
3 as the
mini-batch sizes once every q steps. Datasets include different portfolio datas formed on Size and
Operating Proﬁtability.6 We choose to use 6 different 25-portfolio datasets where N = 25 and T =
7240  same as the ones adopted by Lin et al. (2018). Speciﬁcally  we choose SL
3 = 2000
(roughly optimized to improve the numerical performance). The results are shown in Figure 1.

2 = SL

1 = SL

1   SL

2   SL

5The source code can be found at http://github.com/angeoz/SCGD. Space limiting  we refer the readers
to the full version of this paper for the experiment studies of other applications including reinforcement learning
and stochastic neighborhood embedding.

6http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html

7

We demonstrate the comparison among our algorithm SARAH-Compositional  SCGD (Wang
et al.  2017a)  ASC-PG (Wang et al.  2017b) and VRSC-PG (Huo et al.  2018) (serving as a
baseline for variance-reduced stochastic compositional optimization methods). We plot the ob-
jective function value gap and gradient norm against IFO complexity (measured by gradients
calculation) for all four algorithms in two covariance settings and six real-world datasets. We
observe that SARAH-Compositional outperforms all comparable algorithms. Our range of stepsize is

(cid:8)1 × 10−5  1 × 10−4  2 × 10−4  5 × 10−4  1 × 10−3  1 × 10−2(cid:9)  and we plot the learning curve for

each algorithm corresponding to their individually optimized stepsize. For SCGD and ASC-PG algo-
rithms  we ﬁx the extrapolation parameter β as 0.9. The q-parameters in both SARAH-Compositional
and VRSC-PG algorithms are set as 50.
The toy experiment provides evidence that our proposed SARAH-Compositional algorithm applied to
risk-adverse portfolio management problem achieves state-of-the art performance. Moreover  we note
that due to the small mini-batch sizes  basic SCGD achieves a less satisfactory result  a phenomenon
also shown by Huo et al. (2018); Lian et al. (2017).

5 Conclusion

In this paper  we propose a novel algorithm called SARAH-Compositional for solving stochastic
compositional optimization problems using the idea of a recently proposed variance reduced gradient
method. Our algorithm achieves both outstanding theoretical and experimental results. Theoretically 
we show that the SARAH-Compositional algorithm can achieve desirable efﬁciency and IFO upper
bound complexities for ﬁnding an ε-accurate solution of non-convex compositional problems in
both ﬁnite-sum and online cases. Theoretically  we show that the SARAH-Compositional algorithm
can achieve improved convergence rates and IFO complexities for ﬁnding an ε-accurate solution
to non-convex compositional problems in both ﬁnite-sum and online cases. Experimentally  we
compare our new compositional optimization method with a few rival algorithms for the task of
portfolio management and demonstrate its superior performance. Future directions include handling
the non-smooth case and the theory of lower bounds for stochastic compositional optimization. We
hope this work can provide new perspectives to both optimization and machine learning communities
interested in compositional optimization.

References
Agarwal  A. and Bottou  L. (2015). A lower bound for the optimization of ﬁnite sums. In International

Conference on Machine Learning  pages 78–86.

Allen-Zhu  Z. and Hazan  E. (2016). Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707.

Arjevani  Y.  Carmon  Y.  Duchi  J. C.  Foster  D. J.  Srebro  N.  and Woodworth  B. (2019). Lower

bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365.

Defazio  A.  Bach  F.  and Lacoste-Julien  S. (2014). Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems  pages 1646–1654.

Dentcheva  D.  Penev  S.  and Ruszczy´nski  A. (2017). Statistical estimation of composite risk
functionals and risk optimization problems. Annals of the Institute of Statistical Mathematics 
69(4):737–760.

Fang  C.  Li  C. J.  Lin  Z.  and Zhang  T. (2018). Spider: Near-optimal non-convex optimization via
stochastic path-integrated differential estimator. In Advances in Neural Information Processing
Systems  pages 689–699.

Ghadimi  S. and Lan  G. (2016). Accelerated gradient methods for nonconvex nonlinear and stochastic

programming. Mathematical Programming  156(1-2):59–99.

Huo  Z.  Gu  B.  Liu  J.  and Huang  H. (2018). Accelerated method for stochastic composition
optimization with nonsmooth regularization. In Thirty-Second AAAI Conference on Artiﬁcial
Intelligence.

8

Lei  L.  Ju  C.  Chen  J.  and Jordan  M. I. (2017). Non-convex ﬁnite-sum optimization via scsg

methods. In Advances in Neural Information Processing Systems  pages 2345–2355.

Li  H. and Lin  Z. (2015). Accelerated proximal gradient methods for nonconvex programming. In

Advances in neural information processing systems  pages 379–387.

Lian  X.  Wang  M.  and Liu  J. (2017). Finite-sum composition optimization via variance reduced
In International Conference on Artiﬁcial Intelligence and Statistics  pages

gradient descent.
1159–1167.

Lin  T.  Fan  C.  Wang  M.  and Jordan  M. I. (2018). Improved oracle complexity for stochastic

compositional variance reduced gradient. arXiv preprint arXiv:1806.00458.

Liu  L.  Liu  J.  and Tao  D. (2017). Variance reduced methods for non-convex composition optimiza-

tion. arXiv preprint arXiv:1711.04416.

Nesterov  Y. (2004). Introductory lectures on convex optimization: A basic course  volume 87.

Springer.

Nguyen  L. M.  Liu  J.  Scheinberg  K.  and Takáˇc  M. (2017). Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In International Conference on Machine
Learning  pages 2613–2621.

Nguyen  L. M.  van Dijk  M.  Phan  D. T.  Nguyen  P. H.  Weng  T.-W.  and Kalagnanam  J. R. (2019).
Optimal ﬁnite-sum smooth non-convex optimization with sarah. arXiv preprint arXiv:1901.07648.

Reddi  S. J.  Hefny  A.  Sra  S.  Poczos  B.  and Smola  A. (2016). Stochastic variance reduction for

nonconvex optimization. In International conference on machine learning  pages 314–323.

Schmidt  M.  Le Roux  N.  and Bach  F. (2017). Minimizing ﬁnite sums with the stochastic average

gradient. Mathematical Programming  162(1-2):83–112.

Shalev-Shwartz  S. and Zhang  T. (2013). Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  14(Feb):567–599.

Shalev-Shwartz  S. and Zhang  T. (2014). Accelerated proximal stochastic dual coordinate ascent for
regularized loss minimization. In International Conference on Machine Learning  pages 64–72.

Shapiro  A.  Dentcheva  D.  and Ruszczy´nski  A. (2009). Lectures on stochastic programming:

modeling and theory. SIAM.

Sutton  R. S. and Barto  A. G. (1998). Reinforcement learning: An introduction.

Wang  M.  Fang  E. X.  and Liu  H. (2017a). Stochastic compositional gradient descent: Algorithms
for minimizing compositions of expected-value functions. Mathematical Programming  161(1-
2):419–449.

Wang  M.  Liu  J.  and Fang  E. X. (2017b). Accelerating stochastic composition optimization.

Journal of Machine Learning Research  18:1–23.

Wang  Z.  Ji  K.  Zhou  Y.  Liang  Y.  and Tarokh  V. (2019). Spiderboost and momentum: Faster
variance reduction algorithms. In Advances in Neural Information Processing Systems  pages
2403–2413.

Woodworth  B. E. and Srebro  N. (2016). Tight complexity bounds for optimizing composite

objectives. In Advances in Neural Information Processing Systems  pages 3639–3647.

Xiao  L. and Zhang  T. (2014). A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075.

Yang  S.  Wang  M.  and Fang  E. X. (2019). Multilevel stochastic gradient methods for nested

composition optimization. SIAM Journal on Optimization  29(1):616–659.

Zhou  D.  Xu  P.  and Gu  Q. (2018). Stochastic nested variance reduced gradient descent for
nonconvex optimization. In Advances in Neural Information Processing Systems  pages 3921–
3932.

9

,Naman Agarwal
Ananda Theertha Suresh
Felix Xinnan Yu
Sanjiv Kumar
Brendan McMahan
Wenqing Hu
Chris Junchi Li
Xiangru Lian
Ji Liu
Huizhuo Yuan