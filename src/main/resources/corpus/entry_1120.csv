2018,Constructing Deep Neural Networks by Bayesian Network Structure Learning,We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus  the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then  instead of directly learning the discriminative structure  it learns a generative graph  constructs its stochastic inverse  and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures  while maintaining classification accuracy---state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.,Constructing Deep Neural Networks by Bayesian

Network Structure Learning

Raanan Y. Rohekar

Intel AI Lab

raanan.yehezkel@intel.com

Shami Nisimov

Intel AI Lab

shami.nisimov@intel.com

Yaniv Gurwicz

Intel AI Lab

yaniv.gurwicz@intel.com

Guy Koren
Intel AI Lab

guy.koren@intel.com

Gal Novik
Intel AI Lab

gal.novik@intel.com

Abstract

We introduce a principled approach for unsupervised structure learning of deep
neural networks. We propose a new interpretation for depth and inter-layer con-
nectivity where conditional independencies in the input distribution are encoded
hierarchically in the network structure. Thus  the depth of the network is determined
inherently. The proposed method casts the problem of neural network structure
learning as a problem of Bayesian network structure learning. Then  instead of
directly learning the discriminative structure  it learns a generative graph  constructs
its stochastic inverse  and then constructs a discriminative graph. We prove that
conditional-dependency relations among the latent variables in the generative graph
are preserved in the class-conditional discriminative graph. We demonstrate on
image classiﬁcation benchmarks that the deepest layers (convolutional and dense)
of common networks can be replaced by signiﬁcantly smaller learned structures 
while maintaining classiﬁcation accuracy—state-of-the-art on tested benchmarks.
Our structure learning algorithm requires a small computational cost and runs
efﬁciently on a standard desktop CPU.

1

Introduction

Over the last decade  deep neural networks have proven their effectiveness in solving many chal-
lenging problems in various domains such as speech recognition (Graves & Schmidhuber  2005) 
computer vision (Krizhevsky et al.  2012; Girshick et al.  2014; Szegedy et al.  2015) and machine
translation (Collobert et al.  2011). As compute resources became more available  large scale models
having millions of parameters could be trained on massive volumes of data  to achieve state-of-the-art
solutions. Building these models requires various design choices such as network topology  cost
function  optimization technique  and the conﬁguration of related hyper-parameters.
In this paper  we focus on the design of network topology—structure learning. Generally  exploration
of this design space is a time consuming iterative process that requires close supervision by a human
expert. Many studies provide guidelines for design choices such as network depth (Simonyan &
Zisserman  2014)  layer width (Zagoruyko & Komodakis  2016)  building blocks (Szegedy et al. 
2015)  and connectivity (He et al.  2016; Huang et al.  2016). Based on these guidelines  these studies
propose several meta-architectures  trained on huge volumes of data. These were applied to other
tasks by leveraging the representational power of their convolutional layers and ﬁne-tuning their
deepest layers for the task at hand (Donahue et al.  2014; Hinton et al.  2015; Long et al.  2015; Chen
et al.  2015; Liu et al.  2015). However  these meta-architectures may be unnecessarily large and
require large computational power and memory for training and inference.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

The problem of model structure learning has been widely researched for many years in the proba-
bilistic graphical models domain. Speciﬁcally  Bayesian networks for density estimation and causal
discovery (Pearl  2009; Spirtes et al.  2000). Two main approaches were studied: score-based and
constraint-based. Score-based approaches combine a scoring function  such as BDe (Cooper &
Herskovits  1992)  with a strategy for searching in the space of structures  such as greedy equivalence
search (Chickering  2002). Adams et al. (2010) introduced an algorithm for sampling deep belief
networks (generative model) and demonstrated its applicability to high-dimensional image datasets.
Constraint-based approaches (Pearl  2009; Spirtes et al.  2000) ﬁnd the optimal structures in the large
sample limit by testing conditional independence (CI) between pairs of variables. They are generally
faster than score-based approaches (Yehezkel & Lerner  2009) and have a well-deﬁned stopping
criterion (e.g.  maximal order of conditional independence). However  these methods are sensitive to
errors in the independence tests  especially in the case of high-order CI tests and small training sets.
Motivated by these methods  we propose a new interpretation for depth and inter-layer connectivity in
deep neural networks. We derive a structure learning algorithm such that a hierarchy of independencies
in the input distribution is encoded in a deep generative graph  where lower-order independencies are
encoded in deeper layers. Thus  the number of layers is automatically determined  which is a desirable
virtue in any architecture learning method. We then convert the generative graph into a discriminative
graph  demonstrating the ability of the latter to mimic (preserve conditional dependencies) of the
former. In the resulting structure  a neuron in a layer is allowed to connect to neurons in deeper layers
skipping intermediate layers. This is similar to the shortcut connection (Raiko et al.  2012)  while
our method derives it automatically. Moreover  neurons in deeper layers represent low-order (small
condition sets) independencies and have a wide scope of the input  whereas neurons in the ﬁrst layers
represent higher-order (larger condition sets) independencies and have a narrower scope. An example
of a learned structure  for MNIST  is given in Figure 1 (X are image pixels).

Figure 1: An example of a structure learned by our algorithm (classifying MNIST digits  99.07%
accuracy). Neurons in a layer may connect to neurons in any deeper layer. Depth is determined
automatically. Each gather layer selects a subset of the input  where each input variable is gathered
only once. A neural route  starting with a gather layer  passes through densely connected layers where
it may split (copy) and merge (concatenate) with other routes in correspondence with the hierarchy of
independencies identiﬁed by the algorithm. All routes merge into the ﬁnal output layer.

The paper is organized as follows. We discuss related work in Section 2. In Section 3 we describe
our method and prove its correctness in supplementary material Sec. A. We provide experimental
results in Section 4  and conclude in Section 5.

2 Related Work

Recent studies have focused on automating the exploration of the design space  posing it as a hyper-
parameter optimization problem and proposing various approaches to solve it. Miconi (2016) learns
the topology of an RNN introducing structural parameters into the model and optimizing them along
with the model weights by the common gradient descent methods. Smith et al. (2016) take a similar
approach incorporating the structure learning into the parameter learning scheme  gradually growing
the network up to a maximum size.

2

XYgatherdenseconcatcopyA common approach is to deﬁne the design space in a way that enables a feasible exploration process
and design an effective method for exploring it. Zoph & Le (2016) (NAS) ﬁrst deﬁne a set of
hyper-parameters characterizing a layer (number of ﬁlters  kernel size  stride). Then they use a
controller-RNN for ﬁnding the optimal sequence of layer conﬁgurations for a “trainee network”. This
is done using policy gradients (REINFORCE) for optimizing the objective function that is based
on the accuracy achieved by the “trainee” on a validation set. Although this work demonstrates
capabilities to solve large-scale problems (Imagenet)  it comes with huge computational cost. In a
following work  Zoph et al. (2017) address the same problem but apply a hierarchical approach. They
use NAS to design network modules on a small-scale dataset (CIFAR-10) and transfer this knowledge
to a large-scale problem by learning the optimal topology composed of these modules. Baker et al.
(2016) use reinforcement learning as well and apply Q-learning with epsilon-greedy exploration
strategy and experience replay. Negrinho & Gordon (2017) propose a language that allows a human
expert to compactly represent a complex search-space over architectures and hyper-parameters as a
tree and then use methods such as MCTS or SMBO to traverse this tree. Smithson et al. (2016) present
a multi objective design space exploration  taking into account not only the classiﬁcation accuracy
but also the computational cost. In order to reduce the cost involved in evaluating the network’s
accuracy  they train a Response Surface Model that predicts the accuracy at a much lower cost 
reducing the number of candidates that go through actual validation accuracy evaluation. Another
common approach for architecture search is based on evolutionary strategies to deﬁne and search the
design space. Real et al. (2017) and Miikkulainen et al. (2017) use evolutionary algorithm to evolve
an initial model or blueprint based on its validation performance.
Common to all these recent studies is the fact that structure learning is done in a supervised manner 
eventually learning a discriminative model. Moreoever  these approaches require huge compute
resources  rendering the solution unfeasible for most applications given limited compute and time.

3 Proposed Method
Preliminaries. Consider X = {Xi}N
i=1 a set of observed (input) random variables  H a set of latent
variables  and Y a target (classiﬁcation or regression) variable. Each variable is represented by a
single node  and a single edge connects two distinct nodes. The parent set of a node X in G is denoted
Pa(X; G)  and the children set is denoted Ch(X; G). Consider four graphical models  G  Ginv  Gdis 
and gX. Graph G is a generative DAG deﬁned over X ∪ H  where Ch(X; G) = ∅ ∀X ∈ X. Graph
G can be described as a layered deep Bayesian network where the parents of a node can be in any
deeper layer and not restricted to the previous layer1. In a graph with m latent layers  we index the
deepest layer as 0 and the layer connected to the input as m − 1. The root nodes (parentless) are
latent  H (0)  and the leaves (childless) are the observed nodes  X  and Pa(X; G) ⊂ H. Graph Ginv
is called a stochastic inverse of G  deﬁned over X ∪ H  where Pa(X; Ginv) = ∅ ∀X ∈ X. Graph
Gdis is a discriminative graph deﬁned over X ∪ H ∪ Y   where Pa(X; Gdis) = ∅ ∀X ∈ X and
Ch(Y ; Gdis) = ∅. Graph gX is a CPDAG (a family of Markov equivalent Bayesian networks) deﬁned
over X. Graph gX is generated and maintained as an internal state of the algorithm  serving as an
auxiliary graph. The order of an independence relation between two variables is deﬁned to be the
condition set size. For example  if X1 and X2 are independent given X3  X4  and X5 (d-separated in
the faithful DAG X1 ⊥⊥ X2|{X3  X4  X5})  then the independence order is |{X3  X4  X5}| = 3.

3.1 Key Idea

and the posterior is P (Y |X) = (cid:82) P (H|X)P (Y |H (0))dH  where H = {H (i)}m−1

We cast the problem of learning the structure of a deep neural network as a problem of learning the
structure of a deep (discriminative) probabilistic graphical model  Gdis. That is  a graph of the form
X (cid:32) H (m−1) (cid:32) ··· (cid:32) H (0) → Y   where “(cid:32)” represent a sparse connectivity which we learn 
and “→” represents full connectivity. The joint probability factorizes as P (X)P (H|X)P (Y |H (0))
. We refer
to the P (H|X) part of the equation as the recognition network of an unknown “true” underlying
generative model  P (X|H). That is  the network corresponding to P (H|X) approximates the
posterior (e.g.  as in amortized inference). The key idea is to approximate the latents H that

0

1This differs from the common deﬁnition of deep belief networks (Hinton et al.  2006; Adams et al.  2010)

where the parents are restricted to the next layer.

3

generated the observed X  and then use these values of H (0) for classiﬁcation. That is  avoid
learning Gdis directly and instead  learn a generative structure X (cid:32) H  and then reverse the ﬂow by
constructing a stochastic inverse (Stuhlmüller et al.  2013) X (cid:32) H. Finally  add Y and modify the
graph to preserve conditional dependencies (Gdis can mimic G; Gdis does not include sparsity that is
not supported by G). Lastly  Gdis is converted into a deep neural network by replacing each latent
variable by a neural layer. We call this method B2N (Bayesian to Neural)  as it learns the connectivity
of a deep neural network through Bayesian network structure.

3.2 Constructing a Deep Generative Graph

The key idea of constructing G  the generative graph  is to recursively introduce a new latent layer 
H (n)  after testing n-th order conditional independence in X  and connect it  as a parent  to latent
layers created by subsequent recursive calls that test conditional independence of order n+1. To better
understand why deeper layer represent smaller condition independence sets  consider an ancestral
sampling of the generative graph. First  the values of nodes in the deepest layer  corresponding to
marginal independence  are sampled—each node is sampled independently. In the next layer  nodes
can be sampled independently given the values of deeper nodes. This enables gradually factorizing
(“disentangling”) the joint distribution over X. Hence  approximating the values of latents  H  in
the deepest layer provides us with statistically independent features of the data  which can be fed in
to a single layer linear classiﬁer. Yehezkel & Lerner (2009) introduced an efﬁcient algorithm (RAI)
for constructing a CPDAG over X by a recursive application of conditional independence tests with
increasing condition set sizes. Our algorithm is based on this framework for testing independence in
X and updating the auxiliary graph gX.
Our proposed recursive algorithm for constructing G  is presented in Algorithm 1 (DeepGen) and a
ﬂow chart is shown in the supplementary material Sec. B. The algorithm starts with condition set
n = 0  gX a complete graph (deﬁned over X)  and a set of exogenous nodes  X ex = ∅. The set X ex
is exogenous to gX and consists of parents of X. Note that there are two exit points  lines 4 and 14.
Also  there are multiple recursive calls  lines 8 (within a loop) and 9  leading to multiple parallel
recursive-traces  which will construct multiple generative ﬂows rooted at some deeper layer.
The algorithm starts by testing the exit condition (line 2). It is satisﬁed if there are not enough nodes
in X for a condition set of size n. In this case  the maximal depth is reached and an empty graph is
returned (a layer composed of observed nodes). From this point  the recursive procedure will trace
back  adding latent parent layers.

Algorithm 1: G ←− DeepGen(gX   X  X ex  n)
1 DeepGen (gX   X  X ex  n)

resolution n.

Output: G  a latent structure over X and H

Input: an initial CPDAG gX over endogeneous X & exogenous X ex observed nodes  and a desired

if the maximal indegree of gX (X) is below n + 1 then

G ←−an empty graph over X
return G

(cid:46) exit condition
(cid:46) create a gather layer

2
3
4

5

6

7
8

9

10

11

12

13

14

X ←−IncSeparation(gX   n)
g(cid:48)
{X D  X A1  . . .   X Ak} ←−SplitAutonomous(X  g(cid:48)
for i ∈ {1 . . . k} do

X)

GAi ←− DeepGen(g(cid:48)

X   X Ai  X ex  n + 1)

GD ←− DeepGen(g(cid:48)

X   X D  X ex ∪ {X Ai}k

i=1  n + 1)

G ←− ((cid:83)k

i=1 GAi ) ∪ GD

Create in G  k latent nodes H (n) = {H (n)
Let H A
Set each H (n)
return G

and H (n+1)
to be a parent of {H A

(n+1)
i

(n+1)
i

D

1

i

k }
  . . .   H (n)

∪ H (n+1)

}

D

(cid:46) n-th order independencies
(cid:46) identify autonomies

(cid:46) a recursive call
(cid:46) a recursive call

(cid:46) merge results
(cid:46) create a latent layer

(cid:46) connect

be the sets of parentless nodes in GAi and GD  respectively.

4

The procedure IncSeparation (line 5) disconnects (in gX) conditionally independent variables
in two steps. First  it tests dependency between X ex and X  i.e.  X ⊥⊥ X(cid:48)|S for every connected
pair X ∈ X and X(cid:48) ∈ X ex given a condition set S ⊂ {X ex ∪ X} of size n. Next  it tests
dependency within X  i.e.  Xi ⊥⊥ Xj|S for every connected pair Xi  Xj ∈ X given a condition
set S ⊂ {X ex ∪ X} of size n. After removing the corresponding edges  the remaining edges are
directed by applying two rules (Pearl  2009; Spirtes et al.  2000). First  v-structures are identiﬁed
and directed. Then  edges are continually directed  by avoiding the creation of new v-structures and
directed cycles  until no more edges can be directed. Following the terminology of Yehezkel & Lerner
(2009)  we say that this function increases the graph d-separation resolution from n − 1 to n.
The procedure SplitAutonomous (line 6) identiﬁes autonomous sets  one descendant set  X D 
and k ancestor sets  X A1  . . .   X Ak in two steps. First  the nodes having the lowest topological
order are grouped into X D. Then  X D is removed (temporarily) from gX revealing unconnected
sub-structures. The number of unconnected sub-structures is denoted by k and the nodes set of each
sub-structure is denoted by X Ai (i ∈ {1 . . . k}).
An autonomous set in gX includes all its nodes’ parents (complying with the Markov property) and
therefore a corresponding latent structure can be further learned independently  using a recursive call.
Thus  the algorithm is called recursively and independently for the k ancestor sets (line 8)  and then
for the descendant set  treating the ancestor sets as exogenous (line 9). This recursive decomposition
of X is illustrated in Figure 2. Each recursive call returns a latent structure for each autonomous
set. Recall that each latent structure encodes a generative distribution over the observed variables
where layer H (n+1)  the last added layer (parentless nodes)  is a representation of some input subset
X(cid:48) ⊂ X. Thus  latent variables  H (n)  are introduced as parents of the H (n+1) layers (lines 11–13).
It is important to note that conditional independence is tested only between input variables  X  and
condition sets do not include latent variables. Conditioning on latent variables or testing independence
between them is not required by our approach. A 2-layer toy-example is given in Figure 3.

Figure 2: An example of a recursive decomposition of the observed set  X. Each circle represents
a distinct subset of observed variables (e.g.  X (1)
A1 in different circles represents different subsets).
At n = 0  a single circle represents all the variables. Each set of variables is split into autonomous
ancestors X (n)
D subsets. An arrow indicates a recursive call. Best viewed in
color.

Ai and descendent X (n)

5

 [b]

[c]

[a]
Figure 3: An example of learning a 2-layer generative model. [a] An example Bayesian network
encoding the underlying independencies in X. [b] gX after marginal independence testing (n = 0).
Only A and B are marginally independent (A⊥⊥ B). [c] gX after a recursive call to learn the structure
of nodes {C  D  E} with n = 2 (C ⊥⊥ D|{A  B}). Exit condition is met in subsequent recursive calls
and thus latent variables are added to G at n = 2 [d]  and then at n = 0 [e] (the ﬁnal structure).

[d]

[e]

3.3 Constructing a Discriminative Graph

We now describe how to convert G into a discriminative graph  Gdis  with target variable  Y
(classiﬁcation/regression). First  we construct Ginv  a graphical model that preserves all conditional
dependencies in G but has a different node ordering in which the observed variables  X  have the
highest topological order (parentless)—a stochastic inverse of G. Stuhlmüller et al. (2013) and
Paige & Wood (2016) presented a heuristic algorithm for constructing such stochastic inverses.
However  limiting Ginv to a DAG  although preserving all conditional dependencies  may omit many
independencies and add new edges between layers. Instead  we allow it to be a projection of a latent
structure (Pearl  2009). That is  we assume the presence of additional hidden variables Q that are not
in Ginv but induce dependency (for example  “interactive forks” (Pearl  2009)) among H. For clarity 
we omit these variables from the graph and use bi-directional edges to represent the dependency
induced by them. Ginv is constructed in two steps:

1. Invert the direction of all the edges in G (invert inter-layer connectivity).
2. Connect each pair of latent variables  sharing a common child in G  with a bi-directional

edge.

These steps ensure the preservation of conditional dependence.
Proposition 1. Graph Ginv preserves all conditional dependencies in G (i.e.  G (cid:22) Ginv).
Note that conditional dependencies among X are not required to be preserved in Ginv and Gdis as
these are observed variables (Paige & Wood  2016).
Finally  a discriminative graph Gdis is constructed by replacing the bi-directional dependency relations
in Ginv (induced by Q) with explaining-away relations  which are provided by adding the observed
class variable Y . Node Y is set in Gdis to be the common child of the leaves in Ginv (latents
introduced after testing marginal independencies in X). See an example in Figure 4. This ensures the
preservation of conditional dependency relations in Ginv. That is  Gdis  given Y   can mimic Ginv.

[a]

[b]

[c]

Figure 4: An example of the three graphs constructed by our algorithm: [a] a generative deep latent
structure G  [b] its stochastic inverse Ginv (Stuhlmüller et al.  2013; Paige & Wood  2016)  and [c] a
discriminative structure Gdis (target node Y is added).

Proposition 2. Graph Gdis  conditioned on Y   preserves all conditional dependencies in Ginv
(i.e.  Ginv (cid:22) Gdis|Y ).
It follows that G (cid:22) Ginv (cid:22) Gdis conditioned on Y .
Proposition 3. Graph Gdis  conditioned on Y   preserves all conditional dependencies in G
(i.e.  G (cid:22) Gdis).
Details and proofs for all the propositions are provided in supplementary material Sec. A.

6

CEDBACEDBACEDBACEDBAHCHDCEDBAHAHBHCHDCEDBAHAHBHCHDCEDBAHAHBHCHDCEDBAHAHBHCHDY3.4 Constructing a Feed-Forward Neural Network

(cid:48)

(cid:48)

(cid:48)

X

+ b(cid:48)(cid:1) where sigm(x) = 1/(1 + exp(−x))  X

sigm(cid:0)W
one. They showed that this inﬁnite set can be approximated by(cid:80)N

We construct a neural network based on the connectivity in Gdis. Sigmoid belief networks (Neal 
1992) have been shown to be powerful neural network density estimators (Larochelle & Murray 
2011; Germain et al.  2015). In these networks  conditional probabilities are deﬁned as logistic
regressors. Similarly  for Gdis we may deﬁne for each latent variable H(cid:48) ∈ H  p(H(cid:48) = 1|X
) =
  b(cid:48)) are the
parameters of the neural network. Nair & Hinton (2010) proposed replacing each binary stochastic
node H(cid:48) by an inﬁnite number of copies having the same weights but with decreasing bias offsets by
i=1 sigm(v−i+0.5) ≈ log(1+ev) 
+ b(cid:48). They further approximate this function by max(0  v + ) where  is a zero-
where v = W
centered Gaussian noise. Following these approximations  they provide an approximate probabilistic
interpretation for the ReLU function  max(0  v). As demonstrated by Jarrett et al. (2009) and Nair &
Hinton (2010)  these units are able to learn better features for object classiﬁcation in images.
In order to further increase the representational power  we represent each H(cid:48) by a set of neurons
having ReLU activation functions. That is  each latent variable H(cid:48) in Gdis is represented in the neural
network by a fully-connected layer. Finally  the class node Y is represented by a softmax layer.

= P a(H(cid:48); Gdis)  and (W

(cid:48)

(cid:48)

X

(cid:48)

(cid:48)

4 Experiments

Our structure learning algorithm is implemented using BNT (Murphy  2001) and runs efﬁciently on
a standard desktop CPU (excluding neural network parameter learning). For the learned structures 
all layers were allocated an equal number of neurons. Threshold for independence tests  and the
number of neurons-per-layer were selected by using a validation set. In all the experiments  we used
ReLU activations  ADAM (Kingma & Ba  2015) optimization  batch normalization (Ioffe & Szegedy 
2015)  and dropout (Srivastava et al.  2014) to all the dense layers. All optimization hyper-parameters
that were tuned for the vanilla topologies were also used  without additional tuning  for the learned
structures. In all the experiments  parameter learning was repeated ﬁve times where average and
standard deviation of the classiﬁcation accuracy were recorded. Only test-set accuracy is reported.

4.1 Learning the Structure of the Deepest Layers in Common Topologies

We evaluate the quality of our learned structures using ﬁve image classiﬁcation benchmarks and
seven common topologies (and simpler hand-crafted structures)  which we call “vanilla topologies”.
The benchmarks and vanilla topologies are described in Table 1. Similarly to Li et al. (2017)  we
used the VGG-16 network that was previously modiﬁed and adapted for the CIFAR-10 dataset. This
VGG-16 version contains signiﬁcantly fewer parameters than the original one.

Table 1: Benchmarks and vanilla topologies used in our experiments. MNIST-Man and SVHN-Man
topologies were manually created by us. MNIST-Man has two convolutional layer (32 and 64 ﬁlters
each) and one dense layer with 128 neurons. SVHN-Man was created as a small network reference
having reasonable accuracy (Acc.) compared to Maxout-NiN.

Dataset
MNIST (LeCun et al.  1998) A MNIST-Man

Topology

Id.

Vanilla Topology

Description
32-64-FC:128

SVHN (Netzer et al.  2011)

CIFAR 10 (Krizhevsky &
Hinton  2009)
CIFAR 100 (Krizhevsky &
Hinton  2009)
ImageNet (Deng et al.  2009) G

B
C
D
E
F

Maxout NiN (Chang & Chen  2015)
16-16-32-32-64-FC:256
SVHN-Man
(Simonyan & Zisserman  2014)
(Zagoruyko & Komodakis  2016)
(Simonyan & Zisserman  2014)

VGG-16
WRN-40-4
VGG-16

Size
Acc.
127K 99.35
1.6M 98.10
105K 97.10
15M 92.32
9M
95.09
15M 68.86

AlexNet

(Krizhevsky et al.  2012)

61M 57.20

7

In preliminary experiments we found that  for SVHN and ImageNet  a small subset of the training
data is sufﬁcient for learning the structure. As a result  for SVHN only the basic training data is used
(without the extra data)  i.e.  13% of the available training data  and for ImageNet 5% of the training
data is used. Parameters were optimized using all of the training data.
Convolutional layers are powerful feature extractors for images exploiting spatial smoothness proper-
ties  translational invariance and symmetry. We therefore evaluate our algorithm by using the ﬁrst
convolutional layers of the vanilla topologies as “feature extractors” (mostly below 50% of the vanilla
network size) and then learning a deep structure  “learned head”  from their output. That is  the
deepest layers of the vanilla network  “vanilla head”  is removed and replaced by a structure which is
learned  in an unsupervised manner  by our algorithm2. This results in a new architecture which we
train end-to-end. Finally  a softmax layer is added and the entire network parameters are optimized.
First  we evaluate the accuracy of the learned structure as a function of the number of parameters
and compare it to a densely connected network (fully connected layers) having the same depth and
size (Figure 5). For SVHN  we used the Batch Normalized Maxout Network-in-Network topology
(Chang & Chen  2015) and removed the deepest layers starting from the output of the second NiN
block (MMLP-2-2). For CIFAR-10  we used the VGG-16 and removed the deepest layers starting
from the output of conv.7 layer. It is evident that accuracies of the learned structures are signiﬁcantly
higher (error bars represent 2 standard deviations) than those produced by a set of fully connected
layers  especially in cases where the network is limited to a small number of parameters.

y
c
a
r
u
c
c
a
T
S
I
N
M

99

98

97

96

0

96

94

92

90

y
c
a
r
u
c
c
a
N
H
V
S

fully connected
learned structure

y
c
a
r
u
c
c
a

0
1
-
R
A
F
I
C

90

80

70

fully connected
learned structure

fully connected
learned structure

1

0.5

2
number of parameters (×105)

1.5

1.08

1.1

1.12

number of parameters (×106)

7.67

7.66
number of parameters (×106)

7.68

7.69

Figure 5: Classiﬁcation accuracy of MNIST  SVHN  and CIFAR-10  as a function of network size.
Error bars indicate two standard deviations.

Next  in Figure 6 and Table 2 we provide a summary of network sizes and classiﬁcation accuracies 
achieved by replacing the deepest layers of common topologies (vanilla) with a learned structure. In
all the cases  the size of the learned structure is signiﬁcantly smaller than that of the vanilla topology.

Figure 6: A comparison between the vanilla and our learned structure (B2N)  in terms of normalized
number of parameters. The ﬁrst few layers of the vanilla topology are used for feature extraction.
Stacked bars refer to either the vanilla or our learned structure. The total number of parameters of the
vanilla network is indicated on top of each stacked bar.

2We also learned a structure for classifying MNIST digits directly from image pixels  without using convolu-
tional layers for feature extraction. The resulting network structure (Figure 1)  achieves an accuracy of 99.07% 
whereas a network with 3 fully-connected layers achieves 98.75%.

8

ABCDEFG00.51featureextractionvanillaheadlearnedhead61M15M9M15M105K1.6M127Knormalizednumberofparameters4.2 Comparison to Other Methods

Our structure learning algorithm runs efﬁciently on a standard desktop CPU  while providing struc-
tures with competitive classiﬁcation accuracies and network sizes. First  we compare our method to
the NAS algorithm (Zoph & Le  2016). NAS achieves for CIFAR-10 an error rate of 5.5% with a
network of size 4.2M. Our method  using the feature extraction of the WRN-40-4 network  achieves
this same error rate with a 26% smaller network (3.1M total size). Using the same feature extraction 
the lowest classiﬁcation error rate achieved by our algorithm for CIFAR 10 is 4.58% with a network
of size 6M whereas the NAS algorithm achieves an error rate of 4.47% with a network of size 7.1M.
Recall that the NAS algorithm requires training thousands of networks using hundreds of GPUs 
which is impractical for most real-world applications.
When compared to recent pruning methods  which focus on reducing the number of parameters in
a pre-trained network  our method demonstrates state-of-the-art reduction in parameters. Recently
reported results are summarized in Table 3. It is important to note that although these methods prune
all the network layers  whereas our method only replaces the network head  our method was found
signiﬁcantly superior. Moreover  pruning can be applied to the feature extraction part of the network
which may further improve parameter reduction.

Table 2: Parameter reduction ratio
(vanilla size/learned size) and differ-
ence in classiﬁcation accuracy (Acc.
Diff.=learned−vanilla  higher is better).
“Full”=feature extration+head.

Table 3: Parameter reduction ratio (vanilla/learned size)
compared to recent pruning methods (reducing the size
of a pre-trained network with minimal accuracy degra-
dation). Results indicated by “acc. deg.” correspond to
accuracy degradation after pruning.

Param. Reduc.
Head
Full
4.2×
2.7×
1.4× 10.0×
2.5×
3.5×
7.0× 28.3×
1.5×
2.8×
7.7× 53.2×
13.3× 23.0×

Id.
Acc. Diff.
A +0.10 ± 0.04
B −0.40 ± 0.05
C −0.86 ± 0.05
D +0.29 ± 0.14
E +0.33 ± 0.14
F +0.05 ± 0.17
G +0.00 ± 0.03

5 Conclusions

Network
VGG-16
(CIFAR-10) Ayinde & Zurada (2018)

Method
Li et al. (2017)

AlexNet
(ImageNet)

Ding et al. (2018)
Huang et al. (2018)
B2N (our)
Denton et al. (2014)
Yang et al. (2015)
Han et al. (2015  2016)
Manessi et al. (2017)
B2N (our)

Reduction
3×
4.6×
(acc. deg.) 5.4×
(acc. deg.) 6×
7×
5×
3.2×
9×
(acc. deg.) 12×
13.3×

We presented a principled approach for learning the structure of deep neural networks. Our proposed
algorithm learns in an unsupervised manner and requires small computational cost. The resulting
structures encode a hierarchy of independencies in the input distribution  where a node in one layer
may connect to another node in any deeper layer  and network depth is determined automatically.
We demonstrated that our algorithm learns small structures  and maintains classiﬁcation accuracies
for common image classiﬁcation benchmarks. It is also demonstrated that while convolution layers
are very useful at exploiting domain knowledge  such as spatial smoothness  translational invariance 
and symmetry  in some cases  they are outperformed by a learned structure for the deeper layers.
Moreover  while the use of common topologies (meta-architectures)  for a variety of classiﬁcation
tasks is computationally inefﬁcient  we would expect our approach to learn smaller and more accurate
networks for each classiﬁcation task  uniquely.
As only unlabeled data is required for learning the structure  we expect our approach to be practical
for many domains  beyond image classiﬁcation  such as knowledge discovery  and plan to explore
the interpretability of the learned structures. Casting the problem of learning the connectivity of
deep neural network as a Bayesian network structure learning problem  enables the development
of new principled and efﬁcient approaches. This can lead to the development of new topologies
and connectivity models  and can provide a greater understanding of the domain. One possible
extension to our work which we plan to explore  is learning the connectivity between feature maps in
convolutional layers.

9

References
Adams  Ryan  Wallach  Hanna  and Ghahramani  Zoubin. Learning the structure of deep sparse graphical
models. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics  pp.
1–8  2010.

Ayinde  Babajide O. and Zurada  Jacek M. Building efﬁcient convnets using redundant feature pruning. In

Workshop Track of the International Conference on Learning Representations (ICLR)  2018.

Baker  Bowen  Gupta  Otkrist  Naik  Nikhil  and Raskar  Ramesh. Designing neural network architectures using

reinforcement learning. arXiv preprint arXiv:1611.02167  2016.

Chang  Jia-Ren and Chen  Yong-Sheng. Batch-normalized maxout network in network. arXiv preprint

arXiv:1511.02583  2015.

Chen  Tianqi  Goodfellow  Ian  and Shlens  Jonathon. Net2net: Accelerating learning via knowledge transfer.

arXiv preprint arXiv:1511.05641  2015.

Chickering  David Maxwell. Optimal structure identiﬁcation with greedy search. Journal of machine learning

research  3(Nov):507–554  2002.

Collobert  Ronan  Weston  Jason  Bottou  Léon  Karlen  Michael  Kavukcuoglu  Koray  and Kuksa  Pavel.
Natural language processing (almost) from scratch. Journal of Machine Learning Research  12(Aug):
2493–2537  2011.

Cooper  Gregory F and Herskovits  Edward. A Bayesian method for the induction of probabilistic networks

from data. Machine learning  9(4):309–347  1992.

Deng  Jia  Dong  Wei  Socher  Richard  Li  Li-Jia  Li  Kai  and Fei-Fei  Li. Imagenet: A large-scale hierarchical
image database. In Computer Vision and Pattern Recognition  2009. CVPR 2009. IEEE Conference on  pp.
248–255. IEEE  2009.

Denton  Emily L  Zaremba  Wojciech  Bruna  Joan  LeCun  Yann  and Fergus  Rob. Exploiting linear structure
within convolutional networks for efﬁcient evaluation. In Advances in Neural Information Processing Systems 
pp. 1269–1277  2014.

Ding  Xiaohan  Ding  Guiguang  Han  Jungong  and Tang  Sheng. Auto-balanced ﬁlter pruning for efﬁcient
convolutional neural networks. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence (AAAI) 
2018.

Donahue  Jeff  Jia  Yangqing  Vinyals  Oriol  Hoffman  Judy  Zhang  Ning  Tzeng  Eric  and Darrell  Trevor.
Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on
Machine Learning  volume 32  pp. 647–655  2014.

Germain  Mathieu  Gregor  Karol  Murray  Iain  and Larochelle  Hugo. Made: Masked autoencoder for

distribution estimation. In ICML  pp. 881–889  2015.

Girshick  Ross  Donahue  Jeff  Darrell  Trevor  and Malik  Jitendra. Rich feature hierarchies for accurate object
detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern
recognition  pp. 580–587  2014.

Graves  Alex and Schmidhuber  Jürgen. Framewise phoneme classiﬁcation with bidirectional lstm and other

neural network architectures. Neural Networks  18(5):602–610  2005.

Han  Song  Pool  Jeff  Tran  John  and Dally  William. Learning both weights and connections for efﬁcient

neural networks. In Advances in Neural Information Processing Systems  pp. 1135–1143  2015.

Han  Song  Mao  Huizi  and Dally  William J. Deep compression: Deep compression: Compressing deep
neural networks with pruning  trained quantization and huffman coding. In Proceedings of the International
Conference on Learning Representations (ICLR)  2016.

He  Kaiming  Zhang  Xiangyu  Ren  Shaoqing  and Sun  Jian. Deep residual learning for image recognition. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp. 770–778  2016.

Hinton  Geoffrey  Vinyals  Oriol  and Dean  Jeff. Distilling the knowledge in a neural network. arXiv preprint

arXiv:1503.02531  2015.

Hinton  Geoffrey E  Osindero  Simon  and Teh  Yee-Whye. A fast learning algorithm for deep belief nets. Neural

computation  18(7):1527–1554  2006.

10

Huang  Gao  Liu  Zhuang  Weinberger  Kilian Q  and van der Maaten  Laurens. Densely connected convolutional

networks. arXiv preprint arXiv:1608.06993  2016.

Huang  Qiangui  Zhou  Kevin  You  Suya  and Neumann  Ulrich. Learning to prune ﬁlters in convolutional

neural networks. arXiv preprint arXiv:1801.07365  2018.

Ioffe  Sergey and Szegedy  Christian. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. In International Conference on Machine Learning  pp. 448–456  2015.

Jarrett  Kevin  Kavukcuoglu  Koray  LeCun  Yann  et al. What is the best multi-stage architecture for object
recognition? In Computer Vision  2009 IEEE 12th International Conference on  pp. 2146–2153. IEEE  2009.

Kingma  Diederik and Ba  Jimmy. Adam: A method for stochastic optimization.

International Conference on Learning Representations (ICLR)  2015.

In Proceedings of the

Krizhevsky  Alex and Hinton  Geoffrey. Learning multiple layers of features from tiny images. 2009.

Krizhevsky  Alex  Sutskever  Ilya  and Hinton  Geoffrey E. Imagenet classiﬁcation with deep convolutional

neural networks. In Advances in neural information processing systems  pp. 1097–1105  2012.

Larochelle  Hugo and Murray  Iain. The neural autoregressive distribution estimator. In AISTATS  volume 1  pp.

2  2011.

LeCun  Yann  Bottou  Léon  Bengio  Yoshua  and Haffner  Patrick. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

Li  Hao  Kadav  Asim  Durdanovic  Igor  Samet  Hanan  and Graf  Hans Peter. Pruning ﬁlters for efﬁcient

convnets. In Proceedings of the International Conference on Learning Representations (ICLR)  2017.

Liu  Baoyuan  Wang  Min  Foroosh  Hassan  Tappen  Marshall  and Pensky  Marianna. Sparse convolutional
neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp.
806–814  2015.

Long  Mingsheng  Cao  Yue  Wang  Jianmin  and Jordan  Michael. Learning transferable features with deep

adaptation networks. In International Conference on Machine Learning  pp. 97–105  2015.

Manessi  Franco  Rozza  Alessandro  Bianco  Simone  Napoletano  Paolo  and Schettini  Raimondo. Automated

pruning for deep neural network compression. arXiv preprint arXiv:1712.01721  2017.

Miconi  Thomas. Neural networks with differentiable structure. arXiv preprint arXiv:1606.06216  2016.

Miikkulainen  Risto  Liang  Jason  Meyerson  Elliot  Rawal  Aditya  Fink  Dan  Francon  Olivier  Raju  Bala 
Navruzyan  Arshak  Duffy  Nigel  and Hodjat  Babak. Evolving deep neural networks. arXiv preprint
arXiv:1703.00548  2017.

Murphy  K. The Bayes net toolbox for Matlab. Computing Science and Statistics  33:331–350  2001.

Nair  Vinod and Hinton  Geoffrey E. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings

of the 27th international conference on machine learning (ICML-10)  pp. 807–814  2010.

Neal  Radford M. Connectionist learning of belief networks. Artiﬁcial intelligence  56(1):71–113  1992.

Negrinho  Renato and Gordon  Geoff. Deeparchitect: Automatically designing and training deep architectures.

arXiv preprint arXiv:1704.08792  2017.

Netzer  Yuval  Wang  Tao  Coates  Adam  Bissacco  Alessandro  Wu  Bo  and Ng  Andrew Y. Reading digits in
natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised
feature learning  volume 2011  pp. 5  2011.

Paige  Brooks and Wood  Frank. Inference networks for sequential Monte Carlo in graphical models. In

Proceedings of the 33rd International Conference on Machine Learning  volume 48 of JMLR  2016.

Pearl  Judea. Causality: Models  Reasoning  and Inference. Cambridge university press  second edition  2009.

Raiko  Tapani  Valpola  Harri  and LeCun  Yann. Deep learning made easier by linear transformations in

perceptrons. In Artiﬁcial Intelligence and Statistics  pp. 924–932  2012.

Real  Esteban  Moore  Sherry  Selle  Andrew  Saxena  Saurabh  Suematsu  Yutaka Leon  Le  Quoc  and Kurakin 

Alex. Large-scale evolution of image classiﬁers. arXiv preprint arXiv:1703.01041  2017.

11

Simonyan  Karen and Zisserman  Andrew. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

Smith  Leslie N  Hand  Emily M  and Doster  Timothy. Gradual dropin of layers to train very deep neural
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pp. 4763–
4771  2016.

Smithson  Sean C  Yang  Guang  Gross  Warren J  and Meyer  Brett H. Neural networks designing neural net-
works: Multi-objective hyper-parameter optimization. In Computer-Aided Design (ICCAD)  2016 IEEE/ACM
International Conference on  pp. 1–8. IEEE  2016.

Spirtes  P.  Glymour  C.  and Scheines  R. Causation  Prediction and Search. MIT Press  2nd edition  2000.

Srivastava  Nitish  Hinton  Geoffrey  Krizhevsky  Alex  Sutskever  Ilya  and Salakhutdinov  Ruslan. Dropout:
A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research  15:
1929–1958  2014. URL http://jmlr.org/papers/v15/srivastava14a.html.

Stuhlmüller  Andreas  Taylor  Jacob  and Goodman  Noah. Learning stochastic inverses. In Advances in neural

information processing systems  pp. 3048–3056  2013.

Szegedy  Christian  Liu  Wei  Jia  Yangqing  Sermanet  Pierre  Reed  Scott  Anguelov  Dragomir  Erhan  Dumitru 
Vanhoucke  Vincent  and Rabinovich  Andrew. Going deeper with convolutions. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition  pp. 1–9  2015.

Yang  Zichao  Moczulski  Marcin  Denil  Misha  de Freitas  Nando  Smola  Alex  Song  Le  and Wang  Ziyu.
Deep fried convnets. In Proceedings of the IEEE International Conference on Computer Vision  pp. 1476–
1483  2015.

Yehezkel  Raanan and Lerner  Boaz. Bayesian network structure learning by recursive autonomy identiﬁcation.

Journal of Machine Learning Research  10(Jul):1527–1570  2009.

Zagoruyko  Sergey and Komodakis  Nikos. Wide residual networks. arXiv preprint arXiv:1605.07146  2016.

Zoph  Barret and Le  Quoc V. Neural architecture search with reinforcement learning. arXiv preprint

arXiv:1611.01578  2016.

Zoph  Barret  Vasudevan  Vijay  Shlens  Jonathon  and Le  Quoc V. Learning transferable architectures for

scalable image recognition. arXiv preprint arXiv:1707.07012  2017.

12

,Bo Dai
Bo Xie
Niao He
Yingyu Liang
Anant Raj
Maria-Florina Balcan
Le Song
Raanan Rohekar
Shami Nisimov
Yaniv Gurwicz
Guy Koren
Gal Novik