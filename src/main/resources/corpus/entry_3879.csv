2017,On Fairness and Calibration,The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be "fair." In this paper  we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups)  and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings  which extend and generalize existing results  are empirically confirmed on several datasets.,OnFairnessandCalibrationGeoffPleiss∗ ManishRaghavan∗ FelixWu JonKleinberg KilianQ.WeinbergerCornellUniversity DepartmentofComputerScience{geoff manish kleinber}@cs.cornell.edu {fw245 kwq4}@cornell.eduAbstractThemachinelearningcommunityhasbecomeincreasinglyconcernedwiththepotentialforbiasanddiscriminationinpredictivemodels.Thishasmotivatedagrowinglineofworkonwhatitmeansforaclassiﬁcationproceduretobe“fair.”Inthispaper weinvestigatethetensionbetweenminimizingerrordisparityacrossdifferentpopulationgroupswhilemaintainingcalibratedprobabilityestimates.Weshowthatcalibrationiscompatibleonlywithasingleerrorconstraint(i.e.equalfalse-negativesratesacrossgroups) andshowthatanyalgorithmthatsatisﬁesthisrelaxationisnobetterthanrandomizingapercentageofpredictionsforanexistingclassiﬁer.Theseunsettlingﬁndings whichextendandgeneralizeexistingresults areempiricallyconﬁrmedonseveraldatasets.1IntroductionRecently therehasbeengrowingconcernabouterrorsofmachinelearningalgorithmsinsensitivedomains–includingcriminaljustice onlineadvertising andmedicaltesting[33]–whichmaysystematicallydiscriminateagainstparticulargroupsofpeople[2 4 8].Arecenthigh-proﬁleexampleoftheseconcernswasraisedbythenewsorganizationProPublica whostudiedarisk-assessmenttoolthatiswidelyusedinthecriminaljusticesystem.Thistoolassignstoeachcriminaldefendantanestimatedprobabilitythattheywillcommitafuturecrime.ProPublicafoundthattheriskestimatesassignedtodefendantswhodidnotcommitfuturecrimeswereonaveragehigheramongAfrican-AmericandefendantsthanCaucasiandefendants[1].Thisisaformoffalse-positiveerror andinthiscaseitdisproportionatelyaffectedAfrican-Americandefendants.Tomitigateissuessuchasthese themachinelearningcommunityhasproposeddifferentframeworksthatattempttoquantifyfairnessinclassiﬁcation[2 4 8 19 26 34 37].ArecentandparticularlynoteworthyframeworkisEqualizedOdds[19](alsoreferredtoasDisparateMistreatment[37]) 1whichconstrainsclassiﬁcationalgorithmssuchthatnoerrortype(false-positiveorfalse-negative)disproportionatelyaffectsanypopulationsubgroup.Thisnotionofnon-discriminationisfeasibleinmanysettings andresearchershavedevelopedtractablealgorithmsforachievingit[17 19 34 37].Whenrisktoolsareusedinpractice akeygoalisthattheyarecalibrated:ifwelookatthesetofpeoplewhoreceiveapredictedprobabilityofp wewouldlikeapfractionofthemembersofthissettobepositiveinstancesoftheclassiﬁcationproblem[11].Moreover ifweareconcernedaboutfairnessbetweentwogroupsG1andG2(e.g.African-Americandefendantsandwhitedefendants)thenwewouldlikethiscalibrationconditiontoholdsimultaneouslyforthesetofpeoplewithineachofthesegroupsaswell[16].Calibrationisacrucialconditionforrisktoolsinmanysettings.Ifarisktoolforevaluatingdefendantswerenotcalibratedwithrespecttogroupsdeﬁnedbyrace forexample thenaprobabilityestimateofpcouldcarrydifferentmeaningforAfrican-Americanandwhitedefendants andhencethetoolwouldhavetheunintendedandhighlyundesirableconsequenceofincentivizingjudgestotakeraceintoaccountwheninterpretingitspredictions.Despitethe∗Equalcontribution alphebeticalorder.1Fortheremainderofthepaper wewilluseEqualizedOddstorefertothisnotionofnon-discrimination.31stConferenceonNeuralInformationProcessingSystems(NIPS2017) LongBeach CA USA.importanceofcalibrationasaproperty ourunderstandingofhowitinteractswithotherfairnesspropertiesislimited.Weknowfromrecentworkthat exceptinthemostconstrainedcases itisimpossibletoachievecalibrationwhilealsosatisfyingEqualizedOdds[8 26].However wedonotknowhowbesttoachieverelaxationsoftheseguaranteesthatarefeasibleinpractice.Ourgoalistofurtherinvestigatetherelationshipbetweencalibrationanderrorrates.WeshowthateveniftheEqualizedOddsconditionsarerelaxedsubstantially–requiringonlythatweightedsumsofthegrouperrorratesmatch–itisstillproblematictoalsoenforcecalibration.Weprovidenecessaryandsufﬁcientconditionsunderwhichthiscalibratedrelaxationisfeasible.Whenfeasible ithasauniqueoptimalsolutionthatcanbeachievedthroughpost-processingofexistingclassiﬁers.Moreover weprovideasimplepost-processingalgorithmtoﬁndthissolution:withholdingpredic-tiveinformationforrandomlychoseninputstoachieveparityandpreservecalibration.However thissimplepost-processingmethodisfundamentallyunsatisfactory:althoughthepost-processedpredictionsofourinformation-withholdingalgorithmare“fair”inexpectation mostpractitionerswouldobjecttothefactthatanon-trivialportionoftheindividualpredictionsarewithheldasaresultofcointosses–especiallyinsensitivesettingssuchashealthcareorcriminaljustice.Theoptimalityofthisalgorithmthushastroublingimplicationsandshowsthatcalibrationanderror-ratefairnessareinherentlyatodds(evenbeyondtheinitialresultsby[8]and[26]).Finally weevaluatethesetheoreticalﬁndingsempirically comparingcalibratednotionsofnon-discriminationagainstthe(uncalibrated)EqualizedOddsframeworkonseveraldatasets.Theseexperimentsfurthersupportourconclusionthatcalibrationanderror-rateconstraintsareinmostcasesmutuallyincompatiblegoals.Inpracticalsettings itmaybeadvisabletochooseonlyoneofthesegoalsratherthanattemptingtoachievesomerelaxednotionofboth.2RelatedWorkCalibratedprobabilityestimatesareconsiderednecessaryforempiricalriskanalysistools[4 10 12 16].Inpracticalapplications uncalibratedprobabilityestimatescanbemisleadinginthesensethattheenduseroftheseestimateshasanincentivetomistrust(andthereforepotentiallymisuse)them.Wenotehoweverthatcalibrationdoesnotremoveallpotentialformisuse astheenduser’sbiasesmightcauseherorhimtotreatestimatesdifferentlybasedongroupmembership.Thereareseveralpost-processingmethodsforproducingcalibratedoutputsfromclassiﬁcationalgorithms.Forexample PlattScaling[31]passesoutputsthroughalearnedsigmoidfunction transformingthemintocalibratedprobabilities.HistogramBinningandIsotonicRegression[35]learnageneralmonotonicfunctionfromoutputstoprobabilities.See[30]and[18]forempiricalcomparisonsofthesemethods.EqualizedOdds[19] alsoreferredtoasDisparateMistreatment[37] ensuresthatnoerrortypedisproportionatelyaffectsanyparticulargroup.Hardtetal.[19]provideapost-processingtechniquetoachievethisframework whileZafaretal.[37]introduceoptimizationconstraintstoachievenon-discriminationattrainingtime.Recently thisframeworkhasreceivedsigniﬁcantattentionfromthealgorithmicfairnesscommunity.Researchershavefoundthatitisincompatiblewithothernotionsoffairness[8 9 26].Additionally Woodworthetal.[34]demonstratethat undercertainassumptions post-processingmethodsforachievingnon-discriminationmaybesuboptimal.Alternativefairnessframeworksexistandarecontinuouslyproposed.Wehighlightseveraloftheseworks thoughbynomeansofferacomprehensivelist.(Morethoroughreviewscanbefoundin[2 4 32]).Ithasbeenshownthat undermostframeworksoffairness thereisatrade-offbetweenalgorithmicperformanceandnon-discrimination[4 9 19 39].SeveralworksapproachfairnessthroughthelensofStatisticalParity[6 7 14 20 22 23 29 38].Underthisdeﬁnition groupmembershipshouldnotaffectthepredictionofaclassiﬁer i.e.membersofdifferentgroupsshouldhavethesameprobabilityofreceivingapositive-classprediction.However ithasbeenarguedthatStatisticalParitymaynotbeapplicableinmanyscenarios[8 13 19 26] asitattemptstoguaranteeequalrepresentation.Forexample itisinappropriateincriminaljustice wherebaseratesdifferacrossdifferentgroups.ArelatednotionisDisparateImpact[15 36] whichstatesthatthepredictionratesforanytwogroupsshouldnotdifferbymorethan80%(anumbermotivatedbylegalrequirements).Dworketal.[13]introduceanotionoffairnessbasedontheideathatsimilarindividualsshouldreceivesimilaroutcomes thoughitchallengingtoachievethisnotioninpractice.Fairnesshasalsobeenconsideredinonlinelearning[21 24] unsupervisedlearning[5] andcausalinference[25 27].23ProblemSetupThesetupofourframeworkmostfollowstheEqualizedOddsframework[19 37];however weextendtheirframeworkforusewithprobabilisticclassiﬁers.LetP⊂Rk×{0 1}betheinputspaceofabinaryclassiﬁcationtask.Inourcriminaljusticeexample (x y)∼Prepresentsaperson withxrepresentingtheindividual’shistoryandyrepresentingwhetherornotthepersonwillcommitanothercrime.Additionally weassumethepresenceoftwogroupsG1 G2⊂P whichrepresentdisjointpopulationsubsets suchasdifferentraces.Weassumethatthegroupshavedifferentbaseratesµt orprobabilitiesofbelongingtothepositiveclass:µ1=P(x y)∼G1[y=1]6=P(x y)∼G2[y=1]=µ2.Finally leth1 h2:Rk→[0 1]bebinaryclassiﬁers whereh1classiﬁessamplesfromG1andh2classiﬁessamplesfromG2.2Eachclassiﬁeroutputstheprobabilitythatagivensamplexbelongstothepositiveclass.ThenotionofEqualizedOddsnon-discriminationisbasedonthefalse-positiveandfalse-negativeratesforeachgroup whichwegeneralizehereforusewithprobabilisticclassiﬁers:Deﬁnition1.Thegeneralizedfalse-positiverateofclassiﬁerhtforgroupGtiscfp(ht)=E(x y)∼Gt(cid:2)ht(x)|y=0(cid:3).Similarly thegeneralizedfalse-negativerateofclassiﬁerhtiscfn(ht)=E(x y)∼Gt(cid:2)(1−ht(x))|y=1(cid:3).Iftheclassiﬁerweretooutputeither0or1 thisrepresentsthestandardnotionsoffalse-positiveandfalse-negativerates.WenowdeﬁnetheEqualizedOddsframework(generalizedforprobabilisticclassiﬁers) whichaimstoensurethaterrorsofagiventypearenotbiasedagainstanygroup.Deﬁnition2(ProbabilisticEqualizedOdds).Classiﬁersh1andh2exhibitEqualizedOddsforgroupsG1andG2ifcfp(h1)=cfp(h2)andcfn(h1)=cfn(h2).CalibrationConstraints.Asstatedintheintroduction thesetwoconditionsdonotnecessarilypreventdiscriminationiftheclassiﬁerpredictionsdonotrepresentwell-calibratedprobabilities.Recallthatcalibrationintuitivelysaysthatprobabilitiesshouldcarrysemanticmeaning:ifthereare100peopleinG1forwhomh1(x)=0.6 thenweexpect60ofthemtobelongtothepositiveclass.Deﬁnition3.Aclassiﬁerhtisperfectlycalibratedif∀p∈[0 1] P(x y)∼Gt(cid:2)y=1|ht(x)=p(cid:3)=p.Itiscommonlyacceptedamongstpractitionersthatbothclassiﬁersh1andh2shouldbecalibratedwithrespecttogroupsG1andG2topreventdiscrimination[4 10 12 16].Intuitively thispreventstheprobabilityscoresfromcarryinggroup-speciﬁcinformation.Unfortunately Kleinbergetal.[26](aswellas[8] inabinarysetting)provethataclassiﬁercannotachievebothcalibrationandEqualizedOdds eveninanapproximatesense exceptinthemosttrivialofcases.3.1GeometricCharacterizationofConstraintsWenowwillcharacterizethecalibrationanderror-rateconstraintswithsimplegeometricintuitions.Throughouttherestofthispaper allofourresultscanbeeasilyderivedfromthisinterpretation.Webeginbydeﬁningtheregionofclassiﬁerswhicharetrivial orthosethatoutputaconstantvalueforallinputs(i.e.hc(x)=c where0≤c≤1isaconstant).Wecanvisualizetheseclassiﬁersonagraphwithgeneralizedfalse-positiveratesononeaxisandgeneralizedfalse-negativesontheother.Itfollowsfromthedeﬁnitionsofgeneralizedfalse-positive/false-negativeratesandcalibrationthatalltrivialclassiﬁershlieonthediagonaldeﬁnedbycfp(h)+cfn(h)=1(Figure1a).Therefore allclassiﬁersthatare“betterthanrandom”mustliebelowthisdiagonalinfalse-positive/false-negativespace(thegraytriangleintheﬁgure).Anyclassiﬁerthatliesabovethediagonalperforms“worsethanrandom ”aswecanﬁndapointonthetrivialclassiﬁerdiagonalwithlowerfalse-positiveandfalse-negativerates.NowwewillcharacterizethesetofcalibratedclassiﬁersforgroupsG1andG2 whichwedenoteasH∗1andH∗2.Kleinbergetal.showthatthegeneralizedfalse-positiveandfalse-negativeratesofacalibratedclassiﬁerarelinearlyrelatedbythebaserateofthegroup:3cfn(ht)=(1−µt)/µtcfp(ht).(1)2Inpractice h1andh2canbetrainedjointly(i.e.theyarethesameclassiﬁer).3Throughoutthisworkwewilltreatthecalibrationconstraintasholdingexactly;however ourresultsgeneralizetoapproximatesettingsaswell.SeetheSupplementaryMaterialsformoredetails.311Generalized FP RateGeneralized FN RateH⇤1H⇤2µ2µ11µ21µ10hµ2hµ1(a)Possiblecal.classi-ﬁersH∗1 H∗2(blue/red).11Generalized FP RateGeneralized FN Rate0h2h1(b)Satisfyingcal.andequalF.P.rates.11Generalized FP RateGeneralized FN Rate0h2h1(c)Satisfyingcal.andequalF.N.rates.11Generalized FP RateGeneralized FN Rate0h2h1(d)Satisfyingcal.andageneralconstraint.Figure1:Calibration trivialclassiﬁers andequal-costconstraints–plottedinthefalse-pos./false-neg.plane.H∗1 H∗2arethesetofcal.classiﬁersforthetwogroups andhµ1 hµ2aretrivialclassiﬁers.11Generalized FP RateGeneralized FN RateH⇤1H⇤20g=1g=2g=3(a)Level-ordercurvesofcost.Lowcostimplieslowerrorrates.11Generalized FP RateGeneralized FN Rate0h2h1hµ2˜h2(b)Usually thereisacal-ibratedclassiﬁer˜h2withthesamecostofh1.11Generalized FP RateGeneralized FN Rate0h2h1(c)Cal.andequal-costareincompatibleifh1hashigherror.11Generalized FP RateGeneralized FN Rate0h2hµ2(d)Possiblecal.classi-ﬁersforG2(boldred)bymixingh2andhµ2.Figure2:Calibration-PreservingParitythroughinterpolation.Inotherwords h1liesonalinewithslope(1−µ1)/µ1andh2liesonalinewithslope(1−µ2)/µ2(Figure1a).Thelowerendpointofeachlineistheperfectclassiﬁer whichassignsthecorrectpredictionwithcompletecertaintytoeveryinput.Theupperendpointisatrivialclassiﬁer asnocalibratedclassiﬁercanperform“worsethanrandom”(seeLemma3inSectionS2).TheonlytrivialclassiﬁerthatsatisﬁesthecalibrationconditionforagroupGtistheonethatoutputsthebaserateµt.Wewillrefertohµ1andhµ2asthetrivialclassiﬁers calibratedforgroupsG1andG2respectively.Itfollowsfromthedeﬁnitionsthatcfp(hµ1)=µ1andcfn(hµ1)=1−µ1 andlikewiseforhµ2.Finally itisworthnotingthatforcalibratedclassiﬁers alowerfalse-positiveratenecessarilycorrespondstoalowerfalse-negativerateandvice-versa.Inotherwords foragivenbaserate a“better”calibratedclassiﬁerliesclosertotheoriginonthelineofcalibratedclassiﬁers.ImpossibilityofEqualizedOddswithCalibration.Withthisgeometricintuition wecanprovideasimpliﬁedproofofthemainimpossibilityresultfrom[26]:Theorem(ImpossibilityResult[26]).Leth1andh2beclassiﬁersforgroupsG1andG2withµ16=µ2.h1andh2satisfytheEqualizedOddsandcalibrationconditionsifandonlyifh1andh2areperfectpredictors.Intuitively thethreeconditionsdeﬁneasetofclassiﬁerswhichisoverconstrained.EqualizedOddsstipulatesthattheclassiﬁersh1andh2mustlieonthesamecoordinateinthefalse-positive/false-negativeplane.Ash1mustlieonthebluelineofcalibratedclassiﬁersforH∗1andh2ontheredlineH∗2theycanonlysatisfyEOattheuniqueintersectionpoint—theorigin(andlocationoftheperfectclassiﬁer).Thisimpliesthatunlessthetwoclassiﬁersachieveperfectaccuracy wemustrelaxtheEqualizedOddsconditionsifwewanttomaintaincalibration.4RelaxingEqualizedOddstoPreserveCalibrationInthissection weshowthatasubstantiallysimpliﬁednotionofEqualizedOddsiscompatiblewithcalibration.Weintroduceageneralrelaxationthatseekstosatisfyasingleequal-costconstraintwhilemaintainingcalibrationforeachgroupGt.WebeginwiththeobservationthatEqualized4Oddssetsconstraintstoequalizefalse-positivescfp(ht)andfalse-negativescfn(ht).Tocaptureandgeneralizethis wedeﬁneacostfunctiongttobealinearfunctionincfp(ht)andcfn(ht)witharbitrarydependenceonthegroup’sbaserateµt.Moreformally acostfunctionforgroupGtisgt(ht)=atcfp(ht)+btcfn(ht)(2)whereatandbtarenon-negativeconstantsthatarespeciﬁctoeachgroup(andthusmaydependonµt):seeFigure1d.Wealsomaketheassumptionthatforanyµt atleastoneofatandbtisnonzero meaninggt(ht)=0ifandonlyifcfp(ht)=cfn(ht)=0.4Thisclassofcostfunctionsencompassesavarietyofscenarios.Asanexample imagineanapplicationinwhichtheequalfalse-positiveconditionisessentialbutnotthefalse-negativecondition.Suchascenariomayariseinourrecidivism-predictionexample ifwerequirethatnon-repeatoffendersofanyracearenotdisproportionatelylabeledashighrisk.IfweplotthesetofcalibratedclassiﬁersH∗1andH∗2onthefalse-positive/false-negativeplane wecanseethatensuringthefalse-positiveconditionrequiresﬁndingclassiﬁersh1∈H∗1andh2∈H∗2thatfallonthesameverticalline(Figure1b).Conversely ifweinsteadchoosetosatisfyonlythefalse-negativecondition wewouldﬁndclassiﬁersh1andh2thatfallonthesamehorizontal(Figure1c).Finally ifbothfalse-positiveandfalse-negativeerrorsincuranegativecostontheindividual wemaychoosetoequalizeaweightedcombinationoftheerrorrates[3 4 8] whichcanbegraphicallydescribedbytheclassiﬁerslyingonaconvexandnegatively-slopedlevelset(Figure1d).Withthesedeﬁnitions wecanformallydeﬁneourrelaxation:Deﬁnition4(RelaxedEqualizedOddswithCalibration).Givenacostfunctiongtoftheformin(2) classiﬁersh1andh2achieveRelaxedEqualizedOddswithCalibrationforgroupsG1andG2ifbothclassiﬁersarecalibratedandsatisfytheconstraintg1(h1)=g2(h2).Itisworthnotingthat forcalibratedclassiﬁers anincreaseincoststrictlycorrespondstoanincreaseinboththefalse-negativeandfalse-positiverate.Thiscanbeinterpretedgraphically asthelevel-ordercostcurvesliefurtherawayfromtheoriginascostincreases(Figure2a).Inotherwords thecostfunctioncanalwaysbeusedasaproxyforeithererrorrate.5Feasibility.ItiseasytoseethatDeﬁnition4isalwayssatisﬁable–inFigures1b 1c and1dweseethattherearemanysuchsolutionsthatwouldlieonagivenlevel-ordercostcurvewhilemaintainingcalibration includingthecaseinwhichbothclassiﬁersareperfect.Inpractice however notallclassiﬁersareachievable.Fortherestofthepaper wewillassumethatwehaveaccessto“optimal”(butpossiblydiscriminatory)calibratedclassiﬁersh1andh2suchthat duetowhateverlimitationsthereareonthepredictabilityofthetask weareunabletoﬁndotherclassiﬁersthathavelowercostwithrespecttogt.Weallowh1andh2tobelearnedinanyway aslongastheyarecalibrated.Withoutlossofgenerality fortheremainderofthepaper wewillassumethatg1(h1)≥g2(h2).SincebyassumptionwehavenowaytoﬁndaclassiﬁerforG1withlowercostthanh1 ourgoalisthereforetoﬁndaclassiﬁer˜h2withcostequaltoh1.Thispairofclassiﬁerswouldrepresentthelowestcost(andthereforeoptimal)setofclassiﬁersthatsatisﬁescalibrationandtheequalcostconstraint.Foragivenbaserateµtandvalueofthecostfunctiongt acalibratedclassiﬁer’spositioninthegeneralizedfalse-positive/false-negativeplaneisuniquelydetermined(Figure2a).Thisisbecauseeachlevel-ordercurveofthecostfunctiongthasnegativeslopeinthisplane andeachlevelordercurveonlyintersectsagroup’scalibratedclassiﬁerlineonce.Inotherwords thereisauniquesolutioninthefalse-positive/false-negativeplaneforclassiﬁer˜h2(Figure2b).Considertherangeofvaluesthatgtcantake.Asnotedabove gt(ht)≥0 withequalityifandonlyifhtistheperfectclassiﬁer.Ontheotherhand thetrivialclassiﬁer(again whichoutputstheconstantµtforallinputs)isthecalibratedclassiﬁerthatachievesmaximumcostforanygt(seeLemma3inSectionS2).Asaresult thecostofaclassiﬁerforgroupGtisbetween0andgt(hµt).Thisnaturallyleadstoacharacterizationoffeasibility:Deﬁnition4canbeachievedifandonlyifh1incurslesscostthangroupG2’strivialclassiﬁerhµ2;i.e.ifg1(h1)≤g2(hµ2).ThiscanbeseengraphicallyinFigure2c inwhichthelevel-ordercurveforg1(h1)doesnotintersectthesetofcalibratedclassiﬁersforG2.Since byassumption wecannotﬁndacalibratedclassiﬁerforG1withstrictlysmallercostthanh1 thereisnofeasiblesolution.Ontheotherhand ifh1incurslesscostthanhµ2 thenwewillshowfeasibilitybyconstructionwithasimplealgorithm.AnAlgorithm.WhileitmaybepossibletoencodetheconstraintsofDeﬁnition4intothetrainingprocedureofh1andh2 itisnotimmediatelyobvioushowtodoso.Evennaturallyprobabilistic4Bycalibration wecannothaveoneofcfp(ht)=0orcfn(ht)=0withouttheother seeFigure1a.5Thisholdsevenforapproximatelycalibratedclassiﬁers—seeSectionS3.5algorithms suchaslogisticregression canbecomeuncalibratedinthepresenceofoptimizationconstraints(asisthecasein[37]).Itisnotstraightforwardtoencodethecalibrationconstraintiftheprobabilitiesareassumedtobecontinuous andpost-processingcalibrationmethods[31 35]wouldbreakequal-costconstraintsbymodifyingclassiﬁerscores.Therefore welooktoachievethecalibratedEqualizedOddsrelaxationbypost-processingexistingcalibratedclassiﬁers.Again givenh1andh2withg1(h1)≥g2(h2) wewanttoarriveatacalibratedclassiﬁer˜h2forgroupG2suchthatg1(h1)=g2(˜h2).Recallthat underourassumptions thiswouldbethebestpossiblesolutionwithrespecttoclassiﬁercost.WeshowthatthiscostconstraintcanbeachievedbywithholdingpredictiveinformationforarandomlychosensubsetofgroupG2.Inotherwords ratherthanalwaysreturningh2(x)forallsamples wewilloccasionallyreturnthegroup’smeanprobability(i.e.theoutputofthetrivialclassiﬁerhµ2).InLemma4inSectionS2 weshowthatif˜h2(x)=(cid:26)hµ2(x)=µ2withprobabilityαh2(x)withprobability1−α(3)thenthecostof˜h2isalinearinterpolationbetweenthecostsofh2andhµ2(Figure2d).Moreformally wehavethatg2(˜h2)=(1−α)g2(h2)+αg2(hµ2)) andthussettingα=g1(h1)−g2(h2)g2(hµ2)−g2(h2)ensuresthatg2(˜h2)=g1(h1)asdesired(Figure2b).Moreover thisrandomizationpreservescalibration(seeSectionS4).Algorithm1summarizesthismethod.Algorithm1AchievingCalibrationandanEqual-CostConstraintviaInformationWithholdingInput:classiﬁersh1andh2s.t.g2(h2)≤g1(h1)≤g2(hµ2) holdoutsetPvalid.•Determinebaserateµ2ofG2(usingPvalid)toproducetrivialclassiﬁerhµ2.•Construct˜h2usingwithα=g1(h1)−g2(h2)g2(hµ2)−g2(h2) whereαistheinterpolationparameter.returnh1 ˜h2—whicharecalibratedandsatisfyg1(h1)=g2(˜h2).Implications.Inacertainsense Algorithm1isan“optimal”methodbecauseitarrivesattheuniquefalse-negative/false-positivesolutionfor˜h2 where˜h2iscalibratedandhascostequaltoh1.Therefore(byourassumptions)wecanﬁndnobetterclassiﬁersthatsatisfyDeﬁnition4.Thissimpleresulthasstrongconsequences asthetradeoffstosatisfybothcalibrationandtheequal-costconstraintareoftenunsatisfactory—bothintuitivelyandexperimentally(aswewillshowinSection5).Weﬁndtwoprimaryobjectionstothissolution.First itequalizescostssimplybymakingaclassiﬁerstrictlyworseforoneofthegroups.Second itachievesthiscostincreasebywithholdinginformationonarandomlychosenpopulationsubset makingtheoutcomeinequitablewithinthegroup(asmeasuredbyastandardmeasureofinequalityliketheGinicoefﬁcient).Duetotheoptimalityofthealgorithm theformeroftheseissuesisunavoidableinanysolutionthatsatisﬁesDeﬁnition4.Thelatter however isslightlymoresubtle andbringsupthequestionofindividualfairness(whatguaranteeswewouldlikeanalgorithmtomakewithrespecttoeachindividual)andhowitinteractswithgroupfairness(population-levelguarantees).Whilethiscertainlyisanimportantissueforfuturework inthisparticularsetting evenifonecouldﬁndanotheralgorithmthatdistributestheburdenofadditionalcostmoreequitably anyalgorithmwillmakeatleastasmanyfalse-positive/false-negativeerrorsasAlgorithm1 andthesemisclassiﬁcationswillalwaysbetragictotheindividualswhomtheyaffect.Theperformancelossacrosstheentiregroupisoftensigniﬁcantenoughtomakethiscombinationofconstraintssomewhatworryingtouseinpractice regardlessofthealgorithm.ImpossibilityofSatisfyingMultipleEqual-CostConstraints.Itisnaturaltoarguetheremightbemultiplecostfunctionsthatwewouldliketoequalizeacrossgroups.However satisfyingmorethanonedistinctequal-costconstraint(i.e.differentcurvesintheF.P./F.N.plane)isinfeasible.Theorem1(Generalizedimpossibilityresult).Leth1andh2becalibratedclassiﬁersforG1andG2withequalcostwithrespecttogt.Ifµ16=µ2 andifh1andh2alsohaveequalcostwithrespecttoadifferentcostfunctiong0t thenh1andh2mustbeperfectclassiﬁers.(ProofinSectionS5).Notethatthisisageneralizationoftheimpossibilityresultof[26].Furthermore weshowinTheorem9(inSectionS5)thatthisholdsinanapproximatesense:ifcalibrationandmultipledistinctequal-costconstraintsareapproximatelyachievedbysomeclassiﬁer thenthatclassiﬁermusthaveapproximatelyzerogeneralizedfalse-positiveandfalse-negativerates.6(a)IncomePrediction.0.00.20.40.20.40.6GeneralizedF.N.RateEqualOdds(Derived)H∗1H∗2h1h2heo1heo20.00.20.4Calib.+EqualF.N.H∗1H∗2h1h2˜h1˜h2GeneralizedF.P.Rate(b)HealthPrediction.0.20.40.60.20.40.6GeneralizedF.N.RateEqualOdds(Derived)H∗1H∗2h1h2heo1heo20.20.40.6Calib.+EqualCostH∗1H∗2h1h2˜h1˜h2GeneralizedF.P.Rate(c)RecidivismPrediction.0.250.500.750.20.40.60.8GeneralizedF.N.RateEqualOdds(Trained)H∗1H∗2h1h2heo1heo20.250.500.75EqualOdds(Derived)H∗1H∗2h1h2heo1heo20.250.500.75Calib.+EqualF.P.H∗1H∗2h1h2˜h1˜h2GeneralizedF.P.RateFigure3:GeneralizedF.P.andF.N.ratesfortwogroupsunderEqualizedOddsandthecalibratedrelaxation.Diamondsrepresentpost-processedclassiﬁers.PointsontheEqualizedOdds(trained)graphrepresentclassiﬁersachievedbymodifyingconstrainthyperparameters.5ExperimentsInlightoftheseﬁndings ourgoalistounderstandtheimpactofimposingcalibrationandanequal-costconstraintonreal-worlddatasets.Wewillempiricallyshowthat inmanycases thiswillresultinperformancedegradation whilesimultaneouslyincreasingothernotionsofdisparity.Weperformexperimentsonthreedatasets:anincome-prediction ahealth-prediction andacriminalrecidivismdataset.Foreachtask wechooseacostfunctionwithinourframeworkthatisappropriateforthegivenscenario.Webeginwithtwocalibratedclassiﬁersh1andh2forgroupsG1andG2.Weassumethattheseclassiﬁerscannotbesigniﬁcantlyimprovedwithoutmoretrainingdataorfeatures.Wethenderive˜h2toequalizethecostswhilemaintainingcalibration.Theoriginalclassiﬁersaretrainedonaportionofthedata andthenthenewclassiﬁersarederivedusingaseparateholdoutset.Tocompareagainstthe(uncalibrated)EqualizedOddsframework wederiveF.P./F.N.matchingclassiﬁersusingthepost-processingmethodof[19](EO-Derived).Onthecriminalrecidivismdataset weadditionallylearnclassiﬁersthatdirectlyencodetheEqualizedOddsconstraints usingthemethodsof[37](EO-Trained).(SeeSectionS6fordetailedtrainingandpost-processingprocedures.)WevisualizemodelerrorratesonthegeneralizedF.P.andF.N.plane.Additionally weplotthecalibratedclassiﬁerlinesforG1andG2tovisualizemodelcalibration.IncomePrediction.TheAdultDatasetfromUCIMachineLearningRepository[28]contains14demographicandoccupationalfeaturesforvariouspeople withthegoalofpredictingwhetheraperson’sincomeisabove$50 000.Inthisscenario weseektoachievepredictionswithequalizedcostacrossgenders(G1representswomenandG2representsmen).WemodelascenariowheretheprimaryconcernisensuringequalgeneralizedF.N.ratesacrossgenders whichwould forexample helpjobrecruiterspreventgenderdiscriminationintheformofunderestimatedsalaries.Thus wechooseourcostconstrainttorequireequalgeneralizedF.N.ratesacrossgroups.InFigure3a weseethattheoriginalclassiﬁersh1andh2approximatelylieonthelineofcalibratedclassiﬁers.Intheleftplot(EO-Derived) weseethatitispossibleto(approximately)matchbotherrorratesoftheclassiﬁersatthecostofheo1deviatingfromthesetofcalibratedclassiﬁers.Intherightplot weseethatitisfeasibletoequalizethegeneralizedF.N.rateswhilemaintainingcalibration.h1and˜h2lieonthesamelevel-ordercurveofgt(representedbythedashed-grayline) andsimultaneouslyremainonthe“line”ofcalibratedclassiﬁers.Itisworthnotingthatachievingeithernotionofnon-discriminationrequiressomecosttoatleastoneofthegroups.However maintainingcalibrationfurtherincreasesthedifferenceinF.P.ratesbetweengroups.Insomesense thecalibratedframeworktradesoffonenotionofdisparityforanotherwhilesimultaneouslyincreasingtheoverallerrorrates.7HealthPrediction.TheHeartDatasetfromtheUCIMachineLearningRepositorycontains14processedfeaturesfrom906adultsin4geographicallocations.Thegoalofthisdatasetistoaccuratelypredictwhetherornotanindividualhasaheartcondition.Inthisscenario wewouldliketoreducedisparitybetweenmiddle-agedadults(G1)andseniors(G2).Inthisscenario weconsiderF.P.andF.N.tobothbeundesirable.Afalsepredictionofaheartconditioncouldresultinunnecessarymedicalattention whilefalsenegativesincurcostfromdelayedtreatment.Wethereforeutilizethefollowingcostfunctiongt(ht)=rfpht(x)(1−y)+rfn(1−ht(x))y whichessentiallyassignsaweighttobothF.N.andF.P.predictions.Inourexperiments wesetrfp=1andrfn=3.IntherightplotofFigure3b wecanseethatthelevel-ordercurvesofthecostfunctionformacurvedlineinthegeneralizedF.P./F.N.plane.Becauseouroriginalclassiﬁerslieapproximatelyonthesamelevel-ordercurve littlechangeisrequiredtoequalizethecostsofh1and˜h2whilemaintainingcalibration.Thisistheonlyexperimentinwhichthecalibratedframeworkincurslittleadditionalcost andthereforecouldbeconsideredaviableoption.However itisworthnotingthat inthisexample theequal-costconstraintdoesnotexplicitlymatcheitheroftheerrortypes andthereforethetwogroupswillinexpectationexperiencedifferenttypesoferrors.IntheleftplotofFigure3b(EO-Derived) weseethatitisalternativelyfeasibletoexplicitlymatchboththeF.P.andF.N.rateswhilesacriﬁcingcalibration.CriminalRecidivismPrediction.Finally weexaminetheframeworksinthecontextofourmotivat-ingexample:criminalrecidivism.Asmentionedintheintroduction AfricanAmericans(G1)receiveadisproportionatenumberofF.P.predictionsascomparedwithCaucasians(G2)whenautomatedrisktoolsareusedinpractice.Therefore weaimtoequalizethegeneralizedF.P.rate.Inthisexperiment wemodifythepredictionsmadebytheCOMPAStool[12] arisk-assessmenttoolusedinpracticebytheAmericanlegalsystem.Additionally wealsoseeifitispossibletoimprovetheclassiﬁerswithtraining-timeEqualizedOddsconstraintsusingthemethodsofZafaretal.[37](EO-Trained).InFigure3c weﬁrstobservethattheoriginalclassiﬁersh1andh2havelargegeneralizedF.P.andF.N.rates.BothmethodsofachievingEqualizedOdds—trainingconstraints(leftplot)andpost-processing(middleplot)matchtheerrorrateswhilesacriﬁcingcalibration.However weobservethat assumingh1andh2cannotbeimproved itisinfeasibletoachievethecalibratedrelaxation(Figure3cright).ThisisanexamplewherematchingtheF.P.rateofh1wouldrequireaclassiﬁerworsethanthetrivialclassiﬁerhµ2.Thisexamplethereforerepresentsaninstanceinwhichcalibrationiscompletelyincompatiblewithanyerror-rateconstraints.Iftheprimaryconcernofcriminaljusticepractitionersiscalibration[12 16] thentherewillinherentlybediscriminationintheformofF.P.andF.N.rates.However iftheEqualizedOddsframeworkisadopted themiscalibratedriskscoresinherentlycausediscriminationtoonegroup asarguedintheintroduction.Therefore themostmeaningfulchangeinsuchasettingwouldbeanimprovementtoh2(theclassiﬁerforAfricanAmericans)eitherthroughthecollectionofmoredataortheuseofmoresalientfeatures.Areductioninoverallerrortothegroupwithhighercostwillnaturallyleadtolesserror-ratedisparity.6DiscussionandConclusionWehaveobservedcasesinwhichcalibrationandrelaxedEqualizedOddsarecompatibleandcaseswheretheyarenot.Whenitisfeasible thepenaltyofequalizingcostisampliﬁedifthebaseratesbetweengroupsdiffersigniﬁcantly.Thisisexpected asbaseratedifferencesarewhatgiverisetocost-disparityinthecalibratedsetting.Seekingequalitywithrespecttoasingleerrorrate(e.g.false-negatives asintheincomepredictionexperiment)willnecessarilyincreasedisparitywithrespecttotheothererror.Thismaybetolerable(intheincomepredictioncase someemployeeswillendupover-paid)butcouldalsobehighlyproblematic(e.g.incriminaljusticesettings).Finally wehaveobservedthatthecalibratedrelaxationisinfeasiblewhenthebest(discriminatory)classiﬁersarenotfarfromthetrivialclassiﬁers(leavinglittleroomforinterpolation).Insuchsettings weseethatcalibrationiscompletelyincompatiblewithanequalizederrorconstraint.Insummary weconcludethatmaintainingcostparityandcalibrationisdesirableyetoftendifﬁcultinpractice.Althoughweprovideanalgorithmtoeffectivelyﬁndtheuniquefeasiblesolutiontobothconstraints itisinherentlybasedonrandomlyexchangingthepredictionsofthebetterclassiﬁerwiththetrivialbaserate.Eveniffairnessisreachedinexpectation foranindividualcase itmaybehardtoacceptthatoccasionallyconsequentialdecisionsaremadebyrandomlywithholdingpredictiveinformation irrespectiveofaparticularperson’sfeaturerepresentation.Inthispaperwearguethat aslongascalibrationisrequired nolower-errorsolutioncanbeachieved.8AcknowledgementsGP FW andKQWaresupportedinpartbygrantsfromtheNationalScienceFoundation(III-1149882 III-1525919 III-1550179 III-1618134 andIII-1740822) theOfﬁceofNavalResearchDOD(N00014-17-1-2175) andtheBillandMelindaGatesFoundation.MRissupportedbyanNSFGraduateResearchFellowship(DGE-1650441).JKissupportedinpartbyaSimonsInvestigatorAward anAROMURIgrant aGoogleResearchGrant andaFacebookFacultyResearchGrant.References[1]J.Angwin J.Larson S.Mattu andL.Kirchner.Machinebias:There’ssoftwareusedacrossthecountrytopredictfuturecriminals.Andit’sbiasedagainstblacks.ProPublica 2016.https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.[2]S.BarocasandA.D.Selbst.Bigdata’sdisparateimpact.CaliforniaLawReview 104 2016.[3]R.Berk.Aprimeronfairnessincriminaljusticeriskassessments.Criminology 41(6):6–9 2016.[4]R.Berk H.Heidari S.Jabbari M.Kearns andA.Roth.Fairnessincriminaljusticeriskassessments:Thestateoftheart.arXivpreprintarXiv:1703.09207 2017.[5]T.Bolukbasi K.-W.Chang J.Y.Zou V.Saligrama andA.T.Kalai.Manistocomputerprogrammeraswomanistohomemaker?debiasingwordembeddings.InNIPS pages4349–4357 2016.[6]T.CaldersandS.Verwer.Threenaivebayesapproachesfordiscrimination-freeclassiﬁcation.KDD 2012.[7]T.Calders F.Kamiran andM.Pechenizkiy.Buildingclassiﬁerswithindependencyconstraints.InICDMWorkshops 2009.[8]A.Chouldechova.Fairpredictionwithdisparateimpact:Astudyofbiasinrecidivismpredictioninstruments.arXivpreprintarXiv:1703.00056 2017.[9]S.Corbett-Davies E.Pierson A.Feller S.Goel andA.Huq.Algorithmicdecisionmakingandthecostoffairness.InKDD pages797–806 2017.[10]C.S.Crowson E.J.Atkinson andT.M.Therneau.Assessingcalibrationofprognosticriskscores.StatisticalMethodsinMedicalResearch 25(4):1692–1706 2016.[11]A.P.Dawid.Thewell-calibratedbayesian.JournaloftheAmericanStatisticalAssociation 77(379):605–610 1982.[12]W.Dieterich C.Mendoza andT.Brennan.COMPASriskscales:Demonstratingaccuracyequityandpredictiveparity.Technicalreport Northpointe July2016.http://www.northpointeinc.com/northpointe-analysis.[13]C.Dwork M.Hardt T.Pitassi O.Reingold andR.Zemel.Fairnessthroughawareness.InInnovationsinTheoreticalComputerScience 2012.[14]H.EdwardsandA.Storkey.Censoringrepresentationswithanadversary.InICLR 2016.[15]M.Feldman S.A.Friedler J.Moeller C.Scheidegger andS.Venkatasubramanian.Certifyingandremovingdisparateimpact.InKDD pages259–268 2015.[16]A.Flores C.Lowenkamp andK.Bechtel.Falsepositives falsenegatives andfalseanalyses:Arejoinderto“machinebias:There’ssoftwareusedacrossthecountrytopredictfuturecriminals.andit’sbiasedagainstblacks.”.Technicalreport Crime&JusticeInstitute September2016.http://www.crj.org/cji/entry/false-positives-false-negatives-and-false-analyses-a-rejoinder.[17]G.Goh A.Cotter M.Gupta andM.P.Friedlander.Satisfyingreal-worldgoalswithdatasetconstraints.InNIPS pages2415–2423.2016.[18]C.Guo G.Pleiss Y.Sun andK.Q.Weinberger.Oncalibrationofmodernneuralnetworks.InICML 2017.[19]M.Hardt E.Price andS.Nathan.Equalityofopportunityinsupervisedlearning.InAdvancesinNeuralInformationProcessingSystems 2016.9[20]J.E.JohndrowandK.Lum.Analgorithmforremovingsensitiveinformation:applicationtorace-independentrecidivismprediction.arXivpreprintarXiv:1703.04957 2017.[21]M.Joseph M.Kearns J.H.Morgenstern andA.Roth.Fairnessinlearning:Classicandcontextualbandits.InNIPS 2016.[22]F.KamiranandT.Calders.Classifyingwithoutdiscriminating.InInternationalConferenceonComputerControlandCommunication 2009.[23]T.Kamishima S.Akaho andJ.Sakuma.Fairness-awarelearningthroughregularizationapproach.InICDMWorkshops 2011.[24]M.Kearns A.Roth andZ.S.Wu.Meritocraticfairnessforcross-populationselection.InInternationalConferenceonMachineLearning pages1828–1836 2017.[25]N.Kilbertus M.Rojas-Carulla G.Parascandolo M.Hardt D.Janzing andB.Schölkopf.Avoidingdiscriminationthroughcausalreasoning.InNIPS 2017.[26]J.Kleinberg S.Mullainathan andM.Raghavan.Inherenttrade-offsinthefairdeterminationofriskscores.InInnovationsinTheoreticalComputerScience.ACM 2017.[27]M.J.Kusner J.R.Loftus C.Russell andR.Silva.Counterfactualfairness.arXivpreprintarXiv:1703.06856 2017.[28]M.Lichman.UCImachinelearningrepository 2013.URLhttp://archive.ics.uci.edu/ml.[29]C.Louizos K.Swersky Y.Li M.Welling andR.Zemel.Thevariationalfairautoencoder.InICLR 2016.[30]A.Niculescu-MizilandR.Caruana.Predictinggoodprobabilitieswithsupervisedlearning.InICML 2005.[31]J.Platt.Probabilisticoutputsforsupportvectormachinesandcomparisonstoregularizedlikelihoodmethods.AdvancesinLargeMarginClassiﬁers 10(3):61–74 1999.[32]A.RomeiandS.Ruggieri.Amultidisciplinarysurveyondiscriminationanalysis.TheKnowledgeEngineeringReview 29(05):582–638 2014.[33]White-House.Bigdata:Areportonalgorithmicsystems opportunity andcivilrights.Technicalreport May2016.[34]B.Woodworth S.Gunasekar M.I.Ohannessian andN.Srebro.Learningnon-discriminatorypredictors.InProceedingsofthe2017ConferenceonLearningTheory volume65 pages1920–1953 Amsterdam Netherlands 07–10Jul2017.PMLR.[35]B.ZadroznyandC.Elkan.Obtainingcalibratedprobabilityestimatesfromdecisiontreesandnaivebayesianclassiﬁers.InICML pages609–616 2001.[36]M.B.Zafar I.Valera M.G.Rodriguez andK.P.Gummadi.Learningfairclassiﬁers.arXivpreprintarXiv:1507.05259 2015.[37]M.B.Zafar I.Valera M.G.Rodriguez andK.P.Gummadi.Fairnessbeyonddisparatetreatment&disparateimpact:Learningclassiﬁcationwithoutdisparatemistreatment.InWorldWideWebConference 2017.[38]R.S.Zemel Y.Wu K.Swersky T.Pitassi andC.Dwork.Learningfairrepresentations.InICML 2013.[39]I.Zliobaite.Ontherelationbetweenaccuracyandfairnessinbinaryclassiﬁcation.InICMLWorkshoponFairness Accountability andTransparencyinMachineLearning 2015.10,Geoff Pleiss
Manish Raghavan
Jon Kleinberg
Kilian Weinberger