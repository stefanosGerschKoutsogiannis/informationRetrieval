2017,Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration,Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning  statistics  and computer vision. Despite the recent introduction of several algorithms with good empirical performance  it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances. This result relies on a new analysis of Sinkhorn iterations  which also directly suggests a new greedy coordinate descent algorithm Greenkhorn with the same theoretical guarantees. Numerical simulations  illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice.,Near-linear time approximation algorithms for

optimal transport via Sinkhorn iteration

Jason Altschuler

MIT

jasonalt@mit.edu

Jonathan Weed

MIT

jweed@mit.edu

Philippe Rigollet

MIT

rigollet@mit.edu

Abstract

Computing optimal transport distances such as the earth mover’s distance is a
fundamental problem in machine learning  statistics  and computer vision. Despite
the recent introduction of several algorithms with good empirical performance 
it is unknown whether general optimal transport distances can be approximated
in near-linear time. This paper demonstrates that this ambitious goal is in fact
achieved by Cuturi’s Sinkhorn Distances. This result relies on a new analysis
of Sinkhorn iterations  which also directly suggests a new greedy coordinate
descent algorithm GREENKHORN with the same theoretical guarantees. Numerical
simulations illustrate that GREENKHORN signiﬁcantly outperforms the classical
SINKHORN algorithm in practice.

Dedicated to the memory of Michael B. Cohen

1

Introduction

Computing distances between probability measures on metric spaces  or more generally between point
clouds  plays an increasingly preponderant role in machine learning [SL11  MJ15  LG15  JSCG16 
ACB17]  statistics [FCCR16  PZ16  SR04  BGKL17] and computer vision [RTG00  BvdPPH11 
SdGP+15]. A prominent example of such distances is the earth mover’s distance introduced
in [WPR85] (see also [RTG00])  which is a special case of Wasserstein distance  or optimal transport
(OT) distance [Vil09].
While OT distances exhibit a unique ability to capture geometric features of the objects at hand  they
suffer from a heavy computational cost that had been prohibitive in large scale applications until the
recent introduction to the machine learning community of Sinkhorn Distances by Cuturi [Cut13].
Combined with other numerical tricks  these recent advances have enabled the treatment of large
point clouds in computer graphics such as triangle meshes [SdGP+15] and high-resolution neu-
roimaging data [GPC15]. Sinkhorn Distances rely on the idea of entropic penalization  which has
been implemented in similar problems at least since Schrödinger [Sch31  Leo14]. This powerful
idea has been successfully applied to a variety of contexts not only as a statistical tool for model
selection [JRT08  RT11  RT12] and online learning [CBL06]  but also as an optimization gadget in
ﬁrst-order optimization methods such as mirror descent and proximal methods [Bub15].

Related work. Computing an OT distance amounts to solving the following linear system:

Ur c :=(cid:8)P ∈ IRn×n

+

: P 1 = r   P (cid:62)1 = c(cid:9)  

(cid:104)P  C(cid:105)  

min
P∈Ur c

(1)
where 1 is the all-ones vector in IRn  C ∈ IRn×n
is a given cost matrix  and r ∈ IRn  c ∈ IRn are
given vectors with positive entries that sum to one. Typically C is a matrix containing pairwise

+

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

distances (and is thus dense)  but in this paper we allow C to be an arbitrary non-negative dense
matrix with bounded entries since our results are more general. For brevity  this paper focuses on
square matrices C and P   since extensions to the rectangular case are straightforward.
This paper is at the intersection of two lines of research: a theoretical one that aims at ﬁnding (near)
linear time approximation algorithms for simple problems that are already known to run in polynomial
time and a practical one that pursues fast algorithms for solving optimal transport approximately for
large datasets.
Noticing that (1) is a linear program with O(n) linear constraints and certain graphical structure  one

can use the recent Lee-Sidford linear solver to ﬁnd a solution in time (cid:101)O(n2.5) [LS14]  improving over

the previous standard of O(n3.5) [Ren88]. While no practical implementation of the Lee-Sidford
algorithm is known  it provides a theoretical benchmark for our methods. Their result is part of a long
line of work initiated by the seminal paper of Spielman and Teng [ST04] on solving linear systems
of equations  which has provided a building block for near-linear time approximation algorithms
in a variety of combinatorially structured linear problems. A separate line of work has focused on
obtaining faster algorithms for (1) by imposing additional assumptions. For instance  [AS14] obtain
approximations to (1) when the cost matrix C arises from a metric  but their running times are not
truly near-linear. [SA12 ANOY14] develop even faster algorithms for (1)  but require C to arise from
a low-dimensional (cid:96)p metric.
Practical algorithms for computing OT distances include Orlin’s algorithm for the Uncapacitated
Minimum Cost Flow problem via a standard reduction. Like interior point methods  it has a provable
complexity of O(n3 log n). This dependence on the dimension is also observed in practice  thereby
preventing large-scale applications. To overcome the limitations of such general solvers  various
ideas ranging from graph sparsiﬁcation [PW09] to metric embedding [IT03  GD04  SJ08] have been
proposed over the years to deal with particular cases of OT distance.
Our work complements both lines of work  theoretical and practical  by providing the ﬁrst near-linear
time guarantee to approximate (1) for general non-negative cost matrices. Moreover we show that
this performance is achieved by algorithms that are also very efﬁcient in practice. Central to our
contribution are recent developments of scalable methods for general OT that leverage the idea of
entropic regularization [Cut13  BCC+15  GCPB16]. However  the apparent practical efﬁcacy of these
approaches came without theoretical guarantees. In particular  showing that this regularization yields
an algorithm to compute or approximate general OT distances in time nearly linear in the input size
n2 was an open question before this work.

Our contribution. The contribution of this paper is twofold. First we demonstrate that  with an
appropriate choice of parameters  the algorithm for Sinkhorn Distances introduced in [Cut13] is
in fact a near-linear time approximation algorithm for computing OT distances between discrete
measures. This is the ﬁrst proof that such near-linear time results are achievable for optimal transport.
We also provide previously unavailable guidance for parameter tuning in this algorithm. Core to
our work is a new and arguably more natural analysis of the Sinkhorn iteration algorithm  which we
show converges in a number of iterations independent of the dimension n of the matrix to balance. In
particular  this analysis directly suggests a greedy variant of Sinkhorn iteration that also provably
runs in near-linear time and signiﬁcantly outperforms the classical algorithm in practice. Finally 
while most approximation algorithms output an approximation of the optimum value of the linear
program (1)  we also describe a simple  parallelizable rounding algorithm that provably outputs a
feasible solution to (1). Speciﬁcally  for any ε > 0 and bounded  non-negative cost matrix C  we

describe an algorithm that runs in time (cid:101)O(n2/ε3) and outputs ˆP ∈ Ur c such that

(cid:104) ˆP   C(cid:105) ≤ min
P∈Ur c

(cid:104)P  C(cid:105) + ε

We emphasize that our analysis does not require the cost matrix C to come from an underlying metric;
we only require C to be non-negative. This implies that our results also give  for example  near-linear
time approximation algorithms for Wasserstein p-distances between discrete measures.
Notation. We denote non-negative real numbers by IR+  the set of integers {1  . . .   n} by [n]  and
the n-dimensional simplex by ∆n := {x ∈ IRn
i=1 xi = 1}. For two probability distributions
p  q ∈ ∆n such that p is absolutely continuous w.r.t. q  we deﬁne the entropy H(p) of p and the

+ : (cid:80)n

2

H(p) =

(cid:19)

(cid:18) 1

(cid:19)
(cid:18) pi
n(cid:88)
+   we deﬁne the entropy H(P ) entrywise as(cid:80)

K(p(cid:107)q) :=

n(cid:88)

pi log

pi log

i=1

 

pi

.

qi

i=1

Kullback-Leibler divergence K(p(cid:107)q) between p and q respectively by

Similarly  for a matrix P ∈ IRn×n
. We
use 1 and 0 to denote the all-ones and all-zeroes vectors in IRn. For a matrix A = (Aij)  we denote
by exp(A) the matrix with entries (eAij ). For A ∈ IRn×n  we denote its row and columns sums
(cid:107)A(cid:107)1 =(cid:80)
by r(A) := A1 ∈ IRn and c(A) := A(cid:62)1 ∈ IRn  respectively. The coordinates ri(A) and cj(A)
denote the ith row sum and jth column sum of A  respectively. We write (cid:107)A(cid:107)∞ = maxij |Aij| and
of A and B by (cid:104)A  B(cid:105) =(cid:80)
ij |Aij|. For two matrices of the same dimension  we denote the Frobenius inner product
ij AijBij. For a vector x ∈ IRn  we write D(x) ∈ IRn×n to denote the
write un = (cid:101)O(vn) if there exist positive constants C  c such that un ≤ Cvn(log n)c. For any two
diagonal matrix with entries (D(x))ii = xi. For any two nonnegative sequences (un)n  (vn)n  we
real numbers  we write a ∧ b = min(a  b).

ij Pij log 1
Pij

2 Optimal Transport in near-linear time

ε

8(cid:107)C(cid:107)∞

Algorithm 1 APPROXOT(C  r  c  ε)

η ← 4 log n
\\ Step 1: Approximately project onto Ur c

  ε(cid:48) ← ε

1: A ← exp(−ηC)
2: B ← PROJ(A Ur c  ε(cid:48))

In this section  we describe the main algorithm studied in this paper. Pseudocode appears in
Algorithm 1.
The core of our algorithm is the computation of
an approximate Sinkhorn projection of the matrix
A = exp(−ηC) (Step 1)  details for which will
be given in Section 3. Since our approximate
Sinkhorn projection is not guaranteed to lie in
the feasible set  we round our approximation to
ensure that it lies in Ur c (Step 2). Pseudocode
for a simple  parallelizable rounding procedure is
given in Algorithm 2.
Algorithm 1 hinges on two subroutines: PROJ
and ROUND. We give two algorithms for PROJ:
SINKHORN and GREENKHORN. We devote Sec-
tion 3 to their analysis  which is of independent
interest. On the other hand  ROUND is fairly sim-
ple. Its analysis is postponed to Section 4.
Our main theorem about Algorithm 1 is the follow-
ing accuracy and runtime guarantee. The proof
is postponed to Section 4  since it relies on the
analysis of PROJ and ROUND.
Theorem 1. Algorithm 1 returns a point ˆP ∈ Ur c satisfying

Algorithm 2 ROUND(F Ur c)
1: X ← D(x) with xi = ri
2: F (cid:48) ← XF
3: Y ← D(y) with yj = cj
4: F (cid:48)(cid:48) ← F (cid:48)Y
5: errr ← r − r(F (cid:48)(cid:48))  errc ← c − c(F (cid:48)(cid:48))
c /(cid:107)errr(cid:107)1
6: Output G ← F (cid:48)(cid:48) + errrerr(cid:62)

\\ Step 2: Round to feasible point in Ur c

3: Output ˆP ← ROUND(B Ur c)

ri(F ) ∧ 1
cj (F (cid:48)) ∧ 1

(cid:104) ˆP   C(cid:105) ≤ min
P∈Ur c

(cid:104)P  C(cid:105) + ε

in time O(n2 + S)  where S is the running time of the subroutine PROJ(A Ur c  ε(cid:48)). In particular 
if (cid:107)C(cid:107)∞ ≤ L  then S can be O(n2L3(log n)ε−3)  so that Algorithm 1 runs in O(n2L3(log n)ε−3)
time.
Remark 1. The time complexity in the above theorem reﬂects only elementary arithmetic operations.
In the interest of clarity  we ignore questions of bit complexity that may arise from taking exponentials.
The effect of this simpliﬁcation is marginal since it can be easily shown [KLRS08] that the maximum
bit complexity throughout the iterations of our algorithm is O(L(log n)/ε). As a result  factoring in
bit complexity leads to a runtime of O(n2L4(log n)2ε−4)  which is still truly near-linear.

3

3 Linear-time approximate Sinkhorn projection

The core of our OT algorithm is the entropic penalty proposed by Cuturi [Cut13]:

(cid:8)(cid:104)P  C(cid:105) − η−1H(P )(cid:9) .

Pη := argmin
P∈Ur c

(2)

The solution to (2) can be characterized explicitly by analyzing its ﬁrst-order conditions for optimality.
[Cut13] For any cost matrix C and r  c ∈ ∆n  the minimization program (2) has a
Lemma 1.
unique minimum at Pη ∈ Ur c of the form Pη = XAY   where A = exp(−ηC) and X  Y ∈ IRn×n
are both diagonal matrices. The matrices (X  Y ) are unique up to a constant factor.
We call the matrix Pη appearing in Lemma 1 the Sinkhorn projection of A  denoted ΠS (A Ur c) 
after Sinkhorn  who proved uniqueness in [Sin67]. Computing ΠS (A Ur c) exactly is impractical  so
we implement instead an approximate version PROJ(A Ur c  ε(cid:48))  which outputs a matrix B = XAY
that may not lie in Ur c but satisﬁes the condition (cid:107)r(B) − r(cid:107)1 + (cid:107)c(B) − c(cid:107)1 ≤ ε(cid:48). We stress that
this condition is very natural from a statistical standpoint  since it requires that r(B) and c(B) are
close to the target marginals r and c in total variation distance.

+

3.1 The classical Sinkhorn algorithm

Given a matrix A  Sinkhorn proposed a simple iterative algorithm to approximate the Sinkhorn
projection ΠS (A Ur c)  which is now known as the Sinkhorn-Knopp algorithm or RAS method.
Despite the simplicity of this algorithm and its good performance in practice  it has been difﬁcult
to analyze. As a result  recent work showing that ΠS (A Ur c) can be approximated in near-linear
time [AZLOW17  CMTV17] has bypassed the Sinkhorn-Knopp algorithm entirely1. In our work  we
obtain a new analysis of the simple and practical Sinkhorn-Knopp algorithm  showing that it also
approximates ΠS (A Ur c) in near-linear time.
Pseudocode for the Sinkhorn-Knopp algorithm ap-
pears in Algorithm 3. In brief  it is an alternating
projection procedure which renormalizes the rows
and columns of A in turn so that they match the de-
sired row and column marginals r and c. At each
step  it prescribes to either modify all the rows by
multiplying row i by ri/ri(A) for i ∈ [n]  or to
do the analogous operation on the columns. (We
interpret the quantity 0/0 as 1 in this algorithm if
ever it occurs.) The algorithm terminates when the
matrix A(k) is sufﬁciently close to the polytope
Ur c.

Algorithm 3 SINKHORN(A Ur c  ε(cid:48))
1: Initialize k ← 0
2: A(0) ← A/(cid:107)A(cid:107)1  x0 ← 0  y0 ← 0
3: while dist(A(k) Ur c) > ε(cid:48) do
4:
5:
6:
7:
8:
9:
10:
11:
12: Output B ← A(k)

k ← k + 1
if k odd then
xi ← log
ri(A(k−1)) for i ∈ [n]
xk ← xk−1 + x  yk ← yk−1
y ← log
cj (A(k−1)) for j ∈ [n]
yk ← yk−1 + y  xk ← xk−1

A(k) = D(exp(xk))AD(exp(yk))

3.2 Prior work

Before this work  the best analysis of Algorithm 3

showed that (cid:101)O((ε(cid:48))−2) iterations sufﬁce to obtain a matrix close to Ur c in (cid:96)2 distance:
in O(cid:0)ρ(ε(cid:48))−2 log(s/(cid:96))(cid:1) iterations  where s = (cid:80)

[KLRS08] Let A be a strictly positive matrix. Algorithm 3 with dist(A Ur c) =
Proposition 1.
(cid:107)r(A) − r(cid:107)2 + (cid:107)c(A) − c(cid:107)2 outputs a matrix B satisfying (cid:107)r(B) − r(cid:107)2 + (cid:107)c(B) − c(cid:107)2 ≤ ε(cid:48)
ij Aij  (cid:96) = minij Aij  and ρ > 0 is such that
ri  ci ≤ ρ for all i ∈ [n].
Unfortunately  this analysis is not strong enough to obtain a true near-linear time guarantee. Indeed 
the (cid:96)2 norm is not an appropriate measure of closeness between probability vectors  since very
different distributions on large alphabets can nevertheless have small (cid:96)2 distance: for example 

(n−1  . . .   n−1  0  . . .   0) and (0  . . .   0  n−1  . . .   n−1) in ∆2n have (cid:96)2 distance(cid:112)2/n even though

else

ri

cj

1Replacing the PROJ step in Algorithm 1 with the matrix-scaling algorithm developed in [CMTV17] results
in a runtime that is a single factor of ε faster than what we present in Theorem 1. The beneﬁt of our approach is
that it is extremely easy to implement  whereas the matrix-scaling algorithm of [CMTV17] relies heavily on
near-linear time Laplacian solver subroutines  which are not implementable in practice.

4

they have disjoint support. As noted above  for statistical problems  including computation of the OT
distance  it is more natural to measure distance in (cid:96)1 norm.
The following Corollary gives the best (cid:96)1 guarantee available from Proposition 1.
Corollary 1. Algorithm 3 with dist(A Ur c) = (cid:107)r(A) − r(cid:107)2 + (cid:107)c(A) − c(cid:107)2 outputs a matrix B

satisfying (cid:107)r(B) − r(cid:107)1 + (cid:107)c(B) − c(cid:107)1 ≤ ε(cid:48) in O(cid:0)nρ(ε(cid:48))−2 log(s/(cid:96))(cid:1) iterations.

The extra factor of n in the runtime of Corollary 1 is the price to pay to convert an (cid:96)2 bound to an (cid:96)1
bound. Note that ρ ≥ 1/n  so nρ is always larger than 1. If r = c = 1n/n are uniform distributions 
then nρ = 1 and no dependence on the dimension appears. However  in the extreme where r or c
contains an entry of constant size  we get nρ = Ω(n).

3.3 New analysis of the Sinkhorn algorithm

Our new analysis allows us to obtain a dimension-independent bound on the number of iterations
beyond the uniform case.
Theorem 2. Algorithm 3 with dist(A Ur c) = (cid:107)r(A) − r(cid:107)1 + (cid:107)c(A) − c(cid:107)1 outputs a matrix B
ij Aij

satisfying (cid:107)r(B) − r(cid:107)1 + (cid:107)c(B) − c(cid:107)1 ≤ ε(cid:48) in O(cid:0)(ε(cid:48))−2 log(s/(cid:96))(cid:1) iterations  where s =(cid:80)

and (cid:96) = minij Aij.

Comparing our result with Corollary 1  we see what our bound is always stronger  by up to a factor
of n. Moreover  our analysis is extremely short. Our improved results and simpliﬁed proof follow
directly from the fact that we carry out the analysis entirely with respect to the Kullback–Leibler
divergence  a common measure of statistical distance. This measure possesses a close connection
to the total-variation distance via Pinsker’s inequality (Lemma 4  below)  from which we obtain the
desired (cid:96)1 bound. Similar ideas can be traced back at least to [GY98] where an analysis of Sinkhorn
iterations for bistochastic targets is sketched in the context of a different problem: detecting the
existence of a perfect matching in a bipartite graph.
We ﬁrst deﬁne some notation. Given a matrix A and desired row and column sums r and c  we deﬁne
the potential (Lyapunov) function f : IRn × IRn → IR by

f (x  y) =

Aijexi+yj − (cid:104)r  x(cid:105) − (cid:104)c  y(cid:105) .

This auxiliary function has appeared in much of the literature on Sinkhorn projections [KLRS08 
CMTV17  KK96  KK93]. We call the vectors x and y scaling vectors. It is easy to check that
a minimizer (x∗  y∗) of f yields the Sinkhorn projection of A: writing X = D(exp(x∗)) and
Y = D(exp(y∗))  ﬁrst order optimality conditions imply that XAY lies in Ur c  and therefore
XAY = ΠS (A Ur c).
The following lemma exactly characterizes the improvement in the potential function f from an
iteration of Sinkhorn  in terms of our current divergence to the target marginals.
Lemma 2. If k ≥ 2  then f (xk−1  yk−1) − f (xk  yk) = K(r(cid:107)r(A(k−1))) + K(c(cid:107)c(A(k−1))) .
Proof. Assume without loss of generality that k is odd  so that c(A(k−1)) = c and r(A(k)) = r. (If
k is even  interchange the roles of r and c.) By deﬁnition 
− A(k)

(cid:1) + (cid:104)r  xk − xk−1(cid:105) + (cid:104)c  yk − yk−1(cid:105)

f (xk−1  yk−1) − f (xk  yk) =

(cid:0)A(k−1)

ij

ij

(cid:88)

ij

(cid:88)
(cid:88)

ij

i

=

ri(xk

i − xk−1

i

) = K(r(cid:107)r(A(k−1)) + K(c(cid:107)c(A(k−1))  

where we have used that: (cid:107)A(k−1)(cid:107)1 = (cid:107)A(k)(cid:107)1 = 1 and Y (k) = Y (k−1); for all i  ri(xk
ri log

ri(A(k−1)); and K(c(cid:107)c(A(k−1))) = 0 since c = c(A(k−1)).

ri

i − xk−1

i

) =

The next lemma has already appeared in the literature and we defer its proof to the supplement.
Lemma 3. If A is a positive matrix with (cid:107)A(cid:107)1 ≤ s and smallest entry (cid:96)  then
f (x  y) ≤ log

f (x  y) ≤ f (0  0) − min
x y∈IR

f (x1  y1) − min
x y∈IR

.

s
(cid:96)

5

Lemma 4 (Pinsker’s Inequality). For any probability measures p and q  (cid:107)p − q(cid:107)1 ≤(cid:112)2K(p(cid:107)q).

Proof of Theorem 2. Let k∗ be the ﬁrst iteration such that (cid:107)r(A(k∗)) − r(cid:107)1 + (cid:107)c(A(k∗)) − c(cid:107)1 ≤ ε(cid:48).
Pinsker’s inequality implies that for any k < k∗  we have

ε(cid:48)2 < ((cid:107)r(A(k)) − r(cid:107)1 + (cid:107)c(A(k)) − c(cid:107)1)2 ≤ 4(K(r(cid:107)r(A(k)) + K(c(cid:107)c(A(k)))  

so Lemmas 2 and 3 imply that we terminate in k∗ ≤ 4ε(cid:48)−2 log(s/(cid:96)) steps  as claimed.

3.4 Greedy Sinkhorn

In addition to a new analysis of SINKHORN  we propose a new algorithm GREENKHORN which enjoys
the same convergence guarantee but performs better in practice. Instead of performing alternating
updates of all rows and columns of A  the GREENKHORN algorithm updates only a single row or
column at each step. Thus GREENKHORN updates only O(n) entries of A per iteration  rather than
O(n2).
In this respect  GREENKHORN is similar to the stochastic algorithm for Sinkhorn projection proposed
by [GCPB16]. There is a natural interpretation of both algorithms as coordinate descent algorithms
in the dual space corresponding to row/column violations. Nevertheless  our algorithm differs from
theirs in several key ways. Instead of choosing a row or column to update randomly  GREENKHORN
chooses the best row or column to update greedily. Additionally  GREENKHORN does an exact line
search on the coordinate in question since there is a simple closed form for the optimum  whereas the
algorithm proposed by [GCPB16] updates in the direction of the average gradient. Our experiments
establish that GREENKHORN performs better in practice; more details appear in the Supplement.
We emphasize that although this algorithm is an extremely natural modiﬁcation of SINKHORN  previ-
ous analyses of SINKHORN cannot be modiﬁed to extract any meaningful performance guarantees
on GREENKHORN. On the other hand  our new analysis of SINKHORN from Section 3.3 applies to
GREENKHORN with only trivial modiﬁcations.
Pseudocode for GREENKHORN appears in Algo-
rithm 4. We let dist(A Ur c) = (cid:107)r(A) − r(cid:107)1 +
(cid:107)c(A) − c(cid:107)1 and deﬁne the distance function
ρ : IR+ × IR+ → [0  +∞] by

ρ(a  b) = b − a + a log

a
b

.

Algorithm 4 GREENKHORN(A Ur c  ε(cid:48))
1: A(0) ← A/(cid:107)A(cid:107)1  x ← 0  y ← 0.
2: A ← A(0)
3: while dist(A Ur c) > ε do
I ← argmaxi ρ(ri  ri(A))
4:
J ← argmaxj ρ(cj  cj(A))
5:
if ρ(rI   rI (A)) > ρ(cJ   cJ (A)) then
6:
7:
8:
9:
10:
11: Output B ← A

xI ← xI + log rI
yJ ← yJ + log cJ

else

rI (A)

The choice of ρ is justiﬁed by its appearance in
Lemma 5  below. While ρ is not a metric  it is
easy to see that ρ is nonnegative and satisﬁes
ρ(a  b) = 0 iff a = b.
We note that after r(A) and c(A) are com-
puted once at the beginning of the algorithm 
GREENKHORN can easily be implemented such that each iteration runs in only O(n) time.
Theorem 3. The algorithm GREENKHORN outputs a matrix B satisfying (cid:107)r(B) − r(cid:107)1 + (cid:107)c(B) −
ij Aij and (cid:96) = minij Aij. Since each
iteration takes O(n) time  such a matrix can be found in O(n2(ε(cid:48))−2 log(s/(cid:96))) time.

c(cid:107)1 ≤ ε(cid:48) in O(n(ε(cid:48))−2 log(s/(cid:96))) iterations  where s = (cid:80)

A ← D(exp(x))A(0)D(exp(y))

cJ (A)

The analysis requires the following lemma  which is an easy modiﬁcation of Lemma 2.
Lemma 5. Let A(cid:48) and A(cid:48)(cid:48) be successive iterates of GREENKHORN  with corresponding scaling
vectors (x(cid:48)  y(cid:48)) and (x(cid:48)(cid:48)  y(cid:48)(cid:48)). If A(cid:48)(cid:48) was obtained from A(cid:48) by updating row I  then

f (x(cid:48)  y(cid:48)) − f (x(cid:48)(cid:48)  y(cid:48)(cid:48)) = ρ(rI   rI (A(cid:48)))  

and if it was obtained by updating column J  then

f (x(cid:48)  y(cid:48)) − f (x(cid:48)(cid:48)  y(cid:48)(cid:48)) = ρ(cJ   cJ (A(cid:48))) .

We also require the following extension of Pinsker’s inequality (proof in Supplement).

6

Lemma 6. For any α ∈ ∆n  β ∈ IRn

i ρ(αi  βi). If ρ(α  β) ≤ 1  then

+  deﬁne ρ(α  β) =(cid:80)
(cid:107)α − β(cid:107)1 ≤(cid:112)7ρ(α  β) .

Proof of Theorem 3. We follow the proof of Theorem 2. Since the row or column update is chosen
2n (ρ(r  r(A)) + ρ(c  c(A))). If ρ(r  r(A)) and
greedily  at each step we make progress of at least 1
ρ(c  c(A)) are both at most 1  then under the assumption that (cid:107)r(A) − r(cid:107)1 + (cid:107)c(A) − c(cid:107)1 > ε(cid:48)  our
progress is at least

1
2n

(ρ(r  r(A)) + ρ(c  c(A))) ≥ 1
14n

((cid:107)r(A) − r(cid:107)2

1 + (cid:107)c(A) − c(cid:107)2

1) ≥ 1
28n

ε(cid:48)2

Likewise  if either ρ(r  r(A)) or ρ(c  c(A)) is larger than 1  our progress is at least 1/2n ≥ 1
Therefore  we terminate in at most 28nε(cid:48)−2 log(s/(cid:96)) iterations.

28n ε(cid:48)2.

4 Proof of Theorem 1

First  we present a simple guarantee about the rounding Algorithm 2. The following lemma shows that
the (cid:96)1 distance between the input matrix F and rounded matrix G = ROUND(F Ur c) is controlled
by the total-variation distance between the input matrix’s marginals r(F ) and c(F ) and the desired
marginals r and c.
Lemma 7. If r  c ∈ ∆n and F ∈ IRn×n
G ∈ Ur c satisfying
(cid:107)G − F(cid:107)1 ≤ 2

(cid:104)(cid:107)r(F ) − r(cid:107)1 + (cid:107)c(F ) − c(cid:107)1

+   then Algorithm 2 takes O(n2) time to output a matrix

(cid:105)

.

The proof of Lemma 7 is simple and left to the Supplement. (We also describe in the Supplement a
randomized variant of Algorithm 2 that achieves a slightly better bound than Lemma 7). We are now
ready to prove Theorem 1.
Proof of Theorem 1. ERROR ANALYSIS. Let B be the output of PROJ(A Ur c  ε(cid:48))  and let P ∗ ∈
argminP∈Ur c(cid:104)P  C(cid:105) be an optimal solution to the original OT program.
We ﬁrst show that (cid:104)B  C(cid:105) is not much larger than (cid:104)P ∗  C(cid:105). To that end  write r(cid:48) := r(B) and
c(cid:48) := c(B). Since B = XAY for positive diagonal matrices X and Y   Lemma 1 implies B is the
optimal solution to

(3)
By Lemma 7  there exists a matrix P (cid:48) ∈ Ur(cid:48) c(cid:48) such that (cid:107)P (cid:48) − P ∗(cid:107)1 ≤ 2 ((cid:107)r(cid:48) − r(cid:107)1 + (cid:107)c(cid:48) − c(cid:107)1).
Moreover  since B is an optimal solution of (3)  we have

min
P∈Ur(cid:48)  c(cid:48)

(cid:104)P  C(cid:105) − η−1H(P ) .

(cid:104)B  C(cid:105) − η−1H(B) ≤ (cid:104)P (cid:48)  C(cid:105) − η−1H(P (cid:48)) .

Thus  by Hölder’s inequality

(cid:104)B  C(cid:105) − (cid:104)P ∗  C(cid:105) = (cid:104)B  C(cid:105) − (cid:104)P (cid:48)  C(cid:105) + (cid:104)P (cid:48)  C(cid:105) − (cid:104)P ∗  C(cid:105)

≤ η−1(H(B) − H(P (cid:48))) + 2((cid:107)r(cid:48) − r(cid:107)1 + (cid:107)c(cid:48) − c(cid:107)1)(cid:107)C(cid:107)∞
≤ 2η−1 log n + 2((cid:107)r(cid:48) − r(cid:107)1 + (cid:107)c(cid:48) − c(cid:107)1)(cid:107)C(cid:107)∞  

(4)

where we have used the fact that 0 ≤ H(B)  H(P (cid:48)) ≤ 2 log n.
Lemma 7 implies that the output ˆP of ROUND(B Ur c) satisﬁes the inequality (cid:107)B − ˆP(cid:107)1 ≤
2 ((cid:107)r(cid:48) − r(cid:107)1 + (cid:107)c(cid:48) − c(cid:107)1). This fact together with (4) and Hölder’s inequality yields
(cid:104)P  C(cid:105) + 2η−1 log n + 4((cid:107)r(cid:48) − r(cid:107)1 + (cid:107)c(cid:48) − c(cid:107)1)(cid:107)C(cid:107)∞ .

(cid:104) ˆP   C(cid:105) ≤ min
P∈Ur c

Applying the guarantee of PROJ(A Ur c  ε(cid:48))  we obtain

(cid:104) ˆP   C(cid:105) ≤ min
P∈Ur c

(cid:104)P  C(cid:105) +

2 log n

η

+ 4ε(cid:48)(cid:107)C(cid:107)∞ .

7

Plugging in the values of η and ε(cid:48) prescribed in Algorithm 1 ﬁnishes the error analysis.
RUNTIME ANALYSIS. Lemma 7 shows that Step 2 of Algorithm 1 takes O(n2) time. The runtime
of Step 1 is dominated by the PROJ(A Ur c  ε(cid:48)) subroutine. Theorems 2 and 3 imply that both the
SINKHORN and GREENKHORN algorithms accomplish this in S = O(n2(ε(cid:48))−2 log s
(cid:96) ) time  where s
is the sum of the entries of A and (cid:96) is the smallest entry of A. Since the matrix C is nonnegative 
the entries of A are bounded above by 1  thus s ≤ n2. The smallest entry of A is e−η(cid:107)C(cid:107)∞  so
log 1/(cid:96) = η(cid:107)C(cid:107)∞. We obtain S = O(n2(ε(cid:48))−2(log n+η(cid:107)C(cid:107)∞)). The proof is ﬁnished by plugging
in the values of η and ε(cid:48) prescribed in Algorithm 1.

5 Empirical results

Cuturi [Cut13] already gave experimental evidence that using
SINKHORN to solve (2) outperforms state-of-the-art techniques for
optimal transport. In this section  we provide strong empirical ev-
idence that our proposed GREENKHORN algorithm signiﬁcantly out-
performs SINKHORN.
We consider transportation between pairs of m×m greyscale images 
normalized to have unit total mass. The target marginals r and c
represent two images in a pair  and C ∈ IRm2×m2 is the matrix of
(cid:96)1 distances between pixel locations. Therefore  we aim to compute
the earth mover’s distance.
We run experiments on two datasets: real images  from MNIST  and synthetic images  as in Figure 1.

Figure 1: Synthetic image.

5.1 MNIST

We ﬁrst compare the behavior of GREENKHORN and SINKHORN on real images. To that end  we
choose 10 random pairs of images from the MNIST dataset  and for each one analyze the performance
of APPROXOT when using both GREENKHORN and SINKHORN for the approximate projection step.
We add negligible noise 0.01 to each background pixel with intensity 0. Figure 2 paints a clear
picture: GREENKHORN signiﬁcantly outperforms SINKHORN both in the short and long term.

5.2 Random images

To better understand the empirical
behavior of both algorithms in a
number of different regimes  we de-
vised a synthetic and tunable frame-
work whereby we generate images
by choosing a randomly positioned
“foreground” square in an otherwise
black background. The size of this
square is a tunable parameter var-
ied between 20%  50%  and 80% of
the total image’s area.
Intensities
of background pixels are drawn uni-
formly from [0  1]; foreground pix-
els are drawn uniformly from [0  50].
Such an image is depicted in Figure 1 
and results appear in Figure 2.
We perform two other experiments
with random images in Figure 3.
In the ﬁrst  we vary the number
of background pixels and show that
GREENKHORN performs better when
the number of background pixels is
larger. We conjecture that this is related to the fact that GREENKHORN only updates salient rows and

Figure 2: Comparison of GREENKHORN and SINKHORN
on pairs of MNIST images of dimension 28 × 28 (top) and
random images of dimension 20 × 20 with 20% foreground
(bottom). Left: distance dist(A Ur c) to the transport poly-
tope (average over 10 random pairs of images). Right: maxi-
mum  median  and minimum values of the competitive ratio
ln (dist(AS Ur c)/dist(AG Ur c)) over 10 runs.

8

columns at each step  whereas SINKHORN wastes time updating rows and columns corresponding to
background pixels  which have negligible impact. This demonstrates that GREENKHORN is a better
choice especially when data is sparse  which is often the case in practice.
In the second  we consider the role of the regularization parameter η. Our analysis requires taking η
of order log n/ε  but Cuturi [Cut13] observed that in practice η can be much smaller. Cuturi showed
that SINKHORN outperforms state-of-the art techniques for computing OT distance even when η is a
small constant  and Figure 3 shows that GREENKHORN runs faster than SINKHORN in this regime
with no loss in accuracy.

Figure 3: Left: Comparison of median competitive ratio for random images containing 20%  50% 
and 80% foreground. Right: Performance of GREENKHORN and SINKHORN for small values of η.

9

Acknowledgments

We thank Michael Cohen  Adrian Vladu  John Kelner  Justin Solomon  and Marco Cuturi for helpful
discussions. We are grateful to Pablo Parrilo for drawing our attention to the fact that GREENKHORN
is a coordinate descent algorithm  and to Alexandr Andoni for references.
JA and JW were generously supported by NSF Graduate Research Fellowship 1122374. PR is
supported in part by grants NSF CAREER DMS-1541099  NSF DMS-1541100  NSF DMS-1712596 
DARPA W911NF-16-1-0551  ONR N00014-17-1-2147 and a grant from the MIT NEC Corporation.

References
M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein GAN. ArXiv:1701.07875  January 2017.
[ACB17]
[ANOY14] A. Andoni  A. Nikolov  K. Onak  and G. Yaroslavtsev. Parallel algorithms for geometric graph
problems. In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of Computing 
STOC ’14  pages 574–583  New York  NY  USA  2014. ACM.
P. K. Agarwal and R. Sharathkumar. Approximation algorithms for bipartite matching with metric
and geometric costs. In Proceedings of the Forty-sixth Annual ACM Symposium on Theory of
Computing  STOC ’14  pages 555–564  New York  NY  USA  2014. ACM.

[AS14]

[AZLOW17] Z. Allen-Zhu  Y. Li  R. Oliveira  and A. Wigderson. Much faster algorithms for matrix scaling.

[BCC+15]

[BGKL17]

[Bub15]

arXiv preprint arXiv:1704.02315  2017.
J.-D. Benamou  G. Carlier  M. Cuturi  L. Nenna  and G. Peyré. Iterative Bregman projections for
regularized transportation problems. SIAM Journal on Scientiﬁc Computing  37(2):A1111–A1138 
2015.
J. Bigot  R. Gouet  T. Klein  and A. López. Geodesic PCA in the Wasserstein space by convex
PCA. Ann. Inst. H. Poincaré Probab. Statist.  53(1):1–26  02 2017.
S. Bubeck. Convex optimization: Algorithms and complexity. Found. Trends Mach. Learn. 
8(3-4):231–357  2015.

[BvdPPH11] N. Bonneel  M. van de Panne  S. Paris  and W. Heidrich. Displacement interpolation using

Lagrangian mass transport. ACM Trans. Graph.  30(6):158:1–158:12  December 2011.
N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 
Cambridge  2006.

[CBL06]

[CMTV17] M. B. Cohen  A. Madry  D. Tsipras  and A. Vladu. Matrix scaling and balancing via box

[Cut13]

[FCCR16]

constrained Newton’s method and interior point methods. arXiv:1704.02310  2017.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. J. C. Burges 
L. Bottou  M. Welling  Z. Ghahramani  and K. Q. Weinberger  editors  Advances in Neural
Information Processing Systems 26  pages 2292–2300. Curran Associates  Inc.  2013.
R. Flamary  M. Cuturi  N. Courty  and A. Rakotomamonjy. Wasserstein discriminant analysis.
arXiv:1608.08063  2016.

[GD04]

[GY98]

[IT03]

[GPC15]

[GCPB16] A. Genevay  M. Cuturi  G. Peyré  and F. Bach. Stochastic optimization for large-scale optimal
transport. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett  editors  Advances
in Neural Information Processing Systems 29  pages 3440–3448. Curran Associates  Inc.  2016.
K. Grauman and T. Darrell. Fast contour matching using approximate earth mover’s distance. In
Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition  2004. CVPR 2004.  volume 1  pages I–220–I–227 Vol.1  June 2004.
A. Gramfort  G. Peyré  and M. Cuturi. Fast Optimal Transport Averaging of Neuroimaging Data 
pages 261–272. Springer International Publishing  2015.
L. Gurvits and P. Yianilos. The deﬂation-inﬂation method for certain semideﬁnite programming
and maximum determinant completion problems. Technical report  NECI  1998.
P. Indyk and N. Thaper. Fast image retrieval via embeddings. In Third International Workshop on
Statistical and Computational Theories of Vision  2003.
A. Juditsky  P. Rigollet  and A. Tsybakov. Learning by mirror averaging. Ann. Statist.  36(5):2183–
2206  2008.

[JRT08]

[JSCG16] W. Jitkrittum  Z. Szabó  K. P. Chwialkowski  and A. Gretton. Interpretable distribution features
with maximum testing power. In Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems 2016  December 5-10  2016  Barcelona 
Spain  pages 181–189  2016.

10

[KK93]

[KK96]

[KLRS08]

[Leo14]

[LG15]

[LS14]

[MJ15]

[PW09]

[PZ16]

[Ren88]

[RT11]

[RT12]

[RTG00]

[SA12]

[Sch31]

[SdGP+15]

[Sin67]

[SJ08]

[SL11]

[SR04]

[ST04]

[Vil09]

[WPR85]

B. Kalantari and L. Khachiyan. On the rate of convergence of deterministic and randomized RAS
matrix scaling algorithms. Oper. Res. Lett.  14(5):237–244  1993.
B. Kalantari and L. Khachiyan. On the complexity of nonnegative-matrix scaling. Linear Algebra
Appl.  240:87–103  1996.
B. Kalantari  I. Lari  F. Ricca  and B. Simeone. On the complexity of general matrix scaling and
entropy minimization via the RAS algorithm. Math. Program.  112(2  Ser. A):371–401  2008.
C. Leonard. A survey of the Schrödinger problem and some of its connections with optimal
transport. Discrete and Continuous Dynamical Systems  34(4):1533–1574  2014.
J. R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. In
Proceedings of the 28th International Conference on Neural Information Processing Systems 
NIPS’15  pages 829–837  Cambridge  MA  USA  2015. MIT Press.
Y. T. Lee and A. Sidford. Path ﬁnding methods for linear programming: Solving linear programs
in Õ(√rank) iterations and faster algorithms for maximum ﬂow. In Proceedings of the 2014
IEEE 55th Annual Symposium on Foundations of Computer Science  FOCS ’14  pages 424–433 
Washington  DC  USA  2014. IEEE Computer Society.
J. Mueller and T. Jaakkola. Principal differences analysis: Interpretable characterization of
differences between distributions. In Proceedings of the 28th International Conference on Neural
Information Processing Systems  NIPS’15  pages 1702–1710  Cambridge  MA  USA  2015. MIT
Press.
O. Pele and M. Werman. Fast and robust earth mover’s distances. In 2009 IEEE 12th International
Conference on Computer Vision  pages 460–467  Sept 2009.
V. M. Panaretos and Y. Zemel. Amplitude and phase variation of point processes. Ann. Statist. 
44(2):771–812  04 2016.
J. Renegar. A polynomial-time algorithm  based on Newton’s method  for linear programming.
Mathematical Programming  40(1):59–93  1988.
P. Rigollet and A. Tsybakov. Exponential screening and optimal rates of sparse estimation. Ann.
Statist.  39(2):731–771  2011.
P. Rigollet and A. Tsybakov. Sparse estimation by exponential weighting. Statistical Science 
27(4):558–575  2012.
Y. Rubner  C. Tomasi  and L. J. Guibas. The earth mover’s distance as a metric for image retrieval.
Int. J. Comput. Vision  40(2):99–121  November 2000.
R. Sharathkumar and P. K. Agarwal. A near-linear time -approximation algorithm for geometric
bipartite matching. In H. J. Karloff and T. Pitassi  editors  Proceedings of the 44th Symposium on
Theory of Computing Conference  STOC 2012  New York  NY  USA  May 19 - 22  2012  pages
385–394. ACM  2012.
E. Schrödinger. Über die Umkehrung der Naturgesetze. Angewandte Chemie  44(30):636–636 
1931.
J. Solomon  F. de Goes  G. Peyré  M. Cuturi  A. Butscher  A. Nguyen  T. Du  and L. Guibas.
Convolutional wasserstein distances: Efﬁcient optimal transportation on geometric domains. ACM
Trans. Graph.  34(4):66:1–66:11  July 2015.
R. Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The
American Mathematical Monthly  74(4):402–405  1967.
S. Shirdhonkar and D. W. Jacobs. Approximate earth mover’s distance in linear time. In 2008
IEEE Conference on Computer Vision and Pattern Recognition  pages 1–8  June 2008.
R. Sandler and M. Lindenbaum. Nonnegative matrix factorization with earth mover’s distance
metric for image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 
33(8):1590–1602  Aug 2011.
G. J. Székely and M. L. Rizzo. Testing for equal distributions in high dimension. Inter-Stat
(London)  11(5):1–16  2004.
D. A. Spielman and S.-H. Teng. Nearly-linear time algorithms for graph partitioning  graph
In Proceedings of the Thirty-sixth Annual ACM
sparsiﬁcation  and solving linear systems.
Symposium on Theory of Computing  STOC ’04  pages 81–90  New York  NY  USA  2004. ACM.
C. Villani. Optimal transport  volume 338 of Grundlehren der Mathematischen Wissenschaften
[Fundamental Principles of Mathematical Sciences]. Springer-Verlag  Berlin  2009. Old and new.
M. Werman  S. Peleg  and A. Rosenfeld. A distance metric for multidimensional histograms.
Computer Vision  Graphics  and Image Processing  32(3):328 – 336  1985.

11

,Jason Altschuler
Jonathan Niles-Weed
Philippe Rigollet