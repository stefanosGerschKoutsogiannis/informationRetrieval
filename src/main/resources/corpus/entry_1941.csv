2012,Exponential Concentration for Mutual Information Estimation with Application to Forests,We prove a new exponential concentration inequality for a plug-in estimator of the Shannon mutual information. Previous results on mutual information estimation only bounded expected error. The advantage of having the exponential inequality is that  combined with the union bound  we can guarantee accurate estimators of the mutual information for many pairs of random variables simultaneously. As an application  we show how to use such a result to optimally estimate the density function and graph of a distribution which is Markov to a forest graph.,Exponential Concentration for Mutual Information

Estimation with Application to Forests

Han Liu

Department of Operations Research

and Financial Engineering

Princeton University  NJ 08544

John Lafferty

Department of Computer Science

Department of Statistics

University of Chicago  IL 60637

hanliu@princeton.edu

lafferty@galton.uchicago.edu

Larry Wasserman

Department of Statistics

Machine Learning Department

Carnegie Mellon University  PA 15231

larry@stat.cmu.edu

Abstract

We prove a new exponential concentration inequality for a plug-in estimator of the
Shannon mutual information. Previous results on mutual information estimation
only bounded expected error. The advantage of having the exponential inequality
is that  combined with the union bound  we can guarantee accurate estimators of
the mutual information for many pairs of random variables simultaneously. As an
application  we show how to use such a result to optimally estimate the density
function and graph of a distribution which is Markov to a forest graph.

(cid:90)

(cid:90)

(cid:18) p(x1  x2)

(cid:19)

Introduction

1
We consider the problem of nonparametrically estimating the Shannon mutual information between
two random variables. Let X1 ∈ X1 and X2 ∈ X2 be two random variables with domains X1 and
X2 and joint density p(x1  x2). The mutual information between X1 and X2 is

(cid:90) (cid:90)

dx1 dx2 = H(X1) + H(X2) − H(X1  X2) 

I(X1; X2) :=

p(x1  x2) log

X1

p(x1)p(x2)

X2
where H(X1  X2) = −
p(x1  x2) log p(x1  x2)dx1 dx2 (and similarly for H(X1) and H(X2))
are the corresponding Shannon entropies [4]. The mutual information is a measure of dependence
between X1 and X2. To estimate I(X1; X2) well  it sufﬁces to estimate H(X1  X2) := H(p).
A simple way to estimate the Shannon entropy is to use a kernel density estimator (KDE) [22  1  9 
5  20  7]  i.e.  the densities p(x  y)  p(x)  and p(y) are separately estimated from samples and the
estimated densities are used to calculate the entropy. Alternative methods involve estimation of the
entropies using spacings [25  26  23]  k-nearest neighbors [11  12]  the Edgeworth expansion [24] 
and convex optimization [17]. More discussions can be found in the survey articles [2  19]. There
have been many recent developments in the problem of estimating Shannon entropy and related
quantities as well as application of these results to machine learning problems [18  21  8  6]. Under
n-rate of
weak conditions  it has been shown that there are estimators that achieve the parametric
convergence in mean squared error (MSE)  where n is the sample size.
In this paper  we construct an estimator with this rate  but we also prove an exponential concentration

inequality for the estimator. More speciﬁcally  we show that our estimator (cid:98)H of H(p) satisﬁes

√

P(cid:16)|(cid:98)H − H(p)| > 

(cid:17) ≤ 2 exp

(cid:18)

(cid:19)

− n2
36κ2

sup
p∈Σ

(1.1)

1

where Σ is a nonparametric class of distributions deﬁned in Section 2 and κ is a constant. To
the best of our knowledge  this is the ﬁrst such exponential inequality for nonparametric Shannon
entropy and mutual information estimation. The advantage of this result  over the usual results which

state that E(cid:0)|(cid:98)H − H(p)|2(cid:1) = O(n−1)  is that we can apply the union bound and thus guarantee

2 mutual informations in order to apply the Chow-Liu algorithm. As long as log d

accurate mutual information estimation for many pairs of random variables simultaneously. As an
application  we consider forest density estimation [15]  which  in a d-dimenionsal problem  requires
n →
estimating d(d+1)
0 as n → ∞  we can estimate the forest graph well  even if d = d(n) increases with n exponentially
fast.
The rest of this paper is organized as follows. The assumptions and estimator are given in Section 2.
The main theoretical analysis is in Section 3. In Section 4 we show how to apply the result to forest
density estimation. Some discussion and possible extensions are provided in the last section.
2 Estimator and Main Result
Let X = (X1  X2) ∈ R2 be a random vector with density p(x) := p(x1  x2) and let x1  . . .   xn ∈
X ⊂ R2 be a random sample from p. In this paper  we only consider the case of bounded domain
X = [0  1]2. We want to estimate the Shannon entropy

H(p) = −

p(x) log p(x)dx.

(2.1)

(cid:90)

X

We start with some assumptions on the density function p(x1  x2).
Assumption 2.1 (Density assumption). We assume the density p(x1  x2) belongs to a 2nd-order
H¨older class Σκ(2  L) and is bounded away from zero and inﬁnity. In particular  there exist constants
κ1  κ2

0 < κ1 ≤ min

x∈X p(x) ≤ max

x∈X p(x) ≤ κ2 < ∞ 

(2.2)

and for any (x1  x2)T ∈ X   there exists a constant L such that  for any (u  v)T ∈ X

(cid:12)(cid:12)(cid:12)p(x1 + u  x2 + v) − p(x1  x2) − ∂p(x1  x2)

(2.3)
Assumption 2.2 (Boundary assumption). If {xn} ∈ X is any sequence converging to a boundary
point x∗  we require the density p(x) has vanishing ﬁrst order partial derivatives:

∂x1

∂x2

v

u − ∂p(x1  x2)

(cid:12)(cid:12)(cid:12) ≤ L(u2 + v2).

lim
n→∞

∂p(xn)

∂x1

= lim
n→∞

∂p(xn)

∂x2

= 0.

(2.4)

(cid:101)ph(x1  x2) :=

To efﬁciently estimate the entropy in (2.1)  we use a KDE based “plug-in” estimator. Bias at the
boundaries turns out to be very important in this problem; see [10] for a discussion of boundary
bias. To correct the boundary effects  we use the following “mirror image” kernel density estimator:

K

h

h

1

1

2

2

K

K

i=1

+K

+ K

(cid:19)
(cid:19)

(cid:40)
(cid:18) x1 + xi
(cid:18) x2 − xi
(cid:19)
(cid:18) x1 − xi
n(cid:88)
(cid:18) x1 + xi
(cid:18) x2 + xi
(cid:19)
(cid:18) x1 − xi
(cid:19)
(cid:18) x1 + xi
(cid:19)
(cid:18) x2 − 2 + xi
(cid:18) x1 − xi
(cid:19)
(cid:18) x1 − 2 + xi
(cid:18) x2 − xi
(cid:19)
(cid:19)
(cid:18) x1 − 2 + xi
(cid:18) x1 − 2 + xi
(cid:19)

(cid:18) x2 − xi
(cid:19)
(cid:19)
(cid:18) x2 + xi
(cid:19)
(cid:19)
(cid:18) x2 − 2 + xi
(cid:19)
(cid:18) x2 + xi
(cid:19)
(cid:19)
(cid:19)(cid:41)
(cid:18) x2 − 2 + xi

+ K

+ K

+ K

K

K

K

K

2

2

2

2

1

1

h

h

h

h

h

h

1

K

1

K

2

h

1

h

1

h

h

h

h

h

h

2

1
nh2

+K

+K

(2.5)
Here h is the bandwidth and K(·) is a univariate kernel function. We denote by K2(u  v) :=
K(u)K(v) the bivariate product kernel. This estimator has nine terms; one corresponds to the
original data in the unit square [0  1]2  and each of the remaining terms corresponds to reﬂecting the
data across one of the four sides or four corners of the square.

+K

K

h

h

.

2

1

2

Assumption 2.3 (Kernel assumption). The kernel K(·) is nonnegative and has a bounded support
[−1  1] with

K(u)du = 1 and

uK(u)du = 0.

(cid:90) 1

−1

(cid:90) 1

−1

(cid:90)

H ((cid:98)ph) := −

By Assumption 2.1  the values of the true density lie in the interval [κ1  κ2]. We propose a clipped
KDE estimator

(2.6)
where Tκ1 κ2(a) = κ1 · I(a < κ1) + a · I(κ1 ≤ a ≤ κ2) + κ2 · I(a > κ2)  so that the estimated
density also has this property. Letting g(u) = u log u  we propose the following plug-in entropy
estimator:

(cid:98)ph(x) = Tκ1 κ2 ((cid:101)ph(x))  
g ((cid:98)ph(x)) dx = −

X

Remark 2.1. The clipped estimator(cid:98)ph requires the knowledge of κ1 and κ2. In applications  we do
Our main technical result is the following exponential concentration inequality on H((cid:98)ph) around the

not need to know the exact values of κ1 and κ2; lower and upper bounds are sufﬁcient.

population quantity H(p). Our proof is given in Section 3.
Theorem 2.1. Under Assumptions 2.1  2.2  and 2.3  if we choose the bandwidth according to h (cid:16)
n−1/4  then there exists a constant N0 such that for all n > N0 

(2.7)

(cid:90)
X (cid:98)ph(x) log(cid:98)ph(x)dx.

P(cid:16)|H ((cid:98)ph) − H (p)| > 

(cid:17) ≤ 2 exp

(cid:18)

(cid:19)

− n2
36κ2

sup

p∈Σκ(2 L)

 

(2.8)

√

n-rate of convergence in mean squared error  E(cid:0)|(cid:98)H − H(p)|(cid:1) = O(n−1/2). The

where κ = max{| log κ1| | log κ2|} + 1.
To the best of our knowledge  this is the ﬁrst time an exponential inequality like (2.8) has been
established for Shannon entropy estimation over the H¨older class. It is easy to see that (2.8) implies
the parametric
bandwidth h (cid:16) n−1/4 in the above theorem is different from the usual choice for optimal bivariate
density estimation  which is hP (cid:16) n−1/6 for the 2nd-order H¨older class. By using h (cid:16) n−1/4 
we undersmooth the density estimate. As we show in the next section  such a bandwidth choice is
important for achieving the optimal rate for entropy estimation.
Let I(p) := I(X1; X2) be the Shannon mutual information  and deﬁne

(cid:90)

(cid:90)

I((cid:98)ph) :=

X1

X2

(cid:98)ph(x1  x2) log
P(cid:16)|I ((cid:98)ph) − I (p)| > 

(cid:19)

(cid:18) (cid:98)ph(x1  x2)
(cid:98)ph(x1)(cid:98)ph(x2)
(cid:18)
(cid:17) ≤ 6 exp

(cid:19)

− n2
324κ2

The next corollary provides an exponential inequality for Shannon mutual information estimation.
Corollary 2.1. Under the same conditions as in Theorem 2.1  if we choose h (cid:16) n−1/4  then there
exists a constant N1  such that for all n > N1 

dx1 dx2.

(2.9)

sup

p∈Σκ(2 L)

where κ = max{| log κ1| | log κ2|} + 1.

 

(2.10)

Proof. Using the same proof for Theorem 2.1  we can show that (2.8) also holds for estimating
univariate entropies H(X1) and H(X2). The desired result then follows from the union bound
since I(p) := I(X1; X2) = H(X1) + H(X2) − H(X1  X2).
Remark 2.2. We use the same bandwidth h (cid:16) n−1/4 to estimate the bivariate density p(x1  x2)
and univariate densities p(x1)  p(x2). A related result is presented in [15]. They consider the same
problem setting as ours and also use a KDE based plug-in estimator to estimate the mutual infor-
mation. However  unlike our proposal  they advocate the use of different bandwidths for bivariate
and univariate entropy estimations. For bivariate case they use h2 (cid:16) n−1/6; for univariate case they
use h1 (cid:16) n−1/5. Such bandwidths h1 and h2 are useful for optimally estimating the density func-
tions. However  such a choice achieves a suboptimal rate in terms of mutual information estimation:
supp∈Σκ(2 L)
Our method achieves the faster parametric rate.

(cid:17) ≤ c1 exp(cid:0)−c2n2/32(cid:1)  where c1 and c2 are two constants.

P(cid:16)|I ((cid:98)ph) − I (p)| > 

3

3 Theoretical Analysis

Here we present the detailed proof of Theorem 2.1. To analyze the error |H ((cid:98)ph) − H (p)|  we ﬁrst

decompose it into a bias or approximation error term  and a “variance” or estimation error term:

(cid:124)

Variance

|H ((cid:98)ph) − H (p)| ≤ |H ((cid:98)ph) − EH ((cid:98)ph)|
+|EH ((cid:98)ph) − H (p)|
(cid:124)
(cid:123)(cid:122)
(cid:123)(cid:122)
(cid:125)
(cid:125)
(cid:18)
(cid:17) ≤ 2 exp
P(cid:16)|H ((cid:98)ph) − EH ((cid:98)ph)|
(cid:124)
(cid:123)(cid:122)
(cid:125)
|EH ((cid:98)ph) − H (p)|
(cid:124)
(cid:123)(cid:122)
(cid:125)

≤ c1h2 +

− n2
32κ2

p∈Σκ(2 L)

p∈Σκ(2 L)

c3
nh2  

Variance

sup

sup

> 

Bias

Bias

.

(cid:19)

 

(3.1)

(3.2)

(3.3)

We are going to show that

where c1 and c3 are two constants. Since the bound on the variance in (3.2) does not depend on h 
to optimize the rate  we only need to choose h to minimize the righthand side of (3.3). Therefore
h (cid:16) n−1/4 achieves the optimal rate. In the rest of this section  we bound the bias and variance
terms separately.
3.1 Analyzing the Bias Term
Here we prove (3.3). Let u be a vector. We denote the sup norm by (cid:107)u(cid:107)∞. The next lemma bounds
the integrated squared bias of the kernel density estimator over the support X := [0  1]2.
Lemma 3.1. Under Assumptions 2.1  2.2  and 2.3  there exists a constant c > 0 such that

Proof. We partition the support X := [0  1]2 into three regions X = B ∪ C ∪ I  the boundary area
B  the corner area C  and the interior area I:

C = {x : (cid:107)x − u(cid:107)∞ ≤ h for u = (0  0)T   or (0  1)T   or (1  0)T   or (1  1)T} 
B = {x : x is within distance h to an edge of X   but does not belong to C} 
I = X \ (C ∪ B).

(3.5)
(3.6)
(3.7)

We have the following decomposition:

(E(cid:101)ph(x) − p(x))2 dx =

(cid:90)

(cid:90)

+

I

+

C

B

(cid:90)

X

(E(cid:101)ph(x) − p(x))2 dx = TI + TC + TB.

From standard results on kernel density estimation  we know that supp∈Σ(2 L) TI ≤ ch4. In the next
(E(cid:101)ph(x) − p(x))2 dx.
two subsections  we bound TB :=

(E(cid:101)ph(x) − p(x))2 dx and TC :=

(cid:90)

C

3.1.1 Analyzing TB
Let A := {x : 0 ≤ x1 ≤ h and h ≤ x2 ≤ 1 − h}. We have

For x ∈ A  we have

(cid:101)ph(x) =

1
nh2

TB =

(cid:20)

K

n(cid:88)

i=1

Therefore  for x ∈ A we have
1
h2

E(cid:101)ph(x) =

(cid:90)

B

(cid:90)
(E(cid:101)ph(x) − p(x))2 dx ≤ c
(cid:19)
(cid:18) x2 − xi
(cid:19)
(cid:18) x1 − xi
(cid:18) x1 − t1
(cid:19)
(cid:90) 1
(cid:90) 1

K

h

h

1

2

A

(cid:19)

(E(cid:101)ph(x) − p(x))2 dx.
(cid:18) x2 − xi
(cid:18) x1 + xi
(cid:19)
(cid:18) x2 − t2

K

h

h

1

2

p(t1  t2)dt1dt2

+ K

K

h

K

0

0

h

4

(3.8)

(cid:19)(cid:21)

.

(3.9)

(E(cid:101)ph(x) − p(x))2 dx ≤ ch4.

sup

p∈Σκ(2 L)

X

(3.4)

(cid:90)

(cid:90)
(cid:90)

B

(cid:18) x1 + t1

(cid:19)

h

(cid:18) x2 − t2

(cid:19)

h

K

K

p(t1  t2)dt1dt2

+

1
h2

(cid:90) 1

(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) − x1
(cid:90) 1

− x1

−1

0

0

h

h

+

−1

−1

=

K(u1)K(u2)p(x1 + u1h  x2 + u2h)du1du2

K(u1)K(u2)p(x1 − u1h  x2 − u2h)du1du2.

(3.10)

2]h2.

(cid:12)(cid:12)(cid:12)(cid:12) ∂p(x)
(cid:12)(cid:12)(cid:12)(cid:12) h +
(cid:12)(cid:12)(cid:12)(cid:12) ∂p(x)
(cid:12)(cid:12)(cid:12)(cid:12) h +
(cid:12)(cid:12)(cid:12)(cid:12) ∂p(x)
(cid:12)(cid:12)(cid:12)(cid:12) h + 10Lh2.
(cid:12)(cid:12)(cid:12)(cid:12) h +

∂x2

∂x2

Since p ∈ Σκ(2  L) and 0 < x1 ≤ h  we have

|p(x1−u1h  x2−u2h) − p(x1  x2) +

∂p(x)
∂x1

|p(x1 + u1h  x2 + u2h) − p(x1  x2) − (cid:104)(cid:53)p(x)  u(cid:105)h| ≤ L(cid:107)u(cid:107)2
(u2h)| ≤ L[(2 + u1)2 + u2

(2x1 +u1h) +

2h2 

∂p(x)
∂x2

∂x1

∂x1

(cid:90) 1

(cid:12)(cid:12)(cid:12)(cid:12) ∂p(x)
(cid:12)(cid:12)(cid:12)(cid:12)

Since |u1| |u2| ≤ 1  we have |p(x1 + u1h  x2 + u2h) − p(x1  x2)| ≤
L(cid:107)u(cid:107)2
For any x ∈ A  we can bound the bias term

2h2. Similarly  |p(x1 − u1h  x2 − u2h) − p(x1  x2)| ≤ 9
|E(cid:101)ph(x) − p(x)|
(cid:12)(cid:12)(cid:12)(cid:12)E(cid:101)ph(x) −
(cid:90) 1
(cid:90) 1
(cid:90) 1
K(u1)K(u2)(cid:12)(cid:12)p(x1 + u1h  x2 + u2h) − p(x1  x2)(cid:12)(cid:12)du1du2
(cid:90) 1
(cid:90) − x1
K(u1)K(u2)(cid:12)(cid:12)p(−u1h − x1  x2 − u2h) − p(x1  x2)(cid:12)(cid:12)du1du2
(cid:12)(cid:12)(cid:12)(cid:12) ∂p(x)
(cid:12)(cid:12)(cid:12)(cid:12) ∂p(x)
(cid:12)(cid:12)(cid:12)(cid:12) h + 2

K(u1)K(u2)p(t1  t2)du1du2

− x1

≤

−1

−1

−1

−1

−1

∂x2

+

=

h

h

(3.11)

(3.12)

(3.13)

(3.14)

(cid:12)(cid:12)(cid:12)(cid:12) h + 12Lh2
(cid:12)(cid:12)(cid:12) ∂p(x)

∂x1

≤ 10
≤ 12Lh2 + 12Lh2
= 24Lh2 

(cid:12)(cid:12)(cid:12)  

(cid:12)(cid:12)(cid:12) ∂p(x)

∂x2

(cid:12)(cid:12)(cid:12) ≤ Lh  by the H¨older condition

where the last inequality follows from the fact that
and the assumption that the density p(x) has vanishing partial derivatives on the boundary points.
Therefore  we have TB ≤ ch5.
3.1.2 Analyzing TC
Let A1 := {x : 0 ≤ x1  x2 ≤ h}. We now analyze the term TC:

∂x1

(cid:90)

C

TC =

(E(cid:101)ph(x) − p(x))2 dx ≤ c
(cid:18) x1 − a

Ux h(a  b) = K

h

(cid:90)
(cid:19)

A1

K

(E(cid:101)ph(x) − p(x))2 dx.
(cid:18) x2 − b

(cid:19)

.

h

(3.15)

(3.16)

2)(cid:3) . (3.17)

For notational simplicity  we write

For x ∈ A1  we have

n(cid:88)

(cid:2)Ux h(xi

1  xi
Therefore  for x ∈ A1 we have

i=1

1
nh2

(cid:101)ph(x) =
E(cid:101)ph(x)
(cid:90) 1

(cid:90) 1

=

1
h2

2) + Ux h(−xi

1  xi

2) + Ux h(xi

1 −xi

2) + Ux h(−xi

1 −xi

[Ux h(t1  t2) + Ux h(−t1  t2) + Ux h(t1 −t2) + Ux h(−t1 −t2)] p(t1  t2)dt1dt2

0

0

5

=

K(u1)K(u2)p(x1 + u1h  x2 + u2h)du1du2

K(u1)K(u2)p(u1h − x1  x2 + u2h)du1du2

K(u1)K(u2)p(u1h + x1 −x2 + u2h)du1du2

K(u1)K(u2)p(−x1 + u1h −x2 + u2h)du1du2.

Since K(·) is a symmetric kernel on [−1  1]  we have

h

h

h

h

+

+

x1
h

x2
h

− x1

− x2

− x1

− x2

(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1

− x2

− x1

x2
h

x1
h

x2
h

x1
h

+

h

h

(3.18)

(3.19)

(3.20)

(3.21)

(3.22)

(3.23)

(3.24)
(3.25)
(3.26)
(3.27)

(3.28)

(3.29)

(3.30)

(3.31)

(3.32)

(3.33)

(3.34)

h

h

−1

− x2

(cid:90) − x1
(cid:90) 1
(cid:90) 1
(cid:90) − x2
(cid:90) 1
(cid:90) 1
(cid:90) 1

− x1

−1

h

h

K(u1)K(u2)du1du2 =

K(u1)K(u2)du1du2 

K(u1)K(u2)du1du2 =

K(u1)K(u2)du1du2.

Therefore  for x = (x1  x2)T ∈ A1 

(cid:90) 1

(cid:90) 1

(cid:90) 1

p(x1  x2) =
Using the fact that p ∈ Σκ(2  L)  0 ≤ x1  x2 ≤ h  and −1 ≤ u1  u2 ≤ 1  we have

− x2

− x2

− x1

− x1

x1
h

x1
h

x2
h

x2
h

+

+

+

h

h

h

h

p(x1  x2)K(u1)K(u2)du1du2.

|p(x1 + u1h  x2 + u2h) − p(x1  x2)| ≤ 4Lh2 
|p(u1h − x1  x2 + u2h) − p(x1  x2)| ≤ 20Lh2 
|p(u1h + x1  u2h − x2) − p(x1  x2)| ≤ 20Lh2 
|p(u1h − x1  u2h − x2) − p(x1  x2)| ≤ 36Lh2.

For x ∈ A1  we can then bound the bias term as

(cid:90) 1

(cid:90) 1

(cid:12)(cid:12)(cid:12)(cid:12)

K(u1)K(u2)p(t1  t2)du1du2

−1

−1
K(u1)K(u2)|p(x1 + u1h  x2 + u2h) − p(x1  x2)|du1du2

K(u1)K(u2)|p(u1h − x1  x2 + u2h) − p(x1  x2)|du1du2

K(u1)K(u2)|p(u1h + x1  u2h − x2) − p(x1  x2)|du1du2

K(u1)K(u2)|p(u1h − x1  u2h − x2) − p(x1  x2)|du1du2

=

≤

|E(cid:101)ph(x) − p(x)|
(cid:12)(cid:12)(cid:12)(cid:12)E(cid:101)ph(x) −
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1
(cid:90) 1

− x2

− x1

− x2

− x1

x1
h

x2
h

+

+

h

h

h

h

+

x2
h

x1
h

≤ 80Lh2.

Therefore  we have TC ≤ ch6.
Combining the analysis of TB  TC  and TI  we show that the mirror image kernel density estimator
is free of boundary bias. Thus the desired result of Lemma 3.1 is proved.
3.1.3 Analyzing the Bias of the Entropy Estimator
Lemma 3.2. Under Assumptions 2.1  2.2  and 2.3  there exists a universal constant C∗ that does
not depend on the true density p  such that

(cid:12)(cid:12)(cid:12)EH ((cid:98)ph) − H(p)

(cid:12)(cid:12)(cid:12) ≤ C∗√

n

sup

p∈Σκ(2 L)

.

(3.35)

6

·(cid:104)(cid:98)ph(x) − p(x)
(cid:105)2

 

(3.36)

Let κ be as deﬁned in the statement of the theorem. Using Fubini’s theorem  H¨older’s inequality and
the fact that the Lebesgue measure of X is 1  we have

1

+

2ξ(x)

Proof. Recalling that g(u) = u log u  by Taylor’s theorem we have

g ((cid:98)ph(x)) − g (p(x)) =(cid:0)log(p(x)) + 1(cid:1) ·(cid:104)(cid:98)ph(x) − p(x)
(cid:105)
where ξ(x) lies in between(cid:98)ph(x) and p(x). It is obvious that κ1 ≤ ξ(x) ≤ κ2.
(cid:12)(cid:12)EH ((cid:98)ph) − H(p)(cid:12)(cid:12)
(cid:90)
(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)E
(cid:2)g ((cid:98)ph(x)) − g (p(x))(cid:3)dx
(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12)
E(cid:2)g ((cid:98)ph(x)) − g (p(x))(cid:3)dx
≤ (cid:12)(cid:12)(cid:12)(cid:90)
(cid:0)log(p(x)) + 1(cid:1) · E(cid:104)(cid:98)ph(x) − p(x)
(cid:105)
(cid:115)(cid:90)
(cid:90)
(cid:104)E(cid:98)ph(x) − p(x)
(cid:105)2
(cid:115)(cid:90)
(cid:90)
(cid:104)E(cid:101)ph(x) − p(x)
(cid:105)2

(cid:12)(cid:12)(cid:12)(cid:90)
(cid:12)(cid:12)(cid:12) +
E(cid:104)(cid:98)ph(x) − p(x)
E(cid:104)(cid:101)ph(x) − p(x)

≤ κ

≤ κ

1
2κ1

2ξ(x)

dx +

dx +

dx.

dx

=

=

X

X

X

X

·

·

X

1

1
2κ1

X

X

X

· E(cid:104)(cid:98)ph(x) − p(x)
(cid:105)2
(cid:105)2
(cid:105)2

dx

(3.37)

(3.38)

(3.39)

(cid:12)(cid:12)(cid:12)

(3.40)

(3.41)

dx

≤ c1h2 + c2h4 +

c3
nh2 .

(3.42)
The last inequality follows from standard results of kernel density estimation and Lemma 3.1  where
c1  c2  c3 are three constants. We get the desired result by setting h (cid:16) n−1/4.
3.2 Analyzing the Variance Term
Lemma 3.3. Under Assumptions 2.1  2.2  and 2.3  we have 

(cid:19)

(cid:18)

− n2
32κ2

.

(3.43)

sup

p∈Σκ(2 L)

Proof. Let (cid:98)p(cid:48)
max(cid:2)|g(cid:48) ((cid:98)ph(x))|  |g(cid:48) ((cid:98)p(cid:48)

h(x) be the kernel density estimator deﬁned as in (2.6) but with the jth data point
xj replaced by an arbitrary value (xj)(cid:48). Since g(cid:48)(u) = log u + 1  by Assumption 2.1  we have

For notational simplicity  we write the product kernel as K2 = K·K. Using the mean-value theorem
and the fact that Tκ1 κ2 (·) is a contraction  we have

sup

x1 ... xn (xj )(cid:48)

=

sup

x1 ... xn (xj )(cid:48)

(cid:17) ≤ 2 exp

P(cid:16)|H ((cid:98)ph) − EH ((cid:98)ph)| > 
h(x))|(cid:3) ≤ κ.
|H ((cid:98)ph) − H ((cid:98)p(cid:48)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)
h)|
(cid:90)
(cid:90)
(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)
h(x))(cid:3)dx
(cid:2)g ((cid:98)ph(x)) − g ((cid:98)p(cid:48)
|(cid:98)ph(x) −(cid:98)p(cid:48)
|Tκ1 κ2 [(cid:101)ph(x)] − Tκ1 κ2 [(cid:101)p(cid:48)
(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:18) y − x

(cid:18) xj − x
(cid:19)

h(x)| dx

nh2 K2

(cid:19)

(cid:90)

− 1

sup

sup

sup

X

X

X

h

dx

≤ κ

= κ

≤ 4κ

x1 ... xn (xj )(cid:48)

x1 ... xn (xj )(cid:48)

x1 ... xn (xj )(cid:48)

≤ 8κ sup

1
nh2 K2

X

(cid:90)

y

K2(u)du =

≤ 8κ
n

h

8κ
n

.

(3.44)

(3.45)

(3.46)

(3.47)

(3.48)

(3.49)

(3.50)

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12) dx

h(x)]| dx

(cid:18) (xj)(cid:48) − x

nh2 K2

h

Therefore  using McDiarmaid’s inequality [16]  we get the desired inequality (3.43). The uniformity
result holds since the constant does not depend on the true density p.

7

4 Application to Forest Density estimation
We apply the concentration inequality (2.10) to analyze an algorithm for learning high dimen-
sional forest graph models [15]. In a forest density estimation problem  we observe n data points
x1  . . .   xn ∈ Rd from a d-dimensional random vector X. We have two learning tasks: (i) we want
to estimate an acyclic undirected graph F = (V  E)  where V is the vertex set containing all the
random variables and E is the edge set such that an edge (j  k) ∈ E if and only if the corresponding
random variables Xj and Xk are conditionally independent given the other variables X\{j k}; (ii)

once we have an estimated graph (cid:98)F   we want to estimate the density function p(x).
these two variables. Empirically  we replace I(Xj; Xk) by its estimate(cid:98)I(Xj; Xk) from (2.9). The

Using the negative log-likelihood loss  Liu et al. [15] show that the graph estimation problem can be
recast as the problem of ﬁnding the maximum weight spanning forest for a weighted graph  where
the weight of the edge connecting nodes j and k is I(Xj; Xk)  the mutual information between

forest graph can be obtained by the Chow-Liu algorithm [3  13]  which is an iterative algorithm. At
each iteration the algorithm adds an edge connecting that pair of variables with maximum mutual
information among all pairs not yet visited by the algorithm  if doing so does not form a cycle. When
stopped early  after s < d− 1 edges have been added  it yields the best s-edge weighted forest. Once

a forest graph (cid:98)F = (V (cid:98)E) is estimated  we propose to estimate the forest density as
(cid:101)ph1 (x(cid:96)) 
where (cid:98)U is the set of isolated vertices in the estimated forest (cid:98)F . Our estimator is different from the
estimator proposed by [15]—once the graph (cid:98)F is given  we treat the isolated variables differently

(cid:101)ph2(xu) · (cid:89)
(cid:96)∈V \(cid:98)U

(cid:101)ph2 (xj  xk)
(cid:101)ph2 (xj)(cid:101)ph2(xk)

(cid:89)
(j k)∈(cid:98)E

(cid:98)p(cid:98)F (x) =

· (cid:89)
u∈(cid:98)U

than the connected variables. As will be shown in Theorem 4.2  such a choice leads to minimax
optimal forest density estimation  while the obtained rate from [15] is suboptimal.
Let F s
Kullback-Leibler divergence. We deﬁne the s-oracle forest F ∗
oracle density estimator pF ∗ to be

d denote the set of forest graphs with d nodes and no more than s edges. Let D(·(cid:107)·) be the
s := (V  E∗) and its corresponding

(4.1)

F ∗
s = arg min
F∈F s

D(p(cid:107)pF )

and pF ∗ :=

d

(j k)∈E∗

p(x(cid:96)).
Let Σκ(2  L) be deﬁned as in Assumption (2.1). We deﬁne a density class Pκ as

Pκ :=(cid:8)p : p is a d-dimensional density with p(xj  xk) ∈ Σκ(2  L) for any j (cid:54)= k(cid:9).

Theorem 4.1 (Graph Recovery). Let (cid:98)F be the estimated s-edge forest graph using the Chow-Liu

(4.3)
The next two theorems show that the above forest density estimation procedure is minimax optimal
for both graph recovery and density estimation. Their proofs are provided in a technical report [14].
algorithm. Under the same condition as Theorem 12 in [15]  If we choose h (cid:16) n−1/4 for the mutual
information estimator in (2.9)  then

(cid:96)∈V

(4.2)

sup
p∈Pκ

Theorem 4.2 (Density Estimation). Once the s-edge forest graph (cid:98)F as in Theorem 4.1 has been

obtained  we calculate the density estimator (B.1) by choosing h1 (cid:16) n−1/5 and h2 (cid:16) n−1/6. Then 

(4.4)

= O

s

whenever log d
n

→ 0.

(cid:17)

(cid:19)

(cid:18)(cid:114) s

P(cid:16)(cid:98)F (cid:54)= F ∗
(cid:90)
(cid:12)(cid:12)(cid:98)p(cid:98)F (x) − pF ∗ (x)(cid:12)(cid:12) dx ≤ C ·

n

E

(cid:114)

(cid:89)

(cid:89)

p(xj  xk)
p(xj  xk)

sup
p∈Pκ

X

s

n2/3

+

d − s
n4/5

.

(4.5)

5 Discussions and Conclusions
Theorem 4.1 allows d to increase exponentially fast as n increases and still guarantees graph recov-
ery consistency. Theorem 4.2 provides the rate of convergence for the L1-risk. The obtained rate
is minimax optimal over the class Pκ. The term sn−2/3 corresponds to the price paid to estimate
bivariate densities; while the term (d − s)n−4/5 corresponds to the price paid to estimate univari-
ate densities. In this way  we see that the exponential concentration inequality for Shannon mutual
information leads to signiﬁcantly improved theoretical analysis of the forest density estimation  in
terms of both graph estimation and density estimation. This research was supported by NSF grant
IIS-1116730 and AFOSR contract FA9550-09-1-0373.

8

References
[1] Ibrahim A. Ahmad and Pi-Erh Lin. A nonparametric estimation of the entropy for absolutely continuous

distributions (corresp.). IEEE Transactions on Information Theory  22(3):372–375  1976.

[2] J Beirlant  E J Dudewicz  L Gy¨orﬁ  and E C Van Der Meulen. Nonparametric entropy estimation: An

overview. International Journal of Mathematical and Statistical Sciences  6(1):17–39  1997.

[3] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. Information

Theory  IEEE Transactions on  14(3):462–467  1968.

[4] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley  1991.
[5] Paul P. B. Eggermont and Vincent N. LaRiccia. Best asymptotic normality of the kernel density entropy

estimator for smooth densities. IEEE Transactions on Information Theory  45(4):1321–1326  1999.

[6] A. Gretton  R. Herbrich  and A. J. Smola. The kernel mutual information. In Acoustics  Speech  and
Signal Processing  2003. Proceedings.(ICASSP’03). 2003 IEEE International Conference on  volume 4 
pages IV–880. IEEE  2003.

[7] Peter Hall and Sally Morton. On the estimation of entropy. Annals of the Institute of Statistical Mathe-

matics  45(1):69–88  1993.

[8] A. O. Hero III  B. Ma  O. J. J. Michel  and J. Gorman. Applications of entropic spanning graphs. Signal

Processing Magazine  IEEE  19(5):85–95  2002.

[9] Harry Joe. Estimation of entropy and other functionals of a multivariate density. Annals of the Institute

of Statistical Mathematics  41(4):683–697  December 1989.

[10] M. C. Jones  M. C. Linton  and J. P. Nielsen. A simple bias reduction method for density estimation.

Biometrika  82(2):327–338  1995.

[11] Shiraj Khan  Sharba Bandyopadhyay  Auroop R. Ganguly  Sunil Saigal  David J. Erickson  Vladimir
Protopopescu  and George Ostrouchov. Relative performance of mutual information estimation methods
for quantifying the dependence among short and noisy data. Phys. Rev. E  76:026209  Aug 2007.

[12] Alexander Kraskov  Harald St¨ogbauer  and Peter Grassberger. Estimating mutual information. Physical

review. E  Statistical  nonlinear  and soft matter physics  69(6 Pt 2)  June 2004.

[13] Joseph B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem.

Proceedings of the American Mathematical Society  7(1):48–50  1956.

[14] Han Liu  John Lafferty  and Larry Wasserman. Optimal forest density estimation. Technical Report  2012.
[15] Han Liu  Min Xu  Haijie Gu  Anupam Gupta  John D. Lafferty  and Larry A. Wasserman. Forest density

estimation. Journal of Machine Learning Research  12:907–951  2011.

[16] C. McDiarmid. On the method of bounded differences.

In Surveys in Combinatorics  number 141 in
London Mathematical Society Lecture Note Series  pages 148–188. Cambridge University Press  August
1989.

[17] XuanLong Nguyen  Martin J. Wainwright  and Michael I. Jordan. Estimating divergence function-
IEEE Transactions on Information Theory 

als and the likelihood ratio by convex risk minimization.
56(11):5847–5861  2010.

[18] D. P´al  B. P´oczos  and C. Szepesv´ari. Estimation of R´enyi entropy and mutual information based on

generalized nearest-neighbor graphs. Arxiv preprint arXiv:1003.1954  2010.

[19] L. Paninski. Estimation of entropy and mutual information. Neural Computation  15(6):1191–1253 

2003.

[20] Liam Paninski and Masanao Yajima. Undersmoothed kernel entropy estimators. IEEE Transactions on

Information Theory  54(9):4384–4388  2008.

[21] Barnab´as P´oczos and Jeff G. Schneider. Nonparametric estimation of conditional information and diver-

gences. Journal of Machine Learning Research - Proceedings Track  22:914–923  2012.

[22] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall. New York 

NY  1986.

[23] A.B. Tsybakov and van den Meulen. Root-n Consistent Estimators of Entropy for Densities with Un-

bounded Support  volume 23. Universite catholique de Louvain Institut de statistique  1994.

[24] Marc M. Van Hulle. Edgeworth approximation of multivariate differential entropy. Neural Comput. 

17(9):1903–1910  September 2005.

[25] O Vasicek. A test for normality based on sample entropy. Journal of the Royal Statistical Society Series

B  38(1):54–59  1976.

[26] Ven Es Bert. Estimating functionals related to a density by a class of statistics based on spacings. Scan-

dinavian Journal of Statistics  19(1):61–72  1992.

9

,Sina Tootoonian
Mate Lengyel