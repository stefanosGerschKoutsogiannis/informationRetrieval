2010,Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform,Divisive normalization (DN) has been advocated as an effective nonlinear {\em efficient coding} transform for natural sensory signals with applications in biology and engineering. In this work  we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate {\em t} model to capture some important statistical properties of natural sensory signals. The multivariate {\em t} model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore  using the multivariate {\em t} model and measuring statistical dependency with multi-information  we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an effective efficient coding transform for natural sensory signals. On the other hand  we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small.,Divisive Normalization: Justiﬁcation and
Effectiveness as Efﬁcient Coding Transform

Computer Science Department

University at Albany  State University of New York

Siwei Lyu ∗

Albany  NY 12222  USA

Abstract

Divisive normalization (DN) has been advocated as an effective nonlinear efﬁ-
cient coding transform for natural sensory signals with applications in biology
and engineering. In this work  we aim to establish a connection between the DN
transform and the statistical properties of natural sensory signals. Our analysis
is based on the use of multivariate t model to capture some important statistical
properties of natural sensory signals. The multivariate t model justiﬁes DN as
an approximation to the transform that completely eliminates its statistical de-
pendency. Furthermore  using the multivariate t model and measuring statistical
dependency with multi-information  we can precisely quantify the statistical de-
pendency that is reduced by the DN transform. We compare this with the actual
performance of the DN transform in reducing statistical dependencies of natural
sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm
DN as an effective efﬁcient coding transform for natural sensory signals. On the
other hand  we also observe a previously unreported phenomenon that DN may
increase statistical dependencies when the size of pooling is small.

1

Introduction

It has been widely accepted that biological sensory systems are adapted to match the statistical
properties of the signals in the natural environments. Among different ways such may be achieved 
the efﬁcient coding hypothesis [2  3] asserts that a sensory system might be understood as a transform
that reduces redundancies in its responses to the input sensory stimuli (e.g.  odor  sounds  and time
varying images). Such signal transforms  termed as efﬁcient coding transforms  are also important
to applications in engineering – with the reduced statistical dependencies  sensory signals can be
more efﬁciently stored  transmitted and processed. Over the years  many works  most notably the
ICA methodology  have aimed to ﬁnd linear efﬁcient coding transforms for natural sensory signals
[20  4  15]. These efforts were widely regarded as a conﬁrmation of the efﬁcient coding hypothesis 
as they lead to localized linear basis that are similar to receptive ﬁelds found physiologically in the
cortex. Nonetheless  it has also been noted that there are statistical dependencies in natural images
or sounds  to which linear transforms are not effective to reduce or eliminate [5  17]. This motivates
the study of nonlinear efﬁcient coding transforms.
Divisive normalization (DN) is perhaps the most simple nonlinear efﬁcient coding transform that
has been extensively studied recently. The output of the DN transform is obtained from the response
of a linear basis function divided by the square root of a biased and weighted sum of the squared
responses of neighboring basis functions of adjacent spatial locations  orientations and scales. In
biology  initial interests in DN focused on its ability to model dynamic gain control in retina [24]
and the “masking” behavior in perception [11  33]  and to ﬁt neural recordings from the mammalian

∗This work is supported by an NSF CAREER Award (IIS-0953373).

1

(a)

(b)

(c)

(d)

(e)

Figure 1: Statistical properties of natural images in a band-pass domain and their representations with the
multivariate t model. (a): Marginal densities in the log domain (images: red solid curve  t model: blue dashed
curve). (b): Contour plot of the joint density  p(x1  x2)  of adjacent pairs of band-pass ﬁlter responses. (c):
Contour plot of the optimally ﬁtted multivariate t model of p(x1  x2). (d): Each column of the image correspond
to a conditional density p(x1|x2) of different x2 values. (e): The three red solid curves correspond to E(x1|x2)
and E(x1|x2) ± std(x1|x2). Blue dashed curves correspond to E(x1|x2) and E(x1|x2) ± std(x1|x2) from
the optimally ﬁtted multivariate t model to p(x1  x2).

visual cortex [12  19]. In image processing  nonlinear image representations based on DN have been
applied to image compression and contrast enhancement [18  16] showing improved performance
over linear representations.
As an important nonlinear transform with such a ubiquity  it has been of great interest to ﬁnd the
underlying principle from which DN originates. Based on empirical observations  Schwartz and
Simoncelli [23] suggested that DN can reduce statistical dependencies in natural sensory signals
and is thus justiﬁed by the efﬁcient coding hypothesis. More recent works on statistical models
and efﬁcient coding transforms of natural sensory signals (e.g.  [17  26]) have also hinted that DN
may be an approximation to the optimal efﬁcient coding transform. However  this claim needs to
be rigorously validated based on statistical properties of natural sensory signals  and quantitatively
evaluated with DN’s performance in reducing statistical dependencies of natural sensory signals.
In this work  we aim to establish a connection between the DN transform and the statistical proper-
ties of natural sensory signals. Our analysis is based on the use of multivariate t model to capture
some important statistical properties of natural sensory signals. The multivariate t model justiﬁes DN
as an approximation to the transform that completely eliminates its statistical dependency. Further-
more  using the multivariate t model and measuring statistical dependency with multi-information 
we can precisely quantify the statistical dependency that is reduced by the DN transform. We com-
pare this with the actual performance of the DN transform in reducing statistical dependencies of
natural sensory signals. Our theoretical analysis and quantitative evaluations conﬁrm DN as an ef-
fective efﬁcient coding transform for natural sensory signals. On the other hand  we also observe a
previously unreported phenomenon that DN may increase statistical dependencies when the size of
pooling is small.

2 Statistical Properties of Natural Sensory Signals and Multivariate t Model

Sensory signals in natural environments are highly structured and non-random. Their regularities
exhibit as statistical properties that distinguish them from the rest of the ensemble of all possible
signals. Over the years  many distinct statistical properties of natural sensory signals have been
observed. Particularly  in band-pass ﬁltered domains where local means are removed  three statistical
characteristics have been commonly observed across different signal ensembles1:

- symmetric and sparse non-Gaussian marginal distributions with high kurtosis [7  10]  Fig.1(a);
- joint densities of neighboring responses that have elliptically symmetric (spherically symmetric

after whitening) contours of equal probability [34  32]; Fig.1(b);

- conditional distributions of one response given neighboring responses that exhibit a “bow-tie”

shape when visualized as an image [25  6]  Fig.1(d).

It has been noted that higher order statistical dependencies in the joint and conditional densities
(Fig.1 (b) and (d)) cannot be effectively reduced with linear transform [17].

1The results in Fig.1 are obtained with spatial neighbors in images. Similar behaviors have also been
observed for orientation and scale neighbors [6]  as well as other type of sensory signals such as audios [23  17].

2

A compact mathematical form that can capture all three aforementioned statistical properties is
the multivariate Student’s t model. Formally  the probability density function of a d dimensional t
random vector x is deﬁned as2:

pt(x; α  β) = αβΓ (β + d/2)

Γ(β)(cid:112)det(πΣ)

(cid:0)α + x(cid:48)Σ−1x(cid:1)−β−d/2

 

(1)

where α > 0 is the scale parameter and β > 1 is the shape parameter. Σ is a symmetric and positive
deﬁnite matrix  and Γ(·) is the Gamma function. From data of neighboring responses of natural
sensory signals in the band-pass domain  the parameters (α  β) in the multivariate t model can be
obtained numerically with maximum likelihood  the details of which are given in the supplementary
material.. The joint density of the ﬁtted multivariate t model has elliptically symmetric level curves
of equal probability  and its marginals are 1D Student’s t densities that are non-Gaussian and kurtotic
[14]  all resembling those of the natural sensory signals  Fig.1(a) and (c). It is due to its heavy tail
property that the multivariate t model has been used as models of natural images [35  22].
Furthermore  we provide another property of the multivariate t model that captures the bow-tie
dependency exhibited by the conditional distributions of natural sensory signals.
Lemma 1 Denote x\i as the vector formed by excluding the ith element from x. For a d-
dimensional isotropic t vector x (i.e.  Σ = I)  we have
E(xi|x\i) = 0  and var(xi|x\i) =

2β + d − 3
where E(·) and var(·) denote expectation and variance  respectively.
This is proved in the supplementary material. Lemma 1 can be extended to anisotropic t models
by incorporating a non-diagonal Σ using a linear “un-whitening” procedure  the result of which
is demonstrated in Fig.1(e). The three red solid curves correspond to E(xi|x\i) and E(xi|x\i) ±

(cid:112)var(xi|x\i) for pairs of adjacent band-pass ﬁltered responses of a natural image  and the three

blue dashed curves are the same quantities of the optimally ﬁtted t model. The bow-tie phenomenon
comes directly from the dependencies in the conditional variances  which is precisely captured by
the ﬁtted multivariate t model3.

(α + x(cid:48)

\ix\i) 

1

3 DN as Efﬁcient Coding Transform for Multivariate t Model

Using the multivariate t model as a compact representation of statistical properties of natural sensory
signals in linear band-pass domains  our aim is to ﬁnd an efﬁcient coding transform that can effec-
tively reduce its statistical dependencies. This is based on an important property of the multivariate
t model – it is a special case of the Gaussian scale mixture (GSM) [1]. More speciﬁcally  the joint
density pt(x; α  β) can be written as an inﬁnite mixture of Gaussians with zero mean and covariance
matrix Σ  as

(cid:90) ∞

(cid:19)

(cid:18)

pt(x; α  β) =

1(cid:112)det(2πzΣ)
2β Γ(β) z−β−1 exp(cid:0)− α

2z

0

exp

− 1
2z

x(cid:48)Σ−1x

(cid:1) is the inverse Gamma distribution. Equivalently  for a

pγ−1(z; α  β)dz 

where pγ−1(z) = αβ
√
z  as x = u · √
d dimensional t vector x  we can decompose it into the product of two independent variables u and
z  where u is a d-dim Gaussian vector with zero mean and covariance matrix
Σ  and z > 0 is a scalar variable of an inverse Gamma law with parameter (α  β). To simplify
the discussion  hereafter we will assume that the signals have been whitened so that there is no
second-order dependencies in x. Correspondingly  the Gaussian vector u has a covariance Σ = I.
According to the GSM equivalence of the multivariate t model  we have u = x/
z. As an isotropic
Gaussian vector has mutually independent components  there is no statistical dependency among
elements of u. In other words  x/
z equals to a transform that completely eliminates all statistical
dependencies in x. Unfortunately  this optimal efﬁcient coding transform is not realizable  because
z is a latent variable that we do not have direct access to.
To overcome this difﬁculty  we can use an estimator of z based on the visible data vector x  ˆz 
to approximate the true value of z  and obtain an approximation to the optimal efﬁcient coding

√

√

2Eq.(1) can be shown to be equivalent to the standard deﬁnition of multivariate t density in [14].
3The dependencies illustrated are nonlinear because we use conditional standard deviations.

3

√
ˆz. For the multivariate t model  it turns out that two most common choices for
transform as x/
the estimators z  namely  the maximum a posterior (MAP) and the Bayesian least square (BLS)
estimators  and a third estimator all have similar forms  a result formally stated in the following
lemma (a proof is given in the supplementary material).
Lemma 2 For the d-dimensional isotropic t vector x with parameters (α  β)  we consider three
estimators of z as: (i) the MAP estimator  ˆz1 = argmaxz p(z|x)  which is the mode of the posterior
density  (ii) the BLS estimator  which is the mean of the posterior density ˆz2 = Ez|x(z|x)  and (iii)

the inverse of the conditional mean of 1/z  as ˆz3 =(cid:0)Ez|x (1/z|x)(cid:1)−1  which are:

2β + d − 2   and ˆz3 =(cid:0)Ez|x (1/z|x)(cid:1)−1 = α + x(cid:48)x

ˆz2 = α + x(cid:48)x

2β + d
If we drop the irrelevant scaling factors from each of these estimators and plug them in x/
obtain a nonlinear transform of x as 

ˆz1 = α + x(cid:48)x
2β + d + 2  

.
√

y = φ(x)  where φ(x) ≡

x√
α + x(cid:48)x

=

(cid:107)x(cid:107)(cid:112)α + (cid:107)x(cid:107)2

x
(cid:107)x(cid:107) .

ˆz  we

(2)

This is the standard form of divisive normalization that will be used throughout this paper. Lemma 2
shows that the DN transform is justiﬁed as an approximate to the optimal efﬁcient coding transform
given a multivariate t model of natural sensory signals. Our result also shows that the DN transform
approximately “gaussianizes” the input data  a phenomenon that has been empirically observed by
several authors (e.g.  [6  23]).

3.1 Properties of DN Transform

The standard DN transform given by Eq.(2) has some nice and important properties. Particularly 
the following Lemma shows that it is invertible and its Jacobian determinant has closed form.
Lemma 3 For the standard DN transform given in Eq. (2)  its inversion for y ∈ Rd with (cid:107)y(cid:107) < 1
is φ−1(y) =
y(cid:107)y(cid:107) . The determinant of its Jacobian matrix is also in closed
= α(α + x(cid:48)x)−(d/2+1).

√
αy√
1−(cid:107)y(cid:107)2 =
form  which is given by det

√
√
α(cid:107)y(cid:107)
1−(cid:107)y(cid:107)2

(cid:16) ∂φ(x)

(cid:17)

∂x

Further  the DN transform of a multivariate t vector also has a closed form density function.
Lemma 4 If x ∈ Rd has an isotropic t density with parameter (α  β)  then its DN transform 
y = φ(x)  follows an isotropic r model  whose probability density function is

(cid:40) Γ(β+d/2)
πd/2Γ(β) (1 − y(cid:48)y)β−1
0

pτ (y) =

(cid:107)y(cid:107) < 1
(cid:107)y(cid:107) ≥ 1

(3)

Lemma 4 suggests a duality between t and r models with regards to the DN transform. Proofs of
Lemma 3 and Lemma 4 can be found in [8]. For completeness  we also provide our proofs in the
supplementary material.

3.2 Equivalent Forms of DN Transform

(cid:90)

(cid:32)

In the current literature  the DN transform has been deﬁned in many different forms other than
Eq.(2). However  if we are merely interested in their ability to reduce statistical dependencies  many
of the different forms of DN transform based on l2 norm of the input vector x become equivalent.
To be more speciﬁc  we quantify statistical statistical dependency of a random vector x using the
multi-information (MI) [27]  deﬁned as

x

p(x)/

p(xk)

I(x) =

p(x) log

(4)
where H(·) denotes the Shannon differential entropy. MI is non-negative  and is zero if and only if
the components of x are mutually independent. MI is a generalization of mutual information  and
the two become identical when measures dependency for two dimensional x. Furthermore  MI is
invariant to any operation that operates on individual components of x (e.g.  element-wise rescaling)
k=1 H(xk) and H(x) (see [27]).

since such operations produce an equal effect on the two terms(cid:80)d

dx =

k=1

H(xk) − H(x) 

(cid:33)

d(cid:88)

d(cid:89)

k=1

4

Now consider four different deﬁnitions of the DN transform expressed in terms of the individual
element of the output vector as

yi =

xi√
α + x(cid:48)x

  si =

x2
i

α + x(cid:48)x   vi =

xi(cid:113)

α + x(cid:48)

 

ti =

x2
i
α + x(cid:48)
\ix\i

.

\ix\i

Here x\i denotes the vector formed from x without its ith component. Speciﬁcally  yi is the output
of Eq.(2). si is the output of the original DN transform used by Heeger [12]. vi corresponds to the
DN transform used by Schwartz and Simoncelli [23]. The main difference with Eq.(2) is that the
denominator is formed without element xi. Last  ti is the output of the DN transform used in [31].
These forms of DN4 related with each other by element-wise operations  as we have
  and ti = s2

si = y2

=

=

i   vi =

(cid:112)α + x(cid:48)x − x2

xi

yi(cid:112)1 − y2

xi(cid:113)

α + x(cid:48)

i = y2
1 − y2

i

i

.

i

i

\ix\i

As element-wise operations do not affect MI  in terms of dependency reduction  all three transforms
are equivalent to the standard form in terms of reducing statistical dependencies. Therefore  the
subsequent analysis applies to all these equivalent forms of the DN transform.

4 Quantifying DN Transform as Efﬁcient Coding Transform

We have set up a relation between the DN transform with statistical properties of natural sensory
signals through the multivariate t model. However  its effectiveness as an efﬁcient coding transform
for natural sensory signals needs yet to be quantiﬁed for two reasons. First  DN is only an approx-
imation to the optimal transform that eliminates statistical dependencies in a multivariate t model.
Further  the multivariate t model itself is a surrogate of the true statistical model of natural sensory
signals. It is our goal in this section to quantify the effectiveness of the DN transform in reduc-
ing statistical dependencies. We start with a study of applying DN to the multivariate t model  the
closed form density of which permits us a theoretical analysis of DN’s performance in dependency
reduction. We then appy DN to real natural sensory signal data  and compare its effectiveness as an
efﬁcient coding transform with the theoretical prediction obtained with the multivariate t model.

4.1 Results with Multivariate t Model

For simplicity  we consider isotropic models whose second order dependencies are removed with
whitening. The density functions of multivariate t and r models lead to closed form solutions for
MI  as formally stated in the following lemma (proved in the supplementary material).
Lemma 5 The MI of a d-dimensional isotropic t vector x is

I(x) = (d − 1) log Γ(β) − d log Γ(β + 1/2) + log Γ(β + d/2) − (d − 1)βΨ(β)

+ d(β + 1/2)Ψ(β + 1/2) − (β + d/2)Ψ(β + d/2).

Similarly  the MI of a d-dimensional r vector y = φ(x)  which is the DN transform of x  is

I(y) = d log Γ(β + (d − 1)/2) − log Γ(β) − (d − 1) log Γ(β + d/2) + (β − 1)Ψ(β)

+ (d − 1)(β + d/2 − 1)Ψ(β + d/2) − d(β + (d − 3)/2)Ψ(β + (d − 1)/2).
dβ log Γ(β).

In both cases  Ψ(β) denotes the Digamma function which is deﬁned as Ψ(β) = d
Note that α does not appear in these formulas  as it can be removed by re-scaling data and has
no effect on MI. Using Lemma 5  for a d-dimensional t vector  if we have I(x) > I(y)  the DN
transform reduces its statistical dependency  conversely  if I(x) < I(y)  it increases dependency. As
both Gamma function and Digamma function can be computed to high numerical precision  we can
evaluate ∆I = I(x)−I(y) corresponding to different shape parameter β and data dimensionality d.
The left panel of Fig.2 illustrates the surface of ∆I/I(x)  which measures the relative change in MI
between an isotropic t vector and its DN transform. The right panel of Fig.2 shows one dimensional
curves of ∆I/I(x) corresponding to different d values with varying β.
These plots illustrate several interesting aspects of the DN transform as an approximate efﬁcient
coding transform of the multivariate t models. First  with data dimensionality d > 4  using DN

4There are usually weights to each x2

weights and leads to no change in terms of MI.

i in the denominator  but re-scaling data can remove the different

5

Figure 2: left: Surface plot of [I(x) − I(φ(x))]/I(x)  measuring MI changes after applying the DN trans-
form φ(·) to an isotropic t vector x. I(x) and I(φ(x)) computed numerically using Lemma 5. The two
coordinates correspond with data dimensionality (d) and shape parameters of the multivariate t model (β).
right: one dimensional curves of ∆I/I(x) corresponding to different d values with varying β.
leads to signiﬁcant reduction of statistical dependency  but such reductions become weaker as β
increases. On the other hand  our experiment also showed an unexpected behavior that has not been
reported before  for d ≤ 4  the change of MI caused by the use of DN is negative  i.e.  DN increases
statistical dependency for such cases. Therefore  though effective for high dimensional models  DN
is not an efﬁcient coding transform for low dimensional multivariate t models.

4.2 Results with Natural Sensory Signals

As mentioned previously  the multivariate t model is an approximation to the source model of natural
sensory signals. Therefore  we would like to compare our analysis in the previous section with the
actual dependency reduction performance of the DN transform on real natural sensory signal data.

4.2.1 Non-parametric Estimating MI Changes

To this end  we need to evaluate MI changes after applying DN without relying on any speciﬁc para-
metric density model. This has been achieved previously for two dimensional data using straightfor-
ward nonparametric estimation of MI based on histograms [28]. However  the estimations obtained
this way are prone to strong bias due to the binning scheme in generating the histograms [21]  and
cannot be generalized to higher data dimensions due to the “curse of dimensionality”  as the number
of bins increases exponentially with regards to the data dimension.
Instead  in this work  we directly compute the difference of MI after DN is applied without explicitly
binning data. To see how this is possible  we ﬁrst express the computation of the MI change as

I(x) − I(y) =

(5)
the entropy of y = φ(x) is related to the entropy of x  as H(y) = H(x) −
is the Jacobian determinant of φ(x) [9]. For DN 

k=1

H(yk) − H(x) + H(y).

has closed form (Lemma 3)  and replacing it in Eq.(5) yields

(cid:82)

Next 
x p(x) log
det

(cid:16) ∂φ(x)

∂x

(cid:12)(cid:12)(cid:12)det
(cid:17)

H(xk) − d(cid:88)
(cid:17)
(cid:16) ∂φ(x)

∂x

k=1

d(cid:88)
(cid:17)(cid:12)(cid:12)(cid:12) dx  where det
H(yk) − d(cid:88)

(cid:16) ∂φ(x)
d(cid:88)

∂x

k=1

k=1

(cid:18) d

2

(cid:19)(cid:90)

x

I(y) − I(x) =

H(xk) + log α −

+ 1

p(x) log (α + x(cid:48)x)dx.

(6)

Once we determine α  the last term in Eq.(6) can be approximated with the average of function
log (α + x(cid:48)x) over input data. The ﬁrst two terms requires direct estimation of differential entropies
of scalar random variables  H(yk) and H(xk). For a more reliable estimation  we use the nonpara-
metric “bin-less” m-spacing estimator [30]. As a simple sanity check  Fig.3(a) shows the theoretical
evaluation of (I(y) − I(x))/d obtained with Lemma 5 for isotropic t models with β = 1.10 and
varying d (blue solid curve). The red dashed curve shows the same quantity computed using Eq.(6)
with 10  000 random samples drawn from the same multivariate t models. The small difference
between the two curves in this plot conﬁrms the quality of the non-parametric estimation.

6

11.522.533.544.5210203500.050.10.15dβ∆I/d11.522.533.544.5−0.03−0.02−0.0100.010.020.03d = 2d = 4d = 5d = 6β∆I/d(b) audio data

(c) image data

(a) t model

Figure 3: (a) Comparison of theoretical prediction of MI reduction for isotropic t model with β = 1.1 and dif-
ferent dimensions (blue solid curve) with the non-parametric estimation using Eq.(6) and m-spacing estimator
[30] on 10  000 random samples drawn from the corresponding multivariate t models (red dashed curve). (b)
Top row is the mean and standard deviation of the estimated shape parameter β for natural audio data of dif-
ferent local window sizes. Bottom row is the comparison of MI changes (∆I/d). Blue solid curve corresponds
to the prediction with Lemma 5  red dashed curve is the non-parametric estimation of Eq.(6). (c) Same results
as (b) for natural image data with different local block sizes.

4.2.2 Experimental Evaluation and Comparison

We next experiment with natural audio and image data. For audio  we used 20 sound clips of
animal vocalization and recordings in natural environments  which have a sampling frequency of
44.1 kHz and typical length of 15 − 20 seconds. These sound clips were ﬁltered with a bandpass
gamma-tone ﬁlter of 3 kHz center frequency [13]. For image data  we used eight images in the
van Hateren database [29]. These images have contents of natural scenes such as woods and greens
with linearized intensity values. Each image was ﬁrst cropped to the central 1024 × 1024 region
and then subject to a log transform. The log pixel intensities are further adjusted to have a zero
mean. We further processed the log transformed pixel intensities by convolving with an isotropic
bandpass ﬁlter that captures an annulus of frequencies in the Fourier domain ranging from π/4 to π
radians/pixel. Finally  data used in our experiments are obtained by extracting adjacent samples in
localized 1D temporal (for audios) or 2D spatial (for images) windows of different sizes. We further
whiten the data to remove second order dependencies.
With these data  we ﬁrst ﬁt multivariate t models using maximum likelihood (detailed procedure
given in the supplementary material)  from which we compute the theoretical prediction of MI dif-
ference using Lemma 5. Shown in the top row of Fig.3 (b) and (c) are the means and standard
deviations of the estimated shape parameters of different sizes of local windows for audio and im-
age data  respectively. These plots suggest two properties of the ﬁtted multivariate t model. First 
the estimated β values are typically close to one due to the high kurtosis of these signal ensembles.
Second  the shape parameter in general decreases as the data dimension increases.
Using the same data  we obtain the optimal DN transform by searching for optimal α in Eq.(2) that
maximizes the change in MI given by Eq.(6). However  as entropy is estimated non-parametrically 
we cannot use gradient based optimization for α. Instead  with a range of possible α values  we
perform a binary search  at each step of which we evaluate Eq.(6) using the current α and the non-
parametric estimation of entropy based on the data set.
In the bottom rows of Fig.3 (b) (for audios) and (c) (for images)  we show MI changes of using
DN on natural sensory data that are predicted by the optimally ﬁtted t model (blue solid curves) and
that obtained with optimized DN parameters using nonparametric estimation of Eq.(6) (red dashed
curve). For robustness  these results are the averages over data sets from the 20 audio signals and
8 images  respectively. In general  changes in statistical dependencies obtained with the optimal
DN transforms are in accordance with those predicted by the multivariate t model. The model-

7

2345678910111213141511.021.041.061.081.11.12β4916253649648110012111.021.041.061.081.11.12β246810−0.0200.020.040.06∆I/d model predictionnonparam estimation23456789101112131415−0.04−0.0200.020.04 model predictionnonparam estimation49162536496481100121−0.04−0.0200.020.04 model predictionnonparam estimationbased predictions also tend to be upper-bounds of the actual DN performance. Some discrepancies
between the two start to show as dimensionality increases  as the dependency reductions achieved
with DN become smaller even though the model-based predictions tend to keep increasing. This
may be caused by the approximation nature of the multivariate t model to natural sensory data. As
such  more complex structures in the natural sensory signals  especially with larger local windows 
cannot be effectively captured by the multivariate t models  which renders DN less effective.
On the other hand  our observation based on the multivariate t model that the DN transform tends to
increase statistical dependency for small pooling size also holds to real data. Indeed  the increment
of MI becomes more severe for d ≤ 4. On the surface  our ﬁnding seems to be in contradiction
with [23]  where it was empirically shown that applying an equivalent form of the DN transform as
Eq.(2) (see Section 3.2) over a pair of input neurons can reduce statistical dependencies. However 
one key yet subtle difference is that statistical dependency is deﬁned as the correlations in the con-
ditional variances in [23]  i.e.  the bow-tie behavior as in Fig.1(d). The observation made in [23]
is then based on the empirical observations that after applying DN transform  such dependencies
in the transformed variables become weaker  while our results show that the statistical dependency
measured by MI in that case actually increases.

5 Conclusion

In this work  based on the use of the multivariate t model of natural sensory signals  we have pre-
sented a theoretical analysis showing that DN emerges as an approximate efﬁcient coding transform.
Furthermore  we provide a quantitative analysis of the effectiveness of DN as an efﬁcient coding
transform for the multivariate t model and natural sensory signal data. These analyses conﬁrm the
ability of DN in reducing statistical dependency of natural sensory signals. More interestingly  we
observe a previously unreported result that DN can actually increase statistical dependency when the
size of pooling is small. As a future direction  we would like to extend this study to a generalized
DN transform where the denominator and numerator can have different degrees.
Acknowledgement The author would like to thank Eero Simoncelli for helpful discussions  and the
three anonymous reviewers for their constructive comments.

References
[1] D. F. Andrews and C. L. Mallows. Scale mixtures of normal distributions. Journal of the Royal Statistical

Society. Series B (Methodological)  36(1):99–102  1974.

[2] F Attneave. Some informational aspects of visual perception. Psych. Rev.  61:183–193  1954.
[3] H B Barlow. Possible principles underlying the transformation of sensory messages. In W A Rosenblith 

editor  Sensory Communication  pages 217–234. MIT Press  Cambridge  MA  1961.

[4] A J Bell and T J Sejnowski.

37(23):3327–3338  1997.

The ’independent components’ of natural scenes are edge ﬁlters.

[5] Matthias Bethge. Factorial coding of natural images: how effective are linear models in removing higher-

order dependencies? J. Opt. Soc. Am. A  23(6):1253–1268  2006.

[6] R. W. Buccigrossi and E. P. Simoncelli. Image compression via joint statistical characterization in the

wavelet domain. 8(12):1688–1701  1999.

[7] P.J. Burt and E.H. Adelson. The Laplacian pyramid as a compact image code. IEEE Transactions on

Communication  31(4):532–540  1981.

[8] J. Costa  A. Hero  and C. Vignat. On solutions to multivariate maximum α-entropy problems. In EMM-

CVPR  2003.

[9] T. Cover and J. Thomas. Elements of Information Theory. Wiley-Interscience  2nd edition  2006.
[10] D J Field. Relations between the statistics of natural images and the response properties of cortical cells.

4(12):2379–2394  1987.

[11] J. Foley. Human luminence pattern mechanisims: Masking experimants require a new model. J. of Opt.

Soc. of Amer. A  11(6):1710–1719  1994.

[12] D. J. Heeger. Normalization of cell responses in cat striate cortex. Visual neural science  9:181–198 

1992.

8

[13] P. Johannesma. The pre-response stimulus ensemble of neurons in the cochlear nucleus. In Symposium

on Hearing Theory  pages 58–69  Eindhoven  Holland  1972.

[14] Samuel Kotz and Saralees Nadarajah. Multivariate t Distributions and Their Applications. Cambridge

University Press  2004.

[15] M S Lewicki. Efﬁcient coding of natural sounds. Nature Neuroscience  5(4):356–363  2002.
[16] S. Lyu and E. P. Simoncelli. Nonlinear image representation using divisive normalization.
Conference on Computer Vision and Patten Recognition (CVPR)  Anchorage  AK  June 2008.

In IEEE

[17] S Lyu and E P Simoncelli. Nonlinear extraction of ’independent components’ of natural images using

radial Gaussianization. Neural Computation  18(6):1–35  2009.

[18] J. Malo  I. Epifanio  R. Navarro  and E. P. Simoncelli. Non-linear image representation for efﬁcient

perceptual coding. 15(1):68–80  January 2006.

[19] V. Mante  V. Bonin  and M. Carandini. Functional mechanisms shaping lateral geniculate responses to

artiﬁcial and natural stimuli. Neuron  58:625–638  May 2008.

[20] B A Olshausen and D J Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381:607–609  1996.

[21] Liam Paninski. Estimation of entropy and mutual information. Neural Comput.  15(6):1191–1253  2003.
[22] S. Roth and M. Black. Fields of experts: A framework for learning image priors. volume 2  pages

860–867  2005.

[23] O. Schwartz and E. P. Simoncelli. Natural signal statistics and sensory gain control. Nature Neuroscience 

4(8):819–825  August 2001.

[24] R Shapley and C Enroth-Cugell. Visual adaptation and retinal gain control. Progress in Retinal Research 

3:263–346  1984.

[25] E P Simoncelli and R W Buccigrossi. Embedded wavelet image compression based on a joint probability
model. In Proc 4th IEEE Int’l Conf on Image Proc  volume I  pages 640–643  Santa Barbara  October
26-29 1997. IEEE Sig Proc Society.

[26] Fabian H. Sinz and Matthias Bethge. The conjoint effect of divisive normalization and orientation selec-

tivity on redundancy reduction. In NIPS. 2009.

[27] M. Studeny and J. Vejnarova. The multiinformation function as a tool for measuring stochastic depen-
dence. In M. I. Jordan  editor  Learning in Graphical Models  pages 261–297. Dordrecht: Kluwer.  1998.
Input–output statistical independence in divisive normalization

[28] Roberto Valerio and Rafael Navarro.

models of v1 neurons. Network: Computation in Neural Systems  14(4):733–745  2003.

[29] A van der Schaaf and J H van Hateren. Modelling the power spectra of natural images: Statistics and

information. Vision Research  28(17):2759–2770  1996.

[30] Oldrich Vasicek. A test for normality based on sample entropy. Journal of the Royal Statistical Society 

Series B  38(1):54–59  1976.

[31] M. J. Wainwright  O. Schwartz  and E. P. Simoncelli. Natural image statistics and divisive normaliza-
In Probabilistic Models of the Brain:

tion: Modeling nonlinearity and adaptation in cortical neurons.
Perception and Neural Function  pages 203–222. MIT Press  2002.

[32] M J Wainwright and E P Simoncelli. Scale mixtures of Gaussians and the statistics of natural im-
ages. In S. A. Solla  T. K. Leen  and K.-R. M¨uller  editors  Adv. Neural Information Processing Systems
(NIPS*99)  volume 12  pages 855–861  Cambridge  MA  May 2000. MIT Press.

[33] A. Watson and J. Solomon. A model of visual contrast gain control and pattern masking. J. Opt. Soc.

Amer. A  pages 2379–2391  1997.

[34] B Wegmann and C Zetzsche. Statistical dependence between orientation ﬁlter outputs used in an human
vision based image code. In Proc Visual Comm. and Image Processing  volume 1360  pages 909–922 
Lausanne  Switzerland  1990.

[35] M. Welling  G. E. Hinton  and S. Osindero. Learning sparse topographic representations with products of

Student-t distributions. pages 1359–1366  2002.

9

,Yu-Xiang Wang
Huan Xu
Chenlei Leng