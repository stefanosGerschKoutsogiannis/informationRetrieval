2010,Learning Bounds for Importance Weighting,This paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail  which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and  more significantly  give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded  a condition related to the Renyi divergence of the training and test distributions. These results are based on a series of novel and general bounds we derive for unbounded loss functions  which are of independent interest. We use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits. Finally  we analyze the properties of normalized importance weights which are also commonly used.,Learning Bounds for Importance Weighting

Corinna Cortes
Google Research

New York  NY 10011
corinna@google.com

Yishay Mansour
Tel-Aviv University
Tel-Aviv 69978  Israel
mansour@tau.ac.il

Mehryar Mohri

Courant Institute and Google

New York  NY 10012
mohri@cims.nyu.edu

Abstract

This paper presents an analysis of importance weighting for learning from ﬁnite
samples and gives a series of theoretical and algorithmic results. We point out
simple cases where importance weighting can fail  which suggests the need for an
analysis of the properties of this technique. We then give both upper and lower
bounds for generalization with bounded importance weights and  more signiﬁ-
cantly  give learning guarantees for the more common case of unbounded impor-
tance weights under the weak assumption that the second moment is bounded 
a condition related to the R´enyi divergence of the training and test distributions.
These results are based on a series of novel and general bounds we derive for un-
bounded loss functions  which are of independent interest. We use these bounds to
guide the deﬁnition of an alternative reweighting algorithm and report the results
of experiments demonstrating its beneﬁts. Finally  we analyze the properties of
normalized importance weights which are also commonly used.

1 Introduction
In real-world applications of machine learning  often the sampling of the training and test instances
may differ  which results in a mismatch between the two distributions. For example  in web search
applications  there may be data regarding users who clicked on some advertisement link but little
or no information about other users. Similarly  in credit default analyses  there is typically some
information available about the credit defaults of customers who were granted credit  but no such
information is at hand about rejected costumers. In other problems such as adaptation  the training
data available is drawn from a source domain different from the target domain. These issues of
biased sampling or adaptation have been long recognized and studied in the statistics literature.
There is also a large body of literature dealing with different techniques for sample bias correction
[11  29  16  8  25  6] or domain adaptation [3  7  19  10  17] in the recent machine learning and
natural language processing literature.
A common technique used in several of these publications for correcting the bias or discrepancy is
based on the so-called importance weighting technique. This consists of weighting the cost of errors
on training instances to emphasize the error on some or de-emphasize it on others  with the objective
of correcting the mismatch between the distributions of training and test points  as in sample bias
correction  adaptation  and other related contexts such as active learning [24  14  8  19  5]. Different
deﬁnitions have been adopted for these weights. A common deﬁnition of the weight for point x is
w(x) = P (x)/Q(x) where P is the target or test distribution and Q is the distribution according to
which training points are drawn. A favorable property of this deﬁnition  which is not hard to verify 
is that it leads to unbiased estimates of the generalization error [8].
This paper presents an analysis of importance weighting for learning from ﬁnite samples. Our study
was originally motivated by the observation that  while this corrective technique seems natural  in
some cases in practice it does not succeed. An example in dimension two is illustrated by Figure 1.
The target distribution P is the even mixture of two Gaussians centered at (0  0) and (0  2) both with

1

4

3

2

1

0

−1

−2

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
+
+
−
−
−

−
−
−
−
−
−

−
−
−
σ P
−
x
−
−
σ P
σQ
x
+
+
−
−

+
+
−
−

−
−
−
−
−
−
−
+
+
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
σQ
x
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

−
−
−
−
−
−
−
−
−
−
−
−

r
o
r
r

E

1.0
0.8
0.6
0.4
0.2
0.0

Ratio = 0.3

σ σQ P

Ratio = 0.75

σ σQ P

r
o
r
r

E

1.0
0.8
0.6
0.4
0.2
0.0

20

100

500

5000

20

100

500

5000

−2

−1

0

1

2

3

4

Training set size

Training set size

Figure 1: Example of importance weighting. Left ﬁgure: P (in blue) and Q (in red) are even mixtures
of Gaussians. The labels are positive within the unit sphere centered at the origin (in grey)  negative
elsewhere. The hypothesis class is that of hyperplanestangent to the unit sphere. Right ﬁgures: plots
of test error vs training sample size using importance weighting for two different values of the ratio
σQ/σP. The results indicate mean values of the error over 40 runs ± one standard deviation.
standard deviation σP  while the source distribution Q is the even mixture of two Gaussians centered
at (0  0) and (2  0) but with standard deviation σQ. The hypothesis class is that of hyperplanes
tangent to the unit sphere. The best classiﬁer is selected by empirical risk minimization. As shown
in Figure 1  for σP /σQ = .3  the error of the hypothesis learned using importance weighting is close
to 50% even for a training sample of 5 000 points and the standard deviation of the error is quite
high. In contrast  for σP /σQ = .75  convergence occurs relatively rapidly and learning is successful.
In Section 4  we discuss other examples where importance weighting does not succeed.
The problem just described is not limited to isolated examples. Similar observations have been made
in the past in both the statistics and learning literature  more recently in the context of the analysis
of boosting by [9] who suggest that importance weighting must be used with care and highlight the
need for convergence bounds and learning guarantees for this technique.
We study the theoretical properties of importance weighting. We show using standard generaliza-
tion bounds that importance weighting can succeed when the weights are bounded. However  this
condition often does not hold in practice. We also show that  remarkably  convergence guarantees
can be given even for unbounded weights under the weak assumption that the second moment of the
weights is bounded  a condition that relates to the R´enyi divergence of P and Q. We further extend
these bounds to guarantees for other possible reweightings. These results suggest minimizing a bias-
variance tradeoff that we discuss and that leads to several algorithmic ideas. We explore in detail an
algorithm based on these ideas and report the results of experiments demonstrating its beneﬁts.
Throughout this paper  we consider the case where the weight function w is known. When it is
not  it is typically estimated from ﬁnite samples. The effect of this estimation error is speciﬁcally
analyzed by [8]. This setting is closely related to the problem of importance sampling in statistics
which is that of estimating the expectation of a random variable according to P while using a sample
drawn according to Q  with w given [18]. Here  we are concerned with the effect of the weights on
learning from ﬁnite samples. A different setting is when further full access to Q is assumed  von
Neumann’s rejection sampling technique [28] can then be used. We note however that it requires w
to be bounded by some constant M  which is often not guaranteed and is the simplest case of our
bounds. Even then  the method is wasteful as it requires on average M samples to obtain one point.
The remainder of this paper is structured as follows. Section 2 introduces the deﬁnition of the R´enyi
divergences and gives some basic properties of the importance weights. In Section 3  we give gen-
eralization bounds for importance weighting in the bounded case. We also present a general lower
bound indicating the key role played by the R´enyi divergence of P and Q in this context. Section 4
deals with the more frequent case of unbounded w. Standard generalization bounds do not apply
here since the loss function is unbounded. We give novel generalization bounds for unbounded loss
functions under the assumption that the second moment is bounded (see Appendix) and use them to
derive learning guarantees for importance weighting in this more general setting. In Section 5  we
discuss an algorithm inspired by these guarantees for which we report preliminary experimental re-
sults. We also discuss why the commonly used remedy of truncating or capping importance weights
may not always provide the desired effect of improved performance. Finally  in Section 6  we study

2

the properties of an alternative reweighting also commonly used which is based on normalized im-
portance weights  and discuss its relationship with the (unnormalized) weights w.
2 Preliminaries
Let X denote the input space  Y the label set  and let L : Y×Y → [0  1] be a loss function. We denote
by P the target distribution and by Q the source distribution according to which training points are
drawn. We also denote by H the hypothesis set used by the learning algorithm and by f : X → Y
the target labeling function.
2.1 R´enyi divergences
Our analysis makes use of the notion of R´enyi divergence  an information theoretical measure of
the difference between two distributions directly relevant to the study of importance weighting. For
α≥ 0  the R´enyi divergence Dα(P$Q) between distributions P and Q is deﬁned by [23]

Dα(P$Q) =

(1)
The R´enyi divergence is a non-negative quantity and for any α> 0  Dα(P$Q) = 0 iff P = Q. For
α = 1  it coincides with the relative entropy. We denote by dα(P$Q) the exponential in base 2 of
the R´enyi divergence Dα(P$Q):

α − 1

log2!x

P (x)" P (x)

Q(x)#α−1

1

.

dα(P$Q) = 2Dα(P"Q) =$!x

P α(x)

Qα−1(x)% 1

α−1

.

(2)

2.2 Importance weights
The importance weight for distributions P and Q is deﬁned by w(x) = P (x)/Q(x). In the follow-
ing  the expectations are taken with respect to Q.
Lemma 1. The following identities hold for the expectation  second moment  and variance of w:

(3)
Proof. The ﬁrst equality is immediate. The second moment of w can be expressed as follows in
terms of the R´enyi divergence:

σ2(w) = d2(P$Q) − 1.

E[w2] = d2(P$Q)

E[w] = 1

E
Q

[w2] = !x∈X

w2(x) Q(x) = !x∈X" P (x)
Q(x)#2

P (x)" P (x)
Thus  the variance of w is given by σ2(w) = EQ[w2] − EQ[w]2 = d2(P$Q) − 1.
For any hypothesis h∈H  we denote by R(h) its loss and by &Rw(h) its weighted empirical loss:

Q(x)# = d2(P$Q).

Q(x) = !x∈X

w(xi) L(h(xi)  f (xi)).

[L(h(x)  f (x))]

R(h) = E
x∼P

We shall use the abbreviated notation Lh(x) for L(h(x)  f (x))  in the absence of any ambiguity
about the target function f. Note that the unnormalizedimportance weighting of the loss is unbiased:

m!i=1

1
m

&Rw(h) =
Lh(x) Q(x) =!x

E
Q

[w(x)Lh(x)] =!x

P (x)
Q(x)

P (x)Lh(x) = R(h).

The following lemma gives a bound on the second moment.
Lemma 2. For all α> 0 and x ∈ X  the second moment of the importance weighted loss can be
bounded as follows:
(4)

E
x∼Q

[w2(x) L2

h(x)] ≤ dα+1(P$Q) R(h)1− 1
α .
For α = 1  this becomes R(h)2 ≤ Ex∼Q[w2(x) L2
h(x)] ≤ d2(P$Q).
3

Proof. The second moment can be bounded as follows:

[w2(x) L2

E
x∼Q

L2

Q(x)$ P (x)
Q(x)%2
h(x)] =!x
Q(x)%α% 1
≤$!x
P (x)$ P (x)
= dα+1(P$Q)$!x

h(x) =!x
α$!x

P (x) Lh(x)L

P (x)

P (x) L

1

α$ P (x)
Q(x)% P (x)
(x)% α−1

α

2α
α−1
h

α+1
α−1
h

α

(x)% α−1

α−1
α L2

h(x)

(H¨older’s inequality)

≤ dα+1(P$Q) R(h)1− 1

α = dα+1(P$Q) R(h)1− 1
α .

α B1+ 1
3 Learning Guarantees - Bounded Case
Note that supx w(x)= supx
Q(x) = d∞(P$Q). We ﬁrst examine the case d∞(P$Q)< +∞ and use
the notation M = d∞(P$Q). The following propositionfollows then directly Hoeffding’sinequality.
Proposition 1 (single hypothesis). Fix h ∈ H. For any δ> 0  with probability at least 1 − δ 

P (x)

|R(h) − &Rw(h)|≤ M’ log(2/δ)

2m

.

∀α> 0 

dα+1(P$Q) ≤ d∞(P$Q).

The upper bound M  though ﬁnite  can be quite large. The following theorem provides a more
favorable bound as a function of the ratio M/m when any of the moments of w  dα+1(P$Q) 
is ﬁnite  which is the case when d∞(P$Q) < ∞ since the R´enyi divergence is a non-decreasing
function of α [23  2]  in particular:
(5)
Theorem 1 (single hypothesis). Fix h ∈ H. Then  for any α≥ 1  for any δ> 0  with probability at
least 1−δ  the following bound holds for the importance weighting method:

+( 2)dα+1(P$Q) R(h)1− 1
α − R(h)2* log 1
3m ++ 2d2(P"Q) log 1
For α = 1 after further simpliﬁcation  this gives R(h) ≤ &Rw(h) + 2M log 1
Proof. Let Z denote the random variable w(x) Lh(x)− R(h). Then  |Z|≤ M. By lemma 2  the
variance of the random variable Z can be bounded in terms of the R´enyi divergence dα+1(P$Q):

R(h) ≤ &Rw(h) +

2M log 1
δ

(6)

3m

m

m

.

.

δ

δ

δ

α − R(h)2.

σ2(Z) = E
Q

Thus  by Bernstein’s inequality [4]  it follows that:

[w2(x) Lh(x)2] − R(h)2 ≤ dα+1(P$Q) R(h)1− 1
σ2(Z) + M/3#.

Pr[R(h) − &Rw(h) > ] ≤ exp" −m2/2
R(h) ≤ &Rw(h) +

+( M 2 log2 1
Using the sub-additivity of √· leads to the simpler expression

M log 1
δ

9m2

3m

m

+

δ

2σ2(Z) log 1
δ

.

Setting δ to match this upper bound shows that with probability at least 1−δ  the following bound
holds for the importance weighting method:

2M log 1
δ

3m

+( 2σ2(Z) log 1

m

δ

.

R(h) ≤ &Rw(h) +

These results can be straightforwardly extended to general hypothesis sets. In particular  for a ﬁnite
hypothesis set and for α = 1  the application of the union bound yields the following result.

4

R(h) ≤ &Rw(h) +

Theorem 2 (ﬁnite hypothesis set). Let H be a ﬁnite hypothesis set. Then  for any δ> 0  with
probability at least 1−δ  the following bound holds for the importance weighting method:

2M (log|H| + log 1
δ )

3m

+( 2d2(P$Q)(log|H| + log 1

m

δ )

.

(7)

For inﬁnite hypothesis sets  a similar result can be shown straightforwardly using covering numbers
instead of |H| or a related measure based on samples of size m [20].
In the following proposition  we give a lower bound that further emphasizes the role of the R´enyi
divergence of the second order in the convergence of importance weighting in the bounded case.
Proposition 2 (Lower bound). Assume that M < ∞ and σ2(w)/M 2 ≥ 1/m. Assume that H
contains a hypothesis h0 such that Lh0(x) = 1 for all x. Then  there exists an absolute constant c 
c = 2/412  such that

h∈H  R(h) − &Rw(h)   ≥’ d2(P$Q) − 1
Pr$ sup

4m

% ≥ c > 0.

(8)

Proof. Let σH = suph∈H σ(wLh). If for all x∈ X  Lh0(x) = 1  then σ2(wLh0) = d2(P$Q) − 1 =

H. The result then follows a general theorem  Theorem 9 proven in the Appendix.

σ2(w)= σ2
4 Learning Guarantees - Unbounded Case
The condition d∞(P$Q) <∞ assumed in the previous section does not always hold  even in some
natural cases  as illustrated by the following examples.
4.1 Examples
Assume that P and Q both follow a Gaussian distribution with the standard deviations σP and σQ
and with means µ and µ&:
1

1

% Q(x) =

exp$ −

Q %.
(x − µ&)2

2σ2

√2πσQ
.  thus  even for σP = σQ and µ += µ& the
Q(x) = +∞  and the bound of Theorem 1

P (x)

P (x) =

√2πσP
Q(x) = σQ

(x − µ)2
2σ2
P

exp$ −
exp- − σ2
exp$ −
σP / +∞
P√2π/ +∞

−∞

−∞

σQ

σ2

=

σP

Q(x−µ)2−σ2
P σ2
Q

In that case  P (x)
P (x−µ")2
importance weights are unbounded  d∞(P$Q) = supx
is not informative. The R´enyi divergence of the second order is given by:
P (x − µ&)2

σQ

2σ2

Q(x − µ)2 − σ2
σ2
P σ2
Q

2σ2

d2(P$Q) =

exp$ −

2σ2

Q(x − µ)2 − σ2
P σ2
Q

2σ2

%P (x)dx
%dx.
P (x − µ&)2

√2
That is  for σQ >
2 σP the variance of the importance weights is bounded. By the additivity
property of the R´enyi divergence  a similar situation holds for the product and sums of such Gaussian
distributions. Hence  in the rightmost example of Figure 1  the importance weights are unbounded 
but their second moment is bounded. In the next section we provide learning guarantees even for
this setting in agreement with the results observed. For σQ = 0.3σP  the same favorable guarantees
do not hold  and  as illustrated in Figure 1  learning is signiﬁcantly more difﬁcult.
This example of Gaussians can further illustrate what can go wrong in importance weighting. As-
sume that µ = µ& = 0  σQ = 1 and σP = 10. One could have expected this to be an easy case for
importance weighting since sampling from Q provides useful information about P. The problem
is  however  that a sample from Q will contain a very small number of points far from the mean
(of either negative or positive label) and that these points will be assigned very large weights. For
a sample of size m and σQ = 1  the expected value of an extreme point is √2 log m − o(1) and its

5

P +1/σ2

weight will be in the order of m−1/σ2
Q = m0.99. Therefore  a few extreme points will domi-
nate all other weights and necessarily have a huge inﬂuence on the selection of a hypothesis by the
learning algorithm.
Another related example is when σQ = σP = 1 and µ& = 0. Let µ   0 depend on the sample size
m. If µ is large enough compared to log(m)  then  with high probability  all the weights will be
negligible. This is especially problematic  since the estimate of the probability of any event would
be negligible (in fact both an event and its complement). If we normalize the weights  the issue
is overcome  but then  with high probability  the maximum weight dominates the sum of all other
weights  reverting the situation back to that of the previous example.
4.2 Importance weighting learning bounds - unbounded case
As in these examples  in practice  the importance weights are typically not bounded. However  we
shall show that  remarkably  under the weak assumption that the second moment of the weights
w  d2(P$Q)  is bounded  generalization bounds can be given for this case as well. The follow-
ing result relies on a general learning bound for unbounded loss functions proven in the Appendix
(Corollary 1). We denote by Pdim(U ) the pseudo-dimension of a real-valued function class U [21].
Theorem 3. Let H be a hypothesis set such that Pdim({Lh(x): h ∈ H}) = p < ∞. Assume that
d2(P$Q) < +∞ and w(x) += 0 for all x. Then  for any δ> 0  with probability at least 1 − δ  the
following holds:

3

8( p log 2me

.

h∈H

R(h) − &Rw(h)
0d2(P$Q)

R(h) ≤ &Rw(h) + 25/40d2(P$Q)
Proof. Since d2(P$Q) < +∞  the second moment of w(x)Lh(x) is ﬁnite and upper bounded by
d2(P$Q) (Lemma 2). Thus  by Corollary 1  we can write
Pr$ sup

>% ≤ 4 exp"p log

45/3 # 

2em
p −

p + log 4
δ
m

m8/3

∀xi ∈ B  w(xi)Lh(xi) ≥ ri

∀xi ∈ B  Lh(xi) ≥ ri/w(xi)

∀xi ∈ A − B  w(xi)Lh(xi) < ri.

∀xi ∈ A − B  Lh(xi) < ri/w(xi).

where p is the pseudo-dimension of the function class H&& = {w(x)Lh(x): h ∈ H}. We now show
that p = Pdim({Lh(x): h ∈ H}). Let H& denote {Lh(x): h ∈ H}. Let A = {x1  . . .   xk} be a
set shattered by H&&. Then  there exist real numbers r1  . . .   rk such that for any subset B ⊆ A there
exists h ∈ H such that
(9)
Since by assumption w(xi)> 0 for all i ∈ [1  k]  this implies that
(10)
Thus  H& shatters A with the witnesses si = ri/w(xi)  i ∈ [1  k]. Using the same observations  it is
straightforward to see that conversely  any set shattered by H& is shattered by H&&.
The convergence rate of the bound is slightly weaker (O(m−3/8)) than in the bounded case
(O(m−1/2)). A faster convergence can be obtained however using the more precise bound of Theo-
rem 8 at the expense of readability. The R´enyi divergence d2(P$Q) seems to play a critical role in
the bound and thus in the convergence of importance weighting in the unbounded case.
5 Alternative reweighting algorithms
The previous analysis can be generalized to the case of an arbitrary positive function u : X → R 
m1m
u > 0. Let &Ru(h)= 1
i=1 u(xi)Lh(xi) and let &Q denote the empirical distribution.
Theorem 4. Let H be a hypothesis set such that Pdim({Lh(x): h ∈ H}) = p < ∞. Assume that
0 < EQ[u2(x)] < +∞ and u(x) += 0 for all x. Then  for any δ> 0  with probability at least 1 − δ 
the following holds:
|R(h) − &Ru(h)|≤     E
Q)[w(x) − u(x)]Lh(x)*   +
25/4 max20EQ[u2(x)L2

h(x)] √E bQ[u2(x)L2

8( p log 2me

h(x)]3 3

p + log 4
m

δ

.

6

Unweighted  Ratio = 0.75
1.0

σ σP Q

Importance  Ratio = 0.75
1.0

σ σP Q

r
o
r
r

E

0.8

0.6

0.4

0.2

0.0

r
o
r
r

E

0.8

0.6

0.4

0.2

0.0

r
o
r
r

E

1.0

0.8

0.6

0.4

0.2

0.0

Quantile  Ratio = 0.75

σ σP Q

Capped 1%  Ratio = 0.75
1.0

σ σP Q

r
o
r
r

E

0.8

0.6

0.4

0.2

0.0

20

50

200 500
Training set size

2000

20

50

200 500
Training set size

2000

20

50

200 500
Training set size

2000

20

50

200 500
Training set size

2000

Figure 2: Comparison of the convergence of 4 different algorithms for the learning task of Figure 1:
learning with equal weights for all examples (Unweighted)  Importance weighting  using Quantiles
to parameterize the function u  and Capping the largest weights.
Proof. Since R(h) = E[w(x)Lh(x)]  we can write

and thus

R(h) − &Ru(h) = E
|R(h) − &Ru(h)|≤    E

Q)[w(x) − u(x)]Lh(x)* + E[u(x)Lh(x)] − &Ru(h) 
Q)[w(x) − u(x)]Lh(x)*   + | E[u(x)Lh(x)] − &Ru(h)|.

3

Q

bQ

[u2]3 ≤0E

[u2] 0E
Q)|w(x) − u(x)|* + γ0E

Q

h(x)])

p +log 4
δ
m

h(x)] √E bQ[u2(x)L2

By Corollary 2 applied to the function u Lh 

| E[u(x)Lh(x)] − &Ru(h)| can be bounded by
8+ p log 2me
with probability 1 − δ  with

25/4 max(0EQ[u2(x)L2
p = Pdim({Lh(x): h ∈ H}) by a proof similar to that of Theorem 3.
The theorem suggests that other functions u than w can be used to reweight the cost of an error
on each training point by minimizing the upper bound  which is a trade-off between the bias term
where the coefﬁcients are explicitly given. Function u can be selected from different families. Using
an upper bound on these quantities that is independent of h and a multiplicative bound of the form

| EQ[(w(x)−u(x))Lh(x)]| and the second moment max40EQ[u2(x)L2
[u2]41 + O(1/√m)5  

leads to the following optimization problem:

h(x)] √E bQ[u2(x)L2

h(x)]5 

max20E

Q

E

[u2] 

min
u∈U

(11)
where γ> 0 is a parameter controlling the trade-off between bias and variance minimization and
where U is a family of possible weight functions out of which u is selected.
Here  we consider a family of functions U parameterized by the quantiles q of the weight function
w. A function uq ∈ U is then deﬁned as follows: within each quantile  the value taken by uq is the
average of w over that quantile. For small values of γ  the bias term dominates  and very ﬁne-grained
quantiles minimize the bound of equation (11). For large values of γ the variance term dominates
and the bound is minimized by using just one quantile  corresponding to an even weighting of
the training examples. Hence by varying γ from small to large values  the algorithm interpolates
between standard importanceweighting with just one example per quantile  and unweighted learning
where all examples are given the same weight. Figure 2 also shows the results of experiments for
the learning task of Figure 1 using the algorithm deﬁned by (11) with this family of functions. The
optimal q is determined by 10-fold cross-validation. We see that a more rapid convergence can be
obtained by using these weights compared to the standard importance weights w.
Another natural family of functions is that of thresholded versions of the importance weights
{uθ : θ> 0 ∀x∈X  uθ(x)= min(w(x) θ )}. In fact  in practice  users often cap importance weights
by choosing an arbitrary value θ. The advantage of this family is that  by deﬁnition  the weights are

7

bounded. However  in some cases  larger weights could be critical to achieve a better performance.
Figure 2 illustrates the performance of this approach. Compared to importance weighting  no change
in performance is observed until the largest 1% of the weights are capped  in which case we only
observe a performance degradation. We expect the thresholding to be less beneﬁcial when the large
weights reﬂect the true w and are not an artifact of estimation uncertainties.
6 Relationship between normalized and unnormalized weights
An alternative approach based on the weight function w = P (x)/Q(x) consists of normalizing the
weights. Thus  while in the unnormalized case the unweighted empirical error is replaced by

1
m

m!i=1

w(xi) Lh(xi) =

in the normalized case it is replaced by

w(xi)

m

m!i=1

Lh(xi) 

w(xi)

W

Lh(xi) 

m!i=1

with W =1m

i=1 w(xi). We refer to &w(x) = w(x)/W as the normalized importance weight. An

advantage of the normalized weights is that they are by deﬁnition bounded by one. However  the
price to pay for this beneﬁt is the fact that the weights are no more unbiased. In fact  several issues
similar to those we pointed out in the Section 4 affect the normalized weights as well.
Here  we maintain the assumption that the second moment of the importance weights is bounded
and analyze the relationship between normalized and unnormalized weights. We show that  under
this assumption  normalized and unnormalized weights are in fact very close  with high probability.
Observe that for any i ∈ [1  m] 
&w(xi) −

m% .

W −

w(xi)

W

m

1

m

W

w(xi)

Thus  since w(xi)
ES[W ] = 1
the following inequality holds

m% =
= w(xi)$ 1
W ≤ 1  we can write   &w(xi) − w(xi)
m     ≤  1 − W
m1m
m     ≤ 25/4 max6+d2(P$Q) +d2(P$&Q)7 3
    1 −

W $1 −
m   . Since E[w(x)] = 1  we also have
k=1 E[w(xk)] = 1. Thus  by Corollary 2  for any δ> 0  with probability at least 1− δ 
8( log 2me + log 4
m      simultaneously for all i ∈ [1  m].

which implies the same upper bound on   &w(xi) − w(xi)

7 Conclusion
We presented a series of theoretical results for importance weighting both in the bounded weights
case and in the more general unbounded case under the assumption that the second moment of the
weights is bounded. We also initiated a preliminary exploration of alternative weights and showed its
beneﬁts. A more systematic study of new algorithms based on these learning guarantees could lead
to even more beneﬁcial and practically useful results. Several of the learning guarantees we gave
depend on the R´enyi divergence of the distributions P and Q. Accurately estimating that quantity
is thus critical and should motivate further studies of the convergence of its estimates from ﬁnite
samples. Finally  our novel unbounded loss learning bounds are of independent interest and could
be useful in a variety of other contexts.
References
[1] M. Anthony and J. Shawe-Taylor. A result of Vapnik with applications. Discrete Applied

δ

 

Mathematics  47:207 – 217  1993.

8

[2] C. Arndt. Information Measures: Information and its Description in Science and Engineering.

Signals and Communication Technology. Springer Verlag  2004.

[3] S. Ben-David  J. Blitzer  K. Crammer  and F. Pereira. Analysis of representations for domain

adaptation. NIPS  2007.

[4] S. N. Bernstein. Sur l’extension du th´eor`eme limite du calcul des probabilit´es aux sommes de

quantit´es d´ependantes. Mathematische Annalen  97:1–59  1927.

[5] A. Beygelzimer  S. Dasgupta  and J. Langford. Importance weighted active learning. In ICML 

pages 49–56  New York  NY  USA  2009.

[6] S. Bickel  M. Br¨uckner  and T. Scheffer. Discriminative learning for differing training and test

distributions. In ICML  pages 81–88  2007.

[7] J. Blitzer  K. Crammer  A. Kulesza  F. Pereira  and J. Wortman. Learning bounds for domain

[8] C. Cortes  M. Mohri  M. Riley  and A. Rostamizadeh. Sample selection bias correction theory.

adaptation. NIPS 2007  2008.

In ALT  2008.

[9] S. Dasgupta and P. M. Long. Boosting with diverse base classiﬁers. In COLT  2003.
[10] H. Daum´e III and D. Marcu. Domain adaptation for statistical classiﬁers. Journal of Artiﬁcial

Intelligence Research  26:101–126  2006.

[11] M. Dud´ık  R. E. Schapire  and S. J. Phillips. Correcting sample selection bias in maximum

entropy density estimation. In NIPS  2006.

[12] R. M. Dudley. A course on empirical processes. Lecture Notes in Math.  1097:2 – 142  1984.
[13] R. M. Dudley. UniversalDonsker classes and metric entropy. Annals of Probability  14(4):1306

– 1326  1987.

[14] C. Elkan. The foundations of cost-sensitive learning. In IJCAI  pages 973–978  2001.
[15] D. Haussler. Decision theoretic generalizations of the PAC model for neural net and other

learning applications. Inf. Comput.  100(1):78–150  1992.

[16] J. Huang  A. J. Smola  A. Gretton  K. M. Borgwardt  and B. Sch¨olkopf. Correcting sample

selection bias by unlabeled data. In NIPS  volume 19  pages 601–608  2006.

[17] J. Jiang and C. Zhai. Instance Weighting for Domain Adaptation in NLP. In ACL  2007.
[18] J. S. Liu. Monte Carlo strategies in scientiﬁc computing. Springer  2001.
[19] Y. Mansour  M. Mohri  and A. Rostamizadeh. Domain adaptation: Learning bounds and algo-

rithms. In COLT  2009.

[20] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In

COLT  Montr´eal  Canada  June 2009. Omnipress.

[21] D. Pollard. Convergence of Stochastic Processess. Springer  New York  1984.
[22] D. Pollard. Asymptotics via empirical processes. Statistical Science  4(4):341 – 366  1989.
[23] A. R´enyi. On measures of information and entropy. In Proceedings of the 4th Berkeley Sym-

posium on Mathematics  Statistics and Probability  page 547561  1960.

[24] H. Shimodaira.

likelihood function. Journal of Statistical Planning and Inference  90(2):227–244  2000.

Improving predictive inference under covariate shift by weighting the log-

[25] M. Sugiyama  S. Nakajima  H. Kashima  P. von B¨unau  and M. Kawanabe. Direct importance
estimation with model selection and its application to covariate shift adaptation. In NIPS  2008.

[26] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons  1998.
[27] V. N. Vapnik. Estimation of Dependences Based on Empirical Data  2nd ed. Springer  2006.
[28] J. von Neumann. Various techniques used in connection with random digits. Monte Carlo

methods. Nat. Bureau Standards  12:36–38  1951.

[29] B. Zadrozny  J. Langford  and N. Abe. Cost-sensitive learning by cost-proportionate example

weighting. In ICDM  2003.

9

,Barbara Rakitsch
Christoph Lippert
Karsten Borgwardt
Oliver Stegle