2016,Learning Bound for Parameter Transfer Learning,We consider a transfer-learning problem by using the parameter transfer approach  where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then  we introduce the notion of the local stability of parametric feature mapping and  parameter transfer learnability  and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning  we discuss the performance of sparse coding in self-taught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance  their theoretical analysis has not been studied. In this paper  we also provide the first theoretical learning bound for self-taught learning.,Learning Bound for Parameter Transfer Learning

Wataru Kumagai

Faculty of Engineering
Kanagawa University

kumagai@kanagawa-u.ac.jp

Abstract

We consider a transfer-learning problem by using the parameter transfer approach 
where a suitable parameter of feature mapping is learned through one task and ap-
plied to another objective task. Then  we introduce the notion of the local stability
and parameter transfer learnability of parametric feature mapping  and thereby
derive a learning bound for parameter transfer algorithms. As an application of
parameter transfer learning  we discuss the performance of sparse coding in self-
taught learning. Although self-taught learning algorithms with plentiful unlabeled
data often show excellent empirical performance  their theoretical analysis has not
been studied. In this paper  we also provide the ﬁrst theoretical learning bound for
self-taught learning.

1 Introduction

In traditional machine learning  it is assumed that data are identically drawn from a single distribu-
tion. However  this assumption does not always hold in real-world applications. Therefore  it would
be signiﬁcant to develop methods capable of incorporating samples drawn from different distribu-
tions. In this case  transfer learning provides a general way to accommodate these situations. In
transfer learning  besides the availability of relatively few samples related with an objective task 
abundant samples in other domains that are not necessarily drawn from an identical distribution  are
available. Then  transfer learning aims at extracting some useful knowledge from data in other do-
mains and applying the knowledge to improve the performance of the objective task. In accordance
with the kind of knowledge that is transferred  approaches to solving transfer-learning problems can
be classiﬁed into cases such as instance transfer  feature representation transfer  and parameter trans-
fer (Pan and Yang (2010)). In this paper  we consider the parameter transfer approach  where some
kind of parametric model is supposed and the transferred knowledge is encoded into parameters.
Since the parameter transfer approach typically requires many samples to accurately learn a suitable
parameter  unsupervised methods are often utilized for the learning process. In particular  trans-
fer learning from unlabeled data for predictive tasks is known as self-taught learning (Raina et al.
(2007))  where a joint generative model is not assumed to underlie unlabeled samples even though
the unlabeled samples should be indicative of a structure that would subsequently be helpful in pre-
dicting tasks. In recent years  self-taught learning has been intensively studied  encouraged by the
development of strong unsupervised methods. Furthermore  sparsity-based methods such as sparse
coding or sparse neural networks have often been used in empirical studies of self-taught learning.
Although many algorithms based on the parameter transfer approach have empirically demonstrated
impressive performance in self-taught learning  some fundamental problems remain. First  the theo-
retical aspects of the parameter transfer approach have not been studied  and in particular  no learning
bound was obtained. Second  although it is believed that a large amount of unlabeled data help to
improve the performance of the objective task in self-taught learning  it has not been sufﬁciently
clariﬁed how many samples are required. Third  although sparsity-based methods are typically em-
ployed in self-taught learning  it is unknown how the sparsity works to guarantee the performance
of self-taught learning.

The aim of the research presented in this paper is to shed light on the above problems. We ﬁrst
consider a general model of parametric feature mapping in the parameter transfer approach. Then 
we newly formulate the local stability of parametric feature mapping and the parameter transfer
learnability for this mapping  and provide a theoretical learning bound for parameter transfer learn-
ing algorithms based on the notions. Next  we consider the stability of sparse coding. Then we
discuss the parameter transfer learnability by dictionary learning under the sparse model. Applying
the learning bound for parameter transfer learning algorithms  we provide a learning bound of the
sparse coding algorithm in self-taught learning.
This paper is organized as follows. In the remainder of this section  we refer to some related studies.
In Section 2  we formulate the stability and the parameter transfer learnability of the parametric
feature mapping. Then  we present a learning bound for parameter transfer learning. In Section 3 
we show the stability of the sparse coding under perturbation of the dictionaries. Then  by imposing
sparsity assumptions on samples and by considering dictionary learning  we derive the parameter
transfer learnability for sparse coding. In particular  a learning bound is obtained for sparse coding
in the setting of self-taught learning. In Section 4  we conclude the paper.

1.1 Related Works

Approaches to transfer learning can be classiﬁed into some cases based on the kind of knowledge
being transferred (Pan and Yang (2010)). In this paper  we consider the parameter transfer approach.
This approach can be applied to various notable algorithms such as sparse coding  multiple kernel
learning  and deep learning since the dictionary  weights on kernels  and weights on the neural net-
work are regarded as parameters  respectively. Then  those parameters are typically trained or tuned
on samples that are not necessarily drawn from a target region. In the parameter transfer setting  a
number of samples in the source region are often needed to accurately estimate the parameter to be
transferred. Thus  it is desirable to be able to use unlabeled samples in the source region.
Self-taught learning corresponds to the case where only unlabeled samples are given in the source
region while labeled samples are available in the target domain. In this sense  self-taught learning is
compatible with the parameter transfer approach. Actually  in Raina et al. (2007) where self-taught
learning was ﬁrst introduced  the sparse coding-based method is employed and the parameter trans-
fer approach is already used regarding the dictionary learnt from images as the parameter to be
transferred. Although self-taught learning has been studied in various contexts (Dai et al. (2008);
Lee et al. (2009); Wang et al. (2013); Zhu et al. (2013))  its theoretical aspects have not been sufﬁ-
ciently analyzed. One of the main results in this paper is to provide a ﬁrst theoretical learning bound
in self-taught learning with the parameter transfer approach. We note that our setting differs from
the environment-based setting (Baxter (2000)  Maurer (2009))  where a distribution on distributions
on labeled samples  known as an environment  is assumed. In our formulation  the existence of the
environment is not assumed and labeled data in the source region are not required.
Self-taught learning algorithms are often based on sparse coding. In the seminal paper by Raina et al.
(2007)  they already proposed an algorithm that learns a dictionary in the source region and trans-
fers it to the target region. They also showed the effectiveness of the sparse coding-based method.
Moreover  since remarkable progress has been made in unsupervised learning based on sparse neural
networks (Coates et al. (2011)  Le (2013))  unlabeled samples of the source domain in self-taught
learning are often preprocessed by sparsity-based methods. Recently  a sparse coding-based gen-
eralization bound was studied (Mehta and Gray (2013); Maurer et al. (2012)) and the analysis in
Section 3.1 is based on (Mehta and Gray (2013)).

2 Learning Bound for Parameter Transfer Learning

2.1 Problem Setting of Parameter Transfer Learning

We formulate parameter transfer learning in this subsection. We ﬁrst brieﬂy introduce notations and
terminology in transfer learning (Pan and Yang (2010)). Let X and Y be a sample space and a label
space  respectively. We refer to a pair of Z := X (cid:2) Y and a joint distribution P (x; y) on Z as a
region. Then  a domain comprises a pair consisting of a sample space X and a marginal probability
of P (x) on X and a task consists of a pair containing a label set Y and a conditional distribution
P (yjx).
In addition  let H = fh : X ! Yg be a hypothesis space and ℓ : Y (cid:2) Y ! R(cid:21)0

2

∑

n

n

E(x;y)(cid:24)P [ℓ(y; h(x))] and bRn(h) := 1

represent a loss function. Then  the expected risk and the empirical risk are deﬁned by R(h) :=
j=1 ℓ(yj; h(xj))  respectively. In the setting of transfer
learning  besides samples from a region of interest known as a target region  it is assumed that
samples from another region known as a source region are also available. We distinguish between
the target and source regions by adding a subscript T or S to each notation introduced above  (e.g.
PT   RS). Then  the homogeneous setting (i.e.  XS = XT ) is not assumed in general  and thus  the
heterogeneous setting (i.e.  XS ̸= XT ) can be treated. We note that self-taught learning  which is
treated in Section 3  corresponds to the case when the label space YS in the source region is the set
of a single element.
We consider the parameter transfer approach  where the knowledge to be transferred is encoded into
a parameter. The parameter transfer approach aims to learn a hypothesis with low expected risk for
the target task by obtaining some knowledge about an effective parameter in the source region and
transfer it to the target region. In this paper  we suppose that there are parametric models on both
the source and target regions and that their parameter spaces are partly shared. Then  our strategy
is to learn an effective parameter in the source region and then transfer a part of the parameter to
the target region. We describe the formulation in the following. In the target region  we assume that
YT (cid:26) R and there is a parametric feature mapping (cid:18) : XT ! Rm on the target domain such that
each hypothesis hT ;(cid:18);w : XT ! YT is represented by

hT ;(cid:18);w(x) := ⟨w; (cid:18)(x)⟩

(1)
with parameters (cid:18) 2 (cid:2) and w 2 WT   where (cid:2) is a subset of a normed space with a norm ∥ (cid:1) ∥ and
WT is a subset of Rm. Then the hypothesis set in the target region is parameterized as

In the following  we simply denote RT (hT ;(cid:18);w) and bRT (hT ;(cid:18);w) by RT ((cid:18); w) and bRT ((cid:18); w) 

HT = fhT ;(cid:18);wj(cid:18) 2 (cid:2); w 2 WT g:

respectively. In the source region  we suppose that there exists some kind of parametric model such
as a sample distribution PS;(cid:18);w or a hypothesis hS;(cid:18);w with parameters (cid:18) 2 (cid:2) and w 2 WS  and
S 2 WS
(cid:3)
a part (cid:2) of the parameter space is shared with the target region. Then  let (cid:18)
be parameters that are supposed to be effective in the source region (e.g.  the true parameter of the
sample distribution  the parameter of the optimal hypothesis with respect to the expected risk RS);
however  explicit assumptions are not imposed on the parameters. Then  the parameter transfer
algorithm treated in this paper is described as follows. Let N- and n-samples be available in the
source and target regions  respectively. First  a parameter transfer algorithm outputs the estimator

S 2 (cid:2) and w
(cid:3)

b(cid:18)N 2 (cid:2) of (cid:18)

by using n-samples  where r(w) is a 1-strongly convex function with respect to ∥ (cid:1) ∥2 and (cid:26) > 0.
(cid:3)
If the source region relates to the target region in some sense  the effective parameter (cid:18)
S in the
source region is expected to also be useful for the target task. In the next subsection  we regard
RT ((cid:18)

(cid:3)
T ) as the baseline of predictive performance and derive a learning bound.

(cid:3)
S ; w

2.2 Learning Bound Based on Stability and Learnability

We newly introduce the local stability and the parameter transfer learnability as below. These notions
are essential to derive a learning bound in Theorem 1.
Deﬁnition 1 (Local Stability). A parametric feature mapping (cid:18) is said to be locally stable if there
exist ϵ(cid:18) : X ! R>0 for each (cid:18) 2 (cid:2) and L > 0 such that for (cid:18)

′ 2 (cid:2)

∥(cid:18) (cid:0) (cid:18)

′∥ (cid:20) ϵ(cid:18)(x) ) ∥ (cid:18)(x) (cid:0) (cid:18)

′ (x)∥2 (cid:20) L ∥(cid:18) (cid:0) (cid:18)

′∥:

We term ϵ(cid:18)(x) the permissible radius of perturbation for (cid:18) at x. For samples Xn = fx1; : : : xng 
we denote as ϵ(cid:18)(Xn) := minj2[n] ϵ(cid:18)(xj)  where [n] := f1; : : : ; ng for a positive integer n. Next 
we formulate the parameter transfer learnability based on the local stability.

3

(cid:3)
S by using N-samples. Next  for the parameter
RT ((cid:18)
(cid:3)
S ; w)

(cid:3)
T
w

:= argmin
w2WT

bwN;n

:= argmin
w2WT

bRT ;n(b(cid:18)N ; w) + (cid:26)r(w)

in the target region  the algorithm outputs its estimator

Deﬁnition 2 (Parameter Transfer Learnability). Suppose that N-samples in the source domain and
n-samples Xn in the target domain are available. Let a parametric feature mapping f (cid:18)g(cid:18)2(cid:2) be
locally stable. For (cid:22)(cid:14) 2 [0; 1)  f (cid:18)g(cid:18)2(cid:2) is said to be parameter transfer learnable with probability
1 (cid:0) (cid:22)(cid:14) if there exists an algorithm that depends only on N-samples in the source domain such that 

the outputb(cid:18)N of the algorithm satisﬁes
[
∥b(cid:18)N (cid:0) (cid:18)

Pr

]

S∥ (cid:20) ϵ(cid:18)
(cid:3)

S (Xn)
(cid:3)

(cid:21) 1 (cid:0) (cid:22)(cid:14):

In the following  we assume that parametric feature mapping is bounded as ∥ (cid:18)(x)∥2 (cid:20) R for
arbitrary x 2 X and (cid:18) 2 (cid:2) and linear predictors are also bounded as ∥w∥2 (cid:20) RW for any w 2 W.
In addition  we suppose that a loss function ℓ((cid:1);(cid:1)) is Lℓ-Lipschitz and convex with respect to the
second variable. We denote as Rr := supw2W jr(w)j. Then  the following learning bound is
obtained  where the strong convexity of the regularization term (cid:26)r(w) is essential.
Theorem 1 (Learning Bound). Suppose that the parametric feature mapping (cid:18) is locally stable
probability 1 (cid:0) (cid:22)(cid:14). When (cid:26) = LℓR 
1 (cid:0) ((cid:14) + 2(cid:22)(cid:14)):
RT

and an estimatorb(cid:18)N learned in the source region satisﬁes the parameter transfer learnability with
(cid:13)(cid:13)(cid:13)

  the following inequality holds with probability

(cid:13)(cid:13)(cid:13)b(cid:18)N (cid:0) (cid:18)

2Rr(32 + log(2=(cid:14)))

2 log(2=(cid:14)) + 2

+ LℓL R 

√

√

8(32+log(2=(cid:14)))

(cid:3)
S ; w

(cid:3)
T )

RW

Rrn

(cid:3)
S

)

(b(cid:18)N ;bwN;n
(
√
(cid:20) LℓR 
√

(cid:0) RT ((cid:18)
(
If the estimation error ∥b(cid:18)N (cid:0) (cid:18)

L RW R 

+Lℓ

Rr

2(32 + log(2=(cid:14)))
S∥ can be evaluated in terms of the number N of samples  Theorem
(cid:3)
1 clariﬁes which term is dominant  and in particular  the number of samples required in the source
domain such that this number is sufﬁciently large compared to the samples in the target domain.

)
√(cid:13)(cid:13)(cid:13)b(cid:18)N (cid:0) (cid:18)

1p
n
(cid:3)
S

(cid:13)(cid:13)(cid:13):

) 1

4

1
4

n

2.3 Proof of Learning Bound
We prove Theorem 1 in this subsection. In this proof  we omit the subscript T for simplicity. In
addition  we denote (cid:18)

(cid:3)
S simply by (cid:18)

. We set as

(cid:3)

(cid:3)(xj)⟩) + (cid:26)r(w):

Then  we have

(2)

(3)

ℓ(yj;⟨w; (cid:18)
]

n∑

j=1

(cid:3)
n

1
n

RT

(cid:0) RT ((cid:18)

= E(x;y)(cid:24)P

:= argmin
w2W

(cid:3)
(cid:3)
; w
)
(x)⟩)

bw
(b(cid:18)N ;bwN;n
)
[
ℓ(y;⟨bwN;n; b(cid:18)N
+E(x;y)(cid:24)P [ℓ(y;⟨bwN;n; (cid:18)
+E(x;y)(cid:24)P [ℓ(y;⟨bw
[
ℓ(y;⟨bwN;n; b(cid:18)N
[(cid:13)(cid:13)(cid:13) b(cid:18)N
(cid:13)(cid:13)(cid:13) b(cid:18)N
n∑
(cid:20) LℓRWE(x;y)(cid:24)P
(cid:20) LℓRW 1
(cid:13)(cid:13)(cid:13) + LℓRW R 
(cid:13)(cid:13)(cid:13)b(cid:18)N (cid:0) (cid:18)

(x)⟩)
(x) (cid:0) (cid:18)

(cid:20) LℓL RW

(xj) (cid:0) (cid:18)

E(x;y)(cid:24)P

(cid:3)
n; (cid:18)

(cid:3)(xj)

]

j=1

n

(cid:3)

(cid:0) E(x;y)(cid:24)P [ℓ(y;⟨bwN;n; (cid:18)

(cid:3)(x)⟩)] (cid:0) E(x;y)(cid:24)P [ℓ(y;⟨bw
(cid:3) (x)⟩)] (cid:0) E(x;y)(cid:24)P [ℓ(y;⟨w

(cid:3)
n; (cid:18)
; (cid:18)

(cid:3)(x)⟩)]
(cid:3)(x)⟩)]
(cid:3)(x)⟩)] :

(cid:3)

(cid:3)(x)⟩)]

(cid:0) E(x;y)(cid:24)P [ℓ(y;⟨bwN;n; (cid:18)
(cid:13)(cid:13)(cid:13)]
(cid:13)(cid:13)(cid:13) + LℓRW R 
√

√

2 log(2=(cid:14))

n

(cid:3)(x)

2 log(2=(cid:14))

n

;

4

In the following  we bound three parts of (3). First  we have the following inequality with probability
1 (cid:0) ((cid:14)=2 + (cid:22)(cid:14)):

where we used Hoeffding’s inequality as the third inequality  and the local stability and parameter
transfer learnability in the last inequality. Second  we have the following inequality with probability
1 (cid:0) (cid:22)(cid:14):

E(x;y)(cid:24)P [ℓ(y;⟨bwN;n; (cid:18)
(cid:3)(x)⟩)] (cid:0) E(x;y)(cid:24)P [ℓ(y;⟨bw
(cid:20) LℓE(x;y)(cid:24)P [j⟨bwN;n; (cid:18)
(cid:3)(x)⟩ (cid:0) ⟨bw
(cid:20) LℓR ∥bwN;n (cid:0)bw
(cid:13)(cid:13)(cid:13);
(cid:13)(cid:13)(cid:13)b(cid:18)N (cid:0) (cid:18)
(cid:20) LℓR 

(cid:3)
n
2LℓL RW

(cid:3)(x)⟩j]

(cid:3)
n; (cid:18)

√

∥

(cid:3)

2

(cid:26)

(cid:3)
n; (cid:18)

(cid:3) (x)⟩)]

(4)

where the last inequality is derived by the strong convexity of the regularizer (cid:26)r(w) in the Appendix.
Third  the following holds by Theorem 1 of Sridharan et al. (2009) with probability 1 (cid:0) (cid:14)=2:

E(x;y)(cid:24)P [ℓ(y;⟨bw
= E(x;y)(cid:24)P [ℓ(y;⟨bw
(cid:3)
n; (cid:18)
(cid:3)
(
n; (cid:18)
(cid:0)E(x;y)(cid:24)P [ℓ(y;⟨w
8L2
√

; (cid:18)
 (32 + log(2=(cid:14)))

ℓ R2

(cid:20)

(cid:26)n

(cid:3)

(cid:3) (x)⟩) + (cid:26)r(bw
(cid:3) (x)⟩)] (cid:0) E(x;y)(cid:24)P [ℓ(y;⟨w
(cid:3)
)
n)]
(cid:3)(x)⟩) + (cid:26)r(w
(cid:3)

)] + (cid:26)(r(w

(cid:3)

(cid:3) (x)⟩)]

; (cid:18)

) (cid:0) r(bw

(cid:3)

(cid:3)
n))

+ (cid:26)Rr:

Thus  when (cid:26) = LℓR 

8(32+log(2=(cid:14)))

Rrn

  we have (2) with probability 1 (cid:0) ((cid:14) + 2(cid:22)(cid:14)).

3 Stability and Learnability in Sparse Coding

In this section  we consider the sparse coding in self-taught learning  where the source region es-
sentially consists of the sample space XS without the label space YS. We assume that the sample
spaces in both regions are Rd. Then  the sparse coding method treated here consists of a two-stage
procedure  where a dictionary is learnt on the source region  and then a sparse coding with the learnt
dictionary is used for a predictive task in the target region.
First  we show that sparse coding satisﬁes the local stability in Section 3.1 and next explain that
appropriate dictionary learning algorithms satisfy the parameter transfer learnability in Section 3.4.
As a consequence of Theorem 1  we obtain the learning bound of self-taught learning algorithms
based on sparse coding. We note that the results in this section are useful independent of transfer
learning.
We here summarize the notations used in this section. Let ∥ (cid:1) ∥p be the p-norm on Rd. We deﬁne as
supp(a) := fi 2 [m]jai ̸= 0g for a 2 Rm. We denote the number of elements of a set S by jSj.
When a vector a satisﬁes ∥a∥0 = jsupp(a)j (cid:20) k  a is said to be k-sparse. We denote the ball with
radius R centered at 0 by BRd(R) := fx 2 Rdj∥x∥2 (cid:20) Rg. We set as D := fD = [d1; : : : ; dm] 2
BRd(1)mj ∥dj∥2 = 1 (i = 1; : : : ; m)g and each D 2 D a dictionary with size m.
Deﬁnition 3 (Induced matrix norm). For an arbitrary matrix E = [e1; : : : ; em] 2 Rd(cid:2)m  1) the
induced matrix norm is deﬁned by ∥E∥1;2 := maxi2[m] ∥ei∥2.
We adopt ∥(cid:1)∥1;2 to measure the difference of dictionaries since it is typically used in the framework
of dictionary learning. We note that ∥D (cid:0) ~D∥1;2 (cid:20) 2 holds for arbitrary dictionaries D; ~D 2 D.

3.1 Local Stability of Sparse Representation

We show the local stability of sparse representation under a sparse model. A sparse representation
with dictionary parameter D of a sample x 2 Rd is expressed as follows:

φD(x) := argmin
z2Rm

∥x (cid:0) Dz∥2

2 + (cid:21)∥z∥1;

1
2

1) In general  the (p; q)-induced norm for p; q (cid:21) 1 is deﬁned by ∥E∥p;q := supv2Rm;∥v∥p=1

∥Ev∥q. Then 
∥ (cid:1) ∥1;2 in this general deﬁnition coincides with that in Deﬁnition 3 by Lemma 17 of Vainsencher et al. (2011).

5

where (cid:21) > 0 is a regularization parameter. This situation corresponds to the case where (cid:18) = D
and (cid:18) = φD in the setting of Section 2.1. We prepare some notions to the stability of the sparse
representation. The following margin and incoherence were introduced by Mehta and Gray (2013).
Deﬁnition 4 (k-margin). Given a dictionary D = [d1; : : : ; dm] 2 D and a point x 2 Rd  the
k-margin of D on x is

f(cid:21) (cid:0) j⟨dj; x (cid:0) DφD(x)⟩jg :

Mk(D; x) :=
Deﬁnition 5 ((cid:22)-incoherence). A dictionary matrix D = [d1; : : : ; dm] 2 D is termed (cid:22)-incoherent
p
if j⟨di; dj⟩j (cid:20) (cid:22)=
Then  the following theorem is obtained.
Theorem 2 (Sparse Coding Stability). Let D 2 D be (cid:22)-incoherent and ∥D (cid:0) ~D∥1;2 (cid:20) (cid:21). When

d for all i ̸= j.

min
j2I

max

I(cid:26)[m];jIj=m(cid:0)k

∥D (cid:0) ~D∥1;2 (cid:20) ϵk;D(x) :=

(5)

the following stability bound holds:

∥φD(x) (cid:0) φ ~D(x)∥

2

Mk;D(x)2(cid:21)
64 maxf1;∥x∥g4 ;
p
(cid:20) 4∥x∥2
p
k
(1 (cid:0) (cid:22)k=
d)(cid:21)

∥D (cid:0) ~D∥1;2:

From Theorem 2  ϵk;D(x) becomes the permissible radius of perturbation in Deﬁnition 1.
Here  we refer to the relation with the sparse coding stability (Theorem 4) of Mehta and Gray (2013) 
who measured the difference of dictionaries by ∥ (cid:1) ∥2;2 instead of ∥ (cid:1) ∥1;2 and the permissible radius
of perturbation is given by Mk;D(x)2(cid:21) except for a constant factor. Applying the simple inequality
∥E∥2;2 (cid:20) p
m∥E∥1;2 for E 2 Rd(cid:2)m  we can obtain a variant of the sparse coding stability with the
norm ∥(cid:1)∥1;2. However  then the dictionary size m affects the permissible radius of perturbation and
the stability bound of the sparse coding stability. On the other hand  the factor of m does not appear
in Theorem 2  and thus  the result is effective even for a large m. In addition  whereas ∥x∥ (cid:20) 1
is assumed in Mehta and Gray (2013)  Theorem 2 does not assume that ∥x∥ (cid:20) 1 and clariﬁes the
dependency for the norm ∥x∥.
In existing studies related to sparse coding  the sparse representation φD(x) is modiﬁed as φD(x)(cid:10)
x (Mairal et al. (2009)) or φD(x) (cid:10) (x (cid:0) DφD(x)) (Raina et al. (2007)) where (cid:10) is the tensor
product. By the stability of sparse representation (Theorem 2)  it can be shown that such modiﬁed
representations also have local stability.

3.2 Sparse Modeling and Margin Bound
In this subsection  we assume a sparse structure for samples x 2 Rd and specify a lower bound
for the k-margin used in (5). The result obtained in this section plays an essential role to show the
parameter transfer learnability in Section 3.4.
(cid:3) such that every sample x is indepen-
Assumption 1 (Model). There exists a dictionary matrix D
dently generated by a representation a and noise (cid:24) as

(cid:3)

x = D

a + (cid:24):

Moreover  we impose the following three assumptions on the above model.
= [d1; : : : ; dm] 2 D is (cid:22)-incoherent.
Assumption 2 (Dictionary). The dictionary matrix D
Assumption 3 (Representation). The representation a is a random variable that is k-sparse (i.e. 
∥a∥0 (cid:20) k) and the non-zero entries are lower bounded by C > 0 (i.e.  ai ̸= 0 satisfy jaij (cid:21) C).
p
Assumption 4 (Noise). The noise (cid:24) is independent across coordinates and sub-Gaussian with pa-
rameter (cid:27)=

d on each component.

(cid:3)

We note that the assumptions do not require the representation a or noise (cid:24) to be identically dis-
tributed while those components are independent. This is essential because samples in the source
and target domains cannot be assumed to be identically distributed in transfer learning.

6

(cid:14)t;(cid:21)

:=

Theorem 3 (Margin Bound). Let 0 < t < 1. We set as

(

)

exp

p
2(cid:27)
(1 (cid:0) t)
√
d(cid:21)
{(
d(1 (cid:0) (cid:22)k=

4(cid:27)k

C

+

+

8(cid:27)2

(cid:0) (1 (cid:0) t)2d(cid:21)2
(
2(cid:27)mp
d(cid:21)
(cid:0) C 2d(1 (cid:0) (cid:22)k=
}2

)

8(cid:27)2

exp

p
d)

)

(
)

exp
p
d)

(cid:0) d(cid:21)2
8(cid:27)2
8(cid:27)(d (cid:0) k)

p

+

d(cid:21)

(

exp

(cid:0) d(cid:21)2
32(cid:27)2

)

: (6)

We suppose that d (cid:21)
Assumptions 1-4  the following inequality holds with probability 1 (cid:0) (cid:14)t;(cid:21) at least:

(cid:0)(cid:28) for arbitrary 1=4 (cid:20) (cid:28) (cid:20) 1=2. Under

and (cid:21) = d

1 + 6

(1(cid:0)t)

(cid:22)k

Mk;D(cid:3)(x) (cid:21) t(cid:21):

(7)

(8)

(9)

We refer to the regularization parameter (cid:21). An appropriate reﬂection of the sparsity of samples
requires the regularization parameter (cid:21) to be set suitably. According to Theorem 4 of Zhao and Yu
(2006)2)  when samples follow the sparse model as in Assumptions 1-4 and (cid:21) (cid:24)
(cid:0)(cid:28) for 1=4 (cid:20) (cid:28) (cid:20)
1=2  the representation φD(x) reconstructs the true sparse representation a of sample x with a small
error. In particular  when (cid:28) = 1=4 (i.e.  (cid:21) (cid:24)
(cid:24)
(cid:0)p
=
d on the margin is guaranteed to become sub-exponentially small with respect to dimension d
e
and is negligible for the high-dimensional case. On the other hand  the typical choice (cid:28) = 1=2 (i.e. 
(cid:21) (cid:24)
(cid:0)1=2) does not provide a useful result because (cid:14)t;(cid:21) is not small at all.

(cid:0)1=4) in Theorem 3  the failure probability (cid:14)t;(cid:21)

= d

= d

= d

3.3 Proof of Margin Bound

(cid:3)

We give a sketch of proof of Theorem 3. We denote the ﬁrst term  the second term and the sum of
the third and fourth terms of (6) by (cid:14)1  (cid:14)2 and (cid:14)3  respectively From Assumptions 1 and 3  a sample
a + (cid:24) and ∥a∥0 (cid:20) k. Without loss of generality  we assume that the ﬁrst
is represented as x = D
m (cid:0) k components of a are 0 and the last k components are not 0. Since
Mk;D(cid:3) (x) (cid:21) min
dj; a (cid:0) φD(x)⟩;
(cid:21) (cid:0) ⟨dj; x (cid:0) D
(cid:3)
it is enough to show that the following holds an arbitrary 1 (cid:20) j (cid:20) m (cid:0) k to prove Theorem 3:

φD(x)⟩ = min

(cid:21) (cid:0) ⟨dj; (cid:24)⟩ (cid:0) ⟨D

1(cid:20)j(cid:20)m(cid:0)k

1(cid:20)j(cid:20)m(cid:0)k

(cid:3)⊤

Then  (8) follows from the following inequalities:
⟨dj; (cid:24)⟩ >
dj; a (cid:0) φD(x)⟩ >

[
⟨D

(cid:3)⊤

Pr

Pr[⟨dj; (cid:24)⟩ + ⟨D

(cid:3)⊤

dj; a (cid:0) φD(x)⟩ > (1 (cid:0) t)(cid:21)] (cid:20) (cid:14)t;(cid:21):
[

]
]

(cid:21)

1 (cid:0) t
2
1 (cid:0) t
2

(cid:20) (cid:14)1;
(cid:20) (cid:14)2 + (cid:14)3:

Pr

(10)
The inequality (9) holds since ∥dj∥ = 1 by the deﬁnition and Assumption 4. Thus  all we have to
do is to show (10). We have

(cid:21)

(cid:3)⊤

⟨D

dj; a (cid:0) φD(x)⟩ = ⟨[⟨d1; dj⟩; : : : ;⟨dm; dj⟩]

⊤

; a (cid:0) φD(x)⟩

= ⟨(1supp(a(cid:0)φD(x)) ◦ [⟨d1; dj⟩; : : : ;⟨dm; dj⟩])
⊤
(cid:20) ∥1supp(a(cid:0)φD(x)) ◦ [⟨d1; dj⟩; : : : ;⟨dm; dj⟩]∥2∥a (cid:0) φD(x)∥2;(11)
where u ◦ v is the Hadamard product (i.e. component-wise product) between u and v  and 1A for a
set A (cid:26) [m] is a vector whose i-th component is 1 if i 2 A and 0 otherwise.
Applying Theorem 4 of Zhao and Yu (2006) and using the condition for (cid:21)  the following holds with
probability 1 (cid:0) (cid:14)3:

; a (cid:0) φD(x)⟩

supp(a) = supp(φD(x)):

(12)
2)Theorem 4 of Zhao and Yu (2006) is stated for Gaussian noise. However  it can be easily generalized to
sub-Gaussian noise as in Assumption 4. Our setting corresponds to the case in which c1 = 1=2; c2 = 1; c3 =
(log (cid:20) + log log d)= log d for some (cid:20) > 1 (i.e.  edc3 (cid:24)
= d(cid:20)) and c4 = c in Theorem 4 of Zhao and Yu (2006).
Note that our regularization parameter (cid:21) corresponds to (cid:21)d=d in (Zhao and Yu (2006)).

7

Moreover  under (12)  the following holds with probability 1 (cid:0) (cid:14)2 by modifying Corollary 1 of
Negahban et al. (2009) and using the condition for (cid:21):
p
∥a (cid:0) φD(x)∥2 (cid:20) 6
k(cid:21)
1 (cid:0) (cid:22)kp

(13)

:

d

Thus  if both of (12) and (13) hold  the right hand side of (11) is bounded as follows:
√
∥1supp(a(cid:0)φD(x)) ◦ [⟨d1; dj⟩; : : : ;⟨dm; dj⟩]∥2∥a (cid:0) φD(x)∥2
(cid:21) (cid:20) 1 (cid:0) t
jsupp(a (cid:0) φD(x))j (cid:22)p

(cid:20)

(cid:21);

=

6(cid:22)kp
d (cid:0) (cid:22)k

2

p
k(cid:21)
6
1 (cid:0) (cid:22)kp

d

d

where we used Assumption 2 in the ﬁrst inequality  (12) and Assumption 3 in the equality and the
condition for d in the last inequality. From the above discussion  the left hand side of (10) is bounded
by the sum of the probability (cid:14)3 that (12) does not hold and the probability (cid:14)2 that (12) holds but
(13) does not hold.

3.4 Transfer Learnability for Dictionary Learning

(cid:3) exists as in Assumption 1  we show that the output bDN of a suitable
(cid:3) b(cid:18)N = bDN and ∥ (cid:1) ∥ = ∥ (cid:1) ∥1;2 in Section 2.1.

When the true dictionary D
dictionary learning algorithm from N-unlabeled samples satisﬁes the parameter transfer learnability
for the sparse coding φD. Then  Theorem 1 guarantees the learning bound in self-taught learning
since the discussion in this section does not assume the label space in the source region. This
situation corresponds to the case where (cid:18)
We show that an appropriate dictionary learning algorithm satisﬁes the parameter transfer learnabil-
ity for the sparse coding φD by focusing on the permissible radius of perturbation in (5) under some
(cid:0)(cid:28) for 1=4 (cid:20) (cid:28) (cid:20) 1=2  the margin bound (7)
assumptions. When Assumptions 1-4 hold and (cid:21) = d
for x 2 X holds with probability 1 (cid:0) (cid:14)t;(cid:21)  and thus  we have

(cid:3)
S = D

t2(cid:21)3

(cid:0)3(cid:28) ):

ϵk;D(cid:3)(x) (cid:21)

64 maxf1;∥x∥g4 = (cid:2)(d

Thus  if a dictionary learning algorithm outputs the estimator bDN such that
∥bDN (cid:0) D
with probability 1 (cid:0) (cid:14)N   the estimator bDN of D

(14)
(cid:3) satisﬁes the parameter transfer learnability for the
sparse coding φD with probability (cid:22)(cid:14) = (cid:14)N + n(cid:14)t;(cid:21). Then  by the local stability of the sparse repre-
sentation and the parameter transfer learnability of such a dictionary learning  Theorem 1 guarantees
that sparse coding in self-taught learning satisﬁes the learning bound in (2).
p
We note that Theorem 1 can apply to any dictionary learning algorithm as long as (14) is satisﬁed.
For example  Arora et al. (2015) show that  when k = O(
d= log d)  m = O(d)  Assumptions 1-4

and some additional conditions are assumed  their dictionary learning algorithm outputs bDN which

(cid:3)∥1;2 (cid:20) O(d

(cid:0)3(cid:28) )

satisﬁes

with probability 1 (cid:0) d

(cid:0)M

′

4 Conclusion

∥bDN (cid:0) D

(cid:3)∥1;2 = O(d

(cid:0)M )

for arbitrarily large M; M

′ as long as N is sufﬁciently large.

We derived a learning bound (Theorem 1) for a parameter transfer learning problem based on the
local stability and parameter transfer learnability  which are newly introduced in this paper. Then 
applying it to a sparse coding-based algorithm under a sparse model (Assumptions 1-4)  we obtained
the ﬁrst theoretical guarantee of a learning bound in self-taught learning. Although we only consider
sparse coding  the framework of parameter transfer learning includes other promising algorithms
such as multiple kernel learning and deep neural networks  and thus  our results are expected to
be effective to analyze the theoretical performance of these algorithms. Finally  we note that our
learning bound can be applied to different settings from self-taught learning because Theorem 1
includes the case in which labeled samples are available in the source region.

8

References
[1] S. Arora  R. Ge  T. Ma  and A. Moitra (2015) “Simple  efﬁcient  and neural algorithms for

sparse coding ” arXiv preprint arXiv:1503.00778.

[2]

J. Baxter (2000) “A model of inductive bias learning ” J. Artif. Intell. Res.(JAIR)  Vol. 12  p. 3.

[3] A. Coates  A. Y. Ng  and H. Lee (2011) “An analysis of single-layer networks in unsupervised
feature learning ” in International conference on artiﬁcial intelligence and statistics  pp. 215–
223.

[4] W. Dai  Q. Yang  G.-R. Xue  and Y. Yu (2008) “Self-taught clustering ” in Proceedings of the

25th international conference on Machine learning  pp. 200–207  ACM.

[5] Q. V. Le (2013) “Building high-level features using large scale unsupervised learning ” in
Acoustics  Speech and Signal Processing (ICASSP)  2013 IEEE International Conference on 
pp. 8595–8598  IEEE.

[6] H. Lee  R. Raina  A. Teichman  and A. Y. Ng (2009) “Exponential Family Sparse Coding

with Application to Self-taught Learning ” in IJCAI  Vol. 9  pp. 1113–1119  Citeseer.

[7]

J. Mairal  J. Ponce  G. Sapiro  A. Zisserman  and F. R. Bach (2009) “Supervised dictionary
learning ” in Advances in neural information processing systems  pp. 1033–1040.

[8] A. Maurer (2009) “Transfer bounds for linear feature learning ” Machine learning  Vol. 75 

pp. 327–350.

[9] A. Maurer  M. Pontil  and B. Romera-Paredes (2012) “Sparse coding for multitask and trans-

fer learning ” arXiv preprint arXiv:1209.0738.

[10] N. Mehta and A. G. Gray (2013) “Sparsity-based generalization bounds for predictive sparse
coding ” in Proceedings of the 30th International Conference on Machine Learning (ICML-
13)  pp. 36–44.

[11] S. Negahban  B. Yu  M. J. Wainwright  and P. K. Ravikumar (2009) “A uniﬁed framework for
high-dimensional analysis of M-estimators with decomposable regularizers ” in Advances in
Neural Information Processing Systems  pp. 1348–1356.

[12] S. J. Pan and Q. Yang (2010) “A survey on transfer learning ” Knowledge and Data Engineer-

ing  IEEE Transactions on  Vol. 22  pp. 1345–1359.

[13] R. Raina  A. Battle  H. Lee  B. Packer  and A. Y. Ng (2007) “Self-taught learning: transfer
learning from unlabeled data ” in Proceedings of the 24th international conference on Ma-
chine learning  pp. 759–766  ACM.

[14] K. Sridharan  S. Shalev-Shwartz  and N. Srebro (2009) “Fast rates for regularized objectives ”

in Advances in Neural Information Processing Systems  pp. 1545–1552.

[15] D. Vainsencher  S. Mannor  and A. M. Bruckstein (2011) “The sample complexity of dictio-

nary learning ” The Journal of Machine Learning Research  Vol. 12  pp. 3259–3281.

[16] H. Wang  F. Nie  and H. Huang (2013) “Robust and discriminative self-taught learning ” in

Proceedings of The 30th International Conference on Machine Learning  pp. 298–306.

[17] P. Zhao and B. Yu (2006) “On model selection consistency of Lasso ” The Journal of Machine

Learning Research  Vol. 7  pp. 2541–2563.

[18] X. Zhu  Z. Huang  Y. Yang  H. T. Shen  C. Xu  and J. Luo (2013) “Self-taught dimensionality
reduction on the high-dimensional small-sized data ” Pattern Recognition  Vol. 46  pp. 215–
229.

9

,Andreas Stuhlmüller
Jacob Taylor
Noah Goodman
Wataru Kumagai