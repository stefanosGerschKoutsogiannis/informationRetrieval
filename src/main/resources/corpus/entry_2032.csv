2018,Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions,Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have  recently received increasing attention in the field  of optimization for developing optimization algorithms with fast convergence.  However   the studies of EBC in statistical learning are hitherto still limited.  The main contributions of this paper are two-fold. First   we develop fast and intermediate rates of  empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous  and  smooth  convex random functions. Second  we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization  with Lipschitz continuous random functions  which requires only one pass of $n$ samples and adapts to EBC. For both approaches  the convergence rates span a full spectrum between $\widetilde O(1/\sqrt{n})$ and $\widetilde O(1/n)$ depending on the power constant in EBC  and could be even faster than $O(1/n)$ in special cases for ERM. Moreover  these  convergence rates are automatically adaptive without using any knowledge of EBC. Overall  this work not only strengthens the understanding of ERM for statistical learning but also brings new fast stochastic algorithms for solving a broad range of statistical learning problems.,Fast Rates of ERM and Stochastic Approximation:

Adaptive to Error Bound Conditions

Mingrui Liu†  Xiaoxuan Zhang†  Lijun Zhang‡  Rong Jin(cid:92)  Tianbao Yang†

†Department of Computer Science  The University of Iowa  Iowa City  IA 52242  USA
‡National Key Laboratory for Novel Software Technology  Nanjing University  China

(cid:92) Machine Intelligence Technology  Alibaba Group  Bellevue  WA 98004  USA

mingrui-liu@uiowa.edu  zljzju@gmail.com  tianbao-yang@uiowa.edu

Abstract

Error bound conditions (EBC) are properties that characterize the growth of an
objective function when a point is moved away from the optimal set. They have
recently received increasing attention for developing optimization algorithms with
fast convergence. However  the studies of EBC in statistical learning are hitherto
still limited. The main contributions of this paper are two-fold. First  we develop
fast and intermediate rates of empirical risk minimization (ERM) under EBC
for risk minimization with Lipschitz continuous  and smooth convex random
functions. Second  we establish fast and intermediate rates of an efﬁcient stochastic
approximation (SA) algorithm for risk minimization with Lipschitz continuous
random functions  which requires only one pass of n samples and adapts to EBC.
n)

For both approaches  the convergence rates span a full spectrum between (cid:101)O(1/
and (cid:101)O(1/n) depending on the power constant in EBC  and could be even faster

than O(1/n) in special cases for ERM. Moreover  these convergence rates are
automatically adaptive without using any knowledge of EBC.

√

1

Introduction

In this paper  we focus on the following stochastic convex optimization problems arising in statistical
learning and many other ﬁelds:

w∈W P (w) (cid:44) Ez∼P[f (w  z)] 

min

(1)

and more generally

min

w∈W P (w) (cid:44) Ez∼P[f (w  z)] + r(w) 

(2)
where f (·  z) : W → R is a random function depending on a random variable z ∈ Z that follows
a distribution P  r(w) is a lower semi-continuous convex function. In statistical learning [48]  the
problem above is also referred to as risk minimization where z is interpreted as data  w is interpreted
as a model (or hypothesis)  f (· ·) is interpreted as a loss function  and r(·) is a regularization. For
example  in supervised learning one can take z = (x  y) - a pair of feature vector x ∈ X ⊆ Rd
and label y ∈ Y  f (w  z) = (cid:96)(w(x)  y) - a loss function measuring the error of the prediction
w(x) : X → Y made by the model w. Nonetheless  we emphasize that the risk minimization
problem (1) is more general than supervised learning and could be more challenging (c.f. [35]). In
this paper  we assume that W ⊆ Rd is a compact and convex set. Let W∗ = arg minw∈W P (w)
denote the optimal set and P∗ = minw∈W P (w) denote the optimal risk.
There are two popular approaches for solving the risk minimization problem. The ﬁrst one is by
empirical risk minimization that minimizes the empirical risk deﬁned over a set of n i.i.d. samples
drawn from the same distribution P (sometimes with a regularization term on the model). The second
32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

approach is called stochastic approximation that iteratively learns the model from random samples
zt ∼ P  t = 1  . . .   n. Both approaches have been studied broadly and extensive results are available
about the theoretical guarantee of the two approaches in the machine learning and optimization
community. A central theme in these studies is to bound the excess risk (or optimization error) of a

learned model (cid:98)w measured by P ((cid:98)w) − P∗  i.e.  given a set of n samples (z1  . . .   zn) how fast the
order of (cid:101)O((cid:112)d/n) 1 and O((cid:112)1/n) for ERM and SA  respectively  under appropriate conditions of

learned model converges to the optimal model in terms of the excess risk.
A classical result about the excess risk bound for the considered risk minimization problem is in the

the loss functions (e.g.  Lipschitz continuity  convexity) [29  35]. Various studies have attempted to
establish faster rates by imposing additional conditions on the loss functions (e.g.  strong convexity 
smoothness  exponential concavity) [13  42  21]  or on both the loss functions and the distribution
(e.g.  Tsybakov condition  Bernstein condition  central condition) [45  3  46]. In this paper  we will
study a different family of conditions called the error bound conditions (EBC) (see Deﬁnition 1) 
which has a long history in the community of optimization and variational analysis [31] and recently
revived for developing fast optimization algorithms without strong convexity [4  6  17  28  54].
However  the exploration of EBC in statistical learning for risk minimization is still under-explored
and the connection to other conditions is not fully understood.
Deﬁnition 1. For any w ∈ W  let w∗ = arg minu∈W∗ (cid:107)u− w(cid:107)2 denote an optimal solution closest
to w  where W∗ is the set containing all optimal solutions. Let θ ∈ (0  1] and 0 < α < ∞. The
problem (1) satisﬁes an EBC(θ  α) if for any w ∈ W  the following inequality holds

(cid:107)w − w∗(cid:107)2

2 ≤ α(P (w) − P (w∗))θ.

(3)

dition  and therefore leads to intermediate rates of (cid:101)O((d/n)

This condition has been well studied in optimization and variational analysis. Many results are
available for understanding the condition for different problems. For example  it has been shown that
when P (w) is semi-algebraic and continuous  the inequality (3) is known to hold on any compact
set with certain θ ∈ (0  1] and α > 0 [4] 2. We will study both ERM and SA under the above error
bound condition. In particular  we show that the beneﬁts of exploiting EBC in statistical learning are
noticeable and profound by establishing the following results.
• Result I. First  we show that for Lipchitz continous loss EBC implies a relaxed Bernstein con-
2−θ ) for Lipschitz continuous loss.
Although this result does not improve over existing rates based on Bernstein condition  however 
we emphasize that it provides an alternative route for establishing fast rates and brings richer results
than literature to statistical learning in light of the examples provided in this paper.
• Result II. Second  we develop fast and optimistic rates of ERM for non-negative  Lipschitz
2−θ )  and in the
2−θ ) when the sample size n is sufﬁciently large  which imply

continuous and smooth convex loss functions in the order of (cid:101)O(d/n + (dP∗/n)
order of (cid:101)O((d/n)
that when the optimal risk P∗ is small one can achieve a fast rate of (cid:101)O (d/n) even with θ < 1 and
a faster rate of (cid:101)O((d/n)
(cid:101)O((1/n)

• Result III. Third  we develop an efﬁcient SA algorithm with almost the same per-iteration cost as
stochastic subgradient methods for Lipschitz continuous loss  which achieves the same order of rate
2−θ ) as ERM without an explicit dependence on d. More importantly it is “parameter”-

2−θ ) when n is sufﬁciently large.

2−θ + (dP∗/n)

2

2

1

1

1

free with no need of prior knowledge of θ and α in EBC.

1

Overall  these results not only strengthen the understanding of ERM for statistical learning but also
bring new fast stochastic algorithms for solving a broad range of statistical learning problems. Before
ending this section  we would like to point out that all the results are automatically adaptive to the
largest possible value of θ ∈ (0  1] in hindsight of the problem  and the dependence on d for ERM is
generally unavoidable according to the lower bounds studied in [9].

2 Related Work

The results for statistical learning under EBC are limited. A similar one to our Result I for ERM was
established in [39]. However  their result requires the convexity condition of random loss functions 

1(cid:101)O hides a poly-logarithmic factor of n.

2One may consider θ ∈ (1  2]  which will yield the same order of excess risk bound as θ = 1 in our settings.

2

making it weaker than our result. Ramdas and Singh [33] and Xu et al. [50] considered SA under the
EBC condition and established similar adaptive rates. Nonetheless  their stochastic algorithms require
knowing the values of θ and possibly the constant α in the EBC. In contrast  the SA algorithm in
this paper is “parameter"-free without the need of knowing θ and α while still achieving the adaptive
rates of O(1/n2−θ). Fast rates under strong convexity (a special case of EBC) are well-known for
ERM  online optimization and SA [35  43  13  16  36  14]. In the presence of strong convexity of
P (w)  our results of ERM and SA recover known rates (see below for more discussions).
Fast (intermediate) rates of ERM have been studied under various conditions  including Tsybakov
margin condition [44  25]  Bernstein condition [3  2  19]  exp-concavity condition [21  11  26  51] 
mixability condition [27]  central condition [46]  etc. The Bernstein condition (see Deﬁnition 2)
is a generalization of Tsybakov margin condition for classiﬁcation. The connection between the
exp-concavity condition  the Bernstein condition and the v-central condition was studied in [46].
In particular  the exp-concavity implies a v-central condition under an appropriate condition of
the decision set W (e.g.  well-speciﬁcity or convexity). With the bounded loss condition  the
Bernstein condition implies the v-central condition and the v-central condition also implies a Bernstein
condition.
In this work  we also study the connection between the EBC and the Bernstein condition and the
v-central condition. In particular  we will develop weaker forms of the Bernstein condition and the
v-central condition from the EBC for Lipschitz continuous loss functions. Building on this connection 
we establish our Result I  which is on a par with existing results for bounded loss functions relying
on the Bernstein condition or the central condition. Nevertheless  we emphasize that employing
the EBC for developing fast rates has noticeable beneﬁts: (i) it is complementary to the Bernstein
condition and the central condition and enjoyed by several interesting problems whose fast rates are
not exhibited yet; (ii) it can be leveraged for developing fast and intermediate optimistic rates for
non-negative and smooth loss functions; (iii) it can be leveraged to develop efﬁcient SA algorithms
with intermediate and fast convergence rates.

Sebro et al. [42] established an optimistic rate of O(1/n +(cid:112)P∗/n) of both ERM and SA for

supervised learning with generalized linear loss functions. However  their SA algorithm requires
knowing the value of P∗. Recently  Zhang et al. [55] considered the general stochastic optimization
problem (1) with non-negative and smooth loss functions and achieved a series of optimistic results.
It is worth mentioning that their excess risk bounds for both convex problems and strongly convex
problems are special cases of our Result II when θ = 0 and θ = 1  respectively. However  the
intermediate optimistic rates for θ ∈ (0  1) are ﬁrst shown in this paper. Importantly  our Result II
under the EBC with θ = 1 is more general than the result in [55] under strong convexity assumption.
Finally  we discuss about stochastic approximation algorithms with fast and intermediate rates to
understand the signiﬁcance of our Result III. Different variants of stochastic gradient methods have
been analyzed for stochastic strongly convex optimization [14  32  38] with a fast rate of O(1/n). But
these stochastic algorithms require knowing the strong convexity modulus. A recent work established
1−θ
adaptive regret bounds O(n
2−θ ) for online learning with a total of n rounds under the Bernstein
condition [20]. However  their methods are based on the second-order methods and therefore are not
as efﬁcient as our stochastic approximation algorithm. For example  for online convex optimization
they employed the MetaGrad algorithm [47]  which needs to maintain log(n) copies of the online
Newton step (ONS) [13] with different learning rates. Notice that the per-iteration cost of ONS is
usually O(d4) even for very simple domain W [21]  while that of our SA algorithm is dominated by
the Euclidean projection onto W that is as fast as O(d) for a simple domain.

3 Empirical Risk Minimization (ERM)

We ﬁrst formally state the minimal assumptions that are made throughout the paper. Additional
assumptions will be made in the sequel for developing fast rates for different families of the random
functions f (w  z).
Assumption 1. For the stochastic optimization problems (1) and (2)  we assume: (i) P (w) is a
convex function  W is a closed and bounded convex set  i.e.  there exists R > 0 such that (cid:107)w(cid:107)2 ≤ R
for any w ∈ W  and r(w) is a Lipschitz continuous convex function. (ii) the problem (1) and (2)
satisfy an EBC(θ  α)  i.e.  there exist θ ∈ (0  1] and 0 < α < ∞ such that the inequality (3) hold.

3

(cid:98)w ∈ arg min

w∈W Pn(w) (cid:44) 1

n

n(cid:88)

In this section  we focus on the development of theory of ERM for risk minimization. In particular 

we learn a model (cid:98)w by solving the following ERM problem corresponding to (1):

(4)
where z1  . . .   zn are i.i.d samples following the distribution P. A similar ERM problem can be
formulated for (2). This section is divided into two subsections. First  we establish intermediate
rates of ERM under EBC when the random function is Lipschitz continuous. Second  we develop
intermediate rates of ERM under EBC when the random function is smooth. In the sequel and the
supplement  we use ∨ to denote the max operation and use ∧ to denote the min operation.

f (w  zi)

i=1

3.1 ERM for Lipschitz continuous random functions

In this subsection  w.l.o.g we restrict our attention to (1) since we make the following assumption
besides Assumption 1. If r(w) is present  it can be absorbed into f (w  z).
Assumption 2. For the stochastic optimization problem (1)  we assume that f (w  z) is a G-Lipschitz
continuous function w.r.t w for any z ∈ Z.

It is notable that we do not assume f (w  z) is convex in terms of w or any z. First  we compare
EBC with two very important conditions considered in literature for developing fast rates of ERM 
namely the Bernstein condition and the central condition. We ﬁrst give the deﬁnitions of these two
conditions.
Deﬁnition 2. (Bernstein Condition) Let β ∈ (0  1] and B ≥ 1. Then (f  P W) satisﬁes the (β  B)-
Bernstein condition if there exists a w∗ ∈ W such that for any w ∈ W

Ez[(f (w  z) − f (w∗  z))2] ≤ B(Ez[f (w  z) − f (w∗  z)])β.

(5)

(6)

It is clear that if such an w∗ exists it has to be the minimizer of the risk.
Deﬁnition 3. (v-Central Condition) Let v : [0 ∞) → [0 ∞) be a bounded  non-decreasing function
satisfying v(x) > 0 for all x > 0. We say that (f  P W) satisﬁes the v-central condition if for all
ε ≥ 0  there exists w∗ ∈ W such that for any w ∈ W the following holds with η = v(ε).

(cid:104)
eη(f (w∗ z)−f (w z))(cid:105) ≤ eηε.

Ez∼P

If v(ε) is a constant for all ε ≥ 0  the v-central condition reduces to the strong η-central condition 
which implies the O(1/n) fast rate [46]. The connection between the Bernstein condition or v-central
condition has been studied in [46]. For example  if the random functions f (w  z) take values in [0  a] 
then (β  B)-Bernstein condition implies v-central condition with v(x) ∝ x1−β.
The following lemma shows that for Lipchitz continuous function  EBC condition implies a relaxed
Bernstein condition and a relaxed v-central condition.
Lemma 1. (Relaxed Bernstein condition and v-central condition) Suppose Assumptions 1  2 hold.
For any w ∈ W  there exists w∗ ∈ W∗ (which is actually the one closest to w)  such that

where B = G2α  and Ez∼P(cid:2)eη(f (w∗ z)−f (w z))(cid:3) ≤ eηε  where η = v(ε) := cε1−θ ∧ b. Additionally 
for any ε > 0 if P (w) − P (w∗) ≥ ε  we have Ez∼P(cid:2)ev(ε)(f (w∗ z)−f (w z))(cid:3) ≤ 1  where b > 0 is any

Ez[(f (w  z) − f (w∗  z))2] ≤ B(Ez[f (w  z) − f (w∗  z)])θ 

constant and c = 1/(αG2κ(4GRb))  where κ(x) = (ex − x − 1)/x2.

Remark: There is a subtle difference between the above relaxed Bernstein condition and v-central
condition and their original deﬁnitions in Deﬁnitions 2 and 3. The difference is that in Deﬁnitions 2
and 3  it requires there exists a universal w∗ for all w ∈ W such that (5) and (6) hold. In Lemma 1
it only requires for every w ∈ W there exists one w∗ that could be different for different w such
that (5) and (6) hold. This relaxation enables us to establish richer results by exploring EBC than the
Bernstein condition and v-central condition  which are postponed to Section 5.
Next  we present the main result of this subsection.

4

Theorem 1 (Result I). Suppose Assumptions 1  2 hold. For any n ≥ aC  with probability at least
1 − δ we have

P ((cid:98)w) − P∗ ≤ O

(cid:18) d log n + log(1/δ)

(cid:19) 1

2−θ

n

 

(7)

where a = 3(d log(32GRn1/(2−θ)) + log(1/δ))/c + 1 and C > 0 is some constant.

Remark: The proof utilizes Lemma 1 and follows similarly as the proofs in previous studies [46  26]
based on v-central condition. Our analysis essentially shows that relaxed Bernstein condition and
relaxed v-central condition with non-universal w∗ sufﬁce to establish the intermediate rates. Although
the rate in Theorem 1 does not improve that in previous works [46]  the relaxation brought by EBC
allows us to establish fast rates for interesting problems that were unknown before. More details are
postponed into Section 5. For example  under the condition that the input data x  y are bounded  ERM
for hinge loss minimization with (cid:96)1  (cid:96)∞ norm constraints  and for minimizing a quadratic function

and an (cid:96)1 norm regularization enjoys an (cid:101)O(1/n) fast rate. To the best of our knowledge  such a fast

rate of ERM for these problems has not been shown in literature using other conditions or theories.

3.2 ERM for non-negative  Lipschitz continuous and smooth convex random functions

Below we will present improved optimistic rates of ERM for non-negative smooth loss functions
expanding the results in [55]. To be general  we consider (2) and the following ERM problem:

(cid:98)w ∈ arg min

w∈W Pn(w) (cid:44) 1

n

n(cid:88)

i=1

f (w  zi) + r(w)

(8)

Besides Assumptions 1  2  we further make the following assumption for developing faster rates.
Assumption 3. For the stochastic optimization problem (1)  we assume f (w  z) is a non-negative
and L-smooth convex function w.r.t w for any z ∈ Z.

It is notable that we do not assume that r(w) is smooth. Our main result in this subsection is presented
in the following theorem.
Theorem 2 (Result II). Under Assumptions 1  2  and 3  with probability at least 1 − δ we have

d log n + log(1/δ)

(cid:32)
P ((cid:98)w) − P∗ ≤ O
(cid:16)(cid:0)α1/θd log n(cid:1)2−θ(cid:17)
(cid:32)(cid:20) d log n + log(1/δ)
P ((cid:98)w) − P∗ ≤ O

n

n

+

(cid:21) 2

2−θ

2−θ(cid:33)
(cid:20) (d log n + log(1/δ))P∗
(cid:21) 1
2−θ(cid:33)
(cid:21) 1
(cid:20) (d log n + log(1/δ))P∗

n

.

+

  with probability at least 1 − δ 

n

.

When n ≥ Ω

Remark: The constant in big O and Ω can be seen from the proof  which is tedious and included
in the supplement. Here we focus on the understanding of the results. First  the above results are
optimistic rates that are no worse than that in Theorem 1. Second  the ﬁrst result implies that when the
optimal risk P∗ is less than O((d log n/n)1−θ)  the excess risk bound is in the order of O(d log n/n).
Third  when the number of samples n is sufﬁciently large and the optimal risk is sufﬁciently small 
the second result can imply a faster rate than O(d log n/n). Considering smooth functions presented
in Section 5 with θ = 1  when n ≥ Ω(αd log n) and P∗ ≤ O(d log n/n) (large-sample and small
optimal risk)  the excess risk can be bounded by O((d log n/n)2). In another word  the sample
). To the best of our knowledge 
 for these examples is the ﬁrst result appearing in

complexity for achieving an -excess risk bound is given by (cid:101)O(d/

√

√

the sample complexity of ERM in the order of 1/
the literature.

4 Efﬁcient SA for Lipschitz continuous random functions

In this section  we will present intermediate rates of an efﬁcient stochastic approximation algorithm
for solving (1) adaptive to the EBC under the Assumption 1 and 2. Note that (2) can be considered as
a special case by absorbing r(w) into f (w  z).

5

Algorithm 1 SSG(w1  γ  T W)
Input: w1 ∈ W  γ > 0 and T
1: for t = 1  . . .   T do
2: wt+1 = ΠW (wt − γgt)
3: end for

(cid:80)T +1

Algorithm 2 ASA(w1  n  R)

1: Set R0 = 2R (cid:98)w0 = w1  m = (cid:98) 1

2 log2

2n

log2 n(cid:99)−1  n0 = (cid:98) n
m(cid:99)

G

T +1

t=1 wt

Set γk = Rk−1

√

n0+1 and Rk = Rk−1/2

6: return (cid:98)wm

2: for k = 1  . . .   m do
3:
4:
5: end for

(cid:98)wk = SSG((cid:98)wk−1  γk  n0 W ∩ B((cid:98)wk−1  Rk−1))

4: (cid:98)wT = 1
5: return (cid:98)wT
Denote by z1  . . . zk  . . . i.i.d samples drawn sequentially from the distribution P  by gk ∈
∂f (w  zk)|w=wk a stochastic subgradient evaluated at wk with sample zk  and by B(w  R)
a bounded ball centered at w with a radius R. By the Lipschitz continuity of f  we have
(cid:107)∂f (w  z)(cid:107)2 ≤ G for ∀w ∈ W ∀z ∈ Z.
The proposed adaptive stochastic approximation algorithm is presented in Algorithm 2  which
is referred to as ASA. The updates are divided into m stages  where at each stage a stochastic
subgradient method (Algorithm 1) is employed for running n0 = (cid:98)n/m(cid:99) iterations with a constant
step size γk. The step size γk will be decreased by half after each stage and the next stage will be
warm-started using the solution returned from the last stage as the initial solution. The projection onto
the intersection of W and a shrinking bounded ball at each stage is a commonly used trick for the
high probability analysis [14  15  49]. We emphasize that the subroutine in ASA can be replaced by
other SA algorithms  e.g.  the proximal variant of stochastic subgradient for handling a non-smooth
deterministic component such as (cid:96)1 norm regularization [7]  stochastic mirror descent with with a
p-norm divergence function [8]  and etc. Please see an example in the supplement.
It is worth mentioning that the dividing schema of ASA is due to [15]  which however restricts its
analysis to uniformly convex functions where uniform convexity is a stronger condition than the
EBC. ASA is also similar to a recently proposed accelerated stochastic subgradient (ASSG) method
under the EBC [49]. However  the key differences are that (i) ASA is developed for a ﬁxed number of
iterations while ASSG is developed for a ﬁxed accuracy level ; (ii) the adaptive iteration complexity
of ASSG requires knowing the value of θ ∈ (0  2] while ASA does not require the value of θ. As a
trade-off  we restrict our attention to θ ∈ (0  1].
Theorem 3 (Result III). Suppose Assumptions 1 and 2 hold  and (cid:107)w1 − w∗(cid:107)2 ≤ R0  where w∗
is the closest optimal solution to w1. Deﬁne ¯α = max(αG2  (R0G)2−θ). For n ≥ 100 and any
δ ∈ (0  1)  with probability at least 1 − δ  we have

P ((cid:98)wm) − P∗ ≤ O

(cid:18) ¯α(log(n) log(log(n)/δ))

(cid:19) 1

2−θ

.

n

Remark: The signiﬁcance of the result is that although Algorithm 2 does not utilize any knowledge
about EBC  it is automatically adaptive to the EBC. As a ﬁnal note  the projection onto the intersection
of W and a bounded ball can be efﬁciently computed by employing the projection onto W and a
binary search for the Lagrangian multiplier of the ball constraint. Moreover  we can replace the
subroutine with a slightly different variant of SSG to get around of the projection onto the intersection
of W and a bounded ball  which is presented in the supplement.

5 Applications

From the last two sections  we can see that θ = 1 is a favorable case  which yields the fastest rate
in our results. It is obvious that if f (w  z) is strongly convex or P (w) is strongly convex  then
EBC(θ = 1  α) holds. Below we show some examples of problem (1) and (2) with θ = 1 without

strong convexity  which not only recover some known results of fast rate (cid:101)O(d/n)  but also induce
new results of fast rates that are even faster than (cid:101)O(d/n).

Quadratic Problems (QP):
(9)
where c is a constant. The random function can be taken as f (w  z  z(cid:48)) = w(cid:62)A(z)w + w(cid:62)b(z(cid:48)) + c.
We have the following corollary.

w∈W P (w) (cid:44) w(cid:62)Ez[A(z)]w + w(cid:62)Ez(cid:48)[b(z(cid:48))] + c

min

6

Corollary 1. If Ez[A(z)] is a positive semi-deﬁnite matrix (not necessarily positive deﬁnite)
and W is a bounded polyhedron  then the problem (9) satisﬁes EBC(θ = 1  α). Assume that

max((cid:107)A(z)(cid:107)2 (cid:107)b(z(cid:48))(cid:107)2) ≤ σ < ∞  then ERM has a fast rate at least (cid:101)O(d/n). If f (w  z  z(cid:48)) is
further non-negative  convex and smooth  then ERM has a fast rate of (cid:101)O((d/n)2 + dP∗/n) when
n ≥ Ω(d log n). ASA has a convergence rate of (cid:101)O(1/n).

min

Next  we present some instances of the quadratic problem (9).
Instance 1 of QP: minimizing the expected square loss. Consider the following problem:

w∈W P (w) (cid:44) Ex y[(w(cid:62)x − y)2]

only recover some known results of (cid:101)O(d/n) rate [22  26]  but also imply a faster rate than (cid:101)O(d/n) in

(10)
where x ∈ X   y ∈ Y and W is a bounded polyhedron (e.g.  (cid:96)1-ball or (cid:96)∞-ball). It is not difﬁcult
to show that it is an instance of (9) and has the property that f (w  z  z(cid:48)) is non-negative  smooth 
convex  Lipchitz continuous over W. The convergence results in Corollary 1 for this instance not
a large-sample regime and an optimistic case when n ≥ Ω(d log n)  P∗ ≤ O(d log n/n)  where the
latter result is the ﬁrst such result of its own.
Instance 2 of QP. Let us consider the following problem:

w∈W P (w) (cid:44) Ez[w(cid:62)(S − zz(cid:62))w] − w(cid:62)b

(11)
where S − Ez[zz(cid:62)] (cid:23) 0. It is notable that f (w  z) = w(cid:62)(S − zz(cid:62))w − w(cid:62)b might be non-convex.
A similar problem as (11) could arise in computing the leading eigen-vector of E[zz(cid:62)] by performing
shifted-and-inverted power method over random samples z ∼ P [10].

min

min

w∈W P (w) (cid:44) E[f (w  z)]

Piecewise Linear Problems (PLP):
(12)
where E[f (w  z)] is a piecewise linear convex function and W is a bounded polyhedron. We have
the following corollary.
Corollary 2. If E[f (w  z)] is piecewise linear and convex and W is a bounded polyhedron  then the
problem (12) satisﬁes EBC(θ = 1  α). If f (w  z) is Lipschitz continuous  then ERM has a fast rate at

least (cid:101)O(d/n)  and ASA has a convergence rate of (cid:101)O(1/n). If f (w  z) is further non-negative and
linear  then ERM has a fast rate of (cid:101)O((d/n)2 + dP∗/n) when n ≥ Ω(d log n).

Instance 1 of PLP: minimizing the expected hinge loss for bounded data. Consider the following
problem:

P (w) (cid:44) Ex y[(1 − yw(cid:62)x)+]

min

(cid:107)w(cid:107)p≤B

(13)
where p = 1 ∞ and y ∈ {1 −1}. Suppose that x ∈ X is bounded and scaled such that |w(cid:62)x| ≤ 1.
Koolen et al. [20] has considered this instance with p = 2 and proved that the Bernstein condition
(Deﬁnition 2) holds with β = 1 for the problem (13) when E[yx] (cid:54)= 0 and |w(cid:62)x| ≤ 1. In contrast 
we can show that the problem (13) with any p = 1  2 ∞ norm constraint 3  the EBC(θ = 1  α) holds
since the objective P (w) = 1 − w(cid:62)E[yx] is essentially a linear function of w. Then all results
in Corollary 2 hold. To the best of our knowledge  the fast rates of ERM and SA for this instance
with (cid:96)1 and (cid:96)∞ norm constraint are the new results. In comparison  Koolen et al.’s [20] fast rate of

(cid:101)O(1/n) only applies to SA and (cid:96)2 norm constraint  and their SA algorithm is not as efﬁcient as our

SA algorithm.
Instance 2 of PLP: multi-dimensional newsvendor problem. Consider a ﬁrm that manufactures p
products from q resources. Suppose that a manager must decide on a resource vector x ∈ Rq
+ before
the product demand vector z ∈ Rp is observed. After the demand becomes known  the manager
chooses a production vector y ∈ Rp so as to maximize the operating proﬁt. Assuming that the
demand z is a random vector with discrete probability distribution  the problem is equivalent to

c(cid:62)x − E[Π(x; z)]

x∈Rq

min
+ x≤b

where both Π(x; z) and E[Π(x; z)] are piecewise linear concave functions [18]. Then the problem
ﬁts to the setting in Corollary 2.

3The case of p = 2 is showed later.

7

(a) rcv1_binary

(b) real-sim

(c) E2006-tﬁdf

(d) E2006-log1p

Figure 1: Testing Error vs Iteration of ASA and other baselines for SA

Risk Minimization Problems over an (cid:96)2 ball. Consider the following problem

P (w) (cid:44) Ez[f (w  z)]

min

(cid:107)w(cid:107)2≤B

(14)

Assuming that P (w) is convex and minw∈Rd P (w) < min(cid:107)w(cid:107)2≤B P (w)  we can show that
EBC(θ = 1  α) holds (see supplement). Using this result  we can easily show that the consid-
ered problem (13) with p = 2 satisﬁes EBC(θ = 1  α).

Risk Minimization with (cid:96)1 Regularization Problems. For (cid:96)1 regularized risk minimization:

P (w) (cid:44) E[f (w; z)] + λ(cid:107)w(cid:107)1 

min

(cid:107)w(cid:107)1≤B

(15)

we have the following corollary.
Corollary 3. If the ﬁrst component is quadratic as in (9) or is piecewise linear and convex  then
the problem (15) satisﬁes EBC(θ = 1  α). If the random function is Lipschitz continuous  then

ERM has a fast rate at least (cid:101)O(d/n)  and ASA has a convergence rate of (cid:101)O(1/n). If f (w  z) is
further non-negative  convex and smooth  then ERM has a fast rate of (cid:101)O((d/n)2 + dP∗/n) when

n ≥ Ω(d log n).

To the best of our knowledge  this above general result is the ﬁrst of its kind. Next  we show some
instances satisfying EBC(θ  α) with θ < 1. Consider the problem minw∈W F (w) (cid:44) P (w)+λ(cid:107)w(cid:107)p
p 
where P (w) is quadratic as in (9)  and W is a bounded polyhedron. In the supplement  we prove that
EBC(θ = 2/p  α) holds.
A Case Study for ASA. Finally  we provide some empirical evidence to support the effectiveness
of the proposed ASA algorithm. In particular  we will consider solving an (cid:96)1 regularized expected
√
square loss minimization problem (15) for learning a predictive model. We compare with two
baselines whose convergence rate are known as O(1/
n)  namely proximal stochastic gradient
(PSG) method [7]  and stochastic mirror descent (SMD) method using a p-norm divergence function
(p = 2 log d) other than the Euclidean function. For SMD  we implement the algorithm proposed
in [37]  which was proposed for solving (15) and could be effective for very high-dimensional data.
For ASA  we implement two versions that use PSG and SMD as the subroutine and report the one
that gives the best performance. The two versions differ in using the Euclidean norm or the p-norm
for measuring distance. Since the comparison is focused on the testing error  we also include another
strong baseline  i.e  averaged stochastic gradient (ASGD) with a constant step size  which enjoys an
O(d/n) rate for minimizing the expected square loss without any constraints or regularizations [1].
We use four benchmark datasets from libsvm website4  namely  real-sim  rcv1_binary  E2006-tﬁdf 
E2006-log1p  whose dimensionality is 20958  47236  150360  4272227  respectively. We divide each
dataset into three sets  respectively training  validation  and testing. For E2006-tﬁdf and E2006-log1p
dataset  we randomly split the given testing set into half validation and half testing. For the dataset
real-sim which do not explicitly provides a testing set  we randomly split the entire data into 4:1:1
for training  validation  and testing. For rcv1_binary  despite that the test set is given  the size of the
training set is relatively small. Thus we ﬁrst combine the training and the testing sets and then follow
the above procedure to split it.
The involved parameters of each algorithm are tuned based on the validation data. With the selected
parameters  we run each algorithm by passing through training examples once and evaluate interme-
diate models on the testing data to compute the testing error measured by square loss. The results

4http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/

8

11.522.533.544.5iteration×1050.140.160.180.20.220.240.260.280.30.320.34testing errorASAASGDPSGSMD12345iteration×1040.150.20.250.30.350.40.450.50.550.6testing errorASAASGDPSGSMD0.40.60.811.21.41.6iteration×1040.150.160.170.180.190.20.210.220.23testing errorASAASGDPSGSMD0.40.60.811.21.41.6iteration×1040.10.150.20.250.30.350.40.45testing errorASAASGDPSGSMDon different data sets averaged over 5 random runs over shufﬂed training examples are shown in
Figure 1. From the testing curves  we can see that the proposed ASA has similar convergence rate
to ASGD on two relatively low-dimensional data sets. This is not surprise since both algorithms

enjoy an (cid:101)O(1/n) convergence rate indicated by their theories. For the data set E2006-tﬁdf and

E2006-log1p  we observe that ASA converges much faster than ASGD  which is due to the presence
of (cid:96)1 regularization. In addition  ASA converges much faster than SGD and SMD with one exception
on E2006-log1p  on which ASA performs slightly better than SMD.

6 Conclusion

We have comprehensively studied statistical learning under the error bound condition for both ERM
and SA. We established the connection between the error bound condition and previous conditions
for developing fast rates of empirical risk minimization for Lipschitz continuous loss functions. We
also developed improved rates for non-negative and smooth convex loss functions  which induce
faster rates that were not achieved before. Finally  we analyzed an efﬁcient “parameter"-free SA
algorithm under the error bound condition and showed that it is automatically adaptive to the error
bound condition. Applications in machine learning and other ﬁelds are considered and empirical
studies corroborate the fast rate of the developed algorithms. An open question is how to develop
efﬁcient SA algorithms under the error bound condition with optimistic rates for non-negative smooth
loss functions similar to the results obtained for empirical risk minimization in this paper.

Acknowledgement

The authors thank the anonymous reviewers for their helpful comments. M. Liu and T. Yang are
partially supported by National Science Foundation (IIS-1545995). L. Zhang is partially supported
by YESS (2017QNRC001). We thank Nishant A. Mehta for pointing out the work [12] for the proof
of Theorem 1.

References
[1] Francis R. Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation
with convergence rate O(1/n). In Advances in Neural Information Processing Systems (NIPS) 
pages 773–781  2013.

[2] Peter L. Bartlett  Olivier Bousquet  and Shahar Mendelson. Local rademacher complexities.

The Annals of Statistics  2005.

[3] Peter L. Bartlett and Shahar Mendelson. Empirical minimization. Probability Theory and

Related Fields  2006.

[4] Jerome Bolte  Trong Phong Nguyen  Juan Peypouquet  and Bruce Suter. From error bounds to
the complexity of ﬁrst-order descent methods for convex functions. CoRR  abs/1510.08234 
2015.

[5] James V. Burke and Michael C. Ferris. Weak sharp minima in mathematical programming.

SIAM Journal on Control and Optimization  31(5):1340–1359  1993.

[6] Dmitriy Drusvyatskiy and Adrian S. Lewis. Error bounds  quadratic growth  and linear conver-

gence of proximal methods. arXiv:1602.06661  2016.

[7] John Duchi and Yoram Singer. Efﬁcient online and batch learning using forward backward

splitting. Journal of Machine Learning Research  10:2899–2934  2009.

[8] John C. Duchi  Shai Shalev-Shwartz  Yoram Singer  and Ambuj Tewari. Composite objective

mirror descent. In COLT  pages 14–26. Omnipress  2010.

[9] Vitaly Feldman. Generalization of erm in stochastic convex optimization: The dimension strikes

back. In NIPS. 2016.

9

[10] Dan Garber  Elad Hazan  Chi Jin  Sham M. Kakade  Cameron Musco  Praneeth Netrapalli  and
Aaron Sidford. Faster eigenvector computation via shift-and-invert preconditioning. In ICML 
2016.

[11] Alon Gonen and Shai Shalev-Shwartz. Average stability is invariant to data preconditioning:
Implications to exp-concave empirical risk minimization. J. Mach. Learn. Res.  18(1):8245–
8257  January 2017.

[12] Peter D. Grünwald and Nishant A. Mehta. Fast rates with unbounded losses. CoRR 

abs/1605.00252  2016.

[13] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning  2007.

[14] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: an optimal algorithm for

stochastic strongly-convex optimization. In COLT  2011.

[15] Anatoli Juditsky and Yuri Nesterov. Deterministic and stochastic primal-dual subgradient

algorithms for uniformly convex minimization. Stoch. Syst.  2014.

[16] Sham M. Kakade and Ambuj Tewari. On the generalization ability of online strongly convex

programming algorithms. In NIPS  2008.

[17] Hamed Karimi  Julie Nutini  and Mark W. Schmidt. Linear convergence of gradient and

proximal-gradient methods under the polyak-łojasiewicz condition. In ECML-PKDD  2016.

[18] Sujin Kim  Raghu Pasupathy  and Shane G. Henderson. A Guide to Sample Average Approxima-

tion  pages 207–243. Springer New York  New York  NY  2015.

[19] Vladimir Koltchinskii. Local rademacher complexities and oracle inequalities in risk minimiza-

tion. The Annals of Statistics  2006.

[20] Wouter M. Koolen  Peter Grünwald  and Tim van Erven. Combining adversarial guarantees and

stochastic fast rates in online learning. In NIPS  2016.

[21] Tomer Koren and Kﬁr Y. Levy. Fast rates for exp-concave empirical risk minimization. In NIPS 

2015.

[22] Wee Sun Lee  P. L. Bartlett  and R. C. Williamson. The importance of convexity in learning

with squared loss. IEEE Transactions on Information Theory  44(5):1974–1980  1998.

[23] Guoyin Li. Global error bounds for piecewise convex polynomials. Math. Program.  2013.

[24] Guoyin Li and Ting Kei Pong. Calculus of the exponent of kurdyka- łojasiewicz inequality and

its applications to linear convergence of ﬁrst-order methods. CoRR  abs/1602.02915  2016.

[25] Enno Mammen and Alexandre B. Tsybakov. Smooth discrimination analysis. Ann. Statist. 

27(6):1808–1829  12 1999.

[26] Nishant A. Mehta. Fast rates with high probability in exp-concave statistical learning. In

AISTATS  pages –  2017.

[27] Nishant A. Mehta and Robert C. Williamson. From stochastic mixability to fast rates. In NIPS 

2014.

[28] I. Necoara  Yu. Nesterov  and F. Glineur. Linear convergence of ﬁrst order methods for non-

strongly convex optimization. CoRR  abs/1504.06298  v4  2015.

[29] Arkadi Nemirovski  Anatoli Juditsky  Lan  and Alexander Shapiro. Robust stochastic approxi-

mation approach to stochastic programming. SIAM Journal on Optimization  2009.

[30] Yurii Nesterov. Introductory lectures on convex optimization: a basic course. 2004.

[31] Jong-Shi Pang. Error bounds in mathematical programming. Math. Program.  1997.

10

[32] Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Making gradient descent optimal for

strongly convex stochastic optimization. In ICML  2012.

[33] Aaditya Ramdas and Aarti Singh. Optimal rates for stochastic convex optimization under

tsybakov noise condition. In ICML  2013.

[34] R.T. Rockafellar. Convex Analysis. 1970.

[35] Shai Shalev-Shwartz  Ohad Shamir  Nathan Srebro  and Karthik Sridharan. Stochastic convex

optimization. In COLT  2009.

[36] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal estimated sub-gradient

solver for svm. In ICML  2007.

[37] Shai Shalev-Shwartz and Ambuj Tewari. Stochastic methods for l1-regularized loss minimiza-

tion. Journal of Machine Learning Research  12:1865–1892  2011.

[38] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization:

Convergence results and optimal averaging schemes. In ICML  2013.

[39] Alexander Shapiro  Darinka Dentcheva  and Andrzej Ruszczynski. Lectures on Stochastic
Programming: Modeling and Theory  Second Edition. Society for Industrial and Applied
Mathematics  Philadelphia  PA  USA  2014.

[40] Steve Smale and Ding-Xuan Zhou. Learning theory estimates via integral operators and their

approximations. Constructive Approximation  2007.

[41] Nathan Srebro  Karthik Sridharan  and Ambuj Tewari. Optimistic rates for learning with a

smooth loss. ArXiv e-prints  arXiv:1009.3896  2010.

[42] Nathan Srebro  Karthik Sridharan  and Ambuj Tewari. Smoothness  low noise and fast rates. In

NIPS  2010.

[43] Karthik Sridharan  Shai Shalev-Shwartz  and Nathan Srebro. Fast rates for regularized objectives.

In NIPS  2008.

[44] Alexander B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Ann. Statist. 

32(1):135–166  02 2004.

[45] Alexander B Tsybakov et al. Optimal aggregation of classiﬁers in statistical learning. The

Annals of Statistics  32(1):135–166  2004.

[46] Tim van Erven  Peter D. Grünwald  Nishant A. Mehta  Mark D. Reid  and Robert C. Williamson.

Fast rates in statistical and online learning. JMLR  2015.

[47] Tim van Erven and Wouter M. Koolen. Metagrad: Multiple learning rates in online learning. In

NIPS  2016.

[48] Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.

[49] Yi Xu  Qihang Lin  and Tianbao Yang. Accelerate stochastic subgradient method by leveraging

local error bound. CoRR  abs/1607.01027  2016.

[50] Yi Xu  Qihang Lin  and Tianbao Yang. Stochastic convex optimization: Faster local growth

implies faster global convergence. In ICML  pages 3821–3830  2017.

[51] Tianbao Yang  Zhe Li  and Lijun Zhang. A simple analysis for exp-concave empirical mini-

mization with arbitrary convex regularizer. In AISTATS  pages 445–453  2018.

[52] Tianbao Yang and Qihang Lin. Rsg: Beating subgradient method without smoothness and

strong convexity. CoRR  abs/1512.03107  2016.

[53] W. H. Yang. Error bounds for convex polynomials. SIAM Journal on Optimization  2009.

[54] Hui Zhang. New analysis of linear convergence of gradient-type methods via unifying error

bound conditions. CoRR  abs/1606.00269  2016.

11

[55] Lijun Zhang  Tianbao Yang  and Rong Jin. Empirical risk minimization for stochastic convex

optimization: O(1/n)- and o(1/n2)-type of risk bounds. CoRR  abs/1702.02030  2017.

[56] Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent.

In ICML  pages 928–936  2003.

12

,Chang Liu
Xinyun Chen
Eui Chul Shin
Mingcheng Chen
Dawn Song
Mingrui Liu
Xiaoxuan Zhang
Lijun Zhang
Rong Jin
Tianbao Yang