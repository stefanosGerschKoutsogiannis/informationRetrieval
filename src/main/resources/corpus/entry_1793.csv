2013,Eluder Dimension and the Sample Complexity of Optimistic Exploration,This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper confidence bound (UCB) algorithms  but recent work has shown that a simple posterior sampling algorithm  sometimes called Thompson sampling  also shares a close theoretical connection with optimistic approaches. In this paper  we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension  which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for specific model classes  our general bound matches the best available for linear models and is stronger than the best available for generalized linear models.,Eluder Dimension and the Sample Complexity of

Optimistic Exploration

Daniel Russo

Stanford University
Stanford  CA 94305

djrusso@stanford.edu

Benjamin Van Roy
Stanford University
Stanford  CA 94305

bvr@stanford.edu

Abstract

This paper considers the sample complexity of the multi-armed bandit with depen-
dencies among the arms. Some of the most successful algorithms for this problem
use the principle of optimism in the face of uncertainty to guide exploration. The
clearest example of this is the class of upper conﬁdence bound (UCB) algorithms 
but recent work has shown that a simple posterior sampling algorithm  sometimes
called Thompson sampling  can be analyzed in the same manner as optimistic ap-
proaches. In this paper  we develop a regret bound that holds for both classes of
algorithms. This bound applies broadly and can be specialized to many model
classes. It depends on a new notion we refer to as the eluder dimension  which
measures the degree of dependence among action rewards. Compared to UCB
algorithm regret bounds for speciﬁc model classes  our general bound matches the
best available for linear models and is stronger than the best available for general-
ized linear models.

1

Introduction

Consider a politician trying to elude a group of reporters. She hopes to keep her true position hidden
from the reporters  but each piece of information she provides must be new  in the sense that it’s not
a clear consequence of what she has already told them. How long can she continue before her true
position is pinned down? This is the essence of what we call the eluder dimension. We show this
notion controls the rate at which algorithms using optimistic exploration converge to optimality.
We consider an optimization problem faced by an agent who is uncertain about how her actions
inﬂuence performance. The agent selects actions sequentially  and upon each action observes a
reward. A reward function governs the mean reward of each action. As rewards are observed the
agent learns about the reward function  and this allows her to improve behavior. Good performance
requires adaptively sampling actions in a way that strikes an effective balance between exploring
poorly understood actions and exploiting previously acquired knowledge to attain high rewards.
Unless the agent has prior knowledge of the structure of the mean payoff function  she can only learn
to attain near optimal performance by exhaustively sampling each possible action. In this paper  we
focus on problems where there is a known relationship among the rewards generated by different
actions  potentially allowing the agent to learn without exploring every action. Problems of this form
are often referred to as multi-armed bandit (MAB) problems with dependent arms.
A notable example is the “linear bandit” problem  where actions are described by a ﬁnite number
of features and the reward function is linear in these features. Several researchers have studied
algorithms for such problems and established theoretical guarantees that have no dependence on the
number of actions [1  2  3]. Instead  their bounds depend on the linear dimension of the class of
reward functions. In this paper  we assume that the reward function lies in a known but otherwise
arbitrary class of uniformly bounded real-valued functions  and provide theoretical guarantees that

1

depend on more general measures of the complexity of the class of functions. Our analysis of
this abstract framework yields a result that applies broadly  beyond the scope of speciﬁc problems
that have been studied in the literature  and also identiﬁes fundamental insights that unify more
specialized prior results.
The guarantees we provide apply to two popular classes of algorithms for the stochastic MAB:
upper conﬁdence bound (UCB) algorithms and Thompson sampling. Each algorithm is described
in Section 3. The aforementioned papers on the linear bandit problem study UCB algorithms [1 
2  3]. Other authors have studied UCB algorithms in cases where the reward function is Lipschitz
continuous [4  5]  sampled from a Gaussian process [6]  or takes the form of a generalized [7] or
sparse [8] linear model. More generally  there is an immense literature on this approach to balancing
between exploration and exploitation  including work on bandits with independent arms [9  10  11 
12]  reinforcement learning [13  14]  and Monte Carlo Tree Search [15].
Recently  a simple posterior sampling algorithm called Thompson sampling was shown to share a
close connection with UCB algorithms [16]. This connection enables us to study both types of
algorithms in a uniﬁed manner. Though it was ﬁrst proposed in 1933 [17]  Thompson sampling
has until recently received relatively little attention. Interest in the algorithm grew after empirical
studies [18  19] demonstrated performance exceeding state-of the-art methods. Strong theoretical
guarantees are now available for an important class of problems with independent arms [20  21  22].
A recent paper considers the application of this algorithm to a linear contextual bandit problem [23].
To our knowledge  few other papers have studied MAB problems in a general framework like the
one we consider. There is work that provides general bounds for contextual bandit problems where
the context space is allowed to be inﬁnite  but the action space is small (see e.g.  [24]). Our model
captures contextual bandits as a special case  but we emphasize problem instances with large or
inﬁnite action sets  and where the goal is to learn without sampling every possible action. The closest
related work to ours is that of Amin et al. [25]  who consider the problem of learning the optimum
of a function that lies in a known  but otherwise arbitrary set of functions. They provide bounds
based on a new notion of dimension  but unfortunately this notion does not provide a guarantee for
the algorithms we consider.
We provide bounds on expected regret over a time horizon T that are  up to a logarithmic factor  of
order

Eluder dimension

log–covering number

This quantity depends on the class of reward functions F through two measures of complexity. Each
captures the approximate structure of the class of functions at a scale T −2 that depends on the time
horizon. The ﬁrst measures the growth rate of the covering numbers of F  and is closely related to
measures of complexity that are common in the supervised learning literature. This quantity roughly
captures the sensitivity of F to statistical over-ﬁtting. The second measure  the eluder dimension 
is a new notion we introduce. This captures how effectively the value of unobserved actions can be
inferred from observed samples. We highlight in Section 4.1 why notions of dimension common to
the supervised learning literature are insufﬁcient for our purposes. Finally  we show that our more
general result when specialized to linear models recovers the strongest known regret bound and in
the case of generalized linear models yields a bound stronger than that established in prior literature.

2 Problem Formulation
We consider a model involving a set of actions A and a set of real-valued functions F =
{fρ : A (cid:55)→ R| ρ ∈ Θ}  indexed by a parameter that takes values from an index set Θ. We will
deﬁne random variables with respect to a probability space (Ω  F  P). A random variable θ indexes
the true reward function fθ. At each time t  the agent is presented with a possibly random subset
At ⊆ A and selects an action At ∈ At  after which she observes a reward Rt.
We denote by Ht the history (A1  A1  R1  . . .  At−1  At−1  Rt−1 At) of observations available to
the agent when choosing an action At. The agent employs a policy π = {πt|t ∈ N}  which is a
deterministic sequence of functions  each mapping the history Ht to a probability distribution over
actions A. For each realization of Ht  πt(Ht) is a distribution over A with support At. The action At

2

(cid:118)(cid:117)(cid:117)(cid:116)dimE
(cid:124)

(cid:0)F  T −2(cid:1)
(cid:123)(cid:122)
(cid:125)

log(cid:0)N(cid:0)F  T −2 (cid:107)·(cid:107)∞
(cid:124)

(cid:123)(cid:122)

(cid:1)(cid:1)
(cid:125)

T .

is selected by sampling from the distribution πt(·)  so that P(At ∈ ·|Ht) = πt(Ht). We assume that
E[Rt|Ht  θ  At] = fθ(At). In other words  the realized reward is the mean-reward value corrupted
by zero-mean noise. We will also assume that for each f ∈ F and t ∈ N  arg maxa∈At f (a) is
nonempty with probability one  though algorithms and results can be generalized to handle cases
where this assumption does not hold. We ﬁx constants C > 0 and η > 0 and impose two further
simplifying assumptions. The ﬁrst concerns boundedness of reward functions.
Assumption 1. For all f ∈ F and a ∈ A  f (a) ∈ [0  C].

Our second assumption ensures that observation noise is light-tailed. We say a random variable X
is η-sub-Gaussian if E[exp(λX)] ≤ exp(λ2η2/2) almost surely for all λ.
Assumption 2. For all t ∈ N  Rt − fθ(At) conditioned on (Ht  θ  At) is η-sub-Gaussian.
We let A∗
random variable

t ∈ arg maxa∈Atfθ(a) denote the optimal action at time t. The T period regret is the

T(cid:88)

R(T  π) =

[fθ(A∗

t ) − fθ (At)]  

t=1

where the actions {At : t ∈ N} are selected according to π. We sometimes study expected regret
E[R(T  π)]  where the expectation is taken over the prior distribution of θ  the reward noise  and
the algorithm’s internal randomization. This quantity is sometimes called Bayes risk or Bayesian
regret. Similarly  we study conditional expected regret E [R(T  π) | θ]  which integrates over all
randomness in the system except for θ.
Example 1. Contextual Models. The contextual multi-armed bandit model is a special case of
the formulation presented above. In such a model  an exogenous Markov process Xt taking values
in a set X inﬂuences rewards. In particular  the expected reward at time t is given by fθ(a  Xt).
However  this is mathematically equivalent to a problem with stochastic time-varying decision
sets At. In particular  one can deﬁne the set of actions to be the set of state-action pairs A :=
{(x  a) : x ∈ A  a ∈ A(x)}  and the set of available actions to be At = {(Xt  a) : a ∈ A(Xt)}.

3 Algorithms

We will establish performance bounds for two classes of algorithms: Thompson sampling and UCB
algorithms. As background  we discuss the algorithms in this section. We also provide an example
of each type of algorithm that is designed to address the “linear bandit” problem.
UCB Algorithms: UCB algorithms have received a great deal of attention in the MAB literature.
Here we describe a very broad class of UCB algorithms. We say that a conﬁdence set is a random
subset Ft ⊂ F that is measurable with respect to σ(Ht). Typically  Ft is constructed so that
it contains fθ with high probability. We denote by πF1:∞ a UCB algorithm that makes use of a
sequence of conﬁdence sets {Ft : t ∈ N}. At each time t  such an algorithm selects the action

At ∈ arg max
a∈At

sup
f∈Ft

f (a) 

where sup
f∈Ft

f (a) is an optimistic estimate of fθ(a) representing the greatest value that is statistically
plausible at time t. Optimism encourages selection of poorly-understood actions  which leads to
informative observations. As data accumulates  optimistic estimates are adapted  and this process of
exploration and learning converges toward optimal behavior.
In this paper  we will assume for simplicity that the maximum deﬁning At is attained. Results can be
generalized to handle cases when this technical condition does not hold. Unfortunately  for natural
choices of Ft  it may be exceptionally difﬁcult to solve for such an action. Thankfully  all results in
this paper also apply to a posterior sampling algorithm that avoids this hard optimization problem.
Thompson sampling: The Thompson sampling algorithm simply samples each action according
to the probability it is optimal. In particular  the algorithm applies action sampling distributions
t ∈ arg maxa∈At fθ(a).
πTS
t
Practical implementations typically operate by at each time t sampling an index ˆθt ∈ Θ from the
distribution P (θ ∈ · | Ht) and then generating an action At ∈ arg maxa∈At fˆθt

t is a random variable that satisﬁes A∗

(Ht) = P (A∗

t ∈ · | Ht)  where A∗

(a).

3

Algorithm 1 Linear UCB
1: Initialize: Select d linearly independent ac-

2: Update Statistics:

tions
ˆθt ← OLS estimate of θ

Φt ←(cid:80)t−1
(cid:26)

k=1 φ( ¯Ak)φ( ¯Ak)T
≤ β
ρ :

(cid:13)(cid:13)(cid:13)ρ − ˆθt

Θt ←

(cid:13)(cid:13)(cid:13)Φt

(cid:27)

√

d log t

3: Select Action:

At ∈ arg maxa∈A {maxρ∈Θt (cid:104)φ(a)  ρ(cid:105)}

4: Increment t and Goto Step 2

Algorithm 2
Linear Thompson sampling
1: Sample Model:
ˆθt ∼ N (µt  Σt)
2: Select Action:
At ∈ arg maxa∈A(cid:104)φ(a)  ˆθt(cid:105)
3: Update Statistics:
µt+1 ← E[θ|Ht+1]
Σt+1 ← E[(θ − µt+1)(θ − µt+1)(cid:62)|Ht+1]

4: Increment t and Goto Step 1

Algorithms for Linear Bandits: Here we provide an example of a Thompson sampling and a
UCB algorithm  each of which addresses a problem in which the reward function is linear in a d-
dimensional vector θ. In particular  there is a known feature mapping φ : A → Rd such that an
action a yields expected reward fθ(a) = (cid:104)φ(a)  θ(cid:105). Algorithm 1 is a variation of one proposed by
Rusmevichientong and Tsitsiklis [3] to address such problems. Given past observations  the algo-
rithm constructs a conﬁdence ellipsoid Θt centered around a least squares estimate ˆθt and employs
the upper conﬁdence bound Ut(a) := maxθ∈Θt
.
−1
The term (cid:107)φ(a)(cid:107)Φ
t
captures the amount of previous exploration in the direction φ(a)  and causes
to diminish as the number of observations increases.
the “uncertainty bonus” β

(cid:10)φ(a)  θ(cid:11) =

d log(t)(cid:107)φ(a)(cid:107)Φ

φ(a)  ˆθt

d log(t)(cid:107)φ(a)(cid:107)Φ

(cid:113)

(cid:113)

(cid:68)

(cid:69)

+ β

−1
t

−1
t

Now  consider Algorithm 2. Here we assume θ is drawn from a normal distribution N (µ1  Σ1). We
consider a linear reward function fθ(a) = (cid:104)φ(a)  θ(cid:105) and assume the reward noise Rt − fθ(At) is
normally distributed and independent from (Ht  At  θ). It is easy to show that  conditioned on the
history Ht  θ remains normally distributed. Algorithm 2 presents an implementation of Thompson
sampling for this problem. The expectations can be computed efﬁciently via Kalman ﬁltering.

4 Notions of Dimension

Recently  there has been a great deal of interest in the development of regret bounds for linear UCB
algorithms [1  2  3  26]. These papers show that for a broad class of problems  a variant π∗ of
√
Algorithm 1 satisﬁes the upper bounds E [R(T  π∗)] = ˜O(d
T ).
An interesting feature of these bounds is that they have no dependence on the number actions in A 
and instead depend only on the linear dimension of the set of functions F. Our goal is to provide
bounds that depend on more general measures of the complexity of the class of functions. This
section introduces a new notion  the eluder dimension  on which our bounds will depend. First 
we highlight why common notions from statistical learning theory do not sufﬁce when it comes to
multi–armed bandit problems.

√
T ) and E [R(T  π∗) | θ] = ˜O(d

4.1 Vapnik-Chervonenkis Dimension

a

class

ﬁnite

2. Consider

We begin with an example that illustrates how a class of functions that is learnable in constant time
in a supervised learning context may require an arbitrarily long duration when learning to optimize.
Example
=
{fρ : A (cid:55)→ {0  1} | ρ ∈ {1  . . .   n}} over a ﬁnite action set A = {1  . . .   n}. Let fρ(a) = 1(ρ = a) 
so that each function is an indicator for an action. To keep things simple  assume that Rt = fθ(At) 
so that there is no noise. If θ is uniformly distributed over {1  . . .   n}  it is easy to see that the regret
of any algorithm grows linearly with n. For large n  until θ is discovered  each sampled action is
unlikely to reveal much about θ and learning therefore takes very long.
Consider the closely related supervised learning problem in which at each time an action ˜At is
sampled uniformly from A and the mean–reward value fθ( ˜At) is observed. For large n  the time it

binary-valued

functions

F

of

4

takes to effectively learn to predict fθ( ˜At) given ˜At does not depend on t. In particular  prediction
error converges to 1/n in constant time. Note that predicting 0 at every time already achieves this
low level of error.

In the preceding example  the Vapnik-Chervonenkis (VC) dimension  which characterizes the sam-
ple complexity of supervised learning  is 1. On the other hand  the eluder-dimension  which will
we deﬁne below  is n. To highlight conceptual differences between the eluder dimension and the
VC dimension  we will now deﬁne VC dimension in a way analogous to how will deﬁne eluder
dimension. We begin with a notion of independence.
Deﬁnition 1. An action a is VC-independent of ˜A ⊆ A if for any f  ˜f ∈ F there exists some ¯f ∈ F
which agrees with f on a and with ˜f on ˜A; that is  ¯f (a) = f (a) and ¯f (˜a) = ˜f (˜a) for all ˜a ∈ ˜A.
Otherwise  a is VC-dependent on ˜A.
By this deﬁnition  an action a is said to be VC-dependent on ˜A if knowing the values f ∈ F takes
on ˜A could restrict the set of possible values at a. This notion of independence is intimately related
to the VC dimension of a class of functions. In fact  it can be used to deﬁne VC dimension.
Deﬁnition 2. The VC dimension of a class of binary-valued functions with domain A is the largest
cardinality of a set ˜A ⊆ A such that every a ∈ ˜A is VC-independent of ˜A\{a}.

In the above example  any two actions are VC-dependent because knowing the label fθ(a) of one
action could completely determine the value of the other action. However  this only happens if the
sampled action has label 1. If it has label 0  one cannot infer anything about the value of the other
action. Instead of capturing the fact that one could gain useful information through exploration  we
need a stronger requirement that guarantees one will gain useful information.

4.2 Deﬁning Eluder Dimension

(cid:113)(cid:80)n

Here we deﬁne the eluder dimension of a class of functions  which plays a key role in our results.
Deﬁnition 3. An action a ∈ A is -dependent on actions {a1  ...  an} ⊆ A with respect to F if any
pair of functions f  ˜f ∈ F satisfying
i=1(f (ai) − ˜f (ai))2 ≤  also satisﬁes f (a) − ˜f (a) ≤ .
Further  a is -independent of {a1  ..  an} with respect to F if a is not -dependent on {a1  ..  an}.
Intuitively  an action a is independent of {a1  ...  an} if two functions that make similar predictions
at {a1  ...  an} can nevertheless differ signiﬁcantly in their predictions at a. The above deﬁnition
measures the “similarity” of predictions at -scale  and measures whether two functions make similar
predictions at {a1  ...  an} based on the cumulative discrepancy
i=1(f (ai) − ˜f (ai))2. This
measure of dependence suggests using the following notion of dimension.
Deﬁnition 4. The -eluder dimension dimE(F  ) is the length d of the longest sequence of elements
in A such that  for some (cid:48) ≥   every element is (cid:48)-independent of its predecessors.

(cid:113)(cid:80)n

Recall that a vector space has dimension d if and only if d is the length of the longest sequence of
elements such that each element is linearly independent or equivalently  0-independent of its pre-
decessors. Deﬁnition 4 replaces the requirement of linear independence with -independence. This
extension is advantageous as it captures both nonlinear dependence and approximate dependence.

5 Conﬁdence Bounds and Regret Decompositions

A key to our analysis is recent observation [16] that the regret of both Thompson sampling and a
UCB algorithm can be decomposed in terms of conﬁdence sets. Deﬁne the width of a subset ˜F ⊂ F
at an action a ∈ A by

(cid:0)f (a) − f (a)(cid:1) .

w ˜F (a) = sup
f  f∈ ˜F

(1)

This is a worst–case measure of the uncertainty about the payoff fθ(a) at a given that fθ ∈ ˜F.

5

t=1

t=1

(2)

(3)

Proposition 1. Fix any sequence {Ft : t ∈ N}  where Ft ⊂ F is measurable with respect to σ(Ht).
Then for any T ∈ N  with probability 1 

R(T  πF1:∞ ) ≤ T(cid:88)
[wFt(At) + C1(fθ /∈ Ft)]
T(cid:88)
E(cid:2)R(T  πTS)(cid:3) ≤ E
tially bounds regret in terms of the sum of widths(cid:80)T

[wFt(At) + C1(fθ /∈ Ft)] .

If the conﬁdence sets Ft are constructed to contain fθ with high probability  this proposition essen-
t=1 wFt(At). In this sense  the decomposition
bounds regret only in terms of uncertainty about the actions A1 .. At that the algorithm has actually
sampled. As actions are sampled  the value of fθ(·) at those actions is learned accurately  and hence
we expect that the width wFt(·) of the conﬁdence sets should diminish over time.
It is worth noting that the regret bound of the UCB algorithm πF1:∞ depends on the speciﬁc conﬁ-
dence sets {Ft : t ∈ N} used by the algorithm whereas the bound of πTS applies for any sequence
of conﬁdence sets. However  the decomposition (3) holds only in expectation under the prior distri-
bution. The implications of these decompositions are discussed further in earlier work [16].
In the next section  we design abstract conﬁdence sets Ft that are shown to contain the true function
t=1 wFt (At)
in terms of the eluder dimension of the class of functions F. When combined with Proposition 1 
this analysis provides regret bounds for both Thompson sampling and for a UCB algorithm.

fθ with high probability. Then  in Section 7 we give a worst case bound on the sum(cid:80)T

6 Construction of conﬁdence sets

t

1

1

2 Et

= (cid:80)t−1

:= {f ∈ F : (cid:107)f − ˆf LS

g2(Ak). Hence (cid:107)f − fθ(cid:107)2

arg minf∈F L2 t(f ) where L2 t(f ) = (cid:80)t−1

∈
(f (At) − Rt)2 is the cumulative squared predic-
βt} where βt is
is deﬁned by
measures the cumulative discrepancy between the

The abstract conﬁdence sets we construct are centered around least squares estimates ˆf LS
t (cid:107)2 Et ≤ √
tion error.1 The sets take the form Ft
an appropriately chosen conﬁdence parameter  and the empirical 2-norm (cid:107)·(cid:107)2 Et
(cid:107)g(cid:107)2
previous predictions of f and fθ.
The following lemma is the key to constructing strong conﬁdence sets (Ft : t ∈ N). For an arbitrary
function f  it bounds the squared error of f from below in terms of the empirical loss of the true
function fθ and the aggregate empirical discrepancy (cid:107)f − fθ(cid:107)2
between f and fθ. It establishes
that for any function f  with high probability  the random process (L2 t(f ) : t ∈ N) never falls
2(cid:107)f − fθ(cid:107)2
: t ∈ N) by more than a ﬁxed constant. A proof of
below the process (L2 t(fθ) + 1
the lemma is provided in the appendix. Recall that η is a constant given in Assumption 2.
Lemma 1. For any δ > 0 and f : A (cid:55)→ R 

2 Et

2 Et

2 Et

(cid:18)

(cid:19)

(cid:12)(cid:12)(cid:12)(cid:12) θ

P

L2 t(f ) ≥ L2 t(fθ) +

(cid:107)f − fθ(cid:107)2

2 Et

1
2

− 4η2 log (1/δ) ∀t ∈ N

≥ 1 − δ.

2 Et

By Lemma 1  with high probability  f can enjoy lower squared error than fθ only if its empirical
deviation (cid:107)f − fθ(cid:107)2
from fθ is less than 8η2 log(1/δ). Through a union bound  this property
holds uniformly for all functions in a ﬁnite subset of F. To extend this result to inﬁnite classes of
functions  we measure the function class at some discretization scale α. Let N (F  α  (cid:107)·(cid:107)∞) denote
the α-covering number of F in the sup-norm (cid:107) · (cid:107)∞  and let
t (F  δ  α) := 8η2 log (N (F  α  (cid:107)·(cid:107)∞)/δ) + 2αt
β∗

(cid:17)
8C +(cid:112)8η2 ln(4t2/δ)

(cid:16)

(4)

.

1The results can be extended to the case where the inﬁmum of L2 t(f ) is unattainable by selecting a function

with squared prediction error sufﬁciently close to the inﬁmum.

6

Proposition 2. For all δ > 0 and α > 0  if

Ft =

for all t ∈ N  then

(cid:26)

t

(cid:13)(cid:13)(cid:13)2 Et
(cid:13)(cid:13)(cid:13)f − ˆf LS
(cid:33)
(cid:12)(cid:12)(cid:12)(cid:12) θ
∞(cid:92)

Ft

t=1

f ∈ F :

(cid:32)

P

fθ ∈

≤(cid:112)β∗

t (F  δ  α)

(cid:27)

≥ 1 − 2δ.

Example 3. Suppose Θ ⊂ [0  1]d and for each a ∈ A  fθ(a) is an L–Lipschitz function of θ. Then
N (F  α (cid:107) · (cid:107)∞) ≤ (1 + L/)d and hence log N (F  α (cid:107) · (cid:107)∞) ≤ d log(1 + L/).

7 Measuring the rate at which conﬁdence sets shrink

Our remaining task is to provide a worst case bound on the sum(cid:80)T

algebra leads to a worst case bound on(cid:80)T

1 wFt(At). First consider the
case of a linearly parameterized model where fρ(a) := (cid:104)φ(a)  ρ(cid:105) for each ρ ∈ Θ ⊂ Rd. Then 
it can be shown that our conﬁdence set takes the form Ft := {fρ : ρ ∈ Θt} where Θt ⊂ Rd is
an ellipsoid. When an action At is sampled  the ellipsoid shrinks in the direction φ(At). Here
the explicit geometric structure of the conﬁdence set implies that the width wFt shrinks not only
at At but also at any other action whose feature vector is not orthogonal to φ(At). Some linear
1 wFt (At). For a general class of functions  the situation
is much subtler  and we need to measure the way in which the width at each action can be reduced
by sampling other actions.
The following result uses our new notion of dimension to bound the number of times the width of
the conﬁdence interval for a selected action At can exceed a threshold.
Proposition 3. If (βt ≥ 0|t ∈ N) is a nondecreasing sequence and Ft := {f ∈ F : (cid:107)f −
t (cid:107)2 Et ≤ √
ˆf LS

βt} then with probability 1

(cid:18) 4βT

(cid:19)

1(wFt(At) > ) ≤

2 + 1

dimE(F  )

T(cid:88)

t=1

for all T ∈ N and  > 0.

Using Proposition 3  one can bound the sum(cid:80)T

αF
t = max

t=1 wFt(At)  as established by the following lemma.
(cid:27)
(cid:26) 1
T –eluder dimension of F 
To extend our analysis to inﬁnite classes of functions  we consider the αF
where
t2   inf {(cid:107)f1 − f2(cid:107)∞ : f1  f2 ∈ F  f1 (cid:54)= f2}
(cid:0)F  αF
+ min(cid:8)dimE

Lemma 2. If (βt ≥ 0|t ∈ N) is a nondecreasing sequence and Ft := {f ∈ F : (cid:107)f − ˆf LS
√
βt} then with probability 1  for all T ∈ N 

(cid:1)   T(cid:9) C + 4

(5)
t (cid:107)2 Et ≤

(cid:0)F  αF

(cid:1) βT T .

T(cid:88)

(cid:113)

dimE

(6)

.

T

T

wFt(At) ≤ 1
T

t=1

8 Main Result

1:∞ executed with appropriate conﬁdence sets {F∗

Our analysis provides a new guarantee both for Thompson sampling  and for a UCB algorithm
t : t ∈ N}. Recall  for a sequence of con-
πF∗
ﬁdence sets {Ft : t ∈ N} we denote by πF1:∞ the UCB algorithm that chooses an action ¯At ∈

arg maxa∈A(cid:8)supf∈Ft fθ(a)(cid:9) at each time t. We establish bounds that are  up to a logarithmic

factor  of order

(cid:118)(cid:117)(cid:117)(cid:116)dimE
(cid:124)

(cid:0)F  T −2(cid:1)
(cid:123)(cid:122)
(cid:125)

log(cid:0)N(cid:0)F  T −2 (cid:107)·(cid:107)∞
(cid:124)

(cid:123)(cid:122)

log–covering number

(cid:1)(cid:1)
(cid:125)

T .

Eluder dimension

7

This term depends on two measures of the complexity of the function class F. The ﬁrst  which
controls for statistical over–ﬁtting  grows logarithmically in the cover numbers of the function class.
This is a common feature of notions of dimension from statistical learning theory. The second
measure of complexity  the eluder dimension  measures the extent to which the reward value at one
action can be inferred by sampling other actions.
The next two propositions  which provide ﬁnite time bounds for a particular UCB algorithm and for
Thompson sampling  follow by combining Proposition 1  Propsition 2  and Lemma 2. Deﬁne 

(cid:0)F  αF

T

(cid:1) β∗

T

(cid:0)F  αF

T   δ(cid:1) T .

B(F  T  δ) =

(cid:26)

Notice that B(F  T  δ) is the right hand side of the bound (6) with βT taken to be β∗
T   δ).
Proposition 4. Fix any δ > 0 and T ∈ N  and deﬁne for each t ∈ N  F∗
f ∈ F :

T (F  αF

(cid:13)(cid:13)(cid:13)f − ˆf LS

t

t =

(cid:113)

T

1
T

dimE

+(cid:2)min(cid:8)dimE
(cid:0)F  αF
(cid:1)   T(cid:9)(cid:3) C + 4
(cid:27)
(cid:13)(cid:13)(cid:13)2 Et
≤(cid:112)β∗
(cid:111) ≥ 1 − 2δ
P(cid:110)R(T  πF∗
t (F  αT   δ)
E(cid:104)R(T  πF∗
(cid:105) ≤ B(F  T  δ) + 2δT C
E(cid:2)R(T  πTS)(cid:3) ≤ B(F  T  T −1) + 2C

1:∞) ≤ B(F  T  δ) | θ
1:∞ ) | θ

. Then 

Proposition 5. For any T ∈ N 

The next two examples show how the regret bounds of Proposition 4 and 5 specialize to d-
dimensional linear and generalized linear models. For each of these examples Θ ⊂ Rd and each
action is associated with a known feature vector φ(a). Throughout these two examples  we ﬁx posi-
tive constants γ and s and assume that γ ≥ supa∈A (cid:107)φ(a)(cid:107) and s ≥ supρ∈Θ (cid:107)ρ(cid:107). For each of these
examples  a bound on dimE (F  ) is provided in the supplementary material.
Example 4. Linear Models: Consider the case of a d-dimensional linear model fρ(a) :=
(cid:104)φ(a)  ρ(cid:105). Then  dimE(F  ) = O(d log(1/)) and log N (F   (cid:107)·(cid:107)∞) = O(d log(1/)). Proposi-
T ≥ T −2  This is tight to
tions 4 and 5 therefore yield O(d log(1/αF
T )
within a factor of log T [3]  and matches the best available bound for a linear UCB algorithm [2].
Example 5. Generalized Linear Models: Consider the case of a d-dimensional general-
ized linear model fθ(a) := g ((cid:104)φ(a)  θ(cid:105)) where g is an increasing Lipschitz continuous func-
Then 
tion.
log N (F   (cid:107)·(cid:107)∞) = O(d log(h/)) and dimE(F  ) = O(dr2 log(h/))  and Propositions 4 and
√
5 yield O(rd log(h/αF
T ) regret bounds. To our knowledge  this bound is a slight improvement
T )
√
over the strongest regret bound available for any algorithm in this setting. The regret bound of
Filippi et al. [7] is of order rd log3/2(T )

Set h = sup˜θ a g(cid:48)((cid:104)φ(a)  ˜θ(cid:105))  h = inf ˜θ a g(cid:48)((cid:104)φ(a)  ˜θ(cid:105)) and r = h/h.

T ) regret bounds. Since αF

√

T .

9 Conclusion

In this paper  we have analyzed two algorithms  Thompson sampling and a UCB algorithm  in a
very general framework  and developed regret bounds that depend on a new notion of dimension.
In constructing these bounds  we have identiﬁed two factors that control the hardness of a particular
multi-armed bandit problem. First  an agent’s ability to quickly attain near-optimal performance
depends on the extent to which the reward value at one action can be inferred by sampling other
actions. However  in order to select an action the agent must make inferences about many possible
actions  and an error in its evaluation of any one could result in large regret. Our second measure
of complexity controls for the difﬁculty of maintaining appropriate conﬁdence sets simultaneously
at every action. While our bounds are nearly tight in some cases  further analysis is likely to yield
stronger results in other cases. We hope  however  that our work provides a conceptual foundation
for the study of such problems  and inspires further investigation.

Acknowledgments

The ﬁrst author is supported by a Burt and Deedee McMurty Stanford Graduate Fellowship. This
work was supported in part by Award CMMI-0968707 from the National Science Foundation.

8

References
[1] V. Dani  T.P. Hayes  and S.M. Kakade. Stochastic linear optimization under bandit feedback. In Proceed-

ings of the 21st Annual Conference on Learning Theory (COLT)  pages 355–366  2008.

[2] Y. Abbasi-Yadkori  D. P´al  and C. Szepesv´ari. Improved algorithms for linear stochastic bandits. Advances

in Neural Information Processing Systems  24  2011.

[3] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations

Research  35(2):395–411  2010.

[4] R. Kleinberg  A. Slivkins  and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings of the 40th

ACM Symposium on Theory of Computing  2008.

[5] S. Bubeck  R. Munos  G. Stoltz  and C. Szepesv´ari. X-armed bandits. Journal of Machine Learning

Research  12:15871627  2011.

[6] N. Srinivas  A. Krause  S.M. Kakade  and M. Seeger. Information-theoretic regret bounds for Gaussian
process optimization in the bandit setting. Information Theory  IEEE Transactions on  58(5):3250 –3265 
may 2012. ISSN 0018-9448. doi: 10.1109/TIT.2011.2182033.

[7] S. Filippi  O. Capp´e  A. Garivier  and C. Szepesv´ari. Parametric bandits: The generalized linear case.

Advances in Neural Information Processing Systems  23:1–9  2010.

[8] Y. Abbasi-Yadkori  D. Pal  and C. Szepesv´ari. Online-to-conﬁdence-set conversions and application to

sparse stochastic bandits. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[9] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in applied mathe-

matics  6(1):4–22  1985.

[10] T.L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics 

pages 1091–1114  1987.

[11] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine

learning  47(2):235–256  2002.

[12] O. Capp´e  A. Garivier  O.-A. Maillard  R. Munos  and G. Stoltz. Kullback-Leibler upper conﬁdence

bounds for optimal sequential allocation. Submitted to the Annals of Statistics.

[13] T. Jaksch  R. Ortner  and P. Auer. Near-optimal regret bounds for reinforcement learning. The Journal of

Machine Learning Research  99:1563–1600  2010.

[14] P.L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly
communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelli-
gence  pages 35–42. AUAI Press  2009.

[15] L. Kocsis and C. Szepesv´ari. Bandit based monte-carlo planning. In Machine Learning: ECML 2006 

pages 282–293. Springer  2006.

[16] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. arXiv preprint arXiv:1301.2609 

2013.

[17] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence

of two samples. Biometrika  25(3/4):285–294  1933.

[18] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business

and Industry  26(6):639–658  2010.

[19] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Information Processing

Systems (NIPS)  2011.

[20] S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. 2012.
[21] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. arXiv preprint

arXiv:1209.3353  2012.

[22] E. Kauffmann  N. Korda  and R. Munos. Thompson sampling: an asymptotically optimal ﬁnite time

analysis. In International Conference on Algorithmic Learning Theory  2012.

[23] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. arXiv preprint

arXiv:1209.3352  2012.

[24] A. Beygelzimer  J. Langford  L. Li  L. Reyzin  and R.E. Schapire. Contextual bandit algorithms with
supervised learning guarantees. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  vol-
ume 15. JMLR Workshop and Conference Proceedings  2011.

[25] K. Amin  M. Kearns  and U. Syed. Bandits  query learning  and the haystack dimension. In Proceedings

of the 24th Annual Conference on Learning Theory (COLT)  2011.

[26] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Machine Learn-

ing Research  3:397–422  2003.

9

,Daniel Russo
Benjamin Van Roy
Wei Wen
Chunpeng Wu
Yandan Wang
Yiran Chen
Hai Li