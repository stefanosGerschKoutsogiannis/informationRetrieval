2012,Locating Changes in Highly Dependent Data with Unknown Number of Change Points,The problem of multiple change point estimation is  considered for sequences  with unknown number of change points.  A consistency framework is suggested that is suitable for highly dependent time-series  and an asymptotically consistent algorithm is proposed.   In order for the consistency to be established the only assumption  required is that the data is generated by stationary ergodic time-series distributions. No modeling  independence or parametric assumptions are made; the data are allowed to be dependent and the dependence can be of arbitrary form.  The theoretical results are complemented with experimental evaluations.,Locating Changes in Highly Dependent Data

with Unknown Number of Change Points

Azadeh Khaleghi

SequeL-INRIA/LIFL-CNRS 
Universit´e de Lille  France

azadeh.khaleghi@inria.fr

Daniil Ryabko

SequeL-INRIA/LIFL-CNRS 

daniil@ryabko.net

Abstract

The problem of multiple change point estimation is considered for sequences with
unknown number of change points. A consistency framework is suggested that
is suitable for highly dependent time-series  and an asymptotically consistent al-
gorithm is proposed. In order for the consistency to be established the only as-
sumption required is that the data is generated by stationary ergodic time-series
distributions. No modeling  independence or parametric assumptions are made;
the data are allowed to be dependent and the dependence can be of arbitrary form.
The theoretical results are complemented with experimental evaluations.

1

Introduction

We are given a sequence x := X1  X2  . . .   Xn formed as the concatenation of an unknown num-
ber k + 1 of sequences  such that x = X1 . . . Xπ1Xπ1+1 . . . Xπ2 . . . Xπk . . . Xn. The time-series
distributions that generate a pair of adjacent sequences separated by indices πi  i = 1..k are differ-
ent. (Non-adjacent sequences are allowed to be generated by the same distribution). The so-called
change points πi  i = 1..k are unknown and to be estimated. Change point estimation is one of the
core problems in statistics  and as such  has been studied extensively under various formulations.
However  even nonparametric formulations of the problem typically assume that the data in each
segment are independent and identically distributed  and that the change necessarily affects single-
dimensional marginal distributions. In this paper we consider the most general nonparametric setting
where  the changes may be completely arbitrary (e.g.  in the form of the long-range dependence).
We propose a change point estimation algorithm that is asymptotically consistent under such mini-
mal assumptions.
Motivation. Change point analysis is an indispensable tool in a broad range of applications such
as market analysis  bioinformatics  network trafﬁc  audio/video segmentation only to name a few.
Clearly  in these applications the data can be highly dependent and can not be easily modeled by
parametric families of distributions. From a machine learning perspective  change point estimation
is a difﬁcult unsupervised learning problem: the objective is to estimate the change points in a given
sequence while no labeled examples are available. To better understand the challenging nature of
the problem  it is useful to compare it to time-series clustering. In time-series clustering  a set of
sequences is to be partitioned  whereas in change point estimation the partitioning is done on a se-
quence of sequences. While objectives are the same  in the latter  information about the individual
elements is no longer available  since only a single sequence formed by their concatenation is pro-
vided as input. This makes change point estimation a more challenging problem than time-series
clustering.
In the general setting of highly-dependent time-series correct estimation of the number of change
points is provably impossible  even in the weakest asymptotic sense  and even if there is at most
one change [23]. While a popular mitigation is to consider more restrictive settings  we are inter-
ested in intermediate formulations that can have asymptotically consistent solutions under the most

1

general assumptions. In light of the similarities between clustering and change point analysis  we
propose a formulation that is motivated by hierarchical clustering. When the number of clusters is
unknown  a hierarchical clustering algorithm produces a tree  such that some pruning of this tree
gives the ground-truth clustering (e.g.  [3]). In change point estimation with an unknown number
k of change points  we suggest to aim for a sorted list of change points  whose ﬁrst k elements are
some permutation of the true change points. An algorithm that achieves this goal is called consistent.
Related Work. Change point analysis is a classical problem in mathematical statistics [6  4  5 
17]. In a typical formulation  samples within each segment are assumed to be i.i.d. and the change
usually refers to the change in the mean. More general formulations are often considered as well 
however  it is usually assumed that the samples are i.i.d. in each of the segments [20  8  9  21] or
that they belong to some speciﬁc model class (such as Hidden Markov processes) [15  16  27]. In
these frameworks the problem of estimating the number of change points is usually addressed with
penalized criteria  see  for example  [19  18]. In nonparametric settings  the typical assumptions
usually impose restrictions on the form of the change or the nature of dependence (e.g.  the time-
series are assumed strongly mixing) [6  4  10  12]. Even when more general settings are considered 
it is almost exclusively assumed that the single-dimensional marginal distributions are different [7].
The framework considered in this paper is similar to that of [25] and of our recent paper [13]  in
the sense that the only assumption made is that the distributions generating the data are stationary
ergodic. The particular case of k = 1 is considered in [25]. In [13] we provide a non-trivial extension
of [25] for the case where k > 1 is known and is provided to the algorithm. However  as mentioned
above  when the number k of change points is unknown  it is provably impossible to estimate it 
even under the assumption k ∈ {0  1} [23]. In particular  if the input k is not the correct number of
change points  then the behavior of the algorithm proposed in [13] can be arbitrary bad.
Results. We present a nonparametric change point estimation algorithm for time-series data with
unknown number of change points. We consider the most general framework where the only as-
sumption made is that the unknown distributions generating the data are stationary ergodic. This
means that we make no such assumptions as independence  ﬁnite memory or mixing. Moreover  we
do not need the ﬁnite-dimensional marginals of any ﬁxed size before and after the change points to
be different. Also  the marginal distributions are not required to have densities.
We show that the proposed algorithm is asymptotically consistent in the sense that among the change
point estimates that it outputs  the ﬁrst k converge to the true change points. Moreover  our algorithm
can be efﬁciently calculated; it has a computational complexity O(n2 polylog n) where n is the
length of the input sequence. To the best of our knowledge  this work is the ﬁrst to address the
change point problem with an unknown number of change points in such general framework.
We further conﬁrm our theoretical ﬁndings through experiments on synthetic data. Our experimental
setup is designed so as to demonstrate the generality of the suggested framework. To this end  we
generate our data by time-series distributions that  while being stationary ergodic  do not belong to
any “simpler” class of processes. In particular they cannot be modeled as hidden Markov processes
with ﬁnite or countably inﬁnite set of states. Through our experiments we show that the algorithm
is consistent in the sense that as the length of the input sequence grows  the produced change point
estimations converge to the actual change points.
Organization. In Section 2 we introduce some preliminary notation and deﬁnitions. We formulate
the problem in Section 3. Section 4 presents our main theoretical results  including the proposed
algorithm  and an informal description of how and why it works. In Section 5 we prove that the
proposed algorithm is asymptotically consistent under the general framework considered; we also
show that our algorithm can be computed efﬁciently. In Section 6 we present some experimental
results  and ﬁnally in Section 7 we provide some concluding remarks and future directions.

2 Notation and deﬁnitions
Let X be some measurable space (the domain); in this work we let X = R  but extensions to
more general spaces are straightforward. For a sequence X1  . . .   Xn we use the abbreviation X1..n.
Consider the Borel σ-algebra B on X ∞ generated by the cylinders {B×X ∞ : B ∈ Bm l  m  l ∈ N}
where  the sets Bm l  m  l ∈ N are obtained via the partitioning of X m into cubes of dimension m
and volume 2−ml (starting at the origin). Let also Bm := ∪l∈NBm l. Processes are probability

2

measures on the space (X ∞ B). For x = X1..n ∈ X n and B ∈ Bm let ν(x  B) denote the
frequency with which x falls in B  i.e.

ν(x  B) :=

I{n ≥ m}
n − m + 1

I{Xi..i+m−1 ∈ B}

(1)

A process ρ is stationary if for any i  j ∈ 1..n and B ∈ Bm  m ∈ N  we have ρ(X1..j ∈ B) =
ρ(Xi..i+j−1 ∈ B). A stationary process ρ is called (stationary) ergodic if for all B ∈ B we have
limn→∞ ν(X1..n  B) = ρ(B) with ρ-probability 1. The distributional distance between a pair of
process distributions ρ1 and ρ2 is deﬁned as follows

d(ρ1  ρ2) :=

wmwl

m l=1

B∈Bm l

|ρ1(B) − ρ2(B)|

where  wi := 2−i  i ∈ N. Note that any summable sequence of positive scores also works. It is
easy to see that d(· ·) is a metric. For more on the distributional distance and its properties see [11].
In this work we use empirical estimates of this distance. Speciﬁcally  the empirical estimate of the
distance between a sequence x = X1..n ∈ X n  n ∈ N and a process distribution ρ is deﬁned as

ˆd(x  ρ) :=

wmwl

|ν(x  B) − ρ(B)|

and for a pair of sequences xi ∈ X ni ni ∈ N  i = 1  2. it is deﬁned as

ˆd(x1  x2) :=

wmwl

m l=1

B∈Bm l

|ν(x1  B) − ν(x2  B)|.

Although expressions (2) and (3) involve inﬁnite sums they can be easily calculated [22]. Moreover 
the estimates ˆd(· ·) are asymptotically consistent [25]: for any pair of stationary ergodic distributions
ρ1  ρ2 generating sequences xi ∈ X ni i = 1  2 we have

n−m+1(cid:88)

i=1

(cid:88)

(cid:88)
(cid:88)

B∈Bm l

∞(cid:88)

∞(cid:88)
∞(cid:88)

m l=1

(2)

(3)

(4)

(5)

(6)

lim

n1 n2→∞
lim
ni→∞

ˆd(x1  x2) = d(ρ1  ρ2)  a.s.  and
ˆd(xi  ρj) = d(ρi  ρj)  i  j ∈ 1  2  a.s.

Moreover  a more general estimate of (.· ·) may be obtained as

ˇd(x1  x2) :=

wmwl

|ν(x1  B) − ν(x2  B)|

mn(cid:88)

ln(cid:88)

m=1

l=1

(cid:88)

B∈Bm l

where  mn and ln are any sequences of integers that go to inﬁnity with n. As shown in [22] the
consistency results for ˆd  i.e. (2) and (3) equally hold for ˇd so long as mn  ln go to inﬁnity with n.
Let x = X1..n be a sequence and consider a subsequence Xa..b of x with a < b ∈ 1..n. We deﬁne
the intra-subsequence distance of Xa..b as

(7)

(8)

∆x(a  b) := ˆd(Xa..(cid:98) a+b

2 (cid:99)  X(cid:100) a+b

2 (cid:101)..b)

We further deﬁne the single-change point estimator of Xa..b  a < b as

Φx(a  b  α) := argmax
t∈[a b]

ˆd(Xa−nα..t  Xt..b+nα)  α ∈ (0  1)

3 Problem Formulation

We formalize the multiple change point estimation problem as follows. We are given a sequence

x := X1  . . .   Xn ∈ X n

which is the concatenation of an unknown number κ + 1 of sequences

X1..π1  Xπ1+1..π2   . . .   Xπκ+1..n.

3

Each of these sequences is generated by an unknown stationary ergodic process distribution. More-
over  every two consecutive sequences are generated by two different process distributions. (A pair
of non-consecutive sequences may be generated by the same distribution.) The process distributions
are not required to be independent. The parameters πk are unknown and have to be estimated; they
are called change points. Note that it is not required for the means  variances or single-dimensional
marginals of the distributions to be different. We are considering the most general scenario where
the process distributions are different.
Deﬁnition 1 (change point estimator). A change point estimator is a function that takes a sequence x
and a parameter λ ∈ (0  1) and outputs a sequence of change point estimates  ˆπ := ˆπ1  ˆπ2  . . . ˆπ1/λ.
(Note that the total number of estimated change points 1/λ may be larger than the true number of
change points κ.)

To construct consistent algorithms  we assume that the change points πk are linear in n i.e. πk :=
nθk where θk ∈ (0  1) k = 1..κ are unknown. We also deﬁne the minimum normalized distance
between the change points as

λmin := min

k=1..κ+1

θk − θk−1

(9)

where θ0 := 0 and θκ+1 := 1  and assume λmin > 0. The reason why we impose these conditions is
that the consistency properties we are after are asymptotic in n. If the length of one of the sequences
is constant or sublinear in n then asymptotic consistency is impossible in this setting. We deﬁne the
consistency of a change point estimator as follows.
Deﬁnition 2 (Consistency of a change point estimator). Let ˆπ := ˆπ1  ˆπ2  . . . ˆπ1/λ be a change point
n ˆπκ)  where sort(·) orders the ﬁrst κ elements
estimator. Let ˆθ(κ) = (ˆθ1  . . .   ˆθκ) := sort( 1
ˆπ1  . . .   ˆπκ of ˆπ with respect to their order of appearance in x. We call the change point estimator
ˆπ asymptotically consistent if with probability 1 we have

n ˆπ1  . . .   1

lim
n→∞ sup

k=1..κ

|ˆθk − θk| = 0.

4 Theoretical Results

In this section we introduce a nonparametric multiple change point estimation algorithm for the
case where the number of change points is unknown. We also give an informal description of the
algorithm  and intuitively explain why it works. The main result is Theorem 1 which states that the
proposed algorithm is consistent under the most general assumptions. Moreover  the computational
complexity of the algorithm is O(n2 polylog n) where n denotes the length of the input sequence.
The main steps of the algorithm are as follows. Given λ ∈ (0  1)  a sequence of evenly-spaced
indices is formed. The index-sequence is used to partition x = X1..n into consecutive segments
3 . The single-change point estimator Φ(· · ·) is used to generate a
of length nα  where α := λ
candidate change point within every segment. Moreover  the intra-subsequence-distance ∆(· ·) of
each segment is used as its performance score s(· ·). The change point candidates are ordered
according to the performance-scores of their corresponding segments. The algorithm assumes the
input parameter λ to be a lower-bound on the true normalized minimum distance λmin between
actual change points. Hence  the sorted list of estimated change points is ﬁltered in such a way that
its elements are at least λn apart. The algorithm outputs an ordered sequence ˆπ of change point
estimates  where the ordering is done with respect to the performance scores s(· ·). The length of
ˆπ may be larger than κ. However  as we show in Theorem 1  from some n on  the ﬁrst κ elements
ˆπk  k = 1..κ of the output ˆπ converge to some permutation of the true change points  π1 ···   πκ.
Theorem 1. Let x := X1..n ∈ X n  n ∈ N be a sequence with change points at least nλmin apart 
for some λmin ∈ (0  1). Then Alg1(x  λ) is asymptotically consistent for λ ∈ (0  λmin].
Remark 2 (Computational complexity). While the deﬁnition (3) of ˆd(· ·) involves taking inﬁ-
nite sums  the distance can be calculated efﬁciently.
Indeed  in (3) all summands correspond-
ing to m > maxi=1 2 ni equal 0; moreover  all summands corresponding to l > smin are
equal  where smin corresponds to the partition in which each cell has at most one point in it
smin := mini j∈1..n  Xi(cid:54)=Xj |Xi − Xj|. Thus  even with a most naive implementation the com-
putational complexity of the algorithm is at most polynomial in all arguments. A more efﬁcient
implementation can be obtained if one uses ˇd(· ·) given by (6)  instead of ˆd(· ·)  with m = log n 

4

Algorithm 1 Estimating the change points

input: Sequence x = X1..n  Minimum Normalized Distance between the change points λ
initialize: Step size α ← λ
1. Generate 2 sets of index-sequences:

3   Output change point Sequence ˆπ ← ()

i ← nα(i +
bt

1

t + 1

)  i = 0..

1
α

  t = 1  2

2. Calculate the intra-distance value (given by (7)) of every segment Xbt
performance score:
  t = 1  2

s(t  i) ← ∆x(bt

i+1)  i = 1..

i  bt

i ..bt

i+1

  i = 1.. 1

α   t = 1  2 as its

3. Use the single-change point-estimator (given by (8)) to estimate a change point in every segment:

ˆp(t  i) := Φx(bt

i  bt

i+1  α)  i = 1..

1
α
− 1  t = 1  2

1
α

4. Remove duplicates and sort based on scores:

U ← {(t  i) : i ∈ 1..

− 1  t = 1  2}

1
α

while U (cid:54)= ∅ do

i. Select an available change point estimate of highest score and add it to ˆπ:
(τ  l) ← argmax(t i)∈U s(t  i) - break the ties arbitrarily

ˆπ ← ˆπ ⊕ ˆp(τ  l)  i.e. append ˆπ with ˆp(τ  l)

ii. Remove the estimates within a radius of λn/2 from ˆπ(l):

U ← U \ {(t  i) : ˆp(t  i) ∈ (ˆp(τ  l) − λn/2  ˆp(τ  l) + λn/2)}

end while
return: A sequence ˆπ of change point estimates. Note: Elements of ˆπ are at least nλ apart and are
sorted in decreasing order of their scores s(· ·).

where n is the length of the samples; in this case  the consistency results are unaffected  and the
computational complexity of calculating the distance becomes n polylog n  making the complexity
of the algorithm n2 polylog n. The choice m = log n is further justiﬁed by the fact that the fre-
quencies of cells in Bm l corresponding to higher values of m are not consistent estimates of their
probabilities (and thus only add to the error of the estimate); see [22  14] for further discussion.

2

and X a+b

The proof of the theorem is given in the next section. Here we provide an intuition as to why the
consistency statement holds.
First  recall that the empirical distributional distance between a given pair of sequences converges
to the distributional distance between the corresponding process distributions. Consider a sequence
x = X1..n  and assume that a segment Xa..b  a  b ∈ 1..n does not contain any change points  so
2 ..b are generated by the same process. If the length of Xa..b is linear in n  so
that Xa.. a+b
that b − a = αn for some α ∈ (0  1)  then its intra-subsequence distance ∆x(a  b) (deﬁned by (7))
converges to 0 with n going to inﬁnity. On the other hand  if there is a single change point π within
Xa..b whose distance from a and b is linear in n  then ∆x(a  b) converges to a non-zero constant.
Now assume that Xa..b with its change point at π ∈ a..b is contained within a larger segment
Xa−nα(cid:48)..b+nα(cid:48) for some α(cid:48) ∈ (0  1).
In this case  the single-change point estimator Φ(a  b  α(cid:48))
(deﬁned by (8)) produces an estimate that from some n on converges to π provided that π is the only
change point in Xa−nα(cid:48)..b+nα(cid:48). These observations are key to the consistency of the algorithm.
When λ ≤ λmin  each of the index-sequences generated with α := λ
3 partitions x in such a way
that every three consecutive segments of the partition contain at most one change point. Also  the
segments are of lengths linear in n. In this scenario  from some n on  the change point estimator
Φ(· · ·) produces correct candidates within each of the segments that contains a true change point.
Moreover  from some n on  the performance scores s(· ·) of the segments without change points
converge to 0  while those corresponding to the segments that encompass a change point converge

5

to a non-zero constant. Thus from some n on  the κ change point candidates of highest performance
score that are at least at a distance λn from one another  each converge to a unique change point.
A problem occurs if the generated index-sequence is such that it includes some of the change points
as elements. As a mitigation strategy  we generate two index-sequences with the same gap αn
between their consecutive elements but with distinct starting points: one starts at nα
2 and the other
at nα
3 . Each index-sequence gives a different partitioning of x into consecutive segments. This way 
every change point is fully encompassed by at least one segment from either of the two partitions.
We choose the appropriate segments based on their performance scores. From the above argument
we can see that segments with change points will have higher scores  and the change points within
will be estimated correctly; ﬁnally  this is used to prove the theorem in the next session.

5 Proof of Theorem 1

The proof relies on Lemma 1 and Lemma 2  which we borrow from [13] and state here without proof.
We also require the following additional notation.
Deﬁnition 3. For every change point πk  k = 1..κ and every ﬁxed t = 1  2 we denote by Lt(πk)
and by Rt(πk) the elements of the index-sequence bt
α that appear immediately to the left
and to the right of πk respectively  i.e. Lt(πk) :=
i.
bt
min
(Equality occurs when πk for some k ∈ 1..κ is exactly at the start or at the end of a segment.)
Lemma 1 ([13]). Let x = X1..n be generated by a stationary ergodic process ρ. For all ζ ∈ [0  1)
and α ∈ (0  1) we have 

i  i = 1.. 1
i≤πk  i=0.. 1
bt

i and Rt(πk) :=
bt

i≥πk  i=0.. 1
bt

∆x(b1  b2) = 0.

max

sup

α

α

lim
n→∞

b1≥ζn  b2≥b1+αn

Lemma 2 ([13]). Let δ denote the minimum distance between the distinct distributions generating
the data. Denote by κ the “unknown” number of change points and assume that for some ζ ∈ (0  1)
and some t = 1  2 we have  inf k=1..κ
i=0.. 1
α
lim
n→∞ inf
k∈1..κ

(i) With probability one we have 
(ii) If additionally we have that [Lt(πk) − nα  Rt(πk) + nα] ⊆ [πk−1  πk+1] then with probability
one we obtain 

|Φx(Lt(πk)  Rt(πk)  α) − πk| = 0.

∆x(Lt(πk)  Rt(πk)) ≥ δζ.

i − πk| ≥ ζn.
|bt

n→∞ sup
lim
k∈1..κ

1
n

Proof of Theorem 1. We ﬁrst give an outline of the proof. In order for a change point πk  k ∈ 1..κ
to be estimated correctly through this algorithm  there needs to be at least one t = 1  2 such that

1. πk ∈ (Lt(πk)  Rt(πk)) and 2. [Lt(πk) − nα  Rt(πk) + nα] ⊆ [πk−1  πk]

where α := λ
3   as speciﬁed by the algorithm. We show that from some n on  for every change point
the algorithm selects an appropriate segment satisfying these conditions  and assigns it a perfor-
mance score s(· ·) that converges to a non-zero constant. Moreover  the performance scores of the
segments without change points converge to 0. Recall that  the change point candidates are ﬁnally
sorted according to their performance scores  and the sorted list is ﬁltered to include only elements
that are at least λn apart. For λ ≤ λmin  from some n on  the ﬁrst κ elements of the output change
point sequence ˆπ are some permutation of the true change points. The proof follows.
Fix an ε > 0. Recall that the algorithm speciﬁes α := λ
indicies bt

3 and generates a sequence of evenly-spaced

i := nα(i + 1

t+1 )  i = 1.. 1

α   t = 1  2. Observe that
i − bt
1
bt
α

i−1 = nα  i = 1..

.

(10)

α and t ∈ 1  2 we have that the index bt

For every i ∈ 0.. 1
has a linear distance from it. More formally  deﬁne ζ(t  i) := min
k∈1..κ
1..2. (Note that ζ(t  i) can also be zero). For all i ∈ 0.. 1

i is either exactly equal to a change point or
t+1 )−θk|  i ∈ 0..1/α t ∈

|α(i+ 1

α   t = 1  2 and k ∈ 1..κ we have

i − πk| ≥ nζ(t  i).
|bt

6

(11)

For every t = 1  2 and i = 0..1/α  a performance score s(t  i) is calculated as the intra-subsequence
α s.t. ∃k ∈
distance ∆x(bt
1..κ  πk ∈ (bt
α} \ I. By (10)  (11)
and Lemma 1  there exists some N1 such that for all n ≥ N1 we have 

i+1) of the segment Xbt
i+1)}. Also deﬁne the complement set I(cid:48) := {1  2} × {1.. 1

. Let I := {(t  i) : t ∈ 1  2  i ∈ 1.. 1

i  bt
i  bt

i..bt

i+1

(12)
Since λ ≤ λmin  we have α ∈ (0  λmin/3]. Therefore  for every t = 1  2 and every change point
πk  k ∈ 1..κ we have

sup
(t i)∈I(cid:48)

s(t  i) ≤ ε.

[Lt(πk) − nα  Rt(πk) + nα] ⊆ [πk−1  πk+1].
Deﬁne µmin := min(t i)∈I ζ(t  i). It follows from the deﬁnition of I that

(13)

µmin > 0.

(14)
By (10)  (11)  (13)  (14) and Lemma 2.(i)  there exists some N2 such that for all n ≥ N2 we have
(15)
where δ denotes the minimum distance between the distributions. Let π(t  i)  i ∈ 0..1/α  t = 1  2
i+1  (t  i) ∈ I  i.e. π(t  i) := πk  k ∈
denote the change point that is contained within bt
1..κ s.t. πk ∈ (bt
i+1). As speciﬁed in Step 3  the change point candidates are obtained as ˆp(t  i) :=
i+1   α)  i = 1..1/α − 1. By (10)  (11)  (13)  (14) and Lemma 2.(ii) there exists some N4
  bτ (i)
Φx(bτ (i)
such that for all n ≥ N4 we have

(t i)∈I s(t  i) ≥ δµmin

i..bt

i  bt

inf

i

sup
(t i)∈I

1
n

|ˆp(t  i) − π(t  i)| ≤ ε.

(16)

Let N := maxi=1..4 Ni. Recall that (as speciﬁed in Step 4)  the algorithm generates an output
sequence ˆπ := ˆπ1  . . .   ˆπ1/λ by ﬁrst sorting the change point candidates according to their perfor-
mance scores  and then ﬁltering the sorted list so that the remaining elements are at least nλ apart.
It remains to see that the corresponding estimate of every change point appears exactly once in ˆπ.
By (12) and (15) for all n ≥ N the segments bt
i+1  (t  i) ∈ I are assigned higher scores than
i+1  (t  i) ∈ I(cid:48). Moreover  by construction for every change point πk  k = 1..κ there ex-
bt
i..bt
ists some (t  i) ∈ I such that πk = π(t  i) which  by (16) is estimated correctly for all n ≥ N.
Next we show that every estimate appears at most once in the output sequence ˆπ. By (16) for all
(t  i)  (t(cid:48)  i(cid:48)) ∈ I such that π(t  i) = π(t(cid:48)  i(cid:48)) and all n ≥ N we have

i..bt

1
n

|ˆp(t  i) − ˆp(t(cid:48)  i(cid:48))| ≤ 1
n

|ˆp(t  i) − π(t  i)| +

|ˆp(t(cid:48)  i(cid:48)) − π(t(cid:48)  i(cid:48))| ≤ 2ε.

(17)

1
n

On the other hand  for all (t  i)  (t(cid:48)  i(cid:48)) ∈ I such that π(t  i) (cid:54)= π(t(cid:48)  i(cid:48)) and all n ≥ N we have
|ˆp(t(cid:48)  i(cid:48)) − π(t(cid:48)  i(cid:48))|

1
n

|ˆp(t  i) − ˆp(t(cid:48)  i(cid:48))| ≥ 1
n
≥ 1
n

|π(t  i) − π(t(cid:48)  i(cid:48))| − 1
n
|π(t  i) − π(t(cid:48)  i(cid:48))| − 2ε ≥ λmin − 2ε

|ˆp(t  i) − π(t  i)| − 1
n

(18)

where the last inequality follows from (16) and that the true change points are at least nλmin apart.
By (17) and (18) the duplicate estimates of every change point are ﬁltered  while estimates cor-
responding to different change points are left untouched. Finally  following the notation of Deﬁ-
nition 2  let ˆθ(κ) = (ˆθ1  . . .   ˆθκ) := sort( 1
n ˆπκ)  (sorted with respect to their order of
appearance in x). For n ≥ N we have  supk∈1..κ |ˆθk − θk| ≤ ε and the statement follows.

n ˆπ1 ···   1

6 Experimental Results
In this section we use synthetically generated time-series data to empirically evaluate our algorithm.
To generate the data we have selected distributions that while being stationary ergodic  do not be-
long to any “simpler” class of time-series  and are difﬁcult to approximate by ﬁnite-state models. In
particular they cannot be modeled by a hidden Markov process with a ﬁnite state-space. These dis-
tributions were used in [26] as examples of stationary ergodic processes which are not B-processes.

7

Figure 1: Left (Experiment 1): Average (over 20 runs) error as a function of the length of the input
sequence. Right (Experiment 2): Average (over 25 runs) error as a function the input parameter λ.

Time-series generation. To generate a sequence x = X1..n we proceed as follows. Fix some pa-
rameter α ∈ (0  1) and select r0 ∈ [0  1]. For each i = 1..n let ri = ri−1 + α − (cid:98)ri−1 + α(cid:99). The
samples Xi are obtained from ri by thresholding at 0.5  i.e. Xi := I{ri > 0.5}. We call this pro-
cedure DAS(α). If α is irrational then x forms a stationary ergodic time-series. We simulate α by
a longdouble with a long mantisa. For the purpose of our experiments we use four different process
distributions DAS(αi)  i = 1..4 with α1 = 0.30...  α2 = 0.35...  α3 = 0.40... and α4 = 0.45....
To generate an input sequence x = X1..n we ﬁx some λmin = 0.23 and randomly generate κ = 3
change points at a minimum distance nλmin. We use DAS(αi)  i = 1..4 to respectively generate
the four subsequences between every pair of consecutive change points.
Experiment 1: (Convergence with Sequence Length) In this experiment we demonstrate that the
estimation error converges to 0 as the sequence length grows. We iterate over n = 1000..20000; at
every iteration we generate an input sequence of length n as described above. We apply Algorithm 1
with λ = 0.18 to ﬁnd the change points. Figure 1 (Left) shows the average error-rate as a function
of sequence length.
Experiment 2: (Dependence on λ) Algorithm 1 requires λ ∈ (0  1) as a lower-bound on λmin.
In this experiment we show that this lower bound need not be tight. In particular  there is a rather
large range of λ ≤ λmin for which the estimation error is low. To demonstrate this  we ﬁxed the
sequence length n = 20000 and observed the error-rate as we varied the input parameter λ between
0.01..0.35. Figure 1 (Right) shows the average error-rate as a function of λ.

7 Outlook

In this work we propose a consistency framework for multiple change points estimation in highly
dependent time-series  for the case where the number of change points is unknown. The notion of
consistency that we consider requires an algorithm to produce a list of change points such that the
ﬁrst k change points approach the true unknown change points in asymptotic. While in the general
setting that we consider it is not possible to estimate the number of change points  other related
formulations may be of interest. For example  if the number of different time-series distributions is
known  but the number of change points is not  it may still be possible to estimate the latter. A simple
example of this scenario would be when two distributions generate many segments in alternation.
While the consistency result here (and in the previous works [14  22  25]) rely on the convergence of
frequencies  recent results of [1  2] on uniform convergence can be used (see [24]) to solve related
statistical problems about time-series (e.g.  clustering) and thus may also prove useful in change
point analysis.
Acknowledgements. This work is supported by the French Ministry of Higher Education and Research  Nord-
Pas-de-Calais Regional Council and FEDER through CPER 2007-2013  ANR projects EXPLO-RA (ANR-08-
COSI-004) and Lampada (ANR-09-EMER-007)  by an INRIA Ph.D. grant to Azadeh Khaleghi  by the Euro-
pean Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement 231495 (project
CompLACS)  and by Pascal-2.

8

00.511.522.5x 10400.050.10.150.20.250.30.350.40.45Length of the input sequenceError(cid:239)rate00.050.10.150.20.250.30.3500.10.20.30.40.50.60.70.80.91Input parameter (cid:104)Error(cid:239)rateReferences
[1] Terrence M. Adams and Andrew B. Nobel. Uniform convergence of Vapnik-Chervonenkis

classes under ergodic sampling. The Annals of Probability  38:1345–1367  2010.

[2] Terrence M. Adams and Andrew B. Nobel. Uniform approximation and bracketing properties

of VC classes. Bernoulli  to appear.

[3] M.F. Balcan and P. Gupta. Robust hierarchical clustering. In COLT  2010.
[4] M. Basseville and I.V. Nikiforov. Detection of abrupt changes: theory and application. Pren-

tice Hall information and system sciences series. Prentice Hall  1993.

[5] P.K. Bhattacharya. Some aspects of change-point analysis. Lecture Notes-Monograph Series 

pages 28–56  1994.

[6] B.E. Brodsky and B.S. Darkhovsky. Nonparametric methods in change-point problems. Math-

ematics and its applications. Kluwer Academic Publishers  1993.

[7] E. Carlstein and S. Lele. Nonparametric change-point estimation for data from an ergodic

sequence. Teor. Veroyatnost. i Primenen.  38:910–917  1993.

[8] L. Dumbgen. The asymptotic behavior of some nonparametric change-point estimators. The

Annals of Statistics  19(3):pp. 1471–1495  1991.

[9] D. Ferger. Exponential and polynomial tailbounds for change-point estimators. Journal of

statistical planning and inference  92(1-2):73–109  2001.

[10] L. Giraitis  R. Leipus  and D. Surgailis. The change-point problem for dependent observations.

Journal of Statistical Planning and Inference  53(3)  1996.

[11] R. Gray. Prob. Random Processes  & Ergodic Properties. Springer Verlag  1988.
[12] S. B. Hariz  J. J. Wylie  and Q. Zhang. Optimal rate of convergence for nonparametric change-

point estimators for nonstationary sequences. Annals of Statistics  35(4):1802–1826  2007.

[13] A. Khaleghi and D. Ryabko. Multiple change-point estimation in highly dependent time series.

Technical report  arXiv:1203.1515  2012.

[14] A. Khaleghi  D. Ryabko  J. Mary  and P. Preux. Online clustering of processes. In AISTATS 

JMLR W&CP 22  pages 601–609  2012.

[15] J. Kohlmorgen and S. Lemm. A dynamic hmm for on-line segmentation of sequential data.

Advances in Neural Inf. Proc. Systems  14:793–800  2001.

[16] John D. Lafferty  Andrew McCallum  and Fernando C. N. Pereira. Conditional random ﬁelds:

Probabilistic models for segmenting & labeling sequence data. In ICML  2001.

[17] T.L. Lai. Sequential changepoint detection in quality control and dynamical systems. Journal

of the Royal Statistical Society  pages 613–658  1995.

[18] Marc Lavielle. Using penalized contrasts for the change-point problem. Signal Processing 

85(8):1501 – 1510  2005.

[19] E. Lebarbier. Detecting multiple change-points in the mean of gaussian process by model

selection. Signal Processing  85(4):717 – 736  2005.

[20] C.B. Lee. Nonparametric multiple change-point estimators. Statistics & probability letters 

27(4):295–304  1996.

[21] Hidetoshi Murakami. A nonparametric locationscale statistic for detecting a change point. The

Inter. Journal of Advanced Manufacturing Technology  2001.

[22] D. Ryabko. Clustering processes. In ICML  pages 919–926  Haifa  Israel  2010.
[23] D. Ryabko. Discrimination between B-processes is impossible. Journal of Theoretical Proba-

bility  23(2):565–575  2010.

[24] D. Ryabko and J. Mary. Reducing statistical time-series problems to binary classiﬁcation. In

NIPS  Lake Tahoe  USA  2012.

[25] D. Ryabko and B. Ryabko. Nonparametric statistical inference for ergodic processes. IEEE

Transactions on Information Theory  56(3)  2010.

[26] P. Shields. The Ergodic Theory of Discrete Sample Paths. AMS Bookstore  1996.
[27] X. Xuan and K. Murphy. Modeling changing dependency structure in multivariate time series.

In ICML  pages 1055–1062. ACM  2007.

9

,Tzu-Kuo Huang
Lihong Li
Ara Vartanian
Saleema Amershi
Jerry Zhu