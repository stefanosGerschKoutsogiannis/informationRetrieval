2013,Multilinear Dynamical Systems for Tensor Time Series,Many scientific data occur as sequences of multidimensional arrays called tensors.  How can hidden  evolving trends in such data be extracted while preserving the tensor structure?  The model that is traditionally used is the linear dynamical system (LDS)  which treats the observation at each time slice as a vector.  In this paper  we propose the multilinear dynamical system (MLDS) for modeling tensor time series and an expectation-maximization (EM) algorithm to estimate the parameters.  The MLDS models each time slice of the tensor time series as the multilinear projection of a corresponding member of a sequence of latent  low-dimensional tensors.  Compared to the LDS with an equal number of parameters  the MLDS achieves higher prediction accuracy and marginal likelihood for both simulated and real datasets.,Multilinear Dynamical Systems

for Tensor Time Series

Mark Rogers

markrogersjr@berkeley.edu  {leili russell}@cs.berkeley.edu

EECS Department  University of California  Berkeley

Lei Li

Stuart Russell

Abstract

Data in the sciences frequently occur as sequences of multidimensional arrays
called tensors. How can hidden  evolving trends in such data be extracted while
preserving the tensor structure? The model that is traditionally used is the linear
dynamical system (LDS) with Gaussian noise  which treats the latent state and
observation at each time slice as a vector. We present the multilinear dynamical
system (MLDS) for modeling tensor time series and an expectation–maximization
(EM) algorithm to estimate the parameters. The MLDS models each tensor obser-
vation in the time series as the multilinear projection of the corresponding member
of a sequence of latent tensors. The latent tensors are again evolving with respect
to a multilinear projection. Compared to the LDS with an equal number of param-
eters  the MLDS achieves higher prediction accuracy and marginal likelihood for
both artiﬁcial and real datasets.

1

Introduction

A tenet of mathematical modeling is to faithfully match the structural properties of the data; yet  on
occasion  the available tools are inadequate to perform the task. This scenario is especially common
when the data are tensors  i.e.  multidimensional arrays: vector and matrix models are ﬁtted to them
without justiﬁcation. This is  perhaps  due to the lack of an agreed-upon tensor model. There are
many examples that seem to require such a model: The spatiotemporal grid of atmospheric data in
climate modeling is a time series of n× m× l tensors  where n  m and l are the numbers of latitude 
longitude  and elevation grid points. If k measurements—e.g.  temperature  humidity  and wind
speed for k=3—are made  then a time series of n× m× l× k tensors is constructed. The daily high 
low  opening  closing  adjusted closing  and volume of the stock prices of n multiple companies
comprise a time series of 6 × n tensors. A grayscale video sequence is a two-dimensional tensor
time series because each frame is a two-dimensional array of pixels.
Several queries can be made when one is presented with a tensor time series. As with any time
series  a forecast of future data may be requested. For climate data  successful prediction may
spell out whether the overall ocean temperatures will increase. Prediction of stock prices may not
only inform investors but also help to stabilize the economy and prevent market collapse. The
relationships between particular subsets of tensor elements could be of signiﬁcance. How does the
temperature of the ocean at 8◦N  165◦E affect the temperature at 5◦S  125◦W? For stock price data 
one may investigate how the stock prices of electric car companies affect those of oil companies.
For a video sequence  one might expect adjacent pixels to be more correlated than those far away
from each other. Another way to describe the relationships among tensor elements is in terms of
their covariances. Equipped with a tabulation of the covariances  one may read off how a given
tensor element affects others. Later in this paper  we will deﬁne a tensor time series model and a
covariance tensor that permits the modeling of general noise relationships among tensor elements.
More formally  a tensor X ∈ RI1×···×IM is a multidimensional array with elements that can each be
indexed by a vector of positive integers. That is  every element Xi1···iM ∈ R is uniquely addressed

1

by a vector (i1 ···   iM ) such that 1 ≤ im ≤ Im for all m. Each of the M dimensions of X is called
a mode and represents a particular component of the data. The simplest tensors are vectors and
matrices: vectors are tensors with only a single mode  while matrices are tensors with two modes.
We will consider the tensor time series  which is an ordered  ﬁnite collection of tensors that all share
the same dimensionality. In practice  each member of an observed tensor time series reﬂects the
state of a dynamical system that is measured at discrete epochs.
We propose a novel model for tensor time series: the multilinear dynamical system (MLDS). The
MLDS explicitly incorporates the dynamics  noise  and tensor structure of the data by juxtaposing
concepts in probabilistic graphical models and multilinear algebra. Speciﬁcally  the MLDS gener-
alizes the states of the linear dynamical system (LDS) to tensors via a probabilistic variant of the
Tucker decomposition. The LDS tracks latent vector states and observed vector sequences; this
permits forecasting  estimation of latent states  and modeling of noise but only for vector objects.
Meanwhile  the Tucker decomposition of a single tensor computes a latent “core” tensor but has
no dynamics or noise capabilities. Thus  the MLDS achieves the best of both worlds by uniting
the two models in a common framework. We show that the MLDS  in fact  generalizes LDS and
other well-known vector models to tensors of arbitrary dimensionality. In our experiments on both
synthetic and real data  we demonstrate that the MLDS outperforms the LDS with an equal number
of parameters.

2 Tensor algebra
Let N be the set of all positive integers and R be the set of all real numbers. Given I ∈ NM  
where M ∈ N  we assemble a tensor-product space RI1×···×IM   which will sometimes be written
as RI = R(I1 ... IM ) for shorthand. Then a tensor X ∈ RI1×···×IM is an element of a tensor-product
space. A tensor X may be referenced by either a full vector (i1  . . .   iM ) or a by subvector  using
the • symbol to indicate coordinates that are not ﬁxed. For example  let X ∈ RI1×I2×I3. Then
Xi1i2i3 is a scalar  X•i2i3 ∈ RI1 is the vector obtained by setting the second and third coordinates
to i2 and i3  and X••i3 ∈ RI1×I2 is the matrix obtained by setting the third coordinate to i3. The
concatenation of two M-dimensional vectors I = (I1  . . .   IM ) and J = (J1  . . .   JM ) is given by
IJ = (I1  . . .   IM   J1  . . .   JM )  a vector with 2M entries.
Let X ∈ RI1×···×IM   M ∈ N. The vectorization vec(X) ∈ RI1···IM is obtained by shaping the
tensor into a vector. In particular  the elements of vec(X) are given by vec(X)k = Xi1···iM   where

(cid:81)m−1
(cid:19)
n=1 In(im − 1). For example  if X ∈ R2×3×2 is given by

(cid:18) 7
is given by mat(A)kl = Ai1···iM j1···jM   where k = 1 +(cid:80)M
(cid:80)M
(cid:18) 9

then vec(X) = (1 2 3 4 5 6 7 8 9 10 11 12)T .
(cid:81)m−1
Let I  J ∈ NM   M ∈ N. The matricization mat(A) ∈ RI1···IM×J1···JM of a tensor A ∈ RIJ
(cid:81)m−1
n=1 In(im − 1) and l = 1 +
n=1 Jn(jm − 1). The matricization “ﬂattens” a tensor into a matrix. For example  deﬁne
(cid:19)
(cid:18) 1
(cid:19)

k = 1 +(cid:80)M

(cid:18) 1 3

(cid:18) 13

m=1

A ∈ R2×2×2×2 by

(cid:18) 5

  X••2 =

X••1 =

(cid:19)

9
10

11
12

8

(cid:19)

2 4

m=1

m=1

5
6

 

A••11 =

3
4

2

  A••12 =

11
12

10

  A••22 =

15
16

14

.

(cid:19)
 .

7
8

13
14
15
16

  A••21 =

 1 5

2 6
3 7
4 8

6

9
10
11
12

Then we have mat(A) =

The vec and mat operators put tensors in bijective correspondence with vectors and matrices. To
deﬁne the inverse of each of these operators  a reference must be made to the dimensionality of the
original tensor. In other words  given X ∈ RI and A ∈ RIJ  where I  J ∈ NM   M ∈ N  we have
X = vec−1
(cid:81)M
Let I  J ∈ NM   M ∈ N. The factorization of a tensor A ∈ RIJ is given by Ai1···iM j1···jM =
  where A(m) ∈ RIm×Jm for all m. The factorization exponentially reduces the

(vec(X)) and A = mat−1

IJ (mat(A)).

I

m=1 A(m)
imjm

2

number of parameters needed to express A from(cid:81)M

m=1 ImJm to(cid:80)M

=(cid:80)

m=1 ImJm. In matrix form  we
have mat(A) = A(M ) ⊗ A(M−1) ⊗ ··· ⊗ A(1)  where ⊗ is the Kronecker matrix product [1]. Note
that tensors in RIJ are not factorizable in general [2].
The product A (cid:126) X of two tensors A ∈ RIJ and X ∈ RJ  where I  J ∈ NM   M ∈ N  is given
by (A (cid:126) X)i1···iM
Xj1···jM . The tensor A is called a multilinear operator
when it appears in a tensor product as above. The product is only deﬁned if the dimensionalities of
the last M modes of A match the dimensionalities of X. Note that this tensor product generalizes
the standard matrix-vector product in the case M = 1.
We shall primarily work with tensors in their vector and matrix representations. Hence  we appeal
to the following
Lemma 1. Let I  J ∈ NM   M ∈ N  A ∈ RIJ   X ∈ RJ. Then

Ai1···iM j1···jM

j1···jM

Furthermore  if A is factorizable with matrices A(m)  then

vec(A (cid:126) X) = mat(A) vec(X) .

A(M ) ⊗ ··· ⊗ A(1)(cid:105)
(cid:104)
(cid:81)m−1
n=1 In(im − 1) and l = 1 +(cid:80)M

vec(A (cid:126) X) =

m=1

vec(X) .

(cid:81)m−1
n=1 Jn(jm − 1) for some

(2)

(1)

Proof. Let k = 1 +(cid:80)M
(cid:88)

(j1  . . .   jM ). We have

vec(A (cid:126) X)k =

m=1

j1···jM

(cid:88)

l

Ai1···iM j1···jM

Xj1···jM =

mat(A)kl vec(X)l = (mat(A) vec(X))k  

which holds for all 1 ≤ im ≤ Im  1 ≤ m ≤ M. Thus  (1) holds. To prove (2)  we express mat(A)
as the Kronecker product of M matrices A(1)  . . .   A(M ).
The Tucker decomposition can be expressed using the product (cid:126) deﬁned above.
The
Tucker decomposition models a given tensor X ∈ RI1×···×IM as the result of a multilin-
ear transformation that is applied to a latent core tensor Z ∈ RJ1×···×JM : X = A (cid:126) Z.
The multilinear operator A is a factorizable tensor such that
mat(A) = A(M )⊗A(M−1)⊗···⊗A(1) . where A(1)  . . .   A(M )
are projection matrices (Figure 1). The canonical decomposi-
tion/parallel factors (CP) decomposition is a special case of the
Tucker decomposition in which Z is “superdiagonal”  i.e.  J1 =
··· = JM = R and only the Zj1···jM such that j1 = ··· = jM
can be nonzero. The CP decomposition expresses X as a sum
∈ RIm for all m and r and ◦ denotes the tensor outer

Figure 1: The Tucker decomposi-
tion of a third-order tensor X.

X = (cid:80)R

r ◦ ··· ◦ u(M )

  where u(m)

product [3].
To illustrate  consider the case M = 2 and let X = A(cid:126)Z  where X ∈ Rn×m and Z ∈ Rp×q. Then
X = AZBT  where mat(A) = B ⊗ A. If p ≤ n and q ≤ m  then Z is a dimensionality-reduced
version of X: the matrix A increases the number of rows of Z from p to n via left-multiplication 
while the matrix B increases the number of columns of Z from q to m via right-multiplication. To
reconstruct X  we simply apply A (cid:126) Z. See Figure 1 for an illustration of the case M = 3.

r=1 u(1)

r

r

3 Random tensors
Given I ∈ NM   M ∈ N  we deﬁne a random tensor X ∈ RI1×···×IM as follows. Suppose vec(X)
is normally distributed with expectation vec(U) and positive-deﬁnite covariance mat(S)  where U ∈
RI and S ∈ RII. Then we say that X has the normal distribution with expectation U ∈ RI and
covariance S ∈ RII and write X ∼ N (U  S). The deﬁnition of the normal distribution on tensors
can thus be restated more succinctly as

X ∼ N (U  S) ⇐⇒ vec(X) ∼ N (vec U  mat S) .

(3)

Our formulation extends the normal distribution deﬁned in [4]  which is restricted to symmetric 
second-order tensors.

3

ZX=A(2)A(3)A(1)We will make use of an important special case of the normal distribution deﬁned on tensors: the
multilinear Gaussian distribution. Let I  J ∈ NM   M ∈ N  and suppose X ∈ RI and Z ∈ RJ are
jointly distributed as

Z ∼ N (U  G) and X | Z ∼ N (C (cid:126) Z  S)  

(4)
where C ∈ RIJ. The marginal distribution of X and the posterior distribution of Z given X are given
by the following result.
Lemma 2. Let I  J ∈ NM   M ∈ N  and suppose the joint distribution of random tensors X ∈ RI
and Z ∈ RJ is given by (4). Then the marginal distribution of X is

X ∼ N(cid:0)C (cid:126) U  C (cid:126) G (cid:126) CT + S(cid:1)  

(5)

where CT ∈ RJI and CT

j1···jM i1···iM

= Ci1···iM j1···jM . The conditional distribution of Z given X is

Z | X ∼ N(cid:16) ˆU  ˆG

(cid:17)

 

where ˆU = vec−1

Γ = mat(G)  Σ = mat(S)  and W = Γmat(C)T(cid:104)

J (µ + W (vec(X) − mat(C) µ))  ˆG = mat−1

mat(C) Γmat(C)T + Σ

(cid:105)−1

.

(6)
JJ (Γ − W mat(C) Γ)  µ = vec(U) 

Proof. Lemma 1  (3)  and (4) imply that the vectorizations of Z and X given Z follow vec(Z) ∼
N (µ  Γ) and vec(X) | vec(Z) ∼ N (mat(C) vec(Z)   Σ). By the properties of the multivariate
normal distribution  the marginal distribution of vec(X) and the conditional distribution of vec(Z)
given vec(X) are vec(X) ∼ N (mat(C) vec(U)  mat(C) Γmat(C)T + Σ) and vec(Z) | vec(X) ∼
N (vec( ˆU)  mat(ˆG)). The associativity of (cid:126) implies that mat(C (cid:126) G (cid:126) CT) = mat(C) Γmat(C)T.
Finally  we apply Lemma 1 once more to obtain (5) and (6).

4 Multilinear dynamical system

The aim is to develop a model of a tensor time series X1  . . .   XN that takes into account tensor
structure. In deﬁning the MLDS  we build upon the results of previous sections by treating each
Xn as a random tensor and relating the model components with multilinear transformations. When
the MLDS components are vectorized and matricized  an LDS with factorized transition and projec-
tion matrices is revealed. Hence  the strategy for ﬁtting the MLDS is to vectorize each Xn  run the
expectation-maximization (EM) algorithm of the LDS for all components but the matricized transi-
tion and projection tensors–which are learned via an alternative gradient method–and ﬁnally convert
all model components back to tensor form.

4.1 Deﬁnition
Let I  J ∈ NM   M ∈ N. The MLDS model consists of a sequence Z1  . . .   ZN of latent tensors 
where Zn ∈ RJ1×···×JM for all n. Each latent tensor Zn emits an observation Xn ∈ RI1×···×IM .
The system is initialized by a latent tensor Z1 distributed as
Z1 ∼ N (U0  Q0) .

(7)

Given Zn  1 ≤ n ≤ N − 1  we generate Zn+1 according to the conditional distribution

Zn+1 | Zn ∼ N (A (cid:126) Zn  Q)  

(8)
where Q is the conditional covariance shared by all Zn  2 ≤ n ≤ N  and A is the transition tensor
which describes the dynamics of the evolving sequence Z1  . . .   ZN . The transition tensor A is
factorized into M matrices A(m)  each of which acts on a mode of Zn. In matrix form  we have
mat(A) = A(M ) ⊗ ··· ⊗ A(1). To each Zn there corresponds an observation Xn generated by

Xn | Zn ∼ N (C (cid:126) Zn  R)  

(9)

4

where R is the covariance shared by all Xn and C is the projec-
tion tensor which multilinearly transforms the latent tensor Zn.
Like the transition tensor A  the projection tensor C is factoriz-
able  i.e.  mat(C) = C (M ) ⊗ ··· ⊗ C (1). See Figure 2 for an
illustration of the MLDS.
By vectorizing each Xn and Zn  the MLDS becomes an LDS
with factorized transition and projection matrices mat(A) and
mat(C). For the LDS  the transition and projection operators are
not factorizable in general [2]. The factorizations of A and C
for the MLDS not only allow for a generalized dimensionality
reduction of tensors but exponentially reduce the number of parameters of the transition and projec-
m=1 ImJm down to |AMLDS| + |CMLDS| =

tion operators from |ALDS| + |CLDS| =(cid:81)M
(cid:80)M

Figure 2: Schematic of the MLDS
with three modes.

m +(cid:81)M

m=1 J 2

m +(cid:80)M

m=1 J 2

m=1 ImJm.

4.2 Parameter estimation

Ωmat(C)

 

Given a sequence of observations X1  . . .   XN   we wish to ﬁt the MLDS model by estimating
θ = (U0  Q0  Q  A  R  C). Because the MLDS model contains latent variables Zn  we cannot di-
rectly maximize the likelihood of the data with respect to θ. The EM algorithm circumvents this
difﬁculty by iteratively updating (E(Z1)  . . .   E(ZN )) and θ in an alternating manner until the ex-
pected  complete likelihood of the data converges [5]. The normal distribution of tensors (3) will
facilitate matrix and vector computations rather than compel us to work directly with tensors. In
particular  we can express the complete likelihood of the MLDS model as

where Ω = mat( ˆR)−1  Ψ = (cid:80)N

l(v) =tr

L (θ | Z1  X1  . . .   ZN   XN ) = L (vec θ | vec Z1  vec X1  . . .   vec ZN   vec XN )  

n=1 E(vec Znvec ZT
n=1 vec (Xn)E(vec Zn)T. Now
and let ∆ij ∈ RIm×Jm be the indicator matrix that is one at the

(10)
where vec θ = (vec U0  mat Q0  mat Q  mat A  mat R  mat C). It follows that the vectorized MLDS
is an LDS that inherits the Kalman ﬁlter updates for the E-step and the M-step for all parameters
except mat A and mat C. See [6] for the EM algorithm of the LDS.
Because A and C are factorizable  an alternative to the standard LDS updates is required. We
locally maximize the expected  complete log-likelihood by computing the gradient with respect to
the vector v = [vec C (1)T ··· vec C (M )T]T ∈ R
m ImJm  which is obtained by concatenating the
vectorizations of the projection matrices C (m). The expected  complete log-likelihood (with terms
constant with respect to C deleted) can be written as

(cid:80)
Ψmat(C)T − 2ΦT(cid:105)(cid:111)
(cid:104)
n)  and Φ = (cid:80)N
(cid:80)
let k correspond to some C (m)
(cid:104)
Ψmat(C)T − ΦT(cid:105)(cid:111)
(i  j)th entry and zero elsewhere. The gradient ∇l(v) ∈ R
(cid:27)
C (M−1) ⊗ ··· ⊗ C (1)(cid:105)T
n(cid:54)=M Jn) shifted by(cid:81)

of ∂vkmat(C) by computing the trace of the product of two submatrices each with(cid:81)
and(cid:81)
where Λij is the submatrix of Ω [mat(C) Ψ − Φ] with row indices (1  . . .  (cid:81)
(cid:81)
n(cid:54)=M In(i − 1) and column indices (1  . . .  (cid:81)

(12)
where ∂vkmat(C) = C (M ) ⊗···⊗ ∆ij ⊗···⊗ C (1) [1]. If m = M  then we can exploit the sparsity
n(cid:54)=M In rows

n(cid:54)=M In) shifted by
n(cid:54)=M Jn(j − 1). If m (cid:54)= M 
then the ordering of the modes can be replaced by 1  . . .   m− 1  m + 1  . . .   M  m and the rows and
columns of Ω [mat(C) Ψ − Φ] can be permuted accordingly. In other words  the original tensors Xn
are “rotated” so that the mth mode becomes the M th mode.
The M-step for A can be computed in a manner analogous to that of C by replacing I by J  replacing
−1  Ψ =
mat(C) by mat(A)  and substituting v = [vec(A(1))T ··· vec(A(M ))T]T  Ω = mat(Q)

m ImJm is given elementwise by

n(cid:54)=M Jn columns:

∇l(v)k = 2tr

∇l(v)k = 2tr

Ω∂vkmat(C)

(cid:80)N−1

n=1 E

(cid:104)

vec(Zn) vec(Zn)T(cid:105)

  and Φ =(cid:80)N−1

n=1 E

(cid:104)
vec(Zn+1) vec(Zn)T(cid:105)

(cid:26)(cid:104)

into (11).

(cid:110)

(cid:110)

Λij

 

(13)

(11)

ij

 

5

......X1Z1Z2X2XnZnXn+1Zn+1XNZN4.3 Special cases of the MLDS and their relationships to existing models

m=1 Im and q =(cid:81)M
(cid:81)M

It is clear that the MLDS is exactly an LDS in the case M = 1. Certain constraints on the MLDS
also lead to generalizations of factor analysis  probabilistic principal components analysis (PPCA) 
the CP decomposition  and the matrix factorization model of collaborative ﬁltering (MF). Let p =
m=1 Jm. If A = 0  U0 = 0  and Q0 = Q  then the Xn of the MLDS become
independent and identically distributed draws from the multilinear Gaussian distribution. Setting
mat(Q) = Idq and mat(R) to a diagonal matrix results in a model that reduces to factor analysis
in the case M = 1. A further constraint on R  mat(R) = ρ2Idp  yields a multilinear extension of
PPCA. Removing the constraints on R and forcing mat(Zn) = Idq for all n results in a probabilistic
CP decomposition in which the tensor elements have general covariances. Finally  the constraint
M = 2 yields a probabilistic MF.

5 Experimental results

To determine how well the MLDS could model tensor time series  the ﬁts of the MLDS were com-
pared to those of the LDS for both synthetic and real data. To avoid unnecessary complexity and
highlight the difference between the two models—namely  how the transition and projection oper-
ators are deﬁned—the noises in the models are isotropic. The MLDS parameters are initialized so
that U0 is drawn from the standard normal distribution  the matricizations of the covariance ten-
sors are identity matrices  and the columns of each A(m) and C (m) are the ﬁrst Jm eigenvectors of
singular-value-decomposed matrices with entries drawn from the standard normal distribution. The
LDS parameters are initialized in the same way by setting M = 1.
The prediction error and convergence in likelihood were measured for each dataset. For the
synthetic dataset  model complexity was also measured. The prediction error M
n of a given
model M for the nth member of a tensor time series X1  . . .   XN is the relative Euclidean dis-
n =
vec−1
of the last member of the training sequence. The convergence in likelihood of each model is deter-
mined by monitoring the marginal likelihood as the number of EM iterations increases. Each model
is allowed to run until the difference between consecutive log-likelihood values is less than 0.1%
of the latter value. Lastly  the model complexity is determined by observing how the likelihood
and prediction error of each model vary as the model size |θM| increases. Aside from the model
complexity experiment  the LDS latent dimensionality is always set to the smallest value such that
the number of parameters of the LDS is greater than or equal to that of the MLDS.

(cid:12)(cid:12)(cid:12)(cid:12) /||Xn||  where ||·|| = ||vec(·)||2. Each estimate XM

(cid:0)mat(cid:0)CM(cid:1) mat(cid:0)AM(cid:1)n vec(cid:0)E(cid:2)ZM

(cid:3) is the estimate of the latent state

n is given by XM

tance (cid:12)(cid:12)(cid:12)(cid:12)Xn − XM

n

I

(cid:3)(cid:1)(cid:1)  where E(cid:2)ZM

Ntrain

Ntrain

5.1 Results for synthetic data

The synthetic dataset is an MLDS with dimensions I = (7  11)  J = (3  5)  and N = 1100 and
parameters initialized as described in the ﬁrst paragraph of this section. For the prediction error and
convergence analyses  the latent dimensionality of the MLDS for ﬁtting was set to J = (3  5) as
well. Each model was trained on the ﬁrst 1000 elements and tested on the last 100 elements of the
sequence. The results are shown in Figure 3. According to Figure 3(a)  the prediction error of MLDS
matches that of the true model and is below that of the LDS. Furthermore  the MLDS converges to
the likelihood of the true model  which is greater than that of the LDS (see Figure 3(b)). As for
model complexity  the model size needed for the MLDS to match the likelihood and prediction error
of the true model is much smaller than that of the LDS (see Figure 3(c) and (d)).

5.2 Results for real data

We consider the following datasets:
SST: A 5-by-6 grid of sea-surface temperatures from 5◦N  180◦W to 5◦S  110◦W recorded hourly
from 7:00PM on 4/26/94 to 3:00AM on 7/19/94  yielding 2000 epochs [7].
Tesla: Opening  closing  high  low  and volume of the stock prices of 12 car and oil companies
(e.g.  Tesla Motors Inc.)  from 6/29/10 to 5/10/13 (724 epochs).
NASDAQ-100: Opening  closing  adjusted-closing  high  low  and volume for 20 randomly-
chosen NASDAQ-100 companies  from 1/1/05 to 12/31/09 (1259 epochs).

6

(a)

(b)

function of model size is shown in (c)  and cumulative prediction error(cid:80)Ntrain+Ntest

Figure 3: Results for synthetic data. Prediction error M
the time slice n in (a)  convergence of marginal log-likelihood is shown in (b)  marginal log-likelihood as a
M
n as a function of model

n

n=Ntrain+1

n =(cid:12)(cid:12)(cid:12)(cid:12)Xn − XM

(c)

(cid:12)(cid:12)(cid:12)(cid:12) /||Xn|| is shown as a function of

(d)

size is shown in (d) for LDS  MLDS  and the true model.

(a) SST

(b) Tesla

(c) NASDAQ-100

(d) Video

(e) SST

(f) Tesla

(g) NASDAQ-100

(h) Video

Figure 4: Results for LDS and MLDS applied to real data. The ﬁrst row corresponds to prediction error M
n
as a function of the time slice n  while the second corresponds to convergence in log-likelihood. Sea-surface
temperature  Tesla  NASDAQ-100  and Video results are given by the respective columns.
Video: 1171 grayscale frames of ocean surf during low tide. This dataset was chosen because it
records a quasiperiodic natural scene.
For each dataset  MLDS achieved higher prediction accuracy and likelihood than LDS. For the SST
dataset  each model was trained on the ﬁrst 1800 epochs; occlusions were ﬁlled in using linear
interpolation and reﬁned with an extra step during the learning that replaced the estimates of the
occluded values by the conditional expectation given all the training data. For results when the
MLDS dimensionality is set to (3  3)  see Figure 4(a) and (e). For the Tesla dataset  each time series
((X1)ij  . . .   (XN )ij) were normalized prior to learning by subtracting by the mean and dividing by
the standard deviation. Each model was trained on the ﬁrst 700 epochs. See Figure 4(b) and (f) for
results when the MLDS dimensionality is set to (5  2). For the NASDAQ-100 dataset  each model
was trained on the ﬁrst 1200 epochs. The data were normalized in the same way as with the Tesla
dataset. For results when the MLDS dimensionality is set to (10  3)  see Figure 4(c) and (g). For the
Video dataset  a 100-by-100 patch was selected  spatially downsampled to a 10-by-10 patch for each
frame  and normalized as before. Each model was trained on the ﬁrst 1000 frames. See Figure 4(d)
and (h) for results when the MLDS dimensionality is set to (5  5).

6 Related work

Several existing models can be ﬁtted to tensor time series. If each tensor is “vectorized”  i.e.  reex-
pressed as a vector so that each element is indexed by a single positive integer  then an LDS can be
applied [8  6]. An obvious limitation of the LDS for modeling tensor time series is that the tensor
structure is not preserved. Thus  it is less clear how the latent vector space of the LDS relates to the
various tensor modes. Further  one cannot postulate a latent dimension for each mode as with the
MLDS. The net result  as we have shown  is that the LDS requires more parameters than the MLDS
to model a given system (assuming it does have tensor structure).

7

10201060110000.51Time sliceError  LDSMLDStrue5101520−4−20x 106Number of EM iterationsLog−likelihood  LDSMLDStrue010002000−3−2−1x 105Number of parametersLog−likelihood  LDSMLDStrue010002000050100Number of parametersCumulative error  LDSMLDStrue18501900195020000204060Time sliceError  LDSMLDS7057107157200.20.40.60.81Time sliceError  LDSMLDS1210123012500.20.40.60.81Time sliceError  LDSMLDS105011001150050100150Time sliceError  LDSMLDS510152025−8−6−4x 104Number of EM iterationsLog−likelihood  LDSMLDS10203040−8−6−4−2x 104Number of EM iterationsLog−likelihood  LDSMLDS204060−3−2−10x 105Number of EM iterationsLog−likelihood  LDSMLDS204060−1.5−1−0.5x 105Number of EM iterationsLog−likelihood  LDSMLDSDynamic tensor analysis (DTA) and Bayesian probabilistic tensor factorization (BPTF) are explicit
models of tensor time series [9  10]. For DTA  a latent  low-dimensional “core” tensor and a set of
projection matrices are learned by processing each member Xn ∈ RI1×···×IM of the sequence as
k(cid:54)=m Ik)×Im and then
follows. For each mode m  the tensor is ﬂattened into a matrix X(m)
multiplied by its transpose. The result X(m)T
is added to a matrix S(m) that has accumulated
the ﬂattenings of the previous n − 1 tensors. The eigenvalue decomposition U ΛU T of the updated

S(m) is then computed and the mth projection matrix is given by the ﬁrst rank(cid:0)S(m)(cid:1) columns of

n ∈ R((cid:81)

n X(m)

n

U. After this procedure is carried out for each mode  the core tensor is updated via the multilinear
transformation given by the Tucker decomposition. Like the LDS  DTA is a sequential model. An
advantage of DTA over the LDS is that the tensor structure of the data is preserved. A disadvantage
is that there is no straightforward way to predict future terms of the tensor time series. Another
disadvantage is that there is no mechanism that allows for arbitrary noise relationships among the
tensor elements. In other words  the noise in the system is assumed to be isotropic.
Other families of isotropic models have been devised that “tensorize” the time dimension by con-
catenating the tensors in the time series to yield a single new tensor with an additional temporal
mode. These models include multilinear principal components analysis [11]  the memory-efﬁcient
Tucker algorithm [12]  and Bayesian tensor analysis [13]. For ﬁtting to data  such models rely on
alternating optimization methods  such as alternating least squares  which are applied to each mode.
BPTF allows for prediction and more general noise modeling than DTA. BPTF is a mul-
tilinear extension of collaborative ﬁltering models [14  15  16] that concatenates the mem-
bers of the tensor time series (Xn)  Xn ∈ RI1×···×IM   to yield a higher-order tensor R ∈
RI1×···×IM×K  where K is the sequence length. Each element of R is independently distributed
  Tk(cid:105)  α−1)  where (cid:104)·  . . .  ·(cid:105) denotes the tensor inner product
as Ri1···iM k ∼ N ((cid:104)u(1)
and α is a global precision parameter. Bayesian methods are then used to compute the canonical-
◦Tr  where ◦ is
the tensor outer product. Each u(m)
is independently drawn from a normal distribution with expec-
tation µm and precision matrix Λm  while each Tr is recursively drawn from a normal distribution
with expectation Tr−1 and precision matrix ΛT . The parameters  in turn  have conjugate prior distri-
butions whose posterior distributions are sampled via Markov-chain Monte Carlo for model ﬁtting.
Though BPTF supports prediction and general noise models  the latent tensor structure is limited.
Other models with anisotropic noise include probabilistic tensor factorization (PTF) [17]  tensor
probabilistic independent component analysis (TPICA) [18]  and generalized coupled tensor factor-
ization (GCTF) [19]. As with BPTF  PTF and TPICA utilize the CP decomposition of tensors. PTF
is ﬁt to tensor data by minimizing a heuristic loss function that is expressed as a sum of tensor inner
products. TPICA iteratively ﬂattens the tensor of data  executes a matrix model called probabilistic
ICA (PICA) as a subroutine  and decouples the factor matrices of the CP decomposition that are em-
bedded in the “mixing matrix” of PICA. GCTF relates a collection of tensors by a hidden layer of
disconnected tensors via tensor inner products  drawing analogies to probabilistic graphical models.

decomposition/parallel-factors (CP) decomposition of R: R =(cid:80)R

r ◦···◦u(M )

  . . .   u(M )
iM

r=1 u(1)

i1

r

r

7 Conclusion

We have proposed a novel probabilistic model of tensor time series called the multilinear dynamical
system (MLDS)  based on a tensor normal distribution. By putting tensors and multilinear operators
in bijective correspondence with vectors and matrices in a way that preserves tensor structure  the
MLDS is formulated so that it becomes an LDS when its components are vectorized and matricized.
In matrix form  the transition and projection tensors can each be written as the Kronecker product of
M smaller matrices and thus yield an exponential reduction in model complexity compared to the
unfactorized transition and projection matrices of the LDS. As noted in Section 4.3  the MLDS gen-
eralizes the LDS  factor analysis  PPCA  the CP decomposition  and low-rank matrix factorization.
The results of multiple experiments that assess prediction accuracy  convergence in likelihood  and
model complexity suggest that the MLDS achieves a better ﬁt than the LDS on both synthetic and
real datasets  given that the LDS has the same number of parameters as the MLDS.

8

References
[1] Jan R. Magnus and Heinz Neudecker. Matrix Differential Calculus with Applications in Statistics and

Econometrics. Wiley  revised edition  1999.

[2] Vin De Silva and Lek-Heng Lim. Tensor rank and the ill-posedness of the best low-rank approximation

problem. SIAM Journal on Matrix Analysis and Applications  30(3):1084–1127  2008.

[3] Tamara G. Kolda. Tensor decompositions and applications. SIAM Review  51(3):455–500  2009.
[4] Peter J. Basser and Sinisa Pajevic. A normal distribution for tensor-valued random variables: applications

to diffusion tensor MRI. IEEE Transactions on Medical Imaging  22(7):785–794  2003.

[5] Arthur P. Dempster  Nan M. Laird  and Donald B. Rubin. Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological)  39(1):1–38  1977.

[6] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer  1st edition  2006.
[7] NOAA/Paciﬁc Marine Environmental Laboratory. Tropical Atmosphere Ocean Project. http://www.

pmel.noaa.gov/tao/data_deliv/deliv.html. Accessed: May 23  2013.

[8] Zoubin Ghahramani and Geoffrey E. Hinton. Parameter estimation for linear dynamical systems. Tech-

nical Report CRG-TR-96-2  University of Toronto Department of Computer Science  1996.

[9] Jimeng Sun  Dacheng Tao  and Christos Faloutsos. Beyond streams and graphs: dynamic tensor analysis.
In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  pages 374–383. ACM  2006.

[10] Liang Xiong  Xi Chen  Tzu-Kuo Huang  Jeff Schneider  and Jaime G. Carbonell. Temporal collaborative

ﬁltering with Bayesian probabilistic tensor factorization. In Proceedings of SIAM Data Mining  2010.

[11] Haipin Lu  Konstantinos N. Plataniotis  and Anastasios N. Venetsanopoulos. MPCA: Multilinear principal

components analysis of tensor objects. IEEE Transactions on Neural Networks  19(1)  2008.

[12] Tamara Kolda and Jimeng Sun. Scalable tensor decompositions for multi-aspect data mining. In Eighth

IEEE International Conference on Data Mining. IEEE  2008.

[13] Dacheng Tao  Mingli Song  Xuelong Li  Jialie Shen  Jimeng Sun  Xindong Wu  Christos Faloutsos  and
Stephen J. Maybank. Bayesian tensor approach for 3-D face modeling. IEEE Transactions on Circuits
and Systems for Video Technology  18(10):1397–1410  2008.

[14] Yehuda Koren  Robert Bell  and Chris Volinsky. Matrix factorization techniques for recommender sys-

tems. Computer  42(8):30–37  2009.

[15] Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In Advances in Neural Infor-

mation Processing Systems  volume 20  pages 1257–1264  2008.

[16] Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using Markov chain

Monte Carlo. In Proceedings of the 25th International Conference on Machine Learning. ACM  2008.

[17] Cyril Goutte and Massih-Reza Amini. Probabilistic tensor factorization and model selection. In Tensors 

Kernels  and Machine Learning (TKLM 2010)  pages 1–4  2010.

[18] Christian F. Beckmann and Stephen M. Smith. Tensorial extensions of independent component analysis

for multisubject FMRI analysis. Neuroimage  25(1):294–311  2005.

[19] Y. Kenan Yilmaz  A. Taylan Cemgil  and Umut Simsekli. Generalized coupled tensor factorization. In

Neural Information Processing Systems. MIT Press  2011.

9

,Mark Rogers
Lei Li
Stuart Russell
Yuanjun Gao
Lars Busing
Krishna Shenoy
John Cunningham
Zhilin Yang
Ye Yuan
Yuexin Wu
William Cohen
Russ Salakhutdinov