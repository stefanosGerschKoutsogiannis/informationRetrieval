2019,Total Least Squares Regression in Input Sparsity Time,In the total least squares problem  one is given an $m \times n$ matrix $A$  and an $m \times d$ matrix $B$  and one seeks to ``correct'' both $A$ and $B$  obtaining matrices $\hat{A}$ and $\hat{B}$  so that there exists an $X$ satisfying the equation $\hat{A}X = \hat{B}$. Typically the problem is overconstrained  meaning that $m \gg \max(n d)$. The cost of the solution $\hat{A}  \hat{B}$ is given by $\|A-\hat{A}\|_F^2 + \|B - \hat{B}\|_F^2$. We give an algorithm for finding a solution $X$ to the linear system $\hat{A}X=\hat{B}$ for which the cost $\|A-\hat{A}\|_F^2 + \|B-\hat{B}\|_F^2$ is at most a multiplicative $(1+\epsilon)$ factor times the optimal cost  up to an additive error $\eta$ that may be an arbitrarily small function of $n$. Importantly  our running time is $\tilde{O}(\nnz(A) + \nnz(B)) + \poly(n/\epsilon) \cdot d$  where for a matrix $C$  $\nnz(C)$ denotes its number of non-zero entries. Importantly  our running time does not directly depend on the large parameter $m$. As total least squares regression is known to be solvable via low rank approximation  a natural approach is to invoke fast algorithms for approximate low rank approximation  obtaining matrices $\hat{A}$ and $\hat{B}$ from this low rank approximation  and then solving for $X$ so that $\hat{A}X = \hat{B}$. However  existing algorithms do not apply since in total least squares the rank of the low rank approximation needs to be $n$  and so the running time of known methods would be at least $mn^2$. In contrast  we are able to achieve a much faster running time for finding $X$ by never explicitly forming the equation $\hat{A} X = \hat{B}$  but instead solving for an $X$ which is a solution to an implicit such equation.
Finally  we generalize our algorithm to the total least squares problem with regularization.,TotalLeastSquaresRegressioninInputSparsityTimeHuaianDiaoNortheastNormalUniversity&KLASofMOEhadiao@nenu.edu.cnZhaoSongUniversityofWashingtonzhaosong@uw.eduDavidP.WoodruffCarnegieMellonUniversitydwoodruf@cs.cmu.eduXinYangUniversityofWashingtonyx1992@cs.washington.eduAbstractInthetotalleastsquaresproblem oneisgivenanm×nmatrixA andanm×dmatrixB andoneseeksto“correct”bothAandB obtainingmatricesbAandbB sothatthereexistsanXsatisfyingtheequationbAX=bB.Typicallytheproblemisoverconstrained meaningthatm(cid:29)max(n d).ThecostofthesolutionbA bBisgivenbykA−bAk2F+kB−bBk2F.WegiveanalgorithmforﬁndingasolutionXtothelinearsystembAX=bBforwhichthecostkA−bAk2F+kB−bBk2Fisatmostamultiplicative(1+)factortimestheoptimalcost uptoanadditiveerrorηthatmaybeanarbitrarilysmallfunctionofn.Importantly ourrunningtimeiseO(nnz(A)+nnz(B))+poly(n/)·d whereforamatrixC nnz(C)denotesitsnumberofnon-zeroentries.Importantly ourrunningtimedoesnotdirectlydependonthelargeparameterm.Astotalleastsquaresregressionisknowntobesolvablevialowrankapproximation anaturalapproachistoinvokefastalgorithmsforapproximatelowrankapproximation obtainingmatricesbAandbBfromthislowrankapproximation andthensolvingforXsothatbAX=bB.However existingalgorithmsdonotapplysinceintotalleastsquarestherankofthelowrankapproximationneedstoben andsotherunningtimeofknownmethodswouldbeatleastmn2.Incontrast weareabletoachieveamuchfasterrunningtimeforﬁndingXbyneverexplicitlyformingtheequationbAX=bB butinsteadsolvingforanXwhichisasolutiontoanimplicitsuchequation.Finally wegeneralizeouralgorithmtothetotalleastsquaresproblemwithregularization.33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.1IntroductionIntheleastsquaresregressionproblem wearegivenanm×nmatrixAandanm×1vectorb andweseektoﬁndanx∈RnwhichminimizeskAx−bk22.AnaturalgeometricinterpretationisthatthereisanunknownhyperplaneinRn+1 speciﬁedbythenormalvectorx forwhichwehavempointsonthishyperplane thei-thofwhichisgivenby(Ai hAi xi) whereAiisthei-throwofA.However duetonoisyobservations wedonotseethepoints(Ai hAi xi) butratheronlyseethepoint(Ai bi) andweseektoﬁndthehyperplanewhichbestﬁtsthesepoints wherewemeasure(squared)distanceonlyonthe(n+1)-stcoordinate.ThisnaturallygeneralizestothesettinginwhichBisanm×dmatrix andthetruepointshavetheform(Ai AiX)forsomeunknownn×dmatrixX.Thissettingiscalledmultiple-responseregression inwhichoneseekstoﬁndXtominimizekAX−Bk2F whereforamatrixY kYk2FisitssquaredFrobeniusnorm i.e. thesumofsquaresofeachofitsentries.Thisgeometricallycorrespondstothesettingwhenthepointsliveinalowern-dimensionalﬂatofRn+d ratherthaninahyperplane.Whileextremelyuseful insomesettingstheaboveregressionmodelmaynotbeentirelyrealistic.Forexample itisquitenaturalthatthematrixAmayalsohavebeencorruptedbymeasurementnoise.Inthiscase oneshouldalsobeallowedtoﬁrstchangeentriesofA obtaininganewm×nmatrixbA thentrytoﬁtBtobAbysolvingamultiple-responseregressionproblem.OneshouldagainbepenalizedforhowmuchonechangestheentriesofA andthisleadstoapopularformulationknownasthetotalleastsquaresoptimizationproblemminbA XkA−bAk2F+kbAX−Bk2F.LettingC=[A B] onecanmorecompactlywritethisobjectiveasminbC=[bA bB]kC−bCk2F whereitisrequiredthatthecolumnsofbBareinthecolumnspanofbA.Totalleastsquarescannaturallycapturemanyscenariosthatleastsquarescannot.Forexample imagineacolumnofBisalargemultipleλ·aofacolumnaofAthathasbeencorruptedandsentto0.Theninleastsquares oneneedstopayλ2kak22 butintotalleastsquaresonecan“repairA"tocontainthecolumna andjustpaykak22.Wereferthereaderto[MVH07]foranoverviewoftotalleastsquares.Thereisalsoalargeamountofworkontotalleastsquareswithregularization[RG04 LPT09 LV14].NoticethatbChasrankn andthereforetheoptimalcostisatleastkC−Cnk2F whereCnisthebestrank-napproximationtoC.If intheoptimalrank-napproximationCn onehasthepropertythatthelastdcolumnsareinthecolumnspanoftheﬁrstncolumns thentheoptimalsolutionbCtototalleastsquaresproblemisequaltoCn andsothetotalleastsquarescostisthecostofthebestrank-napproximationtoC.Inthiscase andonlyinthiscase thereisaclosed-formsolution.However ingeneral thisneednotbethecase andkC−Cnk2Fmaybestrictlysmallerthanthetotalleastsquarescost.Fortunately though itcannotbemuchsmaller sinceonecantaketheﬁrstncolumnsofCn andforeachcolumnthatisnotlinearlyindependentoftheremainingcolumns wecanreplaceitwithanarbitrarilysmallmultipleofoneofthelastdcolumnsofCnwhichisnotinthespanoftheﬁrstncolumnsofCn.Iteratingthisprocedure weﬁndthatthereisasolutiontothetotalleastsquaresproblemwhichhascostwhichisarbitrarilyclosetokC−Cnk2F.Wedescribethisprocedureinmoredetailbelow.Theaboveprocedureofconvertingabestrank-napproximationtoanarbitrarilyclosesolutiontothetotalleastsquaresproblemcanbedoneefﬁcientlygivenCn sothisshowsonecangetanarbitrar-ilygoodapproximationbycomputingatruncatedsingularvaluedecompostion(SVD) whichisastandardwayofsolvingforCninO(m(n+d)2)time.However giventheexplosionoflarge-scaledatasetsthesedays thisrunningtimeisoftenprohibitive evenforthesimplerproblemofmultipleresponseleastsquaresregression.Motivatedbythis anemergingbodyofliteraturehaslookedatthesketch-and-solveparadigm whereonesettlesforrandomizedapproximationalgorithmswhichruninmuchfaster ofteninputsparsitytime.Herebyinput-sparsity wemeanintimelinearinthenumbernnz(C)ofnon-zeroentriesoftheinputdescriptionC=[A B].Bynow itisknown forexample howtooutputasolutionmatrixXtomultipleresponseleastsquaresregressionsatisfyingkAX−Bk2F≤(1+)minX0kAX0−Bk2F innnz(A)+nnz(B)+poly(nd/)time.ThisalgorithmworksforarbitraryinputmatricesAandB andsucceedswithhighprobabilityoverthealgorithm’srandomcointosses.Forasurveyofthisandrelatedresults wereferthereaderto[Woo14].Giventheabovecharacterizationoftotalleastsquaresasalowrankapproximationproblem itisnaturaltoaskifonecandirectlyapplysketch-and-solvetechniquestosolveit.Indeed forlowrankapproximation itisknownhowtoﬁndarank-kmatrixbCforwhichkC−bCk2F≤(1+)kC−Ckk2F1intimennz(C)+m·k2/[ACW17] usingthefastestknownresults.Here recall weassumem≥n+d.Fromanapproximationpointofview thisisﬁneforthetotalleastsquaresproblem sincethismeansafterapplyingtheprocedureabovetoensurethelastdcolumnsofbCareinthespanoftheﬁrstncolumns andsettingk=ninthelowrankapproximationproblem ourcostwillbeatmost(1+)kC−Cnk2F+η whereηcanbemadeanarbitrarilysmallfunctionofn.Moreover theoptimaltotalleastsquarescostisatleastkC−Cnk2F soourcostisa(1+)-relativeerrorapproximation uptoanarbitarilysmalladditiveη.Unfortunately thisapproachisinsufﬁcientfortotalleastsquares becauseinthetotalleastsquaresproblemonesetsk=n andsotherunningtimeforapproximatelowrankapproximationbecomesnnz(C)+m·n2/.Sincenneednotbethatsmall them·n2termispotentiallyprohibitivelylarge.Indeed ifd≤n thismaybemuchlargerthanthedescriptionoftheinput whichrequiresatmostmnparameters.NotethatjustoutputtingbCmaytakem·(n+d)parameterstodescribe.However asinthecaseofregression oneisoftenjustinterestedinthematrixXorthehyperplanexforordinaryleastsquaresregression.HerethematrixXfortotalleastsquarescanbedescribedusingonlyndparameters andsoonecouldhopeforamuchfasterrunningtime.1.1OurContributionsOurmaincontributionistodevelopa(1+)-approximationtothetotalleastsquaresregressionproblem returningamatrixX∈Rn×dforwhichthereexistbA∈Rm×nandbB∈Rm×dforwhichbAX=bBandkC−bCk2F≤(1+)kC−Cnk2F+η whereC=[A B] bC=[bA bB] Cnisthebestrank-napproximationtoC andηisanarbitrarilysmallfunctionofn.Importantly weachievearunningtimeofeO(nnz(A)+nnz(B))+poly(n/)·d.NoticethatthisrunningtimemaybefasterthanthetimeittakeseventowritedownbAandbB.Indeed althoughonecanwriteAandBdowninnnz(A)+nnz(B)time itcouldbethatthealgorithmcanonlyefﬁcientlyﬁndanbAandabBthataredense;neverthelessthealgorithmdoesnotneedtowritesuchmatricesdown asitisonlyinterestedinoutputtingthesolutionXtotheequationbAX=bB.Thisismotivatedbyapplicationsinwhichonewantsgeneralizationerror.GivenX andafuturey∈Rn onecancomputeyXtopredicttheremainingunknowndcoordinatesoftheextensionofyton+ddimensions.Ouralgorithmisinspiredbyusingdimensionalityreductiontechniquesforlowrankapproxi-mation suchasfastoblivious“sketching”matrices aswellasleveragescoresampling.Theroughideaistoquicklyreducethelowrankapproximationproblemtoaproblemoftheformminrank-nZ∈Rd1×s1k(D2CD1)Z(S1C)−D2CkF whered1 s1=O(n/) D2andD1arerowandcolumnsubsetselectionmatrices andS1isaso-calledCountSketchmatrix whichisafastobliviousprojectionmatrix.WedescribethematricesD1 D2 andS1inmoredetailinthenextsection thoughthekeytakeawaymessageisthat(D2CD1) (S1C) and(D2C)areeachefﬁcientlycomputablesmallmatriceswithanumberofnon-zeroentriesnolargerthanthatofC.Nowtheproblemisasmall rank-constrainedregressionproblemforwhichthereareclosedformsolutionsforZ.Wethenneedadditionaltechnicalwork oftheformdescribedabove inordertoﬁndanX∈Rn×dgivenZ andtoensurethatXisthesolutiontoanequationoftheformbAX=bB.Surprisingly fastsketchingmethodshavenotbeenappliedtothetotalleastsquaresproblembefore andweconsiderthisapplicationtobeoneofthemaincontributionsofthispaper.WecarefullyboundtherunningtimeateachsteptoachieveeO(nnz(A)+nnz(B)+poly(n/)d)overalltime andproveitsoverallapproximationratio.OurmainresultisTheorem3.10.Wealsogeneralizethetheoremtotheimportantcaseoftotalleastsquaresregressionwithregularization;seeTheorem3.12foraprecisestatement.Weempiricallyvalidateouralgorithmonrealandsyntheticdatasets.Asexpected onanumberofdatasetsthetotalleastsquareserrorcanbemuchsmallerthantheerrorofordinaryleastsquaresregression.Wethenimplementourfasttotalleastsquaresalgorithm andshowitisroughly20−40timesfasterthancomputingtheexactsolutiontototalleastsquares whileretaining95%accuracy.Notation.Forafunctionf wedeﬁneeO(f)tobef·logO(1)(f).Forvectorsx y∈Rn lethx yi:=Pni=1xiyidenotetheinnerproductofxandy.Letnnz(A)denotethenumberofnonzeroentriesofA.Letdet(A)denotethedeterminantofasquarematrixA.LetA>denotethetransposeofA.Let2A†denotetheMoore-PenrosepseudoinverseofA.LetA−1denotetheinverseofafullranksquarematrix.LetkAkFdenotetheFrobeniusnormofamatrixA i.e. kAkF=(PiPjA2i j)1/2.Sketchingmatricesplayanimportantroleinouralgorithm.Theirusefulnesswillbefurtherex-plainedinSection3.ThereadercanrefertoAppendixBfordetailedintroduction.2ProblemFormulationWeﬁrstgivetheprecisedeﬁnitionoftheexact(i.e. non-approximate)versionofthetotalleastsquaresproblem andthendeﬁnetheapproximatecase.Deﬁnition2.1(Exacttotalleastsquares).GiventwomatricesA∈Rm×n B∈Rm×d letC=[A B]∈Rm×(n+d).Thegoalistosolvethefollowingminimizationproblem:minX∈Rn×d ∆A∈Rm×n ∆B∈Rm×dk[∆A ∆B]kF(1)subjectto(A+∆A)X=(B+∆B)Itisknownthattotalleastsquaresproblemhasaclosedformsolution.Foradetaileddiscussion seeAppendixE.Itisnaturaltoconsidertheapproximateversionoftotalleastsquares:Deﬁnition2.2(Approximatetotalleastsquaresproblem).GiventwomatricesA∈Rm×nandB∈Rm×d letOPT=minrank−nC0kC0−[A B]kF forparameters>0 δ>0.ThegoalistooutputX0∈Rn×dsothatthereexistsA0∈Rm×nsuchthatk[A0 A0X0]−[A B]kF≤(1+)OPT+δ.Onecouldsolvetotalleastsquaresdirectly butitismuchslowerthansolvingleastsquares(LS).Wewillusefastrandomizedalgorithms thebasisofwhicharesamplingandsketchingideas[CW13 NN13 MM13 Woo14 RSW16 PSW17 SWZ17 CCLY19 CLS19 LSZ19 SWY+19 SWZ19a SWZ19b SWZ19c DJS+19] tospeedupsolvingtotalleastsquaresinboththeoryandinpractice.Thetotalleastsquaresproblemwithregularizationisalsoanimportantvariantofthisprob-lem[LV10].Weconsiderthefollowingversionoftheregularizedtotalleastsquaresproblem.Deﬁnition2.3(Approximateregularizedtotalleastsquaresproblem).GiventwomatricesA∈Rm×nandB∈Rm×dandλ>0 letOPT=minU∈Rm×n V∈Rn×(n+d)kUV−[A B]k2F+λkUk2F+λkVk2F forparameters>0 δ>0.ThegoalistooutputX0∈Rn×dsothatthereexistA0∈Rm×n U0∈Rm×nandV0∈Rn×(n+d)satisfyingk[A0 A0X0]−U0V0k2F≤δandk[A0 A0X0]−[A B]k2F≤(1+)OPT+δ.Table1:NotationsinAlgorithm1Not.ValueCommentMatrixDim.Comments1O(n/)#rowsinS1S1Rs1×mCountSketchmatrixd1eO(n/)#columnsinD1D1Rn×d1Leveragescoresamplingmatrixd2eO(n/)#rowsinD2D2Rd2×mLeveragescoresamplingmatrixs2O(n/)#rowsinS2S2Rs2×mCountSketchmatrixforfastregressionZ2Rs1×d1Lowrankapproximationsolutionmatrix3FastTotalLeastSquaresAlgorithmWepresentouralgorithminAlgorithm1andgivetheanalysishere.ReaderscanrefertoTable1tochecknotationsinAlgorithm1.Toclearlygivetheintuition wepresentasequenceofapproxima-tions reducingthesizeofourproblemstep-by-step.Wecanfocusonthecasewhend(cid:29)Ω(n/)andtheoptimalsolutionbCtoprogram(6)hastheform[bA bAbX]∈Rm×(n+d).Fortheothercasewhend=O(n/) wedonotneedtousethesamplingmatrixD1.Inthecasewhenthesolutiondoesnothavetheform[bA bAbX] weneedtoincludeSPLITinthealgorithm sinceitwillperturb3Algorithm1OurFastTotalLeastSquaresAlgorithm1:procedureFASTTOTALLEASTSQUARES(A B n d  δ).Theorem3.102:s1←O(n/) s2←O(n/) d1←eO(n/) d2←eO(n/)3:ChooseS1∈Rs1×mtobeaCountSketchmatrix thencomputeS1C.DeﬁnitionB.14:ifd>Ω(n/)then.Reducen+dtoO(n/)5:ChooseD>1∈Rd1×(n+d)tobealeveragescoresamplingandrescalingmatrixaccord-ingtotherowsof(S1C)> thencomputeCD16:else.WedonotneedtousematrixD17:ChooseD>1∈R(n+d)×(n+d)tobetheidentitymatrix8:ChooseD2∈Rd2×mtobealeveragescoresamplingandrescalingmatrixaccordingtotherowsofCD19:Z2←minrank−nZ∈Rd1×s1kD2CD1ZS1C−D2CkF.TheoremD.110:A B π←SPLIT(CD1 Z2 S1C n d δ/poly(m)) X←minkAX−BkF11:ifNeedCFTLSthen.Forexperimentstoevaluatethecost12:EVALUATE(CD1 Z2 S1C X π δ/poly(m))13:returnX14:procedureSPLIT(CD1 Z2 S1C n d δ).Lemma3.815:ChooseS2∈Rs2×mtobeaCountSketchmatrix16:C←(S2·CD1)·Z2·S1C.bC=CD1Z2S1C;C=S2bC17:A←C∗ [n] B←C∗ [n+d]\[n].bA=bC∗ [n] bB=bC∗ [n+d]\[n];A=S2bA B=S2bB18:T←∅ π(i)=−1foralli∈[n]19:fori=1→ndo20:ifA∗ iislinearlydependentofA∗ [n]\{i}then21:j←minj∈[d]\T{B∗ jislinearlyindependentofA} A∗ i←A∗ i+δ·B∗ j T←T∪{j} π(i)←j22:returnA B π.π:[n]→{−1}∪([n+d]\[n])23:procedureEVALUATE(CD1 Z2 S1C X π δ).AppendixF.924:bC←CD1Z2S1C bA←bC∗ [n] bB←C∗ [n+d]\[n]25:fori=1→ndo26:ifπ(i)6=−1then27:bA∗ i←bA∗ i+δ·bB∗ π(i)28:returnk[bA bAX]−CkFsomecolumnsinbAwitharbitrarilysmallnoisetomakesurebAhasrankn.ByapplyingprocedureSPLIT wecanhandleallcases.FixA∈Rm×nandB∈Rm×d.LetOPT=minrank−nC0∈Rm×(n+d)kC0−[A B]kF.Byusingtechniquesinlow-rankapproximation wecanﬁndanapproximationofaspecialform.Moreprecisely letS1∈Rs1×mbeaCountSketchmatrixwiths1=O(n/).ThenweclaimthatitissufﬁcienttolookatsolutionsoftheformUS1C.Claim3.1(CountSketchmatrixforlowrankapproximationproblem).Withprobability0.98 minrank−nU∈Rm×s1kUS1C−Ck2F≤(1+)2OPT2.WeprovidetheproofinAppendixF.1.Weshallmentionthatwecannotuseleveragescoresamplinghere becausetakingleveragescoresamplingonmatrixCwouldtakeatleastnnz(C)+(n+d)2time whilewearelinearindintheadditiveterminourrunningtimeeO(nnz(C))+d·poly(n/).LetU1betheoptimalsolutionoftheprogramminU∈Rm×s1kUS1C−Ck2F i.e. U1=argminrank−nU∈Rm×s1kUS1C−Ck2F.(2)Ifdislargecomparedton thenprogram(2)iscomputationallyexpensivetosolve.Sowecanapplysketchingtechniquestoreducethesizeoftheproblem.LetD>1∈Rd1×(n+d)denotealeveragescoresamplingandrescalingmatrixaccordingtothecolumnsofS1C withd1=eO(n/)4nonzeroentriesonthediagonalofD1.LetU2∈Rm×s1denotetheoptimalsolutiontotheproblemminrank−nU∈Rm×s1kUS1CD1−CD1k2F i.e. U2=argminrank−nU∈Rm×s1kUS1CD1−CD1k2F.(3)Thenthefollowingclaimcomesfromtheconstrainedlow-rankapproximationresult(TheoremD.1).Claim3.2(Solvingregressionwithleveragescoresampling).LetU1bedeﬁnedinEq.(2) andletU2bedeﬁnedinEq.(3).Thenwithprobability0.98 kU2S1C−Ck2F≤(1+)2kU1S1C−Ck2F.WeprovidetheproofinAppendixF.2.Wenowconsiderhowtosolveprogram(3).WeobservethatClaim3.3.U2∈colspan(CD1).Wecanthusconsiderthefollowingrelaxation:givenCD1 S1CandC solve:minrank−nZ∈Rd1×s1kCD1ZS1C−Ck2F.(4)BysettingCD1Z=U wecancheckthatprogram(4)isindeedarelaxationofprogram(3).LetZ1betheoptimalsolutiontoprogram(4).WeshowthefollowingclaimanddelayedtheproofinF.3.Claim3.4(Approximationratioofrelaxation).Withprobability0.98 kCD1Z1S1C−Ck2F≤(1+O())OPT2.However program(4)stillhasapotentiallylargesize i.e. weneedtoworkwithanm×d1matrixCD1.Tohandlethisproblem weagainapplysketchingtechniques.LetD2∈Rd2×mbealeveragescoresamplingandrescalingmatrixaccordingtothematrixCD1∈Rm×d1 sothatD2hasd2=eO(n/)nonzerosonthediagonal.Now wearriveatthesmallprogramthatwearegoingtodirectlysolve:minrank−nZ∈Rd1×s1kD2CD1ZS1C−D2Ck2F.(5)WeshallmentionthathereitisbeneﬁcialtoapplyleveragescoresamplingmatrixbecauseweonlyneedtocomputeleveragescoresofasmallermatrixCD1 andcomputingD2ConlyinvolvessamplingasmallfractionoftherowsofC.Ontheotherhand ifweweretousetheCountSketchmatrix thenwewouldneedtotouchthewholematrixCwhencomputingD2C.Overall usingleveragescoresamplingatthisstepcanreducetheconstantfactorofthennz(C)termintherunningtime andmaybeusefulinpractice.Letrank-nZ2∈Rd1×s1betheoptimalsolutiontothisproblem.Claim3.5(SolvingregressionwithaCountSketchmatrix).Withprobability0.98 kCD1Z2S1C−Ck2F≤(1+)2kCD1Z1S1C−Ck2FWeprovidetheproofinAppendixF.4.Ouralgorithmthusfarisasfollows:wecomputematricesS1 D1 D2accordingly thensolveprogram(5)toobtainZ2.Atthispoint weareabletoobtainthelowrankapproximationbC=CD1·Z2·S1C.WeshowthefollowingclaimanddelayedtheproofinAppendixF.5.Claim3.6(AnalysisofbC).Withprobability0.94 kbC−Ck2F≤(1+O())OPT2.LetbC=[bA bB]wherebA∈Rm×nandbB∈Rm×d.However ifourgoalistoonlyoutputamatrixXsothatbAX=bB thenwecandothisfasterbynotcomputingorstoringthematrixbC.LetS2∈Rs2×mbeaCountSketchmatrixwiths2=O(n/).Wesolvearegressionproblem:minX∈Rn×dkS2bAX−S2bBk2F.NoticethatS2bAandS2bBarecomputeddirectlyfromCD1 Z2 S1CandS2.LetXbetheoptimalsolutiontotheaboveproblem.5Claim3.7(Approximationratioguarantee).AssumebC=[bA bAbX]forsomebX∈Rn×d.Thenwithprobabilityatleast0.9 k[bA bAX]−[A B]k2F≤(1+O())OPT2.WeprovidetheproofinAppendixF.6.IftheassumptionbC=[bA bAbX]inClaim3.7doesnothold thenweneedtoapplyprocedureSPLIT.Becauserank(bC)=nfromourconstruction iftheﬁrstncolumnsofbCcannotspanthelastdcolumns thentheﬁrstncolumnsofbCarenotfullrank.Hencewecankeepaddingasufﬁcientlysmallmultipleofoneofthelastdcolumnsthatcannotbespannedtotheﬁrstncolumnsuntiltheﬁrstncolumnsarefullrank.Formally wehaveLemma3.8(AnalysisofprocedureSPLIT).Fixs1=O(n/) s2=O(n/) d1=eO(n/).GivenCD1∈Rm×d1 Z2∈Rd1×s1 andS1C∈Rs1×(n+d)sothatbC:=CD1·Z2·S1Chasrankn procedureSPLIT(Algorithm1)returnsA∈Rs2×nandB∈Rs2×dintimeO(nnz(C)+d·poly(n/))sothatthereexistsX∈Rn×dsatisfyingA·X=B.Moreover lettingbAbethematrixcomputedinlines(24)to(27) thenwithprobability0.99 k[bA bAX]−CkF≤kbC−CkF+δ.WeprovidetheproofinAppendixF.7.NowthatwehaveAandB andwecancomputeXbysolvingtheregressionproblemminX∈Rn×dkAX−Bk2F.Wenextsummarizetherunningtime.OmmittedproofsareinAppendixF.8.Lemma3.9(Runningtimeanalysis).ProcedureFASTTOTALLEASTSQUARESinAlgorithm1runsintimeeO(nnz(A)+nnz(B)+d·poly(n/)).Tosummarize Theorem3.10showstheperformanceofouralgorithm.OmmittedproofsareinAppendixF.10.Theorem3.10(MainResult).GiventwomatricesA∈Rm×nandB∈Rm×d lettingOPT=minrank−nC0∈Rm×(n+d)kC0−[A B]kF wehavethatforany∈(0 1) thereisanalgorithm(procedureFASTTOTALLEASTSQUARESinAlgorithm1)thatrunsineO(nnz(A)+nnz(B))+d·poly(n/)timeandoutputsamatrixX∈Rn×dsuchthatthereisamatrixbA∈Rm×nsatisfyingthatk[bA bAX]−[A B]kF≤(1+)OPT+δholdswithprobabilityatleast9/10 whereδ>0isarbitrarilysmall.Remark3.11.Thesuccessprobability9/10inTheorem3.10canbeboostedto1−δforanyδ>0inastandardway.Namely werunourFTLSalgorithmO(log(1/δ))timeswhereineachrunweuseindependentrandomness andchoosethesolutionfoundwiththesmallestcost.NotethatforanyﬁxedoutputX thecostk[¯A ¯AX]−[A B]kFcanbeefﬁcientlyapproximated.Toseethis letSbeaCountSketchmatrixwithO(−2)rows.ThenkS[¯A ¯AX]−S[A B]kF=(1±)k[¯A ¯AX]−[A B]kFwithprobability9/10(see forexampleLemma40of[CW13]).WecancomputekS[¯A ¯AX]−S[A B]kFintimeO(d·poly(n/)) andapplyingScanbedoneinnnz(A)+nnz(B)time.WecanthenamplifythesuccessprobabilitybytakingO(log(1/δ))inde-pendentestimatesandtakingthemedianoftheestimates.Thisisa(1±)-approximationwithprobabilityatleast1−O(δ/log(1/δ)).WerunourFTLSalgorithmO(log(1/δ))times obtainingoutputsX1 ... XO(log(1/δ))andforeachXi applythemethodabovetoestimateitscost.SinceforeachXiourestimatetothecostiswithin1±withprobabilityatleast1−O(δ/(log(1/δ)) byaunionboundtheestimatesforallXiarewithin1±withprobabilityatleast1−δ/2.Sincealsothesolutionwithminimalcostisa1±approximationwithprobabilityatleast1−δ/2 byaunionboundwecanachieve1−δprobabilitywithrunningtimeeO(log2(1/δ))·(nnz(A)+nnz(B)+d·poly(n/))).Wefurthergeneralizeouralgorithmtohandleregularization.OmmittedproofscanbefoundinAppendixG.6Theorem3.12(Algorithmforregularizedtotalleastsquares).GiventwomatricesA∈Rm×nandB∈Rm×dandλ>0 lettingOPT=minU∈Rm×n V∈Rn×(n+d)kUV−[A B]k2F+λkUk2F+λkVk2F wehavethatforany∈(0 1) thereisanalgorithm(procedureFASTREGULARIZEDTOTAL-LEASTSQUARESinAlgorithm3)thatrunsineO(nnz(A)+nnz(B)+d·poly(n/))timeandoutputsamatrixX∈Rn×dsuchthatthereisamatrixbA∈Rm×n bU∈Rm×nandbV∈Rn×(n+d)satisfyingk[bA bAX]−bUbVk2F≤δandwithprobability9/10 k[bA bAX]−[A B]k2F+λkbUk2F+λkbVk2F≤(1+)OPT+δ.4ExperimentsWeconductseveralexperimentstoverifytherunningtimeandoptimalityofourfasttotalleastsquaresalgorithm1.Letusﬁrstrecallthemultiple-responseregressionproblem.LetA∈Rm×nandB∈Rm×d.Inthisproblem wewanttoﬁndX∈Rn×dsothatAX∼B.Theleastsquaresmethod(LS)solvesthefollowingoptimizationprogram:cLS:=minX∈Rn×d ∆B∈Rm×dk∆Bk2F subjecttoAX=B+∆B.Ontheotherhand thetotalleastsquaresmethod(TLS)solvesthefollowingoptimizationprogram:cTLS:=minrank−nC0∈Rm×(n+d)kC0−[AB]kF.Thefasttotalleastsquaresmethod(FTLS)returnsX∈Rn×d whichprovidesanapproximationC0=[bAbAX]totheTLSsolution andthecostiscomputedascFTLS=kC0−Ck2F.OurnumericaltestsarecarriedoutonanIntelXeonE7-8850v2serverwith2.30GHzand4GBRAMunderMatlabR2017b.14.1AToyExampleWeﬁrstrunourFTLSalgorithmonthefollowingtoyexample forwhichwehavetheanalyticalsolutionexactly.LetA∈R3×2beA11=A22=1and0everywhereelse.LetB∈R3×1beB3=3and0everywhereelse.WealsoconsiderthegeneralizationofthisexamplewithlargerdimensioninAppendixH2.ThecostofLSis9 sinceAXcanonlyhavenon-zeroentriesontheﬁrst2coordinates sothe3rdcoordinateofAX−Bmusthaveabsolutevalue3.Hencethecostisatleast9.Moreover acost9canbeachievedbysettingX=0and∆B=−B.However fortheTLSalgorithm thecostisonly1.Consider∆A∈R3×2whereA11=−1and0everywhereelse.ThenC0:=[(A+∆A) B]hasrank2 andkC0−CkF=1.Weﬁrstrunexperimentsonthissmallmatrix.SinceweknowthesolutionofLSandTLSexactlyinthiscase itisconvenientforustocomparetheirresultswiththatoftheFTLSalgorithm.WhenweruntheFTLSalgorithm wesample2rowsineachofthesketchingalgorithms.TheexperimentalsolutionofLSisCLS=diag(0 1 3)whichmatchesthetheoreticalsolution.Thecostis9.TheexperimentalsolutionofTLSisCTLS=diag(1 1 0)whichalsomatchesthetheoreticalresult.Thecostis1.FTLSisarandomizedalgorithm sotheoutputvaries.Wepostseveraloutputs:CFTLS=".06−.01.25−.01.99.00.76.012.79# ".14−.26−.22−.26.91−.06−.67−.202.82#1Thecodecanbefoundathttps://github.com/yangxinuw/total_least_squares_code.2Forfullversion pleasereferto[DSWY19]702040608010000.511.522.5TLSLSFTLS 0.9FTLS 0.6FTLS 0.3FTLS 0.10204060801000.10.20.30.40.50.60.70.80.91TLSLSFTLS 0.9FTLS 0.6FTLS 0.3FTLS 0.102040608010000.511.522.5TLSLSFTLS 0.9FTLS 0.6FTLS 0.3FTLS 0.102040608010000.10.20.30.40.50.60.70.80.91TLSLSFTLS 0.9FTLS 0.6FTLS 0.3FTLS 0.1Figure1:RunningtimeandaccuracyofourFTLSalgorithms.Theleft2ﬁguresareforthesparsematrix.Theright2picturesarefortheGaussianmatrix.(Left)They-axisistherunningtimeofeachalgorithm(countedinseconds);thex-axisisthesizeofthematrix.(Right)They-axisiscost-TLS/cost-other wherecost-otheristhecostachievedbyotheralgorithms.(Notewewanttominimizethecost);thex-axisisthesizeofthematrix.MethodCostC-stdTimeT-stdTLS0.1001.120.05LS10600.00120.0002FTLS0.90.100.00020.160.0058FTLS0.60.100.00030.0810.0033FTLS0.30.100.00070.0460.0022FTLS0.10.100.00160.0340.0024MethodCostC-stdTimeT-stdTLS0.9301.360.16LS66600.00120.001FTLS0.90.930.00320.300.025FTLS0.60.940.00500.170.01FTLS0.30.950.010.0950.005FTLS0.10.990.030.0740.004MethodCostC-stdTimeT-stdTLS1.85029.441.44LS279400.00220.001FTLS0.91.8570.0013.120.081FTLS0.61.8580.0021.620.054FTLS0.31.8640.0060.770.027FTLS0.11.8850.0190.600.017MethodCostC-stdTimeT-stdTLS0.5500125.3882.9LS30300.0190.02FTLS0.90.5530.00321.3131.867FTLS0.60.5580.01113.1151.303FTLS0.30.5580.0547.4531.237FTLS0.10.7320.2274.8940.481Table2:UpLeft:AirfoilSelf-Noise.UpRight:Redwine.DownLeft:Whitewine.DownRight:InsuranceCompanyBenchmark.C-stdisthestandarddeviationforcost.T-stdisthestandarddeviationforrunningtime.Thesesolutionshavecostof1.55and1.47.WeruntheFTLSmultipletimestoanalyzethedistributionofcosts.Experimentalresult whichcanbefoundinAppendexI showsthatFTLSisastablealgorithm andconsistentlyperformsbetterthanLS.Wealsoconsiderasecondsmalltoyexample.LetAstillbea10×5matrixandBbea10×1vector.EachentryA(i j)ischoseni.i.d.fromthenormaldistributionN(0 1) andeachentryB(i)ischosenfromN(0 3).BecauseentriesfromAandBhavedifferentvariance weexpecttheresultsofLSandTLStobequitedifferent.WhenweruntheFTLSalgorithm wesample6rows.WerunFTLS1000times andcomputethedistributionofcosts.Theresultsofthisexperiment whichisinAppendexI againdemonstratesthestabilityofthealgorithm.4.2LargeScaleProblemsWehavealreadyseenthatFTLSworksprettywellonsmallmatrices.Wenextshowthatthefasttotalleastsquaresmethodalsoprovidesagoodestimateforlargescaleregressionproblems.Thesettingformatricesisasfollows:fork=5 10 ··· 100 wesetAtobea20k×2kmatrixwhereA(i i)=1fori=1 ··· 2kand0everywhereelse andwesetBtobea20k×1vectorwhereB(2k+1)=3and0elsewhere.Asinthesmallcase thecostofTLSis1 andthecostofLSis9.RecallthatintheFTLSalgorithm weuseCount-Sketch/leveragescoressampling/Gaussiansketchestospeedupthealgorithm.Intheexperiments wetakesampledensityρ=0.1 0.3 0.6 0.9respec-tivelytocheckourperformance.Theleft2picturesinFigure1showtherunningtimetogetherwiththeratioTLS/FTLSfordifferentsampledensities.WecanseethattherunningtimeofFTLSissigniﬁcantlysmallerthanthatofTLS.ThisisbecausetherunningtimeofTLSdependsheavilyonm thesizeofmatrixA.Whenweapplysketching8techniques wesigniﬁcantlyimproveourrunningtime.Thefewerrowswesample thefasterthealgorithmruns.WecanseethatFTLShasprettygoodperformance;evenwith10%sampledensity FTLSstillperformsbetterthanLS.Moreover themorewesample thebetteraccuracyweachieve.Theabovematrixisextremelysparse.Wealsoconsideranotherclassofmatrices.Fork=5 10 ··· 100 wesetAtobea20k×2kmatrixwhereA(i j)∼N(0 1);wesetBtobea20k×1vectorwhereB(i)∼N(0 3).Asinpreviousexperiments wetakesampledensitiesofρ=0.1 0.3 0.6 0.9 respectively tocheckourperformance.Theresultsofthisexperimentareshownintheright2picturesinFigure1.WeseethatcomparedtoTLS ourFTLSsketching-basedalgorithmsigniﬁcantlyreducestherunningtime.FTLSisstillslowerthanLS though becauseintheFTLSalgorithmwestillneedtosolveaLSproblemofthesamesize.However asdiscussed LSisinadequateinanumberofapplicationsasitdoesnotallowforchangingthematrixA.TheaccuracyofourFTLSalgorithmsisalsoshown.WealsoconductedexperimentsonrealdatasetsfromtheUCIMachineLearningRepository[DKT17].Wechoosedatasetswithregressiontask.Eachdatasetconsistsofinputdataandoutputdata.Toturnitintoatotalleastsquaresproblem wesimplywritedowntheinputdataasamatrixAandtheoutputdataasamatrixB thenrunthecorrespondingalgorithmon(A B).Wehavefourrealdatasets:AirfoilSelf-Noise[UCIa]inTable2(a) WineQualityRedwine[UCIc CCA+09]inTable2(b) WineQualityWhitewine[UCIc CCA+09]inTable2(c) InsuranceCompanyBench-mark(COIL2000)DataSet[UCIb PS]Fromtheresults„weseethatFTLSalsoperformswellonrealdata:whenFTLSsamples10%oftherows theresultiswithin5%oftheoptimalresultofTLS whiletherunningtimeis20−40timesfaster.Inthissense FTLSachievestheadvantagesofbothTLSandLS:FTLShasalmostthesameaccuracyasTLS whileFTLSissigniﬁcantlyfaster.References[ACW17]HaimAvron KennethL.Clarkson andDavidP.Woodruff.Sharperboundsforregular-izeddataﬁtting.InApproximation Randomization andCombinatorialOptimization.AlgorithmsandTechniques APPROX/RANDOM2017 August16-18 2017 Berkeley CA USA pages27:1–27:22 2017.[ALS+18]AlexandrAndoni ChengyuLin YingSheng PeilinZhong andRuiqiZhong.Sub-spaceembeddingandlinearregressionwithorlicznorm.InICML.arXivpreprintarXiv:1806.06430 2018.[BWZ16]ChristosBoutsidis DavidPWoodruff andPeilinZhong.Optimalprincipalcomponentanalysisindistributedandstreamingmodels.InProceedingsofthe48thAnnualACMSIGACTSymposiumonTheoryofComputing(STOC) pages236–249.ACM https://arxiv.org/pdf/1504.06729 2016.[CCA+09]PauloCortez AntónioCerdeira FernandoAlmeida TelmoMatos andJoséReis.Modelingwinepreferencesbydataminingfromphysicochemicalproperties.Deci-sionSupportSystems 47(4):547–553 2009.[CCF02]MosesCharikar KevinChen andMartinFarach-Colton.Findingfrequentitemsindatastreams.InAutomata LanguagesandProgramming pages693–703.Springer 2002.[CCLY19]MichaelBCohen BenCousins YinTatLee andXinYang.Anear-optimalalgorithmforapproximatingthejohnellipsoid.InCOLT 2019.[CLS19]MichaelBCohen YinTatLee andZhaoSong.Solvinglinearprogramsinthecurrentmatrixmultiplicationtime.InProceedingsofthe51stAnnualACMSymposiumonTheoryofComputing(STOC).https://arxiv.org/pdf/1810.07896.pdf 2019.[CW87]DonCoppersmithandShmuelWinograd.Matrixmultiplicationviaarithmeticpro-gressions.InProceedingsofthenineteenthannualACMsymposiumonTheoryofcomputing pages1–6.ACM 1987.9[CW13]KennethL.ClarksonandDavidP.Woodruff.Lowrankapproximationandregressionininputsparsitytime.InSymposiumonTheoryofComputingConference STOC’13 PaloAlto CA USA June1-4 2013 pages81–90.https://arxiv.org/pdf/1207.6365 2013.[CWW19]KennethL.Clarkson RuosongWang andDavidPWoodruff.Dimensionalityreduc-tionfortukeyregression.InICML.arXivpreprintarXiv:1904.05543 2019.[DJS+19]HuaianDiao RajeshJayaram ZhaoSong WenSun andDavidP.Woodruff.Optimalsketchingforkroneckerproductregressionandlowrankapproximation.InNeurIPS 2019.[DKT17]DuaDheeruandEﬁKarraTaniskidou.UCImachinelearningrepository 2017.[DMM06a]PetrosDrineas MichaelW.Mahoney andS.Muthukrishnan.Samplingalgorithmsforl2regressionandapplications.InProceedingsoftheSeventeenthAnnualACM-SIAMSymposiumonDiscreteAlgorithm SODA’06 pages1127–1136 Philadelphia PA USA 2006.SocietyforIndustrialandAppliedMathematics.[DMM06b]PetrosDrineas MichaelW.Mahoney andS.Muthukrishnan.Subspacesamplingandrelative-errormatrixapproximation:Column-row-basedmethods.InAlgorithms-ESA2006 14thAnnualEuropeanSymposium Zurich Switzerland September11-13 2006 Proceedings pages304–314 2006.[DMMS11]PetrosDrineas MichaelWMahoney SMuthukrishnan andTamásSarlós.Fasterleastsquaresapproximation.Numerischemathematik 117(2):219–249 2011.[DSSW18]HuaianDiao ZhaoSong WenSun andDavidP.Woodruff.Sketchingforkroneckerproductregressionandp-splines.AISTATS 2018.[DSWY19]HuaianDiao ZhaoSong DavidP.Woodruff andXinYang.TotalLeastSquaresRegressioninInputSparsityTime.InNeurIPS.https://arxiv.org/pdf/1909.12441 2019.[FT07]ShmuelFriedlandandAnatoliTorokhti.Generalizedrank-constrainedmatrixapprox-imations.SIAMJournalonMatrixAnalysisandApplications 29(2):656–659 2007.[LHW17]XingguoLi JarvisHaupt andDavidWoodruff.Nearoptimalsketchingoflow-ranktensorregression.InAdvancesinNeuralInformationProcessingSystems pages3466–3476 2017.[LPT09]ShuaiLu SergeiVPereverzev andUlrichTautenhahn.Regularizedtotalleastsquares:computationalaspectsanderrorbounds.SIAMJournalonMatrixAnalysisandAppli-cations 31(3):918–941 2009.[LSZ19]YinTatLee ZhaoSong andQiuyiZhang.Solvingempiricalriskminimizationinthecurrentmatrixmultiplicationtime.InCOLT.https://arxiv.org/pdf/1905.04447.pdf 2019.[LV10]JörgLampeandHeinrichVoss.Solvingregularizedtotalleastsquaresproblemsbasedoneigenproblems.TaiwaneseJournalofMathematics 14(3A):885–909 2010.[LV14]JorgLampeandHeinrichVoss.Large-scaledualregularizedtotalleastsquares.Elec-tronicTransactionsonNumericalAnalysis 42:13–40 2014.[MM13]XiangruiMengandMichaelWMahoney.Low-distortionsubspaceembeddingsininput-sparsitytimeandapplicationstorobustlinearregression.InProceedingsoftheforty-ﬁfthannualACMsymposiumonTheoryofcomputing pages91–100.ACM https://arxiv.org/pdf/1210.3135 2013.[MVH07]IvanMarkovskyandSabineVanHuffel.Overviewoftotalleast-squaresmethods.Signalprocessing 87(10):2283–2302 2007.10[NN13]JelaniNelsonandHuyLNguyên.Osnap:Fasternumericallinearalgebraalgorithmsviasparsersubspaceembeddings.In2013IEEE54thAnnualSymposiumonFoun-dationsofComputerScience(FOCS) pages117–126.IEEE https://arxiv.org/pdf/1211.1002 2013.[PS]P.vanderPuttenandM.vanSomeren.Coilchallenge2000:Theinsurancecompanycase.Technicalreport.[PSW17]EricPrice ZhaoSong andDavidP.Woodruff.Fastregressionwithan‘∞guarantee.InICALP 2017.[RG04]RosemaryARenautandHongbinGuo.Efﬁcientalgorithmsforsolutionofregularizedtotalleastsquares.SIAMJournalonMatrixAnalysisandApplications 26(2):457–476 2004.[RSW16]IlyaRazenshteyn ZhaoSong andDavidPWoodruff.Weightedlowrankapproxima-tionswithprovableguarantees.InProceedingsofthe48thAnnualSymposiumontheTheoryofComputing(STOC) 2016.[SWY+19]ZhaoSong RuosongWang LinF.Yang HongyangZhang andPeilinZhong.Efﬁcientsymmetricnormregressionvialinearsketching.InNeurIPS 2019.[SWZ17]ZhaoSong DavidPWoodruff andPeilinZhong.Lowrankapproximationwithen-trywise‘1-normerror.InProceedingsofthe49thAnnualSymposiumontheTheoryofComputing(STOC).ACM https://arxiv.org/pdf/1611.00898 2017.[SWZ19a]ZhaoSong DavidPWoodruff andPeilinZhong.Averagecasecolumnsubsetselec-tionforentrywise‘1-normloss.InNeurIPS 2019.[SWZ19b]ZhaoSong DavidPWoodruff andPeilinZhong.Relativeerrortensorlowrankap-proximation.InSODA.https://arxiv.org/pdf/1704.08246 2019.[SWZ19c]ZhaoSong DavidPWoodruff andPeilinZhong.Towardsazero-onelawforcolumnsubsetselection.InNeurIPS 2019.[TZ12]MikkelThorupandYinZhang.Tabulation-based5-independenthashingwithapplica-tionstolinearprobingandsecondmomentestimation.SIAMJournalonComputing 41(2):293–331 2012.[UCIa]UCI.Airfoilself-noise.In.https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise .[UCIb]UCI.Insurancecompanybenchmark(coil2000)dataset.In.https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29 .[UCIc]UCI.Winequality.In.https://archive.ics.uci.edu/ml/datasets/Wine+Quality .[VHV91]SabineVanHuffelandJoosVandewalle.Thetotalleastsquaresproblem:computa-tionalaspectsandanalysis volume9.Siam 1991.[Wil12]VirginiaVassilevskaWilliams.Multiplyingmatricesfasterthancoppersmith-winograd.InProceedingsoftheforty-fourthannualACMsymposiumonTheoryofcomputing(STOC) pages887–898.ACM 2012.[Woo14]DavidP.Woodruff.Sketchingasatoolfornumericallinearalgebra.FoundationsandTrendsinTheoreticalComputerScience 10(1-2):1–157 2014.11,Xiangyu Wang
David Dunson
Huaian Diao
Zhao Song
David Woodruff
Xin Yang