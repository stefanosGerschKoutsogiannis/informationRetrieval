2018,The promises and pitfalls of Stochastic Gradient Langevin Dynamics,Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution  the algorithm is often used with a constant step size in practice and has demonstrated spectacular successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large  we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like as Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them  SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution  with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC  SGLD  SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.,The promises and pitfalls of Stochastic Gradient

Langevin Dynamics

Nicolas Brosse  Éric Moulines

Centre de Mathématiques Appliquées  UMR 7641 

Ecole Polytechnique  Palaiseau  France.

nicolas.brosse@polytechnique.edu  eric.moulines@polytechnique.edu

Alain Durmus

Ecole Normale Supérieure CMLA 

61 Av. du Président Wilson 94235 Cachan Cedex  France.

alain.durmus@cmla.ens-cachan.fr

Abstract

Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC
algorithm for Bayesian learning from large scale datasets. While SGLD with
decreasing step sizes converges weakly to the posterior distribution  the algorithm
is often used with a constant step size in practice and has demonstrated successes
in machine learning tasks. The current practice is to set the step size inversely
proportional to N where N is the number of training samples. As N becomes
large  we show that the SGLD algorithm has an invariant probability measure
which signiﬁcantly departs from the target posterior and behaves like Stochastic
Gradient Descent (SGD). This difference is inherently due to the high variance of
the stochastic gradients. Several strategies have been suggested to reduce this effect;
among them  SGLD Fixed Point (SGLDFP) uses carefully designed control variates
to reduce the variance of the stochastic gradients. We show that SGLDFP gives
approximate samples from the posterior distribution  with an accuracy comparable
to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear
in the number of data points. We provide a detailed analysis of the Wasserstein
distances between LMC  SGLD  SGLDFP and SGD and explicit expressions of
the means and covariance matrices of their invariant distributions. Our ﬁndings are
supported by limited numerical experiments.

1

Introduction

Most MCMC algorithms have not been designed to process huge sample sizes  a typical setting in
machine learning. As a result  many classical MCMC methods fail in this context  because the mixing
time becomes prohibitively long and the cost per iteration increases proportionally to the number of
training samples N. The computational cost in standard Metropolis-Hastings algorithm comes from
1) the computation of the proposals  2) the acceptance/rejection step. Several approaches to solve
these issues have been recently proposed in machine learning and computational statistics.
Among them  the stochastic gradient langevin dynamics (SGLD) algorithm  introduced in [33]  is
a popular choice. This method is based on the Langevin Monte Carlo (LMC) algorithm proposed
in [16  17]. Standard versions of LMC require to compute the gradient of the log-posterior at the
current ﬁt of the parameter  but avoid the accept/reject step. The LMC algorithm is a discretization
of a continuous-time process  the overdamped Langevin diffusion  which leaves invariant the target
distribution π. To further reduce the computational cost  SGLD uses unbiased estimators of the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

gradient of the log-posterior based on subsampling. This method has triggered a huge number of
works among others [1  21  2  6  8  12  24  13  4] and have been successfully applied to a range of
state of the art machine learning problems [27  23].
The properties of SGLD with decreasing step sizes have been studied in [31]. The two key ﬁndings
in this work are that 1) the SGLD algorithm converges weakly to the target distribution π  2) the
optimal rate of convergence to equilibrium scales as n−1/3 where n is the number of iterations  see
[31  Section 5]. However  in most of the applications  constant rather than decreasing step sizes are
used  see [1  8  18  22  30  32]. A natural question for the practical design of SGLD is the choice of
the minibatch size. This size controls on the one hand the computational complexity of the algorithm
per iteration and on the other hand the variance of the gradient estimator. Non-asymptotic bounds in
Wasserstein distance between the marginal distribution of the SGLD iterates and the target distribution
π have been established in [10  11]. These results highlight the cost of using stochastic gradients and
show that  for a given precision  in Wasserstein distance  the computational cost of the plain SGLD
algorithm does not improve over the LMC algorithm; Nagapetyan et al. [25] reports also similar
results on the mean square error.
It has been suggested to use control variates to reduce the high variance of the stochastic gradients.
For strongly log-concave models  Nagapetyan et al. [25]  Baker et al. [3] use the mode of the posterior
distribution as a reference point and introduce the SGLDFP (Stochastic Gradient Langevin Dynamics
Fixed Point) algorithm. Nagapetyan et al. [25]  Baker et al. [3] provide upper bounds on the mean
square error and the Wasserstein distance between the marginal distribution of the iterates of SGLDFP
and the posterior distribution. In addition  Nagapetyan et al. [25]  Baker et al. [3] show that the overall
cost remains sublinear in the number of individual data points  up to a preprocessing step. Other
control variates methodologies are provided for non-concave models in the form of SAGA-Langevin
Dynamics and SVRG-Langevin Dynamics [13  7]  albeit a detailed analysis in Wasserstein distance
of these algorithms is only available for strongly log-concave models [5].
In this paper  we provide further insights on the links between SGLD  SGLDFP  LMC and SGD
(Stochastic Gradient Descent). In our analysis  the algorithms are used with a constant step size
and the parameters are set to the standard values used in practice [1  8  18  22  30  32]. The LMC 
SGLD and SGLDFP algorithms deﬁne homogeneous Markov chains  each of which admits a unique
stationary distribution used as a hopefully close proxy of π. The main contribution of this paper is to
show that  while the invariant distributions of LMC and SGLDFP become closer to π as the number
of data points increases  on the opposite  the invariant measure of SGLD never comes close to the
target distribution π and is in fact very similar to the invariant measure of SGD.
In Section 3.1  we give an upper bound in Wasserstein distance of order 2 between the marginal
distribution of the iterates of LMC and the Langevin diffusion  SGLDFP and LMC  and SGLD
and SGD. We provide a lower bound on the Wasserstein distance between the marginal distribution
of the iterates of SGLDFP and SGLD. In Section 3.2  we give a comparison of the means and
covariance matrices of the invariant distributions of LMC  SGLDFP and SGLD with those of the
target distribution π. Our claims are supported by numerical experiments in Section 4.

2 Preliminaries
Denote by z = {zi}N
i=1 the observations. We are interested in situations where the target distribution
π arises as the posterior in a Bayesian inference problem with prior density π0(θ) and a large number
i=1 p(zi|θ).
i=0 Ui.

N (cid:29) 1 of i.i.d. observations zi with likelihoods p(zi|θ). In this case  π(θ) = π0(θ)(cid:81)N
We denote Ui(θ) = − log(p(zi|θ)) for i ∈ {1  . . .   N}  U0(θ) = − log(π0(θ))  U =(cid:80)N

Under mild conditions  π is the unique invariant probability measure of the Langevin Stochastic
Differential Equation (SDE):

dθt = −∇U (θt)dt +

√

2dBt  

(1)

where (Bt)t≥0 is a d-dimensional Brownian motion. Based on this observation  Langevin Monte
Carlo (LMC) is an MCMC algorithm that enables to sample (approximately) from π using an Euler
discretization of the Langevin SDE:

θk+1 = θk − γ∇U (θk) +(cid:112)2γZk+1  

(2)

2

where γ > 0 is a constant step size and (Zk)k≥1 is a sequence of i.i.d. standard d-dimensional
Gaussian vectors. Discovered and popularised in the seminal works [16  17  29]  LMC has recently
received renewed attention [9  15  14  11]. However  the cost of one iteration is N d which is
prohibitively large for massive datasets. In order to scale up to the big data setting  Welling and
i∈S ∇Ui where S is
a minibatch of {1  . . .   N} with replacement of size p. A single update of SGLD is then given for
k ∈ N by

Teh [33] suggested to replace ∇U with an unbiased estimate ∇U0 + (N/p)(cid:80)
 +(cid:112)2γZk+1 .

∇U0(θk) +

θk+1 = θk − γ

∇Ui(θk)

(cid:88)

(3)

N
p

i∈Sk+1

The idea of using only a fraction of data points to compute an unbiased estimate of the gradient
at each iteration comes from Stochastic Gradient Descent (SGD) which is a popular algorithm to
minimize the potential U. SGD is very similar to SGLD because it is characterised by the same
recursion as SGLD but without Gaussian noise:

 .

∇Ui(θk)

(cid:88)

i∈Sk+1

N
p

θk+1 = θk − γ

∇U0(θk) − ∇U0(θ(cid:63)) +

∇U0(θk) +
(cid:88)

N
p

i∈Sk+1

Assuming for simplicity that U has a minimizer θ(cid:63)  we can deﬁne a control variates version of SGLD 
SGLDFP  see [13  7]  given for k ∈ N by

θk+1 = θk − γ

{∇Ui(θk) − ∇Ui(θ(cid:63))}

(4)

 +(cid:112)2γZk+1 . (5)

(cid:13)(cid:13)Dj Ui(θ)(cid:13)(cid:13) ≤ ˜L. In particular for all i ∈ {0  . . .   N}  Ui is ˜L-gradient Lipschitz  i.e. for

It is worth mentioning that the objectives of the different algorithms presented so far are distinct. On
the one hand  LMC  SGLD and SGDLFP are MCMC methods used to obtain approximate samples
from the posterior distribution π. On the other hand  SGD is a stochastic optimization algorithm used
to ﬁnd an estimate of the mode θ(cid:63) of the posterior distribution. In this paper  we focus on the ﬁxed
step-size SGLD algorithm and assess its ability to reliably sample from π. For that purpose and to
quantify precisely the relation between LMC  SGLD  SGDFP and SGD  we make for simplicity the
following assumptions on U.
H1. For all i ∈ {0  . . .   N}  Ui is four times continuously differentiable and for all j ∈ {2  3  4} 
supθ∈Rd
all θ1  θ2 ∈ Rd  (cid:107)∇Ui(θ1) − ∇Ui(θ2)(cid:107) ≤ ˜L(cid:107)θ1 − θ2(cid:107).
H2. U is m-strongly convex  i.e. for all θ1  θ2 ∈ Rd  (cid:104)∇U (θ1) − ∇U (θ2)  θ1 − θ2(cid:105) ≥ m(cid:107)θ1 − θ2(cid:107)2.
H3. For all i ∈ {0  . . .   N}  Ui is convex.
Note that under H 1  U is
four
{2  3  4}  supθ∈Rd
sup(cid:107)u1(cid:107)≤1 ... (cid:107)uj(cid:107)≤1 Dj U (θ)[u1  . . .   uj]. In particular  U is L-gradient Lipschitz. Furthermore 
under H2  U has a unique minimizer θ(cid:63). In this paper  we focus on the asymptotic N → +∞ . We
assume that lim inf N→+∞ N−1m > 0  which is a common assumption for the analysis of SGLD
and SGLDFP [3  5]. In practice [1  8  18  22  30  32]  γ is of order 1/N and we adopt this convention
in this article.
For a practical implementation of SGLDFP  an estimator ˆθ of θ(cid:63) is necessary. The theoretical analysis
and the bounds remain unchanged if  instead of considering SGLDFP centered w.r.t. θ(cid:63)  we study
SGLDFP centered w.r.t. ˆθ satisfying E[(cid:107)ˆθ − θ(cid:63)(cid:107)2] = O(1/N ). Such an estimator ˆθ can be computed
using for example SGD with decreasing step sizes  see [26  eq.(2.8)] and [3  Section 3.4]  for a
computational cost linear in N.

(cid:13)(cid:13)Dj U (θ)(cid:13)(cid:13) ≤ L  with L = (N + 1) ˜L and where (cid:13)(cid:13)Dj U (θ)(cid:13)(cid:13) =

times continuously differentiable and for

∈

j

3 Results

3.1 Analysis in Wasserstein distance

Before presenting the results  some notations and elements of Markov chain theory have to be
introduced. Denote by P2(Rd) the set of probability measures with ﬁnite second moment and by

3

B(Rd) the Borel σ-algebra of Rd. For λ  ν ∈ P2(Rd)  deﬁne the Wasserstein distance of order 2 by

(cid:18)(cid:90)

(cid:19)1/2

W2(λ  ν) = inf

ξ∈Π(λ ν)

Rd×Rd

(cid:107)θ − ϑ(cid:107)2 ξ(dθ  dϑ)

 

where Π(λ  ν) is the set of probability measures ξ on B(Rd) ⊗ B(Rd) satisfying for all A ∈ B(Rd) 
ξ(A × Rd)) = λ(A) and ξ(Rd × A) = ν(A).
A Markov kernel R on Rd × B(Rd) is a mapping R : Rd × B(Rd) → [0  1] satisfying the following
conditions: (i) for every θ ∈ Rd  R(θ ·) : A (cid:55)→ R(θ  A) is a probability measure on B(Rd) (ii) for
every A ∈ B(Rd)  R(·  A) : θ (cid:55)→ R(θ  A) is a measurable function. For any probability measure
Rd λ(dθ)R(θ  A). For all k ∈ N∗ 
we deﬁne the Markov kernel Rk recursively by R1 = R and for all θ ∈ Rd and A ∈ B(Rd) 

λ on B(Rd)  we deﬁne λR for all A ∈ B(Rd) by λR(A) = (cid:82)
Rk+1(θ  A) =(cid:82)

Rd Rk(θ  dϑ)R(ϑ  A). A probability measure ¯π is invariant for R if ¯πR = ¯π.

The LMC  SGLD  SGD and SGLDFP algorithms deﬁned respectively by (2)  (3)  (4) and (5) are
homogeneous Markov chains with Markov kernels denoted RLMC  RSGLD  RSGD  and RFP. To avoid
overloading the notations  the dependence on γ and N is implicit.
Lemma 1. Assume H 1  H 2 and H 3. For any step size γ ∈ (0  2/L)  RSGLD (respectively
RLMC  RSGD  RFP) has a unique invariant measure πSGLD ∈ P2(Rd) (respectively πLMC  πSGD  πFP).
In addition  for all γ ∈ (0  1/L]  θ ∈ Rd and k ∈ N 

(cid:90)

W2

2(Rk

SGLD(θ ·)  πSGLD) ≤ (1 − mγ)k

Rd
and the same inequality holds for LMC  SGD and SGLDFP.

(cid:107)θ − ϑ(cid:107)2 πSGLD(dϑ)

Proof. The proof is postponed to Section 1.1 in the supplementary document.
Under H1  (1) has a unique strong solution (θt)t≥0 for every initial condition θ0 ∈ Rd [20  Chapter
5  Theorems 2.5 and 2.9]. Denote by (Pt)t≥0 the semigroup of the Langevin diffusion deﬁned for all
θ0 ∈ Rd and A ∈ B(Rd) by Pt(θ0  A) = P(θt ∈ A).
Theorem 2. Assume H1  H2 and H3. For all γ ∈ (0  1/L]  λ  µ ∈ P2(Rd) and n ∈ N  we have the
following upper-bounds in Wasserstein distance between

i) LMC and SGLDFP 

W2

2(λRn

LMC  µRn

FP) ≤ (1 − mγ)n W2

2(λ  µ) +

2L2γd
pm2

+

L2γ2

p

n(1 − mγ)n−1

(cid:90)
(cid:18)

Rd

(cid:107)ϑ − θ(cid:63)(cid:107)2 µ(dϑ)  

(cid:19)(cid:18) 13

6

+

L
m

(cid:19)

3 +

L
m

ii) the Langevin diffusion and LMC 

W2

2(λRn

LMC  µPnγ) ≤ 2

1 − mLγ
m + L

W2

2(λ  µ) + dγ

(cid:18)

(cid:19)n

(cid:18)

+ ne−(m/2)γ(n−1)L3γ3

iii) SGLD and SGD 

m + L

(cid:19)(cid:90)

2m

Rd

1 +

m + L

2m

(cid:107)ϑ − θ(cid:63)(cid:107)2 µ(dϑ)  

W2

2(λRn

SGLD  µRn

SGD) ≤ (1 − mγ)n W2

2(λ  µ) + (2d)/m .

Proof. The proof is postponed to Section 1.2 in the supplementary document.
Corollary 3. Assume H1  H2 and H3. Set γ = η/N with η ∈ (0  1/(2 ˜L)] and assume that
lim inf N→∞ mN−1 > 0. Then 

4

i) for all n ∈ N  we get W2(Rn

W2(πLMC  πFP) =

√

LMC(θ(cid:63) ·)  Rn
dη O(N−1/2)  W2(πLMC  π) =
SGLD(θ(cid:63) ·)  Rn
√

d O(N−1/2).

ii) for all n ∈ N  we get W2(Rn

W2(πSGLD  πSGD) =

√

FP(θ(cid:63) ·)) =

√
dη O(N−1/2) and
dη O(N−1/2).
√

d O(N−1/2) and

SGD(θ(cid:63) ·)) =

√

Theorem 2 implies that the number of iterations necessary to obtain a sample ε-close from π in
Wasserstein distance is the same for LMC and SGLDFP. However for LMC  the cost of one iteration
is N d which is larger than pd the cost of one iteration for SGLDFP. In other words  to obtain an
approximate sample from the target distribution at an accuracy O(1/
N ) in 2-Wasserstein distance 
LMC requires Θ(N ) operations  in contrast with SGLDFP that needs only Θ(1) operations.
We show in the sequel that W2(πFP  πSGLD) = Ω(1) when N → +∞ in the case of a Bayesian linear
regression  where for two sequences (uN )N≥1  (vN )N≥1  uN = Ω(vN ) if lim inf N→+∞ uN /vN >
0. The dataset is z = {(yi  xi)}N
i=1 where yi ∈ R is the response variable and xi ∈ Rd are the
covariates. Set y = (y1  . . .   yN ) ∈ RN and X ∈ RN×d the matrix of covariates such that the ith row
θ > 0. For i ∈ {1  . . .   N}  the conditional distribution of yi given xi is Gaussian
of X is xi. Let σ2
with mean xT
y. The prior π0(θ) is a normal distribution of mean 0 and variance
σ2
where

θ Id. The posterior distribution π is then proportional to π(θ) ∝ exp(cid:0)−(1/2)(θ − θ(cid:63))TΣ(θ − θ(cid:63))(cid:1)

i θ and variance σ2

y  σ2

Σ = Id /σ2

θ + XTX/σ2
y

and θ(cid:63) = Σ−1(XTy)/σ2
y .

We assume that XTX (cid:23) m Id  with lim inf N→+∞ m/N > 0. Let S be a minibatch of {1  . . .   N}
with replacement of size p. Deﬁne
∇U0(θ) + (N/p)

∇Ui(θ) = Σ(θ − θ(cid:63)) + ρ(S)(θ − θ(cid:63)) + ξ(S)

where

ρ(S) =

Id
σ2
θ

+

N
pσ2
y

xixT

i − Σ   ξ(S) =

θ(cid:63)
σ2
θ

+

N
pσ2
y

(cid:0)xT

i θ(cid:63) − yi

(cid:1) xi .

(6)

(cid:88)

i∈S

ρ(S)(θ − θ(cid:63)) is the multiplicative part of the noise in the stochastic gradient  and ξ(S) the additive
part that does not depend on θ. The additive part of the stochastic gradient for SGLDFP disappears
since

∇U0(θ) − ∇U0(θ(cid:63)) + (N/p)

{∇Ui(θ) − ∇Ui(θ(cid:63))} = Σ(θ − θ(cid:63)) + ρ(S)(θ − θ(cid:63)) .

(cid:88)
(cid:88)

i∈S

i∈S

(cid:88)

i∈S

In this setting  the following theorem shows that the Wasserstein distances between the marginal
distribution of the iterates of SGLD and SGLDFP  and πSGLD and π  is of order Ω(1) when N → +∞.
This is in sharp contrast with the results of Corollary 3 where the Wasserstein distances tend to 0 as
N → +∞ at a rate N−1/2. For simplicity  we state the result for d = 1.
Theorem 4. Consider the case of the Bayesian linear regression in dimension 1.

i) For all γ ∈ (0  Σ−1{1 + N/(p(cid:80)N

(cid:18) 1 − µ

(cid:19)1/2

i=1 x2

i )}−1] and n ∈ N∗ 

W2(Rn

1 − µn

FP(θ(cid:63) ·))
N(cid:88)

SGLD(θ(cid:63) ·)  Rn
(cid:40)

(cid:18) (xiθ(cid:63) − yi)xi
ii) Set γ = η/N with η ∈ (0  lim inf N→+∞ N Σ−1{1 + N/(p(cid:80)N
lim inf N→+∞ N−1(cid:80)N

i=1 x2
i > 0. We have W2(πSGLD  π) = Ω(1).

where µ ∈ (0  1 − γΣ].

i=1 x2

2γ +

γ2N

≥

σ2
y

i=1

+

p

(cid:19)2(cid:41)1/2

−(cid:112)2γ  

θ(cid:63)
N σ2
θ

i )}−1] and assume that

Proof. The proof is postponed to Section 1.3 in the supplementary document.

5

The study in Wasserstein distance emphasizes the different behaviors of the LMC  SGLDFP  SGLD
and SGD algorithms. When N → ∞ and limN→+∞ m/N > 0  the marginal distributions of the
kth iterates of the LMC and SGLDFP algorithm are very close to the Langevin diffusion and their
invariant probability measures πLMC and πFP are similar to the posterior distribution of interest π.
In contrast  the marginal distributions of the kth iterates of SGLD and SGD are analogous and their
invariant probability measures πSGLD and πSGD are very different from π when N → +∞.
Note that to ﬁx the asymptotic bias of SGLD  other strategies can be considered: choosing a step
size γ ∝ N−β where β > 1 and/or increasing the batch size p ∝ N α where α ∈ [0  1]. Using the
Wasserstein (of order 2) bounds of SGLD w.r.t. the target distribution π  see e.g. [11  Theorem 3] 
α + β should be equal to 2 to guarantee the ε-accuracy in Wasserstein distance of SGLD for a cost
proportional to N (up to logarithmic terms)  independently of the choice of α and β.

3.2 Mean and covariance matrix of πLMC  πFP  πSGLD

We now establish an expansion of the mean and second moments of πLMC  πFP  πSGLD and πSGD as
N → +∞  and compare them. We ﬁrst give an expansion of the mean and second moments of π as
N → +∞.
Proposition 5. Assume H1 and H2 and that lim inf N→+∞ N−1m > 0. Then 

(θ − θ(cid:63))⊗2π(dθ) = ∇2U (θ(cid:63))−1 + ON→+∞(N−3/2)  

θ π(dθ) − θ(cid:63) = −(1/2)∇2U (θ(cid:63))−1 D3 U (θ(cid:63))[∇2U (θ(cid:63))−1] + ON→+∞(N−3/2) .

(cid:90)

(cid:90)

Rd

Rd

Proof. The proof is postponed to Section 2.1 in the supplementary document.

Contrary to the Bayesian linear regression where the covariance matrices can be explicitly computed 
see Section 3 in the supplementary document  only approximate expressions are available in the gen-
eral case. For that purpose  we consider two types of asymptotic. For LMC and SGLDFP  we assume
that limN→+∞ m/N > 0  γ = η/N  for η > 0  and we develop an asymptotic when N → +∞.
Combining Proposition 5 and Theorem 6   we show that the biases and covariance matrices of πLMC
and πFP are of order Θ(1/N ) with remainder terms of the form O(N−3/2)  where for two sequences
(uN )N≥1  (vN )N≥1  u = Θ(v) if 0 < lim inf N→+∞ uN /vN ≤ lim supN→+∞ uN /vN < +∞.
Regarding SGD and SGLD  we do not have such concentration properties when N → +∞ because
of the high variance of the stochastic gradients. The biases and covariance matrices of SGLD and
SGD are of order Θ(1) when N → +∞. To obtain approximate expressions of these quantities  we
set γ = η/N where η > 0 is the step size for the gradient descent over the normalized potential
U/N. Assuming that m is proportional to N and N ≥ 1/η  we show by combining Proposition 5
and Theorem 7 that the biases and covariance matrices of SGLD and SGD are of order Θ(η) with
remainder terms of the form O(η3/2) when η → 0.
Before giving the results associated to πLMC  πFP  πSGLD and πSGD  we need to introduce some
notations. For any matrices A1  A2 ∈ Rd×d  we denote by A1 ⊗ A2 the Kronecker product deﬁned
on Rd×d by A1 ⊗ A2 : Q (cid:55)→ A1QA2 and A⊗2 = A ⊗ A. Besides  for all θ1 ∈ Rd and θ2 ∈ Rd  we
denote by θ1 ⊗ θ2 ∈ Rd×d the tensor product of θ1 and θ2. For any matrix A ∈ Rd×d  Tr(A) is the
trace of A.
Deﬁne K : Rd×d → Rd×d for all A ∈ Rd×d by

∇2Ui(θ(cid:63)) − 1

N

N(cid:88)

i=1

N(cid:88)

j=1

⊗2

∇2Uj(θ(cid:63))

A .

N
p
and H and G : Rd×d → Rd×d by

K(A) =

H = ∇2U (θ(cid:63)) ⊗ Id + Id⊗∇2U (θ(cid:63)) − γ∇2U (θ(cid:63)) ⊗ ∇2U (θ(cid:63))  
G = ∇2U (θ(cid:63)) ⊗ Id + Id⊗∇2U (θ(cid:63)) − γ(∇2U (θ(cid:63)) ⊗ ∇2U (θ(cid:63)) + K) .

6

(7)

(8)
(9)

K  H and G can be interpreted as perturbations of ∇2U (θ(cid:63))⊗2 and ∇2U (θ(cid:63))  respectively  due to
the noise of the stochastic gradients. It can be shown  see Section 2.2 in the supplementary document 
that for γ small enough  H and G are invertible.
Theorem 6. Assume H1  H2 and H3. Set γ = η/N and assume that lim inf N→+∞ N−1m > 0.
There exists an (explicit) η0 independent of N such that for all η ∈ (0  η0) 

(θ − θ(cid:63))⊗2πLMC(dθ) = H−1(2 Id) + ON→+∞(N−3/2)  
(θ − θ(cid:63))⊗2πFP(dθ) = G−1(2 Id) + ON→+∞(N−3/2)  

(10)

(11)

θπLMC(dθ) − θ(cid:63) = −∇2U (θ(cid:63))−1 D3 U (θ(cid:63))[H−1 Id] + ON→+∞(N−3/2)  
θπFP(dθ) − θ(cid:63) = −∇2U (θ(cid:63))−1 D3 U (θ(cid:63))[G−1 Id] + ON→+∞(N−3/2) .

(cid:90)
(cid:90)

Rd

Rd

(cid:90)
(cid:90)

Rd

Rd

and

(cid:90)
(cid:90)

Rd

Rd

and(cid:90)
(cid:90)

Rd

Rd
where

Proof. The proof is postponed to Section 2.2.2 in the supplementary document.
Theorem 7. Assume H1  H2 and H3. Set γ = η/N and assume that lim inf N→+∞ N−1m > 0.
There exists an (explicit) η0 independent of N such that for all η ∈ (0  η0) and N ≥ 1/η 

(θ − θ(cid:63))⊗2πSGLD(dθ) = G−1 {2 Id +(η/p) M} + Oη→0(η3/2)  
(θ − θ(cid:63))⊗2πSGD(dθ) = (η/p) G−1 M +Oη→0(η3/2)  

(12)

(13)

θπSGLD(dθ) − θ(cid:63) = −(1/2)∇2U (θ(cid:63))−1 D3 U (θ(cid:63))[G−1 {2 Id +(η/p) M}] + Oη→0(η3/2)  
θπSGD(dθ) − θ(cid:63) = −(η/2p)∇2U (θ(cid:63))−1 D3 U (θ(cid:63))[G−1 M] + Oη→0(η3/2)  

∇Ui(θ(cid:63)) − 1

N

N(cid:88)

i=1

M =

⊗2

N(cid:88)

j=1

∇Uj(θ(cid:63))

 

(14)

and G is deﬁned in (9).

Proof. The proof is postponed to Section 2.2.2 in the supplementary document.

Note that this result implies that the mean and the covariance matrix of πSGLD and πSGD stay lower
bounded by a positive constant for any η > 0 as N → +∞. In Section 4 of the supplementary
document  a ﬁgure illustrates the results of Theorem 6 and Theorem 7 in the asymptotic N → +∞.

4 Numerical experiments

Simulated data For illustrative purposes  we consider a Bayesian logistic regression in dimension
d = 2. We simulate N = 105 covariates {xi}N
i=1 drawn from a standard 2-dimensional Gaussian
distribution and we denote by X ∈ RN×d the matrix of covariates such that the ith row of X is xi.
Our Bayesian regression model is speciﬁed by a Gaussian prior of mean 0 and covariance matrix the
identity  and a likelihood given for yi ∈ {0  1} by p(yi|xi  θ) = (1 + e−xT
i θ)yi−1. We
simulate N observations {yi}N
i=1 under this model. In this setting  H1 and H3 are satisﬁed  and H2
holds if the state space is compact.
To illustrate the results of Section 3.2  we consider 10 regularly spaced values of N between 102
and 105 and we truncate the dataset accordingly. We compute an estimator ˆθ of θ(cid:63) using SGD [28]

i θ)−yi(1 + exT

7

Figure 1: Distance to θ(cid:63)  (cid:13)(cid:13)¯θn − θ(cid:63)(cid:13)(cid:13) for LMC  SGLDFP  SGLD and SGD  function of N  in

logarithmic scale.

k=0 θk and {1/(n − 1)}(cid:80)n−1

combined with the BFGS algorithm [19]. For the LMC  SGLDFP  SGLD and SGD algorithms 
the step size γ is set equal to (1 + δ/4)−1 where δ is the largest eigenvalue of XTX. We start the
algorithms at θ0 = ˆθ and run n = 1/γ iterations where the ﬁrst 10% samples are discarded as a
burn-in period.
We estimate the means and covariance matrices of πLMC  πFP  πSGLD and πSGD by their empirical
k=0 (θk − ¯θn)⊗2. We plot the mean and the trace
of the covariance matrices for the different algorithms  averaged over 100 independent trajectories  in
Figure 1 and Figure 2 in logarithmic scale.

averages ¯θn = (1/n)(cid:80)n−1
The slope for LMC and SGLDFP is −1 which conﬁrms the convergence of(cid:13)(cid:13)¯θn − θ(cid:63)(cid:13)(cid:13) to 0 at a rate
N−1. On the other hand  we can observe that(cid:13)(cid:13)¯θn − θ(cid:63)(cid:13)(cid:13) converges to a constant for SGD and SGLD.
not included in the simulations. We truncate the training dataset at N ∈(cid:8)103  104  105(cid:9). For all

Covertype dataset We then illustrate our results on the covertype dataset1 with a Bayesian logistic
regression model. The prior is a standard multivariate Gaussian distribution. Given the size of
the dataset and the dimension of the problem  LMC requires high computational resources and is

algorithms  the step size γ is set equal to 1/N and the trajectories are started at ˆθ  an estimator of θ(cid:63) 
computed using SGD combined with the BFGS algorithm.
We empirically check that the variance of the stochastic gradients scale as N 2 for SGD and SGLD 
and as N for SGLDFP. We compute the empirical variance estimator of the gradients  take the mean
over the dimension and display the result in a logarithmic plot in Figure 3. The slopes are 2 for SGD
and SGLD  and 1 for SGLDFP.
On the test dataset  we also evaluate the negative loglikelihood of the three algorithms for different

values of N ∈(cid:8)103  104  105(cid:9)  as a function of the number of iterations. The plots are shown in

Figure 4. We note that for large N  SGLD and SGD give very similar results that are below the
performance of SGLDFP.

1https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/covtype.libsvm.

binary.scale.bz2

8

102103104105103102101nLMC102103104105103102101SGLDFP102103104105N2×101nSGLD102103104105N2×101SGDFigure 2: Trace of the covariance matrices for LMC  SGLDFP  SGLD and SGD  function of N  in
logarithmic scale.

Figure 3: Variance of the stochastic gradients of SGLD  SGLDFP and SGD function of N  in
logarithmic scale.

number of iterations for different values of N ∈(cid:8)103  104  105(cid:9).

Figure 4: Negative loglikelihood on the test dataset for SGLD  SGLDFP and SGD function of the

9

102103104105103102101Tr(cov(n))LMC102103104105103102101SGLDFP102103104105N6×1017×101Tr(cov(n))SGLD102103104105N5×101SGD103104105N103104105106Variance of the gradientssgd103104105N103104105106sgld103104105N102103104sgldfp020000400006000080000100000iterations0.5340.5360.5380.5400.5420.544Negative loglikelihoodN=10302000004000006000008000001000000iterations0.50600.50650.50700.50750.50800.50850.5090N=1040.00.20.40.60.81.0iterations1e70.50220.50240.50260.50280.50300.50320.50340.5036N=105sgldsgldfpsgdReferences
[1] S. Ahn  A. K. Balan  and M. Welling. Bayesian posterior sampling via stochastic gradient
Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning 
ICML 2012  Edinburgh  Scotland  UK  June 26 - July 1  2012  2012.

[2] S. Ahn  B. Shahbaba  and M. Welling. Distributed stochastic gradient MCMC. In E. P. Xing
and T. Jebara  editors  Proceedings of the 31st International Conference on Machine Learning 
volume 32 of Proceedings of Machine Learning Research  pages 1044–1052  Bejing  China 
22–24 Jun 2014. PMLR.

[3] J. Baker  P. Fearnhead  E. B. Fox  and C. Nemeth. Control variates for stochastic gradient

MCMC. ArXiv e-prints 1706.05439  June 2017.

[4] R. Bardenet  A. Doucet  and C. Holmes. On Markov chain Monte Carlo methods for tall data.

Journal of Machine Learning Research  18(47):1–43  2017.

[5] N. S. Chatterji  N. Flammarion  Y.-A. Ma  P. L. Bartlett  and M. I. Jordan. On the theory of
variance reduction for stochastic gradient Monte Carlo. ArXiv e-prints 1802.05431  Feb. 2018.

[6] C. Chen  N. Ding  and L. Carin. On the convergence of Stochastic Gradient MCMC algorithms
with high-order integrators. In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and
R. Garnett  editors  Advances in Neural Information Processing Systems 28  pages 2278–2286.
Curran Associates  Inc.  2015.

[7] C. Chen  W. Wang  Y. Zhang  Q. Su  and L. Carin. A convergence analysis for a class of
practical variance-reduction stochastic gradient MCMC. ArXiv e-prints 1709.01180  Sept. 2017.

[8] T. Chen  E. Fox  and C. Guestrin. Stochastic gradient hamiltonian Monte Carlo. In Proceedings

of the 31st International Conference on Machine Learning  pages 1683–1691  2014.

[9] A. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  79(3):
651–676  2017.

[10] A. Dalalyan. Further and stronger analogy between sampling and optimization: Langevin
Monte Carlo and gradient descent. In S. Kale and O. Shamir  editors  Proceedings of the 2017
Conference on Learning Theory  volume 65 of Proceedings of Machine Learning Research 
pages 678–689  Amsterdam  Netherlands  07–10 Jul 2017. PMLR.

[11] A. S. Dalalyan and A. G. Karagulyan. User-friendly guarantees for the Langevin Monte Carlo

with inaccurate gradient. ArXiv e-prints 1710.00095  Sept. 2017.

[12] N. Ding  Y. Fang  R. Babbush  C. Chen  R. D. Skeel  and H. Neven. Bayesian sampling using
stochastic gradient thermostats. In Proceedings of the 27th International Conference on Neural
Information Processing Systems - Volume 2  NIPS’14  pages 3203–3211  Cambridge  MA  USA 
2014. MIT Press.

[13] K. A. Dubey  S. J. Reddi  S. A. Williamson  B. Poczos  A. J. Smola  and E. P. Xing. Variance
reduction in stochastic gradient Langevin dynamics. In D. D. Lee  M. Sugiyama  U. V. Luxburg 
I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages
1154–1162. Curran Associates  Inc.  2016.

[14] A. Durmus and E. Moulines. High-dimensional Bayesian inference via the unadjusted Langevin

algorithm. ArXiv e-prints 1605.01559  May 2016.

[15] A. Durmus and E. Moulines. Nonasymptotic convergence analysis for the unadjusted Langevin

algorithm. Ann. Appl. Probab.  27(3):1551–1587  06 2017. doi: 10.1214/16-AAP1238.

[16] U. Grenander. Tutorial in pattern theory. Division of Applied Mathematics  Brown University 

Providence  1983.

[17] U. Grenander and M. I. Miller. Representations of knowledge in complex systems. J. Roy.
Statist. Soc. Ser. B  56(4):549–603  1994. ISSN 0035-9246. With discussion and a reply by the
authors.

10

[18] L. Hasenclever  S. Webb  T. Lienart  S. Vollmer  B. Lakshminarayanan  C. Blundell  and Y. W.
Teh. Distributed Bayesian learning with stochastic natural gradient expectation propagation and
the posterior server. Journal of Machine Learning Research  18(106):1–37  2017.

[19] E. Jones  T. Oliphant  P. Peterson  et al. SciPy: Open source scientiﬁc tools for Python  2001.

[20] I. Karatzas and S. Shreve. Brownian motion and stochastic calculus. Graduate Texts in

Mathematics. Springer New York  1991. ISBN 9780387976556.

[21] A. Korattikara  Y. Chen  and M. Welling. Austerity in MCMC land: cutting the Metropolis-
hastings budget. In Proceedings of the 31st International Conference on International Confer-
ence on Machine Learning - Volume 32  ICML’14  pages I–181–I–189. JMLR.org  2014.

[22] C. Li  C. Chen  D. Carlson  and L. Carin. Preconditioned stochastic gradient Langevin dynamics
In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial

for deep neural networks.
Intelligence  AAAI’16  pages 1788–1794. AAAI Press  2016.

[23] W. Li  S. Ahn  and M. Welling. Scalable MCMC for mixed membership stochastic blockmodels.

In Artiﬁcial Intelligence and Statistics  pages 723–731  2016.

[24] Y.-A. Ma  T. Chen  and E. Fox. A complete recipe for stochastic gradient MCMC. In C. Cortes 
N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Garnett  editors  Advances in Neural Informa-
tion Processing Systems 28  pages 2917–2925. Curran Associates  Inc.  2015.

[25] T. Nagapetyan  A. B. Duncan  L. Hasenclever  S. J. Vollmer  L. Szpruch  and K. Zygalakis. The

true cost of stochastic gradient Langevin dynamics. ArXiv e-prints 1706.02692  June 2017.

[26] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach
to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009. doi:
10.1137/070704277.

[27] S. Patterson and Y. W. Teh. Stochastic gradient riemannian Langevin dynamics on the probability
simplex. In C. J. C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K. Q. Weinberger 
editors  Advances in Neural Information Processing Systems 26  pages 3102–3110. Curran
Associates  Inc.  2013.

[28] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research  12:2825–2830  2011.

[29] G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and their
discrete approximations. Bernoulli  2(4):341–363  1996. ISSN 1350-7265. doi: 10.2307/
3318418.

[30] I. Sato and H. Nakagawa. Approximation analysis of stochastic gradient Langevin dynamics by
using Fokker-Planck equation and Ito process. In E. P. Xing and T. Jebara  editors  Proceedings
of the 31st International Conference on Machine Learning  volume 32 of Proceedings of
Machine Learning Research  pages 982–990  Bejing  China  22–24 Jun 2014. PMLR.

[31] Y. W. Teh  A. H. Thiery  and S. J. Vollmer. Consistency and ﬂuctuations for stochastic gradient

Langevin dynamics. The Journal of Machine Learning Research  17(1):193–225  2016.

[32] S. J. Vollmer  K. C. Zygalakis  and Y. W. Teh. Exploration of the (non-)asymptotic bias and
variance of stochastic gradient Langevin dynamics. Journal of Machine Learning Research  17
(159):1–48  2016.

[33] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on International Conference on Machine
Learning  ICML’11  pages 681–688  USA  2011. Omnipress. ISBN 978-1-4503-0619-5.

11

,Nicolas Brosse
Alain Durmus
Eric Moulines
Bai Li
Changyou Chen
Wenlin Wang
Lawrence Carin