2012,Multiclass Learning with Simplex Coding,In this paper we dicuss a novel  framework for multiclass learning   defined by  a suitable coding/decoding strategy   namely the simplex coding  that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework a  relaxation error analysis can be developed avoiding constraints on the considered hypotheses class.  Moreover  we show that in this setting it is possible to derive the first provably consistent  regularized methods with training/tuning complexity which is {\em independent} to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.,Multiclass Learning with Simplex Coding

Youssef Mroueh(cid:93) ‡  Tomaso Poggio(cid:93) ‡  Lorenzo Rosasco(cid:93) ‡ Jean-Jacques E. Slotine†

(cid:93) - CBCL  McGovern Institute  MIT;† -LCSL  MIT- IIT; † - ME  BCS  MIT

ymroueh  lrosasco jjs@mit.edu tp@ai.mit.edu

Abstract

In this paper we discuss a novel framework for multiclass learning  deﬁned by
a suitable coding/decoding strategy  namely the simplex coding  that allows us
to generalize to multiple classes a relaxation approach commonly used in binary
classiﬁcation. In this framework  we develop a relaxation error analysis that avoids
constraints on the considered hypotheses class. Moreover  using this setting we
derive the ﬁrst provably consistent regularized method with training/tuning com-
plexity that is independent to the number of classes. We introduce tools from
convex analysis that can be used beyond the scope of this paper.

1

Introduction

As bigger and more complex datasets are available  multiclass learning is becoming increasingly im-
portant in machine learning. While theory and algorithms for solving binary classiﬁcation problems
are well established  the problem of multicategory classiﬁcation is much less understood. Practical
multiclass algorithms often reduce the problem to a collection of binary classiﬁcation problems. Bi-
nary classiﬁcation algorithms are often based on a relaxation approach: classiﬁcation is posed as a
non-convex minimization problem and then relaxed to a convex one  deﬁned by suitable convex loss
functions. In this context  results in statistical learning theory quantify the error incurred by relax-
ation and in particular derive comparison inequalities explicitly relating the excess misclassiﬁcation
risk to the excess expected loss. We refer to [2  27  14  29] and [18] Chapter 3 for an exhaustive
presentation as well as generalizations.
Generalizing the above approach and results to more than two classes is not straightforward. Over
the years  several computational solutions have been proposed (among others  see [10  6  5  25  1 
21]). Indeed  most of these methods can be interpreted as a kind of relaxation. Most proposed
methods have complexity which is more than linear in the number of classes and simple one-vs
all in practice offers a good alternative both in terms of performance and speed [15]. Much fewer
works have focused on deriving theoretical guarantees. Results in this sense have been pioneered
by [28  20]  see also [11  7  23]. In these works the error due to relaxation is studied asymptotically
and under constraints on the function class to be considered. More quantitative results in terms of
comparison inequalities are given in [4] under similar restrictions (see also [19]). Notably  the above
results show that seemingly intuitive extensions of binary classiﬁcation algorithms might lead to
methods which are not consistent. Further  it is interesting to note that the restrictions on the func-
tion class  needed to prove the theoretical guarantees  make the computations in the corresponding
algorithms more involved and are in fact often ignored in practice.
In this paper we dicuss a novel framework for multiclass learning  deﬁned by a suitable cod-
ing/decoding strategy  namely the simplex coding  in which a relaxation error analysis can be devel-
oped avoiding constraints on the considered hypotheses class. Moreover  we show that in this frame-
work it is possible to derive the ﬁrst provably consistent regularized method with training/tuning
complexity that is independent to the number of classes. Interestingly  using the simplex coding 
we can naturally generalize results  proof techniques and methods from the binary case  which is
recovered as a special case of our theory. Due to space restriction in this paper we focus on exten-
sions of least squares  and SVM loss functions  but our analysis can be generalized to a large class

1

of simplex loss functions  including extensions of the logistic and exponential loss functions (used
in boosting). Tools from convex analysis are developed in the supplementary material and can be
useful beyond the scope of this paper  in particular in structured prediction.
The rest of the paper is organized as follow. In Section 2 we discuss the problem statement and
background. In Section 3 we discuss the simplex coding framework which we analyze in Section
4. Algorithmic aspects and numerical experiments are discussed in Section 5 and Section 6  respec-
tively. Proofs and supplementary technical results are given in the appendices.

(cid:80)n

2 Problem Statement and Previous Work
Let (X  Y ) be two random variables with values in two measurable spaces X and Y = {1 . . . T} 
T ≥ 2. Denote by ρX   the law of X on X   and by ρj(x)  the conditional probabilities for j ∈ Y. The
data is a sample S = (xi  yi)n
i=1  from n identical and independent copies of (X  Y ). We can think of
X as a set of possible inputs and of Y as a set of labels describing a set of semantic categories/classes
the input can belong to. A classiﬁcation rule is a map b : X → Y  and its error is measured by the
misclassiﬁcation risk R(b) = P(b(X) (cid:54)= Y ) = E(1I[b(x)(cid:54)=y](X  Y )). The optimal classiﬁcation rule
that minimizes R is the Bayes rule bρ(x) = arg maxy∈Y ρy(x)  x ∈ X . Computing the Bayes rule
by directly minimizing the risk R is not possible since the probability distribution is unknown. One
might think of minimizing the empirical risk (ERM) RS(b) = 1
i=1 1I[b(x)(cid:54)=y](xi  yi)  which is an
n
unbiased estimator of the R  but the corresponding optimization problem is in general not feasible.
In binary classiﬁcation  one of the most common ways to obtain computationally efﬁcient methods
is based on a relaxation approach. We recall this approach in the next section and describe its exten-
sion to multiclass in the rest of the paper.
Relaxation Approach to Binary Classiﬁcation. If T = 2  we can set Y = {±1}. Most mod-
ern machine learning algorithms for binary classiﬁcation consider a convex relaxation of the ERM
functional RS. More precisely: 1) the indicator function in RS is replaced by non negative loss
V : Y × R → R+ which is convex in the second argument and is sometimes called a surrogate loss;
2) the classiﬁcation rule b replaced by a real valued measurable function f : X → R. A classiﬁca-
tion rule is then obtained by considering the sign of f. It often sufﬁces to consider a special class
of loss functions  namely large margin loss functions V : R → R+ of the form V (−yf (x)). This
last expression is suggested by the observation that the misclassiﬁcation risk  using the labels ±1 
can be written as R(f ) = E(Θ(−Y f (X)))  where Θ is the Heaviside step function. The quantity
m = −yf (x)  sometimes called the margin  is a natural point-wise measure of the classiﬁcation
error. Among other examples of large margin loss functions (such as the logistic and exponential
loss)  we recall the hinge loss V (m) = |1 + m|+ = max{1 + m  0} used in the support vec-
tor machine  and the square loss V (m) = (1 + m)2 used in regularized least squares (note that
(1 − yf (x))2 = (y − f (x))2). Using large margin loss functions it is possible to design effective
learning algorithms replacing the empirical risk with regularized empirical risk minimization

n(cid:88)

i=1

E λ
S (f ) =

1
n

V (yi  f (xi)) + λR(f ) 

(1)

where R is a suitable regularization functional and λ is the regularization parameter  (see Section
5).

2.1 Relaxation Error Analysis

As we replace the misclassiﬁcation loss with a convex surrogate loss  we are effectively changing
the problem: the misclassiﬁcation risk is replaced by the expected loss  E(f ) = E(V (−Y f (X))) .
The expected loss can be seen as a functional on a large space of functions F = FV ρ  which depend
on V and ρ. Its minimizer  denoted by fρ  replaces the Bayes rule as the target of our algorithm.
The question arises of the price we pay by a considering a relaxation approach: “What is the rela-
tionship between fρ and bρ?” More generally  “What is the price we incur into by estimating the
expected risk rather than the misclassiﬁcation risk?” The relaxation error for a given loss function
can be quantiﬁed by the following two requirements:
1) Fisher Consistency. A loss function is Fisher consistent if sign(fρ(x)) = bρ(x) almost surely
(this property is related to the notion of classiﬁcation-calibration [2]).

2

2) Comparison inequalities. The excess misclassiﬁcation risk  and the excess expected loss are
related by a comparison inequality

R(sign(f )) − R(bρ) ≤ ψ(E(f ) − E(fρ)) 

for any function f ∈ F  where ψ = ψV ρ is a suitable function that depends on V   and possibly
on the data distribution. In particular ψ should be such that ψ(s) → 0 as s → 0  so that if fn
is a (possibly random) sequence of functions  such that E(fn) → E(fρ) (possibly in probability) 
then the corresponding sequences of classiﬁcation rules cn = sign(fn) is Bayes consistent  i.e.
R(cn) → R(bρ) (possibly in probability).
If ψ is explicitly known  then bounds on the excess
expected loss yield bounds on the excess misclassiﬁcation risk.
The relaxation error in the binary case has been thoroughly studied in [2  14]. In particular  Theorem
2 in [2] shows that if a large margin surrogate loss is convex  differentiable and decreasing in a
neighborhood of 0  then the loss is Fisher consistent. Moreover  in this case it is possible to give
√
an explicit expression for the function ψ.
In particular  for the hinge loss the target function is
exactly the Bayes rule and ψ(t) = |t|. For least squares  fρ(x) = 2ρ1(x) − 1  and ψ(t) =
t.
The comparison inequality for the square loss can be improved for a suitable class of probability
distribution satisfying the so-called Tsybakov noise condition [22]  ρX ({x ∈ X  |fρ(x)| ≤ s}) ≤
Bqsq  s ∈ [0  1]  q > 0. Under this condition the probability of points such that ρy(x) ∼ 1
2 decreases
polynomially. In this case the comparison inequality for the square loss is given by ψ(t) = cqt
q+2  
see [2  27].
Previous Works in Multiclass Classiﬁcation. From a practical perspective  over the years  several
computational solutions to multiclass learning have been proposed. Among others  we mention
for example [10  6  5  25  1  21]. Indeed  most of the above methods can be interpreted as a kind
of relaxation of the original multiclass problem. Interestingly  the study in [15] suggests that the
simple one-vs all schemes should be a practical benchmark for multiclass algorithms as it seems
experimentally to achive performance that is similar or better than more sophisticated methods.
As we previously mentioned from a theoretical perspective a general account of a large class of
multiclass methods has been given in [20]  building on results in [2] and [28]. Notably  these results
show that seemingly intuitive extensions of binary classiﬁcation algorithms can lead to inconsistent
methods. These results  see also [11  23]  are developed in a setting where a classiﬁcation rule
is found by applying a suitable prediction/decoding map to a function f : X → RT where f is
found considering a loss function V : Y × RT → R+. The considered functions have to satisfy
y∈Y f y(x) = 0  for all x ∈ X . The latter requirement is problematic as it makes
the computations in the corresponding algorithms more involved. It is in fact often ignored  so that
practical algorithms often come with no consistency guarantees. In all the above papers relaxation
is studied in terms of Fisher and Bayes consistency and the explicit form of the function ψ is not
given. More quantitative results in terms of explicit comparison inequality are given in [4] and (see
also [19])  but also need to to impose the ”sum to zero” constraint on the considered function class.

the constraint(cid:80)

q+1

3 A Relaxation Approach to Multicategory Classiﬁcation

In this section we propose a natural extension of the relaxation approach that avoids constraining
the class of functions to be considered  and allows us to derive explicit comparison inequalities. See
Remark 1 for related approaches.

α

c1

c2

c3

Figure 1: Decoding with simplex coding T = 3.

Simplex Coding. We start by considering a suitable coding/decoding strategy. A coding map turns
a label y ∈ Y into a code vector. The corresponding decoding map given a vector returns a label in

3

Y. Note that this is what we implicitly did while treating binary classiﬁcation we encoded the label
space Y = {1  2} using the coding ±1  so that the naturally decoding strategy is simply sign(f (x)).
The coding/decoding strategy we study here is described by the following deﬁnition.
Deﬁnition 1 (Simplex Coding). The simplex coding is a map C : Y → RT−1  C(y) = cy 
where the code vectors C = {cy | y ∈ Y} ⊂ RT−1 satisfy: 1) (cid:107)cy(cid:107)2 = 1  ∀y ∈ Y  2)(cid:104)cy  cy(cid:48)(cid:105) =
− 1
y∈Y cy = 0. The corresponding decoding is the map
D : RT−1 → {1  . . .   T}  D(α) = arg maxy∈Y (cid:104)α  cy(cid:105)  ∀α ∈ RT−1.

T−1   for y (cid:54)= y(cid:48) with y  y(cid:48) ∈ Y  and 3)(cid:80)

The simplex coding has been considered in [8] [26]  and [16]. It corresponds to T maximally sep-
arated vectors on the hypersphere ST−2 in RT−1  that are the vertices of a simplex (see Figure 1).
For binary classiﬁcation it reduces to the ±1 coding and the decoding map is equivalent to taking
the sign of f. The decoding map has a natural geometric interpretation: an input point is mapped
to a vector f (x) by a function f : X → RT−1  and hence assigned to the class having closest code
vector ( for y  y(cid:48) ∈ Y and α ∈ RT−1  we have (cid:107)cy − α(cid:107)2 ≥ (cid:107)cy(cid:48) − α(cid:107)2 ⇔ (cid:104)cy(cid:48)  α(cid:105) ≤ (cid:104)cy  α(cid:105)).
Relaxation for Multiclass Learning. We use the simplex coding to propose an extension of binary
classiﬁcation. Following the binary case  the relaxation can be described in two steps:

1. using the simplex coding  the indicator function is upper bounded by a non-negative loss
function V : Y ×RT−1 → R+  such that 1I[b(x)(cid:54)=y](x  y) ≤ V (y  C(b(x)))  for all b : X →
Y  and x ∈ X   y ∈ Y 

2. rather than C ◦ b we consider functions with values in f : X → RT−1  so that

V (y  C(b(x))) ≤ V (y  f (x))  for all b : X → Y  f : X → RT−1 and x ∈ X   y ∈ Y.

In the next section we discuss several loss functions satisfying the above conditions and we study in
particular the extension of the least squares and SVM loss functions.
Multiclass Simplex Loss Functions. Several loss functions for binary classiﬁcation can be natu-
rally extended to multiple classes using the simplex coding. Due to space restriction  in this paper
we focus on extensions of the least squares and SVM loss functions  but our analysis can be general-
ized to a large class of loss functions  including extensions of logistic and exponential loss functions
(used in boosting). The Simplex Least Square loss (S-LS) is given by V (y  f (x)) = (cid:107)cy − f (x)(cid:107)2 
and reduces to the usual least square approach to binary classiﬁcation for T = 2. One natural
extension of the SVM’s hinge loss in this setting would be to consider the Simplex Half space
SVM loss (SH-SVM) V (y  f (x)) = |1 − (cid:104)cy  f (x)(cid:105)|+. We will see in the following that while
this loss function would induce efﬁcient algorithms in general is not Fisher consistent unless fur-
ther constraints are assumed. These latter constraints would considerably slow down the computa-
tions. We then consider a second loss function Simplex Cone SVM (SC-SVM)  which is deﬁned as
. The latter loss function is related to the one considered
in the multiclass SVM proposed in [10]. We will see that it is possible to quantify the relaxation er-
ror of the loss function without requiring further constraints. Both of the above SVM loss functions
reduce to the binary SVM hinge loss if T = 2.

T−1 + (cid:104)cy(cid:48)  f (x)(cid:105)(cid:12)(cid:12)(cid:12)+
(cid:12)(cid:12)(cid:12) 1

V (y  f (x)) =(cid:80)

y(cid:48)(cid:54)=y

(cid:80)
Remark 1 (Related approaches). An SVM loss is considered in [8] where V (y  f (x)) =
y(cid:48)(cid:54)=y |ε − (cid:104)f (x)  vy(cid:48)(y)(cid:105)|+ and vy(cid:48)(y) = cy−cy(cid:48)
(cid:107)cy−cy(cid:48)(cid:107)   with ε = (cid:104)cy  vy(cid:48)(y)(cid:105) = 1√
T−1 . More
class boosting loss was introduced in [16]  in our notation V (y  f (x)) = (cid:80)
recently [26] considered the loss function V (y  f (x)) = |(cid:107)cy − f (x)(cid:107) − ε|+  and a simplex multi-
j(cid:54)=y e−(cid:104)cy−cy(cid:48)  f (x)(cid:105).
While all those losses introduce a certain notion of margin that makes use of the geometry of the
simplex coding  it is not to clear how to derive explicit comparison theorems and moreover the com-
putational complexity of the resulting algorithms scales linearly with the number of classes in the
case of the losses considered in [16  26] and O((nT )γ)  γ ∈ {2  3} for losses considered in [8] .

2

(cid:113) T

4

Figure 2: Level sets of the different losses considered for T = 3. A classiﬁcation is correct if an
input (x  y) is mapped to a point f (x) that lies in the neighborhood of the vertex cy. The shape of
the neighborhood is deﬁned by the loss. It takes the form of a cone supported on a vertex  in the
case of SC-SVM  a half space delimited by the hyperplane orthogonal to the vertex in the case of
the SH-SVM  and a sphere centered on the vertex  in the case of S-LS.

4 Relaxation Error Analysis
If we consider the simplex coding  a function f taking values in RT−1  and the decoding operator
X (1 − ρD(f (x)))dρX (x). Then 
following a relaxation approach  we replace the misclassiﬁcation loss by the expected risk induced
by one of the loss functions V deﬁned in the previous section. As in the binary case we consider
ρ =

D  the misclassiﬁcation risk can also be written as: R(D(f )) =(cid:82)
the expected loss E(f ) = (cid:82) V (y  f (x))dρ(x  y). Let Lp(X   ρX ) = {f : X → RT−1 | (cid:107)f(cid:107)p
(cid:82) (cid:107)f (x)(cid:107)p dρX (x) < ∞}  p ≥ 1.

The following theorem studies the relaxation error for SH-SVM  SC-SVM  and S-LS loss functions.
Theorem 1. For SH-SVM  SC-SVM  and S-LS loss functions  there exists a p such that E :
Lp(X   ρX ) → R+ is convex and continuous. Moreover 

1. The minimizer fρ of E over F = {f ∈ Lp(X   ρX ) | f (x) ∈ K a.s.} exists and D(fρ) = bρ.
2. For any f ∈ F  R(D(f )) − R(D(fρ)) ≤ CT (E(f ) − E(fρ))α  where the expressions of

p  K  fρ  CT   and α are given in Table 1.

p K

conv(C)

Loss
SH-SVM 1
SC-SVM 1 RT−1
2 RT−1
S-LS
Table 1: conv(C) is the convex hull of the set C deﬁned in (1).

(cid:113) 2(T−1)

CT
T − 1
T − 1

fρ
cbρ
cbρ

(cid:80)

α
1
1

1
2

y∈Y ρycy

T

The proof of this theorem is given  in Theorems 1 and 2 for S-LS  and Theorems 3  and 4 for SC-
SVM and SH-SVM respectively  in Appendix B.
The above theorem can be improved for Least Squares under certain classes of distribution . Toward
this end we introduce the following notion of misclassiﬁcation noise that generalizes Tsybakov’s
noise condition.
Deﬁnition 2. Fix q > 0  we say that the distribution ρ satisﬁes the multiclass noise condition with
parameter Bq  if

(cid:27)(cid:19)
((cid:10)cD(fρ(x)) − cj  fρ(x)(cid:11)) ≤ s

≤ Bqsq 

(2)

(cid:18)(cid:26)

ρX

x ∈ X | 0 ≤ min

j(cid:54)=D(fρ(x))

T − 1
T

where s ∈ [0  1].

5

1

If a distribution ρ is characterized by a very large q  then  for each x ∈ X   fρ(x) is arbitrarily close
to one of the coding vectors. For T = 2  the above condition reduces to the binary Tsybakov noise.
Indeed  let c1 = 1  and c2 = −1  if fρ(x) > 0  1
2 (c1 − c2)fρ(x) = fρ(x)  and if fρ(x) < 0 
2 (c2 − c1)fρ(x) = −fρ(x).
The following result improves the exponent of simplex-least square to q+1
Theorem 2. For each f ∈ L2(X   ρX )  if (2) holds  then for S-LS we have the following inequality 

2 :
q+2 > 1

(cid:18) 2(T − 1)

T

(cid:19) q+1

q+2

R(D(f )) − R(D(fρ)) ≤ K

with K =(cid:0)2(cid:112)Bq + 1(cid:1) 2q+2

q+2 .

(E(f ) − E(fρ))

 

(3)

Remark 2. Note that the comparison inequalities show a tradeoff between the exponent α and the
constant C(T ) for S-LS and SVM losses. While the constant is order T for SVM it is order 1 for S-
LS  on the other hand the exponent is 1 for SVM losses and 1
2 for S-LS. The latter could be enhanced
to 1 for close to separable classiﬁcation problems by virtue of the Tsybakov noise condition.
Remark 3. The comparison inequalities given in Theorems 1 and 2 can be used to derive gener-
alization bounds on the excess misclassiﬁcation risk. For least squares min-max sharp bound  for
vector valued regression are known [3].
Standard techniques for deriving sample complexity bounds in binary classiﬁcation extended for
multi-class SVM losses  are found in [7] and could be adapted to our setting. The obtained bound
are not known to be tight. Better bounds akin to those in [18]  will be subject of future work.

5 Computational Aspects and Regularization Algorithms

i2

u(cid:62)

(cid:33)

1 − 1

  C[2] = [1−1]  where u = (− 1

(cid:32)1
v C[i] ×(cid:113)

The simplex coding framework allows us to extend batch and online kernel methods to the Multi-
class setting.
Computing the Simplex Coding. We begin by noting that the simplex coding can be easily com-
i ···− 1
puted via the recursion: C[i+1] =
i )
(column vector in Ri) and v = (0  . . .   0)(column vector in Ri−1) (see Algorithm C.1). Indeed we
have the following result (see the Appendix C.1 for the proof).
Lemma 1. The T columns of C[T ] are a set of T − 1 dimensional vectors satisfying the properties
of Deﬁnition 1.
The above algorithm stems from the observation that the simplex in RT−1 can be obtained by pro-
jecting the simplex in RT onto the hyperplane orthogonal to the element (1  . . .   0) of the canonical
basis in RT .
Regularized Kernel Methods. We consider regularized methods of the form (1)  induced by sim-
plex loss functions and where the hypothesis space is a vector-valued reproducing kernel Hilbert
space H(VV-RKHS) the regularizer is the corresponding norm ||f||2H. See Appendix D.2 for a brief
introduction to VV-RKHS.
In the following  we consider a class of kernels K such that if f minimizes (1) for R(f ) = ||f||2H
i=1 K(x  xi)ai  ai ∈ RT−1 [12]  where we note that the coefﬁcients
are vectors in RT−1. In the case that the kernel is induced by a ﬁnite dimensional feature map 
k(x  x(cid:48)) = (cid:104)Φ(x)  Φ(x(cid:48))(cid:105)   where Φ : X → Rp  and (cid:104)· ·(cid:105) is the inner product in Rp  we can
write each function in H as f (x) = W Φ(x)  where W ∈ R(T−1)×p.
(cid:80)n
It is known [12] that the representer theorem [9] can be easily extended to a vector valued set-
ting  so that that minimizer of a simplex version of Tikhonov regularization is given by f λ
S (x) =
j=1 k(x  xj)aj  aj ∈ RT−1  for all x ∈ X   where the explicit expression of the coefﬁcients
depends on the considered loss function. We use the following notation: K ∈ Rn×n  Kij =
k(xi  xj) ∀i  j ∈ {1 . . . n}  A ∈ Rn×(T−1)  A = (a1  ...  an)T .
Simplex Regularized Least squares (S-RLS). S-RLS is obtained by substituting the simplex least
square loss in the Tikhonov functional.
It is easy to see [15] that in this case the coefﬁcients

we have that f (x) = (cid:80)n

6

must satisfy either (K + λnI)A = ˆY or ( ˆX T ˆX + λnI)W = ˆX T ˆY in the linear case  where
ˆX ∈ Rn×p  ˆX = (Φ(x1)  ...  Φ(xn))(cid:62) and ˆY ∈ Rn×(T−1)  ˆY = (cy1   ...  cyn )(cid:62) .
Interestingly  the classical results from [24] can be extended to show that the value fSi(xi)  obtained
computing the solution fSi removing the i − th point from the training set (the leave one out so-
loo ∈ Rn×(T−1)  f λ
lution)  can be computed in closed form. Let f λ
(xn)).
Let K(λ) = (K + λnI)−1and C(λ) = K(λ) ˆY . Deﬁne M (λ) ∈ Rn×(T−1)  such that:
(cid:80)n
loo = ˆY − C(λ) (cid:12) M (λ)  where
M (λ)ij = 1/K(λ)ii  ∀ j = 1 . . . T − 1. One can show that f λ
(cid:12) is the Hadamard product [15]. Then  the leave-one-out error 1
i=1 1Iy(cid:54)=D(fSi (x))(yi  xi)  can
be minimized at essentially no extra cost by precomputing the eigen decomposition of K (or ˆX T ˆX).
Simplex Cone Support Vector Machine (SC-SVM). Using standard reasoning it is easy to show
that (see Appendix C.2)  for the SC-SVM the coefﬁcients in the representer theorem are given by
i )y∈Y ∈ RT   i = 1  . . .   n  solve the quadratic

ai = −(cid:80)

i = 1  . . .   n  where αi = (αy

(x1)  . . .   f λ
Sn

loo = (f λ
S1

i cy 

αy

n

y(cid:54)=yi

programming (QP) problem

− 1

max

α1 ... αn∈RT
2
subject to 0 ≤ αy

n(cid:88)
(cid:88)
y y(cid:48) i j
i ≤ C0δy yi   ∀ i = 1  . . .   n  y ∈ Y

i KijGyy(cid:48)αy(cid:48)
αy

T − 1

j +

i=1

1

T(cid:88)

y=1



αy
i

2nλ  αi = (αy

i )y∈Y ∈ RT   for i = 1  . . .   n and δi j
where Gy y(cid:48) = (cid:104)cy  cy(cid:48)(cid:105)∀y  y(cid:48) ∈ Y and C0 = 1
is the Kronecker delta.
Simplex Halfspaces Support Vector Machine (SH-SVM). A similar  yet more more complicated
procedure  can be derived for the SH-SVM. Here  we omit this derivation and observe instead that
if we neglect the convex hull constraint from Theorem 1  that requires f (x) ∈ co(C) for almost all
x ∈ X   then the SH-SVM has an especially simple formulation at the price of loosing consistency
guarantees. In fact  in this case the coefﬁcients are given by ai = αicyi 
i = 1  . . .   n  where
αi ∈ R  with i = 1  . . .   n solve the quadratic programming (QP) problem

(4)

(cid:88)

n(cid:88)

max

α1 ... αn∈R− 1
subject to 0 ≤ αi ≤ C0  ∀ i = 1 . . . n 

αiKijGyiyj αj +

i=1

i j

2

αi

2nλ. The latter formulation can be solved at the same complexity of the binary SVM

where C0 = 1
(worst case O(n3)) but lacks consistency.
Online/Incremental Optimization The regularized estimators induced by the simplex loss func-
tions can be computed by means of online/incremental ﬁrst order (sub) gradient methods. Indeed 
when considering ﬁnite dimensional feature maps  these strategies offer computationally feasible so-
lutions to train estimators for large datasets where neither a p by p or an n by n matrix ﬁt in memory.
Following [17] we can alternate a step of stochastic descent on a data point : Wtmp = (1− ηiλ)Wi −
)Wtmp (See
ηi∂(V (yi  fWi(xi))) and a projection on the Frobenius ball Wi = min(1 
Algorithn C.5 for details.) The algorithm depends on the used loss function through computation of
the (point-wise) subgradient ∂(V ). The latter can be easily computed for all the loss functions previ-
ously discussed. For the SLS loss we have ∂(V (yi  fW (xi))) = 2(cyi − W xi)x(cid:62)
i   while for the SC-
T−1}.
For the SH-SVM loss we have: ∂(V (y  fW (xi))) = −cyix(cid:62)

SVM loss we have ∂(V (yi  fW (xi))) = ((cid:80)

i where Ii = {y (cid:54)= yi|(cid:104)cy  W xi(cid:105) > − 1
if cyiW xi < 1 and 0 otherwise .

λ||Wtmp||F

ck)x(cid:62)

k∈Ii

√

1

i

5.1 Comparison of Computational Complexity

The cost of solving S-RLS for ﬁxed λ is in the worst case O(n3) (for example via Cholesky decom-
position). If we are interested in computing the regularization path for N regularization parameter
values  then as noted in [15] it might be convenient to perform an eigendecomposition of the ker-
nel matrix rather than solving the systems N times. For explicit p−dimensional feature maps the
cost is O(np2)  so that the cost of computing the regularization path for simplex RLS algorithm is
O(min(n3  np2)) and hence independent of T . One can contrast this complexity with that of a n¨aive
One Versus All (OVA) approach that would lead to a O(N n3T ) complexity. Simplex SVMs can be
solved using solvers available for binary SVMs that are considered to have complexity O(nγ) with
γ ∈ {2  3}(the actual complexity scales with the number of support vectors) . For SC-SVM  though 

7

we have nT rather than n unknowns and the complexity is (O(nT )γ). SH-SVM in which we omit
the constraint  can be trained with the same complexity as the binary SVM (worst case O(n3)) but
lacks consistency. Note that unlike for S-RLS  there is no straightforward way to compute the regu-
larization path and the leave one out error for any of the above SVM. The online algorithms induced
by the different simplex loss functions are essentially the same. In particular  each iteration depends
linearly on the number of classes.

6 Numerical Results

We conduct several experiments to evaluate the performance of our batch and online algorithms 
on 5 UCI datasets as listed in Table 2  as well as on Caltech101 and Pubﬁg83. We compare the
performance of our algorithms to one versus all svm (libsvm)   as well as simplex- based boosting
[16]. For UCI datasets we use the raw features  on Caltech101 we use hierarchical features (hmax) 
and on Pubﬁg83 we use the feature maps from [13]. In all cases the parameter selection is based
either on a hold out (ho) (80% training − 20% validation) or a leave one out error (loo). For the
model selection of λ in S-LS  100 values are chosen in the range [λmin  λmax] (where λmin and
λmax  correspond to the smallest and biggest eigenvalues of K). In the case of a Gaussian kernel
(rbf) we use a heuristic that sets the width of the Gaussian σ to the 25-th percentile of pairwise
distances between distinct points in the training set. In Table 2 we collect the resulting classiﬁcation
accuracies.

Letter

Isolet

SC-SVM Online (ho)
SH-SVM Online (ho)
S-LS Online (ho)
S-LS Batch (loo)
S-LS rbf Batch (loo)
SVM batch ova (ho)
SVM rbf batch ova (ho)
Simplex boosting [16]

Ctech

Pendigit

Landsat Optdigit
Pubﬁg83
65.15% 89.57% 81.62% 52.82% 88.58% 63.33% 84.70%
75.43% 85.58% 72.54% 38.40% 77.65% 45%
49.76%
63.62% 91.68% 81.39% 54.29% 92.62% 58.39% 83.61%
65.88% 91.90% 80.69% 54.96% 92.55% 66.35% 86.63%
90.15% 97.09% 98.17% 96.48% 97.05% 69.38% 86.75%
72.81% 92.13% 86.93% 62.78% 90.59% 70.13% 85.97%
95.33% 98.07% 98.88% 97.12% 96.99% 51.77% 85.60%
86.65% 92.82% 92.94% 59.65% 91.02% −

−

Table 2: Accuracies of our algorithms on several datasets.

As suggested by the theory  the consistent methods SC-SVM and S-LS have large performance
advantage over SH-SVM (where we omitted the convex hull constraint). Batch methods are overall
superior to online methods. Online SC-SVM achieves the best results among online methods. More
generally  we see that rbf S- LS has the best performance amongst the simplex methods  including
simplex boosting [16]. We see that S-LS rbf achieves essentially the same performance as One
Versus All SVM-rbf.

References
[1] Erin L. Allwein  Robert E. Schapire  and Yoram Singer. Reducing multiclass to binary: a
unifying approach for margin classiﬁers. Journal of Machine Learning Research  1:113–141 
2000.

[2] Peter L. Bartlett  Michael I. Jordan  and Jon D. McAuliffe. Convexity  classiﬁcation  and risk

bounds. Journal of the American Statistical Association  101(473):138–156  2006.

[3] A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Founda-

tions of Computational Mathematics  2006.

[4] D. Chen and T. Sun. Consistency of multiclass empirical risk minimization methods based in

convex loss. Journal of machine learning  X  2006.

[5] Crammer.K and Singer.Y. On the algorithmic implementation of multiclass kernel-based vector

machines. JMLR  2001.

[6] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-

correcting output codes. Journal of Artiﬁcial Intelligence Research  2:263–286  1995.

[7] Yann Guermeur. Vc theory of large margin multi-category classiers. Journal of Machine

Learning Research  8:2551–2594  2007.

8

[8] Simon I. Hill and Arnaud Doucet. A framework for kernel-based multi-category classiﬁcation.

J. Artif. Int. Res.  30(1):525–564  December 2007.

[9] G. Kimeldorf and G. Wahba. A correspondence between bayesian estimation of stochastic

processes and smoothing by splines. Ann. Math. Stat.  41:495–502  1970.

[10] Lee.Y  L.Yin  and Wahba.G. Multicategory support vector machines: Theory and application
to the classiﬁcation of microarray data and satellite radiance data. Journal of the American
Statistical Association  2004.

[11] Liu.Y. Fisher consistency of multicategory support vector machines. Eleventh International

Conference on Artiﬁcial Intelligence and Statistics  289-296  2007.

[12] C.A. Micchelli and M. Pontil. On learning vector–valued functions. Neural Computation 

17:177–204  2005.

[13] N. Pinto  Z. Stone  T. Zickler  and D.D. Cox. Scaling-up biologically-inspired computer vision:

A case-study on facebook. 2011.

[14] M.D. Reid and R.C. Williamson. Composite binary losses. JMLR  11  September 2010.
[15] Rifkin.R and Klautau.A. In defense of one versus all classiﬁcation. journal of machine learn-

ing  2004.

[16] Saberian.M and Vasconcelos .N. Multiclass boosting: Theory and algorithms. In NIPS 2011 

2011.

[17] Shai Shalev-Shwartz  Yoram Singer  and Nathan Srebro. Pegasos: Primal estimated sub-
gradient solver for svm. In Proceedings of the 24th ICML  ICML ’07  pages 807–814  New
York  NY  USA  2007. ACM.

[18] I. Steinwart and A. Christmann. Support vector machines. Information Science and Statistics.

Springer  New York  2008.

[19] Van de Geer.S Tarigan.B. A moment bound for multicategory support vector machines. JMLR

9  2171-2185  2008.

[20] A. Tewari and P. L. Bartlett. On the consistency of multiclass classiﬁcation methods.

In
Proceedings of the 18th Annual Conference on Learning Theory  volume 3559  pages 143–
157. Springer  2005.

[21] I. Tsochantaridis  T. Joachims  T. Hofmann  and Y. Altun. Large margin methods for structured

and interdependent output variables. JMLR  6(2):1453–1484  2005.

[22] Alexandre B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of

Statistics  32:135–166  2004.

[23] Elodie Vernet  Robert C. Williamson  and Mark D. Reid. Composite multiclass losses.

Proceedings of Neural Information Processing Systems (NIPS 2011)  2011.

In

[24] G. Wahba. Spline models for observational data  volume 59 of CBMS-NSF Regional Confer-

ence Series in Applied Mathematics. SIAM  Philadelphia  PA  1990.

[25] Weston and Watkins. Support vector machine for multi class pattern recognition. Proceedings

of the seventh european symposium on artiﬁcial neural networks  1999.

[26] Tong Tong Wu and Kenneth Lange. Multicategory vertex discriminant analysis for high-

dimensional data. Ann. Appl. Stat.  4(4):1698–1721  2010.

[27] Y. Yao  L. Rosasco  and A. Caponnetto. On early stopping in gradient descent learning. Con-

structive Approximation  26(2):289–315  2007.

[28] T. Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods.

Journal of Machine Learning Research  5:1225–1251  2004.

[29] Tong Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex

risk minimization. The Annals of Statistics  Vol. 32  No. 1  56134  2004.

9

,Antoine Bordes
Nicolas Usunier
Alberto Garcia-Duran
Jason Weston
Oksana Yakhnenko
Zelda Mariet
Suvrit Sra