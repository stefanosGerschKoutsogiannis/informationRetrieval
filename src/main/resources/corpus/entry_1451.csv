2010,Deterministic Single-Pass Algorithm for LDA,We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.,Deterministic Single-Pass Algorithm for LDA

Issei Sato

University of Tokyo  Japan
sato@r.dl.itc.u-tokyo.ac.jp

Kenichi Kurihara

Google

kenichi.kurihara@gmail.com

Hiroshi Nakagawa

University of Tokyo  Japan

n3@dl.itc.u-tokyo.ac.jp

Abstract

We develop a deterministic single-pass algorithm for latent Dirichlet alloca-
tion (LDA) in order to process received documents one at a time and then
discard them in an excess text stream. Our algorithm does not need to store
old statistics for all data. The proposed algorithm is much faster than a batch
algorithm and is comparable to the batch algorithm in terms of perplexity in
experiments.

1 Introduction

Huge quantities of text data such as news articles and blog
posts arrives in a continuous stream. Online learning has at-
tracted a great deal of attention as a useful method for han-
dling this growing quantity of streaming data because it pro-
cesses data one at a time  whereas batch algorithms are not
feasible in these settings because they need all the data at
the same time. This paper focus on online learning for La-
tent Dirichlet allocation (LDA) (Blei et al.  2003)  which is a
widely used probabilistic model for text data.

Figure 1: Overview of the relation-
ships among inferences.

Online learning for LDA has been already developed (Baner-
jee and Basu  2007; Alsumait et al.  2008; Canini et al.  2009;
Yao et al.  2009). Existing studies were based on sampling
methods such as the incremental Gibbs sampler and particle
ﬁlter. Sampling methods seem to be inappropriate for stream-
ing data because sampling methods have to represent a pos-
terior by using a lot of samples  which basically needs much
time. Moreover  sampling algorithms often need a resampling
step in which a sampling method is applied to old data. Storing old data or old samples adversely
affects the good properties of online algorithms. Particle ﬁlters also need to run m parallel process-
ing. A parallel algorithm needs more memory than a single-process algorithm  which is not useful
for a large quantity of data  especially in the case of a large vocabulary. For example  LDA needs to
store the number of words observed in a topic. If the number of topics is T   the vocabulary size is
V and m  so the required memory size is O(m (cid:3) T (cid:3) V ).
We propose two deterministic online algorithms; an incremental algorithms and a single-pass al-
gorithm. Our incremental algorithm is an incremental variant of the reverse EM (REM) algorithm
(Minka  2001). The incremental algorithm updates parameters by replacing old sufﬁcient statistics
with new one for each datum. Our single-pass algorithm is based on an incremental algorithm  but it
does not need to store old statistics for all data. In our single-pass algorithm  we propose a sequential
update method for the Dirichlet parameters. Asuncion et al. (2009); Wallach et al. (2009) indicated
the importance of estimating the parameters of the Dirichlet distribution  which is the distribution
over the topic distributions of documents. Moreover  we can deal with the growing vocabulary size.
In real life  the total vocabulary size is unknown  i.e.  increasing as a document is observed.

1

Running ƟmeMemory usageiREM-LDACVB-LDAshortlongsREM-LDAlargesmallVB-LDA In summary  Fig.1 shows the relationships among inferences. VB-LDA is the variational inference
for LDA  which is a batch inference; CVB-LDA is the collapsed variational inference for LDA (Teh
et al.  2007); iREM-LDA is our incremental algorithm; and sREM-LDA is our single-pass algorithm
for LDA.

Sections.2 brieﬂy explains inference algorithms for LDA. Section 3 describes the proposed algo-
rithm for online learning. Section 4 presents the experimental results.

2 Overview of Latent Dirichlet Allocation

This section overviews LDA where documents are represented as random mixtures over latent topics
and each topic is characterized by a distribution over words. First  we will deﬁne the notations  and
then  describe the formulation of LDA. T is the number of topics. M is the number of documents.
V is the vocabulary size. Nj is the number of words in document j. wj i denotes the i-th word in
document j. zj i denotes the latent topic of word wj i. M ulti((cid:1)) is a multinomial distribution. Dir((cid:1))
is a Dirichlet distribution. θj denotes a T -dimensional probability vector that is the parameters of the
multinomial distribution  and represents the topic distribution of document j. βt is a multinomial
parameter a V -dimensional probability where βt v speciﬁes the probability of generating word v
given topic t. α is the T -dimensional parameter vector of the Dirichlet distribution over θj (j =
1 (cid:1)(cid:1)(cid:1)   M ).
∏
(cid:24) Dir(βjλ) /
LDA assumes the following generative process. For each of the T topics t  draw βt
θαt(cid:0)1
v βλ(cid:0)1
.
For each of the Nj words wj i in document j  draw topic zj i (cid:24) M ulti(zjθj) and draw word wj i (cid:24)
p(wjzj i  β) where p(w = vjz = t  β) = βt v.
That is to say  the complete-data likelihood of a document wj is given by

t v . For each of the M documents j  draw θj (cid:24) Dir(θjα) where Dir(θjα) /

∏

t

t

p(wj  zj  θjjα  β) = p(θjjα)

p(wj ijzj i  β)p(zjjθj).

(1)

Nj∏

i

j

∏
g given by
q(zj ijϕj i)
∏
βµt;v(cid:0)1
∏

t v

v

∏

t

∫ ∑

z

2.1 Variational Bayes Inference for LDA

∏
The VB inference for LDA(Blei et al.  2003) introduces a factorized variational posterior q(z  θ  β)
over z = fzj ig  θ = fθjg and β = fβt

∏

q(z  θ  β) =

j i

q(θjjγj)

jµt) 

q(βt

t

(2)

where ϕ and γ are variational parameters  ϕj i t speciﬁes the probability that the topic of word wj k
is topic t  and γj and µt are the parameters of the Dirichlet distributions over θj and βt  respectively 
i.e.  q(θjjγj) /

jµt) /

θγj;t(cid:0)1

and q(βt

.

j t

∏

j p(wj  zj  θjjα  β)

The log-likelihood of documents is lower bounded introducing q(z  θ) by

(3)

t p(βt

dθjdβ.

q(z  θ  β)

q(z  θ  β) log

The parameters are updated as
ϕj i t / exp (cid:9)(µt wj;i)
v µt v)

F[q(z  θ  β)] =
∑
∑
i ϕj i tI(wj i = v) and I((cid:1)) is an indicator function.
where nj t v =
We can estimate α with the ﬁxed point iteration (Minka  2000; Asuncion et al.  2009) by introducing
the gamma prior G(αtja0  b0)  i.e.  αt (cid:24) G(αtja0  b0)(t = 1  ...  T )  as
t + nj t) (cid:0) (cid:9)(αold
t
0 ) (cid:0) (cid:9)(αold
0 ))

f(cid:9)(αold
j
j((cid:9)(Nj + αold

a0 (cid:0) 1 +
b0 +

exp (cid:9)(γj t))  γj t = αt +

ϕj i t  µt v = λ +

∑
∑

Nj∑

)gαold

αnew

t =

exp (cid:9)(

nj t v 

t

 

(5)

(4)

i=1

j

jλ)
∑

2

Algorithm 1
VB inference for LDA

Algorithm 2
CVB inference for LDA

1: for iteration it = 1 (cid:1)(cid:1)(cid:1)   L do
2:
3:
4:

for j = 1 (cid:1)(cid:1)(cid:1)   M do
for i = 1 (cid:1)(cid:1)(cid:1)   Nj do
Update ϕj i t (t = 1 (cid:1)(cid:1)(cid:1)   T ) by
Eq. (4)
end for
Update γj t (t = 1 (cid:1)(cid:1)(cid:1)   T ) by Eq.
(4)
end for
Update µ by Eq. (4)
Update α by Eq. (5)

5:
6:

7:
8:
9:
10: end for

1: for iteration it = 1 (cid:1)(cid:1)(cid:1)   L do
2:
3:
4:
5:

for j = 1 (cid:1)(cid:1)(cid:1)   M do
for i = 1 (cid:1)(cid:1)(cid:1)   Nj do
Update ϕj i t by Eq. (7)
Update nj t replacing ϕold
ϕnew
j i t .
Update nt wj;i
with ϕnew
j i t .

6:

j i t with

replacing ϕold
j i t

end for

end for
Update α by Eq. (5)

7:
8:
9:
10: end for

∑

where α0 =
Algorithm 1 has the VB inference scheme of LDA.

t αt  and a0 and b0 are the parameters for the gamma distribution.

2.2 Collapsed Variational Bayes Inference for LDA

Teh et al. (2007) proposed CVB-LDA inspired by collapsed Gibbs sampling and found that the con-
vergence of CVB-LDA is experimentally faster than that of VB-LDA  and CVB-LDA outperformed
VB-LDA in terms of perplexity. The CVB-LDA only introduced a variational posterior q(z) where
it marginalized out θ and β over the priors. The CVB inference optimizes the following lower bound
given by

FCV B[q(z)] =

q(z) log

p(wj  zjjα  λ)

q(z)

.

(6)

M∑

∑

j=1

z

The derivation of the update equation for q(z) is slightly complicated and involves approximations
to compute intractable summations. Although Teh et al. (2007) made use of a second-order Taylor
expansion as an approximation  Asuncion et al. (2009) shows the usefulness of an approximation
using only zero-order information. An update using only zero-order information is given by

Nj∑

∑

(αt + n

(cid:0)j i
j t )  nj t =

ϕj i t  nt v =

i=1

j i

ϕj i tI(wj i = v) 

(7)

∑
ϕj i t / λ + n

V λ +

(cid:0)j i
t wj;i
v n

(cid:0)j i
t v

where “-j i” denotes subtracting ϕj i t. Algorithm 2 provides the CVB inference scheme for LDA.

3 Deterministic Online Algorithm for LDA

The purpose of this study is to process text data such as news articles and blog posts arriving in
a continuous stream by using LDA. We propose a learning algorithm for LDA that can be applied
to these semi-inﬁnite and time-series text streams. For these situations  we want to process text
one at a time and then discard them. We repeat iterations only for each word within a document.
That is  we update parameters from an arriving document and discard the document after doing l
iterations. Therefore  we do not need to store statistics about discarded documents. First  we derived
an incremental algorithm for LDA  and then we extended the incremental algorithm to a single-pass
algorithm.

3.1 Incremental Learning

(Neal and Hinton  1998) provided a framework of incremental learning for the EM algorithm. In
general unsupervised-learning  we estimate sufﬁcient statistics si for each data i  compute whole

3

∑

sufﬁcient statistics σ(=
i si) from all data  and update parameters by using σ. In incremental
learning  for each data i  we estimate si  compute σ(i) from si   and update parameters from σ(i). It
is easy to extend an existing batch algorithm to the incremental learning if whole sufﬁcient statistics
or parameters updates are constructed by simply summarizing all data statistics. The incremental
i + snew
algorithm processes data i by subtracting old sold
.
g for all data. While batch algorithms
The incremental algorithm needs to store old statistics fsold
update parameters sweeping through all data  the incremental algorithm updates parameters for each
data one at a time  which results in more parameter updates than batch algorithms. Therefore  the
incremental algorithm sometimes converge faster than batch algorithms.

  i.e.  σ(i) = (cid:0)sold

and adding new snew

i

i

i

i

3.2 Incremental Learning for LDA

Our motivation for devising the incremental algorithm for LDA was to compare CVB-LDA and
VB-LDA. Statistics fnt vg and fnj tg are updated after each word is updated in CVB-LDA. This
update schedule is similar to that of the incremental algorithm. This incremental property seems to
be the reason CVB-LDA converges faster than VB-LDA. Moreover  since CVB-LDA optimizes a
tighterlower-bound from VB-LDA  CVB-LDA can ﬁnd better optima. Below  let us consider the
incremental algorithm for LDA. We start by optimizing the lower-bound different form VB-LDA by
using the reverse EM (REM) algorithm (Minka  2001) as follows:
p(wjjα  β) =

I(wj;i=v)p(θjjα)dθj =

∫ Nj∏

(θj tβt v)

T∑
(θj tβt wj;i )p(θjjα)dθj 
(8)

t=1

i=1

)ϕj;i;t
p(θjjα)dθj 
∫ T∏
∑
∑
(

i ϕj;i;t

∑

t=1

j t

θ

∑

(cid:0)(

log
x q(x) = 1  and so

p(θjjα)dθj.
∑
∑
∏

∑

t αt)

(9)

(10)

(cid:21)

∑

x f (x) = log

x f (x) (cid:21)∏
∑

x q(x) f (x)
q(x) )q(x).

q(x)

x( f (x)

)

∑

Equation (9) is derived from Jensen’s inequality as follows.

x q(x) log f (x)

q(x) = log

x( f (x)

q(x) )q(x) where

Therefore  the lower bound for the log-likelihood is given by

V∏
(

v=1

i=1

t=1

∫ Nj∏
T∑
∫ Nj∏
T∏
(
T∏
Nj∏
∏

t=1

t=1

i=1

i=1

(cid:21)

=

∑

θj tβt wj;i

ϕj i t

)ϕj;i;t

βt wj;i
ϕj i t

ϕj i t)g  βtv / λ +
∑

i

j

^F[q(z)] =

ϕj i t log

+

log

βt wj;i
ϕj i t

j i t

(cid:0)(αt)
∑
The maximum of ^F[q(z)] with respect to q(zj i = t) = ϕj i t and β is given by
nj t v 

ϕj i t / βt wj;i expf(cid:9)(αt +

∑

(cid:0)(Nj +

t αt)

j

t

(cid:0)(αt +

i ϕj i t)

.

(11)

(12)

j nj t v taking a negative value.

The updates of α are the same as Eq.(5). Note that we use the maximum a posteriori estiamtion for
β  however  we do not use λ (cid:0) 1 to avoid λ (cid:0) 1 +
The lower bound ^F[q(z)] introduces only q(z) like CVB-LDA. Equation (12) incrementally updates
the topic distribution of a document for each word as in CVB-LDA because we do not need γj i in
Eq.(12) due to marginalizing out of θj. Equation (12) is a ﬁxed point update  whereas CVB-LDA
can be interpreted as a coordinate ascent algorithm. α and β are updated from the entire document.
That is  when we compare this algorithm with VB-LDA  it looks like a hybrid variant of a batch
updates for α and β  and incremental updates for γj 
Here  we consider an incremental update for β to be analogous to CVBLDA  in which β is updated
for each word. Note that in the LDA setup  each independent identically distributed data point is
a document not a word. Therefore  we incrementally estimate β for each document by swapping
i ϕj i tI(wj i = v) which is the number of word v generated from topic t in
statistics nj t v =
document j. Algorithm 3 shows our incremental algorithm for LDA. This algorithm incrementally
optimizes the lower bound in Eq.(11).

∑Nj

4

Algorithm 3
Incremental algorithm for LDA

Algorithm 4
Single-pass algorithm for LDA

1: for iteration it = 1 (cid:1)(cid:1)(cid:1)   L do
2:
3:
4:
5:
6:

for j = 1 (cid:1)(cid:1)(cid:1)   M do
for i = 1 (cid:1)(cid:1)(cid:1)   Nj do
Update ϕj i t by Eq. (12)
end for
Replace nold
fwj igNj
end for
Update α by Eq. (5)

i=1 in β of Eq. (12) .

j t v with nnew

7:
8:
9: end for

j t v for v 2

3.3 Single-Pass Algorithm for LDA

for iteration it = 1 (cid:1)(cid:1)(cid:1)   l do

for i = 1  ...  Nj do

Update ϕj i t by Eq. (13).

end for
Update β(j) by Eq.(13).
Update α(j) by Eq.(17).

1: for j = 1 (cid:1)(cid:1)(cid:1)   M do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11: end for

end for
Update λ(j) by Eq.(14).
Update ~a(j) and ~b(j) by Eq.(17).

∑

i=1) / p(xNjθ)p(θjfxigN(cid:0)1

Our single-pass algorithm for LDA was inspired by the Bayesian formulation  which internally
includes a sequential update. The posterior distribution with the contribution from the data point
xN is separated out so that p(θjfxigN
i=1 )  where θ denotes a parameter.
This indicates that we can use a posterior given an observed datum as a prior for the next datum..
We use parameters learned from observed data as prior parameters for the next data. For example 
nj t vg + nM t v. Here  we can interpret
βt v in Eq. (12) is represented as βt v / fλ +
nj t vg as prior parameter λ(M(cid:0)1)
fλ +
Our single-pass algorithm sequentially sets a prior for each arrived document. By using this sequen-
tial setting of prior parameters  we present a single-pass algorithm for LDA as shown in Algorithm
4. First  we update parameters from j-th arrived document given prior parameters fλ(j(cid:0)1)
g for l
iterations

for the M-th document.

M(cid:0)1
j

M(cid:0)1
j

∑

t v

t v

∑

Nj∑

Nj∑

ϕj i t /β(j)

t wj;i expf(cid:9)(α(j)
t +

ϕj i t)g  β(j)

t v

/ λ(j(cid:0)1)

t v +

ϕj i tI(wj i = v) 

(13)

i

i

where λ(0)
the document for the next document as follows  and ﬁnally discard the document.

is explained below. Then  we set prior parameters by using statistics from

t v = λ and α(j)

t

t v =λ(j(cid:0)1)
λ(j)
t v +

(14)
Since the updates are repeated within a document  we need to store statistics fϕj i tg for each word
in a document  but not for all words in all documents.

i

ϕj i tI(wj i = v).

In the CVB and iREM algorithms  the Dirichlet parameter  α  uses batch updates  i.e.  α is up-
dated by using the entire document once in one iteration. We need an online-update algorithm for
α to process a streaming text. However  unlike parameter βt v  the update of α in Eq.(5) is not
constructed by simply summarizing sufﬁcient statistics of data and a prior. Therefore  we derive a
single-pass update for the Dirichlet parameter α using the following interpretation.
We consider Eq.(5) to be the expectation of αt over posterior G(αtj~at  ~b) given documents D and
prior G(αtja0  b0)  i.e  αnew
M∑

t = E[αt]G(αj~at ~b) =

M∑

~at (cid:0) 1

  where

~b

~at =a0 +
aj t = f(cid:9)(αold

aj t  ~b = b0 +
bj 
t + nj t) (cid:0) (cid:9)(αold
)gαold

j

j

t

t

  bj = (cid:9)(Nj + αold

0 ) (cid:0) (cid:9)(αold
0 ).

(15)

(16)

5

We regard aj t and bj as statistics for each document  which indicates that the parameters that we
actually update are ~at and ~b in Eq.(5). These updates are simple summarizations of aj t and bj and
prior parameters a0 and b0. Therefore  we have an update for α(j)
after observing document j given
by

t

t = E[αt]G(αj~a(j)
α(j)
aj t = f(cid:9)(α(j(cid:0)1)

t

t

~a(j)
t

(cid:0) 1
~b(j)

 ~b(j)) =
+ nj t) (cid:0) (cid:9)(α(j(cid:0)1)

t

t = ~a(j(cid:0)1)
  ~a(j)
)gα(j(cid:0)1)

t

t

+ aj t  ~b(j) = ~b(j(cid:0)1) + bj 

  bj = (cid:9)(Nj + α(j(cid:0)1)

0

) (cid:0) (cid:9)(α(j(cid:0)1)

0

) 

(17)

(18)

where ~a(0)
~a(j(cid:0)1)

t = a0 and ~b(0) = b0.
and ~b(j(cid:0)1) are used as prior paramters for the next j-th documents.

t

3.4 Analysis

This section analyze the proposed updates for parameters α and β in the previous section.

We eventually update parameters α(j) and β(j) given document j as

α(j)

t =

ad t + aj t
bd + bj

= α(j(cid:0)1)

t

(1 (cid:0) ηα

j ) + ηα
j

aj t
bj

  ηα

j =

b0 +

.

(19)

β(j)
t v =

λ +

Vjλ +

j(cid:0)1
d nd t v + nj t v
j(cid:0)1
d nd t (cid:1) + nj t (cid:1)

= β(j(cid:0)1)

t v

(1 (cid:0) ηβ

j ) + ηβ

j

nj t v
nj t (cid:1)   ηβ

j =

(Vj (cid:0) Vj(cid:0)1)λ + nj t (cid:1)

Vjλ +

j

d nd t (cid:1)

.

j and ηβ

(20)
v nt v and Vj is the vocabulary size of total observed documents(d = 1 (cid:1)(cid:1)(cid:1)   j). Our
where nt (cid:1) =
single-pass algorithm sequentially sets a prior for each arrived document  and so we can select a prior
(a dimension of Dirichlet distribution) corresponding to observed vocabulary. In fact  this property
is useful for our problem because the vocabulary size is growing in the text stream. These updates
indicate that ηα
j interpolate the parameters estimated from old and new data. These updates
look like a stepwise algorithm (H.Robbins and S.Monro  1951; Sato and Ishii  2000)  although a
stepsize algorithm interpolates sufﬁcient statistics whereas our updates interpolate parameters. In
our updates  how we set the stepsize for parameter updates is equivalent to how we set the hyper-
parameters for priors. Therefore  we do not need to newly introduce a stepsize parameter.
In our update of β  the appearance rate of word v in topic t in document j  nj t v/nj t (cid:1)  is added
to old parameter β(j(cid:0)1)
j   which gradually decreases as the document is observed.
The same relation holds for α. Therefore  the inﬂuence of new data decreases as the number of
document observations increases as shown in Theorem 1. Moreover  Theorem 1 is an important
role in analyzing the convergence of parameter updates by using the super-martingale convergence
theorem (Bertsekas and Tsitsiklis  1996; Brochu et al.  2004). This convergence analysis is our
future work.
Theorem 1. If ϵ and ν exist satisfying 0 < ϵ < Sj < ν for any j 

with weight ηβ

t v

j(cid:0)1
d
j(cid:0)1
d

∑
∑
a0 (cid:0) 1 +
∑
b0 +
∑
∑

bj

∑
∑

j
d bd

∑

Sj

ηj =

τ +

j
d Sd

satisﬁes

1∑

j

j!1 ηj = 0 
lim

ηj = 1 

1∑

j

(21)

(22)

j < 1
η2

Note that ηα

j and ηβ

j are shown as ηj given by Eq. (21). The proof is given in the supporting material.

6

4 Experiments

We carried out experiments on document modeling in terms of perplexity. We compared the infer-
ences for LDA in two sets of text data. The ﬁrst was “Associated Press(AP)” where the number of
documents was M = 10  000 and the vocabulary size was V = 67  291. The second was “The Wall
Street Journal(WSJ)” where M = 10  000 and V = 56  738. The ordering of document is time-
series. The comparison metric for document modeling was the “test set perplexity”. We randomly
split both data sets into a training set and a test set by assigninig 20% of the words in each document
to the test set. Stop words were eliminated in datasets.

We performed experiments on six inferences  PF  VB  CVB0  CVB  iREM and sREM. PF denotes
the particle ﬁlter for LDA used in Canini et al. (2009). We set αt as 50/T in PF. The number of
particles  denoted by P   is 64. The number of words for resampling  denoted by R  is 20. The effec-
tive sample size (ESS) threshold  which controls the number of resamplings  is set at 10. CVB0 and
CVB are collapsed variational inference for LDA using zero-order and second-order information 
respectively. iREM represents the incremental reverse EM algorithm in Algorithm 3. CVB0 and
CVB estimates the Dirichlet parameter α over the topic distribution for all datasets  i.e.  a batch
framework. We estimated α in iREM for all datasets like CVB to clarify the properties of iREM
compared with CVB. L denotes the number of iterations for whole documents in Algorithms 1 and
2. sREM indicates a single-pass variants of iREM in Algorithm 4. l denotes the number of iterations
within a document in Algorithm 4. sREM does not make iterations for whole documents.

Figure 2 demonstrates the results of experiments on the test set perplexity where lower values indi-
cates better performance. We ran experiments ﬁve times with different random initializations and
show the averages 1. PF and sREM calculate the test set perplexity after sweeping through all traing
set.

VB converges slower than CVB and iREM. Moreover  iREM outperforms CVB in the convergence
rate. Although CVB0 outperforms other algorithms for the cases of low number of topics  the
convergence rate of CVB0 depends on the number of topics. sREM does not outperform iREM in
terms of perplexities  however  the performance of sREM is close to that of iREM
As a results  we recommend sREM in a large number of documents or document streams. sREM
does not need to store old statistics for all documents unlike other algorithms.
In addition  the
convergence of sREM depends on the length of a document  rather than the number of documents.
Since we process each document individually  we can control the number of iterations corresponding
to the length of each arrived document. Finally  we discuss the running time. The running time of
sREM is O( L
l ) times shorter than that of VB  CVB0  CVB and iREM. The averaged running times
of PF(T=300 P=64 R=20) are 28.2 hours in AP and 31.2 hours in WSJ. Those of sREM(T=300 l=5)
are 1.2 hours in AP and 1.3 hours in WSJ.

5 Conclusions

We developed a deterministic online-learning algorithm for latent Dirichlet allocation (LDA). The
proposed algorithm can be applied to excess text data in a continuous stream because it processes re-
ceived documents one at a time and then discard them. The proposed algorithm was much faster than
a batch algorithm and was comparable to the batch algorithm in terms of perplexity in experiments.

1We exclude the error bar with standard deviation because it is so small that it is hidden by the plot markers

7

(a)

(c)

(e)

(g)

(b)

(d)

(f)

(h)

Figure 2: Results of experiments. Left line indicates the results in AP corpus. Right line indicates
the results in WSJ corpus. (a) and (b) compared test set perplexity with respect to the number of
topics. (c)  (d)  (e) and (f) compared test set perplexity with respect to the number of iterations
in topic T = 100 and T = 300  respectively. (g) and (h) show the relationships between test set
perplexity and the number of iterations within a document  i.e.  l.

References

Loulwah Alsumait  Daniel Barbara  and Carlotta Domeniconi. On-line lda: Adaptive topic models
IEEE International

for mining text streams with applications to topic detection and tracking.
Conference on Data Mining  0:3–12  2008. ISSN 1550-4786.

8

1500200025003000350050100150200250300Testset PerplexityNumber of TopicsAPVB(L=100)CVB0(L=100)CVB(L=100)iREM(L=100)sREM(l=5)PF(P=64)1500170019002100230025002700290050100150200250300Testset PerplexityNumber of TopicsWSJVB(L=100)CVB0(L=100)CVB(L=100)iREM(L=100)sREM(l=5)PF(P=64)1.50E+032.00E+032.50E+033.00E+033.50E+034.00E+034.50E+03102030405060708090100Testset PerplexityNumber of iteraƟonsAP(T=100)VBCVB0CVBiREMsREMPF1.50E+032.00E+032.50E+033.00E+033.50E+034.00E+03102030405060708090100Testset PerplexityNumber of iteraƟonsWSJ(T=100)VBCVB0CVBiREMsREMPF1.50E+032.00E+032.50E+033.00E+033.50E+034.00E+034.50E+035.00E+035.50E+03102030405060708090100Testset PerplexityNumber of iteraƟonsAP(T=300)VBCVB0CVBiREMsREMPF1.50E+032.00E+032.50E+033.00E+033.50E+034.00E+034.50E+03102030405060708090100Testset PerplexityNumber of iteraƟonsWSJ(T=300)VBCVB0CVBiREMsREMPF17001800190020002100220023002400250050100150200250300Testset PerplexityNumber of TopicsAPsREM(l=3)sREM(l=4)sREM(l=5)1680178018801980208021802280238050100150200250300Testset PerplexityNumber of TopicsWSJsREM(l=3)sREM(l=4)sREM(l=5)A. Asuncion  M. Welling  P. Smyth  and Y. W. Teh. On smoothing and inference for topic models.

In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence  2009.

Arindam Banerjee and Sugato Basu. Topic models over text streams: A study of batch and online

unsupervised learning. In SIAM International Conference on Data Mining  2007.

D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc  1996.
D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

Eric Brochu  Nando de Freitas  and Kejie Bao. Owed to a martingale: A fast bayesian on-line em

algorithm for multinomial models  2004.

Kevin R. Canini  Lei Shi  and Thomas L. Grifﬁths. Online inference of topics with latent dirichlet
allocation. In Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and
Statistics  2009.

H.Robbins and S.Monro. A stochastic approximation method. In Annals of Mathematical Statistics 

pages 400–407  1951.

Thomas P. Minka.

2000.
minka-dirichlet.pdf.

report  Microsoft 
URL http://research.microsoft.com/(cid:24)minka/papers/dirichlet/

Estimating a dirichlet distribution.

Technical

Thomas P. Minka.

report 
Microsoft  2001. URL http://research.microsoft.com/en-us/um/people/
minka/papers/rem.html.

Using lower bounds to approximate integrals.

Technical

R. Neal and G. Hinton. A view of the EM algorithm that justiﬁes incremental  sparse  and other
variants. In M. I. Jordan  editor  Learning in Graphical Models. Kluwer  1998. URL http:
//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.2557.

Masa A. Sato and Shin Ishii. On-line em algorithm for the normalized gaussian network.
Neural Computation  12(2):407–432  2000. URL http://citeseerx.ist.psu.edu/
viewdoc/summary?doi=10.1.1.37.3704.

Yee Whye Teh  David Newman  and Max Welling. A collapsed variational Bayesian inference algo-
rithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 19 
2007.

Hanna Wallach  David Mimno  and Andrew McCallum. Rethinking lda: Why priors matter.

In
Y. Bengio  D. Schuurmans  J. Lafferty  C. K. I. Williams  and A. Culotta  editors  Advances in
Neural Information Processing Systems 22  pages 1973–1981. 2009.

Limin Yao  David Mimno  and Andrew McCallum. Efﬁcient methods for topic model inference
on streaming document collections. In KDD ’09: Proceedings of the 15th ACM SIGKDD inter-
national conference on Knowledge discovery and data mining  pages 937–946  New York  NY 
USA  2009. ACM. ISBN 978-1-60558-495-9.

9

,Crystal Maung
Haim Schweitzer