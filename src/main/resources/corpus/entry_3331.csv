2019,Manifold-regression to predict from MEG/EEG brain signals without source modeling,Magnetoencephalography and electroencephalography (M/EEG) can reveal neuronal dynamics non-invasively in real-time and are therefore appreciated methods in medicine and neuroscience. Recent advances in modeling brain-behavior relationships have highlighted the effectiveness of Riemannian geometry for summarizing the spatially correlated time-series from M/EEG in terms of their covariance. However  after artefact-suppression  M/EEG data is often rank deficient which limits the application of Riemannian concepts. In this article  we focus on the task of regression with rank-reduced covariance matrices. We study two Riemannian approaches that vectorize the M/EEG covariance between sensors through projection into a tangent space. The Wasserstein distance readily applies to rank-reduced data but lacks affine-invariance. This can be overcome by finding a common subspace in which the covariance matrices are full rank  enabling the affine-invariant geometric distance. We investigated the implications of these two approaches in synthetic generative models  which allowed us to control estimation bias of a linear model for prediction. We show that Wasserstein and geometric distances allow perfect out-of-sample prediction on the generative models. We then evaluated the methods on real data with regard to their effectiveness in predicting age from M/EEG covariance matrices. The findings suggest that the data-driven Riemannian methods outperform different sensor-space estimators and that they get close to the performance of biophysics-driven source-localization model that requires MRI acquisitions and tedious data processing. Our study suggests that the proposed Riemannian methods can serve as fundamental building-blocks for automated large-scale analysis of M/EEG.,Manifold-regression to predict from MEG/EEG brain

signals without source modeling

David Sabbagh ∗†‡  Pierre Ablin  Gaël Varoquaux  Alexandre Gramfort  Denis A. Engemann §

Université Paris-Saclay  Inria  CEA  Palaiseau  91120  France

Abstract

Magnetoencephalography and electroencephalography (M/EEG) can reveal neu-
ronal dynamics non-invasively in real-time and are therefore appreciated methods in
medicine and neuroscience. Recent advances in modeling brain-behavior relation-
ships have highlighted the effectiveness of Riemannian geometry for summarizing
the spatially correlated time-series from M/EEG in terms of their covariance. How-
ever  after artefact-suppression  M/EEG data is often rank deﬁcient which limits
the application of Riemannian concepts. In this article  we focus on the task of
regression with rank-reduced covariance matrices. We study two Riemannian ap-
proaches that vectorize the M/EEG covariance between-sensors through projection
into a tangent space. The Wasserstein distance readily applies to rank-reduced
data but lacks afﬁne-invariance. This can be overcome by ﬁnding a common sub-
space in which the covariance matrices are full rank  enabling the afﬁne-invariant
geometric distance. We investigated the implications of these two approaches in
synthetic generative models  which allowed us to control estimation bias of a linear
model for prediction. We show that Wasserstein and geometric distances allow
perfect out-of-sample prediction on the generative models. We then evaluated
the methods on real data with regard to their effectiveness in predicting age from
M/EEG covariance matrices. The ﬁndings suggest that the data-driven Riemannian
methods outperform different sensor-space estimators and that they get close to
the performance of biophysics-driven source-localization model that requires MRI
acquisitions and tedious data processing. Our study suggests that the proposed
Riemannian methods can serve as fundamental building-blocks for automated
large-scale analysis of M/EEG.

1

Introduction

Magnetoencephalography and electroencephalography (M/EEG) measure brain activity with mil-
lisecond precision from outside the head [23]. Both methods are non-invasive and expose rhythmic
signals induced by coordinated neuronal ﬁring with characteristic periodicity between minutes and
milliseconds [10]. These so-called brain-rhythms can reveal cognitive processes as well as health
status and are quantiﬁed in terms of the spatial distribution of the power spectrum over the sensor
array that samples the electromagnetic ﬁelds around the head [3].

Statistical learning from M/EEG commonly relies on covariance matrices estimated from band-
pass ﬁltered signals to capture the characteristic scale of the neuronal events of interest [7  22 
16]. However  covariance matrices do not live in an Euclidean space but a Riemannian manifold.

∗Additional afﬁliation: Inserm  UMRS-942  Paris Diderot University  Paris  France
†Additional afﬁliation: Department of Anaesthesiology and Critical Care  Lariboisière Hospital  Assistance

Publique Hôpitaux de Paris  Paris  France

‡dav.sabbagh@gmail.com
§denis-alexander.engemann@inria.fr

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Fortunately  Riemannian geometry offers a principled mathematical approach to use standard linear
learning algorithms such as logistic or ridge regression that work with Euclidean geometry. This is
achieved by projecting the covariance matrices into a vector space equipped with an Euclidean metric 
the tangent space. The projection is deﬁned by the Riemannian metric  for example the geometric
afﬁne-invariant metric [5] or the Wasserstein metric [6]. As a result  the prediction error can be
substantially reduced when learning from covariance matrices using Riemannian methods [45  14].

In practice  M/EEG data is often provided in a rank deﬁcient form by platform operators but
also curators of public datasets [32  2].
Its contamination with high-amplitude environmental
electromagnetic artefacts often render aggressive ofﬂine-processing mandatory to yield intelligible
signals. Commonly used tools for artefact-suppression project the signal linearly into a lower
dimensional subspace that is hoped to predominantly contain brain signals [40  42  34]. But this
necessarily leads to inherently rank-deﬁcient covariance matrices for which no afﬁne-invariant
distance is deﬁned. One remedy may consist in using anatomically informed source localization
techniques that can typically deal with rank deﬁciencies [17] and can be combined with source-level
estimators of neuronal interactions [31]. However  such approaches require domain-speciﬁc expert
knowledge  imply processing steps that are hard to automate (e.g. anatomical coregistration) and
yields pipelines in which excessive amounts of preprocessing are not under control of the predictive
model.

In this work  we focus on regression with rank-reduced covariance matrices. We propose two
Riemannian methods for this problem. A ﬁrst approach uses a Wasserstein metric that can handle
rank-reduced matrices  yet is not afﬁne-invariant. In a second approach  matrices are projected into a
common subspace in which afﬁne-invariance can be provided. We show that both metrics can achieve
perfect out-of-sample predictions in a synthetic generative model. Based on the SPoC method [15] 
we then present a supervised and computationally efﬁcient approach to learn subspace projections
informed by the target variable. Finally  we apply these models to the problem of inferring age
from brain data [33  31] on 595 MEG recordings from the Cambridge Center of Aging (Cam-CAN 
http://cam-can.org) covering an age range from 18 to 88 years [41]. We compare the data-driven
Riemannian approaches to simpler methods that extract power estimates from the diagonal of the
sensor-level covariance as well as the cortically constrained minimum norm estimates (MNE) which
we use to project the covariance into a subspace deﬁned by anatomical prior knowledge.

Notations We denote scalars s ∈ R with regular lowercase font  vectors s = [s1  . . .   sN ] ∈ RN
with bold lowercase font and matrices S ∈ RN ×M with bold uppercase fonts. IN is the identity
matrix of size N . [·]⊤ represents vector or matrix transposition. The Frobenius norm of a matrix
will be denoted by ||M||2
F = Tr(MM ⊤) = P|Mij|2 with Tr(·) the trace operator. rank(M ) is
the rank of a matrix. The l2 norm of a vector x is denoted by ||x||2
i . We denote by MP
the space of P × P square real-valued matrices  SP = {M ∈ MP   M ⊤ = M} the subspace of
symmetric matrices  S ++
P = {S ∈ SP   x⊤Sx > 0 ∀x ∈ RP} the subspace of P × P symmetric
positive deﬁnite matrices  S +
P = {S ∈ SP   x⊤Sx ≥ 0 ∀x ∈ RP} the subspace of P × P symmetric
semi-deﬁnite positive (SPD) matrices  S +
P   rank(S) = R} the subspace of SPD
matrices of ﬁxed rank R. All matrices S ∈ S ++
P ) and
diagonalizable with real strictly positive eigenvalues: S = U ΛU ⊤ with U an orthogonal matrix of
eigenvectors of S (U U ⊤ = IP ) and Λ = diag(λ1  . . .   λn) the diagonal matrix of its eigenvalues
λ1 ≥ . . . ≥ λn > 0. For a matrix M   diag(M ) ∈ RP is its diagonal. We also deﬁne the exponential
and logarithm of a matrix: ∀S ∈ S ++
P   log(S) = U diag(log(λ1)  . . .   log(λn)) U ⊤ ∈ SP   and
∀M ∈ SP   exp(M ) = U diag(exp(λ1)  . . .   exp(λn)) U ⊤ ∈ S ++
P . N (µ  σ2) denotes the normal
(Gaussian) distribution of mean µ and variance σ2. Finally  Es[x] represents the expectation and
Vars[x] the variance of any random variable x w.r.t. their subscript s when needed.

are full rank  invertible (with S−1 ∈ S ++

2 = P x2

P R = {S ∈ S +

P

Background and M/EEG generative model MEG or EEG data measured on P channels are
multivariate signals x(t) ∈ RP . For each subject i = 1 . . . N   the data are a matrix Xi ∈ RP ×T
where T is the number of time samples. For the sake of simplicity  we assume that T is the same for
each subject  although it is not required by the following method. The linear instantaneous mixing
model is a valid generative model for M/EEG data due to the linearity of Maxwell’s equations [23].
Assuming the signal originates from Q < P locations in the brain  at any time t  the measured signal

2

vector of subject i = 1 . . . N is a linear combination of the Q source patterns as

j ∈ RP   j = 1 . . . Q:

(1)
where the patterns form the time and subject-independent source mixing matrix As = [as
Q] ∈
RP ×Q  si(t) ∈ RQ is the source vector formed by the Q time-dependent sources amplitude  ni(t) ∈
RP is a contamination due to noise. Note that the mixing matrix As and sources si are not known.

xi(t) = As si(t) + ni(t)  

1  . . .   as

Following numerous learning models on M/EEG [7  15  22]  we consider a regression setting where
the target yi is a function of the power of the sources  denoted pi j = Et[s2
i j(t)]. Here we consider
the linear model:

Q

yi =

αjf (pi j)  

(2)

Xj=1

where α ∈ RQ and f : R+ → R is increasing. Possible choices for f that are relevant for neuro-
science are f (x) = x  or f (x) = log(x) to account for log-linear relationships between brain signal
power and cognition [7  22  11]. A ﬁrst approach consists in estimating the sources before ﬁtting
such a linear model  for example using the Minimum Norm Estimator (MNE) approach [24]. This
boils down to solving the so-called M/EEG inverse problem which requires costly MRI acquisitions
and tedious processing [3]. A second approach is to work directly with the signals Xi. To do so 
models that enjoy some invariance property are desirable: these models are blind to the mixing
As and working with the signals x is similar to working directly with the sources s. Riemannian
geometry is a natural setting where such invariance properties are found [18]. Besides  under Gaussian
assumptions  model (1) is fully described by second order statistics [37]. This amounts to working
with covariance matrices  Ci = XiX ⊤
i /T   for which Riemannian geometry is well developed. One
speciﬁcity of M/EEG data is  however  that signals used for learning have been rank-reduced. This
leads to rank-deﬁcient covariance matrices  Ci ∈ S +
P R  for which speciﬁc matrix manifolds need to

be considered.

2 Theoretical background to model invariances on S +

P R manifold

2.1 Riemannian matrix manifolds

Endowing a continuous set M of square matrices with a
metric  that deﬁnes a local Euclidean structure  gives a
Riemannian manifold with a solid theoretical framework.
Let M ∈ M  a K-dimensional Riemannian manifold. For
any matrix M ′ ∈ M  as M ′ → M   ξM = M ′ − M
belongs to a vector space TM of dimension K called the
tangent space at M .
The Riemannian metric deﬁnes an inner product h· ·iM :
TM × TM → R for each tangent space TM   and as a con-
sequence a norm in the tangent space kξkM =phξ  ξiM .
Integrating this metric between two points gives a geodesic
distance d : M × M → R+. It allows to deﬁne means on

the manifold:

Meand(M1  . . .   MN ) = arg min
M ∈M

Figure 1: Tangent Space  exponential
and logarithm on Riemannian manifold
illustration.

d(Mi  M )2 .

(3)

N

Xi=1

The manifold exponential at M ∈ M  denoted ExpM   is a smooth mapping from TM to M that
preserves local properties. In particular  d(ExpM (ξM )  M ) = kξMkM for ξM small enough. Its
inverse is the manifold logarithm LogM from M to TM   with kLogM (M ′)kM = d(M   M ′) for
M   M ′ ∈ M. Finally  since TM is Euclidean  there is a linear invertible mapping φM : TM → RK
such that for all ξM ∈ TM   kξMkM = kφM (ξM )k2. This allows to deﬁne the vectorization
operator at M ∈ M  PM : M → RK   deﬁned by PM (M ′) = φM (LogM (M ′)). Fig. 1
illustrates these concepts.

The vectorization explicitly captures the local Euclidean properties of the Riemannian manifold:

d(M   M ′) = kPM (M ′)k2

(4)

3

Hence  if a set of matrices M1  . . .   MN is located in a small portion of the manifold  denoting
M = Meand(M1  . . .   MN )  it holds:

d(Mi  Mj) ≃ kPM (Mi) − PM (Mj)k2

(5)

For additional details on matrix manifolds  see [1]  chap. 3.

Regression on matrix manifolds The vectorization operator is key for machine learning ap-
it projects points in M on RK   and the distance d on M is approximated by the
plications:
distance ℓ2 on RK . Therefore  those vectors can be used as input for any standard regression
technique  which often assumes a Euclidean structure of the data. More speciﬁcally  through-
out the article  we consider the following regression pipeline. Given a training set of samples
M1  . . .   MN ∈ M and target continuous variables y1  . . .   yN ∈ R  we ﬁrst compute the mean
of the samples M = Meand(M1  . . .   MN ). This mean is taken as the reference to compute the
vectorization. After computing v1  . . .   vN ∈ RK as vi = PM (Mi)  a linear regression technique
(e.g. ridge regression) with parameters β ∈ RK can be employed assuming that yi ≃ v⊤

i β.

2.2 Distances and invariances on positive matrices manifolds

We will now introduce two important distances: the geometric distance on the manifold S ++
known as afﬁne-invariant distance)  and the Wasserstein distance on the manifold S +

P R.

P

(also

The geometric distance Seeking properties of covariance matrices that are invariant by linear
transformation of the signal  leads to endow the positive deﬁnite manifold S++
P with the geometric
distance [18]:

dG(S  S ′) = k log(S−1S ′)kF =" P
Xi=1

1

2

log2 λk#

where λk  k = 1 . . . P are the real eigenvalues of S−1S ′. The afﬁne invariance property writes:

For W invertible  dG(W ⊤SW   W ⊤S′W ) = dG(S  S′) .

(6)

(7)

1

This distance gives a Riemannian-manifold structure to S++
P with the inner product hP   QiS =
Tr(P S−1QS−1) [18].
The corresponding manifold logarithm at S is LogS(S ′) =
2 log(cid:0)S− 1
2 and the vectorization operator PS(S ′) of S ′ w.r.t. S: PS(S ′) =
S
Upper(S− 1
2 ) = Upper(log(S− 1
2 ))  where Upper(M ) ∈ RK is the vector-
ized upper-triangular part of M   with unit weights on the diagonal and √2 weights on the off-diagonal 
and K = P (P + 1)/2.

2 S ′S− 1
2 LogS(S ′)S− 1

2(cid:1)S

2 S ′S− 1

1

∗

∗

P   it is hard to endow the S +

P R manifold with a distance
that yields tractable or cheap-to-compute logarithms [43]. This manifold is classically viewed as
is the set P × R matrices of rank R [30]. This view
/OR  where OR is the orthogonal group of size R.

The Wasserstein distance Unlike S ++
S +
P R = {YY⊤|Y ∈ RP ×R
allows to write S +
This means that each matrix YY⊤ ∈ S +
It has recently been proposed [35] to use the standard Frobenius metric on the total space RP ×R
This metric in the total space is equivalent to the Wasserstein distance [6] on S +

P R is identiﬁed with the set {YQ|Q ∈ OR}.

P R as a quotient manifold RP ×R

}  where RP ×R

P R:

∗

∗

.

dW (S  S′) =hTr(S) + Tr(S′) − 2Tr((S

This provides cheap-to-compute logarithms:

1

2 S′S

1

2 )

1

2

1

2 )i

LogY Y ⊤ (Y ′Y ′⊤) = Y ′Q∗ − Y ∈ RP ×R

∗

 

(8)

(9)

where U ΣV ⊤ = Y ⊤Y ′ is a singular value decomposition and Q∗ = V U ⊤. The vectorization
operator is then given by PY Y ⊤ (Y ′Y ′⊤) = vect(Y ′Q∗ − Y ) ∈ RP R  where the vect of a matrix

is the vector containing all its coefﬁcients.

4

This framework offers closed form projections in the tangent space for the Wasserstein distance 
which can be used to perform regression. Importantly  since S++
P P   we can also use this
distance on the positive deﬁnite matrices. This distance possesses the orthogonal invariance property:

P = S+

For W orthogonal  dW (W ⊤SW   W ⊤S′W ) = dW (S  S′) .

(10)
This property is weaker than the afﬁne invariance of the geometric distance (7). A natural question
is whether such an afﬁne invariant distance also exists on this manifold. Unfortunately  it is shown
in [8] that the answer is negative for R < P (proof in appendix 6.3).

3 Manifold-regression models for M/EEG

3.1 Generative model and consistency of linear regression in the tangent space of S ++

P

1  . . .   as

1   . . .   an

1   . . .   an

Here  we consider a more speciﬁc generative model than (1) by assuming a speciﬁc struc-
the additive noise ni(t) = Anνi(t) with An =
ture on the noise. We assume that
P −Q] ∈ RP ×(P −Q) and νi(t) ∈ RP −Q. This amounts to assuming that the noise
[an
is of rank P − Q and that the noise spans the same subspace for all subjects. Denoting A =
P −Q] ∈ RP ×P and ηi(t) = [si 1(t)  . . . si Q(t)  νi 1(t)  . . .   νi P −Q(t)] ∈
Q  an
[as
RP   this generative model can be compactly rewritten as xi(t) = Aηi(t).
We assume that the sources si are decorrelated and independent from νi: with pi j = Et[s2
the powers  i.e. the variance over time  of the j-th source of subject i  we suppose Et[si(t)s⊤
diag((pi j)j=1...Q) and Et[si(t)νi(t)⊤] = 0. The covariances are then given by:

i j(t)]
i (t)] =

Ci = AEiA⊤  

(11)

where Ei = Et[ηi(t)ηi(t)⊤] is a block diagonal matrix  whose upper Q × Q block is
diag(pi 1  . . .   pi Q).

In the following  we show that different functions f from (2) yield a linear relationship between the
yi’s and the vectorization of the Ci’s for different Riemannian metrics.
Proposition 1 (Euclidean vectorization). Assume f (pi j) = pi j . Then  the relationship between yi
and Upper(Ci) is linear.

Proof. Indeed  if f (p) = p  the relationship between yi and the pi j is linear. Rewriting Eq. (11) as
Ei = A−1CiA−⊤  and since the pi j are on the diagonal of the upper block of Ei  the relationship
between the pi j and the coefﬁcients of Ci is also linear. This means that there is a linear relationship
between the coefﬁcients of Ci and the variable of interest yi. In other words  yi is a linear combination
of the vectorization of Ci w.r.t. the standard Euclidean distance.

Proposition 2 (Geometric vectorization). Assume f (pi j) = log(pi j).
Denote C =
MeanG(C1  . . .   CN ) the geometric mean of the dataset  and vi = PC(Ci) the vectorization of Ci
w.r.t. the geometric distance. Then  the relationship between yi and vi is linear.

The proof is given in appendix 6.1. It relies crucially on the afﬁne invariance property that means that
using Riemannian embeddings of the Ci’s  is equivalent to working directly with the Ei’s.
Proposition 3 (Wasserstein vectorization). Assume f (pi j) = √pi j . Assume that A is orthogonal.
Denote C = MeanW (C1  . . .   CN ) the Wasserstein mean of the dataset  and vi = PC(Ci) the
vectorization of Ci w.r.t. the Wasserstein distance. Then  the relationship between yi and vi is linear.

The proof is given in appendix 6.2. The restriction to the case where A is orthogonal stems from
the orthogonal invariance of the Wasserstein distance. In the neuroscience literature square root
rectiﬁcations are however not commonly used for M/EEG modeling. Nevertheless  it is interesting to
see that the Wasserstein metric that can naturally cope with rank reduced data is consistent with this
particular generative model.

These propositions show that the relationship between the samples and the variable y is linear in
the tangent space  motivating the use of linear regression methods (see simulation study in Sec. 4).
The argumentation of this section relies on the assumption that the covariance matrices are full rank.
However  this is rarely the case in practice.

5

Figure 2: Proposed regression pipeline. The considered choices for each sequential step are detailed
below each box. Identity means no spatial ﬁltering W = I. Only the most relevant combinations
are reported. For example Wasserstein vectorization does not need projections as it directly applies
to rank-deﬁcient matrices. Geometric vectorization is not inﬂuenced by the choice of projections
due to its afﬁne-invariance. Choices for vectorization are depicted by the colors used for visualizing
subsequent analyses.

R

3.2 Learning projections on S ++
In order to use the geometric distance on the Ci ∈ S +
R to make
them full rank. In the following  we consider a linear operator W ∈ RP ×R of rank R which is
common to all samples (i.e. subjects). For consistency with the M/EEG literature we will refer to rows
of W as spatial ﬁlters. The covariance matrices of ‘spatially ﬁltered’ signals W ⊤xi are obtained
as: Σi = W ⊤CiW ∈ RR×R. With probability one  rank(Σi) = min(rank(W )  rank(Ci)) = R 
hence Σi ∈ S++
R . Since the Ci’s do not span the same image  applying W destroys some information.
Recently  geometry-aware dimensionality reduction techniques  both supervised and unsupervised 
have been developed on covariance manifolds [28  25]. Here we considered two distinct approaches
to estimate W .

P R  we have to project them on S ++

Unsupervised spatial ﬁltering A ﬁrst strategy is to project the data into a subspace that captures
most of its variance. This is achieved by Principal Component Analysis (PCA) applied to the averaged
covariance matrix computed across subjects: WUNSUP = U   where U contains the eigenvectors
corresponding to the top R eigenvalues of the average covariance matrix C = 1
i=1 Ci. This step
is blind to the values of y and is therefore unsupervised. Note that under the assumption that the time
series across subjects are independent  the average covariance C is the covariance of the data over
the full population.

N PN

Supervised spatial ﬁltering We use a supervised spatial ﬁltering algorithm [15] originally de-
veloped for intra-subject Brain Computer Interfaces applications  and adapt it to our cross-person
prediction problem. The ﬁlters W are chosen to maximize the covariance between the power of the
ﬁltered signals and y. Denoting by Cy = 1
i=1 yiCi the weighted average covariance matrix  the
ﬁrst ﬁlter wSUP is given by:

N PN

wSUP = arg max

w

w⊤Cyw
w⊤Cw

.

In practice  all the other ﬁlters in WSUP are obtained by solving a generalized eigenvalue decomposi-
tion problem (see the proof in Appendix 6.4).

The proposed pipeline is summarized in Fig. 2.

4 Experiments

4.1 Simulations

P

Independent

We start by illustrating Prop. 2.

identically distributed covariance matrices
C1  . . .   CN ∈ S++
and variables y1  . . .   yN are generated following the above generative model.
The matrix A is taken as exp(µB) with B ∈ RP ×P a random matrix  and µ ∈ R a scalar con-
trolling the distance from A to identity (µ = 0 yields A = IP ). We use the log function for f to
link the source powers (i.e. the variance) to the yi’s. Model reads yi = Pj αj log(pij) + εi  with
εi ∼ N (0  σ2) a small additive random perturbation.

6

PreprocessingRegressionXirawXiCiΣiviỹiIdentitySupervisedUnsupervisedLog-diagEuclideanWassersteinGeometricRidgeCovarianceRepresentationProjectionVectorizationWe compare three methods of vectorization: the geometric distance  the Wasserstein distance and
the non-Riemannian method “log-diag” extracting the log of the diagonals of Ci as features. Note
that the diagonal of Ci contains the powers of each sensor for subject i. A linear regression model
is used following the procedure presented in Sec. 2. We take P = 5  N = 100 and Q = 2. We
measure the score of each method as the average mean absolute error (MAE) obtained with 10-fold
cross-validation. Fig. 3 displays the scores of each method when the parameters σ controlling the
noise level and µ controlling the distance from A to Ip are changed. We also investigated the realistic
scenario where each subject has a mixing matrix deviating from a reference: Ai = A + Ei with
entries of Ei sampled i.i.d. from N (0  σ2).
The same experiment with f (p) = √p yields comparable results  yet with Wasserstein distance
performing best and achieving perfect out-of-sample prediction when σ → 0 and A is orthogonal.

E
A
M
d
e
z

 

i
l

a
m
r
o
N

1.00

0.75

0.50

0.25

0.00

log−diag

Wasserstein

geometric

log−diag

Wasserstein

geometric

chance level

chance level

E
A
M
d
e
z

 

i
l

a
m
r
o
N

1.00

0.75

0.50

0.25

0.00

log−diag
sup. log−diag

chance level

Wasserstein
geometric

sup. geometric

1.00

0.75

0.50

0.25

0.00

E
A
M
d
e
z

 

i
l

a
m
r
o
N

0.01

0.10

1.00

σ

10.00

0.0

0.5

1.0

1.5
µ

2.0

2.5

3.0

0.003

0.010

0.030
σ

0.100

0.300

Figure 3: Illustration of Prop.2. Data is generated following the generative model with f = log.
The regression pipeline consists in projecting the data in the tangent space  and then use a linear
model. The left plot shows the evolution of the score when random noise of variance σ2 is added
to the variables yi. The MAE of the geometric distance pipeline goes to 0 in the limit of no noise 
indicating perfect out-of-sample prediction. This illustrates the linearity in the tangent space for the
geometric distance (Prop. 2). The middle plot explores the effect of the parameter µ controlling
the distance between A and IP . Riemannian geometric method is not affected by µ due to its
afﬁne invariance property. Although the Wasserstein distance is not afﬁne invariant  its performance
does not change much with µ. On the contrary  the log-diag method is sensitive to changes in
A. The right plot shows how the score changes when mixing matrices become sample dependent.
We can see then only when σ = 0 supervised + log-diag and Riemann reach perfect performance.
Geometric Riemann is uniformly better and indifferent to projection choice. Wasserstein  despite
model mismatch  outperforms supervised + log-diag with high σ.

4.2 MEG data

Predicting biological age from MEG on the Cambridge center of ageing dataset
In the follow-
ing  we apply our methods to infer age from brain signals. Age is a dominant driver of cross-person
variance in neuroscience data and a serious confounder [39]. As a consequence of the globally
increased average lifespan  ageing has become a central topic in public health that has stimulated
neuropsychiatric research at large scales. The link between age and brain function is therefore of
utmost practical interest in neuroscientiﬁc research.

To predict age from brain signals  here we use the currently largest publicly available MEG dataset
provided by the Cam-CAN [38]. We only considered the signals from magnetometer sensors
(P = 102) as it turns out that once SSS is applied (detailed in Appendix 6.6)  magnetometers and
gradiometers are linear combination of approximately 70 signals (65 ≤ Ri ≤ 73)  which become
redundant in practice [19]. We considered task-free recordings during which participants were asked
to sit still with eyes closed in the absence of systematic stimulation. We then drew T ≃ 520  000 time
samples from N = 595 subjects. To capture age-related changes in cortical brain rhythms [4  44  12] 
we ﬁltered the data into 9 frequency bands: low frequencies [0.1− 1.5]  δ[1.5− 4]  θ[4− 8]  α[8− 15] 
βlow[15 − 26]  βhigh[26 − 35]  γlow[35 − 50]  γmid[50 − 74] and γhigh[76 − 120] (Hz unit). These
frequencies are compatible with conventional deﬁnitions used in the Human Connectome Project
[32]. We verify that the covariance matrices all lie on a small portion of the manifold  justifying
projection in a common tangent space. Then we applied the covariance pipeline independently in
each frequency band and concatenated the ensuing features.

7

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Data-driven covariance projection for age prediction Three types of approaches are here com-
pared: Riemannian methods (Wasserstein or geometric)  methods extracting log-diagonal of matrices
(with or without supervised spatial ﬁltering  see Sec. 3.2) and a biophysics-informed method based
on the MNE source imaging technique [24]. The MNE method essentially consists in a standard
Tikhonov regularized inverse solution and is therefore linear (See Appendix 6.5 for details). Here it
serves as gold-standard informed by the individual anatomy of each subject. It requires a T1-weighted
MRI and the precise measure of the head in the MEG device coordinate system [3] and the coor-
dinate alignment is hard to automate. We conﬁgured MNE with Q = 8196 candidate dipoles. To
obtain spatial smoothing and reduce dimensionality  we averaged the MNE solution using a cortical
parcellation encompassing 448 regions of interest from [31  21]. We then used ridge regression
and tuned its regularization parameter by generalized cross-validation [20] on a logarithmic grid
of 100 values in [10−5  103] on each training fold of a 10-fold cross-validation loop. All numerical
experiments were run using the Scikit-Learn software [36]  the MNE software for processing M/EEG
data [21] and the PyRiemann package [13]. We also ported to Python some part of the Matlab
code of Manopt toolbox [9] for computations involving Wasserstein distance. The proposed method 
including all data preprocessing  applied on the 500GB of raw MEG data from the Cam-CAN dataset 
runs in approximately 12 hours on a regular desktop computer with at least 16GB of RAM. The
preprocessing for the computation of the covariances is embarrassingly parallel and can therefore be
signiﬁcantly accelerated by using multiple CPUs. The actual predictive modeling can be performed
in less than a minute on standard laptop. Code used for data analysis can be found on GitHub5.

log−diag

Wasserstein

geometric

MNE

identity

supervised

identity

unsupervised

biophysics

6

7

8

9

10

11

mean absolute error (years)

Figure 4: Age prediction on Cam-CAN
MEG dataset for different methods  or-
dered by out-of-sample MAE. The y-
axis depicts the projection method  with
identity denoting the absence of projec-
tion. Colors indicate the subsequent em-
bedding. The biophysics-driven MNE
method (blue) performs best.
The
Riemannian methods (orange) follow
closely and their performance depends
little on the projection method. The non-
Riemannian methods log-diag (green)
perform worse  although the supervised
projection clearly helps.

Riemannian projections are the leading data-driven methods Fig. 4 displays the scores for each
method. The biophysically motivated MNE projection yielded the best performance (7.4y MAE) 
closely followed by the purely data-driven Riemannian methods (8.1y MAE). The chance level
was 16y MAE. Interestingly  the Riemannian methods give similar results  and outperformed the
non-Riemannian methods. When Riemannian geometry was not applied  the projection strategy
turned out to be decisive. Here  the supervised method performed best: it reduced the dimension of
the problem while preserving the age-related variance.

Rejecting a null-hypothesis that differences between models are due to chance would require several
independent datasets. Instead  for statistical inference  we considered uncertainty estimates of paired
differences using 100 Monte Carlo splits (10% test set size). For each method  we counted how often
it was performing better than the baseline model obtained with identity and log-diag. We observed
for supervised log-diag 73%  identity Wasserstein 85%  unsupervised geometric 96% and biophysics
95% improvement over baseline. This suggests that inferences will carry over to new data.

Importantly  the supervised spatial ﬁlters and MNE both support model inspection  which is not the
case for the two Riemannian methods. Fig. 5 depicts the marginal patterns [27] from the supervised
ﬁlters and the source-level ridge model  respectively. The sensor-level results suggest predictive
dipolar patterns in the theta to beta range roughly compatible with generators in visual  auditory
and motor cortices. Note that differences in head-position can make the sources appear deeper than

5 https://www.github.com/DavidSabbagh/NeurIPS19_manifold-regression-meeg

8

they are (distance between the red positive and the blue negative poles). Similarly  the MNE-based
model suggests localized predictive differences between frequency bands highlighting auditory  visual
and premotor cortices. While the MNE model supports more exhaustive inspection  the supervised
patterns are still physiologically informative. For example  one can notice that the pattern is more
anterior in the β-band than the α-band  potentially revealing sources in the motor cortex.

Figure 5: Model inspection.
Upper panel: sensor-level pat-
terns from supervised projec-
tion. One can notice dipolar
conﬁgurations varying across
frequencies.
Lower panel:
standard deviation of patterns
over frequencies from MNE
projection highlighting bilat-
eral visual  auditory and pre-
motor cortices.

5 Discussion

In this contribution  we proposed a mathematically principled approach for regression on rank-reduced
covariance matrices from M/EEG data. We applied this framework to the problem of inferring age
from neuroimaging data  for which we made use of the currently largest publicly available MEG
dataset. To the best of our knowledge  this is the ﬁrst study to apply a covariance-based approach
coupled with Riemannian geometry to regression problem in which the target is deﬁned across
persons and not within persons (as in brain-computer interfaces). Moreover  this study reports
the ﬁrst benchmark of age prediction from MEG resting state data on the Cam-CAN. Our results
demonstrate that Riemannian data-driven methods do not fall far behind the gold-standard methods
with biophysical priors  that depend on manual data processing. One limitation of Riemannian
methods is  however  their interpretability compared to other models that allow to report brain-
region and frequency-speciﬁc effects. These results suggest a trade-off between performance and
explainability. Our study suggests that the Riemannian methods have the potential to support
automated large-scale analysis of M/EEG data in the absence of MRI scans. Taken together  this
potentially opens new avenues for biomarker development.

Acknowledgement

This work was supported by a 2018 “médecine numérique” (for digital Medicine) thesis grant issued
by Inserm (French national institute of health and medical research) and Inria (French national
research institute for the digital sciences). It was also partly supported by the European Research
Council Starting Grant SLAB ERC-YStG-676943.

References

[1] P-A Absil  Robert Mahony  and Rodolphe Sepulchre. Optimization algorithms on matrix

manifolds. Princeton University Press  2009.

[2] Anahit Babayan  Miray Erbey  Deniz Kumral  Janis D. Reinelt  Andrea M. F. Reiter  Joseﬁn Röb-
big  H. Lina Schaare  Marie Uhlig  Alfred Anwander  Pierre-Louis Bazin  Annette Horstmann 
Leonie Lampe  Vadim V. Nikulin  Hadas Okon-Singer  Sven Preusser  André Pampel  Chris-
tiane S. Rohr  Julia Sacher  Angelika Thöne-Otto  Sabrina Trapp  Till Nierhaus  Denise Alt-
mann  Katrin Arelin  Maria Blöchl  Edith Bongartz  Patric Breig  Elena Cesnaite  Sufang Chen 
Roberto Cozatl  Saskia Czerwonatis  Gabriele Dambrauskaite  Maria Dreyer  Jessica Enders 
Melina Engelhardt  Marie Michele Fischer  Norman Forschack  Johannes Golchert  Laura Golz 
C. Alexandrina Guran  Susanna Hedrich  Nicole Hentschel  Daria I. Hoffmann  Julia M. Hunten-
burg  Rebecca Jost  Anna Kosatschek  Stella Kunzendorf  Hannah Lammers  Mark E. Lauckner 
Keyvan Mahjoory  Ahmad S. Kanaan  Natacha Mendes  Ramona Menger  Enzo Morino  Karina

9

Näthe  Jennifer Neubauer  Handan Noyan  Sabine Oligschläger  Patricia Panczyszyn-Trzewik 
Dorothee Poehlchen  Nadine Putzke  Sabrina Roski  Marie-Catherine Schaller  Anja Schiefer-
bein  Benito Schlaak  Robert Schmidt  Krzysztof J. Gorgolewski  Hanna Maria Schmidt  Anne
Schrimpf  Sylvia Stasch  Maria Voss  Annett Wiedemann  Daniel S. Margulies  Michael Gae-
bler  and Arno Villringer. A mind-brain-body dataset of MRI  EEG  cognition  emotion  and
peripheral physiology in young and old adults. Scientiﬁc Data  6:180308 EP –  02 2019.

[3] Sylvain Baillet. Magnetoencephalography for brain electrophysiology and imaging. Nature

Neuroscience  20:327 EP –  02 2017.

[4] Luc Berthouze  Leon M James  and Simon F Farmer. Human eeg shows long-range temporal
correlations of oscillation amplitude in theta  alpha and beta bands across a wide age range.
Clinical Neurophysiology  121(8):1187–1197  2010.

[5] Rajendra Bhatia. Positive Deﬁnite Matrices. Princeton University Press  2007.

[6] Rajendra Bhatia  Tanvi Jain  and Yongdo Lim. On the bures–wasserstein distance between

positive deﬁnite matrices. Expositiones Mathematicae  2018.

[7] B. Blankertz  R. Tomioka  S. Lemm  M. Kawanabe  and K. Muller. Optimizing spatial ﬁlters

for robust eeg single-trial analysis. IEEE Signal Processing Magazine  25(1):41–56  2008.

[8] Silvere Bonnabel and Rodolphe Sepulchre. Riemannian metric and geometric mean for positive
semideﬁnite matrices of ﬁxed rank. SIAM Journal on Matrix Analysis and Applications 
31(3):1055–1070  2009.

[9] N. Boumal  B. Mishra  P.-A. Absil  and R. Sepulchre. Manopt  a Matlab toolbox for optimization

on manifolds. Journal of Machine Learning Research  15:1455–1459  2014.

[10] György Buzsáki and Rodolfo Llinás. Space and time in the brain. Science  358(6362):482–485 

2017.

[11] György Buzsáki and Kenji Mizuseki. The log-dynamic brain: how skewed distributions affect

network operations. Nature Reviews Neuroscience  15(4):264  2014.

[12] C Richard Clark  Melinda D Veltmeyer  Rebecca J Hamilton  Elena Simms  Robert Paul  Daniel
Hermens  and Evian Gordon. Spontaneous alpha peak frequency predicts working memory
performance across the age span. International Journal of Psychophysiology  53(1):1–9  2004.

[13] M. Congedo  A. Barachant  and A. Andreev. A new generation of brain-computer interface

based on Riemannian geometry. arXiv e-prints  October 2013.

[14] Marco Congedo  Alexandre Barachant  and Rajendra Bhatia. Riemannian geometry for EEG-
based brain-computer interfaces; a primer and a review. Brain-Computer Interfaces  4(3):155–
174  2017.

[15] Sven Dähne  Frank C Meinecke  Stefan Haufe  Johannes Höhne  Michael Tangermann  Klaus-
Robert Müller  and Vadim V Nikulin. Spoc: a novel framework for relating the amplitude of
neuronal oscillations to behaviorally relevant parameters. NeuroImage  86:111–122  2014.

[16] Jacek Dmochowski  Paul Sajda  Joao Dias  and Lucas Parra. Correlated components of ongoing
eeg point to emotionally laden attention – a possible marker of engagement? Frontiers in
Human Neuroscience  6:112  2012.

[17] Denis A Engemann and Alexandre Gramfort. Automated model selection in covariance estima-

tion and spatial whitening of meg and eeg signals. NeuroImage  108:328–342  2015.

[18] Wolfgang Förstner and Boudewijn Moonen. A metric for covariance matrices. In Geodesy-The

Challenge of the 3rd Millennium  pages 299–309. Springer  2003.

[19] Pilar Garcés  David López-Sanz  Fernando Maestú  and Ernesto Pereda. Choice of magnetome-

ters and gradiometers after signal space separation. Sensors  17(12):2926  2017.

[20] Gene H. Golub  Michael Heath  and Grace Wahba. Generalized cross-validation as a method

for choosing a good ridge parameter. Technometrics  21(2):215–223  1979.

10

[21] Alexandre Gramfort  Martin Luessi  Eric Larson  Denis A. Engemann  Daniel Strohmeier 
Christian Brodbeck  Lauri Parkkonen  and Matti S. Hämäläinen. MNE software for processing
MEG and EEG data. NeuroImage  86:446–460  Feb. 2014.

[22] M. Grosse-Wentrup* and M. Buss. Multiclass common spatial patterns and information theoretic
feature extraction. IEEE Transactions on Biomedical Engineering  55(8):1991–2000  Aug 2008.

[23] Matti Hämäläinen  Riitta Hari  Risto J Ilmoniemi  Jukka Knuutila  and Olli V Lounasmaa.
Magnetoencephalography—theory  instrumentation  and applications to noninvasive studies of
the working human brain. Reviews of modern Physics  65(2):413  1993.

[24] MS Hämäläinen and RJ Ilmoniemi. Interpreting magnetic ﬁelds of the brain: minimum norm

estimates. Technical Report TKK-F-A559  Helsinki University of Technology  1984.

[25] Mehrtash Harandi  Mathieu Salzmann  and Richard Hartley. Dimensionality reduction on spd
manifolds: The emergence of geometry-aware methods. IEEE transactions on pattern analysis
and machine intelligence  40(1):48–62  2017.

[26] Riitta Hari and Aina Puce. MEG-EEG Primer. Oxford University Press  2017.

[27] Stefan Haufe  Frank Meinecke  Kai Görgen  Sven Dähne  John-Dylan Haynes  Benjamin
Blankertz  and Felix Bießmann. On the interpretation of weight vectors of linear models in
multivariate neuroimaging. NeuroImage  87:96 – 110  2014.

[28] Inbal Horev  Florian Yger  and Masashi Sugiyama. Geometry-aware principal component

analysis for symmetric positive deﬁnite matrices. Machine Learning  106  11 2016.

[29] Mainak Jas  Denis A Engemann  Yousra Bekhti  Federico Raimondo  and Alexandre Gramfort.
Autoreject: Automated artifact rejection for MEG and EEG data. NeuroImage  159:417–429 
2017.

[30] Michel Journée  Francis Bach  P-A Absil  and Rodolphe Sepulchre. Low-rank optimization on
the cone of positive semideﬁnite matrices. SIAM Journal on Optimization  20(5):2327–2351 
2010.

[31] Sheraz Khan  Javeria A Hashmi  Fahimeh Mamashli  Konstantinos Michmizos  Manfred G
Kitzbichler  Hari Bharadwaj  Yousra Bekhti  Santosh Ganesan  Keri-Lee A Garel  Susan
Whitﬁeld-Gabrieli  et al. Maturation trajectories of cortical resting-state networks depend on
the mediating frequency band. NeuroImage  174:57–68  2018.

[32] Linda J Larson-Prior  Robert Oostenveld  Stefania Della Penna  G Michalareas  F Prior  Abbas
Babajani-Feremi  J-M Schoffelen  Laura Marzetti  Francesco de Pasquale  F Di Pompeo  et al.
Adding dynamics to the Human Connectome Project with MEG. Neuroimage  80:190–201 
2013.

[33] Franziskus Liem  Gaël Varoquaux  Jana Kynast  Frauke Beyer  Shahrzad Kharabian Masouleh 
Julia M. Huntenburg  Leonie Lampe  Mehdi Rahim  Alexandre Abraham  R. Cameron Craddock 
Stefﬁ Riedel-Heller  Tobias Luck  Markus Loefﬂer  Matthias L. Schroeter  Anja Veronica Witte 
Arno Villringer  and Daniel S. Margulies. Predicting brain-age from multimodal imaging data
captures cognitive impairment. NeuroImage  148:179 – 188  2017.

[34] Scott Makeig  Anthony J. Bell  Tzyy-Ping Jung  and Terrence J. Sejnowski.

Independent
component analysis of electroencephalographic data. In Proceedings of the 8th International
Conference on Neural Information Processing Systems  NIPS’95  pages 145–151  Cambridge 
MA  USA  1995. MIT Press.

[35] Estelle Massart and Pierre-Antoine Absil. Quotient geometry with simple geodesics for the
manifold of ﬁxed-rank positive-semideﬁnite matrices. Technical report  UCLouvain  2018.
preprint on webpage at http://sites.uclouvain.be/absil/2018.06.

[36] F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel 
P. Prettenhofer  R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher 
M. Perrot  and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research  12:2825–2830  2011.

11

[37] Pedro Luiz Coelho Rodrigues  Marco Congedo  and Christian Jutten. Multivariate time-series
analysis via manifold learning. In 2018 IEEE Statistical Signal Processing Workshop (SSP) 
pages 573–577. IEEE  2018.

[38] Meredith A Shafto  Lorraine K Tyler  Marie Dixon  Jason R Taylor  James B Rowe  Rhodri
Cusack  Andrew J Calder  William D Marslen-Wilson  John Duncan  Tim Dalgleish  et al. The
Cambridge Centre for Ageing and Neuroscience (Cam-CAN) study protocol: a cross-sectional 
lifespan  multidisciplinary examination of healthy cognitive ageing. BMC neurology  14(1):204 
2014.

[39] Stephen M Smith and Thomas E Nichols. Statistical challenges in “big data” human neuroimag-

ing. Neuron  97(2):263–268  2018.

[40] Samu Taulu and Matti Kajola. Presentation of electromagnetic multichannel data: the signal

space separation method. Journal of Applied Physics  97(12):124905  2005.

[41] Jason R Taylor  Nitin Williams  Rhodri Cusack  Tibor Auer  Meredith A Shafto  Marie Dixon 
Lorraine K Tyler  Richard N Henson  et al. The Cambridge Centre for Ageing and Neuroscience
(Cam-CAN) data repository: structural and functional MRI  MEG  and cognitive data from a
cross-sectional adult lifespan sample. Neuroimage  144:262–269  2017.

[42] Mikko A Uusitalo and Risto J Ilmoniemi. Signal-space projection method for separating MEG
or EEG into components. Medical and Biological Engineering and Computing  35(2):135–140 
1997.

[43] Bart Vandereycken  P-A Absil  and Stefan Vandewalle. Embedded geometry of the set of
symmetric positive semideﬁnite matrices of ﬁxed rank. In 2009 IEEE/SP 15th Workshop on
Statistical Signal Processing  pages 389–392. IEEE  2009.

[44] Bradley Voytek  Mark A Kramer  John Case  Kyle Q Lepage  Zechari R Tempesta  Robert T
Knight  and Adam Gazzaley. Age-related changes in 1/f neural electrophysiological noise.
Journal of Neuroscience  35(38):13257–13265  2015.

[45] F. Yger  M. Berar  and F. Lotte. Riemannian approaches in brain-computer interfaces: A review.
IEEE Transactions on Neural Systems and Rehabilitation Engineering  25(10):1753–1762  Oct
2017.

12

,David Sabbagh
Pierre Ablin
Gael Varoquaux
Alexandre Gramfort
Denis Engemann