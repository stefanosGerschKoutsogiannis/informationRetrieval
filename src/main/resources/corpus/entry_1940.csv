2019,Exploring Unexplored Tensor Network Decompositions for Convolutional Neural Networks,Tensor decomposition methods are widely used for model compression and fast inference in convolutional neural networks (CNNs). Although many decompositions are conceivable  only CP decomposition and a few others have been applied in practice  and no extensive comparisons have been made between available methods. Previous studies have not determined how many decompositions are available  nor which of them is optimal. In this study  we first characterize a decomposition class specific to CNNs by adopting a flexible graphical notation. The class includes such well-known CNN modules as depthwise separable convolution layers and bottleneck layers  but also previously unknown modules with nonlinear activations. We also experimentally compare the tradeoff between prediction accuracy and time/space complexity for modules found by enumerating all possible decompositions  or by using a neural architecture search. We find some nonlinear decompositions outperform existing ones.,Einconv: Exploring Unexplored Tensor Network

Decompositions for Convolutional Neural Networks

Kohei Hayashi

Preferred Networks

Taiki Yamaguchi˚

The University of Tokyo

hayasick@preferred.jp

yamaguchi@hep-th.phys.s.u-tokyo.ac.jp

Yohei Sugawara

Preferred Networks

suga@preferred.jp

Shin-ichi Maeda
Preferred Networks

ichi@preferred.jp

Abstract

Tensor decomposition methods are widely used for model compression and fast in-
ference in convolutional neural networks (CNNs). Although many decompositions
are conceivable  only CP decomposition and a few others have been applied in
practice  and no extensive comparisons have been made between available methods.
Previous studies have not determined how many decompositions are available  nor
which of them is optimal. In this study  we ﬁrst characterize a decomposition class
speciﬁc to CNNs by adopting a ﬂexible graphical notation. The class includes
such well-known CNN modules as depthwise separable convolution layers and
bottleneck layers  but also previously unknown modules with nonlinear activa-
tions. We also experimentally compare the tradeoff between prediction accuracy
and time/space complexity for modules found by enumerating all possible de-
compositions  or by using a neural architecture search. We ﬁnd some nonlinear
decompositions outperform existing ones.

1

Introduction

Convolutional neural networks (CNNs) typically process spatial data such as images using multiple
convolutional layers [Goodfellow et al.  2016]. The high performance of CNNs is often offset by
their heavy demands on memory and CPU/GPU  making them problematic to deploy on edge devices
such as mobile phones [Howard et al.  2017].

One straightforward approach to reducing costs is the introduction of a low-dimensional linear
structure into the convolutional layers [Smith et al.  1997  Rigamonti et al.  2013  Tai et al.  2015 
Kim et al.  2015  Denton et al.  2014  Lebedev et al.  2014  Wang et al.  2018]. This typically is done
through tensor decomposition  which represents the convolution ﬁlter in sum-product form  reducing
the number of parameters to save memory space and reduce the calculation cost for forwarding paths.

The manner in which this cost reduction is achieved depends heavily on the structure of the tensor
decomposition. For example  if the target is a two-way tensor  i.e.  a matrix  the only meaningful
decomposition is X “ UV  because others such as X “ ABC are reduced to that form but have
more parameters. However  for higher-order tensors  there are many possible ways to perform tensor
decomposition  of which only a few have been actively studied (e.g.  see [Kolda and Bader  2009]).
Such multi-purpose decompositions have been applied to CNNs but are not necessarily optimal for
them  because of tradeoffs between prediction accuracy and time/space complexity. The need to

˚This work was completed during an internship at Preferred Networks.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

X

h

w

˚

c

˚

h1

w1

i

j

T

c1

X

˚

˚

X

˚

˚

X

˚

˚

(a) Standard

(b) Depthwise Separable

(c) Bottleneck/Tucker-2

(d) Inverted Bottleneck

X

˚

˚

˚

˚

X

˚

˚

X

˚

˚

X

˚

˚

(e) Factoring

(f) Flattened

(g) CP

(h) Low-rank Filter

Figure 1: Visualizing linear structures in various convolutional layers  where X is input and T is a
convolution kernel. The “legs” h1  w1  c1 respectively represent the spatial height  spatial width  and
output channels. We will further explain these diagrams in Section 3.

consider many factors  including application domains  tasks  entire CNN architectures  and hardware
limitations  makes the emergence of new optimization techniques inevitable.

In this study  we investigate a hidden realm of tensor decompositions to identify maximally resource-
efﬁcient convolutional layers. We ﬁrst characterize a decomposition class speciﬁc to CNNs by
adopting a ﬂexible hypergraphical notation based on tensor networks [Penrose  1971]. The class
can deal with nonlinear activations  and includes modern light-weight CNN layers such as the
bottleneck layers used in ResNet [He et al.  2015]  the depthwise separable layers used in Mobilenet
V1 [Howard et al.  2017]  the inverted bottleneck layers used in Mobilenet V2 [Sandler et al. 
2018]  and others  as shown in Figure 1. The notation permits us to handle convolutions in three
or more dimensions straightforwardly.
In our experiments  we study the accuracy/complexity
tradeoff by enumerating all possible decompositions for 2D and 3D image data sets. Furthermore 
we evaluate nonlinear extensions by combining neural architecture search with the LeNet and
ResNet architectures. The code implemented in Chainer [Tokui et al.  2019] is available at https:
//github.com/pfnet-research/einconv.

Notation We use the notation rns “ t1  . . .   nu  where n is a positive integer. Lower-case letters
denote scalars when in ordinary type  vectors when in bold (e.g.  a  a). Upper-case letters denote
matrices when in bold  tensors when in bold script (e.g. A  A).

2 Preliminaries

2.1 Convolution in Neural Networks

Consider a 2D image of height H P N  width W P N  and number of channels C P N  where a
channel is a feature (e.g.  R  G  or B) possessed by each pixel. The image can be represented by a
three-way tensor X P RHˆW ˆC . Typically  the convolution operation  applied to such a tensor  will
change the size and the number of channels. We assume that the size of the convolution ﬁlter is odd.
Let I  J P t1  3  5  . . . u be the ﬁlter’s height and width  P P N be the padding size  and S P N be the
stride. Then  the output has height H 1 “ pH ` 2P ´ Iq{S ` 1 and width W 1 “ pW ` 2P ´ Jq{S ` 1.
When we set the number of output channels to C 1 P N  the convolution layer yields an output
Z P RH 1ˆW 1ˆC1

in which each element is given as

tijcc1 xh1

iw1

j c 

(1)

zh1w1c1 “ ÿ
iPrIs

ÿ
jPrJs

ÿ
cPrCs

2

Here T P RIˆJˆCˆC1
i “ ph1 ´ 1qS ` i ´ P
j “ pw1 ´ 1qS ` j ´ P are spatial indices used for convolution. For simplicity  we omit the
and w1
bias parameter. There are IJ CC 1 parameters  and the time complexity of (1) is OpIJ CH 1W 1C 1q.

is a weight  which is termed as the I ˆ J kernel  and h1

Although (1) is standard  there are several important special cases used to reduce computational
complexity. The case when I “ J “ 1 is called 1 ˆ 1 convolution [Lin et al.  2013  Szegedy et al. 
2015]; it applies a linear transformation to the channels only  and does not affect the spatial directions.
Depthwise convolution [Chollet  2016] is arguably the opposite of 1 ˆ 1 convolution: it works as
though the input and output channels (rather than the spatial dimensions) are one dimensional  i.e. 

zh1w1c1 “ ÿ
iPrIs

ÿ
jPrJs

tijc1 xh1

iw1

j c1 .

(2)

2.2 Tensor Decomposition in Convolution

To reduce computational complexity  Kim et al. [2015] applied Tucker-2 decomposition [Tucker 
1966] to the kernel T   replacing the original kernel by T T2  where each element is given by

ijcc1 “ ÿ
tT2
αPrAs

ÿ
βPrBs

gijαβucαvc1β.

(3)

Here G P RIˆJˆAˆB  U P RCˆA  V P RC1ˆB are new parameters  and A  B P N are rank-
like hyperparameters. Note that convolution with the Tucker-2 kernel T T2 is equivalent to three
consecutive convolutions: 1 ˆ 1 convolution with kernel U  I ˆ J convolution with kernel G  and
1 ˆ 1 convolution with kernel V. The hyperparameters A and B may be viewed as intermediate
channels during the three convolutions. Hence  when A  B are smaller than C  C 1  a cost reduction
is expected  because the heavy I ˆ J convolution is now being taken with the A  B channel pair
instead of with C  C 1. The reduction ratios of the number of parameters and the inference cost for the
Tucker-2 decomposition compared to the original are both at least AB{CC 1.

Similarly  several authors [Denton et al.  2014  Lebedev et al.  2014] have employed CP decomposi-
tion [Hitchcock  1927]  which reparametrizes the kernel as

ijcc1 “ ÿ
tCP
γPrΓs

˜uiγ ˜vjγ ˜wcγ ˜sc1γ  

(4)

where ˜U  ˜V  ˜W  ˜S are new parameters and Γ P N is a hyperparameter.

3 The Einconv Layer

We have seen that both the convolution operation (1)  (2) and the decompositions of the kernel
(3)  (4) are given as the sum-product of tensors with many indices. Although the indices may cause
expressions to appear cluttered  they play important roles. There are two classes of indices: those
connected to the output shape (h1  w1  c1) and those used for summation (i  j  c  α  β  γ). Convolution
and its decomposition are speciﬁed by how the indices interact and are distributed into tensor variables.
For example  in Tucker-2 decomposition  the spatial  input channel  and output channel information
G  U  V are separated through their respective indices pi  jq  c1  c. Moreover  they are joined by
two-step connections: the input channel and spatial information are connected by α  and the output
channel and spatial information by β. Here  we can consider the summation indices to be paths that
deliver input information to the output.

A hypergraph captures the index interaction in a clean manner. The basic idea is that tensors are
distinguished only by the indices they own and we consider them as vertices. Vertices are connected
if the corresponding tensors share indices to be summed. (For notational simplicity  we will often
refer to a tensor by its indices alone  i.e.  U “ puabcqaPrAs bPrBs cPrCs is equivalent to ta  b  cu.)
As an example  consider the decomposition of a kernel T . Let the outer indices O “ ti  j  c  c1u be
the indices of the shape of T   the inner indices I “ pr1  r2  . . . q be the indices used for summation 
and inner dimensions R “ pR1  R2  . . . q P R|I| be the dimensions of I. Assume that M P N tensors
are involved in the decomposition  and let V “ tv1  . . .   vM | vm P 2OYIu denote the set of these
tensors  where 2A denotes the power set of a set A. Given V  each inner index r P I deﬁnes a

3

hyperedge er “ tv | r P v for v P Vu. Let E “ ten | n P O Y Iu denote the set of hyperedges. For
example  suppose I “ tα  βu and V “ tti  j  α  βu  tc  αu  tc1  βuu; then  the undirected weighted
hypergraph pV  E  Rq is equivalent to Tucker-2 decomposition (3).

This idea is also applicable to the convolution operation by the introduction of dummy tensors
that absorb the index patterns used in convolution. Recall that in (1) the special index h1
i indicates
which vertical elements of the kernel and the input image are coupled in the convolution. Let
P P t0  1uHˆH 1ˆI be a (dummy) binary tensor where each element is deﬁned as phh1i “ 1 if h “ h1
i
and 0 otherwise  and let Q P t0  1uW ˆW 1ˆJ be the horizontal counterpart of P. Furthermore  let
us modify the index sets to O “ th1  w1  c1u and I “ ph  w  i  j  cq  and the dimensions to R “
pH  W  I  J  Cq. Then  vertices V “ tth  w  cu  ti  j  c  c1u  th  h1  iu  tw  w1  juu and hyperedges E
that are automatically deﬁned by V exactly represent the convolution operation (1)  where we ensure
that the tensor of th  h1  iu is ﬁxed by P and the tensor of tw  w1  ju is ﬁxed by Q.

The above mathematical explanation may sound too winding  but visualization will help greatly. Let
us introduce several building blocks for the visualization. Let a circle (vertex) indicate a tensor  and a
line (edge) connected to the circle indicate an index associated with that tensor. When an edge is
connected on only one side  it corresponds to an outer index of the tensor; otherwise  it corresponds
to an inner index used for summation. The summation and elimination of inner indices is called
contraction. For example 

A

j

B

i

k

“

C

i

k

ðñ ÿ

aijbjk “ cik.

j

A hyperedge that is connected to more than three vertices is depicted with a black dot:

A

j

B

i

C

k

ðñ ÿ

aijbjcjk.

j

(5)

(6)

Finally  a node with symbol “˚” indicates a dummy tensor. In our context  this implicitly indicates
that some spatial convolution is involved:

h1

˚

B

i

A

h

ðñ ÿ

phh1iahbi

h i

(7)

The use of a single hyperedge to represent the summed inner index is the graphical equivalent of
the Einstein summation convention in tensor algebra. Inspired by this equivalence and by NumPy’s
einsum function [Wiebe  2011]  we term a hypergraphically-representable convolution layer an
Einconv layer.

3.1 Examples

In Figure 1  we give several examples of hypergraphical notation. Many existing CNN modules can
obviously be described as Einconv layers (but without nonlinear activation).

Separable and Low-rank Filters Although a kernel is usually square  i.e.  I “ J   we often take
the convolution separately along the vertical and horizontal directions. In this case  the convolution
operation is equivalent to the application of two ﬁlters of sizes pI  1q and p1  Jq. This can be
considered the rank-1 approximation of the I ˆ J convolution. A separable ﬁlter [Smith et al.  1997]
is a technique to speed up convolution when the ﬁlter is exactly of rank one. Rigamonti et al. [2013]
extended this idea by approximating ﬁlters as low-rank matrices for a single input channel  and Tai
et al. [2015] further extended it for multiple input channels (Figure 1h).

Factored Convolution In the case of a large ﬁlter size  factored convolution is commonly used to
replace the large ﬁlter with multiple small-sized convolutions [Szegedy et al.  2016]. For example 
two consecutive 3 ˆ 3 convolutions are equivalent to one 5 ˆ 5 convolution in which the ﬁrst 3 ˆ 3
ﬁlter has been enlarged by the second 3 ˆ 3 ﬁlter. Interestingly  the factorization of convolution is
exactly represented in Einconv by adding two additional dummy tensors.

4

h

w

d

X

h1

i
w1
j

d1

k

˚
˚
˚
c

c1

T

X

˚
˚
˚

˚
˚
˚

X

(a) Standard

(b) Depthwise Separable

(c) (2+1)D

Figure 2: Graphical visualizations of 3D convolutions.

Bottleneck Layers
In ResNet [He et al.  2015]  the bottleneck module is used as a building
block: input channels are reduced before convolution  and then expanded afterwards. Finally  a
skip connection is used  re-adding the original input. Figure 1c shows the module without the skip
connection. From the diagram  we see that the linear structure of the bottleneck is equivalent to
Tucker-2 decomposition.

Depthwise Separable Convolution Mobilenet V1 [Howard et al.  2017] is a seminal light-weight
architecture. It employs depthwise separable convolution [Sifre and Mallat  2014  Chollet  2016] as a
building block; this is a combination of depthwise and 1 ˆ 1 convolution (Figure 1b)  and works well
with limited computational resources.

Inverted Bottleneck Layers Mobilenet V2 [Sandler et al.  2018]  the second generation of Mo-
bilenet  employs a building block called the inverted bottleneck module (Figure 1d). It is similar
to the bottleneck module  but there are two differences. First  whereas in the bottleneck module 
the number of intermediate channels is smaller than the number of input or of output channels  in
the inverted bottleneck this relationship is reversed  and the intermediate channels are “ballooned”.
Second  there are two intermediate channels in the bottleneck module  while the inverted bottleneck
has only one.

3.2 Higher Order Convolution

We have  thus far  considered 2D convolution  but Einconv can naturally handle higher-order convolu-
tion. For example  consider a 3D convolution. Let d  d1 be the input/output indices for depth  and k
be the index of ﬁlter depth. Then  by adding d1 to O and d  k to I  we can construct a hypergraph
for 3D convolution. Figure 2 shows the hypergraphs for the the standard 3D convolution and for
two light-weight convolutions: the depthwise separable convolution [Köpüklü et al.  2019]  and
the (2+1)D convolution [Tran et al.  2018] which factorizes a full 3D convolution into 2D and 1D
convolutions.

3.3 Reduction and Enumeration

Although the hypergraphical notation is powerful  we need to be careful about its redundancy. For
example  consider a hypergraph pV  Eq where an inner index a P I is only used by the m-th tensor 
i.e  a P vm and a R vn for n ‰ m. Then  any tensors represented by pV  Eq  whatever their inner
dimensions  are also represented by removing a from every element of V and the a-th hyperedge
from E. Similarly  self loops do not increase the representability [Ye and Lim  2018]. In terms
of representability of the Einconv layer  there is no reason to choose redundant hypergraphs.2 We
therefore want to remove them efﬁciently.

For simplicity  let us consider the 2D convolution case  in which the results are straightforwardly
extensible to higher-order cases. Let z denote the set difference operator and n denote the element-
wise set difference operator  which is used to remove an index from all vertices  e.g.  V n a “
tv1za  . . .   vM zau for index a P O Y I. For convenience  we deﬁne a map θ : O Y I Ñ N that

2It might be possible that some redundant Einconv layer outperforms equivalent nonredundant ones  because

parametrization inﬂuences optimization. However  we focus here on representability alone.

5

returns the dimension of an index a P O Y I  e.g. θpiq “ I. To discuss representability  we introduce
the following notation for the space of Einconv layers:
Deﬁnition 1. Given vertices V “ tv1  . . .   vM u and inner dimensions R  let FV : RŚaPv1
¨ ¨ ¨ ˆ RŚaPvM
TV pRq Ď RIˆJˆCˆC1
RŚaPvm

θpaq ˆ
be the contraction of M tensors along with V. In addition  let
be the space that FV covers  i.e.  TV pRq “ tFV pU1  . . .   UM q | Um P

θpaq Ñ RIˆJˆCˆC1

θpaq for m P rM su.

Next  we show several sufﬁcient conditions for hypergraphs to be redundant.
Proposition 1 (Ye and Lim 2018  Proposition 3.5). Given inner dimensions R P R|I|  if Ra “ 1 
TV pRq is equivalent to TVnap. . .   Ra´1  Ra`1  . . . q.
Proposition 2. If vm Ď vn for some m  n P rM s  TV pRq is equivalent to TVzvmpRq.
Proposition 3. If ea “ eb for a  b P I  TV pRq is equivalent to TVnap ˜Rq where ˜R “
p. . .   Ra´1  Ra`1  . . .   Rb´1  RaRb  Rb`1  . . . q.
Proposition 4. Assume the convolution is size-invariant  i.e.  H “ H 1 and W “ W 1. Then  given
ﬁlter height and width I  J P t1  3  5  . . . u  the number of possible combinations that eventually
achieve I ˆ J convolution is πp I´1
2 q  where π : N Ñ N is the partition function of integers.
(See [Sloane  2019] for examples.)

2 qπp J´1

Proposition 1 says that  if the inner dimension of an inner index is one  we can eliminate it from
the hypergraph. Proposition 2 shows that  if the indices of a vertex form a subset of the indices of
another vertex (e.g. v1 “ ta  cu and v2 “ ta  b  cu)  we can remove the ﬁrst vertex. Proposition 3
means that a “double” hyperedge on the dimensions A  B P N is reduced to a single hyperedge on
the dimension AB. Proposition 4 tells us the possible choices of ﬁlter size. We defer the proofs to the
Supplementary material. By combining the above propositions  we can obtain the following theorem:

Theorem 1. If the number of inner indices and the ﬁlter size is ﬁnite  the set of nonredundant
hypergraphs representing convolution (1) is ﬁnite.

To enumerate nonredundant hypergraphs  we ﬁrst use the condition of Proposition 2. Because of the
vertex-subset constraint in Proposition 2  a valid vertex set must be a subset of the power set of all
the indices O Y I  and its size is at most 22|OYI|
. After enumerating the vertex sets satisfying this
constraint  we eliminate some of them using the other propositions.3 We used this algorithm in the
experiments (Section 6).

4 Nonlinear Extension

Tensor decomposition involves multiple linear operations  and each vertex can be seen as a linear
layer. For example  consider a linear map W : RC Ñ RC1
. If W is written as a product of three
matrices W “ ABC  we can consider the linear map to be a composition of three linear layers:
Wpxq “ pA ˝ B ˝ Cqpxq for a vector input x P RC . This might lead one to conclude that  in addition
to reducing computational complexity  tensor decomposition with many vertices also contributes
to an increase in representability. However  because the rank of W is determined by the minimum
rank of either A  B  or C  and the representability of a matrix is solely controlled by its rank  adding
linear layers does not improve representability. This problem arises in Einconv layers.

A simple solution is to add nonlinear functions between linear layers. Although this is easy to
implement  enumeration is no longer possible  because the equivalence relation becomes non-trivial
with the introduction of nonlinearity  causing an inﬁnite number of candidates to exist. It is not
possible to enumerate an inﬁnite number of candidates  thus an efﬁcient neural architecture search
algorithm ( [Zoph and Le  2016]) is needed. Many such algorithms have been proposed  based on
genetic algorithms (GAs) [Real et al.  2018]  reinforcement learning [Zoph and Le  2016]  and other
methods [Zoph et al.  2018  Pham et al.  2018]. In this study  we employ GA because hypergraphs have
a discrete structure that is highly compatible with it. As multiobjective optimization problems need
to be solved (e.g.  number of parameters vs. prediction accuracy)  we use the nondominated sorting

3For more details  see the real code: https://github.com/pfnet-research/einconv/blob/master/

enumerate_graph.py

6

genetic algorithm II (NSGA2) [Deb et al.  2002]  which is one of the most popular multiobjective
GAs. In Section 6.2 we will demonstrate that we can ﬁnd better Einconv layers by GA than by
enumeration.

5 Related Work

Tensor network notation  a graphical notation for linear tensor operations  was developed by the
quantum many-body physics community (see tutorial by Bridgeman and Chubb [2017]). Our notation
is basically a subset of this  except that ours allows hyperedges. Such hyperedges are convenient for
representing certain convolutions  such as depthwise convolution (see Figure 1b; the inclusion of the
rightmost vertex indicates depthwise convolution). The reduction of redundant tensor networks was
recently studied by Ye and Lim [2018]  and we extended the idea to include convolution (Section 3.3).

There are several studies that combine deep neural networks and tensor networks. Stoudenmire and
Schwab [2016] studied shallow fully-connected neural networks  where the weight is decomposed
using the tensor train decomposition [Oseledets  2011]. Novikov et al. [2015] took a similar approach
to deep feed-forward networks  which was later extended to recurrent neural networks [He et al. 
2017  Yang et al.  2017]. Cohen and Shashua [2016] addressed a CNN architecture that can be
viewed as a huge tensor decomposition. They interpreted the entire forward process  including the
pooling operation  as a tensor decomposition; this differs from our approach of reformulating a single
convolutional layer. Another difference is their focus on a speciﬁc decomposition (hierarchical Tucker
decomposition [Hackbusch and Kühn  2009]); we do not impose any restrictions on decomposition
forms.

6 Experiments

We examined the performance tradeoffs of Einconv layers in image classiﬁcation tasks. We measured
time complexity by counting the FLOPs of the entire forwarding path  and space complexity by
counting the total number of parameters. All the experiments were conducted on NVIDIA P100 and
V100 GPUs. The details of the training recipes are described in the Supplementary material.

6.1 Enumeration

First  we investigated the basic classes of Einconv for 2D and 3D convolutions. For 2D convolution
with a ﬁlter size of 3 ˆ 3  we enumerated the 901 nonredundant hypergraphs having at most two
inner indices  where the inner dimensions were all ﬁxed to 2. In addition to these  we compared
baseline Einconv layers that include nonlinear activations and/or more inner indices. We used the
Fashion-MNIST dataset [Xiao et al.  2017] to train the LeNet-5 network [LeCun et al.  1998]. The
result (Figure 3) shows that  in terms of FLOPs  two baselines (standard and CP) achieve Pareto
optimality  but other nameless Einconv layers ﬁll the gap between those two.

Similarly  for a 3 ˆ 3 ˆ 3 ﬁlter  we enumerated 3D Einconv having at most one inner index  of which
there were 492 instances in total. We used the 3D MNIST dataset [de la Iglesia Castro  2016] with
architecture inspired by C3D [Tran et al.  2014]. The results (Figure 4) show that  in contrast to the
2D case  the baselines dominated the Pareto frontier. This could be because we did not enumerate the
case with two inner indices due to its enormous size.4

6.2 GA Search with Non-linear Activation

Next  we evaluated the full potential of Einconv by combining it with a neural architecture search.
In contrast to the previous experiments  we used Einconv layers from a larger space  i.e.  we
allowed nonlinear activations (ReLUs)  factoring-like multiple convolutions  and changes of the
inner dimensions. We employed two architectures: LeNet-5 and ResNet-50. We trained LeNet-5
with the Fashion-MNIST dataset  and the ResNet-50 with the CIFAR-10 dataset. Note that  for
ResNet-50  a signiﬁcant number of Einconv instances could not be trained because the GPU memory

4For 3D convolution  the number of tensor decompositions having two inner indices is more than ten thousand.
Training all of them would require 0.1 million CPU/GPU days  which was infeasible with our computational
resources.

7

●

●
●

●

●

y
c
a
r
u
c
c
A

 
t
s
e
T

0.88

0.86

0.84

●

●

●

●

●
●

●
●
●
●
●

●
●

●

●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●

●

●
●

●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●
●
●

●

●

●
●
●

●
●

●
●

●
●

●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●
●

●
●
●

●
●
●

●
●
●

●
●
●
●

●
●
●
●
●
●
●

●
●
●
●
●
●
●

●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●
●

●
●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●
●

●

●

●
●

●

●

●

●
●

●

●
●

●
●
●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●
●
●
●

●

●
●

●

●

●

●
●
●
●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●

●

●
●

●
●

●
●

●

●

●

●
●

●
●

●

●
●
●
●

●
●
●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●

●
●

●
●

●
●

●

●

●
●

●
●
●
●

●

●

●

●

●
●

●

●

●

●

●
●

●
●
●

●
●

●
●
●
●
●
●
●
●

●
●
●

●
●
●

●

●

●
●

●

●

●

●
●
●

●
●

●

●
●

●
●

●
●

●
●

●

●
●
●
●
●
●
●
●
●
●
●
●

●

●

●
●

●
●
●
●
●
●

●

●

●

●
●
●
●
●
●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●
●
●

●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●

●

●
●
●
●

●
●

●
●
●
●
●
●

●

●

●

●

●
●
●
●

●

●

●
●
●

●

●
●
●

●

●

●

●
●
●

●

●

●

●
●

●
●

●
●
●

●

●
●

●

●

●

●

●

●

●
●

●

●

●

●

●●
●
●
●
●

●

●
●
●
●
●

●

●
●
●

●
●
●

●
●

●
●

●

●

●

●
●
●
●

●
●
●
●
●

●

●

●
●

●

●
●
●
●

●

●

●

●

●
●
●

●

●
●
●

●
●

●

●

●

●

●
●

●

●

●
●

●

●
●
●

●
●

●

●
●

●
●
●

●
●

●

●

●

●

●

●

●
●
●

●

●

●

●

●

●

●
●

●

●
●

●
●
●

●

●

●

●
●
●

●

●
●

●

●

●
●
●

●
●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●

●

●

●
●

●
●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●
●

●
●
●●

●

●
●

●

●
●

●
●

●

●

●

●
●

●
●

●

●
●

●

●
●
●
●
●
●

●

●
●

●

●

●

●

●

●
●

●
●
●

●

●

●

●

●
●
●

●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●
●
●
●

●

●

●

●

●

●

●

●

●
●
●

●

●

●

●

●

●

●

●
●

●
●
●

●

●

●
●

●

●

●

●

●
●
●

●

●
●

●
●

●
●
●

●
●

●

●

●

●

●
●

●
●
●

●
●
●
●

●
●

●

●

●

●
●

●
●
●
●

●
●
●

●

●
●

●

●
●
●
●
●
●
●

●
●
●

●
●
●
●
●
●
●

●

●

●

●
●
●

●

●
●

●

●
●
●
●
●
●
●

●

●
●

●
●
●

●

●

●
●

●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●

●
●

●

●

●

●
●
●
●
●

●

●

●

●

●

●

●
●

●

●
●
●
●
●
●

●
●
●
●

●
●

●

●

●

●

●

●
●

●
●

●
●

●

●

●

●

●
●
●
●
●
●

●

●

●
●
●
●
●
●
●
●

●

●

●

●

●

●
●
●

●
●
●
●

●
●

●

●

●
●
●
●

●
●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

● standard

flattend

dep.sep.

bottleneck

cp

104.8

104.9

105

105.1

105.2

105.3

106

107

108

Number of Parameters

Total FLOPs

Figure 3: Enumeration of 2D Einconv for LeNet-5 trained with Fashion-MNIST. Black dots indicate
unnamed tensor decompositions found by the enumeration.

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
A

 
t
s
e
T

●

●

●

●
●

●

●

●
●

●

●
●
●
●

●

●

●
●
●
●

●

●
●

●

●

●
●

●

●

●

●
●
●

●
●
●
●
●
●
●

●
●

●
●
●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●

●
●

●

●

●

●

●
●
●

●

●

●
●

●
●
●

●

●
●
●

●

●
●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●

●

●

●
●
●
●

●

●

●

●
●

●

●

●

●

●

●
●

●
●
●
●
●
●
●
●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●
●

●
●
●
●
●
●

●

●

●
●
●
●
●

●

●
●
●

●
●
●

●

●

●
●
●
●
●

●
●
●
●
●
●
●
●

●

●

●
●

●

●

●

●

●
●

●

●
●
●

●
●

●

●

●

●

●

●

●

●

●

●

●
●
●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●
●
●

●

●

●

●
●

●

●
●

●

●

●

●
●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●
●

●
●
●

●
●
●
●

●
●

●

●

●

●

●

● standard

dep.sep.

cp

2p1

tt

ht

●

●
●

●

●
●

●

●

●

●

●

●
●

●

●

●

●
●

●

●
●

●
●
●
●
●
●
●

●

●

●

●
●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

106

106.5

107

106.5

107

107.5

108

108.5

109

109.5

Number of Parameters

Total FLOPs

Figure 4: Enumeration of 3D Einconv for C3D-like networks trained with 3D MNIST  where 2p1  tt 
and ht mean (2+1)D convolution [Tran et al.  2018]  tensor train decomposition [Oseledets  2011] 
and hierarchical Tucker decomposition [Hackbusch and Kühn  2009]  respectively.

was insufﬁcient. For the GA search  we followed the strategy of AmoebaNet [Real et al.  2018]:
we did not use crossover operations  and siblings were produced only by mutation. Five mutation
operations were prepared for changing the number of vertices/hyperedges and two for changing the
order of contraction.5 We set test accuracy and the number of parameters as multiobjectives to be
optimized by NSGA2.

The results of LeNet-5 (Figure 5) show the tradeoff between the multiobjectives. Within the clearly
deﬁned and relatively smooth Pareto frontier  nameless Einconv layers outperform the baselines.
The best accuracy achieved by Einconv was „ 0.92  which was better than that of the standard
convolution („ 0.91). Although the results of ResNet-50 (Figure 6) show a relatively rugged Pareto
frontier  Einconv still achieves better tradeoffs than named baselines other than the standard and CP
convolutions.

7 Conclusion and Discussion

Herein  we studied hypergraphical structures in CNNs. We found that a variety of CNN layers may
be described hypergraphically  and that there exists an enormous number of variants never previously
encountered. We found experimentally that the Einconv layers  the proposed generalized CNN layers 
yielded excellent results.

One striking observation from the experiments is that certain existing decompositions  such as CP
decomposition  consistently achieved good accuracy/complexity tradeoffs. This empirical result is
somewhat unexpected; there is no theoretical reason that existing decompositions should outperform

5See https://github.com/pfnet-research/einconv/blob/master/mutation.py for implementa-

tion details.

8

0.92

0.88

0.84

0.80

y
c
a
r
u
c
c
A

 
t
s
e
T

●

●

●

●

●
●

●

●

●

●
●
●
●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●
●

●
●
●

●
●
●

●
●
●
●

●

●

●

●

●
●

●

●
●
●
●

●

●

●
●

●

●

●

●

●
●

●
●

●

●

●
●

●

●

●

●

●

●
●

●

●

●

●

●
●

●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●

●

●

●

●
●

●
●

●

●

●
●

●
●
●

●

●

●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●

●
●
●
●

●
●
● ●
●

●
●
●
●
●
●
●

●

●

●
●
●
●

● ●
●
●

●

●

●
●
●
●
●
●

●
●
●

●

●
●

●
●

●

●

●

●

●

●
●

●

●

●

●

●
●
●

●

●
●

●

●
●

●

●

●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●

●

●
●

●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●

●
●
●

●
●
●

●
●

●
●

●
●

●

●

●
●

●

●
●

●

●

●
●

●

●

●

●
●

●

●

●

●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

● ●
●
●

●

●

●
●
●
●
●

●

●
●

●

●

●

●

●

●

●
●
●

●

●

●
●

●

●

●
●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●
●
●
●
●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●
●

●

●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●
●
●

●

●

●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

●

●

●

●

●

●

●
●
●

●
●

●

●

●

●

●

●

●

●
●
●

●

●

●

●

●

●

●

●

●

●
●
●

●

●
●

●

●

●
●

●

●

●

●

●

●

●

●
●

●

●

●

●

●

●

●

●

● standard

flattend

dep.sep.

bottleneck

cp

factoring

104.6

104.8

105

105.2

105.4

105.6

106

107

108

109

1010

Number of Parameters

Total FLOPs

Figure 5: GA search of 2D Einconv for LeNet-5 trained with Fashion-MNIST. Black dots indicate
unnamed tensor decompositions found by the GA search.

●

●

●
●

●

●

●

●
●
●
●
●
●
●

●

●
●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●

●

●
●
●
●
●
●

●

●

●

●

●
●

●

●

●

●
●

●
●

●
●

●

●

●

●
●
●
●
●
●

●

●
●

●

●

●

●

●

●

●
●
●
●
●
●

●

●

●
●
●

●

●

●

●

●
●
●
●
●
●
●
●
●
●
●

●
●
●
●

●

●
●

●

●

●

●

●

●

●

●

●
●

●

●
●

●

●

●

●

●

●

●
●
●
●
●
●

●
●
●
●
●
●
●
●

●
●
●

●

●
●
●
●

●
●

●

●

●

●
●

●

●

●
●

●

●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●

●
●

●

●

●
●
●
●

●

●

●

●

●
●

●

●
●

●

●

●

y
c
a
r
u
c
c
A

 
t
s
e
T

0.90

0.85

0.80

●

●
●

●
●

●
●

●
●

●

●

●

●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●

●

●
●
●

●

●

●
●

●
●
●
●
●
●
●

●
●
●
●
●
●

●

●

●

●

●

●

●

●

●

●

●

●

●
●

●

●

●
●

●

●

●
●
●
●
●
●

●
●
●
●
●
●
●
●

●
●
●

●

●
●
●
●

●
●

●

●

●

●
●

●

●

●
●

●

●
●
●
●
●
●

●
●
●

●

●

●
●

●
●
●
●
●
●
●
●

●
●
●

●

●

●
●
●
●
●
●

●

●
●

●

●
●

●

●
●

●

●

●

●

●

●

●

●

●

●

●
●
●
●
●
●
●

●

●

●

●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●

●

●
●
●
●
●
●

●

●

●

●

●
●

●

●

●

●
●

●

●

●

●

●
●

●

●

●

●

●
●
●
●

●

●
●

●
●

●

●

●

●

●
●

●
●

●

●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●

●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●

●
●
●
●

●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●

●
●
●

●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●

●

●

●

●

●

●
●
●
●

●

●

●

●

●

●

●
●

●

●

●
●

●
●

●
●

●

●
●

●
●
●
●
●
●
●

●
●
●
●
●
●

●

●
●
●

●

●

●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●

●
●
●

●

●

●

●

●

● standard

flattend

dep.sep.

bottleneck

cp

factoring

107

107.5

108

108.5

108

108.5

109

109.5

1010

Number of Parameters

Total FLOPs

Figure 6: GA search of 2D Einconv for ResNet-50 trained with CIFAR-10.

the new  unnamed ones. Developing a theory capable of explaining this phenomenon  or at least
of characterizing the necessary conditions (e.g. symmetricity of decomposition) to achieve good
tradeoffs would be a promising (but challenging) direction for future work.

One major limitation at present is the computational cost of searching. For example  the GA search
for ResNet-50 in Section 6.2 took 829 CPU/GPU days. This was mainly because of the long training
periods (approximately 10 CPU/GPU hours for each training)  but also because the GA may not
have been leveraging the information on hypergraphs well. Although we incorporated some prior
knowledge of hypergraphs such as the proximity regarding edge removing and vertex adding through
mutation operations  simultaneous optimization of hypergraph structures and neural networks using
sparse methods such as LASSO or Bayesian sparse models may be more promising.

Acknowledgments

We thank our colleagues  especially Tommi Kerola  Mitsuru Kusumoto  Kazuki Matoya  Shotaro
Sano  Gentaro Watanabe  and Toshihiko Yanase  for helpful discussion  and Takuya Akiba for
implementing the prototype of an enumeration algorithm. We also thank Jacob Bridgeman for
sharing an elegant TikZ style for drawing tensor network diagrams. We ﬁnally thank the anonymous
(meta-)reviewers for helpful comments and discussion.

References

J. C. Bridgeman and C. T. Chubb. Hand-waving and interpretive dance: an introductory course on

tensor networks. Journal of Physics A: Mathematical and Theoretical  50(22):223001  2017.

F. Chollet. Xception: Deep learning with depthwise separable convolutions  corr abs/1610.02357.

URL http://arxiv. org/abs/1610.02357  2016.

9

N. Cohen and A. Shashua. Convolutional rectiﬁer networks as generalized tensor decompositions. In

International Conference on Machine Learning  pages 955–963  2016.

D. de la Iglesia Castro. 3d mnist dataset. https://www.kaggle.com/daavoo/3d-mnist  2016.

K. Deb  A. Pratap  S. Agarwal  and T. Meyarivan. A fast and elitist multiobjective genetic algorithm:

Nsga-ii. IEEE transactions on evolutionary computation  6(2):182–197  2002.

E. L. Denton  W. Zaremba  J. Bruna  Y. LeCun  and R. Fergus. Exploiting linear structure within
convolutional networks for efﬁcient evaluation. In Advances in neural information processing
systems  pages 1269–1277  2014.

I. Goodfellow  Y. Bengio  and A. Courville. Deep learning. MIT press  2016.

W. Hackbusch and S. Kühn. A new scheme for the tensor representation. Journal of Fourier analysis

and applications  15(5):706–722  2009.

K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. eprint. arXiv

preprint arXiv:0706.1234  2015.

Z. He  S. Gao  L. Xiao  D. Liu  H. He  and D. Barber. Wider and deeper  cheaper and faster:
Tensorized lstms for sequence learning. In Advances in neural information processing systems 
pages 1–11  2017.

F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of Mathematics

and Physics  6(1-4):164–189  1927.

A. G. Howard  M. Zhu  B. Chen  D. Kalenichenko  W. Wang  T. Weyand  M. Andreetto  and H. Adam.
Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint
arXiv:1704.04861  2017.

Y.-D. Kim  E. Park  S. Yoo  T. Choi  L. Yang  and D. Shin. Compression of deep convolutional neural

networks for fast and low power mobile applications. arXiv preprint arXiv:1511.06530  2015.

T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review  51(3):455–500 

2009.

O. Köpüklü  N. Kose  A. Gunduz  and G. Rigoll. Resource efﬁcient 3d convolutional neural networks.

arXiv preprint arXiv:1904.02422  2019.

V. Lebedev  Y. Ganin  M. Rakhuba  I. Oseledets  and V. Lempitsky. Speeding-up convolutional neural

networks using ﬁne-tuned cp-decomposition. arXiv preprint arXiv:1412.6553  2014.

Y. LeCun  L. Bottou  Y. Bengio  P. Haffner  et al. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

M. Lin  Q. Chen  and S. Yan. Network in network. arXiv preprint arXiv:1312.4400  2013.

A. Novikov  D. Podoprikhin  A. Osokin  and D. P. Vetrov. Tensorizing neural networks. In Advances

in neural information processing systems  pages 442–450  2015.

I. V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientiﬁc Computing  33(5):2295–2317 

2011.

R. Penrose. Applications of negative dimensional tensors. Combinatorial mathematics and its

applications  1:221–244  1971.

H. Pham  M. Y. Guan  B. Zoph  Q. V. Le  and J. Dean. Efﬁcient neural architecture search via

parameter sharing. arXiv preprint arXiv:1802.03268  2018.

E. Real  A. Aggarwal  Y. Huang  and Q. V. Le. Regularized evolution for image classiﬁer architecture

search. arXiv preprint arXiv:1802.01548  2018.

R. Rigamonti  A. Sironi  V. Lepetit  and P. Fua. Learning separable ﬁlters. In Proceedings of the

IEEE conference on computer vision and pattern recognition  pages 2754–2761  2013.

10

M. Sandler  A. Howard  M. Zhu  A. Zhmoginov  and L.-C. Chen. Mobilenetv2: Inverted residuals
and linear bottlenecks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 4510–4520  2018.

L. Sifre and S. Mallat. Rigid-motion scattering for image classiﬁcation. PhD thesis  Ph. D. thesis  1:

3  2014.

N. J. A. Sloane. The On-Line Encyclopedia of Integer Sequences. A000041  2019.

S. W. Smith et al. The scientist and engineer’s guide to digital signal processing. California Technical

Pub. San Diego  1997.

E. Stoudenmire and D. J. Schwab. Supervised learning with tensor networks. In Advances in Neural

Information Processing Systems  pages 4799–4807  2016.

C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke  and
A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition  pages 1–9  2015.

C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna. Rethinking the inception architecture
for computer vision. In Proceedings of the IEEE conference on computer vision and pattern
recognition  pages 2818–2826  2016.

C. Tai  T. Xiao  Y. Zhang  X. Wang  et al. Convolutional neural networks with low-rank regularization.

arXiv preprint arXiv:1511.06067  2015.

S. Tokui  R. Okuta  T. Akiba  Y. Niitani  T. Ogawa  S. Saito  S. Suzuki  K. Uenishi  B. Vogel  and
H. Yamazaki Vincent. Chainer: A deep learning framework for accelerating the research cycle.
In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining  pages 2002–2011. ACM  2019.

D. Tran  L. Bourdev  R. Fergus  L. Torresani  and M. Paluri. “learning spatiotemporal features with

3d convolutional networks ”. arXiv preprint arXiv:1412.0767  1177  2014.

D. Tran  H. Wang  L. Torresani  J. Ray  Y. LeCun  and M. Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision
and Pattern Recognition  pages 6450–6459  2018.

L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika  31(3):279–311 

1966.

W. Wang  Y. Sun  B. Eriksson  W. Wang  and V. Aggarwal. Wide compression: Tensor ring nets.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
9329–9338  2018.

M. Wiebe.

Numpy-discussion:

einsum.

https://mail.python.org/pipermail/

numpy-discussion/2011-January/054586.html  2011.

H. Xiao  K. Rasul  and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine

learning algorithms  2017.

Y. Yang  D. Krompass  and V. Tresp. Tensor-train recurrent neural networks for video classiﬁcation.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
3891–3900. JMLR. org  2017.

K. Ye and L.-H. Lim. Tensor network ranks. arXiv preprint arXiv:1801.02662  2018.

B. Zoph and Q. V. Le. Neural architecture search with reinforcement learning. arXiv preprint

arXiv:1611.01578  2016.

B. Zoph  V. Vasudevan  J. Shlens  and Q. V. Le. Learning transferable architectures for scalable image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 8697–8710  2018.

11

,Kohei Hayashi
Taiki Yamaguchi
Yohei Sugawara
Shin-ichi Maeda