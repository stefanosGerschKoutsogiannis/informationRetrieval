2017,Fast  Sample-Efficient Algorithms for Structured Phase Retrieval,We consider the problem of recovering a signal x in R^n  from magnitude-only measurements  y_i = |a_i^T x|  for i={1 2...m}.  Also known as the phase retrieval problem  it is a fundamental challenge in nano-  bio- and astronomical imaging systems  astronomical imaging  and speech processing. The problem is ill-posed  and therefore additional assumptions on the signal and/or the measurements are necessary.  In this paper  we first study the case where the underlying signal x is s-sparse. We develop a novel recovery algorithm that we call Compressive Phase Retrieval with Alternating Minimization  or CoPRAM. Our algorithm is simple and can be obtained via a natural combination of the classical alternating minimization approach for phase retrieval  with the CoSaMP algorithm for sparse recovery. Despite its simplicity  we prove that our algorithm achieves a sample complexity of O(s^2 log n) with Gaussian samples  which matches the best known existing results. It also demonstrates linear convergence in theory and practice and requires no extra tuning parameters other than the signal sparsity level s.  We then consider the case where the underlying signal x arises from to structured sparsity models. We specifically examine the case of block-sparse signals with uniform block size of b and block sparsity k=s/b. For this problem  we design a recovery algorithm that we call Block CoPRAM that further reduces the sample complexity to O(ks log n). For sufficiently large block lengths of b=Theta(s)  this bound equates to O(s log n). To our knowledge  this constitutes the first end-to-end linearly convergent family of algorithms for phase retrieval where the Gaussian sample complexity has a sub-quadratic dependence on the sparsity level of the signal.,Fast  Sample-Efﬁcient Algorithms for

Structured Phase Retrieval

Electrical and Computer Engineering

Electrical and Computer Engineering

Gauri jagatap

Iowa State University

Chinmay Hegde

Iowa State University

Abstract

(cid:3)

of O(cid:2)

We consider the problem of recovering a signal x∗ ∈ R
n  from magnitude-only
measurements  yi = |(cid:3)ai  x∗(cid:4)| for i = {1  2  . . .   m}. Also known as the phase
retrieval problem  it is a fundamental challenge in nano-  bio- and astronomical
imaging systems  and speech processing. The problem is ill-posed  and therefore
additional assumptions on the signal and/or the measurements are necessary.
In this paper  we ﬁrst study the case where the underlying signal x∗ is s-sparse.
We develop a novel recovery algorithm that we call Compressive Phase Retrieval
with Alternating Minimization  or CoPRAM. Our algorithm is simple and can
be obtained via a natural combination of the classical alternating minimization
approach for phase retrieval  with the CoSaMP algorithm for sparse recovery.
Despite its simplicity  we prove that our algorithm achieves a sample complexity
with Gaussian samples  which matches the best known existing
results. It also demonstrates linear convergence in theory and practice and requires
no extra tuning parameters other than the signal sparsity level s.
We then consider the case where the underlying signal x∗ arises from structured
sparsity models. We speciﬁcally examine the case of block-sparse signals with
uniform block size of b and block sparsity k = s/b. For this problem  we design
a recovery algorithm that we call Block CoPRAM that further reduces the sample
complexity to O (ks log n). For sufﬁciently large block lengths of b = Θ(s)  this
bound equates to O (s log n). To our knowledge  this constitutes the ﬁrst end-to-
end linearly convergent family of algorithms for phase retrieval where the Gaussian
sample complexity has a sub-quadratic dependence on the sparsity level of the
signal.

s2 log n

1

Introduction

1.1 Motivation
In this paper  we consider the problem of recovering a signal x∗ ∈ R
magnitude-only linear measurements. That is  for sampling vector ai ∈ R

n from (possibly noisy)
n  if

yi = |(cid:3)ai  x∗(cid:4)|  

for i = 1  . . .   m 

(1)
(cid:3).
then the task is to recover x∗ using the measurements y and the sampling matrix A = [a1 . . . am]
Problems of this kind arise in numerous scenarios in machine learning  imaging  and statistics.
For example  the classical problem of phase retrieval is encountered in imaging systems such as
diffraction imaging  X-ray crystallography  ptychography  and astronomy [1  2  3  4  5]. For such
imaging systems  the optical sensors used for light acquisition can only record the intensity of the
light waves but not their phase. In terms of our setup  the vector x∗ corresponds to an image (with
a resolution of n pixels) and the measurements correspond to the magnitudes of its 2D Fourier
coefﬁcients. The goal is to stably recover the image x∗ using as few observations m as possible.
31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

recovery  and streaming algorithms [15  16  17]  and it has been established that only O(cid:2)

Despite the prevalence of several heuristic approaches [6  7  8  9]  it is generally accepted that (1) is a
challenging nonlinear  ill-posed inverse problem in theory and practice. For generic ai and x∗  one
can show that (1) is NP-hard by reduction from well-known combinatorial problems [10]. Therefore 
additional assumptions on the signal x∗ and/or the measurement vectors ai are necessary.
A recent line of breakthrough results [11  12] have provided efﬁcient algorithms for the case where the
measurement vectors arise from certain multi-variate probability distributions. The seminal paper by
Netrapalli et al. [13] provides the ﬁrst rigorous justiﬁcation of classical heuristics for phase retrieval
based on alternating minimization. However  all these newer results require an “overcomplete" set
of observations  i.e.  the number of observations m exceeds the problem dimension n (m = O (n)
being the tightest evaluation of this bound [14]). This requirement can pose severe limitations on
computation and storage  particularly when m and n are very large.
One way to mitigate the dimensionality issue is to use the fact that in practical applications  x∗ often
obeys certain low-dimensional structural assumptions. For example  in imaging applications x∗ is
(cid:3)
s-sparse in some known basis  such as identity or wavelet. For transparency  we assume the canonical
basis for sparsity throughout this paper. Similar structural assumptions form the core of sparse
s log n
samples are necessary for stable recovery of x∗  which is information-theoretically optimal [18].
s
Several approaches for solving the sparsity-constrained version of (1) have been proposed  including
alternating minimization [13]  methods based on convex relaxation [19  20  21]  and iterative thresh-
olding [22  23]. Curiously  all of the above techniques incur a sample complexity of Ω(s2 log n) for
1.
Moreover  most of these algorithms have quadratic (or worse) running time [19  22]  stringent as-
sumptions on the nonzero signal coefﬁcients [13  23]  and require several tuning parameters [22  23].
Finally  for speciﬁc applications  more reﬁned structural assumptions on x∗ are applicable. For
example  point sources in astronomical images often produce clusters of nonzero pixels in a given
image  while wavelet coefﬁcients of natural images often can be organized as connected sub-trees.
Algorithms that leverage such structured sparsity assumptions have been shown to achieve con-
siderably improved sample-complexity in statistical learning and sparse recovery problems using
block-sparsity [30  31  32  33]  tree sparsity [34  30  35  36]  clusters [37  31  38]  and graph mod-
els [39  38  40]. However  these models have not been understood in the context of phase retrieval.

stable recovery  which is quadratically worse than the information-theoretic limit [18] of O(cid:2)

s log n
s

(cid:3)

1.2 Our contributions

The contributions in this paper are two-fold. First  we provide a new  ﬂexible algorithm for sparse
phase retrieval that matches state of the art methods both from a statistical as well as computational
viewpoint. Next  we show that it is possible to extend this algorithm to the case where the signal
is block-sparse  thereby further lowering the sample complexity of stable recovery. Our work can
be viewed as a ﬁrst step towards a general framework for phase retrieval of structured signals from
Gaussian samples.
Sparse phase retrieval. We ﬁrst study the case where the underlying signal x∗ is s-sparse. We
develop a novel recovery algorithm that we call Compressive Phase Retrieval with Alternating
Minimization  or CoPRAM2. Our algorithm is simple and can be obtained via a natural combination
of the classical alternating minimization approach for phase retrieval with the CoSaMP [41] algorithm
for sparse recovery (CoSAMP also naturally extends to several sparsity models [30]). We prove that
with Gaussian measurement vectors ai
in order to achieve linear convergence  matching the best among all existing results. An appealing
feature of our algorithm is that it requires no extra a priori information other than the signal sparsity
level s  and no assumptions on the nonzero signal coefﬁcients. To our knowledge  this is the ﬁrst
algorithm for sparse phase retrieval that simultaneously achieves all of the above properties. We use
CoPRAM as the basis to formulate a block-sparse extension (Block CoPRAM).
Block-sparse phase retrieval. We consider the case where the underlying signal x∗ arises from
structured sparsity models  speciﬁcally block-sparse signals with uniform block size b (i.e.  s non-
zeros equally grouped into k = s/b blocks). For this problem  we design a recovery algorithm that we

our algorithm achieves a sample complexity of O(cid:2)

s2 log n

(cid:3)

1Exceptions to this rule are [24  25  26  27  28  29] where very carefully crafted measurements ai are used.
2We use the terms sparse phase retrieval and compressive phase retrieval interchangeably.

2

Table 1: Comparison of (Gaussian sample) sparse phase retrieval algorithms. Here  n  s  k = s/b
denote signal length  sparsity  and block-sparsity. O (·) hides polylogarithmic dependence on 1
 .
Algorithm
AltMinSparse
(cid:2)1-PhaseLift

Sample complexity
O

Running time Assumptions

(cid:6)x∗(cid:6)2

(cid:5)

(cid:3)

s

(cid:2)
O(cid:2)
Thresholded WF O(cid:2)
O(cid:2)
O(cid:2)

(cid:3)
s2 log n + s2 log3 s
(cid:3)
s2 log n
(cid:3)
s2 log n
(cid:3)
s2 log n
SPARTA
s2 log n
CoPRAM
Block CoPRAM O (ks log n)

(cid:3) O
(cid:2)
(cid:4)
min ≈ c√
x∗
(cid:2)
O
(cid:2)
none
O
none
(cid:2)
O
min ≈ c√
x∗
O
none
O (ksn log n) none

s2n log n
n3
2
n2 log n
s2n log n
s2n log n

(cid:3)
(cid:3)

(cid:3)

Parameters
none
none
step μ  thresholds α  β
step μ  threshold γ
none
none

(cid:6)x∗(cid:6)2

s

call Block CoPRAM. We analyze this algorithm and show that leveraging block-structure reduces the
sample complexity for stable recovery to O (ks log n). For sufﬁciently large block lengths b = Θ(s) 
this bound equates to O (s log n). To our knowledge  this constitutes the ﬁrst phase retrieval algorithm
where the Gaussian sample complexity has a sub-quadratic dependence on the sparsity s of the signal.
A comparative description of the performance of our algorithms is presented in Table 1.

1.3 Techniques
Sparse phase retrieval. Our proposed algorithm  CoPRAM  is conceptually very simple. It integrates
existing approaches in stable sparse recovery (speciﬁcally  the CoSaMP algorithm [41]) with the
alternating minimization approach for phase retrieval proposed in [13].
A similar integration of sparse recovery with alternating minimization was also introduced in [13];
however  their approach only succeeds when the true support of the underlying signal is accurately
identiﬁed during initialization  which can be unrealistic. Instead  CoPRAM permits the support of the
estimate to evolve across iterations  and therefore can iteratively “correct" for any errors made during
the initialization. Moreover  their analysis requires using fresh samples for every new update of the
estimate  while ours succeeds in the (more practical) setting of using all the available samples.
Our ﬁrst challenge is to identify a good initial guess of the signal. As is the case with most non-
convex techniques  CoPRAM requires an initial estimate x0 that is close to the true signal x∗. The
basic idea is to identify “important" co-ordinates by constructing suitable biased estimators of each
signal coefﬁcient  followed by a speciﬁc eigendecomposition. The initialization in CoPRAM is far
simpler than the approaches in [22  23]; requiring no pre-processing of the measurements and or
tuning parameters other than the sparsity level s. A drawback of the theoretical results of [23] is
that they impose a requirement on signal coefﬁcients: minj∈S |x∗
s. However  this
assumption disobeys the power-law decay observed in real world signals. Our approach also differs
from [22]  where they estimate an initial support based on a parameter-dependent threshold value.
O(cid:2)
Our analysis removes these requirements; we show that a coarse estimate of the support  coupled
with the spectral technique in [22  23] gives us a suitable initialization. A sample complexity of
is incurred for achieving this estimate  matching the best available previous methods.
Our next challenge is to show that given a good initial guess  alternatingly estimating the phases and
non-zero coefﬁcients (using CoSaMP) gives a rapid convergence to the desired solution. To this end 
we use the analysis of CoSaMP [41] and leverage a recent result by [42]  to show per step decrease in
the signal estimation error using the generic chaining technique of [43  44]. In particular  we show
that any “phase errors" made in the initialization  can be suitably controlled across different estimates.
Block-sparse phase retrieval. We use CoPRAM to establish its extension Block CoPRAM  which is
a novel phase retrieval strategy for block sparse signals from Gaussian measurements. Again  the
algorithm is based on a suitable initialization followed by an alternating minimization procedure 
mirroring the steps in CoPRAM. To our knowledge  this is the ﬁrst result for phase retrieval under
more reﬁned structured sparsity assumptions on the signal.
As above  the ﬁrst stage consists of identifying a good initial guess of the solution. We proceed as in
CoPRAM  isolating blocks of nonzero coordinates  by constructing a biased estimator for the “mass"
of each block. We prove that a good initialization can be achieved using this procedure using only
O (ks log n) measurements. When the block-size is large enough (b = Θ(s))  the sample complexity
of the initialization is sub-quadratic in the sparsity level s and only a logarithmic factor above the

j| = C (cid:6)x∗(cid:6)2 /

s2 log n

√

(cid:3)

3

information-theoretic limit O (s) [30]. In the second stage  we demonstrate a rapid descent to the
desired solution. To this end  we replace the CoSaMP sub-routine in CoPRAM with the model-based
CoSaMP algorithm of [30]  specialized to block-sparse recovery. The analysis proceeds analogously
as above. To our knowledge  this constitutes the ﬁrst end-to-end algorithm for phase retrieval (from
Gaussian samples) that demonstrates a sub-quadratic dependence on the sparsity level of the signal.

1.4 Prior work

The phase retrieval problem has received signiﬁcant attention in the past few years. Convex methodolo-
gies to solve the problem in the lifted framework include PhaseLift and its variations [11  45  46  47].
Most of these approaches suffer severely in terms of computational complexity. PhaseMax  produces
a convex relaxation of the phase retrieval problem similar to basis pursuit [48]; however it is not em-
perically competitive. Non-convex algorithms typically rely on ﬁnding a good initial point  followed
by minimizing a quadratic (Wirtinger Flow [12  14  49]) or moduli ( [50  51]) measurement loss
function. Arbitrary initializations have been studied in a polynomial-time trust-region setting in [52].
Some of the convex approaches in sparse phase retrieval include [19  53]  which uses a combination
of trace-norm and (cid:2)-norm relaxation.Constrained sensing vectors have been used [25] at optimal
. Fourier measurements have been studied extensively in the convex
[54] and non-convex [55] settings. More non-convex approaches for sparse phase retrieval include

sample complexity O(cid:2)
[13  23  22] which achieve Gaussian sample complexities of O(cid:2)

s2 log n

s log n
s

(cid:3)

(cid:3)

.

Structured sparsity models such as groups  blocks  clusters  and trees can be used to model real-world
signals.Applications of such models have been developed for sparse recovery [30  33  39  38  40  56 
34  35  36] as well as in high-dimensional optimization and statistical learning [32  31]. However  to
the best of our knowledge  there have been no rigorous results that explore the impact of structured
sparsity models for the phase retrieval problem.

2 Paper organization and notation

The remainder of the paper is organized as follows. In Sections 3 and 4  we introduce the CoPRAM
and Block CoPRAM algorithms respectively  and provide a theoretical analysis of their statistical
performance. In Section 5 we present numerical experiments for our algorithms.
Standard notation for matrices (capital  bold: A  P  etc.)  vectors (small  bold: x  y  etc.) and scalars
( α  c etc.) hold. Matrix and vector transposes are represented using (cid:8) (eg. x(cid:3) and A(cid:3)) respectively.
The diagonal matrix form of a column vector y ∈ R
m×m. Operator
card(S) represents cardinality of S. Elements of a are distributed according to the zero-mean
standard normal distribution N (0  1). The phase is denoted using sign (y) ≡ y/|y| for y ∈ R
m  and
dist (x1  x2) ≡ min((cid:6)x1 − x2(cid:6)2 (cid:6)x1 + x2(cid:6)2) for every x1  x2 ∈ R
n is used to denote “distance" 
upto a global phase factor (both x = x∗ −x∗ satisfy y = |Ax|). The projection of vector x ∈ R
n
onto a set of coordinates S is represented as xS ∈ R
n  xS j = xj for j ∈ S  and 0 elsewhere.
Projection of matrix M ∈ R
n×n  MS ij = Mij for i  j ∈ S  and 0 elsewhere.
For faster algorithmic implementations  MS can be assumed to be a truncated matrix MS ∈ R
s×s 
discarding all row and column elements corresponding to Sc. The element-wise inner product of
two vectors y1 and y2 ∈ R
m is represented as y1 ◦ y2. Unspeciﬁed large and small constants are
represented by C and δ respectively. The abbreviation w.h.p. denotes “with high probability".

m is represented as diag(y) ∈ R

n×n onto S is MS ∈ R

3 Compressive phase retrieval

In this section  we propose a new algorithm for solving the sparse phase retrieval problem and
analyze its performance. Later  we will show how to extend this algorithm to the case of more reﬁned
structural assumptions about the underlying sparse signal.
We ﬁrst provide a brief outline of our proposed algorithm. It is clear that the sparse recovery version
of (1) is highly non-convex  and possibly has multiple local minima[22]. Therefore  as is typical
in modern non-convex methods [13  23  57] we use an spectral technique to obtain a good initial
estimate. Our technique is a modiﬁcation of the initialization stages in [22  23]  but requires no tuning
parameters or assumptions on signal coefﬁcients  except for the sparsity s. Once an appropriate initial

4

(cid:6)m

Algorithm 1 CoPRAM: Initialization.
input A  y  s.

(cid:6)m
Compute signal power: φ2 = 1
i=1 y2
i .
m
Compute signal marginals: Mjj = 1
i=1 y2
m
Set ˆS ← j’s corresponding to top-s Mjj’s.
Set v1 ← top singular vector of M ˆS = 1
i=1 y2
Compute x0 ← φv  where v ← v1 for ˆS and 0 ∈ R

(cid:6)m

i a2
ij

m

∀j.

output x0.

∈ R

s×s.

(cid:3)
ˆS

i ai ˆSai
n−s for ˆSc.

Algorithm 2 CoPRAM: Descent.
input A  y  x0  s  t0.

Initialize x0 according to Algorithm 1.
for t = 0 ···   t0 − 1 do
Pt+1 ← diag (sign (Axt)) 
xt+1 ← COSAMP( 1√
A  1√

m

m

Pt+1y s xt).

end for

output z ← xt0.

estimate is chosen  we then show that a simple alternating-minimization algorithm  based on the
algorithm in [13] will converge rapidly to the underlying true signal. We call our overall algorithm
Compressive Phase Retrieval with Alternating Minimization (CoPRAM) which is divided into two
stages: Initialization (Algorithm 1) and Descent (Algorithm 2).

3.1

Initialization

The high level idea of the ﬁrst stage of CoPRAM is as follows; we use measurements yi to construct
a biased estimator  marginal Mjj corresponding to the jth signal coefﬁcient and given by:

i a2
y2
ij 

for

j ∈ {1  . . . n}.

(2)

m(cid:7)

i=1

Mjj =

1
m

The marginals themselves do not directly produce signal coefﬁcients  but the “weight" of each
marginal identiﬁes the true signal support. Then  a spectral technique based on [13  23  22] constructs
an initial estimate x0. To accurately estimate support  earlier works [13  23] assume that the
magnitudes of the nonzero signal coefﬁcients are all sufﬁciently large  i.e.  Ω ((cid:6)x∗(cid:6)2 /
s)  which
can be unrealistic  violating the power-decay law. Our analysis resolves this issue by relaxing the
requirement of accurately identifying the support  without any tuning parameters  unlike [22]. We
claim that a coarse estimate of the support is good enough  since the errors would correspond to small
coefﬁcients. Such “noise" in the signal estimate can be controlled with a sufﬁcient number of samples.
Instead  we show that a simple pruning step that rejects the smallest n − k coordinates  followed
by the spectral procedure of [23]  gives us the initialization that we need. Concretely  if elements
of A are distributed as per standard normal distribution N (0  1)  a weighted correlation matrix
M = 1
i   can be constructed  having diagonal elements Mjj. Then  the diagonal
m
elements of this expectation matrix E [M] are given by:

(cid:6)m

i aia(cid:3)

i=1 y2

√

E [Mjj] = (cid:6)x∗(cid:6)2 + 2x∗2

(3)
exhibiting a clear separation when analyzed for j ∈ S and j ∈ Sc. We can hence claim  that signal
marginals at locations on the diagonal of M corresponding to j ∈ S are larger  on an average  than
those for j ∈ Sc. Based on this  we evaluate the diagonal elements Mjj and reject n − k coordinates
corresponding to the smallest marginals obtain a crude approximation of signal support ˆS. Using a
spectral technique  we ﬁnd an initial vector in the reduced space  which is close to the true signal  if

j

(cid:3)

m = O(cid:2)

s2 log n

.

Theorem 3.1. The initial estimate x0  which is the output of Algorithm 1  is a small constant distance
δ0 away from the true s-sparse signal x∗
dist

x0  x∗(cid:3) ≤ δ0 (cid:6)x∗(cid:6)2  

  i.e. 

(cid:2)

5

√

where 0 < δ0 < 1  as long as the number of (Gaussian) measurements satisfy  m ≥ Cs2 log mn 
with probability greater than 1 − 8
m .
This theorem is proved via Lemmas C.1 through C.4 (Appendix C)  and the argument proceeds as
follows. We evaluate the marginals of the signal Mjj  in broadly two cases: j ∈ S and j ∈ Sc.
The key idea is to establish one of the following: (1) If the signal coefﬁcients obey minj∈S |x∗
j| =
C (cid:6)x∗(cid:6)2 /
s  then  w.h.p. there exists a clear separation between the marginals Mjj for j ∈ S
and j ∈ Sc. Then Algorithm 1 picks up the correct support (i.e. ˆS = S); (2) if there is no
such restriction  even then the support picked up in Algorithm 1  ˆS  contains a bulk of the correct
support S. The incorrect elements of ˆS induce negligible error in estimating the intial vector. These
(cid:8)
approaches are illustrated in Figures 4 and 5 in Appendix C. The marginals Mjj < Θ  w.h.p. 
for j ∈ Sc and Mjj > Θ  j ∈ S+  where S+ is a big chunk of the picked support S+ ⊆ ˆS 
S+ = {j ∈ S : x∗
(log mn)/m(cid:6)x∗(cid:6)2} are separated by threshold Θ (Lemmas C.1 and
C.2). The identiﬁcation of the support ˆS (which provably contains a signiﬁcant chunk S+ of the true
support S) is used to construct the truncated correlation matrix M ˆS. The top singular vector of this
matrix M ˆS  gives us a good initial estimate x0.
The ﬁnal step of Algorithm 1 requires a scaling of the normalized vector v1 by a factor φ  which
conserves the power in the signal (Lemma F.1 in Appendix F)  whp  where φ2 which is deﬁned as

2 ≥ 15

j

m(cid:7)

i=1

φ2 =

1
m

y2
i .

(4)

3.2 Descent to optimal solution
After obtaining an initial estimate x0  we construct a method to accurately recover x∗. For this  we
adapt the alternating minimization approach from [13]. The observation model (1) can be restated as:

sign ((cid:3)ai  x∗(cid:4)) ◦ yi = (cid:3)ai  x∗(cid:4)

i = {1  2  . . . m}.

for

We introduce the phase vector p ∈ R
m containing (unknown) signs of measurements  i.e.  pi =
sign ((cid:3)ai  x(cid:4))   ∀i and phase matrix P = diag (p). Then our measurement model gets modiﬁed as
P∗y = Ax∗  where P∗ is the true phase matrix. We then minimize the loss function composed of
variables x and P 

(cid:6)x(cid:6)0≤s P∈P

(5)
min
m×m with diagonal entries constrained to be in {−1  1}.
Here P is a set of all diagonal matrices ∈ R
Hence the problem stated above is not convex. Instead  we alternate between estimating P and x
as follows: (1) if we ﬁx the signal estimate x  then the minimizer P is given in closed form as
P = diag (sign (Ax)); we call this the phase estimation step; (2) if we ﬁx the phase matrix P  the
sparse vector x can be obtained by solving the signal estimation step:

(cid:6)Ax − Py(cid:6)2 .

(cid:6)Ax − Py(cid:6)2.

min

x (cid:6)x(cid:6)0≤s

(6)

We employ the CoSaMP [41] algorithm to (approximately) solve the non-convex problem (6). We do
not need to explicitly obtain the minimizer for (6) but only show a sufﬁcient descent criterion  which
we achieve by performing a careful analysis of the CoSaMP algorithm. For analysis reasons  we
require that the entries of the input sensing matrix are distributed according to N (0  1/
m). This
can be achieved by scaling down the inputs to CoSaMP: At  Pt+1y by a factor of
m (see x-update
step of Algorithm 2). Another distinction is that we use a “warm start" CoSaMP routine for each
iteration where the initial guess of the solution to (6) is given by the current signal estimate.
We now analyze our proposed descent scheme. We obtain the following theoretical result:
Theorem 3.2. Given an initialization x0 satisfying Algorithm 1  if we have number of (Gaussian)
measurements m ≥ Cs log n

s   then the iterates of Algorithm 2 satisfy:

√

√

(cid:2)

xt+1  x∗(cid:3) ≤ ρ0dist

(cid:2)

xt  x∗(cid:3)

.

dist

where 0 < ρ0 < 1 is a constant  with probability greater than 1 − e−γm  for positive constant γ.
The proof of this theorem can be found in Appendix E.

(7)

6

4 Block-sparse phase retrieval

The analysis of the proofs mentioned so far  as well as experimental results suggest that we can reduce
sample complexity for successful sparse phase retrieval by exploiting further structural information
about the signal. Block-sparse signals x∗  can be said to be following a sparsity model Ms b  where
Ms b describes the set of all block-sparse signals with s non-zeros being grouped into uniform pre-
b . We use the index set jb = {1  2 . . . k} 
determined blocks of size b  such that block-sparsity k = s
to denote block-indices. We introduce the concept of block marginals  a block-analogue to signal
marginals  which can be analyzed to crudely estimate the block support of the signal in consideration.
We use this formulation  along with the alternating minimization approach that uses model-based
CoSaMP [30] to descend to the optimal solution.

4.1

Initialization

Analogous to the concept of marginals deﬁned above  we introduce block marginals Mjbjb  where
Mjj is deﬁned as in (2). For block index jb  we deﬁne:

Mjbjb =

M 2
jj 

(8)

(cid:9)(cid:7)

j∈jb

to develop the initialization stage of our Block CoPRAM algorithm. Similar to the proof approach
of CoPRAM  we evaluate the block marginals  and use the top-k such marginals to obtain a crude
approximation ˆSb of the true block support Sb. This support can be used to construct the truncated
correlation matrix M ˆSb. The top singular vector of this matrix M ˆSb gives a good initial estimate x0
(Algorithm 3  Appendix A) for the Block CoPRAM algorithm (Algorithm 4  Appendix A). Through
the evaluation of block marginals  we proceed to prove that the sample complexity required for a
good initial estimate (and subsequently  successful signal recovery of block sparse signals) is given
by O (ks log n). This essentially reduces the sample complexity of signal recovery by a factor equal
to the block-length b over the sample complexity required for standard sparse phase retrieval.
(cid:2)
Theorem 4.1. The initial vector x0  which is the output of Algorithm 3  is a small constant distance
δb away from the true signal x∗ ∈ Ms b  i.e. 

x0  x∗(cid:3) ≤ δb (cid:6)x∗(cid:6)2  

dist

where 0 < δb < 1  as long as the number of (Gaussian) measurements satisfy m ≥ C s2
probability greater than 1 − 8
m .
The proof can be found in Appendix D  and carries forward intuitively from the proof of the
compressive phase-retrieval framework.

b log mn with

4.2 Descent to optimal solution

For the descent of Block CoPRAM to optimal solution  the phase-estimation step is the same as that
in CoPRAM. For the signal estimation step  we attempt to solve the same minimization as in (6) 
except with the additional constraint that the signal x∗ is block sparse 

min
x∈Ms b

(9)
where Ms b describes the block sparsity model. In order to approximate the solution to (9)  we use
the model-based CoSaMP approach of [30]. This is a straightforward specialization of the CoSaMP
algorithm and has been shown to achieve improved sample complexity over existing approaches for
standard sparse recovery.
Similar to Theorem 3.2 above  we obtain the following result (the proof can be found in Appendix E):

(cid:6)Ax − Py(cid:6)2 

(cid:2)

(cid:3)
(cid:2)

Theorem 4.2. Given an initialization x0 satisfying Algorithm 3  if we have number of (Gaussian)
measurements m ≥ C

  then the iterates of Algorithm 4 satisfy:

s + s

b log n
s
dist

xt+1  x∗(cid:3) ≤ ρbdist

(cid:2)

xt  x∗(cid:3)

.

where 0 < ρb < 1 is a constant  with probability greater than 1 − e−γm  for positive constant γ.
The analysis so far has been made for uniform blocks of size b. However the same algorithm can be
extended to the case of sparse signals with non-uniform blocks or clusters (refer Appendix A).

(10)

7

y
r
e
v
o
c
e
r

f
o

y
t
i
l
i
b
a
b
o
r
P

1

0.5

0

500 1 000 1 500 2 000
Number of samples m
(a) Sparsity s = 20

CoPRAM

Block

CoPRAM

ThWF
SPARTA

y
r
e
v
o
c
e
r

f
o

y
t
i
l
i
b
a
b
o
r
P

1

0.5

0

500 1 000 1 500 2 000
Number of samples m

(b) Sparsity s = 30

y
r
e
v
o
c
e
r

f
o

y
t
i
l
i
b
a
b
o
r
P

1

0.5

0

0

500

1 000

1 500

Number of samples m

(c) Block CoPRAM  s = 20

b = 20  k = 1
b = 10  k = 2
b = 5  k = 4
b = 2  k = 10
b = 1  k = 20

Figure 1: Phase transitions for signal of length n = 3  000  sparsity s and block length b (a) s = 20 
b = 5  (b) s = 30  b = 5  and (c) s = 20  b = 20  10  5  2  1 (Block CoPRAM only).

5 Experiments

We explore the performance of the CoPRAM and Block CoPRAM on synthetic data. All numerical
experiments were conducted using MATLAB 2016a on a computer with an Intel Xeon CPU at
3.3GHz and 8GB RAM. The nonzero elements of the unit norm vector x∗ ∈ R
3000 are generated
from N (0  1). We repeated each of the experiments (ﬁxed n  s  b  m) in Figure 1 (a) and (b)  for
50 and Figure 1 (c) for 200 independent Monte Carlo trials. For our simulations  we compared our
algorithms CoPRAM and Block CoPRAM with Thresholded Wirtinger ﬂow (Thresholded WF or
ThWF) [22] and SPARTA [23]. The parameters for these algorithms were carefully chosen as per the
description in their respective papers.
For the ﬁrst experiment  we generated phase transition plots by evaluating the probability of empirical
successful recovery  i.e. number of trials out of 50. The recovery probability for the four algorithms is
displayed in Figure 1. It can be noted that increasing the sparsity of signal shifts the phase transitions
to the right. However  the phase transition for Block CoPRAM has a less apparent shift (suggesting
that sample complexity of m has sub-quadratic dependence on s). We see that Block CoPRAM
exhibits lowest sample complexity for the phase transitions in both cases (a) and (b) of Figure 1.
For the second experiment  we study the variation of phase transition with block length  for Block
CoPRAM (Figure 1(c)). For this experiment we ﬁxed a signal of length n = 3  000  sparsities
s = 20  k = 1 for a block length of b = 20. We observe that the phase transitions improve with
10 = 2 (for large b  b → s)  we observe a saturation
increase in block length. At block sparsity s
effect and the regime of the experiment is very close to the information theoretic limit.
Several additional phase transition diagrams can be found in Figure 2 in Appendix B. The running
time of our algorithms compare favorably with Thresholded WF and SPARTA (see Table 2 in
Appendix B). We also show that Block CoPRAM is more robust to noisy Gaussian measurements  in
comparison to CoPRAM and SPARTA (see Figure 3 in Appendix B).

b = 20

8

References
[1] Y. Shechtman  Y. Eldar  O. Cohen  H. Chapman  J. Miao  and M. Segev. Phase retrieval with application to

optical imaging: a contemporary overview. IEEE Sig. Proc. Mag.  32(3):87–109  2015.

[2] R. Millane. Phase retrieval in crystallography and optics. JOSA A  7(3):394–411  1990.

[3] A. Maiden and J. Rodenburg. An improved ptychographical phase retrieval algorithm for diffractive

imaging. Ultramicroscopy  109(10):1256–1262  2009.

[4] R. Harrison. Phase problem in crystallography. JOSA a  10(5):1046–1055  1993.

[5] J. Miao  T. Ishikawa  Q Shen  and T. Earnest. Extending x-ray crystallography to allow the imaging of
noncrystalline materials  cells  and single protein complexes. Annu. Rev. Phys. Chem.  59:387–410  2008.

[6] R. Gerchberg and W. Saxton. A practical algorithm for the determination of phase from image and

diffraction plane pictures. Optik  35(237)  1972.

[7] J. Fienup. Phase retrieval algorithms: a comparison. Applied optics  21(15):2758–2769  1982.

[8] S. Marchesini. Phase retrieval and saddle-point optimization. JOSA A  24(10):3289–3296  2007.

[9] K. Nugent  A. Peele  H. Chapman  and A. Mancuso. Unique phase recovery for nonperiodic objects.

Physical review letters  91(20):203902  2003.

[10] M. Fickus  D. Mixon  A. Nelson  and Y. Wang. Phase retrieval from very few measurements. Linear Alg.

Appl.  449:475–499  2014.

[11] E. Candes  T. Strohmer  and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude

measurements via convex programming. Comm. Pure Appl. Math.  66(8):1241–1274  2013.

[12] E. Candes  X. Li  and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and algorithms. IEEE

Trans. Inform. Theory  61(4):1985–2007  2015.

[13] P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization. In Adv. Neural Inf.

Proc. Sys. (NIPS)  pages 2796–2804  2013.

[14] Y. Chen and E. Candes. Solving random quadratic systems of equations is nearly as easy as solving linear

systems. In Adv. Neural Inf. Proc. Sys. (NIPS)  pages 739–747  2015.

[15] E. Candes  J. Romberg  and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly

incomplete frequency information. IEEE Trans. Inform. Theory  52(2):489–509  2006.

[16] D. Needell  J. Tropp  and R. Vershynin. Greedy signal recovery review. In Proc. Asilomar Conf. Sig. Sys.

Comput.  pages 1048–1050. IEEE  2008.

[17] E. Candes  J. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate measurements.

Comm. Pure Appl. Math.  59(8):1207–1223  2006.

[18] K. Do Ba  P. Indyk  E. Price  and D. Woodruff. Lower bounds for sparse recovery. In Proc. ACM Symp.

Discrete Alg. (SODA)  pages 1190–1197  2010.

[19] H. Ohlsson  A. Yang  R. Dong  and S. Sastry. Cprl–an extension of compressive sensing to the phase

retrieval problem. In Adv. Neural Inf. Proc. Sys. (NIPS)  pages 1367–1375  2012.

[20] Y. Chen  Y. Chi  and A. Goldsmith. Exact and stable covariance estimation from quadratic sampling via

convex programming. IEEE Trans. Inform. Theory  61(7):4034–4059  2015.

[21] K. Jaganathan  S. Oymak  and B. Hassibi. Sparse phase retrieval: Convex algorithms and limitations. In

Proc. IEEE Int. Symp. Inform. Theory (ISIT)  pages 1022–1026. IEEE  2013.

[22] T. Cai  X. Li  and Z. Ma. Optimal rates of convergence for noisy sparse phase retrieval via thresholded

wirtinger ﬂow. Ann. Stat.  44(5):2221–2251  2016.

[23] G. Wang  L. Zhang  G. Giannakis  M. Akcakaya  and J. Chen. Sparse phase retrieval via truncated

amplitude ﬂow. arXiv preprint arXiv:1611.07641  2016.

[24] M. Iwen  A. Viswanathan  and Y. Wang. Robust sparse phase retrieval made easy. Appl. Comput. Harmon.

Anal.  42(1):135–142  2017.

9

[25] S. Bahmani and J. Romberg. Efﬁcient compressive phase retrieval with constrained sensing vectors. In

Adv. Neural Inf. Proc. Sys. (NIPS)  pages 523–531  2015.

[26] H. Qiao and P. Pal. Sparse phase retrieval using partial nested Fourier samplers. In Proc. IEEE Global

Conf. Signal and Image Processing (GlobalSIP)  pages 522–526. IEEE  2015.

[27] S. Cai  M. Bakshi  S. Jaggi  and M. Chen. Super: Sparse signals with unknown phases efﬁciently recovered.

In Proc. IEEE Int. Symp. Inform. Theory (ISIT)  pages 2007–2011. IEEE  2014.

[28] D. Yin  R. Pedarsani  X. Li  and K. Ramchandran. Compressed sensing using sparse-graph codes for the
continuous-alphabet setting. In Proc. Allerton Conf. on Comm.  Contr.  and Comp.  pages 758–765. IEEE 
2016.

[29] R. Pedarsani  D. Yin  K. Lee  and K. Ramchandran. Phasecode: Fast and efﬁcient compressive phase

retrieval based on sparse-graph codes. IEEE Trans. Inform. Theory  2017.

[30] R. Baraniuk  V. Cevher  M. Duarte  and C. Hegde. Model-based compressive sensing. IEEE Trans. Inform.

Theory  56(4):1982–2001  Apr. 2010.

[31] J. Huang  T. Zhang  and D. Metaxas. Learning with structured sparsity. J. Machine Learning Research 

12(Nov):3371–3412  2011.

[32] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Royal Stat.

Soc. Stat. Meth.  68(1):49–67  2006.

[33] Y. Eldar  P. Kuppinger  and H. Bolcskei. Block-sparse signals: Uncertainty relations and efﬁcient recovery.

IEEE Trans. Sig. Proc.  58(6):3042–3054  2010.

[34] M. Duarte  C. Hegde  V. Cevher  and R. Baraniuk. Recovery of compressible signals from unions of

subspaces. In Proc. IEEE Conf. Inform. Science and Systems (CISS)  March 2009.

[35] C. Hegde  P. Indyk  and L. Schmidt. A fast approximation algorithm for tree-sparse recovery. In Proc.

IEEE Int. Symp. Inform. Theory (ISIT)  June 2014.

[36] C. Hegde  P. Indyk  and L. Schmidt. Nearly linear-time model-based compressive sensing. In Proc. Intl.

Colloquium on Automata  Languages  and Programming (ICALP)  July 2014.

[37] V. Cevher  P. Indyk  C. Hegde  and R. Baraniuk. Recovery of clustered sparse signals from compressive

measurements. In Proc. Sampling Theory and Appl. (SampTA)  May 2009.

[38] C. Hegde  P. Indyk  and L. Schmidt. A nearly linear-time framework for graph-structured sparsity. In Proc.

Int. Conf. Machine Learning (ICML)  July 2015.

[39] V. Cevher  M. Duarte  C. Hegde  and R. Baraniuk. Sparse signal recovery using Markov Random Fields.

In Adv. Neural Inf. Proc. Sys. (NIPS)  Dec. 2008.

[40] C. Hegde  P. Indyk  and L. Schmidt. Approximation-tolerant model-based compressive sensing. In Proc.

ACM Symp. Discrete Alg. (SODA)  Jan. 2014.

[41] D. Needell and J. Tropp. Cosamp: Iterative signal recovery from incomplete and inaccurate samples. Appl.

Comput. Harmon. Anal.  26(3):301–321  2009.

[42] M. Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking sample complexity

barriers via nonconvex optimization. arXiv preprint arXiv:1702.06175  2017.

[43] M. Talagrand. The generic chaining: upper and lower bounds of stochastic processes. Springer Science &

Business Media  2006.

[44] S. Dirksen. Tail bounds via generic chaining. Electronic J. Probability  20  2015.

[45] D. Gross  F. Krahmer  and R. Kueng. Improved recovery guarantees for phase retrieval from coded

diffraction patterns. Appl. Comput. Harmon. Anal.  42(1):37–64  2017.

[46] E. Candes  X. Li  and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns. Appl. Comput.

Harmon. Anal.  39(2):277–299  2015.

[47] I. Waldspurger  A.d’Aspremont  and S. Mallat. Phase recovery  maxcut and complex semideﬁnite program-

ming. Mathematical Programming  149(1-2):47–81  2015.

10

[48] T. Goldstein and C. Studer. Phasemax: Convex phase retrieval via basis pursuit. arXiv preprint

arXiv:1610.07531  2016.

[49] H. Zhang and Y. Liang. Reshaped wirtinger ﬂow for solving quadratic system of equations. In Adv. Neural

Inf. Proc. Sys. (NIPS)  pages 2622–2630  2016.

[50] G. Wang and G. Giannakis. Solving random systems of quadratic equations via truncated generalized

gradient ﬂow. In Adv. Neural Inf. Proc. Sys. (NIPS)  pages 568–576  2016.

[51] K. Wei. Solving systems of phaseless equations via kaczmarz methods: A proof of concept study. Inverse

Problems  31(12):125008  2015.

[52] J. Sun  Q. Qu  and J. Wright. A geometric analysis of phase retrieval. In Proc. IEEE Int. Symp. Inform.

Theory (ISIT)  pages 2379–2383. IEEE  2016.

[53] X. Li and V. Voroninski. Sparse signal recovery from quadratic measurements via convex programming.

SIAM J. Math. Anal.  45(5):3019–3033  2013.

[54] K. Jaganathan  S. Oymak  and B. Hassibi. Recovery of sparse 1-d signals from the magnitudes of their

fourier transform. In Proc. IEEE Int. Symp. Inform. Theory (ISIT)  pages 1473–1477. IEEE  2012.

[55] Y. Shechtman  A. Beck  and Y. C. Eldar. Gespar: Efﬁcient phase retrieval of sparse signals. IEEE Trans.

Sig. Proc.  62(4):928–938  2014.

[56] C. Hegde  P. Indyk  and L. Schmidt. Fast algorithms for structured sparsity. Bulletin of the EATCS 

1(117):197–228  Oct. 2015.

[57] R. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries. IEEE Trans. Inform. Theory 

56(6):2980–2998  2010.

[58] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann. Stat. 

pages 1302–1338  2000.

[59] C. Davis and W. Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM J. Num. Anal.  7(1):1–46 

1970.

[60] V. Bentkus. An inequality for tail probabilities of martingales with differences bounded from one side. J.

Theoretical Prob.  16(1):161–173  2003.

11

,Gauri Jagatap
Chinmay Hegde
Tomoya Murata
Taiji Suzuki