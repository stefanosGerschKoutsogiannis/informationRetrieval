2019,Updates of Equilibrium Prop Match Gradients of Backprop Through Time in an RNN with Static Input,Equilibrium Propagation (EP) is a biologically inspired learning algorithm for
convergent recurrent neural networks  i.e. RNNs that are fed by a static input x and
settle to a steady state. Training convergent RNNs consists in adjusting the weights
until the steady state of output neurons coincides with a target y. Convergent RNNs
can also be trained with the more conventional Backpropagation Through Time
(BPTT) algorithm. In its original formulation EP was described in the case of
real-time neuronal dynamics  which is computationally costly. In this work  we
introduce a discrete-time version of EP with simplified equations and with reduced
simulation time  bringing EP closer to practical machine learning tasks. We first
prove theoretically  as well as numerically that the neural and weight updates of EP 
computed by forward-time dynamics  are step-by-step equal to the ones obtained by
BPTT  with gradients computed backward in time. The equality is strict when the
transition function of the dynamics derives from a primitive function and the steady
state is maintained long enough. We then show for more standard discrete-time
neural network dynamics that the same property is approximately respected and
we subsequently demonstrate training with EP with equivalent performance to
BPTT. In particular  we define the first convolutional architecture trained with EP
achieving ∼ 1% test error on MNIST  which is the lowest error reported with EP.
These results can guide the development of deep neural networks trained with EP.,Updates of Equilibrium Prop Match Gradients of

Backprop Through Time in an RNN with Static Input

Maxence Ernoult1 2  Julie Grollier2  Damien Querlioz1  Yoshua Bengio3 4  Benjamin Scellier3

1Centre de Nanosciences et de Nanotechnologies  Université Paris Sud  Université Paris-Saclay

2Unité Mixte de Physique  CNRS  Thales  Université Paris-Sud  Université Paris-Saclay

3Mila  Université de Montréal

4Canadian Institute for Advanced Research

Abstract

Equilibrium Propagation (EP) is a biologically inspired learning algorithm for
convergent recurrent neural networks  i.e. RNNs that are fed by a static input x and
settle to a steady state. Training convergent RNNs consists in adjusting the weights
until the steady state of output neurons coincides with a target y. Convergent RNNs
can also be trained with the more conventional Backpropagation Through Time
(BPTT) algorithm. In its original formulation EP was described in the case of
real-time neuronal dynamics  which is computationally costly. In this work  we
introduce a discrete-time version of EP with simpliﬁed equations and with reduced
simulation time  bringing EP closer to practical machine learning tasks. We ﬁrst
prove theoretically  as well as numerically that the neural and weight updates of EP 
computed by forward-time dynamics  are step-by-step equal to the ones obtained by
BPTT  with gradients computed backward in time. The equality is strict when the
transition function of the dynamics derives from a primitive function and the steady
state is maintained long enough. We then show for more standard discrete-time
neural network dynamics that the same property is approximately respected and
we subsequently demonstrate training with EP with equivalent performance to
BPTT. In particular  we deﬁne the ﬁrst convolutional architecture trained with EP
achieving ∼ 1% test error on MNIST  which is the lowest error reported with EP.
These results can guide the development of deep neural networks trained with EP.

1

Introduction

The remarkable development of deep learning over the past years [LeCun et al.  2015] has been
fostered by the use of backpropagation [Rumelhart et al.  1985] which stands as the most powerful
algorithm to train neural networks. In spite of its success  the backpropagation algorithm is not
biologically plausible [Crick  1989]  and its implementation on GPUs is energy-consuming [Editorial 
2018]. Hybrid hardware-software experiments have recently demonstrated how physics and dynamics
can be leveraged to achieve learning with energy efﬁciency [Romera et al.  2018  Ambrogio et al. 
2018]. Hence the motivation to invent novel learning algorithms where both inference and learning
could fully be achieved out of core physics.
Many biologically inspired learning algorithms have been proposed as alternatives to backpropagation
to train neural networks. Contrastive Hebbian learning (CHL) has been successfully used to train
recurrent neural networks (RNNs) with static input that converge to a steady state (or ‘equilibrium’) 
such as Boltzmann machines [Ackley et al.  1985] and real-time Hopﬁeld networks [Movellan 
1991]. CHL proceeds in two phases  each phase converging to a steady state  where the learning rule
accommodates the difference between the two equilibria. Equilibrium Propagation (EP) [Scellier
and Bengio  2017] also belongs to the family of CHL algorithms to train RNNs with static input.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In the second phase of EP  the prediction error is encoded as an elastic force nudging the system
towards a second equilibrium closer to the target. Interestingly  EP also shares similar features with
the backpropagation algorithm  and more speciﬁcally recurrent backpropagation (RBP) [Almeida 
1987  Pineda  1987]. It was proved in Scellier and Bengio [2019] that neural computation in the
second phase of EP is equivalent to gradient computation in RBP.
Originally  EP was introduced in the context of leaky-integrate neurons [Scellier and Bengio  2017 
2019]. Computing their dynamics involves long simulation times  hence limiting EP training
experiments to small neural networks. In this paper  we propose a discrete-time formulation of EP.
This formulation allows demonstrating an equivalence between EP and BPTT in speciﬁc conditions 
simpliﬁes equations and speeds up training  and extends EP to standard neural networks including
convolutional ones. Speciﬁcally  the contributions of the present work are the following:

• We introduce a discrete-time formulation of EP (Section 3.1) of which the original real-time

formulation can be seen as a particular case (Section 4.2).

• We show a step-by-step equality between the updates of EP and the gradients of BPTT when
the dynamics converges to a steady state and the transition function of the RNN derives
from a primitive function (Theorem 1  Figure 1). We say that such an RNN has the property
of ‘gradient-descending updates’ (or GDU property).

• We numerically demonstrate the GDU property on a small network  on fully connected
layered and convolutional architectures. We show that the GDU property continues to hold
approximately for more standard – prototypical – neural networks even if these networks do
not exactly meet the requirements of Theorem 1.

• We validate our approach with training experiments on different network architectures using
discrete-time EP  achieving similar performance than BPTT. We show that the number of
iterations in the two phases of discrete-time EP can be reduced by a factor three to ﬁve
compared to the original real-time EP  without loss of accuracy. This allows us training the
ﬁrst convolutional architecture with EP  reaching ∼ 1% test error on MNIST  which is the
lowest test error reported with EP. Our code is available on-line in Pytorch 1.

2 Background

This section introduces the notations and basic concepts used throughout the paper.

2.1 Convergent RNNs With Static Input

We consider the supervised setting where we want to predict a target y given an input x. The model
is a dynamical system - such as a recurrent neural network (RNN) - parametrized by θ and evolving
according to the dynamics:

(1)
We call F the transition function. The input of the RNN at each timestep is static  equal to x.
Assuming convergence of the dynamics before time step T   we have sT = s∗ where s∗ is such that
(2)

st+1 = F (x  st  θ) .

s∗ = F (x  s∗  θ) .

We call s∗ the steady state (or ﬁxed point  or equilibrium state) of the dynamical system. The number
of timesteps T is a hyperparameter chosen large enough to ensure sT = s∗. The goal of learning is to
optimize the parameter θ to minimize the loss:

L∗ = (cid:96) (s∗  y)  

(3)

where the scalar function (cid:96) is called cost function. Several algorithms have been proposed to
optimize the loss L∗  including Recurrent Backpropagation (RBP) [Almeida  1987  Pineda  1987]
and Equilibrium Propagation (EP) [Scellier and Bengio  2017]. Here  we present Backpropagation
Through Time (BPTT) and Equilibrium Propagation (EP) and some of the inner mechanisms of these
two algorithms  so as to enunciate the main theoretical result of this paper (Theorem 1).

1https://github.com/ernoult/updatesEPgradientsBPTT

2

Figure 1: Illustration of the property of Gradient-Descending Updates (GDU property). Top left.
Forward-time pass (or ‘ﬁrst phase’) of an RNN with static input x and target y. The ﬁnal state sT is
the steady state s∗. Bottom left. Backprop through time (BPTT). Bottom right. Second phase of
equilibrium prop (EP). The starting state in the second phase is the ﬁnal state of the ﬁrst phase  i.e.
the steady state s∗. GDU Property (Theorem 1). Step by step correspondence between the neural
updates ∆EP
(t) of BPTT. Corresponding
computations in EP and BPTT at timestep t = 0 (resp. t = 1  2  3) are colored in green (resp. blue 
red  yellow). Forward-time computation in EP corresponds to backward-time computation in BPTT.

s (t) in the second phase of EP and the gradients ∇BPTT

s

2.2 Backpropagation Through Time (BPTT)

With frameworks implementing automatic differentiation  optimization by gradient descent using
Backpropagation Through Time (BPTT) has become the standard method to train RNNs. In particular
BPTT can be used for a convergent RNN such as the one that we study here. To this end  we consider
the loss after T iterations (i.e. the cost of the ﬁnal state sT )  denoted L = (cid:96) (sT   y)  and we substitute
L as a proxy 2 for the loss at the steady state L∗. The gradients of L can be computed with BPTT.
In order to state our Theorem 1 (Section 3.2)  we recall some of the inner working mechanisms
of BPTT. Eq. (1) can be rewritten in the form st+1 = F (x  st  θt+1 = θ)  where θt denotes the
parameter of the model at time step t  the value θ being shared across all time steps. This way of
as the sensitivity of the loss L with
rewriting Eq. (1) enables us to deﬁne the partial derivative ∂L
respect to θt when θ1  . . . θt−1  θt+1  . . . θT remain ﬁxed (set to the value θ). With these notations 
the gradient ∂L

∂θ reads as the sum:

∂θt

∂L
∂θ

∂L
∂θ1

∂L
∂θ2

+

=

+ ··· +

∂L
∂θT

.

(4)

BPTT computes the ‘full’ gradient ∂L
iteratively
and efﬁciently  backward in time  using the chain rule of differentiation. Subsequently  we denote the
gradients that BPTT computes:

∂θ by computing the partial derivatives ∂L

and ∂L

∂θt

∂st

(t) =

(t) =

∂L
∂sT−t
∂L
∂θT−t

 

(5)

 ∇BPTT

∇BPTT

θ

s

∀t ∈ [0  T − 1] :

2The difference between the loss L and the loss L∗ is explained in Appendix B.1.

3

so that

T−1(cid:88)

t=0

∂L
∂θ

=

∇BPTT

θ

(t).

More details about BPTT are provided in Appendix A.2.

3 Equilibrium Propagation (EP) - Discrete Time Formulation

3.1 Algorithm

(6)

In its original formulation  Equilibrium Propagation (EP) was introduced in the case of real-time
dynamics [Scellier and Bengio  2017  2019]. The ﬁrst theoretical contribution of this paper is to adapt
the theory of EP to discrete-time dynamics.3 EP is an alternative algorithm to compute the gradient
of L∗ in the particular case where the transition function F derives from a scalar function Φ  i.e. with
F of the form F (x  s  θ) = ∂Φ

∂s (x  s  θ). In this setting  the dynamics of Eq. (1) rewrites:
∀t ∈ [0  T − 1] 

(x  st  θ).

st+1 =

(7)

This constitutes the ﬁrst phase of EP. At the end of the ﬁrst phase  we have reached steady state  i.e.
sT = s∗. In the second phase of EP  starting from the steady state s∗  an extra term β ∂(cid:96)
∂s (where β is
a positive scaling factor) is introduced in the dynamics of the neurons and acts as an external force
nudging the system dynamics towards decreasing the cost function (cid:96). Denoting sβ
2   . . . the
sequence of states in the second phase (which depends on the value of β)  the dynamics is deﬁned as

1   sβ

0   sβ

(cid:17) − β

(cid:16)

∂(cid:96)
∂s

(cid:17)

sβ
t   y

.

(8)

sβ
0 = s∗

and

∀t ≥ 0 

sβ
t+1 =

∂Φ
∂s

x  sβ

t   θ

The network eventually settles to a new steady state sβ∗ . It was shown in Scellier and Bengio [2017]
that the gradient of the loss L∗ can be computed based on the two steady states s∗ and sβ∗ . More
speciﬁcally  4 in the limit β → 0 

(cid:0)x  sβ∗   θ(cid:1) − ∂Φ

∂θ

(x  s∗  θ)

→ − ∂L∗

∂θ

.

(9)

In fact  we can prove a stronger result. For ﬁxed β > 0 we deﬁne the neural and weight updates

∀t ≥ 0 :

s (β  t) =

θ (β  t) =

(cid:17)

 

(cid:16)
(cid:18) ∂Φ

(cid:16)
t+1 − sβ
sβ
x  sβ

t

∂θ

1
β
1
β

t+1  θ

(cid:16)

(cid:17) − ∂Φ

∂θ

x  sβ

t   θ

(cid:17)(cid:19)

∂Φ
∂s

(cid:16)

(cid:19)

∂θ

1
β

(cid:18) ∂Φ
 ∆EP
∞(cid:88)

∆EP

t=0

 

(10)

(11)

and note that Eq. (9) rewrites as the following telescoping sum:

θ (β  t) → − ∂L∗

∆EP

∂θ

as

β → 0.

We can now state our main theoretical result (Theorem 1 below).

3.2 Forward-Time Dynamics of EP Compute Backward-Time Gradients of BPTT

BPTT and EP compute the gradient of the loss in very different ways: while the former algorithm
iteratively adds up gradients going backward in time  as in Eq. (6)  the latter algorithm adds up weight
updates going forward in time  as in Eq. (11). In fact  under a condition stated below  the sums are
equal term by term: there is a step-by-step correspondence between the two algorithms.

3We explain in Appendix B.2 the relationship between the discrete-time setting (resp. the primitive function

Φ) of this paper and the real-time setting (resp. the energy function E) of Scellier and Bengio [2017  2019].

4The EP learning rule is a form of contrastive Hebbian learning similar to that of Boltzmann machines

[Ackley et al.  1985] and similar to the one presented in Movellan [1991].

4

Theorem 1 (Gradient-Descending Updates  GDU). Consider the setting with a transition function
of the form F (x  s  θ) = ∂Φ
∂s (x  s  θ). Let s0  s1  . . .   sT be the convergent sequence of states and
denote s∗ = sT the steady state. If we further assume that there exists some step K where 0 < K ≤ T
such that s∗ = sT = sT−1 = . . . sT−K  then  in the limit β → 0  the ﬁrst K updates in the second
phase of EP are equal to the negatives of the ﬁrst K gradients of BPTT  i.e.

(cid:26) ∆EP

∀t = 0  1  . . .   K :

s (β  t) → −∇BPTT
θ (β  t) → −∇BPTT
∆EP

θ

s

(t) 
(t).

(12)

We give here a short outline of the proof of Theorem 1 (A complete proof together with a detailed
sketch of the proof are provided in Appendix A). The convergence requirement enables to derive the
equations satisﬁed by the neural and weight updates (Lemma 3). Then  the existence of a primitive
function ensures that these equations are equal to those satisﬁed by the gradients of BPTT (Lemma
2)  with same initial conditions  yielding the desired equality (Theorem 1).
Note that other algorithms such as RTRL [Williams and Zipser  1989] and UORO [Tallec and Ollivier 
2017] also compute the gradients by forward-time dynamics.

4 Experiments

This section uses Theorem 1 as a tool to design neural networks that are trainable with EP: if
a model satisﬁes the GDU property of Eq. 12  then we expect EP to perform as well as BPTT
on this model. After introducing our protocol (Section 4.1)  we deﬁne the energy-based setting
and prototypical setting where the conditions of Theorem 1 are exactly and approximately met
respectively (Section 4.2). We show the GDU property on a toy model (Fig. 2) and on fully connected
layered architectures in the two settings (Section 4.3). We deﬁne a convolutional architecture in the
prototypical setting (Section 4.4) which also satisﬁes the GDU property. Finally  we validate our
approach by training 5 these models with EP and BPTT (Table 1).

Figure 2: Demonstrating the property of gradient-descending updates in the energy-based setting
on a toy model with dummy data x and a target y elastically nudging the output neurons s0 (right).
Dashed and solid lines represent ∆EP and −∇BPTT processes respectively and perfectly coincide
for 5 randomly selected neurons (left) and synapses (middle). Each randomly selected neuron or
synapse corresponds to one color. Details can be found in Appendix C.2.

4.1 Demonstrating the Property of Gradient-Descending Updates (GDU Property)

Property of Gradient-Descending Updates. We say that a convergent RNN model fed with a
ﬁxed input has the GDU property if during the second phase  the updates it computes by EP (∆EP)
on the one hand and the gradients it computes by BPTT (−∇BPTT) on the other hand are ‘equal’ - or
‘approximately equal’  as measured per the RelMSE (Relative Mean Squared Error) metric.

Relative Mean Squared Error (RelMSE).
In order to quantitatively measure how well the GDU
property is satisﬁed  we introduce a metric which we call Relative Mean Squared Error (RelMSE)

5For training  the effective weight update is performed at the end of the second phase.

5

such that RelMSE(∆EP  -∇BPTT) measures the distance between ∆EP and −∇BPTT processes 
averaged over time  over neurons or synapses (layer-wise) and over a mini-batch of samples - see
Appendix C.1 for the details.

Protocol.
In order to measure numerically if a given model satisﬁes the GDU property  we proceed
as follows. Considering an input x and associated target y  we perform the ﬁrst phase for T steps.
Then we perform on the one hand BPTT for K steps (to compute the gradients ∇BPTT)  on the other
hand EP for K steps (to compute the neural updates ∆EP) and compare the gradients and neural
updates provided by the two algorithms  either qualitatively by looking at the plots of the curves (as
in Figs. 2 and 4)  or quantitatively by computing their RelMSE (as in Fig. 3).

4.2 Energy-Based Setting and Prototypical Setting

Energy-based setting. The system is deﬁned in terms of a primitive function of the form:

Φ(s  W ) = (1 − )

1
2

(cid:107)s(cid:107)2 +  σ(s)(cid:62) · W · σ(s) 

(13)

where  is a discretization parameter  σ is an activation function and W is a symmetric weight matrix.
In this setting  we consider ∆EP(β  t) instead of ∆EP(β  t) and write ∆EP(t) for simplicity  so that:

∆EP

s (t) =

t+1 − sβ
sβ

t

β

  ∆EP

W (t) =

1
β

(cid:18)

(cid:16)

(cid:17)(cid:62) · σ

(cid:16)

(cid:17) − σ

(cid:16)

(cid:17)(cid:62) · σ

(cid:16)

(cid:17)(cid:19)

σ

sβ
t+1

sβ
t+1

sβ
t

sβ
t

.

(14)

With Φ as a primitive function and with the hyperparameter β rescaled by a factor   we recover the
discretized version of the real-time setting of Scellier and Bengio [2017]  i.e. the Euler scheme of
dt = − ∂E
2(cid:107)s(cid:107)2 − σ(s)(cid:62) · W · σ(s) – see Appendix B.2  where the link between
ds
discrete-time dynamics and real-time dynamics is explained. Fig. 2 qualitatively demonstrates
Theorem 1 in this setting on a toy model.

∂s with E = 1

∂s − β ∂(cid:96)

Prototypical setting.
function Φ. Instead  the dynamics is directly deﬁned as:

In this case  the dynamics of the system does not derive from a primitive

st+1 = σ (W · st) .

(15)
Again  W is assumed to be a symmetric matrix. The dynamics of Eq. (15) is a standard and simple
neural network dynamics. Although the model is not deﬁned in terms of a primitive function  note that
2 s(cid:62) · W · s if we ignore the activation function σ. Following
st+1 ≈ ∂Φ
(cid:17)
Eq. (10)  we deﬁne:

∂s (st  W ) with Φ(s  W ) = 1

(cid:16)

(cid:16)

(cid:17)

∆EP

s (t) =

t+1 − sβ
sβ

t

 

1
β

∆EP

W (t) =

1
β

sβ(cid:62)
t+1 · sβ

t+1 − sβ(cid:62)

t

· sβ

t

.

(16)

4.3 Effect of Depth and Approximation

We consider a fully connected layered architecture where layers sn are labelled in a backward
fashion: s0 denotes the output layer  s1 the last hidden layer  and so forth. Two consecutive layers
are reciprocally connected with tied weights with the convention that Wn n+1 connects sn+1 to sn.
We study this architecture in the energy-based and prototypical setting as described per Equations
(13) and (15) respectively  with corresponding weight updates (14) and (16) - see details in Appendix
C.3 and C.4. We study the GDU property layer-wise  e.g. RelMSE(∆EP
) measures the
distance between the ∆EP
We display in Fig. 3 the RelMSE  layer-wise for one  two and three hidden layered architecture (from
left to right)  in the energy-based (upper panels) and prototypical (lower panels) settings  so that each
architecture in a given setting is displayed in one panel - see Appendix C.3 and C.4 for a detailed
description of the hyperparameters and curve samples. In terms of RelMSE  we can see that the
GDU property is best satisﬁed in the energy-based setting with one hidden layer where RelMSE is
around ∼ 10−2 (top left). When adding more hidden layers in the energy-based setting (top middle
and top right)  the RelMSE increases to ∼ 10−1  with a greater RelMSE when going away from the
output layer. The same is observed in the prototypical setting when we add more hidden layers (lower

processes  averaged over all elements of layer sn.

sn and −∇BPTT

sn   -∇BPTT

sn

sn

6

Figure 3: RelMSE analysis in the energy-based (top) and prototypical (bottom) setting. For one given
architecture  each bar is labelled by a layer or synapses connecting two layers  e.g. the orange bar
above s1 represents RelMSE(∆EP
). For each architecture  the recurrent hyperparameters
T   K and β have been tuned to make the ∆EP and −∇BPTT processes match best.

s1  −∇BPTT

s1

panels). Compared to the energy-based setting  although the RelMSEs associated with neurons are
signiﬁcantly higher in the prototypical setting  the RelMSEs associated with synapses are similar
or lower. On average  the weight updates provided by EP match well the gradients of BPTT  in the
energy-based setting as well as in the prototypical setting.

4.4 Convolutional Architecture

Figure 4: Demonstrating the GDU property with the convolutional architecture on MNIST. Dashed
and continuous lines represent ∆EP and −∇BPTT processes respectively  for 5 randomly selected
neurons (top) and synapses (bottom) in each layer. Each randomly selected neuron or synapse
corresponds to one color. Dashed and continuous lines mostly coincide. Some ∆EP processes
collapse to zero as an effect of the non-linearity  see Appendix D for details. Interestingly  the ∆EP
and −∇BPTT

processes are saw-teeth-shaped ; Appendix C.6 accounts for this phenomenon.

s

s

n n+1 and W conv

In our convolutional architecture  hn and sn denote convolutional and fully connected layers respec-
tively. W fc
n n+1 denote the fully connected weights connecting sn+1 to sn and the ﬁlters
connecting hn+1 to hn  respectively. We deﬁne the dynamics as:
n−1n · sn−1

nn+1 · sn+1
W fc

t + W fc(cid:62)

(cid:17)

t

 sn

t+1 = σ
hn
t+1 = σ

(cid:16)
(cid:16)P(cid:0)W conv

where ∗ and P denote convolution and pooling  respectively. Transpose convolution is deﬁned through
the convolution by the ﬂipped kernel ˜W conv and P−1 denotes inverse pooling - see Appendix D for a

(cid:1) + ˜W conv

n−1 n ∗ P−1(cid:0)hn−1

t

(cid:1)(cid:17)

 

n n+1 ∗ hn+1

t

(17)

7

Table 1: Training results on MNIST with EP benchmarked against BPTT  in the energy-based and
prototypical settings. "EB" and "P" respectively denote "energy-based" and "prototypical"  "-#h"
stands for the number of hidden layers and WCT for "Wall-clock time" in hours : minutes. We
indicate over ﬁve trials the mean and standard deviation for the test error  the mean error in parenthesis
for the train error. T (resp. K) is the number of iterations in the ﬁrst (resp. second) phase.

EP (error %)
Test

BPTT (error %)
Test

EB-1h
EB-2h
P-1h
P-2h
P-3h
P-conv

2.06 ± 0.17
2.01 ± 0.21
2.00 ± 0.13
1.95 ± 0.10
2.01 ± 0.18
1.02 ± 0.04

Train
(0.13)
(0.11)
(0.20)
(0.14)
(0.10)
(0.54)

2.11 ± 0.09
2.02 ± 0.12
2.00 ± 0.12
2.09 ± 0.12
2.30 ± 0.17
0.88 ± 0.06

Train
(0.46)
(0.29)
(0.55)
(0.37)
(0.32)
(0.12)

T

K Epochs WCT

100
500
30
100
180
200

12
40
10
20
20
10

30
50
30
50
100
40

1 : 33
16 : 04
0 : 17
1 : 56
8 : 27
8 : 58

Table 2: Best training results on MNIST with EP reported in the literature.

EP (error %)
Train
Test
[Scellier and Bengio  2017] ∼ 2.2
(∼ 0)
[O’Connor et al.  2018]
2.37
(0.15)
[O’Connor et al.  2019]
2.19

precise deﬁnition of these operations and their inverse. Noting Nfc and Nconv the number of fully
connected and convolutional layers respectively  we can deﬁne the function:

Φ(x {sn} {hn}) =

Nconv−1(cid:88)

n=0

hn • P(cid:0)W conv

n n+1 ∗ hn+1(cid:1) +

n=0

sn(cid:62) · W fc

Nfc−1(cid:88)
n n+1 · sn+1 
t+1 ≈ ∂Φ
t+1 ≈ ∂Φ
(cid:17)
W fc as in Eq. 16. As for ∆EP

∂s (t) and hn

∂h (t) if we
W conv  we

(18)

(19)

with • denoting generalized scalar product. We note that sn
ignore the activation function σ. We deﬁne ∆EP
follow the deﬁnition of Eq. (10):
1
β

(cid:16)P−1(hn β

t+1) ∗ hn+1 β

sn   ∆EP

W conv
nn+1

∆EP

(t) =

hn and ∆EP

t+1 − P−1(hn β

t

) ∗ hn+1 β

t

As can be seen in Fig. 4  the GDU property is qualitatively very well satisﬁed: Eq. (19) can thus be
safely used as a learning rule. More precisely however  some ∆EP
hn processes collapse to
zero as an effect of the non-linearity used (see Appendix C for greater details): the EP error signals
cannot be transmitted through saturated neurons  resulting in a RelMSE of ∼ 10−1 for the network
parameters - see Fig. 15 in Appendix D.

sn and ∆EP

5 Discussion

Table 1 shows the accuracy results on MNIST of several variations of our approach and of the
literature. First  EP overall performs as well or practically as well as BPTT in terms of test accuracy
in all situations. Second  no degradation of accuracy is seen between using the prototypical (P) rather
than the energy-based (EB) setting  although the prototypical setting requires three to ﬁve times less
time steps in the ﬁrst phase (T) and cuts the simulation time by a factor ﬁve to eight. Finally  the
best EP result  ∼ 1% test error  is obtained with our convolutional architecture. This is also the best
performance reported in the literature on MNIST training with EP. BPTT achieves 0.90% test error
using the same architecture. This slight degradation is due to saturated neurons which do no route
error signals (as reported in the previous section). The prototypical situation allows using highly
reduced number of time steps in the ﬁrst phase than Scellier and Bengio [2017] and O’Connor et al.
[2018]. On the other hand  O’Connor et al. [2019] manages to cut this number even more. This
comes at the cost of using an extra network to learn proper initial states for the EP network  which is
not needed in our approach.

8

Overall  our work broadens the scope of EP from its original formulation for biologically motivated
real-time dynamics and sheds new light on its practical understanding. We ﬁrst extended EP to a
discrete-time setting  which reduces its computational cost and allows addressing situations closer
to conventional machine learning. 6 Theorem 1 demonstrated that the gradients provided by EP are
strictly equal to the gradients computed with BPTT in speciﬁc conditions. Our numerical experiments
conﬁrmed the theorem and showed that its range of applicability extends well beyond the original
formulation of EP to prototypical neural networks widely used today. These results highlight that  in
principle  EP can reach the same performance as BPTT on benchmark tasks  for RNN models with
ﬁxed input. One limitation of our theory however is that it has yet to be adapted to sequential data:
such an extension would require to capture and learn correlations between successive equilibrium
states corresponding to different inputs.
Layer-wise analysis of the gradients computed by EP and BPTT show that the deeper the layer  the
more difﬁcult it becomes to ensure the GDU property. On top of non-linearity effects  this is mainly
due to the fact that the deeper the network  the longer it takes to reach equilibrium.
While this may be a conundrum for current processors  it should not be an issue for alternative
computing schemes. Physics research is now looking at neuromorphic computing approaches that
leverage the transient dynamics of physical devices for computation [Torrejon et al.  2017  Romera
et al.  2018  Feldmann et al.  2019]. In such systems  based on magnetism or optics  dynamical
equations are solved directly by the physical circuits and components  in parallel and at speed much
higher than processors. On the other hand  in such systems  the nonlocality of backprop is a major
concern [Ambrogio et al.  2018]. In this context  EP appears as a powerful approach as computing
gradients only requires measuring the system at the end of each phase  and going backward in time
is not needed. In a longer term  interfacing the algorithmics of EP with device physics could help
cutting drastically the cost of inference and learning of conventional computers  and thereby address
one of the biggest technological limitations of deep learning.

6We also expect that our discrete-time formulation would accelerate simulations in the setting of Scellier

et al. [2018] where the weights are not tied.

9

Acknowledgments

The authors would like to thank Joao Sacramento for feedback and discussions  as well as NSERC 
CIFAR  Samsung and Canada Research Chairs for funding. Julie Grollier and Damien Querlioz
acknowledge funding from the European Research Council  respectively under grants bioSPINspired
(682955) and NANOINFER (715872).

References
D. H. Ackley  G. E. Hinton  and T. J. Sejnowski. A learning algorithm for boltzmann machines.

Cognitive science  9(1):147–169  1985.

L. B. Almeida. A learning rule for asynchronous perceptrons with feedback in a combinatorial

environment. volume 2  pages 609–618  San Diego 1987  1987. IEEE  New York.

S. Ambrogio  P. Narayanan  H. Tsai  R. M. Shelby  I. Boybat  C. Nolfo  S. Sidler  M. Giordano 
M. Bodini  N. C. Farinha  et al. Equivalent-accuracy accelerated neural-network training using
analogue memory. Nature  558(7708):60  2018.

F. Crick. The recent excitement about neural networks. Nature  337(6203):129–132  1989.

Editorial. Big data needs a hardware revolution. Nature  554(7691):145  Feb. 2018. doi: 10.1038/

d41586-018-01683-1.

J. Feldmann  N. Youngblood  C. Wright  H. Bhaskaran  and W. Pernice. All-optical spiking neurosy-

naptic networks with self-learning capabilities. Nature  569(7755):208  2019.

Y. LeCun  Y. Bengio  and G. Hinton. Deep learning. Nature  521(7553):436  2015.

J. R. Movellan. Contrastive hebbian learning in the continuous hopﬁeld model. In Connectionist

models  pages 10–17. Elsevier  1991.

P. O’Connor  E. Gavves  and M. Welling. Initialized equilibrium propagation for backprop-free
training. In International Conference on Machine Learning  Workshop on Credit Assignment in
Deep Learning and Deep Reinforcement Learning  2018. URL https://ivi.fnwi.uva.nl/
isis/publications/2018/OConnorICML2018.

P. O’Connor  E. Gavves  and M. Welling. Training a spiking neural network with equilibrium
In K. Chaudhuri and M. Sugiyama  editors  Proceedings of Machine Learning
propagation.
Research  volume 89 of Proceedings of Machine Learning Research  pages 1516–1523. PMLR 
16–18 Apr 2019. URL http://proceedings.mlr.press/v89/o-connor19a.html.

F. J. Pineda. Generalization of back-propagation to recurrent neural networks. 59:2229–2232  1987.

M. Romera  P. Talatchian  S. Tsunegi  F. A. Araujo  V. Cros  P. Bortolotti  J. Trastoy  K. Yakushiji 
A. Fukushima  H. Kubota  et al. Vowel recognition with four coupled spin-torque nano-oscillators.
Nature  563(7730):230  2018.

D. E. Rumelhart  G. E. Hinton  and R. J. Williams. Learning internal representations by error
propagation. Technical report  California Univ San Diego La Jolla Inst for Cognitive Science 
1985.

F. Scarselli  M. Gori  A. C. Tsoi  M. Hagenbuchner  and G. Monfardini. The graph neural network

model. IEEE Transactions on Neural Networks  20(1):61–80  2009.

B. Scellier and Y. Bengio.

Towards a biologically plausible backprop.

arXiv:1602.05179v2  914  2016.

arXiv preprint

B. Scellier and Y. Bengio. Equilibrium propagation: Bridging the gap between energy-based models

and backpropagation. Frontiers in computational neuroscience  11  2017.

B. Scellier and Y. Bengio. Equivalence of equilibrium propagation and recurrent backpropagation.

Neural computation  31(2):312–329  2019.

10

B. Scellier  A. Goyal  J. Binas  T. Mesnard  and Y. Bengio. Generalization of equilibrium propagation

to vector ﬁeld dynamics. arXiv preprint arXiv:1808.04873  2018.

C. Tallec and Y. Ollivier. Unbiased online recurrent optimization. arXiv preprint arXiv:1702.05043 

2017.

J. Torrejon  M. Riou  F. A. Araujo  S. Tsunegi  G. Khalsa  D. Querlioz  P. Bortolotti  V. Cros 
K. Yakushiji  A. Fukushima  et al. Neuromorphic computing with nanoscale spintronic oscillators.
Nature  547(7664):428  2017.

R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural

networks. Neural computation  1(2):270–280  1989.

11

,Maxence Ernoult
Julie Grollier
Damien Querlioz
Yoshua Bengio
Benjamin Scellier