2018,Efficient Neural Network Robustness Certification with General Activation Functions,Finding minimum distortion of adversarial examples and thus certifying robustness in neural networks classifiers is known to be a challenging problem. Nevertheless  recently it has been shown to be possible to give a non-trivial certified lower bound of minimum distortion  and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However  a generic robustness certification for \textit{general} activation functions still remains largely unexplored. To address this issue  in this paper we introduce CROWN  a general framework to certify robustness of neural networks with general activation functions. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions  hence allowing it to tackle general activation functions including but not limited to the four popular choices: ReLU  tanh  sigmoid and arctan. In addition  we facilitate the search for a tighter certified lower bound by \textit{adaptively} selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin  while having comparable computational efficiency. Furthermore  CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions  including tanh  sigmoid and arctan.,EfﬁcientNeuralNetworkRobustnessCertiﬁcationwithGeneralActivationFunctionsHuanZhang1 † ∗Tsui-WeiWeng2 †Pin-YuChen3Cho-JuiHsieh1LucaDaniel21UniversityofCalifornia LosAngeles LosAngelesCA900952MassachusettsInstituteofTechnology Cambridge MA021393MIT-IBMWatsonAILab IBMResearch YorktownHeights NY10598huan@huan-zhang.com twweng@mit.edupin-yu.chen@ibm.com chohsieh@cs.ucla.edu dluca@mit.eduAbstractFindingminimumdistortionofadversarialexamplesandthuscertifyingrobustnessinneuralnetworkclassiﬁersforgivendatapointsisknowntobeachallengingproblem.Nevertheless recentlyithasbeenshowntobepossibletogiveanon-trivialcertiﬁedlowerboundofminimumadversarialdistortion andsomerecentprogresshasbeenmadetowardsthisdirectionbyexploitingthepiece-wiselinearnatureofReLUactivations.However agenericrobustnesscertiﬁcationforgeneralactivationfunctionsstillremainslargelyunexplored.Toaddressthisissue inthispaperweintroduceCROWN ageneralframeworktocertifyrobustnessofneuralnetworkswithgeneralactivationfunctionsforgiveninputdatapoints.Thenoveltyinouralgorithmconsistsofboundingagivenactivationfunctionwithlinearandquadraticfunctions henceallowingittotacklegeneralactivationfunctionsincludingbutnotlimitedtofourpopularchoices:ReLU tanh sigmoidandarctan.Inaddition wefacilitatethesearchforatightercertiﬁedlowerboundbyadaptivelyselectingappropriatesurrogatesforeachneuronactivation.ExperimentalresultsshowthatCROWNonReLUnetworkscannotablyimprovethecertiﬁedlowerboundscomparedtothecurrentstate-of-the-artalgorithmFast-Lin whilehavingcomparablecomputationalefﬁciency.Furthermore CROWNalsodemonstratesitseffectivenessandﬂexibilityonnetworkswithgeneralactivationfunctions includingtanh sigmoidandarctan.1IntroductionWhileneuralnetworks(NNs)haveachievedremarkableperformanceandaccomplishedunprece-dentedbreakthroughsinmanymachinelearningtasks recentstudieshavehighlightedtheirlackofrobustnessagainstadversarialperturbations[1 2].Forexample inimagelearningtaskssuchasobjectclassiﬁcation[3 4 5 6]orcontentcaptioning[7] visuallyindistinguishableadversarialexamplescanbeeasilycraftedfromnaturalimagestoalteraNN’spredictionresult.Beyondthewhite-boxattacksettingwherethetargetmodelisentirelytransparent visuallyimperceptibleadversarialperturbationscanalsobegeneratedintheblack-boxsettingbyonlyusingthepredictionresultsofthetargetmodel[8 9 10 11].Inaddition real-lifeadversarialexampleshavebeenmadepossiblethroughthelensofrealizingphysicalperturbations[12 13 14].AsNNsarebecomingacoretechniquedeployedinawiderangeofapplications includingsafety-criticaltasks certifyingrobustnessofaNNagainstadversarialperturbationshasbecomeanimportantresearchtopicinmachinelearning.∗WorkdoneduringinternshipatIBMResearch.†Equalcontribution.CorrespondencetoHuanZhang<huan@huan-zhang.com>andTsui-WeiWeng<twweng@mit.edu>32ndConferenceonNeuralInformationProcessingSystems(NeurIPS2018) Montréal Canada.GivenaNN(possiblywithadeepandcomplicatednetworkarchitecture) weareinterestedincertifyingthe(local)robustnessofanarbitrarynaturalexamplex0byensuringallitsneighborhoodhasthesameinferenceoutcome(e.g. consistenttop-1prediction).Inthispaper theneighborhoodofx0ischaracterizedbyan(cid:96)pballcenteredatx0 foranyp≥1.Geometricallyspeaking theminimumdistanceofamisclassiﬁednearbyexampletox0istheleastadversarystrength(a.k.a.minimumadversarialdistortion)requiredtoalterthetargetmodel’sprediction whichisalsothelargestpossiblerobustnesscertiﬁcateforx0.Unfortunately ﬁndingtheminimumdistortionofadversarialexamplesinNNswithRectiﬁedLinearUnit(ReLU)activations whichisoneofthemostwidelyusedactivationfunctions isknowntobeanNP-completeproblem[15 16].ThismakesformalveriﬁcationtechniquessuchasReluplex[15]computationallydemandingevenforsmall-sizedNNsandsufferfromscalabilityissues.AlthoughcertifyingthelargestpossiblerobustnessischallengingforReLUnetworks thepiece-wiselinearnatureofReLUscanbeexploitedtoefﬁcientlycomputeanon-trivialcertiﬁedlowerboundoftheminimumdistortion[17 18 19 20].BeyondReLU onefundamentalproblemthatremainslargelyunexploredishowtogeneralizetherobustnesscertiﬁcationtechniquetootherpopularactivationfunctionsthatarenotpiece-wiselinear suchastanhandsigmoid andhowtomotivateandcertifythedesignofotheractivationfunctionstowardsimprovedrobustness.Inthispaper wetackletheprecedingproblembyproposinganefﬁcientrobustnesscertiﬁcationframeworkforNNswithgeneralactivationfunctions.Ourmaincontributionsinthispaperaresummarizedasfollows:•WeproposeagenericanalysisframeworkCROWNforcertifyingNNsusinglinearorquadraticupperandlowerboundsforgeneralactivationfunctionsthatarenotnecessarilypiece-wiselinear.•Unlikepreviouswork[20] CROWNallowsﬂexibleselectionsofupper/lowerboundsforactivationfunctions enablinganadaptiveschemethathelpstoreduceapproximationerrors.OurexperimentsshowthatCROWNachievesupto26%improvementsincertiﬁedlowerboundscomparedto[20].•OuralgorithmisefﬁcientandcanscaletolargeNNswithvariousactivationfunctions.ForaNNwithover10 000neurons wecangiveacertiﬁedlowerboundinabout1minuteon1CPUcore.2BackgroundandRelatedWorkForReLUnetworks ﬁndingtheminimumadversarialdistortionforagiveninputdatapointx0canbecastasamixedintegerlinearprogramming(MILP)problem[21 22 23].Reluplex[15 24]usesasatisﬁablemodulotheory(SMT)toencodeReLUactivationsintolinearconstraints.Similarly Planet[25]usessatisﬁability(SAT)solvers.However duetotheNP-completenessforsolvingsuchaproblem[15] thesemethodscanonlyﬁndminimumdistortionforverysmallnetworks.ItcantakeReluplexseveralhourstoﬁndtheminimumdistortionofanexampleforaReLUnetworkwith5inputs 5outputsand300neurons[15].Ontheotherhand acomputationallyfeasiblealternativeofrobustnesscertiﬁcateistoprovideanon-trivialandcertiﬁedlowerboundofminimumdistortion.Someanalyticallowerboundsbasedonoperatornormsontheweightmatrices[3]ortheJacobianmatrixinNNs[17]donotexploitspecialpropertyofReLUandthuscanbeveryloose[20].Theboundsin[26 27]arebasedonthelocalLipschitzconstant.[26]assumesacontinuousdifferentiableNNandhenceexcludesReLUnetworks;aclosedformlower-boundisalsohardtoderivefornetworksbeyond2layers.[27]appliestoReLUnetworksandusesExtremeValueTheorytoprovideanestimatedlowerbound(CLEVERscore).AlthoughtheCLEVERscoreiscapableofreﬂectingthelevelofrobustnessindifferentNNsandisscalabletolargenetworks itisnotacertiﬁedlowerbound.Ontheotherhand [18]usetheideaofaconvexouteradversarialpolytopeinReLUnetworkstocomputeacertiﬁedlowerboundbyrelaxingtheMILPcertiﬁcationproblemtolinearprograming(LP).[20]exploittheReLUpropertytoboundtheactivationfunction(orthelocalLipschitzconstant)andprovideefﬁcientalgorithms(Fast-LinandFast-Lip)forcomputingacertiﬁedlowerbound achievingstate-of-the-artperformance.Arecentwork[28]usesabstracttransformationstozonotopesforprovingrobustnesspropertyforReLUnetworks.Nonetheless therearestillsomeapplicationsdemandnon-ReLUactivations e.g.RNNandLSTM thusageneralframeworkthatcanefﬁcientlycomputenon-trivialandcertiﬁedlowerboundsforNNswithgeneralactivationfunctionsisofgreatimportance.WeaimatﬁllingthisgapandproposeCROWNthatcanperformefﬁcientrobustnesscertiﬁcationtoNNswithgeneralactivationfunctions.Table1summarizesthedifferencesofotherapproachesandCROWN.Notethatthesemideﬁniteprogrammingapproachproposedin[19]andarecentwork[29]basedonsolvingLagrangiandualcanbothhandlegeneralactivationfunctions but[19]islimitedtoNNswithonehiddenlayerand[29]tradesoffthequalityofrobustnessboundwithscalability.2Table1:ComparisonofmethodsforprovidingadversarialrobustnesscertiﬁcationinNNs.MethodNon-trivialboundMulti-layerScalabilityBeyondReLUSzegedyet.al.[3]×(cid:88)(cid:88)(cid:88)Reluplex[15] Planet[25](cid:88)(cid:88)××Hein&Andriushchenko[26](cid:88)×(cid:88)differentiable*Raghunathanetal.[19](cid:88)××(cid:88)KolterandWong[18](cid:88)(cid:88)(cid:88)×Fast-lin/Fast-lip[20](cid:88)(cid:88)(cid:88)×CROWN(ours)(cid:88)(cid:88)(cid:88)(cid:88)(general)*Continuouslydifferentiableactivationfunctionrequired(soft-plusisdemonstratedin[26])Somerecentworks(suchasrobustoptimizationbasedadversarialtraining[30]orregion-basedclassiﬁcation[31])empiricallyexhibitstrongrobustnessagainstseveraladversarialattacks whichisbeyondthescopeofprovablerobustnesscertiﬁcation.Inaddition Sinhaetal.[16]providedistributionalrobustnesscertiﬁcationbasedonWassersteindistancebetweendatadistributions whichisdifferentfromthelocal(cid:96)pballrobustnessmodelconsideredinthispaper.3CROWN:AgeneralframeworkforcertifyingneuralnetworksOverviewofourresults.Inthissection wepresentageneralframeworkCROWNforefﬁcientlycomputingacertiﬁedlowerboundofminimumadversarialdistortiongivenanyinputdatapointx0withgeneralactivationfunctionsinlargerNNs.WeﬁrstprovideprinciplesinSection3.1toderiveoutputboundsofNNswhentheinputsareperturbedwithinan(cid:96)pballandeachneuronhasdifferent(adaptive)linearapproximationboundsonitsactivationfunction.InSection3.2 wedemonstratehowtoproviderobustnesscertiﬁcationforfourwidely-usedactivationfunctions(ReLU tanh sigmoidandarctan)usingCROWN.Inparticular weshowthatthestate-of-the-artFast-LinalgorithmisaspecialcaseundertheCROWNframeworkandthattheadaptiveselectionsofapproximationboundsallowCROWNtoachieveatighter(larger)certiﬁedlowerbound(seeSection4).InSection3.3 wefurtherhighlighttheﬂexibilityofCROWNtoincorporatequadraticapproximationsontheactivationfunctionsinadditiontothelinearapproximationsdescribedinSection3.1.3.1GeneralframeworkNotations.Foranm-layerneuralnetworkwithaninputvectorx∈Rn0 letthenumberofneuronsineachlayerbenk ∀k∈[m] where[i]denotesset{1 2 ··· i}.Letthek-thlayerweightmatrixbeW(k)∈Rnk×nk−1andbiasvectorbeb(k)∈Rnk andletΦk:Rn0→Rnkbetheoperatormappingfrominputtolayerk.WehaveΦk(x)=σ(W(k)Φk−1(x)+b(k)) ∀k∈[m−1] whereσ(·)isthecoordinate-wiseactivationfunction.Whileourmethodologyisapplicabletoanyactivationfunctionofinterest weemphasizeonfourmostwidely-usedactivationfunctions namelyReLU:σ(y)=max(y 0) hyperbolictangent:σ(y)=tanh(y) sigmoid:σ(y)=1/(1+e−y)andarctan:σ(y)=arctan(y).NotethattheinputΦ0(x)=x andthevectoroutputoftheNNisf(x)=Φm(x)=W(m)Φm−1(x)+b(m).Thej-thoutputelementisdenotedasfj(x)=[Φm(x)]j.Inputperturbationandpre-activationbounds.Letx0∈Rn0beagivendatapoint andlettheperturbedinputvectorxbewithinan-bounded(cid:96)p-ballcenteredatx0 i.e. x∈Bp(x0 ) whereBp(x0 ):={x|(cid:107)x−x0(cid:107)p≤}.Forther-thneuronink-thlayer letitspre-activationinputbey(k)r wherey(k)r=W(k)r :Φk−1(x)+b(k)randW(k)r :denotesther-throwofmatrixW(k).Whenx0isperturbedwithinan-bounded(cid:96)p-ball letl(k)r u(k)r∈Rbethepre-activationlowerboundandupperboundofy(k)r i.e.l(k)r≤y(k)r≤u(k)r.Below weﬁrstdeﬁnethelinearupperboundsandlowerboundsofactivationfunctionsinDeﬁni-tion3.1 whicharethekeytoderiveexplicitoutputboundsforanm-layerneuralnetworkwithgeneralactivationfunctions.TheformalstatementoftheexplicitoutputboundsisshowninTheorem3.2.Deﬁnition3.1(Linearboundsonactivationfunction).Forther-thneuronink-thlayerwithpre-activationboundsl(k)r u(k)randtheactivationfunctionσ(y) deﬁnetwolinearfunctionsh(k)U r h(k)L r:R→R h(k)U r(y)=α(k)U r(y+β(k)U r) h(k)L r(y)=α(k)L r(y+β(k)L r) suchthath(k)L r(y)≤σ(y)≤h(k)U r(y) y∈[l(k)r u(k)r] ∀k∈[m−1] r∈[nk]andα(k)U r α(k)L r∈R+ β(k)U r β(k)L r∈R.3Notethattheparametersα(k)U r α(k)L r β(k)U r β(k)L rdependonl(k)randu(k)r i.e.fordifferentl(k)randu(k)rwemaychoosedifferentparameters.Also foreaseofexposition inthispaperwerestrictα(k)U r α(k)L r≥0.However Theorem3.2canbeeasilygeneralizedtothecaseofnegativeα(k)U r α(k)L r.Theorem3.2(Explicitoutputboundsofneuralnetworkf).Givenanm-layerneuralnetworkfunctionf:Rn0→Rnm thereexiststwoexplicitfunctionsfLj:Rn0→RandfUj:Rn0→Rsuchthat∀j∈[nm] ∀x∈Bp(x0 ) theinequalityfLj(x)≤fj(x)≤fUj(x)holdstrue wherefUj(x)=Λ(0)j :x+m(cid:88)k=1Λ(k)j :(b(k)+∆(k): j) fLj(x)=Ω(0)j :x+m(cid:88)k=1Ω(k)j :(b(k)+Θ(k): j) (1)Λ(k−1)j :=(cid:26)e(cid:62)jifk=m+1;(Λ(k)j :W(k))(cid:12)λ(k−1)j :ifk∈[m].Ω(k−1)j :=(cid:26)e(cid:62)jifk=m+1;(Ω(k)j :W(k))(cid:12)ω(k−1)j :ifk∈[m].and∀i∈[nk] wedeﬁnefourmatricesλ(k) ω(k) ∆(k) Θ(k)∈Rnm×nk:λ(k)j i=α(k)U iifk(cid:54)=0 Λ(k+1)j :W(k+1): i≥0;α(k)L iifk(cid:54)=0 Λ(k+1)j :W(k+1): i<0;1ifk=0.ω(k)j i=α(k)L iifk(cid:54)=0 Ω(k+1)j :W(k+1): i≥0;α(k)U iifk(cid:54)=0 Ω(k+1)j :W(k+1): i<0;1ifk=0.∆(k)i j=β(k)U iifk(cid:54)=m Λ(k+1)j :W(k+1): i≥0;β(k)L iifk(cid:54)=m Λ(k+1)j :W(k+1): i<0;0ifk=m.Θ(k)i j=β(k)L iifk(cid:54)=m Ω(k+1)j :W(k+1): i≥0;β(k)U iifk(cid:54)=m Ω(k+1)j :W(k+1): i<0;0ifk=m.and(cid:12)istheHadamardproductandej∈Rnmisastandardunitvectoratjthcoordinate.Theorem3.2illustrateshowaNNfunctionfj(x)canbeboundedbytwolinearfunctionsfUj(x)andfLj(x)whentheactivationfunctionofeachneuronisboundedbytwolinearfunctionsh(k)U randh(k)L rinDeﬁnition3.1.Thecentralideaistounwraptheactivationfunctionslayerbylayerbyconsideringthesignsoftheassociated(equivalent)weightsofeachneuronandapplythetwolinearboundsh(k)U randh(k)L r.Aswedemonstrateintheproof whenwereplacetheactivationfunctionswiththecorrespondinglinearupperboundsandlowerboundsatthelayerm−1 wecanthendeﬁneequivalentweightsandbiasesbasedontheparametersofh(m−1)U randh(m−1)L r(e.g.Λ(k) ∆(k) Ω(k) Θ(k)arerelatedtothetermsα(k)U r β(k)U r α(k)L r β(k)L r respectively)andthenrepeattheprocedureto“back-propagate”totheinputlayer.ThisallowsustoobtainfUj(x)andfLj(x)in(1).TheformalproofofTheorem3.2isinAppendixA.Notethatforaneuronrinlayerk theslopesofitslinearupperandlowerboundsα(k)U r α(k)L rcanbedifferent.Thisimplies:1.Fast-Lin[20]isaspecialcaseofourframeworkastheyrequiretheslopesα(k)U r α(k)L rtobethesame;anditonlyappliestoReLUnetworks(cf.Sec.3.2).InFast-Lin Λ(0)andΩ(0)areidentical.2.OurCROWNframeworkallowsadaptiveselectionsonthelinearapproximationwhencomputingcertiﬁedlowerboundsofminimumadversarialdistortion whichisthemaincontributortoimprovethecertiﬁedlowerboundasdemonstratedintheexperimentsinSection4.Globalbounds.Moreimportantly sincetheinputx∈Bp(x0 ) wecantakethemaximum i.e.maxx∈Bp(x0 )fUj(x) andminimum i.e.minx∈Bp(x0 )fLj(x) asapairofglobalupperandlowerboundoffj(x)–whichinfacthasclosed-formsolutionsbecausefUj(x)andfLj(x)aretwolinearfunctionsandx∈Bp(x0 )isaconvexnormconstraint.ThisresultisformallypresentedbelowanditsproofisgiveninAppendixB.Corollary3.3(Closed-formglobalbounds).Givenadatapointx0∈Rn0 (cid:96)pballparametersp≥1and>0.Foranm-layerneuralnetworkfunctionf:Rn0→Rnm thereexiststwoﬁxedvaluesγLjandγUjsuchthat∀x∈Bp(x0 )and∀j∈[nm] 1/q=1−1/p theinequalityγLj≤fj(x)≤γUjholdstrue whereγUj=(cid:107)Λ(0)j :(cid:107)q+Λ(0)j :x0+m(cid:88)k=1Λ(k)j :(b(k)+∆(k): j) γLj=−(cid:107)Ω(0)j :(cid:107)q+Ω(0)j :x0+m(cid:88)k=1Ω(k)j :(b(k)+Θ(k): j).(2)4Table2:Linearupperboundparametersofvariousactivationfunctions:h(k)U r(y)=α(k)U r(y+β(k)U r)Upperboundh(k)U rr∈S+kr∈S−kr∈S±kforactivationfunctionα(k)U rβ(k)U rα(k)U rβ(k)U rα(k)U rβ(k)U rReLU1000a−l(k)r(a≥u(k)ru(k)r−l(k)r e.g.a=u(k)ru(k)r−l(k)r)Sigmoid tanhσ(cid:48)(d)σ(d)α(k)U r−d*σ(u(k)r)−σ(l(k)r)u(k)r−l(k)rσ(l(k)r)α(k)U r−l(k)rσ(cid:48)(d)σ(l(k)r)α(k)U r−l(k)r(denotedasσ(y))(l(k)r≤d≤u(k)r)(σ(d)−σ(l(k)r)d−l(k)r−σ(cid:48)(d)=0 d≥0)(cid:5)*Ifα(k)U riscloseto0 wesuggesttocalculatetheinterceptdirectly α(k)U r·β(k)U r=σ(d)−α(k)U rd toavoidnumericalissuesinimplementation.Sameforothersimilarcases.(cid:5)Alternatively ifthesolutiond≥u(k)r thenwecansetα(k)U r=σ(u(k)r)−σ(l(k)r)u(k)r−l(k)r.Table3:Linearlowerboundparametersofvariousactivationfunctions:h(k)L r(y)=α(k)L r(y+β(k)L r)Lowerboundh(k)L rr∈S+kr∈S−kr∈S±kforactivationfunctionα(k)L rβ(k)L rα(k)L rβ(k)L rα(k)L rβ(k)L rReLU1000a0(0≤a≤1 e.g.a=u(k)ru(k)r−l(k)r 0 1)Sigmoid tanhσ(u(k)r)−σ(l(k)r)u(k)r−l(k)rσ(l(k)r)α(k)L r−l(k)rσ(cid:48)(d)σ(d)α(k)L r−dσ(cid:48)(d)σ(u(k)r)α(k)L r−u(k)r(denotedasσ(y))(l(k)r≤d≤u(k)r)(σ(d)−σ(u(k)r)d−u(k)r−σ(cid:48)(d)=0 d≤0)††Alternatively ifthesolutiond≤l(k)r thenwecansetα(k)L r=σ(u(k)r)−σ(l(k)r)u(k)r−l(k)r.Certiﬁedlowerboundofminimumdistortion.Givenaninputexamplex0andanm-layerNN letcbethepredictedclassofx0andt(cid:54)=cbethetargetedattackclass.WeaimtousetheuniformboundsestablishedinCorollary3.3toobtainthelargestpossiblelowerbound˜tand˜oftargetedanduntargetedattacksrespectively whichcanbeformulatedasfollows:˜t=maxs.t.γLc()−γUt()>0and˜=mint(cid:54)=c˜t.Wenotethatalthoughthereisalineartermin(2) othertermssuchasΛ(k) ∆(k)andΩ(k) Θ(k)alsoimplicitlydependon.Thisisbecausetheparametersα(k)U i β(k)U i α(k)L i β(k)L idependonl(k)i u(k)i whichmayvarywith;thusthevaluesinΛ(k) ∆(k) Ω(k) Θ(k)dependon.ItisthereforedifﬁculttoobtainanexplicitexpressionofγLc()−γUt()intermsof.Fortunately wecanstillperformabinarysearchtoobtain˜twithCorollary3.3.Moreprecisely weﬁrstinitializeatsomeﬁxedpositivevalueandapplyCorollary3.3repeatedlytoobtainl(k)randu(k)rfromk=1tomandr∈[nk].WethencheckiftheconditionγLc−γUt>0issatisﬁed.Ifso weincrease;otherwise wedecrease;andwerepeattheprocedureuntilagiventolerancelevelismet.2TimeComplexity.WithCorollary3.3 wecancomputeanalyticoutputboundsefﬁcientlywithoutresortingtoanyoptimizationsolversforgeneral(cid:96)pdistortion andthetimecomplexityforanm-layerReLUnetworkispolynomialtimeincontrasttoReluplexorMixed-IntegerOptimization-basedapproach[22 23]whereSMTandMIOsolversareexponential-time.Foranmlayernetworkwithnneuronsperlayerandnoutputs timecomplexityofCROWNisO(m2n3).FormingΛ(0)andΩ(0)forthem-thlayerinvolvesmultiplicationsoflayerweightsinasimilarcostofforwardpropagationinO(mn3)time.Also theboundsforallpreviousk∈[m−1]layersneedtobecomputedbeforehandinO(kn3)time;thusthetotaltimecomplexityisO(m2n3).3.2Casestudies:CROWNforReLU tanh sigmoidandarctanactivationsInSection3.1weshowedthataslongasonecanidentifytwolinearfunctionshU(y) hL(y)toboundageneralactivationfunctionσ(y)foreachneuron wecanuseCorollary3.3withabinarysearch2Theboundcanbefurtherimprovedbyconsideringg(x):=fc(x)−ft(x)andreplacingthelastlayer’sweightsbyW(m)c :−W(m)t :.Thisisalsousedby[20].5(a)r∈S+k(b)r∈S−k(c)r∈S±kFigure1:σ(y)=tanh.Greenlinesaretheupperboundsh(k)U r;redlinesarethelowerboundsh(k)L r.toobtaincertiﬁedlowerboundsofminimumdistortion.Inthissection weillustratehowtoﬁndparametersα(k)U r α(k)L randβ(k)U r β(k)L rofhU(y) hL(y)forfourmostwidelyusedactivationfunctions:ReLU tanh sigmoidandarctan.Otheractivations includingbutnotlimitedtoleakyReLU ELUandsoftplus canbeeasilyincorporatedintoourCROWNframeworkfollowingasimilarprocedure.Segmentingactivationfunctions.Basedonthesignsofl(k)randu(k)r wedeﬁneapartition{S+k S±k S−k}ofset[nk]suchthateveryneuronink-thlayerbelongstoexactlyoneofthethreesets.TheformaldeﬁnitionofS+k S±kandS−kisS+k={r∈[nk]|0≤l(k)r≤u(k)r} S±k={r∈[nk]|l(k)r<0<u(k)r} andS−k={r∈[nk]|l(k)r≤u(k)r≤0}.Forneuronsineachpartitionedset wedeﬁnecorrespondingupperboundh(k)U randlowerboundh(k)L rintermsofl(k)randu(k)r.Aswewillseeshortly segmentingtheactivationfunctionsbasedonl(k)randu(k)risusefultoboundagivenactivationfunction.Wenotetherearemultiplewaysofsegmentingtheactivationfunctionsanddeﬁningthepartitionedsets(e.g.basedonthevaluesofl(k)r u(k)rratherthantheirsigns) andwecaneasilyincorporatethisintoourframeworktoprovidethecorrespondingexplicitoutputboundsforthenewpartitionsets.Inthecasestudy weconsiderS+k S±kandS−kforthefouractivations asthispartitionreﬂectsthecurvatureoftanh sigmoidandarctanfunctionsandactivationstatesofReLU.Boundingtanh/sigmoid/arctan.Fortanhactivation σ(y)=1−e−2y1+e−2y;forsigmoidactivation σ(y)=11+e−y;forarctanactivation σ(y)=arctan(y).Allfunctionsareconvexononeside(y<0)andconcaveontheotherside(y>0) thusthesamerulescanbeusedtoﬁndh(k)U randh(k)L r.Belowwecall(l(k)r σ(l(k)r))asleftend-pointand(u(k)r σ(u(k)r))asrightend-point.Forr∈S+k sinceσ(y)isconcave wecanleth(k)U rbeanytangentlineofσ(y)atpointd∈[l(k)r u(k)r] andleth(k)L rpassthetwoend-points.Similarly σ(y)isconcaveforr∈S+k thuswecanleth(k)L rbeanytangentlineofσ(y)atpointd∈[l(k)r u(k)r]andleth(k)U rpassthetwoend-points.Lastly forr∈S±k wecanleth(k)U rbethetangentlinethatpassestheleftend-pointand(d σ(d))whered≥0andh(k)U rbethetangentlinethatpassestherightend-pointand(d σ(d))whered≤0.Thevalueofdfortranscendentalfunctionscanbefoundusingabinarysearch.TheplotsofupperandlowerboundsfortanhandsigmoidareinFigure1and3(inAppendix).Plotsforarctanaresimilarandsoomitted.BoundingReLU.ForReLUactivation σ(y)=max(0 y).Ifr∈S+k wehaveσ(y)=yandsowecanseth(k)U r=h(k)L r=y;ifr∈S−k wehaveσ(y)=0 andthuswecanseth(k)U r=h(k)L r=0;ifr∈S±k wecanseth(k)U r=u(k)ru(k)r−l(k)r(y−l(k)r)andh(k)L r=ay 0≤a≤1.Settinga=u(k)ru(k)r−l(k)rleadstothelinearlowerboundusedinFast-Lin[20].Thus Fast-Linisaspecialcaseunderourframework.Weproposetoadaptivelychoosea whereweseta=1whenu(k)r≥|l(k)r|anda=0whenu(k)r<|l(k)r|.Inthisway theareabetweenthelowerboundh(k)L r=ayandσ(y)(whichreﬂectsthegapbetweenthelowerboundandtheReLUfunction)isalwaysminimized.Asshowninourexperiments theadaptiveselectionofh(k)L rbasedonthevalueofu(k)randl(k)rhelpstoachieveatightercertiﬁedlowerbound.Figure4(inAppendix)illustratestheideadiscussedhere.6Summary.Wesummarizedtheaboveanalysisonchoosingvalidlinearfunctionsh(k)U randh(k)L rinTable2and3.Ingeneral aslongash(k)U randh(k)L rareidentiﬁedfortheactivationfunctions wecanuseCorollary3.3tocomputecertiﬁedlowerboundsforgeneralactivationfunctions.Notethatthereremainmanyotherchoicesofh(k)U randh(k)L rasvalidupper/lowerboundsofσ(y) butideally wewouldlikethemtobeclosetoσ(y)inordertoachieveatighterlowerboundofminimumdistortion.3.3ExtensiontoquadraticboundsInadditiontothelinearboundsonactivationfunctions theproposedCROWNframeworkcanalsoincorporatequadraticboundsbyaddingaquadratictermtoh(k)U randh(k)L r:h(k)U r(y)=η(k)U ry2+α(k)U r(y+β(k)U r) h(k)L r(y)=η(k)L ry2+α(k)L r(y+β(k)L r) whereη(k)U r η(k)L r∈R.Followingtheprocedureofunwrappingtheactivationfunctionsatthelayerm−1 weshowinAppendixDthattheoutputupperboundandlowerboundwithquadraticapproximationsare:fUj(x)=Φm−2(x)(cid:62)Q(m−1)UΦm−2(x)+2p(m−1)UΦm−2(x)+s(m−1)U (3)fLj(x)=Φm−2(x)(cid:62)Q(m−1)LΦm−2(x)+2p(m−1)LΦm−2(x)+s(m−1)L (4)whereQ(m−1)U=W(m−1)(cid:62)D(m−1)UW(m−1) Q(m−1)L=W(m−1)(cid:62)D(m−1)LW(m−1) p(m−1)U p(m−1)L s(m−1)U ands(m−1)LaredeﬁnedinAppendixDduetopagelimit.Whenm=2 Φm−2(x)=xandwecandirectlyoptimizeoverx∈Bp(x0 );otherwise wecanusethepostactivationboundsoflayerm−2astheconstraints.D(m−1)Uin(3)isadiagonalmatrixwithi-thentrybeingW(m)j iη(m−1)U i ifW(m)j i≥0orW(m)j iη(m−1)L i ifW(m)j i<0.Thus ingeneralQ(m−1)Uisindeﬁnite resultinginanon-convexoptimizationwhenﬁndingtheglobalboundsasinCorollary3.3.Fortunately byproperlychoosingthequadraticbounds wecanmaketheproblemmaxx∈Bp(x0 )fUj(x)intoaconvexQuadraticProgrammingproblem;forexample wecanletη(m−1)U i=0forallW(m)j i>0andletη(m−1)L i>0tomakeD(m−1)Uhaveonlynegativeandzerodiagonalsforthemaximizationproblem–thisisequivalenttoapplyingalinearupperboundandaquadraticlowerboundtoboundtheactivationfunction.Similarly forD(m−1)L weletη(m−1)U i=0forallW(m)j i<0andletη(m−1)L i>0tomakeD(m−1)Lhavenon-negativediagonalsandhencetheproblemminx∈Bp(x0 )fLj(x)isconvex.Wecansolvethisconvexprogramwithprojectedgradientdescent(PGD)forx∈Bp(x0 )andArmijolinesearch.Empirically weﬁndthatPGDusuallyconvergeswithinafewiterations.4ExperimentsMethods.ForReLUnetworks CROWN-AdaisCROWNwithadaptivelinearbounds(Sec.3.2) CROWN-QuadisCROWNwithquadraticbounds(Sec.3.3).Fast-LinandFast-Liparestate-of-the-artfastcertiﬁedlowerboundproposedin[20].Reluplexcansolvetheexactminimumadversarialdistortionbutisonlycomputationallyfeasibleforverysmallnetworks.LP-FullisbasedontheLPformulationin[18]andwesolveLPsforeachneuronexactlytoachievethebestpossiblebound.Fornetworkswithotheractivationfunctions CROWN-generalisourproposedmethod.ModelandDataset.WeevaluateCROWNandotherbaselinesonmulti-layerperceptron(MLP)modelstrainedonMNISTandCIFAR-10datasets.Wedenoteafeed-forwardnetworkwithmlayersandnneuronsperlayerasm×[n].FormodelswithReLUactivation weusepretrainedmodelsprovidedby[20]andalsoevaluatethesamesetof100randomtestimagesandrandomattacktargetsasin[20](accordingtotheirreleasedcode)tomakeourresultscomparable.FortrainingNNmodelswithotheractivationfunctions wesearchforbestlearningrateandweightdecayparameterstoachieveasimilarlevelofaccuracyasReLUmodels.ImplementationandSetup.WeimplementouralgorithmusingPython(numpywithnumba).MostcomputationsinourmethodarematrixoperationsthatcanbeautomaticallyparallelizedbytheBLASlibrary;however wesetthenumberofBLASthreadsto1forafaircomparisontoothermethods.ExperimentswereconductedonanIntelSkylakeserverCPUrunningat2.0GHzonGoogleCloud.Ourcodeisavailableathttps://github.com/CROWN-Robustness/Crown7(a)MNIST2×[20] (cid:96)2(b)MNIST2×[20] (cid:96)∞(c)MNIST3×[20] (cid:96)2(d)MNIST3×[20] (cid:96)∞Figure2:Certiﬁedlowerboundsandminimumdistortioncomparisonsfor(cid:96)2and(cid:96)∞distortions.Lefty-axisisdistortionandrighty-axis(blackline)iscomputationtime(seconds logarithmicscale).Onthetopofﬁguresaretheavg.CLEVERscoreandtheupperboundfoundbyC&Wattack[6].Fromlefttorightin(a)-(d):CROWN-Ada (CROWN-Quad) Fast-Lin Fast-Lip LP-Fulland(Reluplex).Table4:ComparisonofcertiﬁedlowerboundsonlargeReLUnetworks.Boundsaretheaverageover100images(skippedmisclassiﬁedimages)withrandomattacktargets.PercentageimprovementsarecalculatedagainstFast-LinasFast-LipisworsethanFast-Lin.NetworkCertiﬁedBoundsImprovement(%)AverageComputationTime(sec)(cid:96)pnormFast-LinFast-LipCROWN-AdaCROWN-AdavsFast-LinFast-LinFast-LipCROWN-AdaMNIST4×[1024](cid:96)11.576490.728001.88217+19%1.802.043.54(cid:96)20.188910.064870.22811+21%1.781.963.79(cid:96)∞0.008230.002640.00997+21%1.532.173.57CIFAR-107×[1024](cid:96)10.864680.092391.09067+26%13.2119.7622.43(cid:96)20.059370.004070.07496+26%12.5718.7121.82(cid:96)∞0.001340.000080.00169+26%8.9820.3416.66Table5:ComparisonofcertiﬁedlowerboundsbyCROWN-AdaonReLUnetworksandCROWN-generalonnetworkswithtanh sigmoidandarctanactivations.CIFARmodelswithsigmoidactiva-tionsachievemuchworseaccuracythanothernetworksandarethusexcluded.NetworkCertiﬁedBoundsbyCROWN-AdaandCROWN-generalAverageComputationTime(sec)(cid:96)pnormReLUtanhsigmoidarctanReLUtanhsigmoidarctanMNIST3×[1024](cid:96)13.002312.484072.942392.332461.251.611.681.70(cid:96)20.508410.272870.444710.303451.261.761.611.75(cid:96)∞0.025760.011820.021220.013631.371.781.761.77CIFAR-106×[2048](cid:96)10.912010.44059-0.4619871.6289.77-83.80(cid:96)20.052450.02538-0.0251571.5184.22-83.12(cid:96)∞0.001140.00055-0.0005549.2859.72-58.04ResultsonSmallNetworks.Figure2showsthecertiﬁedlowerboundfor(cid:96)2and(cid:96)∞distortionsfoundbydifferentalgorithmsonsmallnetworks whereReluplexisfeasibleandwecanobservethegapbetweendifferentcertiﬁedlowerboundsandthetrueminimumadversarialdistortion.ReluplexandLP-Fullareordersofmagnitudesslowerthanothermethods(notethelogarithmicscaleonrighty-axis) andCROWN-Quad(for2-layer)andCROWN-Adaachievethelargestlowerbounds.ImprovementsofCROWN-AdaoverFast-LinaremoresigniﬁcantinlargerNNs asweshowbelow.ResultsonLargeReLUNetworks.Table4demonstratesthelowerboundsfoundbydifferentalgorithmsforallcommon(cid:96)pnorms.CROWN-AdasigniﬁcantlyoutperformsFast-LinandFast-Lip whilethecomputationtimeincreasedbylessthan2XoverFast-Lin andiscomparablewithFast-Lip.SeeAppendixforresultsonmorenetworks.ResultsonDifferentActivations.Table7comparesthecertiﬁedlowerboundcomputedbyCROWN-generalforfouractivationfunctionsanddifferent(cid:96)pnormonlargenetworks.CROWN-generalisabletocertifynon-triviallowerboundsforallfouractivationfunctionsefﬁciently.ComparingtoCROWN-AdaonReLUnetworks certifyinggeneralactivationsthatarenotpiece-wiselinearonlyincursabout20%additionalcomputationaloverhead.5ConclusionWehavepresentedageneralframeworkCROWNtoefﬁcientlycomputeacertiﬁedlowerboundofminimumdistortioninneuralnetworksforanygivendatapointx0.CROWNfeaturesadaptiveboundsforimprovedrobustnesscertiﬁcationandappliestogeneralactivationfunctions.Moreover experimentsshowthat(1)CROWNoutperformsstate-of-the-artbaselinesonReLUnetworksand(2)CROWNcanefﬁcientlycertifynon-triviallowerboundsforlargenetworkswithover10Kneuronsandwithdifferentactivationfunctions.8AcknowledgementThisworkwassupportedinpartbyNSFIIS-1719097 Intelfacultyaward GoogleCloudCreditsforResearchProgramandGPUsdonatedbyNVIDIA.Tsui-WeiWengandLucaDanielarepartiallysupportedbyMIT-IBMWatsonAILabandMIT-Skoltechprogram.References[1]A.Fawzi S.-M.Moosavi-Dezfooli andP.Frossard “Therobustnessofdeepnetworks:Ageometricalperspective ”IEEESignalProcessingMagazine vol.34 no.6 pp.50–62 2017.[2]B.BiggioandF.Roli “Wildpatterns:Tenyearsaftertheriseofadversarialmachinelearning ”arXivpreprintarXiv:1712.03141 2017.[3]C.Szegedy W.Zaremba I.Sutskever J.Bruna D.Erhan I.Goodfellow andR.Fergus “Intriguingpropertiesofneuralnetworks ”arXivpreprintarXiv:1312.6199 2013.[4]I.J.Goodfellow J.Shlens andC.Szegedy “Explainingandharnessingadversarialexamples ”ICLR 2015.[5]S.-M.Moosavi-Dezfooli A.Fawzi andP.Frossard “Deepfool:asimpleandaccuratemethodtofooldeepneuralnetworks ”inIEEEConferenceonComputerVisionandPatternRecognition 2016 pp.2574–2582.[6]N.CarliniandD.Wagner “Towardsevaluatingtherobustnessofneuralnetworks ”inIEEESymposiumonSecurityandPrivacy(SP) 2017 pp.39–57.[7]H.Chen H.Zhang P.-Y.Chen J.Yi andC.-J.Hsieh “Show-and-fool:Craftingadversarialexamplesforneuralimagecaptioning ”arXivpreprintarXiv:1712.02051 2017.[8]N.Papernot P.McDaniel I.Goodfellow S.Jha Z.B.Celik andA.Swami “Practicalblack-boxattacksagainstmachinelearning ”inACMAsiaConferenceonComputerandCommunicationsSecurity 2017 pp.506–519.[9]Y.Liu X.Chen C.Liu andD.Song “Delvingintotransferableadversarialexamplesandblack-boxattacks ”ICLR 2017.[10]P.-Y.Chen H.Zhang Y.Sharma J.Yi andC.-J.Hsieh “ZOO:Zerothorderoptimizationbasedblack-boxattackstodeepneuralnetworkswithouttrainingsubstitutemodels ”inACMWorkshoponArtiﬁcialIntelligenceandSecurity 2017 pp.15–26.[11]W.Brendel J.Rauber andM.Bethge “Decision-basedadversarialattacks:Reliableattacksagainstblack-boxmachinelearningmodels ”ICLR 2018.[12]A.Kurakin I.Goodfellow andS.Bengio “Adversarialexamplesinthephysicalworld ”arXivpreprintarXiv:1607.02533 2016.[13]I.Evtimov K.Eykholt E.Fernandes T.Kohno B.Li A.Prakash A.Rahmati andD.Song “Robustphysical-worldattacksonmachinelearningmodels ”arXivpreprintarXiv:1707.08945 2017.[14]A.AthalyeandI.Sutskever “Synthesizingrobustadversarialexamples ”arXivpreprintarXiv:1707.07397 2017.[15]G.Katz C.Barrett D.L.Dill K.Julian andM.J.Kochenderfer “Reluplex:Anefﬁcientsmtsolverforverifyingdeepneuralnetworks ”inInternationalConferenceonComputerAidedVeriﬁcation.Springer 2017 pp.97–117.[16]A.Sinha H.Namkoong andJ.Duchi “Certiﬁabledistributionalrobustnesswithprincipledadversarialtraining ”ICLR 2018.[17]J.Peck J.Roels B.Goossens andY.Saeys “Lowerboundsontherobustnesstoadversarialperturbations ”inNIPS 2017.[18]J.Z.KolterandE.Wong “Provabledefensesagainstadversarialexamplesviatheconvexouteradversarialpolytope ”ICML 2018.[19]A.Raghunathan J.Steinhardt andP.Liang “Certiﬁeddefensesagainstadversarialexamples ”ICLR 2018.9[20]T.-W.Weng H.Zhang H.Chen Z.Song C.-J.Hsieh D.Boning I.S.Dhillon andL.Daniel “Towardsfastcomputationofcertiﬁedrobustnessforrelunetworks ”ICML 2018.[21]A.LomuscioandL.Maganti “Anapproachtoreachabilityanalysisforfeed-forwardreluneuralnetworks ”arXivpreprintarXiv:1706.07351 2017.[22]C.-H.Cheng G.Nührenberg andH.Ruess “Maximumresilienceofartiﬁcialneuralnetworks ”inInternationalSymposiumonAutomatedTechnologyforVeriﬁcationandAnalysis.Springer 2017 pp.251–268.[23]M.FischettiandJ.Jo “Deepneuralnetworksas0-1mixedintegerlinearprograms:Afeasibilitystudy ”arXivpreprintarXiv:1712.06174 2017.[24]N.Carlini G.Katz C.Barrett andD.L.Dill “Provablyminimally-distortedadversarialexamples ”arXivpreprintarXiv:1709.10207 2017.[25]R.Ehlers “Formalveriﬁcationofpiece-wiselinearfeed-forwardneuralnetworks ”inInterna-tionalSymposiumonAutomatedTechnologyforVeriﬁcationandAnalysis.Springer 2017 pp.269–286.[26]M.HeinandM.Andriushchenko “Formalguaranteesontherobustnessofaclassiﬁeragainstadversarialmanipulation ”inNIPS 2017.[27]T.-W.Weng H.Zhang P.-Y.Chen J.Yi D.Su Y.Gao C.-J.Hsieh andL.Daniel “Evaluatingtherobustnessofneuralnetworks:Anextremevaluetheoryapproach ”ICLR 2018.[28]T.Gehr M.Mirman D.Drachsler-Cohen P.Tsankov S.Chaudhuri andM.Vechev “Ai2:Safetyandrobustnesscertiﬁcationofneuralnetworkswithabstractinterpretation ”inIEEESymposiumonSecurityandPrivacy(SP) vol.00 2018 pp.948–963.[29]K.Dvijotham R.Stanforth S.Gowal T.Mann andP.Kohli “Adualapproachtoscalableveriﬁcationofdeepnetworks ”UAI 2018.[30]A.Madry A.Makelov L.Schmidt D.Tsipras andA.Vladu “Towardsdeeplearningmodelsresistanttoadversarialattacks ”ICLR 2018.[31]X.CaoandN.Z.Gong “Mitigatingevasionattackstodeepneuralnetworksviaregion-basedclassiﬁcation ”inACMAnnualComputerSecurityApplicationsConference 2017 pp.278–287.[32]P.-Y.Chen Y.Sharma H.Zhang J.Yi andC.-J.Hsieh “EAD:elastic-netattackstodeepneuralnetworksviaadversarialexamples ”AAAI 2018.10,Huan Zhang
Tsui-Wei Weng
Pin-Yu Chen
Cho-Jui Hsieh
Luca Daniel