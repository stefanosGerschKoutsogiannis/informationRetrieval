2019,Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks,Graph convolutional networks (GCNs) have recently received wide attentions  due to their successful applications in different graph tasks and different domains. Training GCNs for a large graph  however  is still a challenge. Original full-batch GCN training requires calculating the representation of all the nodes in the graph per GCN layer  which brings in high computation and memory costs. To alleviate this issue  several sampling-based methods are proposed to train GCNs on a subset of nodes. Among them  the node-wise neighbor-sampling method recursively samples a fixed number of neighbor nodes  and thus its computation cost suffers from exponential growing neighbor size across layers; while the layer-wise importance-sampling method discards the neighbor-dependent constraints  and thus the nodes sampled across layer suffer from sparse connection problem. To deal with the above two problems  we propose a new effective sampling algorithm called LAyer-Dependent ImportancE Sampling (LADIES). Based on the sampled nodes in the upper layer  LADIES selects nodes that are in the neighborhood of these nodes and uses the constructed bipartite graph to compute the importance probability. Then  it samples a fixed number of nodes according to the probability for the whole layer  and recursively conducts such procedure per layer to construct the whole computation graph. We prove theoretically and experimentally  that our proposed sampling algorithm outperforms the previous sampling methods regarding both time and memory. Furthermore  LADIES is shown to have better generalization accuracy than original full-batch GCN  due to its stochastic nature.,Layer-Dependent Importance Sampling for Training

Deep and Large Graph Convolutional Networks

Difan Zou∗  Ziniu Hu∗  Yewen Wang  Song Jiang  Yizhou Sun  Quanquan Gu

Department of Computer Science  UCLA  Los Angeles  CA 90095

{knowzou bull wyw10804 songjiang yzsun qgu}@cs.ucla.edu

Abstract

Graph convolutional networks (GCNs) have recently received wide attentions  due
to their successful applications in different graph tasks and different domains. Train-
ing GCNs for a large graph  however  is still a challenge. Original full-batch GCN
training requires calculating the representation of all the nodes in the graph per
GCN layer  which brings in high computation and memory costs. To alleviate this
issue  several sampling-based methods have been proposed to train GCNs on a sub-
set of nodes. Among them  the node-wise neighbor-sampling method recursively
samples a ﬁxed number of neighbor nodes  and thus its computation cost suffers
from exponential growing neighbor size; while the layer-wise importance-sampling
method discards the neighbor-dependent constraints  and thus the nodes sampled
across layer suffer from sparse connection problem. To deal with the above two
problems  we propose a new effective sampling algorithm called LAyer-Dependent
ImportancE Sampling (LADIES) 2. Based on the sampled nodes in the upper layer 
LADIES selects their neighborhood nodes  constructs a bipartite subgraph and
computes the importance probability accordingly. Then  it samples a ﬁxed number
of nodes by the calculated probability  and recursively conducts such procedure
per layer to construct the whole computation graph. We prove theoretically and
experimentally  that our proposed sampling algorithm outperforms the previous
sampling methods in terms of both time and memory costs. Furthermore  LADIES
is shown to have better generalization accuracy than original full-batch GCN  due
to its stochastic nature.

Introduction

1
Graph convolutional networks (GCNs) recently proposed by Kipf et al. [12] adopt the concept of
convolution ﬁlter into graph domain [2  6–8]. For a given node  a GCN layer aggregates the embed-
dings of its neighbors from the previous layer  followed by a non-linear transformation  to obtain an
updated contextualized node representation. Similar to the convolutional neural networks (CNNs)
[13] in the computer vision domain  by stacking multiple GCN layers  each node representation
can utilize a wide receptive ﬁeld from both its immediate and distant neighbors  which intuitively
increases the model capacity.
Despite the success of GCNs in many graph-related applications [12  17  15]  training a deep GCN
for large-scale graphs remains a big challenge. Unlike tokens in a paragraph or pixels in an image 
which normally have limited length or size  graph data in practice can be extremely large. For
example  Facebook social network in 2019 contains 2.7 billion users3. Such a large-scale graph is
impossible to be handled using full-batch GCN training  which takes all the nodes into one batch
to update parameters. However  conducting mini-batch GCN training is non-trivial  as the nodes

∗equal contribution
2codes are avaiable at https://github.com/acbull/LADIES
3https://zephoria.com/top-15-valuable-facebook-statistics/

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: An illustration of the sampling process of GraphSage  FastGCN  and our proposed LADIES.
Black nodes denote the nodes in the upper layer  blue nodes in the dashed circle are their neighbors 
and node with the red frame is the sampled nodes. As is shown in the ﬁgure  GraphSAGE will
redundantly sample a neighboring node twice  denoted by the red triangle  while FastGCN will
sample nodes outside of the neighborhood. Our proposed LADIES can avoid these two problems.

in a graph are closely coupled and correlated. In particular  in GCNs  the embedding of a given
node depends recursively on all its neighbors’ embeddings  and such computation dependency grows
exponentially with respect to the number of layers. Therefore  training deep and large GCNs is still a
challenge  which prevents their application to many large-scale graphs  such as social networks [12] 
recommender systems [17]  and knowledge graphs [15].
To alleviate the previous issues  researchers have proposed sampling-based methods to train GCNs
based on mini-batch of nodes  which only aggregate the embeddings of a sampled subset of neighbors
of each node in the mini-batch. Among them  one direction is to use a node-wise neighbor-sampling
method. For example  GraphSAGE [9] calculates each node embedding by leveraging only a ﬁxed
number of uniformly sampled neighbors. Although this kind of approaches reduces the computation
cost in each aggregation operation  the total cost can still be large. As is pointed out in [11]  the
recursive nature of node-wise sampling brings in redundancy for calculating embeddings. Even if two
nodes share the same sampled neighbor  the embedding of this neighbor has to be calculated twice.
Such redundant calculation will be exaggerated exponentially when the number of layers increases.
Following this line of research as well as reducing the computation redundancy  a series of work
was proposed to reduce the size of sampled neighbors. VR-GCN [3] proposes to leverage variance
reduction techniques to improve the sample complexity. Cluster-GCN [5] considers restricting the
sampled neighbors within some dense subgraphs  which are identiﬁed by a graph clustering algorithm
before the training of GCN. However  these methods still cannot well address the issue of redundant
computations  which may become worse when training very deep and large GCNs.
Another direction uses a layer-wise importance-sampling method. For example  FastGCN [4] calcu-
lates a sampling probability based on the degree of each node  and samples a ﬁxed number of nodes
for each layer accordingly. Then  it only uses the sampled nodes to build a much smaller sampled
adjacency matrix  and thus the computation cost is reduced. Ideally  the sampling probability is calcu-
lated to reduce the estimation variance in FastGCN [4]  and guarantee fast and stable convergence.
However  since the sampling probability is independent for each layer  the sampled nodes from two
consecutive layers are not necessarily connected. Therefore  the sampled adjacency matrix can be
extremely sparse  and may even have all-zero rows  meaning some nodes are disconnected. Such a
sparse connection problem incurs an inaccurate computation graph and further deteriorates the train-
ing and generalization performance of FastGCN. In order to capture the inter-layer correlation and
reduce the estimation variance  Huang et al. [11] proposed an adaptive and trainable sampling method
that conducts layer-wise sampling conditioned on the former layer  which has been demonstrated
to achieve higher accuracy than FastGCN. Yet the advantage of the importance sampling procedure
used in [11] in terms of time and memory costs is not fully justiﬁed theoretically or empirically.
Based on the pros and cons of the aforementioned sampling approaches  we argue that an ideal
sampling method should have the following features: 1) layer-wise  thus the neighbor nodes can be
taken into account together to calculate next layers’ embeddings without redundancy; 2) neighbor-
dependent  thus the sampled adjacency matrix is dense without losing much information for training;
3) the importance sampling method should be adopted to reduce the sampling variance and accelerate

2

Node-wiseNeighbor Sampling(GraphSAGE)Layer-wiseImportance Sampling(FastGCN)Layer-Dependent Importance Sampling(LADIES)convergence. To this end  we propose a new efﬁcient sampling algorithm called LAyer-Dependent
ImportancE-Sampling (LADIES) 4  which fulﬁlls all the above features.
The procedure of LADIES is described as below: (1) For each current layer (l)  based on the nodes
sampled in the upper layer (l + 1)  it picks all their neighbors in the graph  and constructs a bipartite
graph among the nodes between the two layers. (2) Then it calculates the sampling probability
according to the degree of nodes in the current layer  with the purpose to reduce sampling variance.
(3) Next  it samples a ﬁxed number of nodes based on this probability. (4) Finally  it constructs
the sampled adjacency matrix between layers and conducts training and inference  where row-wise
normalization is applied to all sampled adjacency matrices to stabilize training. As illustrated in
Figure 1  our proposed sampling strategy can avoid two pitfalls faced by existing two sampling
strategies: layer-wise structure avoids exponential expansion of receptive ﬁeld; layer-dependent
importance sampling guarantees the sampled adjacency matrix to be dense such that the connectivity
between nodes in two adjacent layers can be well maintained.
We highlight our contributions as follows:
• We propose LAyer-Dependent ImportancE Sampling (LADIES) for training deep and large graph
convolutional networks  which is built upon a novel layer-dependent sampling scheme to avoid
exponential expansion of receptive ﬁeld as well as guarantee the connectivity of the sampled
adjacency matrix.
• We prove theoretically that the proposed algorithm achieves signiﬁcantly better memory and time
complexities compared with node-wise sampling methods including GraphSage [9] and VR-GCN
[3]  and has a dramatically smaller variance compared with FastGCN [4].
• We evaluate the performance of the proposed LADIES algorithm on benchmark datasets and
demonstrate its superior performance in terms of both running time and test accuracy. Moreover 
we show that LADIES achieves high accuracy with an extremely small sample size (e.g.  256 for a
graph with 233k nodes)  which enables the use of LADIES for training very deep and large GCNs.

2 Background
In this section  we review GCNs and several state-of-the-art sampling-based training algorithms 
including full-batch  node-wise neighbor sampling methods  and layer-wise importance sampling
methods. We summarize the notation used in this paper in Table 1.
2.1 Existing GCN Training Algorithms
Full-batch GCN. When given an undirected graph G  with P deﬁned in Table 1  the l-th GCN layer
is deﬁned as:

Z(l) = PH(l−1)W(l−1)  H(l−1) = σ(Z(l−1)) 

(1)
Considering a node-level downstream task  given training dataset {(xi  yi)}vi∈Vs  the weight matrices
W(l) will be learned by minimizing the loss function: L = 1|Vs|
)  where (cid:96)(· ·) is a
i denotes the output of GCN corresponding to the vertex vi  and VS denotes
speciﬁed loss function  zL
the training node set. Gradient descent algorithm is utilized for the full-batch optimization  where the
gradient is computed over all the nodes as

∇(cid:96)(yi  z(L)

(cid:96)(yi  z(L)

(cid:80)

(cid:80)

vi∈Vs

i

1|Vs|

vi∈VS

).

i

When the graph is large and dense  full-batch GCN’s memory and time costs can be very expensive
while computing such gradient  since during both backward and forward propagation process it
requires to store and aggregate embeddings for all nodes across all layer. Also  since each epoch only
updates parameters once  the convergence would be very slow.
One solution to address issues of full-batch training is to sample a a mini-batch of labeled nodes
VB ∈ VS  and compute the mini-batch stochastic gradient
i ). This can help
reduce computation cost to some extent  but to calculate the representation of each output node  we
still need to consider a large-receptive ﬁeld  due to the dependency of the nodes in the graph.
Node-wise Neighbor Sampling Algorithms. GraphSAGE [9] proposed to reduce receptive ﬁeld
size by neighbor sampling (NS). For each node in l-th GCN layer  NS randomly samples snode
4We would like to clarify that the proposed layer-dependent importance sampling is different from “layered

∇(cid:96)(yi  zL

(cid:80)

vi∈VB

1|VB|

importance sampling” proposed in [14].

3

G = (V  A)  (cid:107)A(cid:107)0

Mi ∗  M∗ j Mi j

˜A  ˜D  P

l  σ(·)  H(l)  Z(l)  W(l)

L  K

b  snode  slayer

Table 1: Summary of Notations

2 ˜A ˜D− 1

G denotes a graph consist of a set of nodes V with node number |V|  A is the
adjacency matrix  and (cid:107)A(cid:107)0 denotes the number of non-zero entries in A.
Mi ∗ is the i-th row of matrix M  M∗ j is the j-th column of matrix M  and Mi j
is the entry at the position (i  j) of matrix M.

˜A = A + I  ˜D is a diagonal matrix satisfying ˜Di i =(cid:80)

2 is the normalized Laplacian matrix.

P = ˜D− 1
l denotes the GCN layer index  σ(·) denotes the activation function  H(l) =
σ(Z(l)) denotes the embedding matrix at layer l  H(0) = X  Z(l) =
PH(l−1)W(l−1) is the intermediate embedding matrix  and W(l) denotes the
trainable weight matrix at layer l.
L is the total number of layers in GCN  and K is the dimension of embedding
vectors (for simplicity  assume it is the same across all layers).
For batch-wise sampling  b denotes the batch size  snode is the number of
sampled neighbors per node for node-wise sampling  and slayer is number of
sampled nodes per layer for layer-wise sampling.

˜Ai j  and

j

0

Variance

F /slayer)
F

Table 2: Summary of Complexity and Variance 6. Here φ denotes the upper bound of the (cid:96)2 norm
of embedding vector  ∆φ denotes the bound of the norm of the difference between the embedding
and its history  D denotes the average degree  b denotes the batch size  and ¯V (b) denotes the average
number of nodes which are connected to the nodes sampled in the training batch.
Time Complexity
Methods
Memory Complexity
O(L(cid:107)A(cid:107)0K + L|V|K 2)
Full-Batch [12] O(L|V|K + LK 2)
O(bKsL−1
O(bKsL
node + bK 2sL−1
GraphSage [9]
node + LK 2)
node)
O(L|V|K + LK 2)
O(bDKsL−1
node + bK 2sL−1
VR-GCN [3]
O(LKslayer + LK 2) O(LKs2
layer + LK 2slayer) O(φ(cid:107)P(cid:107)2
FastGCN [4]
O(LKslayer + LK 2) O(LKs2
LADIES
of its neighbors at the (l − 1)-th GCN layer and formulate an unbiased estimator ˆP(l−1)H(l−1) to
approximate PH(l−1) in graph convolution layer:
Pi j 

O(cid:0)Dφ(cid:107)P(cid:107)2
node) O(cid:0)D∆φ(cid:107)P(cid:107)2
layer + LK 2slayer) O(cid:0)φ(cid:107)P(cid:107)2

F /(|V|snode)(cid:1)
F /(|V|snode)(cid:1)
¯V (b)/(|V|slayer)(cid:1)

(2)
where N (vi) and ˆN (l−1)(vi) are the full and sampled neighbor sets of node vi for (l − 1)-th GCN
layer respectively.
VR-GCN [3] is another neighbor sampling work. It proposed to utilize historical activations to reduce
the variance of the estimator under the same sample strategy as GraphSAGE. Though successfully
achieved comparable convergence rate  the memory complexity is higher and the time complexity
remains the same. Though NS scheme alleviates the memory bottleneck of GCN  there exists
redundant computation under NS since the embedding calculation for each node in the same layer is
independent  thus the complexity grows exponentially when the number of layers increases.
Layer-wise Importance Sampling Algorithms. FastGCN [4] proposed a more advanced layer-wise
importance sampling (IS) scheme aiming to solve the scalability issue as well as reducing the variance.
IS conducts sampling for each layer with a degree-based sampling probability. The approximation of
the i-th row of (PH(l−1)) with slayer samples vj1  . . .   vjsl

vj ∈ ˆN (l−1)(vi);
otherwise.

(cid:26) |N (vi)|

ˆP(l−1)
i j =

snode
0 

(PH(l−1))i ∗ (cid:39) 1
slayer

per layer can be estimated as:
Pi jk H(l−1)

jk ∗ /q(vjk )

(3)

slayer(cid:88)
2/(cid:107)P(cid:107)2

k=1 vjk∼q(v)

where q(v) is the distribution over V  q(vj) = (cid:107)P∗ j(cid:107)2
F is the probability assigned to node vj.
Though IS outperforms uniform sampling and the layer-wise sampling successfully reduced both time
and memory complexities  however  this sampling strategy has a major limitation: since the sampling
operation is conducted independently at each layer  FastGCN cannot guarantee connectivity between
sampled nodes at different layers  which incurs large variance of the approximate embeddings.

6For simplicity  when evaluating the variance we only consider two-layer GCN.

4

2.2 Complexity and Variance Comparison
We compare each method’s memory  time complexity  and variance with that of our proposed LADIES
algorithm in Table 2.
Complexity. We now compare the complexity of the proposed LADIES algorithm and existing
sampling-based GCN training algorithms. The complexity for all methods are summarized in Table
2  detailed calculation could be found in Appendix A. Compare with full-batch GCN  the time and
memory complexities of LADIES do not depend on the total number of nodes and edges  thus our
algorithm does not have scalability issue on large and dense graphs. Unlike NS based methods
including GraphSAGE and VR-GCN  LADIES is not sensitive to the number of layers and will not
suffer exponential growth in complexity  therefore it can perform well when the neural network goes
deeper. Compared to layer-wise importance sampling proposed in FastGCN  it maintains the same
complexity while obtaining a better convergence guarantee as analyzed in the next paragraph. In fact 
in order to guarantee good performance  our method requires a much smaller sample size than that
of FastGCN  thus the time and memory burden is much lighter. Therefore  our proposed LADIES
algorithm can achieve the best time and memory complexities and is applicable to training very deep
and large graph convolution networks.
Variance. We compare the variance of our algorithm with existing sampling-based algorithms. To
simplify the analysis  when evaluating the variance we only consider two-layer GCN. The results are
summarized in Table 2. We defer the detailed calculation to Appendix B. Compared with FastGCN 
our variance result is strictly better since ¯V (b) ≤ |V|  where ¯V (b) denotes the number of nodes
which are connected to the nodes sampled in the training batch. Moreover  ¯V (b) scales with b  which
implies that our method can be even better when using a small batch size. Compared with node-wise
sampling  consider the same sample size  i.e.  slayer = bsnode. Ignoring the same factors  the
variance of LADIES is in the order of O( ¯V (b)/b) and the variance of GraphSAGE is O(D)  where
D denotes the average degree. Based on the deﬁnition of ¯V (b)  we strictly have ¯V (b) ≤ O(bD) since
there is no redundant node been calculated in ¯V (b). Therefore our method is also strictly better than
GraphSAGE especially when the graph is dense  i.e.  many neighbors can be shared. The variance of
VR-GCN resembles that of GraphSAGE but relies on the difference between the embedding and its
history  which is not directly comparable to our results.
3 LADIES: LAyer-Dependent ImportancE Sampling
We present our method  LADIES  in this section. As illustrated in previous sections  for node-wise
sampling methods [9  3]  one has to sample a certain number of nodes in the neighbor set of all
sampled nodes in the current layer  then the number of nodes that are selected to build the computation
graph is exponentially large with the number of hidden layers  which further slows down the training
process of GCNs. For the existing layer-wise sampling scheme [4]  it is inefﬁcient when the graph is
sparse  since some nodes may have no neighbor been sampled during the sampling process  which
results in meaningless zero activations [3].
To address the aforementioned drawbacks and weaknesses of existing sampling-based methods for
GCN training  we propose our training algorithms that can achieve good convergence performance as
well as reduce sampling complexity. In the following  we are going to illustrate our method in detail.
3.1 Revisiting Independent Layer-wise Sampling
We ﬁrst revisit the independent layer-wise sampling scheme for building the computation graph
of GCN training. Recall in the forward process of GCN (1)  the matrix production PH(l−1) can
be regarded as the embedding aggregation process. Then  the layer-wise sampling methods aim
to approximate the intermediate embedding matrix Z(l) by only sampling a subset of nodes at the
(l − 1)-th layer and aggregating their embeddings for approximately estimating the embeddings at the
l-th layer. Mathematically  similar to (3)  let S(l−1) with |Sl−1| = sl−1 be the set of sampled nodes
at the (l − 1)-th layer  we can approximate PH(l−1) as

(cid:88)

PH(l−1) (cid:39) 1
sl−1

P∗ kH(l−1)
k ∗

 

1

p(l−1)

k

k∈S (l−1)

where we adopt non-uniformly sampling scheme by assigning probabilities p(l−1)
all nodes in V. Then the corresponding discount weights are {1/(sl−1p(l−1)

1

  . . .   p(l−1)
|V|

to
)}i=1 ... |V|. Then let

i

5

{i(l−1)
be formulated as

k

}k∈Sl−1 be the indices of sampled nodes at the l − 1-th layer  the estimator of PH(l−1) can

where S(l−1) ∈ R|V|×|V| is a diagonal matrix with only nonzero diagnoal matrix  deﬁned by

(4)

(5)

PH(l−1) (cid:39) PS(l−1)H(l−1) 

S(l−1)
s s =

 

1

sl−1p(l−1)
(l−1)
i
k
0 

s = i(l−1)

k

;

otherwise.



(cid:26)

It can be veriﬁed that (cid:107)S(l−1)(cid:107)0 = sl−1 and E[S(l−1)] = I. Assuming at the l-th and l − 1-th layers
the sets of sampled nodes are determined. Then let {i(l)
}k∈Sl−1 denote the indices
of sampled nodes at these two layers  and deﬁne the row selection matrix Q(l) ∈ Rsl×|V| as:

k }k∈Sl and {i(l−1)

k

Q(l)

k s =

(k  s) = (k  i(l)

1
0  otherwise 

k );

(6)

the forward process of GCN with layer-wise sampling can be approximated by
˜H(l−1) = σ( ˜Z(l−1)) 

˜Z(l) = ˜P(l−1) ˜H(l−1)W(l−1) 

(7)
where ˜Z(l) ∈ Rsl×d denotes the approximated intermediate embeddings for sampled nodes at the l-th
layer and ˜P(l−1) = Q(l)PS(l−1)Q(l−1)(cid:62) ∈ Rsl×sl−1 serves as a modiﬁed Laplacian matrix  and
can be also regarded as a sampled bipartite graph after certain rescaling. Since typically we have
sl  sl−1 (cid:28) |V|  the sampled graph is dramatically smaller than the entire one  thus the computation
cost can be signiﬁcantly reduced.
3.2 Layer-dependent Importance Sampling
However  independently conducting layer-wise sampling at different layers is not efﬁcient since the
sampled bipartite graph may still be sparse and even have all-zero rows. This further results in very
poor performance and require us to sample lots of nodes in order to guarantee convergence throughout
the GCN training. To alleviate this issue  we propose to apply neighbor-dependent sampling that can
leverage the dependency between layers which further leads to dense computation graph. Speciﬁcally 
our layer-dependent sampling mechanism is designed in a top-down manner  i.e.  the sampled nodes
at the l-th layer are generated depending on the sampled nodes that have been generated in all upper
layers. Note that for each node we only need to aggregate the embeddings from its neighbor nodes in
the previous layer. Thus  at one particular layer  we only need to generate samples from the union of
neighbors of the nodes we have sampled in the upper layer  which is deﬁned by

V (l−1) = ∪vi∈SlN (vi)

where Sl is the set of nodes we have sampled at the l-th layer and N (vi) denotes the neighbors
set of node vi. Therefore during the sampling process  we only assign probability to nodes in
V (l−1)  denoted by {p(l−1)
}vi∈V (l−1). Similar to FastGCN [4]  we apply importance sampling to
reduce the variance. However  we have no information regarding the activation matrix H(l−1) when
characterizing the samples at the (l−1)-th layer. Therefore  we resort to a important sampling scheme
which only relies on the matrices Q(l) and P. Speciﬁcally  we deﬁne the importance probabilities as:

i

p(l−1)

i

=

.

2

(cid:107)Q(l)P∗ i(cid:107)2
(cid:107)Q(l)P(cid:107)2
2 = 0  which implies that p(l−1)

F

(8)

k

Evidently  if vi /∈ V (l−1)  we have (cid:107)Q(l)P∗ i(cid:107)2
= 0. Then  let
{i(l−1)
}vk∈Sl−1 be the indices of sampled nodes at the (l − 1)-th layer based on the importance
probabilities computed in (8)  we can also deﬁne the random diagonal matrix S(l−1) according to (5) 
and formulate the same forward process of GCN as in (7) but with a different modiﬁed Laplacian
matrix ˜P(l−1) = Q(l)PS(l−1)Q(l−1)(cid:62) ∈ Rsl×sl−1. The computation of ˜P(l−1) can be very efﬁcient
since it only involves sparse matrix productions. Here the major difference between our sampling

i

6

Algorithm 1 Sampling Procedure of LADIES
Require: Normalized Laplacian Matrix P; Batch Size b  Sample Number n;
1: Randomly sample a batch of b output nodes as QL
2: for l = L to 1 do
3:

4:
5:

i ← (cid:107)Q(l)P∗ i(cid:107)2
(cid:107)Q(l)P(cid:107)2

2

F

Get layer-dependent laplacian matrix Q(l)P. Calculate sampling probability for each node
using p(l−1)
Sample n nodes in l − 1 layer using p(l−1). The sampled nodes formulate Q(l−1)
Reconstruct sampled laplacian matrix between sampled nodes in layer l − 1 and l by
˜P(l−1) ← Q(l)PS(l−1)Q(l−1)(cid:62)  then normalize it by ˜P(l) ← D−1
˜P(l)

  and organize them into a random diagonal matrix S(l−1).

˜P(l).

6: end for
7: return Modiﬁed Laplacian Matrices { ˜P(l)}l=1 ... L and Sampled Node at Input Layer Q0;

method and independent layer-wise sampling is the different constructions of matrix S(l−1). In our
sampling mechanism  we have E[S(l−1)] = L(l−1)  where L(l−1) is a diagonal matrix with

(cid:26) 1

L(l−1)
s s =

s ∈ V (l−1)
0  otherwise.

(9)
Since (cid:107)L(l−1)(cid:107)0 = |V (l−1)| (cid:28) |V|  applying independent sampling method results in the fact that
many nodes are in the set V/V (l−1)  which has no contribution to the construction of computation
graph. In contrast  we only sample nodes from V (l−1) which can guarantee more connections between
the sampled nodes at l-th and (l−1)-th layers  and further leads to a dense computation graph between
these two layers.
3.3 Normalization
Note that for original GCN  the Laplacian matrix P is obtained by normalizing the matrix I + A.
Such normalization operation is crucial since it is able to maintain the scale of embeddings in the
forward process and avoid exploding/vanishing gradient. However  the modiﬁed Laplacian matrix
{ ˜P(l)}l=1 ... L may not be able to achieve this  especially when L is large  because its maximum
singular values can be very large without sufﬁcient samples. Therefore  motivated by [12]  we
propose to normalize ˜P(l) such that the sum of all rows are 1  i.e.  we have

˜P(l) ← D−1
˜P(l)

˜P(l) 

where D ˜P(l) ∈ Rsl+1×sl+1 is a diagonal matrix with each diagonal entry to be the sum of the
corresponding row in ˜P(l). Now  we can leverage the modiﬁed Laplacian matrices { ˜P}l=1 ... L to
build the whole computation graph. We formally summarize the proposed algorithm in Algorithm 1.
4 Experiments
In this section  we conduct experiments to evaluate LADIES for training deep GCNs on different
node classiﬁcation datasets  including Cora  Citeseer  Pubmed [16] and Reddit [9].
4.1 Experiment Settings
We compare LADIES with the original GCN (full-batch)   GraphSage (neighborhood sampling) and
FastGCN (important sampling). We modiﬁed the PyTorch implementation of GCN 7 to add our
LADIES sampling mechanism. To make the fair comparison only on the sampling part  we also
choose the online PyTorch implementation of all these baselines released by their authors and use
the same training code for all the methods. By default  we train 5-layer GCNs with hidden state
dimension as 256  using the four methods. We choose 5 neighbors to be sampled for GraphSage  64
and 512 nodes to be sampled for both FastGCN and LADIES per layer. We update the model with a
batch size of 512 and ADAM optimizer with a learning rate of 0.001.
For all the methods and datasets  we conduct training for 10 times and take the mean and variance of
the evaluation results. Each time we stop training when the validation accuracy doesn’t increase a
threshold (0.01) for 200 batches  and choose the model with the highest validation accuracy as the
convergence point. We use the following metrics to evaluate the effectiveness of sampling methods:

7https://github.com/tkipf/pygcn

7

Table 3: Comparison of LADIES with original GCN (Full-Batch)  GraphSage (Neighborhood
Sampling) and FastGCN (Important Sampling)  in terms of accuracy  time  memory and convergence.
Training 5-layer GCNs on different node classiﬁcation datasets (node number is below the dataset
name). Results show that LADIES can achieve the best accuracy with lower time and memory cost.

Full-Batch

Full-Batch

Full-Batch

Cora
(2708)

Citeseer
(3327)

Pubmed
(19717)

Reddit
(232965)

5.89
13.97
23.24
5.89
13.92

137.93
453.58

1.92
4.53
49.41
1.92
4.39

Dataset

Sample Method

30.72
471.39
3.13
7.33
3.13
7.35

68.13
595.71

Full-Batch

GraphSage (5)
FastGCN (64)
FastGCN (512)
LADIES (64)
LADIES (512)

GraphSage (5)
FastGCN (64)
FastGCN (512)
FastGCN (1024)

LADIES (64)
LADIES (512)

GraphSage (5)
FastGCN (64)
FastGCN (512)
FastGCN (8192)

LADIES (64)
LADIES (512)

GraphSage (5)
FastGCN (64)
FastGCN (512)
FastGCN (8192)

LADIES (64)
LADIES (512)

F1-Score(%)
76.5 ± 1.4
75.2 ± 1.5
25.1 ± 8.4
78.0 ± 2.1
77.6 ±1.4
78.3 ±1.6
62.3 ± 3.1
59.4 ± 0.9
19.2 ± 2.7
44.6 ± 10.8
63.5 ± 1.8
65.0 ± .1.4
64.3 ± 2.4
71.9 ± 1.9
70.1 ± 1.4
38.5 ± 6.9
39.3 ± 9.2
74.4 ± 0.8
76.8 ± 0.8
75.9 ± 1.1
91.6 ± 1.6
92.1 ± 1.1
27.8 ± 12.6
17.5 ± 16.7
89.5 ± 1.2
83.5 ± 0.9
92.8 ± 1.6

Total Time(s) Mem(MB) Batch Time(ms)
15.75 ± 0.52
1.19 ± 0.82
6.77 ± 4.94
78.42 ± 0.87
0.55 ± 0.65
9.22 ± 0.20
4.70 ± 1.35
10.08 ± 0.29
9.68 ± 0.48
4.19 ± 1.16
0.72 ± 0.39
9.77 ± 0.28
15.77 ± 0.58
0.61 ± 0.70
4.51 ± 3.68
53.14 ± 1.90
0.53 ± 0.48
8.88 ± 0.40
10.41 ± 0.51
4.34 ± 1.73
10.54 ± 0.27
2.24 ± 1.01
2.17 ± 0.65
9.60 ± 0.39
0.41 ± 0.22
10.32 ± 0.23
44.69 ± 0.57
4.80 ± 1.53
44.73 ± 0.30
5.53 ± 2.57
0.40 ± 0.69
7.42 ± 0.16
0.44 ± 0.61
10.06 ± 0.41
3.47 ± 1.16
17.84 ± 0.33
2.57 ± 0.72
9.43 ± 0.47
10.43 ± 0.36
2.27 ± 1.17
1564 ± 3.41
474.3 ± 84.4
13.12 ± 2.84
121.47 ± 0.72
2.06 ± 1.29
7.85 ± 0.72
10.01 ± 0.31
0.31 ± 0.41
16.57 ± 0.58
5.63 ± 2.12
5.62 ± 1.58
9.42 ± 0.48
6.87 ± 1.17
10.87 ± 0.63

Batch Num
80.8 ± 51.7
65.2 ± 52.1
63.2 ± 71.2
487 ± 147
436 ± 118.4
75.6 ± 37.0
40.6 ± 22.8
57.2 ± 42.1
64.0 ± 57.0
386 ± 167
223 ± 98.6
232 ± 66.8
37.6 ± 11.9
102 ± 33.4
74.8 ± 31.7
58.8 ± 94.8
44.8 ± 55.0
195 ± 56.9
277 ± 82.2
245 ± 84.5
179 ± 75.5
81.5 ± 42.3
57.4 ± 43.7
32.1 ± 72.3
278 ± 51.2
453 ± 88.2
393 ± 74.4
• Accuracy: The micro F1-score of the test data at the convergence point. We calculate it using the
• Total Running Time (s): The total training time (exclude validation) before convergence point.
• Memory (MB): Total memory costs of model parameters and all hidden representations of a batch.
• Batch Time and Num: Time cost to run a batch and the total batch number before convergence.
4.2 Experiment Results
As is shown in Table 4  our proposed LADIES can achieve the highest accuracy score among all the
methods  using a small sampling number. One surprising thing is that the sampling-based method
can achieve higher accuracy than the Full-Batch version  and in some cases using a smaller sampling
number can lead to better accuracy (though it may take longer batches to converge). This is probably
because the graph data is incomplete and noisy  and the stochastic nature of the sampling method
can bring in regularization for training a more robust GCN with better generalization accuracy [10].
Another observation is that no matter the size of the graph  LADIES with a small sample number (64)
can still converge well  and sometimes even better than a larger sample number (512). This indicates
that LADIES is scalable to training very large and deep GCN while maintaining high accuracy.
As a comparison  FastGCN with 64 and 512 sampled nodes can lead to similar accuracy for small
graphs (Cora). But for bigger graphs as Citeseer  Pubmed  and Reddit  it cannot converge to a good
point  partly because of the computation graph sparsity issue. For a fair comparison  we choose a
higher sampling number for FastGCN on these big graphs. For example  in Reddit  we choose 8192
nodes to be sampled  and FastGCN in such cases can converge to a similar accurate result compared
with LADIES  but obviously taking more memory and time cost. GraphSage with 5 nodes to be
sampled takes far more memory and time cost because of the redundancy problem we’ve discussed 
and its uniform sampling makes it fail to converge well and fast compared to importance sampling.

full-batch version to get the most accurate inference (only care about training).

2370.48
1234.63

3.75
6.91
74.28
3.75
7.26

8

Figure 2: F1-score  total time and memory cost at convergence point for training PubMed  when we
choose different sampling numbers of our method. Results show that LADIES can achieve good
generalization accuracy (F1-score = 77.6) even with a small sampling number as 16  while FastGCN
cannot converge (only reach F1-score = 39.3) with a large sampling number as 512.

(a) Training (60 data samples)

(b) Validation (500 data samples)

(c) Testing (1000 data samples)

Figure 3: Experiments on the PubMed dataset. We plot the F1-score of both full-batch GCN and
LADIES every epoch on (a) Training dataset (b) Validation dataset and (c) Testing dataset.
In addition to the above comparison  we show that our proposed LADIES can converge pretty well
with a much fewer sampling number. As is shown in Figure 2  when we choose the sampling number
as small as 16  the algorithm can already converge to the best result  with low time and memory cost.
This implies that although in Table 4  we choose sample number as 64 and 512 for a fair comparison 
but actually  the performance of our method can be further enhanced with a smaller sampling number.
Furthermore  we show that the stochastic nature of LADIES can help to achieve better generalization
accuracy than original full-batch GCN. We plot the F1-score of both full-batch GCN and LADIES
on the PubMed dataset for 300 epochs without early stop in Figure 3. From Figure 3(a)  we can see
that  full-batch GCN can achieve higher F1-Score than LADIES on the training set. Nevertheless 
on the validation and test datasets  we can see from Figures 3(b) and 3(c) that LADIES can achieve
signiﬁcantly higher F1-Score than full-batch GCN. This suggests that LADIES has better generaliza-
tion performance than full-batch GCN. The reason is: real graphs are often noisy and incomplete.
Full-batch GCN uses the entire graph in the training phase  which can cause overﬁtting to the noise.
In sharp contrast  LADIES employs stochastic sampling to use partial information of the graph and
therefore can mitigate the noise of the graph and avoid overﬁtting to the training data. At a high-level 
the sampling scheme adopted in LADIES shares a similar spirit as bagging and bootstrap [1]  which
is known to improve the generalization performance of machine learning predictors.

5 Conclusions
We propose a new algorithm namely LADIES for training deep and large GCNs. The crucial
ingredient of our algorithm is layer-dependent importance sampling  which can both ensure dense
computation graph as well as avoid drastic expansion of receptive ﬁeld. Theoretically  we show that
LADIES enjoys signiﬁcantly lower memory cost  time complexity and estimation variance  compared
with existing GCN training methods including GraphSAGE and FastGCN. Experimental results
demonstrate that LADIES can achieve the best test accuracy with much lower computational time
and memory cost on benchmark datasets.

Acknowledgement
We would like to thank the anonymous reviewers for their helpful comments. D. Zou and Q. Gu
were partially supported by the NSF BIGDATA IIS-1855099  NSF CAREER Award IIS-1906169
and Salesforce Deep Learning Research Award. Z. Hu  Y. Wang  S. Jiang and Y. Sun were partially
supported by NSF III-1705169  NSF 1937599  NSF CAREER Award 1741634  and Amazon Research
Award. We also thank AWS for providing cloud computing credits associated with the NSF BIGDATA
award. The views and conclusions contained in this paper are those of the authors and should not be
interpreted as representing any funding agencies.

9

050100150200250300Epoch0.30.40.50.60.70.80.91.0F1-ScoreFull BatchLADIES (64)050100150200250300Epoch0.20.30.40.50.60.70.8F1-ScoreFull BatchLADIES (64)050100150200250300Epoch0.20.30.40.50.60.7F1-ScoreFull BatchLADIES (64)References
[1] Leo Breiman. Bagging predictors. Machine Learning  24(2):123–140  1996.

[2] Michael M Bronstein  Joan Bruna  Yann LeCun  Arthur Szlam  and Pierre Vandergheynst.
Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine 
34(4):18–42  2017.

[3] Jianfei Chen  Jun Zhu  and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In Proceedings of the 35th International Conference on Machine Learning 
ICML 2018  Stockholmsmässan  Stockholm  Sweden  July 10-15  2018  pages 941–949  2018.

[4] Jie Chen  Tengfei Ma  and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks
via importance sampling. In 6th International Conference on Learning Representations  ICLR
2018  Vancouver  BC  Canada  April 30 - May 3  2018  Conference Track Proceedings  2018.

[5] Wei-Lin Chiang  Xuanqing Liu  Si Si  Yang Li  Samy Bengio  and Cho-Jui Hsieh. Cluster-gcn:
An efﬁcient algorithm for training deep and large graph convolutional networks. In Proceedings
of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining 
KDD 2019  Anchorage  AK  USA  August 4-8  2019.  pages 257–266  2019.

[6] Hanjun Dai  Bo Dai  and Le Song. Discriminative embeddings of latent variable models for
structured data. In Proceedings of the 33nd International Conference on Machine Learning 
ICML 2016  New York City  NY  USA  June 19-24  2016  pages 2702–2711  2016.

[7] Michaël Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral ﬁltering. In Advances in Neural Information Processing
Systems 29: Annual Conference on Neural Information Processing Systems 2016  December
5-10  2016  Barcelona  Spain  pages 3837–3845  2016.

[8] William L. Hamilton  Rex Ying  and Jure Leskovec. Representation learning on graphs: Methods

and applications. IEEE Data Eng.  40(3):52–74  2017.

[9] William L. Hamilton  Zhitao Ying  and Jure Leskovec. Inductive representation learning on
large graphs. In Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017  4-9 December 2017  Long Beach  CA  USA 
pages 1025–1035  2017.

[10] Moritz Hardt  Ben Recht  and Yoram Singer. Train faster  generalize better: Stability of
stochastic gradient descent. In Proceedings of the 33nd International Conference on Machine
Learning  ICML 2016  New York City  NY  USA  June 19-24  2016  pages 1225–1234  2016.

[11] Wen-bing Huang  Tong Zhang  Yu Rong  and Junzhou Huang. Adaptive sampling towards
fast graph representation learning. In Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018  NeurIPS 2018  3-8
December 2018  Montréal  Canada.  pages 4563–4572  2018.

[12] Thomas N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. In 5th International Conference on Learning Representations (ICLR-17).

[13] Yann LeCun  Yoshua Bengio  et al. Convolutional networks for images  speech  and time series.

The handbook of brain theory and neural networks  3361(10):1995  1995.

[14] Luca Martino  Victor Elvira  David Luengo  and Jukka Corander. Layered adaptive importance

sampling. Statistics and Computing  27(3):599–623  2017.

[15] Michael Sejr Schlichtkrull  Thomas N. Kipf  Peter Bloem  Rianne van den Berg  Ivan Titov  and
Max Welling. Modeling relational data with graph convolutional networks. In The Semantic
Web - 15th International Conference  ESWC 2018  Heraklion  Crete  Greece  June 3-7  2018 
Proceedings  pages 593–607  2018.

[16] Prithviraj Sen  Galileo Namata  Mustafa Bilgic  Lise Getoor  Brian Galligher  and Tina Eliassi-

Rad. Collective classiﬁcation in network data. AI magazine  29(3):93–93  2008.

10

[17] Rex Ying  Ruining He  Kaifeng Chen  Pong Eksombatchai  William L. Hamilton  and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems.
In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining  KDD 2018  London  UK  August 19-23  2018  pages 974–983  2018.

11

,Difan Zou
Ziniu Hu
Yewen Wang
Song Jiang
Yizhou Sun
Quanquan Gu