2012,Sketch-Based Linear Value Function Approximation,Hashing is a common method to reduce large  potentially infinite feature vectors to a fixed-size table. In reinforcement learning  hashing is often used in conjunction with tile coding to represent states in continuous spaces. Hashing is also a promising approach to value function approximation in large discrete domains such as Go and Hearts  where feature vectors can be constructed by exhaustively combining a set of atomic features. Unfortunately  the typical use of hashing in value function approximation results in biased value estimates due to the possibility of collisions. Recent work in data stream summaries has led to the development of the tug-of-war sketch  an unbiased estimator for approximating inner products. Our work investigates the application of this new data structure to linear value function approximation. Although in the reinforcement learning setting the use of the tug-of-war sketch leads to biased value estimates  we show that this bias can be orders of magnitude less than that of standard hashing. We provide empirical results on two RL benchmark domains and fifty-five Atari 2600 games to highlight the superior learning performance of tug-of-war hashing.,Sketch-Based Linear Value Function Approximation

Marc G. Bellemare
University of Alberta

Joel Veness

University of Alberta

Michael Bowling

University of Alberta

mg17@cs.ualberta.ca

veness@cs.ualberta.ca

bowling@cs.ualberta.ca

Abstract

Hashing is a common method to reduce large  potentially inﬁnite feature vectors
to a ﬁxed-size table. In reinforcement learning  hashing is often used in conjunc-
tion with tile coding to represent states in continuous spaces. Hashing is also
a promising approach to value function approximation in large discrete domains
such as Go and Hearts  where feature vectors can be constructed by exhaustively
combining a set of atomic features. Unfortunately  the typical use of hashing in
value function approximation results in biased value estimates due to the possibil-
ity of collisions. Recent work in data stream summaries has led to the development
of the tug-of-war sketch  an unbiased estimator for approximating inner products.
Our work investigates the application of this new data structure to linear value
function approximation. Although in the reinforcement learning setting the use of
the tug-of-war sketch leads to biased value estimates  we show that this bias can
be orders of magnitude less than that of standard hashing. We provide empirical
results on two RL benchmark domains and ﬁfty-ﬁve Atari 2600 games to highlight
the superior learning performance obtained when using tug-of-war hashing.

1

Introduction

Recent value-based reinforcement learning applications have shown the beneﬁt of exhaustively gen-
erating features  both in discrete and continuous state domains. In discrete domains  exhaustive
feature generation combines atomic features into logical predicates. In the game of Go  Silver et al.
[19] showed that good features could be generated by enumerating all stone patterns up to a certain
size. Sturtevant and White [21] similarly obtained promising reinforcement learning results using a
feature generation method that enumerated all 2  3 and 4-wise combinations of a set of 60 atomic
features. In continuous-state RL domains  tile coding [23] is a canonical example of exhaustive
feature generation; tile coding has been successfully applied to benchmark domains [22]  to learn to
play keepaway soccer [20]  in multiagent robot learning [4]  to train bipedal robots to walk [18  24]
and to learn mixed strategies in the game of Goofspiel [3].
Exhaustive feature generation  however  can result in feature vectors that are too large to be repre-
sented in memory  especially when applied to continuous spaces. Although such feature vectors are
too large to be represented explicitly  in many domains of interest they are also sparse. For example 
most stone patterns are absent from any particular Go position. Given a ﬁxed memory budget  the
standard approach is to hash features into a ﬁxed-size table  with collisions implicitly handled by
the learning algorithm; all but one of the applications discussed above use some form of hashing.
With respect to its typical use for linear value function approximation  hashing lacks theoretical
guarantees.
In order to improve on the basic hashing idea  we turn to sketches: state-of-the-art
methods for approximately storing large vectors [6]. Our goal is to show that one such sketch 
the tug-of-war sketch [7]  is particularly well-suited for linear value function approximation. Our
work is related to recent developments on the use of random projections in reinforcement learning
[11] and least-squares regression [16  10]. Hashing  however  possesses a computational advantage
over traditional random projections: each feature is hashed exactly once. In comparison  even sparse

1

random projection methods [1  14] carry a per-feature cost that increases with the size of the reduced
space. Tug-of-war hashing seeks to reconcile the computational efﬁciency that makes hashing a
practical method for linear value function approximation on large feature spaces  while preserving
the theoretical appeal of random projection methods.
A natural concern when using hashing in RL is that hash collisions irremediably degrade learning. In
this paper we argue that tug-of-war hashing addresses this concern by providing us with a low-error
approximation of large feature vectors at a fraction of the memory cost. To quote Sutton and Barto
[23]  “Hashing frees us from the curse of dimensionality in the sense that memory requirements
need not be exponential in the number of dimensions  but need merely match the real demands of
the task.”

2 Background
We consider the reinforcement learning framework of Sutton and Barto [23]. An MDP M is a
tuple (cid:104)S A  P  R  γ(cid:105)  where S is the set of states  A is the set of actions  P : S × A × S → [0  1]
is the transition probability function  R : S × A → R is the reward function and γ ∈ [0  1] is
the discount factor. At time step t the agent observes state st ∈ S  selects an action at ∈ A
and receives a reward rt := R(st  at). The agent then observes the new state st+1 distributed
according to P (·|st  at). From state st  the agent’s goal is to maximize the expected discounted
Qπ(s  a)  where the stationary policy π : S ×A → [0  1] represents the agent’s behaviour. Qπ(s  a)
is recursively deﬁned as:

i=0 γiR(st+i  at+i)(cid:3). A typical approach is to learn state-action values

sum of future rewards E(cid:2)(cid:80)∞

(cid:34)(cid:88)

a(cid:48)∈A

(cid:35)

(1)

Qπ(s  a) := R(s  a) + γEs(cid:48)∼P (·|s a)

π(a(cid:48)|s(cid:48))Qπ(s(cid:48)  a(cid:48))

this equation is the optimal value function Q∗(s  a)

A special case of
:= R(s  a) +
γEs(cid:48) [maxa(cid:48) Q∗(s(cid:48)  a(cid:48))]. The optimal value function corresponds to the value under an optimal
policy π∗. For a ﬁxed π  The SARSA(λ) algorithm [23] learns Qπ from sample transitions
(st  at  rt  st+1  at+1). In domains where S is large (or inﬁnite)  learning Qπ exactly is imprac-
tical and one must rely on value function approximation. A common value function approxi-
mation scheme in reinforcement learning is linear approximation. Given φ : S × A → Rn
mapping state-action pairs to feature vectors  we represent Qπ with the linear approximation
Qt(s  a) := θt · φ(s  a)  where θt ∈ Rn is a weight vector. The gradient descent SARSA(λ)
update is deﬁned as:

δt ← rt + γθt · φ(st+1  at+1) − θt · φ(st  at)
et ← γλet−1 + φ(st  at)

θt+1 ← θt + αδtet  

(2)
where α ∈ [0  1] is a step-size parameter and λ ∈ [0  1] controls the degree to which changes in
the value function are propagated back in time. Throughout the rest of this paper Qπ(s  a) refers to
the exact value function computed from Equation 1 and we use Qt(s  a) to refer to the linear ap-
proximation θt · φ(s  a); “gradient descent SARSA(λ) with linear approximation” is always implied
when referring to SARSA(λ). We call φ(s  a) the full feature vector and Qt(s  a) the full-vector
value function.
Asymptotically  SARSA(λ) is guaranteed to ﬁnd the best solution within the span of φ(s  a)  up to
a multiplicative constant that depends on λ [25]. If we let Φ ∈ R|S||A|×n denote the matrix of full
feature vectors φ(s  a)  and let µ : S × A → [0  1] denote the steady state distribution over state-
action pairs induced by π and P then  under mild assumptions  we can guarantee the existence and
uniqueness of µ. We denote by (cid:104)· ·(cid:105)µ the inner product induced by µ  i.e. (cid:104)x  y(cid:105)µ := xT Dy  where
x  y ∈ R|S||A| and D ∈ R|S||A|×|S||A| is a diagonal matrix with entries µ(s  a). The norm (cid:107)·(cid:107)µ is

deﬁned as(cid:112)(cid:104)· ·(cid:105)µ. We assume the following: 1) S and A are ﬁnite  2) the Markov chain induced

by π and P is irreducible and aperiodic  and 3) Φ has full rank. The following theorem bounds the
error of SARSA(λ):
Theorem 1 (Restated from Tsitsiklis and Van Roy [25]). Let M = (cid:104)S A  P  R  γ(cid:105) be an MDP and
π : S × A → [0  1] be a policy. Denote by Φ ∈ R|S||A|×n the matrix of full feature vectors and

2

by µ the stationary distribution on (S A) induced by π and P . Under assumptions 1-3)  SARSA(λ)
converges to a unique θπ ∈ Rn with probability one and
(cid:107)Φθπ − Qπ(cid:107)µ ≤ 1 − λγ
1 − γ

(cid:107)ΠQπ − Qπ(cid:107)µ  

is a vector representing the exact solution to Equation 1 and

where Qπ ∈ R|S||A|
Π := Φ(ΦT DΦ)−1ΦT D is the projection operator.
Because Π is the projector operator for Φ  for any θ we have (cid:107)Φθ − Qπ(cid:107)µ ≥ (cid:107)ΠQπ − Qπ(cid:107)µ;
Theorem 1 thus implies that SARSA(1) converges to θπ = arg minθ (cid:107)Φθ − Qπ(cid:107)µ.

2.1 Hashing in Reinforcement Learning

As discussed previously  it is often impractical to store the full weight vector θt in memory. A
typical example of this is tile coding on continuous-state domains [22]  which generates a number
of features exponential in the dimensionality of the state space. In such cases  hashing can effec-
tively be used to approximate Qπ(s  a) using a ﬁxed memory budget. Let h be a hash function
h : {1  . . .   n} → {1  . . .   m}  mapping full feature vector indices into hash table indices  where
m (cid:28) n is the hash table size. We deﬁne standard hashing features as the feature map ˆφ(s  a) whose
ith component is deﬁned as:

ˆφi(s  a) :=

I[h(j)=i]φj(s  a)  

(3)

j=1

where φj(s  a) denotes the jth component of φ(s  a) and I[x] denotes the indicator function. We
assume that our hash function h is drawn from a universal family: for any i  j ∈ {1  . . .   n}  i (cid:54)= j 
Pr(h(i) = h(j)) ≤ 1
m.1 We deﬁne the standard hashing value function ˆQt(s  a) := ˆθt · ˆφ(s  a) 
where ˆθt ∈ Rm is a weight vector  and ˆφ(s  a) is the hashed vector. Because of hashing collisions 
the standard hashing value function is a biased estimator of Qt(s  a)  i.e.  in general Eh[ ˆQt(s  a)] (cid:54)=
Qt(s  a). For example  consider the extreme case where m = 1: all features share the same weight.
We return to the issue of the bias introduced by standard hashing in Section 4.1.

2.2 Tug-of-War Hashing

The tug-of-war sketch  also known as the Fast-AGMS  was recently introduced as a powerful method
for approximating inner products of large vectors [7]. The name “sketch” refers to the data struc-
ture’s function as a summary of a stream of data. In the canonical sketch setting  we summarize a
count vector θ ∈ Rn using a sketch vector ˜θ ∈ Rm. At each time step a vector φt ∈ Rn is received.
i=0 φi. Given two hash
functions  h and ξ : {1  . . .   n} → {−1  1}  φt is mapped to a vector ˜φt whose ith component is

The purpose of the sketch vector is to approximate the count vector θt :=(cid:80)t−1

n(cid:88)

n(cid:88)

˜φt i :=

I[h(j)=i]φt jξ(j)

(4)

j=1

The tug-of-war sketch vector is then updated as ˜θt+1 ← ˜θt + ˜φt. In addition to h being drawn
from a universal family of hash functions  ξ is drawn from a four-wise independent family of hash
functions: for all sets of four unique indices {i1  i2  i3  i4}  Prξ(ξ(i1) = k1  ξ(i2) = k2  ξ(i3) =
16 with k1 . . . k4 ∈ {−1  1}. For an arbitrary φ ∈ Rn and its corresponding
k3  ξ(i4) = k4) = 1
tug-of-war vector ˜φ ∈ Rm  Eh ξ[˜θt · ˜φ] = θt · φ: the tug-of-war sketch produces unbiased estimates
˜φi.

of inner products [7]. This unbiasedness property can be derived as follows. First let ˜θt =(cid:80)t−1

i=0

1While it may seem odd to randomly select your hash function  this can equivalently be thought as sampling
an indexing assignment for the MDP’s features. While a particular hash function may be well- (or poorly-)
suited for a particular MDP  it is hard to imagine how this could be known a priori. By considering a randomly
selected hash function (or random permutation of the features)  we are simulating the uncertainty of using a
particular hash function on a never before encountered MDP.

3

Then ˜θt · ˜φt(cid:48) =(cid:80)t−1

i=0

˜φi · ˜φt(cid:48) and

 n(cid:88)

n(cid:88)

I[h(j1)=h(j2)]φi j1φt(cid:48) j2ξ(j1)ξ(j2)



j1=1
j2=1
if j1 = j2
otherwise

(by four-wise independence)

(cid:26) 1

0

Eh ξ[ ˜φi · ˜φt(cid:48)] = Eh ξ

Eξ[ξ(j1)ξ(j2)] =

The result follows by noting that I[h(j1)=h(j2)] is independent from ξ(j1)ξ(j2) given j1  j2.

3 Tug-of-War with Linear Value Function Approximation

hashing features as ˜φ : S × A → Rm with ˜φi(s  a) :=(cid:80)n

We now extend the tug-of-war sketch to the reinforcement learning setting by deﬁning the tug-of-war
I[h(j)=i]φj(s  a)ξ(j). The SARSA(λ)

j=1

update becomes:

˜δt ← rt + γ ˜θt · ˜φ(st+1  at+1) − ˜θt · ˜φ(st  at)
˜et ← γλ˜et−1 + ˜φ(st  at)

˜θt+1 ← ˜θt + α˜δt˜et.

(5)
We also deﬁne the tug-of-war value function ˜Qt(s  a) := ˜θt · ˜φ(s  a) with ˜θt ∈ Rm and refer to
˜φ(s  a) as the tug-of-war vector.

3.1 Value Function Approximation with Tug-of-War Hashing

Intuitively  one might hope that the unbiasedness of the tug-of-war sketch for approximating inner
products carries over to the case of linear value function approximation. Unfortunately  this is not
the case. However  it is still possible to bound the error of the tug-of-war value function learned with
SARSA(1) in terms of the full-vector value function. Our bound relies on interpreting tug-of-war
hashing as a special kind of Johnson-Lindenstrauss transform [8].
We deﬁne a ∞-universal family of hash functions H such that for any set of indices i1  i2  . . .   il
Pr(h(i1) = k1  . . .   h(il) = kl) ≤ 1|C|l   where C ⊂ N and h ∈ H : {1  . . .   n} → C .
Lemma 1 (Dasgupta et al. [8]  Theorem 2). Let h : {1  . . .   n} → {1  . . .   m} and ξ : {1  . . .   n} →
{−1  1} be two independent hash functions chosen uniformly at random from ∞-universal families
and let H ∈ {0 ±1}m×n be a matrix with entries Hij = I[h(j)=i]ξ(j). Let  < 1  δ < 1
10  
c  
m = 12
with probability 1 − 3δ  H satisﬁes the following property:

(cid:1). For any given vector x ∈ Rn such that (cid:107)x(cid:107)∞ ≤ 1√

(cid:1) and c = 16

(cid:1) log2(cid:0) m

2 log(cid:0) 1

 log(cid:0) 1

δ

δ

δ

(1 − )(cid:107)x(cid:107)2

2 ≤ (cid:107)Hx(cid:107)2

2 ≤ (1 + )(cid:107)x(cid:107)2
2 .

Lemma 1 states that  under certain conditions on the input vector x  tug-of-war hashing approxi-
mately preserves the norm of x. When δ and  are constant  the requirement on (cid:107)x(cid:107)∞ can be waived
by applying Theorem 1 to the normalized vector u =
c. A clear discussion on hashing as
a Johnson-Lindenstrauss transform can be found in the work of Kane and Nelson [13]  who also
improve Lemma 1 and extend it to the case where the family of hash functions is k-universal rather
than ∞-universal.
Lemma 2 (Based on Maillard and Munos [16]  Proposition 1). Let x1 . . . xK and y be vectors in
Rn. Let H ∈ {0 ±1}m×n    δ and m be deﬁned as in Lemma 1. With probability at least 1− 6Kδ 
for all k ∈ {1  . . .   K} 

x(cid:107)x(cid:107)2

√

xk · y − (cid:107)xk(cid:107)2 (cid:107)y(cid:107)2 ≤ Hxk · Hy ≤ xk · y + (cid:107)xk(cid:107)2 (cid:107)y(cid:107)2 .

Proof (Sketch). The proof follows the steps of Maillard and Munos [16]. Given two unit vectors
u  v ∈ Rn  we can relate (Hu) · (Hv) to (cid:107)Hu + Hv(cid:107)2
2 using the parallelogram
law. We then apply Lemma 1 to bound both sides of each squared norm and substitute xk for u and
y for w to bound Hxk · Hy. Applying the union bound yields the desired statement.

2 and (cid:107)Hu − Hv(cid:107)2

4

We are now in a position to bound the asymptotic error of SARSA(1) with tug-of-war hashing.
Given hash functions h and ξ deﬁned as per Lemma 1  we denote by H ∈ Rm×n the matrix whose
entries are Hij := I[h(j)=i]ξ(j)  such that ˜φ(s  a) = Hφ(s  a). We also denote by ˜Φ := ΦH T the
matrix of tug-of-war vectors. We again assume that 1) S and A are ﬁnite  that 2) π and P induce an
irreducible  aperiodic Markov chain and that 3) Φ has full rank. For simplicity of argument  we also
assume that 4) ˜Φ := ΦH T has full rank; when ˜Φ is rank-deﬁcient  SARSA(1) converges to a set of
solutions ˜Θπ satisfying the bound of Theorem 2  rather than to a unique ˜θπ.
Theorem 2. Let M = (cid:104)S A  P  R  γ(cid:105) be an MDP and π : S × A → [0  1] be a policy. Let
Φ ∈ R|S||A|×n be the matrix of full feature vectors and ˜Φ ∈ R|S||A|×m be the matrix of tug-of-
war vectors. Denote by µ the stationary distribution on (S A) induced by π and P . Let  < 1 
δ < 1  δ(cid:48) = δ
δ(cid:48) . Under assumptions 1-4)  gradient-descent SARSA(1) with
tug-of-war hashing converges to a unique ˜θπ ∈ Rm and with probability at least 1 − δ

6|S||A| and m ≥ 12

2 log 1

(cid:13)(cid:13) ˜Φ˜θπ − Qπ(cid:13)(cid:13)µ ≤ (cid:107)Φθπ − Qπ(cid:107)µ + (cid:107)θπ(cid:107)2

(cid:107)φ(s  a)(cid:107)2  

sup

s∈S a∈A

where Qπ is the exact solution to Equation 1 and θπ = arg minθ (cid:107)Φθ − Qπ(cid:107)µ.

Proof. First note that Theorem 1 implies the convergence of SARSA(1) with tug-of-war hashing to
a unique solution  which we denote ˜θπ. We ﬁrst apply Lemma 2 to the set {φ(s  a) : (s  a) ∈ S×A}
and θπ; note that we can safely assume |S||A| > 1  and therefore δ(cid:48) < 1/10. By our choice of m 
for all (s  a) ∈ S ×A  |Hφ(s  a)· Hθπ − φ(s  a)· θπ| ≤ (cid:107)φ(s  a)(cid:107)2 (cid:107)θπ(cid:107)2 with probability at least
1 − 6|S||A|δ(cid:48) = 1 − δ. As previously noted  SARSA(1) converges to ˜θπ = arg minθ (cid:107) ˜Φθ − Qπ(cid:107)µ;
compared to ˜Φ˜θπ  the solution θπ
H := ˜ΦHθπ is thus an equal or worse approximation to Qπ. It
follows that

H − Qπ(cid:13)(cid:13)µ ≤ (cid:13)(cid:13) ˜Φθπ
(cid:13)(cid:13) ˜Φ˜θπ − Qπ(cid:13)(cid:13)µ ≤ (cid:13)(cid:13) ˜Φθπ
(cid:115) (cid:88)
(cid:115) (cid:88)

H − Φθπ(cid:13)(cid:13)µ +(cid:13)(cid:13)Φθπ − Qπ(cid:13)(cid:13)µ
µ(s  a)(cid:2)Hφ(s  a) · Hθπ − φ(s  a) · θπ(cid:3)2
µ(s  a)(cid:2)(cid:107)φ(s  a)(cid:107)2 (cid:107)θπ(cid:107)2

+ (cid:107)Φθπ − Qπ(cid:107)µ

+ (cid:107)Φθπ − Qπ(cid:107)µ

(Lemma 2)

s∈S a∈A

(cid:3)2

=

≤

s∈S a∈A

≤ (cid:107)θπ(cid:107)2

sup

s∈S a∈A

(cid:107)φ(s  a)(cid:107)2 + (cid:107)Φθπ − Qπ(cid:107)µ  

as desired.

Our proof of Theorem 2 critically requires the use of λ = 1. A natural next step would be to attempt
to drop this restriction on λ. It also seems likely that the ﬁnite-sample analysis of LSTD with random
projections [11] can be extended to cover the case of tug-of-war hashing. Theorem 2 suggests that 
under the right conditions  the tug-of-war value function is a good approximation to the full-vector
value function. A natural question now arises: does tug-of-war hashing lead to improved linear
value function approximation compared with standard hashing? More importantly  does tug-of-war
hashing result in better learned policies? These are the questions we investigate empirically in the
next section.

4 Experimental Study

In the sketch setting  the appeal of tug-of-war hashing over standard hashing lies in its unbiasedness.
We therefore begin with an empirical study of the magnitude of the bias when applying different
hashing methods in a value function approximation setting.

4.1 Value Function Bias

We used standard hashing  tug-of-war hashing  and no hashing to learn a value function over a
short trajectory in the Mountain Car domain [22]. Our evaluation uses a standard implementation
available online [15].

5

Figure 1: Bias and Mean Squared Error of value estimates using standard and tug-of-war hashing in
1 000 learning steps of Mountain Car. Note the log scale of the y axis.

We generated a 1 000-step trajectory using an -greedy policy [23]. For this ﬁxed trajectory we
updated a full feature weight vector θt using SARSA(0) with γ = 1.0 and α = 0.01. We focus on
SARSA(0) as it is commonly used in practice for its ease of implementation and its faster update
speed in sparse settings. Parallel to the full-vector update we also updated both a tug-of-war weight
vector ˜θt and a standard hashing weight vector ˆθt  with the same values of γ and α. Both methods
use a hash table size of m = 100 and the same randomly selected hash function. This hash function
is deﬁned as (ax + b) mod p mod m  where p is a large prime and a  b < p are random integers
[5]. At every step we compute the difference in value between the hashed value functions ˜Qt(st  at)
and ˆQt(st  at)  and the full-vector value function Qt(st  at). We repeated this experiment using 1
million hash functions selected uniformly at random. Figure 1 shows for each time step  estimates
of the magnitude of the biases E[ ˜Qt(st  at)] − Qt(st  at) and E[ ˆQt(st  at)] − Qt(st  at) as well as
estimates of the mean squared errors E[( ˜Qt(st  at)− Qt(st  at))2] and E[( ˆQt(st  at)− Qt(st  at))2]
using the different hash functions. To provide a sense of scale  the estimate of the value of the ﬁnal
state when using no hashing is approximately −4; note that the y-axis uses a logarithmic scale.
The tug-of-war value function has a small  almost negligible bias. In comparison  the bias of stan-
dard hashing is orders of magnitude larger – almost as large as the value it is trying to estimate.
The mean square error estimates show a similar trend. Furthermore  the same experiment on the
Acrobot domain [22] yielded qualitatively similar results. Our results conﬁrm the insights provided
in Section 2: the tug-of-war value function can be signiﬁcantly less biased than the standard hashing
value function.

4.2 Reinforcement Learning Performance

Having smaller bias and mean square error in the Q-value estimates does not necessarily imply
improved agent performance. In reinforcement learning  actions are selected based on relative Q-
values  so a consistent bias may be harmless. In this section we evaluate the performance (cumulative
reward per episode) of learning agents using both tug-of-war and standard hashing.

4.2.1 Tile Coding

We ﬁrst studied the performance of agents using each of the two hashing methods in conjunction
with tile coding. Our study is based on Mountain Car and Acrobot  two standard RL benchmark
domains. For both domains we used the standard environment dynamics [22]; we used the ﬁxed
starting-state version of Mountain Car to reduce the variance in our results. We compared the two
hashing methods using -greedy policies and the SARSA(λ) algorithm.
For each domain and each hashing method we performed a parameter sweep over the learning rate
α and selected the best value which did not cause the value estimates to divergence. The Acrobot
state was represented using 48 6 × 6 × 6 × 6 tilings and the Mountain Car state  10 9 × 9 tilings.
Other parameters were set to γ = 1.0  λ = 0.9   = 0.0; the learning rate was further divided by the
number of tilings.

6

 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 10 0 100 200 300 400 500 600 700 800 900 1000BiasTime StepsTug-of-WarStandard 0.0001 0.001 0.01 0.1 1 10 100 0 100 200 300 400 500 600 700 800 900 1000Mean Squared ErrorTime StepsTug-of-WarStandardFigure 2: Performance of standard hashing and tug-of-war hashing in two benchmark domains. The
performance of the random agent is provided as reference.

We experimented with hash table sizes m ∈ [20  1000] for Mountain Car and m ∈ [100  2000] for
Acrobot. Each experiment consisted of 100 trials  sampling a new hash function for each trial. Each
trial consisted of 10 000 episodes  and episodes were restricted to 5 000 steps. At the end of each
trial  we disabled learning by setting α = 0 and evaluated the agent on an additional 500 episodes.
Figure 2 shows the performance of standard hashing and tug-of-war hashing as a function of the
hash table size. The conclusion is clear: when the hashed vector is small relative to the full vector 
tug-of-war hashing performs better than standard hashing. This is especially true in Acrobot  where
the number of features (over 62 000) necessarily results in harmful collisions.

4.2.2 Atari

We next evaluated tug-of-war hashing and standard hashing on a suite of Atari 2600 games. The
Atari domain was proposed as a game-independent platform for AI research by Naddaf [17]. Atari
games pose a variety of challenges for learning agents. The learning agent’s observation space is the
game screen: 160x210 pixels  each taking on one of 128 colors. In the game-independent setting 
agents are tuned using a small number of training games and subsequently evaluated over a large
number of games for which no game-speciﬁc tuning takes place. The game-independent setting
forces us to use features that are common to all games  for example  by encoding the presence
of color patterns in game screens; such an encoding is a form of exhaustive feature generation.
Different learning methods have been evaluated on the Atari 2600 platform [9  26  12]. We based
our evaluation on prior work on a suite of Atari 2600 games [2]  to which we refer the reader for full
details on handling Atari 2600 games as RL domains. We performed parameter sweeps over ﬁve
training games  and tested our algorithms on ﬁfty testing games.
We used models of contingency awareness to locate the player avatar [2]. From a given game 
we generate feature sets by exhaustively enumerating all single-color patterns of size 1x1 (single
pixels)  2x2  and 3x3. The presence of each different pattern within a 4x5 tile is encoded as a binary
feature. We also encode the relative presence of patterns with respect to the player avatar location.
This procedures gives rise to 569 856 000 different features  of which 5 000 to 15 000 are active at
a given time step.
We trained -greedy SARSA(0) agents using both standard hashing and tug-of-war hashing with
hash tables of size m=1 000  5 000 and 20 000. We chose the step-size α using a parameter sweep
over the training games: we selected the best-performing α which never resulted in divergence in
the value function. For standard hashing  α = 0.01  0.05  0.2 for m = 1 000  5 000 and 20 000 
respectively. For tug-of-war hashing  α = 0.5 across table sizes. We set γ = 0.999 and  = 0.05.
Each experiment was repeated over ten trials lasting 10 000 episodes each; we limited episodes to
18 000 frames to avoid issues with non-terminating policies.

7

 0 500 1000 1500 2000 2500 100 500 1000 1500 2000Random AgentStandard HashingTug-of-War HashingAcrobotSteps to GoalHash Table Size 0 1000 2000 3000 4000 5000 200 400 600 800 1000Mountain CarRandom AgentStandard HashingTug-of-War HashingSteps to GoalHash Table SizeFigure 3: Inter-algorithm score distributions over ﬁfty-ﬁve Atari games. Higher curves reﬂect higher
normalized scores.

Accurately comparing methods across ﬁfty-ﬁve games poses a challenge  as each game exhibits a
different reward function and game dynamics. We compared methods using inter-algorithm score
distributions [2]. For each game  we extracted the average score achieved by our agents over the
last 500 episodes of training  yielding six different scores (three per hashing method) per game.
Denoting these scores by sg i  i = 1 . . . 6  we deﬁned the inter-algorithm normalized score zg i :=
(sg i − rg min)/(rg max − rg min) with rg min := mini {sg i} and rg max := maxi {sg i}. Thus
zg i = 1.0 indicates that the ith score was the highest for game g  and zg i = 0.0 similarly indicates
the lowest score. For each combination of hashing method and memory size  its inter-algorithm
score distribution shows the fraction of games for which the corresponding agent achieves a certain
normalized score or better.
Figure 3 compares the score distributions of agents using either standard hashing or tug-of-war
hashing for m = 1 000  5 000 and 20 000. Tug-of-war hashing consistently outperforms standard
hashing across hash table sizes. For each m and each game  we also performed a two-tailed Welch’s
t-test with 99% conﬁdence intervals to determine the statistical signiﬁcance of the average score
difference between the two methods. For m = 1 000  tug-of-war hashing performed statistically
better in 38 games and worse in 5; for m = 5 000  it performed better in 41 games and worse in 7;
and for m = 20 000 it performed better in 35 games and worse in 5. Our results on Atari games
conﬁrm what we observed on Mountain Car and Acrobot: in practice  tug-of-war hashing performs
much better than standard hashing. Furthermore  computing the ξ function took less than 0.3% of
the total experiment time  a negligible cost in comparison to the beneﬁts of using tug-of-war hashing.

5 Conclusion

In this paper  we cast the tug-of-war sketch into the reinforcement learning framework. We showed
that  although the tug-of-war sketch is unbiased in the setting for which it was developed [7]  the
self-referential component of reinforcement learning induces a small bias. We showed that this bias
can be much smaller than the bias that results from standard hashing and provided empirical results
conﬁrming the superiority of tug-of-war hashing for value function approximation.
As increasingly more complex reinforcement learning problems arise and strain against the bound-
aries of practicality  so the need for fast and reliable approximation methods grows. If standard
hashing frees us from the curse of dimensionality  then tug-of-war hashing goes a step further by
ensuring  when the demands of the task exceed available resources  a robust and principled shift
from the exact solution to its approximation.

Acknowledgements

We would like to thank Bernardo ´Avila Pires  Martha White  Yasin Abbasi-Yadkori and Csaba
Szepesv´ari for the help they provided with the theoretical aspects of this paper  as well as Adam
White and Rich Sutton for insightful discussions on hashing and tile coding. This research was
supported by the Alberta Innovates Technology Futures and the Alberta Innovates Centre for Ma-
chine Learning at the University of Alberta. Invaluable computational resources were provided by
Compute/Calcul Canada.

8

1.00.50.0 00.20.40.60.81.0Fraction of gamesInter-algorithm scoreTug-of-WarStandardHash Table Size: 10001.00.50.0 00.20.40.60.81.0Fraction of gamesInter-algorithm scoreTug-of-WarStandardHash Table Size: 50001.00.50.0 00.20.40.60.81.0Fraction of gamesInter-algorithm scoreTug-of-WarStandardHash Table Size: 20 000References

[1] Dimitris Achlioptas. Database-friendly random projections: Johnson-Lindenstrauss with binary coins.

Journal of Computer and System Sciences  66(4):671–687  2003.

[2] Marc G. Bellemare  Joel Veness  and Michael Bowling. Investigating contingency awareness using Atari

2600 games. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence  2012.

[3] Michael Bowling and Manuela Veloso. Scalable learning in stochastic games. In AAAI Workshop on

Game Theoretic and Decision Theoretic Agents  2002.

[4] Michael Bowling and Manuela Veloso. Simultaneous adversarial multi-robot learning. In Proceedings of

the Eighteenth International Joint Conference on Artiﬁcial Intelligence  pages 699–704  2003.

[5] J. Lawrence Carter and Mark N. Wegman. Universal classes of hash functions. Journal of Computer and

System Sciences  18(2):143–154  1979.

[6] Graham Cormode. Sketch techniques for massive data. In Graham Cormode  Minos Garofalakis  Pe-
ter Haas  and Chris Jermaine  editors  Synopses for Massive Data: Samples  Histograms  Wavelets and
Sketches  Foundations and Trends in Databases. NOW publishers  2011.

[7] Graham Cormode and Minos Garofalakis. Sketching streams through the net: Distributed approximate
query tracking. In Proceedings of the 31st International Conference on Very Large Data Bases  pages
13–24  2005.

[8] Anirban Dasgupta  Ravi Kumar  and Tam´as Sarl´os. A sparse Johnson-Lindenstrauss transform. In Pro-

ceedings of the 42nd ACM Symposium on Theory of Computing  pages 341–350  2010.

[9] Carlos Diuk  A. Andre Cohen  and Michael L. Littman. An object-oriented representation for efﬁcient re-
inforcement learning. In Proceedings of the Twenty-Fifth International Conference on Machine Learning 
pages 240–247  2008.

[10] Mahdi Milani Fard  Yuri Grinberg  Joelle Pineau  and Doina Precup. Compressed least-squares regression
on sparse spaces. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence. AAAI 
2012.

[11] Mohammad Ghavamzadeh  Alessandro Lazaric  Oldaric-Ambrym Maillard  and R´emi Munos. LSTD
In Advances in Neural Information Processing Systems 23  pages 721–729 

with random projections.
2010.

[12] Matthew Hausknecht  Piyush Khandelwal  Risto Miikkulainen  and Peter Stone. HyperNEAT-GGP: A
In Genetic and Evolutionary Computation Conference

HyperNEAT-based Atari general game player.
(GECCO)  2012.

[13] Daniel M. Kane and Jelani Nelson. A derandomized sparse Johnson-Lindenstrauss transform. arXiv

preprint arXiv:1006.3585  2010.

[14] Ping Li  Trevor J. Hastie  and Kenneth W. Church. Very sparse random projections.

In Proceedings
of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages
287–296  2006.

[15] The Reinforcement Learning Library  2010. http://library.rl-community.org.
[16] Oldaric-Ambrym Maillard and R´emi Munos. Compressed least squares regression. In Advances in Neural

Information Processing Systems 22  pages 1213–1221  2009.

[17] Yavar Naddaf. Game-independent AI agents for playing Atari 2600 console games. PhD thesis  University

of Alberta  2010.

[18] E. Schuitema  D.G.E. Hobbelen  P.P. Jonker  M. Wisse  and J.G.D. Karssen. Using a controller based
on reinforcement learning for a passive dynamic walking robot. In Proceedings of the Fifth IEEE-RAS
International Conference on Humanoid Robots  pages 232–237  2005.

[19] David Silver  Richard S. Sutton  and Martin M¨uller. Reinforcement learning of local shape in the game

of Go. In 20th International Joint Conference on Artiﬁcial Intelligence  pages 1053–1058  2007.

[20] Peter Stone  Richard S. Sutton  and Gregory Kuhlmann. Reinforcement learning for RoboCup soccer

[21] Nathan Sturtevant and Adam White. Feature construction for reinforcement learning in Hearts. Comput-

keepaway. Adaptive Behavior  13(3):165  2005.

ers and Games  pages 122–134  2006.

[22] Richard S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse
coding. In David S. Touretzky  Michael C. Mozer  and Michael E. Hasselmo  editors  Advances in Neural
Information Processing Systems  volume 8  pages 1038–1044  1996.

[23] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press  1998.
[24] Russ Tedrake  Teresa Weirui Zhang  and H. Sebastian Seung. Stochastic policy gradient reinforcement
learning on a simple 3D biped. In Proceedings of Intelligent Robots and Systems 2004  volume 3  pages
2849–2854  2004.

[25] John N. Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function

approximation. IEEE Transactions on Automatic Control  42(5):674–690  1997.

[26] Samuel Wintermute. Using imagery to simplify perceptual abstraction in reinforcement learning agents.

In Proceedings of the Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence  2010.

9

,Maria-Florina Balcan
Tuomas Sandholm
Ellen Vitercik