2012,Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL,Since its inception  the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks.   We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL  we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional  operating on the vector of empirical risks. It incorporates minimax MTL  its relaxations  and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.,Minimax Multi-Task Learning and a Generalized

Loss-Compositional Paradigm for MTL

niche@cc.gatech.edu  drselee@gmail.com  agray@cc.gatech.edu

† College of Computing  Georgia Institute of Technology  Atlanta  GA 30332  USA

Nishant A. Mehta†  Dongryeol Lee∗  Alexander G. Gray†

∗ GE Global Research  Niskayuna  NY 12309  USA

Abstract

Since its inception  the modus operandi of multi-task learning (MTL) has been to
minimize the task-wise mean of the empirical risks. We introduce a generalized
loss-compositional paradigm for MTL that includes a spectrum of formulations as
a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formu-
lation that minimizes the maximum of the tasks’ empirical risks. Via a certain re-
laxation of minimax MTL  we obtain a continuum of MTL formulations spanning
minimax MTL and classical MTL. The full paradigm itself is loss-compositional 
operating on the vector of empirical risks. It incorporates minimax MTL  its relax-
ations  and many new MTL formulations as special cases. We show theoretically
that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks
in the learning to learn (LTL) test setting. The results of several MTL formulations
on synthetic and real problems in the MTL and LTL test settings are encouraging.

1

Introduction

The essence of machine learning is to exploit what we observe in order to form accurate predictors
of what we cannot. A multi-task learning (MTL) algorithm learns an inductive bias to learn several
tasks together. MTL is incredibly pervasive in machine learning: it has natural connections to ran-
dom effects models [15]; user preference prediction (including collaborative ﬁltering) can be framed
as MTL [16]; multi-class classiﬁcation admits the popular one-vs-all and all-pairs MTL reductions;
and MTL admits provably good learning in settings where single-task learning is hopeless [4  12].
But if we see examples from a random set of tasks today  which of these tasks will matter tomorrow?
Not knowing in the present what challenges nature has in store for the future  a sensible strategy is
to mitigate the worst case by ensuring some minimum proﬁciency on each task.
Consider a simple learning scenario: A music preference prediction company is in the business of
predicting what 5-star ratings different users would assign to songs. At training time  the com-
pany learns a shared representation for predicting the users’ song ratings by pooling together the
company’s limited data on each user’s preferences. Given this learned representation  a separate
predictor for each user can be trained very quickly. At test time  the environment draws a user
according to some (possibly randomized) rule and solicits from the company a prediction of that
user’s preference for a particular song. The environment may also ask for predictions about new
users  described by a few ratings each  and so the company must leverage its existing representation
to rapidly learn new predictors and produce ratings for these new users.
Classically  multi-task learning has sought to minimize the (regularized) sum of the empirical risks
over a set of tasks. In this way  classical MTL implicitly assumes that once the learner has been
trained  it will be tested on test tasks drawn uniformly at random from the empirical task distribution
of the training tasks. Notably  there are several reasons why classical MTL may not be ideal:

∗Work completed while at Georgia Institute of Technology

1

• While at training time the usual ﬂavor of MTL commits to a ﬁxed distribution over users (typi-
cally either uniform or proportional to the number of ratings available for each user)  at test time
there is no guarantee what user distribution we will encounter. In fact  there may not exist any
ﬁxed user distribution: the sequence of users for which ratings are elicited could be adversarial.
• Even in the case when the distribution over tasks is not adversarial  it may be in the interest of
the music preference prediction company to guarantee some minimum level of accuracy per user
in order to minimize negative feedback and a potential loss of business  rather than maximizing
the mean level of accuracy over all users.

• Whereas minimizing the average prediction error is very much a teleological endeavor  typically
at the expense of some locally egregious outcomes  minimizing the worst-case prediction error
respects a notion of fairness to all tasks (or people).

This work introduces minimax multi-task learning as a response to the above scenario.1 In addi-
tion  we cast a spectrum of multi-task learning. At one end of the spectrum lies minimax MTL 
and departing from this point progressively relaxes the “hardness” of the maximum until full re-
laxation reaches the second endpoint and recovers classical MTL. We further sculpt a generalized
loss-compositional paradigm for MTL which includes this spectrum and several other new MTL
formulations. This paradigm equally applies to the problem of learning to learn (LTL)  in which the
goal is to learn a hypothesis space from a set of training tasks such that this representation admits
good hypotheses on future tasks. In truth  MTL and LTL typically are handled equivalently at train-
ing time — this work will be no exception — and they diverge only in their test settings and hence
the learning theoretic inquiries they inspire.

Contributions. The ﬁrst contribution of this work is to introduce minimax MTL and a continuum
of relaxations. Second  we introduce a generalized loss-compositional paradigm for MTL which
admits a number of new MTL formulations and also includes classical MTL as a special case.
Third  we empirically evaluate the performance of several MTL formulations from this paradigm
in the multi-task learning and learning to learn settings  under the task-wise maximum test risk and
task-wise mean test risk criteria  on four datasets (one synthetic  three real). Finally  Theorem 1
is the core theoretical contribution of this work and shows the following: If it is possible to obtain
maximum empirical risk across a set of training tasks below some level γ  then it is likely that the
maximum true risk obtained by the learner on a new task is bounded by roughly γ. Hence  if the goal
is to minimize the worst case outcome over new tasks  the theory suggests minimizing the maximum
of the empirical risks across the training tasks rather than their mean.
In the next section  we recall the settings of multi-task learning and learning to learn  formally
introduce minimax MTL  and motivate it theoretically.
In Section 3  we introduce a continu-
ously parametrized family of minimax MTL relaxations and the new generalized loss-compositional
paradigm. Section 4 presents an empirical evaluation of various MTL/LTL formulations with differ-
ent models on four datasets. Finally  we close with a discussion.

2 Minimax multi-task learning

We begin with a promenade through the basic MTL and LTL setups  with an effort to abide by the
notation introduced by Baxter [4]. Throughout the rest of the paper  each labeled example (x  y)
will live in X × Y for input instance x and label y. Typical choices of X include Rn or a compact
subset thereof  while Y typically is a compact subset of R or the binary {−1  1}.
In addition 
deﬁne a loss function (cid:96) : R × Y → R+. For simplicity  this work considers (cid:96)2 loss (squared loss)
(cid:96)(y(cid:48)  y) = (y(cid:48) − y)2 for regression and hinge loss (cid:96)(y(cid:48)  y) = max{0  1 − y(cid:48)y} for classiﬁcation.
MTL and LTL often are framed as applying an inductive bias to learn a common hypothesis space 
selected from a ﬁxed family of hypothesis spaces  and thereafter learning from this hypothesis space
a hypothesis for each task observed at training time. It will be useful to formalize the various sets
and elements present in the preceding statement. Let H be a family of hypothesis spaces. Any
hypothesis space H ∈ H itself is a set of hypotheses; each hypothesis h ∈ H is a map h : X → R.

1Note that minimax MTL does not refer to the minimax estimators of statistical decision theory.

2

Learning to learn.
In learning to learn  the goal is to achieve inductive transfer to learn the best
H from H. Unlike in MTL  there is a notion of an environment of tasks: an unknown probability
measure Q over a space of task probability measures P. The goal is to ﬁnd the optimal representation
via the objective

(1)
In practice  T (unobservable) training task probability measures P1  . . .   PT ∈ P are drawn iid from
Q  and from each task t a set of m examples are drawn iid from Pt.

h∈H E(x y)∼P (cid:96)(y  h(x)).

infH∈H EP∼Q inf

Multi-task learning. Whereas in learning to learn there is a distribution over tasks  in multi-task
learning there is a ﬁxed  ﬁnite set of tasks indexed by [T ] := {1  . . .   T}. Each task t ∈ [T ]
is coupled with a ﬁxed but unknown probability measure Pt. Classically  the goal of MTL is to
minimize the expected loss at test time under the uniform distribution on [T ]:

h∈H E(x y)∼Pt(cid:96)(y  h(x)).
inf

(2)

(cid:88)

t∈[T ]

infH∈H

1
T

Notably  this objective is equivalent to (1) when Q is the uniform distribution on {P1  . . .   PT}. In
terms of the data generation model  MTL differs from LTL since the tasks are ﬁxed; however  just
as in LTL  from each task t a set of m examples are drawn iid from Pt .

2.1 Minimax MTL

A natural generalization of classical MTL results by introducing a prior distribution π over the index
set of tasks [T ]. Given π  the (idealized) objective of this generalized MTL is

infH∈H Et∼π inf

h∈H E(x y)∼Pt(cid:96)(y  h(x)) 

(3)
given only the training data {(xt 1  yt 1)  . . .   (xt m  yt m)}t∈[T ]. The classical MTL objective (2)
equals (3) when π is taken to be the uniform prior over [T ]. We argue that in many instances  that
which is most relevant to minimize is not the expected error under a uniform distribution over tasks 
or even any pre-speciﬁed π  but rather the expected error for the worst π. We propose to minimize
the maximum error over tasks under an adversarial choice of π  yielding the objective:

infH∈H sup

π

Et∼π inf

h∈H E(x y)∼Pt(cid:96)(y  h(x)) 

where the supremum is taken over the T -dimensional simplex. As the supremum (assuming it is
attained) is attained at an extreme point of the simplex  this objective is equivalent to

infH∈H max
t∈[T ]

h∈H E(x y)∼Pt(cid:96)(y  h(x)).
inf

In practice  we approximate the true objective via a regularized form of the empirical objective

m(cid:88)

i=1

infH∈H max
t∈[T ]

inf
h∈H

(cid:96)(yt i  h(xt i)).

In the next section  we motivate minimax MTL theoretically by showing that the worst-case perfor-
mance on future tasks likely will not be much higher than the maximum of the empirical risks for
the training tasks. In this short paper  we restrict attention to the case of ﬁnite H.

2.2 A learning to learn bound for the maximum risk

In this subsection  we use the following notation. Let P (1)  . . .   P (T ) be probability measures drawn
iid from Q  and for t ∈ [T ] let z(t) be an m-sample (a sample of m points) from P (t) with corre-
m . Also  if P is a probability measure then P (cid:96) ◦ h := E(cid:96)(y  h(x));
sponding empirical measure P (t)
similarly  if Pm is an empirical measure  then Pm(cid:96) ◦ h := 1
Our focus is the learning to learn setting with a minimax lens: when one learns a representation
H ∈ H from multiple training tasks and observes maximum empirical risk γ  we would like to

i=1 (cid:96)(yi  h(xi)).

(cid:80)m

m

3

guarantee that H’s true risk on a newly drawn test task will be bounded by roughly γ. Such a goal is
in striking contrast to the classical emphasis of learning to learn  where the goal is to obtain bounds
on H’s expected true risk. Using H’s expected true risk and Markov’s inequality  Baxter [4  the
display prior to (25) ] showed that the probability that H’s true risk on a newly drawn test task is
above some level γ decays as the expected true risk over γ:

(cid:26)

(cid:27)

(cid:80)

Pr

h∈H P (cid:96) ◦ h ≥ γ
inf

≤ 1

T

m (cid:96) ◦ ht + ε
t∈[T ] P (t)
γ

(4)

where the size of ε is controlled by T   m  and the complexities of certain spaces.
The expected true risk is not of primary interest for controlling the tail of the (random) true risk 
and a more direct approach yields a much better bound. In this short paper we restrict the space of
representations H to be ﬁnite with cardinality C; in this case  the analysis is particularly simple and
illuminates the idea for proving the general case. The next theorem is the main result of this section:
Theorem 1. Let |H| = C  and let the loss (cid:96) be L-Lipschitz in its second argument and bounded by
B. Suppose T tasks P (1)  . . .   P (T ) are drawn iid from Q and from each task P (t) an iid m-sample
z(t) is drawn. Suppose there exists H ∈ H such that all t ∈ [T ] satisfy minh∈H P (t)
m (cid:96) ◦ h ≤ γ. Let
P be newly drawn probability measure from Q. Let ˆh be the empirical risk minimizer over the test
m-sample. With probability at least 1 − δ with respect to the random draw of the T tasks and their
T corresponding m-samples:

1
T

+ 2L maxH∈H Rm(H) +

8 log 4
δ

m

δ + log(cid:100)B(cid:101) + log(T + 1)

.

(5)

T

P (cid:96) ◦ ˆh > γ +

Pr

(cid:115)

 ≤ log 2C

In the above  Rm(H) is the Rademacher complexity of H (cf. [3]). Critically  in (5) the probability
of observing a task with high true risk decays with T   whereas in (4) the decay is independent of T .
Hence  when the goal is to minimize the probability of bad performance on future tasks uniformly 
this theorem motivates minimizing the maximum of the empirical risks as opposed to their mean.
For the proof of Theorem 1  ﬁrst consider the singleton case H = {H1}. Suppose that for γ ﬁxed a
m (cid:96) ◦ h ≤ γ.
priori  the maximum of the empirical risks is bounded by γ  i.e. maxt∈[T ] minh∈H1 P (t)
Let a new probability measure P drawn from Q correspond to a new test task. Suppose the prob-
ability of the event [minh∈H1 Pm(cid:96) ◦ h > γ] is at least ε. Then the probability that γ bounds all T
empirical risks is at most (1 − ε)T ≤ e−T ε. Hence  with probability at least 1 − e−T ε:

A simple application of the union bound extends this result for ﬁnite H:
Lemma 1. Under the same conditions as Theorem 1  with probability at least 1 − δ/2 with respect
to the random draw of the T tasks and their T corresponding m-samples:

Pr {minh∈H1 Pm(cid:96) ◦ h > γ} ≤ ε.
(cid:26)

(cid:27)

Pr

h∈H Pm(cid:96) ◦ h > γ
min

≤ log 2C

δ

.

T

The bound in the lemma states a 1/T rate of decay for the probability that the empirical risk obtained
by H on a new task exceeds γ. Next  we relate this empirical risk to the true risk obtained by the
empirical risk minimizer. Note that at test time H is ﬁxed and hence independent of any test m-
sample. Then  from by now standard learning theory results of Bartlett and Mendelson [3]:
Lemma 2. Take loss (cid:96) as in Theorem 1. With probability at least 1 − δ/2  for all h ∈ H uniformly:

P (cid:96) ◦ h ≤ Pm(cid:96) ◦ h + 2LRm(H) +(cid:112)(8 log(4/δ))/m.

In particular  with high probability the true risk of the empirical risk minimizer is not much larger
than its empirical risk. Theorem 1 now follows from Lemmas 1 and 2 and a union bound over
γ ∈ Γ := {0  1/T  2/T  . . .  (cid:100)B(cid:101)}; note that mapping the observed maximum empirical risk γ to
min{γ(cid:48) ∈ Γ | γ ≤ γ(cid:48)} picks up the additional 1
In the next section  we introduce a loss-compositional paradigm for multi-task learning which in-
cludes as special cases minimax MTL and classical MTL.

T term in (5).

4

3 A generalized loss-compositional paradigm for MTL

risk for hypothesis ht ∈ H (∈ H) on task t ∈ [T ] as ˆ(cid:96)t(ht) :=(cid:80)m

The paradigm can beneﬁt from a bit of notation. Given a set of T tasks  we represent the empirical
i=1 (cid:96)(yt i  ht(xt i)). Additionally
deﬁne a set of hypotheses for multiple tasks h := (h1  . . .   hT ) ∈ HT and the vector of empirical
risks ˆ(cid:96)(h) := (ˆ(cid:96)1(h1)  . . .   ˆ(cid:96)T (hT )).
With this notation set  the proposed loss-compositional paradigm encompasses any regularized min-
imization of a (typically convex) function φ : RT

+ → R+ of the empirical risks:

φ(cid:0)ˆ(cid:96)(h)(cid:1) + Ω(cid:0)(H  h)(cid:1) 

infH∈H inf
h∈HT

(6)

where Ω(·) : H × ∪H∈HHT → R+ is a regularizer.

(cid:96)p MTL. One notable specialization that is still quite general is the case when φ is an (cid:96)p-norm 
yielding (cid:96)p MTL. This subfamily encompasses classical MTL and many new MTL formulations:

• Classical MTL as (cid:96)1 MTL:

infH∈H inf
h∈HT

1
T

t∈[T ]
• Minimax MTL as (cid:96)∞ MTL:
max
t∈[T ]

infH∈H inf
h∈HT

• A new formulation  (cid:96)2 MTL:

(cid:88)

ˆ(cid:96)(ht) + Ω(cid:0)(H  h)(cid:1)
ˆ(cid:96)(ht) + Ω(cid:0)(H  h)(cid:1)
(cid:0)ˆ(cid:96)(ht)(cid:1)2(cid:17)1/2

(cid:16) 1

T

(cid:88)

t∈[T ]

infH∈H inf
h∈HT

≡ infH∈H inf
h∈HT

≡ infH∈H inf
h∈HT

+ Ω(cid:0)(H  h)(cid:1) ≡ infH∈H inf

h∈HT

1
T

(cid:107)ˆ(cid:96)(h)(cid:107)1 + Ω(cid:0)(H  h)(cid:1).
(cid:107)ˆ(cid:96)(h)(cid:107)∞ + Ω(cid:0)(H  h)(cid:1).
(cid:107)ˆ(cid:96)(h)(cid:107)2 + Ω(cid:0)(H  h)(cid:1).

1√
T

A natural question is why one might consider minimizing (cid:96)p-norms of the empirical risks vector for
1 < p < ∞  as in (cid:96)2 MTL. The contour of the (cid:96)1-norm of the empirical risks evenly trades off
empirical risks between different tasks; however  it has been observed that overﬁtting often happens
near the end of learning  rather than the beginning [14]. More precisely  when the empirical risk is
high  the gradient of the empirical risk (taken with respect to the parameter (H  h)) is likely to have
positive inner product with the gradient of the true risk. Therefore  given a candidate solution with a
corresponding vector of empirical risks  a sensible strategy is to take a step in solution space which
places more emphasis on tasks with higher empirical risk. This strategy is particularly appropriate
when the class of learners has high capacity relative to the amount of available data. This observation
sets the foundation for an approach that minimizes norms of the empirical risks.
In this work  we also discuss an interesting subset of the loss-compositional paradigm which does
not ﬁt into (cid:96)p MTL; this subfamily embodies a continuum of relaxations of minimax MTL.

α-minimax MTL.
In some cases  minimizing the maximum loss can exhibit certain disadvan-
tages because the maximum loss is not robust to situations when a small fraction of the tasks are
fundamentally harder than the remaining tasks. Consider the case when the empirical risk for each
task in this small fraction can not be reduced below a level u. Rather than rigidly minimizing the
maximum loss  a more robust alternative is to minimize the maximize loss in a soft way.
Intu-
itively  the idea is to ensure that most tasks have low empirical risk  but a small fraction of tasks are
permitted to have higher loss. We formalize this as α-minimax MTL  via the relaxed objective:

(cid:110)

(cid:88)

t∈[T ]

1
α

max{0  ˆ(cid:96)t(ht) − b}(cid:111)

+ Ω(cid:0)(H  h)(cid:1).

minimize
H∈H h∈HT

min
b≥0

b +

In the above  φ from the loss-compositional paradigm (6) is a variational function of the empirical
risks vector. The above optimization problem is equivalent to the perhaps more intuitive problem:
ˆ(cid:96)t(ht) ≤ b + ξt  t ∈ [T ].

ξt + Ω(cid:0)(H  h)(cid:1)

(cid:88)

subject to

minimize

b +

H∈H h∈HT  b≥0 ξ≥0

1
α

t∈[T ]

5

Here  b plays the role of the relaxed maximum  and each ξt’s deviation from zero indicates the
deviation from the (loosely enforced) maximum. We expect ξ to be sparse.
To help understand how α affects the learning problem  let us consider a few cases:

(1) When α > T   the optimal value of b is zero  and the problem is equivalent to classical MTL. To
see this  note that for a given candidate solution with b > 0 the objective always can be reduced
by reducing b by some ε and increasing each ξt by the same ε.

(2) Suppose one task is much harder than all the other tasks (e.g. an outlier task)  and its empirical
risk is separated from the maximum empirical risk of the other tasks by ρ. Let 1 < α < 2; now 
at the optimal hard maximum solution (where ξ = 0)  the objective can be reduced by increasing
one of the ξt’s by ρ and decreasing b by ρ. Thus  the objective can focus on minimizing the
maximum risk of the set of T − 1 easier tasks. In this special setting  this argument can be
extended to the more general case k < α < k + 1 and k outlier tasks  for k ∈ [T ].

(3) As α approaches 0  we recover the hard maximum case of minimax MTL.
This work focuses on α-minimax MTL with α = 2/((cid:100)0.1T + 0.5(cid:101)−1 + (cid:100)0.1T + 1.5(cid:101)−1) i.e. the
harmonic mean of (cid:100)0.1T + 0.5(cid:101) and (cid:100)0.1T + 1.5(cid:101). The reason for this choice is that in the idealized
case (2) above  for large T this setting of α makes the relaxed maximum consider all but the hardest
10% of the tasks. We also try the 20% level (i.e. 0.2T replacing 0.1T in the above).

t∈[T ]

(cid:80)

minv0 {vt}t∈[T ]

Models. We now provide examples of how speciﬁc models ﬁt into this framework. We consider
two convex multi-task learning formulations: Evgeniou and Pontil’s regularized multi-task learning
(the EP model) [5] and Argyriou  Evgeniou  and Pontil’s convex multi-task feature learning (the
AEP model) [1]. The EP model is a linear model with a shared parameter v0 ∈ Rd and task-speciﬁc
(cid:80)m
parameters vt ∈ Rd (for t ∈ [T ]). Evgeniou and Pontil presented this model as
i=1 (cid:96)(yt i (cid:104)v0 + vt  xt i(cid:105)) + λ0(cid:107)v0(cid:107)2 + λ1
(cid:80)m
(cid:80)
i=1 (cid:96)(yt i (cid:104)Wt  xt i(cid:105)) + λ(cid:107)W(cid:107)tr 
i=1 (cid:96)(cid:0)yt i (cid:104)ht  xt i(cid:105)(cid:1).
(cid:80)m

for (cid:96) the hinge loss or squared loss. This can be set in the new paradigm via H = {Hv0 | v0 ∈ Rd} 
Hv0 = {h : x (cid:55)→ (cid:104)v0 + vt  x(cid:105) | vt ∈ Rd}  and ˆ(cid:96)t(ht) = 1
The AEP model minimizes the task-wise average loss with the trace norm (nuclear norm) penalty:

i=1 (cid:96)(cid:0)yt i (cid:104)v0 + vt  xt i(cid:105)(cid:1).
(cid:80)m

where (cid:107)·(cid:107)tr : W (cid:55)→(cid:80)

(cid:80)
t∈[T ] (cid:107)vt(cid:107)2 

minW

m

T

t

i σi(W ) is the trace norm. In the new paradigm  H is a set where each element
is a k-dimensional subspace of linear estimators (for k (cid:28) d). Each ht = Wt in some H ∈ H lives
in H’s corresponding low-dimensional subspace. Also  ˆ(cid:96)t(ht) = 1
For easy empirical comparison between the various MTL formulations from the paradigm  at times
it will be convenient to use constrained formulations of the EP and AEP model. If the regularized
forms are used  a fair comparison of the methods warrants plotting results according to the size of
the optimal parameter found (i.e. (cid:107)W(cid:107)tr for AEP). For EP  the constrained form is:
subject to (cid:107)v0(cid:107) ≤ τ0  (cid:107)vt(cid:107) ≤ τ1 for t ∈ [T ].
minv0 {vt}t∈[T ]
For AEP  the constrained form is: minW

(cid:80)m
(cid:80)m
i=1 (cid:96)(yt i (cid:104)v0 + vt  xt i(cid:105))
i=1 (cid:96)(yt i (cid:104)Wt  xt i(cid:105))

subject to (cid:107)W(cid:107)tr ≤ r.

(cid:80)

(cid:80)

t∈[T ]

m

t

4 Empirical evaluation

We consider four learning problems; the ﬁrst three involve regression (MTL model in parentheses):

• A synthetic dataset composed from two modes of tasks (EP model) 
• The school dataset from the Inner London Education Authority (EP model) 
• The conjoint analysis personal computer ratings dataset 2 [11] (AEP model).

The fourth problem is multi-class classiﬁcation from the MNIST digits dataset [10] with a reduction
to multi-task learning using a tournament of pairwise (binary) classiﬁers. We use the AEP model.
Given data  each problem involved a choice of MTL formulation (e.g. minimax MTL)  model (EP
or AEP)  and choice of regularized versus constrained. All the problems were solved with just a few
lines of code using CVX [9  8]. In this work  we considered convex multi-task learning formulations
in order to make clear statements about the optimal solutions attained for various learning problems.
2This data  collected at the University of Michigan MBA program  generously was provided by Peter Lenk.

6

Figure 1: Max (cid:96)2-risk (Top two lines) and mean (cid:96)2-risk (Bottom two lines). At Left and Center: (cid:96)2-risk vs
noise level  for σtask = 0.1 and σtask = 0.5 respectively. At Right: (cid:96)2-risk vs task variation  for σnoise = 0.1.
Dashed red is (cid:96)1  dashed blue is minimax. Error bars indicate one standard deviation. MTL results (not shown)
were similar to LTL results (shown)  with MTL-LTL relative difference below 6.8% for all points plotted.

Two modes. The two modes regression problem consists of 50 linear prediction tasks for the ﬁrst
type of task and 5 linear prediction tasks for the second task type. The true parameter for the ﬁrst
task type is a vector µ drawn uniformly from the sphere of radius 5; the true parameter for the
second task type is −2µ. Each task is drawn from an isotropic Gaussian with mean taken from the
task type and the standard deviation of all dimensions set to σtask. Each data point for each task is
drawn from a product of 10 standard normals (so xt i ∈ R10). The targets are generated according
to (cid:104)Wt  xt i(cid:105) + εt  where the εt’s are iid univariate centered normals with standard deviation σnoise.
We ﬁxed τ0 to a large value (in this case  τ0 = 10 is sufﬁcient since the mean for the largest task
ﬁts into a ball of radius 10) and τ1 to a small value (τ1 = 2). We compute the average mean and
maximum test error over 100 instances of the 55-task multi-task problem. Each task’s training set
and test set are 5 and 15 points respectively. The average maximum (mean) test error is the 100-
experiment-average of the task-wise maximum (mean) of the (cid:96)2 risks. For each LTL experiment  55
new test tasks were drawn using the same µ as from the training tasks.
Figure 1 shows a tradeoff: when each task group is fairly homogeneous (left and center plots) 
minimax is better at minimizing the maximum of the test risks while (cid:96)1 is better at minimizing the
mean of the test risks. As task homogeneity decreases (right plot)  the gap in performance closes
with respect to the maximum of the test risks and remains roughly the same with respect to the mean.

Figure 2: Maximum RMSE (Left) and normalized mean RMSE (Right) versus task-speciﬁc parameter bound
τ1  for shared parameter bound τ0 ﬁxed. In each ﬁgure  Left section is τ0 is 0.2 and Right section is τ0 = 0.6.
Solid red (cid:7) is (cid:96)1  solid blue • is minimax  dashed green (cid:78) is (0.1T )-minimax  dashed black (cid:72) is (0.2T )-
minimax. The results for (cid:96)2 MTL were visually identical to (cid:96)1 MTL and hence were not plotted.
School. The school dataset has appeared in many previous works [7  2  6]. For brevity we just say
the goal is to predict student test scores using certain student-level features. Each school is treated as
a separate task. We report both the task-wise maximum of the root mean square error (RMSE) and
the taskwise-mean of the RMSE (normalized by number of points per task  as in previous works).
The results (see Figure 2) demonstrate that when the learner has moderate shared capacity τ0 and
high task-speciﬁc capacity τ1  minimax MTL outperforms (cid:96)1 MTL for the max objective; addition-
ally  for the max objective in almost all parameter settings (0.1T )-minimax and (0.2T )-minimax
MTL outperform (cid:96)1 MTL  and they also outperform minimax MTL when the task-speciﬁc capacity
τ1 is not too large. We hypothesize that minimax MTL performs the best in the high−τ1 regime be-
cause stopping learning once the maximum of the empirical risks cannot be improved invokes early
stopping and its built-in regularization properties (see e.g. [13]). Interestingly  for the normalized
mean RMSE objective  both minimax relaxations are competitive with (cid:96)1 MTL; however  when the

7

−101234050100150200250σnoisesquared−loss risk−101234050100150200250σnoisesquared−loss risk−0.500.511.522.5050100150200250300350400σtasksquared−loss risk00.20.40.600.20.40.61.31.351.41.451.5τ100.20.40.600.20.40.60.780.80.820.840.860.880.9τ1shared capacity τ0 is high (right section  right plot)  (cid:96)1 MTL performs the best. For high task-speciﬁc
capacity τ1  minimax MTL and its relaxations again seem to resist overﬁtting compared to (cid:96)1 MTL.
Personal computer. The personal
computer dataset is composed of 189
human subjects each of which rated
on a 0-10 scale the same 20 comput-
ers (16 training  4 test). Each com-
puter has 13 binary features (amount
of memory  screen size  price  etc.).
The results are shown in Figure 3. In
the MTL setting  for both the maxi-
mum RMSE objective and the mean
RMSE objective  (cid:96)1 MTL appears to
perform the best. When the trace
norm of W is high  minimax MTL
displays resistance to overﬁtting and
obtains the lowest mean RMSE. In
the LTL setting for the maximum
RMSE objective  (cid:96)2  minimax  and
(0.1T )-minimax MTL all outperform
(cid:96)1 MTL. For the mean RMSE  (cid:96)1
MTL obtains the lowest risk for al-
most all parameter settings.

Figure 3: MTL (Top) and LTL (Bottom). Maximum (cid:96)2 risk (Left)
and Mean (cid:96)2 risk (Right) vs bound on (cid:107)W(cid:107)tr. LTL used 10-fold
cross-validation (10% of tasks left out in each fold). Solid red (cid:7) is
(cid:96)1  solid blue • is minimax  dashed green (cid:78) is (0.1T )-minimax 
dashed black (cid:72) is (0.2T )-minimax  solid gold (cid:4) is (cid:96)2.

MNIST. The MNIST task is a 10-class problem; we ap-
proach it via a reduction to a tournament of 45 binary clas-
siﬁers trained via the AEP model. The dimensionality was
reduced to 50 using principal component analysis (com-
puted on the full training set)  and only the ﬁrst 2% of
each class’s training points were used for training.
Intuitively  the performance of the tournament tree of bi-
nary classiﬁers can only be as accurate as its paths  and
the accuracy of each path depends on the accuracy of
the nodes. Hence  our hypothesis is that minimax MTL
should outperform (cid:96)1 MTL. The results in Figure 4 con-
ﬁrm our hypothesis. Minimax MTL outperforms (cid:96)1 MTL
when the capacity (cid:107)W(cid:107)tr is somewhat limited  with the
gap widening as the capacity decreases. Furthermore  at
every capacity minimax MTL is competitive with (cid:96)1 MTL.

5 Discussion

Figure 4: Test multiclass 0-1 loss vs
(cid:107)W(cid:107)tr. Solid red is (cid:96)1 MTL  solid blue is
minimax  dashed green is (0.1T )-minimax 
dashed black is (0.2T )-minimax. Regular-
ized AEP used for speed and trace norm of
W ’s computed  so samples differ per curve.

We have established a continuum of formulations for MTL which recovers as special cases classical
MTL and the newly formulated minimax MTL. In between these extreme points lies a continuum of
relaxed minimax MTL formulations. More generally  we introduced a loss-compositional paradigm
that operates on the vector of empirical risks  inducing the additional (cid:96)p MTL paradigms. The empir-
ical evaluations indicate that α-minimax MTL at either the 10% or 20% level often outperforms (cid:96)1
MTL in terms of the maximum test risk objective and sometimes even in the mean test risk objective.
All the minimax or α-minimax MTL formulations exhibit a built-in safeguard against overﬁtting in
the case of learning with a model that is very complex relative to the available data.
Although efﬁcient algorithms may make the various new MTL learning formulations practical for
large problems  a proper effort to develop fast algorithms in this setting would have detracted from
the main point of this ﬁrst study. A good direction for the future is to obtain efﬁcient algorithms
for minimax and α-minimax MTL. In fact  such algorithms might have applications beyond MTL
and even machine learning. Another area ripe for exploration is to establish more general learning
bounds for minimax MTL and to extend these bounds to α-minimax MTL.

8

051015200.650.70.750.80.850.9bound on trace norm of WCV−mean Maximum squared loss051015200.120.140.160.180.20.220.24bound on trace norm of WCV−mean Mean squared loss051015200.430.440.450.460.470.480.490.5bound on trace norm of WCV−mean Maximum squared loss051015200.130.1350.140.1450.150.155bound on trace norm of WCV−mean Mean squared loss4060801001201400.10.150.20.250.30.350.40.450.5trace norm of WTest multiclass zero−one lossReferences
[1] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  73(3):243–272  2008.

[2] Bart Bakker and Tom Heskes. Task clustering and gating for bayesian multitask learning.

Journal of Machine Learning Research  4:83–99  2003.

[3] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds

and structural results. Journal of Machine Learning Research  3:463–482  2002.

[4] Jonathan Baxter. A model of inductive bias learning. Journal of Artiﬁcial Intelligence Re-

search  12(1):149–198  2000.

[5] Theodoros Evgeniou and Massimiliano Pontil. Regularized multi-task learning. In Proceed-
ings of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining  pages 109–117. ACM  2004.

[6] Theodoros Evgeniou  Massimiliano Pontil  and Olivier Toubia. A convex optimization ap-
proach to modeling consumer heterogeneity in conjoint estimation. Marketing Science 
26(6):805–818  2007.

[7] Harvey Goldstein. Multilevel modelling of survey data. Journal of the Royal Statistical Society.

Series D (The Statistician)  40(2):235–244  1991.

[8] Michael C. Grant and Stephen P. Boyd. Graph implementations for nonsmooth convex pro-
In V. Blondel  S. Boyd  and H. Kimura  editors  Recent Advances in Learning and
grams.
Control  Lecture Notes in Control and Information Sciences  pages 95–110. Springer-Verlag
Limited  2008.

[9] Michael C. Grant and Stephen P. Boyd. CVX: Matlab software for disciplined convex pro-

gramming  version 1.21  April 2011.

[10] Yann LeCun  L´eon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[11] Peter J. Lenk  Wayne S. DeSarbo  Paul E. Green  and Martin R. Young. Hierarchical bayes
conjoint analysis: Recovery of partworth heterogeneity from reduced experimental designs.
Marketing Science  pages 173–191  1996.

[12] Andreas Maurer. Transfer bounds for linear feature learning. Machine learning  75(3):327–

350  2009.

[13] Noboru Murata and Shun-ichi Amari. Statistical analysis of learning dynamics. Signal Pro-

cessing  74(1):3–28  1999.

[14] Nicolas Le Roux  Pierre-Antoine Manzagol  and Yoshua Bengio. Topmoumoute online natural
gradient algorithm. In J.C. Platt  D. Koller  Y. Singer  and S. Roweis  editors  Advances in
Neural Information Processing Systems 20  pages 849–856. MIT Press  Cambridge  MA  2008.
[15] Kai Yu  John Lafferty  Shenghuo Zhu  and Yihong Gong. Large-scale collaborative prediction
using a nonparametric random effects model. In Proceedings of the 26th Annual International
Conference on Machine Learning  pages 1185–1192. ACM  2009.

[16] Liang Zhang  Deepak Agarwal  and Bee-Chung Chen. Generalizing matrix factorization
In Proceedings of the ﬁfth ACM conference on Recom-

through ﬂexible regression priors.
mender systems  pages 13–20. ACM  2011.

9

,Remi Gribonval
Pierre Machart
Yanping Huang
Rajesh Rao