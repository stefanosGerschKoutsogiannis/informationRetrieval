2012,Pointwise Tracking the Optimal Regression Function,This paper examines the possibility of a `reject option' in the context of least squares regression. It is shown that using rejection it is theoretically possible to learn `selective' regressors that can $\epsilon$-pointwise track the best regressor in hindsight from the same hypothesis class  while rejecting only a bounded portion of the domain. Moreover  the rejected volume vanishes with the training set size  under certain conditions. We then develop efficient and exact implementation of these selective regressors for the case of linear regression. Empirical evaluation over a suite of real-world datasets corroborates the theoretical analysis and indicates that our selective regressors can provide substantial advantage by reducing estimation error.,Pointwise Tracking the Optimal Regression Function

Ran El-Yaniv and Yair Wiener
Computer Science Department

Technion – Israel Institute of Technology

{rani wyair}@{cs tx}.technion.ac.il

Abstract

This paper examines the possibility of a ‘reject option’ in the context of least
squares regression. It is shown that using rejection it is theoretically possible to
learn ‘selective’ regressors that can ǫ-pointwise track the best regressor in hind-
sight from the same hypothesis class  while rejecting only a bounded portion of
the domain. Moreover  the rejected volume vanishes with the training set size 
under certain conditions. We then develop efﬁcient and exact implementation of
these selective regressors for the case of linear regression. Empirical evaluation
over a suite of real-world datasets corroborates the theoretical analysis and indi-
cates that our selective regressors can provide substantial advantage by reducing
estimation error.

1 Introduction

Consider a standard least squares regression problem. Given m input-output training pairs 
(x1  y1)  . . .   (xm  ym)  we are required to learn a predictor  ˆf ∈ F  capable of generating accurate
output predictions  ˆf (x) ∈ R  for any input x. Assuming that input-output pairs are i.i.d. realiza-
tions of some unknown stochastic source  P (x  y)  we would like to choose ˆf so as to minimize the
standard least squares risk functional 

R( ˆf ) =Z (y − ˆf (x))2dP (x  y).

Let f ∗ = argminf ∈F R(f ) be the optimal predictor in hindsight (based on full knowledge of P ).
A classical result in statistical learning is that under certain structural conditions on F and possibly
on P   one can learn a regressor that approaches the average optimal performance  R(f ∗)  when the
sample size  m  approaches inﬁnity [1].
In this paper we contemplate the challenge of pointwise tracking the optimal predictions of f ∗ after
observing only a ﬁnite (and possibly small) set of training samples. It turns out that meeting this
difﬁcult task can be made possible by harnessing the ‘reject option’ compromise from classiﬁcation.
Instead of predicting the output for the entire input domain  the regressor is allowed to abstain from
prediction for part of the domain. We present here new techniques for regression with a reject
option  capable of achieving pointwise optimality on substantial parts of the input domain  under
certain conditions.

Section 3 introduces a general strategy for learning selective regressors. This strategy is guaranteed
to achieve ǫ-pointwise optimality (deﬁned in Section 2) all through its region of action. This result
is proved in Theorem 3.8  which also shows that the guaranteed coverage increases monotonically
with the training sample size and converges to 1. This type of guarantee is quite strong  as it ensures
tight tracking of individual optimal predictions made by f ∗  while covering a substantial portion of
the input domain.

At the outset  the general strategy we propose appears to be out of reach because accept/reject
decisions require the computation of a supremum over a a very large  and possibly inﬁnite hypothesis

1

subset.
In Section 4  however  we show how to compute the strategy for each point of interest
using only two constrained ERM calculations. This useful reduction  shown in Lemma 4.2  opens
possibilities for efﬁcient implementations of optimal selective regressors whenever the hypothesis
class of interest allows for efﬁcient (constrained) ERM (see Deﬁnition 4.1).

For the case of linear least squares regression we utilize known techniques for both ERM and con-
strained ERM and derive in Section 5 exact implementation achieving pointwise optimal selective
regression. The resulting algorithm is efﬁcient and can be easily implemented using standard matrix
operations including (pseudo) inversion. Theorem 5.3 in this section states a novel pointwise bound
on the difference between the prediction of an ERM linear regressor and the prediction of f ∗ for
each individual point. Finally  in Section 6 we present numerical examples over a suite of real-world
regression datasets demonstrating the effectiveness of our methods  and indicating that substantial
performance improvements can be gained by using selective regression.
Related work. Utilizations of a reject option are quite common in classiﬁcation where this technique
was initiated more than 50 years ago with Chow’s pioneering work [2  3]. However  the reject
option is only scarcely and anecdotally mentioned in the context of regression. In [4] a boosting
algorithm for regression is proposed and a few reject mechanisms are considered  applied both
on the aggregate decision and/or on the underlying weak regressors. A straightforward threshold-
based reject mechanism (rejecting low response values) is applied in [5] on top of support vector
regression. This mechanism was found to improve false positive rates.

The present paper is inspired and draws upon recent results on selective classiﬁcation [6  7  8] 
and can be viewed as a natural continuation of the results of [8]. In particular  we adapt the basic
deﬁnitions of selectivity and the general outline of the derivation and strategy presented in [8].

2 Selective regression and other preliminary deﬁnitions

We begin with a deﬁnition of the following general and standard regression setting. A ﬁnite training
sample of m labeled examples  Sm   {(xi  yi)}m
i=1 ⊆ (X × Y)m  is observed  where X is some
feature space and Y ⊆ R. Using Sm we are required to select a regressor ˆf ∈ F  where F is a ﬁxed
hypothesis class containing potential regressors of the form f : X → Y. It is desired that predictions
ˆf (x)  for unseen instances x  will be as accurate as possible. We assume that pairs (x  y)  including
training instances  are sampled i.i.d. from some unknown stochastic source  P (x  y)  deﬁned over
X × Y. Given a loss function  ℓ : Y × Y → [0 ∞)  we quantify the prediction quality of any f
through its true error or risk  R(f )  deﬁned as its expected loss with respect to P  

R(f )   E(x y) {ℓ(f (x)  y)} =Z ℓ(f (x)  y)dP (x  y).

While R(f ) is an unknown quantity  we do observe the empirical error of f   deﬁned as

ˆR(f )   1
m

m

Xi=1

ℓ(f (xi)  yi).

Let ˆf   arg inf f ∈F ˆR(f ) be the empirical risk minimizer (ERM)  and f ∗   arg inf f ∈F R(f )  the
true risk minimizer.

Next we deﬁne selective regression using the following deﬁnitions  which are taken  as is  from the
selective classiﬁcation setting of [6]. Here again  we are given a training sample Sm as above  but
are now required to output a selective regressor deﬁned to be a pair (f  g)  with f ∈ F being a
standard regressor  and g : X → {0  1} is a selection function  which is served as qualiﬁer for f as
follows. For any x ∈ X  

(1)

(f  g)(x)  (cid:26) reject 

f (x) 

if g(x) = 0;
if g(x) = 1.

Thus  the selective regressor abstains from prediction at a point x iff g(x) = 0. The general perfor-
mance of a selective regressor is characterized in terms of two quantities: coverage and risk. The
coverage of (f  g) is

Φ(f  g)   EP [g(x)] .

2

The true risk of (f  g) is the risk of f restricted to its region of activity as qualiﬁed by g  and
normalized by its coverage 

R(f  g)  

EP [ℓ(f (x)  y) · g(x)]

Φ(f  g)

.

We say that the selective regressor (f  g) is ǫ-pointwise optimal if

∀x ∈ {x ∈ X : g(x) = 1}  

|f (x) − f ∗(x)| ≤ ǫ.

Note that pointwise optimality is a considerably stronger property than risk  which only refers to
average performance.
We deﬁne a (standard) distance metric over the hypothesis class F. For any probability measure µ
on X   let L2(µ) be the Hilbert space of functions from X to R  with the inner product deﬁned as

The distance function induced by the inner product is

hf  gi   Eµ(x)f (x)g(x).

ρ(f  g)  k f − g k=phf − g  f − gi =qEµ(x) (f (x) − g(x))2.

Finally  for any f ∈ F we deﬁne a ball in F of radius r around f  
B(f  r)   {f ′ ∈ F : ρ(f  f ′) ≤ r} .

3 Pointwise optimality with bounded coverage

In this section we analyze the following strategy for learning a selective regressor  which turns out
to ensure ǫ-pointwise optimality with monotonically increasing coverage (with m). We call it a
strategy rather than an algorithm because it is not at all clear at the outset how to implement it. In
subsequent sections we develop efﬁcient and precise implementation for linear regression.
We require the following deﬁnition. For any hypothesis class F  target hypothesis f ∈ F  distribu-
tion P   sample Sm  and real r > 0  deﬁne 
ˆV(f  r)  nf ′ ∈ F : ˆR(f ′) ≤ ˆR(f ) + ro .
V(f  r)   {f ′ ∈ F : R(f ′) ≤ R(f ) + r}

and

(2)

Strategy 1 A learning strategy for ǫ-pointwise optimal selective regressors
Input: Sm  m  δ  F  ǫ
Output: A selective regressor ( ˆf   g) achieving ǫ-pointwise optimality
1: Set ˆf = ERM (F  Sm)  i.e.  ˆf is any empirical risk minimizer from F
2: Set G = ˆV “ ˆf  `σ(m  δ/4  F)2 − 1´ · ˆR( ˆf )”
3: Construct g such that g(x) = 1 ⇐⇒ ∀f ′ ∈ G |f ′(x) − ˆf (x)| < ǫ

/* see Deﬁnition 3.3 and (2) */

For the sake of brevity  throughout this section we often write f instead of f (x)  where f is any
regressor. The following Lemma 3.1 is based on the proof of Lemma A.12 in [9].
Lemma 3.1 ([9]). For any f ∈ F. Let ℓ : Y × Y → [0 ∞) be the squared loss function and F be
a convex hypothesis class. Then  E(x y)(f ∗(x) − y)(f (x) − f ∗(x)) ≥ 0.
Lemma 3.2. Under the same conditions of Lemma 3.1  for any r > 0  V(f ∗  r) ⊆ B (f ∗ √r) .
Proof. If f ∈ V(f ∗  r)  then by deﬁnition 

R(f ) ≤ R(f ∗) + r.

(3)

R(f ) − R(f ∗) = E {ℓ(f  y) − ℓ(f ∗  y)} = E(cid:8)(f − y)2 − (f ∗ − y)2(cid:9)
Applying Lemma 3.1 and (3) we get  ρ(f  f ∗) ≤pR(f ) − R(f ∗) ≤ √r.

= En(f − f ∗)2 − 2(y − f ∗)(f − f ∗)o = ρ2(f  f ∗) + 2E(f ∗ − y)(f − f ∗).

3

Deﬁnition 3.3 (Multiplicative Risk Bounds). Let σδ   σ (m  δ F ) be deﬁned such that for any
0 < δ < 1  with probability of at least 1 − δ over the choice of Sm from P m  any hypothesis f ∈ F
satisﬁes

R(f ) ≤ ˆR(f ) · σ (m  δ F ) .

Similarly  the reverse bound   ˆR(f ) ≤ R(f ) · σ (m F   δ)  holds under the same conditions.
Remark 3.1. The purpose of Deﬁnition 3.3 is to facilitate the use of any (known) risk bound as a
plug-in component in subsequent derivations. We deﬁne σ as a multiplicative bound  which is com-
mon in the treatment of unbounded loss functions such as the squared loss (see discussion by Vapnik
in [10]  page 993). Instances of such bounds can be extracted  e.g.  from [11] (Theorem 1)  and from
bounds discussed in [10]. We also developed the entire set of results that follow while relying on
additive bounds  which are common when using bounded loss functions. These developments will
be presented in the full version of the paper.

The proof of the following lemma follows closely the proof of Lemma 5.3 in [8]. However  it
considers a multiplicative risk bound rather than additive.
Lemma 3.4. For any r > 0  and 0 < δ < 1  with probability of at least 1 − δ 
δ/2 − 1) · R(f ∗) + r · σδ/2(cid:17) .
Lemma 3.5. Let F be a convex hypothesis space  ℓ : Y × Y → [0 ∞)  a convex loss function  and
ˆf be an ERM. Then  with probability of at least 1 − δ/2  for any x ∈ X  

ˆV( ˆf   r) ⊆ V(cid:16)f ∗  (σ2

|f ∗(x) − ˆf (x)| ≤

f ∈ ˆV“ ˆf  (σ2

sup
δ/4−1)· ˆR( ˆf )”

|f (x) − ˆf (x)|.

Proof. Applying the multiplicative risk bound  we get that with probability of at least 1 − δ/4 

ˆR(f ∗) ≤ R(f ∗) · σδ/4.

Since f ∗ minimizes the true error  R(f ∗) ≤ R( ˆf ). Applying the multiplicative risk bound on ˆf 
we know also that with probability of at least 1 − δ/4  R( ˆf ) ≤ ˆR( ˆf ) · σδ/4. Combining the three
inequalities by using the union bound we get that with probability of at least 1 − δ/2 

ˆR(f ∗) ≤ ˆR( ˆf ) · σ2

δ/4 = ˆR( ˆf ) +(cid:16)σ2

δ/4 − 1(cid:17) · ˆR( ˆf ).

Hence  with probability of at least 1 − δ/2 we get f ∗ ∈ ˆV(cid:16) ˆf   (σ2
Let G ⊆ F. We generalize the concept of disagreement set [12  6] to real-valued functions. The
ǫ-disagreement set w.r.t. G is deﬁned as

δ/4 − 1) · ˆR( ˆf )(cid:17)

DISǫ(G)   {x ∈ X : ∃f1  f2 ∈ G s.t.

|f1(x) − f2(x)| ≥ ǫ} .

For any G ⊆ F  distribution P   and ǫ > 0  we deﬁne ∆ǫG   P rP {DISǫ(G)} . In the following
deﬁnition we extend Hanneke’s disagreement coefﬁcient [13] to the case of real-valued functions.1
Deﬁnition 3.6 (ǫ-disagreement coefﬁcient). The ǫ-disagreement coefﬁcient of F under P is 

θǫ   sup
r>r0

∆ǫB(f ∗  r)

r

.

(4)

Throughout this paper we set r0 = 0. Our analyses for arbitrary r0 > 0 will be presented in the full
version of this paper.

The proof of the following technical statement relies on the same technique used for the proof of
Theorem 5.4 in [8].

1Our attemps to utilize a different known extension of the disagreement coefﬁcient [14] were not successful.

Speciﬁcally  the coefﬁcient proposed there is unbounded for the squared loss function when Y is unbounded.

4

Lemma 3.7. Let F be a convex hypothesis class  and assume ℓ : Y × Y → [0 ∞) is the squared
loss function. Let ǫ > 0 be given. Assume that F has ǫ-disagreement coefﬁcient θǫ. Then  for any
r > 0 and 0 < δ < 1  with probability of at least 1 − δ 

∆ǫ ˆV( ˆf   r) ≤ θǫr(cid:16)σ2

δ/2 − 1(cid:17) · R(f ∗) + r · σδ/2.

The following theorem is the main result of this section  showing that Strategy 1 achieves ǫ-pointwise
optimality with a meaningful coverage that converges to 1. Although R(f ∗) in the bound (5) is an
unknown quantity  it is still a constant  and as σ approaches 1  the coverage lower bound approaches
1 as well. When using a typical additive risk bound  R(f ∗) disappears from the RHS.
Theorem 3.8. Assume the conditions of Lemma 3.7 hold. Let (f  g) be the selective regressor chosen
by Strategy 1. Then  with probability of at least 1 − δ 

and

Φ(f  g) ≥ 1 − θǫr(cid:16)σ2
∀x ∈ {x ∈ X : g(x) = 1}

δ/4 − 1(cid:17) ·(cid:16)R(f ∗) + σδ/4 · ˆR( ˆf )(cid:17)

|f (x) − f ∗(x)| < ǫ.

(5)

Proof. According to Strategy 1  if g(x) = 1 then sup
Applying Lemma 3.5 we get that  with probability of at least 1 − δ/2 

f ∈ ˆV( ˆf  “σ2

δ/4−1”· ˆR( ˆf )) |f (x) − ˆf (x)| < ǫ.

Since ˆf ∈ ˆV(cid:16) ˆf   (σ2

∀x ∈ {x ∈ X : g(x) = 1}
δ/4 − 1) · ˆR( ˆf )(cid:17) = G wet get
Φ(f  g) = E{g(X)} = E(I sup

|f (x) − f ∗(x)| < ǫ.

f ∈G |f (x) − ˆf (x)| < ǫ!)

= 1 − E(I sup
≥ 1 − E(I  sup

f ∈G |f (x) − ˆf (x)| ≥ ǫ!)
f1 f2∈G|f1(x) − f2(x)| ≥ ǫ!) = 1 − ∆ǫG.

Applying Lemma 3.7 and the union bound we conclude that with probability of at least 1 − δ 

Φ(f  g) = E{g(X)} ≥ 1 − θǫr(cid:16)σ2

δ/4 − 1(cid:17) ·(cid:16)R(f ∗) + σδ/4 · ˆR( ˆf )(cid:17).

4 Rejection via constrained ERM

In Strategy 1 we are required to track the supremum of a possibly inﬁnite hypothesis subset  which
in general might be intractable. The following Lemma 4.2 reduces the problem of calculating the
supremum to a problem of calculating a constrained ERM for two hypotheses.
Deﬁnition 4.1 (constrained ERM). Let x ∈ X and ǫ ∈ R be given. Deﬁne 
f (x) = ˆf (x) + ǫo  

f ∈F n ˆR(f )

ˆfǫ x   argmin

where ˆf (x) is  as usual  the value of the unconstrained ERM regressor at point x.
Lemma 4.2. Let F be a convex hypothesis space  and ℓ : Y × Y → [0 ∞)  a convex loss function.
Let ǫ > 0 be given  and let (f  g) be a selective regressor chosen by Strategy 1 after observing the
training sample Sm. Let ˆf be an ERM. Then 

|

g(x) = 0 ⇔ ˆR( ˆfǫ x) ≤ ˆR( ˆf ) · σ2

δ/4 ∨

ˆR( ˆf−ǫ x) ≤ ˆR( ˆf ) · σ2

δ/4.

5

Proof. Let G   ˆV(cid:16) ˆf   (σ2
ǫ. Assume w.l.o.g. (the other case is symmetric) that f (x) − ˆf (x) = a ≥ ǫ. Since F is convex 

δ/4 − 1) · ˆR( ˆf )(cid:17)  and assume there exists f ∈ G such that |f (x)− ˆf (x)| ≥

We thus have 

f ′ =(cid:16)1 −

ǫ

a(cid:17) · ˆf +

ǫ
a · f ∈ F.

f ′(x) =(cid:16)1 −

ǫ

a(cid:17) · ˆf (x) +

ǫ

a · f (x) =(cid:16)1 −

ǫ

a(cid:17) · ˆf (x) +

ǫ

a ·(cid:16) ˆf (x) + a(cid:17) = ˆf (x) + ǫ.

Therefore  by the deﬁnition of ˆfǫ x  and using the convexity of ℓ  together with Jensen’s inequality 

ǫ

a(cid:17) · ˆf (xi) +

ǫ

a · f (xi)  yi(cid:17)

m

1
m

m

ˆR( ˆfǫ x) ≤ ˆR(f ′) =
Xi=1
ǫ
≤ (cid:16)1 −
a(cid:17) ·
Xi=1
a(cid:17) · ˆR( ˆf ) +
= (cid:16)1 −
a ·(cid:16)σ2

= ˆR( ˆf ) +

1
m

ǫ

ǫ

ℓ (f (xi)  yi)

m

1
m

ǫ

m

1
m
ǫ

ℓ(f ′(xi)  yi) =

Xi=1
ǫ
ℓ(cid:16) ˆf (xi)  yi(cid:17) +
a ·
a · ˆR(f ) ≤(cid:16)1 −

ℓ(cid:16)(cid:16)1 −
Xi=1
a(cid:17) · ˆR( ˆf ) +
δ/4. Then ˆfǫ x ∈ G and(cid:12)(cid:12)(cid:12)

δ/4 − 1(cid:17) · ˆR( ˆf ) ≤ ˆR( ˆf ) · σ2

δ/4.

ǫ

As for the other direction  if ˆR( ˆfǫ x) ≤ ˆR( ˆf ) · σ2

δ/4(cid:17)
a ·(cid:16) ˆR( ˆf ) · σ2
ˆfǫ x(x) − ˆf (x)(cid:12)(cid:12)(cid:12)

= ǫ.

So far we have discussed the case where ǫ is given  and our objective is to ﬁnd an ǫ-pointwise
optimal regressor. Lemma 4.2 provides the means to compute such an optimal regressor assuming
that a method to compute a constrained ERM is available (as is the case for squared loss linear
regressors ; see next section). However  as was discussed in [6]  in many cases our objective is to
explore the entire risk-coverage trade-off  in other words  to get a pointwise bound on |f ∗(x)−f (x)| 
i.e.  individually for any test point x. The following theorem states such a pointwise bound.
Theorem 4.3. Let F be a convex hypothesis class  ℓ : Y × Y → [0 ∞)  a convex loss function  and
let ˆf be an ERM. Then  with probability of at least 1 − δ/2 over the choice of Sm from P m   for any
x ∈ X  

Proof. Deﬁne ˜f  

argmax

f ∈ ˆV“ ˆf  (σ2

δ/4−1)· ˆR( ˆf )”

|f ∗(x) − ˆf (x)| ≤ sup

δ/4o .

ǫ∈Rn|ǫ| : ˆR( ˆfǫ x) ≤ ˆR( ˆf ) · σ2
|f (x)− ˆf (x)|. Assume w.l.o.g (the other case is symmetric)
δ/4. Deﬁne

that ˜f (x) = ˆf (x) + a. Following Deﬁnition 4.1 we get ˆR( ˆfa x) ≤ ˆR( ˜f ) ≤ ˆR( ˆf ) · σ2
ǫ′ = supǫ∈Rn|ǫ| : ˆR( ˆfǫ x) ≤ ˆR( ˆf ) · σ2

δ/4o . We thus have 

f ∈ ˆV“ ˆf  (σ2

sup
δ/4−1)· ˆR( ˆf)”

|f (x) − ˆf (x)| = a ≤ ǫ′.

An application of Lemma 3.5 completes the proof.

We conclude this section with a general result on the monotonicity of the empirical risk attained by
constrained ERM regressors. This property  which will be utilized in the next section  can be easily
proved using a simple application of Jensen’s inequality.
Lemma 4.4 (Monotonicity). Let F be a convex hypothesis space  ℓ : Y × Y → [0 ∞)  a convex
ǫ2 (cid:16) ˆR( ˆfǫ2 x0) − ˆR( ˆf )(cid:17) . The
loss function  and 0 ≤ ǫ1 < ǫ2  be given. Then  ˆR(fǫ1 x0) − ˆR( ˆf ) ≤ ǫ1
result also holds for the case 0 ≥ ǫ1 > ǫ2.

6

5 Selective linear regression

We now restrict attention to linear least squares regression (LLSR)  and  relying on Theorem 4.3 and
Lemma 4.4  as well as on known closed-form expressions for LLSR  we derive efﬁcient implemen-
tation of Strategy 1 and a new pointwise bound. Let X be an m × d training sample matrix whose
ith row  xi ∈ Rd  is a feature vector. Let y ∈ Rm be a column vector of training labels.
Lemma 5.1 (ordinary least-squares estimate [15]). The ordinary least square (OLS) solution of
the following optimization problem  minβ kXβ − yk2  is given by ˆβ   (X T X)+X T y  where the
sign + represents the pseudoinverse.
Lemma 5.2 (constrained least-squares estimate [15]  page 166). Let x0 be a row vector and c a
label. The constrained least-squares (CLS) solution of the following optimization problem

minimize kXβ − yk2
0 (x0(X T X)+xT

s.t x0β = c 

0 )+(cid:16)c − x0 ˆβ(cid:17)   where ˆβ is the OLS solution.
is given by ˆβC (c)   ˆβ + (X T X)+xT
Theorem 5.3. Let F be the class of linear regressors  and let ˆf be an ERM. Then  with probability
of at least 1 − δ over choices on Sm  for any test point x0 we have 
|f ∗(x0) − ˆf (x0)| ≤ kX ˆβ − yk
Proof. According to Lemma 4.4  for squared loss  ˆR( ˆfǫ x0) is strictly monotonically increasing for
ǫ > 0  and decreasing for ǫ < 0. Therefore  the equation  ˆR( ˆfǫ x0) = ˆR( ˆf ) · σ2
δ/4  where ǫ is the
unknown  has precisely two solutions for any σ > 1. Denoting these solutions by ǫ1  ǫ2 we get 

kXKk qσ2

where K = (X T X)+xT

0 (x0(X T X)+xT

δ/4 − 1 

0 )+.

sup

ǫ∈Rn|ǫ| : ˆR( ˆfǫ x0) ≤ ˆR( ˆf ) · σ2

δ/4o = max(|ǫ1| |ǫ2|).

1

Applying Lemma 5.1 and 5.2 and setting c = X0 ˆβ + ǫ  we obtain 
1
mkX ˆβ − yk2 · σ2

Hence  kX ˆβ + XKǫ − yk2 = kX ˆβ − yk2 · σ2
yk2 · (σ2

mkX ˆβC(cid:16)x0 ˆβ + ǫ(cid:17) − yk2 = ˆR( ˆfǫ x0) = ˆR( ˆf ) · σ2
δ/4 − 1). We note that by applying Lemma 5.1 on (X ˆβ − y)T X  we get 
(X ˆβ − y)T X =(cid:0)X T (cid:0)X(X T X)+X T y − y(cid:1)(cid:1)T

= (X T y − X T y)T = 0.
δ/4 − 1). Application of Theorem 4.3 completes the proof.

δ/4 =

kXKk2

δ/4.

δ/4  so  2(X ˆβ − y)T XKǫ + kXKk2ǫ2 = kX ˆβ −

Therefore  ǫ2 = kX ˆβ−yk2

· (σ2
6 Numerical examples

Focusing on linear least squares regression  we empirically evaluated the proposed method. Given a
labeled dataset we randomly extracted two disjoint subsets: a training set Sm  and a test set Sn. The
selective regressor (f  g) is computed as follows. The regressor f is an ERM over Sm  and for any
coverage value Φ  the function g selects a subset of Sn of size n · Φ  including all test points with
lowest value of the bound in Theorem 5.3.2
We compare our method relative to the following simple and natural 1-nearest neighbor (NN) tech-
nique for selection. Given the training set Sm and the test set Sn  let N N (x) denote the nearest

neighbor of x in Sm  with corresponding ρ(x)   pkN N (x) − xk2 distance to x. These ρ(x)
distances  corresponding to all x ∈ Sn  were used as alternative method to reject test points in
decreasing order of their ρ(x) values.
We tested the algorithm on 10 of the 14 LIBSVM [16] regression datasets. From this repository we
took all sets that are not too small and have reasonable feature dimensionality.3 Figure 1 depicts

2We use here the theorem only for ranking test points  so any constant > 1 can be used instead of σ2
3Two datasets having less than 200 samples  and two that have over 150 000 features were excluded.

δ/4.

7

results obtained for ﬁve different datasets  each with training sample size m = 30  and test set size
n = 200. The ﬁgure includes a matrix of 2× 5 graphs. Each column corresponds to a single dataset.
Each of the graphs on the ﬁrst row shows the average absolute difference between the selective
regressor (f  g) and the optimal regressor f ∗ (taken as an ERM over the entire dataset) as a function
of coverage  where the average is taken over the accepted instances. Our method appears in solid
red line  and the baseline NN method  in dashed black line. Each curve point is an average over 200
independent trials (error bars represent standard error of the mean). It is evident that for all datasets
the average distance monotonically increases with coverage. Furthermore  in all cases the proposed
method signiﬁcantly outperforms the NN baseline.

x 10−3

bodyfat

1.64

1.29

|
f

x 10 4

cadata

3.99

3.04

|
f

1.29

0.85

|
f

x 10 1

cpusmall

3.93

2.90

|
f

x 10 0

housing

x 10−2

space

5.66

4.25

|
f

−

*

f
|

−

*

f
|

−

*

f
|

−

*

f
|

−

*

f
|

1.01

0.80
0

2.38

1.31

0.72

0.39
0

)
g

 
f
(

R

2.33

0.56

2.14

3.19

0.5
c

1

1.78
0

0.5
c

1

0.37
0

0.5
c

1

1.58
0

0.5
c

1

2.39
0

0.5
c

x 10−5

bodyfat

x 10 9

cadata

x 10 3

cpusmall

x 10 1

housing

x 10−2

space

9.47

)
g

 
f
(

R

6.46

4.40

4.32

0.61

0.09

)
g

 
f
(

R

6.81

)
g

 
f
(

R

3.59

1.89

2.33

1.72

1.27

)
g

 
f
(

R

0.5
c

1

3.00
0

0.5
c

1

0.01
0

0.5
c

1

1.00
0

0.5
c

1

0.94
0

0.5
c

1

1

Figure 1: (top row) absolute difference between the selective regressor (f  g) and the optimal re-
gressor f ∗. (bottom row) test error of selective regressor (f  g). Our proposed method in solid red
line and the baseline method in dashed black line. In all curves the y-axis has logarithmic scale.

Each of the graphs in the second row shows the test error of the selective regressor (f  g) as a function
of coverage. This curve is known as the RC (risk-coverage) trade-off curve [6]. In this case we see
again that the test error is monotonically increasing with coverage. In four datasets out of the ﬁve
we observe a clear domination of the entire RC curve  and in one dataset the performance of our
method is statistically indistinguishable from that of the NN baseline method.

7 Concluding remarks

Rooted in the centuries-old linear least squares method of Gauss and Legendre  regression estima-
tion remains an indispensable routine in statistical analysis  modeling and prediction. This paper
proposes a novel rejection technique allowing for a least squares regressor  learned from a ﬁnite and
possibly small training sample  to pointwise track  within its selected region of activity  the predic-
tions of the globally optimal regressor in hindsight (from the same class). The resulting algorithm 
which is motivated and derived entirely from the theory  is efﬁcient and practical.

Immediate plausible extensions are the handling of other types of regressions including regularized 
and kernel regression  as well as extensions to other convex loss functions such as the epsilon-
insensitive loss. The presence of the ǫ-disagreement coefﬁcient in our coverage bound suggests a
possible relation to active learning  since the standard version of this coefﬁcient has a key role in
characterizing the efﬁciency of active learning in classiﬁcation [17]. Indeed  a formal reduction of
active learning to selective classiﬁcation was recently found  whereby rejected points are precisely
those points to be queried in a stream based active learning setting. Moreover  “fast” coverage
bounds in selective classiﬁcation give rise to fast rates in active learning [7]. Borrowing their in-
tuition to our setting  one could consider devising a querying function for active regression that is
based on the pointwise bound of Theorem 5.3.

Acknowledgments

The research leading to these results has received funding from both Intel and the European Union’s
Seventh Framework Programme under grant agreement n◦ 216886.

8

References

[1] V. Vapnik. Statistical learning theory. 1998. Wiley  New York  1998.
[2] C.K. Chow. An optimum character recognition system using decision function. IEEE Trans.

Computer  6(4):247–254  1957.

[3] C.K. Chow. On optimum recognition error and reject trade-off. IEEE Trans. on Information

Theory  16:41–36  1970.

[4] B. K´egl. Robust regression by boosting the median. Learning Theory and Kernel Machines 

[5]

pages 258–272  2003.
¨O. Ays¸eg¨ul  G. Mehmet  A. Ethem  and H. T¨urkan. Machine learning integration for predicting
the effect of single amino acid substitutions on protein stability. BMC Structural Biology  9.

[6] R. El-Yaniv and Y. Wiener. On the foundations of noise-free selective classiﬁcation. The

Journal of Machine Learning Research  11:1605–1641  2010.

[7] R. El-Yaniv and Y. Wiener. Active learning via perfect selective classiﬁcation. Journal of

Machine Learning Research  13:255–279  2012.

[8] R. El-Yaniv and Y. Wiener. Agnostic selective classiﬁcation. In Neural Information Processing

Systems (NIPS)  2011.

[9] W.S. Lee. Agnostic Learning and Single Hidden Layer Neural Networks. PhD thesis  Aus-

tralian National University  1996.

[10] V.N. Vapnik. An overview of statistical learning theory. Neural Networks  IEEE Transactions

on  10(5):988–999  1999.

[11] R.M. Kil and I. Koo. Generalization bounds for the regression of real-valued functions. In
Proceedings of the 9th International Conference on Neural Information Processing  volume 4 
pages 1766–1770  2002.

[12] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML  pages

353–360  2007.

[13] S. Hanneke. Theoretical Foundations of Active Learning. PhD thesis  Carnegie Mellon Uni-

versity  2009.

[14] A. Beygelzimer  S. Dasgupta  and J. Langford. Importance weighted active learning. In ICML
’09: Proceedings of the 26th Annual International Conference on Machine Learning  pages
49–56. ACM  2009.

[15] J.E. Gentle. Numerical linear algebra for applications in statistics. Springer Verlag  1998.
[16] C.C. Chang and C.J. Lin. LIBSVM: A library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology  2:27:1–27:27  2011. Software available at
”http://www.csie.ntu.edu.tw/ cjlin/libsvm”.

[17] S. Hanneke. Rates of convergence in active learning. The Annals of Statistics  39(1):333–361 

2011.

9

,Will Hamilton
Payal Bajaj
Marinka Zitnik
Dan Jurafsky
Jure Leskovec