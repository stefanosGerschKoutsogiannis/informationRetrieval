2013,Non-Linear Domain Adaptation with Boosting,A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However  there are many problems when this assumption is grossly violated  as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data  for which annotation is very time-consuming  limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multi-task learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-specific decision boundaries  our method learns a single decision boundary in a shared feature space  common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task  with no need for specific a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a significant improvement over the state-of-the-art.,Non-Linear Domain Adaptation with Boosting

Carlos Becker∗

C. Mario Christoudias

Pascal Fua

CVLab  ´Ecole Polytechnique F´ed´erale de Lausanne  Switzerland

firstname.lastname@epfl.ch

Abstract

A common assumption in machine vision is that the training and test samples
are drawn from the same distribution. However  there are many problems when
this assumption is grossly violated  as in bio-medical applications where differ-
ent acquisitions can generate drastic variations in the appearance of the data due
to changing experimental conditions. This problem is accentuated with 3D data 
for which annotation is very time-consuming  limiting the amount of data that
can be labeled in new acquisitions for training. In this paper we present a multi-
task learning algorithm for domain adaptation based on boosting. Unlike previous
approaches that learn task-speciﬁc decision boundaries  our method learns a sin-
gle decision boundary in a shared feature space  common to all tasks. We use
the boosting-trick to learn a non-linear mapping of the observations in each task 
with no need for speciﬁc a-priori knowledge of its global analytical form. This
yields a more parameter-free domain adaptation approach that successfully lever-
ages learning on new tasks where labeled data is scarce. We evaluate our approach
on two challenging bio-medical datasets and achieve a signiﬁcant improvement
over the state of the art.

1

Introduction

Object detection and segmentation approaches often assume that the training and test samples are
drawn from the same distribution. There are many problems in Computer Vision  however  where
this assumption can be grossly violated  such as in bio-medical applications that usually involve
expensive and complicated data acquisition processes that are not easily repeatable. As illustrated
in Fig. 1  this can result in newly-acquired data that is signiﬁcantly different from the data used for
training. As a result  a classiﬁer trained on data from one acquisition often cannot generalize well to
data obtained from a new one. Furthermore  although it is possible to expect supervised data from
a new acquisition  it is unreasonable to expect the practitioner to re-label large amounts of data for
each new image that is acquired  particularly in the case of 3D image stacks.
A possible solution is to treat each acquisition as a separate  but related classiﬁcation problem  and
exploit their possible relationship to learn from the supervised data available across all of them.
Typically  each such classiﬁcation problem is called a task  which is associated with a domain.
For example  for Fig. 1(a b) the task is mitochondria segmentation in both acquisitions. However 
the domains are different  namely Striatum and Hippocampus EM stacks. Techniques in domain
adaptation [1] and more generally multi-task learning [2  3] seek to leverage data from a set of
different yet related tasks or domains to help learn a classiﬁer in a seemingly new task. In domain
adaptation  it is typically assumed that there is a fairly large amount of labeled data in one domain 
commonly referred to as the source domain  and that a limited amount of supervision is available in
the other  often called the target domain. Our goal is to exploit the labeled data in the source domain
to learn an accurate classiﬁer in the target domain despite having only a few labeled samples in the
latter.

∗This work was supported in part by the ERC grant MicroNano.

1

Mitochondria Segmentation

(3D stacks)

Path Classiﬁcation

(2D images to 3D stacks)

(a) Striatum

(b) Hippocampus

(c) Aerial road images

(d) Neural Axons (OPF)

Figure 1: (a b) Slice cuts from two 3D Electron Microscopy acquisitions from different parts of the
brain of a rat. (c d) 2D aerial road images and 3D neural axons from Olfactory Projection Fibers
(OPF). Top and bottom rows show example images and ground truth respectively.

The data acquisition problem is unique to many multi-task learning problems  however  in that each
task is in fact the same  but what has changed is that the features across different acquisitions have
undergone some unknown transformation. That is to say that each task can be well described by a
single decision boundary in some common feature space that preserves the task-relevant features and
discards the domain speciﬁc ones corresponding to unwanted acquisition artifacts. This contrasts the
more general multi-task setting where each task is comprised of both a common and task-speciﬁc
boundary  even when mapped to a common feature space  as illustrated in Fig. 2. A method that can
jointly optimize over the common decision boundary and shared feature space is therefore desired.
Linear latent variable methods such as those based on Canonical Correlation Analysis (CCA) [4 
5] can be applied to learn a shared feature space across the different acquisitions. However  the
situation is further complicated by the fact that the unknown transformations are generally non-
linear. Although kernel methods can be applied to model the non-linearity [4  6  7]  this requires
the existence of a well-deﬁned kernel function that can often be difﬁcult to specify a priori. Also 
the computational complexity of kernel methods scales quadratically with the number of training
examples  limiting their application to large datasets.
In this paper we propose a solution to the data acquisition problem and devise a method that can
jointly solve for the non-linear decision boundary and transformations across tasks. As illustrated
in Fig. 2 our approach maps features from possibly high-dimensional  task-speciﬁc feature spaces
to a low-dimensional space common to all tasks. We assume that only the mappings are task-
dependent and that in the shared space the problem is linearly separable and the decision boundary
is common to all tasks. We use the boosting-trick [8  9  10] to simultaneously learn the non-linear
task-speciﬁc mappings as well as the decision boundary  with no need for speciﬁc a-priori knowledge
of their global analytical form. This yields a more parameter-free domain adaptation approach that
successfully leverages learning on new tasks where labeled data is scarce.
We evaluate our approach on the two challenging bio-medical datasets depicted by Fig. 1. We
ﬁrst consider the classiﬁcation of curvilinear structures in 3D image stacks of Olfactory Projection
Fibers (OPF) [11] using labeled 2D aerial road images. We then perform mitochondria segmentation
in large 3D Electron Microscopy (EM) stacks of neural rat tissue  demonstrating the ability of our
algorithm to leverage labeled data from different data acquisitions on this challenging task. On both
datasets our approach obtains a signiﬁcant improvement over using labeled data from either domain
alone and outperforms recent multi-task learning baseline methods.

2 Related Work

Initial ideas to multi-task learning exploited supervised data from related tasks to deﬁne a form of
regularization in the target problem [2  12]. In this setting  related tasks  also sometimes referred to

2

(a) Standard Multi-task Learning

(b) Domain Adaptation

Figure 2: Illustration of the difference between (a) standard Multi-task Learning (MTL) and (b) our
Domain Adaptation (DA) approach on two tasks. MTL assumes a single  pre-deﬁned transformation
φ(x) : X → Z and learns shared and task-speciﬁc linear boundaries in Z  namely βo  β1 and
β2 ∈ Z. In contrast  our DA approach learns a single linear boundary β in a common feature space
Z  and task-speciﬁc mappings φ1(x)  φ2(x) : X → Z. Best viewed in color.

as auxiliary problems [13]  are used to learn a latent representation and ﬁnd discriminative features
shared across tasks. This representation is then transferred to the target task to help regularize the
solution and learn from fewer labeled examples. The success of these approaches crucially hinges
on the ability to deﬁne auxiliary tasks. Although this can be easily done in certain situations  e.g.  as
in [13]  in many cases it is unclear how to generate them and the solution can be limiting  especially
when provided only a few auxiliary problems. Unlike such methods  our approach is able to ﬁnd an
informative shared representation even with as little as one related task.
More recent multi-task learning methods jointly optimize over both the shared and task-speciﬁc
components of each task [3  14  10  15]. In [3] it was shown how the two step iterative optimiza-
tion of [13] can be cast into a single convex optimization problem. In particular  for each task their
approach computes a linear decision boundary deﬁned as a linear combination between a shared
hyperplane  shared across tasks  and a task-speciﬁc one in either the original or a kernelized feature
space. This idea was later further generalized to allow for more generic forms [14  16  17  15]  as
in [14] that investigated the use of a hierarchically combined decision boundary. The use of boost-
ing for multi-task learning was explored in [10] as an alternative to kernel-based approaches. For
each task they optimize for a shared and task-speciﬁc decision boundary similar to [3]  except non-
linearities are modeled using a boosted feature space. As with other methods  however  additional
parameters are required to control the degree of sharing between tasks that can be difﬁcult to set 
especially when one or more tasks have only a few labeled samples.
For many problems  such as those common to domain adaptation [1]  the decision problem is in fact
the same across tasks  however  the features of each task have undergone some unknown transforma-
tion. Feature-based approaches seek to uncover this transformation by learning a mapping between
the features across tasks [18  19  7]. A cross-domain Mahalanobis distance metric was introduced
in [18] that leverages across-task correspondences to learn a transformation from the source to target
domain. A similar method was later developed in [20] to handle cross-domain feature spaces of a
different dimensionality. Shared latent variable models have also been proposed to learn a shared
representation across multiple feature sources or tasks [4  19  6  7  21].
Feature-based methods generally rely on the kernel-trick to model non-linearities that requires the
selection of a pre-deﬁned kernel function and is difﬁcult to scale to large datasets. In this paper 
we exploit the boosting-trick [10] to handle non-linearities and learn a shared representation across
tasks  overcoming these limitations. This results in a more parameter-free  scalable domain adapta-
tion approach that can leverage learning on new tasks where labeled data is scarce.

3 Our Approach

We consider the problem of learning a binary decision function from supervised data collected across
multiple tasks or domains. In our setting  each task is an instance of the same underlying decision
problem  however  its features are assumed to have undergone some unknown non-linear transfor-
mation.

3

i  yt

i}N t
Assume that we are given training samples X t = {xt
i=1 from t = 1  . . .   T tasks  where
i ∈ RD represents a feature vector for sample i in task t and yt
i ∈ {−1  1} its label. For each task 
xt
we seek to learn a non-linear transformation  φt(xt)  that maps xt to a common  task-independent
feature space  Z  accounting for any unwanted feature shift. Instead of relying on cleverly chosen
kernel functions we model each transformation using a set of task-speciﬁc non-linear functions
Ht = {ht
A wide variety of task-speciﬁc feature functions can be explored within our framework. We consider
functions of the form 

j : RD → R  to deﬁne φt : X t → Z as φt(xt) = [ht

(cid:124).
M (xt)]

1(xt)  . . .   ht

M}  ht

1  . . .   ht

j(xt) = hj(xt − τ t
ht
j ) 

j = 1  . . .   M

(1)
j ∈ RD. This seems like an appropriate
where H = {h1  . . .   hM} are shared across tasks and τ t
model in the case of feature shift between tasks  for example due to different acquisition parameters.
Each hj can be interpreted as a weak non-linear predictor of the task label and in practice M is
large  possibly inﬁnite. In what follows  we set H to be the set of regression trees or stumps [8] that
in combination with τ t can be used to model highly complex  non-linear transformations.
Assuming that the problem is linearly separable in Z the predictive function ft(·) : RD → R for
each task can then be written as

ft(x) = β

(cid:124)

φt(xt) =

βjhj(xt − τ t
j )

(2)

where β ∈ RM is a linear decision boundary in Z that is common to all tasks. This contrasts
previous approaches to multi-task learning such as [3  10] that learn a separate decision boundary
per task and  as we show later  is better suited for problems in domain adaptation.
We learn the functions ft(·) by minimizing the exponential loss on the training data across each task

β∗  Γ∗ = min

β Γ

where

L(β  Γt; X t) =

N t(cid:88)

i=1

exp(cid:2) − yt

i ft(xt

L(β  Γt; X t) 

N t(cid:88)

i=1

exp

(cid:104) − yt

i

M(cid:88)

j=1

(3)

 

(4)

(cid:105)

βjhj(xt

i − τ t
j )

M ].

1  . . .   τ t

and Γ = [Γ1  . . .   ΓT ] with Γt = [τ t
The explicit minimization of Eq. (3) can be very difﬁcult  since in practice  M can be prohibitively
large and the hj’s are typically discontinuous and highly non-linear. Luckily  this is a problem for
which boosting is particularly well suited [8]  as it has been demonstrated to be an effective method
for constructing a highly accurate classiﬁer from a possibly large collection of weak prediction
functions. Similar to the kernel-trick  the resulting boosting-trick [8  9  10] can be used to deﬁne a
non-linear mapping to a high dimensional feature space for which we assume the data to be linearly
separable. Unlike the kernel-trick  however  the boosting-trick deﬁnes an explicit mapping for which
β is assumed to be sparse [22  10].
We propose to use gradient boosting [8  9] to solve for ft(·). Given any twice-differentiable loss
function  gradient boosting minimizes the loss in a stage-wise manner for iterations k = 1 to K. In
particular  we use the quadratic approximation introduced by [9]. When applied to minimize Eq. (3) 
the goal at each boosting iteration is to ﬁnd the weak learner ˜h ∈ H and the set of { ˜τ 1  . . .   ˜τ T}
that minimize

M(cid:88)

j=1

T(cid:88)
i)(cid:3) =

t=1

T(cid:88)

 N t(cid:88)

(cid:104)˜h(xt − ˜τ t) − rt

ik

wt
ik

(cid:105)2

  

t=1

i=1

where wt
and rt

ik = yt

ik and rt

ik can be computed by differentiating the loss of Eq. (4)  obtaining wt

i ft(xt
i)
i. Once ˜h and { ˜τ 1  . . .   ˜τ T} are found  a line-search procedure is applied to determine

ik = e−yt

4

(5)

Algorithm 1 Non-Linear Domain Adaptation with Boosting
Input: Training samples and labels for T tasks X t = {(xt
i  yt
Number of iterations K  shrinkage factor 0 < γ ≤ 1

i )}N t

i=1

(cid:3)2

i − τ t) − rt

ik

T(cid:88)
N t(cid:88)
N t(cid:88)
T(cid:88)

t=1

i=1

wt
ik

(cid:2)h(xt
(cid:16)
(cid:104) − yt

i

ft(xt

i) + α ˜h(xt

i − ˜τ t)

(cid:17)(cid:105)

1: Set ft(·) = 0 ∀ t = 1  . . .   T
2: for k = 1 to K do
ik = e−yt
3:

Let wt

i ft(xt

(cid:110)˜h(·)  ˜τ 1  . . .   ˜τ T(cid:111)

Find

i ) and rt

ik = yt
i

= argmin

h∈H τ 1 ... τ T

4:

5:

Find ˜α through line search: ˜α = argmin

exp

α

t=1

i=1

Set ˜β = γ ˜α
Update ft(·) = ft(·) + ˜β ˜h( · − ˜τ t) ∀ t = 1  . . .   T

6:
7:
8: end for
9: return ft(·) ∀ t = 1  . . .   T

the optimal weighting for ˜h and the predictive functions ft(·) are updated  as described in Alg. 1.
Shrinkage may be applied to help regularize the solution  particularly when using powerful weak
learners such as regression trees [8].
Our proposed approach is summarized in Alg. 1. The main difﬁculty in applying this method is
in line 4  which ﬁnds the optimal values of ˜h and { ˜τ 1  . . .   ˜τ T} that minimize Eq. 5. This can be
very expensive  depending on the type of weak learners employed. In the next section we show that
regression trees and boosted stumps can be used efﬁciently to minimize Eq. (5) at train time.
3.1 Weak Learners
Regression trees have proven very effective when used as weak learners with gradient boosting [23].
An important advantage is that training regression trees needs practically no parameter tuning and
is very efﬁcient when a greedy top-down approach is used [8].
Decision stumps represent a special case of single-level regression trees. Despite their simplicity 
they have been demonstrated to achieve a high performance in challenging tasks such as face and
object detection [24  25]. In cases where feature dimensionality D is very large  decision stumps
may be preferred over regression trees to reduce training time.

Regression Trees: We use trees whose splits operate on a single dimension of the feature vector 
and follow the top-down greedy tree learning approach described in [8]. The top split is learned ﬁrst 
seeking to minimize

T(cid:88)

 N t(cid:88)

t=1

i=1

(cid:2)η1 − rt

ik

(cid:3)2

+

N t(cid:88)

i=1

(cid:2)η2 − rt

ik

(cid:3)2

¯1{xt

i[n]−τ t} wt

ik

1{xt

i[n]−τ t} wt

ik

  

(6)

argmin

n∈{1 ... D} 

η1 η2 {τ 1 ... τ T }

where x[n] ∈ R denotes the value of the nth dimension of x  1{·} is the indicator function  and
¯1{·} = 1 − 1{·}. The difference w.r.t. classic regression trees is that  besides learning the values of
η1  η2 and n  our approach requires the tree to also learn a threshold τ t ∈ R per task. Given that
each split operates on a single attribute x[n]  the resulting ˜τ t is sparse  and learned one component
at a time as the tree is built.
Once the top split is learned  a new split is trained on each of its child leaves  in a recursive manner.
This process is repeated until the maximum depth L  given as a parameter  is reached  or there are
not enough samples to learn a new node at a given leaf.

5

Decision Stumps: Decision stumps consist of a single split and return values η1  η2 = ±1. If also
ik = ±1  which is true when boosting with the exponential loss  then it can be demonstrated that
rt
minimizing Eq (6) can be separated into T independent minimization problems for all D attributes
for each n. Once this is done  a quick search can be performed to determine the n that minimizes
Eq. (6). This makes decision stumps feasible for large-scale applications with very high dimensional
feature spaces.
In the special case of the exponential loss and decision stumps  it can be shown that Alg. 1 reduces
to a procedure similar to classic AdaBoost [26]  with the exception that weak learner search is done
in the multi-task manner described above.
4 Evaluation
We evaluated our approach on two challenging domain adaptation problems for which annotation
is very time-consuming  representative of the problems we seek to address. We ﬁrst describe the
datasets  our experimental setup and baselines  and ﬁnally present and discuss the obtained results.

4.1 Datasets

Path Classiﬁcation Tracing arbors of curvilinear structures is a well studied problem that ﬁnds
applications in a broad range of ﬁelds from neuroscience to photogrammetry. We consider the
detection of 3D curvilinear structures in 3D image stacks of Olfactory Projection Fibers (OPF)
using 2D aerial road images (see Fig. 1(c d)). For this problem  the task is to predict whether a
tubular path between two image locations belongs to a curvilinear structure. We used a publicly-
available dataset [11] of 2D aerial images of road networks as the source domain and 3D stacks of
Olfactory Projection Fibers (OPF) from the DIADEM challenge as the target domain. The source
domain consists of six fully-labeled 2D aerial road images and the target domain contains eight
fully-labeled 3D stacks. We aim at using large amounts of labeled data from 2D road images to
leverage learning in the 3D stacks. This is a clear scenario where transfer learning can be highly
beneﬁcial  because labeling 2D images is much easier than annotating 3D stacks. Therefore  being
able to take advantage of 2D data is essential to reduce tedious 3D labeling effort.

Mitochondria Segmentation: Mitochondria are organelles that play an important role in cellular
functioning. The goal of this task is to segment mitochondria from large 3D Electron Microscopy
(EM) stacks of 5 nm voxel size  acquired from the brain of a rat. As in the path classiﬁcation
problem  3D annotations are time-consuming and exploiting already-annotated stacks is essential
to speed up analysis. The source domain is a fully-labeled EM stack from the Striatum region
of 853x506x496 voxels with 39 labeled mitochondria. The target domain consists of two stacks
acquired from the Hippocampus  one a training volume of size 1024x653x165 voxels and the other
a test volume that is 1024x883x165 voxels  with 10 and 42 labeled mitochondria in each respectively.
The target test volume is fully-labeled  while the training one is partially annotated  similar to a real
scenario. To capture contextual information  state-of-the-art methods typically use ﬁlter response
vectors of more than 100k dimensions  which in combination with the size of this dataset  makes the
use of linear latent space models difﬁcult and the direct application of kernel methods infeasible.
4.2 Experimental Setup

For path classiﬁcation we employ a dictionary whose codewords are Histogram of Gradient Devi-
ations (HGD) descriptors  as in [11]. This is well suited for characterizing tubular structures and
can be applied in the same way to 2D and 3D images. This allows us  in theory  to apply a classi-
ﬁer trained on 2D images to 3D volumes. However  differences in appearance and geometry of the
structures may potentially adversely affect classiﬁer accuracy when 2D-trained ones are applied to
3D stacks  which motivates domain adaptation. We use half of the target domain for training and
half for testing. 2500 positive and negative samples are extracted from each image through random
sampling  as in [11]. This results in balanced sets of 30k samples for training in the source domain 
and 20k for training and 20k for testing in the target domain.
To simulate the lack of training data  we randomly pick an equal number of positive and negative
samples for training from the target domain. The HGD codewords are extracted from the road
images and used for both domains to generate consistent feature vectors. We employ gradient
boosted trees  which in our experiments outperformed boosted stumps and kernel SVMs. For all

6

Figure 3: Path Classiﬁcation: Median  lower and upper quartiles of the test error as the number of
training samples is varied. Our approach nears Full TD performance with as few as 70 training sam-
ples in the target domain and signiﬁcantly outperforms the baseline methods. Best viewed in color.

the boosting-based baselines we set the maximum tree depth to L = 3  equivalent to a maximum of
8 leaves  and shrinkage γ = 0.1  as in [8]. The number of boosting iterations is set to K = 500. For
this dataset we report the test error computed as the percentage of mis-classiﬁed examples.
For mitochondria segmentation we use the boosting-based method of [27]  which is optimized for 3D
stacks and whose source code is publicly available. This method is based on boosted stumps  which
makes it very efﬁcient at both train and test time. Similar to [27]  we group voxels into supervoxels to
reduce training and testing time  which yields 15k positive and 275k negative supervoxel samples in
the source domain. In the target domain it renders 12k negative training samples. To simulate a real
scenario  we create 10 different transfer learning problems using the samples from one mitochondria
at a time as positives  which translates into approximately 300 positive training supervoxels each.
We use the default parameters provided by the authors of [27] in their source code (K = 2000)  and
we evaluate segmentation performance with the Jaccard Index  as in [27].

4.3 Baselines

On each dataset  we compare our approach against the following baselines: training with reference
or target domain data only (shown as SD only and TD only)  training a single classiﬁer with both tar-
get and source domain data (Pooling)  and with the multi-task approach of [10] (shown as Chapelle
et al.). We evaluate performance with varying amounts of supervision in the target domain  and also
show the performance of a classiﬁer trained with all the available labeled data  shown as Full TD 
which represents fully supervised performance on this domain and is useful in gauging the relative
performance improvement of each method.
We compare to linear Canonical Correlation Analysis (CCA) and Kernel CCA (KCCA) [4] for learn-
ing a shared latent space on the path classiﬁcation dataset  and use a Radial Basis kernel function
for KCCA  which is a commonly used kernel. Its bandwidth is set to the mean distance across the
training observations. The data size and dimensionality of the mitochondria dataset is prohibitive
for these methods  and instead we compare to Mean-Variance Normalization (MVN) and Histogram
Matching (HM) that are common normalizations one might apply to compensate for acquisition ar-
tifacts. MVN normalizes each input 3D intensity patch to have a unit variance and zero-mean  useful
for compensating for linear brightness and contrast changes in the image. HM applies a non-linear
transformation and normalizes the intensity values of one data volume such that the histogram of its
intensities matches the other.

4.4 Results: Path Classiﬁcation

The results of applying our method and the baselines for path classiﬁcation are shown in Fig. 3. Our
approach outperforms the baselines  and the difference in performance is particularly accentuated
in the case of very few training samples. The next best competitor is the multi-task method of [10] 
although it exhibits a much higher variance than our approach and performs poorly when only pro-
vided a few labeled target examples. This is also the case for KCCA. The results of linear CCA
are not shown in the plots because it yielded very low performance compared to the other baselines 

7

2030407010015025050010002%4%6%8%10%Number of training samples in TDTest error  Our ApproachKernel CCAChapelle et al.PoolingTD onlyFull TDFigure 4: Mitochondria Segmentation: Box plot of the Jaccard index measure for our method and
the baselines over 10 runs on the target domain. Simple Mean-Variance Normalization (MVN)
and Histogram Matching (HM) although helpful are unable to fully correct for differences between
acquisitions. In contrast  our method yields a higher performance without the need for such priors
and is able to faithfully leverage the source domain data to learn from relatively few examples in the
target domain  outperforming the baseline methods.

achieving a 14% error rate with 1k labeled examples and its performance signiﬁcantly degrading
with fewer training samples. Similarly  SD only performance is 16%.
Our approach is very close to Full TD in performance when using as few as 70 training samples  even
though the Full TD classiﬁer was trained with 20k samples from the target domain. This highlights
the ability of our method to effectively leverage the large amounts of source-domain data. As shown
in Fig. 3  there is a clear tendency for all methods to converge at the value of Full TD  although
our approach does so signiﬁcantly faster. The low performance of Chapelle et al. [10] suggests
that modeling the domain shift using shared and task-speciﬁc boundaries  as is commonly done in
multi-task learning methods  is not a good model for domain adaptation problems such as the ones
shown in Fig. 1. This gets accentuated by the parameter tuning required by [10]  done through cross-
validation  that performs poorly when only afforded a few labeled samples in the target domain and
yields a longer training time. The method of [10] took 25 minutes to train  while our approach only
took between 2 and 15 minutes  depending on the amount of labeled target data.

4.5 Results: Mitochondria Segmentation

A box plot showing the distribution of the VOC scores throughout 10 different runs is shown in
Fig. 4. Our approach signiﬁcantly outperforms the multi-task method of [10] and the other base-
lines  followed in performance by pooling with mean-variance normalization (MVN) and histogram
matching (HM). In contrast  our method yields higher performance and smaller variance over the
different runs without the need for such priors. From a practical point of view  our approach does
not require parameter tuning and cross-validation is not necessary. This can be a bottleneck in some
scenarios where large volumes of data are used for training. For this task  training our method took
less than an hour per run  while [10] took over 7 hours due to cross-validation.

5 Conclusion

In this paper we presented an approach for performing non-linear domain adaptation with boosting.
Our method learns a task-independent decision boundary in a common feature space  obtained via
a non-linear mapping of the features in each task. This contrasts recent approaches that learn task-
speciﬁc boundaries and is better suited for problems in domain adaptation where each task is of the
same decision problem  but whose features have undergone an unknown transformation. In this set-
ting  we illustrated how the boosting-trick can be used to deﬁne task-speciﬁc feature mappings and
effectively model non-linearity  offering distinct advantages over kernel-based approaches both in
accuracy and efﬁciency. We evaluated our approach on two challenging bio-medical datasets where
it achieved a signiﬁcant gain over using labeled data from either domain alone and outperformed
recent multi-task learning methods.

8

0.40.450.50.550.60.65TD onlyPoolingPooling + MVNPooling + HMChapelle et al.Our ApproachJaccard Index  Full TDSD onlyReferences
[1] Jiang  J.: A literature survey on domain adaptation of statistical classiﬁers. (2008)
[2] Caruana  R.: Multitask Learning. Machine Learning 28 (1997)
[3] Evgeniou  T.  Micchelli  C.  Pontil  M.: Learning Multiple Tasks with Kernel Methods. JMLR

6 (2005)

[4] Bach  F.R.  Jordan  M.I.: Kernel Independent Component Analysis. JMLR 3 (2002) 1–48
[5] Ek  C.H.  Torr  P.H.  Lawrence  N.D.: Ambiguity Modelling in Latent Spaces. In: MLMI.

(2008)

[6] Salzmann  M.  Ek  C.H.  Urtasun  R.  Darrell  T.: Factorized Orthogonal Latent Spaces. In:

AISTATS. (2010)

[7] Memisevic  R.  Sigal  L.  Fleet  D.J.: Shared Kernel Information Embedding for Discrimina-

tive Inference. PAMI (April 2012) 778–790

[8] Hastie  T.  Tibshirani  R.  Friedman  J.: The Elements of Statistical Learning. Springer (2001)
[9] Zheng  Z.  Zha  H.  Zhang  T.  Chapelle  O.  Sun  G.: A General Boosting Method and Its

Application to Learning Ranking Functions for Web Search. In: NIPS. (2007)

[10] Chapelle  O.  Shivaswamy  P.  Vadrevu  S.  Weinberger  K.  Zhang  Y.  Tseng  B.: Boosted

Multi-Task Learning. Machine Learning (2010)

[11] Turetken  E.  Benmansour  F.  Fua  P.: Automated Reconstruction of Tree Structures Using

Path Classiﬁers and Mixed Integer Programming. In: CVPR. (June 2012)

[12] Baxter  J.: A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research

(2000)

[13] Ando  R.K.  Zhang  T.: A Framework for Learning Predictive Structures from Multiple Tasks

and Unlabeled Data. JMLR 6 (2005) 1817–1853

[14] Daum´e  H.: Bayesian Multitask Learning with Latent Hierarchies. In: UAI. (2009)
[15] Kumar  A.  Daum´e  H.: Learning Task Grouping and Overlap in Multi-task Learning.

ICML. (2012)

In:

[16] Xue  Y.  Liao  X.  Carin  L.  Krishnapuram  B.: Multi-task Learning for Classiﬁcation with

Dirichlet Process Priors. JMLR 8 (2007)

[17] Jacob  L.  Bach  F.  Vert  J.P.: Clustered Multi-task Learning: a Convex Formulation.

NIPS. (2008)

In:

[18] Saenko  K.  Kulis  B.  Fritz  M.  Darrell  T.: Adapting Visual Category Models to New Do-

mains. In: ECCV. (2010)

[19] Shon  A.P.  Grochow  K.  Hertzmann  A.  Rao  R.P.N.: Learning Shared Latent Structure for

Image Synthesis and Robotic Imitation. In: NIPS. (2006) 1233–1240

[20] Kulis  B.  Saenko  K.  Darrell  T.: What You Saw is Not What You Get: Domain Adaptation

Using Asymmetric Kernel Transforms. In: CVPR. (2011)

[21] Gopalan  R.  Li  R.  Chellappa  R.: Domain Adaptation for Object Recognition: An Unsuper-

vised Approach. In: ICCV. (2011)

[22] Rosset  S.  Zhu  J.  Hastie  T.: Boosting as a Regularized Path to a Maximum Margin Classiﬁer.

JMLR (2004)

[23] Caruana  R.  Niculescu-Mizil  A.: An Empirical Comparison of Supervised Learning Algo-

rithms. In: ICML. (2006)

[24] Viola  P.  Jones  M.: Rapid Object Detection Using a Boosted Cascade of Simple Features. In:

CVPR. (2001)

[25] Ali  K.  Fleuret  F.  Hasler  D.  Fua  P.: A Real-Time Deformable Detector. PAMI 34(2)

(February 2012) 225–239

[26] Freund  Y.  Schapire  R.: A Short Introduction to Boosting (1999) Journal of Japanese Society

for Artiﬁcial Intelligence  14(5):771-780.

[27] Becker  C.  Ali  K.  Knott  G.  Fua  P.: Learning Context Cues for Synapse Segmentation. TMI

(2013) In Press.

9

,Carlos Becker
Christos Christoudias
Pascal Fua
Qibin Hou
PengTao Jiang
Yunchao Wei
Ming-Ming Cheng
Chunjin Song
Zhijie Wu
Yang Zhou
Minglun Gong
Hui Huang