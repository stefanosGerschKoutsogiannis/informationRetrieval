2011,Agnostic Selective Classification,For a learning problem whose associated excess loss class is $(\beta B)$-Bernstein  we show that it is theoretically possible to track the same classification performance  of the best (unknown) hypothesis in our class  provided that we are free to abstain from prediction in some region of our choice. The (probabilistic) volume of this rejected region of the domain is shown to be diminishing at rate $O(B\theta (\sqrt{1/m}))^\beta)$  where $\theta$ is Hanneke's disagreement coefficient. The strategy achieving this performance has computational barriers because it requires empirical error minimization in an agnostic setting. Nevertheless  we heuristically approximate this strategy and develop a novel selective classification algorithm  using constrained SVMs. We show empirically that the resulting algorithm consistently outperforms  the traditional rejection mechanism based on distance from decision boundary.,Agnostic Selective Classiﬁcation

Ran El-Yaniv and Yair Wiener
Computer Science Department

Technion – Israel Institute of Technology

{rani wyair}@{cs tx}.technion.ac.il

Abstract

For a learning problem whose associated excess loss class is (β  B)-Bernstein  we
show that it is theoretically possible to track the same classiﬁcation performance
of the best (unknown) hypothesis in our class  provided that we are free to abstain
from prediction in some region of our choice. The (probabilistic) volume of this
1/m)β) 
rejected region of the domain is shown to be diminishing at rate O(Bθ(
where θ is Hanneke’s disagreement coefﬁcient. The strategy achieving this perfor-
mance has computational barriers because it requires empirical error minimization
in an agnostic setting. Nevertheless  we heuristically approximate this strategy
and develop a novel selective classiﬁcation algorithm using constrained SVMs.
We show empirically that the resulting algorithm consistently outperforms the tra-
ditional rejection mechanism based on distance from decision boundary.

√

1 Introduction

Is it possible to achieve the same test performance as the best classiﬁer in hindsight? The answer
to this question is “probably not.” However  when changing the rules of the standard game it is
possible. Indeed  consider a game where our classiﬁer is allowed to abstain from prediction  without
penalty  in some region of our choice. For this case  and assuming a noise free “realizable” setting 
it was shown in [1] that there is a “perfect classiﬁer.” This means that after observing only a ﬁnite
labeled training sample  the learning algorithm outputs a classiﬁer that  with certainty  will never
err on any test point. To achieve this  this classiﬁer must refuse to classify in some region of the
domain. Perhaps surprisingly it was shown that the volume of this rejection region is bounded  and
in fact  this volume diminishes with increasing training set sizes (under certain conditions). An open
question  posed in [1]  is what would be an analogous notion of perfection in an agnostic  noisy
setting. Is it possible to achieve any kind of perfection in a real world scenario?
The setting under consideration  where classiﬁers can abstain from prediction  is called classiﬁcation
with a reject option [2  3]  or selective classiﬁcation [1]. Focusing on this model  in this paper
we present a blend of theoretical and practical results. We ﬁrst show that the concept of “perfect
classiﬁcation” that was introduced for the realizable case in [1]  can be extended to the agnostic
setting. While pure perfection is impossible to accomplish in a noisy environment  a more realistic
objective is to perform as well as the best hypothesis in the class within a region of our choice.
We call this type of learning “weakly optimal” selective classiﬁcation and show that a novel strategy
accomplishes this type of learning with diminishing rejection rate under certain Bernstein type con-
ditions (a stronger notion of optimality is mentioned later as well). This strategy relies on empirical
risk minimization  which is computationally difﬁcult. In the practical part of the paper we present
a heuristic approximation algorithm  which relies on constrained SVMs  and mimics the optimal
behavior. We conclude with numerical examples that examine the empirical performance of the new
algorithm and compare its performance with that of the widely used selective classiﬁcation method
for rejection  based on distance from decision boundary.

1

2 Selective classiﬁcation and other deﬁnitions
Consider a standard agnostic binary classiﬁcation setting where X is some feature space  and H
is our hypothesis class of binary classiﬁers  h : X → {±1}. Given a ﬁnite training sample of m
labeled examples  Sm = {(xi  yi)}m
i=1  assumed to be sampled i.i.d. from some unknown underlying
distribution P (X  Y ) over X × {±1}  our goal is to select the best possible classiﬁer from H. For
m∑
any h ∈ H  its true error  R(h)  and its empirical error  ˆR(h)  are 

R(h)   Pr

(X Y )∼P

{h(X) ̸= Y }  

ˆR(h)   1
m

i=1

I (h(xi) ̸= yi) .

Let ˆh   arg inf h∈H ˆR(h) be the empirical risk minimizer (ERM)  and h
true risk minimizer.
In selective classiﬁcation [1]  given Sm we need to select a binary selective classiﬁer deﬁned to be a
pair (h  g)  with h ∈ H being a standard binary classiﬁer  and g : X → {0  1} is a selection function
deﬁning the sub-region of activity of h in X . For any x ∈ X  

∗   arg inf h∈H R(h)  the

{

(h  g)(x)  

reject 
h(x) 

if g(x) = 0;
if g(x) = 1.

(1)

Selective classiﬁcation performance is characterized in terms of two quantities: coverage and risk.
The coverage of (h  g) is
For a bounded loss function ℓ : Y × Y → [0  1]  the risk of (h  g) is deﬁned as the average loss on
the accepted samples 

Φ(h  g)   E [g(X)] .

R(h  g)   E [ℓ(h(X)  Y ) · g(X)]

.

Φ(h  g)

As pointed out in [1]  the trade-off between risk and coverage is the main characteristic of a selective
classiﬁer. This trade-off is termed there the “risk-coverage curve” (RC curve)1
Let G ⊆ H. The disagreement set [4  1] w.r.t. G is deﬁned as

DIS(G)   {x ∈ X : ∃h1  h2 ∈ G s.t. h1(x) ̸= h2(x)} .

′ ∈ H : R(h
′

For any hypothesis class H  target hypothesis h ∈ H  distribution P   sample Sm  and real r > 0 
deﬁne
V(h  r) = {h
(2)
Finally  for any h ∈ H we deﬁne a ball in H of radius r around h [5]. Speciﬁcally  with respect to
class H  marginal distribution P over X   h ∈ H  and real r > 0  deﬁne
(X) ̸= h(X)} ≤ r

′ ∈ H : ˆR(h
′
h
}

) ≤ R(h) + r} and ˆV(h  r) =

) ≤ ˆR(h) + r

B(h  r)  

{

{

}

.

.

′ ∈ H : Pr
h
X∼P

{h
′

3 Perfect and weakly optimal selective classiﬁers

The concept of perfect classiﬁcation was introduced in [1] within a realizable selective classiﬁcation
setting. Perfect classiﬁcation is an extreme case of selective classiﬁcation where a selective classiﬁer
(h  g) achieves R(h  g) = 0 with certainty; that is  the classiﬁer never errs on its region of activity.
Obviously  the classiﬁer must compromise sufﬁciently large part of the domain X in order to achieve
this outstanding performance. Surprisingly  it was shown in [1] that not-trivial perfect classiﬁcation
exists in the sense that under certain conditions (e.g.  ﬁnite hypothesis class) the rejected region
diminishes at rate Ω(1/m)  where m is the size of the training set.
In agnostic environments  as we consider here  such perfect classiﬁcation appears to be out of reach.
In general  in the worst case no hypothesis can achieve zero error over any nonempty subset of the

1Some authors refer to an equivalent variant of this curve as “Accuracy-Rejection Curve” or ARC.

2

domain. We consider here the following weaker  but still extremely desirable behavior  which we
∗ ∈ H be the true risk minimizer of our problem.
call “weakly optimal selective classiﬁcation.” Let h
Let (h  g) be a selective classiﬁer selected after observing the training set Sm. We say that (h  g) is
a weakly optimal selective classiﬁer if  for any 0 < δ < 1  with probability of at least 1 − δ over
random choices of Sm  R(h  g) ≤ R(h
∗
  g). That is  with high probability our classiﬁer is at least
as good as the true risk minimizer over its region of activity. We call this classiﬁer ‘weakly optimal’
because a stronger requirement would be that the classiﬁer should achieve the best possible error
among all hypotheses in H restricted to the region of activity deﬁned by g.

4 A learning strategy

We now present a strategy that will be shown later to achieve non-trivial weakly optimal selective
classiﬁcation under certain conditions. We call it a “strategy” rather than an “algorithm” because it
does not include implementation details.
Let’s begin with some motivation. Using standard concentration inequalities one can show that
∗  cannot be “too far” from the training error of the
the training error of the true risk minimizer  h
empirical risk minimizer  ˆh. Therefore  we can guarantee  with high probability  that the class of
∗. Selecting
all hypothesis with “sufﬁciently low” empirical error includes the true risk minimizer h
only subset of the domain  for which all hypothesis in that class agree  is then sufﬁcient to guarantee
weak optimality. Strategy 1 formulates this idea. In the next section we analyze this strategy and
show that it achieves this optimality with non trivial (bounded) coverage.

(

√

Strategy 1 Learning strategy for weakly optimal selective classiﬁers
Input: Sm; m; (cid:14); d
; g) w.p. 1 (cid:0) (cid:14)
Output: a selective classiﬁer (h; g) such that R(h; g) = R(h
1: Set ^h = ERM (H; Sm)  i.e.  ^h is any empirical risk minimizer from H
2: Set G = ^V
3: Construct g such that g(x) = 1 () x 2 fX n DIS (G)g
4: h = ^h

d(ln 2me
d )+ln 8
m

(see Eq. (2))

^h; 4

2

(cid:3)

)

(cid:14)

5 Analysis

We begin with a few deﬁnitions. Consider an instance of a binary learning problem with hy-
pothesis class H  an underlying distribution P over X × Y  and a loss function ℓ(Y Y). Let
= arg inf h∈H {Eℓ(h(X)  Y )} be the true risk minimizer. The associated excess loss class [6] is
∗
h
deﬁned as
Class F is said to be a (β  B)-Bernstein class with respect to P (where 0 < β ≤ 1 and B ≥ 1)  if
every f ∈ F satisﬁes

F   {ℓ(h(x)  y) − ℓ(h
∗

(x)  y) : h ∈ H} .

Ef 2 ≤ B(Ef )β.

Bernstein classes arise in many natural situations; see discussions in [7  8]. For example  if the prob-
ability P (X  Y ) satisﬁes Tsybakov’s noise conditions then the excess loss function is a Bernstein
[8  9] class. In the following sequence of lemmas and theorems we assume a binary hypothesis class
H with VC-dimension d  an underlying distribution P over X ×{±1}  and ℓ is the 0/1 loss function.
Also  F denotes the associated excess loss class. Our results can be extended to losses other than
0/1 by similar techniques to those used in [10].
Lemma 5.1. If F is a (β  B)-Bernstein class with respect to P   then for any r > 0

  r) ⊆ B(

V(h
∗

)

∗
h

  Brβ

.

Proof. If h ∈ V(h
∗

  r) then  by deﬁnition

E{I(h(X) ̸= Y )} ≤ E{I(h
∗

(X) ̸= Y )} + r.

3

Using the linearity of expectation we have 

Since F is a (β  B)-Bernstein class 

E{I(h(X) ̸= h
∗

By (3)  for any r > 0  E{I(h(X) ̸= h
∗

Throughout this section we denote

E{I(h(X) ̸= Y ) − I(h
∗

(X) ̸= Y )} ≤ r.

(3)

}
(X) ̸= Y )|}
(X)  Y ))2

= E
= B (E{I(h(X) ̸= Y ) − I(h
∗

{
(X))} = E{|I(h(X) ̸= Y ) − I(h
∗
(ℓ(h(X)  Y ) − ℓ(h
∗
(X))} ≤ Brβ. Therefore  by deﬁnition  h ∈ B(
√

(X) ̸= Y )})β .

= Ef 2 ≤ B(Ef )β

(

)

)

.

∗
h

  Brβ

σ(m  δ  d)   2

d

2

ln 2me
d
m

+ ln 2
δ

.

Theorem 5.2 ([11]). For any 0 < δ < 1  with probability of at least 1 − δ over the choice of Sm
from P m  any hypothesis h ∈ H satisﬁes

R(h) ≤ ˆR(h) + σ(m  δ  d).
Similarly ˆR(h) ≤ R(h) + σ(m  δ  d) under the same conditions.
Lemma 5.3. For any r > 0  and 0 < δ < 1  with probability of at least 1 − δ 

ˆV(ˆh  r) ⊆ V (h
∗

  2σ(m  δ/2  d) + r) .

Proof. If h ∈ ˆV(ˆh  r)  then  by deﬁnition  ˆR(h) ≤ ˆR(ˆh) + r. Since ˆh minimizes the empirical error 
we have  ˆR(ˆh) ≤ ˆR(h
∗
). Using Theorem 5.2 twice  and applying the union bound  we know that
w.p. of at least 1 − δ 

R(h) ≤ ˆR(h) + σ(m  δ/2  d) ∧

∗
ˆR(h

) ≤ R(h
∗

) + 2σ(m  δ/2  d) + r  and h ∈ V (h
∗

Therefore  R(h) ≤ R(h
∗
For any G ⊆ H  and distribution P we deﬁne  ∆G   Pr{DIS(G)}. Hanneke introduced a
complexity measure for active learning problems termed the disagreement coefﬁcient [5]. The dis-
agreement coefﬁcient of h with respect to H under distribution P is 

) + σ(m  δ/2  d).
  2σ(m  δ/2  d) + r).

(4)
where ϵ = 0. The disagreement coefﬁcient of the hypothesis class H with respect to P is deﬁned as

r>ϵ

r

 

∆B(h  r)

θh   sup

θ   lim sup

k→∞ θh(k) 

h(k)

is any sequence of h(k) ∈ H with R(h(k)) monotonically decreasing.

where
Theorem 5.4. Assume that H has disagreement coefﬁcient θ and that F is a (β  B)-Bernstein class
w.r.t. P . Then  for any r > 0 and 0 < δ < 1  with probability of at least 1 − δ 

∆ ˆV(ˆh  r) ≤ Bθ (2σ(m  δ/2  d) + r)β .

Proof. Applying Lemmas 5.3 and 5.1 we get that with probability of at least 1 − δ 

ˆV(ˆh  r) ⊆ B

  B (2σ(m  δ/2  d) + r)β

Therefore

∆ ˆV(ˆh  r) ≤ ∆B

∗
h

By the deﬁnition of the disagreement coefﬁcient  for any r

  B (2σ(m  δ/2  d) + r)β
> 0  ∆B(h
∗

′

.

  r

′

) ≤ θr

′.

)

.

)

{

}

(

∗
h

(

4

Theorem 5.5. Assume that H has disagreement coefﬁcient θ and that F is a (β  B)-Bernstein class
w.r.t. P . Let (h  g) be the selective classiﬁer chosen by Algorithm 1. Then  with probability of at
least 1 − δ 

Φ(h  g) ≥ 1 − Bθ (4σ(m  δ/4  d))β

∗
R(h  g) = R(h
Proof. Applying Theorem 5.2 we get that with probability of at least 1 − δ/4 

∧

  g).

∗
ˆR(h

) ≤ R(h
∗
(

) + σ(m  δ/4  d).

∗ minimizes the true error  wet get that R(h
∗

) ≤ R(ˆh). Applying again Theorem 5.2 we
Since h
)
know that with probability of at least 1 − δ/4  R(ˆh) ≤ ˆR(ˆh) + σ(m  δ/4  d). Applying the union
) ≤ ˆR(ˆh) + 2σ(m  δ/4  d). Hence 
bound we have that with probability of at least 1 − δ/2  ˆR(h
∗
with probability of at least 1 − δ/2  h
ˆh  2σ(m  δ/4  d)
= G. We note that the selection
function g(x) equals one only for x ∈ X \DIS (G) . Therefore  for any x ∈ X   for which g(x) = 1 
all the hypotheses in G agree  and in particular h
E{I(ˆh(X) ̸= Y ) · g(X)}

∗ and ˆh agree. Thus 

E{I(h
∗

∗ ∈ ˆV

(X) ̸= Y ) · g(X)}
E{g(X)}

∗
= R(h

  g).

Applying Theorem 5.4 and the union bound we therefore know that with probability of at least 1−δ 

R(ˆh  g) =

E{g(X)}

=

Φ(ˆh  g) = E{g(X)} = 1 − ∆G ≥ 1 − Bθ (4σ(m  δ/4  d))β .

Hanneke introduced  in his original work [5]  an alternative deﬁnition of the disagreement coefﬁcient
θ  for which the supermum in (4) is taken with respect to any ﬁxed ϵ > 0. Using this alternative def-
inition it is possible to show that fast coverage rates are achievable  not only for ﬁnite disagreement
coefﬁcients (Theorem 5.5)  but also if the disagreement coefﬁcient grows slowly with respect to 1/ϵ
(as shown by Wang [12]  under sufﬁcient smoothness conditions). This extension will be discussed
in the full version of this paper.

6 A disbelief principle and the risk-coverage trade-off

Theorem 5.5 tells us that the strategy presented in Section 4 not only outputs a weakly optimal
selective classiﬁer  but this classiﬁer also has guaranteed coverage (under some conditions). As
emphasized in [1]  in practical applications it is desirable to allow for some control on the trade-off
between risk and coverage; in other words  we would like to be able to develop the entire risk-
coverage curve for the classiﬁer at hand and select ourselves the cutoff point along this curve in
accordance with other practical considerations we may have. How can this be achieved?
The following lemma facilitates a construction of a risk-coverage trade-off curve. The result is an
alternative characterization of the selection function g  of the weakly optimal selective classiﬁer
chosen by Strategy 1. This result allows for calculating the value of g(x)  for any individual test
point x ∈ X   without actually constructing g for the entire domain X .
(
Lemma 6.1. Let (h  g) be a selective classiﬁer chosen by Strategy 1 after observing the training
sample Sm. Let ˆh be the empirical risk minimizer over Sm. Let x be any point in X and

{

)}
ehx   argmin
g(x) = 0 ⇐⇒ ˆR(ehx) − ˆR(ˆh) ≤ 2σ(m  δ/4  d).

| h(x) = −sign

ˆR(h)

ˆh(x)

h∈H

 

an empirical risk minimizer forced to label x the opposite from ˆh(x). Then

Proof. According to the deﬁnition of ˆV (see Eq. (2)) 

)
Thus  ˆh ehx ∈ ˆV. However  by construction  ˆh(x) = −eh(x)  so x ∈ DIS( ˆV) and g(x) = 0.

ˆR(ehx) − ˆR(ˆh) ≤ 2σ(m  δ/4  d) ⇐⇒ eh ∈ ˆV

ˆh  2σ(m  δ/4  d)

(

5

empirical error ˆR(ehx) of a special empirical risk minimizer ehx  which is constrained to label x the

Lemma 6.1 tells us that in order to decide if point x should be rejected we need to measure the

opposite from ˆh(x). If this error is sufﬁciently close to ˆR(ˆh) our classiﬁer cannot be too sure about
the label of x and we must reject it. This result strongly motivates the following deﬁnition of a
“disbelief index” for each individual point.
Deﬁnition 6.2 (disbelief index). For any x ∈ X   deﬁne its disbelief index w.r.t. Sm and H 

D(x)   D(x  Sm)   ˆR(ehx) − ˆR(ˆh).

Observe that D(x) is large whenever our model is sensitive to label of x in the sense that when we
are forced to bend our best model to ﬁt the opposite label of x  our model substantially deteriorates 
giving rise to a large disbelief index. This large D(x) can be interpreted as our disbelief in the
possibility that x can be labeled so differently. In this case we should deﬁnitely predict the label of
x using our unforced model. Conversely  if D(x) is small  our model is indifferent to the label of x
and in this sense  is not committed to its label. In this case we should abstain from prediction at x.
This “disbelief principle” facilitates an exploration of the risk-coverage trade-off curve for our clas-
siﬁer. Given a pool of test points we can rank these test points according to their disbelief index  and
points with low index should be rejected ﬁrst. Thus  this ranking provides the means for constructing
a risk-coverage trade-off curve.
A similar technique of using an ERM oracle that can enforce an arbitrary number of example-based
constraints was used in [13  14] in the context of active learning. As in our disbelief index  the
difference between the empirical risk (or importance weighted empirical risk [14]) of two ERM
oracles (with different constraints) is used to estimate prediction conﬁdence.

7 Implementation

m  S2

At this point in the paper we switch from theory to practice  aiming at implementing rejection
methods inspired by the disbelief principle and see how well they work on real world (well  ...  UCI)
problems. Attempting to implement a learning algorithm driven by the disbelief index we face a
major bottleneck because the calculation of the index requires the identiﬁcation of ERM hypotheses.
To handle this computationally difﬁcult problem  we “approximate” the ERM as follows. Focusing
on SVMs we use a high C value (105 in our experiments) to penalize more on training errors than
on small margin. In this way the solution to the optimization problem tend to get closer to the ERM.
Another problem we face is that the disbelief index is a noisy statistic that highly depends on the
sample Sm. To overcome this noise we use robust statistics. First we generate 11 different samples
m ) using bootstrap sampling. For each sample we calculate the disbelief index for
(S1
all test points and for each point take the median of these measurements as the ﬁnal index.
We note that for any ﬁnite training sample the disbelief index is a discrete variable. It is often the
case that several test points share the same disbelief index. In those cases we can use any conﬁdence
measure as a tie breaker. In our experiments we use distance from decision boundary to break ties.

In order to estimate ˆR(ehx) we have to restrict the SVM optimizer to only consider hypotheses that

classify the point x in a speciﬁc way. To accomplish this we use a weighted SVM for unbalanced
data. We add the point x as another training point with weight 10 times larger than the weight of all
training points combined. Thus  the penalty for misclassiﬁcation of x is very large and the optimizer
ﬁnds a solution that doesn’t violate the constraint.

m  . . . S11

8 Empirical results

Focusing on SVMs with a linear kernel we compared the RC (Risk-Coverage) curves achieved by
the proposed method with those achieved by SVM with rejection based on distance from decision
boundary. This latter approach is very common in practical applications of selective classiﬁcation.
For implementation we used LIBSVM [15].
Before presenting these results we wish to emphasize that the proposed method leads to rejection
regions fundamentally different than those obtained by the traditional distance-based technique. In

6

Figure 1 we depict those regions for a training sample of 150 points sampled from a mixture of
two identical normal distributions (centered at different locations). The height map reﬂects the
“conﬁdence regions” of each technique according to its own conﬁdence measure.

Figure 1: conﬁdence height map using (a) disbelief index; (b) distance from decision boundary.

We tested our algorithm on standard medical diagnosis problems from the UCI repository  including
all datasets used in [16]. We transformed nominal features to numerical ones in a standard way using
binary indicator attributes. We also normalized each attribute independently so that its dynamic
range is [0  1]. No other preprocessing was employed.
In each iteration we choose uniformly at random non overlapping training set (100 samples) and test
set (200 samples) for each dataset.SVM was trained on the entire training set and test samples were
sorted according to conﬁdence (either using distance from decision boundary or disbelief index).
Figure 2 depicts the RC curves of our technique (red solid line) and rejection based on distance from
decision boundary (green dashed line) for linear kernel on all 6 datasets. All results are averaged
over 500 iterations (error bars show standard error).

Figure 2: RC curves for SVM with linear kernel. Our method in solid red  and rejection based on
distance from decision boundary in dashed green. Horizntal axis (c) represents coverage.

With the exception of the Hepatitis dataset  in which both methods were statistically indistinguish-
able  in all other datasets the proposed method exhibits signiﬁcant advantage over the traditional
approach. We would like to highlight the performance of the proposed method on the Pima dataset.
While the traditional approach cannot achieve error less than 8% for any rejection rate  in our ap-
proach the test error decreases monotonically to zero with rejection rate. Furthermore  a clear ad-
vantage for our method over a large range of rejection rates is evident in the Haberman dataset.2.

2The Haberman dataset contains survival data of patients who had undergone surgery for breast cancer.
With estimated 207 090 new cases of breast cancer in the united states during 2010 [17] an improvement of 1%
affects the lives of more than 2000 women.

7

(a)(b)00.20.40.60.8100.010.020.03cHypo00.20.40.60.8100.050.10.150.20.25ctest errorPima00.20.40.60.8100.050.10.15ctest errorHepatitis00.20.40.60.8100.10.20.3cHaberman00.20.40.60.8100.10.20.3ctest errorBUPA00.20.40.60.8100.010.020.03ctest errorBreastFor the sake of fairness  we note that the running time of our algorithm (as presented here) is substan-
tially longer than the traditional technique. The performance of our algorithm can be substantially
improved when many unlabeled samples are available. Details will be provided in the full paper.

9 Related work

The literature on theoretical studies of selective classiﬁcation is rather sparse. El-Yaniv and Wiener
[1] studied the performance of a simple selective learning strategy for the realizable case. Given an
hypothesis class H  and a sample Sm  their method abstain from prediction if all hypotheses in the
version space do not agree on the target sample. They were able to show that their selective classiﬁer
achieves perfect classiﬁcation with meaningful coverage under some conditions. Our work can be
viewed as an extension of the above algorithm to the agnostic case.
Freund et al.
[18] studied another simple ensemble method for binary classiﬁcation. Given an
hypothesis class H  the method outputs a weighted average of all the hypotheses in H  where the
weight of each hypothesis exponentially depends on its individual training error. Their algorithm
abstains from prediction whenever the weighted average of all individual predictions is close to
∗
zero. They were able to bound the probability of misclassiﬁcation by 2R(h
) + ϵ(m) and  under
) + ϵ(F  m) on the rejection rate. Our algorithm
∗
some conditions  they proved a bound of 5R(h
can be viewed as an extreme variation of the Freund et al. method. We include in our “ensemble”
only hypotheses with sufﬁciently low empirical error and we abstain if the weighted average of all
predictions is not deﬁnitive ( ̸= ±1). Our risk and coverage bounds are asymptotically tighter.
Excess risk bounds were developed by Herbei and Wegkamp [19] for a model where each rejection
incurs a cost 0 ≤ d ≤ 1/2. Their bound applies to any empirical risk minimizer over a hypothesis
class of ternary hypotheses (whose output is in {±1  reject}). See also various extensions [20  21].
A rejection mechanism for SVMs based on distance from decision boundary is perhaps the most
widely known and used rejection technique. It is routinely used in medical applications [22  23  24].
Few papers proposed alternative techniques for rejection in the case of SVMs. Those include taking
the reject area into account during optimization [25]  training two SVM classiﬁers with asymmetric
cost [26]  and using a hinge loss [20]. Grandvalet et al. [16] proposed an efﬁcient implementation
of SVM with a reject option using a double hinge loss. They empirically compared their results with
two other selective classiﬁers: the one proposed by Bartlett and Wegkamp [20] and the traditional
rejection based on distance from decision boundary. In their experiments there was no statistically
signiﬁcant advantage to either method compared to the traditional approach for high rejection rates.

10 Conclusion

We presented and analyzed a learning strategy for selective classiﬁcation that achieves weak op-
timality. We showed that the coverage rate directly depends on the disagreement coefﬁcient  thus
linking between active learning and selective classiﬁcation. Recently it has been shown that  for
the noise-free case  active learning can be reduced to selective classiﬁcation [27]. We conjecture
that such a reduction also holds in noisy settings. Exact implementation of our strategy  or exact
computation of the disbelief index may be too difﬁcult to achieve or even obtain with approximation
guarantees. We presented one algorithm that heuristically approximate the required behavior and
there is certainly room for other  perhaps better methods and variants. Our empirical examination
of the proposed algorithm indicate that it can provide signiﬁcant and consistent advantage over the
traditional rejection technique with SVMs. This advantage can be of great value especially in medi-
cal diagnosis applications and other mission critical classiﬁcation tasks. The algorithm itself can be
implemented using off-the-shelf packages.

Acknowledgments

This work was supported in part by the IST Programme of the European Community  under the
PASCAL2 Network of Excellence  IST-2007-216886. This publication only reﬂects the authors’
views.

8

References
[1] R. El-Yaniv and Y. Wiener. On the foundations of noise-free selective classiﬁcation. JMLR  11:1605–

1641  2010.

[2] C.K. Chow. An optimum character recognition system using decision function. IEEE Trans. Computer 

6(4):247–254  1957.

[3] C.K. Chow. On optimum recognition error and reject trade-off.

16:41–36  1970.

IEEE Trans. on Information Theory 

[4] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML  pages 353–360  2007.
[5] S. Hanneke. Theoretical Foundations of Active Learning. PhD thesis  Carnegie Mellon University  2009.
[6] P.L. Bartlett  S. Mendelson  and P. Philips. Local complexities for empirical risk minimization. In COLT:
Proceedings of the Workshop on Computational Learning Theory  Morgan Kaufmann Publishers  2004.
[7] V. Koltchinskii. 2004 IMS medallion lecture: Local rademacher complexities and oracle inequalities in

risk minimization. Annals of Statistics  34:2593–2656  2006.

[8] P.L. Bartlett and S. Mendelson. Discussion of ”2004 IMS medallion lecture: Local rademacher complex-
ities and oracle inequalities in risk minimization” by V. koltchinskii. Annals of Statistics  34:2657–2663 
2006.

[9] A.B. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. Annals of Mathematical Statis-

tics  32:135–166  2004.

[10] A. Beygelzimer  S. Dasgupta  and J. Langford.

In ICML ’09:
Proceedings of the 26th Annual International Conference on Machine Learning  pages 49–56. ACM 
2009.

Importance weighted active learning.

[11] O. Bousquet  S. Boucheron  and G. Lugosi.

In Advanced
Lectures on Machine Learning  volume 3176 of Lecture Notes in Computer Science  pages 169–207.
Springer  2003.

Introduction to statistical learning theory.

[12] L. Wang. Smoothness  disagreement coefﬁcient  and the label complexity of agnostic active learning.

JMLR  pages 2269–2292  2011.

[13] S. Dasgupta  D. Hsu  and C. Monteleoni. A general agnostic active learning algorithm. In NIPS  2007.
[14] A. Beygelzimer  D. Hsu  J. Langford  and T. Zhang. Agnostic active learning without constraints. Ad-

vances in Neural Information Processing Systems 23  2010.

[15] C.C. Chang and C.J. Lin.

LIBSVM: A library for support vector machines.

ACM Trans-
Software available at

actions on Intelligent Systems and Technology  2:27:1–27:27  2011.
”http://www.csie.ntu.edu.tw/ cjlin/libsvm”.

[16] Y. Grandvalet  A. Rakotomamonjy  J. Keshet  and S. Canu. Support vector machines with a reject option.

In NIPS  pages 537–544. MIT Press  2008.

[17] American Cancer Society. Cancer facts and ﬁgures. 2010.
[18] Y. Freund  Y. Mansour  and R.E. Schapire. Generalization bounds for averaged classiﬁers. Annals of

Statistics  32(4):1698–1722  2004.

[19] R. Herbei and M.H. Wegkamp. Classiﬁcation with reject option. The Canadian Journal of Statistics 

34(4):709–721  2006.

[20] P.L. Bartlett and M.H. Wegkamp. Classiﬁcation with a reject option using a hinge loss. Journal of

Machine Learning Research  9:1823–1840  2008.

[21] M.H. Wegkap. Lasso type classiﬁers with a reject option. Electronic Journal of Statistics  1:155–168 

2007.

[22] S. Mukherjee  P. Tamayo  D. Slonim  A. Verri  T. Golub  J. P. Mesirov  and T. Poggio. Support vector
machine classiﬁcation of microarray data. Technical report  AI Memo 1677  Massachusetts Institute of
Technology  1998.

[23] I. Guyon  J. Weston  S. Barnhill  and V. Vapnik. Gene selection for cancer classiﬁcation using support

vector machines. Machine Learning  pages 389–422  2002.

[24] S. Mukherjee. Chapter 9. classifying microarray data using support vector machines. In of scientists from
the University of Pennsylvania School of Medicine and the School of Engineering and Applied Science.
Kluwer Academic Publishers  2003.

[25] G. Fumera and F. Roli. Support vector machines with embedded reject option. In Pattern Recognition

with Support Vector Machines: First International Workshop  pages 811–919  2002.

[26] R. Sousa  B. Mora  and J.S. Cardoso. An ordinal data method for the classiﬁcation with reject option. In

ICMLA  pages 746–750. IEEE Computer Society  2009.

[27] R. El-Yaniv and Y. Wiener. Active learning via perfect selective classiﬁcation. Accepted to JMLR  2011.

9

,Carl-Johann Simon-Gabriel
Adam Scibior
Ilya Tolstikhin
Bernhard Schölkopf