2013,Approximate Inference in Continuous Determinantal Processes,Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning  the focus of DPP-based models has been on diverse subset selection from a discrete and finite base set. This discrete setting admits an efficient algorithm for sampling based on the eigendecomposition of the defining kernel matrix. Recently  there has been growing interest in using DPPs defined on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case  computationally  the steps required cannot be directly extended except in a few restricted cases. In this paper  we present efficient approximate DPP sampling schemes based on Nystrom and random Fourier feature approximations that apply to a wide range of kernel functions. We demonstrate the utility of continuous DPPs in repulsive mixture modeling applications and synthesizing human poses spanning activity spaces.,Approximate Inference in Continuous

Determinantal Point Processes

Raja Haﬁz Affandi1  Emily B. Fox2  and Ben Taskar2

1University of Pennsylvania  rajara@wharton.upenn.edu

2University of Washington  {ebfox@stat taskar@cs}.washington.edu

Abstract

Determinantal point processes (DPPs) are random point processes well-suited for
modeling repulsion. In machine learning  the focus of DPP-based models has been
on diverse subset selection from a discrete and ﬁnite base set. This discrete setting
admits an efﬁcient sampling algorithm based on the eigendecomposition of the
deﬁning kernel matrix. Recently  there has been growing interest in using DPPs
deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to
the continuous case  computationally  the steps required are not tractable in general.
In this paper  we present two efﬁcient DPP sampling schemes that apply to a wide
range of kernel functions: one based on low rank approximations via Nystr¨om
and random Fourier feature techniques and another based on Gibbs sampling. We
demonstrate the utility of continuous DPPs in repulsive mixture modeling and
synthesizing human poses spanning activity spaces.

Introduction

1
Samples from a determinantal point process (DPP) [15] are sets of points that tend to be spread out.
More speciﬁcally  given Ω ⊆ Rd and a positive semideﬁnite kernel function L : Ω × Ω (cid:55)→ R  the
probability density of a point conﬁguration A ⊂ Ω under a DPP with kernel L is given by

PL(A) ∝ det(LA)  

(1)
where LA is the |A| × |A| matrix with entries L(x  y) for each x  y ∈ A. The tendency for repulsion
is captured by the determinant since it depends on the volume spanned by the selected points in the
associated Hilbert space of L. Intuitively  points similar according to L or points that are nearly
linearly dependent are less likely to be selected.
Building on the foundational work in [5] for the case where Ω is discrete and ﬁnite  DPPs have been
used in machine learning as a model for subset selection in which diverse sets are preferred [2  3 
9  12  13]. These methods build on the tractability of sampling based on the algorithm of Hough et
al. [10]  which relies on the eigendecomposition of the kernel matrix to recursively sample points
based on their projections onto the subspace spanned by the selected eigenvectors.
Repulsive point processes  like hard core processes [7  16]  many based on thinned Poisson processes
and Gibbs/Markov distributions  have a long history in the spatial statistics community  where
considering continuous Ω is key. Many naturally occurring phenomena exhibit diversity—trees tend
to grow in the least occupied space [17]  ant hill locations are over-dispersed relative to uniform
placement [4] and the spatial distribution of nerve ﬁbers is indicative of neuropathy  with hard-core
processes providing a critical tool [25]. Repulsive processes on continuous spaces have garnered
interest in machine learning as well  especially relating to generative mixture modeling [18  29].
The computationally attractive properties of DPPs make them appealing to consider in these appli-
cations. On the surface  it seems that the eigendecomposition and projection algorithm of [10] for
discrete DPPs would naturally extend to the continuous case. While this is true in a formal sense as L

1

becomes an operator instead of a matrix  the key steps such as the eigendecomposition of the kernel
and projection of points on subspaces spanned by eigenfunctions are computationally infeasible
except in a few very limited cases where approximations can be made [14]. The absence of a tractable
DPP sampling algorithm for general kernels in continuous spaces has hindered progress in developing
DPP-based models for repulsion.
In this paper  we propose an efﬁcient algorithm to sample from DPPs in continuous spaces using
low-rank approximations of the kernel function. We investigate two such schemes: Nystr¨om and
random Fourier features. Our approach utilizes a dual representation of the DPP  a technique that has
proven useful in the discrete Ω setting as well [11]. For k-DPPs  which only place positive probability
on sets of cardinality k [13]  we also devise a Gibbs sampler that iteratively samples points in the
k-set conditioned on all k − 1 other points. The derivation relies on representing the conditional
DPPs using the Schur complement of the kernel. Our methods allow us to handle a broad range of
typical kernels and continuous subspaces  provided certain simple integrals of the kernel function
can be computed efﬁciently. Decomposing our kernel into quality and similarity terms as in [13] 
this includes  but is not limited to  all cases where the (i) spectral density of the quality and (ii)
characteristic function of the similarity kernel can be computed efﬁciently. Our methods scale well
with dimension  in particular with complexity growing linearly in d.
In Sec. 2  we review sampling algorithms for discrete DPPs and the challenges associated with
sampling from continuous DPPs. We then propose continuous DPP sampling algorithms based on
low-rank kernel approximations in Sec. 3 and Gibbs sampling in Sec. 4. An empirical analysis of the
two schemes is provided in Sec. 5. Finally  we apply our methods to repulsive mixture modeling and
human pose synthesis in Sec. 6 and 7.

2 Sampling from a DPP

kernel matrix L =(cid:80)N

When Ω is discrete with cardinality N  an efﬁcient algorithm for sampling from a DPP is given
in [10]. The algorithm  which is detailed in the supplement  uses an eigendecomposition of the
n and recursively samples points xi as follows  resulting in a set
A ∼ DPP(L) with A = {xi}:

n=1 λnvnv(cid:62)

λn+1. Let V be the selected eigenvectors (k = |V |).
Phase 1 Select eigenvector vn with probability λn
Phase 2 For i = 1  . . .   k  sample points xi ∈ Ω sequentially with probability based on the projection
of xi onto the subspace spanned by V . Once xi is sampled  update V by excluding the
subspace spanned by the projection of xi onto V .

When Ω is discrete  both steps are straightforward since the ﬁrst phase involves eigendecomposing a
kernel matrix and the second phase involves sampling from discrete probability distributions based
on inner products between points and eigenvectors. Extending this algorithm to a continuous space
was considered by [14]  but for a very limited set of kernels L and spaces Ω. For general L and Ω 
we face difﬁculties in both phases. Extending Phase 1 to a continuous space requires knowledge of
the eigendecomposition of the kernel function. When Ω is a compact rectangle in Rd  [14] suggest
approximating the eigendecomposition using an orthonormal Fourier basis.
Even if we are able to obtain the eigendecomposition of the kernel function (either directly or via
approximations as considered in [14] and Sec. 3)  we still need to implement Phase 2 of the sampling
algorithm. Whereas the discrete case only requires sampling from a discrete probability function 
here we have to sample from a probability density. When Ω is compact  [14] suggest using a rejection
sampler with a uniform proposal on Ω. The authors note that the acceptance rate of this rejection
sampler decreases with the number of points sampled  making the method inefﬁcient in sampling large
sets from a DPP. In most other cases  implementing Phase 2 even via rejection sampling is infeasible
since the target density is in general non-standard with unknown normalization. Furthermore  a
generic proposal distribution can yield extremely low acceptance rates.
In summary  current algorithms can sample approximately from a continuous DPP only for translation-
invariant kernels deﬁned on a compact space. In Sec. 3  we propose a sampling algorithm that allows
us to sample approximately from DPPs for a wide range of kernels L and spaces Ω.

2

3 Sampling from a low-rank continuous DPP
Again considering Ω discrete with cardinality N  the sampling algorithm of Sec. 2 has complexity
dominated by the eigendecomposition  O(N 3). If the kernel matrix L is low-rank  i.e. L = B(cid:62)B 
with B a D × N matrix and D (cid:28) N  [11] showed that the complexity of sampling can be reduced to
O(N D2 + D3). The basic idea is to exploit the fact that L and the dual kernel matrix C = BB(cid:62) 
which is D × D  share the same nonzero eigenvalues  and for each eigenvector vk of L  Bvk is the
corresponding eigenvector of C. See the supplement for algorithmic details.
While the dependence on N in the dual is sharply reduced  in continuous spaces  N is inﬁnite. In
order to extend the algorithm  we must ﬁnd efﬁcient ways to compute C for Phase 1 and manipulate
eigenfunctions implicitly for the projections in Phase 2. Generically  consider sampling from a DPP
n=1 λnφn(x)φn(y) where λn and φn(x) are
eigenvalues and eigenfunctions  and φn(y) is the complex conjugate of φn(y). Assume that we can
approximate L by a low-dimensional (generally complex-valued) mapping  B(x) : Ω (cid:55)→ CD:

on a continuous space Ω with kernel L(x  y) = (cid:80)∞

˜L(x  y) = B(x)∗B(y)   where B(x) = [B1(x)  . . .   BD(x)](cid:62).

(2)
Here  A∗ denotes complex conjugate transpose of A. We consider two efﬁcient low-rank approxima-
tion schemes in Sec. 3.1 and 3.2. Using such a low-rank representation  we propose an analog of
the dual sampling algorithm for continuous spaces  described in Algorithm 1. A similar algorithm
provides samples from a k-DPP  which only gives positive probability to sets of a ﬁxed cardinality
k [13]. The only change required is to the for-loop in Phase 1 to select exactly k eigenvectors using
an efﬁcient O(Dk) recursion. See the supplement for details.

PHASE 1

a rank-D DPP kernel

Input: ˜L(x  y) = B(x)∗B(y) 

Compute C =(cid:82)
Compute eigendecomp. C =(cid:80)D

Algorithm 1 Dual sampler for a low-rank continuous DPP
PHASE 2
X ← ∅
while |V | > 0 do
Sample ˆx from f (x) = 1|V |
X ← X ∪ {ˆx}
Let v0 be a vector in V such that v∗
Update V ← {v − v∗B(ˆx)
Orthonormalize V w.r.t. (cid:104)v1  v2(cid:105) = v∗

(cid:80)
v∈V |v∗B(x)|2
0B(ˆx) (cid:54)= 0
0 B(ˆx) v0 | v ∈ V − {v0}}
v∗

J ← J ∪ {k} with probability λk

Ω B(x)B(x)∗dx

k=1 λkvkv∗

k

λk+1

1Cv2

J ← ∅
for k = 1  . . .   D do
V ← {
}k∈J

vk√
v∗
kCvk

Output: X

In this dual view  we still have the same two-phase structure  and must address two key challenges:
Phase 1 Assuming a low-rank kernel function decomposition as in Eq. (2)  we need to able to

compute the dual kernel matrix  given by an integral:

C =

B(x)B(x)∗dx .

(3)

(cid:90)

Ω

Phase 2 In general  sampling directly from the density f (x) is difﬁcult; instead  we can compute the
cumulative distribution function (CDF) and sample x using the inverse CDF method [21]:

F (ˆx = (ˆx1  . . .   ˆxd)) =

f (x)1{xl∈Ω}dxl.

(4)

(cid:90) ˆxl

d(cid:89)

−∞

l=1

Assuming (i) the kernel function ˜L is ﬁnite-rank and (ii) the terms C and f (x) are computable 
Algorithm 1 provides exact samples from a DPP with kernel ˜L. In what follows  approximations only
arise from approximating general kernels L with low-rank kernels ˜L. If given a ﬁnite-rank kernel L
to begin with  the sampling procedure is exact.
One could imagine approximating L as in Eq. (2) by simply truncating the eigendecomposition
(either directly or using numerical approximations). However  this simple approximation for known
decompositions does not necessarily yield a tractable sampler  because the products of eigenfunctions
required in Eq. (3) might not be efﬁciently integrable. For our approximation algorithm to work  not
only do we need methods that approximate the kernel function well  but also that enable us to solve
Eq. (3) and (4) directly for many different kernel functions. We consider two such approaches that
enable an efﬁcient sampler for a wide range of kernels: Nystr¨om and random Fourier features.

3

Sampling from RFF-approximated DPP

3.1
Random Fourier features (RFF) [19] is an approach for approximating shift-invariant kernels 
k(x  y) = k(x − y)  using randomly selected frequencies. The frequencies are sampled inde-
pendently from the Fourier transform of the kernel function  ωj ∼ F(k(x − y))  and letting:

˜k(x − y) =

1
D

exp{iω(cid:62)

j (x − y)}   x  y ∈ Ω .

To apply RFFs  we factor L into a quality function q and similarity kernel k (i.e.  q(x) =(cid:112)L(x  x)):

j=1

L(x  y) = q(x)k(x  y)q(y)  

x  y ∈ Ω where k(x  x) = 1.

(5)

(6)

D(cid:88)

The RFF approximation can be applied to cases where the similarity function has a known char-
acteristic function  e.g.  Gaussian  Laplacian and Cauchy. Using Eq. (5)  we can approximate the
similarity kernel function to obtain a low-rank kernel and dual matrix:

˜LRF F (x  y) =

1
D

q(x) exp{iω(cid:62)

D(cid:88)

j=1

jk =

j (x − y)}q(y)  C RF F
D(cid:88)

(cid:90) ˆxl

d(cid:89)

vjv∗

k

(cid:88)

D(cid:88)

The CDF of the sampling distribution f (x) in Algorithm 1 is given by:

FRF F (ˆx) =

1
|V |

v∈V

j=1

k=1

l=1

−∞

q2(x) exp{i(ωj − ωk)(cid:62)x}1{xl∈Ω}dxl.

(7)

where vj denotes the jth element of vector v. Note that equations C RF F and FRF F can be computed
for many different combinations of Ω and q(x). In fact  this method works for any combination
of (i) translation-invariant similarity kernel k with known characteristic function and (ii) quality
function q with known spectral density. The resulting kernel L need not be translation invariant. In
the supplement  we illustrate this method by considering a common and important example where
Ω = Rd  q(x) is Gaussian  and k(x  y) is any kernel with known Fourier transform.
3.2 Sampling from a Nystr¨om-approximated DPP
Another approach to kernel approximation is the Nystr¨om method [27]. In particular  given z1  . . .   zD
landmarks sampled from Ω  we can approximate the kernel function and dual matrix as 

(cid:90)

1
D

Ω

q2(x) exp{i(ωj − ωk)(cid:62)x}dx.

D(cid:88)

D(cid:88)

j=1

k=1

W 2

˜LN ys(x  y) =

jkL(x  zj)L(zk  y)  C N ys

where Wjk = L(zj  zk)−1/2. Denoting wj(v) =(cid:80)D
d(cid:89)

D(cid:88)

D(cid:88)

(cid:88)

jk =

FN ys(ˆx) =

1
|V |

(cid:90)

D(cid:88)

D(cid:88)
(cid:90) ˆxl

n=1

WjnWmk

L(zn  x)L(x  zm)dx 

m=1

Ω

n=1 Wjnvn  the CDF of f (x) in Alg. 1 is:

wj(v)wk(v)

L(x  zj)L(zk  x)1{xl∈Ω}dxl.

(8)

v∈V

j=1

k=1

−∞

l=1

As with the RFF case  we consider a decomposition L(x  y) = q(x)k(x  y)q(y). Here  there are no
translation-invariant requirements  even for the similarity kernel k. In the supplement  we provide the
important example where Ω = Rd and both q(x) and k(x  y) are Gaussians and also when k(x  y) is
polynomial  a case that cannot be handled by RFF since it is not translationally invariant.
4 Gibbs sampling
For k-DPPs  we can consider a Gibbs sampling scheme. In the supplement  we derive that the full
conditional for the inclusion of point xk given the inclusion of the k− 1 other points is a 1-DPP with a
modiﬁed kernel  which we know how to sample from. Let the kernel function be represented as before:
L(x  y) = q(x)k(x  y)q(y). Denoting J\k = {xj}j(cid:54)=k and M\k = L−1
J\k the full conditional can
be simpliﬁed using Schur’s determinantal equality [22]:

p(xk|{xj}j(cid:54)=k) ∝ L(xk  xk) − (cid:88)

\k
ij L(xi  xk)L(xj  xk).

M

(9)

i j(cid:54)=k

4

(a)

(b)

(c)

(d)

Figure 1: Estimates of total variational distance for Nystr¨om and RFF approximation methods to a DPP with
Gaussian quality and similarity with covariances Γ = diag(ρ2  . . .   ρ2) and Σ = diag(σ2  . . .   σ2)  respectively.
(a)-(c) For dimensions d=1  5 and 10  each plot considers ρ2 = 1 and varies σ2. (d) Eigenvalues for the Gaussian
kernels with σ2 = ρ2 = 1 and varying dimension d.
In general  sampling directly from this full conditional is difﬁcult. However  for a wide range of
kernel functions  including those which can be handled by the Nystr¨om approximation in Sec. 3.2 
the CDF can be computed analytically and xk can be sampled using the inverse CDF method:

i j(cid:54)=k M

\k
ij L(xi  xl)L(xj  xl)1{xl∈Ω}dxl

i j(cid:54)=k M

\k
ij L(xi  x)L(xj  x)dx

(10)

F (ˆxl|{xj}j(cid:54)=k) =

(cid:82) ˆxl−∞ L(xl  xl) −(cid:80)
(cid:82)
Ω L(x  x) −(cid:80)

In the supplement  we illustrate this method by considering the case where Ω = Rd and q(x) and
k(x  y) are Gaussians. We use this same Schur complement scheme for sampling from the full
conditionals in the mixture model application of Sec. 6. A key advantage of this scheme for several
types of kernels is that the complexity of sampling scales linearly with the number of dimensions d
making it suitable in handling high-dimensional spaces.
As with any Gibbs sampling scheme  the mixing rate is dependent on the correlations between
variables. In cases where the kernel introduces low repulsion we expect the Gibbs sampler to mix well 
while in a high repulsion setting the sampler can mix slowly due to the strong dependencies between
points and fact that we are only doing one-point-at-a-time moves. We explore the dependence of
convergence on repulsion strength in the supplementary materials. Regardless  this sampler provides
a nice tool in the k-DPP setting. Asymptotically  theory suggests that we get exact (though correlated)
samples from the k-DPP. To extend this approach to standard DPPs  we can ﬁrst sample k (this
assumes knowledge of the eigenvalues of L) and then apply the above method to get a sample. This is
fairly inefﬁcient if many samples are needed. A more involved but potentially efﬁcient approach is to
consider a birth-death sampling scheme where the size of the set can grow/shrink by 1 at every step.
5 Empirical analysis
(cid:80)
To evaluate the performance of the RFF and Nystr¨om approximations  we compute the total variational
X |PL(X) − P ˜L(X)|  where PL(X) denotes the probability of set X
distance (cid:107)PL − P ˜L(cid:107)1 = 1
under a DPP with kernel L  as given by Eq. (1). We restrict our analysis to the case where the quality
function and similarity kernel are Gaussians with isotropic covariances Γ = diag(ρ2  . . .   ρ2) and Σ =
diag(σ2  . . .   σ2)  respectively  enabling our analysis based on the easily computed eigenvalues [8].
We also focus on sampling from k-DPPs for which the size of the set X is always k. Details are in
the supplement.
Fig. 1 displays estimates of the total variational distance for the RFF and Nystr¨om approximations
when ρ2 = 1  varying σ2 (the repulsion strength) and the dimension d. Note that the RFF method
performs slightly worse as σ2 increases and is rather invariant to d while the Nystr¨om method
performs much better for increasing σ2 but worse for increasing d.
While this phenomenon seems perplexing at ﬁrst  a study of the eigenvalues of the Gaussian kernel
across dimensions sheds light on the rationale (see Fig. 1). Note that for ﬁxed σ2 and ρ2  the decay
of eigenvalues is slower in higher dimensions. It has been previously demonstrated that the Nystr¨om
method performs favorably in kernel learning tasks compared to RFF in cases where there is a
large eigengap in the kernel matrix [28]. The plot of the eigenvalues seems to indicate the same
phenomenon here. Furthermore  this result is consistent with the comparison of RFF to Nystr¨om in
approximating DPPs in the discrete Ω case provided in [3].
This behavior can also be explained by looking at the theory behind these two approximations.
For the RFF  while the kernel approximation is guaranteed to be an unbiased estimate of the true
kernel element-wise  the variance is fairly high [19]. In our case  we note that the RFF estimates of
minors are biased because of non-linearity in matrix entries  overestimating probabilities for point

2

5

01234500.20.40.60.81σ2Variational Distance  NystromRFF01234500.20.40.60.81σ2Variational Distance  NystromRFF01234500.20.40.60.81σ2Variational Distance  NystromRFF02040608010000.20.40.60.81Eigenvalues  d=1d=5d=10conﬁgurations that are more spread out  which leads to samples that are overly-dispersed. For the
Nystr¨om method  on the other hand  the quality of the approximation depends on how well the
landmarks cover Ω. In our experiments the landmarks are sampled i.i.d. from q(x). When either the
similarity bandwidth σ2 is small or the dimension d is high  the effective distance between points
increases  thereby decreasing the accuracy of the approximation. Theoretical bounds for the Nystr¨om
DPP approximation in the case when Ω is ﬁnite are provided in [3]. We believe the same result holds
for continuous Ω by extending the eigenvalues and spectral norm of the kernel matrix to operator
eigenvalues and operator norms  respectively.
In summary  for moderate values of σ2 it is generally good to use the Nystr¨om approximation for
low-dimensional settings and RFF for high-dimensional settings.
6 Repulsive priors for mixture models
Mixture models are used in a wide range of applications from clustering to density estimation.
A common issue with such models  especially in density estimation tasks  is the introduction of
redundant  overlapping components that increase the complexity and reduce interpretability of the
resulting model. This phenomenon is especially prominent when the number of samples is small. In
a Bayesian setting  a common ﬁx to this problem is to consider a sparse Dirichlet prior on the mixture
weights  which penalizes the addition of non-zero-weight components. However  such approaches
run the risk of inaccuracies in the parameter estimates [18]. Instead  [18] show that sampling the
location parameters using repulsive priors leads to better separated clusters while maintaining the
accuracy of the density estimate. They propose a class of repulsive priors that rely on explicitly
deﬁning a distance metric and the manner in which small distances are penalized. The resulting
posterior computations can be fairly complex.
The theoretical properties of DPPs make them an appealing choice as a repulsive prior. In fact  [29]
considered using DPPs as repulsive priors in latent variable models. However  in the absence of
a feasible continuous DPP sampling algorithm  their method was restricted to performing MAP
inference. Here we propose a fully generative probabilistic mixture model using a DPP prior for the
location parameters  with a K-component model using a K-DPP.
In the common case of mixtures of Gaussians (MoG)  our posterior computations can be performed
using Gibbs sampling with nearly the same simplicity of the standard case where the location
parameters µk are assumed to be i.i.d.. In particular  with the exception of updating the location
parameters {µ1  . . .   µK}  our sampling steps are identical to standard MoG Gibbs updates in the
uncollapsed setting. For the location parameters  instead of sampling each µk independently from its
conditional posterior  our full conditional depends upon the other locations µ\k as well. Details are
in the supplement  where we show that this full conditional has an interpretation as a single draw
from a tilted 1-DPP. As such  we can employ the Gibbs sampling scheme of Sec. 4.
We assess the clustering and density estimation performance of the DPP-based model on both
synthetic and real datasets. In each case  we run 10 000 Gibbs iterations  discard 5 000 as burn-in
and thin the chain by 10. Hyperparameter settings are in the supplement. We randomly permute the
labels in each iteration to ensure balanced label switching. Draws are post-processed following the
algorithm of [23] to address the label switching issue.
Synthetic data To assess the role of the prior in a density estimation task  we generated a small
sample of 100 observations from a mixture of two Gaussians. We consider two cases  the ﬁrst with
well-separated components and the second with poorly-separated components. We compare a mixture
model with locations sampled i.i.d. (IID) to our DPP repulsive prior (DPP). In both cases  we set
an upper bound of six mixture components. In Fig. 2  we see that both IID and DPP provide very
similar density estimates. However  IID uses many large-mass components to describe the density.
As a measure of simplicity of the resulting density description  we compute the average entropy of the
posterior mixture membership distribution  which is a reasonable metric given the similarity of the
overall densities. Lower entropy indicates a more concise representation in an information-theoretic
sense. We also assess the accuracy of the density estimate by computing both (i) Hamming distance
error relative to true cluster labels and (ii) held-out log-likelihood on 100 observations. The results are
summarized in Table 1. We see that DPP results in (i) signiﬁcantly lower entropy  (ii) lower overall
clustering error  and (iii) statistically indistinguishable held-out log-likelihood. These results signify
that we have a sparser representation with well-separated (interpretable) clusters while maintaining
the accuracy of the density estimate.

6

Well-Sep

Poor-Sep

Galaxy

Enzyme

Acidity

Figure 2: For each synthetic and real dataset: (top) histogram of data overlaid with actual Gaussian mixture
generating the synthetic data  and posterior mean mixture model for (middle) IID and (bottom) DPP. Red
dashed lines indicate resulting density estimate.

Table 1: For IID and DPP on synthetic datasets: mean (stdev) for mixture membership entropy  cluster
assignment error rate and held-out log-likelihood of 100 observations under the posterior mean density estimate.

DATASET

ENTROPY

CLUSTERING ERROR HELDOUT LOG-LIKE.

Well-separated
Poorly-separated

IID

1.11 (0.3)
1.46 (0.2)

DPP

0.88 (0.2)
0.92 (0.3)

IID

0.19 (0.1)
0.47 (0.1)

DPP

0.19 (0.1)
0.39 (0.1)

IID

-169 (6)
-211(10)

DPP
-171(8)
-207(9)

Real data We also tested our DPP model on three real density estimation tasks considered in [20]:
82 measurements of velocity of galaxies diverging from our own (galaxy)  acidity measurement of 155
lakes in Wisconsin (acidity)  and the distribution of enzymatic activity in the blood of 245 individuals
(enzyme). We once again judge the complexity of the density estimates using the posterior mixture
membership entropy as a proxy. To assess the accuracy of the density estimates  we performed 5-fold
cross validation to estimate the predictive held-out log-likelihood. As with the synthetic data  we
ﬁnd that DPP visually results in better separated clusters (Fig. 2). The DPP entropy measure is also
signiﬁcantly lower for data that are not well separated (acidity and galaxy) while the differences in
predictive log-likelihood estimates are not statistically signiﬁcant (Table 2).
Finally  we consider a classiﬁcation task based on the iris dataset: 150 observations from three iris
species with four length measurements. For this dataset  there has been signiﬁcant debate on the
optimal number of clusters. While there are three species in the data  it is known that two have
very low separation. Based on loss minimization  [24  26] concluded that the optimal number of
clusters was two. Table 2 compares the classiﬁcation error using DPP and IID when we assume
for evaluation the real data has three or two classes (by collapsing two low-separation classes)   but
consider a model with a maximum of six components. While both methods perform similarly for
three classes  DPP has signiﬁcantly lower classiﬁcation error under the assumption of two classes 
since DPP places large posterior mass on only two mixture components. This result hints at the
possibility of using the DPP mixture model as a model selection method.
7 Generating diverse sample perturbations
We consider another possible application of continuous-space sampling. In many applications of
inverse reinforcement learning or inverse optimal control  the learner is presented with control
trajectories executed by an expert and tries to estimate a reward function that would approximately
reproduce such policies [1]. In order to estimate the reward function  the learner needs to compare
the rewards of a large set of trajectories (or all  if possible)  which becomes intractable in high-
dimensional spaces with complex non-linear dynamics. A typical approximation is to use a set of
perturbed expert trajectories as a comparison set  where a good set of trajectories should cover as
large a part of the space as possible.

7

−3−2−1012300.10.20.30.40.5−3−2−1012300.10.20.30.40.5−4−202400.10.20.30.40.50.60.7−10123400.20.40.60.811.21.41.6−3−2−1012300.10.20.30.40.50.60.7−3−2−1012300.10.20.30.40.5−3−2−1012300.10.20.30.40.5−4−202400.10.20.30.40.50.60.7−10123400.20.40.60.811.21.41.6−3−2−1012300.10.20.30.40.50.60.7−3−2−1012300.10.20.30.40.5−3−2−1012300.10.20.30.40.5−4−202400.10.20.30.40.50.60.7−10123400.20.40.60.811.21.41.6−3−2−1012300.10.20.30.40.50.60.7Table 2: For IID and DPP  mean (stdev) of (left) mixture membership entropy and held-out log-likelihood for
three density estimation tasks and (right) classiﬁcation error under 2 vs. 2 of true classes for the iris data.
DATA

ENTROPY

Galaxy
Acidity
Enzyme

IID

0.89 (0.2)
1.32 (0.1)
1.01 (0.1)

DPP

0.74 (0.2)
0.98 (0.1)
0.96 (0.1)

HELDOUT LL.
IID
DPP
-21(2)
-20(2)
-48(3)
-49 (2)
-55(2)
-55(3)

DATA

CLASS ERROR
IID
DPP

Iris (3 cls)
Iris (2 cls)

0.43 (0.02)
0.23 (0.03)

0.43 (0.02)
0.15 (0.03)

Original

DPP Samples

Figure 3: Left: Diverse set of human poses relative to an original pose by sampling from an RFF (top) and
Nystr¨om (bottom) approximations with kernel based on MoCap of the activity dance. Right: Fraction of data
having a DPP/i.i.d. sample within an  neighborhood.

We propose using DPPs to sample a large-coverage set of trajectories  in particular focusing on a
human motion application where we assume a set of motion capture (MoCap) training data taken
from the CMU database [6]. Here  our dimension d is 62  corresponding to a set of joint angle
measurements. For a given activity  such as dancing  we aim to select a reference pose and synthesize
a set of diverse  perturbed poses. To achieve this  we build a kernel with Gaussian quality and
similarity using covariances estimated from the training data associated with the activity. The
Gaussian quality is centered about the selected reference pose and we synthesize new poses by
sampling from our continuous DPP using the low-rank approximation scheme. In Fig. 3  we show
an example of such DPP-synthesized poses. For the activity dance  to quantitatively assess our
performance in covering the activity space  we compute a coverage rate metric based on a random
sample of 50 poses from a DPP. For each training MoCap frame  we compute whether the frame has
a neighbor in the DPP sample within an  neighborhood. We compare our coverage to that of i.i.d.
sampling from a multivariate Gaussian chosen to have variance matching our DPP sample. Despite
favoring the i.i.d. case by inﬂating the variance to match the diverse DPP sample  the DPP poses still
provide better average coverage over 100 runs. See Fig. 3 (right) for an assessment of the coverage
metric. A visualization of the samples is in the supplement. Note that the i.i.d. case requires on
average  = 253 to cover all data whereas the DPP only requires  = 82. By  = 40  we cover over
90% of the data on average. Capturing the rare poses is extremely challenging with i.i.d. sampling 
but the diversity encouraged by the DPP overcomes this issue.
8 Conclusion
Motivated by the recent successes of DPP-based subset modeling in ﬁnite-set applications and the
growing interest in repulsive processes on continuous spaces  we considered methods by which
continuous-DPP sampling can be straightforwardly and efﬁciently approximated for a wide range of
kernels. Our low-rank approach harnessed approximations provided by Nystr¨om and random Fourier
feature methods and then utilized a continuous dual DPP representation. The resulting approximate
sampler garners the same efﬁciencies that led to the success of the DPP in the discrete case. One can
use this method as a proposal distribution and correct for the approximations via Metropolis-Hastings 
for example. For k-DPPs  we devised an exact Gibbs sampler that utilized the Schur complement
representation. Finally  we demonstrated that continuous-DPP sampling is useful both for repulsive
mixture modeling (which utilizes the Gibbs sampling scheme) and in synthesizing diverse human
poses (which we demonstrated with the low-rank approximation method). As we saw in the MoCap
example  we can handle high-dimensional spaces d  with our computations scaling just linearly with
d. We believe this work opens up opportunities to use DPPs as parts of many models.
Acknowledgements: RHA and EBF were supported in part by AFOSR Grant FA9550-12-1-0453
and DARPA Grant FA9550-12-1-0406 negotiated by AFOSR. BT was partially supported by NSF CA-
REER Grant 1054215 and by STARnet  a Semiconductor Research Corporation program sponsored
by MARCO and DARPA.

8

05010000.20.40.60.81εCoverage Rate  DPPIIDReferences
[1] P. Abbeel and A.Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc.

ICML  2004.

[2] R. H. Affandi  A. Kulesza  and E. B. Fox. Markov determinantal point processes. In Proc. UAI 

2012.

[3] R.H. Affandi  A. Kulesza  E.B. Fox  and B. Taskar. Nystr¨om approximation for large-scale

determinantal processes. In Proc. AISTATS  2013.

[4] R. A. Bernstein and M. Gobbel. Partitioning of space in communities of ants. Journal of Animal

Ecology  48(3):931–942  1979.

[5] A. Borodin and E.M. Rains. Eynard-Mehta theorem  Schur process  and their Pfafﬁan analogs.

Journal of statistical physics  121(3):291–317  2005.

[6] CMU.

http://mocap.cs.cmu.edu/  2009.

Carnegie Mellon University graphics

lab motion capture database.

[7] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes: Volume I:

Elementary theory and methods. Springer  2003.

[8] G.E. Fasshauer and M.J. McCourt. Stable evaluation of Gaussian radial basis function inter-

polants. SIAM Journal on Scientiﬁc Computing  34(2):737–762  2012.

[9] J. Gillenwater  A. Kulesza  and B. Taskar. Discovering diverse and salient threads in document

collections. In Proc. EMNLP  2012.

[10] J.B. Hough  M. Krishnapur  Y. Peres  and B. Vir´ag. Determinantal processes and independence.

Probability Surveys  3:206–229  2006.

[11] A. Kulesza and B. Taskar. Structured determinantal point processes. In Proc. NIPS  2010.
[12] A. Kulesza and B. Taskar. k-DPPs: Fixed-size determinantal point processes. In ICML  2011.
[13] A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Foundations

and Trends in Machine Learning  5(2–3)  2012.

[14] F. Lavancier  J. Møller  and E. Rubak. Statistical aspects of determinantal point processes. arXiv

preprint arXiv:1205.4818  2012.

[15] O. Macchi. The coincidence approach to stochastic point processes. Advances in Applied

Probability  pages 83–122  1975.

[16] B. Mat´ern. Spatial variation. Springer-Verlag  1986.
[17] T. Neeff  G. S. Biging  L. V. Dutra  C. C. Freitas  and J. R. Dos Santos. Markov point processes
for modeling of spatial forest patterns in Amazonia derived from interferometric height. Remote
Sensing of Environment  97(4):484–494  2005.

[18] F. Petralia  V. Rao  and D. Dunson. Repulsive mixtures. In NIPS  2012.
[19] A. Rahimi and B. Recht. Random features for large-scale kernel machines. NIPS  2007.
[20] S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number of

components (with discussion). JRSS:B  59(4):731–792  1997.

[21] C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer  2nd edition  2004.
[22] J Schur. ¨Uber potenzreihen  die im innern des einheitskreises beschr¨ankt sind. Journal f¨ur die

reine und angewandte Mathematik  147:205–232  1917.

[23] M. Stephens. Dealing with label switching in mixture models. JRSS:B  62(4):795–809  2000.
[24] C.A. Sugar and G.M. James. Finding the number of clusters in a dataset: An information-

theoretic approach. JASA  98(463):750–763  2003.

[25] L. A. Waller  A. S¨arkk¨a  V. Olsbo  M. Myllym¨aki  I.G. Panoutsopoulou  W.R. Kennedy  and
G. Wendelschafer-Crabb. Second-order spatial analysis of epidermal nerve ﬁbers. Statistics in
Medicine  30(23):2827–2841  2011.

[26] J. Wang. Consistent selection of the number of clusters via crossvalidation. Biometrika  97(4):

893–904  2010.

[27] C.K.I. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. NIPS 

2000.

[28] T. Yang  Y.-F. Li  M. Mahdavi  R. Jin  and Z.-H. Zhou. Nystr¨om method vs random fourier

features: A theoretical and empirical comparison. NIPS  2012.

[29] J. Zou and R.P. Adams. Priors for diversity in generative latent variable models. In NIPS  2012.

9

,Raja Hafiz Affandi
Emily Fox
Ben Taskar