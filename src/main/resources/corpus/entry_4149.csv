2017,Optimistic posterior sampling for reinforcement learning: worst-case regret bounds,We present an algorithm based on posterior sampling (aka Thompson sampling) that achieves near-optimal worst-case regret bounds when the underlying Markov Decision Process (MDP) is communicating with a finite  though unknown  diameter. Our main result is a high probability regret upper bound of $\tilde{O}(D\sqrt{SAT})$ for any communicating MDP with $S$ states  $A$ actions and diameter $D$  when $T\ge S^5A$. Here  regret compares the total reward achieved by the algorithm to the total expected reward of an optimal infinite-horizon undiscounted average reward policy  in time horizon $T$. This result improves over the best previously known upper bound of $\tilde{O}(DS\sqrt{AT})$ achieved by any algorithm in this setting  and matches the dependence on $S$ in the established lower bound of $\Omega(\sqrt{DSAT})$ for this problem. Our techniques involve proving some novel results about the anti-concentration of Dirichlet distribution  which may be of independent interest.,Optimistic posterior sampling for reinforcement

learning: worst-case regret bounds

Shipra Agrawal

Columbia University

sa3305@columbia.edu

Abstract

Randy Jia

Columbia University

rqj2000@columbia.edu

We present an algorithm based on posterior sampling (aka Thompson sampling)
that achieves near-optimal worst-case regret bounds when the underlying Markov
Decision Process (MDP) is communicating with a ﬁnite  though unknown  diameter.
Our main result is a high probability regret upper bound of ˜O(D
SAT ) for any
communicating MDP with S states  A actions and diameter D  when T ≥ S5A.
Here  regret compares the total reward achieved by the algorithm to the total
expected reward of an optimal inﬁnite-horizon undiscounted average reward policy 
in time horizon T . This result improves over the best previously known upper
bound of ˜O(DS
AT ) achieved by any algorithm in this setting  and matches the
dependence on S in the established lower bound of Ω(
DSAT ) for this problem.
Our techniques involve proving some novel results about the anti-concentration of
Dirichlet distribution  which may be of independent interest.

√

√

√

1

Introduction

Reinforcement Learning (RL) refers to the problem of learning and planning in sequential decision
making systems when the underlying system dynamics are unknown  and may need to be learned by
trying out different options and observing their outcomes. A typical model for the sequential decision
making problem is a Markov Decision Process (MDP)  which proceeds in discrete time steps. At each
time step  the system is in some state s  and the decision maker may take any available action a to
obtain a (possibly stochastic) reward. The system then transitions to the next state according to a ﬁxed
state transition distribution. The reward and the next state depend on the current state s and the action
a  but are independent of all the previous states and actions. In the reinforcement learning problem 
the underlying state transition distributions and/or reward distributions are unknown  and need to be
learned using the observed rewards and state transitions  while aiming to maximize the cumulative
reward. This requires the algorithm to manage the tradeoff between exploration vs. exploitation  i.e. 
exploring different actions in different states in order to learn the model more accurately vs. taking
actions that currently seem to be reward maximizing.
Exploration-exploitation tradeoff has been studied extensively in the context of stochastic multi-
armed bandit (MAB) problems  which are essentially MDPs with a single state. The performance of
MAB algorithms is typically measured through regret  which compares the total reward obtained by
the algorithm to the total expected reward of an optimal action. Optimal regret bounds have been
established for many variations of MAB (see Bubeck et al. [2012] for a survey)  with a large majority
of results obtained using the Upper Conﬁdence Bound (UCB) algorithm  or more generally  the
optimism in the face of uncertainty principle. Under this principle  the learning algorithm maintains
tight over-estimates (or optimistic estimates) of the expected rewards for individual actions  and
at any given step  picks the action with the highest optimistic estimate. More recently  posterior
sampling  aka Thompson Sampling [Thompson  1933]  has emerged as another popular algorithm
design principle in MAB  owing its popularity to a simple and extendible algorithmic structure  an

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

√

√

√

√

SAT + DS7/4A3/4T 1/4)  which is ˜O(D

attractive empirical performance [Chapelle and Li  2011  Kaufmann et al.  2012]  as well as provably
optimal performance bounds that have been recently obtained for many variations of MAB [Agrawal
and Goyal  2012  2013b a  Russo and Van Roy  2015  2014  Bubeck and Liu  2013]. In this approach 
the algorithm maintains a Bayesian posterior distribution for the expected reward of every action;
then at any given step  it generates an independent sample from each of these posteriors and takes the
action with the highest sample value.
We consider the reinforcement learning problem with ﬁnite states S and ﬁnite actions A in a similar
regret based framework  where the total reward of the reinforcement learning algorithm is compared
to the total expected reward achieved by a single benchmark policy over a time horizon T . In our
setting  the benchmark policy is the inﬁnite-horizon undiscounted average reward optimal policy for
the underlying MDP  under the assumption that the MDP is communicating with (unknown) ﬁnite
diameter D. The diameter D is an upper bound on the time it takes to move from any state s to any
other state s(cid:48) using an appropriate policy  for each pair s  s(cid:48). A ﬁnite diameter is understood to be
necessary for interesting bounds on the regret of any algorithm in this setting [Jaksch et al.  2010].
The UCRL2 algorithm of Jaksch et al. [2010]  which is based on the optimism principle  achieved the
best previously known upper bound of ˜O(DS
AT ) for this problem. A similar bound was achieved
by Bartlett and Tewari [2009]  though assuming the knowledge of the diameter D. Jaksch et al.
[2010] also established a worst-case lower bound of Ω(
DSAT ) on the regret of any algorithm for
this problem.
Our main contribution is a posterior sampling based algorithm with a high probability worst-case
SAT ) when T ≥ S5A.
regret upper bound of ˜O(D
This improves the previously best known upper bound for this problem by a factor of
S  and
matches the dependence on S in the lower bound  for large enough T .
Our algorithm uses an ‘optimistic version’ of the posterior sampling heuristic  while utilizing several
ideas from the algorithm design structure in Jaksch et al. [2010]  such as an epoch based execution
and the extended MDP construction. The algorithm proceeds in epochs  where in the beginning of
every epoch  it generates ψ = ˜O(S) sample transition probability vectors from a posterior distribution
for every state and action  and solves an extended MDP with ψA actions and S states formed using
these samples. The optimal policy computed for this extended MDP is used throughout the epoch.
Posterior Sampling for Reinforcement Learning (PSRL) approach has been used previously in Osband
et al. [2013]  Abbasi-Yadkori and Szepesvari [2014]  Osband and Van Roy [2016]  but in a Bayesian
regret framework. Bayesian regret is deﬁned as the expected regret over a known prior on the
transition probability matrix. Osband and Van Roy [2016] demonstrate an ˜O(H
SAT ) bound on
the expected Bayesian regret for PSRL in ﬁnite-horizon episodic Markov decision processes  when
the episode length is H. In this paper  we consider the stronger notion of worst-case regret  aka
minimax regret  which requires bounding the maximum regret for any instance of the problem. 1
Further  we consider a non-episodic communicating MDP setting  and produce a comparable bound
of ˜O(D
SAT ) for large T   where D is the unknown diameter of the communicating MDP. In
comparison to a single sample from the posterior in PSRL  our algorithm is slightly inefﬁcient as
it uses multiple ( ˜O(S)) samples. It is not entirely clear if the extra samples are only an artifact
of the analysis. In an empirical study of a multiple sample version of posterior sampling for RL 
Fonteneau et al. [2013] show that multiple samples can potentially improve the performance of
posterior sampling in terms of probability of taking the optimal decision. Our analysis utilizes some
ideas from the Bayesian regret analysis  most importantly the technique of stochastic optimism from
Osband et al. [2014] for deriving tighter deviation bounds. However  bounding the worst-case regret
requires several new technical ideas  in particular  for proving ‘optimism’ of the gain of the sampled
MDP. Further discussion is provided in Section 4.
We should also compare our result with the very recent result of Azar et al. [2017]  which provides
an optimistic version of value-iteration algorithm with a minimax (i.e.  worst-case) regret bound of

√

√

√

1Worst-case regret is a strictly stronger notion of regret in case the reward distribution function is known
and only the transition probability distribution is unknown  as we will assume here for the most part. In case of
unknown reward distribution  extending our worst-case regret bounds would require an assumption of bounded
rewards  where as the Bayesian regret bounds in the above-mentioned literature allow more general (known)
priors on the reward distributions with possibly unbounded support. Bayesian regret bounds in those more
general settings are incomparable to the worst-case regret bounds presented here.

2

√
HSAT ) when T ≥ H 3S3A. However  the setting considered in Azar et al. [2017] is that of an
˜O(
episodic MDP  where the learning agent interacts with the system in episodes of ﬁxed and known
length H. The initial state of each episode can be arbitrary  but importantly  the sequence of these
initial states is shared by the algorithm and any benchmark policy. In contrast  in the non-episodic
setting considered in this paper  the state trajectory of the benchmark policy over T time steps can be
completely different from the algorithm’s trajectory. To the best of our understanding  the shared
sequence of initial states and the ﬁxed known length H of episodes seem to form crucial components
of the analysis in Azar et al. [2017]  making it difﬁcult to extend their analysis to the non-episodic
communicating MDP setting considered in this paper.
Among other related work  Burnetas and Katehakis [1997] and Tewari and Bartlett
[2008] present optimistic linear programming approaches that achieve logarithmic regret
bounds with problem dependent constants.
Strong PAC bounds have been provided in
Kearns and Singh [1999]  Brafman and Tennenholtz [2002]  Kakade et al. [2003]  Asmuth et al.
[2009]  Dann and Brunskill [2015]. There  the aim is to bound the performance of the policy
learned at the end of the learning horizon  and not the performance during learning as quantiﬁed here
by regret. Notably  the BOSS algorithm proposed in Asmuth et al. [2009] is similar to the algorithm
proposed here in the sense that the former also takes multiple samples from the posterior to form
an extended (referred to as merged) MDP. Strehl and Littman [2005  2008] provide an optimistic
algorithm for bounding regret in a discounted reward setting  but the deﬁnition of regret is slightly
different in that it measures the difference between the rewards of an optimal policy and the rewards
of the learning algorithm along the trajectory taken by the learning algorithm.

2 Preliminaries and Problem Deﬁnition

2.1 Markov Decision Process (MDP)
We consider a Markov Decision Process M deﬁned by tuple {S A  P  r  s1}  where S is a ﬁnite
state-space of size S  A is a ﬁnite action-space of size A  P : S × A → ∆S is the transition model 
r : S × A → [0  1] is the reward function  and s1 is the starting state. When an action a ∈ A is taken
in a state s ∈ S  a reward rs a is generated and the system transitions to the next state s(cid:48) ∈ S with

probability Ps a(s(cid:48))  where(cid:80)

s(cid:48)∈S Ps a(s(cid:48)) = 1.

We consider ‘communicating’ MDPs with ﬁnite ‘diameter’ (see Bartlett and Tewari [2009]
for an in-depth discussion). Below we deﬁne communicating MDPs  and recall some useful known
results for such MDPs.
Deﬁnition 1 (Policy). A deterministic policy π : S → A is a mapping from state space to action
space.
Deﬁnition 2 (Diameter D(M)). Diameter D(M) of an MDP M is deﬁned as the minimum time
required to go from one state to another in the MDP using some deterministic policy:

D(M) = max

s(cid:54)=s(cid:48) s s(cid:48)∈S min

π:S→A T π

s→s(cid:48) 

s→s(cid:48) is the expected number of steps it takes to reach state s(cid:48) when starting from state s and
where T π
using policy π.
Deﬁnition 3 (Communicating MDP). An MDP M is communicating if and only if it has a ﬁnite
diameter. That is  for any two states s (cid:54)= s(cid:48)  there exists a policy π such that the expected number of
steps to reach s(cid:48) from s  T π
Deﬁnition 4 (Gain of a policy). The gain of a policy π  from starting state s1 = s  is deﬁned as the
inﬁnite horizon undiscounted average reward  given by

s→s(cid:48)  is at most D  for some ﬁnite D ≥ 0.

λπ(s) = E[ lim
T→∞

1
T

rst π(st)|s1 = s].

T(cid:88)

i=1

where st is the state reached at time t.
Lemma 2.1 (Optimal gain for communicating MDPs). For a communicating MDP M with diameter
D:

3

(a) (Puterman [2014] Theorem 8.1.2  Theorem 8.3.2) The optimal (maximum) gain λ∗ is state
independent and is achieved by a deterministic stationary policy π∗  i.e.  there exists a
deterministic policy π∗ such that

Here  π∗ is referred to as an optimal policy for MDP M.

λ∗ := max

s(cid:48)∈S max

π

λπ(s(cid:48)) = λπ∗

(s) ∀s ∈ S.

(b) (Tewari and Bartlett [2008]  Theorem 4) The optimal gain λ∗ satisﬁes the following equa-

tions 

λ∗ = min
h∈RS

max
s a

rs a + P T

s ah − hs = max

rs a + P T

s ah∗ − h∗

s ∀s

(1)

where h∗  referred to as the bias vector of MDP M  satisﬁes:

a

s − min
h∗

s ≤ D.
h∗

s

max

s

Given the above deﬁnitions and results  we can now deﬁne the reinforcement learning problem
studied in this paper.

2.2 The reinforcement learning problem

The reinforcement learning problem proceeds in rounds t = 1  . . .   T . The learning agent starts from
a state s1 at round t = 1. In the beginning of every round t  the agent takes an action at ∈ A and
observes the reward rst at as well as the next state st+1 ∼ Pst at  where r and P are the reward
function and the transition model  respectively  for a communicating MDP M with diameter D.
The learning agent knows the state-space S  the action space A  as well as the rewards rs a ∀s ∈
S  a ∈ A  for the underlying MDP  but not the transition model P or the diameter D. (The assumption
of known and deterministic rewards has been made here only for simplicity of exposition  since the
unknown transition model is the main source of difﬁculty in this problem. Our algorithm and results
can be extended to bounded stochastic rewards with unknown distributions using standard Thompson
Sampling for MAB  e.g.  using the techniques in Agrawal and Goyal [2013b].)
The agent can use the past observations to learn the underlying MDP model and decide future actions.
t=1 rst at  or equivalently  minimize the total regret over

The goal is to maximize the total reward(cid:80)T

a time horizon T   deﬁned as

R(T M) := T λ∗ −(cid:80)T

t=1 rst at

where λ∗ is the optimal gain of MDP M.
We present an algorithm for the learning agent with a near-optimal upper bound on the regret
R(T M) for any communicating MDP M with diameter D  thus bounding the worst-case regret
over this class of MDPs.

(2)

3 Algorithm Description

Our algorithm combines the ideas of Posterior sampling (aka Thompson Sampling) with the extended
MDP construction used in Jaksch et al. [2010]. Below we describe the main components of our
algorithm.
Some notations: N t
s a denotes the total number of times the algorithm visited state s and played
action a until before time t  and N t
s a steps
where the next state was i  i.e.  a transition from state s to i was observed. We index the states from 1
s a for any t. We use the symbol 1 to denote the vector of all 1s  and

s a(i) denotes the number of time steps among these N t

to S  so that(cid:80)S

s a(i) = N t

i=1 N t

1i to denote the vector with 1 at the ith coordinate and 0 elsewhere.

Doubling epochs: Our algorithm uses the epoch based execution framework of Jaksch et al. [2010].
An epoch is a group of consecutive rounds. The rounds t = 1  . . .   T are broken into consecutive
epochs as follows: the kth epoch begins at the round τk immediately after the end of (k − 1)th epoch
and ends at the ﬁrst round τ such that for some state-action pair s  a  N τ
s a. The algorithm
computes a new policy ˜πk at the beginning of every epoch k  and uses that policy through all the
rounds in that epoch. It is easy to observe that irrespective of how the policy ˜πk is computed  the
number of epochs in T rounds is bounded by SA log(T ).

s a ≥ 2N τk

4

Posterior Sampling: We use posterior sampling to compute the policy ˜πk in the beginning of
every epoch. Dirichlet distribution is a convenient choice maintaining posteriors for the transition
probability vectors Ps a for every s ∈ S  a ∈ A  as they satisfy the following useful property: given
a prior Dirichlet(α1  . . .   αS) on Ps a  after observing a transition from state s to i (with underlying
probability Ps a(i))  the posterior distribution is given by Dirichlet(α1  . . .   αi + 1  . . .   αS). By this
property  for any s ∈ S  a ∈ A  on starting from prior Dirichlet(1) for Ps a  the posterior at time t is
Dirichlet({N t
Our algorithm uses a modiﬁed  optimistic version of this approach. At the beginning of every epoch
k  for every s ∈ S  a ∈ A such that Ns a ≥ η  it generates multiple samples for Ps a from a ‘boosted’
posterior. Speciﬁcally  it generates ψ = O(S log(SA/ρ)) independent sample probability vectors
Q1 k

s a(i) + 1}i=1 ... S).

s a  . . .   Qψ k

s a as

s a ∼ Dirichlet(Mτk
Qj k

s a) 

where Mt

s a denotes the vector [M t

s a(i)]i=1 ... S  with

M t

κ (N t

s a(i) := 1

s a(i) + ω)  for i = 1  . . .   S.

(cid:113) T S
(3)
A + 12ωS2  and ρ ∈ (0  1) is a parameter of
Here  κ = O(log(T /ρ))  ω = O(log(T /ρ))  η =
the algorithm. In the regret analysis  we derive sufﬁciently large constants that can be used in the
deﬁnition of ψ  κ  ω to guarantee the bounds. However  no attempt has been made to optimize those
constants  and it is likely that much smaller constants sufﬁce.
For every remaining s  a  i.e.  those with small Ns a (Ns a < η) the algorithm use a simple optimistic
sampling described in Algorithm 1. This special sampling for s  a with small Ns a has been introduced
to handle a technical difﬁculty in analyzing the anti-concentration of Dirichlet posteriors when the
parameters are very small. We suspect that with an improved analysis  this may not be required.

Extended MDP: The policy ˜πk to be used in epoch k is computed as the optimal policy of an
extended MDP ˜Mk deﬁned by the sampled transition probability vectors  using the construction of
Jaksch et al. [2010]. Given sampled vectors Qj k
s a  j = 1  . . .   ψ  for every state-action pair s  a  we
deﬁne extended MDP ˜Mk by extending the original action space as follows: for every s  a  create
ψ actions for every action a ∈ A  denoting by aj the action corresponding to action a and sample
j; then  in MDP ˜Mk  on taking action aj in state s  reward is rs a but transitions to the next state
follows the transition probability vector Qj k
s a.
Note that the algorithm uses the optimal policy ˜πk of extended MDP ˜Mk to take actions in the action
space A which is technically different from the action space of MDP ˜Mk  where the policy ˜πk is
deﬁned. We slightly abuse the notation to say that the algorithm takes action at = ˜π(st) to mean that
the algorithm takes action at = a ∈ A when ˜πk(st) = aj for some j.
Our algorithm is summarized as Algorithm 1.

4 Regret Bounds

We prove the following bound on the regret of Algorithm 1 for the reinforcement learning problem.
Theorem 1. For any communicating MDP M with S states  A actions  and diameter D  with
probability 1 − ρ. the regret of Algorithm 1 in time T ≥ CDA log2(T /ρ) is bounded as:

(cid:16)

√

(cid:17)

R(T M) ≤ ˜O

SAT + DS7/4A3/4T 1/4 + DS5/2A
where C is an absolute constant. For T ≥ S5A  this implies a regret bound of

D

(cid:16)

√

(cid:17)

.

R(T M) ≤ ˜O

D

SAT

Here ˜O hides logarithmic factors in S  A  T  ρ and absolute constants.

The rest of this section is devoted to proving the above theorem. Here  we provide a sketch of the
proof and discuss some of the key lemmas  all missing details are provided in the supplementary
material.

5

Algorithm 1 A posterior sampling based algorithm for the reinforcement learning problem

Inputs: State space S  Action space A  starting state s1  reward function r  time horizon
T   parameters ρ ∈ (0  1]  ψ = O(S log(SA/ρ))  ω = O(log(T /ρ))  κ = O(log(T /ρ))  η =

(cid:113) T S

A + 12ωS2.

Initialize: τ 1 := 1  Mτ1
s a = ω1.
for all epochs k = 1  2  . . .   do

Sample transition probability vectors: For each s  a  generate ψ independent sample probability
vectors Qj k
s a ≥ η  use samples from the Dirichlet

s a  j = 1  . . .   ψ  as follows:
(Posterior sampling): For s  a such that N τk
distribution:

•

s a ∼ Dirichlet(Mτk
Qj k

s a) 

•

(Simple optimistic sampling): For remaining s  a  with N τk
simple optimistic sampling: let

s a < η  use the following

τk
s a(i)
where ˆPs a(i) = N
  ˆPs a(i)
N
let z be a random vector picked uniformly at random from {11  . . .   1S}; set

  and ∆i = min

+ 3 log(4S)

3 ˆPs a(i) log(4S)

τk
s a

N

τk
s a

N

τk
s a

  and

s a = ˆPs a − ∆ 
P −

(cid:26)(cid:114)
s a + (1 −(cid:80)S

s a = P −
Qj k

i=1 P −

s a(i))z.

(cid:27)

s a  j = 1  . . .   ψ  s ∈ S  a ∈ A}.

Compute policy ˜πk: as the optimal gain policy for extended MDP ˜Mk constructed using sample
set {Qj k
Execute policy ˜πk:
for all time steps t = τk  τk + 1  . . .   until break epoch do

Play action at = ˜πk(st).
Observe the transition to the next state st+1.
Set N t+1
s a (i)  M t+1
≥ 2N τk
If N t+1
st at

s a (i) for all a ∈ A  s  i ∈ S as deﬁned (refer to Equation (3)).
st at  then set τk+1 = t + 1 and break epoch.

end for

end for

4.1 Proof of Theorem 1

As deﬁned in Section 2  regret R(T M) is given by R(T M) = T λ∗ −(cid:80)T

t=1 rst at  where λ∗ is
the optimal gain of MDP M  at is the action taken and st is the state reached by the algorithm at
time t. Algorithm 1 proceeds in epochs k = 1  2  . . .   K  where K ≤ SA log(T ). To bound its regret
in time T   we ﬁrst analyze the regret in each epoch k  namely 

Rk := (τk+1 − τk)λ∗ −(cid:80)τk+1−1

t=τk

rst at 

and bound Rk by roughly

D

(cid:88)

s a − N τk
N τk+1

s a

(cid:112)N τk

s a

s a
where  by deﬁnition  for every s  a  (N τk+1
visited in epoch k. The proof of this bound has two main components:

s a − N τk

s a) is the number of times this state-action pair is

(a) Optimism: The policy ˜πk used by the algorithm in epoch k is computed as an optimal gain policy
of the extended MDP ˜Mk. The ﬁrst part of the proof is to show that with high probability  the
extended MDP ˜Mk is (i) a communicating MDP with diameter at most 2D  and (ii) optimistic 
i.e.  has optimal gain at least (close to) λ∗. Part (i) is stated as Lemma 4.1  with a proof provided
in the supplementary material. Now  let ˜λk be the optimal gain of the extended MDP ˜Mk. In

6

Lemma 4.2  which forms one of the main novel technical components of our proof  we show that
with probability 1 − ρ 

(cid:113) SA

T ).

˜λk ≥ λ∗ − ˜O(D

We ﬁrst show that above holds if for every s  a  there exists a sample transition probability vector
whose projection on a ﬁxed unknown vector (h∗) is optimistic. Then  in Lemma 4.3 we prove this
optimism by deriving a fundamental new result on the anti-concentration of any ﬁxed projection
of a Dirichlet random vector (Proposition A.1 in the supplementary material).
Substituting this upper bound on λ∗  we have the following bound on Rkwith probability 1 − ρ:

Rk ≤(cid:80)τk+1−1

t=τk

(cid:16)˜λk − rst at + ˜O(D

(cid:113) SA

T )

(cid:17)

.

(4)

(b) Deviation bounds: Optimism guarantees that with high probability  the optimal gain ˜λk for MDP
˜Mk is at least λ∗. And  by deﬁnition of ˜πk  ˜λk is the gain of the chosen policy ˜πk for MDP ˜Mk.
However  the algorithm executes this policy on the true MDP M. The only difference between
the two is the transition model: on taking an action aj := ˜πk(s) in state s in MDP ˜Mk  the next
state follows the sampled distribution

˜Ps a := Qj k
s a 

(5)
where as on taking the corresponding action a in MDP M  the next state follows the distribution
Ps a. The next step is to bound the difference between ˜λk and the average reward obtained by the
algorithm by bounding the deviation ( ˜Ps a − Ps a). This line of argument bears similarities to the
analysis of UCRL2 in Jaksch et al. [2010]  but with tighter deviation bounds that we are able to
guarantee due to the use of posterior sampling instead of deterministic optimistic bias used in
UCRL2. Now  since at = ˜πk(st)  using the relation between the gain ˜λk  the bias vector ˜h  and
reward vector of optimal policy ˜πk for communicating MDP ˜Mk (refer to Lemma 2.1)

(cid:80)τk+1−1

t=τk

(cid:16)˜λ − rst at

(cid:17)

= (cid:80)τk+1−1
= (cid:80)τk+1−1

t=τk

t=τk

( ˜Pst at − 1st)T ˜h
( ˜Pst at − Pst at + Pst at − 1st)T ˜h

(6)

where with high probability  ˜h ∈ RS  the bias vector of MDP ˜Mk satisﬁes

maxs

˜hs − mins

˜hs ≤ D( ˜Mk) ≤ 2D (refer to Lemma 4.1).

Next  we bound the deviation ( ˜Ps a − Ps a)T ˜h for all s  a  to bound the ﬁrst term in above.
Note that ˜h is random and can be arbitrarily correlated with ˜P   therefore  we need to bound
maxh∈[0 2D]S ( ˜Ps a − Ps a)T h. (For the above term  w.l.o.g. we can assume ˜h ∈ [0  2D]S).
For s  a such that N τk
we show that with high probability 

s a is a sample from the Dirichlet posterior. In Lemma 4.4 

s a > η  ˜Ps a = Qj k

max

h∈[0 2D]S

s a − Ps a)T h ≤ ˜O(
( ˜P k
√

D(cid:112)N τk

s a

+

DS
N τk
s a

).

(7)

S factor over the corresponding deviation bound obtainable
This bound is an improvement by a
for the optimistic estimates of Ps a in UCRL2. The derivation of this bound utilizes and extends
s a ≤ η  ˜Ps a = Qj k
the stochastic optimism technique from Osband et al. [2014]. For s  a with N τk
s a
is a sample from the simple optimistic sampling  where we can only show the following weaker
bound  but since this is used only while N τk
s a is small  the total contribution of this deviation will
be small:

(cid:32)

(cid:115)

max

S
N τk
s a
Finally  to bound the second term in (6)  we observe that E[1T
Azuma-Hoeffding inequality to obtain with probability (1 − ρ

s a − Ps a)T h ≤ ˜O

h∈[0 2D]S

( ˜P k

D

(cid:80)τk+1−1

t=τk

(Pst at − 1st)T ˜h ≤ O((cid:112)(τk+1 − τk) log(SA/ρ)).

st+1
SA ):

+

.

DS
N τk
s a
˜h|˜πk  ˜h  st] = P T

st at

(8)

˜h and use

(9)

(cid:33)

7

Combining the above observations (equations (4)  (6)  (7)  (8)  (9))  we obtain the following bound
on Rk within logarithmic factors:
s a − N τk
N τk+1

(cid:114)

(cid:16)

√

(cid:88)

+D(cid:112)τk+1 − τk.

(cid:17)
s a ≤ η)

D(τk+1−τk)

+D

1(N τk+1

s a > η) +

S1(N τk+1

s a

(cid:112)N τk

s a

SA
T

s a

(10)
We can ﬁnish the proof by observing that (by deﬁnition of an epoch) the number of visits of any
state-action pair can at most double in an epoch 

s a − N τk
N τk+1

s a ≤ N τk
s a 

and therefore  substituting this observation in (10)  we can bound (within logarithmic factors) the

total regret R(T ) =(cid:80)K
K(cid:80)

k=1 Rk as:

D(τk+1 − τk)

(cid:32)
(cid:113) SA
T + D (cid:80)
(cid:112)N τK
SAT + D log(K)((cid:80)
s a and(cid:80)
s a ≤ T   by simple worst scenario analysis (cid:80)

s a ≤ 2N τk

τk
s a>η

s a:N

s a

where we used N τk+1
√
and SA

(cid:80)

s a N τK

k=1

√

≤ D

R(T M) ≤ ˜O(D

4.2 Main lemmas

√

(cid:33)

√

τk+1 − τk

s a + D

(cid:112)N τk
s a + D (cid:80)

√
s a ) + D log(K)(SA

s a:N

√

τk
s a<η

(cid:112)SN τk
(cid:113) T S
(cid:112)N τK
s a ≤ √

Sη) + D

KT

k(τk+1 − τk) = T . Now  we use that K ≤ SA log(T ) 
A + 12ωS2). Also  since

s a

SAT   and we obtain 

Sη = O(S7/4A3/4T 1/4 + S5/2A log(T /ρ)) (using η =

SAT + DS7/4A3/4T 1/4 + DS5/2A).

Following lemma form the main technical components of our proof. All the missing proofs are
provided in the supplementary material.
Lemma 4.1. Assume T ≥ CDA log2(T /ρ) for a large enough constant C. Then  with probability
1 − ρ  for every epoch k  the diameter of MDP ˜Mk is bounded by 2D.
Lemma 4.2. With probability 1 − ρ  for every epoch k  the optimal gain ˜λk of the extended MDP
˜Mk satisﬁes:

(cid:16)

(cid:113) SA

(cid:17)

 

T

D log2(T /ρ)
where λ∗ the optimal gain of MDP M and D is the diameter.

˜λk ≥ λ∗ − O

Proof. Let h∗ be the bias vector for an optimal policy π∗ of MDP M (refer to Lemma 2.1 in the
preliminaries section). Since h∗ is a ﬁxed (though unknown) vector with |hi − hj| ≤ D  we can
apply Lemma 4.3 to obtain that with probability 1 − ρ  for all s  a  there exists a sample vector Qj k
for some j ∈ {1  . . .   ψ} such that

s a

(cid:16)

(cid:113) SA

(cid:17)

s a)T h∗ ≥ P T

s ah∗ − δ

T

D log2(T /ρ)

(Qj k
. Now  consider the policy π for MDP ˜Mk which for any s  takes
where δ = O
action aj  with a = π∗(s) and j being a sample satisfying above inequality. Let Qπ be the transition
matrix for this policy  whose rows are formed by the vectors Qj k
s π∗(s)  and Pπ∗ be the transition
matrix whose rows are formed by the vectors Ps π∗(s). Above implies Qπh∗ ≥ Pπ∗ h∗ − δ1.
We use this inequality along with the known relations between the gain and the bias of optimal
policy in communicating MDPs to obtain that the gain ˜λ(π) of policy in π for MDP ˜Mk satisﬁes
˜λ(π) ≥ λ∗ − δ (details provided in the supplementary material)  which proves the lemma statement
since by optimality ˜λk ≥ ˜λ(π).

8

Lemma 4.3. (Optimistic Sampling) Fix any vector h ∈ RS such that |hi − hi(cid:48)| ≤ D for any i  i(cid:48) 
and any epoch k. Then  for every s  a  with probability 1 − ρ
SA there exists at least one j such that

(cid:16)

D log2(T /ρ)

(cid:113) SA

(cid:17)

.

T

Lemma 4.4. (Deviation bound) With probability 1 − ρ  for all epochs k  sample j  all s  a

(Qj k

s a)T h ≥ P T
(cid:32)
(cid:32)

s ah − O
(cid:115)
(cid:115)

D

O



N τk
s a

N τk
s a

log(SAT /ρ)

S log(SAT /ρ)

+ D

N τk
s a

  N τk

s a > η

S log(SAT /ρ)

O

D

+ D

S log(S)

N τk
s a

 

s a ≤ η
N τk

(cid:33)

(cid:33)

max

h∈[0 2D]S

(Qj k

s a − Ps a)T h ≤

5 Conclusions

√

We presented an algorithm inspired by posterior sampling that achieves near-optimal worst-case
regret bounds for the reinforcement learning problem with communicating MDPs in a non-episodic 
undiscounted average reward setting. Our algorithm may be viewed as a more efﬁcient randomized
version of the UCRL2 algorithm of Jaksch et al. [2010]  with randomization via posterior sampling
forming the key to the
S factor improvement in the regret bound provided by our algorithm. Our
analysis demonstrates that posterior sampling provides the right amount of uncertainty in the samples 
so that an optimistic policy can be obtained without excess over-estimation.
While our work surmounts some important technical difﬁculties in obtaining worst-case regret bounds
for posterior sampling based algorithms for communicating MDPs  the provided bound is tight in
its dependence on S and A only for large T (speciﬁcally  for T ≥ S5A). Other related results on
√
tight worst-case regret bounds have a similar requirement of large T (Azar et al. [2017] produce an
HSAT ) bound when T ≥ H 3S3A). Obtaining a cleaner worst-case regret bound that does
˜O(
not require such a condition remains an open question. Other important directions of future work
include reducing the number of posterior samples required in every epoch from ˜O(S) to constant or
logarithmic in S  and extensions to contextual and continuous state MDPs.

9

References
Yasin Abbasi-Yadkori and Csaba Szepesvari. Bayesian optimal control of smoothly parameterized

systems: The lazy posterior sampling algorithm. arXiv preprint arXiv:1406.3926  2014.

Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions: with formulas  graphs 

and mathematical tables  volume 55. Courier Corporation  1964.

Shipra Agrawal and Navin Goyal. Analysis of Thompson Sampling for the Multi-armed Bandit

Problem. In Proceedings of the 25th Annual Conference on Learning Theory (COLT)  2012.

Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In

Proceedings of the 30th International Conference on Machine Learning (ICML)  2013a.

Shipra Agrawal and Navin Goyal. Further Optimal Regret Bounds for Thompson Sampling. In

AISTATS  pages 99–107  2013b.

John Asmuth  Lihong Li  Michael L Littman  Ali Nouri  and David Wingate. A Bayesian sampling
approach to exploration in reinforcement learning. In Proceedings of the Twenty-Fifth Conference
on Uncertainty in Artiﬁcial Intelligence  pages 19–26. AUAI Press  2009.

Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for reinforce-

ment learning. arXiv preprint arXiv:1703.05449  2017.

Peter L Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement
learning in weakly communicating MDPs. In Proceedings of the Twenty-Fifth Conference on
Uncertainty in Artiﬁcial Intelligence  pages 35–42. AUAI Press  2009.

Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research  3(Oct):213–231  2002.

Sébastien Bubeck and Che-Yu Liu. Prior-free and prior-dependent regret bounds for Thompson

sampling. In Advances in Neural Information Processing Systems  pages 638–646  2013.

Sébastien Bubeck  Nicolo Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends R(cid:13) in Machine Learning  5(1):1–122  2012.
Apostolos N Burnetas and Michael N Katehakis. Optimal adaptive policies for Markov decision

processes. Mathematics of Operations Research  22(1):222–255  1997.

Olivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in

neural information processing systems  pages 2249–2257  2011.

Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforcement

learning. In Advances in Neural Information Processing Systems  pages 2818–2826  2015.

Raphaël Fonteneau  Nathan Korda  and Rémi Munos. An optimistic posterior sampling strat-
egy for bayesian reinforcement learning. In NIPS 2013 Workshop on Bayesian Optimization
(BayesOpt2013)  2013.

Charles Miller Grinstead and James Laurie Snell. Introduction to probability. American Mathematical

Soc.  2012.

Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11(Apr):1563–1600  2010.

Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD thesis 

University of London London  England  2003.

Emilie Kaufmann  Nathaniel Korda  and Rémi Munos. Thompson Sampling: An Optimal Finite

Time Analysis. In International Conference on Algorithmic Learning Theory (ALT)  2012.

Michael J Kearns and Satinder P Singh. Finite-sample convergence rates for Q-learning and indirect

algorithms. In Advances in neural information processing systems  pages 996–1002  1999.

10

Robert Kleinberg  Aleksandrs Slivkins  and Eli Upfal. Multi-armed bandits in metric spaces. In
Proceedings of the fortieth annual ACM symposium on Theory of computing  pages 681–690.
ACM  2008.

Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement

learning. arXiv preprint arXiv:1607.00215  2016.

Ian Osband  Dan Russo  and Benjamin Van Roy. (More) efﬁcient reinforcement learning via posterior

sampling. In Advances in Neural Information Processing Systems  pages 3003–3011  2013.

Ian Osband  Benjamin Van Roy  and Zheng Wen. Generalization and exploration via randomized

value functions. arXiv preprint arXiv:1402.0635  2014.

Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons  2014.

Daniel Russo and Benjamin Van Roy. Learning to Optimize Via Posterior Sampling. Mathematics of

Operations Research  39(4):1221–1243  2014.

Daniel Russo and Benjamin Van Roy. An Information-Theoretic Analysis of Thompson Sampling.

Journal of Machine Learning Research (to appear)  2015.

Yevgeny Seldin  François Laviolette  Nicolo Cesa-Bianchi  John Shawe-Taylor  and Peter Auer.
PAC-Bayesian inequalities for martingales. IEEE Transactions on Information Theory  58(12):
7086–7093  2012.

I. G. Shevtsova. An improvement of convergence rate estimates in the Lyapunov theorem. Doklady

Mathematics  82(3):862–864  2010.

Alexander L Strehl and Michael L Littman. A theoretical analysis of model-based interval estimation.
In Proceedings of the 22nd international conference on Machine learning  pages 856–863. ACM 
2005.

Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for
Markov decision processes. Journal of Computer and System Sciences  74(8):1309–1331  2008.

Ambuj Tewari and Peter L Bartlett. Optimistic linear programming gives logarithmic regret for
irreducible MDPs. In Advances in Neural Information Processing Systems  pages 1505–1512 
2008.

William R Thompson. On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples. Biometrika  25(3/4):285–294  1933.

11

,Shipra Agrawal
Randy Jia