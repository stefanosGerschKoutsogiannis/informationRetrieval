2018,Active Learning for Non-Parametric Regression Using Purely Random Trees,Active learning is the task of using labelled data to select additional points to label  with the goal of fitting the most accurate model with a fixed budget of labelled points. In binary classification active learning is known to produce faster rates than passive learning for a broad range of settings. However in regression restrictive structure and tailored methods were previously needed to obtain theoretically superior performance. In this paper we propose an intuitive tree based active learning algorithm for non-parametric regression with provable improvement over random sampling. When implemented with Mondrian Trees our algorithm is tuning parameter free  consistent and minimax optimal for Lipschitz functions.,Active Learning for Non-Parametric Regression

Using Purely Random Trees

Jack Goetz

Ambuj Tewari

University of Michigan
Ann Arbor  MI 48109

{jrgoetz  tewaria  paulzim}@umich.edu

Paul Zimmerman

Abstract

Active learning is the task of using labelled data to select additional points to
label  with the goal of ﬁtting the most accurate model with a ﬁxed budget of la-
belled points. In binary classiﬁcation active learning is known to produce faster
rates than passive learning for a broad range of settings. However in regression
restrictive structure and tailored methods were previously needed to obtain theo-
retically superior performance. In this paper we propose an intuitive tree based ac-
tive learning algorithm for non-parametric regression with provable improvement
over random sampling. When implemented with Mondrian Trees our algorithm is
tuning parameter free  consistent and minimax optimal for Lipschitz functions.

1

Introduction

In this paper we study active learning for regression in the pool setting. In our setup we are given
a pool of unlabelled data points and want to build the best model with a ﬁxed number of samples 
allowing selection of new points to use labels already obtained. Active learning is motivated by
scenarios where the experimenter has control over the data labelling process and where unlabelled
points are cheap but labels are expensive.
Our primary motivation comes from computational chemistry  where chemical properties of inter-
est can be computed by solving approximations to the Schrödinger equation. One key property to
chemists  the rate of chemical reaction  can be quantiﬁed via the activation energy  which controls
the rate of reaction as a function of temperature [9]. While calculating the activation energy is
expensive  there are a small number of readily available features of the reaction that inﬂuence the
activation energy. This incentivizes building a metamodel for the activation energy to avoid exces-
sive analysis of undesirable (high activation energy) reactions. Since we are restricted in the number
of simulations used to build our metamodel  we want to use the most informative data points. Be-
cause chemical reactions are discrete entities  we are restricted to a ﬁnite (but often large) pool of
reactions  thus requiring pool setting active learning even though we are selecting simulations.
Active learning methods are usually built on top of existing prediction algorithms. Decision trees
and forests are a popular class of such predictors due to their simplicity  expressiveness  state-of-
the-art performance and tuning parameter free nature. In this paper we focus our attention on purely
random trees [4]  decision trees built independently of any data  due to their amenability to theo-
retical analysis. We use a recently proposed version called Mondrian Trees [17]  which have been
shown to produce trees with many attractive properties such as consistency and minimax optimal
rate of convergence for Lipschitz functions [19].
As in some previous work [7]  our active learning algorithm will be developed in two stages. First
we introduce a simple and intuitive oracle querying algorithm for purely random trees which is
optimal among a natural class of sampling schemes which includes random sampling (Theorem

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

4.4). This algorithm is not active but uses statistics of the true joint distribution which are generally
unknown. Second we propose an active learning scheme where we ﬁrst sample passively to estimate
the required statistics  and then use those estimates to approximate the oracle algorithm. We show
this algorithm is consistent for the oracle algorithm (Theorem 5.1) and behaves well when our labels
are normally distributed (Theorem 5.4). Finally we examine the empirical performance of our active
learning algorithm to show that beneﬁts  though sometimes modest  can be signiﬁcant.

2 Setting and background

(cid:104)

( ˆf (X)− f (X))2(cid:105)

We begin by describing the pool based active learning setting  as well as introducing purely
random and Mondrian trees. We have a pool of m data points {Xi}m
1   with Xi ∈
[0  1]d (rescaling our X as needed) and Xi ∼ pX  which are always available to the algorithm. For
each Xi we have a corresponding label Yi ∈ R with the relationship Yi = f (Xi) + σ(Xi)i with
i ∼ p iid  i ⊥ Xj ∀ j  E(i) = 0  Var(i) = 1  σ(Xi) : [0  1]d → R+  meaning our noise is the
product of a function of X with an independent random variable. We assume the (Xi  Yi) = Di
have been drawn iid from a joint distribution pX Y . We will assume that f (x) and σ(x) are bounded.
Initially none of these Yi are known to the algorithm. Instead we have the ability to gain access to
any of the Yi  and the task is to select n (cid:28) m labels with the goal of building a model with the
lowest quadratic risk E
  where the expectation is taken over our test point X  the
random process which builds our tree and the labelled data we select. Throughout we will assume
that our pool is arbitrarily large; in particular we will assume that the marginal density pX is known 
and that there is enough unlabelled data to implement any sampling scheme for selecting n points.
We use active sampling (or learning) to describe any sampling scheme which samples in multiple
batches and uses both X(cid:48)
i s from previous batches when picking points for
the next batch. We use passive sampling to denote any sampling scheme which only uses the Xi to
pick points  and we use random sampling to denote picking the points uniformly at random from
our pool (which is the same as sampling from pX Y ).
Our active learning method is for purely random trees [4]  which are decision trees (or partitions
of the space) built using a random process that is independent of the data. We will interchangeably
discuss the partition of the space generated by the tree and the leaves of the tree. Let Ik ∈ I
enumerate the leaves of a tree (partitions of the space)  where k ∈ {1...K}. We will abuse notation
slightly and use the set of partitions I to denote our tree. These partitions can be used to build
regressograms  which make predictions using the average of labelled points within the partition of
the test point. With the partitions ﬁxed  the best (in L2) approximation to f which is piece-wise
constant on each partition predicts the conditional mean on that partition [14]. We will denote true
values and estimates of this approximation using "tilde" and "hat" notation as shown below.

is as well as known Y (cid:48)

True best approximation
1(x ∈ Ik) ˜βk

˜fI(x) =

K(cid:88)

k=1

˜βk = EpX Y [Y |X ∈ Ik]

ˆβk =

Estimate of best approximation
1(x ∈ Ik) ˆβk

ˆfI(x) =

K(cid:88)
1(cid:80) 1(Xi ∈ Ik)

k=1

(cid:88)

Yi

Xi∈Ik

Our experiments and some results will use particular purely random trees built using the Mondrian
Process [17]. The Mondrian Process is a stochastic process for partitioning a hypercube in Rd  a
single realization of this process gives a Mondrian Tree. The Mondrian Process iteratively splits
existing partitions  and the number of partitions is controlled by a parameter λ which  since the
Mondrian Process is a generalization of a Poisson Process  is referred to as the lifetime parameter.
As this parameter increases the number of partitions increases  and the rate at which the number
of partitions increase depends on the dimension and size of the hypercube. We will use Mondrian
Trees on a ﬁxed domain [0  1]d with varying lifetime as in [19]  which describes how these random
partitions are built.

2

3 Related work on Active Learning

The majority of theoretical work in active learning has taken place in binary classiﬁcation  and
there are many approaches which have been studied (see  e.g. [13]  [10]  [22]  [16]  [3]  [2]). These
algorithms are studied under fairly nonrestrictive assumptions (except occasionally requiring a linear
classiﬁcation boundary). It has been shown that for a variety of realistic noise conditions active
learning provides a better minimax learning rate than passive learning ([15]).
In contrast the theory for active learning in regression is less well developed. A negative result [23]
showed that for a Lipschitz regression function and constant noise variance  the minimax learning
rate for active learning was the same as that for passive (up to a constant). Additional assumptions
are required to obtain better rates. Such structure includes assumptions of piece-wise constantness of
regression function [23]  approximation of a non-linear model by a linear one [21]  locally varying
smoothness [6]  well-speciﬁed parametric model [8] or heteroskedasticity [11]  [7].
While many of these regression methods are able to provide provably better learning rates in terms of
n  d  they are often tailored for their speciﬁc assumptions and may perform poorly if the assumptions
do not hold. As a recent summary [18] of numerous ﬂexible but guarantee free methods shows  there
is great demand for active learning methods without such stringent conditions. Our active learning
algorithm will make very mild assumptions  but the improvement will not be in rates in n  d (since
it is known this is not always possible). Rather we will adopt the approach [13] of comparing the
sampling generated by our algorithm to an optimal sampling scheme  as well as to random sampling.

4 Oracle label querying algorithm
We ﬁrst describe a simple family of querying algorithms for a ﬁxed purely random tree I which are
not active. In the ﬁrst two subsections below  we will be implicitly conditioning on the tree I  but
will suppress this in the notation.

4.1 Generic algorithm

In our generic algorithm family  the tree is built without using any data. So we build the tree ﬁrst
and query based on the tree’s structure. We call it an "oracle" algorithm since it requires pX Y .

Algorithm 1: Generic "oracle" querying algorithm
Input: Leaves of our tree I  pool of data points {Xi}m
Output: The set of points to label
foreach Ik ∈ I do

i=1  label budget n and joint distribution pX Y

Calculate qk the proportion of points to select from leaf Ik  using I {Xi}m
Select nk = n · qk points uniformly at random from the pool of unlabelled points in that leaf. ;

i=1  n  pX Y . ;

end

The algorithm is described as picking nk deterministically for simpliﬁcation of notation in proofs.
However it is clear that if the nk are random then it is easy (in principle) to discuss the probabilistic
properties of the algorithm  and the details of the risk under random versions of Algorithm 1 are
discussed in the proof for Corollary 4.6. The pool marginal distribution pX and the proportion in
each leaf qk from the querying algorithm above induce a marginal distribution p(cid:48)
X  as well as a joint
distribution p(cid:48)
X. The scheme is very general  and it is worth noting that random sam-
pling is a (randomized) version of Algorithm 1. But this is enough structure to produce a somewhat
obvious but very important property of our sampling distribution restricted to each leaf.
Proposition 4.1. Fix a tree structure I  pool marginal density pX and version of Algorithm 1  giving
X (X|X ∈ Ik) denote the induced marginal
us an induced marginal density p(cid:48)
density conditioned on X ∈ Ik. Then as long as qk (cid:54)= 0  p(cid:48)
X (X|Ik) = pX (X|Ik) for any version of
Algorithm 1.

X (X|Ik) = p(cid:48)

X Y = pY |X p(cid:48)

X. Let p(cid:48)

[ ˆβk] = ˜βk (as long as Ik has at least 1 labelled
One important property this gives us is that Ep(cid:48)
point to estimate ˆβk)  meaning our sampling scheme produces unbiased estimates of the optimal
regressogram for this tree. It also allows for a bias-variance decomposition of the risk of the tree.

X Y

3

This decomposition was already known [12] under the assumption of independence between tree
structure and the data. We relax this assumption slightly as the distribution of the data depends on
the structure of the tree  but still permits this decomposition.
Corollary 4.2. For a ﬁxed tree structure I  under any sampling distribution generated by Algorithm
1 we have the following bias-variance decomposition of our risk:

( ˜fI(X) − f (X))2(cid:105)
(cid:104)

(cid:104)

( ˆfI(X) − ˜fI(X))2(cid:105)

.

+ E

(cid:104)

( ˆfI(X) − f (X))2(cid:105)

E

= E

We will refer to these as the risk bias term and risk variance term. The risk bias term depends only
on the structure of the tree  which does not depend our sampling scheme. We thus focus on the risk
variance term. Again using Proposition 4.1 we show this term for a single leaf takes a simple form.
Lemma 4.3. For a ﬁxed tree structure I  under any sampling distribution generated by Algorithm
1 we have that the variance error term on the leaf Ik is:

( ˆfI(X) − ˜fI(X))2|X ∈ Ik

E

(cid:104)

1
nk
(f (X) − ˜βk)2|X ∈ Ik

=

bias2

k := EpX Y

k + σ2
 k

σ2
 k := EpX Y

1
nk

(cid:104)

Var(Y |X ∈ Ik) 
(σ(X))2|X ∈ Ik

(cid:105)

.

(cid:105)

(cid:0)bias2
(cid:105)

 

(cid:1) =

(cid:104)

4.2 Optimal algorithm

 k have expectations taken with
In the above lemma we have emphasized that the terms bias2
respect to the data generating distribution pX Y and do not depend on the induced distribution p(cid:48)
X Y .
Thus the only way our sampling distribution affects the variance term is through nk. Averaging out
over the contribution of each leaf we get that our overall variance error term is.

k and σ2

(cid:104)
( ˆfI(X) − ˜fI(X))2(cid:105)

E

=

(cid:88)

k

(cid:0)bias2

k + σ2
 k

(cid:1).

P (X ∈ Ik)

1
nk

(1)

Let pk = P (X ∈ Ik) under the pool marginal distribution and σ2
 k. Now we are
given a budget of n data points  and we want to minimize our variance error term subject to this
budget. This gives us the following optimization problem which can be easily solved:

Y k = bias2

k + σ2

(cid:80)
subject to (cid:80)

minimize

nk

k

k

1
nk

pkσ2

Y k

nk = n

→

n∗
k = n

(cid:113)
(cid:113)
(cid:80)

k(cid:48)

pkσ2

Y k

pk(cid:48)σ2

Y k(cid:48)

The proportions are very intuitive; cells with high bias and/or noise  or high (test) marginal density
will get more samples. These results are summarized in the following theorem:
Theorem 4.4. Let Yi = f (Xi) + σ(Xi)i and ﬁx the partitions I of our tree. The risk minimizing
oracle querying algorithm out of the family of algorithms described by Algorithm 1 is the one with
the following nk and error

(cid:113)
(cid:113)
(cid:80)

k(cid:48)

n∗
k = n

pkσ2

Y k

pk(cid:48)σ2

Y k(cid:48)

 

E

(cid:104)

( ˆfI(X) − ˜fI(X))2(cid:105)

(cid:88)

(cid:113)

=

1
n

(

k

pkσ2

Y k)2.

Deﬁnition 4.5. The distribution induced by the sampling in Theorem 4.4 will be referred to as p∗
X.
Remark. This has a similar ﬂavour to uncertainty sampling methods from classiﬁcation in that
regions with greater variation will get more samples. However whereas in classiﬁcation sampling
can focus locally near the decision boundary  in regression sampling must remain global.

Random sampling is a randomized version of Algorithm 1  so the risk under random sampling is
the bias term plus a weighted average of the variance terms for different (n1  ...  nK). The sampling

4

scheme from Theorem 4.4 has the same bias term  but minimizes the variance term meaning our
optimal sampling scheme is better than any randomized version of Algorithm 1 (as long as m > n) 
including random sampling.
Corollary 4.6. For a ﬁxed tree structure I  the risk from any randomized version of Algorithm 1
is greater than the risk from sampling according to p∗
K) = 1. In particular
sampling according to p∗

X is strictly better than random sampling.

X unless P (n∗

1  ...  n∗

We can also calculate the excess error if we use the incorrect values of σ2
Y k  so
ak is a multiplicative error (we will see that our errors will be multiplicative). Given ﬁxed leaf errors
a1  ...  aK we can calculate the additional risk generated by using ˜σ2
Y k in our optimal algorithm
instead of the true σ2
Lemma 4.7. For a ﬁxed tree structure I  if nk = n
k(cid:48)
each leaf is as in Lemma 4.3  then our risk variance is:

and the variance error term for

Y k = akσ2

Y k. Let ˜σ2

Y k

√
(cid:113)
(cid:80)
(cid:88)

Y k

pk ˜σ2
pk(cid:48) ˜σ2
Y k(cid:48)
√
ak√
al

(

1
n

k<l

(cid:113)

√
al√
ak

+

− 2)

(cid:104)

( ˆfI(X) − ˜fI(X))2(cid:105)

E

(cid:88)

(cid:113)

=

1
n

(

k

pkσ2

Y k)2 +

pkplσ2

Y kσ2
Y l

:= OPT + EXCESS.

This also lets us get a sense for the suboptimality of random sampling. If we let ak = pk
then we
σ2
get nk = npk which is the expected number of samples per leaf under random sampling  and so for
large n the calculated EXCESS term will be close to the excess risk under random sampling. This
gives us the following excess error  which can be small (or even zero) as expected since random
sampling can be near-optimal. But if there is varying Y variance across the space this can be large:
Corollary 4.8. For a ﬁxed tree structure I let ak = pk

. Then our excess error is:

Y k

(cid:113)

(cid:88)

k<l

(

pkσ2

σ2

Y k

Y l −(cid:113)

EXCESS =

1
n

plσ2

Y k)2 ≤ K
n

max

k

σ2
Y k.

4.3 Additional results using Mondrian Trees
The above results hold for any purely random tree. We will now not assume that I is ﬁxed  but is
randomly built using the Mondrian Process and will take expectation over the tree building process
as well. Mondrian Trees trained using random sampling are minimax optimal for Lipschitz regres-
sion functions when the sequence of lifetime parameters satisfy λn (cid:16) n1/(d+2) and Var(Y ) < ∞
[19]. Additionally Mondrian Trees with random sampling are weakly universally consistent under
the same lifetime sequence and variance assumption. Since the optimal oracle algorithm has smaller
risk we immediately get minimax optimal rates in terms of n  d under the same assumptions lifetime
sequence by Proposition 4 in [19] and Theorem 4.4  and weak consistency under Theorem 1 in [20].
Corollary 4.9. Let our purely random trees be Mondrian Trees with lifetime parameters λn (cid:16)
n1/(d+2)  and let Y = f (X) + σ(X)  Var(Y ) < ∞. If our training data is sampled according
X then the resulting regressogram has (as n  m → ∞) minimax optimal rates  in terms of n  d 
to p∗
over Lipschitz functions with E

(cid:104)
( ˆf (X) − f (X))2(cid:105)

−2
2+d ) and is weakly consistent.

= O(n

5 Active learning algorithm

The oracle querying algorithm has many appealing qualities. However it requires knowledge of
the σ2
Y k which are never known in practice. In this section we propose a two stage active "oracle
estimating" algorithm to remedy this deﬁciency. In our ﬁrst stage we sample n(1) points according
to Algorithm 1 and use those samples to calculate estimates ˆσ2
Y k  which in turn produce
k. In the second stage we sample n(2) = n− n(1) points such that the total number
estimates ˆnk of n∗
of samples in each leaf are ˆnk. We analyze the consequences of using these estimates  and show that
in the case when Y are normal  our trees are Mondrian Trees  and our Stage 1 samples equally in
each leaf  our active algorithm is eventually near optimal with high probability. We also show that

Y k of σ2

5

in general this algorithm’s estimates ˆnk are consistent for n∗
k. Below we give the active algorithm.
By using this algorithm we have introduced two complications: One is the estimates will have errors
from using estimates ˆσ2
Y k. The other comes from reusing the data from Stage 1 in our estimates
of ˆβk. Since active learning is used exactly when data is difﬁcult to label  to make an algorithm
which is practically appealing it is important to make the most out of any labelled data. However
this introduces dependency between ˆβk and ˆnk. These issues will each be addressed separately.

Algorithm 2: Active "oracle estimating" algorithm
Input: Leaves of our tree I  pool of data points {Xi}m
n(1)  n(2)  n = n(1) + n(2).
Output: The set of labelled points.
Stage 1 ;
Query n(1) data points using a version of Algorithm 1. ;
Use those samples (Xi  Yi) to estimate ˆσ2
n(1) k−1

Y k =

1

i=1  and label budgets

(cid:80)

Xi∈Ik

( ˆβ(1) k − Yi)2 for each leaf. ;

Stage 2 ;
foreach Ik ∈ I do

Calculate ˆnk = n

end

√
(cid:113)
(cid:80)

k(cid:48)

Y k

pk ˆσ2
pk(cid:48) ˆσ2

Y k(cid:48)

the number of points in the leaf to sample. ;

Select uniformly at random n(2) k points to query from the leaf so the number of points is ˆnk. ;

5.1 Using estimates of n∗

k

σ4
(σ2

Y k

Y k. Therefore as long as our leaf kurtosis κY k =

Y k. Let us ﬁx a sequence of
First we analyze (as n increases) the effect of using the estimates ˆσ2
trees I(n) |I(n)| = Kn. Typically our trees will contain more partitions as we get more data. For
a given tree we can estimate the required leaf variances unbiasedly using the standard unbiased
sample variance on each leaf ˆσ2
Y k)2 (and thus
the variance of our sample variance) are all ﬁnite  and asymptotically our sample variances on each
leaf are consistent for the true variances on each leaf  our estimates ˆnk → n∗
k. We require strong
consistency of our variance estimates as a function of both our partitioning method and Stage 1
sampling method  which gives us ˆnk → n∗
k almost surely. If our trees are grown according to a
random process then this strong consistency may be depend on attributes of the tree which my only
be true in probability  and in this case we get ˆnk → n∗
k in probability. Both cases are covered in the
below theorem  where generally the bn denote statistics of the tree and B is either 0 or ∞.
Theorem 5.1. Assume κY k < ∞ ∀ k  n  and our sequence of trees I(n) and Stage 1 sampling
algorithm is strongly consistent for estimating the conditional variance E[(Y − f (X))2|X = x] as
some statistic bn → B. Then if bn → B almost surely our estimates ˆnk → n∗
k almost surely and if
bn → B in probability our estimates ˆnk → n∗
Remark. Note that the condition κY k < ∞ ∀ k  n is met if f  σ(X) are bounded and κ < ∞.
Now let our sequence of trees be randomly built Mondrian Trees. If we again use λn (cid:16) n1/(d+2) 
as long as n(1) increases linearly with n  these conditions are met when our ﬁrst round of sampling
entails sampling equally in each leaf.
Corollary 5.2. Let our purely random trees be Mondrian Trees with lifetime parameter sequence
λn (cid:16) n1/(d+2) and let n(1) = cn  c ∈ (0  1) a constant. Additionally let Stage 1 query by n(1) k =
∀ k. If κY k < ∞∀ k  n and pX is bounded away from 0 and ∞ on it’s support  so when pX > 0
there exists c  C s.t. c ≤ pX ≤ C  then our estimates ˆnk → n∗
Even with consistency our ﬁnite sample estimates will give us some error in ˆnk. The variance of our
Y k − (σ2
Y k)2  so our errors
sample variance is Var(ˆσ2
Y k) = 1
(σ4
nk
will scale multiplicatively with σ2
Y k when our kurtosis κY k are bounded. This allows us to use
Lemma 4.7 to bound our excess error given bounds on the (multiplicative) error ak = ˆσ2

Y k)2) +O( 1

(κY k − 1)(σ2

k in probability.

k in probability.

) ≈ 1

n(1)
Kn

Y k/σ2

Y k.

n2
k

nk

6

5.2 Reusing data

Since we are using the data in Stage 1 both to estimate ˆnk as well as in our estimator ˆβk  we have
introduced dependence between the estimated optimal leaf sample size ˆnk and leaf mean estimate
contribution from Stage 1. To understand the effects of this dependence we will break up our es-
timates of the leaf mean as ˆβk = n(1) k
  where n(i) k  ˆβ(i) k are the number and
mean estimate during sampling round i ∈ {1  2}. By writing our ﬁnal mean estimate in terms of our
stage-wise mean estimates we can ﬁnd an expression for this dependence.
Lemma 5.3. For a ﬁxed tree structure I  under Algorithm 2 the risk variance term becomes:

ˆβ(1) k+n(2) k
n(1) k+n(2) k

ˆβ(2) k

E[( ˆβk − ˜βk)2] = En(2) k

(cid:104)

n2

(1) k

(n(1) k + n(2) k)2 ED1:n(1)

(cid:2)( ˆβ(1) k − ˜βk)2|n(2) k

(cid:2)( ˆβ(1) k − ˜βk)2|n(2) k

(cid:105)
(cid:3) quantiﬁes the dependency introduced by reusing the

(n(1) k + n(2) k)2

(cid:3) +

n(2) kσ2

Y k

.

The term ED1:n(1)
samples from n(1). The dependency is between the variance of part of our mean estimators
Y K). When ˆβ(1) k ⊥ n(2) k we get back
( ˆβ(1) 1  ...  ˆβ(1) k) and (n(2) 1  ...  n(2) K) = g(ˆσ2
our risk variance term from Lemma 4.3. However when there is dependence we no longer have that
the n∗
k from Theorem 4.4 are optimal over algorithms with an active stage as in Algorithm 2  since
the optimal nk will depend on the sampling during Stage 1. This dependency can be complex and
is generally unknown  though as long as the effect is not too large the n∗
k will still provide a very
good solution  and the n∗
k are still better than random sampling. It is worth noting that our active
algorithm can take advantage of this dependency in some cases to outperform Algorithm 1  and we
informally discuss this in the appendix.

Y 1  ...  ˆσ2

5.3 The Normal case

The complications above depend on the distribution of ak =
and the function g  which in
general are extremely complicated and hard to analyze for arbitrary f  p  pX. However in the case
where Y are normally distributed these become tractable.
Theorem 5.4. Let Y ∼ N (µ(X)  σ2(X)) and X queried according to Algorithm 2 for a ﬁxed tree
I. Then the risk variance term for a leaf is as in Lemma 4.3 and we have that with probability at

Y k

Y k

ˆσ2
σ2

least 1 − K(cid:80)

e− (n(1) k−1)α2

8

k=1

EXCESS ≤ 1
n

the excess error is bounded by:

(cid:104)(cid:0) 1 + α

1 − α

(cid:1)1/4 −(cid:0) 1 − α

1 + α

(cid:88)

k<l

(cid:1)1/4(cid:105)2(cid:113)

pkplσ2

Y kσ2

Y l .

Additionally if our trees are a sequence of Mondrian Trees with lifetime parameter sequence λn (cid:16)
n1/(d+2) and our Stage 1 sampling procedure is to sample equally in each leaf with n(1) = cn  c ∈
(0  1) a constant  then the above bound occurs with probability at least 1 − δ1 − δ2 where

δ1 =

(1 + n1/(d+2))d

n(d+1)/(d+2)

δ2 = n(d+1)/(d+2) exp(cid:0)−α2

((cn)1/(d+2)) − 1(cid:1).

8

First  note that a larger n allows us to choose a smaller α and the bound on excess error goes to 0 as
α → 0. Second  even for the normal case  d large requires a very large n before we get any control
on the error probability δ2. This is consistent with the empirical observation that Mondrian Trees
struggle with large d.
Finally we also note that there are many reasons why in practice it is impossible to use the exact
n∗
k. These include the fact that usually n∗
k points in it  or
when using the active algorithm n(1) k > ˆnk. These issues will be less signiﬁcant as n → ∞ and
we discuss how each is dealt with in the appendix.

k will be fractional  a leaf may not have n∗

7

6 Simulations and experiments

2

We now examine the beneﬁts of active learning on both simulated and real world data. We sim-
ulate 2 data sets  one with differing noise variance (our σ2
 k term)  the other with differing func-
tion complexity (our bias2
k term)  in different regions of [0  1]d. We also examine performance
on the Wine quality data set from UCI and a data set of activation energies of Claisen rear-
rangement reactions (Cl). We compare the performance of selecting points to label using ran-
dom sampling  our active algorithm  and a naive uncertainty sampling version of our active al-
gorithm  where each leaf nk is proportional its variance. In all experiments n(1) = n
2 and Mon-
2+d − 1  which is theoretically motivated  but corrected
drian Trees are grown using λn = n
so when n = 1  λn = 0. We use both Mondrian and Breiman Trees [5] as our ﬁnal regres-
sor. Details of the data sets are in the appendix  which also contains forest versions of these ex-
periments. Additionally all code and experiments (as well as other experiments) are available at
https://github.com/jackrgoetz/Mondrian_Tree_AL.
When using Mondrian Trees as the ﬁnal regressor  the active learning method always provides some
improvement  and in the simulations this improvement persists when using Breiman Trees. Addi-
tionally the uncertainty sampling method sometimes produces worse sampling than random sam-
pling  which is common for direct translations of classiﬁcation active learning methods.
In the
real data our beneﬁts are less pronounced  with active learning even being slightly harmful when
used with Breiman Trees (although with forests the active learning is beneﬁcial). We believe this
performance drop may be due to the inability of the Mondrian Tree to adapt to differing variable
importance. It is also possible that our assumptions that Y has changing variance does not hold  and
even here the active algorithm is not harmful  where as the naive uncertainty sampling algorithm can
be detrimental.

Figure 1: Active learning experiments

8

9.810.010.2Heteroskedastic sim. m = 40000  d = 10MT - ActiveMT - RandomMT - Uncertainty1000200030004000500018.519.019.5BT - ActiveBT - RandomBT - UncertaintyMSEFinal number of labelled points8283848586Varying complexity sim. m = 40000  d = 1010002000300040005000148150152154MSEFinal number of labelled points0.720.740.76Wine experiment. m = 4898  d = 11250500750100012501500175020000.60.81.0MSEFinal number of labelled points1415161718Cl experiment. m = 1508  d = 12100200300400500600700246MSEFinal number of labelled points7 Conclusion and further directions

In this paper we provide a theoretically justiﬁed active learning method for non-parametric regres-
sion which can take advantage of beneﬁcial structure when present without being detrimental when
such structure is absent. When used with Mondrian Trees the method requires no tuning param-
eters (which are difﬁcult to tune while actively sampling [1])  is asymptotically minimax optimal
for Lipschitz regression functions  and is consistent. Although the improvement for active learning
in regression is often restricted to constant factor improvements  these constant improvements are
important in real world applications.
Despite technical theoretical arguments needed for the theory  the method itself is simple  leading
to many interesting avenues for further exploration. One direction would be extending theory to
ensembles of trees  or developing tools to deal with high dimensions. Another possibility is to
exploit the online nature of Mondrian Trees to develop a parallel theory for streaming based active
learning. Finally it may be possible to extend the ideas here to non tree based active learning for
regression.

Acknowledgements

JG acknowledges the support of NSF via grant DMS-1646108. AT acknowledges the support of a
Sloan Research Fellowship.

References
[1] Attenberg  J. and Provost  F. (2011). Inactive learning?: difﬁculties employing active learning

in practice. ACM SIGKDD Explorations Newsletter  12(2):36–41.

[2] Awasthi  P.  Balcan  M. F.  and Long  P. M. (2014). The power of localization for efﬁciently
learning linear separators with noise. In Proceedings of the forty-sixth annual ACM symposium
on Theory of computing  pages 449–458. ACM.

[3] Balcan  M.-F.  Beygelzimer  A.  and Langford  J. (2009). Agnostic active learning. Journal of

Computer and System Sciences  75(1):78–89.

[4] Breiman  L. (2000). Some inﬁnity theory for predictor ensembles. Technical report  Technical

Report 579  Statistics Dept. UCB.

[5] Breiman  L. (2017). Classiﬁcation and regression trees. Routledge.

[6] Bull  A. D. (2013). Spatially-adaptive sensing in nonparametric regression. The Annals of

Statistics  41(1):41–62.

[7] Chaudhuri  K.  Jain  P.  and Natarajan  N. (2017). Active heteroscedastic regression. In Inter-

national Conference on Machine Learning  pages 694–702.

[8] Chaudhuri  K.  Kakade  S. M.  Netrapalli  P.  and Sanghavi  S. (2015). Convergence rates of ac-
tive learning for maximum likelihood estimation. In Advances in Neural Information Processing
Systems  pages 1090–1098.

[9] Cramer  C. J. (2013). Essentials of computational chemistry: theories and models. John Wiley

& Sons.

[10] Dasgupta  S.  Hsu  D. J.  and Monteleoni  C. (2008). A general agnostic active learning algo-

rithm. In Advances in neural information processing systems  pages 353–360.

[11] Efromovich  S. (2008). Optimal sequential design in a controlled non-parametric regression.

Scandinavian Journal of Statistics  35(2):266–285.

[12] Genuer  R. (2012). Variance reduction in purely random forests. Journal of Nonparametric

Statistics  24(3):543–562.

9

[13] Golovin  D. and Krause  A. (2011). Adaptive submodularity: Theory and applications in active

learning and stochastic optimization. Journal of Artiﬁcial Intelligence Research  42:427–486.

[14] Györﬁ  L.  Kohler  M.  Krzyzak  A.  and Walk  H. (2006). A distribution-free theory of non-

parametric regression. Springer Science & Business Media.

[15] Hanneke  S. and Yang  L. (2015). Minimax analysis of active learning. Journal of Machine

Learning Research  16(12):3487–3602.

[16] Hoang  T. N.  Low  B. K. H.  Jaillet  P.  and Kankanhalli  M. (2014). Nonmyopic -bayes-
optimal active learning of gaussian processes. In Proceedings of the 31st International Confer-
ence on Machine Learning  volume 32 of Proceedings of Machine Learning Research  pages
739–747.

[17] Lakshminarayanan  B.  Roy  D. M.  and Teh  Y. W. (2014). Mondrian forests: Efﬁcient online

random forests. In Advances in neural information processing systems  pages 3140–3148.

[18] Liu  H.  Ong  Y.-S.  and Cai  J. (2017). A survey of adaptive sampling for global metamodeling
in support of simulation-based complex engineering design. Structural and Multidisciplinary
Optimization  pages 1–24.

[19] Mourtada  J.  Gaïffas  S.  and Scornet  E. (2017). Universal consistency and minimax rates for
online mondrian forests. In Advances in Neural Information Processing Systems  pages 3761–
3770.

[20] Mourtada  J.  Gaïffas  S.  and Scornet  E. (2018). Minimax optimal rates for mondrian trees

and forests. arXiv preprint arXiv:1803.05784.

[21] Sabato  S. and Munos  R. (2014). Active regression by stratiﬁcation. In Advances in Neural

Information Processing Systems  pages 469–477.

[22] Sourati  J.  Akcakaya  M.  Leen  T. K.  Erdogmus  D.  and Dy  J. G. (2017). Asymptotic
analysis of objectives based on ﬁsher information in active learning. Journal of Machine Learning
Research  18(34):1–41.

[23] Willett  R.  Nowak  R.  and Castro  R. M. (2006). Faster rates in regression via active learning.

In Advances in Neural Information Processing Systems  pages 179–186.

10

,Hossein Azari Soufiani
William Chen
David Parkes
Lirong Xia
Gabriel Goh
Andrew Cotter
Maya Gupta
Michael Friedlander
Jack Goetz
Ambuj Tewari
Paul Zimmerman