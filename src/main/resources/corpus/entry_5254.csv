2018,Stochastic Primal-Dual Method for Empirical Risk Minimization with O(1) Per-Iteration Complexity,Regularized empirical risk minimization problem with linear predictor appears frequently in machine learning. In this paper  we propose a new stochastic primal-dual method to solve this class of problems. Different from existing methods  our proposed methods only require O(1) operations in each iteration. We also develop a variance-reduction variant of the algorithm that converges linearly. Numerical experiments suggest that our methods are faster than existing ones such as proximal SGD  SVRG and SAGA on high-dimensional problems.,Stochastic Primal-Dual Method for Empirical Risk
Minimization with O(1) Per-Iteration Complexity

Conghui Tan∗

The Chinese University of Hong Kong

chtan@se.cuhk.edu.hk

Tong Zhang
Tencent AI Lab

tongzhang@tongzhang-ml.org

Shiqian Ma

University of California  Davis

sqma@math.ucdavis.edu

Ji Liu

Tencent AI Lab  University of Rochester

ji.liu.uwisc@gmail.com

Abstract

Regularized empirical risk minimization problem with linear predictor appears
frequently in machine learning. In this paper  we propose a new stochastic primal-
dual method to solve this class of problems. Different from existing methods  our
proposed methods only require O(1) operations in each iteration. We also develop
a variance-reduction variant of the algorithm that converges linearly. Numerical
experiments suggest that our methods are faster than existing ones such as proximal
SGD  SVRG and SAGA on high-dimensional problems.

(cid:40)

n(cid:88)

i=1

(cid:41)

d(cid:88)

1

Introduction

In this paper  we consider the convex regularized empirical risk minimization with linear predictors:

min
x∈X

P (x) (cid:44) 1
n

φi(a(cid:62)

i x) + g(x)

 

(1)

where X ⊂ Rd is a convex closed feasible set  ai ∈ Rd is the i-th data sample  φi is its corresponding
convex closed loss function  and g(x) : X → R is a convex closed regularizer for model parameter x.
Here we assume the feasible set X and the regularizer function g(x) are both separable  i.e. 

X = X1 × ··· × Xd

and g(x) =

gj(xj).

(2)

j=1

2(cid:107)x(cid:107)2

Problem (1) with structure (2) generalizes many well-known classiﬁcation and regression problems.
For example  support vector machine is with this form by choosing φi(u) = max{0  1 − biu} and
2. Other examples include (cid:96)1 logistic regression  (cid:96)2 logistic regression  and LASSO.
g(x) = λ
One popular choice for solving (1) is the proximal stochastic gradient descent method (PSGD). In
each iteration of PSGD  an index i is randomly sampled from {1  2  . . .   n}  and then the iterates are
updated using only the information of ai and φi. As a result  the per-iteration cost of PSGD is O(d)
and independent of n.
It is well known that PSGD converges at a sub-linear rate [9] even for strongly convex problems  due
to non-diminishing variance of the stochastic gradients. One line of research tried is dedicated to
improve the convergence rate of PSGD by utilizing the ﬁnite sum structure in (1). Some representative

∗This work was done while Conghui Tan was a research intern at Tencent AI lab.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

works include SVRG [5  16]  SDCA [13  14]  SAGA [4] and SPDC [18]. All these accelerated
variants enjoy linear convergence when g(x) is strongly convex and all φi’s are smooth.
Since all of these algorithms need to sample at least one data ai in each iteration (so their per-
iteration cost is at least O(d))  their potential drawbacks include: 1) they are not suitable for the
distributed learning with features distributed; 2) it may incur heavy computation per iteration in the
high-dimensional case  i.e.  when d is very large.
Our contributions. In this paper  we explore the possibility of accelerating PSGD by making each
iteration more light-weighted: only one coordinate of one data is sampled  i.e.  one entry aij  in
each iteration of the algorithm. This leads to a new algorithm  named SPD1 (stochastic primal-dual
method with O(1) per-iteration complexity)  whose per-iteration cost is only O(1). We prove that
√
the convergence rate of the new method is O(1/
t) for convex problems and O(ln t/t) for strongly
convex and smooth problems  where t is the iteration counter. Moreover  the overall computational
cost is the same as PSGD in high-dimensional settings. Therefore  we managed to reduce the
per-iteration complexity from O(d) to O(1) while keep the total computational cost at the same
order. Furthermore  by incorporating the variance reduction technique  we develop a variant of
SPD1  named SPD1-VR  that converges linearly for strongly convex problems. Comparing with
existing methods  our SPD1 and SPD1-VR algorithms are more suitable for distributed systems
by allowing the ﬂexibility of either feature distributed or data distributed. An additional advantage
of our O(1) per-iteration complexity algorithms is that they are more favorable by asynchronous
parallelization and bring more speedup since they admit much better tolerance to the staleness caused
by asynchronity. Our numerical tests indicate that our methods are faster than both PSGD and SVRG
on high-dimensional problems  even in single-machine setting.
We notice that [6] and [17] used similar ideas in the sense that in each iteration only one coordinate
of the iterate is updated using one sampled data. However  we need to point out that their algorithms
still need to sample the full vector ai to compute the directional gradient  and thus the per-iteration
cost is still O(d).

1.1 Notation
We use A ∈ Rn×d to denote the data matrix  whose rows are denoted by ai  i = 1  2  . . .   n. We use
aij to denote the j-th entry of ai. [n] denotes the set {1  2  . . .   n}. xt denotes the iterate in the t-th
j is its j-th entry. We always use (cid:107)w(cid:107) to denote the (cid:96)2 norm of w unless otherwise
iteration and xt
speciﬁed.
For function f (z) whose domain is Z  its proximal mapping is deﬁned as

(cid:26)

(cid:27)

proxf (z) = arg min

z(cid:48)∈Z

f (z(cid:48)) +

1
2

(cid:107)z(cid:48) − z(cid:107)2

.

(3)

We use ∂f (z) to denote the subdifferential of f at point z. f is said to be µ-strongly convex (µ > 0)
if

f (z(cid:48)) ≥ f (z) + s(cid:62)(z(cid:48) − z) +

(cid:107)z(cid:48) − z(cid:107)2 ∀s ∈ ∂f (z) ∀z  z(cid:48) ∈ Z.

µ
2

The conjugate function of φi : R → R is deﬁned as

φ∗
i (y) = sup
x∈R

{y · x − φi(x)} .

(4)

Function φi is L-Lipschitz continuous if

which is equivalent to

|φi(x) − φi(y)| ≤ L|x − y| 

∀x  y ∈ R 

|s| ≤ L 

∀s ∈ ∂φi(x) ∀x.

φi is said to be (1/γ)-smooth if it is differentiable and its derivative is (1/γ)-Lipschitz continuous.
We use R (cid:44) maxi∈[n] (cid:107)ai(cid:107) to denote the maximum row norm of A  and R(cid:48) (cid:44) maxj∈[d] (cid:107)a(cid:48)
j(cid:107) to
denote the maximum column norm of A  where a(cid:48)

d are the columns of matrix A.

1  . . .   a(cid:48)

2

2 Stochastic Primal-Dual Method with O(1) Per-Iteration Cost
(cid:41)

Our algorithm solves the following equivalent primal-dual reformulation of (1):

(cid:40)

n(cid:88)

φ∗
i (yi) + g(x)

F (x  y) (cid:44) 1
n

y(cid:62)Ax − 1
n

 

i=1

min
x∈X

max
y∈Y

(5)
i (yi) < ∞} is the dual feasible set resulted by the
i . For example  when φi is L-Lipschitz continuous  we have Yi ⊂ [−L  L] (see

where Y = Y1 × ··· × Yn and Yi = {yi ∈ Rn|φ∗
conjugate function φ∗
Lemma 4 in the Supplementary Materials.)
Our SPD1 algorithm for solving (5) is presented in Algorithm 1. In each iteration of the algorithm 
only one coordinate of x and one coordinate of y are updated by using a randomly sampled data entry
aitjt. Therefore  SPD1 only requires O(1) time per iteration. Because of this  SPD1 is a kind of
randomized coordinate descent method.
Algorithm 1 Stochastic Primal-Dual Method with O(1) Per-Iteration Cost (SPD1)

Parameters: primal step sizes {ηt}  dual step sizes {τt}
Initialize x0 = arg minx∈X g(x) and y0
for t = 0  1 ···   T − 1 do

Randomly sample it ∈ [n] and jt ∈ [d] independently and uniformly
xt+1
j =

j − ηt · aitjyt

it

i = arg minyi∈Yi φ∗
(cid:1)
i + τt · aijtxt

if j = jt
if j (cid:54)= jt

if i = it
if i (cid:54)= it

i (yi) for all i ∈ [n]

i

xt
j

(cid:0)yt

(cid:0)xt
(cid:26) proxηtgj
(cid:26) prox(τt/d)φ∗
(cid:80)T−1
F (x  y) =

max
y∈Y

min
x∈X

jt

(cid:1)
(cid:80)T−1
(cid:20)

d(cid:88)

n(cid:88)

1
n

i=1

j=1

t=0 xt and ˆyT = 1

T

t=0 yt

yt+1
i =
end for
Output: ˆxT = 1
T

yt
i

The intuition of this algorithm is as follows. One can rewrite (5) into:

aijyixj − 1
d

φ∗
i (yi) + gj(xj)

(cid:21)  

which has a two-layer ﬁnite-sum structure. Then Algorithm 1 can be viewed as a primal-dual version
of stochastic gradient descent (SGD) on this ﬁnite-sum problem  which samples a pair of induces
(it  jt) and then only utilizes the corresponding summand to do updates in each iteration. Hence  one
can view SPD1 as a combination of randomized coordinate descent method and stochastic gradient
descent method applied to the primal-dual reformulation (5).
Note that in the initialization stage  we need to minimize g(x) and φ∗
i (yi). Since we assume the
proximal mappings of g(x) and φ∗
i are easy to solve  these two direct minimization problems should
be even easier  and thus would not bring any trouble in implementation of this algorithm. For example 
when g(x) = λ(cid:107)x(cid:107)1  it is well known its proximal mapping is the soft thresholding operator. While
its direct minimizer  namely x0 = minx g(x)  is simply x0 = 0.
As a ﬁnal remark  we point out that the primal-dual reformulation (5) is a convex-concave bilinear
saddle point problem (SSP). This problem has drawn a lot of research attentions recently. For example 
Chambolle and Pock developed an efﬁcient primal-dual gradient method for solving bilinear SSP in
[2]  which has an accelerated rate in certain circumstances. Besides  in [3]  Dang and Lan proposed a
randomized algorithm for solving (5). However  their algorithm needs to use the full-dimensional
gradient in each iteration  so the per-iteration cost is much higher than SPD1.

3 SPD1 with Variance Reduction

As we have discussed above  SPD1 has some close connection to SGD. Hence  we can incorporate the
variance reduction technique [5] to reduce the variance of the stochastic gradients so that to improve
the convergence rate of SPD1. This new algorithm  named SPD1-VR  is presented in Algorithm 2.
Similar to SVRG [5]  SPD1-VR has a two-loop structure. In the outer loop  the snapshots of the full

3

gradients are computed for both ˜xk and ˜yk. In the inner loop  the updates are similar to Algorithm 1 
but the stochastic gradient is replaced by its variance reduced version. That is  aitjtyt
it is replaced by
x jt is the jt-th coordinate of the latest snapshot of full gradient.
aitjt (yt
it
This variance reduced stochastic gradient is still an unbiased estimator of the full gradient along
direction xjt  i.e. 

x jt  where Gk

− ˜yk

) + Gk

it

(cid:2)aitjt(yt

it

Eit

− ˜yk

it

) + Gk

x jt

(cid:3) = Eit

(cid:2)aitjtyt

it

(cid:3) =

n(cid:88)

i=1

1
n

aijtyt
i .

Because of the variance reduction technique  ﬁxed step sizes η and τ can be used instead of diminish-
ing ones.

Algorithm 2 SPD1 with Variance Reduction (SPD1-VR)

y = (1/d)A˜xk

Parameters: primal step size η  dual step size τ
Initialize ˜x0 ∈ X and ˜y0 ∈ Y
for k = 0  1  . . .   K − 1 do
Compute full gradients Gk
Let (x0  y0) = (˜xk  ˜yk)
for t = 0  1  . . .   T − 1 do
Randomly sample it  i(cid:48)

x = (1/n)A(cid:62) ˜yk and Gk

(cid:40)
(cid:40)

t

xt

ai(cid:48)

(cid:104)

tj(yt
i(cid:48)

proxηgj
xt
j

t ∈ [n] and jt  j(cid:48)

j − η ·(cid:16)
(cid:104)
i + τ ·(cid:16)
(cid:2)xt
j − η ·(cid:0)aitj(¯yt
(cid:26) proxηgj
(cid:2)yt
i + τ ·(cid:0)aijt(¯xt
(cid:26) prox(τ /d)φ∗

prox(τ /d)φ∗
yt
i

aij(cid:48)

xt
j

yt

it

t

i

t

(xt
j(cid:48)
− ˜yk

jt

(cid:17)(cid:105)

t ∈ [d] independently
− ˜yk
(cid:105)
i(cid:48)

) + Gk

(cid:17)

x j

t

− ˜xk
j(cid:48)

t

)

+ Gk
y i

(cid:1)(cid:3)

x j

it

) + Gk
− ˜xk

jt

) + Gk
y i

(cid:1)(cid:3)

¯xt
j =

¯yt
i =

xt+1
j =

if j = jt
if j (cid:54)= jt

if i = it
if i (cid:54)= it
if j = jt
if j (cid:54)= jt

if i = it
if i (cid:54)= it

yt+1
i =
end for
Set (˜xk+1  ˜yk+1) = (xT   yT )

yt
i

i

end for
Output: ˜xK and ˜yK

t  j(cid:48)

Besides the variance reduction technique  another crucial difference between SPD1 and SPD1-VR
is that the latter is in fact an extragradient method [7]. Note that each iteration of the inner loop
of SPD1-VR consists two gradient steps: the ﬁrst step is a normal gradient descent/ascent  while
in the second step  it starts from xt and yt but uses the gradient estimations at (¯xt  ¯yt). For saddle
point problems  extragradient method has stronger convergence guarantees than simple gradient
methods [8]. Moreover  in each iteration of SPD1-VR  two independent pairs of random indices
(it  jt) and (i(cid:48)
t) are drawn. This is because two stochastic gradients are needed for the extragradient
framework. Similar to the classical analysis of stochastic algorithms  we need the stochastic gradients
to be independent. However  when updating ¯xt and xt+1  we choose the same coordinate jt  so the
independence property is only required for two directional stochastic gradients along coordinate jt.
We note that every iteration of the inner loop only involves O(1) operations in SPD1-VR. Full
gradients are computed in each outer loop  whose computational cost is O(nd).
Finally  we have to mention that [12] also developed a variance-reduction method for solving convex-
concave saddle point problems  which is related to Algorithm 2. However  except for the common
variance reduction ideas used in both methods  the method in [12] and SPD1-VR are quite different.
First  there is no coordinate descent counterpart in their method  so the per-iteration cost is much
higher than our method. Second  their method is a gradient method instead of extragradient method
like SPD1-VR. Third  their method has quadratic dependence on the problem condition number
unless extra acceleration technique is combined  while our method depends only linearly on condition
number as shown in Section 4 .

4

4

Iteration Complexity Analysis

4.1

Iteration Complexity of SPD1

In this subsection  we analyze the convergence rate of SPD1 (Algorithm 1). We measure the optimality
of the solution by primal-dual gap  which is deﬁned as

G(ˆxT   ˆyT ) (cid:44) sup
y∈Y

F (ˆxT   y) − inf
x∈X

F (x  ˆyT ).

Note that primal-dual gap equals 0 if and only if (ˆxT   ˆyT ) is a pair of primal-dual optimal solutions
to problem (5). Besides  primal-dual gap is always an upper bound of primal sub-optimality:

G(ˆxT   ˆyT ) ≥ sup
y∈Y
≥ sup
y∈Y
=P (ˆxT ) − inf
x∈X

F (ˆxT   y) − sup
y∈Y
F (ˆxT   y) − inf
x∈X

P (x).

F (x  y)

F (x  y)

inf
x∈X

sup
y∈Y

Our main result for the iteration complexity of SPD1 is summarized in Theorem 1.
Theorem 1. Assume each φi is L-Lipschitz continuous  and the primal feasible set X is bounded 
i.e. 

D (cid:44) sup
x∈X

(cid:107)x(cid:107) < ∞.

If we choose the step sizes in SPD1 as
√
√
2dD

ηt =

LR

t + 1

and

τt =

√
DR(cid:48)√

2dLn

 

t + 1

then we have the following convergence rate for SPD1:

E(cid:2)G(ˆxT   ˆyT )(cid:3) ≤

√

2dLD · (R + R(cid:48))

√

T

.

(6)

Note that when the problem is high-dimensional  i.e.  d ≥ n  it usually holds that R ≥ R(cid:48). In this
case  Theorem 1 implies that SPD1 requires
O

(cid:18) dL2D2R2

(cid:19)

(7)

2

iterations to ensure that the primal-dual gap is smaller than .
Under the same assumptions  if we directly apply the classical result by Nemirovski et al. [9] for
PSGD on the primal problem (1)  the number of iterations needed by PSGD is

(cid:18) L2D2R2

(cid:19)

 

2

O

in order to reduce the primal sub-optimality to be smaller than . Considering that each iteration of
PSGD costs O(d) computation  its overall complexity is actually the same as (7).
If we further impose strong convexity and smoothness assumptions  we get an improved iteration
complexity shown in Theorem 2.
Theorem 2. We assume the same assumptions as in Theorem 1. Moreover  we assume that g(x) is
µ-strongly convex (µ > 0)  and all φi are (1/γ)-smooth (γ > 0). If the step sizes in SPD1 are chosen
as

2

ηt =

µ(t + 4)

and τt =

2nd

 

γ(t + 4)

we have the following convergence rate for SPD1:

E(cid:2)G(ˆxT   ˆyT )(cid:3) ≤ 4dD2µ + 4L2γ + 2L2R2/µ · ln(T + 4) + 2dD2R(cid:48)2/γ · ln(T + 4)

.

(8)

T

5

Comparing with the classical convergence rate result of PSGD  we note that the convergence rate
of SPD1 not only depends on µ  but also depends on the dual strong convexity parameter γ. This
is reasonable because SPD1 has stochastic updates for both primal and dual variables  and this is
why γ > 0 is necessary for ensuring the O(ln T /T ) convergence rate. Furthermore  we believe that
the factor ln T is removable in (8) so O(1/T ) convergence rate can be obtained  by applying more
sophisticated analysis technique such as optimal averaging [15]. We do not dig into this to keep the
paper more succinct.

4.2

Iteration Complexity of SPD1-VR

In this subsection  we analyze the iteration complexity of SPD1-VR (Algorithm 2). Before stating the
main result  we ﬁrst introduce the notion of condition number. When each φi is (1/γ)-smooth  and
g(x) is µ-strongly convex  the condition number of the primal problem (1) deﬁned in the literature is
(see  e.g.  [18]):

κ =

R2
µγ

.

Here we also deﬁne another condition number:
κ(cid:48) =

dR(cid:48)2
nµγ

.

Since R is the maximum row norm and R(cid:48) is the maximum column norm of data matrix A ∈ Rn×d 
usually R2 and (d/n)R(cid:48)2 should be in the same order  which means κ(cid:48) ≈ κ. Without loss of
generality  we assume that κ ≥ 1 and κ(cid:48) ≥ 1.
Theorem 3. Assume each φi is (1/γ)-smooth (γ > 0)  and g(x) is µ-strongly convex (µ > 0). If we
choose the step sizes in SPD1-VR as

γ

128R2 · min

η =

(9)
and let T ≥ c · max{dκ  nκ(cid:48)} for some uniform constant c > 0 independent of problem setting 
SPD1-VR converges linearly in expectation:

nκ(cid:48)   1

τ =

and

dκ

  1

 

nµ

128R(cid:48)2 · min

(cid:26) dκ

(cid:27)

(cid:26) nκ(cid:48)

(cid:27)

(cid:19)K · ∆0 

(cid:18) 3

5

E [∆K] ≤

(cid:107)˜xk − x∗(cid:107)2

(cid:107)˜yk − y∗(cid:107)2

∆k =

+

(cid:18)

(cid:18)

O

(cid:19)

1


(cid:19)

1


6

where

τ
and (x∗  y∗) is a pair of primal-dual optimal solution to (5).
This theorem implies that we need O(log(1/)) outer loops  or O(max{dκ  nκ(cid:48)} log(1/)) inner
loops to ensure E [∆k] ≤ . Considering that there are Θ(nd) extra computation cost for computing
the full gradient in every outer loop  the total complexity of this algorithm is

η

O

(nd + max{dκ  nκ(cid:48)}) log

.

(10)

As a comparison  the complexity of SVRG under same setting is [16]:

 

d(n + κ) log

(11)
Since usually it holds κ(cid:48) ≈ κ  dκ will dominate nκ(cid:48) when d > n. In this case  the two complexity
bounds (10) and (11) are the same.
Although the theoretical complexity of SPD1-VR is same as SVRG when d ≥ n  we empirically
found that SPD1-VR is signiﬁcantly faster than SVRG in high-dimensional problems (see Section
5)  by allowing much larger step sizes than the ones in (9) suggested by theory. We conjecture that
this is due to the power of coordinate descent. Nesterov’s seminal work [11] has rigorously proved
that coordinate descent can reduce the Lipschitz constant of the problem  and thus allows larger step

Figure 1: Experimental results on synthetic data. n is set to 1000 in all ﬁgures  while d varies
from 100 to 10000. λ is ﬁxed as λ = 10−3. The y-axis here is the primal sub-optimality  namely
P (xt) − P (x∗).

sizes than gradient descent. However  due to the sophisticated coupling of primal and dual variable
updates in our algorithm  our analysis is currently unable to reﬂect this property.
We point out that the existing accelerated algorithms such as SPDC [18] and Katyusha [1] have better
complexity given by

(cid:18)

O

d(n +

√

nκ) log

1


(cid:19)

.

(12)

These accelerated algorithms employ Nesterov’s extrapolation techniques [10] to accelerate the
algorithms. We believe that it is also possible to incorporate the same technique to further accelerate
SPD1-VR  but we leave this as a future work at this moment.

5 Experiments

In this section  we conduct numerical experiments of our proposed algorithms. Due to space limitation 
we only present part of the results here  more experiments can be found in supplementary materials.
Here we consider solving a classiﬁcation problem with logistic loss function

φi(u) = log (1 + exp{−biu})  

where bi ∈ {±1} is the class label. Note that this loss function is smooth. Although this φ∗
i does not
admit closed-form solution for its proximal mapping  following [13]  we apply Newton’s method
to compute its proximal mapping  which can achieve very high accuracy in very few (say  5) steps.
Since the proximal sub-problem of φ∗
i is a 1-dimensional optimization problem  using Newton’s
method here is actually quite cheap. Besides  We use g(x) = λ
We compare our SPD1 and SPD1-VR with some standard stochastic algorithms for solving (1) 
including PSGD  SVRG and SAGA. We always set T = nd for SPD1-VR and T = n for SVRG 
where T is the number of inner loops in each outer loop.

2(cid:107)x(cid:107)2 as the regularizer.

5.1 Results on Synthetic Data

Since our theory in Section 4 suggest that the performance of our proposed methods relies on the
relationship between n and d. Here we will test our methods on synthetic dataset with different n
and d to see their effects to the performance. To generate the data  we ﬁrst randomly sample the data
matrix A and a vector ¯x ∈ Rd with entries i.i.d. drawn from N (0  1)  and then generate the labels as

bi = sign(cid:0)a(cid:62)

(cid:1)  

i ¯x + εi

εi ∼ N (0  σ2) 

for some constant σ2 > 0. Since the focus here is the relationship between n and d  in order to
simplify the experiments  we ﬁx n as n = 1000  but vary the value of d with values chosen from
d ∈ {100  1000  10000}.
The results are presented in Figure 1. When d = 100 < n  it is clear that SPD1 is slower than PSGD 
and SPD1-VR is also inferior than both SVRG and SAGA. While when d = 1000 = n  even though
SPD1 falls behind PSGD at the beginning  their ﬁnal performance is quite close at last  and SPD1-VR

7

0102030405060#Pass through data108107106105104103102101d=100SAGASVRGPSGDSPD1SPD1-VR020406080100#Pass through data108107106105104103102101100d=1000SAGASVRGPSGDSPD1SPD1-VR020406080100#Pass through data104103102101100d=10000SAGASVRGPSGDSPD1SPD1-VRTable 1: Summary of datasets

Dataset

colon-cancer

gisette

rcv1.binary

n
62

6 000
20 242

d

2 000
5 000
47 236

λ
1
10−2
10−3

Figure 2: Numerical results on three real datasets. The y-axis is also primal sub-optimality.

begins to beat both SVRG and SAGA. Finally  when d > n  SPD1 becomes obviously faster than
PSGD  and SPD1-VR is also signiﬁcantly better than SVRG and SAGA. This indicates that our
algorithms SPD1 and SPD1-VR are preferable in practice for high-dimensional problems.

5.2 Results on Real Datasets

In this part  we will demonstrate the efﬁciency of our proposed methods on real datasets. Here we
only focus on the high-dimensional case where d > n or d ≈ n.
We will test all the algorithms on three real datasets: colon-cancer  gisette and rcv1.binary 
downloaded from the LIBSVM website 2. The attributes of these data and λ used for each dataset are
summarized in Table 1.
The experimental results on these real datasets are shown in Figure 2. For colon-cancer dataset 
where d is much larger than n  the performance of SPD1-VR is really dominating over other methods 
and SPD1 also performs better than PSGD. For gisette dataset  where n is slightly larger than d 
SPD1-VR still outperforms all other competitors  but this time SPD1 is slower than PSGD. Besides 
for rcv1.binary  both SPD1 and SPD1-VR are better than PSGD and SVRG/SAGA respectively.
These results on real datasets further conﬁrm that our proposed methods  especially SPD1-VR  are
faster than existing algorithms on high-dimensional problems.

6 Conclusion

In this paper  we developed two stochastic primal-dual algorithms  named SPD1 and SPD1-VR for
solving regularized empirical risk minimization problems. Different from existing methods  our
proposed algorithms have a brand-new updating style  which only need to use one coordinate of one
sampled data in each iteration. As a result  the per-iteration cost is very low and the algorithms are
very suitable for distributed systems. We proved that the overall convergence property of SPD1 and
SPD1-VR resembles PSGD and SVRG respectively under certain condition  and empirically showed
that they are faster than existing methods such as PSGD  SVRG and SAGA in high-dimensional
settings. We believe that our new methods have great potential to be used in large-scale distributed
optimization applications.

2www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets

8

020406080100#Pass through data105104103102101100colon-cancerSAGASVRGPSGDSPD1SPD1-VR020406080100#Pass through data105104103102101100gisetteSAGASVRGPSGDSPD1SPD1-VR01020304050#Pass through data1010108106104102100rcv1.binarySAGASVRGPSGDSPD1SPD1-VR7 Acknowledgement

S. Ma is partly supported by a startup package in Department of Mathematics at UC Davis and the
National Natural Science Foundation of China under Grant 11631013. J. Liu is in part supported by
NSF CCF1718513  IBM faculty award  and NEC fellowship.

References
[1] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[2] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems
with applications to imaging. Journal of mathematical imaging and vision  40(1):120–145 
2011.

[3] Cong Dang and Guanghui Lan. Randomized ﬁrst-order methods for saddle point optimization.

https://arxiv.org/abs/1409.8625  2014.

[4] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in neural
information processing systems  pages 1646–1654  2014.

[5] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[6] Jakub Koneˇcn`y  Zheng Qu  and Peter Richtárik. Semi-stochastic coordinate descent. Optimiza-

tion Methods and Software  32(5):993–1005  2017.

[7] GM Korpelevich. The extragradient method for ﬁnding saddle points and other problems.

Matecon  12:747–756  1976.

[8] Arkadi Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequali-
ties with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization  15(1):229–251  2004.

[9] Arkadi Nemirovski  Anatoli Juditsky  Guanghui Lan  and Alexander Shapiro. Robust stochastic
approximation approach to stochastic programming. SIAM Journal on optimization  19(4):1574–
1609  2009.

[10] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Applied Opti-

mization. Kluwer Academic Publishers  Boston  MA  2004.

[11] Yurii Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems.

SIAM Journal on Optimization  22(2):341–362  2012.

[12] Balamurugan Palaniappan and Francis Bach. Stochastic variance reduction methods for saddle-
point problems. In Advances in Neural Information Processing Systems  pages 1416–1424 
2016.

[13] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized

loss minimization. Journal of Machine Learning Research  14(Feb):567–599  2013.

[14] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent
for regularized loss minimization. In International Conference on Machine Learning  pages
64–72  2014.

[15] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization:
Convergence results and optimal averaging schemes. In International Conference on Machine
Learning  pages 71–79  2013.

[16] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

9

[17] Adams Wei Yu  Qihang Lin  and Tianbao Yang. Doubly stochastic primal-dual coordinate
method for empirical risk minimization and bilinear saddle-point problem. arXiv preprint
arXiv:1508.03390  2015.

[18] Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical

risk minimization. The Journal of Machine Learning Research  18(1):2939–2980  2017.

10

,Conghui Tan
Tong Zhang
Ji Liu