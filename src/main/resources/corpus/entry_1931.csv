2019,Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs,The exploration bonus is an effective approach to manage the exploration-exploitation trade-off in Markov Decision Processes (MDPs).
While it has been analyzed in infinite-horizon discounted and finite-horizon problems  we focus on designing and analysing the exploration bonus in the more challenging infinite-horizon undiscounted setting.
We first introduce SCAL+  a variant of SCAL (Fruit et al. 2018)  that uses a suitable exploration bonus to solve any discrete unknown weakly-communicating MDP for which an upper bound $c$ on the span of the optimal bias function is known. We prove that SCAL+ enjoys the same regret guarantees as SCAL  which relies on the less efficient extended value iteration approach.
Furthermore  we leverage the flexibility provided by the exploration bonus scheme to generalize SCAL+ to smooth MDPs with continuous state space and discrete actions. We show that the resulting algorithm (SCCAL+) achieves the same regret bound as UCCRL (Ortner and Ryabko  2012) while being the first implementable algorithm for this setting.,Exploration Bonus for Regret Minimization in
Discrete and Continuous Average Reward MDPs

Jian Qian  Ronan Fruit
Sequel Team - Inria Lille

jian.qian@ens.fr  ronan.fruit@inria.fr

Matteo Pirotta  Alessandro Lazaric

Facebook AI Research

{pirotta  lazaric}@fb.com

Abstract

The exploration bonus is an effective approach to manage the exploration-
exploitation trade-off in Markov Decision Processes (MDPs). While it has been
analyzed in inﬁnite-horizon discounted and ﬁnite-horizon problems  we focus on
designing and analysing the exploration bonus in the more challenging inﬁnite-
horizon undiscounted setting. We ﬁrst introduce SCAL+  a variant of SCAL [1] 
that uses a suitable exploration bonus to solve any discrete unknown weakly-
communicating MDP for which an upper bound c on the span of the optimal bias
function is known. We prove that SCAL+ enjoys the same regret guarantees as
SCAL  which relies on the less efﬁcient extended value iteration approach. Fur-
thermore  we leverage the ﬂexibility provided by the exploration bonus scheme
to generalize SCAL+ to smooth MDPs with continuous state space and discrete
actions. We show that the resulting algorithm (SCCAL+) achieves the same regret
bound as UCCRL [2] while being the ﬁrst implementable algorithm for this setting.

Introduction

1
While learning in an unknown environment  a reinforcement learning (RL) agent must trade off the
exploration needed to collect information about the dynamics and reward  and the exploitation of
the experience gathered so far to gain reward. An effective strategy to trade off exploration and
exploitation is the optimism in the face of uncertainty (OFU) principle. A popular technique to
ensure optimism is to use an exploration bonus. This approach has been successfully implemented in
H-step ﬁnite-horizon and inﬁnite-horizon γ-discounted settings with provable guarantees in ﬁnite
MDPs. Furthermore  its simple structure (i.e.  it only requires solving an estimated MDP with a
reward increased by the bonus) allowed it to be integrated in deep RL algorithms [e.g.  3  4  5  6].
As the exploration bonus is designed to bound estimation errors on the value function  it requires
knowing the maximum reward rmax and the intrinsic horizon of the problem [e.g.  7  8  9] (e.g. 
H in ﬁnite-horizon and 1/(1 − γ) in discounted problems). Here we consider the challenging
inﬁnite-horizon undiscounted setting [10  Chap. 8]  which generalizes the two previous settings when
H → ∞ and γ → 1. While several algorithms implementing the OFU principle in this setting have
been proposed [11  2  12  1  13]  none of them exploits the idea of an exploration bonus.
In this paper we study the problem of deﬁning and analysing an exploration bonus approach in the
inﬁnite-horizon undiscounted setting. Contrary to the other settings  in average reward there is no
information about the intrinsic horizon. As a consequence  we follow the approach in [14  1] and we
assume that an upper-bound c on the range of the optimal bias (i.e.  value function) is known. We

deﬁne SCAL+ and we show that its regret is bounded by (cid:101)O(cid:0) max{c  rmax}√ΓSAT(cid:1) w.h.p. for any
optimistic reward(cid:98)r(s  a) + b(s  a) dominates the Bellman operator of the true MDP when applied to

MDP with S states  A actions and Γ possible next states. We prove that the bonus used by SCAL+
ensures optimism using a novel technical argument. We no longer use an inclusion argument (i.e. 
the true MDP is contained in a set of plausible MDPs) but we reason directly at the level of the
Bellman operator. We show that the optimistic Bellman operator deﬁned by the empirical MDP with

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the optimal bias function. This is sufﬁcient to prove that the solution of the optimistic MDP is indeed
(gain-)optimistic. This proof technique has two main advantages w.r.t. the inclusion argument. First 
it directly applies to slightly perturbed empirical MDPs  without re-deriving conﬁdence sets. Second 
as we study the optimistic Bellman operator applied only to the optimal bias function (rather than
all possible vector in RS)  we save a factor √Γ in designing the exploration bonus  compared to the
(implicit) bounds obtained by algorithms relying on conﬁdence sets on the MDP. Furthermore  as
SCAL+ only solves the estimated MDP with optimistic reward  it is computationally cheaper than
UCRL-based algorithms  which require computing the optimal policy for an extended MDP with a
continuous action space deﬁned by the conﬁdence set over MDPs.
Surprisingly  the “tighter” optimism of SCAL+ does not translate into a better regret  which actually
matches the one of SCAL and still depends on the factor √Γ. We isolate and discuss where the
term √Γ appears in the proof sketch of Sect. 3.3. While Azar et al. [8]  Jin et al. [9] managed to
remove the √Γ term in the ﬁnite-horizon setting  their proof techniques cannot be directly applied

to the inﬁnite-horizon case. Recently Ortner [15] derived an algorithm achieving O(cid:0)√tmixSAT(cid:1)

regret bound under the assumption that the true MDP is ergodic (tmix denotes the maximum mixing
time of any policy). It remains an open question if a regret scaling with √S (instead of √ΓS) can
be achieved in the inﬁnite-horizon case without any ergodicity assumption. We report preliminary
experiments showing that the exploration bonus may indeed limit over-exploration and lead to better
empirical performance w.r.t. approaches based on conﬁdence intervals on the MDP itself (i.e.  UCRL
and SCAL). A more detailed comparison to existing literature is postponed to App. A.
To further illustrate the generality of the exploration bonus approach  we also present SCCAL+  an
extension of SCAL+ to continuous state MDPs. As in [2  16]  we require the reward and transition
functions to be Hölder continuous with parameters ρL and α. SCCAL+ is also the ﬁrst implementable
algorithm in continuous average reward problems with theoretical guarantees (existing algorithms with
theoretical guarantees such as UCCRL [2] cannot be implemented). The key result is a regret bound

of (cid:101)O(cid:0) max{c  rmax}ρL√AT (α + 2)/(2α + 2)(cid:1) w.h.p. Finally  we provide an empirical comparison of

SCCAL+ with a Q-learning algorithm with exploration bonus for average reward problems (RVIQ-
UCB) inspired by [17  9] and the results in this paper (to deal with continuous states).

2 Preliminaries
We consider a weakly-communicating MDP [10  Sec. 8.3] M = (S A  p  r) with state space S
and action space A. Every state-action pair (s  a) is characterized by a reward distribution with
mean r(s  a) and support in [0  rmax]  and a transition distribution p(·|s  a) over next states. In this
section  we assume the ﬁnite case (i.e.  |S| |A| < +∞)  although all following deﬁnitions extend to
continuous state spaces under mild assumptions on r and p (see Sect. 4). We denote by S = |S| and
A = |A| the number of states and action  by Γ(s  a) = (cid:107)p(·|s  a)(cid:107)0 the number of states reachable
by selecting action a in state s  and by Γ = maxs a Γ(s  a) its maximum. A stationary Markov
randomized policy π : S → P (A) maps states to distributions over actions. The set of stationary
randomized (resp. deterministic) policies is denoted by ΠSR (resp. ΠSD). Any policy π ∈ ΠSR has an
associated long-term average reward (or gain) and a bias function deﬁned as

(cid:20) T(cid:88)

t=1

(cid:0)r(st  at) − gπ(st)(cid:1)(cid:21)

 

gπ(s) := lim

T→+∞

Eπ
s

r(st  at)

and hπ(s) := C- lim
T→+∞

Eπ
s

(cid:20) 1

T(cid:88)

T

t=1

(cid:21)

where Eπ
s denotes the expectation over trajectories generated starting from s1 = s with at ∼ π(st).
The bias hπ(s) measures the expected total difference between the reward and the stationary reward
in Cesaro-limit (denoted by C- lim). Accordingly  the difference of bias hπ(s) − hπ(s(cid:48)) quantiﬁes
the (dis-)advantage of starting in state s rather than s(cid:48). We denote by sp (hπ) := maxs hπ(s) −
mins hπ(s) the span of the bias function. In weakly communicating MDPs  any optimal policy
π∗
∈ arg maxπ gπ(s) has constant gain  i.e.  gπ∗ (s) = g∗ for all s ∈ S. Moreover  there exists a
policy π∗

∈ arg maxπ gπ(s) for which (g∗  h∗) = (gπ∗   hπ∗ ) satisfy the optimality equation 

(1)
where L is the optimal Bellman operator. Finally  D = maxs(cid:54)=s(cid:48){τ (s → s(cid:48))} denotes the diameter
of M  where τ (s → s(cid:48)) is the minimal expected number of steps needed to reach s(cid:48) from s.

∗
a∈A{r(s  a) + p(·|s  a)Th

∗

∗
= Lh

∀s ∈ S 

∗
h

(s) + g

(s) := max

} 

2

Input: Conﬁdence δ ∈]0  1[  rmax  S (I for SCCAL+)  A  c ≥ 0 (and ρL and α for SCCAL+)
For episodes k = 1  2  ... do
1. Set tk = t and episode counters νk(s  a) = 0.

k (I(s)  a)  bk(I(s)  a) (Eq. 3 or 6) and build the MDP (cid:99)M +

k

2. Compute estimates (cid:98)p+
(SCAL+) or(cid:99)M ag+

k (I(s(cid:48))|I(s)  a) (cid:98)r+
-approximate solution of Eq. 4 for(cid:99)M +

(SCCAL+).

k
3. Compute an rmax
tk
4. Sample action at ∼ πk(·|I(st)).
5. While νk(I(st)  at) < max{1  Nk(I(st)  at)} do

k (SCAL+) or(cid:99)M ag+

k

(SCCAL+) using SCOPT

(a) Execute at  obtain reward rt  and observe next state st+1.
(b) Increment counter νk(st  at) += 1.
(c) Sample action at+1 ∼ πk(·|I(st+1)) and increment t += 1.

6. Increment counters Nk+1(s  a) := Nk(s  a) + νk(s  a) for all s  a.

Figure 1: Shared pseudo-code for SCAL+ and SCCAL+. For SCAL+ I(s) = s by deﬁnition.

t=1(g∗

− rt(st  at)). Finally  we make the following assumption.

Learning Problem. Let M∗ be the true MDP. We consider the learning problem where S  A and
∆(A  T ) =(cid:80)T
rmax are known  while rewards r and dynamics p are unknown and need to be estimated on-line.
We evaluate the performance of a learning algorithm A after T time steps by its cumulative regret
Assumption 1. There exists a known upper-bound c > 0 to the optimal bias span i.e.  c ≥ sp (h∗).
This assumption is common in the literature [see e.g.  18  2  1]. Such a bound to the “range” of the
value function is already available in discounted and ﬁnite horizon problems (i.e.  as
1−γ and H) 
so Asm. 1 is not more restrictive. While the span sp (h∗) is a non-trivial function of the dynamics
and the rewards of the MDP  some intuition about how the cumulative reward varies depending on
different starting states is often available. Furthermore  as sp (h∗) ≤ rmaxD [e.g.  14]  it is sufﬁcient
to have prior knowledge about the diameter D and the range of the reward rmax  to provide a rough
upper-bound on the span.
3
In this section  we introduce SCAL+  the ﬁrst online RL algorithm –in the inﬁnite horizon undiscounted
setting– that leverages an exploration bonus to achieve near-optimal regret guarantees. Similar to
SCAL [1]  SCAL+ takes as input an upper-bound c on the optimal bias span (i.e.  sp (h∗) ≤ c) to
constrain the planning problem solved over time. The crucial difference is that SCAL+ does not
compute an optimistic MDP within a high-probability conﬁdence set  but it directly computes the
optimal policy of the estimated MDP  with the reward increased by an exploration bonus. The bonus
is carefully tuned so as to guarantee optimism and small regret at the same time (Thm. 1).

SCAL+: SCAL with exploration bonus

1

3.1 The Algorithm
Similar to other OFU-based algorithms  SCAL+ proceeds in episodes (see Fig. 1)1. Denote by tk the
starting time of episode k  Nk(s  a  s(cid:48)) the number of observations of tuple (s  a  s(cid:48)) before episode

s(cid:48) Nk(s  a  s(cid:48)). We deﬁne the estimators of the transitions and rewards as

rt(st  at)1(cid:0)(st  at) = (s  a)(cid:1)

Nk(s  a)

(2)

(cid:48)
k (s

|s  a) =

1(s(cid:48) = s)
Nk(s  a) + 1

Nk(s  a  s(cid:48))
Nk(s  a) + 1

k and Nk(s  a) :=(cid:80)
(cid:98)p+
where s ∈ S is an arbitrary state and rk(s  a) := rmax (cid:98)p+
transition model(cid:98)p+
k (s(cid:48)
ln(cid:0)20SAN +
(cid:123)(cid:122)

further deﬁne the exploration bonus

bk(s  a) := (c + rmax)

(cid:115)
(cid:124)

N +

+

 

k (s  a)

rk(s  a) =

tk−1(cid:88)
k (s(cid:48)

t=1

k (s  a)/δ(cid:1)
(cid:125)

:=βsa
k

|s  a) is a biased (but asymptotically consistent) estimator of p(s(cid:48)

|s  a) = 1/S when Nk(s  a) = 0. The
|s  a). We

+

c

Nk(s  a) + 1

 

(3)

1The algorithm is reported in its general form  which applies to both discrete and continuous state space.

3

πk := arg

∗

g

(4)

(5)

sup

π∈Πc((cid:99)M +

∀s ∈
otherwise

is executed until the number of visits in at least one state-action pair during the episode has doubled.

k (s  ai) :=(cid:0)rk(s  a) + bk(s  a)(cid:1)
π∈Πc((cid:99)M +

k ) := sup

k ){gπ} 
k   given v ∈ RS and c ≥ 0  we deﬁne the value operator (cid:98)T +
(cid:40)(cid:98)L+v(s)
c + mins{(cid:98)L+v(s)}
c as vn+1 = (cid:98)T +

where N +
k (s  a) = max{1  Nk(s  a)}. Intuitively  the exploration bonus is large for poorly visited
state-action pairs  while it decreases with the number of visits. A crucial aspect in the formulation
of bk is that it scales with the span c. In fact  the exploration bonus is not used to obtain an upper-
k would be sufﬁcient)  but it is designed to
conﬁdence bound on the reward (setting bk(s  a) = βsa
take into consideration how estimation errors on p and r  which are bounded by βsa
k   may propagate
to the bias and gain through repeated applications of the Bellman operator. As the span c provides
prior knowledge about the “range” of the optimal bias function  the exploration bonus is obtained by
considering that “local” estimation errors may be ampliﬁed up to a factor c. The speciﬁc shape of bk
MDP (cid:99)M +
k = (S A+ (cid:98)p+
k  (cid:98)r+
and βsa
k and their theoretical properties are derived in Lem. 1. At each episode k  SCAL+ builds an
k ) obtained by duplicating every action in A with transition probabilities
(a  i) ∈ A × {1  2} by ai. We then deﬁne(cid:98)r+
unchanged and optimistic reward set to 0. Formally  let A+ := A × {1  2} and we denote any pair
proceeds by computing the optimal policy of the MDP(cid:99)M +
· 1(i = 1). SCAL+
c ((cid:99)M +
k subject to the constrains on the bias span:
where the constraint set is Πc(M ) :=(cid:8)π ∈ ΠSR : sp (hπ) ≤ c ∧ sp (gπ) = 0(cid:9). The optimal policy
k ){gπ};
Problem 4 is well posed and can be solved using SCOPT.Let(cid:98)L+ be the optimal Bellman operator
associated to(cid:99)M +
(cid:110)
(cid:111)
s ∈ S|(cid:98)L+v(s) ≤ mins{(cid:98)L+v(s)} + c
: RS → RS as
(cid:98)T +
c v = Γc(cid:98)L+v =
c applies a span truncation to the one-step application of (cid:98)L+  which guarantees that
operator (cid:98)T +
where Γc is the span constrain projection operator (see [1  App. D] for details). In other words 
sp((cid:98)T +
where(cid:98)L+ is replaced by (cid:98)T +
c v) ≤ c. Given a vector v0 ∈ RS and a reference state s  SCOPT runs relative value iteration
c vn(s)e. The policy πk returned by SCOPT takes
action in the augmented set A+ and it can be “projected” on A as πk(s  a) ← πk(s  a1) + πk(s  a2)
episode. Following similar steps as in [1]  we can prove that(cid:99)M +
(we use the same notation for the two policies)  which is the policy actually executed through the
k satisﬁes all sufﬁcient conditions for
Proposition 1. The MDP (cid:99)M +
operator(cid:98)L+ is a γ-span-contraction; 2) all policies are unichain; 3) the operator (cid:98)T +
k satisﬁes the following properties: 1) the associated optimal Bellman
c is globally fea-
sible at any vector v ∈ RS such that sp (v) ≤ c  i.e.  for all s ∈ S  mina∈A{r(s  a)+p(·|s  a)Tv} ≤
mins(cid:48){Lv(s(cid:48))} + c. As a consequence  SCOPT converges and returns a policy πk solving (4).
3.2 Optimistic Exploration Bonus
to compute πk ((cid:99)M +
(cid:0)(cid:99)M +
All regret proofs for OFU-based algorithms rely on the property that the optimal gain of the MDP used
for SCAL+  we need to ensure that the policy πk is gain-optimistic  i.e. (cid:98)g+
k in our case) is an upper-bound on g∗. If we want to use the same proof technique
Recall that the optimal gain and bias of the true MDP (g∗  h∗) satisfy the optimality equation
Lh∗ = h∗ + g∗e where e = (1  . . .   1). Since sp (h∗) ≤ c (by assumption)  we also have sp (Lh∗) =
shows that a sufﬁcient condition to prove optimistic gain is to show that the operator (cid:98)T +
sp (h∗ + g∗e) = sp (h∗) ≤ c and so Tch∗ = Lh∗. A minor variation to Lemma 8 of Fruit et al. [1]
c is optimistic
w.r.t. its exact version when applied to the optimal bias function  i.e.  (see Prop. 3 in App. B)
As the truncation operated by Tc (i.e.  Γc) is monotone  this inequality is implied by(cid:98)L+
Finally  since(cid:98)p+
k (s  a2) ≤(cid:98)r+
k h∗
see that(cid:98)L+
k ≥ g∗ is to have(cid:98)Lkh∗
k h∗ =(cid:98)Lkh∗  thus implying that a sufﬁcient condition for(cid:98)g+
k (s(cid:48)
which reduces to verifying optimism for the Bellman operator of (cid:99)M +

≥ Lh∗.
k (s  a1) it is immediate to
≥ Lh∗ 
k when applied to the exact
optimal bias function. The exploration bonus is tailored to achieve this condition with high probability.

∗
c h

∗
≥ h

(cid:98)T +
|s  a2) =(cid:98)pk(s(cid:48)

SCOPT to converge and return the optimal policy (see App. B).

|s  a) and(cid:98)r+

|s  a1) =(cid:98)p+

c vn − (cid:98)T +

∗

∗
e = Tch

.

+ g

k := g∗

c

k

≥ g∗.

k (s(cid:48)

4

c

(cid:1)

≥ Lh∗ (componentwise) and as a consequence(cid:98)g+

Lemma 1. Denote by(cid:98)Lk the optimal Bellman operator of (cid:99)Mk. With probability at least 1 − δ
all k ≥ 1 (cid:98)Lkh∗
the MLE of p). We also need to take into account the small bias introduced by(cid:98)pk(·|s  a) compared to
all k ≥ 1  rk(s  a) + bk(s  a) +(cid:98)pk(·|s  a)

Proof (see App. D). By using Hoeffding-Azuma inequality and union bound  we can show that for
k w.h.p. (pk is
all k ≥ 1  |rk(s  a) − r(s  a)| ≤ rmaxβsa
k and |(pk(·|s  a) − p(·|s  a))
pk(·|s  a) which is not bigger than c/(Nk(s  a) + 1) by deﬁnition. Then  with high probability  for
(cid:124)

h∗ for all (s  a) ∈ S × A.

(cid:124)
≥ r(s  a) + p(·|s  a)

k ≥ g∗.
h∗

(cid:124)

| ≤ c βsa

5   for

h∗

The argument used to prove optimism (Lem. 1)) signiﬁcantly differs from the one used for UCRL and
SCAL. Conﬁdence-based methods compute the optimal policy of an extended MDP that “contains”
the true MDP M∗ (w.h.p.)  which directly implies that the gain of the extended MDP is bigger than
to over-exploration). In fact  the exploration bonus quantiﬁes by how much (cid:98)L+
g∗. The main advantage of our argument is that it allows for a “tighter” optimism (i.e.  less prone
(cid:112)
Lh∗ and it approximately scales as bk(s  a) = (cid:101)Θ(cid:0) max{rmax  c}/
Nk(s  a)(cid:1). In contrast  UCRL
k h∗ is bigger than
and SCAL use an optimistic Bellman operator(cid:101)L such that(cid:101)Lh∗ is bigger than Lh∗ by respectively
(cid:112)
(cid:112)
Γ/Nk(s  a)(cid:1) (UCRL) and(cid:101)Θ(cid:0) max{rmax  c}
Γ/Nk(s  a)(cid:1) (SCAL). In other words  the
(cid:101)Θ(cid:0)rmaxD
optimism in SCAL+ is tighter by a multiplicative factor √Γ.
3.3 Regret Analysis of SCAL+
We report the main result of this section.
Theorem 1. For any weakly communicating MDP M such that sp (h∗) ≤ c  with probability at
least 1 − δ it holds that for any T ≥ 1  the regret of SCAL+ is bounded as

(cid:17)

T ln(cid:0)T /δ(cid:1) + S2A ln2(cid:16) T

(cid:17)(cid:19)(cid:19)

(cid:18)(cid:115)(cid:16)(cid:88)

(cid:18)

∆(SCAL+  T ) = O

max{rmax  c}

Γ(s  a)

s a

δ

Since the optimism in SCAL+ is tighter than in UCRL and SCAL by a factor √Γ  one may expect to get
a regret bound scaling as c√SAT instead of c√ΓSAT   thus matching the lower bound of Jaksch et al.
[11] as for the dependency in S. Unfortunately  such a bound seems difﬁcult to achieve with SCAL+
(and even SCAL) due to the correlation between hk and pk (see App. D). Azar et al. [8] managed to
achieve the optimal dependence in S in ﬁnite-horizon problems. In this setting  the deﬁnition of regret
is different and it is not clear whether it is possible to adapt their guarantees and techniques to inﬁnite
sampling has a regret of (cid:101)O(D√SAT ) in the inﬁnite horizon undiscounted setting. Unfortunately 
horizon without introducing a Θ(T )-term. Agrawal and Jia [19] showed the optimistic posterior
their proof critically relies on the concentration inequality |(pk(·|s  a) − p(·|s  a))Thk| (cid:46) rmaxDβsa
which is incorrect.2 It remains as an open question whether the √Γ term can be actually removed.
Finally  SCAL+’s regret does not scale min{rmaxD  c} as for SCAL  implying that SCAL+ may
perform worse when c is too large. The difference resides in the fact SCAL builds an extended MDP
that contains the true MDP (w.h.p.). The shortest path between two states in the extended MDP is
therefore shorter than in the true MDP and consequently  the diameter of the extended MDP is smaller
than the true diameter D. This explains why the regret of SCAL depends on both D and c (which is
provided as input to the algorithm). Unfortunately  in SCAL+ it is not clear how to bound the diameter

k

k and the only information that can be exploited to bound the regret is the constraint c.
SCCAL+: SCAL+ for continuous state space

4
We now consider an MDP with continuous state space S = [0  1] and discrete action space A. In
general  it is impossible to learn an arbitrary real-valued function with only a ﬁnite number of samples.
We therefore introduce the same smoothness assumption as Ortner and Ryabko [2]:
Assumption 2 (Hölder continuity). There exist ρL  α > 0 s.t. for any two states s  s(cid:48)
∈ S and any
action a ∈ A  |r(s  a)− r(s(cid:48)  a)| ≤ rmaxρL|s− s(cid:48)
|α and (cid:107)p(·|s  a)− p(·|s(cid:48)  a)(cid:107)1 ≤ ρL|s− s(cid:48)
|α.
As in Sec. 3  we start by introducing our proposed algorithm SCCAL+ which is a variant of SCAL+
for continuous state space (Sec. 4.1)  and then analyze its regret (Sec. 4.2).

of(cid:99)M +

2See https://arxiv.org/abs/1705.07041.

5

S

1

t=1

s(cid:48)∈J

S   k

S

Nk(I  a)

rag
k (I  a) :=

s∈I Nk(s  a) 

(cid:3) and Ik =(cid:3) k−1

(cid:3) for k = 2  . . .   S. The set of

4.1 The algorithm
In order to apply SCAL+ to a continuous problem  we discretize the state space as in [2]. We

aggregated states is then I := {I1  . . .   IS} (|I| = S). The number of intervals S is a parameter of
the algorithm and plays a central role in its performance. Note that the terms Nk(s  a  s(cid:48)) and Nk(s  a)
deﬁned in Sec. 3 are still well-deﬁned for s and s(cid:48) lying in [0  1] but are 0 except for a ﬁnite number
s∈I us is also well-deﬁned as long as the collection
(us)s∈I contains only a ﬁnite number of non-zero elements. We can therefore deﬁne the aggregated

partition S into S intervals deﬁned as I1 :=(cid:2)0  1
of s and s(cid:48). For any subset I ⊆ S  the sum(cid:80)
counts  rewards and transition probabilities for all I  J ∈ I as: Nk(I  a) :=(cid:80)
(cid:80)
(cid:80)
(cid:80)
s∈I Nk(s  a  s(cid:48))
s∈I Nk(s  a)

k (J|I  a) :=
Similar to Eq. 3  we deﬁne the exploration bonus of an aggregated state as

rt(st  at)1(st ∈ I  at = a)  pag

tk−1(cid:88)
bk(I  a) :=(c + rmax)(cid:0)βIa

k = (I A (cid:98)pag
k  (cid:98)rag

(6)
k is deﬁned in (3). The main difference is an additional O(cρLS−α) term that accounts for
where βIa
the fact that the states that we aggregate are not completely identical but have parameters that differ by
k ) 

at most ρLS−α. We pick an arbitrary reference aggregated state I and deﬁne(cid:99)M ag
the aggregated (discrete) analogue of(cid:99)Mk deﬁned in Sec. 3  where(cid:98)rag
Similarly we “augment” (cid:99)M ag
into (cid:99)M ag+
 (cid:98)rag+
= (I A+ (cid:98)pag+
by duplicating each transition in (cid:99)M ag
parameters as in Sec. 3) to solve optimization problem (4) on(cid:99)M ag+
the state space of M∗ is uncountable (cid:99)M ag+

k in Sec. 3)
k . At each episode k  SCCAL+ uses SCOPT (with the same
. This is possible because although
has only S < +∞ states. SCOPT returns an optimistic
optimal policy πk satisfying the span constraint. This policy is deﬁned in the discrete aggregated
state space but can easily be extended to the continuous case by setting πk(s  a) := πk(I(s)  a) for
any (s  a) (with I(s) mapping a state to the interval containing it).

) (analogue of (cid:99)M +

−α(cid:1) +

k (J|I  a) :=

k (J|I  a)

Nk(I  a)pag

k = rag

k + bk and

Nk(I  a) + 1

Nk(I  a) + 1

Nk(I  a) + 1

(cid:98)pag

k + ρLS

1(J = I)

+

c

k

k

k

k

.

 

k

k

4.2 Regret Analysis of SCCAL+
This section is devoted to the regret analysis of SCCAL+  with the main result summarized in Thm.2.
Theorem 2. For any MDP M satisfying Asm. 2 and such that sp (h∗
(cid:19)(cid:19)
M ) ≤ c  with probability at least
1 − δ it holds that for any T ≥ 1  the regret of SCCAL+ is bounded as
(cid:17)

AT ln(cid:0)T /δ(cid:1) + S2A ln2(cid:0)T /δ(cid:1) + ρLS

(cid:18)
(cid:17)1/(α + 1)

∆(SCCAL+  T ) = O

max{rmax  c}

(cid:113)

−αT

(cid:18)

(cid:16)

(cid:16)

S

α

1

the bound becomes: (cid:101)O

By setting S =

(cid:113) T

A

(2α+2) T

(α+2)
(2α+2)

αρL

.

(α+1)
L

A

max{rmax  c}ρ

Thm. 2 shows that SCCAL+ achieves the same regret as UCCRL [2] while being the only imple-
mentable algorithm with such theoretical guarantees for this setting. Thm. 2 can be extended to
are needed for the discretization leading to a regret bound of order (cid:101)O(T (2d + α)/(2d + 2α)) after tuning
the more general case where S is d-dimensional. As pointed out by [2]  in this case Sd intervals
S = T 1/(2d + 2α). Finally  we believe that SCCAL+ can be extended to the setting considered by [16]
asymptotic regret (as κ → ∞) of (cid:101)O(T 2/3) while SCCAL+ is achieving (cid:101)O(T 3/4).
where  in addition to Hölder conditions  the transition function is assumed to be κ-times smoothly
differentiable. In the case of Lipschitz model  i.e.  α = 1  this means that it is possible obtain an
Proof sketch. Thm. 2 can be seen as a generalization of Thm. 1 but the continuous nature of the state
with different state spaces: (cid:99)M ag
space makes the analysis more difﬁcult. The main technical challenge lies in relating two MDPs
k (with ﬁnite state space) and M∗ (with continuous state space). For
we introduce an “intermediate” MDP(cid:99)Mk which has continuous state space like M∗  but which also
instance  It is necessary to compare these two MDPs to prove optimism. To facilitate the comparison 
depends on the samples collected before episode k like(cid:99)M ag

k .

6

S(cid:82)

+

s(cid:48)≤s

J

Nk(I(s)  a) + 1

.

c

:= g∗

c

k

k

(cid:48)

|s  a) :=

k (J|I(s)  a)

+

J
Nk(I(s)  a) + 1

k (I(s)  a) 
Nk(I(s)  a)pk(s(cid:48)
Nk(I(s)  a) + 1

c vn and un+1 := (cid:98)T ag+

c

n  vn is piece-wise constant and its discrete analogue is un i.e.  un = v(cid:48)

∈ J ∈ I are piece-wise
k . More precisely  ∀J ∈ I 
=(cid:98)pag

k (J|I(s)  a)

(7)

∈ I(s))
Nk(I(s)  a) + 1
|s  a) is the

|s  a)
(cid:80)
x∈I(s) Nk(x a s(cid:48))
Nk(I(s) a)

k and(cid:99)Mk) although they have different state spaces and obtain:
(cid:0)(cid:99)M ag+

(cid:1) =(cid:98)g+

k := g∗

k (J|I(s)  a) and so ∀(s  J) ∈ S × I:
1(s(cid:48)
Nk(I(s)  a)pag
=
k and (cid:99)Mk can be easily compared (and as a consequence  so can (cid:99)M ag+

Deﬁnition 1 (Empirical MDP with continuous state space). Let(cid:99)Mk = (S A (cid:98)pk (cid:98)rk) be the continu-
ous state space MDP s.t. for all (s  a) ∈ S × A  rk(s  a) := rag
(cid:98)rk(s  a) := rk(s  a) + bk(I(s)  a) and (cid:98)pk(s
S · 1(s(cid:48)
Radon-Nikodym derivative of the cumulative density function F (s) =(cid:80)
where I : S → I is the function mapping a state s to the interval containing s  and pk(s(cid:48)
MDP(cid:99)Mk is designed so that: 1) the reward function is piece-wise constant over any interval in I and
matches the reward function of (cid:99)M ag
constant and match the transitions of the discrete state space MDP (cid:99)M ag
k   2) the transitions integrated over s(cid:48)
(cid:82)
(cid:90)
J pk(s(cid:48)
|s  a)ds(cid:48) = pag
(cid:98)pk(s
∈ I(s))ds(cid:48)
(cid:48)
(cid:48)
|s  a)ds
This ensures that (cid:99)M ag
(cid:99)M +
k   the augmented versions of(cid:99)M ag
(cid:1)
(cid:0)(cid:99)M +
Lemma 2. For any k ≥ 1 (cid:98)gag+
Proof (see App. C.2). We notice that for any continuous function v(s) deﬁned on S and piece-wise
constant on the intervals of I  we can associate a discrete function v(cid:48)(I) (deﬁned on I) such that
analogue. We deﬁne the sequences (vn)n∈N and (un)n∈N by recursively applying (cid:98)T +
c and (cid:98)T ag+
for all s ∈ S  v(cid:48)(I(s)) = v(s). Let v0 = 0 (continuous function) and denote by v(cid:48)
0 its discrete
respectively: vn+1 := (cid:98)T +
vn+1(s) − vn(s) and un+1(I(s)) − un(I(s)) have the same limits  respectively(cid:98)g+
k and(cid:98)gag+
Leveraging Lem. 2  it is sufﬁcient to compare the gains of (cid:99)M +
(cid:98)Lkh∗
Lemma 3. Denote by(cid:98)Lk the optimal Bellman operator of (cid:99)Mk. With probability at least 1 − δ
≥ Lh∗ (analogue of Lem. 1)  with the difference that h∗ is deﬁned on a continuous space.
all k ≥ 1 we have(cid:98)Lkh∗
k ≥ g∗.
Proof (see Lem. 4 and 5) in App. C). The proof is similar to Lem. 1: we compare(cid:98)rk and(cid:98)pk with the
that(cid:98)pk is even more biased than before. Thanks to the smoothness assumption (Asm. 2)  the extra

true reward function r and transition probabilities p using concentration inequalities. Due to the
aggregation of states  there are two major differences with the discrete case. The ﬁrst difference is
bias is only of order O(LS−α) (this explains why this term appears in the deﬁnition of the bonus
in (6)). The second difference is that since there are uncountably many states  it is impossible to use a
union bound argument on the set of states (like in Lem. 1). Instead  we show using optional skipping
that the terms of interest are martingales and we apply Azuma’s and Freedman’s inequalities.
The rest of the proof is similar to SCAL+ with additional steps to deal with the continuous state space.
5 Numerical Simulations
We design experiments to investigate the learning performance in discrete and continuous MDP
(see App. E for details). In the discrete case  the main theoretical open question is whether the
tighter exploration bonus does translate in a better regret  that is  whether the dependency on the
branching factor Γ in the regret bound is due to the analysis or not. Unfortunately  it is difﬁcult
to design experiments to thoroughly investigate the actual dependency. First  it is challenging to
design MDPs with all parameters ﬁxed (i.e.  gain  span  diameters  number of states and actions)
but Γ (e.g.  the bigger Γ  the smaller the span as the MDP is more connected). Furthermore  the
regret bound is worst-case w.r.t. all MDPs with a given set of parameters  which is difﬁcult to
design in practice. For these reasons  instead of investigating the exact dependency  we rather
focus on comparing the performance of SCAL+ to UCRL for different values of Γ. We consider

k and M∗ to prove optimism. Since
both MDPs have the same (continuous) state space  we can proceed as in Sec. 3.2 and just show that

≥ Lh∗ (on the whole state space) and as a consequence(cid:98)gag+

0. It is easy to show that for all
n. Therefore the sequences

un with u0 := v(cid:48)

c

k

and

k

.

k

5   for

7

Figure 2: Cumulative regret (Garnet MDPs) and cumulative reward (continuous state MDPs). We
report mean  max and min curves obtained over 50 independent runs.

(cid:112)
Lk/Nk(s  a) and βp(s  a) =
k(s  a) + cβp

the Garnet(S  A  Γ) family [20] of random MDPs.
In all the experiments we take S = 200 
A = 3 and c = 2 and we guarantee the MDPs to be communicating by setting p(s0|s  a) ≥ 0.01
(cid:112)
for every pair (s  a) and an arbitrary state s0. In order to provide a fair comparison of UCRL 
SCAL and SCAL+  we consider Hoeffdin-based conﬁdence bounds with standardized constants:
ΓLk/Nk(s  a) with Lk = log(SA/δk)/4 for
βr
k(s  a) = rmax
UCRL and SCAL  and bk(s  a) = βr
k(s  a) for SCAL+. Since Garnet(S  A  Γ) deﬁnes
a distribution over MDPs  we evaluate the algorithms on the MDP with median bias span (since
the distribution shows relatively long tails  see App. E). According to the theoretical analysis  the
per-episode regret of UCRL scales as O(sp (hk)√Γ)  where sp (hk) is the span of the optimistic
MDP  while SCAL+ has regret O(c√Γ)  where c is an upper-bound on sp (h∗). While in the worst
case sp (hk) ≤ D  in the MDP we selected  UCRL always generates optimistic MDPs with span
sp (hk) smaller than sp (h∗) ≤ c. In this favorable case for UCRL  the only hope for SCAL+ to
achieve better performance is if the tighter optimism translates into a per-episode regret of O(c)  with
no dependency on Γ. This is indeed what we observed empirically. When Γ = 5  as expected  UCRL
outperforms SCAL+ as sp (hk)√Γ ≤ c for most of the episodes. On the other hand  when Γ = 144 
as sp (hk)√Γ ≥ c. Although this result does not provide a deﬁnite answer on whether and how the

the tighter optimism of SCAL+ allows a faster convergence to the optimal solution compared to UCRL

regret of SCAL+ scales with Γ  it hints to the fact that tighter optimism does indeed translate to better
empirical performance w.r.t. conﬁdence-based algorithms such as UCRL.
As SCCAL+ is the ﬁrst implementable model-based algorithm with regret guarantees in continuous
MDPs  we compare it to model-free heuristic variants. We consider RVI Q-learning [17] with either
-greedy and UCB [9] exploration.3 Since Q-learning is model-free  it does not perform planning and
updates the policy at each time step (the action selection is greedy w.r.t. the current estimate). Even
in this case we harmonize the bonus such that b(s  a) = βr(s  a) + cβp(s  a) + (rmax + c)ρLS−α.
We use the same uniform discretization of the state space for all the algorithms. We considering a
continuous version of the RiverSwim [7] discretized into S = 50 states (ρL = α = 1  c = 30  S ⊆ R)
and the ShipSteering domain [21] with S = |I| = 512 discrete states (ρL = 5  α = 1  c = 1.5 
S ⊆ R3) (see the App. E for MountainCar [22]). In both cases  RVIQ shows an unstable behaviour.
In the RiverSwim it outperforms the other approaches when optimistically initialized (i.e.  q0 = c)
while the same conﬁguration fails to learn in the ShipSteering. Moreover  RVIQ with q0 = 0 shows
the ability to learn in the ShipSteering but also high variance. This undesired behavior is typical
of unstable algorithms (we observed linear regret in some run). RVIQ-UCB is able to learn in the
RiverSwim but not in the ShipSteering. The only stable algorithm in both domains is SCCAL+.
6 Conclusion
We derive the ﬁrst regret analysis of exploration bonus for average reward with discrete and continuous
state space by leveraging on an upper-bound to the range of the optimal bias function to properly scale
the bonus (as done in other settings). It is an open question whether an exploration bonus approach
is still possible when no prior knowledge on the span of the optimal bias function is available [see
e.g.  11  23]. Despite the √Γ improvement in the deﬁnition of the exploration bonus (i.e.  optimism)
compared to conﬁdence-set-based algorithms  the ﬁnal regret still scales with Γ leaving it as an open
question whether such dependency can be actually removed in non-ergodic MDPs.

3Refer to App. E.1 for details about RVIQ and RVIQ-UCB. There is no known regret bound for model-free

algorithms in average reward  we think this is an interesting line of research for future work.

8

00.511.52·10700.20.40.60.81·105TimeCumulativeRegretGarnet(200 3 5)SCALSCAL+UCRL00.511.52·107012·105TimeGarnet(200 3 144)0246810·1070123·106TimeCumulativeRewardShipSteeringRVIQq0=0RVIQq0=cRVIQ-UCBSCCAL+02468·10600.511.5·106TimeContinuousRiverSwimReferences
[1] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Ronald Ortner. Efﬁcient bias-span-
In ICML  Proceedings of

constrained exploration-exploitation in reinforcement learning.
Machine Learning Research. PMLR  2018.

[2] Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforce-

ment learning. In NIPS  pages 1772–1780  2012.

[3] Marc G. Bellemare  Sriram Srinivasan  Georg Ostrovski  Tom Schaul  David Saxton  and Rémi
Munos. Unifying count-based exploration and intrinsic motivation. In NIPS  pages 1471–1479 
2016.

[4] Haoran Tang  Rein Houthooft  Davis Foote  Adam Stooke  Xi Chen  Yan Duan  John Schulman 
Filip De Turck  and Pieter Abbeel. #exploration: A study of count-based exploration for deep
reinforcement learning. In NIPS  pages 2750–2759  2017.

[5] Georg Ostrovski  Marc G. Bellemare  Aäron van den Oord  and Rémi Munos. Count-based
In ICML  volume 70 of Proceedings of Machine

exploration with neural density models.
Learning Research  pages 2721–2730. PMLR  2017.

[6] Jarryd Martin  Suraj Narayanan Sasikumar  Tom Everitt  and Marcus Hutter. Count-based

exploration in feature space for reinforcement learning. CoRR  abs/1706.08090  2017.

[7] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for
markov decision processes. Journal of Computer and System Sciences  74(8):1309–1331  2008.

[8] Mohammad Gheshlaghi Azar  Ian Osband  and Rémi Munos. Minimax regret bounds for
reinforcement learning. In ICML  volume 70 of Proceedings of Machine Learning Research 
pages 263–272. PMLR  2017.

[9] Chi Jin  Zeyuan Allen-Zhu  Sébastien Bubeck  and Michael I. Jordan. Is q-learning provably

efﬁcient? CoRR  abs/1807.03765  2018.

[10] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming.

John Wiley & Sons  Inc.  New York  NY  USA  1994. ISBN 0471619779.

[11] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[12] Ronan Fruit  Matteo Pirotta  Alessandro Lazaric  and Emma Brunskill. Regret minimization in

mdps with options without prior knowledge. In NIPS  pages 3169–3179  2017.

[13] Mohammad Sadegh Talebi and Odalric-Ambrym Maillard. Variance-aware regret bounds for
undiscounted reinforcement learning in mdps. In ALT  volume 83 of Proceedings of Machine
Learning Research  pages 770–805. PMLR  2018.

[14] Peter L. Bartlett and Ambuj Tewari. REGAL: A regularization based algorithm for reinforcement

learning in weakly communicating MDPs. In UAI  pages 35–42. AUAI Press  2009.

[15] Ronald Ortner. Regret bounds for reinforcement learning via markov chain concentration.

CoRR  abs/1808.01813  2018. URL http://arxiv.org/abs/1808.01813.

[16] K. Lakshmanan  Ronald Ortner  and Daniil Ryabko. Improved regret bounds for undiscounted
continuous reinforcement learning. In ICML  volume 37 of JMLR Workshop and Conference
Proceedings  pages 524–532. JMLR.org  2015.

[17] Jinane Abounadi  Dimitri P. Bertsekas  and Vivek S. Borkar. Learning algorithms for markov
decision processes with average cost. SIAM J. Control and Optimization  40(3):681–698  2001.

[18] Ronald Ortner. Optimism in the face of uncertainty should be refutable. Minds and Machines 

18(4):521–526  2008.

[19] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:

worst-case regret bounds. In NIPS  pages 1184–1194  2017.

9

[20] TW Archibald  KIM McKinnon  and LC Thomas. On the generation of markov decision

processes. Journal of the Operational Research Society  46(3):354–361  1995.

[21] Michael T. Rosenstein and Andrew G. Barto. Supervised actor-critic reinforcement learning.

Handbook of learning and approximate dynamic programming  2:359  2004.

[22] Andrew William Moore. Efﬁcient memory-based learning for robot control. Technical report 

University of Cambridge  1990.

[23] Ronan Fruit  Matteo Pirotta  and Alessandro Lazaric. Near optimal exploration-exploitation in

non-communicating markov decision processes. In NIPS  2018.

[24] Sébastien Bubeck and Nicolò Cesa-Bianchi. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends® in Machine Learning  5(1):1–122 
2012. ISSN 1935-8237. doi: 10.1561/2200000024.

[25] Sham Kakade  Mengdi Wang  and Lin F. Yang. Variance reduction methods for sublinear

reinforcement learning. CoRR  abs/1802.09184  2018.

[26] Dimitri P Bertsekas. Dynamic programming and optimal control. Vol II. Number 2. Athena

scientiﬁc Belmont  MA  1995.

[27] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Pre-publication version  2018. URL

http://downloads.tor-lattimore.com/banditbook/book.pdf.

[28] Y.S. Chow and H. Teicher. Probability Theory: Independence  Interchangeability  Martingales.

Springer texts in statistics. World Publishing Company  1988. ISBN 9780387966953.

[29] Eyal Even-Dar and Yishay Mansour. Convergence of optimistic and incremental q-learning. In

NIPS  pages 1499–1506. MIT Press  2001.

[30] A. Klenke and M. Loève. Probability Theory: A Comprehensive Course. Graduate texts in

mathematics. Springer  2013. ISBN 9781447153627.

[31] David A. Freedman. On tail probabilities for martingales. Ann. Probab.  3(1):100–118  02 1975.

doi: 10.1214/aop/1176996452.

[32] Nicolò Cesa-Bianchi and Claudio Gentile. Improved risk tail bounds for on-line algorithms. In

NIPS  pages 195–202  2005.

10

,Jian QIAN
Ronan Fruit
Matteo Pirotta
Alessandro Lazaric