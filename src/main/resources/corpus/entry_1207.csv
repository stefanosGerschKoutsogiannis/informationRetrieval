2018,Coordinate Descent with Bandit Sampling,Coordinate descent methods minimize a cost function by updating a single decision variable (corresponding to one coordinate) at a time. Ideally  we would update the decision variable that yields the largest marginal decrease in the cost function. However  finding this coordinate would require checking all of them  which is not computationally practical. Therefore  we propose a new adaptive method for coordinate descent. First  we define a lower bound on the decrease of the cost function when a coordinate is updated and  instead of calculating this lower bound for all coordinates  we use a multi-armed bandit algorithm to learn which coordinates result in the largest marginal decrease and simultaneously perform coordinate descent. We show that our approach improves the convergence of the coordinate methods both theoretically and experimentally.,Coordinate Descent with Bandit Sampling

Farnood Salehi1

Patrick Thiran2

L. Elisa Celis3

1 2 3 School of Computer and Communication Sciences

École Polytechnique Fédérale de Lausanne (EPFL)

firstname.lastname@epfl.ch

Abstract

Coordinate descent methods usually minimize a cost function by updating a random
decision variable (corresponding to one coordinate) at a time. Ideally  we would
update the decision variable that yields the largest decrease in the cost function.
However  ﬁnding this coordinate would require checking all of them  which would
effectively negate the improvement in computational tractability that coordinate
descent is intended to afford. To address this  we propose a new adaptive method
for selecting a coordinate. First  we ﬁnd a lower bound on the amount the cost
function decreases when a coordinate is updated. We then use a multi-armed
bandit algorithm to learn which coordinates result in the largest lower bound by
interleaving this learning with conventional coordinate descent updates except
that the coordinate is selected proportionately to the expected decrease. We show
that our approach improves the convergence of coordinate descent methods both
theoretically and experimentally.

1

Introduction

Most supervised learning algorithms minimize an empirical risk cost function over a dataset. Design-
ing fast optimization algorithms for these cost functions is crucial  especially as the size of datasets
continues to increase. (Regularized) empirical risk cost functions can often be written as

F (x) = f (Ax) +

gi(xi) 

(1)

dXi=1

where f (·) : Rn ! R is a smooth convex function  d is the number of decision variables (coordi-
nates) on which the cost function is minimized  which are gathered in vector x 2 Rd  gi(·) : R ! R
are convex functions for all i 2 [d]  and A 2 Rn⇥d is the data matrix. As a running example  consider
Lasso: if Y 2 Rn are the labels  f (Ax) = 1/2nkY  Axk2  where k·k stands for the Euclidean
norm  and gi(xi) = |xi|. When Lasso is minimized  d is the number of features  whereas when the
dual of Lasso is minimized  d is the number of datapoints.
The gradient descent method is widely used to minimize (1). However  computing the gradient of the
cost function F (·) can be computationally prohibitive. To bypass this problem  two approaches have
been developed: (i) Stochastic Gradient Descent (SGD) selects one datapoint to compute an unbiased
estimator for the gradient at each time step  and (ii) Coordinate Descent (CD) selects one coordinate
to optimize over at each time step. In this paper  we focus on improving the latter technique.
When CD was ﬁrst introduced  algorithms did not differentiate between coordinates; each coordinate
i 2 [d] was selected uniformly at random at each time step (see  e.g.  [19  20]). However  recent
works (see  e.g.  [10  24  15]) have shown that exploiting the structure of the data and sampling the

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

xt

¯r

i

Coordinate	
Selection

Compute
rt+1
i

Update	the	

decision	variable

xt
i

Update	

coordinate	

selection	strategy

xt+1

¯r

Figure 1: Our approach for coordinate descent. The top (green) part of the approach handles the
updates to the decision variable xt
i (using whichever CD update is desired; our theoretical results
hold for updates in the class H in Deﬁnition 4 in the supplementary materials. The bottom (yellow)
part of the approach handles the selection of i 2 [d] according to a coordinate selection strategy
which is updated via bandit optimization (using whichever bandit algorithm is desired) from rt+1
.

i

coordinates from an appropriate non-uniform distribution can result in better convergence guarantees 
both in theory and practice. The challenge is to ﬁnd the appropriate non-uniform sampling distribution
with a lightweight mechanism that maintains the computational tractability of CD.
In this work  we propose a novel adaptive non-uniform coordinate selection method that can be
applied to both the primal and dual forms of a cost function. The method exploits the structure of the
data to optimize the model by ﬁnding and frequently updating the most predictive decision variables.
In particular  for each i 2 [d] at time t  a lower bound rt
i is derived (which we call the marginal
decrease) on the amount by which the cost function will decrease when only the ith coordinate is
updated.
i quantiﬁes by how much updating the ith coordinate is guaranteed to improve
The marginal decrease rt
i is then the one that is updated by the algorithm max_r 
the model. The coordinate i with the largest rt
described in Section 2.3. This approach is particularly beneﬁcial when the distribution of rt
is has a
high variance across i; in such cases updating different coordinates can yield very different decreases
in the cost function. For example  if the distribution of rt
is has a high variance across i  max_r is up
to d2 times better than uniform sampling  whereas state-of-the-art methods can be at most d3/2 better
than uniform sampling in such cases (see Theorem 2 in Section 2.3). More precisely  in max_r the
convergence speed is proportional to the ratio of the duality gap to the maximum coordinate-wise
duality gap. max_r is able to outperform existing adaptive methods because it explicitly ﬁnds the
coordinates that yield a large decrease of the cost function  instead of computing a distribution over
coordinates based on an approximation of the marginal decreases.
However  the computation of the marginal decrease rt
i for all i 2 [d] may still be computationally
prohibitive. To bypass this obstacle  we adopt in Section 2.4 a principled approach (B_max_r) for
learning the best rt
is  instead of explicitly computing all of them: At each time t  we choose a single
coordinate i and update it. Next  we compute the marginal decrease rt
i of the selected coordinate i and
use it as feedback to adapt our coordinate selection strategy using a bandit framework. Thus  in effect 
we learn estimates of the rt
is and simultaneously optimize the cost function (see Figure 1). We prove
that this approach can perform almost as well as max_r  yet decreases the number of calculations
required by a factor of d (see Proposition 2).
We test this approach on several standard datasets  using different cost functions (including Lasso 
logistic and ridge regression) and for both the adaptive setting (the ﬁrst approach) and the bandit
setting (the second approach). We observe that the bandit coordinate selection approach accelerates
the convergence of a variety of CD methods (e.g.  StingyCD [11] for Lasso in Figure 2  dual CD [18]
for L1-regularized logistic-regression in Figure 3  and dual CD [13] for ridge-regression in Figure 3).
Furthermore  we observe that in most of the experiments B_max_r (the second approach) converges
as fast as max_r (the ﬁrst approach)  while it has the same computational complexity as CD with
uniform sampling (see Section 4).

2

2 Technical Contributions

2.1 Preliminaries
Consider the following primal-dual optimization pairs

dXi=1

dXi=1

(2)

i (a>i w) 
g?

FD(w) = f ?(w) +

min
x2Rd

F (x) = f (Ax) +

gi(xi)  min
w2Rn
i are the convex conjugates of f and gi  respectively.1
where A = [a1  . . .   ad]  ai 2 Rn  and f ? and g?
The goal is to ﬁnd ¯x := argminx2RdF (x). In rest of the paper  we will need the following notations.
We denote by ✏(x) = F (x)  F ( ¯x) the sub-optimality gap of F (x)  and by G(x  w) = F (x) 
(FD(w)) the duality gap between the primal and the dual solutions  which is an upper bound
on ✏(x) for all x 2 Rd. We further use the shorthand G(x) for G(x  w) when w = rf (Ax).
For w = rf (Ax)  using the Fenchel-Young property f (Ax) + f ?(w) = (Ax)>w  G(x) can
i (a>i w) + gi(xi) + xia>i w is the ith
be written as G(x) = Pd
coordinate-wise duality gap. Finally  we denote by i = ¯u  xi the ith dual residue where
¯u = arg minu2@g ?
2.2 Marginal Decreases
Our coordinate selection approach works for a class H of update rules for the decision variable
xi. For the ease of exposition  we defer the formal deﬁnition of the class H (Deﬁnition 4) to the
supplementary materials and give here an informal but more insightful deﬁnition. The class H uses
the following reference update rule for xi  when f (·) is 1/-smooth and gi is µi-strongly convex:
xt+1
i = xt

i=1 Gi(x) where Gi(x) = g?
i (a>i w) |u  xi| with w = rf (Ax).

i  where

i + st

it

st

i = min⇢1 

i + µi|t
Gt

i|2(µi + kaik2/) .

i|2/2

|t

(3)

i when Gt

i  the ith coordinate-wise duality gap at time t  quantify
i  the ith dual residue at time t  and Gt
t
i is to increase the step size of
the sub-optimality along coordinate i. Because of (3)  the effect of st
i is large. The class H contains also all update rules that decrease the cost
the update of xt
function faster than the reference update rule (see two criteria (11) and (12) in Deﬁnition 4 in the
supplementary materials. For example  the update rules in [18] and [11] for Lasso  the update rules in
[20] for hinge-loss SVM and ridge regression  the update rule in [6] for the strongly convex functions 
in addition to the reference update rule deﬁned above  belong to this class H.
We begin our analysis with a lemma that provides the marginal decrease rt
i 2 [d] according to any update rule in the class H.
Lemma 1 In (1)  let f be 1/-smooth and each gi be µi-strongly convex with convexity parameter
µi  0 8i 2 [d]. For µi = 0  we assume that gi has a L-bounded support. After selecting the
coordinate i 2 [d] and updating xt

i with an update rule in H  we have the following guarantee:

i of updating a coordinate

where

F (xt+1)  F (xt)  rt
i 
if st
i = 1 
otherwise.

i|2
i  kaik2|t
2
i|2/2)
i+µi|t

i(Gt
st

i =( Gt

rt

2

(4)

(5)

In the proof of Lemma 1 in the supplementary materials  the decrease of the cost function is upper-
bounded using the smoothness property of f (·) and the convexity of gi(·) for any update rule in the
class H.
Remark 1 In the well-known SGD  the cost function F (xt) might increase at some iterations t. In
contrast  if we use CD with an update rule in H  it follows from (5) and (3) that rt
i  0 for all t 
and from (4) that the cost function F (xt) never increases. This property provides a strong stability
guarantee  and explains (in part) the good performance observed in the experiments in Section 4.
1Recall that the convex conjugate of a function h(·) : Rd ! R is h?(x) = supv2Rd{x>v  h(v)}.

3

2.3 Greedy Algorithms (Full Information Setting)

In ﬁrst setting  which we call full information setting  we assume that we have computed rt
i for
all i 2 [d] and all t (we will relax this assumption in Section 2.4). Our ﬁrst algorithm max_r makes
then a greedy use of Lemma 1  by simply choosing at time t the coordinate i with the largest rt
i.
Proposition 1 (max_r) Under the assumptions of Lemma 1  the optimal coordinate it for minimizing
the right-hand side of (4) at time t is it = arg maxj2[d] rt
j.
Remark 2 This rule can be seen as an extension of the Gauss-Southwell rule [13] for the class of
cost functions that the gradient does not exist  which selects the coordinate whose gradient has the
largest magnitude (when riF (x) exits)  i.e.  it = arg maxi2[d] |riF (x)|. Indeed  Lemma 2 in the
supplementary materials shows that for the particular case of L2-regularized cost functions F (x) 
the Gauss-Southwell rule and max_r are equivalent.
If functions gi(·) are strongly convex (i.e.  µi > 0)  then max_r results in a linear convergence rate
and matches the lower bound in [2].
Theorem 1 Let gi in (1) be µi-strongly convex with µi > 0 for all i 2 [d]. Under the assumptions of
Lemma 1  we have the following linear convergence guarantee:

✏(xt)  ✏(x0)

tYl=1

0@1  max

i2[d]

Gi(xt)µi

G(xt)⇣µi + kaik2
 ⌘

1A  

for all t > 0  where ✏(x0) is the sub-optimality gap at t = 0.
Now  if functions gi(·) are not necessary strongly convex (i.e.  µi = 0)  max_r is also very effective
and outperforms the state-of-the-art.
Theorem 2 Under the assumptions of Lemma 1  let µi  0 for all i 2 [d]. Then 

(6)

(7)

✏(xt) 

8L2⌘2/
2d + t  t0

G(xt) kaik/Gi(xt) for all iterations l 2 [t].

for all t  t0  where t0 = max{1  2d log d✏(x0)/4L2⌘2}  ✏(x0) is the sub-optimality gap at t = 0
and ⌘ = O(d) is an upper bound on mini2[d]
To make the convergence bounds (6) and (7) easier to understand  assume that µi = µ1 and that the
data is normalized  so that kaik = 1 for all i 2 [d]. First  by letting ⌘ = O(d) be an upper bound
on mini2[d]
G(xt)/Gi(xt) for all iterations l 2 [t]  Theorem 1 results in a linear convergence rate  i.e. 
✏(xt) = O (exp(c1t/⌘)) for some constant c1 > 0 that depends on µ1 and   whereas Theorem 2
provides a sublinear convergence guarantee  i.e.  ✏(xt) = O⌘2/t.
Second  note that in both convergence guarantees  we would like to have a small ⌘. The ratio ⌘ can
be as large as d  when the different coordinate-wise gaps Gi(xt) are equal. In this case  non-uniform
sampling does not bring any advantage over uniform sampling  as expected. In contrast  if for instance
c · G(xt)  maxi2[d] Gi(xt) for some constant 1/d  c  1  then choosing the coordinate with the
largest rt
i results in a decrease in the cost function  that is 1  c · d times larger compared to uniform
sampling. Theorems 1 and 2 are proven in the supplementary materials.
Finally  let us compare the bound of max_r given in Theorem 2 with the state-of-the-art bounds of
ada_gap in Theorem 3.7 of [15] and of CD algorithm in Theorem 2 of [8]. For the sake of simplicity 
assume that kaik = 1 for all i 2 [d]. When c · G(xt)  maxi2[d] Gi(xt) and some constant 1/d 
c  1  the convergence guarantee for ada_gap is E [✏(xt)] = OpdL2/(c2+1/d)3/2(2d+t) and the
convergence guarantee of the CD algorithm in [8] is E [✏(xt)] = OdL2/c(2d+t)  which are much
tighter than the convergence guarantee of CD with uniform sampling E [✏(xt)] = Od2L2/(2d+t).
In contrast  the convergence guarantee of max_r is ✏(xt) = OL2/c2(2d+t)  which is pd/c times
better than ada_gap  dc times better than the CD algorithm in [8] and c2d2 times better than uniform
sampling for the same constant c  1/d.
Remark 3 There is no randomness in the selection rule used in max_r (beyond tie breaking)  hence
the convergence results given in Theorems 1 and 2 a.s. hold for all t.

4

2.4 Bandit Algorithms (Partial Information Setting)

i in [15  8]  the
State-of-the-art algorithms and max_r require knowing a sub-optimality metric (e.g.  Gt
i in this work) for all coordinates i 2 [d] 
norm of gradient riF (xt) in [13]  the marginal decreases rt
which can be computationally expensive if the number of coordinates d is large. To overcome this
problem  we use a novel approach inspired by the bandit framework that learns the best coordinates
over time from the partial information it receives during the training.
Multi-armed Bandit: In a multi-armed bandit (MAB) problem  there are d possible arms (which
are here the coordinates) that a bandit algorithm can choose from for a reward (rt
i in this work) at
time t. The goal of the MAB is to maximize the cumulative rewards that it receives over T rounds
it  where it is the arm (coordinate) selected at time t). After each round  the MAB only
observes the reward of the selected arm it  and hence has only access to partial information  which it
then uses to reﬁne its arm (coordinate) selection strategy for the next round.

(i.e. PT

i=1 rt

i = rte
i

else

end for

it = rt+1

it

j

i for all i 6= it

set ¯rt

i = rt

i for all i 2 [d]

it = rt+1

it

and ¯rt+1

i = ¯rt

i = r0

i for all i 2 [d]

if t mod E == 0 then

end if
Update xt
Set ¯rt+1

it by an update rule in H

end if
Generate K ⇠ Bern(")
if K == 1 then

Algorithm 1 B_max_r
input: x0  " and E
initialize: set ¯r0
for t = 1 to T do

Select it 2 [d] uniformly at random
Select it = arg maxi2[d] ¯rt

In our second algorithm B_max_r 
the
marginal decreases rt
i computed for all i 2
[d] at each round t by max_r are replaced
by estimates ¯ri computed by an MAB as
follows. First  time is divided into bins of
size E. At the beginning of a bin te  the
marginal decreases rte
i of all coordinates
i 2 [d] are computed  and the estimates
for all
are set to these values (¯rt
i 2 [d]). At each iteration te  t  te + E
within that bin  with probability " a coor-
dinate it 2 [d] is selected uniformly at
random  and otherwise (with probability
(1  ")) the coordinate with the largest ¯rt
i
is selected. Coordinate it is next updated 
as well as the estimate of the marginal de-
crease ¯rt+1
  whereas the other es-
timates ¯rt+1
remain unchanged for j 6= it.
The algorithm can be seen as a modiﬁed
version of "-greedy (see [3]) that is devel-
oped for the setting where the reward of arms follow a ﬁxed probability distribution  "-greedy uses
the empirical mean of the observed rewards as an estimate of the rewards. In contrast  in our setting 
the rewards do not follow such a ﬁxed probability distribution and the most recently observed reward
is the best estimate of the reward that we could have. In B_max_r  we choose E not too large and
" large enough such that every arm (coordinate) is sampled often enough to maintain an accurate
estimate of the rewards rt
The next proposition shows the effect of the estimation error on the convergence rate.
Proposition 2 Consider the same assumptions as Lemma 1 and Theorem 2. For simplicity  let kaik =
ka1k for all i 2 [d] and ✏(x0) p2↵L2ka1k2/ ("/d + 1"/c) = O(d).2 Let jt
i.
? = arg maxi2[d] ¯rt
?  c(E  ") for some ﬁnite constant c = c(E  ")  then by using B_max_r (with bin
If maxi2[d] rt
size E and exploration parameter ") we have
E⇥✏(xt)⇤ 

G(xt)/Gi(xt) for iterations l 2 [t].

for all t  t0 = max1  4✏(x0)/↵ log(2✏(x0)/↵) = O(d) and where ⌘ is an upper bound on

mini2[d]
What is the effect of c(E  ")? In Proposition 2  c = c(E  ") upper bounds the estimation error of
the marginal decreases rt
i. To make the effect of c(E  ") on the convergence bound (8) easier to
2These assumptions are not necessary but they make the analysis simpler. For example  even if ✏(x0) does
not satisfy the required condition  we can scale down F (x) by m so that F (x)/m is minimized. The new
sub-optimality gap becomes ✏(x0)/m  and for a sufﬁciently large m the initial condition is satisﬁed.

i (we use E = O(d) and " = 1/2 in the experiments of Section 4).

8L2ka1k2

 ("/d2 + (1")/⌘2c)

 

↵

2 + t  t0

i/rt
jt

  where ↵ =

(8)

i

5

Table 1: The shaded rows correspond to the algorithms introduced in this paper. ¯z denotes the

number of non-zero entries of the data matrix A. The numbers below the column dataset/cost are the
clock time (in seconds) needed for the algorithms to reach a sub-optimality gap of ✏(xt) = exp (5).

aloi/Lasso

27.8
52.8
6.2
75
16.3

-
11

dataset/cost
a9a/log reg

11.8
42.4
4.5
11.1
2.3
-
1.9

method

uniform
ada_gap
max_r

gap_per_epoch

Approx

NUACDM
B_max_r

computational cost

(per epoch)

O(¯z)
O(d · ¯z)
O(d · ¯z)

O(¯z + d log d)
O(¯z + d log d)
O(¯z + d log d)
O(¯z + d log d)

usps/ridge reg

1
88
9.5
300
-
6
1

understand  let " = 1/2  then ↵ ⇠ 1/(1/d2+1/⌘2c). We can see from the convergence bound (8) and
the value of ↵ that if c is large  the convergence rate is proportional to d2 similarly to uniform
sampling (i.e.  ✏(xt) 2 O(d2/t)). Otherwise  if c is small  the convergence rate is similar to max_r
(✏(xt) 2 O(⌘2/t)  see Theorem 2).
How to control c = c(E  ")? We can control the value of c by varying the bin size E. Doing so 
there is a trade-off between the value of c and the average computational cost of an iteration. On the
one hand  if we set the bin size to E = 1 (i.e.  full information setting)  then c = 1 and B_max_r
boils down to max_r  while the average computational cost of an iteration is O(nd). On the other
hand  if E > 1 (i.e.  partial information setting)  then c  1  while the average computational
complexity of an iteration is O(nd/E). In our experiments  we ﬁnd that by setting d/2  E  d 
B_max_r converges faster than uniform sampling (and other state-of-the-art methods) while the
average computational cost of an iteration is O(n + log d)  similarly to the computational cost of an
iteration of CD with uniform sampling (O(n))  see Figures 2 and 3. We also ﬁnd that any exploration
parameter " 2 [0.2  0.7] in B_max_r works reasonably well. The proof of Proposition 2 is similar to
the proof of Theorem 2 and is given in the supplementary materials.
3 Related Work
Non-uniform coordinate selection has been proposed ﬁrst for constant (non-adaptive) probability
distributions p over [d]. In [24]  pi is proportional to the Lipschitz constant of g?
i . Similar distributions
are used in [1  23] for strongly convex f in (1).
Time varying (adaptive) distributions  such as pt
i = Gi(xt)/G(xt)
[15  14]  have also been considered. In all these cases  the full information setting is used  which
requires the computation of the distribution pt (⌦(nd) calculations) at each step. To bypass this
problem  heuristics are often used; e.g.  pt is calculated once at the beginning of an epoch of length
E and is left unchanged throughout the remainder of that epoch. This heuristic approach does not
work well in a scenario where Gi(xt) varies signiﬁcantly. In [8] a similar idea to max_r is used
with ri replaced by Gi  but only in the full information setting. Because of the update rule used in
[8]  the convergence rate is O (d · max Gi(xt)/G(xt)) times slower than Theorem 2 (see also the
comparison at the end of Section 2.3). The Gauss-Southwell rule (GS) is another coordinate selection
strategy for smooth cost functions [21] and its convergence is studied in [13] and [22]. GS selects
the coordinate to update as the one that maximizes |riF (xt)| at time t. max_r can be seen as an
extension of GS to a broader class of cost functions (see Lemma 2 in the supplementary materials).
Furthermore  when only sub-gradients are deﬁned for gi(·)  GS needs to solve a proximal problem.
To address the computational tractability of GS  in [22]  lower and upper bounds on the gradients
are computed (instead of computing the gradient itself) and used for selecting the coordinates  but
these lower and upper bounds might be loose and/or difﬁcult to ﬁnd. For example  without a heavy
pre-processing of the data  ASCD in [22] converges with the same rate as uniform sampling when the
data is normalized and f (Ax) = kAx  Y k2.
In contrast  our principled approach leverages a bandit algorithm to learn a good estimate of rt
i;
this allows for theoretical guarantees and outperforms the state-of-the-art methods  as we will see
in Section 4. Furthermore  our approach does not require the cost function to be strongly convex
(contrary to e.g.  [6  13])

i|/(Pd

j|) [6]  and pt

i = |t

j=1 |t

6

e
v
i
t
p
a
d
A

t
i

d
n
a
B
-
e
v
i
t
p
a
d
A

(a) usps

(b) aloi

(c) protein

(d) usps

(e) aloi

(f) protein

Figure 2: CD for regression using Lasso (i.e.  a non-smooth cost function). Y-axis is the log of
sub-optimality gap and x-axis is the number of epochs. The algorithms presented in this paper

(max_r  B_max_r) outperform the state-of-the-art across the board.

Bandit approaches have very recently been used to accelerate various stochastic optimization algo-
rithms; among these works [12  17  16  4] focus on improving the convergence of SGD by reducing
the variance of the estimator for the gradient. A bandit approach is also used in [12] to sample for CD.
However  instead of using the bandit to minimize the cost function directly as in B_max_r  it is used
to minimize the variance of the estimated gradient. This results in a O(1/pt) convergence  whereas
the approach in our paper attains an O(1/t) rate of convergence. In [16] bandits are used to ﬁnd the
coordinate i whose gradient has the largest magnitude (similar to GS). At each round t a stochastic
bandit problem is solved from scratch  ignoring all past information prior to t  which  depending on
the number of datapoints  might require many iterations. In contrast  our method incorporates past
information and needs only one sample per iteration.
4 Empirical Simulations
We compare the algorithms from this paper with the state-of-the-art approaches  in two ways. First 
we compare the algorithm (max_r) for full information setting as in Section 2.3 against other state-of-
the-art methods that similarly use O(d · ¯z) computations per epoch of size d  where ¯z denotes the
number of non-zero elements of A. Next  we compare the algorithm for partial information setting as
in Section 2.4 (B_max_r) against other methods with appropriate heuristic modiﬁcations that also
allow them to use O(¯z) computations per epoch. The datasets we use are found in [5]; we consider
usps  aloi and protein for regression  and w8a and a9a for binary classiﬁcation (see Table 2 in the
supplementary materials for statistics about these datasets).
Various cost functions are considered for the experiments  including a strongly convex cost function
(ridge regression) and non-smooth cost functions (Lasso and L1-regularized logistic regression).
These cost functions are optimized using different algorithms  which minimize either the primal or the
dual cost function. The convergence time is the metric that we use to evaluate different algorithms.

4.1 Experimental Setup
Benchmarks for Adaptive Algorithm (max_r):
• uniform [18]: Sample a coordinate i 2 [n] uniformly at random.3
• ada_gap [15]: Sample a coordinate i 2 [n] with probability Gi(xt)/G(xt).
Benchmarks for Adaptive-Bandit Algorithm (B_max_r): For comparison  in addition to the uniform
sampling  we consider the coordinate selection method that has the best performance empirically in
[15] and two accelerated CD methods NUACDM in [1] and Approx in [9].
• gpe [15]: This algorithm is a heuristic version of ada_gap  where the sampling probability
i = Gi(xt)/G(xt) for i 2 [d] is re-computed once at the beginning of each bin of length
pt
E.
3 If kaik = kajk 8i  j 2 [n]  importance sampling method in [24] is equivalent to uniform in Lasso and

logistic regression.

7

o
f
n
I
-
l
l
u
F

o
f
n
I
-
l
a
i
t
r
a
P

(a) w8a  logistic reg.

(b) a9a  logistic reg.

(c) usps  ridge reg.

(d) protein  ridge reg.

(e) w8a  logistic reg.

(f) a9a  logistic reg.

(g) usps  ridge reg.

(h) protein  ridge reg.

Figure 3: CD for binary Classiﬁcation using L1-regularized logistic regression and CD for regression

using Lasso. The algorithms presented in this paper (max_r and B_max_r) outperform the

state-of-the-art across the board.

the gradient to update the decision variables.

• NUACDM [1]: Sample a coordinate i 2 [d] with probability proportional to the square root of
smoothness of the cost function along the ith coordinate  then use an unbiased estimator for the
gradient to update the decision variables.
• Approx [9]: Sample a coordinate i 2 [d] uniformly at random  then use an unbiased estimator for
NUACDM is the state-of-the-art accelerated CD method (see Figures 2 and 3 in [1]) for smooth
cost functions. Approx is an accelerated CD method proposed for cost functions with non-smooth
gi(·) in (1). We implemented Approx for such cost functions in Lasso and L1-reguralized logistic
regression. We also implemented Approx for ridge-regression but NUACDM converged faster in our
setting  whereas for the smoothen version of Lasso but Approx converged faster than NUACDM in
our setting. The origin of the computational cost is two-fold: Sampling a coordinate i and updating
it. The average computational cost of the algorithms for E = d/2 is depicted in Table 1. Next  we
explain the setups and update rules used in the experiments.

i = arg minz [f (Axt + (z  xt

For Lasso F (x) = 1/2nkY  Axk2 +Pn
i=1 |xi|. We consider the stingyCD update proposed in
[11]: xt+1
i)ai)] + gi(xi). In Lasso  the gis are not strongly convex
(µi = 0). Therefore  for computing the dual residue  the Lipschitzing technique in [7] is used  i.e. 
i (ui) = B max{|ui|   0}.
gi(·) is assumed to have bounded support of size B = F (x0)/ and g?
For logistic regression F (x) = 1/nPn
i=1 |xi|. We consider the
update rule proposed in [18]: xt+1
i  4@f (Axt)/@xi)  where s(q) = sign(q) max{|q|
  0}.
For ridge regression F (x) = 1/nkY  Axk2 + /2kxk2 and it is strongly convex. We consider the
update proposed for the dual of ridge regression in [20]  hence B_max_r and other adaptive methods
select one of the dual decision variables to update.
In all experiments  s are chosen such that the test and train errors are comparable  and all update
rules belong to H. In addition  in all experiments  E = d/2 in B_max_r and gap_per_epoch. Recall
that when minimizing the primal  d is the number of features and when minimizing the dual  d is the
number of datapoints.

i=1 log1 + exp(yi · x>ai)+Pn

i = s4(xt

4.2 Empirical Results

Figure 2 shows the result for Lasso. Among the adaptive algorithms  max_r outperforms the state-of-
the-art (see Figures 2a  2b and 2c). Among the adaptive-bandit algorithms  B_max_r outperforms the
benchmarks (see Figures 2d  2e and 2f). We also see that B_max_r converges slower than max_r for
the same number of iterations  but we note that an iteration of B_max_r is O(d) times cheaper than
max_r. For logistic regression  see Figures 3a  3b  3e and 3f. Again  those algorithms outperform
the state-of-the-art. We also see that B_max_r converges with the same rate as max_r. We see that
the accelerated CD method Approx converges faster than uniform sampling and gap_per_epoch  but
using B_max_r improves the convergence rate and reaches a lower sub-optimality gap ✏ with the
same number of iterations. For ridge regression  we see in Figures 3c  3d that max_r converges faster

8

(a) Number of iterations to reach

log ✏(xt) = 5.

(b) Per-epoch clock time for

different values of E.

Figure 4: Analysis of the running time of B_max_r for different values of " and E. A smaller E
results in fewer iterations  and results in larger clock time per epoch (an epoch is d iterations of CD).

than the state-of-the-art ada-gap. We also see in Figures 3g  3h that B_max_r converges faster than
other algorithms. gap_per_epoch performs poorly because it is unable to adapt to the variability of
the coordinate-wise duality gaps Gi that vary a lot from one iteration to the next. In contrast  this
variation slows down the convergence of B_max_r compared to max_r  but B_max_r is still able to
cope with this change by exploring and updating the estimations of the marginal decreases. In the
experiments we report the sub-optimality gap as a function of the number of iterations  but the results
are also favourable when we report them as a function of actual time. To clarify  we compare the
clock time needed by each algorithm to reach a sub-optimality gap ✏(xt) = exp(5) in Table 1.4
Next  we study the choice of parameters " and E in Algorithm 1. As explained in Section 2.4 the
choice of these two parameters affect c in Proposition 2  hence the convergence rate. To test the effect
of " and E on the convergence rate  we choose a9a dataset and perform a binary classiﬁcation on it
by using the logistic regression cost function. Figure 4a depicts the number of iterations required to
reach the log-suboptimality gap log ✏ of 5. In the top-right corner  " = 1 and B_max_r becomes
CD with uniform sampling (for any value of E). As expected  for any "  the smaller E  the smaller
the number of iterations to reach the log-suboptimality gap of 5. This means that c("  E) is a
decreasing function of E. Also  we see that as " increases  the convergence becomes slower. That
implies that for this dataset and cost function c("  E) is close to 1 for all " hence there is no need
for exploration and a smaller value for " can be chosen. Figure 4b depicts the per epoch clock time
for " = 0.5 and different values of E. Note that the clock time is not a function of ". As expected  a
smaller bin size E results in a larger clock time  because we need to compute the marginal decreases
for all coordinates more often. After E = 2d/5 we see that clock time does not decrease much  this
can be explained by the fact that for large enough E computing the gradient takes more clock time
than computing the marginal decreases.

5 Conclusion
In this work  we propose a new approach to select the coordinates to update in CD methods. We
derive a lower bound on the decrease of the cost function in Lemma 1  i.e.  the marginal decrease 
when a coordinate is updated  for a large class of update methods H. We use the marginal decreases
to quantify how much updating a coordinate improves the model. Next  we use a bandit algorithm
to learn which coordinates decrease the cost function signiﬁcantly throughout the course of the
optimization algorithm by using the marginal decreases as feedback (see Figure 1). We show that
the approach converges faster than state-of-the-art approaches both theoretically and empirically.
We emphasize that our coordinate selection approach is quite general and works for a large class of
update rules H  which includes Lasso  SVM  ridge and logistic regression  and a large class of bandit
algorithms that select the coordinate to update.
The bandit algorithm B_max_r uses only the marginal decrease of the selected coordinate to update
the estimations of the marginal decreases. An important open question is to understand the effect
of having additional budget to choose multiple coordinates at each time t. The challenge lies in
designing appropriate algorithms to invest this budget to update the coordinate selection strategy such
that B_max_r performance becomes even closer to max_r.

4In our numerical experiments  all algorithms are optimized as much as possible by avoiding any unnecessary
computations  by using efﬁcient data structures for sampling  by reusing the computed values from past iterations
and (if possible) by writing the computations in efﬁcient matrix form.

9

References

[1] Z Allen-Zhu  Z Qu  P Richtárik  and Y Yuan. Even faster accelerated coordinate descent using
non-uniform sampling. In International Conference on Machine Learning  pages 1110–1119 
2016.

[2] Y Arjevani and O Shamir. Dimension-free iteration complexity of ﬁnite sum optimization

problems. In Advances in Neural Information Processing Systems  pages 3540–3548  2016.

[3] P Auer  N Cesa-Bianchi  Y Freund  and R Schapire. The nonstochastic multiarmed bandit

problem. SIAM journal on computing  32(1):48–77  2002.

[4] Z Borsos  A Krause  and K Levy. Online variance reduction for stochastic optimization. In

International Conference on Learning Theory  2018.

[5] C Chang and C Lin. Libsvm: a library for support vector machines. ACM Transactions on

Intelligent Systems and Technology  2(3):27  2011.

[6] D Csiba  Z Qu  and P Richtárik. Stochastic dual coordinate ascent with adaptive probabilities.

In International Conference on Machine Learning  2015.

[7] C Dünner  S Forte  M Takáˇc  and M Jaggi. Primal-dual rates and certiﬁcates. In International

Conference on Machine Learning  2016.

[8] C Dünner  T Parnell  and M Jaggi. Efﬁcient use of limited-memory accelerators for linear
learning on heterogeneous systems. In Advances in Neural Information Processing Systems 
pages 4261–4270  2017.

[9] O Fercoq and P Richtárik. Accelerated  parallel  and proximal coordinate descent. SIAM

Journal on Optimization  25(4):1997–2023  2015.

[10] T Glasmachers and U Dogan. Accelerated coordinate descent with adaptive coordinate frequen-

cies. In Asian Conference on Machine Learning  pages 72–86  2013.

[11] T Johnson and C Guestrin. Stingycd: Safely avoiding wasteful updates in coordinate descent.

In International Conference on Machine Learning  pages 1752–1760  2017.

[12] H Namkoong  A Sinha  S Yadlowsky  and J Duchi. Adaptive sampling probabilities for

non-smooth optimization. In International Conference on Machine Learning  2017.

[13] J Nutini  M Schmidt  I Laradji  M Friedlander  and H Koepke. Coordinate descent converges
faster with the gauss-southwell rule than random selection. In International Conference on
Machine Learning  pages 1632–1641  2015.

[14] A Osokin  J Alayrac  I Lukasewitz  P Dokania  and S Lacoste-Julien. Minding the gaps for
block frank-wolfe optimization of structured svms. In International Conference on Machine
Learning  2016.

[15] D Perekrestenko  V Cevher  and M Jaggi. Faster coordinate descent via adaptive importance

sampling. In International Conference on Artiﬁcial Intelligence and Statistics  2017.

[16] A Rakotomamonjy  S Koço  and L Ralaivola. Greedy methods  randomization approaches  and
multiarm bandit algorithms for efﬁcient sparsity-constrained optimization. IEEE transactions
on neural networks and learning systems  28(11):2789–2802  2017.

[17] F Salehi  L.E Celis  and P Thiran. Stochastic optimization with bandit sampling. arXiv preprint

arXiv:1708.02544v2  2017.

[18] S Shalev-Shwartz and A Tewari. Stochastic methods for l1-regularized loss minimization.

Journal of Machine Learning Research  12(Jun):1865–1892  2011.

[19] S Shalev-Shwartz and T Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In

Advances in Neural Information Processing Systems  pages 378–385  2013a.

10

[20] S Shalev-Shwartz and T Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research  14(Feb):567–599  2013b.

[21] H Shi  S Tu  Y Xu  and W Yin. A primer on coordinate descent algorithms. arXiv preprint

arXiv:1610.00040  2016.

[22] S Stich  At Raj  and M Jaggi. Approximate steepest coordinate descent. In International

Conference on Machine Learning  2017.

[23] A Zhang and Q Gu. Accelerated stochastic block coordinate descent with optimal sampling. In
International Conference on Knowledge Discovery and Data Mining  pages 2035–2044. ACM 
2016.

[24] P Zhao and T Zhang. Stochastic optimization with importance sampling for regularized loss

minimization. In International Conference on Machine Learning  2015.

11

,Farnood Salehi
Elisa Celis