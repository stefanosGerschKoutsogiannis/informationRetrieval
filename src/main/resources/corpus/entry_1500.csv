2013,Dimension-Free Exponentiated Gradient,We present a new online learning algorithm that extends the exponentiated gradient to infinite dimensional spaces. Our analysis shows that the algorithm is implicitly able to estimate the $L_2$ norm of the unknown competitor  $U$  achieving a regret bound of the order of $O(U \log (U T+1))\sqrt{T})$  instead of the standard $O((U^2 +1) \sqrt{T})$  achievable without knowing $U$. For this analysis  we introduce novel tools for algorithms with time-varying regularizers  through the use of local smoothness. Through a lower bound  we also show that the algorithm is optimal up to $\sqrt{\log T}$ term for linear and Lipschitz losses.,Dimension-Free Exponentiated Gradient

Francesco Orabona

Toyota Technological Institute at Chicago

Chicago  USA

francesco@orabona.com

Abstract

I present a new online learning algorithm that extends the exponentiated gradient
framework to inﬁnite dimensional spaces. My analysis shows that the algorithm is
implicitly able to estimate the L2 norm of the unknown competitor  U  achieving
a regret bound of the order of O(U log(U T + 1))
T )  instead of the standard
O((U 2 + 1)
T )  achievable without knowing U. For this analysis  I introduce
novel tools for algorithms with time-varying regularizers  through the use of local
smoothness. Through a lower bound  I also show that the algorithm is optimal up

to(cid:112)log(U T ) term for linear and Lipschitz losses.

√

√

1

Introduction

Online learning provides a scalable and ﬂexible approach for solving a wide range of prediction
problems  including classiﬁcation  regression  ranking  and portfolio management. These algorithms
work in rounds  where at each round a new instance is given and the algorithm makes a prediction.
After the true label of the instance is revealed  the learning algorithm updates its internal hypothesis.
The aim of the classiﬁer is to minimize the cumulative loss it suffers due to its prediction  such as
the total number of mistakes.
Popular online algorithms for classiﬁcation include the standard Perceptron and its many variants 
such as kernel Perceptron [6]  and p-norm Perceptron [7]. Other online algorithms  with properties
different from those of the standard Perceptron  are based on multiplicative (rather than additive)
updates  such as Winnow [10] for classiﬁcation and Exponentiated Gradient (EG) [9] for regression.
Recently  Online Mirror Descent (OMD)1 and has been proposed as a general meta-algorithm for
online learning  parametrized by a regularizer [16]. By appropriately choosing the regularizer  most
online learning algorithms are recovered as special cases of OMD. Moreover  performance guaran-
tees can also be derived simply by instantiating the general OMD bounds to the speciﬁc regularizer
being used. So  for all the ﬁrst-order online learning algorithms  it is possible to prove regret bounds
of the order of O(f (u)
T )  where T is the number of rounds and f (u) is the regularizer used in
OMD  evaluated on the competitor vector u. Hence  different choices of the regularizer will give
rise to different algorithms and guarantees. For example  p-norm algorithms can be derived from
the squared Lp-norm regularization  while EG can be derived from the entropic one. In particular
T ((cid:107)u(cid:107)2/η + η)). Knowing
(cid:107)u(cid:107) it is possible to tune η to have a O((cid:107)u(cid:107)√
for the Euclidean regularizer η
T ) bound  that is optimal [1]. On the other hand  EG
has a regret bound of O(
In this paper  I use OMD to extend EG to inﬁnite dimensional spaces  through the use of a carefully
designed time-varying regularizer. The algorithm  that I call Dimension-Free Exponentiated Gradi-
ent (DFEG)  does not need direct access to single components of the vectors  rather it only requires

√
T(cid:107)u(cid:107)2  we have a regret bound of O(

T log d)  where d is the dimension of the space.

√

√

√

1The algorithm should be more correctly called Follow the Regularized Leader  however here I follow

Shalev-Shwartz in [16]  and I will denote it by OMD.

1

√
to access them through inner products. Hence  DFEG can be used with kernels too  extending for the
ﬁrst time EG to the kernel domain. I prove a regret bound of O((cid:107)u(cid:107) log((cid:107)u(cid:107)T + 1)
T ). Up to log-
arithmic terms  the bound of DFEG is equal to the optimal bound obtained through the knowledge
of (cid:107)u(cid:107)  but it does not require the tuning of any parameter.
I built upon ideas of [19]  but I designed my new algorithm as an instantiation of OMD  rather than
using an ad-hoc analysis. I believe that this route increases the comprehension of the inner working
of the algorithm  its relation to other algorithms  and it makes easier to extend it in other directions
as well. In order to analyze DFEG  I also introduce new and general techniques to cope with time-
varying regularizers for OMD  using the local smoothness of the dual of the regularization function 
that might be of independent interest. I also extend and improve the lower bound in [19]  to match
the upper bound of DFEG up to a
log T term  and to show an implicit trade-off on the regret versus
different competitors.

√

1.1 Related works

Exponentiated gradient algorithms have been proposed by [9]. The algorithms have multiplicative
updates and regret bounds that depend logarithmically on the dimension of the input space.
In
particular  they proposed a version of EG where the weights are not normalized  called EGU.
A closer algorithm to mine is the epoch-free in [19]. Indeed  DFEG is equivalent to theirs when used
on one dimensional problems. However  the extension to inﬁnite dimensional spaces is nontrivial
and very different in nature from their extension to d-dimensional problems  that consists on run-
ning a copy of the algorithm independently on each coordinate. Their regret bound depends on the
dimension of the space and can neither be used with inﬁnite dimensional spaces nor with kernels.
Vovk proposed two algorithms for square loss  with regret bounds of O(((cid:107)u(cid:107) + Y )
O((cid:107)u(cid:107)√
Indeed  the lower bound I prove shows that for linear and Lipschitz losses a(cid:112)log((cid:107)u(cid:107)T ) term

T ) and
T ) respectively  where Y is an upper bound on the range of the target values [20]. A
matching lower bound is also presented  proving the optimality of the second algorithm. However 
the algorithms seem speciﬁc to the square loss and it is not possible to adapt them to other losses.

√

is unavoidable. Moreover  the second algorithm  being an instantiation of the Aggregating Algo-
rithm [21]  does not seem to have an efﬁcient implementation.
My algorithm also shares similarities in spirit with the family of self-conﬁdent algorithms [2  7  15] 
in which the algorithm self-tunes its parameters based on internal estimates.
From the point of view of the proof technique  the primal-dual analysis of OMD is due to [15  17].
Starting from the work of [8]  it is now clear that OMD can be easily analyzed using only a few basic
convex duality properties. See the recent survey [16] for a lucid description of these developments.
The time-varying regularization for OMD has been explored in [4  12  15]  but in none of these
works does the negative terms in the bound due to the time-varying regularizer play a decisive role.
The use of the local estimates of strong smoothness is new  as far as I know. A related way to have
a local analysis is through the local norms [16]  but my approach is better tailored to my needs.

2 Problem setting and deﬁnitions

In the online learning scenario the learning algorithms work in rounds [3]. Let X a Euclidean vector
space2  at each round t  an instance xt ∈ X  is presented to the algorithm  which then predicts a label
ˆyt ∈ R. Then  the correct label yt is revealed  and the algorithm pays a loss (cid:96)(ˆyt  yt)  for having
predicted ˆyt  instead of yt. The aim of the online learning algorithm is to minimize the cumulative
sum of the losses  on any sequence of data/labels {(xt  yt)}T
t=1. Typical examples of loss functions
are  for example  the absolute loss  |ˆyt − yt|  and the hinge loss  max(1 − ˆytyt  0). Note that the
loss function can change over time  so in the following I will denote by (cid:96)t : R → R the generic loss
function received by the algorithm at time t. In this paper I focus on linear prediction of the form
ˆyt = (cid:104)wt  xt(cid:105)  where wt ∈ X represents the hypothesis of the online algorithm at time t.

2All the theorems hold also in general Hilbert spaces  but for simplicity of exposition I consider a Euclidean

setting.

2

Algorithm 1 Dimension-Free Exponentiated Gradient.

Parameters: 0.882 ≤ a ≤ 1.109  L > 0  δ > 0.
Initialize: θ1 = 0 ∈ X  H0 = δ
for t = 1  2  . . . do
Receive (cid:107)xt(cid:107)  where xt ∈ X

Set Ht = Ht−1 + L2 max(cid:0)(cid:107)xt(cid:107) (cid:107)xt(cid:107)2(cid:1)
(cid:17)

√
Set αt = a
if (cid:107)θt(cid:107) == 0 then choose wt = 0
else choose wt = θt
Suffer loss (cid:96)t((cid:104)wt  xt(cid:105))
Update θt+1 = θt − ∂(cid:96)t((cid:104)wt  xt(cid:105))xt

(cid:16)(cid:107)θt(cid:107)

Ht  βt = H

βt(cid:107)θt(cid:107) exp

3
2
t

αt

end for

algorithm (cid:80)T

t=1 (cid:96)t((cid:104)wt  xt(cid:105))  and the one of an arbitrary and ﬁxed competitor u (cid:80)T
(cid:0)(cid:104)v  u(cid:105)− f (v)(cid:1). The Fenchel-Young inequality states that f (u) + f∗(v) ≥ (cid:104)u  v(cid:105)

We strive to design online learning algorithms for which it is possible to prove a relative regret
bound. Such analysis bounds the regret  that is the difference between the cumulative loss of the
t=1 (cid:96)t((cid:104)u  xt(cid:105)).
We will consider L-Lipschitz losses  that is |(cid:96)t(y) − (cid:96)t(y(cid:48))| ≤ L|y − y(cid:48)|  ∀y  y(cid:48).
I now introduce some basic notions of convex analysis that are used in the paper. I refer to [14] for
deﬁnitions and terminology. I consider functions f : X → R that are closed and convex. Given a
closed and convex function f with domain S ⊆ X  its Fenchel conjugate f∗ : X → R is deﬁned as
f∗(u) = supv∈S
for all v  u. A vector x is a subgradient of a convex function f at v if f (u) − f (v) ≥ (cid:104)u − v  x(cid:105)
for any u in the domain of f. The differential set of f at v  denoted by ∂f (v)  is the set of all
the subgradients of f at v. If f is also differentiable at v  then ∂f (v) contains a single vector 
denoted by ∇f (v)  which is the gradient of f at v. Strong convexity and strong smoothness are key
properties in the design of online learning algorithms  they are deﬁned as follows. A function f is
γ-strongly convex with respect to a norm (cid:107)·(cid:107) if for any u  v in its domain  and any x ∈ ∂f (u) 

f (v) ≥ f (u) + (cid:104)v − u  x(cid:105) +

(cid:107)u − v(cid:107)2 .

γ
2

The Fenchel conjugate f∗ of a γ-strongly convex function f is everywhere differentiable and 1
γ -
strongly smooth [8]  this means that for all u  v ∈ X 

(cid:107)u − v(cid:107)2∗ .
In the remainder of the paper all the norms considered will be the L2 ones.

f∗(v) ≤ f∗(u) + (cid:104)v − u ∇f∗(u)(cid:105) +

1
2γ

3 Dimension-Free Exponentiated Gradient

In this section I describe the DFEG algorithm. The pseudo-code is in Algorithm 1. It shares some
similarities with the exponentiated gradient with unnormalized weights algorithm [9]  to the self-
tuning variant of exponentiated gradient in [15]  and to the epoch-free algorithm in [19]. However 
note that it does not access to single coordinates of wt and xt  but only their inner products. Hence 
we expect the algorithm not to depend on the dimension of X  that can be even inﬁnite. In other
words  DFEG can be used with kernels as well  on contrary of all the mentioned algorithms above.
For the DFEG algorithm we have the following regret bound  that will be proved in Section 4.
Theorem 1. Let 0.882 ≤ a ≤ 1.109  δ > 0  then  for any sequence of input vectors {xt}T
sequence of L-Lipschitz convex losses {(cid:96)t(·)}T
holds for Algorithm 1

t=1  any
t=1  and any u ∈ X  the following bound on the regret

+ a(cid:107)u(cid:107)(cid:112)

HT

ln

H

(cid:16)

(cid:16)

T (cid:107)u(cid:107)(cid:17) − 1
(cid:17)

3
2

 

(cid:96)t((cid:104)wt  xt(cid:105)) − T(cid:88)

T(cid:88)
where HT = δ +(cid:80)T

t=1

t=1

(cid:96)t((cid:104)u  xt(cid:105)) ≤ 4 exp(1 + 1
a )

√
L

δ

t=1 L2 max((cid:107)xt(cid:107) (cid:107)xt(cid:107)2).

3

t

The bound has a logarithmic part  typical of the family of exponentiated gradient algorithms  but
instead of depending on the dimension  it depends on the norm of the competitor  (cid:107)u(cid:107). Hence  the
regret bound of DFEG holds for inﬁnite dimensional spaces as well  that is  it is dimension-free.
It is interesting to compare this bound with the usual bound for online learning using an L2 regular-
√
η (cid:107)w(cid:107)2 it is easy to see  e.g. [15]  that the bound
izer. Using a time-varying regularizer ft(w) =
√
would be3 O(((cid:107)u(cid:107)2/η + η)
T ). If an upper bound U on (cid:107)u(cid:107) is known  we can use it to tune η to
√
obtain an upper bound of the order of O(U
√
T ). On the other hand  we obtain for DFEG a bound
of O((cid:107)u(cid:107) log((cid:107)u(cid:107)T + 1)
T )  that is optimal bound  up to logarithmic terms  without knowing U.
So my bound goes to constant if the norm of the competitor goes to zero. However  note that  for
any ﬁxed competitor  the gradient descent bound is asymptotically better.
The lower bound on the range of a we get comes from technical details of the analysis. The param-
eter a is directly linked to the leading constant of the regret bound; therefore  it is intuitive that the
range of acceptable values must have a lower bound different from zero. This is also conﬁrmed by
the lower bound in Theorem 2 below.
Notice that the bound is data-dependent because it depends on the sequence of observed input vec-
tors xt. A data-independent bound can be easily obtained from the upper bound on the norm of the
input vectors. The use of the function max((cid:107)xt(cid:107) (cid:107)xt(cid:107)2) is necessary to have such a data-dependent
bound and it seems that it cannot be avoided in order to prove the regret bound.
It is natural to ask if the log term in the bound can be avoided. Extending Theorem 7 in [19]  we can
reply in the negative to this question. In particular  the following theorem shows that the regret of any
online learning algorithm has a satisfy to a trade-off between the guarantees against the competitor
with norm equal to zero and the ones against other competitors. A similar trade-off has been proven
in the expert settings [5].
Theorem 2. Fix a non-trivial vector space X   a speciﬁc online learning algorithm  and let the
sequence of losses be composed by linear losses. If the algorithm guarantees a zero regret against
the competitor with zero L2 norm  then there exists a sequence of T vectors in X   such that the
regret against any other competitor is Ω(T ). On the other hand  if the algorithm guarantees a regret
at most of  > 0 against the competitor with zero L2 norm  then  for any 0 < η < 1  there exists a
T0 and a sequence of T ≥ T0 unitary norm vectors zt ∈ X   and a vector u ∈ X such that

(cid:118)(cid:117)(cid:117)(cid:116)T log
(cid:114) 1
(cid:113) 1
The proof can be found in the supplementary material. It is possible to show that the optimal η is of
log 2 ≈ 1.2011 when T goes to inﬁnity.
the order of
It is also interesting to note that an L2 regularizer suffers a loss of O(
T ) against a competitor with
zero norm  that cancels the

log T   so that the leading constant approaches

(cid:104)u  zt(cid:105) − T(cid:88)

(cid:104)wt  zt(cid:105) ≥ (1 − η)(cid:107)u(cid:107)

η(cid:107)u(cid:107)√

T

T(cid:88)

t=1

(cid:32)

√

log 2

3

√

log T term.

(cid:33)

− 2 .

t=1

1

4 Analysis

In this section I prove my main result. I will ﬁrst brieﬂy introduce the general OMD algorithm with
time-varying regularizers on which my algorithm is based.

4.1 Online mirror descent and local smoothness

Algorithm 2 is a generic meta-algorithm for online learning. Most of the online learning algorithms
can be derived from it  choosing the functions ft and the vectors zt. The following lemma  that is
a generalization of Corollary 4 in [8]  Corollary 3 in [4]  and Lemma 1 in [12]  is the main tool to
prove the regret bound for the DFEG algorithm. The proof is in the supplementary material.
√
η (cid:107)w(cid:107)2

3Despite what claimed in Section 1 of [19]  the use of the time-varying regularizer ft(w) =

t

guarantees a sublinear regret for unconstrained online convex optimization  for any η > 0.

4

Algorithm 2 Time-varying Online Mirror Descent

Parameters: A sequence of convex functions f1  f2  . . . deﬁned on S ⊆ X.
Initialize: θ1 = 0 ∈ X
for t = 1  2  . . . do
Choose wt = ∇f∗
Observe zt ∈ X
Update θt+1 = θt + zt

t (θt)

end for

Lemma 1. Assume Algorithm 2 is run with functions f1  f2  . . . deﬁned on a common domain
S ⊆ X. Then for any w(cid:48)

t  u ∈ S we have

T(cid:88)

t=1

(cid:104)zt  u − w(cid:48)

t(cid:105) ≤ fT (u) +

t  zt(cid:105)(cid:1)  

t−1(θt) − (cid:104)w(cid:48)

(cid:0)f∗

T(cid:88)

t=1

t (θt+1) − f∗
if f∗

1   f∗

where we set f∗
0 (w(cid:48)
Moreover 
max0≤τ≤1 (cid:107)∇2f∗
t (θt + τ zt)(cid:107) ≤ λt  then we have
t (θt+1) − f∗
f∗

t−1(θt) − (cid:104)wt  zt(cid:105) ≤ f∗

1) = 0.

2   . . . are twice differentiable  and

t (θt) − f∗

t−1(θt) +

(cid:107)zt(cid:107)2 .

λt
2

t (θt) − f∗

Note that the above Lemma is usually stated assuming the strong convexity of ft  that is equivalent
to the strong smoothness of f∗
t   that in turns for twice differentiable functions is equivalent to a
global bound on the norm of the Hessian of f∗
t (see Theorem 2.1.6 in [11]). Here I take a different
route  assuming the functions f∗
t to be twice differentiable  but using the weaker hypothesis of local
boundedness of the Hessian of f∗
t . Hence  for twice differentiable conjugate functions  this bound is
always tighter than the ones in [4  8  12]. Indeed  in our case  the global strong smoothness cannot
be used to prove any meaningful regret bound.
We derive the Dimension-Free Exponentiated Gradient from the general OMD above. Set in Al-
gorithm 2 ft(w) = αt(cid:107)w(cid:107)(log(βt(cid:107)w(cid:107)) − 1)  where αt and βt are deﬁned in Algorithm 1  and
zt = −∂(cid:96)t((cid:104)wt  xt(cid:105))xt. The proof idea of my theorem is the following. First  assume that we
are on a round where we have a local upper bound on the norm of the Hessian f∗
√
t . The usual ap-
proach in these kind of proof is to have a regularizer that is growing over time as
t  so that the
terms f∗
√
√
t−1(θt) are negative and can be safely discarded. At the same time the sum of the
squared norms of the gradients will typically be of the order of O(
T )  giving us a O(
T ) regret
√
bound (see for example the proofs in [4]). However  following this approach in DFEG we would
have that the sum of norms of the squared gradients grows much faster than O(
T ). This is due to
the fact that the global strong smoothness is too small. Hence I introduce a different proof method.
In the following  I will show the surprising result that with my choice of the regularizers ft  the
terms f∗
t−1(θt) and the squared norm of the gradient cancel out. Notice that already in
[12  13] it has been advocated not to discard those terms to obtain tighter bounds. Here the same
terms play a major role in the proof and they are present thanks to the time-varying regularization.
This is in agreement with Theorem 9 in [19] that rules out algorithms with a ﬁxed regularizer to
obtain regret bounds like Theorem 1.
It remains to bound the regret in the rounds where we do not have an upper bound on the norm of
the Hessian. In these rounds I show that the norm of wt (and θt) is small enough so that the regret
is still bounded  thanks to the choice of βt.

t (θt) − f∗

4.2 Proof of the main result

We start deﬁning the new regularizer and show its properties in the following Lemma (proof
in the supplementary material). Note the similarities with EGU  where the regularizer is

(cid:80)d
i=1 wi(log(wi) − 1)  w ∈ Rd  wi ≥ 0 [9].

Lemma 2. Deﬁne f (w) = α(cid:107)w(cid:107)(ln(β(cid:107)w(cid:107)) − 1)  for α  β > 0. The following properties hold

• f∗(θ) = α

β exp

(cid:107)θ(cid:107)
α .

5

β(cid:107)θ(cid:107) exp

(cid:107)θ(cid:107)
α .

• ∇f∗(θ) = θ
• (cid:107)∇2f∗(θ)(cid:107)2 ≤

1

β min((cid:107)θ(cid:107) α) exp(

(cid:107)θ(cid:107)
α ).

√

T ) too.

√
T )  αt must be ˜O(

Equipped with a local upper bound on the Hessian of f∗  we can now use Lemma 1. We notice
that Lemma 1 also guides us in the choice of the sequences αt. In fact if we want the regret to be
˜O(
In the proof of Theorem 1 we also use the following three technical lemmas  whose proofs are in
the supplementary material. The ﬁrst two are used to upper bound the exponential function with
quadratic functions.
M 2+1 ≤ p ≤ exp(M )  and 0 ≤ x ≤ M  we have exp(x) ≤
Lemma 3. Let M > 0  then for any exp(M )
p + exp(M )−p
Lemma 4. Let M > 0  then for any 0 ≤ x ≤ M  we have exp(x) ≤ 1 + x + exp(M )−1−M
Lemma 5. For any p  q > 0 we have that 2√

x2 .

x2.

p − 2√

p+q ≥ q

M 2

M 2

.

(p+q)

3
2

Proof of Theorem 1. In the following denote by n(x) := max((cid:107)x(cid:107) (cid:107)x(cid:107)2). We will use Lemma 1
to upper bound the regret of DFEG. Hence  using the notation in Algorithm 1  set zt =
−∂(cid:96)t((cid:104)wt  xt(cid:105))xt  and ft(w) = αt(cid:107)w(cid:107)(log(βt(cid:107)w(cid:107)) − 1). Observe that  by the hypothesis on
(cid:96)t  we have (cid:107)zt(cid:107) ≤ L(cid:107)xt(cid:107). We ﬁrst consider two cases  based on the norm of θt.
Case 1: (cid:107)θt(cid:107) > αt + (cid:107)zt(cid:107).
With this assumption  and using the third property of Lemma 2  we have

(cid:16)(cid:107)θt+τ zt(cid:107)

(cid:17)

(cid:16)(cid:107)θt(cid:107)+(cid:107)zt(cid:107)

(cid:17)

exp

(cid:18)(cid:107)θt(cid:107)
(cid:19)
(cid:18)(cid:107)zt(cid:107)
(cid:19)
(cid:16)(cid:107)zt(cid:107)

αt
βt

αt

+

exp

αt

max
0≤τ≤1

(cid:107)∇2f∗

βt min((cid:107)θt + τ zt(cid:107)  αt)
We now use the second statement of Lemma 1. We have that λt(cid:107)zt(cid:107)2
upper bounded by
(cid:107)zt(cid:107)2
2αtβt

− αt−1
βt−1

t (θt + τ zt)(cid:107) ≤ max
0≤τ≤1
(cid:18)(cid:107)θt(cid:107) + (cid:107)zt(cid:107)
(cid:19)
(cid:18)(cid:107)θt(cid:107) + (cid:107)zt(cid:107)
≤ (cid:107)zt(cid:107)2
(cid:19)(cid:18)(cid:107)zt(cid:107)2
(cid:18)(cid:107)θt(cid:107)

(cid:19)
(cid:18)(cid:107)θt(cid:107)

(cid:19)

2αtβt

αt
βt

exp

exp

exp

αt

αt

αt

+

+

2

≤ exp
+ f∗

αt
βtαt
t (θt) − f∗
(cid:19)
(cid:18)(cid:107)θt(cid:107)

(cid:18)(cid:107)θt(cid:107)
(cid:19)

αt−1

exp

αt

− αt−1
βt−1

exp

(cid:19)

.

αt
− a
Ht−1

a
Ht

.

(1)

= exp

exp

+

αt

αt

exp

(cid:19)

a
Ht

2aH 2
t

(cid:18)(cid:107)zt(cid:107)

(cid:17) − 2a2Ht−1L2n(xt) − 2a2L4(n(xt))2

− a
Ht−1

(cid:107)zt(cid:107)2Ht−1 exp

We will now prove that the term in the parenthesis of (1) is negative. It can be rewritten as
(cid:107)zt(cid:107)2
2aH 2
t
and from the expression of αt we have that (cid:107)zt(cid:107)
≤ 1
M = 1/a. These are valid settings because exp( 1
a )
can be veriﬁed numerically.
a
Ht

a2 +1 ≤ 2a2 ≤ exp( 1

a  so we now use Lemma 3 with p = 2a2 and
a )  ∀ 0.825 ≤ a ≤ 1.109  as it

(cid:107)zt(cid:107)2
2aH 2
t

t Ht−1

2aH 2

+

=

αt

αt

1

 

t−1(θt) can be

(cid:18)(cid:107)zt(cid:107)
(cid:19)
(cid:16)

exp
αt
(cid:107)zt(cid:107)2Ht−1

(cid:16)

≤
≤ L2(cid:107)xt(cid:107)2Ht−1
≤ L4(cid:107)xt(cid:107)4(exp( 1
t Ht−1

2aH 2

− a
Ht−1
2a2 + a2(exp( 1

2a2 + a2(exp( 1

a ) − 4a2)

(cid:17) − 2a2Ht−1L2n(xt) − 2a2L4(n(xt))2
(cid:17) − 2a2Ht−1L2(cid:107)xt(cid:107)2 − 2a2L4(cid:107)xt(cid:107)2

a ) − 2a2)

(cid:107)zt(cid:107)2
α2
t
t Ht−1
2aH 2
a ) − 2a2) L2(cid:107)xt(cid:107)2
a2Ht
t Ht−1

2aH 2

≤ 0 

6

(2)

a ) ≤ 4a2 ∀ a ≥ 0.882  as again it can be veriﬁed

where in last step we used the fact that exp( 1
numerically.
Case 2: (cid:107)θt(cid:107) ≤ αt + (cid:107)zt(cid:107).
We use the ﬁrst statement of Lemma 1  setting w(cid:48)
way  from the second property of Lemma 2  we have that (cid:107)w(cid:48)
choice of w(cid:48)

t(cid:107) ≤ 1
(cid:18)(cid:107)θt(cid:107)
(cid:19)
t satisfying the the previous relation on the norm of w(cid:48)
(cid:19) exp
(cid:18)(cid:107)θt(cid:107)

(cid:18)(cid:107)θt+1(cid:107)
(cid:19)

(cid:18)(cid:107)zt(cid:107)

(cid:18)(cid:107)θt(cid:107)

t (θt+1) − f∗
f∗

− αt−1
βt−1

t−1(θt) =

t = wt if (cid:107)θ(cid:107) (cid:54)= 0  and w(cid:48)
(cid:107)θt(cid:107)
αt

(cid:16) (cid:107)zt(cid:107)

(cid:19)

(cid:19)

αt−1

αt
βt

exp(

exp

exp

αt

√

a

− αt−1
βt−1

αt

= a exp

αt

(cid:17)

t = 0 otherwise. In this
). Note that any other

βt
t would have worked as well.

Ht−1 − Ht

Ht
Ht−1Ht

.

(3)

≤ exp

αt

βt
Remembering that (cid:107)zt(cid:107)

Ht−1 exp

exp

≤ 1

(cid:19)

(cid:19)(cid:18) αt
(cid:18) (cid:107)zt(cid:107)
(cid:18)

Ht

αt

√
a
≤ Ht−1

=

1 +
LHt−1(cid:107)xt(cid:107)
≤ LHt−1(cid:107)xt(cid:107)

√
a
√
a

Ht

Ht

(cid:18)
(cid:19)
(cid:18) 1
(cid:18)

a  and using Lemma 4 with M = 1
a  we have
L(cid:107)xt(cid:107)
√
− Ht−1 − L2n(xt) ≤ Ht−1 exp
Ht
a
L(cid:107)xt(cid:107)
√
Ht
a

− 1 − 1
a

+ a2

exp

(cid:18)

a2Ht

(cid:19)

− Ht−1 − L2(cid:107)xt(cid:107)2

− Ht−1 − L2(cid:107)xt(cid:107)2

(cid:19)
(cid:19) L2(cid:107)xt(cid:107)2
(cid:19) L2Ht−1(cid:107)xt(cid:107)2
(cid:19)
(cid:19)
Ht
− 2 − 1
(4)
a
a ≤ 0 ∀ a ≥ 0.873  veriﬁed numerically.
a ) − 2 − 1

− L2(cid:107)xt(cid:107)2
≤ LHt−1(cid:107)xt(cid:107)

√
a

Ht

 

+

exp

a
+ L2(cid:107)xt(cid:107)2

exp

a
− 1 − 1
a

(cid:18) 1
(cid:18) 1
(cid:18)(cid:107)θt(cid:107)

a

αt

where in the last step we used the fact that exp( 1
Putting together (3) and (4)  we have

t (θt+1) − f∗
f∗

t  zt(cid:105) ≤ exp

t−1(θt) − (cid:104)w(cid:48)

(cid:18)(cid:107)θt(cid:107)
(cid:18)(cid:107)θt(cid:107)

(cid:19) L(cid:107)xt(cid:107)
(cid:19) L(cid:107)xt(cid:107)

αt

3
2
t

H

αt

3
H
2
t

≤ exp

= 2 exp

+ L(cid:107)w(cid:48)

t(cid:107)(cid:107)xt(cid:107) ≤ exp

a )L(cid:107)xt(cid:107)

≤ 2 exp(1 + 1
H

3
2
t

3
H
2
t

αt

 

(cid:19) L(cid:107)xt(cid:107)
(cid:18)(cid:107)θt(cid:107)

3
H
2
t

t  zt(cid:105)

− (cid:104)w(cid:48)

(cid:19) L(cid:107)xt(cid:107)

+ exp

(cid:18)(cid:107)θt(cid:107)

(cid:19) L(cid:107)xt(cid:107)

αt

βt

(5)

where in the second inequality we used the Cauchy-Schwarz inequality and the Lipschitzness of (cid:96)t 
t  and in the last inequality the fact that (cid:107)θt(cid:107) ≤ αt + (cid:107)zt(cid:107)
in the third the bound on the norm of w(cid:48)
implies exp(

a ). Putting together (2) and (5) and summing over t  we have

) ≤ exp(1 + 1

(cid:107)θt(cid:107)
αt

(cid:0)f∗

T(cid:88)

t=1

t (θt+1) − f∗
t−1(θt) − (cid:104)w(cid:48)
T(cid:88)

≤ 4 exp(1 + 1
a )



(cid:113)(cid:80)t−1

t  zt(cid:105)(cid:1) ≤ T(cid:88)

t=1

1

2 exp(1 + 1

a )L(cid:107)xt(cid:107)

≤ 2
L

3
2
t

1

H

(cid:113)(cid:80)t

−

a )L2(cid:107)xt(cid:107)
exp(1 + 1
j=1 L2(cid:107)xt(cid:107) + δ) 3

2

T(cid:88)
((cid:80)t
 ≤ 4 exp(1 + 1

t=1

a )

 

√
L

L

t=1

j=1 L2(cid:107)xt(cid:107) + δ
where in the third inequality we used Lemma 5.
The stated bound can be obtained observing that (cid:96)t((cid:104)wt  xt(cid:105)) − (cid:96)t((cid:104)u  xt(cid:105)) ≤ (cid:104)u − wt  zt(cid:105)  from
the convexity of (cid:96)t and the deﬁnition of zt.

j=1 L2(cid:107)xt(cid:107) + δ

δ

5 Experiments

A full empirical evaluation of DFEG is beyond the scope of this paper. Here I just want to show the
empirical effect of some of its theoretical properties. In all the experiments I used the absolute loss 

7

Figure 1: Left: regret versus number of input vectors on synthetic dataset. Center and Right: total loss for
DFEG and Kernel GD on the cadata and cpusmall dataset respectively.

so L = 1  a is set to the minimal value allowed by Theorem 1 and δ = 1. I denote by Kernel GD
the OMD with the regularizer

√
η (cid:107)w(cid:107)2.

t

First  I generated synthetic data as in the proof of Theorem 2  that is the input vectors are all the same
and the yt is equal to 1 for the t even and −1 for the others. In this case we know that the optimal
predictor has norm equal to zero and we can exactly calculate the value of the regret. Figure 1(left)
I have plotted the regret as a function of the number of input vectors. As predicted by the theory 
DFEG has a constant regret  while Kernel GD has a regret of the form O(η
T ). Hence  it can have
a constant regret only when η is set to zero  and this can be done only with prior knowledge of (cid:107)u(cid:107) 
that is impossible in practical applications.
For the second experiment  I analyzed the behavior of DFEG on two real word regression datasets 
cadata and cpusmall4. I used the Gaussian kernel with variance equal to the average distance be-
tween training input vectors. I have plotted in Figure 1(central) the ﬁnal cumulative loss of DFEG
and the ones of GD with varying values of η. We see that  while the performance of Kernel GD can
be better of the one of DFEG  as predicted by the theory  its performance varies greatly in relation
to η. On the other hand the performance of DFEG is close to the optimal one without the need to
tune any parameters. It is also worth noting the catastrophic result we can get from a wrong tuning
of η in GD. Similar considerations hold for the cpusmall dataset in Figure 1(right).

√

6 Discussion

√

√

I have presented a new algorithm for online learning  the ﬁrst one in the family of exponentiated
gradient to be dimension-free. Thanks to new analysis tools  I have proved that DFEG attains a
regret bound of O(U log(U T + 1))
T )  without any parameter to tune. I also proved a lower
bound that shows that the algorithm is optimal up to
The problem of deriving a regret bound that depends on the sequence of the gradients  rather than
t=1 (cid:96)t((cid:104)wt  xt(cid:105)))
on the xt  remains open. Resolving this issue would result in the tighter O(
regret bounds in the case that the (cid:96)t are smooth [18]. The difﬁculty in proving these kind of bounds
seem to lie in the fact that (2) is negative only because Ht − Ht−1 is bigger than (cid:107)zt(cid:107)2.

log T term for linear and Lipschitz losses.

(cid:113)(cid:80)T

Acknowledgments

I am thankful to Jennifer Batt for her help and support during the writing of this paper  to Nicol`o
Cesa-Bianchi for the useful comments on an early version of this work  and to Tamir Hazan for his
writing style suggestions. I also thank the anonymous reviewers for their precise comments  which
helped me to improve the clarity of this manuscript.

4http://www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/

8

020004000600080001000002468101214161820Numer of samplesRegretSynthetic dataset DFEGKernel GD  eta=0.05Kernel GD  eta=0.1Kernel GD  eta=0.210−210−11001011021.51.551.61.651.71.751.81.851.91.952x 104etaTotal losscadata dataset DFEGKernel GD  various eta10−11001011022000400060008000100001200014000etaTotal losscpusmall dataset DFEGKernel GD  various etaReferences
[1] J. Abernethy  A. Agarwal  P. L. Bartlett  and A. Rakhlin. A stochastic view of optimal regret

through minimax duality. In COLT  2009.

[2] P. Auer  N. Cesa-Bianchi  and C. Gentile. Adaptive and self-conﬁdent on-line learning algo-

rithms. J. Comput. Syst. Sci.  64(1):48–75  2002.

[3] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[4] J. C. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[5] E. Even-Dar  M. Kearns  Y. Mansour  and J. Wortman. Regret to the best vs. regret to the
average. In N. H. Bshouty and C. Gentile  editors  COLT  volume 4539 of Lecture Notes in
Computer Science  pages 233–247. Springer  2007.

[6] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the Perceptron algorithm.

Machine Learning  pages 277–296  1999.

[7] C. Gentile. The robustness of the p-norm algorithms. Machine Learning  53(3):265–299  2003.
[8] S. M. Kakade  S. Shalev-Shwartz  and A. Tewari. Regularization techniques for learning with

matrices. CoRR  abs/0910.0610  2009.

[9] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predic-

tors. Information and Computation  132(1):1–63  January 1997.

[10] N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold

algorithm. Machine Learning  2(4):285–318  1988.

[11] Y. Nesterov.

Introductory lectures on convex optimization: A basic course  volume 87.

Springer  2003.

[12] F. Orabona and K. Crammer. New adaptive algorithms for online classiﬁcation. In J. Lafferty 
C. K. I. Williams  J. Shawe-Taylor  R.S. Zemel  and A. Culotta  editors  Advances in Neural
Information Processing Systems 23  pages 1840–1848. 2010.

[13] F. Orabona  K. Crammer  and N. Cesa-Bianchi. A generalized online mirror descent with

applications to classiﬁcation and regression  2013. arXiv:1304.2994.

[14] R. T. Rockafellar. Convex Analysis (Princeton Mathematical Series). Princeton University

Press  1970.

[15] S. Shalev-Shwartz. Online learning: Theory  algorithms  and applications. Technical report 

The Hebrew University  2007. PhD thesis.

[16] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends

in Machine Learning  4(2)  2012.

[17] S. Shalev-Shwartz and Y. Singer. A primal-dual perspective of online learning algorithms.

Machine Learning Journal  2007.

[18] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates. In J. Lafferty 
C. K. I. Williams  J. Shawe-Taylor  R. S. Zemel  and A. Culotta  editors  Advances in Neural
Information Processing Systems 23  pages 2199–2207. 2010.

[19] M. Streeter and B. McMahan. No-regret algorithms for unconstrained online convex optimiza-
tion. In P. Bartlett  F.C.N. Pereira  C.J.C. Burges  L. Bottou  and K.Q. Weinberger  editors 
Advances in Neural Information Processing Systems 25  pages 2411–2419. 2012.

[20] V. Vovk. On-line regression competitive with reproducing kernel hilbert spaces.

In Jin-Yi
Cai  S.Barry Cooper  and Angsheng Li  editors  Theory and Applications of Models of Com-
putation  volume 3959 of Lecture Notes in Computer Science  pages 452–463. Springer Berlin
Heidelberg  2006.

[21] V. G. Vovk. Aggregating strategies. In COLT  pages 371–386  1990.

9

,Francesco Orabona
Michael Andersen
Ole Winther
Lars Hansen
Corinna Cortes
Giulia DeSalvo
Mehryar Mohri
Bo-Jian Hou
Lijun Zhang
Zhi-Hua Zhou
Horia Mania
Aurelia Guy
Benjamin Recht
Rui Li