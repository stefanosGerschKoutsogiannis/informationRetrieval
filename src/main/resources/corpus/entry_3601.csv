2019,Multi-source Domain Adaptation for Semantic Segmentation,Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting  which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper  we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically  we design a novel framework  termed Multi-source Adversarial Domain Aggregation Network (MADAN)  which can be trained in an end-to-end manner. First  we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second  we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally  feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches.  Our source code is released at: https://github.com/Luodian/MADAN.,Multi-source Domain Adaptation for

Semantic Segmentation

Sicheng Zhao1∗†  Bo Li23†  Xiangyu Yue1†  Yang Gu2 
Pengfei Xu2  Runbo Hu2  Hua Chai2  Kurt Keutzer1

1University of California  Berkeley  USA 2Didi Chuxing  China

schzhao@gmail.com  drluodian@gmail.com  {xyyue keutzer}@berkeley.edu

{guyangdavid xupengfeipf hurunbo chaihua}@didiglobal.com

3Harbin Institute of Technology  China

Abstract

Simulation-to-real domain adaptation for semantic segmentation has been actively
studied for various applications such as autonomous driving. Existing methods
mainly focus on a single-source setting  which cannot easily handle a more practical
scenario of multiple sources with different distributions. In this paper  we propose
to investigate multi-source domain adaptation for semantic segmentation. Specif-
ically  we design a novel framework  termed Multi-source Adversarial Domain
Aggregation Network (MADAN)  which can be trained in an end-to-end manner.
First  we generate an adapted domain for each source with dynamic semantic
consistency while aligning at the pixel-level cycle-consistently towards the target.
Second  we propose sub-domain aggregation discriminator and cross-domain cycle
discriminator to make different adapted domains more closely aggregated. Finally 
feature-level alignment is performed between the aggregated domain and target
domain while training the segmentation network. Extensive experiments from
synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate
that the proposed MADAN model outperforms state-of-the-art approaches. Our
source code is released at: https://github.com/Luodian/MADAN.

1

Introduction

Semantic segmentation assigns a semantic label (e.g. car  cyclist  pedestrian  road) to each pixel
in an image. This computer vision kernel plays a crucial role in many applications  ranging from
autonomous driving [1] and robotic control [2] to medical imaging [3] and fashion recommenda-
tion [4]. With the advent of deep learning  especially convolutional neural networks (CNNs)  several
end-to-end approaches have been proposed for semantic segmentation [5  6  7  8  9  10  11  12  13  14].
Although these methods have achieved promising results  they suffer from some limitations. On the
one hand  training these methods requires large-scale labeled data with pixel-level annotations  which
is prohibitively expensive and time-consuming to obtain. For example  it takes about 90 minutes to
label each image in the Cityscapes dataset [15]. On the other hand  they cannot well generalize their
learned knowledge to new domains  because of the presence of domain shift or dataset bias [16  17].
To sidestep the cost of data collection and annotation  unlimited amounts of synthetic labeled data can
be created from simulators like CARLA and GTA-V [18  19  20]  thanks to the progress in graphics
and simulation infrastructure. To mitigate the gap between different domains  domain adaptation
(DA) or knowledge transfer techniques have been proposed [21] with both theoretical analysis [22 

∗Corresponding Author.
†Equal Contribution.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

23  24  25] and algorithm design [26  27  28  29  30  31  32]. Besides the traditional task loss on the
labeled source domain  deep unsupervised domain adaptation (UDA) methods are generally trained
with another loss to deal with domain shift  such as a discrepancy loss [31  33  34  35]  adversarial
loss [36  37  38  39  37  40  41  42  43  32  44]  reconstruction loss [30  45  46]  etc. Current
simulation-to-real DA methods for semantic segmentation [47  48  49  50  51  52  32  53  54  55  56]
all focus on the single-source setting and do not consider a more practical scenario where the labeled
data are collected from multiple sources with different distributions. Simply combining different
sources into one source and directly employing single-source DA may not perform well  since images
from different source domains may interfere with each other during the learning process [57].
Early efforts on multi-source DA (MDA) used shallow models [58  59  60  61  62  63  64  65  66  67].
Recently  some multi-source deep UDA methods have been proposed which only focus on image
classiﬁcation [68  69  70]. Directly extending these MDA methods from classiﬁcation to segmentation
may not perform well due to the following reasons. (1) Segmentation is a structured prediction
task  the decision function of which is more involved than classiﬁcation because it has to resolve the
predictions in an exponentially large label space [48  71]. (2) Current MDA methods mainly focus on
feature-level alignment  which only aligns high-level information. This may be enough for coarse-
grained classiﬁcation tasks  but it is obviously insufﬁcient for ﬁne-grained semantic segmentation 
which performs pixel-wise prediction. (3) These MDA methods only align each source and target pair.
Although different sources are matched towards the target  there may exist signiﬁcant mis-alignment
across different sources.
To address the above challenges  in this paper we propose a novel framework  termed Multi-source
Adversarial Domain Aggregation Network (MADAN)  which consists of Dynamic Adversarial Image
Generation  Adversarial Domain Aggregation  and Feature-aligned Semantic Segmentation. First  for
each source  we generate an adapted domain using a Generative Adversarial Network (GAN) [36]
with cycle-consistency loss [39]  which enforces pixel-level alignment between source images and
target images. To preserve the semantics before and after image translation  we propose a novel
semantic consistency loss by minimizing the KL divergence between the source predictions of a
pretrained segmentation model and the adapted predictions of a dynamic segmentation model. Second 
instead of training a classiﬁer for each source domain [68  70]  we propose sub-domain aggregation
discriminator to directly make different adapted domains indistinguishable  and cross-domain cycle
discriminator to discriminate between the images from each source and the images transferred from
other sources. In this way  different adapted domains can be better aggregated into a more uniﬁed
domain. Finally  the segmentation model is trained based on the aggregated domain  while enforcing
feature-level alignment between the aggregated domain and the target domain.
In summary  our contributions are three-fold. (1) We propose to perform domain adaptation for
semantic segmentation from multiple sources. To the best of our knowledge  this is the ﬁrst work
on multi-source structured domain adaptation. (2) We design a novel framework termed MADAN
to do MDA for semantic segmentation. Besides feature-level alignment  pixel-level alignment is
further considered by generating an adapted domain for each source cycle-consistently with a novel
dynamic semantic consistency loss. Sub-domain aggregation discriminator and cross-domain cycle
discriminator are proposed to better align different adapted domains. (3) We conduct extensive
experiments from synthetic GTA [18] and SYNTHIA [19] to real Cityscapes [15] and BDDS [72]
datasets  and the results demonstrate the effectiveness of our proposed MADAN model.

2 Problem Setup

We consider the unsupervised MDA scenario  in which there are multiple labeled source domains
S1  S2 ···   SM   where M is number of sources  and one unlabeled target domain T . In the ith source
i}Ni
i}Ni
domain Si  suppose Xi = {xj
j=1 and Yi = {yj
j=1 are the observed data and corresponding
labels drawn from the source distribution pi(x  y)  where Ni is the number of samples in Si. In
the target domain T   let XT = {xj
T}NT
j=1 denote the target data drawn from the target distribution
pT (x  y) without label observation  where NT is the number of target samples. Unless otherwise
T ∈ Rd  indicating that the
speciﬁed  we have two assumptions: (1) homogeneity  i.e. xj
data from different domains are observed in the same image space but with different distributions;
T ∈ Y  where Y is the label set  which means that all the domains
(2) closed set  i.e. yj
share the same space of classes. Based on covariate shift and concept drift [21]  we aim to learn an

i ∈ Rd  xj

i ∈ Y  yj

2

Figure 1: The framework of the proposed Multi-source Adversarial Domain Aggregation Network
(MADAN). The colored solid arrows represent generators  while the black solid arrows indicate the
segmentation network F . The dashed arrows correspond to different losses.

adaptation model that can correctly predict the labels of a sample from the target domain trained on
{(Xi  Yi)}M

i=1 and {XT}.

3 Multi-source Adversarial Domain Aggregation Network

In this section  we introduce the proposed Multi-source Adversarial Domain Aggregation Network
(MADAN) for semantic segmentation adaptation. The framework is illustrated in Figure 1  which
consists of three components: Dynamic Adversarial Image Generation (DAIG)  Adversarial Domain
Aggregation (ADA)  and Feature-aligned Semantic Segmentation (FSS). DAIG aims to generate
adapted images from source domains to the target domain from the perspective of visual appearance
while preserving the semantic information with a dynamic segmentation model. In order to reduce
the distances among the adapted domains and thus generate a more aggregated uniﬁed domain 
ADA is proposed  including Cross-domain Cycle Discriminator (CCD) and Sub-domain Aggregation
Discriminator (SAD). Finally  FSS learns the domain-invariant representations at the feature-level in
an adversarial manner. Table 1 compares MADAN with several state-of-the-art DA methods.

3.1 Dynamic Adversarial Image Generation

The goal of DAIG is to make images from different source domains visually similar to the target
images  as if they are drawn from the same target domain distribution. To this end  for each source
domain Si  we introduce a generator GSi→T mapping to the target T in order to generate adapted im-
ages that fool DT   which is a pixel-level adversarial discriminator. DT is trained simultaneously with
each GSi→T to classify real target images XT from adapted images GSi→T (Xi). The corresponding
GAN loss function is:
LSi→T
GAN (GSi→T   DT   Xi  XT ) = Exi∼Xi log DT (GSi→T (xi)) + ExT ∼XT log[1 − DT (xT )]. (1)

Since the mapping GSi→T is highly under-constrained [36]  we employ an inverse mapping GT→Si as
well as a cycle-consistency loss [39] to enforce GT→Si (GSi→T (xi)) ≈ x and vice versa. Similarly 
we introduce Di to classify Xi from GT→Si (XT )  with the following GAN loss:

LT→Si
GAN (GT→Si  Di  XT   Xi) = Exi∼Xi log[1 − Di(xi)] + Ext∼XT log Di(GT→Si(xt)).

(2)

3

Semantic consistency lossReconstruction lossReconstruction lossSemantic consistency loss𝐹𝐹Feature‐level GAN lossPixel‐level GAN lossTasklossSourceiimageAdapted imagesSource jimageReconstructed source jimageReconstructed source iimageTarget imageSource LabelSource PredictionTarget predictionTest phaseSemantic SegmentationSub‐domain aggregation lossCross‐domain cycle lossCross‐domain cycle lossTransferred image from jto iTransferred image from ito j:    Sub‐domain aggregation lossAdversarial Domain Aggregation:    Cross‐domain cycle Loss :   Task lossFeature-aligned Semantic Segmentation:   Feature‐level GAN loss:    Reconstruction lossDynamic Adversarial Image Generation:    Semantic consistency loss:    Pixel‐level GAN lossTable 1: Comparison of the proposed MADAN model with several state-of-the-art domain adaptation
methods. The full names of each property from the second to the last columns are pixel-level
alignment  feature-level alignment  semantic consistency  cycle consistency  multiple sources  domain
aggregation  one task network  and ﬁne-grained prediction  respectively.
aggr

pixel

ADDA [25]

CycleGAN [39]
PiexlDA [37]
SBADA [41]
GTA-GAN [42]
DupGAN [43]
CyCADA [32]

DCTN [68]
MDAN [69]
MMN [70]

MADAN (ours)













feat












sem cycle multi
–






–
–
–


–






–
–
–














one












ﬁne












–
–
–
–
–
–
–





The cycle-consistency loss [39] ensures that the learned mappings GSi→T and GT→Si are cycle-
consistent  thereby preventing them from contradicting each other  is deﬁned as:

LSi↔T

cyc

(GSi→T   GT→Si  Xi  XT ) =Exi∼Xi (cid:107) GT→Si(GSi→T (xi)) − xi (cid:107)1 +
ExT ∼XT (cid:107) GSi→T (GT→Si(xt)) − xt (cid:107)1 .

(3)

The adapted images are expected to contain the same semantic information as original source images 
but the semantic consistency is only partially constrained by the cycle consistency loss. The semantic
consistency loss in CyCADA [32] was proposed to better preserve semantic information. xi and
GSi→T (xi) are both fed into a segmentation model Fi pretrained on (Xi  Yi). However  since xi and
GSi→T (xi) are from different domains  employing the same segmentation model  i.e. Fi  to obtain
the segmentation results and then computing the semantic consistency loss may be detrimental to
image generation. Ideally  the adapted images GSi→T (xi) should be fed into a network FT trained
on the target domain  which is infeasible since target domain labels are not available in UDA. Instead
of employing Fi on GSi→T (xi)  we propose to dynamically update the network FA  which takes
GSi→T (xi) as input  so that its optimal input domain (the domain that the network performs best
on) gradually changes from that of Fi to FT . We employ the task segmentation model F trained on
the adapted domain as FA  i.e. FA = F   which has two advantages: (1) GSi→T (xi) becomes the
optimal input domain of FA  and as F is trained to have better performance on the target domain  the
semantic loss after FA would promote GSi→T to generate images that are closer to target domain at
the pixel-level; (2) since FA and F can share the parameters  no additional training or memory space
is introduced  which is quite efﬁcient. The proposed dynamic semantic consistency (DSC) loss is:

sem(GSi→T   Xi  Fi  FA) = Exi∼XiKL(FA(GSi→T (xi))||Fi(xi)) 
LSi

where KL(·||·) is the KL divergence between two distributions.

(4)

3.2 Adversarial Domain Aggregation

We can train different segmentation models for each adapted domain and combine different predictions
with speciﬁc weights for target images [68  70]  or we can simply combine all adapted domains
together and train one model [69]. In the ﬁrst strategy  it is challenging to determine how to select
the weights for different adapted domains. Moreover  each target image needs to be fed into all
segmentation models at reference time  and this is rather inefﬁcient. For the second strategy  since
the alignment space is high-dimensional  although the adapted domains are relatively aligned with
the target  they may be signiﬁcantly mis-aligned with each other. In order to mitigate this issue  we
propose adversarial domain aggregation to make different adapted domains more closely aggregated
with two kinds of discriminators. One is the sub-domain aggregation discriminator (SAD)  which is
designed to directly make the different adapted domains indistinguishable. For Si  a discriminator
A is introduced with the following loss function:
Di
LSi
SAD(GS1→T   . . . GSi→T   . . .   GSM→T  Di

A(GSi→T (xi))+

Exj∼Xj log[1 − Di

A(GSj→T (xj))].

(5)

(cid:88)

A) = Exi∼Xi log Di
1
M − 1

j(cid:54)=i

4

The other is the cross-domain cycle discriminator (CCD). For each source domain Si  we transfer the
images from the adapted domains GSj→T (Xj)  j = 1 ···   M  j (cid:54)= i back to Si using GT→Si and
employ the discriminator Di to classify Xi from GT→Si (GSj→T (Xj))  which corresponds to the
following loss function:

LSi
CCD(GT→S1   . . . GT→Si−1 GT→Si+1  . . .   GT→SM   GSi→T   Di) = Exi∼Xi log Di(xi)+
Exj∼Xj log[1 − Di(GT→Si((GSj→T (xj)))].

1

(cid:88)

(6)

M − 1

j(cid:54)=i

Please note that using a more sophisticated combination of different discriminators’ losses to better
aggregate the domains with larger distances might improve the performance. We leave this as future
work and would explore this direction by dynamic weighting of the loss terms and incorporating
some prior domain knowledge of the sources.

3.3 Feature-aligned Semantic Segmentation
i(i = 1 ···   M )
After adversarial domain aggregation  the adapted images of different domains X(cid:48)
are more closely aggregated and aligned. Meanwhile  the semantic consistency loss in dynamic
adversarial image generation ensures that the semantic information  i.e. the segmentation labels  is
preserved before and after image translation. Suppose the images of the uniﬁed aggregated domain
are X(cid:48) =
model F based on X(cid:48) and Y with the following cross-entropy loss:

Yi. We can then train a task segmentation

i and corresponding labels are Y =

M(cid:83)

X(cid:48)

i=1

i=1

(cid:48)

 Y )

l=1

h=1

Ltask(F  X(cid:48)  Y ) = −E

1[l=yh w] log(σ(Fl h w(x(cid:48)))) 

(x(cid:48) y)∼(X

(7)
where L is the number of classes  H  W are the height and width of the adapted images  σ is the
softmax function  1 is an indicator function  and Fl h w(x(cid:48)) is the value of F (x(cid:48)) at index (l  h  w).
Further  we impose a feature-level alignment between X(cid:48) and XT   which can improve the segmenta-
tion performance during inference of XT on the segmentation model F . We introduce a discriminator
DF to achieve this goal. The feature-level GAN loss is deﬁned as:

Lf eat(Ff   DFf   X(cid:48)  XT ) = Ex(cid:48)∼X(cid:48) log DFf (Ff (x(cid:48))) + ExT ∼XT log[1 − DFf (Ff (xT ))] 
where Ff (·) is the output of the last convolution layer (i.e. a feature map) of the encoder in F .

(8)

w=1

M(cid:83)
(cid:88)W

(cid:88)L

(cid:88)H

3.4 MADAN Learning

The proposed MADAN learning framework utilizes adaptation techniques including pixel-level
alignment  cycle-consistency  semantic consistency  domain aggregation  and feature-level alignment.
Combining all these components  the overall objective loss function of MADAN is:
A ··· DM

LM ADAN (GS1→T ··· GSM→T   GT→S1 ··· GT→SM   D1 ··· DM   D1

A   DFf   F )

(cid:88)

(cid:104)LSi→T

i

cyc

GAN (GT→Si  Di  XT   Xi)

GAN (GSi→T   DT   Xi  XT ) + LT→Si
(GSi→T   GT→Si   Xi  XT ) + LSi

=
+ LSi↔T
+ LSi
+ LSi
+ Ltask(F  X(cid:48)  Y ) + Lf eat(Ff   DFf   X(cid:48)  XT ).

SAD(GS1→T   . . . GSi→T   . . .   GSM→T   Di
CCD(GT→S1   . . . GT→Si−1  GT→Si+1  . . .   GT→SM   GSi→T   Di)

sem(GSi→T   Xi  Fi  F )

A)

(9)

(cid:105)

The training process corresponds to solving for a target model F according to the optimization:

F ∗ = arg min

min
D

max

G

F

LM ADAN (G  D  F ) 

(10)

where G and D represent all the generators and discriminators in Eq. (9)  respectively.

4 Experiments

In this section  we ﬁrst introduce the experimental settings and then compare the segmentation results
of the proposed MADAN and several state-of-the-art approaches both quantitatively and qualitatively 
followed by some ablation studies.

5

Table 2: Comparison with the state-of-the-art DA methods for semantic segmentation from GTA
and SYNTHIA to Cityscapes. The best class-wise IoU and mIoU trained on the source domains are
emphasized in bold (similar below).

Standards

Method

d
a
o
r

k
l
a
w
e
d
i
s

19.6
14.5
19.0
32.4
22.0
31.2
29.8
37.2
26.7
19.6
26.1
30.0
29.6
30.4
35.8
19.7
37.7
74.5

g
n
i
d
l
i
u
b

47.4
45.0
60.1
62.1
71.7
78.6
78.6
76.5
77.4
30.8
74.9
77.5
65.3
70.8
78.2
63.8
79.1
87.1

l
l
a
w
3.3
0.7
11.1
14.9
6.0
27.9
21.1
21.8
23.7
4.4
0.1
9.6
0.5
1.6
17.5
13.1
20.1
35.3

e
c
n
e
f

5.2
0.0
13.7
5.4
11.9
22.2
18.2
15.0
20.5
0.0
0.5
0.3
0.2
0.6
15.1
19.4
17.8
37.8

e
l
o
p

3.3
14.6
10.1
10.9
8.4
21.9
22.5
23.8
20.4
20.3
10.7
25.8
15.1
22.3
10.8
5.5
15.5
36.4

t
h
g
i
l
-
t

0.5
0.7
5.0
14.2
16.3
23.7
21.5
22.9
30.3
0.1
3.7
10.3
4.5
6.7
6.1
5.2
14.5
46.9

n
g
i
s
-
t

3.0
2.6
4.7
2.7
11.1
11.4
11.0
21.5
15.9
11.7
3.0
15.6
6.9
23.0
19.4
6.8
21.4
60.1

n
o
i
t
t
e
g
e
v

69.2
68.2
74.7
79.2
75.7
80.7
79.7
80.5
80.9
42.3
76.1
77.6
67.1
76.9
78.6
71.6
78.5
89.0

n
o
s
r
e
p

31.3
31.5
40.8
44.1
38.0
48.5
46.8
50.5
52.6
51.2
47.1
44.5
42.8
41.9
44.5
42.0
49.7
65.6

r
e
d
i
r

0.1
4.6
2.3
4.2
9.3
14.1
6.5
9.0
11.1
3.8
8.2
16.6
14.1
16.7
15.3
12.0
16.8
35.9

y
k
s

43.0
68.4
65.3
64.6
66.5
68.9
71.3
60.7
69.5
68.7
70.6
79.8
68.2
73.9
77.2
61.1
73.4
89.8

r
a
c

59.3
31.5
43.0
70.4
55.2
78.0
80.1
76.9
79.6
54.0
43.2
67.8
51.2
61.7
74.9
62.7
77.8
76.9

s
u
b

8.3
7.4
15.9
7.3
18.9
23.8
26.9
28.2
21.2
3.2
20.7
14.5
12.6
11.5
17.0
2.9
28.3
64.1

e
k
i
b
-
m
0.2
0.3
1.3
3.5
16.8
8.3
10.6
4.5
17.0
0.2
0.7
7.0
2.4
10.3
10.3
12.3
17.7
40.5

e
l
c
y
c
i
b

0.0
1.4
1.4
0.0
14.6
0.0
0.3
0.0
6.7
0.6
13.1
23.8
20.7
38.6
12.9
8.1
27.5
65.1

U
o
I
m
21.7
18.5
25.8
27.1
28.9
39.0
38.3
38.7
39.8
20.2
29.0
36.2
29.2
35.4
37.3
29.4
41.4
62.6

54.1
GTA
SYNTHIA
3.9
GTA+SYNTHIA 44.0
70.4
FCN Wld [47]
74.8
CDA [48]
85.4
ROAD [50]
87.3
AdaptSeg [71]
CyCADA [32]
85.2
82.3
DCAN [55]
11.5
FCN Wld [47]
65.2
CDA [48]
77.7
ROAD [50]
66.2
CyCADA [32]
79.9
DCAN [55]
82.8
64.2
86.2
96.4

MDAN [69]
MADAN (Ours)
FCN [5]

Source-only

GTA-only DA

SYNTHIA-only DA

Source-combined DA CyCADA [32]

Multi-source DA
Oracle-Train on Tgt

Table 3: Comparison with the state-of-the-art DA methods for semantic segmentation from GTA and
SYNTHIA to BDDS. The best class-wise IoU and mIoU are emphasized in bold.

Standards

Method

d
a
o
r

Source-only

GTA-only DA

50.2
GTA
SYNTHIA
7.0
GTA+SYNTHIA 54.5
77.9
CyCADA [32]
55
SYNTHIA-only DA
CyCADA [32]
61.5
Source-combined DA CyCADA [32]
35.9
60.2
91.7

MDAN [69]
MADAN (Ours)
FCN [5]

Multi-source DA
Oracle-Train on Tgt

k
l
a
w
e
d
i
s

18.0
6.0
19.6
26.8
13.8
27.6
15.8
29.5
54.7

g
n
i
d
l
i
u
b

55.1
50.5
64.0
68.8
45.2
72.1
56.9
66.6
79.5

l
l
a
w
3.1
0.0
3.2
13.0
0.1
6.5
5.8
16.9
25.9

e
c
n
e
f

7.8
0.0
3.6
19.7
0.0
2.8
16.3
10.0
42.0

e
l
o
p

7.0
15.1
5.2
13.5
13.2
15.7
9.5
16.6
23.6

t
h
g
i
l
-
t

0.0
0.2
0.0
18.2
0.5
10.8
8.6
10.9
30.9

n
g
i
s
-
t

3.5
2.4
0.0
22.3
10.6
18.1
6.2
16.4
34.6

n
o
i
t
t
e
g
e
v

61.0
60.3
61.3
64.2
63.3
78.3
59.1
78.8
81.2

n
o
s
r
e
p

19.2
16.5
13.9
39.0
22.0
44.9
24.5
47.5
49.6

r
e
d
i
r

0.0
0.5
0.0
22.6
6.9
16.3
9.9
17.3
23.5

y
k
s

50.4
85.6
82.2
84.2
67.4
73.8
80.1
75.1
91.6

r
a
c

58.1
36.7
55.5
72.0
52.5
41.5
53.8
48.0
85.4

s
u
b

3.2
3.3
16.7
11.5
10.5
21.1
11.8
24.0
64.2

e
k
i
b
-
m
19.8
0.0
13.4
15.9
10.4
21.8
2.9
13.2
28.4

e
l
c
y
c
i
b

0.0
3.5
0.0
2.0
13.3
25.9
1.6
17.3
41.1

U
o
I
m
22.3
17.1
24.6
35.7
24.0
33.7
25.0
36.3
53.0

4.1 Experimental Settings

(1) Source-only  i.e.

Datasets. In our adaptation experiments  we use synthetic GTA [18] and SYNTHIA [19] datasets as
the source domains and real Cityscapes [15] and BDDS [72] datasets as the target domains.
Baselines. We compare MADAN with the following methods.
train on
the source domains and test on the target domain directly. We can view this as a lower bound
of DA. (2) Single-source DA  perform multi-source DA via single-source DA  including FCNs
Wld [47]  CDA [48]  ROAD [50]  AdaptSeg [71]  CyCADA [32]  and DCAN [55]. (3) Multi-source
DA  extend some single-source DA method to multi-source settings  including MDAN [69]. For
comparison  we also report the results of an oracle setting  where the segmentation model is both
trained and tested on the target domain. For the source-only and single-source DA standards  we
employ two strategies: (1) single-source  i.e. performing adaptation on each single source; (2)
source-combined  i.e. all source domains are combined into a traditional single source. For MDAN 
we extend the original classiﬁcation network for our segmentation task.
Evaluation Metric. Following [47  48  32  56]  we employ mean intersection-over-union (mIoU) to
evaluate the segmentation results. In the experiments  we take the 16 intersection classes of GTA and
SYNTHIA  compatible with Cityscapes and BDDS  for all mIoU evaluations.
Implementation Details. Although MADAN could be trained in an end-to-end manner  due to
constrained hardware resources  we train it in three stages. First  we train two CycleGANs (9 residual
blocks for generator and 4 convolution layers for discriminator) [39] without semantic consistency
loss  and then train an FCN F on the adapted images with corresponding labels from the source
domains. Second  after updating FA with F trained above  we generate adapted images using
CycleGAN with the proposed DSC loss in Eq. (4) and aggregate different adapted domains using
SAD and CCD. Finally  we train an FCN on the newly adapted images in the aggregated domain with
feature-level alignment. The above stages are trained iteratively.
We choose to use FCN [5] as our semantic segmentation network  and  as the VGG family of networks
is commonly used in reporting DA results  we use VGG-16 [73] as the FCN backbone. The weights

6

Figure 2: Qualitative semantic segmentation result from GTA and SYNTHIA to Cityscapes. From
left to right are: (a) original image  (b) ground truth annotation  (c) source only from GTA  (d)
CycleGANs on GTA and SYNTHIA  (e) +CCD+DSC  (f) +SAD+DSC  (g) +CCD+SAD+DSC  and
(h) +CCD+SAD+DSC+Feat (MADAN).

Figure 3: Visualization of image translation. From left to right are: (a) original source image 
(b) CycleGAN  (c) CycleGAN+DSC  (d) CycleGAN+CCD+DSC  (e) CycleGAN+SAD+DSC  (f)
CycleGAN+CCD+SAD+DSC  and (g) target Cityscapes image. The top two rows and bottom rows
are GTA → Cityscapes and SYNTHIA → Cityscapes  respectively.

of the feature extraction layers in the networks are initialized from models trained on ImageNet [74].
The network is implemented in PyTorch and trained with Adam optimizer [75] using a batch size of
8 with initial learning rate 1e-4. All the images are resized to 600 × 1080  and are then cropped to
400 × 400 during the training of the pixel-level adaptation for 20 epochs. SAD and CCD are frozen
in the ﬁrst 5 and 10 epochs  respectively.

4.2 Comparison with State-of-the-art

The performance comparisons between the proposed MADAN model and the other baselines  includ-
ing source-only  single-source DA  and multi-source DA  as measured by class-wise IoU and mIoU
are shown in Table 2 and Table 3. From the results  we have the following observations:
(1) The source-only method that directly transfers the segmentation models trained on the source
domains to the target domain obtains the worst performance in most adaptation settings. This is
obvious  because the joint probability distributions of observed images and labels are signiﬁcantly
different among the sources and the target  due to the presence of domain shift. Without domain
adaptation  the direct transfer cannot well handle this domain gap. Simply combining different source
domains performs better than each single source  which indicates the superiority of multiple sources
over single source despite the domain shift among different sources.
(2) Comparing source-only with single-source DA respectively on GTA and SYNTHIA  it is clear that
all adaptation methods perform better  which demonstrates the effectiveness of domain adaptation in
semantic segmentation. Comparing the results of CyCADA in single-source and source-combined

7

(a)(b)(c)(d)(e)(f)(g)(h)(a)(b)(c)(d)(e)(f)(g)(h)(a)(b)(c)(d)(e)(f)(g)Table 4: Comparison between the proposed dynamic semantic consistency (DSC) loss in MADAN
and the original SC loss in [32] on Cityscapes. The better mIoU for each pair is emphasized in bold.

Source

Method

d
a
o
r

GTA

SYNTHIA

85.6
CycleGAN+SC
76.6
CycleGAN+DSC
85.2
CyCADA w/ SC
84.1
CyCADA w/ DSC
CycleGAN+SC
64.0
CycleGAN + DSC 68.4
66.2
CyCADA w/ SC
CyCADA w/ DSC
69.8

k
l
a
w
e
d
i
s

30.7
26.0
37.2
27.3
29.4
29.0
29.6
27.2

g
n
i
d
l
i
u
b

74.7
76.3
76.5
78.3
61.7
65.2
65.3
68.5

l
l
a
w
14.4
17.3
21.8
21.6
0.3
0.6
0.5
5.8

e
c
n
e
f

13.0
18.8
15.0
18.0
0.1
0.0
0.2
0.0

e
l
o
p

17.6
13.6
23.8
13.8
15.3
15.0
15.1
11.6

t
h
g
i
l
-
t

13.7
13.2
21.5
14.1
3.4
0.1
4.5
0.0

n
g
i
s
-
t

5.8
17.9
22.9
16.7
5.0
4.0
6.9
2.8

n
o
i
t
t
e
g
e
v

74.6
78.8
80.5
78.1
63.4
75.1
67.1
75.7

n
o
s
r
e
p

38.2
47.4
50.5
47.8
39.4
45.0
42.8
44.3

r
e
d
i
r

3.5
14.8
9.0
15.4
11.5
11.0
14.1
10.5

y
k
s

69.9
63.9
60.7
66.9
68.4
70.6
68.2
58.3

r
a
c

72.3
72.2
76.9
78.7
46.6
54.9
51.2
68.1

s
u
b

5.0
24.1
28.2
23.4
10.4
18.2
12.6
22.1

e
k
i
b
-
m
3.6
19.8
9.8
22.3
2.0
3.9
2.4
11.8

e
l
c
y
c
i
b

0.0
10.8
0.0
14.4
16.4
26.7
20.7
32.7

U
o
I
m
32.7
38.1
38.7
40.0
27.3
30.5
29.2
31.8

Table 5: Comparison between the proposed dynamic semantic consistency (DSC) loss in MADAN
and the original SC loss in [32] on BDDS. The better mIoU for each pair is emphasized in bold.

Source

Method

d
a
o
r

GTA

SYNTHIA

62.1
CycleGAN+SC
74.4
CycleGAN+DSC
68.8
CyCADA w/ SC
70.5
CyCADA w/ DSC
CycleGAN+SC
50.6
CycleGAN + DSC 57.3
49.5
CyCADA w/ SC
CyCADA w/ DSC
55

k
l
a
w
e
d
i
s

20.9
23.7
23.7
32.4
13.6
13.4
11.1
13.8

g
n
i
d
l
i
u
b

59.2
65.0
67.0
68.2
50.5
56.1
46.6
45.2

l
l
a
w
6.0
8.6
7.5
10.5
0.2
2.7
0.7
0.1

e
c
n
e
f

23.5
17.2
16.2
17.3
0.0
14.1
0.0
0.0

e
l
o
p

12.8
10.7
9.4
18.4
7.9
9.8
10.0
13.2

t
h
g
i
l
-
t

9.2
14.2
11.3
16.6
0.0
7.7
0.4
0.5

n
g
i
s
-
t

22.4
19.7
22.2
21.8
0.0
17.1
7.0
10.6

n
o
i
t
t
e
g
e
v

65.9
59.0
60.5
65.6
63.8
65.5
61.0
63.3

n
o
s
r
e
p

34.7
36.3
36.1
38.1
21.6
11.4
17.5
22.0

r
e
d
i
r

11.4
19.6
20.6
16.1
7.8
1.4
7.2
6.9

y
k
s

78.4
82.8
82.1
82.2
58.3
53.1
74.6
67.4

r
a
c

64.4
69.7
63.2
73.3
50.2
51.4
50.9
52.5

s
u
b

14.2
4.3
15.2
20.8
1.8
13.9
5.8
10.5

e
k
i
b
-
m
10.9
17.6
16.6
12.6
2.2
3.9
13.1
10.4

e
l
c
y
c
i
b

1.9
4.2
3.4
3.7
19.9
8.7
4.3
13.3

U
o
I
m
31.1
32.9
32.0
35.5
21.8
22.5
23.4
24.0

settings  we can conclude that simply combining different source domains and performing single-
source DA may result in performance degradation.
(3) MADAN achieves the highest mIoU score among all adaptation methods  and beneﬁts from the
joint consideration of pixel-level and feature-level alignments  cycle-consistency  dynamic semantic-
consistency  domain aggregation  and multiple sources. MADAN also signiﬁcantly outperforms
source-combined DA  in which domain shift also exists among different sources. By bridging this gap 
multi-source DA can boost the adaptation performance. On the one hand  compared to single-source
DA [47  48  50  71  32  55]  MADAN utilizes more useful information from multiple sources. On the
other hand  other multi-source DA methods [68  69  70] only consider feature-level alignment  which
may be enough for course-grained tasks  e.g. image classiﬁcation  but is obviously insufﬁcient for
ﬁne-grained tasks  e.g. semantic segmentation  a pixel-wise prediction task. In addition  we consider
pixel-level alignment with a dynamic semantic consistency loss and further aggregate different
adapted domains.
(4) The oracle method that is trained on the target domain performs signiﬁcantly better than the
others. However  to train this model  the ground truth segmentation labels from the target domain are
required  which are actually unavailable in UDA settings. We can deem this performance as a upper
bound of UDA. Obviously  a large performance gap still exists between all adaptation algorithms and
the oracle method  requiring further efforts on DA.
Visualization. The qualitative semantic segmentation results are shown in Figure 2. We can clearly
see that after adaptation by the proposed method  the visual segmentation results are improved notably.
We also visualize the results of pixel-level alignment from GTA and SYNTHIA to Cityscapes in
Figure 3. We can see that with our ﬁnal proposed pixel-level alignment method (f)  the styles of the
images are close to Cityscapes while the semantic information is well preserved.

4.3 Ablation Study

First  we compare the proposed dynamic semantic consistency (DSC) loss in MADAN with the
original semantic consistency (SC) loss in CyCADA [32]. As shown in Table 4 and Table 5  we can
see that for all simulation to real adaptations  DSC achieves better results. After demonstrating its
value  we employ the DSC loss in subsequent experiments.
Second  we incrementally investigate the effectiveness of different components in MADAN on both
Cityscapes and BDDS. The results are shown in Table 6 and Table 7. We can observe that: (1)
both domain aggregation methods  i.e. SAD and CCD  can obtain better performance by making
different adapted domains more closely aggregated  while SAD outperforms CCD; (2) adding the

8

Table 6: Ablation study on different components in MADAN on Cityscapes. Baseline denotes
using piexl-level alignment with cycle-consistency  +SAD denotes using the sub-domain aggregation
discriminator  +CCD denotes using the cross-domain cycle discriminator  +DSC denotes using the
dynamic semantic consistency loss  and +Feat denotes using feature-level alignment.

Method

Baseline
+SAD
+CCD
+SAD+CCD
+SAD+DSC
+CCD+DSC
+SAD+CCD+DSC
+SAD+CCD+DSC+Feat

k
l
a
w
e
d
i
s

27.6
33.2
36.3
35.3
36.6
36.9
35.1
37.7

g
n
i
d
l
i
u
b

67.5
75.9
69.8
76.5
78.0
78.6
78.7
79.1

d
a
o
r

74.9
79.7
82.1
82.7
83.1
86.8
84.2
86.2

l
l
a
w
9.1
11.8
9.5
15.4
23.3
16.2
17.1
20.1

e
c
n
e
f

10.0
3.6
4.9
19.4
12.6
8.1
18.7
17.8

e
l
o
p

12.8
15.9
11.8
14.1
11.8
17.7
15.4
15.5

t
h
g
i
l
-
t

1.4
8.6
12.5
7.2
3.5
8.9
15.7
14.5

n
g
i
s
-
t

13.6
15.0
15.3
13.9
11.3
13.7
24.1
21.4

n
o
i
t
t
e
g
e
v

63.0
74.7
61.3
75.3
75.5
75.0
77.9
78.5

n
o
s
r
e
p

41.7
44.2
49.7
50.9
42.2
42.2
49.2
49.7

r
e
d
i
r

13.5
17.1
10.0
19.0
17.9
18.2
17.1
16.8

y
k
s

47.1
78.9
54.1
74.2
74.8
74.8
72.0
73.4

r
a
c

60.8
68.2
70.7
66.5
72.2
74.6
75.2
77.8

s
u
b

22.4
24.9
9.7
26.6
27.2
22.5
24.1
28.3

e
k
i
b
-
m
6.0
16.7
19.7
16.3
13.8
22.9
18.9
17.7

Table 7: Ablation study on different components in MADAN on BDDS.

Method

Baseline
+SAD
+CCD
+SAD+CCD
+SAD+DSC
+CCD+DSC
+SAD+CCD+DSC
+SAD+CCD+DSC+Feat

k
l
a
w
e
d
i
s

17.4
18.7
13.6
20.2
29.5
27.6
38.0
36.3

g
n
i
d
l
i
u
b

55.4
61.8
63.0
61.7
66.6
72.1
75.8
77.9

d
a
o
r

31.3
58.9
52.7
61.6
60.2
61.5
64.6
69.1

l
l
a
w
2.6
6.4
6.6
7.2
16.9
6.5
17.8
21.5

e
c
n
e
f

12.9
10.7
11.2
12.1
10.0
12.8
13.0
17.4

e
l
o
p

12.4
17.1
17.8
18.5
16.6
15.7
9.8
13.8

t
h
g
i
l
-
t

6.5
20.3
21.5
19.8
10.9
10.8
5.9
4.1

n
g
i
s
-
t

18.0
17.0
18.9
16.7
16.4
18.1
4.6
16.2

n
o
i
t
t
e
g
e
v

63.2
67.3
67.4
64.2
78.8
78.3
74.8
76.5

n
o
s
r
e
p

21.2
21.1
9.2
25.9
47.5
44.9
41.8
42.2

r
e
d
i
r

5.6
6.7
2.2
7.3
17.3
16.3
24.0
16.4

y
k
s

79.9
83.7
84.0
83.2
75.1
73.8
76.9
76.2

r
a
c

44.1
66.6
63.0
66.8
48.0
41.5
69.0
56.3

s
u
b

14.2
22.7
21.6
22.2
24.0
21.1
20.4
22.4

e
k
i
b
-
m
6.1
4.5
2.0
5.3
13.2
21.8
23.7
24.5

e
l
c
y
c
i
b

8.1
14.0
12.4
6.7
10.0
12.7
19.2
27.5

e
l
c
y
c
i
b

11.7
14.9
14.0
14.9
17.3
15.9
11.3
13.5

U
o
I
m
30.0
36.4
33.1
37.5
37.1
38.1
40.2
41.4

U
o
I
m
24.6
31.2
29.3
31.8
34.3
33.7
35.3
36.3

DSC loss could further improve the mIoU score  again demonstrating the effectiveness of DSC; (3)
feature-level alignments also contribute to the adaptation task; (4) the modules are orthogonal to each
other to some extent  since adding each one of them does not introduce performance degradation.

4.4 Discussions

Computation cost. Since the proposed framework deals with a harder problem  i.e. multi-source
domain adaptation  more modules are used to align different sources  which results in a larger model.
In our experiments  MADAN is trained on 4 NVIDIA Tesla P40 GPUs for 40 hours using two source
domains which is about twice the training time as on a single source. However  MADAN does not
introduce any additional computation during inference  which is the biggest concern in real industrial
applications  e.g. autonomous driving.
On the poorly performing classes. There are two main reasons for the poor performance on certain
classes (e.g. fence and pole): 1) lack of images containing these classes and 2) structural differences
of objects between simulation images and real images (e.g. the trees in simulation images are much
taller than those in real images). Generating more images for different classes and improving the
diversity of objects in the simulation environment are two promising directions for us to explore in
future work that may help with these problems.

5 Conclusion

In this paper  we studied multi-source domain adaptation for semantic segmentation from synthetic
data to real data. A novel framework  termed Multi-source Adversarial Domain Aggregation Network
(MADAN)  is designed with three components. For each source domain  we generated adapted images
with a novel dynamic semantic consistency loss. Further we proposed a sub-domain aggregation
discriminator and cross-domain cycle discriminator to better aggregate different adapted domains.
Together with other techniques such as pixel- and feature-level alignments as well as cycle-consistency 
MADAN achieves 15.6%  1.6%  4.1%  and 12.0% mIoU improvements compared with best source-
only  best single-source DA  source-combined DA  and other multi-source DA  respectively on
Cityscapes from GTA and SYNTHIA  and 11.7%  0.6%  2.6%  11.3% on BDDS. For further studies 
we plan to investigate multi-modal DA  such as using both image and LiDAR data  to better boost the
adaptation performance. Improving the computational efﬁciency of MADAN  with techniques such
as neural architecture search  is another direction worth investigating.

9

Acknowledgments

This work is supported by Berkeley DeepDrive and the National Natural Science Foundation of China
(No. 61701273).

References
[1] Andreas Geiger  Philip Lenz  and Raquel Urtasun. Are we ready for autonomous driving? the
kitti vision benchmark suite. In IEEE Conference on Computer Vision and Pattern Recognition 
pages 3354–3361  2012.

[2] Zhang-Wei Hong  Yu-Ming Chen  Hsuan-Kung Yang  Shih-Yang Su  Tzu-Yun Shann  Yi-
Hsiang Chang  Brian Hsi-Lin Ho  Chih-Chieh Tu  Tsu-Ching Hsiao  Hsin-Wei Hsiao  et al.
Virtual-to-real: learning to control in visual semantic segmentation. In International Joint
Conference on Artiﬁcial Intelligence  pages 4912–4920  2018.

[3] Özgün Çiçek  Ahmed Abdulkadir  Soeren S Lienkamp  Thomas Brox  and Olaf Ronneberger.
3d u-net: learning dense volumetric segmentation from sparse annotation. In International
Conference on Medical Image Computing and Computer Assisted Intervention  pages 424–432 
2016.

[4] Shatha Jaradat. Deep cross-domain fashion recommendation. In ACM Conference on Recom-

mender Systems  pages 407–410  2017.

[5] Jonathan Long  Evan Shelhamer  and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In IEEE Conference on Computer Vision and Pattern Recognition  pages 3431–
3440  2015.

[6] Ziwei Liu  Xiaoxiao Li  Ping Luo  Chen-Change Loy  and Xiaoou Tang. Semantic image
segmentation via deep parsing network. In IEEE International Conference on Computer Vision 
pages 1377–1385  2015.

[7] Shuai Zheng  Sadeep Jayasumana  Bernardino Romera-Paredes  Vibhav Vineet  Zhizhong Su 
Dalong Du  Chang Huang  and Philip HS Torr. Conditional random ﬁelds as recurrent neural
networks. In IEEE International Conference on Computer Vision  pages 1529–1537  2015.

[8] Guosheng Lin  Chunhua Shen  Anton Van Den Hengel  and Ian Reid. Efﬁcient piecewise training
of deep structured models for semantic segmentation. In IEEE Conference on Computer Vision
and Pattern Recognition  pages 3194–3203  2016.

[9] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In

International Conference on Learning Representations  2016.

[10] Vijay Badrinarayanan  Alex Kendall  and Roberto Cipolla. Segnet: A deep convolutional
encoder-decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis
and Machine Intelligence  39(12):2481–2495  2017.

[11] Hengshuang Zhao  Jianping Shi  Xiaojuan Qi  Xiaogang Wang  and Jiaya Jia. Pyramid scene
parsing network. In IEEE Conference on Computer Vision and Pattern Recognition  pages
2881–2890  2017.

[12] Liang-Chieh Chen  George Papandreou  Iasonas Kokkinos  Kevin Murphy  and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets  atrous convolution 
and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence 
40(4):834–848  2017.

[13] Panqu Wang  Pengfei Chen  Ye Yuan  Ding Liu  Zehua Huang  Xiaodi Hou  and Garrison
Cottrell. Understanding convolution for semantic segmentation. In IEEE Winter Conference on
Applications of Computer Vision  pages 1451–1460  2018.

[14] Bolei Zhou  Hang Zhao  Xavier Puig  Tete Xiao  Sanja Fidler  Adela Barriuso  and Antonio
Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal
of Computer Vision  127(3):302–321  2019.

10

[15] Marius Cordts  Mohamed Omran  Sebastian Ramos  Timo Rehfeld  Markus Enzweiler  Rodrigo
Benenson  Uwe Franke  Stefan Roth  and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In IEEE Conference on Computer Vision and Pattern Recognition 
pages 3213–3223  2016.

[16] Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In IEEE Conference on

Computer Vision and Pattern Recognition  pages 1521–1528  2011.

[17] Bichen Wu  Xuanyu Zhou  Sicheng Zhao  Xiangyu Yue  and Kurt Keutzer. Squeezesegv2:
Improved model structure and unsupervised domain adaptation for road-object segmentation
from a lidar point cloud. In IEEE International Conference on Robotics and Automation  pages
4376–4382  2019.

[18] Stephan R Richter  Vibhav Vineet  Stefan Roth  and Vladlen Koltun. Playing for data: Ground
truth from computer games. In European Conference on Computer Vision  pages 102–118 
2016.

[19] German Ros  Laura Sellart  Joanna Materzynska  David Vazquez  and Antonio M Lopez. The
synthia dataset: A large collection of synthetic images for semantic segmentation of urban
scenes. In IEEE Conference on Computer Vision and Pattern Recognition  pages 3234–3243 
2016.

[20] Xiangyu Yue  Bichen Wu  Sanjit A Seshia  Kurt Keutzer  and Alberto L Sangiovanni-Vincentelli.
A lidar point cloud generator: from a virtual world to autonomous driving. In ACM International
Conference on Multimedia Retrieval  pages 458–464  2018.

[21] Vishal M Patel  Raghuraman Gopalan  Ruonan Li  and Rama Chellappa. Visual domain
adaptation: A survey of recent advances. IEEE Signal Processing Magazine  32(3):53–69 
2015.

[22] Shai Ben-David  John Blitzer  Koby Crammer  Alex Kulesza  Fernando Pereira  and Jen-
nifer Wortman Vaughan. A theory of learning from different domains. Machine learning 
79(1-2):151–175  2010.

[23] Raghuraman Gopalan  Ruonan Li  and Rama Chellappa. Unsupervised adaptation across
domain shifts by generating intermediate data representations. IEEE Transactions on Pattern
Analysis and Machine Intelligence  36(11):2288–2302  2014.

[24] Christos Louizos  Kevin Swersky  Yujia Li  Max Welling  and Richard Zemel. The variational

fair autoencoder. arXiv:1511.00830  2015.

[25] Eric Tzeng  Judy Hoffman  Kate Saenko  and Trevor Darrell. Adversarial discriminative domain
adaptation. In IEEE Conference on Computer Vision and Pattern Recognition  pages 2962–2971 
2017.

[26] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning.

Knowledge and Data Engineering  22(10):1345–1359  2010.

IEEE Transactions on

[27] Xavier Glorot  Antoine Bordes  and Yoshua Bengio. Domain adaptation for large-scale sentiment
classiﬁcation: A deep learning approach. In International Conference on Machine Learning 
pages 513–520  2011.

[28] I-Hong Jhuo  Dong Liu  DT Lee  and Shih-Fu Chang. Robust visual domain adaptation with
low-rank reconstruction. In IEEE Conference on Computer Vision and Pattern Recognition 
pages 2168–2175  2012.

[29] Carlos J Becker  Christos M Christoudias  and Pascal Fua. Non-linear domain adaptation with

boosting. In Advances in Neural Information Processing Systems  pages 485–493  2013.

[30] Muhammad Ghifary  W Bastiaan Kleijn  Mengjie Zhang  and David Balduzzi. Domain general-
ization for object recognition with multi-task autoencoders. In IEEE International Conference
on Computer Vision  pages 2551–2559  2015.

11

[31] Mingsheng Long  Yue Cao  Jianmin Wang  and Michael Jordan. Learning transferable features
In International Conference on Machine Learning  pages

with deep adaptation networks.
97–105  2015.

[32] Judy Hoffman  Eric Tzeng  Taesung Park  Jun-Yan Zhu  Phillip Isola  Kate Saenko  Alexei A
Efros  and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In Interna-
tional Conference on Machine Learning  pages 1994–2003  2018.

[33] Baochen Sun  Jiashi Feng  and Kate Saenko. Return of frustratingly easy domain adaptation. In

AAAI Conference on Artiﬁcial Intelligence  pages 2058–2065  2016.

[34] Baochen Sun  Jiashi Feng  and Kate Saenko. Correlation alignment for unsupervised domain

adaptation. In Domain Adaptation in Computer Vision Applications  pages 153–171. 2017.

[35] Junbao Zhuo  Shuhui Wang  Weigang Zhang  and Qingming Huang. Deep unsupervised
convolutional domain adaptation. In ACM International Conference on Multimedia  pages
261–269  2017.

[36] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems  pages 2672–2680  2014.

[37] Konstantinos Bousmalis  Nathan Silberman  David Dohan  Dumitru Erhan  and Dilip Krishnan.
Unsupervised pixel-level domain adaptation with generative adversarial networks. In IEEE
Conference on Computer Vision and Pattern Recognition  pages 3722–3731  2017.

[38] Ming-Yu Liu and Oncel Tuzel. Coupled generative adversarial networks. In Advances in Neural

Information Processing Systems  pages 469–477  2016.

[39] Jun-Yan Zhu  Taesung Park  Phillip Isola  and Alexei A Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. In IEEE International Conference on
Computer Vision  pages 2223–2232  2017.

[40] Sicheng Zhao  Xin Zhao  Guiguang Ding  and Kurt Keutzer. Emotiongan: unsupervised
domain adaptation for learning discrete probability distributions of image emotions. In ACM
International Conference on Multimedia  pages 1319–1327  2018.

[41] Paolo Russo  Fabio M Carlucci  Tatiana Tommasi  and Barbara Caputo. From source to target
and back: symmetric bi-directional adaptive gan. In IEEE Conference on Computer Vision and
Pattern Recognition  pages 8099–8108  2018.

[42] Swami Sankaranarayanan  Yogesh Balaji  Carlos D Castillo  and Rama Chellappa. Generate
to adapt: Aligning domains using generative adversarial networks. In IEEE Conference on
Computer Vision and Pattern Recognition  pages 8503–8512  2018.

[43] Lanqing Hu  Meina Kan  Shiguang Shan  and Xilin Chen. Duplex generative adversarial
network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and
Pattern Recognition  pages 1498–1507  2018.

[44] Sicheng Zhao  Chuang Lin  Pengfei Xu  Sendong Zhao  Yuchen Guo  Ravi Krishna  Guiguang
Ding  and Kurt Keutzer. Cycleemotiongan: Emotional semantic consistency preserved cyclegan
for adapting image emotions. In AAAI Conference on Artiﬁcial Intelligence  pages 2620–2627 
2019.

[45] Muhammad Ghifary  W Bastiaan Kleijn  Mengjie Zhang  David Balduzzi  and Wen Li. Deep
reconstruction-classiﬁcation networks for unsupervised domain adaptation. In European Con-
ference on Computer Vision  pages 597–613  2016.

[46] Konstantinos Bousmalis  George Trigeorgis  Nathan Silberman  Dilip Krishnan  and Dumitru
Erhan. Domain separation networks. In Advances in Neural Information Processing Systems 
pages 343–351  2016.

[47] Judy Hoffman  Dequan Wang  Fisher Yu  and Trevor Darrell. Fcns in the wild: Pixel-level

adversarial and constraint-based adaptation. arXiv:1612.02649  2016.

12

[48] Yang Zhang  Philip David  and Boqing Gong. Curriculum domain adaptation for semantic
segmentation of urban scenes. In IEEE International Conference on Computer Vision  pages
2020–2030  2017.

[49] Xingchao Peng  Ben Usman  Neela Kaushik  Judy Hoffman  Dequan Wang  and Kate Saenko.

Visda: The visual domain adaptation challenge. arXiv:1710.06924  2017.

[50] Yuhua Chen  Wen Li  and Luc Van Gool. Road: Reality oriented adaptation for semantic
segmentation of urban scenes. In IEEE Conference on Computer Vision and Pattern Recognition 
pages 7892–7901  2018.

[51] Swami Sankaranarayanan  Yogesh Balaji  Arpit Jain  Ser Nam Lim  and Rama Chellappa.
Learning from synthetic data: Addressing domain shift for semantic segmentation. In IEEE
Conference on Computer Vision and Pattern Recognition  pages 3752–3761  2018.

[52] Yiheng Zhang  Zhaofan Qiu  Ting Yao  Dong Liu  and Tao Mei. Fully convolutional adaptation
networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern
Recognition  pages 6810–6818  2018.

[53] Aysegul Dundar  Ming-Yu Liu  Ting-Chun Wang  John Zedlewski  and Jan Kautz. Do-
main stylization: A strong  simple baseline for synthetic to real image domain adaptation.
arXiv:1807.09384  2018.

[54] Xinge Zhu  Hui Zhou  Ceyuan Yang  Jianping Shi  and Dahua Lin. Penalizing top performers:
Conservative loss for semantic segmentation adaptation. In European Conference on Computer
Vision  pages 568–583  2018.

[55] Zuxuan Wu  Xintong Han  Yen-Liang Lin  Mustafa Gokhan Uzunbas  Tom Goldstein  Ser
Nam Lim  and Larry S Davis. Dcan: Dual channel-wise alignment networks for unsupervised
scene adaptation. In European Conference on Computer Vision  pages 518–534  2018.

[56] Xiangyu Yue  Yang Zhang  Sicheng Zhao  Alberto Sangiovanni-Vincentelli  Kurt Keutzer  and
Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real general-
ization without accessing target domain data. In IEEE International Conference on Computer
Vision  2019.

[57] Matthew Riemer  Ignacio Cases  Robert Ajemian  Miao Liu  Irina Rish  Yuhai Tu  and Ger-
ald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing
interference. In International Conference on Learning Representations  2019.

[58] Shiliang Sun  Honglei Shi  and Yuanbin Wu. A survey of multi-source domain adaptation.

Information Fusion  24:84–92  2015.

[59] Lixin Duan  Ivor W Tsang  Dong Xu  and Tat-Seng Chua. Domain adaptation from multiple
sources via auxiliary classiﬁers. In International Conference on Machine Learning  pages
289–296  2009.

[60] Qian Sun  Rita Chattopadhyay  Sethuraman Panchanathan  and Jieping Ye. A two-stage
weighting framework for multi-source domain adaptation. In Advances in Neural Information
Processing Systems  pages 505–513  2011.

[61] Lixin Duan  Dong Xu  and Shih-Fu Chang. Exploiting web images for event recognition in
consumer videos: A multiple source domain adaptation approach. In IEEE Conference on
Computer Vision and Pattern Recognition  pages 1338–1345  2012.

[62] Rita Chattopadhyay  Qian Sun  Wei Fan  Ian Davidson  Sethuraman Panchanathan  and Jieping
Ye. Multisource domain adaptation and its application to early detection of fatigue. ACM
Transactions on Knowledge Discovery from Data  6(4):18  2012.

[63] Lixin Duan  Dong Xu  and Ivor Wai-Hung Tsang. Domain adaptation from multiple sources:
A domain-dependent regularization approach. IEEE Transactions on Neural Networks and
Learning Systems  23(3):504–518  2012.

13

[64] Jun Yang  Rong Yan  and Alexander G Hauptmann. Cross-domain video concept detection
using adaptive svms. In ACM International Conference on Multimedia  pages 188–197  2007.

[65] Gabriele Schweikert  Gunnar Rätsch  Christian Widmer  and Bernhard Schölkopf. An empirical
analysis of domain adaptation algorithms for genomic sequence analysis. In Advances in Neural
Information Processing Systems  pages 1433–1440  2009.

[66] Zhijie Xu and Shiliang Sun. Multi-source transfer learning with multi-view adaboost. In

International Conference on Neural information processing  pages 332–339  2012.

[67] Shi-Liang Sun and Hong-Lei Shi. Bayesian multi-source domain adaptation. In International

Conference on Machine Learning and Cybernetics  volume 1  pages 24–28  2013.

[68] Ruijia Xu  Ziliang Chen  Wangmeng Zuo  Junjie Yan  and Liang Lin. Deep cocktail network:
Multi-source unsupervised domain adaptation with category shift. In IEEE Conference on
Computer Vision and Pattern Recognition  pages 3964–3973  2018.

[69] Han Zhao  Shanghang Zhang  Guanhang Wu  José MF Moura  Joao P Costeira  and Geoffrey J
Gordon. Adversarial multiple source domain adaptation. In Advances in Neural Information
Processing Systems  pages 8568–8579  2018.

[70] Xingchao Peng  Qinxun Bai  Xide Xia  Zijun Huang  Kate Saenko  and Bo Wang. Moment

matching for multi-source domain adaptation. arXiv:1812.01754  2018.

[71] Yi-Hsuan Tsai  Wei-Chih Hung  Samuel Schulter  Kihyuk Sohn  Ming-Hsuan Yang  and
Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation.
In IEEE Conference on Computer Vision and Pattern Recognition  pages 7472–7481  2018.

[72] Fisher Yu  Wenqi Xian  Yingying Chen  Fangchen Liu  Mike Liao  Vashisht Madhavan  and
Trevor Darrell. Bdd100k: A diverse driving video database with scalable annotation tooling.
arXiv:1805.04687  2018.

[73] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. In International Conference on Learning Representations  2015.

[74] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition 
pages 248–255  2009.

[75] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Interna-

tional Conference on Learning Representations  2015.

14

,Sicheng Zhao
Bo Li
Xiangyu Yue
Yang Gu
Pengfei Xu
Runbo Hu
Hua Chai
Kurt Keutzer