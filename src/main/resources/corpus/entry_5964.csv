2010,Worst-Case Linear Discriminant Analysis,Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper  we first analyze the scatter measures used in the conventional linear discriminant analysis~(LDA) model and note that the formulation is based on the average-case view. Based on this analysis  we then propose a new dimensionality reduction method called worst-case linear discriminant analysis~(WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large  we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise  we take a greedy approach by finding one direction of the transformation at a time. Moreover  we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.,Worst-Case Linear Discriminant Analysis

Yu Zhang and Dit-Yan Yeung

Department of Computer Science and Engineering
Hong Kong University of Science and Technology

zhangyu dyyeung@cse.ust.hk

Abstract

Dimensionality reduction is often needed in many applications due to the high
dimensionality of the data involved.
In this paper  we ﬁrst analyze the scatter
measures used in the conventional linear discriminant analysis (LDA) model and
note that the formulation is based on the average-case view. Based on this analysis 
we then propose a new dimensionality reduction method called worst-case linear
discriminant analysis (WLDA) by deﬁning new between-class and within-class
scatter measures. This new model adopts the worst-case view which arguably is
more suitable for applications such as classiﬁcation. When the number of training
data points or the number of features is not very large  we relax the optimiza-
tion problem involved and formulate it as a metric learning problem. Otherwise 
we take a greedy approach by ﬁnding one direction of the transformation at a
time. Moreover  we also analyze a special case of WLDA to show its relationship
with conventional LDA. Experiments conducted on several benchmark datasets
demonstrate the effectiveness of WLDA when compared with some related di-
mensionality reduction methods.

1

Introduction

With the development of advanced data collection techniques  large quantities of high-dimensional
data are commonly available in many applications. While high-dimensional data can bring us more
information  processing and storing such data poses many challenges. From the machine learning
perspective  we need a very large number of training data points to learn an accurate model due
to the so-called ‘curse of dimensionality’. To alleviate these problems  one common approach is
to perform dimensionality reduction on the data. An assumption underlying many dimensionality
reduction techniques is that the most useful information in many high-dimensional datasets resides
in a low-dimensional latent space. Principal component analysis (PCA) [8] and linear discriminant
analysis (LDA) [7] are two classical dimensionality reduction methods that are still widely used in
many applications. PCA  as an unsupervised linear dimensionality reduction method  ﬁnds a low-
dimensional subspace that preserves as much of the data variance as possible. On the other hand 
LDA is a supervised linear dimensionality reduction method which seeks to ﬁnd a low-dimensional
subspace that keeps data points from different classes far apart and those from the same class as
close as possible.
The focus of this paper is on the supervised dimensionality reduction setting like that for LDA. To set
the stage  we ﬁrst analyze the between-class and within-class scatter measures used in conventional
LDA. We then establish that conventional LDA seeks to maximize the average pairwise distance
between class means and minimize the average within-class pairwise distance over all classes. Note
that if the purpose of applying LDA is to increase the accuracy of the subsequent classiﬁcation task 
then it is desirable for every pairwise distance between two class means to be as large as possible and
every within-class pairwise distance to be as small as possible  but not just the average distances.
To put this thinking into practice  we incorporate a worst-case view to deﬁne a new between-class

1

scatter measure as the minimum of the pairwise distances between class means  and a new within-
class scatter measure as the maximum of the within-class pairwise distances over all classes. Based
on the new scatter measures  we propose a novel dimensionality reduction method called worst-case
linear discriminant analysis (WLDA). WLDA solves an optimization problem which simultaneously
maximizes the worst-case between-class scatter measure and minimizes the worst-case within-class
scatter measure. If the number of training data points or the number of features is not very large 
e.g.  below 100  we propose to relax the optimization problem and formulate it as a metric learning
problem. In case both the number of training data points and the number of features are large  we
propose a greedy approach based on the constrained concave-convex procedure (CCCP) [24  18] to
ﬁnd one direction of the transformation at a time with the other directions ﬁxed. Moreover  we also
analyze a special case of WLDA to show its relationship with conventional LDA. We will report
experiments conducted on several benchmark datasets.

2 Worst-Case Linear Discriminant Analysis
We are given a training set of  data points    N     N ࣪ Ó. Let  be partitioned into
 ࣙ disjoint classes           where class  contains  examples. We perform linear
dimensionality reduction by ﬁnding a transformation matrix 9 ࢠ Ó.

2.1 Objective Function

We ﬁrst brieﬂy review the conventional LDA. The between-class scatter matrix and within-class
scatter matrix are deﬁned as

5 

  ࢤ   ࢤ  

5 

N ࢤ N ࢤ  

ࢣ

ࢣ



Nࢠ




ࢣ






ࢣ

where   
 N is the

sample mean of all data points. Based on the scatter matrices  the between-class scatter measure and
within-class scatter measure are deﬁned as

N is the class mean of the th class  and   


Nࢠ

  JH9 59

  JH9 59

where JH denotes the trace of a square matrix. LDA seeks to ﬁnd the optimal solution of 9 that
maximizes the ratio  as the optimality criterion.

By using the fact that   


    we can rewrite 5 as

ࢣ

ࢣ

ࢣ

ࢣ





5 


  

  ࢤ   ࢤ  

According to this and the deﬁnition of the within-class scatter measure  we can see that LDA tries
to maximize the average pairwise distance between class means   and minimize the average
within-class pairwise distance over all classes. Instead of taking this average-case view  our WLDA
model adopts a worst-case view which arguably is more suitable for classiﬁcation applications.
We deﬁne the sample covariance matrix for the th class  as

N ࢤ N ࢤ  

(1)

ࢣ

Nࢠ

5 




Unlike LDA which uses the average of the distances between each class mean and the sample mean
as the between-class scatter measure  here we use the minimum of the pairwise distances between
class means as the between-class scatter measure:


JH9   ࢤ   ࢤ  9

  E




(2)

(3)

Also  we deﬁne the new within-class scatter measure as



JH9 59



  =N



which is the maximum of the average within-class pairwise distances.

2

Similar to LDA  we deﬁne the optimality criterion of WLDA as the ratio of the between-class scatter
measure to the within-class scatter measure:

9 


=N

9
IJ 9 9  1

(4)
where 1 denotes the    identity matrix. The orthonormality constraint in problem (4) is widely
used by many existing dimensionality reduction methods. Its role is to limit the scale of each column
of 9 and eliminate the redundancy among all columns of 9.

2.2 Optimization Procedure

Since problem (4) is not easy to optimize with respect to 9  we resort to formulate this dimen-
sionality reduction problem as a metric learning problem [22  21  4]. We deﬁne a new variable
  99 which can be used to deﬁne a metric. Then we express  and  in terms of  as

JH  ࢤ   ࢤ  

JH5




  E


  =N



due to a property of the matrix trace that JH)*  JH*) for any matrices ) and * with proper
sizes. The orthonormality constraint in problem (4) is non-convex with respect to 9 and cannot be
expressed in terms of .
We deﬁne a set ࡕ as

ࡕ  ࢯ   99  9 9  1 9 ࢠ Ó

Apparently  ࢠ ࡕ. It has been shown in [16] that the convex hull of ࡕ can be precisely
expressed as a convex set ࡕ given by

ࡕ  ࢯ JH       1



where  denotes the zero vector or matrix of appropriate size and )  * means that the matrix
* ࢤ ) is positive semideﬁnite. Each element in ࡕ is referred to as an extreme point of ࡕ.
Since ࡕ consists of all convex combinations of the elements in ࡕ  ࡕ is the smallest convex
set that contains ࡕ  and hence ࡕ ࣮ ࡕ. Then problem (4) can be relaxed as

=N



 

E

=N


JH5
JH5


(5)
where 5    ࢤ   ࢤ  . For notational simplicity  we denote the constraint set as
࠶   ࢯ JH       1. Table 1 shows an iterative algorithm for solving problem (5).

IJ

JH       1

Table 1: Algorithm for solving optimization problem (5)

2.1: Compute the ratio  from ࢤ as:   ࢤ;
2.2: Solve the optimization problem

JH5 ࢤ  =N


  =HC =Nࢠ࠶ E

JH5


;

2.3: If ࢱ ࢤ ࢤࢱ ࣘ  (here we set   ࢤ")

Input:    5 and 
1: Initialize ;
2: For        EJAH

break;

Output: 

We now present the solution of the optimization problem in step 2.2. It is equivalent to the following
problem


JH5 ࢤ E


JH5





E
ࢠ࠶  =N



(6)

3

(7)
Note that problem (7) is a semideﬁnite programming (SDP) problem [19] which can be solved using
a standard SDP solver. After obtaining the optimal ਭ  we can recover the optimal 9ਭ as the top 
eigenvectors of ਭ. In the following  we will prove the convergence of the algorithm in Table 1.
Theorem 1 For the algorithm in Table 1  we have  ࣙ ࢤ

JH5 ࢤ  =N


JH5



JH5 ࢤ

JH5ࢤ . Because   =HC =Nࢠ࠶  and ࢤ ࢠ ࠶  we have

. Then ࢤ   since

Proof: We deﬁne   E

E

=N

E

 
 ࣙ ࢤ  . This means


JH5
JH5 ࣙ 

=N
which implies that  ࣙ ࢤ.
Theorem 2 For any  ࢠ ࠶  we have  ࣘ  ࣘ
value of 5.
Proof: It is obvious that  ࣙ . The numerator of  can be upper-bounded as

 tr5
 ࢤ

ࢣ
 JH5
ࢣ
ࢣ

JH5 ࣘ
ࢣ
ࢣ
 JH5
ࢣ
JH5 ࣙ
 JH5 ࣙ ࢣ
ࢣ

 

 JH5 ࣘ JH5

Moreover  the denominator of  can be lower-bounded as

ࢤ ࣙ ࢣ
where  is the th largest eigenvalue of  and satisﬁes  ࣘ  ࣘ  andࢣ

 

=N

E














ࢤ

where  is the th largest eigen-



(8)

(9)

According to [3]  we know that =N
is a concave function because it is the minimum of
several convex functions  and E
several concave functions. Moreover   is a positive scalar since   ࢤ. So problem (6)
is a convex optimization problem. We introduce new variables  and  to simplify problem (6) as

is a convex function because it is the maximum of

JH5

JH5

JH5 ࣘ  ࢘
JH5 ࣙ    ࢘ 

 ࢤ 

JH       1

E

s.t.

   due to the

constraints ࠶ on . By utilizing Eqs. (8) and (9)  we can reach the conclusion.
From Theorem 2  we can see that  is bounded and our method is non-decreasing. So our
method can achieve a local optimum when converged.



2.3 Optimization in Dual Form

In the previous subsection  we need to solve the SDP problem in problem (7). However  SDP is
not scalable to high dimensionality .
In many real-world applications to which dimensionality
reduction is applied  the number of data points  is much smaller than the dimensionality . Under
such circumstances  speedup can be obtained by solving the dual form of problem (4) instead.
It is easy to show that the solution of problem (4) satisﬁes 9  :) [14] where :  N     N
is the data matrix and ) ࢠ Ó. Then problem (4) can be formulated as

JH) : 5:)


JH) : 5:)

=N

)

E

=N

IJ

) )  1

(10)

4

where   : : is the linear kernel matrix. Here we assume that  is positive deﬁnite since the
data points are independent and identically distributed and  is much larger than . We deﬁne a new
variable *   

 ) and problem (10) can be reformulated as


JH* ࢤ 

JH* ࢤ 

=N

*

E

=N

 *
 *

 : 5:ࢤ 
 : 5:ࢤ 

IJ

* *  1

(11)
Note that problem (11) is almost the same as problem (4) and so we can use the same relaxation
technique above to solve problem (11). In the relaxed problem  the variable   ** used to
deﬁne the metric in the dual form is of size    which is much smaller than that (  ) of  in
the primal form when   . So solving the problem in the dual form is more efﬁcient. Moreover 
the dual form facilitates kernel extension of our method.

2.4 Alternative Optimization Procedure

In case the number of training data points  and the dimensionality  are both large  the above
optimization procedures will be infeasible. Here we introduce yet another optimization procedure
based on a greedy approach to solve problem (4) when both  and  are large.
We ﬁnd the ﬁrst column of 9 by solving problem (4) where 9 is a vector  then ﬁnd the second
column of 9 by assuming the ﬁrst column is ﬁxed  and so on. This procedure consists of  steps.
In the th step  we assume that the ﬁrst  ࢤ  columns of 9 have been obtained and we ﬁnd the
th column according to problem (4). We use 9ࢤ to denote the matrix in which the ﬁrst  ࢤ 
columns are already known and the constraint in problem (4) becomes

When     9
exist. So in the th step  we need to solve the following problem

ࢤ can be viewed as an empty matrix and the constraint 9

ࢤM   does not

M

 M   9

ࢤM  

IJ

E
M



 5M   ࢤ  ࣘ  ࢘
M
 ࢤ M
   
 M ࣘ  9
M
ࢤ59ࢤ

 and   JH9

ࢤM  

where   JH9

   ࢤ   ࢤ  M ࢤ  ࣘ  ࢘ 

. In the last

(12)

ࢤ  ࢤ   ࢤ  9ࢤ

constraint of problem (12)  we relax the constraint on M as M
The function 
 is not convex with respect to   since the Hessian matrix is not positive semidef-
inite. So the objective function of problem (12) is non-convex. Moreover  the second constraint in
problem (12)  which is the difference of two convex functions  is also non-convex. We rewrite the
objective function as

 M ࣘ  to make it convex.






   

"

ࢤ  ࢤ  



"

which is also the difference of two convex functions since      
for    is convex
with respect to  and  according to [3]. Then we can use the constrained concave-convex pro-
cedure (CCCP) [24  18] to optimize problem (12). More speciﬁcally  in the   th iteration of
CCCP  we replace the non-convex parts of the objective function and the second constraint with
their ﬁrst-order Taylor expansions at the solution   M
  in the th iteration and solve the
following problem



E
M

IJ

   

ࢤ    

"
 5M   ࢤ  ࣘ  ࢘
M
 ࢤ M
   
 M ࣘ  9
M

ࢤM  

    ࢤ   ࢤ  M  Ò

 ࢤ  ࣘ  ࢘ 

(13)

5

where   ࢤ
  i.e.   

 

  M

  and Ò
" ࣘ   and using the fact that

"

    ࢤ   ࢤ  M

ࣘ       (cid:13)(cid:13)(cid:13)   

 ࢤ 

(cid:13)(cid:13)(cid:13) 

   

"

ࣘ   

 . By putting an upper bound on

where ࢱ  ࢱ denotes the 2-norm of a vector  we can reformulate problem (13) into a second-order
cone programming (SOCP) problem [12] which is more efﬁcient than SDP:

E

M

IJ

 ࢤ    
 5M   ࢤ  ࣘ  ࢘
M
 ࢤ M

(cid:13)(cid:13)(cid:13)   

(cid:13)(cid:13)(cid:13) 

 ࢤ 
 M ࣘ  9
M

ࢤM  

ࣘ    with     

    ࢤ   ࢤ  M  Ò

 ࢤ  ࣘ  ࢘ 

(14)

2.5 Analysis

It is well known that in binary classiﬁcation problems when both classes are normally distributed
with the same covariance matrix  the solution given by conventional LDA is the Bayes optimal
solution. We will show here that this property still holds for WLDA.
The objective function for WLDA in a binary classiﬁcation problem is formulated as

M   ࢤ    ࢤ   M

=NM 5M M 5 M

M ࢠ Ó M M ࣘ 

=N

M
s.t.

(15)
Here  similar to conventional LDA  the reduced dimensionality  is set to 1. When the two classes
have the same covariance matrix  i.e.  5  5   the problem degenerates to the optimization problem
of conventional LDA since M 5M  M 5 M for any M and M is the solution of conventional
LDA.1 So WLDA also gives the same Bayes optimal solution as conventional LDA.
Since the scale of M does not affect the ﬁnal solution in problem (15)  we simplify problem (15) as

=N

M
s.t.

M   ࢤ    ࢤ   M
M 5M ࣘ  M 5 M ࣘ 

(16)
Since problem (16) is to maximize a convex function  it is not a convex problem. We can still use
CCCP to optimize problem (16). In the   th iteration of CCCP  we need to solve the following
problem

=N

M
s.t.

M   ࢤ    ࢤ   M
M 5M ࣘ  M 5 M ࣘ 

(17)

The Lagrangian is given by

  ࢤM   ࢤ    ࢤ   M  M 5M ࢤ   M 5 M ࢤ 

where  ࣙ  and  ࣙ . We calculate the gradients of  with respect to M and set them to 0 to
obtain

M   5  5 

ࢤ  ࢤ    ࢤ   M

From this  we can see that when the algorithm converges  the optimal Mਭ satisﬁes

Mਭ Ý  ਭ5  ਭ5 

ࢤ  ࢤ  

This is similar to the following property of the optimal solution in conventional LDA

Mਭ Ý 5

   ࢤ   Ý 5   5 
ࢤ

ࢤ  ࢤ  

1The constraint M M ࣘ  in problem (15) only serves to limit the scale of M.

6

However  in our method  ਭ and ਭ are not ﬁxed but learned from the following dual problem

  ࢤ  5  5 

"
 ࣙ   ࣙ 

ࢤ  ࢤ      

(18)



E

s.t.

  ࢤ   M 

where  
. Note that the ﬁrst term in the objective function of problem (18)
is just the scaled optimality criterion of conventional LDA when we assume the within-class scatter
matrix 5 to be 5  5  5 . From this view  WLDA seeks to ﬁnd a linear combination of 5
and 5 as the within-class scatter matrix to maximize the optimality criterion of conventional LDA
while controlling the complexity of the within-class scatter matrix as reﬂected by the second and
third terms of the objective function in problem (18).

3 Related Work





9 5 ࢤ 59

In [11]  Li et al. proposed a maximum margin criterion for dimensionality reduction by changing
the optimization problem of conventional LDA to: =N9 tr
. The objective
function has a physical meaning similar to that of LDA which favors a large between-class scatter
measure and a small within-class scatter measure. However  similar to LDA  the maximum margin
criterion also uses the average distances to describe the between-class and within-class scatter mea-
sures. Kocsor et al. [10] proposed another maximum margin criterion for dimensionality reduction.
The objective function in [10] is identical to that of support vector machine (SVM) and it treats the
decision function in SVM as one direction in the transformation matrix 9.
In [9]  Kim et al. proposed a robust LDA algorithm to deal with data uncertainty in classiﬁcation
applications by formulating the problem as a convex problem. However  in many applications  it is
not easy to obtain the information about data uncertainty. Moreover  its limitation is that it can only
handle binary classiﬁcation problems but not more general multi-class problems.
The orthogonality constraint on the transformation matrix 9 has been widely used by dimension-
ality reduction methods  such as Foley-Sammon LDA (FSLDA) [6  5] and orthogonal LDA [23].
The orthogonality constraint can help to eliminate the redundant information in 9. This has been
shown to be effective for dimensionality reduction.

4 Experimental Validation

In this section  we evaluate WLDA empirically on some benchmark datasets and compare WLDA
with several related methods  including conventional LDA  trace-ratio LDA [20]  FSLDA [6  5]  and
MarginLDA [11]. For fair comparison with conventional LDA  we set the reduced dimensionality
of each method compared to  ࢤ  where  is the number of classes in the dataset. After dimen-
sionality reduction  we use a simple nearest-neighbor classiﬁer to perform classiﬁcation. Our choice
of the optimization procedure follows this strategy: when the number of features  or the number
of training data points  is smaller than 100  the optimization method in Section 2.2 or 2.3 is used
depending on which one is smaller; otherwise  we use the greedy method in Section 2.4.

4.1 Experiments on UCI Datasets

Ten UCI datasets [1] are used in the ﬁrst set of experiments. For each dataset  we randomly select
70% to form the training set and the rest for the test set. We perform 10 random splits and report
in Table 2 the average results across the 10 trials. For each setting  the lowest classiﬁcation error is
shown in bold. We can see that WLDA gives the best result for most datasets. For some datasets 
e.g.  balance-scale and hayes-roth  even though WLDA is not the best  the difference between it and
the best one is very small. Thus it is fair to say that the results obtained demonstrate convincingly
the effectiveness of WLDA.

4.2 Experiments on Face and Object Datasets

Dimensionality reduction methods have been widely used for face and object recognition applica-
tions. Previous research found that face and object images usually lie in a low-dimensional subspace

7

Table 2: Average classiﬁcation errors on the UCI datasets. Here tr-LDA denotes the trace-ratio
LDA [20].

Dataset

diabetes
heart
liver
sonar
spambase
balance-scale
iris
hayes-roth
waveform
mfeat-factors

LDA
0.3233
0.2448
0.4001
0.2806
0.1279
0.1193
0.0244
0.3125
0.1861
0.0732

tr-LDA FSLDA MarginLDA WLDA
0.2996
0.3143
0.2157
0.2259
0.3779
0.3933
0.2661
0.2895
0.1260
0.1301
0.1174
0.1198
0.0211
0.0267
0.3050
0.3104
0.1671
0.1865
0.0250
0.0518

0.4143
0.2407
0.5058
0.2806
0.1440
0.1150
0.0644
0.2958
0.2303
0.0817

0.4039
0.4395
0.4365
0.3694
0.3093
0.1176
0.0622
0.3104
0.2261
0.0868

of the ambient image space. Fisherface (based on LDA) [2] is one representative dimensionality re-
duction method. We use three face databases  ORL [2]  PIE [17] and AR [13]  and one object
database  COIL [15]  in our experiments. In the AR face database  2 600 images of 100 persons
(50 men and 50 women) are used. Before the experiment  each image is converted to gray scale
and normalized to a size of !!  " pixels. The ORL face database contains 400 face images of
40 persons  each having 10 images. Each image is preprocessed to a size of &  ! pixels. In our
experiment  we choose the frontal pose from the PIE database with varying lighting and illumina-
tion conditions. There are about 49 images for each subject. Before the experiment  we resize each
image to a resolution of !  ! pixels. The COIL database contains 1 440 grayscale images with
black background for 20 objects with each object having 72 different images.
In face and object recognition applications  the size of the training set is usually not very large since
labeling data is very laborious and costly. To simulate this realistic situation  we randomly choose
4 images of a person or object in the database to form the training set and the remaining images
to form the test set. We perform 10 random splits and report the average classiﬁcation error rates
across the 10 trials in Table 3. From the result  we can see that WLDA is comparable to or even
better than the other methods compared.

Table 3: Average classiﬁcation errors on the face and object datasets. Here tr-LDA denotes the
trace-ratio LDA [20].

Dataset
ORL
PIE
AR
COIL

LDA
0.1529
0.4305
0.2498
0.2554

5 Conclusion

tr-LDA FSLDA MarginLDA WLDA
0.0446
0.1042
0.2469
0.2527
0.1919
0.1965
0.1593
0.1737

0.0536
0.2936
0.4282
0.1653

0.0654
0.6715
0.7726
0.1726

In this paper  we have presented a new supervised dimensionality reduction method by exploiting
the worst-case view instead of average-case view in the formulation. One interesting direction of
our future work is to extend WLDA to handle tensors for 2D or higher-order data. Moreover  we
will investigate the semi-supervised extension of WLDA to exploit the useful information contained
in the unlabeled data available in some applications.

Acknowledgement

This research has been supported by General Research Fund 621407 from the Research Grants
Council of Hong Kong.

8

References
[1] A. Asuncion and D.J. Newman. UCI machine learning repository  2007.
[2] P. N. Belhumeur  J. P. Hespanha  and D. J. Kriegman. Eigenfaces vs. Fisherfaces: Recognition using class
speciﬁc linear projection. IEEE Transactions on Pattern Analysis and Machine Intelligence  19(7):711–
720  1997.

[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  New York  NY  2004.
[4] J. V. Davis  B. Kulis  P. Jain  S. Sra  and I. S. Dhillon. Information-theoretic metric learning. In Pro-
ceedings of the Twenty-Fourth International Conference on Machine Learning  pages 209–216  Corvalis 
Oregon  USA  2007.

[5] J. Duchene and S. Leclercq. An optimal transformation for discriminant and principal component analy-

sis. IEEE Transactions on Pattern Analysis and Machine Intelligence  10(6):978–983  1988.

[6] D. H. Foley and J. W. Sammon. An optimal set of discriminant vectors. IEEE Transactions on Computers 

24(3):281–289  1975.

[7] K Fukunnaga. Introduction to Statistical Pattern Recognition. Academic Press  New York  1991.
[8] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag  New York  2nd edition  2002.
[9] S.-J. Kim  A. Magnani  and S. Boyd. Robust Fisher discriminant analysis. In Y. Weiss  B. Sch¨olkopf 
and J. Platt  editors  Advances in Neural Information Processing Systems 18  pages 659–666. Vancouver 
British Columbia  Canada  2006.

[10] A. Kocsor  K. Kov´acs  and C. Szepesv´ari. Margin maximizing discriminant analysis. In Proceedings of

the 15th European Conference on Machine Learning  pages 227–238  Pisa  Italy  2004.

[11] H. Li  T. Jiang  and K. Zhang. Efﬁcient and robust feature extraction by maximum margin criterion. In
S. Thrun  L. K. Saul  and B. Sch¨olkopf  editors  Advances in Neural Information Processing Systems 16 
Vancouver  British Columbia  Canada  2003.

[12] M. S. Lobo  L. Vandenberghe  S. Boyd  and H. Lebret. Applications of second-order cone programming.

Linear Algebra and its Applications  284:193–228  1998.

[13] A. M. Mart´ınez and R. Benavente. The AR-face database. Technical Report 24  CVC  1998.
[14] S. Mika  G. R¨atsch  J. Weston  B. Sch¨olkopf  A. J. Smola  and K.-R. M¨uller. Constructing descriptive and
discriminative nonlinear features: Rayleigh coefﬁcients in kernel feature spaces. IEEE Transactions on
Pattern Analysis and Machine Intelligence  25(5):623–633  2003.

[15] S. A. Nene  S. K. Nayar  and H. Murase. Columbia object image library (COIL-20). Technical Report

005  CUCS  1996.

[16] M. L. Overton and R. S. Womersley. Optimality conditions and duality theory for minimizing sums of

the largest eigenvalues of symmetric matrices. Math Programming  62(2):321–357  1993.

[17] T. Sim  S. Baker  and M. Bsat. The CMU pose  illumination and expression database. IEEE Transactions

on Pattern Analysis and Machine Intelligence  25(12):1615–1618  2003.

[18] A. J. Smola  S. V. N. Vishwanathan  and T. Hofmann. Kernel methods for missing variables. In Proceed-

ings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics  Barbados  2005.

[19] L. Vandenberghe and S. Boyd. Semideﬁnite prgramming. SIAM Review  38(1):49–95  1996.
[20] H. Wang  S. Yan  D. Xu  X. Tang  and T. Huang. Trace ratio vs. ratio trace for dimensionality reduction.
In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 
pages 1–8  Minneapolis  Minnesota  USA  2007.

[21] K. Q. Weinberger  J. Blitzer  and L. K. Saul. Distance metric learning for large margin nearest neighbor
classiﬁcation. In Y. Weiss  B. Sch¨olkopf  and J. Platt  editors  Advances in Neural Information Processing
Systems 18  pages 1473–1480  Vancouver  British Columbia  Canada  2005.

[22] E. P. Xing  A. Y. Ng  M. I. Jordan  and S. J. Russell. Distance metric learning with application to clustering
with side-information. In S. Becker  S. Thrun  and K. Obermayer  editors  Advances in Neural Information
Processing Systems 15  pages 505–512  Vancouver  British Columbia  Canada  2002.

[23] J.-P. Ye and T. Xiong. Computational and theoretical analysis of null space and orthogonal linear discrim-

inant analysis. Journal of Machine Learning Research  7:1183–1204  2006.

[24] A. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation  15(4):915–936  2003.

9

,Tengyang Xie
Bo Liu
Yangyang Xu
Mohammad Ghavamzadeh
Yinlam Chow
Daoming Lyu
Daesub Yoon