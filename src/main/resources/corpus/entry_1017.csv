2018,Communication Efficient Parallel Algorithms for Optimization on Manifolds,The last decade has witnessed an explosion in the development of models  theory and computational algorithms for ``big data'' analysis. In particular  distributed inference has served as a natural and dominating paradigm for statistical inference. However  the existing literature on parallel inference almost exclusively focuses on Euclidean data and parameters. While this assumption is valid for many applications  it is increasingly more common to encounter problems where the data or the parameters lie on a non-Euclidean space  like a manifold for example. Our work aims to fill a critical gap in the literature by generalizing parallel inference algorithms to optimization on manifolds. We show that our proposed algorithm is both communication efficient and carries theoretical convergence guarantees. In addition  we demonstrate the performance of our algorithm to the estimation of Fr\'echet means on simulated spherical data and the low-rank matrix completion problem over Grassmann manifolds applied to the Netflix prize data set.,Communication Efﬁcient Parallel Algorithms for

Optimization on Manifolds

Bayan Saparbayeva

Department of Applied and

Computational Mathematics and Statistics

Univeristy of Notre Dame

Notre Dame  Indiana 46556  USA

bsaparba@nd.edu

Michael Minyi Zhang

Department of Computer Science

Princeton University

Princeton  New Jersey 08540  USA

mz8@cs.princeton.edu

Lizhen Lin

Department of Applied and

Computational Mathematics and Statistics

Univeristy of Notre Dame

Notre Dame  Indiana 46556  USA

lizhen.lin@nd.edu

Abstract

The last decade has witnessed an explosion in the development of models  theory
and computational algorithms for “big data” analysis. In particular  distributed
computing has served as a natural and dominating paradigm for statistical inference.
However  the existing literature on parallel inference almost exclusively focuses
on Euclidean data and parameters. While this assumption is valid for many ap-
plications  it is increasingly more common to encounter problems where the data
or the parameters lie on a non-Euclidean space  like a manifold for example. Our
work aims to ﬁll a critical gap in the literature by generalizing parallel inference
algorithms to optimization on manifolds. We show that our proposed algorithm is
both communication efﬁcient and carries theoretical convergence guarantees. In
addition  we demonstrate the performance of our algorithm to the estimation of
Fréchet means on simulated spherical data and the low-rank matrix completion
problem over Grassmann manifolds applied to the Netﬂix prize data set.

1

Introduction

A natural representation for many statistical and machine learning problems is to assume the parameter
of interest lies on a more general space than the Euclidean space. Typical examples of this situation
include diffusion matrices in large scale diffusion tensor imaging (DTI) which are 3× 3 positive
deﬁnite matrices  now commonly used in neuroimaging for clinical trials [1]. In computer vision 
images are often preprocessed or reduced to a collection of subspaces [11  27] or  a digital image
can also be represented by a set of k-landmarks  forming landmark based shapes [13]. One may also
encounter data that are stored as orthonormal frames [8]  surfaces[15]  curves[16]  and networks [14].
In addition  parallel inference has become popular in overcoming the computational burden arising
from the storage  processing and computation of big data  resulting in a vast literature in statistics and
machine learning dedicated to this topic. The general scheme in the frequentist setting is to divide the
data into subsets  obtain estimates from each subset which are combined to form an ultimate estimate
for inference [9  30  17]. In the Bayesian setting  the subset posterior distributions are ﬁrst obtained
in the dividing step  and these subset posterior measures or the MCMC samples from each subset

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

posterior are then combined for ﬁnal inference [20  29  28  21  25  22]. Most of these methods are
“embarrassingly parallel” which often do not require communication across different machines or
subsets. Some communication efﬁcient algorithms have also been proposed with prominent methods
including [12] and [26].
Despite tremendous advancement in parallel inference  previous work largely focuses only on
Euclidean data and parameter spaces. To better address challenges arising from inference of big
non-Euclidean data or data with non-Euclidean parameters  there is a crucial need for developing
valid and efﬁcient inference methods including parallel or distributed inference and algorithms that
can appropriately incorporate the underlying geometric structure.
For a majority of applications  the parameter spaces fall into the general category of manifolds  whose
geometry is well-characterized. Although there is a recent literature on inference of manifold-valued
data including methods based on Fréchet means or model based methods [3  4  5  2  18] and even
scalable methods for certain models [23  19  24]  there is still a vital lack of general parallel algorithms
on manifolds. We aim to ﬁll this critical gap by introducing our parallel inference strategy. The
novelty of our paper is in the fact that is generalizable to a wide range of loss functions for manifold
optimization problems and that we can parallelize the algorithm by splitting the data across processors.
Furthermore  our theoretical development does not rely on previous results. In fact  generalizing
Theorem 1 to the manifold setting requires totally different machineries from that of previous work.
Notably  our parallel optimization algorithm has several key features:

(1) Our parallel algorithm efﬁciently exploits the geometric information of the data or parame-

ters.

(2) The algorithm minimizes expensive inter-processor communication.
(3) The algorithm has theoretical guarantees in approximating the true optimizer  characterized

in terms of convergence rates.

(4) The algorithm has outstanding practical performance in simulation studies and real data

examples.

Our paper is organized as follows: In Section 2 we introduce related work to the topic of parallel
inference. Next we present our proposed parallel optimization framework in Section 3 and present
theoretical convergence results for our parallel algorithm in Section 4. In Section 5  we consider a
simulation study of estimating the Fréchet means on the spheres and a real data example using the
Netﬂix prize data set. The paper ends with a conclusion and discussion of future work in Section 6.

2 Related work

In the typical “big data” scenario  it is usually the case that the entire data set cannot ﬁt onto one
machine. Hence  parallel inference algorithms with provably good theoretic convergence properties
are crucial for this situation. In such a setting  we assume that we have N = mn identically distributed
observations {xi j : i = 1 ... n  j = 1 ... m}  which are i.i.d divided into m subsets X j = {xi j  i =
1 ... n}  j = 1 ... m and stored in m separate machines. While it is important to consider inference
problems when the data are not i.i.d. distributed across processors  we will only consider the i.i.d.
setting as a simplifying assumption for the theory.
(cid:80)n
For a loss function L : Θ× D → (cid:82)  each machine j has access to a local loss function  L j (θ) =
L (θ  xi j )  where D is the data space. Then  the local loss functions are combined into a
global loss function LN (θ) = 1
L j (θ). For our intended optimization routine  we are actually
(θ) = (cid:69)x∈D L (θ  x). In the parallel setting 
looking for the minimizer of an expected loss function L ∗
we cannot investigate L ∗ directly and we may only analyze it through LN . However  calculating
the total loss function directly and exactly requires excessive inter-processor communication  which
carries a huge computational burden as the number of processors increase. Thus  we must approximate
the true parameter θ∗ = argminθ∈Θ L ∗
(θ) by an empirical risk minimizer ˆθ = argminθ∈Θ LN (θ).
In this work  we focus on generalizing a particular parallel inference framework  the Iterative Local
Estimation Algorithm (ILEA) [12]  to manifolds. This algorithm optimizes an approximate  surrogate
loss function instead of the global loss function as a way to avoid processor communication. The

(cid:80)m

j=1

1
n

i=1

m

2

idea of the surrogate function starts from the Taylor series expansion of LN

(cid:161) ¯θ+ t(θ− ¯θ)(cid:162) = LN ( ¯θ)+ t〈∇LN ( ¯θ)  θ− ¯θ〉+ ∞(cid:88)

LN

∇s LN ( ¯θ)(θ− ¯θ)

⊗s.

t s
s!

s=2

The global high-order derivatives ∇s LN ( ¯θ) (s ≥ 2) are replaced by local high-order derivatives
∇s L1( ¯θ)(s ≥ 2) from the ﬁrst machine

˜L (θ) = LN ( ¯θ)+〈∇LN ( ¯θ)  θ− ¯θ〉+ ∞(cid:88)

∇s L1( ¯θ)(θ− ¯θ)

⊗s.

1
s!

s=2

⊗s

1
s!

s=2
= 1
2
= O

So the approximation error is

˜L (θ)− LN (θ) = ∞(cid:88)
(cid:161)∇s L1( ¯θ)−∇s LN ( ¯θ)(cid:162)(θ− ¯θ)
θ− ¯θ (cid:161)∇2L1( ¯θ)−∇2LN ( ¯θ)(cid:162)(cid:161)θ− ¯θ(cid:162)(cid:69)+O(cid:161) ∥ θ− ¯θ ∥3(cid:162)
(cid:68)
(cid:179) 1(cid:112)
∥ θ− ¯θ ∥2 + ∥ θ− ¯θ ∥3(cid:180)
⊗s in the ˜L (θ) can be replaced by L1(θ)−L1( ¯θ)−〈∇L1( ¯θ)  θ−

The inﬁnite sum(cid:80)∞
We can omit the additive constant(cid:161)L1( ¯θ)− LN ( ¯θ)(cid:162)+〈∇L1( ¯θ)−∇LN ( ¯θ)  ¯θ〉. Thus the surrogate loss

˜L (θ) = L1(θ)−(cid:161)L1( ¯θ)− LN ( ¯θ)(cid:162)−(cid:173)∇L1( ¯θ)−∇LN ( ¯θ)  θ− ¯θ(cid:174).

n
∇s L1( ¯θ)(θ− ¯θ)

s=2

1
s!

¯θ〉

.

function ˜L (θ) is deﬁned as

˜L (θ) = L1(θ)−〈∇L1( ¯θ)−∇LN ( ¯θ)  θ〉.

Thus  the surrogate minimizer ˜θ = argminΘ ˜L approximates the empirical risk minimizer ˆθ.
[12] show that the consequent surrogate minimizers have a provably good convergence rate to ˆθ given
the following regularity conditions:

R = supθ∈Θ ∥ θ− θ∗ ∥> 0 

1. The parameter space Θ is a compact and convex subset of (cid:82)d . Besides  θ∗ ∈ int(Θ) and
2. The Hessian matrix I (θ) = ∇2L ∗

  that is there exist constants (µ−  µ+)

such that

3. For any δ > 0  there exists  > 0  such that

(θ) is invertible at θ∗
µ−Id (cid:185) I (θ∗
) (cid:185) µ+Id  
(cid:111) = 1 
(cid:175)(cid:175)L (θ)− L (θ∗
)(cid:175)(cid:175) ≥ 

inf

(cid:110)

inf (cid:80)

∥θ−θ∗∥≥δ

4. For a ball around the true parameter U (ρ) = {θ :∥ θ− θ∗ ∥≤ ρ} there exist constants (G L)

and a function K (x) such that

(cid:69) ∥ ∇L (θ) ∥16≤ G16

(cid:129)L (θ  x)− L (θ(cid:48)

(cid:69)(cid:129)∇2L (θ)− I (θ)(cid:129) ≤ L16 
  x)(cid:129) ≤ K (x) ∥ θ− θ(cid:48) ∥ 

for all θ  θ(cid:48) ∈ U (ρ).

which leads to the following theorem:
Theorem 1. Suppose that the standard regularity conditions hold and initial estimator ¯θ lies in the
neighborhood U (ρ) of θ∗

. Then the minimizer ˜θ of the surrogate loss function ˜L (θ) satisﬁes

∥ ˜θ− ˆθ ∥≤ C2(∥ ¯θ− ˆθ ∥ + ∥ ˆθ− θ∗ ∥ +(cid:129)∇2L1(θ∗

)−∇2LN (θ∗

)(cid:129)) ∥ ¯θ− ˆθ ∥ 

with probability at least 1−C1mn

−8  where the constants C1 and C2 are independent of (m n  N ).

3

3 Parallel optimizations on manifolds

Our work aims to generalize the typical gradient descent optimization framework to manifold
optimization. In particular  we will use the ILEA framework as our working example to generalize
parallel optimization algorithms. Instead of working with (cid:82)d   we have a d-dimensional manifold M.
We also consider a surrogate loss function ˜L j : Θ×Z → (cid:82)  where Θ is a subset of the manifold M  that
approximates the global loss function LN . Here we choose to optimize ˜L j on the jth machine–that
is  on different iterations we optimize on different machine for efﬁcient exploration unlike from
previous algorithm  where the surrogate function is always optimized on the ﬁrst machine.
To generalize the idea of moving along a gradient on the manifold M  we use the retraction map 
which is not necessarily the exponential map that one would typically use in manifold gradient
descent  but shares several important properties with the exponential map. Namely  a retraction on M
is a smooth mapping R : T M → M with the following properties

1. Rθ(0θ) = R(θ 0θ) = θ  where Rθ is the restriction of R from T M to the point θ and the
2. DRθ(0θ) = DR(θ 0θ) = idTθ M   where idTθ M denotes the identity mapping on TθM.

tangent space TθM  0θ denotes the zero vector on TθM 

We also demand that

1. For any θ1  θ2 ∈ M  curves Rθ1 t R−1
θ1  where s  t ∈ [0 1]  must coincide 
2. The triangle inequality holds  that is for any θ1  θ2  θ3 ∈ M  it is the case that dR(θ1  θ2) ≤
θ2 for t ∈ [0 1].

dR(θ2  θ3)+dR(θ3  θ1) where dR(θ1  θ2) is the length of the curve Rθ1 t R−1

θ2 and Rθ2 sR−1

θ1

θ2

θ1

Our construction starts with the Taylor’s formula for LN on the manifold M

LN (θ) = LN ( ¯θ)+〈∇LN ( ¯θ) log ¯θ θ〉+ ∞(cid:88)

∇s LN ( ¯θ)(log ¯θ θ)

⊗s

1
s!

s=2

Because we split the data across machines  evaluating the derivatives ∇s LN ( ¯θ) requires excessive
processor communication. We want to reduce the amount of communication by replacing the global
high-order derivatives ∇s LN ( ¯θ) (s ≥ 2) with the high-order local derivatives ∇s L j ( ¯θ). This gives us
the following surrogate to LN

˜L j (θ) = LN ( ¯θ)+〈∇LN ( ¯θ) log ¯θ θ〉+ ∞(cid:88)

∇s L j ( ¯θ)(log ¯θ θ)

⊗s.

1
s!

s=2

Then we have the following approximation error

˜L j (θ)− LN (θ) = 1
2
= O

〈log ¯θ θ (∇2L j ( ¯θ)−∇2LN ( ¯θ))log ¯θ θ〉+O(cid:161)dg ( ¯θ  θ)3(cid:162)
dg ( ¯θ  θ)2 + dg ( ¯θ  θ)3(cid:180)
(cid:179) 1(cid:112)
⊗s with L j (θ)− L j ( ¯θ)−(cid:173)∇L j ( ¯θ) log ¯θ θ(cid:174) :

n

.

∇s L j ( ¯θ)(log ¯θ θ)

1
s!

s=2
˜L j (θ) = LN ( ¯θ)+〈∇LN ( ¯θ) log ¯θ θ〉+ L j (θ)− L j ( ¯θ)−〈∇L j ( ¯θ) log ¯θ θ〉

= L j (θ)+ (LN ( ¯θ)− L j ( ¯θ))+〈∇LN ( ¯θ)−∇L j ( ¯θ) log ¯θ θ〉.

We replace(cid:80)∞

˜L j but in its minimizer  we omit the additive constant
Since we are not interested in the value of
(LN ( ¯θ)− L j ( ¯θ)) and redeﬁne ˜L j as
˜L j (θ) := L j (θ)−〈∇L j ( ¯θ)−∇LN ( ¯θ) log ¯θ θ〉. Then we can
generalize the exponential map exp ¯θ and the inverse exponential map log ¯θ to the retraction map R ¯θ
and the inverse retraction map R−1

¯θ   which is also called the lifting  and redeﬁne ˜L j

˜L j (θ) := L j (θ)−〈∇L j ( ¯θ)−∇LN ( ¯θ)  R−1

¯θ

θ〉.

Therefore we have the following generalization of the Iterative Local Estimation Algorithm (ILEA)
for the manifold M:

4

Algorithm 1: ILEA for Manifolds
Initialize θ0 = ¯θ;
for s = 0 1 ... T − 1 do
Transmit the current iterate θs to local machines {Mj }m
j=1;
for j = 1 ... m do
Compute the local gradient ∇L j (θs) at machine Mj ;
Transmit the local gradient ∇L j (θs) to machine Ms;

(cid:80)m
Calculate the global gradient ∇LN (θs) = 1
j=1
Form the surrogate function ˜Ls(θ) = Ls(θ)−〈R−1
θs
Update θs+1 ∈ argmin ˜Ls;

m

Return θT

∇L j (θs)) in Machine Ms;
θ ∇Ls(θs)−∇LN (θs)〉;

4 Convergence rates of the algorithm

To establish some theoretical convergence rates on our algorithm  we consequently have to impose
some regularity conditions on the parameter space Θ  the loss function L and the population
risk L ∗. We must establish these conditions speciﬁcally for manifolds instead of simply using
the regularity conditions placed on Euclidean spaces. For example  in the manifold the Hessians
∇2L (θ  x) ∇2L (θ(cid:48)
  x) are deﬁned in different tangent spaces meaning there cannot be any linear
expressions of the second-order derivatives.
In the manifold for any ξ ∈ Tθ(cid:48) M we can deﬁne the vector ﬁeld as ξ(θ) = D(R−1
take the covariant derivative of ξ(θ) along the retraction Rθ(cid:48) t Rθ(cid:48) θ :

)ξ. We can also

θ(cid:48)

θ

ξ(Rθ(cid:48) t Rθ(cid:48) θ) =
∇

D(cid:161)R

θ(cid:48) (Rθ(cid:48) t Rθ(cid:48) θ)(cid:162)−1

−1

R−1
θ(cid:48) θ

(cid:179)

D

Rθ(cid:48) t Rθ(cid:48) θθ(cid:48)(cid:180)

R−1

ξ = ∇D(t  θ  θ(cid:48)

)ξ.

(1)

The expression (1) deﬁnes the linear map ∇D(t  θ  θ(cid:48)
) from Tθ(cid:48) M to TRθ(cid:48) t Rθ(cid:48) θM and want to impose
some conditions to this map. Finally  we impose the following regularity conditions on the parameter
space Θ  the loss function L and the population risk L ∗.

D(cid:161)R

θ(cid:48) (Rθ(cid:48) t Rθ(cid:48) θ)(cid:162)−1

−1

∇

R−1
θ(cid:48) θ

θ1  θ2 ∈ Θ curves Rθ1 t Rθ1
also demand that there exists L

1. The parameter space Θ is a compact and R-convex subset of M  which means that for any
θ2 must be within Θ for any θ1  θ2 ∈ M and
(cid:48)

θ2 and expθ1 t logθ1
(cid:48) ∈ (cid:82) such that
dR(θ1  θ2) ≤ L

dg (θ1  θ2) 

where dg (θ1  θ2) is the geodesic distance 

2. The matrix I (θ) = ∇2L ∗

(θ) is invertible at θ∗
µ−idθ∗ (cid:185) I (θ∗

: ∃ constants µ−  µ+ ∈ (cid:82) such that
) (cid:185) µ+idθ∗ 

3. For any δ > 0  there exists ε > 0 such that

(cid:110)

inf (cid:80)

inf

dg (θ∗ θ)≥δ

(cid:175)(cid:175)L (θ)− L (θ∗

4. There exist constants (G L) and a function K (x) such that for all θ  θ(cid:48) ∈ U and t ∈ [0 1]

)(cid:175)(cid:175) ≥ ε

(cid:111) = 1 
(cid:69)(cid:147)(cid:147)∇2L (θ  D)− I (θ)(cid:147)(cid:147)16 ≤ L16 
θ(cid:48)(cid:162)−1(cid:147)(cid:147)(cid:147) ≤ K (x)dR ¯θ (θ  θ(cid:48)
  x)(cid:161)DR−1
θ(cid:48) ˆθ(cid:162)(cid:147)(cid:147)(cid:147) ≤ K (x)dR ¯θ (θ  θ(cid:48)
  x)(cid:161)DR−1

∗∇L (Rθ(cid:48) t Rθ(cid:48) θ  x) ∥≤ K (x)dR(θ  θ(cid:48)

) 

) 

) 

ˆθ

∥ x ∥= 1}. Moreover 

(cid:69) ∥ ∇L (θ  D) ∥16≤ G16 

(cid:147)(cid:147)(cid:147)(cid:161)DR−1
ˆθ(cid:162)∗∇2L (θ  x)(cid:161)DR−1
(cid:147)(cid:147)(cid:147)(cid:161)DR−1

ˆθ(cid:162)∗∇2L (θ  x)(DR−1

∥ ∇D(t  θ  θ(cid:48)

θ(cid:162)−1 −(cid:161)DR−1
ˆθ(cid:162)−(cid:161)DR−1

θ(cid:48) ˆθ(cid:162)∗∇2L (θ(cid:48)
θ(cid:48) ˆθ(cid:162)∗∇2L (θ(cid:48)

)

ˆθ

θ

θ

θ

where (cid:129)(cid:129) is a spectral norm of matrices  (cid:129)A(cid:129) = sup{∥ Ax ∥: x ∈ (cid:82)n 
K satisﬁes (cid:69)K ≤ K 16 for some constant K > 0.

5

Given these conditions  we have the following theorem:
Theorem 2. If the standard regularity conditions holds  the initial estimator ¯θ lies in the neighborhood
U of θ∗ and

(cid:147)(cid:147)(cid:147)(cid:161)DR−1
ˆθ(cid:162)−(cid:161)DR−1

θ∗ ˆθ(cid:162)∗(cid:161)∇2 ˜Ls(θ∗
θ(cid:48) ˆθ(cid:162)∗∇2 ˜Ls(θ(cid:48)

θ∗ ˆθ(cid:162)(cid:147)(cid:147)(cid:147) ≤ ρµ−R−
)(cid:162)(cid:161)DR−1
θ(cid:48) ˆθ(cid:162)(cid:147)(cid:147)(cid:147) ≤ K (x)dR ¯θ (θ  θ(cid:48)

4

 

)− I (θ∗

  x)(cid:161)DR−1

θ

θ∗ ˆθ)(cid:162)−1(cid:147)(cid:147)(cid:147)   then any minimizer ˜θ of the surrogate loss function ˜Ls(θ)

) 

(cid:147)(cid:147)(cid:147)(cid:161)DR−1

θ

where R− =

satisﬁes

dR( ˜θ  ˆθ) ≤ C2

ˆθ(cid:162)∗∇2 ˜Ls(θ  x)(DR−1
(cid:147)(cid:147)(cid:147)(cid:161)(DR−1
(cid:179)
1+ dR( ¯θ  ˆθ)+ dR(θ∗

θ∗ ˆθ)∗(DR−1

1

dR( ¯θ  ˆθ) 
C3
with probability at least 1−C1mn
−8  where constants C1 C2 and C3 are independent of (m n  N ).

ˆθ

)−∇2LN (θ∗

  ˆθ)+

(cid:147)(cid:147)(cid:147)(cid:161)DR−1

θ∗ ˆθ(cid:162)∗(cid:161)∇2Ls(θ∗

)(cid:162)(cid:161)DR−1

θ∗(cid:162)−1(cid:147)(cid:147)(cid:147)(cid:180)

5 Simulation study and data analysis

To examine the quality of our parallel algorithm we ﬁrst apply it to the estimation of Fréchet means
on spheres  which has closed form expressions for the estimation of the extrinsic mean (true empirical
minimizer). In addition  we apply our algorithm to Netﬂix movie-ranking data set as an example
of optimization over Grassmannian manifolds in the low-rank matrix completion problem. In the
following results  we demonstrate the utility of our algorithm both for high dimensional manifold-
valued data (Section 5.1) and Euclidean space data with non-Euclidean parameters (Section 5.2).
We wrote the code for our implementations in Python and carried out the parallelization of the code
through MPI1[7].

5.1 Estimation of Fréchet means on manifolds

We ﬁrst consider the estimation problem of Fréchet means [10] on manifolds. In particular  the
manifold under consideration is the sphere in which we wish to estimate both the extrinsic and
intrinsic mean [3]. Let M be a general manifold and ρ be a distance on M which can be an intrinsic
distance  by employing a Riemannian structure of M  or an extrinsic distance  via some embedding J
onto some Euclidean space. Also  let x1 ...  xN be sample of point on the hypersphere Sd   the sample
Fréchet mean of x1 ...  xn is deﬁned as

ˆθ = arg min
θ∈M=Sd

N(cid:88)

i=1

ρ2(θ  xi ) 

(2)

where ρ is some distance on the sphere.
The extrinsic distance  for our spherical example  is deﬁned to be ρ(x  y) = (cid:107)J(x)− J(y)(cid:107) = (cid:107)x − y(cid:107)
with (cid:107)·(cid:107) as the Euclidean distance and the embedding map J(x) = x ∈ (cid:82)d+1 as the identity map. We
call ˆθ the extrinsic Fréchet mean on the sphere. We choose this example in our simulation  as we
know the true global optimizer which is given by ¯x/(cid:107) ¯x(cid:107) where ¯x is the standard sample mean of
x1 ...  xN in Euclidean distance. The intrinsic Fréchet mean  on the other hand  is deﬁned to be where
the distance ρ is the geodesic distance (or the arc length). In this case we compare the estimator
obtained from the parallel algorithm with the optimizer obtained from a gradient descent algorithm
along the sphere applied to the entire data set. Despite that the spherical case may be an “easy” setting
as it has a Betti number of zero  we chose this example so that we have ground truth to compare our
results with and we  in fact  perform favorably even when the dimensionality of the data is high even
as we increase the number of processors.
For this example  we simulate one million observations from a 100-dimensional von Mises distribution
projected onto the unit sphere with mean sampled randomly from N (0  I ) and a precision of 2. For

1Our code is available at https://github.com/michaelzhang01/parallel_manifold_opt

6

Figure 1: Extrinsic mean comparison (left) and intrinsic mean comparison (right) on spheres in S99

Figure 2: Extrinsic mean results on S1  for one (left) and ten (right) processors

the extrinsic mean example  the closed form expression of the sample mean acts as a “ground truth”
to which we can compare our results. In both the extrinsic and intrinsic mean examples  we run 20
trials of our algorithm over 1  2  4  6  8 and 10 processors. For the extrinsic mean simulations we
compare our results to the true global optimizer in terms of root mean squared error (RMSE) and for
the intrinsic mean simulations we compare our distributed results to the single processor results  also
in terms of RMSE.
As we can see in Figure 1  even if we divide our observations to as many as 10 processors we still
obtain favorable results for the estimation of the Fréchet mean in terms of RMSE to the ground truth
for the extrinsic mean case and the single processor results for the intrinsic mean case. To visualize
this comparison  we show in Figure 2 an example of our method’s performance on two dimensional
data so that we may see that our optimization results yield a very close estimate to the true global
optimizer.

5.2 Real data analysis: the Netﬂix example

Next  we consider an application of our algorithm to the Netﬂix movie rating dataset. This dataset
of over a million entries  X ∈ (cid:82)M×N   consists of M = 17770 movies and N = 480189 users  in which
only a sparse subset of the users and movies have ratings. In order to build a better recommendation
systems to users  we can frame the problem of predicting users’ ratings for movies as a low-rank
matrix completion problem by learning the rank-r Grassmannian manifold U ∈ Gr(M  r) which
optimizes for the set of observed entries (i   j ) ∈ Ω the loss function
(cid:88)

(cid:88)

(cid:161)(UW )i j − Xi j

(cid:162)2 + λ2

(3)

L(U ) = 1
2

(i  j )∈Ω

(UW )i j  

2

(i  j )∉Ω

7

where W is r -by-N matrix. Each user k has the loss function L (U  k) = 1
where ◦ is the Hadamard product  (wk)i = Wi k  and

2

|ck ◦ (U wk(U )− Xk)|2  

(cid:40)

(cid:40)

(ck)i =

1 
λ 

if
if

(i  k) ∈ Ω
(i  k) ∉ Ω  

(Xk)i =

wk(U ) =(cid:161)U T diag(ck ◦ ck)U(cid:162)−1U T(cid:161)ck ◦ ck ◦ Xk

(cid:162).

Xi k 
0 

if
if

(i  k) ∈ Ω
(i  k) ∉ Ω 

L (U  k) = 1
N

L(U ).

Which results in the following gradient

∇L (U  k) =(cid:161)ck ◦ ck ◦ (U wk(U )− Xk)(cid:162)wk(U )T = diag(ck ◦ ck)(U wk(U )− Xk)wk(U )T .
(cid:80)j q

We can assume that N = pq  then for each local machine Mj   j = 1 ...  p  we have the local function
L j (U ) = 1

L (U  k). So the global function is

q

p(cid:88)

k=(j−1)q+1

pq(cid:88)
For iterations s = 0 1 ... P − 1 we have ∇L j (Us) =(cid:80)j q
gradient is ∇LN (Us) = 1
retraction map

LN (U ) = 1
p
(cid:80)p
j=1
[U ] : Gr(m r ) →
−1
R

L j (U ) = 1
pq

k=1

j=1

p

∇L (Us k). Therefore the global
∇L j (Us). Instead of the logarithm map we will use the inverse

k=(j−1)q+1

T[U ]Gr(m r )

(cid:55)→ V −U (U T U )

−1U T V.

[V ]

Which gives us the following surrogate function
˜Ls(V ) = Ls(V )−〈V −Us(U T

−1U T

s V ∇Ls(Us)−∇LN (Us)〉

s Us)

= Ls(V )−〈V ∇Ls(Us)−∇LN (Us)〉.

and its gradient

∇ ˜Ls(V ) = ∇Ls(V )− (Im −V (V T V )

−1V T )(∇Ls(Us)−∇LN (Us)).

(cid:161)λ0∇ ˜Ls(Us)(cid:162).2

To optimize with respect to our loss function  we have to ﬁnd Us+1 = argmin ˜Ls. To do this  we
move according to the steepest descent by taking step size λ0 in the direction ∇ ˜Ls(Us) by taking the
retraction  Us+1 = R[Us ]
For our example we set the matrix rank to r = 10 and the regularization parameter to λ = 0.1 and
divided the data randomly across 4 processors. Figure 3 shows that we can perform distributed
manifold gradient descent in this complicated problem and we can reach convergence fairly quickly
(after about 1000 seconds).

6 Conclusion

We propose in this paper a communication efﬁcient parallel algorithm for general optimization
problems on manifolds which is applicable to many different manifold spaces and loss functions.
Moreover  our proposed algorithm can explore the geometry of the underlying space efﬁciently and
perform well in simulation studies and practical examples all while having theoretical convergence
guarantees.
In the age of “big data”  the need for distributable inference algorithms is crucial as we cannot reliably
expect entire datasets to sit on a single processor anymore. Despite this  much of the previous work
in parallel inference has only focused on data and parameters in Euclidean space. Realistically  much
of the data that we are interested in is better modeled by manifolds and thus we need fast inference
algorithms that are provably suitable for situations beyond the Euclidean setting. In future work  we
aim to extend the situations under which parallel inference algorithms are generalizable to manifolds
and demonstrate more critical problems (in neuroscience or computer vision  for example) in which
parallel inference is a crucial solution.

2We select the step size parameter according to the modiﬁed Armijo algorithm seen in [6].

8

Figure 3: Test set RMSE of the Netﬂix example over time  evaluated on 10 trials.

.

Acknowledgments
Bayan Saparbayeva was partially supported by DARPA N66001-17-1-4041. Michael Zhang was
supported by NSF grant 1447721. Lizhen Lin acknowledges the support from NSF grants IIS
1663870  DMS Career 1654579 and a DARPA grant N66001-17-1-4041.

References
[1] Andrew L Alexander  Jee Eun Lee  Mariana Lazar  and Aaron S. Field. Diffusion tensor imaging

of the brain. Neurotherapeutics  4(3):316–329  2007.

[2] Abhishek Bhattacharya and Rabi Bhattacharya. Nonparametric Inference on Manifolds: With

Applications to Shape Spaces. IMS Monograph #2. Cambridge University Press  2012.

[3] Rabi Bhattacharya and Lizhen Lin. Omnibus CLTs for Fréchet means and nonparametric
inference on non-Euclidean spaces. The Proceedings of the American Mathematical Society 
145:13–428  2017.

[4] Rabi Bhattacharya and Vic Patrangenaru. Large sample theory of intrinsic and extrinsic sample

means on manifolds. The Annals of Statistics  31(1):1–29  2003.

[5] Rabi Bhattacharya and Vic Patrangenaru. Large sample theory of intrinsic and extrinsic sample

means on manifolds: II. Ann. Statist.  33:1225–1259  2005.

[6] Nicolas Boumal. Optimization and estimation on manifolds. PhD thesis  Université catholique

de Louvain  2014.

[7] Lisandro Dalcín  Rodrigo Paz  and Mario Storti. MPI for Python. Journal of Parallel and

Distributed Computing  65(9):1108 – 1115  2005.

[8] T. Downs  J. Liebman  and W. Mackay. Statistical methods for vectorcardiogram orientations.

International Symposium on Vectorcardiography  pages 216–222  1971.

[9] John C. Duchi  Alekh Agarwal  and Martin J. Wainwright. Dual averaging for distributed
optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic
Control  57:592–606  2012.

[10] Maurice Fréchet. Lés élements aléatoires de nature quelconque dans un espace distancié. Ann.

Inst. H. Poincaré  10:215–310  1948.

9

[11] Jeffrey Ho  Kuang-Chih Lee  Ming-Hsuan Yang  and David Kriegman. Visual tracking using
In Computer Vision and Pattern Recognition  volume 1  pages

learned linear subspaces.
I–782–I–789 Vol.1  June 2004.

[12] Michael I. Jordan  Jason D. Lee  and Yun Yang. Communication-efﬁcient distributed statistical

inference. Journal of the American Statistical Association  0(ja):0–0  2018.

[13] David G. Kendall. Shape manifolds  Procrustean metrics  and complex projective spaces. Bull.

of the London Math. Soc.  16:81–121  1984.

[14] Eric Kolaczyk  Lizhen Lin  Steven Rosenberg  and Jackson Walters. Averages of Unlabeled
Networks: Geometric Characterization and Asymptotic Behavior. ArXiv e-prints  September
2017.

[15] Sebastian Kurtek  Eric Klassen  John C. Gore  Zhaohua Ding  and Anuj Srivastava. Elastic
geodesic paths in shape space of parameterized surfaces. IEEE Transactions on Pattern Analysis
and Machine Intelligence  34(9):1717–1730  Sept 2012.

[16] Sebastian Kurtek  Anuj Srivastava  Eric Klassen  and Zhaohua Ding. Statistical modeling
of curves using shapes and related features. Journal of the American Statistical Association 
107(499):1152–1165  2012.

[17] Jason D. Lee  Yuekai Sun  Qiang Liu  and Jonathan E. Taylor. Communication-efﬁcient sparse

regression: a one-shot approach. CoRR  abs/1503.04337  2015.

[18] Lizhen Lin  Vinayak Rao  and David B. Dunson. Bayesian nonparametric inference on the

Stiefel manifold. Statistics Sinica  27:535–553  2017.

[19] Lester Mackey  Ameet Talwalkar  and Michael I Jordan. Distributed matrix completion and

robust factorization. The Journal of Machine Learning Research  16(1):913–960  2015.

[20] Stanislav Minsker  Sanvesh Srivastava  Lizhen Lin  and David B. Dunson. Robust and scalable
Bayes via a median of subset posterior measures. Journal of Machine Learning Research 
18(124):1–40  2017.

[21] Willie Neiswanger  Chong Wang  and Eric P. Xing. Asymptotically exact  embarrassingly
In Proceedings of the Thirtieth Conference on Uncertainty in Artiﬁcial

parallel MCMC.
Intelligence  pages 623–632  2914.

[22] Christopher Nemeth  Chris Sherlock  et al. Merging MCMC subposteriors through Gaussian-

process approximations. Bayesian Analysis  13(2):507–530  2018.

[23] Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale

matrix completion. Mathematical Programming Computation  5(2):201–226  2013.

[24] Hesamoddin Salehian  Rudrasis Chakraborty  Edward Ofori  David Vaillancourt  and Baba C.
Vemuri. An efﬁcient recursive estimator of the Fréchet mean on a hypersphere with applications
to medical image analysis. In Mathematical Foundations of Computational Anatomy  volume 3 
2015.

[25] Steven L. Scott  Alexander W. Blocker  Fernando V. Bonassi  Hugh A. Chipman  Edward I.
George  and Robert E. McCulloch. Bayes and big data: the consensus Monte Carlo algorithm.
International Journal of Management Science and Engineering Management  11(2):78–88 
2016.

[26] Ohad Shamir  Nathan Srebro  and Tong Zhang. Communication-efﬁcient distributed opti-
mization using an approximate newton-type method. In International Conference on Machine
Learning  2014.

[27] G. Prabhu Teja and Sundaram Ravi. Face recognition using subspaces techniques. In In-
ternational Conference on Recent Trends In Information Technology  pages 103–107  April
2012.

10

[28] Xiangyu Wang  Fangjian Guo  Katherine A. Heller  and David B. Dunson. Parallelizing MCMC
with random partition trees. In Advances in Neural Information Processing Systems  pages
451–459. 2015.

[29] Michael Minyi Zhang  Henry Lam  and Lizhen Lin. Robust and parallel Bayesian model

selection. Computational Statistics and Data Analysis  127:229 – 247  2018.

[30] Yuchen Zhang  John C. Duchi  and Martin J. Wainwright. Communication-efﬁcient algorithms

for statistical optimization. Journal of Machine Learning Research  14:3321–3363  2013.

11

,Arun Rajkumar
Shivani Agarwal
Meisam Razaviyayn
David Tse
Bayan Saparbayeva
Michael Zhang
Lizhen Lin