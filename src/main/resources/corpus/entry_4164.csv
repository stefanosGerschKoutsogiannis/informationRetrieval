2019,Oracle-Efficient Algorithms for Online Linear Optimization with Bandit Feedback,We propose computationally efficient algorithms for \textit{online linear optimization with bandit feedback}  in which a player chooses an \textit{action vector} from a given (possibly infinite) set $\mathcal{A} \subseteq \mathbb{R}^d$  and then suffers a loss that can be expressed as a linear function in action vectors. Although existing algorithms achieve an optimal regret bound of $\tilde{O}(\sqrt{T})$ for $T$ rounds (ignoring factors of $\mathrm{poly} (d  \log T)$)  computationally efficient ways of implementing them have not yet been specified  in particular when $|\mathcal{A}|$ is not bounded by a polynomial size in $d$. A standard way to pursue computational efficiency is to assume that we have an efficient algorithm referred to as \textit{oracle} that solves (offline) linear optimization problems over $\mathcal{A}$. Under this assumption  the computational efficiency of a bandit algorithm can then be measured in terms of \textit{oracle complexity}  i.e.  the number of oracle calls. Our contribution is to propose algorithms that offer optimal regret bounds of $\tilde{O}(\sqrt{T})$ as well as low oracle complexity for both \textit{non-stochastic settings} and \textit{stochastic settings}. Our algorithm for non-stochastic settings has an oracle complexity of $\tilde{O}( T )$ and is the first algorithm that achieves both a regret bound of $\tilde{O}( \sqrt{T} )$ and an oracle complexity of $\tilde{O} ( \mathrm{poly} ( T ) )$  given only linear optimization oracles. Our algorithm for stochastic settings calls the oracle only $O( \mathrm{poly} (d  \log T))$ times  which is smaller than the current best oracle complexity of $O( T )$ if $T$ is sufficiently large.,Oracle-Efﬁcient Algorithms for

Online Linear Optimization with Bandit Feedback∗

Shinji Ito†

NEC Corporation  The University of Tokyo

i-shinji@nec.com

Hanna Sumita

Tokyo Metropolitan University

sumita@tmu.ac.jp

Takuro Fukunaga‡

Chuo University  RIKEN AIP  JST PRESTO

fukunaga.07s@g.chuo-u.ac.jp

Daisuke Hatano

RIKEN AIP

daisuke.hatano@riken.jp

Kei Takemura
NEC Corporation

kei_takemura@nec.com

Naonori Kakimura§

Keio University

kakimura@math.keio.ac.jp

Ken-ichi Kawarabayashi§
National Institute of Informatics

k-keniti@nii.ac.jp

Abstract

We propose computationally efﬁcient algorithms for online linear optimization
with bandit feedback  in which a player chooses an action vector from a given
(possibly inﬁnite) set A ⊆ Rd  and then suffers a loss that can be expressed
√
as a linear function in action vectors. Although existing algorithms achieve an
optimal regret bound of ˜O(
T ) for T rounds (ignoring factors of poly(d  log T )) 
computationally efﬁcient ways of implementing them have not yet been speciﬁed 
in particular when |A| is not bounded by a polynomial size in d. A standard way to
pursue computational efﬁciency is to assume that we have an efﬁcient algorithm
referred to as oracle that solves (ofﬂine) linear optimization problems over A.
Under this assumption  the computational efﬁciency of a bandit algorithm can then
be measured in terms of oracle complexity  i.e.  the number of oracle calls. Our
contribution is to propose algorithms that offer optimal regret bounds of ˜O(
T )
as well as low oracle complexity for both non-stochastic settings and stochastic
√
settings. Our algorithm for non-stochastic settings has an oracle complexity of
˜O(T ) and is the ﬁrst algorithm that achieves both a regret bound of ˜O(
T ) and
an oracle complexity of ˜O(poly(T ))  given only linear optimization oracles. Our
algorithm for stochastic settings calls the oracle only O(poly(d  log T )) times 
which is smaller than the current best oracle complexity of O(T ) if T is sufﬁciently
large.

√

∗This work was supported by JST  ERATO  Grant Number JPMJER1201  Japan.
†This work was supported by JST  ACT-I  Grant Number JPMJPR18U5  Japan.
‡This work was supported by JST  PRESTO  Grant Number JPMJPR1759  Japan.
§This work was supported by JSPS  KAKENHI  Grant Number JP18H05291  Japan.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1

Introduction

√

(cid:80)T
t=1 (cid:96)(cid:62)

t at −(cid:80)T

t=1 (cid:96)(cid:62)

√

T )-regret bounds achieve optimal performance w.r.t. dependence on T .

Online linear optimization with bandit feedback  or bandit linear optimization  is an important problem
that has a wide range of applications. In it  a player is given A ⊆ Rd  referred to as a set of action
vectors  and T   the number of rounds of decision-making. In each round t ∈ [T ] := {1  2  . . .   T} 
t at  where (cid:96)t ∈ Rd is an unknown loss
the player chooses an action at ∈ A  and then observes loss (cid:96)(cid:62)
vector that can change over rounds. The bandit linear optimization includes a variety of important
online decision-making problems as special cases. For example  given a graph G = (V  E) and
s  t ∈ V   by setting A ⊆ R|E| to be the set of all characteristic vectors of s-t paths  we can take
into account bandit shortest path or adaptive routing [9]. In this setting  (cid:96)t ∈ R|E| corresponds to
(unknown) lengths of the edges  and bandit feedback (cid:96)(cid:62)
t at represents the length of a chosen s-t path
at. In addition to this application  bandit linear optimization includes bandit versions of combinatorial
optimization problems such as minimum spanning tree  minimum cut  and knapsack problem  as well
as continuous optimization problems such as linear programming and semideﬁnite programming.
The performance of the player is evaluated in terms of regret RT (a∗)  deﬁned as RT (a∗) =
t a∗ for a∗ ∈ A  which represents the difference between the cumulative
loss for decision {at} of the player and that for an arbitrarily ﬁxed strategy a∗. The primary goal in
bandit linear optimization is to achieve small regret for arbitrary a∗ ∈ A. Some existing algorithms
√
achieve regret bounds of ˜O(
T ) 1 as shown in Tables 1 and 2. In contrast  papers [6; 8; 12; 21]
√
showed that any algorithm will suffer at least Ω(
T ) regret in the worst case. Thus  algorithms with
˜O(
Algorithms achieving an optimal ˜O(
T )-regret  however  have computational issues  especially
when the action set A is exponentially large or is an inﬁnite set. For example  well-known LinUCB
methods [1; 16; 29] need to solve quadratic programming over A  which has time complexity of
Ω(|A|) if there are no additional assumptions. The ComBand algorithm [11] runs efﬁciently if
there is an efﬁcient sampling algorithm for A (such as k-sets  spanning trees  or bipartite perfect
matchings)  but such sampling algorithms are open for many important examples  including s-t
paths. For the special case in which the convex hull of A can be expressed by c linear inequalities 
CombExp [13] runs in O(poly(c  d)T )-time. However  c (the size of the linear inequality expression)
can be exponentially large for many examples.
In this study  we aim to develop computationally efﬁcient algorithms that achieve an ˜O(
T ) regret
bound  under the assumption that we can call a linear optimization oracle. The oracle solves
ofﬂine linear optimization problems over A  i.e  given a loss vector (cid:96) ∈ Rd  the oracle outputs
a∗ ∈ arg min
(cid:96)(cid:62)a. This assumption is standard in the context of online optimization [15; 23]. Under
a∈A
it  the computational efﬁciency of online optimization algorithms is evaluated in terms of oracle
complexity: the number of oracle calls for the linear optimization oracle.
For online linear optimization with full information  in which a player can observe all entries of
(cid:96)t ∈ Rd after choosing at  Kalai and Vempala [23] have proposed algorithms with an ˜O(
T )-regret
bound and an oracle complexity of O(T ). Using this algorithm  McMahan and Blum [26] and Dani
and Hayes [15] showed that one can achieve ˜O(T 2/3)-regret and O(T 1/2)-oracle complexity for
√
bandit linear optimization. However  it has been an open question as to whether or not we can
achieve ˜O(
T )-regret and ˜O(poly(T ))-oracle complexity for bandit linear optimization  with only
√
linear optimization oracles. In this study  we solve this open problem by proposing an algorithm that
achieves ˜O(
Here  we separately consider here two different settings for bandit linear optimization: a non-
stochastic setting and a stochastic setting. In the non-stochastic setting  we do not assume any
generative models  but (cid:96)t may be chosen in an adversarial manner  depending on previous actions
a1  . . .   at−1. The performance of an algorithm is measured in terms of the expectation of regret
RT (a∗) w.r.t. the randomness of the algorithm’s internal randomness and (cid:96)t. In the stochastic setting 
by way of contrast  the loss vectors (cid:96)t are assumed to follow a probability distribution D over Rd 
i.i.d. for t = 1  . . .   T .

T )-regret as well as ˜O(T )-oracle complexity.

√

√

1 In ˜O(·) notation  we ignore factors of polynomials in d and log(T ).

2

1.1 Our Contribution

√
In this paper  we present computationally efﬁcient algorithms that achieve O(poly(d)
T )-regret.
Speciﬁcally  we present algorithms with a small oracle complexity  i.e.  algorithms that call the oracle
as infrequently as possible. Our contribution is summarized in Tables 1 and 2.

For the non-stochastic setting  we propose an algorithm (Algorithm 1) that achieves O((cid:112)d3T log T )-

regret in expectation and has O(poly(d  log T )T )-oracle complexity.
Theorem 1. For the non-stochastic setting  Algorithm 1 satisﬁes the following conditions:

• The output of the algorithm satisﬁes E[RT (a∗)] = O((cid:112)d3T log T ) for all a∗ ∈ A.

• The algorithm calls the linear optimization oracle O(poly(d  log T )T ) times.
• The computational time  except for that of the oracle  is of O(poly(d  T )).
√
As shown in Table 1  our Algorithm 1 achieves the smallest oracle complexity among algorithms
T )-regret. Noting that GeometricHedge assumes A to be a convex body  we can see that
with ˜O(
Algorithm 1 is the ﬁrst algorithm that is applicable to discrete A and that achieves ˜O(
T )-regret and
˜O(poly(T ))-oracle complexity.
Although the ﬁrst algorithm in Table 1 with ˜O(T 2/3)-regret and O(T 2/3)-oracle complexity might
look incomparable to our results  algorithms with such bounds can be easily constructed from our
Algorithm 1. In fact  by dividing T rounds into T /B blocks of size B > 1 and regarding each block
as an individual round  we obtain the following statement:
Proposition 1. Suppose there exists an algorithm with O(f (T ))-regret and O(g(T ))-oracle com-
plexity. Then  for arbitrary positive integer B  there exists an algorithm with O(B · f (T /B))-regret
and O(g(T /B))-oracle complexity.

√

By setting the block size to be B = Θ(T 1/3) and applying Algorithm 1  we can achieve

O(B(cid:112)T /B) = ˜O(T 2/3)-regret and O(T /B) = ˜O(T 2/3)-oracle complexity  which is equiva-
O((cid:112)d3T log(d log T /δ))-regret with probability 1 − δ and has O(poly(d  log T ))-oracle
• The output of the algorithm satisﬁes RT (a∗) = O((cid:112)d3T log(d log T /δ)) for all a∗ ∈ A 

√
lent to the the uppermost result in Table 1. Note that Proposition 1 does not lead to an ˜O(
T )-regret
algorithm given an ˜O(T 2/3)-regret algorithm  conversely  since the block size B must be at least 1.
For
achieves
complexity  where δ ∈ (0  1) is an arbitrary parameter.
Theorem 2. Suppose (cid:96)t follows a distribution over Rd  i.i.d. for t = 1  2  . . .   T . Algorithm 2 then
satisﬁes the following conditions:

an algorithm (Algorithm 2)

that

the

stochastic

setting  we propose

with probability 1 − δ.

• The algorithm calls the linear optimization oracle O(poly(d  log T )) times.
• The computational time  except for that of the oracle  is of O(poly(d  T )).

√
A complete description of Algorithm 2 and a proof of this theorem are given in Appendix B. As
√
shown in Table 2  all existing algorithms that achieve ˜O(
T )-regret require at least Ω(T ) oracle
complexity  and our Algorithm 2 is the ﬁrst with an ˜O(
T )-regret bound and a sublinear oracle
complexity in T .
In both Algorithms 1 and 2  we use the well-known techniques [30] of reduction among linear
optimization  separation  and decomposition over a given convex body. Deﬁnitions of these three
problems are given in Section 4. The reduction algorithms enable us to solve separation and
decomposition problems by calling the linear optimization oracle O(poly(d)) times. Using these
2 In this algorithm  A is assumed to be a convex body  and a membership oracle for A is assumed. Because
we can construct a membership oracle from a linear optimization oracle and vice versa by a polynomial-time
reduction [30]  the assumption regarding the oracle is equivalent to ours  modulo polynomial-time reduction.

3

Table 1: Regret Bound and Oracle Complexity of Non-Stochastic Bandit Linear Optimization

Algorithm
MV algorithm [15; 26] with FPL [23]
ComBand [11]  GeometricHedge [17]  Exp2 [6]
GeometricHedge with Volumetric Spanners2[19]
Algorithm 1 [This paper]

Regret Bound Oracle Complexity
˜O(T 2/3)
˜O(T 1/2)
˜O(T 1/2)
˜O(T 1/2)

O(T 2/3)
−
˜O(T 7)
˜O(T )

Table 2: Regret Bound and Oracle Complexity for Stochastic Bandit Linear Optimization

Algorithm
LinRel [7]  LinUCB with (cid:96)2-ball [1; 16; 29]
LinUCB with (cid:96)1-ball [16] 
Linear Thompson sampling [2; 4]
Algorithm 2 [This paper]

Regret Bound Oracle Complexity
˜O(T 1/2)
˜O(T 1/2)
˜O(T 1/2)
˜O(T 1/2)

−
O(T )
O(T )
O(poly(d  log T ))

reduction techniques  Algorithms 1 and 2 maintain  respectively  supersets and subsets of the convex
hull of A (=: Conv(A)).
To construct Algorithm 1 for the non-stochastic setting  we extend a cutting-plane approach to our
bandit-feedback setting. The cutting-plane approach  a way of reducing oracle complexity  has been
applied only to full-information settings [20]  not a bandit-feedback setting. A major difference
between bandit-feedback and full-information settings is that the former needs exploration  i.e. 
chosen actions should be randomized with sufﬁciently large variance  whereas the latter does not
need it and chooses actions deterministically. In full-information settings  hence  it sufﬁces to focus
on a deterministically chosen action alone. In contrast to this  to deal with the bandit-feedback setting 
the difﬁculty lies in constructing a distribution of actions with sufﬁciently large variance  for which
cutting planes can be efﬁciently computed and the number of them can be bounded.
To this end  we design relevant probability distributions so that the cutting-plane approach works 
which successfully reduces oracle complexity. Speciﬁcally  the cutting-plane approach maintains
convex bodies Kt that include and approximate Conv(A)  from which we choose candidates for
actions  employing the support of the probability distribution of actions to choose. It is only when
some candidates are invalid  i.e.  when some are outside of Conv(A)  that Kt is updated with a
cutting plane excluding such an invalid candidate. To bound the number of oracle calls  we design
candidates for actions that satisfy two conditions: the set of candidates has a bounded cardinality  and
each candidate is sufﬁciently close to the weighted center of Kt. Thanks to the ﬁrst condition  we
can efﬁciently decide if invalid candidates exist. The second condition is essential for bounding the
number of oracle calls in each update of Kt.
Our Algorithm 2 for the stochastic setting is based on the framework of phased elimination of actions 
in which T rounds are divided into phases that are segments of consequent rounds  and  in each
phase  actions are eliminated so that only promising ones are left. Existing works employing this
framework [7; 24; 31] are computationally inefﬁcient  mainly for the following two reasons: (i) We
need to maintain a set of promising actions that may be an exponentially large combinatorial set  and 
(ii) when choosing actions  we need to solve hard optimization problems  e.g.  G-optimal design [24]
or quadratic programming [7].
Our idea for resolving the ﬁrst computational issue is to maintain the set of promising actions
as a convex set instead of a subset of actions. The convex set here can be represented with only
O(poly(d) log T ) linear inequalities  which implies that operations over it can be conducted efﬁ-
ciently. We resolve the second computational issue by combining barycentric spanners [9] and
the decomposition technique over convex bodies [30]  both of which are efﬁciently computed with
√
O(poly(d)) oracle calls. We show that  thanks to these techniques  we can estimate the loss vector
with enough accuracy to achieve an ˜O(
T )-regret bound. The oracle complexity is bounded as
follows: In our algorithm  all at chosen in each phase are determined at the beginning of the phase 
which means that the oracle complexity depends not on the number of rounds  but on the number
of phases. The number of phases is of O(poly(d) log T ) and that of oracle calls in each phase is of
O(poly(d  log T ))  which results in overall O(poly(d  log T ))-oracle complexity.

4

√

t at  Follow
T )-regret and O(T )-oracle

2 Related Work
For the full-information setting in which a player can observe (cid:96)t ∈ Rd rather than (cid:96)(cid:62)
√
the Perturbed Leader (FPL) by Kalai and Vempara [23] achieves O(
complexity. This algorithm is used as a subroutine in MV algorithm [15; 26] (see Table 2).
For a more general problem referred to as online improper learning  in which only an approximate
linear optimization oracle is given  Kakade et al. [22] have proposed the ﬁrst efﬁcient algorithms
that achieve approximate regret of O(
T ) for the full-feedback setting  and O(T 2/3) in the bandit-
feedback setting. Recent papers by Garber [18] and Hazan et al. [20] have improved oracle complexity.
Algorithms in [20] achieve oracle complexity of ˜O(T ) in the full-feedback setting and ˜O(T 2/3) in
the bandit feedback setting with the same regret bound as in Kakade et al. [22]. For online improper
learning with bandit feedback  however  constructing an efﬁcient algorithm achieving ˜O(
T ) poses
difﬁculties that have yet to be overcome.
In addition to the studies listed in Tables 1 and 2  there exist efﬁcient algorithms for bandit linear
optimization that work under different assumptions. Abernethy et al. [3] proposed a computationally
T )-regret under the assumption that A is a convex body and that
efﬁcient algorithm achieving O(
a self-concordant barrier [27] for A is given. However  constructing self-concordant barriers is not
always possible with a linear optimization oracle alone  and  hence  this algorithm does not always
work under our assumptions of linear optimization oracle and Assumption 1 given in the next section.

√

√

3 Problem Setting

The bandit linear optimization problem is a repeated game described as follows: Before the game
starts  a player is given the number T of rounds and the dimensionality d of the action set A ⊆ Rd.
In each round t = 1  2  . . .   T   the player chooses at ∈ A while an environment chooses a loss vector
(cid:96)t ∈ Rd  and then the player observes a loss (cid:96)(cid:62)
t at. The goal of the player is to achieve a small regret

RT (a)  which is deﬁned for an arbitrary a ∈ A as RT (a) :=(cid:80)T

t at −(cid:80)T

We assume the action set A to be a compact set. Suppose that we have an algorithm for linear
optimization over A for any vector w ∈ Rd  which we call linear optimization oracle OA : Rd → A
that receives an input w ∈ Rd and returns a point OA(w) ∈ K satisfying w(cid:62)OA(w) = mina∈A w(cid:62)a.
Assumption 1. We assume that there exist positive real numbers L and R such that (a) (cid:107)(cid:96)t(cid:107)2 ≤ L for
all t ∈ [T ]  and (b) (cid:107)a(cid:107)2 ≤ R for all a ∈ A. In addition  we assume that (c) K := Conv(A) has a

positive volume  i.e.  Vol(K) :=(cid:82)

t=1 (cid:96)(cid:62)
t a.

t=1 (cid:96)(cid:62)

K 1dx > 0.

The ﬁrst two assumptions (a) and (b) are standard in bandit linear optimization. If we are given
a linear optimization oracle over A  we can assume (c) without loss of generality. In fact  if A
is included in a subspace with a smaller dimension than d  we can then detect it by calling the
linear optimization oracle polynomial times (see  e.g.  Corollary 14.1g in [30])  and we can make K
full-dimensional by ignoring redundant dimensions.

4 Preliminaries

4.1 Linear Optimization  Separation  and Decomposition

We deﬁne a linear optimization problem (LP)  separation problem (SP)  and decomposition problem
(DP) for a compact convex body P ⊆ Rd as follows:
Problem 1 (linear optimization problem  LP). Given a vector w ∈ Rd  ﬁnd a vector x∗ ∈ P such that
w(cid:62)x∗ = minx∈P w(cid:62)x.
Problem 2 (separation problem  SP). Given a vector y ∈ Rd  decide whether y belong to P or not 
and  in the latter case  ﬁnd a vector w ∈ Rd such that w(cid:62)y < minx∈P w(cid:62)x.
Problem 3 (decomposition problem  DP). Given a vector x ∈ P  ﬁnd vertices x0  . . .   xd of P and
λ0  . . .   λd ≥ 0 such that x = λ0x0 + ··· + λdxd.
Ellipsoid methods provide reductions among these problems  which imply that
LP: solvable ⇐⇒ SP: solvable =⇒ DP: solvable.

5

Theorem 3 (Corollaries 14.1a  14.1b and 14.1g in [30]). Suppose that P ⊆ Rd is a polytope of
which each vertex can be expressed by rationals with bit-lengths of at most ϕ  and that each entry of
x  y  w ∈ Qd is also a rational  with bit-length of at most ϕ. Then  the following holds:
(a) If there is an algorithm SEP that solves the separation problem  we can solve the linear opti-

mization problem for w ∈ Qd by calling SEP at most poly(d  ϕ) times.

(b) If there is an algorithm OPT that solves the linear optimization problem  we can solve the

separation problem for y ∈ Qd by calling OPT at most poly(d  ϕ) times.

decomposition problem for x ∈ P by calling OPT at most poly(d  ϕ) times.

(c) If there is an algorithm OPT that solves the linear optimization problem  we can solve the
Remark 1. For any ε > 0 and any real number x ∈ [−1  1]  we can approximate x by a rational
ˆx ∈ Q with a bit-length of at most O(log(1/ε)) so that |x − ˆx| ≤ ε. Hence  we can assume that ϕ
in Theorem 3 is bounded as ϕ = O(log T ) by ignoring O(1/T ) errors. This implies that the above
reductions can be computed in O(poly(d  log T )) time.

4.2 Algorithms for Logconcave Distributions
If a probability distribution over convex body P ⊆ Rd has a probability density function (PDF)
p : P → R>0 such that log p is a concave function  we refer to it as a logconcave distribution. The
following theorem means that  given the value oracle of a convex function f : P → R  we can
approximately sample a vector in P from a logconcave distribution p(x) ∝ exp(−f (x)).
Theorem 4 (Theorems 2.1 and 2.2 in [25]  Lemma 10 in [19]). Let P ⊆ Rd be a convex body with
non-zero Lubesgue measure  and let f : P → R be a convex function and let p be a logconcave
distribution proportional to exp(−f (x)). Suppose ε > 0 and δ ∈ (0  1). Then  given access to the
membership oracle of P and the value oracle of f  there is an algorithm that samples approximately
from p such that (i) the total variation distance between the produced distribution and p is at most ε 
and (ii) after preprocessing for a time of O(d5(log d)O(1))  each sample can be produced in a time of
O(d4/ε4 · (log(d/ε))O(1)).
As an implication of this theorem  we can efﬁciently approximate mean µ(p) ∈ Rd and covariance
matrix Cov(p) ∈ Rd×d of distribution p. In fact  from Corollary 5.52 in [32] and standard concentra-
tion of logcancave distribution (see  e.g.  Lemma 5.17 in [25])  it takes (n log(1/δ)/ε)O(1) samples
to get a matrix ˆΣ such that (1 − ε)Cov(p) (cid:22) ˆΣ (cid:22) (1 + ε)Cov(p) with probability of at least 1 − δ.3
Similarly  we can get ˆµ ∈ Rd such that (cid:107)ˆµ − µ(p)(cid:107)Cov(p)−1 ≤ ε from (n log(1/δ)/ε)O(1) samples.4
Accordingly  we obtain the following corollary:
Corollary 1. Suppose the same assumption as in Theorem 4. There is an algorithm that outputs
a vector ˆµ ∈ Rd and a symmetric matrix ˆΣ ∈ Rd×d satisfying 1
2 Cov(p) (cid:22) ˆΣ (cid:22) 2Cov(p) and
(cid:107)ˆµ − µ(p)(cid:107)Cov(p)−1 ≤ ε with a probability of at least 1 − δ. The computational time  the number of
calls for the membership oracle of P  and the value oracle of f are bounded by poly(d  1
δ ).

ε   log 1

5 Algorithm for Non-stochastic Bandit Linear Optimization

Our algorithm uses the framework of a continuous multiplicative weight update (CMWU) [5; 14; 33].
A straightforward way of applying CMWU is to maintain probability distributions over K :=
Conv(A)  which  however  requires a large number of oracle calls. In fact  the algorithm by Hazan
and Karnin [19] for bandit linear optimization over convex bodies calls an oracle ˜O(T 7) times. This
inefﬁciency is due to that we need to sample from K; the sampling algorithm in Theorem 4 requires
O(d4/ε4)-oracle complexity.
We reduce oracle complexity by means of a cutting-plane approach [20]. In this approach  we
maintain convex bodies K(j)
that include and approximate K  and we update a distribution over
K(j)
instead of K. The advantage of this approach is that we can sample from K(j)
t without calling
√
3A similar argument can be found in Section 6.3 in [10].
4 For a vector x ∈ Rd and a positive semideﬁnite matrix A ∈ Rd×d  denote (cid:107)x(cid:107)A :=

x(cid:62)Ax.

t

t

6

t

t ⊆ K(j)

an oracle. On the other hand  updating K(j)
requires oracle calls; therefore  we need to bound the
number of the updates as well as the number of oracle calls in each update. We design a strategy
achieving these as follows: We set candidates of actions E (j)
  from which we choose action.
When some actions among the candidates are invalid  i.e.  outside of K  we then reduce K(j)
t by a
cutting plane excluding such an invalid candidate. With this strategy  we need oracle calls to check if
invalid candidates exist. Our algorithm bounds the oracle complexity here by setting E (j)
to have
O(d) elements. Further  we design E (j)
so that its elements are sufﬁciently close to the weighted
center of K(j)
. Indeed  when
a convex body is updated by a cutting plane that excludes a point close to its center  its volume then
decreases by a constant factor less than 1 (see  e.g.  [25]). On the other hand  K(j)
always includes K
with a positive volume; hence  the volume of K(j)
cannot be smaller than that of K  which implies
that the number of updates is bounded.

. This plays an important role in bounding the number of updates of K(j)

t

t

t

t

t

t

t

2 ⊇ ··· ⊇ K(s2)

5.1 Algorithm
Our algorithm maintains a convex body K(j)
1 =
K(0)
2 ⊇ K(1)
2 ⊇ ··· ⊇ K(sT )
T ⊇ K = Conv(A)  where t corresponds to the round 
j ∈ {0  1  . . .} is an index  and st ∈ {0  1  . . .   T} will be deﬁned later. It also updates a logconcave
function zt : Rd → R>0 in each round t based on the multiplicative weight update [5; 14; 33]. Before
the ﬁrst round  zt is initialized to be a constant function z1(x) = 1. Let q(j)
denote the PDF of a
distribution over K(j)

t ⊆ Rd such that K(0)

that is proportional to the function zt  i.e. 

1 ⊇ ··· ⊇ K(s1)

1 ⊇ K(1)

t

t

(cid:90)

K(j)

t

Z (j)

t =

zt(x)dx 

q(j)
t

(x) =

(cid:40) zt(x)

t

Z(j)
0

t

 

if a ∈ K(j)
if a ∈ Rd \ K(j)
t ∈
by µ(j)
t )(cid:62)]. From Corollary 1 

t ∈ Rd and Σ(j)

(1)

.

t

t

Let us denote the mean and the covariance matrix for distribution of q(j)
Rd×d  respectively: µ(j)
we can compute estimators ˆµ(j)

t = E a∼q(j)

[a]  Σ(j)

t

t

t

t = E a∼q(j)
and Σ(j)
(cid:107)ˆµ(j)

t

t

and ˆΣ(j)
t of µ(j)
t (cid:22) 2Σ(j)

 

t

t )(a−µ(j)

[(a−µ(j)
  respectively  such that
t (cid:107)(Σ(j)

1
2

t (cid:22) ˆΣ(j)
Σ(j)
with probability of at least 1 − δ(j)
t = (b(j)
B(j)

t − µ(j)
(2)
t ∈ (0  1)  which will be deﬁned later. Let
  where ε > 0 and δ(j)
(cid:26)
t B(j)(cid:62)
td ) ∈ Rd×d be a matrix such that B(j)
t1   . . .   b(j)
t − 1
E (j)
ˆµ(j)
t =
4e

(cid:27)
d . Deﬁne E (j)

(cid:12)(cid:12)(cid:12)(cid:12) i ∈ [d]

(cid:12)(cid:12)(cid:12)(cid:12) i ∈ [d]

t ⊆ Rd as

)−1 ≤ ε

ˆµ(j)
t +

= ˆΣ(j)

(cid:26)

(cid:27)

1
4e

b(j)
ti

b(j)
ti

(3)

∪

.

t

t

t

In each round t  our algorithm checks if E (j)
in Step 7 of Algorithm 1  to exclude an element in E (j)
four conditions are satisﬁed:

t

t

is included in K  and if not  it updates K(j)

\ K. Set E (j)

t

  as described
is designed so that the following

t

1. The cardinality of E (j)
O(poly(d)) oracle calls.

t

is bounded as |E (j)

| = O(d). Hence  we can decide if E (j)

t

2. Each y ∈ E (j)

t

is sufﬁciently close to µ(j)

t

  i.e.  it satisﬁes (cid:107)y − µ(j)

t (cid:107)(Σ(j)

t

is important to bound the number of oracle calls.

t ⊆ K by
)−1 ≤ 1/(2e). This

3. The mean of E (j)

t

E (j)

t

  we then have E[(cid:96)(cid:62)

is equal to ˆµ(j)
t y] = (cid:96)(cid:62)

t

. This implies that if y follows a uniform distribution over
t ≈ (cid:96)(cid:62)

t x] for x ∼ q(j)

t = E[(cid:96)(cid:62)

t µ(j)

t ˆµ(j)

.

t

Thanks to this  empirical estimates of (cid:96)t based on E (j)

4. The covariance matrix Σ of a uniform distribution over E (j)

satisﬁes Σ (cid:23) O(1/d2) · Σ(j)
.
t will have a sufﬁciently small variance.
The conditions 1 and 2 are used to bound the oracle complexity  and 3 and 4 are necessary to bound
the regret. Once E (j)
. An integer st

is included in K  our algorithm escapes the loop of updating K(j)

t

t

t

t

7

t

Assumption 1.

for j = 0  1  2  . . . do

on the basis of (1) ∼ (3).

1 = B∞(0  R) = {x ∈ Rd | (cid:107)x(cid:107)∞ ≤ R} and deﬁne z1 : Rd → R>0 by z1(x) = 1.

Compute E (j)
Solve SP for P = K and for each y ∈ E (j)
if There is a hyperplane w ∈ Rd s.t. w(cid:62)y < minx∈K w(cid:62)x for some y ∈ E (j)

Algorithm 1 An oracle efﬁcient algorithm for non-stochastic bandit linear optimization
Require: Learning rate η > 0  error bound ε > 0  time horizon T ∈ N  R > 0 satisfying
1: Set K(0)
2: for t = 1  2  . . .   T do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

end if
end for
Let ˆµt = ˆµ(st)
Choose it ∈ [d] and σt ∈ {1 −1} uniformly at random.
Solve DP for P = K and x = ˆµt + σt
4e btit = λt0xt0 + ··· + λtdxtd.
λt0  . . .   λtd such that ˆµt + σt
Play at = xts with probability λts (s = 0  . . .   d)  and receive loss (cid:96)(cid:62)
t (x − ˆµt)).
Set ˆ(cid:96)t by (4) and update zt by zt+1(x) = zt(x) exp(−η ˆ(cid:96)(cid:62)
Set K(0)

t ∩ {x ∈ Rd | w(cid:62)x ≥ w(cid:62)y}.
. Break the for loop w.r.t. the index j.

4e btit to get a decomposition xt0  . . .   xtd ∈ A and

Update K(j)
Set st = j and Kt = K(st)

for i ∈ [n]  which are deﬁned in (1) – (3).

and bti = b(st)

by K(j+1)

= K(j)

t at.

then

else

t+1 = K(st)

ti

.

.

t

t

t

t

t

t

t

15:
16:
17:
18: end for

t

denotes the number of the updates in the round t. We denote Et = E (st)
  and
. We randomly choose x from Et as follows: choose σt ∈ {−1  1} and it ∈ [d] uniformly
Bt = B(st)
at random  and deﬁne x = ˆµt + σt
4e btit. If we can play this x  then we can construct a good estimate
of (cid:96)t from the above condition 4  which leads to a small degree of regret. However  x ∈ Et does not
always belong to A  particularly when A is discrete. To address this issue  we solve DP for this x
and P = K to derive a decomposition of x  i.e.  compute xt0  . . .   xtd ∈ K and λt0  . . .   λtd ≥ 0 as
in Step 14. Then  the algorithm plays at = xti with probability λti  and obtains feedback of (cid:96)(cid:62)
t at.
Based on this feedback  we compute an estimator ˆ(cid:96)t of the loss vector (cid:96)t as

  ˆΣt = ˆΣ(st)

  ˆµt = ˆµ(st)

t

t

t

ˆ(cid:96)t = 4edσt(cid:96)(cid:62)

t at ˆΣ−1

t btit.

(4)

t

This is an unbiased estimator of (cid:96)t  i.e.  we have E[ˆ(cid:96)t] = (cid:96)t. The existence of ˆΣ−1
deﬁnition of ˆΣ and Assumption 1. In fact  from A ⊆ K(j)
volume and q(j)
q(j)
t
updated by zt+1(x) = zt(x) exp(−η ˆ(cid:96)(cid:62)
the learning rate  which will be optimized later. Let

follows from the
has a positive
  which implies that the covariance matrix Σ(j)
t of
is positive deﬁnite for all t and j. The function zt is
t (x − ˆµt))  where η > 0 is an input parameter standing for

has a positive density over K(j)
t
is positive deﬁnite. From this and (2)  ˆΣ(j)

and Assumption 1  K(j)

t = 1/(T (j + 2 +(cid:80)t−1

i=1(si + 1))(j + 3 +(cid:80)t−1

i=1(si + 1))).

δ(j)

(5)

t

t

t

t

t

t

and ˆµ(j)

satisfying (2) with probability at least 1 − δ(j)

To compute ˆΣ(j)
Corollary 1.

t=1 st denote the number of updates of K(j)

Let ST =(cid:80)T
E[RT (a∗)] ≤ 27eLRd3/2 max{(cid:112)T (1 + ψ + log T )  d(1 + ψ + log T )2}(cid:0)1 − ST /210(cid:1) .

. Suppose at is given by Algorithm 1 with parameters
24d3/2(1+ψ+log T )}. Then  for all a∗ ∈ A  we have

2eLR min{(cid:113) 1+ψ+log T

d log Vol(B∞(0 R))
 

. We show the following regret bound.

Theorem 5. Deﬁne ψ = 1

12eT and η = 1

  we use the algorithm in

ε = 1

Vol(K)

dT

1

t

t

(6)
r if K includes an (cid:96)∞-ball of radius r > 0.

We note that ψ in the above theorem satisﬁes ψ ≤ log R
The proof of this theorem is given in Appendix A.

8

5.2 Oracle Complexity Analysis

(cid:80)st
j=0 |E (j)

t

t

t=1

t

t

  the number of elements in E (j)

of solutions of SP is(cid:80)T
which contradicts to RT (a∗) = (cid:80)T

Here  we show that Algorithm 1 calls the linear optimization oracles only O(poly(d)T ) times.
To implement Algorithm 1  the linear optimization oracle is required only in Steps 5 and 14. In Step
5  we need to solve SP to decide if there exists x ∈ E (j)
such that x /∈ K. From the deﬁnition (3) of
E (j)
is equal to 2d for each t and j  and  accordingly  the total number
t=1(st + 1) = 2d(T + ST ). The number ST can be
bounded as ST = O(T ). Indeed  from Theorem 5  if ST > 210(1+T ) then E[RT (a∗)] < −27eLRT  
t (at − a∗) ≥ −2LRT . Consequently  the total number
of solutions of SP is O(dT ). In Step 14  we solve DP in each round t; hence the total number of
solutions of DP is equal to T . Because we can solve SP and DP by calling the linear optimization
oracle poly(d  log T ) times from Theorem 4 and Remark 1  we can implement Algorithm 1 so that it
calls the linear optimization oracle O(poly(d  log T )T ) times.

| = 2d(cid:80)T

t=1 (cid:96)(cid:62)

References
[1] Y. Abbasi-Yadkori  D. Pál  and C. Szepesvári. Improved algorithms for linear stochastic bandits.

In Advances in Neural Information Processing Systems  pages 2312–2320  2011.

[2] M. Abeille  A. Lazaric  et al. Linear thompson sampling revisited. Electronic Journal of

Statistics  11(2):5165–5197  2017.

[3] J. D. Abernethy  E. Hazan  and A. Rakhlin. An efﬁcient algorithm for bandit linear optimization.

In Conference on Learning Theory  2008.

[4] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In

International Conference on Machine Learning  pages 127–135  2013.

[5] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta-algorithm

and applications. Theory of Computing  8(1):121–164  2012.

[6] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In

Conference on Learning Theory  pages 217–226  2009.

[7] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3(Nov):397–422  2002.

[8] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[9] B. Awerbuch and R. D. Kleinberg. Adaptive routing with end-to-end feedback: Distributed
learning and geometric approaches. In Proceedings of the Thirty-sixth Annual ACM Symposium
on Theory of Computing  pages 45–53. ACM  2004.

[10] S. Bubeck  Y. T. Lee  and R. Eldan. Kernel-based methods for bandit convex optimization.
In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing  pages
72–85. ACM  2017.

[11] N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System

Sciences  78(5):1404–1422  2012.

[12] A. Cohen  T. Hazan  and T. Koren. Tight bounds for bandit combinatorial optimization. In

Conference on Learning Theory  pages 629–642  2017.

[13] R. Combes  M. S. T. M. Shahi  A. Proutiere  and M. Lelarge. Combinatorial bandits revisited.

In Advances in Neural Information Processing Systems  pages 2116–2124  2015.

[14] T. M. Cover. Universal portfolios. In The Kelly Capital Growth Investment Criterion: Theory

and Practice  pages 181–209. World Scientiﬁc  2011.

9

[15] V. Dani and T. P. Hayes. Robbing the bandit: Less regret in online geometric optimization
against an adaptive adversary. In Proceedings of the Seventeenth Annual ACM-SIAM Symposium
on Discrete Algorithm  pages 937–943  2006.

[16] V. Dani  T. P. Hayes  and S. M. Kakade. Stochastic linear optimization under bandit feedback.

In Conference on Learning Theory  pages 355–366  2008.

[17] V. Dani  S. M. Kakade  and T. P. Hayes. The price of bandit information for online optimization.

In Advances in Neural Information Processing Systems  pages 345–352  2008.

[18] D. Garber. Efﬁcient online linear optimization with approximation algorithms. In Advances in

Neural Information Processing Systems  pages 627–635  2017.

[19] E. Hazan and Z. Karnin. Volumetric spanners: an efﬁcient exploration basis for learning. The

Journal of Machine Learning Research  17(1):4062–4095  2016.

[20] E. Hazan  W. Hu  Y. Li  and z. li. Online improper learning with an approximation oracle. In

Advances in Neural Information Processing Systems 31  pages 5652–5660. 2018.

[21] S. Ito  D. Hatano  H. Sumita  K. Takemura  T. Fukunaga  N. Kakimura  and K. Kawarabayashi.
Improved regret bounds for bandit combinatorial optimization. In Advances in Neural Informa-
tion Processing Systems  2019  to appear.

[22] S. M. Kakade  A. T. Kalai  and K. Ligett. Playing games with approximation algorithms. SIAM

Journal on Computing  39(3):1088–1106  2009.

[23] A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. Journal of

Computer and System Sciences  71(3):291–307  2005.

[24] T. Lattimore and C. Szepesvári. Bandit Algorithms. preprint  Revision: 1699  2019.

[25] L. Lovász and S. Vempala. The geometry of logconcave functions and sampling algorithms.

Random Structures & Algorithms  30(3):307–358  2007.

[26] H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an
adaptive adversary. In International Conference on Computational Learning Theory  pages
109–123  2004.

[27] Y. Nesterov and A. Nemirovskii. Interior-Point Polynomial Algorithms in Convex Programming 

volume 13. SIAM  1994.

[28] A. Prékopa. Logarithmic concave measures with application to stochastic programming. Acta

Scientiarum Mathematicarum  32:301–316  1971.

[29] P. Rusmevichientong and J. N. Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research  35(2):395–411  2010.

[30] A. Schrijver. Theory of Linear and Integer Programming. John Wiley & Sons  1998.

[31] M. Valko  R. Munos  B. Kveton  and T. Kocák. Spectral bandits for smooth graph functions. In

International Conference on Machine Learning  pages 46–54  2014.

[32] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint

arXiv:1011.3027  2010.

[33] V. G. Vovk. Aggregating strategies. Proc. of Computational Learning Theory  1990  1990.

10

,Shinji Ito
Daisuke Hatano
Hanna Sumita
Kei Takemura
Takuro Fukunaga
Naonori Kakimura
Ken-Ichi Kawarabayashi