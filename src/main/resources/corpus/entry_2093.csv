2019,Few-shot Video-to-Video Synthesis,Video-to-video synthesis (vid2vid) aims at converting an input semantic video  such as videos of human poses or segmentation masks  to an output photorealistic video. While the state-of-the-art of vid2vid has advanced significantly  existing approaches share two major limitations. First  they are data-hungry. Numerous images of a target human subject or a scene are required for training. Second  a learned model has limited generalization capability. A pose-to-human vid2vid model can only synthesize poses of the single person in the training set. It does not generalize to other humans that are not in the training set. To address the limitations  we propose a few-shot vid2vid framework  which learns to synthesize videos of previously unseen subjects or scenes by leveraging few example images of the target at test time. Our model achieves this few-shot generalization capability via a novel network weight generation module utilizing an attention mechanism. We conduct extensive experimental validations with comparisons to strong baselines using several large-scale video datasets including human-dancing videos  talking-head videos  and street-scene videos. The experimental results verify the effectiveness of the proposed framework in addressing the two limitations of existing vid2vid approaches.,Few-shot Video-to-Video Synthesis

Ting-Chun Wang  Ming-Yu Liu  Andrew Tao  Guilin Liu  Jan Kautz  Bryan Catanzaro

{tingchunw mingyul atao guilinl jkautz bcatanzaro}@nvidia.com

NVIDIA Corporation

Abstract

Video-to-video synthesis (vid2vid) aims at converting an input semantic video 
such as videos of human poses or segmentation masks  to an output photorealistic
video. While the state-of-the-art of vid2vid has advanced signiÔ¨Åcantly  existing
approaches share two major limitations. First  they are data-hungry. Numerous
images of a target human subject or a scene are required for training. Second  a
learned model has limited generalization capability. A pose-to-human vid2vid
model can only synthesize poses of the single person in the training set. It does not
generalize to other humans that are not in the training set. To address the limitations 
we propose a few-shot vid2vid framework  which learns to synthesize videos of
previously unseen subjects or scenes by leveraging few example images of the target
at test time. Our model achieves this few-shot generalization capability via a novel
network weight generation module utilizing an attention mechanism. We conduct
extensive experimental validations with comparisons to strong baselines using
several large-scale video datasets including human-dancing videos  talking-head
videos  and street-scene videos. The experimental results verify the effectiveness
of the proposed framework in addressing the two limitations of existing vid2vid
approaches. Code is available at our website.

Introduction

1
Video-to-video synthesis (vid2vid) refers to the task of converting an input semantic video to an
output photorealistic video. It has a wide range of applications  including generating a human-dancing
video using a human pose sequence [7  12  57  67]  or generating a driving video using a segmentation
mask sequence [57]. Typically  to obtain such a model  one begins with collecting a training dataset
for the target task. It could be a set of videos of a target person performing diverse actions or a set
of street-scene videos captured by using a camera mounted on a car driving in a city. The dataset is
then used to train a model that converts novel input semantic videos to corresponding photorealistic
videos at test time. In other words  we expect a vid2vid model for humans can generate videos of
the same person performing novel actions that are not in the training set and a street-scene vid2vid
model can videos of novel street-scenes with the same style as those in the training set. With the
advance of the generative adversarial networks (GANs) framework [13] and its image-conditional
extensions [22  58]  existing vid2vid approaches have shown promising results.
We argue that generalizing to novel input semantic videos is insufÔ¨Åcient. One should also aim for
a model that can generalize to unseen domains  such as generating videos of human subjects that
are not included in the training dataset. More ideally  a vid2vid model should be able to synthesize
videos of unseen domains by leveraging just a few example images given at test time. If a vid2vid
model cannot generalize to unseen persons or scene styles  then we must train a model for each new
subject or scene style. Moreover  if a vid2vid model cannot achieve this domain generalization
capability with only a few example images  then one has to collect many images for each new subject
or scene style. This would make the model not easily scalable. Unfortunately  existing vid2vid
approaches suffer from these drawbacks as they do not consider such generalization.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Comparison between the vid2vid (left) and the proposed few-shot vid2vid (right).
Existing vid2vid methods [7  12  57] do not consider generalization to unseen domains. A trained
model can only be used to synthesize videos similar to those in the training set. For example  a
vid2vid model can only be used to generate videos of the person in the training set. To synthesize
a new person  one needs to collect a dataset of the new person and uses it to train a new vid2vid
model. On the other hand  our few-shot vid2vid model does not have the limitations. Our model
can synthesize videos of new persons by leveraging few example images provided at the test time.

To address these limitations  we propose the few-shot vid2vid framework. The few-shot vid2vid
framework takes two inputs for generating a video  as shown in Figure 1. In addition to the input
semantic video as in vid2vid  it takes a second input  which consists of a few example images
of the target domain made available at test time. Note that this is absent in existing vid2vid
approaches [7  12  57  67]. Our model uses these few example images to dynamically conÔ¨Ågure
the video synthesis mechanism via a novel network weight generation mechanism. SpeciÔ¨Åcally  we
train a module to generate the network weights using the example images. We carefully design the
learning objective function to facilitate learning the network weight generation module.
We conduct extensive experimental validation with comparisons to various baseline approaches using
several large-scale video datasets including dance videos  talking head videos  and street-scene videos.
The experimental results show that the proposed approach effectively addresses the limitations of
existing vid2vid frameworks. Moreover  we show that the performance of our model is positively
correlated with the diversity of the videos in the training dataset  as well as the number of example
images available at test time. When the model sees more different domains in the training time  it can
better generalize to deal with unseen domains (Figure 7(a)). When giving the model more example
images at test time  the quality of synthesized videos improves (Figure 7(b)).
2 Related Work
GANs. The proposed few-shot vid2vid model is based on GANs [13]. SpeciÔ¨Åcally  we use
a conditional GAN framework. Instead of generating outputs by converting samples from some
noise distribution [13  42  32  14  25]  we generate outputs based on user input data  which allows
more Ô¨Çexible control over the outputs. The user input data can take various forms  including
images [22  68  30  41]  categorical labels [39  35  65  4]  textual descriptions [43  66  62]  and
videos [7  12  57  67]. Our model belongs to the last one. However  different from the existing video-
conditional GANs  which take the video as the sole data input  our model also takes a set of example
images. These example images are provided at test time  and we use them to dynamically determine
the network weights of our video synthesis model through a novel network weight generation module.
This helps the network generate videos of unseen domains.
Image-to-image synthesis  which transfers an input image from one domain to a corresponding
image in another domain [22  50  3  46  68  30  21  69  58  8  41  31  2]  is the foundation of vid2vid.
For videos  the new challenge lies in generating sequences of frames that are not only photorealistic
individually but also temporally consistent as a whole. Recently  the FUNIT [31] was proposed for
generating images of unseen domains via the adaptive instance normalization technique [19]. Our
work is different in that we aim for video synthesis and achieve generalization to unseen domains via
a network weight generation scheme. We compare these techniques in the experiment section.
Video generative models can be divided into three main categories  including 1) unconditional video
synthesis models [54  45  51]  which convert random noise samples to video clips  2) future video
prediction models [48  24  11  34  33  63  55  56  10  53  29  27  18  28  16  40]  which generate
future video frames based on the observed ones  and 3) vid2vid models [57  7  12  67]  which
convert semantic input videos to photorealistic videos. Our work belongs to the last category  but

2

few-shot vid2vidExample images of person 1Example images of person NOutput videosInput videosvid2vid for person 1vid2vid for person NOutput videosInput videosin contrast to the prior works  we aim for a vid2vid model that can synthesize videos of unseen
domains by leveraging few example images given at test time.
Adaptive networks refer to networks where part of the weights are dynamically computed based
on the input data. This class of networks has a different inductive bias to regular networks and has
found use in several tasks including sequence modeling [15]  image Ô¨Åltering [23  59  49]  frame
interpolation [38  37]  and neural architecture search [64]. Here  we apply it to the vid2vid task.
Human pose transfer synthesizes a human in an unseen pose by utilizing an image of the human in
a different pose. To achieve high quality generation results  existing human pose transfer methods
largely utilize human body priors such as body part modeling [1] or human surface-based coordinate
mapping [36]. Our work differs from these works in that our method is more general. We do not use
speciÔ¨Åc human body priors other than the input semantic video. As a result  the same model can
be directly used for other vid2vid tasks such as street scene video synthesis  as shown in Figure 5.
Moreover  our model is designed for video synthesis  while existing human pose transfer methods are
mostly designed for still image synthesis and do not consider the temporal aspect of the problem. As
a result  our method renders more temporally consistent results (Figure 4).

3 Few-shot Video-to-Video Synthesis
Video-to-video synthesis aims at learning a mapping function that can convert a sequence of input
1 ‚â° Àúx1  Àúx2  ...  ÀúxT   in a
semantic images1  sT
way that the conditional distribution of ÀúxT
1 is similar to the conditional distribution of the
1 . In other words  it aims to achieve
ground truth image sequence  xT
1 )) ‚Üí 0  where D is a distribution divergence measure such as the Jensen-
D(p(ÀúxT
Shannon divergence or the Wasserstein distance. To model the conditional distribution  existing
works make a simpliÔ¨Åed Markov assumption  leading to a sequential generative model given by

1 ‚â° s1  s2  ...  sT   to a sequence of output images  ÀúxT

1 ‚â° x1  x2  ...  xT   given sT

1 given sT

1 )  p(xT

1 |sT

1 |sT

Àúxt = F (Àúxt‚àí1

t‚àíœÑ   st

t‚àíœÑ )

(1)

In other words  it generates the output image  Àúxt  based on the observed œÑ + 1 input semantic images 
t‚àíœÑ   and the past œÑ generated images  Àúxt‚àí1
t‚àíœÑ . The sequential generator F can be modeled in several
st
different ways [7  12  57  67]. A popular choice is to use an image matting function given by

t‚àíœÑ   st

F (Àúxt‚àí1

t‚àíœÑ ) = (1 ‚àí Àúmt) (cid:12) Àúwt‚àí1(Àúxt‚àí1) + Àúmt (cid:12) Àúht

(2)
where the symbol 1 is an image of all ones  (cid:12) is the element-wise product operator  Àúmt is a soft
occlusion map  Àúwt‚àí1 is the optical Ô¨Çow from t ‚àí 1 to t  and Àúht is a synthesized intermediate image.
Figure 2(a) visualizes the vid2vid architecture and the matting function  which shows the output
image Àúxt is generated by combining the optical-Ô¨Çow warped version of the last generated image 
Àúwt‚àí1(Àúxt‚àí1)  and the synthesized intermediate image  Àúht. The soft occlusion map  Àúmt  dictates
how these two images are combined at each pixel location. Intuitively  if a pixel is observed in the
previously generated frame  it would favor duplicating the pixel value from the warped image. In
practice  these quantities are generated via neural network-parameterized functions M  W   and H:

Àúmt = MŒ∏M (Àúxt‚àí1
t‚àíœÑ   st
Àúwt‚àí1 = WŒ∏W (Àúxt‚àí1
Àúht = HŒ∏H (Àúxt‚àí1
t‚àíœÑ   st

t‚àíœÑ   st
t‚àíœÑ )

t‚àíœÑ ) 

t‚àíœÑ ) 

(3)
(4)
(5)

where Œ∏M   Œ∏W   and Œ∏H are learnable parameters. They are kept Ô¨Åxed once the training is done.
Few-shot vid2vid. While the sequential generator in (1) is trained for converting novel input semantic
videos  it is not trained for synthesizing videos of unseen domains. For example  a model trained
for a particular person can only be used to generate videos of the same person. In order to adapt
F to unseen domains  we let F depend on extra inputs. SpeciÔ¨Åcally  we let F take two more input
arguments: one is a set of K example images {e1  e2  ...  eK} of the target domain  and the other is
the set of their corresponding semantic images {se1  se2  ...  seK}. That is

Àúxt = F (Àúxt‚àí1

t‚àíœÑ   st

t‚àíœÑ  {e1  e2  ...  eK} {se1  se2  ...  seK}).

(6)

1For example  a segmentation mask or an image denoting a human pose.

3

Figure 2: (a) Architecture of the vid2vid framework [57]. (b) Architecture of the proposed few-shot
vid2vid framework. It consists of a network weight generation module E that maps example images
to part of the network weights for video synthesis. The module E consists of three sub-networks: EF  
EP   and EA (used when K > 1). The sub-network EF extracts features q from the example images.
When there are multiple example images (K > 1)  EA combines the extracted features by estimating
soft attention maps Œ± and weighted averaging different extracted features. The Ô¨Ånal representation is
then fed into the network EP to generate the weights Œ∏H for the image synthesis network H.

Œ∏H = E(Àúxt‚àí1

t‚àíœÑ   st

This modeling allows F to leverage the example images given at the test time to extract some useful
patterns to synthesize videos of the unseen domain. We propose a network weight generation module
E for extracting the patterns. SpeciÔ¨Åcally  E is designed to extract patterns from the provided
example images and use them to compute network weights Œ∏H for the intermediate image synthesis
network H:

t‚àíœÑ  {e1  e2  ...  eK} {se1   se2  ...  seK}).

(7)
Note that the network E does not generate the weights Œ∏M or Œ∏W because the Ô¨Çow prediction network
W and the soft occlusion map prediction network W are designed for warping the last generated
image  and warping is a mechanism that is naturally shared across domains.
We build our few-shot vid2vid framework based on Wang et al. [57]  which is the state-of-the-art
for the vid2vid task. SpeciÔ¨Åcally  we reuse their proposed Ô¨Çow prediction network W and the soft
occlusion map prediction network M. The intermediate image synthesis network H is a conditional
image generator. Instead of reusing the architecture proposed by Wang et al. [57]  we adopt the
SPADE generator [41]  which is the current state-of-the-art semantic image synthesis model.
The SPADE generator contains several spatial modulation branches and a main image synthesis branch.
Our network weight generation module E only generates the weights for the spatial modulation
branches. This has two main advantages. First  it greatly reduces the number of parameters that E
has to generate  which avoids the overÔ¨Åtting problem. Second  it avoids creating a shortcut from
the example images to the output image  since the generated weights are only used in the spatial
modulation modules  which generates the modulation values for the main image synthesis branch. In
the following  we discuss details of the design of the network E and the learning objective.
Network weight generation module. As discussed above  the goal of the network weight generation
module E is to learn to extract appearance patterns that can be injected into the video synthesis branch
by controlling its weights. We Ô¨Årst consider the case where only one example image is available
(K = 1). We then extend the discussion to handle the case of multiple example images.
We decompose E into two sub-networks: an example feature extractor EF   and a multi-layer
perceptron EP . The network EF consists of several convolutional layers and is applied on the
example image e1 to extract an appearance representation q. The representation q is then fed into
EP to generate the weights Œ∏H in the intermediate image synthesis network H.
Let the image synthesis network H has L layers H l  where l ‚àà [1  L]. We design the weight
generation network E to also have L layers  each El generates the weights for the corresponding H l.
SpeciÔ¨Åcally  to generate the weights Œ∏l
H for layer H l  we Ô¨Årst take the output ql from l-th layer in

4

weight generation module (E)SPADEResBlkSPADEResBlkSPADEResBlkconvconvmodified SPADE generator (H)ùúΩùëØùüêùúΩùëØùüëùúΩùëØùüèFC(c) Intermediate image synthesis network (for ùêæ=1)(a) vid2vidùêªùúΩùêªùëÄùëäùíîùë°‚àíùëáùë°‡∑•ùíôùë°‚àíùëáùë°‚àí1‡∑•ùíéùë°‡∑©ùíâùë°‡∑•ùíòùë°‡∑•ùíôùë°‚àíùüè‡∑•ùíòùë°(‡∑•ùíôùë°‚àíùüè)WarpMatting‡∑•ùíôùë°ùúΩùëÄùúΩùëäfixed(b) Few-shot vid2vidùê∏ùêπùê∏ùêπùê∏ùê¥ùê∏ùê¥ùíÜ1ùíÜùêæùú∂1ùú∂ùêæùê∏ùê¥ùíÇ1ùíÇùêæùíÇùë°ùíÇùë°ùíîùíÜ1ùíîùíÜùë≤ùíí1ùííùêæùíît‡∑çùëò=1ùêæùííùëò‚äóùú∂ùëòùê∏ùëÉùêªùúΩùêªdynamicùíît‡∑©ùíâùë°ùíÜ1ùê∏ùêπùê∏ùëÉFCFCùê™ùüèùê™ùüêùê™ùüëùíëùë∫ùüèùíëùë∫ùüêùíëùë∫ùüëAvgPoolAvgPoolAvgPoolconvconvconvconvconvconvconv(cid:1)  otherwise

if l = 0

(cid:126) Œ∏l

S

Œ≥l = pl
S

Œ≥  Œ≤l = pl
S

(cid:126) Œ∏l

Œ≤

(cid:126) Œ∏l
H = Œ≥l (cid:12) ÀÜpl
pl

H + Œ≤l

(8)

(9)

(10)

H = El

F (ql‚àí1)  and Œ∏l

P to generate the weights Œ∏l

EF . Then  we average pool ql (since ql might be still a feature map with spatial dimensions.) and
H. Mathematically  if we deÔ¨Åne q0 ‚â° e1 
apply a multi-layer perceptron El
then ql = El
P (ql). These generated weights are then used to convolve the
current input semantic map st to generate the normalization parameters used in SPADE (Figure 2(c)).
For each layer in the main SPADE generator  we use Œ∏l
H to compute the denormalization parameters
Œ≥l and Œ≤l to denormalize the input features. We note that  in the original SPADE module  the scale
map Œ≥l and bias map Œ≤l are generated by Ô¨Åxed weights operated on the input semantic map st. In
H contains three sets
our setting  these maps are generated by dynamic weights  Œ∏l
S  Œ∏l
of weights: Œ∏l
Œ≥ and Œ∏l
Œ≥ and Œ∏l
Œ≤
take the output of Œ∏l
S to generate Œ≥l and Œ≤l maps  respectively. For each BatchNorm layer in Gl  we
compute the denormalized features pl

S acts as a shared layer to extract common features  and Œ∏l

H from the normalized features ÀÜpl

H. Moreover  Œ∏l

Œ≤. Œ∏l

H by

(cid:26)st 
œÉ(cid:0)pl‚àí1

S

pl

S =

where (cid:126) stands for convolution  and œÉ is the nonlinearity function.
Attention-based aggregation (K > 1). In addition  we want E to be capable of extracting the
patterns from an arbitrary number of example images. As different example images may carry
different appearance patterns  and they have different degrees of relevance to different input images 
we design an attention mechanism [61  52] to aggregate the extracted appearance patterns q1 ... qK.
To this end  we construct a new attention network EA  which consists of several fully convolutional
layers. EA is applied to each of the semantic images of the example images  sek. This results in
a key vector ak ‚àà RC√óN   where C is the number of channels and N = H √ó W is the spatial
dimension of the feature map. We also apply EA to the current input semantic image st to extract its
key vector at ‚àà RC√óN . We then compute the attention weight Œ±k ‚àà RN√óN by taking the matrix
product Œ±k = (ak)T ‚äó at. The attention weights are then used to compute a weighted average of the
k=1 qk ‚äó Œ±k  which is then fed into the multi-layer perceptron EP
to generate the network weights (Figure 2(b)). This aggregation mechanism is helpful when different
example images contain different parts of the subject. For example  when example images include
both front and back of the target person  the attention maps can help capture corresponding body
parts during synthesis (Figure 7(c)).
Warping example images. To ease the burden of the image synthesis network  we can also (op-
tionally) warp the given example image and combine it with the intermediate synthesized output Àúht.
SpeciÔ¨Åcally  we make the model estimate an additional Ô¨Çow Àúwet and mask Àúmet  which are used to
warp the example image e1 to the current input semantics  similar to how we warp and combine with
previous frames. The new intermediate image then becomes

appearance representation q =(cid:80)K

t = (1 ‚àí Àúmet) (cid:12) Àúwet(e1) + Àúmet (cid:12) Àúht
Àúh(cid:48)

(11)
In the case of multiple example images  we pick e1 to be the image that has the largest similarity
score to the current frame by looking at the attention weights Œ±. In practice  we found this helpful
when example and target images are similar in most regions  such as synthesizing poses where the
background remains static.
Training. We use the same learning objective as in the vid2vid framework [57]. But instead of
training the vid2vid model using data from one domain  we use data from multiple domains. In
Figure 7(a)  we show the performance of our few-shot vid2vid model is positively correlated
with the number of domains included in the training dataset. This shows that our model can gain from
increased visual experiences. Our framework is trained in the supervised setting where paired sT
1  
and xT
1 by using K example images randomly
sampled from x. We adopt a progressive training technique  which gradually increases the length of
training sequences. Initially  we set T = 1  which means the network only generates single frames.
After that  we double the sequence length (T ) for every few epochs.

1 are available. We train our model to convert sT

1 to xT

5

Inference. At test time  our model can take an arbitrary number of example images. In Figure 7(b) 
we show that our performance is positively correlated with the number of example images. Moreover 
we can also (optionally) Ô¨Ånetune the network using the given example images to improve performance.
Note that we only Ô¨Ånetune the weight generation module E and the intermediate image synthesis
network H  and leave all parameters related to Ô¨Çow estimation (Œ∏M   Œ∏H) Ô¨Åxed. We found this can
better preserve the person identity in the example image.

4 Experiments
Implementation details. Our training procedure follows the procedure from the vid2vid work [57].
We use the ADAM optimizer [26] with lr = 0.0004 and (Œ≤1  Œ≤2) = (0.5  0.999). Training was
conducted using an NVIDIA DGX-1 machine with 8 32GB V100 GPUs.
Datasets. We adopt three video datasets to validate our method.
‚Ä¢ YouTube dancing videos. It consists of 1  500 dancing videos from YouTube. We divide them
into a training set and a test set with no overlapping subjects. Each video is further divided into
short clips of continuous motions. This yields about 15  000 clips for training. At each iteration 
we randomly pick a clip and select one or more frames in the same clip as the example images.
At test time  both the example images and the input human poses are not seen during training.
‚Ä¢ Street-scene videos. We use street-scene videos from three different geographical areas: 1)
Germany  from the Cityscapes dataset [9]  2) Boston  collected using a dashcam  and 3) NYC 
collected by a different dashcam. We apply a pretrained segmentation network [60] to get the
segmentation maps. Again  during training  we randomly select one frame of the same area as the
example image. At test time  in addition to the test set images from these three areas  we also test
on the ApolloScape [20] and CamVid [5] datasets  which are not included in the training set.
‚Ä¢ Face videos. We use the real videos in the FaceForensics dataset [44]  which contains 854 videos
of news brieÔ¨Ång from different reporters. We split the dataset into 704 videos for training and 150
videos for validation. We extract sketches from the input videos similar to vid2vid  and select
one frame of the same video as the example image to convert sketches to face videos.

Baselines. Since no existing vid2vid method can adapt to unseen domains using few example im-
ages  we construct 3 strong baselines that consider different ways of achieving the target generalization
capability. For the following comparisons and Ô¨Ågures  all methods use 1 example image.
‚Ä¢ Encoder. In this baseline approach  we encode the example images into a style vector and then

decode the features using the image synthesis branch in our H to generate Àúht.

‚Ä¢ ConcatStyle. In this baseline approach  we also encode the example images into a style vector.
However  instead of directly decoding the style vector using the image synthesis branch in our
H  it concatenates the vector with each of the input semantic images to produce an augmented
semantic input image. This image is then used as input to the spatial modulation branches in our
H for generating the intermediate image Àúht.

‚Ä¢ AdaIN. In this baseline  we insert an AdaIN normalization layer after each spatial modulation
layer in the image synthesis branch of H. We generate the AdaIN normalization parameters by
feeding the example images to an encoder  similar to the FUNIT method [31].

In addition to these baselines  for the human synthesis task  we also compare our approach with the
following methods using the pretrained models provided by the authors.
‚Ä¢ PoseWarp [1] synthesizes humans in unseen poses using an example image. The idea is to
assume each limb undergoes a similarity transformation. The Ô¨Ånal output image is obtained by
combining all transformed limbs together.
‚Ä¢ MonkeyNet [47] is proposed for transferring motions from a sequence to a still image. It Ô¨Årst

detects keypoints in the images  and then predicts their Ô¨Çows for warping the still image.

Evaluation metrics. We use the following metrics for quantitative evaluation.
‚Ä¢ Fr√©chet Inception Distance (FID) [17] measures the distance between the distributions of real
‚Ä¢ Pose error. We estimate the poses of the synthesized subjects using OpenPose [6]. This renders a
set of joint locations for each video frame. We then compute the absolute error in pixels between

data and generated data. It is commonly used to quantify the Ô¨Ådelity of synthesized images.

6

YouTube Dancing videos

Pose Error

Human Pref.

Street Scene videos

Human Pref.

Method
Encoder
ConcatStyle
AdaIN
PoseWarp [1]
MonkeyNet [47]
Ours

13.30
13.32
12.66
16.84
13.73
6.01

FID
234.71
140.87
207.18
180.31
260.77
80.44

Table 1: Our method outperforms existing pose transfer methods and our baselines for both dancing
and street scene video synthesis tasks. For pose error and FID  lower is better. For pixel accuracy and
mIoU  higher is better. The human preference score indicates the fraction of subjects favoring results
synthesized by our method.

0.96
0.95
0.93
0.83
0.93
‚Äî

Pixel Acc mIoU
0.222
0.240
0.360
N/A
N/A
0.408

0.400
0.479
0.756
N/A
N/A
0.831

FID
187.10
154.33
205.54
N/A
N/A
144.24

0.97
0.97
0.87
N/A
N/A
‚Äî

Figure 3: Visualization of human video synthesis results. Given the same pose video but different
example images  our method synthesizes realistic videos of the subjects  who are not seen during
training. The Ô¨Ågure is best viewed with Acrobat Reader. Click the image to play the video clip.

the estimated pose and the original pose input to the model. The idea behind this metric is that
if the image is well-synthesized  a well-trained human pose estimation network should be able
to recover the original pose used to synthesize the image. We note similar ideas were used in
evaluating image synthesis performance in several prior works [22  58  57].

‚Ä¢ Segmentation accuracy. To evaluate the performance of street scene videos  we run a state-of-
the-art street scene segmentation network on the result videos generated by all the competing
methods. We then report the pixel accuracy and mean intersection-over-union (IoU) ratio. The
idea of using segmentation accuracy as a performance metric follows the discussion of using the
pose error as discussed above.

‚Ä¢ Human subjective score. Finally  we use Amazon Mechanical Turk (AMT) to evaluate the
quality of generated videos. We perform AB tests where we provide the user videos from

7

Figure 4: Comparisons against different baselines for human motion synthesis. Note that the
competing methods either have many visible artifacts or completely fail to transfer the motion. The
Ô¨Ågure is best viewed with Acrobat Reader. Click the image to play the video clip.

Figure 5: Visualization of street scene video synthesis results. Our approach is able to synthesize
videos that realistically reÔ¨Çect the style in the example images even if the style is not included in the
training set. The Ô¨Ågure is best viewed with Acrobat Reader. Click the image to play the video clip.

two different approaches and ask them to choose the one with better quality. For each pair of
comparisons  we generate 100 clips  each of them viewed by 60 workers. Orders are randomized.
Main results. In Figure 3  we show results of using different example images when synthesizing
humans. It can be seen that our method can successfully transfer motion to all the example images.
Figure 4 shows comparisons of our approaches against other methods. It can be seen that other
methods either generate obvious artifacts or fail to transfer the motion faithfully.
Figure 5 shows the results of synthesizing street scene videos with different example images. It can
be seen that even with the same input segmentation map  our method can achieve different visual
results using different example images.
Table 1 shows quantitative comparisons of both tasks against the other methods. It can be seen that
our method consistently achieves better results than the others on all the performance metrics.
In Figure 6  we show results of using different example images when synthesizing faces. Our method
can faithfully preserve the person identity while capturing the motion in the input videos.
Finally  to verify our hypothesis that a larger training dataset helps improve the quality of synthesized
videos  we conduct an experiment where part of the dataset is held out during training. We vary the
number of videos in the training set and plot the resulting performance in Figure 7(a). We Ô¨Ånd that
the results support our hypothesis. We also evaluate whether having access to more example images

8

Figure 6: Visualization of face video synthesis results. Given the same input video but different
example images  our method synthesizes realistic videos of the subjects  who are not seen during
training. The Ô¨Ågure is best viewed with Acrobat Reader. Click the image to play the video clip.

Figure 7: (a) The plot shows the quality of our synthesized videos improves when it is trained with
a larger dataset. Large variety helps learn a more generalizable network weight generation module
and hence improves adaptation capability. (b) The plot shows the quality of our synthesized videos
is correlated with the number of example images provided at test time. The proposed attention
mechanism can take advantage of a larger example set to better generate the network weights. (c)
Visualization of attention maps when multiple example images are given. Note that when synthesizing
the front of the target  the attention map indicates that the network utilizes more of the front example
image  and vice versa.

at test time helps with the video synthesis performance. As shown in Figure 7(b)  the result conÔ¨Årms
our assumption.
Limitations. Although our network can  in principal  generalize to unseen domains  when the test
domain is too different from the training domains it will not perform well. For example  when
testing on CG characters which look very different from real-world people  the network will struggle.
In addition  since our network is based on semantic estimations as input such as pose maps or
segmentation maps  when these estimations fail our network will also likely fail.

5 Conclusion

We presented a few-shot video-to-video synthesis framework that can synthesize videos of unseen
subjects or street scene styles at the test time. This was enabled by our novel adaptive network
weight generation scheme  which dynamically determines the weights based on the example images.
Experimental results showed that our method performs favorably against the competing methods.

9

(c) Example Attn mapsfrontbackInput framesAttention mapsfrontfrontbackbackExample images(b) Effect of number ofexamples at inference(a) Effect of trainingdatasetsizeReferences

[1] G. Balakrishnan  A. Zhao  A. V. Dalca  F. Durand  and J. Guttag. Synthesizing images of humans in unseen

poses. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[2] S. Benaim and L. Wolf. One-shot unsupervised cross domain translation. In Advances in Neural Information

Processing Systems (NIPS)  2018.

[3] K. Bousmalis  N. Silberman  D. Dohan  D. Erhan  and D. Krishnan. Unsupervised pixel-level domain
adaptation with generative adversarial networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2017.

[4] A. Brock  J. Donahue  and K. Simonyan. Large scale gan training for high Ô¨Ådelity natural image synthesis.

In International Conference on Learning Representations (ICLR)  2019.

[5] G. J. Brostow  J. Shotton  J. Fauqueur  and R. Cipolla. Segmentation and recognition using structure from

motion point clouds. In European Conference on Computer Vision (ECCV)  2008.

[6] Z. Cao  G. Hidalgo  T. Simon  S.-E. Wei  and Y. Sheikh. OpenPose: realtime multi-person 2D pose

estimation using Part AfÔ¨Ånity Fields. In arXiv preprint arXiv:1812.08008  2018.

[7] C. Chan  S. Ginosar  T. Zhou  and A. A. Efros. Everybody dance now. arXiv preprint arXiv:1808.07371 

2018.

[8] Y. Choi  M. Choi  M. Kim  J.-W. Ha  S. Kim  and J. Choo. Stargan: UniÔ¨Åed generative adversarial
networks for multi-domain image-to-image translation. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2018.

[9] M. Cordts  M. Omran  S. Ramos  T. Rehfeld  M. Enzweiler  R. Benenson  U. Franke  S. Roth  and
B. Schiele. The Cityscapes dataset for semantic urban scene understanding. In IEEE Conference on
Computer Vision and Pattern Recognition (CVPR)  2016.

[10] E. L. Denton and V. Birodkar. Unsupervised learning of disentangled representations from video. In

Advances in Neural Information Processing Systems (NIPS)  2017.

[11] C. Finn  I. Goodfellow  and S. Levine. Unsupervised learning for physical interaction through video

prediction. In Advances in Neural Information Processing Systems (NIPS)  2016.

[12] O. Gafni  L. Wolf  and Y. Taigman. Vid2game: Controllable characters extracted from real-world videos.

arXiv preprint arXiv:1904.08379  2019.

[13] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial networks. In Advances in Neural Information Processing Systems (NIPS)  2014.

[14] I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. C. Courville. Improved training of wasserstein

GANs. In Advances in Neural Information Processing Systems (NIPS)  2017.

[15] D. Ha  A. Dai  and Q. V. Le. Hypernetworks. In International Conference on Learning Representations

(ICLR)  2016.

[16] Z. Hao  X. Huang  and S. Belongie. Controllable video generation with sparse trajectories. In IEEE

Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[17] M. Heusel  H. Ramsauer  T. Unterthiner  B. Nessler  and S. Hochreiter. GANs trained by a two time-scale
update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems
(NIPS)  2017.

[18] Q. Hu  A. Waelchli  T. Portenier  M. Zwicker  and P. Favaro. Video synthesis from a single image and

motion stroke. arXiv preprint arXiv:1812.01874  2018.

[19] X. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In

IEEE International Conference on Computer Vision (ICCV)  2017.

[20] X. Huang  X. Cheng  Q. Geng  B. Cao  D. Zhou  P. Wang  Y. Lin  and R. Yang. The ApolloScape dataset
for autonomous driving. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.
[21] X. Huang  M.-Y. Liu  S. Belongie  and J. Kautz. Multimodal unsupervised image-to-image translation.

European Conference on Computer Vision (ECCV)  2018.

[22] P. Isola  J.-Y. Zhu  T. Zhou  and A. A. Efros. Image-to-image translation with conditional adversarial

networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2017.

[23] X. Jia  B. De Brabandere  T. Tuytelaars  and L. V. Gool. Dynamic Ô¨Ålter networks. In Advances in Neural

Information Processing Systems (NIPS)  2016.

[24] N. Kalchbrenner  A. v. d. Oord  K. Simonyan  I. Danihelka  O. Vinyals  A. Graves  and K. Kavukcuoglu.

Video pixel networks. arXiv preprint arXiv:1610.00527  2016.

[25] T. Karras  S. Laine  and T. Aila. A style-based generator architecture for generative adversarial networks.

In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2019.

[26] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations (ICLR)  2015.

arXiv preprint arXiv:1804.01523  2018.

[27] A. X. Lee  R. Zhang  F. Ebert  P. Abbeel  C. Finn  and S. Levine. Stochastic adversarial video prediction.

[28] Y. Li  C. Fang  J. Yang  Z. Wang  X. Lu  and M.-H. Yang. Flow-grounded spatial-temporal video prediction
from still images. In Proceedings of the European Conference on Computer Vision (ECCV)  pages 600‚Äì615 
2018.

[29] X. Liang  L. Lee  W. Dai  and E. P. Xing. Dual motion GAN for future-Ô¨Çow embedded video prediction.

In Advances in Neural Information Processing Systems (NIPS)  2017.

[30] M.-Y. Liu  T. Breuel  and J. Kautz. Unsupervised image-to-image translation networks. In Advances in

Neural Information Processing Systems (NIPS)  2017.

10

[31] M.-Y. Liu  X. Huang  A. Mallya  T. Karras  T. Aila  J. Lehtinen  and J. Kautz. Few-shot unsupervised

image-to-image translation. arXiv preprint arXiv:1905.01723  2019.

[32] M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information

Processing Systems (NIPS)  2016.

[33] W. Lotter  G. Kreiman  and D. Cox. Deep predictive coding networks for video prediction and unsupervised

learning. In International Conference on Learning Representations (ICLR)  2017.

[34] M. Mathieu  C. Couprie  and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In

International Conference on Learning Representations (ICLR)  2016.

[35] T. Miyato and M. Koyama. cGANs with projection discriminator. In International Conference on Learning

Representations (ICLR)  2018.

Vision (ECCV)  2018.

[36] N. Neverova  R. Alp Guler  and I. Kokkinos. Dense pose transfer. In European Conference on Computer

[37] S. Niklaus  L. Mai  and F. Liu. Video frame interpolation via adaptive convolution. In IEEE Conference on

Computer Vision and Pattern Recognition (CVPR)  2017.

[38] S. Niklaus  L. Mai  and F. Liu. Video frame interpolation via adaptive separable convolution. In IEEE

International Conference on Computer Vision (ICCV)  2017.

[39] A. Odena  C. Olah  and J. Shlens. Conditional image synthesis with auxiliary classiÔ¨Åer GANs.

In

International Conference on Machine Learning (ICML)  2017.

[40] J. Pan  C. Wang  X. Jia  J. Shao  L. Sheng  J. Yan  and X. Wang. Video generation from single semantic

label map. arXiv preprint arXiv:1903.04480  2019.

[41] T. Park  M.-Y. Liu  T.-C. Wang  and J.-Y. Zhu. Semantic image synthesis with spatially-adaptive normal-

ization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2019.

[42] A. Radford  L. Metz  and S. Chintala. Unsupervised representation learning with deep convolutional
generative adversarial networks. In International Conference on Learning Representations (ICLR)  2015.
[43] S. Reed  Z. Akata  X. Yan  L. Logeswaran  B. Schiele  and H. Lee. Generative adversarial text to image

synthesis. In International Conference on Machine Learning (ICML)  2016.

[44] A. R√∂ssler  D. Cozzolino  L. Verdoliva  C. Riess  J. Thies  and M. Nie√üner. Faceforensics: A large-scale

video dataset for forgery detection in human faces. arXiv preprint arXiv:1803.09179  2018.

[45] M. Saito  E. Matsumoto  and S. Saito. Temporal generative adversarial nets with singular value clipping.

In IEEE International Conference on Computer Vision (ICCV)  2017.

[46] A. Shrivastava  T. PÔ¨Åster  O. Tuzel  J. Susskind  W. Wang  and R. Webb. Learning from simulated and
unsupervised images through adversarial training. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2017.

[47] A. Siarohin  S. Lathuili√®re  S. Tulyakov  E. Ricci  and N. Sebe. Animating arbitrary objects via deep

motion transfer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2019.

[48] N. Srivastava  E. Mansimov  and R. Salakhudinov. Unsupervised learning of video representations using

lstms. In International Conference on Machine Learning (ICML)  2015.

[49] H. Su  V. Jampani  D. Sun  O. Gallo  E. Learned-Miller  and J. Kautz. Pixel-adaptive convolutional neural

networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2019.

[50] Y. Taigman  A. Polyak  and L. Wolf. Unsupervised cross-domain image generation. In International

Conference on Learning Representations (ICLR)  2017.

[51] S. Tulyakov  M.-Y. Liu  X. Yang  and J. Kautz. MoCoGAN: Decomposing motion and content for video

generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2018.

[52] A. Vaswani  N. Shazeer  N. Parmar  J. Uszkoreit  L. Jones  A. N. Gomez  ≈Å. Kaiser  and I. Polosukhin.

Attention is all you need. In Advances in Neural Information Processing Systems (NIPS)  2017.

[53] R. Villegas  J. Yang  S. Hong  X. Lin  and H. Lee. Decomposing motion and content for natural video

sequence prediction. In International Conference on Learning Representations (ICLR)  2017.

[54] C. Vondrick  H. Pirsiavash  and A. Torralba. Generating videos with scene dynamics. In Advances in

Neural Information Processing Systems (NIPS)  2016.

[55] J. Walker  C. Doersch  A. Gupta  and M. Hebert. An uncertain future: Forecasting from static images using

variational autoencoders. In European Conference on Computer Vision (ECCV)  2016.

[56] J. Walker  K. Marino  A. Gupta  and M. Hebert. The pose knows: Video forecasting by generating pose

futures. In IEEE International Conference on Computer Vision (ICCV)  2017.

[57] T.-C. Wang  M.-Y. Liu  J.-Y. Zhu  G. Liu  A. Tao  J. Kautz  and B. Catanzaro. Video-to-video synthesis. In

Advances in Neural Information Processing Systems (NIPS)  2018.

[58] T.-C. Wang  M.-Y. Liu  J.-Y. Zhu  A. Tao  J. Kautz  and B. Catanzaro. High-resolution image synthesis
and semantic manipulation with conditional GANs. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)  2018.

[59] J. Wu  D. Li  Y. Yang  C. Bajaj  and X. Ji. Dynamic sampling convolutional neural networks. In European

Conference on Computer Vision (ECCV)  2018.

[60] T. Wu  S. Tang  R. Zhang  and Y. Zhang. Cgnet: A light-weight context guided network for semantic

segmentation. arXiv preprint arXiv:1811.08201  2018.

[61] K. Xu  J. Ba  R. Kiros  K. Cho  A. Courville  R. Salakhutdinov  R. Zemel  and Y. Bengio. Show  attend
and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044  2015.
[62] T. Xu  P. Zhang  Q. Huang  H. Zhang  Z. Gan  X. Huang  and X. He. Attngan: Fine-grained text to image
generation with attentional generative adversarial networks. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)  2018.

11

Conference on Learning Representations (ICLR)  2019.

[65] H. Zhang  I. Goodfellow  D. Metaxas  and A. Odena. Self-attention generative adversarial networks. In

International Conference on Machine Learning (ICML)  2019.

[66] H. Zhang  T. Xu  H. Li  S. Zhang  X. Huang  X. Wang  and D. Metaxas. StackGAN: Text to photo-realistic
image synthesis with stacked generative adversarial networks. In IEEE International Conference on
Computer Vision (ICCV)  2017.

[67] Y. Zhou  Z. Wang  C. Fang  T. Bui  and T. L. Berg. Dance dance generation: Motion transfer for internet

[63] T. Xue  J. Wu  K. Bouman  and B. Freeman. Visual dynamics: Probabilistic future frame synthesis via

cross convolutional networks. In Advances in Neural Information Processing Systems (NIPS)  2016.

[64] C. Zhang  M. Ren  and R. Urtasun. Graph hypernetworks for neural architecture search. In International

videos. arXiv preprint arXiv:1904.00129  2019.

[68] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros. Unpaired image-to-image translation using cycle-consistent

adversarial networks. In IEEE International Conference on Computer Vision (ICCV)  2017.

[69] J.-Y. Zhu  R. Zhang  D. Pathak  T. Darrell  A. A. Efros  O. Wang  and E. Shechtman. Toward multimodal

image-to-image translation. In Advances in Neural Information Processing Systems (NIPS)  2017.

12

,Ting-Chun Wang
Ming-Yu Liu
Andrew Tao
Guilin Liu
Bryan Catanzaro
Jan Kautz