2010,Variational Inference over Combinatorial Spaces,Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems (Karzanov et al.  1991; Jerrum et al.  2001; Wilson  2004)  theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods.  Despite their strong theoretical guarantees  the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning.  Because of this  in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (Siepel et al.  2004).   Variational inference would appear to provide an appealing alternative  given the success of variational methods for graphical models (Wainwright et al.  2008); unfortunately  however  it is not obvious how to develop variational approximations for combinatorial objects such as matchings  partial orders  plane partitions and sequence alignments.   We propose a new framework that extends variational inference to a wide range of combinatorial spaces.  Our method is based on a simple assumption: the existence of a tractable measure factorization  which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm.   We also apply the framework to the problem of multiple alignment of protein sequences  obtaining state-of-the-art results on the BAliBASE dataset (Thompson et al.  1999).,Variational Inference over Combinatorial Spaces

Alexandre Bouchard-Cˆot´e∗ Michael I. Jordan∗ †
†Department of Statistics
∗Computer Science Division

University of California at Berkeley

Abstract

Since the discovery of sophisticated fully polynomial randomized algorithms for a range of
#P problems [1  2  3]  theoretical work on approximate inference in combinatorial spaces
has focused on Markov chain Monte Carlo methods. Despite their strong theoretical guar-
antees  the slow running time of many of these randomized algorithms and the restrictive
assumptions on the potentials have hindered the applicability of these algorithms to ma-
chine learning. Because of this  in applications to combinatorial spaces simple exact mod-
els are often preferred to more complex models that require approximate inference [4].
Variational inference would appear to provide an appealing alternative  given the success
of variational methods for graphical models [5]; unfortunately  however  it is not obvious
how to develop variational approximations for combinatorial objects such as matchings 
partial orders  plane partitions and sequence alignments. We propose a new framework that
extends variational inference to a wide range of combinatorial spaces. Our method is based
on a simple assumption: the existence of a tractable measure factorization  which we show
holds in many examples. Simulations on a range of matching models show that the algo-
rithm is more general and empirically faster than a popular fully polynomial randomized
algorithm. We also apply the framework to the problem of multiple alignment of protein
sequences  obtaining state-of-the-art results on the BAliBASE dataset [6].

Introduction

1
The framework we propose is applicable in the following setup: let C denote a combinatorial space 
by which we mean a ﬁnite but large set  where testing membership is tractable  but enumeration is
x∈C f(x)  where f is a positive function. This setup
subsumes many probabilistic inference and classical combinatorics problems. It is often intractable
to compute this sum  so approximations are used.
We approach this problem by exploiting a ﬁnite collection of sets {Ci} such that C = ∩iCi. Each Ci
is larger than C  but paradoxically it is often possible to ﬁnd such a decomposition where for each i 
f(x) is tractable. We give many examples of this in Section 3 and Appendix B.1 This paper

not  and suppose that the goal is to computeP
P

x∈Ci

describes an effective way of using this type of decomposition to approximate the original sum.
Another way of viewing this setup is in terms of exponential families. In this view  described in
detail in Section 2  the decomposition becomes a factorization of the base measure. As we will
show  the exponential family view gives a principled way of deﬁning variational approximations.
In order to make variational approximations tractable in the combinatorial setup  we use what we
call an implicit message representation. The canonical parameter space of the exponential family
enables such representation. We also show how additional approximations can be introduced in
cases where the factorization has a large number of factors. These further approximations rely on
an outer bound of the partition function  and therefore preserve the guarantees of convex variational
objective functions.
While previous authors have proposed mean ﬁeld or loopy belief propagation algorithms to approx-
imate the partition function of a few speciﬁc combinatorial models—for example [7  8] for parsing 

1The appendices can be found in the supplementary material.

1

and [9  10] for computing the permanent of a matrix—we are not aware of a general treatment of
variational inference in combinatorial spaces.
There has been work on applying variational algorithms to the problem of maximization over combi-
natorial spaces [11  12  13  14]  but maximization over combinatorial spaces is rather different than
summation. For example  in the bipartite matching example considered in both [13] and this paper 
there is a known polynomial algorithm for maximization  but not for summation. Our approach
is also related to agreement-based learning [15  16]  although agreement-based learning is deﬁned
within the context of unsupervised learning using EM  while our framework is agnostic with respect
to parameter estimation.
The paper is organized as follows: in Section 2 we present the measure factorization framework; in
Section 3 we show examples of this framework applied to various combinatorial inference problems;
and in Section 4 we present empirical results.

2 Variational measure factorization

In this section  we present the variational measure factorization framework. At a high level  the ﬁrst
step is to construct an equivalent but more convenient exponential family. This exponential family
will allow us to transform variational algorithms over graphical models into approximation algo-
rithms over combinatorial spaces. We ﬁrst describe the techniques needed to do this transformation
in the case of a speciﬁc variational inference algorithm—loopy belief propagation—and then discuss
mean-ﬁeld and tree-reweighted approximations.
To make the exposition more concrete  we use the running example of approximating the value and
gradient of the log-partition function of a Bipartite Matching model (BM) over KN N   a well-known
#P problem [17]. Unless we mention otherwise  we will consider bipartite perfect matchings; non-
bipartite and non-perfect matchings are discussed in Section 3.1. The reader should keep in mind 
however  that our framework is applicable to a much broader class of combinatorial objects. We
develop several other examples in Section 3 and in Appendix B.

2.1 Setup

Since we are dealing with discrete-valued random variables X  we can assume without loss of
generality that the probability distribution for which we want to compute the partition function and
moments is a member of a regular exponential family with canonical parameters θ ∈ RJ:

P(X ∈ B) =

exp{hφ(x)  θi − A(θ)}ν(x)  A(θ) = log

exp{hφ(x)  θi}ν(x) 

(1)

X

x∈X

X

x∈B

of computing P

for a J-dimensional sufﬁcient statistic φ and base measure ν over F = 2X   both of which are
assumed (again  without loss of generality) to be indicator functions : φj  ν : X → {0  1}. Here
X is a superset of both C and all of the Cis. The link between this setup and the general problem
x∈C f(x) is the base measure ν  which is set to the indicator function over C:
ν(x) = 1[x ∈ C]  where 1[·] is equal to one if its argument holds true  and zero otherwise.
The goal is to approximate A(θ) and ∇A(θ) (recall that the j-th coordinate of the gradient  ∇jA  is
equal to the expectation of the sufﬁcient statistic φj under the exponential family with base measure
ν [5]). We want to exploit situations where the base measure can be written as a product of I
i=1 νi(x) such that each factor νi : X → {0  1} induces a super-partition
x∈X exp{hφ(x)  θi}νi(x). This computation is
typically done using dynamic programming (DP). We also assume that the gradient of the super-
partition functions is tractable  which is typical for DP formulations.
the space X is a product of N 2 binary alignment variables  x =
In the case of BM 
x1 1  x1 2  . . .   xN N . In the Standard Bipartite Matching formulation (which we denote by SBM) 
the sufﬁcient statistic takes the form φj(x) = xm n. The measure factorization we use to enforce
the matching property is ν = ν1ν2  where:

measures ν(x) = QI
function assumed to be tractable: Ai(θ) = logP

ν1(x) =

1[

xm n ≤ 1] 

ν2(x) =

1[

xm n ≤ 1].

(2)

NY

NX

NY

NX

m=1

n=1

n=1

m=1

2

i j = 0

BPMF(θ  A1  . . .   AI)
1: ζ (1)
2: for t = 1  2  . . .   T do
3:

i = θ +P
(cid:16)∇Ai

¯ξ(t)
ζ(t)
i = logit
4:
5: end for
6: return ˆµ = logistic

i0

(cid:16)¯ξ(t)
(cid:17)(cid:17) − ¯ξ(t)
i0:i06=i ζ(t−1)
(cid:16)
(cid:17)
θ +P

i ζ(T )

i

i

i

Figure 1: Left: the bipartite graphical model used for the MRF construction described in Section 2.2. Right:
pseudocode for the BPMF algorithm. See Section 2 and Appendix A.2 for the derivation.

We show in Appendix A.3 that A1 and A2 can be computed in time O(N 2) for the SBM.
The last assumption we make is that given a vector s ∈ RJ  there is at most one possible conﬁgu-
ration x with φ(x) = s. We call this the rich sufﬁcient statistic condition. Since we are concerned
in this framework with computing expectations  not with parameter estimation  this can be done
without loss of generality. For example  if the original exponential family is curved (e.g.  by param-
eter tying)  for the purpose of computing expectations one can always work in the over-complete
parameterization  and then project back to the coarse sufﬁcient statistic for parameter estimation.

2.2 Markov random ﬁeld reformulation

We start by constructing an equivalent but more convenient exponential family. This general con-
struction has an associated bipartite Markov Random Field (MRF) with structure KI J  shown in
Figure 1. This new bipartite structure should not be confused with the bipartite graph from the
KN N bipartite graph speciﬁc to the BM example: the former is part of the general theory  the latter
is speciﬁc to the bipartite matching example.
The bipartite MRF has I random variables in the ﬁrst graph component  B1  . . .   BI  each having a
copy of X as its domain. In the second component  the graph has J random variables  S1  . . .   SJ 
where Sj has a binary domain {0  1}. The pairwise potential between an event {Bi = x} in the ﬁrst
component and one {Sj = s} in the second is given by Ψi j(x  s) = 1[φj(x) = s]. The following
one-node potentials are also included: Ψi(x) = νi(x) and Ψj(s) = eθj s.
The equivalence between the two formulations follows from the rich sufﬁcient statistic condition 
which implies (for a full proof of the equivalence  see Appendix A.1):

(cid:26) 1 if x1 = x2 = ··· = xI

0 otherwise.

1[φj(xi) = sj] =

(3)

X

X

··· X

IY

JY

s1∈{0 1}

s2∈{0 1}

sJ∈{0 1}

i=1

j=1

This transformation into an equivalent MRF reveals several possible variational approximations.
We show in the next section how loopy belief propagation [18] can be modiﬁed to tractably accom-
modate this transformed exponential family  even though some nodes in the graphical model—the
Bis—have a domain of exponential size. We then describe similar updates for mean ﬁeld [19] and
tree-reweighted [20] variational algorithms. We will refer to these algorithms as BPMF (Belief Prop-
agation on Measure Factorizations)  MFMF (Mean Field on Measure Factorizations) and TRWMF
(Tree-Reweighted updates on Measure Factorizations). In contrast to BPMF  MFMF is guaranteed
to converge2  and TRWBF is guaranteed to provide an upper bound on the partition function.3

2.3

Implicit message representation

The variables Bi have a domain of exponential size  hence if we applied belief propagation updates
naively  the messages going from Bi to Sj would require summing over an exponential number of
terms  and messages going from Sj to Bi would require an exponential amount of storage. To avoid
summing explicitly over exponentially many terms  we adapt an idea from [7] and exploit the fact

2Although we did not have convergence issues with BPMF in our experiments.
3Surprisingly  MFMF does not provide a lower bound (see Appendix A.6).

3

B1BIS1S2SJ.........familyiscurvedbyparametertying forthepurposeofcomputingexpectationsonecanalwaysworkintheover-completeparameterization andthenprojectbacktothecoarsesufﬁcientstatisticforparameterestimation.2.2MarkovrandomﬁeldreformulationWestartbyconstructinganequivalentbutmoreconvenientexponentialfamily.Thisgeneralcon-structionhasanassociatedbipartiteMarkovRandomField(MRF)withstructureKI J.ThisnewbipartitestructureshouldnotbeconfusedwiththebipartitegraphfromtheKM NbipartitegraphspeciﬁctotheBMexample:theformerispartofthegeneraltheory andisnotspeciﬁctothebipartitematchingexample.ThebipartiteMRFhasIrandomvariablesintheﬁrstgraphcomponent B1 ... BI eachhavingacopyofXasitsdomain.Inthesecondcomponent thegraphhasJrandomvariables S1 ... SJ whereSjhasabinarydomain{0 1}.Thepairwisepotentialsbetweenanevent(Bi=x)intheﬁrstcomponentandone(Sj=s)inthesecondisgivenbyΨi j(x s)=1[φj(x)=s].Thefollowingonenodepotentialsarealsoincluded:Ψi(x)=νi(x)andΨj(s)=eθjs.Theequivalencebetweenthetwoformulationsfollowsfromtherichsufﬁcientstatisticcondition whichimplies:Xs1∈{0 1}Xs2∈{0 1}···XsJ∈{0 1}IYi=1JYj=11[φj(xi)=sj]=1ifx1=x2=···=xI0otherwise.ThistransformationintoanequivalentMRFrevealsseveralpossiblevariationalapproximations.WeshowinthenextsectionhowloopyBPupdates[14]canbedeﬁnedoverthisgraph eventhoughsomenodesinthisgraph—theBis—havedomainsofexponentialsize.Wethendescribetheupdatesformeanﬁeld[15]andTRW[16].IncontrasttoBP thesealgorithmscanprovidedboundsonthepartitionfunction.2.3ImplicitmessagerepresentationThevariablesBihaveexponentialsizedomains henceifweappliedBPupdatesnaively themes-sagesgoingfromBitoSjwouldrequiresummingoveranexponentialnumberofterms andmes-sagesgoingfromSjtoBiwouldrequireanexponentialamountofstorage.Toavoidsummingexplicitlyoverexponentiallymanyterms weuseatechniqueinspiredby[7]andexploitthefactthatanefﬁcientalgorithmisassumedforcomputingthesuper-partitionfunctionAianditsderiva-tives.ToavoidtheexponentialstorageofmessagesgoingtoBi weuseanimplicitrepresentationofthesemessagesinthecanonicalparameterspace.LetusdenotethemessagesgoingfromSjtoBibyMj→i(s) s∈{0 1}andthereversemessages mi→j(x) x∈X.UsingthedeﬁnitionsofΨi j Ψi Ψj thestandardBPupdatesbecome:mi→j(s)∝Xx∈X1[φj(x)=s]νi(x)Yj!:j!#=jMj!→i(x)Mj→i(x)∝Xs∈{0 1}eθjs1[φj(x)=s]Yi!:i!#=imi!→j(s).(3)ThetaskistogetanupdateequationthatdoesnotrepresentMj→i(x)explicitly byexploitingthefactthatthesuper-partitionfunctionsAiandtheirderivativescanbecomputedefﬁciently.Todoso itisconvenienttousethefollowingequivalentrepresentationforthemessagesmi→j(s):ζi j=logmi→j(1)mi→j(0)∈[−∞ +∞].Ifwealsoletfi j(x)denoteanyfunctionproportionalto!j!:j!"=jMj!→i(x) wecanwrite:ζi j=log„Px∈Xφj(x)fi j(x)νi(x)Px∈X(1−φj(x))fi j(x)νi(x)«=logit„Px∈Xφj(x)fi j(x)νi(x)Px∈Xfi j(x)νi(x)« (4)3familyiscurvedbyparametertying forthepurposeofcomputingexpectationsonecanalwaysworkintheover-completeparameterization andthenprojectbacktothecoarsesufﬁcientstatisticforparameterestimation.2.2MarkovrandomﬁeldreformulationWestartbyconstructinganequivalentbutmoreconvenientexponentialfamily.Thisgeneralcon-structionhasanassociatedbipartiteMarkovRandomField(MRF)withstructureKI J.ThisnewbipartitestructureshouldnotbeconfusedwiththebipartitegraphfromtheKM NbipartitegraphspeciﬁctotheBMexample:theformerispartofthegeneraltheory andisnotspeciﬁctothebipartitematchingexample.ThebipartiteMRFhasIrandomvariablesintheﬁrstgraphcomponent B1 ... BI eachhavingacopyofXasitsdomain.Inthesecondcomponent thegraphhasJrandomvariables S1 ... SJ whereSjhasabinarydomain{0 1}.Thepairwisepotentialsbetweenanevent(Bi=x)intheﬁrstcomponentandone(Sj=s)inthesecondisgivenbyΨi j(x s)=1[φj(x)=s].Thefollowingonenodepotentialsarealsoincluded:Ψi(x)=νi(x)andΨj(s)=eθjs.Theequivalencebetweenthetwoformulationsfollowsfromtherichsufﬁcientstatisticcondition whichimplies:Xs1∈{0 1}Xs2∈{0 1}···XsJ∈{0 1}IYi=1JYj=11[φj(xi)=sj]=1ifx1=x2=···=xI0otherwise.ThistransformationintoanequivalentMRFrevealsseveralpossiblevariationalapproximations.WeshowinthenextsectionhowloopyBPupdates[14]canbedeﬁnedoverthisgraph eventhoughsomenodesinthisgraph—theBis—havedomainsofexponentialsize.Wethendescribetheupdatesformeanﬁeld[15]andTRW[16].IncontrasttoBP thesealgorithmscanprovidedboundsonthepartitionfunction.2.3ImplicitmessagerepresentationThevariablesBihaveexponentialsizedomains henceifweappliedBPupdatesnaively themes-sagesgoingfromBitoSjwouldrequiresummingoveranexponentialnumberofterms andmes-sagesgoingfromSjtoBiwouldrequireanexponentialamountofstorage.Toavoidsummingexplicitlyoverexponentiallymanyterms weuseatechniqueinspiredby[7]andexploitthefactthatanefﬁcientalgorithmisassumedforcomputingthesuper-partitionfunctionAianditsderiva-tives.ToavoidtheexponentialstorageofmessagesgoingtoBi weuseanimplicitrepresentationofthesemessagesinthecanonicalparameterspace.LetusdenotethemessagesgoingfromSjtoBibyMj→i(s) s∈{0 1}andthereversemessages mi→j(x) x∈X.UsingthedeﬁnitionsofΨi j Ψi Ψj thestandardBPupdatesbecome:mi→j(s)∝Xx∈X1[φj(x)=s]νi(x)Yj!:j!#=jMj!→i(x)Mj→i(x)∝Xs∈{0 1}eθjs1[φj(x)=s]Yi!:i!#=imi!→j(s).(3)ThetaskistogetanupdateequationthatdoesnotrepresentMj→i(x)explicitly byexploitingthefactthatthesuper-partitionfunctionsAiandtheirderivativescanbecomputedefﬁciently.Todoso itisconvenienttousethefollowingequivalentrepresentationforthemessagesmi→j(s):ζi j=logmi→j(1)mi→j(0)∈[−∞ +∞].Ifwealsoletfi j(x)denoteanyfunctionproportionalto!j!:j!"=jMj!→i(x) wecanwrite:ζi j=log„Px∈Xφj(x)fi j(x)νi(x)Px∈X(1−φj(x))fi j(x)νi(x)«=logit„Px∈Xφj(x)fi j(x)νi(x)Px∈Xfi j(x)νi(x)« (4)3familyiscurvedbyparametertying forthepurposeofcomputingexpectationsonecanalwaysworkintheover-completeparameterization andthenprojectbacktothecoarsesufﬁcientstatisticforparameterestimation.2.2MarkovrandomﬁeldreformulationWestartbyconstructinganequivalentbutmoreconvenientexponentialfamily.Thisgeneralcon-structionhasanassociatedbipartiteMarkovRandomField(MRF)withstructureKI J.ThisnewbipartitestructureshouldnotbeconfusedwiththebipartitegraphfromtheKM NbipartitegraphspeciﬁctotheBMexample:theformerispartofthegeneraltheory andisnotspeciﬁctothebipartitematchingexample.ThebipartiteMRFhasIrandomvariablesintheﬁrstgraphcomponent B1 ... BI eachhavingacopyofXasitsdomain.Inthesecondcomponent thegraphhasJrandomvariables S1 ... SJ whereSjhasabinarydomain{0 1}.Thepairwisepotentialsbetweenanevent(Bi=x)intheﬁrstcomponentandone(Sj=s)inthesecondisgivenbyΨi j(x s)=1[φj(x)=s].Thefollowingonenodepotentialsarealsoincluded:Ψi(x)=νi(x)andΨj(s)=eθjs.Theequivalencebetweenthetwoformulationsfollowsfromtherichsufﬁcientstatisticcondition whichimplies:Xs1∈{0 1}Xs2∈{0 1}···XsJ∈{0 1}IYi=1JYj=11[φj(xi)=sj]=1ifx1=x2=···=xI0otherwise.ThistransformationintoanequivalentMRFrevealsseveralpossiblevariationalapproximations.WeshowinthenextsectionhowloopyBPupdates[14]canbedeﬁnedoverthisgraph eventhoughsomenodesinthisgraph—theBis—havedomainsofexponentialsize.Wethendescribetheupdatesformeanﬁeld[15]andTRW[16].IncontrasttoBP thesealgorithmscanprovidedboundsonthepartitionfunction.2.3ImplicitmessagerepresentationThevariablesBihaveexponentialsizedomains henceifweappliedBPupdatesnaively themes-sagesgoingfromBitoSjwouldrequiresummingoveranexponentialnumberofterms andmes-sagesgoingfromSjtoBiwouldrequireanexponentialamountofstorage.Toavoidsummingexplicitlyoverexponentiallymanyterms weuseatechniqueinspiredby[7]andexploitthefactthatanefﬁcientalgorithmisassumedforcomputingthesuper-partitionfunctionAianditsderiva-tives.ToavoidtheexponentialstorageofmessagesgoingtoBi weuseanimplicitrepresentationofthesemessagesinthecanonicalparameterspace.LetusdenotethemessagesgoingfromSjtoBibyMj→i(s) s∈{0 1}andthereversemessages mi→j(x) x∈X.UsingthedeﬁnitionsofΨi j Ψi Ψj thestandardBPupdatesbecome:mi→j(s)∝Xx∈X1[φj(x)=s]νi(x)Yj!:j!#=jMj!→i(x)Mj→i(x)∝Xs∈{0 1}eθjs1[φj(x)=s]Yi!:i!#=imi!→j(s).(3)ThetaskistogetanupdateequationthatdoesnotrepresentMj→i(x)explicitly byexploitingthefactthatthesuper-partitionfunctionsAiandtheirderivativescanbecomputedefﬁciently.Todoso itisconvenienttousethefollowingequivalentrepresentationforthemessagesmi→j(s):ζi j=logmi→j(1)mi→j(0)∈[−∞ +∞].Ifwealsoletfi j(x)denoteanyfunctionproportionalto!j!:j!"=jMj!→i(x) wecanwrite:ζi j=log„Px∈Xφj(x)fi j(x)νi(x)Px∈X(1−φj(x))fi j(x)νi(x)«=logit„Px∈Xφj(x)fi j(x)νi(x)Px∈Xfi j(x)νi(x)« (4)3wherelogit(x)=logx−log(1−x).Thismeansthatifwecanﬁndaparametervectorξi j∈Rdsuchthatfi j(x)=exp!φ(x) ξi j"∝Yj!:j!!=jMj!→i(x) thenwecouldwrite1ζi j=logit!∇jAi(ξi j)".Wederivesuchavectorξi jasfollows:Yj!:j!!=jMj!→i(x)=Yj!:j!!=jXsj!∈{0 1}eθj!sj!1[φj!(x)=sj!]Yi!:i!!=imi!→j!(sj!)=Yj!:j!!=jeθj!φj!(x)Yi!:i!!=imi!→j!(φj!(x))∝exp8<:Xj!:j!!=jφj!(x)0@θj!+Xi!:i!!=iζi! j!1A9=;.Therequiredparametersaretherefore:!ξi j"j!=1[j$=j!]#θj!+$i!:i!"=iζi! j!%.2.4ReuseofpartitionfunctioncomputationsNaively theupdatesderivedsofarwouldrequirecomputingJtimeseachsuper-partitionfunctionAiateachmessagepassingiteration.WeshowthatthiscanbereducedtocomputingeachAionlyonceperiteration aconsiderablegain.Weﬁrstdeﬁnethevectors:¯ξi=θ+Xi!:i!!=iζi! andthenrewritethenumeratorinsidethelogitfunctioninEquation(4)asfollows:Xx∈Xφj(x)fi j(x)νi(x)=Xs∈{0 1}Xx:φj(x)=sexp!φ(x) ¯ξi"e−¯ξi jssνi(x)=e−¯ξi j∇jAi(¯ξi) andsimilarityforthedenominator:Xx∈Xfi j(x)νi(x)=e−¯ξi j∇jAi(¯ξi)+(1−∇jAi(¯ξi))=1+(e−¯ξi j−1)∇jAi(¯ξi)Thisargumentholdsforξijﬁnite.Addingconditionshandlingtheothercases wegetthefollowingmessageupdates:ζi j=logit`∇Ai(¯ξi j)´−¯ξi jif¯ξi jisﬁnite¯ξi jotherwise.2.5OthervariationalalgorithmsTheideasusedtoderivetheBPupdatescanbeextendedtoothervariationalalgorithmswithminormodiﬁcations.Weshowheretwoexamples:anaivemeanﬁeldalgorithm andaTRWapproxima-tion.1Notethatinordertohandlethecaseswhereacanonicalparametercoordinateis+∞ weneedtoslightlyredeﬁnethesuper-partitionfunctionsasfollows:Ai(θ)=Xx∈Cexp(JXj=11[θj<+∞]θjφj(x))νi(x)JYj=11[θj=+∞⇒φj(x)=1]4wherelogit(x)=logx−log(1−x).Thismeansthatifwecanﬁndaparametervectorξi j∈Rdsuchthatfi j(x)=exp!φ(x) ξi j"∝Yj!:j!!=jMj!→i(x) thenwecouldwrite1ζi j=logit!∇jAi(ξi j)".Wederivesuchavectorξi jasfollows:Yj!:j!!=jMj!→i(x)=Yj!:j!!=jXsj!∈{0 1}eθj!sj!1[φj!(x)=sj!]Yi!:i!!=imi!→j!(sj!)=Yj!:j!!=jeθj!φj!(x)Yi!:i!!=imi!→j!(φj!(x))∝exp8<:Xj!:j!!=jφj!(x)0@θj!+Xi!:i!!=iζi! j!1A9=;.Therequiredparametersaretherefore:!ξi j"j!=1[j$=j!]#θj!+$i!:i!"=iζi! j!%.2.4ReuseofpartitionfunctioncomputationsNaively theupdatesderivedsofarwouldrequirecomputingJtimeseachsuper-partitionfunctionAiateachmessagepassingiteration.WeshowthatthiscanbereducedtocomputingeachAionlyonceperiteration aconsiderablegain.Weﬁrstdeﬁnethevectors:¯ξi=θ+Xi!:i!!=iζi! andthenrewritethenumeratorinsidethelogitfunctioninEquation(4)asfollows:Xx∈Xφj(x)fi j(x)νi(x)=Xs∈{0 1}Xx:φj(x)=sexp!φ(x) ¯ξi"e−¯ξi jssνi(x)=e−¯ξi j∇jAi(¯ξi) andsimilarityforthedenominator:Xx∈Xfi j(x)νi(x)=e−¯ξi j∇jAi(¯ξi)+(1−∇jAi(¯ξi))=1+(e−¯ξi j−1)∇jAi(¯ξi)Thisargumentholdsforξijﬁnite.Addingconditionshandlingtheothercases wegetthefollowingmessageupdates:ζi j=logit`∇Ai(¯ξi j)´−¯ξi jif¯ξi jisﬁnite¯ξi jotherwise.2.5OthervariationalalgorithmsTheideasusedtoderivetheBPupdatescanbeextendedtoothervariationalalgorithmswithminormodiﬁcations.Weshowheretwoexamples:anaivemeanﬁeldalgorithm andaTRWapproxima-tion.1Notethatinordertohandlethecaseswhereacanonicalparametercoordinateis+∞ weneedtoslightlyredeﬁnethesuper-partitionfunctionsasfollows:Ai(θ)=Xx∈Cexp(JXj=11[θj<+∞]θjφj(x))νi(x)JYj=11[θj=+∞⇒φj(x)=1]4............that an efﬁcient algorithm is assumed for computing the super-partition function Ai and its deriva-
tives. To avoid the exponential storage of messages going to Bi  we use an implicit representation
of these messages in the canonical parameter space.
Let us denote the messages going from Sj to Bi by Mj→i(s)  s ∈ {0  1} and the reverse messages
by mi→j(x)  x ∈ X . From the deﬁnitions of Ψi j  Ψi  Ψj  the explicit belief propagation updates
are:

1[φj(x) = s]νi(x)

Mj0→i(x)

eθj s1[φj(x) = s]

mi0→j(s).

(4)

mi→j(s) ∝ X
Mj→i(x) ∝ X

x∈X

s∈{0 1}

Y
Y

j0:j06=j

i0:i06=i

The task is to get an update equation that does not represent Mj→i(x) explicitly  by exploiting
the fact that the super-partition functions Ai and their derivatives can be computed efﬁciently. To
do so  it is convenient to use the following equivalent representation for the messages mi→j(s):
ζi j = log mi→j(1) − log mi→j(0) ∈ [−∞  +∞].4
(cid:19)

If we also let fi j(x) denote any function proportional toQ

j0:j06=j Mj0→i(x)  we can write:

(5)
where logit(x) = log x − log(1 − x). This means that if we can ﬁnd a parameter vector ξi j ∈ RJ
such that

x∈X fi j(x)νi(x)

(cid:18) P
P
x∈X (1 − φj(x))fi j(x)νi(x)

x∈X φj(x)fi j(x)νi(x)

x∈X φj(x)fi j(x)νi(x)

(cid:18)P

ζi j = log

P

= logit

(cid:19)

 

fi j(x) = exphφ(x)  ξi ji ∝ Y

then we could write ζi j = logit(cid:0)∇jAi(ξi j)(cid:1) . We derive such a vector ξi j as follows:

Mj0→i(x) 

j0:j06=j

j0:j06=j

j0:j06=j

Y

sj0∈{0 1}

Mj0→i(x) =

Y
X
eθj0 φj0 (x) Y
Y
 X
θj0 +
X
(cid:16)
(cid:1)
required parameters are therefore:(cid:0)ξi j
θj0 +P

j0 = 1[j 6= j0]

∝ exp

φj0 (x)

j0:j06=j

j0:j06=j

i0:i06=i

i0:i06=i

=

mi0→j0 (φj0 (x))

i0:i06=i

Y
  

ζi0 j0

(cid:17)

i0:i06=i ζi0 j0

.

where in the last step we have used the assumption that φj has domain {0  1}  which implies that
mi→j(φj(x)) = exp{φj(x) log mi→j(1) + (1 − φj(x)) log mi→j(0)} ∝ exp{φj(x)ζi j}. The

eθj0 sj0 1[φj0 (x) = sj0 ]

mi0→j0 (sj0 )

2.4 Reuse of partition function computations

Naively  the updates derived so far would require computing each super-partition function J times at
each message passing iteration. We show that this can be reduced to computing each super-partition
function only once per iteration  a considerable gain.
We ﬁrst deﬁne the vectors:

X

¯ξi = θ +

ζi0  

i0:i06=i

X

X
= eAi(¯ξi)− ¯ξi j∇jAi(¯ξi) 

s∈{0 1}

x:φj (x)=s

and then rewrite the numerator inside the logit function in Equation (5) as follows:

φj(x)fi j(x)νi(x) =

exp{hφ(x)  ¯ξii} · e

− ¯ξi j s · s · νi(x)

X

x∈X

4In what follows  we will assume that ζi j ∈ (−∞  +∞). The extended real line is treated in Appendix C.1.

4

and similarly for the denominator:

X

x∈X

fi j(x)νi(x) = eAi(¯ξi)− ¯ξi j∇jAi(¯ξi) + eAi(¯ξi)(1 − ∇jAi(¯ξi))

= eAi(¯ξi)(cid:16)

1 + (e

(cid:17)
− ¯ξi j − 1)∇jAi(¯ξi)

.

After plugging in the reparameterization of the numerator and denominator back into the logit
function in Equation (5) and doing some algebra  we obtain the more efﬁcient update ζi j =

logit(cid:0)∇Ai(¯ξi j)(cid:1) − ¯ξi j  where the logit function of a vector  logit v  is deﬁned as the vector of

the logit function applied to each entry of the vector v. See Figure 1 for a summary of the BPMF
algorithm.

2.5 Other variational algorithms

The ideas used to derive the BPMF updates can be extended to other variational algorithms with
minor modiﬁcations. We sketch here two examples: a naive mean ﬁeld algorithm  and a TRW
approximation. See Appendix A.2 for details.
In the case of naive mean ﬁeld applied the graphical model described in Section 2.2  the updates
take a form similar to Equations (4)  except that the reverse incoming message is not omitted when
computing an outgoing message. As a consequence  the updates are not directional and can be
associated to nodes in the graphical model rather than edges:

Mj(s) ∝ X
mi(x) ∝ X

x∈X

1[φj(x) = s]νi(x)

mi(x)

eθj s1[φj(x) = s]

Mj(s).

Y
Y

j

i

ξ(t)(cid:17)(cid:17)
 ·

s∈{0 1}
This yields the following implicit updates:5

ξ(t) = θ +

X
(cid:16)∇Ai

i

i

ζ(t−1)

(cid:16)

i = logit
and the moment approximation ˆµ = logistic(ξ).
In the case of TRW  lines 3 and 6 in the pseudocode of Figure 1 stay the same  while the update in

 

(6)

ζ(t)

line 4 becomes:(cid:0)ξi j

(cid:1)

j0 =

θj0 − ρi→j0 ζi j0 +

X

i0:i06=i

(cid:26) ρj0→i

if j0 6= j

(1 − ρi→j) otherwise 

ρi0→j0 ζi0 j0

(7)

where ρi→j are marginals of a spanning tree distribution over KI J. We show in Appendix A.2 how
the idea in Section 2.4 can be exploited to reuse computations of super-partition functions in the
case of TRW as well.

2.6 Large factorizations

In some cases  it might not be possible to write the base measure as a succinct product of factors.
Fortunately  there is a simple and elegant workaround to this problem that retains good theoretical
guarantees. The basic idea is that dropping measures with domain {0  1} in a factorization can only
increase the value of the partition function. This solution is especially attractive in the context of
outer approximations such as the TRW algorithm  because it preserves the upper bound property of
the approximation. We show an example of this in Section 3.2.

3 Examples of factorizations

In this section  we show three examples of measure factorizations. See Appendix B for two more
examples (partitions of the plane  and traveling salesman problems).

5Assuming that naive mean ﬁeld is optimized coordinate-wise  with an ordering that optimizes all of the

mi’s  then all of the Mj’s.

5

(a)

(b)

(c)

Figure 2: (a) An example of a valid multiple alignment between three sequences. (b) Examples of invalid
multiple sequence alignments illustrating what is left out by the factors in the decomposition of Section 3.2.
(c) The DAG representation of a partial order. An example of linearization is A C D B E F G H I. The ﬁne red
dashed lines and blue lines demonstrate an example of two forests covering the set of edges  forming a measure
decomposition with two factors. The linearization A D B E F G H I C is an example of a state allowed by one
factor but not the other.

3.1 More matchings

Our approach extends naturally to matchings with higher-order (augmented) sufﬁcient statistic 
and to non-bipartite/non-perfect matchings. Let us ﬁrst consider an Higher-order Bipartite Model
(HBM)  which has all the basic sufﬁcient statistic coordinates found in SBM  plus those of the form
φj(x) = xm n · xm+1 n+1. We claim that with the factorization of Equation (2)  the super-partition
functions A1 and A2 are still tractable in HBM. To see why  note that computing A1 can be done
by building an auxiliary exponential family with associated graphical model given by a chain of
length N  and where the state space of each node in this chain is {1  2  . . .   N}. The basic sufﬁ-
cient statistic coordinates φj(x) = xm n are encoded as node potentials  and the augmented ones as
edge potentials in the chain. This yields a running time of O(N 3) for computing one super-partition
function and its gradient (see Appendix A.3 for details). The auxiliary exponential family technique
used here is reminiscent of [21].
Extension to non-perfect and non-bipartite matchings can also be done easily. In the ﬁrst case  a
dummy “null” node is added to each bipartite component. In the second case  where the original
n0=1 xn n0 ≤ 1].

space is the set of(cid:0)N
one checks that a single node is connected to at most one other node: νn(x) = 1[PN

(cid:1) alignment indicators  we propose a decomposition into N measures. Each

2

3.2 Multiple sequence alignment

We start by describing the space of pairwise alignments (which is tractable)  and then discuss the
extension to multiple sequences (which quickly becomes infeasible as the number of sequences
increases). Consider two sequences of length M and N respectively. A pairwise sequence alignment
is a bipartite graph on the characters of the two sequences (where each bipartite component has
the characters of one of the sequences) constrained to be monotonic: if a character at index m ∈
{1  . . .   M} is aligned to a character at index n ∈ {1  . . .   N} and another character at index m0 >
m is aligned to index n0  then we must have n0 > n. A multiple alignment between K sequences of
lengths N1  N2  . . .   NK is a K-partite graph  where the k-th components’ vertices are the characters
of the k-th sequence  and such that the following three properties hold: (1) each pair of components
forms a pairwise alignment as described above; (2) the alignments are transitive  i.e.  if character
c1 is aligned to c2 and c2 is aligned to c3 then c1 must be aligned to c3; (3) the alignments satisfy
a partial order property: there exists a partial order p on the connected components of the graph
with the property that if C1 <p C2 are two distinct connected components and c1 ∈ C1  c2 ∈ C2
are in the same sequence  then the index of c1 in the sequence is smaller than the index of c2. See
Figure 2(a b) for an illustration.
We use the technique of Section 2.6  and include only the pairwise alignment and transitiv-
ity constraints  creating a variational objective function that is an outer bound of the origi-
nal objective.

(cid:1) pairwise alignment measures  and T =

there are (cid:0)K

In this factorization 

2

6

ACTACACGTACCACACACTACTACTCTMonotonicity violationTransitivity violationPartial order violationACDFGBEFI(a)

(b)

(c)

Figure 3: Experiments discussed in Section 4.1 on two of the matching models discussed. (a) and (b) on SBM 
(c)  on HBM.

P

k k0 k00:k6=k06=k006=k NkNk0Nk00 transitivity measures. We show in Appendix A.4 that all the mes-

sages for one iteration can be computed in time O(T ).

3.3 Linearization of partial orders

A linearization of a partial order p over N objects is a total order t over the same objects such
that x ≤p y ⇒ x ≤t y. Counting the number of linearizations is a well-known #P problem [22].
Equivalently  the problem can be view as a matching between a DAG G = (V  E) and the integers
{1  2  . . .   N} with the order constraints speciﬁed on the edges of the DAG.
To factorize the base measure  consider a collection of I directed forests on V   Gi = (V  Ei)  i ∈ I
such that their union covers G: ∪iEi = E. See Figure 2(c) for an example. For a single forest Gi  a
straightforward generalization of the algorithm used to compute HBM’s super-partition can be used.
This generalization is simply to use sum-product with graphical model Gi instead of sum-product
on a chain as in HBM (see Appendix A.5 for details). Again  the state space of the node of the
graphical model is {1  2  . . .   N}  but this time the edge potentials enforce the ordering constraints
of the current forest.

4 Experiments

4.1 Matchings

As a ﬁrst experiment  we compared the approximation of SBM described in Section 2 to the Fully
Polynomial Randomized Approximation Scheme (FPRAS) described in [23]. We performed all our
experiments on 100 iid random bipartite graphs of size N  where each edge has iid appearance prob-
ability p  a random graph model that we denote by RB(N  p). In the ﬁrst and second experiments  we
used RB(10  0.9). In this case  exact computation is still possible  and we compared the mean Root
Mean Squared (RMS) of the estimated moments to the truth. In Figure 3(a)  we plot this quantity as
a function of the time spent to compute the 100 approximations. In the variational approximation 
we measured performance at each iteration of BPMF  and in the sampling approach  we measured
performance after powers of two sampling rounds. The conclusion is that the variational approxi-
mation attains similar levels of error in at least one order of magnitude less time in the RB(10  0.9)
regime.
Next  we show in Figure 3(b) the behavior of the algorithms as a function of p  where we also
added the mean ﬁeld algorithm to the comparison. In each data point in the graph  the FPRAS was
run no less than one order of magnitude more time than the variational algorithms. Both variational
strategies outperform the FPRAS in low-density regimes  where mean ﬁeld also slightly outperforms
BPMF. On the other hand  for high-density regimes  only BPMF outperforms the FPRAS  and mean
ﬁeld has a bias compared to the other two methods.
The third experiment concerns the augmented matching model  HBM. Here we compare two types
of factorization and investigate the scalability of the approaches to larger graphs. Factorization F1
is a simpler factorization of the form described in Section 3.1 for non-bipartite graphs. This ignores
the higher-order sufﬁcient statistic coordinates  creating an outer approximation. Factorization F2 

7

0.11101001000Time (s)00.10.20.30.4Mean RMSFPRASBPMF0.50.60.70.80.91Bipartite Graph Density00.10.20.30.40.50.60.7Mean RMSFPRASMean FieldLoopy BP102030405060Graph size00.020.040.060.080.10.120.14Mean Normalized LossHBM-F1HBM-F2Sum of Pairs score (SP)

BAliBASE protein group
short  < 25% identity
short  20% — 40% identity
short  > 35% identity
All

BPMF-1 BPMF-2 BPMF-3 Clustal [24]

ProbCons [25]

0.68
0.94
0.97
0.88

0.74
0.95
0.98
0.91

0.76
0.95
0.98
0.91

0.71
0.89
0.97
0.88

0.72
0.92
0.98
0.89

Table 1: Average SP scores in the ref1/test1 directory of BAliBASE. BPMF-i denotes the average SP of the
BPMF algorithm after i iterations of (parallel) message passing.

described in Section 3.1 speciﬁcally for HBM  is tighter. The experimental setup is based on a gen-
erative model over noisy observations of bipartite perfect matchings described in Appendix C.2. We
show in Figure 3(c) the results of a sequence of these experiments for different bipartite component
sizes N/2. This experiments demonstrates the scalability of sophisticated factorizations  and their
superiority over simpler ones.

4.2 Multiple sequence alignment

To assess the practical signiﬁcance of this framework  we also apply it to BAliBASE [6]  a standard
protein multiple sequence alignment benchmark. We compared our system to Clustal 2.0.12 [24] 
the most popular multiple alignment tool  and ProbCons 1.12  a state-of-the-art system [25] that also
relies on enforcing transitivity constraints  but which is not derived via the optimization of an objec-
tive function. Our system uses a basic pair HMM [26] to score pairwise alignments. This scoring
function captures a proper subset of the biological knowledge exploited by Clustal and ProbCons.6
The advantage of our system over the other systems is the better optimization technique  based on
the measure factorization described in Section 3.2. We used a standard technique to transform the
pairwise alignment marginals into a single valid multiple sequence alignment (see Appendix C.3).
Our system outperformed both baselines after three BPMF parallel message passing iterations. The
algorithm converged in all protein groups  and performance was identical after more than three itera-
tions. Although the overall performance gain is not statistically signiﬁcant according to a Wilcoxon
signed-rank test  the larger gains were obtained in the small identity subset  the “twilight zone”
where research on multiple sequence alignment has focused.
One caveat of this multiple alignment approach is its running time  which is cubic in the length of
the longest sequence  while most multiple sequence alignment approaches are quadratic. For exam-
ple  the running time for one iteration of BPMF in this experiment was 364.67s  but only 0.98s for
Clustal—this is why we have restricted the experiments to the short sequences section of BAliBASE.
Fortunately  several techniques are available to decrease the computational complexity of this algo-
rithm: the transitivity factors can be subsampled using a coarse pass  or along a phylogenetic tree;
and computation of the factors can be entirely parallelized. These improvements are orthogonal to
the main point of this paper  so we leave them for future work.

5 Conclusion

Computing the moments of discrete exponential families can be difﬁcult for two reasons: the struc-
ture of the sufﬁcient statistic that can create junction trees of high tree-width  and the structure of
the base measures that can induce an intractable combinatorial space. Most previous work on vari-
ational approximations has focused on the ﬁrst difﬁculty; however  the second challenge also arises
frequently in machine learning. In this work  we have presented a framework that ﬁlls this gap.
It is based on an intuitive notion of measure factorization  which  as we have shown  applies to
a variety of combinatorial spaces. This notion enables variational algorithms to be adapted to the
combinatorial setting. Our experiments both on synthetic and naturally-occurring data demonstrate
the viability of the method compared to competing state-of-the-art algorithms.

6More precisely it captures long gap and hydrophobic core modeling.

8

References
[1] Alexander Karzanov and Leonid Khachiyan. On the conductance of order Markov chains. Order 

V8(1):7–15  March 1991.

[2] Mark Jerrum  Alistair Sinclair  and Eric Vigoda. A polynomial-time approximation algorithm for the
In Proceedings of the Annual ACM Symposium on

permanent of a matrix with non-negative entries.
Theory of Computing  pages 712–721  2001.

[3] David Wilson. Mixing times of lozenge tiling and card shufﬂing Markov chains. The Annals of Applied

Probability  14:274–325  2004.

[4] Adam Siepel and David Haussler. Phylogenetic estimation of context-dependent substitution rates by

maximum likelihood. Mol Biol Evol  21(3):468–488  2004.

[5] Martin J. Wainwright and Michael I. Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends in Machine Learning  1:1–305  2008.

[6] Julie Thompson  Fr´ed´eric Plewniak  and Olivier Poch. BAliBASE: A benchmark alignments database for

the evaluation of multiple sequence alignment programs. Bioinformatics  15:87–88  1999.

[7] David A. Smith and Jason Eisner. Dependency parsing by belief propagation.

In Proceedings of the
Conference on Empirical Methods in Natural Language Processing (EMNLP)  pages 145–156  Honolulu 
October 2008.

[8] David Burkett  John Blitzer  and Dan Klein.

Joint parsing and alignment with weakly synchronized

grammars. In North American Association for Computational Linguistics  Los Angeles  2010.

[9] Bert Huang and Tony Jebara. Approximating the permanent with belief propagation. ArXiv e-prints 

2009.

[10] Yusuke Watanabe and Michael Chertkov. Belief propagation and loop calculus for the permanent of a

non-negative matrix. J. Phys. A: Math. Theor.  2010.

[11] Ben Taskar  Dan Klein  Michael Collins  Daphne Koller  and Christopher Manning. Max-margin parsing.

In EMNLP  2004.

[12] Ben Taskar  Simon Lacoste-Julien  and Dan Klein. A discriminative matching approach to word align-

ment. In EMNLP 2005  2005.

[13] John Duchi  Daniel Tarlow  Gal Elidan  and Daphne Koller. Using combinatorial optimization within

max-product belief propagation. In Advances in Neural Information Processing Systems  2007.

[14] Aron Culotta  Andrew McCallum  Bart Selman  and Ashish Sabharwal. Sparse message passing algo-
rithms for weighted maximum satisﬁability. In New England Student Symposium on Artiﬁcial Intelligence 
2007.

[15] Percy Liang  Ben Taskar  and Dan Klein. Alignment by agreement. In North American Association for

Computational Linguistics (NAACL)  pages 104–111  2006.

[16] Percy Liang  Dan Klein  and Michael I. Jordan. Agreement-based learning.

Information Processing Systems (NIPS)  2008.

In Advances in Neural

[17] Leslie G. Valiant. The complexity of computing the permanent. Theoret. Comput. Sci.  1979.
[18] Jonathan S. Yedidia  William T. Freeman  and Yair Weiss. Generalized belief propagation. In Advances

in Neural Information Processing Systems  pages 689–695  Cambridge  MA  2001. MIT Press.

[19] Carsten Peterson and James R. Anderson. A mean ﬁeld theory learning algorithm for neural networks.

Complex Systems  1:995–1019  1987.

[20] Martin J. Wainwright  Tommi S. Jaakkola  and Alan S. Willsky. Tree-reweighted belief propagation algo-
rithms and approximate ML estimation by pseudomoment matching. In Proceedings of the International
Conference on Articial Intelligence and Statistics  2003.

[21] Alexandre Bouchard-Cˆot´e and Michael I. Jordan. Optimization of structured mean ﬁeld objectives. In

Proceedings of Uncertainty in Artiﬁcal Intelligence  2009.

[22] Graham Brightwell and Peter Winkler. Counting linear extensions. Order  1991.
[23] Lars Eilstrup Rasmussen. Approximating the permanent: A simple approach. Random Structures and

Algorithms  1992.

[24] Des G. Higgins and Paul M. Sharp. CLUSTAL: a package for performing multiple sequence alignment

on a microcomputer. Gene  73:237–244  1988.

[25] Chuong B. Do  Mahathi S. P. Mahabhashyam  Michael Brudno  and Seraﬁm Batzoglou. PROBCONS:

Probabilistic consistency-based multiple sequence alignment. Genome Research  15:330–340  2005.

[26] David B. Searls and Kevin P. Murphy. Automata-theoretic models of mutation and alignment. In Proc Int

Conf Intell Syst Mol Biol.  1995.

9

,Tzu-Kuo Huang
Jeff Schneider
Evan Archer
Urs Koster
Jonathan Pillow
Jakob Macke
Nikolaos Tziavelis
Ioannis Giannakopoulos
Katerina Doka
Nectarios Koziris
Panagiotis Karras