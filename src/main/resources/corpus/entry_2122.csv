2018,FishNet: A Versatile Backbone for Image  Region  and Pixel Level Prediction,The basic principles in designing convolutional neural network (CNN) structures for predicting objects on different levels  e.g.  image-level  region-level  and pixel-level  are diverging. Generally  network structures designed specifically for image classification are directly used as default backbone structure for other tasks including detection and segmentation  but there is seldom backbone structure designed under the consideration of unifying the advantages of networks designed for pixel-level or region-level predicting tasks  which may require very deep features with high resolution. Towards this goal  we design a fish-like network  called FishNet. In FishNet  the information of all resolutions is preserved and refined for the final task. Besides  we observe that existing works still cannot \emph{directly} propagate the gradient information from deep layers to shallow layers. Our design can better handle this problem. Extensive experiments have been conducted to demonstrate the remarkable performance of the FishNet. In particular  on ImageNet-1k  the accuracy of FishNet is able to surpass the performance of DenseNet and ResNet with fewer parameters. FishNet was applied as one of the modules in the winning entry of the COCO Detection 2018 challenge. The code is available at https://github.com/kevin-ssy/FishNet.,FishNet: A Versatile Backbone for Image  Region 

and Pixel Level Prediction

Shuyang Sun1  Jiangmiao Pang3  Jianping Shi2  Shuai Yi2  Wanli Ouyang1

1The University of Sydney 2SenseTime Research 3Zhejiang University

shuyang.sun@sydney.edu.au

Abstract

The basic principles in designing convolutional neural network (CNN) structures
for predicting objects on different levels  e.g.  image-level  region-level  and pixel-
level  are diverging. Generally  network structures designed speciï¬cally for image
classiï¬cation are directly used as default backbone structure for other tasks includ-
ing detection and segmentation  but there is seldom backbone structure designed
under the consideration of unifying the advantages of networks designed for pixel-
level or region-level predicting tasks  which may require very deep features with
high resolution. Towards this goal  we design a ï¬sh-like network  called FishNet.
In FishNet  the information of all resolutions is preserved and reï¬ned for the ï¬nal
task. Besides  we observe that existing works still cannot directly propagate the
gradient information from deep layers to shallow layers. Our design can better
handle this problem. Extensive experiments have been conducted to demonstrate
the remarkable performance of the FishNet. In particular  on ImageNet-1k  the
accuracy of FishNet is able to surpass the performance of DenseNet and ResNet
with fewer parameters. FishNet was applied as one of the modules in the win-
ning entry of the COCO Detection 2018 challenge. The code is available at
https://github.com/kevin-ssy/FishNet.

1

Introduction

Convolutional Neural Network (CNN) has been found to be effective for learning better feature
representations in the ï¬eld of computer vision [17  26  28  9  37  27  4]. Thereby  the design of CNN
becomes a fundamental task that can help to boost the performance of many other related tasks. As
the CNN becomes increasingly deeper  recent works endeavor to reï¬ne or reuse the features from
previous layers through identity mappings [8] or concatenation [13].
The CNNs designed for image-level  region-level  and pixel-level tasks begin to diverge in network
structure. Networks for image classiï¬cation use consecutive down-sampling to obtain deep features
of low resolution. However  the features with low resolution are not suitable for pixel-level or even
region-level tasks. Direct use of high-resolution shallow features for region and pixel-level tasks 
however  does not work well. In order to obtain deeper features with high resolution  the well-known
network structures for pixel-level tasks use U-Net or hourglass-like networks [22  24  30]. Recent
works on region-level tasks like object detection also use networks with up-sampling mechanism
[21  19] so that small objects can be described by the features with relatively high resolution.
Driven by the success of using high-resolution features for region-level and pixel-level tasks  this
paper proposes a ï¬sh-like network  namely FishNet  which enables the features of high resolution to
contain high-level semantic information. In this way  features pre-trained from image classiï¬cation
are more friendly for region and pixel level tasks.
We carefully design a mechanism that have the following three advantages.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  MontrÃ©al  Canada.

Figure 1: The up/down-sampling block for ResNet (left)  and FishNet (right). The 1 Ã— 1 convolution
layer in yellow indicates the Isolated convolution (I-conv  see Section 2)  which makes the direct BP
incapable and degrades the gradient from the output to shallow layers.

First  it is the ï¬rst backbone network that uniï¬es the advantages of networks designed for pixel-
level  region-level  and image-level tasks. Compared to the networks designed purely for the image
classiï¬cation task  our network as a backbone is more effective for pixel-level and region-level tasks.
Second  it enables the gradient from the very deep layer to be directly propagated to shallow
layers  called direct BP in this paper. Recent works show that there are two designs that can
enable direct BP  identity mapping with residual block [8] and concatenation [13]. However  the
untold fact is that existing network designs  e.g. [9  8  13  28  34  32]  still do not enable direct BP.
This problem is caused by the convolutional layer between features of different resolutions. As shown
in the Figure 1  the ResNet [9] utilize a convolutional layer with stride on the skip connection to deal
with the inconsistency between the numbers of input and output channels  which makes the identity
mapping inapplicable. Convolution without identity mapping or concatenation degrades the gradient
from the output to shallow layers. Our design better solves this problem by concatenating features of
very different depths to the ï¬nal output. We also carefully design the components in the network to
ensure the direct BP. With our design  the semantic meaning of features are also preserved throughout
the whole network.
Third  features of very different depth are preserved and used for reï¬ning each other. Features
with different depth have different levels of abstraction of the image. All of them should be kept to
improve the diversity of features. Because of their complementarity  they can be used for reï¬ning
each other. Therefore  we design a feature preserving-and-reï¬ning mechanism to achieve this goal.
A possibly counter-intuitive effect of our design is that it performs better than traditional convolutional
networks in the trade-off between the number of parameters and accuracy for image classiï¬cation.
The reasons are as follows: 1) the features preserved and reï¬ned are complementary to each other
and more useful than designing networks with more width or depth; and 2) it facilitates the direct BP.
Experimental results show that our compact model FishNet-150  of which the number of parameters
is close to ResNet-50  is able to surpass the accuracy of ResNet-101 and DenseNet-161(k=48) on
ImageNet-1k. For region and pixel level tasks like object detection and instance segmentation  our
model as a backbone for Mask R-CNN [10] improves the absolute AP by 2.8% and 2.3% respectively
on MS COCO compared to the baseline ResNet-50.

1.1 Related works

CNN architectures for image classiï¬cation. The design of deep CNN architecture is a fundamental
but challenging task in deep learning. Networks with better design extract better features  which can
boost the performance of many other tasks. The remarkable improvement in the image recognition
challenge ILSVRC [25] achieved by AlexNet [17] symbolizes a new era of deep learning for computer
vision. After that  a number of works  e.g. VGG [26]  Inception [28]  all propose to promote the
network capability by making the network deeper. However  the network at this time still cannot be
too deep because of the problem of vanishing gradient. Recently  the problem of vanishing gradient
is greatly relieved by introducing the skip connections into the network [9]. There is a series of
on-going works on this direction [29  34  32  13  2  11  31  33]. However  among all these networks
designed for image classiï¬cation  the features of high resolution are extracted by the shallow layers
with small receptive ï¬eld  which lack the high-level semantic meaning that can only be obtained on

2

1Ã—1 ğ‘ğ‘–ğ‘›3Ã—3 ğ‘ğ‘–ğ‘›1Ã—1 ğ‘ğ‘œğ‘¢ğ‘¡1Ã—1 ğ‘ğ‘œğ‘¢ğ‘¡ğ‘ğ‘œğ‘¢ğ‘¡ğ‘†ğ‘¡ğ‘Ÿğ‘–ğ‘‘ğ‘’=21Ã—1 ğ‘ğ‘–ğ‘›3Ã—3 ğ‘ğ‘–ğ‘›1Ã—1 ğ‘ğ‘–ğ‘›CLow-level features ğ‘ğ‘œğ‘¢ğ‘¡âˆ’ğ‘ğ‘–ğ‘›ğ‘ğ‘œğ‘¢ğ‘¡CConcatğ‘¢ğ‘/ğ‘‘ğ‘œğ‘¤ğ‘›ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’Figure 2: Overview of the FishNet. It has three parts. Tail uses existing works to obtain deep
low-resolution features from the input image. Body obtains high-resolution features of high-level
semantic information. Head preserves and reï¬nes the features from the three parts.

deeper layers. Our work is the ï¬rst to extract high-resolution deep feature with high-level semantic
meaning and improve image classiï¬cation accuracy at the same time.
Design in combining features from different layers. Features from different resolution or depth
could be combined using nested sparse networks [16]  hyper-column [7]  addition [18] and residual
blocks [22  21](conv-deconv using residual blocks). Hyper-column networks directly concatenate
features from different layers for segmentation and localization in [7]. However  features from
deep layers and shallow layers were not used for reï¬ning each other. Addition [18] is a fusion of
the features from deep and shallow layers. However  addition only mix the features of different
abstraction levels  but cannot preserve or reï¬ne both of them. Concatenation followed by convolution
is similar to addition [33]. When residual blocks [22  21]  also with addition  are used for combining
features  existing works have a pre-deï¬ned target to be reï¬ned. If the skip layer is for the deep
features  then the shallow features serve only for reï¬ning the deep features  which will be discarded
after the residual blocks in this case. In summary  addition and residual blocks in existing works do
not preserve features from both shallow and deep layers  while our design preserves and reï¬nes them.
Networks with up-sampling mechanism. As there are many other tasks in computer vision  e.g.
object detection  segmentation  that require large feature maps to keep the resolution  it is necessary
to apply up-sampling methods to the network. Such mechanism often includes the communication
between the features with very different depths. The series of works including U-Net [24]  FPN [21] 
stacked hourglass [22] etc.  have all shown their capability in pixel-level tasks [22] and region-level
tasks [21  19]. However  none of them has been proven to be effective for the image classiï¬cation task.
MSDNet [12] tries to keep the feature maps with large resolution  which is the most similar work
to our architecture. However  the architecture of MSDNet still uses convolution between features
of different resolutions  which cannot preserve the representations. Besides  it does not provide an
up-sampling pathway to enable features with large resolution and more semantic meaning. The aim
of MSDNet introducing the multi-scale mechanism into its architecture is to do budget prediction.
Such design  however  did not show improvement in accuracy for image classiï¬cation. Our FishNet
is the ï¬rst in showing that the U-Net structure can be effective for image classiï¬cation. Besides  our
work preserves and reï¬nes features from both shallow and deep layers for the ï¬nal task  which is not
achieved in existing networks with up-sampling or MSDNet.
Message passing among features/outputs. There are some approaches using message passing
among features for segmentation [36]  pose estimation [3] and object detection [35]. These designs
are based on backbone networks  and the FishNet is a backbone network complementary to them.

3

224x224â€¦56x5628x2814x147x714x1428x2856x5628x2814x147x71x1â€¦â€¦â€¦â€¦â€¦â€¦â€¦Features in the tail partFeatures in the body partResidual BlocksFeatures inthe head partConcatFish TailFish BodyFish Headâ€¦â€¦â€¦Ls(cid:88)

l=1

Ls(cid:88)

l=1

2

Identity Mappings in Deep Residual Networks and Isolated Convolution

The basic building block for ResNet is called the residual block. The residual blocks with identity
mapping [8] can be formulated as

(1)
where xl denotes the input feature for the residual block at layer l  and F(xl  Wl) denotes the residual
function with input xl and parameters Wl. We consider the stack of all residual blocks for the same
resolution as a stage. Denote the feature at the lth layer of stage s by xl s. We have:

xl+1 = xl + F(xl  Wl) 

xLs s = x0 s +

F(xl s  Wl s) 

âˆ‚L
âˆ‚x0 s

âˆ‚L
âˆ‚xLs s

=

(1 +

âˆ‚

âˆ‚x0 s

F(xl s  Wl s))

(2)

0 s+1 are different  identity mapping is not applicable.

where Ls denotes the number of stacked residual blocks at the stage s  L is a loss function. The
additive term âˆ‚L
in (2) ensures that the gradient of xLs s can be directly propagated to x0 s. We
âˆ‚xLs s
consider features with different resolutions as having different stages. In the original ResNet  the
features of different resolutions are different in number of channels. Therefore  a transition function
h(Â·) is needed to change the number of channels before down-sampling:
0 s+1 = h(xLs s) = Ïƒ(Î»s âŠ— xLs s + bLs s)
x(cid:48)

(3)
where Ïƒ(Â·) is the activation function. Î»s is the ï¬lter and bLs s is the bias at the transition layer of
stage s respectively. The symbol âŠ— represents the convolution. Since the numbers of channels for
xLs s and x(cid:48)
Gradient propagation problem from Isolated convolution (I-conv). Isolated convolution (I-conv)
is the convolution in (3) without identity mapping or concatenation. As analyzed and validated
by experiments in [8]  it is desirable to have the gradients from a deep layer directly transmitted
to shallow layers. Residual blocks with identity mapping [8] and dense block with concatenation
[13] facilitate such direct gradient propagation. Gradients from the deep layer cannot be directly
transmitted to the shallow layers if there is an I-conv. The I-conv between features with different
resolutions in ResNet [8] and the I-conv (called transition layer in [13]) between adjacent dense
blocks  however  hinders the direct gradient propagation. Since ResNet and DenseNet still have
I-convs  the gradients from the output cannot be directly propagated to shallow layers for them 
similarly for the networks in [17  26]. The invertible down-sampling in [15] avoids the problem of
I-conv by using all features from the current stage for the next stage. The problem is that it will
exponentially increase the number of parameters as the stage ID increases (188M in [15]).
We have identiï¬ed the gradient propagation problem of I-conv in existing networks. Therefore  we
propose a new architecture  namely FishNet  to solve this problem.

3 The FishNet
Figure 2 shows an overview of the FishNet. The whole "ï¬sh" is divided into three parts: tail  body 
and head. The ï¬sh tail is an existing CNN  e.g. ResNet  with the resolution of features becoming
smaller as the CNN goes deeper. The ï¬sh body has several up-sampling and reï¬ning blocks for
reï¬ning features from the tail and the body. The ï¬sh head has several down-sampling and reï¬ning
blocks for preserving and reï¬ning features from the tail  body and head. The reï¬ned features at the
last convolutional layer of the head are used for the ï¬nal task.
Stage in this paper refers to a bunch of convolutional blocks fed by the features with the same
resolution . Each part in the FishNet could be divided into several stages according to the resolution
of the output features. With the resolution becoming smaller  the stage ID goes higher. For
example  the blocks with outputs resolution 56 Ã— 56 and 28 Ã— 28 are at stage 1 and 2 respectively in
all the three parts of the FishNet. Therefore  in the ï¬sh tail and head  the stage ID is becoming higher
while forwarding  while in the body part the ID is getting smaller.
Figure 3 shows the interaction among tail  body  and head for features of two stages. The ï¬sh tail
in Figure 3(a) could be regarded as a residual network. The features from the tail undergo several
residual blocks and are also transmitted to the body through the horizontal arrows. The body in
Figure 3(a) preserves both the features from the tail and the features from the previous stage of the
body by concatenation. Then these concatenated features will be up-sampled and reï¬ned with details

4

Figure 3: (Better seen in color and zoomed in.) (a) Interaction among the tail  body and head for
features of two stages  the two ï¬gures listed on the right exhibit the detailed structure for (b) the
Up-sampling & Reï¬nement block (UR-block)  and (c) the Down-sampling & Reï¬nement block
(DR-block). In the Figure (a)  feature concatenation is used when vertical and horizontal arrows
meet. The notations Câˆ— âˆ—H âˆ—W denote the number of channels  height  and width respectively. k
represents the channel-wise reduction rate described in Equation 8 and Section 3.1. Note that there is
no Isolated convolution (I-conv) in the ï¬sh body and head. Therefore  the gradient from the loss can
be directly propagated to shallow layers in tail  body and head.

shown in Figure 3(b) and the details about the UR-block will be discussed in Section 3.1. The reï¬ned
features are then used for the head and the next stage of the body. The head preserves and reï¬nes
all the features from the body and the previous stage of the head. The reï¬ned features are then used
for the next stage of the head. Details for message passing at the head are shown in Figure 3(c) and
discussed in Section 3.1. The horizontal connections represent the transferring blocks between the
tail  the body and the head. In Figure 3(a)  we use the residual block as the transferring blocks.

3.1 Feature reï¬nement

In the FishNet  there are two kinds of blocks for up/down sampling and feature reï¬nement: the
Up-sampling & Reï¬nement block (UR-block) and Down-sampling & Reï¬nement block (DR-block).
The UR-block. Denote the output features from the ï¬rst layer at the stage s by xt
s for the tail
and body respectively. s âˆˆ {1  2  ...  min(N t âˆ’ 1  N b âˆ’ 1)}  N t and N b represent the number of
stages for the tail part and the body part. Denote feature concatenation as concat(Â·). The UR-block
can be represented as follows:

s and xb

xb
sâˆ’1 = U R(xb

s T (xt

s)) = up(Ëœx(cid:48)b
s )

(4)

where the T denotes residual block transferring the feature xt
represents the feature reï¬ned from the previous stage in the ï¬sh body. The output xb

sâˆ’1 from tail to the body  the up(Ëœx(cid:48)b
s )
sâˆ’1 for next stage

5

ğ¶1+ğ¶2+ğ¶3ğ‘˜ğ¶2+ğ¶3ğ‘Šğ¶22ğ‘Š2ğ»ğ¶1ğ¶3ğ‘Šğ»ğ¶2+ğ¶3ğ‘˜2ğ‘Š2ğ»ğ‘Šğ»ğ‘Š2ğ‘Š2ğ»ğ¶4â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦TransferringBlocks T (â‹…)DR BlocksUR BlocksRegular Connectionsğ¶1+ğ¶2+ğ¶3ğ‘˜+ğ¶4ğ»ğ»2ğ»2ğ‘Šğ¶1+ğ¶2+ğ¶3ğ‘˜+ğ¶4ğ»ğ‘Š2ğ‘Š2ğ»â€¦â€¦FishTailFishBodyFishHead(ğ‘˜+1)ğ¶2+ğ¶3ğ‘˜+ğ¶1+ğ¶4(ğ‘)(ğ‘)(ğ‘)M(â‹…)ğ‘‘ğ‘œğ‘¤ğ‘›(â‹…)ğ‘¢ğ‘(â‹…)ğ‘Ÿ(â‹…)M(â‹…)â€¦â€¦â€¦â€¦ ğ‘¥ğ‘ ğ‘ğ‘¥ğ‘ âˆ’1ğ‘ğ‘¥ğ‘ +1â„ ğ‘¥ğ‘ â€²ğ‘ğ‘Ÿ( ğ‘¥ğ‘ ğ‘) ğ‘¥ğ‘ â„ ğ‘¥ğ‘ â€²â„ConcatT (ğ‘¥ğ‘ ğ‘)ğ‘¥ğ‘ ğ‘T (ğ‘¥ğ‘ ğ‘¡)ğ‘¥ğ‘ â„is reï¬ned from xt

s and xb

s as follows:

sâˆ’1 = up(Ëœx(cid:48)b
xb
s ) 
s) + M(Ëœxb
Ëœx(cid:48)b
s = r(Ëœxb
s) 
s T (xt
Ëœxb
s = concat(xb

s)) 

(5)
(6)
(7)

where up(Â·) denotes the up-sampling function. As a summary  the UR-block concatenates features
from body and tail in (7) and reï¬ne them in (6)  then upsample them in (5) to obtain the output xb
sâˆ’1.
The M in (6) denotes the function that extracts the message from features Ëœxb
s. We implemented M
as convolutions. Similar to the residual function F in (1)  the M in (6) is implemented by bottleneck
Residual Unit [8] with 3 convolutional layers. The channel-wise reduction function r in (6) can be
formulated as follows:

k(cid:88)

r(x) = Ë†x = [Ë†x(1)  Ë†x(2)  . . .   Ë†x(cout)]  Ë†x(n) =

x(k Â· n + j)  n âˆˆ {0  1  ...  cout} 

(8)

j=0

where x = {x(1)  x(2)  . . .   x(cin)} denotes cin channels of input feature maps and Ë†x denotes cout
channels of output feature maps for the function r  cin/cout = k. It is an element-wise summation
of feature maps from the adjacent k channels to 1 channel. We use this simple operation to reduce
the number of channels into 1/k  which makes the number of channels concatenated to the previous
stage to be small for saving computation and parameter size.
The DR-block. The DR-block at the head is similar to the UR-block. There are only two different
implementations between them. First  we use 2 Ã— 2 max-pooling for down-sampling in the DR-block.
Second  in the DR-block  the channel reduction function in the UR-block is not used so that the
gradient at the current stage can be directly transmitted to the parameters at the previous stage.
Following the UR-block in (5)-(7)  the DR block can be implemented as follows:

s+1 = down(Ëœx(cid:48)h
xh
s ) 
s + M(Ëœxh
Ëœx(cid:48)h
s = Ëœxh
s ) 
s  T (xb
Ëœxh
s = concat(xh

s)) 

(9)

where the xh
s+1 denotes the features at the head part for the stage s + 1. In this way  the features
from every stage of the whole network is able to be directly connected to the ï¬nal layer through
concatenation  skip connection  and max-pooling. Note that we do not apply the channel-wise
summation operation r(Â·) deï¬ned in (6) to obtain Ëœxh
s for the DR-block in (9). Therefore  the
layers obtaining Ëœxh

s in the DR-block could be actually regarded as a residual block [8].

s from xh

s from xh

3.2 Detailed design and discussion

Design of FishNet for handling the gradient propagation problem. With the body and head
designed in the FishNet  the features from all stages at the tail and body are concatenated at the head.
We carefully designed the layers in the head so that there is no I-conv in it. The layers in the head are
composed of concatenation  convolution with identity mapping  and max-pooling. Therefore  the
gradient propagation problem from the previous backbone network in the tail are solved with the
FishNet by 1) excluding I-conv at the head; and 2) using concatenation at the body and the head.
Selection of up/down-sampling function. The kernel size is set as 2 Ã— 2 for down-sampling with
stride 2 to avoid the overlapping between pixels. Ablation studies will show the effect of different
kinds of kernel sizes in the network. To avoid the problem from I-conv  the weighted de-convolution
in up-sampling method should be avoided. For simplicity  we choose nearest neighbor interpolation
for up-sampling. Since the up-sampling operation will dilute input features with lower resolution  we
apply dilated convolution in the reï¬ning blocks.
Bridge module between the ï¬sh body and tail. As the tail part will down sample the features into
resolution 1 Ã— 1  these 1 Ã— 1 features need to be upsampled to 7 Ã— 7. We apply a SE-block [11] here
to map the feature from 1 Ã— 1 into 7 Ã— 7 using a channel-wise attentive operation.

6

Figure 4: The comparison of the classiï¬cation top-1 (top-5) error rates as a function of the number of
parameters (left) and FLOP (right) for FishNet  DenseNet and ResNet (single-crop testing) on the
validation set of ImageNet.

25.0M

22.2%

21.5%

Params Top-1 Error

Method
ResNeXt-50
(32 Ã— 4d)
FishNeXt-150
(4d)
Table 1: ImageNet-1k val Top-1 error for
the ResNeXt-based architectures. The
4d here for FishNeXt-150 (4d) indicates
that the minimum number of channels
for a single group is 4.

26.2M

Method
Max-Pooling
(3 Ã— 3  stride=2)
Max-Pooling
(2 Ã— 2  stride=2)
Avg-Pooling
(2 Ã— 2  stride=2)
Convolution
(stride=2)

Params Top-1 Error

26.4M

26.4M

26.4M

30.2M

22.51%

21.93%

22.86%

22.75%

Table 2: ImageNet-1k val Top-1 error for different down-
sampling methods based on FishNet-150.

4 Experiments and Results

4.1

Implementation details on image classiï¬cation

For image classiï¬cation  we evaluate our network on the ImageNet 2012 classiï¬cation dataset [25]
that consists of 1000 classes. This dataset has 1.2 million images for training  and 50 000 images for
validation (denoted by ImageNet-1k val). We implement the FishNet based on the prevalent deep
learning framework PyTorch [23]. For training  we randomly crop the images into the resolution
of 224 Ã— 224 with batch size 256  and choose stochastic gradient descent (SGD) as the training
optimizer with the base learning rate set to 0.1. The weight decay and momentum are 10âˆ’4 and 0.9
respectively. We train the network for 100 epochs  and the learning rate is decreased by 10 times
every 30 epochs. The normalization process is done by ï¬rst converting the value of each pixel into the
interval [0  1]  and then subtracting the mean and dividing the variance for each channel of the RGB
respectively. We follow the way of augmentation (random crop  horizontal ï¬‚ip and standard color
augmentation [17]) used in [9] for fair comparison. All the experiments in this paper are evaluated
through single-crop validation process on the validation dataset of ImageNet-1k. Speciï¬cally  an
image region of size 224 Ã— 224 is cropped from the center of an input image with its shorter side
being resized to 256.This 224 Ã— 224 image region is the input of the network.
FishNet is a framework. It does not specify the building block. For the experimental results in
this paper  FishNet uses the Residual block with identity mapping [8] as the basic building block 
FishNeXt uses the Residual block with identity mapping and grouping [29] as the building block.

4.2 Experimental results on ImageNet

Figure 4 shows the top-1 error for ResNet  DenseNet  and FishNet as a function of the number of
parameters on the validation dataset of ImageNet-1k. When our network uses pre-activation ResNet
as the tail part of the FishNet  the FishNet performs better than ResNet and DenseNet.
FishNet vs. ResNet. For fair comparison  we re-implement the ResNet and report the result of
ResNet-50 and ResNet-101 in Figure 4. Our reported single-crop result for ResNet-50 and ResNet-
101 with identity mapping is higher than that in [9] as we select the residual block with pre-activation
to be our basic building block. Compared to ResNet  FishNet achieves a remarkable reduction in error
rate. The FishNet-150 (21.93%  26.4M)  for which the number of parameters is close to ResNet-50

7

22.59%21.93%(5.92%)21.55%(5.86%)21.25%(5. 76%)22.58%(6.35%)22.20%(6.20%)22.15%(6.12%)21.20%23.78%(7.00%)22.30%(6.20%)21.69%(5.94%)21.00%21.50%22.00%22.50%23.00%23.50%24.00%10203040506070FishNetDenseNetResNetTop-1(Top-5) ErrorTop-1 Error22.59%21.93%21.55%21.25%22.58%22.20%21.20%23.78%22.30%21.69%21.00%21.50%22.00%22.50%23.00%23.50%24.00%24681012FishNetDenseNetResNetParams  Ã—106FLOP  Ã—109Instance Segmentation

Object Detection

Mask R-CNN

Mask R-CNN

FPN
S/APd

Backbone
ResNet-50 [5]
ResNet-50â€ 
ResNeXt-50 (32x4d)â€ 
FishNet-150
vs. ResNet-50â€ 
vs. ResNeXt-50â€ 

S/APs

APs/APs
M /APs
34.5/15.6/37.1/52.1
34.7/18.5/37.4/47.7
35.7/19.1/38.5/48.5
37.0/19.8/40.2/50.3
+2.3/+1.3/+2.8/+2.6
+1.3/+0.7/+1.7/+1.8

L APd/APd

S/APd

L APd/APd

M /APd
M /APd
L
37.9/21.5/41.1/49.9
38.6/22.2/41.5/50.8
38.0/21.4/41.6/50.1
38.7/22.3/42.0/51.2
39.3/23.2/42.3/51.7
40.0/23.1/43.0/52.8
41.5/24.1/44.9/55.0
40.6/23.3/43.9/53.7
+2.8/+1.8/+2.9/+3.8 +2.6/+1.9/+2.3/+3.6
+1.5/+1.0/+1.9/+2.2 +1.3/+0.1/+1.6/+2.0

Table 3: MS COCO val-2017 detection and segmentation Average Precision (%) for different methods.
APsâˆ— and APdâˆ— denote the average precision for segmentation and detection respectively. APâˆ—
S  APâˆ—
M  
and APâˆ—
L respectively denote the AP for the small  medium and large objects. The back-bone networks
are used for two different segmentation and detection approaches  i.e. Mask R-CNN [10] and FPN
[21]. The model re-implemented by us is denoted by a symbol â€ . FishNet-150 does not use grouping 
and the number of parameters for FishNet-150 is close to that of ResNet-50 and ResNeXt-50.
(23.78%  25.5M)  is able to surpass the performance of ResNet-101 (22.30%  44.5M). In terms of
FLOPs  as shown in the right ï¬gure of Figure 4  the FishNet is also able to achieve better performance
with lower FLOPs compared with the ResNet.
FishNet vs. DenseNet. DenseNet iteratively aggregates the features with the same resolution
by concatenation and then reduce the dimension between each dense-block by a transition layer.
According to the results in Figure 4  DenseNet is able to surpass the accuracy of ResNet using
fewer parameters. Since FishNet preserves features with more diversity and better handles the
gradient propagation problem  FishNet is able to achieve better performance than DenseNet with
fewer parameters. Besides  the memory cost of the FishNet is also lower than the DenseNet. Take
the FishNet-150 as an example  when the batch size on a single GPU is 32  the memory cost of
FishNet-150 is 6505M  which is 2764M smaller than the the cost of DenseNet-161 (9269M).
FishNeXt vs. ResNeXt The architecture of FishNet could be combined with other kinds of designs 
e.g.  the channel-wise grouping adopted by ResNeXt. We follow the criterion that the number of
channels in a group for each block (UR/DR block and transfer block) of the same stage should be the
same. The width of a single group will be doubled once the stage index increase by 1. In this way  the
ResNet-based FishNet could be constructed into a ResNeXt-based network  namely FishNeXt. We
construct a compact model FishNeXt-150 with 26 million of parameters. The number of parameters
for FishNeXt-150 is close to ResNeXt-50. From Table 1  the absolute top-1 error rate can be reduced
by 0.7% when compared with the corresponding ResNeXt architecture.

4.3 Ablation studies

Pooling vs. convolution with stride. We investigated four kinds of down-sampling methods based
on the network FishNet-150  including convolution  max-pooling with the kernel size of 2 Ã— 2 and
3Ã— 3  and average pooling with kernel size 2Ã— 21. As shown in Table 2  the performance of applying
2 Ã— 2 max-pooling is better than the other methods. Stride-Convolution will hinder the loss from
directly propagating the gradient to the shallow layer while pooling will not. We also ï¬nd that
max-pooling with kernel size 3 Ã— 3 performs worse than size 2 Ã— 2  as the structural information
might be disturbed by the max-pooling with the 3 Ã— 3 kernel  which has overlapping pooling window.
Dilated convolution. Yu et al. [32] found that the loss of spatial acuity may lead to the limitation of
the accuracy for image classiï¬cation. In FishNet  the UR-block will dilute the original low-resolution
features  therefore  we adopt dilated convolution in the ï¬sh body. When the dilated kernels is used at
the ï¬sh body for up-sampling  the absolute top-1 error rate is reduced by 0.13% based on FishNet-150.
However  there is 0.1% absolute error rate increase if dilated convolution is used in both the ï¬sh body
and head compared to the model without any dilation introduced. Besides  we replace the ï¬rst 7 Ã— 7
stride-convolution layer with two residual blocks  which reduces the absolute top-1 error by 0.18%.

1When convolution with a stride of 2 is used  it is used for both the tail and the head of the FishNet. When
pooling is used  we still put a 1 Ã— 1 convolution on the skip connection of the last residual blocks for each stage
at the tail to change the number of channels between two stages  but we do not use such convolution at the head.

8

4.4 Experimental investigations on MS COCO

We evaluate the generalization capability of FishNet on object detection and instance segmentation
on MS COCO [20]. For fair comparison  all models implemented by ourselves use the same settings
except for the network backbone. All the codes implementing the results reported in this paper about
object detection and instance segmentation are released at [1].
Dataset and Metrics MS COCO [20] is one of the most challenging datasets for object detection and
instance segmentation. There are 80 classes with bounding box annotations and pixel-wise instance
mask annotations. It consists of 118k images for training (train-2017) and 5k images for validation
(val-2017). We train our models on the train-2017 and report results on the val-2017. We evaluate
all models with the standard COCO evaluation metrics AP (averaged mean Average Precision over
different IoU thresholds) [10]  and the APS  APM   APL (AP at different scales).
Implementation Details We re-implement the Feature Pyramid Networks (FPN) and Mask R-CNN
based on PyTorch [23]  and report the re-implemented results in Table 3. Our re-implemented results
are close to the results reported in Detectron[5]. With FishNet  we trained all networks on 16 GPUs
with batch size 16 (one per GPU) for 32 epochs. SGD is used as the training optimizer with a learning
rate 0.02  which is decreased by 10 at the 20 epoch and 28 epoch. As the mini-batch size is small 
the batch-normalization layers [14] in our network are all ï¬xed during the whole training process.
A warming-up training process [6] is applied for 1 epoch and the gradients are clipped below a
maximum hyper-parameter of 5.0 in the ï¬rst 2 epochs to handle the huge gradients during the initial
training stage. The weights of the convolution on the resolution of 224 Ã— 224 are all ï¬xed. We use a
weight decay of 0.0001 and a momentum of 0.9. The networks are trained and tested in an end-to-end
manner. All other hyper-parameters used in experiments follow those in [5].
Object Detection Results Based on FPN. We report the results of detection using FPN with FishNet-
150 on val-2017 for comparison. The top-down pathway and lateral connections in FPN are attached
to the ï¬sh head. As shown in Table 3  the FishNet-150 obtains a 2.6% absolute AP increase to
ResNet-50  and a 1.3% absolute AP increase to ResNeXt-50.
Instance Segmentation and Object Detection Results Based on Mask R-CNN. Similar to the
method adopted in FPN  we also plug FishNet into Mask R-CNN for simultaneous segmentation and
detection. As shown in Table 3  for the task of instance segmentation  2.3% and 1.3% absolute AP
gains are achieved compared to the ResNet-50 and ResNeXt-50. Moreover  when the network is
trained in such multi-task fashion  the performance of object detection could be even better. With
the FishNet plugged into the Mask R-CNN  2.8% and 1.5% improvement in absolute AP have been
observed compared to the ResNet-50 and ResNeXt-50 respectively.
Note that FishNet-150 does NOT use channel-wise grouping  and the number of parameters for
FishNet-150 is close to that of ResNet-50 and ResNeXt-50. When compared with ResNeXt-50 
FishNet-150 only reduces absolute error rate by 0.2% for image classiï¬cation  while it improves the
absolute AP by 1.3% and 1.5% respectively for object detection and instance segmentation. This
shows that the FishNet provides features that are more effective for the region-level task of object
detection and the pixel-level task of segmentation.
COCO Detection Challenge 2018. FishNet was used as one of the network backbones of the
winning entry. By embedding the FishNet into our framework  the single model FishNeXt-229 could
ï¬nally achieve 43.3% on the task of instance segmentation on the test-dev set.

5 Conclusion
In this paper  we propose a novel CNN architecture to unify the advantages of architectures designed
for the tasks recognizing objects on different levels. The design of feature preservation and reï¬nement
not only helps to handle the problem of direct gradient propagation  but also is friendly to pixel-level
and region-level tasks. Experimental results have demonstrated and validated the improvement of
our network. For future works  we will investigate more detailed settings of our network  e.g.  the
number of channels/blocks for each stage  and also the integration with other network architectures.
The performance for larger models on both datasets will also be reported.
Acknowledgement We would like to thank Guo Lu and Olly Styles for their careful proofreading.
We also appreciate Mr. Hui Zhou at SenseTime Research for his broad network that could incredibly
organize the authors of this paper together.

9

References
[1] K. Chen  J. Pang  J. Wang  Y. Xiong  X. Li  S. Sun  W. Feng  Z. Liu  J. Shi  W. Ouyang  C. C. Loy  and

D. Lin. mmdetection. https://github.com/open-mmlab/mmdetection  2018.

[2] Y. Chen  J. Li  H. Xiao  X. Jin  S. Yan  and J. Feng. Dual path networks. In Advances in Neural Information

Processing Systems  pages 4470â€“4478  2017.

[3] X. Chu  W. Ouyang  X. Wang  et al. Crf-cnn: Modeling structured information in human pose estimation.

In Advances in Neural Information Processing Systems  pages 316â€“324  2016.

[4] P. Gao  H. Li  S. Li  P. Lu  Y. Li  S. C. Hoi  and X. Wang. Question-guided hybrid convolution for visual

question answering. arXiv preprint arXiv:1808.02632  2018.

[5] R. Girshick  I. Radosavovic  G. Gkioxari  P. DollÃ¡r  and K. He. Detectron. https://github.com/

facebookresearch/detectron  2018.

[6] P. Goyal  P. DollÃ¡r  R. Girshick  P. Noordhuis  L. Wesolowski  A. Kyrola  A. Tulloch  Y. Jia  and K. He.

Accurate  large minibatch sgd: training imagenet in 1 hour. arXiv preprint arXiv:1706.02677  2017.

[7] B. Hariharan  P. ArbelÃ¡ez  R. Girshick  and J. Malik. Hypercolumns for object segmentation and ï¬ne-
grained localization. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 447â€“456  2015.

[8] K. He  X. Zhang  S. Ren  and J. Sun. Identity mappings in deep residual networks. In European Conference

on Computer Vision  pages 630â€“645. Springer  2016.

[9] K. He  X. Zhang  S. Ren  and J. Sun. Deep residual learning for image recognition. In CVPR  pages

770â€“778  2016.

[10] K. He  G. Gkioxari  P. DollÃ¡r  and R. Girshick. Mask r-cnn. In Computer Vision (ICCV)  2017 IEEE

International Conference on  pages 2980â€“2988. IEEE  2017.

[11] J. Hu  L. Shen  and G. Sun. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507  2017.

[12] G. Huang  D. Chen  T. Li  F. Wu  L. van der Maaten  and K. Q. Weinberger. Multi-scale dense convolutional

networks for efï¬cient prediction. arXiv preprint arXiv:1703.09844  2017.

[13] G. Huang  Z. Liu  K. Q. Weinberger  and L. van der Maaten. Densely connected convolutional networks.

In Proceedings of the IEEE conference on computer vision and pattern recognition  2017.

[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal

covariate shift. arXiv preprint arXiv:1502.03167  2015.

[15] J.-H. Jacobsen  A. Smeulders  and E. Oyallon.

arXiv:1802.07088  2018.

i-revnet: Deep invertible networks. arXiv preprint

[16] E. Kim  C. Ahn  and S. Oh. Nestednet: Learning nested sparse structures in deep neural networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 8669â€“8678 
2018.

[17] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiï¬cation with deep convolutional neural

networks. In Advances in neural information processing systems  pages 1097â€“1105  2012.

[18] G. Larsson  M. Maire  and G. Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals.

arXiv preprint arXiv:1605.07648  2016.

[19] H. Li  Y. Liu  W. Ouyang  and X. Wang. Zoom out-and-in network with map attention decision for region
proposal and object detection. International Journal of Computer Vision  Jun 2018. ISSN 1573-1405. doi:
10.1007/s11263-018-1101-7. URL https://doi.org/10.1007/s11263-018-1101-7.

[20] T.-Y. Lin  M. Maire  S. Belongie  J. Hays  P. Perona  D. Ramanan  P. DollÃ¡r  and C. L. Zitnick. Microsoft
coco: Common objects in context. In European conference on computer vision  pages 740â€“755. Springer 
2014.

[21] T.-Y. Lin  P. DollÃ¡r  R. Girshick  K. He  B. Hariharan  and S. Belongie. Feature pyramid networks for

object detection. In CVPR  2017.

[22] A. Newell  K. Yang  and J. Deng. Stacked hourglass networks for human pose estimation. In European

Conference on Computer Vision  pages 483â€“499. Springer  2016.

[23] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison  L. Antiga  and

A. Lerer. Automatic differentiation in pytorch. 2017.

10

[24] O. Ronneberger  P. Fischer  and T. Brox. U-net: Convolutional networks for biomedical image segmentation.
In International Conference on Medical image computing and computer-assisted intervention  pages 234â€“
241. Springer  2015.

[25] O. Russakovsky  J. Deng  H. Su  J. Krause  S. Satheesh  S. Ma  Z. Huang  A. Karpathy  A. Khosla 
M. Bernstein  et al. Imagenet large scale visual recognition challenge. International Journal of Computer
Vision  115(3):211â€“252  2015.

[26] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

arXiv preprint arXiv:1409.1556  2014.

[27] S. Sun  Z. Kuang  L. Sheng  W. Ouyang  and W. Zhang. Optical ï¬‚ow guided feature: A fast and robust
motion representation for video action recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition  pages 1390â€“1399  2018.

[28] C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke  A. Rabinovich 

et al. Going deeper with convolutions. In CVPR  2015.

[29] S. Xie  R. Girshick  P. DollÃ¡r  Z. Tu  and K. He. Aggregated residual transformations for deep neural
In Computer Vision and Pattern Recognition (CVPR)  2017 IEEE Conference on  pages

networks.
5987â€“5995. IEEE  2017.

[30] W. Yang  S. Li  W. Ouyang  H. Li  and X. Wang. Learning feature pyramids for human pose estimation. In

arXiv preprint arXiv:1708.01101  2017.

[31] Y. Yang  Z. Zhong  T. Shen  and Z. Lin. Convolutional neural networks with alternately updated clique.

arXiv preprint arXiv:1802.10419  2018.

[32] F. Yu  V. Koltun  and T. Funkhouser. Dilated residual networks.

Recognition  volume 1  2017.

In Computer Vision and Pattern

[33] F. Yu  D. Wang  and T. Darrell. Deep layer aggregation. arXiv preprint arXiv:1707.06484  2017.

[34] S. Zagoruyko and N. Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146  2016.

[35] X. Zeng  W. Ouyang  B. Yang  J. Yan  and X. Wang. Gated bi-directional cnn for object detection. In

European Conference on Computer Vision  pages 354â€“369. Springer  2016.

[36] S. Zheng  S. Jayasumana  B. Romera-Paredes  V. Vineet  Z. Su  D. Du  C. Huang  and P. H. Torr. Conditional
random ï¬elds as recurrent neural networks. In Proceedings of the IEEE International Conference on
Computer Vision  pages 1529â€“1537  2015.

[37] H. Zhou  W. Ouyang  J. Cheng  X. Wang  and H. Li. Deep continuous conditional random ï¬elds with
asymmetric inter-object constraints for online multi-object tracking. IEEE Transactions on Circuits and
Systems for Video Technology  2018.

11

,Daniel HernÃ¡ndez-Lobato
JosÃ© Miguel HernÃ¡ndez-Lobato
Tomoya Murata
Taiji Suzuki
Shuyang Sun
Jiangmiao Pang
Jianping Shi
Shuai Yi
Wanli Ouyang