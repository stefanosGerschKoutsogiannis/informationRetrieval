2014,Graph Clustering With Missing Data: Convex Algorithms and Analysis,We consider the problem of finding clusters in an unweighted graph  when the graph is partially observed. We analyze two programs  one which works for dense graphs and one which works for both sparse and dense graphs  but requires some a priori knowledge of the total cluster size  that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model  we obtain \emph{explicit} bounds on the parameters of the problem (size and sparsity of clusters  the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk  and observe significant performance improvement over traditional methods such as k-means.,Graph Clustering With Missing Data : Convex

Algorithms and Analysis

Ramya Korlakai Vinayak  Samet Oymak  Babak Hassibi

Department of Electrical Engineering

{ramya  soymak}@caltech.edu  hassibi@systems.caltech.edu

California Institute of Technology  Pasadena  CA 91125

Abstract

We consider the problem of ﬁnding clusters in an unweighted graph  when the
graph is partially observed. We analyze two programs  one which works for dense
graphs and one which works for both sparse and dense graphs  but requires some a
priori knowledge of the total cluster size  that are based on the convex optimization
approach for low-rank matrix recovery using nuclear norm minimization. For
the commonly used Stochastic Block Model  we obtain explicit bounds on the
parameters of the problem (size and sparsity of clusters  the amount of observed
data) and the regularization parameter characterize the success and failure of the
programs. We corroborate our theoretical ﬁndings through extensive simulations.
We also run our algorithm on a real data set obtained from crowdsourcing an
image classiﬁcation task on the Amazon Mechanical Turk  and observe signiﬁcant
performance improvement over traditional methods such as k-means.

1

Introduction

(cid:1)

Clustering [1] broadly refers to the problem of identifying data points that are similar to each other.
It has applications in various problems in machine learning  data mining [2  3]  social networks [4–
6]  bioinformatics [7  8]  etc. In this paper we focus on graph clustering [9] problems where the data

is in the form of an unweighted graph. Clearly  to observe the entire graph on n nodes requires(cid:0)n

2
measurements. In most practical scenarios this is infeasible and we can only expect to have partial
observations. That is  for some node pairs we know whether there exists an edge between them
or not  whereas for the rest of the node pairs we do not have this knowledge. This leads us to the
problem of clustering graphs with missing data.
Given the adjacency matrix of an unweighted graph  a cluster is deﬁned as a set of nodes that are
densely connected to each other when compared to the rest of the nodes. We consider the problem of
identifying such clusters when the input is a partially observed adjacency matrix. We use the popular
Stochastic Block Model (SBM) [10] or Planted Partition Model [11] to analyze the performance of
the proposed algorithms. SBM is a random graph model where the edge probability depends on
whether the pair of nodes being considered belong to the same cluster or not. More speciﬁcally  the
edge probability is higher when both nodes belong to the same cluster. Further  we assume that each
entry of the adjacency matrix of the graph is observed independently with probability r. We will
deﬁne the model in detail in Section 2.1.

1.1 Clustering by Low-Rank Matrix Recovery and Completion

The idea of using convex optimization for clustering has been proposed in [12–21]. While each of
these works differ in certain ways  and we will comment on their relation to the current paper in
Section 1.3  the common approach they use for clustering is inspired by recent work on low-rank
matrix recovery and completion via regularized nuclear norm (trace norm) minimization [22–26].

1

In the case of unweighted graphs  an ideal clustered graph is a union of disjoint cliques. Given
the adjacency matrix of an unweighted graph with clusters (denser connectivity inside the clusters
compared to outside)  we can interpret it as an ideal clustered graph with missing edges inside the
clusters and erroneous edges in between clusters. Recovering the low-rank matrix corresponding to
the disjoint cliques is equivalent to ﬁnding the clusters.
We will look at the following well known convex program which aims to recover and complete the
low-rank matrix (L) from the partially observed adjacency matrix (Aobs):
Simple Convex Program:

(1.1)

(cid:107)L(cid:107)(cid:63) + λ(cid:107)S(cid:107)1

minimize

L S

subject to

1 ≥ Li j ≥ 0 for all i  j ∈ {1  2  . . . n}
Lobs + Sobs = Aobs

(1.2)
(1.3)
where λ ≥ 0 is the regularization parameter  (cid:107).(cid:107)(cid:63) is the nuclear norm (sum of the singular values
of the matrix)  and (cid:107).(cid:107)1 is the l1-norm (sum of absolute values of the entries of the matrix). S is
the sparse error matrix that accounts for the missing edges inside the clusters and erroneous edges
outside the clusters on the observed entries. Lobs and Sobs denote entries of L and S that correspond
to the observed part of the adjacency matrix.
Program 1.1 is very simple and intuitive. Further  it does not require any information other than
the observed part of the adjacency matrix. In [13]  the authors analyze Program 1.1 without the
constraint (1.2). While dropping (1.2) makes the convex program less effective  it does allow [13] to
make use of low-rank matrix completion results for its analysis. In [16] and [21]  the authors analyze
Program 1.1 when the entire adjacency matrix is observed. In [17]  the authors study a slightly more
general program  where the regularization parameter is different for the extra edges and the missing
edges. However  the adjacency matrix is completely observed.
It is not difﬁcult to see that  when the edge probability inside the cluster is p < 1/2  that (as n → ∞)
Program 1.1 will return L0 = 0 as the optimal solution (since if the cluster is not dense enough it is
more costly to complete the missing edges). As a result our analysis of Program 1.1  and the main
result of Theorem 1  assumes p > 1/2. Clearly  there are many instances of graphs we would like
to cluster where p < 1/2. If the total size of the cluster region (i.e  the total number of edges in
the cluster  denoted by |R|) is known  then the following convex program can be used  and can be
shown to work for p < 1/2 (see Theorem 2).
Improved Convex Program:

minimize

L S

(cid:107)L(cid:107)(cid:63) + λ(cid:107)S(cid:107)1

(1.4)

subject to

i j = 0

1 ≥ Li j ≥ Si j ≥ 0 for all i  j ∈ {1  2  . . . n}
Li j = Si j whenever Aobs
sum(L) ≥ |R|

(1.5)
(1.6)
(1.7)
As before  L is the low-rank matrix corresponding to the ideal cluster structure and λ ≥ 0 is the
regularization parameter. However  S is now the sparse error matrix that accounts only for the
missing edges inside the clusters on the observed part of adjacency matrix. [16] and [19] study
programs similar to Program 1.4 for the case of a completely observed adjacency matrix. In [19] 
the constraint 1.7 is a strict equality. In [15] the authors analyze a program close to Program 1.4 but
without the l1 penalty.
If R is not known  it is possible to solve Problem 1.4 for several values of R until the desired
performance is obtained. Our empirical results reported in Section 3  suggest that the solution is not
very sensitive to the choice of R.

1.2 Our Contributions
• We analyze the Simple Convex Program 1.1 for the SBM with partial observations. We provide
explicit bounds on the regularization parameter as a function of the parameters of the SBM  that

2

characterizes the success and failure conditions of Program 1.1 (see results in Section 2.2). We
show that clusters that are either too small or too sparse constitute the bottleneck. Our analysis is
helpful in understanding the phase transition from failure to success for the simple approach.
• We also analyze the Improved Convex Program 1.4. We explicitly characterize the conditions on
the parameters of the SBM and the regularization parameter for successfully recovering clusters
using this approach (see results in Section 2.3).
• Apart from providing theoretical guarantees and corroborating them with simulation results (Sec-
tion 3)  we also apply Programs 1.1 and 1.4 on a real data set (Section 3.3) obtained by crowd-
sourcing an image labeling task on Amazon Mechanical Turk.

1.3 Related Work

In [13]  the authors consider the problem of identifying clusters from partially observed un-
weighted graphs. For the SBM with partial observations  they analyze Program 1.1 without con-
straint (1.2)  and show that under certain conditions  the minimum cluster size must be at least

O((cid:112)n(log(n))4/r) for successful recovery of the clusters. Unlike our analysis  the exact require-

ment on the cluster size is not known (since the constant of proportionality is not known). Also
they do not provide conditions under which the approach fails to identify the clusters. Finding the
explicit bounds on the constant of proportionality is critical to understanding the phase transition
from failure to successfully identifying clusters.
√
In [14–19]  analyze convex programs similar to the Programs 1.1 and 1.4 for the SBM and show
that the minimum cluster size should be at least O(
n) for successfully recovering the clusters.
However  the exact requirement on the cluster size is not known. Also  they do not provide explicit
conditions for failure  and except for [16] they do not address the case when the data is missing.
In contrast  we consider the problem of clustering with missing data. We explicitly characterize
the constants by providing bounds on the model parameters that decide if Programs 1.1 and 1.4
can successfully identify clusters. Furthermore  for Program 1.1  we also explicitly characterize the
conditions under which the program fails.
In [16]  the authors extend their results to partial observations by scaling the edge probabilities by r
(observation probability)  which will not work for r < 1/2 or 1/2 < p < 1/2r in Program 1.1 . [21]
analyzes Program 1.1 for the SBM and provides conditions for success and failure of the program
when the entire adjacency matrix is observed. The dependence on the number of observed entries
emerges non-trivially in our analysis. Further  [21] does not address the drawback of Program 1.1 
which is p > 1/2  whereas in our work we analyze Program 1.4 that overcomes this drawback.
2 Partially Observed Unweighted Graph

2.1 Model

Deﬁnition 2.1 (Stochastic Block Model). Let A = AT be the adjacency matrix of a graph on n
nodes with K disjoint clusters of size ni each  i = 1  2 ···   K. Let 1 ≥ pi ≥ 0  i = 1 ···   K and
1 ≥ q ≥ 0. For l > m 

(cid:26)1 w.p. pi 

1 w.p. q 

Al m =

if both nodes l  m are in the same cluster i.
if nodes l  m are not in the same cluster.

(2.1)

If pi > q for each i  then we expect the density of edges to be higher inside the clusters compared to
outside. We will say the random variable Y has a Φ(r  δ) distribution  for 0 ≤ δ  r ≤ 1  written as
Y ∼ Φ(r  δ)  if

1  w.p. rδ

0  w.p. r(1 − δ)
∗  w.p. (1 − r)

Y =

where ∗ denotes unknown.
Deﬁnition 2.2 (Partial Observation Model). Let A be the adjacency matrix of a random graph
generated according to the Stochastic Block Model of Deﬁnition 2.1. Let 0 < r ≤ 1. Each entry of

3

the adjacency matrix A is observed independently with probability r. Let Aobs denote the observed
adjacency matrix. Then for l > m: (Aobs)l m ∼ Φ(r  pi) if both the nodes l and m belong to the
same cluster i. Otherwise  (Aobs)l m ∼ Φ(r  q).

2.2 Results : Simple Convex Program
Let [n] = {1  2 ···   n}. Let R be the union of regions induced by the clusters and Rc = [n]× [n]−
ni 

R its complement. Note that |R| =(cid:80)K

i and |Rc| = n2 −(cid:80)K

i=1 n2

i . Let nmin := min
1≤i≤K

pmin := min
1≤i≤K

pi and nmax := max
1≤i≤K

i=1 n2
ni.

The following deﬁnitions are important to describe our results.
• Deﬁne Di := ni r (2pi − 1) as the effective density of cluster i and Dmin = min
1≤i≤K
• γsucc := max
1≤i≤K
√
• Λ−1

(cid:113)
(cid:113) 1
ni
r − 1 + 4q(1 − q) + γsucc and Λ−1

r − 1) + 4 (q(1 − q) + pi(1 − pi)) and γfail :=(cid:80)K

fail :=(cid:112)rq(n − γfail).

succ := 2r

2( 1

√

n2
i
n

2r

i=1

n

Di.

We note that the thresholds  Λsucc and Λfail depend only the parameters of the model. Some simple
algebra shows that Λsucc < Λfail.
Theorem 1 (Simple Program). Consider a random graph generated according to the Partial Obser-
i=1  and probabilities {pi}K
vation Model of Deﬁnition (2.2) with K disjoint clusters of sizes {ni}K
i=1
and q  such that pmin > 1
2 such that 
1. If λ ≥ (1 + )Λfail  then Program 1.1 fails to correctly recover the clusters with probability

2 > q > 0. Given  > 0  there exists positive constants c(cid:48)

1  c(cid:48)

1 − c(cid:48)

1 exp(−c(cid:48)

2|Rc|).

2. If 0 < λ ≤ (1 − )Λsucc 
• If Dmin ≥ (1 + ) 1
• If Dmin ≤ (1 − ) 1
2nmin).

2nmin).

1n2 exp(−c(cid:48)

λ   then Program 1.1 succeeds in correctly recovering the clusters with

λ   then Program 1.1 fails to correctly recover the clusters with probability

probability 1 − c(cid:48)
1 − c(cid:48)
1 exp(−c(cid:48)
Discussion:
1. Theorem 1 characterizes the success and failure of Program 1.1 as a function of the regularization
parameter λ. In particular  if λ > Λfail  Program 1.1 fails with high probability. If λ < Λsucc 
Program 1.1 succeeds with high probability if and only if Dmin > 1
λ. However  Theorem 1 has
nothing to say about Λsucc < λ < Λfail.
√

2. Small Cluster Regime: When nmax = o(n)  we have Λ−1

(cid:113)(cid:0) 1
r − 1 + 4q(1 − q)(cid:1).

For simplicity let pi = p  ∀ i  which yields Dmin = nminr(2p− 1). Then Dmin > Λ−1

succ = 2r

succ implies 

n

(cid:115)(cid:18) 1

r

√

2
n
2p − 1

(cid:19)

nmin >

− 1 + 4q(1 − q)

 

(2.2)

giving a lower bound on the minimum cluster size that is sufﬁcient for success.

2.3 Results: Improved Convex Program

The following deﬁnitions are critical to describe our results.
• Deﬁne ˜Di := ni r (pi − q) as the effective density of cluster i and ˜Dmin = min
1≤i≤K
• ˜γsucc := 2 max
1≤i≤K

r − 1 + pi) + (1 − q)( 1

r − 1 + q)

(1 − pi)( 1

(cid:113)

√

ni

r

˜Di.

4

(a)

(b)

Figure 1: Region of success (white region) and failure (black region) of Program 1.1 with λ =
1.01D−1
min. The solid red curve is the threshold for success (λ < Λsucc) and the dashed green line
which is the threshold for failure (λ > Λfail) as predicted by Theorem 1.

(cid:113)

• ˜Λ−1

succ := 2r

√

n

r − 1 + q)(1 − q) + ˜γsucc.
( 1

i=1 and q  such that pmin > q > 0. Given  > 0  there exists positive constants c(cid:48)

We note that the threshold  ˜Λsucc depends only on the parameters of the model.
Theorem 2 (Improved Program). Consider a random graph generated according to the Partial
Observation Model of Deﬁnition 2.2  with K disjoint clusters of sizes {ni}K
i=1  and probabilities
{pi}K
2 such
that: If 0 < λ ≤ (1 − ) ˜Λsucc and ˜Dmin ≥ (1 + ) 1
λ   then Program 1.4 succeeds in recovering the
clusters with probability 1 − c(cid:48)
Discussion:1
1. Theorem 2 gives a sufﬁcient condition for the success of Program 1.4 as a function of λ. In

1n2 exp(−c(cid:48)

2nmin).

1  c(cid:48)

particular  for any λ > 0  we succeed if ˜D−1

min < λ < ˜Λsucc.
2. Small Cluster Regime: When nmax = o(n)  we have ˜Λ−1

succ = 2r

n

(cid:113)(cid:0) 1
r − 1 + q(cid:1) (1 − q). For

√

simplicity let pi = p  ∀ i  which yields ˜Dmin = nminr(p − q). Then ˜Dmin > ˜Λ−1

succ implies 

(cid:115)(cid:18) 1

r

√
2
n
p − q

(cid:19)

nmin >

− 1 + q

(1 − q) 

(2.3)

which gives a lower bound on the minimum cluster size that is sufﬁcient for success.

3. (p  q) as a function of n: We now brieﬂy discuss the regime in which cluster sizes are large
(i.e. O(n)) and we are interested in the parameters (p  q) as a function of n that allows proposed
approaches to be successful. Critical to Program 1.4 is the constraint (1.6): Li j = Si j when
i j = 0 (which is the only constraint involving the adjacency Aobs). With missing data 
Aobs
i j = 0 with probability r(1− p) inside the clusters and r(1− q) outside the clusters. Deﬁning
Aobs
ˆp = rp + 1 − r and ˆq = rq + 1 − r  the number of constraints in (1.6) becomes statistically
equivalent to those of a fully observed graph where p and q are replaced by ˆp and ˆq. Consequently 
for a ﬁxed r > 0  from (2.3)  we require p ≥ p − q (cid:38) O( 1√
n ) for success. However  setting
the unobserved entries to 0  yields Ai j = 0 with probability 1 − rp inside the clusters and
1 − rq outside the clusters. This is equivalent to a fully observed graph where p and q are
replaced by rp and rq. In this case  we can allow p ≈ O( 1
n ) for success which is order-wise
better  and matches the results in McSherry [27]. Intuitively  clustering a fully observed graph
with parameters ˆp = rp + 1 − r and ˆq = rq + 1 − r is much more difﬁcult than one with rp
and rq  since the links are more noisy in the former case. Hence  while it is beneﬁcial to leave
the unobserved entries blank in Program 1.1  for Program 1.4 it is in fact beneﬁcial to set the
unobserved entries to 0.

1The proofs for Theorems 1 and 2 are provided in the supplementary material.

5

Edge Probability inside the cluster (p)Observation Probability (r) 0.60.70.80.910.20.40.60.81SuccessFailureMinimum Cluster SizeObservation Probability (r) 501001502000.20.40.60.81SuccessFailure(a) Region of success (white region) and failure
(black region) of Program 1.4 with λ = 0.49 ˜Λsucc.
The solid red curve is the threshold for success
( ˜Dmin > λ−1) as predicted by Theorem 2.

(b) Comparison range of edge probability p for Sim-
ple Program 1.1 and Improved Program 1.4.

Figure 2: Simulation results for Improved Program.

3 Experimental Results

We implement Program 1.1 and 1.4 using the inexact augmented Lagrange method of multipli-
ers [28]. Note that this method solves the Program 1.1 and 1.4 approximately. Further  the numerical
imprecisions will prevent the entries of the output of the algorithms from being strictly equal to 0 or
1. We use the mean of all the entries of the output as a hard threshold to round each entry. That is 
if an entry is less than the threshold  it is rounded to 0 and to 1 otherwise. We compare the output
of the algorithm after rounding to the optimal solution (L0)  and declare success if the number of
wrong entries is less than 0.1%.
Set Up: We consider at an unweighted graph on n = 600 nodes with 3 disjoint clusters. For
simplicity the clusters are of equal size n1 = n2 = n3  and the edge probability inside the clusters
are same p1 = p2 = p3 = p. The edge probability outside the clusters is ﬁxed  q = 0.1. We generate
the adjacency matrix randomly according to the Stochastic Block Model 2.1 and Partial Observation
Model 2.2. All the results are an average over 20 experiments.
3.1 Simulations for Simple Convex Program

Dependence between r and p: In the ﬁrst set of experiments we keep n1 = n2 = n3 = 200  and
vary p from 0.55 to 1 and r from 0.05 to 1 in steps of 0.05.
Dependence between nmin and r: In the second set of experiments we keep the edge probability
inside the clusters ﬁxed  p = 0.85. The cluster size is varied from nmin = 20 to nmin = 200 in steps
of 20 and r is varied from 0.05 to 1 in steps of 0.05.
In both the experiments  we set the regularization parameter λ = 1.01D−1
min  ensuring that Dmin >
1/λ  enabling us to focus on observing the transition around Λsucc and Λfail. The outcome of the
experiments are shown in the Figures 1a and 1b. The experimental region of success is shown in
white and the region of failure is shown in black. The theoretical region of success is about the solid
red curve (λ < Λsucc) and the region of failure is below dashed green curve (λ > Λfail). As we can
see the transition indeed occurs between the two thresholds Λsucc and Λfail.

3.2 Simulations for Improved Convex Program

We keep the cluster size  n1 = n2 = n3 = 200 and vary p from 0.15 to 1 and r from 0.05 to 1 in
steps of 0.05. We set the regularization parameter  λ = 0.49 ˜Λsucc  ensuring that λ < ˜Λsucc  enabling
us to focus on observing the condition of success around ˜Dmin. The outcome of this experiment is
shown in the Figure 2a. The experimental region of success is shown in white and region of failure
is shown in black. The theoretical region of success is above solid red curve.
Comparison with the Simple Convex Program: In this experiment  we are interested in observing
the range of p for which the Programs 1.1 and 1.4 work. Keeping the cluster size n1 = n2 = n3 =

6

Observation Probability (r)Edge Probability inside the cluster (p) 0.20.40.60.810.20.40.60.81Success0.20.40.60.8100.51Edge Probability inside the clusters (p)Probability of Success SimpleImproved(a)

(b)

(c) Comparing with k-means clustering.

Figure 3: Result of using (a) Program 1.1 (Simple) and (b) Program 1.4 (Improved) on the real data
set. (c) Comparing the clustering output after running Program 1.1 and Program 1.4 with the output
of applying k-means clustering directly on A (with unknown entries set to 0).

200 and r = 1  we vary the edge probability inside the clusters from p = 0.15 to p = 1 in steps
of 0.05. For each instance of the adjacency matrix  we run both Program 1.1 and 1.4. We plot the
probability of success of both the algorithms in Figure 2b. As we can observe  Program 1.1 starts
succeeding only after p > 1/2  whereas for Program 1.4 it starts at p ≈ 0.35.

3.3 Labeling Images: Amazon MTurk Experiment

Creating a training dataset by labeling images is a tedious task. It would be useful to crowdsource
this task instead. Consider a speciﬁc example of a set of images of dogs of different breeds. We want
to cluster them such that the images of dogs of the same breed are in the same cluster. One could
show a set of images to each worker  and ask him/her to identify the breed of dog in each of those
images. But such a task would require the workers to be experts in identifying the dog breeds. A
relatively reasonable task is to ask the workers to compare pairs of images  and for each pair  answer
whether they think the dogs in the images are of the same breed or not. If we have n images  then

(cid:1) distinct pairs of images  and it will pretty quickly become unreasonable to compare all

there are(cid:0)n

possible pairs. This is an example where we could obtain a subset of the data and try to cluster the
images based on the partial observations.
Image Data Set: We used images of 3 different breeds of dogs : Norfolk Terrier (172 images)  Toy
Poodle (151 images) and Bouvier des Flandres (150 images) from the Standford Dogs Dataset [29].
We uploaded all the 473 images of dogs on an image hosting server (we used imgur.com).
MTurk Task: We used Amazon Mechanical Turk [30] as the platform for crowdsourcing. For

each worker  we showed 30 pairs of images chosen randomly from the(cid:0)n

(cid:1) possible pairs. The task

2

2

assigned to the worker was to compare each pair of images  and answer whether they think the dogs
belong to the same breed or not. If the worker’s response is a “yes”  then there we ﬁll the entry of
the adjacency matrix corresponding to the pair as 1  and 0 if the answer is a “no”.
Collected Data: We recorded around 608 responses. We were able to ﬁll 16  750 out of 111  628
entries in A. That is  we observed 15% of the total number of entries. Compared with true answers
(which we know a priori)  the answers given by the workers had around 23.53% errors (3941 out of
16750). The empirical parameters for the partially observed graph thus obtained is shown Table 1.
√
n. Further  for Pro-
We ran Program 1.1 and Program 1.4 with regularization parameter  λ = 1/

gram 1.4  we set the size of the cluster region  R to 0.125 times(cid:0)n

(cid:1). Figure 3a shows the recovered

matrices. Entries with value 1 are depicted by white and 0 is depicted by black. In Figure 3c we
compare the clusters output by running the k-means algorithm directly on the adjacency matrix
A (with unknown entries set to 0) to that obtained by running k-means algorithm on the matrices
recovered after running Program 1.1 (Simple Program) and Program 1.4 (Improved Program) re-
spectively. The overall error with k-means was 40.8% whereas the error signiﬁcantly reduced to
15.86% and 7.19% respectively when we used the matrices recoverd from Programs 1.1 and 1.4
respectively (see Table 2). Further  note that for running the k-means algorithm we need to know
the exact number of clusters. A common heuristic is to identify the top K eigenvalues that are much

2

7

Matrix Recovered by Simple Program10020030040050100150200250300350400450Matrix Recovered by Improved Program10020030040050100150200250300350400450Ideal Clusters501001502002503003504004500.511.5Clusters identifyed by k−means on A501001502002503003504004500.511.5Clusters Identified from Simple Program501001502002503003504004500.511.5Clusters Identified from Improved Program501001502002503003504004500.511.5Table 1: Empirical Parameters from the real data.

Params Value
473
n
3
K
172
n1
151
n2
150
n3

Params

r
q
p1
p2
p3

Value
0.1500
0.1929
0.7587
0.6444
0.7687

Table 2: Number of miss-classiﬁed images

Clusters→ 1
K-means
39
9
Simple
Improved
1

2
150
57
29

3 Total
4
193
74
8
4
34

larger than the rest. In Figure 4 we plot the sorted eigenvalues for the adjacency matrix A and the
recovered matrices. We can see that the top 3 eigen values are very easily distinguished from the
rest for the matrix recovered after running Program 1.4.
A sample of the data is shown in Figure 5. We observe that factors such as color  grooming  posture 
face visibility etc. can result in confusion while comparing image pairs. Also  note that the ability
of the workers to distinguish the dog breeds is neither guaranteed nor uniform. Thus  the edge
probability inside and outside clusters are not uniform. Nonetheless  Programs 1.1 and Program 1.4 
especially Program 1.4  are quite successful in clustering the data with only 15% observations.

Figure 4: Plot of sorted eigen values for (1) Adjacency matrix with unknown entries ﬁlled by 0  (2)
Recovered adjacency matrix from Program 1.1  (3) Recovered adjacency matrix from Program 1.4

Figure 5: Sample images of three breeds of dogs that were used in the MTurk experiment.

The authors thank the anonymous reviewers for their insightful comments. This work was supported
in part by the National Science Foundation under grants CCF-0729203  CNS-0932428 and CIF-
1018927  by the Ofﬁce of Naval Research under the MURI grant N00014-08-1-0747  and by a grant
from Qualcomm Inc. The ﬁrst author is also supported by the Schlumberger Foundation Faculty for
the Future Program Grant.

References
[1] A. K. Jain  M. N. Murty  and P. J. Flynn. Data clustering: A review. ACM Comput. Surv.  31(3):264–323 

September 1999.

[2] M. Ester  H.-P. Kriegel  and X. Xu. A database interface for clustering in large spatial databases.

In
Proceedings of the 1st international conference on Knowledge Discovery and Data mining (KDD’95) 
pages 94–99. AAAI Press  August 1995.

[3] Xiaowei Xu  Jochen J¨ager  and Hans-Peter Kriegel. A fast parallel clustering algorithm for large spatial

databases. Data Min. Knowl. Discov.  3(3):263–290  September 1999.

[4] Nina Mishra  Robert Schreiber  Isabelle Stanton  and Robert Tarjan. Clustering Social Networks. In An-
thony Bonato and Fan R. K. Chung  editors  Algorithms and Models for the Web-Graph  volume 4863 of
Lecture Notes in Computer Science  chapter 5  pages 56–67. Springer Berlin Heidelberg  Berlin  Heidel-
berg  2007.

8

0200400600−100102030 A0200400600−1000100200300 Simple0200400600−1000100200300 ImprovedNorfolk Terrier Toy Poodle Bouvier des Flandres [5] Pedro Domingos and Matt Richardson. Mining the network value of customers. In Proceedings of the
seventh ACM SIGKDD international conference on Knowledge discovery and data mining  KDD ’01 
pages 57–66  New York  NY  USA  2001. ACM.

[6] Santo Fortunato. Community detection in graphs. Physics Reports  486(3-5):75 – 174  2010.
[7] Ying Xu  Victor Olman  and Dong Xu. Clustering gene expression data using a graph-theoretic approach:

an application of minimum spanning trees. Bioinformatics  18(4):536–545  2002.

[8] Qiaofeng Yang and Stefano Lonardi. A parallel algorithm for clustering protein-protein interaction net-

works. In CSB Workshops  pages 174–177. IEEE Computer Society  2005.

[9] Satu Elisa Schaeffer. Graph clustering. Computer Science Review  1(1):27 – 64  2007.
[10] Paul W. Holland  Kathryn Blackmond Laskey  and Samuel Leinhardt. Stochastic blockmodels: First

steps. Social Networks  5(2):109 – 137  1983.

[11] Anne Condon and Richard M. Karp. Algorithms for graph partitioning on the planted partition model.

Random Struct. Algorithms  18(2):116–140  2001.

[12] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust pca via outlier pursuit. In John D. Lafferty 
Christopher K. I. Williams  John Shawe-Taylor  Richard S. Zemel  and Aron Culotta  editors  NIPS  pages
2496–2504. Curran Associates  Inc.  2010.

[13] Ali Jalali  Yudong Chen  Sujay Sanghavi  and Huan Xu. Clustering partially observed graphs via convex
optimization. In Lise Getoor and Tobias Scheffer  editors  Proceedings of the 28th International Confer-
ence on Machine Learning (ICML-11)  ICML ’11  pages 1001–1008  New York  NY  USA  June 2011.
ACM.

[14] Brendan P. W. Ames and Stephen A. Vavasis. Convex optimization for the planted k-disjoint-clique

problem. Math. Program.  143(1-2):299–337  2014.

[15] Brendan P. W. Ames and Stephen A. Vavasis. Nuclear norm minimization for the planted clique and

biclique problems. Math. Program.  129(1):69–89  September 2011.

[16] S. Oymak and B. Hassibi.
arXiv:1104.5186  April 2011.

Finding Dense Clusters via ”Low Rank + Sparse” Decomposition.

[17] Yudong Chen  Sujay Sanghavi  and Huan Xu. Clustering sparse graphs. In Peter L. Bartlett  Fernando
C. N. Pereira  Christopher J. C. Burges  Lon Bottou  and Kilian Q. Weinberger  editors  NIPS  pages
2213–2221  2012.

[18] Yudong Chen  Ali Jalali  Sujay Sanghavi  and Constantine Caramanis. Low-rank matrix recovery from

errors and erasures. IEEE Transactions on Information Theory  59(7):4324–4337  2013.

[19] Brendan P. W. Ames. Robust convex relaxation for the planted clique and densest k-subgraph problems.

2013.

[20] Nir Ailon  Yudong Chen  and Huan Xu. Breaking the small cluster barrier of graph clustering. CoRR 

abs/1302.4549  2013.

[21] Ramya Korlakai Vinayak  Samet Oymak  and Babak Hassibi. Sharp performance bounds for graph clus-
tering via convex optimizations. In Proceedings of the 39th International Conference on Acoustics  Speech
and Signal Processing  ICASSP ’14  2014.

[22] Emmanuel J. Candes and Justin Romberg. Quantitative robust uncertainty principles and optimally sparse

decompositions. Found. Comput. Math.  6(2):227–254  April 2006.

[23] Emmanuel J. Candes and Benjamin Recht. Exact matrix completion via convex optimization. Found.

Comput. Math.  9(6):717–772  December 2009.

[24] Emmanuel J. Cand`es  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis? J.

ACM  58(3):11:1–11:37  June 2011.

[25] Venkat Chandrasekaran  Sujay Sanghavi  Pablo A. Parrilo  and Alan S. Willsky. Rank-sparsity incoher-

ence for matrix decomposition. SIAM Journal on Optimization  21(2):572–596  2011.

[26] Venkat Chandrasekaran  Pablo A. Parrilo  and Alan S. Willsky. Rejoinder: Latent variable graphical

model selection via convex optimization. CoRR  abs/1211.0835  2012.

[27] Frank McSherry. Spectral partitioning of random graphs.

Society  2001.

In FOCS  pages 529–537. IEEE Computer

[28] Zhouchen Lin  Minming Chen  and Yi Ma. The Augmented Lagrange Multiplier Method for Exact

Recovery of Corrupted Low-Rank Matrices. Mathematical Programming  2010.

[29] Aditya Khosla  Nityananda Jayadevaprakash  Bangpeng Yao  and Li Fei-Fei. Novel dataset for ﬁne-
grained image categorization. In First Workshop on Fine-Grained Visual Categorization  IEEE Confer-
ence on Computer Vision and Pattern Recognition  Colorado Springs  CO  June 2011.

[30] Michael Buhrmester  Tracy Kwang  and Samuel D. Gosling. Amazon’s Mechanical Turk: A new source

of inexpensive  yet high-quality  data? Perspectives on Psychological Science  6(1):3–5  January 2011.

9

,Ramya Korlakai Vinayak
Samet Oymak
Babak Hassibi
Moontae Lee
David Bindel
David Mimno