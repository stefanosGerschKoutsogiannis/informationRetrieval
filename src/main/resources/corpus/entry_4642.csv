2018,Regret bounds for meta Bayesian optimization with an unknown Gaussian process prior,Bayesian optimization usually assumes that a Bayesian prior is given. However  the strong theoretical guarantees in Bayesian optimization are often regrettably compromised in practice because of unknown parameters in the prior. In this paper  we adopt a variant of empirical Bayes and show that   by estimating the Gaussian process prior from offline data sampled from the same prior and constructing unbiased estimators of the posterior  variants of both GP-UCB and \emph{probability of improvement} achieve a near-zero regret bound  which decreases to a constant proportional to the observational noise as the number of offline data and the number of online evaluations increase. Empirically  we have verified our approach on challenging simulated robotic problems featuring task and motion planning.,Regret bounds for meta Bayesian optimization

with an unknown Gaussian process prior

Zi Wang∗
MIT CSAIL

ziw@csail.mit.edu

Beomjoon Kim∗

MIT CSAIL

beomjoon@mit.edu

Leslie Pack Kaelbling

MIT CSAIL

lpk@csail.mit.edu

Abstract

Bayesian optimization usually assumes that a Bayesian prior is given. However 
the strong theoretical guarantees in Bayesian optimization are often regrettably
compromised in practice because of unknown parameters in the prior. In this paper 
we adopt a variant of empirical Bayes and show that  by estimating the Gaussian
process prior from ofﬂine data sampled from the same prior and constructing
unbiased estimators of the posterior  variants of both GP-UCB and probability
of improvement achieve a near-zero regret bound  which decreases to a constant
proportional to the observational noise as the number of ofﬂine data and the
number of online evaluations increase. Empirically  we have veriﬁed our approach
on challenging simulated robotic problems featuring task and motion planning.

1

Introduction

Bayesian optimization (BO) is a popular approach to optimizing black-box functions that are expen-
sive to evaluate. Because of expensive evaluations  BO aims to approximately locate the function
maximizer without evaluating the function too many times. This requires a good strategy to adaptively
choose where to evaluate based on the current observations.
BO adopts a Bayesian perspective and assumes that there is a prior on the function; typically  we use
a Gaussian process (GP) prior. Then  the information collection strategy can rely on the prior to focus
on good inputs  where the goodness is determined by an acquisition function derived from the GP
prior and current observations. In past literature  it has been shown both theoretically and empirically
that if the function is indeed drawn from the given prior  there are many acquisition functions that
BO can use to locate the function maximizer quickly [51  5  53].
However  in reality  the prior we choose to use in BO often does not reﬂect the distribution from
which the function is drawn. Hence  we sometimes have to estimate the hyper-parameters of a chosen
form of the prior on the ﬂy as we collect more data [50]. One popular choice is to estimate the prior
parameters using empirical Bayes with  e.g.  the maximum likelihood estimator [44] .
Despite the vast literature that shows many empirical Bayes approaches have well-founded theoretical
guarantees such as consistency [40] and admissibility [26]  it is difﬁcult to analyze a version of BO
that uses empirical Bayes because of the circular dependencies between the estimated parameters and
the data acquisition strategies. The requirement to select the prior model and estimate its parameters
leads to a BO version of the chicken-and-egg dilemma: the prior model selection depends on the data
collected and the data collection strategy depends on having a “correct” prior. Theoretically  there is
little evidence that BO with unknown parameters in the prior can work well. Empirically  there is
evidence showing it works well in some situations  but not others [33  23]  which is not surprising in
light of no free lunch results [56  22].

∗Equal contribution.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this paper  we propose a simple yet effective strategy for learning a prior in a meta-learning setting
where training data on functions from the same Gaussian process prior are available. We use a variant
of empirical Bayes that gives unbiased estimates for both the parameters in the prior and the posterior
given observations of the function we wish to optimize. We analyze the regret bounds in two settings:
(1) ﬁnite input space  and (2) compact input space in Rd. We clarify additional assumptions on the
training data and form of Gaussian processes of both settings in Sec. 4.1 and Sec. 4.2. We prove
theorems that show a near-zero regret bound for variants of GP-UCB [2  51] and probability of
improvement (PI) [29  53]. The regret bound decreases to a constant proportional to the observational
noise as online evaluations and ofﬂine data size increase.
From a more pragmatic perspective on Bayesian optimization for important areas such as robotics 
we further explore how our approach works for problems in task and motion planning domains [27] 
and we explain why the assumptions in our theorems make sense for these problems in Sec. 5. Indeed 
assuming a common kernel  such as squared exponential or Matérn  is very limiting for robotic
problems that involve discontinuity and non-stationarity. However  with our approach of setting the
prior and posterior parameters  BO outperforms all other methods in the task and motion planning
benchmark problems.
The contributions of this paper are (1) a stand-alone BO module that takes in only a multi-task training
data set as input and then actively selects inputs to efﬁciently optimize a new function and (2) analysis
of the regret of this module. The analysis is constructive  and determines appropriate hyperparameter
settings for the GP-UCB acquisition function. Thus  we make a step forward to resolving the problem
that  despite being used for hyperparameter tuning  BO algorithms themselves have hyperparameters.

2 Background and related work

BO optimizes a black-box objective function through sequential queries. We usually assume knowl-
edge of a Gaussian process [44] prior on the function  though other priors such as Bayesian neural
networks and their variants [17  30] are applicable too. Then  given possibly noisy observations
and the prior distribution  we can do Bayesian posterior inference and construct acquisition func-
tions [29  38  2] to search for the function optimizer.
However  in practice  we do not know the prior and it must be estimated. One of the most popular
methods of prior estimation in BO is to optimize mean/kernel hyper-parameters by maximizing
data-likelihood of the current observations [44  19]. Another popular approach is to put a prior on
the mean/kernel hyper-parameters and obtain a distribution of such hyper-parameters to adapt the
model given observations [20  50]. These methods require a predetermined form of the mean function
and the kernel function. In the existing literature  mean functions are usually set to be 0 or linear
and the popular kernel functions include Matérn kernels  Gaussian kernels  linear kernels [44] or
additive/product combinations of the above [11  24].
Meta BO aims to improve the optimization of a given objective function by learning from past
experiences with other similar functions. Meta BO can be viewed as a special case of transfer
learning or multi-task learning. One well-studied instance of meta BO is the machine learning (ML)
hyper-parameter tuning problem on a dataset  where  typically  the validation errors are the functions
to optimize [14]. The key question is how to transfer the knowledge from previous experiments on
other datasets to the selection of ML hyper-parameters for the current dataset.
To determine the similarity between validation error functions on different datasets  meta-features of
datasets are often used [6]. With those meta-features of datasets  one can use contextual Bayesian
optimization approaches [28] that operate with a probabilistic functional model on both the dataset
meta-features and ML hyper-parameters [3]. Feurer et al. [16]  on the other hand  used meta-features
of datasets to construct a distance metric  and to sort hyper-parameters that are known to work for
similar datasets according to their distances to the current dataset. The best k hyper-parameters are
then used to initialize a vanilla BO algorithm. If the function meta-features are not given  one can
estimate the meta-features  such as the mean and variance of all observations  using Monte Carlo
methods [52]  maximum likelihood estimates [57] or maximum a posteriori estimates [43  42].
As an alternative to using meta-features of functions  one can construct a kernel between functions.
For functions that are represented by GPs  Malkomes et al. [36] studied a “kernel kernel”  a kernel
for kernels  such that one can use BO with a “kernel kernel” to select which kernel to use to model or

2

optimize an objective function [35] in a Bayesian way. However  [36] requires an initial set of kernels
to select from. Instead  Golovin et al. [18] introduced a setting where the functions come in sequence
and the posterior of the former function becomes the prior of the current function. Removing the
assumption that functions come sequentially  Feurer et al. [15] proposed a method to learn an additive
ensemble of GPs that are known to ﬁt all of those past “training functions”.
Theoretically  it has been shown that meta BO methods that use information from similar functions
may result in an improvement for the cumulative regret bound [28  47] or the simple regret bound [42]
with the assumptions that the GP priors are given. If the form of the GP kernel is given and the prior
mean function is 0 but the kernel hyper-parameters are unknown  it is possible to obtain a regret
bound given a range of these hyper-parameters [54]. In this paper  we prove a regret bound for meta
BO where the GP prior is unknown; this means  neither the range of GP hyper-parameters nor the
form of the kernel or mean function is given.
A more ambitious approach to solving meta BO is to train an end-to-end system  such as a recurrent
neural network [21]  that takes the history of observations as an input and outputs the next point to
evaluate [8]. Though it has been demonstrated that the method in [8] can learn to trade-off exploration
and exploitation for a short horizon  it is unclear how many “training instances”  in the form of
observations of BO performed on similar functions  are necessary to learn the optimization strategies
for any given horizon of optimization. In this paper  we show both theoretically and empirically how
the number of “training instances” in our method affects the performance of BO.
Our methods are most similar to the BOX algorithm [27]  which uses evaluations of previous
functions to make point estimates of a mean and covariance matrix on the values over a discrete
domain. Our methods for the discrete setting (described in Sec. 4.1) directly improve on BOX by
choosing the exploration parameters in GP-UCB more effectively. This general strategy is extended
to the continuous-domain setting in Sec. 4.2  in which we extend a method for learning the GP
prior [41] and the use the learned prior in GP-UCB and PI.
Learning how to learn  or “meta learning”  has a long history in machine learning [46]. It was
argued that learning how to learn is “learning the prior” [4] with “point sets” [37]  a set of iid sets
of potentially non-iid points. We follow this simple intuition and present a meta BO approach that
learns its GP prior from the data collected on functions that are assumed to have been drawn from the
same prior distribution.
Empirical Bayes [45  26] is a standard methodology for estimating unknown parameters of a
Bayesian model. Our approach is a variant of empirical Bayes. We can view our computations
as the construction of a sequence of estimators for a Bayesian model. The key difference from
traditional empirical Bayes methods is that we are able to prove a regret bound for a BO method
that uses estimated parameters to construct priors and posteriors. In particular  we use frequentist
concentration bounds to analyze Bayesian procedures  which is one way to certify empirical Bayes in
statistics [49  13].

3 Problem formulation and notations

Unlike the standard BO setting  we do not assume knowledge of the mean or covariance in the GP
prior  but we do assume the availability of a dataset of iid sets of potentially non-iid observations on
functions sampled from the same GP prior. Then  given a new  unknown function sampled from that
same distribution  we would like to ﬁnd its maximizer.
More formally  we assume there exists a distribution GP (µ  k)  and both the mean µ : X → R and the
kernel k : X×X → R are unknown. Nevertheless  we are given a dataset ¯DN = {[(¯xij  ¯yij)]Mi
j=1}N
i=1 
where ¯yij is drawn independently from N (fi(¯xij)  σ2) and fi : X → R is drawn independently from
GP (µ  k). The noise level σ is unknown as well. We will specify inputs ¯xij in Sec. 4.1 and Sec. 4.2.
Given a new function f sampled from GP (µ  k)  our goal is to maximize it by sequentially querying
t=1  yt ∼ N (f (xt)  σ2). We study two evaluation
the function and constructing DT = [(xt  yt)]T
criteria: (1) the best-sample simple regret rT = maxx∈X f (x) − maxt∈[T ] f (xt) which indicates the
value of the best query in hindsight  and (2) the simple regret  RT = maxx∈X f (x) − f (ˆx∗
T ) which
measures how good the inferred maximizer ˆx∗

T is.

3

Notation We use N (u  V ) to denote a multivariate Gaussian distribution with mean u and variance
V and use W(V  n) to denote a Wishart distribution with n degrees of freedom and scale matrix
V . We also use [n] to denote [1 ···   n] ∀n ∈ Z+. We overload function notation for evaluations
on vectors x = [xi]n
i=1 
and the output matrix as k(x  x(cid:48)) = [k(xi  x(cid:48)
j)]i∈[n] j∈[n(cid:48)]  and we overload the kernel function
k(x) = k(x  x).

j=1 by denoting the output column vector as µ(x) = [µ(xi)]n

i=1  x(cid:48) = [xj]n(cid:48)

4 Meta BO and its theoretical guarantees

Instead of hand-crafting the mean µ and
kernel k  we estimate them using the train-
ing dataset ¯DN . Our approach is fairly
straightforward: in the ofﬂine phase  the
training dataset ¯DN is collected and we
obtain estimates of the mean function ˆµ
and kernel ˆk; in the online phase  we treat
GP (ˆµ  ˆk) as the Bayesian “prior” to do
Bayesian optimization. We illustrate the
two phases in Fig. 1.
In Alg. 1  we de-
pict our algorithm  assuming the dataset
¯DN has been collected. We use ES-
TIMATE( ¯DN ) to denote the “prior” esti-
mation and INFER(Dt; ˆµ  ˆk) the “poste-
rior” inference  both of which we will
introduce in Sec. 4.1 and Sec. 4.2. For
acquisition functions  we consider spe-
cial cases of probability of improvement
(PI) [53  29] and upper conﬁdence bound
(GP-UCB) [51  2]:

Algorithm 1 Meta Bayesian optimization
1: function META-BO( ¯DN   f)
2:
3:
4: end function

ˆµ(·)  ˆk(· ·) ← ESTIMATE( ¯DN )
return BO(f  ˆµ  ˆk)

D0 ← ∅
for t = 1 ···   T do

5: function BO (f  ˆµ  ˆk)
6:
7:
8:
9:
10:
11:
12:
end for
13:
return DT
14:
15: end function

ˆµt−1(·)  ˆkt−1(·) ← INFER(Dt−1; ˆµ  ˆk)
αt−1(·) ←ACQUISITION (ˆµt−1  ˆkt−1)
xt ← arg maxx∈X αt−1(x)
yt ← OBSERVE(f (xt))
Dt ← Dt−1 ∪ [(xt  yt)]

αPI
t−1(x) =

  αGP-UCB

t−1

(x) = ˆµt−1(x) + ζt

ˆkt−1(x)

1
2 .

ˆµt−1(x) − ˆf∗
ˆkt−1(x) 1

2

Here  PI assumes additional information2 in the form of the upper bound on function value ˆf∗ ≥
maxx∈X f (x). For GP-UCB  we set its hyperparameter ζt to be

(cid:16)

ζt =

6(N − 3 + t + 2

t log 6

(cid:113)

(cid:17) 1
δ )/(δN (N − t − 1))
N−t log 6

δ ) 1

2 ) 1

2

2

δ + 2 log 6
(1 − 2( 1

+ (2 log( 3

δ )) 1

2

 

where N is the size of the dataset ¯DN and δ ∈ (0  1). With probability 1 − δ  the regret bound in
Thm. 2 or Thm. 4 holds with these special cases of GP-UCB and PI. Under two different settings of
the search space X  ﬁnite X and compact X ∈ Rd  we show how our algorithm works in detail and
why it works via regret analyses on the best-sample simple regret. Finally in Sec. 4.3 we show how
the simple regret can be bounded. The proofs of the analyses can be found in the appendix.

4.1 X is a ﬁnite set

We ﬁrst study the simplest case  where the function domain X = [¯xj]M
j=1 is a ﬁnite set with cardinality
|X| = M ∈ Z+. For convenience  we treat this set as an ordered vector of items indexed by
j ∈ [M ]. We collect the training dataset ¯DN = {[(¯xj  ¯δij ¯yij)]M
j=1}N
i=1  where ¯yij are independently
drawn from N (fi(¯xj)  σ2)  fi are drawn independently from GP (µ  k) and ¯δij ∈ {0  1}. Because
the training data can be collected ofﬂine by querying the functions {fi}N
i=1 in parallel  it is not
unreasonable to assume that such a dataset ¯DN is available. If ¯δij = 0  it means the (i  j)-th entry of
the dataset ¯DN is missing  perhaps as a result of a failed experiment.

2Alternatively  an upper bound ˆf∗ can be estimated adaptively [53]. Note that here we are maximizing the PI

acquisition function and hence αPI

t−1(x) is a negative version of what was deﬁned in [53].

4

i=1

j=1

[7] 

including

(cid:80)M

tions (cid:80)N

Estimating GP param-
eters
If ¯δij < 1  we
have missing entries in
the
observation matrix
¯Y = [¯δij ¯yij]i∈[N ] j∈[M ] ∈
RN×M . Under additional
speciﬁed
assumptions
in
that
rank(Y ) = r and the total
number of valid observa-
¯δij ≥
O(rN 6
5 log N )  we can use
matrix completion [7] to
fully recover the matrix ¯Y
with high probability.
In
the following  we proceed
by considering completed
observations only.
Let the completed observation matrix be Y = [¯yij]i∈[N ] j∈[M ]. We use an unbiased sample mean and
N−1 (Y − 1N ˆµ(X)T)T(Y −
covariance estimator for µ and k; that is  ˆµ(X) = 1
1N ˆµ(X)T)  where 1N is an N by 1 vector of ones. It is well known that ˆµ and ˆk are independent and
ˆµ(X) ∼ N (µ(X)  1
Constructing estimators of the posterior Given noisy observations Dt = {(xτ   yτ )}t
do Bayesian posterior inference to obtain f ∼ GP (µt  kt). By the GP assumption  we get

Figure 1: Our approach estimates the mean function ˆµ and kernel ˆk
from functions sampled from GP (µ  k) in the ofﬂine phase. Those
sampled functions are illustrated by colored lines. In the online phase 
a new function f sampled from the same GP (µ  k) is given and we
can estimate its posterior mean function ˆµt and covariance function ˆkt
which will be used for Bayesian optimization.

N Y T1N and ˆk(X) = 1
N−1 (k(X) + σ2I)  N − 1) [1].

N (k(X) + σ2I))  ˆk(X) ∼ W( 1

τ =1  we can

µt(x) = µ(x) + k(x  xt)(k(xt) + σ2I)−1(yt − µ(xt))  ∀x ∈ X

kt(x  x(cid:48)) = k(x  x(cid:48)) − k(x  xt)(k(xt) + σ2I)−1k(xt  x(cid:48))  ∀x  x(cid:48) ∈ X 

(1)
(2)

τ =1  xt = [xτ ]T

where yt = [yτ ]T
τ =1 [44]. The problem is that neither the posterior mean µt nor
the covariance kt are computable because the Bayesian prior mean µ  the kernel k and the noise
parameter σ are all unknown. How to estimate µt and kt without knowing those prior parameters?
We introduce the following unbiased estimators for the posterior mean and covariance 

ˆµt(x) = ˆµ(x) + ˆk(x  xt)ˆk(xt  xt)

ˆkt(x  x(cid:48)) =

N − 1
N − t − 1

−1

(cid:16)ˆk(x  x(cid:48)) − ˆk(x  xt)ˆk(xt  xt)

(cid:17)
(yt − ˆµ(xt))  ∀x ∈ X 
−1ˆk(xt  x(cid:48))

  ∀x  x(cid:48) ∈ X.

(3)

(4)

Notice that unlike Eq. (1) and Eq. (2)  our estimators ˆµt and ˆkt do not depend on any unknown values
or an additional estimate of the noise parameter σ. In Lemma 1  we show that our estimators are
indeed unbiased and we derive their concentration bounds.
Lemma 1. Pick probability δ ∈ (0  1). For any nonnegative integer t < T   conditioned on
the observations Dt = {(xτ   yτ )}t
τ =1  the estimators in Eq. (3) and Eq. (4) satisfy E[ˆµt(X)] =
µt(X)  E[ˆkt(X)] = kt(X) + σ2I. Moreover  if the size of the training dataset satisﬁes N ≥ T + 2 
then for any input x ∈ X  with probability at least 1 − δ  both
|ˆµt(x) − µt(x)|2 < at(kt(x) + σ2) and 1 − 2

bt < ˆkt(x)/(kt(x) + σ2) < 1 + 2

(cid:112)

bt + 2bt

(cid:112)
(cid:17)

(cid:16)

4

√
N−2+t+2
δN (N−t−2)

t log (4/δ)+2 log (4/δ)

hold  where at =

and bt = 1

δ .
N−t−1 log 4

Regret bounds We show a near-zero upper bound on the best-sample simple regret of meta BO
with GP-UCB and PI that uses speciﬁc parameter settings in Thm. 2. In particular  for both GP-UCB
and PI  the regret bound converges to a residual whose scale depends on the noise level σ in the
observations.
Theorem 2. Assume there exists constant c ≥ maxx∈X k(x) and a training dataset is available
whose size is N ≥ 4 log 6
δ + T + 2. Then  with probability at least 1 − δ  the best-sample simple

5

regret in T iterations of meta BO with special cases of either GP-UCB or PI satisﬁes

rUCB
T < ηUCB

T

T < ηPI

T (N )λT   λ2

T = O(ρT /T ) + σ2 

(N )λT   rPI
√
1+m√
1−m

T (N ) = (m+C2)(

+1)+C3  m = O(

√
1+m√
1−m

where ηU CB
C1  C2  C3 > 0 are constants  and ρT = max

(N ) = (m+C1)(

+1)  ηPI

T

1

2 log |I + σ−2k(A)|.

A∈X |A|=T

(cid:113) 1

N−T ) 

T

T

and ηPI

(cid:113) d log(T )

This bound reﬂects how training instances N and BO iterations T affect the best-sample simple
regret. The coefﬁcients ηUCB
T both converge to constants (more details in the appendix)  with
components converging at rate O(1/(N − T ) 1
2 ). The convergence of the shared term λT depends on
ρT   the maximum information gain between function f and up to T observations yT . If  for example 
each input has dimension Rd and k(x  x(cid:48)) = xTx(cid:48)  then ρT = O(d log(T )) [51]  in which case λT
converges to the observational noise level σ at rate O(
). Together  the bounds indicate
that the best-sample simple regret of both our settings of GP-UCB and PI decreases to a constant
proportional to noise level σ.
4.2 X ⊂ Rd is compact
For compact X ⊂ Rd  we consider the primal form of GPs. We further assume that there exist basis
s=1 : X → RK  mean parameter u ∈ RK and covariance parameter Σ ∈ RK×K
functions Φ = [φs]K
such that µ(x) = Φ(x)Tu and k(x  x(cid:48)) = Φ(x)TΣΦ(x(cid:48)). Notice that Φ(x) ∈ RK is a column vector
and Φ(xt) ∈ RK×t for any xt = [xτ ]t
τ =1. This means  for any input x ∈ X  the observation satisﬁes
y ∼ N (f (x)  σ2)  where f = Φ(x)TW ∼ GP (µ  k) and the linear operator W ∼ N (u  Σ) [39]. In
the following analyses  we assume the basis functions Φ are given.
We assume that a training dataset ¯DN = {[(¯xj  ¯yij)]M
i=1 is given  where ¯xj ∈ X ⊂ Rd  yij are
independently drawn from N (fi(¯xj)  σ2)  fi are drawn independently from GP (µ  k) and M ≥ K.
Estimating GP parameters Because the basis functions Φ are given  learning the mean function
µ and the kernel k in the GP is equivalent to learning the mean parameter u and the covariance
parameter Σ that parameterize distribution of the linear operator W . Notice that ∀i ∈ [N ] 

j=1}N

¯yi = Φ( ¯x)TWi + ¯i ∼ N (Φ( ¯x)Tu  Φ( ¯x)TΣΦ( ¯x) + σ2I) 
j=1 ∈ RM   ¯x = [¯xj]M

j=1 ∈ RM×d and ¯i = [¯ij]M

where ¯yi = [¯yij]M
Φ( ¯x) ∈ RK×M has linearly independent rows  one unbiased estimator of Wi is

j=1 ∈ RM . If the matrix

ˆWi = (Φ( ¯x)T)+ ¯yi = (Φ( ¯x)Φ( ¯x)T)−1Φ( ¯x) ¯yi ∼ N (u  Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1).

i=1 ∈ RN×K. We use the estimator ˆu = 1

Let W = [ ˆWi]N
1N ˆu) to the estimate GP parameters. Again  ˆu and ˆΣ are independent and

N (Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1)(cid:1)   ˆΣ ∼ W(cid:16) 1

(cid:17)
(cid:0)Σ + σ2(Φ( ¯x)Φ( ¯x)T)−1(cid:1)   N − 1

ˆu ∼ N(cid:0)u  1

N WT1N and ˆΣ = 1

N−1 (W − 1N ˆu)T(W −

N−1

[1].

Constructing estimators of the posterior We assume the total number of evaluations T < K.
Given noisy observations Dt = {(xτ   yτ )}t
τ =1  we have µt(x) = Φ(x)Tut and kt(x  x(cid:48)) =
Φ(x)TΣtΦ(x(cid:48))  where the posterior of W ∼ N (ut  Σt) satisﬁes

ut = u + ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1(yt − Φ(xt)Tu) 
Σt = Σ − ΣΦ(xt)(Φ(xt)TΣΦ(xt) + σ2I)−1Φ(xt)TΣ.

Similar to the strategy used in Sec. 4.1  we construct an estimator for the posterior of W to be

ˆut = ˆu + ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1(yt − Φ(xt)Tu) 
ˆΣt =

(cid:16) ˆΣ − ˆΣΦ(xt)(Φ(xt)T ˆΣΦ(xt))−1Φ(xt)T ˆΣ

(cid:17)

N − 1
N − t − 1

(8)
We can compute the conditional mean and variance of the observation on x ∈ X to be
ˆµt(x) = Φ(x)T ˆut and ˆkt(x) = Φ(x)T ˆΣtΦ(x). For convenience of notation  we deﬁne ¯σ2(x) =
σ2Φ(x)T(Φ( ¯x)Φ( ¯x)T)−1Φ(x).

.

(5)
(6)

(7)

6

Lemma 3. Pick probability δ ∈ (0  1). Assume Φ( ¯x) has full row rank. For any nonnegative integer
t < T   T ≤ K  conditioned on the observations Dt = {(xτ   yτ )}t
τ =1  E[ˆµt(x)] = µt(x)  E[ˆkt(x)] =
kt(x) + ¯σ2(x). Moreover  if the size of the training dataset satisﬁes N ≥ T + 2  then for any input
x ∈ X  with probability at least 1 − δ  both
|ˆµt(x) − µt(x)|2 < at(kt(x) + ¯σ2(x)) and 1 − 2

bt < ˆkt(x)/(kt(x) + ¯σ2(x)) < 1 + 2

(cid:112)

bt + 2bt

(cid:16)

N−2+t+2

4

t log (4/δ)+2 log (4/δ)

√
δN (N−t−2)

hold  where at =

and bt = 1

δ .
N−t−1 log 4

(cid:112)
(cid:17)

T converges to ¯σ2(·) instead of σ2 in Thm. 2 and ¯σ2(·) is proportional to σ2 .

Regret bounds Similar to the ﬁnite X case  we can also show a near-zero regret bound for compact
X ∈ Rd. The following theorem clariﬁes our results. The convergence rates are the same as Thm. 2.
Note that λ2
Theorem 4. Assume all the assumptions in Thm. 2 and that Φ( ¯x) has full row rank. With probability
at least 1 − δ  the best-sample simple regret in T iterations of meta BO with either GP-UCB or PI
satisﬁes

rUCB
T < ηUCB

T

(N )λT   rPI

T < ηPI

T (N )λT   λ2

T = O(ρT /T ) + ¯σ(xτ )2 

where ηU CB
C1  C2  C3 > 0 are constants  τ = arg mint∈[T ] kt−1(xt) and ρT = max

T (N ) = (m+C2)(

(N ) = (m+C1)(

+1)  ηPI

T

+1)+C3  m = O(

A∈X |A|=T

√
1+m√
1−m

√
1+m√
1−m

(cid:113) 1
N−T ) 
2 log |I + σ−2k(A)|.

1

4.3 Bounding the simple regret by the best-sample simple regret
Once we have the observations DT = {(xt  yt)}T
t=1  we can infer where the arg max of the function
is. For all the cases in which X is discrete or compact and the acquisition function is GP-UCB or PI 
we choose the inferred arg max to be ˆx∗
T = xτ where τ = arg maxt∈[T ] yt. We show in Lemma 5
that with high probability  the difference between the simple regret RT and the best-sample simple
regret rT is proportional to the observation noise σ.
Lemma 5. With probability at least 1 − δ  RT ≤ rT + 2(2 log 1
Together with the bounds on the best-sample simple regret from Thm. 2 and Thm. 4  our result shows
that  with high probability  the simple regret decreases to a constant proportional to the noise level σ
as the number of iterations and training functions increases.

δ ) 1

2 σ.

5 Experiments

We evaluate our algorithm in four different
black-box function optimization problems  in-
volving discrete or continuous function domains.
One problem is optimizing a synthetic function
in R2  and the rest are optimizing decision vari-
ables in robotic task and motion planning prob-
lems that were used in [27]3.
At a high level  our task and motion planning
benchmarks involve computing kinematically
feasible collision-free motions for picking and
placing objects in a scene cluttered with obsta-
cles. This problem has a similar setup to exper-
imental design: the robot can “experiment” by
assigning values to decision variables including
grasps  base poses  and object placements until
it ﬁnds a feasible plan. Given the assigned val-
ues for these variables  the robot program makes

Figure 2: Two instances of a picking problem. A
problem instance is deﬁned by the arrangement and
number of obstacles  which vary randomly across
different instances. The objective is to select a
grasp that can pick the blue box  marked with a
circle  without violating kinematic and collision
constraints. [27].

3 Our code is available at https://github.com/beomjoonkim/MetaLearnBO.

7

Figure 3: Learning curves (top) and rewards vs number of iterations (bottom) for optimizing synthetic
functions sampled from a GP and two scoring functions from.

a call to a planner4 which then attempts to ﬁnd a sequence of motions that achieve these grasps and
placements. We score the variable assignment based on the results of planning  assigning a very low
score if the problem was infeasible and otherwise scoring based on plan length or obstacle clearance.
An example problem is given in Figure 2.
Planning problem instances are characterized by arrangements of obstacles in the scene and the
shape of the target object to be manipulated  and each problem instance deﬁnes a different score
function. Our objective is to optimize the score function for a new problem instance  given sets of
decision-variable and score pairs from a set of previous planning problem instances as training data.
In two robotics domains  we discretize the original function domain using samples from the past
planning experience  by extracting the values of the decision variables and their scores from successful
plans. This is inspired by the previous successful use of BO in a discretized domain [9] to efﬁciently
solve an adaptive locomotion problem.
We compare our approach  called point estimate meta Bayesian optimization (PEM-BO)  to three
baseline methods. The ﬁrst is a plain Bayesian optimization method that uses a kernel function to
represent the covariance matrix  which we call Plain. Plain optimizes its GP hyperparameters by
maximizing the data likelihood. The second is a transfer learning sequential model-based optimiza-
tion [57] method  that  like PEM-BO  uses past function evaluations  but assumes that functions
sampled from the same GP have similar response surface values. We call this method TLSM-BO.
The third is random selection  which we call Random. We present the results on the UCB acquisition
function in the paper and results on the PI acquisition function are available in the appendix.
In all domains  we use the ζt value as speciﬁed in Sec. 4. For continuous domains  we use Φ(x) =
[cos(xT β(i) + β(i)
0   we
represent the function Φ(x)T Wi with a 1-hidden-layer neural network with cosine activation function
and a linear output layer with function-speciﬁc weights Wi. We then train this network on the entire
dataset ¯DN . Then  ﬁxing Φ(x)  for each set of pairs ( ¯yi  ¯xi)  i = {1··· N}  we analytically solve
the linear regression problem yi ≈ Φ(xi)T Wi as described in Sec. 4.2.
Optimizing a continuous synthetic function In this problem  the objective is to optimize a black-
box function sampled from a GP  whose domain is R2  given a set of evaluations of different functions
from the same GP. Speciﬁcally  we consider a GP with a squared exponential kernel function. The
purpose of this problem is to show that PEM-BO  which estimates mean and covariance matrix based
on ¯DN   would perform similarly to BO methods that start with an appropriate prior. We have training
data from N = 100 functions with M = 1000 sample points each.

i=1 as our basis functions. In order to train the weights Wi  β(i)  and β(i)

0 )]K

4We use Rapidly-exploring random tree (RRT) [32] with predeﬁned random seed  but other choices are

possible.

8

020406080100120140160Number of evaluations3.43.23.02.82.62.42.2RewardsRandomPlain-UCBPEM-BO-UCBTLSM-BO-UCB051015202530Number of evaluations65432RewardsRandomPlain-UCBPEM-BO-UCBTLSM-BO-UCB020406080Number of evaluations0255075100125150RewardsRandomPlain-UCBPEM-BO-UCBTLSM-BO-UCB0.00.10.20.30.40.50.60.70.80.9Portions of N707580859095100105RewardsPEM-BO-UCBPlain-UCB0.000.010.020.030.040.050.060.070.080.09Portions of N3.43.23.02.82.62.4RewardsPEM-BO-UCBPlain-UCB0.000.010.020.030.040.050.060.070.080.09Portions of N65432RewardsPEM-BO-UCBPlain-UCBNumber of evaluationsNumber of evaluationsNumber of evaluationsProportion of training datasetProportion of training datasetProportion of training datasetRewardsRewardsRewardsRewardsRewardsRewards(a)(b)(c)(d)(e)Figure 3(a) shows the learning curve  when we have different portions of data. The x-axis represents
the percentage of the dataset used to train the basis functions  u  and W from the training dataset  and
the y-axis represents the best function value found after 10 evaluations on a new function. We can see
that even with just ten percent of the training data points  PEM-BO performs just as well as Plain 
which uses the appropriate kernel for this particular problem. Compared to PEM-BO  which can
efﬁciently use all of the dataset  we had to limit the number of training data points for TLSM-BO to
1000  because even performing inference requires O(N M ) time. This leads to its noticeably worse
performance than Plain and PEM-BO.
Figure 3(d) shows the how maxt∈[T ] yt evolves  where T ∈ [1  100]. As we can see  PEM-BO using
the UCB acquisition function performs similarly to Plain with the same acquisition function. TLSM-
BO again suffers because we had to limit the number of training data points.
Optimizing a grasp In the robot-planning problem shown in Figure 2  the robot has to choose a
grasp for picking the target object in a cluttered scene. A planning problem instance is deﬁned by the
poses of obstacles and the target objects  which changes the feasibility of a grasp across different
instances.
The reward function is the negative of the length of the picking motion if the motion is feasible  and
−k ∈ R otherwise  where −k is a suitably lower number than the lengths of possible trajectories.
We construct the discrete set of grasps by using grasps that worked in the past planning problem
instances. The original space of grasps is R58  which describes position  direction  roll  and depth of
a robot gripper with respect to the object  as used in [10]. For both Plain and TLSM-BO  we use
squared exponential kernel function on this original grasp space to represent the covariance matrix.
We note that this is a poor choice of kernel  because the grasp space includes angles  making it a
non-vector space. These methods also choose a grasp from the discrete set. We train on dataset with
N = 1800 previous problems  and let M = 162.
Figure 3(b) shows the learning curve with T = 5. The x-axis is the percentage of the dataset used
for training  ranging from one percent to ten percent. Initially  when we just use one percent of the
training data points  PEM-BO performs as poorly as TLSM-BO  which again  had only 1000 training
data points. However  PEM-BO outperforms both TLSM-BO and Plain after that. The main reason
that PEM-BO outperforms these approaches is because their prior  which is deﬁned by the squared
exponential kernel  is not suitable for this problem. PEM-BO  on the other hand  was able to avoid
this problem by estimating a distribution over values at the discrete sample points that commits only
to their joint normality  but not to any metric on the underlying space. These trends are also shown
in Figure 3(e)  where we plot maxt∈[T ] yt for T ∈ [1  100]. PEM-BO outperforms the baselines
signiﬁcantly.
Optimizing a grasp  base pose  and placement We now consider a more difﬁcult task that involves
both picking and placing objects in a cluttered scene. A planning problem instance is deﬁned by
the poses of obstacles and the poses and shapes of the target object to be pick and placed. The
reward function is again the negative of the length of the picking motion if the motion is feasible 
and −k ∈ R otherwise. For both Plain and TLSM-BO  we use three different squared exponential
kernels on the original spaces of grasp  base pose  and object placement pose respectively and then
add them together to deﬁne the kernel for the whole set. For this domain  N = 1500  and M = 1000.
Figure 3(c) shows the learning curve  when T = 5. The x-axis is the percentage of the dataset used
for training  ranging from one percent to ten percent. Initially  when we just use one percent of
the training data points  PEM-BO does not perform well. Similar to the previous domain  it then
signiﬁcantly outperforms both TLSM-BO and Plain after increasing the training data. This is also
reﬂected in Figure 3(f)  where we plot maxt∈[T ] yt for T ∈ [1  100]. PEM-BO outperforms baselines.
Notice that Plain and TLSM-BO perform worse than Random  as a result of making inappropriate
assumptions on the form of the kernel.

6 Conclusion

We proposed a new framework for meta BO that estimates its Gaussian process prior based on
past experience with functions sampled from the same prior. We established regret bounds for our
approach without the reliance on a known prior and showed its good performance on task and motion
planning benchmark problems.

9

Acknowledgments

We would like to thank Stefanie Jegelka  Tamara Broderick  Trevor Campbell  Tomás Lozano-
Pérez for discussions and comments. We would like to thank Sungkyu Jung and Brian Axelrod for
discussions on Wishart distributions. We gratefully acknowledge support from NSF grants 1420316 
1523767 and 1723381  from AFOSR grant FA9550-17-1-0165  from Honda Research and Draper
Laboratory. Any opinions  ﬁndings  and conclusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the views of our sponsors.

References
[1] Theodore Wilbur Anderson. An Introduction to Multivariate Statistical Analysis. Wiley New

York  1958.

[2] Peter Auer. Using conﬁdence bounds for exploitation-exploration tradeoffs. JMLR  3:397–422 

2002.

[3] Rémi Bardenet  Mátyás Brendel  Balázs Kégl  and Michele Sebag. Collaborative hyperparameter

tuning. In ICML  2013.

[4] J Baxter. A Bayesian/information theoretic model of bias learning. In COLT  New York  New

York  USA  1996.

[5] Ilija Bogunovic  Jonathan Scarlett  Andreas Krause  and Volkan Cevher. Truncated variance
reduction: A uniﬁed approach to bayesian optimization and level-set estimation. In NIPS  2016.

[6] Pavel Brazdil  Jo¯ao Gama  and Bob Henery. Characterizing the applicability of classiﬁcation

algorithms using meta-level learning. In ECML  1994.

[7] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics  9(6):717  2009.

[8] Yutian Chen  Matthew W Hoffman  Sergio Gómez Colmenarejo  Misha Denil  Timothy P
Lillicrap  Matt Botvinick  and Nando de Freitas. Learning to learn without gradient descent by
gradient descent. In ICML  2017.

[9] A. Cully  J. Clune  D. Tarapore  and J. Mouret. Robots that adapt like animals. Nature  2015.

[10] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis  CMU

Robotics Institute  August 2010.

[11] David K Duvenaud  Hannes Nickisch  and Carl E Rasmussen. Additive Gaussian processes. In

NIPS  2011.

[12] M. L. Eaton. Multivariate Statistics: A Vector Space Approach. Beachwood  Ohio  USA:

Institute of Mathematical Statistics  2007.

[13] Bradley Efron. Bayes  oracle Bayes  and empirical Bayes. 2017.

[14] Matthias Feurer  Aaron Klein  Katharina Eggensperger  Jost Springenberg  Manuel Blum  and

Frank Hutter. Efﬁcient and robust automated machine learning. In NIPS  2015.

[15] Matthias Feurer  Benjamin Letham  and Eytan Bakshy. Scalable meta-learning for Bayesian

optimization. arXiv preprint arXiv:1802.02219  2018.

[16] Matthias Feurer  Jost Springenberg  and Frank Hutter. Initializing Bayesian hyperparameter

optimization via meta-learning. In AAAI  2015.

[17] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model

uncertainty in deep learning. In ICML  2016.

[18] Daniel Golovin  Benjamin Solnik  Subhodeep Moitra  Greg Kochanski  John Elliot Karro  and

D. Sculley. Google vizier: A service for black-box optimization. In KDD  2017.

10

[19] Philipp Hennig and Christian J Schuler. Entropy search for information-efﬁcient global opti-

mization. JMLR  13:1809–1837  2012.

[20] José Miguel Hernández-Lobato  Matthew W Hoffman  and Zoubin Ghahramani. Predictive

entropy search for efﬁcient global optimization of black-box functions. In NIPS  2014.

[21] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation 

9(8):1735–1780  1997.

[22] Christian Igel and Marc Toussaint. A no-free-lunch theorem for non-uniform distributions of

target functions. Journal of Mathematical Modelling and Algorithms  3(4):313–322  2005.

[23] Kirthevasan Kandasamy  Willie Neiswanger  Jeff Schneider  Barnabas Poczos  and Eric Xing.
Neural architecture search with Bayesian optimisation and optimal transport. arXiv preprint
arXiv:1802.07191  2018.

[24] Kirthevasan Kandasamy  Jeff Schneider  and Barnabas Poczos. High dimensional Bayesian

optimisation and bandits via additive models. In ICML  2015.

[25] Kenji Kawaguchi  Bo Xie  Vikas Verma  and Le Song. Deep semi-random features for nonlinear

function approximation. In AAAI  2017.

[26] Robert W Keener. Theoretical Statistics: Topics for a Core Course. Springer  2011.

[27] Beomjoon Kim  Leslie Pack Kaelbling  and Tomás Lozano-Pérez. Learning to guide task and

motion planning using score-space representation. In ICRA  2017.

[28] Andreas Krause and Cheng S Ong. Contextual Gaussian process bandit optimization. In NIPS 

2011.

[29] Harold J Kushner. A new method of locating the maximum point of an arbitrary multipeak

curve in the presence of noise. Journal of Fluids Engineering  86(1):97–106  1964.

[30] Balaji Lakshminarayanan  Alexander Pritzel  and Charles Blundell. Simple and scalable

predictive uncertainty estimation using deep ensembles. In NIPS  2017.

[31] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model

selection. Annals of Statistics  pages 1302–1338  2000.

[32] Steven M LaValle and James J Kuffner Jr. Rapidly-exploring random trees: Progress and

prospects. In Workshop on the Algorithmic Foundations of Robotics (WAFR)  2000.

[33] Lisha Li  Kevin Jamieson  Giulia DeSalvo  Afshin Rostamizadeh  and Ameet Talwalkar. Hy-
perband: A novel bandit-based approach to hyperparameter optimization. In International
Conference on Learning Representations (ICLR)  2016.

[34] Karim Lounici et al. High-dimensional covariance matrix estimation with missing observations.

Bernoulli  20(3):1029–1058  2014.

[35] Gustavo Malkomes and Roman Garnett. Towards automated Bayesian optimization. In ICML

AutoML Workshop  2017.

[36] Gustavo Malkomes  Charles Schaff  and Roman Garnett. Bayesian optimization for automated

model selection. In NIPS  2016.

[37] T P Minka and R W Picard. Learning how to learn is learning with point sets. Technical report 

MIT Media Lab  1997.

[38] J. Mo˘ckus. On Bayesian methods for seeking the extremum. In Optimization Techniques IFIP

Technical Conference  1974.

[39] R.M. Neal. Bayesian Learning for Neural Networks. Lecture Notes in Statistics 118. Springer 

1996.

[40] Sonia Petrone  Judith Rousseau  and Catia Scricciolo. Bayes and empirical Bayes: do they

merge? Biometrika  101(2):285–302  2014.

11

[41] John C Platt  Christopher JC Burges  Steven Swenson  Christopher Weare  and Alice Zheng.
Learning a Gaussian process prior for automatically generating music playlists. In NIPS  2002.

[42] Matthias Poloczek  Jialei Wang  and Peter Frazier. Multi-information source optimization. In

NIPS  2017.

[43] Matthias Poloczek  Jialei Wang  and Peter I Frazier. Warm starting Bayesian optimization. In

Winter Simulation Conference (WSC). IEEE  2016.

[44] Carl Edward Rasmussen and Christopher KI Williams. Gaussian processes for machine learning.

The MIT Press  2006.

[45] Herbert Robbins. An empirical Bayes approach to statistics. In Third Berkeley Symp. Math.

Statist. Probab.  1956.

[46] J Schmidhuber. On learning how to learn learning strategies. Technical report  FKI-198-94

(revised)  1995.

[47] Alistair Shilton  Sunil Gupta  Santu Rana  and Svetha Venkatesh. Regret bounds for transfer

learning in Bayesian optimisation. In AISTATS  2017.

[48] Mlnoru Slotani. Tolerance regions for a multivariate normal population. Annals of the Institute

of Statistical Mathematics  16(1):135–153  1964.

[49] Suzanne Sniekers  Aad van der Vaart  et al. Adaptive Bayesian credible sets in regression with

a Gaussian process prior. Electronic Journal of Statistics  9(2):2475–2527  2015.

[50] Jasper Snoek  Hugo Larochelle  and Ryan P Adams. Practical Bayesian optimization of machine

learning algorithms. In NIPS  2012.

[51] Niranjan Srinivas  Andreas Krause  Sham M Kakade  and Matthias Seeger. Gaussian process

optimization in the bandit setting: No regret and experimental design. In ICML  2010.

[52] Kevin Swersky  Jasper Snoek  and Ryan P Adams. Multi-task Bayesian optimization. In NIPS 

2013.

[53] Zi Wang and Stefanie Jegelka. Max-value entropy search for efﬁcient Bayesian optimization.

In ICML  2017.

[54] Ziyu Wang and Nando de Freitas. Theoretical analysis of Bayesian optimisation with unknown

Gaussian process hyper-parameters. In NIPS workshop on Bayesian Optimization  2014.

[55] Eric W. Weisstein. Square root inequality. MathWorld–A Wolfram Web Resource. http:

//mathworld.wolfram.com/SquareRootInequality.html  1999-2018.

[56] David H Wolpert and William G Macready. No free lunch theorems for optimization. IEEE

transactions on evolutionary computation  1(1):67–82  1997.

[57] Dani Yogatama and Gideon Mann. Efﬁcient transfer learning method for automatic hyperpa-

rameter tuning. In AISTATS  2014.

12

,Zi Wang
Beomjoon Kim
Leslie Kaelbling