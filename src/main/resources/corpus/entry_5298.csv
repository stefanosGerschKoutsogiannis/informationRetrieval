2018,Structure-Aware Convolutional Neural Networks,Convolutional neural networks (CNNs) are inherently subject to invariable filters that can only aggregate local inputs with the same topological structures. It causes that CNNs are allowed to manage data with Euclidean or grid-like structures (e.g.  images)  not ones with non-Euclidean or graph structures (e.g.  traffic networks). To broaden the reach of CNNs  we develop structure-aware convolution to eliminate the invariance  yielding a unified mechanism of dealing with both Euclidean and non-Euclidean structured data. Technically  filters in the structure-aware convolution are generalized to univariate functions  which are capable of aggregating local inputs with diverse topological structures. Since infinite parameters are required to determine a univariate function  we parameterize these filters with numbered learnable parameters in the context of the function approximation theory. By replacing the classical convolution in CNNs with the structure-aware convolution  Structure-Aware Convolutional Neural Networks (SACNNs) are readily established. Extensive experiments on eleven datasets strongly evidence that SACNNs outperform current models on various machine learning tasks  including image classification and clustering  text categorization  skeleton-based action recognition  molecular activity detection  and taxi flow prediction.,Structure-Aware Convolutional Neural Networks

Jianlong Chang1 2

Jie Gu1 2

Shiming Xiang1 2

1NLPR  Institute of Automation  Chinese Academy of Sciences

2School of Artiﬁcial Intelligence  University of Chinese Academy of Sciences

{jianlong.chang  jie.gu  lfwang  gfmeng  smxiang  chpan}@nlpr.ia.ac.cn

Lingfeng Wang1

Chunhong Pan1

Gaofeng Meng1

Abstract

Convolutional neural networks (CNNs) are inherently subject to invariable ﬁlters
that can only aggregate local inputs with the same topological structures. It causes
that CNNs are allowed to manage data with Euclidean or grid-like structures (e.g. 
images)  not ones with non-Euclidean or graph structures (e.g.  trafﬁc networks). To
broaden the reach of CNNs  we develop structure-aware convolution to eliminate
the invariance  yielding a uniﬁed mechanism of dealing with both Euclidean and
non-Euclidean structured data. Technically  ﬁlters in the structure-aware convolu-
tion are generalized to univariate functions  which are capable of aggregating local
inputs with diverse topological structures. Since inﬁnite parameters are required
to determine a univariate function  we parameterize these ﬁlters with numbered
learnable parameters in the context of the function approximation theory. By re-
placing the classical convolution in CNNs with the structure-aware convolution 
Structure-Aware Convolutional Neural Networks (SACNNs) are readily estab-
lished. Extensive experiments on eleven datasets strongly evidence that SACNNs
outperform current models on various machine learning tasks  including image
classiﬁcation and clustering  text categorization  skeleton-based action recognition 
molecular activity detection  and taxi ﬂow prediction.

1

Introduction

Convolutional neural networks (CNNs) provide an effective and efﬁcient framework to deal with
Euclidean structured data  including speeches and images. As a core module in CNNs  the convolution
unit explicitly allows to share parameters among the whole spatial domains to extremely reduce the
number of parameters  without sacriﬁcing the expressive capability of networks [3]. Beneﬁting from
such artful modeling  signiﬁcant successes have been achieved in a multitude of ﬁelds  including the
image classiﬁcation [15  24] and clustering [5  6]  the object detection [9  32]  and amongst others.
Although the achievements in the literature are brilliant  CNNs are still incompetent to handle non-
Euclidean structured data  such as the trafﬁc ﬂow data on trafﬁc networks  the relational data on
social networks  and the active data on molecule structure networks. The major limitation originates
from that the classical ﬁlters are invariant at each location. As a result  the ﬁlters can only be applied
to aggregate local inputs with the same topological structures  not with diverse topological structures.
In order to eliminate the limitation  we develop structure-aware convolution in which a single share-
able ﬁlter sufﬁces to aggregate local inputs with diverse topological structures. For this purpose  we
generalize the classical ﬁlters to univariate functions that can be effectively and efﬁciently parameter-
ized under the guidance of the function approximation theory. Then  we introduce local structure
representations to quantiﬁcationally encode topological structures. By modeling these representations
into the generalized ﬁlters  the corresponding local inputs can be aggregated based on the generalized
ﬁlters consequently. In practice  Structure-Aware Convolutional Neural Networks (SACNNs) can
be readily established by replacing the classical convolution in CNNs with our structure-aware

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

convolution. Since all the operations in our structure-aware convolution are differentiable  SACNNs
can be trained end-to-end by the standard back-propagation.
To sum up  the key contributions of this paper are:

capability of capturing the latent structures of data in a purely data-driven way.

• The structure-aware convolution is developed to establish SACNNs to uniformly deal with
both Euclidean and non-Euclidean structured data  which broadens the reach of convolution.
• We introduce the learnable local structure representations  which endow SACNNs with the
• By taking advantage of the function approximation theory  SACNNs can be effectively and
• Extensive experiments demonstrate that SACNNs are superior to current models in various

efﬁciently trained with the standard back-propagation to guarantee the practicability.

machine learning tasks  including classiﬁcation  clustering  and regression.

2 Related work

2.1 Convolutional neural networks (CNNs)

To elevate the performance of CNNs  much research has been devoted to designing the convolution
units  which can be roughly divided into two classes  i.e.  handcrafted and learnable ones.
Handcrafted convolution units generally derive from the professional knowledge. Primary convolution
units [24  26] present large sizes  e.g.  7 × 7 pixels in images. To increase the nonlinearity  stacking
multiple small ﬁlters (e.g.  3 × 3 pixels) instead of using a single large ﬁlter has become a common
design in CNNs [38]. To obtain larger receptive ﬁelds  the dilated convolution [41]  whose receptive
ﬁeld size grows exponentially while the number of parameters grows linearly  is proposed. In addition 
the separable convolution [7] promotes performance by integrating various ﬁlters with diverse sizes.
Among the latter  lots of efforts have been widely made to learn convolution units. By introducing
additional parameters named offsets  the active convolution [19] is explored to learn the shape of
convolution. To achieve dynamic offsets that vary with inputs  the deformable convolution [9] is
proposed. Contrary to such modiﬁcations  some approaches have been devoted to directly capturing
structures of data to improve the performance of CNNs  such as the spatial transform networks [18].
While these models have been successful on Euclidean domains  they can hardly be applied to
non-Euclidean domains. In contrast  our SACNNs can be utilized on these two domains uniformly.

2.2 Graph convolutional neural networks (GCNNs)

Recently  there has been a growing interest in applying CNNs to non-Euclidean domains [3  29  31 
35]. Generally  existing methods can be summarized into two types  i.e.  spectral and spatial methods.
Spectral methods explore an analogical convolution operator over non-Euclidean domains on the basis
of the spectral graph theory [4  16  27]. Relying on the eigenvectors of graph Laplacian  data with non-
Euclidean structures can be ﬁltered on the corresponding spectral domain. To enhance the efﬁciency
and acquire spectrum-free methods without performing eigen-decomposition  polynomial-based
networks are developed to execute convolution on non-Euclidean domains efﬁciently [10  22].
Contrary to the spectral methods  spatial methods always analogize the convolutional strategy based
on the local spatial ﬁltering [1  2  30  31  37  40]. The major difference between these methods lies in
the intrinsic coordinate systems used for encoding local patches. Typically  the diffusion CNNs [1]
encode local patches based on the random walk process on graphs  the anisotropic CNNs [2] employ
an anisotropic patch-extraction method  and the geodesic CNNs [30] represent local patches in polar
coordinates. In the mixture-model CNNs [31]  synthetically  learnable local pseudo-coordinates are
developed to parameterize local patches in a general way. Additionally  a series of spatial methods
without the classical convolutional strategy have also been explored  including the message passing
neural networks [12  28  34]  and the graph attention networks [39].
In spite of considerable achievements  both spectral and spatial methods partially rely on ﬁxed
structures (i.e.  ﬁxed relationship matrix) in graphs. Beneﬁting from the proposed structure-aware
convolution  by comparison  the structures can be learned from data automatically in our SACNNs.

2

3 Structure-aware convolution

Convolution  intrinsically  is an aggregation operation between local inputs and ﬁlters. In practice 
local inputs involve not only their input values but also topological structures. Accordingly  ﬁlters
should be in a position to aggregate local inputs with diverse topological structures. To this end  we
develop the structure-aware convolution by generalizing the ﬁlters in the classical convolution and
modeling the local structure information into the generalized ﬁlters.
The ﬁlters in the classical convolution can be smoothly generalized to univariate functions. Without
loss of generality and for simplicity  we elaborate such generalization with 1-Dimensional data. Given
an input x ∈ Rn and a ﬁlter w ∈ R2m−1  the output at the i-th vertex (location) is
i ∈ {1  2 ···   n} 

(1)
where xi = [xi−m+1 ···   xi+m−1]T is the local input at the i-th vertex  i−m < j < i+m indicates
that the j-th vertex is a neighbor of the i-th vertex  wj−i+m and xj signify the (j − i + m)-th and
j-th elements in w and x  respectively. For any univariate function f (·)  Eq. (1) can be equivalently
rewritten as follows when f (j − i + m) = wj−i+m is always satisﬁed  i.e. 

wj−i+m · xj 

¯yi = wTxi =

i−m<j<i+m

(cid:88)

(cid:88)

¯yi = f TRxi =

f (j − i + m) · xj 

i ∈ {1  2 ···   n} 

i−m<j<i+m

(2)
where f (·) is called a functional ﬁlter  R = {j − i + m | i− m < j < i + m} = {1  2 ···   2m− 1} 
and fR = {f (r)|r ∈ R}. Actually  R encodes relationships between a vertex and its neighbors. For
example  r ∈ R means that the (i− m + r)-th vertex is the r-th neighbor of the i-th vertex. Since the
relationships in R can reﬂect the structure information around a vertex  we call R a local structure
representation. Generally  the local structure representation R is constant in the classical convolution 
which causes that the same fR is shared at each vertex. As a result  the classical convolution solely
pertains to manage data with the same local topological structures  not with diverse ones.
To handle this limitation  we introduce general local structure representations to quantiﬁcationally
encode any local topological structure  and then develop structure-aware convolution by replacing the
constant R in classical convolution with the introduced general ones. Technically  both Euclidean
and non-Euclidean structured data can be represented by a graph G = (V E  R)  where the vertices
in V store the values of data  the edges in E indicate whether two vertices are connected  and the
relationship matrix R signiﬁes the structure information in the graph G. For a vertex i ∈ V  the local
structure representation at i is encoded via the relationships with its neighbors  i.e. 
(3)
where eji ∈ E means that the j-th vertex is a neighbor of the i-th vertex  rji is the element of R at (j  i)
and indicates the relationship from the j-th vertex to the i-th vertex. Note that S = {Ri|i ∈ V} can
include the whole structure information in the graph G by integrating the local structure representations
together. This implies that Eq. (3) is a reasonable formulation for local topological structures. Based
on the introduced local structure representations  the structure-aware convolution is developed by
modeling these representations into the generalized functional ﬁlters. Formally  given an input x
embedded on the graph G and a functional ﬁlter f (·)  we deﬁne the structure-aware convolution as
(4)
where fRi = {f (rji)|eji ∈ E} varies with Ri. Beneﬁting from this modiﬁcation  the structure-aware
convolution is capable of aggregating local inputs with diverse topological structures.

Ri = {rji|eji ∈ E} 

i ∈ {1  2 ···   n} 

i ∈ {1  2 ···   n} 

f (rji) · xj 

(cid:88)

¯yi = f TRi

xi =

eji∈E

4 Structure-aware convolutional neural networks

Replacing the classical convolution in CNNs with the structure-aware convolution  SACNNs are
established. Intuitively  a structure-aware convolutional layer is illustrated in Figure 1. However 
two essential problems need to be tackled before training SACNNs. First  functional ﬁlters in the
structure-aware convolution are univariate functions  which need inﬁnite parameters to be determined.
This implies that SACNNs can not be learned in a common way  and an effective and efﬁcient strategy
is required to learn these ﬁlters with numbered parameters. Second  local structure representations
(or the relationship matrix R) may be hardly deﬁned in advance and thus a learning mechanism is
needed. In the following  Section 4.1 and 4.2 focus on tackling these two problems  respectively.

3

Figure 1: A structure-aware convolutional layer. For clarity of exposition  the input x has c = 2
channels with n = 6 vertices  the output y has a single channel  and ¯xj  ¯xi ∈ Rc indicate the j-th
and i-th rows of the input x  respectively. For each vertex i  its local structure representation is ﬁrst
captured from the input and represented as Ri  which is identically shared for each channel of the
input x. Afterwards  the local inputs in the ﬁrst and second channels are aggregated via the ﬁrst ﬁlter
f1(·) and the second ﬁlter f2(·) respectively  with the same Ri. Note that f1(·) and f2(·) are shared
for every location in the ﬁrst and second channels  respectively.
4.1 Polynomial parametrization for functional ﬁlters

We parameterize the developed functional ﬁlters with numbered learnable parameters under the
(cid:80)t
guidance of the function approximation theory. In mathematics  for an arbitrary univariate function
h(x)  it can be composed of a group basis functions {h1(x)  h2(x) ···} with a set of coefﬁcients
k=1 vk · hk(x)  where hk(x) and vk are the k-th basis function
{v1  v2 ···}  denoted by h(x) (cid:39)
and the corresponding coefﬁcient  respectively. The equation is satisﬁed when t tends to inﬁnity.
Because of the high efﬁciency [14]  our functional ﬁlters are parameterized based on the Chebyshev
polynomials that form an orthogonal basis for L2([−1  1]  dy/
1 − y2)  the Hilbert space of square
integrable functions with respect to the measure dy/
1 − y2. Formally  the Chebyshev polynomial
hk(x) of order k−1 (k ≥ 3) can be generated by the stable recurrence relation hk(x) = 2xhk−1(x)−
(cid:33)
hk−2(x)  with h1(x) = 1 and h2(x) = x. In practice  the truncated expansion of Chebyshev
polynomials is employed to approximate the functional ﬁlter f (·) in Eq. (4)  i.e. 

(cid:112)

(cid:112)

yi =

f (rji) · xj =

vk · hk(rji)

· xj 

i ∈ {1  2 ···   n} 

(5)

(cid:88)

eji∈E

(cid:32) t(cid:88)

(cid:88)

eji∈E

k=1

where t is the number of the truncated polynomials  and {v1 ···   vt} are t learnable coefﬁcients cor-
responding to the polynomials {h1(x) ···   ht(x)}. Note that f (rji) can be cumulatively computed
based on the recurrence relation  leading to an efﬁcient computing strategy.

4.2 Local structure representations learning

Ri = {rji = T (¯xT

To eliminate the feature engineering  we consider to learn local structure representations from data
rather than using predeﬁned ones. To preserve the structure consistency between channels  for every
structure-aware convolutional layer  only a single local structure representation set S = {Ri|i ∈ V}
is identically learned for each channel of the input. Formally  given a multi-channel input feature
map x ∈ Rn×c  where n and c denote the numbers of vertices and channels respectively  the local
structure representation at each vertex is learned as
(6)
where ¯xj  ¯xi ∈ Rc indicate the j-th and i-th rows of the input x respectively  M ∈ Rc×c is a matrix
with c × c learnable parameters to measure relationships between local vertices  and T (·) is the Tanh
function to normalize elements in local structure representations into [−1  1] strictly.
This local structure learning formulation has two good properties. First  M is identically shared for
each channel of the input  so every channel possesses the same structure in each structure-aware
convolutional layer and the size of M only depends on the number of channels. As a result  only a
few additional parameters are required to be learned  which can alleviate the overﬁtting when training
data is limited. Second  M is not constrained as a symmetric matrix  namely rji may not be equal to
rij. This implies that our approach is capable of modeling not only undirected structures  but also
direct structures  such as the trafﬁc networks and the social networks.

j M¯xi) | eji ∈ E} 

i ∈ {1  2 ···   n} 

4

x22(cid:21)(cid:24)(cid:23)(cid:25)(cid:20)(cid:22)x12x32x42x52x62(cid:21)(cid:24)(cid:23)(cid:25)(cid:20)(cid:22)y2y1y3y4y5y6x22(cid:21)(cid:23)(cid:20)(cid:22)x12(cid:24)(cid:23)(cid:25)(cid:22)x32x4252x62(cid:24)x52(cid:22)(cid:21)(cid:20)r11r31r21local structuresfunctional filtersr11r21r31f1r11r21r31f2(cid:258)(cid:258)(cid:3)(cid:258)(cid:258)(cid:3)c(cid:23)(cid:25)r66r46r46r66f1r46r66f2(cid:21)(cid:24)(cid:23)(cid:25)(cid:20)(cid:22)x21x11x31x41x51x61outputinput4.3 Understanding the structure-aware convolution

In this subsection  we give the following theorem to reveal the essence of our structure-aware
convolution (the proof is reported in the supplementary material).
Theorem 1. Under the Chebyshev polynomial basis  the structure-aware convolution is equivalent to

yi = vTPixi 

i ∈ {1  2 ···   n} 

where v ∈ Rt is the coefﬁcients of the polynomials  Pi ∈ Rt×m is a matrix determined by the local
structure representation Ri and the polynomials  and xi ∈ Rm is the local input at the i-th vertex.
Theorem 1 indicates that the structure-aware convolution can be split into two independent units  i.e. 
a transformation Pi ∈ Rt×m and a vector v ∈ Rt. In the ﬁrst unit  the transformation Pi devotes
to encoding the m-Dimensional local inputs as t-Dimensional vectors. Since the basis functions
are ﬁxed in the Chebyshev polynomial basis  Pi is purely depended on the corresponding local
structure representation Ri that is varied with the vertex i and can be learned according to Eq. (6).
It is worth noting that this transformation Pi is similar to a speciﬁc local spatial transformer in the
spatial transform networks [18]. In the second unit  the learnable vector v is shared by every vertex
to aggregate these encoded local inputs  which is akin to the classical convolution. By integrating
these two learnable units together  the structure-aware convolution can simultaneously focus on local
input values and local topological structures to capture high-level representations.

5 Experiments

In this section  we systematically carry out extensive experiments to verify the capability of SACNNs.
Due to the space restriction we report some experimental details in the supplement  such as the
gradients during training  descriptions of datasets  descriptions of datasets  and network architectures.
Speciﬁcally  Our core code will be released at https://github.com/vector-1127/SACNNs.

5.1 Experimental settings

We perform experiments on six Euclidean and ﬁve non-Euclidean structured datasets to verify the
capability of SACNNs. Six Euclidean structured datasets include the Mnist [26]  Cifar-10 [23] 
Cifar-100 [23]  STL-10 [8]  Image10 [6]  and ImageDog [6] image datasets. Five non-Euclidean
structured datasets contain the text categorization datasets 20NEWS and Reuters [25]  the action
recognition dataset NTU [36]  the molecular activity dataset DPP4 [20]  and the taxi ﬂow dataset
TF-198 [42] that consists of the taxis ﬂow data at 198 trafﬁc intersections in a city.
With respect to the scope of applications  two types of methods are compared  i.e.  CNNs and GCNNs.
On Euclidean domains  popular CNN models  including the classical convolution (ClaCNNs) [26] 
the separable convolution (SepCNNs) [7]  the active convolution (ActCNNs) [19]  and the deformable
convolution (DefCNNs) [9] are utilized for comparisons. On non-Euclidean domains  both spatial
and spectral GCNNs are taken as competitors to SACNNs  including the local connected networks
(LCNs) [4]  the dynamic ﬁlters based networks (DFNs) [40]  the edge-conditioned convolution
(ECC) [37]  the mixture-model networks (MoNets) [31] (which is a generalization of the diffusion
CNNs [1]  the anisotropic CNNs [2]  and the geodesic CNNs [30])  the spectral networks (SCNs) [16] 
the Chebyshev based SCNs (ChebNets) [10]  and the graph convolution networks (GCNs) [22].
Furthermore  SACNNs† that omit the structure learning in SACNNs are used as a baseline of our
method and to show the effectiveness of structure learning. In SACNNs†  Ri is assigned by uniformly
sampling on [−1  1]  e.g.  Ri = {− 1
2} is predeﬁned when a 3-Dimensional ﬁlter is required.
The hyper-parameters in SACNNs are set as follows. In our experiments  the max pooling and the
Graclus method [11] are employed as the pooling operations to coarsen the feature maps in SACNNs
when managing Euclidean and non-Euclidean structured data respectively  the ReLU function [13]
is used as the activation function  batch normalization [17] is employed to normalize the inputs of
all layers  parameters are randomly initialized with a uniform distribution U (−0.1  0.1)  the order
of polynomials t is set to the maximum number of neighbors among the whole spatial domains
(e.g.  t = 9 if we attempt to learn 3 × 3 ﬁlters in images). During the training stage  the Adam
optimizer [21] with the initial learning rate 0.001 is utilized to train SACNNs  the mini-batch size is
set to 32  the categorical cross entropy loss is used in the classiﬁcation tasks  and the mean squared

2   0  1

5

Table 1: The classiﬁcation or clustering accuracies on the experimental Euclidean structured datasets.
For clarity  ‡ indicates that DAC [6] is used to cluster the whole samples in each experimental dataset.

Datasets
Mnist
ClaCNNs [26] 0.9953
SepCNNs [7]
0.9910
ActCNNs [19] 0.9926
0.9908
DefCNNs [9]
SACNNs†
0.9957
0.9961
SACNNs

Cifar-10 Cifar-100
0.6629
0.9075
0.9062
0.6643
0.6648
0.9086
0.6349
0.8718
0.6759
0.9091
0.9167
0.6938

STL-10
0.6635
0.6685
0.6761
0.6564
0.7175
0.7358

Image10‡ ImageDog‡ Time (s)
53±1
0.5272
68±1
0.5637
0.5478
83±2
0.4853
125±3
0.5953
78±2
0.6007
136±2

0.2748
0.2754
0.2786
0.2355
0.2801
0.2913

Figure 2: Invariance properties of various CNNs. (a) Gaussion noises with mean 0 and variance δ.
(b) Rotation. (c) Shift. (d) Scale. (e) Normalized total variations at the initial stage. (f) Normalized
total variations at the ﬁnal stage. Large ﬁgures can be found in the supplementary material.

error loss is used in the regression tasks. During the testing stage  the squared correlation and the
root mean square error are used to evaluate the results on DPP4 and TF-198 respectively  and the
classiﬁcation or clustering accuracy is used for the others. For a reasonable evaluation  we perform 5
random restarts and the average results are used for comparisons.

5.2 Compared with various CNNs on Euclidean domains

To validate the capability of SACNNs on the Euclidean domains  several SACNNs are modeled
to classify images in Mnist  Cifar-10  Cifar-100 and STL-10  and to cluster images in Image10
and ImageDog based on the DAC model [6]. In this experiment  images are recast as speciﬁc
multi-channel graphs on 2-Dimensional regular grids. In the graphs  each vertex is provided with 9
neighbors including itself  which is similar to the classical convolution with a 3 × 3 ﬁlter.
In Table 1  we report the quantitative results of the modeled networks with diverse convolution units
on various Euclidean structured datasets. Note that SACNNs achieve the superior performance on
both classiﬁcation and clustering tasks  which implies that SACNNs and SACNNs† are capable of
managing Euclidean structured data effectively. In Figure 2  we empirically verify the invariance
property of the compared CNNs on the Mnist dataset. In this experiment  we disturb the testing
data in Mnist with four typical transformations  including Gaussion noise  rotation  shift  and scale.
Then  these disturbed data is utilized to validate the trained networks with the evaluated convolution
units. From Figure 2  the results assuredly prove that SACNNs and SACNNs† are in possession of
excellent robustness to such transformations. Furthermore  we analyze the learned ﬁlters via the
normalized total variation [33] that can reveal the smoothness of ﬁlters. Figure 2 (e) and (f) show that
smoother ﬁlters are obtained in SACNNs† at both initial and ﬁnal stages. Based on the conclusion
in [33]  higher deformation stability will be achieved when smoother ﬁlters are learned  which is in
agreement with the results of our experiments in Figure 2 (a)-(d).

6

00.10.20.30.40.50.20.40.60.81varianceδclassiﬁcationaccuracy(a)ClaCNNsSepCNNsActCNNsDefCNNsSACNNs†SACNNs−70−35035700.20.40.60.81rotationangle(degrees)classiﬁcationaccuracy(b)ClaCNNsSepCNNsActCNNsDefCNNsSACNNs†SACNNs−8−40480.20.40.60.81shift(pixels)classiﬁcationaccuracy(c)ClaCNNsSepCNNsActCNNsDefCNNsSACNNs†SACNNs0.50.7511.251.50.60.81scaleclassiﬁcationaccuracy(d)ClaCNNsSepCNNsActCNNsDefCNNsSACNNs†SACNNs1234567891000.10.2layernormalizedtotalvariation(e)initialstageSACNN†ClaCNN1234567891000.10.2layernormalizedtotalvariation(f)ﬁnalstageSACNN†ClaCNNTable 2: The results on the experimental non-Euclidean structured datasets. For each dataset  ↑ (↓)
indicates that the larger (the smaller) values  the better results are.

Mnist↑
Datasets
0.9914
LCNs [4]
0.9840
DFNs [40]
0.9937
ECC [37]
0.9919
MoNets [31]
SCNs [16]
0.9726
ChebNets [10] 0.9914
0.9867
GCNs [22]
SACNNs†
0.9957
0.9961
SACNNs

20News↑ Reuters↑
0.9162
0.6491
0.7017
0.9046
0.9114
0.7003
0.9113
0.6929
0.8985
0.6453
0.6826
0.9124
0.8992
0.6278
0.9365
0.7362
0.7436
0.9452

NTU↑
0.5457
0.6346
0.6416
0.6354
0.5818
0.6384
0.5983
0.6844
0.6931

DPP4↑
0.225
0.214
0.249
0.256
0.248
0.265
0.258
0.279
0.285

TF-198↓ Time (s)
68.83
175±2
192±3
70.35
238±4
65.35
69.35
252±4
75.83
1384±11
673±8
65.86
341±4
71.54
78±2
58.82
53.72
136±2

5.3 Compared with diverse GCNNs on non-Euclidean domains

To verify the versatility of SACNNs for non-Euclidean structured data  we build SACNNs to classify
the texts in 20News and Reuters  recognize the skeleton-based actions in NTU  estimate the activities
of molecules in DPP4  and predict the taxis ﬂows in TF-198  respectively. In addition  Mnist is also
used to see how these GCNNs perform on Euclidean structured data.
Table 2 gives the results in this experiment  which shows that SACNNs and SACNNs† outperform all
the compared methods with signiﬁcant margins. In addition  we have several observations from the
table. First  dramatical improvements are achieved by SACNNs on both Euclidean and non-Euclidean
domains in numerous tasks. Such a good performance veriﬁes that SACNNs can effectively deal
with data on different domains  without any human intervention. Second  Table 1  Table 2 and
Figure 2 consistently show that SACNNs always achieve better performance than SACNNs†. These
results empirically conﬁrm that the local structure representation learning is capable of capturing the
signiﬁcant structure information from data  thus improving the capability of SACNNs with only a few
additional learnable parameters. Furthermore  Table 1 and Table 2 report the time consumptions of
the evaluated methods when one epoch is executed on Mnist during training. From these tables  we
observe that SACNNs are obviously faster than the competitive GCNN methods. Compared with the
CNN methods  the timing cost of SACNNs is tolerably  which ensures the practicability of SACNNs.

5.4 Ablation study

In this subsection  we perform extensive ablation studies on diverse datasets to synthetically analyze
the developed SACNNs. Intuitively  all the results are illustrated in Figure 3. Due to the space
limitation  the learned ﬁlters in SACNNs are presented in the supplementary material.

Impact of polynomial order To show the impact of polynomial order t on the structure-aware
convolution  we select t from {5  40  80  120  160} to generate 11 × 11 ﬁlters to classify STL-10.
Figure 3 (a) illustrates the validation errors of SACNNs with different t. One can observe that the
performance generally improves if we increase the polynomial order t  then the performance will
saturate when ﬁlters can be well approximated  i.e.  t ≥ 80 is satisﬁed. Moreover  it is worthy to
note that the developed SACNNs can utilize parameters more effectively than ClaCNNs. This is
empirically supported by the observation that SACNNs with only 40 parameters per ﬁlter can achieve
signiﬁcant better performance than ClaCNNs with 11 × 11 = 121 parameters per ﬁlter.
Inﬂuence of channels On the Cifar-10 dataset  we model SACNNs with different numbers of
channels c (i.e.  8  16  32) to study its inﬂuence on the local structure representations learning.
Speciﬁcally  we observe the following two tendencies from Figure 3 (b). The ﬁrst one is that the
performance of both SACNNs and ClaCNNs beneﬁts from the increase of the channel numbers. This
is reasonable since more parameters may improve the expressive capability of networks in general.
Second  our SACNNs work consistently better than ClaCNNs  especially when the channel number
is relatively large. One considerable reason is that more information can be exploited to model the
latent structure information to assist SACNNs achieving superior performance.

7

Figure 3: Ablation studies on various datasets. (a) Impact of polynomial order. (b) Inﬂuence of
channels. (c) Transfer learning from Reuters to 20News. (d) Impact of training samples. (e) Inﬂuence
of basis functions. (f) Integration with recent networks. (g) Sensitivity to initialization. (h) Parameters
distribution. Large ﬁgures can be found in the supplementary material.

Transfer learning from Reuters to 20News To reveal the transferability of SACNNs  we ﬁne-
tune the SACNNs that are pre-trained on Reuters (denoted as *SACNNs)  with a small number of
labeled samples (i.e.  1k  2k  3k) in the 20News dataset. Figure 3 (c) shows that the pre-training on
Reuters can signiﬁcantly elevate the performance of SACNNs on 20News and stabilize the training
process simultaneously  especially when labeled training samples are limited. This demonstrates that
SACNNs learned on a domain can be seamlessly transferred to similar domains.

Impact of training samples We randomly sample three sub-datasets with various sizes (i.e.  10k 
25k  50k) from Cifar-100 to evaluate the impact of number of training samples on SACNNs. As
illustrated in Figure 3 (d)  the performance of SACNNs improves when more training samples are
used. Furthermore  the superiority of our SACNNs against ClaCNNs holds on all these cases  which
means that SACNNs are capable of tackling machine learning tasks with both rich and limited data.

Inﬂuence of basis functions To investigate the inﬂuence of basis functions on SACNNs  the
Legendre polynomials are employed as basis functions to learn ﬁlters on Cifar-10. Similar to the
Chebyshev polynomials  the Legendre polynomial hk(x) of order k−1 (k ≥ 3) can be obtained based
on the recurrence relation hk(x) = 2k+1
k+1 hk−2(x)  with h1(x) = 1 and h2(x) = x.
From Figure 3 (e)  almost the same training processes are generated in spite of diverse bases. The
slight mismatching may come from the randomness in training  e.g.  random mini-batch selections.
This demonstrates that the learnability of SACNNs is robust to the basis functions.

k+1 hk−1(x) − k

Integration with recent networks A class of popular networks  i.e.  ResNets [15]  are employed
to survey the range of applications of our structure-aware convolution. The results in Figure 3 (f)
clearly indicate that better improvements will be achieved by replacing the classical convolution in
ResNets with the structure-aware convolution. This adequately validates that the structure-aware
convolution sufﬁces to be applied to general ClaCNNs  not conﬁned to simple and shallow networks.

Sensitivity to initialization We carry out an experiment on Mnist to contrastively analyze the
sensitivities to initializations in SACNNs and ClaCNNs. In this experiment  parameters in networks
are randomly initialized with a uniform distribution U (−α  α)  where α is randomly selected from
[0  1]. Figure 3 (g) illustrates the descending processes of loss functions in ClaCNNs and SACNNs 
indicating that SACNNs generally converge faster than ClaCNNs and are robust to initializations. A
possible reason is that the whole values in the generated discrete ﬁlters fRi = {f (rji)|eji ∈ E} can
be together modiﬁed by adjusting each coefﬁcient of basis functions  which may yield more precise
gradients to accelerate and stabilize the training processes.

Parameters distribution Figure 3 (h) shows the distributions of parameters learned by ClaCNNs
and SACNNs on Mnist in ten convolutional layers. From the ﬁgure  we have the following two
observations. First  the parameters in both SACNNs and ClaCNNs have almost the same standard
deviations. Second  the expectations of parameters in SACNNs are more closer to 0 than ClaCNNs.

8

0204060801000.30.40.50.6epochvalidationerror(STL-10)(a)SACNN:t=5SACNN:t=120SACNN:t=40SACNN:t=160SACNN:t=80ClaCNN:11×110204060801000.20.3epochvalidationerror(Cifar-10)(b)SACNN:c=32ClaCNN:c=32SACNN:c=16ClaCNN:c=16SACNN:c=8ClaCNN:c=8010203040500.40.60.8epochvalidationerror(20News)(c)∗SACNN:1kSACNN:1k∗SACNN:2kSACNN:2k∗SACNN:3kSACNN:3k0501001502000.40.60.8epochvalidationerror(Cifar-100)(d)SACNN:10kClaCNN:10kSACNN:25kClaCNN:25kSACNN:50kClaCNN:50k0204060801000.10.20.3epochvalidationerror(Cifar-10)(e)SACNN:ChebyshevSACNN:LegendreClaCNN:3×3Res20Res32Res44Res56Res1106789networktestingerror%(Cifar-10)(f)SACNNsClaCNNs0246123iteration(1e2)cross-entropyloss(Mnist)(g)SACNNsstdSACNNsmeanClaCNNsstdClaCNNsmean1234567891000.20.4layerparametersdistribution(Mnist)(h)SACNNstdSACNNmeanClaCNNstdClaCNNmeanThese observations reveal that SACNNs have more sparse parameters than ClaCNNs. As a result 
more robust models will be achieved  which is in accordance with the results in Section 5.2.

6 Conclusion

We present a conceptually simple yet powerful structure-aware convolution to establish SACNNs.
In the structure-aware convolution  ﬁlters are represented via univariate functions  which sufﬁce to
aggregate local inputs with diverse topological structures. By feat of the function approximation
theory  a numerical strategy is proposed to learn these ﬁlters in an effectively and efﬁciently way.
Furthermore  rather than using the predeﬁned local structures of data  we incorporate them into the
structure-aware convolution to learn the underlying structure information from data automatically.
Extensive experimental results strongly demonstrate that the structure-aware convolution can be
equipped in SACNNs to learn high-level representations and latent structures for both Euclidean and
non-Euclidean structured data. In the future  we plan to systematically investigate the interpretability
of SACNNs based on their functional ﬁlters  i.e.  univariate functions.

Acknowledgments

This work was supported by the National Natural Science Foundation of China under Grants
91646207  61773377 and 61573352  and the Beijing Natural Science Foundation under Grants
L172053. We would like to thank Lele Yu  Bin Fan  Cheng Da  Tingzhao Yu  Xue Ye  Hongfei Xiao 
and Qi Zhang for their invaluable contributions in shaping the early stage of this work.

References
[1] James Atwood and Don Towsley. Diffusion-convolutional neural networks. In NIPS  pages 1993–2001 

2016.

[2] Davide Boscaini  Jonathan Masci  Emanuele Rodolà  and M. M. Bronstein. Learning shape correspondence

with anisotropic convolutional neural networks. In NIPS  pages 3189–3197  2016.

[3] M. M. Bronstein  Joan Bruna  Yann LeCun  Arthur Szlam  and Pierre Vandergheynst. Geometric deep

learning: Going beyond euclidean data. IEEE Signal Process. Mag.  34(4):18–42  2017.

[4] Joan Bruna  Wojciech Zaremba  Arthur Szlam  and Yann LeCun. Spectral networks and locally connected

networks on graphs. CoRR  abs/1312.6203  2013.

[5] Jianlong Chang  Lingfeng Wang  Gaofeng Meng  Shiming Xiang  and Chunhong Pan. Deep unsupervised

learning with consistent inference of latent representations. Pattern Recognition  77:438–453  2017.

[6] Jianlong Chang  Lingfeng Wang  Gaofeng Meng  Shiming Xiang  and Chunhong Pan. Deep adaptive

image clustering. In ICCV  pages 5880–5888  2017.

[7] François Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR  pages

1800–1807  2017.

[8] Adam Coates  Andrew Ng  and Honglak Lee. An analysis of single-layer networks in unsupervised feature

learning. In AISTATS  pages 215–223  2011.

[9] Jifeng Dai  Haozhi Qi  Yuwen Xiong  Yi Li  Guodong Zhang  Han Hu  and Yichen Wei. Deformable

convolutional networks. In ICCV  pages 764–773  2017.

[10] Micha¨el Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional neural networks on graphs

with fast localized spectral ﬁltering. In NIPS  pages 3837–3845  2016.

[11] I. S. Dhillon  Yuqiang Guan  and Brian Kulis. Weighted graph cuts without eigenvectors A multilevel

approach. IEEE Trans. Pattern Anal. Mach. Intell.  29(11):1944–1957  2007.

[12] Justin Gilmer  S. S. Schoenholz  P. F. Riley  Oriol Vinyals  and G. E. Dahl. Neural message passing for

quantum chemistry. In ICML  pages 1263–1272  2017.

[13] Xavier Glorot  Antoine Bordes  and Yoshua Bengio. Deep sparse rectiﬁer neural networks. In AISTATS 

pages 315–323  2011.

[14] D. K. Hammond  Pierre Vandergheynst  and Rémi Gribonval. Wavelets on graphs via spectral graph theory.

Applied & Computational Harmonic Analysis  30(2):129–150  2009.

[15] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.

In CVPR  pages 770–778  2016.

9

[16] Mikael Henaff  Joan Bruna  and Yann LeCun. Deep convolutional networks on graph-structured data.

CoRR  abs/1506.05163  2015.

[17] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. In ICML  pages 448–456  2015.

[18] Max Jaderberg  Karen Simonyan  Andrew Zisserman  and Koray Kavukcuoglu. Spatial transformer

networks. In NIPS  pages 2017–2025  2015.

[19] Yunho Jeon and Junmo Kim. Active convolution: Learning the shape of convolution for image classiﬁcation.

In CVPR  pages 1846–1854  2017.

[20] Kaggle. Merck molecular activity challenge. https://www.kaggle.com/c/MerckActivity  2012.
[21] D. P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR  abs/1412.6980  2014.
[22] T. N. Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. CoRR 

abs/1609.02907  2016.

[23] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Master’s

Thesis  Department of Computer Science  University of Torono  2009.

[24] Alex Krizhevsky  Ilya Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS  pages 1106–1114  2012.

[25] Ken Lang. Newsweeder: Learning to ﬁlter netnews. In ICML  pages 331–339  1995.
[26] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[27] Ruoyu Li  Sheng Wang  Feiyun Zhu  and Junzhou Huang. Adaptive graph convolutional neural networks.

CoRR  abs/1801.03226  2018.

[28] Yujia Li  Daniel Tarlow  Marc Brockschmidt  and R. S. Zemel. Gated graph sequence neural networks.

CoRR  abs/1511.05493  2015.

[29] Renjie Liao  Marc Brockschmidt  Daniel Tarlow  A. L. Gaunt  Raquel Urtasun  and R. S. Zemel. Graph

partition neural networks for semi-supervised classiﬁcation. CoRR  abs/1803.06272  2018.

[30] Jonathan Masci  Davide Boscaini  M. M. Bronstein  and Pierre Vandergheynst. Geodesic convolutional

neural networks on riemannian manifolds. In ICCV Workshops  pages 832–840  2015.

[31] Federico Monti  Davide Boscaini  Jonathan Masci  Emanuele Rodolà  Jan Svoboda  and M. M. Bronstein.
Geometric deep learning on graphs and manifolds using mixture model cnns. In CVPR  pages 5425–5434 
2017.

[32] Shaoqing Ren  Kaiming He  R. B. Girshick  and Jian Sun. Faster R-CNN: towards real-time object

detection with region proposal networks. In NIPS  pages 91–99  2015.

[33] Avraham Ruderman  N. C. Rabinowitz  A. S. Morcos  and Daniel Zoran. Learned deformation stability in

convolutional neural networks. CoRR  abs/1804.04438  2018.

[34] K.T. Sch¨utt  F. Arbabzadah  S. Chmiela  K.R. M¨uller  and A. Tkatchenko. Quantum-chemical insights

from deep tensor neural networks. Nature Communications  8(13890)  2017.

[35] M. S. Schlichtkrull  T. N. Kipf  Peter Bloem  Rianne van den Berg  Ivan Titov  and Max Welling. Modeling

relational data with graph convolutional networks. CoRR  abs/1703.06103  2017.

[36] Amir Shahroudy  Jun Liu  Tian-Tsong Ng  and Gang Wang. NTU RGB+D: A large scale dataset for 3d

human activity analysis. In CVPR  pages 1010–1019  2016.

[37] Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned ﬁlters in convolutional neural

networks on graphs. In CVPR  pages 29–38  2017.

[38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. CoRR  abs/1409.1556  2014.

[39] Petar Velickovic  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Liò  and Yoshua Bengio.

Graph attention networks. CoRR  abs/1710.10903  2017.

[40] Nitika Verma  Edmond Boyer  and Jakob Verbeek. Dynamic ﬁlters in graph convolutional networks. CoRR 

abs/1706.05206  2017.

[41] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. CoRR  ab-

s/1511.07122  2015.

[42] Qi Zhang  Qizhao Jin  Jianlong Chang  Shiming Xiang  and Chunhong Pan. Kernel-weighted graph
convolutional network: A deep learning approach for trafﬁc forcasting. In ICPR  pages 1018–1023  2018.

10

,Jianlong Chang
Jie Gu
Lingfeng Wang
GAOFENG MENG
SHIMING XIANG
Chunhong Pan