2009,Group Sparse Coding,Bag-of-words document representations are often used in text  image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents  there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set  and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors  those methods however do not ensure that images have sparse representation. In this work  we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class  providing a discriminative signal in the construction of image representations. Experimental results on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency  the proposed approach yields better mean average precision in classification.,Group Sparse Coding

Samy Bengio

Google

Mountain View  CA

Fernando Pereira

Google

Mountain View  CA

Yoram Singer

Google

Mountain View  CA

bengio@google.com

pereira@google.com

singer@google.com

Dennis Strelow

Google

Mountain View  CA

strelow@google.com

Abstract

Bag-of-words document representations are often used in text  image and video
processing. While it is relatively easy to determine a suitable word dictionary for
text documents  there is no simple mapping from raw images or videos to dictio-
nary terms. The classical approach builds a dictionary using vector quantization
over a large set of useful visual descriptors extracted from a training set  and uses a
nearest-neighbor algorithm to count the number of occurrences of each dictionary
word in documents to be encoded. More robust approaches have been proposed
recently that represent each visual descriptor as a sparse weighted combination of
dictionary words. While favoring a sparse representation at the level of visual de-
scriptors  those methods however do not ensure that images have sparse represen-
tation. In this work  we use mixed-norm regularization to achieve sparsity at the
image level as well as a small overall dictionary. This approach can also be used to
encourage using the same dictionary words for all the images in a class  providing
a discriminative signal in the construction of image representations. Experimen-
tal results on a benchmark image classiﬁcation dataset show that when compact
image or dictionary representations are needed for computational efﬁciency  the
proposed approach yields better mean average precision in classiﬁcation.

1

Introduction

Bag-of-words document representations are widely used in text  image  and video processing [14  1].
Those representations abstract from spatial and temporal order to encode a document as a vector of
the numbers of occurrences in the document of descriptors from a suitable dictionary. For text
documents  the dictionary might consist of all the words or of all the n-grams of a certain minimum
frequency in the document collection [1].

For images or videos  however  there is no simple mapping from the raw document to descriptor
counts. Instead  visual descriptors must be ﬁrst extracted and then represented in terms of a care-
fully constructed dictionary. We will not discuss further here the intricate processes of identifying
useful visual descriptors  such as color  texture  angles  and shapes [14]  and of measuring them at
appropriate document locations  such as on regular grids  on special interest points  or at multiple
scales [6].

For dictionary construction  the standard approach in computer vision is to use some unsupervised
vector quantization (VQ) technique  often k-means clustering [14]  to create the dictionary. A new
image is then represented by a vector indexed by dictionary elements (codewords)  which for el-
ement d counts the number of visual descriptors in the image whose closest codeword is d. VQ

1

representations are maximally sparse per descriptor occurrence since they pick a single codeword
for each occurrence  but they may not be sparse for the image as a whole; furthermore  such repre-
sentations are not that robust with respect to descriptor variability.

Sparse representations have obvious computational beneﬁts  by saving both processing time in han-
dling visual descriptors and space in storing encoded images. To alleviate the brittleness of VQ
representations  several studies proposed representation schemes where each visual descriptor is en-
coded as a weighted sum of dictionary elements  where the encoding optimizes a tradeoff between
reconstruction error and the ℓ1 norm of the reconstruction weights [3  5  7  8  9  16]. These tech-
niques promote sparsity in determining a small set of codewords from the dictionary that can be
used to efﬁciently represent each visual descriptor of each image [13].

However  those approaches consider each visual descriptor in the image as a separate coding prob-
lem and do not take into account the fact that descriptor coding is just an intermediate step in creating
a bag of codewords representation for the whole image. Thus  sparse coding of each visual descrip-
tor does not guarantee sparse coding of the whole image. This might prevent the use of such methods
in real large scale applications that are constrained by either time or space resources. In this study 
we propose and evaluate the mixed-norm regularizers [12  10  2] to take into account the structure
of bags of visual descriptors present in images. Using this approach  we can for example specify an
encoder that exploits the fact that once a codeword has been selected to help represent one of the
visual descriptors of an image  it may as well be used to represent other visual descriptors of the
same image without much additional regularization cost.

Furthermore  while images are represented as bags  the same idea could be used for sets of images 
such as all the images from a given category.
In this case  mixed regularization can be used to
specify that when a codeword has been selected to help represent one of the visual descriptors of an
image of a given category  it could as well be used to represent other visual descriptors of any image
of the same category at no additional regularization cost. This form of regularization thus promotes
the use of a small subset of codewords for each category that could be different from category to
category  thus including an indirect discriminative signal in code construction.

Mixed regularization can be applied at two levels: for image encoding  which can be expressed
as a convex optimization problem  and for dictionary learning  using an alternating minimization
procedure. Dictionary regularization promotes a small dictionary size directly  instead of indirectly
through the sparse encoding step.

The paper is organized as follows: Sec. 2 introduces the notation used in the rest of the paper  and
summarizes the technical approach. Sec. 3 describes and solves the convex optimization problem for
mixed-regularization encoding. Sec. 4 extends the technique to learn the dictionary by alternating
optimization. Finally  Sec. 5 presents experimental results on a well-known image database.

2 Problem Statement

v  u · v = Pn

We denote scalars with lower-case letters  vectors with bold lower-case letters such as v. We assume
that the instance space is Rn endowed with the standard inner product between two vectors u and
j=1 ujvj. We also use the standard ℓp norms k · kp over Rn with p ∈ 1  2  ∞. We often

make use of the fact that u · u = kuk2  where as usual we omit the norm subscript for p = 2..
Our main goal is to encode effectively groups of instances in terms of a set of dictionary codewords
D = {dj}|D|
j=1. For example  if instances are image patches  each group may be the set of patches in
a particular image  and each codeword may represent some kind of average patch. The m’th group
is denoted Gm where Gm = {xm i}|Gm|
i=1 where each xm i ∈ Rn is an instance. When discussing
operations on a single group  we use G for the group in discussion and denote by xi its i’th instance.
Given D and G  our ﬁrst subgoal  encoding  is to minimize a tradeoff between the reconstruction
error for G in terms of D  and a suitable mixed norm for the matrix of reconstruction weights
that express each xi as a positive linear combination of dj ∈ D. The tradeoff between accurate
reconstruction or compact encoding is governed through a regularization parameter λ.
Our second subgoal  learning  is to estimate a good dictionary D given a set of training groups
{Gm}n
m=1. We achieve these goals by alternating between (i) ﬁxing the dictionary to ﬁnd recon-

2

struction weights that minimize the sum of encoding objectives for all groups  and (ii) ﬁxing the
reconstruction weights for all groups to ﬁnd the dictionary that minimizes a tradeoff between the
sum of group encoding objectives and the mixed norm of the dictionary.

3 Group Coding

To encode jointly all the instances in a group G with dictionary D  we solve the following convex
optimization problem:

A⋆ = arg minA Q(A  G  D)

where
and

Q(A  G  D) = 1

2 Pi∈G(cid:13)(cid:13)(cid:13)xi −P|D|

j=1 αi

2

jdj(cid:13)(cid:13)(cid:13)

αi

j ≥ 0 ∀i  j .

+ λP|D|

j=1 kαjkp

(1)

j=1 consists of non-negative vectors αj = (α1

The reconstruction matrix A = {αj}|D|
j   . . .   α|G|
j )
specifying the contribution of dj to each instance. The second term of the objective weighs the
mixed ℓ1/ℓp norm of A  which measures reconstruction complexity  with the regularization param-
eter λ that balances reconstruction quality (the ﬁrst term) and reconstruction complexity.
The problem of Eq. (1) can be solved by coordinate descent. Leaving all indices intact except for
index r  omitting ﬁxed arguments of the objective  and denoting by c1 and c2 terms which do not
depend on αr  we obtain the following reduced objective:

Q(αr) =

αi
jdj − αi

+ λ kαrkp + c1

1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
2 Xi∈G
xi −Xj6=r

Xj6=r
=Xi∈G

αi
jαi

2

rdr(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

r(dj · dr)−αi

r(xi · dr)+

(αi

r)2kdrk2

1
2

+λ kαrkp +c2 .

(2)

We next show how to ﬁnd the optimum αr for p = 1 and p = 2. Let ˜Q be just the reconstruction
term of the objective. Its partial derivatives with respect to each αi

r are

∂
∂αi
r

˜Q = Xj6=r

αi
j(dj · dr) − xi · dr + αi

rkdrk2 .

Let us make the following abbreviation for a given index r 

µi = xi · dr −Xj6=r

αi

j(dj · dr) .

(3)

(4)

r is zero. In the derivation below we therefore
It is clear that if µi ≤ 0 then the optimum for αi
employ µ+
i = [µi]+ where [z]+ = max{0  z}. Next we derive the optimal solution for each of the
norms we consider starting with p = 1. For p = 1 the objective function is separable and we get the
following sub-gradient condition for optimality 

0 ∈ −µ+

i + αi

rkdrk2 + λ

∂
∂αi
r

|αi
r|

⇒ αi

r ∈

µ+

i − [0  λ]
kdrk2

.

(5)

∈[0 1]

| {z }

i = (µ+

i − λ)/kdrk2.

i ≥ 0 the above subgradient condition for optimality implies that αr

Since αr
otherwise αr
The objective function is not separable when p = 2. In this case we need to examine the entire
set of values {µ+
i . Assume for now that the
optimal solution has a non-zero norm  kαrk2 > 0. In this case  the gradient of Q(αr) with an ℓ2
regularization term is

i }. We denote by µ+ the vector whose i’th value is µ+

i = 0 when µ+

i ≤ λ and

αr
kαrk

.

kdrk2αr − µ+ + λ

3

which implies that

λ

s µ+ = (cid:18)kdrk2 +
kdrk2 (cid:18)1 −

skµ+k(cid:19)−1
kµ+k(cid:19) .

s =

1

λ

µ+  

At the optimum this vector must be zero  so after rearranging terms we obtain

αr = (cid:18)kdrk2 +

λ

kαrk(cid:19)−1

µ+ .

(6)

Therefore  the vector αr is in the same direction as µ+ which means that we can simply write
αr = s µ+ where s is a non-negative scalar. We thus can rewrite Eq. (6) solely as a function of the
scaling parameter s

(7)

We now revisit the assumption that the norm of the optimal solution is greater than zero. Since s
cannot be negative the above expression also provides the condition for obtaining a zero vector for
αr. Namely  the term 1 − λ/kµ+k must be positive  thus  we get that αr = 0 if kµ+k ≤ λ and
otherwise αr = sµ+ where s is deﬁned in Eq. (7).
Once the optimal group reconstruction matrix A is found  we compress the matrix into a single
vector. This vector is of ﬁxed dimension and does not depend on the number of instances that
constitute the group. To do so we simply take the p-norm of each αj  thus yielding a |D| dimensional
vector. Since we use mixed-norms which are sparsity promoting  in particular the ℓ1/ℓ2 mixed-norm 
the resulting vector is likely to be sparse  as we show experimentally in Section 6.

Since visual descriptors and dictionary elements are only accessed through inner products in the
above method  it could be easily generalized to work with Mercer kernels instead.

4 Dictionary Learning

Now that we know how to achieve optimal reconstruction for a given dictionary  we examine how to
learn a good dictionary  that is  a dictionary that balances between reconstruction error  reconstruc-
tion complexity  overall complexity relative to the given training set. In particular  we seek a learning
method that facilitates both induction of new dictionary words and the removal of dictionary words
with low predictive power. To achieve this goal  we will apply ℓ1/ℓ2 regularization controlled by a
new hyperparameter γ  to dictionary words. For this approach to work  we assume that instances
have been mean-subtracted so that the zero vector 0 is the (uninformative) mean of the data and
regularization towards 0 is equivalent to removing words that do not contribute much to compact
representation of groups.
Let G = {G1  . . .   Gn} be a set of groups and A = {A1  . . .   An} the corresponding reconstruction
coefﬁcients relative to dictionary D. Then  the following objective meets the above requirements:

Q(A  D) =

n

Xm=1

Q(Am  Gm  D) + γ

|D|

Xk=1

kdkkp s.t. αi

m j ≥ 0 ∀i  j  m  

(8)

where the single group objective Q(Am  Gm  D) is as in Eq. (1).
In our application we set p = 2 as the norm penalty of the dictionary words. For ﬁxed A  the ob-
jective above is convex in D. Moreover  the same coordinate descent technique described above for
ﬁnding the optimum reconstruction weights can be used again here after simple algebraic manipu-
lations. Deﬁne the following auxiliary variables:

vr = Xm Xi

αi

m rxm i and νj k = Xm Xi

αi
m jαi

m k .

(9)

Then  we can express dr compactly as follows. As before  assume that kdrk > 0. Calculating the
gradient with respect to each dr and equating it to zero  we obtain

Xm Xi∈Gm


Xj6=r

αi
m jαi

m rdj + (αi

m r)2dr − αi

m rxm i

 + γ

dr
kdrk

= 0 .

4

Swapping the sums over m and i with the sum over j  using the auxiliary variables  and noting that
dj does not depend neither on m nor on i  we obtain

Xj6=r

νj rdj + νr rdr − vr + γ

dr
kdrk

= 0 .

(10)

Similarly to the way we solved for αr  we now deﬁne the vector ur = vr −Pj6=r νj rdj to get the

following iterate for dr:

(11)

dr = ν−1

r r (cid:20)1 −

γ

kurk(cid:21)+

ur  

where  as above  we incorporated the case dr = 0  by applying the operator [·]+ to the term
1 − γ/kurk. The form of the solution implies that we can eliminate dr  as it becomes 0  when-
ever the norm of the residual vector ur is smaller than γ. Thus  the dictionary learning procedure
naturally facilitates the ability to remove dictionary words whose predictive power falls below the
regularization parameter.

5 Experimental Setting

We compare our approach to image coding with previous sparse coding methods by measuring their
impact on classiﬁcation performance on the PASCAL VOC (Visual Object Classes) 2007 dataset [4].
The VOC datasets contain images from 20 classes  including people  animals (bird)  vehicles (aero-
plane)  and indoor objects (chair)  and are considered natural  difﬁcult images for classiﬁcation.
There are around 2500 training images  2500 validation images and 5000 test images in total.

For each coding technique under consideration  we explore a range of values for the hyperparameters
λ and γ.
In the past  many features have been used for VOC classiﬁcation  with bag-of-words
histograms of local descriptors like SIFT [6] being most popular. In our experiments  we extract
local descriptors based on a regular grid for each image. The grid points are located at every seventh
pixel horizontally and vertically  which produces an average of 3234 descriptors per image. We
used a custom local descriptor that collects Gabor wavelet responses at different orientations  spatial
scales  and spatial offsets from the interest point. Four orientations (0◦  45◦  90◦  135◦) and 27
(scale  offset) combinations are used  for a total of 108 components. The 27 (scale  offset) pairs were
chosen by optimizing a previous image recognition task  unrelated to this paper  using a genetic
algorithm. Tola et al. [15] independently described a descriptor that similarly uses responses at
different orientations  scales  and offsets (see their Figure 2). Overall  this descriptor is generally
comparable to SIFT and results in similar performance.

To build an image feature vector from the descriptors  we thus investigate the following methods:

1. Build a bag-of-words histogram over hierarchical k-means codewords by looking up each
descriptor in a hierarchical k-means tree [11]. We use branching factors of 6 to 13 and a
depth of 3 for a total of between 216 and 2197 codewords. When used with multiple feature
types  this method results in very good classiﬁcation performance on the VOC task.

2. Jointly train a dictionary and encode each descriptor using an ℓ1 sparse coding approach

with γ = 0  which was studied previously [5  7  9].

3. Jointly train a dictionary and encode sets of descriptors where each set corresponds to a

single image  using ℓ1/ℓ2 group sparse coding  varying both γ and λ.

4. Jointly train a dictionary and encode sets of descriptors where each set corresponds to all
descriptors or all images of a single class  using ℓ1/ℓ2 sparse coding  varying both γ and λ.
Then  use ℓ1/ℓ2 sparse coding to encode the descriptors in individual images and obtain a
single α vector per image.

As explained before  we normalized all descriptors to have zero mean so that regularizing dictionary
words towards the zero vector implies dictionary sparsity.

In all cases  the initial dictionary used during training was obtained from the same hierarchical k-
means tree  with a branching factor of 10 and depth 4 rather than 3 as used in the baseline method.
This scheme yielded an initial dictionary of size 7873.

5

n
o
i
s
i
c
e
r
P
e
g
a
r
e
v
A
n
a
e

M

0.4

0.35

0.3

0.25

0.2

0.15

Mean Average Precision vs Dictionary Size

HKMeans
ℓ1 ;γ = 0;λ vary
ℓ1/ℓ2 ;γ = 0;λ vary;group=image
ℓ1/ℓ2 ;γ vary;λ = 6.8e − 5;group=image
ℓ1/ℓ2 ;γ = 0;λ vary;group=class
ℓ1/ℓ2 ;γ vary;λ vary;group=class

500

1000
Dictionary Size

1500

2000

Figure 1: Mean Average Precision on the 2007 PASCAL VOC database as a function of the size of
the dictionary obtained by both ℓ1 and ℓ1/ℓ2 regularization approaches when varying λ or γ. We
show results where descriptors are grouped either by image or by class. The baseline system using
hierarchical k-means is also shown.

To evaluate the impact of different coding methods on an important end-to-end task  image classi-
ﬁcation  we selected the VOC 2007 training set for classiﬁer training  the VOC 2007 validation set
for hyperparameter selection  and the VOC 2007 test set for for evaluation. After the datasets are
encoded with each of the methods being evaluated  a one-versus-all linear SVM is trained on the
encoded training set for each of the 20 classes  and the best SVM hyperparameter C is chosen on
the validation set. Class average precisions on the encoded test set are then averaged across the 20
classes to produce the mean average precision shown in our graphs.

6 Results and Discussion

In Figure 1 we compare the mean average precisions of the competing approaches as encoding
hyperparameters are varied to control the overall dictionary size. For the ℓ1 approach  achieving
different dictionary size was obtained by tuning λ while setting γ = 0. For the ℓ1/ℓ2 approach 
since it was not possible to compare all possible combinations of λ and γ  we ﬁrst ﬁxed γ to be
zero  so that it could be comparable to the standard ℓ1 approach with the same setting. Then we
ﬁxed λ to a value which proved to yield good results and varied γ. As it can be seen in Figure 1 
when the dictionary is allowed to be very large  the pure ℓ1 approach yields the best performance.
On the other hand  when the size of the dictionary matters  then all the approaches based on ℓ1/ℓ2
regularization performed better than the ℓ1 counterpart. Even hierarchical k-means performed better
than the pure ℓ1 in that case. The version of ℓ1/ℓ2 in which we allowed γ to vary provided the best
tradeoff between dictionary size and classiﬁcation performance when descriptors were grouped per
image  which was to be expected as γ directly promotes sparse dictionaries. More interestingly 
when grouping descriptors per class instead of per image  we get even better performance for small
dictionary sizes by varying λ.
In Figure 2 we compare the mean average precisions of ℓ1 and ℓ1/ℓ2 regularization as average image
size varies. When image size is constrained  which is often the case is large-scale applications  all

6

n
o
i
s
i
c
e
r
P
e
g
a
r
e
v
A
n
a
e

M

0.4

0.35

0.3

0.25

0.2

0.15

Mean Average Precision vs Average Image Size

HKMeans
ℓ1 ;γ = 0;λ vary
ℓ1/ℓ2 ;γ = 0;λ vary;group=image
ℓ1/ℓ2 ;γ vary;λ = 6.8e − 5;group=image
ℓ1/ℓ2 ;γ = 0;λ vary;group=class
ℓ1/ℓ2 ;γ vary;λ vary;group=class

500

1000

1500

2000

Average Image Size

Figure 2: Mean Average Precision on the 2007 PASCAL VOC database as a function of the average
size of each image as encoded using the trained dictionary obtained by both ℓ1 and ℓ1/ℓ2 regular-
ization approaches when varying λ and γ. We show results where descriptors are grouped either by
image or by class. The baseline system using hierarchical k-means is also shown.

200

400

600

800

1000

1200

1400

200

400

600

800

1000

1200

1400

1600

1800

2000

100

200

300

400

500

600

700

800

900

1000

100

200

300

400

500

600

700

800

900

1000

Figure 3: Comparison of the dictionary words used to reconstruct the same image. A pure ℓ1 coding
was used on the left  while a mixed ℓ1/ℓ2 encoding was used on the right plot. Each row represents
the number of times each dictionary word was used in the reconstruction of the image.

the ℓ1/ℓ2 regularization choices yield better performance than ℓ1 regularization. Once again ℓ1
regularization performed even worse than hierarchical k-means for small image sizes
Figure 3 compares the usage of dictionary words to encode the same image  either using ℓ1 (on the
left) or ℓ1/ℓ2 (on the right) regularization. Each graph shows the number of times a dictionary word
(a row in the plot) was used in the reconstruction of the image. Clearly  ℓ1 regularization yields
an overall sparser representation in terms of total number of dictionary coefﬁcients that are used.
However  almost all of the resulting dictionary vectors are non-zero and used at least once in the
coding process. As expected  with ℓ1/ℓ2 regularization  a dictionary word is either always used or
never used yielding a much more compact representation in terms of the total number of dictionary
words that are used.

7

Overall  mixed-norm regularization yields better performance when the problem to solve includes
resource constraints  either time (a smaller dictionary yields faster image encoding) or space (one
can store or convey more images when they take less space). They might thus be a good ﬁt when
a tradeoff between pure performance and resources is needed  as is often the case for large-scale
applications or online settings.

Finally  grouping descriptors per class instead of per image during dictionary learning promotes the
use of the same dictionary words for all images of the same class  hence yielding some form of weak
discrimination which appears to help under space or time constraints.

Acknowledgments

We would like to thanks John Duchi for numerous discussions and suggestions.

References

[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information Retrieval. Addison Wesley  Harlow 

England  1999.

[2] J. Duchi and Y. Singer. Boosting with structural sparsity.

Machine Learning (ICML)  2009.

In International Conference on

[3] M. Elad and M. Aharon. Image denoising via sparse and redundant representation over learned

dictionaries. IEEE Transaction on Image Processing  15(12):3736–3745  2006.

[4] M. Everingham  L. Van Gool  C. K. I. Williams  J. Winn  and A. Zisserman.

The
PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.

[5] H. Lee  A. Battle  R. Raina  and A. Y. Ng. Efﬁcient sparse coding algorithms. In Advances in

Neural Information Processing Systems (NIPS)  2007.

[6] D. G. Lowe. Object recognition from local scale-invariant features. In International Confer-

ence on Computer Vision (ICCV)  pages 1150–1157  1999.

[7] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online dictionary learning for sparse coding. In

International Conference on Machine Learning (ICML)  2009.

[8] J. Mairal  M. Elad  and G. Sapiro. Sparse representation for color image restoration. IEEE

Transaction on Image Processing  17(1)  2008.

[9] J. Mairal  M. Leordeanu  F. Bach  M. Hebert  and J. Ponce. Discriminative sparse image
models for class-speciﬁc edge detection and image interpretation. In European Conference on
Computer Vision (ECCV)  2008.

[10] S. Negahban and M. Wainwright. Phase transitions for high-dimensional joint support recov-

ery. In Advances in Neural Information Processing Systems 22  2008.

[11] D. Nister and H. Stewenius. Scalable recognition with a vocabulary tree. In Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2006.

[12] G. Obozinski  B. Taskar  and M. Jordan. Joint covariate selection for grouped classiﬁcation.

Technical Report 743  Dept. of Statistics  University of California Berkeley  2007.

[13] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy

employed by v1? Vision Research  37  1997.

[14] P. Quelhas  F. Monay  J. M. Odobez  D. Gatica-Perez  T. Tuytelaars  and L. J. Van Gool.
In International Conference on

Modeling scenes with local descriptors and latent aspects.
Computer Vision (ICCV)  2005.

[15] E. Tola  V. Lepetit  and P. Fua. A fast local descriptor for dense matching. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2008.

[16] J. yang  K. Yu  Y. Gong  and T. Huang. Linear spatial pyramid matching using sparse coding
In Proceedings of the IEEE Conference on Computer Vision and

for image classiﬁcation.
Pattern Recognition (CVPR)  2009.

8

,Amir Dezfouli
Edwin Bonilla
Tim Roughgarden
Okke Schrijvers
Abhishek Sharma