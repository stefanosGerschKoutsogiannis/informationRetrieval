2016,Learning Transferrable Representations for Unsupervised Domain Adaptation,Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However  this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset  recent papers have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions.  Nevertheless  these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters and overfitting during the fine-tuning stage. In this paper  we propose a unified deep learning framework where the representation  cross domain transformation  and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. We will make our learned models as well as the source code available immediately upon acceptance.,Learning Transferrable Representations for

Unsupervised Domain Adaptation

Ozan Sener1  Hyun Oh Song1  Ashutosh Saxena2  Silvio Savarese1

Stanford University1 Brain of Things2

{ozan hsong asaxena ssilvio}@cs.stanford.edu

Abstract

Supervised learning with large scale labelled datasets and deep layered models has
caused a paradigm shift in diverse areas in learning and recognition. However  this
approach still suffers from generalization issues under the presence of a domain
shift between the training and the test data distribution. Since unsupervised domain
adaptation algorithms directly address this domain shift problem between a labelled
source dataset and an unlabelled target dataset  recent papers [11  33] have shown
promising results by ﬁne-tuning the networks with domain adaptation loss functions
which try to align the mismatch between the training and testing data distributions.
Nevertheless  these recent deep learning based domain adaptation approaches still
suffer from issues such as high sensitivity to the gradient reversal hyperparameters
[11] and overﬁtting during the ﬁne-tuning stage. In this paper  we propose a uniﬁed
deep learning framework where the representation  cross domain transformation 
and target label inference are all jointly optimized in an end-to-end fashion for
unsupervised domain adaptation. Our experiments show that the proposed method
signiﬁcantly outperforms state-of-the-art algorithms in both object recognition and
digit classiﬁcation experiments by a large margin.

1

Introduction

Recently  deep convolutional neural networks [17  26  30] have propelled unprecedented advances
in artiﬁcial intelligence including object recognition  speech recognition  and image captioning.
Although these networks are very good at learning state of the art feature representations and
recognizing discriminative patterns  one major drawback is that the network requires huge amounts
of labelled training data to ﬁt millions of parameters in the complex network. However  creating such
datasets with complete annotations is not only tedious and error prone  but also extremely costly. In
this regard  the research community has proposed different mechanisms such as semi-supervised
learning [27  37]  transfer learning [23  31]  weakly labelled learning  and domain adaptation. Among
these approaches  domain adaptation is one of the most appealing techniques when a fully annotated
dataset (e.g. ImageNet [7]  Sports1M [14]) is already available as a reference.
The goal of unsupervised domain adaptation  in particular  is as follows. Given a fully labeled
source dataset and an unlabeled target dataset  to learn a model which can generalize to the target
domain while taking the domain shift across the datasets into account. The majority of the literature
[13  29  9  28  32] in unsupervised domain adaptation formulates a learning problem where the task
is to ﬁnd a transformation matrix to align the labelled source data distribution to the unlabelled target
data distribution. Although these approaches have shown promising results  they show accuracy
degradation because of the discrepancy between the learning procedure and the actual target inference
procedure. In this paper  we aim to address this issue by incorporating the unknown target labels into
the training procedure.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In this regard  we formulate a uniﬁed deep learning framework where the feature representation 
domain transformation  and target labels are all jointly optimized in an end-to-end fashion. The
proposed framework ﬁrst takes as input a batch of labelled source and unlabelled target examples  and
maps this batch of raw input examples into a deep representation. Then  the framework computes the
loss of the input batch based on a two stage optimization in which it alternates between inferring the
labels of the target examples transductively and optimizing the domain transformation parameters.
Concretely  in the transduction stage  given the ﬁxed domain transform parameter  we jointly infer
all target labels by solving a discrete multi-label energy minimization problem. In the adaptation
stage  given a ﬁxed target label assignment  we seek to ﬁnd the optimal asymmetric metric between
the source and the target data. The advantage of our method is that we can jointly learn the optimal
feature representation and the optimal domain transformation parameter  which are aware of the
subsequent transductive inference procedure.
Following the standard evaluation protocol in the unsupervised domain adaptation community 
we evaluate our method on the digit classiﬁcation task using MNIST [19] and SVHN[21]
as well as the object recognition task using the Ofﬁce [25] dataset  and demonstrate state
of the art performance in comparison to all existing unsupervised domain adaptation meth-
ods.
Learned models and the source code can be reached from the project webpage
http://cvgl.stanford.edu/transductive_adaptation.

2 Related Work

This paper is closely related to two active research areas: (1) Unsupervised domain adaptation  and
(2) Transductive learning.
Unsupervised domain adaptation: [16] casts the zero-shot learning [22] problem as an unsupervised
domain adaptation problem in the dictionary learning and sparse coding framework  assuming access
to additional attribute information. Recently  [3] proposed the active nearest neighbor algorithm 
which combines the component of active learning into the domain adaptation problem and makes
a bounded number of active queries to users. Also  [13  9  28] proposed subspace alignment based
approaches to unsupervised domain adaptation where the task is to learn a joint transformation
and projection in which the difference between the source and the target covariance is minimized.
However  these methods learn the transformation matrices on the whole source and target dataset
without utilizing the source labels.
[32] utilizes a local max margin metric learning objective [35] to ﬁrst assign the target labels with the
nearest neighbor scheme and then learn a distance metric to enforce the negative pairwise distances
to be larger than the positive pairwise distances. However  this method learns a symmetric distance
matrix shared by both the source and the target domains so the method is susceptible to the discrep-
ancies between the source and the target distributions. Recently  [11  33] proposed a deep learning
based method to learn domain invariant features by providing the reversed gradient signal from the
binary domain classiﬁers. Although this method performs better than aforementioned approaches 
their accuracy is limited since domain invariance does not necessarily imply discriminative features
in the target domain.
Transductive learning: In the transductive learning [10]  the model has access to unlabelled test
samples during training. [24] utilizes a semi-supervised label propagation algorithm into the semi-
supervised transfer learning problem assuming access to few labeled examples and additional human
speciﬁed semantic knowledge. [15] tackled a classiﬁcation problem where predictions are made
jointly across all test examples in a transductive [10] setting. The method essentially enforces the
notion that the true labels vary smoothly with respect to the input data. We extend this notion to
jointly infer the labels of unsupervised target data points in a k-NN graph.
To summarize  our main contribution is to formulate an end-to-end deep learning framework where
we learn the optimal feature representation  infer target labels via discrete energy minimization
(transduction)  and learn the transformation (adaptation) between source and target examples all
jointly. Our experiments on digit classiﬁcation using MNIST [19] and SVHN[21] as well as the
object recognition experiments on Ofﬁce [25] datasets show state of the art results  outperforming all
existing methods by a substantial margin.

2

3 Method

3.1 Problem Deﬁnition and Notation
In the unsupervised domain adaptation  one of the domains (source) is supervised {ˆxi  ˆyi}i∈[N s] with
N s data points ˆxi and the corresponding labels ˆyi from a discrete set ˆyi ∈ Y = {1  . . .   Y }. The
other domain (target)  on the other hand is unsupervised and has N u data points {xi}i∈[N u].
We further assume that two domains have different distributions ˆxi ∼ ps and xi ∼ pt deﬁned on the
same space ˆxi  xi ∈ X . We consider a case in which there are two feature functions Φs  Φt : X → Rd
applicable to source and target separately. These feature functions extract the information both shared
among domains and explicit to the individual ones. The way we model common features is by
sharing a subset of parameters between feature functions as Φs = Φθc θs and Φt = Φθc θt. We use
deep neural networks to implement these functions. In our implementation  θc corresponds to the
parameters in the ﬁrst few layers of the networks and θs  θt correspond to the respective ﬁnal layers.
In general  our model is applicable to any hierarchical and differentiable feature function which can
be expressed as a composite function Φs = fθs(gθc(·)) for both source and target.
3.2 Consistent Structured Transduction

Our method is based on jointly learning the transferable domain speciﬁc representations for source
and target as well as estimating the labels of the unsupervised data-points. We denote these two main
components of our method as transduction and adaptation. The transduction is the sub-problem of
labelling unsupervised data points and the adaptation is the sub-problem of solving for the domain
shift. In order to solve this joint problem tractably  we exploit two heuristics: cyclic consistency for
adaptation and structured consistency for transduction.
Cyclic consistency: One desired property of Φs and Φt is consistency. If we estimate the labels of
the unsupervised data points and then use these points with their estimated labels to estimate the
labels of supervised data-points  we want the predicted labels of the supervised data-points to be
consistent with the ground truth labels. Using the inner product as an asymmetric similarity metric as
(cid:124)
s(ˆxi  xj) = Φs(ˆxi)

Φt(xj)  this consistency can be represented with the following diagram.

(ˆxi  ˆyi)

Transduction

/ (xj  yj)

Transduction

/ (ˆxi  ˆypred

i

)

Cyclic Consistency: ˆyi = ˆypred

i

i

(cid:124)

(cid:124)

Φt(xi+) > Φs(ˆxi)

It can be shown that if the transduction from target to source follows a nearest neighbor rule  cyclic
consistency can be enforced without explicitly computing ˆypred
using the large-margin nearest
neighbor (LMNN)[35] rule. For each source point  we enforce a margin such that the similarity
between the source point and the nearest neighbor from the target with the same label is greater than
the similarity between the source point and the nearest neighbor from the target with a different label.
Formally; Φs(ˆxi)
Φt(xi− ) + α where xi+ is the nearest target having the same
class label as ˆxi and xi− is the nearest target having a different class label.
Structured consistency: We enforce a structured consistency when we label the target points during
the transduction. The structure we enforce is; if two target points are similar to each other  they are
more likely to have the same label. To do so  we create a k-NN graph of target points using a similarity
Φt(xj). We denote the neighbors of the point ˆxi as N (ˆxi). We enforce structured
(cid:124)
metric Φt(xi)
consistency by penalizing neighboring points of different labels proportional to their similarity score.
Our model leads to the following optimization problem  over the target labels yi and the feature
function parameters θc  θs  θt  jointly solving transduction and adaptation.

(cid:88)

i∈[N s]

(cid:124)

min

θc θs θt 
y1 ...yN u

s.t.

[Φs(ˆxi)

(cid:124)

(cid:124)
Φt(xi− ) − Φs(ˆxi)

(cid:123)(cid:122)

Φt(xi+ ) + α]+

(cid:125)

(cid:124)
Φt(xi)

Φt(xj)1(yi (cid:54)= yj)

(cid:125)

i+ = arg maxj|yj =ˆyi

Φt(xj)

and

Cyclic Consistency
(cid:124)
Φs(ˆxi)

Structured Consistency
(cid:124)

= arg maxj|yj(cid:54)=ˆyi

Φs(ˆxi)

Φt(xj)

(1)
where 1(a) is an indicator function which is 1 if a is true and 0 otherwise. [a]+ is a rectiﬁer function
which is equal to max(0  a).

(cid:88)

(cid:88)

i∈[N u]

xj∈N (xi)

(cid:123)(cid:122)

+ λ

(cid:124)

−
i

3

O
O
/
/
We solve this optimization problem via alternating minimization through iterating over solving for
unsupervised labels yi(transduction) and learning the similarity metric θc  θs  θt (adaptation). We
explain these two steps in detail in the following sections.

3.3 Transduction: Labeling Target Domain

ky(xi)

In order to label the unsupervised points  we base our model on the k-nearest-neighbor rule. We
simply compute the k-NN supervised data point for each unsupervised data point using the learned
metric and transfer the corresponding majority label. Formally  given a similarity metric θc  θs  θt 
the k-NN rule is (yi)pred = arg maxy
k where ky(xi) is the number of samples having label y
in the k nearest neighbors of xi from the source domain. One major issue with this approach is the
inaccuracy of transduction during the initial stage of the algorithm. Since the learned metric will not
be accurate  we expect to see some noisy k-NN sets. Hence  we propose two solutions to solve this
problem.
Structured Consistency: Similar to existing graph transduction algorithms [4  36]  we create a
k-nearest neighbor (k-NN) graph over the unsupervised data points and penalize disagreements of
labels between neighbors.
Reject option: In the initial stage of the algorithm  we let the transduction step use the reject R as
an additional label (besides the class labels) to label the unsupervised target points. In other words 
our transduction algorithm can decide to not label (reject) some of the points so that they will not be
used for adaptation. As the learned metric gets more accurate in the future iterations  transduction
algorithm can change the label from R to other class labels.
Using aforementioned heuristics  we deﬁne our transduction sub-problem as1:

(cid:124)
Φt(xi)

Φt(xj)1(yi (cid:54)= yj)

(2)

(cid:88)

(cid:88)

(cid:88)

i∈[N u]

xj∈N (xi)

min

y1 ...yN u∈Y∪R

l(xi  yi) + λ

(cid:40)

i∈[N u]
1 − ky(xi)
γ maxy(cid:48)∈Y k(cid:48)

k

y(xi)

k

where l(xi  y) =

y ∈ Y
y = R

and γ is relative cost of the reject option.

The l(xi  R) is smaller if none of the class has a majority  promoting the reject option for undecided
cases. We also modulate the γ during learning to decrease number of reject options in the later stage
of the adaptation. This problem can approximately be solved using many existing methods. We use
the α-β swapping algorithm from [5] since it is experimentally shown to be efﬁcient and accurate.

3.4 Adaptation: Learning the Metric

Given the predicted labels yi for unsupervised data points xi  we can then learn a metric in order
to minimize the loss function deﬁned in (1). Following the cyclic consistency construction  the
LMNN rule can be represented using the triplet loss deﬁned between the supervised source data
points and their nearest positive and negative neighbors among the unsupervised target points. We do
not include the target-data points with reject labels during this construction. Formally  we can deﬁne
the adaptation problem given unsupervised labels as;

[Φs(ˆxi)

(cid:124)

Φt(xi− ) − Φs(ˆxi)

(cid:124)

Φt(xi+ ) + α]+ + λ

Φt(xi)

(cid:124)

Φt(xj)1(yi (cid:54)= yj)

(cid:88)

(cid:88)

i∈[N u]

xj∈N (xi)

(cid:88)

i∈[N s]

min

θc θs θt

where

i+ = arg maxj|yj =ˆyiΦs(ˆxi)

(cid:124)

Φt(xj)

(cid:124)
and i− = arg maxj|yj(cid:54)=ˆyi yj(cid:54)=RΦs(ˆxi)

Φt(xj)

(3)

(4)

We optimize this function via stochastic gradient descent using the sub-gradients ∂loss
∂θs
∂loss
∂θc

and
. These sub-gradients can be efﬁciently computed with back-propagation (see [1] for details).

  ∂loss
∂θt

1The subproblem we deﬁne here does not directly correspond to optimization of (1) with respect to
y1  . . . yN u. It is extension of the exact sub-problem by replacing 1-NN rule with k-NN rule and introducing
reject option.

4

3.5

Implementation Details

We use Alexnet [17] and LeNet [18] architectures with small modiﬁcations. We remove their ﬁnal
softmax layer and change the size of the ﬁnal fully connected layer according to the desired feature
dimension. We consider the last fully connected layer as domain speciﬁc (θs  θt) and the rest as
common network θc. Common network weights are tied between domains  and the ﬁnal layers are
learned separately. In order to have a fair comparison  we use the same architectures from [11] only
modifying the embedding size. (See supplementary material [1] for details).
Since the ofﬁce dataset is quite small  we do not
learn the network from scratch for ofﬁce experi-
ments and instead we initialize with the weights
pre-trained on ImageNet. In all of our experi-
ments  we set the feature dimension as 128. We
use stochastic gradient descent to learn the fea-
ture function with AdaGrad[8]. We initialize
convolutional weights with truncated normals
having std-dev 0.1  biases with constant value
0.1  and use a learning rate of 2.5 × 10−4 with
batch size 512. We start the rejection penalty
with γ = 0.1 and linearly increase with each
epoch as γ = #epoch−1
+ 0.1. In our experi-
ments  we use M = 20  λ = 0.001 and α = 1.

if ˆyi ∈ y1···B and ∃k yk ∈ Y \ ˆyi then
Compute (i+  i−) using {y1···B} in (4)
Update ∂loss
∂θc

Input: Source ˆx1···N s   ˆy1 ···N s  Target x1 ···  N u 
Batch Size 2 × B
for t = 0 to max_iter do

Sample {ˆx1 ... B  ˆy1 ... B}  {x1 ... B}
Solve (2) for {y1···B}
for i = 1 to B do

Algorithm 1 Transduction with Domain Shift

  ∂loss
∂θs

  ∂loss
∂θt

  θs ← θs + η(t) ∂loss

 

∂θs

M

end if
end for
η(t) ← Adagrad Rule [8]
θc ← θc + η(t) ∂loss
θt ← θt + η(t) ∂loss

∂θc

∂θt

end for

4 Experimental Results

We evaluate our algorithm on various unsupervised domain adaptation tasks while focusing on two
different problems: hand-written digit classiﬁcation and object recognition.

Datasets: We use MNIST [19]  Street View House Number [21] and the artiﬁcially generated version
of MNIST -MNIST-M- [11] to experiment our algorithm on the digit classiﬁcation task. MNIST-M is
simply a blend of the digit images of the original MNIST dataset and the color images of BSDS500 [2]
following the method explained in [11]. Since the dataset is not distributed directly by the authors 
we generated the dataset using the same procedure and further conﬁrmed that the performance is
the same as the one reported in [11]. Street View House Numbers is a collection of house numbers
collected from Google street view images. Each of these three domains are quite different from each
other. Among many important differences  the most signiﬁcant ones are MNIST being grayscale
whilw the others are colored  and SVHN images having extra confusing digits around the centered
digit of interest. Moreover  all domains are large-scale  having at least 60k examples over 10 classes.
In addition  we use the Ofﬁce [25] dataset to evaluate our algorithm on the object recognition task.
The ofﬁce dataset includes images of the objects taken from Amazon  captured with a webcam and
captured with a D-SLR. Differences between domains include the white background of Amazon
images versus realistic webcam images  and the resolution differences. The Ofﬁce dataset has fewer
images  with a maximum of 2478 per domain over 31 classes.

Baselines: We compare our method against a variety of methods with and without feature learning.
SA*[9] is the dominant state-of-the-art approach not employing any feature learning  and Back-
prop(BP)[11] is the dominant state-of-the-art employing feature learning. We use the available source
code of [11] and [9] and following the evaluation procedure in [11]  we choose the hyper-parameter
of [9] as the highest performing one among various alternatives. We also compare our method with
the source only baseline which is a convolutional neural network trained only using the source data.
This classiﬁer is clearly different from our nearest neighbor classiﬁer; however  we experimentally
validated that the CNN always outperformed the nearest neighbor based classiﬁer. Hence  we report
the highest performing source only method.

Evaluation: We evaluate all algorithms in a fully transductive setup [12]. We feed training images
and labels of ﬁrst domain as the source and training images of the second domain as the target. We
evaluate the accuracy on the target domain as the ratio of correctly labeled images to all target images.

5

4.1 Results

Following the fully transductive evaluation  we summarize the results in Table 1 and Table 2. Table 1
summarizes the results on the object recognition task using ofﬁce dataset whereas Table 2 summarizes
the digit classiﬁcation task on MNIST and SVHN.

Table 1: Accuracy of our method and the state-of-the-art algorithms on Ofﬁce dataset.

SOURCE AMAZON D-SLR WEBCAM WEBCAM AMAZON D-SLR
TARGET WEBCAM WEBCAM D-SLR AMAZON D-SLR AMAZON

GFK [12]
SA* [9]
DLID [6]
DDC [33]
DAN [20]
BACKPROP [11]
SOURCE ONLY
OUR METHOD (K-NN ONLY)
OUR METHOD (NO REJECT)
OUR METHOD (FULL)

.398
.450
.519
.618
.685
.730

.642
.727
.804
.811

.791
.648
.782
.950
.960
.964

.961
.952
.962
.964

.746
.699
.899
.985
.990
.992

.978
.915
.989
.992

.371
.393

-

.522
.531
.536

.452
.575
.625
.638

.379
.388

-

.644
.670
.728

.668
.791
.839
.841

.379
.420

-

.521
.540
.544

.476
.521
.567
.583

.593
.738
.549
.713
.774
.788

.211
.289

.162
.158
.323
.403

.569
.766

.522
.795
.855
.867

.523
.732
SOURCE ONLY .483
.805
.835
.839

OUR METHOD(K-NN ONLY)
OUR METHOD(NO REJECT)
OUR METHOD(FULL)

Table 2: Accuracy on the digit classiﬁcation task.

SOURCE M-M MNIST SVHN MNIST
TARGET MNIST M-M MNIST SVHN
SA* [9]
BP [11]

Tables 1&2 show results on object recogni-
tion and digit classiﬁcation tasks covering
all adaptation scenarios. Our experiments
show that our proposed method outperforms
all state-of-the-art algorithms. Moreover  the
increase in the accuracy is rather signiﬁ-
cant when there is a large domain difference
such as MNIST↔MNIST-M  MNIST↔SVHN 
Amazon↔Webcam and Amazon↔D-SLR. Our
hypothesis is that the state-of-the-art algorithms
such as [11] are seeking features invariant to the domains whereas we seek an explicit similarity
metric explaining both differences and similarities of domains. In other words  instead of seeking an
invariance  we seek an equivariance.
Table 2 further suggests that our algorithm is the only one which can successfully perform adaptation
from MNIST to SVHN. Clearly the features which are learned from MNIST cannot generalize to
SVHN since the SVHN has concepts like color and occlusion which are not available in MNIST.
Hence  our algorithm learns SVHN speciﬁc features by enforcing accurate transduction in the
adaptation.
Another interesting conclusion is the asymmetric results. For example  adapting webcam to Amazon
and adapting Amazon to webcam yield very different accuracies. The similar asymmetry exists in
MNIST and SVHN as well. This observation validates the importance of an asymmetric modeling.
To evaluate the importance of joint labelling and reject option  we compare our method with self
baselines. Our self-baselines are versions of our algorithm not using the reject option (no reject) and
the version using neither reject option nor joint labelling (k-NN only). Results on both experiments
suggest that joint labelling and the reject option are both crucial for successful transduction. Moreover 
the reject option is more important when the domain shift is large (e.g. MNIST→SVHN). This is
expected since transduction under a large shift is more likely to fail a situation that can be prevented
with reject option.

4.1.1 Qualitative Analysis

To further study the learned representations and the similarity metric  we performed a series of
qualitative analysis in the form of nearest neighbor and tSNE[34] plots.
Figure 1 visualizes example target images from MNIST and their corresponding source images. First
of all  our experimental analysis suggests that MNIST and SVHN are the two domains with the largest
difference. Hence  we believe MNIST↔SVHN is a very challenging set-up and despite the huge

6

1:

visual differences  our algorithm results in accurate nearest neighbors. On the other hand  Figure 2
visualizes the example target images from webcam and their corresponding nearest source images
from Amazon.
The difference between invariance and equivariance is
clearer in the tSNE plots of the Ofﬁce dataset in Figure 3
and the digit classiﬁcation task in Figure 4. In Figure 3  we
plot the distribution of features before and after adaptation
for source and target while color coding class labels. We
use the learned embeddings as output of Φs and Φt as an
input to tSNE algorithm[34]. As Figure 3 suggests  the
source domain is well clustered according to the object
classes with and without adaptation. This is expected
since the features are speciﬁcally ﬁne-tuned to the source
domain before the adaptation starts. However  the target
domain features have no structure before adaptation. This
is also expected since the algorithm did not see any image
from the target domain. After the adaptation  target images
also get clustered according to the object classes.
In Figure 4  we show the digit images of the source and
target after the adaptation. In order to see the effect of com-
mon features and domain speciﬁc features separately  we
compute the low-dimensional embeddings of the output
of the shared network (output of the ﬁrst fully connected
layer). We further compute the NN points between the
source and target using Φs and Φt  and draw an edge be-
tween NNs. Clearly  the target is well clustered according
to the classes and the source is not very well clustered
although it has some structure. Since we learn the en-
tire network for digit classiﬁcation  our networks learn
discriminative features in the target domain as our loss de-
pends directly on classiﬁcation scores in the target domain.
Moreover  discriminative features in the target arises be-
cause of the transductive modeling. In comparison  state
of the art domain invariance based algorithms only try to
be invariant to the domains without explicit modeling of
discriminative behavior on the target. Hence  our method
explicitly models the relationship between the domains
and results in an equivarient model while enforcing dis-
criminative behavior in the target.

Figure
Nearest
SVHN→MNIST exp.
example MNIST image and its 5-NNs.

neighbors
for
We show an

5 Conclusion

2:

for
Figure
Amazon↔Webcam exp. We show an
example Amazon image and its 3-NNs.

neighbors

Nearest

We described an end-to-end deep learning framework for jointly optimizing the optimal deep feature
representation  cross domain transformation  and the target label inference for state of the art
unsupervised domain adaptation.
Experimental results on digit classiﬁcation using MNIST[19] and SVHN[21] as well as on object
recognition using the Ofﬁce[25] dataset show state of the art performance with a signiﬁcant margin.

Acknowledgments

We acknowledge the support of ONR-N00014-13-1-0761  MURI - WF911NF-15-1-0479 and Toyota
Center grant 1191689-1-UDAWF.

7

(a) S. w/o Adaptation

(c) T w/o Adaptation

(d) T with Adaptation
Figure 3: tSNE plots for ofﬁce dataset Webcam(S)→Amazon(T). Source features were discriminative and stayed
discriminative as expected. On the other hand  target features became quite discriminative after the adaptation.

(b) S. with Adaptation

Figure 4: tSNE plot for SVHN→MNIST experiment. Please note that the discriminative behavior only emerges
in the unsupervised target instead of the source domain. This explains the motivation behind modeling the
problem as transduction. In other words  our algorithm is designed to be accurate and discriminative in the target
domain which is the domain we are interested in.

8

References
[1] Supplementary details for the paper. http://cvgl.stanford.edu/transductive_adaptation.
[2] P. Arbelaez  M. Maire  C. Fowlkes  and J. Malik. Contour detection and hierarchical image segmentation.

T-PAMI  33:898–916  2011.

[3] C. Berlind and R. Urner. Active nearest neighbors in changing environments. In ICML  2015.
[4] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML  2001.
[5] Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. T-PAMI  26:1124–1137  2004.

[6] S. Chopra  S. Balakrishnan  and R. Gopalan. Dlid: Deep learning for domain adaptation by interpolating

between domains. In ICML W  2013.

[7] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. Imagenet: A large-scale hierarchical image

database. In CVPR  2009.

[8] J. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and stochastic

optimization. JMLR  pages 2121–2159  2011.

[9] B. Fernando  A. Habrard  M. Sebban  and T. Tuytelaars. Unsupervised visual domain adaptation using

subspace alignment. In ICCV  2013.

[10] A. Gammerman  V. Vovk  and V. Vapnik. Learning by transduction. In UAI  1998.
[11] Y. Ganin and V. S. Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML  2015.
[12] B. Gong  K. Grauman  and F. Sha. Connecting the dots with landmarks: Discriminatively learning

domain-invariant features for unsupervised domain adaptation. In ICML  2013.

[13] B. Gong  Y. Shi  F. Sha  and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In

CVPR  2012.

[14] A. Karpathy  G. Toderici  S. Shetty  T. Leung  R. Sukthankar  and L. Fei-Fei. Large-scale video classiﬁcation

with convolutional neural networks. In CVPR  2014.

[15] S. Khamis and C. Lampert. Coconut: Co-classiﬁcation with output space regularization. In BMVC  2014.
[16] E. Kodirov  T. Xiang  Z. Fu  and S. Gong. Domain adaptation for zero-shot learning. In ICCV  2015.
[17] A. Krizhevsky  I. Sutskever  and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural

networks. In NIPS  2012.

[18] Y. LeCun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE  86:2278–2324  1998.

[19] Y. LeCun  C. Cortes  and C. J. Burges. The mnist database of handwritten digits  1998.
[20] M. Long  C. Yue  J. Wang  and J. Michael. Learning transferable features with deep adaptation networks.

arXiv  2015.

[21] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading digits in natural images with

unsupervised feature learning. In NIPS W  2011.

[22] M. Palatucci  D. Pomerleau  G. E. Hinton  and T. M. Mitchell. Zero-shot learning with semantic output

codes. In NIPS  2009.

[23] R. Raina  A. Battle  H. Lee  B. Packer  and A. Y. Ng. Self-taught learning: transfer learning from unlabeled

data. In ICML. ACM  2007.

[24] M. Rohrbach  S. Ebert  and B. Schiele. Transfer learning in a transductive setting. In NIPS  2013.
[25] K. Saenko  B. Kulis  M. Fritz  and T. Darrell. Adapting visual category models to new domains. In ECCV 

pages 213–226. Springer  2010.

[26] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.

CoRR  abs/1409.1556  2014.

[27] V. Sindhwani  P. Niyogi  and M. Belkin. Beyond the point cloud: from transductive to semi-supervised

learning. In ICML  2005.

[28] B. Sun  J. Feng  and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI  2016.
[29] B. Sun and K. Saenko. Subspace alignment for unsupervised domain adaptation. In BMVC  2015.
[30] C. Szegedy  W. Liu  Y. Jia  P. Sermanet  S. Reed  D. Anguelov  D. Erhan  V. Vanhoucke  and A. Rabinovich.

Going deeper with convolutions. arXiv:1409.4842  2014.

[31] S. Thrun and L. Pratt. Learning to learn. Springer Science & Business  2012.
[32] T. Tommasi and B. Caputo. Frustratingly easy NBNN domain adaptation. In ICCV  2013.
[33] E. Tzeng  J. Hoffman  N. Zhang  K. Saenko  and T. Darrell. Deep domain confusion: Maximizing for

domain invariance. arXiv:1412.3474  2014.

[34] L. van der maaten. Accelerating t-sne using tree-based algorithms. In JMLR  2014.
[35] K. Q. Weinberger  J. Blitzer  and L. K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. In NIPS  2006.

[36] X. Zhu and Z. Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002.
[37] X. Zhu  Z. Ghahramani  J. Lafferty  et al. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In ICML  2003.

9

,David Carlson
Jana Schaich Borg
Kafui Dzirasa
Lawrence Carin
Ozan Sener
Hyun Oh Song
Ashutosh Saxena
Silvio Savarese