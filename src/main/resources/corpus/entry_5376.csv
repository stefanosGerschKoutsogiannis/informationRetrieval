2010,Switched Latent Force Models for Movement Segmentation,Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model  that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives  but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.,Switched Latent Force Models
for Movement Segmentation

Mauricio A. ´Alvarez 1  Jan Peters 2  Bernhard Sch¨olkopf 2  Neil D. Lawrence 3 4
1 School of Computer Science  University of Manchester  Manchester  UK M13 9PL

2 Max Planck Institute for Biological Cybernetics  T¨ubingen  Germany 72076
3 School of Computer Science  University of Shefﬁeld  Shefﬁeld  UK S1 4DP
4 The Shefﬁeld Institute for Translational Neuroscience  Shefﬁeld  UK S10 2HQ

Abstract

Latent force models encode the interaction between multiple related dynamical
systems in the form of a kernel or covariance function. Each variable to be mod-
eled is represented as the output of a differential equation and each differential
equation is driven by a weighted sum of latent functions with uncertainty given
by a Gaussian process prior. In this paper we consider employing the latent force
model framework for the problem of determining robot motor primitives. To deal
with discontinuities in the dynamical systems or the latent driving force we intro-
duce an extension of the basic latent force model  that switches between different
latent functions and potentially different dynamical systems. This creates a ver-
satile representation for robot movements that can capture discrete changes and
non-linearities in the dynamics. We give illustrative examples on both synthetic
data and for striking movements recorded using a Barrett WAM robot as haptic in-
put device. Our inspiration is robot motor primitives  but we expect our model to
have wide application for dynamical systems including models for human motion
capture data and systems biology.

Introduction

1
Latent force models [1] are a new approach for modeling data that allows combining dimensionality
reduction with systems of differential equations. The basic idea is to assume an observed set of
D correlated functions to arise from an unobserved set of R forcing functions. The assumption is
that the R forcing functions drive the D observed functions through a set of differential equation
models. Each differential equation is driven by a weighted mix of latent forcing functions. Sets
of coupled differential equations arise in many physics and engineering problems particularly when
the temporal evolution of a system needs to be described. Learning such differential equations has
important applications  e.g.  in the study of human motor control and in robotics [6]. A latent force
model differs from classical approaches as it places a probabilistic process prior over the latent
functions and hence can make statements about the uncertainty in the system. A joint Gaussian
process model over the latent forcing functions and the observed data functions can be recovered
using a Gaussian process prior in conjunction with linear differential equations [1]. The resulting
latent force modeling framework allows the combination of the knowledge of the systems dynamics
with a data driven model. Such generative models can be used to good effect  for example in ranked
target prediction for transcription factors [5].
If a single Gaussian process prior is used to represent each latent function then the models we con-
sider are limited to smooth driving functions. However  discontinuities and segmented latent forces
are omnipresent in real-world data. For example  impact forces due to contacts in a mechanical
dynamical system (when grasping an object or when the feet touch the ground) or a switch in an
electrical circuit result in discontinuous latent forces. Similarly  most non-rhythmic natural mo-

1

tor skills consist of a sequence of segmented  discrete movements. If these segments are separate
time-series  they should be treated as such and not be modeled by the same Gaussian process model.
In this paper  we extract a sequence of dynamical systems motor primitives modeled by second
order linear differential equations in conjunction with forcing functions (as in [1  6]) from human
movement to be used as demonstrations of elementary movements for an anthropomorphic robot.
As human trajectories have a large variability: both due to planned uncertainty of the human’s
movement policy  as well as due to motor execution errors [7]  a probabilistic model is needed to
capture the underlying motor primitives. A set of second order differential equations is employed
as mechanical systems are of the same type and a temporal Gaussian process prior is used to allow
probabilistic modeling [1]. To be able to obtain a sequence of dynamical systems  we augment the
latent force model to include discontinuities in the latent function and change dynamics. We intro-
duce discontinuities by switching between different Gaussian process models (superﬁcially similar
to a mixture of Gaussian processes; however  the switching times are modeled as parameters so that
at any instant a single Gaussian process is driving the system). Continuity of the observed functions
is then ensured by constraining the relevant state variables (for example in a second order differential
equation velocity and displacement) to be continuous across the switching points. This allows us
to model highly non stationary multivariate time series. We demonstrate our approach on synthetic
data and real world movement data.

2 Review of Latent force models (LFM)
Latent force models [1] are hybrid models that combine mechanistic principles and Gaussian pro-
cesses as a ﬂexible way to introduce prior knowledge for data modeling. A set of D functions
{yd(t)}D
d=1 is modeled as the set of output functions of a series of coupled differential equations 
whose common input is a linear combination of R latent functions  {ur(t)}R
r=1. Here we focus on a
second order ordinary differential equation (ODE). We assume the output yd(t) is described by

Ad

d2yd(t)
dt2 + Cd

dyd(t)

dt

+ κdyd(t) =(cid:80)R

r=1Sd rur(t) 

where  for a mass-spring-damper system  Ad would represent the mass  Cd the damper and κd  the
spring constant associated to the output d. We refer to the variables Sd r as the sensitivity parameters.
They are used to represent the relative strength that the latent force r exerts over the output d. For
simplicity we now focus on the case where R = 1  although our derivations apply more generally.
Note that models that learn a forcing function to drive a linear system have proven to be well-suited
for imitation learning for robot systems [6]. The solution of the second order ODE follows

initial conditions (IC). The angular frequency is given by ωd = (cid:112)(4Adκd − C 2

(1)
where yd(0) and ˙yd(0) are the output and the velocity at time t = 0  respectively  known as the
d) and the

yd(t) = yd(0)cd(t) + ˙yd(0)ed(t) + fd(t  u) 

d)/(4A2

remaining variables are given by

cd(t) = e−αdt(cid:104)
(cid:90) t

fd(t  u) = Sd
Adωd

0

cos(ωdt) + αd
ωd

sin(ωdt)
Gd(t − τ)u(τ)dτ = Sd
Adωd

(cid:105)
(cid:90) t

 

0

ed(t) = e−αdt
e−αd(t−τ ) sin[(t − τ)ωd]u(τ)dτ 

sin(ωdt) 

ωd

with αd = Cd/(2Ad). Note that fd(t  u) has an implicit dependence on the latent function u(t). The
uncertainty in the model of Eq. (1) is due to the fact that the latent force u(t) and the initial conditions
yd(0) and ˙yd(0) are not known. We will assume that the latent function u(t) is sampled from a zero
mean Gaussian process prior  u(t) ∼ GP(0  ku u(t  t(cid:48)))  with covariance function ku u(t  t(cid:48)).
If the initial conditions  yIC = [y1(0)  y2(0)  . . .   yD(0)  v1(0)  v2(0)  . . .   vD(0)](cid:62)  are indepen-
dent of u(t) and distributed as a zero mean Gaussian with covariance KIC the covariance function
between any two output functions  d and d(cid:48) at any two times  t and t(cid:48)  kyd yd(cid:48) (t  t(cid:48)) is given by
cd(t)cd(cid:48)(t(cid:48))σyd yd(cid:48) + cd(t)ed(cid:48)(t(cid:48))σyd vd(cid:48) + ed(t)cd(cid:48)(t(cid:48))σvd yd(cid:48) + ed(t)ed(cid:48)(t(cid:48))σvd vd(cid:48) + kfd fd(cid:48) (t  t(cid:48)) 
where σyd yd(cid:48)   σyd vd(cid:48)   σvd yd(cid:48) and σvd vd(cid:48) are entries of the covariance matrix KIC and

kfd fd(cid:48) (t  t(cid:48)) = K0

0 Gd(cid:48)(t(cid:48) − τ(cid:48))ku u(t  t(cid:48))dτ(cid:48)dτ 

(2)

(cid:82) t
0Gd(t − τ)(cid:82) t(cid:48)

2

where K0 = SdSd(cid:48)/(AdAd(cid:48)ωdωd(cid:48)). So the covariance function kfd fd(cid:48) (t  t(cid:48)) depends on the covari-
ance function of the latent force u(t). If we assume the latent function has a radial basis function
(RBF) covariance  ku u(t  t(cid:48)) = exp[−(t − t(cid:48))2/(cid:96)2]  then kfd fd(cid:48) (t  t(cid:48)) can be computed analyti-
cally [1] (see also supplementary material). The latent force model induces a joint Gaussian process
model across all the outputs. The parameters of the covariance function are given by the parameters
of the differential equations and the length scale of the latent force. Given a multivariate time series
data set these parameters may be determined by maximum likelihood.
The model can be thought of as a set of mass-spring-dampers being driven by a function sampled
from a Gaussian process. In this paper we look to extend the framework to the case where there can
be discontinuities in the latent functions. We do this through switching between different Gaussian
process models to drive the system.

zd(t) = yq

d(tq−1) + eq

d(t − tq−1) ˙yq

˙zd(t) = ˙yq
where gd(t) = −e−αdt sin(ωdt)(α2

d(t − tq−1) = gq
(cid:20) αd

d(t − tq−1)yq
dω−1

hd(t) = −e−αdt

d(t − tq−1) = cq

3 Switching dynamical latent force models (SDLFM)
We now consider switching the system between different latent forces. This allows us to change the
dynamical system and the driving force for each segment. By constraining the displacement and
velocity at each switching time to be the same  the output functions remain continuous.
3.1 Deﬁnition of the model
We assume that the input space is divided in a series of non-overlapping intervals [tq−1  tq]Q
q=1.
During each interval  only one force uq−1(t) out of Q forces is active  that is  there are {uq−1(t)}Q
q=1
forces. The force uq−1(t) is activated after time tq−1 (switched on) and deactivated (switched off)
after time tq. We can use the basic model in equation (1) to describe the contribution to the output
due to the sequential activation of these forces. A particular output zd(t) at a particular time instant
t  in the interval (tq−1  tq)  is expressed as
d(t − tq−1)yq
d (t − tq−1  uq−1).
d(tq−1) + f q
This equation is assummed to be valid for describing the output only inside the interval (tq−1  tq).
d(t − tq−1) to represent the interval
Here we highlighted this idea by including the superscript q in yq
q for which the equation holds  although later we will omit it to keep the notation uncluttered. Note
that for Q = 1 and t0 = 0  we recover the original latent force model given in equation (1). We also
deﬁne the velocity ˙zd(t) at each time interval (tq−1  tq) as
d(t − tq−1  uq−1) 
(cid:21)
(cid:19)
sin(ωdt) − cos(ωdt)
Given the parameters θ = {{Ad  Cd  κd  Sd}D
}  the uncertainty in the outputs is
induced by the prior over the initial conditions yq
d(tq−1) for all values of tq−1 and the
prior over latent force uq−1(t) that is active during (tq−1  tq). We place independent Gaussian
process priors over each of these latent forces uq−1(t)  assuming independence between them.
d(tq−1)  ˙yq
For initial conditions yq
d(tq−1)  we could assume that they are either parameters to
be estimated or random variables with uncertainty governed by independent Gaussian distribu-
tions with covariance matrices K q
IC as described in the last section. However  for the class
of applications we will consider: mechanical systems  the outputs should be continuous across
the switching points. We therefore assume that the uncertainty about the initial conditions
for the interval q  yq
d(tq−1) are proscribed by the Gaussian process that describes the
outputs zd(t) and velocities ˙zd(t) in the previous interval q − 1.
In particular  we assume
(tq−1 − tq−2) and
yq
d(tq−1)  ˙yq
(tq−1 − tq−2) and covariances kzd zd(cid:48) (tq−1  tq(cid:48)−1) = cov[yq−1
(tq−1 −
(tq−1 − tq−2)  yq−1
˙yq−1
d(cid:48)
(tq−1 − tq−2)]. We also consider
tq−2)] and k ˙zd  ˙zd(cid:48) (tq−1  tq(cid:48)−1) = cov[ ˙yq−1
covariances between zd(tq−1) and ˙zd(cid:48)(tq(cid:48)−1)  this is  between positions and velocities for different
values of q and d.
Example 1. Let us assume we have one output (D = 1) and three switching intervals (Q = 3)
with switching points t0  t1 and t2. At t0  we assume that yIC follows a Gaussian distribution with

d(tq−1) are Gaussian-distributed with mean values given by yq−1

  md(t) = Sd
Adωd
d=1 {(cid:96)q−1}Q
d(tq−1)  ˙yq

(tq−1 − tq−2)  ˙yq−1
d(cid:48)

Gd(t − τ)u(τ)dτ

.

(cid:18)(cid:90) t

d(tq−1) + hq

d(t − tq−1) ˙yq

d(tq−1) + mq

d + ωd) and

d

d

d
dt

0

q=1

ωd

d(tq−1)  ˙yq

d

d

3

mean zero and covariance KIC. From t0 to t1  the output z(t) is described by

z(t) = y1(t − t0) = c1(t − t0)y1(t0) + e1(t − t0) ˙y1(t0) + f 1(t − t0  u0).

The initial condition for the position in the interval (t1  t2) is given by the last equation evaluated a
t1  this is  z(t1) = y2(t1) = y1(t1 − t0). A similar analysis is used to obtain the initial condition
associated to the velocity  ˙z(t1) = ˙y2(t1) = ˙y1(t1 − t0). Then  from t1 to t2  the output z(t) is

z(t) = y2(t − t1) = c2(t − t1)y2(t1) + e2(t − t1) ˙y2(t1) + f 2(t − t1  u1) 

= c2(t − t1)y1(t1 − t0) + e2(t − t1) ˙y1(t1 − t0) + f 2(t − t1  u1).

Following the same train of thought  the output z(t) from t2 is given as

z(t) = y3(t − t2) = c3(t − t2)y3(t2) + e3(t − t2) ˙y3(t2) + f 3(t − t2  u2) 

where y3(t2) = y2(t2 − t1) and ˙y3(t2) = ˙y2(t2 − t1). Figure 1 shows an example of the switching
dynamical latent force model scenario. To ensure the continuity of the outputs  the initial condition
is forced to be equal to the output of the last interval evaluated at the switching point.

3.2 The covariance function

The derivation of the co-
variance function for the
switching model is rather
involved.
For contin-
uous output signals  we
must take into account con-
straints at each switching
time. This causes initial
conditions for each inter-
val to be dependent on ﬁnal
conditions for the previous
interval and induces cor-
relations across the inter-
vals. This effort is worth-
while though as the result-
ing model is very ﬂexible
and can take advantage of
the switching dynamics to represent a range of signals.
As a taster  Figure 2 shows samples from a covariance function of a switching dynamical latent
force model with D = 1 and Q = 3. Note that while the latent forces (a and c) are discrete 
the outputs (b and d) are continuous and have matching gradients at the switching points. The
outputs are highly nonstationary. The switching times turn out to be parameters of the covariance
function. They can be optimized along with the dynamical system parameters to match the location
of the nonstationarities. We now give an overview of the covariance function derivation. Details are
provided in the supplementary material.

Figure 1: Representation of an output constructed through a switching dynam-
ical latent force model with Q = 3. The initial conditions yq(tq−1) for each
interval are matched to the value of the output in the last interval  evaluated at
the switching point tq−1  this is  yq(tq−1) = yq−1(tq−1 − tq−2).

(a) System 1.
from the latent force.

Samples

(b) System 1.
from the output.

Samples

(c) System 2.
from the latent force.

Samples

(d) System 2.
from the output.

Samples

Figure 2: Joint samples of a switching dynamical LFM model with one output  D = 1  and three intervals 
Q = 3  for two different systems. Dashed lines indicate the presence of switching points. While system 2
responds instantaneously to the input force  system 1 delays its reaction due to larger inertia.

4

y1(t−t0)y2(t−t1)y3(t−t2)y1(t0)y1(t1−t0)y2(t1)y2(t2−t1)y3(t2)z(t)t0t1t20246810−4−3−2−1012340246810−10−505100246810−3−2−101230246810−6−4−20246In general  we need to compute the covariance kzd zd(cid:48) (t  t(cid:48)) = cov[zd(t)  zd(cid:48)(t(cid:48))] for zd(t) in time
interval (tq−1  tq) and zd(cid:48)(t(cid:48)) in time interval (tq(cid:48)−1  tq(cid:48)). By deﬁnition  this covariance follows

cov[zd(t)  zd(cid:48)(t(cid:48))] = cov(cid:2)yq

d(cid:48)(t − tq(cid:48)−1))(cid:3).

d(t − tq−1)  yq(cid:48)

We assumme independence between the latent forces uq(t) and independence between the initial
conditions yIC and the latent forces uq(t).1 With these conditions  it can be shown2 that the covari-
ance function3 for q = q(cid:48) is given as
d(cid:48)(t(cid:48) − tq−1)kzd  ˙zd(cid:48) (tq−1  tq−1)
d(cid:48)(t(cid:48) − tq−1)k ˙zd  ˙zd(cid:48) (tq−1  tq−1)
(3)

d(cid:48)(t(cid:48) − tq−1)kzd zd(cid:48) (tq−1  tq−1) + cq
d(cid:48)(t(cid:48) − tq−1)k ˙zd zd(cid:48) (tq−1  tq−1) + eq

d(t − tq−1)cq
cq
d(t − tq−1)cq
+eq

d(t − tq−1)eq
d(t − tq−1)eq

+kq

fd fd(cid:48) (t  t(cid:48)) 

where
kzd zd(cid:48) (tq−1  tq−1) = cov[yq
k ˙zd zd(cid:48) (tq−1  tq−1) = cov[ ˙yq

d(tq−1)yq
d(tq−1)yq

d(cid:48)(tq−1)] 
d(cid:48)(tq−1)] 
fd fd(cid:48) (t  t(cid:48)) = cov[f q
kq

kzd  ˙zd(cid:48) (tq−1  tq−1) = cov[yq
k ˙zd  ˙zd(cid:48) (tq−1  tq−1) = cov[ ˙yq

d(tq−1) ˙yq
d(tq−1) ˙yq

d(cid:48)(tq−1)] 
d(cid:48)(tq−1)].

d (t − tq−1)f q

d(cid:48)(t(cid:48) − tq−1)].
(tq−1 − tq−2)  yq−1
d(cid:48)

d

(tq−1 − tq−2)] and values
In expression (3)  kzd zd(cid:48) (tq−1  tq−1) = cov[yq−1
for kzd  ˙zd(cid:48) (tq−1  tq−1)  k ˙zd zd(cid:48) (tq−1  tq−1) and k ˙zd  ˙zd(cid:48) (tq−1  tq−1) can be obtained by similar ex-
fd fd(cid:48) (t  t(cid:48)) follows a similar expression that the one for kfd fd(cid:48) (t  t(cid:48)) in
pressions. The covariance kq
equation (2)  now depending on the covariance kuq−1 uq−1(t  t(cid:48)). We will assume that the covari-
ances for the latent forces follow the RBF form  with length-scale (cid:96)q.
When q > q(cid:48)  we have to take into account the correlation between the initial conditions yq
d(tq−1) 
d(tq−1) and the latent force uq(cid:48)−1(t(cid:48)). This correlation appears because of the contribution of
˙yq
uq(cid:48)−1(t(cid:48)) to the generation of the initial conditions  yq
d(tq−1). It can be shown4 that the
covariance function cov[zd(t)  zd(cid:48)(t(cid:48))] for q > q(cid:48) follows
d(cid:48)(t(cid:48) − tq(cid:48)−1)kzd  ˙zd(cid:48) (tq−1  tq(cid:48)−1)
d(t − tq−1)cq(cid:48)
d(cid:48)(t(cid:48) − tq(cid:48)−1)kzd zd(cid:48) (tq−1  tq(cid:48)−1) + cq
cq
d(t − tq−1)cq(cid:48)
d(cid:48)(t(cid:48) − tq(cid:48)−1)k ˙zd zd(cid:48) (tq−1  tq(cid:48)−1) + eq
d(cid:48)(t(cid:48) − tq(cid:48)−1)k ˙zd  ˙zd(cid:48) (tq−1  tq(cid:48)−1)
+eq
d(t − tq−1)X 1
d kq(cid:48)
d kq(cid:48)
fd fd(cid:48) (tq(cid:48)−1  t(cid:48)) + cq
md fd(cid:48) (tq(cid:48)−1  t(cid:48))
+cq
d(t − tq−1)X 3
d kq(cid:48)
d kq(cid:48)
fd fd(cid:48) (tq(cid:48)−1  t(cid:48)) + eq
md fd(cid:48) (tq(cid:48)−1  t(cid:48)) 

d(tq−1)  ˙yq
d(t − tq−1)eq(cid:48)
d(t − tq−1)eq(cid:48)
d(t − tq−1)X 2
d(t − tq−1)X 4

+eq

(4)

kzd  ˙zd(cid:48) (tq−1  tq(cid:48)−1) = cov[yq
k ˙zd  ˙zd(cid:48) (tq−1  tq(cid:48)−1) = cov[ ˙yq

d(tq−1) ˙yq(cid:48)
d(tq−1) ˙yq(cid:48)

d(cid:48)(tq(cid:48)−1)] 
d(cid:48)(tq(cid:48)−1)] 

where
kzd zd(cid:48) (tq−1  tq(cid:48)−1) = cov[yq
k ˙zd zd(cid:48) (tq−1  tq(cid:48)−1) = cov[ ˙yq

d(tq−1)yq(cid:48)
d(tq−1)yq(cid:48)

d(cid:48)(tq(cid:48)−1)] 
d(cid:48)(tq(cid:48)−1)] 
md fd(cid:48) (t  t(cid:48)) = cov[mq
kq
d and X 4

d(t − tq−1)f q
d are functions of the form (cid:80)q−q(cid:48)

d(cid:48)(t(cid:48) − tq−1)] 
(cid:81)q−q(cid:48)
i=2 xq−i+1

d

d   X 3

d   X 2
being equal to cq−i+1

and X 1
xq−i+1
A similar expression to (4) can be obtained for q(cid:48) > q. Examples of these functions for speciﬁc
values of q and q(cid:48) and more details are also given in the supplementary material.

  depending on the values of q and q(cid:48).

(tq−i+1 − tq−i)  with

or hq−i+1

  gq−i+1

  eq−i+1

n=2

d

d

d

d

d

4 Related work
There has been a recent interest in employing Gaussian processes for detection of change points in
time series analysis  an area of study that relates to some extent to our model. Some machine learning
related papers include [3  4  9]. [3  4] deals speciﬁcally with how to construct covariance functions

1Derivations of these equations are rather involved. In the supplementary material  section 2  we include a

detailed description of how to obtain the equations (3) and (4)

2See supplementary material  section 2.2.1.
d (t − tq−1  uq−1) as f q
3We will write f q
4See supplementary material  section 2.2.2

d (t − tq−1) for notational simplicity.

5

in the presence of change points (see [3]  section 4). The authors propose different alternatives
according to the type of change point. From these alternatives  the closest ones to our work appear
in subsections 4.2  4.3 and 4.4. In subsection 4.2  a mechanism to keep continuity in a covariance
function when there are two regimes described by different GPs  is proposed. The authors call this
covariance continuous conditionally independent covariance function. In our switched latent force
model  a more natural option is to use the initial conditions as the way to transit smoothly between
different regimes. In subsections 4.3 and 4.4  the authors propose covariances that account for a
sudden change in the input scale and a sudden change in the output scale. Both type of changes
are automatically included in our model due to the latent force model construction: the changes in
the input scale are accounted by the different length-scales of the latent force GP process and the
changes in the output scale are accounted by the different sensitivity parameters. Importantly  we
also concerned about multiple output systems.
On the other hand  [9] proposes an efﬁcient inference procedure for Bayesian Online Change Point
Detection (BOCPD) in which the underlying predictive model (UPM) is a GP. This reference is less
concerned about the particular type of change that is represented by the model: in our application
scenario  the continuity of the covariance function between two regimes must be assured beforehand.

Implementation

5
In this section  we describe additional details on the implementation  i.e.  covariance function  hy-
perparameters  sparse approximations.
Additional covariance functions. The covariance functions k ˙zd zd(cid:48) (t  t(cid:48))  kzd  ˙zd(cid:48) (t  t(cid:48)) and
k ˙zd  ˙zd(cid:48) (t  t(cid:48)) are obtained by taking derivatives of kzd zd(cid:48) (t  t(cid:48)) with respect to t and t(cid:48) [10].
Estimation of hyperparameters. Given the number of outputs D and the number of intervals
Q  we estimate the parameters θ by maximizing the marginal-likelihood of the joint Gaussian pro-
cess {zd(t)}D
n=1  the
marginal-likelihood is given as p(z|θ) = N (z|0  Kz z + Σ)  where z = [z(cid:62)
D](cid:62)  with
zd = [zd(t1)  . . .   zd(tN )](cid:62)  Kz z is a D × D block-partitioned matrix with blocks Kzd zd(cid:48) . The
entries in each of these blocks are evaluated using kzd zd(cid:48) (t  t(cid:48)). Furthermore  kzd zd(cid:48) (t  t(cid:48)) is com-
puted using the expressions (3)  and (4)  according to the relative values of q and q(cid:48).
Efﬁcient approximations Optimizing the marginal likelihood involves the inversion of the ma-
trix Kz z  inversion that grows with complexity O(D3N 3). We use a sparse approximation based
on variational methods presented in [2] as a generalization of [11] for multiple output Gaussian
processes. The approximations establish a lower bound on the marginal likelihood and reduce com-
putational complexity to O(DN K 2)  being K a reduced number of points used to represent u(t).

d=1 using gradient-descent methods. With a set of input points  t = {tn}N
1   . . .   z(cid:62)

6 Experimental results

We now show results with artiﬁcial data and data recorded from a robot performing a basic set of
actions appearing in table tennis.

6.1 Toy example

Using the model  we generate samples from the GP with covariance function as explained before.
In the ﬁrst experiment  we sample from a model with D = 2  R = 1 and Q = 3  with switching
points t0 = −1  t1 = 5 and t2 = 12. For the outputs  we have A1 = A2 = 0.1  C1 = 0.4  C2 = 1 
κ1 = 2  κ2 = 3. We restrict the latent forces to have the same length-scale value (cid:96)0 = (cid:96)1 = (cid:96)2 =
1e−3  but change the values of the sensitivity parameters as S1 1 = 10  S2 1 = 1  S1 2 = 10  S2 2 =
5  S1 3 = −10 and S2 3 = 1  where the ﬁrst subindex refers to the output d and the second subindex
refers to the force in the interval q. In this ﬁrst experiment  we wanted to show the ability of the
model to detect changes in the sensitivities of the forces  while keeping the length scales equal along
the intervals. We sampled 5 times from the model with each output having 500 data points and add
some noise with variance equal to ten percent of the variance of each sampled output. In each of the
ﬁve repetitions  we took N = 200 data points for training and the remaining 300 for testing.

6

Q = 1
76.27±35.63
MSLL −0.98±0.46
7.27±6.88
MSLL −1.79±0.28

1 SMSE
2 SMSE

Q = 2

Q = 3
Q = 5
0.72±0.56
0.30±0.02
14.66±11.74
−1.79±0.26
−2.90±0.03 −2.87±0.04 −2.55±0.41
1.10±0.05
1.10±0.09
1.08±0.05
−2.26±0.02 −2.25±0.02 −2.27±0.03 −2.26±0.06

Q = 4
0.31±0.03
1.06±0.05

Table 1: Standarized mean square error (SMSE) and mean standardized log loss (MSLL) using different values
of Q for both toy examples. The ﬁgures for the SMSE must be multiplied by 10−2. See the text for details.
(c) Output 2 toy example 1.
(a) Latent force toy example 1.

(b) Output 1 toy example 1.

(d) Latent force toy example 2.

(e) Output 1 toy example 2.

(f) Output 3 toy example 2.

Figure 4: Mean and two standard deviations for the predictions over the latent force and two of the three outputs
in the test set. Dashed lines indicate the ﬁnal value of the swithcing points after optimization. Dots indicate
training data.

Optimization of the hyperparameters (including t1 and t2) is done
by maximization of the marginal likelihood through scaled conju-
gate gradient. We train models for Q = 1  2  3  4 and 5 and measure
the mean standarized log loss (MSLL) and the mean standarized
mean square error (SMSE) [8] over the test set for each value of Q.
Table 1  ﬁrst two rows  show the corresponding average results over
the 5 repetitions together with one standard deviation. Notice that
for Q = 3  the model gets by the ﬁrst time the best performance 
performance that repeats again for Q = 4. The SMSE performance
remains approximately equal for values of Q greater than 3. Fig-
ures 4(a)  4(b) and 4(c) shows the kind of predictions made by the
model for Q = 3.
We generate also a different toy example  in which the length-scales of the intervals are different.
For the second toy experiment  we assume D = 3  Q = 2 and switching points t0 = −2 and
t1 = 8. The parameters of the outputs are A1 = A2 = A3 = 0.1  C1 = 2  C2 = 3  C3 = 0.5 
κ1 = 0.4  κ2 = 1  κ3 = 1 and length scales (cid:96)0 = 1e − 3 and (cid:96)1 = 1. Sensitivities in this case are
S1 1 = 1  S2 1 = 5  S3 1 = 1  S1 2 = 5  S2 2 = 1 and S3 2 = 1. We follow the same evaluation
setup as in toy example 1. Table 1  last two rows  show the performance again in terms of MLSS
and SMSE. We see that for values of Q > 2  the MLSS and SMSE remain similar. In ﬁgures 4(d) 
4(e) and 4(f)  the inferred latent force and the predictions made for two of the three outputs.
6.2 Segmentation of human movement data for robot imitation learning
In this section  we evaluate the feasibility of the model for motion segmentation with possible appli-
cations in the analysis of human movement data and imitation learning. To do so  we had a human
teacher take the robot by the hand and have him demonstrate striking movements in a cooperative
game of table tennis with another human being as shown in Figure 3. We recorded joint positions 

Figure 3: Data collection was
performed using a Barrett WAM
robot as haptic input device.

7

051015−2−1012051015−0.2−0.100.10.20.30.4051015−0.25−0.2−0.15−0.1−0.0500.0505101520−101205101520−1−0.500.511.505101520−0.100.10.20.30.40.5(a) Log-Likelihood Try 1.

(b) Latent force Try 1.

(c) HR Output Try 1.

(d) Log-Likelihood Try 2.

(e) Latent force Try 2.

(f) SFE Output Try 2.

Figure 5: Employing the switching dynamical LFM model on the human movement data collected as in
Fig.3 leads to plausible segmentations of the demonstrated trajectories. The ﬁrst row corresponds to the log-
likelihood  latent force and one of four outputs for trial one. Second row shows the same quantities for trial two.
Crosses in the bottom of the ﬁgure refer to the number of points used for the approximation of the Gaussian
process  in this case K = 50.

angular velocities  and angular acceleration of the robot for two independent trials of the same ta-
ble tennis exercise. For each trial  we selected four output positions and train several models for
different values of Q  including the latent force model without switches (Q = 1). We evaluate the
quality of the segmentation in terms of the log-likelihood. Figure 5 shows the log-likelihood  the
inferred latent force and one output for trial one (ﬁrst row) and the corresponding quantities for trial
two (second row). Figures 5(a) and 5(d) show peaks for the log-likelihood at Q = 9 for trial one and
Q = 10 for trial two. As the movement has few gaps and the data has several output dimensions 
it is hard even for a human being to detect the transitions between movements (unless it is visual-
ized as in a movie). Nevertheless  the model found a maximum for the log-likelihood at the correct
instances in time where the human transits between two movements. At these instances the human
usually reacts due to an external stimulus with a large jerk causing a jump in the forces. As a result 
we obtained not only a segmentation of the movement but also a generative model for table tennis
striking movements.

7 Conclusion
We have introduced a new probabilistic model that develops the latent force modeling framework
with switched Gaussian processes. This allows for discontinuities in the latent space of forces. We
have shown the application of the model in toy examples and on a real world robot problem  in
which we were interested in ﬁnding and representing striking movements. Other applications of the
switching latent force model that we envisage include modeling human motion capture data using
the second order ODE and a ﬁrst order ODE for modeling of complex circuits in biological networks.
To ﬁnd the order of the model  this is  the number of intervals  we have used cross-validation. Future
work includes proposing a less expensive model selection criteria.

Acknowledgments

MA and NL are very grateful for support from a Google Research Award “Mechanistically Inspired
Convolution Processes for Learning” and the EPSRC Grant No EP/F005687/1 “Gaussian Processes
for Systems Identiﬁcation with Applications in Systems Biology”. MA also thanks PASCAL2 Inter-
nal Visiting Programme. We also thank to three anonymous reviewers for their helpful comments.

8

123456789101112−1000−800−600−400−2000200400Value of the log−likelihoodNumber of intervals5101520−2−1012TimeLatent Force5101520−3−2.5−2−1.5−1−0.500.5TimeHR123456789101112−1200−1000−800−600−400−2000200Value of the log−likelihoodNumber of intervals51015−2−101234TimeLatent Force51015−1−0.500.511.522.5TimeSFEReferences
[1] Mauricio ´Alvarez  David Luengo  and Neil D. Lawrence. Latent Force Models. In David van Dyk and
Max Welling  editors  Proceedings of the Twelfth International Conference on Artiﬁcial Intelligence and
Statistics  pages 9–16  Clearwater Beach  Florida  16-18 April 2009. JMLR W&CP 5.

[2] Mauricio A. ´Alvarez  David Luengo  Michalis K. Titsias  and Neil D. Lawrence. Efﬁcient multioutput

Gaussian processes through variational inducing kernels. In JMLR: W&CP 9  pages 25–32  2010.

[3] Roman Garnett  Michael A. Osborne  Steven Reece  Alex Rogers  and Stephen J. Roberts. Sequential
Bayesian prediction in the presence of changepoints and faults. The Computer Journal  2010. Advance
Access published February 1  2010.

[4] Roman Garnett  Michael A. Osborne  and Stephen J. Roberts. Sequential Bayesian prediction in the pres-
ence of changepoints. In Proceedings of the 26th Annual International Conference on Machine Learning 
pages 345–352  2009.

[5] Antti Honkela  Charles Girardot  E. Hilary Gustafson  Ya-Hsin Liu  Eileen E. M. Furlong  Neil D.
Lawrence  and Magnus Rattray. Model-based method for transcription factor target identiﬁcation with
limited data. PNAS  107(17):7793–7798  2010.

[6] A. Ijspeert  J. Nakanishi  and S. Schaal. Learning attractor landscapes for learning motor primitives. In

Advances in Neural Information Processing Systems 15  2003.

[7] T. Oyama  Y. Uno  and S. Hosoe. Analysis of variability of human reaching movements based on the
similarity preservation of arm trajectories. In International Conference on Neural Information Processing
(ICONIP)  pages 923–932  2007.

[8] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT

Press  Cambridge  MA  2006.

[9] Yunus Saatc¸i  Ryan Turner  and Carl Edward Rasmussen. Gaussian Process change point models. In

Proceedings of the 27th Annual International Conference on Machine Learning  pages 927–934  2010.

[10] E. Solak  R. Murray-Smith W. E. Leithead  D. J. Leith  and C. E. Rasmussen. Derivative observations in
Gaussian process models of dynamic systems. In Sue Becker  Sebastian Thrun  and Klaus Obermayer 
editors  NIPS  volume 15  pages 1033–1040  Cambridge  MA  2003. MIT Press.

[11] Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In JMLR:

W&CP 5  pages 567–574  2009.

9

,Yunwen Lei
Urun Dogan
Alexander Binder
Marius Kloft
Wenlin Wang
Chenyang Tao
Zhe Gan
Guoyin Wang
Liqun Chen
Xinyuan Zhang
Ruiyi Zhang
Qian Yang
Ricardo Henao
Lawrence Carin