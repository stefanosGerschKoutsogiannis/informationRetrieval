2011,Relative Density-Ratio Estimation for Robust Distribution Comparison,Divergence estimators based on direct approximation of density-ratios without going through separate approximation of numerator and denominator densities have been successfully applied to machine learning tasks that involve distribution comparison such as outlier detection  transfer learning  and two-sample homogeneity test. However  since density-ratio functions often possess high fluctuation  divergence estimation is still a challenging task in practice. In this paper  we propose to use relative divergences for distribution comparison  which involves approximation of relative density-ratios. Since relative density-ratios are always smoother than corresponding ordinary density-ratios  our proposed method is favorable in terms of the non-parametric convergence speed. Furthermore  we show that the proposed divergence estimator has asymptotic variance independent of the model complexity under a parametric setup  implying that the proposed estimator hardly overfits even with complex models. Through experiments  we demonstrate the usefulness of the proposed approach.,Relative Density-Ratio Estimation
for Robust Distribution Comparison

Makoto Yamada

Tokyo Institute of Technology

Taiji Suzuki

The University of Tokyo

yamada@sg.cs.titech.ac.jp

s-taiji@stat.t.u-tokyo.ac.jp

Takafumi Kanamori
Nagoya University

kanamori@is.nagoya-u.ac.jp

Hirotaka Hachiya Masashi Sugiyama

Tokyo Institute of Technology

{hachiya@sg. sugi@}cs.titech.ac.jp

Abstract

Divergence estimators based on direct approximation of density-ratios without go-
ing through separate approximation of numerator and denominator densities have
been successfully applied to machine learning tasks that involve distribution com-
parison such as outlier detection  transfer learning  and two-sample homogeneity
test. However  since density-ratio functions often possess high ﬂuctuation  diver-
gence estimation is still a challenging task in practice. In this paper  we propose to
use relative divergences for distribution comparison  which involves approxima-
tion of relative density-ratios. Since relative density-ratios are always smoother
than corresponding ordinary density-ratios  our proposed method is favorable in
terms of the non-parametric convergence speed. Furthermore  we show that the
proposed divergence estimator has asymptotic variance independent of the model
complexity under a parametric setup  implying that the proposed estimator hardly
overﬁts even with complex models. Through experiments  we demonstrate the
usefulness of the proposed approach.

1 Introduction

Comparing probability distributions is a fundamental task in statistical data processing. It can be
used for  e.g.  outlier detection [1  2]  two-sample homogeneity test [3  4]  and transfer learning
[5  6].
A standard approach to comparing probability densities p(x) and p0(x) would be to estimate a
divergence from p(x) to p0(x)  such as the Kullback-Leibler (KL) divergence [7]:
r(x) := p(x)/p0(x) 

KL[p(x)  p0(x)] := Ep(x) [log r(x)]  

where Ep(x) denotes the expectation over p(x). A naive way to estimate the KL divergence is to
separately approximate the densities p(x) and p0(x) from data and plug the estimated densities in
the above deﬁnition. However  since density estimation is known to be a hard task [8]  this approach
does not work well unless a good parametric model is available. Recently  a divergence estimation
approach which directly approximates the density-ratio r(x) without going through separate approx-
imation of densities p(x) and p0(x) has been proposed [9  10]. Such density-ratio approximation
methods were proved to achieve the optimal non-parametric convergence rate in the mini-max sense.
However  the KL divergence estimation via density-ratio approximation is computationally rather
expensive due to the non-linearity introduced by the ‘log’ term. To cope with this problem  another
divergence called the Pearson (PE) divergence [11] is useful. The PE divergence is deﬁned as

(cid:163)

(cid:164)

(r(x) − 1)2

.

PE[p(x)  p0(x)] := 1

2Ep0(x)

1

The PE divergence is a squared-loss variant of the KL divergence  and they both belong to the class
of the Ali-Silvey-Csisz´ar divergences (which is also known as the f-divergences  see [12  13]). Thus 
the PE and KL divergences share similar properties  e.g.  they are non-negative and vanish if and
only if p(x) = p0(x).
Similarly to the KL divergence estimation  the PE divergence can also be accurately estimated based
on density-ratio approximation [14]:
the density-ratio approximator called unconstrained least-
squares importance ﬁtting (uLSIF) gives the PE divergence estimator analytically  which can be
computed just by solving a system of linear equations. The practical usefulness of the uLSIF-based
PE divergence estimator was demonstrated in various applications such as outlier detection [2]  two-
sample homogeneity test [4]  and dimensionality reduction [15].
In this paper  we ﬁrst establish the non-parametric convergence rate of the uLSIF-based PE di-
vergence estimator  which elucidates its superior theoretical properties. However  it also reveals
that its convergence rate is actually governed by the ‘sup’-norm of the true density-ratio function:
maxx r(x). This implies that  in the region where the denominator density p0(x) takes small values 
the density-ratio r(x) = p(x)/p0(x) tends to take large values and therefore the overall convergence
speed becomes slow. More critically  density-ratios can even diverge to inﬁnity under a rather simple
setting  e.g.  when the ratio of two Gaussian functions is considered [16]. This makes the paradigm
of divergence estimation based on density-ratio approximation unreliable.
In order to overcome this fundamental problem  we propose an alternative approach to distribution
comparison called α-relative divergence estimation.
In the proposed approach  we estimate the
α-relative divergence  which is the divergence from p(x) to the α-mixture density:

qα(x) = αp(x) + (1 − α)p0(x) for 0 ≤ α < 1.

For example  the α-relative PE divergence is given by

PEα[p(x)  p0(x)] := PE[p(x)  qα(x)] = 1

2Eqα(x)

(rα(x) − 1)2

where rα(x) is the α-relative density-ratio of p(x) and p0(x):

rα(x) := p(x)/qα(x) = p(x)/

αp(x) + (1 − α)p0(x)

.

(cid:163)

(cid:164)

 

(1)

(2)

(cid:180)

(cid:179)

We propose to estimate the α-relative divergence by direct approximation of the α-relative density-
ratio.
A notable advantage of this approach is that the α-relative density-ratio is always bounded above by
1/α when α > 0  even when the ordinary density-ratio is unbounded. Based on this feature  we the-
oretically show that the α-relative PE divergence estimator based on α-relative density-ratio approx-
imation is more favorable than the ordinary density-ratio approach in terms of the non-parametric
convergence speed.
We further prove that  under a correctly-speciﬁed parametric setup  the asymptotic variance of our
α-relative PE divergence estimator does not depend on the model complexity. This means that the
proposed α-relative PE divergence estimator hardly overﬁts even with complex models.
Through experiments on outlier detection  two-sample homogeneity test  and transfer learning  we
demonstrate that our proposed α-relative PE divergence estimator compares favorably with alterna-
tive approaches.

2 Estimation of Relative Pearson Divergence via Least-Squares Relative

Density-Ratio Approximation

samples {xi}n
i=1 from
Suppose we are given independent and identically distributed (i.i.d.)
j}n0
a d-dimensional distribution P with density p(x) and i.i.d. samples {x0
j=1 from another d-
dimensional distribution P 0 with density p0(x). Our goal is to compare the two underlying dis-
j}n0
tributions P and P 0 only using the two sets of samples {xi}n
j=1.
In this section  we give a method for estimating the α-relative PE divergence based on direct ap-
proximation of the α-relative density-ratio.

i=1 and {x0

2

(cid:80)n

Direct Approximation of α-Relative Density-Ratios: Let us model the α-relative density-ratio
‘=1 θ‘K(x  x‘)  where θ := (θ1  . . .   θn)>
rα(x) (2) by the following kernel model g(x; θ) :=
are parameters to be learned from data samples  > denotes the transpose of a matrix or a vector  and
K(x  x0) is a kernel basis function. In the experiments  we use the Gaussian kernel.
The parameters θ in the model g(x; θ) are determined so that the following expected squared-error
J is minimized:

(cid:163)

(cid:164) − Ep(x) [g(x; θ)] + Const. 

2 Ep0(x)

g(x; θ)2

J(θ) := 1

2Eqα(x)
2 Ep(x)

= α

(cid:105)

(cid:164)

(g(x; θ) − rα(x))2
+ (1−α)
g(x; θ)2

(cid:104)
(cid:163)
(cid:98)θ := argmin θ∈Rn

>

>

1
2 θ

(cid:105)

where a penalty term λθ

where we used rα(x)qα(x) = p(x) in the third term. Approximating the expectations by empirical
averages  we obtain the following optimization problem:

(3)
θ/2 is included for regularization purposes  and λ (≥ 0) denotes the

(cid:104)
>(cid:99)Hθ −(cid:98)h
regularization parameter.(cid:99)H and(cid:98)h are deﬁned as
(cid:80)n0
(cid:98)H‘ ‘0 := α
It is easy to conﬁrm that the solution of Eq.(3) can be analytically obtained as(cid:98)θ = ((cid:99)H + λI n)−1(cid:98)h 
(cid:98)rα(x) := g(x;(cid:98)θ) =

(cid:80)n
i=1K(xi  x‘)K(xi  x‘0)+ (1−α)
n0

where I n denotes the n-dimensional identity matrix. Finally  a density-ratio estimator is given as

j  x‘0) (cid:98)h‘ := 1

(cid:98)θ‘K(x  x‘).

j  x‘)K(x0

j=1K(x0

(cid:80)n

(cid:80)n

i=1K(xi  x‘).

θ + λ

2 θ

>

θ

n

n

 

‘=1

(cid:163)

When α = 0  the above method is reduced to a direct density-ratio estimator called unconstrained
least-squares importance ﬁtting (uLSIF) [14]. Thus  the above method can be regarded as an ex-
tension of uLSIF to the α-relative density-ratio. For this reason  we refer to our method as relative
uLSIF (RuLSIF).
The performance of RuLSIF depends on the choice of the kernel function (the kernel width in the
case of the Gaussian kernel) and the regularization parameter λ. Model selection of RuLSIF is
possible based on cross-validation (CV) with respect to the squared-error criterion J.
Using an estimator of the α-relative density-ratio rα(x)  we can construct estimators of the α-
relative PE divergence (1). After a few lines of calculation  we can show that the α-relative PE
divergence (1) is equivalently expressed as
PEα = − α
2Ep(x) [rα(x)] − 1
2 Ep0(x)
2 .
Note that the middle expression can also be obtained via Legendre-Fenchel convex duality of the
divergence functional [17].
Based on these expressions  we consider the following two estimators:

(cid:164) − (1−α)
(cid:80)n
(cid:99)PEα := − α
i=1(cid:98)rα(xi)2 − (1−α)
(cid:80)n
(cid:102)PEα := 1
i=1(cid:98)rα(xi) − 1

(5)
We note that the α-relative PE divergence (1) can have further different expressions than the above
ones  and corresponding estimators can also be constructed similarly. However  the above two

expressions will be particularly useful: the ﬁrst estimator (cid:99)PEα has superior theoretical properties
(see Section 3) and the second one (cid:102)PEα is simple to compute.

(cid:80)n
i=1(cid:98)rα(xi) − 1

(cid:80)n0
j=1(cid:98)rα(x0

+ Ep(x) [rα(x)] − 1

2 Ep(x)

j)2 + 1

rα(x)2

rα(x)2

2 = 1

(cid:164)

(cid:163)

(4)

2n0

2  

2 .

2n

2n

n

3 Theoretical Analysis

In this section  we analyze theoretical properties of the proposed PE divergence estimators. Since
our theoretical analysis is highly technical  we focus on explaining practical insights we can gain
from the theoretical results here; we describe all the mathematical details in the supplementary
material.

3

(cid:104)

For theoretical analysis  let us consider a rather abstract form of our relative density-ratio estimator
described as

argming∈G

(6)
where G is some function space (i.e.  a statistical model) and R(·) is some regularization functional.

i=1 g(xi) + λ

2 R(g)2

α
2n

n

 

j=1 g(x0

j)2 − 1

(cid:80)n
i=1 g(xi)2 + (1−α)
2n0

(cid:80)n0

(cid:80)n

(cid:105)

Non-Parametric Convergence Analysis: First  we elucidate the non-parametric convergence rate
of the proposed PE estimators. Here  we practically regard the function space G as an inﬁnite-
dimensional reproducing kernel Hilbert space (RKHS) [18] such as the Gaussian kernel space  and
R(·) as the associated RKHS norm.
Let us represent the complexity of the function space G by γ (0 < γ < 2); the larger γ is  the
more complex the function class G is (see the supplementary material for its precise deﬁnition). We
analyze the convergence rate of our PE divergence estimators as ¯n := min(n  n0) tends to inﬁnity
for λ = λ¯n under

λ¯n → o(1) and λ−1

¯n = o(¯n2/(2+γ)).

The ﬁrst condition means that λ¯n tends to zero  but the second condition means that its shrinking
speed should not be too fast.
Under several technical assumptions detailed in the supplementary material  we have the following

asymptotic convergence results for the two PE divergence estimators (cid:99)PEα (4) and (cid:102)PEα (5):
(cid:180)

(cid:99)PEα − PEα = Op(¯n−1/2ckrαk∞ + λ¯n max(1  R(rα)2)) 
(cid:102)PEα − PEα = Op

(cid:179)

  R(rα)krαk(1−γ/2)/2

∞

  R(rα)}

 

(7)

(8)

¯n krαk1/2∞ max{1  R(rα)}
λ1/2
+ λ¯n max{1 krαk(1−γ/2)/2
(cid:113)

∞

(cid:113)

where Op denotes the asymptotic order in probability 

c := (1 + α)

Vp(x)[rα(x)] + (1 − α)

Vp0(x)[rα(x)] 

and Vp(x) denotes the variance over p(x):

Vp(x)[f(x)] =

f(x)p(x)dx

p(x)dx.

(cid:82)(cid:161)
f(x) −(cid:82)

(cid:162)2
(cid:176)(cid:176)(cid:176)∞ < 1

(cid:162)−1

(cid:176)(cid:176)(cid:176)(cid:161)

In both Eq.(7) and Eq.(8)  the coefﬁcients of the leading terms (i.e.  the ﬁrst terms) of the asymptotic
convergence rates become smaller as krαk∞ gets smaller. Since

krαk∞ =

α + (1 − α)/r(x)

α for α > 0 

larger α would be more preferable in terms of the asymptotic approximation error. Note that when
α = 0  krαk∞ can tend to inﬁnity even under a simple setting that the ratio of two Gaussian func-
tions is considered [16]. Thus  our proposed approach of estimating the α-relative PE divergence
(with α > 0) would be more advantageous than the naive approach of estimating the plain PE
divergence (which corresponds to α = 0) in terms of the non-parametric convergence rate.

The above results also show that (cid:99)PEα and (cid:102)PEα have different asymptotic convergence rates. The
slightly slower (depending on the complexity γ) than ¯n−1/2. Thus  (cid:99)PEα would be more accurate
than (cid:102)PEα in large sample cases. Furthermore  when p(x) = p0(x)  Vp(x)[rα(x)] = 0 holds and
thus c = 0 holds. Then the leading term in Eq.(7) vanishes and therefore (cid:99)PEα has the even faster

leading term in Eq.(7) is of order ¯n−1/2  while the leading term in Eq.(8) is of order λ1/2

convergence rate of order λ¯n  which is slightly slower (depending on the complexity γ) than ¯n−1.
Similarly  if α is close to 1  rα(x) ≈ 1 and thus c ≈ 0 holds.
When ¯n is not large enough to be able to neglect the terms of o(¯n−1/2)  the terms of O(λ¯n) matter.
If krαk∞ and R(rα) are large (this can happen  e.g.  when α is close to 0)  the coefﬁcient of the

O(λ¯n)-term in Eq.(7) can be larger than that in Eq.(8). Then (cid:102)PEα would be more favorable than
(cid:99)PEα in terms of the approximation accuracy.

¯n   which is

See the supplementary material for numerical examples illustrating the above theoretical results.

4

Parametric Variance Analysis: Next  we analyze the asymptotic variance of the PE divergence

estimator (cid:99)PEα (4) under a parametric setup.

samples {xi}n

∗ such that g(x; θ

As the function space G in Eq.(6)  we consider the following parametric model: G = {g(x; θ) | θ ∈
Θ ⊂ Rb} for a ﬁnite b. Here we assume that this parametric model is correctly speciﬁed  i.e.  it
∗) = rα(x).
includes the true relative density-ratio function rα(x): there exists θ
Here  we use RuLSIF without regularization  i.e.  λ = 0 in Eq.(6).

Let us denote the variance of (cid:99)PEα (4) by V[(cid:99)PEα]  where randomness comes from the draw of
normality [19]  V[(cid:99)PEα] can be expressed and upper-bounded as
V[(cid:99)PEα] = Vp(x)
Let us denote the variance of (cid:102)PEα by V[(cid:102)PEα]. Then  under a standard regularity condition for the
asymptotic normality [19]  the variance of (cid:102)PEα is asymptotically expressed as

≤ krαk2∞/n + α2krαk4∞/(4n) + (1 − α)2krαk4∞/(4n0) + o(n−1  n0−1).

j}n0
j=1. Then  under a standard regularity condition for the asymptotic

i=1 and {x0
(cid:163)

/n0 + o(n−1  n0−1)

(1 − α)rα(x)2/2

rα − αrα(x)2/2

/n + Vp0(x)

(9)
(10)

(cid:164)

(cid:163)

(cid:164)

V[(cid:102)PEα] = Vp(x)

(cid:163)(cid:161)

(cid:163)(cid:161)

rα + (1 − αrα)Ep(x)[∇g]>H−1
α ∇g
where ∇g is the gradient vector of g with respect to θ at θ = θ

(1 − α)rαEp(x)[∇g]>H−1

+ Vp0(x)

(cid:164)

(cid:162)
(cid:164)

(cid:162)
α ∇g
/2
∗ and

/n

/2
/n0 + o(n−1  n0−1) 

(11)

Eq.(9) shows that  up to O(n−1  n0−1)  the variance of (cid:99)PEα depends only on the true relative

H α = αEp(x)[∇g∇g>] + (1 − α)Ep0(x)[∇g∇g>].

density-ratio rα(x)  not on the estimator of rα(x). This means that the model complexity does not
affect the asymptotic variance. Therefore  overﬁtting would hardly occur in the estimation of the
relative PE divergence even when complex models are used. We note that the above superior prop-
erty is applicable only to relative PE divergence estimation  not to relative density-ratio estimation.
This implies that overﬁtting occurs in relative density-ratio estimation  but the approximation error
cancels out in relative PE divergence estimation.

the variance of (cid:102)PEα is affected by the model G 
∗) = rα(x) holds  the variances of (cid:102)PEα and (cid:99)PEα are asymptotically
the same. However  in general  the use of (cid:99)PEα would be more recommended.
Eq.(10) shows that the variance V[(cid:99)PEα] can be upper-bounded by the quantity depending on krαk∞ 

On the other hand  Eq.(11) shows that
since the factor Ep(x)[∇g]>H−1
Ep(x)[∇g]>H−1

α ∇g depends on the model in general. When the equality

which is monotonically lowered if krαk∞ is reduced. Since krαk∞ monotonically decreases as α
increases  our proposed approach of estimating the α-relative PE divergence (with α > 0) would
be more advantageous than the naive approach of estimating the plain PE divergence (which corre-
sponds to α = 0) in terms of the parametric asymptotic variance.
See the supplementary material for numerical examples illustrating the above theoretical results.

α ∇g(x; θ

4 Experiments

In this section  we experimentally evaluate the performance of the proposed method in two-sample
homogeneity test  outlier detection  and transfer learning tasks.

Two-Sample Homogeneity Test: First  we apply the proposed divergence estimator to two-
sample homogeneity test.
Given two sets of samples X = {xi}n
i.i.d.∼ P 0  the goal of the two-
sample homogeneity test is to test the null hypothesis that the probability distributions P and P 0
are the same against its complementary alternative (i.e.  the distributions are different). By using

an estimator(cid:100)Div of some divergence between the two distributions P and P 0  homogeneity of two

i.i.d.∼ P and X 0 = {x0

j}n0

j=1

i=1

distributions can be tested based on the permutation test procedure [20].

5

Table 1: Experimental results of two-sample test. The mean (and standard deviation in the bracket)
rate of accepting the null hypothesis (i.e.  P = P 0) for IDA benchmark repository under the sig-
niﬁcance level 5% is reported. Left: when the two sets of samples are both taken from the positive
training set (i.e.  the null hypothesis is correct). Methods having the mean acceptance rate 0.95 ac-
cording to the one-sample t-test at the signiﬁcance level 5% are speciﬁed by bold face. Right: when
the set of samples corresponding to the numerator of the density-ratio are taken from the positive
training set and the set of samples corresponding to the denominator of the density-ratio are taken
from the positive training set and the negative training set (i.e.  the null hypothesis is not correct).
The best method having the lowest mean accepting rate and comparable methods according to the
two-sample t-test at the signiﬁcance level 5% are speciﬁed by bold face.

Datasets
banana
thyroid
titanic
diabetes
b-cancer
f-solar
heart
german
ringnorm
waveform

d n = n0
100
2
19
5
21
5
85
8
29
9
100
9
13
38
100
20
100
20
21
66

MMD

.96 (.20)
.96 (.20)
.94 (.24)
.96 (.20)
.98 (.14)
.93 (.26)
1.00 (.00)
.99 (.10)
.97 (.17)
.98 (.14)

P = P 0

LSTT

(α = 0.0)
.93 (.26)
.95 (.22)
.86 (.35)
.87 (.34)
.91 (.29)
.91 (.29)
.85 (.36)
.91 (.29)
.93 (.26)
.92 (.27)

LSTT

(α = 0.5)
.92 (.27)
.95 (.22)
.92 (.27)
.91 (.29)
.94 (.24)
.95 (.22)
.91 (.29)
.92 (.27)
.91 (.29)
.93 (.26)

LSTT

(α = 0.95)
.92
(.27)
(.33)
.88
(.31)
.89
(.39)
.82
(.27)
.92
(.26)
.93
.93
(.26)
(.31)
.89
(.36)
.85
.88
(.33)

MMD
.52 (.50)
.52 (.50)
.87 (.34)
.31 (.46)
.87 (.34)
.51 (.50)
.53 (.50)
.56 (.50)
.00 (.00)
.00 (.00)

P 6= P 0

LSTT

(α = 0.0)
.10 (.30)
.81 (.39)
.86 (.35)
.42 (.50)
.75 (.44)
.81 (.39)
.28 (.45)
.55 (.50)
.00 (.00)
.00 (.00)

LSTT

(α = 0.5)
.02 (.14)
.65 (.48)
.87 (.34)
.47 (.50)
.80 (.40)
.55 (.50)
.40 (.49)
.44 (.50)
.00 (.00)
.02 (.14)

LSTT

(α = 0.95)
.17
(.38)
(.40)
.80
(.33)
.88
(.50)
.57
(.41)
.79
(.48)
.66
.62
(.49)
(.47)
.68
(.14)
.02
.00
(.00)

When an asymmetric divergence such as the KL divergence [7] or the PE divergence [11] is adopted
for two-sample test  the test results depend on the choice of directions: a divergence from P to
P 0 or from P 0 to P .
[4] proposed to choose the direction that gives a smaller p-value—it was
experimentally shown that  when the uLSIF-based PE divergence estimator is used for the two-
sample test (which is called the least-squares two-sample test; LSTT)  the heuristic of choosing the
direction with a smaller p-value contributes to reducing the type-II error (the probability of accepting
incorrect null-hypotheses  i.e.  two distributions are judged to be the same when they are actually
different)  while the increase of the type-I error (the probability of rejecting correct null-hypotheses 
i.e.  two distributions are judged to be different when they are actually the same) is kept moderate.
We apply the proposed method to the binary classiﬁcation datasets taken from the IDA benchmark
repository [21]. We test LSTT with the RuLSIF-based PE divergence estimator for α = 0  0.5  and
0.95; we also test the maximum mean discrepancy (MMD) [22]  which is a kernel-based two-sample
test method. The performance of MMD depends on the choice of the Gaussian kernel width. Here 
we adopt a version proposed by [23]  which automatically optimizes the Gaussian kernel width. The
p-values of MMD are computed in the same way as LSTT based on the permutation test procedure.
First  we investigate the rate of accepting the null hypothesis when the null hypothesis is correct
(i.e.  the two distributions are the same). We split all the positive training samples into two sets and
perform two-sample test for the two sets of samples. The experimental results are summarized in
the left half of Table 1  showing that LSTT with α = 0.5 compares favorably with those with α = 0
and 0.95 and MMD in terms of the type-I error.
Next  we consider the situation where the null hypothesis is not correct (i.e.  the two distributions
are different). The numerator samples are generated in the same way as above  but a half of denom-
inator samples are replaced with negative training samples. Thus  while the numerator sample set
contains only positive training samples  the denominator sample set includes both positive and nega-
tive training samples. The experimental results are summarized in the right half of Table 1  showing
that LSTT with α = 0.5 again compares favorably with those with α = 0 and 0.95. Furthermore 
LSTT with α = 0.5 tends to outperform MMD in terms of the type-II error.
Overall  LSTT with α = 0.5 is shown to be a useful method for two-sample homogeneity test. See
the supplementary material for more experimental evaluation.

Inlier-Based Outlier Detection: Next  we apply the proposed method to outlier detection.
Let us consider an outlier detection problem of ﬁnding irregular samples in a dataset (called an
“evaluation dataset”) based on another dataset (called a “model dataset”) that only contains regular
samples. Deﬁning the density-ratio over the two sets of samples  we can see that the density-ratio

6

100

having

Table 2: Experimental
results of outlier detec-
tion. Mean AUC score
(and standard devi-
ation in the bracket)
trials
over
is
The best
reported.
method
the
highest mean AUC
score and comparable
methods according to
the two-sample t-test
at
signiﬁcance
level 5% are speciﬁed
The
by bold face.
sorted
datasets
are
ascending
in
the
order of
the
input
dimensionality d.

the

Datasets
IDA:banana
IDA:thyroid
IDA:titanic
IDA:diabetes
IDA:breast-cancer
IDA:ﬂare-solar
IDA:heart
IDA:german
IDA:ringnorm
IDA:waveform
Speech
20News (‘rec’)
20News (‘sci’)
20News (‘talk’)
USPS (1 vs. 2)
USPS (2 vs. 3)
USPS (3 vs. 4)
USPS (4 vs. 5)
USPS (5 vs. 6)
USPS (6 vs. 7)
USPS (7 vs. 8)
USPS (8 vs. 9)
USPS (9 vs. 0)

d
2
5
5
8
9
9
13
20
20
21
50
100
100
100
256
256
256
256
256
256
256
256
256

OSVM

(ν = 0.05)
.668 (.105)
.760 (.148)
.757 (.205)
.636 (.099)
.741 (.160)
.594 (.087)
.714 (.140)
.612 (.069)
.991 (.012)
.812 (.107)
.788 (.068)
.598 (.063)
.592 (.069)
.661 (.084)
.889 (.052)
.823 (.053)
.901 (.044)
.871 (.041)
.825 (.058)
.910 (.034)
.938 (.030)
.721 (.072)
.920 (.037)

OSVM

(ν = 0.1)
.676 (.120)
.782 (.165)
.752 (.191)
.610 (.090)
.691 (.147)
.590 (.083)
.694 (.148)
.604 (.084)
.993 (.007)
.843 (.123)
.830 (.060)
.593 (.061)
.589 (.071)
.658 (.084)
.926 (.037)
.835 (.050)
.939 (.031)
.890 (.036)
.859 (.052)
.950 (.025)
.967 (.021)
.728 (.073)
.966 (.023)

RuLSIF
(α = 0)
.597 (.097)
.804 (.148)
.750 (.182)
.594 (.105)
.707 (.148)
.626 (.102)
.748 (.149)
.605 (.092)
.944 (.091)
.879 (.122)
.804 (.101)
.628 (.105)
.620 (.094)
.672 (.117)
.848 (.081)
.803 (.093)
.950 (.056)
.857 (.099)
.863 (.078)
.972 (.038)
.941 (.053)
.721 (.084)
.982 (.048)

RuLSIF
(α = 0.5)
.619 (.101)
.796 (.178)
.701 (.184)
.575 (.105)
.737 (.159)
.612 (.100)
.769 (.134)
.597 (.101)
.971 (.062)
.875 (.117)
.821 (.076)
.614 (.093)
.609 (.087)
.670 (.102)
.878 (.088)
.818 (.085)
.961 (.041)
.874 (.082)
.867 (.068)
.984 (.018)
.951 (.039)
.728 (.083)
.989 (.022)

RuLSIF

(α = 0.95)
.623 (.115)
.722 (.153)
.712 (.185)
.663 (.112)
.733 (.160)
.584 (.114)
.726 (.127)
.605 (.095)
.992 (.010)
.885 (.102)
.836 (.083)
.767 (.100)
.704 (.093)
.823 (.078)
.898 (.051)
.879 (.074)
.984 (.016)
.941 (.031)
.901 (.049)
.994 (.010)
.980 (.015)
.761 (.096)
.994 (.011)

values for regular samples are close to one  while those for outliers tend to be signiﬁcantly deviated
from one. Thus  density-ratio values could be used as an index of the degree of outlyingness [1  2].
Since the evaluation dataset usually has a wider support than the model dataset  we regard the eval-
uation dataset as samples corresponding to the denominator density p0(x)  and the model dataset as
samples corresponding to the numerator density p(x). Then  outliers tend to have smaller density-
ratio values (i.e.  close to zero). Thus  density-ratio approximators can be used for outlier detection.
We evaluate the proposed method using various datasets: IDA benchmark repository [21]  an in-
house French speech dataset  the 20 Newsgroup dataset  and the USPS hand-written digit dataset
(the detailed speciﬁcation of the datasets is explained in the supplementary material).
We compare the area under the ROC curve (AUC) [24] of RuLSIF with α = 0  0.5  and 0.95  and
one-class support vector machine (OSVM) with the Gaussian kernel [25]. We used the LIBSVM
implementation of OSVM [26]. The Gaussian width is set to the median distance between samples 
which has been shown to be a useful heuristic [25]. Since there is no systematic method to determine
the tuning parameter ν in OSVM  we report the results for ν = 0.05 and 0.1.
The mean and standard deviation of the AUC scores over 100 runs with random sample choice are
summarized in Table 2  showing that RuLSIF overall compares favorably with OSVM. Among the
RuLSIF methods  small α tends to perform well for low-dimensional datasets  and large α tends to
work well for high-dimensional datasets.

Transfer Learning: Finally  we apply the proposed method to transfer learning.
j )}ntr
Let us consider a transductive transfer learning setup where labeled training samples {(xtr
drawn i.i.d. from p(y|x)ptr(x) and unlabeled test samples {xte
i=1 drawn i.i.d. from pte(x) (which
is generally different from ptr(x)) are available. The use of exponentially-weighted importance
weighting was shown to be useful for adaptation from ptr(x) to pte(x) [5]:

i }nte

j   ytr

j=1

(cid:180)τ

(cid:184)

(cid:183)

(cid:179)

(cid:80)ntr

j=1

minf∈F

1
ntr

pte(xtr
j )
ptr(xtr
j )

loss(ytr

j   f(xtr

j ))

 

where f(x) is a learned function and 0 ≤ τ ≤ 1 is the exponential ﬂattening parameter. τ = 0 corre-
sponds to plain empirical-error minimization which is statistically efﬁcient  while τ = 1 corresponds
to importance-weighted empirical-error minimization which is statistically consistent; 0 < τ < 1
will give an intermediate estimator that balances the trade-off between statistical efﬁciency and con-
sistency. τ can be determined by importance-weighted cross-validation [6] in a data dependent
fashion.

7

Table 3: Experimental results of transfer learning in human activity recognition. Mean classiﬁcation
accuracy (and the standard deviation in the bracket) over 100 runs for human activity recognition of
a new user is reported. We compare the plain kernel logistic regression (KLR) without importance
weights  KLR with relative importance weights (RIW-KLR)  KLR with exponentially-weighted im-
portance weights (EIW-KLR)  and KLR with plain importance weights (IW-KLR). The method hav-
ing the highest mean classiﬁcation accuracy and comparable methods according to the two-sample
t-test at the signiﬁcance level 5% are speciﬁed by bold face.

(cid:104)

(cid:80)ntr

(cid:105)

Task

Walks vs. run
Walks vs. bicycle
Walks vs. train

KLR

(α = 0  τ = 0)
(0.082)
0.803
(0.025)
0.880
0.985
(0.017)

RIW-KLR
(α = 0.5)
0.889 (0.035)
0.892 (0.035)
0.992 (0.008)

EIW-KLR
(τ = 0.5)
0.882 (0.039)
0.867 (0.054)
0.989 (0.011)

IW-KLR

(α = 1  τ = 1)
(0.035)
0.882
(0.070)
0.854
0.983
(0.021)

However  a potential drawback is that estimation of r(x) (i.e.  τ = 1) is rather hard  as shown in this
paper. Here we propose to use relative importance weights instead:

minf∈F

1
ntr

j=1

(1−α)pte(xtr

j )+αptr(xtr

j )loss(ytr

j   f(xtr

j ))

.

pte(xtr
j )

We apply the above transfer learning technique to human activity recognition using accelerometer
data. Subjects were asked to perform a speciﬁc task such as walking  running  and bicycle riding 
which was collected by iPodTouch. The duration of each task was arbitrary and the sampling rate
was 20Hz with small variations (the detailed experimental setup is explained in the supplementary
material). Let us consider a situation where a new user wants to use the activity recognition system.
However  since the new user is not willing to label his/her accelerometer data due to troublesome-
ness  no labeled sample is available for the new user. On the other hand  unlabeled samples for
the new user and labeled data obtained from existing users are available. Let labeled training data
{(xtr
j=1 be the set of labeled accelerometer data for 20 existing users. Each user has at most
100 labeled samples for each action. Let unlabeled test data {xte
i=1 be unlabeled accelerometer
data obtained from the new user.
The experiments are repeated 100 times with different sample choice for ntr = 500 and nte = 200.
The classiﬁcation accuracy for 800 test samples from the new user (which are different from the
200 unlabeled samples) are summarized in Table 3  showing that the proposed method using relative
importance weights for α = 0.5 works better than other methods.

j )}ntr

i }nte

j   ytr

5 Conclusion

In this paper  we proposed to use a relative divergence for robust distribution comparison. We gave
a computationally efﬁcient method for estimating the relative Pearson divergence based on direct
relative density-ratio approximation. We theoretically elucidated the convergence rate of the pro-
posed divergence estimator under non-parametric setup  which showed that the proposed approach
of estimating the relative Pearson divergence is more preferable than the existing approach of esti-
mating the plain Pearson divergence. Furthermore  we proved that the asymptotic variance of the
proposed divergence estimator is independent of the model complexity under a correctly-speciﬁed
parametric setup. Thus  the proposed divergence estimator hardly overﬁts even with complex mod-
els. Experimentally  we demonstrated the practical usefulness of the proposed divergence estimator
in two-sample homogeneity test  inlier-based outlier detection  and transfer learning tasks.
In addition to two-sample homogeneity test  inlier-based outlier detection  and transfer learning 
density-ratios can be useful for tackling various machine learning problems  for example  multi-task
learning  independence test  feature selection  causal inference  independent component analysis 
dimensionality reduction  unpaired data matching  clustering  conditional density estimation  and
probabilistic classiﬁcation. Thus  it would be promising to explore more applications of the pro-
posed relative density-ratio approximator beyond two-sample homogeneity test  inlier-based outlier
detection  and transfer learning.
Acknowledgments
MY was supported by the JST PRESTO program  TS was partially supported by MEXT KAKENHI
22700289 and Aihara Project  the FIRST program from JSPS  initiated by CSTP  TK was partially
supported by Grant-in-Aid for Young Scientists (20700251)  HH was supported by the FIRST pro-
gram  and MS was partially supported by SCAT  AOARD  and the FIRST program.

8

References
[1] A. J. Smola  L. Song  and C. H. Teo. Relative novelty detection. In Proceedings of the Twelfth Interna-

tional Conference on Artiﬁcial Intelligence and Statistics (AISTATS2009)  pages 536–543  2009.

[2] S. Hido  Y. Tsuboi  H. Kashima  M. Sugiyama  and T. Kanamori. Statistical outlier detection using direct

density ratio estimation. Knowledge and Information Systems  26(2):309–336  2011.

[3] A. Gretton  K. M. Borgwardt  M. Rasch  B. Sch¨olkopf  and A. J. Smola. A kernel method for the two-
In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural Information

sample-problem.
Processing Systems 19  pages 513–520. MIT Press  Cambridge  MA  2007.

[4] M. Sugiyama  T. Suzuki  Y. Itoh  T. Kanamori  and M. Kimura. Least-squares two-sample test. Neural

Networks  24(7):735–751  2011.

[5] H. Shimodaira.

Improving predictive inference under covariate shift by weighting the log-likelihood

function. Journal of Statistical Planning and Inference  90(2):227–244  2000.

[6] M. Sugiyama  M. Krauledat  and K.-R. M¨uller. Covariate shift adaptation by importance weighted cross

validation. Journal of Machine Learning Research  8:985–1005  May 2007.

[7] S. Kullback and R. A. Leibler. On information and sufﬁciency. Annals of Mathematical Statistics  22:79–

86  1951.

[8] V. N. Vapnik. Statistical Learning Theory. Wiley  New York  NY  1998.
[9] M. Sugiyama  T. Suzuki  S. Nakajima  H. Kashima  P. von B¨unau  and M. Kawanabe. Direct importance
estimation for covariate shift adaptation. Annals of the Institute of Statistical Mathematics  60:699–746 
2008.

[10] X. Nguyen  M. J. Wainwright  and M. I. Jordan. Estimating divergence functionals and the likelihood
ratio by convex risk minimization. IEEE Transactions on Information Theory  56(11):5847–5861  2010.
[11] K. Pearson. On the criterion that a given system of deviations from the probable in the case of a correlated
system of variables is such that it can be reasonably supposed to have arisen from random sampling.
Philosophical Magazine  50:157–175  1900.

[12] S. M. Ali and S. D. Silvey. A general class of coefﬁcients of divergence of one distribution from another.

Journal of the Royal Statistical Society  Series B  28:131–142  1966.

[13] I. Csisz´ar. Information-type measures of difference of probability distributions and indirect observation.

Studia Scientiarum Mathematicarum Hungarica  2:229–318  1967.

[14] T. Kanamori  S. Hido  and M. Sugiyama. A least-squares approach to direct importance estimation.

Journal of Machine Learning Research  10:1391–1445  2009.

[15] T. Suzuki and M. Sugiyama. Sufﬁcient dimension reduction via squared-loss mutual information estima-
tion. In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics
(AISTATS2010)  pages 804–811  2010.

[16] C. Cortes  Y. Mansour  and M. Mohri. Learning bounds for importance weighting. In J. Lafferty  C. K. I.
Williams  J. Shawe-Taylor  R. S. Zemel  and A. Culotta  editors  Advances in Neural Information Pro-
cessing Systems 23  pages 442–450. 2010.

[17] R. T. Rockafellar. Convex Analysis. Princeton University Press  Princeton  NJ  USA  1970.
[18] N. Aronszajn. Theory of reproducing kernels. Transactions of the American Mathematical Society 

68:337–404  1950.

[19] A. W. van der Vaart. Asymptotic Statistics. Cambridge University Press  2000.
[20] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall  New York  NY  1993.
[21] G. R¨atsch  T. Onoda  and K.-R. M¨uller. Soft margins for adaboost. Machine Learning  42(3):287–320 

2001.

[22] K. M. Borgwardt  A. Gretton  M. J. Rasch  H.-P. Kriegel  B. Sch¨olkopf  and A. J. Smola. Integrating
structured biological data by kernel maximum mean discrepancy. Bioinformatics  22(14):e49–e57  2006.
[23] B. Sriperumbudur  K. Fukumizu  A. Gretton  G. Lanckriet  and B. Sch¨olkopf. Kernel choice and clas-
siﬁability for RKHS embeddings of probability distributions. In Y. Bengio  D. Schuurmans  J. Lafferty 
C. K. I. Williams  and A. Culotta  editors  Advances in Neural Information Processing Systems 22  pages
1750–1758. 2009.

[24] A. P. Bradley. The use of the area under the ROC curve in the evaluation of machine learning algorithms.

Pattern Recognition  30:1145–1159  1997.

[25] B. Sch¨olkopf  J. C. Platt  J. Shawe-Taylor  A. J. Smola  and R. C. Williamson. Estimating the support of

a high-dimensional distribution. Neural Computation  13(7):1443–1471  2001.

[26] C.-C. Chang and C.h-J. Lin. LIBSVM: A Library for Support Vector Machines  2001. Software available

at http://www.csie.ntu.edu.tw/∼cjlin/libsvm.

9

,Hesham Mostafa
Lorenz. Mueller
Giacomo Indiveri
Róbert Busa-Fekete
Balázs Szörényi
Krzysztof Dembczynski
Eyke Hüllermeier
Moran Feldman
Amin Karbasi
Ehsan Kazemi