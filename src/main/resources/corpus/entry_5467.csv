2007,The Infinite Gamma-Poisson Feature Model,We address the problem of factorial learning which associates a set of latent causes or features with the observed data. Factorial models usually assume that each feature has a single occurrence in a given data point. However  there are data such as images where latent features have multiple occurrences  e.g. a visual object class can have multiple instances shown in the same image. To deal with such cases  we present a probability model over non-negative integer valued matrices with possibly unbounded number of columns. This model can play the role of the prior in an nonparametric Bayesian learning scenario where both the latent features and the number of their occurrences are unknown. We use this prior together with a likelihood model for unsupervised learning from images using a Markov Chain Monte Carlo inference algorithm.,The Inﬁnite Gamma-Poisson Feature Model

Michalis K. Titsias

School of Computer Science 
University of Manchester  UK
mtitsias@cs.man.ac.uk

Abstract

We present a probability distribution over non-negative integer valued matrices
with possibly an inﬁnite number of columns. We also derive a stochastic process
that reproduces this distribution over equivalence classes. This model can play
the role of the prior in nonparametric Bayesian learning scenarios where multiple
latent features are associated with the observed data and each feature can have
multiple appearances or occurrences within each data point. Such data arise nat-
urally when learning visual object recognition systems from unlabelled images.
Together with the nonparametric prior we consider a likelihood model that ex-
plains the visual appearance and location of local image patches. Inference with
this model is carried out using a Markov chain Monte Carlo algorithm.

1 Introduction

Unsupervised learning using mixture models assumes that one latent cause is associated with each
data point. This assumption can be quite restrictive and a useful generalization is to consider factorial
representations which assume that multiple causes have generated the data [11]. Factorial models
are widely used in modern unsupervised learning algorithms; see e.g. algorithms that model text
data [2  3  4]. Algorithms for learning factorial models should deal with the problem of specifying
the size of the representation. Bayesian learning and especially nonparametric methods such as the
Indian buffet process [7] can be very useful for solving this problem.

Factorial models usually assume that each feature occurs once in a given data point. This is inef-
ﬁcient to model the precise generation mechanism of several data such as images. An image can
contain views of multiple object classes such as cars and humans and each class may have multiple
occurrences in the image. To deal with features having multiple occurrences  we introduce a prob-
ability distribution over sparse non-negative integer valued matrices with possibly an unbounded
number of columns. Each matrix row corresponds to a data point and each column to a feature
similarly to the binary matrix used in the Indian buffet process [7]. Each element of the matrix
can be zero or a positive integer and expresses the number of times a feature occurs in a speciﬁc
data point. This model is derived by considering a ﬁnite gamma-Poisson distribution and taking
the inﬁnite limit for equivalence classes of non-negative integer valued matrices. We also present a
stochastic process that reproduces this inﬁnite model. This process uses the Ewens’s distribution [5]
over integer partitions which was introduced in population genetics literature and it is equivalent to
the distribution over partitions of objects induced by the Dirichlet process [1].

The inﬁnite gamma-Poisson model can play the role of the prior in a nonparametric Bayesian learn-
ing scenario where both the latent features and the number of their occurrences are unknown. Given
this prior  we consider a likelihood model which is suitable for explaining the visual appearance and
location of local image patches. Introducing a prior for the parameters of this likelihood model  we
apply Bayesian learning using a Markov chain Monte Carlo inference algorithm and show results in
some image data.

2 The ﬁnite gamma-Poisson model

Let X = {X1  . . .   XN } be some data where each data point Xn is a set of attributes. In section
4 we specify Xn to be a collection of local image patches. We assume that each data point is
associated with a set of latent features and each feature can have multiple occurrences. Let znk
denote the number of times feature k occurs in the data point Xn. Given K features  Z = {znk} is
a N × K non-negative integer valued matrix that collects together all the znk values so as each row
corresponds to a data point and each column to a feature. Given that znk is drawn from a Poisson
with a feature-speciﬁc parameter λk  Z follows the distribution

P (Z|{λk}) =

NYn=1

KYk=1

λznk
k

exp{−λk}
znk!

=

KYk=1

λmk
k

exp{−N λk}
n=1 znk!

QN

 

(1)

that favors sparsity (in a sense that will be explained shortly):

n=1 znk. We further assume that each λk parameter follows a gamma distribution

where mk =PN

G(λk;

λ

  1) =

α
K

α
K −1
k

exp{−λk}
Γ( α
K )

.

(2)

The hyperparameter α itself is given a vague gamma prior G(α; α0  β0). Using the above equations
we can easily integrate out the parameters {λk} as follows

P (Z|α) =

KYk=1

Γ(mk + α
K )
K )(N + 1)mk+ α

Γ( α

n=1 znk!

K QN

 

(3)

which shows that given the hyperparameter α the columns of Z are independent. Note that the above
distribution is exchangeable since reordering the rows of Z does not alter the probability. Also as
K increases the distribution favors sparsity. This can be shown by taking the expectation of the sum
n=1 E(znk) and

of all elements of Z. Since the columns are independent this expectation is KPN

E(znk) is given by

E(znk) =

znkN B(znk;

α
K

 

1
2

) =

α
K

 

∞Xznk=0

(4)

where N B(znk; r  p)  with r > 0 and 0 < p < 1  denotes the negative binomial distribution over
positive integers

N B(znk; r  p) =

Γ(r + znk)
znk!Γ(r)

pr(1 − p)znk  

(5)

that has a mean equal to r(1−p)
. Using Equation (4) the expectation of the sum of znks is αN and
is independent of the number of features. As K increases  Z becomes sparser and α controls the
sparsity of this matrix.

p

There is an alternative way of deriving the joint distribution P (Z|α) according to the following
generative process:

(θ1  . . .   θK) ∼ D(cid:16) α
Ln ∼ P oisson(λ)  (zn1  . . .   znK) ∼(cid:18)

K(cid:17)   λ ∼ G(λ; α  1) 
zn1 . . . znK(cid:19) KYk=1

Ln

θznk
k

  n = 1  . . .   N 

K ) denotes the symmetric Dirichlet. Marginalizing out θ and λ gives rise to the same
where D( α
distribution P (Z|α). The above process generates a gamma random variable and multinomial pa-
rameters and then samples the rows of Z independently by using the Poisson-multinomial pair. The
connection with the Dirichlet-multinomial pair implies that the inﬁnite limit of the gamma-Poisson
model must be related to the Dirichlet process. In the next section we see how this connection is
revealed through the Ewens’s distribution [5].

Models that combine gamma and Poisson distributions are widely applied in statistics. We point out
that the above ﬁnite model shares similarities with the techniques presented in [3  4] that model text
data.

3 The inﬁnite limit and the stochastic process

To express the probability distribution in (3) for inﬁnite many features K we need to consider equiv-
alence classes of Z matrices similarly to [7]. The association of columns in Z with features deﬁnes
an arbitrary labelling of the features. Given that the likelihood p(X|Z) is not affected by relabelling
the features  there is an equivalence class of matrices that all can be reduced to the same standard
form after column reordering. We deﬁne the left-ordered form of non-negative integer valued ma-
trices as follows. We assume that for any possible znk holds znk ≤ c − 1  where c is a sufﬁciently
large integer. We deﬁne h = (z1k . . . zN k) as the integer number associated with column k that is
expressed in a numeral system with basis c. The left-ordered form is deﬁned so as the columns of Z
appear from left to right in a decreasing order according to the magnitude of their numbers.

Starting from Equation (3) we wish to deﬁne the probability distribution over matrices constrained in
a left-ordered standard form. Let Kh be the multiplicity of the column with number h; for example
K0 is the number of zero columns. An equivalence class [Z] consists of
different matri-
ces that they are generated from the distribution in (3) with equal probabilities and can be reduced
to the same left-ordered form. Thus  the probability of [Z] is

K!
PcN −1

h=0 Kh!

P ([Z]) =

K!

PcN −1

h=0 Kh!

KYk=1

Γ(mk + α
K )
K )(N + 1)mk+ α

Γ( α

n=1 znk!

K QN

We assume that the ﬁrst K+ features are represented i.e. mk > 0 for k ≤ K+  while the rest K −K+
features are unrepresented i.e. mk = 0 for k > K+. The inﬁnite limit of (6) is derived by following
a similar strategy with the one used for expressing the distribution over partitions of objects as a
limit of the Dirichlet-multinomial pair [6  9]. The limit takes the following form:

.

(6)

P (Z|α) =

1

h=1 Kh!

PcN −1

αK+

(N + 1)m+α QK+
QK+
k=1QN

k=1(mk − 1)!
n=1 znk!

where m =PK+

k=1 mk. This expression deﬁnes an exchangeable joint distribution over non-negative
integer valued matrices with inﬁnite many columns in a left-ordered form. Next we present a se-
quential stochastic process that reproduces this distribution.

 

(7)

3.1 The stochastic process

The distribution in Equation (7) can be derived from a simple stochastic process that constructs
the matrix Z sequentially so as the data arrive one at each time in a ﬁxed order. The steps of this
stochastic process are discussed below.

When the ﬁrst data point arrives all the features are currently unrepresented. We sample feature
occurrences from the set of unrepresented features as follows. Firstly  we draw an integer number
g1 from the negative binomial N B(g1; α  1
2 ) which has a mean value equal to α. g1 is the total
number of feature occurrences for the ﬁrst data point. Given g1  we randomly select a partition
(z11  . . .   z1K1 ) of the integer g1 into parts1  i.e. z11 + . . . + z1K1 = g1 and 1 ≤ K1 ≤ g1  by
drawing from Ewens’s distribution [5] over integer partitions which is given by

P (z11  . . .   z1K1 ) = αK1

Γ(α)

g1!

Γ(g1 + α)

z11 × . . . × z1K1

g1Yi=1

1
v(1)

i

!

 

(8)

i

where v(1)
is the multiplicity of integer i in the partition (z11  . . .   z1K1 ). The Ewens’s distribution
is equivalent to the distribution over partitions of objects induced by the Dirichlet process and the
Chinese restaurant process since we can derive the one from the other using simple combinatorics
arguments. The difference between them is that the former is a distribution over integer partitions
while the latter is a distribution over partitions of objects.
Let Kn−1 be the number of represented features when the nth data point arrives. For each feature
k  with k ≤ Kn−1  we choose znk based on the popularity of this feature in the previous n − 1 data

1The partition of a positive integer is a way of writing this integer as a sum of positive integers where order

does not matter  e.g. the partitions of 3 are: (3) (2 1) and (1 1 1).

i=1 zik. Particularly  we draw znk from N B(znk; mk  n

given by mk =Pn−1

points. This popularity is expressed by the total number of occurrences for the feature k which is
n+1 ) which has a mean
value equal to mk
n . Once we have sampled from all represented features we need to consider a
sample from the set of unrepresented features. Similarly to the ﬁrst data point  we ﬁrst draw an
n+1 )  and subsequently we select a partition of that integer by drawing
integer gn from N B(gn; α  n
from the Ewens’s formula. This process produces the following distribution:

P (Z|α) =

1

i=1 v(1)

i

Qg1

! × . . . ×QgN

i=1 v(N )

i

!

αK+

(N + 1)m+α QK+
QK+
k=1QN

k=1(mk − 1)!
n=1 znk!

 

(9)

where {v(n)
i } are the integer-multiplicities for the nth data point which arise when we draw from
the Ewens’s distribution. Note that the above expression does not have exactly the same form as the
distribution in Equation (7) and is not exchangeable since it depends on the order the data arrive.
However  if we consider only the left-ordered class of matrices generated by the stochastic process
then we obtain the exchangeable distribution in Equation (7). Note that a similar situation arises
with the Indian buffet process.

3.2 Conditional distributions

When we combine the prior P (Z|α) with a likelihood model p(X|Z) and we wish to do in-
ference over Z using Gibbs-type sampling  we need to express the conditionals of the form
P (znk|Z−(nk)  α) where Z−(nk) = Z \ znk. We can derive such conditionals by taking limits
of the conditionals for the ﬁnite model or by using the stochastic process.
Suppose that for the current value of Z  there exist K+ represented features i.e. mk > 0 for

k ≤ K+. Let m−n k = Pen6=n zenk. When m−n k > 0  the conditional of znk is given by

In all different cases  we need a special conditional that samples from
N B(znk; m−n k  N
new features2 and accounts for all k such that m−n k = 0. This conditional draws an integer num-
ber from N B(gn; a  N
N +1 ) and then determines the occurrences for the new features by choosing a
partition of the integer gn using the Ewens’s distribution. Finally the conditional p(α|Z)  which can
be directly expressed from Equation (7) and the prior of α  is given by

N +1 ).

p(α|Z) ∝ G(α; α0  β0)

αK+

(N + 1)α .

(10)

Typically the likelihood model does not depend on α and thus the above quantity is also the posterior
conditional of α given data and Z.

4 A likelihood model for images

An image can contain multiple objects of different classes. Each object class can have more than
one occurrences  i.e. multiple instances of the class may appear simultaneously in the image. Un-
supervised learning should deal with the unknown number of object classes in the images and also
the unknown number of occurrences of each class in each image separately. If object classes are the
latent features  what we wish to infer is the underlying feature occurrence matrix Z. We consider
an observation model that is a combination of latent Dirichlet allocation [2] and Gaussian mixture
models. Such a combination has been used before [12]. Each image n is represented by dn local
patches that are detected in the image so as Xn = (Yn  Wn) = {(yni  wni)  i = 1  . . .   dn}. yni
is the two-dimensional location of patch i and wni is an indicator vector (i.e. is binary and satisﬁes
ni = 1) that points into a set of L possible visual appearances. X  Y   and W denote all
the data the locations and the appearances  respectively. We will describe the probabilistic model
starting from the joint distribution of all variables which is given by

PL

ℓ=1 wℓ

joint = p(α)P (Z|α)p({θk}|Z)×

NYn=1"p(πn|Zn)p(mn  Σn|Zn)

dnYi=1

P (sni|πn)P (wni|sni  {θk})p(yni|sni  mn  Σn)# .

(11)

2Features of this kind are the unrepresented features (k > K+) as well as all the unique features that occur

only in the data point n (i.e. m−n k = 0  but znk > 0).

Z

α

{θk}

πn

(mn  Σn)

sni

wni

yni

dn

N

Figure 1: Graphical model for the joint distribution in Equation (11).

The graphical representation of this distribution is depicted in Figure 1. We now explain all the
pieces of this joint distribution following the causal structure of the graphical model. Firstly  we
generate α from its prior and then we draw the feature occurrence matrix Z using the inﬁnite
gamma-Poisson prior P (Z|α). The matrix Z deﬁnes the structure for the remaining part of the
model. The parameter vector θk = {θk1  . . .   θkL} describes the appearance of the local patches W
for the feature (object class) k. Each θk is generated from a symmetric Dirichlet so as the whole
k=1 D(θk|γ)  where γ is the hyperparameter of
the symmetric Dirichlet and it is common for all features. Note that the feature appearance param-
eters {θk} depend on Z only through the number of represented features K+ which is obtained by
counting the non-zero columns of Z.
The parameter vector πn = {πnkj} deﬁnes the image-speciﬁc mixing proportions for the mixture
model associated with image n. To see how this mixture model arises  notice that a local patch in
image n belongs to a certain occurrence of a feature. We use the double index kj to denote the j

set of {θk} vectors is drawn from p({θk}|Z) =QK+

occurrence of feature k where j = 1  . . .   znk and k ∈ {ek : znek > 0}. This mixture model has
Mn =PK+

k=1 znk components  i.e. as many as the total number of feature occurrences in image n.
The assignment variable sni = {skj
ni}  which takes Mn values  indicates the feature occurrence of
patch i. πn is drawn from a symmetric Dirichlet given by p(πn|Zn) = D(πn|β/Mn)  where Zn
denotes the nth row of Z and β is a hyperparameter shared by all images. Notice that πn depends
only on the nth row of Z.
The parameters (mn  Σn) determine the image-speciﬁc distribution for the locations {yni} of the
local patches in image n. We assume that each occurrence of a feature forms a Gaussian cluster
of patch locations. Thus yni follows a image-speciﬁc Gaussian mixture with Mn components. We
assume that the component kj has mean mnkj and covariance Σnkj. mnkj describes object location
and Σnkj object shape. mn and Σn collect all the means and covariances of the clusters in the image
n. Given that any object can be anywhere in the image and have arbitrary scale and orientation 
(mnkj  Σnkj) should be drawn from a quite vague prior. We use a conjugate normal-Wishart prior
for the pair (mnkj  Σnkj) so as

p(mn  Σn|Zn) = Yk:znk>0

znkYj=1

N (mnkj|µ  τ Σnkj)W (Σ−1

nkj|v  V ) 

(12)

where (µ  τ  v  V ) are the hyperparameters shared by all features and images. The assignment sni
which determines the allocation of a local patch in a certain feature occurrence follows a multino-
ni. Similarly the observed data pair (wni  yni) of a

j=1(πnkj)skj

mial: P (sni|πn) = Qk:znk>0Qznk

local image patch is generated according to

P (wni|sni  {θk}) =

K+Yk=1

LYℓ=1

θ

ni Pznk
wℓ
kℓ

j=1 skj

ni

and

p(yni|sni  mn  Σn) = Yk:znk>0

znkYj=1

[N (yni|mnkj  Σnkj)]skj
ni .

The hyperparameters (γ  β  µ  τ  v  V ) take ﬁxed values that give vague priors and they are not
depicted in the graphical model shown in Figure 1.

Since we have chosen conjugate priors  we can analytically marginalize out from the joint distri-
bution all the parameters {πn}  {θk}  {mn} and {Σn} and obtain p(X  S  Z  α). Marginalizing
out the assignments S is generally intractable and the MCMC algorithm discussed next produces
samples from the posterior P (S  Z  α|X).

4.1 MCMC inference

Inference with our model involves expressing the posterior P (S  Z  α|X) over the feature occur-
rences Z  the assignments S and the parameter α. Note that the joint P (S  Z  α  X) factorizes
n=1 P (Sn|Zn)p(Yn|Sn  Zn) where Sn denotes the assign-
ments associated with image n. Our algorithm uses mainly Gibbs-type sampling from conditional
posterior distributions. Due to space limitations we brieﬂy discuss the main points of this algorithm.

according to p(α)P (Z|α)P (W |S  Z)QN

nk − zold

nk | ≤ 1. Initially Z is such that Mn =PK+

The MCMC algorithm processes the rows of Z iteratively and updates its values. A single step can
change an element of Z by one so as |znew
k=1 znk ≥
1  for any n which means that at least one mixture component explains the data of each image. The
proposal distribution for changing znks ensures that this constraint is satisﬁed.
Suppose we wish to sample a new value for znk using the joint model p(S  Z  α  X). Simply witting
P (znk|S  Z−(nk)  α  X) is not useful since when znk changes the number of states the assignments
Sn can take also changes. This is clear since znk is a structural variable that affects the number of
k=1 znk of the mixture model associated with image n and assignments Sn.
On the other hand the dimensionality of the assignments S−n = S \ Sn of all other images is not
affected when znk changes. To deal with the above we marginalize out Sn and we sample znk from
the marginalized posterior conditional P (znk|S−n  Z−(nk)  α  X) which is computed according to

components Mn =PK+
P (znk|S−n  Z−(nk)  α  X) ∝ P (znk|Z−(nk)  α)XSn

P (W |S  Z)p(Yn|Sn  Zn)P (Sn|Zn) 

(13)

where P (znk|Z−n k  α) for the inﬁnite case is computed as described in section 3.2 while computing
the sum requires an approximation. This sum is a marginal likelihood and we apply importance
sampling using as an importance distribution the posterior conditional P (Sn|S−n  Z  W  Yn) [10].
Sampling from P (Sn|S−n  Z  W  Yn) is carried out by applying local Gibbs sampling moves and
global Metropolis moves that allow two occurrences of different features to exchange their data
clusters. In our implementation we consider a single sample drawn from this posterior distribution
n is a sample accepted
so that the sum is approximated by P (W |S∗
after a burn in period. Additionally to scans that update Z and S we add few Metropolis-Hastings
steps that update the hyperparameter α using the posterior conditional given by Equation (10).

n  S−n  Z)p(Yn|S∗

n  Zn) and S∗

5 Experiments

In the ﬁrst experiment we use a set of 10 artiﬁcial images. We consider four features that have
the regular shapes shown in Figure 2. The discrete patch appearances correspond to pixels and
can take 20 possible grayscale values. Each feature has its own multinomial distribution over the
appearances. To generate an image we ﬁrst decide to include each feature with probability 0.5.
Then for each included feature we randomly select the number of occurrences from the range [1  3].
For each feature occurrence we select the pixels using the appearance multinomial and place the
respective feature shape in a random location so that feature occurrences do not occlude each other.
The ﬁrst row of Figure 2 shows a training image (left)  the locations of pixels (middle) and the
discrete appearances (right). The MCMC algorithm was initialized with K+ = 1  α = 1 and
zn1 = 1  n = 1  . . .   10. The third row of Figure 2 shows how K+ (left) and the sum of all znks
(right) evolve through the ﬁrst 500 MCMC iterations. The algorithm in the ﬁrst 20 iterations has

training image n

locations Yn

appearances Wn

1 3 3 1

3 2 3 0

0 2 1 2

Figure 2: The ﬁrst row shows a training image (left)  the locations of pixels (middle) and the discrete
appearances (right). The second row shows the localizations of all feature occurrences in three
images. Below of each image the corresponding row of Z is also shown. The third row shows how
K+ (left) and the sum of all znks (right) evolve through the ﬁrst 500 MCMC iterations.

Figure 3: The left most plot on the ﬁrst row shows the locations of detected patches and the bounding
boxes in one of the annotated images. The remaining ﬁve plots show examples of detections and
localizations of the three most dominant features (including the car-category) in ﬁve non-annotated
images.

visited the matrix Z that was used to generate the data and then stabilizes. For 86% of the samples
K+ is equal to four. For the state (Z  S) that is most frequently visited  the second row of Figure
2 shows the localizations of all different feature occurrences in three images. Each ellipse is drawn
using the posterior mean values for a pair (mnkj  Σnkj) and illustrates the predicted location and
shape of a feature occurrence. Note that ellipses with the same color correspond to the different
occurrences of the same feature.
In the second experiment we consider 25 real images from the UIUC3 cars database. We used the
patch detection method presented in [8] and we constructed a dictionary of 200 visual appearances
by clustering the SIFT [8] descriptors of the patches using K-means. Locations of detected patches
are shown in the ﬁrst row (left) of Figure 3. We partially labelled some of the images. Particularly 
for 7 out of 25 images we annotated the car views using bounding boxes (Figure 3). This allows
us to specify seven elements of the ﬁrst column of the matrix Z (the ﬁrst feature will correspond
to the car-category). These znks values plus the assignments of all patches inside the boxes do not
change during sampling. Also the patches that lie outside the boxes in all annotated images are not
allowed to be part of car occurrences. This is achieved by applying partial Gibbs sampling updates
and Metropolis moves when sampling the assignments S. The algorithm is initialized with K+ = 1 
after 30 iterations stabilizes and then ﬂuctuates between nine to twelve features. To keep the plots
uncluttered  Figure 3 shows the detections and localizations of only the three most dominant features
(including the car-category) in ﬁve non-annotated images. The red ellipses correspond to different
occurrences of the car-feature  the green ones to a tree-feature and the blue ones to a street-feature.

6 Discussion

We presented the inﬁnite gamma-Poisson model which is a nonparametric prior for non-negative
integer valued matrices with inﬁnite number of columns. We discussed the use of this prior for
unsupervised learning where multiple features are associated with our data and each feature can
have multiple occurrences within each data point. The inﬁnite gamma-Poisson prior can be used for
other purposes as well. For example  an interesting application can be Bayesian matrix factorization
where a matrix of observations is decomposed into a product of two or more matrices with one of
them being a non-negative integer valued matrix.

References

[1] C. Antoniak. Mixture of Dirichlet processes with application to Bayesian nonparametric problems. The

Annals of Statistics  2:1152–1174  1974.

[2] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. JMLR  3  2003.
[3] W. Buntime and A. Jakulin. Applying discrete PCA in data analysis. In UAI  2004.
[4] J. Canny. GaP: A factor model for discrete data. In SIGIR  pages 122–129. ACM Press  2004.
[5] W. Ewens. The sampling theory of selectively neutral alleles. Theoretical Population Biology  3:87–112 

1972.

[6] P. Green and S. Richardson. Modelling heterogeneity with and without the Dirichlet process. Scandina-

vian Journal of Statistics  28:355–377  2001.

[7] T. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In NIPS 18 

2006.

[8] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer

Vision  60(2):91–110  2004.

[9] R. M. Neal. Bayesian mixture modeling.

In 11th International Workshop on Maximum Entropy and

Bayesian Methods of Statistical Analysis  pages 197–211  1992.

[10] M. A. Newton and A. E Raftery. Approximate Bayesian inference by the weighted likelihood bootstrap.

Journal of the Royal Statistical Society  Series B  3:3–48  1994.

[11] E. Saund. A multiple cause mixture model for unsupervised learning. Neural Computation  7:51–71 

1995.

[12] E. Sudderth  A. Torralba  W. T. Freeman  and A. Willsky. Describing Visual Scenes using Transformed

Dirichlet Processes. In NIPS 18  2006.

3available from http://l2r.cs.uiuc.edu/∼cogcomp/Data/Car/.

,Xiaozhi Chen
Kaustav Kundu
Yukun Zhu
Andrew Berneshawi
Huimin Ma
Sanja Fidler
Raquel Urtasun