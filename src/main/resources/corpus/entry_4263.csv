2010,Permutation Complexity Bound on Out-Sample Error,We define a data dependent permutation complexity for a hypothesis set \math{\hset}  which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based like the maximum discrepancy on (dependent) sampling. We prove a uniform bound on the generalization error  as well as a concentration result which means that the permutation estimate can be efficiently estimated.,Permutation Complexity Bound on Out-Sample Error

Malik Magdon-Ismail

Computer Science Department
Rensselaer Ploytechnic Institute

110 8th Street  Troy  NY 12180  USA

magdon@cs.rpi.edu

Abstract

We deﬁne a data dependent permutation complexity for a hypothesis set H  which
is similar to a Rademacher complexity or maximum discrepancy. The permutation
complexity is based (like the maximum discrepancy) on dependent sampling. We
prove a uniform bound on the generalization error  as well as a concentration result
which means that the permutation estimate can be efﬁciently estimated.

1 Introduction

1

i=1(1 − yih(xi)). The out-sample error eout(h) = 1

Assume a standard setting with data D = {(xi  yi)}n
i=1  where (xi  yi) are sampled iid from the joint
distribution p(x  y) on Rd×{±1}. Let H = {h : Rd 7→ {±1}} be a learning model which produces
a hypothesis g ∈ H when given D (we use g for the hypothesis returned by the learning algorithm
and h for a generic hypothesis in H). We assume the 0-1 loss  so the in-sample error is ein(h) =
2nPn
E [(1 − yh(x))]; the expectation is over
the joint distribution p(x  y). We wish to bound eout(g). To do so  we will bound |eout(h) − ein(h)|
uniformly over H for all distributions p(x  y); however  the bound itself will depend on the data 
and hence the distribution. The classic distribution independent bound is the VC-bound (Vapnik and
Chervonenkis  1971); the hope is that by taking into account the data one can get a tighter bound.
The data dependent permutation complexity1 for H is deﬁned by:

2

PH(n  D) = Eπ"max

h∈H

1
n

yπih(xi)# .

n

Xi=1

Here  π is a uniformly random permutation on {1  . . .   n}. PH(n  D) is an intuitively plausible
measure of the complexity of a model  measuring its ability to correlate with a random permutation
of the target values. The difﬁculty in analyzing PH is that {yπi} is an ordered random sample
from y = [y1  . . .   yn]  sampled without replacement; as such it is a dependent sampling from a
data driven distribution. Analogously  we may deﬁne the bootstrap complexity  using the bootstrap
is independent and uniformly random over y1  . . .   yn:
distribution B on y  where each sample yB
i

BH(n  D) = EB"max

h∈H

1
n

yB

i h(xi)# .

n

Xi=1

When the average y-value ¯y = 0  the bootstrap complexity is exactly the Rademacher complexity
(Bartlett and Mendelson  2002; Fromont  2007; K¨a¨ari¨ainen and Elomaa  2003; Koltchinskii  2001;
Koltchinskii and Panchenko  2000; Lozano  2000; Lugosi and Nobel  1999; Massart  2000):

RH(n  D) = E

r"max

h∈H

1
n

rih(xi)#  

n

Xi=1

1For simplicity  we assume that H is closed under negation; generally  all the results hold with the complex-

ities deﬁned using absolute values  so for example PH(n  D) = Eπ ˆmaxh∈H ˛
˛

1

n Pn

˛˜.
i=1 yπi h(xi)˛

1

where r is a random vector of i.i.d. fair ±1’s. The maximum discrepancy complexity measure
∆H(n  D) is similar to the Rademacher complexity  with the expectation over r being restricted to
those r satisfyingPn

i=1 ri = 0 

n

∆H(n  D) = E

r"max

h∈H

1
n

Xi=1

riyih(xi)# .

When ¯y = 0  the permutation complexity is the maximum discrepancy; the permutation complexity
is to maximum discrepancy as the bootstrap complexity is to the Rademacher complexity. The per-
mutation complexity maintains a little more information regarding the distribution. Indeed we prove
a uniform bound very similar to the uniform bound obtained using the Rademacher complexity:
Theorem 1 With probability at least 1 − δ  for every h ∈ H 

eout(h) ≤ ein(h) + PH(n  D) + 13r 1

2n

ln

6
δ

.

The probability in this theorem is with respect to the data distribution. The challenge in proving this
theorem is to accomodate samples (yπi) constructed according to the data  and in a dependent way.
Using our same proof technique  one can also obtain a similar uniform bound with the bootstrap
complexity  where the samples are independent  but according to the data. The proof starts with the
standard ghost sample and symmetrization argument. We then need to handle the data dependent
sampling in the complexity measure  and this is done by introducing a second ghost data set to
govern the sampling. The crucial aspect about sampling according to a second ghost data set is
that the samples are now independent of the data; this is acceptable  provided the two methods of
sampling are close enough; this is what constitutes the meat of the proof given in Section 2.2.
For a given permutation π  one can compute maxh∈H
i=1 yπih(xi) using an empirical risk
minimization; however  the computation of the expectation over permutations is an exponential task 
which needless to say is not feasible. Fortunately  we can establish that the permutation complexity
is concentrated around its expectation  which means that in principle a single permutation sufﬁces
to compute the permutation complexity. Let π be a single random permutation.

nPn

1

Theorem 2 For an absolute constant c ≤ 6 +p2/ ln 2  with probability at least 1 − δ 

n

PH(n  D) ≤ sup

h∈H

1
n

Xi=1

yπih(xi) + cr 1

2n

ln

3
δ

.

The probability here is with respect to random permutations (i.e.  it holds for any data set). It is easy
to show concentration for the bootstrap complexity about its expectation – this follows from Mc-
Diarmid’s inequality because the samples are independent. The complication with the permutation
complexity is that the samples are not independent. Nevertheless  we can show the concentration
indirectly by ﬁrst relating the two complexities for any data set  and then using the concentration of
the bootstrap complexity (see Section 2.3).

Empirical Results. For a single random permutation  with probability at least 1 − δ 

eout(h) ≤ ein(h) + sup

h∈H

1
n

n

Xi=1

yπih(xi) + O r 1

n

ln

1

δ! .

Asymptotically  one random permutation sufﬁces; in practice  one should average over a few. In-
deed  a permutation based validation estimate for model selection has been extensively tested (see
Magdon-Ismail and Mertsalov (2010) for details); for classiﬁcation  this permutation estimate is the
permutation complexity after removing a bias term. It outperformed LOO-cross validation and the
Rademacher complexity on real data. We restate those results here  comparing model selection us-
ing the permutation estimate versus using the Rademacher complexity (using real data sets from the
UCI Machine Learning repository (Asuncion and Newman  2007)). The performance metric is the
regret when compared to oracle model selection on a held out set (lower regret is better). We con-
sidered two model selection tasks: choosing the number of leafs in a decision tree; and  selecting k
in the k-nearest neighbor method. The results reported here are averaged over several (10 000 or
more) random splits of the data into a training set and held out set. We deﬁne a learning episode as
an empirical risk minimization on O(n) data points.

2

10 Learning Episodes
k-NN

100 Learning Episodes
k-NN

Data

Abalone
Ionosphere
M.Mass
Parkinsons
Pima Ind.
Spambase
Transfusion
WDBC
Diffusion

n

3 132
263
667
144
576
3 450
561
426
2 665

Decision Trees
Rad.
Perm.
0.02
0.02
0.18
0.19
0.06
0.06
0.34
0.40
0.07
0.07
0.07
0.07
0.08
0.09
0.24
0.37
0.02
0.03

Perm. Rad.
0.09
0.12
0.75
0.84
0.11
0.12
0.32
0.44
0.12
0.15
0.43
0.54
0.12
0.19
0.33
0.50
0.04
0.06

Decision Trees
Rad.
Perm.
0.02
0.02
0.16
0.17
0.05
0.05
0.34
0.41
0.07
0.07
0.06
0.07
0.08
0.09
0.23
0.34
0.02
0.03

Perm. Rad.
0.04
0.04
0.70
0.83
0.11
0.11
0.33
0.43
0.11
0.14
0.43
0.55
0.12
0.19
0.34
0.51
0.03
0.06

The permutation complexity appears to dominate most of the time (especially when n is small);
and  when it fails to dominate  it is as good or only slightly worse than the Rademacher estimate.
It is not surprising that as n increases  the performances of the various complexities converges.
Asymptotically  one can deduce several relationships between them  for example the maximum
discrepancy can be asymptotically bounded from above and below by the Rademacher complexity.
Similarly  (see Lemma 5)  the bootstrap and permutation complexities are equal  asymptotically. The
small sample performance of the complexities as bounding tools is not easy to discern theoretically 
which is where the empirics comes in. An intuition for why the permutation complexity performs
relatively well is because it maintains more of the true data distribution. Indeed  the permutation
method for validation was found to work well empirically  even in regression (Magdon-Ismail and
Mertsalov  2010); however  our permutation complexity bound only applies to classiﬁcation.

Open Questions. Can the permutation complexity bound be extended beyond classiﬁcation to (for
example) regression with bounded loss? The permutation complexity displays a bias for severely
unbalanced data; can this bias be removed. We conjecture that it should be possible to get a better
uniform bound in terms of Eπ[maxh∈H

i=1(yπi − ¯y)h(xi)].

1

nPn

1.1 Related Work

Out-sample error estimation has extensive coverage  both in the statistics and learning commuities.

(i) Statistical methods try to estimate the out-sample error asymptotically in n  and give consis-
tent estimates under certain model assumptions  for example: ﬁnal prediction error (FPE) (Akaike 
1974); Generalized Cross Validation (GCV) (Craven and Wahba  1979); or  covariance-type penal-
ties (Efron  2004; Wang and Shen  2006). Statistical methods tend to work well when the model has
been well speciﬁed. Such methods are not our primary focus.

(ii) Sampling methods  such as leave-one-out cross validation (LOO-CV)  try to estimate the out-
sample error directly. Cross validation is perhaps the most used validation method  dating as far
back as 1931 (Larson  1931; Wherry  1931  1951; Katzell  1951; Cureton  1951; Mosier  1951;
Stone  1974). The permutation complexity uses a “sampled” data set on which to compute the
complexity; other than this superﬁcial similarity  the estimates are inherently different.

(iii) Bounds. The most celebrated uniform bound on generalization error is the distribution inde-
pendent bound of Vapnik-Chervonenkis (VC-bound) (Vapnik and Chervonenkis  1971). Since the
VC-dimension may be hard to compute  empirical estimates have been suggested  (Vapnik et al. 
1994). The VC-bound is optimal among distribution independent bounds; however  for a particular
distribution  it could be sub-optimal. Several data dependent bounds have already been mentioned 
which can typically be estimated in-sample via optimization: maximum discrepancy (Bartlett et al. 
2002); Rademacher-style penalties (Bartlett and Mendelson  2002; Fromont  2007; K¨a¨ari¨ainen and
Elomaa  2003; Koltchinskii  2001; Koltchinskii and Panchenko  2000; Lozano  2000; Lugosi and
Nobel  1999; Massart  2000); margin based bounds  for example (Shawe-Taylor et al.  1998). Gen-
eralizations to Gaussian and symmetric  bounded variance r have also been suggested  (Bartlett and
Mendelson  2002; Fromont  2007) . One main application of such bounds is that any such approx-
imate estimate of the out-sample error (which satisﬁes some bound of the form of the permutation
complexity bound) can be used for model selection  after adding a (small) penalty for the “complex-

3

ity of model selection” (see Bartlett et al. (2002)). In practice  this penalty for the complexity of
model selection is ignored (as in Bartlett et al. (2002)).

(iv) Permutation Methods are not new to statistics (Good  2005; Golland et al.  2005; Wiklund et al. 
2007). Golland et al. (2005) show concentration for a permutation based test of signiﬁcance for the
improved performance of a more complex model  using the Rademacher complexity. We directly
give a uniform bound for the out-sample error in terms of a permutation complexity  answering a
question posed in (Golland et al.  2005) which asks whether there is a direct link between permu-
tation statistics and generalization errors. Indeed  Magdon-Ismail and Mertsalov (2010) construct a
permutation estimate for validation which they empirically test in both classiﬁcation and regression
problems. For classiﬁcation  their estimate is related to the permutation complexity.

Most relevant to this work are Rademacher penalties and the corresponding (sampling without re-
placement) maximum discrepancy. Bartlett et al. (2002) give a uniform bound using the maximum
discrepancy which is in some sense a uniform bound based on a sampling without replacement
(dependent sampling); however  the sampling distribution is ﬁxed  independent of the data. It is
illustrative to brieﬂy sketch the derivation of the maximum discrepancy bound. Adapting the proof

in Bartlett et al. (2002) and ignoring terms which are O(cid:0)( 1

(a)

n ln 1

δ )1/2(cid:1)  with probability at least 1−δ:

eout(h) ≤ ein(h) + sup

n

n

(c)

1
2n

′
ih(x

(b)
= ein(h) + ED sup

h∈H(ED′
h∈H( 1
≤ ein(h) + ED D′ max
h∈H


≤ ein(h) + ED D′ max

= ein(h) + ED∆H(n  D)

yih(xi) − y′

h∈H{eout(h) − ein(h)}
Xi=1
Xi=1
yih(xi) − y′
Xi=1
yih(xi) − y′
≤ ein(h) + ∆H(n  D) 

≤ ein(h) + ED sup
h∈H{eout(h) − ein(h)}  
i))  
i))  
i)


′
ih(x

′
ih(x

1
n

2n

n/2

(e)

(d)

 

(a) follows from McDiarmid’s inequality because eout(h) − ein(h) is stable to a single point pertur-
bation for every h  hence the supremum is also stable; in (b) appears a ghost data set and (c) follows
by convexity of the supremum; in (d)  we break the sum into two equal parts  which adds the factor
of two; ﬁnally  (e) follows again by McDiarmid’s inequality because ∆H is stable to single point
perturbations. The discrepancy automatically drops out from using the ghost sample; this does not
happen with data dependent permutation sampling  which is where the difﬁculty lies.

2 Permutation Complexity Uniform Bound

We now give the proof of Theorem 1. We will adapt the standard ghost sample approach in VC-type
proofs and the symmetrization trick in (Gin´e and Zinn  1984) which has greatly simpliﬁed VC-style
proofs. In general  high probability results are with respect to the distribution over data sets. Our
main bounding tool will be McDiarmid’s inequality:

Lemma 1 (McDiarmid (1989)) Let Xi ∈ Ai be independent; suppose f :Qi
|f (x) − f (x1  . . .   xj−1  z  xj+1  . . .   xn)| ≤ cj 

sup

(x1  ... xn )∈Qi Ai

Ai 7→ R satisﬁes

z∈Aj

for j = 1  . . .   n. Then  with probability at least 1 − δ 

f (X1  . . .   Xn) ≤ Ef (X1  . . .   Xn) +vuut

1
2

n

Xi=1

c2
i ln

1
δ

.

i=1 c2

i ln 1

δ by using −f in McDiarmid’s inequality.

We also obtain Ef ≤ f +q 1

2Pn

4

2.1 Permutation Complexity

The out-sample permutation complexity of a model is:

PH(n) = EDPH(n  D) = ED π"max

h∈H

1
n

yπih(xi)#  

n

Xi=1

j).

′
j  y′

where the expectation is over the data D = (x1  y1)  . . .   (xn  yn) and a random permutation π. Let
D′ differ from D only in one example  (xj   yj) → (x
Lemma 2 |PH(n  D) − PH(n  D′)| ≤ 4
n .
Proof: For any permutation π and every h ∈ H  the sumPn
going from D to D′; thus  the maximum over h ∈ H changes by at most 4.
Lemma 2 together with McDiarmid’s inequality implies a concentration of PH(n  D) about PH(n) 
which means we can work with PH(n  D) instead of the unknown PH(n).
Corollary 1 With probability at least 1 − δ  PH(n) ≤ PH(n  D) + 4r 1

i=1 yπih(xi) changes by at most 4 in

1
δ

2n

ln

.

2 (1 − 1

Since ein(h) = 1
π can be used to compute PH(n  D) for a particular permutation π.
y
2.2 Bounding the Out-Sample Error

nPn

i=1 yih(xi))  the empirical risk minimizer gπ on the permuted targets

To bound suph∈H{eout(h) − ein(h)}  we ﬁrst use the standard ghost sample and symmetrization
arguments typical of modern generalization error proofs (see for example Bartlett and Mendelson
(2002); Shawe-Taylor and Cristianini (2004)). Let r
Lemma 3 With probability at least 1 − δ:
h∈H{eout(h) − ein(h)} ≤ ED D′"sup

n] be a ±1 sequence.

r′′
i (yih(xi) − y′

i)))# +r 1

h∈H( 1

1   . . .   r′′

′′ = [r′′

′
ih(x

sup

1
δ

2n

2n

ln

n

.

Xi=1

Proof: We proceed as in the proof of the maximum discrepancy bound in Section 1.1:

sup
h∈H{eout(h) − ein(h)}

(a)

≤ ED D′"sup
= ED D′"sup

h∈H( 1
h∈H( 1

(b)

2n

2n

n

n

Xi=1
Xi=1

yih(xi) − y′

′
ih(x

r′′
i (yih(xi) − y′

ln

2n

i))# +r 1
i)))# +r 1

′
ih(x

2n

1
δ

 

ln

1
δ

.

n ln 1

In (a)  the O(( 1
at most 1
because r′′
expectation (it amounts to relabeling of random variables).

δ )1/2) term is from applying McDiarmid’s inequality because ein(h) changes by
n if one data point changes  and so the supremum changes by at most that much; (b) follows
i = −1 corresponds to exchanging xi  x
i in the expectation which does not change the
′

Lemma 3 holds for an arbitrary sequence r
tation with respect to r

′′  for arbitrarily distributed r

′′ which is independent of D  D′; we can take the expec-

′′  as long as r

′′ is independent of D  D′.

2.2.1 Generating Permutations with ±1 Sequences
i = yπiyi; then 
Fix y; for a given permutation π  deﬁne a corresponding ±1 sequence r
i yi. Thus  given y  for each of the n! permutations π1  . . .   πn!  we have a correspond-
yπi = rπ
ing ±1 sequence r
πn!} (there may be
repetitions as two different permutations may result in the same sequence of ±1 values); we thus
have a mapping from permutations to the ±1 sequences in Sy. If r  a random vector of ±1s  is

πi; we thus obtain a multiset of sequences Sy = {r

π by rπ

π1   . . .   r

5

uniform on Sy  then r.y (componentwise product) is uniform over the permutations of y. We say
that Sy generates the permutations on y. Similarly  we can deﬁne Sy′  the generator of permutations
′. Unfortunately  Sy  Sy′ depend on D  D′  and so we can’t take the expectation uniformly over
on y
(for example) r ∈ Sy. We can overcome this by introducing a second ghost sample D′′ to “approx-
imately” generate the permutations for y  y
Theorem 3 With probability at least 1 − 5δ 

′  ultimately allowing us to prove the main result.

sup

h∈H{eout(h) − ein(h)} ≤ PH(n) + 9r 1

2n

ln

1
δ

 

We obtain Theorem 1 by combining Theorem 3 with Corollary 1.

2.2.2 Proof of Theorem 3

Let D′′ be a second  independent ghost sample  and Sy′′ the generator of permutations for y
Lemma 3  take the expectation over r

′′ uniform on Sy′′. The ﬁrst term on the RHS becomes

′′. In

ED D′ D′′

1

n!Xπ "sup

h∈H

1
2n

n

Xi=1

r′′
i (π)(yih(xi) − y′

′
ih(x

i))#  

(1)

where each permutation π induces a particular sequence r
′′(π) ∈ Sy′′ (previously we used rπ
i
which is now ri(π)). Consider the sequences r  r
′ corresponding to the permutations on y and y
′.
The next lemma will ultimately relate the expectation over permutations in the second ghost data set
to the permutations over D  D′.
Lemma 4 With probability at least 1 − 2δ  there is a one-to-one mapping from the sequences in
Sy′′ = {r

′′(π)}π to Sy = {r(π)}π such that

(r′′
i − ri(r

1
2n

n

Xi=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

≤r 8

n

ln

1
δ

 

′′))yih(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

for every r
mapped). Similarly  there exists such a mapping from Sy′′ to Sy′.

′′ ∈ Sy′′ and every h ∈ H (we write r(r

′′) to denote the sequence r ∈ Sy to which r

′′ is

The probability here is with respect to y  y
ing sets Sy′′  Sy′  and Sy are essentially equivalent.

′ and y

′′. This lemma says that the permutation generat-

i = y′′

πiy′′

Proof: We can (without loss of generality) reorder the points in D′′ so that the ﬁrst k′′ are +1  so
k′′ = +1  and the remaining are −1. Similarily  we can order the points in D so that
y′′
1 = ··· = y′′
the ﬁrst k are +1  so y1 = ··· = yk = +1. We now construct the mapping from Sy′′ to Sy as
′′(π) ∈ Sy′′ to r(π) ∈ Sy. This mapping is clearly
follows. For a given permutation π  we map r
bijective since every permutation corresponds uniquely to a sequence in Sy (and Sy′′).
πi or yi 6= y′′
′′ disagree
Let ri = yπiyi and r′′
on exactly |k − k′′| locations (and similarly for yπ and y
π)  the number of locations where r and r
′′
′′
disagree is therefore at most 2|k − k′′|. Thus  for any r
′′ and any h ∈ H 
1
Xi=1
2n
Xi=1

′′))yih(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

i   either yπi 6= y′′

i . If ri 6= r′′

′′)||yih(xi)|

i . Since y and y

(r′′
i − ri(r

|r′′
i − ri(r

|r′′
i − ri(r

2|k − k′′|

Xi=1

′′)| ≤

≤

n

n

n

.

n

1
2n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
We observe thatPn
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

n

i=1(yi − y′′
1
Xi=1
2n
where zi = yi − y′′
We consider the function f (z1  . . .   zn) = 1

(r′′
i − ri(r
i . Since y and y

=

1
2n
i ) = 2(k − k′′) and so 
1
n

′′))yih(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
≤(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
nPn

′′ are identically distributed  zi are independent and zero mean.
i=1 zi. Since zi ∈ {0 ±2}  if you change one of the

n

Xi=1

(yi − y′′

1
n

n

Xi=1

i )(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

 

zi(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

6

zi  f changes by at most 4

n   and so the conditions hold to apply McDiarmid’s inequality to f . Thus 

using the symmetry of zi  with probability at least 1 − 2δ (cid:12)(cid:12)

′′)
′′). We can rewrite the internal summand in the expression of Equation (1) using the equality

Given D  D′  D′′  assume the mappings which are known to exist by the previous lemma are r(r
and r
′(r
r′′
i (yih(xi) − y′
′
i).
ih(x
Using Lemma 4  we can  with probability at least 1 − 2δ  bound the term which involves (r′′
i −
′′)) in Equation (1); and  similarly  with probability at least 1− 2δ  we bound the term involving
ri(r
′′)). Thus  with probability at least 1 − 4δ  the expression in Equation (1) is bounded by:
(r′′
i − r′

′′))yih(xi) − (r′′

′
i)) = (r′′
ih(x

i − ri(r

′′) + ri(r

i − r′

′′) + r′

′′))y′

i(r

i(r

i(r

8

i=1 zi(cid:12)(cid:12) ≤q 1
nPn

2n ln 1
δ .

ED D′ D′′

1

n!Xπ "sup

h∈H

1
2n

n

Xi=1

(ri(r

′′)yih(xi) − r′
i(r

′′)y′

′
ih(x

i))# + 2r 8

n

ln

1
δ

 

′′(π) cycles through the sequences in Sy′′. Since the mappings r(r
′′).y cycles through the permutations of y  and similarly for r
′(r

where r
one  r(r
under negation  we ﬁnally obtain the bound

′′) and r
′′).y

′(r

′′) are one-to-
′. Since H is closed
i)# + 2r 8

1
δ

ln

n

;

′
y′
πih(x

Using this in Lemma 3  with probability at least 1 − 5δ 

ED

1

n!Xπ "sup

h∈H

1
2n

n

n

1

1
2n

n!Xπ "sup

yπi h(xi)# + ED′

Xi=1
Xi=1
h∈H{eout(h) − ein(h)} ≤ PH(n) + 9r 1

sup

h∈H

2n

ln

1
δ

.

Commentary.
(i) The permutation complexity bound needs empirical risk minimization  which is
notoriously hard; however  if the same algorithm is used for learning as well as computing P  we can
view it as optimization over a constrained hypothesis set (this is especially so with regularization);
the bounds now hold. (ii) The same proof technique can be used to get a bootstrap complexity
bound; the result is similar. (iii) One could bound PH for VC function classes  showing that this
data dependent bound is asymptotically no worse than a VC-type bound. Bounding permutation
complexity on speciﬁc domains could follow the methods in Bartlett and Mendelson (2002).

2.3 Estimating PH(n  D) Using a Single Permutation
We now prove Theorem 2  which states that one can essentially estimate PH(n  D) (an average over
all permutations) by suph∈H
i=1 yπih(xi)  using just a single randomly selected permutation π.
Our proof is indirect: we will link PH to the bootstrap complexity BH. The bootstrap complexity
is concentrated via an easy application of McDiarmid’s inequality  which will ultimately allow us to
conclude that the permutation estimate is also concentrated. The bootstrap distribution B constructs
a random sequence y
B of n independent uniform samples from y1  . . .   yn; the key requirement is
that yB

i are independent samples. There are nn (not distinct) possible bootstrap sequences.

nPn

1

Lemma 5 |BH(n  D) − PH(n  D)| ≤
Proof: Let k be the number of yi which are +1; we condition on κ  the number of +1 in the
bootstrap sample. Suppose B|κ samples uniformly among all sequences with κ entries being +1.

.

1
√n

The key observation is that we can generate all samples uniformly according to B|κ by ﬁrst gener-
ating a random permutation and then selecting randomly |k − κ| +1’s (or −1’s) to ﬂip  so:

BH(n  D) = Eκ EB|κ" sup

h∈H

1
n

EB|κ" sup

h∈H

1
n

κ# = EF|k−κ|

n

Xi=1

yB

i h(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

7

κ#  

n

yB

Xi=1

i h(xi)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
Eπ "sup
Xi=1

1
n

h∈H

n

yF

πih(xi)# .

(F denotes the ﬂipping random process.) Since yF

1
n

sup
h∈H

Thus 

n

Xi=1

yπi h(xi) −

2|k − κ|

n

≤ sup

h∈H

1
n

n

Xi=1

πi

differs from yπi in exactly |k − κ| positions 
2|k − κ|
yF
πih(xi) ≤ sup

yπi h(xi) +

1
n

n

n

.

h∈H

Xi=1

|BH(n  D) − PH(n  D)| ≤

Eκ [|k − κ|].

2√n (because κ is binomial)  the result follows.

2
n

Since Eκ[|k − κ|] ≤pVar[k − κ] ≤ 1

In addition to furthering our cause toward the proof of Theorem 2  Lemma 5 is interesting in its own
right  because it says that permutation and bootstrap sampling are asymptotically similar. The nice
thing about the bootstrap estimate is that the expectation is over independent yB
n . Since the
bootstrap complexity changes by at most 2
n if you change one sample  by McDiarmid’s inequality 
Lemma 6 For a random bootstrap sample B  with probability at least 1 − δ 

1   . . .   yB

BH(n  D) ≤ sup

h∈H

1
n

n

Xi=1

yB

i h(xi) + 2r 1

2n

ln

1
δ

.

We now prove concentration for estimating PH(n  D). As in the proof of Lemma 5  generate y
B
B; κ is binomial. Now  generate a random
in two steps. First generate κ  the number of +1’s in y
π  and ﬂip (as appropriate) a randomly selected |k − κ| entries  where k is the number
permutation y
of +1’s in y.
If we apply McDiarmid’s inequality to the function which equals the number of
+1’s  we immediately get that with probability at least 1 − 2δ  |κ − k| ≤ ( 1
δ )1/2. Thus  with
π in at most (2n ln 1
probability at least 1 − 2δ  y
δ )1/2 positions. Each ﬂip changes
the complexity by at most 2  hence  with probability at least 1 − 2δ 

B differs from y

2 n ln 1

n

1
δ
We conclude that for a random permutation π  with probability at least 1 − 3δ 

yπih(xi) + 4r 1

yB
i h(xi) ≤ sup

Xi=1

Xi=1

sup
h∈H

1
n

1
n

h∈H

2n

ln

.

n

BH(n  D) ≤ sup

h∈H

1
n

n

Xi=1

yπi h(xi) + 6r 1

2n

ln

1
δ

.

Now  combining with Lemma 5  we obtain Theorem 2 after a little algebra  because δ < 1.
We have not only established that PH is concentrated  but we have also established a general con-
nection between the permutation and bootstrap based estimates. In this particular case  we see that
sampling with and without replacement are very closely related. In practice  sampling without re-
placement can be very different  because one is never in the truly asymptotic regime. Along that
vein  even though we have concentration  it pays to take the average over a few permutations.

References
Akaike  H. (1974). A new look at the statistical model identiﬁcation. IEEE Trans. Aut. Cont.  19 

716–723.

Asuncion  A. and Newman  D. (2007). UCI machine learning repository.
Bartlett  P. L. and Mendelson  S. (2002). Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3  463–482.

Bartlett  P. L.  Boucheron  S.  and Lugosi  G. (2002). Model selection and error estimation. Machine

Learning  48  85–113.

Craven  P. and Wahba  G. (1979). Smoothing noisy data with spline functions. Numerische Mathe-

matik  31  377–403.

Cureton  E. E. (1951). Symposium: The need and means of cross-validation: II approximate linear

restraints and best predictor weights. Education and Psychology Measurement  11  12–15.

8

Efron  B. (2004). The estimation of prediction error: Covariance penalties and cross-validation.

Journal of the American Statistical Association  99(467)  619–632.

Fromont  M. (2007). Model selection by bootstrap penalization for classiﬁcation. Machine Learning 

66(2-3)  165–207.

Gin´e  E. and Zinn  J. (1984). Some limit theorems for empirical processes. Annals of Prob.  12 

929–989.

Golland  P.  Liang  F.  Mukherjee  S.  and Panchenko  D. (2005). Permutation tests for classiﬁcation.

Learning Theory  pages 501–515.

Good  P. (2005). Permutation  parametric  and bootstrap tests of hypotheses. Springer.
K¨a¨ari¨ainen  M. and Elomaa  T. (2003). Rademacher penalization over decision tree prunings. In In

Proc. 14th European Conference on Machine Learning  pages 193–204.

Katzell  R. A. (1951). Symposium: The need and means of cross-validation: III cross validation of

item analyses. Education and Psychology Measurement  11  16–22.

Koltchinskii  V. (2001). Rademacher penalties and structural risk minimization. IEEE Transactions

on Information Theory  47(5)  1902–1914.

Koltchinskii  V. and Panchenko  D. (2000). Rademacher processes and bounding the risk of function
learning. In E. Gine  D. Mason  and J. Wellner  editors  High Dimensional Prob. II  volume 47 
pages 443–459.

Larson  S. C. (1931). The shrinkage of the coefﬁcient of multiple correlation. Journal of Education

Psychology  22  45–55.

Lozano  F. (2000). Model selection using Rademacher penalization. In Proc. 2nd ICSC Symp. on

Neural Comp.

Lugosi  G. and Nobel  A. (1999). Adaptive model selection using empirical complexities. Annals of

Statistics  27  1830–1864.

Magdon-Ismail  M. and Mertsalov  K. (2010). A permutation approach to validation. In Proc. 10th

SIAM International Conference on Data Mining (SDM).

Massart  P. (2000). Some applications of concentration inequalities to statistics. Annales de la

Facult´e des Sciencies de Toulouse  X  245–303.

McDiarmid  C. (1989). On the method of bounded differences. In Surveys in Combinatorics  pages

148–188. Cambridge University Press.

Mosier  C. I. (1951). Symposium: The need and means of cross-validation: I problem and designs

of cross validation. Education and Psychology Measurement  11  5–11.

Shawe-Taylor  J. and Cristianini  N. (2004). Kernel Methods for Pattern Analysis. Camb. Univ.

Press.

Shawe-Taylor  J.  Bartlett  P. L.  Williamson  R. C.  and Anthony  M. (1998). Structural risk mini-
mization over data dependent hierarchies. IEEE Transactions on Information Theory  44  1926–
1940.

Stone  M. (1974). Cross validatory choice and assessment of statistical predictions. Journal of the

Royal Statistical Society  36(2)  111–147.

Vapnik  V. N. and Chervonenkis  A. (1971). On the uniform convergence of relative frequencies of

events to their pr obabilities. Theory of Probability and its Applications  16  264–280.

Vapnik  V. N.  Levin  E.  and Le Cun  Y. (1994). Measuring the VC-dimension of a learning machine.

Neural Computation  6(5)  851–876.

Wang  J. and Shen  X. (2006). Estimation of generalization error: random and ﬁxed inputs. Statistica

Sinica  16  569–588.

Wherry  R. J. (1931). A new formula for predicting the shrinkage of the multiple correlation coefﬁ-

cient. Annals of Mathematical Statistics  2  440–457.

Wherry  R. J. (1951). Symposium: The need and means of cross-validation: III comparison of cross
validation with statistical inference of betas and multiple r from a single sample. Education and
Psychology Measurement  11  23–28.

Wiklund  S.  Nilsson  D.  Eriksson  L.  Sjostrom  M.  Wold  S.  and Faber  K. (2007). A randomiza-

tion test for PLS component selection. Journal of Chemometrics  21(10-11)  427–439.

9

,Ke Sun
Jun Wang
Stephane Marchand-Maillet
Jian Zhao
Lin Xiong
Panasonic Karlekar Jayashree
Jianshu Li
Fang Zhao
Zhecan Wang
Panasonic Sugiri Pranata
Panasonic Shengmei Shen
Shuicheng Yan
Jiashi Feng