2017,Alternating Estimation for Structured High-Dimensional Multi-Response Models,We consider the problem of learning high-dimensional multi-response linear models with structured parameters. By exploiting the noise correlations among different responses  we propose an alternating estimation (AltEst) procedure to estimate the model parameters based on the generalized Dantzig selector (GDS). Under suitable sample size and resampling assumptions  we show that the error of the estimates generated by AltEst  with high probability  converges linearly to certain minimum achievable level  which can be tersely expressed by a few geometric measures  such as Gaussian width of sets related to the parameter structure. To the best of our knowledge  this is the first non-asymptotic statistical guarantee for such AltEst-type algorithm applied to estimation with general structures.,Alternating Estimation for Structured

High-Dimensional Multi-Response Models

Sheng Chen
Arindam Banerjee
Dept. of Computer Science & Engineering

University of Minnesota  Twin Cities

{shengc banerjee}@cs.umn.edu

Abstract

We consider the problem of learning high-dimensional multi-response linear mod-
els with structured parameters. By exploiting the noise correlations among different
responses  we propose an alternating estimation (AltEst) procedure to estimate
the model parameters based on the generalized Dantzig selector (GDS). Under
suitable sample size and resampling assumptions  we show that the error of the
estimates generated by AltEst  with high probability  converges linearly to certain
minimum achievable level  which can be tersely expressed by a few geometric
measures  such as Gaussian width of sets related to the parameter structure. To the
best of our knowledge  this is the ﬁrst non-asymptotic statistical guarantee for such
AltEst-type algorithm applied to estimation with general structures.

1

Introduction

Multi-response (a.k.a. multivariate) linear models [2  8  20  21] have found numerous applications in
real-world problems  e.g. expression quantitative trait loci (eQTL) mapping in computational biology
[28]  land surface temperature prediction in climate informatics [17]  neural semantic basis discovery
in cognitive science [30]  etc. Unlike simple linear model where each response is a scalar  one obtains
a response vector at each observation in multi-response model  given as a (noisy) linear combinations
of predictors  and the parameter (i.e.  coefﬁcient vector) to learn can be either response-speciﬁc
(i.e.  allowed to be different for every response)  or shared by all responses. The multi-response
model has been well studied under the context of the multi-task learning [10]  where each response is
coined as a task. In recent years  the multi-task learning literature have largely focused on exploring
the parameter structure across tasks via convex formulations [15  3  26]. Another emphasis area in
multi-response modeling is centered around the exploitation of the noise correlation among different
responses [35  36  29  40  42]  instead of assuming that the noise is independent for each response.
To be speciﬁc  we consider the following multi-response linear models with m real-valued outputs 

yi = Xiθ∗ + ηi 

ηi ∼ N (0  Σ∗)  

(1)
where yi ∈ Rm is the response vector  Xi ∈ Rm×p consists of m p-dimensional feature vectors 
and ηi ∈ Rm is a noise vector sampled from a multivariate zero-mean Gaussian distribution with
covariance Σ∗. For simplicity  we assume Diag(Σ∗) = Im×m throughout the paper. The m
responses share the same underlying parameter θ∗ ∈ Rp  which corresponds to the so-called pooled
model [19]. In fact  this seemingly restrictive setting is general enough to encompass the model
with response-speciﬁc parameters  which can be realized by block-diagonalizing rows of Xi and
stacking all coefﬁcient vectors into a “long” vector. Under the assumption of correlated noise  the
true noise covariance structure Σ∗ is usually unknown. Therefore it is typically required to estimate
the parameter θ∗ along with the covariance Σ∗. In practice  we observe n data points  denoted by

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

D = {(Xi  yi)}n

(cid:16) ˆθMLE  ˆΣMLE

(cid:17)

i=1  and the maximum likelihood estimator (MLE) is simply as follows 

= argmin
θ∈Rp  Σ(cid:23)0

log |Σ| +

1
2

1
2n

2 (yi − Xiθ)

(2)

(cid:13)(cid:13)(cid:13)Σ− 1

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)2

2

Although being convex w.r.t. either θ or Σ when the other is ﬁxed  the optimization problem
associated with the MLE is jointly non-convex for θ and Σ. A popular approach to dealing with such
problem is alternating minimization (AltMin)  i.e.  alternately solving for θ (and Σ) while keeping
Σ (and θ) ﬁxed. The AltMin algorithm for (2) iteratively performs two simple steps  solving least
squares for θ and computing empirical noise covariance for Σ. Recent work [24] has established
the non-asymptotic error bound of this approach for (2) with a brief extension to sparse parameter
setting using iterative hard thresholding method [25]. But they did not allow more general structure
of the parameter. Previous works [35  29  33] also considered the regularized MLE approaches for
multi-response models with sparse parameters  which are solved by AltMin-type algorithms as well.
Unfortunately  none of those works provide ﬁnite-sample statistical guarantees for their algorithms.
AltMin technique has also been applied to many other problems  such as matrix completion [23] 
sparse coding [1]  and mixed linear regression [41]  with provable performance guarantees. Despite
the success of AltMin  most existing works are dedicated to recovering unstructured sparse or
low-rank parameters  with little attention paid to general structures  e.g.  overlapping sparsity [22] 
hierarchical sparsity [27]  k-support sparsity [4]  etc.
In this paper  we study the multi-response linear model in high-dimensional setting  i.e.  sample size n
is smaller than the problem dimension p  and the coefﬁcient vector θ∗ is assumed to possess a general
low-complexity structure  which can be essentially captured by certain norm (cid:107) · (cid:107) [5]. Structured
estimation using norm regularization/minimization has been extensively studied for simple linear
models over the past decade  and recent advances manage to characterize the estimation error for
convex approaches including Lasso-type (regularized) [38  31  6] and Dantzig-type (constrained)
estimator [7  12  14]  via a few simple geometric measures  e.g.  Gaussian width [18  11] and
restricted norm compatibility [31  12]. Here we propose an alternating estimation (AltEst) procedure
for ﬁnding the true parameters  which essentially alternates between estimating θ through the
generalized Dantzig selector (GDS) [12] using norm (cid:107) · (cid:107) and computing the approximate empirical
noise covariance for Σ. Our analysis puts no restriction on what the norm can be  thus the AltEst
framework is applicable to general structures. In contrast to AltMin  our AltEst procedure cannot
be casted as a minimization of some joint objective function for θ and Σ  thus is conceptually more
general than AltMin. For the proposed AltEst  we provide the statistical guarantees for the iterate
ˆθt with the resampling assumption (see Section 2)  which may justify the applicability of AltEst
technique to other problems without joint objectives for two set of parameters. Speciﬁcally  we
show that with overwhelming probability  the estimation error (cid:107) ˆθt − θ∗(cid:107)2 for generally structured
θ∗ converges linearly to a minimum achievable error given sub-Gaussian design under moderate
sample size. With a straightforward intuition  this minimum achievable error can be tersely expressed
by the aforementioned geometric measures which simply depend on the structure of θ∗. Moreover 
our analysis implies the error bound for single response high-dimensional models as a by-product
−1/2
∗ X( ˆθt − θ∗)]
[12]. Note that the analysis in [24] focuses on the expected prediction error E[Σ
for unstructured θ∗  which is related but different from our (cid:107) ˆθt − θ∗(cid:107)2 for generally structured θ∗.
Compared with the error bound derived for unstructured θ∗ in [24]  our result also yields better
dependency on sample size by removing the log n factor  which seems unnatural to appear.
The rest of the paper is organized as follows. We elaborate our AltEst algorithm in Section 2  along
with the resampling assumption. In Section 3  we present the statistical guarantees for AltEst. We
provide experimental results in Section 4 to support our theoretical development. Finally we conclude
in Section 5. Due to space limitations  all proofs are deferred to the supplementary material.

2 Alternating Estimation for High-Dimensional Multi-Response Models

Given the high-dimensional setting for (1)  it is natural to consider the regularized MLE for (1) by
adding the norm (cid:107) · (cid:107) to (2)  which captures the structural information of θ∗ in (1) 

(cid:13)(cid:13)(cid:13)Σ− 1

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)2

2

2 (yi − Xiθ)

+ γn(cid:107)θ(cid:107)  

(3)

(cid:16) ˆθ  ˆΣ

(cid:17)

= argmin
θ∈Rp  Σ(cid:23)0

log |Σ| +

1
2

1
2n

2

where γn is a tuning parameter. Using AltMin the update of (3) can be given as

n(cid:88)

(cid:16)

1
2n
yi − Xi

i=1

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13) ˆΣ
− 1
t−1(yi − Xiθ)
(cid:17)(cid:16)
(cid:17)T

2

2

yi − Xi

ˆθt

ˆθt

n(cid:88)

i=1

ˆθt = argmin
θ∈Rp

ˆΣt =

1
n

+ γn(cid:107)θ(cid:107)

(4)

(5)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗

n(cid:88)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

The update of ˆθt is basically solving a regularized least squares problem  and the new ˆΣt is obtained
by computing the approximated empirical covariance of the residues evaluated at ˆθt. In this work 
we consider an alternative to (4)  the generalized Dantzig selector (GDS) [12]  which is given by

(cid:107)θ(cid:107)

t−1(Xiθ − yi)
ˆΣ−1

≤ γn  

i=1

s.t.

XT
i

ˆθt = argmin
θ∈Rp

(6)
where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107). Compared with (4)  GDS has nicer geometrical properties 
which is favored in the statistical analysis. More importantly  since iteratively solving (6) followed by
covariance estimation (5) no longer minimizes a speciﬁc objective function jointly  the updates go
beyond the scope of AltMin  leading to our broader alternating estimation (AltEst) framework  i.e. 
alternately estimating one parameter by suitable approaches while keeping the other ﬁxed. For the
ease of exposition  we focus on the m ≤ n scenario  so that ˆΣt can be easily computed in closed
form as shown in (5). When m > n and Σ−1∗
is sparse  it is beneﬁcial to directly estimate Σ−1∗
using more advanced estimators [16  9]. Especially the CLIME estimator [9] enjoys certain desirable
properties  which ﬁts into our AltEst framework but not AltMin  and our AltEst analysis does not
rely on the particular estimator we use to estimate noise covariance or its inverse. The algorithmic
details are given in Algorithm 1  for which it is worth noting that every iteration t uses independent
new samples  D2t−1 and D2t in Step 3 and 4  respectively. This assumption is known as resampling 
which facilitates the theoretical analysis by removing the statistical dependency between iterates.
Several existing works beneﬁt from such assumption when analyzing their AltMin-type algorithms
[23  32  41]. Conceptually resampling can be implemented by partitioning the whole dataset into T
subsets  though it is unusual to do so in practice. Loosely speaking  AltEst (AltMin) with resampling
is an approximation of the practical AltEst (AltMin) with a single dataset D used by all iterations.
For AltMin  attempts have been made to directly analyze its practical version without resampling 
by studying the properties of the joint objective [37]  which come at the price of invoking highly
sophisticated mathematical tools. This technique  however  might fail to work for AltEst since the
procedure is not even associated with a joint objective. In the next section  we will leverage such
resampling assumption to show that the error of ˆθt generated by Algorithm 1 will converge to a
small value with high probability. We again emphasize that the AltEst framework may work for other
suitable estimators for (θ∗  Σ∗) although (5) and (6) are considered in our analysis.

Algorithm 1 Alternating Estimation with Resampling
Input: Number of iterations T   Datasets D1 = {(Xi  yi)}n
1: Initialize ˆΣ0 = Im×m
2: for t:= 1 to T do
3:
4:
5: end for
6: return ˆθT

Solve the GDS (6) for ˆθt using dataset D2t−1
Compute ˆΣt according to (5) using dataset D2t

i=1  . . .   D2T = {(Xi  yi)}2T n

i=(2T−1)n+1

3 Statistical Guarantees for Alternating Estimation

In this section  we establish the statistical guarantees for our AltEst algorithm. The road map for the
analysis is to ﬁrst derive the error bounds separately for both (5) and (6)  and then combine them
through AltEst procedure to show the error bound of ˆθt. Throughout the analysis  the design X is
assumed to centered  i.e.  E[X] = 0m×p. λmax(·) and λmin(·) are used to denote the largest and
smallest eigenvalue of a real symmetric matrix. Before presenting the results  we provide some basic
but important concepts. First of all  we give the deﬁnition of sub-Gaussian matrix X.

3

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)vT Γ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ψ2

Deﬁnition 1 (Sub-Gaussian Matrix) X ∈ Rm×p is sub-Gaussian if the ψ2-norm below is ﬁnite 
(7)

≤ κ < +∞  

− 1
u XT u

sup

=

2

|||X|||ψ2

v∈Sp−1  u∈Sm−1

where Γu = E[XT uuT X]. Further we assume there exist constants µmin and µmax such that

0 < µmin ≤ λmin(Γu) ≤ λmax(Γu) ≤ µmax < +∞  

∀ u ∈ Sm−1

(8)

The deﬁnition (7) is also used in earlier work [24]  which assumes the left end of (8) implicitly.
Lemma 1 gives an example of sub-Gaussian X  showing that condition (7) and (8) are reasonable.
Lemma 1 Assume that X ∈ Rm×p has dependent anisotropic rows such that X = Ξ 1
2   where
Ξ ∈ Rm×m encodes the dependency between rows  ˜X ∈ Rm×p has independent isotropic rows  and
Λ ∈ Rp×p introduces the anisotropy. In this setting  if each row of ˜X satisﬁes |||˜xi|||ψ2
≤ ˜κ  then
condition (7) and (8) hold with κ = C ˜κ  µmin = λmin(Ξ)λmin(Λ)  and µmax = λmax(Ξ)λmax(Λ).

2 ˜XΛ 1

The recovery guarantee of GDS relies on an important notion called restricted eigenvalue (RE). In
multi-response setting  it is deﬁned jointly for designs Xi and a noise covariance Σ as follows.

Deﬁnition 2 (Restricted Eigenvalue Condition) The designs X1  X2  . . .   Xn and the covariance
Σ together satisfy the restricted eigenvalue condition for set A ⊆ Sp−1 with parameter α > 0  if

(cid:32)

n(cid:88)

i=1

1
n

v∈A vT
inf

(cid:33)

i Σ−1Xi
XT

v ≥ α .

(9)

Apart from RE condition  the analysis of GDS is carried out on the premise that tuning parameter γn
is suitably selected  which we deﬁne as “admissible”.

Deﬁnition 3 (Admissible Tuning Parameter) The γn for GDS (6) is said to be admissible if γn is
chosen such that θ∗ belongs to the constraint set  i.e. 

i Σ−1(Xiθ∗ − yi)
XT

=

i Σ−1ηi
XT

≤ γn

(10)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

For structured estimation  one also needs to characterize the structural complexity of θ∗  and an
appropriate choice is the Gaussian width [18]. For any set A ⊆ Rp  its Gaussian width is given
by w(A) = E [supu∈A (cid:104)u  g(cid:105)]  where g ∼ N (0  Ip×p) is a standard Gaussian random vector. In
the analysis  the set A of our interests typically relies on the structure of θ∗. Previously Gaussian
width has been applied to statistical analyses for various problems [11  6  39]  and recent works
[34  13] show that Gaussian width is computable for many structures. For the rest of the paper  we
use C  C0  C1 and so on to denote universal constants  which are different from context to context.

3.1 Estimation of Coefﬁcient Vector
In this subsection  we focus on estimating θ∗  i.e.  Step 3 of Algorithm 1  using GDS of the form 

ˆθ = argmin
θ∈Rp

(cid:107)θ(cid:107)

s.t.

i Σ−1(Xiθ − yi)
XT

≤ γn  

(11)

where Σ is an arbitrary but ﬁxed input noise covariance matrix. The following lemma shows a
deterministic error bound for ˆθ under the RE condition and admissible γn deﬁned in (9) and (10).

Lemma 2 Suppose the RE condition (9) is satisﬁed by X1  . . .   Xn and Σ with α > 0 for the set
A (θ∗) = cone{v | (cid:107)θ∗ + v(cid:107) ≤ (cid:107)θ∗(cid:107) } ∩ Sp−1. If γn is admissible  ˆθ in (11) satisﬁes

(cid:13)(cid:13)(cid:13) ˆθ − θ∗(cid:13)(cid:13)(cid:13)2

≤ 2Ψ(θ∗) · γn
α

 

(12)

in which Ψ(θ∗) is the restricted norm compatibility deﬁned as Ψ(θ∗) = supv∈A(θ∗)

(cid:107)v(cid:107)
(cid:107)v(cid:107)2

.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

4

From Lemma 2  we can ﬁnd that the L2-norm error is mainly determined by three quantities–Ψ(θ∗) 
γn and α. The restricted norm compatibility Ψ(θ∗) purely hinges on the geometrical structure of
θ∗ and (cid:107) · (cid:107)  thus involving no randomness. On the contrary  γn and α need to satisfy their own
conditions  which are bound to deal with random Xi and ηi. The set A(θ∗) involved in RE condition
and restricted norm compatibility has relatively simple structure  which will favor the derivation of
error bound for varieties of norms [13]. If RE condition fails to hold  i.e. α = 0  the error bound is
meaningless. Though the error is proportional to the user-speciﬁed γn  assigning arbitrarily small
value to γn may not be admissible. Hence  in order to further derive the recovery guarantees for GDS 
we need to verify RE condition and ﬁnd the smallest admissible value of γn.
Restricted Eigenvalue Condition: Firstly the following lemma characterizes the relation between
the expectation and empirical mean of XT Σ−1X.
Lemma 3 Given sub-Gaussian X ∈ Rm×p with its i.i.d. copies X1  . . .   Xn  and covariance
Σ ∈ Rm×m with eigenvectors u1  . . .   um  let Γ = E[XT Σ−1X] and ˆΓ = 1
i Σ−1Xi.
− 1
Deﬁne the set AΓj for A ⊆ Sp−1 and each Γj = E[XT ujuT
j v ∈
cone(A)}. If n ≥ C1κ4 · maxj
have

(cid:8)w2(AΓj )(cid:9)  with probability at least 1 − m exp(−C2n/κ4)  we

i=1 XT
j X] as AΓj = {v ∈ Sp−1 | Γ

(cid:80)n

n

2

vT ˆΓv ≥ 1
2

vT Γv 

∀ v ∈ A .

(13)

Instead of w(AΓj )  ideally we want the condition above on n to be characterized by w(A)  which
can be easier to compute in general. The next lemma accomplishes this goal.
Lemma 4 Let κ0 be the ψ2-norm of standard Gaussian random vector and Γu = E[XT uuT X] 
where u ∈ Sm−1 is ﬁxed. For AΓu deﬁned in Lemma 3  we have

w(AΓu) ≤ Cκ0

(14)
Lemma 4 implies that the Gaussian width w(AΓj ) appearing in Lemma 3 is of the same order as
w(A). Putting Lemma 3 and 4 together  we can obtain the RE condition for the analysis of GDS.
· (w(A) + 3)2  then the
Corollary 1 Under the notations of Lemma 3 and 4  if n ≥ C1κ2
following inequality holds for all v ∈ A ⊆ Sp−1 with probability at least 1 − m exp(−C2n/κ4) 
(15)

0κ4 · µmax

· Tr(Σ−1)

µmin

(cid:112)µmax/µmin · (w(A) + 3)  

vT ˆΓv ≥ µmin
2

(cid:80)n

i=1 XT

Admissible Tuning Parameter: Finding the admissible γn amounts to estimating the value of
(cid:107) 1
i Σ−1ηi(cid:107)∗ in (10)  which involves random Xi and ηi. The next lemma establishes a
n
high-probability bound for this quantity  which can be viewed as the smallest “safe” choice of γn.
Lemma 5 Assume that Xi is sub-Gaussian and ηi ∼ N (0  Σ∗). The following inequality holds
with probability at least 1 − exp

(cid:17) − C2 exp

(cid:17)

(cid:16)− C2
·(cid:112)Tr (Σ−1Σ∗Σ−1) · w(B)  

1 w2(B)
4ρ2

i Σ−1ηi
XT

√
µmax√
≤ Cκ
n

(16)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

(cid:16)− nτ 2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗

2

where B denotes the unit ball of norm (cid:107) · (cid:107)  ρ = supv∈B (cid:107)v(cid:107)2  and τ = (cid:107)Σ−1Σ
Estimation Error of GDS: Building on Corollary 1  Lemma 2 and 5  the theorem below characterizes
the estimation of GDS for the multi-response linear model.
Theorem 1 Under the setting of Lemma 5  if n ≥ C1κ2
to C2κ

· w(B)  the estimation error of ˆθ given by (11) satisﬁes

(cid:113) µmax Tr(Σ−1Σ∗Σ−1)

· (w(A (θ∗)) + 3)2  and γn is set

2∗ (cid:107)F /(cid:107)Σ−1Σ

0κ4 · µmax

2∗ (cid:107)2.

µmin

1

1

n

(cid:107) ˆθ − θ∗(cid:107)2 ≤ Cκ

(cid:114) µmax

µ2

min

·

(cid:112)Tr (Σ−1Σ∗Σ−1)

Tr (Σ−1)

· Ψ(θ∗) · w(B)

√

n

 

(17)

5

with probability at least 1 − m exp(cid:0)− C3n
role in the error bound through the multiplicative factor ξ(Σ) =(cid:112)Tr (Σ−1Σ∗Σ−1)/ Tr(cid:0)Σ−1(cid:1). By

Remark: We can see from the theorem above that the noise covariance Σ input to GDS plays a

(cid:1) − exp

(cid:17) − C4 exp

(cid:16)− nτ 2

(cid:16)− C2

5 w2(B)
4ρ2

(cid:17)

κ4

.

2

taking the derivative of ξ2(Σ) w.r.t. Σ−1 and setting it to 0  we have

2 Tr2(cid:0)Σ−1(cid:1) Σ∗Σ−1 − 2 Tr(cid:0)Σ−1(cid:1) Tr(cid:0)Σ−1Σ∗Σ−1(cid:1) · Im×m
(cid:113)

Tr4 (Σ−1)

= 0

Then we can verify that Σ = Σ∗ is the solution to the equation above  and thus is the minimizer of
Tr(Σ−1∗ ). This calculation conﬁrms that multi-response regression could
ξ(Σ) with ξ(Σ∗) = 1/
√
beneﬁt from taking into account the noise covariance  and the best performance is achieved when Σ∗
is known. If we perform ordinary GDS by setting Σ = Im×m  then ξ(Σ) = 1/
m. Therefore using
Σ∗ will reduce the error by a factor of
One simple structure of θ∗ to consider for Theorem 1 is the sparsity encoded by L1 norm. Given s-
sparse θ∗  it follows from previous results [31  11] that Ψ(θ∗) = O(
√
s log p)
and w(B) = O(

√
s)  w(A(θ∗)) = O(
log p). Therefore if n ≥ O(s log p)  then with high probability we have

m/ Tr(Σ−1∗ )  compared with ordinary GDS.

(cid:113)

√

∂ξ2(Σ)
∂Σ−1 =

(cid:32)

(cid:114)

(cid:33)

(cid:107) ˆθ − θ∗(cid:107)2 ≤ O

ξ(Σ) ·

s log p

n

(18)

(19)

Implications for Simple Linear Models: Our general result in multi-response scenario implies
some existing results for simple linear models. If we set n = 1 and Σ = Σ∗ = Im×m  i.e.  only one
data point (X  y) is observed and the noise is independent for each response  the GDS is reduced to

ˆθsg = argmin
θ∈Rp

(cid:107)θ(cid:107)

s.t.

(cid:13)(cid:13)XT (Xθ − y)(cid:13)(cid:13)∗ ≤ γ  

which exactly matches that in [12]. To bound its estimation error  we need X to be more structured
beyond the sub-Gaussianity. Essentially we consider the model of X in Lemma 1  where rows of ˜X
are additionally assumed to be identical. For such X  a specialized RE condition is as follows.

Lemma 6 Assume X is deﬁned as in Lemma 1 such that X = Ξ 1
i.i.d. with |||˜xj||| ≤ ˜κ. If mn ≥ C1κ2
1 − exp(−C2mn/˜κ4)  the following inequality is satisﬁed by all v ∈ A ⊆ Sp−1 

2   and rows of ˜X are
λmin(Ξ)λmin(Λ) · (w(A) + 3)2  with probability at least
(cid:16)

0˜κ4 · λmax(Ξ)λmax(Λ)

2 ˜XΛ 1

1

(cid:17) · λmin (Λ) .

vT ˆΓv ≥ m
2

· λmin

2 Σ−1Ξ

1
2

Ξ

(20)

Remark: Lemma 6 characterizes the RE condition for a class of speciﬁcally structured design X. If
we specialize the general RE condition in Corollary 1 for this setting  X = Ξ 1

2   it becomes

2 ˜XΛ 1

n ≥ C1κ2

0˜κ4 λmax(Ξ)λmax(Λ)
λmin(Ξ)λmin(Λ)

(w(A) + 3)2

with probability 1−
m exp(−C2n/˜κ4)
==========⇒ vT ˆΓv ≥ λmin(Ξ)λmin(Λ)

2

Tr(Σ−1)

Comparing the general result above with Lemma 6  there are two striking differences. Firstly  Lemma
6 requires the same sample size of mn rather than n  which improves the general one. Secondly  (20)
holds with much higher probability 1 − exp(−C2mn/˜κ4) instead of 1 − m exp(−C2n/˜κ4).
Given this specialized RE condition  we have the recovery guarantees of GDS for simple linear
models  which encompass the settings discussed in [6  12] as special cases.
Corollary 2 Suppose y = Xθ∗ + η ∈ Rm  where X is described as in Lemma 6  and η ∼ N (0  I).

With probability at least 1 − exp(cid:0)− m
(cid:13)(cid:13)(cid:13) ˆθsg − θ∗(cid:13)(cid:13)(cid:13)2

2

≤ C ˜κ ·

(cid:1) − C2 exp
(cid:115)

(cid:16)− C2

1 w2(B)
4ρ2

(cid:17) − exp(cid:0)−C3m/˜κ4(cid:1)  ˆθsg satisﬁes
· Ψ(θ∗) · w(B)

√

 

(21)

m

λmax(Ξ)λmax(Λ)
λ2
min(Ξ)λ2
min(Λ)

6

(22)

and

(23)

(24)

(cid:107)θ∗ − θ(cid:107)2

2

3.2 Estimation of Noise Covariance

In this subsection  we consider the estimation of noise covariance Σ∗ given an arbitrary parameter
vector θ. When m is small  we estimate Σ∗ by simply using the sample covariance

Theorem 2 reveals the relation between ˆΣ and Σ∗  which is sufﬁcient for our AltEst analysis.
Theorem 2 If n ≥ C 4m · max
Xi is sub-Gaussian  with probability at least 1 − 2 exp(−C1m)  ˆΣ given by (22) satisﬁes

λmin(Σ∗)µmin

4

  κ4(cid:16) λmax(Σ∗)µmax

(cid:17)2(cid:27)

ˆΣ =

(cid:26)

i=1

1
n

(yi − Xiθ) (yi − Xiθ)T .

n(cid:88)
(cid:16)
(cid:113) µmax
λmin(Σ∗) (cid:107)θ∗ − θ(cid:107)2
(cid:17) ≤ 1 + C 2κ2

(cid:112)m/n +
(cid:17) ≥ 1 − C 2κ2

κ0 + κ

2µmax

(cid:112)m/n

(cid:17)4

− 1
2∗

− 1
2∗

0

0

λmin (Σ∗)

Σ
− 1
2∗

ˆΣΣ
− 1
2∗

(cid:16)

λmax

− 1
2∗

Σ

ˆΣΣ

− 1
2∗

(cid:16)

λmin

ˆΣΣ

Remark: If ˆΣ = Σ∗  then λmax(Σ
) = 1. Hence ˆΣ is nearly equal
to Σ∗ when the upper and lower bounds (23) (24) are close to 1. We would like to point out that there
is nothing speciﬁc to the particular form of estimator (22)  which makes AltEst work. Similar results
can be obtained for other methods that estimate the inverse covariance matrix Σ−1∗
instead of Σ∗.
For instance  when m < n and Σ−1∗
is sparse  we can replace (22) with GLasso [16] or CLIME [9] 
and AltEst only requires the counterparts of (23) and (24) in order to work.

) = λmin(Σ

ˆΣΣ

− 1
2∗

− 1
2∗

3.3 Error Bound for Alternating Estimation

Section 3.1 shows that the noise covariance in GDS affects the error bound by the factor ξ(Σ). In
order to bound the error of ˆθT given by AltEst  we need to further quantify how θ affects ξ( ˆΣ).

Lemma 7 If ˆΣ is given as (22) and the condition in Theorem 2 holds  then the inequality below
holds with probability at least 1 − 2 exp(−C1m) 

(cid:16) m

(cid:17) 1

4

n

(cid:114) µmax

λmin (Σ∗)

1 + 2Cκ0

+ 2

(cid:19)

(cid:107)θ∗ − θ(cid:107)2

(25)

Based on Lemma 7  the following theorem provides the error bound for ˆθT given by Algorithm 1.

(cid:18)

ξ

(cid:17) ≤ ξ (Σ∗) ·
(cid:16) ˆΣ
(cid:113) µmax
(cid:113) λmin(Σ∗)
(cid:13)(cid:13)(cid:13) ˆθT − θ∗(cid:13)(cid:13)(cid:13)2

max(Σ∗)
λ2

µ2

min

(cid:18)

≤ emin +

2eorc

(cid:40)

(cid:16)

Theorem 3 Let eorc = C1κ

ξ(Σ∗)·Ψ(θ∗)w(B)

max

4

κ0 + C1
C2

Ψ(θ∗)w(B)

m

and also satisﬁes the condition in Theorem 1  with high probability  the iterate ˆθT returned by
Algorithm 1 satisﬁes

. If n ≥ C 4m·

(cid:19)2(cid:41)

· ξ(Σ∗)Ψ(θ∗)w(B)
m·λmin(Σ∗)

1+2Cκ0( m
n )

1
4

(cid:113) µmax
λmin (Σ∗ )
√

√

(cid:17)4

n

2C1κµmax

(cid:18)

1−2eorc

λmin(Σ∗)µmin

and emin = eorc·

  κ4(cid:16) λmax(Σ∗)µmax
(cid:114) µmax

(cid:17)2
(cid:19)T−1 ·(cid:16)(cid:13)(cid:13)(cid:13) ˆθ1 − θ∗(cid:13)(cid:13)(cid:13)2

λmin (Σ∗)

C2µmin

 

(cid:17)

− emin

(26)

Remark: The three lower bounds for n inside curly braces correspond to three intuitive requirements.
The ﬁrst one guarantees that the covariance estimation is accurate enough  and the other two respec-
tively ensure that the initial error of ˆθ1 and eorc are reasonably small   such that the subsequent errors
can contract linearly. eorc is the estimation error incurred by the following oracle estimator 

ˆθorc = argmin
θ∈Rp

(cid:107)θ(cid:107)

s.t.

i Σ−1∗ (Xiθ − yi)
XT

≤ γn  

(27)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

n

n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∗

which is impossible to implement in practice. On the other hand  emin is the minimum achievable error 
which has an extra multiplicative factor compared with eorc. The numerator of the factor compensates

7

for the error of estimated noise covariance provided that θ = θ∗ is plugged in (22)  which merely
depends on sample size. Since having θ = θ∗ is also unrealistic for (22)  the denominator further
accounts for the ballpark difference between θ and θ∗. As we remark after Theorem 1  if we perform
Tr(Σ−1∗ )/m.
ordinary GDS with Σ set to Im×m in (11)  its error bound eodn satisﬁes eodn = eorc
Tr(Σ−1∗ )/m is independent of n  whereas emin will approach eorc with

Note that this factor
increasing n as the factor between them converges to one.

(cid:113)

(cid:113)

4 Experiments

In this section  we present some experimental results to support our theoretical analysis. Specif-
ically we focus on the sparse structure of θ∗ captured by L1 norm. Throughout the experi-
ment  we ﬁx problem dimension p = 500  sparsity level of θ∗ s = 20  and number of iter-
ations for AltEst T = 5. Entries of design X is generated by i.i.d. standard Gaussians  and
θ∗ = [1  . . .   1
]T . Σ∗ is given as a block diagonal matrix with blocks

 −1  . . .  −1

  0  . . .   0

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

480

(cid:124)

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:104) 1
(cid:105)

10
a
1

(cid:123)(cid:122)

10

a

Σ(cid:48) =
replicated along diagonal  and number of responses m is assumed to be even.
All plots are obtained by averaging 100 trials. In the ﬁrst set of experiments  we set a = 0.8  m = 10
and investigate the error of ˆθt as n varies from 40 to 90. We run AltEst (with and without resampling) 
the oracle GDS  and the ordinary GDS with Σ = I. The results are given in Figure 1.
For the second experiment  we ﬁx the product mn ≈ 500  and let m = 2  4  . . .   10. For our choice
of Σ∗  the error incurred by oracle GDS eorc is the same for every m. We compare AltEst with both
oracle and ordinary GDS  and the result is shown in Figure 2(a) and 2(b).
In the third experiment  we test AltEst under different covariance matrices Σ∗ by varying a from
0.5 to 0.9. m is set to 10 and sample size n is 90. We also compare AltEst against both oracle and
ordinary GDS  and the errors are reported in Figure 2(c) and 2(d).

(a) Error for AltEst

(b) Error for Resampled AltEst

(c) Comparison of Estimators

Figure 1: (a) When n = 40  AltEst is not quite stable due to the large initial error and poor quality of estimated
covariance. Then the errors start to decrease for n ≥ 50. (b) Resampld AltEst does beneﬁt from fresh samples 
and its error is slightly smaller than AltEst as well as more stable when n is small. (c) Oracle GDS outperforms
the others  but the performance of AltEst is also competitive. Ordinary GDS is unable to utilize the noise
correlation  thus resulting in relatively large error. By comparing the two implementations of AltEst  we can see
that resampled AltEst yields smaller error especially when data is inadequate  but their errors are very close if n
is suitably large.

(a) AltEst (for m)

(b) Comparison (for m)

(c) AltEst (for a)

(d) Comparison (for a)

Figure 2: (a) Larger error comes with bigger m  which conﬁrms that emin is increasing along with m when mn
is ﬁxed. (b) The plots for oracle and ordinary GDS imply that eorc and eodn remain unchanged  which matches
the error bounds in Theorem 1. Though emin increases  AltEst still outperform the ordinary GDS by a margin.
(c) The error goes down when the true noise covariance becomes closer to singular  which is expected in view of
Theorem 3. (d) eorc also decreases as a gets larger  and the gap between emin and eodn widens. The deﬁnition of
emin in Theorem 3 indicates that the ratio between emin and eorc is almost a constant because both n and m are
ﬁxed. Here we observe that all the ratios at different a are between 1.05 and 1.1  which supports the theoretical
results. Also  Theorem 1 suggests that eodn does not change as Σ∗ varies  which is veriﬁed here.

8

11.522.533.544.55Iterationt0.040.060.080.10.120.140.160.18NormalizedErrorforˆθtn = 40n = 50n = 60n = 70n = 80n = 9011.522.533.544.55Iterationt0.040.060.080.10.120.140.160.18NormalizedErrorforˆθtn = 40n = 50n = 60n = 70n = 80n = 904045505560657075808590SampleSizen0.040.060.080.10.120.140.16NormalizedErrorOracle GDSResampled AltEstAltEstOrdinary GDS11.522.533.544.55Iterationt0.040.060.080.10.120.140.16NormalizedErrorforˆθtm = 2m = 4m = 6m = 8m = 102345678910NumberofResponsesm0.040.060.080.10.120.140.16NormalizedErrorOracle GDSAltEstOrdinary GDS11.522.533.544.55Iterationt0.040.060.080.10.120.140.16NormalizedErrorforˆθta = 0.9a = 0.8a = 0.7a = 0.6a = 0.50.50.550.60.650.70.750.80.850.9a0.020.040.060.080.10.120.14NormalizedErrorOracle GDSAltEstOrdinary GDS5 Conclusions

In this paper  we propose an alternating estimation (AltEst) procedure for solving the multi-response
linear models in high dimension. Our framework is based on the generalized Dantzig selector (GDS)
and allows for general structures of the parameter vector  whose recovery guarantees are simply
determined by a few geometric measures. Also  by leveraging the noise correlation among responses 
AltEst can achieve signiﬁcantly smaller estimation error than ignoring the noise structure. With
moderate sample size and the resampling assumption  we show that the estimation error will converge
linearly to a minimal achievable error  which is comparable to the one incurred by the oracle estimator.
In the experiment  we demonstrate the numerical superiority of AltEst over the vanilla GDS  and it is
also suggested that the resampled version of AltEst give little beneﬁt in practice and we should better
use all data in every iteration.

Acknowledgements
The research was supported by NSF grants IIS-1563950  IIS-1447566  IIS-1447574  IIS-1422557 
CCF-1451986  CNS- 1314560  IIS-0953274  IIS-1029711  NASA grant NNX12AQ39A  and gifts
from Adobe  IBM  and Yahoo.

References
[1] A. Agarwal  A. Anandkumar  P. Jain  P. Netrapalli  and R. Tandon. Learning sparsely used

overcomplete dictionaries via alternating minimization. CoRR  abs/1310.7991  2013.

[2] T. W. Anderson. An introduction to multivariate statistical analysis. 2003.

[3] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning 

73(3):243–272  2008.

[4] A. Argyriou  R. Foygel  and N. Srebro. Sparse prediction with the k-support norm. In NIPS 

2012.

[5] F. Bach  R. Jenatton  J. Mairal  and G. Obozinski. Convex optimization with sparsity-inducing

norms. Optimization for Machine Learning  5  2011.

[6] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with norm regularization. In

Advances in Neural Information Processing Systems (NIPS)  2014.

[7] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector.

The Annals of Statistics  37(4):1705–1732  2009.

[8] L. Breiman and J. H. Friedman. Predicting multivariate responses in multiple linear regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  59(1):3–54  1997.

[9] T. T. Cai  W. Liu  and X. Luo. A constrained (cid:96)1 minimization approach to sparse precision
matrix estimation. Journal of the American Statistical Association  106(494):594–607  2011.

[10] R. Caruana. Multitask learning. Machine Learning  28(1):41–75  1997.

[11] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[12] S. Chatterjee  S. Chen  and A. Banerjee. Generalized dantzig selector: Application to the

k-support norm. In Advances in Neural Information Processing Systems (NIPS)  2014.

[13] S. Chen and A. Banerjee. Structured estimation with atomic norms: General bounds and

applications. In NIPS  pages 2908–2916  2015.

[14] S. Chen and A. Banerjee. Structured matrix recovery via the generalized dantzig selector. In

Advances in Neural Information Processing Systems  2016.

[15] T. Evgeniou and M. Pontil. Regularized multi–task learning. In KDD  pages 109–117  2004.

9

[16] J. Friedman  T. Hastie  and R. Tibshirani. Sparse inverse covariance estimation with the

graphical lasso. Biostatistics  9(3):432–441  2008.

[17] A. R. Goncalves  P. Das  S. Chatterjee  V. Sivakumar  F. J. Von Zuben  and A. Banerjee.

Multi-task sparse structure learning. In CIKM  pages 451–460  2014.

[18] Y. Gordon. Some inequalities for gaussian processes and applications.

Mathematics  50(4):265–289  1985.

Israel Journal of

[19] W. H. Greene. Econometric Analysis. Prentice Hall  7. edition  2011.

[20] A. J. Izenman. Reduced-rank regression for the multivariate linear model. Journal of multivari-

ate analysis  5(2):248–264  1975.

[21] A. J. Izenman. Modern Multivariate Statistical Techniques: Regression  Classiﬁcation  and

Manifold Learning. Springer  2008.

[22] L. Jacob  G. Obozinski  and J.-P. Vert. Group lasso with overlap and graph lasso. In ICML 

2009.

[23] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimiza-

tion. In STOC  pages 665–674  2013.

[24] P. Jain and A. Tewari. Alternating minimization for regression problems with vector-valued
outputs. In Advances in Neural Information Processing Systems (NIPS)  pages 1126–1134 
2015.

[25] P. Jain  A. Tewari  and P. Kar. On iterative hard thresholding methods for high-dimensional

m-estimation. In NIPS  pages 685–693  2014.

[26] A. Jalali  S. Sanghavi  C. Ruan  and P. K. Ravikumar. A dirty model for multi-task learning. In

Advances in Neural Information Processing Systems (NIPS)  pages 964–972  2010.

[27] R. Jenatton  J. Mairal  G. Obozinski  and F. Bach. Proximal methods for hierarchical sparse

coding. J. Mach. Learn. Res.  12:2297–2334  2011.

[28] S. Kim and E. P. Xing. Tree-guided group lasso for multi-response regression with structured

sparsity  with an application to eqtl mapping. Ann. Appl. Stat.  6(3):1095–1117  2012.

[29] W. Lee and Y. Liu. Simultaneous multiple response regression and inverse covariance matrix
estimation via penalized gaussian maximum likelihood. J. Multivar. Anal.  111:241–255  2012.

[30] H. Liu  M. Palatucci  and J. Zhang. Blockwise coordinate descent procedures for the multi-task

lasso  with applications to neural semantic basis discovery. In ICML  pages 649–656  2009.

[31] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis

of regularized M-estimators. Statistical Science  27(4):538–557  2012.

[32] P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization. In NIPS 

2013.

[33] P. Rai  A. Kumar  and H. Daume. Simultaneously leveraging output and task structures for

multiple-output regression. In NIPS  pages 3185–3193  2012.

[34] N. Rao  B. Recht  and R. Nowak. Universal Measurement Bounds for Structured Sparse Signal
Recovery. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS)  2012.

[35] A. J. Rothman  E. Levina  and J. Zhu. Sparse multivariate regression with covariance estimation.

Journal of Computational and Graphical Statistics  19(4):947–962  2010.

[36] K.-A. Sohn and S. Kim. Joint estimation of structured sparsity and output structure in multiple-
output regression via inverse-covariance regularization. In AISTATS  pages 1081–1089  2012.

[37] R. Sun and Z.-Q. Luo. Guaranteed matrix completion via nonconvex factorization. In FOCS 

2015.

10

[38] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical

Society  Series B  58(1):267–288  1996.

[39] J. A. Tropp. Convex Recovery of a Structured Signal from Independent Random Linear

Measurements  pages 67–101. Springer International Publishing  2015.

[40] M. Wytock and Z. Kolter. Sparse gaussian conditional random ﬁelds: Algorithms  theory  and
application to energy forecasting. In International conference on machine learning  pages
1265–1273  2013.

[41] X. Yi  C. Caramanis  and S. Sanghavi. Alternating minimization for mixed linear regression. In

ICML  pages 613–621  2014.

[42] X.-T. Yuan and T. Zhang. Partial gaussian graphical model estimation. IEEE Transactions on

Information Theory  60:1673–1687  2014.

11

,Syama Sundar Rangapuram
Pramod Kaushik Mudrakarta
Matthias Hein
Walid Krichene
Alexandre Bayen
Peter Bartlett
Sheng Chen
Arindam Banerjee