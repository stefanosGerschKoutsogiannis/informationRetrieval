2012,Algorithms for Learning Markov Field Policies,We present a new graph-based approach for incorporating domain knowledge in reinforcement learning applications. The domain knowledge is given as a weighted graph  or a kernel matrix  that loosely indicates which states should have similar optimal actions. We first introduce a bias into the policy search process by deriving a distribution on policies such that policies that disagree with the provided graph have low probabilities. This distribution corresponds to a Markov Random Field. We then present a reinforcement and an apprenticeship learning algorithms for finding such policy distributions. We also illustrate the advantage of the proposed approach on three problems: swing-up cart-balancing with nonuniform and smooth frictions  gridworlds  and teaching a robot to grasp new objects.,Algorithms for Learning Markov Field Policies

Abdeslam Boularias

Max Planck Institute for Intelligent Systems

boularias@tuebingen.mpg.de

Oliver Kr¨omer  Jan Peters

Technische Universit¨at Darmstadt

{oli jan}@robot-learning.de

Abstract

We use a graphical model for representing policies in Markov Decision Processes.
This new representation can easily incorporate domain knowledge in the form of
a state similarity graph that loosely indicates which states are supposed to have
similar optimal actions. A bias is then introduced into the policy search process
by sampling policies from a distribution that assigns high probabilities to policies
that agree with the provided state similarity graph  i.e. smoother policies. This
distribution corresponds to a Markov Random Field. We also present forward
and inverse reinforcement learning algorithms for learning such policy distribu-
tions. We illustrate the advantage of the proposed approach on two problems:
cart-balancing with swing-up  and teaching a robot to grasp unknown objects.

1

Introduction

Markov Decision Processes (MDP) provide a rich and elegant mathematical framework for solving
sequential decision-making problems. In practice  signiﬁcant domain knowledge is often necessary
for ﬁnding a near-optimal policy in a reasonable amount of time. For example  one needs a suitable
set of basis functions  or features  to approximate the value functions in reinforcement learning and
the reward functions in inverse reinforcement learning. Designing value or reward features can itself
be a challenging problem. The features can be noisy  misspeciﬁed or insufﬁcient  particularly in
certain complex robotic tasks such as grasping and manipulating objects. In this type of applications 
the features are mainly acquired through vision  which is inherently noisy. Many features are also
nontrivial  such as the features related to the shape of an object  used for calculating grasp stability.
In this paper  we show how to overcome the difﬁcult problem of designing precise value or reward
features. We draw our inspiration from computer vision wherein similar problems have been efﬁ-
ciently solved using a family of graphical models known as Markov Random Fields (MRFs) (Kohli
et al.  2007; Munoz et al.  2009). We start by specifying a graph that loosely indicates which pairs of
states are supposed to have similar actions under an optimal policy. In an object manipulation task
for example  the states correspond to the points of contact between the robot hand and the object
surface. A state similarity graph can be created by sampling points on the surface of the object and
connecting each point to its k nearest neighbors using the geodesic or the Euclidean distance. The
adjacency matrix of this graph can be interpreted as the Gram matrix of a kernel that can be used
to approximate the optimal value function. Kernels have been widely used before in reinforcement
learning (Ormoneit & Sen  1999)  however  they were used for approximating the values of different
policies in a search for an optimal policy. Therefore  the kernels should span not only the optimal
value function  but also the values of intermediate policies.
In this paper  kernels will be used for a different purpose. We only require that the kernel spans the
value function of an optimal policy. Therefore  the value function of an optimal policy is assumed to
have a low approximation error  measured by the Bellman error  using that kernel. Subsequently  we
derive a distribution on policies  wherein the probability of a policy is proportional to its estimated
value  and inversely proportional to its Bellman error. In other terms  the Bellman error is used
as a surrogate function for measuring how close a policy is to an optimal one. We show that this

1

probability distribution is an MRF  and use a Markov chain Monte Carlo algorithm for sampling
policies from it. We also describe an apprenticeship learning algorithm based on the same principal.
A preliminary version of some parts of this work was presented in (Boularias et al.  2012).

at time t  Vπt:H (s) =(cid:80)H

2 Notations
Formally  a ﬁnite-horizon Markov Decision Process (MDP) is a tuple (S A  T  R  H  γ)  where S
is a set of states and A is a set of actions  T is a transition function with T (s  a  s(cid:48)) = P (st+1 =
s(cid:48)|st = s  at = a) for s  s(cid:48) ∈ S  a ∈ A  and R is a reward function where R(s  a) is the reward given
for action a in state s. To ease notation and without loss of generality  we restrict our theoretical
analysis to the case where rewards depend only on states  and denote by R an |S| × 1 vector. H is
the planning horizon and γ ∈ [0  1] is a discount factor. A deterministic policy π is a function that
returns an action a = π(s) for each state s. Tπ is deﬁned as Tπ(s  s(cid:48)) = T (s  π(s)  s(cid:48)). We denote
by πt:H a non-stationary policy (πt  πt+1  . . .   πH )  where πi is a policy at time-step i. The value
of policy πt:H is the expected sum of rewards received by following πt:H  starting from a state s
t:H is one satisfying
t:H ∈ arg maxπt:H Vπt:H (s) ∀s ∈ S. Searching for an optimal policy is generally an iterative
π∗
process with two phases: policy evaluation  and policy improvement.
When the state space S is large or continuous  the value function Vπt:H is approximated by a linear
combination of n basis functions  or features. Let fi be a |S||A| × 1 vector corresponding to the ith
basis function  and let F be the |S||A|× n matrix of columns fi. Let Ππt be an |S|×|S||A| action-
selection matrix deﬁned as Ππt(s  (s  πt(s))) = 1 and 0 otherwise. Then Vπt:H = Fπtw  where w
is a n× 1 weights vector and Fπt = ΠπtF . We deﬁne the Bellman error of two consecutive policies
πt and πt+1 using the feature matrix F and the weights wt  wt+1 ∈ Rn as BE(F  wt:t+1  πt:t+1) =
(cid:107)Fπtwt − γTπtFπt+1wt+1 − R(cid:107)1. Similarly  we deﬁne the Bellman error of a distribution P on
policies πt and πt+1 as BE(F  wt:t+1  P ) = (cid:107)Eπt:t+1∼P [Fπtwt − γTπtFπt+1wt+1]− R(cid:107)1. We also
deﬁne the minimum Bellman error as BE∗(F  πt:t+1) = minwt:t+1 BE(F  wt:t+1  πt:t+1) and the

i=t γi−tEsi [R(si)|st = s  Tπt:i]. An optimal policy π∗

total Bellman error as BE(F  w0:H   π0:H ) =(cid:80)H−1

t=0 BE(F  wt:t+1  πt:t+1).

3 Markov Random Field Policies for Reinforcement Learning

We now present the reinforcement learning approach using the Bellman error as a structure penalty.

3.1 Structure penalty

Optimal policies of many real-world problems are structured and change smoothly over the state
space. Therefore  the optimal value function can often be approximated by simple features  com-
pared to the value functions of arbitrary policies. We exploit this property and propose to indirectly
use these features  provided as domain knowledge  for accelerating the search for an optimal policy.
Speciﬁcally  we restrain the policy search to a set of policies that have a low estimated Bellman error
when their values are approximated using the provided features  knowing that the optimal policy has
a low Bellman error. Note that our approach is complementary to function approximation methods.
We only use the features for calculating Bellman errors  the value functions can be approximated by
using other methods  such as LSTD (Boyan  2002).
Let Kπ be the Gram matrix deﬁned as Kπ = ΠπKΠT
π   where K = F F T . Matrix K is the adjacency
matrix of a graph that indicates which states and actions are similar under an optimal policy. Feature
matrix F is not explicitly required  as only the matrix K will be used later. Therefore  the user needs
only to provide a similarity measure between states  such as the Euclidean distance.
Let wt  wt+1 ∈ R|S|   ∈ R 
if (cid:107)Eπt:t+1∼P [Kπtwt − γTπtKπt+1wt+1] − R(cid:107)1 ≤  then
BE∗(F  P ) ≤ . This result is obtained by setting F T ΠT
π wt+1 as the weight vec-
tors of the values of policies πt and πt+1. The condition above implies that the policy distribution
P has a value function that can be approximated by using F . Enforcing this condition results in a
bias favoring policies with a low Bellman error. Thus  we are interested in learning a distribution
P (π0:H ) that satisﬁes this condition  while maximizing its expected value.

π wt and F T ΠT

2

Distribution P can be decomposed using the chain rule as P (π0:H ) = P (πH )(cid:81)H−1

t=0 P (πt|πt+1:H ).
We start by calculating a distribution over deterministic policies πH that will be executed at the last
time-step H. Then  for each step t ∈ {H − 1  . . .   0}  we calculate a distribution P (πt|πt+1:H ) over
deterministic policies πt given policies πt+1:H that we sample from P (πt+1:H ). In the following 
we show how to calculate P (πt|πt+1:H ).

3.2 Primal problem
Let ρ ∈ R be a lower bound on the entropy of a distribution P on deterministic policies πt  condi-
tioned on πt+1:H. ρ is used for tuning the exploration. Our problem can then be formulated as

EP [V πt:H ](s)

  subject to

g1(P ) = 1  g2(P ) ≥ ρ (cid:107)g3(P ) − R(cid:107)1 ≤ 

 

(cid:17)

where

g1(P ) =

P (πt|πt+1:H )

 

P (πt|πt+1:H ) log P (πt|πt+1:H )

 

P (πt|πt+1:H )[Kπtwt − γTπtKπt+1wt+1]

 

P (πt|πt+1:H )V πt:H

(cid:17)

max

P

(cid:16)

(cid:16)(cid:88)

s∈S

(cid:88)

g3(P ) =

πt∈A|S|

(cid:16)

(cid:88)

πt∈A|S|

(cid:17)

(cid:16)
(cid:16)
g2(P ) = − (cid:88)
(cid:17)

πt∈A|S|

(cid:16)EP [V πt:H ] =

(cid:88)

πt∈A|S|

(1)

(cid:17)

(cid:17)

.

The objective function in Equation 1 is linear and its constraints deﬁne a convex set. Therefore  the
optimal solution to Problem 1 can be found by solving its Lagrangian dual.

3.3 Dual problem

The Lagrangian dual is given by

L(P  τ  η  λ)=

EP [V πt:H ](s)

(cid:17) − η

(cid:16)

(cid:17)

g1(P )−1

(cid:16)

+ τ

g2(P )−ρ

(cid:17)

+ λT(cid:16)

g3(P )−R

(cid:17)

+ (cid:107)λ(cid:107)1 

where η  τ ∈ R and λ ∈ R|S|. We refer the reader to Dudik et al. (2004) for a detailed derivation.
V πt:H (s) + λT [Kπtwt − γTπtKπt+1wt+1] − τ log P (πt|πt+1:H )) − η − 1.
∂L(P  τ  η  λ)
∂P (πt|πt+1:H )

=

(cid:16)(cid:88)
(cid:88)

s∈S

s∈S

By setting ∂L(P τ η λ)

∂P (πt|πt+1:H ) = 0 (Karush-Kuhn-Tucker condition)  we get the solution

(cid:16) 1
τ(cid:124)(cid:123)(cid:122)(cid:125)

(cid:122)
(cid:123)
(cid:88)
(cid:0)expected sum of rewards

(cid:125)(cid:124)
(cid:125)
V πt:H (s) + λT [Kπtwt − γTπtKπt+1wt+1]

(cid:123)(cid:122)

s∈S

(cid:124)

(cid:1)(cid:17)

.

smoothness term

exploration factor

P (πt|πt+1:H ) ∝ exp

This distribution on joint actions is a Markov Random Field.
In fact  the kernel K = F F T
is the adjacency matrix of a graph (E S)  where (si  sj) ∈ E if and only if ∃ai  aj ∈ A :
K((si  ai)  (sj  aj)) (cid:54)= 0. Local Markov property is veriﬁed  ∀si ∈ S :
P (πt(si)|πt+1:H  {πt(sj) : sj ∈ S  sj (cid:54)= si})=P (πt(si)|πt+1:H  {πt(sj) : (si  sj) ∈ E  sj (cid:54)= si}).
In other terms  the probability of selecting an action in a given state depends on the expected long
term reward of the action  as well as on the selected actions in the neighboring states. Dependencies
between neighboring states are due to the smoothness term in the distribution.

3.4 Learning parameters

Our goal now is to learn the distribution P   which is parameterized by τ  λ  wt:t+1 and V πt:H . Given
that the transition function T is unknown  we use samples D = {(st  at  rt  st+1)} for approximat-
ing the gradients of the parameters and the value function V πt:H . We also restrain Kπt to states and
actions that appear in the samples  and denote by ˆTπt the empirical transition matrix of the sampled

states. Since P (π0:H ) = P (πH )(cid:81)H−1
(cid:0)(cid:80)
t=0 P (πt|πt+1:H )  then
s∈D V πt:H (s) + λT

P (π0:H ) ∝ exp

t [Kπtwt − γ ˆTπtKπt+1wt+1](cid:1)(cid:17)

(cid:80)H

.

(2)

(cid:16) 1

t=0

τ

3

The value function V πt:H is empirically calculated from the samples by using a standard value
function approximation algorithm  such as LSTD (Boyan  2002). Temperature τ determines the
entropy of the distribution P   τ is initially set to a high value and gradually decreased over time as
more samples are collected. One can use the same temperature for all time-steps within the same
episode  or a different one for each step. Since the Lagrangian L is convex  parameters λt can be
learned by a simple gradient descent. Algorithm 1 summarizes the principal steps of the proposed
approach. The algorithm iterates between two main steps: (i) sampling and executing policies from
Equation 2  and (ii)  updating the value functions and the parameters λt using the samples. The
weight vectors w0:H are the ones that minimize the empirical Bellman error in samples D  they are
also found by a gradient descent   wherein ∂w0:H BE(K  w0:H   π0:H ) is estimated from D.
Algorithm 1 Episodic Policy Search with Markov Random Fields
Initialize the temperature τ with a large value  and λ0:H with 0.
repeat

1. Sample policies π0:H from P (Equation 2).
2. Discard policies π0:H that have an empirical Bellman error higher than .
3. Execute π0:H and collect D = {(st  at  rt  st+1)}.
4. Update the value functions V πt:H by using LSTD with D.
5. Find λ0:H that minimizes the dual L by a gradient descent  ∂λL is estimated from D.
6. Decrease the temperature τ.

until τ ≤ τ

The main assumption behind this algorithm is that the kernel K approximates sufﬁciently well the
optimal value function  what happens when this is not the case? The introduced bias will favor
suboptimal policies. However  this problem can be solved by setting the threshold  to a high value
when the user is uncertain about the domain knowledge provided by K. Our experiments conﬁrm
that even a binary matrix K  corresponding to a k-NN graph  can yield an improved performance.
This approach is straightforward to extend to handle samples of continuous states and actions   in
which case  a policy is represented by a vector Θt ∈ RN of continuous parameters (for instance  the
center and the width of a gaussian). Therefore  Equation 2 deﬁnes a distribution P (Θ0:H ). In our
experiments  we use the Metropolis-Hastings algorithm for sampling Θ0:H from P .

4 Markov Random Field Policies for Apprenticeship Learning

We now derive a policy shaping approach for apprenticeship learning using Markov Random Fields.

4.1 Apprenticeship learning

θk  ∀s ∈ S : R(s) = (cid:80)m
tions  Vπt:H (s) =(cid:80)m

The aim of apprenticeship learning is to ﬁnd a policy π that is nearly as good as a policy ˆπ demon-
strated by an expert  i.e.  Vπ(s) ≥ Vˆπ(s) −  ∀s ∈ S. Abbeel & Ng (2004) proposed to learn a
reward function  assuming that the expert is optimal  and to use it to recover the expert’s general-
ized policy. The process of learning a reward function is known as inverse reinforcement learning.
The reward function is assumed to be a linear combination of m feature vectors φk with weights
k=1 θkφk(s). The expected discounted sum of feature φk  given policy
i=t γi−tEst:H [φk(si)|st = s  Tπt:i]. Using this
πt:H and starting from s  is deﬁned as φπt:H
deﬁnition  the expected return of a policy π can be written as a linear function of the feature expecta-
(s). Since this problem is ill-posed  Ziebart et al. (2008) proposed
to use the maximum entropy regularization  while matching the expected return of the examples.
This latter constraint can be satisﬁed by ensuring that ∀k  s : φπ
k (s) = ˆφk  where ˆφk denotes the
empirical expectation of feature φk calculated from the demonstration.

(s) =(cid:80)H

k

k=1 θkφπt:H

k

4.2 Structure matching

The classical framework of apprenticeship learning is based on designing features φ of the reward
and learning corresponding weights θ. In practice  as we show in the experiments  it is often difﬁcult
to ﬁnd an appropriate set of reward features. Moreover  the values of the reward features are usually

4

  then BE∗(F  ˆπt:t+1) = BE∗(F  P ).

obtained from empirical data and are subject to measurement errors. However  most real-world
problems exhibit a structure wherein states that are close together tend to have the same optimal
action. This information about the structure of the expert’s policy can be used to partially overcome
the problem of ﬁnding reward features. The structure is given by a kernel that measures similarities
between states. Given an expert’s policy ˆπ0:H and feature matrix F   we are interested in ﬁnding a
distribution P on policies π0:H that has a Bellman error similar to that of the expert’s policy. The
following proposition states the sufﬁcient conditions for solving this problem.
Proposition 1. Let F be a feature matrix  K = F F T   Kπt = ΠπtKΠT
πt
tion on policies πt and πt+1 such that Eπt:t+1∼P [Kπt] = Kˆπt  and Eπt:t+1∼P [γTπtKπt+1T T
γTˆπtKˆπt+1T T
ˆπt
Proof. We prove that BE∗(F  P ) ≤ BE∗(F  ˆπt:t+1). The same argument can be used for proving
that BE∗(F  ˆπt:t+1) ≤ BE∗(F  P ). This proof borrows the orthogonality technique used for prov-
ing the Representer Theorem (Sch¨olkopf et al.  2001). Let ˆwt  ˆwt+1 ∈ R|S| be the weight vectors
that minimize the Bellman error of the expert’s policy  i.e. (cid:107)ΠˆπtF ˆwt − γTˆπtΠˆπt+1F ˆwt+1 − R(cid:107)p =
BE∗(F  ˆπt:t+1). Let us write ˆwt = ˆwt(cid:107) + ˆwt⊥  where ˆwt(cid:107) is the projection of ˆwt on the rows
of ΠˆπtF   i.e. ∃ˆαt ∈ R|S|
ˆαt  and ˆwt⊥ is orthogonal to the rows of ΠˆπtF .
Thus  ΠˆπtF ˆwt = ΠˆπtF ( ˆwt(cid:107) + ˆwt⊥) = ΠˆπtF ˆwt(cid:107) = Kˆπt ˆαt. Similarly  one can show that
ˆαt+1 
γTˆπtΠˆπt+1F ˆwt+1 = γTˆπtKˆπt+1T T
ˆπt
then we have BE∗(F  P ) ≤ (cid:107)Eπt:t+1[ΠπtF wt − γTπtΠπt+1 F wt+1] − R(cid:107)1 = (cid:107)Eπt:t+1[Kπt ˆαt −
(cid:3)
γTπtKπt+1T T
πt

ˆαt and wt+1 = F T ΠT
T T
πt
− R(cid:107)1 = BE∗(F  ˆπt:t+1).

. Let P be a distribu-
] =

ˆαt+1] − R(cid:107)1 = (cid:107)KˆπtαT

ˆαt+1. Let wt = F T ΠT
πt

− γTˆπtKˆπt+1T T

: ˆwt(cid:107) = F T ΠT
ˆπt

αT

πt+1

πt

ˆπt+1

ˆπt

ˆπt

4.3 Problem statement

Our problem now is to ﬁnd a distribution on deterministic policies P that satisﬁes the conditions
k (s) = ˆφk. The conditions
stated in Proposition 1 in addition to the feature matching conditions φπ
of Proposition 1 ensure that P assigns high probabilities to policies that have a structure similar to
the expert’s policy ˆπ. The feature matching constraints ensure that the expected value under P is the
same as the value of the expert’s policy. Given that there are inﬁnite solutions to this problem  we
select a distribution P that has a maximal entropy (Ziebart et al.  2008).

−P (πt|πt+1:H ) log P (πt|πt+1:H )

 

(cid:17)

(cid:17)

(cid:16) (cid:88)
(cid:17)

πt∈A|S|

(cid:16)

max

P

(cid:16) (cid:88)
(cid:16)(cid:88)
(cid:16)(cid:88)

πt∈A|S|

πt∈A|S|

4.4 Solution

(cid:17)

(cid:88)

=
πt∈A|S|

(cid:88)

(cid:17)

.

(cid:17)

subject to

P (πt|πt+1:H ) = 1

 

P (πt|πt+1:H )φπt:H = ˆφ

 

P (πt|πt+1:H )Kπt = Kˆπt

 

γTˆπtKˆπt+1 T T
ˆπt

P (πt|πt+1:H )γTπtKπt+1T T

πt

πt∈A|S|
where φπt:H (s  k)
is concave and the constraints are linear. Note that the three last equalities are between matrices.

(s) (deﬁned in subsection 4.1). The objective function of this problem

def
= φπt:H

k

(cid:16)(cid:88)

k

(cid:88)

s∈S

(cid:88)

By setting the derivatives of the Lagrangian to zero (as in subsection 3.3)  we derive the distribution
P (πt|πt+1:H )∝ exp

λi jKπt(si  sj)+γ

ξi j(TπtKπt+1T T
πt

kφπt:H
θs

)(si  sj)

(s)+

k

.

(si sj )∈S 2

(si sj )∈S 2

Again  this distribution is a Markov Random Field. The parameters θ  λ and ξ are learned by
maximizing the likelihood P (ˆπt:H ) of the expert’s policy ˆπt:H. The learned parameters can then
be used for sampling policies that have the same expected value (from the second constraint)  and
the same Bellman error (from the last two constraints and Proposition 1) as the expert’s policy. If
kernel K is inaccurate  then the learned λ and ξ will take low values to maximize the likelihood of
the expert’s policy. Hence  our approach will be reduced to MaxEnt IRL (Ziebart et al.  2008).
For simplicity  we consider an approximate solution with fewer parameters in our experiments 
k is replaced by θk ∈ R. This simpliﬁcation is based on the fact that the reward
where each θs
function is independent of the initial state. We also replace λi j by λ ∈ R  and ξi j by ξ ∈ R.

5

(cid:16)(cid:88)
k θkφk(s) + γ(cid:80)

V πt:H
θ

s∈S

(s) =(cid:80)

For a sparse matrix K  one can create a corresponding graph (E S)  where (si  sj) ∈ E if and only if
∃ai  aj ∈ A : K((si  ai)  (sj  aj)) (cid:54)= 0 or ∃ai  aj ∈ A  (s(cid:48)
j) (cid:54)=
0. Finally  the policy distribution can be rewritten as
P (πt|πt+1:H ) ∝ exp

j) ∈ E : γT (si  ai  s(cid:48)
(cid:88)

(s) + λ(cid:0)(cid:88)

)(si  sj)(cid:1)(cid:17)

i)T (sj  aj  s(cid:48)

Kπt(si  sj) + γξ

(Tπt Kπt+1 T T
πt

i  s(cid:48)

  (3)

(si sj )∈E
s(cid:48)∈S Tπt(s  s(cid:48))V πt+1:H

θ

(si sj )∈E
(s(cid:48)).

θ

where V πt:H
The distribution given by Equation 3 is a Markov Random Field. The probability of choosing action
a in a given state s depends on the expected value of (s  a) and the actions chosen in neighboring
states. There is a clear similarity between this distribution of joint actions and the distribution of joint
labels in Associative Markov Networks (AMN) (Taskar  2004). In fact  the proposed framework
generalizes AMN to sequential decision making problems. Also  the MaxEnt method (Ziebart et al. 
2008) can be derived from Equation 3 by setting λ = 0.

λ = 0

γ = 0
γ (cid:54)= 0 MaxEnt IRL (Ziebart et al.  2008)

Logistic regression

λ (cid:54)= 0

AMN (Taskar  2004)

AL-MRF

Table 1: Relation between Apprenticeship Learning with MRFs (AL-MRF) and other methods.

4.5 Learning procedure

In the learning phase  Equation 3 is used for ﬁnding parameters θ  λ and ξ that maximize the like-
lihood of the expert’s policy ˆπ. Since this likelihood function is concave  a global optimal can
be found by using standard optimization methods  such as BFGS. A main drawback of our ap-
proach is the high computational cost of calculating the partition function of Equation 3  which is
O(|A||S||S|2). In practice  this problem can be addressed by using several possible tricks. For in-
stance  we reuse the values calculated for a given policy π as the initial values of all the policies that
differ from π in one state only. We also decompose the state space into a set of weakly connected
components  and separately calculate the partition of each component. One can also use recent
efﬁcient learning techniques for MRFs  such as (Kr¨ahenb¨uhl & Koltun  2011).

4.6 Planning procedure
Algorithm 2 describes a dynamic programming procedure for ﬁnding a policy (π∗
H ) that
satisﬁes ∀t ∈ [0  H] : π∗
t+1:H ). The planning problem is reduced to a
sequence of inference problems in Markov Random Fields. The inference problem itself can also be
efﬁciently solved using techniques such as graph min-cut (Boykov et al.  1999)  α-expansions and
linear programming relaxation (Taskar  2004). We use the α-expansions for our experiments.

t ∈ arg maxπt∈A|S| P (πt|π∗

1  . . .   π∗

0  π∗

Algorithm 2 Dynamic Programming for Markov Random Field Policies

∀(s  a) ∈ S × A : QH+1(s  a) = 0.
for t = H : 0 do

1. ∀(s  a) ∈ S × A : Qt(s  a) =(cid:80)

k θkφk(s) + γ(cid:80)

2. Use an inference algorithm (such as the α-expansions) in the MRF deﬁned on the graph
(S E) to label states with actions: the cost of labeling s with a is −Qt(s  a) and the potential of
(s(cid:48)
i  s(cid:48)
(si  ai  sj  aj) is λ
.
j)
3. Denote by π∗

K(si  ai  sj  aj) + γξ(cid:80)

t the labeling policy returned by the inference algorithm;

j )∈E T (si  ai  s(cid:48)
i s(cid:48)

i)T (sj  aj  s(cid:48)

j)Kπ∗

(s(cid:48)

t+1

(cid:17)

(cid:16)

s(cid:48) T (s  a  s(cid:48))Qt+1(s(cid:48)  π∗

t+1(s(cid:48)))

end for
Return the policy π∗ = (π∗

0  π∗

1  . . .   π∗

H );

5 Experimental Results
We present experiments on two problems: learning to swing-up and balance an inverted pendulum
on a cart  and learning to grasp unknown objects.

6

5.1 Swing-up cart-balancing

The simulated swing-up cart-balancing system (Figure 1) consists of a 6 kg cart running on a 2 m
track and a freely-swinging 1 kg pendulum with mass attached to the cart with a 50 cm rod. The
state of the system is the position and velocity of the cart (x  ˙x)  as well as the angle and angular
velocity of the pendulum (θ  ˙θ). An action a ∈ R is a horizontal force applied to the cart. The
dynamics of the system are nonlinear. States and actions are continuous  but time is discretized to
steps of 0.1 s. The objective is to learn  in a series of 5s episodes  a policy that swings the pendulum
up and balances it in the inverted position. Since the pendulum falls down after hitting one of the two
track limits  the policy should also learn to maintain the cart in the middle of the track. Moreover 
the track has a nonuniform friction modeled as a force slowing down the cart. Part of the track has
a friction of 30 N  while the remaining part has no friction. This variant is more difﬁcult than the
standard ones (Deisenroth & Rasmussen  2011).

We consider parametric policies of the form π(x  ˙x  θ  ˙θ) = (cid:80)

i piqi(x  ˙x  θ  ˙θ)  where pi are real
weights and qi are basis functions corresponding to the signs of the angle and the angular velocity
and an exponential function centered at the middle of the track. Moreover  we discretize the track
into 10 segments  and use 10 binary basis functions for friction compensation  each one is nonzero
only in a particular segment. A reward of 1 is given for each step the pendulum is above the horizon.

|

.(cid:126)a
.
.

|

˙θi

K(cid:0)(cid:104)xi  ˙xi  θi  ˙θi  ui(cid:105) (cid:104)xj  ˙xj  θj  ˙θj  uj(cid:105)(cid:1) = 1 iff

Since the friction changes smoothly along the
track (domain knowledge)  we use the adjacency
matrix of a nearest-neighbor graph as the MRF
Speciﬁcally  we set
kernel K in Equation 2.
|xi − xj| ≤ 0.2m  θiθj ≥ 0 
˙θj ≥ 0  and
|ui − uj| ≤ 5N  otherwise K is set to 0. Fig-
ure 1 shows the average reward per time-step of
the learned policies as a function of the learning
time. Our attempts to solve this variant using differ-
ent policy gradient methods  e.g. (Kober & Peters 
2008)  mainly resulted in poor policies. We report
the values of the policies sampled with Metropolis-
Hastings using Equation 2  and compare to the case
where the policies are sampled solely according to
their expected values  i.e. λt = 0. The expected
values are estimated from the samples. The results 
averaged over 50 independent trials  show that the
convergence is faster when the MRF is used. More-
over  the performance increases as the threshold set
on the maximum Bellman error () in Algorithm 1
is decreased. In fact  policies that change smoothly
have a lower Bellman error as their values can be
better approximated with kernel K.
5.2 Precision grasps of unknown objects
From a high-level point of view  grasping an object can be seen as an MDP with three steps: reach-
ing  preshaping  and grasping. At any step  the robot can either proceed to the next step or restart
from the beginning and get a reward of 0. At t = 0  the robot always starts from the same initial
state s0  and the set of actions corresponds to the set of points on the surface of the object. Given
a grasping point  we set the approach direction to the surface normal vector. At t = 1  the state is
given by a surface point and an approach direction  and the set of actions corresponds to the set of
all possible hand orientations. At t = 2  the state is given by a surface point  an approach direction
and a hand orientation. There are two possible last actions  closing the ﬁngers or restarting.
In this experiment  we are interested in learning to grasp objects from their handles. The reward of
each step depends on the current state. There is no reward at t = 0. The reward R1 deﬁned at t = 1
is a function of the ﬁrst three eigenvalues of the scatter matrix deﬁned by the 3D coordinates of the
points inside a small ball centered on the selected point (Boularias et al.  2011). The reward R2 

Figure 1: Swing-up cart-balancing. The
friction is nonuniform  the red area has a
higher friction than the blue one. However 
the friction changes only at one point of the
track. Consequently  restraining the search to
smooth policies yield faster convergence.

7

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 20 40 60 80 100Average reward per time−stepLearning time in secondsOptimalMetropolis−Hastings with MRF  BE < 0.6 Metropolis−Hastings with MRF  BE < 1 Metropolis−Hastings with MRF  BE < 2 Metropolis−Hastingsn
o
i
s
s
e
r
g
e
R

N
M
A

L
R

I

t
n
E
x
a
M

-

F
R
M
L
A
Table 2: Learned Q-values at t = 0. Each point on an object corresponds to a reaching action. Blue
indicates low values and red indicates high values. The black arrow indicates the approach direction in the
optimal policy according to the learned reward function.
deﬁned at t = 2  is a function of collision features. We simulate the trajectories of 10 equidistant
points on each ﬁnger of a Barrett hand (a three-ﬁngered gripper). The collision features are binary
variables indicating whether or not the corresponding ﬁnger points will make contact with the object.
Based on the domain knowledge that points that are close to each other should have the same action
(i.e. same approach direction and hand orientation)  the kernel K is given by the k-nearest neighbors
graph  using the Euclidean distance and k = 6 in the state space of positions (or surface points)  and
the angular distance  with k = 2 in the discretized state space of hand orientations. We also use a
quadratic kernel for learning R1  and the Hamming distance between the feature vectors as a kernel
for learning R2. We also use a single constant feature for all the edges.
We used one object for training and provided six trajectories leading
to a successful grasp from its handle. For testing  we compared our
approach (Apprenticeship Learning with MRF) with MaxEnt IRL 
AMN and Logistic Regression  which is equivalent to AMN with-
out the graph structure. For AMN and Logistic Regression  only
the reward R1 at time-step 1 is learned  since these are classiﬁca-
tion methods and do not consider subsequent rewards.
Table 2 shows the Q-values at t = 0 and the approach directions at
optimal grasping points. AL-MRF improves over the other methods
by generally giving high values to handle points only. The values of
the other points are zeros because the optimal action at these points
is to restart rather than to grasp. The confusion in the other meth-
ods comes from noised point coordinates and self-occlusions. More
importantly  AL-MRF improves over AMN  a structured supervised
learning technique  by considering the reward at t = 2 while mak-
ing a decision at t = 1. This can be seen as a type of object recognition by functionality. Figure 2
shows the percentage of successful grasps using the objects in Table 2. A grasp is successful if it is
located on a handle and the hand orientation is orthogonal to the handle and the approach direction.
6 Conclusion
Based on the observation that the value function of an optimal policy is often smooth and can be
approximated with a simple kernel  we introduced a general framework for incorporating this type
of domain knowledge in forward and inverse reinforcement learning. Our approach uses Markov
Random Fields for deﬁning distributions on deterministic policies  and assigns high probabilities to
smooth policies. We also provided strong empirical evidence of the advantage of this approach.
Acknowledgement
This work was partly supported by the EU-FP7 grant 248273 (GeRT).

Figure 2:
Percentage of
grasps located on a handle
with a correct approach direc-
tion and hand orientation.

8

 0 20 40 60 80 100RegressionAMNMaxEnt IRLAL-MRFPercentage of successful graspsReferences
Abbeel  Pieter and Ng  Andrew Y. Apprenticeship Learning via Inverse Reinforcement Learning. In
Proceedings of the Twenty-ﬁrst International Conference on Machine Learning (ICML’04)  pp.
1–8  2004.

Boularias  Abdeslam  Kr¨omer  Oliver  and Peters  Jan. Learning robot grasping from 3-D images
with Markov Random Fields. In Proceedings of the 2011 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS’11)  pp. 1548–1553  2011.

Boularias  Abdeslam  Kr¨omer  Oliver  and Peters  Jan. Structured Apprenticeship Learning.

In
Proceedings of the European Conference on Machine Learning and Knowledge Discovery in
Databases (ECML-PKDD’12)  pp. 227–242  2012.

Boyan  Justin A. Technical Update: Least-Squares Temporal Difference Learning. Machine Learn-

ing  49:233–246  November 2002. ISSN 0885-6125.

Boykov  Yuri  Veksler  Olga  and Zabih  Ramin. Fast Approximate Energy Minimization via Graph

Cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence  23:2001  1999.

Deisenroth  Marc Peter and Rasmussen  Carl Edward. PILCO: A Model-Based and Data-Efﬁcient
Approach to Policy Search. In Proceedings of the Twenty-Eighth International Conference on
Machine Learning (ICML’11)  pp. 465–472  2011.

Dudik  Miroslav  Phillips  Steven J.  and Schapire  Robert E. Performance guarantees for regular-
In Proceedings of the 17th Annual Conference on

ized maximum entropy density estimation.
Computational Learning Theory (COLT’04)  pp. 472–486  2004.

Kober  Jens and Peters  Jan. Policy search for motor primitives in robotics. In NIPS  pp. 849–856 

2008.

Kohli  Pushmeet  Kumar  Pawan  and Torr  Philip. P3 and beyond: Solving energies with higher
order cliques. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition
(CVPR’07)  2007.

Kr¨ahenb¨uhl  Philipp and Koltun  Vladlen. Efﬁcient Inference in Fully Connected CRFs with Gaus-
sian Edge Potentials. In Advances in Neural Information Processing Systems 24  pp. 109–117.
2011.

Munoz  Daniel  Vandapel  Nicolas  and Hebert  Martial. Onboard contextual classiﬁcation of 3-D
point clouds with learned high-order Markov random ﬁelds. In Proceedings of IEEE International
Conference on Robotics and Automation (ICRA’09)  2009.

Ormoneit  Dirk and Sen  Saunak. Kernel-based reinforcement learning. In Machine Learning  pp.

161–178  1999.

Sch¨olkopf  Bernhard  Herbrich  Ralf  and Smola  Alex. A Generalized Representer Theorem .

Computational Learning Theory  2111:416–426  2001.

Taskar  Ben. Learning Structured Prediction Models: A Large Margin Approach. PhD thesis 

Stanford University  CA  USA  2004.

Ziebart  B.  Maas  A.  Bagnell  A.  and Dey  A. Maximum Entropy Inverse Reinforcement Learning.
In Proceedings of the Twenty-Second AAAI Conference on Artiﬁcial Intelligence (AAAI’08)  pp.
1433–1438  2008.

9

,Gautam Goel
Yiheng Lin
Haoyuan Sun
Adam Wierman