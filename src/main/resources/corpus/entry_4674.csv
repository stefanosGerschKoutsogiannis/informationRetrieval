2018,Relating Leverage Scores and Density using Regularized Christoffel Functions,Statistical leverage scores emerged as a fundamental tool for matrix sketching and column sampling with applications to low rank approximation  regression  random feature learning and quadrature. Yet  the very nature of this quantity is barely understood. Borrowing ideas from the orthogonal polynomial literature  we introduce the regularized Christoffel function associated to a positive definite kernel. This uncovers a variational formulation for leverage scores for kernel methods and allows to elucidate their relationships with the chosen kernel as well as population density. Our main result quantitatively describes a decreasing relation between leverage score and population density for a broad class of kernels on Euclidean spaces. Numerical simulations support our findings.,Relating Leverage Scores and Density
using Regularized Christoffel Functions

Edouard Pauwels

IRIT-AOC

Université Toulouse 3

Paul Sabatier

Toulouse  France

Francis Bach

INRIA

Ecole Normale Supérieure
PSL Research University

Paris  France

Jean-Philippe Vert

Google Brain

CBIO Mines ParisTech
PSL Research University

Paris  France

Abstract

Statistical leverage scores emerged as a fundamental tool for matrix sketching
and column sampling with applications to low rank approximation  regression 
random feature learning and quadrature. Yet  the very nature of this quantity is
barely understood. Borrowing ideas from the orthogonal polynomial literature  we
introduce the regularized Christoffel function associated to a positive deﬁnite kernel.
This uncovers a variational formulation for leverage scores for kernel methods and
allows to elucidate their relationships with the chosen kernel as well as population
density. Our main result quantitatively describes a decreasing relation between
leverage score and population density for a broad class of kernels on Euclidean
spaces. Numerical simulations support our ﬁndings.

1

Introduction

Statistical leverage scores have been historically used as a diagnosis tool for linear regression
[16  34  10]. To be concrete  for a ridge regression problem with design matrix X and regularization
parameter λ > 0  the leverage score of each data point is given by the diagonal elements of X(X(cid:62)X +
λI)−1X(cid:62). These leverage scores characterize the importance of the corresponding observations
and are key to efﬁcient subsampling with optimal approximation guarantees. Therefore  leverage
scores emerged as a fundamental tool for matrix sketching and column sampling [22  21  13  36] 
and play an important role in low rank matrix approximation [11  6]  regression [2  28  20]  random
feature learning [29] and quadrature [7]. The notion of leverage score is seen as an intrinsic  setting-
dependent quantity  and should eventually be estimated. In this work we elucidate a relation between
leverage score and the learning setting (population measure and statistical model) when used with
kernel methods.
For that purpose  we introduce a variant of the Christoffel function  a classical tool in polynomial
algebra which provides a bound for the evaluation at a given point of a given degree polynomial P
in terms of an average value of P 2. The Christoffel function is an important object in the theory of
orthogonal polynomials [32  14] and found applications in approximation theory [26] and spectral
analysis of random matrices [5]. It is parametrized by the degree of polynomials considered and an
associated measure  and we know that  as the polynomial degree increases  it encodes information
about the support and the density of its associated measures  see [23  24  33] for the univariate case
and [8  9  37  38  18  19] in the multivariate case.
The variant we propose amounts to replacing the set of polynomials with ﬁxed degree  used in the
deﬁnition of the Christoffel function  by a set of function with bounded norm in a reproducing kernel
Hilbert space (RKHS)1. More precisely  given a density p on Rd and a regularization parameter
1Kernelized Christoffel functions were ﬁrst proposed by Laurent El Ghaoui and independently studied in [4].

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

λ > 0  we introduce Cλ : Rd → R  the regularized Christoffel function where λ plays a similar
role as the degree for polynomials. The function Cλ turns out to have intrinsic connections with
statistical leverage scores  as the quantity 1/Cλ corresponds precisely to a notion of leverage used in
[6  2  28  7]. As a consequence  we uncover a variational formulation for leverage scores which helps
elucidate connections with the RKHS and the density p on Rd.
Our main contribution is a precise asymptotic expansion of Cλ as λ → 0 under restrictions on the
RKHS. To give a concrete example  if we consider the Sobolev space of functions on Rd with squared
integrable derivatives of order up to s > d/2  we obtain  the asymptotic equivalent

Cλ(z)

∼

λ→0  λ>0

q−1
0 λd/(2s)p(z)1−d/2s 

for z a continuity point of p with p(z) > 0. Here q0 is an explicit constant which only depends on the
RKHS. We recover scalings with respect to λ which matches known estimates for the usual degrees
of freedom [28  7]. More importantly  we also obtain a precise spatial description of Cλ(z) (i.e. 
dependency on z)  and deduce that the leverage score is itself proportional to p(z)d/(2s)−1 in the
limit. Roughly speaking  large scores are given to low density regions (note that d/(2s) − 1 < 0).
This result has several potential consequences for machine learning:
(i) The Christoffel function could be used for density or support estimation. This has connections
with the spectral approach proposed in [35] for support learning. (ii) This could provide a more
efﬁcient way to estimate leverage scores through density estimation. (iii) When leverage scores are
used for sampling  the required sample size depends on the ratio between the maximum and the
average leverage scores [28  7]. Our results imply that this ratio can be large if there exists low-density
regions  while it remains bounded otherwise.

Organization of the paper. We introduce the regularized Christoffel function in Section 2 and
explicit connections with leverage scores and orthogonal polynomials. Our main result and assump-
tions are described in abstract form in Section 3  they are presented as a general recipe to compute
asymptotic expansions for the regularized Christoffel function. Section 3.3 describes an explicit
example and a precise asymptotic for an important class of RKHS related to Sobolev spaces. We
illustrate our results numerically in Section 4. The proofs are postponed to Appendix B while
Appendix A contains additional properties and simulations  and Appendix C contains further lemmas.

Notations. Let d denote the ambient dimension  0 denote the origin in Rd and
C(Rd)  L1(Rd)  L2(Rd)  L∞(Rd) denote the complex-valued function on Rd which are respec-
(cid:82)
tively continuous  absolutely integrable  square integrable  measurable and essentially bounded.
Rd f (x)e−ix(cid:62)ωdx. For
g ∈ L1(Rd)  its inverse Fourier transform is x (cid:55)→ 1
Rd g(x)eix(cid:62)ωdω. If f ∈ L1(Rd) ∩ C(Rd)
and ˆf ∈ L1(Rd)  then inverse transform composed with direct transform leaves f unchanged. The
Fourier transform is extended to L2(Rd) by a density argument. It deﬁnes an isometry: if f ∈ L2(Rd) 

For any f ∈ L1(Rd)  let ˆf : Rd (cid:55)→ C be its Fourier transform  ˆf : ω (cid:55)→ (cid:82)
Parseval formula writes(cid:82)

Rd | ˆf (ω)|2dω. See  e.g.  [17  Chapter 11].

Rd |f (x)|2dx = 1

(cid:82)

(2π)d

(2π)d

We identify x with a set of d real variables x1  . . .   xd. We associate to a multi-index β =
(βi)i=1 ... d ∈ Nd the monomial xβ := xβ1
i=1 βi. The
linear span of monomials forms the set of d-variate polynomials. The degree of a polynomial is
the highest of the degrees of its monomials with nonzero coefﬁcients (null for the null polyno-
mial). A polynomial P is said to be homogeneous of degree 2s ∈ N if for all λ ∈ R  x ∈ Rd 
P (λx) = λ2sP (x)  it is then composed only of monomials of degree 2s. See [14] for further details.

2 . . . xβd

1 xβ2

d whose degree is |β| := (cid:80)d

2 Regularized Christoffel function

2.1 Deﬁnition

(cid:82)

In what follows  k is a positive deﬁnite  continuous  bounded  integrable  real-valued kernel on
Rd × Rd and p is an integrable real function over Rd. We denote by H the RKHS associated to
k which is assumed to be dense in L2(p)  the normed space of functions  f : Rd (cid:55)→ R  such that
Rd f 2(x)p(x)dx < +∞. This will be made more precise in Section 3.1.

2

Figure 1: The black lines represent a density and the corresponding Christoffel function. The colored
lines are solutions of problem in (1)  the corresponding z being represented by the dots. Outside
the support  the optimum is smooth and high values have small overlap with the support. Inside the
support  the optimum is less smooth  it forms a peak  sharper in higher density regions.

Deﬁnition 1 The regularized Christoffel function  is given for any λ > 0  z ∈ Rd by

Cλ p k(z) = inf
f∈H

Rd

f (x)2p(x)dx + λ(cid:107)f(cid:107)2

H such that

f (z) = 1 .

(1)

(cid:90)

L2(p) + λ(cid:107)f(cid:107)2

If there is no confusion about the kernel k and the density p  we will use the notation Cλ =
Cλ p k. More compactly  setting for z ∈ Rd  Hz = {f ∈ H  f (z) = 1}  we have Cλ : z (cid:55)→
inf f∈Hz (cid:107)f(cid:107)2
H. The value of (1) is intuitively connected to the density p. Indeed 
the constraint f (z) = 1 forces f to remains far from zero in a neighborhood of z. Increasing the p
measure of this neighborhood increases the value of the Christoffel function. In low density regions 
the constraint has little effect which allows to consider smoother functions with little overlap with
higher density regions and decreases the overall cost. An illustration is given in Figure 1.

2.2 Relation with orthogonal polynomials

The name Christoffel is borrowed from the orthogonal polynomial literature [32  14  26]. In this
context  the Christoffel function is deﬁned as follows for any degree l ∈ N:

(cid:90)

Λl : z (cid:55)→ min
P∈Rl[X]

(P (x))2p(x)dx

such that P (z) = 1  

where Rl[X] denotes the set of d variate polynomials of degree at most l. The regularized Christoffel
function in (1) is a direct extension  replacing the polynomials of increasing degree by functions in a
RKHS with increasing norm. Λl has connections with quadrature and interpolation [26]  potential
theory and random matrices [5]  orthogonal polynomials [26  14]. Relating the asymptotic for large l
and properties of p has also been a long lasting subject of research [23  24  33  8  9  37  38  18  19].
The idea of studying the relation between Cλ and p was directly inspired by these works.

2.3 Relation with leverage scores for kernel methods
The (non-centered) covariance of p on H is the bilinear form Cov : H × H → R given by:

∀(f  g) ∈ H2   Cov(f  g) =

f (x)g(x)p(x)dx .

(cid:90)

Rd

The covariance operator Σ : H → H is then deﬁned such that for all f  g ∈ H  Cov(f  g) =
(cid:104)Σf  g(cid:105)H . If Σ is bounded with respect to (cid:107) · (cid:107)H  then Lemma 5 in Appendix C shows that:

∀z ∈ Rd  Cλ(z) =(cid:0)(cid:10)k(z ·)  (Σ + λI)−1k(z ·)(cid:11)

(cid:1)−1

 

H

As λ → 0  we typically have(cid:10)k(z ·)  (Σ + λI)−1k(z ·)(cid:11)
(cid:10)k(z ·)  Σ(Σ + λI)−1k(z ·)(cid:11)

which provides a direct link with leverage scores [7]  as Cλ(z) is exactly the inverse of the population
leverage score at z.
H → +∞. It is worth emphasizing that
spectral estimators (with other functions of the covariance operator than (Σ + λI)−1) have been
proposed for support inference in [35]. An example of such estimator has the form Fλ : z (cid:55)→
H  for which ﬁnite level sets encode information about the support
of p as λ → 0 [35]. Our main result should extend to broader classes of spectral functions.

3

0.000.250.500.751.00-2-1012xySolution1234FunctionDensityChristoﬀel2.4 Estimation from a discrete measure

integration with weight p by a discrete approximation of the form dρn =(cid:80)n

Practical computation of the regularized Christoffel function requires to have access to the covariance
operator Σ  which is not available in closed form in general. A plugin solution consists in replacing
i=1 ηiδxi  where for
each i = 1  . . .   n  ηi ∈ R+ is a weight  xi ∈ Rd and δxi denotes the Dirac measure at xi. We
may assume without loss of generality that the points are distinct. Given a kernel function k on
Rd × Rd  let K = (k(xi  xj))i j=1 ... n ∈ Rn×n be the Gram matrix and Ki the i-th column of K
for i = 1  . . .   n. We have a closed form expression for the Christoffel function with plug-in measure
dρn  for any λ > 0  i = 1  . . .   n:

Cλ ρn k(xi) = inf

f (xi)=1

1
n

ηj(f (xj))2 + λ(cid:107)f(cid:107)2

H =

K(cid:62)

i (K diag(η)K + λK)

−1 Ki

. (2)

(cid:17)−1

(cid:16)

n(cid:88)

j=1

This is a consequence of the representer theorem [30]; Lemma 5 allows to deal with the constraint
explicitely. Note that if ηi > 0 for all i = 1  . . .   n  then the Christoffel function may be obtained as
a weighted diagonal element of a smoothing matrix  as for all i  thanks to the matrix inversion lemma 
K(cid:62)
ii. This draws an important con-
nection with statistical leverage score [21  13] as it corresponds to the notion introduced for kernel
ridge regression [6  2  28]. It remains to choose η so that dρn approximates integration with weight p.

(cid:0)K(K + λ Diag(η)−1)−1(cid:1)

i (K diag(η)K + λK)

−1 Ki = η−1

i

Monte Carlo approximation: Assuming that(cid:82)

Rd p(x)dx = 1  if one has the possibility to draw
an i.i.d. sample (xi)i=1 ... n  with density p  then one can use ηi = 1
n for i = 1  . . .   n. The quality
of this approximation is of order λ−2n−1/2 (see Appendix A). If λ2n1/2 is large enough  then we
obtain a good estimation of the Christoffel function (note that better bounds could be obtained with
respect to λ using tools from [6  2  28]).

Riemann sums:
If the density p is piecewise smooth  one can approximate integrals with weight p
by using a uniform grid and a Riemann sum with weights proportional to p. The bound in Eq. (8)
also holds  the quality of this approximation is typically of the order of n−1/d which is attractive in
dimension 1 but quickly degrades in larger dimensions.
Depending on properties of the integrand  quasi Monte Carlo methods could yeld faster quadrature
rules [12]  more quantitative deviation bounds and faster rates is left for future research.

3 Relating regularized Christoffel functions to density

We ﬁrst make precise our notations and assumptions in Section 3.1 and describe our main result
in Section 3.2 using Assumption 2 which is given in abstract form. We then describe how this
assumption is satisﬁed by a broad class of kernels in Section 3.3.

3.1 Assumptions

Assumption 1

1. The kernel k is translation invariant: for any x  y ∈ Rd  k(x  y) = q(x − y) where
q ∈ L1(Rd) is the inverse Fourier transform of ˆq ∈ L1(Rd) which is real valued and strictly
positive.

2. The density p ∈ L1(Rd) ∩ L∞(Rd) is ﬁnite and nonnegative everywhere.

Under Assumption 1  k is a positive deﬁnite kernel by Bochner’s theorem and we have an explicit
characterization of the associated RKHS (see e.g. [35  Proposition 4]) 

H =

f ∈ C(Rd) ∩ L2(Rd);

 

(3)

(cid:110)

(cid:90)

| ˆf (ω)|2
ˆq(ω)

Rd

dω < +∞(cid:111)

4

with inner product

(cid:104)· ·(cid:105)H : (f  g) (cid:55)→ 1
(2π)d

(cid:90)

ˆf (ω)¯ˆg(ω)

Rd

ˆq(ω)

dω .

(4)

Remark 1 The assumption that ˆq ∈ L1(Rd) implies by the Riemann-Lebesgue theorem that q is in
C0(Rd)  the set of continuous functions vanishing at inﬁnity. Since ˆq is strictly positive  its support
is Rd and [31  Proposition 8] implies that k is c0-universal  i.e.  that H is dense in C0(Rd) w.r.t. the
uniform norm. As a result  H is also dense in L2(dρ) for any probability measure ρ.
Remark 2 For any f ∈ H  we have by Cauchy-Schwartz inequality

| ˆf (ω)|dω

and the last term is ﬁnite by Assumption 1. Hence ˆf ∈ L1(Rd) and we have f (0) =(cid:82)

ˆf where the
integral is understood in the usual sense. In this setting any f ∈ H is uniquely determined everywhere
on Rd by its Fourier transform and we have for any f ∈ H  (cid:107)f(cid:107)L∞(Rd) ≤ (cid:107) ˆf(cid:107)L1(Rd) ≤ (cid:107)f(cid:107)H

(cid:112)q(0).

ˆq(ω)dω 

dω

Rd

Rd

Rd

Rd

(cid:18)(cid:90)

(cid:90)

(cid:19)2 ≤

(cid:90)

| ˆf (ω)|2
ˆq(ω)

3.2 Main result

Problem (1) is related to a simpler variational problem with explicit solution. For any λ > 0  let

D(λ) := min
f∈H

(5)
Note that D(·) does not depend on p and corresponds to the Christoffel function at the origin 0  or
any other points by translation invariance  for the Lebesgue measure on Rd. The solutions of (5) have
an explicit description which proof is presented in Appendix B.2.

H subject to f (0) = 1.

Rd

f (x)2dx + λ(cid:107)f(cid:107)2

Lemma 1 For any λ > 0  D(λ) =

  and this value is attained by the function

(cid:90)

(cid:90)

fλ : x (cid:55)→ D(λ)

(2π)d

ˆq(ω)eiω(cid:62)x
ˆq(ω) + λ

dω.

(cid:82)Rd

(2π)d
ˆq(ω)
λ+ ˆq(ω) dω

1

(cid:90)
λ+ˆq(ω) dω ≥ (cid:82)

ˆq(ω)

Rd

tion 1 ensures that limλ→0 D(λ) = 0 as(cid:82)

Remark 3 We directly obtain D(λ) ≥ (2π)dλ
Rd

q(0)   for any λ > 0. Finally  let us mention that Assump-
2 which diverges as λ → 0.

ˆq(ω)≥λ

dω

We denote by gλ the inverse Fourier transform of
D(λ).
λ+ˆq   i.e.  gλ = fλ/D(λ). It satisﬁes gλ(0) = 1
Intuitively  as λ tends to 0  gλ  should be approaching a Dirac in the sense that gλ tends to 0
everywhere except at the origin where it goes to +∞. The purpose of the next Assumption is to
quantify this intuition.

ˆq

Assumption 2 For the kernel k given in Assumption 1 and fλ given in Lemma 1  there exists
ε : R+ → R+ such that  as λ → 0  ε(λ) → 0  and

f 2
λ(x)dx = o(λD(λ)).

(cid:107)x(cid:107)≥ε(λ)

See Section 3.3 for speciﬁc examples. We are now ready to describe the asymptotic inside the support
of p  the proof is given in Appendix B.1.
Theorem 1 Let q  k and p be given as in Assumption 1 and let Cλ be deﬁned as in (1). If Assumption
2 holds  then  for any z ∈ Rd such that p(z) > 0 and p is continuous at z  we have

(cid:18) λ

p(z)

(cid:19)

.

Cλ(z)

∼

λ→0  λ>0

p(z)D

5

Proof sketch. The equivalent is shown by using the variational formulation in Eq. (1). A natural
candidate for the optimal function f is the optimizer obtained from Lebesgue measure in Eq. (5) 
scaled by p(z). Together with Assumption 2  this leads to the desired upper bound. In order to obtain
the corresponding lower bound  we consider Lebesgue measure restricted to a small ball around z.
Using linear algebra and expansions of operator inverses  we relate the optimal value directly to the
optimal value D(λ) of Eq. (5).
This result is complemented by the following which describes the asymptotic behavior outside the
support of p  the proof is given in Appendix B.3.
Theorem 2 Let q  k and p be given as in Theorem 1. Then  for any z ∈ Rd  such that there exists

 > 0 with(cid:82)

(cid:107)z−x(cid:107)≤ p(x)dx = 0  we have

(i)

Cλ(z)

=

λ→0  λ>0

√
O(

λD(

√

λ)).

If furthermore there exists a ≥ 0 and c > 0 such that  for any ω ∈ Rd  ˆq(ω) ≥
any such z ∈ Rd  we have

c

1+(cid:107)ω(cid:107)a   then  for

(ii)

Cλ(z)

=

λ→0  λ>0

O(λ).

Proof sketch. Since only an upper-bound is needed  we simply have to propose a candidate function
for f  and we build one from the solution of Eq. (5) for (i) and directly from properties of kernels
for (ii).

Remark 4 Theorems 1 and 2 underline separation between the “inside” and the “outside” of the
support of p and describes the fact that the convergence to 0 as λ decreases is faster outside: (i) 
√
if log(D(λ)) = α log(λ) + o(1) with α < 1 (which is the case in most interesting situations)  then
λ)) = o(D(λ)). (ii)  it holds that λ = o(D(λ)). Hence in most cases  the
Cλ(z) = O(
values of the Christoffel function outside of the support of p are negligible compared to the ones
inside the support of p.

λD(

√

Combining Theorem 1 and 2 does not describe what happens in the limit case where neither of the
conditions on z hold  for example on the boundary of the support or at discontinuity points of the
density. We expect that this highly depends on the geometry of p and its support. In the polynomial
case on the simplex  the rate depends on the dimension of the largest face containing the point of
interest [38]. Settling down this question in the RKHS setting is left for future research.

3.3 A general construction

We describe a class of kernels for which Assumptions 1 and 2 hold  and Theorem 1 can be applied 
which includes Sobolev spaces. We also compute explicit equivalents for D(·) in (5). We ﬁrst
introduce a deﬁnition and an assumption.
Deﬁnition 2 For any s ∈ N∗  a d-variate polynomial P of degree 2s is called 2s-positive if it satisﬁes
the following.

• Let Q denote the 2s-homogeneous part of P (the sum of its monomial of degree 2s). Q is

(strictly) positive on the unit sphere in Rd.

• The polynomial R = P − Q satisﬁes R(x) ≥ 1 for all x ∈ Rd.

form(cid:81)d
(cid:81)d

i=1 w2

Remark 5 If P is 2s-positive  then it is always greater than 1 and its 2s-homogeneous part is
strictly positive except at the origin. The positivity of Q forbids the use of polynomial P of the
i ) which would allow to treat product kernels. Indeed  this would lead to Q(ω) =
i=1(1 + w2
i which is not positive on the unit sphere. The last condition on R is not very restrictive as it

can be ensured by a proper rescaling of P if we have R > 0 only.
Assumption 3 Let P be a 2s-positive  d-variate polynomial and let γ ≥ 1 be such that 2sγ > d.
The kernel k is given as in Assumption 1 with ˆq = 1
P γ .

6

One can check that q in Assumption 3 is well deﬁned and satisﬁes Assumption 1. A famous example
of such a kernel is the Laplace kernel (x  y) (cid:55)→ e−(cid:107)x−y(cid:107) which amounts  up to a rescaling  to choose
P of the form 1 + a(cid:107) · (cid:107)2 for a > 0 and γ = d+1
2 . In addition  Assumption 3 allows to capture the
usual multi-dimensional Sobolev space of functions with square integrable partial derivatives up to
order s  with s > 2/d  and the corresponding norm. We now provide the main result of this section.

Lemma 2 Assume that p and k are given as in Assumption 1 and 3. Then Assumption 2 is satisﬁed.
More precisely  set q0 = 1
following holds true as λ → 0  λ > 0 :

1+Q(ω)γ dω and p = (cid:100)sγ(cid:101)  then for any l <(cid:0)1 − d

(cid:1)/(8p) the

(cid:82)

(2π)d

Rd

2sγ

1

(i) D(λ) ∼ λ
q0

d

2sγ

 

(ii)

f 2
λ(x)dx = o (λD(λ)) .

(cid:107)x(cid:107)≥λl

(cid:90)

(cid:90)

Remark 6 If Q : ω (cid:55)→ (cid:107)ω(cid:107)2s  using spherical coordinate integration  we obtain
1
2d−1π d

rd−1
1 + r2sγ dω =

1 + Q(ω)γ dω =

2 Γ(cid:0) d

1
2d−1π d

(cid:1)(cid:90) +∞

(2π)d

q0 =

Rd

1

1

0

2

2 Γ(cid:0) d

2

(cid:1)

(cid:17) .

(cid:16) dπ

2sγ

π

2sγ sin

The proof is presented in Appendix B.4. We have the following corollary which is a direct application
of Theorem 1. It explicits the asymptotic for the Christoffel function  in terms of the density p.
Corollary 1 Assume that p and k are given as in Assumption 1 and 3 and that z ∈ Rd is such that
p(z) > 0 and p is continuous at z. Then as λ → 0  λ > 0 
2sγ p(z)1− d

Cλ(z) ∼ λ

.

d

2sγ 1
q0

4 Numerical illustration

In this section we provide numerical evidence conﬁrming the rate described in Corollary 1. We use
the Matérn kernel  a parametric radial kernel allowing different values of γ in Assumption 3.

4.1 Matérn kernel

We follow the description of [27  Section 4.2.1]  note that the Fourier transform is normalized
differently in our paper. For any ν > 0 and l > 0  we let for any x ∈ Rd 

(cid:16)√

qν l(x) =

21−ν
Γ(ν)

2ν(cid:107)x(cid:107)
l

(cid:17)ν

(cid:16)√

Kν

(cid:17)

2ν(cid:107)x(cid:107)
l

 

(6)

where Kν is the modiﬁed Bessel function of the second kind [1  Section 9.6]. This choice of q
satisﬁes Assumption 3  with s = 1 and γ = ν + d
2 . Indeed  for any ν  l > 0  its Fourier transform is
given for any ω ∈ Rd

ˆqν l(ω) =

2dπ d

2 Γ (ν + d/2) (2ν)ν

Γ(ν)l2ν

l2 + (cid:107)ω(cid:107)2(cid:1)ν+ d
(cid:0) 2ν

1

2

.

(7)

4.2 Empirical validation of the convergence rate estimate
Corollary 1 ensures that  given ν  l > 0 and q in (6)  as λ → 0  we have for appropriate z  Cλ(z) ∼
2ν+d /q0(ν  l). We use the Riemann sum plug-in approximation described in Section 2.4
λ
2ν+d p(z)
to illustrate this result numerically. We perform extensive investigations with compactly supported
sinusoidal density in dimension 1. Note that from Remark 6 we have the closed form expression

2ν

d

(cid:16) 2dπ

(cid:17) 1

2ν+d

q0(ν  l) =

d
2 Γ(ν+d/2)(2ν)ν

Γ(ν)l2ν

1

(2ν+d) sin( dπ

2ν+d )

.

7

Figure 2: The target density p represented in red. We consider different choices of ν and l for q as in
(6). We use the Riemann sum plug-in approximation described in (2) with n = 2000. Left: the fact
that the estimate is close to the density is clear for small values of λ. Right: the dotted line represents
the identity. This suggests that the rate estimate is of the correct order in λ.

Relation with the density: For a given choice of ν  l > 0  as λ → 0  we should obtain for
appropriate z that the quantity 
is roughly equal to p(z). This is conﬁrmed
numerically as presented in Figure 2 (left)  for different choices of the parameters ν.

λd/(d+2ν)

(cid:16) Cλ(z)q0(ν l)

(cid:17)1+d/(2ν)

(cid:16)

Convergence rate: For a given choice of ν  l > 0  as λ → 0  we should obtain for appropriate z
that the quantity
is roughly equal to λ. Considering the same experiment
conﬁrms this ﬁnding as presented in Figure 2 right  which suggests that the exponent in λ is of the
correct order.

p(z)2ν/(2ν+1) q0(ν  l)

Cλ(z)

(cid:17) 2ν+d

d

Additional experiments: A piecewise constant density is considered in Appendix A which also
contains simulations suggesting that the asymptotic has a different nature for the Gaussian kernel for
which we conjecture that our result does not hold.

5 Conclusion and future work

We have introduced a notion of Christoffel function in RKHS settings. This allowed to derive precise
asymptotic expansions for a quantity known as statistical leverage score which has a wide variety
of applications in machine learning with kernel methods. Our main result states that the leverage
score is inversely proportional to a power of the population density at the considered point. This
has intuitive meaning as leverage score is a measure of the contribution of a given observation to a
statistical estimate. For densely populated region  a speciﬁc observation  which should have many
close neighbors  has less effect on a statistical estimate than observations in less populated areas of
space. Our observation gives a precise meaning to this statement and sheds new light on the relevance
of the notion of leverage score. Furthermore  it is coherent with known results in the orthogonal
polynomial literature from which the notion of Christoffel function was inspired.
Direct extensions of this work include approximation bounds for our proposed plug-in estimate
and tuning of the regularization parameter λ. A related question is the relevance of the proposed
variational formulation for the statistical estimation of leverage scores when learning from random
features  in particular random Fourier features and density/support estimation. Another line of future
research would be the extension of our estimates to broader classes of RKHS  for example  kernels
with product structure  such as the (cid:96)1 counterpart of the Laplace kernel. Finally  it would be interesting
to extend the concepts to abstract topological spaces beyond Rd.

Acknowledgements

We acknowledge support from the European Research Council (grant SEQUOIA 724063).

8

ν=3 l=0.2ν=4 l=0.2ν=1 l=0.2ν=2 l=0.2-2-1012-2-10120.000.250.500.751.000.000.250.500.751.00z(cid:16)Cλ(z)q0(ν l)λ1/(1+2ν)(cid:17)1+1/(2ν)-6-5-4-3-2log10(λ)1e-051e-031e-011e-051e-03λ(cid:16)Cλ(z)p(z)2ν/(2ν+1)q0(ν l)(cid:17)1+2νReferences

[1] M. Abramowitz and I. Stegun. Handbook of mathematical functions: with formulas  graphs 

and mathematical tables. Courier Corporation  1964.

[2] A. Alaoui and M. Mahoney. Fast randomized kernel ridge regression with statistical guarantees.

In In Advances in Neural Information Processing Systems  pages 775–783  2015.

[3] N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical

society  68(3):337–404  1950.

[4] A. Askari  F. Yang  and L. E. Ghaoui. Kernel-based outlier detection using the inverse christoffel

function. Technical report  2018. arXiv preprint arXiv:1806.06775.

[5] W. V. Assche. Asymptotics for orthogonal polynomials. Springer-Verlag Berlin Heidelberg 

1987.

[6] F. Bach. Sharp analysis of low-rank kernel matrix approximations. In Conference on Learning

Theory  pages 185–209  2013.

[7] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions.

Journal of Machine Learning Research  18(21):1–38  2017.

[8] L. Bos. Asymptotics for the Christoffel function for Jacobi like weights on a ball in Rm. New

Zealand Journal of Mathematics  23(99):109–116  1994.

[9] L. Bos  B. D. Vecchia  and G. Mastroianni. On the asymptotics of Christoffel functions for
centrally symmetric weight functions on the ball in Rd. Rend. Circ. Mat. Palermo  2(52):277–
290  1998.

[10] S. Chatterjee and A. Hadi. Inﬂuential observations  high leverage points  and outliers in linear

regression. Statistical Science  1(3):379–393  1986.

[11] K. Clarkson and D. Woodruff. Low rank approximation and regression in input sparsity time.

In ACM symposium on Theory of computing  pages 81–90. ACM  2013.

[12] J. Dick  F. Y. Kuo  and I. H. Sloan. High-dimensional integration: the quasi-monte carlo way.

Acta Numerica  22:133–288  2013.

[13] P. Drineas  M. Magdon-Ismail  M. Mahoney  and D. Woodruff. Fast approximation of matrix
coherence and statistical leverage. Journal of Machine Learning Research  13:3475–3506 
2012.

[14] C. Dunkl and Y. Xu. Orthogonal polynomials of several variables. Cambridge University Press 

Cambridge  UK  2001.

[15] M. Hardy. Combinatorics of partial derivatives. The electronic journal of combinatorics  13(1) 

2006.

[16] D. Hoaglin and Welsch. The hat matrix in regression and ANOVA. The American Statistician 

32(1):17–22  1978.

[17] J. Hunter and B. Nachtergaele. Applied analysis. World Scientiﬁc Publishing Company  2001.
[18] A. Kroò and D. S. Lubinsky. Christoffel functions and universality in the bulk for multivariate

orthogonal polynomials. Canadian Journal of Mathematics  65(3):600–620  2012.

[19] J. Lasserre and E. Pauwels. The empirical Christoffel function in Statistics and Machine

Learning. arXiv preprint arXiv:1701.02886  2017.

[20] P. Ma  M. Mahoney  and B. Yu. A statistical perspective on algorithmic leveraging. The Journal

of Machine Learning Research  16(1):861–911  2015.

[21] M. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in

Machine Learning  3(2):123–224  2011.

[22] M. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proceed-

ings of the National Academy of Sciences  106(3):697–702  2009.

[23] A. Máté and P. Nevai. Bernstein’s Inequality in Lp for 0 < p < 1 and (C  1) Bounds for

Orthogonal Polynomials. Annals of Mathematics  111(1):145–154.  1980.

[24] A. Máté  P. Nevai  and V. Totik. Szego’s extremum problem on the unit circle. Annals of

Mathematics  134(2):433–53  1991.

[25] S. Minsker. On some extensions of Bernstein’s inequality for self-adjoint operators. arXiv

preprint arXiv:1112.5448  2011.

9

[26] P. N. P. Géza Freud  orthogonal polynomials and Christoffel functions. A case study. Journal

of Approximation Theory  48(1):3–167  1986.

[27] C. Rasmussen and K. Williams. Gaussian Processes for Machine Learning. The MIT Press 

2006.

[28] A. Rudi  R. Camoriano  and L. Rosasco. Less is more: Nyström computational regularization.

In Advances in Neural Information Processing Systems  pages 1657–1665  2015.

[29] A. Rudi and L. Rosasco. Generalization properties of learning with random features.

Advances in Neural Information Processing Systems  pages 3218–3228  2017.

In

[30] B. Schölkopf  R. Herbrich  and A. Smola. A generalized representer theorem. In International
conference on computational learning theory  pages 416–426. Springer  Berlin  Heidelberg 
2001.

[31] B. Sriperumbudur  K. Fukumizu  and G. Lanckriet. On the relation between universality  char-
acteristic kernels and RKHS embedding of measures. In Thirteenth International Conference
on Artiﬁcial Intelligence and Statistics  volume 9  pages 773–780  2010.

[32] G. Szegö. Orthogonal polynomials. In Colloquium publications  AMS  (23)  fourth edition 

1974.

[33] V. Totik. Asymptotics for Christoffel functions for general measures on the real line. Journal

d’Analyse Mathématique  81(1):283–303  2000.

[34] P. Velleman and R. Welsch. Efﬁcient computing of regression diagnostics. The American

Statistician  35(4):234–242  1981.

[35] E. D. Vito  L. Rosasco  and A. Toigo. Learning sets with separating kernels. Applied and

Computational Harmonic Analysis  37(2):185–217  2014.

[36] S. Wang and Z. Zhang. Improving cur matrix decomposition and the nyström approximation

via adaptive sampling. The Journal of Machine Learning Research  14(1):2729–2769  2013.

[37] Y. Xu. Asymptotics for orthogonal polynomials and Christoffel functions on a ball. Methods

and Applications of Analysis  3(2):257–272  1996.

[38] Y. Xu. Asymptotics of the Christoffel Functions on a Simplex in Rd. Journal of approximation

theory  99(1):122–133  1999.

10

,Changbo Zhu
Huan Xu
Chenlei Leng
Shuicheng Yan
Ricardo Silva
Edouard Pauwels
Francis Bach
Jean-Philippe Vert