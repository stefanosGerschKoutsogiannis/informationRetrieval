2012,Effective Split-Merge Monte Carlo Methods for Nonparametric Models of Sequential Data,Applications of Bayesian nonparametric methods require learning and inference algorithms which efficiently explore models of unbounded complexity. We develop new Markov chain Monte Carlo methods for the beta process hidden Markov model (BP-HMM)  enabling discovery of shared activity patterns in large video and motion capture databases. By introducing split-merge moves based on sequential allocation  we allow large global changes in the shared feature structure. We also develop data-driven reversible jump moves which more reliably discover rare or unique behaviors. Our proposals apply to any choice of conjugate likelihood for observed data  and we show success with multinomial  Gaussian  and autoregressive emission models. Together  these innovations allow tractable analysis of hundreds of time series  where previous inference required clever initialization and at least ten thousand burn-in iterations for just six sequences.,Effective Split-Merge Monte Carlo Methods for

Nonparametric Models of Sequential Data

Michael C. Hughes1  Emily B. Fox2  and Erik B. Sudderth1

1Department of Computer Science  Brown University  {mhughes sudderth}@cs.brown.edu

2Department of Statistics  University of Washington  ebfox@stat.washington.edu

Abstract

Applications of Bayesian nonparametric methods require learning and inference
algorithms which efﬁciently explore models of unbounded complexity. We de-
velop new Markov chain Monte Carlo methods for the beta process hidden
Markov model (BP-HMM)  enabling discovery of shared activity patterns in large
video and motion capture databases. By introducing split-merge moves based on
sequential allocation  we allow large global changes in the shared feature struc-
ture. We also develop data-driven reversible jump moves which more reliably
discover rare or unique behaviors. Our proposals apply to any choice of conjugate
likelihood for observed data  and we show success with multinomial  Gaussian 
and autoregressive emission models. Together  these innovations allow tractable
analysis of hundreds of time series  where previous inference required clever ini-
tialization and lengthy burn-in periods for just six sequences.

1

Introduction

Bayesian nonparametric time series models  including various “inﬁnite” Markov switching pro-
cesses [1  2  3]  provide a promising modeling framework for complex sequential data. We focus
on the problem of discovering coherent  short-term activity patterns  or “behaviors”  shared among
related time series. For example  given collections of videos or human motion capture sequences 
our goal is to (i) identify a concise global library of behaviors that explain the observed motions 
(ii) annotate each sequence with the subset of behaviors used  and (iii) label each timestep with one
active behavior. The beta process hidden Markov model (BP-HMM) [4] offers a promising solution
to such problems  allowing an unbounded set of relevant behaviors to be learned from data.
Learning BP-HMMs from large datasets poses signiﬁcant computational challenges. Fox et al. [4]
considered a dataset containing only six motion capture sequences and proposed a Markov chain
Monte Carlo (MCMC) method that required careful initialization and tens of thousands of burn-in it-
erations. Their sampler included innovations like block state sequence resampling [5] and marginal-
ization of some variables. However  like most MCMC samplers  their proposals only modiﬁed
small subsets of variables at each step. Additionally  the sampler relied on parameter proposals from
priors  leading to low acceptance rates for high-dimensional data. Alternative single-site MCMC
moves  such as those based on slice sampling [6  7]  can exhibit similarly slow mixing. Our goal
is to expose this pervasive issue with conventional MCMC  and develop new samplers that rapidly
explore the structural uncertainty inherent in Bayesian nonparametric models. While our focus is on
the BP-HMM  the technical innovations underlying our samplers are much more broadly applicable.
We make two complementary improvements to previous BP-HMM samplers [4]. First  we develop
split-merge moves which change many variables simultaneously  allowing rapid improvements in
the discovered behavior library. Our approach builds on previous work on restricted Gibbs propos-
als [8] and sequential allocation strategies [9]  both of which were formulated for static Dirichlet

1

Figure 1: Left: The BP-HMM as a directed graphical model. Right: Illustration of our split and merge
proposals  which modify both binary feature assignment matrices F (white indicates present feature) and state
sequences z. We show F  z before (top) and after (bottom) feature km (yellow) is split into ka  kb (red orange).
An item i with fikm = 1 can have either ka  kb  or both after the split  and its new zi sequence can use any
features available in fi. An item without km cannot possess ka  kb  and its state sequence zi does not change.

process (DP) mixture models [10]. Second  we design data-driven [11] reversible jump moves [12]
which efﬁciently discover behaviors unique to a single sequence. These data-driven proposals are
especially important for high-dimensional observation sequences. Both innovations apply to any
likelihood model with a conjugate prior; we show success with multinomial models of discrete
video descriptors  and Gaussian autoregressive models of continuous motion capture trajectories.
We begin in Sec. 2 by reviewing the BP-HMM model. We describe previous BP-HMM samplers [4]
in Sec. 3.1  and then develop our novel split-merge and data-driven proposals in Sec. 3.3-3.4. We
evaluate our contributions in Sec. 4  showing improvements over prior work modeling human mo-
tions captured both via a marker-based mocap system [4] and video cameras [13].

2 Beta Process Hidden Markov Models

Latent feature models intuitively capture the sparse sharing patterns occurring in collections of
human action sequences. For example  one sequence may contain jogging and boxing  while another
has jogging and dancing. We assign the ith sequence or “item” with a sparse binary vector fi =
[fi1  fi2  . . .] indicating the presence or absence of each feature in the unbounded global collection.
Given N items  corpus-wide assignments are denoted by a binary matrix F whose ith row is fi.1
The feature matrix F is generated by an underlying stochastic process  the beta process (BP) [14]:

∞(cid:88)

k=1

bkδθk .

B | B0  γ  β ∼ BP(β  γB0)  B =

(1)
A realization B of the BP contains inﬁnitely many features k. For each feature  θk ∼ B0 marks its
data-generation parameters  while bk ∈ (0  1) denotes its inclusion probability. The binary feature
vector for item i is determined by independent Bernoulli draws fik ∼ Ber(bk). Marginalizing over
B  the number of active features in item i has distribution Poisson(γ)  determined by mass param-
eter γ. The concentration parameter β controls how often features are shared between items [15].
To apply feature models to time series data  Fox et al. [4] combine the BP with a hidden Markov
model to form the BP-HMM  shown in Fig. 1. The binary vector fi determines a ﬁnite set of states
available for item i. Each timestep t is assigned a single state zit = k from the set {k : fik = 1} 
determining which parameters θk generate data xit. Many different data-generation models are
possible. As in [4]  for motion capture data we use a ﬁrst-order Gaussian autoregressive process
with parameters θk = (Ak  Σk) drawn from a matrix-normal inverse-Wishart conjugate prior
Ak  Σk | B0 ∼ MNW−1(ν  S0  R0)

(2)
To study video  we use a Dirichlet-multinomial model for quantized interest point descriptors [13]
(3)

xit | zit = k  xit−1 ∼ N (Akxit−1  Σk)

xit | zit = k ∼ Multinomial(θk)

θk | B0 ∼ Dirichlet(λ0  λ0  . . . λ0)

1Throughout this paper  for variables wijk we use w to denote the vector or collection of wijk’s over the

entire set of subscripts  and wi for the collection over only the omitted subscripts j and k

2

 time series i = 1  2  … Nfeaturesk=1  2  ...zi tzi 2zi 1zi Txi txi 2xi 1xi T......ifiibkkz1z3z2z4z1z3z2z4f1f2f3f4f1f2f3f4splitmergekmkakbThe BP-HMM allows each item independent transition dynamics. The transition distribution πij
from each state j for the HMM of item i is built by drawing a set of transition weights ηi  and then
normalizing these over the set of active features fi:
ηijk ∼ Gamma(α + κδjk  1) 

ηijkfik(cid:80)

πijk =

.

(cid:96) fi(cid:96)ηij(cid:96)

(4)

Here  δjk = 1 if j = k  and 0 otherwise. This construction assigns positive transition mass πijk only
to features k active in fi. The sticky parameter κ places extra expected mass on self-transitions [3] 
biasing the model to learn state sequences z with temporally persistent states.

3 MCMC Inference with Split-Merge Proposals

We ﬁrst summarize the MCMC methods previously proposed for the BP-HMM [4]. We then present
our novel contributions: split-merge moves and data-driven reversible jump proposals. Full algo-
rithmic details for all samplers are found in the supplement  and our code is available online.

3.1 Local Monte Carlo Proposals

Fox et al. [4]’s sampler alternates updates to HMM parameters θ and η  discrete state sequences z 
and feature assignments F. Fixing F deﬁnes a collection of ﬁnite HMMs  so each zi can be block
sampled by dynamic programming [5]  and then θ  η drawn from conjugate posteriors.2 Sampling
each item’s features requires separate updates to features shared with some other time series and
features unique to item i. Both updates marginalize state sequences z and inclusion weights b.
For each shared feature k  Fox et al. propose ﬂipping fik to the complementary binary value and
accept or reject according to the Metropolis-Hastings (MH) rule. This local move alters only one
entry in F while holding all others ﬁxed; the split-merge moves of Sec. 3.3 improve upon it.
For unique features  Fox et al. [4] deﬁne a reversible pair of birth and death moves which add or
delete features to single sequences. While this approach elegantly avoids approximating the inﬁnite
BP-HMM  their birth proposals use the (typically vague) prior to propose emission parameters θk∗
for new features k∗. We remedy this slow exploration with data-driven proposals in Sec. 3.4.

3.2

Split-Merge Proposals for Dirichlet Process Models

Split-merge MCMC methods were ﬁrst applied to nonparametric models by Jain and Neal [8] in
work focusing on DP mixture models with conjugate likelihoods. Conjugacy allows samplers to
operate directly on discrete partitions of observations into clusters  marginalizing emission parame-
ters. Jain and Neal present valid proposals that reversibly split a single cluster km into two (ka  kb) 
or merge two clusters into one. Since merges are deterministic  the primary contribution of [8] is a
generic technique – restricted Gibbs (RG) sampling – for proposing splits consistent with the data.
To construct an initial split of km  the RG sampler ﬁrst assigns items in cluster km at random to either
ka or kb. Starting from this partition  the proposal is constructed by performing one-at-a-time Gibbs
updates  forgetting an item’s current cluster and reassigning to either ka or kb conditioned on the
remaining partitioned data. After several sweeps  these Gibbs updates encourage proposed clusters
ka and kb which agree with the data and thus are more likely to be accepted. For non-conjugate
models  more complex RG proposals can be constructed which instantiate emission parameters [16].
Even in small datasets  there can be signiﬁcant beneﬁts from performing ﬁve or more sweeps for
each RG proposal [8]. For large datasets  however  requiring many sweeps for a single proposal
is computationally expensive. An alternative sequential allocation [9] method replaces the random
initialization of RG by using two randomly chosen items to “anchor” the two new clusters ka  kb.
Remaining items are then sequentially assigned to either ka or kb one-at-a-time  using RG moves
conditioning only on previously assigned data. This creates a proposed partition in agreement with
the data after only one sampling sweep. Recent work has shown some success with sequentially-
allocated split-merge moves for a hierarchical DP topic model [17].

2Fox et al. [4] contains a small error in the resampling of η  as detailed and corrected in the supplement.

3

For nonparametric models not based on the DP  split-merge moves are not well studied. Several
authors have considered RG split-merge proposals for beta process models [18  19  20]. However 
these papers lack technical details  and do not contain experiments showing improved mixing.

3.3

Split-Merge Proposals for the BP-HMM

We now adapt RG and sequential allocation to deﬁne BP-HMM split-merge moves. In the mixture
models considered by prior work [8  9]  each data item i is associated with a single cluster ki 
so selecting two anchors i  j also identiﬁes two cluster indices ki  kj. However  in feature-based
models such as the BP-HMM  each data item i is associated with a collection of features indexed by
fi. Therefore  our proposals require mechanisms for selecting anchors and for choosing candidate
states to split or merge from fi  fj. Additionally  our proposals must allow changes to state sequences
z to reﬂect changes in F. Our proposals thus jointly create a new conﬁguration (F∗  z∗)  collapsing
away HMM parameters θ  η. Fig. 1 illustrates (F  z) before and after a split move.

Selecting Anchors Following [9]  we ﬁrst randomly select distinct anchor data items i and j. The
ﬁxed choice of i  j deﬁnes a split-merge transition kernel satisfying detailed balance. Next  we select
from each anchor one feature it possesses  denoted ki  kj. This choice determines the proposed
move: we merge ki  kj if they are distinct  and split ki = kj into two new features otherwise.
Selecting ki  kj uniformly at random is problematic. First  in datasets with many features choosing
ki = kj is unlikely  making split moves rare. We need to bias the selection process to propose splits
more often. Second  in a well-trained model most feature pairs will not make a sensible merge.
Selecting a pair that explains similar data is crucial for efﬁciency. We thus develop a proposal
distribution which ﬁrst draws ki uniformly from fi  and then selects kj given ﬁxed ki as follows:
qk(ki  kj) = Unif(ki | fi)q(kj | ki  fj) 
(5)
Here  xk is the data assigned to k  m(·) denotes the marginal likelihood of data collapsing away
gives large mass (2/3) to a split move when possible  and also encourages choices ki (cid:54)= kj for a
merge that explain similar data via the marginal likelihood ratio. A large ratio means the model
prefers to explain all data assigned to ki  kj together rather than separately  biasing selection to-
wards promising merge candidates. We ﬁnd higher acceptance rates for merges under this qk  which
justiﬁes the small cost of computing m(·) from cached sufﬁcient statistics.
Once ki  kj are ﬁxed  we construct the candidate state F∗  z∗. As shown in Fig. 1  we only alter
f(cid:96)  z(cid:96) for items (cid:96) which possess either ki or kj. We call this set of items the active set S.

fjkj m(xki  xkj )/(cid:2)m(xki)m(xkj )(cid:3). This construction

emission parameters θ  and Cj = (cid:80)

(cid:40)2Cjfjk

q(kj = k | ki  fj) ∝

if k = ki
o.w.

m(xki )m(xk)

m(xki  xk)

kj(cid:54)=ki

fjk

(cid:96)kb

ika

(cid:96)ka

f∗

(cid:96) given f∗

Split Our split proposal is deﬁned in Alg. 1. Sweeping through a random permutation of items
(cid:96) in the active set S  we draw each item’s assignment to new features ka  kb and resample its state
sequence. We sample [f∗
] from its conditional posterior given previously visited items in
S  requiring that (cid:96) must possess at least one of the new features. We then block sample its state
sequence z∗
(cid:96) . The dynamic programming recursions underlying these proposals use non-
random auxiliary parameters: ˆη(cid:96) is set to its prior mean  and ˆθk to its posterior mean given the current
z. For new states k∗ ∈ {ka  kb}  we initialize ˆθk∗ from anchor sequences and then update to account
for new data assigned to k∗ at each item (cid:96). This enables better matching of proposed features to data
statistics. Finally  we sample f∗  z∗ for anchor items  enforcing f∗
= 1 so the move
remains reversible under a merge. This does not force z∗

i to use ka nor z∗
= 1 for (cid:96) ∈ S  and 0
Merge For a merge move  constructing F∗ is deterministic: we set f∗
(cid:96) for items in S  using a block sampler as in Alg. 1. Again
otherwise. We thus need only to sample z∗
this requires auxiliary HMM parameters ˆθ  ˆη  which we emphasize are deterministic tools enabling
collapsed proposals of discrete indicators F∗  z∗. We never sample θ  η.

= 1 and f∗

j to use kb.

(cid:96)km

jkb

Accept-Reject After drawing a candidate value F∗  z∗  the ﬁnal step is to compute a Metropolis-
Hastings acceptance probability min(ρ  1). We give the ratio for a split move which creates features

4

Alg. 1 Propose split of feature km into ka  kb given F  z  x  anchor items i  j  set S={(cid:96):f(cid:96) km=1}
1: fi [ka kb] ← [1 0]
2: fj [ka kb] ← [0 1]
3: ˆθ ← E [θ | x  z  λ]
4: Sprev = {i  j}
5: for non-anchor items (cid:96) in random permutation of active set S:

use anchor i to create ka
use anchor j to create kb
set HMM params deterministically
initialize set of previously visited items

zi t:zi t=km ← ka
zj t:zj t=km ← kb
ˆη(cid:96) ← E [η(cid:96) | α  κ]  (cid:96) ∈ S

draw f  z and update ˆθ for each item
condition on previously visited items

ﬁnish by sampling f  z for anchors

[0 1]
(cid:26)[1 0]

[1 0] ∝ p(f(cid:96) [kakb] | fSprev [kakb])p(x(cid:96) | f(cid:96)  ˆθ  ˆη(cid:96))
[1 1]

f(cid:96) [kakb] ∼
z(cid:96) ∼ p(z(cid:96) | x(cid:96)  f(cid:96)  ˆθ  ˆη(cid:96))
add (cid:96) to Sprev
for k = ka  kb : ˆθk ← E [θk | λ {xnt : znt = k  n ∈ Sprev}]

(cid:26)[0 1]

fj [kakb] ∼
zj ∼ p(zj | xj  fj  ˆθ  ˆηj )

[1 1]

6:

7:
8:
9:
10: fi [kakb] ∼
11: zi ∼ p(zi | xi  fi  ˆθ  ˆηi )

[1 1]

ka  kb from km below. The acceptance ratio for a merge move is the reciprocal of Eq. (6).
qk(ka  kb | x  F∗  z∗  i  j)
qk(km  km | x  F  z  i  j)

qmerge(F  z | x  F∗  z∗  ka  kb)
qsplit(F∗  z∗ | x  F  z  km)

p(x  z∗  F∗)
p(x  z  F)

ρsplit =

(6)

The joint probability p(x  z  F) is only tractable with conjugate likelihoods. Proposals which instan-
tiate emission parameters θ  as in [16]  would be required in the non-conjugate case.

3.4 Data-Driven Reversible Jump Birth and Death Proposals

Efﬁciently adding or deleting unique features is crucial for good mixing. To accept the birth of new
feature k∗ = K + 1 for item i  this feature must explain some of the observed data xi at least as well
as existing features 1  2  . . . K. High-dimensional emission parameters θk∗ drawn from a vague prior
are unlikely to match the data at hand. Instead  we suggest a data-driven proposal [11  13] for θk∗.
First  select at random a subwindow W of the current sequence i. Next  use data in this subwindow
xW = {xit : t ∈ W} to create a proposal distribution: qθ(θ) = 1
2 p(θ | xW )  which is
a mixture of θ’s prior and posterior given xW . This mixture strikes a balance between proposing
promising new features (via the posterior) while also making death moves possible  since the diffuse
prior will place some mass on the reverse birth move.

2 p(θ) + 1

Let Ui denote the number of unique features in fi and ν = γ
a birth move to candidate state f∗

β

i   η∗
p(xi | f∗
i   θ∗)
p(xi | fi  ηi  θ)

N−1+β . The acceptance probability for
i   θ∗ is then min(ρbirth  1)  where
i   η∗
pθ(θ∗
k∗ )
qθ(θ∗
k∗ )

Poi(Ui + 1 | ν)
Poi(Ui | ν)

qf (fi | f∗
i )
i | fi)
qf (f∗

(7)

ρbirth =

Eq. (7) is similar to the ratio for the birth proposal from the prior  adding only one term to account
for the proposed θ∗
k∗. Note that each choice of W deﬁnes a valid pair of birth-death moves satisfying
detailed balance  so we need not account for this choice in the acceptance ratio [21].

4 Experimental Results

Our experiments compare competing inference algorithms for the BP-HMM; for comparisons to
alternative models  see [4]. To evaluate how well our novel moves explore the posterior distribu-
tion  we compare three possible methods for adding or removing features: split-merge moves (SM 
Sec. 3.3)  data-driven moves (DD  Sec. 3.4)  and reversible jump moves using the prior (Prior [4] 
Sec. 3.1). All experiments interleave the chosen update with the standard MH updates to shared
features of F and Gibbs updates to HMM parameters θ  η described in Sec. 3.1.
For each comparison  we run multiple initializations and rank the chains from “best” to “worst”
according to joint probability. Each chain is allowed the same amount of computer time.

5

Gaussian

AR

Multinomial

Worst SM

Worst DD

Best Prior

Figure 2: Feature creation for synthetic data with Gaussian (left)  AR (middle)  or multinomial (right) likeli-
hoods. Each run begins with one feature used by all items  and must add new features via split-merge proposals
(SM)  or reversible-jump moves using data-driven (DD) or prior (Prior) proposals. Top: Log joint probability
versus computation time  for 10 random initializations of each sampling algorithm. Bottom: Emission pa-
rameters associated with the last sample after one hour of computation time. Gaussian θ = (µ  Σ) and AR
θ = (A  Σ) shown as contour lines in ﬁrst two dimensions  with location determined by µ  A. Multinomial θ
shown as image where row k gives the emission distribution over vocabulary symbols for state k.

4.1 Synthetic Data

We examined toy datasets generated by a known set of 8 features (behaviors) θtrue. To validate
that our contributions apply for many choices of likelihood  we create three datasets: multinomial
“bag-of-words“ emissions using 500 vocabulary words  8–dimensional Gaussian emissions  and a
ﬁrst-order autoregressive (AR) processes with 5 dimensions. Each dataset has N = 100 sequences.
First  we study how well each method creates necessary features from scratch. We initialize the
sampler with just one feature used by all items  and examine how many true states are recovered
after one hour of computation time across 10 runs. We show trace plots as well as illustrations of
recovered emission parameters θ in Fig. 2. All runs of both SM and DD moves ﬁnd all true states
within several minutes  while no Prior run recovers all true states  remaining stuck with merged
versions of true features. DD moves add new features most rapidly due to low computational cost.
We next examine whether each inference method can remove unnecessary features. We consider a
different toy dataset of several hundred sequences and a redundant initialization in which 2 copies
of each true state exist. Half of the sequences are initialized with f   z set to corresponding true
values in copy 1  and the second half using copy 2. Using Gaussian and AR likelihoods  all SM runs
merge down to the 8 true states  at best within ﬁve minutes  but no DD or Prior run ever reaches
this optimal conﬁguration in the allotted hour. Merge moves enable critical global changes  while
the one-at-a-time updates of [4] (and our DD variant) must take long random walks to completely
delete a popular feature. Further details are provided in the supplementary material.
These results demonstrate the importance of DD birth and split moves for exploring new features 
and merge moves for removing features via proposals of large assignment changes. As such  we
consider a sampler that interleaves SM and DD moves in our subsequent analyses.

4.2 Motion Capture Data

We next apply our improved MCMC methods to motion capture (mocap) sequences from the CMU
database [22]. First  we consider the small dataset examined by [4]: 6 sequences of physical ex-
ercises with motion measurements at 12 joint angles  modeled with an AR(1) likelihood. Human
annotation identiﬁes 12 actions  which we take as ground truth. Previous results [4] show that the
BP-HMM outperforms competitors in segmenting these actions  although they report that some true
actions like jogging are split across multiple recovered features (see their Fig. 5). We set likelihood
hyperparameters similarly to [4]  with further details provided in the supplement.

6

0500100015002000250030003500−4−2024x 104cpu time (sec)log joint prob. SMDDPrior0500100015002000250030003500−1.4−1.2−1−0.8x 106cpu time (sec)log joint prob. SMDDPrior0500100015002000250030003500−6.5−6−5.5−5−4.5x 105cpu time (sec)log joint prob. SMDDPrior−101−1.5−1−0.500.511.5−1−0.500.51−0.200.21002003004005002468−101−1.5−1−0.500.511.5−1−0.500.51−0.200.21002003004005002468−101−1.5−1−0.500.511.5−1−0.500.51−0.200.2100200300400500246Figure 3: Analysis of 6 motion capture sequences previously considered by [4]. Left: Joint log probability
and Hamming distance (from manually annotated behaviors) for 20 runs of each method over 10 hours. Right:
Examples of arm circles and jogging from 3 sequences  along with estimated zi of last sample from the best
run of each method. SM+DD moves (top row started from one feature  middle row started with 5 unique
states per sequence) successfully explain each action with one primary state  while [4]’s sampler (bottom row)
started from 5 unique features remains stuck with multiple unique states for one true action.

Ballet

Walk

Squat

Sword

Lambada

Dribble Basketball

Box

Figure 4: Analysis of 124 mocap sequences  showing 10 of 33 recovered behaviors. Skeleton trajectories are
built from contiguous segments ≥ 1 sec long assigned to each behavior. Boxed groups contain segments from
distinct sequences assigned to the same state. Some states only appear in one sequence.

Climb

Indian Dance

Tai Chi

In Fig. 3  we compare a sampler which interleaves SM and DD moves with [4]’s original method.
We run 20 chains of each method for ten hours from two initializations: unique5  which assigns
5 unique features per sequence (as done in [4])  and one  using a single feature across all items.
In both log probability and Hamming distance  SM+DD methods are noticeably superior. Most
interestingly  SM+DD starting from a parsimonious one feature achieves best performance overall 
showing that clever initialization is not necessary with our algorithm. The best run of SM+DD from
one achieves Hamming distance of 0.22  compared to 0.3 reported in [4]. No Prior proposal run
from one created any additional states  indicating the importance of using our improved methods
of feature exploration even in moderate dimensions.
Our SM moves are critical for effectively creating and deleting features. Example segmentations of
arm-circles and jogging actions in Fig. 3 show that SM+DD consistently use one dominant behavior
across all segments where the action appears. In contrast  the Prior remains stuck with some unique
behaviors used in different sequences  yielding lower probability and larger Hamming distance.
Next  we study a larger dataset of 124 sequences  all “Physical Activities & Sports” examples from
the CMU mocap dataset. Analyzing a dataset of this size is computationally infeasible using the
methods of [4]. Initializing from unique5 would create over 500 features  requiring a prohibitively
long sampling run to merge related behaviors. When initialized from one  the Prior sampler creates
no additional features. In contrast  starting from one  our SM+DD moves rapidly identify a diverse
set of 33 behaviors. A set of 10 behaviors representative of this dataset are shown in Fig. 4. Our
improved algorithm robustly explores the posterior space  enabling this large-scale analysis.

4.3 CMU Kitchen: Activity Discovery from Video

Finally  we apply our new inference methods to discover common motion patterns from videos of
recipe preparation in the CMU Kitchen dataset [23]. Each video is several minutes long and depicts
a single actor in the same kitchen preparing either a pizza  a sandwich  a salad  brownies  or eggs.
Our previous work [13] showed promising results in activity discovery with the BP-HMM using a

7

00.511.522.533.5x 104−6−5.8−5.6−5.4x 104cpu time (sec)joint log prob. SM+DD from oneSM+DD from unique5Prior from unique5−15−10−50510246510152025xzy−15−10−505102468510152025xzy−10−5051015−2024510152025xzy−6−4−202−20246510152025xzy−4−2024−4−202451015202530xzy−6−4−202−4−202451015202530xzy00.511.522.533.5x 1040.20.30.40.50.60.7cpu time (sec)Hamming dist. SM+DD from oneSM+DD from unique5Prior from unique555555555555555555555520020521021522022523023524055655555555555555510511011512012513013514055555555515555551851901952002052102152222222222222222222224247525762677277822222222222222222222221651701751801852222222222222222111111451501551601655555555555555555555552002052102152202252302352405565555555555555551051101151201251301351405555555555555555185190195200205210215222222222222222222222424752576267727782222222222222222222222165170175180185222222222222222221111145150155160165555555555555555555555200205210215220225230235240202020202020202020202055555551051101151201251301351405555555528285555528185190195200205210215141414141414141422222222222224247525762677277822121212121212121212121212121212121212121211651701751801852522262525252525222252525111111145150155160165Figure 5: Activity discovery with 126 Kitchen videos  showing locations of select behaviors over time. Each
row summarizes zi for a single video  labeled at left by recipe type (label not provided to the BP-HMM). We
show only behaviors assigned to at least two timesteps in a local window.

small collection of 30 videos from this collection. We compare our new SM moves on this small
dataset  and then study a larger set of 126 Kitchen videos using our improved sampler.
Using only the 30 video subset  Fig. 6 compares the combined SM+DD sampler with just DD or
Prior moves  using ﬁxed hyperparameter settings as in [13] and starting with just one feature. DD
proposals offer signiﬁcant gains over the prior  and further interleaving DD and SM moves achieves
the best overall conﬁguration  showing the beneﬁts of proposing global changes to F  z.
Finally  we run the SM+DD sampler on 126 Kitchen se-
quences  choosing the best of 4 chains after several days
of computer time (trace plots show convergence in half this
time). Fig. 5 maps behavior assignments over time across all
ﬁve recipes  using the last MCMC sample. Several intuitive
behavior sharing patterns exist: chopping happens with car-
rots (salad) and pepperoni (pizza)  while stirring occurs when
preparing brownies and eggs. Non-uniform behavior usage
patterns within a category are due to differences in available
cooking equipment across videos. Please see the supplement
for more experimental details and results.

Figure 6: Joint log probability versus
computation time for various samplers
on the CMU Kitchen data [23] previ-
ously studied by [13].

5 Discussion

We have developed efﬁcient MCMC inference methods for the BP-HMM. Our proposals do not re-
quire careful initialization or parameter tuning  and enable exploration of large datasets intractable
under previous methods. Our approach makes efﬁcient use of data and applies to any choice of con-
jugate emission model. We expect the guiding principles of data-driven and sequentially-allocated
proposals to apply to many other models  enabling new applications of nonparametric analysis.

Acknowledgments

M. Hughes was supported in part by an NSF Graduate Research Fellowship under Grant No.
DGE0228243. E. Fox was funded in part by AFOSR Grant FA9550-12-1-0453.

8

BrownieFraction of Time Elapsed0.250.50.751PizzaSandwichSaladEggs 65 OthersLight SwitchOpen FridgeStir Bowl 1Stir Bowl 2Pour BowlGrate CheeseSlice/ChopFlip Omelette012345678x 104−7.2−7.15−7.1−7.05x 106cpu time (sec)log joint prob. SM+DDDDPriorReferences
[1] M. Beal  Z. Ghahramani  and C. Rasmussen. The inﬁnite hidden Markov model. In NIPS  2002.
[2] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581  2006.

[3] E. B. Fox  E. B. Sudderth  M. I. Jordan  and A. S. Willsky. A sticky HDP-HMM with application to

speaker diarization. Annals of Applied Statistics  5(2A):1020–1056  2011.

[4] E. B. Fox  E. B. Sudderth  M. I. Jordan  and A. S. Willsky. Sharing features among dynamical systems

with beta processes. In NIPS  2010.

[5] S. L. Scott. Bayesian methods for hidden Markov models: Recursive computing in the 21st century.

JASA  97(457):337–351  2002.

[6] J. Van Gael  Y. Saatci  Y. W. Teh  and Z. Ghahramani. Beam sampling for the inﬁnite hidden Markov

model. In ICML  2008.

[7] C. Yau  O. Papaspiliopoulos  G. O. Roberts  and C. Holmes. Bayesian non-parametric hidden Markov

models with applications in genomics. JRSS B  73(1):37–57  2011.

[8] S. Jain and R.M. Neal. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process

mixture model. Journal of Computational and Graphical Statistics  13(1):158–182  2004.

[9] D. B. Dahl. Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process

mixture models. Submitted to Journal of Computational and Graphical Statistics  2005.

[10] M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures.

90(430):577–588  1995.

JASA 

[11] Z. Tu and S. C. Zhu. Image segmentation by data-driven Markov chain Monte Carlo. PAMI  24(5):657–

673  2002.

[12] P.J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.

Biometrika  82(4):711–732  1995.

[13] M. C. Hughes and E. B. Sudderth. Nonparametric discovery of activity patterns from video collections.

In CVPR Workshop on Perceptual Organization in Computer Vision  2012.

[14] R. Thibaux and M. I. Jordan. Hierarchical beta processes and the Indian buffet process. In AISTATS 

2007.

[15] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In NIPS 

2007.

[16] S. Jain and R.M. Neal. Splitting and merging components of a nonconjugate Dirichlet process mixture

model (with invited discussion). Bayesian Analysis  2(3):445–500  2007.

[17] C. Wang and D. Blei. A split-merge MCMC algorithm for the hierarchical Dirichlet process.

arXiv:1201.1657v1 [stat.ML]  2012.

[18] E. Meeds  R. Neal  Z. Ghahramani  and S. Roweis. Modeling dyadic data with binary latent factors. In

NIPS  2008.

[19] K. Miller  T. Grifﬁths  and M. Jordan. Nonparametric latent feature models for link prediction. In NIPS 

2009.

[20] M. Mørup  M. N. Schmidt  and L. K. Hansen.

Inﬁnite multiple membership relational modeling for
complex networks. In IEEE International Workshop on Machine Learning for Signal Processing  2011.
[21] L. Tierney. Markov chains for exploring posterior distributions (with discussion). Annals of Statistics 

22:1701–1762  1994.

[22] Carnegie Mellon University. Graphics lab motion capture database. http://mocap.cs.cmu.edu/.
[23] F. De la Torre et al. Guide to the Carnegie Mellon University Multimodal Activity (CMU-MMAC)

database. Technical Report CMU-RI-TR-08-22  Robotics Institute  Carnegie Mellon University  2009.

9

,Yixiao Ge
Zhuowan Li
Haiyu Zhao
Guojun Yin
Shuai Yi
Xiaogang Wang
hongsheng Li