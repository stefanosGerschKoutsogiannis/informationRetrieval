2018,A no-regret generalization of hierarchical softmax to extreme multi-label classification,Extreme multi-label classification (XMLC) is a problem of tagging an instance with a small subset of relevant labels chosen from an extremely large pool of possible labels. Large label spaces can be efficiently handled by organizing labels as a tree  like in the hierarchical softmax (HSM) approach commonly used for multi-class problems. In this paper  we investigate probabilistic label trees (PLTs) that have been recently devised for tackling XMLC problems.  We show that PLTs are a no-regret multi-label generalization of HSM when precision@$k$ is used as a model evaluation metric.  Critically  we prove that pick-one-label heuristic---a reduction technique from multi-label to multi-class that is routinely used along with HSM---is not consistent in general.  We also show that our implementation of PLTs  referred to as extremeText (XT)  obtains significantly better results than HSM with the pick-one-label heuristic and XML-CNN  a deep network specifically designed for XMLC problems. Moreover  XT is competitive to many state-of-the-art approaches in terms of statistical performance  model size and prediction time which makes it amenable to deploy in an online system.,A no-regret generalization of hierarchical softmax to

extreme multi-label classiﬁcation

Marek Wydmuch

Institute of Computing Science

Poznan University of Technology  Poland

mwydmuch@cs.put.poznan.pl

Kalina Jasinska

Institute of Computing Science

Poznan University of Technology  Poland

kjasinska@cs.put.poznan.pl

Mikhail Kuznetsov
Yahoo! Research
New York  USA

kuznetsov@oath.com

Róbert Busa-Fekete

Yahoo! Research
New York  USA

busafekete@oath.com

Krzysztof Dembczy´nski

Institute of Computing Science

Poznan University of Technology  Poland

kdembczynski@cs.put.poznan.pl

Abstract

Extreme multi-label classiﬁcation (XMLC) is a problem of tagging an instance with
a small subset of relevant labels chosen from an extremely large pool of possible
labels. Large label spaces can be efﬁciently handled by organizing labels as a tree 
like in the hierarchical softmax (HSM) approach commonly used for multi-class
problems. In this paper  we investigate probabilistic label trees (PLTs) that have
been recently devised for tackling XMLC problems. We show that PLTs are a
no-regret multi-label generalization of HSM when precision@k is used as a model
evaluation metric. Critically  we prove that pick-one-label heuristic—a reduction
technique from multi-label to multi-class that is routinely used along with HSM—is
not consistent in general. We also show that our implementation of PLTs  referred
to as EXTREMETEXT (XT)  obtains signiﬁcantly better results than HSM with the
pick-one-label heuristic and XML-CNN  a deep network speciﬁcally designed for
XMLC problems. Moreover  XT is competitive to many state-of-the-art approaches
in terms of statistical performance  model size and prediction time which makes it
amenable to deploy in an online system.

1

Introduction

In several machine learning applications  the label space can be enormous  containing even millions
of different classes. Learning problems of this scale are often referred to as extreme classiﬁcation.
To name a few examples of such problems  consider image and video annotation for multimedia
search (Deng et al.  2011)  tagging of text documents for categorization of Wikipedia articles (Dekel
& Shamir  2010)  recommendation of bid words for online ads (Prabhu & Varma  2014)  or prediction
of the next word in a sentence (Mikolov et al.  2013).
To tackle extreme classiﬁcation problems in an efﬁcient way  one can organize the labels into a
tree. A prominent example of such label tree model is hierarchical softmax (HSM) (Morin &
Bengio  2005)  often used with neural networks to speed up computations in multi-class classiﬁcation
with large output spaces. For example  it is commonly applied in natural language processing
problems such as language modeling (Mikolov et al.  2013). To adapt HSM to extreme multi-label
classiﬁcation (XMLC)  several very popular tools  such as FASTTEXT (Joulin et al.  2016) and
LEARNED TREE (Jernite et al.  2017)  apply the pick-one-label heuristic. As the name suggests  this
heuristic randomly picks one of the labels from a multi-label training example and treats the example
as a multi-class one.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

In this work  we exhaustively investigate the multi-label extensions of HSM. First  we show that the
pick-one-label strategy does not lead to a proper generalization of HSM for multi-label setting. More
precisely  we prove that using the pick-one-label reduction one cannot expect any multi-class learner
to achieve zero regret in terms of marginal probability estimation and maximization of precision@k.
As a remedy to this issue  we are going to revisit probabilistic label trees (PLTs) (Jasinska et al. 
2016) that have been recently introduced for solving XMLC problems. We show that PLTs are a
theoretically motivated generalization of HSM to multi-label classiﬁcation  that is  1) PLTs and
HSM are identical in multi-class case  and 2) a PLT model can get zero regret (i.e.  it is consistent)
in terms of marginal probability estimation and precision@k in the multi-label setting.
Beside our theoretical ﬁndings  we provide an efﬁcient implementation of PLTs  referred to as XT 
that we build upon FASTTEXT. The comprehensive empirical evaluation shows that it gets signiﬁcantly
better results than the original FASTTEXT  LEARNED TREE  and XML-CNN  a speciﬁcally designed
deep network for XMLC problems. XT also achieves competitive results to other state-of-the-art
approaches  being very efﬁcient in model size and prediction time  particularly in the online setting.
This paper is organized as follows. First we discuss the related work and situate our approach in
the context. In Section 3 we formally state the XMLC problem and present some useful theoretical
insights. Next  we brieﬂy introduce the HSM approach  and in Section 5 we show theoretical results
concerning the pick-one-label heuristic. Section 6 formally introduces PLTs and presents the main
theoretical results concerning them and their relation to HSM. Section 7 provides implementation
details of PLTs. The experimental results are presented in Section 8. Finally we make concluding
remarks.

2 Related work

Historically  problems with a large number of labels were usually solved by nearest neighbor or
decision tree methods. Some of today’s algorithms are still based on these classical approaches 
signiﬁcantly extending them by a number of new tricks. If the label space is of moderate size (like
a few thousands of labels) then an independent model can be trained for each class. This is the
so-called 1-VS-ALL approach. Unfortunately  it scales linearly with the number of labels  which
is too costly for many applications. The extreme classiﬁcation algorithms try to improve over this
approach by following different paradigms such as sparsity of labels (Yen et al.  2017; Babbar &
Schölkopf  2017)  low-rank approximation (Mineiro & Karampatziakis  2015; Yu et al.  2014; Bhatia
et al.  2015)  tree-based search (Prabhu & Varma  2014; Choromanska & Langford  2015)  or label
ﬁltering (Vijayanarasimhan et al.  2014; Shrivastava & Li  2015; Niculescu-Mizil & Abbasnejad 
2017).
In this paper we focus on tree-based algorithms  therefore we discuss them here in more detail. There
are two distinct types of these algorithms: decision trees and label trees. The former type follows
the idea of classical decision trees. However  the direct use of the classic algorithms can be very
costly (Agrawal et al.  2013). Therefore  the FASTXML algorithm (Prabhu & Varma  2014) tackles
the problem in a slightly different way. It uses sparse linear classiﬁers in internal tree nodes to split
the feature space. Each linear classiﬁer is trained on two classes that are formed in a random way
ﬁrst and then reshaped by optimizing the normalized discounted cumulative gain. To improve the
overall accuracy FASTXML uses an ensemble of trees. This algorithm  like many other decision tree
methods  works in a batch mode. Choromanska & Langford (2015) have succeeded to introduce a
fully online decision tree algorithm that also uses linear classiﬁers in internal nodes of the tree.
In label trees each label corresponds to one and only one path from the root to a leaf. Besides PLTs
and HSM  there exist several other instances of this approach  for example  ﬁlter trees (Beygelzimer
et al.  2009b; Li & Lin  2014) or label embedding trees (Bengio et al.  2010). It is also worth to
underline that algorithms similar to HSM have been introduced independently in many different
research ﬁelds  such as nested dichotomies (Fox  1997) in statistics  conditional probability estimation
trees (Beygelzimer et al.  2009a) in multi-class classiﬁcation  multi-stage classiﬁers (Kurzynski  1988)
in pattern recognition  and probabilistic classiﬁer chains (Dembczynski et al.  2010) in multi-label
classiﬁcation under the subset 0/1 loss. All these methods have been jointly analyzed in (Dembczynski
et al.  2016).
A still open problem in label tree approaches is the tree structure learning. FASTTEXT (Joulin
et al.  2016) uses HSM with a Huffman tree built on the label frequencies. Jernite et al. (2017)

2

have introduced a new algorithm  called LEARNED TREE  which combines HSM with a speciﬁc
hierarchical clustering that reassigns labels to paths in the tree in a semi-online manner. Prabhu et al.
(2018) follows another approach in which a model similar to PLTs is trained in a batch mode and a
tree is built by using recursively balanced k-means over the label proﬁles. In Section 7 we discuss
this approach in more detail.
The HSM model is often used as an output layer in neural networks. The FASTTEXT implementation
can also be viewed as a shallow architecture with one hidden layer that represents instances as
averaged feature (i.e.  word) vectors. Another neural network-based model designed for XMLC
has been introduced in (Liu et al.  2017). This model  referred to as XML-CNN  uses a complex
convolutional deep network with a narrow last layer to make it work with large output spaces. As we
show in the experimental part  this quite expensive architecture gets inferior results in comparison to
our PLTs built upon FASTTEXT.

3 Problem statement
Let X denote an instance space  and let L = {1  . . .   m} be a ﬁnite set of m class labels. We assume
that an instance x 2X is associated with a subset of labels Lx 2 2L (the subset can be empty); this
subset is often called a set of relevant labels  while the complement L\Lx is considered as irrelevant
for x. We assume m to be a large number (e.g.   105)  but the size of the set of relevant labels
Lx is much smaller than m  i.e.  |Lx|⌧ m. We identify a set Lx of relevant labels with a binary
(sparse) vector y = (y1  y2  . . .   ym)  in which yj = 1   j 2L x. By Y = {0  1}m we denote a set
of all possible label vectors. We assume that observations (x  y) are generated independently and
identically according to the probability distribution P(X = x  Y = y) (denoted later by P(x  y))
deﬁned on X⇥Y .
The problem of XMLC can be deﬁned as ﬁnding a classiﬁer h(x) = (h1(x)  h2(x)  . . .   hm(x)) 
which in general can be deﬁned as a mapping X!R m  that minimizes the expected loss (or risk):

L`(h) = E(x y)⇠P(x y)(`(y  h(x))  

where `(y  ˆy) is the (task) loss. The optimal classiﬁer  the so-called Bayes classiﬁer  for a given loss
function ` is:

h⇤` = arg min

L`(h) .

h

The regret of a classiﬁer h with respect to ` is deﬁned as:

reg`(h) = L`(h)  L`(h⇤` ) = L`(h)  L⇤` .

The regret quantiﬁes the suboptimality of h compared to the optimal classiﬁer h⇤. The goal could be
then deﬁned as ﬁnding h with a small regret  ideally equal to zero.
In the following  we aim at estimating the marginal probabilities ⌘j(x) = P(yj = 1| x). As we will
show below  marginal probabilities are a key element to optimally solve extreme classiﬁcation for
many performance measures  like Hamming loss  macro-F measure  and precision@k. To obtain the
marginal probability estimates one can use the label-wise log loss as a surrogate:

`log(y  h(x)) =

`log(yj  hj(x)) =

(yj log(hj(x)) + (1  yj) log(1  hj(x))) .

mXj=1

mXj=1

Then the expected label-wise log loss for a single x (i.e.  the so-called conditional risk) is:

Ey`log(y  h(x)) =

Ey`log(yj  hj(x)) =

mXj=1

mXj=1

Llog(hj(x)| x) .

Therefore  it is easy to see that the pointwise optimal prediction for the j-th label is given by:

h⇤j (x) = arg min

h

Llog(hj(x)| x) = ⌘j(x) .

For the macro F-measure it sufﬁces in turn to ﬁnd an optimal threshold on marginal probabilities for

As shown in (Dembczynski et al.  2010)  the Hamming loss is minimized by h⇤j (x) =J⌘j(x) > 0.5K .

3

each label separately as proven in (Ye et al.  2012; Narasimhan et al.  2014; Jasinska et al.  2016;
Dembczynski et al.  2017). In the following  we will show a similar result for precision@k which has
become a standard measure in extreme classiﬁcation (although it is also often criticized  as it favors
the most frequent labels).
Precision@k can be formally deﬁned as:

precision@k(y  x  h) =

(1)

1

k Xj2 ˆYkJyj = 1K 
k Xj2 ˆYk

1

where ˆYk is a set of k labels predicted by h for x. To be consistent with the former discussion  let us
deﬁne a loss function for precision@k as `p@k = 1  precision@k. The conditional risk is then:1

Lp@k(h| x) = Ey`p@k(y  x  h) = 1 

⌘j(x) .

The above result shows that the optimal strategy for precision@k is to predict k labels with the
highest marginal probabilities ⌘j(x). As the main theoretical result given in this paper is a regret
bound for precision@k  let us deﬁne here the conditional regret for this metric:

regp@k(h| x) =

⌘i(x) 

1

k Xi2Yk

1

k Xj2 ˆYk

⌘j(x)  

where Yk is a set containing the top k labels with respect to the true marginal probabilities.
From the above results  we see that estimation of marginal probabilities is crucial for XMLC problems.
To obtain these probabilities we can use the vanilla 1-VS-ALL approach trained with the label-wise
log loss. Unfortunately  1-VS-ALL is too costly in the extreme setting. In the following sections  we
discuss an alternative approach based on the label trees that estimates the marginal probabilities with
the competitive accuracy  but in a much more efﬁcient way.

4 Hierarchical softmax approaches

Hierarchical softmax (HSM) is designed for multi-class classiﬁcation. Using our notation  for multi-
i=1 yi = 1  i.e.  there is one and only one label assigned to an instance

class problems we havePm
(x  y). The marginal probabilities ⌘j(x) in this case sum up to 1.
The HSM classiﬁer h(x) takes a form of a label tree. We encode all labels from L using a preﬁx
code. Any such code can be given in a form of a tree in which a path from the root to a leaf node
corresponds to a code word. Under the coding  each label yj = 1 is uniquely represented by a code
word z = (z1  . . .   zl) 2C   where l is the length of the code word and C is a set of all code words.
For zi 2{ 0  1}  the code and the label tree are binary. In general  the code alphabet can contain more
than two symbols. Furthermore  zis can take values from different sets of symbols depending on the
previous values in the code word. In other words  the code can result with nodes of a different arity
even in the same tree  like in (Grave et al.  2017) and (Prabhu et al.  2018). We will brieﬂy discuss
different tree structures in Section 7.
A tree node can be uniquely identiﬁed by the partial code word zi = (z1  . . .   zi). We denote the
root node by z0  which is an empty vector (without any elements). The probability of a given label is
determined by a sequence of decisions made by node classiﬁers that predict subsequent values of the
code word. By using the chain rule of probability  we obtain:

⌘j(x) = P(yj = 1| x) = P(z | x) =

lYi=1

P(zi | zi1  x) .

By using logistic loss and a linear model fzi(x) in each node zi for estimating P(zi | zi1  x)  we
obtain the popular formulation of HSM. Let us notice that since we deal here with a multi-class
distribution  we have that:

P(zi = c| zi1  x) = 1 .

(2)

Xc

1The derivation is given in Appendix A.

4

Because of this normalization  we can assume that a multi-class (or binary in the case of binary
trees) classiﬁer is situated in all internal nodes and there are no classiﬁers in the leaves of the tree.
Alternatively  we can assume that each node  except the root  is associated with a binary classiﬁer that
estimates P(zi = c| zi1  x)  but then the additional normalization (2) has to be performed. This
alternative formulation is important for the multi-label extension of HSM discussed in Section 6. In
either way  learning of the node classiﬁers can be performed simultaneously as independent tasks.
Note that estimate ˆ⌘j(x) of the probability of label j can be easily obtained by traversing the tree
along the path indicated by the code of the label. Unfortunately  the task of predicting top k labels
is more involved as it requires searching over the tree. Popular solutions are beam search (Kumar
et al.  2013; Prabhu et al.  2018)  uniform-cost search (Joulin et al.  2016)  and its approximate
variant (Dembczynski et al.  2012  2016).

5 Suboptimality of HSM for multi-label classiﬁcation

To deal with multi-label problems  some popular tools  such as FASTTEXT (Joulin et al.  2016) and its
extension LEARNED TREE (Jernite et al.  2017)  apply HSM with the pick-one-label heuristic which
randomly picks one of the positive labels from a given training instance. The resulting instance is then
treated as a multi-class instance. During prediction  the heuristic returns a multi-class distribution and
the k most probable labels. We show below that this speciﬁc reduction of the multi-label problem to
multi-class classiﬁcation is not consistent in general.

maps the multi-label distribution to a multi-class distribution in the following way:

Since the probability of picking a label j from y is equal to yj/Pm
yjPm

⌘0j(x) = P0(yj = 1| x) =Xy2Y

It can be easily checked that the resulting ⌘0j(x) form a multi-class distribution as the probabilities
sum up to 1. It is obvious that that the heuristic changes the marginal probabilities of labels  unless
the initial distribution is multi-class. Therefore this method cannot lead to consistent classiﬁers in
terms of estimating ⌘j(x). As we show below  it is also not consistent for precision@k in general.
Proposition 1. A classiﬁer h such that hj(x) = ⌘0j(x) for all j 2{ 1  . . .   m} has in general a
non-zero regret in terms of precision@k.

j0=1 yj0  the pick-one-label heuristic

P(y | x)

(3)

j0=1 yj0

Proof. We prove the proposition by giving a simple counterexample. Consider the following condi-
tional distribution for some x:

P(y = (1  0  0)| x) = 0.1   P(y = (1  1  0)| x) = 0.5   P(y = (0  0  1)| x) = 0.4 .

The optimal top 1 prediction for this example is obviously label 1  since the marginal probabilities are
⌘1(x) = 0.6 ⌘ 2(x) = 0.5 ⌘ 3(x) = 0.4. However  the pick-one-label heuristic will transform the
original distribution to the following one: ⌘01(x) = 0.35 ⌘ 02(x) = 0.25 ⌘ 03(x) = 0.4. The predicted
top label will be then label 3  giving the regret of 0.2 for precision@1.

The proposition shows that the heuristic is in general inconsistent for precision@k. Interestingly  the
situation changes when the labels are conditionally independent  i.e.  P(y | x) =Qm
j=1 P(yi | x) .
Proposition 2. Given conditionally independent labels  a classiﬁer h such that hj(x) = ⌘0j(x) for
all j 2{ 1  . . .   m} has zero regret in terms of the precision@k loss.
Proof. We show here only a sketch of the proof. The full proof is given in Appendix B. To prove
the theorem  it is enough to show that in the case of conditionally independent labels the pick-
one-label heuristic does not change the order of marginal probabilities. Let yi and yj be so that
P(yi = 1| x)  P(yj = 1| x). Then in the summation over all ys in (3)  we are interested
in four different subsets of Y  Su w
i j = {y 2Y : yi = u ^ yj = w}  where u  w 2{ 0  1}.
Remark that during mapping none of y 2 S0 0
i j   the value
of yt/(Pm
t0=1 yt0) ⇥ P(y | x)  for t 2{ i  j}  is the same for both yi and yj. Now  let y0 2 S1 0
i j
and y00 2 S0 1
i j be the same on all elements except the i-th and the j-th one. Then  because

i j plays any role  and for each y 2 S1 1

5

of the label independence and the assumption that P(yi = 1| x)  P(yj = 1| x)  we have
P(y0 | x)  P(y00 | x). Therefore  after mapping we obtain ⌘0i(x)  ⌘0j(x). Thus  for independent
labels  the pick-one-label heuristic is consistent for precision@k.

6 Probabilistic label trees

The section above has revealed that HSM cannot be properly adapted to multi-label problems by
the pick-one-label heuristic. There is  however  a different way to generalize HSM to obtain no-
regret estimates of marginal probabilities ⌘j(x). The probabilistic label trees (PLTs) (Jasinska
et al.  2016) can be derived in the following way. Let us encode yj = 1 by a slightly extended
code z = (1  z1  . . .   zl) in comparison to HSM. The new code gets 1 at the zero position what
corresponds to a question whether there exists at least one label assigned to the example. As before 
each node is uniquely identiﬁed by a partial code zi which says that there is at least one positive
label in a subtree rooted in that node. It can be easily shown by the chain rule of probability that the
marginal probabilities can be expressed in the following way:

⌘j(x) = P(z | x) =

P(zi | zi1  x) .

(4)

lYi=0

Xc

The difference to HSM is the probability P(z0 = 1| x) in the chain and a different normalization 
i.e.:
(5)

P(zi = c| zi1  x)  1 .

Only for z0 we have P(z0 = 1| x) + P(z0 = 0| x) = 1. Because of (5)  the binary models that
estimate P(zi = c| zi1  x) (against P(zi 6= c| zi1  x)) are situated in all nodes of the tree (i.e. 
also in the leaves). The models can be trained independently as before for HSM. Only during
prediction  one can re-calibrate the estimates when (5) is not satisﬁed  for example  by normalizing
them to sum up to 1. It can be easily noticed that for a multi-class distribution  the resulting model
of PLTs boils down to HSM  since P(z0 = 1| x) is always equal 1  and in addition  normalization
(5) will take the form of (2). In Appendix D we additionally present the pseudocode of training and
predicting with PLTs.
Next  we show that the PLT model obeys strong theoretical guarantees. Let us ﬁrst revise the result
from (Jasinska et al.  2016) that relates the absolute difference between the true and the estimated
marginal probability of label j  |⌘j(x)  ˆ⌘j(x)|  to the surrogate loss ` used to train node classiﬁers
fzi. It is assumed here that ` is a strongly proper composite loss (e.g  logistic  exponential  or squared
loss) characterized by a constant  (e.g.  = 4 for logistic loss).2
Theorem 1. For any distribution P and internal node classiﬁers fzi  the following holds:

|⌘j(x)  ˆ⌘j(x)|

P(zi1 | x)r 2

lXi=0

preg`(fzi | zi1  x)  

where reg`(fzi | zi1  x) is a binary classiﬁcation regret for a strongly proper composite loss ` and
 is a constant speciﬁc for loss `.
Due to ﬁltering of the distribution imposed by the PLT  the regret reg`(fzi | zi1  x) of a classiﬁer
fzi exists only for x such that P(zi1 | x) > 0  therefore we condition the regret not only on x  but
also on zi1. The above result shows that the absolute error of estimating the marginal probability of
label j can be upper bounded by the regret of the node classiﬁers on the corresponding path from
the root to a leaf. The proof of Theorem 1 is given in Appendix A. Moreover  for zero-regret (i.e. 
optimal) node classiﬁers we obtain an optimal multi-label classiﬁer in terms of estimation of marginal
probabilities ⌘j(x). This result can be further extended for precision@k.
Theorem 2. For any distribution P and classiﬁer h delivering estimates ˆ⌘j(x) of the marginal
probabilities of labels  the following holds:

regp@k(h| x) =

⌘i(x) 

⌘j(x)  2 max

l

|⌘l(x)  ˆ⌘l(x)|

2For more detailed introduction to strongly proper composite losses  we refer the reader to (Agarwal  2014).

1

k Xi2Yk

1

k Xj2 ˆYk

6

The proof is based on adding and subtracting the following terms 1
ˆ⌘j(x)
to the regret (a detailed proof is given in Appendix A). By getting together both theorems we get an
upper bound of the precision@k regret expressed in terms of the regret of the node classiﬁers. Again 
for the zero-regret node classiﬁers  we get optimal solution in terms of precision@k.

ˆ⌘i(x) and 1

kPi2Yk

kPj2 ˆYk

7

Implementation details of PLTs

Given the tree structure  the node classiﬁers of PLTs can be trained as logistic regression either in
online (Jasinska et al.  2016) or batch mode (Prabhu et al.  2018). Both training modes have their pros
and cons  but the online implementation gives a possibility of learning more complex representation
of input instances. The above cited implementations are both based on sparse representation  given
either in a form of a bag-of-words or its TF-IDF variant. We opt here for training a PLT in the online
mode along with the dense representation. We build our implementation upon FASTTEXT and refer
to it as XT which stands for EXTREMETEXT.3 In this way  we succeeded to obtain a very powerful
and compressed model. The small dense models are important for fast online prediction as they
do not need too much resources. The sparse models  in turn  can be slow and expensive in terms
of memory usage as they need to decompress the node models to work fast. Remark also that  in
general  PLTs can be used as an output layer of any neural network architecture (also that one used
in XML-CNN (Yen et al.  2017)) to speed up training and prediction time.
In contrast to the original implementation of FASTTEXT  we use L2 regularization for all parameters
of the model. To obtain representation of input instances we do not compute simple averages of
the feature vectors  but use weights proportional to the TF-IDF scores of features. The competitive
results can be obtained with feature and instance vectors of size 500. If a node classiﬁcation task
contains only positive instances  we use a constant classiﬁer predicting 1 without any training. The
training of PLT in either mode  online or batch  can be easily parallelized as each node classiﬁer can
be trained in isolation from the other classiﬁers. In our current implementation  however  we follow
the parallelization on the level of training and test instances as in original FASTTEXT.
Our implementation  because of the additional use of the L2 regularization  has more parameters
than original FASTTEXT. We have found  however  that our model is remarkably robust for the
hyperparameter selection  since it achieves close to optimal performance for a large set of hyperpa-
rameters that is in the vicinity of the optimal one. Moreover  the optimal hyperparameters are close
to each other across all datasets. We report more information about the hyperparameter selection in
Appendix E.4.
The tree structure of a PLT is a crucial modeling decision. The vanishing regret for probability
estimates and precision@k holds regardless of the tree structure (see Theorem 1 and 2)  however  this
theory requires the regret of the node classiﬁers also to vanish. In practice  we can only estimate the
conditional probabilities in the nodes  therefore the tree structure does indeed matter as it affects the
difﬁculty of the node learning problems. The original PLT paper (Jasinska et al.  2016) uses simple
complete trees with labels assigned to leaves according to their frequencies. Another option  routinely
used in HSM (Joulin et al.  2016)  is the Huffman tree built over the label frequencies. Such tree takes
into account the computational complexity by putting the most frequent labels close to the root. This
approach has been further extended to optimize GPU operations in (Grave et al.  2017). Unfortunately 
it ignores the statistical properties of the tree structure. Furthermore  for multi-label case the Huffman
tree is no longer optimal even in terms of computational cost as we show it in Appendix C. There
exist  however  other methods that focus on building a tree with high overall accuracy (Tagami  2017;
Prabhu et al.  2018). In our work  we follow the later approach  which performs a simple top-down
hierarchical clustering. Each label in this approach is represented by a proﬁle vector being an average
of the training vectors tagged by this label. Then the proﬁle vectors are clustered using balanced
k-means which divides the labels into two or more clusters with approximately the same size. This
procedure is then repeated recursively until the clusters are smaller than a given value (for example 
100). The nodes of the resulting tree are then of different arities. The internal nodes up to the leaves’
parent nodes have k children  but the leaves’ parent nodes are usually of higher arity. Thanks to this
clustering  similar labels are close to each other in the tree. Moreover  the tree is balanced  so its
depth is logarithmic in terms of the number of labels.

3Implementation of XT is available at https://github.com/mwydmuch/extremeText.

7

8 Empirical results

We carried out three sets of experiments. In the ﬁrst  we compare exhaustively the performance of
PLTs and HSM on synthetic and benchmark data. Due to lack of space  the results are deferred to
Appendix E.1 and E.2. The results on synthetic data conﬁrm our theoretical ﬁndings: the models
are the same in the case of multi-class data  the performance of HSM and PLTs is on par using
multi-label data with independent labels  and PLTs signiﬁcantly outperform HSM on multi-label
data with conditionally dependent labels. The results on the benchmark data clearly indicate the
better performance of PLTs over HSM.
In the second experiment  we compare XT  the variant of PLTs discussed in the previous section  to
the state-of-the-art algorithms on ﬁve benchmark datasets taken from XMLC repository 4 and their text
equivalents  by courtesy of Liu et al. (2017). We compare the models in terms of precision@{1  3  5} 
model size  training and test time. The competitors for our XT are original FASTTEXT  its vari-
ant LEARNED TREE  a PLT-like batch learning algorithm PARABEL (we use the variant that uses
a single tree instead of an ensemble)  a XMLC-designed convolutional deep network XML-CNN 
a decision tree ensemble FASTXML  and two 1-vs-All approaches tailored to XMLC problems 
PPD-SPARSE and DISMEC. The hyperparameters of the models have been tuned using grid search.
The range of the hyperparameters is reported in E.4.
The results presented in Table 1 demonstrate that XT outperforms the HSM approaches with the
pick-one-label heuristic  namely FASTTEXT and LEARNED TREE  with a large margin. This proves
the superiority of PLTs as the proper generalization of HSM to multi-label setting. In all the above
methods we use vectors of length 500 and we tune the other hyperparameters appropriately for a fair
comparison.
Moreover  XT scales well to extreme datasets achieving performance close to the state-of-the-art 
being at the same time 10000x and 100x faster compared to DISMEC and PPDSPARSE during
prediction. XT always responds below 2ms  what makes it a competitive alternative for an online
setting. XT is also close to PARABEL in terms of performance. However  the reported times and
model sizes of PARABEL are given for the batch prediction. The prediction times seem to be faster 
but PARABEL needs to decompress the model during prediction  what makes it less suitable for online
prediction. It is only efﬁcient when the batches are sufﬁciently large. Finally  we would like to
underline that XT outperforms XML-CNN  the more complex neural network  in terms of predictive
performance with computational costs that are an order of magnitude smaller. Moreover  XML-CNN
requires pretrained embedding vectors  whereas XT can be used with random initialization.
In the third experiment we perform an ablation analysis in which we compare different components
of the XT algorithm. We analyze the inﬂuence of the Huffman tree vs. top-down clustering  the
simple averaging of features vectors vs.
the TF-IDF-based weighting  and no regularization vs.
L2 regularization. Figure 1 clearly shows that the components need to be combined together to
obtain the best results. The best combination uses top-down clustering  TF-IDF-based weighting 
and L2 regularization  while top-down clustering alone gets worse results than Huffman trees with
TF-IDF-based weighting and L2 regularization. In Appendix E.3 we give more detailed results of the
ablation analysis performed on a larger spectrum of benchmark datasets.

9 Conclusions

In this paper we have proven that probabilistic label trees (PLTs) are no-regret generalization of HSM
to the multi-label setting. Our main theoretical contribution is the precision@k regret bound for PLTs.
Moreover  we have shown that the pick-one-label heuristic commonly-used with HSM in multi-label
problems leads to inconsistent results in terms of marginal probability estimation and precision@k.
Our implementation of PLTs referred to as XT  built upon FASTTEXT  gets state-of-the-art results 
being signiﬁcantly better than the original FASTTEXT  LEARNED TREE  and XML-CNN. The XT
results are also close to the best known ones that are obtained by expensive 1-vs-All approaches  such
as PPDSPARSE and DISMEC  and outperforms the other tree-based methods on many benchmarks.
Our online variant has the advantage of producing very often much smaller models that can be
efﬁciently used in fast online prediction.

4Additional statistics of these datasets are also included in Appendix F. Address of the XMLC repository:

http://manikvarma.org/downloads/XC/XMLRepository.html

8

Table 1: Precision@k scores with k = {1  3  5} and statistics of FASTXML  PPDSPARSE  DISMEC 
PARABEL (with 1 tree)  FASTTEXT (FT)  LEARNED TREE (LT)  EXTREMETEXT (XT) and XML-CNN
methods. Notation: N – number of samples  T – CPU time  m – number of labels  d – number of features  ⇤ –
result of ofﬂine prediction  ? – calculated on GPU  † – not reported by authors  ‡ – cannot be calculated due to
lack of a text version of a dataset.

Metrics
P@1
P@3
P@5
Ttrain
Ttest/Ntest
model size
P@1
P@3
P@5
Ttrain
Ttest/Ntest
model size
P@1
P@3
P@5
Ttrain
Ttest/Ntest
model size
P@1
P@3
P@5
Ttrain
Ttest/Ntest
model size
P@1
P@3
P@5
Ttrain
Ttest/Ntest
model size

FASTXML

PPDSPARSE DISMEC

82.03
67.47
57.76
16m
3.00ms
354M
42.81
38.76
36.34
458m
4.86ms
15.4G
49.35
32.69
24.03
724m
2.17ms
9.3G
54.10
29.45
21.21
3214m
8.03ms
63G
34.24
29.30
26.12
422m
3.39ms
10G

73.80
60.90
50.40
†
†
†
45.05
38.34
34.90
4781m
275ms
9.4G
64.08
41.26
30.12
236m
37.76ms
5.2G
70.16
50.57
39.66
1771m
113.70ms
3.4G
45.32
40.37
36.92
102m
66.09ms
6.0G

85.20
74.60
65.90
†
†
†
44.71
38.08
34.7
1080h
5m
18.0G
64.94
42.71
31.5
750h
43m
3.8G
70.20
50.60
39.70
7495h
155m
14.7G
45.37
40.40
36.96
373h
23m
3.8G

FT
80.78
50.46
36.79
10m
1.88ms
513M
42.22
37.90
35.05
271m
1.97ms
9.0G
41.13
24.09
17.44
207
1.25ms
6.5G
32.73
19.02
14.46
496m
2.05ms
11G
25.47
21.47
18.61
162m
7.84ms
3.2G

LT
80.85
50.59
37.68
12m
10.09ms
513M
42.71
36.27
33.43
563m
1.98ms
9.0G
50.15
31.95
23.59
212m
4.76ms
6.5G
37.18
21.62
16.01
531m
6.43ms
11G
27.67
20.96
17.72
182m
5.13ms
3.2G

XT
85.23
73.18
63.39
18m
0.83ms
259M
47.85
42.08
39.13
502m
1.41ms
1.9G
58.73
39.24
29.26
550m
0.81ms
3.3G
64.48
45.84
35.46
1253m
1.07ms
5.5G
39.90
35.36
32.04
241m
1.72ms
1.5G

PARABEL
83.77
71.96
62.44
5m
1.63ms⇤
109M⇤
43.32
38.49
35.83
105m
1.31ms⇤
1.8G⇤
61.53
40.07
29.25
34m
0.92ms⇤
1.1G⇤
66.12
47.02
36.45
168m
4.68ms⇤
2.0G⇤
41.59
37.18
33.85
8m
0.68ms⇤
0.7G⇤

XML-CNN

82.78
66.34
56.23
88m?
1.39ms?
?
‡
‡
‡
‡
‡
‡
‡
‡
‡
‡
‡
‡
59.85
39.28
29.81
7032m?
21.06ms?
3.7G?
35.39
33.74
32.64
3134m?
16.18ms?
1.5G?

WikiLSHTC

Wiki-500K

Amazon-670K

Dataset

Wiki-30K
Ntrain = 14146
Ntest = 6616
d = 101938
m = 30938

Delicious-200K
Ntrain = 196606
Ntest = 100095
d = 782585
m = 205443

WikiLSHTC
Ntrain = 1778351
Ntest = 587084
d = 617899
m = 325056

Wiki-500K
Ntrain = 1813391
Ntest = 783743
d = 2381304
m = 501070

Amazon-670K
Ntrain = 490449
Ntest = 153025
d = 135909
m = 670091

1
@
n
o
i
s
i
c
e
r
p

60

40

20

0

Huffman

tree

Huffman

+L2

Huffman
+TF-IDF

Huffman

+TF-IDF+L2

top-down
clustering

top-down

+L2

top-down
+TF-IDF

top-down

+TF-IDF+L2

Figure 1: The ablation analysis of different variants of XT on WIKILSHTC  WIKI-500K  and
AMAZON-670K.

Acknowledgements

The work of Kalina Jasinska was supported by the Polish National Science Center under grant no.
2017/25/N/ST6/00747. The work of Krzysztof Dembczy´nski was supported by the Polish Ministry
of Science and Higher Education under grant no. 09/91/DSPB/0651. Computational experiments
have been performed in Poznan Supercomputing and Networking Center.

9

References
Agarwal  Shivani. Surrogate regret bounds for bipartite ranking via strongly proper losses. Journal

of Machine Learning Research  15(1):1653–1674  2014.

Agrawal  Rahul  Gupta  Archit  Prabhu  Yashoteja  and Varma  Manik. Multi-label learning with
millions of labels: Recommending advertiser bid phrases for web pages. In 22nd International
World Wide Web Conference  WWW ’13  Rio de Janeiro  Brazil  May 13-17  2013  pp. 13–24.
International World Wide Web Conferences Steering Committee / ACM  2013.

Babbar  Rohit and Schölkopf  Bernhard. DiSMEC: Distributed Sparse Machines for Extreme Multi-
label Classiﬁcation. In Proceedings of the Tenth ACM International Conference on Web Search
and Data Mining  WSDM 2017  Cambridge  United Kingdom  February 6-10  2017  pp. 721–729.
ACM  2017.

Bengio  Samy  Weston  Jason  and Grangier  David. Label Embedding Trees for Large Multi-Class
Tasks. In Advances in Neural Information Processing Systems 23: 24th Annual Conference on
Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010 
Vancouver  British Columbia  Canada.  pp. 163–171. Curran Associates  Inc.  2010.

Beygelzimer  Alina  Langford  John  Lifshits  Yury  Sorkin  Gregory B.  and Strehl  Alexander L.
Conditional Probability Tree Estimation Analysis and Algorithms. In UAI 2009  Proceedings of
the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence  Montreal  QC  Canada  June
18-21  2009  pp. 51–58. AUAI Press  2009a.

Beygelzimer  Alina  Langford  John  and Ravikumar  Pradeep. Error-Correcting Tournaments. In
Algorithmic Learning Theory  20th International Conference  ALT 2009  Porto  Portugal  October
3-5  2009. Proceedings  volume 5809 of Lecture Notes in Computer Science  pp. 247–262. Springer 
2009b.

Bhatia  Kush  Jain  Himanshu  Kar  Purushottam  Varma  Manik  and Jain  Prateek. Sparse Local
Embeddings for Extreme Multi-label Classiﬁcation. In Advances in Neural Information Processing
Systems 28: Annual Conference on Neural Information Processing Systems 2015  December 7-12 
2015  Montreal  Quebec  Canada  pp. 730–738  2015.

Choromanska  Anna and Langford  John. Logarithmic Time Online Multiclass prediction.

In
Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information
Processing Systems 2015  December 7-12  2015  Montreal  Quebec  Canada  pp. 55–63  2015.

Dekel  Ofer and Shamir  Ohad. Multiclass-Multilabel Classiﬁcation with More Classes than Examples.
In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics 
AISTATS 2010  Chia Laguna Resort  Sardinia  Italy  May 13-15  2010  volume 9 of JMLR
Proceedings  pp. 137–144. JMLR.org  2010.

Dembczynski  Krzysztof  Cheng  Weiwei  and Hüllermeier  Eyke. Bayes Optimal Multilabel Classiﬁ-
cation via Probabilistic Classiﬁer Chains. In Proceedings of the 27th International Conference on
Machine Learning (ICML-10)  June 21-24  2010  Haifa  Israel  pp. 279–286. Omnipress  2010.

Dembczynski  Krzysztof  Waegeman  Willem  and Hüllermeier  Eyke. An Analysis of Chaining in
Multi-Label Classiﬁcation. In ECAI 2012 - 20th European Conference on Artiﬁcial Intelligence.
Including Prestigious Applications of Artiﬁcial Intelligence (PAIS-2012) System Demonstrations
Track  Montpellier  France  August 27-31   2012  volume 242 of Frontiers in Artiﬁcial Intelligence
and Applications  pp. 294–299. IOS Press  2012.

Dembczynski  Krzysztof  Kotlowski  Wojciech  Waegeman  Willem  Busa-Fekete  Róbert  and
Hüllermeier  Eyke. Consistency of Probabilistic Classiﬁer Trees.
In Machine Learning and
Knowledge Discovery in Databases - European Conference  ECML PKDD 2016  Riva del Garda 
Italy  September 19-23  2016  Proceedings  Part II  volume 9852 of Lecture Notes in Computer
Science  pp. 511–526. Springer  2016.

Dembczynski  Krzysztof  Kotlowski  Wojciech  Koyejo  Oluwasanmi  and Natarajan  Nagarajan.
Consistency Analysis for Binary Classiﬁcation Revisited. In Proceedings of the 34th Interna-
tional Conference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017 
volume 70 of Proceedings of Machine Learning Research  pp. 961–969. PMLR  2017.

10

Deng  Jia  Satheesh  Sanjeev  Berg  Alexander C.  and Li  Fei-Fei. Fast and Balanced: Efﬁcient
Label Tree Learning for Large Scale Object Recognition. In Advances in Neural Information
Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011.
Proceedings of a meeting held 12-14 December 2011  Granada  Spain.  pp. 567–575  2011.

Fan  Rong-En  Chang  Kai-Wei  Hsieh  Cho-Jui  Wang  Xiang-Rui  and Lin  Chih-Jen. LIBLINEAR:
A Library for Large Linear Classiﬁcation. Journal of Machine Learning Research  9:1871–1874 
2008.

Fox  John. Applied regression analysis  linear models  and related methods. Sage  1997.

Grave  Edouard  Joulin  Armand  Cissé  Moustapha  Grangier  David  and Jégou  Hervé. Efﬁcient
softmax approximation for GPUs. In Proceedings of the 34th International Conference on Machine
Learning  ICML 2017  Sydney  NSW  Australia  6-11 August 2017  volume 70 of Proceedings of
Machine Learning Research  pp. 1302–1310  International Convention Centre  Sydney  Australia 
2017. PMLR.

Jasinska  Kalina  Dembczynski  Krzysztof  Busa-Fekete  Róbert  Pfannschmidt  Karlson  Klerx  Timo 
and Hüllermeier  Eyke. Extreme F-measure Maximization using Sparse Probability Estimates. In
Proceedings of the 33nd International Conference on Machine Learning  ICML 2016  New York
City  NY  USA  June 19-24  2016  volume 48 of JMLR Workshop and Conference Proceedings  pp.
1435–1444. JMLR.org  2016.

Jernite  Yacine  Choromanska  Anna  and Sontag  David. Simultaneous Learning of Trees and
Representations for Extreme Classiﬁcation and Density Estimation. In Proceedings of the 34th
International Conference on Machine Learning  ICML 2017  Sydney  NSW  Australia  6-11 August
2017  volume 70 of Proceedings of Machine Learning Research  pp. 1665–1674. PMLR  2017.

Joulin  Armand  Grave  Edouard  Bojanowski  Piotr  and Mikolov  Tomas. Bag of Tricks for Efﬁcient

Text Classiﬁcation. CoRR  abs/1607.01759  2016.

Kumar  Abhishek  Vembu  Shankar  Menon  Aditya Krishna  and Elkan  Charles. Beam search

algorithms for multilabel learning. Machine Learning  92:65–89  2013.

Kurzynski  Marek. On the multistage Bayes classiﬁer. Pattern Recognition  21(4):355–365  1988.
Langford  John  Strehl  Alex  and Li  Lihong. Vowpal Wabbit  2007. http://hunch.net/~vw/.
Li  Chun-Liang and Lin  Hsuan-Tien. Condensed Filter Tree for Cost-Sensitive Multi-Label Classiﬁ-
cation. In Proceedings of the 31th International Conference on Machine Learning  ICML 2014 
Beijing  China  21-26 June 2014  volume 32 of JMLR Workshop and Conference Proceedings  pp.
423–431. JMLR.org  2014.

Liu  Jingzhou  Chang  Wei-Cheng  Wu  Yuexin  and Yang  Yiming. Deep Learning for Extreme
Multi-label Text Classiﬁcation. In Proceedings of the 40th International ACM SIGIR Conference
on Research and Development in Information Retrieval  Shinjuku  Tokyo  Japan  August 7-11 
2017  pp. 115–124. ACM  2017.

Mikolov  Tomas  Sutskever  Ilya  Chen  Kai  Corrado  Gregory S.  and Dean  Jeffrey. Distributed
Representations of Words and Phrases and their Compositionality. In Advances in Neural Informa-
tion Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems
2013. Proceedings of a meeting held December 5-8  2013  Lake Tahoe  Nevada  United States.  pp.
3111–3119  2013.

Mineiro  Paul and Karampatziakis  Nikos. Fast Label Embeddings via Randomized Linear Algebra.
In Machine Learning and Knowledge Discovery in Databases - European Conference  ECML
PKDD 2015  Porto  Portugal  September 7-11  2015  Proceedings  Part I  volume 9284 of Lecture
Notes in Computer Science  pp. 37–51. Springer  2015.

Morin  Frederic and Bengio  Yoshua. Hierarchical Probabilistic Neural Network Language Model. In
Proceedings of the Tenth International Workshop on Artiﬁcial Intelligence and Statistics  AISTATS
2005  Bridgetown  Barbados  January 6-8  2005. Society for Artiﬁcial Intelligence and Statistics 
2005.

11

Narasimhan  Harikrishna  Vaish  Rohit  and Agarwal  Shivani. On the Statistical Consistency of Plug-
in Classiﬁers for Non-decomposable Performance Measures. In Advances in Neural Information
Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014 
December 8-13 2014  Montreal  Quebec  Canada  pp. 1493–1501  2014.

Niculescu-Mizil  Alexandru and Abbasnejad  Ehsan. Label Filters for Large Scale Multilabel
Classiﬁcation. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and
Statistics  AISTATS 2017  20-22 April 2017  Fort Lauderdale  FL  USA  volume 54 of Proceedings
of Machine Learning Research  pp. 1448–1457. PMLR  2017.

Prabhu  Yashoteja and Varma  Manik. FastXML: a fast  accurate and stable tree-classiﬁer for extreme
multi-label learning. In The 20th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining  KDD ’14  New York  NY  USA - August 24 - 27  2014  pp. 263–272. ACM  2014.
Prabhu  Yashoteja  Kag  Anil  Harsola  Shrutendra  Agrawal  Rahul  and Varma  Manik. Parabel:
Partitioned Label Trees for Extreme Classiﬁcation with Application to Dynamic Search Advertising.
In Proceedings of the 2018 World Wide Web Conference on World Wide Web  WWW 2018  Lyon 
France  April 23-27  2018  pp. 993–1002. ACM  2018.

Shrivastava  Anshumali and Li  Ping. Improved Asymmetric Locality Sensitive Hashing (ALSH)
for Maximum Inner Product Search (MIPS). In Proceedings of the Thirty-First Conference on
Uncertainty in Artiﬁcial Intelligence  UAI 2015  July 12-16  2015  Amsterdam  The Netherlands 
pp. 812–821. AUAI Press  2015.

Tagami  Yukihiro. AnnexML: Approximate Nearest Neighbor Search for Extreme Multi-label
Classiﬁcation. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  Halifax  NS  Canada  August 13 - 17  2017  pp. 455–464. ACM  2017.
Vijayanarasimhan  Sudheendra  Shlens  Jonathon  Monga  Rajat  and Yagnik  Jay. Deep Networks

With Large Output Spaces. CoRR  abs/1412.7479  2014.

Ye  Nan  Chai  Kian Ming Adam  Lee  Wee Sun  and Chieu  Hai Leong. Optimizing F-measure:
A Tale of Two Approaches. In Proceedings of the 29th International Conference on Machine
Learning  ICML 2012  Edinburgh  Scotland  UK  June 26 - July 1  2012. icml.cc / Omnipress 
2012.

Yen  Ian En-Hsu  Huang  Xiangru  Dai  Wei  Ravikumar  Pradeep  Dhillon  Inderjit S.  and Xing 
Eric P. PPDsparse: A Parallel Primal-Dual Sparse Method for Extreme Classiﬁcation. In Pro-
ceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining  Halifax  NS  Canada  August 13 - 17  2017  pp. 545–553. ACM  2017.

Yu  Hsiang-Fu  Jain  Prateek  Kar  Purushottam  and Dhillon  Inderjit S. Large-scale Multi-label
Learning with Missing Labels. In Proceedings of the 31th International Conference on Machine
Learning  ICML 2014  Beijing  China  21-26 June 2014  volume 32 of JMLR Workshop and
Conference Proceedings  pp. 593–601. JMLR.org  2014.

12

,Zeyuan Allen-Zhu
Elad Hazan
Wei Hu
Yuanzhi Li
Marek Wydmuch
Kalina Jasinska
Mikhail Kuznetsov
Róbert Busa-Fekete
Krzysztof Dembczynski