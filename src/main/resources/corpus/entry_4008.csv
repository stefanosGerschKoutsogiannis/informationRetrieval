2019,On the equivalence between graph isomorphism testing and function approximation with GNNs,Graph neural networks (GNNs) have achieved lots of success on graph-structured data. In light of this  there has been increasing interest in studying their representation power. One line of work focuses on the universal approximation of permutation-invariant functions by certain classes of GNNs  and another demonstrates the limitation of GNNs via graph isomorphism tests.
           
Our work connects these two perspectives and proves their equivalence. We further develop a framework of the representation power of GNNs with the language of sigma-algebra  which incorporates both viewpoints. Using this framework  we compare the expressive power of different classes of GNNs as well as other methods on graphs. In particular  we prove that order-2 Graph G-invariant networks fail to distinguish non-isomorphic regular graphs with the same degree. We then extend them to a new architecture  Ring-GNN  which succeeds in distinguishing these graphs as well as for tasks on real-world datasets.,On the equivalence between graph isomorphism
testing and function approximation with GNNs

Courant Institute of Mathematical Sciences

Courant Institute of Mathematical Sciences

Courant Institute of Mathematical Sciences

Courant Institute of Mathematical Sciences

Zhengdao Chen

New York University
zc1216@nyu.edu

Lei Chen

New York University
lc3909@nyu.edu

Soledad Villar

Center for Data Science
New York University

soledad.villar@nyu.edu

Joan Bruna

Center for Data Science
New York University
bruna@cims.nyu.edu

Abstract

Graph neural networks (GNNs) have achieved lots of success on graph-structured
data. In light of this  there has been increasing interest in studying their repre-
sentation power. One line of work focuses on the universal approximation of
permutation-invariant functions by certain classes of GNNs  and another demon-
strates the limitation of GNNs via graph isomorphism tests.
Our work connects these two perspectives and proves their equivalence. We further
develop a framework of the representation power of GNNs with the language of
sigma-algebra  which incorporates both viewpoints. Using this framework  we
compare the expressive power of different classes of GNNs as well as other methods
on graphs. In particular  we prove that order-2 Graph G-invariant networks fail to
distinguish non-isomorphic regular graphs with the same degree. We then extend
them to a new architecture  Ring-GNN  which succeeds in distinguishing these
graphs as well as for tasks on real-world datasets.

1

Introduction

Graph structured data naturally occur in many areas of knowledge  including computational biology 
chemistry and social sciences. Graph neural networks  in all their forms  yield useful representations
of graph data partly because they take into consideration the intrinsic symmetries of graphs  such as
invariance and equivariance with respect to a relabeling of the nodes [27  7  15  8  10  28  3  36].
All these different architectures are proposed with different purposes (see [31] for a survey and
references therein)  and a priori it is not obvious how to compare their power. The recent work [32]
proposes to study the representation power of GNNs via their performance on graph isomorphism
tests. They developed the Graph Isomorphism Networks (GINs) that are as powerful as the one-
dimensional Weisfeiler-Lehman (1-WL or just WL) test for graph isomorphism [30]  and showed
that no other neighborhood-aggregating (or message passing) GNN can be more powerful than the
1-WL test. Variants of message passing GNNs include [27  9].
On the other hand  for feed-forward neural networks  many results have been obtained regarding
their ability to approximate continuous functions  commonly known as the universal approximation
theorems  such as the seminal works of [6  12]. Following this line of work  it is natural to study

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

the expressivity of graph neural networks in terms of function approximation. Since we could argue
that many if not most functions on a graph that we are interested in are invariant or equivariant to
permutations of the nodes in the graph  GNNs are usually designed to be invariant or equivariant 
and therefore the natural question is whether certain classes GNNs can approximate any continuous
and invariant or equivariant functions. Recent work [19] showed the universal approximation of
G-invariant networks  constructed based on the linear invariant and equivariant layers studied in
[18]  if the order of the tensor involved in the networks can grow as the graph gets larger. Such a
dependence on the graph size was been theoretically overcame by the very recent work [13]  though
there is no known upper bound on the order of the tensors involved. With potentially very-high-order
tensors  these models that are guaranteed of univeral approximation are not quite feasible in practice.
The foundational part of this work aims at building the bridge between graph isomorphism testing
and invariant function approximation  the two main perspectives for studying the expressive power
of graph neural networks. We demonstrate an equivalence between the the ability of a class of
GNNs to distinguish between any pairs of non-isomorphic graph and its power of approximating
any (continuous) invariant functions  for both the case with ﬁnite feature space and the case with
continuous feature space. Furthermore  we argue that the concept of sigma-algebras on the space
of graphs is a natural description of the power of graph neural networks  allowing us to build
a taxonomy of GNNs based on how their respective sigmas-algebras interact. Building on this
theoretical framework  we identify an opportunity to increase the expressive power of order-2 G-
invariant networks with computational tractability  by considering a ring of invariant matrices under
addition and multiplication. We show that the resulting model  which we refer to as Ring-GNN 
is able to distinguish between non-isomorphic regular graphs where order-2 G-invariant networks
provably fail. We illustrate these gains numerically in synthetic and real graph classiﬁcation tasks.
Summary of main contributions:

• We show the equivalence between graph isomorphism testing and approximation of
permutation-invariant functions for analyzing the expressive power of graph neural networks.
• We introduce a language of sigma algebra for studying the representation power of graph
neural networks  which uniﬁes both graph isomorphism testing and function approximation 
and use this framework to compare the power of some GNNs and other methods.

• We propose Ring-GNN  a tractable extension of order-2 Graph G-invariant Networks that
uses the ring of matrix addition and multiplication. We show this extension is necessary and
sufﬁcient to distinguish Circular Skip Links graphs.

2 Related work

Graph Neural Networks and graph isomorphism. Graph isomorphism is a fundamental problem
in theoretical computer science. It amounts to deciding  given two graphs A  B  whether there exists a
permutation ⇡ such that ⇡A = B⇡. There exists no known polynomial-time algorithm to solve it  but
recently Babai made a breakthrough by showing that it can be solved in quasi-polynomial-time [1].
Recently [32] introduced graph isomorphism tests as a characterization of the power of graph neural
networks. They show that if a GNN follows a neighborhood aggregation scheme  then it cannot
distinguish pairs of non-isomorphic graphs that the 1-WL test fails to distinguish. Therefore this
class of GNNs is at most as powerful as the 1-WL test. They further propose the Graph Isomorphism
Networks (GINs) based on approximating injective set functions by multi-layer perceptrons (MLPs) 
which can be as powerful as the 1-WL test. Based on k-WL tests [4]  [20] proposes k-GNN  which
can take higher-order interactions among nodes into account. Concurrently to this work  [17] proves
that order-k invariant graph networks are at least as powerful as the k-WL tests  and similarly to us  it
and augments order-2 networks with matrix multiplication. They show they achieve at least the power
of 3-WL test. [21] proposes relational pooling (RP)  an approach that combines permutation-sensitive
functions under all permutations to obtain a permutation-invariant function. If RP is combined with
permutation-sensitive functions that are sufﬁciently expressive  then it can be shown to be a universal
approximator. A combination of RP and GINs is able to distinguish certain non-isomorphic regular
graphs which GIN alone would fail on. A drawback of RP is that its full version is intractable
computationally  and therefore it needs to be approximated by averaging over randomly sampled
permutations  in which case the resulting functions is not guaranteed to be permutation-invariant.

2

2

Universal approximation of functions with symmetry. Many works have discussed the function
approximation capabilities of neural networks that satisfy certain symmetries. [2] studies the probab-
lisitic and functional symmetry in neural networks  and we discuss its relationship to our work in more
detail in Appendix D. [25] shows that equivariance of a neural network corresponds to symmetries in
its parameter-sharing scheme. [35] proposes a neural network architecture with polynomial layers
that is able to achieve universal approximation of invariant or equivariant functions. [18] studies the
spaces of all invariant and equivariant linear functions  and obtained bases for such spaces. Building
upon this work  [19] proposes the G-invariant network for a symmetry group G  which achieves
universal approximation of G-invariant functions if the maximal tensor order involved in the network
to grow as n(n1)
  but such high-order tensors are prohibitive in practice. Upper bounds on the
approximation power of the G-invariant networks when the tensor order is limited remains open
except for when G = An [19]. The very recent work [13] extends the result to the equivariant case 
although it suffers from the same problem of possibly requiring high-order tensors. Speciﬁcally for
learning in graphs  [26] proposes the compositional networks  which achieve equivariance and are
inspired by the WL test. In the context of machine perception of visual scenes  [11] proposes an
architecture that can potentially express all equivariant functions.
To the best our knowledge  this is the ﬁrst work that shows an explicit connection between the two
aforementioned perspectives of studying the representation power of graph neural networks - graph
isomorphism testing and universal approximation. Our main theoretical contribution lies in showing
an equivalence between them  for both ﬁnite and continuous feature space cases  with a natural
generalization of the notion of graph isomorphism testing to the latter case. Then we focus on the
Graph G-invariant network based on [18  19]  and showed that when the maximum tensor order
is restricted to be 2  then it cannot distinguish between non-isomorphic regular graphs with equal
degrees. As a corollary  such networks are not universal. Note that our result shows an upper bound
on order 2 G-invariant networks  whereas concurrently to us  [17] provides a lower bound by relating
to k-WL tests. Concurrently to [17]  we propose a modiﬁed version of order-2 graph networks to
capture higher-order interactions among nodes without computing tensors of higher-order.

3 Graph isomorphism testing and universal approximation

In this section we show that there exists a very close connection between the universal approximation
of permutation-invariant functions by a class of functions  and its ability to perform graph isomor-
phism tests. We consider graphs with nodes and edges labeled by elements of a compact set X⇢ R.
We represent graphs with n nodes by an n by n matrix G 2X n⇥n  where a diagonal term Gii
represents the label of the ith node  and a non-diagonal Gij represents the label of the edge from the
ith node to the jth node. An undirected graph will then be represented by a symmetric G.
Thus  we focus on analyzing a collection C of functions from X n⇥n to R. We are especially
interested in collections of permutation-invariant functions  deﬁned so that f (⇡|G⇡) = f (G)  for
all G 2X n⇥n  and all ⇡ 2 Sn  where Sn is the permutation group of n elements. For classes of
functions  we deﬁne the property of being able to discriminate non-isomorphic graphs  which we call
GIso-discriminating  which as we will see generalizes naturally to the continuous case.
Deﬁnition 1. Let C be a collection of permutation-invariant functions from X n⇥n to R. We say C is
GIso-discriminating if for all non-isomorphic G1  G2 2X n⇥n (denoted G1 6' G2)  there exists a
function h 2C such that h(G1) 6= h(G2). This deﬁnition is illustrated by ﬁgure 2 in the appendix.
Deﬁnition 2. Let C be a collection of permutation-invariant functions from X n⇥n to R. We say C is
universally approximating if for all permutation-invariant function f from X n⇥n to R  and for all
✏> 0  there exists hf ✏ 2C such that kf  hf ✏k1 := supG2X n⇥n |f (G)  h(G)| <✏
3.1 Finite feature space

As a warm-up we ﬁrst consider the space of graphs with a ﬁnite set of possible features for nodes and
edges  X = {1  . . .   M}.
Theorem 1. Universally approximating classes of functions are also GIso-discriminating.
Proof. Given G1  G2 2X n⇥n  we consider the permutation-invariant function 1
such that 1

'G1 : X n⇥n ! R
'G1(G) = 1 if G is isomorphic to G1 and 0 otherwise. Therefore  it can be approximated

3

L0

with ✏ = 0.1 by a function h 2C . Then h is a function that distinguishes G1 from G2  as in
Deﬁnition 1. Hence C is GIso-discriminating.
To obtain a result on the reverse direction  we ﬁrst introduce the concept of an augmented collection
of functions  which is especially natural when C is a collection of neural networks.
Deﬁnition 3. Given C  a collection of functions from X n⇥n to R  we consider an augmented
collection of functions also from X n⇥n to R consisting of functions that map an input graph G to
NN ([h1(G)  ...  hd(G)]) for some ﬁnite d  where NN is a feed-forward neural network / multi-layer
perceptron  and h1  ...  hd 2C . When NN is restricted to have L layers  we denoted this augmented
collection by C+L. In this work  we consider ReLU as the nonlinear activation function in the neural
networks.
Remark 1. If CL0 is the collection of feed-forward neural networks with L0 layers  then C+L
represents the collection of feed-forward neural networks with L0 + L layers.
Remark 2. If C is a collection of permutation-invariant functions  so is C+L.
Theorem 2. If C is GIso-discriminating  then C+2 is universal approximating.
The proof is simple and it is a consequence of the following lemmas that we prove in Appendix A.
Lemma 1. If C is GIso-discriminating  then for all G 2X n⇥n  there exists a function ˜hG 2C +1
such that for all G0  ˜hG(G0) = 0 if and only if G ' G0.
Lemma 2. Let C be a class of permutation-invariant functions from X n⇥n to R satisfying the
consequences of Lemma 1  then C+1 is universally approximating.
3.2 Extension to the case of continuous (Euclidean) feature space
Graph isomorphism is an inherently discrete problem  whereas universal approximation is usually
more interesting when the input space is continuous. With our deﬁnition 1 of GIso-discriminating 
we can achieve a natural generalization of the above results to the scenarios of continuous input space.
All proofs for this section can be found in Appendix A.
Let X be a compact subset of R  and we consider graphs with n nodes represented by G 2 K =
X n⇥n; that is  the node features are {Gii}i=1 ... n and the edge features are {Gij}i j=1 ... n;i6=j.
Theorem 3. If C is universally approximating  then it is also GIso-discriminating
The essence of the proof is similar to that of Theorem 1. The other direction - showing that pairwise
discrimination can lead to universal approximation - is less straightforward. As an intermediate step
between  we make the following deﬁnition:
Deﬁnition 4. Let C be a class of functions K ! R. We say it is able to locate every isomorphism
class if for all G 2 K and for all ✏> 0 there exists hG 2C such that:

d is the Euclidean distance deﬁned on Rn⇥n

• for all G0 2 K  hG(G0)  0;
• for all G0 2 K  if G0 ' G  then hG(G0) = 0; and
• there exists G > 0 such that if hG < G  then 9⇡ 2 Sn such that d(⇡(G0)  G) <✏   where
Lemma 3. If C  a collection of continuous permutation-invariant functions from K to R  is GIso-
discriminating  then C+1 is able to locate every isomorphism class.
Heuristically  we can think of the hG in the deﬁnition above as a “loss function” that penalizes the
deviation of G0 from the equivalence class of G. In particular  the second condition says that if the
loss value is small enough  then we know that G0 has to be close to the equivalence class of G.
Lemma 4. Let C be a class of permutation-invariant functions K ! R. If C is able to locate every
isomorphism class  then C+2 is universally approximating.
Combining the two lemmas above  we arrive at the following theorem:
Theorem 4. If C  a collection of continuous permutation-invariant functions from K to R  is GIso-
discriminating  then C+3 is universaly approximating.

4

4 A framework of representation power based on sigma-algebra

4.1

Introducing sigma-algebra to this context

Let K = X n⇥n be a ﬁnite input space. Let QK := K/' be the set of isomorphism classes under
the equivalence relation of graph isomorphism. That is  for all ⌧ 2 QK ⌧ = {⇡|G⇡ : ⇡ 2 n} for
some G 2 K.
Intuitively  a maximally expressive collection of permutation-invariant functions  C  will allow us to
know exactly which isomorphism class ⌧ a given graph G belongs to  by looking at the outputs of
certain functions in the collection applied to G. Heuristically  we can consider each function in C as a
“measurement”  which partitions that graph space K according to the function value at each point. If
C is powerful enough  then as a collection it will partition K to be as ﬁne as QK. If not  it is going to
be coarser than QK. These intuitions motivate us to introduce the language of sigma-algebra.
Recall that an algebra on a set K is a collection of subsets of K that includes K itself  is closed
under complement  and is closed under ﬁnite union. Because K is ﬁnite  we have that an algebra
on K is also a sigma-algebra on K  where a sigma-algebra further satisﬁes the condition of being
closed under countable unions. Since QK is a set of (non-intersecting) subsets of K  we can obtain
the algebra generated by QK  deﬁned as the smallest algebra that contains QK  and use (QK) to
denote the algebra (and sigma-algebra) generated by QK.
Observation 1. If f : X n⇥n ! R is a permutation-invariant function  then f is measurable with
respect to (QK)  and we denote this by f 2M [(QK)]
Now consider a class of functions C that is permutation-invariant. Then for all f 2C   f 2M [(QK)].
We deﬁne the sigma-algebra generated by f as the set of all the pre-images of Borel sets on R under
f  and denote it by (f ). It is the smallest sigma-algebra on K that makes f measurable. For a class
of functions C  (C) is deﬁned as the smallest sigma-algebra on K that makes all functions in C
measurable. Because we assume K is ﬁnite  it does not matter whether C is a countable collection.

4.2 Reformulating graph isomorphism testing and universal approximation with

sigma-algebra

We restrict our attention to ﬁnite feature space case. Given a graph G 2X n⇥n  we use E(G) to
denote its isomorphism class  {G0 2X n⇥n : G0 ' G}. We prove the following results in Appendix
B.
Theorem 5. If C is a class of permutation-invariant functions on X n⇥n and C is GIso-discriminating 
then (C) = (QK)
Together with Theorem 1  the following is an immediate consequence:
Corollary 1. If C is a class of permutation-invariant functions on X n⇥n and C achieves universal
approximation  then (C) = (QK).
Theorem 6. Let be C a class of permutation-invariant functions on X n⇥n with (C) = (QK).
Then C is GIso-discriminating.
Thus  this sigma-algebra language is a natural notion for characterizing the power of graph neural
networks  because as shown above  generating the ﬁnest sigma-algebra (QK) is equivalent to being
GIso-discriminating  and therefore to universal approximation.
Moreover  when C is not GIso-discriminating or universal  we can evaluate its representation power
by studying (C)  which gives a measure for comparing the power of different GNN families. Given
two classes of functions C1 C2  there is (C1) ✓ (C2) if and only if M[(C1)] ✓M [(C2)] if and
only if C1 is less powerful than C2 in terms of representation power. In Appendix C  we use this
notion to compare the expressive power of different families of GNNs as well as other algorithms
like 1-WL  linear programming and semideﬁnite programming in terms of their ability to distinguish
non-isomorphic graphs. We summarize our ﬁndings in Figure 1.

5

sGNN(I  A)

LP ⌘ 1  W L ⌘ GIN

SDP

MPNN⇤

sGNN(I  D  A {min{At  1}}T

t=1)

order-2 graph G-invariant networks⇤

spectral methods

SoS hierarchy

Ring-GNN

Figure 1: Comparison of classes of functions on graphs in terms of their expressive power under the sigma-
algebra framework proposed in Section 4. Remarks: (a) GIN being deﬁned in [32] as a form of message passing
neural network (MPNN) justiﬁes the inclusion GIN  ! MPNN. (b) [18] shows that message passing neural
networks can be expressed as a modiﬁed form of order-2 graph G-invariant networks (which may not coincide
with the deﬁnition we consider in this paper). Therefore this branch of the hierarchy has yet to be established
rigorously. The rest of the ﬁgure is explained in Appendix C.

5 Ring-GNN: a GNN deﬁned on the ring of equivariant functions

5.1 The limitation of order-2 Graph G-invariant Networks

We ﬁrst investigate the G-invariant networks proposed in [19]. They are constructed by interleaving
compositions of equivariant linear layers between tensors of potentially different orders and point-
wise nonlinear activation functions. We deﬁne its adaptation to graph-structured data in Appendix E 
and refer to it as Graph G-invariant Networks. It is a powerful framework that can achieve universal
approximation if the order of the tensors can grow polynomially in the number of nodes [19]  but
less is known about its approximation power when the tensor order is restricted. One particularly
interesting subclass of G-invariant networks is the ones with maximum tensor order 2 (we will call
them order-2 Graph G-invariant Networks)  because [18] shows that it can approximate any Message
Passing Neural Network (MPNN) [8]  and moreover  it would be both mathematically cumbersome
and computationally expensive to include linear layers involving tensors with order higher than 2.
Our following result shows that the class of order-2 Graph G-invariant Networks is quite restrictive.
The proof is given in Appendix E.
Theorem 7. Order-2 Graph G-invariant Networks cannot distinguish between non-isomorphic
regular graphs with the same degree.

5.2 Ring-GNN as an extension of order-2 Graph G-invariant Networks

Motivated by this limitation  we propose a GNN architecture that extends the family of order-2
Graph G-invariant Networks without going into higher order tensors. In particular  we want the new
family to include GNNs that can distinguish some pairs of non-isomorphic regular graphs with the
same degree. For instance  take the pair of Circular Skip Link graphs G8 2 and G8 3  illustrated
in Figure 5.2. Roughly speaking  if all the nodes in both graphs have the same node feature  then
because they all have the same degree  the updates of node states in both graph neural networks
based on neighborhood aggregation and the WL test will fail to distinguish the nodes. However 
the power graphs1 of G8 2 and G8 3 have different degrees. Another important example comes
from spectral methods that operate on normalized operators  such as the normalized Laplacian
= I  D1/2AD1/2  where D is the diagonal degree operator. Such normalization preserves the
permutation symmetries and in many clustering applications leads to dramatic improvements [29].

1If A is the adjacency matrix of a graph  its power graph has adjacency matrix min(A2  1). The matrix
min(A2  1) has been used in [5] in graph neural networks for community detection and in [22] for the quadratic
assignment problem  and it leverages multiscale information in the graph. Note that it differs from taking the
power of certain matrices  which is exploited in [16] for example.

6

Figure 2: The Circular Skip Link graphs Gn k are undirected graphs in n nodes q0  . . .   qn1 so that
(i  j) 2 E if and only if |i  j|⌘ 1 or k (mod n). In this ﬁgure we depict (left) G8 2 and (right)
G8 3. It is very easy to check that Gn k and Gn0 k0 are not isomorphic unless n = n0 and k ⌘± k0
(mod n). Both 1-WL and G-invariant networks fail to distinguish them.

This motivates us to consider a polynomial ring generated by the matrices that are the outputs of
permutation-equivariant linear layers  rather than just the linear space of those outputs. Together
with point-wise nonlinear activation functions such as ReLU  power graph adjacency matrices like
min(A2  1) can be expressed with suitable choices of parameters.
To start with  we revisit the theory of linear equivariant functions developed in [18]. It is shown that
any linear equivariant layer from Rn⇥n to Rn⇥n can be represented as L✓(A) =P15
i=1 ✓iLi(A) +
P17
i=16 ✓iLi  where {Li}i=1 ... 15 is the set of 15 basis functions for all linear equivariant functions
from Rn⇥n to Rn⇥n  L16 and L17 are the basis for the bias terms  and ✓ 2 R17 are the parameters
that determine L. Generalizing to an equivariant linear layer from Rn⇥n⇥d to Rn⇥n⇥d0  it becomes
L✓(A)· · k0 =Pd
With this in mind  we now deﬁne a new architecture. Suppose the input is A(0) 2 Rn⇥n⇥d  containing
data on a graph with n nodes. We ﬁx some integer T   and for t = {0  ...  T  1}  iteratively deﬁne

i=1 ✓k k0 iLi(A· · i) +P17

i=16 ✓k k0 iLi  with ✓ 2 Rd⇥d0⇥17.

k=1P15

B(t)
1
B(t)
2

= (L↵(t)(A(t)))
= (L(t)(A(t)) · L(t)(A(t)))

A(t+1) = k(t)

1 B(t)

1 + k(t)

2 B(t)

2

1   k(t)

ij + ✓DPi i A(T )

compute ✓SPi j A(T )

2 2 R  ↵(t)  (t)  (t) 2 Rd(t)⇥d0(t)⇥17 are learnable parameters  and  is a pointwise
where k(t)
nonlinear activation function such as ReLU. If a scalar output is desired  then in the ﬁnal layer we
  where ✓S ✓ D 2 R are trainable parameters. We call the

ii
resulting architecture the Ring-GNN.2
Note that each layer is equivariant  and the map from A to the ﬁnal scalar output is invariant. A
Ring-GNN can reduce to an order-2 Graph G-invariant Network if k(t)
2 = 0. With J + 1 layers
and suitable choices of the parameters  it is possible to obtain min(A2J   1) in the (J + 1)th layer.
Therefore  we expect it to succeed in distinguishing certain pairs of regular graphs that order-2
Graph G-invariant Networks fail on  such as the Circular Skip Link graphs. Indeed  this is veriﬁed
in the synthetic experiment presented in the next section. The normalized Laplacian can also be
approximated  since the degree matrix can be inverted by taking the reciprocal on the diagonal  and
then entry-wise inversion and square root on the diagonal can be approximated by MLPs.
Computationally  the complexity of running the forward model grows as O(n3)  dominated by the
matrix multiplications  which are what enable the computations of certain higher-order information
as the depth increases. In comparison  a Graph G-invariant Network with maximal tensor order k
will have complexity at least O(nk). Therefore  the Ring-GNN is able to explore some higher-order
interactions in the graph (which order-2 Graph G-invariant Networks neglect) while remaining
computationally tractable. We note also that Ring-GNN can be augmented with matrix inverses or
more generally with functional calculus on the spectrum of any of the intermediate representations3
while keeping O(n3) computational complexity.

2We call it Ring-GNN since the main object we consider is the ring of matrices  but technically we can

express an associative algebra since our model includes scalar multiplications.

3When A = A(0) is the adjacency matrix of an undirected graph  one easily veriﬁes that A(t) contains only

symmetric matrices for all t.

7

6 Experiments

The different models and the detailed setup of the experiments are discussed in Appendix F4. All
experiments are conducted on GeForce GTX 1080 Ti and RTX 2080 Ti.

6.1 Classifying Circular Skip Links (CSL) graphs
The following experiment on synthetic data demonstrates the connection between function ﬁtting
and graph isomorphism testing. The Circular Skip Links graphs5 are undirected regular graphs with
node degree 4 [21]  as illustrated in Figure 5.2. Note that two CSL graphs Gn k and Gn0 k0 are not
isomorphic unless n = n0 and k ⌘± k0 (mod n). In the experiment  which has the same setup as in
[21]  we ﬁx n = 41  and set k 2{ 2  3  4  5  6  9  11  12  13  16}  and each k corresponds to a distinct
isomorphism class. The task is then to classify a graph Gn k by its skip length k.
Note that since the 10 classes have the same size  a naive uniform classiﬁer would obtain 10%
accuracy. As we see from Table 1  both GIN and G-invariant network with tensor order 2 do not
outperform the naive classiﬁer. Their failure in this task is unsurprising: WL tests are proved to fall
short of distinguishing such pairs of non-isomorphic regular graphs [4]  and hence neither can GIN
[32]; by the theoretical results from the previous section  order-2 Graph G-invariant network are
unable to distinguish them either. Therefore  their failure as graph isomorphism tests is consistent
with their failure in this classiﬁcation task  which can be understood as trying to approximate the
function that maps the graph to their class labels.
It should be noted that  since graph isomorphism tests are not entirely well-posed as classﬁcation
tasks  the performance of GNN models could vary due to randomness. But the fact that Ring-GNNs
achieve a relatively high maximum accuracy (compared to RP for example) demonstrates that as a
class of GNNs it is rich enough to contain functions that distinguish the CSL graphs to a large extent.

GNN architecture
RP-GIN †
GIN † ‡
Order-2 Graph G-inv. †
sGNN-5
sGNN-2
sGNN-1
LGNN [5]
Ring-GNN
Ring-GNN (w/ degree) ‡

IMDBB

Circular Skip Links
max min
10
53.3
10
10
10
10
80
80
30
30
10
10
30
30
80
10
-
-

std
12.9
0
0
0
0
0
0
15.7
-

mean
-
75.1
71.3
72.8
73.1
72.7
74.1
73.0
73.3

IMDBM
std
-
2.8
3.9
3.2
2.1
2.1
3.0
2.7
4.2

std mean
-
5.1
4.5
3.8
5.2
4.9
4.6
5.4
4.9

-
52.3
48.6
49.4
49.0
49.0
50.9
48.2
51.3

Table 1: (left) Accuracy of different GNNs at classifying CSL (see Section 6.1). We report the best
and worst performances among 10 experiments. (right) Accuracy of different GNNs at classifying
real datasets IMDBB  IMDBM [34] (see Section 6.2). We report the best performance among all 350
epochs on 10-fold cross-validation  as was done in [32]. †: Reported performance by [21]  [32] and
[18]. ‡: On the IMDB datasets  unlike the other models  both GIN and the Ring-GNN (w/ degree) on
the last row take the node degrees as input node features (see Section 6.2).

IMDB datasets

6.2
We use the two IMDB datasets (IMDBBINARY  IMDBMULTI)6 [34] to test different models in
real-world scenarios. Since our focus is on distinguishing graph structures  these datasets are suitable
as they do not contain node features. IMDBBINARY has 1000 graphs  with average number of nodes
19.8 and 2 classes. IMDBMULTI has 1500 graphs  with average number of nodes 13.0 and 3 classes.
Both datasets are randomly partitioned into 9 : 1 for training/validation. As these two social network
datasets have no informative node features  GIN uses one-hot encodings of node degrees as input
node features  while the other baseline models treat all nodes to have identical features. For a fairer
comparison  we apply two versions of Ring-GNN: the ﬁrst one treats all nodes as having identical

4The code is available at https://github.com/leichen2018/Ring-GNN.
5CSL dataset: https://github.com/PurdueMINDS/RelationalPooling/tree/master/
6IMDB datasets: https://github.com/weihua916/powerful-gnns/blob/master/dataset.zip

8

input features and has identical depth and widths as the order-2 Graph G-invariant Network [18] 
denoted as “Ring-GNN" in Table 1; the second one uses the node degree as input features (though not
as one-hot encodings  due to computational constraints  but simply as one integer per node)  denoted
as “Ring-GNN w/ degree" in Table 1. All models are evaluated via 10-fold cross validation and best
accuracy is calculated through averaging across folds followed by maximizing along epochs [32].
Table 1 shows that Ring-GNN models achieve higher or similar performance compared to the order-2
Graph G-invariant networks on both datasets  and slightly worse performance compared to GIN.

6.3 Other real-world datasets
We perform further experiments on four other real-world datasets for classiﬁcation tasks  including a
social network dataset  COLLAB  and three bioinformatics datasets  MUTAG  PTC  PROTEINS7 [34].
The experiment setup (10-fold cross validation  training/validation split) is identical to that of the
IMDB datasets  except that all the bioinformatics datasets contain node features  and more details
of hyperparameters are included in Appendix F. As shown in Table 2  Ring-GNN outperforms
order-2 Graph G-invariant Network in all four datasets  and outperforms GIN in one out of the four
datasets. Moreover  we note that the main goal of this part of our work is not necessarily to ﬁnd the
best-performing GNN through hyperparameter optimization  but rather to propose Ring-GNN as
an augmented version of order-2 Graph G-invariant Networks and show experimental results that
support the theory.

COLLAB MUTAG
80.1±1.4
86.8±6.4
89.4±5.6
80.2±1.9
84.6±10.0
77.9 ± 1.7

PTC
65.7±7.1
64.6±7.0
59.5±7.3

PROTEINS
75.7±2.9
76.2±2.8
75.2±4.3

Ring-GNN
GIN †
Order-2 Graph G-inv. †

Table 2: Accuracy of different GNNs evaluated on several other real-world datasets. We report the
best performance among all epochs on 10-fold cross-validation. †: Reported by [32] and [18].

7 Conclusions

In this work we address the important question of organizing the fast-growing zoo of GNN architec-
tures in terms of what functions they can and cannot represent. We follow the approach via the graph
isomorphism test  and show that is equivalent to the other perspective via function approximation.
We leverage our graph isomorphism reduction to augment order-2 G-invariant nets with the ring of
operators associated with matrix multiplication  which gives provable gains in expressive power with
complexity O(n3)  and is amenable to efﬁciency gains by leveraging sparsity in the graphs.
Our general framework leaves many interesting questions unresolved. First  a more comprehensive
analysis on which elements of the algebra are really needed depending on the application. Next  our
current GNN taxonomy is still incomplete  and in particular we believe it is important to further
discern the abilities between spectral and neighborhood-aggregation-based architectures. Finally 
and most importantly  our current notion of invariance (based on permutation symmetry) deﬁnes a
topology in the space of graphs that is too strong; in other words  two graphs are either considered
equal (if they are isomorphic) or not. Extending the theory of symmetric universal approximation to
take into account a weaker metric in the space of graphs  such as the Gromov-Hausdorff distance  is a
natural next step  that will better reﬂect the stability requirements of powerful graph representations
to small graph perturbations in real-world applications.

Acknowledgements We would like to thank Haggai Maron and Thomas Kipf for fruitful discus-
sions and for pointing us towards G-invariant networks as powerful models to study representational
power in graphs. We thank Prof. Michael M. Bronstein for supporting this research with computing
resources. This work was partially supported by NSF grant RI-IIS 1816753  NSF CAREER CIF
1845360  the Alfred P. Sloan Fellowship  Samsung GRP and Samsung Electronics. SV was partially
funded by EOARD FA9550-18-1-7007 and the Simons Collaboration Algorithms and Geometry.

7Real datasets: https://github.com/weihua916/powerful-gnns/blob/master/dataset.zip

9

,Zhengdao Chen
Soledad Villar
Lei Chen
Joan Bruna