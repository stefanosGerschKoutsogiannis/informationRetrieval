2019,Connections Between Mirror Descent  Thompson Sampling and the Information Ratio,The information-theoretic analysis by Russo and Van Roy [2014] in combination with minimax duality has proved a powerful tool for the analysis of online learning algorithms in full and partial information settings. In most applications there is a tantalising similarity to the classical analysis based on mirror descent. We make a formal connection  showing that the information-theoretic bounds in most applications are derived from existing techniques from online convex optimisation. Besides this  we improve best known regret guarantees for $k$-armed adversarial bandits  online linear optimisation on $\ell_p$-balls and bandits with graph feedback.,Connections Between Mirror Descent  Thompson

Sampling and the Information Ratio

Julian Zimmert

DeepMind  London/

University of Copenhagen

zimmert@di.ku.dk

Tor Lattimore

DeepMind  London

lattimore@google.com

Abstract

The information-theoretic analysis by Russo and Van Roy [25] in combination
with minimax duality has proved a powerful tool for the analysis of online learning
algorithms in full and partial information settings. In most applications there
is a tantalising similarity to the classical analysis based on mirror descent. We
make a formal connection  showing that the information-theoretic bounds in most
applications can be derived from existing techniques for online convex optimisation.
Besides this  for k-armed adversarial bandits we provide an efﬁcient algorithm
with regret that matches the best information-theoretic upper bound and improve
best known regret guarantees for online linear optimisation on (cid:96)p-balls and bandits
with graph feedback.

1

Introduction

The combination of minimax duality and the information-theoretic machinery by Russo and Van Roy
[25] has proved a powerful tool in the analysis of online learning algorithms. This has led to short
and insightful analysis for k-armed bandits  linear bandits  convex bandits and partial monitoring  all
improving on prior best known results. The downside is that the approach is non-constructive. The
application of minimax duality demonstrates the existence of an algorithm with a given bound in the
adversarial setting  but provides no way of constructing that algorithm.
The fundamental quantity in the information-theoretic analysis is the ‘information ratio’ in round t 
which informally is

information ratiot =

(expected regret in round t)2

expected information gain in round t

 

where the information gain is either measured using the mutual information [25] or a generalisation
based on a Bregman divergence [21]. Proving the information ratio is small corresponds to showing
that either the learner is suffering small regret in round t or gaining information  which ultimately
leads to a bound on the cumulative regret. The aforementioned generalisation by Lattimore and
Szepesvári [21] (restated in the supplementary) lead to a short analysis for k-armed adversarial
bandits that is minimax optimal except for small constant factors. The authors speculated that the
new idea should lead to improved bounds for a range of online learning problems and suggested a
number of applications  including bandits with graph feedback [3] and linear bandits on (cid:96)p-balls [11].
We started to follow this plan  successfully improving existing minimax bounds for bandits with
graph feedback and online linear optimisation for (cid:96)p-balls with full information (the bandit setting
remains a mystery). Along the way  however  we noticed a striking connection between the analysis
techniques for bounding the information ratio and controlling the stability of online stochastic mirror
descent (OSMD)  which is a classical algorithm for online convex optimisation. A connection was

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

already hypothesised by Lattimore and Szepesvári [21]  who noticed a similarity between the bounds
obtained. Notably  why does using the negentropy potential in the information-theoretic analysis lead
to almost identical bounds for k-armed bandits as Exp3? Why does this continue to hold with the
Tsallis entropy and the INF strategy [6]?

Contribution Our main contribution is a formal connection between the information-theoretic
analysis and OSMD. Speciﬁcally  we show how tools for analysing OSMD can be applied to a
modiﬁed version of Thompson sampling that uses the same sampling strategy as OSMD  but replaces
the mirror descent update with a Bayesian update. This contribution is valuable for several reasons:
(a) it explains the similarity between the information-theoretic and OSMD style analysis  (b) it allows
for the transfer of techniques for OSMD to Bayesian regret analysis and (c) it opens the possibility of
a constructive transfer of ideas from Bayesian regret analysis to the adversarial framework  as we
illustrate in the next contribution.
A curiosity in the Bayesian analysis of adversarial k-armed bandits is that the resulting bound was
always a factor of 2 smaller than the corresponding bound for OSMD. This was true in the original
analysis [25] and its generalisation [21]. Our new theorem entirely explains the difference  and indeed 
allows us to improve the bounds for OSMD. This leads to an efﬁcient algorithm for adversarial
2kn + O(k)  matching the information-theoretic upper bound

k-armed bandits with regret Rn ≤ √

except for small lower-order terms.
Finally  we improve the regret guarantees for two online learning problems. First  for bandits with
graph feedback we improve the minimax regret in the ‘easy’ setting by a log(n) factor  matching the
lower bound up to a factor of log3/2(k). Second  for online linear optimisation over the (cid:96)p-balls we
improve existing bounds by arbitrarily large constant factors. At ﬁrst we had proved these results
using the information-theoretic tools and minimax duality  but here we present the uniﬁed view and
consequentially the analysis also applies to OSMD for which we have efﬁcient algorithms.

Related work The information-theoretic Bayesian regret analysis was introduced by [24  25  26].
The focus in these papers is on the analysis of Bayesian algorithms in the stochastic setting  a line
of work continued recently by [15]. [10] noticed that the stochastic assumption is not required and
that the results continued to hold in a Bayesian adversarial setting where the prior is over arbitrary
sequences of losses  rather than over (parametric) distributions as is usual in Bayesian statistics. The
idea to use minimax duality to derive minimax regret bounds is due to [1] and has been applied and
generalised by a number of authors [10  17  21  9]. Mirror descent was developed by [22] and [23]
for optimization. As far as we know its ﬁrst application to bandits was by [2]  which precipitated a
ﬂood of papers as summarised in the books by [8  20]. We work in the partial monitoring framework 
which goes back to [27]. Most of the focus since then has been on classifying the growth of the regret
on the horizon for ﬁnite partial monitoring games [13  16  5  7  19]. Bandits with graph feedback are
a special kind of partial monitoring problem and have been studied extensively [3  14  4  and others] 
with a monograph on the subject by [28]. Online linear optimisation is an enormous subject by itself.
We refer the reader to the books by [12  18].
Notation The reader will ﬁnd omitted proofs in the appendix. Let [n] = {1  2  . . .   n} and Bd
p =
{x ∈ Rd : (cid:107)x(cid:107)p ≤ 1} be the standard (cid:96)p-ball. For positive deﬁnite A we write (cid:107)x(cid:107)2
A = x(cid:62)Ax.
Given a topological space X  let int(X) be its interior and ∆(X) be the space of probability
measures on X with the Borel σ-algebra. We write X◦ = {y ∈ Rd : supx∈X |(cid:104)x  y(cid:105)| ≤ 1} for the
functional analysts polar and co(X) for the convex hull of X. The domain of a convex function
F : Rd → R ∪ {∞} is dom(F ) = {x : F (x) < ∞}. For x  y ∈ dom(F ) the Bregman divergence
between x and y with respect to F is DF (x  y) = F (x) − F (y) − ∇Fx−y(y) where ∇vF (x) is
the directional derivative of F at x in the direction v. The diameter of X with respect to F is
diamF (X) = supx y∈X F (x) − F (y). We abuse notation by writing ∇−2F (x) = (∇2F (x))−1.
For x  y ∈ Rd we let [x  y] = co({x  y}) be the convex hull of x and y  which is the set of points on
the chord between x and y.

Linear partial monitoring Our results are most easily expressed in a linear version of the partial
monitoring framework  which extends the standard adversarial linear bandit framework to general
feedback structures. Let A be the action space and L the loss space  which are subsets of Rd with
A compact. The convex hull of A is X = co(A). When A is ﬁnite we let k = |A|. The signal

2

function is a known function Φ : A × L → Σ for some observation space Σ. An adversary and
t=1 with (cid:96)t ∈ L for all t. In
learner interact over n rounds. First the adversary secretly chooses ((cid:96)t)n
each round t the learner samples an action At ∈ A from a distribution depending on observations
A1  Φ1  . . .   At−1  Φt−1 where Φs = Φ(As  (cid:96)s) is the observation in round s. The regret of policy π
in environment ((cid:96)t)n

t=1 is

Rn(π  ((cid:96)t)n

t=1) = max
a∈A

E

(cid:104)At − a  (cid:96)t(cid:105)

 

(cid:35)

(cid:34) n(cid:88)

t=1

where the expectation is with respect to the randomness in the actions. The regret depends on a policy
and the losses. The minimax regret is
R∗
n = inf
π

Rn(π  ((cid:96)t)n

t=1)  

sup
((cid:96)t)n

t=1

where the inﬁmum is over all policies and the supremum over all loss sequences in Ln. From here on
the dependence of Rn on the policy and loss sequence is omitted.
Examples The standard k-armed bandit is recovered when A = {e1  . . .   ek}  L = [0  1]k and
Φ(a  (cid:96)) = (cid:104)a  (cid:96)(cid:105) ∈ Σ = [0  1]. For linear bandits the set A is an arbitrary compact set and L is
typically A◦. Bandits with graph feedback have a richer signal function as we explain in Section 4.

as normal. The optimal action is now a random variable A∗ = arg mina∈A(cid:80)n

Bayesian setting
t=1 are sampled from a known
prior probability measure ν on Ln and subsequently the learner interacts with the sampled losses
t=1(cid:104)a  (cid:96)t(cid:105) and the

In the Bayesian setting the sequence of losses ((cid:96)t)n

Bayesian regret is

(cid:34) n(cid:88)

(cid:35)

BRn = E

(cid:104)At − A∗  (cid:96)t(cid:105)

.

Finally  deﬁne Pt(·) = P(·|Ft) and Et[·] = E[·|Ft] with Ft = σ(A1  Φ1  . . .   At  Φt)  ∆t =
(cid:104)At − A∗  (cid:96)t(cid:105). A crucial piece of notation is Xt = Et−1[At] ∈ X   which is the conditional expected
action played in round t.

t=1

2 Mirror descent  Thompson sampling and the information ratio

Algorithm 1: OSMD
Input: A = (P  E  F ) and η
Initialize X1 = arg mina∈X F (a)
for t = 1  . . .   n do

We now develop the connection between OSMD and the
information-theoretic Bayesian regret analysis. Speciﬁ-
cally we show that instances of OSMD can be transformed
into an algorithm similar to Thompson sampling (TS) for
which the Bayesian regret can be bounded in the same way
as the regret of the original algorithm. The similarity to
TS is important. Any instance of OSMD with a uniform
bound on the adversarial regret enjoys the same bound on
the Bayesian regret for any prior without modiﬁcation. Our
result has a different ﬂavour because we prove a bound for a variant of OSMD that replaces the mirror
descent update with a Bayesian update.
OSMD is a modular algorithm that depends on deﬁning three components: (1) A sampling scheme
that determines how the algorithm explores  (2) a method for estimating the unobserved loss vectors 
and (3) a convex ‘potential’ and learning rate that determines how the algorithm updates its iterates.
The following deﬁnition makes this more precise.
Deﬁnition 1. An instance of OSMD is determined by a tuple A = (P  F  E) and learning rate η > 0
such that

Sample At ∼ PXt and observe Φt
Construct: ˆ(cid:96)t = E(Xt  At  Φt)
Update: Xt+1 = ft(Xt  At)

(a) The sampling scheme is a collection P = {Px : x ∈ X} of probability measures in ∆(A)

such that EA∼Px [A] = x for all x ∈ X .

(b) The potential is a Legendre function F : Rd → R ∪ {∞} with dom(F ) ∩ X (cid:54)= ∅ and η > 0

is the learning rate.

3

(c) The estimation function is E : X × A × Σ → Rd  which we assume satisﬁes

EA∼Px [E(x  A  Φ(A  (cid:96)))] = (cid:96) for all (cid:96) ∈ L and x ∈ X .

The assumptions on the mean of Px and that E is unbiased are often relaxed in minor ways  but for
simplicity we maintain the strict deﬁnition. For the remainder we ﬁx A = (P  F  E) and η > 0 and
abbreviate

Et(x  a) = E(x  a  Φ(a  (cid:96)t))

and

ˆ(cid:96)t = E(Xt  At  Φt) .

You should think of Et(x  a) as the estimated loss vector when the learner plays action a while
sampling from Px and ˆ(cid:96)t as the realisation of this estimate in round t. OSMD starts by initialising X1
as the minimiser of F constrained to X . Subsequently it samples At ∼ PXt and updates

A useful notation is to let (ft)n

t=1 and (gt)n

Xt+1 = arg min

η(cid:104)y  ˆ(cid:96)t(cid:105) + DF (y  Xt) .

y∈X
t=1 be sequences of functions from X × A to Rd with
(η(cid:104)y  Et(x  a)(cid:105) + DF (y  x))

and
(η(cid:104)y  Et(x  a)(cid:105) + DF (y  x))  

ft(x  a) = arg min

y∈X

gt(x  a) = arg min

y∈int(dom(F ))

which means that Xt+1 = ft(Xt  At)  while gt is the same as ft  but without the constraint to X .
The complete algorithm is summarised in Algorithm 1. The next theorem is well known [20  §28].
Theorem 2 (OSMD REGRET BOUND). The regret of OSMD satisﬁes

(cid:34) n(cid:88)

t=1

Rn ≤ diamF (X )
(cid:20)

η

+

E

η
2

stabt(Xt; η)

 

(cid:35)

η

where stabt(x; η) =

2
η

EA∼Px

(cid:104)x − ft(x  A)  Et(x  A)(cid:105) − DF (ft(x  A)  x)

The random variable stabt(Xt; η) measures the stability of the algorithm relative to the learning rate
and is usually almost surely bounded. The diameter term depends on how fast the algorithm can
move from the starting point to optimal  which is large when the learning rate is small. In this sense
the learning rate is tuned to balance the stability of the algorithm and the requirement that (Xt) can
tend towards an optimal point. Note that stabt(x) depends on P   E  F   η and the loss vector (cid:96)t 
which means that in the Bayesian setting the stability function is random. The next lemma is also
known and is often useful for bounding the stability function.
Lemma 3. Suppose that F is twice differentiable on int(dom(F ))  then

(cid:21)

.

(cid:35)

(cid:35)

.

.

stabt(x; η) ≤ EA∼Px

sup

z∈[x ft(x A)]

(cid:107)Et(x  A)(cid:107)2∇−2F (z)

Furthermore  provided that gt(x  a) exists for all a in the support of Px  then

stabt(x; η) ≤ EA∼Px

sup

z∈[x gt(x A)]

(cid:107)Et(x  A)(cid:107)2∇−2F (z)

(cid:34)

(cid:34)

Bayesian analysis Modiﬁed Thompson sampling (MTS)
is a variant of TS summarised in Algorithm 2 that depends
on a prior distribution ν and a sampling scheme P . The
algorithm differs from Algorithm 1 in the computation
of Xt. Rather than using the mirror descent update  it
uses the Bayesian expected optimal action conditioned on
the observations. Expectations in this subsection are with
respect to both the prior and the actions  which means that
((cid:96)t)n
random. Our main theorem is the following bound on the Bayesian regret of MTS.

Algorithm 2: MTS
Input: Prior ν and P
Initialize X1 = E[A∗]
for t = 1  . . .   n do

t=1 are randomly distributed according to ν and consequently the functions ft  gt and stabt are

Sample At ∼ PXt and observe Φt
Update: Xt+1 = Et−1[A∗]

4

(cid:34) n(cid:88)

(cid:35)

.

Theorem 4. MTS satisﬁes BRn ≤ diamF (X )
Remark 5. The stability function depends on A = (P  F  E) and η while Algorithm 2 only uses P .
In this sense Theorem 4 shows that MTS satisﬁes the given bound for all E  F and η. MTS is the
same as TS when sampling from the posterior is the same as sampling from PXt. A fundamental
case where this always holds is when A = {e1  . . .   ed} because each x ∈ X is uniquely represented
as a linear combination of elements in A and hence Px is unique.

stabt(Xt; η)

η
2

t=1

E

+

η

Proof of Theorem 4. Beginning with the deﬁnition of the per-step regret 

Et−1 [∆t] = (cid:104)Xt  Et−1[(cid:96)t](cid:105) − Et−1 [(cid:104)A∗  (cid:96)t(cid:105)]

(cid:104)(cid:104)A∗  ˆ(cid:96)t(cid:105)(cid:105)
(cid:104)(cid:104)Et−1[A∗ | At  Φt]  ˆ(cid:96)t(cid:105)(cid:105)

= (cid:104)Xt  Et−1[ˆ(cid:96)t](cid:105) − Et−1
= (cid:104)Xt  Et−1[ˆ(cid:96)t](cid:105) − Et−1
= Et−1
≤ Et−1

(cid:104)(cid:104)Xt − Xt+1  ˆ(cid:96)t(cid:105)(cid:105)
(cid:20)
(cid:20) η

≤ Et−1

2

1
η

(cid:104)Xt − ft(Xt  At)  ˆ(cid:96)t(cid:105) − 1
η

stabt(Xt; η) +

DF (Xt+1  Xt)

.

(cid:21)

1
η

DF (Xt+1  Xt)

(1)

(2)

(3)

(4)

(5)

DF (ft(Xt  At)  Xt) +

(cid:21)

Eq. (1) uses that the loss estimators are unbiased. Eq. (2) follows using the tower rule for conditional
expectations and the fact that ˆ(cid:96)t is a measurable function of Xt  At and Φt so that
Et−1[(cid:104)A∗  ˆ(cid:96)t(cid:105)] = Et−1[Et−1[(cid:104)A∗  ˆ(cid:96)t(cid:105)| At  Φt]] = Et−1[(cid:104)Et−1[A∗ | At  Φt]  ˆ(cid:96)t(cid:105)] = Et−1[(cid:104)Xt+1  ˆ(cid:96)t(cid:105)] .
Eq. (3) uses the deﬁnitions of Xt+1. Eq. (4) follows from the deﬁnition of ft  which implies that

(cid:104)ft(Xt  At)  ˆ(cid:96)t(cid:105) +

1
η

DF (ft(Xt  At)  Xt) ≤ (cid:104)Xt+1  ˆ(cid:96)t(cid:105) +

1
η

DF (Xt+1  Xt) .

Finally  Eq. (5) follows from the deﬁnition of stabt. The proof is completed by summing over the
per-step regret  noting that (Xt)n

t=1 is a (Ft)t-adapted martingale and by [21  Theorem 3] 

(cid:34) n(cid:88)

E

(cid:35)

DF (Xt+1  Xt)

≤ E[F (Xn+1)] − F (X1) ≤ diamF (X ) .

t=1

The stability coefﬁcient The only difference between Theorems 2 and 4 is the trajectory of (Xt)n
t=1
and the randomness of the stability function. In most analyses of OSMD the ﬁnal bound is obtained
via a uniform bound on stabt(x; η) that holds regardless of the losses and in this case the trajectory
Xt is irrelevant. This is formalised in the following deﬁnition and corollary. Deﬁne the stability
coefﬁcients by

stab(A ; η) = sup
x∈X

max
t∈[n]

stabt(x; η)

and

stab(A ) = sup
η>0

stab(A ; η) .

Corollary 6. The regret of Algorithm 1 for an appropriately tuned learning rate is bounded by

The Bayesian regret of Algorithm 2 is bounded by BRn ≤(cid:112)2 diamF (X ) ess sup(stab(A ))n.

Rn ≤(cid:112)2 diamF (X ) stab(A )n .

t=1 
The essential supremum is needed because the stability coefﬁcient depends on the losses ((cid:96)t)n
which are random in the Bayesian setting. Generally speaking  however  bounds on the stability
coefﬁcient are proven in a manner that is independent of the losses.
Remark 7. Often stab(A ; η) ≤ a + bη for constants a  b ≥ 0 and stab(A ) = ∞. Nevertheless 
the same argument shows that the regret of Algorithm 1 is bounded by
b diamF (X )

Rn ≤(cid:112)2a diamF (X )n +

 

a

and similarly for the Bayesian regret of Algorithm 2.

5

Stability and the information ratio The generalised information-theoretic analysis by [21] starts
by assuming there exists a constant α > 0 such that the following bound on the information ratio
holds almost surely:

information ratiot = Et−1[∆t]2(cid:46)Et−1[DF (Xt+1  Xt)] ≤ α .

(6)

Then [21  Theorem 3] shows that

BRn ≤(cid:112)αn diamF (X ) .

(7)
The proof of Theorem 4 directly provides a bound on the information ratio in terms of the stability
coefﬁcient. To see this  notice that Eq. (5) holds for all measurable η and let

η =(cid:112)2Et−1[DF (Xt+1  Xt)]/ ess sup(stab(A )) .

(8)

Then by Eq. (5) and the deﬁnition of stab(A ) it follows that

Et−1[∆t]2(cid:46)Et−1[DF (Xt+1  Xt)] ≤ 2 ess sup(stab(A )) a.s. .

In other words  the usual methods for bounding the stability coefﬁcient in the analysis of OSMD can
be used to bound the information ratio in the information-theoretic analysis.
Example 8. To make the abstraction more concrete  consider the k-armed bandit problem where
L = [0  1]k and A = {e1  . . .   ek}. In this case there is a unique sampling scheme deﬁned by
Px(a) = (cid:104)x  a(cid:105). The standard loss estimation function is to use importance-weighting  which leads to
A commonly used potential is the unnormalised negentropy F (x) = (cid:80)k
(9)
i=1 xi log(xi) − xi that
satisﬁes ∇−2F (x) = diag(x). The instance of OSMD resulting from these choices is called Exp3
(cid:16)−η(cid:80)t−1
(cid:17)(cid:17)
for which an explicit form for Xt is well known:

Et(x  a)i = (cid:96)ti1(a = ei)(cid:14)xi .
(cid:16)−η(cid:80)t−1

(cid:17)(cid:46)(cid:16)(cid:80)k

Xti = exp

ˆ(cid:96)sj

ˆ(cid:96)si

.

j=1 exp

s=1

s=1

A short calculation shows that gt(x  a)i = xi exp(−η ˆ(cid:96)ti) ≤ xi. The stability function is bounded
using the second part of Lemma 3 by

(cid:34)

(cid:34)

z∈[x gt(x A)]

sup

k(cid:88)

i=1

zti

stabt(x; η) ≤ EA∼Px

(cid:107)Et(x  A)(cid:107)2∇−2F (z)

= EA∼Px

sup

z∈[x gt(x A)]

1(A = ei)(cid:96)2
ti

x2
ti

= EA∼Px

(cid:35)

(cid:35)

(cid:20) 1(A = ei)(cid:96)2

ti

(cid:21)

xti

≤ k(cid:88)

ti ≤ k .
(cid:96)2

i=1

Finally  the diameter of the probability simplex X with respect to the unnormalised negentropy is
diamF (X ) = log(k). Applying Theorem 2 shows that the regret of OSMD and Bayesian regret of
MTS satisfy

BRn ≤(cid:112)2nk log(k)

(MTS) .

Rn ≤(cid:112)2nk log(k)

Remark 9. Theorems 2 and 4 are vacuous when diamF (X ) = ∞. The most straightforward
resolution is to restrict Xt to a subset of X on which the diameter is bounded and then control the
additive error. This idea also works in the Bayesian setting as described by [21]. We omit a detailed
discussion to avoid technicalities.

(OSMD)

and

3 Bandits
F (x) = −2(cid:80)k
The best known bound on the minimax regret for k-armed bandits is Rn ≤ √

xi be the 1/2-Tsallis entropy and prove that

Et−1[∆t]2(cid:46)Et−1[DF (Xt+1  Xt)] ≤

√

√

k .

i=1

√
By Cauchy-Schwarz diamF (X ) ≤ 2
2nk
for all priors ν. Minimax duality is used to conclude that R∗
2kn. Meanwhile 
√
using the importance-weighted estimator in Eq. (9) leads to a bound on the stability co-
efﬁcient of stab(A ) ≤ 2
8nk.

k and then Eq. (7) shows that BRn ≤ √
k and then Theorem 2 yields a bound of Rn ≤ √

n ≤ √

2kn by [21]. They let

6

The discrepancy between these methods is entirely ex-
plained by the naive choice of importance-weighted
estimator. The approach based on bounding the infor-
mation ratio is effectively shifting the losses  which
can be achieved in the OSMD framework by shifting
the importance-weighted estimators (see Fig. 1). This
idea reduces the worst-case variance of the importance
weighted estimators by a factor of 4.
Lemma 10. If the loss estimator in Example 8 with

F (s) = −2(cid:80)k

√
xi is replaced by
((cid:96)ti − cti)1(a = ei)

i=1

Et(x  a)i =

xi

+ cti  

800

600

400

200

0

0

INF
INF+shift

25000

50000

75000

100000

where cti =

(1 − 1(Xti < η2))  

1
2

2kn + 48k.

Figure 1: Comparison of INF with and with-
out shifted loss estimators. x-axis is number
of time-steps and y-axis the empirical regret
estimation. η is tuned to the horizon and
all experiments use Bernoulli losses with
E[(cid:96)t] = (0.45  0.55  . . .   0.55)T (k = 5).
We repeat the experiment 100 times with er-
ror bars indicating three standard deviations.
The empirical result matches our theoretical
improvement of a factor 2.

then the stability coefﬁcient for any η ≤ 1/2 is bounded
by stab(A ; η) ≤ k1/2/2 + 12kη.
Theorem 11. The regret of OSMD with the loss estima-
Rn ≤ √
tor of Lemma 10 and appropriate learning rate satisﬁes:
4 Bandits with graph feedback
In bandits with graph feedback the action set is A = {e1  . . .   ek} and L = [0  1]k. Let E ⊆ [k]× [k]
be a set of directed edges over vertex set [k] so that G = ([k]  E) is a directed graph. The signal
function is Φ(ei  (cid:96)) = {(j  (cid:96)j) : j ∈ N (i)}. The standard bandit framework is recovered when
E = {(i  i) : i ∈ [k]} while the full information setup corresponds to E = [k] × [k]. Of course there
are settings between and beyond these extremes. The difﬁculty of the graph feedback problem is
determined by the connectivity of the graph. For example  when E = ∅  the learner has no way to
estimate the losses and the regret is linear in the worst case. Like ﬁnite partial monitoring  graph
feedback problems can be classiﬁed into one of four regimes for which:

n ∈(cid:110)O(1)  ˜Θ(n1/2)  Θ(n2/3)  Ω(n)

(cid:111)

R∗

.

Our focus is on graph feedback problems that ﬁt in the second category  which is the most challenging
to analyse.
Deﬁnition 12. G is called strongly observable if for every vertex i ∈ [k] at least one of the following
holds: (a) a ∈ N (b) for all b (cid:54)= a or (b) a ∈ N (a).

Alon et al. [3] prove the minimax regret for bandits with graph feedback is ˜Θ(n1/2) if and only if
k > 1 and G is strongly observable. They also prove the following theorem upper and lower bounding
the dependence of the minimax regret on the horizon  the number of actions and a graph functional
called the independence number.
Theorem 13 ([3]). Let Gind be the independence number of G  which is the cardinality of the largest
subset of vertices such that no tow distinct vertices are connected by an edge. Suppose k > 1 and G
is strongly observable. Then R∗
The logarithmic dependence on n in the proof of Theorem 13 appears quite naturally  which raises the
√
question of whether or not the upper or lower bound is tight. In fact  as n tends to inﬁnity the upper
bound in Theorem 13 could be improved to O(
nk) by using a ﬁnite-armed algorithm that ignores
the feedback except for the played action. Perhaps the independence number is not as fundamental as
ﬁrst thought? The following theorem shows the upper bound can be improved.
Theorem 14. Let A = (P  E  F ) be a triple deﬁning OSMD with Px(a) = (cid:104)a  x(cid:105) 

√Gindn log(kn)) and R∗

√Gindn).

n = O(

n = Ω(

F (x) =

1

α(1 − α)

xα
i

where α = 1 − 1/ log(k) .

k(cid:88)

i=1

7

Finally  deﬁne the unbiased loss estimation function E by

(cid:80)

(cid:96)ti1(a ∈ N (i))
b∈N (i) xb

Et(x  a)i =

for i (cid:54)∈ It  and Et(x  a)i =

((cid:96)ti − 1)1(a (cid:54)= i)

1 − xi

+ 1 otherwise  

where It = {i ∈ [k] : i (cid:54)∈ N (i) and Xti > 1/2}. Then for any k ≥ 8 and an appropriately tuned

learning rate the regret of OSMD with A satisﬁes Rn = O((cid:112)Gindn log(k)3).
p = 1 (cid:112)n log(d)
p > 1 (cid:112)n/(p − 1)

5 Online linear optimisation over (cid:96)p-balls

p and L = Bd

We now consider full information online linear optimization
on the (cid:96)p balls with p ∈ [1  2]  which is modelled in our
framework by choosing A = Bd
q with 1/p +
Table 1: Known results for (cid:96)p-balls
1/q = 1 and Φ(a  (cid:96)) = (cid:96). Table 1 summarises the known
results. When p = 1 the situation is unambiguous  with matching upper and lower bounds. For
p ∈ (1  2] there exist algorithms for which the regret is dimension free  but with constants that become
arbitrarily large as p tends to 1. Known results for online gradient descent (OGD) prove the blowup
in terms of p is avoidable  but with a price that is polynomial in the dimension.
Theorem 15. For any p ∈ [1  2]  let h be the following convex and twice continuously differentiable
function:

Algorithm
Hedge
[12  §11.5]
OGD [18]

d2/p−1n

p ≥ 1

Regret

√

p

(cid:40) d

h(x) =

Then for OSMD using potential F (x) =(cid:80)d

p−2|x| +
p−1

2 x2
p−2
p−1 d

1

p−2

if |x| ≤ d
otherwise .

p

p−2

|x|p
p(p−1) + 2−p
2p d
i=1 h(xi)  loss estimator E(x  a  σ) = σ  an arbitrary

exploration scheme and appropriately tuned learning rate 

Rn = O(cid:16)(cid:112)min{1/(p − 1)  log(d)} n
(cid:17)

.

Furthermore  the Bayesian regret of TS is bounded by the same quantity.
Remark 16. In the full information setting the loss estimation is independent of the action  which
explains the arbitrariness of the exploration scheme. The intuitive justiﬁcation for the slightly cryptic
potential function is provided in the supplementary material.

6 Discussion

We demonstrated a connection between the information-theoretic analysis and OSMD. For k-armed
bandits  we explained the factor of two difference between the regret analysis using information-
theoretic and convex-analytic machinery and improved the bound for the latter. For graph bandits
we improved the regret by a factor of log(n). Finally  we designed a new potential for which the
regret for online linear optimisation over the (cid:96)p-balls improves the previously best known bound by
arbitrarily large constant factors.

Open problems The main open problem is whether or not we can ‘close the circle’ and use the
information-theoretic analysis to directly construct OSMD algorithms. Another direction is to try
and relax the assumption that the loss is linear. The leading constant in the new bandit analysis now
matches the best known information-theoretic bound [21]. There is still a constant lower-order term 
which presently seems challenging to eliminate. In bandits with graph feedback one can ask whether
the log(k) dependency can be improved. Lower bounds are still needed for (cid:96)p-balls and extending
the idea to the bandit setting is an obvious followup. Finally  the best known algorithms for ﬁnite
partial monitoring also use the information-theoretic machinery. Understanding how to borrow the
ideas for OSMD remains a challenge.

References
[1] J. Abernethy  A. Agarwal  P. L. Bartlett  and A. Rakhlin. A stochastic view of optimal regret
through minimax duality. In Proceedings of the 22nd Annual Conference on Learning Theory 
2009.

8

[2] J. D. Abernethy  E. Hazan  and A. Rakhlin. Competing in the dark: An efﬁcient algorithm for
bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory 
pages 263–274. Omnipress  2008.

[3] N. Alon  N. Cesa-Bianchi  O. Dekel  and T. Koren. Online learning with feedback graphs:
Beyond bandits. In Peter Grünwald  Elad Hazan  and Satyen Kale  editors  Proceedings of The
28th Conference on Learning Theory  volume 40 of Proceedings of Machine Learning Research 
pages 23–35  Paris  France  03–06 Jul 2015. PMLR.

[4] N. Alon  N. Cesa-Bianchi  C. Gentile  S. Mannor  Y. Mansour  and O. Shamir. Nonstochastic
multi-armed bandits with graph-structured feedback. SIAM Journal on Computing  46(6):
1785–1826  2017.

[5] A. Antos  G. Bartók  D. Pál  and Cs. Szepesvári. Toward a classiﬁcation of ﬁnite partial-

monitoring games. Theoretical Computer Science  473:77–99  2013.

[6] J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In

Proceedings of Conference on Learning Theory (COLT)  pages 217–226  2009.

[7] G. Bartók  D. P. Foster  D. Pál  A. Rakhlin  and Cs. Szepesvári. Partial monitoring—
classiﬁcation  regret bounds  and algorithms. Mathematics of Operations Research  39(4):
967–997  2014.

[8] S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed
Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated 
2012.

[9] S. Bubeck and M. Sellke. First-order regret analysis of Thompson sampling. arXiv preprint

arXiv:1902.00681  2019.

√

[10] S. Bubeck  O. Dekel  T. Koren  and Y. Peres. Bandit convex optimization:

T regret in one
dimension. In P. Grünwald  E. Hazan  and S. Kale  editors  Proceedings of The 28th Conference
on Learning Theory  volume 40 of Proceedings of Machine Learning Research  pages 266–278 
Paris  France  03–06 Jul 2015. PMLR.

[11] S. Bubeck  M. Cohen  and Y. Li. Sparsity  variance and curvature in multi-armed bandits. In
F. Janoos  M. Mohri  and K. Sridharan  editors  Proceedings of Algorithmic Learning Theory 
volume 83 of Proceedings of Machine Learning Research  pages 111–127. PMLR  07–09 Apr
2018.

[12] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge university press 

2006.

[13] N. Cesa-Bianchi  G. Lugosi  and G. Stoltz. Regret minimization under partial monitoring.

Mathematics of Operations Research  31:562–580  2006.

[14] A. Cohen  T. Hazan  and T. Koren. Online learning with feedback graphs without the graphs. In

International Conference on Machine Learning  pages 811–819  2016.

[15] S. Dong and B. Van Roy. An information-theoretic analysis for thompson sampling with many
actions. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett 
editors  Advances in Neural Information Processing Systems 31  pages 4157–4165. Curran
Associates  Inc.  2018.

[16] D. Foster and A. Rakhlin. No internal regret via neighborhood watch. In N. D. Lawrence and
M. Girolami  editors  Proceedings of the 15th International Conference on Artiﬁcial Intelligence
and Statistics  volume 22 of Proceedings of Machine Learning Research  pages 382–390  La
Palma  Canary Islands  21–23 Apr 2012. PMLR.

[17] N. Gravin  Y. Peres  and B. Sivan. Towards optimal algorithms for prediction with expert advice.
In Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms 
pages 528–547. SIAM  2016.

[18] E. Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimiza-

tion  2(3-4):157–325  2016.

[19] T. Lattimore and Cs. Szepesvári. Cleaning up the neighbourhood: A full classiﬁcation for
adversarial partial monitoring. In International Conference on Algorithmic Learning Theory 
2019.

9

[20] T. Lattimore and Cs. Szepesvári. Bandit Algorithms. Cambridge University Press (preprint) 

2019.

[21] T. Lattimore and Cs. Szepesvári. An information-theoretic approach to minimax regret in partial

monitoring. In Conference on Learning Theory  2019.

[22] A. S. Nemirovsky. Efﬁcient methods for large-scale convex optimization problems. Ekonomika

i Matematicheskie Metody  15  1979.

[23] A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efﬁciency in Optimization.

Wiley  1983.

[24] D. Russo and B. Van Roy. Learning to optimize via information-directed sampling.

In
Z. Ghahramani  M. Welling  C. Cortes  N. D. Lawrence  and K. Q. Weinberger  editors  Advances
in Neural Information Processing Systems 27  NIPS  pages 1583–1591. Curran Associates  Inc. 
2014.

[25] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. Mathematics of

Operations Research  39(4):1221–1243  2014.

[26] D. Russo and B. Van Roy. An information-theoretic analysis of Thompson sampling. Journal

of Machine Learning Research  17(1):2442–2471  2016. ISSN 1532-4435.

[27] A. Rustichini. Minimizing regret: The general case. Games and Economic Behavior  29(1):

224–243  1999.

[28] M. Valko. Bandits on graphs and structures  2016.

10

,Julian Zimmert
Tor Lattimore