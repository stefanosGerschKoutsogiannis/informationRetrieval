2017,Estimation of the covariance structure of heavy-tailed distributions,We propose and analyze a new estimator of the covariance matrix that admits strong theoretical guarantees under weak assumptions on the underlying distribution  such as existence of moments of only low order. While estimation of covariance matrices corresponding to sub-Gaussian distributions is well-understood  much less in known in the case of heavy-tailed data.  As K. Balasubramanian and M. Yuan write  ``data from real-world experiments oftentimes tend to be corrupted with outliers and/or exhibit heavy tails. In such cases  it is not clear that those covariance matrix estimators .. remain optimal'' and ``..what are the other possible strategies to deal with heavy tailed distributions warrant further studies.'' We make a step towards answering this question and prove tight deviation inequalities for the proposed estimator that depend only on the parameters controlling the ``intrinsic dimension'' associated to the covariance matrix (as opposed to the dimension of the ambient space); in particular  our results are applicable in the case of high-dimensional observations.,Estimation of the covariance structure of heavy-tailed

distributions

Stanislav Minsker

Department of Mathematics

University of Southern California

Los Angeles  CA 90007

minsker@usc.edu

Xiaohan Wei

Department of Electrical Engineering

University of Southern California

Los Angeles  CA 90007

xiaohanw@usc.edu

Abstract

We propose and analyze a new estimator of the covariance matrix that admits strong
theoretical guarantees under weak assumptions on the underlying distribution 
such as existence of moments of only low order. While estimation of covariance
matrices corresponding to sub-Gaussian distributions is well-understood  much
less in known in the case of heavy-tailed data. As K. Balasubramanian and M.
Yuan write 1  “data from real-world experiments oftentimes tend to be corrupted
with outliers and/or exhibit heavy tails. In such cases  it is not clear that those
covariance matrix estimators .. remain optimal” and “..what are the other possible
strategies to deal with heavy tailed distributions warrant further studies.” We make
a step towards answering this question and prove tight deviation inequalities for the
proposed estimator that depend only on the parameters controlling the “intrinsic
dimension” associated to the covariance matrix (as opposed to the dimension of
the ambient space); in particular  our results are applicable in the case of high-
dimensional observations.

1

Introduction

Estimation of the covariance matrix is one of the fundamental problems in data analysis: many
important statistical tools  such as Principal Component Analysis (PCA  Hotelling  1933) and
regression analysis  involve covariance estimation as a crucial step. For instance  PCA has immediate
applications to nonlinear dimension reduction and manifold learning techniques (Allard et al.  2012) 
genetics (Novembre et al.  2008)  computational biology (Alter et al.  2000)  among many others.
However  assumptions underlying the theoretical analysis of most existing estimators  such as various
modiﬁcations of the sample covariance matrix  are often restrictive and do not hold for real-world
scenarios. Usually  such estimators rely on heuristic (and often bias-producing) data preprocessing 
such as outlier removal. To eliminate such preprocessing step from the equation  one has to develop a
class of new statistical estimators that admit strong performance guarantees  such as exponentially
tight concentration around the unknown parameter of interest  under weak assumptions on the
underlying distribution  such as existence of moments of only low order. In particular  such heavy-
tailed distributions serve as a viable model for data corrupted with outliers – an almost inevitable
scenario for applications.
We make a step towards solving this problem: using tools from the random matrix theory  we will
develop a class of robust estimators that are numerically tractable and are supported by strong
theoretical evidence under much weaker conditions than currently available analogues. The term
“robustness” refers to the fact that our estimators admit provably good performance even when the
underlying distribution is heavy-tailed.

1Balasubramanian and Yuan (2016)

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

√

√

AT A)  where

used in the paper. Everywhere below  (cid:107) · (cid:107) stands for the operator norm (cid:107)A(cid:107) :=(cid:112)λmax (AT A). If
The Frobenius (or Hilbert-Schmidt) norm is (cid:107)A(cid:107)F =(cid:112)tr(AT A)  and the associated inner product

1.1 Notation and organization of the paper
Given A ∈ Rd1×d2  let AT ∈ Rd2×d1 be transpose of A. If A is symmetric  we will write λmax (A)
and λmin (A) for the largest and smallest eigenvalues of A. Next  we will introduce the matrix norms
d1 = d2 = d  we denote by trA the trace of A. For A ∈ Rd1×d2  the nuclear norm (cid:107) · (cid:107)1 is deﬁned
as (cid:107)A(cid:107)1 = tr(
AT A)2 = AT A.
1A2). For z ∈ Rd  (cid:107)z(cid:107)2 stands for the usual Euclidean norm of z. Let A  B be
is (cid:104)A1  A2(cid:105) = tr(A∗
two self-adjoint matrices. We will write A (cid:23) B (or A (cid:31) B) iff A − B is nonnegative (or positive)
deﬁnite. For a  b ∈ R  we set a ∨ b := max(a  b) and a ∧ b := min(a  b). We will also use the
standard Big-O and little-o notation when necessary.
Finally  we give a deﬁnition of a matrix function. Let f be a real-valued function deﬁned on an interval
T ⊆ R  and let A ∈ Rd×d be a symmetric matrix with the eigenvalue decomposition A = U ΛU∗
such that λj(A) ∈ T  j = 1  . . .   d. We deﬁne f (A) as f (A) = U f (Λ)U∗  where

AT A is a nonnegative deﬁnite matrix such that (

√

 .

...

f (λd)



λ1


 :=

f (λ1)

...

λd

f (Λ) = f

Few comments about organization of the material in the rest of the paper: section 1.2 provides an
overview of the related work. Section 2 contains the mains results of the paper. The proofs are
outlined in section 4; longer technical arguments can be found in the supplementary material.

E(cid:2)(X − µ0)(X − µ0)T(cid:3)  and assume E(cid:107)X − µ0(cid:107)4

1.2 Problem formulation and overview of the existing work
Let X ∈ Rd be a random vector with mean EX = µ0  covariance matrix Σ0 =
2 < ∞. Let X1  . . .   Xm be i.i.d. copies of
X. Our goal is to estimate the covariance matrix Σ from Xj  j ≤ m. This problem and its variations
have previously received signiﬁcant attention by the research community: excellent expository papers
by Cai et al. (2016) and Fan et al. (2016) discuss the topic in detail. However  strong guarantees
for the best known estimators hold (with few exceptions mentioned below) under the restrictive
assumption that X is either bounded with probability 1 or has sub-Gaussian distribution  meaning
that there exists σ > 0 such that for any v ∈ Rd of unit Euclidean norm 

Pr (|(cid:104)v  X − µ0(cid:105)| ≥ t) ≤ 2e− t2σ2

2

.

In the discussion accompanying the paper by Cai et al. (2016)  Balasubramanian and Yuan (2016)
write that “data from real-world experiments oftentimes tend to be corrupted with outliers and/or
exhibit heavy tails. In such cases  it is not clear that those covariance matrix estimators described
in this article remain optimal” and “..what are the other possible strategies to deal with heavy tailed
distributions warrant further studies.” This motivates our main goal: develop new estimators of the
covariance matrix that (i) are computationally tractable and perform well when applied to heavy-tailed
data and (ii) admit strong theoretical guarantees (such as exponentially tight concentration around
the unknown covariance matrix) under weak assumptions on the underlying distribution. Note that 
unlike the majority of existing literature  we do not impose any further conditions on the moments of
X  or on the “shape” of its distribution  such as elliptical symmetry.
Robust estimators of covariance and scatter have been studied extensively during the past few decades.
However  majority of rigorous theoretical results were obtained for the class of elliptically symmetric
distributions which is a natural generalization of the Gaussian distribution; we mention just a small
subsample among the thousands of published works. Notable examples include the Minimum
Covariance Determinant estimator and the Minimum Volume Ellipsoid estimator which are discussed
in (Hubert et al.  2008)  as well Tyler’s (Tyler  1987) M-estimator of scatter. Works by Fan et al.
(2016); Wegkamp et al. (2016); Han and Liu (2017) exploit the connection between Kendall’s tau
and Pearson’s correlation coefﬁcient (Fang et al.  1990) in the context of elliptical distributions to
obtain robust estimators of correlation matrices. Interesting results for shrinkage-type estimators
have been obtained by Ledoit and Wolf (2004); Ledoit et al. (2012). In a recent work  Chen et al.
(2015) study Huber’s ε-contamination model which assumes that the data is generated from the

2

distribution of the form (1 − ε)F + εQ  where Q is an arbitrary distribution of “outliers” and F is
an elliptical distribution of “inliers”  and propose novel estimator based on the notion of “matrix
depth” which is related to Tukey’s depth function (Tukey  1975); a related class of problems has
been studies by Diakonikolas et al. (2016). The main difference of the approach investigated in this
paper is the ability to handle a much wider class of distributions that are not elliptically symmetric
and only satisfy weak moment assumptions. Recent papers by Catoni (2016)  Giulini (2015)  Fan
et al. (2016  2017); Fan and Kim (2017) and Minsker (2016) are closest in spirit to this direction.
For instance  Catoni (2016) constructs a robust estimator of the Gram matrix of a random vector
Z ∈ Rd (as well as its covariance matrix) via estimating the quadratic form E(cid:104)Z  u(cid:105)2 uniformly over
all (cid:107)u(cid:107)2 = 1. However  the bounds are obtained under conditions more stringent than those required
by our framework  and resulting estimators are difﬁcult to evaluate in applications even for data of
moderate dimension. Fan et al. (2016) obtain bounds in norms other than the operator norm which
the focus of the present paper (however  we plan to address optimality guarantees with respect to
other norms in the future). Minsker (2016) and Fan et al. (2016) use adaptive truncation arguments to
construct robust estimators of the covariance matrix. However  their results are only applicable to the
situation when the data is centered (that is  µ0 = 0). In the robust estimation framework  rigorous
extension of the arguments to the case of non-centered high-dimensional observations is non-trivial
and requires new tools  especially if one wants to avoid statistically inefﬁcient procedures such as
sample splitting. We formulate and prove such extensions in this paper.

2 Main results

Deﬁnition of our estimator has its roots in the technique proposed by Catoni (2012). Let

1
mθ

(1)

(2)

m(cid:88)

i=1

ψ(cid:0)θ(Xi −(cid:98)µ)(Xi −(cid:98)µ)T(cid:1)  

be the usual truncation function. As before  let X1  . . .   Xm be i.i.d. copies of X  and assume that(cid:98)µ
is a suitable estimator of the mean µ0 from these samples  to be speciﬁed later. We deﬁne(cid:98)Σ as

ψ(x) = (|x| ∧ 1) sign(x)

where θ (cid:39) m−1/2 is small (the exact value will be given later). It easily follows from the deﬁnition

(cid:98)Σ :=
m(cid:88)
of the matrix function that(cid:98)Σ =
all random variables θ (cid:107)Xi −(cid:98)µ(cid:107)2
hence it is easily computable. Note that ψ(x) = x in the neighborhood of 0; it implies that whenever
sample mean (cid:98)Σ is close to the usual sample covariance estimator. On the other hand  ψ “truncates”
2   1 ≤ i ≤ m are “small” (say  bounded above by 1) and ˆµ is the
(cid:107)Xi −(cid:98)µ(cid:107)2

(Xi −(cid:98)µ)(Xi −(cid:98)µ)T
(cid:107)Xi −(cid:98)µ(cid:107)2

m  thus limiting the effect of outliers. Our results (formally stated below 

θ (cid:107)Xi −(cid:98)µ(cid:107)2

2

2 on level (cid:39) √

see Theorem 2.1) imply that for an appropriate choice of θ = θ(t  m  σ) 

(cid:17)

 

(cid:16)

ψ

1
mθ

i=1

2

(cid:114)

(cid:13)(cid:13)(cid:13)(cid:98)Σ − Σ0
(cid:13)(cid:13)(cid:13)E(cid:107)X − µ0(cid:107)2

(cid:13)(cid:13)(cid:13) ≤ C0σ0
2 (X − µ0)(X − µ0)T(cid:13)(cid:13)(cid:13)

β
m

with probability ≥ 1 − de−β for some positive constant C0  where

σ2
0 :=

is the "matrix variance".

2.1 Robust mean estimation

There are several ways to construct a suitable estimator of the mean µ0. We present the one obtained
via the “median-of-means” approach. Let x1  . . .   xk ∈ Rd. Recall that the geometric median of
x1  . . .   xk is deﬁned as

k(cid:88)

j=1

(cid:107)z − xj(cid:107)2 .

med (x1  . . .   xk) := argmin
z∈Rd

3

Let 1 < β < ∞ be the conﬁdence parameter  and set k =
Divide the sample X1  . . .   Xm into k disjoint groups G1  . . .   Gk of size

3.5β

+ 1; we will assume that k ≤ m
2 .

each  and deﬁne

(cid:106) m

(cid:107)

k

(cid:106)

(cid:107)

(cid:88)

i∈Gj

ˆµj :=

1
|Gj|

Xi  j = 1 . . . k 

ˆµ := med (ˆµ1  . . .   ˆµk) .
It then follows from Corollary 4.1 in (Minsker  2015) that

(cid:16)(cid:107)ˆµ − µ(cid:107)2 ≥ 11

Pr

(cid:114)tr(Σ0)(β + 1)

(cid:17) ≤ e−β.

(3)

(4)

m

2.2 Robust covariance estimation

Let(cid:98)Σ be the estimator deﬁned in (2) with(cid:98)µ being the “median-of-means” estimator (3). Then(cid:98)Σ

admits the following performance guarantees:
Lemma 2.1. Assume that σ ≥ σ0  and set θ = 1
that m ≥ Cdβ  where C > 0 is an absolute constant. Then
β
m

(cid:13)(cid:13)(cid:13)(cid:98)Σ − Σ0

(cid:114)

σ

(cid:113) β
(cid:13)(cid:13)(cid:13) ≤ 3σ

m . Moreover  let d := σ2

0/(cid:107)Σ0(cid:107)2  and suppose

(cid:13)(cid:13)(cid:13)(cid:98)Σ∗ − Σ0

(cid:13)(cid:13)(cid:13) ≤ 18σ0

(cid:114)

β
m

.

4

(5)

with probability at least 1 − 5de−β.
Remark 2.1. The quantity ¯d is a measure of “intrinsic dimension” akin to the “effective rank”
r = tr(Σ0)
(cid:107)Σ0(cid:107) ; see Lemma 2.3 below for more details. Moreover  note that the claim of Lemma 2.1
holds for any σ ≥ σ0  rather than just for σ = σ0; this “degree of freedom” allows construction of
adaptive estimators  as it is shown below.

0 in order to obtain a good estimator(cid:98)Σ. More often than not  such information is

The statement above suggests that one has to know the value of (or a tight upper bound on) the
“matrix variance” σ2
unavailable. To make the estimator completely data-dependent  we will use Lepski’s method (Lepski 
1992). To this end  assume that σmin   σmax are “crude” preliminary bounds such that

σmin ≤ σ0 ≤ σmax .

Usually  σmin and σmax do not need to be precise  and can potentially differ from σ0 by several
orders of magnitude. Set

σj := σmin 2j and J = {j ∈ Z : σmin ≤ σj < 2σmax } .

Note that the cardinality of J satisﬁes card(J ) ≤ 1 + log2(σmax /σmin ). For each j ∈ J   deﬁne
θj := θ(j  β) = 1
σj

m. Deﬁne

(cid:113) β

(cid:98)Σm j =

1
mθj

m(cid:88)

i=1

ψ(cid:0)θj(Xi −(cid:98)µ)(Xi −(cid:98)µ)T(cid:1) .
(cid:13)(cid:13)(cid:13)(cid:98)Σm k −(cid:98)Σm j
(cid:13)(cid:13)(cid:13) ≤ 6σk

(cid:114)

(cid:41)

β
m

j∗ := min

j ∈ J : ∀k > j s.t. k ∈ J  

Our main result is the following statement regarding the performance of the data-dependent estimator

and(cid:98)Σ∗ :=(cid:98)Σm j∗. Note that the estimator(cid:98)Σ∗ depends only on X1  . . .   Xm  as well as σmin   σmax .
(cid:98)Σ∗:
Theorem 2.1. Suppose m ≥ Cdβ  then  the following inequality holds with probability at least
1 − 5d log2

(cid:16) 2σmax

e−β:

(cid:17)

(6)

σmin

Finally  set

(cid:40)

An immediate corollary of Theorem 2.1 is the quantitative result for the performance of PCA based

on the estimator (cid:98)Σ∗. Let Projk be the orthogonal projector on a subspace corresponding to the k
largest positive eigenvalues λ1  . . .   λk of Σ0 (here  we assume for simplicity that all the eigenvalues
largest eigenvalues of(cid:98)Σ∗. The following bound follows from the Davis-Kahan perturbation theorem
are distinct)  and (cid:91)Projk – the orthogonal projector of the same rank as Projk corresponding to the k

(Davis and Kahan  1970)  more speciﬁcally  its version due to Zwald and Blanchard (2006  Theorem
3 ).
Corollary 2.1. Let ∆k = λk − λk+1  and assume that ∆k ≥ 72σ0

(cid:113) β

m . Then

(cid:13)(cid:13)(cid:91)Projk − Projk
(cid:16) 2σmax

(cid:17)

e−β.

(cid:13)(cid:13) ≤ 36

∆k

(cid:114)

σ0

β
m

σmin

with probability ≥ 1 − 5d log2
Fan et al. (2016)  which constructs a covariance estimator(cid:98)Σ(cid:48)
(cid:33)

vector X is centered  and supv∈Rd:(cid:107)v(cid:107)2≤1
the inequality

It is worth comparing the bound of Lemma 2.1 and Theorem 2.1 above to results of the paper by
m under the assumption that the random
m satisﬁes

E(cid:2)|(cid:104)v  X(cid:105)|4(cid:3) = B < ∞. More speciﬁcally (cid:98)Σ(cid:48)
(cid:13)(cid:13)(cid:13) ≥

(cid:32)(cid:13)(cid:13)(cid:13)(cid:98)Σ(cid:48)

≤ de−β 

m − Σ0

(cid:114)

C1βBd

(7)

P

m

where C1 > 0 is an absolute constant. The main difference between (7) and the bounds of Lemma
2.1 and Theorem 2.1 is that the latter are expressed in terms of σ2
0  while the former is in terms of B.
The following lemma demonstrates that our bounds are at least as good:
Lemma 2.2. Suppose that EX = 0 and supv∈Rd:(cid:107)v(cid:107)2≤1

E(cid:2)|(cid:104)v  X(cid:105)|4(cid:3) = B < ∞. Then Bd ≥ σ2
estimator(cid:98)Σ∗ is bounded above by O((cid:112)d/m) if m (cid:38) d. It has been shown (for example  Lounici 
2014) that the minimax lower bound of covariance estimation is of order Ω((cid:112)d/m). Hence  the

0.
0/(cid:107)Σ0(cid:107)2 (cid:46) d. Hence  By Theorem 2.1  the error rate of

It follows from the above lemma that d = σ2

bounds of Fan et al. (2016) as well as our results imply correct order of the error. That being said  the
“intrinsic dimension” ¯d reﬂects the structure of the covariance matrix and can potentially be much
smaller than d  as it is shown in the next section.

2.3 Bounds in terms of intrinsic dimension

vector X  the bound O((cid:112)d/m) is suboptimal  while our estimator can achieve a much better rate

In this section  we show that under a slightly stronger assumption on the fourth moment of the random

in terms of the “intrinsic dimension” associated to the covariance matrix. This makes our estimator
useful in applications involving high-dimensional covariance estimation  such as PCA. Assume the
following uniform bound on the kurtosis:

(cid:114)
E(cid:16)
E(cid:16)

X (k) − µ(k)
X (k) − µ(k)

0

0

(cid:17)4
(cid:17)2 = R < ∞ 

max

k=1 2 ... d

(8)

where X (k)  µ(k)
covariance matrix Σ0 can be measured by the effective rank deﬁned as

0 denotes the k-th entry of X and µ0 respectively. The intrinsic dimension of the

r(Σ0) =

tr(Σ0)
(cid:107)Σ0(cid:107) .

Note that we always have r(Σ0) ≤ rank(Σ0) ≤ d  and it some situations r(Σ0) (cid:28) rank(Σ0)  for
instance if the covariance matrix is “approximately low-rank”  meaning that it has many small
eigenvalues. The constant σ2
0 is closely related to the effective rank as is shown in the following
lemma (the proof of which is included in the supplementary material):

5

Lemma 2.3. Suppose that (8) holds. Then 

r(Σ0)(cid:107)Σ0(cid:107)2 ≤ σ2

0 ≤ R2r(Σ0)(cid:107)Σ0(cid:107)2.

As a result  we have r(Σ0) ≤ d ≤ R2r(Σ0). The following corollary immediately follows from
Theorem 2.1 and Lemma 2.3:
Corollary 2.2. Suppose that m ≥ Cβr(Σ0) for an absolute constant C > 0 and that (8) holds.
Then

(cid:114)
(cid:13)(cid:13)(cid:13) ≤ 18R(cid:107)Σ0(cid:107)
(cid:13)(cid:13)(cid:13)(cid:98)Σ∗ − Σ0
(cid:17)
(cid:16) 2σmax

e−β.

σmin

r(Σ0)β

m

with probability at least 1 − 5d log2

3 Applications: low-rank covariance estimation

In many data sets encountered in modern applications (for instance  gene expression proﬁles (Saal
et al.  2007))  dimension of the observations  hence the corresponding covariance matrix  is larger
than the available sample size. However  it is often possible  and natural  to assume that the unknown
matrix possesses special structure  such as low rank  thus reducing the “effective dimension” of the
problem. The goal of this section is to present an estimator of the covariance matrix that is “adaptive”
to the possible low-rank structure; such estimators are well-known and have been previously studied
for the bounded and sub-Gaussian observations (Lounici  2014). We extend these results to the case
of heavy-tailed observations; in particular  we show that the estimator obtained via soft-thresholding

applied to the eigenvalues of(cid:98)Σ∗ admits optimal guarantees in the Frobenius (as well as operator)
Let(cid:98)Σ∗ be the estimator deﬁned in the previous section  see equation (6)  and set

norm.

(cid:21)

A∈Rd×d

(cid:20)(cid:13)(cid:13)(cid:13)A −(cid:98)Σ∗
(cid:13)(cid:13)(cid:13)2
(cid:98)Στ∗ = argmin
+ τ (cid:107)A(cid:107)1
(cid:16)
(cid:16)(cid:98)Σ∗
(cid:17) − τ /2  0
(cid:17)
d(cid:88)

max

F

2n can be written explicitly as

where τ > 0 controls the amount of penalty. It is well-known (e.g.  see the proof of Theorem 1 in

Lounici (2014)) that(cid:98)Στ
where λi((cid:98)Σ∗) and vi((cid:98)Σ∗) are the eigenvalues and corresponding eigenvectors of(cid:98)Σ∗. We are ready to

vi((cid:98)Σ∗)vi((cid:98)Σ∗)T  

(cid:98)Στ∗ =

λi

i=1

 

(9)

state the main result of this section.
Theorem 3.1. For any τ ≥ 36σ0

(cid:13)(cid:13)(cid:13)(cid:98)Στ∗ − Σ0

(cid:13)(cid:13)(cid:13)2

F

with probability ≥ 1 − 5d log2

(cid:107)A − Σ0(cid:107)2

F +

(1 +

√

8

2)2

(cid:35)

τ 2rank(A)

.

(10)

m  

(cid:34)

A∈Rd×d

≤ inf

(cid:113) β
(cid:17)
(cid:16) 2σmax
(cid:13)(cid:13)(cid:13)(cid:98)Στ∗ − Σ0
(cid:13)(cid:13)(cid:13)2
(cid:16) 2σmax
(cid:17)

σmin

σmin

e−β.

(cid:113) β

≤ 162 σ2

0

F

e−β.

(cid:16)

√

1 +

2

(cid:17)2 βr

m

In particular  if rank(Σ0) = r and τ = 36σ0

m  we obtain that

with probability ≥ 1 − 5d log2

4 Proofs

4.1 Proof of Lemma 2.1

The result is a simple corollary of the following statement.

6

(cid:113) β
m   where σ ≥ σ0 and m ≥ β. Let d := σ2

0/(cid:107)Σ0(cid:107)2. Then  with probability

Lemma 4.1. Set θ = 1
σ
at least 1 − 5de−β 

(cid:13)(cid:13)(cid:13) ≤ 2σ
(cid:13)(cid:13)(cid:13)(cid:98)Σ − Σ0
(cid:115)

(cid:114)
(cid:18) β

β
m

(cid:19) 3

4

(cid:115)

(cid:18) β

(cid:19) 5

4

(cid:18) β

(cid:19) 3

2

+

+

m

β
m

dσ
(cid:107)Σ0(cid:107)

+C(cid:48)(cid:107)Σ0(cid:107)

√
dσ
(cid:107)Σ0(cid:107)
where C(cid:48) > 1 is an absolute constant.
0/(cid:107)Σ0(cid:107)2 ≥ tr(Σ0)/(cid:107)Σ0(cid:107) ≥ 1. Thus 
Now  by Corollary ?? in the supplement  it follows that d = σ2
assuming that the sample size satisﬁes m ≥ (6C(cid:48))4dβ  then  dβ/m ≤ 1/(6C(cid:48))4 < 1  and by some
(cid:114)
algebraic manipulations we have that

dβ2
m2 + d

dσ
(cid:107)Σ0(cid:107)

(cid:114)

(cid:114)

+ d

m

m

m

+

5
4

(cid:18) β

(cid:19) 9

4

  

β
m

β
m

β
m

+ σ

= 3σ

.

(11)

(cid:13)(cid:13)(cid:13)(cid:98)Σ − Σ0

(cid:13)(cid:13)(cid:13) ≤ 2σ

For completeness  a detailed computation is given in the supplement. This ﬁnishes the proof.

4.2 Proof of Lemma 4.1

Let Bβ = 11(cid:112)2tr(Σ0)β/m be the error bound of the robust mean estimator(cid:98)µ deﬁned in (3). Let
Zi = Xi − µ0  Σµ = E(cid:2)(Zi − µ)(Zi − µ)T(cid:3)  ∀i = 1  2 ···   d  and

for any (cid:107)µ(cid:107)2 ≤ Bβ. We begin by noting that the error can be bounded by the supremum of an
empirical process indexed by µ  i.e.

(cid:107)Xi − µ(cid:107)2

(Xi − µ)(Xi − µ)T

i=1

ˆΣµ =

1
mθ

m(cid:88)
(cid:13)(cid:13)(cid:13) ˆΣµ − Σ0
(cid:107)Σµ − Σ0(cid:107) =(cid:13)(cid:13)E(cid:2)(Zi − µ)(Zi − µ)T − ZiZ T

(cid:13)(cid:13)(cid:13) ≤ sup

(cid:13)(cid:13)(cid:13) ˆΣ − Σ0

(cid:107)µ(cid:107)2≤Bβ

(cid:13)(cid:13)(cid:13) ≤ sup
(cid:3)(cid:13)(cid:13) =

2

i

θ (cid:107)Xi − µ(cid:107)2

ψ

(cid:16)
(cid:13)(cid:13)(cid:13) ˆΣµ − Σµ

 

2

(cid:17)
(cid:13)(cid:13)(cid:13) + (cid:107)Σµ − Σ0(cid:107)
(cid:12)(cid:12)(cid:12)E(cid:104)(cid:104)Zi − µ  v(cid:105)2 − (cid:104)Zi  v(cid:105)2(cid:105)(cid:12)(cid:12)(cid:12)

(12)
with probability at least 1 − e−β. We ﬁrst estimate the second term (cid:107)Σµ − Σ0(cid:107). For any (cid:107)µ(cid:107)2 ≤ Bβ 

(cid:107)µ(cid:107)2≤Bβ

sup

v∈Rd:(cid:107)v(cid:107)2≤1
= (µT v)2 ≤ (cid:107)µ(cid:107)2

2 ≤ B2

β = 242

tr(Σ0)β

 

m

with probability at least 1 − e−β. It follows from Corollary ?? in the supplement that with the same
probability

(cid:107)Σµ − Σ0(cid:107) ≤ 242

σ2
0β
(cid:107)Σ0(cid:107)m

≤ 242

σ2β
(cid:107)Σ0(cid:107)m

= 242(cid:107)Σ0(cid:107) dβ
m

.

(13)

Our main task is then to bound the ﬁrst term in (12). To this end  we rewrite it as a double supremum
of an empirical process:

(cid:13)(cid:13)(cid:13) ˆΣµ − Σµ

(cid:13)(cid:13)(cid:13) =

(cid:12)(cid:12)(cid:12)vT(cid:16) ˆΣµ − Σµ

(cid:17)

(cid:12)(cid:12)(cid:12)

v

It remains to estimate the supremum above.
Lemma 4.2. Set θ = 1
σ
at least 1 − 4de−β 

sup

sup

(cid:107)µ(cid:107)2≤Bβ

(cid:107)µ(cid:107)2≤Bβ  (cid:107)v(cid:107)2≤1

(cid:113) β
m   where σ ≥ σ0 and m ≥ β. Let d := σ2
(cid:12)(cid:12)(cid:12)vT(cid:16) ˆΣµ − Σµ
(cid:18) β
(cid:19) 3

(cid:12)(cid:12)(cid:12) ≤ 2σ

(cid:114)
(cid:115)

(cid:18) β

(cid:19) 5

(cid:17)

β
m

v

4

β
m

+

dσ
(cid:107)Σ0(cid:107)

m

+ d

sup

(cid:115)

(cid:107)µ(cid:107)2≤Bβ  (cid:107)v(cid:107)2≤1

+C(cid:48)(cid:48)(cid:107)Σ0(cid:107)

√
dσ
(cid:107)Σ0(cid:107)
where C(cid:48)(cid:48) > 1 is an absolute constant.

dσ
(cid:107)Σ0(cid:107)

m

+

4

0/(cid:107)Σ0(cid:107)2. Then  with probability

(cid:18) β

m

(cid:19) 3

2

+

dβ2
m2 + d

5
4

  

(cid:18) β

(cid:19) 9

4

m

7

Note that σ ≥ σ0 by defnition  thus  d ≤ σ2/(cid:107)Σ0(cid:107)2. Combining the above lemma with (12) and (13)
ﬁnishes the proof.

4.3 Proof of Theorem 2.1
Deﬁne ¯j := min{j ∈ J : σj ≥ σ0}  and note that σ¯j ≤ 2σ0. We will demonstrate that j∗ ≤ ¯j with
high probability. Observe that

(cid:32)(cid:13)(cid:13)(cid:13)(cid:98)Σm k − Σ0

(cid:13)(cid:13)(cid:13) > 3σk

(cid:114)

(cid:33)

β
m

where we applied (5) to estimate each of the probabilities in the sum under the assumption that the
number of samples m ≥ Cdβ and σk ≥ σ¯j ≥ σ0. It is now easy to see that the event

 (cid:91)
(cid:32)(cid:13)(cid:13)(cid:13)(cid:98)Σm ¯j − Σ0

k∈J :k>¯j

Pr (j∗ > ¯j) ≤ Pr

≤ Pr

≤ 5de−β + 5d log2

(cid:41)

β
n

Pr

k∈J : k>¯j

β
m

(cid:114)
(cid:13)(cid:13)(cid:13) > 6σk
(cid:33)
(cid:88)

(cid:40)(cid:13)(cid:13)(cid:13)(cid:98)Σm k − Σm ¯j
(cid:114)
(cid:13)(cid:13)(cid:13) > 3σ¯j
(cid:18) σmax
(cid:19)
(cid:40)(cid:13)(cid:13)(cid:13)(cid:98)Σm k − Σ0

e−β 

σmin

+

(cid:41)

(cid:114)

B =

k∈J :k≥¯j

(cid:13)(cid:13)(cid:13) ≤ 3σk
(cid:92)
(cid:16) 2σmax
(cid:17)
(cid:13)(cid:13)(cid:13) ≤ (cid:107)(cid:98)Σ∗ −(cid:98)Σm ¯j(cid:107) + (cid:107)(cid:98)Σm ¯j − Σ0(cid:107) ≤ 6σ¯j
(cid:114)
(cid:114)

(cid:114)

σmin

β
m

(cid:114)

≤ 12σ0

β
m

+ 6σ0

= 18σ0

β
m

β
m

 

and the claim follows.

4.4 Proof of Theorem 3.1

The proof is based on the following lemma:
Lemma 4.3. Inequality (10) holds on the event E =

(cid:110)

τ ≥ 2

of probability ≥ 1 − 5d log2

e−β is contained in E = {j∗ ≤ ¯j}. Hence  on B

(cid:13)(cid:13)(cid:13)(cid:98)Σ∗ − Σ0

(cid:114)

β
m

+ 3σ¯j

β
m

(cid:13)(cid:13)(cid:13)(cid:111)

.

(cid:13)(cid:13)(cid:13)(cid:98)Σ∗ − Σ0
(cid:16) 2σmax
(cid:17)

σmin

To verify this statement  it is enough to repeat the steps of the proof of Theorem 1 in Lounici (2014) 

replacing each occurrence of the sample covariance matrix by its “robust analogue”(cid:98)Σ∗.
It then follows from Theorem 2.1 that Pr(E) ≥ 1 − 5d log2

e−β whenever τ ≥ 36σ0

(cid:113) β

m.

Acknowledgments

Research of S. Minsker and X. Wei was partially supported by the National Science Foundation grant
NSF DMS-1712956.

References
Allard  W. K.  G. Chen  and M. Maggioni (2012). Multi-scale geometric methods for data sets II: Geometric

multi-resolution analysis. Applied and Computational Harmonic Analysis 32(3)  435–462.

Alter  O.  P. O. Brown  and D. Botstein (2000). Singular value decomposition for genome-wide expression data

processing and modeling. Proceedings of the National Academy of Sciences 97(18)  10101–10106.

Balasubramanian  K. and M. Yuan (2016). Discussion of “Estimating structured high-dimensional covariance
and precision matrices: optimal rates and adaptive estimation”. Electronic Journal of Statistics 10(1)  71–73.

Bhatia  R. (2013). Matrix analysis  Volume 169. Springer Science & Business Media.

8

Boucheron  S.  G. Lugosi  and P. Massart (2013). Concentration inequalities: A nonasymptotic theory of

independence. Oxford university press.

Cai  T. T.  Z. Ren  and H. H. Zhou (2016). Estimating structured high-dimensional covariance and precision

matrices: optimal rates and adaptive estimation. Electron. J. Statist. 10(1)  1–59.

Catoni  O. (2012). Challenging the empirical mean and empirical variance: a deviation study. In Annales de

l’Institut Henri Poincaré  Probabilités et Statistiques  Volume 48  pp. 1148–1185.

Catoni  O. (2016). PAC-Bayesian bounds for the Gram matrix and least squares regression with a random design.

arXiv preprint arXiv:1603.05229.

Chen  M.  C. Gao  and Z. Ren (2015). Robust covariance matrix estimation via matrix depth. arXiv preprint

arXiv:1506.00691.

Davis  C. and W. M. Kahan (1970). The rotation of eigenvectors by a perturbation. iii. SIAM Journal on

Numerical Analysis 7(1)  1–46.

Diakonikolas  I.  G. Kamath  D. M. Kane  J. Li  A. Moitra  and A. Stewart (2016). Robust estimators in high
dimensions without the computational intractability. In Foundations of Computer Science (FOCS)  2016
IEEE 57th Annual Symposium on  pp. 655–664. IEEE.

Fan  J. and D. Kim (2017). Robust high-dimensional volatility matrix estimation for high-frequency factor

model. Journal of the American Statistical Association.

Fan  J.  Q. Li  and Y. Wang (2017). Estimation of high dimensional mean regression in the absence of symmetry
and light tail assumptions. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 79(1) 
247–265.

Fan  J.  Y. Liao  and H. Liu (2016). An overview of the estimation of large covariance and precision matrices.

The Econometrics Journal 19(1)  C1–C32.

Fan  J.  W. Wang  and Y. Zhong (2016). An (cid:96)∞ eigenvector perturbation bound and its application to robust

covariance estimation. arXiv preprint arXiv:1603.03516.

Fan  J.  W. Wang  and Z. Zhu (2016). Robust low-rank matrix recovery. arXiv preprint arXiv:1603.08315.

Fang  K.-T.  S. Kotz  and K. W. Ng (1990). Symmetric multivariate and related distributions. Chapman and Hall.

Giulini  I. (2015). PAC-Bayesian bounds for Principal Component Analysis in Hilbert spaces. arXiv preprint

arXiv:1511.06263.

Han  F. and H. Liu (2017). ECA: high dimensional elliptical component analysis in non-Gaussian distributions.

Journal of the American Statistical Association.

Hotelling  H. (1933). Analysis of a complex of statistical variables into principal components. Journal of

educational psychology 24(6)  417.

Hubert  M.  P. J. Rousseeuw  and S. Van Aelst (2008). High-breakdown robust multivariate methods. Statistical

Science  92–119.

Ledoit  O. and M. Wolf (2004). A well-conditioned estimator for large-dimensional covariance matrices. Journal

of multivariate analysis 88(2)  365–411.

Ledoit  O.  M. Wolf  et al. (2012). Nonlinear shrinkage estimation of large-dimensional covariance matrices.

The Annals of Statistics 40(2)  1024–1060.

Lepski  O. (1992). Asymptotically minimax adaptive estimation. I: Upper bounds. optimally adaptive estimates.

Theory of Probability & Its Applications 36(4)  682–697.

Lounici  K. (2014). High-dimensional covariance matrix estimation with missing observations. Bernoulli 20(3) 

1029–1058.

Minsker  S. (2015). Geometric median and robust estimation in Banach spaces. Bernoulli 21(4)  2308–2335.

Minsker  S. (2016). Sub-Gaussian estimators of the mean of a random matrix with heavy-tailed entries. arXiv

preprint arXiv:1605.07129.

Novembre  J.  T. Johnson  K. Bryc  Z. Kutalik  A. R. Boyko  A. Auton  A. Indap  K. S. King  S. Bergmann 

M. R. Nelson  et al. (2008). Genes mirror geography within Europe. Nature 456(7218)  98–101.

9

Saal  L. H.  P. Johansson  K. Holm  S. K. Gruvberger-Saal  Q.-B. She  M. Maurer  S. Koujak  A. A. Ferrando 
P. Malmström  L. Memeo  et al. (2007). Poor prognosis in carcinoma is associated with a gene expression
signature of aberrant PTEN tumor suppressor pathway activity. Proceedings of the National Academy of
Sciences 104(18)  7564–7569.

Tropp  J. A. (2012). User-friendly tail bounds for sums of random matrices. Found. Comput. Math. 12(4) 

389–434.

Tropp  J. A. (2015). An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571.

Tukey  J. W. (1975). Mathematics and the picturing of data. In Proceedings of the international congress of

mathematicians  Volume 2  pp. 523–531.

Tyler  D. E. (1987). A distribution-free M-estimator of multivariate scatter. The Annals of Statistics  234–251.

Wegkamp  M.  Y. Zhao  et al. (2016). Adaptive estimation of the copula correlation matrix for semiparametric

elliptical copulas. Bernoulli 22(2)  1184–1226.

Zwald  L. and G. Blanchard (2006). On the convergence of eigenspaces in kernel principal component analysis.

In Advances in Neural Information Processing Systems 18  pp. 1649–1656. Cambridge  MA: MIT Press.

10

,Xiaohan Wei
Stanislav Minsker