2009,Sparse Estimation Using General Likelihoods and Non-Factorial Priors,Finding maximally sparse representations from overcomplete feature dictionaries frequently involves minimizing a cost function composed of a likelihood (or data fit) term and a prior (or penalty function) that favors sparsity.  While typically the prior is factorial  here we examine non-factorial alternatives that have a number of desirable properties relevant to sparse estimation and are easily implemented using an efficient  globally-convergent reweighted $\ell_1$ minimization procedure.  The first method under consideration arises from the sparse Bayesian learning (SBL) framework.  Although based on a highly non-convex underlying cost function  in the context of canonical sparse estimation problems  we prove uniform superiority of this method over the Lasso in that  (i) it can never do worse  and (ii) for any dictionary and sparsity profile  there will always exist cases where it does better.  These results challenge the prevailing reliance on strictly convex penalty functions for finding sparse solutions.  We then derive a new non-factorial variant with similar properties that exhibits further performance improvements in empirical tests.  For both of these methods  as well as traditional factorial analogs  we demonstrate the effectiveness of reweighted $\ell_1$-norm algorithms in handling more general sparse estimation problems involving classification  group feature selection  and non-negativity constraints.  As a byproduct of this development  a rigorous reformulation of sparse Bayesian classification (e.g.  the relevance vector machine) is derived that  unlike the original  involves no approximation steps and descends a well-defined objective function.,Sparse Estimation Using General Likelihoods and

Non-Factorial Priors

David Wipf and Srikantan Nagarajan  (cid:3)
Biomagnetic Imaging Lab  UC San Francisco
fdavid.wipf  srig@mrsc.ucsf.edu

Abstract

Finding maximally sparse representations from overcomplete feature dictionaries
frequently involves minimizing a cost function composed of a likelihood (or data
(cid:2)t) term and a prior (or penalty function) that favors sparsity. While typically the
prior is factorial  here we examine non-factorial alternatives that have a number of
desirable properties relevant to sparse estimation and are easily implemented using
an ef(cid:2)cient and globally-convergent  reweighted ‘1-norm minimization procedure.
The (cid:2)rst method under consideration arises from the sparse Bayesian learning
(SBL) framework. Although based on a highly non-convex underlying cost func-
tion  in the context of canonical sparse estimation problems  we prove uniform
superiority of this method over the Lasso in that  (i) it can never do worse  and (ii)
for any dictionary and sparsity pro(cid:2)le  there will always exist cases where it does
better. These results challenge the prevailing reliance on strictly convex penalty
functions for (cid:2)nding sparse solutions. We then derive a new non-factorial variant
with similar properties that exhibits further performance improvements in some
empirical tests. For both of these methods  as well as traditional factorial analogs 
we demonstrate the effectiveness of reweighted ‘1-norm algorithms in handling
more general sparse estimation problems involving classi(cid:2)cation  group feature
selection  and non-negativity constraints. As a byproduct of this development  a
rigorous reformulation of sparse Bayesian classi(cid:2)cation (e.g.  the relevance vector
machine) is derived that  unlike the original  involves no approximation steps and
descends a well-de(cid:2)ned objective function.

1 Introduction
With the advent of compressive sensing and other related applications  there has been growing inter-
est in (cid:2)nding sparse signal representations from redundant dictionaries [3  5]. The canonical form
of this problem is given by 

(1)
where (cid:8) 2 Rn(cid:2)m is a matrix whose columns (cid:30)i represent an overcomplete or redundant basis (i.e. 
rank((cid:8)) = n and m > n)  x 2 Rm is a vector of unknown coef(cid:2)cients to be learned  and y is the
signal vector. The cost function being minimized represents the ‘0 norm of x (i.e.  a count of the
number of nonzero elements in x). If measurement noise or modeling errors are present  we instead
solve the alternative problem

s.t. y = (cid:8)x;

x

min

kxk0;

min

x

ky (cid:0) (cid:8)xk2

2 + (cid:21)kxk0;

(cid:21) > 0;

(2)

noting that in the limit as (cid:21) ! 0  the two problems are equivalent (the limit must be taken outside
of the minimization). From a Bayesian perspective  optimization of either problem can be viewed 
after a exp[(cid:0)((cid:1))] transformation  as a challenging MAP estimation task with a quadratic likelihood
function and a prior that is both improper and discontinuous. Unfortunately  an exhaustive search

for the optimal representation requires the solution of up to (cid:0)m

(cid:3)This research was supported by NIH grants R01DC04855 and R01DC006435.

n(cid:1) linear systems of size n (cid:2) n  a

prohibitively expensive procedure for even modest values of m and n. Consequently  in practical
situations there is a need for approximate methods that ef(cid:2)ciently solve (1) or (2) with high prob-
ability. Moreover  we would ideally like these methods to generalize to other likelihood functions
and priors for applications such as non-negative sparse coding  classi(cid:2)cation  and group variable
selection.
One common strategy is to replace kxk0 with a more manageable penalty function g(x) (or prior)
that still favors sparsity. Typically this replacement is a concave  non-decreasing function of

jxj   [jx1j; : : : ; jxmj]T . It is also generally assumed to be factorial  meaning g(x) = Pi g(xi).

Given this selection  a recent  very successful optimization technique involves iterative reweighted
‘1 minimization  a process that produces more focal estimates with each passing iteration [3  19].
To implement this procedure  at the (k + 1)-th iteration we compute
w(k)

x(k+1) ! arg min

ky (cid:0) (cid:8)xk2

(3)

jxij;

x

2 + (cid:21)Xi

i

i

i

i

)=@jx(k)

  @g(x(k)

where w(k)
j. As discussed in [6]  these updates are guaranteed to converge to
a local minimum of the underlying cost function by satisfying the conditions of the Global Con-
vergence Theorem (see for example [24]). Moreover  empirical evidence from [3] suggests that
generally only a few iterations  which can be readily computed using standard convex programming
packages  are required. Note that a single iteration with unit weights is equivalent to the traditional
Lasso estimator [14]. However  given an appropriate selection for g((cid:1))  e.g.  g(xi) = log(jxij + (cid:11))
with (cid:11) > 0  subsequent iterations have been shown to exhibit substantial improvements over the
Lasso in approximating the solution of (1) or (2) [3].
While certainly successful in practice  there remain fundamental limitations as to what can be
achieved using factorial penalties to approximate kxk0. Perhaps counterintuitively  it has been
shown in [19] that by considering the wider class of non-factorial penalties  more effective sur-
rogates for kxk0 can be obtained  potentially leading to better approximate solutions of either
(1) or (2). In this paper we consider two non-factorial methods that rely on the same basic itera-
tive reweighted ‘1 minimization procedure outlined above. In Section 2  we brie(cid:3)y introduce the
non-factorial penalty function (cid:2)rst proposed in [19] (based on a dual-form interpretation of sparse
Bayesian learning) and then derive a new iterative reweighted ‘1 implementation that builds upon
these ideas. We then demonstrate that this algorithm satis(cid:2)es two desirable properties pertaining to
problem (1): (i) each iteration can only improve the sparsity and  (ii) for any (cid:8) and sparsity pro(cid:2)le 
there will always exist cases where performance improves over standard ‘1 minimization  which
represents the best convex approximation to (1). Together  these results imply that this reweighting
scheme can never do worse than Lasso (assuming w(0)
i = 1; 8i)  and that there will always be cases
where improvement over Lasso is achieved. To a large extent  this removes much of the stigma com-
monly associated with using non-convex sparsity penalties. Later in Section 3  we derive a second
promising non-factorial variant by starting with a plausible ‘1 reweighting scheme and then working
backwards to determine the form and properties of the underlying penalty function.
In general  iterative reweighted ‘1 procedures of any kind are attractive for our purposes because
they can easily be augmented to handle other likelihoods and priors  provided convexity of the
update (3) is preserved (of course the overall cost function being minimized will be non-convex).
For example  to address the extensions mentioned above  in Section 4 we explore adding constraints
such as xi (cid:21) 0  replacing jxij with a norm on groups of variables  and using a logistic instead of
quadratic likelihood term for classi(cid:2)cation. The latter extension leads to a rigorous reformulation
of sparse Bayesian classi(cid:2)cation (e.g.  the relevance vector machine [15]) that  unlike the original 
involves no approximation steps and descends a well-de(cid:2)ned objective function. Finally  Section 5
contains empirical comparisons while Section 6 provides brief concluding remarks.

2 Non-Factorial Methods Based on Sparse Bayesian Learning

A particularly useful non-factorial penalty emerges from a dual-space view [19] of sparse Bayesian
learning (SBL) [15]  which is based on the notion of automatic relevance determination (ARD)
[10]. SBL assumes a Gaussian likelihood function p(yjx) = N (y; (cid:8)x; (cid:21)I)  consistent with the
data (cid:2)t term from (2). The basic ARD prior incorporated by SBL is p(x; (cid:13)) = N (x; 0; diag[(cid:13)]) 
where (cid:13) 2 Rm
+ is a vector of m non-negative hyperparameters governing the prior variance of each

(4)

unknown coef(cid:2)cient. These hyperparameters are estimated from the data by (cid:2)rst marginalizing over
the coef(cid:2)cients x and then performing what is commonly referred to as evidence maximization or
type-II maximum likelihood [10  15]. Mathematically  this is equivalent to minimizing

L((cid:13))   (cid:0) logZ p(yjx)p(x; (cid:13))dx = (cid:0) log p(y; (cid:13)) (cid:17) log j(cid:6)yj + yT (cid:6)(cid:0)1

y y;

where (cid:6)y   (cid:21)I + (cid:8)(cid:0)(cid:8)T and (cid:0)   diag[(cid:13)]. Once some (cid:13)(cid:3) = arg min(cid:13) L((cid:13)) is computed  an
estimate of the unknown coef(cid:2)cients can be obtained by setting xSBL to the posterior mean computed
using (cid:13)(cid:3):

xSBL = E[xjy; (cid:13)(cid:3)] = (cid:0)(cid:3)(cid:8)T (cid:6)(cid:0)1

(5)
Note that if any (cid:13)(cid:3);i = 0  as often occurs during the learning process  then xSBL;i = 0 and the
corresponding feature is effectively pruned from the model. The resulting coef(cid:2)cient vector xSBL is
therefore sparse  with nonzero elements corresponding with the ‘relevant’ features.
It is not immediately apparent how the SBL procedure  which requires optimizing a cost function in
(cid:13)-space and is based on a factorial prior p(x; (cid:13))  relates to solving/approximating (1) and/or (2) via
a non-factorial penalty in x-space. However  it has been shown in [19] that xSBL satis(cid:2)es

y(cid:3) y:

where

xSBL = arg min

x

ky (cid:0) (cid:8)xk2

2 + (cid:21)gSBL(x);

gSBL(x)   min
(cid:13)(cid:21)0

xT (cid:0)(cid:0)1x + log j(cid:11)I + (cid:8)(cid:0)(cid:8)T j;

(6)

(7)

assuming (cid:11) = (cid:21) and jxj   [jx1j; : : : ; jxmj]T . While not discussed in [19]  gSBL(x) is a general
penalty function that only need have (cid:11) = (cid:21) to obtain equivalence with SBL; other selections may
lead to better performance (more on this in Section 4 below).
The analysis in [19] reveals that replacing kxk0 with gSBL(x) and (cid:11) ! 0 leaves the globally mini-
mizing solution to (1) unchanged but drastically reduces the number of local minima (more so than
any possible factorial penalty function). While space precludes the details here  these ideas can be
extended signi(cid:2)cantly to form conditions  which again are only satis(cid:2)able by a non-factorial penalty 
whereby all local minima are smoothed away [21]. Note that while basic ‘1-norm minimization also
has no local minima  the global minimum need not always correspond with the global solution to
(1)  unlike when using gSBL(x).
It can also be shown that gSBL(x) is a non-decreasing  concave function of jxj (see Appendix) 
a desirable property of sparsity-promoting penalties. Importantly  as a direct consequence of this
concavity  (6) can be optimized using a reweighted ‘1 algorithm (in an analogous fashion to the
factorial case) using

(8)
Although this quantity is not available in closed form (except for the special case where (cid:11) ! 0) 
it can be estimated by executing: Step I - Initialize by setting w(k+1) ! w(k)  the k-th vector of
weights  Step II - Repeat until convergence

w(k+1)

=

i

:

@jxij

@gSBL(x)

(cid:12)(cid:12)(cid:12)(cid:12)x=x(k+1)
i (cid:16)(cid:11)I + (cid:8)fW (k+1)eX (k+1)(cid:8)T(cid:17)(cid:0)1

2

(cid:30)i(cid:21) 1

w(k+1)

i

!(cid:20)(cid:30)T

;

(9)

Appendix  while further details and analyses are deferred to [20]. Note that cost function descent
is guaranteed with only a single iteration  so we need not execute (9) until convergence. In fact  it
can be shown that a more rudimentary form of reweighted ‘1 applied to this model in [19] amounts
to performing exactly one such iteration. However  repeated execution of (9) is cheap computation-

wherefW (k+1)   diag[w(k+1)](cid:0)1 and eX (k+1)   diag[jx(k+1)j]. The derivation is shown in the
ally since it scales as O(cid:0)nmkx(k+1)k0(cid:1)  where typically kx(k+1)k0 (cid:20) n  and is substantially less

intensive than the subsequent ‘1 step given by (3).
From a theoretical standpoint  ‘1 reweighting applied to gSBL(x) is guaranteed to aid performance
in the sense described by the following two results  which apply in the case where (cid:21) ! 0; (cid:11) ! 0.
Before proceeding  we de(cid:2)ne spark((cid:8)) as the smallest number of linearly dependent columns in (cid:8)
[5]. It follows then that 2 (cid:20) spark((cid:8)) (cid:20) n + 1.

Theorem 1. When applying iterative reweighted ‘1 using (9) and w(1)
satis(cid:2)es kx(k+1)k0 (cid:20) kx(k)k0 (i.e.  continued iteration can never do worse).

i

6= 0; 8i  the solution sparsity

Theorem 2. Assume that spark((cid:8)) = n+1 and consider any instance where standard ‘1 minimiza-
. Then there exists
tion fails to (cid:2)nd some x(cid:3) drawn from support set S with cardinality jSj < (n+1)
a set of signals y (with non-zero measure) generated from S such that non-factorial reweighted ‘1 

2

withfW (k+1) updated using (9)  always succeeds but standard ‘1 always fails.

Note that Theorem 2 does not in any way indicate what is the best non-factorial reweighting scheme
in practice (for example  in our limited experience with empirical simulations  the selection (cid:11) ! 0
is not necessarily always optimal). However  it does suggest that reweighting with non-convex  non-
factorial penalties is potentially very effective  motivating other selections as discussed next. Taken
together  Theorems 1 and 2 challenge the prevailing reliance on strictly convex cost functions  since
they ensure that we can never do worse than the Lasso (which uses the tightest convex approximation
to the ‘0 norm)  and that there will always be cases where improvement over the Lasso is obtained.

3 Bottom-Up Construction of Non-Factorial Penalty

In the previous section  we described what amounts to a top-down formulation of a non-factorial
penalty function that emerges from a particular hierarchical Bayesian model. Based on the insights
gleaned from this procedure (and its distinction from factorial penalties)  it is possible to stipulate
alternative penalty functions from the bottom up by creating plausible  non-factorial reweighting
schemes. The following is one such possibility.
Assume for simplicity that (cid:21) ! 0. The Achilles heel of standard  factorial penalties is that if we
want to retain a global minimum similar to that of (1)  we require a highly concave penalty on each
xi [21]. However  this implies that almost all basic feasible solutions (BFS) to y = (cid:8)x  de(cid:2)ned as
a solution with kxk0 (cid:20) n  will form local minima of the penalty function constrained to the feasible

region. This is a very undesirable property since there are on the order of(cid:0)m

which is equal to the signal dimension and not very sparse. We would really like to (cid:2)nd degenerate
BFS  where kxk0 is strictly less than n. Such solutions are exceedingly rare and dif(cid:2)cult to (cid:2)nd.
Consequently we would like to utilize a non-factorial  yet highly concave penalty that explicitly
favors degenerate BFS. We can accomplish this by constructing a reweighting scheme designed to
avoid non-degenerate BFS whenever possible.

n(cid:1) BFS with kxk0 = n 

construct weights using the projection of each basis vector (cid:30)i as de(cid:2)ned via

Now consider the covariance-like quantity (cid:11)I + (cid:8)(eX (k+1))2(cid:8)T   where (cid:11) may be small  and then

(10)

w(k+1)

i

! (cid:30)T

i (cid:16)(cid:11)I + (cid:8)(eX (k+1))2(cid:8)T(cid:17)(cid:0)1

(cid:30)i:

i

Ideally  if at iteration k + 1 we are at a bad or non-degenerate BFS  we do not want the newly
computed w(k+1)
to favor the present position at the next iteration of (3) by assigning overly large

rank and so all weights will be relatively modest sized. In contrast  if a rare  degenerate BFS is

weights to the zero-valued xi. In such a situation  the factor (cid:8)(eX (k+1))2(cid:8)T in (10) will be full
found  then (cid:8)(eX (k+1))2(cid:8)T will no longer be full rank  and the weights associated with zero-valued

coef(cid:2)cients will be set to large values  meaning this solution will be favored in the next iteration.
In some sense  the distinction between (10) and its factorial counterparts  such as the method of
Cand(cid:30)es et al. [3] which uses w(k+1)
j+(cid:11))  can be summarized as follows: the factorial
methods assign the largest weight whenever the associated coef(cid:2)cient goes to zero; with (10) the
largest weight is only assigned when the associated coef(cid:2)cient goes to zero and kx(k+1)k0 < n.
The reweighting option (10)  which bears some resemblance to (9)  also has some very desirable
properties beyond the intuitive justi(cid:2)cation given above. First  since we are utilizing (10) in the
context of reweighted ‘1 minimization  it would productive to know what cost function  if any  we
are minimizing when we compute each iteration. Using the fundamental theorem of calculus for line
integrals (or the gradient theorem)  it follows that the bottom-up (BU) penalty function associated

! 1=(jx(k+1)

i

i

0

with (10) is

gBU(x)  Z 1

trace(cid:20)eX(cid:8)T(cid:16)(cid:11)I + (cid:8)((cid:23)eX)2(cid:8)T(cid:17)(cid:0)1

(11)
Moreover  because each weight wi is a non-increasing function of each xj; 8j  from Kachurovskii’s
theorem [12] it directly follows that (11) is concave and non-decreasing in jxj  and thus naturally
promotes sparsity. Additionally  for (cid:11) suf(cid:2)ciently small  it can be shown that the global minimum
of (11) on the constraint y = (cid:8)x must occur at a degenerate BFS (Theorem 1 from above also holds
when using (10); Theorem 2 may as well  although we have not formally shown this). And (cid:2)nally 
regarding implementational issues and interpretability  (10) avoids any recursive weight assignments
or inner-loop optimization as when using (9).

(cid:8)(cid:21) d(cid:23):

4 Extensions
One of the motivating factors for using iterative reweighted ‘1 optimization is that it is very easy to
incorporate alternative likelihoods and priors. This section addresses three such examples.
Non-Negative Sparse Coding: Numerous applications require sparse solutions where all coef(cid:2)cients
xi are constrained to be non-negative [2]. By adding the contraint x (cid:21) 0 to (3) at each iteration  we
can easily compute such solutions using gSBL(x)  gBU(x)  or any other appropriate penalty function.
Note that in the original SBL formulation  this is not a possibility since the integrals required to
compute the associated cost function or update rules no longer have closed-form expressions.
Group Feature Selection: Another common generalization is to seek sparsity at the level of groups
of features  e.g.  the group Lasso [23]. The simultaneous sparse approximation problem [17] is a par-
ticularly useful adaptation of this idea relevant to compressive sensing [18]  manifold learning [13] 
and neuroimaging [22]. In this situation  we are presented with r signals Y   [y(cid:1)1; y(cid:1)2; : : : ; y(cid:1)r]
that were produced by coef(cid:2)cient vectors X   [x(cid:1)1; x(cid:1)2; : : : ; x(cid:1)r] characterized by the same spar-
sity pro(cid:2)le or support  meaning that the coef(cid:2)cient matrix X is row sparse. Here we adopt the
notation that x(cid:1)j represents the j-th column of X while xi(cid:1) represents the i-th row of X. The sparse
recovery problems (1) and (2) then become

d(X); s.t. Y = (cid:8)X;

and

min
X

kY (cid:0) (cid:8)Xk2

F + (cid:21)d(X); (cid:21) > 0;

(12)

min
X

where d(X)  Pm

i=1 I [kxi(cid:1)k > 0] and I[(cid:1)] is an indicator function. d(X) favors row sparsity and

is a natural extension of the ‘0 norm to the simultaneous approximation problem.
As before  the combinatorial nature of each optimization problem renders them intractable and so
approximate procedures are required. All of the algorithms discussed herein can naturally be ex-
panded to this domain essentially by substituting the scalar coef(cid:2)cient magnitudes from a given
iteration jx(k)
j with some row-vector penalty  such as a norm. If we utilize kxi(cid:1)k2  then the co-
ef(cid:2)cient matrix update analogous to (3) requires the solution of the more complicated weighted
second-order cone (SOC) program

i

X (k+1) ! arg min
X

kY (cid:0) (cid:8)Xk2

w(k)

i kxi(cid:1)k2:

(13)

Other selections such as the ‘1 norm are possible as well  providing added generality.
Sparse Classiﬁer Design: At a high level  sparse classi(cid:2)ers can be trained by simply substituting
a (preferrably) convex likelihood function for the quadratic term in (2). For example  to perform
sparse logistic regression we would solve

F + (cid:21)Xi

min

x Xj (cid:2)yj log((cid:30)T

j(cid:1)x) + (1 (cid:0) yj) log(1 (cid:0) (cid:30)T

(14)

j(cid:1)x)(cid:3) + (cid:21)g(x);

where now yj 2 f0; 1g and g(x) is an arbitrary  concave-in-jxj penalty. This can be implemented
by iteratively solving an ‘1-norm penalized logistic regression problem  which can be ef(cid:2)ciently ac-
complished using a simple majorization-maximization approach [7]. Note that cost function descent
does not require that we compute the full reweighted ‘1 solution; the iterations from [7] naturally
lend themselves to an ef(cid:2)cient partial (or greedy) update before recomputing the weights.
It is very insightful to compare this methodology with the original SBL (or relevance vector ma-
chine) classi(cid:2)er derived in [15]. When the Gaussian likelihood p(yjx) is replaced with a Bernoulli

distribution (which leads to the logistic data (cid:2)t term above)  it is no longer possible to compute
the marginalization (4) or the posterior distribution p(xjy; (cid:13))  which is used both for optimization
purposes and to make predictive statements about test data. Consequently  a heuristic Laplace ap-
proximation is adopted  which requires a second-order Newton inner-loop to (cid:2)t a Gaussian about
the mode of p(xjy; (cid:13)). This Gaussian is then used to transform the classi(cid:2)cation problem into a
standard regression one with data-dependent (herteroscedastic) noise  and then whatever approach
is used to minimize (4)  either the MacKay update rules [15] or a greedy constructive method [16] 
can be used in the outer-loop. When (if) a (cid:2)xed point (cid:13)(cid:3) is reached  the corresponding classi(cid:2)er
coef(cid:2)cients are chosen as the mode of p(xjy; (cid:13)(cid:3)).
While demonstrably effective in a wide variety of empirical classi(cid:2)cation tests  the problem with
this formulation of SBL is threefold. First  there are no convergence guarantees of any kind  regard-
less of which method is used for the outer-loop. Secondly  it is completely unclear what  if any  cost
function is being descended (even approximately) to obtain the classi(cid:2)er coef(cid:2)cients  making it dif-
(cid:2)cult to explore the model for enhancements or analytical purposes. Thirdly  in certain applications
it has been observed that SBL achieves extreme sparsity at the expense of classi(cid:2)cation accuracy
[4  11]. There is currently no (cid:3)exibility in the model to remedy this problem.
These issues are directly addressed by dispensing with the Bayesian hierarchical derivation of SBL
altogether and considering classi(cid:2)cation in light of (14). Both the MacKay and greedy SBL updates
are equivalent to minimizing (14) with g(x) = gSBL(x)  and assuming (cid:11) = (cid:21) = 1  using coordinate
descent over a set of auxiliary functions (details provided in a forthcoming paper). Unfortunately
however  because these auxiliary functions are based in part on a second-order Laplace approxima-
tion  they do not form a strict upper bound and so provable convergence (or even descent) is not
possible. Of course we can always substitute the reweighted ‘1 scheme discussed above to avoid
this issue  since the underlying cost function in x-space is the same. Perhaps more importantly  to
properly regulate sparsity  when we deviate from the original Bayesian inspiration for this model 
we are free to adjust (cid:11) and/or (cid:21). For example  with (cid:11) small  the penalty gSBL(x) is more highly
concave favoring sparsity  while in the limit at (cid:11) becomes large  it acts like a standard ‘1 norm  still
favoring sparsity but not exceedingly so (the same phenomena occurs when using the penalty (11)).
Likewise  (cid:21) is as a natural trade-off parameter balancing the contribution from the two terms in (6)
or (14). Both (cid:11) and (cid:21) can be tuned via cross-validation if desired.
There is one additional concern regarding SBL that involves marginal likelihood (sometimes called
evidence) calculations. In the standard regression case where marginalization was possible  the op-
timized quantity (cid:0) log p(y; (cid:13)) represents an approximation to (cid:0) log p(y) that can be used  among
other things  for model comparison. This notion is completely lost when we move to the classi(cid:2)-
cation case under consideration. While space precludes the details  if we are willing to substitute
a probit likelihood function for the logistic  it is possible to revert (14) back to the original hierar-
chical  (cid:13)-dependent Bayesian model and obtain a rigorous upper bound on (cid:0) log p(y; (cid:13)). Finally 
detailed empirical simulations with both logistic- and probit-based classi(cid:2)ers is an area of future
research; preliminary results are promising.

5 Empirical Comparisons
To further examine the algorithms discussed herein  we performed simulations similar to those in [3].
In the (cid:2)rst experiment  each trial consisted of generating a 100 (cid:2) 256 dictionary (cid:8) with iid Gaussian
entries and a sparse vector x(cid:3) with 60 nonzero  non-negative (truncated Gaussian) coef(cid:2)cients.
A signal is then computed using y = (cid:8)x(cid:3). We then attempted to recover x(cid:3) by applying non-
negative ‘1 reweighting strategies with four different penalty functions: (i) gSBL(x) implemented
using a single iteration of (9)  referred to as SBL-I (equivalent to the method from [19]); (ii) gSBL(x)
implemented using multiple iterations of (9) as discussed in Section 2  referred to as SBL-II; (iii)

gBU(x); and (cid:2)nally (iv) g(x) = Pi log(jxij + (cid:11))  the factorial method of Cand(cid:30)es et al.  which

represents the current state-of-the-art in reweighted ‘1 algorithms. In all cases (cid:11) was chosen via
coarse cross-validation. Additionally  since we are working with a noise-free signal  we assume
(cid:21) ! 0 and so the requisite coef(cid:2)cient update (3) with xi (cid:21) 0 reduces to a standard linear program.
i = 1; 8i for each algorithm  the (cid:2)rst iteration amounts to the non-negative minimum
Given w(0)
‘1-norm solution (i.e.  the Lasso). Average results from 1000 random trials are displayed in Figure
1 (left)  which plots the empirical probability of success in recovering x(cid:3) versus the iteration num-
ber. We observe that standard non-negative ‘1 never succeeds (see (cid:2)rst iteration results); however 

with only a few reweighted iterations drastic improvement is possible  especially for the bottom-up
approach. By 10 iterations  the non-factorial variants have all exceeded the method of Cand(cid:30)es et al.
(There was no appreciable improvement by any method after 10 iterations.) This shows both the
ef(cid:2)cacy of non-factorial reweighting and the ability to handle constraints on x.
For the second experiment  we used a randomly generated 50 (cid:2) 100 dictionary for each trial with
iid Gaussian entries as above  and created 5 coef(cid:2)cient vectors X (cid:3) = [x(cid:3)
(cid:1)5] with matching
sparsity pro(cid:2)le and iid Gaussian nonzero coef(cid:2)cients. We then generate the signal matrix Y = (cid:8)X (cid:3)
and attempt to learn X (cid:3) using various group-level reweighting schemes. In this experiment we var-
ied the row sparsity of X (cid:3) from d(X (cid:3)) = 30 to d(X (cid:3)) = 40; in general  the more nonzero rows 
the harder the recovery problem becomes. A total of (cid:2)ve algorithms modi(cid:2)ed to the simultaneous
sparse approximation problem were tested using an ‘2-norm penalty on each coef(cid:2)cient row: the
four methods from above (executed for 5 iterations each) plus the standard group Lasso (equiva-
lent to a single iteration of any of the other algorithms). Results are presented in Figure 1 (right) 
where the performance gap between the factorial and non-factorial approaches is very signi(cid:2)cant.
Additionally  we have successfully applied this methodology to large neuroimaging data sets [22] 
obtaining signi(cid:2)cant improvements over existing convex approaches such as the group Lasso  con-
sistent with the results in Figure 1. Other related simulation results are contained in [20].

(cid:1)1; :::; x(cid:3)

)
s
s
e
c
c
u
s
(
p

PSfrag replacements

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

row sparsity  d(X (cid:3))

 

0
1

2

3

 

)
s
s
e
c
c
u
s
(
p

SBL−I
SBL−II
Bottom−Up
Candes et al.

PSfrag replacements
‘1 iteration number

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

SBL−I
SBL−II
Bottom−Up
Candes et al.
Group Lasso

4

5

6

7

‘1 iteration number

8

9

10

 

0
30

32

34

36

row sparsity  d(X (cid:3))

 

38

40

Figure 1: Left: Probability of success recovering sparse non-negative coef(cid:2)cients as a function of
reweighted ‘1 iterations. Right: Iterative reweighted results using 5 simultaneous signal vectors.
Probability of success recovering sparse coef(cid:2)cients for different row sparsity values  i.e.  d(X (cid:3)).

6 Conclusion
In this paper we have examined concave  non-factorial priors (which previously have received little
attention) for the purpose of estimating sparse coef(cid:2)cients. When coupled with general likelihood
models and minimized using ef(cid:2)cient iterative reweighted ‘1 methods  these priors offer a powerful
alternative to existing state-of-the-art sparse estimation techniques. We have also shown (for the (cid:2)rst
time) exactly what the underlying cost function associated with the SBL classi(cid:2)er is and provided a
more principled algorithm for minimizing it.

Appendix
Concavity of gSBL(x) and derivation of weight updates (9): Because log j(cid:11)I + (cid:8)(cid:0)(cid:8)T j is concave
and non-decreasing with respect to (cid:13) (cid:21) 0  we can express it as

log j(cid:11)I + (cid:8)(cid:0)(cid:8)T j = min
z(cid:21)0

zT (cid:13) (cid:0) h(cid:3)(z);

(15)

where h(cid:3)(z) is de(cid:2)ned as the concave conjugate of h((cid:13))   log j(cid:11)I + (cid:8)(cid:0)(cid:8)T j [1]. We can then
express gSBL(x) via

gSBL(x) = min
(cid:13)(cid:21)0

xT (cid:0)(cid:0)1x + log j(cid:11)I + (cid:8)(cid:0)(cid:8)T j = min

Minimizing over (cid:13) for (cid:2)xed x and z  we get

(cid:13)i = z(cid:0)1=2

i

jxij; 8i:

(cid:13);z(cid:21)0Xi (cid:18) x2

i
(cid:13)i

+ zi(cid:13)i(cid:19) (cid:0) h(cid:3)(z):

(16)

(17)

Substituting this expression into (16) gives the representation

gSBL(x) = min

z(cid:21)0Xi   x2

i
z(cid:0)1=2
i

+ ziz(cid:0)1=2

i

jxij

jxij! (cid:0) h(cid:3)(z) = min
z(cid:21)0Xi

2z1=2

i

jxij (cid:0) h(cid:3)(z);

(18)

which implies that gSBL(x) can be represented as a minimum of upper-bounding hyperplanes with
respect to jxj  and thus must be concave and non-decreasing since z (cid:21) 0 [1]. We also observe that
for (cid:2)xed z  solving (6) is a weighted ‘1 minimization problem.
To derive the weight update (9)  we only need the optimal value of each zi  which from basic convex
analysis will satisfy

(19)
Since this quantity is not available in closed form  we can instead iteratively minimize (16) over
(cid:13) and z. We start by initializing z1=2
; 8i and then minimize over (cid:13) using (17). We then
compute the optimal z for (cid:2)xed (cid:13)  which can be done analytically using

i ! w(k)

z1=2
i =

@gSBL(x)

2@jxij

:

i

z = O

(cid:13) log(cid:12)(cid:12)(cid:11)I + (cid:8)(cid:0)(cid:8)T(cid:12)(cid:12) = diagh(cid:8)T(cid:0)(cid:11)I + (cid:8)(cid:0)(cid:8)T(cid:1)(cid:0)1

i

i

(cid:8)i :

By substituting (17) into (20) and de(cid:2)ning w(k+1)
  we obtain the weight update (9). This
procedure is guaranteed to converge to a solution satisfying (19) [20] although  as mentioned
previously  only one iteration is actually required for the overall algorithm.
(cid:4)

  z1=2

(20)

i

Proof of Theorem 1: Before we begin  we should point out that for (cid:11) ! 0  the weight update

(9) is still well-speci(cid:2)ed regardless of the value of the diagonal matrix fW (k+1)eX (k+1). If (cid:30)i is
not in the span of (cid:8)fW (k+1)eX (k+1)(cid:8)T   then w(k+1)

can be set to zero for all future iterations. Otherwise w(k+1)
Moore-Penrose pseudoinverse and will be strictly nonzero.
For simplicity we will now assume that spark((cid:8)) = n + 1  which is equivalent to requiring that
each subset of n columns of (cid:8) forms a basis in Rn. The extension to the more general case is
discussed in [20]. From basic linear programming [8]  at any iteration the coef(cid:2)cients will satisfy

! 1 and the corresponding coef(cid:2)cient xi
can be computed ef(cid:2)ciently using the

two possibilities. If kx(k)k0 = n  then we will automatically satisfy kx(k+1)k0 (cid:20) kx(k)k0 at the

kx(k)k0 (cid:20) n for arbitrary weightsfW (k(cid:0)1). Given our simplifying assumptions  there exists only
next iteration regardless offW (k). In contrast  if kx(k)k0 < n  then rankhfW (k)i (cid:20) kx(k)k0 for all

evaluations of (9) with (cid:11) ! 0  enforcing kx(k+1)k0 (cid:20) kx(k)k0.

(cid:4)

i

x

2

(i+1) (cid:20) (cid:23)ix0

Proof of Theorem 2: For a (cid:2)xed dictionary (cid:8) and coef(cid:2)cient vector x(cid:3)  we are assuming that
. Now consider a second coef(cid:2)cient vector x0 with support and sign pattern equal to
kx(cid:3)k0 < (n+1)
x(cid:3) and de(cid:2)ne x0
(i) as the i-th largest coef(cid:2)cient magnitude of x0. Then there exists a set of kx(cid:3)k0 (cid:0)1
scaling constants (cid:23)i 2 (0; 1] (i.e.  strictly greater than zero) such that  for any signal y generated via
y = (cid:8)x0 and x0

(i)  i = 1; : : : ; kx(cid:3)k0 (cid:0) 1  the minimization problem
^x   arg min

gSBL(x);

s.t. (cid:8)x0 = (cid:8)x; (cid:11) ! 0;

(21)
is unimodal and has a unique minimizing stationary point which satis(cid:2)es ^x = x0. This result
follows from [21] and the dual-space characterization of the penalty gSBL(x) from [19]. Note that
(21) is equivalent to (6) with (cid:21) ! 0  so the reweighted non-factorial update (9) can be applied.
Furthermore  based on the global convergence of these updates discussed above  the sequence of
estimates are guaranteed to satisfy x(k) ! ^x = x0. So we will necessarily learn the generative x0.
Let x‘1   arg minx kxk1  subject to (cid:8)x(cid:3) = (cid:8)x. By assumption we know that x‘1 6= x(cid:3).
Moreover  we can conclude using [9  Theorem 6] that if x‘1 fails for some x(cid:3)  it will fail for any
other x with matching support and sign pattern; it will therefore fail for any x0 as de(cid:2)ned above.
Finally  by construction  the set of feasible x0 will have nonzero measure over the support S since
each (cid:23)i is strictly nonzero. Note also that this result can likely be extended to the case where
spark((cid:8)) < n + 1 and to any x(cid:3) that satis(cid:2)es kx(cid:3)k0 < spark((cid:8)) (cid:0) 1. The more speci(cid:2)c case
addressed above was only assumed to allow direct application of [9  Theorem 6].
(cid:4)

References
[1] S. Boyd and L. Vandenberghe  Convex Optimization  Cambridge University Press  2004.
[2] A. Bruckstein  M. Elad  and M. Zibulevsky  (cid:147)A non-negative and sparse enough solution of an
underdetermined linear system of equations is unique (cid:148) IEEE Trans. Information Theory  vol.
54  no. 11  pp. 4813(cid:150)4820  Nov. 2008.

[3] E. Cand(cid:30)es  M. Wakin  and S. Boyd  (cid:147)Enhancing sparsity by reweighted ‘1 minimization (cid:148) J.

Fourier Anal. Appl.  vol. 14  no. 5  pp. 877(cid:150)905  2008.

[4] G. Cawley and N. Talbot  (cid:147)Gene selection in cancer classi(cid:2)cation using sparse logistic regres-

sion with Bayesian regularization (cid:148) Bioinformatics  vol. 22  no. 19  pp. 2348(cid:150)2355  2006.

[5] D. Donoho and M. Elad  (cid:147)Optimally sparse representation in general (nonorthogonal) dictio-

naries via ‘1 minimization (cid:148) Proc. Nat. Acad. Sci.  vol. 100  no. 5  pp. 2197(cid:150)2202  2003.

[6] M. Fazel  H. Hindi  and S. Boyd  (cid:147)Log-Det heuristic for matrix rank minimization with appli-
cations to hankel and Euclidean distance matrices (cid:148) Proc. American Control Conf.  vol. 3  pp.
2156(cid:150)2162  June 2003.

[7] B. Krishnapuram  L. Carin  M. Figueiredo  and A. Hartemink  (cid:147)Sparse multinomial logistic
regression: Fast algorithms and generalization bounds (cid:148) IEEE Trans. Pattn Anal. Mach. Intell. 
vol. 27  pp. 957(cid:150)968  2005.

[8] D. Luenberger  Linear and Nonlinear Programming  Addison(cid:150)Wesley  Reading  Massachusetts 

second edition  1984.

[9] D. Malioutov  M. C‚ etin  and A.S. Willsky  (cid:147)Optimal sparse representations in general overcom-

plete bases (cid:148) IEEE Int. Conf. Acoust.  Speech  and Sig. Proc.  vol. 2  pp. II(cid:150)793(cid:150)796  2004.

[10] R. Neal  Bayesian Learning for Neural Networks  Springer-Verlag  New York  1996.
[11] Y. Qi  T. Minka  R. Picard  and Z. Ghahramani  (cid:147)Predictive automatic relevance determination

by expectation propagation (cid:148) Int. Conf. Machine Learning (ICML)  pp. 85(cid:150)92  2004.

[12] R. Showalter  (cid:147)Monotone operators in Banach space and nonlinear partial differential equa-

tions (cid:148) Mathematical Surveys and Monographs 49. AMS  Providence  RI  1997.

[13] J. Silva  J. Marques  and J. Lemos  (cid:147)Selecting landmark points for sparse manifold learning (cid:148)

Advances in Neural Information Processing Systems 18  pp. 1241(cid:150)1248  2006.

[14] R. Tibshirani  (cid:147)Regression shrinkage and selection via the Lasso (cid:148) Journal of the Royal

Statistical Society  vol. 58  no. 1  pp. 267(cid:150)288  1996.

[15] M. Tipping  (cid:147)Sparse bayesian learning and the relevance vector machine (cid:148) J. Machine Learning

Research  vol. 1  pp. 211(cid:150)244  2001.

[16] M. Tipping and A. Faul  (cid:147)Fast marginal likelihood maximisation for sparse Bayesian models (cid:148)

Ninth Int. Workshop. Artiﬁcial Intelligence and Statistics  Jan. 2003.

[17] J. Tropp  (cid:147)Algorithms for simultaneous sparse approximation. Part II: Convex relaxation (cid:148)

Signal Processing  vol. 86  pp. 589(cid:150)602  April 2006.

[18] M. Wakin  M. Duarte  S. Sarvotham  D. Baron  and R. Baraniuk  (cid:147)Recovery of jointly sparse
signals from a few random projections (cid:148) Advances in Neural Information Processing Systems
18  pp. 1433(cid:150)1440  2006.

[19] D. Wipf and S. Nagarajan  (cid:147)A new view of automatic relevance determination (cid:148) Advances in

Neural Information Processing Systems 20  pp. 1625(cid:150)1632  2008.

[20] D. Wipf and S. Nagarajan  (cid:147)Iterative reweighted ‘1 and ‘2 methods for (cid:2)nding sparse solu-

tions (cid:148) Submitted  2009.

[21] D. Wipf and S. Nagarajan  (cid:147)Latent variable Bayesian models for promoting sparsity (cid:148) Submit-

ted  2009.

[22] D. Wipf  J. Owen  H. Attias  K. Sekihara  and S. Nagarajan  (cid:147)Robust Bayesian Estimation of
the Location  Orientation  and Time Course of Multiple Correlated Neural Sources using MEG (cid:148)
NeuroImage  vol. 49  no. 1  pp. 641(cid:150)655  Jan. 2010.

[23] M. Yuan and Y. Lin  (cid:147)Model selection and estimation in regression with grouped variables (cid:148) J.

R. Statist. Soc. B  vol. 68  pp. 49(cid:150)67  2006.

[24] W. Zangwill  Nonlinear Programming: A Uniﬁed Approach  Prentice Hall  New Jersey  1969.

,JUN HAN
Qiang Liu
Omer Ben-Porat
Moshe Tennenholtz