2013,Estimating the Unseen: Improved Estimators for Entropy and other Properties,Recently  [Valiant and Valiant] showed that a class of distributional properties  which includes such practically relevant properties as entropy  the number of distinct elements  and distance metrics between pairs of distributions  can be estimated given a SUBLINEAR sized sample.  Specifically  given a sample consisting of independent draws from any distribution over at most n distinct elements  these properties can be estimated accurately using a sample of size O(n / log n).  We propose a novel modification of this approach and show: 1) theoretically  our estimator is optimal (to constant factors  over worst-case instances)  and 2) in practice  it performs exceptionally well for a variety of estimation tasks  on a variety of natural distributions  for a wide range of parameters.  Perhaps unsurprisingly  the key step in this approach is to first use the sample to characterize the unseen" portion of the distribution.  This goes beyond such tools as the Good-Turing frequency estimation scheme  which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the "shape"of the unobserved portion of the distribution.  This approach is robust  general  and theoretically principled;  we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. ",Estimating the Unseen:

Improved Estimators for Entropy and other

Properties

Gregory Valiant ∗
Stanford University
Stanford  CA 94305

valiant@stanford.edu

Paul Valiant †
Brown University

Providence  RI 02912

pvaliant@gmail.com

Abstract

Recently  Valiant and Valiant [1  2] showed that a class of distributional proper-
ties  which includes such practically relevant properties as entropy  the number
of distinct elements  and distance metrics between pairs of distributions  can be
estimated given a sublinear sized sample. Speciﬁcally  given a sample consisting
of independent draws from any distribution over at most n distinct elements  these
properties can be estimated accurately using a sample of size O(n/ log n). We
propose a novel modiﬁcation of this approach and show: 1) theoretically  this esti-
mator is optimal (to constant factors  over worst-case instances)  and 2) in practice 
it performs exceptionally well for a variety of estimation tasks  on a variety of nat-
ural distributions  for a wide range of parameters. Perhaps unsurprisingly  the key
step in our approach is to ﬁrst use the sample to characterize the “unseen” portion
of the distribution. This goes beyond such tools as the Good-Turing frequency
estimation scheme  which estimates the total probability mass of the unobserved
portion of the distribution: we seek to estimate the shape of the unobserved portion
of the distribution. This approach is robust  general  and theoretically principled;
we expect that it may be fruitfully used as a component within larger machine
learning and data analysis systems.

Introduction

1
What can one infer about an unknown distribution based on a random sample? If the distribution
in question is relatively “simple” in comparison to the sample size—for example if our sample
consists of 1000 independent draws from a distribution supported on 100 domain elements—then
the empirical distribution given by the sample will likely be an accurate representation of the true
distribution. If  on the other hand  we are given a relatively small sample in relation to the size
and complexity of the distribution—for example a sample of size 100 drawn from a distribution
supported on 1000 domain elements—then the empirical distribution may be a poor approximation
of the true distribution. In this case  can one still extract accurate estimates of various properties of
the true distribution?
Many real–world machine learning and data analysis tasks face this challenge; indeed there are
many large datasets where the data only represent a tiny fraction of an underlying distribution we
hope to understand. This challenge of inferring properties of a distribution given a “too small”
sample is encountered in a variety of settings  including text data (typically  no matter how large the
corpus  around 30% of the observed vocabulary only occurs once)  customer data (many customers
or website users are only seen a small number of times)  the analysis of neural spike trains [15] 

∗http://theory.stanford.edu/~valiant/ A portion of this work was done while at Microsoft Research.
†http://cs.brown.edu/people/pvaliant/

1

and the study of genetic mutations across a population1. Additionally  many database management
tasks employ sampling techniques to optimize query execution; improved estimators would allow
for either smaller sample sizes or increased accuracy  leading to improved efﬁciency of the database
system (see  e.g. [6  7]).
We introduce a general and robust approach for using a sample to characterize the “unseen” portion
of the distribution. Without any a priori assumptions about the distribution  one cannot know what
the unseen domain elements are. Nevertheless  one can still hope to estimate the “shape” or his-
togram of the unseen portion of the distribution—essentially  we estimate how many unseen domain
elements occur in various probability ranges. Given such a reconstruction  one can then use it to
estimate any property of the distribution which only depends on the shape/histogram; such prop-
erties are termed symmetric and include entropy and support size. In light of the long history of
work on estimating entropy by the neuroscience  statistics  computer science  and information the-
ory communities  it is compelling that our approach (which is agnostic to the property in question)
outperforms these entropy-speciﬁc estimators.
Additionally  we extend this intuition to develop estimators for properties of pairs of distributions 
the most important of which are the distance metrics. We demonstrate that our approach can ac-
curately estimate the total variational distance (also known as statistical distance or ￿1 distance)
between distributions using small samples. To illustrate the challenge of estimating variational dis-
tance (between distributions over discrete domains) given small samples  consider drawing two sam-
ples  each consisting of 1000 draws from a uniform distribution over 10 000 distinct elements. Each
sample can contain at most 10% of the domain elements  and their intersection will likely contain
only 1% of the domain elements; yet from this  one would like to conclude that these two samples
must have been drawn from nearly identical distributions.
1.1 Previous work: estimating distributions  and estimating properties
There is a long line of work on inferring information about the unseen portion of a distribution 
beginning with independent contributions from both R.A. Fisher and Alan Turing during the 1940’s.
Fisher was presented with data on butterﬂies collected over a 2 year expedition in Malaysia  and
sought to estimate the number of new species that would be discovered if a second 2 year expedition
were conducted [8]. (His answer was “≈ 75.”) At nearly the same time  as part of the British WWII
effort to understand the statistics of the German enigma ciphers  Turing and I.J. Good were working
on the related problem of estimating the total probability mass accounted for by the unseen portion of
a distribution [9]. This resulted in the Good-Turing frequency estimation scheme  which continues
to be employed  analyzed  and extended by our community (see  e.g. [10  11]).
More recently  in similar spirit to this work  Orlitsky et al. posed the following natural question:
given a sample  what distribution maximizes the likelihood of seeing the observed species frequen-
cies  that is  the number of species observed once  twice  etc.? [12  13] (What Orlitsky et al. term
the pattern of a sample  we call the ﬁngerprint  as in Deﬁnition 1.) Orlitsky et al. show that such
likelihood maximizing distributions can be found in some speciﬁc settings  though the problem of
ﬁnding or approximating such distributions for typical patterns/ﬁngerprints may be difﬁcult. Re-
cently  Acharya et al. showed that this maximum likelihood approach can be used to yield a near-
optimal algorithm for deciding whether two samples originated from identical distributions  versus
distributions that have large distance [14].
In contrast to this approach of trying to estimate the “shape/histogram” of a distribution  there has
been nearly a century of work proposing and analyzing estimators for particular properties of distri-
butions. In Section 3 we describe several standard  and some recent estimators for entropy  though
we refer the reader to [15] for a thorough treatment. There is also a large literature on estimating
support size (also known as the “species problem”  and the related “distinct elements” problem)  and
we refer the reader to [16] and to [17] for several hundred references.
Over the past 15 years  the theoretical computer science community has spent signiﬁcant effort
developing estimators and establishing worst-case information theoretic lower bounds on the sample
size required for various distribution estimation tasks  including entropy and support size (e.g. [18 
19  20  21]).

1Three recent studies (appearing in Science last year) found that very rare genetic mutations are especially
abundant in humans  and observed that better statistical tools are needed to characterize this “rare events”
regime  so as to resolve fundamental problems about our evolutionary process and selective pressures [3  4  5].

2

The algorithm we present here is based on the intuition of the estimator described in our theoretical
work [1]. That estimator is not practically viable  and additionally  requires as input an accurate
upper bound on the support size of the distribution in question. Both the algorithm proposed in this
current work and that of [1] employ linear programming  though these programs differ signiﬁcantly
(to the extent that the linear program of [1] does not even have an objective function and simply
deﬁnes a feasible region). Our proof of the theoretical guarantees in this work leverages some of
the machinery of [1] (in particular  the “Chebyshev bump construction”) and achieves the same
theoretical worst-case optimality guarantees. See Appendix A for further theoretical and practical
comparisons with the estimator of [1].

1.2 Deﬁnitions and examples
We begin by deﬁning the ﬁngerprint of a sample  which essentially removes all the label-information
from the sample. For the remainder of this paper  we will work with the ﬁngerprint of a sample 
rather than the with the sample itself.
Deﬁnition 1. Given a samples X = (x1  . . .   xk)  the associated ﬁngerprint  F = (F1 F2  . . .) 
is the “histogram of the histogram” of the sample. Formally  F is the vector whose ith component 
Fi  is the number of elements in the domain that occur exactly i times in sample X.
For estimating entropy  or any other property whose value is invariant to relabeling the distribution
support  the ﬁngerprint of a sample contains all the relevant information (see [21]  for a formal proof
of this fact). We note that in some of the literature  the ﬁngerprint is alternately termed the pattern 
histogram  histogram of the histogram or collision statistics of the sample.
In analogy with the ﬁngerprint of a sample  we deﬁne the histogram of a distribution  a representation
in which the labels of the domain have been removed.
Deﬁnition 2. The histogram of a distribution D is a mapping hD : (0  1] → N∪{ 0}  where hD(x)
is equal to the number of domain elements that each occur in distribution D with probability x.
Formally  hD(x) = |{α : D(α) = x}|  where D(α) is the probability mass that distribution D
assigns to domain element α. We will also allow for “generalized histograms” in which hD does
not necessarily take integral values.
Since h(x) denotes the number of elements that have probability x  we have￿x:h(x)￿=0 x·h(x) = 1 

as the total probability mass of a distribution is 1. Any symmetric property is a function of only the
histogram of the distribution:

• The Shannon entropy H(D) of a distribution D is deﬁned to be

H(D) := − ￿α∈sup(D)

D(α) log2 D(α) = − ￿x:hD(x)￿=0
|sup(D)| := |{α : D(α) > 0}| = ￿x:hD(x)￿=0

hD(x)x log2 x.

hD(x).

• The support size is the number of domain elements that occur with positive probability:

We provide an example to illustrate the above deﬁnitions:
Example 3. Consider a sequence of animals  obtained as a sample from the distribution of animals
on a certain island  X = (mouse  mouse  bird  cat  mouse  bird  bird  mouse  dog  mouse). We
have F = (2  0  1  0  1)  indicating that two species occurred exactly once (cat and dog)  one species
occurred exactly three times (bird)  and one species occurred exactly ﬁve times (mouse).
Consider the following distribution of animals:
P r(mouse) = 1/2  P r(bird) = 1/4  P r(cat) = P r(dog) = P r(bear) = P r(wolf ) = 1/16.
The associated histogram of this distribution is h : (0  1] → Z deﬁned by h(1/16) = 4  h(1/4) = 1 
h(1/2) = 1  and for all x ￿∈ {1/16  1/4  1/2}  h(x) = 0.
As we will see in Example 5 below  the ﬁngerprint of a sample is intimately related to the Binomial
distribution; the theoretical analysis will be greatly simpliﬁed by reasoning about the related Poisson
distribution  which we now deﬁne:
Deﬁnition 4. We denote the Poisson distribution of expectation λ as P oi(λ)  and write poi(λ  j) :=
e−λλj

  to denote the probability that a random variable with distribution P oi(λ) takes value j.

j!

3

Example 5. Let D be the uniform distribution with support size 1000. Then hD(1/1000) = 1000 
and for all x ￿= 1/1000  hD(x) = 0. Let X be a sample consisting of 500 independent draws
from D. Each element of the domain  in expectation  will occur 1/2 times in X  and thus the
number of occurrences of each domain element in the sample X will be roughly distributed as
P oi(1/2). (The exact distribution will be Binomial(500  1/1000)  though the Poisson distribu-
tion is an accurate approximation.) By linearity of expectation  the expected ﬁngerprint satisﬁes
E[Fi] ≈ 1000· poi(1/2  i). Thus we expect to see roughly 303 elements once  76 elements twice  13
elements three times  etc.  and in expectation 607 domain elements will not be seen at all.

2 Estimating the unseen
Given the ﬁngerprint F of a sample of size k  drawn from a distribution with histogram h  our high-
level approach is to ﬁnd a histogram h￿ that has the property that if one were to take k independent
draws from a distribution with histogram h￿  the ﬁngerprint of the resulting sample would be similar
to the observed ﬁngerprint F. The hope is then that h and h￿ will be similar  and  in particular  have
similar entropies  support sizes  etc.
As an illustration of this approach  suppose we are given a sample of size k = 500  with ﬁngerprint
F = (301  78  13  1  0  0  . . .); recalling Example 5  we recognize that F is very similar to the
expected ﬁngerprint that we would obtain if the sample had been drawn from the uniform distribution
over support 1000. Although the sample only contains 391 unique domain elements  we might be
justiﬁed in concluding that the entropy of the true distribution from which the sample was drawn is
close to H(U nif (1000)) = log2(1000).
In general  how does one obtain a “plausible” histogram from a ﬁngerprint in a principled fashion?
We must start by understanding how to obtain a plausible ﬁngerprint from a histogram.
Given a distribution D  and some domain element α occurring with probability x = D(α)  the prob-
ability that it will be drawn exactly i times in k independent draws from D is P r[Binomial(k  x) =
i] ≈ poi(kx  i). By linearity of expectation  the expected ith ﬁngerprint entry will roughly satisfy
(1)

h(x)poi(kx  i).

E[Fi] ≈ ￿x:hD(x)￿=0

This mapping between histograms and expected ﬁngerprints is linear in the histogram  with coefﬁ-
cients given by the Poisson probabilities. Additionally  it is not hard to show that V ar[Fi] ≤ E[Fi] 
and thus the ﬁngerprint is tightly concentrated about its expected value. This motivates a “ﬁrst mo-
ment” approach. We will  roughly  invert the linear map from histograms to expected ﬁngerprint
entries  to yield a map from observed ﬁngerprints  to plausible histograms h￿.
There is one additional component of our approach. For many ﬁngerprints  there will be a large space
of equally plausible histograms. To illustrate  suppose we obtain ﬁngerprint F = (10  0  0  0  . . .) 
and consider the two histograms given by the uniform distributions with respective support sizes
10 000  and 100 000. Given either distribution  the probability of obtaining the observed ﬁngerprint
from a set of 10 samples is > .99  yet these distributions are quite different and have very different
entropy values and support sizes. They are both very plausible–which distribution should we return?
To resolve this issue in a principled fashion  we strengthen our initial goal of “returning a histogram
that could have plausibly generated the observed ﬁngerprint”: we instead return the simplest his-
togram that could have plausibly generated the observed ﬁngerprint. Recall the example above 
where we observed only 10 distinct elements  but to explain the data we could either infer an ad-
ditional 9 900 unseen elements  or an additional 99 000. In this sense  inferring “only” 9 900 addi-
tional unseen elements is the simplest explanation that ﬁts the data  in the spirit of Occam’s razor.2
2.1 The algorithm
We pose this problem of ﬁnding the simplest plausible histogram as a pair of linear programs. The
ﬁrst linear program will return a histogram h￿ that minimizes the distance between its expected ﬁn-
gerprint and the observed ﬁngerprint  where we penalize the discrepancy between Fi and E[F h￿
] in
proportion to the inverse of the standard deviation of Fi  which we estimate as 1/√1 + Fi  since

i

2The practical performance seems virtually unchanged if one returns the “plausible” histogram of minimal

entropy  instead of minimal support size (see Appendix B).

4

Poisson distributions have variance equal to their expectation. The constraint that h￿ corresponds to
a histogram simply means that the total probability mass is 1  and all probability values are nonneg-
ative. The second linear program will then ﬁnd the histogram h￿￿ of minimal support size  subject to
the constraint that the distance between its expected ﬁngerprint  and the observed ﬁngerprint  is not
much worse than that of the histogram found by the ﬁrst linear program.
To make the linear programs ﬁnite  we consider a ﬁne mesh of values x1  . . .   x￿ ∈ (0  1] that be-
tween them discretely approximate the potential support of the histogram. The variables of the linear
program  h￿1  . . .   h￿￿ will correspond to the histogram values at these mesh points  with variable h￿i
representing the number of domain elements that occur with probability xi  namely h￿(xi).
A minor complicating issue is that this approach is designed for the challenging “rare events” regime 
where there are many domain elements each seen only a handful of times. By contrast if there is
a domain element that occurs very frequently  say with probability 1/2  then the number of times
it occurs will be concentrated about its expectation of k/2 (and the trivial empirical estimate will
be accurate)  though ﬁngerprint Fk/2 will not be concentrated about its expectation  as it will take
an integer value of either 0  1 or 2. Hence we will split the ﬁngerprint into the “easy” and “hard”
portions  and use the empirical estimator for the easy portion  and our linear programming approach
for the hard portion. The full algorithm is below (see our websites or Appendix D for Matlab code).

Algorithm 1. ESTIMATE UNSEEN
Input: Fingerprint F = F1 F2  . . .  Fm  derived from a sample of size k 
vector x = x1  . . .   x￿ with 0 < xi ≤ 1  and error parameter α> 0.
Output: List of pairs (y1  h￿y1 )  (y2  h￿y2 )  . . .   with yi ∈ (0  1]  and h￿yi ≥ 0.
• Initialize the output list of pairs to be empty  and initialize a vector F￿ to be equal to F.
• For i = 1 to k 

– If￿j∈{i−￿√i￿ ... i+￿√i￿} Fj ≤ 2√i

Set F￿i = 0  and append the pair (i/k Fi) to the output list.

[i.e. if the ﬁngerprint is “sparse” at index i]

• Let vopt be the objective function value returned by running Linear Program 1 on input F￿  x.
• Let h be the histogram returned by running Linear Program 2 on input F￿  x  vopt α .
• For all i s.t. hi > 0  append the pair (xi  hi) to the output list.

vector x = x1  . . .   x￿ consisting of a ﬁne mesh of points in the interval (0  1].

Linear Program 1. FIND PLAUSIBLE HISTOGRAM
Input: Fingerprint F = F1 F2  . . .  Fm  derived from a sample of size k 
Output: vector h￿ = h￿1  . . .   h￿￿  and objective value vopt ∈ R.
Let h￿1  . . .   h￿￿ and vopt be  respectively  the solution assignment  and corresponding objective function
value of the solution of the following linear program  with variables h￿1  . . .   h￿￿:

Minimize:

m￿i=1
Subject to: ￿￿

1

￿￿j=1

√1 + Fi￿￿￿￿￿Fi −
h￿j · poi(kxj  i)￿￿￿￿￿
j=1 xjh￿j =￿i Fi/k  and ∀j  h￿j ≥ 0.

Linear Program 2. FIND SIMPLEST PLAUSIBLE HISTOGRAM
Input: Fingerprint F = F1 F2  . . .  Fm  derived from a sample of size k 

vector x = x1  . . .   x￿ consisting of a ﬁne mesh of points in the interval (0  1] 
optimal objective function value vopt from Linear Program 1  and error parameter α> 0.

Output: vector h￿ = h￿1  . . .   h￿￿.
Let h￿1  . . .   h￿￿ be the solution assignment of the following linear program  with variables h￿1  . . .   h￿￿:

Minimize: ￿￿

j=1 h￿j

1√1+Fi￿￿￿Fi −￿￿
Subject to: ￿m
￿￿
j=1 xjh￿j =￿i Fi/k  and ∀j  h￿j ≥ 0.

j=1 h￿j · poi(kxj  i)￿￿￿ ≤ vopt+α 

i=1

Theorem 1. There exists a constant C0 > 0 and assignment of parameter α := α(k) of Algorithm 1
such that for any c > 0  for sufﬁciently large n  given a sample of size k = c n
log n consisting of
independent draws from a distribution D over a domain of size at most n  with probability at least
1 − e−nΩ(1) over the randomness in the selection of the sample  Algorithm 13  when run with a
sufﬁciently ﬁne mesh x1  . . .   x￿  returns a histogram h￿ such that |H(D) − H(h￿)|≤ C0√c .

3For simplicity  we prove this statement for Algorithm 1 with the second bullet step of the algorithm modi-
ﬁed as follows: there is an explicit cutoff N such that the linear programming approach is applied to ﬁngerprint
entries Fi for i ≤ N  and the empirical estimate is applied to ﬁngerprints Fi for i > N.

5

The above theorem characterizes the worst-case performance guarantees of the above algorithm in
terms of entropy estimation. The proof of Theorem 1 is rather technical and we provide the complete
proof together with a high-level overview of the key components  in Appendix C. In fact  we prove
a stronger theorem—guaranteeing that the histogram returned by Algorithm 1 is close (in a speciﬁc
metric) to the histogram of the true distribution; this stronger theorem then implies that Algorithm 1
can accurately estimate any statistical property that is sufﬁciently Lipschitz continuous with respect
to the speciﬁc metric on histograms.
The information theoretic lower bounds of [1] show that there is some constant C1 such that for
sufﬁciently large k  no algorithm can estimate the entropy of (worst-case) distributions of support
size n to within ±0.1 with any probability of success greater 0.6 when given a sample of size at most
log n . Together with Theorem 1  this establishes the worst-case optimality of Algorithm 1
k = C1
(to constant factors).

n

3 Empirical results

i

2k

i

k|.

k| log2

In this section we demonstrate that Algorithm 1 performs well  in practice. We begin by brieﬂy
discussing the ﬁve entropy estimators to which we compare our estimator in Figure 1. The ﬁrst
three are standard  and are  perhaps  the most commonly used estimators [15]. We then describe two
recently proposed estimators that have been shown to perform well [22].
The “naive” estimator: the entropy of the empirical distribution  namely  given a ﬁngerprint F
derived from a set of k samples  H naive(F) := −￿i Fi
The Miller-Madow corrected estimator [23]: the naive estimator H naive corrected to try to ac-
count for the second derivative of the logarithm function  namely H MM (F) := H naive(F) +
(￿i Fi)−1
  though we note that the numerator of the correction term is sometimes replaced by vari-
ous related quantities  see [24].
k ￿k
The jackknifed naive estimator [25  26]: H JK(F) := k· H naive(F)− k−1
where F−j is the ﬁngerprint given by removing the contribution of the jth sample.
The coverage adjusted estimator (CAE) [27]: Chao and Shen proposed the CAE  which is specif-
ically designed to apply to settings in which there is a signiﬁcant component of the distribution that
is unseen  and was shown to perform well in practice in [22].4 Given a ﬁngerprint F derived from
a set of k samples  let Ps := 1 −F 1/k be the Good–Turing estimate of the probability mass of
the “seen” portion of the distribution [9]. The CAE adjusts the empirical probabilities according to
Ps  then applies the Horvitz–Thompson estimator for population totals [28] to take into account the
probability that the elements were seen. This yields:

j=1 H naive(F−j) 

H CAE(F) := −￿i

Fi

(i/k)Ps log2 ((i/k)Ps)
1 − (1 − (i/k)Ps)k

.

The Best Upper Bound estimator [15]: The ﬁnal estimator to which we compare ours is the Best
Upper Bound (BUB) estimator of Paninski. This estimator is obtained by searching for a minimax
linear estimator  with respect to a certain error metric. The linear estimators of [2] can be viewed
as a variant of this estimator with provable performance bounds.5 The BUB estimator requires  as
input  an upper bound on the support size of the distribution from which the samples are drawn;
if the bound provided is inaccurate  the performance degrades considerably  as was also remarked
in [22]. In our experiments  we used Paninski’s implementation of the BUB estimator (publicly
available on his website)  with default parameters. For the distributions with ﬁnite support  we gave
the true support size as input  and thus we are arguably comparing our estimator to the best–case
performance of the BUB estimator.
See Figure 1 for the comparison of Algorithm 1 with these estimators.

4One curious weakness of the CAE  is that its performance is exceptionally poor on some simple large
instances. Given a sample of size k from a uniform distribution over k elements  it is not hard to show that
the bias of the CAE is Ω(log k). This error is not even bounded! For comparison  even the naive estimator has
error bounded by a constant in the limit as k → ∞ in this setting. This bias of the CAE is easily observed in
our experiments as the “hump” in the top row of Figure 1.
5We also implemented the linear estimators of [2]  though found that the BUB estimator performed better.

6

Naive
Miller−Madow
Jackknifed
CAE
BUB
Unseen

Unif[n]  n=10 000

3
4
10
10
Sample Size

5
10

MixUnif[n]  n=10 000

3
4
10
10
Sample Size

5
10

Zipf[n]  n=10 000

Unif[n]  n=100 000

5
4
10
10
Sample Size

6
10

MixUnif[n]  n=100 000

5
4
10
10
Sample Size

6
10

Zipf[n]  n=100 000

1

E
S
M
R

0.5

0

1

E
S
M
R

0.5

0

1.5

E
S
M
R

1

0.5

3
4
10
10
Sample Size

5
10

Zipf2[n]  n=10 000

0

1.5

5
4
10
10
Sample Size

6
10

Zipf2[n]  n=100 000

E
S
M
R

1

0.5

3
4
10
10
Sample Size

5
10

Geom[n]  n=10 000

0

1.5

5
4
10
10
Sample Size

6
10

Geom[n]  n=100 000

E
S
M
R

1

0.5

4
3
10
10
Sample Size

0

5
10

5
4
10
10
Sample Size

6
10

Unif[n]  n=1 000

2
10

3
10
Sample Size

MixUnif[n]  n=1 000

2
10

3
10
Sample Size

Zipf[n]  n=1 000

2
10

3
10
Sample Size

Zipf2[n]  n=1 000

2
10

3
10
Sample Size

Geom[n]  n=1 000

2
10

3
10
Sample Size

1

E
S
M
R

0.5

0

1

E
S
M
R

0.5

0

E
S
M
R

1.5

1

0.5

0

1.5

E
S
M
R

1

0.5

0

1.5

E
S
M
R

1

0.5

0

MixGeomZipf[n]  n=1 000

MixGeomZipf[n]  n=10 000

1.5

MixGeomZipf[n]  n=100 000
 

1.5

E
S
M
R

1

0.5

0

E
S
M
R

1

0.5

0

 

5
10

5
4
10
10
Sample Size

6
10

1

E
S
M
R

0.5

0

1

E
S
M
R

0.5

0

E
S
M
R

1.5

1

0.5

0

E
S
M
R

1.5

1

0.5

0

1.5

E
S
M
R

1

0.5

0

E
S
M
R

1.5

1

0.5

0

2
10

3
10
Sample Size

3
4
10
10
Sample Size

5 ] and U nif [ 4n

5 and probability pi = 5

Figure 1: Plots depicting the square root of the mean squared error (RMSE) of each entropy estimator over
500 trials  plotted as a function of the sample size; note the logarithmic scaling of the x-axis. The samples are
drawn from six classes of distributions: the uniform distribution  U nif [n] that assigns probability pi = 1/n
for i = 1  2  . . .   n; an even mixture of U nif [ n
2n for
5 + 1  . . .   n; the Zipf distribution Zipf [n] that assigns
i = 1  . . .   n
1/i￿n
probability pi =
j=1 1/j for i = 1  2  . . .   n and is commonly used to model naturally occurring “power law”
distributions  particularly in natural language processing; a modiﬁed Zipf distribution with power–law exponent
1/i0.6
0.6  Zipf 2[n]  that assigns probability pi =
j=1 1/j0.6 for i = 1  2  . . .   n; the geometric distribution
￿n
Geom[n]  which has inﬁnite support and assigns probability pi = (1/n)(1 − 1/n)i  for i = 1  2 . . .; and
lastly an even mixture of Geom[n/2] and Zipf [n/2]. For each distribution  we considered three settings of
the parameter n: n = 1  000 (left column)  n = 10  000 (center column)  and n = 100  000 (right column). In
each plot  the sample size ranges over the interval [n0.6  n1.25].

5 ]  which assigns probability pi = 5

8n for i = n

All experiments were run in Matlab. The error parameter α in Algorithm 1 was set to be 0.5 for all
trials  and the vector x = x1  x2  . . . used as the support of the returned histogram was chosen to be a coarse
geometric mesh  with x1 = 1/k2  and xi = 1.1xi−1. The experimental results are essentially unchanged
if the parameter α varied within the range [0.25  1]  or if x1 is decreased  or if the mesh is made more ﬁne
(see Appendix B). Appendix D contains our Matlab implementation of Algorithm 1 (also available from our
websites).

The unseen estimator performs far better than the three standard estimators  dominates the CAE estimator
for larger sample sizes and on samples from the Zipf distributions  and also dominates the BUB estimator  even
for the uniform and Zipf distributions for which the BUB estimator received the true support sizes as input.

7

Estimating Distance (d=0)

Estimating Distance (d=0.5)

Estimating Distance (d=1)

 

Naive

Unseen

5

10

i

 

e
c
n
a
t
s
D
1
L
d
e
t
a
m

 

i
t
s
E

1

0.8

0.6

0.4

0.2

0

 

 

Naive

Unseen

5

10

i

 

e
c
n
a
t
s
D
1
L
d
e
t
a
m

 

i
t
s
E

1

0.8

0.6

0.4

0.2

0

 

 

Naive
Unseen

 

i

e
c
n
a
t
s
D
1
L
d
e
t
a
m

 

i
t
s
E

5

10

1

0.8

0.6

0.4

0.2

0

 

3

10
10
 Sample Size 

4

3

10
10
 Sample Size 

4

3

10
10
 Sample Size 

4

Figure 2: Plots depicting the estimated the total variation distance (￿1 distance) between two uniform distri-
butions on n = 10  000 points  in three cases: the two distributions are identical (left plot  d = 0)  the supports
overlap on half their domain elements (center plot  d = 0.5)  and the distributions have disjoint supports (right
plot  d = 1). The estimate of the distance is plotted along with error bars at plus and minus one standard
deviation; our results are compared with those for the naive estimator (the distance between the empirical dis-
tributions). The unseen estimator can be seen to reliably distinguish between the d = 0  d = 1
2   and d = 1
cases even for samples as small as several hundred.

3.1 Estimating ￿1 distance and number of words in Hamlet

The other two properties that we consider do not have such widely-accepted estimators as entropy 
and thus our evaluation of the unseen estimator will be more qualitative. We include these two exam-
ples here because they are of a substantially different ﬂavor from entropy estimation  and highlight
the ﬂexibility of our approach.
Figure 2 shows the results of estimating the total variation distance (￿1 distance). Because total
variation distance is a property of two distributions instead of one  ﬁngerprints and histograms are
two-dimensional objects in this setting (see Section 4.6 of [29])  and Algorithm 1 and the linear pro-
grams are extended accordingly  replacing single indices by pairs of indices  and Poisson coefﬁcients
by corresponding products of Poisson coefﬁcients.
Finally  in contrast to the synthetic tests above  we also evaluated our estimator on a real-data prob-
lem which may be seen as emblematic of the challenges in a wide gamut of natural language pro-
cessing problems: given a (contiguous) fragment of Shakespeare’s Hamlet  estimate the number
of distinct words in the whole play. We use this example to showcase the ﬂexibility of our linear
programming approach—our estimator can be customized to particular domains in powerful and
principled ways by adding or modifying the constraints of the linear program. To estimate the his-
togram of word frequencies in Hamlet  we note that the play is of length ≈ 25  000  and thus the
25 000. Thus in contrast to our previous
minimum probability with which any word can occur is
approach of using Linear Program 2 to bound the support of the returned histogram  we instead
25 000 
simply modify the input vector x of Linear Program 1 to contain only probability values ≥ 1
and forgo running Linear Program 2. The results are plotted in Figure 3. The estimates converge
towards the true value of 4268 distinct words extremely rapidly  and are slightly negatively biased 
perhaps reﬂecting the fact that words appearing close together are correlated.
In contrast to Hamlet’s charge that “there are more things in heaven and earth...than are dreamt of
in your philosophy ” we can say that there are almost exactly as many things in Hamlet as can be
dreamt of from 10% of Hamlet.

1

8000

6000

4000

2000

e
t
a
m

i
t
s
E

0

 
0

Estimating # Distinct Words in Hamlet

 

Naive

CAE

Unseen

0.5

1

1.5

2

Length of Passage

2.5

4
x 10

Figure 3: Estimates of the total number of distinct word forms in Shakespeare’s Hamlet (excluding stage
directions and proper nouns) as a functions of the length of the passage from which the estimate is inferred.
The true value  4268  is shown as the horizontal line.

8

References
[1] G. Valiant and P. Valiant. Estimating the unseen: an n/ log(n)–sample estimator for entropy and support

size  shown optimal via new CLTs. In Symposium on Theory of Computing (STOC)  2011.

[2] G. Valiant and P. Valiant. The power of linear estimators. In IEEE Symposium on Foundations of Computer

Science (FOCS)  2011.

[3] M. R. Nelson et al. An abundance of rare functional variants in 202 drug target genes sequenced in 14 002

people. Science  337(6090):100–104  2012.

[4] J. A. Tennessen et al. Evolution and functional impact of rare coding variation from deep sequencing of

human exomes. Science  337(6090):64–69  2012.

[5] A. Keinan and A. G. Clark. Recent explosive human population growth has resulted in an excess of rare

genetic variants. Science  336(6082):740–743  2012.

[6] F. Olken and D. Rotem. Random sampling from database ﬁles: a survey. In Proceedings of the Fifth

International Workshop on Statistical and Scientiﬁc Data Management  1990.

[7] P. J. Haas  J. F. Naughton  S. Seshadri  and A. N. Swami. Selectivity and cost estimation for joins based

on random sampling. Journal of Computer and System Sciences  52(3):550–569  1996.

[8] R.A. Fisher  A. Corbet  and C.B. Williams. The relation between the number of species and the number
of individuals in a random sample of an animal population. Journal of the British Ecological Society 
12(1):42–58  1943.

[9] I. J. Good. The population frequencies of species and the estimation of population parameters. Biometrika 

40(16):237–264  1953.

[10] D. A. McAllester and R.E. Schapire. On the convergence rate of Good-Turing estimators. In Conference

on Learning Theory (COLT)  2000.

[11] A. Orlitsky  N.P. Santhanam  and J. Zhang. Always Good Turing: Asymptotically optimal probability

estimation. Science  302(5644):427–431  October 2003.

[12] A. Orlitsky  N. Santhanam  K.Viswanathan  and J. Zhang. On modeling proﬁles instead of values. Un-

certainity in Artiﬁcial Intelligence  2004.

[13] J. Acharya  A. Orlitsky  and S. Pan. The maximum likelihood probability of unique-singleton  ternary 

and length-7 patterns. In IEEE Symp. on Information Theory  2009.

[14] J. Acharya  H. Das  A. Orlitsky  and S. Pan. Competitive closeness testing. In COLT  2011.
[15] L. Paninski. Estimation of entropy and mutual information. Neural Comp.  15(6):1191–1253  2003.
[16] J. Bunge and M. Fitzpatrick. Estimating the number of species: A review. Journal of the American

Statistical Association  88(421):364–373  1993.

[17] J. Bunge.

Bibliography of references on the problem of estimating support size  available at

http://www.stat.cornell.edu/˜bunge/bibliography.html.

[18] Z. Bar-Yossef  R. Kumar  and D. Sivakumar. Sampling algorithms: lower bounds and applications. In

STOC  2001.

[19] T. Batu Testing Properties of Distributions Ph.D. thesis  Cornell  2001.
[20] M. Charikar  S. Chaudhuri  R. Motwani  and V.R. Narasayya. Towards estimation error guarantees for

distinct values. In SODA  2000.

[21] T. Batu  L. Fortnow  R. Rubinfeld  W.D. Smith  and P. White. Testing that distributions are close. In IEEE

Symposium on Foundations of Computer Science (FOCS)  2000.

[22] V.Q. Vu  B. Yu  and R.E. Kass. Coverage-adjusted entropy estimation.

26(21):4039–4060  2007.

Statistics in Medicine 

[23] G. Miller. Note on the bias of information estimates.

Quastler (Glencoe  IL: Free Press):pp 95–100  1955.

Information Theory in Psychology II-B  ed H

[24] S. Panzeri and A Treves. Analytical estimates of limited sampling biases in different information mea-

sures. Network: Computation in Neural Systems  7:87–107  1996.

[25] S. Zahl. Jackkniﬁng an index of diversity. Ecology  58:907–913  1977.
[26] B. Efron and C. Stein. The jacknife estimate of variance. Annals of Statistics  9:586–596  1981.
[27] A. Chao and T.J. Shen. Nonparametric estimation of shannons index of diversity when there are unseen

species in sample. Environmental and Ecological Statistics  10:429–443  2003.

[28] D.G. Horvitz and D.J. Thompson. A generalization of sampling without replacement from a ﬁnite uni-

verse. Journal of the American Statistical Association  47(260):663–685  1952.

[29] P. Valiant. Testing Symmetric Properties of Distributions. SIAM J. Comput.  40(6):1927–1968 2011.

9

,Paul Valiant
Gregory Valiant