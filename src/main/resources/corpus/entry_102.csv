2013,A Latent Source Model for Nonparametric Time Series Classification,For classifying time series  a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks  decision trees  and support vector machines. We develop theoretical justification for the effectiveness of nearest-neighbor-like classification of time series. Our guiding hypothesis is that in many applications  such as forecasting which topics will become trends on Twitter  there aren't actually that many prototypical time series to begin with  relative to the number of time series we have access to  e.g.  topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis  we propose a latent source model for time series  which naturally leads to a weighted majority voting" classification rule that can be approximated by a nearest-neighbor classifier. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classification under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassification rate as nearest-neighbor classification while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends  where we are able to detect such "trending topics" in advance of Twitter 79% of the time  with a mean early advantage of 1 hour and 26 minutes  a true positive rate of 95%  and a false positive rate of 4%.",A Latent Source Model for

Nonparametric Time Series Classiﬁcation

George H. Chen

MIT

georgehc@mit.edu

snikolov@twitter.com

devavrat@mit.edu

Stanislav Nikolov

Twitter

Devavrat Shah

MIT

Abstract

For classifying time series  a nearest-neighbor approach is widely used in practice
with performance often competitive with or better than more elaborate methods
such as neural networks  decision trees  and support vector machines. We develop
theoretical justiﬁcation for the effectiveness of nearest-neighbor-like classiﬁca-
tion of time series. Our guiding hypothesis is that in many applications  such as
forecasting which topics will become trends on Twitter  there aren’t actually that
many prototypical time series to begin with  relative to the number of time series
we have access to  e.g.  topics become trends on Twitter only in a few distinct man-
ners whereas we can collect massive amounts of Twitter data. To operationalize
this hypothesis  we propose a latent source model for time series  which naturally
leads to a “weighted majority voting” classiﬁcation rule that can be approximated
by a nearest-neighbor classiﬁer. We establish nonasymptotic performance guar-
antees of both weighted majority voting and nearest-neighbor classiﬁcation under
our model accounting for how much of the time series we observe and the model
complexity. Experimental results on synthetic data show weighted majority voting
achieving the same misclassiﬁcation rate as nearest-neighbor classiﬁcation while
observing less of the time series. We then use weighted majority to forecast which
news topics on Twitter become trends  where we are able to detect such “trending
topics” in advance of Twitter 79% of the time  with a mean early advantage of 1
hour and 26 minutes  a true positive rate of 95%  and a false positive rate of 4%.

1

Introduction

Recent years have seen an explosion in the availability of time series data related to virtually every
human endeavor — data that demands to be analyzed and turned into valuable insights. A key
recurring task in mining this data is being able to classify a time series. As a running example used
throughout this paper  consider a time series that tracks how much activity there is for a particular
news topic on Twitter. Given this time series up to present time  we ask “will this news topic go
viral?” Borrowing Twitter’s terminology  we label the time series a “trend” and call its corresponding
news topic a trending topic if the news topic goes viral; otherwise  the time series has label “not
trend”. We seek to forecast whether a news topic will become a trend before it is declared a trend (or
not) by Twitter  amounting to a binary classiﬁcation problem. Importantly  we skirt the discussion
of what makes a topic considered trending as this is irrelevant to our mathematical development.1
Furthermore  we remark that handling the case where a single time series can have different labels
at different times is beyond the scope of this paper.

1While it is not public knowledge how Twitter deﬁnes a topic to be a trending topic  Twitter does provide
information for which topics are trending topics. We take these labels to be ground truth  effectively treating
how a topic goes viral to be a black box supplied by Twitter.

1

Numerous standard classiﬁcation methods have been tailored to classify time series  yet a simple
nearest-neighbor approach is hard to beat in terms of classiﬁcation performance on a variety of
datasets [20]  with results competitive to or better than various other more elaborate methods such
as neural networks [15]  decision trees [16]  and support vector machines [19]. More recently 
researchers have examined which distance to use with nearest-neighbor classiﬁcation [2  7  18] or
how to boost classiﬁcation performance by applying different transformations to the time series
before using nearest-neighbor classiﬁcation [1]. These existing results are mostly experimental 
lacking theoretical justiﬁcation for both when nearest-neighbor-like time series classiﬁers should be
expected to perform well and how well.
If we don’t conﬁne ourselves to classifying time series  then as the amount of data tends to inﬁnity 
nearest-neighbor classiﬁcation has been shown to achieve a probability of error that is at worst
twice the Bayes error rate  and when considering the nearest k neighbors with k allowed to grow
with the amount of data  then the error rate approaches the Bayes error rate [5]. However  rather
than examining the asymptotic case where the amount of data goes to inﬁnity  we instead pursue
nonasymptotic performance guarantees in terms of how large of a training dataset we have and how
much we observe of the time series to be classiﬁed. To arrive at these nonasymptotic guarantees  we
impose a low-complexity structure on time series.
Our contributions. We present a model for which nearest-neighbor-like classiﬁcation performs well
by operationalizing the following hypothesis: In many time series applications  there are only a small
number of prototypical time series relative to the number of time series we can collect. For example 
posts on Twitter are generated by humans  who are often behaviorally predictable in aggregate. This
suggests that topics they post about only become trends on Twitter in a few distinct manners  yet we
have at our disposal enormous volumes of Twitter data. In this context  we present a novel latent
source model: time series are generated from a small collection of m unknown latent sources  each
having one of two labels  say “trend” or “not trend”. Our model’s maximum a posteriori (MAP) time
series classiﬁer can be approximated by weighted majority voting  which compares the time series
to be classiﬁed with each of the time series in the labeled training data. Each training time series
casts a weighted vote in favor of its ground truth label  with the weight depending on how similar
the time series being classiﬁed is to the training example. The ﬁnal classiﬁcation is “trend” or “not
trend” depending on which label has the higher overall vote. The voting is nonparametric in that it
does not learn parameters for a model and is driven entirely by the training data. The unknown latent
sources are never estimated; the training data serve as a proxy for these latent sources. Weighted
majority voting itself can be approximated by a nearest-neighbor classiﬁer  which we also analyze.
Under our model  we show sufﬁcient conditions so that if we have n =⇥( m log m
 ) time series in
our training data  then weighted majority voting and nearest-neighbor classiﬁcation correctly clas-
 ) time steps. As
sify a new time series with probability at least 1  after observing its ﬁrst ⌦(log m
our analysis accounts for how much of the time series we observe  our results readily apply to the
“online” setting in which a time series is to be classiﬁed while it streams in (as is the case for fore-
casting trending topics) as well as the “ofﬂine” setting where we have access to the entire time series.
Also  while our analysis yields matching error upper bounds for the two classiﬁers  experimental re-
sults on synthetic data suggests that weighted majority voting outperforms nearest-neighbor classiﬁ-
cation early on when we observe very little of the time series to be classiﬁed. Meanwhile  a speciﬁc
instantiation of our model leads to a spherical Gaussian mixture model  where the latent sources are
Gaussian mixture components. We show that existing performance guarantees on learning spherical
Gaussian mixture models [6  10  17] require more stringent conditions than what our results need 
suggesting that learning the latent sources is overkill if the goal is classiﬁcation.
Lastly  we apply weighted majority voting to forecasting trending topics on Twitter. We emphasize
that our goal is precognition of trends: predicting whether a topic is going to be a trend before it
is actually declared to be a trend by Twitter or  in theory  any other third party that we can collect
ground truth labels from. Existing work that identify trends on Twitter [3  4  13] instead  as part
of their trend detection  deﬁne models for what trends are  which we do not do  nor do we assume
we have access to such deﬁnitions. (The same could be said of previous work on novel document
detection on Twitter [11  12].)
In our experiments  weighted majority voting is able to predict
whether a topic will be a trend in advance of Twitter 79% of the time  with a mean early advantage
of 1 hour and 26 minutes  a true positive rate of 95%  and a false positive rate of 4%. We empirically
ﬁnd that the Twitter activity of a news topic that becomes a trend tends to follow one of a ﬁnite
number of patterns  which could be thought of as latent sources.

2

Outline. Weighted majority voting and nearest-neighbor classiﬁcation for time series are pre-
sented in Section 2. We provide our latent source model and theoretical performance guarantees
of weighted majority voting and nearest-neighbor classiﬁcation under this model in Section 3. Ex-
perimental results for synthetic data and forecasting trending topics on Twitter are in Section 4.

2 Weighted Majority Voting and Nearest-Neighbor Classiﬁcation
Given a time-series2 s : Z ! R  we want to classify it as having either label +1 (“trend”) or 1
(“not trend”). To do so  we have access to labeled training data R+ and R  which denote the sets
of all training time series with labels +1 and 1 respectively.
Weighted majority voting. Each positively-labeled example r 2R + casts a weighted vote
ed(T )(r s) for whether time series s has label +1  where d(T )(r  s) is some measure of similar-
ity between the two time series r and s  superscript (T ) indicates that we are only allowed to look
at the ﬁrst T time steps (i.e.  time steps 1  2  . . .   T ) of s (but we’re allowed to look outside of these
time steps for the training time series r)  and constant   0 is a scaling parameter that determines
the “sphere of inﬂuence” of each example. Similarly  each negatively-labeled example in R also
casts a weighted vote for whether time series s has label 1.
The similarity measure d(T )(r  s) could  for example  be squared Euclidean distance: d(T )(r  s) =
PT
t=1(r(t)  s(t))2   kr  sk2
T . However  this similarity measure only looks at the ﬁrst T time
steps of training time series r. Since time series in our training data are known  we need not restrict
our attention to their ﬁrst T time steps. Thus  we use the following similarity measure:

TXt=1

min

d(T )(r  s) =

2{max ... 0 ... max}

(r(t +)  s(t))2 =

2{max ... 0 ... max}kr⇤  sk2
T  
(1)
where we minimize over integer time shifts with a pre-speciﬁed maximum allowed shift max  0.
Here  we have used q⇤ to denote time series q advanced by  time steps  i.e.  (q⇤)(t) = q(t+).
Finally  we sum up all of the weighted +1 votes and then all of the weighted 1 votes. The label
with the majority of overall weighted votes is declared as the label for s:

min

bL(T )(s; ) =(+1 if Pr2R+

otherwise.

1

ed(T )(r s) Pr2R

ed(T )(r s) 

(2)

Using a larger time window size T corresponds to waiting longer before we make a prediction.
We need to trade off how long we wait and how accurate we want our prediction. Note that k-
nearest-neighbor classiﬁcation corresponds to only considering the k nearest neighbors of s among
all training time series; all other votes are set to 0. With k = 1  we obtain the following classiﬁer:

Then we declare the label for s to be:

Nearest-neighbor classiﬁer. Letbr = arg minr2R+[R

N N (s) =⇢+1 ifbr 2R + 
bL(T )
1 ifbr 2R .

3 A Latent Source Model and Theoretical Guarantees

d(T )(r  s) be the nearest neighbor of s.

(3)

We assume there to be m unknown latent sources (time series) that generate observed time series.
Let V denote the set of all such latent sources; each latent source v : Z ! R in V has a true label
+1 or 1. Let V+ ⇢V be the set of latent sources with label +1  and V ⇢V be the set of those
with label 1. The observed time series are generated from latent sources as follows:

1. Sample latent source V from V uniformly at random.3 Let L 2 {±1} be the label of V .
2We index time using Z for notationally convenience but will assume time series to start at time step 1.
3While we keep the sampling uniform for clarity of presentation  our theoretical guarantees can easily be
extended to the case where the sampling is not uniform. The only change is that the number of training data
needed will be larger by a factor of
  where ⇡min is the smallest probability of a particular latent source
occurring.

m⇡min

1

3

Figure 1: Example of latent sources superimposed  where each latent source is shifted vertically in
amplitude such that every other latent source has label +1 and the rest have label 1.

2. Sample integer time shift  uniformly from {0  1  . . .   max}.
3. Output time series S : Z ! R to be latent source V advanced by  time steps  followed
by adding noise signal E : Z ! R  i.e.  S(t) = V (t +) + E(t). The label associated
with the generated time series S is the same as that of V   i.e.  L. Entries of noise E are
i.i.d. zero-mean sub-Gaussian with parameter   which means that for any time index t 

E[exp(E(t))]  exp⇣ 1

2

22⌘

for all  2 R.

(4)

The family of sub-Gaussian distributions includes a variety of distributions  such as a zero-
mean Gaussian with standard deviation  and a uniform distribution over [  ].

The above generative process deﬁnes our latent source model. Importantly  we make no assumptions
about the structure of the latent sources. For instance  the latent sources could be tiled as shown in
Figure 1  where they are evenly separated vertically and alternate between the two different classes
+1 and 1. With a parametric model like a k-component Gaussian mixture model  estimating
these latent sources could be problematic. For example  if we take any two adjacent latent sources
with label +1 and cluster them  then this cluster could be confused with the latent source having
label 1 that is sandwiched in between. Noise only complicates estimating the latent sources. In
this example  the k-component Gaussian mixture model needed for label +1 would require k to be
the exact number of latent sources with label +1  which is unknown. In general  the number of
samples we need from a Gaussian mixture mixture model to estimate the mixture component means
is exponential in the number of mixture components [14]. As we discuss next  for classiﬁcation 
we sidestep learning the latent sources altogether  instead using training data as a proxy for latent
sources. At the end of this section  we compare our sample complexity for classiﬁcation versus
some existing sample complexities for learning Gaussian mixture models.
Classiﬁcation. If we knew the latent sources and if noise entries E(t) were i.i.d. N (0  1
2 ) across t 
then the maximum a posteriori (MAP) estimate for label L given an observed time series S = s is

where

MAP(s; ) =(+1
bL(T )
MAP(s; )   Pv+2V+P+2D+
Pv2VP2D+

1

⇤(T )

if ⇤(T )
otherwise 

MAP(s; )  1 

exp  kv+ ⇤ +  sk2
T
exp  kv ⇤   sk2
T  

and D+   {0  . . .   max}.
However  we do not know the latent sources  nor do we know if the noise is i.i.d. Gaussian. We
assume that we have access to training data as given in Section 2. We make a further assumption
that the training data were sampled from the latent source model and that we have n different training
time series. Denote D   {max  . . .   0  . . .   max}. Then we approximate the MAP classiﬁer by
using training data as a proxy for the latent sources. Speciﬁcally  we take ratio (6)  replace the inner
sum by a minimum in the exponent  replace V+ and V by R+ and R  and replace D+ by D to
obtain the ratio:

(5)

(6)

(7)

⇤(T )(s; )   Pr+2R+
Pr2R

exp   min+2D kr+ ⇤ +  sk2
T
exp   min2D kr ⇤   sk2
T .

4

time activity +1 {1 +1 {1 +1 {1 Plugging ⇤(T ) in place of ⇤(T )
MAP in classiﬁcation rule (5) yields the weighted majority voting rule (2).
Note that weighted majority voting could be interpreted as a smoothed nearest-neighbor approxima-
tion whereby we only consider the time-shifted version of each example time series that is closest
to the observed time series s. If we didn’t replace the summations over time shifts with minimums
in the exponent  then we have a kernel density estimate in the numerator and in the denominator
[9  Chapter 7] (where the kernel is Gaussian) and our main theoretical result for weighted majority
voting to follow would still hold using the same proof.4
Lastly  applications may call for trading off true and false positive rates. We can do this by general-
izing decision rule (5) to declare the label of s to be +1 if ⇤(T )(s  )  ✓ and vary parameter ✓> 0.
The resulting decision rule  which we refer to as generalized weighted majority voting  is thus:

(s; ) =⇢+1

1

✓

bL(T )

if ⇤(T )(s  )  ✓ 
otherwise 

(8)

where setting ✓ = 1 recovers the usual weighted majority voting (2). This modiﬁcation to the
classiﬁer can be thought of as adjusting the priors on the relative sizes of the two classes. Our
theoretical results to follow actually cover this more general case rather than only that of ✓ = 1.
Theoretical guarantees. We now present the main theoretical results of this paper which identify
sufﬁcient conditions under which generalized weighted majority voting (8) and nearest-neighbor
classiﬁcation (3) can classify a time series correctly with high probability  accounting for the size of
the training dataset and how much we observe of the time series to be classiﬁed. First  we deﬁne the
“gap” between R+ and R restricted to time length T and with maximum time shift max as:

G(T )(R+ R  max)  

min

r+2R+ r2R 

+ 2D

kr+ ⇤ +  r ⇤ k2
T .

(9)

This quantity measures how far apart the two different classes are if we only look at length-T chunks
of each time series and allow all shifts of at most max time steps in either direction.
Our ﬁrst main result is stated below. We defer proofs to the longer version of this paper.
Theorem 1. (Performance guarantee for generalized weighted majority voting) Let m+ = |V+| be
the number of latent sources with label +1  and m = |V| = m  m+ be the number of latent
sources with label 1. For any > 1  under the latent source model with n >m log m time series
in the training data  the probability of misclassifying time series S with label L using generalized
weighted majority votingbL(T )
P(bL(T )
⇣ ✓m+
42 ) 
An immediate consequence is that given error tolerance  2 (0  1) and with choice  2 (0  1
then upper bound (10) is at most  (by having each of the two terms on the right-hand side be  
2)
if n > m log 2m


✓m⌘(2max + 1)n exp  (  422)G(T )(R+ R  max) + m+1.

(·; ) satisﬁes the bound

(S; ) 6= L)
m

(i.e.   = 1 + log 2

(10)

m

+

✓

✓

✓m ) + log(2max + 1) + log n + log 2


.

(11)

G(T )(R+ R  max) 

  422
 ) time series  then we can subsample n =⇥( m log m

This means that if we have access to a large enough pool of labeled time series  i.e.  the pool has
 ) of them to use as training data.
⌦(m log m
Then with choice  = 1
82   generalized weighted majority voting (8) correctly classiﬁes a new time
series S with probability at least 1   if

G(T )(R+ R  max) =⌦✓2⇣ log⇣ ✓m+

m

+

m

✓m⌘ + log(2max + 1) + log

m

⌘◆.

(12)

Thus  the gap between sets R+ and R needs to grow logarithmic in the number of latent sources m
in order for weighted majority voting to classify correctly with high probability. Assuming that the
4We use a minimum rather a summation over time shifts to make the method more similar to existing time

series classiﬁcation work (e.g.  [20])  which minimize over time warpings rather than simple shifts.

 / log m)  and
log( ✓m+
m + m

5

original unknown latent sources are separated (otherwise  there is no hope to distinguish between
the classes using any classiﬁer) and the gap in the training data grows as G(T )(R+ R  max) =
⌦(2T ) (otherwise  the closest two training time series from opposite classes are within noise of
each other)  then observing the ﬁrst T = ⌦(log(✓ + 1
 ) time steps from
✓ ) + log(2max + 1) + log m
the time series is sufﬁcient to classify it correctly with probability at least 1  .
A similar result holds for the nearest-neighbor classiﬁer (3).
Theorem 2. (Performance guarantee for nearest-neighbor classiﬁcation) For any > 1  under
the latent source model with n >m log m time series in the training data  the probability of
N N (·) satisﬁes the
(13)

misclassifying time series S with label L using the nearest-neighbor classiﬁerbL(T )
N N (S) 6= L)  (2max + 1)n exp⇣ 
P(bL(T )

162 G(T )(R+ R  max)⌘ + m+1.

bound

Our generalized weighted majority voting bound (10) with ✓ = 1 (corresponding to regular weighted
majority voting) and  = 1
82 matches our nearest-neighbor classiﬁcation bound  suggesting that
the two methods have similar behavior when the gap grows with T . In practice  we ﬁnd weighted
majority voting to outperform nearest-neighbor classiﬁcation when T is small  and then as T grows
large  the two methods exhibit similar performance in agreement with our theoretical analysis. For
small T   it could still be fairly likely that the nearest neighbor found has the wrong label  dooming
the nearest-neighbor classiﬁer to failure. Weighted majority voting  on the other hand  can recover
from this situation as there may be enough correctly labeled training time series close by that con-
tribute to a higher overall vote for the correct class. This robustness of weighted majority voting
makes it favorable in the online setting where we want to make a prediction as early as possible.
Sample complexity of learning the latent sources. If we can estimate the latent sources accurately 
then we could plug these estimates in place of the true latent sources in the MAP classiﬁer and
achieve classiﬁcation performance close to optimal.
If we restrict the noise to be Gaussian and
assume max = 0  then the latent source model corresponds to a spherical Gaussian mixture model.
We could learn such a model using Dasgupta and Schulman’s modiﬁed EM algorithm [6]. Their
theoretical guarantee depends on the true separation between the closest two latent sources  namely
G(T )⇤   minv v02V s.t. v6=v0 kv  v0k2
⌦(max{1  2T
T =⌦✓ max⇢1 

2  which needs to satisfy G(T )⇤  2pT . Then with n =
(G(T )⇤)2 log m

their algorithm achieves  with probability at least 1    an additive "pT error (in Euclidean
distance) close to optimal in estimating every latent source. In contrast  our result is in terms of gap
G(T )(R+ R  max) that depends not on the true separation between two latent sources but instead
on the minimum observed separation in the training data between two time series of opposite labels.
In fact  our gap  in their setting  grows as ⌦(2T ) even when their gap G(T )⇤ grows sublinear in T .
 )  G(T )⇤  2pT  
In particular  while their result cannot handle the regime where O(2 log m
 ) time
ours can  using n =⇥( m log m
steps to classify a time series correctly with probability at least 1  ; see the longer version of this
paper for details.
Vempala and Wang [17] have a spectral method for learning Gaussian mixture models that can han-

 ) training time series and observing the ﬁrst T = ⌦(log m

4T 2

(G(T )⇤)2◆ 

 )  G(T )⇤ =⌦( 2 log m

" )  and

G(T )⇤}m log m

dle smaller G(T )⇤ than Dasgupta and Schulman’s approach but requires n =e⌦(T 3m2) training data 

where we’ve hidden the dependence on 2 and other variables of interest for clarity of presentation.
Hsu and Kakade [10] have a moment-based estimator that doesn’t have a gap condition but  under a
different non-degeneracy condition  requires substantially more samples for our problem setup  i.e. 
n =⌦(( m14 + T m11)/"2) to achieve an " approximation of the mixture components. These results
need substantially more training data than what we’ve shown is sufﬁcient for classiﬁcation.
To ﬁt a Gaussian mixture model to massive training datasets  in practice  using all the training data
could be prohibitively expensive. In such scenarios  one could instead non-uniformly subsample
O(T m3/"2) time series from the training data using the procedure given in [8] and then feed the
resulting smaller dataset  referred to as an (m  ")-coreset  to the EM algorithm for learning the latent
sources. This procedure still requires more training time series than needed for classiﬁcation and
lacks a guarantee that the estimated latent sources will be close to the true latent sources.

4T 2

max⇢1 



(14)

1

6

a
t
a
d
 
t
s
e
t
 
n
o
 
e
t
a
r
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

 

Weighted majority voting
Nearest−neighbor classifier
Oracle MAP classifier

0.6

0.5

0.4

0.3

0.2

0.1

0

 
0

50

150

200

100
T

(a)

a
t
a
d
 
t
s
e
t
 
n
o
 
e
t
a
r
 
r
o
r
r
e
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

0.25

0.2

0.15

0.1

0.05

0

 
1

 

Weighted majority voting
Nearest−neighbor classifier
Oracle MAP classifier

2

3

4

5

6

7

8

β

(b)

Figure 2: Results on synthetic data. (a) Classiﬁcation error rate vs. number of initial time steps T
used; training set size: n = m log m where  = 8. (b) Classiﬁcation error rate at T = 100 vs. .
All experiments were repeated 20 times with newly generated latent sources  training data  and test
data each time. Error bars denote one standard deviation above and below the mean value.

Figure 3: How news topics become trends on Twitter. The top left shows some time series of activity
leading up to a news topic becoming trending. These time series superimposed look like clutter  but
we can separate them into different clusters  as shown in the next ﬁve plots. Each cluster represents
a “way” that a news topic becomes trending.

4 Experimental Results

Synthetic data. We generate m = 200 latent sources  where each latent source is constructed by
ﬁrst sampling i.i.d. N (0  100) entries per time step and then applying a 1D Gaussian smoothing
ﬁlter with scale parameter 30. Half of the latent sources are labeled +1 and the other half 1. Then
n = m log m training time series are sampled as per the latent source model where the noise added
is i.i.d. N (0  1) and max = 100. We similarly generate 1000 time series to use as test data. We
set  = 1/8 for weighted majority voting. For  = 8  we compare the classiﬁcation error rates on
test data for weighted majority voting  nearest-neighbor classiﬁcation  and the MAP classiﬁer with
oracle access to the true latent sources as shown in Figure 2(a). We see that weighted majority voting
outperforms nearest-neighbor classiﬁcation but as T grows large  the two methods’ performances
converge to that of the MAP classiﬁer. Fixing T = 100  we then compare the classiﬁcation error
rates of the three methods using varying amounts of training data  as shown in Figure 2(b); the
oracle MAP classiﬁer is also shown but does not actually depend on training data. We see that as
 increases  both weighted majority voting and nearest-neighbor classiﬁcation steadily improve in
performance.
Forecasting trending topics on twitter. We provide only an overview of our Twitter results here 
deferring full details to the longer version of this paper. We sampled 500 examples of trends at
random from a list of June 2012 news trends  and 500 examples of non-trends based on phrases
appearing in user posts during the same month. As we do not know how Twitter chooses what
phrases are considered as candidate phrases for trending topics  it’s unclear what the size of the

7

time activity (a)

(b)

(c)

Figure 4: Results on Twitter data. (a) Weighted majority voting achieves a low error rate (FPR
of 4%  TPR of 95%) and detects trending topics in advance of Twitter 79% of the time  with a mean
of 1.43 hours when it does; parameters:  = 10  T = 115  Tsmooth = 80  h = 7. (b) Envelope of
all ROC curves shows the tradeoff between TPR and FPR. (c) Distribution of detection times for
“aggressive” (top)  “conservative” (bottom) and “in-between” (center) parameter settings.

non-trend category is in comparison to the size of the trend category. Thus  for simplicity  we
intentionally control for the class sizes by setting them equal. In practice  one could still expressly
assemble the training data to have pre-speciﬁed class sizes and then tune ✓ for generalized weighted
majority voting (8). In our experiments  we use the usual weighted majority voting (2) (i.e.  ✓ = 1)
to classify time series  where max is set to the maximum possible (we consider all shifts).
Per topic  we created its time series based on a pre-processed version of the raw rate of how often
the topic was shared  i.e.  its Tweet rate. We empirically found that how news topics become trends
tends to follow a ﬁnite number of patterns; a few examples of these patterns are shown in Figure 3.
We randomly divided the set of trends and non-trends into into two halves  one to use as training
data and one to use as test data. We applied weighted majority voting  sweeping over   T   and
data pre-processing parameters. As shown in Figure 4(a)  one choice of parameters allows us to
detect trending topics in advance of Twitter 79% of the time  and when we do  we detect them an
average of 1.43 hours earlier. Furthermore  we achieve a true positive rate (TPR) of 95% and a false
positive rate (FPR) of 4%. Naturally  there are tradeoffs between TPR  FPR  and how early we make
a prediction (i.e.  how small T is). As shown in Figure 4(c)  an “aggressive” parameter setting yields
early detection and high TPR but high FPR  and a “conservative” parameter setting yields low FPR
but late detection and low TPR. An “in-between” setting can strike the right balance.
Acknowledgements. This work was supported in part by the Army Research Ofﬁce under MURI
Award 58153-MA-MUR. GHC was supported by an NDSEG fellowship.

8

References
[1] Anthony Bagnall  Luke Davis  Jon Hills  and Jason Lines. Transformation based ensembles for time
series classiﬁcation. In Proceedings of the 12th SIAM International Conference on Data Mining  pages
307–319  2012.

[2] Gustavo E.A.P.A. Batista  Xiaoyue Wang  and Eamonn J. Keogh. A complexity-invariant distance mea-
sure for time series. In Proceedings of the 11th SIAM International Conference on Data Mining  pages
699–710  2011.

[3] Hila Becker  Mor Naaman  and Luis Gravano. Beyond trending topics: Real-world event identiﬁcation

on Twitter. In Proceedings of the Fifth International Conference on Weblogs and Social Media  2011.

[4] Mario Cataldi  Luigi Di Caro  and Claudio Schifanella. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceedings of the 10th International Workshop on Multimedia
Data Mining  2010.

[5] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classiﬁcation.

Information Theory  13(1):21–27  1967.

IEEE Transactions on

[6] Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of EM for mixtures of separated 

spherical gaussians. Journal of Machine Learning Research  8:203–226  2007.

[7] Hui Ding  Goce Trajcevski  Peter Scheuermann  Xiaoyue Wang  and Eamonn Keogh. Querying and min-
ing of time series data: experimental comparison of representations and distance measures. Proceedings
of the VLDB Endowment  1(2):1542–1552  2008.

[8] Dan Feldman  Matthew Faulkner  and Andreas Krause. Scalable training of mixture models via coresets.

In Advances in Neural Information Processing Systems 24  2011.

[9] Keinosuke Fukunaga. Introduction to statistical pattern recognition (2nd ed.). Academic Press Profes-

sional  Inc.  1990.

[10] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: Moment methods and

spectral decompositions  2013. arXiv:1206.5766.

[11] Shiva Prasad Kasiviswanathan  Prem Melville  Arindam Banerjee  and Vikas Sindhwani. Emerging topic
In Proceedings of the 20th ACM Conference on Information and

detection using dictionary learning.
Knowledge Management  pages 745–754  2011.

[12] Shiva Prasad Kasiviswanathan  Huahua Wang  Arindam Banerjee  and Prem Melville. Online l1-
In Advances in Neural Information

dictionary learning with application to novel document detection.
Processing Systems 25  pages 2267–2275  2012.

[13] Michael Mathioudakis and Nick Koudas. Twittermonitor: trend detection over the Twitter stream. In

Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data  2010.

[14] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In 51st

Annual IEEE Symposium on Foundations of Computer Science  pages 93–102  2010.

[15] Alex Nanopoulos  Rob Alcock  and Yannis Manolopoulos. Feature-based classiﬁcation of time-series

data. International Journal of Computer Research  10  2001.

[16] Juan J. Rodr´ıguez and Carlos J. Alonso.

Proceedings of the 2004 ACM Symposium on Applied Computing  2004.

Interval and dynamic time warping-based decision trees.

In

[17] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Com-

puter and System Sciences  68(4):841–860  2004.

[18] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor

classiﬁcation. Journal of Machine Learning Research  10:207–244  2009.

[19] Yi Wu and Edward Y. Chang. Distance-function design and fusion for sequence data. In Proceedings of

the 2004 ACM International Conference on Information and Knowledge Management  2004.

[20] Xiaopeng Xi  Eamonn J. Keogh  Christian R. Shelton  Li Wei  and Chotirat Ann Ratanamahatana. Fast
time series classiﬁcation using numerosity reduction. In Proceedings of the 23rd International Conference
on Machine Learning  2006.

9

,George Chen
Stanislav Nikolov
Devavrat Shah
Se-Young Yun
Alexandre Proutiere