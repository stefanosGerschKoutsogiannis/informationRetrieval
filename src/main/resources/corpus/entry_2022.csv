2019,Fast Low-rank Metric Learning for Large-scale and High-dimensional Data,Low-rank metric learning aims to learn better discrimination of data subject to low-rank constraints. It keeps the intrinsic low-rank structure of datasets and reduces the time cost and memory usage in metric learning. However  it is still a challenge for current methods to handle datasets with both high dimensions and large numbers of samples. To address this issue  we present a novel fast low-rank metric learning (FLRML) method. FLRML casts the low-rank metric learning problem into an unconstrained optimization on the Stiefel manifold  which can be efficiently solved by searching along the descent curves of the manifold. FLRML significantly reduces the complexity and memory usage in optimization  which makes the method scalable to both high dimensions and large numbers of samples. Furthermore  we introduce a mini-batch version of FLRML to make the method scalable to larger datasets which are hard to be loaded and decomposed in limited memory. The outperforming experimental results show that our method is with high accuracy and much faster than the state-of-the-art methods under several benchmarks with large numbers of high-dimensional data. Code has been made available at https://github.com/highan911/FLRML.,Fast Low-rank Metric Learning for Large-scale and

High-dimensional Data

Han Liu †  Zhizhong Han‡  Yu-Shen Liu† ∗   Ming Gu†
† School of Software  Tsinghua University  Beijing  China

‡ Department of Computer Science  University of Maryland  College Park  USA

BNRist & KLISS  Beijing  China

liuhan15@mails.tsinghua.edu.cn

h312h@umd.edu

liuyushen@tsinghua.edu.cn

guming@tsinghua.edu.cn

Abstract

Low-rank metric learning aims to learn better discrimination of data subject to
low-rank constraints. It keeps the intrinsic low-rank structure of datasets and
reduces the time cost and memory usage in metric learning. However  it is still a
challenge for current methods to handle datasets with both high dimensions and
large numbers of samples. To address this issue  we present a novel fast low-rank
metric learning (FLRML) method. FLRML casts the low-rank metric learning
problem into an unconstrained optimization on the Stiefel manifold  which can be
efﬁciently solved by searching along the descent curves of the manifold. FLRML
signiﬁcantly reduces the complexity and memory usage in optimization  which
makes the method scalable to both high dimensions and large numbers of samples.
Furthermore  we introduce a mini-batch version of FLRML to make the method
scalable to larger datasets which are hard to be loaded and decomposed in limited
memory. The outperforming experimental results show that our method is with
high accuracy and much faster than the state-of-the-art methods under several
benchmarks with large numbers of high-dimensional data. Code has been made
available at https://github.com/highan911/FLRML.

1

Introduction

Metric learning aims to learn a distance (or similarity) metric from supervised or semi-supervised
information  which provides better discrimination between samples. Metric learning has been widely
used in various area  such as dimensionality reduction [1  2  3]  robust feature extraction [4  5] and
information retrieval [6  7]. For existing metric learning methods  the huge time cost and memory
usage are major challenges when dealing with high-dimensional datasets with large numbers of
samples. To resolve this issue  low-rank metric learning (LRML) methods optimize a metric matrix
subject to low-rank constraints. These methods tend to keep the intrinsic low-rank structure of the
dataset  and also  reduce the time cost and memory usage in the learning process. Reducing the
matrix size in optimization is an important idea to reduce time and memory usage. However  the
size of the matrix to be optimized still increases linearly or squarely with either the dimensions  the
number of samples  or the number of pairwise/triplet constraints. As a result  it is still a research
challenge when dealing with the metric learning task on datasets with both high dimensions and large
numbers of samples [8  9].
To address this issue  we present a Fast Low-Rank Metric Learning (FLRML). In contrast to state-of-
the-art methods  FLRML introduces a novel formulation to better employ the low rank constraints to
further reduce the complexity  the size of involved matrices  which enables FLRML to achieve high

∗Corresponding author: Yu-Shen Liu.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

accuracy and faster speed on large numbers of high-dimensional data. Our main contributions are
listed as follows.
- Modeling the constrained metric learning problem as an unconstrained optimization that can be
efﬁciently solved on the Stiefel manifold  which makes our method scalable to large numbers of
samples and constraints.
- Reducing the matrix size and complexity in optimization as much as possible while ensuring the
accuracy  which makes our method scalable to both large numbers of samples and high dimensions.
- Furthermore  a mini-batch version of FLRML is proposed to make the method scalable to larger
datasets which are hard to be fully loaded in memory.

2 Related Work
In metric learning tasks  the training dataset can be represented as a matrix X = [x1  ...  xn] ∈ RD×n 
where n is the number of training samples and each sample xi is with D dimensions. Metric
learning methods aim to learn a metric matrix M ∈ RD×D from the training set in order to obtain
better discrimination between samples. Some low-rank metric learning (LRML) methods have
been proposed to obtain the robust metric of data  and to reduce the computational costs for high-
dimensional metric learning tasks. Since the optimization with ﬁxed low-rank constraint is nonconvex 
the naive gradient descent methods are easy to fall into bad local optimal solutions [2  10]. In terms
of different strategies to remedy this issue  the existing LRML methods can be roughly divided into
the following two categories.
One type of method [1  11  12  13  14] introduces the low-rankness encouraging norms (such as
nuclear norm) as regularization  which relaxes the nonconvex low-rank constrained problems to
convex problems. The two disadvantages of such methods are: (1) the norm regularization can only
encourage the low-rankness  but cannot limit the upper bound of rank; (2) the matrix to be optimized
is still the size of either D2 or n2.
Another type of method [2  3  15  16  17  18] considers the low-rank constrained space as Rieman-
nian manifold. This type of method can obtain high-quality solutions of the nonconvex low-rank
constrained problems. However  for these methods  the matrices to be optimized are at least a
linear size of either D or n. The performance of these methods is still suffering on large-scale and
high-dimensional datasets.
Besides low-rank metric learning methods  there are some other types of methods for speeding up
metric learning on large and high-dimensional datasets. Online metric leaning [6  7  19  20] randomly
takes one sample at each time. Sparse metric leaning [21  22  23  24  25  26] represents the metric
matrix as the sparse combination of some pre-generated rank-1 bases. Non-iterative metric leaning
[27  28] avoids iterative calculation by providing explicit optimal solutions. In the experiments 
some state-of-the-art methods of these types will also be included for comparison. Compared with
these methods  our method also has advantage in time and memory usage on large-scale and high-
dimensional datasets. A literature review of many available metric learning methods is beyond the
scope of this paper. The reader may consult Refs. [8  9  29] for detailed expositions.

3 Fast Low-Rank Metric Learning (FLRML)

The metric matrix M is usually semideﬁnite  which guarantees the non-negative distances and non-
negative self-similarities. A semideﬁnite M can be represented as the transpose multiplication of
two identical matrices  M = L(cid:62)L  where L ∈ Rd×D is a row-full-rank matrix  and rank(M) = d.
Using the matrix L as a linear transformation  the training set X can be mapped into Y ∈ Rd×n 
which is denoted by Y = LX. Each column vector yi in Y is the corresponding d-dimensional
vector of the column vector xi in X.
In this paper  we present a fast low-rank metric learning (FLRML) method  which typically learns
the low-rank cosine similarity metric from triplet constraints T . The cosine similarity between
a pair of vectors (xi  xj) is measured by their corresponding low dimensional vector (yi  yj) as
sim(xi  xj) = y(cid:62)
||yi|| ||yj||. Each constraint {i  j  k} ∈ T refers to the comparison of a pair of
similarities sim(xi  xj) > sim(xi  xk).

i yj

2

To solve the problem of metric learning on large-scale and high-dimensional datasets  our motivation
is to reduce matrix size and complexity in optimization as much as possible while ensuring the
accuracy. To address this issue  our idea is to embed the triplet constraints into a matrix K  so that
the constrained metric learning problem can be casted into an unconstrained optimization in the
“form” of tr(WK)  where W is a low-rank semideﬁnite matrix to be optimized (Section 3.1). By
reducing the sizes of W and K to Rr×r (r = rank(X))  the complexity and memory usage are
greatly reduced. An unconstrained optimization in this form can be efﬁciently solved on the Stiefel
manifold (Section 3.2). In addition  a mini-batch version of FLRML is proposed  which makes our
method scalable to larger datasets that are hard to be fully loaded and decomposed in the memory
(Section 3.3).

ji = 1 and c(t)

1|Ti|+1 on both sides  we can get

i=1(zi + m)λ(zi + m)  which can be further represented in matrices:

L(T ) = −tr(Y(cid:62)YCTΛ) + M (Λ) 

C =(cid:80)
ﬁrst item  and ˜yi be the i-th column of YC  then ˜yi can be written as ˜yi =(cid:80){i j k}∈Ti

3.1 Forming the Objective Function
Using margin loss  each triplet t = {i  j  k} in T corresponds to a loss function l({i  j  k}) =
max(0  m − sim(xi  xj) + sim(xi  xk)). A naive idea is to sum the loss functions  but when n and
|T | are very large  the evaluation of loss functions will be time consuming. Our idea is to embed the
evaluation of loss functions into matrices to speed up their calculation.
For each triplet t = {i  j  k} in T   a matrix C(t) with the size n × n is generated  which is a
ki = −1. The summation of all C(t) matrices is represented as
sparse matrix with c(t)
t∈T C(t). The matrix YC is with the size Rd×n. Let Ti be the subset of triplets with i as the
(−yj + yk).
This is the sum of negative samples minus the sum of positive samples for the set Ti. By multiplying
1|Ti|+1 ˜yi as the mean of negative samples minus the mean of
on
positive samples (in which “|Ti| + 1” is to avoid zero on the denominator).
Let zi = 1|Ti|+1 y(cid:62)
i ˜yi  then by minimizing zi  the vector yi tends to be closer to the positive samples
than the negative samples. Let T be a diagonal matrix with Tii = 1|Ti|+1  then zi is the i-th diagonal
element of −Y(cid:62)YCT. The loss function can be constructed by putting zi into the margin loss as
i=1 max(0  zi + m). A binary function λ(x) is deﬁned as: if x > 0  then λ(x) = 1;
otherwise  λ(x) = 0. By introducing the function λ(x)  the loss function L(T ) can be written as

L(T ) = (cid:80)n
L(T ) =(cid:80)n
where Λ is a diagonal matrix with Λii = λ(zi +m)  and M (Λ) = m(cid:80)n

(1)
i=1 Λii is the sum of constant
terms in the margin loss.
In Eq.(1)  Y ∈ Rd×n is the optimization variable. For any value of Y  the corresponding value of
L can be obtained by solving the linear equation group Y = LX. A minimum norm least squares
solution of Y = LX is L = YVΣ−1U(cid:62)  where [U ∈ RD×r  Σ ∈ Rr×r  V ∈ Rn×r] is the SVD
of X. Based on this  the size of the optimization variable can be reduced from Rd×n to Rd×r  as
shown in Theorem 1.
Theorem 1. {Y : Y = BV(cid:62)  B ∈ Rd×r} is a subset of Y that covers all the possible minimum
norm least squares solutions of L.
Proof. By substituting Y = BV(cid:62) into L = YVΣ−1U(cid:62) 
L = BΣ−1U(cid:62).

(2)
Since U and Σ are constants  then B ∈ Rd×r covers all the possible minimum norm least squares
solutions of L.
By substituting Y = BV(cid:62) into Eq.(1)  the sizes of W and K can be reduced to Rr×r  which are
represented as

L(T ) = −tr(B(cid:62)BV(cid:62)CTΛV) + M (Λ).

(3)
This function is in the form of tr(WK)  where W = B(cid:62)B and K = −V(cid:62)CTΛV. The size of W
and K are reduced to Rr×r  and r ≤ min(n  D). In addition  V(cid:62)CT ∈ Rr×n is a constant matrix
that will not change in the process of optimization. So this model is with low complexity.

3

It should be noted that the purpose of SVD here is not for approximation. If all the ranks of X are
kept  i.e. r = rank(X)  the solutions are supposed to be exact. In practice  it is also reasonable to
neglect the smallest eigenvalues of X to speed up the calculation. In the experiments  an upper bound
is set as r = min(rank(X)  3000)  since most computers can easily handle a matrix of 30002 size 
and the information in most of the datasets can be preserved well.

3.2 Optimizing on the Stiefel Manifold
The matrix W = B(cid:62)B is the low-rank semideﬁnite matrix to be optimized. Due to the non-convexity
of low-rank semideﬁnite optimization  directly optimizing B in the linear space often falls into bad
local optimal solutions [2  10]. The mainstream strategy of low-rank semideﬁnite problem is to
achieve the optimization on manifolds.
The Stiefel manifold St(d  r) is deﬁned as the set of r×d column-orthogonal matrices  i.e.  St(d  r) =
{P ∈ Rr×d : P(cid:62)P = Id}. Any semideﬁnite W with rank(W) = d can be represented as
W = PSP(cid:62)  where P ∈ St(d  r) and S ∈ {diag(s) : s ∈ Rd
+}. Since P is already restricted on the
Stiefel manifold  we only need to add regularization term for s. We want to guarantee the existence
of dense ﬁnite optimal solution of s  so the L2-norm of s is used as a regularization term. By adding
2||s||2 into Eq.(3)  we get
1

d(cid:88)

i=1

f0(W) = tr(PSP(cid:62)K) +

||s||2 + M (Λ) =

1
2

(

i + sip(cid:62)
s2

i Kpi) + M (Λ) 

1
2

(4)

where pi is the i-th column of P.
Let ki = −p(cid:62)
corresponding optimal solution of s ∈ Rd

+ is

i Kpi. Since f0(W) is a quadratic function for each si  for any value of P  the only

ˆs = {[ˆs1  ...  ˆsd](cid:62) : ˆsi = max(0  ki)}.

(5)

By substituting the ˆsi values into Eq.(4)  the existence of s in f0(W) can be eliminated  which
converts it to a new function f (P) that is only relevant with P  as shown in the following Theorem 2.
Theorem 2.

(max(0  ki)ki) + M (Λ).

(6)

d(cid:88)

i=1

f (P) = − 1
2

(cid:80)d

Proof. An original form of f (P) can be obtained by substituting Eq.(5) into Eq.(4):
The f (P) in Eq.(6) is equal to this formula in both ki ≤ 0 and ki > 0 conditions.

2 max(0  ki) − ki)max(0  ki)) + M (Λ).

i=1(( 1

In order to make the gradient of f (P) continuous  and to keep s dense and positive  we adopt function
µ(x) = −log(σ(−x)) as the smoothing of max(0  x) [2]  where σ(x) = 1/(1 + exp(−x)) is the
sigmoid function. Function µ(x) satisﬁes limx→+∞ = x and limx→−∞ = 0. The derivative of µ(x)
is dµ(x)/dx = σ(x). Figure 1 displays the sample plot of max(0  x) and µ(x). Using this smoothed

Figure 1: A sample plot of max(0  x) and µ(x).

4

x-5-4-3-2-101230123max(0;x)7(x)trix C ∈ Rn×n  the low-rank constraint d

Algorithm 1 FLRML
1: Input: the data matrix X ∈ RD×n  the sparse supervision ma-
2: [U ∈ RD×r  Σ ∈ Rr×r  V ∈ Rn×r] ← SVD(X)
3: Calculating constant matrix V(cid:62)CT
4: Randomly initialize P ∈ St(d  r)
5: Initialize S satisﬁes Eq.(8)
6: repeat
7:
8:
9:
10: until convergence
11: Output: L ← √

Update Λ and K by Eq.(3)
Update G by Eq.(9)
Update P and S by searching on h(τ ) in Eq.(10)

SP(cid:62)Σ−1U(cid:62)

ces CI ∈ RnI ×nI   the low-rank constraint d

Algorithm 2 M-FLRML
1: Input: the data matrices XI ∈ RD×nI   the supervision matri-
2: Randomly initialize L ∈ Rd×D
3: for I = 1 : T do
[UI   ΣI   VI ] ← SVD(XI )
4:
5:
Get PI and sI by Eq.(11)
6:
Update Λ and K by Eq.(3)
7:
Update PI and sI by Eq.(12) and Eq.(5)
8:
Get ∆L by Eq.(13)
9: L ← L + 1√
∆L
10: end for
11: Output: L

I

function  the loss function f (P) is redeﬁned as

f (P) = − 1
2
+} needs to satisfy the condition
The initialization of S ∈ {diag(s) : s ∈ Rd

(µ(ki)ki) + M (Λ).

i=1

d(cid:88)

s = ˆs.

(8)
When P is ﬁxed  ˆs is a linear function of K (see Eq.(5))  K is a linear function of Λ (see Eq.(3)) 
and the 0-1 values in Λ are relevant with s (see Eq.(3)). So Eq.(8) is a nonlinear equation in a form
that can be easily solved iteratively by updating s with ˆs. Since ˆs ∈ O(K1)  K ∈ O(Λ1)  and
Λ ∈ O(s0)  this iterative process has a superlinear convergence rate.
To solve the model of this paper  for a matrix P ∈ St(d  r)  we need to get its gradient G = ∂f (P)
∂P .
Theorem 3.

∂f (P)

∂P

= −(K + K(cid:62))Pdiag(q) 

(7)

(9)

G =
2 (µ(ki) + kiσ(ki)).
= − 1

where qi = − 1

(cid:80)d

Proof. Since qi = ∂f (P)
∂ki

2 (µ(ki) + kiσ(ki))  the gradient can be derived from ∂f (P)

∂P =

∂P .
∂ki

i=1 qi
∂P can be easily obtained since ki = −p(cid:62)
The ∂ki

i Kpi.

For solving optimizations on manifolds  the commonly used method is the “projection and retraction” 
which ﬁrst projects the gradient G onto the tangent space of the manifold as ˆG  and then retracts
(P − ˆG) back to the manifold. For Stiefel manifold  the projection of G on the tangent space
is ˆG = G − PG(cid:62)P [10]. The retraction of the matrix (P − ˆG) to the Stiefel manifold can be
represented as retract(P − ˆG)  which is obtained by setting all the singular values of (P − ˆG) to 1
[30].
For Stiefel manifolds  we adopt a more efﬁcient algorithm [10]  which performs a non-monotonic
line search with Barzilai-Borwein step length [31  32] on a descent curve of the Stiefel manifold. A
descent curve with parameter τ is deﬁned as

τ
2

H)−1(I − τ
2

H)P 

h(τ ) = (I +

(10)
where H = G P(cid:62) − P G(cid:62). The optimization is performed by searching the optimal τ along the
descent curve. The Barzilai-Borwein method predicts a step length according to the step lengths in
previous iterations  which makes the method converges faster than the “projection and retraction”.
The outline of the FLRML algorithm is shown in Algorithm 1. It can be mainly divided into four
stages: SVD preprocessing (line 2)  constant initializing (line 3)  variable initializing (lines 4 and
5)  and the iterative optimization (lines 6 to 11). In one iteration  the complexity of each step is: (a)
updating Y and Λ: O(nrd); (b) updating K: O(nr2); (c) updating G : O(r2d); (d) optimizing P
and S: O(rd2).

5

3.3 Mini-batch FLRML
In FLRML  the maximum size of constant matrices in the iterations is only Rr×n (V and V(cid:62)CT) 
and the maximum size of variable matrices is only Rr×r. Smaller matrix size theoretically means the
ability to process larger datasets on the same size of memory. However  in practice  we ﬁnd that the
bottleneck is not the optimization process of FLRML. On large-scale and high-dimensional datasets 
SVD preprocessing may take more time and memory than the FLRML optimization process. And for
very large datasets  it will be difﬁcult to load all data into memory. In order to break the bottleneck 
and make our method scalable to larger numbers of high-dimensional data in limited memory  we
further propose Mini-batch FLRML (M-FLRML).
Inspired by the stochastic gradient descent method [18  33]  M-FLRML calculates a descent direction
from each mini-batch of data  and updates L at a decreasing ratio. For the I-th mini-batch  we
randomly select Nt triplets from the triplet set  and use the union of the samples to form a mini-batch
with nI samples. Considering that the Stiefel manifold St(d  r) requires r ≥ d  if the number of
samples in the union of triplets is less than d  we randomly add some other samples to make nI > d.
The matrix XI ∈ RD×nI is composed of the extracted columns from X  and CI ∈ RnI×nI is
composed of the corresponding columns and rows in C.
The objective f0(W) in Eq.(4) consists of small matrices with size Rr×r and Rr×d. Our idea is to
ﬁrst ﬁnd the descent direction for small matrices  and then maps it back to get the descent direction of
large matrix L ∈ Rd×D. Matrix XI can be decomposed as XI = UI ΣI V(cid:62)
I   and the complexity of
decomposition is signiﬁcantly reduced from O(Dnr) to O(Dn2
I ) on this mini-batch. According to
√
Eq.(2)  a matrix BI can be represented as BI = LUI ΣI. Using SVD  matrix BI can be decomposed
as BI = QI diag(

I   and then the variable W for objective f0(W) can be represented as

sI )P(cid:62)

W = B(cid:62)

I BI = PI diag(sI )P(cid:62)
I .

(11)

In FLRML  in order to convert f0(W) into f (P)  the initial value of s satisﬁes the condition s = ˆs.
But in M-FLRML  sI is generated from BI  so generally this condition is not satisﬁed. So instead 
we take PI and sI as two variables  and ﬁnd the descent direction of them separately. In Mini-batch
FLRML  when a different mini-batch is taken in next iteration  the predicted Barzilai-Borwein step
length tends to be improper  so we use “projection and retraction” instead. The updated matrix ˆPI is
obtained as

ˆPI = retract(PI − GI + PI G(cid:62)

(12)
√
For sI  we use Eq.(5) to get an updated vector ˆsI. Then the updated matrix for BI can be obtained as
ˆsI ) ˆP(cid:62)
ˆBI = QI diag(
I . By mapping ˆBI back to the high-dimensional space  the descent direction
of L can be obtained as

I PI ).

(cid:112)

∆L = QI diag(

ˆsI ) ˆP(cid:62)

I Σ−1

I U(cid:62)

I − L.

For the I-th mini-batch  L is updated at a decreasing ratio as L ← L + 1√
analysis of the stochastic strategy which updates in step sizes by 1√
The outline of M-FLRML is shown in Algorithm 2.

I

(13)
∆L. The theoretical
can refer to the reference [18].

I

4 Experiments

4.1 Experiment Setup

In the experiments  our FLRML and M-FLRML are compared with 5 state-of-the-art low-rank
metric learning methods  including LRSML [1]  FRML [2]  LMNN [34]  SGDINC [18]  and
DRML [3]. For these methods  the complexities  maximum variable size and maximum constant
size in one iteration are compared in Table 2. Considering that d (cid:28) D and nI (cid:28) n  the relatively
small items in the table are omitted.
In addition  four state-of-the-art metric learning methods of other types are also compared  including
one sparse method (SCML [23])  one online method (OASIS [6])  and two non-iterative methods
(KISSME [27]  RMML [28]).
The methods are evaluated on eight datasets with high dimensions or large numbers of samples: three
datasets NG20  RCV1-4 and TDT2-30 derived from three text collections respectively [35  36]; one

6

n
15 935
4 813
4 697
60 000
47 892
118 116
47 892
118 116

dataset
NG20
RCV1
TDT2
MNIST
M10-16
M40-16
M10-100
M40-100

D
62 061
29 992
36 771
780
4 096
4 096
1 000 000
1 000 000

Table 1: The datasets used in the experiments.

Table 2: The complexity  variable matrix size and
constant matrix size of 7 low-rank metric learning
methods (in one iteration).
methods
LMNN
LRSML
FRML
DRML
SGDINC
FLRML
M-FLRML Dn2

size(const)
nr
n2
Dn
D|T |
DnI
nr
DnI
Table 3: The classiﬁcation accuracy (left) and training time (right  in seconds) of 7 metric learning
methods with SVD preprocessing.
RCV1

complexity
|T |r2 + r3
n2d
Dn2 + D2d
D2|T | + D2d
Dd2 + Dn2
I
nrd + nr2
I + DnI d

size(var)
|T |r + r2
n2
D2
Dd
Dd
r2 + nd
Dd

ntest
3 993
4 812
4 697
10 000
10 896
29 616
10 896
29 616

ncat
20
4
30
10
10
40
10
40

M10-16

M40-16

MNIST

NG20

TDT2

24.3%
67.8%
43.7%
62.2%
46.9%
75.7%
80.2%

191
405
627
342
1680
90
227
37

91
368
84.6%
224
92.1%
476
66.5%
261
88.4%
93.7%
50
94.2% 391
14
93.5%

107
306
89.4%
220
95.9%
87.5%
509
97.6% 1850
1093
85.3%
371
97.0%
10
96.3%

10
21
97.7%
74
95.7%
97.6%
18
97.8% 3634

M

90.5%
95.9%

datasets
tsvd
OASIS
KISSME
RMML
LMNN
LRSML
FRML
FLRML

355
492
1750
366

83.3%
76.2%
82.1%

M
M

67.9%
40.8%

814
380
7900

M
M
M

48
7

76.1%
83.4%

172
41

69.1%
75.0%

931
101

Table 4: The classiﬁcation accuracy (left) and training time (right  in seconds) of 4 metric learning
methods without SVD preprocessing.
datasets
SCML
DRML
SGDINC
M-FLRML 54.2% 26 92.7%

25.1% 2600 91.4% 216 82.2% 750 82.1% 1109 72.5% 6326
54.0% 3399 94.2% 1367 96.9% 1121 97.6% 44 83.5% 174 74.2% 163 56.9% 4308 35.5% 5705
2 82.7% 637 73.8% 654

93.9% 8508 96.9% 211 91.3% 310

14 96.1%

11 95.0%

2 74.0%

1 83.0%

M10-100

M40-100

M10-16

M40-16

MNIST

RCV1

NG20

TDT2

E
M

M
M

M
M

M

E

handwritten characters dataset MNIST [37]; four voxel datasets of 3D models M10-16  M10-100 
M40-16  and M40-100 with different resolutions in 163 and 1003 dimensions  respectively  generated
from “ModelNet10” and “ModelNet40” [38] which are widely used in 3D shape understanding
[39  40  41  42  43  44  45  46  47  48  49  50]. To measure the similarity  the data vectors are
normalized to the unit length. The dimensions D  the number of training samples n  the number of
test samples ntest  and the number of categories ncat of all the datasets are listed in Table 1.
Different methods have different requirements for SVD preprocessing. In our experiments  a fast
SVD algorithm [51] is adopted. The time tsvd in SVD preprocessing is listed at the top of Table 3.
Using the same decomposed matrices as input  seven methods are compared: three methods (LRSML 
LMNN  and our FLRML) require SVD preprocessing; four methods (FRML  KISSME  RMML 
OASIS) do not mention SVD preprocessing  but since they need to optimize large dense RD×D
matrices  SVD has to be performed to prevent them from out-of-memory error on high-dimensional
datasets. For all these methods  the rank for SVD is set as r = min(rank(X)  3000). The rest
four methods (SCML  DRML  SGDINC  and our M-FLRML) claim that there is no need for SVD
preprocessing  which are compared using the original data matrices as input. Speciﬁcally  since the
SVD calculation for datasets M10-100 and M40-100 has exceeded the memory limit of common PCs 
only these four methods are tested on these two datasets.
Most tested methods use either pairwise or triplet constraints  except for LMNN and FRML that
requires directly inputting the labels in the implemented codes. For the other methods  5 triplets
are randomly generated for each sample  which is also used as 5 positive pairs and 5 negative
pairs for the methods using pairwise constraints. The accuracy is evaluated by a 5-NN classiﬁer
using the output metric of each method. For each low-rank metric learning method  the rank
constraint for M is set as d = 100. All the experiments are performed on the Matlab R2015a
platform on a PC with 3.60GHz processor and 16GB of physical memory. The code is available at
https://github.com/highan911/FLRML.

4.2 Experimental Results

Table 3 and Table 4 list the classiﬁcation accuracy (left) and training time (right  in seconds) of all the
compared metric learning methods in all the datasets. The symbol “E” indicates that the objective

7

Figure 2: The convergence behavior of FLRML
in optimization on 6 datasets.

Figure 3: The change in accuracy of FLRML
with different m/ ¯ly values on 6 datasets.

Figure 4: The change in accuracy of M-FLRML on “TDT2” with different Nt values and number of
mini-batches.

fails to converge to a ﬁnite non-zero solution  and “M” indicates that its computation was aborted
due to out-of-memory error. The maximum accuracy and minimum time usage for each dataset are
boldly emphasized.
Comparing the results with the analysis of complexity in Table 2  we ﬁnd that for many tested
methods  if the complexity or matrix is a polynomial of D  n or |T |  the efﬁciency on datasets with
large numbers of samples is still limited. As shown in Table 3 and Table 4  FLRML and M-FLRML
are faster than the state-of-the-art methods on all datasets. Our methods can achieve comparable
accuracy with the state-of-the-art methods on all datasets  and obtain the highest accuracy on several
datasets with both high dimensions and large numbers of samples.
Both our M-FLRML and SGDINC use mini-batches to improve efﬁciency. The theoretical complexity
of these two methods is close  but in the experiment M-FLRML is faster. Generally  M-FLRML is
less accurate than FLRML  but it signiﬁcantly reduces the time and memory usage on large datasets.
In the experiments  the largest dataset “M40-100” is with size 1  000  000 × 118  116. If there is a
dense matrix of such size  it will take up 880 GB of memory. When using M-FLRML to process this
data set  the recorded maximum memory usage of Matlab is only 6.20 GB (Matlab takes up 0.95
GB of memory on startup). The experiment shows that M-FLRML is suitable for metric learning of
large-scale high-dimensional data on devices with limited memory.
In the experiments  we ﬁnd the initialization of s usually converges within 3 iterations. The optimiza-
tion on the Stiefel manifold usually converges in less than 15 iterations. Figure 2 shows the samples
of convergence behavior of FLRML in optimization on each dataset. The plots are drawn in relative
values  in which the values of ﬁrst iteration are scaled to 1.
In FLRML  one parameter m is about the margin in the margin loss. An experiment is performed
to study the effect of the margin parameter m on accuracy. Let ¯ly be the mean of y(cid:62)
i yi values  i.e.
i yi). We test the change in accuracy of FLRML when the ratio m/ ¯ly varies between
¯ly = 1
n
0.1 and 2. The mean values and standard deviations of 5 repeated runs are plotted in Figure 3  which
shows that FLRML works well on most datasets when m/ ¯ly is around 1. So we use m/ ¯ly = 1 in the
experiments in Table 3 and Table 4.
In M-FLRML  another parameter is the number of triplets Nt used to generate a mini-batch. We
test the effect of Nt on the accuracy of M-FLRML with the increasing number of mini-batches. The
mean values and standard deviations of 5 repeated runs are plotted in Figure 4  which shows that
a larger Nt makes the accuracy increase faster  and usually M-FLRML is able to get good results
within 20 mini-batches. So in Table 4  all the results are obtained with Nt = 80 and T = 20.

(cid:80)n
i=1(y(cid:62)

8

iterations2468101214relative objective value00.51NG20RCV1TDT2MNISTM10-16M40-16m=7ly00.511.52accuracy0.60.70.80.91NG20RCV1TDT2MNISTM10-16M40-16mini-batches05101520accuracy0.80.850.90.951Nt=20Nt=40Nt=805 Conclusion and Future Work

In this paper  FLRML and M-FLRML are proposed for efﬁcient low-rank similarity metric learning
on high-dimensional datasets with large numbers of samples. With a novel formulation  FLRML and
M-FLRML can better employ low-rank constraints to further reduce the complexity and matrix size 
based on which optimization is efﬁciently conducted on Stiefel manifold. This enables FLRML and
M-FLRML to achieve good accuracy and faster speed on large numbers of high-dimensional data.
One limitation of our current implementation of FLRML and M-FLRML is that the algorithm still
runs on a single processor. Recently  there is a trend about distributed metric learning for big data
[52  53]. It is an interest of our future research to implement M-FLRML on distributed architecture
for scaling to larger datasets.

Acknowledgments

This research is sponsored in part by the National Key R&D Program of China (No. 2018YF-
B0505400  2016QY07X1402)  the National Science and Technology Major Project of China (No.
2016ZX 01038101)  and the NSFC Program (No. 61527812).

References
[1] W. Liu  C. Mu  R. Ji  S. Ma  J. Smith  S. Chang  Low-rank similarity metric learning in high

dimensions  in: AAAI  2015  pp. 2792–2799.

[2] Y. Mu  Fixed-rank supervised metric learning on Riemannian manifold  in: AAAI  2016  pp.

1941–1947.

[3] M. Harandi  M. Salzmann  R. Hartley  Joint dimensionality reduction and metric learning: a

geometric take  in: ICML  2017.

[4] Z. Ding  Y. Fu  Robust transfer metric learning for image classiﬁcation  IEEE Transactions on

Image Processing PP (99) (2017) 1–1.

[5] L. Luo  H. Huang  Matrix variate gaussian mixture distribution steered robust metric learning 

in: AAAI  2018.

[6] G. Chechik  U. Shalit  V. Sharma  S. Bengio  An online algorithm for large scale image similarity

learning  in: Advances in Neural Information Processing Systems  2009  pp. 306–314.

[7] U. Shalit  D. Weinshall  G. Chechik  Online learning in the embedded manifold of low-rank

matrices  Journal of Machine Learning Research 13 (Feb) (2012) 429–458.

[8] A. Bellet  A. Habrard  M. Sebban  A survey on metric learning for feature vectors and structured

data  arXiv preprint arXiv:1306.6709.

[9] F. Wang  J. Sun  Survey on distance metric learning and dimensionality reduction in data mining 

Data Mining and Knowledge Discovery 29 (2) (2015) 534–564.

[10] Z. Wen  W. Yin  A feasible method for optimization with orthogonality constraints  Mathemati-

cal Programming 142 (1-2) (2013) 397–434.

[11] M. Schultz  T. Joachims  Learning a distance metric from relative comparisons  in: Advances in

Neural Information Processing Systems  2004  pp. 41–48.

[12] J. T. Kwok  I. W. Tsang  Learning with idealized kernels  in: ICML  2003  pp. 400–407.
[13] C. Hegde  A. C. Sankaranarayanan  W. Yin  R. G. Baraniuk  Numax: A convex approach for
learning near-isometric linear embeddings  IEEE Transactions on Signal Processing 63 (22)
(2015) 6109–6121.

[14] B. Mason  L. Jain  R. Nowak  Learning low-dimensional metrics  in: Advances in Neural

Information Processing Systems  2017  pp. 4139–4147.

[15] L. Cheng  Riemannian similarity learning  in: ICML  2013  pp. 540–548.
[16] Z. Huang  R. Wang  S. Shan  X. Chen  Projection metric learning on Grassmann manifold with

application to video based face recognition  in: CVPR  2015  pp. 140–149.

[17] A. Shukla  S. Anand  Distance metric learning by optimization on the Stiefel manifold  in:
International Workshop on Differential Geometry in Computer Vision for Analysis of Shapes 
Images and Trajectories  2015.

9

[18] J. Zhang  L. Zhang  Efﬁcient stochastic optimization for low-rank distance metric learning  in:

AAAI  2017.

[19] S. Gillen  C. Jung  M. Kearns  A. Roth  Online learning with an unknown fairness metric  in:

Advances in Neural Information Processing Systems  2018  pp. 2600–2609.

[20] W. Li  Y. Gao  L. Wang  L. Zhou  J. Huo  Y. Shi  OPML: A one-pass closed-form solution for

online metric learning  Pattern Recognition 75 (2018) 302–314.

[21] G.-J. Qi  J. Tang  Z.-J. Zha  T.-S. Chua  H.-J. Zhang  An efﬁcient sparse metric learning in
high-dimensional space via l 1-penalized log-determinant regularization  in: ICML  2009  pp.
841–848.

[22] A. Bellet  A. Habrard  M. Sebban  Similarity learning for provably accurate sparse linear

classiﬁcation  in: ICML  2012  pp. 1871–1878.

[23] Y. Shi  A. Bellet  F. Sha  Sparse compositional metric learning  in: AAAI  2014.
[24] K. Liu  A. Bellet  F. Sha  Similarity learning for high-dimensional sparse data  in: AISTATS 

2015.

[25] L. Zhang  T. Yang  R. Jin  Z. hua Zhou  Sparse learning for large-scale and high-dimensional

data: a randomized convex-concave optimization approach  in: ALT  2016  pp. 83–97.

[26] W. Liu  J. He  S.-F. Chang  Large graph construction for scalable semi-supervised learning  in:
Proceedings of the 27th international conference on machine learning (ICML-10)  2010  pp.
679–686.

[27] M. Koestinger  M. Hirzer  P. Wohlhart  P. Roth  H. Bischof  Large scale metric learning from
equivalence constraints  in: Computer Vision and Pattern Recognition (CVPR)  2012  pp.
2288–2295.

[28] P. Zhu  H. Cheng  Q. Hu  Q. Wang  C. Zhang  Towards generalized and efﬁcient metric learning

on riemannian manifold  in: IJCAI  2018  pp. 3235–3241.

[29] B. Kulis  Metric learning: A survey  Foundations and Trends R(cid:13) in Machine Learning 5 (4)

(2013) 287–364.

[30] P.-A. Absil  J. Malick  Projection-like retractions on matrix manifolds  SIAM Journal on

Optimization 22 (1) (2012) 135–158.

[31] H. Zhang  W. W. Hager  A nonmonotone line search technique and its application to uncon-

strained optimization  SIAM journal on Optimization 14 (4) (2004) 1043–1056.

[32] J. Barzilai  J. M. Borwein  Two-point step size gradient methods  IMA Journal of Numerical

Analysis 8 (1) (1988) 141–148.

[33] Q. Qian  R. Jin  J. Yi  L. Zhang  S. Zhu  Efﬁcient distance metric learning by adaptive sampling
and mini-batch stochastic gradient descent (SGD)  Machine Learning 99 (3) (2015) 353–372.
[34] K. Weinberger  L. Saul  Distance metric learning for large margin nearest neighbor classiﬁcation 

Journal of Machine Learning Research 10 (Feb) (2009) 207–244.

[35] K. Lang  Newsweeder: Learning to ﬁlter netnews  in: ICML  1995  pp. 331–339.
[36] D. Cai  X. He  Manifold adaptive experimental design for text categorization  IEEE Transactions

on Knowledge and Data Engineering 24 (4) (2012) 707–719.

[37] Y. LeCun  L. Bottou  Y. Bengio  P. Haffner  Gradient-based learning applied to document

recognition  Proceedings of the IEEE 86 (11) (1998) 2278–2324.

[38] Z. Wu  S. Song  A. Khosla  F. Yu  L. Zhang  X. Tang  J. Xiao  3D ShapeNets: A deep

representation for volumetric shapes  in: CVPR  2015  pp. 1912–1920.

[39] Z. Han  M. Shang  Y.-S. Liu  M. Zwicker  View Inter-Prediction GAN: Unsupervised representa-
tion learning for 3D shapes by learning global shape memories to support local view predictions 
in: AAAI  2019.

[40] Z. Han  X. Liu  Y.-S. Liu  M. Zwicker  Parts4Feature: Learning 3D global features from

generally semantic parts in multiple views  in: IJCAI  2019.

[41] Z. Han  X. Wang  C.-M. Vong  Y.-S. Liu  M. Zwicker  C. P. Chen  3DViewGraph: Learning
global features for 3d shapes from a graph of unordered views with attention  in: IJCAI  2019.

10

[42] X. Liu  Z. Han  Y.-S. Liu  M. Zwicker  Point2Sequence: Learning the shape representation of

3D point clouds with an attention-based sequence to sequence network  in: AAAI  2019.

[43] Z. Han  X. Wang  Y.-S. Liu  M. Zwicker  Multi-angle point cloud-vae:unsupervised feature
learning for 3D point clouds from multiple angles by joint self-reconstruction and half-to-half
prediction  in: ICCV  2019.

[44] Z. Han  H. Lu  Z. Liu  C.-M. Vong  Y.-S. Liu  M. Zwicker  J. Han  C. P. Chen  3D2SeqViews:
Aggregating sequential views for 3d global feature learning by cnn with hierarchical attention
aggregation  IEEE Transactions on Image Processing 28 (8) (2019) 3986–3999.

[45] Z. Han  M. Shang  Z. Liu  C.-M. Vong  Y.-S. Liu  M. Zwicker  J. Han  C. P. Chen  Se-
qViews2SeqLabels: Learning 3D global features via aggregating sequential views by rnn with
attention  IEEE Transactions on Image Processing 28 (2) (2019) 685–672.

[46] X. Liu  Z. Han  W. Xin  Y.-S. Liu  M. Zwicker  L2g auto-encoder: Understanding point clouds

by local-to-global reconstruction with hierarchical self-attention  in: ACMMM  2019.

[47] Z. Han  M. Shang  X. Wang  Y.-S. Liu  M. Zwicker  Y2Seq2Seq: Cross-modal representation
learning for 3D shape and text by joint reconstruction and prediction of view and word sequences 
in: AAAI  2019.

[48] Z. Han  Z. Liu  C.-M. Vong  Y.-S. Liu  S. Bu  J. Han  C. Chen  BoSCC: Bag of spatial context
correlations for spatially enhanced 3D shape representation  IEEE Transactions on Image
Processing 26 (8) (2017) 3707–3720.

[49] Z. Han  Z. Liu  J. Han  C.-M. Vong  S. Bu  C. Chen  Unsupervised learning of 3D local features
from raw voxels based on a novel permutation voxelization strategy  IEEE Transactions on
Cybernetics 49 (2) (2019) 481–494.

[50] Z. Han  Z. Liu  C.-M. Vong  Y.-S. Liu  S. Bu  J. Han  C. Chen  Deep Spatiality: Unsupervised
learning of spatially-enhanced global and local 3D features by deep neural network with coupled
softmax  IEEE Transactions on Image Processing 27 (6) (2018) 3049–3063.

[51] H. Li  G. Linderman  A. Szlam  K. Stanton  Y. Kluger  M. Tygert  Algorithm 971: An imple-
mentation of a randomized algorithm for principal component analysis  ACM Transactions on
Mathematical Software 43 (3) (2017) 28:1–28:14.

[52] Y. Su  H. Yang  I. King  M. Lyu  Distributed information-theoretic metric learning in apache

spark  in: IJCNN  2016  pp. 3306–3313.

[53] E. P. Xing  Q. Ho  W. Dai  J. K. Kim  J. Wei  S. Lee  X. Zheng  P. Xie  A. Kumar  Y. Yu  Petuum:
a new platform for distributed machine learning on big data  IEEE Transactions on Big Data
1 (2) (2015) 49–67.

11

,Han Liu
Zhizhong Han
Yu-Shen Liu
Ming Gu