2012,Super-Bit Locality-Sensitive Hashing,Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic dimension reduction method which provides an unbiased estimate of angular similarity  yet suffers from the large variance of its estimation. In this work  we propose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement  which orthogonalizes the random projection vectors in batches  and it is theoretically guaranteed that SBLSH also provides an unbiased estimate of angular similarity  yet with a smaller variance when the angle to estimate is within $(0 \pi/2]$. The extensive experiments on real data well validate that given the same length of binary code  SBLSH may achieve significant mean squared error reduction in estimating pairwise angular similarity. Moreover  SBLSH shows the superiority over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.,Super-Bit Locality-Sensitive Hashing

Jianqiu Ji⇤  Jianmin Li⇤  Shuicheng Yan†  Bo Zhang⇤  Qi Tian‡

⇤State Key Laboratory of Intelligent Technology and Systems 

Tsinghua National Laboratory for Information Science and Technology (TNList) 

Department of Computer Science and Technology 

Tsinghua University  Beijing 100084  China
jijq10@mails.tsinghua.edu.cn 

{lijianmin  dcszb}@mail.tsinghua.edu.cn
†Department of Electrical and Computer Engineering 
National University of Singapore  Singapore  117576

eleyans@nus.edu.sg

‡Department of Computer Science  University of Texas at San Antonio 

One UTSA Circle  University of Texas at San Antonio  San Antonio  TX 78249-1644

qitian@cs.utsa.edu

Abstract

Sign-random-projection locality-sensitive hashing (SRP-LSH) is a probabilistic
dimension reduction method which provides an unbiased estimate of angular sim-
ilarity  yet suffers from the large variance of its estimation. In this work  we pro-
pose the Super-Bit locality-sensitive hashing (SBLSH). It is easy to implement 
which orthogonalizes the random projection vectors in batches  and it is theoreti-
cally guaranteed that SBLSH also provides an unbiased estimate of angular sim-
ilarity  yet with a smaller variance when the angle to estimate is within (0 ⇡/ 2].
The extensive experiments on real data well validate that given the same length
of binary code  SBLSH may achieve signiﬁcant mean squared error reduction in
estimating pairwise angular similarity. Moreover  SBLSH shows the superiority
over SRP-LSH in approximate nearest neighbor (ANN) retrieval experiments.

1

Introduction

Locality-sensitive hashing (LSH) method aims to hash similar data samples to the same hash code
with high probability [7  9]. There exist various kinds of LSH for approximating different distances
or similarities  e.g.  bit-sampling LSH [9  7] for Hamming distance and `1-distance  min-hash [2  5]
for Jaccard coefﬁcient. Among them are some binary LSH schemes  which generate binary codes.
Binary LSH approximates a certain distance or similarity of two data samples by computing the
Hamming distance between the corresponding compact binary codes. Since computing Hamming
distance involves mainly bitwise operations  it is much faster than directly computing other dis-
tances  e.g. Euclidean  cosine  which require many arithmetic operations. On the other hand  the
storage is substantially reduced due to the use of compact binary codes. In large-scale applications
[22  11  5  17]  e.g. near-duplicate image detection  object and scene recognition  etc.  we are often
confronted with the intensive computing of distances or similarities between samples  then binary
LSH may act as a scalable solution.

1.1 Locality-Sensitive Hashing for Angular Similarity

For many data representations  the natural pairwise similarity is only related with the angle between
the data  e.g.  the normalized bag-of-words representation for documents  images  and videos  and
the normalized histogram-based local features like SIFT [20]. In these cases  angular similarity

1

can serve as a similarity measurement  which is deﬁned as sim(a  b) = 1 cos1( ha bi
kakkbk
ha  bi denotes the inner product of a and b  and k·k denotes the `2-norm of a vector.
One popular LSH for approximating angular similarity is the sign-random-projection LSH (SRP-
LSH) [3]  which provides an unbiased estimate of angular similarity and is a binary LSH method.
Formally  in a d-dimensional data space  let v denote a random vector sampled from the normal
distribution N (0  Id)  and x denote a data sample  then an SRP-LSH function is deﬁned as hv(x) =
sgn(vT x)  where the sign function sgn(·) is deﬁned as
sgn(z) =⇢ 1 

z  0
z < 0

)/⇡. Here

0 

Given two data samples a  b  let ✓a b = cos1( ha bi
kakkbk

)  then it can be proven that [8]

P r(hv(a) 6= hv(b)) = ✓a b

⇡

This property well explains the essence of locality-sensitive  and also reveals the relation between
Hamming distance and angular similarity.
By independently sampling K d-dimensional vectors v1  ...  vK from the normal distribution
N (0  Id)  we may deﬁne a function h(x) = (hv1(x)  hv2(x)  ...  hvK (x))  which consists of K
SRP-LSH functions and thus produces K-bit codes. Then it is easy to prove that

E[dHamming(h(a)  h(b))] = K✓a b

⇡ = C✓a b

That is  the expectation of the Hamming distance between the binary hash codes of two given data
samples a and b is an unbiased estimate of their angle ✓a b  up to a constant scale factor C = K/⇡.
Thus SRP-LSH provides an unbiased estimate of angular similarity.
Since dHamming(h(a)  h(b)) follows a binomial distribution 
i.e.
B(K  ✓a b
⇡ ) 
implies
dHamming(h(a)  h(b))/K  i.e. V ar[dHamming(h(a)  h(b))/K]  satisﬁes

dHamming(h(a)  h(b)) ⇠
the variance of

(1  ✓a b
⇡ ).

its variance

This

is K✓a b

⇡

that

V ar[dHamming(h(a)  h(b))/K] = ✓a b

K⇡ (1  ✓a b
⇡ )

Though being widely used  SRP-LSH suffers from the large variance of its estimation  which leads
to large estimation error. Generally we need a substantially long code to accurately approximate
the angular similarity [24  12  23]. Since any two of the random vectors may be close to being
linearly dependent  the resulting binary code may be less informative as it seems  and even contains
many redundant bits. An intuitive idea would be to orthogonalize the random vectors. However 
once being orthogonalized  the random vectors can no longer be viewed as independently sampled.
Moreover  it remains unclear whether the resulting Hamming distance is still an unbiased estimate
of the angle ✓a b multiplied by a constant  and what its variance will be. Later we will give answers
with theoretical justiﬁcations to these two questions.
In the next section  based on the above intuitive idea  we propose the so-called Super-Bit locality-
sensitive hashing (SBLSH) method. We provide theoretical guarantees that after orthogonalizing the
random projection vectors in batches  we still get an unbiased estimate of angular similarity  yet with
a smaller variance when ✓a b 2 (0 ⇡/ 2]  and thus the resulting binary code is more informative. Ex-
periments on real data show the effectiveness of SBLSH  which with the same length of binary code
may achieve as much as 30% mean squared error (MSE) reduction compared with the SRP-LSH in
estimating angular similarity on real data. Moreover  SBLSH performs best among several widely
used data-independent LSH methods in approximate nearest neighbor (ANN) retrieval experiments.

2 Super-Bit Locality-Sensitive Hashing
The proposed SBLSH is founded on SRP-LSH. When the code length K satisﬁes 1 < K  d 
where d is the dimension of data space  we can orthogonalize N (1  N  min(K  d) = K) of the
random vectors sampled from the normal distribution N (0  Id). The orthogonalization procedure

2

is the Gram-Schmidt process  which projects the current vector orthogonally onto the orthogonal
complement of the subspace spanned by the previous vectors. After orthogonalization  these N
random vectors can no longer be viewed as independently sampled  thus we group their resulting
bits together as an N-Super-Bit. We call N the Super-Bit depth.
However  when the code length K > d  it is impossible to orthogonalize all K vectors. Assume
that K = N ⇥ L without loss of generality  and 1  N  d  then we can perform the Gram-
Schmidt process to orthogonalize them in L batches. Formally  K random vectors {v1  v2...  vK}
are independently sampled from the normal distribution N (0  Id)  and then divided into L batches
with N vectors each. By performing the Gram-Schmidt process to these L batches of N vectors
respectively  we get K = N ⇥ L projection vectors {w1  w2...  wK}. This results in K SBLSH
functions (hw1  hw2...  hwK )  where hwi(x) = sgn(wT
i x). These K functions produce L N-Super-
Bits and altogether produce binary codes of length K. Figure 1 shows an example of generating
12 SBLSH projection vectors. Algorithm 1 lists the algorithm for generating SBLSH projection
vectors. Note that when the Super-Bit depth N = 1  SBLSH becomes SRP-LSH. In other words 
SRP-LSH is a special case of SBLSH. The algorithm can be easily extended to the case when the
code length K is not a multiple of the Super-Bit depth N. In fact one can even use variable Super-Bit
depth Ni as long as 1  Ni  d. With the same code length  SBLSH has the same running time
O(Kd) as SRP-LSH in on-line processing  i.e. generating binary codes when applying to data.

Figure 1: An illustration of 12 SBLSH projection vectors {wi} generated by orthogonalizing {vi}
in 4 batches.

Algorithm 1 Generating Super-Bit Locality-Sensitive Hashing Projection Vectors

Input: Data space dimension d  Super-Bit depth 1  N  d  number of Super-Bit L  1 
resulting code length K = N ⇥ L.
Generate a random matrix H with each element sampled independently from the normal distribu-
tion N (0  1)  with each column normalized to unit length. Denote H = [v1  v2  ...  vK].
for i = 0 to L  1 do
for j = 1 to N do
wiN +j = viN +j.
for k = 1 to j  1 do
end for
wiN +j = wiN +j/kwiN +jk.

wiN +j = wiN +j  wiN +kwT

iN +kviN +j.

end for

end for
Output: ˜H = [w1  w2  ...  wK].

2.1 Unbiased Estimate
In this subsection we prove that SBLSH provides an unbiased estimate of ✓a b of a  b 2 Rd.
Lemma 1. ([8]  Lemma 3.2) Let S d1 denote the unit sphere in Rd. Given a random vector v
uniformly sampled from S d1  we have P r[hv(a) 6= hv(b)] = ✓a b/⇡.
Lemma 2. If v 2 Rd follows an isotropic distribution  then ¯v = v/kvk is uniformly distributed on
S d1.
This lemma can be proven by the deﬁnition of isotropic distribution  and we omit the details here.

3

2 1v1w2w3w4w5w6w7w8w9w10w11w12w7v8v9v4v5v6v1v2v12v11v10v3v Orthogonalize in 4 batches Random projection vectors sampled from N(0  I) Resulting SBLSH projection vectors Lemma 3. Given k vectors v1  ...  vk 2 Rd  which are sampled i.i.d. from the normal distribution
N (0  Id)  and span a subspace Sk  let PSk denote the orthogonal projection onto Sk  then PSk is a
random matrix uniformly distributed on the Grassmann manifold Gk dk.
This lemma can be proven by applying the Theorem 2.2.1(iii)  Theorem 2.2.2(iii) in [4].
Lemma 4. If P is a random matrix uniformly distributed on the Grassmann manifold Gk dk 
1  k  d  and v ⇠N (0  Id) is independent of P   then the random vector ˜v = P v follows an
isotropic distribution.

From the uniformity of P on the Grassmann manifold and the property of the normal distribution
N (0  Id)  we can get this result directly. We give a sketch of proof below.
Proof. We can write P = U U T   where the columns of U = [u1  u2  ...  uk] constitute an orthonor-
mal basis of a random k-dimensional subspace. Since the standard normal distribution is 2-stable
[6]  ˆv = U T v = [ ˆv1  ˆv2  ...  ˆvk]T is a N (0  Ik)-distributed vector  where each ˆvi ⇠N (0  1)  and it
is easy to verify that ˆv is independent of U. Therefore ˜v = P v = U ˆv =⌃ k
i=1 ˆviui. Since ui  ...  uk
can be any orthonormal basis of any k-dimensional subspace with equal probability density  and
{ ˆv1  ˆv2  ...  ˆvk} are i.i.d. N (0  1) random variables  ˜v follows an isotropic distribution.
Theorem 1. Given N i.i.d. random vectors v1  v2  ...  vN 2 Rd sampled from the normal distri-
bution N (0  Id)  where 1  N  d  perform the Gram-Schmidt process on them and produce N
orthogonalized vectors w1  w2  . . .   wN  then for any two data vectors a  b 2 Rd  by deﬁning N
indicator random variables X1  X2  ...  XN as

Xi =⇢ 1  hwi(a) 6= hwi(b)

0  hwi(a) = hwi(b)

we have E[Xi] = ✓a b/⇡  for any 1  i  N.
Proof. Denote Si1 the subspace spanned by {w1  ...  wi1}  and the orthogonal projection onto its
orthogonal complement as P ?Si1. Then wi = P ?Si1vi. Denote ¯w = wi/kwik.
For any 1  i  N  E[Xi] = P r[Xi = 1] = P r[hwi(a) 6= hwi(b)] = P r[h ¯w(a) 6= h ¯w(b)]. For
i = 1  by Lemma 2 and Lemma 1  we have P r[X1 = 1] = ✓a b/⇡.
For any 1 < i  N  consider the distribution of wi. By lemma 3  PSi1 is a random matrix
uniformly distributed on the Grassmann manifold Gi1 di+1  thus P ?Si1 = I  PSi1 is uniformly
distributed on Gdi+1 i1. Since vi ⇠N (0  Id) is independent of v1  v2  ...  vi1  vi is independent
of P ?Si1. By Lemma 4  we have that wi = P ?Si1vi follows an isotropic distribution. By Lemma
2  ¯w = wi/kwik is uniformly distributed on the unit sphere in Rd. By Lemma 1  P r[h ¯w(a) 6=
h ¯w(b)] = ✓a b/⇡.
Corollary 1. For any Super-Bit depth N  1  N  d  assuming that the code length K = N ⇥ L 
the Hamming distance dHamming(h(a)  h(b)) is an unbiased estimate of ✓a b  for any two data
vectors a and b 2 Rd  up to a constant scale factor C = K/⇡.
Proof. Apply Theorem 1 and we get E[dHamming(h(a)  h(b))] = L ⇥ E[⌃N
⌃N
i=1E[Xi] = L ⇥ ⌃N
2.2 Variance
In this subsection we prove that when the angle ✓a b 2 (0 ⇡/ 2]  the variance of SBLSH is strictly
smaller than that of SRP-LSH.
Lemma 5. For the random variables {Xi} deﬁned in Theorem 1  we have the following equality
P r[Xi = 1|Xj = 1] = P r[Xi = 1|X1 = 1]  1  j < i  N  d.
Proof. P r[Xi = 1|Xj = 1] = P r[hwi(a) 6= hwi(b)|Xj = 1] = P r[hvi⌃i1
hvi⌃i1

(a) 6=
(b)|hwj (a) 6= hwj (b)]. Since {w1  ...wi1} is a uniformly random orthonormal

i=1Xi] = L ⇥

k=1wkwT

i=1✓a b/⇡ = K✓a b

⇡ = C✓a b.

k=1wkwT

k vi

k vi

4

k=1wkwT

k vi

(a) 6= hvi⌃i1

k=1wkwT

k vi

⇡ +N(N1) p2 1✓a b

⇡ ( N✓ a b

2 ]  we have P r[X2 = 1|X1 = 1] < ✓a b
⇡ .

basis of a random subspace uniformly distributed on Grassmann manifold  by exchanging the index
j and 1 we have that equals P r[hvi⌃i1
(b)|hw1(a) 6= hw1(b)] =
P r[Xi = 1|X1 = 1].
Lemma 6. For {Xi} deﬁned in Theorem 1  we have P r[Xi = 1|Xj = 1] = P r[X2 = 1|X1 = 1] 
1  j < i  N  d. Given ✓a b 2 (0  ⇡
The proof of this lemma is long  thus we provide it in the Appendix (in supplementary ﬁle).
Theorem 2. Given two vectors a  b 2 Rd and random variables {Xi} deﬁned as in Theorem 1 
denote p2 1 = P r[X2 = 1|X1 = 1]  and SX =⌃ N
i=1Xi which is the Hamming distance between
the N-Super-Bits of a and b  for 1 < N  d  then V ar[SX] = N✓ a b
⇡ )2.
Proof. By Lemma 6  P r[Xi = 1|Xj = 1] = P r[X2 = 1|X1 = 1] = p2 1 when 1  j < i  N.
Therefore P r[Xi = 1  Xj = 1] = P r[Xi = 1|Xj = 1]P r[Xj = 1] = p2 1✓a b
  for any 1  j <
i  N. Therefore V ar[SX] = E[S2
i ] + 2⌃j<iE[XiXj]  N 2E[X1]2 =
⇡ + 2⌃j<iP r[Xi = 1  Xj = 1]  ( N✓ a b
Corollary 2. Denote V ar[SBLSHN K] as the variance of the Hamming distance produced by
SBLSH  where 1  N  d is the Super-Bit depth  and K = N ⇥ L is the code length. Then
V ar[SBLSHN K] = L⇥V ar[SBLSHN N]. Furthermore  given ✓a b 2 (0  ⇡
2 ]  if K = N1⇥L1 =
N2 ⇥ L2 and 1  N2 < N1  d  then V ar[SBLSHN1 K] < V ar[SBLSHN2 K].
Proof. Since v1  v2  ...  vK are independently sampled  and w1  w2  ...  wK are produced by orthog-
onalizing every N vectors  the Hamming distances produced by different N-Super-Bits are inde-
pendent  thus V ar[SBLSHN K] = L ⇥ V ar[SBLSHN N].
Therefore V ar[SBLSHN1 K] = L1⇥( N1✓a b
1) p2 1✓a b
Therefore V ar[SBLSHN1 K]  V ar[SBLSHN2 K] = K✓a b
N1 > N2 = 1  V ar[SBLSHN1 K]  V ar[SBLSHN2 K] = K✓a b
Corollary 3. Denote V ar[SRP LSHK] as the variance of the Hamming distance produced by SRP-
LSH  where K = N ⇥ L is the code length and L is a positive integer  1 < N  d. If ✓a b 2 (0  ⇡
2 ] 
then V ar[SRP LSHK] > V ar[SBLSHN K].

⇡ +K(N1
2 ]  for N1 > N2 > 1  0  p2 1 < ✓a b
⇡ .
⇡ ) < 0. For
⇡ ) < 0

)2) = K✓a b

⇡ ( N1✓a b
(N1  N2)(p2 1  ✓a b
(N1  1)(p2 1  ✓a b

i=1E[X 2
⇡ + N(N  1) p2 1✓a b

⇡  KN1( ✓a b

⇡ )2. By Lemma 6  when ✓a b 2 (0  ⇡

X]  E[SX]2 =⌃ N
⇡ )2 = N✓ a b

⇡ +N1(N11) p2 1✓a b

⇡

⇡

N✓ a b

⇡

⇡  ( N✓ a b

⇡ )2.

⇡

Proof. By Corollary 2  V ar[SRP LSHK] = V ar[SBLSH1 K] > V ar[SBLSHN K].

2.2.1 Numerical veriﬁcation

Figure 2: The variances of SRP-LSH and SBLSH against the angle ✓a b to estimate.

In this subsection we verify numerically the behavior of the variances of both SRP-LSH and SBLSH
for different angles ✓a b 2 (0 ⇡ ]. By Theorem 2  the variance of SBLSH is closely related to p2 1
deﬁned in Theorem 2. We randomly generate 30 points in R10  which involves 435 angles. For
each angle  we numerically approximate p2 1 using sampling method  where the sample number is
1000. We ﬁx K = N = d  and plot the variances V ar[SRP LSHN] and V ar[SBLSHN N] against
various angles ✓a b. Figure 2 shows that when ✓a b 2 (0 ⇡/ 2]  SBLSH has a much smaller variance
than SRP-LSH  which veriﬁes the correctness of Corollary 3 to some extent. Furthermore  Figure 2
shows that even when ✓a b 2 (⇡/2 ⇡ ]  SBLSH still has a smaller variance.

5

 (cid:2024)/2 2.3 Discussion

From Corollary 1  SBLSH provides an unbiased estimate of angular similarity. From Corollary
3  when ✓a b 2 (0 ⇡/ 2]  with the same length of binary code  the variance of SBLSH is strictly
smaller than SRP-LSH. In real applications  many vector representations are limited in non-negative
orthant with all vector entries being non-negative  e.g.  bag-of-words representation of documents
and images  and histogram-based representations like SIFT local descriptor [20]. Usually they are
normalized to unit length  with only their orientations maintained. For this kind of data  the angle
of any two different samples is limited in (0 ⇡/ 2]  and thus SBLSH will provide more accurate
estimation than SRP-LSH on such data. In fact  our later experiments show that even when ✓a b is
not constrained in (0 ⇡/ 2]  SBLSH still gives much more accurate estimate of angular similarity.

3 Experimental Results

We conduct two sets of experiments  angular similarity estimation and approximate nearest neighbor
(ANN) retrieval  to evaluate the effectiveness of our proposed SBLSH method. In the ﬁrst set of
experiments we directly measure the accuracy in estimating pairwise angular similarity. The second
set of experiments then test the performance of SBLSH in real retrieval applications.

3.1 Angular Similarity Estimation

In this experiment  we evaluate the accuracy of estimating pairwise angular similarity on several
datasets. Speciﬁcally  we test the effect to the estimation accuracy when the Super-Bit depth N
varies and the code length K is ﬁxed  and vice versa. For each preprocessed data D  we get DLSH
after performing SRP-LSH  and get DSBLSH after performing the proposed SBLSH. We compute
the angles between each pair of samples in D  the corresponding Hamming distances in DLSH and
DSBLSH. We compute the mean squared error between the true angle and the approximated angles
from DLSH and DSBLSH respectively. Note that after computing the Hamming distance  we divide
the result by C = K/⇡ and get the approximated angle.

3.1.1 Datasets and Preprocessing
We conduct the experiment on the following datasets:
1) Photo Tourism patch dataset1 [26]  Notre Dame  which contains 104 106 patches  each of which
is represented by a 128D SIFT descriptor (Photo Tourism SIFT); and 2) MIR-Flickr2  which con-
tains 25 000 images  each of which is represented by a 3125D bag-of-SIFT-feature histogram;
For each dataset  we further conduct a simple preprocessing step as in [12]  i.e. mean-centering each
data sample  so as to obtain additional mean-centered versions of the above datasets  Photo Tourism
SIFT (mean)  and MIR-Flickr (mean). The experiment on these mean-centered datasets will test the
performance of SBLSH when the angles of data pairs are not constrained in (0 ⇡/ 2].

3.1.2 The Effect of Super-Bit Depth N and Code Length K

Figure 3: The effect of Super-Bit depth N (1 < N  min(d  K)) with ﬁxed code length K (K =
N ⇥ L)  and the effect of code length K with ﬁxed Super-Bit depth N.

1http://phototour.cs.washington.edu/patches/default.htm
2http://users.ecs.soton.ac.uk/jsh2/mirflickr/

6

 SRP-LSH SBLSH Mean+SRP-LSH Mean+SBLSH SRP-LSH SBLSH Mean+SRP-LSH Mean+SBLSH SRP-LSH SBLSH Mean+SRP-LSH Mean+SBLSH SRP-LSH SBLSH Mean+SRP-LSH Mean+SBLSH Table 1: ANN retrieval results  measured by proportion of good neighbors within query’s Hamming
ball of radius 3. Note that the code length K = 30.

Data

E2LSH

SRP-LSH

SBLSH

Notre Dame
Half Dome
Trevi

0.4675 ± 0.0900
0.4503 ± 0.0712
0.4661 ± 0.0849

0.7500 ± 0.0525
0.7137 ± 0.0413
0.7591 ± 0.0464

0.7845± 0.0352
0.7535± 0.0276
0.7891± 0.0329

In each dataset  for each (N  K) pair  i.e. Super-Bit depth N and code length K  we randomly
sample 10 000 data  which involve about 50 000 000 data pairs  and randomly generate SRP-LSH
functions  together with SBLSH functions by orthogonalizing the generated SRP in batches. We
repeat the test for 10 times  and compute the mean squared error (MSE) of the estimation.
To test the effect of Super-Bit depth N  we ﬁx K = 120 for Photo Tourism SIFT and K = 3000 for
MIR-Flickr respectively  and to test the effect of code length K  we ﬁx N = 120 for Photo Tourism
SIFT and N = 3000 for MIR-Flickr. We repeat the experiment on the mean-centered versions of
these datasets  and denote the methods by Mean+SRP-LSH and Mean+SBLSH respectively.
Figure 3 shows that when using ﬁxed code length K  as the Super-Bit depth N gets larger
(1 < N  min(d  K))  the MSE of SBLSH gets smaller  and the gap between SBLSH and SRP-
LSH gets larger. Particularly  when N = K  over 30% MSE reduction can be observed on all the
datasets. This veriﬁes Corollary 2 that when applying SBLSH  the best strategy would be to set the
Super-Bit depth N as large as possible  i.e. min(d  K). An informal explanation to this interesting
phenomenon is that as the degree of orthogonality of the random projections gets higher  the code
becomes more and more informative  and thus provides better estimate. On the other hand  it can be
observed that the performances on the mean-centered datasets are similar as on the original datasets.
This shows that even when the angle between each data pair is not constrained in (0 ⇡/ 2]  SBLSH
still gives much more accurate estimation.
Figure 3 also shows that with ﬁxed Super-Bit depth N SBLSH signiﬁcantly outperforms SRP-LSH.
When increasing the code length K  the accuracies of SBLSH and SRP-LSH shall both increase.
The performances on the mean-centered datasets are similar as on the original datasets.

3.2 Approximate Nearest Neighbor Retrieval

In this subsection  we conduct ANN retrieval experiment  which compares SBLSH with two other
widely used data-independent binary LSH methods: SRP-LSH and E2LSH (we use the binary ver-
sion in [25  1]). We use the datasets Notre Dame  Half Dome and Trevi from the Photo Tourism
patch dataset [26]  which is also used in [12  10  13] for ANN retrieval. We use 128D SIFT repre-
sentation and normalize the vectors to unit norm. For each dataset  we randomly pick 1 000 samples
as queries  and the rest samples (around 100 000) as the corpus for the retrieval task. We deﬁne
the good neighbors to a query q as the samples within the top 5% nearest neighbors (measured in
Euclidean distance) to q. We adopt the evaluation criterion used in [12  25]  i.e. the proportion of
good neighbors in returned samples that are within the query’s Hamming ball of radius r. We set
r = 3. Using code length K = 30  we repeat the experiment for 10 times and take the mean of the
results. For SBLSH  we ﬁx the Super-Bit depth N = K = 30. Table 1 shows that SBLSH performs
best among all these data-independent hashing methods.

4 Relations to Other Hashing Methods

There exist different kinds of LSH methods  e.g. bit-sampling LSH [9  7] for Hamming distance
and `1-distance  min-hash [2] for Jaccard coefﬁcient  p-stable-distribution LSH [6] for `p-distance
when p 2 (0  2]. These data-independent methods are simple  thus easy to be integrated as a module
in more complicated algorithms involving pairwise distance or similarity computation  e.g. nearest
neighbor search. New data-independent methods for improving these original LSH methods have
been proposed recently. [1] proposed a near-optimal LSH method for Euclidean distance. Li et al.
[16] proposed b-bit minwise hash which improves the original min-hash in terms of compactness.

7

[17] shows that b-bit minwise hash can be integrated in linear learning algorithms for large-scale
learning tasks. [14] reduces the variance of random projections by taking advantage of marginal
norms  and compares the variance of SRP with regular random projections considering the margins.
[15] proposed very sparse random projections for accelerating random projections and SRP.
Prior to SBLSH  SRP-LSH [3] was the only hashing method proven to provide unbiased estimate of
angular similarity. The proposed SBLSH method is the ﬁrst data-independent method that outper-
forms SRP-LSH in terms of higher accuracy in estimating angular similarity.
On the other hand  data-dependent hashing methods have been extensively studied. For example 
spectral hashing [25] and anchor graph hashing [19] are data-dependent unsupervised methods.
Kulis et al. [13] proposed kernelized locality-sensitive hashing (KLSH)  which is based on SRP-
LSH  to approximate the angular similarity in very high or even inﬁnite dimensional space induced
by any given kernel  with access to data only via kernels. There are also a bunch of works devoted
to semi-supervised or supervised hashing methods [10  21  23  24  18]  which try to capture not only
the geometry of the original data  but also the semantic relations.

5 Discussion

Instead of the Gram-Schmidt process  we can use other method to orthogonalize the projection
vectors  e.g. Householder transformation  which is numerically more stable. The advantage of
Gram-Schmidt process is its simplicity in describing the algorithm.
In the paper we did not test the method on data of very high dimension. When the dimension is high 
and N is not small  the Gram-Schmidt process will be computationally expensive. In fact  when the
dimension of data is very high  the random normal projection vectors {vi}i=1 2... K will tend to be
orthogonal to each other  thus it may not be very necessary to orthogonalize the vectors deliberately.
From Corollary 2 and the results in Section 3.1.2  we can see that the closer the Super-Bit depth N
is to the data dimension d  the larger the variance reduction SBLSH achieves over SRP-LSH.
A technical report3 (Li et al.) shows that b-bit minwise hashing almost always has a smaller variance
than SRP in estimating Jaccard coefﬁcient on binary data. The comparison of SBLSH with b-bit
minwise hashing for Jaccard coefﬁcient is left for future work.

6 Conclusion and Future Work

The proposed SBLSH is a data-independent hashing method which signiﬁcantly outperforms SRP-
LSH. We have theoretically proved that SBLSH provides an unbiased estimate of angular similarity 
and has a smaller variance than SRP-LSH when the angle to estimate is in (0 ⇡/ 2]. The algorithm
is simple  easy to implement and can be integrated as a basic module in more complicated algo-
rithms. Experiments show that with the same length of binary code  SBLSH achieves over 30%
mean squared error reduction over SRP-LSH in estimating angular similarity  when the Super-Bit
depth N is close to the data dimension d. Moreover  SBLSH performs best among several widely
used data-independent LSH methods in approximate nearest neighbor retrieval experiments. Theo-
retically exploring the variance of SBLSH when the angle is in (⇡/2 ⇡ ] is left for future work.

Acknowledgments

This work was supported by the National Basic Research Program (973 Program) of China (Grant
Nos. 2013CB329403 and 2012CB316301)  National Natural Science Foundation of China (Grant
Nos. 91120011 and 61273023)  and Tsinghua University Initiative Scientiﬁc Research Program
No.20121088071  and NExT Research Center funded under the research grant WBS. R-252-300-
001-490 by MDA  Singapore. And it was supported in part to Dr. Qi Tian by ARO grant W911BF-
12-1-0057  NSF IIS 1052851  Faculty Research Awards by Google  FXPAL  and NEC Laboratories
of America  respectively.

3www.stat.cornell.edu/˜li/hashing/RP_minwise.pdf

8

References
[1] Alexandr Andoni and Piotr Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in

high dimensions. In Annual IEEE Symposium on Foundations of Computer Science  2006.

[2] Andrei Z. Broder  Steven C. Glassman  Mark S. Manasse  and Geoffrey Zweig. Syntactic clustering of

the web. Computer Networks  29(8-13):1157–1166  1997.

[3] Moses Charikar. Similarity estimation techniques from rounding algorithms.

Theory of Computing  2002.

In ACM Symposium on

[4] Yasuko Chikuse. Statistics on Special Manifolds. Springer  February 2003.
[5] Ondrej Chum  James Philbin  and Andrew Zisserman. Near duplicate image detection: min-hash and

tf-idf weighting. In British Machine Vision Conference  2008.

[6] Mayur Datar  Nicole Immorlica  Piotr Indyk  and Vahab S. Mirrokni. Locality-sensitive hashing scheme

based on p-stable distributions. In Symposium on Computational Geometry  2004.

[7] Aristides Gionis  Piotr Indyk  and Rajeev Motwani. Similarity search in high dimensions via hashing. In

International Conference on Very Large Databases  1999.

[8] Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and

satisﬁability problems using semideﬁnite programming. Journal of the ACM  42(6):1115–1145  1995.

[9] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the curse of dimen-

sionality. In ACM Symposium on Theory of Computing  1998.

[10] Prateek Jain  Brian Kulis  and Kristen Grauman. Fast image search for learned metrics. In IEEE Confer-

ence on Computer Vision and Pattern Recognition  2008.

[11] Herv´e J´egou  Matthijs Douze  and Cordelia Schmid. Product quantization for nearest neighbor search.

IEEE Trans. Pattern Anal. Mach. Intell.  33(1):117–128  2011.

[12] Brian Kulis and Trevor Darrell. Learning to hash with binary reconstructive embeddings. In Advances in

Neural Information Processing Systems  2009.

[13] Brian Kulis and Kristen Grauman. Kernelized locality-sensitive hashing for scalable image search. In

IEEE International Conference on Computer Vision  2009.

[14] Ping Li  Trevor Hastie  and Kenneth Ward Church. Improving random projections using marginal infor-

mation. In COLT  pages 635–649  2006.

[15] Ping Li  Trevor Hastie  and Kenneth Ward Church. Very sparse random projections.

287–296  2006.

In KDD  pages

[16] Ping Li and Arnd Christian K¨onig. b-bit minwise hashing. In International World Wide Web Conference 

2010.

[17] Ping Li  Anshumali Shrivastava  Joshua L. Moore  and Arnd Christian K¨onig. Hashing algorithms for

large-scale learning. In Advances in Neural Information Processing Systems  2011.

[18] Wei Liu  Jun Wang  Rongrong Ji  Yu-Gang Jiang  and Shih-Fu Chang. Supervised hashing with kernels.

In CVPR  pages 2074–2081  2012.

[19] Wei Liu  Jun Wang  Sanjiv Kumar  and Shih-Fu Chang. Hashing with graphs. In ICML  pages 1–8  2011.
International Journal of
[20] David G. Lowe. Distinctive image features from scale-invariant keypoints.

Computer Vision  60(2):91–110  2004.

[21] Yadong Mu  Jialie Shen  and Shuicheng Yan. Weakly-supervised hashing in kernel space.

Conference on Computer Vision and Pattern Recognition  2010.

In IEEE

[22] Antonio Torralba  Robert Fergus  and William T. Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE Trans. Pattern Anal. Mach. Intell.  30(11):1958–1970 
2008.

[23] Jun Wang  Sanjiv Kumar  and Shih-Fu Chang. Sequential projection learning for hashing with compact

codes. In International Conference on Machine Learning  2010.

[24] Jun Wang  Sanjiv Kumar  and Shih-Fu Chang. Semi-supervised hashing for large scale search. IEEE

Transactions on Pattern Analysis and Machine Intelligence  99(PrePrints)  2012.

[25] Yair Weiss  Antonio Torralba  and Robert Fergus. Spectral hashing. In Advances in Neural Information

Processing Systems  2008.

[26] Simon A. J. Winder and Matthew Brown. Learning local image descriptors.

Computer Vision and Pattern Recognition  2007.

In IEEE Conference on

9

,Fabian Pedregosa
Rémi Leblond
Simon Lacoste-Julien
Aravind Rajeswaran
Chelsea Finn
Sham Kakade
Sergey Levine