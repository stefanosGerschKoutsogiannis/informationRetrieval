2019,Successor Uncertainties: Exploration and Uncertainty in Temporal Difference Learning,Posterior sampling for reinforcement learning (PSRL) is an effective method for balancing exploration and exploitation in reinforcement learning. Randomised value functions (RVF) can be viewed as a promising approach to scaling PSRL. However  we show that most contemporary algorithms combining RVF with neural network function approximation do not possess the properties which make PSRL effective  and provably fail in sparse reward problems. Moreover  we find that propagation of uncertainty  a property of PSRL previously thought important for exploration  does not preclude this failure. We use these insights to design Successor Uncertainties (SU)  a cheap and easy to implement RVF algorithm that retains key properties of PSRL. SU is highly effective on hard tabular exploration benchmarks. Furthermore  on the Atari 2600 domain  it surpasses human performance on 38 of 49 games tested (achieving a median human normalised score of 2.09)  and outperforms its closest RVF competitor  Bootstrapped DQN  on 36 of those.,Successor Uncertainties: Exploration and

Uncertainty in Temporal Difference Learning

David Janz∗†

University of Cambridge

dj343@cam.ac.uk

Jiri Hron∗

University of Cambridge

jh2084@cam.ac.uk

Katja Hofmann
Microsoft Research

José Miguel Hernández-Lobato

University of Cambridge

Alan Turing Institute
Microsoft Research

Przemysław Mazur
Wayve Technologies

Sebastian Tschiatschek

Microsoft Research

Abstract

Posterior sampling for reinforcement learning (PSRL) is an effective method for
balancing exploration and exploitation in reinforcement learning. Randomised
value functions (RVF) can be viewed as a promising approach to scaling PSRL.
However  we show that most contemporary algorithms combining RVF with neural
network function approximation do not possess the properties which make PSRL
effective  and provably fail in sparse reward problems. Moreover  we ﬁnd that
propagation of uncertainty  a property of PSRL previously thought important for ex-
ploration  does not preclude this failure. We use these insights to design Successor
Uncertainties (SU)  a cheap and easy to implement RVF algorithm that retains key
properties of PSRL. SU is highly effective on hard tabular exploration benchmarks.
Furthermore  on the Atari 2600 domain  it surpasses human performance on 38
of 49 games tested (achieving a median human normalised score of 2.09)  and
outperforms its closest RVF competitor  Bootstrapped DQN  on 36 of those.

1

Introduction

Perhaps the most important open question within reinforcement learning is how to effectively balance
exploration of an unknown environment with exploitation of the already accumulated knowledge
(Kaelbling et al.  1996; Sutton et al.  1998; Busoniu et al.  2010). In this paper  we study this in the
classic setting where the unknown environment is modelled as a Markov Decision Process (MDP).
Speciﬁcally  we focus on developing an algorithm that combines effective exploration with neural
network function approximation. Our approach is inspired by Posterior Sampling for Reinforcement
Learning (PSRL; Strens  2000; Osband et al.  2013). PSRL approaches the exploration/exploitation
trade-off by explicitly accounting for uncertainty about the true underlying MDP. In tabular settings 
PSRL achieves impressive results and close to optimal regret (Osband et al.  2013; Osband & Van Roy 
2016). However  many existing attempts to scale PSRL and combine it with neural network function
approximation sacriﬁce the very aspects that make PSRL effective. In this work  we examine several
of these algorithms in the context of PSRL and:

1. Prove that a previous avenue of research  propagation of uncertainty (O’Donoghue et al. 
2018)  is neither sufﬁcient nor necessary for effective exploration under posterior sampling.

∗Equal contribution
†Work partly done during an internship at Microsoft Research Cambridge

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2. Introduce Successor Uncertainties (SU)  a cheap and scalable model-free exploration algo-

rithm that retains crucial elements of the PSRL algorithm.

3. Show that SU is highly effective on hard tabular exploration problems.
4. Present Atari 2600 results: SU outperforms Bootstrapped DQN (Osband et al.  2016a) on

36/49 and Uncertainty Bellman Equation (O’Donoghue et al.  2018) on 43/49 games.

2 Background

t := Et(P∞

τ=0 γτ Rτ+1) with γ ∈ [0  1).
τ=t γτ−tRτ+1) = Et(Rt+1) + γEt(Qπ

We use the following notation: for X a random variable  we denote its distribution by PX. Further  if
f is a measurable function  then f(X) follows the distbution f#PX (the pushforward of PX by f).
We consider ﬁnite MDPs: a tuple (S A T )  where S is a ﬁnite state space  A a ﬁnite action space 
and T : S × A → P(S × R) a transition probability kernel mapping from the state-action space
S × A to the set of probability distributions P(S × R) on the product space of states S and rewards
R ⊂ R; R is assumed to be bounded throughout. For each time step t ∈ N  the agent selects an
action At by sampling from a distribution speciﬁed by its policy π : S → P(A) for the current state
St  and receives a new state and reward (St+1  Rt+1) ∼ T (St  At). This gives rise to a Markov
process (St  At)t≥0 and a reward process (Rt)t≥1. The task of solving an MDP amounts to ﬁnding

a policy π? which maximises the expected return E(P∞

(T π ˆQ)(s  a) = E(S0 R0)∼T (s a)[R0 + γEA0∼π(S0) ˆQ(S0  A0)] .

Crucial to many so called model-free methods for solving MDPs is the state-action value function
t+1)   where Et is
(Q function) for a policy π: Qπ
used to denote an expectation conditional on (Sτ   Aτ)τ≤t. Model-free methods use the recursive
nature of the Bellman equation to construct a model ˆQπ : S × A → R  which estimates Qπ
t for any
given (St = s  At = a)  through repeated application of the Bellman operator T π : RS×A → RS×A:
(1)
Since T π is a contraction on RS×A with a unique ﬁxed point ˆQπ  that is T π ˆQπ = ˆQπ  the iterated
application of T π to any initial ˆQ ∈ RS×A yields ˆQπ. The expectations in equation (1) can be
estimated via Monte Carlo using experiences (s  a  r  s0) obtained through interaction with the MDP.
A key challenge is then in obtaining experiences that are highly informative about the optimal policy.
A simple and effective approach to collecting such experiences is PSRL  a model-based algorithm
based on two components: (i) a distribution over rewards and transition dynamics P ˆT obtained using
a Bayesian modelling approach  treating rewards and transition probabilities as random variables;
and (ii) the posterior sampling exploration algorithm (Thompson  1933; Dearden et al.  1998) which
samples ˆT ∼ P ˆT   computes the optimal policy ˆπ with respect to the sampled ˆT   and follows ˆπ for
the duration of a single episode. The collected data are then used to update the P ˆT model  and
the whole process is iterated until convergence.
While PSRL performs very well on tabular problems  it is computationally expensive and does not
utilise any additional information about the state space structure (e.g. visual similarity when states
are represented by images). A family of methods called Randomised Value Functions (RVF; Osband
et al.  2016b) attempt to overcome these issues by directly modelling a distribution over Q functions 
P ˆQ  instead of over MDPs  P ˆT . Rather than acting greedily with respect to a sampled MDP as in
PSRL  the agent then acts greedily with respect to a sample ˆQ ∼ P ˆQ drawn at the beginning of each
episode  removing the main computational bottleneck. Since a parametric model is often chosen for
P ˆQ  the switch to Q function modelling also directly facilitates use of function approximation and
thus generalisation between states.

3 Exploration under function approximation

Many exploration methods  including (Osband et al.  2016b a; Moerland et al.  2017; O’Donoghue
et al.  2018; Azizzadenesheli et al.  2018)  can be interpreted as combining the concept of RVF with
neural network function approximation. While the use of neural network function approximation
allows these methods to scale to problems too complex for PSRL  it also brings about conceptual
difﬁculties not present within PSRL and tabular RVF methods. Speciﬁcally  because a Q function is
deﬁned with respect to a particular policy  constructing P ˆQ requires selection of a reference policy
or distribution over policies. Methods that utilise a distribution over reference policies typically

2

U P

DOWN

...

s5

U P

DOWN

s4

s3

s0

U P

DOWN

s2

s1

Figure 1: Binary tree MDP of size L. States S = {s0  . . .   s2L} are one-hot encoded; actions
A = {a1  a2} are mapped to movements {UP  DOWN} according to a random mapping drawn
independently for each state. Reward of one is obtained after reaching s2L and zero otherwise. States
with odd indices and s2L are terminal.

#P ˆT [ ˆQπ(s  a)p] = EP ˆT

(cid:8)[E(R0 S0)∼ ˆT (s a)R0 + EA0∼π(S0)F π( ˆT )(S0  A0)]p(cid:9).

employ a bootstrapped estimator of the Q function as we will discuss in more depth later. For
now  we focus on methods that employ a single reference policy which commonly interleave two
steps: (i) inference of P ˆQπi for a given policy πi using the available data (value prediction step);
(ii) estimation of an improved policy πi+1 based on P ˆQπi (policy improvement step). While a
common policy improvement choice is πi+1 : s 7→ EP ˆQπi [G( ˆQ)(s)]  methods vary greatly in how
they implement value prediction. To gain a better insight into the value prediction step  we examine
its idealised implementation: Suppose we have access to a belief over MDPs  P ˆT (as in PSRL) 
and want to compute the implied distribution P ˆQπ for a single policy π. The intuitive (albeit still
computationally expensive) procedure is to: (i) draw ˆT ∼ P ˆT ; and (ii) repeatedly apply the Bellman
operator T π to an initial ˆQ for the drawn ˆT until convergence. Denoting by F π: ˆT 7→ ˆQπ the map
from ˆT to the corresponding ˆQπ for a policy π  the distribution of resulting samples is P ˆQπ = F π
#P ˆT .
This idealised value prediction step motivates  for example  the Uncertainty Bellman Equation
(UBE; O’Donoghue et al.  2018). O’Donoghue et al. argue that to achieve effective exploration  it is
necessary that the uncertainty about each ˆQπ(s  a)  quantiﬁed by variance  is equal to the uncertainty
about the immediate reward and the next state’s Q value. This requirement can be formalised as
follows:
Deﬁnition 1 (Propagation of uncertainty). For a given distribution P ˆT and policy π  we say that
a model P ˆQπ propagates uncertainty according to P ˆT if for each (s  a) ∈ S × A and p = 1  2
EP ˆQπ [ ˆQπ(s  a)p] = EF π
In words  propagation of uncertainty requires that the ﬁrst two moments behave consistently under
application of the Bellman operator.
Propagation of uncertainty is a desirable property when using Upper Conﬁdence Bounds (UCB; Auer 
2002) for exploration  since UCB methods rely only on the ﬁrst two moments of P ˆQπ. However 
propagation of uncertainty is not sufﬁcient for effective exploration under posterior sampling. We
show this in the context of the binary tree MDP depicted in ﬁgure 1. To solve the MDP  the agent
must execute a sequence of L uninterrupted UP movements. In the following proposition  we show
that any algorithm combining factorised symmetric distributions with posterior sampling (e.g. UBE)
will solve this MDP with probability of at most 2−L per episode  thus failing to outperform a uniform
exploration policy. Importantly  the sizes of marginal variances have no bearing on this result 
meaning that propagation of uncertainty on its own does not preclude this failure mode.
Proposition 1. Let |A| > 1  and P ˆQ be a factorised distribution  i.e. for ˆQ ∼ P ˆQ  ˆQ(s  a) and
ˆQ(s0  a0) are independent  ∀(s  a) 6= (s0  a0)  with symmetric marginals. Assume that for each s ∈ S 
the marginal distributions of { ˆQ(s  a): a ∈ A} are all symmetric around the same value cs ∈ R.
Then the probability of executing any given sequence of L actions under ˆπ ∼ G#P ˆQ is at most 2−L.
Propagation of uncertainty is furthermore not necessary for posterior sampling. To see this  ﬁrst note
that for any given P ˆQπ  the posterior sampling procedure only depends on the induced distribution
over greedy policies  i.e. the pushforward of P ˆQπ by the greedy operator G. This means that from
the point of view of posterior sampling  two Q function models are equivalent as long as they induce
the same distribution over greedy policies. In what follows  we formalise this equivalence relationship
(deﬁnition 2)  and then show that each of the induced equivalence classes contains a model that
does not propagate uncertainty (proposition 2)  implying that posterior sampling does not rely on
propagation of uncertainty.

3

Deﬁnition 2 (Posterior sampling policy matching). For a given distribution P ˆT and a policy π  we say
that a model P ˆQπ matches the posterior sampling policy implied by P ˆT if G#P ˆQπ = (G ◦ F π)#P ˆT .
#P ˆT [ ˆQπ(s  a)] is
Proposition 2. For any distribution P ˆT and policy π such that the variance VF π
greater than zero for some (s  a)  there exists a distribution P ˆQπ which matches the posterior sampling
policy (deﬁnition 2)  but does not propagate uncertainty (deﬁnition 1)  according to P ˆT .
We conclude by addressing a potential criticism of proposition 1  i.e. that the described issues may be
circumvented by initialising expected Q values to a value higher than the maximal attainable Q value
in given MDP  an approach known as optimistic initialisation (Osband et al.  2016b). In such case 
symmetries in the Q function may break as updates move the distribution towards more realistic
Q values. However  when neural network function approximation is used  the effect of optimistic
initialisation can disappear quickly with optimisation (Osband et al.  2018). In particular  with
non-orthogonal state-action embeddings  Q value estimates may decrease for yet unseen state-action
pairs  and estimates for different state-action states can move in tandem. In practice  most recent
models employing neural network function approximation do not use optimistic initialisation (Osband
et al.  2016a; Azizzadenesheli et al.  2018; Moerland et al.  2017; O’Donoghue et al.  2018).

4 Successor Uncertainties

We present Successsor Uncertainties  an algorithm which both propagates uncertainty and matches
the posterior sampling policy. As our work is motivated by PSRL  we focus on the use with posterior
sampling  leaving combination with other exploration algorithms for future research.

4.1 Q function model deﬁnition
Suppose we are given an embedding function φ: S×A → Rd  such that for all (s  a)  kφ(s  a)k2 = 1
and φ(s  a) ≥ 0 elementwise  and EtRt+1 = hφt  wi for some w ∈ Rd. Denote φt = φ(St  At).
τ=t γτ−tφτ]  the (discounted)
Then we can express Qπ
expected future occurrence of each φ(s  a) feature under a policy π  as follows:
γτ−tφτ   w

t = Et[P∞
(cid:28)
∞X

t as an inner product of w and ψπ

γτ−tRτ+1 = Et

γτ−thφτ   wi =

∞X

∞X

(cid:29)

t = Et
Qπ

Et

= hψπ

t   wi  

(2)

τ=t

τ=t

τ=t

where the second equality follows from the tower property of conditional expectation and the third
from the dominated convergence theorem combined with the unit norm assumption.
t is known in the literature as the successor features (Dayan  1993; Barreto et al. 
The quantity ψπ
t+1  an estimator of the successor features  ˆψπ  can be obtained
2017). Noting that ψπ
by applying standard temporal difference learning techniques. The other quantity involved  w  can
be estimated by regressing embeddings of observed states φt onto the corresponding rewards. We
perform Bayesian linear regression to infer a distribution over rewards  using N (0  θI) as the prior
over w and N (hφ  wi  β) as the likelihood  which leads to posterior N (µw  Σw) over w with known
analytical expressions for both µw and Σw. This induces posterior distribution over ˆQπ
SU given by

t = φt + γ Etψπ

SU ∼ N (ˆΨπµw  ˆΨπΣw(ˆΨπ)>)  
ˆQπ

(3)
where ˆΨπ = [ ˆψπ(s  a)]>
(s a)∈S×A. This is our Successor Uncertainties (SU) model for the Q function.
The ﬁnal element of the SU model is the selection of a sequence of reference policies (πi)i≥1 for
which the Q function model is learnt. We follow O’Donoghue et al. (2018) in constructing these
iteratively as πi+1(s) = Eˆπ∼G#P ˆQπi [ˆπ(s)].
4.2 Properties of the model

The non-diagonal covariance matrix of the SU Q function model (see equation (3)) means that SU
does not suffer from the shortcomings of previous methods with factorised posterior distributions
described in proposition 1. Moreover  note that ˆQπ
#P ˆT for the MDP model P ˆT composed of
a delta distribution concentrated on empirical transition frequencies  and the Bayesian linear model
for rewards (assuming convergence of successor features  i.e. ˆψπ = ψπ). SU thus both propagates
uncertainty and matches the posterior sampling policy according to this choice of P ˆT .

SU ∼ F π

4

However  due to its use of a point estimate for the transition probabilities  SU may underestimate
Q function uncertainty  and a good model of transition probabilities which scales beyond tabular
settings can lead to improved performance. Furthermore  SU estimates P ˆQπi+1 for a single policy 
which we choose to be πi+1(s) = Eˆπ∼G#P ˆQπi [ˆπ(s)]. This approach may not adequately capture the
uncertainty over ˆπ implied by P ˆQπi . We expect that incorporation of this uncertainty  or an improved
method of choosing πi+1  may further improve the SU algorithm.

4.3 Neural network function approximation

{z

}

|

{z

}

One of the main assumptions we made so far is that the embedding function φ is known a priori. This
section considers the scenario where φ is to be estimated jointly with the other quantities using neural
network function approximation. For reference  the pseudocode is included in appendix C.
Let ˆφ: S × A → Rd+ be the current estimate of φ  (st  at) the state-action pair observed at step t  rt+1
the reward observed after taking action at in state st. Suppose we want to estimate the Q function
of some given policy π  and denote ˆφt := ˆφ(st  at)  ˆψt := ˆψπ(st  at). We propose to jointly learn ˆφ
and ˆψ by enforcing the known relationships between φt  ψπ

t and EtRt+1:
+|h ˆw  ˆψti− γ(h ˆw  ˆψt+1i)†− rt+1|2

(4)

+|h ˆw  ˆφti− rt+1|2

2

}
min ˆφ  ˆψ  ˆw k ˆψt − ˆφt − γ ( ˆψt+1)†k2

{z

|

successor feature loss

|

reward loss

Q value loss

in expectation over the observed data {(st  at  rt+1st+1): t = 0  . . .   N} with at+1 ∼ π(st+1);
ˆφt  ˆψt ∈ Rd+ kˆφtk2 = 1 ∀t  are respectively ensured by the use of ReLU activations and explicit
normalisation. The ˆw ∈ Rd are the ﬁnal layer weights shared by the the reward and the Q value
networks. Quantities superscripted with † are treated as ﬁxed during optimisation.
The need for the successor feature and reward losses follows directly from the deﬁnition of the SU
model. We add the explicit Q value loss to ensure accuracy of Q value predictions. Assuming that
there exists a (ReLU) network that achieves zero successor feature and reward loss  the added Q value
loss has no effect. However  ﬁnding such an optimal solution is difﬁcult in practice and empirically
the addition of the Q value loss improves performance. Our modelling assumptions cause all
constituent losses in equation (4) to have similar scale  and thus we found it unnecessary to introduce
weighting factors. Furthermore  unlike in previous work utilising successor features (Kulkarni
et al.  2016; Machado et al.  2017  2018)  SU does not rely on any auxiliary state reconstruction or
state-transition prediction tasks for learning  which simpliﬁes implementation and greatly reduces
the required amount of computation.
We employ the neural network output weights ˆw in prediction of the mean Q function  and use
the Bayesian linear model only to provide uncertainty estimates. In estimating the covariance matrix
i )−1   ζ ∈
ˆφ>
[0  1]  so as to counter non-stationarity of the learnt state-action embeddings ˆφ .

Σw  we decay the contribution of old data-points  ˆΣw = (ζ N θ−1I + β−1PN

i=0 ζ N−i ˆφi

4.4 Comparison to existing methods

We discuss two popular classes of Q function models compatible with neural network function
approximation: methods relying on Bayesian linear Q function models and methods based on
bootstrapping. We omit variational Q-learning methods such as (Gal  2016; Lipton et al.  2018)  as
conceptual issues with these algorithms have already been identiﬁed in an illuminating line of work
by Osband et al. (2016a  2018).
Bayesian linear Q function models encompass our SU algorithm  UBE (O’Donoghue et al.  2018) im-
plemented with value function approximation  Bayesian Deep Q Networks (BDQN; Azizzadenesheli
et al.  2018)  and a range of other related work (Levine et al.  2017; Moerland et al.  2017). The algo-
rithms within this category tend to use a Q function model of the form ˆQπ(s  a) = hˆφπ
s   wai  where
s are state embeddings and wa ∼ Pwa are weights of a Bayesian linear model. The embeddings ˆφπ
ˆφπ
s
are produced by a neural network  and are usually optimised using a temporal difference algorithm
applied to Q values. However  these methods do not enforce any explicit structure within the embed-
dings ˆφπ
s which would be required for posterior sampling policy matching  and prevent these methods
from falling victim to proposition 1. SU can thus be viewed as a simple and computationally cheap
alternative ﬁxing the issues of existing Bayesian linear Q function models.
Bootstrapped DQN (Osband et al.  2016a  2018) is a model which consists of an ensemble of K
standard Q networks  each initialised independently and trained on a random subset of the observed

5

data. Each network is augmented with a ﬁxed additive prior network  so as to ensure the ensemble
distribution does not collapse in sparse environments. If all networks within the ensemble are trained
to estimate the Q function for a single policy π  then Bootstrapped DQN both propagates uncertainty
and matches the posterior sampling policy for a distribution over MDPs formed by the mixture over
empirical MDPs corresponding to each subsample of the data. In practice  Bootstrapped DQN does
not assume a single policy π and instead each network learns for its corresponding greedy policy.
Bootstrapped DQN is  however  more computationally expensive: its performance increases with
the size of the ensemble K  but so does the amount of computation required. Our experiments show
that SU is much cheaper computationally  and that despite using only a single reference policy  it
manages to outperform Bootstrapped DQN on a wide range of exploration tasks (see section 5).

5 Tabular experiments

We present results for: (i) the binary tree MDP accompanied by theoretical analysis showing how
SU succeeds and avoids the pitfalls identiﬁed in proposition 1; (ii) a hard exploration task proposed
by Osband et al. (2018) together with the Boostrapped DQN algorithm which SU outperforms by
a signiﬁcant margin.3 We also provide an analysis explaining why some of the previously discussed
algorithms perform well on seemingly similar experiments present in existing literature.

5.1 Binary tree MDP

We study the behaviour of SU and its competitors on the binary tree MDP introduced in ﬁgure 1.
Figure 2 shows the empirical performance of each algorithm as a function of the tree size L. Evidently 
both BDQN and UBE fail to outperform a uniform exploration policy. For UBE  this is a consequence
of proposition 1  and the similarly poor behaviour of BDQN suggests it may suffer from an analogous
issue. In contrast  SU and Bootstrapped DQN are able to succeed on large binary trees despite the
very sparse reward structure and randomised action effects. However  Bootstrapped DQN requires
approximately 25 times more computation than SU to approach similar levels of performance due to
the necessity to train a whole ensemble of Q networks.

Figure 2: Median number of episodes required to learn the optimal policy on the tree MDP. Blue
points indicate all 5 seeds succeeded within 5000 episodes  orange indicates only some of the runs
succeeded  and red all runs failed. Dashed lines correspond to the median for a uniform exploration
policy. Note the reduced size of the x-axis for BDQN and UBE.

The next proposition and its proof provide intuition for the success of SU on the tree MDP. The proof
is based on a lemma stated just after the proposition (see appendix B.1 for formal treatment).
Proposition 3 (Informal statement). Assume the SU model with: (i) ﬁxed one-hot state-action
embeddings φ  (ii) uniform exploration thus far  (iii) successor representations learnt to convergence
for a uniform policy. Let sk for 2 ≤ k < 2L  even  be a state visited N times thus far. Then
the probability of selecting UP in sk  given UP was selected in s0  s2  . . .   sk−2  is greater than one
half with probability greater than 1 − N   where N decreases exponentially with N.
Lemma 4 (Informal statement). Under the SU model ˆQ ∼ P ˆQπ for the uniform policy π  the proba-
bility that the greedy policy ˆπ = G( ˆQ) selects UP in sk  given UP was selected in s0  s2  . . .   sk−2  is
greater than one half if there exists an even 0 ≤ j < k such that

Cov( ˆQ(sk  UP)  ˆQ(sj  UP)) > Cov( ˆQ(sk  DOWN)  ˆQ(sj  UP)) .

3Code for the tabular experiments: https://djanz.org/successor_uncertainties/tabular_code

6

01020problem scale L025005000median learning timeBDQN01020problem scale LUBE0100200problem scale LBootstrap+Prior (1x compute)0100200problem scale LBootstrap+Prior (25x compute)0100200problem scale LSU (1x compute)Sketch proof of proposition 3. Under SU ˆQ(sj  UP) = ˆr(sj  UP)+. . .+ρ ˆQ(sk  UP)+ρ ˆQ(sk  DOWN)
with ρ = 2−( k−j
2 ) the probability of getting from sj to sk under the uniform policy. Note that
ˆQ(sj  UP) and ˆQ(sk  DOWN) only share the ˆQ(sk  DOWN) = ˆr(sk  DOWN) term  whereas ˆQ(sk  UP)
and ˆQ(sj  UP) share ˆr(sj  UP)  . . .   ˆr(sp  DOWN)  where sp is the state with the highest index seen
so far. Thus covariance between ˆQ(sk  UP) and ˆQ(sj  UP) is higher than that between ˆQ(sk  DOWN)
and ˆQ(sj  UP) with high probability (at least 1 − N )  and the result follows from lemma 4.

Proposition 3 implies that (at least under the simplifying assumption of prior exploration being
uniform) SU is likely to assign higher probability to Q functions for which a greedy policy leads
towards the furthest visited state (cf. the role of the state sp in the sketch proof). This is a strategy
actively aimed for in exploration algorithms such as Go-Explore where the agent uses imitation
learning to return to the furthest discovered states (Ecoffet et al.  2019).

5.2 Chain MDP from (Osband et al.  2018)

We present results on the chain environment introduced by Osband et al. (2018)  described in detail
in appendix C.1. Osband et al. describe their MDP as being “akin to looking for a piece of hay in a
needle-stack” and state that it “may seem like an impossible task”. Figure 3 shows the scaling for
Successor Uncertainties and Bootstrap+Prior for this problem. Learning time T scales empirically as
O(L2.5) for SU  versus O(L3) for Bootstrap+Prior (as reported in Osband et al.  2018).

Figure 3: Learning time T for SU and Bootstrap+Prior for a range of problem sizes L on the chain
MDP. Curve for SU is log10 T = 2.5 log10 L−0.95. Curve for Bootstrap+Prior is taken from ﬁgure 8
in (Osband et al.  2018).

5.3 On the success of BDQN in environments with tied actions

We brieﬂy address prior results in the literature where BDQN is seen solving problems seemingly
similar to our binary tree MDP with ease (as in  for example  ﬁgure 1 of Touati et al.  2018).
The discrepancy occurs because previous work often does not randomise the effects of actions
(for example Osband et al.  2016a; Plappert et al.  2018; Touati et al.  2018)  i.e. if a1 leads UP
in any state sk  then a1 leads UP in all states. We refer to this as the tied actions setting. In the
following proposition  we show that MDPs with tied actions are trivial for BDQN with strictly
positive activations (e.g. sigmoid). We offer a similar result for ReLU in appendix B.2.
Proposition 5. Let ˆQ(s  a) = hφ(s)  wai be a Bayesian Q function model with φ(s) = ϕ(U1s) ∈ Rd 
1s a one-hot encoding of s  and ϕ a strictly positive activation function (e.g. sigmoid) applied
wI)  Uhs ∼ N (0  σ2
elementwise. Then sampling independently from the prior wa ∼ N (0  σ2
u) solves
a tied action binary tree of size L in T ≤ −[log2(1 − 2−d)]−1 median number of episodes.

Proof. Deﬁne ∆ := wUP − wDOWN and observe UP is selected if ˆQ(s  UP) − ˆQ(s  DOWN) =
hφ(s)  wUP − wDOWNi > 0. By strict positivity of ϕ  the probability that UP is always selected

{hφ(s2j)  ∆i >0} | ∆ >0(cid:3)P(∆ >0) = P(∆ > 0)  

P(cid:2)L−1\

{ ˆQ(s2j  UP) > ˆQ(s2j  DOWN)}(cid:3)≥P(cid:2)L−1\

j=0

j=0

where ∆ > 0 is to be interpreted elementwise. As ∆ ∼ N (0  2σ2

wI)  P(∆ > 0) = 2−d for all L.

7

20406080100120140160180200problem scale L02500050000median learning time1.41.61.82.02.2log10 problem scale L2.53.03.54.04.55.0Bootstrap+Prior fitSuccessor Uncertainties fitSuccessor Uncertainties dataFigure 4: Bars show the difference in human normalised score between SU and Bootstrap DQN (top) 
UBE (middle) and DQN (bottom) for each of the 49 Atari 2600 games. Blue indicates SU performed
better  red worse. SU outperforms the baselines on 36/49  43/49 and 42/49 games respectively.
Y-axis values have been clipped to [−2.5  2.5].

A single layer BDQN with one neuron can thus solve a tied action binary tree of any size L in one
episode (median) while completely ignoring all state information. That such an approach can be
successful implies tied actions MDPs generally do not make for good exploration benchmarks.

6 Atari 2600 experiments

We have tested the SU algorithm on the standard set of 49 games from the Arcade Learning Environ-
ment  with the aim of showing that SU can be scaled to complex domains that require generalisation
between states. We use a standard network architecture as in (Mnih et al.  2015; Van Hasselt et al. 
2016) endowed with an extra head for prediction of ˆφ and one-step value updates. More detail on our
implementation  network architecture and training procedure can be found in appendix C.2.4
SU obtains a median human normalised score of 2.09 (averaged over 3 seeds) after 200M training
frames under the ‘no-ops start 30 minute emulator time’ test protocol described in (Hessel et al. 
2018). Table 1 shows we signiﬁcantly outperform competing methods. The raw scores are reported
in table 2 (appendix)  and the difference in human normalised score between SU and the competing
algorithms for individual games is charted in ﬁgure 4. Since Azizzadenesheli et al. (2018) only report
scores for a small subset of the games and use a non-standard testing procedure  we do not compare
against BDQN. Osband et al. (2018)  who introduce Bootstrap+Prior  do not report Atari results; we
thus compare with results for the original plain Bootstrapped DQN (Osband et al.  2016a) instead.

Table 1: Human normalised Atari scores. Superhuman performance is the percentage of games on
which each algorithm surpasses human performance (as reported in Mnih et al.  2015).

Human normalised score percentiles

Algorithm

Successor Uncertainties
Bootstrapped DQN
UBE
DQN + -greedy

7 Conclusion

25%
1.06
0.76
0.38
0.50

50%
2.09
1.60
1.07
1.00

75%
5.95
5.16
4.14
3.41

Superhuman
performance %

77.55%
67.35%
51.02%
48.98%

We studied the Posterior Sampling for Reinforcement Learning algorithm and its extensions within the
Randomised Value Function framework  focusing on use with neural network function approximation.
We have shown theoretically that exploration techniques based on the concept of propagation of uncer-
tainty are neither sufﬁcient nor necessary for posterior sampling exploration in sparse environments.
We instead proposed posterior sampling policy matching  a property motivated by the probabilistic
model over rewards and state transitions within the PSRL algorithm. Based on the theoretical insights 

4Code for the Atari experiments: djanz.org/successor_uncertainties/atari_code

8

BootstrapUBEAliAmiAssAstAstAtlBanBatBeaBowBoxBreCenChoCraDemDouEndFisFreFroGopGraH.EIceJamKanKruKunMonMs.NamPonPriQ*BRivRoaRobSeaSpaStaTenTimTutUp VenVidWizZaxAtari 2600 games  alphabeticalDQNdifference in human normalised score versus Successor Uncertaintieswe developed Successor Uncertainties  a randomised value function algorithm that avoids some of
the pathologies present within previous work. We showed empirically that on hard tabular examples 
SU signiﬁcantly outperforms competing methods  and provided theoretical analysis of its behaviour.
On Atari 2600  we demonstrated Successor Uncertainties is also highly effective when combined
with neural network function approximation.
Performance on the hardest exploration tasks often beneﬁts greatly from multi-step temporal dif-
ference learning (Precup  2000; Munos et al.  2016; O’Donoghue et al.  2018) which we believe is
the most promising direction for improving Successor Uncertainties. Since modiﬁcation of existing
models to incorporate Successor Uncertainties is relatively simple  other standard techniques used
within model-free reinforcement learning like (Schaul et al.  2015; Wang et al.  2016) can be leveraged
to obtain further gains. This paper thus opens many exciting directions for future research which we
hope will translate into both further performance improvements and a more thorough understanding
of exploration in modern reinforcement learning.

Acknowledgements

We thank Matej Balog and the anonymous reviewers for their helpful comments and suggestions. Jiri
Hron acknowledges support by a Nokia CASE Studentship.

References
Auer  P. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3(Nov):397–422  2002.

Azizzadenesheli  K.  Brunskill  E.  and Anandkumar  A. Efﬁcient exploration through bayesian deep

Q-networks. arXiv preprint arXiv:1802.04412  2018.

Barreto  A.  Dabney  W.  Munos  R.  Hunt  J. J.  Schaul  T.  van Hasselt  H. P.  and Silver  D. Successor
features for transfer in reinforcement learning. In Advances in Neural Information Processing
Systems (NeurIPS)  2017.

Busoniu  L.  Babuska  R.  Schutter  B. D.  and Ernst  D. Reinforcement Learning and Dynamic

Programming Using Function Approximators. CRC Press  2010.

Dayan  P. Improving generalization for temporal difference learning: the successor representation.

Neural Computation  5(4):613–624  1993.

Dearden  R.  Friedman  N.  and Russell  S. J. Bayesian Q-Learning. In AAAI/IAAI  pp. 761–768.

AAAI Press / The MIT Press  1998.

Ecoffet  A.  Huizinga  J.  Lehman  J.  Stanley  K. O.  and Clune  J. Go-explore: a new approach for

hard-exploration problems. arXiv preprint arXiv:1901.10995  2019.

Gal  Y. Uncertainty in deep learning. PhD thesis  University of Cambridge  2016.

Hessel  M.  Modayil  J.  van Hasselt  H.  Schaul  T.  Ostrovski  G.  Dabney  W.  Horgan  D.  Piot  B. 
Azar  M. G.  and Silver  D. Rainbow: combining improvements in deep reinforcement learning. In
AAAI Conference on Artiﬁcial Intelligence  2018.

Kaelbling  L. P.  Littman  M. L.  and Moore  A. W. Reinforcement learning: a survey. Journal of

artiﬁcial intelligence research  4:237–285  1996.

Kingma  D. P. and Ba  J. Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980 

2014.

Kulkarni  T. D.  Saeedi  A.  Gautam  S.  and Gershman  S. J. Deep successor reinforcement learning.

arXiv preprint arXiv:1606.02396  2016.

Levine  N.  Zahavy  T.  Mankowitz  D. J.  Tamar  A.  and Mannor  S. Shallow updates for deep
reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS)  2017.

9

Lipton  Z. C.  Li  X.  Gao  J.  Li  L.  Ahmed  F.  and Deng  L. BBQ-Networks: efﬁcient exploration in
deep reinforcement learning for task-oriented dialogue systems. In AAAI Conference on Artiﬁcial
Intelligence  2018.

Machado  M. C.  Rosenbaum  C.  Guo  X.  Liu  M.  Tesauro  G.  and Campbell  M. Eigenoption

discovery through the deep successor representation. arXiv preprint arXiv:1710.11089  2017.

Machado  M. C.  Bellemare  M. G.  and Bowling  M. Count-based exploration with the successor

representation. arXiv preprint arXiv:1807.11622  2018.

Mnih  V.  Kavukcuoglu  K.  Silver  D.  Rusu  A. A.  Veness  J.  Bellemare  M. G.  Graves  A.  Ried-
miller  M.  Fidjeland  A. K.  Ostrovski  G.  et al. Human-level control through deep reinforcement
learning. Nature  518(7540):529  2015.

Moerland  T. M.  Broekens  J.  and Jonker  C. M. Efﬁcient exploration with double uncertain value

networks. arXiv preprint arXiv:1711.10789  2017.

Munos  R.  Stepleton  T.  Harutyunyan  A.  and Bellemare  M. Safe and efﬁcient off-policy reinforce-

ment learning. In Advances in Neural Information Processing Systems (NeurIPS)  2016.

O’Donoghue  B.  Osband  I.  Munos  R.  and Mnih  V. The uncertainty Bellman equation and

exploration. In International Conference on Machine Learning (ICML)  2018.

Osband  I. and Van Roy  B. On lower bounds for regret in reinforcement learning. arXiv preprint

arXiv:1608.02732  2016.

Osband  I.  Russo  D.  and Van Roy  B. (More) efﬁcient reinforcement learning via posterior sampling.

In Advances in Neural Information Processing Systems  2013.

Osband  I.  Blundell  C.  Pritzel  A.  and Van Roy  B. Deep exploration via bootstrapped DQN. In

Advances in Neural Information Processing Systems (NeurIPS)  2016a.

Osband  I.  Van Roy  B.  and Wen  Z. Generalization and exploration via randomized value functions.

International Conference on Machine Learning (ICML)  2016b.

Osband  I.  Aslanides  J.  and Cassirer  A. Randomized prior functions for deep reinforcement

learning. In Advances in Neural Information Processing Systems (NeurIPS)  2018.

Plappert  M.  Houthooft  R.  Dhariwal  P.  Sidor  S.  Chen  R. Y.  Chen  X.  Asfour  T.  Abbeel  P. 
and Andrychowicz  M. Parameter space noise for exploration. In International Conference on
Learning Representations (ICLR)  2018.

Precup  D. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty

Publication Series  2000.

Schaul  T.  Quan  J.  Antonoglou  I.  and Silver  D. Prioritized experience replay. arXiv preprint

arXiv:1511.05952  2015.

Strens  M. A Bayesian framework for reinforcement learning. In Conference on Machine Learning

(ICML)  2000.

Sutton  R. S.  Barto  A. G.  et al. Reinforcement learning: An introduction. MIT press  1998.

Thompson  W. R. On the likelihood that one unknown probability exceeds another in view of the

evidence of two samples. Biometrika  25(3/4):285–294  1933.

Touati  A.  Satija  H.  Romoff  J.  Pineau  J.  and Vincent  P. Randomized value functions via

multiplicative normalizing ﬂows. arXiv preprint arXiv:1806.02315  2018.

Van Hasselt  H.  Guez  A.  and Silver  D. Deep reinforcement learning with double Q-learning. In

AAAI Conference on Artiﬁcial Intelligence  2016.

Wang  Z.  Schaul  T.  Hessel  M.  Van Hasselt  H.  Lanctot  M.  and De Freitas  N. Dueling network
architectures for deep reinforcement learning. In International Conference on Machine Learning
(ICML)  2016.

10

,David Janz
Jiri Hron
Przemysław Mazur
Katja Hofmann
José Miguel Hernández-Lobato
Sebastian Tschiatschek