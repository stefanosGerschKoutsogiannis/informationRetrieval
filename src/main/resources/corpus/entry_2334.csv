2017,Good Semi-supervised Learning That Requires a Bad GAN,Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results  but it is not clear 1) how the discriminator benefits from joint training with a generator  and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically we show that given the discriminator objective  good semi-supervised learning indeed requires a bad generator  and propose the definition of a preferred generator. Empirically  we derive a novel formulation based on our analysis that substantially improves over feature matching GANs  obtaining state-of-the-art results on multiple benchmark datasets.,Good Semi-supervised Learning

That Requires a Bad GAN

Zihang Dai∗  Zhilin Yang∗  Fan Yang  William W. Cohen  Ruslan Salakhutdinov

School of Computer Science
Carnegie Melon University

dzihang zhiliny fanyang1 wcohen rsalakhu@cs.cmu.edu

Abstract

Semi-supervised learning methods based on generative adversarial networks
(GANs) obtained strong empirical results  but it is not clear 1) how the discrimina-
tor beneﬁts from joint training with a generator  and 2) why good semi-supervised
classiﬁcation performance and a good generator cannot be obtained at the same
time. Theoretically we show that given the discriminator objective  good semi-
supervised learning indeed requires a bad generator  and propose the deﬁnition
of a preferred generator. Empirically  we derive a novel formulation based on
our analysis that substantially improves over feature matching GANs  obtaining
state-of-the-art results on multiple benchmark datasets2.

1

Introduction

Deep neural networks are usually trained on a large amount of labeled data  and it has been a challenge
to apply deep models to datasets with limited labels. Semi-supervised learning (SSL) aims to leverage
the large amount of unlabeled data to boost the model performance  particularly focusing on the
setting where the amount of available labeled data is limited. Traditional graph-based methods [2  26]
were extended to deep neural networks [22  23  8]  which involves applying convolutional neural
networks [10] and feature learning techniques to graphs so that the underlying manifold structure
can be exploited. [15] employs a Ladder network to minimize the layerwise reconstruction loss
in addition to the standard classiﬁcation loss. Variational auto-encoders have also been used for
semi-supervised learning [7  12] by maximizing the variational lower bound of the unlabeled data
log-likelihood.
Recently  generative adversarial networks (GANs) [6] were demonstrated to be able to generate
visually realistic images. GANs set up an adversarial game between a discriminator and a generator.
The goal of the discriminator is to tell whether a sample is drawn from true data or generated by the
generator  while the generator is optimized to generate samples that are not distinguishable by the
discriminator. Feature matching (FM) GANs [16] apply GANs to semi-supervised learning on K-
class classiﬁcation. The objective of the generator is to match the ﬁrst-order feature statistics between
the generator distribution and the true distribution. Instead of binary classiﬁcation  the discriminator
employs a (K + 1)-class objective  where true samples are classiﬁed into the ﬁrst K classes and
generated samples are classiﬁed into the (K + 1)-th class. This (K + 1)-class discriminator objective
leads to strong empirical results  and was later widely used to evaluate the effectiveness of generative
models [5  21].
Though empirically feature matching improves semi-supervised classiﬁcation performance  the
following questions still remain open. First  it is not clear why the formulation of the discriminator

∗Equal contribution. Ordering determined by dice rolling.
2Code is available at https://github.com/kimiyoung/ssl_bad_gan.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

can improve the performance when combined with a generator. Second  it seems that good semi-
supervised learning and a good generator cannot be obtained at the same time. For example  [16]
observed that mini-batch discrimination generates better images than feature matching  but feature
matching obtains a much better semi-supervised learning performance. The same phenomenon was
also observed in [21]  where the model generated better images but failed to improve the performance
on semi-supervised learning.
In this work  we take a step towards addressing these questions. First  we show that given the
current (K + 1)-class discriminator formulation of GAN-based SSL  good semi-supervised learning
requires a “bad” generator. Here by bad we mean the generator distribution should not match the
true data distribution. Then  we give the deﬁnition of a preferred generator  which is to generate
complement samples in the feature space. Theoretically  under mild assumptions  we show that a
properly optimized discriminator obtains correct decision boundaries in high-density areas in the
feature space if the generator is a complement generator.
Based on our theoretical insights  we analyze why feature matching works on 2-dimensional toy
datasets. It turns out that our practical observations align well with our theory. However  we also ﬁnd
that the feature matching objective has several drawbacks. Therefore  we develop a novel formulation
of the discriminator and generator objectives to address these drawbacks. In our approach  the
generator minimizes the KL divergence between the generator distribution and a target distribution
that assigns high densities for data points with low densities in the true distribution  which corresponds
to the idea of a complement generator. Furthermore  to enforce our assumptions in the theoretical
analysis  we add the conditional entropy term to the discriminator objective.
Empirically  our approach substantially improves over vanilla feature matching GANs  and obtains
new state-of-the-art results on MNIST  SVHN  and CIFAR-10 when all methods are compared under
the same discriminator architecture. Our results on MNIST and SVHN also represent state-of-the-art
amongst all single-model results.

2 Related Work

Besides the adversarial feature matching approach [16]  several previous works have incorporated the
idea of adversarial training in semi-supervised learning. Notably  [19] proposes categorical generative
adversarial networks (CatGAN)  which substitutes the binary discriminator in standard GAN with a
multi-class classiﬁer  and trains both the generator and the discriminator using information theoretical
criteria on unlabeled data. From the perspective of regularization  [14  13] propose virtual adversarial
training (VAT)  which effectively smooths the output distribution of the classiﬁer by seeking virtually
adversarial samples. It is worth noting that VAT bears a similar merit to our approach  which is to
learn from auxiliary non-realistic samples rather than realistic data samples. Despite the similarity 
the principles of VAT and our approach are orthogonal  where VAT aims to enforce a smooth function
while we aim to leverage a generator to better detect the low-density boundaries. Different from
aforementioned approaches  [24] proposes to train conditional generators with adversarial training
to obtain complete sample pairs  which can be directly used as additional training cases. Recently 
Triple GAN [11] also employs the idea of conditional generator  but uses adversarial cost to match
the two model-deﬁned factorizations of the joint distribution with the one deﬁned by paired data.
Apart from adversarial training  there has been other efforts in semi-supervised learning using deep
generative models recently. As an early work  [7] adapts the original Variational Auto-Encoder
(VAE) to a semi-supervised learning setting by treating the classiﬁcation label as an additional
latent variable in the directed generative model. [12] adds auxiliary variables to the deep VAE
structure to make variational distribution more expressive. With the boosted model expressiveness 
auxiliary deep generative models (ADGM) improve the semi-supervised learning performance upon
the semi-supervised VAE. Different from the explicit usage of deep generative models  the Ladder
networks [15] take advantage of the local (layerwise) denoising auto-encoding criterion  and create a
more informative unsupervised signal through lateral connection.

3 Theoretical Analysis
Given a labeled set L = {(x  y)}  let {1  2 ···   K} be the label space for classiﬁcation. Let D and
G denote the discriminator and generator  and PD and pG denote the corresponding distributions.

2

Consider the discriminator objective function of GAN-based semi-supervised learning [16]:
Ex y∼L log PD(y|x  y ≤ K) + Ex∼p log PD(y ≤ K|x) + Ex∼pG log PD(K + 1|x) 

max

D

(1)

where p is the true data distribution. The probability distribution PD is over K + 1 classes where
the ﬁrst K classes are true classes and the (K + 1)-th class is the fake class. The objective function
consists of three terms. The ﬁrst term is to maximize the log conditional probability for labeled data 
which is the standard cost as in supervised learning setting. The second term is to maximize the log
probability of the ﬁrst K classes for unlabeled data. The third term is to maximize the log probability
of the (K + 1)-th class for generated data. Note that the above objective function bears a similar
merit to the original GAN formulation if we treat P (K + 1|x) to be the probability of fake samples 
while the only difference is that we split the probability of true samples into K sub-classes.
Let f (x) be a nonlinear vector-valued function  and wk be the weight vector for class k. As a standard
setting in previous work [16  5]  the discriminator D is deﬁned as PD(k|x) =
.
k(cid:48) f (x))
Since this is a form of over-parameterization  wK+1 is ﬁxed as a zero vector [16]. We next discuss
the choices of different possible G’s.

exp(w(cid:62)
k(cid:48)=1

(cid:80)K+1

exp(w(cid:62)

k f (x))

3.1 Perfect Generator

Here  by perfect generator we mean that the generator distribution pG exactly matches the true data
distribution p  i.e.  pG = p. We now show that when the generator is perfect  it does not improve the
generalization over the supervised learning setting.
Proposition 1. If pG = p  and D has inﬁnite capacity  then for any optimal solution D = (w  f ) of
the following supervised objective 

Ex y∼L log PD(y|x  y ≤ K) 

D

max

(2)
there exists D∗ = (w∗  f∗) such that D∗ maximizes Eq. (1) and that for all x  PD(y|x  y ≤ K) =
PD∗ (y|x  y ≤ K).
The proof is provided in the supplementary material. Proposition 1 states that for any optimal solution
D of the supervised objective  there exists an optimal solution D∗ of the (K + 1)-class objective such
that D and D∗ share the same generalization error. In other words  using the (K + 1)-class objective
does not prevent the model from experiencing any arbitrarily high generalization error that it could
suffer from under the supervised objective. Moreover  since all the optimal solutions are equivalent
w.r.t. the (K + 1)-class objective  it is the optimization algorithm that really decides which speciﬁc
solution the model will reach  and thus what generalization performance it will achieve. This implies
that when the generator is perfect  the (K + 1)-class objective by itself is not able to improve the
generalization performance. In fact  in many applications  an almost inﬁnite amount of unlabeled
data is available  so learning a perfect generator for purely sampling purposes should not be useful.
In this case  our theory suggests that not only the generator does not help  but also unlabeled data is
not effectively utilized when the generator is perfect.

3.2 Complement Generator

The function f maps data points in the input space to the feature space. Let pk(f ) be the density of the
data points of class k in the feature space. Given a threshold k  let Fk be a subset of the data support
where pk(f ) > k  i.e.  Fk = {f : pk(f ) > k}. We assume that given {k}K
k=1  the Fk’s are disjoint
with a margin. More formally  for any fj ∈ Fj  fk ∈ Fk  and j (cid:54)= k  we assume that there exists a
real number 0 < α < 1 such that αfj + (1 − α)fk /∈ Fj ∪ Fk. As long as the probability densities
of different classes do not share any mode  i.e.  ∀i (cid:54)= j  argmaxf pi(f ) ∩ argmaxf pj(f ) = ∅  this
assumption can always be satisﬁed by tuning the thresholds k’s. With the assumption held  we will
show that the model performance would be better if the thresholds could be set to smaller values
(ideally zero). We also assume that each Fk contains at least one labeled data point.
Suppose ∪K
k=1Fk is bounded by a convex set B. If the support FG of a generator G in the feature
space is a relative complement set in B  i.e.  FG = B − ∪K
k=1Fk  we call G a complement generator.
The reason why we utilize a bounded B to deﬁne the complement is presented in the supplementary

3

material. Note that the deﬁnition of complement generator implies that G is a function of f. By
treating G as function of f  theoretically D can optimize the original objective function in Eq. (1).
Now we present the assumption on the convergence conditions of the discriminator. Let U and G be
the sets of unlabeled data and generated data.
Assumption 1. Convergence conditions. When D converges on a ﬁnite training set {L U G}  D
learns a (strongly) correct decision boundary for all training data points. More speciﬁcally  (1) for
any (x  y) ∈ L  we have w(cid:62)
k f (x) for any other class k (cid:54)= y; (2) for any x ∈ G  we have
0 > maxK

k f (x); (3) for any x ∈ U  we have maxK

y f (x) > w(cid:62)

k f (x) > 0.

k=1 w(cid:62)

k=1 w(cid:62)

log(cid:80)

In Assumption 1  conditions (1) and (2) assume classiﬁcation correctness on labeled data and
true-fake correctness on generated data respectively  which is directly induced by the objective
function. Likewise  it is also reasonable to assume true-fake correctness on unlabeled data  i.e. 
k f (x) > 0 for x ∈ U. However  condition (3) goes beyond this and assumes
k exp w(cid:62)
maxk w(cid:62)
k f (x) > 0. We discuss this issue in detail in the supplementary material and argue that these
assumptions are reasonable. Moreover  in Section 5  our approach addresses this issue explicitly by
adding a conditional entropy term to the discriminator objective to enforce condition (3).
Lemma 1. Suppose for all k  the L2-norms of weights wk are bounded by (cid:107)wk(cid:107)2 ≤ C. Suppose that
there exists  > 0 such that for any fG ∈ FG  there exists f(cid:48)
G(cid:107)2 ≤ . With
the conditions in Assumption 1  for all k ≤ K  we have w(cid:62)
Corollary 1. When unlimited generated data samples are available  with the conditions in Lemma 1 
we have lim|G|→∞ w(cid:62)
See the supplementary material for the proof.
Proposition 2. Given the conditions in Corollary 1  for all class k ≤ K  for all feature space points
fk ∈ Fk  we have w(cid:62)

G ∈ G such that (cid:107)fG − f(cid:48)
k fG < C.

j fk for any j (cid:54)= k.

k fG ≤ 0.

k fk > w(cid:62)

k fk ≤ w(cid:62)

Proof. Without loss of generality  suppose j = arg maxj(cid:54)=k w(cid:62)
j fk. Now we prove it by contradiction.
j fk. Since Fk’s are disjoint with a margin  B is a convex set and FG =
Suppose w(cid:62)
B − ∪kFk  there exists 0 < α < 1 such that fG = αfk + (1 − α)fj with fG ∈ FG and fj
j fG ≤ 0. Thus 
being the feature of a labeled data point in Fj. By Corollary 1  it follows that w(cid:62)
j fk + (1 − α)w(cid:62)
w(cid:62)
j fG = αw(cid:62)
j fj > 0  leading to
contradiction. It follows that w(cid:62)

j fj ≤ 0. By Assumption 1  w(cid:62)
k fk > w(cid:62)

j fk for any j (cid:54)= k.

j fk > 0 and w(cid:62)

Proposition 2 guarantees that when G is a complement generator  under mild assumptions  a near-
optimal D learns correct decision boundaries in each high-density subset Fk (deﬁned by k) of the
data support in the feature space. Intuitively  the generator generates complement samples so the
logits of the true classes are forced to be low in the complement. As a result  the discriminator
obtains class boundaries in low-density areas. This builds a connection between our approach with
manifold-based methods [2  26] which also leverage the low-density boundary assumption.
With our theoretical analysis  we can now answer the questions raised in Section 1. First  the (K + 1)-
class formulation is effective because the generated complement samples encourage the discriminator
to place the class boundaries in low-density areas (Proposition 2). Second  good semi-supervised
learning indeed requires a bad generator because a perfect generator is not able to improve the
generalization performance (Proposition 1).

4 Case Study on Synthetic Data

In the previous section  we have established the fact a complement generator  instead of a perfect
generator  is what makes a good semi-supervised learning algorithm. Now  to get a more intuitive
understanding  we conduct a case study based on two 2D synthetic datasets  where we can easily
verify our theoretical analysis by visualizing the model behaviors. In addition  by analyzing how
feature matching (FM) [16] works in 2D space  we identify some potential problems of it  which
motivates our approach to be introduced in the next section. Speciﬁcally  two synthetic datasets are
four spins and two circles  as shown in Fig. 1.

4

Figure 1: Labeled and unlabeled data are denoted by
cross and point respectively  and different colors indicate
classes.

Figure 2: Left: Classiﬁcation decision boundary 
where the white line indicates true-fake boundary;
Right: True-Fake decision boundary

Figure 3: Feature space
at convergence

Figure 4: Left: Blue points are generated data  and the black shadow indicates
unlabeled data. Middle and right can be interpreted as above.

Soundness of complement generator Firstly  to verify that the complement generator is a preferred
choice  we construct the complement generator by uniformly sampling from the a bounded 2D box
that contains all unlabeled data  and removing those on the manifold. Based on the complement
generator  the result on four spins is visualized in Fig. 2. As expected  both the classiﬁcation
and true-fake decision boundaries are almost perfect. More importantly  the classiﬁcation decision
boundary always lies in the fake data area (left panel)  which well matches our theoretical analysis.

Visualization of feature space Next  to verify our analysis about the feature space  we choose the
feature dimension to be 2  apply the FM to the simpler dataset of two circles  and visualize the feature
space in Fig. 3. As we can see  most of the generated features (blue points) resides in between the
features of two classes (green and orange crosses)  although there exists some overlap. As a result 
the discriminator can almost perfectly distinguish between true and generated samples as indicated
by the black decision boundary  satisfying the our required Assumption 1. Meanwhile  the model
obtains a perfect classiﬁcation boundary (blue line) as our analysis suggests.

Pros and cons of feature matching Finally  to further understand the strength and weakness of
FM  we analyze the solution FM reaches on four spins shown in Fig. 4. From the left panel  we can
see many of the generated samples actually fall into the data manifold  while the rest scatters around
in the nearby surroundings of data manifold. It suggests that by matching the ﬁrst-order moment by
SGD  FM is performing some kind of distribution matching  though in a rather weak manner. Loosely
speaking  FM has the effect of generating samples close to the manifold. But due to its weak power
in distribution matching  FM will inevitably generate samples outside of the manifold  especially
when the data complexity increases. Consequently  the generator density pG is usually lower than
the true data density p within the manifold and higher outside. Hence  an optimal discriminator
PD∗ (K + 1 | x) = p(x)/(p(x) + pG(x)) could still distinguish between true and generated samples
in many cases. However  there are two types of mistakes the discriminator can still make

1. Higher density mistake inside manifold: Since the FM generator still assigns a signiﬁcant amount
of probability mass inside the support  wherever pG > p > 0  an optimal discriminator will
incorrectly predict samples in that region as “fake”. Actually  this problem has already shown up
when we examine the feature space (Fig. 3).

2. Collapsing with missing coverage outside manifold: As the feature matching objective for the
generator only requires matching the ﬁrst-order statistics  there exists many trivial solutions the
generator can end up with. For example  it can simply collapse to mean of unlabeled features 
or a few surrounding modes as along as the feature mean matches. Actually  we do see such

5

collapsing phenomenon in high-dimensional experiments when FM is used (see Fig. 5a and
Fig. 5c) As a result  a collapsed generator will fail to cover some gap areas between manifolds.
Since the discriminator is only well-deﬁned on the union of the data supports of p and pG  the
prediction result in such missing area is under-determined and fully relies on the smoothness of
the parametric model. In this case  signiﬁcant mistakes can also occur.

5 Approach

As discussed in previous sections  feature matching GANs suffer from the following drawbacks: 1)
the ﬁrst-order moment matching objective does not prevent the generator from collapsing (missing
coverage); 2) feature matching can generate high-density samples inside manifold; 3) the discriminator
objective does not encourage realization of condition (3) in Assumption 1 as discussed in Section 3.2.
Our approach aims to explicitly address the above drawbacks.
Following prior work [16  6]  we employ a GAN-like implicit generator. We ﬁrst sample a latent
variable z from a uniform distribution U(0  1) for each dimension  and then apply a deep convolutional
network to transform z to a sample x.

5.1 Generator Entropy

Fundamentally  the ﬁrst drawback concerns the entropy of the distribution of generated features 
H(pG(f )). This connection is rather intuitive  as the collapsing issue is a clear sign of low entropy.
Therefore  to avoid collapsing and increase coverage  we consider explicitly increasing the entropy.
Although the idea sounds simple and straightforward  there are two practical challenges. Firstly  as
implicit generative models  GANs only provide samples rather than an analytic density form. As a
result  we cannot evaluate the entropy exactly  which rules out the possibility of naive optimization.
More problematically  the entropy is deﬁned in a high-dimensional feature space  which is changing
dynamically throughout the training process. Consequently  it is difﬁcult to estimate and optimize the
generator entropy in the feature space in a stable and reliable way. Faced with these difﬁculties  we
consider two practical solutions.
The ﬁrst method is inspired by the fact that input space is essentially static  where estimating and
optimizing the counterpart quantities would be much more feasible. Hence  we instead increase the
generator entropy in the input space  i.e.  H(pG(x))  using a technique derived from an information
theoretical perspective and relies on variational inference (VI). Specially  let Z be the latent variable
space  and X be the input space. We introduce an additional encoder  q : X (cid:55)→ Z  to deﬁne
a variational upper bound of the negative entropy [3]  −H(pG(x)) ≤ −Ex z∼pG log q(z|x) =
LVI. Hence  minimizing the upper bound LVI effectively increases the generator entropy. In our
implementation  we formulate q as a diagonal Gaussian with bounded variance  i.e. q(z|x) =
N (µ(x)  σ2(x))  with 0 < σ(x) < θ  where µ(·) and σ(·) are neural networks  and θ is the threshold
to prevent arbitrarily large variance.
Alternatively  the second method aims at increasing the generator entropy in the feature space by
optimizing an auxiliary objective. Concretely  we adapt the pull-away term (PT) [25] as the auxiliary
cost  LPT =
  where N is the size of a mini-batch and x are
samples. Intuitively  the pull-away term tries to orthogonalize the features in each mini-batch by
minimizing the squared cosine similarity. Hence  it has the effect of increasing the diversity of
generated features and thus the generator entropy.

(cid:16) f (xi)(cid:62)f (xj )

(cid:107)f (xi)(cid:107)(cid:107)f (xj )(cid:107)

(cid:80)N

(cid:80)

1

N (N−1)

i=1

j(cid:54)=i

(cid:17)2

5.2 Generating Low-Density Samples

The second drawback of feature matching GANs is that high-density samples can be generated in the
feature space  which is not desirable according to our analysis. Similar to the argument in Section
5.1  it is infeasible to directly minimize the density of generated features. Instead  we enforce the
generation of samples with low density in the input space. Speciﬁcally  given a threshold   we
minimize the following term as part of our objective:

Ex∼pG log p(x)I[p(x) > ]

(3)

6

where I[·] is an indicator function. Using a threshold   we ensure that only high-density samples are
penalized while low-density samples are unaffected. Intuitively  this objective pushes the generated
samples to “move” towards low-density regions deﬁned by p(x). To model the probability distribution
over images  we simply adapt the state-of-the-art density estimation model for natural images  namely
the PixelCNN++ [17] model. The PixelCNN++ model is used to estimate the density p(x) in Eq. (3).
The model is pretrained on the training set  and ﬁxed during semi-supervised training.

5.3 Generator Objective and Interpretation

Combining our solutions to the ﬁrst two drawbacks of feature matching GANs  we have the following
objective function of the generator:

−H(pG) + Ex∼pG log p(x)I[p(x) > ] + (cid:107)Ex∼pG f (x) − Ex∼U f (x)(cid:107)2.

(4)

min

G

This objective is closely related to the idea of complement generator discussed in Section 3. To see
that  let’s ﬁrst deﬁne a target complement distribution in the input space as follows

(cid:40) 1

Z
C

p∗(x) =

1

p(x)

if p(x) >  and x ∈ Bx
if p(x) ≤  and x ∈ Bx 

(cid:0)I[p(x) > ] log Z−I[p(x) ≤ ] log C(cid:1).

where Z is a normalizer  C is a constant  and Bx is the set deﬁned by mapping B from the feature
space to the input space. With the deﬁnition  the KL divergence (KLD) between pG(x) and p∗(x) is
KL(pG(cid:107)p∗) = −H(pG)+Ex∼pG log p(x)I[p(x) > ]+Ex∼pG
The form of the KLD immediately reveals the aforementioned connection. Firstly  the KLD shares
two exactly the same terms with the generator objective (4). Secondly  while p∗(x) is only deﬁned in
Bx  there is not such a hard constraint on pG(x). However  the feature matching term in Eq. (4) can
be seen as softly enforcing this constraint by bringing generated samples “close” to the true data (Cf.
Section 4). Moreover  because the identity function I[·] has zero gradient almost everywhere  the last
term in KLD would not contribute any informative gradient to the generator. In summary  optimizing
our proposed objective (4) can be understood as minimizing the KL divergence between the generator
distribution and a desired complement distribution  which connects our practical solution to our
theoretical analysis.

5.4 Conditional Entropy

Instead  it only needs(cid:80)K

In order for the complement generator to work  according to condition (3) in Assumption 1  the
discriminator needs to have strong true-fake belief on unlabeled data  i.e.  maxK
k f (x) > 0.
However  the objective function of the discriminator in [16] does not enforce a dominant class.
k=1 PD(k|x) > PD(K + 1|x) to obtain a correct decision boundary  while
the probabilities PD(k|x) for k ≤ K can possibly be uniformly distributed. To guarantee the strong
true-fake belief in the optimal conditions  we add a conditional entropy term to the discriminator
objective and it becomes 

k=1 w(cid:62)

Ex y∼L log pD(y|x  y ≤ K) + Ex∼U log pD(y ≤ K|x)+

max

D

Ex∼pG log pD(K + 1|x) + Ex∼U

pD(k|x) log pD(k|x).

(5)

K(cid:88)

k=1

By optimizing Eq. (5)  the discriminator is encouraged to satisfy condition (3) in Assumption 1. Note
that the same conditional entropy term has been used in other semi-supervised learning methods
[19  13] as well  but here we motivate the minimization of conditional entropy based on our theoretical
analysis of GAN-based semi-supervised learning.
To train the networks  we alternatively update the generator and the discriminator to optimize Eq. (4)
and Eq. (5) based on mini-batches. If an encoder is used to maximize H(pG)  the encoder and the
generator are updated at the same time.

6 Experiments

We mainly consider three widely used benchmark datasets  namely MNIST  SVHN  and CIFAR-10.
As in previous work  we randomly sample 100  1 000  and 4 000 labeled samples for MNIST  SVHN 

7

MNIST (# errors)

SVHN (% errors) CIFAR-10 (% errors)

Methods
CatGAN [19]
SDGM [12]
Ladder network [15]
ADGM [12]
FM [16] ∗
ALI [4]
VAT small [13] ∗
Our best model ∗
Triple GAN [11] ∗‡
Π model [9] †‡
VAT+EntMin+Large [13]†

191 ± 10
132 ± 7
106 ± 37
96 ± 2
93 ± 6.5

-
136

79.5 ± 9.8
91± 58

16.61 ± 0.24

-

-

22.86

6.83

8.11 ± 1.3
7.42 ± 0.65
4.25 ± 0.03
5.77 ± 0.17
5.43 ± 0.25

-

-

19.58 ± 0.46
20.40 ± 0.47
18.63 ± 2.32
17.99 ± 1.62
14.41 ± 0.30
16.99 ± 0.36
16.55 ± 0.29

14.87

-
-

4.28

13.15

Table 1: Comparison with state-of-the-art methods on three benchmark datasets. Only methods without data
augmentation are included. ∗ indicates using the same (small) discriminator architecture  † indicates using a
larger discriminator architecture  and ‡ means self-ensembling.

(a) FM on SVHN

(b) Ours on SVHN

(c) FM on CIFAR

(d) Ours on CIFAR

Figure 5: Comparing images generated by FM and our model. FM generates collapsed samples  while our
model generates diverse “bad” samples.

and CIFAR-10 respectively during training  and use the standard data split for testing. We use the
10-quantile log probability to deﬁne the threshold  in Eq. (4). We add instance noise to the input of
the discriminator [1  18]  and use spatial dropout [20] to obtain faster convergence. Except for these
two modiﬁcations  we use the same neural network architecture as in [16]. For fair comparison  we
also report the performance of our FM implementation with the aforementioned differences.

6.1 Main Results

We compare the the results of our best model with state-of-the-art methods on the benchmarks in
Table 1. Our proposed methods consistently improve the performance upon feature matching. We
achieve new state-of-the-art results on all the datasets when only small discriminator architecture is
considered. Our results are also state-of-the-art on MNIST and SVHN among all single-model results 
even when compared with methods using self-ensembling and large discriminator architectures.
Finally  note that because our method is actually orthogonal to VAT [13]  combining VAT with our
presented approach should yield further performance improvement in practice.

6.2 Ablation Study

We report the results of ablation study in Table 2. In the following  we analyze the effects of several
components in our model  subject to the intrinsic features of different datasets.
First  the generator entropy terms (VI and PT) (Section 5.1) improve the performance on SVHN and
CIFAR by up to 2.2 points in terms of error rate. Moreover  as shown in Fig 5  our model signiﬁcantly
reduces the collapsing effects present in the samples generated by FM  which also indicates that
maximizing the generator entropy is beneﬁcial. On MNIST  probably due to its simplicity  no
collapsing phenomenon was observed with vanilla FM training [16] or in our setting. Under such
circumstances  maximizing the generator entropy seems to be unnecessary  and the estimation bias
introduced by approximation techniques can even hurt the performance.

8

Setting
MNIST FM
MNIST FM+VI
MNIST FM+LD
MNIST FM+LD+Ent

Setting
SVHN FM
SVHN FM+VI
SVHN FM+PT
SVHN FM+PT+Ent
SVHN FM+PT+LD+Ent

Setting

CIFAR FM+VI+Ent

Error
85.0 ± 11.7 CIFAR FM
86.5 ± 10.6 CIFAR FM+VI
79.5 ± 9.8
89.2 ± 10.5
Error
6.83
5.29
4.63
4.25
4.19

Setting
MNIST FM
MNIST FM+LD
SVHN FM+PT+Ent
SVHN FM+PT+LD+Ent
SVHN 10-quant

Error
16.14
14.41
15.82

Max log-p
-297
-659
-5809
-5919
-5622

Setting  as q-th centile
Error on MNIST

q = 2
77.7 ± 6.1

q = 10
79.5 ± 9.8

q = 20
80.1 ± 9.6

q = 100
85.0 ± 11.7

Table 2: Ablation study. FM is feature matching. LD is the low-density enforcement term in Eq. (3). VI and
PT are two entropy maximization methods described in Section 5.1. Ent means the conditional entropy term in
Eq. (5). Max log-p is the maximum log probability of generated samples  evaluated by a PixelCNN++ model.
10-quant shows the 10-quantile of true image log probability. Error means the number of misclassiﬁed examples
on MNIST  and error rate (%) on others.

Second  the low-density (LD) term is useful when FM indeed generates samples in high-density areas.
MNIST is a typical example in this case. When trained with FM  most of the generated hand written
digits are highly realistic and have high log probabilities according to the density model (Cf. max
log-p in Table 2). Hence  when applied to MNIST  LD improves the performance by a clear margin.
By contrast  few of the generated SVHN images are realistic (Cf. Fig. 5a). Quantitatively  SVHN
samples are assigned very low log probabilities (Cf. Table 2). As expected  LD has a negligible effect
on the performance for SVHN. Moreover  the “max log-p” column in Table 2 shows that while LD
can reduce the maximum log probability of the generated MNIST samples by a large margin  it does
not yield noticeable difference on SVHN. This further justiﬁes our analysis. Based on the above
conclusion  we conjecture LD would not help on CIFAR where sample quality is even lower. Thus 
we did not train a density model on CIFAR due to the limit of computational resources.
Third  adding the conditional entropy term has mixed effects on different datasets. While the
conditional entropy (Ent) is an important factor of achieving the best performance on SVHN  it hurts
the performance on MNIST and CIFAR. One possible explanation relates to the classic exploitation-
exploration tradeoff  where minimizing Ent favors exploitation and minimizing the classiﬁcation loss
favors exploration. During the initial phase of training  the discriminator is relatively uncertain and
thus the gradient of the Ent term might dominate. As a result  the discriminator learns to be more
conﬁdent even on incorrect predictions  and thus gets trapped in local minima.
Lastly  we vary the values of the hyper-parameter  in Eq. (4). As shown at the bottom of Table 2 
reducing  clearly leads to better performance  which further justiﬁes our analysis in Sections 4 and 3
that off-manifold samples are favorable.

6.3 Generated Samples

We compare the generated samples of FM and our approach in Fig. 5. The FM images in Fig. 5c are
extracted from previous work [16]. While collapsing is widely observed in FM samples  our model
generates diverse “bad” images  which is consistent with our analysis.

7 Conclusions

In this work  we present a semi-supervised learning framework that uses generated data to boost
task performance. Under this framework  we characterize the properties of various generators and
theoretically prove that a complementary (i.e. bad) generator improves generalization. Empirically our
proposed method improves the performance of image classiﬁcation on several benchmark datasets.

9

Acknowledgement

This work was supported by the DARPA award D17AP00001  the Google focused award  and the
Nvidia NVAIL award. The authors would also like to thank Han Zhao for his insightful feedback.

References
[1] Martin Arjovsky and Léon Bottou. Towards principled methods for training generative adver-
sarial networks. In NIPS 2016 Workshop on Adversarial Training. In review for ICLR  volume
2016  2017.

[2] Mikhail Belkin  Partha Niyogi  and Vikas Sindhwani. Manifold regularization: A geometric
framework for learning from labeled and unlabeled examples. Journal of machine learning
research  7(Nov):2399–2434  2006.

[3] Zihang Dai  Amjad Almahairi  Philip Bachman  Eduard Hovy  and Aaron Courville. Calibrating

energy-based generative adversarial networks. arXiv preprint arXiv:1702.01691  2017.

[4] Jeff Donahue  Philipp Krähenbühl  and Trevor Darrell. Adversarial feature learning. arXiv

preprint arXiv:1605.09782  2016.

[5] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Alex Lamb  Martin Arjovsky  Olivier
arXiv preprint

Mastropietro  and Aaron Courville. Adversarially learned inference.
arXiv:1606.00704  2016.

[6] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[7] Diederik P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-
supervised learning with deep generative models. In Advances in Neural Information Processing
Systems  pages 3581–3589  2014.

[8] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. arXiv preprint arXiv:1609.02907  2016.

[9] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint

arXiv:1610.02242  2016.

[10] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning

applied to document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[11] Chongxuan Li  Kun Xu  Jun Zhu  and Bo Zhang. Triple generative adversarial nets. arXiv

preprint arXiv:1703.02291  2017.

[12] Lars Maaløe  Casper Kaae Sønderby  Søren Kaae Sønderby  and Ole Winther. Auxiliary deep

generative models. arXiv preprint arXiv:1602.05473  2016.

[13] Takeru Miyato  Shin-ichi Maeda  Masanori Koyama  and Shin Ishii. Virtual adversarial train-
ing: a regularization method for supervised and semi-supervised learning. arXiv preprint
arXiv:1704.03976  2017.

[14] Takeru Miyato  Shin-ichi Maeda  Masanori Koyama  Ken Nakae  and Shin Ishii. Distributional

smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677  2015.

[15] Antti Rasmus  Mathias Berglund  Mikko Honkala  Harri Valpola  and Tapani Raiko. Semi-
supervised learning with ladder networks. In Advances in Neural Information Processing
Systems  pages 3546–3554  2015.

[16] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.

Improved techniques for training gans. In NIPS  2016.

10

[17] Tim Salimans  Andrej Karpathy  Xi Chen  and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modiﬁcations. arXiv preprint
arXiv:1701.05517  2017.

[18] Casper Kaae Sønderby  Jose Caballero  Lucas Theis  Wenzhe Shi  and Ferenc Huszár. Amortised

map inference for image super-resolution. arXiv preprint arXiv:1610.04490  2016.

[19] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical genera-

tive adversarial networks. arXiv preprint arXiv:1511.06390  2015.

[20] Jonathan Tompson  Ross Goroshin  Arjun Jain  Yann LeCun  and Christoph Bregler. Efﬁcient
object localization using convolutional networks. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 648–656  2015.

[21] Dmitry Ulyanov  Andrea Vedaldi  and Victor Lempitsky. Adversarial generator-encoder net-

works. arXiv preprint arXiv:1704.02304  2017.

[22] Jason Weston  Frédéric Ratle  Hossein Mobahi  and Ronan Collobert. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks of the Trade  pages 639–655. Springer 
2012.

[23] Zhilin Yang  William W Cohen  and Ruslan Salakhutdinov. Revisiting semi-supervised learning

with graph embeddings. arXiv preprint arXiv:1603.08861  2016.

[24] Zhilin Yang  Junjie Hu  Ruslan Salakhutdinov  and William W Cohen. Semi-supervised qa with

generative domain-adaptive nets. arXiv preprint arXiv:1702.02206  2017.

[25] Junbo Zhao  Michael Mathieu  and Yann LeCun. Energy-based generative adversarial network.

arXiv preprint arXiv:1609.03126  2016.

[26] Xiaojin Zhu  Zoubin Ghahramani  and John D Lafferty. Semi-supervised learning using gaussian
ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03)  pages 912–919  2003.

11

,Zihang Dai
Zhilin Yang
Fan Yang
William Cohen