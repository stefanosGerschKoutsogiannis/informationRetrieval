2018,Learning Others' Intentional Models in Multi-Agent Settings Using Interactive POMDPs,Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable  stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs  we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs  preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs  strategy levels  and transition  observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy.,Learning Others’ Intentional Models in Multi-Agent

Settings Using Interactive POMDPs

Piotr Gmytrasiewicz

Yanlin Han
Department of Computer Science
University of Illinois at Chicago

Chicago  IL 60607

{yhan37 piotr}@uic.edu

Abstract

Interactive partially observable Markov decision processes (I-POMDPs) provide a
principled framework for planning and acting in a partially observable  stochastic
and multi-agent environment.
It extends POMDPs to multi-agent settings by
including models of other agents in the state space and forming a hierarchical
belief structure. In order to predict other agents’ actions using I-POMDPs  we
propose an approach that effectively uses Bayesian inference and sequential Monte
Carlo sampling to learn others’ intentional models which ascribe to them beliefs 
preferences and rationality in action selection. Empirical results show that our
algorithm accurately learns models of the other agent and has superior performance
than methods that use subintentional models. Our approach serves as a generalized
Bayesian learning algorithm that learns other agents’ beliefs  strategy levels  and
transition  observation and reward functions. It also effectively mitigates the belief
space complexity due to the nested belief hierarchy.

1

Introduction

Partially observable Markov decision processes (POMDPs) [11] is a general decision-theoretic
framework for planning under uncertainty in a partially observable  stochastic environment. An
autonomous agent acts rationally in such settings by constantly maintaining beliefs of the physical
state and sequentially choosing the optimal actions that maximize the expected value of future rewards.
Thus  solutions of POMDPs map an agent’s beliefs to actions. Although POMDPs can be used in
multi-agent settings  it usually treats the impacts of other agents’ actions as noise and embeds them
into the state transition function. Examples of such POMDPs are Utile Sufﬁx Memory [14]  inﬁnite
generalized policy representation [13]  and inﬁnite POMDPs [3]. Therefore  an agent’s beliefs about
other agents are not part of the solutions of POMDPs.
Interactive POMDPs (I-POMDPs) [7] generalize POMDPs to multi-agent settings by replacing
POMDP belief spaces with interactive belief systems. Speciﬁcally  an I-POMDP augments the plain
beliefs about the physical states in POMDP by including models of other agents. The models of
other agents included in the new augmented belief space consist of two types: intentional models and
subintentional models. An intentional model ascribes beliefs  preferences  and rationality to other
agents [7]  while a simpler subintentional model  such as ﬁnite state controllers [15]  does not. The
augmentation of intentional models forms a hierarchical belief structure that represents an agent’s
belief about the physical state  belief about the other agents and their beliefs about others’ beliefs 
and so on. Solutions of I-POMDPs map an agent’s belief about the environment and other agents’
models to actions. It has been shown [7] that the added sophistication of modeling others as rational
agents results in a higher value function compared to one obtained from treating others as noise 
which implies the modeling superiority of I-POMDPs over other approaches.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

However  the interactive belief augmentation of I-POMDPs results in a drastic increase of the belief
space complexity  because the agent models grow exponentially as the belief nesting level increases.
Therefore  the complexity of the belief representation is proportional to belief dimensions  which
is known as the curse of dimensionality. Moreover  due to the fact that exact solutions to POMDPs
are PSPACE-complete and undecidable for ﬁnite and inﬁnite time horizon respectively[16]  the time
complexity of more generalized I-POMDPs is at least PSPACE-complete and undecidable for ﬁnite
and inﬁnite horizon  since an I-POMDP may contain multiple POMDPs or I-POMDPs of other agents.
Due to these complexities  a solution which accounts for an agent’s belief over an entire intentional
model has not been implemented up to date. There are partial solutions that depend on what is known
about other agents’ beliefs about the physical states [2]  but they do not include the state of an agent’s
knowledge about others’ reward  transition  and observation functions. Indirect approach such as
subintentional ﬁnite state controllers [15] do not include any of these elements either. To unleash the
full modeling power of intentional models and mitigate the aforementioned complexities  a robust
approximation algorithm is needed. The purpose of this algorithm is to compute the nested interactive
belief over elements of the intentional models and predict other agents’ actions. It is crucial to the
trade-off between solution quality and computational complexity.
To address these issues  we propose an approach that uses Bayesian inference and customized
sequential Monte Carlo sampling [4] to obtain approximate solutions to I-POMDPs. We assume that
the modeling agent maintains beliefs over intentional models of other agents and make sequential
Bayesian updates using observations from the environment. While in multi-agent settings  others
agents’ models other than their beliefs are usually assumed to be known  in our assumption the
modeling agent does not know any information about others’ beliefs  strategy levels  and transition 
observation  and reward functions. It only relies on learning indirectly from observations about the
environment  which is inﬂuenced by others agents’ actions. Since this Bayesian inference task is
analytically intractable due to the requirement of computing high dimensional integrations  we have
devised a customized sequential Monte Carlo method  extending the interactive particle ﬁlter (I-PF)
[2] to the entire intentional model space. The main idea of this method is to descend the nested
belief hierarchy  parametrize other agents’ model functions  and sample all model parameters at each
nesting level according to observations.
Our approach successfully recovers other agents’ models over the intentional model space which
contains their beliefs  strategy levels  and transition  observation and reward functions. It extends
I-POMDP’s belief update to larger model space  and therefore it serves as a generalized Bayesian
learning method for multi-agent systems in which other agents’ beliefs  transition  observation and
reward functions are unknown. By approximating Bayesian inference using a customized sequential
Monte Carlo sampling method  we signiﬁcantly mitigate the belief space complexity of I-POMDPs.

2 The Model

2.1

I-POMDP framework

I-POMDPs [7] generalize POMDPs [11] to multi-agent settings by including models of other agents
in the belief state space. The resulting hierarchical belief structure represents an agent’s belief about
the physical state  belief about the other agents and their beliefs about others’ beliefs  and can be
nested inﬁnitely in this recursive manner. Here we focus on the computable counterparts of inﬁnitely
nested I-POMDPs: ﬁnitely nested I-POMDPs. For simplicity of presentation  we consider two
interacting agents i and j. This formalism generalizes to more number of agents in a straightforward
manner.
A ﬁnitely nested interactive POMDP of agent i   I-POMDPi l  is deﬁned as:

I-P OM DPi l = (cid:104)ISi l  A  Ωi  Ti  Oi  Ri(cid:105)

(1)
where ISi l is a set of interactive states  deﬁned as ISi l = S × Mj l−1  l ≥ 1  S is the set of physical
states  Mj l−1 is the set of possible models of agent j  and l is the strategy (nesting) level. The set of
models  Mj l−1  can be divided into two classes  the intentional models  IMj l−1  and subintentional
models  SMj. Thus  Mj l−1 = IMj l−1 ∪ SMj.
The intentional models  IMj l−1  ascribe beliefs  preferences  and rationality in action selection to
other agents  thus they are analogous to types  θj  used in Bayesian games [10]. The intentional
models  Θj l−1  of agent j at level l − 1 is deﬁned as θj l−1 = (cid:104)bj l−1  A  Ωj  Tj  Oj  Rj  OCj(cid:105) 

2

where bj l−1 is agent j’s belief nested to the level (l − 1)  bj l−1 ∈ ∆(ISj l−1)  and OCj is j’s
optimality criterion. It can be rewritten as θj l−1 = (cid:104)bj l−1  ˆθj(cid:105)  where ˆθj includes all elements of the
intentional model other than the belief and is called the agent j’s frame.
The subintentional models  SMj  constitute the remaining models in Mj l−1. Examples of subinten-
tional models are ﬁnite state controllers [15]  no-information models [8] and ﬁctitious play models
[5].
The ISi l can be deﬁned in an inductive manner:

ISi 0 = S 
ISi 1 = S × Mj 0 
......
ISi l = S × Mj l−1 

Θj 0 = {(cid:104)bj 0  ˆθj(cid:105) : bj 0 ∈ ∆(ISj 0)}
Θj 1 = {(cid:104)bj 1  ˆθj(cid:105) : bj 1 ∈ ∆(ISj 1)}

Mj 0 = Θj 0 ∪ SMj
Mj 1 = Θj 1 ∪ Mj 0

(2)

Θj l = {(cid:104)bj l  ˆθj(cid:105) : bj l ∈ ∆(ISj l)}

Mj l = Θj l ∪ Mj l−1

All remaining components in an I-POMDP are similar to those in a POMDP. A = Ai × Aj is the set
of joint actions of all agents. Ωi is the set of agent i’s possible observations. Ti : S × A × S → [0  1]
is the transition function. Oi : S × A × Ωi → [0  1] is the observation function. Ri : ISi × A → R
is the reward function.

2.2

Interactive belief update

Given the deﬁnitions above  the interactive belief update can be performed as follows  by considering
others’ actions and anticipated observations:

(cid:88)
i l(ist) = P r(ist|bt−1
bt
= α

bi l(ist−1)

i l

  at−1

i

  ot
i)
P r(at−1

j

(cid:88)
×(cid:88)

ot
j

ist−1
Oj(st  at−1  ot

at−1
j)τ (bt−1

j

j l−1  at−1

j

  ot

j  bt

j l−1)

|θt−1
j l−1)T (st−1  at−1  st)Oi(st  at−1  ot
i)

(3)

Compared with POMDP  the interactive belief update in I-POMDP takes two additional elements
into account. First  the probability of other’s actions given his models needs to be computed since
the state now depends on both agents’ actions (the second summation). Second  the modeling agent
needs to update his beliefs based on the anticipation of what observations the other agent might get
and how it updates (the third summation).
Similarly to POMDPs  the value associated with a belief state in I-POMDPs can be updated using
value iteration:

(cid:111)
P (oi|ai  bi l)V ((cid:104)SEθi(bi l  ai  oi)  ˆθi(cid:105))

bi l(is)ERi(is  ai) + γ

(cid:88)

oi∈Ωi

(cid:110) (cid:88)
where ERi(is  ai) =(cid:80)

V (θi l) = max
ai∈Ai

is∈IS

Then the optimal action  a∗
optimal actions  OP T (θi)  for the belief state:

Ri(is  ai  aj)P r(aj|θj l−1).
i   for an inﬁnite horizon criterion with discounting  is part of the set of

(4)

(cid:111)

(cid:88)

oi∈Ωi

P (oi|ai  bi l)V ((cid:104)SEθi (bi l  ai  oi)  ˆθi(cid:105))
(5)

OP T (θi l) = arg max

ai∈Ai

bi l(is)ERi(is  ai) + γ

aj

(cid:110) (cid:88)

is∈IS

3 Sampling Algorithms

The Markov Chain Monte Carlo (MCMC) method [6] is widely used to approximate probability
distributions that are difﬁcult to compute directly. Sequential versions of Monte Carlo methods  such
as as particle ﬁlters [1]  work on sequential inference tasks  especially sequential decision making
under Markov assumption. At each time step  a particle ﬁlter draws samples (or particles) from a

3

proposal distribution  commonly the conditional distribution p(xt|xt−1) of the current state xt given
the previous xt−1  then uses the observation function p(yt|xt) to compute importance weights for all
particles and resample them according to the weights.
The Interactive Particle Filter (I-PF) was devised as a ﬁltering algorithm for interactive belief update
in I-POMDP  which generalizes the classic particle ﬁlter algorithm to multi-agent settings [2]. It uses
the state transition function as the proposal distribution  which is usually used in a speciﬁc particle
ﬁlter algorithm called bootstrap ﬁlter [9]. However  due to the enormous belief space  the I-PF
implementation assumes that the other agent’s frame ˆθj is known to the modeling agent  therefore it
simpliﬁes the belief update from S × Θj l−1 to a signiﬁcantly smaller space S × {bj l−1}  where j
represents the other agent and Θj l−1 is j’s model space.
Our interactive belief update described in Algorithm 1 and 2  however  generalizes I-POMDP’s
belief update to larger intentional model space which contains other agents’ beliefs  and transition 
observation and reward functions. In the remaining part of this section  we will ﬁrstly give a brief
introduction of our algorithms and discuss the motivations of each sampling step. Then we will show
the major differences between our algorithm and the I-PF  since this generalization is nontrivial. A
concrete example of the algorithm is given in Figure 1 in the next section as well.

k

  ot

k

k

 at−1−k   ot−k)

=< s(n) t  θ(n) t−k 0 >

Algorithm 1: Interactive Belief Update
k l = InteractiveBeliefUpdate(˜bt−1
k l   at−1
˜bt
k  l > 0)
=< s(n) t−1  θ(n) t−1
1 For is(n) t−1
−k l−1 >∈ ˜bt−1
k l :
sample at−1−k ∼ P (a−k|θ(n) t−1
2
−k l−1 )
sample s(n) t ∼ Tk(st|s(n) t−1  at−1
  at−1−k )
3
for ot−k ∈ Ω−k:
4
5
6
7
8
9
10
11
12
13
14
k
15 normalize all w(n)
n=1 w(n)
t = 1
16 resample {is(n) t
according to normalized {w(n)
t }
17 resample θ(n) t−k l−1 ∼ N (θt−k l−1|θ(n) t−1
=< s(n) t  θ(n) t−k l−1 >
18 return ˜bt

if l = 1:
b(n) t−k 0 = Level0BeliefUpdate(θ(n) t−1
−k 0
is(n) t
k
else:
b(n) t−k l−1 = InteractiveBeliefUpdate(˜b(n) t−1
θ(n) t−k l−1 =< b(n) t−k l−1  ˆθ(n) t−1
=< s(n) t  θ(n) t−k l−1 >
is(n) t
k
t = O(n)−k (ot−k|s(n) t  at−1
w(n)
t × Ok(ot
w(n)
t = w(n)
  w(n)
k l =< is(n) t
˜btemp

k|s(n) t  at−1
t >

so that(cid:80)N

} from ˜btemp

k l = is(n) t

k

−k l−1   Σ)

  at−1−k )

−k l−1 >

k

  at−1−k )

k

t

k

k l

−k l−1   at−1−k   ot−k  l − 1)

k

k

  the observation  ot

  along with the action  at−1

The Algorithm 1 requires inputs of the modeling agent’s prior belief  ˜bt−1
k l   which is represented
as a set of n samples is(n) t−1
k  and the belief
nesting level  l > 0. Here k represents either agent i or j  and −k represents the other agent  j or
i  correspondingly. We assume that the modeled agent’s action set A−k  observation set Ω−k and
optimality criteria OCk are known to all agents. We want to learn the other agent’s initial belief about
the physical state  b0−k  the transition function  T−k  the observation function  O−k and the reward
function  R−k.
The initial belief samples  is(n) t−1
  are generated from the prior nested belief in the similar way
as described in the I-PF literature [2] except that T (n)−k   O(n)−k   and R(n)−k are sampled from their
prior distributions as well. Notice that T (n)−k   O(n)−k   and R(n)−k are all part of the frame  namely
ˆθ(n)−k =< A−k  Ω−k  T (n)−k   O(n)−k   R(n)−k   OCk >  as appeared in line 7 and 11 in Algorithm 1.

k

4

With initial belief samples  the Algorithm 1 starts from propagating each sample forward in time
and computing their weights (line 1-15)  then it resamples according to the weights and similarity
between models (line 16-18). Intuitively  the samples associated with actual observations perceived
by agent k will gradually carry larger weights and be resampled more often  therefore they will
approximately represent the exact belief. Speciﬁcally  for each of is(n) t−1
  the algorithm samples
the other agent’s optimal actions at−1−k given its model from P (A−k|θ(n) t−1
) (line 2)  which equals
1|OP T| if a−k ∈ OP T or 0 otherwise. Then it samples the physical state s(n) t using the state
transition function Tk(St|S(n) t−1  at−1
  at−1−k ) (line 3). Then for each possible observation  if the
current nesting level l is 1  it calls the 0-level belief update  described in Algorithm 2  to update other
agents’ beliefs over physical state bt−k 0 (line 5 to 7); or it recursively calls itself at a lower level l − 1
(line 8 to 11)  if l is greater than 1. The sample weights w(n)
are computed according to observation
likelihoods of modeling and modeled agents (line 12  13). Lastly  the algorithm normalizes the
weights (line 15)  resamples the intermediate particles(line 16) and resamples another time from
similar neighboring models using a Gaussian distribution to avoid divergence (line 17).

k
−k

k

t

  ot
k)

k

k 0

get Tk and Ok from θt−1

Algorithm 2: Level-0 Belief Update
k 0   at−1
k 0 =Level0BeliefUpdate(θt−1
bt
1
2 P (at−1−k ) = 1/|A−k|
3 for st ∈ S:
sum=0
4
for st−1:
5
for at−1−k ∈ A−k:
6
P (st|st−1  at−1
7
sum+ = P (st|st−1  at−1
8
for at−1−k ∈ A−k:
9
k|st  at−1
10
P (ot
k 0 = sum × P (ot
11
bt
12 normalize and return bt

)+ = Ok(ot
k|st  at−1

k

k

k

k 0

)+ = Tk(st|st−1  at−1

k

  at−1−k )P (at−1−k )

k

)bt−1
k 0 (st−1)
k|st  at−1
)

k

  at−1−k )P (at−1−k )

k

k

k

k 0   action  at−1

  at−1−k ) and Ok(ot

k  as input arguments and returns the belief about the physical state  bt

The 0-level belief update  described in Algorithm 2  takes agent model  θt−1
  and
observation  ot
k 0. The
other agent’s actions are treated as noise (line 2)  and transition and observation functions are
k 0 . For each possible action at−1−k   it computes the
passed in within the ﬁrst input argument θt−1
actual state transition (line 7) and observation function (line 10) by marginalizing over others’
k 0. Notice that the transition and observation functions 
actions  and returns the normalized belief bt
Tk(st|st−1  at−1
k|st  at−1
  at−1−k ) contained in θt−1
  depend on particular model
parameters of the actual agent on the 0th level.
Our interactive belief update algorithm differs in three major ways from the I-PF. First  in order
to update the belief over this intentional model space of other agents  their initial belief  transition
function  observation function and reward function in their frames are all unknown and become
samples. For instance  the set of n samples of other agents’ intentional models θ(n) t−1
−k l−1 =<
b(n) t−1
−k l−1   A−k  Ω−k  T (n)−k   O(n)−k   R(n)−k   OCk >. The observation function of the modeled agents 
O(n)−k (ot−k|s(n) t  at−1
  at−1−k ) in line 12 of Algorithm 1  is now randomized consequently. Second  the
transition and observation functions of the level-0 agent  in line 7 and 10 of Algorithm 2  are passed
in as input arguments which correspond to each model sample. Lastly  we add another resampling
step in line 17 of Algorithm to avoid divergence  by resampling the model samples from a Gaussian
distribution with the mean of current sample value. This additional resampling step is nontrivial 
since empirically the samples diverge quickly due to the enormously enlarged sample space.

k

k

5

4 Experiments

We evaluate our algorithm on the multi-agent tiger problem [7] and UAV reconnaissance problem [2].
The multi-agent tiger game is a generalization of the classic single agent tiger game [11]. It contains
additional observations caused by others’ actions  and the transition and reward functions involve
others’ actions as well. The UAV reconnaissance problem contains a 3x3 grid in which the agent
(UAV) tries to capture the moving target [2].

4.1 Parameterization

The initial step of solving an I-POMDP in our approach is to parameterize other agents’ models in
terms of an I-POMDP or POMDP  depending on the modeling agent’s strategy level. Then  the model
parameters can be sampled and updated using the interactive belief update algorithm for solving the
planning task.
Here we give an example of parameterization using the tiger problem. The UAV problem has a
similar process accordingly. For the simplicity of presentation  assume there are two agent i and
j in the game and the strategy level is 1 (but we experiment with higher strategy levels in later
sections)  then for the two-agent tiger problem: ISi 1 = S × θj 0  where S = {tiger on the left (TL) 
tiger on the right (TR)} and θj 0 =< bj(s)  Aj  Ωj  Tj  Oj  Rj  OCj >}; A = Ai × Aj are joint
actions of listen (L)  open left door (OL) and open right door(OR); Ωi: {growl from left (GL) or right
(GR)} × {creak from left (CL)  right (CR) or silence (S)}; Ti = Tj : S × Ai × Aj × S → [0  1];
Oi : S × Ai × Aj × Ωi → [0  1]; Ri : IS × Ai × Aj → R.
As mentioned before we assume that Aj and Ωj are known  and OCj is inﬁnite horizon with
discounting. We want to recover the possible initial belief b0
j about the physical state  the transition 
Tj  the observation  Oj and the reward  Rj. Thus the main idea of our experiment is to do Bayesian
parametric learning with the help of our sampling algorithm.

Table 1: Parameters for transition  observation and reward functions

A
S
L
TL
TR L
*
*

p(TL)
pT 1
1 − pT 1
OL
pT 2
OR 1 − pT 2

p(TR)
1 − pT 1
pT 1
1 − pT 2
pT 2

A
S
L
TL
TR L
*
*

p(GL)
pO1
1 − pO1
OL
pO2
OR 1 − pO2

p(GR)
1 − pO1
pO1
1 − pO2
pO2

A
L

S
R
*
rR1
TL OL
rR2
TR OR rR2
TL OR rR3
TR OL
rR3

j × pT 1 × pT 2 × pO1 × pO2 ×
We see in Table 1 that it is a large 8-dimensional space to learn from: b0
rR1× rR2× rR3  where {bj  pT 1  pT 2  pO1  pO2} ∈ [0  1]5 ⊂ R and {rR1  rR2  rR3} ∈ [−∞  +∞]5.
Figure 1 illustrates the interactive belief update in the game described above  assuming the sample
size is 8. The subscripts denote the corresponding agents and each dot represents a particular belief
sample. The propagation step in implemented in lines 2 to 11 in Algorithm 1  the weighting step is in
lines 12 to 15  and the resampling step is in lines 16 and 17. The belief update for a particular level-0
model sample  θj = (cid:104)0.5  0.67  0.5  0.85  0.5  -1  -100  10(cid:105)  is solved using Algorithm 2.

4.2 Results

We ﬁrstly ﬁx the modeled agent j to be a level-2 I-POMDP agent and experiment with different
modeling approaches for agent i in order to compare the performance in terms of average reward. We
compare level-3  level-2  level-1 intentional I-POMDP models with a subintentional model  in which
agent j is assumed to choose his action according to a ﬁxed but unknown distribution and therefore is
called a frequency-based (ﬁctitious play) model [5].
In Figure 2  we see that the intentional I-POMDP approaches has signiﬁcantly higher reward as agent
i perceives more observations  and level-2 I-POMDP performs slightly better than level-1 while
level-3 has high variance but at least competes with level-2. The subintentional approach has certain
learning ability but is not sophisticated enough to model a rational (level-2 intentional I-POMDP)
agent  therefore its performance is worse than all I-POMDP models.

6

Figure 1: An illustration of interactive belief update algorithm using tiger problem for two-agent and
level-1 nesting.

Figure 2: Performance comparison in terms of average reward per time step versus observation length.
The plot is averaged on 5 runs and uses 2000 and 1000 samples for tiger and UAV respectively. The
vertical bars stand for standard deviations.

Figure 3: Learning quality  measured by KL-divergence  improves as the number of particles increases.
It measures the difference between the ground truth of the model parameters and the learned posterior
distributions. The vertical bars are the standard deviations.

In Figure 3 we show that the learning quality  in terms of the sum of independent KL-divergence of
each model parameter dimension  becomes better as the number of particles increases. It measures the

7

difference between the ground truth of the model parameters and the learned posterior distributions
by giving the relative entropy of the truth with respect to the posteriors.

Figure 4: Agent i learns agent j’s most likely nesting level. Samples representing j’s models of
different nesting levels evolve as agent i perceives more observations. Totally 1000 samples are used
and experiments start from equal number of level-1  level-0 and frequency-based samples.

Then we ﬁx the modeling agent i’s strategy level to be 2 and try to observe the changes of j’s samples
which represent different possible models or strategy levels. Speciﬁcally  we start from equal number
of samples that representing j as level-1 I-POMDP  level-0 POMDP  and frequency based agents  and
then gradually learn that the majority of samples converge or become close to the ground truth: j is a
level-1 I-POMDP.

Table 2: Running time for tiger and UAV problems using various number of samples

Belief Level

1

2

N=500
1.96s±0.43s
5m27.23s
±5.19s

N=1000
3.68s±1.01s
16m36.07s
±10.84s

Tiger

N=2000
35.2s±2.82s
49m36.07s
(single run)

Belief Level

1

2

N=100
4.86s±1.34ss
2m43.1s
±3.98s

UAV

N=500
12.31s±1.39s
9m53.7s
±6.48s

N=1000
2m1.43s
±3.29s
36m19.5s
±18.63s

Lastly  we report the running time of our sampling algorithm in Table 2. The computing machine has
an Intel Core i5 2GHz  8GB RAM  and runs macOS 10.13 and MATLAB R2017.

5 Conclusions and Future Work

We have described a novel approach to learn other agents’ intentional models by making the interactive
belief update using Bayesian inference and Monte Carlo sampling methods. We show the correctness
of our theoretical framework using the multi-agent tiger and UAV problems in which it accurately
learns others’ models over the entire intentional model space and can be generalized to problems
of larger scale in a straightforward manner. Therefore  it provides a generalized Bayesian learning
algorithm for multi-agent sequential decision making problems.
For future research opportunities  the applications presented in this paper can be extended to more
complex problems by leveraging emerging deep reinforcement learning (DRL) methods  which
already solves POMDPs in an neural analogy [12]. DRL should also be capable of approximating
key functions in I-POMDPs  thus has the potential to serve as an efﬁcient computational tool for
I-POMDPs.

8

References
[1] Pierre Del Moral. Non-linear ﬁltering: interacting particle resolution. Markov processes and related ﬁelds 

2(4):555–581  1996.

[2] Prashant Doshi and Piotr J Gmytrasiewicz. Monte Carlo sampling methods for approximating interactive

POMDPs. Journal of Artiﬁcial Intelligence Research  34:297–337  2009.

[3] Finale Doshi-Velez  David Pfau  Frank Wood  and Nicholas Roy. Bayesian nonparametric methods
for partially-observable reinforcement learning. IEEE transactions on pattern analysis and machine
intelligence  37(2):394–407  2015.

[4] Arnaud Doucet  Nando De Freitas  and Neil Gordon. An introduction to sequential monte carlo methods.

In Sequential Monte Carlo methods in practice  pages 3–14. Springer  2001.

[5] Drew Fudenberg and David K Levine. The theory of learning in games  volume 2. MIT press  1998.

[6] Walter R Gilks  Sylvia Richardson  and David J Spiegelhalter. Introducing Markov chain Monte Carlo.

Markov chain Monte Carlo in practice  1:19  1996.

[7] Piotr J Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multi-agent settings. J.

Artif. Intell. Res.(JAIR)  24:49–79  2005.

[8] Piotr J Gmytrasiewicz and Edmund H Durfee. Rational coordination in multi-agent environments. Au-

tonomous Agents and Multi-Agent Systems  3(4):319–350  2000.

[9] Neil J Gordon  David J Salmond  and Adrian FM Smith. Novel approach to nonlinear/non-Gaussian
Bayesian state estimation. In IEE Proceedings F (Radar and Signal Processing)  volume 140  pages
107–113. IET  1993.

[10] John C Harsanyi. Games with incomplete information played by “Bayesian” players  i–iii part i. the basic

model. Management science  14(3):159–182  1967.

[11] Leslie Pack Kaelbling  Michael L Littman  and Anthony R Cassandra. Planning and acting in partially

observable stochastic domains. Artiﬁcial intelligence  101(1):99–134  1998.

[12] Peter Karkus  David Hsu  and Wee Sun Lee. Qmdp-net: Deep learning for planning under partial

observability. In Advances in Neural Information Processing Systems  pages 4697–4707  2017.

[13] Miao Liu  Xuejun Liao  and Lawrence Carin. The inﬁnite regionalized policy representation. In Proceedings

of the 28th International Conference on Machine Learning (ICML-11)  pages 769–776  2011.

[14] Andrew Kachites McCallum and Dana Ballard. Reinforcement learning with selective perception and

hidden state. PhD thesis  University of Rochester. Dept. of Computer Science  1996.

[15] Alessandro Panella and Piotr J Gmytrasiewicz. Bayesian learning of other agents’ ﬁnite controllers for
interactive POMDPs. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence  pages
2530–2536  2016.

[16] Christos H Papadimitriou and John N Tsitsiklis. The complexity of Markov decision processes. Mathemat-

ics of operations research  12(3):441–450  1987.

9

,Yanlin Han
Piotr Gmytrasiewicz
Xiao Sun
Jungwook Choi
Chia-Yu Chen
Naigang Wang
Swagath Venkataramani
Vijayalakshmi (Viji) Srinivasan
Xiaodong Cui
Wei Zhang
Kailash Gopalakrishnan