2017,Gauging Variational Inference,Computing partition function is the most important statistical inference task arising in applications of Graphical Models (GM). Since it is computationally intractable  approximate  methods have been used in practice  where mean-field  (MF) and belief propagation (BP) are arguably the most popular and successful approaches of a variational type. In this paper  we propose two new variational schemes  coined Gauged-MF (G-MF) and Gauged-BP (G-BP)  improving MF and BP  respectively. Both provide lower bounds for the partition function by utilizing the so-called gauge transformation which modifies factors of GM while keeping the partition function invariant. Moreover  we prove that both G-MF and G-BP are exact for GMs with a single loop of a special structure  even though the bare MF and BP perform badly in this case. Our extensive experiments indeed confirm that the proposed algorithms outperform and generalize MF and BP.,Gauging Variational Inference

Sungsoo Ahn∗ Michael Chertkov†

∗School of Electrical Engineering 

Jinwoo Shin∗

Korea Advanced Institute of Science and Technology  Daejeon  Korea

†1 Theoretical Division  T-4 & Center for Nonlinear Studies 

Los Alamos National Laboratory  Los Alamos  NM 87545  USA 

†2Skolkovo Institute of Science and Technology  143026 Moscow  Russia
∗{sungsoo.ahn  jinwoos}@kaist.ac.kr
†chertkov@lanl.gov

Abstract

Computing partition function is the most important statistical inference task arising
in applications of Graphical Models (GM). Since it is computationally intractable 
approximate methods have been used in practice  where mean-ﬁeld (MF) and belief
propagation (BP) are arguably the most popular and successful approaches of a
variational type. In this paper  we propose two new variational schemes  coined
Gauged-MF (G-MF) and Gauged-BP (G-BP)  improving MF and BP  respectively.
Both provide lower bounds for the partition function by utilizing the so-called
gauge transformation which modiﬁes factors of GM while keeping the partition
function invariant. Moreover  we prove that both G-MF and G-BP are exact for
GMs with a single loop of a special structure  even though the bare MF and BP
perform badly in this case. Our extensive experiments indeed conﬁrm that the
proposed algorithms outperform and generalize MF and BP.

1

Introduction

Graphical Models (GM) express factorization of the joint multivariate probability distributions in
statistics via a graph of relations between variables. The concept of GM has been developed and/or
used successfully in information theory [1  2]  physics [3  4  5  6  7]  artiﬁcial intelligence [8]  and
machine learning [9  10]. Of many inference problems one can formulate using a GM  computing the
partition function (normalization)  or equivalently computing marginal probability distributions  is
the most important and universal inference task of interest. However  this paradigmatic problem is
known to be computationally intractable in general  i.e.  it is #P-hard even to approximate [11].
The Markov chain monte carlo (MCMC) [12] is a classical approach addressing the inference task 
but it typically suffers from exponentially slow mixing or large variance. Variational inference is
an approach stating the inference task as an optimization. Hence  it does not have such issues of
MCMC and is often more favorable. The mean-ﬁeld (MF) [6] and belief propagation (BP) [13] are
arguably the most popular algorithms of the variational type. They are distributed  fast and overall
very successful in practical applications even though they are heuristics lacking systematic error
control. This has motivated researchers to seek for methods with some guarantees  e.g.  providing
lower bounds [14  15] and upper bounds [16  17  15] for the partition function of GM.
In another line of research  which this paper extends and contributes  the so-called re-parametrizations
[18]  gauge transformations (GT) [19  20] and holographic transformations [21  22] were explored.
This class of distinct  but related  transformations consist in modifying a GM by changing fac-
tors  associated with elements of the graph  continuously such that the partition function stays the
same/invariant.1 In this paper  we choose to work with GT as the most general one among the three

1See [23  24  25] for discussions of relations between the aforementioned techniques.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

approaches. Once applied to a GM  it transforms the original partition function  deﬁned as a weighted
series/sum over states  to a new one  dependent on the choice of gauges. In particular  a ﬁxed point
of BP minimizes the so-called Bethe free energy [26]  and it can also be understood as an optimal
GT [19  20  27  28]. Moreover  ﬁxing GT in accordance with BP results in the so-called loop series
expression for the partition function [19  20]. In this paper we generalize [19  20] and explore a
more general class of GT: we develop a new gauge-optimization approach which results in ‘better’
variational inference schemes than MF  BP and other related methods.
Contribution. The main contribution of this paper consists in developing two novel variational
methods  called Gauged-MF (G-MF) and Gauged-BP (G-BP)  providing lower bounds on the
partition function of GM. While MF minimizes the (exact) Gibbs free energy under (reduced) product
distributions  G-MF does the same task by introducing an additional GT. Due to the the additional
degree of freedom in optimization  G-MF improves the lower bound of the partition function provided
by MF systematically. Similarly  G-BP generalizes BP  extending interpretation of the latter as an
optimization of the Bethe free energy over GT [19  20  27  28]  by imposing additional constraints
on GT  and thus forcing all the terms in the resulting series for the partition function to remain
non-negative. Consequently  G-BP results in a provable lower bound for the partition function  while
BP does not (except for log-supermodular models [29]).
We prove that both G-MF and G-BP are exact for GMs deﬁned over single cycle  which we call
‘alternating cycle/loop’  as well as over line graph. The alternative cycle case is surprising as it
represents the simplest ‘counter-example’ from [30]  illustrating failures of MF and BP. For general
GMs  we also establish that G-MF is better than  or at least as good as  G-BP. However  we also
develop novel error correction schemes for G-BP such that the lower bound of the partition function
provided by G-BP is improved systematically/sequentially  eventually outperforming G-MF on the
expense of increasing computational complexity. Such error correction scheme has been studied for
improving BP by accounting for the loop series consisting of positive and negative terms [31  32].
According to to our design of G-BP  the corresponding series consists of only non-negative terms 
which leads to easier systematic corrections to G-BP.
We also show that the proposed GT-based optimizations can be restated as smooth and unconstrained 
thus allowing efﬁcient solutions via algorithms of a gradient descent type or any generic optimization
solver  such as IPOPT [33]. We experiment with IPOPT on complete GMs of relatively small size and
on large GM (up-to 300 variables) of ﬁxed degree. Our experiments indeed conﬁrm that the newly
proposed algorithms outperform and generalize MF and BP. Finally  we remark that all statements of
the paper are made within the framework of the so-called Forney-style GMs [34] which is general
as it allows interactions beyond pair-wise (i.e.  high-order GM) and includes other/alternative GM
formulations  based on factor graphs [35].

2 Preliminaries

2.1 Graphical model
Factor-graph model. Given (undirected) bipartite factor graph G = (X  F E)  a joint distribution
of (binary) random variables x = [xv ∈ {0  1} : v ∈ X ] is called a factor-graph Graphical Model
(GM) if it factorizes as follows:

p(x) =

fa(x∂a) 

neighboring factor a  and the normalization constant Z :=(cid:80)

where fa are some non-negative functions called factor functions  ∂a ⊆ X consists of nodes
a∈F fa(x∂a)  is called the
partition function. A factor-graph GM is called pair-wise if |∂a| ≤ 2 for all a ∈ F  and high-order
otherwise. It is known that approximating the partition function is #P-hard in general [11].
Forney-style model. In this paper  we primarily use the Forney-style GM [34] instead of factor-graph
GM. Elementary random variables in the Forney-style GM are associated with edges of an undirected
graph  G = (V E). Then the random vector  x = [xab ∈ {0  1} : {a  b} ∈ E] is realized with the
probability distribution

x∈{0 1}X(cid:81)

p(x) =

1
Z

fa(xa) 

(1)

(cid:89)

a∈F

1
Z

(cid:89)

a∈V

2

Z := (cid:80)

x∈{0 1}E(cid:81)

where xa is associated with set of edges neighboring node a  i.e. xa = [xab

: b ∈ ∂a] and
a∈V fa(xa). As argued in [19  20]  the Forney-style GM constitutes a more
universal/compact description of gauge transformations without any restriction of generality: given
any factor-graph GM  one can construct an equivalent Forney-style (see the supplementary material).

2.2 Mean-ﬁeld and belief propagation

We now introduce two most popular methods for approximating the partition function: the mean-ﬁeld
and Bethe (i.e.  belief propagation) approximation methods. Given any (Forney-style) GM p(x)
deﬁned as in (1) and any distribution q(x) over all variables  the Gibbs free energy is deﬁned as

(cid:88)

x∈{0 1}E

(cid:81)

q(x)

a∈V fa(xa)

FGibbs(q) :=

q(x) log

.

(2)

The partition function is related to the Gibbs free energy according to − log Z = minq FGibbs(q) 
where the optimum is achieved at q = p [35]. This optimization is over all valid probability
distributions from the exponentially large space  and obviously intractable.
In the case of the mean-ﬁeld (MF) approximation  we minimize the Gibbs free energy over a family of

tractable probability distributions factorized into the following product: q(x) =(cid:81){a b}∈E qab(xab) 

where each independent qab(xab) is a proper probability distribution  behaving as a (mean-ﬁeld)
proxy to the marginal of q(x) over xab. By construction  the MF approximation provides a lower
bound for log Z. In the case of the Bethe approximation  the so-called Bethe free energy approximates
the Gibbs free energy [36]:

FBethe(b) =

a∈V

xa∈{0 1}∂a

ba(xa) log

ba(xa)
fa(xa)

{a b}∈E

xab∈{0 1}

bab(xab) log bab(xab) 

(3)

where beliefs b = [ba  bab : a ∈ V {a  b} ∈ E] should satisfy following ‘consistency’ constraints:

0 ≤ ba  bab ≤ 1 

ba(xab) = 1 

xab∈{0 1}

a\xab∈{0 1}∂a
x(cid:48)

b(x(cid:48)

a) = b(xab) ∀{a  b} ∈ E.

a\xab denotes a vector with x(cid:48)

Here  x(cid:48)
ab = xab ﬁxed and minb FBethe(b) is the Bethe estimation for
− log Z. The popular belief propagation (BP) distributed heuristics solves the optimization iteratively
[36]. The Bethe approximation is exact over trees  i.e.  − log Z = minb FBethe(b). However  in
the case of a general loopy graph  the BP estimation lacks approximation guarantees. It is known 
however  that the result of BP-optimization lower bounds the log-partition function  log Z  if the
factors are log-supermodular [29].

2.3 Gauge transformation

(cid:88)

− (cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

Gauge transformation (GT) [19  20] is a family of linear transformations of the factor functions in (1)
which leaves the the partition function Z invariant. It is deﬁned with respect to the following set of
invertible 2 × 2 matrices Gab for {a  b} ∈ E  coined gauges:

(cid:21)

(cid:20) Gab(0  0) Gab(0  1)
(cid:88)
(cid:89)

fa(x(cid:48)
a)

a∈{0 1}∂a
x(cid:48)

b∈∂a

Gab(1  0) Gab(1  1)
The GM  gauge transformed with respect to G = [Gab  Gba
expressed as:

Gab =

.
: {a  b} ∈ E]  consists of factors

fa G(xa) =

Gab(xab  x(cid:48)

ab).

Here one treats independent xab and xba equivalently for notational convenience  and {Gab  Gba}
is a conjugated pair of distinct matrices satisfying the gauge constraint G(cid:62)
abGba = I  where I is the
identity matrix. Then  one can prove invariance of the partition function under the transformation:

(cid:88)

(cid:89)

Z =

(cid:88)

(cid:89)

x∈{0 1}|E|

a∈V

x∈{0 1}|E|

a∈V

fa G(xa).

(4)

fa(xa) =

3

(cid:81)

Consequently  GT results in the gauge transformed distribution pG(x) = 1
Z
that some components of pG(x) can be negative  in which case it is not a valid distribution.
We remark that the Bethe/BP approximation can be interpreted as a speciﬁc choice of GT [19  20].
Indeed any ﬁxed point of BP corresponds to a special set of gauges making an arbitrarily picked
conﬁguration/state x to be least sensitive to the local variation of the gauge. Formally  the following
non-convex optimization is known to be equivalent to the Bethe approximation:

a∈V fa G(xa). Note

log fa G(0  0  . . . )

maximize

G

a∈V
subject to G(cid:62)
abGba = I 

∀ {a  b} ∈ E 

(5)
and the set of BP-gauges correspond to stationary points of (5)  having the objective as the respective

Bethe free energy  i.e. (cid:80)

a∈V log fa G(0  0  . . . ) = −FBethe.

(cid:88)

3 Gauge optimization for approximating partition functions

Now we are ready to describe two novel gauge optimization schemes (different from (5)) providing
guaranteed lower bound approximations for log Z. Our ﬁrst GT scheme  coined Gauged-MF (G-MF) 
shall be considered as modifying and improving the MF approximation  while our second GT scheme 
coined Gauged-BP (G-BP)  modiﬁes and improves the Bethe approximation in a way that it now
provides a provable lower bound for log Z  while the bare BP does not have such guarantees. The
G-BP scheme also allows further improvement (in terms of the output quality) on the expense of
making underlying algorithm/computation more complex.

We ﬁrst propose the following optimization inspired by  and also improving  the MF approximation:

3.1 Gauged mean-ﬁeld

(cid:88)

(cid:88)

maximize

q G

a∈V
xa∈{0 1}∂a
subject to G(cid:62)
abGba = I 
fa G(xa) ≥ 0 
q(x) =

(cid:89)

{a b}∈E

qa(xa) log fa G(xa) − (cid:88)
(cid:89)

∀ {a  b} ∈ E 
∀a ∈ V  ∀xa ∈ {0  1}∂a 
qab(xab) 

qa(xa) =

(cid:88)

{a b}∈E

xab∈{0 1}

qab(xab) 

b∈∂a

qab(xab) log qab(xab)

∀a ∈ V.

(6)

Recall that the MF approximation optimizes the Gibbs free energy with respect to q given the original
GM  i.e. factors. On the other hand  (6) jointly optimizes it over q and G. Since the partition function
of the gauge transformed GM is equal to that of the original GM  (6) also outputs a lower bound on
the (original) partition function  and always outperforms MF due to the additional degree of freedom
in G. The non-negative constraints fa G(xa) ≥ 0 for each factor enforce that the gauge transformed
GM results in a valid probability distribution (all components are non-negative).
To solve (6)  we propose a strategy  alternating between two optimizations  formally stated in
Algorithm 1. The alternation is between updating q  within Step A  and updating G  within Step C.
The optimization in Step A is simple as one can apply any solver of the mean-ﬁeld approximation.
On the other hand  Step C requires a new solver and  at the ﬁrst glance  looks complicated due to
nonlinear constraints. However  the constraints can actually be eliminated. Indeed  one observes
that the non-negative constraint fa G(xa) ≥ 0 is redundant  because each term q(xa) log fa G(xa)
in the optimization objective already prevents factors from getting close to zero  thus keeping
them positive. Equivalently  once current G satisﬁes the non-negative constraints  the objective 
q(xa) log fa G(xa)  acts as a log-barrier forcing the constraints to be satisﬁed at the next step within
an iterative optimization procedure. Furthermore  the gauge constraint  G(cid:62)
abGba = I  can also be
removed simply expressing one (of the two) gauge via another  e.g.  Gba via (G(cid:62)
ab)−1. Then  Step
C can be resolved by any unconstrained iterative optimization method of a gradient descent type.
Next  the additional (intermediate) procedure Step B was considered to handle extreme cases when
for some {a  b}  qab(xab) = 0 at the optimum. We resolve the singularity perturbing the distribution
by setting zero probabilities to a small value  qab(xab) = δ where δ > 0 is sufﬁciently small. In

4

barrier terms δ1 > δ2 > ··· > δT > 0 (to handle extreme cases).

Algorithm 1 Gauged mean-ﬁeld
1: Input: GM deﬁned over graph G = (V E) with factors {fa}a∈V. A sequence of decreasing
2: for t = 1  2 ···   T do
3:

Step A. Update q by solving the mean-ﬁeld approximation  i.e.  solve the following optimiza-
tion:

(cid:88)

qa(xa) log fa G(xa) − (cid:88)
(cid:89)

qab(xab) 

qa(xa) =

(cid:88)
(cid:89)

{a b}∈E

{a b}∈E

xab∈{0 1}

qab(xab) 

b∈∂a

(cid:88)

maximize

q

a∈V

xa∈{0 1}∂a

subject to q(x) =

qab(xab) log qab(xab)

∀a ∈ V.

Step B. For factors with zero values  i.e. qab(xab) = 0  make perturbation by setting

4:

5:

Step C. Update G by solving the following optimization:

qab(x(cid:48)

ab) =

(cid:26)δt
(cid:88)

1 − δt

(cid:88)

if x(cid:48)
otherwise.

ab = xab

(cid:89)

a∈V

maximize

G

a∈V
x∈{0 1}E
subject to G(cid:62)
abGba = I 

q(x) log

fa G(xa)

∀ {a  b} ∈ E.

6: end for
7: Output: Set of gauges G and product distribution q.

summary  it is straightforward to check that the Algorithm 1 converges to a local optimum of (6) 
similar to some other solvers developed for the mean-ﬁeld and Bethe approximations.
We also provide an important class of GMs where the Algorithm 1 provably outperforms both the
MF and BP (Bethe) approximations. Speciﬁcally  we prove that the optimization (6) is exact in the
case when the graph is a line (which is a special case of a tree) and  somewhat surprisingly  a single
loop/cycle with odd number of factors represented by negative deﬁnite matrices. In fact  the latter
case is the so-called ‘alternating cycle’ example which was introduced in [30] as the simplest loopy
example where the MF and BP approximations perform quite badly. Formally  we state the following
theorem whose proof is given in the supplementary material.
Theorem 1. For GM deﬁned on any line graph or alternating cycle  the optimal objective of (6) is
equal to the exact log partition function  i.e.  log Z.

3.2 Gauged belief propagation

We start discussion of the G-BP scheme by noticing that  according to [37]  the G-MF gauge
optimization (6) can be reduced to the BP/Bethe gauge optimization (5) by eliminating the non-
negative constraint fa G(xa) ≥ 0 for each factor and replacing the product distribution q(x) by:

Motivated by this observation  we propose the following G-BP optimization:

(cid:26)1

0

q(x) =

(cid:88)

if x = (0  0 ··· ) 
otherwise.

log fa G(0  0 ··· )

maximize

G

a∈V
subject to G(cid:62)
abGba = I 
fa G(xa) ≥ 0 

∀(a  b) ∈ E 
∀a ∈ V  ∀xa ∈ {0  1}∂a.

5

(7)

(8)

The only difference between (5) and (8) is addition of the non-negative constraints for factors in (8).
Hence  (8) outputs a lower bound on the partition function  while (5) can be larger or smaller then
log Z. It is also easy to verify that (8) (for G-BP) is equivalent to (6) (for G-MF) with q ﬁxed to (7).
Hence  we propose the algorithmic procedure for solving (8)  formally described in Algorithm 2  and
it should be viewed as a modiﬁcation of Algorithm 1 with q replaced by (7) in Step A  also with a
properly chosen log-barrier term in Step C. As we discussed for Algorithm 1  it is straightforward to
verify that Algorithm 2 also converges to a local optimum of (8) and one can replace Gba by (G(cid:62)
ab)−1
for each pair of the conjugated matrices in order to build a convergent gradient descent algorithmic
implementation for the optimization.

Algorithm 2 Gauged belief propagation
1: Input: GM deﬁned over graph G = (V E) with and factors {fa}a∈V. A sequence of decreasing
2: for t = 1  2 ··· do
3:

barrier terms δ1 > δ2 > ··· > δT > 0.

Update G by solving the following optimization:
log fa G(0  0 ··· ) + δt

maximize

(cid:88)

q(x) log

fa G(xa)

(cid:88)

(cid:88)

(cid:89)

a∈V

G

a∈V
subject to G(cid:62)
abGba = I 

a∈V

x∈{0 1}E

∀ {a  b} ∈ E.

4: end for
5: Output: Set of gauges G.

Zi

x∈Xi

(cid:81)

(cid:81)

a∈V fa G(x) and

Xi := {x : xei = 1  xej = 0  xek ∈ {0  1} ∀ j  k  such that 1 ≤ j < i < k ≤ |E|}.

Since ﬁxing q(x) eliminates the degree of freedom in (6)  G-BP should perform worse than G-MF 
i.e.  (8) ≤ (6). However  G-BP is still meaningful due to the following reasons. First  Theorem 1
still holds for (8)  i.e.  the optimal q of (6) is achieved at (7) for any line graph or alternating cycle
(see the proof of the Theorem 1 in the supplementary material). More importantly  G-BP can be
corrected systematically. At a high level  the “error-correction" strategy consists in correcting the
approximation error of (8) sequentially while maintaining the desired lower bounding guarantee.
The key idea here is to decompose the error of (8) into partition functions of multiple GMs  and
then repeatedly lower bound each partition function. Formally  we ﬁx an arbitrary ordering of edges
e1 ··· e|E| and deﬁne the corresponding GM for each ei as follows: p(x) = 1
a∈V fa G(xa) for

x ∈ Xi  where Zi :=(cid:80)
GM. Next  recall that (8) maximizes and outputs a single conﬁguration(cid:81)
|E|(cid:88)

Namely  we consider GMs from sequential conditioning of xe1 ···   xei in the gauge transformed
a fa G(0  0 ··· ). Then 
since Xi
i=1 Xi = {0  1}E\(0  0 ··· )  the error of (8) can be decomposed as follows:

(cid:84)Xj = ∅ and(cid:83)|E|
Z −(cid:89)
(cid:89)
(cid:88)
Now  one can run G-MF  G-BP or any other methods (e.g.  MF) again to obtain a lower bound (cid:98)Zi
of Zi for all i and then output(cid:81)
a∈V fa G(0  0 ··· ) +(cid:80)|E|
i=1 (cid:98)Zi. However  such additional runs of
(cid:81)
a ) for x(i) = [xei = 1  xej = 0  ∀ j (cid:54)= i] from Xi  as a choice of (cid:98)Zi just after solving
|E|(cid:88)
(cid:89)

optimization inevitably increase the overall complexity. Instead  one can also pick a single term

fa G(0  0 ··· ) =

(8) initially  and output

a fa G(x(i)

|E|(cid:88)

i=1

x∈Xi

a∈V

fa G(x) =

Zi 

(9)

i=1

a

x(i) = [xei = 1  xej = 0  ∀ j (cid:54)= i] 

(10)

fa G(0  0 ··· ) +

as a better lower bound for log Z than(cid:81)

a∈V

i=1

fa G(x(i)

a ) 

a∈V fa G(0  0 ··· ). This choice is based on the intuition
that conﬁgurations partially different from (0  0 ··· ) may be signiﬁcant too as they share most of
the same factor values with the zero conﬁguration maximized in (8). In fact  one can even choose
more conﬁgurations (partially different from (0  0 ··· )) by paying more complexity  which is always

6

Figure 1: Averaged log-partition approximation error vs interaction strength β in the case of generic
(non-log-supermodular) GMs on complete graphs of size 4  5 and 6 (left  middle  right)  where the
average is taken over 20 random models.

Figure 2: Averaged log-partition approximation error vs interaction strength β in the case of log-
supermodular GMs on complete graphs of size 4  5 and 6 (left  middle  right)  where the average is
taken over 20 random models.

better as it brings the approximation closer to the true partition function. In our experiments  we
consider additional conﬁgurations {x : [xei = 1  xei(cid:48) = 1  xej = 0  ∀ i  i(cid:48) (cid:54)= j] for i(cid:48) = i ···|E|} 
i.e.  output(cid:89)

|E|(cid:88)

|E|(cid:88)

fa G(0  0 ··· ) +

fa G(x(i i(cid:48))

a

) 

a∈V

i=1

i(cid:48)=i

as a better lower bound of log Z than (10).
4 Experimental results

x(i i(cid:48)) = [xei = 1  xei(cid:48) = 1  xej = 0  ∀ j (cid:54)= i  i(cid:48)] 
(11)

We report results of our experiments with G-MF and G-BP introduced in Section 3. We also
experiment here with improved G-BPs correcting errors by accounting for single (10) and multiple
(11) terms  as well as correcting G-BP by applying it (again) sequentially to each residual partition
function Zi. The error decreases  while the evaluation complexity increases  as we move from G-BP-
single to G-BP-multiple and then to G-BP-sequential. To solve the proposed gauge optimizations 
e.g.  Step C. of Algorithm 1  we use the generic optimization solver IPOPT [33]. Even though the
gauge optimizations can be formulated as unconstrained optimizations  IPOPT runs faster on the
original constrained versions in our experiments.2 However  the unconstrained formulations has a
strong future potential for developing fast gradient descent algorithms. We generate random GMs
with factors dependent on the ‘interaction strength’ parameters {βa}a∈V (akin inverse temperature)
according to:

fa(xa) = exp(−βa|h0(xa) − h1(xa)|) 

where h0 and h1 count numbers of 0 and 1 contributions in xa  respectively. Intuitively  we expect
that as |βa| increases  it becomes more difﬁcult to approximate the partition function. See the
supplementary material for additional information on how we generate the random models.
In the ﬁrst set of experiments  we consider relatively small  complete graphs with two types of fac-
tors: random generic (non-log-supermodular) factors and log-supermodular (positive/ferromagnetic)
factors. Recall that the bare BP also provides a lower bound in the log-supermodular case [29] 
thus making the comparison between each proposed algorithm and BP informative. We use the
log partition approximation error deﬁned as | log Z − log ZLB|/| log Z|  where ZLB is the algorithm

2 The running time of the implemented algorithms are reported in the supplementary material.

7

Figure 3: Averaged ratio of the log partition function compared to MF vs graph size (i.e.  number
of factors) in the case of generic (non-log-supermodular) GMs on 3-regular graphs (left) and grid
graphs (right)  where the average is taken over 20 random models.

Figure 4: Averaged ratio of the log partition function compared to MF vs interaction strength β in
the case of log-supermodular GMs on 3-regular graphs of size 200 (left) and grid graphs of size 100
(right)  where the average is taken over 20 random models.

output (a lower bound of Z)  to quantify the algorithm’s performance. In the ﬁrst set of experiments 
we deal with relatively small graphs and the explicit computation of Z (i.e.  the approximation error)
is feasible. The results for experiments over the small graphs are illustrated in Figure 1 and Figure 2
for the non-log-supermodular and log-supermodular cases  respectively. Figure 1 shows that  as
expected  G-MF always outperforms MF. Moreover  we observe that G-MF typically provides the
tightest low-bound  unless it is outperformed by G-BP-multiple or G-BP-sequential. We remark that
BP is not shown in Figure 1  because in this non-log-supermodular case  it does not provide a lower
bound in general. According to Figure 2  showing the log-supermodular case  both G-MF and G-BP
outperform MF  while G-BP-sequential outperforms all other algorithms. Notice that G-BP performs
rather similar to BP in the log-supermodular case  thus suggesting that the constraints  distinguishing
(8) from (5)  are very mildly violated.
In the second set of experiments  we consider more sparse  larger graphs of two types: 3-regular
and grid graphs with size up to 200 factors/300 variables. As in the ﬁrst set of experiments  the
same non-log-supermodular/log-supermodular factors are considered. Since computing the exact
approximation error is not feasible for the large graphs  we instead measure here the ratio of estimation
by the proposed algorithm to that of MF  i.e.  log(ZLB/ZMF) where ZMF is the output of MF. Note
that a larger value of the ratio indicates better performance. The results are reported in Figure 3 and
Figure 4 for the non-log-supermodular and log-supermodular cases  respectively. In Figure 3  we
observe that G-MF and G-BP-sequential outperform MF signiﬁcantly  e.g.  up-to e14 times better in
3-regular graphs of size 200. We also observe that even the bare G-BP outperforms MF. In Figure 4 
algorithms associated with G-BP outperform G-MF and MF (up to e25 times). This is because the
choice of q(x) for G-BP is favored by log-supermodular models  i.e.  most of conﬁgurations are
concentrated around (0  0 ··· ) similar to the choice (7) of q(x) for G-BP. One observes here (again)
that performance of G-BP in this log-supermodular case is almost on par with BP. This implies that
G-BP generalizes BP well: the former provides a lower bound of Z for any GMs  while the latter
does only for log-supermodular GMs.
5 Conclusion

We explore the freedom in gauge transformations of GM and develop novel variational inference
methods which result in signiﬁcant improvement of the partition function estimation. We note that
the GT methodology  applied here to improve MF and BP  can also be used to improve and extend
utility of other variational methods.

8

Acknowledgments

This work was supported in part by the National Research Council of Science & Technology
(NST) grant by the Korea government (MSIP) (No. CRC-15-05-ETRI)  Institute for Information
& communications Technology Promotion(IITP) grant funded by the Korea government(MSIT)
(No.2017-0-01778  Development of Explainable Human-level Deep Machine Learning Inference
Framework) and ICT R&D program of MSIP/IITP [2016-0-00563  Research on Adaptive Machine
Learning Technology Development for Intelligent Autonomous Digital Companion].

References
[1] Robert Gallager. Low-density parity-check codes. IRE Transactions on information theory 

8(1):21–28  1962.

[2] Frank R. Kschischang and Brendan J. Frey. Iterative decoding of compound codes by proba-
bility propagation in graphical models. IEEE Journal on Selected Areas in Communications 
16(2):219–230  1998.

[3] Hans .A. Bethe. Statistical theory of superlattices. Proceedings of Royal Society of London A 

150:552  1935.

[4] Rudolf E. Peierls. Ising’s model of ferromagnetism. Proceedings of Cambridge Philosophical

Society  32:477–481  1936.

[5] Marc Mézard  Georgio Parisi  and M. A. Virasoro. Spin Glass Theory and Beyond. Singapore:

World Scientiﬁc  1987.

[6] Giorgio Parisi. Statistical ﬁeld theory  1988.

[7] Marc Mezard and Andrea Montanari. Information  Physics  and Computation. Oxford Univer-

sity Press  Inc.  New York  NY  USA  2009.

[8] Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference.

Morgan Kaufmann  2014.

[9] Michael Irwin Jordan. Learning in graphical models  volume 89. Springer Science & Business

Media  1998.

[10] William T Freeman  Egon C Pasztor  and Owen T Carmichael. Learning low-level vision.

International journal of computer vision  40(1):25–47  2000.

[11] Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the ising

model. SIAM Journal on computing  22(5):1087–1116  1993.

[12] Ethem Alpaydin. Introduction to machine learning. MIT press  2014.

[13] Judea Pearl. Reverend Bayes on inference engines: A distributed hierarchical approach.
Cognitive Systems Laboratory  School of Engineering and Applied Science  University of
California  Los Angeles  1982.

[14] Qiang Liu and Alexander T Ihler. Negative tree reweighted belief propagation. arXiv preprint

arXiv:1203.3494  2012.

[15] Stefano Ermon  Ashish Sabharwal  Bart Selman  and Carla P Gomes. Density propagation
and improved bounds on the partition function. In Advances in Neural Information Processing
Systems  pages 2762–2770  2012.

[16] Martin J Wainwright  Tommi S Jaakkola  and Alan S Willsky. A new class of upper bounds on
the log partition function. IEEE Transactions on Information Theory  51(7):2313–2335  2005.

[17] Qiang Liu and Alexander T Ihler. Bounding the partition function using holder’s inequality.
In Proceedings of the 28th International Conference on Machine Learning (ICML-11)  pages
849–856  2011.

9

[18] Martin J. Wainwright  Tommy S. Jaakkola  and Alan S. Willsky. Tree-based reparametrization
Information Theory  IEEE

framework for approximate estimation on graphs with cycles.
Transactions on  49(5):1120–1146  2003.

[19] Michael Chertkov and Vladimir Chernyak. Loop calculus in statistical physics and information

science. Physical Review E  73:065102(R)  2006.

[20] Michael Chertkov and Vladimir Chernyak. Loop series for discrete statistical models on graphs.

Journal of Statistical Mechanics  page P06009  2006.

[21] Leslie G Valiant. Holographic algorithms. SIAM Journal on Computing  37(5):1565–1594 

2008.

[22] Ali Al-Bashabsheh and Yongyi Mao. Normal factor graphs and holographic transformations.

IEEE Transactions on Information Theory  57(2):752–763  2011.

[23] Martin J. Wainwright and Michael E. Jordan. Graphical models  exponential families  and

variational inference. Foundations and Trends in Machine Learning  1(1):1–305  2008.

[24] G David Forney Jr and Pascal O Vontobel. Partition functions of normal factor graphs. arXiv

preprint arXiv:1102.0316  2011.

[25] Michael Chertkov. Lecture notes on “statistical inference in structured graphical models: Gauge

transformations  belief propagation & beyond"  2016.

[26] J. S. Yedidia  W. T. Freeman  and Y. Weiss. Constructing free-energy approximations and gen-
eralized belief propagation algorithms. Information Theory  IEEE Transactions on  51(7):2282–
2312  2005.

[27] Vladimir Y Chernyak and Michael Chertkov. Loop calculus and belief propagation for q-ary
alphabet: Loop tower. In Information Theory  2007. ISIT 2007. IEEE International Symposium
on  pages 316–320. IEEE  2007.

[28] Ryuhei Mori. Holographic transformation  belief propagation and loop calculus for generalized
probabilistic theories. In Information Theory (ISIT)  2015 IEEE International Symposium on 
pages 1099–1103. IEEE  2015.

[29] Nicholas Ruozzi. The bethe partition function of log-supermodular graphical models.

Advances in Neural Information Processing Systems  pages 117–125  2012.

In

[30] Adrian Weller  Kui Tang  Tony Jebara  and David Sontag. Understanding the bethe approxima-

tion: when and how can it go wrong? In UAI  pages 868–877  2014.

[31] Michael Chertkov  Vladimir Y Chernyak  and Razvan Teodorescu. Belief propagation and
loop series on planar graphs. Journal of Statistical Mechanics: Theory and Experiment 
2008(05):P05003  2008.

[32] Sung-Soo Ahn  Michael Chertkov  and Jinwoo Shin. Synthesis of mcmc and belief propagation.

In Advances in Neural Information Processing Systems  pages 1453–1461  2016.

[33] Andreas Wächter and Lorenz T Biegler. On the implementation of an interior-point ﬁlter
line-search algorithm for large-scale nonlinear programming. Mathematical programming 
106(1):25–57  2006.

[34] G David Forney. Codes on graphs: Normal realizations. IEEE Transactions on Information

Theory  47(2):520–548  2001.

[35] Martin Wainwright and Michael Jordan. Graphical models  exponential families  and variational

inference. Technical Report 649  UC Berkeley  Department of Statistics  2003.

[36] Jonathan S Yedidia  William T Freeman  and Yair Weiss. Bethe free energy  kikuchi approxima-
tions  and belief propagation algorithms. Advances in neural information processing systems 
13  2001.

[37] Michael Chertkov and Vladimir Y Chernyak. Loop series for discrete statistical models on

graphs. Journal of Statistical Mechanics: Theory and Experiment  2006(06):P06009  2006.

10

,Wouter Koolen
Alan Malek
Peter Bartlett
Sung-Soo Ahn
Michael Chertkov
Jinwoo Shin