2019,Latent distance estimation for random geometric graphs,Random geometric graphs are a popular choice for a latent points generative model for networks. Their definition is based on a sample of $n$ points $X_1 X_2 \cdots X_n$ on the Euclidean sphere~$\mathbb{S}^{d-1}$ which represents the latent positions of nodes of the network. The connection probabilities between the nodes are determined by an unknown function (referred to as the ``link'' function) evaluated at the distance between the latent points. We introduce a spectral estimator of the pairwise distance between latent points and we prove that its rate of convergence is the same as the nonparametric estimation of a function on $\mathbb{S}^{d-1}$  up to a logarithmic factor. In addition  we provide an efficient spectral algorithm to compute this estimator without any knowledge on the nonparametric link function. As a byproduct  our method can also consistently estimate the dimension $d$ of the latent space.,Latent Distance Estimation for Random Geometric

Graphs

Ernesto Araya

Laboratoire de Mathématiques d’Orsay (LMO)

Université Paris-Sud

91405 Orsay Cedex  France

ernesto.araya-valdivia@u-psud.fr

Yohann De Castro

Institut Camille Jordan
École Centrale de Lyon
69134 Écully  France

yohann.de-castro@ec-lyon.fr

Abstract

Random geometric graphs are a popular choice for a latent points generative model
for networks. Their deﬁnition is based on a sample of n points X1  X2 ···   Xn
on the Euclidean sphere Sd−1 which represents the latent positions of nodes of
the network. The connection probabilities between the nodes are determined by
an unknown function (referred to as the “link” function) evaluated at the distance
between the latent points. We introduce a spectral estimator of the pairwise distance
between latent points and we prove that its rate of convergence is the same as the
nonparametric estimation of a function on Sd−1  up to a logarithmic factor. In
addition  we provide an efﬁcient spectral algorithm to compute this estimator
without any knowledge on the nonparametric link function. As a byproduct  our
method can also consistently estimate the dimension d of the latent space.

1

Introduction

Random geometric graph (RGG) models have received attention lately as alternative to some simpler
yet unrealistic models as the ubiquitous Erdös-Rényi model [12]. They are generative latent point
models for graphs  where it is assumed that each node has associated a latent point in a metric
space (usually the Euclidean unit sphere or the unit cube in Rd) and the connection probability
between two nodes depends on the position of their associated latent points. In many cases  the
connection probability depends only on the distance between the latent points and it is determined by
a one-dimensional “link” function.
Because of its geometric structure  this model is appealing for applications in wireless networks
modeling [18]  social networks [17] and biological networks [15]  to name a few. In many of these
real-world networks  the probability that a tie exists between two agents (nodes) depends on the
similarity of their proﬁles. In other words  the connection probability depends on some notion of
distance between the position of the agents in a metric space  which in the social network literature
has been called the social space.
In the classical RGG model  as introduced by Gilbert in [13]  we consider n independent and
identically distributed latent points {Xi}n
i=1 in Rd and the construct the graph with vertex set
V = {1  2 ···   n}  where the node i and j are connected if and only if the Euclidean distance
(cid:107)Xi − Xj(cid:107)d is smaller than a certain predeﬁned threshold τ. The seminal reference on the classical
RGG model  from the probabilistic point of view  is the monograph [27]. Another good reference is
the survey paper [30]. In such a case  the “link” function  which we have not yet formally deﬁned  is
the threshold function 1t≤τ (t). Otherwise stated  two points are connected only if their distance is
smaller than τ. In that case  all the randomness lies in the fact that we are sampling the latent points
with a certain distribution. We choose to maintain the name of random geometric graphs for more
general “link” functions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The angular version of the RGG model has also received attention. On that model  the latent points
are uniformly distributed on Sd−1 (the unit sphere on Rd)  and two points are connected if their
angle is bellow a certain threshold. This model has been used in the context of sensor and wireless
networks [14]. In [9] the authors show that in when the size of the graph n is ﬁxed and the dimension d
goes to inﬁnity  the RGG model on the sphere is indistinguishable from the Erdös-Renyi model  in
the sense that the total variation distance between both graph distributions converges to zero. On
the other hand  in [5] the authors prove that in the dense case if d satisfy a bound with respect to n
(speciﬁcally  if d/n3 → 0) then we can distinguish between both models  by a counting the number
of triangles. The angular RGG model has also been used in the context of approximate graph coloring
[19].
We are interested in the problem of recovering the pairwise distances between the latent
points {Xi}n
i=1 for geometric graphs on Sd−1 given a single observation of the network. We
limit ourselves to the case when the network is a simple graph. Furthermore  we will assume that
the dimension d is ﬁxed and that the “link" function is not known. This problem and some of its
variants have been studied for different versions of the model and under a different set of hypothesis 
see for example the recent work [1] and the references therein. In that work the authors propose a
method for estimating the latent distances based on the graph theoretic distance between two nodes
(that is the length of the shortest path between the nodes). Independently  in [10] the authors develop
a similar method which has slightly less recovery error  but for a less general model. In both cases 
the authors consider the cube in Rd (or the whole Rd) but not the sphere. Our strategy is similar to
the one developed in [28]  where they considered the latent point estimation problem in random dot
product graphs  which is a more restricted model compared to the one considered here. However 
they considered more general Euclidean spaces and latent points distributions other than the uniform.
Similar ideas has been used in the context of vertex classiﬁcation for latent position graphs [29].
We will use the notion of graphon function to formalize the concept of “link” function. Graphons are
central objects to the theory of dense graph limits. They were introduced by Lovász and Szegedy
in [25] and further developed in a series of papers  see [3] [4]. Formally  they are symmetric kernels
that take values in [0  1]  thus they will act as the “link” function for the latent points. The spectrum
of the graphon is deﬁned as the spectrum of an associated integral operator  as in [24  Chap.7]. In this
paper  they will play the role of limit models for the adjacency matrix of a graph  when the size goes
to inﬁnity. This is justiﬁed in light of the work of Koltchinskii and Giné [22] and Koltchinskii [21].
In particular  the adjacency matrix of the observed graph can be though as a ﬁnite perturbed version
of this operator  combining results from [22] and [2].
We will focus on the case of dense graphs on the sphere Sd−1 where the connection probability
depends only on the angle between two nodes. This allows us to use the harmonic analysis on the
sphere to have a nice characterization of the graphon spectrum  which has a very particular structure.
More speciﬁcally  the following two key elements holds: ﬁrst of all  the basis of eigenfunctions is
ﬁxed (do not depend on the particular graphon considered) and equal to the well-known spherical
harmonic polynomials. Secondly  the multiplicity of each eigenvalue is determined by a sequence of
integers that depends only on the dimension d of the sphere and is given by a known formula and the
associated eigenspaces are composed by spherical harmonics of the same polynomial degree.
The graphon eigenspace composed only with linear eigenfunctions (harmonic polynomials of degree
one) will play an important role in the latent distances matrix recovery as all the information we
need to reconstruct the distances matrix is contained in those eigenfunctions. We will prove that
it is possible to approximately recover this information from the observed adjacency matrix of the
graph under regularity conditions (of the Sobolev type) on the graphon and assuming an eigenvalue
gap condition (similar hypotheses are made in [6] in the context of matrix estimation and in [23] in
the context of manifold learning). We do this by proving that a suitable projection of the adjacency
matrix  onto a space generated by exactly d of its eigenvectors  approximates well the latent distances
matrix considering the mean squared error in the Frobenius norm. We give nonasymptotic bounds
for this quantity obtaining the same rate as the nonparametric rate of estimation of a function on the
sphere Sd−1  see [11  Chp.2] for example. Our approach includes the adaptation of some perturbation
theorems for matrix projections from the orthogonal to a “nearly” orthogonal case  which combined
with concentration inequalities for the spectrum gives a probabilistic ﬁnite sample bound  which is
novel to the best of our knowledge. More speciﬁcally  we prove concentration inequalities for the
sampled eigenfunctions of the integral operator associated to a geometric graphon  which are not
necessarily orthogonal as vectors in Rn. Our method shares some similarities with the celebrated

2

USVT method  introduced by Chatterjee in [6]  but in that case they obtained an estimator of the
probability matrix described in Section 2.2 and not of the population Gram matrix as our method. We
develop an efﬁcient algorithm  which we call Harmonic EigenCluster(HEiC) to reconstruct the latent
positions from the data and illustrate its usefulness with synthetic data.

2 Preliminaries

2.1 Notation
We will consider Rd with the Euclidean norm (cid:107)·(cid:107) and the Euclidean scalar product (cid:104)   (cid:105). We deﬁne the
sphere Sd−1 := {x ∈ Rd : (cid:107)x(cid:107) = 1}. For a set A ⊂ R its diameter diam(A) := supx y∈A |x − y|
and if B ⊂ R the distance between A and B is dist(A  B) := inf x∈A y∈B |x− y|.We will use (cid:107)·(cid:107)F
the Frobenius norm for matrices and (cid:107)·(cid:107)op for the operator norm. The identity matrix in Rd×d will be
Idd. If X is a real valued random variable and α ∈ (0  1)  X ≤α C means that P(X ≤ C) ≥ 1 − α.

2.2 Generative model

We describe the generative model for networks which is a generalization of the classical random
geometric graph model introduced by Gilbert in [13]. We base our deﬁnition on the W -random graph
model described in [24  Sec. 10.1]. The central objects will be graphon functions on the sphere 
which are symmetric measurable functions of the form W : Sd−1 × Sd−1 → [0  1]. Throughout this
paper  we consider the measurable space (Sd−1  σ)  where σ is the uniform measure on the sphere.
On Sd−1 × Sd−1 we consider the product measure σ × σ.
To generate a simple graph from a graphon function  we ﬁrst sample n points {Xi}n
i=1 independently
on the sphere Sd−1  according to the uniform measure σ. These are the so-called latent points.
Secondly  we construct the matrix of distances between these points  called the Gram matrix G∗ (we
will often call it population Gram matrix) deﬁned by

G∗
ij := (cid:104)Xi  Xj(cid:105)

and the so-called probability matrix

Θij = ρnW (Xi  Xj)

which is also a n × n matrix. The function W gives the precise meaning for the “link” function 
because it determines the connection probability between Xi and Xj. The introduction of the scale
parameter 0 < ρn ≤ 1 allow us to control the edge density of the sampled graph given a function
W   see [20] for instance. The case ρn = 1 corresponds to the dense case (the parameter Θij do not
depend on n) and when ρn → 0 the graph will be sparser. Our main results will hold in the regime
ρn = Ω( log n
n )  which we call relatively sparse. Most of the time we will work with the normalized
n Θ. If there exists a function f : [−1  1] → [0  1] such that
version of the probability matrix Tn := 1
W (x  y) = f ((cid:104)x  y(cid:105)) for all x  y ∈ Sd−1 we will say that W is a geometric graphon.
Finally  we deﬁne the random adjacency matrix ˆTn  which is a n × n symmetric random matrix that
has independent entries (except for the symmetry constraint ˆTn = ˆT T
n )  conditional on the probability
matrix  with laws
where B(m) is the Bernoulli distribution with mean parameter m. Since the probability matrix
contains the mean parameters for the Bernoulli distributions that deﬁne the random adjacency matrix
it has been also called the parameter matrix [6]. Observe that the classical RGG model on the sphere
is a particular case of the described W -random graph model when W (x  y) = 1(cid:104)x y(cid:105)≤τ . In that case 
since the entries of the probability matrix only have values in {0  1}  the adjacency matrix and the
probability matrix are equal. Depending on the context  we use ˆTn for the random matrix as described
above or for an instance of this random matrix  that is for the adjacency matrix of the observed graph.
This will be clear from the context.
It is worth noting that graphons can be  without loss of generality  deﬁned in [0  1]2. The previous
afﬁrmation means that for any graphon there exists a graphon in [0  1]2 that generates the same distri-
bution on graphs for any given number of nodes. However  in many cases the [0  1]2 representation

n( ˆTn)ij ∼ B(Θij)

3

can be less revealing than other representations using a different underlying space. This is illustrated
in the case of the preﬁx attachment model in [24  example 11.41].
In the sequel we use the notation λ0  λ1 ···   λn−1 for the eigenvalues of the normalized probability
matrix Tn. Similarly  we denote by ˆλ0  ˆλ1 ···   ˆλn−1 the eigenvalues of the matrix ˆTn. We recall that
ˆTn ) have the same set of eigenvectors. We will denote by vj for
Tn (resp. ˆTn) and 1
ρn
1 ≤ j ≤ n the eigenvector of Tn associated to λj  which is also the eigenvector of 1
Tn associated
ρn
ˆλj of ˆTn.
λj. Similarly  we denote by ˆvj to the eigenvector associated to the eigenvalue ρn
to 1
ρn

Tn (resp. 1
ρn

Our main result is that we can recover the Gram matrix using the eigenvectors of ˆTn as follows
Theorem 1 (Informal statement). There exists a constant c1 > 0 that depends only on the dimension
d such that the following is true. Given a graphon W on the sphere such that W (x  y) = f ((cid:104)x  y(cid:105))
with f : [−1  1] → [0  1] unknown  which satisﬁes an eigenvalue gap condition and has Sobolev
regularity s  there exists a subset of the eigenvectors of ˆTn  such that ˆG := 1
ˆV ˆV T converges to the
population Gram matrix G∗ := 1
2s+d−1 (up to a log factor). This estimate
ˆV ˆV T can be found in linear time given the spectral decomposition of ˆTn.
We will say that a geometric graphon W (x  y) = f ((cid:104)x  y(cid:105)) on Sd−1 has regularity s if f belongs the
weighted Sobolev space Z s
2   as deﬁned in [26].
In order to make the statement of 1 rigorous  we need to precise the eigenvalue gap condition and
deﬁne the graphon eigensystem.

γ([−1  1]) with weight function wγ(t) = (1 − t)γ− 1

n ((cid:104)Xi  Xj(cid:105))i j at rate n

−s

c1

2.3 Geometric graphon eigensystem

Here we gather some asymptotic and concentration properties for the eigenvalues and eigenfunctions
of the matrices ˆTn  Tn and the operator TW   which allows us to recover the Gram matrix from data.
The key fact is that the eigenvalues (resp. eigenvectors) of the matrix 1
Tn converge to
ρn
the eigenvalues (resp. sampled eigenfunctions) of the integral operator TW : L2(Sd−1) → L2(Sd−1)

ˆTn and 1
ρn

(cid:90)

TW g(x) =

g(y)W (x  y)dσ(y)

Sd−1

which is compact [16  Sec.6  example 1] and self-adjoint (which follows directly from the symmetry
of W ). Then by a classic theorem in functional analysis [16  Sec.6  Thm. 1.8] its spectrum is a
discrete set {λ∗
k}k∈N ⊂ R and its only accumulation point is zero. In consequence  we can see the
spectra of ˆTn  Tn and TW (which we denote λ( ˆTn)  λ(Tn) and λ(TW ) resp.) as elements of the space
C0 of inﬁnite sequences that converge to 0 (where we complete the ﬁnite sequences with zeros). It is
worth noting that in the case of geometric graphons with regularity s (in the Sobolev sense deﬁned
above) the rate of convergence of λ(TW ) is determined by the regularity parameter s. We have the
following:

• The spectrum of λ( 1

ρn

follows

Tn) converges to λ(TW ) (almost surely) in the δ2 metric  deﬁned as

(cid:115)(cid:88)

i∈N

δ2(x  y) = inf
p∈P

(xi − yp(i))2

where P is the set of all permutations of the non-negative integers. This is proved in [22].
In [8] they prove the following

(cid:16)

δ2

λ(

1
ρn

Tn)  λ(TW )

(cid:17) ≤α/4 C

(cid:16) log n

(cid:17) s

2s+d−1

n

(1)

• Matrices ˆTn approach to matrix Tn in operator norm as n gets larger. Applying [2  Cor.3.3]

to the centered matrix Y = ˆTn − Tn we get

D∗

0

√

log n
n

(2)

E((cid:107) ˆTn − Tn(cid:107)op) (cid:46) D0
n

+

4

(cid:80)n
j=1 Yij(1− Yij) and

(cid:111)
0 ≤ 1  which implies that

where (cid:46) denotes inequality up to constant factors  D0 = max0≤i≤n
0 = maxij |Yij|. We clearly have that D0 = O(nρn) and D∗
D∗
log n
n

√

E(cid:107) ˆTn − Tn(cid:107)op (cid:46) max

(cid:110) ρn√
(cid:107) ˆTn − Tn(cid:107)op ≤α/4 C max(cid:8) 1√

n

 

√

 

We see that this inequality do not improve if ρn is smaller than in the relatively sparse case 
that is ρn = Ω( log n

n ). We prove that  as a corollary of the results in [2]  we have

(cid:9)

(3)

log n
ρnn

ρnn

1
ρn

An analogous bound can be obtained for the Frobenius norm replacing ˆTn with ˆT usvt
the
USVT estimator deﬁned in [6]. For our main results  Proposition 3 and Theorem 4 the
operator norm bound will sufﬁce.

n

A remarkable fact in the case of geometric graphons on Sd−1 is that the eigenfunctions {φk}k∈N of
the integral operator TW are a ﬁxed set that do not depend on the particular function f considered.
This comes from the fact that TW is a convolution operator on the sphere and its eigenfunctions are
the well-known spherical harmonics of dimension d  which are harmonic polynomials in d variables
deﬁned on Sd−1 corresponding to the eigenfunctions of the Laplace-Beltrami operator on the sphere.
This follows from [7  Thm.1.4.5] and from the Funck-Hecke formula given in [7  Thm.1.2.9]. Let
dk denote the dimension of the k-th spherical harmonic space. It is well-known [7  Cor.1.1.4] that

(cid:1). Another important fact  known as the addition

(cid:1) −(cid:0)k+d−3
d0 = 1  d1 = d and dk = (cid:0)k+d−1
dk(cid:88)

k−2
theorem [7  Lem.1.2.3 and Thm.1.2.6]  is that

k

φj(x)φj(y) = ckGγ

k((cid:104)x  y(cid:105))

i=dk−1

k are the Gegenbauer polynomials of degree k with parameter γ = d−2

where Gγ
The Gegenbauer polynomial of degree one is Gγ
1 ((cid:104)Xi  Xj(cid:105)) = 2γ(cid:104)Xi  Xj(cid:105) for every i and j. In consequence  by the addition theorem
Gγ

and ck = 2k+d−2
.
d−2
1 (t) = 2γt (see [7  Appendix B2])  hence we have

2

1 ((cid:104)Xi  Xj(cid:105)) =
Gγ

1
c1

φk(Xi)φk(Xj)

d(cid:88)

k=1

d(cid:88)

where we recall that d1 = d. This implies the following relation for the Gram matrix  observing that
2γc1 = d

1

1
n

2γc1

T =

G∗ :=

((cid:104)Xi  Xj(cid:105))i j =

j v∗
v∗
√
j=1
where v∗
n and V ∗ is the matrix with columns v∗
j is the Rn vector with i-th coordinate φj(Xi)/
j .
In a similar way  we deﬁne for any matrix U in Rn×d with columns u1  u2 ···   ud  the matrix
GU := 1
d U U T . As part of our main theorem we prove that for n large enough there exists a matrix ˆV
in Rn×d where each column is one of the eigenvector of ˆTn  such that ˆG := G ˆV approximates G∗
well  in the sense that the norm (cid:107) ˆG − G∗(cid:107)F converges to 0 at a rate which is that of the nonparametric
estimation of a function on Sd−1.

V ∗V ∗T

(4)

1
d

j

2.4 Eigenvalue gap condition

1  v∗

2 ···   v∗
Informally  we assume that the eigenvalue λ∗

In this section we describe one of our main hypotheses on W needed to ensure that the space
d} can be effectively recovered with the vectors ˆv1  ˆv2 ···   ˆvd using our al-
span{v∗
gorithm.
1 is sufﬁciently isolated from the rest
of the spectrum of TW (not counting multiplicity). We assume without loss of generality that
λ∗
1 = λ∗
. Given a geometric graphon W   we deﬁne the spectral gap of W relative to
the eigenvalue λ∗

2 = ··· = λ∗
d1
1 by

j /∈{1 ···  d1}
which quantiﬁes the distance between the eigenvalue λ∗
we have the following elementary proposition.

Gap1(W ) := min

1 − λ∗
|λ∗
j|
1 and the rest of the spectrum. In particular 

5

Proposition 2. It holds that Gap1(W ) = 0 if and only if there exists j /∈ {1 ···   d1} such that
λ∗
j = λ∗

1 or λ∗

1 = 0.

Proof. Observe that the unique accumulation point of the spectrum of TW is zero. The proposition
follows from this observation.
To recover the population Gram matrix G∗ with our Gram matrix estimator ˆG we require the spectral
gap ∆∗ := Gap1(W ) to be different from 0. This assumption have been made before in the
literature  in results that are based in some version of the Davis-Kahan sin θ theorem (see for
instance [6]  [23]  [29]). More precisely  our results will hold on the following event

(cid:110)

(cid:16)

λ(cid:0) 1

E :=

δ2

(cid:1)  λ(TW )

(cid:17) ∨ 2 9
√
ρn∆∗ (cid:107)Tn − ˆTn(cid:107)op ≤ ∆∗

d

4

2

(cid:111)

 

Tn

ρn

for which we prove the following: given an arbitrary α we have that

for n large enough (depending on W and α). This dependence can be made explicit using (1) and (3)

} ≤

log n
n

∆∗2
√
215/2C

d

and

log n

n

≤(cid:0) ∆∗

8C(cid:48)

(cid:1) 2s+d−1

s

P(E) ≥ 1 − α
2

(cid:114) ρn

 

n

√

max{

where C  C(cid:48) > 0. The following theorems are the main results of this paper. Their proofs can be
found in the supplementary material.
Proposition 3. On the event E  there exists one and only one set Λ1  consisting of d eigenvalues of
ˆTn  whose diameter is smaller than ρn∆∗/2 and whose distance to the rest of the spectrum of ˆTn
is at least ρn∆∗/2. Furthermore  on the event E  our algorithm (Algorithm 1) returns the matrix
ˆG = (1/c1) ˆV ˆV T   where ˆV has by columns the eigenvectors corresponding to the eigenvalues on Λ1.
Theorem 4. Let W be a regular geometric graphon on Sd−1 with regularity parameter s and such
that ∆∗ > 0. Then there exists a set of eigenvectors ˆv1 ···   ˆvd of ˆTn such that

(cid:107)G∗ − ˆG(cid:107)F = O(n− s

2s+d−1 )

where ˆG = G ˆV and ˆV is the matrix with columns ˆv1 ···   ˆvd. Moreover  this rate is the minimax rate
of nonparametric estimation of a regression function f with Sobolev regularity s in dimension d − 1.
The condition ∆∗ > 0 allow us to use Davis-Kahan type results for matrix perturbation to prove
Theorem 4. With this and concentration for the spectrum we are able to control with high probability
the terms (cid:107) ˆG − G(cid:107)F and (cid:107)G − G∗(cid:107)F . The rate of nonparametric estimation of a function in Sd−1
can be found in [11  Chp.2].

3 Algorithms

The Harmonic EigenCluster algorithm(HEiC) (see Algorithm 1 below) receives the observed adja-
cency matrix ˆTn and the sphere dimension as its inputs to reconstruct the eigenspace associated to
1. In order to do so  the algorithm selects d vectors in the set ˆv1  ˆv2 ··· ˆvn  whose
the eigenvalue λ∗
2 ···   v∗
1  v∗
linear span is close to the span of the vectors v∗
d deﬁned in Section 2.3. The main idea
is to ﬁnd a subset of {ˆλ0  ˆλ2 ···   ˆλn−1}  which we call Λ1  consisting on d1 elements (recall that
1. This can be done assuming that the event E
d1 = d) and where all its elements are close to λ∗
deﬁned above holds (which occurs with high probability). Once we have the set Λ1  we return the
span of the eigenvectors associated to the eigenvalues in Λ1.
For a given set of indices i1 ···   id we deﬁne

Gap1( ˆTn; i1 ···   id) := min

i /∈{i1 ···  id} max

j∈{i1 ···  ij}

|ˆλj − ˆλi|

and

Gap1( ˆTn) :=

max

{i1 ···  id}∈S n

d

Gap1( ˆTn; i1 ···   id)

6

n−1} ←eigenvalues of ˆTn sorted in decreasing order
1+d}: where Λsort

is the i-th element in Λsort

i

Algorithm 1: Harmonic EigenCluster(HEiC) algorithm
Input: ( ˆTn  d) adjacency matrix and sphere dimension

1

1

 ···   ˆλsort
 ···   Λsort

Λsort = {ˆλsort
Λ1 ← {Λsort
Initialize i = 2  gap = Gap1( ˆTn; 1  2 ···   d)
if Gap1( ˆTn; i  i + 1 ···   i + d) > gap then
end if
i ← i + 1

while i ≤ n − d do
Λ1 ← {Λsort

 ···   Λsort
i+d}

i

end while
Return: Λ1  gap

0

where S n
d contains all the subsets of {1 ···   n − 1} of size d. This deﬁnition parallels that of
Gap1(W ) for the graphon. Observe any set of indices in S n
d will not include 0. Otherwise stated  we
can leave ˆλsort
out of this deﬁnition and it will not be candidate to be in Λ1. In the supplementary
material we prove that the largest eigenvalue of the adjacency matrix will be close to the eigenvalue λ∗
and in consequence can not be close enough to λ∗
0
1 to be in the set Λ1  given the deﬁnition of the
event E.
To compute Gap1( ˆTn) we consider the set of eigenvalues ˆλj ordered in decreasing order. We use the
notation ˆλsort

to emphasize this fact. We deﬁne the right and left differences on the sorted set by

j

left(i) = |ˆλsort
right(i) = left(i + 1)

i − ˆλsort
i−1|

where left(·) is deﬁned for 1 ≤ i ≤ n and right(·) is deﬁned for 0 ≤ i ≤ n− 1. With these deﬁnition 
we have the following lemma  which we prove in the supplementary material.
Lemma 5. On the event E  the following equality holds

Gap1( ˆTn) = max

max

1≤i≤n−d−1

min{left(i)  right(i + d)}  left(n − d + 1)

(cid:110)

(cid:111)

The set Λ1 has the form Λ1 = {ˆλsort
i∗
that either

  ˆλsort
i∗ = arg max
1≤i≤n−d−1

i∗+1 ···   ˆλsort

i∗+d} for some 1 ≤ i∗ ≤ n − d − 1. We have

min{left(i)  right(i + d)}

or i∗ = n − d depending whether or not one has max1≤i≤n−d−1 min{left(i)  right(i + d)} >
left(n− d + 1). The algorithm then constructs the matrix ˆV having columns {ˆvi∗   ˆvi∗+1 ···   ˆvi∗+d}
and returns ˆV ˆV T .
It is worth noting that Algorithm 1 time complexity n3 + n  where n3 comes from the fact that we
compute the eigenvalues and eigenvectors of the n × n matrix ˆTn and the linear term is because
we explore the whole set of eigenvalues to ﬁnd the maximum gap for the size d. In terms of space
complexity the algorithm is n2 because we need to store the matrix ˆTn.
Remark 1. If we change ˆTn in the input of Algorithm 1 to ˆT usvt
(obtained by the USVT algorithm [6])
we predict that the algorithm will give similar results. This is because discarding some eigenvalues
bellow a prescribed threshold do not have effect on our method. However  as preprocessing step the
USVT might help in speeding up the eigenspace detection  but this step is already linear in time.

n

3.1 Estimation of the dimension d
So far we have focused on the estimation of the population Gram matrix G∗. We now give an
algorithm to ﬁnd the dimension d  when it is not provided as input. This method receives the

7

matrix ˆTn as input and uses Algorithm 1 as a subroutine to compute a score  which is simply the value
of the variable Gap1( ˆTn) returned by Algorithm 1. We do this for each d in a set of candidates  which
we call D. This set of candidates will be usually  but not necessarily  ﬁxed to {1  2  3 ···   dmax}.
Once we have computed the scores  we pick the candidate that have the maximum score.
Given the guarantees provided by Theorem 4  the previously described procedure will ﬁnd the correct
dimension  with high probability (on the event E)  if the true dimension of the graphon is on the
candidate set D. This will happen  in particular  if the assumptions of Theorem 4 are satisﬁed. We
recall that the main hypothesis on the graphon is that the spectral gap Gap1(W ) should be different
from 0.

4 Experiments

We generate synthetic data using different geometric graphons. In the ﬁrst set of examples  we focus
in recovering the Gram matrix when the dimension is provided. In the second set we tried to recover
the dimension as well.

4.1 Recovering the Gram matrix

We start by considering the graphon W1(x  y) = 1(cid:104)x y(cid:105)≤0 which deﬁnes  through the sampling
scheme given in Section 2.2  the same random graph model as the classical RGG model on Sd−1
with threshold 0. Thus two sampled points Xi  Xj ∈ Sd−1 will be connected if and only if they lie in
the same semisphere.

Figure 1: In the left we have a boxplot of M SEn for different values of n. In the right  we plot the
score for a set of candidate dimensions D = {1 ···   19}. Data were sampled with W1 on Sd−1 with
d = 3.
We consider different values for the sample size n and for each of them we sample 100 Gram matrices
in the case d = 3 and run the Algorithm 1 for each. We compute each time the mean squared error 
deﬁned by

M SEn =

1

n2(cid:107) ˆG − G∗(cid:107)2

F

In Figure 1 we put the M SEn for different values of n  showing how M SEn decrease in terms of n.
For each n  the M SEn we plot is the mean over the 100 sampled graphs.

4.2 Recovering the dimension d

We conducted a simulation study using graphon W1  sampling 1000 point on the sphere of dimension
d = 3 and we use Algorithm 1 to compute a score and recover d. We consider a set of candidates
with dmax = 15. In Figure 1 we provide a boxplot for the score of each candidate repeating the
procedure 50 times. We see that for this graphon  the algorithm can each time differentiates the true
dimension from the “noise".

8

Figure 2: The mean (25 repetitions) runtime of the HEiC algorithm for the graphon W1. The
experiments were performed on a 3 3Ghz Intel i5 with 16GB RAM.

5 Discussion

Although in this paper we have focused on the sphere as the latent metric space  our main result
can be extended to other latent space where the distance is translation invariant  such as compact
Lie groups or compact symmetric spaces. In that case  the geometric graphon will be of the form
W (x  y) = f (cos ρ(x  y)) where x  y are points in the compact Lie group S and ρ(· ·) is the metric.
We will have

f (cos ρ(x  y)) = f (cos ρ(x · y−1  e1)) = ˜f (x · y−1)

where e1 is the identity element in S and ˜f (x) = f (ρ(x  e1)). In consequence W (x  y) = ˜f (x· y−1).
Furthermore  there exists an addition theorem in this case (which is central to our recovery result).
Analogous regularity notions to the one considered in this work are also worth exploring. In [8] the
authors give more details on the model of geometric graphon in compact Lie groups with focus on
the graphon estimation.
In principle  it would be possible to extend most of the results of this paper to the case when the
underlying space is Bd = {x ∈ Rd : (cid:107)x(cid:107) ≤ 1} and the link function depends only on the inner
products of the points in Bd. As detailed in [7]  the harmonic analysis on the sphere can be extended
to the unit ball. In particular  an analogous addition theorem exists. Besides  one fundamental fact
that used in the proof of Theorem 1 is the control on the growth of the L2(Sd−1) norm of the spherical
harmonics  which has its analog for the polynomial base in L2(Bd). Despite the similarities between
the model on the unit sphere and the model on the unit ball  they might generate very different graphs.
For instance  an interesting feature of the model on Bd is that is not only angle dependent (as in the
case of the unit sphere)  but also norm dependent. This would allow to generate graphs with more
heterogenous node distribution. The study in depth of this model is left for a future work as well as
the study of the sparser case.

References
[1] E. Arias-Castro  A. Channarond  B. Pelletier  and N. Verzelen. On the estimation of latent

distances using graph distances. arXiv:1804.10611  2018.

[2] A. Bandeira and R. Van Handel. Sharp nonasymptotic bounds on the norm of random matrices

with independent entries. Annals of Probability  44(4):2479–2506  2016.

[3] C. Borgs  J.T. Chayes  L. Lovasz  V.T Sos  and K. Vesztergombi. Convergent sequences of dense
graphs i. subgraph frequencies metric properties and testing. Adv. Math  219(6):1801–1851 
2008.

[4] C. Borgs  J.T Chayes  L. Lovasz  V.T. Sos  and K. Vesztergombi. Convergent sequences of
dense graphs ii. multiway cuts and statistical physics. Annals of Mathematics  176(1):151–219 
2012.

[5] S. Bubeck  J. Ding  R. Eldan  and M. Rácz. Testing for high dimensional geometry in random

graphs. Random Structures and Algorithms  49:503–532  2016.

9

[6] S. Chatterjee. Matrix estimation by universal singular value thresholding. Annals of Statistics 

43(1):177–214  2015.

[7] F. Dai and Y. Xu. Approximation theory and harmonic Analysis on spheres and balls. Springer

Verlag Monographs in Mathematics  2013.

[8] Y. De Castro  C. Lacour  and T.M. Pham Ngoc. Adaptive estimation of nonparametric geometric

graphs. arxiv.org/pdf/1708.02107.

[9] L. Devroye  A. György  L. Backstrom  and C. Marlow. High-dimensional random geometric

graphs and their clique number. Electronic journal of probability  16(90):2481–2508  2011.

[10] J. Diaz  C. McDiarmid  and D. Mitsche. Learning random points from geometric graphs or

orderings. arXiv:1804.10611  2018.

[11] M. Emery  A. Nemirovski  and D. Voiculescu. Lectures on probability theory . Springer-Verlag

Berlin Heidelberg  Ecole d’ete de probabilites de saint-ﬂour XXVIII edition  1998.

[12] P. Erdös and A. Rényi. On the evolution of random graphs. Publ. Math. Inst. Hungar. Acad. Sci 

5:17–60  1960.

[13] E.N. Gilbert. Random plane networks. J.Soc.Industrial Applied Mathematics  9(5):533–543 

1961.

[14] R. Gupta  T. Roughgarden  and C. Sheshadhri. Decomposition of triangle-dense graphs. SIAM

Journal on Computing  45(2):197–215  2016.

[15] D.J. Higham  M. Rasajski  and N. Przulj. Fitting a geometric graph to a protein-protein

interaction network. Bioinformatics  24(8):1093–1099  2008.

[16] F. Hirsch and G. Lacombe. Elements of functional analysis. Springer-Verlag New York  1999.

[17] P. Hoff  A. Raftery  and M. Handcock. Latent space approaches to social network analysis.

Journal of the American Statistical Association  97(460):1090–1098  2002.

[18] X. Jia. Wireless networks and random geometric graphs. Proc. Int. Symp. Parallel Architectures 

Algorithms and Networks  pages 575–579  2004.

[19] D Karger  R. Motwani  and M. Sudan. Approximate graph coloring by semideﬁnite program-

ming. Journal of the ACM (JACM)  45(2):246–265  1998.

[20] O. Klopp  A. Tsybakov  and N. Verzelen. Oracle inequalities for network models and sparse

graphon estimation. Annals of Statistics  45(1):316–354  2017.

[21] V. Koltchinskii. Asymptotics of spectral projections of some random matrices approximating
integral operators. Progress in Probability  43(In: Eberlein E.  Hahn M.  Talagrand M. (eds)
High Dimensional Probability):191–227  1998.

[22] V. Koltchinskii and E. Giné. Random matrix approximation of spectra of integral operators.

Bernoulli  pages 113–167  2000.

[23] K. Levin and V. Lyzinski. Laplacian eigenmaps from sparse  noisy similarity measurements.

IEEE Transactions on Signal Processing  65:1998–2003  2017.

[24] L. Lovasz. Large networks and graph limits. Colloquium Publications (AMS)  2012.

[25] L. Lovász and B. Szegedy. Limits of dense graph sequences. J.Combin.Theory.Ser B  96(6):197–

215  2006.

[26] S. Nicaise. Jacobi polynomials  weighted Sobolev spaces and approximation results of some

singularities. Math. Nachr.  213:117–140  2000.

[27] M Penrose. Random geometric graphs. Oxford University Press  ﬁrst edition  2003.

10

[28] D.L. Sussman  M. Tang  and C.E. Priebe. Consistent latent position estimation and vertex
IEEE transactions on Pattern Analysis and

classiﬁcation for random dot product graphs.
Machine Intelligence  36:48–57  2014.

[29] M Tang  D.L Sussman  and C.E Priebe. Universally consistent vertex classiﬁcation for latent

position graphs. Annals of Statistics  41:1406–1430  2013.

[30] M. Walters. Random geometric graphs. Surveys in Combinatorics  pages 365–402  2011.

11

,Ernesto Araya Valdivia
De Castro Yohann