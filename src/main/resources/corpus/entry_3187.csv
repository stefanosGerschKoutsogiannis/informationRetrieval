2018,Learning without the Phase: Regularized PhaseMax Achieves Optimal Sample Complexity,The problem of estimating an unknown signal  $\mathbf x_0\in \mathbb R^n$  from a vector $\mathbf y\in \mathbb R^m$ consisting of $m$ magnitude-only measurements of the form $y_i=|\mathbf a_i\mathbf x_0|$  where  $\mathbf a_i$'s are the rows of a known measurement matrix $\mathbf A$ is a classical problem known as phase retrieval. This problem arises when measuring the phase is costly or altogether infeasible. In many applications in machine learning  signal processing  statistics  etc.  the underlying signal has certain structure (sparse  low-rank  finite alphabet  etc.)  opening of up the possibility of recovering $\mathbf x_0$ from a number of measurements smaller than the ambient dimension  i.e.  $m<n$. Ideally  one would like to recover the signal from a number of phaseless measurements that is on the order of the "degrees of freedom" of the structured $\mathbf x_0$. To this end  inspired by the PhaseMax algorithm  we formulate a convex optimization problem  where the objective function relies on an initial estimate of the true signal and also includes an additive regularization term to encourage structure. The new formulation is referred to as {\textbf{regularized PhaseMax}}. We analyze the performance of regularized PhaseMax to find the minimum number of phaseless measurements required for perfect signal recovery. The results are asymptotic and are in terms of the geometrical properties (such as the Gaussian width) of certain convex cones. When the measurement matrix has i.i.d. Gaussian entries  we show that our proposed method is indeed order-wise optimal  allowing perfect recovery from a number of phaseless measurements that is only a constant factor away from the degrees of freedom. We explicitly compute this constant factor  in terms of the quality of the initial estimate  by deriving the exact phase transition. The theory well matches empirical results from numerical simulations.,Learning without the Phase: Regularized PhaseMax

Achieves Optimal Sample Complexity

Department of Electrical Engineering

Department of Electrical Engineering

Ehsan Abbasi

Caltech

Fariborz Salehi

Caltech

fsalehi@caltech.edu

eabbasi@caltech.edu

Babak Hassibi

Department of Electrical Engineering

Caltech

hassibi@caltech.edu∗

Abstract

The problem of estimating an unknown signal  x0 ∈ Rn  from a vector y ∈ Rm
consisting of m magnitude-only measurements of the form yi = |aix0|  where
ai’s are the rows of a known measurement matrix A  is a classical problem known
as phase retrieval. This problem arises when measuring the phase is costly or
altogether infeasible. In many applications in machine learning  signal processing 
statistics  etc.  the underlying signal has certain structure (sparse  low-rank  ﬁnite
alphabet  etc.)  opening of up the possibility of recovering x0 from a number of
measurements smaller than the ambient dimension  i.e.  m < n. Ideally  one
would like to recover the signal from a number of phaseless measurements that
is on the order of the "degrees of freedom" of the structured x0. To this end 
inspired by the PhaseMax algorithm  we formulate a convex optimization problem 
where the objective function relies on an initial estimate of the true signal and
also includes an additive regularization term to encourage structure. The new
formulation is referred to as regularized PhaseMax. We analyze the performance
of regularized PhaseMax to ﬁnd the minimum number of phaseless measurements
required for perfect signal recovery. The results are asymptotic and are in terms of
the geometrical properties (such as the Gaussian width) of certain convex cones.
When the measurement matrix has i.i.d. Gaussian entries  we show that our
proposed method is indeed order-wise optimal  allowing perfect recovery from a
number of phaseless measurements that is only a constant factor away from the
optimal number of measurements required when phase information is available.
We explicitly compute this constant factor  in terms of the quality of the initial
estimate  by deriving the exact phase transition. The theory well matches empirical
results from numerical simulations.

1

Introduction

Recovering an unknown signal or model given a limited number of linear measurements is an
important problem that appears in many applications. Researchers have developed various methods
∗This work was supported in part by the National Science Foundation under grants CNS-0932428  CCF-
1018927  CCF-1423663 and CCF-1409204  by a grant from Qualcomm Inc.  by NASA’s Jet Propulsion
Laboratory through the President and Director’s Fund  and by King Abdullah University of Science and
Technology.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

with rigorous theoretical guarantees for perfect signal reconstruction  e.g. [5  16  38  43]. However 
there are many practical scenarios in which the signal should be reconstructed from nonlinear
measurements. In particular  in many physical devices  measuring the phase is expensive or even
infeasible. For instance  detection devices such as CCD cameras and photosensitive ﬁlms cannot
measure the phase of a light wave and instead measure the photon ﬂux [22].
The fundamental problem of recovering a signal from magnitude-only measurements is known as
phase retrieval. It has a rich history and occurs in many areas in engineering and applied sciences
such as medical imaging [15]  X-ray crystallography [27]  astronomical imaging [17]  and optics [45].
Due to the loss of phase information  signal reconstruction from magnitude-only measurements can
be quite challenging. Therefore  despite a variety of proposed methods and analysis frameworks 
phase retrieval still faces fundamental theoretical and algorithmic challenges.
Recently  convex methods have gained signiﬁcant attention to solve the phase retrieval problem. The
ﬁrst convex-relaxation-based methods were based on semideﬁnite programs [7  10] and resorted
to the idea of lifting [2  8  23  36] the signal from a vector to a matrix to linearize the quadratic
constraints. While the convex nature of this formulation allows theoretical guarantees  the resulting
algorithms are computationally inefﬁcient since the number of unknowns is effectively squared. This
makes these approaches intractable when the system dimension is large.
Introduced in two independent papers [3  19]  PhaseMax is a novel convex relaxation for phase
retrieval which works in the original n-dimensional parameter space. Since it does not require lifting
and does not square the number of unknowns  it is appealing in practice. It does  however  require
an intial estimate of the signal. Preliminary theoretical analysis [3  13  19  21] indicates the method
achieves perfect recovery for an order optimal number of random measurements. The exact phase
transition for PhaseMax has been recently computed in a sequence of papers  ﬁrst for the case of real
measurements [14] and then for the case of complex ones [35].
Non-convex methods for phase retrieval have a long history [18]. Recent non-convex methods start
with a careful initialization [25  26  28] and update the solution iteratively using a gradient-descent-
like scheme. Examples of such methods include Wirtinger ﬂow algorithms [9  12  37]  truncated
amplitude ﬂow [46]  and alternating minimization [29  49]. Despite having lower computational cost 
precise theoretical analysis of such algorithms seems very technically challenging.
All the aforementioned algorithms essentially demonstrate that a signal of dimension n can be
perfectly recovered through m > Cn amplitude-only measurements  where C > 1 is a constant that
depends on the algorithm as well as the measurement vectors. However  many interesting signals
in practice contain fewer degrees of freedom than the ambient dimension (sparse signals  low-rank
matrices  ﬁnite alphabet signals  etc.). Such low-dimensional structures open up the possibility of
perfect signal recovery with a number of measurements signiﬁcantly smaller than n.

1.1 Summary of contributions

In this paper we propose a new approach for recovering structured signals. Inspired by the PhaseMax
algorithm  we introduce a new convex formulation and investigate necessary and sufﬁcient conditions 
in terms of the number of measurements  for perfect recovery. We refer to this new framework as
regularized PhaseMax. The constrained set in this optimization is obtained by relaxing the non-convex
equality constraints in the original phase retrieval problem to convex inequality constraints. The
objective function consists of two terms. One is a linear functional that relies on an initial estimate of
the true signal which must be externally provided. The second term is an additive regularization term
that is formed based on a priori structural information about the signal.
We utilize the recently developed Convex Gaussian Min-Max Theorem (CGMT) [39] to precisely
compute the necessary and sufﬁcient number of measurements for perfect signal recovery when the
entries of the measurement matrix are i.i.d. Gaussian. To the extent of our knowledge  this is the ﬁrst
convex optimization formulation for the problem of structured signal recovery given phaseless linear
Gaussian measurements that provably requires an order optimal number of measurements. In this
paper we focus on real signals and real measurements. The complex case is more involved  requires
a different analysis  and will be considered in a separate work. Through our analysis  we make the
following main contributions:

2

• We ﬁrst provide a sufﬁcient recovery condition  in Section 3.1  in terms of the number of
measurements  for perfect signal recovery. We use this to infer that our proposed method is
order-wise optimal.
• We characterize the exact phase transition behavior for the class of absolutely scalable
• We apply our ﬁndings to two special examples: unstructured signal recovery and sparse
recovery. We observe that the theory well matches the result of numerical simulations for
these two examples.

regularization functions.

1.2 Prior work

Phase retrieval for structured signals has gained signiﬁcant attention in recent years. A review of all
of the results is beyond the scope of this paper  and we instead brieﬂy mention some of the most
relevant literature for the Gaussian measurement model. Oymak et. al. [30] analyzed the performance
of the regularized PhaseLift algorithm and observed that the required sample complexity is of a
suboptimal order compared to the optimal number of measurements required when phase information
is available. For the special case of sparse phase retrieval similar results have been reported in [24]
which indicates O(k2 log(n)) measurements are required for recovering of a k-sparse signal  using
regularized PhaseLift. Recently  there has been a stream of work on solving phase retrieval using non-
convex methods [6  47]. In particular  Soltanolkotabi [37] has shown that amplitude-based Wirtinger
ﬂow can break the O(k2 log(n)) barrier. We also note that the paper [20] analyzed the PhaseMax
algorithm with (cid:96)1 regularizer and observed that it achieves perfect recovery with O(k log(n/k))
samples  provided a well-correlated initialization point.

2 Preliminaries

2.1 Problem setup
Let x0 ∈ Rn denote the the underlying structured signal. We consider the real phase retrieval
problem with the goal of recovering x0 from m magnitude-only measurements of the form 

yi = |aT

i x0|  i = 1  2  . . .   m  

(1)
where {ai ∈ Rn}m
i=1 is the set of (known) measurement vectors. In practice  this set is identiﬁed
based on the experimental settings; however  throughout this paper (for our analysis purposes) we
assume that the ai’s are drawn independently from a Gaussian distribution with mean zero and
covariance matrix In. In order to exploit the structure of the signal we assume f (·) is a convex
function that measures the "complexity" of the structured solution. The regularized PhaseMax
algorithm also relies on an initial estimate of the true signal. Here  xinit is used to represent this initial
guess. Our analysis is based on the critical assumption that both xinit and x0 are independent of all
the measurement vectors. The constraint set in generalized PhaseMax is derived by simply relaxing
the equality constraints in (1) into convex inequality constraints. We introduce the following convex
optimization problem to recover the signal:

ˆx = argmin
x∈Rn
subject to:

Lλ(x) = −xinit
|aT
i x| ≤ yi  

Tx + λf (x)
for 1 ≤ i ≤ m.

(2)

The function f is assumed to be sign invariant  i.e.  f (x) = f (−x) for all x ∈ Rn (−x has the
same "complexity" as x.) Note that because of the global phase ambiguity of measurements in (1) 
we can only estimate x0 up to a sign. Up to this sign ambiguity  we can use the normalized mean
squared error (NMSE)  deﬁned as ||ˆx−x0||2
  to measure the performance of the solution. In this paper
||x0||2
we investigate the conditions under which the optimization program (2) uniquely identiﬁes the true
signal  i.e.  ˆx = x0 (up to the sign). Our results are asymptotic which is valid when m  n → ∞.

2.2 Background on convex analysis

Our results give the required number of measurements as a function of certain geometrical properties
of the descent cone of the objective function. Here  we recall these deﬁnitions from convex analysis.

3

Deﬁnition 1. (Descent cone) For a function R : Rn → R the descent(tangent) cone at point x is
deﬁned as 

TR(x) = cone({z ∈ Rn : R(x + z) ≤ R(x)})  

where cone(S) denotes the closed conical hull of the set S.
Deﬁnition 2. Let S be a closed convex set in Rn. For x ∈ Rn the projection of x on S  denoted by
ΠS (x)  is deﬁned as follows 

(3)

ΠS (x) := argmin

y∈S

||x − y||  

(4)

where || · || is the Euclidean norm. The distance function is deﬁned as: distS (x) = ||x − ΠS (x)||.
Deﬁnition 3. (Statistical dimension) [1] The statistical dimension of a closed convex cone C in Rn
is deﬁned as 

d(C) = Eg [||ΠC(g)||2]  

(5)

where g ∈ Rn is a standard normal vector.

The statistical dimension canonically extends the dimension of linear spaces to convex cones. This
quantity has been extensively studied in linear inverse problems. It is well-known that as n →
∞  m > d(TLλ(x0)) is the necessary and sufﬁcient condition for perfect signal recovery under
noiseless linear Gaussian measurements [11  38]. Our analysis indicates that given phaseless linear
measurements  the regularized PhaseMax algorithm requires O(d(TLλ(x0))) measurements for
perfect signal reconstruction. Therefore  it is order-wise optimal in that sense.

3 Main Results

In this section we present the main results of the paper which provide us with the required number of
measurements for perfect signal recovery in the regularized PhaseMax optimization (2). This gives
the value m0 = m0(n  x0  xinit  λ)  such that the regularized PhaseMax algorithm uniquely identiﬁes
the underlying signal x0 with high probability whenever m > m0.
In Section 3.1  we ﬁnd sufﬁcient conditions for recovery of the underlying signal. Theorem 1 provides
an upper bound on the number of measurements that is equal to a constant factor times the statistical
dimension of the descent cone  d(TLλ(x0)). Therefore  even though our analysis is not exact in this
section  it leads us to the important observation that our proposed method is order-wise optimal in
terms of the required sample complexity for perfect signal reconstruction.
In Section 3.2  we provide an exact analysis for the phase transition behavior of regularized PhaseMax
when the regularizer is an absolutely scalable function. We apply this result to the case of unstructured
phaseless recovery as well as sparse phaseless recovery to compute the exact phase transitions. We
then compare the result of theory with the empirical results from numerical simulations.

3.1 Sufﬁcient recovery condition

0 and P⊥ := I − P denote the projectors onto the span of x0 and its orthogonal
Let P := 1||x0||2 x0xT
complement  respectively  where || · || denotes the (cid:96)2-norm of the vectors. We also deﬁne d(n) :=
d(TLλ(x0)) as the statistical dimension of the descent cone of the objective function at point x0. Our
analysis rigorously characterizes the phase transition behavior of the regularized PhaseMax in the
large system limit  i.e.  when n → ∞  while m and d(n) grow at a proportional ratio δ = m
d(n) . δ
is often called the oversampling ratio. Here  the superscript (n) is used to denote the elements of a
sequence. To streamline the notations  we often drop this when understood from the context.
Theorem 1 provides sufﬁcient conditions for the successful recovery of x0. The recovery threshold
depends on λ and the initialization vector  xinit. We deﬁne ρinit := xinit
Tx0 to quantify the caliber
of the initial estimate. Due to the sign invariance property of the solution  we can assume without
loss of generality that ρinit ≥ 0. Before stating the theorem  we shall introduce the function
R(·) : (2  +∞) → R+.

4

Figure 1: R(x) for different values of x. R is a monotonically decreasing function.

Deﬁnition 4. For x > 2  R(x) is the unique nonzero solution of the following equation:

t2 =

x
π

((1 + t2)atan(t) − t) .

(6)

Figure 1 depicts the evaluation of the function R(x) for different input values x. As observed  R(x)
is a decreasing function with respect to x  and it approaches zero as x grows to inﬁnity. It can be
shown that for large values of the input x  R(x) decays with the rate 1
x.
Theorem 1 (Sufﬁcient recovery condition). For a ﬁxed oversampling ratio δ > 2  the regularized
P{||ˆx − x0||2 >
PhaseMax optimization (2) perfectly recovers the target signal (in the sense that lim
n→∞
||x0||2} = 0  for any ﬁxed  > 0) if 

R(δ) < sup

v∈∂Lλ(x0)

||Pv||
||P⊥v||  

where ∂Lλ(x0) denotes the sub-differential set of the objective function Lλ(·) at point x0.
It is worth noting that ∂Lλ(x0) is a convex and compact set  and it can be expressed in terms of the
sub-differential of the regularization function ∂f (x0) as following 

∂Lλ(x0) = {λu − xinit : u ∈ ∂f (x0)} .

(8)
Observe that since R(·) is a monotonically decreasing function  the inequality (7) gives a lower bound
for the oversampling ratio δ. Indeed  we can restate the result in terms of this lower bound as the
following corollary:
Corollary 1. If there exist a ﬁxed constant τ > 0 such that 

(7)

(9)

||Pv||
||P⊥v|| > τ 

sup

v∈∂Lλ(x0)

then the regularized PhaseMax optimization (2) has perfect recovery for δ > C  where C is a
constant that only depends on τ.
Proof. It is an immediate consequence of Theorem 1 by choosing C = R−1(τ ) and noting that R(·)
is monotonically decreasing.

This result indicates that if xinit and λ are chosen in such a way that the inequality (9) is satisﬁed
for some positive constant τ  then one needs m > Cd(n) measurement samples for perfect recovery 
where C is a constant and d(n)(= d) is the statistical dimension of the descent cone of the objective
function at point x0. As motivating examples  we use Theorem 1 to ﬁnd upper bounds on the phase
transition when x0 has no structure or it is a sparse signal.
Example 1: Assume the target signal x0 has no a priori structure. The objective function in this case
would be L(x) = −xinit
Tx  and ∂L(x0) = {−xinit}. It can be shown that the statistical dimension is

5

(a)

(b)

Figure 2: Phase transition regimes for the regularized PhaseMax problem in terms of the oversampling ratio
δ and ρinit = xinit
Tx0  for the cases of x0 with (a) no structure and (b) sparse signal recovery . The blue lines
indicate the theoretical estimate for the phase transition derived from Theorem 2. The red line in (a) correspond
to the upper bound calculated by Theorem 1. In the simulations we used signals of size n = 128. The result is
averaged over 10 independent realization of the measurements.

d(n) = n − 1/2. Due to the absence of the regularization term in this case  without loss of generality
we can assume (cid:107)x0(cid:107) = (cid:107)xinit(cid:107) = 1. Theorem 1 provides the following sufﬁcient condition for perfect
recovery:

> R(δ) .

(10)
This indicates O(n) measurements is sufﬁcient for perfect recovery as long as ρinit ≥ ρ0  where
ρ0 > 0 is a constant that does not approach zero as n → ∞. The exact phase transition for
the unstructured case (PhaseMax) has been derived in [14] which is compatible with this result.
Figure 2(a) shows the result of numerical simulation for different values of δ and ρinit  when n = 128.
As depicted in the ﬁgure  the sufﬁcient recovery condition from Theorem 1 is approximately a factor
of 2 away from the actual phase transition.
Example 2: Let x0 be a k-sparse signal. In this case we use || · ||1 as the regularization function.
  then d(n) ≤ Ck log(n/k)  for some constants c  C > 0.
We show in Section 5.5 that if λ > c√
This matches the well-known order for the statistical dimension derived in the compressive sensing
literature [38].
Moreover  in order to satisfy the condition in Corollary 1 we need to have
> (1 + )λ  for
some  > 0. Therefore  x0 can be perfectly recovered having O(k log(n/k)) samples when the
hyper-parameter λ is tuned properly  i.e. 
. Figure 3(a) compares this upper bounds
with the precise analysis that we will show in Section 3.2. As depicted in this ﬁgure  the sufﬁcient
recovery condition is a valid upper bound on the phase transition  but it is not sharp.

< λ < ρinit
||x0||1

ρinit
||x0||1

||Pxinit||
||P⊥xinit|| =

ρinit(cid:112)1 − ρ2

init

k

c√
k

3.2 Precise phase transition

So far  we have provided a sufﬁcient condition for perfect signal recovery in the regularized PhaseMax.
In this section we give the exact phase transition  i.e.  the minimum number of measurements m0
required for perfect recovery of the unknown vector x0. For our analysis  we assume that the function
f (x) is absolutely homogeneous (scalable)  i.e.  f (τ · x) = |τ| · f (x)  for any scalar τ. This covers a
(x0) ⊂ Rn denote
large range of regularization functions such as norms and semi-norms. Let ∂Lλ
the projection of the sub-differential set into the orthogonal complement of x0  i.e. 

⊥

⊥

(x0) = {P⊥u : u ∈ ∂Lλ(x0)}  

∂Lλ

(11)

which is a convex and compact set. To state the result in a general framework  we require a further
assumption on functions L(n)

λ (·).

6

(a)

(b)

Figure 3: (a) Comparing the upper bounds on the phase transition  derived by Theorem 1 (dashed lines) and
the precise phase transition by Theorem 2 (solid lines)  for three values of the sparsity factor s = k/n. (b) The
phase transition behavior as a function of the regularization parameter λ  derived from the result of Theorem 2.

Assumption 1 (Asymptotic functionals) We say Assumption 1 holds if the following uniform conver-
gences exist  as n → ∞ 

β − E(cid:2) 1√
E(cid:2)dist∂Lλ

n

h)(cid:3) Unif.−−→ Fλ(β)  and 

hT Π∂Lλ
⊥(x0)(
β√
n

⊥(x0)(

β√
n

h)(cid:3) Unif.−−→ Gλ(β)  

λ(β)  where G(cid:48)

This assumption especially holds for the class of separable regularizers  where f (v) =(cid:80)

(12)
where h ∈ Rn has i.i.d. standard normal entries and Fλ  Gλ : R+ → R denote the functions that
the sequences uniformly converge to.
One can show that  under some mild conditions on the regularization function f (·)  Assumption 1
λ(·) denotes the derivative of the function Gλ(·).
holds and also Fλ(β) = Gλ(β)G(cid:48)
˜f (vi)
(e.g. (cid:96)1 norm for the case of sparse phase-retrieval). Later in this section  we will see validity of
this assumption for two examples discussed earlier in Section 3. Our precise phase transition results
indicate the required number of measurements as the solution of a set of two nonlinear equations
with two unknowns. We deﬁne a new parameter α := m
n indicates the exact
phase transition of the regularized PhaseMax optimization. The following theorem gives an implicit
formula to derive αopt.
Theorem 2 (Precise phase transition). Let ˆx be the solution to the regularized PhaseMax optimization
(2) with the objective function Lλ(x) = −xinit
Tx + λf (x)  where the convex function f (·) is
absolutely homogeneous and Assumption 1 holds. The regularized PhaseMax optimization would
perfectly recover the target signal x0 if and only if:

n   where αopt = m0

i

1. α > αopt  where αopt is the solution of the following system of non-linear equations with

two unknowns  α and β 

(cid:40)−Gλ(β) Lλ(x0) = tan( π

tan( π

αβ Fλ(β)) (Gλ(β) + π

λ(β) − βFλ(β))  

αβ Fλ(β)) (G2
αβ Fλ(β) Lλ(x0)) = π

αβ Fλ(β) Gλ(β)  

(13)

2. and  Lλ(x0) < Lλ(0) = 0 .

where the functions Fλ(·) and Gλ(·) are deﬁned in (12).
A few remarks are in place for this theorem:
[Solving equations (13)] The system of nonlinear equations (13) only involves two scalars β and
α  and the functions Fλ(β) and Gλ(β) are determined by the objective function Lλ(x). For our
numerical simulations in the examples of Section 3.2.1 and Section 3.2.2  we used a ﬁxed-point

7

iteration method that can quickly ﬁnd the solution given a proper initialization.
[Tuning λ] Theorem 2 requires the objective function to satisfy Lλ(x0) = λf (x0) − ρinit <
0. Therefore  it is necessary to choose λ in such a way that λ < ρinit/f (x0). Some additional
assumptions on the unknown vector x0 enables us to calculate the proper range for λ. For instance 
if we consider a random ensemble for x0 where the non-zero entries of x0 are Gaussian (or other)
random variables  E[f (x0)] gives a reasonable estimation on f (x0) that can help us choosing λ
appropriately. We will see an example of such case in section 3.2.2. Figure 3(b) shows an example of
how the phase transition of the regularized PhaseMax  or equivalently the required sample complexity 
behaves as a function of the hyper-parameter λ.
In the next sections  we use the result of Theorem 2 to compute the exact phase transition for the case
of unstructured signal as well as the sparse signal recovery. Since the regularizer f (x) is absolutely
scalable  for both examples  we assume that (cid:107)x0(cid:107) = 1.

3.2.1 Unstructured signal recovery

When there is no a priori information about the structure of the target signal  we use the following
optimization (PhaseMax) for signal recovery:

ˆx = argmin
x∈Rn
subject to:

L(x) = −xinit
|aT
i x| ≤ yi  

Tx
for 1 ≤ i ≤ m .

(14)

(15)

(16)

(17)

Due to the absence of the regularization term  without loss of generality we can assume ||xinit|| = 1.
Moreover  L(x0) = −ρinit which indicates that the second condition in Theorem 2 . To apply the
result of our theorem  we ﬁrst compute explicit formulas for the functions Fλ(β)  and Gλ(β)  as
follows 

(cid:113)

β2 + 1 − ρ2
We can now form the system of nonlinear equations (13) as follows 

Fλ(β) = β   Gλ(β) =

init .

(cid:40) (cid:112)β2 + 1 − ρ2

α ) ((cid:112)β2 + 1 − ρ2

ρinit
1−ρ2

init

init

tan( π

= tan( π
α )  
init − πρinit
α ) = π

α

(cid:112)β2 + 1 − ρ2

init .

Finally  solving equations (16) yields the following necessary and sufﬁcient condition for perfect
recovery 

π

> 1 − ρ2

init  

α tan(π/α)

which also veriﬁes the result of [14].
Figure 2(a) shows the result of numerical simulations of running the PhaseMax algorithm for different
values of ρinit and δ. The intensity level of the color of each square in Figure 2  represents the
error of PhaseMax in recovering x0. As seen in the ﬁgure  although our theoretical results has been
established for the asymptotic setting (when the problem dimensions approach inﬁnity)  the blue
line  which is derived from (17)  reasonably predicts the phase transition for n = 128. The sufﬁcient
conditions that is derived from Theorem 1 is also depicted by the red line in the same ﬁgure.

3.2.2 Sparse recovery

We consider the case where the target signal x0 is sparse with k non-zero entries. The convex function
n||x||1  which is known to be a proper regularizer that enforces sparsity [41]  is used in the
f (x) = 1√
regularized PhaseMax optimization to recover x0 

ˆx = argmin
x∈Rn
subject to:

Lλ(x) = −xinit
i x| ≤ yi  
|aT

λ√
n

||x||1
Tx +
for 1 ≤ i ≤ m .

(18)

(cid:20) v∆

(cid:21)

To streamline notations  we assume the non-zero entries of x0 are the ﬁrst k entries and decompose
vector v ∈ Rn as v =
  where v∆ ∈ Rk denotes the ﬁrst k entries of v  and v∆c ∈ Rn−k
is the remaining n − k entries. As m  n → ∞  we would like to apply the result of Theorem 2

v∆c

8

Fλ(β) = β(s + 2(1 − s) · Q(

λ(cid:113)
β2 +
λ(β) = s · (β2 + λ2) + (cid:107)x∆
init(cid:107)2 − 2λ
G2
init(cid:107)2
(cid:107)x∆c
1 − s

+ (1 − s)(β2 +

) )  

(cid:107)x∆c
init (cid:107)2
1−s
√
s˜ρ − L2(x0)

) · EH [ shrink2(H 

λ(cid:113)

β2 +

(cid:107)x∆c
init (cid:107)2
1−s

) ]

(20)

(cid:20)x∆

(cid:21)

to compute the exact phase transition. Due to the rotational invariance property of the Gaussian
distribution  it can be shown that multiplying the last (n − k) entries of xinit  by a unitary matrix
U ∈ R(n−k)×(n−k) does not change the phase transition behavior in (2). Hence  we can assume the
entries of x∆c

init have Gaussian distribution  i.e. 

 

and x∆c

init
x∆c
init

init =

xinit =

(19)
where g ∈ Rn−k has standard normal entries. This observation enables us to establish the following
lemma:
Lemma 1. Consider the optimization problem (18) to recover the k-sparse signal x0. We assume the
init  where sign(·) denotes
entries of xinit are distributed as in (19) and deﬁne ˜ρ := 1√
the component-wise sign function. Then  Assumption 1 holds with:

sign(x∆

0 )Tx∆

k

1√
n − k

||x∆c

init|| g  

where Q(·) is the tail distribution of the standard normal distribution  H has standard normal
distribution and s := k/n is the sparsity factor. The shrinkage function shrink(· ·) : R × R+ → R+
is deﬁned as:

shrink(x  τ ) = (|x| − τ )1{|x| ≥ τ} .

(21)
It is worth noting that the function shrink(· ·) also appeared in computing the statistical dimension
for (cid:96)1 regularization (see Section 5.5) which indicates some implicit relation to αopt.
We have numerically computed the solution of the nonlinear system (20). Figure 2(a)  and Figure
2(b) shows the error of regularized PhaseMax over a range of ρinit and δ. The comparison between
our upper bound derived from Theorem 1 and precise analysis of Theorem 2 is depicted in Figure 3(a)
for three values of the sparsity factor s = 0.05  0.1  0.2. Observe that the upper bound is only a
constant factor away from the precise phase transition  while its derivation involves simpler formulas.
Finally  Figure 3(b)  illustrates impact of the regularization parameter λ on the phase transition of
the regularized PhaseMax optimization for four values of ρinit. The values of λ in this ﬁgure are
normalized by ρinit

(cid:107)x0(cid:107)   which is the maximum acceptable value of λ in the regularized PhaseMax.

√

n

4 Conclusion and Future Directions

In this paper  we introduced a new convex optimization framework  regularized PhaseMax  to
solve the structured phase retrieval problem. We have shown that  given a proper initialization 
the regularized PhaseMax optimization perfectly recovers the underlying signal from a number
of phaseless measurements that is only a constant factor away from the number of measurements
required when the phase information is available. We explicitly computed this constant factor.
An important (yet still open) research problem is to investigate the required sample complexity to
construct a proper initialization vector  xinit. As an example  for the case of sparse phase retrieval  even
though our analysis indicates that O(k log n
k ) is the required sample complexity of the regularized
PhaseMax optimization  the best known initialization technique [6] needs O(k2 log n) samples to
generate a meaningful initialization  which is suboptimal. An important future direction is to study
initialization techniques that break this sample complexity barrier  or to use information theoretic
arguments (as in [28]) to show that the sample complexity for the initialization cannot be improved.
To form the objective function in the regularized PhaseMax  we exploited some a-priori knowledge
about the structure of the underlying signal. In many practical settings such prior information is
not available. There has been some interesting recent publications (e.g. [4  48]) which introduce
efﬁcient algorithms to learn the structure of the underlying signal. An interesting research direction
is to investigate new optimization framework that does not rely on the prior information about the
structure of the underlying signal.

9

References
[1] Dennis Amelunxen  Martin Lotz  Michael B McCoy  and Joel A Tropp. Living on the edge:
Phase transitions in convex programs with random data. Information and Inference: A Journal
of the IMA  3(3):224–294  2014.

[2] Sohail Bahmani and Justin Romberg. Efﬁcient compressive phase retrieval with constrained
sensing vectors. In Advances in Neural Information Processing Systems  pages 523–531  2015.

[3] Sohail Bahmani and Justin Romberg. Phase retrieval meets statistical learning theory: A ﬂexible

convex relaxation. In Artiﬁcial Intelligence and Statistics  pages 252–260  2017.

[4] Milad Bakhshizadeh  Arian Maleki  and Shirin Jalali. Compressive phase retrieval of structured
signals. In 2018 IEEE International Symposium on Information Theory (ISIT)  pages 2291–2295.
IEEE  2018.

[5] Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. IEEE Transactions

on Information Theory  58(4):1997–2017  2012.

[6] T Tony Cai  Xiaodong Li  Zongming Ma  et al. Optimal rates of convergence for noisy sparse
phase retrieval via thresholded wirtinger ﬂow. The Annals of Statistics  44(5):2221–2251  2016.

[7] Emmanuel J Candes  Yonina C Eldar  Thomas Strohmer  and Vladislav Voroninski. Phase

retrieval via matrix completion. SIAM review  57(2):225–251  2015.

[8] Emmanuel J Candes  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval from coded

diffraction patterns. Applied and Computational Harmonic Analysis  39(2):277–299  2015.

[9] Emmanuel J Candes  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger
ﬂow: Theory and algorithms. IEEE Transactions on Information Theory  61(4):1985–2007 
2015.

[10] Emmanuel J Candes  Thomas Strohmer  and Vladislav Voroninski. Phaselift: Exact and stable
signal recovery from magnitude measurements via convex programming. Communications on
Pure and Applied Mathematics  66(8):1241–1274  2013.

[11] Venkat Chandrasekaran  Benjamin Recht  Pablo A Parrilo  and Alan S Willsky. The convex
geometry of linear inverse problems. Foundations of Computational mathematics  12(6):805–
849  2012.

[12] Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly
as easy as solving linear systems. In Advances in Neural Information Processing Systems  pages
739–747  2015.

[13] Oussama Dhifallah and Yue M Lu. Fundamental limits of phasemax for phase retrieval: A
replica analysis. In Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP) 
2017 IEEE 7th International Workshop on  pages 1–5. IEEE  2017.

[14] Oussama Dhifallah  Christos Thrampoulidis  and Yue M Lu. Phase retrieval via polytope opti-
mization: Geometry  phase transitions  and new algorithms. arXiv preprint arXiv:1805.09555 
2018.

[15] Martin Dierolf  Andreas Menzel  Pierre Thibault  Philipp Schneider  Cameron M Kewish  Roger
Wepf  Oliver Bunk  and Franz Pfeiffer. Ptychographic x-ray computed tomography at the
nanoscale. Nature  467(7314):436–439  2010.

[16] David L Donoho  Arian Maleki  and Andrea Montanari. Message-passing algorithms for
compressed sensing. Proceedings of the National Academy of Sciences  106(45):18914–18919 
2009.

[17] C Fienup and J Dainty. Phase retrieval and image reconstruction for astronomy. Image Recovery:

Theory and Application  pages 231–275  1987.

[18] James R Fienup. Phase retrieval algorithms: a comparison. Applied optics  21(15):2758–2769 

1982.

10

[19] Tom Goldstein and Christoph Studer. Phasemax: Convex phase retrieval via basis pursuit. IEEE

Transactions on Information Theory  2018.

[20] Paul Hand and Vladislav Voroninski. Compressed sensing from phaseless gaussian measure-
ments via linear programming in the natural parameter space. arXiv preprint arXiv:1611.05985 
2016.

[21] Paul Hand and Vladislav Voroninski. An elementary proof of convex phase retrieval in the
natural parameter space via the linear program phasemax. arXiv preprint arXiv:1611.03935 
2016.

[22] Kishore Jaganathan  Yonina C Eldar  and Babak Hassibi. Phase retrieval: An overview of recent

developments. arXiv preprint arXiv:1510.07713  2015.

[23] Kishore Jaganathan  Samet Oymak  and Babak Hassibi. Sparse phase retrieval: Uniqueness
guarantees and recovery algorithms. IEEE Transactions on Signal Processing  65(9):2402–2410 
2017.

[24] Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements
via convex programming. SIAM Journal on Mathematical Analysis  45(5):3019–3033  2013.

[25] Yue M Lu and Gen Li. Phase transitions of spectral initialization for high-dimensional nonconvex

estimation. arXiv preprint arXiv:1702.06435  2017.

[26] Junjie Ma  Ji Xu  and Arian Maleki. Optimization-based amp for phase retrieval: The impact of

initialization and (cid:96)2-regularization. arXiv preprint arXiv:1801.01170  2018.

[27] Rick P Millane. Phase retrieval in crystallography and optics. JOSA A  7(3):394–411  1990.

[28] Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with applications

to phase retrieval. arXiv preprint arXiv:1708.05932  2017.

[29] Praneeth Netrapalli  Prateek Jain  and Sujay Sanghavi. Phase retrieval using alternating mini-

mization. In Advances in Neural Information Processing Systems  pages 2796–2804  2013.

[30] Samet Oymak  Amin Jalali  Maryam Fazel  Yonina C Eldar  and Babak Hassibi. Simultaneously
structured models with application to sparse and low-rank matrices. IEEE Transactions on
Information Theory  61(5):2886–2908  2015.

[31] Samet Oymak  Christos Thrampoulidis  and Babak Hassibi. The squared-error of generalized

lasso: A precise analysis. arXiv preprint arXiv:1311.0830  2013.

[32] Ralph Tyrell Rockafellar. Convex analysis. Princeton university press  2015.

[33] Mark Rudelson and Roman Vershynin. Sparse reconstruction by convex relaxation: Fourier and
gaussian measurements. In Information Sciences and Systems  2006 40th Annual Conference
on  pages 207–212. IEEE  2006.

[34] Walter Rudin et al. Principles of mathematical analysis  volume 3. McGraw-hill New York 

1976.

[35] Fariborz Salehi  Ehsan Abbasi  and Babak Hassibi. A precise analysis of phasemax in phase
retrieval. In Information Theory (ISIT)  2018 IEEE International Symposium on  pages 976–980.
IEEE  2018.

[36] Fariborz Salehi  Kishore Jaganathan  and Babak Hassibi. Multiple illumination phaseless
super-resolution (mips) with applications to phaseless doa estimation and diffraction imaging.
In Acoustics  Speech and Signal Processing (ICASSP)  2017 IEEE International Conference on 
pages 3949–3953. IEEE  2017.

[37] Mahdi Soltanolkotabi. Structured signal recovery from quadratic measurements: Breaking
sample complexity barriers via nonconvex optimization. arXiv preprint arXiv:1702.06175 
2017.

[38] Mihailo Stojnic. Various thresholds for l1-optimization in compressed sensing. 2009.

11

[39] Christos Thrampoulidis  Ehsan Abbasi  and Babak Hassibi. Precise error analysis of regularized

m-estimators in high-dimensions. IEEE Transactions on Information Theory  2018.

[40] Christos Thrampoulidis  Samet Oymak  and Babak Hassibi. Regularized linear regression: A
precise analysis of the estimation error. In Conference on Learning Theory  pages 1683–1709 
2015.

[41] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 267–288  1996.

[42] Joel A Tropp. Convex recovery of a structured signal from independent random linear measure-

ments. In Sampling Theory  a Renaissance  pages 67–101. Springer  2015.

[43] Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via orthogonal

matching pursuit. IEEE Transactions on information theory  53(12):4655–4666  2007.

[44] Roman Vershynin. High-dimensional probability. An Introduction with Applications  2016.

[45] Adriaan Walther. The question of phase retrieval in optics. Journal of Modern Optics  10(1):41–

49  1963.

[46] Gang Wang  Georgios B Giannakis  and Yonina C Eldar. Solving systems of random quadratic
equations via truncated amplitude ﬂow. IEEE Transactions on Information Theory  64(2):773–
794  2018.

[47] Gang Wang  Liang Zhang  Georgios B Giannakis  Mehmet Akcakaya  and Jie Chen. Sparse
phase retrieval via truncated amplitude ﬂow. IEEE Transactions on Signal Processing  66(2):479–
491  2016.

[48] Shanshan Wu  Alexandros G Dimakis  Sujay Sanghavi  Felix X Yu  Daniel Holtmann-Rice 
Dmitry Storcheus  Afshin Rostamizadeh  and Sanjiv Kumar. The sparse recovery autoencoder.
arXiv preprint arXiv:1806.10175  2018.

[49] Teng Zhang. Phase retrieval using alternating minimization in a batch setting. arXiv preprint

arXiv:1706.08167  2017.

12

,Fariborz Salehi
Ehsan Abbasi
Babak Hassibi
Steve Hanneke
Samory Kpotufe