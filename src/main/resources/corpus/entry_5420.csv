2015,Testing Closeness With Unequal Sized Samples,We consider the problem of testing whether two unequal-sized samples were drawn from identical distributions  versus distributions that differ significantly.  Specifically  given a target error parameter $\eps > 0$   $m_1$ independent draws from an unknown distribution $p$ with discrete support  and $m_2$ draws from an unknown distribution $q$ of discrete support  we describe a test for distinguishing the case that $p=q$ from the case that $||p-q||_1 \geq \eps$. If $p$ and $q$ are supported on at most $n$ elements  then our test is successful with high probability provided $m_1\geq n^{2/3}/\varepsilon^{4/3}$ and $m_2 = \Omega\left(\max\{\frac{n}{\sqrt m_1\varepsilon^2}  \frac{\sqrt n}{\varepsilon^2}\}\right).$ We show that this tradeoff is information theoretically optimal throughout this range  in the dependencies on all parameters  $n m_1 $ and $\eps$  to constant factors. As a consequence  we obtain an algorithm for estimating the mixing time of a Markov chain on $n$ states up to a $\log n$ factor that uses $\tilde{O}(n^{3/2} \tau_{mix})$ queries to a ``next node'' oracle. The core of our testing algorithm is a relatively simple statistic that seems to perform well in practice  both on synthetic data and on natural language data.  We believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems.,Testing Closeness With Unequal Sized Samples

Bhaswar B. Bhattacharya
Department of Statistics

Stanford University
California  CA 94305

bhaswar@stanford.edu

Gregory Valiant∗

Department of Computer Science

Stanford University
California  CA 94305

valiant@stanford.edu

Abstract

(cid:16)

ε2 }(cid:17)

We consider the problem of testing whether two unequal-sized samples were
drawn from identical distributions  versus distributions that differ signiﬁcantly.
Speciﬁcally  given a target error parameter ε > 0  m1 independent draws from
an unknown distribution p with discrete support  and m2 draws from an unknown
distribution q of discrete support  we describe a test for distinguishing the case that
p = q from the case that ||p − q||1 ≥ ε. If p and q are supported on at most n ele-
ments  then our test is successful with high probability provided m1 ≥ n2/3/ε4/3
and m2 = Ω
. We show that this tradeoff is information the-
oretically optimal throughout this range in the dependencies on all parameters 
n  m1  and ε  to constant factors for worst-case distributions. As a consequence 
we obtain an algorithm for estimating the mixing time of a Markov chain on n
states up to a log n factor that uses ˜O(n3/2τmix) queries to a “next node” ora-
cle. The core of our testing algorithm is a relatively simple statistic that seems to
perform well in practice  both on synthetic and on natural language data. We be-
lieve that this statistic might prove to be a useful primitive within larger machine
learning and natural language processing systems.

n√
m1ε2  

max{

√

n

Introduction

1
One of the most basic problems in statistical hypothesis testing is the question of distinguishing
whether two unknown distributions are very similar  or signiﬁcantly different. Classical tests  like
the Chi-squared test or the Kolmogorov-Smirnov statistic  are optimal in the asymptotic regime 
for ﬁxed distributions as the sample sizes tend towards inﬁnity. Nevertheless  in many modern
settings—such as the analysis of customer  web logs  natural language processing  and genomics 
despite the quantity of available data—the support sizes and complexity of the underlying distribu-
tions are far larger than the datasets  as evidenced by the fact that many phenomena are observed
only a single time in the datasets  and the empirical distributions of the samples are poor represen-
tations of the true underlying distributions.1 In such settings  we must understand these statistical
tasks not only in the asymptotic regime (in which the amount of available data goes to inﬁnity)  but
in the “undersampled” regime in which the dataset is signiﬁcantly smaller than the size or complex-
ity of the distribution in question. Surprisingly  despite an intense history of study by the statistics 
information theory  and computer science communities  aspects of basic hypothesis testing and esti-
mation questions–especially in the undersampled regime—remain unresolved  and require both new
algorithms  and new analysis techniques.

∗Supported in part by NSF CAREER Award CCF-1351108
1To give some speciﬁc examples  two recent independent studies [19  26] each considered the genetic se-
quences of over 14 000 individuals  and found that rare variants are extremely abundant  with over 80% of
mutations observed just once in the sample. A separate recent paper [16] found that the discrepancy in rare mu-
tation abundance cited in different demographic modeling studies can largely be explained by discrepancies in
the sample sizes of the respective studies  as opposed to differences in the actual distributions of rare mutations
across demographics  highlighting the importance of improved statistical tests in this “undersampled” regime.

1

In this work  we examine the basic hypothesis testing question of deciding whether two unknown
distributions over discrete supports are identical (or extremely similar)  versus have total variation
distance at least ε  for some speciﬁed parameter ε > 0. We consider (and largely resolve) this
question in the extremely practically relevant setting of unequal sample sizes. Informally  taking
ε to be a small constant  we show that provided p and q are supported on at most n elements  for
any γ ∈ [0  1/3]  the hypothesis test can be successfully performed (with high probability over the
random samples) given samples of size m1 = Θ(n2/3+γ) from p  and m2 = Θ(n2/3−γ/2) from
q  where n is the size of the supports of the distributions p and q. Furthermore  for every γ in
this range  this tradeoff between m1 and m2 is necessary  up to constant factors. Thus  our results
√
smoothly interpolate between the known bounds of Θ(n2/3) on the sample size necessary in the
setting where one is given two equal-sized samples [6  9]  and the bound of Θ(
n) on the sample
size in the setting in which the sample is drawn from one distribution and the other distribution is
known to the algorithm [22  29]. Throughout most of the regime of parameters  when m1 (cid:28) m2
2 
our algorithm is a natural extension of the algorithm proposed in [9]  and is similar to the algorithm
information theoretic optimality. In the extreme regime when m1 ≈ n and m2 ≈ √
proposed in [3] except with the addition of a normalization term that seems crucial to obtaining our
n  our algorithm
introduces an additional statistic which (we believe) is new. Our algorithm is relatively simple  and
practically viable. In Section 4 we illustrate the efﬁcacy of our approach on both synthetic data  and
on the real-world problem of deducing whether two words are synonyms  based on a small sample
of the bi-grams in which they occur.
We also note that  as pointed out in several related work [3  12  6]  this hypothesis testing question
has applications to other problems  such as estimating or testing the mixing time of Markov chains 
and our results yield improved algorithms in these settings.

1.1 Related Work

The general question of how to estimate or test properties of distributions using fewer samples
than would be necessary to actually learn the distribution  has been studied extensively since the
late ’90s. Most of the work has focussed on “symmetric” properties (properties whose value is
invariant to relabeling domain elements) such as entropy  support size  and distance metrics between
distributions (such as (cid:96)1 distance). This has included both algorithmic work (e.g. [4  5  7  8  10  13 
20  21  27  28  29])  and results on developing techniques and tools for establishing lower bounds
(e.g.
[23  30  27]). See the recent survey by Rubinfeld for a more thorough summary of the
developments in this area [24]).
The speciﬁc problem of “closeness testing” or “identity testing”  that is  deciding whether two dis-
tributions  p and q  are similar  versus have signiﬁcant distance  has two main variants: the one-
unknown-distribution setting in which q is known and a sample is drawn from p  and the two-
unknown-distributions settings in which both p and q are unknown and samples are drawn from
both. We brieﬂy summarize the previous results for these two settings.
In the one-unknown-distribution setting (which can be thought of as the limiting setting in the case
that we have an arbitrarily large sample drawn from distribution q  and a relatively modest sized
sample from p)  initial work of Goldreich and Ron [12] considered the problem of testing whether
p is the uniform distribution over [n]  versus has distance at least ε. The tight bounds of Θ(
n/ ε2)
were later shown by Paninski [22]  essentially leveraging the birthday paradox and the intuition
that  among distributions supported on n elements  the uniform distribution maximizes the number
of domain elements that will be observed once. Batu et al. [8] showed that  up to polylogarithmic
factors of n  and polynomial factors of ε  this dependence was optimal for worst-case distributions
over [n]. Recently  an “instance–optimal” algorithm and matching lower bound was shown: for any
ε   ε−2||q− max−Θ(ε)||2/3} samples from p are both necessary
distribution q  up to constant factors  max{ 1
and sufﬁcient to test p = q versus ||p − q|| ≥ ε  where ||q− max−Θ(ε)||2/3 ≤ ||q||2/3 is the 2/3-rd norm
of the vector of probabilities of distribution q after the maximum element has been removed  and
the smallest elements up to Θ(ε) total mass have been removed. (This immediately implies the tight
n/ ε2) samples are sufﬁcient to test its
bounds that if q is any distribution supported on [n]  O(
identity.)
The two-unknown-distribution setting was introduced to this community by Batu et al. [6]. The
optimal sample complexity of this problem was recently determined by Chan et al. [9]: they showed

√

√

2

n

√

ε3

m1

 

ε2

√

n log n

that m = Θ(n2/3/ε4/3) samples are necessary and sufﬁcient. In a slightly different vein  Acharya et
al. [1  2] recently considered the question of closeness testing with two unknown distributions from
the standpoint of competitive analysis. They proposed an algorithm that performs the desired task
using O(s3/2 polylog s) samples  and established a lower bound of Ω(s7/6)  where s represents the
number of samples required to determine whether a set of samples were drawn from p versus q  in
the setting where p and q are explicitly known.
A natural generalization of this hypothesis testing problem  which interpolates between the two-
unknown-distribution setting and the one-unknown-distribution setting  is to consider unequal sized
samples from the two distributions. More formally  given m1 samples from the distribution p  the
asymmetric closeness testing problem is to determine how many samples  m2  are required from the
distribution q such that the hypothesis p = q versus ||p − q||1 > ε can be distinguished with large
constant probability (say 2/3). Note that the results of Chan et al. [9] imply that it is sufﬁcient to
consider m1 ≥ Θ(n2/3/ε4/3). This problem was studied recently by Acharya et al. [3]: they gave
an algorithm that given m1 samples from the distribution p uses m2 = O(max{ n log n
})
samples from q  to distinguish the two distributions with high probability. They also proved a lower
bound of m2 = Ω(max{√
}). There is a polynomial gap in these upper and lower bounds
ε2   n2
√
ε4m2
1
m1 and ε.
in the dependence on n 
As a corollary to our main hypothesis testing result  we obtain an improved algorithm for testing
the mixing time of a Markov chain. The idea of testing mixing properties of a Markov chain goes
back to the work of Goldreich and Ron [12]  which conjectured an algorithm for testing expansion
of bounded-degree graphs. Their test is based on picking a random node and testing whether ran-
√
dom walks from this node reach a distribution that is close to the uniform distribution on the nodes
n) query complexity. Later  Czumaj
of the graph. They conjectured that their algorithm had O(
and Sohler [11]  Kale and Seshadhri [15]  and Nachmias and Shapira [18] have independently con-
cluded that the algorithm of Goldreich and Ron is provably a test for expansion property of graphs.
Rapid mixing of a chain can also be tested using eigenvalue computations. Mixing is related to the
separation between the two largest eigenvalues [25  17]  and eigenvalues of a dense n × n matrix
can be approximated in O(n3) time and O(n2) space. However  for a sparse n × n symmetric
matrix with m nonzero entries  the same task can be achieved in O(n(m + log n)) operations and
O(n + m) space. Batu et al. [6] used their (cid:96)1 distance test on the t-step distributions  to test mixing
properties of Markov chains. Given a ﬁnite Markov chain with state space [n] and transition matrix
PPP = ((P (x  y)))  they essentially show that one can estimate the mixing time τmix up to a factor
of log n using ˜O(n5/3τmix) queries to a next node oracle  which takes a state x ∈ [n] and outputs a
state y ∈ [n] drawn from the distribution P (x ·). Such an oracle can often be simulated signiﬁcantly
more easily than actually computing the transition matrix P (x  y).
We conclude this related work section with a comment on “robust” hypothesis testing and distance
estimation. A natural hope would be to simply estimate ||p − q|| to within some additive ε  which is
a strictly more difﬁcult task than distinguishing p = q from ||p − q|| ≥ ε. The results of Valiant and
Valiant [27  28  29] show that this problem is signiﬁcantly more difﬁcult than hypothesis testing:
the distance can be estimated to additive error ε for distributions supported on ≤ n elements using
samples of size O(n/ log n) (in both the setting where either one  or both distributions are unknown).
Moreover  Ω(n/ log n) samples are information theoretically necessary  even if q is the uniform
distribution over [n]  and one wants to distinguish the case that ||p − q||1 ≤ 1
10 from the case that
10 . Recall that the non-robust test of distinguishing p = q versus ||p − q|| > 9/10
||p − q||1 ≥ 9
n). The exact worst-case sample complexity of distinguishing
requires a sample of size only O(
whether ||p − q||1 ≤ 1
nc versus ||p − q||1 ≥ ε is not well understood  though in the case of constant
ε  up to logarithmic factors  the required sample size seems to scale linearly in the exponent between
n2/3 and n as c goes from 1/3 to 0.

√

1.2 Our results

Our main result resolves the minimax sample complexity of the closeness testing problem in the
unequal sample setting  to constant factors  in terms of n  the support sizes of the distributions in
question:

3

Theorem 1. Given m1 ≥ n2/3/ε4/3 and ε > n−1/12  and sample access to distributions p and q
over [n]  there is an O(m1) time algorithm which takes m1 independent draws from p and m2 =
√
O(max{
ε2 }) independent draws from q  and with probability at least 2/3 distinguishes
whether

n√
m1ε2  

n

||p − q||1 ≤ O

||p − q||1 ≥ ε.
(1)
√
ε2 }) samples from q are information-
Moreover  given m1 samples from p  Ω(max{
theoretically necessary to distinguish p = q from ||p − q||1 ≥ ε with any constant probability
bounded below by 1/2.

n√
m1ε2  

versus

n

(cid:18) 1

(cid:19)

m2

√
The lower bound in the above theorem is proved using the machinery developed in Valiant [30] 
and “interpolates” between the Θ(
n/ ε2) lower bound in the one-unknown-distribution setting of
testing uniformity [22] and the Θ(n2/3/ ε4/3) lower bound in the setting of equal sample sizes from
two unknown distributions [9]. The algorithm establishing the upper bound involves a re-weighted
version of a statistic proposed in [9]  and is similar to the algorithm proposed in [3] modulo the
addition of a normalizing term  which seems crucial to obtaining our tight results. In the extreme
n/ ε2  we incorporate an additional statistic that has not appeared

regime when m1 ≈ n and m2 ≈ √
before in the literature.
As an application of Theorem 1 in the extreme regime when m1 ≈ n  we obtain an improved
algorithm for estimating the mixing time of a Markov chain:
Corollary 1. Consider a ﬁnite Markov chain with state space [n] and a next node oracle; there is
an algorithm that estimates the mixing time  τmix  up to a multiplicative factor of log n  that uses
˜O(n3/2τmix) time and queries to the next node oracle.

Concurrently to our work  Hsu et al. [14] considered the question of estimating the mixing time
based on a single sample path (as opposed to our model of a sampling oracle). In contrast to our
approach via hypothesis testing  they considered the natural spectral approach  and showed that the
mixing time can be approximated  up to logarithmic factors  given a path of length ˜O(τ 3
mix/πmin) 
where πmin is the minimum probability of a state under the stationary distribution. Hence  if the
stationary distribution is uniform over n states  this becomes ˜O(nτ 3
mix). It remains an intriguing
open question whether one can simultaneously achieve both the linear dependence on τmix of our
results and the linear dependence on 1/πmin or the size of the state space  n  as in their results.

1.3 Outline

We begin by stating our testing algorithm  and describe the intuition behind the algorithm. The
formal proof of the performance guarantees of the algorithm require rather involved bounds on the
moments of various parameters  and are provided in the supplementary material. We also defer
the entirety of the matching information theoretic lower bounds to the supplementary material  as
the techniques may not appeal to as wide an audience as the algorithmic portion of our work. The
application of our testing results to the problem of testing or estimating the mixing time of a Markov
chain is discussed in Section 3. Finally  Section 4 contains some empirical results  suggesting that
the statistic at the core of our testing algorithm performs very well in practice. This section contains
both results on synthetic data  as well as an illustration of how to apply these ideas to the problem
of estimating the semantic similarity of two words based on samples of the n-grams that contain the
words in a corpus of text.

2 Algorithms for (cid:96)1 Testing

In this section we describe our algorithm for (cid:96)1 testing with unequal samples. This gives the upper
bound in Theorem 1 on the sample sizes necessary to distinguish p = q from ||p − q||1 ≥ ε. For
clarity and ease of exposition  in this section we consider ε to be some absolute constant  and supress
the dependency on ε . The slightly more involved algorithm that also obtains the optimal dependency
on the parameter ε is given in the supplementary material.
We begin by presenting the algorithm  and then discuss the intuition for the various steps.

4

Algorithm 1 The Closeness Testing Algorithm
Suppose ε = Ω(1) and m1 = O(n1−γ) for some γ ≥ 0. Let S1  S2 denote two independent sets of
m1 samples drawn from p and let T1  T2 denote two independent sets of m2 samples drawn from q.
We wish to test p = q versus ||p − q||1 > ε.

• Let b = C0

log n
m2

  for an absolute constant C0  and deﬁne the set

B = {i ∈ [n] : X S1
occurrences of i in S1  and Y T1

i
m1

> b} ∪ {i ∈ [n] : Y T1

i
m2

• Let Xi denote the number of occurrences of element i in S2  and Yi denote the number of

i

denotes the number of occurrences of i in T1.

> b}  where X S1

i denotes the number of

occurrences of element i in T2:

(cid:12)(cid:12)(cid:12)(cid:12) Xi

m1

(cid:88)

i∈B

(cid:12)(cid:12)(cid:12)(cid:12) ≤ ε/6.

− Yi
m2

(m2Xi − m1Yi)2 − (m2
Xi + Yi

2Xi + m2

1Yi)

≤ Cγm3/2

1 m2 

1. Check if

2. Check if

(cid:88)

i∈[n]\B

Z :=

3. If γ ≥ 1/9:

(2)

(3)

(4)

for an appropriately chosen constant Cγ (depending on γ).

• If (2) and (3) hold  then ACCEPT. Otherwise  REJECT.

4. Otherwise  if γ < 1/9 :

• Check if

(cid:88)

i∈[n]\B

R :=

111{Yi = 2}
Xi + 1

≤ C1

m2
2
m1

 

where C1 is an appropriately chosen absolute constant.

• REJECT if there exists i ∈ [n] such that Yi ≥ 3 and Xi ≤ C2
• If (2)  (3)  and (4) hold  then ACCEPT. Otherwise  REJECT.

appropriately chosen absolute constant.

m1

m2n1/3   where C2 is an

P oisson(m1pi) and Yi ← P oisson(m2qi)  then E(cid:2)(m2Xi − m1Yi)2 − (m2

The intuition behind the above algorithm is as follows: with high probability  all elements in the
set B satisfy either pi > b/2  or qi > b/2 (or both). Given that these elements are “heavy”  their
contribution to the (cid:96)1 distance will be accurately captured by the (cid:96)1 distance of their empirical
frequencies (where these empirical frequencies are based on the second set of samples  S2  T2).
For the elements that are not in set B—the “light” elements—their empirical frequencies will 
in general  not accurately reﬂect their true probabilities  and hence the distance between the em-
pirical distributions of the “light” elements will be misleading. The Z statistic of Equation 3 is
designed speciﬁcally for this regime. If the denominator of this statistic were omitted  then this
would give an estimator for the squared (cid:96)2 distance between the distributions (scaled by a factor of
2). To see this  note that if pi and qi are small  then Binomial(m1  pi) ≈ P oisson(m1pi)
1m2
m2
and Binomial(m2  qi) ≈ P oisson(m2qi); furthermore  a simple calculation yields that if Xi ←
2(p − q)2. The normalization by Xi + Yi “linearizes” the Z statistic  essentially turning the
m2
squared (cid:96)2 distance into an estimate of the (cid:96)1 distance between light elements of the two distri-
butions. Similar results can possibly be obtained using other linear functions of Xi and Yi in the
denominator  though we note that the “obvious” normalizing factor of Xi + m1
Yi does not seem to
m2
work theoretically  and seems to have extremely poor performance in practice.
For the extreme case (corresponding to γ < 1/9) where m1 ≈ n and m2 ≈ √
sample of size m2 ≈ √

n/ ε2  the statistic
Z might have a prohibitively large variance; this is essentially due to the “birthday paradox” which
might cause a constant number of rare elements (having probability O(1/n) to occur twice in a
1) ≈ n2 to the Z statistic 

n/ ε2). Each such element will contribute Ω(m2

1Yi)(cid:3) =

2Xi + m2

1m2

5

and hence the variance can be ≈ n4. The statistic R of Equation (4) is tailored to deal with these
cases  and captures the intuition that we are more tolerant of indices i for which Yi = 2 if the
corresponding Xi is larger. It is worth noting that one can also deﬁne a natural analog of the R
statistic corresponding to the indices i for which Yi = 3  etc.  using which the robustness parameter
of the test can be improved. The ﬁnal check—ensuring that in this regime with m1 (cid:29) m2 there are
no elements for which Yi ≥ 3 but Xi is small—rules out the remaining sets of distributions  p  q  for
which the variance of the Z statistic is intolerably large.
Finally  we should emphasize that the crude step of using two independent batches of samples—
the ﬁrst to obtain the partition of the domain into “heavy” and “light” elements  and the second to
actually compute the statistics  is for ease of analysis. As our empirical results of Section 4 suggest 
for practical applications one may want to use only the Z-statistic of (3)  and one certainly should
not “waste” half the samples to perform the “heavy”/“light” partition.

3 Estimating Mixing Times in Markov Chains

The basic hypothesis testing question of distinguishing identical distributions from those with sig-
niﬁcant (cid:96)1 distance can be employed for several other practically relevant tasks. One example is the
problem of estimating the mixing time of Markov chains.
Consider a ﬁnite Markov chain with state space [n]  transition matrix PPP = ((P (x  y)))  with sta-
x(·) is the probability
tionary distribution π. The t-step distribution starting at the point x ∈ [n]  P t
distribution on [n] obtained by running the chain for t steps starting from x.
Deﬁnition 1. The ε-mixing time of a Markov chain with transition matrix PPP = ((P (x  y))) is deﬁned
as tmix(ε) := inf

(cid:110)
t ∈ [n] : supx∈[n]

(cid:80)
y∈[n] |P t

x(y) − π(y)| ≤ ε

(cid:111)

.

1
2

t

Deﬁnition 2. The average t-step distribution of a Markov chain PPP with n states is the distribution
x  that is  the distribution obtained by choosing x uniformly from [n] and walking
P
t steps from the state x.

x∈[n] P t

= 1
n

(cid:80)

The connection between closeness testing and testing whether a Markov chain is close to mixing
was ﬁrst observed by Batu et al. [6]  who proposed testing the (cid:96)1 difference between distributions
t0  for every x ∈ [n]. The algorithm leveraged their equal sample-size hypothesis testing
x and P
P t0
t0. This yields an
results  drawing ˜O(n2/3 log n) samples from both the distributions P t0
overall running time of ˜O(n5/3t0).
Here  we note that our unequal sample-size hypothesis testing algorithm can yield an improved
t0 is independent of the starting state x  it sufﬁces to take ˜O(n)
runtime. Since the distribution P
√
x  for every x ∈ [n]. This results in a query and
samples from P
n) samples from P t
runtime complexity of ˜O(n3/2t0). We sketch this algorithm below.

t0 once and ˜O(

x and P

Algorithm 2 Testing for Mixing Times in Markov Chains
Given t0 ∈ R and a ﬁnite Markov chain with state space [n] and transition matrix PPP = ((P (x  y))) 
we wish to test

H0 : tmix

O

≤ t0 

versus H1 : tmix (1/4) > t0.

(5)

(cid:18)

(cid:19)(cid:19)

(cid:18) 1√

n

distribution.

1. Draw O(log n) samples S1  . . .   SO(log n)  each of size Pois(C1n) from the average t0-step
2. For each state x ∈ [n] we will distinguish whether ||P t0

t0||1 ≤ O( 1√
||P t0
x − P
√
runs of Algorithm 1  with the i-th run using Si and a fresh set of Pois(O(
from P t
x.

n )  versus
t0||1 > 1/4  with probability of error (cid:28) 1/n. We do this by running O(log n)
n)) samples

x − P

3. If all n of the (cid:96)1 closeness testing problems are accepted  then we ACCEPT H0.

6

The above testing algorithm can be leveraged to estimate the mixing time of a Markov chain  via the
n) ≤
basic observation that if tmix(1/4) ≤ t0  then for any ε  tmix(ε) ≤ log ε
√
2 log n · tmix(1/4). Because tmix(1/4) and tmix(O(1/
n)) differ by at most a factor of log n 
by applying Algorithm 2 for a geometrically increasing sequence of t0’s  and repeating each test
O(log t0 + log n) times  one obtains Corollary 1  restated below:
Corollary 1 For a ﬁnite Markov chain with state space [n] and a next node oracle  there is an
algorithm that estimates the mixing time  τmix  up to a multiplicative factor of log n  that uses
˜O(n3/2τmix) time and queries to the next node oracle.

√
log 1/2 t0  and thus tmix(1/

4 Empirical Results
Both our formal algorithms and the corresponding theorems involve some unwieldy constant factors
(that can likely be reduced signiﬁcantly). Nevertheless  in this section we provide some evidence
that the statistic at the core of our algorithm can be fruitfully used in practice  even for surprisingly
small sample sizes.

(cid:80)

i

m3/2

1 m2(Xi+Yi)

2Xi+m2

1Yi)

(m2Xi−m1Yi)2−(m2

4.1 Testing similarity of words
An extremely important primitive in natural
mate the semantic similarity of two words. Here  we show that

language processing is the ability to esti-
the Z statistic  Z =
  which is the core of our testing algorithm  can accurately dis-
tinguish whether two words are very similar based on surprisingly small samples of the contexts in
which they occur. Speciﬁcally  for each pair of words  a  b that we consider  we select m1 random
occurrences of a and m2 random occurrences of word b from the Google books corpus  using the
Google Books Ngram Dataset.2 We then compare the sample of words that follow a with the sample
of words that follow b. Henceforth  we refer to these as samples of the set of bi-grams involving
each word.
Figure 1(a) illustrates the Z statistic for various pairs of words that range from rather similar words
like “smart” and “intelligent”  to essentially identical word pairs such as “grey” and “gray” (whose
usage differs mainly as a result of historical variation in the preference for one spelling over the
other); the sample size of bi-grams containing the ﬁrst word is ﬁxed at m1 = 1  000  and the sample
size corresponding to the second word varies from m2 = 50 through m2 = 1  000. To provide a
frame of reference  we also compute the value of the statistic for independent samples corresponding
to the same word (i.e. two different samples of words that follow “wolf”); these are depicted in red.
For comparison  we also plot the total variation distance between the empirical distributions of
the pair of samples  which does not clearly differentiate between pairs of identical words  versus
different words  particularly for the smaller sample sizes.
One subtle point is that the issue with using the empirical distance between the distributions goes
beyond simply not having a consistent reference point. For example  let X denote a large sample
of size m1 from distribution p  X(cid:48) denote a small sample of size m2 from p  and Y denote a
small sample of size m2 from a different distribution q. It is tempting to hope that the empirical
distance between X and X(cid:48) will be smaller than the empirical distance between X and Y . As
Figure 1(b) illustrates  this is not always the case  even for natural distributions: for the speciﬁc
example illustrated in the ﬁgure  over much of the range of m2  the empirical distance between X
and X(cid:48) is indistinguishable from that of X and Y   though the Z statistic easily discerns that these
distributions are very different.
This point is further emphasized in Figure 2  which depicts this phenomena in the synthetic setting
where p = Unif[n] is the uniform distribution over n elements  and q is the distribution whose
elements have probabilities (1 ± ε)/n  for ε = 1/2. The second and fourth plots represent the
probability that the distance between two empirical distributions of samples from p is smaller than
the distance between the empirical distributions of the samples from p and q; the ﬁrst and third
plots represent the analogous probability involving the Z statistic. The ﬁrst two plots correspond to
n = 1  000 and the last two correspond to n = 50  000. In all plots  we consider a pair of samples
of respective sizes m1 and m2  as m1 and m2 range between

√

n and n.

2The Google Books Ngram Dataset is freely available here: http://storage.googleapis.com/

books/ngrams/books/datasetsv2.html

7

Figure 1: (a) Two measures of the similarity between words  based on samples of the bi-grams
containing each word. Each line represents a pair of words  and is obtained by taking a sample of
m1 = 1  000 bi-grams containing the ﬁrst word  and m2 = 50  . . .   1  000 bi-grams containing the
second word  where m2 is depicted along the x-axis in logarithmic scale. In both plots  the red lines
represent pairs of identical words (e.g. “wolf/wolf” “almost/almost” . . . ). The blue lines represent
pairs of similar words (e.g. “wolf/fox”  “almost/nearly” . . . )  and the black line represents the pair
“grey/gray” whose distribution of bi-grams differ because of historical variations in preference for
each spelling. Solid lines indicate the average over 200 trials for each word pair and choice of m2 
with error bars of one standard deviation depicted. The left plot depicts our statistic  which clearly
distinguishes identical words  and demonstrates some intuitive sense of semantic distance. The
right plot depicts the total variation distance between the empirical distributions—which does not
successfully distinguish the identical words  given the range of sample sizes considered. The plot
would not be signiﬁcantly different if other distance metrics between the empirical distributions 
such as f-divergence  were used in place of total variation distance. Finally  note the extremely
uniform magnitudes of the error bars in the left plot  as m2 increases  which is an added beneﬁt
of the Xi + Yi normalization term in the Z statistic. (b) Illustration of how the empirical distance
can be misleading: here  the empirical distance between the distributions of samples of bi-grams for
“wolf/wolf” is indistinguishable from that for the pair “wolf/fox*” over much of the range of m2;
nevertheless  our statistic clearly discerns that these are signiﬁcantly different distributions. Here 
“fox*” denotes the distribution of bi-grams whose ﬁrst word is “fox”  restricted to only the most
common 100 bi-grams.

Figure 2: The ﬁrst and third plot depicts the probability that the Z statistic applied to samples of
sizes m1  m2 drawn from p = U nif [n] is smaller than the Z statistic applied to a sample of size m1
drawn from p and m2 drawn from q  where q is a perturbed version of p in which all elements have
probability (1 ± 1/2)/n. The second and fourth plots depict the probability that empirical distance
between a pair of samples (of respective sizes m1  m2) drawn from p is less than the empirical
distribution between a sample of size m1 drawn from p and m2 drawn from q. The ﬁrst two plots
√
correspond to n = 1  000 and the last two correspond to n = 50  000. In all plots  m1 and m2 range
between
n and n on a logarithmic scale. In all plots the colors depict the average probability based
on 100 trials.

8

!"#!"#!"#$%$&'()*$+ '-&.)/0%)!)+ '1+1&)##$%&'#'%()#2$"$('%$ 3)4. 5..-)6'$%+)78)97%:+)$%&'#$%&'#$%&'#$%&'#$%&'#'%()#!"#!"#!"#$%$&'()*$+ '-&.)/0%)!)+ '1+1&)$%&'#$%('#)*+ # *-#./0#.1*2#(+!*30#4&(%+'#####5%&6#3!(%0#2$"$('%$ 3)4. 5..-)6'$%+)78)97%:+)740&++7$&40#3!(%0#(a) 	  	  	  	  	  	  102	  	  	  	  	  	  	  	  	  103	  	  	  	  	  	  	  102	  	  	  	  	  	  	  	  	  	  103	  m2 m2 !"#!"#!"#$%$&'()*$+ '-&.)/0%)!)+ '1+1&)##$%&'#'%()#2$"$('%$ 3)4. 5..-)6'$%+)78)97%:+)$%&'#$%&'#$%&'#$%&'#$%&'#'%()#!"#!"#!"#$%$&'()*$+ '-&.)/0%)!)+ '1+1&)##$%&'#'%()#2$"$('%$ 3)4. 5..-)6'$%+)78)97%:+)$%&'#$%&'#$%&'#$%&'#$%&'#'%()#(b) 	  	  	  	  	  	  102	  	  	  	  	  	  	  	  	  	  103	  	  	  	  	  	  	  102	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  103	  m2 m2 Pr [ Z(pm1 qm2) > Z(pm1 pm2) ] n = 1 000m1m2n 0.5 n 0.75 n n 0.75 nm2Pr [ Z(pm1 qm2) > Z(pm1 pm2) ] n = 50 000m1n 0.5 n 0.75 n n 0.75 nPr [ || pm1 – qm2 || > || pm1 – pm2 || ] n = 1 000m1m2n 0.5 n 0.75 n n 0.75 nm1n 0.5 n 0.75 n n 0.75 nm2Pr [ || pm1 – qm2 || > || pm1 – pm2 || ] n = 50 0001 0.9 0.8 0.7 0.6 0.5References
[1] J. Acharya  H. Das  A. Jafarpour  A. Orlitsky  and S. Pan  Competitive closeness testing  COLT  2011.
[2] J. Acharya  H. Das  A. Jafarpour  A. Orlitsky  and S. Pan  Competitive classiﬁcation and closeness testing.

COLT  2012.

[3] J. Acharya  A. Jafarpour  A. Orlitsky  and A. T. Suresh  Sublinear algorithms for outlier detection and

generalized closeness testing  ISIT  3200–3204  2014.

[4] J. Acharya  C. Daskalakis  and G. Kamath  Optimal testing for properties of distributions  NIPS  2015.
[5] Z. Bar-Yossef  R. Kumar  and D. Sivakumar. Sampling algorithms: lower bounds and applications  STOC 

2001.

[6] T. Batu  L. Fortnow  R. Rubinfeld  W. D. Smith  and P. White  Testing that distributions are close  FOCS 

2000.

[7] T. Batu  S. Dasgupta  R. Kumar  and R. Rubinfeld  The complexity of approximating the entropy  SIAM

Journal on Computing  2005.

[8] T. Batu  E. Fischer  L. Fortnow  R. Kumar  R. Rubinfeld  and P. White  Testing random variables for

independence and identity  FOCS  2001.

[9] S.-on Chan  I. Diakonikolas  P. Valiant  G. Valiant  Optimal Algorithms for Testing Closeness of Discrete

Distributions  Symposium on Discrete Algorithms (SODA)  1193–1203  2014 

[10] M. Charikar  S. Chaudhuri  R. Motwani  and V.R. Narasayya  Towards estimation error guarantees for

distinct values  Symposium on Principles of Database Systems (PODS)  2000.

[11] A. Czumaj and C. Sohler  Testing expansion in bounded-degree graphs  FOCS  2007.
[12] O. Goldreich and D. Ron  On testing expansion in bounded-degree graphs  ECCC  TR00-020  2000.
[13] S. Guha  A. McGregor  and S. Venkatasubramanian  Streaming and sublinear approximation of entropy

and information distances  Symposium on Discrete Algorithms (SODA)  2006.

[14] D. Hsu  A. Kontorovich  and C. Szepesv´ari  Mixing time estimation in reversible Markov chains from a

single sample path  NIPS  2015.

[15] S. Kale and C. Seshadhri  An expansion tester for bounded degree graphs  ICALP  LNCS  Vol. 5125 

527–538  2008.

[16] A. Keinan and A. G. Clark. Recent explosive human population growth has resulted in an excess of rare

genetic variants. Science  336(6082):740743  2012.

[17] D. A. Levin  Y. Peres  and E. L. Wilmer  Markov Chains and Mixing Times  Amer. Math. Soc.  2009.
[18] A. Nachmias and A. Shapira  Testing the expansion of a graph  Electronic Colloquium on Computational

Complexity (ECCC)  Vol. 14 (118)  2007.

[19] M. R. Nelson and D. Wegmann et al.  An abundance of rare functional variants in 202 drug target genes

sequenced in 14 002 people. Science  337(6090):100104  2012.

[20] L. Paninski  Estimation of entropy and mutual information  Neural Comp.  Vol. 15 (6)  1191–1253  2003.
[21] L. Paninski  Estimating entropy on m bins given fewer than m samples  IEEE Transactions on Informa-

tion Theory  Vol. 50 (9)  2200–2203  2004.

[22] L. Paninski  A coincidence-based test for uniformity given very sparsely-sampled discrete data  IEEE

Transactions on Information Theory  Vol. 54  4750–4755  2008.

[23] S. Raskhodnikova  D. Ron  A. Shpilka  and A. Smith  Strong lower bounds for approximating distribution
support size and the distinct elements problem  SIAM Journal on Computing  Vol. 39(3)  813–842  2009.

[24] R. Rubinfeld  Taming big probability distributions  XRDS  Vol. 19(1)  24–28  2012.
[25] A. Sinclair and M. Jerrum  Approximate counting  uniform generation and rapidly mixing Markov chains 

Information and Computation  Vol. 82(1)  93–133  1989.

[26] J. A. Tennessen  A.W. Bigham  and T.D. O’Connor et al. Evolution and functional impact of rare coding

variation from deep sequencing of human exomes. Science  337(6090):6469  2012

[27] G. Valiant and P. Valiant  Estimating the unseen: an n/ log n-sample estimator for entropy and support

size  shown optimal via new CLTs  STOC  2011.

[28] G. Valiant and P. Valiant  Estimating the unseen: improved estimators for entropy and other properties 

NIPS  2013.

[29] G. Valiant and P. Valiant  An Automatic Inequality Prover and Instance Optimal Identity Testing  FOCS 

51–60  2014.

[30] P. Valiant  Testing symmetric properties of distributions  STOC  2008.
[31] P. Valiant  Testing Symmetric Properties of Distributions  PhD thesis  M.I.T.  2008.

9

,Brendan McMahan
Matthew Streeter
Bhaswar Bhattacharya
Gregory Valiant