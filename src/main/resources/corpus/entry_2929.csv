2007,Learning Horizontal Connections in a Sparse Coding Model of Natural Images,It has been shown that adapting a dictionary of basis functions to the statistics of natural images so as to maximize sparsity in the coefficients results in a set of dictionary elements whose spatial properties resemble those of V1 (primary visual cortex) receptive fields. However  the resulting sparse coefficients still exhibit pronounced statistical dependencies  thus violating the independence assumption of the sparse coding model. Here  we propose a model that attempts to capture the dependencies among the basis function coefficients by including a pairwise coupling term in the prior over the coefficient activity states. When adapted to the statistics of natural images  the coupling terms learn a combination of facilitatory and inhibitory interactions among neighboring basis functions. These learned interactions may offer an explanation for the function of horizontal connections in V1  and we discuss the implications of our findings for physiological experiments.,Learning Horizontal Connections in a Sparse Coding

Model of Natural Images

Pierre J. Garrigues
Department of EECS

Bruno A. Olshausen

Helen Wills Neuroscience Inst.

Redwood Center for Theoretical Neuroscience

School of Optometry

Univ. of California  Berkeley

Berkeley  CA 94720

garrigue@eecs.berkeley.edu

Redwood Center for Theoretical Neuroscience

Univ. of California  Berkeley

Berkeley  CA 94720

baolshausen@berkeley.edu

Abstract

It has been shown that adapting a dictionary of basis functions to the statistics
of natural images so as to maximize sparsity in the coefﬁcients results in a set
of dictionary elements whose spatial properties resemble those of V1 (primary vi-
sual cortex) receptive ﬁelds. However  the resulting sparse coefﬁcients still exhibit
pronounced statistical dependencies  thus violating the independence assumption
of the sparse coding model. Here  we propose a model that attempts to capture
the dependencies among the basis function coefﬁcients by including a pairwise
coupling term in the prior over the coefﬁcient activity states. When adapted to the
statistics of natural images  the coupling terms learn a combination of facilitatory
and inhibitory interactions among neighboring basis functions. These learned in-
teractions may offer an explanation for the function of horizontal connections in
V1 in terms of a prior over natural images.

1 Introduction

Over the last decade  mathematical explorations into the statistics of natural scenes have led to the
observation that these scenes  as complex and varied as they appear  have an underlying structure that
is sparse [1]. That is  one can learn a possibly overcomplete basis set such that only a small fraction
of the basis functions is necessary to describe a given image  where the operation to infer this sparse
representation is non-linear. This approach is known as sparse coding. Exploiting this structure has
led to advances in our understanding of how information is represented in the visual cortex  since
the learned basis set is a collection of oriented  Gabor-like ﬁlters that resemble the receptive ﬁelds
in primary visual cortex (V1). The approach of using sparse coding to infer sparse representations
of unlabeled data is useful for classiﬁcation as shown in the framework of self-taught learning [2].
Note that classiﬁcation performance relies on ﬁnding “hard-sparse” representations where a few
coefﬁcients are nonzero while all the others are exactly zero.

An assumption of the sparse coding model is that the coefﬁcients of the representation are indepen-
dent. However  in the case of natural images  this is not the case. For example  the coefﬁcients
corresponding to quadrature pair or colinear Gabor ﬁlters are not independent. This has been shown
and modeled in the early work of [3]  in the case of the responses of model complex cells [4] 
feedforward responses of wavelet coefﬁcients [5  6  7] or basis functions learned using indepen-
dent component analysis [8  9]. These dependencies are informative and exploiting them leads to
improvements in denoising performance [5  7].

We develop here a generative model of image patches that does not make the independence as-
sumption. The prior over the coefﬁcients is a mixture of a Gaussian when the corresponding basis

1

function is active  and a delta function centered at zero when it is silent as in [10]. We model the bi-
nary variables or “spins” that control the activation of the basis functions with an Ising model  whose
coupling weights model the dependencies among the coefﬁcients. The representations inferred by
this model are also “hard-sparse”  which is a desirable feature [2].

Our model is motivated in part by the architecture of the visual cortex  namely the extensive network
of horizontal connections among neurons in V1 [11]. It has been hypothesized that they facilitate
contour integration [12] and are involved in computing border ownership [13]. In both of these
models the connections are set a priori based on geometrical properties of the receptive ﬁelds. We
propose here to learn the connection weights in an unsupervised fashion. We hope with our model to
gain insight into the the computations performed by this extensive collateral system and compare our
ﬁndings to known physiological properties of these horizontal connections. Furthermore  a recent
trend in neuroscience is to model networks of neurons using Ising models  and it has been shown
to predict remarkably well the statistics of groups of neurons in the retina [14]. Our model gives a
prediction for what is expected if one ﬁts an Ising model to future multi-unit recordings in V1.

2 A non-factorial sparse coding model

Let x ∈ Rn be an image patch  where the xi’s are the pixel values. We propose the following
generative model:

x = Φa + ν =

m

Xi=1

aiϕi + ν 

where Φ = [ϕ1 . . . ϕm] ∈ Rn×m is an overcomplete transform or basis set  and the columns ϕi
are its basis functions. ν ∼ N (0  ǫ2In) is small Gaussian noise. Each coefﬁcient ai = si+1
2 ui is a
Gaussian scale mixture (GSM). We model the multiplier s with an Ising model  i.e. s ∈ {−1  1}m
2 sT W s+bT s  where Z is the normalization constant.
has a Boltzmann-Gibbs distribution p(s) = 1
If the spin si is down (si = −1)  then ai = 0 and the basis function ϕi is silent. If the spin si is up
(si = 1)  then the basis function is active and the analog value of the coefﬁcient ai is drawn from a
Gaussian distribution with ui ∼ N (0  σ2
i ). The prior on a can thus be described as a “hard-sparse”
prior as it is a mixture of a point mass at zero and a Gaussian.

Z e

1

The corresponding graphical model is shown in Figure 1.
It is a chain graph since it contains
both undirected and directed edges. It bears similarities to [15]  which however does not have the
intermediate layer a and is not a sparse coding model. To sample from this generative model  one
ﬁrst obtains a sample s from the Ising model  then samples coefﬁcients a according to p(a | s)  and
then x according to p(x | a) ∼ N (Φa  ǫ2In).

W1m

s1

s2

sm

W2m

a1

a2

am

Φ

x1

x2

xn

Figure 1: Proposed graphical model

The parameters of the model to be learned from data are θ = (Φ  (σ2
i )i=1..m  W  b). This model
does not make any assumption about which linear code Φ should be used  and about which units
should exhibit dependencies. The matrix W of the interaction weights in the Ising model describes
these dependencies. Wij > 0 favors positive correlations and thus corresponds to an excitatory
connection  whereas Wij < 0 corresponds to an inhibitory connection. A local magnetic ﬁeld
bi < 0 favors the spin si to be down  which in turn makes the basis function ϕi mostly silent.

2

3 Inference and learning

3.1 Coefﬁcient estimation

We describe here how to infer the representation a of an image patch x in our model. To do so  we
ﬁrst compute the maximum a posteriori (MAP) multiplier s (see Section 3.2). Indeed  a GSM model
reduces to a linear-Gaussian model conditioned on the multiplier s  and therefore the estimation of
a is easy once s is known.
Given s = ˆs  let Γ = {i : ˆsi = 1} be the set of active basis functions. We know that ∀i /∈ Γ  ai = 0.
Hence  we have x = ΦΓaΓ + ν  where aΓ = (ai)i∈Γ and ΦΓ = [(ϕi)i∈Γ]. The model reduces thus
to linear-Gaussian  where aΓ ∼ N (0  H = diag((σ2
i )i∈Γ)). We have aΓ | x  ˆs ∼ N (µ  K)  where
Γ x. Hence  conditioned on x and ˆs  the Bayes
K = (ǫ−2ΦΓΦT
Least-Square (BLS) and maximum a posteriori (MAP) estimators of aΓ are the same and given by
µ.

Γ + H−1)−1 and µ = ǫ−2KΦT

3.2 Multiplier estimation

The MAP estimate of s given x is given by ˆs = arg maxs p(s | x). Given s  x has a Gaussian
i . Using Bayes’ rule  we can write

i ϕiϕT

distribution N (0  Σ)  where Σ = ǫ2In + Pi : si=1 σ2

p(s | x) ∝ p(x | s)p(s) ∝ e−Ex(s)  where

Ex(s) =

1
2

xT Σ−1x +

1
2

log det Σ −

1
2

sT W s − bT s.

We can thus compute the MAP estimate using Gibbs sampling and simulated annealing. In the
Gibbs sampling procedure  the probability that node i changes its value from si to ¯si given x  all the
other nodes s¬i and at temperature T is given by

p(si → ¯si|s¬i  x) = (cid:18)1 + exp(cid:18)−

∆Ex

T (cid:19)(cid:19)−1

 

where ∆Ex = Ex(si  s¬i) − Ex( ¯si  s¬i). Note that computing Ex requires the inverse and the
determinant of Σ  which is expensive. Let ¯Σ and Σ be the covariance matrices corresponding to the
proposed state ( ¯si  s¬i) and current state (si  s¬i) respectively. They differ only by a rank 1 matrix 
i.e. ¯Σ = Σ + αϕiϕT
i   where α = 1
i . Therefore  to compute ∆Ex we can take advantage
of the Sherman-Morrison formula

2 ( ¯si − si)σ2

¯Σ−1 = Σ−1 − αΣ−1ϕi(1 + αϕT

i Σ−1ϕi)−1ϕT

i Σ−1

and of a similar formula for the log det term

Using (1) and (2) ∆Ex can be written as

log det ¯Σ = log det Σ + log(cid:0)1 + αϕT

(1)

(2)

∆Ex =

1
2

α(xT Σ−1ϕi)2
1 + αϕT
i Σ−1ϕi

−

1
2

log(cid:0)1 + αϕT

i Σ−1ϕi(cid:1) .
i Σ−1ϕi(cid:1) + ( ¯si − si)
Xj6=i

Wij sj + bi
 .

The transition probabilities can thus be computed efﬁciently  and if a new state is accepted we update
Σ and Σ−1 using (1).

3.3 Model estimation

i )i=1..m  W  b) that offer the best explanation of the data. Let p∗(x) = 1

Given a dataset D = {x(1)  . . .   x(N )} of image patches  we want to learn the parameters θ =
(Φ  (σ2
i=1 δ(x − x(i))
be the empirical distribution. Since in our model the variables a and s are latent  we use a variational
expectation maximization algorithm [16] to optimize θ  which amounts to maximizing a lower bound
on the log-likelihood derived using Jensen’s inequality

N PN

log p(x | θ) ≥ Xs Za

q(a  s | x) log

p(x  a  s | θ)
q(a  s | x)

da 

3

where q(a  s | x) is a probability distribution. We restrict ourselves to the family of point mass
distributions Q = {q(a  s | x) = δ(a − ˆa)δ(s − ˆs)}  and with this choice the lower bound on the
log-likelihood of D can be written as

L(θ  q) = Ep∗ [log p(x  ˆa  ˆs | θ)]

= Ep∗ [log p(x | ˆa  Φ)]

+ Ep∗[log p(ˆa | ˆs  (σ2

i )i=1..m)]

|

LΦ

{z

}

|

Lσ

{z

We perform coordinate ascent in the objective function L(θ  q).

(3)

+ Ep∗ [log p(ˆs | W  b)]

.

|

LW b

{z

}

}

3.3.1 Maximization with respect to q
We want to solve maxq∈Q L(θ  q)  which amounts to ﬁnding arg maxa s log p(x  a  s) for every
x ∈ D. This is computationally expensive since s is discrete. Hence  we introduce two phases in
the algorithm.

In the ﬁrst phase  we infer the coefﬁcients in the usual sparse coding model where the prior over a

is factorial  i.e. p(a) = Qi p(ai) ∝ Qi exp{−λS(ai)}. In this setting  we have
2 + λXi

1
2ǫ2 kx − Φak2

p(x|a)Yi

e−λS(ai) = arg min

ˆa = arg max

a

a

S(ai).

(4)

With S(ai) = |ai|  (4) is known as basis pursuit denoising (BPDN) whose solution has been shown
to be such that many coefﬁcient of ˆa are exactly zero [17]. This allows us to recover the sparsity
pattern ˆs  where ˆsi = 2.1[ ˆai 6= 0] − 1 ∀i. BPDN can be solved efﬁciently using a competitive
algorithm [18]. Another possible choice is S(ai) = 1[ai 6= 0] (p(ai) is not a proper prior though) 
where (4) is combinatorial and can be solved approximately using orthogonal matching pursuits
(OMP) [19].

After several iterations of coordinate ascent and convergence of θ using the above approximation 
we enter the second phase of the algorithm and reﬁne θ by using the GSM inference described in
Section 3.1 where ˆs = arg max p(s|x) and ˆa = E[a | ˆs  x].

3.3.2 Maximization with respect to θ

i )i=1..m and (W  b) of our optimization problem.

We want to solve maxθ L(θ  q). Our choice of variational posterior allowed us to write the objective
function as the sum of the three terms LΦ  Lσ and LW b (3)  and hence to decouple the variables Φ 
(σ2
Maximization of LΦ. Note that LΦ is the same objective function as in the standard sparse cod-
ing problem when the coefﬁcients a are ﬁxed. Let {ˆa(i)  ˆs(i)} be the coefﬁcients and multipliers
corresponding to x(i). We have

LΦ = −

1
2ǫ2

N

Xi=1

kx(i) − Φˆa(i)k2

2 −

N n

2

log 2πǫ2.

We add the constraint that kϕik2 ≤ 1 to avoid the spurious solution where the norm of the basis
functions grows and the coefﬁcients tend to 0. We solve this ℓ2 constrained least-square problem
using the Lagrange dual as in [20].
Maximization of Lσ. The problem of estimating σ2
i is a standard variance estimation problem for
a 0-mean Gaussian random variable  where we only consider the samples ˆai such that the spin ˆsi is
equal to 1  i.e.

σ2
i =

1
card{k : ˆsi

(k) = 1} Xk : ˆsi

(k)=1

( ˆai

(k))2.

Maximization of LW b. This problem is tantamount to estimating the parameters of a fully visible
Boltzmann machine [21] which is a convex optimization problem. We do gradient ascent in LW b 
where the gradients are given by ∂LW b
= −Ep∗ [si] + Ep[si].
∂Wij
We use Gibbs sampling to obtain estimates of Ep[sisj] and Ep[si].

= −Ep∗ [sisj] + Ep[sisj] and ∂LW b
∂bi

4

Note that since computing the parameters (ˆa  ˆs) of the variational posterior in phase 1 only depends
on Φ  we ﬁrst perform several steps of coordinate ascent in (Φ  q) until Φ has converged  which is
the same as in the usual sparse coding algorithm. We then maximize Lσ and LW b  and after that we
enter the second phase of the algorithm.

4 Recovery of the model parameters

Although the learning algorithm relies on a method where the family of variational posteriors q(a  s |
x) is quite limited  we argue here that if data D = {x(1)  . . .   x(N )} is being sampled according
to parameters θ0 that obey certain conditions that we describe now  then our proposed learning
algorithm is able to recover θ0 with good accuracy using phase 1 only.
Let η be the coherence parameter of the basis set which equals the maximum absolute inner product
between two distinct basis functions. It has been shown that given a signal that is a sparse linear
combination of p basis functions  BP and OMP will identify the optimal basis functions and their
coefﬁcients provided that p < 1
2 (η−1 + 1)  and the sparsest representation of the signal is unique
[19]. Similar results can be derived when noise is present (ǫ > 0) [22]  but we restrict ourselves to
the noiseless case for simplicity. Let ksk↑ be the number of spins that are up. We require (W0  b0)
2 (η−1 + 1)(cid:1) ≈ 1  which can be enforced by imposing strong negative
to be such that P r(cid:0)ksk↑ < 1

biases. A data point x(i) ∈ D thus has a high probability of yielding a unique sparse representation in
the basis set Φ. Provided that we have a good estimate of Φ we can recover its sparse representation
using OMP or BP  and therefore identify s(i) that was used to originally sample x(i). That is we
recover with high probability all the samples from the Ising model used to generate D  which allows
us to recover (W0  b0).
We provide for illustration a simple example of model recovery where n = 7 and m = 8. Let

(e1  . . .   e7) be an orthonormal basis in R7. We let Φ0 = [e1  . . . e7  1√7 Pi ei]. We ﬁx the biases
b0 at −1.2 such that the model is sufﬁciently sparse as shown by the histogram of ksk↑ in Figure
2  and the weights W0 are sampled according to a Gaussian distribution. The variance parameters
σ0 are ﬁxed to 1. We then generate synthetic data by sampling 100000 data from this model using
θ0. We then estimate θ from this synthetic data using the variational method described in Section 3
using OMP and phase 1 only. We found that the basis functions are recovered exactly (not shown) 
and that the parameters of the Ising model are recovered with high accuracy as shown in Figure 2.

x 104 sparsity histogram

0

1

2

3

4

5

6

7

14

12

10

8

6

4

2

0

0

−1

−2

0

−1

−2

b0

4
b

1

2

3

5

6

7

W0

 

W

 

0.2

0.1

0

−0.1

−0.2

0.2

0.1

0

−0.1

−0.2

1

2

3

4

5

6

7

 

 

Figure 2: Recovery of the model. The histogram of ksk↑ is such that the model is sparse. The
parameters (W  b) learned from synthetic data are close to the parameters (W0  b0) from which this
data was generated.

5 Results for natural images

We build our training set by randomly selecting 16 × 16 image patches from a standard set of 10
512 × 512 whitened images as in [1]. It has been shown that change of luminance or contrast have
little inﬂuence on the structure of natural scenes [23]. As our goal is to uncover this structure  we
subtract from each patch its own mean and divide it by its standard deviation such that our dataset
is contrast normalized (we do not consider the patches whose variance is below a small threshold).
We ﬁx the number of basis functions to 256. In the second phase of the algorithm we only update
Φ  and we have found that the basis functions do not change dramatically after the ﬁrst phase.
Figure 3 shows the learned parameters Φ  σ and b. The basis functions resemble Gabor ﬁlters at
a variety of orientations  positions and scales. We show the weights W in Figure 4 according to

5

Φ

σ

50

100

150

200

250

b

50

100

150

200

250

2

1

0
0

0

−0.5

−1
0

Figure 3: On the left is shown the entire set of basis functions Φ learned on natural images. On the
right are the learned variances (σ2

i )i=1..m (top) and the biases b in the Ising model (bottom).

the spatial properties (position  orientation  length) of the basis functions that are linked together
by them. Each basis function is denoted by a bar that indicates its position  orientation  and length
within the 16 × 16 patch.

(a) 10 most positive weights

(b) 10 most negative weights

(c) Weights visualization

ϕi

ϕj

ϕk

Wij < 0
Wik > 0

(d) Association ﬁelds

Figure 4: (a) (resp. (b)) shows the basis function pairs that share the strongest positive (resp. neg-
ative) weights ordered from left to right. Each subplot in (d) shows the association ﬁeld for a basis
function ϕi whose position and orientation are denoted by the black bar. The horizontal connec-
tions (Wij )j6=i are displayed by a set of colored bars whose orientation and position denote those
of the basis functions ϕj to which they correspond  and the color denotes the connection strength
(see (c)). We show a random selection of 36 association ﬁelds  see www.eecs.berkeley.edu/ gar-
rigue/nips07.html for the whole set.

We observe that the connections are mainly local and connect basis functions at a variety of orien-
tations. The histogram of the weights (see Figure 5) shows a long positive tail corresponding to a
bias toward facilitatory connections. We can see in Figure 4a b that the 10 most “positive” pairs
have similar orientations  whereas the majority of the 10 most “negative” pairs have dissimilar ori-
entations. We compute for a basis function the average number of basis functions sharing with it
a weight larger than 0.01 as a function of their orientation difference in four bins  which we refer
to as the “orientation proﬁle” in Figure 5. The error bars are a standard deviation. The resulting
orientation proﬁle is consistent with what has been observed in physiological experiments [24  25].

We also show in Figure 5 the tradeoff between the signal to noise ratio (SNR) of an image patch x
and its reconstruction Φˆa  and the ℓ0 norm of the representation kˆak0. We consider ˆa inferred using
both the Laplacian prior and our proposed prior. We vary λ (see Equation (4)) and ǫ respectively 
and average over 1000 patches to obtain the two tradeoff curves. We see that at similar SNR the
representations inferred by our model are more sparse by about a factor of 2  which bodes well for
compression. We have also compared our prior for tasks such as denoising and ﬁlling-in  and have
found its performance to be similar to the factorial Laplacian prior even though it does not exploit
the dependencies of the code. One possible explanation is that the greater sparsity of our inferred
representations makes them less robust to noise. Thus we are currently investigating whether this

6

property may instead have advantages in the self-taught learning setting in improving classiﬁcation
performance.

coupling weights histogram

(W ΦTΦ) correlation

7000

6000

5000

4000

3000

2000

1000

0

−0.05

0

0.05

weights

0.1

0.15

j
i

W

0.12

0.1

0.08

0.06

0.04

0.02

0

−0.02

−0.04

−0.06
0

0.1

0.2
|ϕT
i ϕj|

0.3

0.4

s
n
o
i
t
c
e
n
n
o
c
 
f
o
 
#
 
e
g
a
r
e
v
a

14

12

10

8

6

4

2

0

−2

−π / 4

1

orientation profile

0

π / 4

2

3

orientation bins

y
t
i
s
r
a
p
s

110

100

90

80

70

60

50

40

30

 

20
5

π / 2

4

tradeoff SNR−sparsity

 

Laplacian prior
proposed prior

6

7

8

9

SNR

10

11

12

13

Figure 5: Properties of the weight matrix W and comparison of the tradeoff curve SNR - ℓ0 norm
between a Laplacian prior over the coefﬁcients and our proposed prior.

To access how much information is captured by the second-order statistics  we isolate a group
(ϕi)i∈Λ of 10 basis functions sharing strong weights. Given a collection of image patches that
we sparsify using (4)  we obtain a number of spins (ˆsi)i∈Λ from which we can estimate the em-
pirical distribution pemp  the Boltzmann-Gibbs distribution pIsing consistent with ﬁrst and second
order correlations  and the factorial distribution pf act (i.e. no horizontal connections) consistent
with ﬁrst order correlations. We can see in Figure 6 that the Ising model produces better estimates
of the empirical distribution  and results in better coding efﬁciency since KL(pemp||pIsing) = .02
whereas KL(pemp||pf act) = .1.

10−1

10−2

factorial model
Ising model

10−3

all spins up

y
t
i
l
i

a
b
o
r
p

 
l

a
c
i
r
i
p
m
E

all spins down

3 spins up

10−4

10−5

10−5

10−4

10−3

Model probability

10−2

10−1

Figure 6: Model validation for a group of 10 basis functions (right). The empirical probabilities of
the 210 patterns of activation are plotted against the probabilities predicted by the Ising model (red) 
the factorial model (blue)  and their own values (black). These patterns having exactly three spins
up are circled. The prediction of the Ising model is noticably better than that of the factorial model.

6 Discussion

In this paper  we proposed a new sparse coding model where we include pairwise coupling terms
among the coefﬁcients to capture their dependencies. We derived a new learning algorithm to adapt
the parameters of the model given a data set of natural images  and we were able to discover the de-
pendencies among the basis functions coefﬁcients. We showed that the learned connection weights
are consistent with physiological data. Furthermore  the representations inferred in our model have
greater sparsity than when they are inferred using the Laplacian prior as in the standard sparse coding
model. Note however that we have not found evidence that these horizontal connections facilitate
contour integration  as they do not primarily connect colinear basis functions. Previous models in
the literature simply assume these weights according to prior intuitions about the function of hori-
zontal connections [12  13]. It is of great interest to develop new models and unsupervised learning
schemes possibly involving attention that will help us understand the computational principles un-
derlying contour integration in the visual cortex.

7

References
[1] B.A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse

code for natural images. Nature  381(6583):607–609  June 1996.

[2] R. Raina  A. Battle  H. Lee  B. Packer  and A.Y. Ng. Self-taught learning: Transfer learning from unla-

beled data. Proceedings of the Twenty-fourth International Conference on Machine Learning  2007.

[3] G. Zetzsche and B. Wegmann. The atoms of vision: Cartesian or polar? J. Opt. Soc. Am.  16(7):1554–

1565  1999.

[4] P. Hoyer and A. Hyv¨arinen. A multi-layer sparse coding network learns contour coding from natural

images. Vision Research  42:1593–1605  2002.

[5] M.J. Wainwright  E.P. Simoncelli  and A.S. Willsky. Random cascades on wavelet trees and their use in
modeling and analyzing natural imagery. Applied and Computational Harmonic Analysis  11(1):89–123 
July 2001.

[6] O. Schwartz  T. J. Sejnowski  and P. Dayan. Soft mixer assignment in a hierarchical generative model of

natural scene statistics. Neural Comput  18(11):2680–2718  November 2006.

[7] S. Lyu and E. P. Simoncelli. Statistical modeling of images with ﬁelds of gaussian scale mixtures. In

Advances in Neural Computation Systems (NIPS)  Vancouver  Canada  2006.

[8] A. Hyv¨arinen  P.O. Hoyer  J. Hurri  and M. Gutmann. Statistical models of images and early vision.
Proceedings of the Int. Symposium on Adaptive Knowledge Representation and Reasoning (AKRR2005) 
Espoo  Finland  2005.

[9] Y. Karklin and M.S. Lewicki. A hierarchical bayesian model for learning non-linear statistical regularities

in non-stationary natural signals. Neural Computation  17(2):397–423  2005.

[10] B.A. Olshausen and K.J. Millman. Learning sparse codes with a mixture-of-gaussians prior. Advances in

Neural Information Processing Systems  12  2000.

[11] D. Fitzpatrick. The functional organization of local circuits in visual cortex: insights from the study of

tree shrew striate cortex. Cerebral Cortex  6:329–41  1996.

[12] O. Ben-Shahar and S. Zucker. Geometrical computations explain projection patterns of long-range hori-

zontal connections in visual cortex. Neural Comput  16(3):445–476  March 2004.

[13] L. Zhaoping. Border ownership from intracortical interactions in visual area v2. Neuron  47:143–153 

2005.

[14] E. Schneidman  M.J. Berry  R. Segev  and W. Bialek. Weak pairwise correlations imply strongly correlated

network states in a neural population. Nature  April 2006.

[15] G. Hinton  S. Osindero  and K. Bao. Learning causally linked markov random ﬁelds. Artiﬁcial Intelligence

and Statistics  Barbados  2005.

[16] M.I. Jordan  Z. Ghahramani  T. Jaakkola  and L.K. Saul. An introduction to variational methods for

graphical models. Learning in Graphical Models  Cambridge  MA: MIT Press  1999.

[17] S.S. Chen  D.L. Donoho  and M.A. Saunders. Atomic decomposition by basis pursuit. SIAM Review 

43(1):129–159  2001.

[18] C.J. Rozell  D.H. Johnson  R.G. Baraniuk  and B.A. Olshausen. Neurally plausible sparse coding via com-
petitive algorithms. In Proceedings of the Computational and Systems Neuroscience (Cosyne) meeting 
Salt Lake City  UT  February 2007.

[19] J.A. Tropp. Greed is good: algorithmic results for sparse approximation. IEEE Transactions on Informa-

tion Theory  50(10):2231–2242  2004.

[20] H. Lee  A. Battle  R. Raina  and A.Y. Ng. Efﬁcient sparse coding algorithms. In Advances in Neural

Information Processing Systems 19  pages 801–808. MIT Press  Cambridge  MA  2007.

[21] D.H. Ackley  G.E. Hinton  and T.J. Sejnowski. A learning algorithm for boltzmann machines. Cognitive

Science  9(1):147–169  1985.

[22] J.A. Tropp.

Just relax: convex programming methods for identifying sparse signals in noise.

Transactions on Information Theory  52(3):1030–1051  2006.

IEEE

[23] Z. Wang  A.C. Bovik  and E.P. Simoncelli. Structural approaches to image quality assessment. In Alan
Bovik  editor  Handbook of Image and Video Processing  chapter 8.3  pages 961–974. Academic Press 
May 2005. 2nd edition.

[24] R. Malach  Y. Amir  M. Harel  and A. Grinvald. Relationship between intrinsic connections and functional
architecture revealed by optical imaging and in vivo targeted biocytin injections in primate striate cortex.
Proc. Natl. Acad. Sci. U.S.A.  82:935–939  1993.

[25] W. Bosking  Y. Zhang  B. Schoﬁeld  and D. Fitzpatrick. Orientation selectivity and the arrangement of

horizontal connections in the tree shrew striate cortex. J. Neuroscience  17(6):2112–2127  1997.

8

,Ilija Ilievski
Jiashi Feng