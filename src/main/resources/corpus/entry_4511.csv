2016,Quantized Random Projections and Non-Linear Estimation of Cosine Similarity,Random projections constitute a simple  yet effective technique for dimensionality reduction with applications in learning and search problems. In the present paper  we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to $b$ bits. We here argue that the maximum likelihood estimator (MLE) is a principled approach to deal with the non-linearity resulting from quantization  and subsequently study its computational and statistical properties. A specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission. Along the way  we also touch upon the existence of a qualitative counterpart to the Johnson-Lindenstrauss lemma in the presence of quantization.,Quantized Random Projections and Non-Linear

Estimation of Cosine Similarity

Ping Li
Rutgers University
pingli@stat.rutgers.edu

Michael Mitzenmacher
Harvard University
michaelm@eecs.harvard.edu

Martin Slawski
Rutgers University
martin.slawski@rutgers.edu

Abstract

Random projections constitute a simple  yet effective technique for dimensionality
reduction with applications in learning and search problems. In the present paper 
we consider the problem of estimating cosine similarities when the projected
data undergo scalar quantization to b bits. We here argue that the maximum
likelihood estimator (MLE) is a principled approach to deal with the non-linearity
resulting from quantization  and subsequently study its computational and statistical
properties. A speciﬁc focus is on the on the trade-off between bit depth and the
number of projections given a ﬁxed budget of bits for storage or transmission.
Along the way  we also touch upon the existence of a qualitative counterpart to the
Johnson-Lindenstrauss lemma in the presence of quantization.

Introduction

1
The method of random projections (RPs) is an important approach to linear dimensionality reduc-
tion [23]. RPs have established themselves as an alternative to principal components analysis which
is computationally more demanding. Instead of determining an optimal low-dimensional subspace
via a singular value decomposition  the data are projected on a subspace spanned by a set of directions
picked at random (e.g. by sampling from the Gaussian distribution). Despite its simplicity  this
approach comes with a theoretical guarantee: as asserted by the celebrated Johnson-Lindenstrauss
(J-L) lemma [6  12]  k = O(log n/ε2) random directions are enough to preserve the squared distances
between all pairs from a data set of size n up to a relative error of ε  irrespective of the dimension d the
data set resides in originally. Inner products are preserved similarly. As a consequence  procedures
only requiring distances or inner products can be approximated in the lower-dimensional space 
thereby achieving substantial reductions in terms of computation and storage  or mitigating the curse
of dimensionality. The idea of RPs has thus been employed in linear learning [7  19]  fast matrix
factorization [24]  similarity search [1  9]  clustering [2  5]  statistical testing [18  22]  etc.
The idea of data compression by RPs has been extended to the case where the projected data are
additionally quantized to b bits so as to achieve further reductions in data storage and transmission.
The extreme case of b = 1 is well-studied in the context of locality sensitive hashing [4]. More
recently  b-bit quantized random projections for b ≥ 1 have been considered from different perspec-
tives. The paper [17] studies Hamming distance-based estimation of cosine similarity and linear
classiﬁcation when using a coding scheme that maps a real value to a binary vector of length 2b. It
is demonstrated that for similarity estimation  taking b > 1 may yield improvements if the target
similarity is high. The paper [10] is dedicated to J-L-type results for quantized RPs  considerably
improving over an earlier result of the same ﬂavor in [15]. The work [15] also discusses the trade-off
between the number of projections k and number of bits b per projection under a given budget of bits
as it also appears in the literature on quantized compressed sensing [11  14].
In the present paper  all of these aspects and some more are studied for an approach that can be
substantially more accurate for small b (speciﬁcally  we focus on 1 ≤ b ≤ 6) than those in [10  17  15].
In [10  15] the non-linearity of quantization is ignored by treating the quantized data as if they had
been observed directly. Such “linear” approach beneﬁts from its simplicity  but it is geared towards
ﬁne quantization  whereas for small b the bias resulting from quantization dominates. By contrast 
the approach proposed herein makes full use of the knowledge about the quantizer. As in [17] we
suppose that the original data set is contained in the unit sphere of Rd  or at least that the Euclidean

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

norms of the data points are given. In this case  approximating distances boils down to estimating
inner products (or cosine similarity) which can be done by maximum likelihood (ML) estimation
based on the quantized data. Several questions of interest can be addressed by considering the Fisher
information of the maximum likelihood estimator (MLE). With regard to the aforementioned trade-off
between k and b  it turns out that the choice b = 1 is optimal (in the sense of yielding maximum
Fisher information) as long as the underlying similarity is smaller than 0.2; as the latter increases  the
more effective it becomes to increase b. By considering the rate of growth of the Fisher information
near the maximum similarity of one  we discover a gap between the ﬁnite bit and inﬁnite bit case with
rates of Θ((1 − ρ∗)−3/2) and Θ((1 − ρ∗)−2)  respectively  where ρ∗ denotes the target similarity.
As an implication  an exact equivalent of the J-L lemma does not exist in the ﬁnite bit case.
The MLE under study does not have a closed form solution. We show that it is possible to approximate
the MLE by a non-iterative scheme only requiring pre-computed look-up tables. Derivation of this
scheme lets us draw connections to alternatives like the Hamming distance-based estimator in [17].
We present experimental results concerning applications of the proposed approach in nearest neighbor
search and linear classiﬁcation. In nearest neighbor search  we focus on the high similarity regime and
conﬁrm theoretical insights into the trade-off between k and b. For linear classiﬁcation  we observe
empirically that intermediate values of b can yield better trade-offs than single-bit quantization.
Notation. We let [d] = {1  . . .   d}. I(P ) denotes the indicator function of expression P . For a
function f (ρ)  we use ˙f (ρ) and ¨f (ρ) for its ﬁrst resp. second derivative. Pρ and Eρ denote probabil-
ity/expectation w.r.t. a zero mean  unit variance bivariate normal distribution with correlation ρ.

Supplement: Proofs and additional experimental results can be found in the supplement.
2 Quantized random projections  properties of the MLE  and implications
We start by formally introducing the setup  the problem and the approach that is taken before
discussing properties of the MLE in this speciﬁc case  along with important implications.
Setup. Let X = {x1  . . .   xn} ⊂ Sd−1  where Sd−1 := {x ∈ Rd : (cid:107)x(cid:107)2 = 1} denotes the unit
sphere in Rd  be a set of data points. We think of d being large. As discussed below  the requirement
of having all data points normalized to unit norm is not necessary  but it simpliﬁes our exposition
considerably. Let x  x(cid:48) be a generic pair of elements from X and let ρ∗ = (cid:104)x  x(cid:48)(cid:105) denote their inner
product. Alternatively  we may refer to ρ∗ as (cosine) similarity or correlation. Again for simplicity 
we assume that 0 ≤ ρ∗ < 1; the case of negative ρ∗ is a trivial extension because of symmetry.
We aim at reducing the dimensionality of the given data set by means of a random projection  which
is realized by sampling a random matrix A of dimension k by d whose entries are i.i.d. N (0  1)
(i.e.  zero-mean Gaussian with unit variance). Applying A to X yields Z = {zi}n
i=1 ⊂ Rk with
zi = Axi  i ∈ [n]. Subsequently  the projected data points {zi}n
i=1 are subject to scalar quantization.
A b-bit scalar quantizer is parameterized by 1) thresholds t = (t1  . . .   tK−1) with 0 = t0 < t1 <
. . . < tK−1 < tK = +∞ inducing a partitioning of the positive real line into K = 2b−1 intervals
{[tr−1  tr)  r ∈ [K]} and 2) a codebook M = {µ1  . . .   µK} with code µr representing interval
[tr−1  tr)  r ∈ [K]. Given t and M  the scalar quantizer (or quantization map) is deﬁned by

K(cid:88)

r=1

Q : R → M± := −M ∪ M 

z (cid:55)→ Q(z) = sign(z)

µrI(|z| ∈ [tr−1  tr))

i=1 ⊂ (M±)k  qi = ( Q(zij) )k

2 = 2(1 − ρ∗). If z  z(cid:48) were given  it would be standard to use 1

(1)
The projected  b-bit quantized data result as Q = {qi}n
j=1  i ∈ [n].
Problem statement. Let z  z(cid:48) and q  q(cid:48) denote the pairs corresponding to x  x(cid:48) in Z respectively
Q. The goal is to estimate ρ∗ = (cid:104)x  x(cid:48)(cid:105) from q  q(cid:48) which automatically yields an estimate of
k (cid:104)z  z(cid:48)(cid:105) as an unbiased
(cid:107)x − x(cid:48)(cid:107)2
estimator of ρ∗. This "linear" approach is commonly adopted when the data undergo uniform
quantization with saturation level T (i.e.  tr = T · r/(K − 1)  µr = (tr − tr−1)/2  r ∈ [K − 1] 
k (cid:104)z  z(cid:48)(cid:105) which in turn is sharply
µK = T )  based on the rationale that as b → ∞  1
concentrated around its expectation ρ∗.
k (cid:104)q  q(cid:48)(cid:105) has a
There are two major concerns about this approach. First  for ﬁnite b the estimator 1
bias resulting from the non-linearity of Q that does not vanish as k → ∞. For small b  the effect of
this bias is particularly pronounced. Lloyd-Max quantization (see Proposition 1 below) in place of

k (cid:104)q  q(cid:48)(cid:105) → 1

2

p1 = Pρ(Z ∈ (0  t1]  Z(cid:48) ∈ (0  t1])
p2 = Pρ(Z ∈ (0  t1]  Z(cid:48) ∈ (t1 ∞))
p3 = Pρ(Z ∈ (t1 ∞)  Z(cid:48) ∈ (t1 ∞))
p4 = P−ρ(Z ∈ (0  t1]  Z(cid:48) ∈ (0  t1])
p5 = P−ρ(Z ∈ (0  t1]  Z(cid:48) ∈ (t1 ∞))
p6 = P−ρ(Z ∈ (t1 ∞)  Z(cid:48) ∈ (t1 ∞))

k((cid:98)ρMLE − ρ∗)2 for b = 3 (averaged over 104 i.i.d. data sets with k = 100) compared to the inverse

Figure 1: (L  M): Partitioning into cells for b = 2 and cell probabilities. (R): Empirical MSE
information. The disagreement for ρ ≤ 0.2 results from positive truncation of the MLE at zero.

uniform quantization provides some remedy  but the issue of non-vanishing bias remains. Second 
even for inﬁnite b  the approach is statistically not efﬁcient. In order to see this  note that

{(zj  z(cid:48)

j)}k

j=1

i.i.d.∼ (Z  Z(cid:48))  where (Z  Z(cid:48)) ∼ N2

0 

.

(2)

(cid:18)

(cid:18) 1

ρ∗

(cid:19)(cid:19)

ρ∗
1

k(cid:89)

It is shown in [16] that the MLE of ρ∗ under the above bivariate normal model has a variance of
(1 − ρ2∗)2/{k (1 + ρ2∗)}  while Var((cid:104)z  z(cid:48)(cid:105) /k) = (1 + ρ2∗)/k which is a substantial difference for
large ρ∗. The higher variance results from not using the information that the components of z and z(cid:48)
have unit variance [16]. In conclusion  the linear approach as outlined above suffers from noticeable
bias/and or high variance if the similarity ρ∗ is high  and it thus makes sense to study alternatives.
Maximum likelihood estimation of ρ∗. We here propose the MLE in place of the linear approach.
The advantage of the MLE is that it can have substantially better statistical performance as the
quantization map is explicitly taken into account. The MLE is based on bivariate normality according
to (2). The effect of quantization is identical to that of what is known as interval censoring in statistics 
i.e.  in place of observing a speciﬁc value  one only observes that the datum is contained in an interval.
The concept is easiest to understand in the case of one-bit quantization. For any j ∈ [k]  each of
the four possible outcomes of (qj  q(cid:48)
j) corresponds to one of the four orthants of R2. By symmetry 
the probability of (qj  q(cid:48)
j) falling into the positive or into the negative orthant are identical; both
correspond to a “collision”  i.e.  to the event {qj = q(cid:48)
j) falling
j}.
into one of the remaining two orthants are identical  corresponding to a disagreement {qj (cid:54)= q(cid:48)
Accordingly  the likelihood function in ρ is given by

j}. Likewise  the probability of (qj  q(cid:48)

{π(ρ)I(qj =q(cid:48)

j )(1 − π(ρ))I(qj(cid:54)=q(cid:48)

j )} 

π(ρ) := Pρ(sign(Z) = sign(Z(cid:48))) 

j=1

where π(ρ) denotes the probability of a collision after quantization for (Z  Z(cid:48)) as in (2) with ρ∗

replaced by ρ. It is straightforward to show that the MLE is given by(cid:98)ρMLE = cos(π(1 −(cid:98)π))  where
π is the circle constant and(cid:98)π = k−1(cid:80)k
that the expression for(cid:98)ρMLE follows the same rationale as used for the simhash in [4].

j) is the empirical counterpart to π(ρ). We note

j=1 I(qj = q(cid:48)

With these preparations  it is not hard to see how the MLE generalizes to cases with more than one
bit. For b = 2  there is a single non-trivial threshold t1 that yields a partitioning of the real axis into
four bins and accordingly a component (qj  q(cid:48)
j) of a quantized pair can fall into 16 possible cells
(rectangles)  cf. Figure 1. By orthant symmetry and symmetries within each orthant  one ends up
with six distinct probabilities p1  . . .   p6 for (qj  q(cid:48)
j) falling into one of those cells depending on ρ.
Weighting those probabilities according to the number of their occurrences in the left part of Figure 1 
we end up with probabilities π1 = π1(ρ)  . . .   π6 = π6(ρ) that sum up to one. The corresponding
j=1 form a sufﬁcient statistic for ρ. For
general b  we have 22bcells and L = K(K + 1) (recall that K = 2b−1) distinct probabilities  so
that L = 20  72  272  1056 for b = 3  . . .   6. This yields the following compact expressions for the

relative cell frequencies(cid:98)π1  . . .  (cid:98)π6 resulting from (qj  q(cid:48)

j)k

3

p1p2p2p4p5p5p4p5p5p1p2p2p3p3p6p600.20.40.60.8100.20.40.60.811.21.4ρ empirical MSEI−1(ρ)b = 3(ρ)/I−1

Figure 2: b · I−1
1 (ρ) vs. ρ for different choices of t: Lloyd-Max and uniform quantization
with saturation levels T0.9  T0.95  T0.99  cf. §4.1 for a deﬁnition. The latter are better suited for high
similarity. The differences become smaller as b increases. Note that for b = 6  ρ > 0.7 is required for
either quantization scheme to achieve a better trade-off than the one-bit MLE.

b

negative log-likelihood l(ρ) and the Fisher information I(ρ) = Eρ[¨l(ρ)] (up to a factor of k)

L(cid:88)

(cid:98)π(cid:96) log(π(cid:96)(ρ)) 

L(cid:88)

( ˙π(cid:96)(ρ))2
π(cid:96)(ρ)

.

(3)

l(ρ) =

I(ρ) =

(cid:96)=1

(cid:96)=1

The information I(ρ) is of particular interest. By classical statistical theory [21]  {E[(cid:98)ρMLE] − ρ∗}2 =
O(1/k2)  Var((cid:98)ρMLE) = I−1(ρ)/k  E[((cid:98)ρMLE − ρ∗)2] = I−1(ρ)/k + O(1/k2) as k → ∞. While
(cid:98)ρMLE in subsequent analysis.

this is an asymptotic result  it agrees to a good extent with what one observes for ﬁnite  but not too
small samples  cf. Figure 1. We therefore treat the inverse information as a proxy for the accuracy of

Remark. We here brieﬂy address the case of known  but possibly non-unit norms  i.e.  (cid:107)x(cid:107)2 = σx 
(cid:107)x(cid:48)(cid:107)2 = σx(cid:48). This can be handled by re-scaling the thresholds of the quantizer (1) by σx resp. σx(cid:48) 
estimating ρ∗ based on q  q(cid:48) as in the unit norm case  and subsequently re-scaling the estimate by
σxσx(cid:48) to obtain an estimate of (cid:104)x  x(cid:48)(cid:105). The assumption that the norms are known is not hard to satisfy
in practice as they can be computed by one linear scan during data collection. With a limited bit
budget  the norms additionally need to be quantized. It is unclear how to accurately estimate them
from quantized data (for b = 1  it is deﬁnitely impossible).
Choice of the quantizer. Equipped with the Fisher information (3)  one of the questions that can
be addressed is quantizer design. Note that as opposed to the linear approach  the speciﬁc choice
of the {µr}K
r=1 in (1) is not important as ML estimation only depends on cell frequencies but not
on the values associated with the intervals {(tr−1  tr]}K
r=1. The thresholds t  however  turn out to
have a considerable impact  at least for small b. An optimal set of thresholds can be determined by
minimizing the inverse information I−1(ρ; t) w.r.t. t for ﬁxed ρ. As the underlying similarity is not
known  this may not seem practical. On the other hand  prior knowledge about the range of ρ may be
available  or the closed form one-bit estimator can be used as pilot estimator. For ρ = 0  the optimal
set of thresholds coincide with those of Lloyd-Max quantization [20].
Proposition 1. Let g ∼ N (0  1) and consider Lloyd-Max quantization given by

(t∗ {µ∗

r}K
r=1) = argmin
t {µr}K

r=1

E[{g − Q(g; t {µr}K

r=1)}2]. We also have t∗ = argmin

I−1(0; t).

t

(cid:96)=1 and their derivatives { ˙π(cid:96)(ρ; t)}L

The Lloyd-Max problem can be solved numerically by means of an alternating scheme which can
be shown to converge to a global optimum [13]. For ρ > 0  an optimal set of thresholds can be
determined by general procedures for nonlinear optimization. Evaluation of I−1(ρ; t) requires
computation of the probabilities {π(cid:96)(ρ; t)}L
(cid:96)=1. The latter are
available in closed form (cf. supplement)  while for the former specialized numerical integration
procedures [8] can be used. In order to avoid multi-dimensional optimization  it makes sense to
conﬁne oneself to thresholds of the form tr = T · r/(K − 1)  r ∈ [K − 1]  so that only T needs to
be optimized. Even though the Lloyd-Max scheme performs reasonably also for large values of ρ 
the one-parameter scheme may still yield signiﬁcant improvements in that case  cf. Figure 2. Once
b ≥ 5  the differences between the two schemes become marginal.
Trade-off between k and b. Suppose we are given a ﬁxed budget of bits B = k · b for transmission
or storage  and we are in free choosing b. The optimal choice of b can be determined by comparing

4

00.20.40.60.810.40.60.811.21.41.61.8b = 2 ρb∗I−1b(ρ)/I−11(ρ) Lloyd-MaxT0.9T0.95T0.9900.20.40.60.8100.20.40.60.811.21.41.61.82b = 4 ρb∗I−1b(ρ)/I−11(ρ) Lloyd-MaxT0.9T0.95T0.9900.20.40.60.8100.511.522.5b = 6 ρb∗I−1b(ρ)/I−11(ρ) Lloyd-MaxT0.9T0.95T0.99b

b

b

(ρ) vs. ρ.

(ρ) vs. ρ for 1 ≤ b ≤ 6 with t chosen by Lloyd-Max.

schemes above. Since the mean squared error of(cid:98)ρMLE decays with 1/k for any b  for b(cid:48) with b(cid:48) > b

Figure 3: Trade-off between k and b. (L): b· I−1
(M): Zoom into the range 0.9 ≤ ρ ≤ 1. (R): choice of b minimizing b · I−1
the inverse Fisher information I−1
(ρ) for changing b with t chosen according to either of the two
to be more efﬁcient than b at the bit scale it  is required that Ib(cid:48)(ρ)/Ib(ρ) > b(cid:48)/b as with the smaller
choice b one would be allowed to increase k by a factor of b(cid:48)/b. Again  this comparison is dependent
on a speciﬁc ρ. From Figure 3  however  one can draw general conclusions: for ρ < 0.2  it does not
pay off to increase b beyond one; as ρ increases  higher values of b achieve a better trade-off with
even b = 6 being the optimal choice for ρ > 0.98. The intuition is that two points of high similarity
agree on their ﬁrst signiﬁcant bit for most coordinates  in which case increasing the number of bits
becomes beneﬁcial. This ﬁnding is particularly relevant to (near-)duplicate detection/nearest neighbor
search where high similarities prevail  an application investigated in §4.
Rate of growth of the Fisher information near ρ = 1. Interestingly  we do not observe a “saturation”
even for b = 6 in the sense that for ρ close enough to 1  one can still achieve an improvement
at the bit scale compared to 1 ≤ b ≤ 5. This raises the question about the rate of growth of
the Fisher information near one relative to the full precision case (b → ∞). As shown in [16]
I∞(ρ) = (1 + ρ2)/(1 − ρ2)2 = Θ((1 − ρ)−2) as ρ → 1. As stated below  in the ﬁnite bit case  the
exponent is only 3/2 for all b. This is a noticeable gap.
Theorem 1. For 1 ≤ b < ∞  we have I(ρ) = Θ((1 − ρ)−3/2) as ρ → 1.
The theorem has an interesting implication with regard to the existence of a Johnson-Lindenstrauss
(J-L)-type result for quantized random projections. In a nutshell  the J-L lemma states that as long as
k = Ω(log n/ε2)  with high probability we have that

(1 − ε)(cid:107)xi − xj(cid:107)2

2 ≤ (cid:107)zi − zj(cid:107)2

2/k ≤ (1 + ε)(cid:107)xi − xj(cid:107)2

2 for all pairs (i  j) 

i.e.  the distances of the data in X are preserved in Z up to a relative error of ε. In our setting  one
would hope for an equivalent of the form

(4)
MLE denotes the MLE for ρij given quantized RPs. The
standard proof of the J-L lemma [6] combines norm preservation for each individual pair of the form

(1 − ε)2(1 − ρij) ≤ 2(1 −(cid:98)ρij
where ρij = (cid:104)xi  xj(cid:105)  i  j ∈ [n]  and (cid:98)ρij
with a union bound. Such a concentration result does not appear to be attainable for(cid:98)ρMLE − ρ∗  not
2 ≤ (cid:107)zi − zj(cid:107)2
even asymptotically as k → ∞ in which case(cid:98)ρMLE − ρ∗ is asymptotically normal with mean zero

MLE) ≤ (1 + ε)2(1 − ρij) ∀(i  j) as long as k = Ω(log n/ε2) 

and variance I−1(ρ∗)/k. This yields an asymptotic tail bound of the form

2/k ≤ (1 + ε)(cid:107)xi − xj(cid:107)2

2) ≤ 2 exp(−kΘ(ε2))

P((1 − ε)(cid:107)xi − xj(cid:107)2

P(|(cid:98)ρMLE − ρ∗| > δ) ≤ 2 exp(−δ2k/{2I−1(ρ∗)}).

(5)
For a result of the form (4)  which is about relative distance preservation  one would need to choose δ
proportional to ε(1− ρ∗). In virtue of Theorem 1  I−1(ρ∗) = Θ((1− ρ∗)3/2) as ρ∗ → 1 so that with
δ chosen in that way the exponent in (5) would vanish as ρ∗ → 1. By constrast  the required rate of
decay of I−1(ρ∗) is achieved in the full precision case. Given the asymptotic optimality of the MLE
according to the Cramer-Rao lower bound suggests that a qualitative counterpart to the J-L lemma (4)
is out of reach. Weaker versions in which the required lower bound on k would depend inversely on
the minimum distance of points in X are still possible. Similarly  a weaker result of the form
MLE) ≤ 2(1 − ρij) + ε ∀(i  j) as long as k = Ω(log n/ε2) 

2(1 − ρij) − ε ≤ 2(1 −(cid:98)ρij

is known to hold already in the one-bit case and follows immediately from the closed form expression
of the MLE  Hoeffdings’s inequality  and the union bound; cf. e.g. [10].

5

00.20.40.60.81ρ01234567b∗I−1b(ρ)allρb=1b=2b=3b=4b=5b=60.90.920.940.960.981ρ00.050.10.150.20.25b∗I−1b(ρ)ρ≥0.9b=1b=2b=3b=4b=5b=600.20.40.60.81123456ρoptimal b3 A general class of estimators and approximate MLE computation
A natural concern about the MLE relative to the linear approach is that it requires optimization via an
iterative scheme. The optimization problem is smooth  one-dimensional and over the unit interval 
hence not challenging for modern solvers. However  in applications it is typically required to compute
the MLE many times  hence avoiding an iterative scheme for optimization is worthwhile. In this
section  we introduce an approximation to the MLE that only requires at most two table look-ups.

A general class of estimators. Let π(ρ) = (π1(ρ)  . . .   πL(ρ))(cid:62) (cid:80)L

(cid:96)=1 π(cid:96)(ρ) = 1  be the normal-
ized cell frequencies depending on ρ as deﬁned in §2  let further w ∈ RL be a ﬁxed vector of weights 
and consider the map ρ (cid:55)→ θ(ρ; w) := (cid:104)π(ρ)  w(cid:105). If (cid:104) ˙π(ρ)  w(cid:105) > 0 uniformly in ρ (such w always
exist)  θ(·; w) is increasing and has an inverse θ−1(· ; w). We can then consider the estimator

two-fold application of the continuous mapping theorem. By choosing w such that w(cid:96) = 1 for (cid:96)

where we recall that(cid:98)π = ((cid:98)π  . . .  (cid:98)πL)(cid:62) are the empirical cell frequencies given quantized data q  q(cid:48).
It is easy to see that(cid:98)ρw is a consistent estimator of ρ∗: we have(cid:98)π → π(ρ∗) in probability by the
law of large numbers  and θ−1((cid:104)(cid:98)π  w(cid:105) ; w) → θ−1((cid:104)π(ρ∗)  w(cid:105) ; w) = θ−1(θ(ρ∗; w); w) = ρ∗ by
corresponding to cells contained in the positive/negative orthant and w(cid:96) = −1 otherwise (cid:98)ρw becomes
the one-bit MLE. By choosing w(cid:96) = 1 for diagonal cells (cf. Figure 1) corresponding to a collision
Alternatively  we may choose w such that the asymptotic variance of(cid:98)ρw is minimized.
j} and w(cid:96) = 0 otherwise  we obtain the Hamming distance-based estimator in [17].
event {qj = q(cid:48)
Theorem 2. For any w s.t. ˙π(ρ∗)(cid:62)w (cid:54)= 0  we have Var((cid:98)ρw) = V (w; ρ∗)/k + O(1/k2) as k → ∞ 

(cid:98)ρw = θ−1((cid:104)(cid:98)π  w(cid:105) ; w) 

V (w; ρ∗) = (w(cid:62)Σ(ρ∗)w)/{ ˙π(ρ∗)(cid:62)w}2  Σ(ρ∗) := Π(ρ∗) − π(ρ∗)π(ρ∗)(cid:62) 

(6)

and Π(ρ∗) := diag( (π(cid:96)(ρ∗))L

(cid:96)=1 ). Moreover  let w∗ = Π−1(ρ∗) ˙π(ρ∗). Then:

V (w∗; ρ∗) = I−1(ρ∗) 

argminw V (w; ρ∗) = {α(w∗ + c1)  α (cid:54)= 0  c ∈ R} 

Theorem 2 yields an expression for the optimal weights w∗ = Π−1(ρ∗) ˙π(ρ∗). This optimal choice is
on the choice w = w∗ achieves asymptotically the same statistical performance as the MLE.

and E[((cid:98)ρw∗ − ρ∗)2] = E[((cid:98)ρMLE − ρ∗)2] + O(1/k2).
unique up to translation by a multiple of the constant vector 1 and scaling. The estimator(cid:98)ρw∗ based
Approximate computation. The estimator (cid:98)ρw∗ is not operational as the optimal choice of the
weights depends on the estimand itself. This issue can be dealt with by using a pilot estimator(cid:98)ρ0 like
the one-bit MLE  the Hamming distance-based estimator in [17] or(cid:98)ρ0 =(cid:98)ρw  where w =(cid:82) 1
estimator  we may then replace w∗ by w((cid:98)ρ0) and use(cid:98)ρw((cid:98)ρ0) as a proxy for(cid:98)ρw∗ which achieves the
A second issue is that computation of(cid:98)ρw (6) entails inversion of the function θ(·; w). The inverse
computing(cid:98)ρw((cid:98)ρ0)  the weights depends on the data via the pilot estimator. We thus need to tabulate

may not be deﬁned in general  but for the choices of w that we have in mind  this is not a concern
(cf. supplement). Inversion of θ(·; w) can be carried out with tolerance ε by tabulating the function
values on a uniform grid of cardinality (cid:100)1/ε(cid:101) and performing a table lookup for each query. When

0 w(ρ) dρ
averages the expression w(ρ) = Π−1(ρ) ˙π(ρ) for the optimal weights over ρ. Given the pilot

same statistical performance asymptotically.

w(ρ) on a grid  too. Accordingly  a whole set of look-up tables is required for function inversion  one
for each set of weights. Given parameters ε  δ > 0  a formal description of our scheme is as follows.
1. Set R = (cid:100)1/ε(cid:101)  ρr = r/R  r ∈ [R]  and B = (cid:100)1/δ(cid:101)  ρb = b/B  b ∈ [B].
2. Tabulate w(ρb)  b ∈ [B]  and function values θ(ρr; w(ρb)) = (cid:104)w(ρb)  π(ρr)(cid:105)  r ∈ [R]  b ∈ [B].

Steps 1. and 2. constitute a one-time pre-processing. Given data q  q(cid:48)  we proceed as follows.

3. Obtain(cid:98)π and the pilot estimator(cid:98)ρ0 = θ−1((cid:104)(cid:98)π  w(cid:105) ; w)  with w deﬁned in the previous paragraph.
4. Return(cid:98)ρ = θ−1((cid:104)(cid:98)π  w((cid:101)ρ0)(cid:105) ; w((cid:101)ρ0))  where(cid:101)ρ0 is the value closest to(cid:98)ρ0 among the {ρb}.

Step 2. requires about C = (cid:100)1/ε(cid:101) · (cid:100)1/δ(cid:101) · L computations/storage. From experimental results we
ﬁnd that ε = 10−4 and δ = .02 appear sufﬁcient for practical purposes  which is still manageable
even for b = 6 with L = 1056 cells in which case C ≈ 5 × 108. Again  this cost is occurred
lookups. By organizing computations efﬁciently  the frequencies(cid:98)π can be obtained from one pass
only once independent of the data. The function inversions in steps 3. and 4. are replaced by table
over (qj · q(cid:48)
j)  j ∈ [k]. Equipped with the look-up tables  estimating the similarity of two points
requires O(k + L + log(1/ε)) ﬂops which is only slightly more than a linear scheme with O(k).

6

Figure 4: Average fraction of K = 10 nearest neighbors retrieved vs. total # of bits (log2 scale) for
1 ≤ b ≤ 6. b = ∞ (dashed) represents the MLE based on unquantized data  with k as for b = 6. The
oracle curve (dotted) corresponds to b = ∞ with maximum k (i.e.  as for b = 1).

4 Experiments

similarity of the quantized data is measured in terms of their Hamming distance(cid:80)k

We here illustrate the approach outlined above in nearest neighbor search and linear classiﬁcation.
The focus is on the trade-off between b and k  in particular in the presence of high similarity.
4.1 Nearest Neighbor Search
Finding the most similar data points for a given query is a standard task in information retrieval.
Another application is nearest neighbor classiﬁcation. We here investigate how the performance of
our approach is affected by the choice of k  b and the quantization scheme. Moreover  we compare
to two baseline competitors  the Hamming distance-based approach in [17] and the linear approach
in which the quantized data are treated like the original unquantized data. For the approach in [17] 
j=1 I(qj (cid:54)= q(cid:48)
j).
Synthetic data. We generate k i.i.d. samples of Gaussian data  where each sample X =
(X0  X1  . . .   X96) is generated as X0 ∼ N (0  1)  Xj = ρjX0 + (1 − ρ2
j )1/2Zj  1 ≤ j ≤ 96  where
the {Zj}96
j=1 are i.i.d. N (0  1) and independent of X0. We have E[(X0 − Xj)2] = 2(1 − ρj)  where
ρj = min{0.8+(j−1)0.002  0.99}  1 ≤ j ≤ 96. The thus generated data subsequently undergo b-bit
quantization  for 1 ≤ b ≤ 6. Regarding the number of samples  we let k ∈ {26/b  27/b  . . .   213/b}
which yields bit budgets between 26 and 213 for all b. The goal is to recover the K nearest neighbors
of X0 according to the {ρj}  i.e.  X96 is the nearest neighbor etc. The purpose of this speciﬁc setting
is to mimic the use of quantized random projections in the situation of a query x0 and data points
X = {x1  . . .   x96} having cosine similarities {ρj}96
Real data. We consider the Farm Ads data set (n = 4  143  d = 54  877) from the UCI repository and
the RCV1 data set (n = 20  242  d = 47  236) from the LIBSVM webpage [3]. For both data sets 
each instance is normalized to unit norm. As queries we select all data points whose ﬁrst neighbor
has (cosine) similarity less than 0.999  whose tenth neighbor has similarity at least 0.8 and whose
hundredth neighbor has similarity less than 0.5. These restrictions allow for a more clear presentation
of our results. Prior to nearest neighbor search  b-bit quantized random projections are applied to the
data  where the ranges for b and for the number of projections k is as for the synthetic data.
Quantization. Four different quantization schemes are considered: Lloyd-Max quantization and
thresholds tr = Tρ · r/(K − 1)  r ∈ [K − 1]  where Tρ is chosen to minimize I−1(ρ); we consider
ρ ∈ {0.9  0.95  0.99}. For the linear approach  we choose µr = E[g|g ∈ (tr−1  tr)]  r ∈ [K]  where
g ∼ N (0  1). For our approach and that in [17] the speciﬁc choice of the {µr} is not important.
Evaluation. We perform 100 respectively 20 independent replications for synthetic respectively
real data. We then inspect the top K neighbors for K ∈ {3  5  10} returned by the methods under
consideration  and for each K we report the average fraction of true K neighbors that have been
retrieved over 100 respectively 20 replications  where for the real data  we also average over the
chosen queries (366 for farm and 160 for RCV1).
The results of our experiments point to several conclusions that can be summarized as follows.
One-bit quantization is consistently outperformed by higher-bit quantization. The optimal choice of b
depends on the underlying similarities  and interacts with the choice of t. It is an encouraging result
that the performance based on full precision data (with k as for b = 6) can essentially be matched

j=1 with the query.

7

678910111213log2(bits)0.50.60.70.80.91fraction retrievedsynthetic  K = 10 b=1b=2b=3b=4b=5b=6b=∞oracle678910111213log2(bits)0.60.650.70.750.80.850.90.951fraction retrievedfarm  K = 10 b=1b=2b=3b=4b=5b=6b=∞oracle678910111213log2(bits)0.70.750.80.850.90.951fraction retrievedrcv1  K = 10 b=1b=2b=3b=4b=5b=6b=∞oracleFigure 5: Average fraction of K = 10 nearest neighbors retrieved vs. total # of bits (log2 scale) of our
approach (MLE) relative to that based on the Hamming distance and the linear approach for b = 2  4.

when quantized data is used. For b = 2  the performance of the MLE is only marginally better than
the approach based on the Hamming distance. The superiority of the former becomes apparent once
b ≥ 4 which is expected since for increasing b the Hamming distance is statistically inefﬁcient as it
only uses the information whether a pair of quantized data agrees/disagrees. Some of these ﬁndings
are reﬂected in Figures 4 and 5. We refer to the supplement for additional ﬁgures.
4.2 Linear Classiﬁcation
i(cid:105))1≤i i(cid:48)≤n from (cid:98)G =
We here outline an application to linear classiﬁcation given features generated by (quantized) random
((cid:98)gii(cid:48))  where for i (cid:54)= i(cid:48) (cid:98)gii(cid:48) =(cid:98)ρMLE(qi  qi(cid:48)) equals the MLE of (cid:104)xi  x(cid:48)
projections. We aim at reconstructing the original Gram matrix G = ((cid:104)xi  x(cid:48)
i  and(cid:98)gii(cid:48) = 1 else (assuming normalized data). The matrix (cid:98)G is subsequently fed into LIBSVM.
i(cid:105) given a quantized data pair
qi  q(cid:48)
For testing  the inner products between test and training pairs are approximated accordingly.
Setup. We work with the farm data set using the ﬁrst 3 000 samples for training  and the Arcene
data set from the UCI repository with 100 training and 100 test samples in dimension d = 104. The
choice of k and b is as in §4.1; for arcene  the total bit budget is lowered by a factor of 2. We perform
20 independent replications for each combination of k and b. For SVM classiﬁcation  we consider
logarithmically spaced grids between 10−3 and 103 for the parameter C (cf. LIBSVM manual).

Figure 6: (L  M): accuracy vs. bits  optimized over the SVM parameter C. (R) accuracy vs. C for a
ﬁxed # bits. b = ∞ indicates the performance based on unquantized data with k as for b = 6. The
oracle curve (dotted) corresponds to b = ∞ with maximum k (i.e.  as for b = 1).
Figure 6 (L  M) displays the average accuracy on the test data (after optimizing over C) in dependence
of the bit budget. For the farm Ads data set  b = 2 achieves the best trade-off  followed by b = 1 and
b = 3. For the Arcene data set  b = 3  4 is optimal. In both cases  it does not pay off to go for b ≥ 5.
5 Conclusion
In this paper  we bridge the gap between random projections with full precision and random pro-
jections quantized to a single bit. While Theorem 1 indicates that an exact counterpart to the J-L
lemma is not attainable  other theoretical and empirical results herein point to the usefulness of the
intermediate cases which give rise to an interesting trade-off that deserves further study in contexts
where random projections can naturally be applied e.g. linear learning  nearest neighbor classiﬁcation
or clustering. The optimal choice of b eventually depends on the application: increasing b puts an
emphasis on local rather than global similarity preservation.

8

678910111213log2(bits)0.60.650.70.750.80.850.90.95fraction retrievedfarm  b = 2  K = 10 MLEHammingLinear678910111213log2(bits)0.40.50.60.70.80.91fraction retrievedfarm  b = 4  K = 10 MLEHammingLinear678910111213log2(bits)0.750.80.850.90.951fraction retrievedrcv1  b = 2  K = 10 MLEHammingLinear678910111213log2(bits)0.40.50.60.70.80.91fraction retrievedrcv1  b = 4  K = 10 MLEHammingLinear8910111213log2(bits)0.70.750.80.850.9accuracy on test setfarmb=1b=2b=3b=4b=5b=6b=∞oracle789101112log2(bits)0.70.750.80.85accuracy on test setarceneb=1b=2b=3b=4b=5b=6b=∞oracle00.511.522.5log10(C parameter)0.650.70.750.80.85accuracy on test setarcene  total #bits = 210b=1b=2b=3b=4b=5b=6b=∞oracleAcknowledgement
The work of Ping Li and Martin Slawski is supported by NSF-Bigdata-1419210 and NSF-III-1360971.
The work of Michael Mitzenmacher is supported by NSF CCF-1535795 and NSF CCF-1320231.
References
[1] E. Bingham and H. Mannila. Random projection in dimensionality reduction: applications to image and

text data. In Conference on Knowledge discovery and Data mining (KDD)  pages 245–250  2001.

[2] C. Boutsidis  A. Zouzias  and P. Drineas. Random Projections for k-means Clustering. In Advances in

Neural Information Processing Systems (NIPS)  pages 298–306. 2010.

[3] C-C. Chang and C-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent

Systems and Technology  2:27:1–27:27  2011. http://www.csie.ntu.edu.tw/~cjlin/libsvm.

[4] M. Charikar. Similarity estimation techniques from rounding algorithms. In Proceedings of the Symposium

on Theory of Computing (STOC)  pages 380–388  2002.

[5] S. Dasgupta. Learning mixtures of Gaussians. In FOCS  pages 634–644  1999.
[6] S. Dasgupta. An elementary proof of a theorem of Johnson and Lindenstrauss. Random Structures and

Algorithms  22:60–65  2003.

[7] D. Fradkin and D. Madigan. Experiments with random projections for machine learning. In Conference on

Knowledge discovery and Data mining (KDD)  pages 517–522  2003.

[8] A. Genz. BVN: A function for computing bivariate normal probabilities. http://www.math.wsu.edu/

faculty/genz/homepage.

[9] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality.

In Proceedings of the Symposium on Theory of Computing (STOC)  pages 604–613  1998.

[10] L. Jacques. A Quantized Johnson-Lindenstrauss Lemma: The Finding of Buffon’s needle. IEEE Transac-

tions on Information Theory  61:5012–5027  2015.

[11] L. Jacques  K. Degraux  and C. De Vleeschouwer. Quantized iterative hard thresholding: Bridging 1-bit

and high-resolution quantized compressed sensing. arXiv:1305.1786  2013.

[12] W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary

Mathematics  pages 189–206  1984.

[13] J. Kieffer. Uniqueness of locally optimal quantizer for log-concave density and convex error weighting

function. IEEE Transactions on Information Theory  29:42–47  1983.

[14] J. Laska and R. Baraniuk. Regime change: Bit-depth versus measurement-rate in compressive sensing.

IEEE Transactions on Signal Processing  60:3496–3505  2012.

[15] M. Li  S. Rane  and P. Boufounos. Quantized embeddings of scale-invariant image features for mobile
augmented reality. In International Workshop on Multimedia Signal Processing (MMSP)  pages 1–6  2012.
[16] P. Li  T. Hastie  and K. Church. Improving Random Projections Using Marginal Information. In Annual

Conference on Learning Theory (COLT)  pages 635–649  2006.

[17] P. Li  M. Mitzenmacher  and A. Shrivastava. Coding for Random Projections. In Proceedings of the

International Conference on Machine Learning (ICML)  2014.

[18] M. Lopes  L. Jacob  and M. Wainwright. A More Powerful Two-Sample Test in High Dimensions using
Random Projection. In Advances in Neural Information Processing Systems 24  pages 1206–1214. 2011.
[19] O. Maillard and R. Munos. Compressed least-squares regression. In Advances in Neural Information

Processing Systems (NIPS)  pages 1213–1221. 2009.

[20] J. Max. Quantizing for Minimum Distortion. IRE Transactions on Information Theory  6:7–12  1960.
[21] L. Shenton and K. Bowman. Higher Moments of a Maximum-likelihood Estimate. Journal of the Royal

Statistical Society  Series B  pages 305–317  1963.

[22] R. Srivastava  P. Li  and D. Ruppert. RAPTT: An exact two-sample test in high dimensions using random

projections. Journal of Computational and Graphical Statistics  25(3):954–970  2016.
[23] S. Vempala. The Random Projection Method. American Mathematical Society  2005.
[24] F. Wang and P. Li. Efﬁcient nonnegative matrix factorization with random projections. In SDM  pages

281–292  Columbus  Ohio  2010.

9

,Jean-Baptiste SCHIRATTI
Stéphanie ALLASSONNIERE
Olivier Colliot
Stanley DURRLEMAN
Ping Li
Michael Mitzenmacher
Martin Slawski
Avrim Blum
Nika Haghtalab
Ariel Procaccia
Mingda Qiao