2009,Who’s Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation,Given a corpus of news items consisting of images accompanied by text captions  we want to find out ``whos doing what  i.e. associate names and action verbs in the captions to the face and body pose of the persons in the images. We present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus. These models can then be used to recognize people and actions in novel images without captions. We demonstrate experimentally that our joint `face and pose model solves the correspondence problem better than earlier models covering only the face  and that it can perform recognition of new uncaptioned images.,Who’s Doing What: Joint Modeling of Names and
Verbs for Simultaneous Face and Pose Annotation

Luo Jie

Idiap and EPF Lausanne

jluo@idiap.ch

Barbara Caputo

Idiap Research Institute
bcaputo@idiap.ch

Vittorio Ferrari

ETH Zurich

ferrari@vision.ee.ethz.ch

Abstract

Given a corpus of news items consisting of images accompanied by text captions 
we want to ﬁnd out “who’s doing what”  i.e. associate names and action verbs in
the captions to the face and body pose of the persons in the images. We present
a joint model for simultaneously solving the image-caption correspondences and
learning visual appearance models for the face and pose classes occurring in the
corpus. These models can then be used to recognize people and actions in novel
images without captions. We demonstrate experimentally that our joint ‘face and
pose’ model solves the correspondence problem better than earlier models cover-
ing only the face  and that it can perform recognition of new uncaptioned images.

Introduction

1
A huge amount of images with accompanying text captions are available on the Internet. Websites
selling various items such as houses and clothing provide photographs of their products along with
concise descriptions. Online newspapers 1 have pictures illustrating events and comment them in
the caption. These news websites are very popular because people are interested in other people 
especially if they are famous (ﬁgure 1). Exploiting the associations between images and text hidden
in this wealth of data can lead to a virtually inﬁnite source of annotations from which to learn visual
models without explicit manual intervention.

The learned models could then be used in a variety of Computer Vision applications  including face
recognition  image search engines  and to annotate new images for which no caption is available.
Moreover  recovering image-text associations is useful for auto-annotating a closed corpus of data 
e.g. for users of news website to see “who’s in the picture” [6]  or to search for images where a
certain person does a certain thing.

Previous works on news items has focused on associating names in the captions to faces in the im-
ages [5  6  16  21]. This is difﬁcult due to the correspondence ambiguity problem: multiple persons
appear in the image and the caption. Moreover  persons in the image are not always mentioned in the
caption  and not all names in the caption appear in the image. The techniques tackle the correspon-
dence problem by exploiting the fact that different images show different combinations of persons.
As a result  these methods work well for frequently occurring persons (typical for famous people)
appearing in dataset with thousands of news items.

In this paper we propose to go beyond the above works  by modeling both names and action verbs
jointly. These correspond to faces and body poses in the images (ﬁgure 3). The connections be-
tween the subject (name) and verb in a caption can be found by well established language analysis
techniques [1  8]. Essentially  by considering the subject-verb language construct  we generalize the
“who’s in the picture” line of works to “who’s doing what”. We present a new generative model
where the observed variables are names and verbs in the caption as well as detected persons in the
image. The image-caption correspondences are carried by latent variables  while the visual appear-
ance of face and pose classes corresponding to different names and verbs are model parameters.
During learning  we simultaneously solve for the correspondence and learn the appearance models.

1www.daylife.com  news.yahoo.com 

news.google.com

1

(a) Four sets ... Roger Federer prepares
to hit a backhand in a quarter-ﬁnal match
with Andy Roddick at the US Open.

(b) US Democratic presidential candidate Senator Barack
Obama waves to supporters together with his wife Michelle
Obama standing beside him at his North Carolina and In-
diana primary election night rally in Raleigh.

Figure 1: Examples of image-caption pairs in our dataset. The face and upper body of the persons in the image
are marked by bounding-boxes. We stress a caption might contain names and/or verbs not visible in the image 
and vice versa.

In our joint model  the correspondence ambiguity is reduced because the face and pose information
help each other. For example  in ﬁgure 1b  knowing what ‘waves’ means would reveal who of the
two imaged persons is Obama. The other way around  knowing who is Obama would deliver a
visual example for the ‘waving’ pose.

We show experimentally that (i) our joint ‘face and pose’ model solves the correspondence problem
better than simpler models covering either face or pose alone; (ii) the learned model can be used to
effectively annotate new images with or without captions; (iii) our model with face alone performs
better than the existing face-only methods based on Gaussian mixture appearance models.

Related works. This paper is most closely related to works on associating names and faces  which
we discussed above. There exist also works on associating nouns to image regions [2  3  10]  starting
from images annotated with a list of nouns indicating the objects it contains (typical datasets contain
natural scenes and objects such as ‘water’ and ‘tiger’). A recent work in this line is that of Gupta
and Davis [17]  who model prepositions in addition to nouns (e.g. ‘bear in water’  ‘car on street’).
To the best of our knowledge  ours is the ﬁrst work on jointly modeling names and verbs.
2 Generative model for faces and body poses
The news item corpus used to train our face and pose model consists of still images of person(s)
performing some action(s). Each image is annotated with a caption describing “who’s doing what”
in the image (ﬁgure 1). Some names from the caption might not appear in the image  and vice-
versa some imaged persons might not be mentioned in the caption. The basic units in our model
are persons in the image  consisting of their face and upper body. Our system automatically detects
them by bounding-boxes in the image using a face detector [23] and an upper body detector [14].
In the rest of the paper  we say “person” to indicate a detected face and the upper body associated
with it (including false positive detections). A face and an upper-body are considered to belong to
the same person if the face lies near the center of the upper body bounding-box. For each person 
we obtain a pose estimate using [11] (ﬁgure 3(right)). In addition to these image features  we use
a language parser [1  8] to extract a set of name-verb pairs from each caption. Our goals are to:
(i) associate the persons in the images to the name-verb pairs in the captions  and (ii) learn visual
appearance models corresponding to names and verbs. These can then be used for recognition on
new images with or without caption. Learning in our model can be seen as a constrained clustering
problem [4  24  25].

2.1 Generative model
We start by describing how our generative model explains the image-caption data (ﬁgure 2). The
notation is summarized in Table I. Suppose we have a collection of documents D = {D1  . . .   DM }
with each document Di consisting of an image I i and its caption C i. These captions implicitly
provide the labels of the person(s)’ name(s) and pose(s) in the corresponding images. For each
caption C i  we consider only the name-verb pairs ni returned by a language parser [1  8] and ignore
other words. We make the same assumptions as for the name-face problem [5  6  16  21] that the
labels can only come from the name-verb pairs in the captions or null (for persons not mentioned
in the caption). Based on this  we generate the set of all possible assignments Ai from the ni in

2

i=1 = {I i  C i}i=M

i=1

I i p: pth person in image I i
I i p = (I i p

face   I i p
pose)

M: Number of documents in D (image-caption pairs) D = {Di}i=M
P i: Number of detected persons in image I i
W i: Number of name-verb pairs in caption C i
Y : Latent variables encoding the true assignments
Y i: Y i = (yi 1  . . .   yi P i
Ai: Set of possible assignments for document i
Li: Number of possible assignments for document Di
l: lth assignment ai
ai
Θ: Appearance models for face and pose classes
V : Number of different verbs
U: Number of different names
θk: Sets of class representative vectors for class k
θv
verb = {µv 1

pose   . . .   µv Rv
pose }

  . . .   ai P i

}  where ai p

l = {ai 1

Ai = {ai

l

l

l

is the label for the pth person
Θ = (θname  θverb)
θverb = (θ1
θname = (θ1
µk
r : a representative vector for class k
name = {µu 1
θu

verb  . . .   θV
name  . . .   θU

verb  βverb)
name  βname)

face   . . .   µu Ru
face }

)  yi p is the assignment of the pth person in ith image

1  . . .   ai

Li }

Table I: The mathematical notation used in the paper

θ

Verb

V

θ

name

U

I

Y

P

W

C

A

L

M

Figure 2: Graphical
plate representation of
the generative model.

C i (see section 2.4 for details). Hence  we replace the captions by the sets of possible assignments
A = {A1  . . .   AM }. Let Y = {Y 1  . . .   Y M } be latent variables encoding the true assignments
(i.e. name/verb labels for the faces/poses)  and Y i = (yi 1  . . .   yi P i
) be the assignment for the P i
persons in the ith image. Each yi p = (yi p
pose) is a pair of indices deﬁning the assignment of
a person’s face to a name and pose to a verb. These take on values from the set of name indices
{1  . . .   U  null}  and verb indices {1  . . .   V  null}. N/V is the number of different names/verbs
over all the captions and null represents unknown names/verbs and false positive person detections.
Document collection likelihood. Assuming independence between documents  the likelihood of
the whole document collection is

face  yi p

P (I  Y   A|Θ) =

M

Y

i=1

P (I

i

  Y

i

  Ai|Θ) =

M

Y

i=1

P (I

i|Y

i

  Ai

  Θ)P (Y

i|Ai

  Θ)P (Ai|Θ)

(1)

where Θ are the model parameters explaining the visual appearance of the persons’ faces and poses

in the images. Therefore  equation (1) can be written asQ P (I i|Y i  Θ)P (Y i|Ai)P (Ai). The goal

of learning is to ﬁnd the parameters Θ and the labels Y that maximize the likelihood. Below we
focus on P (I i|Y i  Θ)  and then deﬁne P (Y i|Ai) and P (Ai) in section 2.4.

Image likelihood. The basic image units in our model are persons. Assuming independence be-
tween multiple persons in an image  the likelihood of an image can be expressed as the product over
the likelihood of each person:

P (I i|Y i  Θ) = YI i p∈I i

P (I i p|yi p  Θ)

(2)

where yi p deﬁne the name-verb indices of the pth person in the image. A person I i p = (I i p
pose)
is represented by the appearance of her face I i p
pose. Assuming independence between
the face and pose appearance of a person  the conditional probability for the appearance of the pth
person in image I i given the latent variable yi p is:
face|yi p

P (I i p|yi p  Θ) = P (I i p

face and pose I i p

face  θname)P (I i p

pose  θverb)

face  I i p

pose|yi p

(3)

verb in θverb = (θ1

where Θ = (θname  θverb) are the appearance models associated with the various names and verbs.
Each θv
verb  βverb) is a set of representative vectors modeling the variability
within the pose class corresponding to a verb v. For example  the verb “serve” in tennis could
correspond to different poses such as holding the ball on the racket  tossing the ball and hitting it.
Analogously  θu

name models the variability within the face class corresponding to a name u.

verb  . . .   θV

2.2 Face and pose descriptors and similarity measures
After detecting faces from the images with the multi-view algorithm [23]  we use [12] to detect nine
distinctive feature points within the face bounding box (ﬁgure 3(left)). Each feature is represented by
SIFT descriptors [18]  and their concatenation gives the overall descriptor vector for the face. We use
the cosine as a naturally normalized similarity measure between two face descriptors: simface(a  b) =
aT b
kak kbk . The distance between two faces is distface(a  b) = 1 − simcos(a  b).
We use [14] to detect upper-bodies and [11] to estimate their pose. A pose E consists of a distri-
bution over the position (x  y and orientation) for each of 6 body parts (head  torso  upper/lower

3

Figure 3: Example images with facial features and pose estimates superimposed. Left Facial features (left
and right corners of each eye  two nostrils  tip of the nose  and the left and right corners of the mouth) located
using [12] in the detected face bounding-box. Right Example estimated poses corresponding to verbs: “hit
backhand”  “shake hands” and “hold”. Red indicates torso  blue upper arms  green lower arms and head.
Brighter pixels are more likely to belong to a part. Color planes are added up  so that yellow indicates overlap
between lower-arm and torso  purple between upper-arm and torso  and so on (best viewed in color).
left/right arms). The pose estimator factors out variations due to clothing and background  so E
conveys purely spatial arrangements of body parts. We derive three relatively low-dimensional pose
descriptors from E  as proposed in [13]. These descriptors represent pose in different ways  such as
the relative position between pairs of body parts  and part-speciﬁc soft-segmentations of the image
(i.e. the probability of pixels as belonging to a part). We refer to [13  11] for more details and the
similarity measure associated with each descriptor. We normalize the range of each similarity to
[0  1]  and denote their average as simpose(a  b). The ﬁnal distance between two poses a  b used in
the rest of this paper is distpose(a  b) = 1 − simpose(a  b).

2.3 Appearance model
The appearance model for a pose class (corresponding to a verb) is deﬁned as:
pose|θk

pose  k) · P (I i p

pose|yi p

P (I i p

δ(yi p

verb)

pose  θverb) = Xk∈{1 ... V null}

where θk
δ(yi p
pose class  as the face model is derived analogously.

verb are the parameters of the kth pose class (or βverb if k = null). The indicator function
pose  k) = 0 otherwise. We only explain here the model for a

pose = k and δ(yi p

pose  k) = 1 if yi p

(4)

pose|θk

How to model the conditional probability P (I i p
verb) is a key ingredient for the success of our
approach. Some previous works on names-faces used a Gaussian mixture model [6  21]: each name
is associated with a Gaussian density  plus an additional Gaussian to model the null class. Using
functions of the exponential family like a Gaussian simpliﬁes computations. However  a Gaussian
may restrict the representative power of the appearance model. Problems such as face and pose
recognition are particularly challenging because they involve complex non-Gaussian multimodal
distributions. Figure 3(right) shows a few examples of the variance within the pose class for a verb.
Moreover  we cannot easily employ existing pose similarity measures [13]. Therefore  we represent
the conditional probability using a exemplar-based likelihood function:

P (I i p

pose|θk

verb) =( 1

1

Zθverb

Zθverb

pose θk

verb)

e−dpose(I i p
e−βverb

if k ∈ {known verbs}
if k = null

(5)

r ∈ θk

verb = {µk 1

pose  . . .   µk Rk

is the normalizer and dpose is the distance between the pose descriptor I i p

where Zθverb
pose and its
pose }  where Rk is the number of
closest class representative vector µk
representative poses for verb k. The likelihood depends on the model parameters θk
verb  and the
distance function dpose. The scalar βverb represents the null model  thus poses assigned to null have
likelihood
e−βverb. It is important to have this null model  as some detected persons might not
correspond to any verb in the caption or they might be false detections. By generalizing the similarity
measure simpose(a  b) as a kernel product K(a  b) = φ(a) · φ(b)  the distance from a vector a to the
sample center vector µk

r can be written similarly as in the weighted kernel k-means method [9]:
r w(b)w(d)k(b  d)

r w(b)k(a  b)

r w(b)φ(b)

Σb d∈πk

2Σb∈πk

Σb∈πk

Zθverb

2

1

= K(a  a) −

+

(Σb∈πk

r w(b))2

(6)

φ(a) −

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Σb∈πk

r w(b) (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

Σb∈πk

r w(b)

4

r w(b)φ(b)(cid:1)/(cid:0)Σb∈πk

r is deﬁned as (cid:0)Σb∈πk

The center vector µk
r is the cluster of
r   and w(b) is the weight for each point b  representing the likelihood that b
vectors assigned to µk
belongs to the class of µk
r (as in equation (11)). This formulation can be considered as a modiﬁed
version of the k-means [19] clustering algorithm. The number of centers Rk can vary for different
verbs  depending on the distribution of the data and the number of samples. As we are interested
only in computing the distance between µk
r and each data point  and not in the explicit value of µk
r  
the only term that needs to be computed in equation (6) is the second (the third term is constant for
each assigned µk

r w(b)(cid:1)  where πk

r ).

2.4 Name-verb assignments
The name-verb pairs ni for a document are observed in its caption C i. We derive from them the set
} of name-verb pairs to persons in the image. The
of all possible assignments Ai = {ai
number of possible assignments Li depends both on the number of persons and of name-verb pairs.
As opposed to the standard matching problem  here the assignments have to take into account null.
Moreover  we have the same constraints as in the name-face problem [6]: a person can be assigned
to at most one name-verb pair  and vice-versa. Therefore  given a document with P i persons and

1  . . .   ai
Li

W i name-verb pairs  the number of possible assignments is Li =Pmin(P i W i)

j is the number of persons assigned to a name-verb pair instead of null. Even by imposing the
above constraints  this number grows rapidly with P i and W i. However  since different assignments
share many common sub-assignments  the number of unique likelihood computations is much lower 
namely P i · (W i + 1). Thus  we can evaluate all possible assignments for an image efﬁciently.
Although certain assignments are unlikely to happen (e.g. all persons are assigned to null)  here we
use an uniform prior over all assignments  i.e. P (ai
l) = 1/Li. Since the true assignment Y i can
only come from Ai  we deﬁne the conditional probability over the latent variables Y i as:

j (cid:1) ·(cid:0)W i
(cid:0)P i

j (cid:1)  where

j=0

P (Y i|Ai) =(cid:26)1/Li

0

if Y i ∈ Ai
otherwise

(7)

The latent assignment Y i play the role of the annotations necessary for learning appearance models.
3 Learning the model
The task of learning is to ﬁnd the model parameters Θ and the assignments Y which maximize
the likelihood of the complete dataset {I  Y   A}. The joint probability of {I  Y   A} given Θ from
equation (1) can be written as

P (I  Y   A|Θ) =

M

Yi=1


P (Y i|Ai)P (Ai)

P i

Yp=1

P (I i p

face|yi p

face  θname)P (I i p

pose|yi p

(8)

pose  θverb)


Maximizing the log of this joint likelihood is equivalent to minimizing the following clustering
objective function over the latent variables Y and parameters Θ:

dface(I i p

face  θ

yi p
face

face 6=null

J = Xi p yi p
+ Xi p yi p

pose=null

face =null

name) + Xi p yi p
βname + Xi p yi p
(logP (Y i|Ai) + logP (Ai)) +Xi p

pose6=null

βverb −Xi

dpose(I i p

pose  θ

yi p
pose
verb )

(9)

(logZθname + logZθverb)

Thus  to minimize J   each latent variable Y i must belong to the set of possible assignments Ai. If
Y would be known  the cluster centers µ ∈ θname  µ ∈ θverb which minimize J could be determined
uniquely (given also the number of class centers R). However  it is difﬁcult to set R before seeing
the data. In our implementation  we determine the centers approximately using the data points and
their K nearest neighbors. Since estimating the normalization constants Zθname and Zθverb is compu-
tationally expensive  we make an approximation by considering them as constant in the clustering
process (i.e. drop their terms from J ).
In our experiments  this did not signiﬁcantly affect the
results  as also noted in several other works (e.g. [4]).

Since the assignments Y are unknown  we use a generalized EM procedure [7  22] for simultane-
ously learning the parameters Θ and solving the correspondence problem (i.e. ﬁnd Y ):

5

Name
Ass.

Verb
Ass.

Name
Ass.

Verb
Ass.

 

 GMM Face
 Model Face
 Model Face+Pose
 Model Pose

Name
Ass.

Verb
Ass.

1

0.95

0.9

0.85

0.8

0.75

0.7

i

i

n
o
s
c
e
r
P

]

%

[
 
y
c
a
r
u
c
c
A

90

85

80

75

70

65

60

55

50

45

40

 

a. ground−truth

b. automated 

c. multiple

 Model Face
 Model Face+Pose

0.2

0.3

0.4

0.65

 
0.1

B. Clinton

B. Clinton

S. Williams S. Williams

A. Agassi

R. Federer

 

A. Agassi

R. Federer

J. Jankovic

J. Jankovic

G. Bush

G. Bush

K. Garnett

N. Sarkozy

T. Woods

A. Merkel

T. Woods

N. Sarkozy

A. Merkel

K. Garnett

0.5

0.6

Recall

0.7

0.8

0.9

1

Figure 4: Left. Comparison of different models under different setups: using the manually annotated name-
verb pairs (ground-truth); using the Named Entity detector and language parser (automated); and using the
more difﬁcult subset (multiple). The accuracy for name (Name Ass.) and verb (Verb Ass.) assignments
are reported separately. GMM Face refers to the face-only model using GMM appearance models  as in [6].
Right. Comparison of precision and recall for 10 individuals using the stripped-down face only model  and our
face+pose model. The reported results are based on automatically parsed captions for learning.

Input. Data D; hyper-parameters βname  βverb  K
1. Initialization. We start by computing the distance matrix between faces/poses from images
sharing some name/verb in the caption. Next we initialize Θ using all documents in D. For each
different name/verb  we select all captions containing only this name/verb. If the corresponding
verb.
images contain only one person  their faces/poses are used to initialize the center vectors θk
The center vectors are found approximately using each data point and their K nearest neighbors
of the same name/verb class. If a name/verb only appears in captions with multiple names/verbs
or if the corresponding images always contain multiple persons (e.g. verbs like “shake hand”) 
we randomly assign the name/verb to any face/pose in each image. The center vectors are then
initialized using these data points. The initial weights w for all data points are set to one (equation 6).
This step yields an initial estimate of the model parameters Θ. We reﬁne the parameters and assign-
ments by repeating the following EM-steps until convergence.
2. E-step. Compute the labels Y using the parameters Θold from the previous iteration

name/θk

arg max

P (Y |I  A  Θold) ∝ arg max

P (I|Y   Θold)P (Y |A)

(10)

Y

Y

3. M-step. Given the labels Y   update Θ so as to minimize J (i.e. update the cluster centers µ).
Our algorithm assigns each point to exactly one cluster. Each point I i p in a cluster is given a weight

wi p

Y i =

P (Y i|I i p  Ai  Θ)

PY j ∈Ai P (Y j|I i p  Ai  Θ)

(11)

which represents the likelihood that I i p
pose belong to the name and verb deﬁned by Y i.
Therefore  faces and poses from images with many detections have a lower weights and contribute
less to the cluster centers  reﬂecting the larger uncertainty in their assignments.

face and I i p

4 Experiments and conclusions
Datasets There are datasets of news image-caption pairs such as those in [6  16]. Unfortunately 
these datasets are not suitable in our scenario for two reasons. Faces often occupy most of the image
so the body pose is not visible. Second  the captions frequently describe the event at an abstract
level  rather than using a verb to describe the actions of the persons in the image (compare ﬁgure 1
to the ﬁgures in [6  16]). Therefore  we collected a new dataset 2 by querying Google-images using
a combination of names and verbs (from sports and social interactions)  corresponding to distinct
upper body poses. An example query is “Barack Obama” + “shake hands”. Our dataset contains
1610 images  each with at least one person whose face occupies less than 5% of the image  and with
the accompanying snippet of text returned by Google-images. External annotators were asked to

2We released this dataset online at http://www.vision.ee.ethz.ch/∼ferrari

6

C:

F::
FP:

R. Nadal - clench ﬁst
E. Gulbis - null
E. Gulbis
R. Nadal

K. Garnett - hold
Celtics - null
Celtics
K. Garnett

J. Jankovic - serve
M. Bartoli - null
null
J. Jankovic

J. Jankovic - hold
D. Saﬁna - null
D. Saﬁna
J. Jankovic

R. Nadal - null
R. Federer - hit forehand
R. Nadal; null
R. Federer; null

C:

V. Williams - hit backhand
S. Williams - hold

R. Nadal - hit forehand

F::
FP:

V. Williams
S. Williams

null
R. Nadal

C. Clinton - clap
B. Clinton - kiss
H. Clinton - kiss
C. Clinton
null

N. Sarkozy - embrace
Brian Cowen - null

Hu Jintao - Wave
R. Venables - wave

Brian Cowen
N. Sarkozy

null
Hu Jintao

Hu Jintao - shake hands
J. Chirac - shake hands

Hu Jintao - shake hands
N. Sarkozy - shake hands

A. Garcia - toast
A. Merkel - drink

A. Merkel - gesture

null;null;null
null;null;Hu Jintao

null;Hu Jintao
N. Sarkozy; Hu Jintao

A. Merkel
A. Garcia

null;null;A. Merkel
A. Merkel;null;null;

C:

F::
FP:

Hu Jintao - shake hands
K. Bakjyev - shake hands
Kyrgyzstan - null
Hu Jintao;null
Hu Jintao;K. Bakjyev

Figure 5: Examples of when modeling pose improves the results at learning time. Below the images we report
the name-verb pairs (C) from the caption as returned by the automatic parser and compare the association
recovered by a model using only faces (F) and using both faces and poses (FP). The assigned names (left to
right) correspond to the detected face bounding-boxes (left to right).

]

%

[
 
y
c
a
r
u
c
c
A

0

110

100

90

80

70

60

50

40

30

20

10

 

1

Image Query Keywords
2
4

3

5

6
 

 Baseline on Face Annoation (with caption)
 Face Model on Face Annoation (with caption)
 Face + Pose Model on Face Annotation (with caption)
 Face + Pose Model on Face Annotation (without caption)
 Face + Pose Model on Pose Annotation (without caption)

Federer
Backhand

Sharapova
Hold trophy

Nadal

Forehand

Obama
Wave

Hu Jintao
Shakehands

Wave

Shake Hand

Hold

Wave

J. Jankoviv

B. Obama

B. Obama

B. Obama

NULL

Shake Hands

R. Federer

Hit Backhand

NULL

Hu Jintao

NULL

Hu Jintao

Shake Hands

Shake Hands

Shake Hands

M.Sharapova

Hold

Figure 6: Recognition results on images without text captions (using models learned from automatically parsed
captions). Left compares face annotation using different models and scenarios (see main text); Right shows a
few examples of the labels predicted by the joint face and pose model (without using captions).

extend these snippets into realistic captions when necessary  with varied long sentences  mentioning
the action of the persons in the image as well as names/verbs not appearing in the image (as ‘noise’ 
ﬁgure 1). Moreover  they also annotated the ground-truth name-verb pairs mentioned in the captions
as well as the location of the target persons in the images  enabling to evaluate results quantitatively.
In total the ground-truth consists of 2627 name-verb pairs. In our experiments we only consider

7

names and verbs occurring in at least 3 captions for a name  and 20 captions for a verb. This leaves
69 names corresponding to 69 face classes and 20 verbs corresponding to 20 pose classes.

We used an open source Named Entity recognizer [1] to detect names in the captions and a language
parser [8] to ﬁnd name-verbs pairs (or name-null if the language parser could not ﬁnd a verb as-
sociated with a name). By using simple stemming rules  the same verb under different tenses and
possessive adjectives was merged together. For instance “shake their hands”  “is shaking hands” and
“shakes hands” all correspond to the action verb “shake hands”. In total  the algorithms achieves
precision 85.5% and recall 68.8% on our dataset over the ground-truth name-verb pair. By discard-
ing infrequent names and verbs as explained above  we retain 85 names and 20 verbs to be learned by
our model (recall that some of these are false positives rather than actual person names and verbs).

Results for learning The learning algorithm takes about ﬁve iterations to converge. We compare
experimentally our face and pose model to stripped-down versions using only face or pose informa-
tion. For comparison  we also implement the constrained mixture model [6] described in section 2.3.
Although [6] also originally incorporates also a language model of the caption  we discard it here
so that both methods use the same amount of information. We run the experiments in three setups:
(a) using the ground-truth name-verb annotations from the captions; (b) using the name-verb pairs
automatically extracted by the language parser; (c) similar as (b) but only on documents with multi-
ple persons in the image or multiple name-verb pairs in the caption. These setups are progressively
more difﬁcult  as (b) has more noisy name-verb pairs  and (c) has no documents with a single name
and person  where our initialization is very reliable.

Figure 4(left) compares the accuracy achieved by different models on these setups. The accuracy is
deﬁned as the percentage of correct assignments over all detected persons  including assignments to
null  as in [5  16]. As the ﬁgure shows  our joint ‘face and pose’ model outperforms both models
using face or pose alone in all setups. Both the annotation of faces and poses improve  demonstrating
they help each other when successfully integrated by our model. This is the main point of the
paper. Figure 4(right) shows improvements on precision and recall over models using faces or poses
alone. As a second point  our model with face alone also outperforms the baseline approach using
Gaussian mixture appearance models (e.g. used in [6]). Figure 5 shows a few examples of how
including pose improves the learning results and solve some of the correspondence ambiguities.
Improvements happen mainly in three situations: (a) when there are multiple names in a caption  as
not all names in the captions are associated to action verbs (ﬁgure 1(a) and ﬁgure 5(top)); (b) when
there are multiple persons in an image  because the pose disambiguates the assignment (ﬁgure 1(b)
and ﬁgure 5(bottom)) and (c) when there are false detections  rare faces or faces at viewpoints
different than frontal (i.e. where face recognition works less well  e.g. ﬁgure 5(middle)).
Results for recognition Once the model is learned  we can use it to recognize “who’s doing what”
in novel images with or without captions. We collected a new set of 100 images and captions from
Google-images using ﬁve keywords based on names and verbs from the training dataset. We evaluate
the learned model in two scenarios: (a) the test data consists of images and captions. Here we run
inference on the model  recovering the best assignment Y from the set of possible assignments
generated from the captions; (b) the same test images are used but the captions are not given  so
the problem degenerates to a standard face and pose recognition task. Figure 6(left) reports face
annotation accuracy for three methods using captions (scenario (a)): (⋄) a baseline which randomly
assigns a name (or null) from the caption to each face in the image; (x) our face and pose model; ((cid:3))
our model using only faces. The ﬁgure also shows results for scenario (b)  where our full model tries
to recognize faces (+) and poses (△) in the test images without captions. On scenario (a) all models
outperform the baseline  and our joint face and pose model improves signiﬁcantly on the face-only
model for all keywords  especially when there are multiple persons in the image.
Conclusions. We present an approach for the joint modeling of faces and poses in images and
their association to names and action verbs in accompanying text captions. Experimental results
show that our joint model performs better than face-only models both in solving the image-caption
correspondence problem on the training data  and in annotating new images. Future work aims at
incorporating an effective web crawler and html/language parsing tools to harvest image-caption
pairs from the internet fully automatically. Other techniques such as learning distance functions [4 
15  20] may also be incorporated during learning to improve recognition results.

Acknowledgments We thank K. Deschacht and M.F. Moens for providing the language parser. L. J. and B.
Caputo were supported by EU project DIRAC IST-027787 and V. Ferrari by the Swiss National Science Found.

8

References

[1] http://opennlp.sourceforge.net/.
[2] K. Barnard  P. Duygulu  D. Forsyth  N. de Freitas  D. Blei  and M. Jordan. Matching words

and pictures. JMLR  3:1107–1135  2003.

[3] K. Barnard and Q. Fan. Reducing correspondence ambiguity in loosely labeled training data.

In Proc. CVPR’07.

[4] S. Basu  M. Bilenko  A. Banerjee  and R. J. Mooney. Probabilistic semi-supervised cluster-
In O. Chapelle  B. Sch¨olkopf  and A. Zien  editors  Semi-Supervised

ing with constraints.
Learning  pages 71–98. MIT Press  2006.

[5] T. Berg  A. Berg  J. Edwards  and D. Forsyth. Names and faces in the news. In Proc. CVPR’04.
[6] T. Berg  A. Berg  J. Edwards  and D. Forsyth. Who’s in the picture? In Proc. NIPS’04.
[7] A. P. Dempster  N. Laird  and D. Rubin. Maximum likelihood from incomplete data via the

em algorithm. Journal Royal Statistical Society  39:1–38  1977.

[8] K. Deschacht and M.-F. Moens. Semi-supervised semantic role labeling using the latent words

language model. In Proc. EMNLP’09.

[9] I. Dhillon  Y. Guan  and B. Kulis. Kernel k-means: spectral clustering and normalized cuts. In

Proc. KDD’04.

[10] P. Duygulu  K. Barnard  N. de Freitas  and D. Forsyth. Object recognition as machine transla-

tion: Learning a lexicon for a ﬁxed image vocabulary. In Proc. ECCV’02.

[11] M. Eichner and V. Ferrari. Better appearance models for pictorial structures.

BMVC’09.

In Proc.

[12] M. Everingham  J. Sivic  and A. Zisserman. Hello! my name is... buffy - automatic naming of

characters in tv video. In Proc. BMVC’06.

[13] V. Ferrari  M. Marin  and A. Zisserman. Pose search: retrieving people using their pose. In

Proc. CVPR’09.

[14] V. Ferrari  M. Marin  and A. Zisserman. Progressive search space reduction for human pose

estimation. In Proc. CVPR’08.

[15] A. Frome  Y. Singer  and J. Malik.

functions. In Proc. NIPS’06.

Image retrieval and classiﬁcation using local distance

[16] M. Guillaumin  T. Mensink  J. Verbeek  and C. Schmid. Automatic face naming with caption-

based supervision. In Proc. CVPR’08.

[17] A. Gupta and L. Davis. Beyond nouns: Exploiting prepositions and comparative adjectives for

learning visual classiﬁers. In Proc. ECCV’08.

[18] D. Lowe. Distinctive image features from scale-invariant keypoints.

2004.

IJCV  60(2):91–110 

[19] J. B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In

Proc. of 5th Berkeley Symposium on Mathematical Statistics and Probability  1967.

[20] T. Malisiewicz and A. Efros. Recognition by association via learning per-exemplar distances.

In Proc. CVPR’08.

[21] T. Mensink and J. Verbeek. Improving people search using query expansions: How friends

help to ﬁnd people. In Proc. ECCV’08.

[22] R. Neal and G. E. Hinton. A view of the em algorithm that justiﬁes incremental  sparse  and
other variants. In M. I. Jordan  editor  Learning in Graphical Models  pages 355–368. Kluwer
Academic Publishers  1998.

[23] Y. Rodriguez. Face Detection and Veriﬁcation using Local Binary Patterns. PhD thesis  ´Ecole

Polytechnique F´ed´erale de Lausanne  2006.

[24] N. Shental  A. Bar-Hillel  T. Hertz  and D. Weinshall. Computing gaussian mixture models

with em using equivalence constraints. In Proc. NIPS’03.

[25] K. Wagstaff  C. Cardie  S. Rogers  and S. Schroedl. Constrained k-means clustering with

background knowledge. In Proc. ICML’01.

9

,Maria-Florina Balcan
Christopher Berlind
Avrim Blum
Emma Cohen
Kaushik Patnaik
Le Song