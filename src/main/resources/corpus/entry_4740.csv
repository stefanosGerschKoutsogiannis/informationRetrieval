2017,Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned Linear Systems,The problem of estimating a random vector x from noisy linear measurements y=Ax+w with unknown parameters on the distributions of x and w  which must also be learned  arises in a wide range of statistical learning and linear inverse problems.  We show that a computationally simple iterative message-passing algorithm can provably obtain asymptotically consistent estimates in a certain high-dimensional large-system limit (LSL) under very general parameterizations.  Previous message passing techniques have required i.i.d. sub-Gaussian A matrices and often fail when the matrix is ill-conditioned. The proposed algorithm  called adaptive vector approximate message passing (Adaptive VAMP) with auto-tuning  applies to all right-rotationally random A.  Importantly  this class includes matrices with arbitrarily bad conditioning.  We show that the parameter estimates and mean squared error (MSE) of x in each iteration converge to deterministic limits that can be precisely predicted by a simple set of state evolution (SE) equations.  In addition  a simple testable condition is provided in which the MSE matches the Bayes-optimal value predicted by the replica method.  The paper thus provides a computationally simple method with provable guarantees of optimality and consistency over a large class of linear inverse problems.,Rigorous Dynamics and Consistent Estimation in

Arbitrarily Conditioned Linear Systems

Alyson K. Fletcher

Dept. Statistics
UC Los Angeles

akfletcher@ucla.edu

Mojtaba Sahraee-Ardakan

Sundeep Rangan

msahraee@ucla.edu

srangan@nyu.edu

Dept. EE 

UC Los Angeles

Dept. ECE 

NYU

Philip Schniter

Dept. ECE 

The Ohio State Univ.

schniter@ece.osu.edu

Abstract

We consider the problem of estimating a random vector x from noisy linear mea-
surements y = Ax + w in the setting where parameters θ on the distribution of
x and w must be learned in addition to the vector x. This problem arises in a
wide range of statistical learning and linear inverse problems. Our main contribu-
tion shows that a computationally simple iterative message passing algorithm can
provably obtain asymptotically consistent estimates in a certain high-dimensional
large system limit (LSL) under very general parametrizations. Importantly  this
LSL applies to all right-rotationally random A – a much larger class of matrices
than i.i.d. sub-Gaussian matrices to which many past message passing approaches
are restricted. In addition  a simple testable condition is provided in which the
mean square error (MSE) on the vector x matches the Bayes optimal MSE pre-
dicted by the replica method. The proposed algorithm uses a combination of
Expectation-Maximization (EM) with a recently-developed Vector Approximate
Message Passing (VAMP) technique. We develop an analysis framework that
shows that the parameter estimates in each iteration of the algorithm converge to
deterministic limits that can be precisely predicted by a simple set of state evolution
(SE) equations. The SE equations  which extends those of VAMP without param-
eter adaptation  depend only on the initial parameter estimates and the statistical
properties of the problem and can be used to predict consistency and precisely
characterize other performance measures of the method.

1

Introduction

Consider the problem of estimating a random vector x0 from linear measurements y of the form

2 I)  x0 ∼ p(x|θ1) 

y = Ax0 + w  w ∼ N (0  θ−1

(1)
where A ∈ RM×N is a known matrix  p(x|θ1) is a density on x0 with parameters θ1  w is additive
white Gaussian noise (AWGN) independent of x0  and θ2 > 0 is the noise precision (inverse variance).
The goal is to estimate x0 along with simultaneously learning the unknown parameters θ := (θ1  θ2)
from the data y and A. This problem arises in Bayesian forms of linear inverse problems in signal
processing  as well as in linear regression in statistics.
Exact estimation of the parameters θ via maximum likelihood or other methods is generally intractable.
One promising class of approximate methods combines approximate message passing (AMP) [1]

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

with expectation-maximization (EM). AMP and its generalizations [2] are a powerful  relatively
recent  class of algorithms based on expectation propagation-type techniques. The AMP methodology
has the beneﬁt of being computationally fast and has been successfully applied to a wide range of
problems. Most importantly  for large  i.i.d.  sub-Gaussian random matrices A  the performance of
AMP methods can be exactly predicted by a scalar state evolution (SE) [3  4] that provides testable
conditions for optimality  even for non-convex priors. When the parameters θ are unknown  AMP
can be easily combined with EM for joint learning of the parameters θ and vector x [5–7].
A recent work [8] has combined EM with the so-called Vector AMP (VAMP) method of [9]. Similar to
AMP  VAMP is based on expectation propagation (EP) approximations of belief propagation [10  11]
and can also be considered as a special case of expectation consistent (EC) approximate inference
[12–14]. VAMP’s key attraction is that it applies to a larger class of matrices A than standard AMP
methods. Aside from Gaussian i.i.d. A  standard AMP techniques often diverge and require a variety
of modiﬁcations for stability [15–18]. In contrast  VAMP has provable SE analyses and convergence
guarantees that apply to all right-rotationally invariant matrices A [9  19] – a signiﬁcantly larger class
of matrices than i.i.d. Gaussians. Under further conditions  the mean-squared error (MSE) of VAMP
matches the replica predictions for optimality [20–23]. For the case when the distribution on x and
w are unknown  the work [8] proposed to combine EM and VAMP using the approximate inference
framework of [24]. The combination of AMP with EM methods have been particularly successful
in neural modeling problems [25  26]. While [8] provides numerical simulations demonstrating
excellent performance of this EM-VAMP method on a range of synthetic data  there were no provable
convergence guarantees.

Contributions of this work The SE analysis thus provides a rigorous and exact characterization of
the dynamics of EM-VAMP. In particular  the analysis can determine under which initial conditions
and problem statistics EM-VAMP will yield asymptotically consistent parameter estimates.

• Rigorous state evolution analysis: We provide a rigorous analysis of a generalization of
EM-VAMP that we call Adaptive VAMP. Similar to the analysis of VAMP  we consider
a certain large system limit (LSL) where the matrix A is random and right-rotationally
invariant. Importantly  this class of matrices is much more general than i.i.d. Gaussians used
in the original LSL analysis of Bayati and Montanari [3]. It is shown (Theorem 1) that in
the LSL  the parameter estimates at each iteration converge to deterministic limits θk that
can be computed from a set of SE equations that extend those of VAMP. The analysis also

exactly characterizes the asymptotic joint distribution of the estimates(cid:98)x and the true vector

x0. The SE equations depend only on the initial parameter estimate  the adaptation function 
and statistics on the matrix A  the vector x0 and noise w.
• Asymptotic consistency: It is also shown (Theorem 2) that under an additional identiﬁability
condition and a simple auto-tuning procedure  Adaptive VAMP can yield provably consistent
parameter estimates in the LSL. The technique uses an ML estimation approach from [7].
Remarkably  the result is true under very general problem formulations.
• Bayes optimality: In the case when the parameter estimates converge to the true value  the
behavior of adaptive VAMP matches that of VAMP. In this case  it is shown in [9] that  when
the SE equations have a unique ﬁxed point  the MSE of VAMP matches the MSE of the
Bayes optimal estimator predicted by the replica method [21–23].

In this way  we have developed a computationally efﬁcient method for a large class of linear inverse
problems with the properties that  in a certain high-dimensional limit: (1) the performance of the

algorithm can be exactly characterized  (2) the parameter estimates(cid:98)θ are asymptotically consistent;
and (3) the algorithm has testable conditions for which the signal estimates(cid:98)x match replica predictions

for Bayes optimality.

2 VAMP with Adaptation

Assume the prior on x can be written as

p(x|θ1) =

1

Z1(θ1)

exp [−f1(x|θ1)]  

f1(x|θ1) =

2

N(cid:88)

n=1

f1(xn|θ1) 

(2)

Algorithm 1 Adaptive VAMP
Require: Matrix A ∈ RM×N   measurement vector y  denoiser function g1(·)  statistic function

φ1(·)  adaptation function T1(·) and number of iterations Nit.

1: Select initial r10  γ10 ≥ 0 (cid:98)θ10 (cid:98)θ20.

1(r1k  γ1k (cid:98)θ1k)(cid:105)

// Input parameter update

1k = γ1k/(cid:104)g(cid:48)
η−1
µ1k = (cid:104)φ1(r1k  γ1k (cid:98)θ1k)(cid:105)

// Input denoising
γ2k = η1k − γ1k

2: for k = 0  1  . . .   Nit − 1 do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for

(cid:98)x1k = g1(r1k  γ1k (cid:98)θ1k)) 
r2k = (η1k(cid:98)x1k − γ1kr1k)/γ2k
(cid:98)θ1 k+1 = T1(µ1k) 
k ((cid:98)θ2kATy + γ2kr2k) 
(cid:98)x2k = Q−1
// Output estimation
η−1
2k = (1/N ) tr(Q−1
r1 k+1 = (η2k(cid:98)x2k − γ2kr2k)/γ1 k+1
k )
γ1 k+1 = η2k − γ2k
(cid:98)θ−1
2 k+1 = (1/N ){(cid:107)y − A(cid:98)x2k(cid:107)2 + tr(AQ−1

// Output parameter update

k AT)}

Qk =(cid:98)θ2kATA + γ2kI

where f1(·) is a separable penalty function  θ1 is a parameter vector and Z1(θ1) is a normalization
constant. With some abuse of notation  we have used f1(·) for the function on the vector x and
its components xn. Since f1(x|θ1) is separable  x has i.i.d. components conditioned on θ1. The
likelihood function under the Gaussian model (1) can be written as

p(y|x  θ2) :=

1

Z2(θ2)

exp [−f2(x  y|θ2)]  

f2(x  y|θ2) :=

(cid:107)y − Ax(cid:107)2 

θ2
2

where Z2(θ2) = (2π/θ2)N/2. The joint density of x  y given parameters θ = (θ1  θ2) is then

p(x  y|θ) = p(x|θ1)p(y|x  θ2).

(3)

(4)

The problem is to estimate the parameters θ = (θ1  θ2) along with the vector x0.
The steps of the proposed adaptive VAMP algorithm to perform this estimation are shown in Al-
gorithm 1  which is a generalization of the the EM-VAMP method in [8]. In each iteration  the
vector x0. The algorithm is tuned by selecting three key functions: (i) a denoiser function g1(·); (ii)
an adaptation statistic φ1(·); and (iii) a parameter selection function T1(·). The denoiser is used to

algorithm produces  for i = 1  2  estimates(cid:98)θi of the parameter θi  along with estimates(cid:98)xik of the
produce the estimates(cid:98)x1k  while the adaptation statistic and parameter estimation functions produce
the estimates(cid:98)θ1k.

(cid:104)−fi(x  y|(cid:98)θi) − γi

Denoiser function The denoiser function g1(·) is discussed in detail in [9] and is generally based
on the prior p(x|θ1). In the original EM-VAMP algorithm [8]  g1(·) is selected as the so-called
minimum mean-squared error (MMSE) denoiser. Speciﬁcally  in each iteration  the variables ri  γi

(5)
which represent estimates of the posterior density p(x|y  θ). To keep the notation symmetric  we
The EM-VAMP method then selects g1(·) to be the mean of the belief estimate 

and(cid:98)θi were used to construct belief estimates 
bi(x|ri  γi (cid:98)θi) ∝ exp
have written f1(x  y|(cid:98)θ1) for f1(x|(cid:98)θ1) even though the ﬁrst penalty function does not depend on y.
(cid:104)·(cid:105) for the empirical mean of a vector  i.e.  (cid:104)u(cid:105) = (1/N )(cid:80)N

(6)
1(r1k  γ1k  θ1)]n := ∂[g1(r1k  γ1k  θ1)]n/∂r1n and we use
n=1 un. Hence  η1k in line 4 is a scaled

For line 4 of Algorithm 1  we deﬁne [g(cid:48)

g1(r1  γ1  θ1) := E [x|r1  γ1  θ1] .

(cid:107)x − ri(cid:107)2(cid:105)

2

 

3

inverse divergence. It is shown in [9] that  for the MMSE denoiser (6)  η1k is the inverse average
posterior variance.

Estimation for θ1 with ﬁnite statistics For the EM-VAMP algorithm [8]  the parameter update

for(cid:98)θ1 k+1 is performed via a maximization
(cid:98)θ1 k+1 = arg max

E(cid:104)

(cid:12)(cid:12)(cid:12)r1k  γ1k (cid:98)θ1k

(cid:105)

ln p(x|θ1)

N(cid:88)

n=1

1
N

(7)
where the expectation is with respect to the belief estimate bi(·) in (5). It is shown in [8] that using (7)
is equivalent to an approximation of the M-step in the standard EM method. In the adaptive VAMP
method in Algorithm 1  the M-step maximization (7) is replaced by line 9. Note that line 9 again uses
(cid:104)·(cid:105) to denote empirical average 

θ1

 

µ1k = (cid:104)φ1(r1k  γ1k (cid:98)θ1k)(cid:105) :=

φ1(r1k n  γ1k (cid:98)θ1k) ∈ Rd 

(8)

so µ1k is the empirical average of some d-dimensional statistic φ1(·) over the components of r1k.

The parameter estimate update(cid:98)θ1 k+1 is then computed from some function of this statistic  T1(µ1k).

We show in the full paper [27] that there are two important cases where the EM update (7) can
be computed from a ﬁnite-dimensional statistic as in line 9: (i) The prior p(x|θ1) is given by an
exponential family  f1(x|θ1) = θT
1 ϕ(x) for some sufﬁcient statistic ϕ(x); and (ii) There are a
ﬁnite number of values for the parameter θ1. For other cases  we can approximate more general
parametrizations via discretization of the parameter values (cid:126)θ1. The updates in line 9 can also
incorporate other types of updates as we will see below. But  we stress that it is preferable to compute
the estimate for θ1 directly from the maximization (7) – the use of a ﬁnite-dimensional statistic is for
the sake of analysis.

Estimation for θ2 with ﬁnite statistics
It will be useful to also write the adaptation of θ2 in line 18
of Algorithm 1 in a similar form as line 9. First  take a singular value decomposition (SVD) of A of
the form

A = USVT  S = Diag(s) 

(9)

and deﬁne the transformed error and transformed noise 

ξ := UTw.

qk := VT(r2k − x0) 

Then  it is shown in the full paper [27] that(cid:98)θ2 k+1 in line 18 can be written as
  µ2k = (cid:104)φ2(q2  ξ  s  γ2k (cid:98)θ2k)(cid:105)
s2(cid:98)θ2 + γ2
(s2(cid:98)θ2 + γ2)2

(cid:98)θ2 k+1 = T2(µ2k) :=
φ2(q  ξ  s  γ2 (cid:98)θ2) :=

(sq + ξ)2 +

1
µ2k

where

γ2
2

s2

.

(10)

(11)

(12)

Of course  we cannot directly compute qk in (10) since we do not know the true x0. Nevertheless 
this form will be useful for analysis.

3 State Evolution in the Large System Limit

3.1 Large System Limit

Similar to the analysis of VAMP in [9]  we analyze Algorithm 1 in a certain large system limit (LSL).
The LSL framework was developed by Bayati and Montanari in [3] and we review some of the key
deﬁnitions in full paper [27]. As in the analysis of VAMP  the LSL considers a sequence of problems
indexed by the vector dimension N. For each N  we assume that there is a “true” vector x0 ∈ RN
that is observed through measurements of the form

y = Ax0 + w ∈ RN   w ∼ N (0  θ−1

2 IN ) 

(13)

4

where A ∈ RN×N is a known transform  w is Gaussian noise and θ2 represents a “true” noise
precision. The noise precision θ2 does not change with N.
Identical to [9]  the transform A is modeled as a large  right-orthogonally invariant random matrix.
Speciﬁcally  we assume that it has an SVD of the form (9) where U and V are N × N orthogonal
matrices such that U is deterministic and V is Haar distributed (i.e. uniformly distributed on the set
of orthogonal matrices). As described in [9]  although we have assumed a square matrix A  we can
consider general rectangular A by adding zero singular values.
Using the deﬁnitions in full paper [27]  we assume that the components of the singular-value vector
s ∈ RN in (9) converge empirically with second-order moments as

(14)
for some non-negative random variable S with E[S] > 0 and S ∈ [0  Smax] for some ﬁnite maximum
value Smax. Additionally  we assume that the components of the true vector  x0  and the initial input
to the denoiser  r10  converge empirically as

= S 

N→∞{sn} P L(2)

lim

N→∞{(r10 n  x0

lim

n)} P L(2)

= (R10  X 0)  R10 = X 0 + P0  P0 ∼ N (0  τ10) 

(15)

where X 0 is a random variable representing the true distribution of the components x0; P0 is an
initial error and τ10 is an initial error variance. The variable X 0 may be distributed as X 0 ∼ p(·|θ1)
for some true parameter θ1. However  in order to incorporate under-modeling  the existence of such
a true parameter is not required. We also assume that the initial second-order term and parameter
estimate converge almost surely as

N→∞(γ10 (cid:98)θ10 (cid:98)θ20) = (γ10  θ10  θ20)

lim

(16)

for some γ10 > 0 and (θ10  θ20).

3.2 Error and Sensitivity Functions

E1(γ1  τ1 (cid:98)θ1) := E(cid:104)

We next need to introduce parametric forms of two key terms from [9]: error functions and sensitivity
functions. The error functions describe MSE of the denoiser and output estimators under AWGN

measurements. Speciﬁcally  for the denoiser g1(·  γ1 (cid:98)θ1)  we deﬁne the error function as
function E1(γ1  τ1 (cid:98)θ1) thus represents the MSE of the estimate (cid:98)X = g1(R1  γ1 (cid:98)θ1) from a measure-
ment R1 corrupted by Gaussian noise of variance τ1 under the parameter estimate(cid:98)θ1. For the output

where X 0 is distributed according to the true distribution of the components x0 (see above). The

(g1(R1  γ1 (cid:98)θ1) − X 0)2(cid:105)

  R1 = X 0 + P  P ∼ N (0  τ1) 

(17)

estimator  we deﬁne the error function as

E2(γ2  τ2 (cid:98)θ2) := lim

N→∞

1
N

E(cid:107)g2(r2  γ2 (cid:98)θ2) − x0(cid:107)2 

x0 = r2 + q  q ∼ N (0  τ2I)  y = Ax0 + w  w ∼ N (0  θ−1

(18)
which is the average per component error of the vector estimate under Gaussian noise. The dependence
on the true noise precision  θ2  is suppressed.
The sensitivity functions describe the expected divergence of the estimator. For the denoiser  the
sensitivity function is deﬁned as

2 I) 

A1(γ1  τ1 (cid:98)θ1) := E(cid:104)

g(cid:48)

(cid:105)
1(R1  γ1 (cid:98)θ1)
A2(γ2  τ2 (cid:98)θ2) := lim

N→∞

(cid:34)

∂g2(r2  γ2 (cid:98)θ2)

(cid:35)

∂r2

1
N

tr

which is the average derivative under a Gaussian noise input. For the output estimator  the sensitivity
is deﬁned as

  R1 = X 0 + P  P ∼ N (0  τ1) 

(19)

 

(20)

where r2 is distributed as in (18). The paper [9] discusses the error and sensitivity functions in detail
and shows how these functions can be easily evaluated.

5

3.3 State Evolution Equations

lim

n)  n = 1  . . .   N}.

We can now describe our main result  which are the SE equations for Adaptive VAMP. The equations
are an extension of those in the VAMP paper [9]  with modiﬁcations for the parameter estimation.
For a given iteration k ≥ 1  consider the set of components 

{((cid:98)x1k n  r1k n  x0
N→∞{((cid:98)x1k n  r1k n  x0

This set represents the components of the true vector x0  its corresponding estimate(cid:98)x1k and the

denoiser input r1k. We will show that  under certain assumptions  these components converge
empirically as

= ((cid:98)X1k  R1k  X 0) 
(cid:98)X1k = g1(R1k  γ1k  θ1k) 

R1k = X 0 + Pk  Pk ∼ N (0  τ1k) 

n)} P L(2)
where the random variables ((cid:98)X1k  R1k  X 0) are given by
for constants γ1k  θ1k and τ1k that will be deﬁned below. We will also see that (cid:98)θ1k → θ1k  so θ1k
n plus Gaussian noise. The corresponding estimate (cid:98)x1k n then
n and its corresponding(cid:98)x1k n is identical to a simple scalar

represents the asymptotic parameter estimate. The model (22) shows that each component r1k n
appears as the true component x0
appears as the denoiser output with r1k n as the input and θ1k as the parameter estimate. Hence  the
asymptotic behavior of any component x0
system. We will refer to (21)-(22) as the denoiser’s scalar equivalent model.
We will also show that these transformed errors qk and noise ξ in (10) and singular values s converge
empirically to a set of independent random variables (Qk  Ξ  S) given by

(22)

(21)

N→∞{(qk n  ξn  sn)} P L(2)

lim

= (Qk  Ξ  S)  Qk ∼ N (0  τ2k)  Ξ ∼ N (0  θ−1
2 ) 

(23)

where S has the distribution of the singular values of A  τ2k is a variance that will be deﬁned below
and θ2 is the true noise precision in the measurement model (13). All the variables in (23) are
independent. Thus (23) is a scalar equivalent model for the output estimator.
The variance terms are deﬁned recursively through the state evolution equations 
γ2k = η1k − γ1k

α1k = A1(γ1k  τ1k  θ1k) 

η1k =

(24a)

 

γ1k
α1k

θ1 k+1 = T1(µ1k)  µ1k = E(cid:2)φ1(R1k  γ1k  θ1k)(cid:3)
(cid:3)  
(cid:2)E1(γ1k  τ1k  θ1k) − α2
θ2 k+1 = T2(µ2k)  µ2k = E(cid:2)φ2(Qk  Ξ  S  γ2k  θ2k)(cid:3)

α2k = A2(γ2k  τ2k  θ2k) 

(1 − α1k)2

γ2k
α2k

η2k =

τ2k =

1

 

1kτ1k
γ1 k+1 = η2k − γ2k

(cid:2)E2(γ2k  τ2k) − α2

(cid:3)  

1

(24b)

(24c)

(24d)

(24e)

2kτ2k

τ1 k+1 =

(1 − α2k)2

(24f)
which are initialized with τ10 = E[(R10 − X 0)2] and the (γ10  θ10  θ20) deﬁned from the limit (16).
The expectation in (24b) is with respect to the random variables (21) and the expectation in (24e) is
with respect to the random variables (23).
Theorem 1. Consider the outputs of Algorithm 1. Under the above assumptions and deﬁnitions 
assume additionally that for all iterations k:
(i) The solution α1k from the SE equations (24) satisﬁes α1k ∈ (0  1).

(ii) The functions Ai(·)  Ei(·) and Ti(·) are continuous at (γi  τi (cid:98)θi  µi) = (γik  τik  θik  µik).
1(r1  γ1 (cid:98)θ1) are uniformly Lipschitz
(iii) The denoiser function g1(r1  γ1 (cid:98)θ1) and its derivative g(cid:48)
in r1 at (γ1 (cid:98)θ1) = (γ1k  θ1k). (See the full paper [27]. for a precise deﬁnition of uniform

Lipschitz continuity.)

6

(iv) The adaptation statistic φ1(r1  γ1 (cid:98)θ1) is uniformly pseudo-Lipschitz of order 2 in r1 at
(γ1 (cid:98)θ1) = (γ1k  θ1k).

Then  for any ﬁxed iteration k ≥ 0 

N→∞(αik  ηik  γik  µik (cid:98)θik) = (αik  ηik  γik  µik  θik)

lim

(25)

almost surely. In addition  the empirical limit (21) holds almost surely for all k > 0  and (23) holds
almost surely for all k ≥ 0.

Theorem 1 shows that  in the LSL  the parameter estimates(cid:98)θik converge to deterministic limits θik

that can be precisely predicted by the state-evolution equations. The SE equations incorporate the true
distribution of the components on the prior x0  the true noise precision θ2  and the speciﬁc parameter
estimation and denoiser functions used by the Adaptive VAMP method. In addition  similar to the SE
analysis of VAMP in [9]  the SE equations also predict the asymptotic joint distribution of x0 and

their estimates(cid:98)xik. This joint distribution can be used to measure various performance metrics such

as MSE – see [9]. In this way  we have provided a rigorous and precise characterization of a class of
adaptive VAMP algorithms that includes EM-VAMP.

4 Consistent Parameter Estimation with Variance Auto-Tuning

By comparing the deterministic limits θik with the true parameters θi  one can determine under which
problem conditions the parameter estimates of adaptive VAMP are asymptotically consistent. In this
section  we show with a particular choice of parameter estimation functions  one can obtain provably
asymptotically consistent parameter estimates under suitable identiﬁability conditions. We call the
method variance auto-tuning  which generalizes the approach in [7].
Deﬁnition 1. Let p(x|θ1) be a parametrized set of densities. Given a ﬁnite-dimensional statistic
φ1(r)  consider the mapping

(τ1  θ1) (cid:55)→ E [φ1(R)|τ1  θ1]   R = X + N (0  τ1)  X ∼ p(x|θ1).

(26)
We say the p(x|θ1) is identiﬁable in Gaussian noise if there exists a ﬁnite-dimensional statistic
φ1(r) ∈ Rd such that (i) φ1(r) is pseudo-Lipschitz continuous of order 2; and (ii) the mapping (26)
has a continuous inverse.
Theorem 2. Under the assumptions of Theorem 1  suppose that X 0 follows X 0 ∼ p(x|θ0
true parameter θ0

that  for any iteration k  the estimate(cid:98)θ1k and noise estimate(cid:98)τ1k are asymptotically consistent in that
limN→∞(cid:98)θ1k = θ0

1) for some
1. If p(x|θ1) is identiﬁable in Gaussian noise  there exists an adaptation rule such
1 and limN→∞(cid:98)τ1k = τ1k almost surely.

The theorem is proved in full paper [27]. which also provides details on how to perform the
adaptation. A similar result for consistent estimation of the noise precision θ2 is also given. The
result is remarkable as it shows that a simple variant of EM-VAMP can provide provably consistent
parameter estimates under extremely general distributions.

5 Numerical Simulations

Sparse signal recovery: The paper [8] presented several numerical experiments to assess the
performance of EM-VAMP relative to other methods. Here  our goal is to conﬁrm that EM-VAMP’s
performance matches the SE predictions. As in [8]  we consider a sparse linear regression problem of
estimating a vector x from measurements y from (1) without knowing the signal parameters θ1 or
the noise precision θ2 > 0. Details are given in the full paper [27]. Brieﬂy  to model the sparsity  x is
drawn as an i.i.d. Bernoulli-Gaussian (i.e.  spike and slab) prior with unknown sparsity level  mean
and variance. The true sparsity is βx = 0.1. Following [15  16]  we take A ∈ RM×N to be a random
right-orthogonally invariant matrix with dimensions under M = 512  N = 1024 with the condition
number set to κ = 100 (high condition number matrices are known to be problem for conventional
AMP methods). The left panel of Fig. 1 shows the normalized mean square error (NMSE) for various
algorithms. The full paper [27] describes the algorithms in details and also shows similar results for
κ = 10.

7

Figure 1: Numerical simulations. Left panel: Sparse signal recovery: NMSE versus iteration for
condition number for a random matrix with a condition number κ = 100. Right panel: NMSE for
sparse image recovery as a function of the measurement ratio M/N.

We see several important features. First  for all variants of VAMP and EM-VAMP  the SE equations
provide an excellent prediction of the per iteration performance of the algorithm. Second  consistent
with the simulations in [9]  the oracle VAMP converges remarkably fast (∼ 10 iterations). Third 
the performance of EM-VAMP with auto-tuning is virtually indistinguishable from oracle VAMP 
suggesting that the parameter estimates are near perfect from the very ﬁrst iteration. Fourth  the EM-
VAMP method performs initially worse than the oracle-VAMP  but these errors are exactly predicted
by the SE. Finally  all the VAMP and EM-VAMP algorithm exhibit much faster convergence than the
EM-BG-AMP. In fact  consistent with observations in [8]  EM-BG-AMP begins to diverge at higher
condition numbers. In contrast  the VAMP algorithms are stable.

Compressed sensing image recovery While the theory is developed on theoretical signal priors 
we demonstrate that the proposed EM-VAMP algorithm can be effective on natural images. Specif-
ically  we repeat the experiments in [28] for recovery of a sparse image. Again  see the full paper
[27] for details including a picture of the image and the various reconstructions. An N = 256 × 256
image of a satellite with K = 6678 pixels is transformed through an undersampled random transform
A = diag(s)PH  where H is fast Hadamard transform  P is a random subselection to M measure-
ments and s is a scaling to adjust the condition number. As in the previous example  the image vector
x is modeled as a sparse Bernoulli-Gaussian and the EM-VAMP algorithm is used to estimate the
sparsity ratio  signal variance and noise variance. The transform is set to have a condition number
of κ = 100. We see from the right panel of Fig. 1 we see that the that the EM-VAMP algorithm is
able to reconstruct the images with improved performance over the standard basis pursuit denoising
method spgl1 [29] and the EM-BG-GAMP method from [16].

6 Conclusions

Due to its analytic tractability  computational simplicity  and potential for Bayes optimal inference 
VAMP is a promising technique for statistical linear inverse problems. However  a key challenge in
using VAMP and related methods is the need to precisely specify the distribution on the problem
parameters. This work provides a rigorous foundation for analyzing VAMP in combination with
various parameter adaptation techniques including EM. The analysis reveals that VAMP with
appropriate tuning  can also provide consistent parameter estimates under very general settings  thus
yielding a powerful approach for statistical linear inverse problems.

Acknowledgments

A. K. Fletcher and M. Saharee-Ardakan were supported in part by the National Science Foundation
under Grants 1254204 and 1738286 and the Ofﬁce of Naval Research under Grant N00014-15-1-2677.
S. Rangan was supported in part by the National Science Foundation under Grants 1116589  1302336 
and 1547332  and the industrial afﬁliates of NYU WIRELESS. The work of P. Schniter was supported
in part by the National Science Foundation under Grant CCF-1527162.

8

References
[1] D. L. Donoho  A. Maleki  and A. Montanari  “Message-passing algorithms for compressed sensing ” Proc.

Nat. Acad. Sci.  vol. 106  no. 45  pp. 18 914–18 919  Nov. 2009.

[2] S. Rangan  “Generalized approximate message passing for estimation with random linear mixing ” in Proc.

IEEE Int. Symp. Inform. Theory  Saint Petersburg  Russia  Jul.–Aug. 2011  pp. 2174–2178.

[3] M. Bayati and A. Montanari  “The dynamics of message passing on dense graphs  with applications to

compressed sensing ” IEEE Trans. Inform. Theory  vol. 57  no. 2  pp. 764–785  Feb. 2011.

[4] A. Javanmard and A. Montanari  “State evolution for general approximate message passing algorithms 

with applications to spatial coupling ” Information and Inference  vol. 2  no. 2  pp. 115–144  2013.

[5] F. Krzakala  M. Mézard  F. Sausset  Y. Sun  and L. Zdeborová  “Statistical-physics-based reconstruction in

compressed sensing ” Physical Review X  vol. 2  no. 2  p. 021005  2012.

[6] J. P. Vila and P. Schniter  “Expectation-maximization Gaussian-mixture approximate message passing ”

IEEE Trans. Signal Processing  vol. 61  no. 19  pp. 4658–4672  2013.

[7] U. S. Kamilov  S. Rangan  A. K. Fletcher  and M. Unser  “Approximate message passing with consistent
parameter estimation and applications to sparse learning ” IEEE Trans. Info. Theory  vol. 60  no. 5  pp.
2969–2985  Apr. 2014.

[8] A. K. Fletcher and P. Schniter  “Learning and free energies for vector approximate message passing ” Proc.

IEEE ICASSP  March 2017.

[9] S. Rangan  P. Schniter  and A. K. Fletcher  “Vector approximate message passing ” Proc. IEEE ISIT  June

2017.

[10] M. Seeger  “Bayesian inference and optimal design for the sparse linear model ” J. Machine Learning

Research  vol. 9  pp. 759–813  Sep. 2008.

[11] M. W. Seeger and H. Nickisch  “Fast convergent algorithms for expectation propagation approximate
bayesian inference ” in International Conference on Artiﬁcial Intelligence and Statistics  2011  pp. 652–660.

[12] M. Opper and O. Winther  “Expectation consistent free energies for approximate inference ” in Proc. NIPS 

2004  pp. 1001–1008.

[13] ——  “Expectation consistent approximate inference ” J. Mach. Learning Res.  vol. 1  pp. 2177–2204 

2005.

[14] A. K. Fletcher  M. Sahraee-Ardakan  S. Rangan  and P. Schniter  “Expectation consistent approximate

inference: Generalizations and convergence ” in Proc. IEEE ISIT  2016  pp. 190–194.

[15] S. Rangan  P. Schniter  and A. Fletcher  “On the convergence of approximate message passing with arbitrary

matrices ” in Proc. IEEE ISIT  Jul. 2014  pp. 236–240.

[16] J. Vila  P. Schniter  S. Rangan  F. Krzakala  and L. Zdeborová  “Adaptive damping and mean removal for
the generalized approximate message passing algorithm ” in Proc. IEEE ICASSP  2015  pp. 2021–2025.

[17] A. Manoel  F. Krzakala  E. W. Tramel  and L. Zdeborová  “Swept approximate message passing for sparse

estimation ” in Proc. ICML  2015  pp. 1123–1132.

[18] S. Rangan  A. K. Fletcher  P. Schniter  and U. S. Kamilov  “Inference for generalized linear models via
alternating directions and Bethe free energy minimization ” IEEE Transactions on Information Theory 
vol. 63  no. 1  pp. 676–697  2017.

[19] K. Takeuchi  “Rigorous dynamics of expectation-propagation-based signal recovery from unitarily invariant

measurements ” Proc. IEEE ISIT  June 2017.

[20] S. Rangan  A. Fletcher  and V. K. Goyal  “Asymptotic analysis of MAP estimation via the replica method
and applications to compressed sensing ” IEEE Trans. Inform. Theory  vol. 58  no. 3  pp. 1902–1923  Mar.
2012.

[21] A. M. Tulino  G. Caire  S. Verdú  and S. Shamai  “Support recovery with sparsely sampled free random

matrices ” IEEE Trans. Inform. Theory  vol. 59  no. 7  pp. 4243–4271  2013.

[22] J. Barbier  M. Dia  N. Macris  and F. Krzakala  “The mutual information in random linear estimation ”

arXiv:1607.02335  2016.

9

[23] G. Reeves and H. D. Pﬁster  “The replica-symmetric prediction for compressed sensing with Gaussian

matrices is exact ” in Proc. IEEE ISIT  2016.

[24] T. Heskes  O. Zoeter  and W. Wiegerinck  “Approximate expectation maximization ” NIPS  vol. 16  pp.

353–360  2004.

[25] A. K. Fletcher  S. Rangan  L. Varshney  and A. Bhargava  “Neural reconstruction with approximate
message passing (NeuRAMP) ” in Proc. Neural Information Process. Syst.  Granada  Spain  Dec. 2011  pp.
2555–2563.

[26] A. K. Fletcher and S. Rangan  “Scalable inference for neuronal connectivity from calcium imaging ” in

Proc. Neural Information Processing Systems  2014  pp. 2843–2851.

[27] A. Fletcher  M. Sahraee-Ardakan  S. Rangan  and P. Schniter  “Rigorous dynamics and consistent estimation

in arbitrarily conditioned linear systems ” arxiv  2017.

[28] J. P. Vila and P. Schniter  “An empirical-Bayes approach to recovering linearly constrained non-negative

sparse signals ” IEEE Trans. Signal Process.  vol. 62  no. 18  pp. 4689–4703  2014.

[29] E. Van Den Berg and M. P. Friedlander  “Probing the pareto frontier for basis pursuit solutions ” SIAM

Journal on Scientiﬁc Computing  vol. 31  no. 2  pp. 890–912  2008.

10

,Michael Shvartsman
Vaibhav Srivastava
Jonathan Cohen
Alyson Fletcher
Mojtaba Sahraee-Ardakan
Sundeep Rangan
Philip Schniter
Eldar Insafutdinov
Alexey Dosovitskiy