2007,Testing for Homogeneity with Kernel Fisher Discriminant Analysis,We propose to test for the homogeneity of two samples by using Kernel Fisher discriminant Analysis. This provides us with a consistent nonparametric test statistic  for which we derive the asymptotic distribution under the null hypothesis. We give experimental evidence of the relevance of our method on both artificial and real datasets.,Testing for Homogeneity

with Kernel Fisher Discriminant Analysis

Za¨ıd Harchaoui

LTCI  TELECOM ParisTech and CNRS

46  rue Barrault  75634 Paris cedex 13  France

zaid.harchaoui@enst.fr

Francis Bach

Willow Project  INRIA-ENS

45  rue d’Ulm  75230 Paris  France
francis.bach@mines.org

´Eric Moulines

LTCI  TELECOM ParisTech and CNRS

46  rue Barrault  75634 Paris cedex 13  France

eric.moulines@enst.fr

Abstract

We propose to investigate test statistics for testing homogeneity based on kernel
Fisher discriminant analysis. Asymptotic null distributions under null hypothesis
are derived  and consistency against ﬁxed alternatives is assessed. Finally  exper-
imental evidence of the performance of the proposed approach on both artiﬁcial
and real datasets is provided.

1 Introduction

1   . . .   X (1)

An important problem in statistics and machine learning consists in testing whether the distributions
of two random variables are identical under the alternative that they may differ in some ways. More
precisely  let {X (1)
n2 } be independent random variables taking val-
ues in the input space (X  d)  with common distributions P1 and P2  respectively. The problem con-
sists in testing the null hypothesis H0 : P1 = P2 against the alternative HA : P1 6= P2. This problem
arises in many applications  ranging from computational anatomy [10] to process monitoring [7]. We
shall allow the input space X to be quite general  including for example ﬁnite-dimensional Euclidean
spaces or more sophisticated structures such as strings or graphs (see [17]) arising in applications
such as bioinformatics [4].

n1 } and {X (2)

1   . . .   X (2)

Traditional approaches to this problem are based on distribution functions and use a certain distance
between the empirical distributions obtained from the two samples. The most popular procedures
are the two-sample Kolmogorov-Smirnov tests or the Cramer-Von Mises tests  that have been the
standard for addressing these issues (at least when the dimension of the input space is small  and
most often when X = R). Although these tests are popular due to their simplicity  they are known
to be insensitive to certain characteristics of the distribution  such as densities containing high-
frequency components or local features such as bumps. The low-power of the traditional density
based statistics can be improved on using test statistics based on kernel density estimators [2] and
[1] and wavelet estimators [6]. Recent work [11] has shown that one could difference in means in
RKHSs in order to consistently test for homogeneity. In this paper  we show that taking into account
the covariance structure in the RKHS allows to obtain simple limiting distributions.

The paper is organized as follows: in Section 2 and Section 3  we state the main deﬁnitions and we
construct the test statistics. In Section 4  we give the asymptotic distribution of our test statistic under
the null hypothesis  and investigate  the consistency and the power of the test for ﬁxed alternatives. In

1

Section 5 we provide experimental evidence of the performance of our test statistic on both artiﬁcial
and real datasets. Detailed proofs are presented in the last sections.

2 Mean and covariance in reproducing kernel Hilbert spaces

We ﬁrst highlight the main assumptions we make in the paper on the reproducing kernel  then intro-
duce operator-theoretic tools for working with distributions in inﬁnite-dimensional spaces.

2.1 Reproducing kernel Hilbert spaces

Let (X  d) be a separable metric space  and denote by X the associated σ-algebra. Let X be X-
valued random variable  with probability measure P; the corresponding expectation is denoted E.
Consider a Hilbert space (H h· ·iH) of functions from X to R. The Hilbert space H is an RKHS if
at each x ∈ X  the point evaluation operator δx : H → R  which maps f ∈ H to f (x) ∈ R  is a
bounded linear functional. To each point x ∈ X  there corresponds an element Φ(x) ∈ H (we call Φ
the feature map) such that hΦ(x)  fiH = f (x) for all f ∈ H  and hΦ(x)  Φ(y)iH = k(x  y)  where
k : X × X → R is a positive deﬁnite kernel. We denote by kfkH = hf  fi1/2
H the associated norm.
It is assumed in the remainder that H is a separable Hilbert space. Note that this is always the case
if X is a separable metric space and if the kernel is continuous (see [18]). Throughout this paper  we
make the following two assumptions on the kernel:

(A1) The kernel k is bounded  that is |k|∞ = sup(x y)∈X×X k(x  y) < ∞.
(A2) For all probability measures P on (X X )  the RKHS associated with k(· ·) is dense in

L2(P).

The asymptotic normality of our test statistics is valid without assumption (A2)  while consistency
results against ﬁxed alternatives does need (A2). Assumption (A2) is true for translation-invariant
kernels [8]  and in particular for the Gaussian kernel on Rd [18]. Note that we do not require the
compactness of X as in [18] 

2.2 Mean element and covariance operator

We shall need some operator-theoretic tools to deﬁne mean elements and covariance operators in
RKHS. A linear operator T is said to be bounded if there is a number C such that kT fkH ≤ C kfkH
for all f ∈ H. The operator-norm of T is then deﬁned as the inﬁmum of such numbers C  that is
kTk = supkf kH≤1 kT fkH (see [9]).
We recall below some basic facts about ﬁrst and second-order moments of RKHS-valued random
variables. IfR k1/2(x  x)P(dx) < ∞  the mean element µP is deﬁned for all functions f ∈ H as the
unique element in H satisfying 

(1)

(2)

If furthermoreR k(x  x)P(dx) < ∞  then the covariance operator ΣP is deﬁned as the unique linear
operator onto H satisfying for all f  g ∈ H 

hµP  fiH = Pf def= Z f dP .

hf  ΣPgiH

def= Z (f − Pf )(g − Pg)dP .

Note that when assumption (A2) is satisﬁed  then the map from P 7→ µP is injective. The operator
ΣP is a self-adjoint nonnegative trace-class operator. In the sequel  the dependence of µP and ΣP in
P is omitted whenever there is no risk of confusion.
Given a sample {X1  . . .   Xn}  the empirical estimates respectively of the mean element and the
covariance operator are then deﬁned using empirical moments and lead to:

ˆµ = n−1

nXi=1

k(Xi ·)  

ˆΣ = n−1

nXi=1

2

k(Xi ·) ⊗ k(Xi ·) − ˆµ ⊗ ˆµ .

(3)

The operator Σ is a self-adjoint nonnegative trace-class operators. Hence  it can de diagonalized in
an orthonormal basis  with a spectrum composed of a strictly decreasing sequence λp > 0 tending

to zero and potentially a null space N (Σ) composed of functions f in H such thatR {f − Pf}2dP =

0 [5]  i.e.  functions which are constant in the support of P.
The null space may be reduced to the null element (in particular for the Gaussian kernel)  or may
be inﬁnite-dimensional. Similarly  there may be inﬁnitely many strictly positive eigenvalues (true
nonparametric case) or ﬁnitely many (underlying ﬁnite dimensional problems).

3 KFDA-based test statistic

1   . . .   X (1)

1   . . .   X (2)

n1 } and {X (2)

In the feature space  the two-sample homogeneity test procedure can be formulated as follows. Given
{X (1)
n2 } from distributions P1 and P2  two independent identically
distributed samples respectively from P1 and P2  having mean and covariance operators respectively
given by (µ1  Σ1) and (µ2  Σ2)  we wish to test the null hypothesis H0  µ1 = µ2 and Σ1 = Σ2 
against the alternative hypothesis HA  µ1 6= µ2.
In this paper  we tackle the problem by using a (regularized) kernelized version of the Fisher dis-
def= (n1/n)Σ1 +(n2/n)Σ2 the pooled covariance operator  where
criminant analysis. Denote by ΣW
n def= n1 + n2  corresponding to the within-class covariance matrix in the ﬁnite-dimensional setting
def= (n1n2/n2)(µ2−µ1)⊗(µ2−µ1) the between-class covariance oper-
(see [14]. Let us denote ΣB
ator. For a = 1  2  denote by (ˆµa  ˆΣa) respectively the empirical estimates of the mean element and
def= (n1/n) ˆΣ1 + (n2/n) ˆΣ2
the covariance operator  deﬁned as previously stated in (3). Denote ˆΣW
def= (n1n2/n2)(ˆµ2 − ˆµ1) ⊗ (ˆµ2 − ˆµ1) the em-
the empirical pooled covariance estimator  and ˆΣB
pirical between-class covariance operator. Let {γn}n≥0 be a sequence of strictly positive numbers.
The maximum Fisher discriminant ratio serves as a basis of our test statistics:

n max
f ∈H

Df  ˆΣBfEH

Df  ( ˆΣW + γnI)fEH

=

n1n2

n

(cid:13)(cid:13)(cid:13)( ˆΣW + γnI)− 1
2 ˆδ(cid:13)(cid:13)(cid:13)

2

H

 

(4)

where I denotes the identity operator. Note that if the input space is Euclidean  e.g. X = Rd  the
kernel is linear k(x  y) = x⊤y and γn = 0  this quantity matches the so-called Hotelling’s T 2-
statistic in the two-sample case [15]. Moreover  in practice it may be computed thanks to the kernel
trick  adapted to the kernel Fisher discriminant analysis and outlined in [17  Chapter 6]. We shall
make the following assumptions respectively on Σ1 and Σ2

p

p=1 λ1/2

(B1) For u = 1  2  the eigenvalues {λp(Σu)}p≥1 satisfyP∞
(B2) For u = 1  2  there are inﬁnitely many strictly positive eigenvalues {λp(Σu)}p≥1 of Σu.
The statistical analysis conducted in Section 4 shall demonstrate  as γn → 0 at an appropriate
rate  the need to respectively recenter and rescale (a standard statistical transformation known as
studentization) the maximum Fisher discriminant ratio  in order to get a theoretically well-calibrated
test statistic. These roles  recentering and rescaling  will be played respectively by d1(ΣW   γ) and
d2(ΣW   γ)  where for a given compact operator Σ with decreasing eigenvalues λp(S)  the quantity
dr(Σ  γ) is deﬁned for all q ≥ 1 as

(Σu) < ∞.

dr(Σ  γ) def= ( ∞Xp=1

p)1/r

(λp + γ)−rλr

.

(5)

4 Theoretical results

We consider in the sequel the following studentized test statistic:

n1n2

n

bTn(γn) =

2

(cid:13)(cid:13)(cid:13)( ˆΣW + γnI)−1/2ˆδ(cid:13)(cid:13)(cid:13)

√2d2( ˆΣW   γn)

3

H − d1( ˆΣW   γn)

.

(6)

In this paper  we ﬁrst consider the asymptotic behavior of bTn under the null hypothesis  and then

against a ﬁxed alternative. This will establish that our nonparametric test procedure is consistent in
power.

4.1 Asymptotic normality under null hypothesis

In this section  we derive the distribution of the test statistics under the null hypothesis H0 : P1 = P2
of homogeneity  i.e. µ1 = µ2 and Σ1 = Σ2 = Σ. As γn → 0 tends to zero 
Theorem 1. Assume (A1) and (B1). If P1 = P2 = P and if γn + γ−1

n n−1/2 → 0  then

(7)

bTn(γn) D−→ N (0  1)

The proof is postponed to Section 7. Under the assumptions of Theorem 1  the sequence of tests that
rejects the null hypothesis when ˆTn(γn) ≥ z1−α  where z1−α is the (1− α)-quantile of the standard
normal distribution  is asymptotically level α. Note that the limiting distribution does not depend on
the kernel nor on the regularization parameter.

4.2 Power consistency

We study the power of the test based on bTn(γn) under alternative hypotheses. The minimal re-

quirement is to to prove that this sequence of tests is consistent in power. A sequence of tests of
constant level α is said to be consistent in power if the probability of accepting the null hypothesis
of homogeneity goes to zero as the sample size goes to inﬁnity under a ﬁxed alternative.

The following proposition shows that the limit is ﬁnite  strictly positive and independent of the kernel
otherwise (see [8] for similar results for canonical correlation analysis). The following result gives
on

  i.e.the population counterpart of(cid:13)(cid:13)(cid:13)( ˆΣ−1/2

n n−1/2 → 0  then for any probability distributions

which our test statistics is based upon.
Proposition 2. Assume (A1) and (A2). If γn +γ−1
P1 and P2 

some useful insights on(cid:13)(cid:13)(cid:13)Σ−1/2
W δ(cid:13)(cid:13)(cid:13)H
ρ1ρ2(cid:18)1 −Z

=

1

H

2

W + γnI)−1/2ˆδ(cid:13)(cid:13)(cid:13)H
dρ(cid:19)−1

 

p1p2

ρ1p1 + ρ2p2

dν(cid:19)(cid:18)Z

p1p2

ρ1p1 + ρ2p2

W δ(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)Σ−1/2
The norm(cid:13)(cid:13)(cid:13)Σ−1/2
W δ(cid:13)(cid:13)(cid:13)

2

H

where ν is any probability measure such that P1 and P2 are absolutely continuous w.r.t. ν and p1
and p2 are the densities of P1 and P2 with respect to ν.

zero if the χ2-divergence is null  that is  if and only if P1 = P2.

is ﬁnite when the χ2-divergenceR p−1

1 (p2 − p1)2dρ is ﬁnite. It is equal to

By combining the two previous propositions  we therefore obtain the following consistency Theo-
rem.
Theorem 3. Assume (A1) and (A2). Let P1 and P2 be two distributions over (X X )  such that
P2 6= P1. If γn + γ−1

n n−1/2 → 0  then

5 Experiments

PHA(bTn(γ) > z1−α) → ∞ .

(8)

In this section  we investigate the experimental performances of our test statistic KFDA  and com-
pare it in terms of power against other nonparametric test statistics.

5.1 Artiﬁcial data

We shall focus here on a particularly simple setting  in order analyze the major issues arising in
applying our approach in practice. Indeed  we consider the periodic smoothing spline kernel (see

4

10−1

γ =
KFDA 0.01±0.0032
MMD 0.01±0.0023

10−4
0.11±0.0062
id.

10−7
0.98±0.0031
id.

10−10
0.99±0.0001
id.

Table 1: Evolution of power of KFDA and MMD respectively  as γ goes to 0.

[19] for a detailed derivation)  for which explicit formulae are available for the eigenvalues of the
corresponding covariance operator when the underlying distribution is uniform. This allows us to
alleviate the issue of estimating the spectrum of the covariance operator  and weigh up the practical
impact of the regularization on the power of our test statistic.

Periodic smoothing spline kernel Consider X as the two-dimensional circle identiﬁed with the
interval [0  1] (with periodicity conditions). We consider the strictly positive sequence Kν =
(2πν)−2m and the following norm:

H = hf  c0i2
K0

kfk2

hf  cνi2 + hf  sνi2

Kν

+Xν>0

where cν(t) = √2 cos 2πνt and sν(t) = √2 sin 2πνt for ν ≥ 1 and c0(t) = 1X. This is always an

RKHS norm associated with the following kernel
(−1)m−1
(2m)!

K(s  t) =

B2m((s − t) − ⌊s − t⌋)

where B2m is the 2m-th Bernoulli polynomial. We have B2(x) = x2 − x + 1/6.
We consider the following testing problem

H0 :
HA :

p1 = p2
p2 6= p2

with p1 the uniform density (i.e.  the density with respect to the Lebesgue measure is equal to c0) 
and densities p2 = p1(c0 + .25∗ c4). The covariance operator Σ(p1) has eigenvectors c0  cν  sν with
eigenvalues 0 for c0 and Kν for others.

Comparison with MMD We conducted experimental comparison in terms of power  for m = 2
and n = 104 and ε = 0.5. All quantities involving the eigenvalues of the covariance operator were
computed from their counterparts instead of being estimated. The sampling from pn
2 was performed
by inverting the cumulative distribution function. The table below displays the results  averaged
over 10 Monte-Carlo runs.

5.2 Speaker veriﬁcation

We conducted experiments in a speaker veriﬁcation task [3]  on a subset of 8 female speakers using
data from the NIST 2004 Speaker Recognition Evaluation. We refer the reader to [16] for instance
for details on the pre-processing of data. The ﬁgure shows averaged results over all couples of speak-
ers. For each couple of speaker  at each run we took 3000 samples of each speaker and launched our
KFDA-test to decide whether samples come from the same speaker or not  and computed the type
II error by comparing the prediction to ground truth. We averaged the results for 100 runs for each
couple  and all couples of speaker. The level was set to α = 0.05  since the empirical level seemed
to match the prescribed for this value of the level as we noticed in previous subsection. We per-
formed the same experiments for the Maximum Mean Discrepancy and the Tajvidi-Hall test statistic
(TH  [13]). We summed up the results by plotting the ROC-curve for all competing methods. Our
method reaches good empirical power for a small value of the prescribed level (1 − β = 90% for
α = 0.05%). Maximum Mean Discrepancy also yields good empirical performance on this task.

6 Conclusion

We proposed a well-calibrated test statistic  built on kernel Fisher discriminant analysis  for which
we proved that the asymptotic limit distribution under null hypothesis is standard normal distribu-
tion. Our test statistic can be readily computed from Gram matrices once a kernel is deﬁned  and

5

1

0.8

0.6

0.4

0.2

r
e
w
o
P

0
 
0

0.1

ROC Curve

 

KFDA
MMD
TH

0.4

0.5

0.2

0.3

Level

Figure 1: Comparison of ROC curves in a speaker veriﬁcation task

allows us to perform nonparametric hypothesis testing for homogeneity for high-dimensional data.
The KFDA-test statistic yields competitive performance for speaker identiﬁcation.

7 Sketch of proof of asymptotic normality under null hypothesis

Outline. The proof of the asymptotic normality of the test statistics under null hypothesis follows
four steps. As a ﬁrst step  we derive an asymptotic approximation of the test statistics as γn +
n n−1/2 → 0   where the only remaining stochastic term is ˆδ. The test statistics is then spanned
γ−1
onto the eigenbasis of Σ  and decomposed into two terms Bn and Cn. The second step allows to
prove the asymptotic negligibility of Bn  while the third step establishes the asymptotic normality
of Cn by a martingale central limit theorem (MCLT).

Step 1: bTn(γn) = ˜Tn(γn) + oP (1). First  we may prove  using perturbation results of covariance

n n−1/2 → 0   we have

operators  that  as γn + γ−1

(n1n2/n) (cid:13)(cid:13)(cid:13)(Σ + γI)−1/2 ˆδ(cid:13)(cid:13)(cid:13)

√2d2(Σ  γ)

2

H − d1(Σ  γ)

+ oP (1) .

(9)

For ease of notation  in the following  we shall often omit Σ in quantities involving it. Hence  from
now on  λp  λq  d2 n stand for λp(Σ)  λq(Σ)  d2(Σ  γn). Deﬁne

bTn(γn) =
def= 
(cid:16) n2
n1n(cid:17)1/2(cid:16)ep(X (1)
−(cid:16) n1
n2n(cid:17)1/2(cid:16)ep(X (2)
nXi=1

i

Yn p i

1 )](cid:17)
) − E[ep(X (1)
1 )](cid:17) n1 + 1 ≤ i ≤ n .
i−n1 ) − E[ep(X (2)

1 ≤ i ≤ n1  

(10)

We now give formulas for the moments of {Yn p i}1≤i≤n p≥1  often used in the proof. Straightfor-
ward calculations give

E[Yn p iYn q i] = λ1/2

p λ1/2

q

δp q  

(11)

while the Cauchy-Schwarz inequality and the reproducing property give

Denote Sn p
with

An

def=

n1n2

n

.

p λ1/2

q

Cov(Y 2

n p i  Y 2

n q i) ≤ Cn−2|k|∞λ1/2

(12)
i=1 Yn p i. Using Eq. (11)  our test statistics now writes as ˜Tn = (√2d2 n)−1An
n p(cid:9) = Bn + 2Cn .

(λp + γn)−1(cid:8)S2

def= Pn
(cid:13)(cid:13)(cid:13)(Σ + γnI)−1/2ˆδ(cid:13)(cid:13)(cid:13)

n p − ES2

− d1 n =

∞Xp=1

(13)

2

6

Step 2: Bn = oP (1). The proof consists in computing the variance of this term. Since the variables

(14)

(15)

.

where Bn and Cn are deﬁned as follows

Bn

def=

∞Xp=1
∞Xp=1

nXi=1(cid:8)Y 2

n p i − EY 2
nXi=1

n p i(cid:9)  
Yn p i
i−1Xj=1
Yn p i and Yn q j are independent if i 6= j  then Var(Bn) =Pn
n p i − E[Y 2

(λp + γn)−1{Y 2

(λp + γn)−1

def=

Cn

Yn p j
n p i]}!

i=1 vn i  where

vn i

def= Var ∞Xp=1
∞Xp q=1
Using Eq. (12)  we get Pn

=

negligible  since by assumption we have γ−1

i=1 vn i ≤ Cn−1γ−2

p=1 λ1/2

p (cid:17)2
n (cid:16)P∞
n n−1/2 → 0 andP∞

p=1 λ1/2

p < ∞.

(λp + γn)−1(λq + γn)−1Cov(Y 2

n p i  Y 2

n q i) .

where the RHS above is indeed

Step 3: d−1
martingale differences (see e.g. [12  Theorem 3.2]). For = 1  . . .   n  denote

D−→ N(0  1/2). We use the central limit theorem (MCLT) for triangular arrays of

2 nCn

ξn i

def= d−1
2 n

(λp + γn)−1Yn p iMn p i−1   where Mn p i

def=

Yn p j  

(16)

and let Fn i = σ (Yn p j  p ∈ {1  . . .   n}  j ∈ {0  . . .   i}). Note that  by construction  ξn i is a mar-
tingale increment  i.e. E [ ξn i |Fn i−1] = 0. The ﬁrst step in the proof of the CLT is to establish
that

iXj=1

∞Xp=1

s2
n =

nXi=1

n i(cid:12)(cid:12)Fn i−1(cid:3) P−→ 1/2 .
E(cid:2) ξ2

The second step of the proof is to establish the negligibility condition. We use [12  Theorem
3.2]  which requires to establish that max1≤i≤n |ξn i| P−→ 0 (smallness) and E(max1≤i≤n ξ2
n i)
is bounded in n (tightness)  where ξn i is deﬁned in (16). We will establish the two conditions
simultaneously by checking that

ξ2

n i(cid:19) = o(1) .

1≤i≤n

E(cid:18) max
nXi=1

M 2

nXi=1

∞Xp=1
2 nXp6=q

Splitting the sum s2

n  between diagonal terms Dn  and off-diagonal terms En  we have

Dn = d−2
2 n

(λp + γn)−2

n p i−1

E[Y 2

n p i]  

En = d−2

(λp + γn)−1(λq + γn)−1

Mn p i−1Mn q i−1E[Yn p iYn q i] .

Consider ﬁrst the diagonal terms En. We ﬁrst compute its mean. Note that E[M 2

n p i] =

j=1

Pi
∞Xp=1

E[Y 2

n p j]. Using Eq. (11) we get

(λp + γn)−2

n p j]E[Y 2

n p i]

E[Y 2

nXi=1
i−1Xj=1
(λp + γn)−2
" nXi=1

=

1
2

∞Xp=1

E[Y 2

n p i]#2

−

nXi=1

E2[Y 2

n p i]

7

=

1
2

d2

2 n(cid:8)1 + O(n−1)(cid:9) .

(17)

(18)

(19)

(20)

Therefore  E[Dn] = 1/2 + o(1). Next  we may prove that Dn − E[Dn] = oP (1) is negligible  by
checking that Var[Dn] = o(1). We ﬁnally consider En deﬁned in (20)  and prove that En = oP (1)
using Eq. (11). This concludes the proof of Eq. (17).
We ﬁnally show Eq. (18). Since |Yn p i| ≤ n−1/2|k|1/2

∞ P-a.s we may bound

1≤i≤n|ξn i| ≤ Cd−1
max

2 nn−1/2

∞Xp=1

(λp + γn)−1 max

1≤i≤n|Mn p i−1| .

(21)

n p n−1] ≤ Cλ1/2

p

.

Then  the Doob inequality implies that E1/2[max1≤i≤n |Mn p i−1|2] ≤ E1/2[M 2
Plugging this bound in (21)  the Minkowski inequality
p )  

n i(cid:19) ≤ C(d−1

E1/2(cid:18) max

2 nγ−1

n n−1/2

λ1/2

∞Xp=1

ξ2

1≤i≤n

and the proof is concluded using the fact that γn + γ−1

n n−1/2 → 0 and Assumption (B1).

References
[1] D. L. Allen. Hypothesis testing using an L1-distance bootstrap. The American Statistician  51(2):145–

150  1997.

[2] N. H. Anderson  P. Hall  and D. M. Titterington. Two-sample test statistics for measuring discrepancies
between two multivariate probability density functions using kernel-based density estimates. Journal of
Multivariate Analysis  50(1):41–54  1994.

[3] F. Bimbot  J.-F. Bonastre  C. Fredouille  G. Gravier  I. Magrin-Chagnolleau  S. Meignier  T. Merlin 
J. Ortega-Garcia  D. Petrovska-Delacretaz  and D. A. Reynolds. A tutorial on text-independent speaker
veriﬁcation. EURASIP  4:430–51  2004.

[4] K. Borgwardt  A. Gretton  M. Rasch  H.-P. Kriegel  Sch¨olkopf  and A. J. Smola. Integrating structured

biological data by kernel maximum mean discrepancy. Bioinformatics  22(14):49–57  2006.

[5] H. Brezis. Analyse Fonctionnelle. Masson  1980.
[6] C. Butucea and K. Tribouley. Nonparametric homogeneity tests. Journal of Statistical Planning and

Inference  136(3):597–639  2006.

[7] E. Carlstein  H. M¨uller  and D. Siegmund  editors. Change-point Problems  number 23 in IMS Mono-

graph. Institute of Mathematical Statistics  Hayward  CA  1994.

[8] K. Fukumizu  A. Gretton  X. Sunn  and B. Sch¨olkopf. Kernel measures of conditional dependence. In

Adv. NIPS  2008.

[9] I. Gohberg  S. Goldberg  and M. A. Kaashoek. Classes of Linear Operators Vol. I. Birkh¨auser  1990.
[10] U. Grenander and M. Miller. Pattern Theory: from representation to inference. Oxford Univ. Press  2007.
[11] A. Gretton  K. Borgwardt  M. Rasch  B. Schoelkopf  and A. Smola. A kernel method for the two-sample

problem. In Adv. NIPS  2006.

[12] P. Hall and C. Heyde. Martingale Limit Theory and Its Application. Academic Press  1980.
[13] P. Hall and N. Tajvidi. Permutation tests for equality of distributions in high-dimensional settings.

Biometrika  89(2):359–374  2002.

[14] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer Series in

Statistics. Springer  2001.

[15] E. Lehmann and J. Romano. Testing Statistical Hypotheses (3rd ed.). Springer  2005.
[16] J. Louradour  K. Daoudi  and F. Bach. Feature space mahalanobis sequence kernels: Application to svm

speaker veriﬁcation. IEEE Transactions on Audio  Speech and Language Processing  2007. To appear.

[17] J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern Analysis. Cambridge Univ. Press  2004.
[18] I. Steinwart  D. Hush  and C. Scovel. An explicit description of the reproducing kernel hilbert spaces of

gaussian RBF kernels. IEEE Transactions on Information Theory  52:4635–4643  2006.

[19] G. Wahba. Spline Models for Observational Data. SIAM  1990.

8

,Jianshu Chen
Ji He
Yelong Shen
Lin Xiao
Xiaodong He
Jianfeng Gao
Xinying Song
Li Deng