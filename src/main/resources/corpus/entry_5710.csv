2019,Understanding Attention and Generalization in Graph Neural Networks,We aim to better understand attention over nodes in graph neural networks (GNNs) and identify factors influencing its effectiveness. We particularly focus on the ability of attention GNNs to generalize to larger  more complex or noisy graphs. Motivated by insights from the work on Graph Isomorphism Networks  we design simple graph reasoning tasks that allow us to study attention in a controlled environment. We find that under typical conditions the effect of attention is negligible or even harmful  but under certain conditions it provides an exceptional gain in performance of more than 60% in some of our classification tasks. Satisfying these conditions in practice is challenging and often requires optimal initialization or supervised training of attention. We propose an alternative recipe and train attention in a weakly-supervised fashion that approaches the performance of supervised models  and  compared to unsupervised models  improves results on several synthetic as well as real datasets. Source code and datasets are available at https://github.com/bknyaz/graph_attention_pool.,Understanding Attention and Generalization

in Graph Neural Networks

Boris Knyazev

University of Guelph

Vector Institute

bknyazev@uoguelph.ca

Graham W. Taylor
University of Guelph

gwtaylor@uoguelph.ca

Vector Institute  Canada CIFAR AI Chair

mohamed@robust.ai

Mohamed R. Amer∗

Robust.AI

Abstract

We aim to better understand attention over nodes in graph neural networks (GNNs)
and identify factors inﬂuencing its effectiveness. We particularly focus on the ability
of attention GNNs to generalize to larger  more complex or noisy graphs. Motivated
by insights from the work on Graph Isomorphism Networks  we design simple
graph reasoning tasks that allow us to study attention in a controlled environment.
We ﬁnd that under typical conditions the effect of attention is negligible or even
harmful  but under certain conditions it provides an exceptional gain in performance
of more than 60% in some of our classiﬁcation tasks. Satisfying these conditions
in practice is challenging and often requires optimal initialization or supervised
training of attention. We propose an alternative recipe and train attention in a
weakly-supervised fashion that approaches the performance of supervised models 
and  compared to unsupervised models  improves results on several synthetic as
well as real datasets. Source code and datasets are available at https://github.
com/bknyaz/graph_attention_pool.

1 Attention meets pooling in graph neural networks

The practical importance of attention in deep learning is well-established and there are many argu-
ments in its favor [1]  including interpretability [2  3]. In graph neural networks (GNNs)  attention
can be deﬁned over edges [4  5] or over nodes [6]. In this work  we focus on the latter  because 
despite being equally important in certain tasks  it is not as thoroughly studied [7]. To begin our
description  we ﬁrst establish a connection between attention and pooling methods. In convolutional
neural networks (CNNs)  pooling methods are generally based on uniformly dividing the regular grid
(such as one-dimensional temporal grid in audio) into local regions and taking a single value from
that region (average  weighted average  max  stochastic  etc.)  while attention in CNNs is typically a
separate mechanism that weights C-dimensional input X ∈ RN×C:

where Zi = αiXi - output for unit (node in a graph) i (cid:80)N

Z = α (cid:12) X 

(1)
i αi = 1  (cid:12) - element-wise multiplication 

N - the number of units in the input (i.e. number of nodes in a graph).
In GNNs  pooling methods generally follow the same pattern as in CNNs  but the pooling regions
(sets of nodes) are often found based on clustering [8  9  10]  since there is no grid that can be
uniformly divided into regions in the same way across all examples (graphs) in the dataset. Recently 
top-k pooling [11] was proposed  diverging from other methods: instead of clustering “similar” nodes 
it propagates only part of the input and this part is not uniformly sampled from the input. Top-k
pooling can thus select some local part of the input graph  completely ignoring the rest. For this
reason at ﬁrst glance it does not appear to be logical.

∗Most of this work was done while the author was at SRI International.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a) COLORS

(b) TRIANGLES

(c) MNIST

Figure 1: Three tasks with a controlled environment we consider in this work. The values inside the
nodes are ground truth attention coefﬁcients  αGT

  which we ﬁnd heuristically (see Section 3.1).

i

However  we can notice that pooled feature maps in [11  Eq. 2] are computed in the same way as
attention outputs Z in Eq. 1 above  if we rewrite their Eq. 2 in the following way:

(cid:26)αiXi  ∀i ∈ P

Zi =

∅ 

otherwise 

(2)
where P is a set of indices of pooled nodes  |P| ≤ N  and ∅ denotes the unit is absent in the output.
The only difference between Eq. 2 and Eq. 1 is that Z ∈ R|P|×C  i.e. the number of units in the output
is smaller or  formally  there exists a ratio r = |P|/N ≤ 1 of preserved nodes. We leverage this
ﬁnding to integrate attention and pooling into a uniﬁed computational block of a GNN. In contrast  in
CNNs  it is challenging to achieve this  because the input is deﬁned on a regular grid  so we need to
maintain resolution for all examples in the dataset after each pooling layer. In GNNs  we can remove
any number of nodes  so that the next layer will receive a smaller graph. When applied to the input
layer  this form of attention-based pooling also brings us interpretability of predictions  since the
network makes a decision only based on pooled nodes.
Despite the appealing nature of attention  it is often unstable to train and the conditions under which
it fails or succeeds are unclear. Motivated by insights of [12] recently proposed Graph Isomorphism
Networks (GIN)  we design two simple graph reasoning tasks that allow us to study attention in
a controlled environment where we know ground truth attention. The ﬁrst task is counting colors
in a graph (COLORS)  where a color is a unique discrete feature. The second task is counting the
number of triangles in a graph (TRIANGLES). We conﬁrm our observations on a standard benchmark 
MNIST [13] (Figure 1)  and identify factors inﬂuencing the effectiveness of attention.
Our synthetic experiments also allow us to study the ability of attention GNNs to generalize to larger 
more complex or noisy graphs. Aiming to provide a recipe to train more effective  stable and robust
attention GNNs  we propose a weakly-supervised scheme to train attention  that does not require
ground truth attention scores  and as such is agnostic to a dataset and the choice of a model. We
validate the effectiveness of this scheme on our synthetic datasets  as well as on MNIST and on real
graph classiﬁcation benchmarks in which ground truth attention is unavailable and hard to deﬁne 
namely COLLAB [14  15]  PROTEINS [16]  and D&D [17].

2 Model

We study two variants of GNNs: Graph Convolutional Networks (GCN) [18] and Graph Isomorphism
Networks (GIN) [12]. One of the main ideas of GIN is to replace the MEAN aggregator over
nodes  such as the one in GCN  with a SUM aggregator  and add more fully-connected layers after
aggregating neigboring node features. The resulting model can distinguish a wider range of graph
structures than previous models [12  Figure 3].

2.1 Thresholding by attention coefﬁcients
To pool the nodes in a graph using the method from[11] a predeﬁned ratio r = |P|/N (Eq. 2) must be
chosen for the entire dataset. For instance  for r = 0.8 only 80% of nodes are left after each pooling

2

GNNNode attentionClass=70.50.5Node attention0.50000.500.30.30.200.200.30.30.20.2Node attentionGNNGNNGNNNode attentionClass=70.50.5Node attention0.50000.500.30.30.200.200.30.30.20.2Node attentionGNNGNNGNNNode attentionClass=70.50.5Node attention0.50000.500.30.30.200.200.30.30.20.2Node attentionGNNGNNlayer. Intuitively  it is clear that this ratio should be different for small and large graphs. Therefore 
we propose to choose threshold ˜α  such that only nodes with attention values αi > ˜α are propagated:

(cid:26)αiXi  ∀i : αi > ˜α

otherwise.

Zi =

∅ 

(3)

Note  that dropping nodes from a graph is different from keeping nodes with very small  or even
zero  feature values  because a bias is added to node features after the following graph convolution
layer affecting features of neighbors. An important potential issue of dropping nodes is the change of
graph structure and emergence of isolated nodes. However  in our experiments we typically observe
that the model predicts similar α for nearby nodes  so that an entire local neighborhood is pooled or
dropped  as opposed to clustering-based methods which collapse each neighborhood to a single node.
We provide a quantitative and qualitative comparison in Section 3.

2.2 Attention subnetwork

To train an attention model that predicts the coefﬁcients for nodes  we consider two approaches:
(1) Linear Projection [11]  where a single layer projection p ∈ RC is trained: αpre = Xp; and
(2) DiffPool [10]  where a separate GNN is trained:

(4)
where A is the adjacency matrix of a graph.
In all cases  we use a softmax activation [1  2]
instead of tanh in [11]  because it provides more interpretable results and ecourages sparse outputs:
α = softmax(αpre). To train attention in a supervised or weakly-supervised way  we use the
Kullback-Leibler divergence loss (see Section 3.3).

αpre = GNN(A  X) 

2.3 ChebyGIN

In some of our experiments  the performance of both GCNs and GINs is quite poor and  consequently 
it is also hard for the attention subnetwork to learn. By combining GIN with ChebyNet [8]  we
propose a stronger model  ChebyGIN. ChebyNet is a multiscale extension of GCN [18]  so that for
the ﬁrst scale  K = 1  node features are node features themselves  for K = 2 features are averaged
over one-hop neighbors  for K = 3 - over two-hop neighbors and so forth. To implement the SUM
j Aij starting from K = 2.

aggregator in ChebyGIN  we multiply features by node degrees Di =(cid:80)

We also add more fully-connected layers after feature aggregation as in GIN.

3 Experiments

We introduce the color counting task (COLORS) and the triangle counting task (TRIANGLES) in
which we generate synthetic training and test graphs. We also experiment with MNIST images [13]
and three molecule and social datasets. In COLORS  TRIANGLES and MNIST tasks (Figure 1)  we
assume to know ground truth attention  i.e. for each node i we heuristically deﬁne its importance
i ∈ [0  1]  which is necessary to train (in the supervised case) and
in solving the task correctly  αGT
evaluate our attention models.

3.1 Datasets

COLORS. We introduce the color counting task. We generate random graphs where features for
each node are assigned to one of the three one-hot values (colors): [1 0 0] (red)  [0 1 0] (green) 
[0 0 1] (blue). The task is to count the number of green nodes  Ngreen. This is a trivial task  but it
lets us study the inﬂuence of initialization of the attention model p ∈ R3 on the training dynamics.
In this task  graph structure is unimportant and edges of graphs act like a medium to exchange
node features. Ground truth attention is αGT
i = 1/Ngreen  when i corresponds to green nodes and
i = 0 otherwise. We also extend this dataset to higher n-dimensional cases p ∈ Rn to study how
αGT
model performance changes with n. In these cases  node features are still one-hot vectors and we
classify the number of nodes where the second feature is one.
TRIANGLES. Counting the number of triangles in a graph is a well-known task which can be solved
analytically by computing trace(A3)/6  where A is an adjacency matrix. This task turned out to

3

i = Ti/(cid:80)

i Ti  where Ti is the number of triangles that include node i  so that αGT

be hard for GNNs  so we add node degree features as one-hot vectors to all graphs  so that the
model can exploit both graph structure and features. Compared to the COLORS task  here it is more
challenging to study the effect of initializing p  but we can still calculate ground truth attention as
i = 0 for
αGT
nodes that are not part of triangles.
MNIST-75SP. MNIST [13] contains 70k grayscale images of size 28×28 pixels. While each of
784 pixels can be represented as a node  we follow [19  20] and consider an alternative approach to
highlight the ability of GNNs to work on irregular grids. In particular  each image can be represented
as a small set of superpixels without losing essential class-speciﬁc information (see Figure 2). We
compute SLIC [21] superpixels for each image and build a graph  in which each node corresponds
to a superpixel with node features being pixel intensity values and coordinates of their centers of
masses. We extract N ≤ 75 superpixels  hence the dataset is denoted as MNIST-75SP. Edges are
formed based on spatial distance between superpixel centers as in [8  Eq. 8]. Each image depicts a
handwritten digit from 0 to 9 and the task is to classify the image. Ground truth attention is considered
to be αGT
i = 1/Nnonzero for superpixels with nonzero intensity  and Nnonzero is the total number of
such superpixels. The idea is that only nonzero superpixels determine the digit class.
Molecule and social datasets. We extend our study to more practical cases  where ground truth
attention is not available  and experiment with protein datasets: PROTEINS [16] and D&D [17]  and a
scientiﬁc collaboration dataset  COLLAB [14  15]. These are standard graph classiﬁcation benchmarks.
A standard way to evaluate models on these datasets is to perform 10-fold cross-validation and report
average accuracy [22  10]. In this work  we are concerned about a model’s ability to generalize
to larger and more complex or noisy graphs  therefore  we generate splits based on the number of
nodes. For instance  for PROTEINS we train on graphs with N ≤ 25 nodes and test on graphs with
6 ≤ N ≤ 620 nodes (see Table 2 for details about splits of other datasets and results).
A detailed description of tasks and model hyperparameters is provided in the Supp. Material.

3.2 Generalization to larger and noisy graphs

One of the core strengths of attention is that it makes it easier to generalize to unseen  potentially
more complex and/or noisy  inputs by reducing them to better resemble certain inputs in the training
set. To examine this phenomenon  for COLORS and TRIANGLES tasks we add test graphs that can
be several times larger (TEST-LARGE) than the training ones. For COLORS we further extend it by
adding unseen colors to the test set (TEST-LARGEC) in the format [c1  c2  c3  c4]  where ci = 0 for
i (cid:54)= 2 if c2 = 1 and ci ∈ [0  1] for i (cid:54)= 2 if c2 = 0  i.e. there is no new colors that have nonzero values
in a green channel. This can be interpreted as adding mixtures of red  blue and transparency channels 
with nine possible colors in total as opposed to three in the training set (Figure 2).

S
R
O
L
O
C

S
E
L
G
N
A
I
R
T

P
S
5
7
-

T
S
I
N
M

TRAIN (N ≤ 25)

TEST-ORIG (N ≤ 25)

TEST-LARGE (25 < N ≤ 200)

TEST-LARGEC (25 < N ≤ 200)

TRAIN (N ≤ 25)

TEST-ORIG (N ≤ 25)

TEST-LARGE (25 < N ≤ 100)

TRAIN(N = 64)

TEST-ORIG(N = 63)

TEST-NOISY(N = 63)

TEST-NOISYC(N = 63)

Figure 2: Examples from training and test sets. For COLORS  the correct label is Ngreen = 4 in all
cases; for TRIANGLES Ntri = 3 and color intensities denote ground truth attention values αGT . The
range of the number of nodes  N  is shown in each case. For MNIST-75SP  we visualize graphs
for digit 7 by assigning an average intensity value to all pixels within a superpixel. Even though
superpixels have certain shapes and borders between each other (visible only on noisy graphs)  we
feed only superpixel intensities and coordinates of their centers of masses to our GNNs.

4

0.00.10.20.30.00.10.20.30.00.10.2Neural networks (NNs) have been observed to be brittle if they are fed with test samples corrupted
in a subtle way  i.e. by adding a noise [23] or changing a sample in an adversarial way [24]  such
that a human can still recognize them fairly well. To study this problem  test sets of standard image
benchmarks have been enlarged by adding corrupted images [25].
Graph neural networks  as a particular case of NNs  inherit this weakness. The attention mechanism 
if designed and trained properly  can improve a net’s robustness by attending to only important and
ignoring misleading parts (nodes) of data. In this work  we explore the ability of GNNs with and
without attention to generalize to noisy graphs and unseen node features. This should help us to
understand the limits of GNNs  and potentially NNs in general  with attention and conditions when it
succeedes and when it does not. To this end  we generate two additional test sets for MNIST-75SP.
In the ﬁrst set  TEST-NOISY  we add Gaussian noise  drawn from N (0  0.4)  to superpixel intensity
features  i.e. the shape and coordinates of superpixels are the same as in the original clean test
set. In the second set  TEST-NOISY-C  we colorize images by adding two more channels and add
independent Gaussian noise  drawn from N (0  0.6)  to each channel (Figure 2).

3.3 Network architectures and training

We build 2 layer GNNs for COLORS and 3 layer GNNs for other tasks with 64 ﬁlters in each layer 
except for MNIST-75SP where we have more ﬁlters. Our baselines are GNNs with global sum or
max pooling (gpool)  DiffPool [10] and top-k pooling [11]. We add two layers of our pooling for
TRIANGLES  each of which is a GNN with 3 layers and 32 ﬁlters (Eq. 4); whereas a single pooling
layer in the form of vector p is used in other cases. We train all models with Adam [26]  learning rate
1e-3  batch size 32  weight decay 1e-4 (see the Supp. Material for details).
For COLORS and TRIANGLES we minimize the regression loss (MSE) and cross entropy (CE) for other
tasks  denoted as LM SE/CE. For experiments with supervised and weakly-supervised (described
below in Section 3.4) attention  we additionally minimize the Kullback-Leibler (KL) divergence loss
between ground truth attention αGT and predicted coefﬁcients α. The KL term is weighted by scale
β  so that the total loss for some training graph with N nodes becomes:

L = LM SE/CE +

β
N

αGT

i

log(

αGT
i
αi

).

(5)

We repeat experiments at least 10 times and report an average accuracy and standard deviation in
Tables 1 and 2. For COLORS we run experiments 100 times  since we observe larger variance. In
Table 1 we report results on all test subsets independently. In all other experiments on COLORS 
TRIANGLES and MNIST-75SP  we report an average accuracy on the combined test set. For COLLAB 
PROTEINS and D&D  we run experiments 10 times using splits described in Section 3.1.
The only hyperparameters that we tune in our experiments are threshold ˜α in our method (Eq. 3)  ratio
r in top-k (Eq. 2) and β in Eq. 5. For synthetic datasets  we tune them on a validation set generated
in the same way as TEST-ORIG. For MNIST-75SP  we use part of the training set. For COLLAB 
PROTEINS and D&D  we tune them using 10-fold cross-validation on the training set.
Attention correctness. We evaluate attention correctness using area under the ROC curve (AUC) as
an alternative to other methods  such as [27]  which can be overoptimistic in some extreme cases 
such as when all attention is concentrated in a single node or attention is uniformly spread over all
nodes. AUC allows us to evaluate the ranking of α instead of their absolute values. Compared to
ranking metrics  such as rank correlation  AUC enables us to directly choose a pooling threshold
˜α from the ROC curve by ﬁnding a desired balance between false-positives (pooling unimportant
nodes) and false-negatives (dropping important nodes).
To evaluate attention correctness of models with global pooling  we follow the idea from convolutional
neural networks [28]. After training a model  we remove node i ∈ [1  N ] and compute an absolute
difference from prediction y for the original graph:

(cid:88)

i

αW S

i =

(cid:80)N
|yi − y|
j=1 |yj − y|  

(6)

where yi is a model’s prediction for the graph without node i. While this method shows surprisingly
high AUC in some tasks  it is not built-in in training and thus does not help to train a better model
and only implicitly interprets a model’s prediction (Figures 5 and 7). However  these results inspired
us to design a weakly-supervised method described below.

5

3.4 Weakly-supervised attention supervision

i

Although for COLORS  TRIANGLES and MNIST-75SP we can deﬁne ground truth attention  so that
it does not require manual labeling  in practice it is usually not the case and such annotations are hard
to deﬁne and expensive  or even unclear how to produce. Based on results in Table 1  supervision
of attention is necessary to reveal its power. Therefore  we propose a weakly-supervised approach 
agnostic to the choice of a dataset and model  that does not require ground truth attention labels 
but can improve a model’s ability to generalize. Our approach is based on generating attention
coefﬁcients αW S
(Eq. 6) and using them as labels to train our attention model with the loss deﬁned in
Eq 5. We apply this approach to COLORS  TRIANGLES and MNIST-75SP and observe peformance
and robustness close to supervised models. We also apply it to COLLAB  PROTEINS and D&D  and
in all cases we are able to improve results compared to unsupervised attention.
Training weakly-supervised models. Assume we want to train model A with “weak-sup” attention
on a dataset without ground truth attention. We ﬁrst need to train model B that has the same
architecture as A  but does not have any attention/pooling between graph convolution layers. So 
model B has only global pooling. After training B with the LM SE/CE loss  we need to evaluate
training graphs on B in the same way as during computation of αW S in Eq. 6. In particular  for each
training graph G with N nodes  we ﬁrst make a prediction y for the entire G. Then  for each i ∈ [1  N ] 
we remove node i from G  and feed this reduced graph with N − 1 nodes to model B recording the
model’s prediction yi. We then use Eq. 6 to compute αW S based on y and yi. Now  we can train A
and use αW S instead of ground truth αGT in Eq. 5 to optimize both MSE/CE and KL losses.

4 Analysis of results

In this work  we aim to better understand attention and generalization in graph neural networks  and 
based on our empirical ﬁndings  below we provide our analysis for the following questions.
How powerful is attention over nodes in GNNs? Our results on the COLORS  TRIANGLES and
MNIST-75SP datasets suggest that the main strength of attention over nodes in GNNs is the ability to
generalize to more complex or noisy graphs at test time. This ability essentially transforms a model
that fails to generalize into a fairly robust one. Indeed  a classiﬁcation accuracy gap for COLORS-
LARGEC between the best model without supervised attention (GIN with global pooling) and a

Table 1: Results on three tasks for different test subsets. ± denotes standard deviation  not shown
in case of small values (large values are explained in Section 4). ATTN denotes attention accuracy in
terms of AUC and is computed for the combined test set. The best result in each column (ignoring up-
denotes poor results with relatively low accuracy and/or high variance;
per bound results) is bolded.
denotes failed cases with accuracy close to random and/or extremely high variance. † For COL-
ORS and MNIST-75SP  ChebyNets are used instead of ChebyGINs as described in the Supp. Material.

l
a
b
o
l
G

l
o
o
p

GCN
GIN
ChebyGIN†

. GIN  top-k
GIN  ours
ChebyGIN†  top-k
ChebyGIN†  ours

v
r
e
p
u
s
n
U

d GIN  topk
GIN  ours
ChebyGIN†  topk
ChebyGIN†  ours

e
s
i
v
r
e
p
u
S

k
a
e

W

.

p
u
s

ChebyGIN†  ours

d GIN

r
e
p
p
U

n
u
o
b

ChebyGIN†

ORIG

97
96±10
100

99.6
94±18
100
80±30

87±1
100
100
100

100

100
100

COLORS

LARGE LARGEC ATTN

TRIANGLES

ORIG LARGE ATTN ORIG

MNIST-75SP

NOISY

NOISYC ATTN

72±15
71±22
93±12

17±4
13±7
11±7
16±10

39±18
96±9
86±15
94±8

20±3
26±11
15±7

9±3
11±6
6±6
11±6

28±8
89±18
31±15
75±17

90±6

73±14

100
100

100
100

99.6
99.2
99.8

75±6
72±15
79±20
67±31

99.9
99.8
99.8
99.8

99.9

100
100

46±1
50±1
66±1

47±2
47±3
64±5
67±3

49±1
49±1
83±1
88±1

23±1
22±1
30±1

18±1
20±2
25±2
26±2

20±1
22±1
39±1
48±1

68±1

30±1

94±1
99.8

85±2
99.4±1

79
77
79

63±5
68±3
76±6
77±4

88
76±1
97
96

88

100
100

78.3±2
87.6±3
97.4

86±6
82.6±8
92.9±4
94.6±3

38±4
55±11
80±12

59±26
51±28
68±26
80±23

90.5±1
90.9±0.4
95.1±0.3
95.4±0.2

85.5±2
85.0±1
90.6±0.8
92.3±0.4

36±4
51±12
79±11

55±23
47±24
67±25
77±22

79±5
80±3
83±16
86±16

72±2
71±5
72±3

65±34
58±31
52±37
78±31

99.3
99.3
100
100

95.8±0.4

88.8±4

86±9

96.5±1

93.6±0.4
96.9±0.1

90.8±1
94.8±0.3

90.8±1
95.1±0.3

100
100

6

(a)

(c)

(a)-zoomed

(d)

(b)

(e)

(b)-zoomed

(f)

Figure 3: Disentangling factors inﬂuencing attention and classiﬁcation accuracy for COLORS (a-e)
and TRIANGLES (f). Accuracies are computed over all test subsets. Notice the exponential growth
of classiﬁcation accuracy depending on attention correctness (a b)  see zoomed plots (a)-zoomed 
(b)-zoomed for cases when attention AUC>95%. (d) Probability of a good initialization is estimated
as the proportion of cases when cosine similarity > 0.5; error bars indicate standard deviation. (c-e)
show results using a higher dimensional attention model  p ∈ Rn.

similar model with supervised attention (GIN  sup) is more than 60%. For TRIANGLES-LARGE this
gap is 18% and for MNIST-75SP-NOISY it is more than 12%. This gap is even larger if compared to
upper bound cases indicating that our supervised models can be further tuned and improved. Models
with supervised or weakly-supervised attention also have a more narrow spread of results (Figure 3).

bad initialization (cos. sim.=-0.75)

good initialization (cos. sim.=0.75)

optimal initialization (cos. sim.=1.00)

D
E
S
I

V
R
E
P
U
S
N
U

N
O

I
T
N
E
T
T
A

(a)

(b)

(c)

Figure 4: Inﬂuence of initialization on training dynamics for COLORS using GIN trained in the
unsupervised way. For the supervised cases  see the Supp. Material. The nodes that should be pooled
according to our ground truth prior  must have larger attention values α. However  in the unsupervised
cases  only the model with an optimal initialization (c) reaches a high accuracy  while other models
(a b) are stuck in a suboptimal state and wrong nodes are pooled  which degrades performance. In
these experiments  we train models longer to see if they can recover from a bad initialization.

What are the factors inﬂuencing performance of GNNs with attention? We identify three key
factors inﬂuencing performance of GNNs with attention: initialization of the attention model (i.e. vec-
tor p or GNN in Eq. 4)  strength of the main GNN model (i.e. the model that actually performs
classiﬁcation)  and ﬁnally other hyperparameters of the attention and GNN models.
We highlight initialization as the critical factor. We ran 100 experiments on COLORS with random
initializations (Figure 3  (a-e)) of the vector p and measured how performance of both attention and
classiﬁcation is affected depending on how close (in terms of cosine similarity) the initialized p was
to the optimal one  p = [0  1  0]. We disentangle the dependency between the classiﬁcation accuracy
and cos. sim. into two functions to make the relationship clearer (Figure 3  (a  c)). Interestingly  we
found that classiﬁcation accuracy depends exponentially on attention correctness and becomes close
to 100% only when attention is also close to being perfect. In the case of slightly worse attention  even
starting from 99%  classiﬁcation accuracy drops signiﬁcantly. This is an important ﬁnding that can
also be valid for other more realistic applications. In the TRIANGLES task we only partially conﬁrm
this ﬁnding  because our attention models could not achieve AUC high enough to boost classiﬁcation.
However  by observing the upper bound results obtained by training with ground truth attention  we
assume that this boost potentially should happen once attention becomes accurate enough.

7

020406080100Attention correctness (AUC  %)20406080100Avg. class. accuracy  %GIN (unsup  top-k)GIN (sup  top-k)GIN (unsup  ours)GIN (sup  ours)9596979899100Attention correctness (AUC  %)20406080100Avg. class. accuracy  %GIN (unsup  top-k)GIN (sup  top-k)GIN (unsup  ours)GIN (sup  ours)020406080100Attention correctness (AUC  %)20406080100Avg. class. accuracy  %ChebyGIN (unsup  top-k)ChebyGIN (sup  top-k)ChebyGIN (unsup  ours)ChebyGIN (sup  ours)ChebyGIN (weaksup  ours)9596979899100Attention correctness (AUC  %)20406080100Avg. class. accuracy  %ChebyGIN (unsup  top-k)ChebyGIN (sup  top-k)ChebyGIN (unsup  ours)ChebyGIN (sup  ours)ChebyGIN (weaksup  ours)1.00.50.00.51.0Cosine similarity with GT attention020406080100Attention correct. (AUC  %)GIN (unsup  ours  n=3)GIN (unsup  ours  n=16)GIN (sup  ours  n=16)51015202530Dimensionality of attention model p  n020406080100Avg. class. acc. / Prob.  %Prob. of good initChebyGIN (sup  ours)ChebyGIN (unsup  ours)ChebyGIN (weaksup  ours)20406080100Attention correctness (AUC  %)20406080100Avg. class. accuracy  %GIN (unsup  ours  n=16)GIN (sup  ours  n=16)405060708090100Attention correctness (AUC  %)3040506070Avg. class. accuracy  %ChebyGIN (sup  ours)ChebyGIN (unsup  ours)ChebyGIN (sup  top-k)ChebyGIN (unsup  top-k)ChebyGIN (weaksup  ours)02004006008001000Training epoch20406080100Accuracy / Ratio of nodes (%)Attn AUCAvg. class. accuracy of "should be pooled" nodes of "should be dropped" nodesRatio of pooled nodes  r0.010.020.030.040.05Predicted attention coeff.  02004006008001000Training epoch102030405060708090Accuracy / Ratio of nodes (%)0.010.020.030.040.05Predicted attention coeff.  02004006008001000Training epoch20406080100Accuracy / Ratio of nodes (%)0.0100.0150.0200.0250.0300.035Predicted attention coeff.  02004006008001000Training epoch20406080100Accuracy / Ratio of nodes (%)0.010.020.030.040.050.06Predicted attention coeff.  TEST-ORIG

TEST-NOISY

TEST-NOISYC

TRIANGLES TEST-LARGE

T
U
P
N
I

L
O
O
P
F
F
I

D

S
W
α

α

N = 93

N = 16

N = 93

N = 27

Figure 5: Qualitative analysis. For MNIST-75SP (on the left) we show examples of input test images
(top row)  results of DiffPool [10] (second row)  attention weights αW S generated using a model with
global pooling based on Eq. 6 (third row)  and α predicted by our weakly-supervised model (bottom
row). Both our attention-based pooling and DiffPool can be strong and interpretable depending on
the task  but in our tasks DiffPool was inferior (see the Supp. Material). For TRIANGLES (on the
right) we show an example of a test graph with N = 93 nodes with six triangles and the results of
pooling based on ground truth attention weights αGT (top row); in the bottom row we show attention
weights predicted by our weakly-supervised model and results of our threshold-based pooling (Eq. 3).
Note that during training  our model has not encountered noisy images (MNIST-75SP) nor graphs
larger than with N = 25 nodes (TRIANGLES).

Why is the variance of some results so high? In Table 1 we report high variance of results  which is
mainly due to initialization of the attention model as explained above. This variance is also caused by
initialization of other trainable parameters of a GNN  but we show that once the attention model is per-
fect  other parameters can recover from a bad initialization leading to better results. The opposite  how-
ever  is not true: we never observed recovery of a model with poorly initialized attention (Figure 4).
How top-k compares to our threshold-based pooling method? Our method to attend and pool
nodes (Eq. 3) is based on top-k pooling [11] and we show that the proposed threshold-based pooling
is superior in a principle way. When we use supervised attention our results are better by more than
40% on COLORS-LARGEC  by 9% on TRIANGLES-LARGE and by 3% on MNIST-75SP. In Figure 3
((a b)-zoomed) we show that GIN and ChebyGIN models with supervised top-k pooling never reach
an average accuracy of more than 80% as opposed to our method which reaches 100% in many cases.
How results change with increase of attention model input dimensionality or capacity? We
performed experiments using ChebyGIN-h - a model with higher dimensionality of an input to the
attention model (see the Supp. Material for details). In such cases  it becomes very unlikely to
initialize it in a way close to optimal (Figure 3  (c-e))  and attention accuracy is concentrated in the 60-
80% region. Effect of the attention model of such low accuracy is neglible or even harmful  especially
on the large and noisy graphs. We also experimented with a deeper attention model (ChebyGIN-h) 
i.e. a 2 layer fully-connected layer with 32 hidden units for COLORS and MNIST-75SP  and a deeper
GNN (Eq. 4) for TRIANGLES. This has a positive effect overall  except for TRIANGLES  where our
attention models were already deep GNNs.
Can we improve initialization of attention? In all our experiments  we initialize p from the Normal
distribution  N (0  1). To verify if the performance can be improved by choosing another distribution 
we evaluate GIN and GCN models on a wide range of random distributions  Normal N (0  σ) and
Uniform U (−σ  σ)  by varying scale σ (Figure 6). We found out that for unsupervised training

Figure 6: Inﬂuence of distribution parameters used to initialize the attention model p in the COL-
ORS task with n = 3 dimensional features. We show points corresponding to the commonly used
initialization strategies of Xavier [29] and Kaiming [29]. (a-c) Shaded areas show range  bars show
±1 std. For n = 16 see the Supp. Material.

8

0.000.030.060.080.110.000.010.030.040.050.00.51.01.52.02.5Scale/std of initialized attention weights  020406080100Avg. class. accuracy  %(a)GIN: COLORS (n=3)  unsup attentionXavier's normalXavier's uniformKaiming's normalKaiming's uniformnormaluniform0.00.51.01.52.02.5Scale/std of initialized attention weights  20406080100Avg. class. accuracy  %(b)GIN: COLORS (n=3)  sup attentionXavier's normalXavier's uniformKaiming's normalKaiming's uniformnormaluniform0.00.51.01.52.02.5Scale/std of initialized attention weights  20406080100Avg. class. accuracy  %(c)GIN: COLORS (n=3)  wsup attentionXavier's normalXavier's uniformKaiming's normalKaiming's uniformnormaluniform1.00.50.00.51.0Cosine similarity with GT attention0246810Std of init. attention for   (d)GIN: COLORS (n=3)  unsup attention020406080100Avg. class. accuracy  %1.00.50.00.51.0Cosine similarity with GT attention0246810Std of init. attention for   (e)GCN: COLORS (n=3)  unsup attention020406080100Avg. class. accuracy  %Table 2: Results on the social (COLLAB) and
molecule (PROTEINS and D&D) datasets. We
use 3 layer GCNs [18] or ChebyNets [8] (see
Supp. Material for architecture details). Dataset
subscripts denote the maximum number of nodes in
the training set according to our splits (Section 3.1).

(Figure 6  (a))  larger initial values and the Normal distribution should be used to make it possible to
converge to an optimal solution  which is still unlikely and greatly depends on cosine similarity with
GT attention (Figure 6  (d e)). For supervised and “weak-sup” attention  smaller initial weights and
either the Normal or Uniform distribution should be used (Figure 6  (b c)).
What is the recipe for more powerful atten-
tion GNNs? We showed that GNNs with su-
pervised training of attention are signiﬁcantly
more accurate and robust  although in case of
a bad initialization it can take a long time to
reach the performance of a better initialization.
However  supervised attention is often infea-
sible. We suggested an alternative approach
based on weakly-supervised training and val-
idated it on our synthetic (Table 1) and real
(Table 2) datasets. In case of COLORS  TRI-
ANGLES and MNIST-75SP we can compare
to both unsupervised and supervised models
and conclude that our approach shows perfor-
mance  robustness and relatively low variation
(i.e. sensitivity to initialization) similar to su-
pervised models and much better than unsu-
pervised models. In case of COLLAB  PROTEINS and D&D we can only compare to unsupervised
and global pooling models and conﬁrm that our method can be effectively employed for a wide
diversity of graph classiﬁcation tasks and attends to more relevant nodes (Figures 5 and 7). Tuning
the distribution and scale σ for the initialization of attention can further improve results. For instance 
on PROTEINS for the weakly-supervised case  we obtain 76.4% as opposed to 76.2%.

462 / 716 500 / 678
30-200
30-300
30-5748
201-5748

# train / test graphs 500 / 4500
# nodes (N) train
# nodes (N) test

32-35
32-492

COLLAB35 PROTEINS25 D&D200 D&D300

29.7±4.9 72.7±3.6
51.9±5.3 77.2±2.9

76.2±0.7

54.3±5.0 78.4±1.1

500 / 613

4-25
6-620

74.4±1.0
75.6±1.4

Global max
Unsup  ours

Weak-sup

65.9±3.4
65.7±3.5

67.0±1.7

GLOBAL POOL

UNSUP

UNSUP POOLED

WEAK-SUP

WEAK-SUP POOLED

5
3
B
A
L
L
O
C

5
2
S
N

I
E
T
O
R
P

0
0
2
D
&
D

Figure 7: Qualitative results. In COLLAB  a graph represents an ego-network of a researcher  therefore
center nodes are important. In PROTEINS and D&D  a graph is a protein and nodes are amino acids 
so it is important to attend to a connected chain of amino acids to distinguish an enzyme from
a non-enzyme protein. Our weakly-supervised method attends to and pools more relevant nodes
compared to global and unsupervised models  leading to better classiﬁcation results.

5 Conclusion

We have shown that learned attention can be extremely powerful in graph neural networks  but only
if it is close to optimal. This is difﬁcult to achieve due to the sensitivity of initialization  especially
in the unsupervised setting where we do not have access to ground truth attention. Thus  we have
identiﬁed initialization of attention models for high dimensional inputs as an important open issue.
We also show that attention can make GNNs more robust to larger and noisy graphs  and that the
weakly-supervised approach proposed in our work brings advantages similar to the ones of supervised
models  yet at the same time can be effectively applied to datasets without annotated attention.

9

Acknowledgments
This research was developed with funding from the Defense Advanced Research Projects Agency
(DARPA). The views  opinions and/or ﬁndings expressed are those of the author and should not be
interpreted as representing the ofﬁcial views or policies of the Department of Defense or the U.S. Gov-
ernment. The authors also acknowledge support from the Canadian Institute for Advanced Research
and the Canada Foundation for Innovation. We are also thankful to Angus Galloway for feedback.

References
[1] Ashish Vaswani  Noam Shazeer  Niki Parmar  Jakob Uszkoreit  Llion Jones  Aidan N Gomez  Łukasz
Kaiser  and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
Systems  pages 5998–6008  2017.

[2] Dong Huk Park  Lisa Anne Hendricks  Zeynep Akata  Bernt Schiele  Trevor Darrell  and Marcus Rohrbach.
Attentive explanations: Justifying decisions and pointing to the evidence. arXiv preprint arXiv:1612.04757 
2016.

[3] Andreea Deac  Petar Veli ˇCkovi´c  and Pietro Sormanni. Attentive cross-modal paratope prediction. Journal

of Computational Biology  2018.

[4] Petar Velickovic  Guillem Cucurull  Arantxa Casanova  Adriana Romero  Pietro Lio  and Yoshua Bengio.

Graph attention networks. In International Conference on Learning Representations (ICLR)  2018.

[5] Jiani Zhang  Xingjian Shi  Junyuan Xie  Hao Ma  Irwin King  and Dit-Yan Yeung. Gaan: Gated attention

networks for learning on large and spatiotemporal graphs. In UAI  2018.

[6] John Boaz Lee  Ryan Rossi  and Xiangnan Kong. Graph classiﬁcation using structural attention. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining 
pages 1666–1674. ACM  2018.

[7] John Boaz Lee  Ryan A Rossi  Sungchul Kim  Nesreen K Ahmed  and Eunyee Koh. Attention models in

graphs: A survey. arXiv preprint arXiv:1807.07984  2018.

[8] Michaël Defferrard  Xavier Bresson  and Pierre Vandergheynst. Convolutional neural networks on graphs
In Advances in Neural Information Processing Systems  pages

with fast localized spectral ﬁltering.
3844–3852  2016.

[9] Uri Shaham  Kelly Stanton  Henry Li  Boaz Nadler  Ronen Basri  and Yuval Kluger. Spectralnet: Spectral
clustering using deep neural networks. In International Conference on Learning Representations (ICLR) 
2018.

[10] Zhitao Ying  Jiaxuan You  Christopher Morris  Xiang Ren  Will Hamilton  and Jure Leskovec. Hierarchical
graph representation learning with differentiable pooling. In Advances in Neural Information Processing
Systems  pages 4805–4815  2018.

[11] Hongyang Gao and Shuiwang Ji. Graph U-Nets. In Proceedings of the 36th International Conference on

Machine Learning (ICML)  2018.

[12] Keyulu Xu  Weihua Hu  Jure Leskovec  and Stefanie Jegelka. How powerful are graph neural networks?

In International Conference on Learning Representations (ICLR)  2019.

[13] Yann LeCun  Léon Bottou  Yoshua Bengio  Patrick Haffner  et al. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[14] Jure Leskovec  Jon Kleinberg  and Christos Faloutsos. Graph evolution: Densiﬁcation and shrinking

diameters. ACM Transactions on Knowledge Discovery from Data (TKDD)  1(1):2  2007.

[15] Anshumali Shrivastava and Ping Li. A new space for comparing graphs. In Proceedings of the 2014
IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining  pages 62–71.
IEEE Press  2014.

[16] Karsten M Borgwardt  Cheng Soon Ong  Stefan Schönauer  SVN Vishwanathan  Alex J Smola  and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics  21(suppl_1):i47–i56 
2005.

[17] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without

alignments. Journal of molecular biology  330(4):771–783  2003.

[18] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. In

International Conference on Learning Representations (ICLR)  2017.

[19] Federico Monti  Davide Boscaini  Jonathan Masci  Emanuele Rodola  Jan Svoboda  and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition  volume 1  page 3  2017.

10

[20] Matthias Fey  Jan Eric Lenssen  Frank Weichert  and Heinrich Müller. Splinecnn: Fast geometric deep
learning with continuous b-spline kernels. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition  pages 869–877  2018.

[21] Radhakrishna Achanta  Appu Shaji  Kevin Smith  Aurelien Lucchi  Pascal Fua  Sabine Süsstrunk  et al.
Slic superpixels compared to state-of-the-art superpixel methods. IEEE Transactions on Pattern Analysis
and Machine Intelligence  2012.

[22] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD

International Conference on Knowledge Discovery and Data Mining  pages 1365–1374. ACM  2015.

[23] Samuel Dodge and Lina Karam. A study and comparison of human and deep learning recognition
performance under visual distortions. In 2017 26th international conference on computer communication
and networks (ICCCN)  pages 1–7. IEEE  2017.

[24] Christian Szegedy  Wojciech Zaremba  Ilya Sutskever  Joan Bruna  Dumitru Erhan  Ian Goodfellow 
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations (ICLR)  2014.

[25] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions

and perturbations. In International Conference on Learning Representations (ICLR)  2019.

[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

Conference on Learning Representations (ICLR)  2015.

In International

[27] Chenxi Liu  Junhua Mao  Fei Sha  and Alan L Yuille. Attention correctness in neural image captioning. In

Proceedings of the Thirty-First AAAI Conference on Artiﬁcial Intelligence  2017.

[28] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European

conference on computer vision  pages 818–833. Springer  2014.

[29] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international conference
on computer vision  pages 1026–1034  2015.

11

,Leena Chennuru Vankadara
Ulrike von Luxburg
Boris Knyazev
Graham Taylor
Mohamed Amer