2016,Optimal Black-Box Reductions Between Optimization Objectives,The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods  finely tuned to the specific regression or classification task at hand.  We reduce the complexity of algorithm design for machine learning by reductions:  we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications.  Furthermore  unlike existing results  our new reductions are OPTIMAL and more PRACTICAL. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions  and conclude with experiments showing their successes also in practice.,Optimal Black-Box Reductions
Between Optimization Objectives∗

Zeyuan Allen-Zhu

zeyuan@csail.mit.edu

Institute for Advanced Study

& Princeton University

Elad Hazan

ehazan@cs.princeton.edu

Princeton University

Abstract

The diverse world of machine learning applications has given rise to a plethora
of algorithms and optimization methods  ﬁnely tuned to the speciﬁc regression
or classiﬁcation task at hand. We reduce the complexity of algorithm design
for machine learning by reductions: we develop reductions that take a method
developed for one setting and apply it to the entire spectrum of smoothness and
strong-convexity in applications.
Furthermore  unlike existing results  our new reductions are optimal and more
practical. We show how these new reductions give rise to new and faster running
times on training linear classiﬁers for various families of loss functions  and
conclude with experiments showing their successes also in practice.

1

Introduction

The basic machine learning problem of minimizing a regularizer plus a loss function comes in
numerous different variations and names. Examples include Ridge Regression  Lasso  Support Vector
Machine (SVM)  Logistic Regression and many others. A multitude of optimization methods were
introduced for these problems  but in most cases specialized to very particular problem settings.
Such specializations appear necessary since objective functions for different classiﬁcation and
regularization tasks admin different convexity and smoothness parameters. We list below a few recent
algorithms along with their applicable settings.
• Variance-reduction methods such as SAGA and SVRG [9  14] intrinsically require the objective to
be smooth  and do not work for non-smooth problems like SVM. This is because for loss functions
such as hinge loss  no unbiased gradient estimator can achieve a variance that approaches to zero.
• Dual methods such as SDCA or APCG [20  30] intrinsically require the objective to be strongly
convex (SC)  and do not directly apply to non-SC problems. This is because for a non-SC objective
such as Lasso  its dual is not well-behaved due to the (cid:96)1 regularizer.
• Primal-dual methods such as SPDC [35] require the objective to be both smooth and SC. Many

other algorithms are only analyzed for both smooth and SC objectives [7  16  17].

In this paper we investigate whether such specializations are inherent. Is it possible to take a convex
optimization algorithm designed for one problem  and apply it to different classiﬁcation or regression
settings in a black-box manner? Such a reduction should ideally take full and optimal advantage of
the objective properties  namely strong-convexity and smoothness  for each setting.
Unfortunately  existing reductions are still very limited for at least two reasons. First  they incur at
least a logarithmic factor log(1/ε) in the running time so leading only to suboptimal convergence
∗The full version of this paper can be found on https://arxiv.org/abs/1603.05642. This paper is partially

supported by an NSF Grant  no. 1523815  and a Microsoft Research Grant  no. 0518584.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

rates.2 Second  after applying existing reductions  algorithms become biased so the objective value
does not converge to the global minimum. These theoretical concerns also translate into running time
losses and parameter tuning difﬁculties in practice.
In this paper  we develop new and optimal regularization and smoothing reductions that can
• shave off a non-optimal log(1/ε) factor
• produce unbiased algorithms
Besides such technical advantages  our new reductions also enable researchers to focus on designing
algorithms for only one setting but infer optimal results more broadly. This is opposed to results such
as [4  25] where the authors develop ad hoc techniques to tweak speciﬁc algorithms  rather than all
algorithms  and apply them to other settings without losing extra factors and without introducing bias.
Our new reductions also enable researchers to prove lower bounds more broadly [32].

1.1 Formal Setting and Classical Approaches
Consider minimizing a composite objective function

(cid:8)F (x) def= f (x) + ψ(x)(cid:9)  

min
x∈Rd

(1.1)

where f (x) is a differentiable convex function and ψ(x) is a relatively simple (but possibly non-
(cid:80)n
differentiable) convex function  sometimes referred to as the proximal function. Our goal is to ﬁnd a
point x ∈ Rd satisfying F (x) ≤ F (x∗) + ε  where x∗ is a minimizer of F .
i=1 fi((cid:104)x  ai(cid:105))
In most classiﬁcation and regression problems  f (x) can be written as f (x) = 1
where each ai ∈ Rd is a feature vector. We refer to this as the ﬁnite-sum case of (1.1).
n
• CLASSICAL REGULARIZATION REDUCTION.

2(cid:107)x0 − x(cid:107)2 in which
Given a non-SC F (x)  one can deﬁne a new objective F (cid:48)(x) def= F (x) + σ
σ is on the order of ε. In order to minimize F (x)  the classical regularization reduction calls an
oracle algorithm to minimize F (cid:48)(x) instead  and this oracle only needs to work with SC functions.
EXAMPLE. If F is L-smooth  one can apply accelerated gradient descent to minimize F (cid:48) and
ε ) iterations in terms of minimizing the
original F . This complexity has a suboptimal dependence on ε and shall be improved using our
new regularization reduction.

obtain an algorithm that converges in O((cid:112)L/ε log 1
Given a non-smooth F (x) of a ﬁnite-sum form  one can deﬁne a smoothed variant (cid:98)fi(α) for each
(cid:80)n
i=1 (cid:98)fi((cid:104)ai  x(cid:105)) + ψ(x). 4 In order to minimize F (x)  the classical
yields an algorithm that converges in O(cid:0) 1√
(cid:1) iterations for minimizing the original F (x).

fi(α) and let F (cid:48)(x) = 1
smoothing reduction calls an oracle algorithm to minimize F (cid:48)(x) instead  and this oracle only
needs to work with smooth functions.
EXAMPLE. If F (x) is σ-SC and one applies accelerated gradient descent to minimize F (cid:48)  this

• CLASSICAL SMOOTHING REDUCTION (FINITE-SUM CASE).3

n

σε log 1

ε

Again  the additional factor log(1/ε) can be removed using our new smoothing reduction.

Besides the non-optimality  applying the above two reductions gives only biased algorithms. One has
to tune the regularization or smoothing parameter  and the algorithm only converges to the minimum
of the regularized or smoothed problem F (cid:48)(x)  which can be away from the true minimizer of F (x)
by a distance proportional to the parameter. This makes the reduction hard to use in practice.

2Recall that obtaining the optimal convergence rate is one of the main goals in operations research and
machine learning. For instance  obtaining the optimal 1/ε rate for online learning was a major breakthrough
since the log(1/ε)/ε rate was discovered [13  15  26].

3Smoothing reduction is typically applied to the ﬁnite sum form only. This is because  for a general high

dimensional function f (x)  its smoothed variant (cid:98)f (x) may not be efﬁciently computable.
4More formally  one needs this variant to satisfy |(cid:98)fi(α) − fi(α)| ≤ ε for all α and be smooth at the
same time. This can be done at least in two classical ways if (cid:98)fi(α) is Lipschitz continuous. One is to deﬁne
(cid:98)fi(α) = Ev∈[−1 1][fi(α + εv)] as an integral of f over the scaled unit interval  see for instance Chapter 2.3 of
[12]  and the other is to deﬁne (cid:98)fi(α) = maxβ

(cid:8)β · α − f∗

2 α2} using the Fenchel dual f∗

i (β) − ε

i (β) of fi(α) 

see for instance [24].

2

dient ∇fi(x) and perform a proximal gradient update.

• SVRG and SAGA [14  34] solve the ﬁnite-sum form of (1.1) and satisfy HOOD with

• Katyusha [1] solves the ﬁnite-sum form of (1.1) and satisﬁes HOOD with Time(L  σ) =

To introduce our new reductions  we ﬁrst deﬁne a property on the oracle algorithm.
Our Black-Box Oracle. Consider an algorithm A that minimizes (1.1) when the objective F is
L-smooth and σ-SC. We say that A satisﬁes the homogenous objective decrease (HOOD) property in
time Time(L  σ) if  for every starting vector x0  A produces an output x(cid:48) satisfying F (x(cid:48))− F (x∗) ≤
F (x0)−F (x∗)
in time Time(L  σ). In other words  A decreases the objective value distance to the
minimum by a constant factor in time Time(L  σ)  regardless of how large or small F (x0) − F (x∗)
is. We give a few example algorithms that satisfy HOOD:
• Gradient descent and accelerated gradient descent satisfy HOOD with Time(L  σ) = O(L/σ)· C
∇f (x) and perform a proximal gradient update [23]. Many subsequent works in this line of
research also satisfy HOOD  including [3  7  16  17].

and Time(L  σ) = O((cid:112)L/σ) · C respectively  where C is the time needed to compute a gradient
Time(L  σ) = O(cid:0)n + L/σ(cid:1) · C1 where C1 is the time needed to compute a stochastic gra-
O(cid:0)n +(cid:112)nL/σ(cid:1) · C1.
AdaptReg. For objectives F (x) that are non-SC and L-smooth  our AdaptReg reduction calls
output(cid:98)x satisfying F ((cid:98)x) − F (x∗) ≤ ε with a total running time(cid:80)∞
the an oracle satisfying HOOD a logarithmic number of times  each time with a SC objective
2(cid:107)x − x0(cid:107)2 for an exponentially decreasing value σ. In the end  AdaptReg produces an
F (x) + σ
to the old reduction. In addition  AdaptReg is an unbiased and anytime algorithm. F ((cid:98)x) converges
• Applying AdaptReg to SVRG  we obtain a running time O(cid:0)n log 1
(cid:1) · C1 for minimizing
on known theoretical running time obtained by non-accelerated methods  including O(cid:0)n log 1
(cid:1) · C1 through direct methods such as
• Applying AdaptReg to Katyusha  we obtain a running time O(cid:0)n log 1
(cid:1)· C1 for minimiz-

Since most algorithms have an inverse polynomial dependence on σ in Time(L  σ)  when summing
up Time(L  ε · 2t) for positive values t  we do not incur the additional factor log(1/ε) as opposed
to F (x∗) as the time goes without the necessity of changing parameters  so the algorithm can be
interrupted at any time. We mention some theoretical applications of AdaptReg:

√
nL√
ε
√
ing ﬁnite-sum  non-SC  and smooth objectives (such as Lasso and Logistic Regression). This is
the ﬁrst and only known stochastic method that converges with the optimal 1/
ε rate (as opposed
to log(1/ε)/

ﬁnite-sum  non-SC  and smooth objectives (such as Lasso and Logistic Regression). This improves
ε +

(cid:1) · C1 through the old reduction  as well as O(cid:0) n+L

ε log 1
L
SAGA [9] and SAG [27].

ε

√

ε) for this class of objectives. [1]

ε

ε + L

ε

ε +

t=0 Time(L  ε · 2t).

1.2 Our New Results

4

• Applying AdaptReg to methods that do not originally work for non-SC objectives such as [7  16 
17]  we improve their running times by a factor of log(1/ε) for working with non-SC objectives.

AdaptSmooth and JointAdaptRegSmooth. For objectives F (x) that are ﬁnite-sum  σ-SC  but
non-smooth  our AdaptSmooth reduction calls an oracle satisfying HOOD a logarithmic number
of times  each time with a smoothed variant of F (λ)(x) and an exponentially decreasing smoothing

parameter λ. In the end  AdaptSmooth produces an output(cid:98)x satisfying F ((cid:98)x) − F (x∗) ≤ ε with a
total running time(cid:80)∞

t=0 Time( 1

ε·2t   σ).

Since most algorithms have a polynomial dependence on L in Time(L  σ)  when summing up
ε·2t   σ) for positive values t  we do not incur an additional factor of log(1/ε) as opposed to
Time( 1
the old reduction. AdaptSmooth is also an unbiased and anytime algorithm for the same reason as
AdaptReg.
In addition  AdaptReg and AdaptSmooth can effectively work together  to solve ﬁnite-sum  non-SC 
and non-smooth case of (1.1)  and we call this reduction JointAdaptRegSmooth.
We mention some theoretical applications of AdaptSmooth and JointAdaptRegSmooth:

3

• Applying AdaptReg to Katyusha  we obtain a running time O(cid:0)n log 1
• Applying JointAdaptRegSmooth to Katyusha  we obtain a running time O(cid:0)n log 1

√
ﬁnite-sum  SC  and non-smooth objectives (such as SVM). Therefore  Katyusha combined with
√
AdaptReg is the ﬁrst and only known stochastic method that converges with the optimal 1/
ε
rate (as opposed to log(1/ε)/
√

(cid:1)·C1 for minimizing
(cid:1) ·

ε) for this class of objectives. [1]

√
n√
σε

ε +

C1 for minimizing ﬁnite-sum  SC  and non-smooth objectives (such as L1-SVM). Therefore 
Katyusha combined with JointAdaptRegSmooth gives the ﬁrst and only known stochastic
method that converges with the optimal 1/ε rate (as opposed to log(1/ε)/ε) for this class of
objectives. [1]

ε +

n
ε

Roadmap. We provide notations in Section 2 and discuss related works in Section 3. We propose
AdaptReg in Section 4 and AdaptSmooth in Section 5. We leave proofs as well as the description
and analysis of JointAdaptRegSmooth to the full version of this paper. We include experimental
results in Section 7.

2 Preliminaries
In this paper we denote by ∇f (x) the full gradient of f if it is differentiable  or the subgradient if f
is only Lipschitz continuous. Recall some classical deﬁnitions on strong convexity and smoothness.
Deﬁnition 2.1 (smoothness and strong convexity). For a convex function f : Rn → R 
• f is σ-strongly convex if ∀x  y ∈ Rn  it satisﬁes f (y) ≥ f (x) + (cid:104)∇f (x)  y − x(cid:105) + σ
• f is L-smooth if ∀x  y ∈ Rn  it satisﬁes (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107).

2(cid:107)x − y(cid:107)2.

Case 2: ψ(x) is non-SC and f (x) is L-smooth. Examples:

Characterization of SC and Smooth Regimes.
In this paper we give numbers to the following
4 categories of objectives F (x) in (1.1). Each of them corresponds to some well-known training
problems in machine learning. (Letting (ai  bi) ∈ Rd × R be the i-th feature vector and label.)
Case 1: ψ(x) is σ-SC and f (x) is L-smooth. Examples:

• ridge regression: f (x) = 1
• elastic net: f (x) = 1

(cid:80)n
(cid:80)n
i=1((cid:104)ai  x(cid:105) − bi)2 and ψ(x) = σ
2(cid:107)x(cid:107)2
i=1((cid:104)ai  x(cid:105) − bi)2 and ψ(x) = σ
(cid:80)n
(cid:80)n
i=1((cid:104)ai  x(cid:105) − bi)2 and ψ(x) = λ(cid:107)x(cid:107)1.
i=1 log(1 + exp(−bi(cid:104)ai  x(cid:105))) and ψ(x) = λ(cid:107)x(cid:107)1.
(cid:80)n
Case 3: ψ(x) is σ-SC and f (x) is non-smooth (but Lipschitz continuous). Examples:
i=1 max{0  1 − bi(cid:104)ai  x(cid:105)} and ψ(x) = σ(cid:107)x(cid:107)2
(cid:80)n
2.
i=1 max{0  1 − bi(cid:104)ai  x(cid:105)} and ψ(x) = λ(cid:107)x(cid:107)1.

Case 4: ψ(x) is non-SC and f (x) is non-smooth (but Lipschitz continuous). Examples:

• Lasso: f (x) = 1
• logistic regression: f (x) = 1

• (cid:96)1-SVM: f (x) = 1

2(cid:107)x(cid:107)2
2.
2 + λ(cid:107)x(cid:107)1.

• SVM: f (x) = 1

2n

n

n

2n

2n

n

Deﬁnition 2.2 (HOOD property). We say an algorithm A(F  x0) solving Case 1 of problem (1.1)
satisﬁes the homogenous objective decrease (HOOD) property with time Time(L  σ) if  for every
starting point x0  it produces output x(cid:48) ← A(F  x0) such that F (x(cid:48))−minx F (x) ≤ F (x0)−minx F (x)
in time Time(L  σ).5
(cid:8) 1
In this paper  we denote by C the time needed for computing a full gradient ∇f (x) and performing
ψ(x))(cid:9). For the ﬁnite-sum case of problem (1.1)  we denote by C1 the time needed for computing
2(cid:107)x − x0(cid:107)2 + η((cid:104)∇f (x)  x − x0(cid:105) +
a proximal gradient update of the form x(cid:48) ← arg minx
(cid:8) 1
2(cid:107)x − x0(cid:107)2 + η((cid:104)∇fi((cid:104)ai  x(cid:105))ai  x − x0(cid:105) + ψ(x))(cid:9). For ﬁnite-sum forms of (1.1) 
a stochastic (sub-)gradient ∇fi((cid:104)ai  x(cid:105)) and performing a proximal gradient update of the form
x(cid:48) ← arg minx
C is usually on the magnitude of n × C1.
5Although our deﬁnition is only for deterministic algorithms  if the guarantee is probabilistic  i.e.  E(cid:2)F (x(cid:48))(cid:3)−

4

minx F (x) ≤ F (x0)−minx F (x)
and can be replaced with any other constant bigger than 1.

4

  all the results of this paper remain true. Also  the constant 4 is very arbitrary

4

Algorithm 1 The AdaptReg Reduction
Input: an objective F (·) in Case 2 (smooth and not necessarily strongly convex);

x0 a starting vector  σ0 an initial regularization parameter  T the number of epochs;
an algorithm A that solves Case 1 of problem (1.1).

Output: (cid:98)xT .
1: (cid:98)x0 ← x0.

(cid:98)xt+1 ← A(F (σt) (cid:98)xt).

Deﬁne F (σt)(x) def= σt
σt+1 ← σt/2.

2: for t ← 0 to T − 1 do
3:
4:
5:
6: end for

7: return(cid:98)xT .

2 (cid:107)x − x0(cid:107) + F (x).

3 Related Works
Catalyst/APPA [11  19] turn non-accelerated methods into accelerated ones  which is different from
the purpose of this paper. They can be used as regularization reductions from Case 2 to Case 1 (but
not from Cases 3 or 4); however  they suffer from two log-factor loss in the running time  and perform
poor in practice [1]. In particular  Catalyst/APPA ﬁx the regularization parameter throughout the
algorithm but our AdaptReg decreases it exponentially. Their results cannot imply ours.
Beck and Teboulle [5] give a reduction from Case 4 to Case 2. Such a reduction does not suffer from
√
a log-factor loss for almost trivial reason: by setting the smoothing parameter λ = ε and applying
any O(1/
ε)-convergence method for Case 2  we immediately obtain an O(1/ε) method for Case
4 without an extra log factor. Our main challenge in this paper is to provide log-free reductions to
Case 1;6 simple ideas fail to produce log-free reductions in this case because all efﬁcient algorithms
solving Case 1 (due to linear convergence) have a log factor. In addition  the Beck-Teboulle reduction
is biased but ours is unbiased.
The so-called homotopy methods (e.g. methods with geometrically decreasing regularizer/smoothing
weights) appeared a lot in the literature [6  25  31  33]. However  to the best of our knowledge  all
existing homotopy analysis either only work for speciﬁc algorithms [6  25  31] or solve only special
problems [33]. In other words  none of them provides all-purpose black-box reductions like we do.

4 AdaptReg: Reduction from Case 2 to Case 1
We now focus on solving Case 2 of problem (1.1): that is  f (·) is L-smooth  but ψ(·) is not necessarily
SC. We achieve so by reducing the problem to an algorithm A solving Case 1 that satisﬁes HOOD.

AdaptReg works as follows (see Algorithm 1). At the beginning of AdaptReg  we set(cid:98)x0 to equal x0 
an arbitrary given starting vector. AdaptReg consists of T epochs. At each epoch t = 0  1  . . .   T − 1 
2 (cid:107)x − x0(cid:107)2 + F (x). Here  the parameter
we deﬁne a σt-strongly convex objective F (σt)(x) def= σt
We run A on F (σt)(x) with starting vector(cid:98)xt in each epoch  and let the output be(cid:98)xt+1. After all T
σt+1 = σt/2 for each t ≥ 0 and σ0 is an input parameter to AdaptReg that will be speciﬁed later.
epochs are ﬁnished  AdaptReg simply outputs(cid:98)xT .
T = log2(∆/ε) produces an output(cid:98)xT satisfying F ((cid:98)xT ) − minx F (x) ≤ O(ε) in a total running
time of(cid:80)T−1

We state our main theorem for AdaptReg below and prove it in the full version of this paper.
Theorem 4.1 (AdaptReg). Suppose that in problem (1.1) f (·) is L-smooth. Let x0 be a starting
vector such that F (x0)− F (x∗) ≤ ∆ and (cid:107)x0 − x∗(cid:107)2 ≤ Θ. Then  AdaptReg with σ0 = ∆/Θ and

t=0 Time(L  σ0 · 2−t).7

Remark 4.2. We compare the parameter tuning effort needed for AdaptReg against the classical
regularization reduction. In the classical reduction  there are two parameters: T   the number of

6Designing reductions to Case 1 (rather than for instance Case 2) is crucial for various reasons. First 
algorithm design for Case 1 is usually easier (esp. in stochastic settings). Second  Case 3 can only be reduced to
Case 1 but not Case 2. Third  lower bound results [32] require reductions to Case 1.

probabilistic  i.e.  E(cid:2)F ((cid:98)xT )(cid:3) − minx F (x) ≤ O(ε). This is also true for other reduction theorems of this paper.

7If the HOOD property is only satisﬁed probabilistically as per Footnote 5  our error guarantee becomes

5

t=0 Time(L  σ0 · 2−t) =(cid:80)T−1

iterations that does not need tuning; and σ  which had better equal ε/Θ which is an unknown quantity
so requires tuning. In AdaptReg  we also need tune only one parameter  that is σ0. Our T need not

outputted. In our experiments later  we spent the same effort turning σ in the classical reduction and
σ0 in AdaptReg. As it can be easily seen from the plots  tuning σ0 is much easier than σ.
Corollary 4.3. When AdaptReg is applied to SVRG  we solve the ﬁnite-sum case of Case 2 with
ε ) · C1. This is

be tuned because AdaptReg can be interrupted at any moment and(cid:98)xt of the current epoch can be
running time(cid:80)T−1
(cid:1)·C1
(cid:1)·C1 obtained through the old reduction  and faster than O(cid:0) n+LΘ
faster than O(cid:0)(cid:0)n+ LΘ
ε +(cid:112)nLΘ/ε) · C1. This is
(cid:80)T−1
t=0 Time(L  σ0 · 2−t) = (cid:80)T−1
faster than O(cid:0)(cid:0)n +(cid:112)nL/ε(cid:1) log ∆
(cid:1) · C1 obtained through the old reduction on Katyusha [1].8

obtained by SAGA [9] and SAG [27].
When AdaptReg is applied to Katyusha  we solve the ﬁnite-sum case of Case 2 with running time

) · C1 = O(n log ∆

) · C1 = O(n log ∆

(cid:1) log ∆

t=0 O(n + L2t

√
nL2t√
σ0

t=0 O(n +

ε + LΘ

σ0

ε

ε

ε

ε

(cid:80)n

i (β) − λ

(cid:8)β · α − f∗

2 β2(cid:9) . Accordingly  we deﬁne F (λ)(x) def= 1

5 AdaptSmooth: Reduction from Case 3 to 1
We now focus on solving the ﬁnite-sum form of Case 3 for problem (1.1). Without loss of generality 
we assume (cid:107)ai(cid:107) = 1 for each i ∈ [n] because otherwise one can scale fi accordingly. We solve
this problem by reducing it to an oracle A which solves the ﬁnite-sum form of Case 1 and satisﬁes
HOOD. Recall the following deﬁnition using Fenchel conjugate:9
Deﬁnition 5.1. For each function fi : R → R  let f∗
conjugate. Then  we deﬁne the following smoothed variant of fi parameterized by λ > 0: f (λ)
maxβ

i (β) def= maxα{α · β − fi(α)} be its Fenchel
(α) def=
((cid:104)ai  x(cid:105)) + ψ(x) .
(·) is
From the property of Fenchel conjugate (see for instance the textbook [28])  we know that f (λ)
a (1/λ)-smooth function and therefore the objective F (λ)(x) falls into the ﬁnite-sum form of Case 1
for problem (1.1) with smoothness parameter L = 1/λ.
Our AdaptSmooth works as follows (see pseudocode in the full version). At the beginning of
T epochs. At each epoch t = 0  1  . . .   T − 1  we deﬁne a (1/λt)-smooth objective F (λt)(x) using
Deﬁnition 5.1. Here  the parameter λt+1 = λt/2 for each t ≥ 0 and λ0 is an input parameter to

AdaptSmooth  we set(cid:98)x0 to equal x0  an arbitrary given starting vector. AdaptSmooth consists of
AdaptSmooth that will be speciﬁed later. We run A on F (λt)(x) with starting vector(cid:98)xt in each
epoch  and let the output be(cid:98)xt+1. After all T epochs are ﬁnished  AdaptSmooth outputs(cid:98)xT .
λ0 = ∆/G2 and T = log2(∆/ε) produces an output(cid:98)xT satisfying F ((cid:98)xT ) − minx F (x) ≤ O(ε)
in a total running time of(cid:80)T−1

We state our main theorem for AdaptSmooth below and prove it the full version of this paper.
Theorem 5.2. Suppose that in problem (1.1)  ψ(·) is σ strongly convex and each fi(·) is G-Lipschitz
continuous. Let x0 be a starting vector such that F (x0) − F (x∗) ≤ ∆. Then  AdaptSmooth with

i=1 f (λ)

t=0 Time(2t/λ0  σ).

n

i

i

i

Remark 5.3. We emphasize that AdaptSmooth requires less parameter tuning effort than the old
reduction for the same reason as in Remark 4.2. Also  AdaptSmooth  when applied to Katyusha  pro-
vides the fastest running time on solving the Case 3 ﬁnite-sum form of (1.1)  similar to Corollary 4.3.

JointAdaptRegSmooth: From Case 4 to 1

6
We show in the full version that AdaptReg and AdaptSmooth can work together to reduce the
ﬁnite-sum form of Case 4 to Case 1. We call this reduction JointAdaptRegSmooth and it relies
on a jointly exponentially decreasing sequence of (σt  λt)  where σt is the weight of the convexity
parameter that we add on top of F (x)  and λt is the smoothing parameter that determines how we

will be lost.

8If the old reduction is applied on APCG  SPDC  or AccSDCA rather than Katyusha  then two log factors
9For every explicitly given fi(·)  this Fenchel conjugate can be symbolically computed and fed into the
algorithm. This pre-process is needed for nearly all known algorithms in order for them to apply to non-smooth
settings (such as SVRG  SAGA  SPDC  APCG  SDCA  etc).

6

(a) covtype  λ = 10−6

(b) mnist  λ = 10−5

(c) rcv1  λ = 10−5

Figure 1: Comparing AdaptReg and the classical reduction on Lasso (with (cid:96)1 regularizer weight λ).
y-axis is the objective distance to minimum  and x-axis is the number of passes to the dataset. The
blue solid curves represent APCG under the old regularization reduction  and the red dashed curve
represents APCG under AdaptReg. For other values of λ  or the results on SDCA  please refer to the
full version of this paper.
change each fi(·). The analysis is analogous to a careful combination of the proofs for AdaptReg
and AdaptSmooth.

7 Experiments
We perform experiments to conﬁrm our theoretical speed-ups obtained for AdaptSmooth and
AdaptReg. We work on minimizing Lasso and SVM objectives for the following three well-known
datasets that can be found on the LibSVM website [10]: covtype  mnist  and rcv1. We defer some
dataset and implementation details the full version of this paper.

7.1 Experiments on AdaptReg

To test the performance of AdaptReg  consider the Lasso objective which is in Case 2 (i.e. non-SC
but smooth). We apply AdaptReg to reduce it to Case 1 and apply either APCG [20]  an accelerated
method  or (Prox-)SDCA [29  30]  a non-accelerated method. Let us make a few remarks:
• APCG and SDCA are both indirect solvers for non-strongly convex objectives and therefore
regularization is intrinsically required in order to run them for Lasso or more generally Case 2.
• APCG and SDCA do not satisfy HOOD in theory. However  they still beneﬁt from AdaptReg as

we shall see  demonstrating the practical value of AdaptReg.

A Practical Implementation.
In principle  one can implement AdaptReg by setting the termination
criteria of the oracle in the inner loop as precisely suggested by the theory  such as setting the number
of iterations for SDCA to be exactly T = O(n + L
) in the t-th epoch. However  in practice  it is
σt
more desirable to automatically terminate the oracle whenever the objective distance to the minimum
has been sufﬁciently decreased. In all of our experiments  we simply compute the duality gap and
terminate the oracle whenever the duality gap is below 1/4 times the last recorded duality gap of the
previous epoch. For details see the full version of this paper.

Experimental Results. For each dataset  we consider three different magnitudes of regularization
weights for the (cid:96)1 regularizer in the Lasso objective. This totals 9 analysis tasks for each algorithm.
2(cid:107)x(cid:107)2 term to the
For each such a task  we ﬁrst implement the old reduction by adding an additional σ
Lasso objective and then apply APCG or SDCA. We consider values of σ in the set {10k  3 · 10k :
k ∈ Z} and show the most representative six of them in the plots (blue solid curves in Figure 3 and
Figure 4). Naturally  for a larger value of σ the old reduction converges faster but to a point that is
farther from the exact minimizer because of the bias. We implement AdaptReg where we choose the
initial parameter σ0 also from the set {10k  3 · 10k : k ∈ Z} and present the best one in each of 18
plots (red dashed curves in Figure 3 and Figure 4). Due to space limitations  we provide only 3 of the
18 plots for medium-sized λ in the main body of this paper (see Figure 1)  and include Figure 3 and
4 only in the full version of this paper.
It is clear from our experiments that
• AdaptReg is more efﬁcient than the old regularization reduction;
• AdaptReg requires no more parameter tuning than the classical reduction;

7

1E-101E-091E-081E-071E-061E-051E-041E-031E-021E-010204060801001E-081E-071E-061E-051E-041E-031E-021E-010204060801001E-051E-041E-031E-021E-011E+00020406080100(a) covtype  σ = 10−5

(b) mnist  σ = 10−4

(c) rcv1  σ = 10−4

Figure 2: Comparing AdaptSmooth and the classical reduction on SVM (with (cid:96)2 regularizer weight
λ). y-axis is the objective distance to minimum  and x-axis is the number of passes to the dataset.
The blue solid curves represent SVRG under the old smoothing reduction  and the red dashed curve
represents SVRG under AdaptSmooth. For other values of σ  please refer to the full version.
• AdaptReg is unbiased so simpliﬁes the parameter selection procedure.10

7.2 Experiments on AdaptSmooth

To test the performance of AdaptSmooth  consider the SVM objective which is non-smooth but SC.
We apply AdaptSmooth to reduce it to Case 1 and apply SVRG [14]. We emphasize that SVRG is
an indirect solver for non-smooth objectives and therefore regularization is intrinsically required in
order to run SVRG for SVM or more generally for Case 3.
A Practical Implementation.
In principle  one can implement AdaptSmooth by setting the termi-
nation criteria of the oracle in the inner loop as precisely suggested by the theory  such as setting
the number of iterations for SVRG to be exactly T = O(n + 1
) in the t-th epoch. In practice 
σλt
however  it is more desirable to automatically terminate the oracle whenever the objective distance
to the minimum has been sufﬁciently decreased. In all of our experiments  we simply compute the
Euclidean norm of the full gradient of the objective  and terminate the oracle whenever the norm is
below 1/3 times the last recorded Euclidean norm of the previous epoch. For details see full version.
Experimental Results. For each dataset  we consider three different magnitudes of regularization
weights for the (cid:96)2 regularizer in the SVM objective. This totals 9 analysis tasks. For each such a task 
we ﬁrst implement the old reduction by smoothing the hinge loss functions (using Deﬁnition 5.1) with
parameter λ > 0 and then apply SVRG. We consider different values of λ in the set {10k  3 · 10k :
k ∈ Z} and show the most representative six of them in the plots (blue solid curves in Figure 5).
Naturally  for a larger λ  the old reduction converges faster but to a point that is farther from the exact
minimizer due to its bias. We then implement AdaptSmooth where we choose the initial smoothing
parameter λ0 also from the set {10k  3 · 10k : k ∈ Z} and present the best one in each of the 9
plots (red dashed curves in Figure 5). Due to space limitations  we provide only 3 of the 9 plots for
small-sized σ in the main body of this paper (see Figure 2  and include Figure 5 only in full version.
It is clear from our experiments that
• AdaptSmooth is more efﬁcient than the old smoothing reduction  especially when the desired
• AdaptSmooth requires no more parameter tuning than the classical reduction;
• AdaptSmooth is unbiased and simpliﬁes the parameter selection for the same reason as

training error is small;

Footnote 10.

References
[1] Zeyuan Allen-Zhu. Katyusha: The First Direct Acceleration of Stochastic Gradient Methods. ArXiv

e-prints  abs/1603.05953  March 2016.

[2] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate uniﬁcation of gradient and mirror

descent. ArXiv e-prints  abs/1407.1537  July 2014.

[3] Zeyuan Allen-Zhu  Peter Richtárik  Zheng Qu  and Yang Yuan. Even faster accelerated coordinate descent

using non-uniform sampling. In ICML  2016.

10It is easy to determine the best σ0 in AdaptReg  and in contrast  in the old reduction if the desired error is

somehow changed for the application  one has to select a different σ and restart the algorithm.

8

1E-071E-061E-051E-041E-031E-021E-011E+000204060801001E-071E-061E-051E-041E-031E-021E-011E+000204060801001E-041E-031E-021E-011E+000204060801002015-06.

[4] Zeyuan Allen-Zhu and Yang Yuan. Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex

Objectives. In ICML  2016.

[5] Amir Beck and Marc Teboulle. Smoothing and ﬁrst order methods: A uniﬁed framework. SIAM Journal

on Optimization  22(2):557–580  2012.

[6] Radu Ioan Bo¸t and Christopher Hendrich. A variable smoothing algorithm for solving convex optimization

problems. TOP  23(1):124–150  2015.

[7] Sébastien Bubeck  Yin Tat Lee  and Mohit Singh. A geometric alternative to Nesterov’s accelerated

gradient descent. ArXiv e-prints  abs/1506.08187  June 2015.

[8] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm for convex problems with

applications to imaging. Journal of Mathematical Imaging and Vision  40(1):120–145  2011.

[9] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. SAGA: A Fast Incremental Gradient Method

With Support for Non-Strongly Convex Composite Objectives. In NIPS  2014.

[10] Rong-En Fan and Chih-Jen Lin. LIBSVM Data: Classiﬁcation  Regression and Multi-label. Accessed:

[11] Roy Frostig  Rong Ge  Sham M. Kakade  and Aaron Sidford. Un-regularizing: approximate proximal point
and faster stochastic algorithms for empirical risk minimization. In ICML  volume 37  pages 1–28  2015.
[12] Elad Hazan. DRAFT: Introduction to online convex optimimization. Foundations and Trends in Machine

Learning  XX(XX):1–168  2015.

[13] Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: Optimal algorithms for stochastic

strongly-convex optimization. The Journal of Machine Learning Research  15(1):2489–2512  2014.

[14] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In Advances in Neural Information Processing Systems  NIPS 2013  pages 315–323  2013.

[15] Simon Lacoste-Julien  Mark W. Schmidt  and Francis R. Bach. A simpler approach to obtaining an o(1/t)

convergence rate for the projected stochastic subgradient method. ArXiv e-prints  abs/1212.2002  2012.
[16] Yin Tat Lee and Aaron Sidford. Efﬁcient accelerated coordinate descent methods and faster algorithms for

solving linear systems. In FOCS  pages 147–156. IEEE  2013.

[17] Laurent Lessard  Benjamin Recht  and Andrew Packard. Analysis and design of optimization algorithms

via integral quadratic constraints. CoRR  abs/1408.3595  2014.

[18] Hongzhou Lin. private communication  2016.
[19] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A Universal Catalyst for First-Order Optimization. In

NIPS  2015.

[20] Qihang Lin  Zhaosong Lu  and Lin Xiao. An Accelerated Proximal Coordinate Gradient Method and its

Application to Regularized Empirical Risk Minimization. In NIPS  pages 3059–3067  2014.

[21] Arkadi Nemirovski. Prox-Method with Rate of Convergence O(1/t) for Variational Inequalities with
Lipschitz Continuous Monotone Operators and Smooth Convex-Concave Saddle Point Problems. SIAM
Journal on Optimization  15(1):229–251  January 2004.

[22] Yurii Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2). In

Doklady AN SSSR (translated as Soviet Mathematics Doklady)  volume 269  pages 543–547  1983.

[23] Yurii Nesterov. Introductory Lectures on Convex Programming Volume: A Basic course  volume I. Kluwer

Academic Publishers  2004.

152  December 2005.

[24] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming  103(1):127–

[25] Francesco Orabona  Andreas Argyriou  and Nathan Srebro. Prisma: Proximal iterative smoothing algorithm.

arXiv preprint arXiv:1206.2372  2012.

[26] Alexander Rakhlin  Ohad Shamir  and Karthik Sridharan. Making gradient descent optimal for strongly

convex stochastic optimization. In ICML  2012.

[27] Mark Schmidt  Nicolas Le Roux  and Francis Bach. Minimizing ﬁnite sums with the stochastic average
gradient. arXiv preprint arXiv:1309.2388  pages 1–45  2013. Preliminary version appeared in NIPS 2012.
[28] Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and Trends in

[29] Shai Shalev-Shwartz and Tong Zhang. Proximal Stochastic Dual Coordinate Ascent. arXiv preprint

Machine Learning  4(2):107–194  2012.

arXiv:1211.2717  pages 1–18  2012.

[30] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research  14:567–599  2013.

[31] Quoc Tran-Dinh. Adaptive smoothing algorithms for nonsmooth composite convex minimization. arXiv

preprint arXiv:1509.00106  2015.

[32] Blake Woodworth and Nati Srebro. Tight Complexity Bounds for Optimizing Composite Objectives. In

NIPS  2016.

[33] Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares problem.

SIAM Journal on Optimization  23(2):1062–1091  2013.

[34] Lin Xiao and Tong Zhang. A Proximal Stochastic Gradient Method with Progressive Variance Reduction.

SIAM Journal on Optimization  24(4):2057—-2075  2014.

[35] Yuchen Zhang and Lin Xiao. Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk

Minimization. In ICML  2015.

9

,Marius Pachitariu
Adam Packer
Noah Pettit
Henry Dalgleish
Michael Hausser
Maneesh Sahani
Branislav Kveton
Zheng Wen
Azin Ashkan
Csaba Szepesvari
Zeyuan Allen-Zhu
Elad Hazan