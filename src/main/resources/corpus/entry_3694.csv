2014,Near-Optimal-Sample Estimators for Spherical Gaussian Mixtures,Many important distributions are high dimensional  and often they can be modeled as Gaussian mixtures. We derive the first sample-efficient polynomial-time estimator for high-dimensional spherical Gaussian mixtures. Based on intuitive spectral reasoning  it approximates mixtures of $k$ spherical Gaussians in $d$-dimensions to within$\ell_1$ distance $\epsilon$ using $\mathcal{O}({dk^9(\log^2 d)}/{\epsilon^4})$ samples and $\mathcal{O}_{k \epsilon}(d^3\log^5 d)$ computation time. Conversely  we show that any estimator requires $\Omega\bigl({dk}/{\epsilon^2}\bigr)$ samples  hence the algorithm's sample complexity is nearly optimal in the dimension. The implied time-complexity factor \mathcal{O}_{k \epsilon}$ is exponential in $k$  but much smaller than previously known. We also construct a simple estimator for one-dimensional Gaussian mixtures that uses $\tilde\mathcal{O}(k /\epsilon^2)$ samples and $\tilde\mathcal{O}((k/\epsilon)^{3k+1})$ computation time.,Near-Optimal-Sample Estimators for Spherical

Gaussian Mixtures

Jayadev Acharya∗

MIT

jayadev@mit.edu

Ashkan Jafarpour  Alon Orlitsky  Ananda Theertha Suresh

{ashkan  alon  asuresh}@ucsd.edu

UC San Diego

Abstract

Many important distributions are high dimensional  and often they can be modeled
as Gaussian mixtures. We derive the ﬁrst sample-efﬁcient polynomial-time esti-
mator for high-dimensional spherical Gaussian mixtures. Based on intuitive spec-
tral reasoning  it approximates mixtures of k spherical Gaussians in d-dimensions

to within (cid:96)1 distance  using O(dk9(log2 d)~4) samples and Ok (d3 log5 d)
computation time. Conversely  we show that any estimator requires Ωdk~2
mension. The implied time-complexity factorOk  is exponential in k  but much
uses ̃O(k~2) samples and ̃O((k~)3k+1) computation time.

smaller than previously known.
We also construct a simple estimator for one-dimensional Gaussian mixtures that

samples  hence the algorithm’s sample complexity is nearly optimal in the di-

1

Introduction

1.1 Background

Meaningful information often resides in high-dimensional spaces: voice signals are expressed in
many frequency bands  credit ratings are inﬂuenced by multiple parameters  and document topics
are manifested in the prevalence of numerous words. Some applications  such as topic modeling
and genomic analysis consider data in over 1000 dimensions [31  14]. Typically  information can
be generated by different types of sources: voice is spoken by men or women  credit parameters
correspond to wealthy or poor individuals  and documents address topics such as sports or politics.
In such cases the overall data follow a mixture distribution [26  27]. Mixtures of high-dimensional
distributions are therefore central to the understanding and processing of many natural phenomena.
Methods for recovering the mixture components from the data have consequently been extensively
studied by statisticians  engineers  and computer scientists.
Initially  heuristic methods such as expectation-maximization were developed [25  21]. Over the
past decade  rigorous algorithms were derived to recover mixtures of d-dimensional spherical Gaus-
sians [10  18  4  8  29] and general Gaussians [9  2  5  19  22  3]. Many of these algorithms consider

mixtures where the (cid:96)1 distance between the mixture components is 2− od(1)  namely approaches

the maximum of 2 as d increases. They identify the distribution components in time and samples
that grow polynomially in d. Recently  [5  19  22] showed that the parameters of any k-component
d-dimensional Gaussian mixture can be recovered in time and samples that grow as a high-degree
polynomial in d and exponentially in k.
A different approach that avoids the large component-distance requirement and the high time and
sample complexity  considers a slightly relaxed notion of approximation  sometimes called PAC
learning [20]  or proper learning  that does not approximate each mixture component  but instead

∗Author was a student at UC San Diego at the time of this work

1

derives a mixture distribution that is close to the original one. Speciﬁcally  given a distance bound

> 0  error probability δ> 0  and samples from the underlying mixture f  where we use boldface
such that D(f   ˆf)≤  with probability≥ 1− δ  where D(⋅ ⋅) is some given distance measure  for

letters for d-dimensional objects  PAC learning seeks a mixture estimate ˆf with at most k components

example (cid:96)1 distance or KL divergence.
An important and extensively studied special case of Gaussian mixtures is mixture of spherical-
Gaussians [10  18  4  8  29]  where for each component the d coordinates are distributed indepen-
dently with the same variance  though possibly with different means. Note that different components
can have different variances. Due to their simple structure  spherical-Gaussian mixtures are easier to
analyze and under a minimum-separation assumption have provably-practical algorithms for clus-
tering and parameter estimation. We consider spherical-Gaussian mixtures as they are important on
their own and form a natural ﬁrst step towards learning general Gaussian mixtures.

1.2 Sample complexity

Reducing the number of samples required for learning is of great practical signiﬁcance. For example 
in topic modeling every sample is a whole document  in credit analysis every sample is a person’s
credit history  and in genetics  every sample is a human DNA. Hence samples can be very scarce
and obtaining them can be very costly. By contrast  current CPUs run at several Giga Hertz  hence
samples are typically much more scarce of a resource than time.
For one-dimensional distributions  the need for sample-efﬁcient algorithms has been broadly recog-
nized. The sample complexity of many problems is known quite accurately  often to within a con-

stant factor. For example  for discrete distributions over{1  . . .  s}  an approach was proposed in [23]
and its modiﬁcations were used in [28] to estimate the probability multiset using Θ(s~ log s) sam-
ples. Learning one-dimensional m-modal distributions over{1  . . .  s} requires Θ(m log(s~m)~3)
tone hazard rate  and unimodal) over{1  . . .  s} can be learned withO(k~4) O(k log(s~)~4)  and
O(k log(s)~4) samples  respectively  and these bounds are tight up to a factor of  [6].
example  to learn a mixture of k= 2 spherical Gaussians  existing estimators useO(d12) samples 

Unlike the 1-dimensional case  in high dimensions  sample complexity bounds are quite weak. For

samples [11]. Similarly  one-dimensional mixtures of k structured distributions (log-concave  mono-

and this number increases exponentially with k [16]. We close this gap by constructing estimators
with near-linear sample complexity.

1.3 Previous and new results

Our main contribution is PAC learning d-dimensional spherical Gaussian mixtures with near-linear
samples. In the process of deriving these results we also prove results for learning one-dimensional
Gaussians and for ﬁnding which distribution in a class is closest to the one generating samples.
d-dimensional Gaussian mixtures
Several papers considered PAC learning of discrete- and Gaussian-product mixtures. [17] considered
mixtures of two d-dimensional Bernoulli products where all probabilities are bounded away from 0.

They showed that this class is PAC learnable in ̃O(d2~4) time and samples  where the ̃O notation
mixtures are PAC learnable in ̃O(d~)2k2(k+1) time. Although they did not explicitly mention
sample complexity  their algorithm uses ̃O(d~)4(k+1) samples. [16] generalized these results
means is bounded by B times the standard deviation  are PAC learnable in ̃O(dB~)2k2(k+1) time 
and can be shown to use ̃O(dB~)4(k+1) samples. These algorithms consider the KL divergence

hides logarithmic factors.
[15] eliminated the probability constraints and generalized the results
from binary to arbitrary discrete alphabets and from 2 to k mixture components  showing that these

to Gaussian products and showed that mixtures of k Gaussians  where the difference between the

between the distribution and its estimate  but it can be shown that the (cid:96)1 distance would result in
similar complexities. It can also be shown that these algorithms or their simple modiﬁcations have
similar time and sample complexities for spherical Gaussians as well.
Our main contribution for this problem is to provide an algorithm that PAC learns mixtures of
spherical-Gaussians in (cid:96)1 distance with number of samples nearly-linear  and running time polyno-

2

mial in the dimension d. Speciﬁcally  in Theorem 11 we show that mixtures of k spherical-Gaussian
distributions can be learned using

n=O dk9
On2d log n+ d k7

4

log2 d
δ

=Ok d log2 d

2= ̃Ok (d3).
 k2

δ

samples and in time

δ

3

3 log2 d

log2 d
δ

Conversely  Theorem 2 shows that any algorithm for PAC learning a mixture of k spherical Gaus-

In that sense  for small k  the time complexity we derive is comparable to the best such algo-
rithms one can hope for. Additionally  the exponential dependence on k in the time complexity

)k2~2  signiﬁcantly lower than the dO(k3) dependence in previous results.

In addition  their time complexity signiﬁcantly improves on previously known ones.
One-dimensional Gaussian mixtures
To prove the above results we derive two simpler results that are interesting on their own. We

Recall that for similar problems  previous algorithms used ̃O(d~)4(k+1) samples. Furthermore 
recent algorithms typically construct the covariance matrix [29  16]  hence require≥ nd2 time.
is d( k7
sians requires Ω(dk~2) samples  hence our algorithms are nearly sample optimal in the dimension.
construct a simple estimator that learns mixtures of k one-dimensional Gaussians using ̃O(k−2)
samples and in time ̃O((k~)3k+1). We note that independently and concurrently with this work [12]
showed that mixtures of two one-dimensional Gaussians can be learnt with ̃O(−2) samples and in
timeO(−5). Combining with some of the techniques in this paper  they extend their algorithm to
mixtures of k Gaussians  and reduce the exponent to 3k− 1.
Let d(f  F) be the smallest (cid:96)1 distance between a distribution f and any distribution in a collection
F. The popular SCHEFFE estimator [13] takes a surprisingly smallO(logF) independent samples
from an unknown distribution f and timeO(F2) to ﬁnd a distribution inF whose distance from f
is at most a constant factor larger than d(f  F). In Lemma 1  we reduce the time complexity of the
Scheffe algorithm fromO(F2) to ̃O(F)  helping us reduce the running time of our algorithms.

A detailed analysis of several such estimators are provided in [1] and here we outline a proof for one
particular estimator for completeness.

1.4 The approach and technical contributions

Given the above  our goal is to construct a small class of distributions such that one of them is -close
to the underlying distribution.
Consider for example mixtures of k components in one dimension with means and variances
bounded by B. Take the collection of all mixtures derived by quantizing the means and variances of
all components to m accuracy  and quantizing the weights to w accuracy. It can be shown that if

m  w≤ ~k2 then one of these candidate mixtures would beO()-close to any mixture  and hence
to the underlying one. There are at most(B~m)2k⋅(1~w)k=(B~)̃O(k) candidates and running

SCHEFFE on these mixtures would lead to an estimate. However  this approach requires a bound on
the means and variances. We remove this requirement on the bound  by selecting the quantizations
based on samples and we describe it in Section 3.
In d dimensions  consider spherical Gaussians with the same variance and means bounded by B.
Again  take the collection of all distributions derived by quantizing the means of all components
in all coordinates to m accuracy  and quantizing the weights to w accuracy. It can be shown that
for d-dimensional Gaussian to get distance  from the underlying distribution  it sufﬁces to take

m  w≤ 2~poly(dk). There are at most(B~m)dk⋅(1~w)k= 2
̃O(dk) possible combinations of
complexity ̃O(dk). To reduce the dependence on d  one can approximate the span of the k mean
collection of size 2O(k2)  with SCHEFFE sample complexity of justO(k2). [15  16] constructs the

vectors. This reduces the problem from d to k dimensions  allowing us to consider a distribution

the k mean vectors and weights. Hence SCHEFFE implies an exponential-time algorithm with sample

sample correlation matrix and uses k of its columns to approximate the span of mean vectors. This

3

approach requires the k columns of the sample correlation matrix to be very close to the actual
correlation matrix  requiring a lot more samples.
We derive a spectral algorithm that approximates the span of the k mean vectors using the top k
eigenvectors of the sample covariance matrix. Since we use the entire covariance matrix instead of
just k columns  a weaker concentration sufﬁces and the sample complexity can be reduced.
Using recent tools from non-asymptotic random matrix theory [30]  we show that the span of the

means can be approximated with ̃O(d) samples. This result allows us to address most “reasonable”

distributions  but still there are some “corner cases” that need to be analyzed separately. To address
them  we modify some known clustering algorithms such as single-linkage  and spectral projections.
While the basic algorithms were known before  our contribution here  which takes a fair bit of effort
and space  is to show that judicious modiﬁcations of the algorithms and rigorous statistical analysis
yield polynomial time algorithms with near-linear sample complexity. We provide a simple and

practical spectral algorithm that estimates all such mixtures inOk (d log2 d) samples.

The paper is organized as follows. In Section 2  we introduce notations  describe results on the
Scheffe estimator  and state a lower bound. In Sections 3 and 4  we present the algorithms for one-
dimensional and d-dimensional Gaussian mixtures respectively. Due to space constraints  most of
the technical details and proofs are given in the appendix.

2 Preliminaries

2.1 Notation

the (cid:96)1 distance between two distributions.

2.2 Selection from a pool of distributions

tures and then perform Maximum Likelihood test using the samples to output a distribution [15  17].
Our algorithm also obtains a set of distributions containing at least one that is close to the underlying

For arbitrary product distributions p1  . . .   pk over a d dimensional space let pj i be the distribution
of pj over coordinate i  and let µj i and σj i be the mean and variance of pj i respectively. Let

f=(w1  . . .   wk  p1  . . .   pk) be the mixture of these distributions with mixing weights w1  . . .   wk.
We denote estimates of a quantity x by ˆx. It can be empirical mean or a more complex estimate.⋅
denotes the spectral norm of a matrix and⋅2 is the (cid:96)2 norm of a vector. We use D(⋅ ⋅) to denote
Many algorithms for learning mixtures over the domainX ﬁrst obtain a small collectionF of mix-
in (cid:96)1 distance. The estimation problem now reduces to the following. Given a classF of distribu-
tions and samples from an unknown distribution f  ﬁnd a distribution inF that is close to f. Let
D(f  F) def= minfi∈F D(f   fi).
The well-known Scheffe’s method [13] usesO(−2 logF) samples from the underlying distribution
f  and in timeO(−2F2T logF) outputs a distribution inF with (cid:96)1 distance of at most 9.1⋅
max(D(f  F)  ) from f  where T is the time required to compute the probability of an x∈X by
a distribution inF. A naive application of this algorithm requires time quadratic in the number of
distributions inF. We propose a variant of this  that works in near linear time. More precisely 
 independent samples
Lemma 1 (Appendix B). Let > 0. For some constant c  given c
from a distribution f  with probability≥ 1−δ  the output ˆf of MODIFIED SCHEFFE satisﬁes D(ˆf   f)≤
1000⋅ max(D(f  F)  ). Furthermore  the algorithm runs in timeOFT log(F~δ)
component mixtures in d-dimensions  T=O(dk) andF= ̃Ok (d2).
Using Fano’s inequality  we show an information theoretic lower bound of Ω(dk~2) samples to

Several such estimators have been proposed in the past [11  12]. A detailed analysis of the estimator
presented here was studied in [1]. We outline a proof in Appendix B for completeness. Note that
the constant 1000 in the above lemma has not been optimized. For our problem of estimating k

2.3 Lower bound

2 logF

δ

2

.

learn k-component d-dimensional spherical Gaussian mixtures for any algorithm. More precisely 

4

Gaussian mixtures to (cid:96)1 distance  with probability≥ 1~2 requires Ω(dk~2) samples.

Theorem 2 (Appendix C). Any algorithm that learns all k-component d-dimensional spherical

3 Mixtures in one dimension

def= N(µi  σ2



2n

.

2n

The above lemma states that given samples from a Gaussian distribution  there would be a sample
close to the mean and there would be two samples that are about a standard deviation apart. Hence 

Over the past decade estimation of one dimensional distributions has gained signiﬁcant atten-
tion [24  28  11  6  12  7]. We provide a simple estimator for learning one dimensional Gaussian
mixtures using the MODIFIED SCHEFFE estimator. Formally  given samples from f  a mixture of
Gaussian distributions pi

means or the variances of the components. While we do not use the one dimensional algorithm in
the d-dimensional setting  it provides insight to the usage of the MODIFIED SCHEFFE estimator and
may be of independent interest. As stated in Section 1.4  our quantizations are based on samples and
is an immediate consequence of the following observation for samples from a Gaussian distribution.

i) with weights w1  w2  . . . wk  our goal is to ﬁnd a mixture
ˆf =( ˆw1  ˆw2  . . . ˆwk  ˆp1  ˆp2  . . . ˆpk) such that D(f  ˆf)≤ . We make no assumption on the weights 
Lemma 3 (Appendix D.1). Given n independent samples x1  . . .   xn from N(µ  σ2)  with probabil-
ity≥ 1− δ there are two samples xj  xk such thatxj− µ≤ σ 7 log 2~δ
andxj− xk− σ≤ 2σ 7 log 2~δ
if we consider the set of all Gaussians N(xj (xj− xk)2)∶ 1≤ j  k≤ n  then that set would contain
Lemma 4 (Appendix D.2). Given n≥ 120k log(4k~δ)
S={N(xj (xj− xk)2)∶ 1≤ j  k≤ n} and W={0  
F def= {( ˆw1  ˆw2  . . .   ˆwk  ˆp1  ˆp2  . . . ˆpk)∶ ˆpi∈ S ∀1≤ i≤ k−1  ˆwi∈ W  ˆwk= 1−( ˆw1+. . . ˆwk−1)≥ 0}
be a set of n2k(2k~)k−1≤ n3k−1 candidate distributions. There exists ˆf∈F such that D(f  ˆf)≤ .
Running the MODIFIED SCHEFFE algorithm on the above set of candidatesF yields a mixture that
Corollary 5 (Appendix D.3). Let n≥ c⋅ k
   and returns a mixture ˆf such that D(f  ˆf)≤ 1000
runs in timeO k log(k~δ)
k2 log(k~δ)
with probability≥ 1− 2δ.

a Gaussian close to the underlying one. The same holds for mixtures and for a Gaussian mixture
and we can create the set of candidate mixtures as follows.

2k . . .   1} be a set of weights. Let

samples from a mixture f of k Gaussians. Let
2k   2

is close to the underlying one. By Lemma 1 and the above lemma we obtain

2 log k

δ for some constant c. There is an algorithm that

3k−1



2

[12] considered the one dimensional Gaussian mixture problem for two component mixtures. While
the process of identifying the candidate means is same for both the papers  the process of identifying
the variances and proof techniques are different.

4 Mixtures in d dimensions

Algorithm LEARN k-SPHERE learns mixtures of k spherical Gaussians using near-linear samples.
For clarity and simplicity of proofs  we ﬁrst prove the result when all components have the same

variance σ2  i.e.  pi= N(µi  σ2Id) for 1≤ i≤ k. A modiﬁcation of this algorithm works for com-

ponents with different variances. The core ideas are same and we discuss the changes in Section 4.3.
The algorithm starts out by estimating σ2 and we discuss this step later. We estimate the means in
three steps  a coarse single-linkage clustering  recursive spectral clustering and search over span of
means. We now discuss the necessity of these steps.

4.1 Estimating the span of means

A simple modiﬁcation of the one dimensional algorithm can be used to learn mixtures in d di-
mensions  however  the number of candidate mixtures would be exponential in d  the number of
dimensions. As stated in Section 1.4  given the span of the mean vectors µi  we can grid the k
dimensional span to the required accuracy g and use MODIFIED SCHEFFE  to obtain a polynomial

5

time algorithm. One of the natural and well-used methods to estimate the span of mean vectors is
using the correlation matrix [29]. Consider the correlation-type matrix 

X(i)X(i)t− σ2Id.

n

S= 1
nQ
i=1
E[S]= kQ
j=1
j=1 wjµjµj

t.

wjµjµj

t  and the expected fraction

of samples from pj is wj. Hence

While the above intuition is well understood  the number of samples necessary for convergence

For a sample X from a particular component j  E[XXt]= σ2Id+ µjµj
Therefore  as n→∞  S converges to∑k
is not well studied. We wish ̃O(d) samples to be sufﬁcient for the convergence irrespective of the
Example 6. Consider the special case  d= 1  k= 2  σ2= 1  w1= w2= 1~2  and mean differences
µ1− µ2= Lâ 1. Given this prior information  one can estimate the average of the mixture  that
yields(µ1+ µ2)~2. Solving equations obtained by µ1+ µ2 and µ1− µ2= L yields µ1 and µ2. The
variance of the mixture is 1+ L2~4> L2~4. With additional Chernoff type bounds  one can show
Hence  estimating the means to high precision requires n≥ L2  i.e.  the higher separation  the more

values of the means. However this is not true when the means are far apart. In the following example
we demonstrate that the convergence of averages can depend on their separation.

µ1+ µ2− ˆµ1− ˆµ2≈ ΘL~√
n .

that given n samples the error in estimating the average is

t  and its top k eigenvectors span the means.

samples are necessary if we use the sample mean.

√

algorithm divides the cluster into two nonempty clusters without any mis-clustering. For technical
reasons similar to the above example  we ﬁrst use a coarse clustering algorithm that ensures that the

A similar phenomenon happens in the convergence of the correlation matrices  where the variances
of quantities of interest increase with separation. In other words  for the span to be accurate the
number of samples necessary increases with the separation. To overcome this  a natural idea is to
cluster the Gaussians such that the component means in the same cluster are close and then estimate
the span of means  and apply SCHEFFE on the span within each cluster.
For clustering  we use another spectral algorithm. Even though spectral clustering algorithms are
studied in [29  2]  they assume that the weights are strictly bounded away from 0  which does
not hold here. We use a simple recursive clustering algorithm that takes a cluster C with average

µ(C). If there is a component in the cluster such that
wiµi− µ(C)2 is Ω(log(n~δ)σ)  then the
mean separation of any two components within each cluster is ̃O(d1~4σ).
Our algorithm thus comprises of(i) variance estimation(ii) a coarse clustering ensuring that means
are within ̃O(d1~4σ) of each other in each cluster(iii) a recursive spectral clustering that reduces
the mean separation toO(
k3 log(n~δ)σ)(iv) estimating the span of mean within each cluster 
and(v) quantizing the means and running MODIFIED SCHFEE on the resulting candidate mixtures.
To simplify the bounds and expressions  we assume that d> 1000 and δ ≥ min(2n2e−d~10  1~3).
For smaller values of δ  we run the algorithm with error 1~3 and repeat itO(log 1
) times to choose
a set of candidate mixturesFδ. By the Chernoff-bound with error≤ δ Fδ contains a mixture -close
to f. Finally  we run MODIFIED SCHEFFE onFδ to obtain a mixture that is close to f. By the union
bound and Lemma 1  the error of the new algorithm is≤ 2δ.
Variance estimation: Let ˆσ be the variance estimate from step 1. If X(1) and X(2) are two samples
from the components i and j respectively  then X(1)−X(2) is distributed N(µi−µj  2σ2Id). Hence
for large d X(1)− X(2)2
given k+ 1 samples  two of them are from the same component. Therefore  the minimum pairwise

2 concentrates around 2dσ2+µi− µj2

We now describe the steps stating the performance of each step of Algorithm LEARN k-SPHERE.

. By the pigeon-hole principle 

4.2 Sketch of correctness

2

δ

6

distance between k+ 1 samples is close to 2dσ2. This is made precise in the next lemma which
Lemma 7 (Appendix E.1). Given n samples from the k-component mixture  with probability 1− 2δ 
ˆσ2− σ2≤ 2.5σ2

states that ˆσ2 is a good estimate of the variance.

log(n2~δ)~d.



δ



Algorithm LEARN k-SPHERE

δ

2. Coarse single-linkage clustering: Start with each sample as a cluster 

from each component will be in the same cluster and the maximum distance between two components

)1~4σ). More precisely 
1~4.

Coarse single-linkage clustering: The second step is a single-linkage routine that clusters mixture
components with far means. Single-linkage is a simple clustering scheme that starts out with each
data point as a cluster  and at each step merges the two nearest clusters to form a larger cluster. The
algorithm stops when the distance between clusters is larger than a pre-speciﬁed threshold.
Suppose the samples are generated by a one-dimensional mixture of k components that are far 
then with high probability  when the algorithm generates k clusters all the samples within a cluster

all the n samples concentrate around their respective means and the separation between any two
samples from different components would be larger than the largest separation between any two
samples from the same component. Hence for a suitable value of threshold  single-linkage correctly
identiﬁes the clusters. For d-dimensional Gaussian mixtures a similar property holds  with minimum

are generated by a single component. More precisely  if∀i  j ∈[k] µi− µj= Ω(σ log n)  then
separation Ω((d log n
Lemma 8 (Appendix E.2). After Step 2 of LEARN k-SPHERE  with probability≥ 1−2δ  all samples
within each cluster is≤ 10kσd log n2
Input: n samples x(1)  x(2)  . . .   x(n) from f and .
1. Sample variance: ˆσ2= mina≠b∶a b∈[k+1]x(a)− x(b)2
2~2d.
d log(n2~δ)  merge them.
• While∃ two clusters with squared-distance≤ 2dˆσ2+ 23ˆσ2
3. Recursive spectral-clustering: While there is a cluster C withC ≥ n~5k and spectral
norm of its sample covariance matrix≥ 12k2 ˆσ2 log n3~δ 
• Use n~8k2 of the samples to ﬁnd the largest eigenvector and discard these samples.
is> 3ˆσ
log(n2k~δ) creating new clusters.
4. Exhaustive search: Let g = ~(16k3~2)  L = 200
G = {−L  . . .  −g  0  g  2g  . . . L}. Let W = {0  ~(4k)  2~(4k)  . . . 1} and Σ def= {σ2 ∶
√
128dk2) ∀− L′< i≤ L′}.
σ2= ˆσ2(1+ i~d
• For each cluster C ﬁnd its top k− 1 eigenvectors u1  . . . uk−1. Let Span(C)={ ˆµ(C)+
i=1 gi ˆσui∶ gi∈ G}.
∑k−1
• Let Span=∪C∶C≥ n
Span(C).
i∈ W   σ′2∈ Σ  ˆµi∈ Span 
• For all w′
k−1  1−∑k−1
add{(w′
1  . . .   w′
i=1 w′
5. Run MODIFIED SCHEFFE onF and output the resulting distribution.
ponents with mean separationO(σd1~4 log n
S(C) def= 1CQ
x∈C

i  N( ˆµ1  σ′2)  . . .   N( ˆµk  σ′2)} toF.
). We now recursively zoom into the clusters formed
(x− ˆµ(C))(x− ˆµ(C))t− ˆσ2Id 

and show that it is possible to cluster the components with much smaller mean separation. Note that
since the matrix is symmetric  the largest magnitude of the eigenvalue is the same as the spectral
norm. We ﬁrst ﬁnd the largest eigenvector of

• Project the remaining samples on the largest eigenvector.
• Perform single-linkage in the projected space (as before) till the distance between clusters

Recursive spectral-clustering: The clusters formed at the beginning of this step consist of com-

k4−1 log n2

δ   L′ = 32k



log n2~δ



  and





5k

δ

7





C 
remain in a single cluster.

δ . After recursive clustering  with probability

which is the sample covariance matrix with its diagonal term reduced by ˆσ2. We then project our
samples to this vector and if there are two components with means far apart  then using single-
linkage we divide the cluster into two. The following lemma shows that this step performs accurate
clustering of components with well separated means.
log n3

Lemma 9 (Appendix E.3). Let n ≥ c⋅ dk4
≥ 1− 4δ  the samples are divided into clusters such that for each component i within a cluster
√
wiµi− µ(C)2 ≤ 25σ
k3 log(n3~δ) . Furthermore  all the samples from one component

√
wiµi− µ(C)2≤ 25σ
mate of the span of µi− µ(C) within each cluster. More precisely 
Lemma 10 (Appendix E.4). Let n≥ c⋅ dk9
≥ 1− 7δ  ifC≥ n~5k  then the projection of[µi− µ(C)]~µi− µ(C)2 on the space orthogonal
to the span of top k− 1 eigenvectors has magnitude≤

Exhaustive search and Scheffe: After step 3  all clusters have a small weighted radius
δ . It can be shown that the eigenvectors give an accurate esti-

δ for some constant c. After step 3  with probability

4 log2 d

k3 log n3

√

√

wiµi−µ(C) 2

σ

.

8

2k

We now have accurate estimates of the spans of the cluster means and each cluster has components
with close means. It is now possible to grid the set of possibilities in each cluster to obtain a set of
distributions such that one of them is close to the underlying. There is a trade-off between a dense
grid to obtain a good estimation and the computation time required. The ﬁnal step takes the sparsest

grid possible to ensure an error≤ . This is quantized below.
Theorem 11 (Appendix E.5). Let n≥ c⋅ dk9
SPHERE  with probability≥ 1− 9δ  outputs a distribution ˆf such that D(ˆf   f)≤ 1000. Furthermore 
 k2
the algorithm runs in timeOn2d log n+ d k7
2.

δ for some constant c. Then Algorithm LEARN k-

4 log2 d

3 log2 d

δ

Note that the run time is calculated based on an efﬁcient implementation of single-linkage clustering
and the exponential term is not optimized.

i

4.3 Mixtures with unequal variances

gorithm for learning mixtures with unequal variances are:

1. In LEARN k-SPHERE  we ﬁrst estimated the component variance σ and divided the samples

We generalize the results to mixtures with components having different variances. Let pi =
N(µi  σ2
Id) be the ith component. The key differences between LEARN k-SPHERE and the al-
into clusters such that within each cluster the means are separated by ̃O(d1~4σ). We modify
only have mean separationO(d1~4σ)  but variances are also a factor at most 1+̃O1~√
d apart.
2. Once the variances in each cluster are within a multiplicative factor of 1+ ̃O1~√
d of each
lows  though instead of having a single σ′ for all clusters  we can have a different σ′ for each

other  it can be shown that the performance of the recursive spectral clustering step does not
change more than constants.

3. After obtaining clusters with similar means and variances  the exhaustive search algorithm fol-

this step such that the samples are clustered such that within each cluster the components not

cluster  which is estimated using the average pair wise distance between samples in the cluster.

The changes in the recursive clustering step and the exhaustive search step are easy to see and we
omit them. The coarse clustering step requires additional tools and we describe them in Appendix F.

5 Acknowledgements

We thank Sanjoy Dasgupta  Todd Kemp  and Krishnamurthy Vishwanathan for helpful discussions.

8

References
[1] J. Acharya  A. Jafarpour  A. Orlitksy  and A. T. Suresh. Sorting with adversarial comparators and appli-

cation to density estimation. In ISIT  2014.

[2] D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In COLT  2005.
[3] J. Anderson  M. Belkin  N. Goyal  L. Rademacher  and J. R. Voss. The more  the merrier: the blessing of

dimensionality for learning large gaussian mixtures. In COLT  2014.

[4] M. Azizyan  A. Singh  and L. A. Wasserman. Minimax theory for high-dimensional gaussian mixtures

with sparse mean separation. In NIPS  2013.

[5] M. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS  2010.
[6] S. O. Chan  I. Diakonikolas  R. A. Servedio  and X. Sun. Learning mixtures of structured distributions

over discrete domains. In SODA  2013.

[7] S. O. Chan  I. Diakonikolas  R. A. Servedio  and X. Sun. Efﬁcient density estimation via piecewise

polynomial approximation. In STOC  2014.

[8] K. Chaudhuri  S. Dasgupta  and A. Vattani. Learning mixtures of gaussians using the k-means algorithm.

CoRR  abs/0912.0086  2009.

[9] S. Dasgupta. Learning mixtures of gaussians. In FOCS  1999.
[10] S. Dasgupta and L. J. Schulman. A two-round variant of EM for gaussian mixtures. In UAI  2000.
[11] C. Daskalakis  I. Diakonikolas  and R. A. Servedio. Learning k-modal distributions via testing. In SODA 

2012.

[12] C. Daskalakis and G. Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of

gaussians. In COLT  2014.

[13] L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer  2001.
[14] I. S. Dhillon  Y. Guan  and J. Kogan. Iterative clustering of high dimensional text data augmented by local

search. In ICDM  2002.

[15] J. Feldman  R. O’Donnell  and R. A. Servedio. Learning mixtures of product distributions over discrete

domains. In FOCS  2005.

[16] J. Feldman  R. A. Servedio  and R. O’Donnell. PAC learning axis-aligned mixtures of gaussians with no

separation assumption. In COLT  2006.

[17] Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In COLT  1999.
[18] D. Hsu and S. M. Kakade. Learning mixtures of spherical gaussians: moment methods and spectral

decompositions. In ITCS  2013.

[19] A. T. Kalai  A. Moitra  and G. Valiant. Efﬁciently learning mixtures of two gaussians. In STOC  2010.
[20] M. J. Kearns  Y. Mansour  D. Ron  R. Rubinfeld  R. E. Schapire  and L. Sellie. On the learnability of

discrete distributions. In STOC  1994.

[21] J. Ma  L. Xu  and M. I. Jordan. Asymptotic convergence rate of the em algorithm for gaussian mixtures.

Neural Computation  12(12)  2001.

[22] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In FOCS  2010.
[23] A. Orlitsky  N. P. Santhanam  K. Viswanathan  and J. Zhang. On modeling proﬁles instead of values. In

UAI  2004.

[24] L. Paninski. Variational minimax estimation of discrete distributions under kl loss. In NIPS  2004.
[25] R. A. Redner and H. F. Walker. Mixture densities  maximum likelihood and the em algorithm. SIAM

Review  26(2)  1984.

[26] D. A. Reynolds and R. C. Rose. Robust text-independent speaker identiﬁcation using gaussian mixture

speaker models. IEEE Transactions on Speech and Audio Processing  3(1):72–83  1995.

[27] D. M. Titterington  A. F. Smith  and U. E. Makov. Statistical analysis of ﬁnite mixture distributions. Wiley

New York  1985.

[28] G. Valiant and P. Valiant. Estimating the unseen: an n/log(n)-sample estimator for entropy and support

size  shown optimal via new clts. In STOC  2011.

[29] S. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In FOCS  2002.
[30] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. CoRR  abs/1011.3027 

2010.

[31] E. P. Xing  M. I. Jordan  and R. M. Karp. Feature selection for high-dimensional genomic microarray

data. In ICML  2001.

[32] B. Yu. Assouad  Fano  and Le Cam. In Festschrift for Lucien Le Cam. Springer New York  1997.

9

,Ananda Theertha Suresh
Alon Orlitsky
Jayadev Acharya
Ashkan Jafarpour
Sebastien Bubeck