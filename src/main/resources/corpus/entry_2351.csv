2012,Gradient-based kernel method for feature extraction and variable selection,We propose a novel kernel approach to dimension reduction for supervised learning: feature extraction and variable selection; the former constructs a small number of features from predictors  and the latter finds a subset of predictors. First  a method of linear feature extraction is proposed using the gradient of regression function  based on the recent development of the kernel method.  In comparison with other existing methods  the proposed one has wide applicability without strong assumptions on the regressor or type of variables  and uses computationally simple eigendecomposition  thus applicable to large data sets.  Second  in combination of a sparse penalty  the method is extended to variable selection  following the approach by Chen et al. (2010).  Experimental results show that the proposed methods successfully find effective features and variables without parametric models.,Gradient-based kernel method for feature extraction

and variable selection

Kenji Fukumizu

Chenlei Leng

The Institute of Statistical Mathematics

National University of Singapore

10-3 Midori-cho  Tachikawa  Tokyo 190-8562 Japan

6 Science Drive 2  Singapore  117546

fukumizu@ism.ac.jp

stalc@nus.edu.sg

Abstract

We propose a novel kernel approach to dimension reduction for supervised learn-
ing: feature extraction and variable selection; the former constructs a small num-
ber of features from predictors  and the latter ﬁnds a subset of predictors. First 
a method of linear feature extraction is proposed using the gradient of regression
function  based on the recent development of the kernel method.
In compari-
son with other existing methods  the proposed one has wide applicability without
strong assumptions on the regressor or type of variables  and uses computationally
simple eigendecomposition  thus applicable to large data sets. Second  in combi-
nation of a sparse penalty  the method is extended to variable selection  following
the approach by Chen et al. [2]. Experimental results show that the proposed meth-
ods successfully ﬁnd effective features and variables without parametric models.

1

Introduction

Dimension reduction is involved in most of modern data analysis  in which high dimensional data
must be handled. There are two categories of dimension reduction: feature extraction  in which a
linear or nonlinear mapping to a low-dimensional space is pursued  and variable selection  in which
a subset of variables is selected. This paper discusses both the methods in supervised learning.
Let (X  Y ) be a random vector such that X = (X 1  . . .   X m) ∈ Rm. The domain of Y can be
arbitrary  either continuous  discrete  or structured. The goal of dimension reduction in supervised
setting is to ﬁnd such features or a subset of variables X that explain Y as effectively as possible.
This paper focuses linear dimension reduction  in which linear combinations of the components of
X are used to make effective features. Although there are many methods for extracting nonlinear
features  this paper conﬁnes its attentions on linear features  since linear methods are more stable
than nonlinear feature extraction  which depends strongly on the choice of the nonlinearity  and after
establishing a linear method  extension to a nonlinear one would not be difﬁcult.

We ﬁrst develop a method for linear feature extraction with kernels  and extend it to variable selec-
tion with a sparseness penalty. The most signiﬁcant point of the proposed methods is that we do
not assume any parametric models on the conditional probability  or make strong assumptions on
the distribution of variables. This differs from many other methods  particularly for variable selec-
tion  where a speciﬁc parametric model is often assumed. Beyond the classical approaches such as
Fisher Discriminant Analysis and Canonical Correlation Analysis to linear dimension reduction  the
modern approach is based on the notion of conditional independence; we assume for the distribution
(1)
where B is a projection matrix (BT B = Id) onto a d-dimensional subspace (d < m) in Rm  and
wish to estimate B. For variable selection  we further assume that some rows of B may be zero.
The subspace spanned by the columns of B is called the effective direction for regression  or EDR
space [14]. Our goal is thus to estimate B without speciﬁc parametric models for p(y|x).

p(Y |X) = ˜p(Y |BT X)

Y ⊥⊥X | BT X 

or equivalently

1

First  consider the linear feature extraction based on Eq. (1). The ﬁrst method using this formula-
tion is the sliced inverse regression (SIR  [13])  which employs the fact that the inverse regression
E[X|Y ] lies in the EDR space under some assumptions. Many methods have been proposed in this
vein of inverse regression ([4  12] among others). While the methods are computationally simple 
they often need some strong assumptions on the distribution of X such as elliptic symmetry.
There are two most relevant works to this paper. The ﬁrst one is the dimension reduction with the
gradient of regressor E[Y |X = x] [11  17]. As explained in Sec. 2.1  under Eq. (1) the gradient
is contained in the EDR space. One can thus estimate the space by some standard nonparametric
method. There are some limitations in this approach  however: the nonparametric gradient esti-
mation in high-dimensional spaces is challenging  and the method may not work unless the noise
is additive. The second one is the kernel dimension reduction (KDR  [8  9  28])  which uses the
kernel method for characterizing the conditional independence to overcome various limitations of
existing methods. While KDR applies to a wide class of problems without any strong assumptions
on the distributions or types of X or Y   and shows high estimation accuracy for small data sets  its
optimization has a problem: the gradient descent method used for KDR may have local optima  and
needs many matrix inversions  which prohibits application to high-dimensional or large data.

We propose a kernel method for linear feature extraction using the gradient-based approach  but
unlike the existing ones [11  17]  the gradient is estimated based on the recent development of the
kernel method [9  19]. It solves the problems of existing methods: by virtue of the kernel method  Y
can be of arbitrary type  and the kernel estimator is stable without careful decrease of bandwidth. It
solves also the problem of KDR: the estimator by an eigenproblem needs no numerical optimization.
The method is thus applicable to large and high-dimensional data  as we demonstrate experimentally.

Second  by using the above feature extraction in conjunction with a sparseness penalty  we propose a
novel method for variable selection. Recently extensive studies have been done for variable selection
with a sparseness penalty such as LASSO [23] and SCAD [6]. It is also known that with appropriate
choice of regularization coefﬁcients they have oracle property [6  25  30]. These methods  however 
use some speciﬁc model for regression such as linear regression  which is a limitation of the methods.
Chen et al. [2] proposed a novel method for sparse variable selection based on the objective function
of linear feature extraction formulated as an eigenproblem such as SIR. We follow this approach to
derive our method for variable selection. Unlike the methods used in [2]  the proposed one does not
require strong assumptions on the regressor or distribution  and thus provides a variable selection
method based on the conditional independence irrespective of the regression model.

2 Gradient-based kernel dimension reduction

2.1 Gradient of a regression function and dimension reduction

We review the basic idea of the gradient-based method [11  17] for dimension reduction. Suppose
Y is an R-valued random variable. If the assumption of Eq. (1) holds  we have

∂

∂x E[Y |X = x] = ∂

∂xR yp(y|x)dy =R y ∂

∂x ˜p(y|BT x)dy = BR y ∂

∂z ˜p(y|z)(cid:12)(cid:12)z=BT x dy 

which implies that the gradient ∂
∂x E[Y |X = x] at any x is contained in the EDR space. Based on
this fact  the average derivative estimates (ADE  [17]) has been proposed to estimate B. In the more
recent method [11]  a standard local linear least squares with a smoothing kernel (not necessarily
positive deﬁnite  [5]) is used for estimating the gradient  and the dimensionality of the projection
is continuously reduced to the desired one in the iteration. Since the gradient estimation for high-
dimensional data is difﬁcult in general  the iterative reduction is expected to give more accurate
estimation. We call the method in [11] iterative average derivative estimates (IADE) in the sequel.

2.2 Kernel method for estimating gradient of regression

For a set Ω  a (R-valued) positive deﬁnite kernel k on Ω is a symmetric kernel k : Ω × Ω → R
such thatPn
i j=1 cicjk(xi  xj) ≥ 0 for any x1  . . .   xn in Ω and c1  . . .   cn ∈ R. It is known that
a positive deﬁnite kernel on Ω uniquely deﬁnes a Hilbert space H consisting of functions on Ω 
in which the reproducing property hf  k(·  x)iH = f (x) (∀f ∈ H) holds  where h· ·iH is the inner
product of H. The Hilbert space H is called the reproducing kernel Hilbert space (RKHS) associated
with k. We assume that an RKHS is always separable.

2

In deriving a kernel method based on the approach in Sec. 2.1  the fundamental tool is the repro-
ducing property for the derivative of a function. It is known (e.g.  [21] Sec. 4.3) that if a positive
deﬁnite kernel k(x  y) on an open set in the Euclidean space is continuously differentiable with re-
spect to x and y  every f in the corresponding RKHS H is continuously differentiable. If further
∂x k(·  x) ∈ H  we have

∂

(2)

∂f
∂x

=Df 

∂
∂x

k(·  x)EH

.

This reproducing property combined with the following kernel estimator of the conditional expec-
tation (see [8  9  19] for details) will provide a method for dimension reduction. Let (X  Y ) be a
random variable on X × Y with probability P . We always assume that the p.d.f. p(x  y) and the
conditional p.d.f. p(y|x) exist  and that a positive deﬁnite kernel is measurable and bounded. Let kX
and kY be positive deﬁnite kernels on X and Y  respectively  with respective RKHS HX and HY.
The (uncentered) covariance operator CY X : HX → HY is deﬁned by the equation
hg  CY X fiHY = E[f (X)g(Y )] = E(cid:2)hf  ΦX (X)iHX hΦY (Y )  giHY(cid:3)
(3)
for all f ∈ HX   g ∈ HY  where ΦX (x) = kX (·  x) and ΦY (y) = kY (·  y). Similarly  CXX
denotes the operator on HX that satisﬁes hf2  CXX f1i = E[f2(X)f1(X)] for any f1  f2 ∈ HX .
These deﬁnitions are straightforward extensions of the ordinary covariance matrices  if we con-
sider the covariance of the random vectors ΦX (X) and ΦY (Y ) on the RKHSs. One of the advan-
tages of the kernel method is that estimation with ﬁnite data is straightforward. Given i.i.d. sample
(X1  Y1)  . . .   (Xn  Yn) with law P   the covariance operator is estimated by

nPn

Y X f = 1

i=1kY (·  Yi)hkX (·  Xi)  fiHX

bC (n)
i=1kX (·  Xi)hkX (·  Xi)  fiHX . (4)
It is known [8] that if E[g(Y )|X = ·] ∈ HX holds for g ∈ HY  then we have CXX E[g(Y )|X =
·] = CXY g. If further CXX is injective1  this relation can be expressed as

bC (n)

XX f = 1

nPn

E[g(Y )|X = ·] = CXX−1CXY g.

(5)

While the assumption E[g(Y )|X = ·] ∈ HX may not hold in general  we can nonetheless obtain an
empirical estimator based on Eq. (5)  namely 

XX + εnI)−1bC (n)
(bC (n)

XY g 

where εn is a regularization coefﬁcient in Tikhonov-type regularization. Note that the above expres-
sion is the kernel ridge regression of g(Y ) on X. As we discuss in Supplements  we can in fact
prove rigorously that this estimator converges to E[g(Y )|X = ·].
Assume now that X = Rm  CXX is injective  kX (x  ˜x) is continuously differentiable  E[g(Y )|X =
x] ∈ HX for any g ∈ HY  and ∂
∂x kX (·  x) ∈ R(CXX )  where R denotes the range of the operator.
∂kX (· x)
From Eqs. (5) and (2)  ∂
i.
With g = kY (·  ˜y)  we obtain the gradient of regression of the feature vector ΦY (Y ) on X as

∂x E[g(Y )|X = x] = hC−1

i = hg  CY X C−1

XX CXY g  ∂kX (· x)

XX

∂x

∂x

∂
∂x

E[ΦY (Y )|X = x] = CY X C−1

XX

∂kX (·  x)

∂x

.

(6)

2.3 Gradient-based kernel method for linear feature extraction

It follows from the same argument as in Sec. 2.1 that ∂
∂x E[kY (·  y)|X = x] = Ξ(x)B with an
operator Ξ(x) from Rm to HY  where we use a slight abuse of notation by identifying the operator
Ξ(x) with a matrix. In combination with Eq. (6)  we have

BThΞ(x)  Ξ(x)iHY B =D ∂kX (·  x)

∂x

  C−1

XX CXY CY X C−1

XX

∂kX (·  x)

∂x

EHX

=: M (x) 

(7)

which shows that the eigenvectors for non-zero eigenvalues of m × m matrix M (x) are contained
in the EDR space. This fact is the basis of our method. In contrast to the conventional gradient-
based method described in Sec. 2.1  this method incorporates high (or inﬁnite) dimensional regressor
E[ΦY (Y )|X = x].

1Noting hCXX f  f i = E[f (X)2]  it is easy to see that CXX is injective  if kX is a continuous kernel on a

topological space X   and PX is a Borel probability measure such that P (U ) > 0 for any open set U in X .

3

∂x

∂x

(cid:11)

Given i.i.d. sample (X1  Y1)  . . .   (Xn  Yn) from the true distribution  based on the empirical covari-
ance operators Eq. (4) and regularized inversions  the matrix M (x) is estimated by

Y X(cid:0)bC (n)

XX + εnI(cid:1)−1 ∂kX (· x)

cMn(x) =(cid:10) ∂kX (· x)

XX + εnI(cid:1)−1bC (n)
 (cid:0)bC (n)

= ∇kX (x)T (GX + nεnI)−1GY (GX + nεnI)−1∇kX (x) 

XY bC (n)
(8)
where GX and GY are the Gram matrices (kX (Xi  Xj)) and (kY (Yi  Yj))  respectively  and
∇kX (x) = (∂kX (X1  x)/∂x ···   ∂kX (Xn  x)/∂x)T ∈ Rn.
As the eigenvectors of M (x) are contained in the EDR space for any x  we propose to use the
average of M (Xi) over all the data points Xi  and deﬁne
˜Mn := 1
i=1∇kX (Xi)T (GX + nεnIn)−1GY (GX + nεnIn)−1∇kX (Xi).
We call the dimension reduction with the matrix ˜Mn the gradient-based kernel dimension reduction
(gKDR). For linear feature extraction  the projection matrix B in Eq. (1) is then estimated simply
by the top d eigenvectors of ˜Mn. We call this method gKDR-FEX.
The proposed method applies to a wide class of problems; in contrast to many existing methods 
the gKDR-FEX can handle any type of data for Y including multinomial or structured variables 
and make no strong assumptions on the regressor or distribution of X. Additionally  since the
gKDR incorporates the high dimensional feature vector ΦY (Y )  it works for any regression relation
including multiplicative noise  for which many existing methods such as SIR and IADE fail.

i=1cMn(Xi) = 1

nPn

nPn

As in all kernel methods  the results of gKDR depend on the choice of kernels. We use the cross-
validation (CV) for choosing kernels and parameters  combined with some regression or classiﬁca-
tion method. In this paper  the k-nearest neighbor (kNN) regression / classiﬁcation is used in CV
for its simplicity: for each candidate of a kernel or parameter  we compute the CV error by the kNN
method with (BT Xi  Yi)  where B is given by gKDR  and choose the one that gives the least error.
The time complexity of the matrix inversions and the eigendecomposition for gKDR are O(n3) 
which is prohibitive for large data sets. We can apply  however  low-rank approximation of Gram
matrices  such as incomplete Cholesky decomposition. The space complexity may be also a problem
of gKDR  since (∇kX (Xi))n
In the case of Gaussian kernel  where
∂xa kX (Xj  x)|x=Xi = 1
i ) exp(−kXj − Xik2/(2σ2))  we have a way of reducing
σ2 (X a
the necessary memory by low rank approximation. Let GX ≈ RRT and GY ≈ HH T be the
low rank approximation with rx = rkR  ry = rkH (rx  ry < n  m). With the notation F :=
i = 1
(GX + nεnIn)−1H and Θas
j=1RjsFjt(cid:1).
˜Mn ab =Pn
iaΓt
With this method  the complexity is O(nmr) in space and O(nm2r) in time (r = max{rx  ry}) 
which is much more efﬁcient in memory than straightforward implementation.

i Ris  we have  for 1 ≤ a  b ≤ m 
j Fjt(cid:1) −Prx
ia =Prx

i=1 has n2 × m dimension.
j − X a

s=1Ris(cid:0)Pn

i=1Pry

i (cid:0)Pn

s=1Θas

j=1Θas

ib  Γt

σ2 X a

t=1Γt

∂

We introduce two variants of gKDR-FEX. First  since accurate nonparametric estimation with high-
dimensional X is not easy  we propose a method for decreasing the dimensionality iteratively. Using
gKDR-FEX  we ﬁrst ﬁnd a matrix B1 of dimensionality d1 larger than the target d  project data Xi
onto the subspace as Z (1)
1 Xi  ﬁnd the projection matrix B2 (d1 × d2 matrix) for Z (1)
onto a
d2 (d2 < d1) dimensional subspace  and repeat this process. We call this method gKDR-FEXi.
Second  if Y takes only L points as in classiﬁcation  the Gram matrix GY and thus ˜Mn are of rank L
at most (see Eq. (8))  which is a strong limitation of gKDR. Note that this problem is shared by many
linear dimension reduction methods including CCA and slice-based methods. To solve this problem 

i = BT

i

we propose to use the variation of cMn(x) over the points x = Xi instead of the average ˜Mn. By
partitioning {1  . . .   n} into T1  . . .   T`  the projection matrices bB[a] given by the eigenvectors of
cM[a] =Pi∈Ta cM (Xi) are used to deﬁne bP = 1
` P`
a=1 bB[a]bBT
by the top d eigenvectors of bP . We call this method gKDR-FEXv.

[a]. The estimator of B is then given

2.4 Theoretical analysis of gKDR

We have derived the gKDR method based on the necessary condition of EDR space. The following
theorem shows that it is also sufﬁcient  if kY is characteristic. A positive deﬁnite kernel k on a

4

gKDR
-FEX
0.1989
0.1264
0.1500
0.0755
0.1919
0.1346

gKDR
-FEXi
0.1639
0.0995
0.1358
0.0750
0.2322
0.1372

gKDR
-FEXv
0.2002
0.1287
0.1630
0.0802
0.1930
0.1369

IADE
0.1372
0.0857
0.1690
0.0940
0.7724
0.7863

SIR II
0.2986
0.2077
0.3137
0.2129
0.7326
0.7167

KDR
0.2807
0.1175
0.2138
0.1440
0.1479
0.0897

gKDR-FEX

+KDR
0.0883
0.0501
0.1076
0.0506
0.1285
0.0893

(A) n = 100
(A) n = 200
(B) n = 100
(B) n = 200
(C) n = 200
(C) n = 400

Table 1: gKDE-FEX for synthetic data: mean discrepancies over 100 runs.

measurable space is characteristic if EP [k(·  X)] = EQ[k(·  X)] means P = Q  i.e.  the mean of
feature vector uniquely determines a probability [9  20]. Examples include Gaussian kernel.
In the following theoretical results  we assume (i) ∂kX (·  x)/∂xa ∈ R(CXX ) (a = 1  . . .   m)  (ii)
E[kY (y  X)|X = ·] ∈ HX for any y ∈ Y  and (iii) E[g(Y )|BT X = z] is a differentiable function
of z for any g ∈ HY and the linear functional g 7→ ∂E[g(Y )|BT X = z]/∂z is continuous for any z.
In the sequel  the subspace spanned by the columns of B is denoted by Span(B)  and the Frobenius
norm of a matrix M by kMkF . The proofs are given in Supplements.
Theorem 1. In addition to the above assumptions (i)-(iii)  assume that the kernel kY is character-
istic. If the eigenspaces for the non-zero eigenvalues of E[M (X)] are included in Span(B)  then Y
and X are conditionally independent given BT X.

XX ) (a = 1  . . .   m) for some

We can obtain the rate of consistency for cMn(x) and ˜Mn.
Theorem 2. In addition to (i)-(iii)  assume that ∂kX (· x)
β ≥ 0  and E[kY (y  Y )|X = ·] ∈ HX for every y ∈ Y. Then  for εn = n− max{ 1
cMn(x) − M (x) = Op(cid:0)n− min{ 1
If further E[kM (X)k2

∈ R(C β+1
4β+4 }(cid:1)
XkHX < ∞  then ˜Mn → E[M (X)] in the same order as above.

for every x ∈ X as n → ∞.
Ekha
Note that  assuming that the eigenvalues of M (x) or E[M (X)] are all distinct  the convergence
of matrices implies the convergence of the eigenvectors [22]  thus the estimator of gKDR-FEX is
consistent to the subspace given by the top eigenvectors of E[M (X)].

F ] < ∞ and ∂kX (· x)

2β+2 }  we have

= C β+1

XX ha

x with

3   2β+1

∂xa

1

3  

∂xa

1 +Z2)(Z1−Z 3

(1  2  0  . . .   0)T X  (B): Y = (Z 3

(1  1  0  . . .   0)T X  Z2 = 1√2

2.5 Experiments with gKDR-FEX
We always use the Gaussian kernel k(x  ˜x) = exp(− 1
2σ2kx− ˜xk2) in the kernel method below. First
we use three synthetic data to verify the basic performance of gKDR-FEX(i v). The data are gener-
ated by (A): Y = Z sin(√5Z)+W   Z = 1√5
2 )+W  
Z1 = 1√2
(1 −1  0  . . .   0)T X  where 10-dimensional X is generated
by the uniform distribution on [−1  1]10 and W is independent noise with N (0  10−2)  and (C):
Y = Z 4E  Z = (1  0  . . .   0)T X  where each component of 10-dimensional X is independently
generated by the truncated normal distribution N (0  1/4) ∗ I[−1 1] and E ∼ N (0  1) is a multi-
plicative noise. The discrepancy between the estimator B and the true projector B0 is measured by
0 (Im − BBT )kF /d. For choosing the parameter σ in Gaussian kernel and the regularization
kB0BT
parameter εn  the CV in Sec. 2.3 with kNN (k = 5  manually chosen to optimize the results) is
used with 8 different values given by cσmed (0.5 ≤ c ≤ 10)  where σmed is the median of pairwise
distances of data [10]  and ` = 4  5  6  7 for εn = 10−` (a similar strategy is used for the CV below).
We compare the results with those of IADE  SIR II [13]  and KDR. The IADE has seven parameters
[11]  and we tuned two of them (h1 and ρmin) manually to optimize the performance. For SIR II 
we tried several numbers of slices  and chose the one that gave the best result. From Table 1  we see
that gKDR-FEX(i v) show much better results than SIR II in all the cases. The IADE works better
than these methods for (A)  while for (B) and (C) it works worse. Since (C) has multiplicative noise 
the IADE does not obtain meaningful estimation. The KDR attains higher accuracy for (C)  but less
accurate for (A) and (B) with n = 100; this undesired result is caused by failure of optimization in

5

 

100

 

gKDR−v
KDR
All variables

85

80

75

)

%

(
 
e
t
a
r
 

n
o
i
t
a
c
i
f
i
s
s
a
l
C

)

%

(
 
e
t
a
r
 

n
o
i
t
a
c
i
f
i
s
s
a
l
C

95

90

85

80

75

95

90

85

)

%

(
 
e
t
a
r
 

n
o
i
t
a
c
i
f
i
s
s
a
l
C

 

gKDR−v
KDR
All variables

10

15

20
Dimensionality

30

(B) Breast-cancer-Wisconsin
(m:30  ntr:200  ntest:369)

70

 

3

5

7

9
Dimensionality

11

13

(H) Heart Disease

70

 

3 5

(m:13  ntr:129  ntest:148)

(m:34  ntr:151  ntest:200)

gKDR−v
KDR
All variables

10

15

20

Dimensionality
(I) Ionoshpere

34

80

 

3 5

Figure 1: Classiﬁcation accuracy with gKDR-v and KDR for binary classiﬁcation problems. m  ntr
and ntest are the dimension of X  training data size  and testing data size  respectively.

Dim.

10

gKDR + kNN
gKDR-v + kNN

13.53
13.15
22.77
CCA + kNN
SIR-II + kNN
77.42
gKDR + SVM 14.43
gKDR-v + SVM 16.87
13.09

CCA + SVM

20
4.55
4.55
6.74
70.11
5.00
4.75
6.54

30
–

4.81

–

63.44

40
–

5.26

–

52.66

50
–

5.58

–

50.61

3.85

–

–

3.59

–

–

3.08

–

–

gKDR
+SVM

12.0
16.2
18.0
21.8
19.5

Corr
+SVM
(500)
15.7
30.2
29.2
35.4
41.1

Corr
+SVM
(2000)

8.3
18.0
24.0
25.0
29.0

L

10
20
30
40
50

Table 2: Left: ISOLET - classiﬁcation errors for test data (percentage). Right: Amazon Reviews -
10-fold cross-validation errors (%) for classiﬁcation

some runs (see Supplements for error bars). We also used the results of gKDR-FEX as the initial
state for KDR  which improved the accuracy signiﬁcantly for (A) and (B). Note however that these
data sets are very small in size and dimension  and KDR is not applicable to large data used later.

One way of evaluating dimension reduction methods in supervised learning is to consider the classi-
ﬁcation or regression accuracy after projecting data onto the estimated subspaces. We next used three
data sets for binary classiﬁcation  heart-disease (H)  ionoshpere (I)  and breast-cancer-Wisconsin
(B)  from UCI repository [7]  and evaluated the classiﬁcation rates of gKDR-FEXv with kNN clas-
siﬁers (k = 7). We compared them with KDR  as KDR shows high accuracy for small data sets.
From Fig. 1  we see gKDR-FEXv shows competitive accuracy with KDR: slightly worse for (I)  and
slightly better for (B). The computation of gKDR-FEXv for these data sets can be much faster than
that of KDR. For each parameter set  the computational time of gKDR vs KDR was  in (H) 0.044
sec / 622 sec (d = 11)  in (I) 0.l03 sec / 84.77 sec (d = 20)  and in (B) 0.116 sec / 615 sec (d = 20).
The next two data sets taken from UCI repository are larger in the sample size and dimensionality 
for which the optimization of KDR is difﬁcult to apply. The ﬁrst one is ISOLET  which provides
617 dimensional continuous features of speech signals to classify 26 alphabets. In addition to 6238
training data  1559 test data are separately provided. We evaluate the classiﬁcation errors with the
kNN classiﬁer (k = 5) and 1-vs-1 SVM to see the effectiveness of the estimated subspaces (see
Table 2). From the information on the data at the UCI repository  the best performance with neural
networks and C4.5 with ECOC are 3.27% and 6.61%  respectively. In comparison with these results 
the low dimensional subspaces found by gKDR-FEX and gKDR-FEXv maintain the information for
classiﬁcation effectively. SIR-II does not ﬁnd meaningful features.

The second data set is author identiﬁcation of Amazon commerce reviews with 10000 dimensional
linguistic features. The total number of authors is 50  and 30 reviews were collected for each author;
the total size of data is thus 1500. We varied the used number of authors (L) to make different levels
of difﬁculty for the tasks. The reduced dimensionality by gKDR-FEX is set to the same as L  and the
10-fold CV errors with data projected on the estimated EDR space are evaluated using 1-vs-1 SVM.
`=1 Corr[X a  Y `]2  is also
used for choosing explanatory variables (a = 1  . . .   10000). Such variable selection methods with
Pearson correlation are popularly used for very high dimensional data. The variables with top 500
and 2000 correlations are used to make SVM classiﬁers. As we can see from Table 2  the gKDR-
FEX gives much more effective subspaces for regression than the Pearson correlation method  when

As comparison  the squared sum of variable-wise Pearson correlations PL

6

the number of authors is large. The creator of the data set has also reported the classiﬁcation result
with a neural network model [15]; for 50 authors  the 10-fold CV error with 2000 selected variables
is 19.51%  which is similar to the gKDR-FEX result with only 50 linear features.

3 Variable selection with gKDR

In recent years  extensive studies have been done on variable selection with a sparseness penalty
([6  16  18  23–27  29  30] among many others). In supervised setting  these studies often consider
some speciﬁc model for the regression such as least square or logistic regression. While consistency
and oracle property have been also established for many methods  the assumption that there is a true
parameter in the model may not hold in practice  and thus a strong restriction of the methods. It
is then important to consider more ﬂexible ways of variable selection without assuming any para-
metric model on the regression. The gKDR approach is appealing to this problem  since it realizes
conditional independence without strong assumptions for regression or distribution of variables.

Chen et al. [2] recently proposed the Coordinate-Independent Sparse (CIS) method  which is a semi-
parametric method for sparse variable selection. In CIS  the linear feature BT X is assumed with
some rows of B zero  but no parametric model is speciﬁed for regression. We wish to estimate B so
that the zero-rows should be estimated as zeros. This is achieved by imposing the sparseness penalty
of the group LASSO [29] in combination with an objective function of linear feature extraction
written in the form of eigenproblem such as SIR and PFC [3].

We follow the CIS method for our variable selection with gKDR; since the gKDR is given by the
eigenproblem with matrix ˜Mn  the CIS method is applied straightforwardly. The signiﬁcance of our
method is that the gKDR formulates the conditional independence of Y and X given BT X  while
the existing CIS-based methods in [2] realize only weaker conditions under strong assumptions.

3.1 Sparse variable selection with gKDR

01  . . .   vT

Throughout this section  it is assumed that the true probability satisﬁes Eq. (1) with B = B0 =
0m)T   and with some 1 ≤ q ≤ m the j-th row v0j is non-zero for j ≤ q and v0j = 0
(vT
m)T   where bi is the i-th
for j ≥ q + 1. The projection matrix is B = (b1  . . .   bd) = (vT
column and vj is the j-th row. The proposed variable selection method  gKDR-VS  estimates B by
B:BT B=Idh−Tr[BT ˜MnB] +
mXi=1
+ is the regularization coefﬁcients.
where kvjk is the Euclidean norm and λ = (λ1  . . .   λm) ∈ Rm
To optimize Eq. (9)  as in [2]  we used the local quadratic approximation [6]  which is simple and
fast. We used the matlab code provided at the homepage of X. Chen.

bBλ = arg min

λikviki 

1   . . .   vT

(9)

The choice of λ is crucial on the practical performance of sparse variable selection. As a theoretical
guarantee  we will show that some asymptotic condition provides model consistency. In practice  as
in the Adaptive Lasso [30]  it is suitable to consider λ = λ(θ) deﬁne by

λi = θk˜vik−r

where θ and r are positive numbers  and ˜vi is the row vector of ˜B0  the solution to gKDR without
penalty  i.e.  ˜B0 = arg minBT B=Id −Tr[BT ˜MnB]. We used r = 1/2 for all of our experiments.
To choose the parameter θ  a BIC-based method is often used in sparse variable selection [27  31]
with theoretical guarantee of model consistency. We use a BIC-type method for choosing θ by
minimizing

log n

n

 

(10)

BICθ = −Tr[bBT

λ(θ)

˜MnbBλ(θ)] + Cndfθ

eigenvalue of ˜Mn. The log log(m) factor is used in [27]  where increasing number of variables is

where dfθ = d(p − d) is the degree of freedom of bBλ(θ) with p the number of non-zero rows in
bBλ(θ)  and Cn is a positive number of Op(1). We used Cn = α1 log log(m) with α1 is the largest
discussed  and α1 is introduced to adjust the scale of Tr[bBT
˜MnbBλ]; we use CV for choosing the
hyperparameters (kernel and regularization coefﬁcient)  in which the values of Tr[bBT
˜MnbBλ] is not

normalized well for different choices.

λ

λ

7

gKDR
-VS

.94/.99/75
1.0/1.0/98
.92/.84/63
.98/.89/75

CIS
-SIR

.89/1.0/65
.99/1.0/97
.19/.85/1
.18/.85/1

(A) n = 60
(A) n = 120
(B) n = 100
(B) n = 200

Table 3: gKDR-VS and CIS-SIR
with synthetic data (ratio of non-
zeros in 1 ≤ j ≤ q / ratio of ze-
ros in q + 1 ≤ j ≤ m / number of
correct models among 100 runs).

Method
CRIM

ZN

INDUS
CHAS
NOX
RM
AGE
DIS
RAD
TAX

PTRATIO

B

LSTAT

gKDR-VS

0
0
0
0
0
0.896
0
-0.169
0.018
0
-0.376
0
-0.165

0
0
0
0
0
0.393
0
0.022
-0.000
0
0.919
0
0.017

CIS-SIR
0
0
-0.008
-0.000
0
0
0
0
0
0
-1.00
-1.253
-0.022
0.005
0
0
0
0
-0.001
0.001
0.003
0.049
-0.001
0.002
-0.114
0.043

CIS-PFC
0
0
0
0
0
0
0
0
0
0
1.045
-1.390
-0.011
-0.003
0
0
0
0
-0.005
-0.001
0.007
-0.038
0.001
0.005
-0.113
-0.043

Table 4: Boston Housing Data: estimated sparse EDR.

3.2 Theoretical results on gKDR-VS

1 − B2BT

2 k  where k · k is the operator norm.

This subsection shows the model consistency of the gKDR-VS. All the proofs are shown in Supple-
ments. Let αn = max{λj | 1 ≤ j ≤ q} and βn = min{λj | q + 1 ≤ j ≤ m}. The eigenvalues of
M = E[M (X)] are η1 ≥ . . . ≥ ηm ≥ 0. For two m × d matrices Bi (i = 1  2) with BT
i Bi = Id 
we deﬁne D(B1  B2) = kB1BT
Theorem 3. Suppose k ˜Mn − MkF = Op(n−τ ) for some τ > 0. If nτ αn → 0 as n → ∞ and
ηq > ηq+1  then the estimator bBλ in Eq. (9) satisﬁes D(bBλ  B0) = Op(n−τ ) as n → ∞.
1/4 ≤ τ ≤ 1/3. Thus Theorem 3 shows that bBλ is also consistent of the same rate.
Theorem 4. In addition to the assumptions in Theorem 3  assume nτ βn → ∞ as n → ∞. Then 
for all q + 1 ≤ j ≤ m  Pr(bvj = 0) → 1 as n → ∞  wherebvj is the j-th row of bBλ.

We saw in Theorem 2 that under some conditions ˜Mn converges to M at the rate Op(n−τ ) with

3.3 Experiments with gKDR-VS

We ﬁrst apply the gKDR-VS with d = 1 to synthetic data generated by the following two models:
(A): Y = X 1 + X 2 + X 3 + W and (B): Y = (X 1 + X 2 + X 3)4W   where the noise W follows
N (0  1). For (A)  X = (X 1  . . .   X 24) is generated by N (0  Σ) with Σij = (1/2)|i−j| (1 ≤ i  j ≤
24)  and for (B) X = (X 1  . . .   X 10) by N (0  4I10). Note that (B) includes multiplicative noise 
which cannot be handled by many dimension reduction methods. In comparison  the CIS method
with SIR is also applied to the same data. The regularization parameter of CIS-SIR is chosen by
BIC described in [2]. While both the methods work effectively for (A)  only gKDR-VS can handle
the multiplicative noise of (C).

The next experiment uses Boston Housing data  which has been often used for variable selection.
The response Y is the median value of homes in each tract  and thirteen variables are used to explain
it. The detail of the variables is described in Supplements  Sec. E. The results of gKDR-VS and CIS-
SIR / CIS-PFC with d = 2 are shown in Table 4. The variables selected by gKDR-VS are RM  DIS 
RAD  PTRATIO and LSTAT  which are slightly different from the CIS methods. In a previous study
[1]  the four variables RM  TAX  PTRATIO and LSTAT are considered to have major contribution.

4 Conclusions

We have proposed a gradient-based kernel approach for dimension reduction in supervised learn-
ing. The method is based on the general kernel formulation of conditional independence  and thus
has wide applicability without strong restrictions on the model or variables. The linear feature
extraction  gKDR-FEX  ﬁnds effective features with simple eigendecomposition  even when other
conventional methods are not applicable by multiplicative noise or high-dimensionality. The con-
sistency is also guaranteed. We have extended the method to variable selection (gKDR-VS) with a
sparseness penalty  and demonstrated its promising performance with synthetic and real world data.
The model consistency has been also proved.
Acknowledgements. KF has been supported in part by JSPS KAKENHI (B). 22300098.

8

References

[1] L. Breiman and J. Friedman. Estimating optimal transformations for multiple regression and correlation.

J. Amer. Stat. Assoc.  80:580–598  1985.

[2] X. Chen  C. Zou  and R. Dennis Cook. Coordinate-independent sparse sufﬁcient dimension reduction and

variable selection. Ann. Stat.  38(6):3696–3723  2010.

[3] R. Dennis Cook and L. Forzani. Principal ﬁtted components for dimension reduction in regression. Sta-

tistical Science  23(4):485–501  2008.

[4] R. Dennis Cook and S. Weisberg. Discussion of Li (1991). J. Amer. Stat. Assoc.  86:328–332  1991.
[5] J. Fan and I. Gijbels. Local Polynomial Modelling and its Applications. Chapman and Hall  1996.
[6] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. J. Amer.

Stat. Assoc.  96(456):1348–1360  2001.

[7] A. Frank and A. Asuncion. UCI machine learning repository  [http://archive.ics.uci.edu/ml]. Irvine  CA:

University of California  School of Information and Computer Science. 2010.

[8] K. Fukumizu  F.R. Bach  and M.I. Jordan. Dimensionality reduction for supervised learning with repro-

ducing kernel Hilbert spaces. JMLR  5:73–99  2004.

[9] K. Fukumizu  F.R. Bach  and M.I. Jordan. Kernel dimension reduction in regression. Ann. Stat. 

37(4):1871–1905  2009.

[10] A. Gretton  K. Fukumizu  C.H. Teo  L. Song  B. Sch¨olkopf  and Alex Smola. A kernel statistical test of

independence. In Advances in NIPS 20  pages 585–592. 2008.

[11] M. Hristache  A. Juditsky  J. Polzehl  and V. Spokoiny. Structure adaptive approach for dimension reduc-

tion. Ann. Stat.  29(6):1537–1566  2001.

[12] B. Li  H. Zha  and F. Chiaromonte. Contour regression: A general approach to dimension reduction. Ann.

Stat.  33(4):1580–1616  2005.

[13] K.-C. Li. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Stat. Assoc. 

86:316–342  1991.

[14] K.-C. Li. On principal Hessian directions for data visualization and dimension reduction: Another appli-

cation of Stein’s lemma. J. Amer. Stat. Assoc.  87:1025–1039  1992.

[15] S. Liu  Z. Liu  J. Sun  and L. Liu. Application of synergetic neural network in online writeprint identiﬁ-

cation. Intern. J. Digital Content Technology and its Applications  5(3):126–135  2011.

[16] L. Meier  S. Van De Geer  and P. B¨uhlmann. The group lasso for logistic regression. J. Royal Stat. Soc.:

Ser. B  70(1):53–71  2008.

[17] A.M. Samarov. Exploring regression structure using nonparametric functional estimation. J. Amer. Stat.

Assoc.  88(423):836–847  1993.

[18] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic

regression. Bioinformatics  19(17):2246–2253  2003.

[19] L. Song  J. Huang  A. Smola  and K. Fukumizu. Hilbert space embeddings of conditional distributions

with applications to dynamical systems. In Proc. ICML2009  pages 961–968. 2009.

[20] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Sch¨olkopf  and G.R.G. Lanckriet. Hilbert space

embeddings and metrics on probability measures. JMLR  11:1517–1561  2010.

[21] I. Steinwart and A. Christmann. Support Vector Machines. Springer  2008.
[22] G.W. Stewart and J.-Q. Sun. Matrix Perturbation Theory. Academic Press  1990.
[23] R. Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Soc.: Ser. B  58(1):pp.

267–288  1996.

[24] H. Wang and C. Leng. Uniﬁed lasso estimation by least squares approximation. J. Amer. Stat. Assoc.  102

(479):1039–1048  2007.

[25] H. Wang  G. Li  and C.-L. Tsai. Regression coefﬁcient and autoregressive order shrinkage and selection

via the lasso. J. Royal Stat. Soc.: Ser. B  69(1):63–78  2007.

[26] H. Wang  G. Li  and C.-L. Tsai. On the consistency of SCAD tunign parameter selector. Biometrika  94:

553–558  2007.

[27] H. Wang  B. Li  and C. Leng. Shrinkage tuning parameter selection with a diverging number of parame-

ters. J. Royal Stat. Soc.: Ser. B  71(3):671–683  2009.

[28] M. Wang  F. Sha  and M. Jordan. Unsupervised kernel dimension reduction. NIPS 23  pages 2379–2387.

2010.

[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. Royal Stat.

Soc.: Ser. B  68(1):49–67  2006.

[30] H. Zou. The adaptive lasso and its oracle properties. J. Amer. Stat. Assoc.  101:1418–1429  2006.
[31] C. Zou and X. Chen. On the consistency of coordinate-independent sparse estimation with BIC. J. Multi-

variate Analysis  112:248–255  2012.

9

,Jonas Mueller
Tommi Jaakkola
Bo Li
Yining Wang
Aarti Singh
Yevgeniy Vorobeychik