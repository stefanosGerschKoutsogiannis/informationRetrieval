2017,Learning Unknown Markov Decision Processes: A Thompson Sampling Approach,We consider the problem of learning an unknown Markov Decision Process (MDP) that is weakly communicating in the infinite horizon setting. We propose a Thompson Sampling-based reinforcement learning algorithm with dynamic episodes (TSDE). At the beginning of each episode  the algorithm generates a sample from the posterior distribution over the unknown model parameters. It then follows the optimal stationary policy for the sampled model for the rest of the episode. The duration of each episode is dynamically determined by two stopping criteria. The first stopping criterion controls the growth rate of episode length. The second stopping criterion happens when the number of visits to any state-action pair is doubled. We establish $\tilde O(HS\sqrt{AT})$ bounds on expected regret under a Bayesian setting  where $S$ and $A$ are the sizes of the state and action spaces  $T$ is time  and $H$ is the bound of the span. This regret bound matches the best available bound for weakly communicating MDPs. Numerical results show it to perform better than existing algorithms for infinite horizon MDPs.,Learning Unknown Markov Decision Processes:

A Thompson Sampling Approach

Yi Ouyang

University of California  Berkeley

ouyangyi@berkeley.edu

Mukul Gagrani

University of Southern California

mgagrani@usc.edu

Ashutosh Nayyar

University of Southern California

ashutosn@usc.edu

Rahul Jain

University of Southern California

rahul.jain@usc.edu

Abstract

We consider the problem of learning an unknown Markov Decision Process (MDP)
that is weakly communicating in the inﬁnite horizon setting. We propose a Thomp-
son Sampling-based reinforcement learning algorithm with dynamic episodes
(TSDE). At the beginning of each episode  the algorithm generates a sample from
the posterior distribution over the unknown model parameters. It then follows the
optimal stationary policy for the sampled model for the rest of the episode. The
duration of each episode is dynamically determined by two stopping criteria. The
ﬁrst stopping criterion controls the growth rate of episode length. The second
stopping criterion happens when the number of visits to any state-action pair is
doubled. We establish ˜O(HS
AT ) bounds on expected regret under a Bayesian
setting  where S and A are the sizes of the state and action spaces  T is time  and
H is the bound of the span. This regret bound matches the best available bound for
weakly communicating MDPs. Numerical results show it to perform better than
existing algorithms for inﬁnite horizon MDPs.

√

1

Introduction

We consider the problem of reinforcement learning by an agent interacting with an environment while
trying to minimize the total cost accumulated over time. The environment is modeled by an inﬁnite
horizon Markov Decision Process (MDP) with ﬁnite state and action spaces. When the environment
is perfectly known  the agent can determine optimal actions by solving a dynamic program for the
MDP [1]. In reinforcement learning  however  the agent is uncertain about the true dynamics of the
MDP. A naive approach to an unknown model is the certainty equivalence principle. The idea is to
estimate the unknown MDP parameters from available information and then choose actions as if the
estimates are the true parameters. But it is well-known in adaptive control theory that the certainty
equivalence principle may lead to suboptimal performance due to the lack of exploration [2]. This
issue actually comes from the fundamental exploitation-exploration trade-off: the agent wants to
exploit available information to minimize cost  but it also needs to explore the environment to learn
system dynamics.
One common way to handle the exploitation-exploration trade-off is to use the optimism in the
face of uncertainty (OFU) principle [3]. Under this principle  the agent constructs conﬁdence sets
for the system parameters at each time  ﬁnd the optimistic parameters that are associated with the
minimum cost  and then selects an action based on the optimistic parameters. The optimism procedure
encourages exploration for rarely visited states and actions. Several optimistic algorithms are proved
to possess strong theoretical performance guarantees [4–10].

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

An alternative way to incentivize exploration is the Thompson Sampling (TS) or Posterior Sampling
method. The idea of TS was ﬁrst proposed by Thompson in [11] for stochastic bandit problems. It has
been applied to MDP environments [12–17] where the agent computes the posterior distribution of
unknown parameters using observed information and a prior distribution. A TS algorithm generally
proceeds in episodes: at the beginning of each episode a set of MDP parameters is randomly
sampled from the posterior distribution  then actions are selected based on the sampled model during
the episode. TS algorithms have the following advantages over optimistic algorithms. First  TS
algorithms can easily incorporate problem structures through the prior distribution. Second  they are
more computationally efﬁcient since a TS algorithm only needs to solve the sampled MDP  while an
optimistic algorithm requires solving all MDPs that lie within the conﬁdent sets. Third  empirical
studies suggest that TS algorithms outperform optimistic algorithms in bandit problems [18  19] as
well as in MDP environments [13  16  17].
Due to the above advantages  we focus on TS algorithms for the MDP learning problem. The main
challenge in the design of a TS algorithm is the lengths of the episodes. For ﬁnite horizon MDPs
under the episodic setting  the length of each episode can be set as the time horizon [13]. When there
exists a recurrent state under any stationary policy  the TS algorithm of [15] starts a new episode
whenever the system enters the recurrent state. However  the above methods to end an episode can
not be applied to MDPs without the special features. The work of [16] proposed a dynamic episode
schedule based on the doubling trick used in [7]  but a mistake in their proof of regret bound was
pointed out by [20]. In view of the mistake in [16]  there is no TS algorithm with strong performance
guarantees for general MDPs to the best of our knowledge.
We consider the most general subclass of weakly communicating MDPs in which meaningful ﬁnite
time regret guarantees can be analyzed. We propose the Thompson Sampling with Dynamic Episodes
(TSDE) learning algorithm. In TSDE  there are two stopping criteria for an episode to end. The
ﬁrst stopping criterion controls the growth rate of episode length. The second stopping criterion
is the doubling trick similar to the one in [7–10  16] that stops when the number of visits to any
state-action pair is doubled. Under a Bayesian framework  we show that the expected regret of TSDE
accumulated up to time T is bounded by ˜O(HS
AT ) where ˜O hides logarithmic factors. Here S
and A are the sizes of the state and action spaces  T is time  and H is the bound of the span. This
regret bound matches the best available bound for weakly communicating MDPs [7]  and it matches
the theoretical lower bound in order of T except for logarithmic factors. We present numerical results
that show that TSDE actually outperforms current algorithms with known regret bounds that have the
same order in T for a benchmark MDP problem as well as randomly generated MDPs.

√

2 Problem Formulation

2.1 Preliminaries
An inﬁnite horizon Markov Decision Process (MDP) is described by (S A  c  θ). Here S is the state
space  A is the action space  c : S × A → [0  1]1 is the cost function  and θ : S 2 × A → [0  1]
represents the transition probabilities such that θ(s(cid:48)|s  a) = P(st+1 = s(cid:48)|st = s  at = a) where
st ∈ S and at ∈ A are the state and the action at t = 1  2  3 . . . . We assume that S and A are ﬁnite
spaces with sizes S ≥ 2 and A ≥ 2  and the initial state s1 is a known and ﬁxed state. A stationary
policy is a deterministic map π : S → A that maps a state to an action. The average cost per stage of
a stationary policy is deﬁned as

Jπ(θ) = lim sup
T→∞

1
T

E(cid:104) T(cid:88)

t=1

(cid:105)

c(st  at)

.

Here we use Jπ(θ) to explicitly show the dependency of the average cost on θ.
To have meaningful ﬁnite time regret bounds  we consider the subclass of weakly communicating
MDPs deﬁned as follows.
Deﬁnition 1. An MDP is weakly communicating (or weak accessible) if its states can be partitioned
into two subsets: in the ﬁrst subset all states are transient under every stationary policy  and every
two states in the second subset can be reached from each other under some stationary policy.

1Since S and A are ﬁnite  we can normalize the cost function to [0  1] without loss of generality.

2

From MDP theory [1]  we know that if the MDP is weakly communicating  the optimal average cost
per stage J(θ) = minπ Jπ(θ) satisﬁes the Bellman equation

J(θ) + v(s  θ) = min
a∈A

c(s  a) +

(cid:48)|s  a)v(s
(cid:48)

θ(s

  θ)

(1)

(cid:110)

(cid:88)

s(cid:48)∈S

(cid:111)

for all s ∈ S. The corresponding optimal stationary policy π∗ is the minimizer of the above
optimization given by

a = π

(s  θ).

(2)
Since the cost function c(s  a) ∈ [0  1]  J(θ) ∈ [0  1] for all θ. If v satisﬁes the Bellman equation  v
plus any constant also satisﬁes the Bellman equation. Without loss of generality  let mins∈S v(s  θ) =
0 and deﬁne the span of the MDP as sp(θ) = maxs∈S v(s  θ). 2
We deﬁne Ω∗ to be the set of all θ such that the MDP with transition probabilities θ is weakly
communicating  and there exists a number H such that sp(θ) ≤ H. We will focus on MDPs with
transition probabilities in the set Ω∗.

∗

2.2 Reinforcement Learning for Weakly Communicating MDPs

We consider the reinforcement learning problem of an agent interacting with a random weakly
communicating MDP (S A  c  θ∗). We assume that S  A and the cost function c are completely
known to the agent. The actual transition probabilities θ∗ is randomly generated at the beginning
before the MDP interacts with the agent. The value of θ∗ is then ﬁxed but unknown to the agent. The
complete knowledge of the cost is typical as in [7  15]. Algorithms can generally be extended to the
unknown costs/rewards case at the expense of some constant factor for the regret bound.
At each time t 
the agent selects an action according to at = φt(ht) where ht =
(s1  s2  . . .   st  a1  a2  . . .   at−1) is the history of states and actions. The collection φ = (φ1  φ2 . . . )
is called a learning algorithm. The functions φt allow for the possibility of randomization over actions
at each time.
We focus on a Bayesian framework for the unknown parameter θ∗. Let µ1 be the prior distribution
for θ∗  i.e.  for any set Θ  P(θ∗ ∈ Θ) = µ1(Θ). We make the following assumptions on µ1.
Assumption 1. The support of the prior distribution µ1 is a subset of Ω∗. That is  the MDP is weakly
communicating and sp(θ∗) ≤ H.
In this Bayesian framework  we deﬁne the expected regret (also called Bayesian regret or Bayes risk)
of a learning algorithm φ up to time T as

R(T  φ) = E(cid:104) T(cid:88)

(cid:104)

c(st  at) − J(θ∗)

(cid:105)(cid:105)

(3)

where st  at  t = 1  . . .   T are generated by φ and J(θ∗) is the optimal per stage cost of the MDP.
The above expectation is with respect to the prior distribution µ1 for θ∗  the randomness in state
transitions  and the randomized algorithm. The expected regret is an important metric to quantify the
performance of a learning algorithm.

t=1

3 Thompson Sampling with Dynamic Episodes

In this section  we propose the Thompson Sampling with Dynamic Episodes (TSDE) learning
algorithm. The input of TSDE is the prior distribution µ1. At each time t  given the history ht  the
agent can compute the posterior distribution µt given by µt(Θ) = P(θ∗ ∈ Θ|ht) for any set Θ. Upon
applying the action at and observing the new state st+1  the posterior distribution at t + 1 can be
updated according to Bayes’ rule as

µt+1(dθ) =

.

(4)

θ(st+1|st  at)µt(dθ)

(cid:82) θ(cid:48)(st+1|st  at)µt(dθ(cid:48))

2See [7]for a discussion on the connection of the span with other parameters such as the diameter appearing

in the lower bound on regret.

3

Let Nt(s  a) be the number of visits to any state-action pair (s  a) before time t. That is 

Nt(s  a) = |{τ < t : (sτ   aτ ) = (s  a)}|.

(5)

With these notations  TSDE is described as follows.

Algorithm 1 Thompson Sampling with Dynamic Episodes (TSDE)

Input: µ1
Initialization: t ← 1  tk ← 0
for episodes k = 1  2  ... do

Tk−1 ← t − tk
tk ← t
Generate θk ∼ µtk and compute πk(·) = π∗(·  θk) from (1)-(2)
while t ≤ tk + Tk−1 and Nt(s  a) ≤ 2Ntk (s  a) for all (s  a) ∈ S × A do

Apply action at = πk(st)
Observe new state st+1
Update µt+1 according to (4)
t ← t + 1

end while

end for

The TSDE algorithm operates in episodes. Let tk be start time of the kth episode and Tk = tk+1 − tk
be the length of the episode with the convention T0 = 1. From the description of the algorithm 
t1 = 1 and tk+1  k ≥ 1  is given by

tk+1 = min{t > tk :

t > tk + Tk−1 or Nt(s  a) > 2Ntk (s  a) for some (s  a)}.

(6)

At the beginning of episode k  a parameter θk is sampled from the posterior distribution µtk. During
each episode k  actions are generated from the optimal stationary policy πk for the sampled parameter
θk. One important feature of TSDE is that its episode lengths are not ﬁxed. The length Tk of each
episode is dynamically determined according to two stopping criteria: (i) t > tk + Tk−1  and (ii)
Nt(s  a) > 2Ntk (s  a) for some state-action pair (s  a). The ﬁrst stopping criterion provides that the
episode length grows at a linear rate without triggering the second criterion. The second stopping
criterion ensures that the number of visits to any state-action pair (s  a) during an episode should not
be more than the number visits to the pair before this episode.
Remark 1. Note that TSDE only requires the knowledge of S  A  c  and the prior distribution µ1.
TSDE can operate without the knowledge of time horizon T   the bound H on span used in [7]  and
any knowledge about the actual θ∗ such as the recurrent state needed in [15].

3.1 Main Result

Theorem 1. Under Assumption 1 

R(T  TSDE) ≤ (H + 1)

(cid:112)

2SAT log(T ) + 49HS

(cid:112)

AT log(AT ).

The proof of Theorem 1 appears in Section 4.
Remark 2. Note that our regret bound has the same order in H  S  A and T as the optimistic
algorithm in [7] which is the best available bound for weakly communicating MDPs. Moreover  the
bound does not depend on the prior distribution or other problem-dependent parameters such as the
recurrent time of the optimal policy used in the regret bound of [15].

3.2 Approximation Error

At the beginning of each episode  TSDE computes the optimal stationary policy πk for the parameter
θk. This step requires the solution to a ﬁxed ﬁnite MDP. Policy iteration or value iteration can be used
to solve the sampled MDP  but the resulting stationary policy may be only approximately optimal in
practice. We call π an −approximate policy if

(cid:110)

c(s  a) +

(cid:88)

s(cid:48)∈S

(cid:111)

(cid:48)|s  a)v(s
(cid:48)

θ(s

  θ)

+ .

(cid:88)

s(cid:48)∈S

c(s  π(s)) +

(cid:48)|s  π(s))v(s
(cid:48)

θ(s

  θ) ≤ min
a∈A

4

When the algorithm returns an k−approximate policy ˜πk instead of the optimal stationary policy πk
at episode k  we have the following regret bound in the presence of such approximation error.
Theorem 2. If TSDE computes an k−approximate policy ˜πk instead of the optimal stationary policy
πk at each episode k  the expected regret of TSDE satisﬁes

R(T  TSDE) ≤ ˜O(HS

k+1   E(cid:104)(cid:80)

Furthermore  if k ≤ 1

k:tk≤T Tkk

(cid:105)

Tkk

.

√

AT ) + E(cid:104) (cid:88)
(cid:105) ≤(cid:112)

2SAT log(T ).

k:tk≤T

Theorem 2 shows that the approximation error in the computation of optimal stationary policy is only
additive to the regret under TSDE. The regret bound would remain ˜O(HS
AT ) if the approximation
error is such that k ≤ 1

k+1. The proof of Theorem 2 is in the appendix due to the lack of space.

√

4 Analysis

4.1 Number of Episodes
To analyze the performance of TSDE over T time steps  deﬁne KT = arg max{k : tk ≤ T} be the
number of episodes of TSDE until time T . Note that KT is a random variable because the number
of visits Nt(x  u) depends on the dynamical state trajectory. In the analysis for time T we use the
convention that t(KT +1) = T + 1. We provide an upper bound on KT as follows.
Lemma 1.

KT ≤(cid:112)

2SAT log(T ).

Proof. Deﬁne macro episodes with start times tni   i = 1  2  . . . where tn1 = t1 and
tni+1 = min{tk > tni : Ntk (s  a) > 2Ntk−1(s  a) for some (s  a)}.

The idea is that each macro episode starts when the second stopping criterion happens. Let M be the
number of macro episodes until time T and deﬁne n(M +1) = KT + 1.

k=ni Tk be the length of the ith macro episode. By the deﬁnition of macro episodes 
any episode except the last one in a macro episode must be triggered by the ﬁrst stopping criterion.
Therefore  within the ith macro episode  Tk = Tk−1 + 1 for all k = ni  ni + 1  . . .   ni+1 − 2. Hence 

(Tni−1 + j) + Tni+1−1

(j + 1) + 1 = 0.5(ni+1 − ni)(ni+1 − ni + 1).

Let ˜Ti =(cid:80)ni+1−1
ni+1−1(cid:88)

˜Ti =

Tk =

k=ni

≥

Consequently  ni+1 − ni ≤(cid:112)
Using (7) and the fact that(cid:80)M

ni+1−ni−1(cid:88)
ni+1−ni−1(cid:88)

j=1

j=1

2 ˜Ti for all i = 1  . . .   M. From this property we obtain

(cid:113)

2 ˜Ti.

KT =nM +1 − 1 =
(cid:113)

KT ≤ M(cid:88)

i=1
˜Ti = T we get

2 ˜Ti ≤

i=1

M(cid:88)
(ni+1 − ni) ≤ M(cid:88)
(cid:118)(cid:117)(cid:117)(cid:116)M
M(cid:88)

2 ˜Ti =

√

i=1

2M T

(7)

(8)

i=1

i=1

where the second inequality is Cauchy-Schwarz.
From Lemma 6 in the appendix  the number of macro episodes M ≤ SA log(T ). Substituting this
bound into (8) we obtain the result of this lemma.
Remark 3. TSDE computes the optimal stationary policy of a ﬁnite MDP at each episode. Lemma 1

ensures that such computation only needs to be done at a sublinear rate of(cid:112)

2SAT log(T ).

5

4.2 Regret Bound

As discussed in [13  20  21]  one key property of Thompson/Posterior Sampling algorithms is that for
any function f  E[f (θt)] = E[f (θ∗)] if θt is sampled from the posterior distribution at time t. This
property leads to regret bounds for algorithms with ﬁxed sampling episodes since the start time tk of
each episode is deterministic. However  our TSDE algorithm has dynamic episodes that requires us
to have the stopping-time version of the above property.
Lemma 2. Under TSDE  tk is a stopping time for any episode k. Then for any measurable function
f and any σ(htk )−measurable random variable X  we have

E(cid:104)

(cid:105)

= E(cid:104)

(cid:105)

f (θk  X)

f (θ∗  X)

Proof. From the deﬁnition (6)  the start time tk is a stopping-time  i.e. tk is σ(htk )−measurable.
Note that θk is randomly sampled from the posterior distribution µtk. Since tk is a stopping time 
tk and µtk are both measurable with respect to σ(htk ). From the assumption  X is also measurable
with respect to σ(htk ). Then conditioned on htk  the only randomness in f (θk  X) is the random
sampling in the algorithm. This gives the following equation:

f (θk  X)|htk

f (θk  X)|htk   tk  µtk

=

f (θ∗  X)|htk

(9)

(cid:105)

(cid:90)

f (θ  X)µtk (dθ) = E(cid:104)

(cid:105)

E(cid:104)

(cid:105)

= E(cid:104)

since µtk is the posterior distribution of θ∗ given htk. Now the result follows by taking the expectation
of both sides.

For tk ≤ t < tk+1 in episode k  the Bellman equation (1) holds by Assumption 1 for s = st  θ = θk
and action at = πk(st). Then we obtain

c(st  at) = J(θk) + v(st  θk) −(cid:88)

s(cid:48)∈S

(cid:48)|st  at)v(s
(cid:48)

θk(s

  θk).

(10)

Using (10)  the expected regret of TSDE is equal to

tk+1−1(cid:88)

E(cid:104) KT(cid:88)
= E(cid:104) KT(cid:88)

k=1

c(st  at)

(cid:105) − T E(cid:104)

t=tk

TkJ(θk)

(cid:105) − T E(cid:104)
(cid:105)

J(θ∗)

J(θ∗)

(cid:105)
+ E(cid:104) KT(cid:88)

k=1

=R0 + R1 + R2 

k=1

t=tk

tk+1−1(cid:88)

(cid:104)

v(st  θk) −(cid:88)

s(cid:48)∈S

(cid:48)|st  at)v(s
(cid:48)

θk(s

(cid:105)(cid:105)

  θk)

(11)

where R0  R1 and R2 are given by

R0 = E(cid:104) KT(cid:88)
R1 = E(cid:104) KT(cid:88)
R2 = E(cid:104) KT(cid:88)

k=1

k=1

TkJ(θk)

tk+1−1(cid:88)
tk+1−1(cid:88)

t=tk

(cid:104)
(cid:104)

k=1

t=tk

(cid:105) − T E(cid:104)

(cid:105)

J(θ∗)

 

(cid:105)(cid:105)

 

v(st  θk) − v(st+1  θk)

v(st+1  θk) −(cid:88)

s(cid:48)∈S

(cid:48)|st  at)v(s
(cid:48)

θk(s

  θk)

(cid:105)(cid:105)

.

We proceed to derive bounds on R0  R1 and R2.
Based on the key property of Lemma 2  we derive an upper bound on R0.
Lemma 3. The ﬁrst term R0 is bounded as

R0 ≤ E[KT ].

6

Proof. From monotone convergence theorem we have

(cid:105) − T E(cid:104)

(cid:105)

∞(cid:88)

E(cid:104)

1{tk≤T}TkJ(θk)

(cid:105) − T E(cid:104)

(cid:105)

J(θ∗)

.

1{tk≤T}TkJ(θk)

J(θ∗)

=

R0 = E(cid:104) ∞(cid:88)

k=1

k=1

Note that the ﬁrst stopping criterion of TSDE ensures that Tk ≤ Tk−1 + 1 for all k. Because
J(θk) ≥ 0  each term in the ﬁrst summation satisﬁes

1{tk≤T}TkJ(θk)

1{tk≤T}(Tk−1 + 1)J(θk)

.

Note that 1{tk≤T}(Tk−1 + 1) is measurable with respect to σ(htk ). Then  Lemma 2 gives

1{tk≤T}(Tk−1 + 1)J(θk)

1{tk≤T}(Tk−1 + 1)J(θ∗)

.

(cid:105) ≤ E(cid:104)
= E(cid:104)
(cid:105)

(cid:105)

(cid:105)

E(cid:104)

E(cid:104)

k=1

E(cid:104)
∞(cid:88)
= E(cid:104) KT(cid:88)
= E(cid:104)

k=1

Combining the above equations we get

R0 ≤

1{tk≤T}(Tk−1 + 1)J(θ∗)

(Tk−1 + 1)J(θ∗)

(cid:105)

+ E(cid:104)(cid:16) KT(cid:88)

where the last equality holds because J(θ∗) ≤ 1 and(cid:80)KT

KT J(θ∗)

k=1

(cid:105)

J(θ∗)

(cid:105) − T E(cid:104)
(cid:105)

(cid:105) − T E(cid:104)

Tk−1 − T

J(θ∗)

(cid:105) ≤ E(cid:104)

(cid:17)
(cid:105)
k=1 Tk−1 = T0 +(cid:80)KT −1

J(θ∗)

KT

k=1 Tk ≤ T .

Note that the ﬁrst stopping criterion of TSDE plays a crucial role in the proof of Lemma 3. It allows
us to bound the length of an episode using the length of the previous episode which is measurable
with respect to the information at the beginning of the episode.
The other two terms R1 and R2 of the regret are bounded in the following lemmas. Their proofs
follow similar steps to those in [13  16]. The proofs are in the appendix due to the lack of space.
Lemma 4. The second term R1 is bounded as

Lemma 5. The third term R2 is bounded as

R2 ≤ 49HS

AT log(AT ).

R1 ≤ E[HKT ].

(cid:112)

We are now ready to prove Theorem 1.
Proof of Theorem 1. From (11)  R(T  TSDE) = R0 + R1 + R2 ≤ E[KT ] + E[HKT ] + R2 where
the inequality comes from Lemma 3  Lemma 4. Then the claim of the theorem directly follows from
Lemma 1 and Lemma 5.

5 Simulations

In this section  we compare through simulations the performance of TSDE with three learning
algorithms with the same regret order: UCRL2 [8]  TSMDP [15]  and Lazy PSRL [16]. UCRL2 is
an optimistic algorithm with similar regret bounds. TSMDP and Lazy PSRL are TS algorithms for
inﬁnite horizon MDPs. TSMDP has the same regret order in T given a recurrent state for resampling.
The original regret analysis for Lazy PSRL is incorrect  but the regret bounds are conjectured to
be correct [20]. We chose δ = 0.05 for the implementation of UCRL2 and assume an independent
Dirichlet prior with parameters [0.1  . . .   0.1] over the transition probabilities for all TS algorithms.
We consider two environments: randomly generated MDPs and the RiverSwim example [22]. For
randomly generated MDPs  we use the independent Dirichlet prior over 6 states and 2 actions but

7

with a ﬁxed cost. We select the resampling state s0 = 1 for TSMDP here since all states are recurrent
under the Dirichlet prior. The RiverSwim example models an agent swimming in a river who can
choose to swim either left or right. The MDP consists of six states arranged in a chain with the agent
starting in the leftmost state (s = 1). If the agent decides to move left i.e with the river current then
he is always successful but if he decides to move right he might fail with some probability. The cost
function is given by: c(s  a) = 0.8 if s = 1  a = left; c(s  a) = 0 if s = 6  a = right; and c(s  a) = 1
otherwise. The optimal policy is to swim right to reach the rightmost state which minimizes the cost.
For TSMDP in RiverSwim  we consider two versions with s0 = 1 and with s0 = 3 for the resampling
state. We simulate 500 Monte Carlo runs for both the examples and run for T = 105.

(a) Expected Regret vs Time for random MDPs

(b) Expected Regret vs Time for RiverSwim

Figure 1: Simulation Results

From Figure 1(a) we can see that TSDE outperforms all the three algorithms in randomly generated
MDPs. In particular  there is a signiﬁcant gap between the regret of TSDE and that of UCRL2
and TSMDP. The poor performance of UCRL2 assures the motivation to consider TS algorithms.
From the speciﬁcation of TSMDP  its performance heavily hinges on the choice of an appropriate
resampling state which is not possible for a general unknown MDP. This is reﬂected in the randomly
generated MDPs experiment.
In the RiverSwim example  Figure 1(b) shows that TSDE signiﬁcantly outperforms UCRL2  Lazy
PSRL  and TSMDP with s0 = 3. Although TSMDP with s0 = 1 performs slightly better than TSDE 
there is no way to pick this speciﬁc s0 if the MDP is unknown in practice. Since Lazy PSRL is also
equipped with the doubling trick criterion  the performance gap between TSDE and Lazy PSRL
highlights the importance of the ﬁrst stopping criterion on the growth rate of episode length. We also
like to point out that in this example  the MDP is ﬁxed and is not generated from the Dirichlet prior.
Therefore  we conjecture that TSDE also has the same regret bounds under a non-Bayesian setting.

6 Conclusion

√

We propose the Thompson Sampling with Dynamic Episodes (TSDE) learning algorithm and establish
˜O(HS
AT ) bounds on expected regret for the general subclass of weakly communicating MDPs.
Our result ﬁlls a gap in the theoretical analysis of Thompson Sampling for MDPs. Numerical results
validate that the TSDE algorithm outperforms other learning algorithms for inﬁnite horizon MDPs.
The TSDE algorithm determines the end of an episode by two stopping criteria. The second criterion
comes from the doubling trick used in many reinforcement learning algorithms. But the ﬁrst criterion
on the linear growth rate of episode length seems to be a new idea for episodic learning algorithms.
The stopping criterion is crucial in the proof of regret bound (Lemma 3). The simulation results of
TSDE versus Lazy PSRL further shows that this criterion is not only a technical constraint for proofs 
it indeed helps balance exploitation and exploration.

8

0246810x 1040100200300400500600TRegret UCRL2TSMDPLazyPSRLTSDE0246810x 1040100020003000400050006000TRegret UCRL2TSMDPwiths0=3TSMDPwiths0=1LazyPSRLTSDEAcknowledgments

Yi Ouyang would like to thank Yang Liu from Harvard University for helpful discussions. Rahul Jain
and Ashutosh Nayyar were supported by NSF Grants 1611574 and 1446901.

References
[1] D. P. Bertsekas  Dynamic programming and optimal control  vol. 2. Athena Scientiﬁc  Belmont 

MA  2012.

[2] P. R. Kumar and P. Varaiya  Stochastic systems: Estimation  identiﬁcation  and adaptive control.

SIAM  2015.

[3] T. L. Lai and H. Robbins  “Asymptotically efﬁcient adaptive allocation rules ” Advances in

applied mathematics  vol. 6  no. 1  pp. 4–22  1985.

[4] A. N. Burnetas and M. N. Katehakis  “Optimal adaptive policies for markov decision processes ”

Mathematics of Operations Research  vol. 22  no. 1  pp. 222–255  1997.

[5] M. Kearns and S. Singh  “Near-optimal reinforcement learning in polynomial time ” Machine

Learning  vol. 49  no. 2-3  pp. 209–232  2002.

[6] R. I. Brafman and M. Tennenholtz  “R-max-a general polynomial time algorithm for near-
optimal reinforcement learning ” Journal of Machine Learning Research  vol. 3  no. Oct 
pp. 213–231  2002.

[7] P. L. Bartlett and A. Tewari  “Regal: A regularization based algorithm for reinforcement learning

in weakly communicating mdps ” in UAI  2009.

[8] T. Jaksch  R. Ortner  and P. Auer  “Near-optimal regret bounds for reinforcement learning ”

Journal of Machine Learning Research  vol. 11  no. Apr  pp. 1563–1600  2010.

[9] S. Filippi  O. Capp´e  and A. Garivier  “Optimism in reinforcement learning and kullback-leibler

divergence ” in Allerton  pp. 115–122  2010.

[10] C. Dann and E. Brunskill  “Sample complexity of episodic ﬁxed-horizon reinforcement learning ”

in NIPS  2015.

[11] W. R. Thompson  “On the likelihood that one unknown probability exceeds another in view of

the evidence of two samples ” Biometrika  vol. 25  no. 3/4  pp. 285–294  1933.

[12] M. Strens  “A bayesian framework for reinforcement learning ” in ICML  2000.

[13] I. Osband  D. Russo  and B. Van Roy  “(More) efﬁcient reinforcement learning via posterior

sampling ” in NIPS  2013.

[14] R. Fonteneau  N. Korda  and R. Munos  “An optimistic posterior sampling strategy for bayesian

reinforcement learning ” in BayesOpt2013  2013.

[15] A. Gopalan and S. Mannor  “Thompson sampling for learning parameterized markov decision

processes ” in COLT  2015.

[16] Y. Abbasi-Yadkori and C. Szepesv´ari  “Bayesian optimal control of smoothly parameterized

systems. ” in UAI  2015.

[17] I. Osband and B. Van Roy  “Why is posterior sampling better than optimism for reinforcement

learning ” EWRL  2016.

[18] S. L. Scott  “A modern bayesian look at the multi-armed bandit ” Applied Stochastic Models in

Business and Industry  vol. 26  no. 6  pp. 639–658  2010.

[19] O. Chapelle and L. Li  “An empirical evaluation of thompson sampling ” in NIPS  2011.

[20] I. Osband and B. Van Roy  “Posterior sampling for reinforcement learning without episodes ”

arXiv preprint arXiv:1608.02731  2016.

9

[21] D. Russo and B. Van Roy  “Learning to optimize via posterior sampling ” Mathematics of

Operations Research  vol. 39  no. 4  pp. 1221–1243  2014.

[22] A. L. Strehl and M. L. Littman  “An analysis of model-based interval estimation for markov
decision processes ” Journal of Computer and System Sciences  vol. 74  no. 8  pp. 1309–1331 
2008.

10

,Tamara Broderick
Nicholas Boyd
Andre Wibisono
Ashia Wilson
Michael Jordan
Paul Lagrée
Claire Vernade
Yi Ouyang
Mukul Gagrani
Ashutosh Nayyar
Rahul Jain