2015,Policy Gradient for Coherent Risk Measures,Several authors have recently developed risk-sensitive policy gradient methods that augment the standard expected cost minimization problem with a measure of variability in cost. These studies have focused on specific risk-measures  such as the variance or conditional value at risk (CVaR). In this work  we extend the policy gradient method to the whole class of coherent risk measures  which is widely accepted in finance and operations research  among other fields. We consider both static and time-consistent dynamic risk measures. For static risk measures  our approach is in the spirit of policy gradient algorithms and combines a standard sampling approach with convex programming. For dynamic risk measures  our approach is actor-critic style and involves explicit approximation of value function. Most importantly  our contribution presents a unified approach to risk-sensitive reinforcement learning that generalizes and extends previous results.,Policy Gradient for Coherent Risk Measures

Aviv Tamar
UC Berkeley

avivt@berkeley.edu

Mohammad Ghavamzadeh
Adobe Research & INRIA

mohammad.ghavamzadeh@inria.fr

Yinlam Chow

Stanford University

ychow@stanford.edu

Shie Mannor

Technion

shie@ee.technion.ac.il

Abstract

Several authors have recently developed risk-sensitive policy gradient methods
that augment the standard expected cost minimization problem with a measure of
variability in cost. These studies have focused on speciﬁc risk-measures  such as
the variance or conditional value at risk (CVaR). In this work  we extend the pol-
icy gradient method to the whole class of coherent risk measures  which is widely
accepted in ﬁnance and operations research  among other ﬁelds. We consider
both static and time-consistent dynamic risk measures. For static risk measures 
our approach is in the spirit of policy gradient algorithms and combines a standard
sampling approach with convex programming. For dynamic risk measures  our ap-
proach is actor-critic style and involves explicit approximation of value function.
Most importantly  our contribution presents a uniﬁed approach to risk-sensitive
reinforcement learning that generalizes and extends previous results.

Introduction

1
Risk-sensitive optimization considers problems in which the objective involves a risk measure of
the random cost  in contrast to the typical expected cost objective. Such problems are important
when the decision-maker wishes to manage the variability of the cost  in addition to its expected
outcome  and are standard in various applications of ﬁnance and operations research. In reinforce-
ment learning (RL) [27]  risk-sensitive objectives have gained popularity as a means to regularize
the variability of the total (discounted) cost/reward in a Markov decision process (MDP).
Many risk objectives have been investigated in the literature and applied to RL  such as the cel-
ebrated Markowitz mean-variance model [16]  Value-at-Risk (VaR) and Conditional Value at Risk
(CVaR) [18  29  21  10  8  30]. The view taken in this paper is that the preference of one risk measure
over another is problem-dependent and depends on factors such as the cost distribution  sensitivity to
rare events  ease of estimation from data  and computational tractability of the optimization problem.
However  the highly inﬂuential paper of Artzner et al. [2] identiﬁed a set of natural properties that
are desirable for a risk measure to satisfy. Risk measures that satisfy these properties are termed co-
herent and have obtained widespread acceptance in ﬁnancial applications  among others. We focus
on such coherent measures of risk in this work.
For sequential decision problems  such as MDPs  another desirable property of a risk measure is
time consistency. A time-consistent risk measure satisﬁes a “dynamic programming” style property:
if a strategy is risk-optimal for an n-stage problem  then the component of the policy from the t-th
time until the end (where t < n) is also risk-optimal (see principle of optimality in [5]). The recently
proposed class of dynamic Markov coherent risk measures [24] satisﬁes both the coherence and time
consistency properties.
In this work  we present policy gradient algorithms for RL with a coherent risk objective. Our
approach applies to the whole class of coherent risk measures  thereby generalizing and unifying
previous approaches that have focused on individual risk measures. We consider both static coherent

1

risk of the total discounted return from an MDP and time-consistent dynamic Markov coherent
risk. Our main contribution is formulating the risk-sensitive policy-gradient under the coherent-risk
framework. More speciﬁcally  we provide:

using sampling.

• A new formula for the gradient of static coherent risk that is convenient for approximation
• An algorithm for the gradient of general static coherent risk that involves sampling with
• A new policy gradient theorem for Markov coherent risk  relating the gradient to a suitable

convex programming and a corresponding consistency result.

value function and a corresponding actor-critic algorithm.

Several previous results are special cases of the results presented here; our approach allows to re-
derive them in greater generality and simplicity.
Related Work Risk-sensitive optimization in RL for speciﬁc risk functions has been studied re-
cently by several authors. [6] studied exponential utility functions  [18]  [29]  [21] studied mean-
variance models  [8]  [30] studied CVaR in the static setting  and [20]  [9] studied dynamic coherent
risk for systems with linear dynamics. Our paper presents a general method for the whole class of
coherent risk measures (both static and dynamic) and is not limited to a speciﬁc choice within that
class  nor to particular system dynamics.
Reference [19] showed that an MDP with a dynamic coherent risk objective is essentially a ro-
bust MDP. The planning for large scale MDPs was considered in [31]  using an approximation of
the value function. For many problems  approximation in the policy space is more suitable (see 
e.g.  [15]). Our sampling-based RL-style approach is suitable for approximations both in the policy
and value function  and scales-up to large or continuous MDPs. We do  however  make use of a
technique of [31] in a part of our method.
Optimization of coherent
risk measures was thoroughly investigated by Ruszczynski and
Shapiro [25] (see also [26]) for the stochastic programming case in which the policy parameters
do not affect the distribution of the stochastic system (i.e.  the MDP trajectory)  but only the reward
function  and thus  this approach is not suitable for most RL problems. For the case of MDPs and
dynamic risk  [24] proposed a dynamic programming approach. This approach does not scale-up
to large MDPs  due to the “curse of dimensionality”. For further motivation of risk-sensitive policy
gradient methods  we refer the reader to [18  29  21  8  30].
2 Preliminaries
Consider a probability space (Ω F  Pθ)  where Ω is the set of outcomes (sample space)  F is a
σ-algebra over Ω representing the set of events we are interested in  and Pθ ∈ B  where B :=
parameterized by some tunable parameter θ ∈ RK. In the following  we suppress the notation of θ
in θ-dependent quantities.
To ease the technical exposition  in this paper we restrict our attention to ﬁnite probability spaces 
i.e.  Ω has a ﬁnite number of elements. Our results can be extended to the Lp-normed spaces without
loss of generality  but the details are omitted for brevity.
Denote by Z the space of random variables Z : Ω (cid:55)→ (−∞ ∞) deﬁned over the probability space
(Ω F  Pθ). In this paper  a random variable Z ∈ Z is interpreted as a cost  i.e.  the smaller the
realization of Z  the better. For Z  W ∈ Z  we denote by Z ≤ W the point-wise partial order 
i.e.  Z(ω) ≤ W (ω) for all ω ∈ Ω. We denote by Eξ[Z]
ω∈Ω Pθ(ω)ξ(ω)Z(ω) a ξ-weighted
expectation of Z.
An MDP is a tuple M = (X  A  C  P  γ  x0)  where X and A are the state and action spaces;
C(x) ∈ [−Cmax  Cmax] is a bounded  deterministic  and state-dependent cost; P (·|x  a) is the tran-
sition probability distribution; γ is a discount factor; and x0 is the initial state.1 Actions are chosen
according to a θ-parameterized stationary Markov2 policy µθ(·|x). We denote by x0  a0  . . .   xT   aT
a trajectory of length T drawn by following the policy µθ in the MDP.

ω∈Ω ξ(ω) = 1  ξ ≥ 0(cid:9) is the set of probability distributions  is a probability measure over F

(cid:8)ξ :(cid:82)

= (cid:80)

1Our results may easily be extended to random costs  state-action dependent costs  and random initial states.
2For Markov coherent risk  the class of optimal policies is stationary Markov [24]  while this is not nec-
essarily true for static risk. Our results can be extended to history-dependent policies or stationary Markov

.

2

2.1 Coherent Risk Measures
A risk measure is a function ρ : Z → R that maps an uncertain outcome Z to the extended real line

R ∪ {+∞ −∞}  e.g.  the expectation E [Z] or the conditional value-at-risk (CVaR) minν∈R(cid:8)ν +
E(cid:2)(Z − ν)+(cid:3)(cid:9). A risk measure is called coherent  if it satisﬁes the following conditions for all
A1 Convexity: ∀λ ∈ [0  1]  ρ(cid:0)λZ + (1 − λ)W(cid:1) ≤ λρ(Z) + (1 − λ)ρ(W );

Z  W ∈ Z [2]:

1
α

A2 Monotonicity: if Z ≤ W   then ρ(Z) ≤ ρ(W );
A3 Translation invariance: ∀a∈R  ρ(Z + a) = ρ(Z) + a;
A4 Positive homogeneity: if λ ≥ 0  then ρ(λZ) = λρ(Z).

Intuitively  these condition ensure the “rationality” of single-period risk assessments: A1 ensures
that diversifying an investment will reduce its risk; A2 guarantees that an asset with a higher cost
for every possible scenario is indeed riskier; A3  also known as ‘cash invariance’  means that the
deterministic part of an investment portfolio does not contribute to its risk; the intuition behind A4
is that doubling a position in an asset doubles its risk. We further refer the reader to [2] for a more
detailed motivation of coherent risk.
The following representation theorem [26] shows an important property of coherent risk measures
that is fundamental to our gradient-based approach.
Theorem 2.1. A risk measure ρ : Z → R is coherent if and only if there exists a convex bounded
and closed set U ⊂ B such that3

ρ(Z) =

max

ξ : ξPθ∈U (Pθ)

Eξ[Z].

(1)

The result essentially states that any coherent risk measure is an expectation w.r.t. a worst-case
density function ξPθ  i.e.  a re-weighting of Pθ by ξ  chosen adversarially from a suitable set of test
density functions U(Pθ)  referred to as risk envelope. Moreover  a coherent risk measure is uniquely
represented by its risk envelope.
In the sequel  we shall interchangeably refer to coherent risk
measures either by their explicit functional representation  or by their corresponding risk-envelope.
In this paper  we assume that the risk envelope U(Pθ) is given in a canonical convex programming
formulation  and satisﬁes the following conditions.
Assumption 2.2 (The General Form of Risk Envelope). For each given policy parameter θ ∈ RK 
(cid:88)
the risk envelope U of a coherent risk measure can be written as
ξPθ : ge(ξ  Pθ) = 0  ∀e ∈ E  fi(ξ  Pθ) ≤ 0  ∀i ∈ I 
U(Pθ) =

ξ(ω)Pθ(ω) = 1  ξ(ω) ≥ 0

(cid:27)

(cid:26)

(2)

 

where each constraint ge(ξ  Pθ) is an afﬁne function in ξ  each constraint fi(ξ  Pθ) is a convex
function in ξ  and there exists a strictly feasible point ξ. E and I here denote the sets of equality
and inequality constraints  respectively. Furthermore  for any given ξ ∈ B  fi(ξ  p) and ge(ξ  p) are
twice differentiable in p  and there exists a M > 0 such that

Assumption 2.2 implies that the risk envelope U(Pθ) is known in an explicit form. From Theorem
6.6 of [26]  in the case of a ﬁnite probability space  ρ is a coherent risk if and only if U(Pθ) is a
convex and compact set. This justiﬁes the afﬁne assumption of ge and the convex assumption of fi.
Moreover  the additional assumption on the smoothness of the constraints holds for many popular
coherent risk measures  such as the CVaR  the mean-semi-deviation  and spectral risk measures [1].
2.2 Dynamic Risk Measures
The risk measures deﬁned above do not take into account any temporal structure that the random
variable might have  such as when it is associated with the return of a trajectory in the case of
MDPs. In this sense  such risk measures are called static. Dynamic risk measures  on the other hand 

policies on a state space augmented with accumulated cost. The latter has shown to be sufﬁcient for optimizing
the CVaR risk [4].

3When we study risk in MDPs  the risk envelope U(Pθ) in Eq. 1 also depends on the state x.

3

ω∈Ω

(cid:26)

max

max
i∈I

(cid:12)(cid:12)(cid:12)(cid:12) dfi(ξ  p)

dp(ω)

(cid:12)(cid:12)(cid:12)(cid:12)   max

e∈E

(cid:12)(cid:12)(cid:12)(cid:12) dge(ξ  p)

dp(ω)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:27)

≤ M  ∀ω ∈ Ω.

explicitly take into account the temporal nature of the stochastic outcome. A primary motivation for
considering such measures is the issue of time consistency  usually deﬁned as follows [24]: if a
certain outcome is considered less risky in all states of the world at stage t + 1  then it should also
be considered less risky at stage t. Example 2.1 in [13] shows the importance of time consistency
in the evaluation of risk in a dynamic setting. It illustrates that for multi-period decision-making 
optimizing a static measure can lead to “time-inconsistent” behavior. Similar paradoxical results
could be obtained with other risk metrics; we refer the readers to [24] and [13] for further insights.
Markov Coherent Risk Measures. Markov risk measures were introduced in [24] and constitute
a useful class of dynamic time-consistent risk measures that are important to our study of risk in
MDPs. For a T -length horizon and MDP M  the Markov coherent risk measure ρT (M) is

(cid:32)

C(xT−1) + γρ(cid:0)C(xT )(cid:1)(cid:17)(cid:33)
(cid:16)

 

ρT (M) = C(x0) + γρ

C(x1) + . . . + γρ

(3)

ρ at state x ∈ X is induced by the transition probability Pθ(·|x) =(cid:80)

where ρ is a static coherent risk measure that satisﬁes Assumption 2.2 and x0  . . .   xT is a trajectory
drawn from the MDP M under policy µθ. It is important to note that in (3)  each static coherent risk
a∈A P (x(cid:48)|x  a)µθ(a|x). We
also deﬁne ρ∞(M)
= limT→∞ ρT (M)  which is well-deﬁned since γ < 1 and the cost is bounded.
.
We further assume that ρ in (3) is a Markov risk measure  i.e.  the evaluation of each static coherent
risk measure ρ is not allowed to depend on the whole past.
3 Problem Formulation
In this paper  we are interested in solving two risk-sensitive optimization problems. Given a random
variable Z and a static coherent risk measure ρ as deﬁned in Section 2  the static risk problem (SRP)
is given by

For example  in an RL setting  Z may correspond to the cumulative discounted cost Z = C(x0) +
γC(x1) + ··· + γT C(xT ) of a trajectory induced by an MDP with a policy parameterized by θ.
For an MDP M and a dynamic Markov coherent risk measure ρT as deﬁned by Eq. 3  the dynamic
risk problem (DRP) is given by

Except for very limited cases  there is no reason to hope that neither the SRP in (4) nor the DRP
in (5) should be tractable problems  since the dependence of the risk measure on θ may be complex
and non-convex. In this work  we aim towards a more modest goal and search for a locally optimal
θ. Thus  the main problem that we are trying to solve in this paper is how to calculate the gradients
of the SRP’s and DRP’s objective functions

∇θρ(Z)

and

∇θρ∞(M).

We are interested in non-trivial cases in which the gradients cannot be calculated analytically. In
the static case  this would correspond to a non-trivial dependence of Z on θ. For dynamic risk  we
also consider cases where the state space is too large for a tractable computation. Our approach for
dealing with such difﬁcult cases is through sampling. We assume that in the static case  we may
obtain i.i.d. samples of the random variable Z. For the dynamic case  we assume that for each state
and action (x  a) of the MDP  we may obtain i.i.d. samples of the next state x(cid:48) ∼ P (·|x  a). We
show that sampling may indeed be used in both cases to devise suitable estimators for the gradients.
To ﬁnally solve the SRP and DRP problems  a gradient estimate may be plugged into a standard
stochastic gradient descent (SGD) algorithm for learning a locally optimal solution to (4) and (5).
From the structure of the dynamic risk in Eq. 3  one may think that a gradient estimator for ρ(Z) may
help us to estimate the gradient ∇θρ∞(M). Indeed  we follow this idea and begin with estimating
the gradient in the static risk case.
4 Gradient Formula for Static Risk
In this section  we consider a static coherent risk measure ρ(Z) and propose sampling-based es-
timators for ∇θρ(Z). We make the following assumption on the policy parametrization  which is
standard in the policy gradient literature [15].
Assumption 4.1. The likelihood ratio ∇θ log P (ω) is well-deﬁned and bounded for all ω∈ Ω.

4

min

θ

ρ(Z).

ρ∞(M).

min

θ

(4)

(5)

(cid:88)

ω∈Ω

(cid:32)(cid:88)

ω∈Ω

Moreover  our approach implicitly assumes that given some ω ∈ Ω  ∇θ log P (ω) may be easily
calculated. This is also a standard requirement for policy gradient algorithms [15] and is satisﬁed
in various applications such as queueing systems  inventory management  and ﬁnancial engineering
(see  e.g.  the survey by Fu [11]).
Using Theorem 2.1 and Assumption 2.2  for each θ  we have that ρ(Z) is the solution to the con-
vex optimization problem (1) (for that value of θ). The Lagrangian function of (1)  denoted by
Lθ(ξ  λP   λE   λI)  may be written as

Lθ(ξ  λ

P
  λ

E
  λ

I

) =

ξ(ω)Pθ(ω)Z(ω)−λ

P

ξ(ω)Pθ(ω)−1

I

λ

(i)fi(ξ Pθ).

(cid:33)
−(cid:88)

e∈E

(e)ge(ξ Pθ)−(cid:88)

E

λ

i∈I

(cid:104)∇θ log P (ω)(Z − λ

(6)
The convexity of (1) and its strict feasibility due to Assumption 2.2 implies that Lθ(ξ  λP   λE   λI)
has a non-empty set of saddle points S. The next theorem presents a formula for the gradient
∇θρ(Z). As we shall subsequently show  this formula is particularly convenient for devising sam-
pling based estimators for ∇θρ(Z).
) ∈ S
Theorem 4.2. Let Assumptions 2.2 and 4.1 hold. For any saddle point (ξ∗
of (6)  we have
∇θρ(Z) = Eξ∗
The proof of this theorem  given in the supplementary material  involves an application of the Enve-
lope theorem [17] and a standard ‘likelihood-ratio’ trick. We now demonstrate the utility of Theorem
4.2 with several examples in which we show that it generalizes previously known results  and also
enables deriving new useful gradient formulas.
4.1 Example 1: CVaR
The CVaR at level α ∈ [0  1] of a random variable Z  denoted by ρCVaR(Z; α)  is a very popular
coherent risk measure [23]  deﬁned as

θ ; Pθ) −(cid:88)

(cid:105) −(cid:88)

(e)∇θge(ξ∗

(i)∇θfi(ξ∗

∗ P
θ   λ
θ

∗ I
  λ
θ

∗ E
  λ
θ

∗ E
λ
θ

θ ; Pθ).

∗ P
θ

∗ I
θ

e∈E

i∈I

λ

)

θ

(cid:8)t + α−1E [(Z − t)+](cid:9).

ρCVaR(Z; α)

.
= inf
t∈R

When Z is continuous  ρCVaR(Z; α) is well-known to be the mean of the α-tail distribution of Z 
E [ Z| Z > qα]  where qα is a (1 − α)-quantile of Z. Thus  selecting a small α makes CVaR partic-
ularly sensitive to rare  but very high costs.
The

ω∈Ω ξ(ω)Pθ(ω) = 1(cid:9). Furthermore  [26] show that the saddle points of (6) satisfy

[0  α−1]  (cid:80)

= (cid:8)ξPθ

for CVaR is known to be

risk envelope

[26] U

ξ(ω)

∈

:

∗ P
ξ∗
θ (ω) = α−1 when Z(ω) > λ
  where λ
θ
quantile of Z. Plugging this result into Theorem 4.2  we can easily show that
∇θρCVaR(Z; α) = E [∇θ log P (ω)(Z − qα)| Z(ω) > qα] .

θ (ω) = 0 when Z(ω) < λ

  and ξ∗

∗ P
θ

∗ P
θ

is any (1 − α)-

This formula was recently proved in [30] for the case of continuous distributions by an explicit
calculation of the conditional expectation  and under several additional smoothness assumptions.
Here we show that it holds regardless of these assumptions and in the discrete case as well. Our
proof is also considerably simpler.
4.2 Example 2: Mean-Semideviation
The semi-deviation of a random variable Z is deﬁned as SD[Z]
semi-deviation captures the variation of the cost only above its mean  and is an appealing alternative
to the standard deviation  which does not distinguish between the variability of upside and downside
deviations. For some α ∈ [0  1]  the mean-semideviation risk measure is deﬁned as ρMSD(Z; α)
.
=
E [Z] + αSD[Z]  and is a coherent risk measure [26]. We have the following result:
Proposition 4.3. Under Assumption 4.1  with ∇θE [Z] = E [∇θ log P (ω)Z]  we have

= (cid:0)E(cid:2)(Z − E [Z])2

(cid:3)(cid:1)1/2. The

+

.

∇θρMSD(Z; α) = ∇θE [Z] +

αE [(Z−E [Z])+(∇θ log P (ω)(Z−E [Z])−∇θE [Z])]

.

This proposition can be used to devise a sampling based estimator for ∇θρMSD(Z; α) by replacing
all the expectations with sample averages. The algorithm along with the proof of the proposition are
in the supplementary material. In Section 6 we provide a numerical illustration of optimization with
a mean-semideviation objective.

SD(Z)

5

4.3 General Gradient Estimation Algorithm
In the two previous examples  we obtained a gradient formula by analytically calculating the La-
grangian saddle point (6) and plugging it into the formula of Theorem 4.2. We now consider a
general coherent risk ρ(Z) for which  in contrast to the CVaR and mean-semideviation cases  the
Lagrangian saddle-point is not known analytically. We only assume that we know the structure of the
risk-envelope as given by (2). We show that in this case  ∇θρ(Z) may be estimated using a sample
average approximation (SAA; [26]) of the formula in Theorem 4.2.
Assume that we are given N i.i.d. samples ωi ∼ Pθ  i = 1  . . .   N  and let Pθ;N (ω)
.
=
I{ωi = ω} denote the corresponding empirical distribution. Also  let the sample risk en-
velope U(Pθ;N ) be deﬁned according to Eq. 2 with Pθ replaced by Pθ;N . Consider the following
SAA version of the optimization in Eq. 1:
max

(cid:80)N

Pθ;N (ωi)ξ(ωi)Z(ωi).

(cid:88)

ρN (Z) =

(7)

1
N

i=1

ξ:ξPθ;N∈U (Pθ;N )

i∈1 ... N

Note that (7) deﬁnes a convex optimization problem with O(N ) variables and constraints.
In
the following  we assume that a solution to (7) may be computed efﬁciently using standard con-
vex programming tools such as interior point methods [7]. Let ξ∗
θ;N denote a solution to (7) and
∗ P
∗ I
θ;N denote the corresponding KKT multipliers  which can be obtained from the con-
λ
θ;N   λ
vex programming algorithm [7]. We propose the following estimator for the gradient-based on
Theorem 4.2:

∗ E
θ;N   λ

∇θ;N ρ(Z) =

N(cid:88)
−(cid:88)

i=1

e∈E

θ;N (ωi)∇θ log P (ωi)(Z(ωi) − λ

Pθ;N (ωi)ξ∗
∗ E
θ;N (e)∇θge(ξ∗

θ;N ; Pθ;N ) −(cid:88)

∗ P
θ;N )
∗ I
θ;N (i)∇θfi(ξ∗

λ

λ

θ;N ; Pθ;N ).

i∈I

(8)

Thus  our gradient estimation algorithm is a two-step procedure involving both sampling and convex
programming. In the following  we show that under some conditions on the set U(Pθ)  ∇θ;N ρ(Z)
is a consistent estimator of ∇θρ(Z). The proof has been reported in the supplementary material.
Proposition 4.4. Let Assumptions 2.2 and 4.1 hold. Suppose there exists a compact set C = Cξ×Cλ
such that: (I) The set of Lagrangian saddle points S ⊂ C is non-empty and bounded. (II) The
functions fe(ξ  Pθ) for all e ∈ E and fi(ξ  Pθ) for all i ∈ I are ﬁnite-valued and continuous (in ξ)
on Cξ. (III) For N large enough  the set SN is non-empty and SN ⊂ C w.p. 1. Further assume that:
(IV) If ξN Pθ;N ∈ U(Pθ;N ) and ξN converges w.p. 1 to a point ξ  then ξPθ ∈ U(Pθ). We then have
that limN→∞ ρN (Z) = ρ(Z) and limN→∞ ∇θ;N ρ(Z) = ∇θρ(Z) w.p. 1.
The set of assumptions for Proposition 4.4 is large  but rather mild. Note that (I) is implied by
the Slater condition of Assumption 2.2. For satisfying (III)  we need that the risk be well-deﬁned
for every empirical distribution  which is a natural requirement. Since Pθ;N always converges to Pθ
uniformly on Ω  (IV) essentially requires smoothness of the constraints. We remark that in particular 
constraints (I) to (IV) are satisﬁed for the popular CVaR  mean-semideviation  and spectral risk.
It is interesting to compare the performance of the SAA estimator (8) with the analytical-solution
based estimator  as in Sections 4.1 and 4.2. In the supplementary material  we report an empirical
comparison between the two approaches for the case of CVaR risk  which showed that the two
√
approaches performed very similarly. This is well-expected  since in general  both SAA and standard
N [26].
likelihood-ratio based estimators obey a law-of-large-numbers variance bound of order 1/
To summarize this section  we have seen that by exploiting the special structure of coherent risk
measures in Theorem 2.1 and by the envelope-theorem style result of Theorem 4.2  we are able to
derive sampling-based  likelihood-ratio style algorithms for estimating the policy gradient ∇θρ(Z)
of coherent static risk measures. The gradient estimation algorithms developed here for static risk
measures will be used as a sub-routine in our subsequent treatment of dynamic risk measures.
5 Gradient Formula for Dynamic Risk
In this section  we derive a new formula for the gradient of the Markov coherent dynamic risk mea-
sure  ∇θρ∞(M). Our approach is based on combining the static gradient formula of Theorem 4.2 
with a dynamic-programming decomposition of ρ∞(M).

6

The risk-sensitive value-function for an MDP M under the policy θ is deﬁned as Vθ(x) =
ρ∞(M|x0 = x)  where with a slight abuse of notation  ρ∞(M|x0 = x) denotes the Markov-
coherent dynamic risk in (3) when the initial state x0 is x. It is shown in [24] that due to the structure
of the Markov dynamic risk ρ∞(M)  the value function is the unique solution to the risk-sensitive
Bellman equation

Vθ(x) = C(x) + γ

max

ξPθ(·|x)∈U (x Pθ(·|x))

Eξ[Vθ(x(cid:48))] 

(9)

where the expectation is taken over the next state transition. Note that by deﬁnition  we have
ρ∞(M) = Vθ(x0)  and thus  ∇θρ∞(M) = ∇θVθ(x0).
We now develop a formula for ∇θVθ(x); this formula extends the well-known “policy gradient
theorem” [28  14]  developed for the expected return  to Markov-coherent dynamic risk measures.
We make a standard assumption  analogous to Assumption 4.1 of the static case.
Assumption 5.1. The likelihood ratio ∇θ log µθ(a|x) is well-deﬁned and bounded for all x ∈ X
and a ∈ A.
∗ I
For each state x ∈ X   let (ξ∗
θ x ) denote a saddle point of (6)  corresponding to the
state x  with Pθ(·|x) replacing Pθ in (6) and Vθ replacing Z. The next theorem presents a formula
for ∇θVθ(x); the proof is in the supplementary material.
Theorem 5.2. Under Assumptions 2.2 and 5.1  we have

∗ P
θ x   λ

∗ E
θ x  λ

θ x  λ

∇Vθ(x) = Eξ∗

θ

(cid:35)

(cid:34) ∞(cid:88)

γt∇θ log µθ(at|xt)hθ(xt  at)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x0 = x
θ x(·)  and the stage-wise cost function hθ(x  a) is deﬁned as
−(cid:88)

θ x −(cid:88)

∗ I
θ x (i)
λ

(cid:48)|x  a)ξ

(cid:48)
γVθ(x

dfi(ξ∗

∗
(cid:48)
θ x(x

)

θ x  p)

)−λ

∗ P

(cid:34)

t=0

 

i∈I

dp(x(cid:48))

e∈E

θ

where Eξ∗
probabilities Pθ(·|x)ξ∗
(cid:88)

hθ(x  a) = C(x)+

P (x

x(cid:48)∈X

[·] denotes the expectation w.r.t. trajectories generated by the Markov chain with transition

(cid:35)

.

∗ E
θ x (e)

λ

dge(ξ∗

θ x  p)

dp(x(cid:48))

Theorem 5.2 may be used to develop an actor-critic style [28  14] sampling-based algorithm for
solving the DRP problem (5)  composed of two interleaved procedures:
Critic: For a given policy θ  calculate the risk-sensitive value function Vθ  and
Actor: Using the critic’s Vθ and Theorem 5.2  estimate ∇θρ∞(M) and update θ.
Space limitation restricts us from specifying the full details of our actor-critic algorithm and its
analysis. In the following  we highlight only the key ideas and results. For the full details  we refer
the reader to the full paper version  provided in the supplementary material.
For the critic  the main challenge is calculating the value function when the state space X is large
and dynamic programming cannot be applied due to the ‘curse of dimensionality’. To overcome
this  we exploit the fact that Vθ is equivalent to the value function in a robust MDP [19] and modify
a recent algorithm in [31] to estimate it using function approximation.
For the actor  the main challenge is that in order to estimate the gradient using Thm. 5.2  we need to
sample from an MDP with ξ∗
θ -weighted transitions. Also  hθ(x  a) involves an expectation for each
s and a. Therefore  we propose a two-phase sampling procedure to estimate ∇Vθ in which we ﬁrst
use the critic’s estimate of Vθ to derive ξ∗
θ -weighted
transitions. For each state in the trajectory  we then sample several next states to estimate hθ(x  a).
The convergence analysis of the actor-critic algorithm and the gradient error incurred from function
approximation of Vθ are reported in the supplementary material. We remark that our actor-critic
algorithm requires a simulator for sampling multiple state-transitions from each state. Extending
our approach to work with a single trajectory roll-out is an interesting direction for future research.
6 Numerical Illustration
In this section  we illustrate our approach with a numerical example. The purpose of this illustration
is to emphasize the importance of ﬂexibility in designing risk criteria for selecting an appropriate
risk-measure – such that suits both the user’s risk preference and the problem-speciﬁc properties.
We consider a trading agent that can invest in one of three assets (see Figure 1 for their distributions).
The returns of the ﬁrst two assets  A1 and A2  are normally distributed: A1 ∼ N (1  1) and A2 ∼

θ   and sample a trajectory from an MDP with ξ∗

7

Figure 1: Numerical illustration - selection between 3 assets. A: Probability density of asset return.
B C D: Bar plots of the probability of selecting each asset vs. training iterations  for policies π1  π2 
and π3  respectively. At each iteration  10 000 samples were used for gradient estimation.
zα+1 ∀z > 1  with α =
N (4  6). The return of the third asset A3 has a Pareto distribution: f (z) = α
1.5. The mean of the return from A3 is 3 and its variance is inﬁnite; such heavy-tailed distributions
are widely used in ﬁnancial modeling [22]. The agent selects an action randomly  with probability
P (Ai) ∝ exp(θi)  where θ ∈ R3 is the policy parameter. We trained three different policies π1  π2 
and π3. Policy π1 is risk-neutral  i.e.  maxθ E [Z]  and it was trained using standard policy gradient
[15]. Policy π2 is risk-averse and had a mean-semideviation objective maxθ E [Z] − SD[Z]  and
was trained using the algorithm in Section 4. Policy π3 is also risk-averse  with a mean-standard-

deviation objective  as proposed in [29  21]  maxθ E [Z] −(cid:112)Var[Z]  and was trained using the

algorithm of [29]. For each of these policies  Figure 1 shows the probability of selecting each asset
vs. training iterations. Although A2 has the highest mean return  the risk-averse policy π2 chooses
A3  since it has a lower downside  as expected. However  because of the heavy upper-tail of A3 
policy π3 opted to choose A1 instead. This is counter-intuitive as a rational investor should not avert
high returns. In fact  in this case A3 stochastically dominates A1 [12].
7 Conclusion
We presented algorithms for estimating the gradient of both static and dynamic coherent risk mea-
sures using two new policy gradient style formulas that combine sampling with convex program-
ming. Thereby  our approach extends risk-sensitive RL to the whole class of coherent risk measures 
and generalizes several recent studies that focused on speciﬁc risk measures.
On the technical side  an important future direction is to improve the convergence rate of gradient
estimates using importance sampling methods. This is especially important for risk criteria that are
sensitive to rare events  such as the CVaR [3].
From a more conceptual point of view  the coherent-risk framework explored in this work provides
the decision maker with ﬂexibility in designing risk preference. As our numerical example shows 
such ﬂexibility is important for selecting appropriate problem-speciﬁc risk measures for managing
the cost variability. However  we believe that our approach has much more potential than that.
In almost every real-world application  uncertainty emanates from stochastic dynamics  but also 
and perhaps more importantly  from modeling errors (model uncertainty). A prudent policy should
protect against both types of uncertainties. The representation duality of coherent-risk (Theorem
2.1)  naturally relates the risk to model uncertainty. In [19]  a similar connection was made between
model-uncertainty in MDPs and dynamic Markov coherent risk. We believe that by carefully shap-
ing the risk-criterion  the decision maker may be able to take uncertainty into account in a broad
sense. Designing a principled procedure for such risk-shaping is not trivial  and is beyond the scope
of this paper. However  we believe that there is much potential to risk shaping as it may be the key
for handling model misspeciﬁcation in dynamic decision making.

Acknowledgments

The research leading to these results has received funding from the European Research Council
under the European Unions Seventh Framework Program (FP7/2007-2013) / ERC Grant Agreement
n. 306638. Yinlam Chow is partially supported by Croucher Foundation Doctoral Scholarship.

8

References
[1] C. Acerbi. Spectral measures of risk: a coherent representation of subjective risk aversion. Journal of

Banking & Finance  26(7):1505–1518  2002.

[2] P. Artzner  F. Delbaen  J. Eber  and D. Heath. Coherent measures of risk. Mathematical ﬁnance  9(3):203–

228  1999.

[3] O. Bardou  N. Frikha  and G. Pag`es. Computing VaR and CVaR using stochastic approximation and
adaptive unconstrained importance sampling. Monte Carlo Methods and Applications  15(3):173–210 
2009.

[4] N. B¨auerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Mathematical

Methods of Operations Research  74(3):361–379  2011.

[5] D. Bertsekas. Dynamic programming and optimal control. Athena Scientiﬁc  4th edition  2012.
[6] V. Borkar. A sensitivity formula for risk-sensitive cost and the actor–critic algorithm. Systems & Control

Letters  44(5):339–346  2001.

[7] S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press  2009.
[8] Y. Chow and M. Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In NIPS 27  2014.
[9] Y. Chow and M. Pavone. A unifying framework for time-consistent  risk-averse model predictive control:

theory and algorithms. In American Control Conference  2014.

[10] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncer-

tainty. Operations Research  58(1):203213  2010.

[11] M. Fu. Gradient estimation. In Simulation  volume 13 of Handbooks in Operations Research and Man-

agement Science  pages 575 – 616. Elsevier  2006.

[12] J. Hadar and W. R. Russell. Rules for ordering uncertain prospects. The American Economic Review 

pages 25–34  1969.

[13] D. Iancu  M. Petrik  and D. Subramanian.

arXiv:1106.6102  2011.

Tight approximations of dynamic risk measures.

[14] V. Konda and J. Tsitsiklis. Actor-critic algorithms. In NIPS  2000.
[15] P. Marbach and J. Tsitsiklis. Simulation-based optimization of Markov reward processes. IEEE Transac-

tions on Automatic Control  46(2):191–209  1998.

[16] H. Markowitz. Portfolio selection: Efﬁcient diversiﬁcation of investment. John Wiley and Sons  1959.
[17] P. Milgrom and I. Segal. Envelope theorems for arbitrary choice sets. Econometrica  70(2):583–601 

2002.

[18] J. Moody and M. Saffell. Learning to trade via direct reinforcement. Neural Networks  IEEE Transactions

on  12(4):875–889  2001.

[19] T. Osogami. Robustness and risk-sensitivity in Markov decision processes. In NIPS  2012.
[20] M. Petrik and D. Subramanian. An approximate solution method for large risk-averse Markov decision

processes. In UAI  2012.

[21] L. Prashanth and M. Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In NIPS 26  2013.
[22] S. Rachev and S. Mittnik. Stable Paretian models in ﬁnance. John Willey & Sons  New York  2000.
[23] R. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of risk  2:21–42  2000.
[24] A. Ruszczy´nski. Risk-averse dynamic programming for Markov decision processes. Mathematical Pro-

gramming  125(2):235–261  2010.

[25] A. Ruszczy´nski and A. Shapiro. Optimization of convex risk functions. Math. OR  31(3):433–452  2006.
[26] A. Shapiro  D. Dentcheva  and A. Ruszczy´nski. Lectures on stochastic programming  chapter 6  pages

253–332. SIAM  2009.

[27] R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge Univ Press  1998.
[28] R. Sutton  D. McAllester  S. Singh  and Y. Mansour. Policy gradient methods for reinforcement learning

with function approximation. In NIPS 13  2000.

[29] A. Tamar  D. Di Castro  and S. Mannor. Policy gradients with variance related risk criteria. In Interna-

tional Conference on Machine Learning  2012.

[30] A. Tamar  Y. Glassner  and S. Mannor. Optimizing the CVaR via sampling. In AAAI  2015.
[31] A. Tamar  S. Mannor  and H. Xu. Scaling up robust MDPs using function approximation. In International

Conference on Machine Learning  2014.

9

,Matthew Lawlor
Steven Zucker
Aviv Tamar
Yinlam Chow
Mohammad Ghavamzadeh
Shie Mannor
Patrick Rebeschini
Sekhar Tatikonda