2019,Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods,Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper  we study the problem in the non-convex regime and show that an $\varepsilon$--first order stationary point of the game can be computed when one of the player’s objective can be optimized to global optimality efficiently. In particular  we first consider the case where the objective of one of the players satisfies the Polyak-{\L}ojasiewicz (PL) condition. For such a game  we show that a simple multi-step gradient descent-ascent algorithm finds an $\varepsilon$--first order stationary point of the problem in $\widetilde{\mathcal{O}}(\varepsilon^{-2})$ iterations. Then we show that our framework can also be applied to the case where the objective of the ``max-player" is concave. In this case  we propose a multi-step gradient descent-ascent algorithm that finds an $\varepsilon$--first order stationary point of the game in $\widetilde{\cal O}(\varepsilon^{-3.5})$ iterations  which is the best known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization.,Solving a Class of Non-Convex Min-Max Games

Using Iterative First Order Methods

Maher Nouiehed
nouiehed@usc.edu ∗

Maziar Sanjabi
sanjabi@usc.edu †

Tianjian Huang
tianjian@usc.edu ‡

Jason D. Lee

jasonlee@princeton.edu §

Meisam Razaviyayn
razaviya@usc.edu ¶

Abstract

Recent applications that arise in machine learning have surged signiﬁcant interest
in solving min-max saddle point games. This problem has been extensively studied
in the convex-concave regime for which a global equilibrium solution can be
computed efﬁciently. In this paper  we study the problem in the non-convex regime
and show that an ε–ﬁrst order stationary point of the game can be computed when
one of the player’s objective can be optimized to global optimality efﬁciently. In
particular  we ﬁrst consider the case where the objective of one of the players
satisﬁes the Polyak-Łojasiewicz (PL) condition. For such a game  we show that a
simple multi-step gradient descent-ascent algorithm ﬁnds an ε–ﬁrst order stationary

point of the problem in (cid:101)O(ε−2) iterations. Then we show that our framework can
an ε–ﬁrst order stationary point of the game in (cid:101)O(ε−3.5) iterations  which is the

also be applied to the case where the objective of the “max-player" is concave.
In this case  we propose a multi-step gradient descent-ascent algorithm that ﬁnds

best known rate in the literature. We applied our algorithm to a fair classiﬁcation
problem of Fashion-MNIST dataset and observed that the proposed algorithm
results in smoother training and better generalization.

1

Introduction

Recent years have witnessed a wide range of machine learning and robust optimization applications
being formulated as a min-max saddle point game; see [51  11  10  50  20  53] and the references
therein. Examples of problems that are formulated under this framework include generative ad-
versarial networks (GANs) [51]  reinforcement learning [11]  adversarial learning [53]  learning
exponential families [10]  fair statistical inference [17  56  52  37]  generative adversarial imitation
learning [6  27]  distributed non-convex optimization [35] and many others. These applications
require solving an optimization problem of the form

min
θ∈Θ

max
α∈A f (θ  α).

(1)

This optimization problem can be viewed as a zero-sum game between two players. The goal of the
ﬁrst player is to minimize f (θ  α) by tuning θ  while the other player’s objective is to maximize

∗Department of Industrial and Systems Engineering  University of Southern California
†Data Science and Operations Department  Marshall School of Business  University of Southern California
‡Department of Industrial and Systems Engineering  University of Southern California
§Department of Electrical Engineering  Princeton University
¶Department of Industrial and Systems Engineering  University of Southern California

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

f (θ  α) by tuning α. Gradient-based methods  especially gradient descent-ascent (GDA)  are widely
used in practice to solve these problems. GDA alternates between a gradient ascent steps on α and a
gradient descent steps on θ. Despite its popularity  this algorithm fails to converge even for simple
bilinear zero-sum games [41  39  14  2  32]. This failure was ﬁxed by adding negative momentum or
by using primal-dual methods proposed by [22  21  8  13  15  33].
When the objective f is convex in θ and concave in α  the corresponding variational inequality
becomes monotone. This setting has been extensively studied and different algorithms have been
developed for ﬁnding a Nash equilibrium [46  21  44  29  40  23  26  43  18  45]. Moreover  [12]
proposed an algorithm for solving a more general setting that covers both monotone and psuedo-
monotone variational problems.
While the convex-concave setting has been extensively studied in the literature  recent machine
learning applications urge the necessity of moving beyond these classical settings. For example  in
a typical GAN problem formulation  two neural networks (generator and discriminator) compete
in a non-convex zero-sum game framework [24]. For general non-convex non-concave games  [28 
Proposition 10] provides an example for which local Nash equilibrium does not exist. Similarly 
one can show that even second-order Nash equilibrium may not exist for non-convex games  see
Section 2 for more details. Therefore  a well-justiﬁed objective is to ﬁnd ﬁrst order Nash equilibria
of such games [48]; see deﬁnitions and discussion in Section 2. The ﬁrst order Nash equilibrium
can be viewed as a direct extension of the concept of ﬁrst order stationarity in optimization to the
above min-max game. While ε–ﬁrst order stationarity in the context of optimization can be found
efﬁciently in O(ε−2) iterations with gradient descent algorithm [47]  the question of whether it is
possible to design a gradient-based algorithm that can ﬁnd an ε–ﬁrst order Nash equilibrium for
general non-convex saddle point games remains open.
Several recent results provided a partial answer to the problem of ﬁnding ﬁrst-order stationary points
of a non-convex min-max game. For instance  [51] proposed a stochastic gradient descent algorithm
for the case when f (· ·) is strongly concave in α and showed convergence of the algorithm to an
gradient descent algorithm with Max-oracle and shows O(−4) gradient evaluations and max-oracle
calls for solving min-max problems where the inner problem can be solved in one iteration using an
existing oracle. More recently  [35  36] considered the case where f is concave in α. They developed

ε–ﬁrst-order Nash equilibrium with (cid:101)O(ε−2) gradient evaluations. Also  the work [28] analyzes the
a descent-ascent algorithm with iteration complexity (cid:101)O(ε−4). In this non-convex concave setting 
[50] proposed a stochastic sub-gradient descent method with worst-case complexity (cid:101)O(ε−6). Under
that ﬁnds an ε–ﬁrst order Nash equilibrium/stationary with (cid:101)O(ε−3.5) gradient evaluations.

the same concavity assumption on f  in this paper  we propose an alternative multi-step framework

In an effort to solve the more general non-convex non-concave setting  [34] developed a framework
that converges to ε-ﬁrst order stationarity/Nash equilibrium under the assumption that there exists a
solution to the Minty variational inequality at each iteration. Although among the ﬁrst algorithms
with have theoretical convergence guarantees in the non-convex non-concave setting  the conditions
required are strong and difﬁcult to check. To the best of our knowledge  there is no practical
problem for which the Minty variational inequality condition has been proven. With the motivation of
exploring the non-convex non-concave setting  we propose a simple multi-step gradient descent ascent
algorithm for the case where the objective of one of the players satisﬁes the Polyak-Łojasiewicz (PL)

condition. We show the worst-case complexity of (cid:101)O(ε−2) for our algorithm. This rate is optimal in

terms of dependence on ε up to logarithmic factors as discussed in Section 3. Compared to Minty
variational inequality condition used in [34]  the PL condition is very well studied in the literature
and has been theoretically veriﬁed for objectives of optimization problems arising in many practical
problems. For example  it has been proven to be true for objectives of over-parameterized deep
networks [16]  learning LQR models [19]  phase retrieval [54]  and many other simple problems
discussed in [30]. In the context of min-max games  it has also been proven useful in generative
adversarial imitation learning with LQR dynamics [6]  as discussed in Section 3.
The rest of this paper is organized as follows. In Section 2 we deﬁne the concepts of First-order Nash
equilibrium (FNE) and ε–FNE. In Section 3  we describe our algorithm designed for min-max games
with the objective of one player satisfying the PL condition. Finally  in Section 4 we describe our
method for solving games in which the function f (θ  α) is concave in α (or convex in θ).

2

2 Two-player Min-Max Games and First-Order Nash Equilibrium

Consider the two-player zero sum min-max game
max
α∈A f (θ  α) 

(2)
where Θ and A are both convex sets  and f (θ  α) is a continuously differentiable function. We say
(θ∗  α∗) ∈ Θ × A is a Nash equilibrium of the game if

min
θ∈Θ

f (θ∗  α) ≤ f (θ∗  α∗) ≤ f (θ  α∗) ∀ θ ∈ Θ  ∀ α ∈ A.

In convex-concave games  such a Nash equilibrium always exists [28] and several algorithms were
proposed to ﬁnd Nash equilibria [23  26]. However  in the non-convex non-concave regime  comput-
ing these points is in general NP-Hard. In fact  even ﬁnding local Nash equilibria is NP-hard in the
general non-convex non-concave regime.In addition  as shown by [28  Proposition 10]  local Nash
equilibria for general non-convex non-concave games may not exist. Thus  in this paper we aim for
the less ambitious goal of ﬁnding ﬁrst-order Nash equilibrium which is deﬁned in the sequel.
Deﬁnition 2.1 (FNE). A point (θ∗  α∗) ∈ Θ × A is a ﬁrst order Nash equilibrium (FNE) of the
game (2) if

(cid:104)∇θf (θ∗  α∗)  θ − θ∗(cid:105) ≥ 0 ∀ θ ∈ Θ and (cid:104)∇αf (θ∗  α∗)  α − α∗(cid:105) ≤ 0 ∀ α ∈ A.

(3)

Notice that this deﬁnition  which is also used in [48  49]  contains the ﬁrst order necessary optimality
conditions of the objective function of each player [5]. Thus they are necessary conditions for
local Nash equilibrium. Moreover  in the absence of constraints  the above deﬁnition simpliﬁes
to ∇θf (θ∗  α∗) = 0 and ∇αf (θ∗  α∗) = 0  which are the well-known unconstrained ﬁrst-order
optimality conditions. Based on this observation  it is tempting to think that the above ﬁrst-order
Nash equilibrium condition does not differentiate between the min-max type solutions of (2) and
min-min solutions of the type minθ∈Θ α∈A f (θ  α). However  the direction of the second inequality
in (3) would be different if we have considered the min-min problem instead of min-max problem.
This different direction makes the problem of ﬁnding a FNE non-trivial. The following theorem
guarantees the existence of ﬁrst-order Nash equilibria under some mild assumptions.
Theorem 2.2 (Restated from Proposition 2 in [48]). Suppose the sets Θ and A are no-empty  compact 
and convex. Moreover  assume that the function f (· ·) is twice continuously differentiable. Then
there exists a feasible point ( ¯θ  ¯α) that is ﬁrst-order Nash equilibrium.

The above theorem guarantees existence of FNE points even when (local) Nash equilibria may not
exist. The next natural question is about the computability of such methods. Since in practice we use
iterative methods for computation  we need to deﬁne the notion of approximate–FNE.
Deﬁnition 2.3 (Approximate FNE). A point (θ∗  α∗) is said to be an ε–ﬁrst-order Nash equilibrium
(ε–FNE) of the game (2) if

X (θ∗  α∗) ≤ ε and Y(θ∗  α∗) ≤ ε 

where

and

X (θ∗  α∗) (cid:44) − min

θ

(cid:104)∇θf (θ∗  α∗)  θ − θ∗(cid:105) s.t. θ ∈ Θ  (cid:107)θ − θ∗(cid:107) ≤ 1 

(4)

α

Y(θ∗  α∗) (cid:44) max

(cid:104)∇αf (θ  α)  α − α∗(cid:105) s.t. α ∈ A  (cid:107)α − α∗(cid:107) ≤ 1.

(5)
In the absence of constraints  ε–FNE in Deﬁnition 2.3 reduces to (cid:107)∇θf (θ∗  α∗)(cid:107) ≤
ε and (cid:107)∇αf (θ∗  α∗)(cid:107) ≤ ε.
Remark 2.4. The ε–FNE deﬁnition above is based on the ﬁrst order optimality measure of the
objective of each player. Such ﬁrst-order optimality measure has been used before in the context of
optimization; see [9]. Such a condition guarantees that each player cannot improve their objective
function using ﬁrst order information. Similar to the optimization setting  one can deﬁne the second-
order Nash equilibrium as a point that each player cannot improve their objective further by using
ﬁrst and second order information of their objectives. However  the use of second order Nash
equilibria is more subtle in the context of games. The following example shows that such a point may
not exist. Consider the game

min−1≤θ≤1

max

−2≤α≤2

−θ2 + α2 + 4θα.

Then (0  0) is the only ﬁrst-order Nash equilibrium and is not a second-order Nash equilibrium.

3

In this paper  our goal is to ﬁnd an ε–FNE of the game (2) using iterative methods. To proceed  we
make the following standard assumptions about the smoothness of the objective function f.
Assumption 2.5. The function f is continuously differentiable in both θ and α and there exists
constants L11  L22 and L12 such that for every α  α1  α2 ∈ A  and θ  θ1  θ2 ∈ Θ  we have

(cid:107)∇θf (θ1  α) − ∇θf (θ2  α)(cid:107) ≤ L11(cid:107)θ1 − θ2(cid:107) 
(cid:107)∇αf (θ1  α) − ∇αf (θ2  α)(cid:107) ≤ L12(cid:107)θ1 − θ2(cid:107) 

(cid:107)∇αf (θ  α1) − ∇αf (θ  α2)(cid:107) ≤ L22(cid:107)α1 − α2(cid:107) 
(cid:107)∇θf (θ  α1) − ∇θf (θ  α2)(cid:107) ≤ L12(cid:107)α1 − α2(cid:107).

3 Non-Convex PL-Game

In this section  we consider the problem of developing an “efﬁcient" algorithm for ﬁnding an ε–FNE
of (2) when the objective of one of the players satistys Polyak-Łojasiewicz (PL) condition. To
proceed  let us ﬁrst formally deﬁne the Polyak-Łojasiewicz (PL) condition.
Deﬁnition 3.1 (Polyak-Łojasiewicz Condition). A differentiable function h(x) with the minimum
value h∗ = minx h(x) is said to be µ-Polyak-Łojasiewicz (µ-PL) if
∀x.

(cid:107)∇h(x)(cid:107)2 ≥ µ(h(x) − h∗) 

(6)

1
2

The PL-condition has been established and utilized for analyzing many practical modern problems
[30  19  16  54  6]. Moreover  it is well-known that a function can be non-convex and still satisfy the
PL condition [30]. Based on the deﬁnition above  we deﬁne a class of min-max PL-games.
Deﬁnition 3.2 (PL-Game). We say that the min-max game (2) is a PL-Game if the max player is
unconstrained  i.e.  A = Rn  and there exists a constant µ > 0 such that the function hθ(α) (cid:44)
−f (θ  α) is µ-PL for any ﬁxed value of θ ∈ Θ.
A simple example of a practical PL-game is detailed next.
Example 3.1 (Generative adversarial imitation learning of linear quadratic regulators). Imitation
learning is a paradigm that aims to learn from an expert’s demonstration of performing a task [6]. It
is known that this learning process can be formulated as a min-max game [27]. In such a game the
minimization is performed over all the policies and the goal is to minimize the discrepancy between the
accumulated reward for expert’s policy and the proposed policy. On the other hand  the maximization
is done over the parameters of the reward function and aims at maximizing this discrepancy over
the parameters of the reward function. This approach is also referred to as generative adversarial
imitation learning (GAIL) [27]. The problem of generative adversarial imitation learning for linear
quadratic regulators [6] refers to solving this problem for the speciﬁc case where the underlying
dynamic and the reward function come from a linear quadratic regulator [19]. To be more speciﬁc 
this problem can be formulated [6] as minK maxθ∈Θ m(K  θ)  where K represents the choice of the
policy and θ represents the parameters of the dynamic and the reward functions. Under the discussed
setting  m is strongly concave in θ and PL in K (see [6] for more details). Note that since m is
strongly concave in θ and P L in K  any FNE of the game would also be a Nash equilibrium point.
Also note that the notion of FNE does not depend on the ordering of the min and max. Thus  to be
consistent with our notion of PL-games  we can formulate the problem as

min
θ∈Θ

max

K

−m(K  θ)

(7)

Thus  generative adversarial imitation learning of linear quadratic regulators is an example of ﬁnding
a FNE for a min-max PL-game.

In what follows  we present a simple iterative method for computing an ε–FNE of PL games.

3.1 Multi-step gradient descent ascent for PL-games

In this section  we propose a multi-step gradient descent ascent algorithm that ﬁnds an ε–FNE point
for PL-games. At each iteration  our method runs multiple projected gradient ascent steps to estimate
the solution of the inner maximization problem. This solution is then used to estimate the gradient of
the inner maximization value function  which directly provides a descent direction. In a nutshell  our
proposed algorithm is a gradient descent-like algorithm on the inner maximization value function. To
present the ideas of our multi-step algorithm  let us re-write (2) as

min
θ∈Θ

g(θ) 

4

(8)

where

g(θ) (cid:44) max

α∈A f (θ  α).

(9)

A famous classical result in optimization is Danskin’s theorem [4] which provides a sufﬁcient
condition under which the gradient of the value function maxα∈A f (θ  α) can be directly evaluated
using the gradient of the objective f (θ  α∗) evaluated at the optimal solution α∗. This result
requires the optimizer α∗ to be unique. Under our PL assumption on f (θ ·)  the inner maximization
problem (9) may have multiple optimal solutions. Hence  Danskin’s theorem does not directly apply.
However  as we will show in Lemma A.5 in the supplementary  under the PL assumption  we still can
show the following result

∇θg(θ) = ∇θf (θ  α∗) with α∗ ∈ arg max
α∈A

f (θ  α) 

despite the non-uniqueness of the optimal solution.
Motivated by this result  we propose a Multi-step Gradient Descent Ascent algorithm that solves the
inner maximization problem to “approximate” the gradient of the value function g. This gradient
direction is then used to descent on θ. More speciﬁcally  the inner loop (Step 4) in Algorithm 1
solves the maximization problem (9) for a given ﬁxed value θ = θt. The computed solution of
this optimization problem provides an approximation for the gradient of the function g(θ)  see
Lemma A.6 in Appendix A. This gradient is then used in Step 7 to descent on θ.

Set α0(θt) = αt
for k = 0 ···   K − 1 do

Algorithm 1 Multi-step Gradient Descent Ascent
1: INPUT: K  T   η1 = 1/L22  η2 = 1/L  α0 ∈ A and θ0 ∈ Θ
2: for t = 0 ···   T − 1 do
3:
4:
5:
6:
7:
8: end for
9: Return (θt  αK(θt)) for t = 0 ···   T − 1.

Set αk+1(θt) = αk(θt) + η1∇αf (θt  αk(θt))

end for
Set θt+1 = projΘ

θt − η2∇θf(θt  αK(θt))

(cid:16)

(cid:17)

3.2 Convergence analysis of Multi-Step Gradient Descent Ascent Algorithm for PL games

Throughout this section  we make the following assumption.
Assumption 3.3. The constraint set Θ is convex and compact. Moreover  there exists a ball with
radius R  denoted by BR  such that Θ ⊆ BR.
We are now ready to state the main result of this section.
Theorem 3.4. Under Assumptions 2.5 and 3.3  for any given scalar ε ∈ (0  1)  if we choose K and
T large enough such that

and K ≥ NK(ε) (cid:44) O(log(cid:0)ε−1)(cid:1) 

T ≥ NT (ε) (cid:44) O(ε−2)

then there exists an iteration t ∈ {0 ···   T} such that (θt  αt+1) is an ε–FNE of (2).

Proof. The proof is relegated to Appendix A.2.
Corollary 3.5. Under Assumption 2.5 and Assumption 3.3  Algorithm 1 ﬁnds an ε-FNE of the
game (2) with O(ε−2) gradient evaluations of the objective with respect to θ and O(ε−2 log(ε−1))
gradient evaluations with respect to α. If the two gradient oracles have the same complexity  the
overall complexity of the method would be O(ε−2 log(ε−1)).
Remark 3.6. The iteration complexity order O(ε−2 log(ε−1)) in Theorem 3.4 is tight (up to log-
arithmic factors). This is due to the fact that for general non-convex smooth problems  ﬁnding an
ε–stationary solution requires at least Ω(ε−2) gradient evaluations [7  47]. Clearly  this lower bound
is also valid for ﬁnding an ε–FNE of PL-games. This is because we can assume that the function
f (θ  α) does not depend on α (and thus PL in α).

5

Remark 3.7. Theorem 3.4 shows that under the PL assumption  the pair (θt  αK(θt)) computed
by Algorithm 1 is an ε–FNE of the game (2). Since αK(θt) is an approximate solution of the inner
maximization problem  we get that θt is concurrently an ε–ﬁrst order stationary solution of the
optimization problem (8).
Remark 3.8. In [51  Theorem 4.2]  a similar result was shown for the case when f (θ  α) is strongly
concave in α. Hence  Theorem 3.4 can be viewed as an extension of [51  Theorem 4.2]. Similar
to [51  Theorem 4.2]  one can easily extend the result of Theorem 3.4 to the stochastic setting by
replacing the gradient of f with respect to θ in Step 7 by the stochastic version of the gradient.

In the next section we consider the non-convex concave min-max saddle game. It is well-known
that convexity/concavity does not imply the PL condition and PL condition does not imply convex-
ity/concavity [30]. Therefore  the problems we consider in the next section are neither restriction nor
extension of our results on PL games.

4 Non-Convex Concave Games

In this section  we focus on “non-convex concave" games satisfying the following assumption:
Assumption 4.1. The objective function f (θ  α) is concave in α for any ﬁxed value of θ. Moreover 
the set A is convex and compact  and there exists a ball with radius R that contains the feasible set A.
One major difference of this case with the PL-games is that in this case the function g(θ) =
maxα∈A f (θ  α) might not be differentiable. To see this  consider the example g(α) =
max0≤α≤1(2α − 1)θ which is concave in α. However  the value function g(θ) = |θ| is non-smooth.
Using a small regularization term  we approximate the function g(·) by a differentiable function

gλ(θ) (cid:44) max

α∈A fλ(θ  α) 

(10)

(cid:107)α − ¯α(cid:107)2. Here ¯α ∈ A is some given ﬁxed point and λ > 0 is
where fλ(θ  α) (cid:44) f (θ  α) − λ
2
a regularization parameter that we will specify later. Since f (θ  α) is concave in α  fλ(θ ·) is λ-
strongly concave. Thus  the function gλ(·) becomes smooth with Lipschitz gradient; see Lemma B.1
in the supplementary. Using this property  we propose an algorithm that runs at each iteration multiple
steps of Nesterov accelerated projected gradient ascent to estimate the solution of (10). This solution
is then used to estimate the gradient of gλ(θ) which directly provides a descent direction on θ.

Our algorithm computes an ε–FNE point for non-convex concave games with (cid:101)O(ε−3.5) gradient

evaluations. Then for sufﬁciently small regularization coefﬁcient  we show that the computed point is
an ε-FNE.
Notice that since fλ is Lipschitz smooth and based on the compactness assumption  we can deﬁne
gθ (cid:44) maxθ∈Θ (cid:107)∇gλ(θ)(cid:107)  gα (cid:44) maxθ∈Θ (cid:107)∇αfλ(θ  α∗(θ))(cid:107)  and gmax = max{gθ  gα  1} 
(11)
where α∗(θ) (cid:44) arg maxα∈A fλ(θ  α). We are now ready to describe our proposed algorithm.

4.1 Algorithm Description

Our proposed method is outlined in Algorithm 2. This algorithm has two steps: step 2 and step 3. In
step 2  K steps of accelerated gradient ascent method is run over the variable α to ﬁnd an approximate
maximizer of the problem maxα fλ(θt  α). Then using approximate maximizer αt+1  we update θ
variable using one step of ﬁrst order methods in step 3.
In step 2  we run K step of accelerated gradient ascent algorithm over the variable α with restart
every N iterations. The details of this subroutine can be found in subsection B.1 of the supplementary
materials. In step 3 of Algorithm 2  we can either use projected gradient descent update rule

(cid:0)θt  αt+1

(cid:1)(cid:17)

 

(cid:16)

θt+1 (cid:44) projΘ

θt −

1

L11 + L2

12/λ

∇θfλ

or Frank-Wolfe update rule described in subsection B.2 in the supplementary material. We show
convergence of the algorithm to ε–FNE in Theorems 4.2.

6

Algorithm 2 Multi-Step Frank Wolfe/Projected Gradient Step Framework

Require: Constants(cid:101)L (cid:44) max{L  L12  gmax}  N (cid:44) (cid:98)(cid:112)8L22/λ(cid:99)  K  T   η  λ  θ0 ∈ Θ  α0 ∈ A

1: for t = 0  1  2  . . .   T do
2:

Set αt+1 = APGA(αt  θt  η  N  K) by running K steps of Accelerated Projected Gradient

Ascent subroutine (Algorithm 3) with periodic restart at every N iteration.

Compute θt+1 using ﬁrst-order information (Frank-Wolfe or projected gradient descent).

3:
4: end for

Theorem 4.2. Given a scalar ε ∈ (0  1). Assume that Step 3 in Algorithm 2 sets either runs projected
gradient descent or Frank-Wolfe iteration. Under Assumptions 4.1 and 2.5 

and K ≥ NK(ε) (cid:44) O(cid:0)ε−1/2 log(ε−1)(cid:1) 

η =

1
L22

 

λ (cid:44) ε
4R

  T ≥ NT (ε) (cid:44) O(ε−3) 

then there exists t ∈ {0  . . .   T} such that (θt  αt+1) is an ε–FNE of problem (2).

Proof. The proof is relegated to Appendix B.4.
Corollary 4.3. Under Assumptions 2.5 and 4.1  Algorithm 2 ﬁnds an ε-ﬁrst-order stationary
solution of the game (2) with O(ε−3) gradient evaluations of the objective with respect θ and
O(ε−0.5 log(ε−1)) gradient evaluations with respect to α. If the two oracles have the same complex-
ity  the overall complexity of the method would be O(ε−3.5 log(ε−1)).

5 Numerical Results

We evaluate the numerical performance of Algorithm 2 in the following two applications:

5.1 Fair Classiﬁer
We conduct two experiment on the Fashion MNIST dataset [55]. This dataset consists of 28 × 28
arrays of grayscale pixel images classiﬁed into 10 categories of clothing. It includes 60  000 training
images and 10  000 testing images.
Experimental Setup: The recent work in [42] observed that training a logisitic regression model
to classify the images of the Fashion MNIST dataset can be biased against certain categories. To
remove this bias  [42] proposed to minimize the maximum loss incurred by the different categories.
We repeat the experiment when using a more complex non-convex Convolutional Neural Network
(CNN) model for classiﬁcation. Similar to [42]  we limit our experiment to the three categories
T-shirt/top  Coat  and Shirts  that correspond to the lowest three testing accuracies achieved by the
trained classiﬁer. To minimize the maximum loss over these three categories  we train the classiﬁer to
minimize

(12)
where W represents the parameters of the CNN; and L1  L2  and L3 correspond to the loss incurred
by samples in T-shirt/top  Coat  and Shirt categories. Problem (12) can be re-written as

max{L1(W) L2(W) L3(W)} 

min
W

3(cid:88)

i=1

3(cid:88)

Clearly the inner maximization problem is concave; and thus our theory can be applied. To empirically
evaluate the regularization scheme proposed in Section 4  we implement two versions of Algorithm 2.
The ﬁrst version solves at each iteration the regularized strongly concave sub-problem

max
t1 t2 t3

tiLi(W) − λ
2

t2
i

s.t.

ti ≥ 0 ∀ i = 1  2  3;

ti = 1 

(13)

i=1

i=1

and use the optimum t to perform a gradient descent step on W (notice that ﬁxing the value of W 
the optimum t can be computed using KKT conditions and a simple sorting or bisection procedure).

7

3(cid:88)

min
W

max
t1 t2 t3

tiLi(W)

s.t.

ti ≥ 0 ∀ i = 1  2  3;

ti = 1.

i=1

i=1

3(cid:88)

3(cid:88)

Figure 1: The effect of regularization on the convergence of the training loss  λ = 0.1.

The second version of Algorithm 2 solves at each iteration the concave inner maximization problem
without the regularization term. Then uses the computed solution to perform a descent step on
W. Notice that in both cases  the optimization with respect to t variable can be done in (almost)
closed-form update. Although regularization is required to have theoretical convergence guarantees 
we compare the two versions of the algorithm on empirical data to determine whether we lose by
adding such regularization. We further compare these two algorithms with normal training that uses
gradient descent to minimize the average loss among the three categories. We run all algorithms
for 5500 epochs and record the test accuracy of the categories. To reduce the effect of random
initialization  we run our methods with 50 different random initializations and record the average
and standard deviation of the test accuracy collected. For fair comparison  the same initialization
is used for all methods in each run. The results are summarized in Tables 1. To test our framework
in stochastic settings  we repeat the experiment running all algorithms for 12  000 iterations with
Adam and SGD optimizer with a bath size of 600 images (200 from each category). The results of
the second experiment with Adam optimizer are summarized in Table 2. The model architecture and
parameters are detailed in Appendix F. The choice of Adam optimizer is mainly because it is more
robust to the choice of the step-size and thus can be easily tuned. In fact  the use of SGD or Adam
does not change the overall takeaways of the experiments. The results of using SGD optimizer are
relegated to Appendix C.
Results: Tables 1 and 2 show the average and standard deviation of the number of correctly classiﬁed
samples. The average and standard deviation are taken over 50 runs. For each run 1000 testing
samples are considered for each category. The results show that when using MinMax and MinMax
with regularization  the accuracies across the different categories are more balanced compared to
normal training. Moreover  the tables show that Algorithm 2 with regularization provides a slightly
better worst-case performance compared to the unregularized approach. Note that the empirical
advantages due to regularization appears more in the stochastic setting. To see this compare the
differences between MinMax and MinMax with Regularization in Tables 1 and 2. Figure 1 depicts a
sample trajectory of deterministic algorithm applied to the regularized and regularized formulations.
This ﬁgures shows that regularization provides a smoother and slightly faster convergence compared
to the unregularized approach. In addition  we apply our algorithm to the exact similar logistic
regression setup as in [42]. Results of this experiment can be found in Appendix D.

Normal
MinMax

MinMax with Regularization

T-shirt/top
std
8.58
10.40
10.53

mean
850.72
774.14
779.84

mean
843.50
753.88
765.56

std
17.24
22.52
22.28

Coat

Shirt

Worst

mean
658.74
766.14
762.34

std
17.81
13.59
11.91

mean
658.74
750.04
755.66

std
17.81
18.92
15.11

Table 1: The mean and standard deviation of the number of correctly classiﬁed samples when gradient
descent is used in training  λ = 0.1.

8

Normal
MinMax

MinMax with Regularization

T-shirt/top
std
10.04
15.12
14.12

mean
853.86
753.44
764.02

Coat

Shirt

Worst

mean
852.22
715.24
739.80

std
18.27
32.00
27.60

mean
683.32
733.42
748.84

std
17.96
18.51
15.79

mean
683.32
711.64
734.34

std
17.96
29.02
23.54

Table 2: The mean and standard deviation of the number of correctly classiﬁed samples when Adam
(mini-batch) is used in training  λ = 0.1.

5.2 Robust Neural Network Training

Experimental Setup: Neural networks have been widely used in various applications  especially in
the ﬁeld of image recognition. However  these neural networks are vulnerable to adversarial attacks 
such as Fast Gradient Sign Method (FGSM) [25] and Projected Gradient Descent (PGD) attack [31].
These adversarial attacks show that a small perturbation in the data input can signiﬁcantly change the
output of a neural network. To train a robust neural network against adversarial attacks  researchers
reformulate the training procedure into a robust min-max optimization formulation [38]  such as

N(cid:88)

i=1

min

w

max

δi  s.t. |δi|∞≤ε

(cid:96)(f (xi + δi; w)  yi).

Here w is the parameter of the neural network  the pair (xi  yi) denotes the i-th data point  and δi
is the perturbation added to data point i. As discussed in this paper  solving such a non-convex
non-concave min-max optimization problem is computationally challenging. Motivated by the theory
developed in this work  we approximate the above optimization problem with a novel objective
function which is concave in the parameters of the (inner) maximization player. To do so  we ﬁrst
approximate the inner maximization problem with a ﬁnite max problem

max{(cid:96)(f (ˆxi0(w); w)  yi)  . . .   (cid:96)(f (ˆxi9(w); w)  yi)}  

(14)

N(cid:88)

i=1

min

w

where each ˆxij(w) is the result of a targeted attack on sample xi aiming at changing the output of
the network to label j. These perturbed inputs  which are explained in details in Appendix E  are the
function of the weights of the network. Then we replace this ﬁnite max inner problem with a concave
problem over a probability simplex. Such a concave inner problem allows us to use the multi-step
gradient descent-ascent method. The structure of the network and the details of the formulation is
detailed in Appendix E.
Results: We compare our results with [38  57]. Note [57] is the state-of-the-art algorithm and has
won the ﬁrst place  out of ≈ 2000 submissions  in the NeurIPS 2018 Adversarial Vision Challenge.
The accuracy of our formulation against popular attacks  FGSM [25] and PGD [31]  are summarized
in Table 3. This table shows that our formulation leads to a comparable results against state-of-the-art
algorithms (while in some cases it also outperform those methods by as much as ≈ 15% accuracy).

Natural

FGSM L∞ [25]

PGD40 L∞ [31]

[38] with ε = 0.35
[57] with ε = 0.35
[57] with ε = 0.40
Proposed with ε = 0.40

ε = 0.4

ε = 0.2

ε = 0.3

ε = 0.3

ε = 0.2

ε = 0.4
98.58% 96.09% 94.82% 89.84% 94.64% 91.41% 78.67%
97.37% 95.47% 94.86% 79.04% 94.41% 92.69% 85.74%
97.21% 96.19% 96.17% 96.14% 95.01% 94.36% 94.11%
98.20% 97.04% 96.66% 96.23% 96.00% 95.17% 94.22%

Table 3: Test accuracies under FGSM and PGD attacks. All adversarial images are quantiﬁed to 256
levels (0 − 255 integer).

Links to code and pre-trained models of above two simulations are available at Appendix G.

9

,Maher Nouiehed
Maziar Sanjabi
Tianjian Huang
Jason Lee
Meisam Razaviyayn