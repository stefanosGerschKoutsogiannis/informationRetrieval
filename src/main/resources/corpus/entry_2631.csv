2017,Inference in Graphical Models via Semidefinite Programming Hierarchies,Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured  combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation  within the Sherali-Adams hierarchy. Despite the popularity of these algorithms   it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately  SOS relaxations for a graph with $n$ vertices require solving an SDP with $n^{\Theta(d)}$ variables where $d$ is the degree in the hierarchy. In practice  for $d\ge 4$  this approach does not scale beyond a few tens of variables. In this paper  we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly  in analogy to BP and its variants  we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly  we solve the resulting SDP using a non-convex Burer-Monteiro style method  and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes  and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally  for specific graph types  we establish a sufficient condition for the tightness of the proposed partial SOS relaxation.,Inference in Graphical Models

via Semideﬁnite Programming Hierarchies

Murat A. Erdogdu
Microsoft Research

erdogdu@cs.toronto.edu

Yash Deshpande

MIT and Microsoft Research

yash@mit.edu

Andrea Montanari
Stanford University

montanari@stanford.edu

Abstract

Maximum A posteriori Probability (MAP) inference in graphical models amounts
to solving a graph-structured combinatorial optimization problem. Popular infer-
ence algorithms such as belief propagation (BP) and generalized belief propagation
(GBP) are intimately related to linear programming (LP) relaxation within the
Sherali-Adams hierarchy. Despite the popularity of these algorithms  it is well
understood that the Sum-of-Squares (SOS) hierarchy based on semideﬁnite pro-
gramming (SDP) can provide superior guarantees. Unfortunately  SOS relaxations
for a graph with n vertices require solving an SDP with n⇥(d) variables where
d is the degree in the hierarchy. In practice  for d  4  this approach does not
scale beyond a few tens of variables. In this paper  we propose binary SDP relax-
ations for MAP inference using the SOS hierarchy with two innovations focused
on computational efﬁciency. Firstly  in analogy to BP and its variants  we only
introduce decision variables corresponding to contiguous regions in the graphical
model. Secondly  we solve the resulting SDP using a non-convex Burer-Monteiro
style method  and develop a sequential rounding procedure. We demonstrate that
the resulting algorithm can solve problems with tens of thousands of variables
within minutes  and outperforms BP and GBP on practical problems such as image
denoising and Ising spin glasses. Finally  for speciﬁc graph types  we establish a
sufﬁcient condition for the tightness of the proposed partial SOS relaxation.

Introduction

1
Graphical models provide a powerful framework for analyzing systems comprised by a large number
of interacting variables. Inference in graphical models is crucial in scientiﬁc methodology with
countless applications in a variety of ﬁelds including causal inference  computer vision  statistical
physics  information theory  and genome research [WJ08  KF09  MM09].
In this paper  we propose a class of inference algorithms for pairwise undirected graphical models.
Such models are fully speciﬁed by assigning: (i) a ﬁnite domain X for the variables; (ii) a ﬁnite
graph G = (V  E) for V = [n] ⌘{ 1  . . .   n} capturing the interactions of the basic variables;
(iii) a collection of functions ✓ = ({✓v
ij}(i j)2E) that quantify the vertex potentials and
interactions between the variables; whereby for each vertex i 2 V we have ✓v
i : X! R and for each
edge (i  j) 2 E  we have ✓e
ij : X⇥X! R (an arbitrary ordering is ﬁxed on the pair of vertices
{i  j}). These parameters can be used to form a probability distribution on X V for the random vector
x = (x1  x2  ...  xn) 2X V by letting 
eU (x;✓)  

i }i2V  {✓e

(1.1)

1

✓v
i (xi)  

p(x|✓) =

Z(✓)

U (x; ✓) = X(i j)2E

✓e

ij(xi  xj) +Xi2V

where Z(✓) is the normalization constant commonly referred to as the partition function. While such
models can encode a rich class of multivariate probability distributions  basic inference tasks are

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

intractable except for very special graph structures such as trees or small treewidth graphs [CD+06].
In this paper  we will focus on MAP estimation  which amounts to solving the combinatorial
optimization problem

ˆx(✓) ⌘ arg max
x2X V

U (x; ✓).

Intractability plagues other classes of graphical models as well (e.g. Bayesian networks  factor
graphs)  and has motivated the development of a wide array of heuristics. One of the simplest such
heuristics is the loopy belief propagation (BP) [WJ08  KF09  MM09]. In its max-product version
(that is well-suited for MAP estimation)  BP is intimately related to the linear programming (LP)
relaxation of the combinatorial problem maxx2X V U (x; ✓). Denoting the decision variables by
b = ({bi}i2V  {bij}(i j)2E)  LP relaxation form of BP can be written as
✓ij(xi  xj)bij(xi  xj) +Xi2V Xxi2X

X(i j)2E Xxi xj2X

✓i(xi)bi(xi)  

maximize

b

(1.2)

(1.3)

(1.4)

subject to Xxj2X

bij(xi  xj) = bi(xi)

8(i  j) 2 E  
8(i  j) 2 E  

bij 2 X⇥X

bi 2 X 8i 2 V 

(1.5)
where S denotes the simplex of probability distributions over set S. The decision variables are
referred to as ‘beliefs’  and their feasible set is a relaxation of the polytope of marginals of distributions.
The beliefs satisfy the constraints on marginals involving at most two variables connected by an edge.
Loopy belief propagation is successful on some applications  e.g. sparse locally tree-like graphs
that arise  for instance  decoding modern error correcting codes [RU08] or in random constraint
satisfaction problems [MM09]. However  in more structured instances – arising for example in
computer vision – BP can be substantially improved by accounting for local dependencies within
subsets of more than two variables. This is achieved by generalized belief propagation (GBP)
[YFW05] where the decision variables are beliefs bR that are deﬁned on subsets of vertices (a
‘region’) R ✓ [n]  and that represent the marginal distributions of the variables in that region. The
basic constraint on the beliefs is the linear marginalization constraint:PxR\S
bR(xR) = bS(xS) 
holding whenever S ✓ R. Hence GBP itself is closely related to LP relaxation of the polytope
of marginals of probability distributions. The relaxation becomes tighter as larger regions are
incorporated. In a prototypical application  G is a two-dimensional grid  and regions are squares
induced by four contiguous vertices (plaquettes)  see Figure 1  left frame. Alternatively in the right
frame of the same ﬁgure  the regions correspond to triangles.
The LP relaxations that correspond to GBP are closely related to the Sherali-Adams hierarchy
[SA90]. Similar to GBP  the variables within this hierarchy are beliefs over subsets of variables
bR = (bR(xR))xR2X R which are consistent under marginalization: PxR\S
bR(xR) = bS(xS).
However  these two approaches differ in an important point: Sherali-Adams hierarchy uses beliefs
over all subsets of |R| d variables  where d is the degree in the hierarchy; this leads to an LP of size
⇥(nd). In contrast  GBP only retains regions that are contiguous in G. If G has maximum degree k 
this produces an LP of size O(nkd)  a reduction which is signiﬁcant for large-scale problems.
Given the broad empirical success of GBP  it is natural to develop better methods for inference in
graphical models using tighter convex relaxations. Within combinatorial optimization  it is well
understood that the semideﬁnite programming (SDP) relaxations provide superior approximation
guarantees with respect to LP [GW95]. Nevertheless  SDP has found limited applications in inference
tasks for graphical models for at least two reasons. A structural reason: standard SDP relaxations (e.g.
[GW95]) do not account exactly for correlations between neighboring vertices in the graph which is
essential for structured graphical models. As a consequence  BP or GBP often outperforms basic
SDPs. A computational reason: basic SDP relaxations involve ⇥(n2) decision variables  and generic
interior point solvers do not scale well for the large-scale applications. An exception is [WJ04] which
employs the simplest SDP relaxation (degree 2 Sum-Of-Squares  see below) in conjunction with a
relaxation of the entropy and interior point methods – higher order relaxations are brieﬂy discussed
without implementation as the resulting program suffers from the aforementioned limitations.
In this paper  we revisit MAP inference in graphical models via SDPs  and propose an approach that
carries over the favorable performance guarantees of SDPs into inference tasks. For simplicity  we
focus on models with binary variables  but we believe that many of the ideas developed here can be
naturally extended to other ﬁnite domains. We present the following contributions:

2

Figure 1: A two dimensional grid  and two typical choices for regions for GBP and PSOS.
Left: Regions are plaquettes comprising four vertices. Right: Regions are triangles.
Partial Sum-Of-Squares relaxations. We use SDP hierarchies  speciﬁcally the Sum-Of-Squares
(SOS) hierarchy [Sho87  Las01  Par03] to formulate tighter SDP relaxations for binary MAP inference
that account exactly for the joint distributions of small subsets of variables xR  for R ✓ V . However 
SOS introduces decision variables for all subsets R ✓ V with |R| d/2 (d is a ﬁxed even integer) 
and hence scales poorly for large-scale inference problems. We propose a similar modiﬁcation as in
GBP. Instead of accounting for all subsets R with |R| d/2  we only introduce decision variables to
represent a certain family of such subsets (regions) of vertices in G. The resulting SDP has (for d and
the maximum degree of G bounded) only O(n2) decision variables which is suitable for practical
implementations. We refer to these relaxations as Partial Sum-Of-Squares (PSOS)  cf. Section 2.
Theoretical analysis. In Section 2.1  we prove that suitable PSOS relaxations are tight for certain
classes of graphs  including planar graphs  with ✓v = 0. While this falls short of explaining the
empirical results (which uses simpler relaxations  and ✓v 6= 0)  it points in the right direction.
Optimization algorithm and rounding. Despite the simpliﬁcation afforded by PSOS  interior-point
solvers still scale poorly to large instances. In order to overcome this problem  we adopt a non-convex
approach proposed by Burer and Monteiro [BM03]. We constrain the rank of the SDP matrix in
PSOS to be at most r  and solve the resulting non-convex problem using a trust-region coordinate
ascent method  cf. Section 3.1. Further  we develop a rounding procedure called Conﬁdence Lift and
Project (CLAP) which iteratively uses PSOS relaxations to obtain an integer solution  cf. Section 3.2.
Numerical experiments. In Section 4  we present numerical experiments with PSOS by solving
problems of size up to 10  000 within several minutes. While additional work is required to scale
this approach to massive sizes  we view this as an exciting proof-of-concept. To the best of our
knowledge  no earlier attempt was successful in scaling higher order SOS relaxations beyond tens
of dimensions. More speciﬁcally  we carry out experiments with two-dimensional grids – an image
denoising problem  and Ising spin glasses. We demonstrate through extensive numerical studies that
PSOS signiﬁcantly outperforms BP and GBP in the inference tasks we consider.
2 Partial Sum-Of-Squares Relaxations
For concreteness  throughout the paper we focus on pairwise models with binary variables. We do not
expect fundamental problems extending the same approach to other domains. For binary variables
x = (x1  x2  ...  xn)  MAP estimation amounts to solving the following optimization problem

maximize

x

X(i j)2E

✓e

ijxixj +Xi2V

subject to xi 2{ +1 1}  

8i 2 V  

✓v
i xi  

(INT)

ij)1i jn and ✓v = (✓v

where ✓e = (✓e
For the reader’s convenience  we recall a few basic facts about SOS relaxations  referring to [BS16]
for further details. For an even integer d  SOS(d) is an SDP relaxation of INT with decision variable

i )1in are the parameters of the graphical model.

X :[n]

d ! R where[n]

maximize

d denotes the set of subsets S ✓ [n] of size |S| d; it is given as
ijX({i  j}) +Xi2V
subject to X(;) = 1  M(X) < 0 .

X(i j)2E

✓v
i X({i})  

✓e

X

(SOS)

The moment matrix M(X) is indexed by sets S  T ✓ [n]  |S| |T| d/2  and has entries M(X)S T =
X(S4T ) with 4 denoting the symmetric difference of two sets. Note that M(X)S S = X(;) = 1.

3

Region 1Region 2Region 1Region 2Region 4Region 3l

 

e
u
a
V
e
v
i
t
c
e
b
O

j

662
600

400

200

0

p
a
G
 
y
t
i
l

a
u
D

1e+01

1e−01

1e−03

1e−05

1e−07

Rank
20
10
5
3
2

Rank
20
10
5
3
2

0

50

100

Iterations

150

200

0

50

100

Iterations

150

200

Figure 2: Effect of the rank constraint r on n = 400 square lattice (20 ⇥ 20): Left plot shows the
change in the value of objective at each iteration. Right plot shows the duality gap of the Lagrangian.
We can equivalently represent M(X) as a Gram matrix by letting M(X)S T = hS  Ti for a
collection of vectors S 2 Rr indexed by S 2 [n]
semideﬁnite matrix; however  in what follows it is convenient from a computational perspective to
consider smaller choices of r. The constraint M(X)S S = 1 is equivalent to kSk = 1  and the
condition M (X)S T = X(S4T ) can be equivalently written as
(2.1)

d/2. The case r = [n]

d/2 can represent any

hS1  T1i = hS2  T2i  

8S14T1 = S24T2.

In the case d = 2  SOS(2) recovers the classical Goemans-Williamson SDP relaxation [GW95].
In the following  we consider the simplest higher-order SDP  namely SOS(4) for which the general
constraints in Eq. (2.1) can be listed explicitly. Fixing a region R ✓ V   and deﬁning the Gram vectors
;  (i)i2V   (ij){i j}✓V   we list the constraints that involve vectors S for S ✓ R and |S| = 1  2:
(Sphere i )
(Undirected i  j)
(Directed i ! j)
jV k)
(V-shaped i
i
j4k)
(Triangle
k⇤j
l )
Given an assignment of the Gram vectors  = (;  (i)i2V   (ij){i j}✓V )  we denote by |R its
restriction to R  namely |R = (;  (i)i2R  (ij){i j}✓R). We denote by ⌦(R)  the set of vectors
|R that satisfy the above constraints. With these notations  the SOS(4) SDP can be written as

kik = 1
hi  ji = hij  ;i
hi  iji = hj  ;i
hi  jki = hk  iji
hij  jki = hik  ;i
hij  kli = hik  jli

8i 2 S [ {;} 
8i  j 2 S 
8i  j 2 S 
8i  j  k 2 S 
8i  j  k 2 S 
8i  j  k  l 2 S.

(Loop i

X(i j)2E

✓e

ijhi  ji +Xi2V

✓v
i hi  ;i  

maximize



subject to  2 ⌦(V ) .

(SOS(4))

A speciﬁc Partial SOS (PSOS) relaxation is deﬁned by a collection of regions R =
{R1  R2  . . .   Rm}  Ri ✓ V . We will require R to be a covering  i.e. [m
i=1Ri = V and for
each (i  j) 2 E there exists ` 2 [m] such that {i  j}✓ R`. Given such a covering  the PSOS(4)
relaxation is
(PSOS(4))

maximize

✓e



X(i j)2E

ijhi  ji +Xi2V

✓v
i hi  ;i  
8i 2{ 1  2  . . .   m} .

subject to |Ri 2 ⌦(Ri)

Notice that variables ij only enter the above program if {i  j}✓ R` for some `. As a consequence 
the dimension of the above optimization problem is O(rPm
`=1 |R`|2)  which is O(nr) if the regions
have bounded size; this will be the case in our implementation. Of course  the speciﬁc choice of
regions R is crucial for the quality of this relaxation. A natural heuristic is to choose each region R`
to be a subset of contiguous vertices in G  which is generally the case for GBP algorithms.

4

= 0
for s 2 Actives do
if s 2 V then
cs =Pt2@s ✓e
else

cs = ✓e

stt + ✓v

s ;

s1s2; + ✓v

s1s2 + ✓v

s2s1

/* s 2 V is a vertex */
/* s = (s1  s2) 2 E is an edge */

Algorithm 1: Partial-SOS
Input :G = (V  E)  ✓e 2 Rn⇥n  ✓v 2 Rn   2 Rr⇥(1+|V |+|E|)  Reliables = ;
Actives = V [ E \ Reliables  and = 1 
while  > tol do

kk=1 hcs  i + ⇢

Form matrix As  vector bs  and the corresponding Lagrange multipliers s (see text).
new
s  arg max
  + knew
s  new
s  s + Ass  bs

2kAs  bs + sk2 

s  sk2 + kAss  bsk2

s

/* update variables */

/* sub-problem */

ij)(i j)2E  (✓v

2.1 Tightness guarantees
Solving exactly INT is NP-hard even if G is a three-dimensional grid [Bar82]. Therefore  we do
not expect PSOS(4) to be tight for general graphs G. On the other hand  in our experiments (cf.
Section 4)  PSOS(4) systematically achieves the exact maximum of INT for two-dimensional grids
with random edge and vertex parameters (✓e
i )i2V . This ﬁnding is quite surprising and
calls for a theoretical explanation. While full understanding remains an open problem  we present
here partial results in that direction.
Recall that a cycle in G is a sequence of distinct vertices (i1  . . .   i`) such that  for each j 2 [`] ⌘
{1  2  . . .  `}  (ij  ij+1) 2 E (where ` + 1 is identiﬁed with 1). The cycle is chordless if there is no
j  k 2 [`]  with j  k 6= ±1 mod ` such that (ij  ik) 2 E. We say that a collection of regions R
on graph G is circular if for each chordless cycle in G there exists a region in R 2R such that all
vertices of the cycle belong to R. We also need the following straightforward notion of contractibility.
A contraction of G is a new graph obtained by identifying two vertices connected by an edge in G.
G is contractible to H if there exists a sequence of contractions transforming G into H.
The following theorem is a direct consequence of a result of Barahona and Mahjoub [BM86] (see
Supplement for a proof).
Theorem 1. Consider the problem INT with ✓v = 0. If G is not contractible to K5 (the complete
graph over 5 vertices)  then PSOS(4) with a circular covering R is tight.
The assumption that ✓v = 0 can be made without loss of generality (see Supplement for the reduction
from the general case). Furthermore  INT can be solved in polynomial time if G is planar  and ✓v = 0
[Bar82]. Note however  the reduction from ✓v 6= 0 to ✓v = 0 can transform a planar graph to a
non-planar graph. This theorem implies that (full) SOS(4) is also tight if G is not contractible to K5.
Notice that planar graphs are not contractible to K5  and we recover the fact that INT can be solved
in polynomial time if ✓v = 0. This result falls short of explaining the empirical ﬁndings in Section 4 
for at least two reasons. Firstly the reduction to ✓v = 0 induces K5 subhomomorphisms for grids.
Second  the collection of regions R described in the previous section does not include all chordless
cycles. Theoretically understanding the empirical performance of PSOS(4) as stated remains open.
However  similar cycle constraints have proved useful in analyzing LP relaxations [WRS16].
3 Optimization Algorithm and Rounding
3.1 Solving PSOS(4) via Trust-Region Coordinate Ascent
We will approximately solve PSOS(4) while keeping r = O(1). Earlier work implies that (under
suitable genericity condition on the SDP) there exists an optimal solution with rank p2 # constraints
[Pat98]. Recent work [BVB16] shows that for r > p2 # constraints  the non-convex optimization
problem has no non-global local maxima. For SOS(2)  [MM+17] proves that setting r = O(1)
is sufﬁcient for achieving O(1/r) relative error from the global maximum for speciﬁc choices of
potentials ✓e ✓ v. We ﬁnd that there is little or no improvement beyond r = 10 (cf. Figure 2).

5

Algorithm 2: CLAP: Confidence Lift And Project
Input :G = (V  E)  ✓e 2 Rn⇥n  ✓v 2 Rn  regions R = {R1  ...  Rm}
Initialize variable matrix  2 Rr⇥(1+|V |+|E|) and set Reliables = ;.
while Reliables 6= V [ E do
Run Partial-SOS on inputs G = (V  E)  ✓e  ✓v    Reliables
Promotions = ; and Conﬁdence = 0.9
while Conﬁdence > 0 and Promotions 6= ; do

for s 2 V [ E \ Reliables do

if |h;  si| > Conﬁdence then

s = sign(h;  si) · ;
Promotions  Promotions [{ sc}

if Promotions = ; then
Conﬁdence  Conﬁdence  0.1
Reliables  Reliables [ Promotions

/* lift procedure */

/* find promotions */

/* project procedure */

/* decrease confidence level */

/* update Reliables */

⇢

.

E =(i  j) 2 V ⇥ V :

Output :(hi  ;i)i2V 2 {1  +1}n
We will assume that R = (R1  . . .   Rm) is a covering of G (in the sense introduced in the previous
section)  and –without loss of generality– we will assume that the edge set is
(3.1)
In other words  E is the maximal set of edges that is compatible with R being a covering. This can
always be achieved by adding new edges (i  j) to the original edge set with ✓e
ij = 0. Hence  the
decision variables s are indexed by s 2S = {;}[V [E. Apart from the norm constraints  all other
consistency constraints take the form hs  ri = ht  pi for some 4-tuple of indices (s  r  t  p). We
denote the set of all such 4-tuples by C  and construct the augmented Lagrangian of PSOS(4) as
L(  ) =Xi2V

2 X(s r t p)2C⇣hs  ri  ht  pi + s r t p⌘2

such that {i  j}✓ R` .

i hi  ;i + X(i j)2E

✓e
ijhi  ji +

9` 2 [m]

✓v

At each step  our algorithm execute two operations: (i) maximize the cost function with respect to one
of the vectors s; (ii) perform one step of gradient descent with respect to the corresponding subset
of Lagrangian parameters  to be denoted by s. More precisely  ﬁxing s 2 S \ {;} (by rotational
invariance  it is not necessary to update ;)  we note that s appears in the constraints linearly (or
it does not appear). Hence  we can write these constraints in the form Ass = bs where As  bs
depend on (r)r6=s but not on s. We stack the corresponding Lagrangian parameters in a vector s;
therefore the Lagrangian term involving s reads (⇢/2)kAss  bs + sk2. On the other hand  the
graphical model contribution is that the ﬁrst two terms in L(  ) are linear in s  and hence they
can be written as hcs  si. Summarizing  we have

L(  ) =hcs  si + kAss  bs + sk2 + eL(r)r6=s   .

(3.2)
It is straightforward to compute As  bs  cs; in particular  for (s  r  t  p) 2C   the rows of As and bs
are indexed by r such that the vectors r form the rows of As  and ht  pi form the corresponding
entry of bs. Further  if s is a vertex and @s are its neighbors  we set cs = Pt2@s ✓e
s ;
while if s = (s1  s2) is an edge  we set cs = ✓e
s2s1. Note that we are using the
equivalent representations hi  ji = hij  ;i  hij  ji = hi  ;i  and hij  ii = hj  ;i.
Finally  we maximize Eq. (3.2) with respect to s by a Moré-Sorenson style method [MS83].
3.2 Rounding via Conﬁdence Lift and Project
After Algorithm 1 generates an approximate optimizer  for PSOS(4)  we reduce its rank to produce
a solution of the original combinatorial optimization problem INT. To this end  we interpret hi  ;i
as our belief about the value of xi in the optimal solution of INT  and hij  ;i as our belief about
the value of xixj. This intuition can be formalized using the notion of pseudo-probability [BS16].
We then recursively round the variables about which we have strong beliefs; we ﬁx rounded variables
in the next iteration  and solve the induced PSOS(4) on the remaining ones.
More precisely  we set a conﬁdence threshold Conﬁdence. For any variable s such that |hs  ;i| >
Conﬁdence  we let xs = sign(hs  ;i) and ﬁx s = xs ;. These variables s are no longer

s1s2; + ✓v

s1s2 + ✓v

stt + ✓v

6

True	

Noisy	

BP-SP	

BP-MP	

GBP	

PSOS(2)	

PSOS(4)	

	

.

2
0
=
p

	

	

	
i
l
l

u
o
n
r
e
B

	U(x)	:				25815																	19237																	26165																		26134																26161																	26015																		26194	
	Time:											-																										-																					2826s																		2150s 	
											5059s	

									454s	

						7894s 	

	

	

.

6
0
0
0
=
p
e
s
i
w
k
c
o
B

l

							8844s	

										248s	

	U(x)	:				27010																	26808																	27230																		27012																	27232																	26942																	27252	
	Time:											-																										-																					1674s																				729s 	
											4457s	
Figure 3: Denoising a binary image by maximizing the objective function Eq. (4.1). Top row: i.i.d.
Bernoulli error with ﬂip probability p = 0.2 with ✓0 = 1.26. Bottom row: blockwise noise where
each pixel is the center of a 3 ⇥ 3 error block independently with probability p = 0.006 and ✓0 = 1.
updated  and instead the reduced SDP is solved. If no variable satisﬁes the conﬁdence condition  the
threshold is reduced until variables are found that satisfy it. After the ﬁrst iteration  most variables
yield strong beliefs and are ﬁxed; hence the consequent iterations have fewer variables and are faster.
4 Numerical Experiments
In this section  we validate the performance of the Partial SOS relaxation and the CLAP rounding
scheme on models deﬁned on two-dimensional grids. Grid-like graphical models are common in a
variety of ﬁelds such as computer vision [SSZ02]  and statistical physics [MM09]. In Section 4.1  we
study an image denoising example and in Section 4.2 we consider the Ising spin glass – a model in
statistical mechanics that has been used as a benchmark for inference in graphical models.
Our main objective is to demonstrate that Partial SOS can be used successfully on large-scale
graphical models  and is competitive with the following popular inference methods:
• Belief Propagation - Sum Product (BP-SP): Pearl’s belief propagation computes exact marginal
distributions on trees [Pea86]. Given a graph structured objective function U (x)  we apply BP-SP
to the Gibbs-Boltzmann distribution p(x) = exp{U (x)}/Z using the standard sum-product update
rules with an inertia of 0.5 to help convergence [YFW05]  and threshold the marginals at 0.5.
• Belief Propagation - Max Product (BP-MP): By replacing the marginal probabilities in the sum-
product updates with max-marginals  we obtain BP-MP  which can be used for exact inference
on trees [MM09]. For general graphs  BP-MP is closely related to an LP relaxation of the
combinatorial problem INT [YFW05  WF01]. Similar to BP-SP  we use an inertia of 0.5. Note
that the Max-Product updates can be equivalently written as Min-Sum updates [MM09].
• Generalized Belief Propagation (GBP): The decision variables in GBP are beliefs (joint prob-
ability distributions) over larger subsets of variables in the graph G  and they are updated in a
message passing fashion [YFW00  YFW05]. We use plaquettes in the grid (contiguous groups of
four vertices) as the largest regions  and apply message passing with inertia 0.1 [WF01].
• Partial SOS - Degree 2 (PSOS(2)): By deﬁning regions as single vertices and enforcing only
the sphere constraints  we recover the classical Goemans-Williamson SDP relaxation [GW95].
Non-convex Burer-Monteiro approach is extremely efﬁcient in this case [BM03]. We round the
SDP solution by ˆxi = sign(hi  ;i) which is closely related to the classical approach of [GW95].
• Partial SOS - Degree 4 (PSOS(4)): This is the algorithm developed in the present paper. We
take the regions R` to be triangles  cf. Figure 1  right frame. In an pn ⇥ pn grid  we have
2(pn  1)2 such regions resulting in O(n) constraints. In Figures 3 and 4  PSOS(4) refers to the
CLAP rounding scheme applied together with PSOS(4) in the lift procedure.
4.1
Given a pn ⇥ pn binary image x0 2{ +1 1}n  we generate a corrupted version of the same
image y 2{ +1 1}n. We then try to denoise y by maximizing the following objective function:
(4.1)

Image Denoising via Markov Random Fields

yixi  

U (x) = X(i j)2E

xixj + ✓0Xi2V

7

Figure 4: Solving the MAP inference problem INT for Ising spin glasses on two-dimensional grids.
U and N represent uniform and normal distributions. Each bar contains 100 independent realizations.
We plot the ratio between the objective value achieved by that algorithm and the exact optimum for
n 2{ 16  25}  or the best value achieved by any of the 5 algorithms for n 2{ 100  400  900}.
where the graph G is the pn ⇥ pn grid  i.e.  V = {i = (i1  i2) :
i1  i2 2{ 1  . . .  pn}} and
E = {(i  j) : ki  jk1 = 1}. In applying Algorithm 1  we add diagonals to the grid (see right plot
in Figure 1) in order to satisfy the condition (3.1) with corresponding weight ✓e
In Figure 3  we report the output of various algorithms for a 100 ⇥ 100 binary image. We are
not aware of any earlier implementation of SOS(4) beyond tens of variables  while PSOS(4) is
applied here to n = 10  000 variables. Running times for CLAP rounding scheme (which requires
several runs of PSOS(4)) are of order an hour  and are reported in Figure 3. We consider two noise
models: i.i.d. Bernoulli noise and blockwise noise. The model parameter ✓0 is chosen in each
case as to approximately optimize the performances under BP denoising. In these (as well as in 4
other experiments of the same type reported in the supplement)  PSOS(4) gives consistently the best
reconstruction (often tied with GBP)  in reasonable time. Also  it consistently achieves the largest
value of the objective function among all algorithms.

ij = 0.

Ising Spin Glass

ij}(i j)2E  {✓v

i ⇠U ({+1/2 1/2})  (iii) ✓e

4.2
The Ising spin glass (also known as Edwards-Anderson model [EA75]) is one of the most studied
models in statistical physics. It is given by an objective function of the form INT with G a d-
dimensional grid  and i.i.d. parameters {✓e
i }i2V . Following earlier work [YFW05] 
we use Ising spin glasses as a testing ground for our algorithm. Denoting the uniform and normal
distributions by U and N respectively  we consider two-dimensional grids (i.e. d = 2)  and the
following parameter distributions: (i) ✓e
ij ⇠
U({+1 1}) and ✓v
i ⇠ N(0  2) with  = 0.1
(this is the setting considered in [YFW05])  and (iv) ✓e
i ⇠ N(0  2) with  = 1.
For each of these settings  we considered grids of size n 2{ 16  25  100  400  900}.
In Figure 4  we report the results of 8 experiments as a box plot. We ran the ﬁve inference algorithms
described above on 100 realizations; a total of 800 experiments are reported in Figure 4. For each of
the realizations  we record the ratio of the achieved value of an algorithm to the exact maximum (for
n 2{ 16  25})  or to the best value achieved among these algorithms (for n 2{ 100  400  900}). This
is because for lattices of size 16 and 25  we are able to run an exhaustive search to determine the true
maximizer of the integer program. Further details are reported in the supplement.
In every single instance of 800 experiments  PSOS(4) achieved the largest objective value  and
whenever this could be veriﬁed by exhaustive search (i.e. for n 2{ 16  25}) it achieved an exact
maximizer of the integer program.

i ⇠U ({+1 1})  (ii) ✓e

ij ⇠U ({+1 1}) and ✓v

ij ⇠ N(0  1) and ✓v

ij ⇠ N(0  1) and ✓v

8

Ratio to the best algorithmPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPPSOS(4)PSOS(2)GBPBP-SPBP-MPReferences

[Bar82] Francisco Barahona. On the computational complexity of Ising spin glass models. Journal of Physics

A: Mathematical and General  15(10):3241  1982.

[BM86] Francisco Barahona and Ali Ridha Mahjoub. On the cut polytope. Mathematical programming 

36(2):157–173  1986.

[BM03] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semideﬁnite

programs via low-rank factorization. Mathematical Programming  95(2):329–357  2003.

[BS16] Boaz Barak and David Steurer. Proofs  beliefs  and algorithms through the lens of sum-of-squares.

Course notes: http://www. sumofsquares. org/public/index. html  2016.

[BVB16] Nicolas Boumal  Vlad Voroninski  and Afonso Bandeira. The non-convex Burer-Monteiro approach
works on smooth semideﬁnite programs. In Advances in Neural Information Processing Systems 
pages 2757–2765  2016.

[CD+06] Robert G Cowell  Philip Dawid  Steffen L Lauritzen  and David J Spiegelhalter. Probabilistic
networks and expert systems: Exact computational methods for Bayesian networks. Springer Science
& Business Media  2006.

[EA75] Samuel Frederick Edwards and Phil W Anderson. Theory of spin glasses. Journal of Physics F:

Metal Physics  5(5):965  1975.

[EM15] Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In

Advances in Neural Information Processing Systems  pages 3052–3060  2015.

[GW95] Michel X Goemans and David P Williamson. Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. Journal of the ACM (JACM) 
42(6):1115–1145  1995.

[KF09] Daphne Koller and Nir Friedman. Probabilistic graphical models. MIT press  2009.
[Las01] Jean B Lasserre. An explicit exact SDP relaxation for nonlinear 0-1 programs. In International

Conference on Integer Programming and Combinatorial Optimization  pages 293–303  2001.

[MM09] Marc Mézard and Andrea Montanari. Information  physics  and computation. Oxford Press  2009.
[MM+17] Song Mei  Theodor Misiakiewicz  Andrea Montanari  and Roberto I Oliveira. Solving SDPs
arXiv preprint

for synchronization and MaxCut problems via the Grothendieck inequality.
arXiv:1703.08729  2017.

[MS83] Jorge J Moré and Danny C Sorensen. Computing a trust region step. SIAM Journal on Scientiﬁc and

Statistical Computing  4(3):553–572  1983.

[Par03] Pablo A Parrilo. Semideﬁnite programming relaxations for semialgebraic problems. Mathematical

programming  96(2):293–320  2003.

[Pat98] Gábor Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of

optimal eigenvalues. Mathematics of operations research  23(2):339–358  1998.

[Pea86] Judea Pearl. Fusion  propagation  and structuring in belief networks. Artiﬁcial intelligence  29(3):241–

288  1986.

[RU08] Tom Richardson and Ruediger Urbanke. Modern coding theory. Cambridge Press  2008.
[SA90] Hanif D Sherali and Warren P Adams. A hierarchy of relaxations between the continuous and convex
hull representations for zero-one programming problems. SIAM Journal on Discrete Mathematics 
3(3):411–430  1990.

[Sho87] Naum Z Shor. Class of global minimum bounds of polynomial functions. Cybernetics and Systems

Analysis  23(6):731–734  1987.

[SSZ02] Jian Sun  Heung-Yeung Shum  and Nan-Ning Zheng. Stereo matching using belief propagation. In

European Conference on Computer Vision  pages 510–524. Springer  2002.

[WF01] Yair Weiss and William T Freeman. On the optimality of solutions of the max-product belief-

propagation algorithm in arbitrary graphs. IEEE Trans. on Info. Theory  47(2):736–744  2001.

[WJ04] Martin J Wainwright and Michael I Jordan. Semideﬁnite relaxations for approximate inference on
graphs with cycles. In Advances in Neural Information Processing Systems  pages 369–376  2004.
[WJ08] Martin J Wainwright and Michael I Jordan. Graphical models  exponential families  and variational

inference. Foundations and Trends in Machine Learning  1(1–2):1–305  2008.

[WRS16] Adrian Weller  Mark Rowland  and David Sontag. Tightness of lp relaxations for almost balanced

models. In Artiﬁcial Intelligence and Statistics  pages 47–55  2016.

[YFW00] Jonathan S Yedidia  William T Freeman  and Yair Weiss. Generalized belief propagation.

Advances in Neural Information Processing Systems  pages 689–695  2000.

In

[YFW05] Jonathan S Yedidia  William T Freeman  and Yair Weiss. Constructing free-energy approxima-
tions and generalized belief propagation algorithms. IEEE Transactions on Information Theory 
51(7):2282–2312  2005.

9

,Murat Erdogdu
Yash Deshpande
Andrea Montanari
Zhuwen Li
Qifeng Chen
Vladlen Koltun
Nikolas Ioannou
Celestine Mendler-Dünner
Thomas Parnell