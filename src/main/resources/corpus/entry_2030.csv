2015,Fast Rates for Exp-concave Empirical Risk Minimization,We consider Empirical Risk Minimization (ERM) in the context of stochastic optimization with exp-concave and smooth losses---a general optimization framework that captures several important learning problems including linear and logistic regression  learning SVMs with the squared hinge-loss  portfolio selection and more. In this setting  we establish the first evidence that ERM is able to attain fast generalization rates  and show that the expected loss of the ERM solution in $d$ dimensions converges to the optimal expected loss in a rate of $d/n$. This rate matches existing lower bounds up to constants and improves by a $\log{n}$ factor upon the state-of-the-art  which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.,Fast Rates for Exp-concave
Empirical Risk Minimization

Tomer Koren

Technion

Haifa 32000  Israel

tomerk@technion.ac.il

kfiryl@tx.technion.ac.il

Kﬁr Y. Levy

Technion

Haifa 32000  Israel

Abstract

We consider Empirical Risk Minimization (ERM) in the context of stochastic op-
timization with exp-concave and smooth losses—a general optimization frame-
work that captures several important learning problems including linear and lo-
gistic regression  learning SVMs with the squared hinge-loss  portfolio selection
and more. In this setting  we establish the ﬁrst evidence that ERM is able to at-
tain fast generalization rates  and show that the expected loss of the ERM solution
in d dimensions converges to the optimal expected loss in a rate of d/n. This
rate matches existing lower bounds up to constants and improves by a log n factor
upon the state-of-the-art  which is only known to be attained by an online-to-batch
conversion of computationally expensive online algorithms.

1

Introduction

Statistical learning and stochastic optimization with exp-concave loss functions captures several
fundamental problems in statistical machine learning  which include linear regression  logistic re-
gression  learning support-vector machines (SVMs) with the squared hinge loss  and portfolio se-
lection  amongst others. Exp-concave functions constitute a rich class of convex functions  which is
substantially richer than its more familiar subclass of strongly convex functions.
Similarly to their strongly-convex counterparts  it is well-known that exp-concave loss functions
are amenable to fast generalization rates. Speciﬁcally  a standard online-to-batch conversion [6]
of either the Online Newton Step algorithm [8] or exponential weighting schemes [5  8] in d di-
mensions gives rise to convergence rate of d/n  as opposed to the standard 1/
n rate of generic
(Lipschitz) stochastic convex optimization. Unfortunately  the latter online methods are highly in-
efﬁcient computationally-wise; e.g.  the runtime complexity of the Online Newton Step algorithm
scales as d4 with the dimension of the problem  even in very simple optimization scenarios [13].
An alternative and widely-used learning paradigm is that of Empirical Risk Minimization (ERM) 
which is often regarded as the strategy of choice due to its generality and its statistical efﬁciency. In
this scheme  a sample of training instances is drawn from the underlying data distribution  and the
minimizer of the sample average (or the regularized sample average) is computed. As opposed to
methods based on online-to-batch conversions  the ERM approach enables the use of any optimiza-
tion procedure of choice and does not restrict one to use a speciﬁc online algorithm. Furthermore 
the ERM solution often enjoys several distribution-dependent generalization bounds in conjunction 
and thus is able to obliviously adapt to the properties of the underlying data distribution.
√
In the context of exp-concave functions  however  nothing is known about the generalization abilities
n convergence rate that applies to any convex losses. Surprisingly 
of ERM besides the standard 1/
it appears that even in the speciﬁc and extensively-studied case of linear regression with the squared
loss  the state of affairs remains unsettled: this important case was recently addressed by Shamir

√

1

[19]  who proved a Ω(d/n) lower bound on the convergence rate of any algorithm  and conjectured
that the rate of an ERM approach should match this lower bound.
In this paper  we explore the convergence rate of ERM for stochastic exp-concave optimization.
We show that when the exp-concave loss functions are also smooth  a slightly-regularized ERM
approach yields a convergence rate of O(d/n)  which matches the lower bound of Shamir [19] up
to constants. In fact  our result shows for ERM a generalization rate tighter than the state-of-the-art
obtained by the Online Newton Step algorithm  improving upon the latter by a log n factor. Even in
the speciﬁc case of linear regression with the squared loss  our result improves by a log(n/d) factor
upon the best known fast rates provided by the Vovk-Azoury-Warmuth algorithm [3  22].
Our results open an avenue for potential improvements to the runtime complexity of exp-concave
stochastic optimization  by permitting the use of accelerated methods for large-scale regularized
loss minimization. The latter has been the topic of an extensive research effort in recent years  and
numerous highly-efﬁcient methods have been developed; see  e.g.  Johnson and Zhang [10]  Shalev-
Shwartz and Zhang [16  17] and the references therein.
On the technical side  our convergence analysis relies on stability arguments introduced by Bousquet
and Elisseeff [4]. We prove that the expected loss of the regularized ERM solution does not change
signiﬁcantly when a single instance  picked uniformly at random from the training sample  is dis-
carded. Then  the technique of Bousquet and Elisseeff [4] allows us to translate this average stability
property into a generalization guarantee. We remark that in all previous stability analyses that we
are aware of  stability was shown to hold uniformly over all discarded training intances  either with
probability one [4  16] or in expectation [20]; in contrast  in the case of exp-concave functions it is
crucial to look at the average stability.
In order to bound the average stability of ERM  we make use of a localized notion of strong con-
vexity  deﬁned with respect to a local norm at a certain point in the optimization domain. Roughly
speaking  we show that when looking at the right norm  which is determined by the local properties
of the empirical risk at the right point  the minimizer of the empirical risk becomes stable. This
part of our analysis is inspired by recent analysis techniques of regularization-based online learning
algorithms [1]  that use local norms to study the regret performance of online linear optimization
algorithms.

1.1 Related Work

The study of exp-concave loss functions was initiated in the online learning community by Kivinen
and Warmuth [12]  who considered the problem of prediction with expert advice with exp-concave
losses. Later  Hazan et al. [8] considered a more general framework that allows for a continuous
decision set  and proposed the Online Newton Step (ONS) algorithm that attains a regret bound that
grows logarithmically with the number of optimization rounds. Mahdavi et al. [15] considered the
ONS algorithm in the statistical setting  and showed how it can be used to establish generalization
bounds that hold with high probability  while still keeping the fast 1/n rate.
Fast convergence rates in stochastic optimization are known to be achievable under various condi-
tions. Bousquet and Elisseeff [4] and Shalev-Shwartz et al. [18] have shown  via a uniform stability
argument  that ERM guarantees a convergence rate of 1/n for strongly convex functions. Sridharan
et al. [21] proved a similar result  albeit using the notion of localized Rademacher complexity. For
the case of smooth and non-negative losses  Srebro et al. [20] established a 1/n rate in low-noise
conditions  i.e.  when the expected loss of the best hypothesis is of order 1/n. For further discussion
of fast rates in stochastic optimization and learning  see [20] and the references therein.

2 Setup and Main Results

We consider the problem of minimizing a stochastic objective
F (w) = E[f (w  Z)]

(1)
over a closed and convex domain W ⊆ Rd in d-dimensional Euclidean space. Here  the expec-
tation is taken with respect to a random variable Z distributed according to an unknown distri-
bution over a parameter space Z. Given a budget of n samples z1  . . .   zn of the random vari-

able Z  we are required to produce an estimate (cid:98)w ∈ W whose expected excess loss  deﬁned by

2

E[F ((cid:98)w)]− minw∈W F (w)  is small. (Here  the expectation is with respect the randomization of the
training set z1  . . .   zn used to produce (cid:98)w.)

We make the following assumptions over the loss function f. First  we assume that for any ﬁxed
parameter z ∈ Z  the function f (·  z) is α-exp-concave over the domain W for some α > 0  namely 
that the function exp (−αf (·  z)) is concave over W. We will also assume that f (·  z) is β-smooth
over W with respect to Euclidean norm (cid:107)·(cid:107)2  which means that its gradient is β-Lipschitz with
respect to the same norm:

∀ w  w(cid:48) ∈ W  

(cid:107)∇f (w  z) − ∇f (w(cid:48)  z)(cid:107)2 ≤ β(cid:107)w − w(cid:48)(cid:107)2 .

(2)
In particular  this property implies that f (·  z) is differentiable. For simplicity  and without loss of
generality  we assume β ≥ 1. Finally  we assume that f (·  z) is bounded over W  in the sense that
|f (w  z) − f (w(cid:48)  z)| ≤ C for all w  w(cid:48) ∈ W for some C > 0.
In this paper  we analyze a regularized Empirical Risk Minimization (ERM) procedure for optimiz-
ing the stochastic objective in Eq. (1)  that based on the sample z1  . . .   zn computes

(cid:98)w = arg min

w∈W

(cid:98)F (w)  

where

(cid:98)F (w) =

1
n

n(cid:88)

i=1

f (w  zi) +

1
n

R(w) .

(3)

(4)

The function R : W (cid:55)→ R serves as a regularizer  which is assumed to be 1-strongly-convex with
respect to the Euclidean norm; for instance  one can simply choose R(w) = 1
2. The strong

constant B > 0.
Our main result  which we now present  establishes a fast 1/n convergence rate for the expected

convexity of R implies in particular that (cid:98)F is also strongly convex  which ensures that the optimizer
(cid:98)w is unique. For our bounds  we will assume that |R(w) − R(w(cid:48))| ≤ B for all w  w(cid:48) ∈ W for some
excess loss of the ERM estimate (cid:98)w given in Eq. (3).
ERM estimate (cid:98)w deﬁned in Eqs. (3) and (4) based on an i.i.d. sample z1  . . .   zn  the expected excess

Theorem 1. Let f : W × Z (cid:55)→ R be a loss function deﬁned over a closed and convex domain W ⊆
Rd  which is α-exp-concave  β-smooth and B-bounded with respect to its ﬁrst argument. Let R :
W (cid:55)→ R be a 1-strongly-convex and B-bounded regularization function. Then  for the regularized

2(cid:107)w(cid:107)2

loss is bounded as

E[F ((cid:98)w)] − min

w∈W F (w) ≤ 24βd

αn

+

100Cd

n

+

B
n

= O

(cid:18) d

(cid:19)

.

n

In other words  the theorem states that for ensuring an expected excess loss of at most   a sample
of size n = O(d/) sufﬁces. This result improves upon the best known fast convergence rates for
exp-concave functions by a O(log n) factor  and matches the lower bound of Shamir [19] for the
special case where the loss function is the squared loss. For this particular case  our result afﬁrms
the conjecture of Shamir [19] regarding the sample complexity of ERM for the squared loss; see
Section 2.1 below for details.
It is important to note that Theorem 1 establishes a fast convergence rate with respect to the actual
expected loss F itself  and not for a regularized version thereof (and in particular  not with respect

to the expectation of (cid:98)F ). Notably  the magnitude of the regularization we use is only O(1/n)  as

n) regularization used in standard regularized loss minimization methods

√
opposed to the O(1/
(that can only give rise to a traditional O(1/

√

n) rate).

2.1 Results for the Squared Loss

In this section we focus on the important special case where the loss function f is the squared loss 
2 (w·x − y)2 where x ∈ Rd is an instance vector and y ∈ R is a target value.
namely  f (w; x  y) = 1
This case  that was extensively studied in the past  was recently addressed by Shamir [19] who gave
lower bounds on the sample complexity of any learning algorithm under mild assumptions.

3

2(cid:107)w(cid:107)2

Shamir [19] analyzed learning with the squared loss in a setting where the domain is W = {w ∈
Rd : (cid:107)w(cid:107)2 ≤ B} for some constant B > 0  and the parameters distribution is supported over
|y| ≤ B}. It is not hard to verify that in this setup  for the
{x ∈ Rd : (cid:107)x(cid:107)2 ≤ 1} × {y ∈ R :
squared loss we can take β = 1  α = 4B2 and C = 2B2. Furthermore  if we choose the standard
Theorem 1 implies that the expected excess loss of the regularized ERM estimator (cid:98)w we deﬁned in
2 B2 for all w  w(cid:48) ∈ W. As a consequence 
regularizer R(w) = 1

2  we have |R(w)− R(w(cid:48))| ≤ 1

Eq. (3) is bounded by O(B2d/n).
√
On the other hand  standard uniform convergence results for generalized linear functions [e.g.  11]
show that  under the same conditions  ERM also enjoys an upper bound of O(B2/
n) over its
expected excess risk. Overall  we conclude:
2 (w· x − y)2 over the domain W = {w ∈ Rd :
Corollary 2. For the squared loss f (w; x  y) = 1
|y| ≤ B}  the regularized ERM
(cid:107)w(cid:107)2 ≤ B} with Z = {x ∈ Rd : (cid:107)x(cid:107)2 ≤ 1} × {y ∈ R :
(cid:18)

estimator (cid:98)w deﬁned in Eqs. (3) and (4) based on an i.i.d. sample of n instances has

(cid:27)(cid:19)

(cid:26) B2d

w∈W F (w) = O

min

 

B2√
n

n

.

E[F ((cid:98)w)] − min

This result slightly improves  by a log(n/d) factor  upon the bound conjectured by Shamir [19] for
the ERM estimator  and matches the lower bound proved therein up to constants.1 Previous fast-rate
results for ERM that we are aware of either included excess log factors [2] or were proven under
additional distributional assumptions [14  9]; see also the discussion in [19]. We remark that Shamir
conjectures this bound for ERM without any regularization. For the speciﬁc case of the squared loss 
it is indeed possible to obtain the same rates without regularizing; we defer details to the full version
of the paper. However  in practice  regularization has several additional beneﬁts: it renders the ERM
optimization problem well-posed (i.e.  ensures that the underlying matrix that needs to be inverted
is well-conditioned)  and guarantees it has a unique minimizer.

3 Proof of Theorem 1

Our proof of Theorem 1 proceeds as follows. First  we relate the expected excess risk of the ERM

estimator (cid:98)w to its average leave-one-out stability [4]. Then  we bound this stability in terms of
certain local properties of the empirical risk at the point (cid:98)w. To introduce the average stability notion

we study  we ﬁrst deﬁne for each i = 1  . . .   n the following empirical leave-one-out risk:

(cid:98)F i(w) =

(cid:88)

j(cid:54)=i

1
n

1
n

R(w)

f (w  zj) +

(i = 1  . . .   n) .

Namely  (cid:98)F i is the regularized empirical risk corresponding to the sample obtained by discard-
ing the instance zi. Then  for each i we let (cid:98)wi = arg minw∈W (cid:98)F i(w) be the ERM estimator
corresponding to (cid:98)F i. The average leave-one-out stability of (cid:98)w is then deﬁned as the quantity
(cid:80)n
i=1(f ((cid:98)wi  zi) − f ((cid:98)w  zi)).

1
n
Intuitively  the average leave-one-out stability serves as an unbiased estimator of the amount of
change in the expected loss of the ERM estimator when one of the instances z1  . . .   zn  chosen
uniformly at random  is removed from the training sample. We note that looking at the average
is crucial for us  and the stronger condition of (expected) uniform stability does not hold for exp-
concave functions. For further discussion of the various stability notions  refer to Bousquet and
Elisseeff [4].

Our main step in proving Theorem 1 involves bounding the average leave-one-out stability of (cid:98)w
Theorem 3 (average leave-one-out stability). For any z1  . . .   zn ∈ Z and for (cid:98)w1  . . .  (cid:98)wn and (cid:98)w as

deﬁned in Eq. (3)  which is the purpose of the next theorem.

deﬁned above  we have

n(cid:88)

i=1

1
n

(cid:0)f ((cid:98)wi  zi) − f ((cid:98)w  zi)(cid:1) ≤ 24βd

αn

+

100Cd

n

.

1We remark that Shamir’s result assumes two different bounds over the magnitude of the predictors w and
the target values y  while here we assume both are bounded by the same constant B. We did not attempt to
capture this reﬁned dependence on the two different parameters.

4

Before proving this theorem  we ﬁrst show how it can be used to obtain our main theorem. The proof
follows arguments similar to those of Bousquet and Elisseeff [4] and Shalev-Shwartz et al. [18].

Proof of Theorem 1. To obtain the stated result  it is enough to upper bound the expected excess loss

of (cid:98)wn  which is the minimizer of the regularized empirical risk over the i.i.d. sample {z1  . . .   zn−1}.

To this end  ﬁx an arbitrary w(cid:63) ∈ W. We ﬁrst write

R(w(cid:63)) = E[(cid:98)F (w(cid:63))] ≥ E[(cid:98)F ((cid:98)w)]  

1
n

F (w(cid:63)) +

which holds true since (cid:98)w is the minimizer of (cid:98)F over W. Hence 
E[F ((cid:98)wn)]− F (w(cid:63)) ≤ E[F ((cid:98)wn) − (cid:98)F ((cid:98)w)] +
Next  notice that the random variables (cid:98)w1  . . .  (cid:98)wn have exactly the same distribution: each is the
output of regularized ERM on an i.i.d. sample of n − 1 examples. Also  notice that (cid:98)wi  which is the

minimizer of the sample obtained by discarding the i’th example  is independent of zi. Thus  we
have

R(w(cid:63)) .

1
n

(5)

n(cid:88)

i=1

i=1

1
n

n(cid:88)
E[F ((cid:98)wi)] =
n(cid:88)
E[f ((cid:98)w  zi)] +

1
n

E[f ((cid:98)wi  zi)] .
E[R((cid:98)w)] .

1
n

1
n

E[F ((cid:98)wn)] =
E[(cid:98)F ((cid:98)w)] =
n(cid:88)

Furthermore  we can write

of the average stability:

Plugging these expressions into Eq. (5) gives a bound over the expected excess loss of (cid:98)wn in terms

i=1

E[F ((cid:98)wn)]− F (w(cid:63)) ≤ 1
Using Theorem 3 for bounding average stability term on the right-hand side  and our assumption
the expected excess loss of (cid:98)wn.
that supw w(cid:48)∈W |R(w) − R(w(cid:48))| ≤ B to bound the second term  we obtain the stated bound over

E[f ((cid:98)wi  zi) − f ((cid:98)w  zi)] +

E[R(w(cid:63)) − R((cid:98)w)] .

1
n

i=1

n

The remainder of the section is devoted to the proof of Theorem 3. Before we begin with the proof
of the theorem itself  we ﬁrst present a useful tool for analyzing the stability of minimizers of convex
functions  which we later apply to the empirical leave-one-out risks.

3.1 Local Strong Convexity and Stability

Our stability analysis for exp-concave functions is inspired by recent analysis techniques of
regularization-based online learning algorithms  that make use of strong convexity with respect to
local norms [1]. The crucial strong-convexity property is summarized in the following deﬁnition.
Deﬁnition 4 (Local strong convexity). We say that a function g : K (cid:55)→ R is locally σ-strongly-
convex over a domain K ⊆ Rd at x with respect to a norm (cid:107)·(cid:107)  if

∀ y ∈ K  

g(y) ≥ g(x) + ∇g(x)·(y − x) +

(cid:107)y − x(cid:107)2 .

σ
2

In words  a function is locally strongly-convex at x if it can be lower bounded (globally over its entire
domain) by a quadratic tangent to the function at x; the nature of the quadratic term in this lower
bound is determined by a choice of a local norm  which is typically adapted to the local properties
of the function at the point x.
With the above deﬁnition  we can now prove the following stability result for optima of convex
functions  that underlies our stability analysis for exp-concave functions.

5

Lemma 5. Let g1  g2 : K (cid:55)→ R be two convex functions deﬁned over a closed and convex domain
K ⊆ Rd  and let x1 ∈ arg minx∈K g1(x) and x2 ∈ arg minx∈K g2(x). Assume that g2 is locally
σ-strongly-convex at x1 with respect to a norm (cid:107)·(cid:107). Then  for h = g2 − g1 we have

Furthermore  if h is convex then

(cid:107)x2 − x1(cid:107) ≤ 2
σ

(cid:107)∇h(x1)(cid:107)∗ .

0 ≤ h(x1) − h(x2) ≤ 2
σ

(cid:0)(cid:107)∇h(x1)(cid:107)∗(cid:1)2

.

Proof. The local strong convexity of g2 at x1 implies

∇g2(x1)·(x1 − x2) ≥ g2(x1) − g2(x2) +

(cid:107)x2 − x1(cid:107)2 .

σ
2

Notice that g2(x1) − g2(x2) ≥ 0  since x2 is a minimizer of g2. Also  since x1 is a minimizer of g1 
ﬁrst-order optimality conditions imply that ∇g1(x1)·(x1 − x2) ≤ 0  whence

∇g2(x1)·(x1 − x2) = ∇g1(x1)·(x1 − x2) + ∇h(x1)·(x1 − x2) ≤ ∇h(x1)·(x1 − x2) .

Combining the observations yields

(cid:107)x2 − x1(cid:107)2 ≤ ∇h(x1)·(x1 − x2) ≤ (cid:107)∇h(x1)(cid:107)∗·(cid:107)x1 − x2(cid:107)  

σ
2

where we have used H¨older’s inequality in the last inequality. This gives the ﬁrst claim of the lemma.
To obtain the second claim  we ﬁrst observe that

g1(x2) + h(x2) ≤ g1(x1) + h(x1) ≤ g1(x2) + h(x1)

where we used the fact that x2 is the minimizer of g2 = g1 + h for the ﬁrst inequality  and the fact
that x1 is the minimizer of g1 for the second. This establishes the lower bound 0 ≤ h(x1) − h(x2).
For the upper bound  we use the assumed convexity of h to write

(cid:0)(cid:107)∇h(x1)(cid:107)∗(cid:1)2

 

h(x1) − h(x2) ≤ ∇h(x1)·(x1 − x2) ≤ (cid:107)∇h(x1)(cid:107)∗·(cid:107)x1 − x2(cid:107) ≤ 2
σ

where the second inequality follows from H¨older’s inequality  and the ﬁnal one from the ﬁrst claim
of the lemma.

3.2 Average Stability Analysis

σ Id +(cid:80)n

With Lemma 5 at hand  we now turn to prove Theorem 3. First  a few deﬁnitions are needed. For

brevity  we henceforth denote fi(·) = f (·  zi) for all i. We let hi = ∇fi((cid:98)w) be the gradient of fi
at the point (cid:98)w deﬁned in Eq. (3)  and let H = 1
j for
4C   α}. Finally  we will use (cid:107)·(cid:107)M to denote the norm induced by a positive
M induced by M simply
stability expressions fi((cid:98)wi)−fi((cid:98)w) in terms of a certain norm of the gradient hi of the corresponding
function fi. As the proof below reveals  this norm is the local norm at (cid:98)w with respect to which the
leave-one-out risk (cid:98)F i is locally strongly convex.

all i  where σ = 1
deﬁnite matrix M  i.e.  (cid:107)x(cid:107)M =
equals (cid:107)x(cid:107)M−1 =
xTM−1x.
In order to obtain an upper bound over the average stability  we ﬁrst bound each of the individual

xTM x. In this case  the dual norm (cid:107)x(cid:107)∗

σ Id +(cid:80)

2 min{ 1
√

i=1 hihT

i and Hi = 1

j(cid:54)=i hjhT

√

Lemma 6. For all i = 1  . . .   n it holds that

fi((cid:98)wi) − fi((cid:98)w) ≤ 6β

σ

(cid:0)(cid:107)hi(cid:107)∗

Hi

(cid:1)2

.

Notice that the expression on the right-hand side might be quite large for a particular function fi;
indeed  uniform stability does not hold in our case. However  as we show below  the average of these
expressions is small. The proof of Lemma 6 relies on Lemma 5 above and the following property of
exp-concave functions  established by Hazan et al. [8].

6

.

σ
2

(6)

2 min{ 1

∀ x  y ∈ K  

Lemma 7 (Hazan et al. [8]  Lemma 3). Let f : K (cid:55)→ R be an α-exp-concave function over a convex
domain K ⊆ Rd such that |f (x) − f (y)| ≤ C for any x  y ∈ K. Then for any σ ≤ 1
4C   α} it
holds that

(cid:0)∇f (x)·(y − x)(cid:1)2
Proof of Lemma 6. We apply Lemma 5 with g1 = (cid:98)F and g2 = (cid:98)F i (so that h = − 1
ﬁrst verify that (cid:98)F i is indeed (σ/n)-strongly-convex at (cid:98)w with respect to the norm (cid:107)·(cid:107)Hi. Since each

f (y) ≥ f (x) + ∇f (x)·(y − x) +

(7)
4C   α}. Also  the strong convexity of the regularizer R implies that
(8)

fi is α-exp-concave  Lemma 7 shows that for all w ∈ W 

(cid:0)hi·(w − (cid:98)w)(cid:1)2
fi(w) ≥ fi((cid:98)w) + ∇fi((cid:98)w)·(w − (cid:98)w) +
2 min{ 1
(cid:107)w − (cid:98)w(cid:107)2
R(w) ≥ R((cid:98)w) + ∇R((cid:98)w)·(w − (cid:98)w) +
(cid:88)
(cid:0)hi·(w − (cid:98)w)(cid:1)2
(cid:107)w − (cid:98)w(cid:107)2

(cid:98)F i(w) ≥ (cid:98)F i((cid:98)w) + ∇(cid:98)F i((cid:98)w)·(w − (cid:98)w) +
= (cid:98)F i((cid:98)w) + ∇(cid:98)F i((cid:98)w)·(w − (cid:98)w) +

2 .
Summing Eq. (7) over all j (cid:54)= i with Eq. (8) and dividing through by n gives

(cid:107)w − (cid:98)w(cid:107)2

2

with our choice of σ = 1

n fi). We should

1
2n

σ
2n

j(cid:54)=i

σ
2

1
2

+

Hi

 

 

σ
2n

which establishes the strong convexity.
Now  applying Lemma 5 gives

.

Hi

Hi

=

2
σ

(cid:107)hi(cid:107)∗

(cid:107)∇h((cid:98)w)(cid:107)∗

σ
On the other hand  since fi is convex  we have

(cid:107)(cid:98)wi − (cid:98)w(cid:107)Hi ≤ 2n
fi((cid:98)wi) − fi((cid:98)w) ≤ ∇fi((cid:98)wi)·((cid:98)wi − (cid:98)w)
= ∇fi((cid:98)w)·((cid:98)wi − (cid:98)w) +(cid:0)∇fi((cid:98)wi) − ∇fi((cid:98)w)(cid:1)·((cid:98)wi − (cid:98)w) .
(cid:1)2
(cid:0)(cid:107)hi(cid:107)∗
∇fi((cid:98)wi)·((cid:98)wi − (cid:98)w) = hi·((cid:98)wi − (cid:98)w) ≤ (cid:107)hi(cid:107)∗
(cid:0)∇fi((cid:98)wi) − ∇fi((cid:98)w)(cid:1)·((cid:98)wi − (cid:98)w) ≤ (cid:107)∇fi((cid:98)wi) − ∇fi((cid:98)w)(cid:107)2·(cid:107)(cid:98)wi − (cid:98)w(cid:107)2 ≤ β(cid:107)(cid:98)wi − (cid:98)w(cid:107)2

·(cid:107)(cid:98)wi − (cid:98)w(cid:107)Hi ≤ 2

Also  since fi is β-smooth (with respect to the Euclidean norm)  we can bound the second term in
Eq. (10) as follows:

The ﬁrst term can be bounded using H¨older’s inequality and Eq. (9) as

(9)

(10)

2  

Hi

Hi

σ

.

and since Hi (cid:23) (1/σ)Id  we can further bound using Eq. (9) 
≤ 4
σ

2 ≤ σ(cid:107)(cid:98)wi − (cid:98)w(cid:107)2

(cid:107)(cid:98)wi − (cid:98)w(cid:107)2

Hi

(cid:0)(cid:107)hi(cid:107)∗

Hi

(cid:1)2

.

Combining the bounds (and simplifying using our assumption β ≥ 1) gives the lemma.

Next  we bound a sum involving the local-norm terms introduced in Lemma 6.
Lemma 8. Let I = {i ∈ [n] : (cid:107)hi(cid:107)∗

H > 1

(cid:88)
2}. Then |I| ≤ 2d  and we have

(cid:1)2 ≤ 2d .

(cid:0)(cid:107)hi(cid:107)∗

Hi

(cid:80)
i H−1hi for all i = 1  . . .   n. First  we claim that ai > 0 for all i  and
i ai ≤ d. The fact that ai > 0 is evident from H−1 being positive-deﬁnite. For the sum of the

Proof. Denote ai = hT

ai =

i H−1hi =
hT

tr(H−1hihT

i ) ≤ tr(H−1H) = tr(Id) = d  

(11)

i /∈I

n(cid:88)

ai’s  we write:

n(cid:88)

n(cid:88)

i=1

i=1

i=1

7

where we have used the linearity of the trace  and the fact that H (cid:23)(cid:80)n
(cid:80)
i∈I ai =(cid:80)

Now  our claim that |I| ≤ 2d is evident: if (cid:107)hi(cid:107)∗
our second claim  we ﬁrst write Hi = H − hihT
obtain

2 for more than 2d terms  then the sum
i H−1hi must be larger than d  which is a contradiction to Eq. (11). To prove
i and use the Sherman-Morrison identity [e.g.  7] to

i .
i=1 hihT

H > 1

i∈I hT

i = (H − hihT
H−1
for all i /∈ I. Note that for i /∈ I we have hT
on the right-hand side is well deﬁned. We therefore have:

i )−1 = H−1 +
i H−1hi < 1  so that the identity applies and the inverse

H−1hihT
1 − hT

i H−1
i H−1hi

= hT

i H−1

i hi = hT

i H−1hi +

= ai +

a2
i
1 − ai

≤ 2ai  

where the inequality follows from the fact that 1 − ai ≥ ai for i /∈ I. Summing this inequality over
i /∈ I and recalling that the ai’s are nonnegative  we obtain
ai ≤ 2

(cid:1)2 ≤ 2

(cid:0)(cid:107)hi(cid:107)∗

n(cid:88)

(cid:88)

(cid:88)

ai = 2d  

Hi

i /∈I

i /∈I

i=1

(cid:0)(cid:107)hi(cid:107)∗

Hi

(cid:1)2

(cid:1)2

(cid:0)hT

i H−1hi

1 − hT

i H−1hi

which concludes the proof.

Theorem 3 is now obtained as an immediate consequence of our lemmas above.

Proof of Theorem 3. As a consequence of Lemmas 6 and 8  we have

1
n

(cid:88)
(cid:0)fi((cid:98)wi) − fi((cid:98)w)(cid:1) ≤ 6β

(cid:0)fi((cid:98)wi) − fi((cid:98)w)(cid:1) ≤ C|I|
(cid:88)
(cid:0)(cid:107)hi(cid:107)∗

i∈I

n

≤ 2Cd
n

 

(cid:1)2 ≤ 12βd

σn

.

σn

i /∈I
α} ≤ 2(4C + 1
σ = 2 max{4C  1

Hi

α ) gives the result.

and

(cid:88)

i /∈I

1
n

Summing the inequalities and using 1

4 Conclusions and Open Problems

We have proved the ﬁrst fast convergence rate for a regularized ERM procedure for exp-concave
loss functions. Our bounds match the existing lower bounds in the speciﬁc case of the squared loss
up to constants  and improve by a logarithmic factor upon the best known upper bounds achieved by
online methods.
Our stability analysis required us to assume smoothness of the loss functions  in addition to their
exp-concavity. We note  however  that the Online Newton Step algorithm of Hazan et al. [8] for
online exp-concave optimization does not require such an assumption. Even though most of the
popular exp-concave loss functions are also smooth  it would be interesting to understand whether
smoothness is indeed required for the convergence of the ERM estimator we study in the present
paper  or whether it is simply a limitation of our analysis.
Another interesting issue left open in our work is how to obtain bounds on the excess risk of ERM
that hold with high probability  and not only in expectation. Since the excess risk is non-negative 
one can always apply Markov’s inequality to obtain a bound that holds with probability 1 − δ but
scales linearly with 1/δ. Also  using standard concentration inequalities (or success ampliﬁcation

techniques)  we may also obtain high probability bounds that scale with(cid:112)log(1/δ)/n  losing the

fast 1/n rate. We leave the problem of obtaining bounds that depends both linearly on 1/n and
logarithmically on 1/δ for future work.

8

References
[1] J. D. Abernethy  E. Hazan  and A. Rakhlin. Interior-point methods for full-information and

bandit online learning. Information Theory  IEEE Transactions on  58(7):4164–4175  2012.

[2] M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. cambridge

university press  2009.

[3] K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the

exponential family of distributions. Machine Learning  43(3):211–246  2001.

[4] O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning

Research  2:499–526  2002.

[5] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge University Press 

2006.

[6] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning

algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[7] G. H. Golub and C. F. Van Loan. Matrix computations  volume 3. JHU Press  2012.
[8] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimiza-

tion. Machine Learning  69(2-3):169–192  2007.

[9] D. Hsu  S. M. Kakade  and T. Zhang. Random design analysis of ridge regression. Foundations

of Computational Mathematics  14(3):569–600  2014.

[10] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in Neural Information Processing Systems  pages 315–323  2013.

[11] S. M. Kakade  K. Sridharan  and A. Tewari. On the complexity of linear prediction: Risk
In Advances in neural information processing

bounds  margin bounds  and regularization.
systems  pages 793–800  2009.

[12] J. Kivinen and M. K. Warmuth. Averaging expert predictions. In Computational Learning

Theory  pages 153–167. Springer  1999.

[13] T. Koren. Open problem: Fast stochastic exp-concave optimization. In Conference on Learning

Theory  pages 1073–1075  2013.

[14] G. Lecu´e and S. Mendelson. Performance of empirical risk minimization in linear aggregation.

arXiv preprint arXiv:1402.5763  2014.

[15] M. Mahdavi  L. Zhang  and R. Jin. Lower and upper bounds on the generalization of stochas-
tic exponentially concave optimization. In Proceedings of The 28th Conference on Learning
Theory  2015.

[16] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. The Journal of Machine Learning Research  14(1):567–599  2013.

[17] S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for

regularized loss minimization. Mathematical Programming  pages 1–41  2014.

[18] S. Shalev-Shwartz  O. Shamir  N. Srebro  and K. Sridharan. Learnability  stability and uniform

convergence. The Journal of Machine Learning Research  11:2635–2670  2010.

[19] O. Shamir. The sample complexity of learning linear predictors with the squared loss. arXiv

preprint arXiv:1406.5143  2014.

[20] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise and fast rates. In Advances in

neural information processing systems  pages 2199–2207  2010.

[21] K. Sridharan  S. Shalev-Shwartz  and N. Srebro. Fast rates for regularized objectives.

Advances in Neural Information Processing Systems  pages 1545–1552  2009.

In

[22] V. Vovk. Competitive on-line statistics. International Statistical Review  69(2):213–248  2001.

9

,Tomer Koren
Kfir Levy