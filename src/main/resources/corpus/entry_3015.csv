2018,Learning Temporal Point Processes via Reinforcement Learning,Social goods  such as healthcare  smart city  and information networks  often produce ordered event data in continuous time. The generative processes of these event data can be very complex  requiring flexible models to capture their dynamics. Temporal point processes offer an elegant framework for modeling event data without discretizing the time. However  the existing maximum-likelihood-estimation (MLE) learning paradigm requires hand-crafting the intensity function beforehand and cannot directly monitor the goodness-of-fit of the estimated model in the process of training. To alleviate the risk of model-misspecification in MLE  we propose to generate samples from the generative model and monitor the quality of the samples in the process of training until the samples and the real data are indistinguishable. We take inspiration from reinforcement learning (RL) and treat the generation of each event as the action taken by a stochastic policy. We parameterize the policy as a flexible recurrent neural network and gradually improve the policy to mimic the observed event distribution. Since the reward function is unknown in this setting  we uncover an analytic and nonparametric form of the reward function using an inverse reinforcement learning formulation. This new RL framework allows us to derive an efficient policy gradient algorithm for learning flexible point process models  and we show that it performs well in both synthetic and real data.,Learning Temporal Point Processes via Reinforcement Learning

Shuang Li∗1  Shuai Xiao 2  Shixiang Zhu1  Nan Du3  Yao Xie1  and Le Song1 2

1Georgia Institute of Technology

2Ant Financial
3Google Brain

Abstract

Social goods  such as healthcare  smart city  and information networks  often pro-
duce ordered event data in continuous time. The generative processes of these event
data can be very complex  requiring ﬂexible models to capture their dynamics.
Temporal point processes offer an elegant framework for modeling event data with-
out discretizing the time. However  the existing maximum-likelihood-estimation
(MLE) learning paradigm requires hand-crafting the intensity function beforehand
and cannot directly monitor the goodness-of-ﬁt of the estimated model in the
process of training. To alleviate the risk of model-misspeciﬁcation in MLE  we
propose to generate samples from the generative model and monitor the quality
of the samples in the process of training until the samples and the real data are
indistinguishable. We take inspiration from reinforcement learning (RL) and treat
the generation of each event as the action taken by a stochastic policy. We param-
eterize the policy as a ﬂexible recurrent neural network and gradually improve
the policy to mimic the observed event distribution. Since the reward function is
unknown in this setting  we uncover an analytic and nonparametric form of the
reward function using an inverse reinforcement learning formulation. This new RL
framework allows us to derive an efﬁcient policy gradient algorithm for learning
ﬂexible point process models  and we show that it performs well in both synthetic
and real data.

Introduction

1
Many natural and artiﬁcial systems produce a large volume of discrete events occurring in continuous
time  for example  the occurrence of crime events  earthquakes  patient visits to hospitals  ﬁnancial
transactions  and user behavior in mobile applications [5]. It is essential to understand and model these
complex and intricate event dynamics so that accurate prediction  recommendation or intervention
can be carried out subsequently depending on the context.
Temporal point processes offer an elegant mathematical framework for modeling the generative
processes of these event data. Typically  parametric (or semi-parametric) assumptions are made on the
intensity function [11  9] based on prior knowledge of the processes  and the maximum-likelihood-
estimation (MLE) is used to ﬁt the model parameters from data. These models often work well when
the parametric assumptions are correct. However  in many cases where the real event generative
process is unknown  these parametric assumptions may be too restricted and do not reﬂect the reality.
Thus there emerge some recent efforts in increasing the expressiveness of the intensity function using
nonparametric forms [7] and recurrent neural networks [6  19]. However  these more sophisticated
models still rely on maximizing the likelihood which now involves intractable integrals and needs to
be approximated. Most recently  [27] proposed to bypass the problem of maximum likelihood by
adopting a generative adversarial network (GAN) framework  where a recurrent neural network is
∗Correspondence to: Shuang Li <sli370@gatech.edu>  Yao Xie <yao.xie@isye.gatech.edu>  and Le Song

<lsong@cc.gatech.edu>

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

learned to transform event sequence from a Poisson process to the target event sequence. However 
this approach is rather computationally intensive  since it requires ﬁtting another recurrent neural
network as the discriminator  and it takes many iterations and careful tuning for both neural networks
to reach equilibrium.
In this paper  we take a new perspective and establish an under-explored connection between temporal
point processes and reinforcement learning: the generation of each event can be treated as the action
taken by a stochastic policy  and the intensity function learning problem in temporal point processes
can be viewed as the policy learning problem in reinforcement learning.
More speciﬁcally  we parameterize a stochastic policy
π using a recurrent neural network over event history
and learn the unknown reward function via inverse re-
inforcement learning [1  20  28  15]. Our algorithm for
policy optimization iterates between learning the reward
function and the stochastic policy π. Inverse reinforce-
ment learning is known to be time-consuming  which
requires solving a reinforcement learning problem in
every inner-loop. To tackle this problem  we convert the
inverse reinforcement learning step to a minimization
problem over the discrepancy between the expert point
process and the learner point process. By choosing the
function class of reward to be the unit ball in reproduc-
ing kernel Hilbert space (RKHS) [13  16  8]  we can get
an explicit nonparametric closed form for the optimal re-
ward function. Then the stochastic policy can be learned
by a customized policy gradient with the optimal reward
function having an analytical expression. An illustration
of our modeling framework is shown in Figure 1.
We conducted experiments on various synthetic and real
sequences of event data and showed that our approach
outperforms the state-of-the-art regarding both data de-
scription and computational efﬁciency.
2 Preliminaries
Temporal Point Processes. A temporal point process is a stochastic process whose realization is a
sequence of discrete events {ti} with ti ∈ R+ and i ∈ Z+ abstracted as points on a timeline [5]. Let
the history st = {t1  t2  . . .   tn|tn < t} be the sequence of event times up to but not including time t.
The intensity function (rate function) λ(t|st) conditioned on the history st uniquely characterizes the
generative process of the events. Different functional forms of λ(t|st)dt capture different generating
patterns of events. For example  a plain homogeneous Poisson process has λ(t|st) = λ0 (cid:62) 0 
implying that each event occurs independently of each and uniformly on the timeline. A Hawkes
exp(−(t − ti)) where the occurrences of past events will
boost future occurrences. Given the intensity function  the survival function deﬁned as S(t|st) =
λ(τ )dτ ) is the conditional probability that no event occurs in the window [tn  t)  and
the likelihood of observing event at time t is deﬁned as f (t|st) = λ(t|st)S(t|st). Then we can
express the joint likelihood of observing a sequence of events sT = {t1  t2  . . .   tn|tn < T} up to an
observation window T as

Figure 1: Illustration of our modeling frame-
work. The observed trajectories of events will
be viewed as the actions generated by an expert
policy πE. The goal is to learn a policy which
we call learner that mimics the distribution
of the observed expert event sequences. The
learner policy π(a|st) provides the probabil-
ity of the next event occurring at a after t  and
st := {ti}ti<t is the history of events before
t. We parametrize π(a|st) by a recurrent neu-
ral network (RNN) with stochastic neurons [4] 
where the generated events are fed back to the
RNN leading to a double stochastic point pro-
cess [12]. Furthermore  each generated event
ti will be also associated with a reward r(ti) 
and the policy will be learned by maximizing
the expected cumulative rewards [26].

process has λ(t|st) = λ0 +(cid:80)
exp(−(cid:82) t

ti∈st

tn

p({t1  t2  . . .   tn|tn < T}) =

λ(ti|sti) · exp

−

λ(τ|sτ )dτ

(cid:89)

ti∈sT

(cid:32)

(cid:90) T

0

(cid:33)

.

(1)

The integral normalization in the likelihood function can be intensive to compute especially in cases
where λ(t|st) do not have a simple form. In this case  a numerical approximation is typically needed
which may affect the accuracy of the ﬁtting process.
Reproducing Kernel Hilbert Spaces. A reproducing kernel Hilbert space (RKHS) H on T
with a kernel k(t  t(cid:48)) is a Hilbert space of functions f (·) : T (cid:55)→ R with inner product (cid:104)· ·(cid:105)H.
Its element k(t ·) satisﬁes the reproducing property: (cid:104)f (·)  k(t ·)(cid:105)H = f (t)  and consequently 
(cid:104)k(t ·)  k(t(cid:48) ·)(cid:105)H = k(t  t(cid:48))  meaning that we can view the evaluation of a function f at any point

2

0	t1t2t3stπ(a|st)0	t1t2t3t4aPolicy	Gradient	πEr(t1)r(t2)r(t3)r(t4)Updatereward	T   we deﬁne the mapping of P to RKHS  µP := EP[k(t ·)] = (cid:82)

t ∈ T as an inner product. Commonly used RKHS kernel function includes Gaussian radial basis
function (RBF) kernel k(t  t(cid:48)) = exp(−(cid:107)t − t(cid:48)(cid:107)2 /2σ2) where σ > 0 is the kernel bandwidth  and
polynomial kernel k(t  t(cid:48)) = ((cid:104)t  t(cid:48)(cid:105) + a)d where a > 0 and d ∈ N [23  3  13]. In this paper  if
not otherwise stated  we will assume that Gaussian RBF kernel is used. Let P be a measure on
t∈T k(t ·) dP(t)  as the Hilbert
space embedding of P [24]. Then for all f ∈ H  EP[f (t)] = (cid:104)f  µP(cid:105)H by the reproducing property.
Similarly  one can also embed another measure Q on T into RKHS as µQ. Then a distance between
measure P and Q can be deﬁned as (cid:107)µP − µQ(cid:107)H := sup(cid:107)f(cid:107)H(cid:54)1 (cid:104)f  µP − µQ(cid:105)H. A characteristic
RKHS is one for which the embedding is injective: that is  each measure has a unique embedding [25] 
and (cid:107)µP − µQ(cid:107)H = 0 if and only if P = Q. This property holds for many commonly used kernels.
For T = Rd  this includes the Gaussian kernels.
3 A Reinforcement Learning Framework
Suppose we are interested in modeling the daily crime patterns  or monthly occurrences of disease
for patients  then the data are collected as trajectories of events within a predeﬁned time window T .
We regard the observed paths as actions taken by an expert (nature).
Let ξ = {τ1  τ2  . . .   τN ξ
T is the
total number of events up to T   and it can be different for different sequences. Then  each trajectory
ξ ∼ πE can be seen as an expert demonstration sampled from the expert policy πE. Hence  on a high
level  given a set of expert demonstrations D = {ξ1  ξ2  . . .   ξj  . . .|ξj ∼ πE}  we can treat ﬁtting a
temporal point process to D as searching for a learner policy πθ which can generate another set of
sequences ˜D = {η1  η2  . . .   ηj  . . .|ηj ∼ πθ} with similar patterns as D. We will elaborate on this
reinforcement learning framework below.
Reinforcement Learning Formulation (RL). Given a sequence of past events st = {ti}ti<t  the
stochastic policy πθ(a|st) samples an inter-event time a as its action to generate the next event
time as ti+1 = ti + a. Then  a reward r(ti+1) is provided and the state st will be updated to
st = {t1  . . .   ti  ti+1}. Fundamentally  the policy πθ(a|st) corresponds to the conditional probability
of the next event time in temporal point process  which in turn uniquely determines the corresponding
πθ (t−ti|sti )
intensity function as λθ(t|sti ) =
πθ (τ−ti|sti )dτ . This builds the connection between the intensity
ti
function in temporal point processes and the stochastic policy in reinforcement learning. If reward
function r(t) is given  the optimal policy π∗

} represent a single trajectory of events from the expert where N ξ

θ can be directly computed via

1−(cid:82) t

T

(cid:20)(cid:88)N η

T

i=1

(cid:21)

 

(2)
r(ti)
} is one sampled roll-out from

π∗
θ = arg max

πθ∈G J(πθ) := Eη∼πθ

(cid:18)

(cid:20)(cid:88)N ξ

(cid:21)

(cid:20)(cid:88)N η

T

(cid:21)(cid:19)

T can be different for different roll-out samples.

where G is the family of all candidate policies πθ  η = {t1  . . .   tN η
policy πθ  and N η
Inverse Reinforcement Learning (IRL). Eq.(2) shows that when the reward function is given  the
optimal policy can be determined by maximizing the expected cumulative reward. However  in our
case  only the expert’s sequences of events can be observed  but the real reward function is unknown.
Given the expert policy πE  IRL can help to uncover the optimal reward function r∗(t) by

T

T

i=1

r(τi)

− max
πθ∈G

r∗ = max
r∈F

Eξ∼πE

Eη∼πθ
(3)
} is one event sequence generated
where F is the family class for reward function  ξ = {τ1  . . .   τN ξ
by the expert πE  and η = {t1  . . .   tN η
} is one roll-out sequence from the learner πθ. The formula-
tion means that a proper reward function should give the expert policy higher reward than any other
learner policy in G  and thus the learner can approach the expert performance by maximizing this
reward. Denote the procedure (2) and (3) as RL(r) and IRL(πE)  accordingly. The optimal policy
can be obtained by

r(ti)

i=1

 

T

T

θ = RL ◦ IRL(πE).
π∗

(4)
Overview of the Proposed Learning Framework. Solving the optimization problem (3) is very
time-consuming in that it requires to solve the inner loop RL problem repeatedly. We relieve the
computational challenge by choosing the space of functions F for r(t) to be the unit ball in RKHS
H  which allows us to obtain an analytical expression for the updated reward function ˆr(t) given any

3

current learner policy ˆπ(θ). This ˆr(t) is determined by ﬁnite sample expert trajectories and ﬁnite
sample roll-outs from the current learner policy  and it directly quantiﬁes the discrepancy between
the expert’s policy (or intensity function) and current learner policy (or intensity function). Then by
solving a simple RL problem as in (2)  the learner policy can be improved to close its gap with the
expert policy using a simple policy gradient type of algorithm.

4 Model

In this section  we present model parametrization and the analytical expression of optimal reward
function.
Policy Network. The function class of the policy πθ ∈ G should be ﬂexible and expressive enough to
capture the potential complex point process patterns of the expert. We  therefore  adopt the recurrent
neural network (RNN) with stochastic neurons [4] which is ﬂexible to capture the nonlinear and
long-range sequential dependency structure. More speciﬁcally 

ai ∼ π(a| Θ(hi−1))  hi = ψ(V ai + W hi−1)  h0 = 0 

(5)
where the hidden state hi ∈ Rd encodes the sequence of past events {t1  . . .   ti}  ai ∈ R+  V ∈ Rd 
and W ∈ Rd×d. Here ψ is a nonlinear activation function applied element-wise  and Θ is a nonlinear
mapping from Rd to the parameter space of the probability distribution π. For instance  one can
choose ψ(z) = ez−e−z
ez+e−z to be the tanh function  and design the output layer of Θ such that Θ(hi−1)
is a valid parameter for a probability density function π. The output ai = ti − ti−1  serves as the
i-th inter-event time (let t0 = 0)  and ai > 0. The choice of model π is quite ﬂexible  only with
the constraint that the random variable should be positive since a is always positive. Common
distributions such as exponential and Rayleigh distributions would satisfy such constraint  leading
to π(a|Θ(hi−1)) = Θ(h)e−Θ(h)a and π(a|Θ(hi−1)) = Θ(h)ae−Θ(h)a2/2 respectively. In this way 
we specify a nonlinear and ﬂexible dependency over the history.
The architecture of our model in (5) is shown in Fig-
ure 2. Different from traditional RNN  the outputs
ai are sampled from π rather than obtained by deter-
ministic transformations. This is what “stochastic"
policy means. Randomly sampling will allow the
policy to explore the temporary space. Furthermore 
the sampled time point will be fed back to the RNN.
The proposed model aims to capture that the state hi
is attributed by two parts. One is the deterministic inﬂuence from the previous hidden state hi−1  and
the other is the stochastic inﬂuence from the latest sampled action ai. Action ai is sampled from the
previous distribution π(a|Θ(hi−1)) with parameter Θ(hi−1) and will be fed back to inﬂuence the
current hidden state hi.
In some sense  our RNN with stochastic neurons mimics the event generating mechanism of the
doubly stochastic point process  such as Hawkes process and self-correcting process. For these types
of point processes  the intensity is stochastic  which depends on history  and the intensity function
will control the occurrence rate of the next event.
Reward Function Class. The reward function directly quantiﬁes the discrepancy between πE and
πθ  and it guides the learning of the optimal policy π∗
θ. On the one hand  we want its function class
r ∈ F to be sufﬁciently ﬂexible so that it can represent the reward function of various shapes. On
the other hand  it should be restrictive enough to be efﬁciently learned with ﬁnite samples [3  13].
With these competing considerations  we choose F to be the unit ball in RKHS H  (cid:107)r(cid:107)H (cid:54) 1. An
immediate beneﬁt of this function class is that we can show the optimal policy can be directly learned
via a minimization formulation given in Theorem 1 instead of the original minimax formulation (3).
A sketch of proof is provided as follows. For short notation  we denote

Figure 2: Illustration of generator πθ.

k(t ·)dN (η)

 

t

and

µπθ := Eη∼πθ [φ(η)]

(cid:124)

(cid:123)(cid:122)

(cid:125)

feature mapping from data space to R

mean embeddings of the intensity function in RKHS

(cid:90)

(cid:123)(cid:122)

[0 T )

φ(η) :=

(cid:124)

(cid:125)

4

hi−1a1h0h1ai−1hiaihi+1ai+1……anhnan+10T……t1ti−1titi+1tnN (η)
T(cid:88)

 = Eη∼πθ

(cid:34)(cid:90)

(cid:35)

where dN (η)
kernel. Then using the reproducing property 

t

is the counting process associated with sample path η  and k(t  t(cid:48)) is a universal RKHS

J(πθ) := Eη∼πθ

(cid:104)r  k (t ·)(cid:105)HdN (η)
Similarly  we can obtain J(πE) = (cid:104)r  µπE(cid:105)H. From (3)  r∗ is obtained by

r(ti)

[0 T )

i=1

t

= (cid:104)r  µπθ(cid:105)H.

max
(cid:107)r(cid:107)H≤1

πθ∈G (cid:104)r  µπE − µπθ(cid:105)H = min

min

(cid:104)r  µπE − µπθ(cid:105)H = min

πθ∈G (cid:107)µπE − µπθ(cid:107)H 

where the ﬁrst equality is guaranteed by the minimax theorem  and

r∗(·|πE  πθ) =

∝ µπE − µπθ

(6)

πθ∈G max
(cid:107)r(cid:107)H≤1
µπE − µπθ
(cid:107)µπE − µπθ(cid:107)H

can be empirically evaluated by data. In this way  we change the original minimax formulation for
solving π∗
θ to a simple minimization problem  which will be more efﬁcient and stable to solve in
practice. We summarize the formulation in Theorem 1.
Theorem 1 Let the family of reward function be the unit ball in RKHS H  i.e.  (cid:107)r(cid:107)H (cid:54) 1. Then the
optimal policy obtained by (4) can also be obtained by solving

(7)
where D(πE  πθ H) is the maximum expected cumulative reward discrepancy between πE and πθ 

π∗
θ = arg min

πθ∈G D(πE  πθ H)
(cid:20)(cid:88)N (ξ)
(cid:21)

T

r(τi)

i=1

− Eη∼πθ

(cid:20)(cid:88)N (η)

T

i=1

D(πE  πθ H) := max
(cid:107)r(cid:107)H(cid:54)1

Eξ∼πE

r(ti)

.

(8)

(cid:21)(cid:19)

(cid:18)

Theorem 1 implies that we can transform the inverse reinforcement learning procedure of (4) to a
simple minimization problem which minimizes the maximum expected cumulative reward discrep-
ancy between πE and πθ. This enables us to sidestep the expensive computation of (4) caused by
the solving the inner RL problem repeatedly. What’s more interesting  we can derive an analytical
solution to (8) given by (6).
Finite Sample Estimation. Given L trajectories of expert point processes  and M trajectories
of events generated by πθ  mean embeddings µπE and µπθ can be estimated by their respective
 ·). Then for
empirical mean: ˆµπE = 1
any t ∈ [0  T )  the estimated optimal reward is (without normalization) is
L

(cid:80)N (m)

i=1 k(t(m)

i=1 k(τ (l)

l=1

M

T

T

i

i

ˆr∗(t) ∝ 1
L

k(τ (l)

i

  t) − 1
M
and t(m)
. Unbiased estimator can also be obtained and

k(t(m)

  t).

(9)

m=1

i=1

i

i

Note this empirical estimator is biased at τ (l)
will be provided in Algorithm RLPP discussed later for simplicity.
Kernel Choice. The unit ball in RKHS is dense and expressive. Fundamentally  our proposed
framework and theoretical results are general and can be directly applied to other types of kernels.
For example  we can use the Matérn kernel  which generates spaces of differentiable functions known
as the Sobolev spaces [10  2]. In later experiments  we have used Gaussian kernel and obtained
promising results.

i

(cid:80)L
(cid:88)L

(cid:80)N (l)
(cid:88)N (l)

T

l=1

i=1

 ·) and ˆµπθ = 1
(cid:88)M

(cid:80)M
(cid:88)N (m)

m=1

T

5 Learning Algorithm
Learning via Policy Gradient. In practice  instead of minimizing D(πE  πθ H) as in (7)  we can
equivalently minimize D(πE  πθ H)2 since square is a monotonic transformation. Now  we can
learn π∗
θ from the RL formulation (2) using policy gradient with variance reduction. First  with the
likelihood ratio trick  the gradient of ∇θD(πE  πθ H)2 can be computed as

(cid:18)(cid:88)N η

T

ˆr∗(ti)

i=1

(cid:19)  

(10)

∇θD(πE  πθ H)2 = Eη∼πθ

(∇θ log πθ(ai|Θ(hi−1))) ·

where (cid:80)N η

T

i=1 (∇θ log πθ(ai|Θ(hi−1))) is the gradient of the log-likelihood of a roll-out sample

η = {t1  . . .   tN η

} using the learner policy πθ.

T

 N η
T(cid:88)

i=1

5

Algorithm RLPP: Mini-batch Reinforcement Learning
for Learning Point Processes
1. Initialize model parameters θ;
2. For number of training iterations do

1

};

};

N

(l)
T

  . . .   t(m)
NT

1   . . .   τ (l)

(cid:88)M

(cid:18)(cid:88)N

• Sample minibatch of L trajectories of events
{ξ(1)  . . .   ξ(L)} from expert policy πE  where
ξ(l) = {τ (l)
• Sample minibatch of M trajectories of events
{η(1)  . . .   η(M )} from learner policy πθ  where
η(m) = {t(m)
(cid:19)
• Estimate policy gradient ∇θD(πE  πθ H)2 as
where log pθ(η(m)) =(cid:80)N η
∇θ
∗
) log pθ(η(m))
ˆr
i=1 (log πθ(ai|Θ(hi−1)))
is the log-likelihood of the sample η(m)  and
r∗(t(m)
) can be estimated by L expert trajectories
(cid:88)N
and (M − 1) roll-out samples without η(m)
∗
M(cid:88)
ˆr

(cid:88)N

(cid:88)L

k(τ (l)
i
(m(cid:48) )

(t(m)) =

1
M

(t(m)

i

(m)
T

l=1

i=1

m=1

i=1

  t)
k(t(m(cid:48))

j

  t);

1
L
− 1

M − 1

T

(l)
T

i

(a)

(b)

Figure 3: The reward function ˆr∗(t) is estimated us-
ing 100 sampled sequences from πE and πθ. In (a) 
ˆr∗(t) > 0 when the expert’s intensity is above the
learner’s intensity  and ˆr∗(t) < 0 when the expert’s
intensity is below the learner’s intensity. In order to
maximize the cumulative reward given the current
reward  the learner should generate more events in
the region when ˆr∗(t) > 0 and reduce the number
of events when ˆr∗(t) < 0. Based on our formula-
tion  the optimal reward function always quantiﬁes the
discrepancy between the expert and current learner
by considering the worst case. As a result  once the
learner is changed  the current optimal reward ˆr∗(t)
is updated accordingly  and ˆr∗(t) guides the learner
to update its policy towards mimicking the expert’s
behavior until they exactly match each other in (b)
where ˆr∗(t) becomes zero.

T

j=1

m(cid:48)=1 m(cid:48)(cid:54)=m
• Update policy parameters as

θ ← θ + α∇θD(πE  πθ H)2.

T

T

where

(cid:16)(cid:80)NT

(cid:17)
l=i ˆr∗(tl)

(cid:17)(cid:105)
l=i [ˆr∗(tl) − bl]

i=1 (∇θ log πθ(ai|Θ(hi−1))) ·(cid:16)(cid:80)N η
(cid:104)(cid:80)N η

To reduce the variance of the gradient  we can exploit the observation that future actions do not
depend on past rewards. This leads to a variance reduced gradient estimate ∇θD(πE  πθ H)2 =
Eη∼πθ
is referred to
as the “reward to go" and bl is the baseline to further reduce the variance. The overall procedure is
given in Algorithm RLPP. In the algorithm  after we sample M trajectories from the current policy 
we use one trajectory ηm for evaluation and the rest M − 1 samples to estimate reward function. An
example reward function learned at a different stage of the algorithm is also illustrated in Figure 3.
Comparison with MLE. During training  our generative model directly compares the generated
temporal events with the observed events to iteratively correct the mistakes  which can effectively
avoid model misspeciﬁcation. Since the training only involves the policy gradient  it bypasses the
intractability issue of the log-survival term in the likelihood (Eq. (1)). On the other hand  because the
learned policy is in fact the conditional density of a point process  our approach still resembles the
form of MLE in the RL reformulation and can thus be interpreted in a statistically principled way.
Comparison with GAN and GAIL. By Theorem 1  our policy is learned directly by minimizing the
discrepancy between πE and πθ which has a closed form expression. Thus  we convert the original
IRL problem to a minimization problem with only one set of parameters with respect to the policy.
In each training iteration with the policy gradient  we have an unbiased estimator of the gradient 
and the estimated reward function also depends on the current policy πθ. In contrast  in GAN or
GAIL formulation  they have two sets of parameters related to the generator and the discriminator.
The gradient estimator is biased because each min-/max-problem is in fact nonconvex and cannot be
solved in one-shot. Thus  our framework is more stable and efﬁcient than the mini-max formulation
for learning point processes.
6 Experiments
We evaluate our algorithm by comparing with state-of-the-arts on both synthetic and real datasets.
Synthetic datasets. To show the robustness to model-misspeciﬁcations of our approach  we propose
the following four different point processes as the ground-truth: (I) Inhomogeneous Poisson (IP)
with λ(t) = at + b where a = −0.2 and b = 3.5; Here we omit st since λ(t) does not depend on
ti<t exp{−(t − ti)} where µ = 2 
and α = 0.5. (III) Mixture of IP and HP version 1 (IP + HP1). For the IP component  its λ(t)

the history. (II) Hawkes Process (HP) with λ(t|st) = µ + α(cid:80)

6

05101520Time-15-10-5051015ˆr∗(t)λπEλπθ05101520Time-15-10-5051015ˆr∗(t)λπEλπθ(a) IP

(b) HP

(c) IP + HP1

(d) IP+HP2

Figure 4: Comparison of empirical intensity functions on the synthetic data.

We select one beat zone data with call timestamps ranging from 7:00 AM to 1:00 PM.

is piece-wise linear with monotonic increasing slopes of pieces from {0.2  0.3  0.4  0.5}. The HP
component has the parameter µ = 1 and α = 0.5; (IV) Mixture of IP and HP version 2 (IP + HP2)
where the IP component also has piece-wise linear intensity but the slopes have the zig-zag pattern
chosen from {1 −1  2 −2}  and the HP component has the parameter µ = 1 and α = 0.1.
Real datasets. We evaluate our approach on four real datasets across a diverse range of domains:
• 911 call dataset contains 220 000 crime incident call records from 2011 to 2017 in Atlanta area.
• Microsoft Academic Search (MAS) provides access to publication venues  time  citations  etc. We
• Medical Information Mart for Intensive Care III (MIMIC-III) contains de-identiﬁed clinical visit
records from 2001 to 2012 for more than 40 000 patients. Our data contain 2 246 patients with at
least 3 visits. For a given patient  each clinical visit will be treated as an event.
• NYSE contains 0.7 million high-frequency trading records from NYSE for a given stock within
one day. All transactions are evenly divided into 3 200 segments. All segments have the same
temporal duration. Each trading record is treated as a event.

collect citation records for 50 000 papers and treat each citation time as an event.

Baselines. We compare our approach against two state-of-the-arts as well as conventional paramet-
ric baselines. The two state-of-the-art methods are WGANTPP [27] and RMTPP2 [6]. In addition 
three parametric methods based on maximum likelihood estimation are compared  including: (1)
Inhomogeneous Poisson process where the intensity function is modeled using a mixture of Gaussian
components  (2) Hawkes Process (or Self-Excitation process denoted as SE)  and (3) Self-Correcting

ti<t α(cid:9). In contrast to Hawkes process  the self-correcting

process (SC) with λ(t|st) = exp(cid:8)µt −(cid:80)

process seeks to produce regular point patterns. The intuition is that while the intensity increases
steadily  every time when a new event appears  it is decreased by multiplying a constant e−α < 1  so
the chance of new points decreases after an event has occurred recently.

Experimental Setup. The policy in our method RLPP is parameterized as LSTM with 64 hidden
neurons  and π(a|Θ(h)) is chosen to be exponential distribution. Batch size is 32 (the number of
sampled sequences L and M are 32 in Algorithm 1  and learning rate is 1e-3. We use Gaussian
kernel k(t  t(cid:48)) = exp(−(cid:107)t − t(cid:48)(cid:107)2/σ2) for the reward function. The kernel bandwidth σ is estimated
using the “median trick” based on the observations [13]. For WGANTPP and RMTPP  we are using
the open source codes. For WGANTPP3  we have used the exact experimental setup as [27]  which
adopts Adam optimization method [17] with learning rate 1e-4  β1 = 0.5  β2 = 0.9  and the batch
size is 256. For RMTPP4  batch size is 256  state size is 64  and learning rate is 1e-4.

Comparison of Learned Empirical Intensity. We ﬁrst compare the empirical intensity of the
learner point process to the expert point process. This is a straightforward comparison: one can
visually assess the performance and localize the discrepancy. Fig. 4 and Fig. 5 demonstrate the
empirical intensity functions of generated sequences based on synthetic and real data. It clearly shows
that RLPP consistently outperforms RMTPP  and achieves comparable and sometimes even better
ﬁtting against WGANTPP. Furthermore  RLPP consistently outperforms the other three conventional
parametric models when there exist model-misspeciﬁcations. Without any prior knowledge  RLPP
can capture the major trends in data and can accurately learn the nonlinear dependency structure

2RMTPP has very similar performance with [19].
3https://github.com/xiaoshuai09/Wasserstein-Learning-For-Point-Process
4https://github.com/dunan/NeuralPointProcess

7

012344812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC02464812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC05104812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC0.02.55.07.54812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC(a) 911 Call

(b) MAS

(c) MIMIC III

(d) NYSE

Figure 5: Comparison of empirical intensity functions on the real datasets. For each dataset  we have
used all learned models to generate new sequences. The comparisons are based on the empirical
intensities estimated from the generated temporal events and those estimated from the observed
temporal events.

hidden in data. In the Hawkes example  RLPP performs even as accurate as the ground-truth model.
On the real-world data  the underlying true model is unknown and the point process patterns are more
complicated. RLPP still shows a decent performance in the real datasets.

ti−1

intensity λ(t)  then the respective value achieved from the integral Λ =(cid:82) ti

Comparison of Data Fitting. Quantile plot (QQ-plot) for residual analysis is a standard model
checking approach for general point processes. Given a set of real input samples t1  . . .   tn  by
the Time Changing Theorem [5]  if such set of samples is one realization of a process with the
λ(t)dt should conform
to the unit-rate exponential distribution [18]. For the synthetic experiments  since we know the
exact ground-truth parametric form of λ(t|st)  we can perform this explicit transformation for a test.
Ideally  the QQ-plot for the generated sequences should follow a 45-degree straight line. We use
Hawkes Process (HP) and Inhomogeneous Poisson Process + Hawkes Process (IP+HP1) dataset to
produce the QQ-plot and compare different methods in Fig. 6. In both cases  RLPP consistently
stands out even without any prior knowledge about the parametric form of the true underlying
generative point process and the ﬁtting slope is very close to the diagonal line in both cases. More
rigorously  we perform the KS test. Fig. 7 illustrates the cumulative distributions (CDF) of p-values.
We followed the experiment setup in [21]: we generated samples from each learned point process
models  transformed the time interval  and applied the KS test to compare with unit rate exponential
distribution. Under this null hypothesis  the distribution of the p-values over tests should follow a
uniform distribution  whose CDF should be a diagonal line. If the target distribution is the Hawkes
process (Fig. 7)  both the learned SE (Hawkes process) and the RLPP models are indistinguishable
from that.

Figure 6: QQ-plot for dataset HP (left) and HP+IP1 (right).

Figure 7: KS test results: CDF of p-values.
Comparison of Runtime. The runtime for all methods averaged on all datasets is shown in Table
1. We note that both RLPP and WGANTPP are written in Tensorﬂow. However  WGANTPP adopts
the adversarial training framework based on Wasserstein divergence  where both the generator and
the discriminator are modeled as LSTMS. In contrast  RLPP only models the policy as a single
LSTM with the reward function learned in an analytical form. As a consequence  RLPP requires less
parameters and is more simpler to train while at the same time achieving comparable or even better
performance.

Table 1: Comparison of runtime.

Method RLPP WGANTPP RMTPP
Time
Ratio

1560m
780x

60m
30x

80m
40x

SE
SC IP
2m 2m 2m
1x
1x

1x

8

0.000.250.500.751.004812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC02464812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC012344812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC0.00.51.01.52.02.54812Time IndexIntensityMethodsRealRLPPWGANIPSERMTPPSC0246810Theoretical quantiles0246810Ordered ValuesProbability PlotRealIPSESCRMTPPWGANRLPP0246810Theoretical quantiles0246810Ordered ValuesProbability PlotRealIPSESCRMTPPWGANRLPP0.00.20.40.60.81.0p-value0.00.20.40.60.81.0cumulative distributionKS test for dataset HPRealRLRMTPPSESCIPWGAN(a): 911 dataset

Comparisons to LGCP and non-parametric
Hawkes. We also compared RLPP to log-
Gaussian Cox process (LGCP) model and non-
parametric Hawkes with non-stationary back-
ground rate (Nonpar Hawkes) model regarding
learned empirical intensity function. Represen-
tative comparison results are showed in Fig. 8.
Our proposed method (RL) performs similarly
to LGCP and outperforms Nonpar Hawkes on
Figure 8: Comparison of empirical intensity functions.
real datasets. However  LGCP needs to dis-
cretize time into windows and aggregate event into counts. This leads to some information loss and
introduces additional tuning parameters. Moreover  the standard LGCP is not scalable  typically
requiring O(n3) in computation and O(n2) in storage (n = sequence # × window #). We used an
implementation in GPy package5  which requires 50% more time than our method (127 mins vs
80 mins) in processing 5% of the dataset. The nonparametric Hawkes model is parametrized by
weighted sum of basis functions  similar to that of the inhomogeneous Poisson process baseline  and
it is difﬁcult to generalize outside the observation window.
7 Discussions
1. RMTPP we compared in experiments is a state-of-the-art maximum-likelihood-based model 
which uses a similar RNN outputting parametrization of exponential distributions but ﬁts the
model parameters with maximum likelihood. Across our experiments over eight synthetic and
real-world datasets  our proposed method performs consistently better than the MLE.

(b): MIMIC dataset

2. In theory  although MLE has many attractive limiting properties  it has no optimum properties for
ﬁnite samples  in the sense that (when evaluated on ﬁnite samples) other estimators may provide a
better estimate for the true parameters  e.g. [22]. Likelihood is related to KL divergence. Since KL
divergence is asymmetric and has a number of drawbacks for ﬁnite sample (such as high variance
and mode dropping)  many other divergences have been proposed and shown to perform better in
the ﬁnite sample case  e.g. [14]. Our proposed discrepancy is inspired by a similar use of RKHS
discrepancy in two sample tests in [14]. RKHS discrepancy has been shown to perform nicely on
ﬁnite sample and also preserve the asymptotic properties.

3. Another potential beneﬁt of our proposed framework is that one may use the RNN to deﬁne a
transformation for the temporal random variable instead of deﬁning its output distribution. For
example  we can establish our policy as a transformation of a sample from a unit rate exponential
distribution. The same empirical objective in Eq. (8) will be used  but a different optimization
algorithm is needed. Since no explicit parameterization of the output distribution is needed  this
may lead to even more ﬂexible models and this is left for future investigation.

8 Conclusions
This paper proposes a reinforcement learning framework to learn point process models. We
parametrized our policy as RNNs with stochastic neurons  which can sequentially sample dis-
crete events. The policy is updated by directly minimizing the discrepancy between the generated
sequences with the observed sequences  which can avoid model misspeciﬁcation and the limitation
of likelihood based approach. Furthermore  the discrepancy is explicitly evaluated in terms of the
reward function in our setting. By choosing the function class of reward to be the unit ball in RKHS 
we successfully derived an analytical optimal reward which maximizes the discrepancy. The optimal
reward will iteratively encourage the policy to sample events as close as the observation. We show
that our proposed approach performs well on both synthetic and real data.
Acknowledgments
This project was supported in part by NSF grants CCF-1442635  CMMI-1538746  DMS-1830210  NSF
CAREER Award CCF-1650913  Atlanta Police Foundation fund  and an S.F. Express fund awarded to Yao
Xie. This project was supported in part by NSF IIS-1218749  NIH BIGDATA 1R01GM108341  NSF CAREER
IIS-1350983  NSF IIS-1639792 EAGER  NSF CNS-1704701  ONR N00014-15-1-2340  Intel ISTC  NVIDIA
and Amazon AWS  NSF CCF-1836822  NSF IIS-1841351 EAGER  and Siemens awarded to Le Song.

5https://github.com/SheffieldML/GPy

9

051015t00.51IntensityReal DataRLNonpar HawkesLGCP51015t02468IntensityReal DataRLNonpar HawkesLGCPReferences
[1] Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning.
In Proceedings of the twenty-ﬁrst international conference on Machine learning  page 1. ACM 
2004.

[2] Robert A Adams and John JF Fournier. Sobolev spaces  volume 140. Academic press  2003.
[3] Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability

and statistics. Springer Science & Business Media  2011.

[4] Junyoung Chung  Kyle Kastner  Laurent Dinh  Kratarth Goel  Aaron C Courville  and Yoshua
Bengio. A recurrent latent variable model for sequential data. In Advances in neural information
processing systems  pages 2980–2988  2015.

[5] Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: general

theory and structure. Springer Science & Business Media  2007.

[6] Nan Du  Hanjun Dai  Rakshit Trivedi  Utkarsh Upadhyay  Manuel Gomez-Rodriguez  and
Le Song. Recurrent marked temporal point processes: Embedding event history to vector. In
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining  pages 1555–1564. ACM  2016.

[7] Nan Du  Le Song  Manuel Gomez Rodriguez  and Hongyuan Zha. Scalable inﬂuence estimation
in continuous-time diffusion networks. In Advances in neural information processing systems 
pages 3147–3155  2013.

[8] Gintare Karolina Dziugaite  Daniel M Roy  and Zoubin Ghahramani. Training generative neural
networks via maximum mean discrepancy optimization. arXiv preprint arXiv:1505.03906 
2015.

[9] Mehrdad Farajtabar  Yichen Wang  Manuel Gomez Rodriguez  Shuang Li  Hongyuan Zha 
and Le Song. Coevolve: A joint point process model for information diffusion and network
co-evolution. In Advances in Neural Information Processing Systems  pages 1954–1962  2015.
[10] Marc G Genton. Classes of kernels for machine learning: a statistics perspective. Journal of

machine learning research  2(Dec):299–312  2001.

[11] Manuel Gomez Rodriguez  Jure Leskovec  and Andreas Krause. Inferring networks of diffusion
and inﬂuence. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge
discovery and data mining  pages 1019–1028. ACM  2010.

[12] Jan Grandell. Doubly stochastic Poisson processes  volume 529. Springer  2006.

[13] Arthur Gretton  Karsten M Borgwardt  Malte Rasch  Bernhard Schölkopf  and Alex J Smola.
A kernel method for the two-sample-problem. In Advances in neural information processing
systems  pages 513–520  2007.

[14] Arthur Gretton  Karsten M Borgwardt  Malte J Rasch  Bernhard Schölkopf  and Alexander
Smola. A kernel two-sample test. Journal of Machine Learning Research  13(Mar):723–773 
2012.

[15] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in

Neural Information Processing Systems  pages 4565–4573  2016.

[16] Beomjoon Kim and Joelle Pineau. Maximum mean discrepancy imitation learning. In Robotics:

Science and systems  2013.

[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[18] John Frank Charles Kingman. Poisson processes. Wiley Online Library  1993.

[19] Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating
multivariate point process. In Advances in Neural Information Processing Systems  pages
6757–6767  2017.

10

[20] Andrew Y Ng  Stuart J Russell  et al. Algorithms for inverse reinforcement learning. In Icml 

pages 663–670  2000.

[21] Takahiro Omi  Yoshito Hirata  and Kazuyuki Aihara. Hawkes process model with a time-
dependent background rate and its application to high-frequency ﬁnancial data. Physical Review
E  96(1):012303  2017.

[22] Johann Pfanzagl. Parametric statistical theory. Walter de Gruyter  2011.

[23] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. MIT press  2001.

[24] Alex Smola  Arthur Gretton  Le Song  and Bernhard Schölkopf. A hilbert space embedding
for distributions. In International Conference on Algorithmic Learning Theory  pages 13–31.
Springer  2007.

[25] Bharath Sriperumbudur  Kenji Fukumizu  and Gert Lanckriet. On the relation between univer-
sality  characteristic kernels and rkhs embedding of measures. In Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics  pages 773–780  2010.

[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction  volume 1.

MIT press Cambridge  1998.

[27] Shuai Xiao  Mehrdad Farajtabar  Xiaojing Ye  Junchi Yan  Xiaokang Yang  Le Song  and
Hongyuan Zha. Wasserstein learning of deep generative point process models. In Advances in
Neural Information Processing Systems  pages 3250–3259  2017.

[28] Brian D Ziebart  Andrew L Maas  J Andrew Bagnell  and Anind K Dey. Maximum entropy
inverse reinforcement learning. In AAAI  volume 8  pages 1433–1438. Chicago  IL  USA  2008.

11

,Shuang Li
Shuai Xiao
Shixiang Zhu
Nan Du
Yao Xie
Le Song