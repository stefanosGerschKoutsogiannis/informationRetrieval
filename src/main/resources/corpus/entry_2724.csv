2018,Is Q-Learning Provably Efficient?,Model-free reinforcement learning (RL) algorithms directly parameterize and update value functions or policies  bypassing the modeling of the environment. They are typically simpler  more flexible to use  and thus more prevalent in modern deep RL than model-based approaches. However  empirical work has suggested that they require large numbers of samples to learn.  The theoretical question of whether not model-free algorithms are in fact \emph{sample efficient} is one of the most fundamental questions in RL. The problem is unsolved even in the basic scenario with finitely many states and actions. We prove that  in an episodic MDP setting  Q-learning with UCB exploration achieves regret $\tlO(\sqrt{H^3 SAT})$ where $S$ and $A$ are the numbers of states and actions  $H$ is the number of steps per episode  and $T$ is the total number of steps. Our regret matches the optimal regret up to a single $\sqrt{H}$ factor.  Thus we establish the sample efficiency of a classical model-free approach. Moreover  to the best of our knowledge  this is the first model-free analysis to establish $\sqrt{T}$ regret \emph{without} requiring access to a ``simulator.'',Is Q-learning Provably Efﬁcient?

Chi Jin∗

University of California  Berkeley

chijin@cs.berkeley.edu

Zeyuan Allen-Zhu∗

Microsoft Research  Redmond

zeyuan@csail.mit.edu

Sebastien Bubeck

Microsoft Research  Redmond
sebubeck@microsoft.com

Michael I. Jordan

University of California  Berkeley

jordan@cs.berkeley.edu

Abstract

Model-free reinforcement learning (RL) algorithms  such as Q-learning  directly
parameterize and update value functions or policies without explicitly modeling
the environment. They are typically simpler  more ﬂexible to use  and thus more
prevalent in modern deep RL than model-based approaches. However  empiri-
cal work has suggested that model-free algorithms may require more samples to
learn [7  22]. The theoretical question of “whether model-free algorithms can
be made sample efﬁcient” is one of the most fundamental questions in RL  and
remains unsolved even in the basic scenario with ﬁnitely many states and actions.
√
We prove that  in an episodic MDP setting  Q-learning with UCB exploration
achieves regret ˜O(
H 3SAT )  where S and A are the numbers of states and ac-
tions  H is the number of steps per episode  and T is the total number of steps.
This sample efﬁciency matches the optimal regret that can be achieved by any
H factor. To the best of our knowledge 
model-based approach  up to a single
T regret without
this is the ﬁrst analysis in the model-free setting that establishes
requiring access to a “simulator.”

√

√

1

Introduction

Reinforcement Learning (RL) is a control-theoretic problem in which an agent tries to maximize its
cumulative rewards via interacting with an unknown environment through time [26]. There are two
main approaches to RL: model-based and model-free. Model-based algorithms make use of a model
for the environment  forming a control policy based on this learned model. Model-free approaches
dispense with the model and directly update the value function—the expected reward starting from
each state  or the policy—the mapping from states to their subsequent actions. There has been a long
debate on the relative pros and cons of the two approaches [7].
From the classical Q-learning algorithm [27] to modern DQN [17]  A3C [18]  TRPO [22]  and oth-
ers  most state-of-the-art RL has been in the model-free paradigm. Its pros—model-free algorithms
are online  require less space  and  most importantly  are more expressive since specifying the value
functions or policies is often more ﬂexible than specifying the model for the environment—arguably
outweigh its cons relative to model-based approaches. These relative advantages underly the signif-
icant successes of model-free algorithms in deep RL applications [17  24].
On the other hand it is believed that model-free algorithms suffer from a higher sample complexity
compared to model-based approaches. This has been evidenced empirically in [7  22]  and recent
work has tried to improve the sample efﬁciency of model-free algorithms by combining them with
∗The ﬁrst two authors contributed equally. Full paper (and future edition) available at https://arxiv.

org/abs/1807.03765.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

model-based approaches [19  21]. There is  however  little theory to support such blending  which
requires a more quantitative understanding of relative sample complexities. Indeed  the following
basic theoretical questions remain open:

Can we design model-free algorithms that are sample efﬁcient?

In particular  is Q-learning provably efﬁcient?

The answers remain elusive even in the basic tabular setting where the number of states and actions
are ﬁnite.
In this paper  we attack this problem head-on in the setting of the episodic Markov
Decision Process (MDP) formalism (see Section 2 for a formal deﬁnition). In this setting  an episode
consists of a run of MDP dynamics for H steps  where the agent aims to maximize total reward
over multiple episodes. We do not assume access to a “simulator” (which would allow us to query
arbitrary state-action pairs of the MDP) and the agent is not allowed to “reset” within each episode.
This makes our setting sufﬁciently challenging and realistic. In this setting  the standard Q-learning
heuristic of incorporating ε-greedy exploration appears to take exponentially many episodes to learn
[14].
As seen in the literature on bandits  the key to achieving good sample efﬁciency generally lies in
managing the tradeoff between exploration and exploitation. One needs an efﬁcient strategy to ex-
plore the uncertain environment while maximizing reward. In the model-based setting  a recent line
of research has imported ideas from the bandit literature—including the use of upper conﬁdence
bounds (UCB) and improved design of exploration bonuses—and has obtained asymptotically op-
timal sample efﬁciency [1  5  10  12]. In contrast  the understanding of model-free algorithms is
still very limited. To the best of our knowledge  the only existing theoretical result on model-free
RL that applies to the episodic setting is for delayed Q-learning; however  this algorithm is quite
sample-inefﬁcient compared to model-based approaches [25].
In this paper  we answer the two aforementioned questions afﬁrmatively. We show that Q-learning 
√
when equipped with a UCB exploration policy that incorporates estimates of the conﬁdence of Q
values and assign exploration bonuses  achieves total regret ˜O(
H 3SAT ). Here  S and A are the
numbers of states and actions  H is the number of steps per episode  and T is the total number
of steps. Up to a
H factor  our regret matches the information-theoretic optimum  which can be
achieved by model-based algorithms [5  12]. Since our algorithm is just Q-learning  it is online
and does not store additional data besides the table of Q values (and a few integers per entry of
this table). Thus  it also enjoys a signiﬁcant advantage over model-based algorithms in terms of
time and space complexities. To our best knowledge  this is the ﬁrst sharp analysis for model-free
T regret or equivalently O(1/ε2) samples for ε-optimal policy—without
algorithms—featuring
requiring access to a “simulator.”
For practitioners  there are two key takeaways from our theoretical analysis:
1. The use of UCB exploration instead of ε-greedy exploration in the model-free setting allows for

√

√

better treatment of uncertainties for different states and actions.

2. It is essential to use a learning rate which is αt = O(H/t)  instead of 1/t  when a state-action
pair is being updated for the t-th time. The former learning rate assigns more weight to updates
that are more recent  as opposed to assigning uniform weights to all previous updates. This deli-
cate choice of reweighting leads to the crucial difference between our sample-efﬁcient guarantee
versus earlier highly inefﬁcient results that require exponentially many samples in H.

1.1 Related Work

In this section  we focus our attention on theoretical results for the tabular MDP setting  where the
numbers of states and actions are ﬁnite. We acknowledge that there has been much recent work in
RL for continuous state spaces [see  e.g.  9  11]  but this setting is beyond our scope.
With simulator. Some results assume access to a simulator [15] (a.k.a.  a generative model [3]) 
which is a strong oracle that allows the algorithm to query arbitrary state-action pairs and return
the reward and the next state. The majority of these results focus on an inﬁnite-horizon MDP with
discounted reward [e.g.  2  3  8  16  23]. When a simulator is available  model-free algorithms
[2] (variants of Q-learning) are known to be almost as sample efﬁcient as the best model-based
algorithms [3]. However  the simulator setting is considered to much easier than standard RL  as it
“does not require exploration” [2]. Indeed  a naive exploration strategy which queries all state-action

2

Algorithm
RLSVI [? ]
UCRL2 [10] 1

Model-based

Agrawal and Jia [1] 1

UCBVI [5] 2
vUCQ [12] 2

Q-learning (ε-greedy) [14]

(if 0 initialized)

Model-free

Delayed Q-learning [25] 3

Q-learning (UCB-H)
Q-learning (UCB-B)

lower bound

H 4S2AT )

H 3S2AT )

˜O(

Regret
√
H 3SAT )
√
√

at least ˜O(
at least ˜O(
√
˜O(
H 2SAT )
√
˜O(
H 2SAT )
Ω(min{T  AH/2})
˜OS A H (T 4/5)
√
˜O(
H 4SAT )
√
˜O(
H 3SAT )
√
H 2SAT )

Ω(

Time

Space

˜O(T S2A2) O(S2A2H)

Ω(T S2A)

˜O(T S2A)

O(S2AH)

O(T )

O(SAH)

-

-

Table 1: Regret comparisons for RL algorithms on episodic MDP. T = KH is totally number of steps  H is
the number of steps per episode  S is the number of states  and A is the number of actions. For clarity 
this table is presented for T ≥ poly(S  A  H)  omitting low order terms.

pairs uniformly at random already leads to the most efﬁcient algorithm for ﬁnding optimal policy
[3].

Without simulator. Reinforcement learning becomes much more challenging without the presence
of a simulator  and the choice of exploration policy can now determine the behavior of the learning
algorithm. For instance  Q-learning with ε-greedy may take exponentially many episodes to learn
the optimal policy [14] (for the sake of completeness  we present this result in our episodic language
in Appendix A).
Mathemtically  this paper deﬁnes “model-free” algorithms as in existing literature [25  26]:
Deﬁnition 1. A reinforcement learning algorithm is model-free if its space complexity is always
sublinear (for any T ) relative to the space required to store an MDP. In episodic setting of this
paper  a model-free algorithm has space complexity o(S2AH) (independent of T ).

√

H 2SAT ). The additional

H 4S2AT ) and ˜O(

In the model-based setting  UCRL2 [10] and Agrawal and Jia [1] form estimates of the transition
probabilities of the MDP using past samples  and add upper-conﬁdence bounds (UCB) to the es-
√
timated transition matrix. When applying their results to the episodic MDP scenario  their total
regret is at least ˜O(
√
H 3S2AT ) respectively.1 In contrast  the information-
theoretic lower bound is ˜O(
H factors were later removed by
the UCBVI algorithm [5] which adds a UCB bonus directly to the Q values instead of the estimated
transition matrix.2 The vUCQ algorithm [12] is similar to UCBVI but improves lower-order regret
√
terms using variance reduction. Finally  RLSVI [? ]  an algorithm designed for setting of linear ap-
proxmation  provides ˜O(
H 3SAT ) regret bound when adapted to tabular MDP setting. However 
it is batch algorithm in nature  and requires O(d2H) space where in tabular setting d = SA.
We note that despite the sharp regret guarantees  most of the results in this line of research crucially
rely on estimating and storing the entire transition matrix and thus suffer from unfavorable time and
space complexities compared to model-free algorithms.

S and

√

√

1Jaksch et al. [10] and Agrawal and Jia [1] apply to the more general setting of weakly communicating
MDPs with S(cid:48) states and diameter D; our episodic MDP is a special case obtained by augmenting the state
space so that S(cid:48) = SH and D ≥ H.
2Azar et al. [5] and Kakade et al. [12] assume equal transition matrices P1 = ··· = PH; in the setting of
this paper P1 ···   PH can be entirely different. This adds a factor of
3Strehl et al. [25] applies to MDPs with S(cid:48) states and discount factor γ; our episodic MDP can be converted
to that case by setting S(cid:48) = SH and 1 − γ = 1/H. Their result only applies to the stochastic setting where
initial states xk
1 come from a ﬁxed distribution  and only gives a PAC guarantee. See our full version for a
comparison between PAC and regret guarantees.

H to their total regret.

√

3

In the model-free setting  Strehl et al. [25] introduced delayed Q-learning  where  to ﬁnd an ε-
optimal policy  the Q value for each state-action pair is updated only once every m = ˜O(1/ε2)
times this pair is visited. In contrast to the incremental update of Q-learning  delayed Q-learning
always replaces old Q values with the average of the most recent m experiences. When translated
to the setting of this paper  this gives ˜O(T 4/5) total regret  ignoring factors in S  A and H.3 This is
quite suboptimal compared to the ˜O(

T ) regret achieved by model-based algorithm.

√

2 Preliminary
We consider the setting of a tabular episodic Markov decision process  MDP(S A  H  P  r)  where
S is the set of states with |S| = S  A is the set of actions with |A| = A  H is the number of steps in
each episode  P is the transition matrix so that Ph(·|x  a) gives the distribution over states if action
a is taken for state x at step h ∈ [H]  and rh : S ×A → [0  1] is the deterministic reward function at
step h.4
In each episode of this MDP  an initial state x1 is picked arbitrarily by an adversary. Then  at each
step h ∈ [H]  the agent observes state xh ∈ S  picks an action ah ∈ A  receives reward rh(xh  ah) 
and then transitions to a next state  xh+1  that is drawn from the distribution Ph(·|xh  ah). The
episode ends when xH+1 is reached.
h : S → R
to denote the value function at step h under policy π  so that V π
h (x) gives the expected sum of
remaining rewards received under policy π  starting from xh = x  until the end of the episode. In
symbols:

A policy π of an agent is a collection of H functions(cid:8)πh : S → A(cid:9)

h∈[H]. We use V π

h(cid:48)=h rh(cid:48)(xh(cid:48)  πh(cid:48)(xh(cid:48)))|xh = x

.

(cid:105)

Accordingly  we also deﬁne Qπ
h(x  a)
gives the expected sum of remaining rewards received under policy π  starting from xh = x  ah = a 
till the end of the episode. In symbols:

h : S ×A → R to denote Q-value function at step h so that Qπ

h(cid:48)=h+1 rh(cid:48)(xh(cid:48)  πh(cid:48)(xh(cid:48)))|xh = x  ah = a] .

Since the state and action spaces  and the horizon  are all ﬁnite  there always exists (see  e.g.  [5]) an
h (x) for all x ∈ S and h ∈ [H].
optimal policy π(cid:63) which gives the optimal value V (cid:63)
For simplicity  we denote [PhVh+1](x  a) := Ex(cid:48)∼P(·|x a)Vh+1(x(cid:48)). Recall the Bellman equation
and the Bellman optimality equation:

h (x) = supπ V π

V π

h (x) := E(cid:104)(cid:80)H
h(x  a) := rh(x  a) + E[(cid:80)H

Qπ

 V π

 V (cid:63)
(cid:2)V (cid:63)

1 (xk

h (x) = Qπ
h(x  πh(x))
h(x  a) := (rh + PhV π
Qπ
∀x ∈ S
V π
H+1(x) = 0

h+1)(x  a)

and

h(x  a)

h (x) = maxa∈A Q(cid:63)
h(x  a) := (rh + PhV (cid:63)
Q(cid:63)
V (cid:63)
H+1(x) = 0

∀x ∈ S .

h+1)(x  a)

(2.1)

The agent plays the game for K episodes k = 1  2  . . .   K  and we let the adversary pick a starting
1 for each episode k  and let the agent choose a policy πk before starting the k-th episode.
state xk
The total (expected) regret is then

Regret(K) =(cid:80)K

k=1

1)(cid:3) .

1) − V πk

1 (xk

3 Main Results

In this section  we present our main theoretical result—a sample complexity result for a variant
of Q-learning that incorporates UCB exploration. We also present a theorem that establishes an
information-theoretic lower bound for episodic MDP.
As seen in the bandit setting  the choice of exploration policy plays an essential role in the efﬁ-
ciency of a learning algorithm. In episodic MDP  Q-learning with the commonly used ε-greedy
exploration strategy can be very inefﬁcient: it can take exponentially many episodes to learn [14]

4While we study deterministic reward functions for notational simplicity  our results generalize to random-

ized reward functions. Also  we assume the reward is in [0  1] without loss of generality.

4

Algorithm 1 Q-learning with UCB-Hoeffding
1: initialize Qh(x  a) ← H and Nh(x  a) ← 0 for all (x  a  h) ∈ S × A × [H].
2: for episode k = 1  . . .   K do
3:
4:
5:
6:
7:
8:

Take action ah ← argmaxa(cid:48) Qh(xh  a(cid:48))  and observe xh+1.
Qh(xh  ah) ← (1 − αt)Qh(xh  ah) + αt[rh(xh  ah) + Vh+1(xh+1) + bt].
Vh(xh) ← min{H  maxa(cid:48)∈A Qh(xh  a(cid:48))}.

t = Nh(xh  ah) ← Nh(xh  ah) + 1; bt ← c(cid:112)H 3ι/t.

receive x1.
for step h = 1  . . .   H do

(see also Appendix A).
In contrast  our algorithm (Algorithm 1)  which is Q-learning with an
upper-conﬁdence bound (UCB) exploration strategy  will be seen to be efﬁcient. This algorithm
maintains Q values  Qh(x  a)  for all (x  a  h) ∈ S × A × [H] and the corresponding V values
Vh(x) ← min{H  maxa(cid:48)∈A Qh(x  a(cid:48))}. If  at time step h ∈ [H]  the state is x ∈ S  the algorithm
takes the action a ∈ A that maximizes the current estimate Qh(x  a)  and is apprised of the next
state x(cid:48) ∈ S. The algorithm then updates the Q values:

Qh(x  a) ← (1 − αt)Qh(x  a) + αt[rh(x  a) + Vh+1(x(cid:48)) + bt]  

where t is the counter for how many times the algorithm has visited the state-action pair (x  a) at
step h  bt is the conﬁdence bonus indicating how certain the algorithm is about current state-action
pair  and αt is a learning rate deﬁned as follows:

αt :=

H + 1
H + t

.

(3.1)

As mentioned in the introduction  our choice of learning rate αt scales as O(H/t) instead of
O(1/t)—this is crucial to obtain regret that is not exponential in H.
We present analyses for two different speciﬁcations of the upper conﬁdence bonus bt in this paper:

Q-learning with Hoeffding-style bonus. The ﬁrst (and simpler) choice is bt = O((cid:112)H 3ι/t).

(Here  and throughout this paper  we use ι := log(SAT /p) to denote a log factor.) This choice
of bonus makes sense intuitively because: (1) Q-values are upper-bounded by H  and  accordingly 
(2) Hoeffding-type martingale concentration inequalities imply that if we have visited (x  a) for t
times  then a conﬁdence bound for the Q value scales as 1/
t. For this reason  we call this choice
UCB-Hoeffding (UCB-H). See Algorithm 1.
Theorem 2 (Hoeffding). There exists an absolute constant c > 0 such that  for any p ∈ (0  1)  if

we choose bt = c(cid:112)H 3ι/t  then with probability 1 − p  the total regret of Q-learning with UCB-

√

√
Hoeffding (see Algorithm 1) is at most O(

H 4SAT ι)  where ι := log(SAT /p).

√

Theorem 2 shows  under a rather simple choice of exploration bonus  Q-learning can be made very
efﬁcient  enjoying a ˜O(
T ) regret which is optimal in terms of dependence on T . To the best of
T regret without
our knowledge  this is the ﬁrst analysis of a model-free procedure that features a
requiring access to a “simulator.”
Compared to the previous model-based results  Theorem 2 shows that the regret (or equivalently
the sample complexity; see discussion in full version) of this version of Q-learning is as good as
the best model-based one in terms of the dependency on the number of states S  actions A and the
total number of steps T . Although our regret slightly increases the dependency on H  the algorithm
is online and does not store additional data besides the table of Q values (and a few integers per
entry of this table). Thus  it enjoys an advantage over model-based algorithms in time and space
complexities  especially when the number of states S is large.

√

Q-learning with Bernstein-style bonus. Our second speciﬁcation of bt makes use of a Bernstein-
style upper conﬁdence bound. The key observation is that  although in the worst case the value
function is at most H for any state-action pair  if we sum up the “total variance of the value function”
for an entire episode  we obtain a factor of only O(H 2) as opposed to the naive O(H 3) bound (see
Lemma C.5). This implies that the use of a Bernstein-type martingale concentration result could be

5

sharper than the Hoeffding-type bound by an additional factor of H.5 (The idea of using Bernstein
instead of Hoeffding for reinforcement learning applications has appeared in previous work; see 
e.g.  [3  4  16].)
Using Bernstein concentration requires us to design the bonus term bt more carefully  as it now
depends on the empirical variance of Vh+1(x(cid:48)) where x(cid:48) is the next state over the previous t visits
of current state-action (x  a). This empirical variance can be computed in an online fashion without
increasing the space complexity of Q-learning. We defer the full speciﬁcation of bt to Algorithm 2
in Appendix C. We now state the regret theorem for this approach.
Theorem 3 (Bernstein). For any p ∈ (0  1)  one can specify bt so that with probability 1−p  the total
√
H 9S3A3 ·
regret of Q-learning with UCB-Bernstein (see Algorithm 2) is at most O(
ι2).

H 3SAT ι +

√

√

√

T ) improves by a factor of

Theorem 3 shows that for Q-learning with UCB-B exploration  the leading term in regret (which
scales as
H over UCB-H exploration  at the price of using a more
complicated exploration bonus design. The asymptotic regret of UCB-B is now only one
H factor
worse than the best regret achieved by model-based algorithms.

√
H 9S3A3 · ι2) in its regret  which dominates
We also note that Theorem 3 has an additive term O(
the total regret when T is not very large compared with S  A and H. It is not clear whether this
lower-order term is essential  or is due to technical aspects of the current analysis.

√

Information-theoretical limit.
information-theoretic lower bound for the episodic MDP setting studied in this paper:
√
Theorem 4. For the episodic MDP problem studied in this paper  the expected regret for any algo-
rithm must be at least Ω(

To demonstrate the sharpness of our results  we also note an

H 2SAT ).

Theorem 4 (see Appendix D for details) shows that both variants of our algorithm are nearly optimal 
in the sense they differ from the optimal regret by a factor of H and

H  respectively.

√

4 Proof for Q-learning with UCB-Hoeffding

In this section  we provide the full proof of Theorem 2. Intuitively  the episodic MDP with H steps
per epsiode can be viewed as a contextual bandit of H “layers.” The key challenge here is to control
the way error and conﬁdence propagate through different “layers” in an online fashion  where our
speciﬁc choice of exploration bonus and learning rate make the regret as sharp as possible.
Notation. We denote by I[A] the indicator function for event A. We denote by (xk
h) the
actual state-action pair observed and chosen at step h of episode k. We also denote by Qk
h   N k
h
respectively the Qh  Vh  Nh functions at the beginning of episode k. Using this notation  the update
equation at episode k can be rewritten as follows  for every h ∈ [H]:

h  ak
h  V k

(cid:26)(1 − αt)Qk

Qk

h(x  a)

Qk+1

h

(x  a) =

Accordingly 

h(x  a) + αt[rh(x  a) + V k

h+1(xk

h+1) + bt]

h (x) ← min(cid:8)H  max

V k

h(x  a(cid:48))(cid:9) 

a(cid:48)∈A Qk

∀x ∈ S .

if (x  a) = (xk
otherwise .

h  ak
h)

(4.1)

hVh+1](x  a) := Vh+1(xk

h+1)  which is deﬁned only for (x  a) = (xk

Recall that we have [PhVh+1](x  a) := Ex(cid:48)∼Ph(·|x a)Vh+1(x(cid:48)). We also denote its empirical coun-
terpart of episode k as [ˆPk
h).
h  ak
H+t . For notational convenience  we also
Recall that we have chosen the learning rate as αt := H+1
introduce the following related quantities:
j=1(1 − αj) 

(4.2)
5Recall that for independent zero-mean random variables X1  . . .   XT satisfying |Xi| ≤ M  their sum-
T ) with high probability using Hoeffding concentration. If we have in hand a

(cid:81)t
j=i+1(1 − αj) .
E[Xi]2(cid:1) using Bernstein concentration.

better variance bound  this can be improved to ˜O(cid:0)M +(cid:112)(cid:80)

mation does not exceed ˜O(M

t =(cid:81)t

t = αi

√

α0

αi

i

6

t(cid:88)

(cid:104)

(cid:105)

It is easy to verify that (1)(cid:80)t

t = 0 for t ≥ 1; (2)(cid:80)t

i=1 αi

t = 1 and α0

i=1 αi

t = 0 and α0

t = 1 for

t = 0.
Favoring Later Updates. At any (x  a  h  k) ∈ S ×A× [H]× [K]  let t = N k
h (x  a) and suppose
(x  a) was previously taken at step h of episodes k1  . . .   kt < k. By the update equation (4.1) and
the deﬁnition of αi

t in (4.2)  we have:

Qk

h(x  a) = α0

t H +

αi
t

rh(x  a) + V ki

h+1(xki

h+1) + bi

.

(4.3)

i=1

t   . . .   αt

t. Our choice of the learning rate αt = H+1

According to (4.3)  the Q value at episode k equals a weighted average of the V values of the “next
states” with weights α1
H+t ensures that  approxi-
mately speaking  the last 1/H fraction of the indices i is given non-negligible weights  whereas the
ﬁrst 1 − 1/H fraction is forgotten. This ensures that the information accumulates smoothly across
t would all equal
the H layers of the MDP. If one were to use αt = 1
√
1/t  and using those V values from earlier episodes would hurt the accuracy of the Q function. In
contrast  if one were to use αt = 1/
t instead  the weights α1
t would concentrate too much
on the most recent episodes  which would incur high variance.

t instead  the weights α1

t   . . .   αt

t   . . .   αt

4.1 Proof Details

We ﬁrst present an auxiliary lemma which exhibits some important properties that result from our
choice of learning rate. The proof is based on simple manipulations on the deﬁnition of αt  and is
provided in Appendix B.
Lemma 4.1. The following properties hold for αi
t:

i=1

(a)

1√
t

≤ 2√

for every t ≥ 1.

≤(cid:80)t
(c) (cid:80)∞
can blow up the regret by a multiplicative factor of(cid:80)∞

i=1(αi
H for every i ≥ 1.

αi
t√
i
t ≤ 2H
t = 1 + 1

t and(cid:80)t

(b) maxi∈[t] αi

t)2 ≤ 2H

t=i αi

t

t

for every t ≥ 1.

t=i αi

We note that property (c) is especially important—as we will show later  each step in one episode
t. With our choice of learning rate  we

ensure that this blow-up is at most (1 + 1/H)H  which is a constant factor.
We now proceed to the formal proof. We start with a lemma that gives a recursive formula for
Q − Q(cid:63)  as a weighted average of previous updates.
Lemma 4.2 (recursion on Q). For any (x  a  h) ∈ S × A × [H] and episode k ∈ [K]  let t =
h (x  a) and suppose (x  a) was previously taken at step h of episodes k1  . . .   kt < k. Then:
N k
h−Q(cid:63)

h)(x  a) = α0

h − Ph)V (cid:63)

h+1) + [(ˆPki

t (H−Q(cid:63)

h+1)(xki

h(x  a))+

(Qk

(cid:104)

h+1](x  a) + bi

(cid:105)

.

i=1

αi
t

h+1 − V (cid:63)
(V ki

t(cid:88)
h+1)  and the fact that(cid:80)t
(cid:104)
rh(x  a) +(cid:0)Ph − ˆPki

αi
t

h

t(cid:88)

(cid:1)V (cid:63)

Proof of Lemma 4.2. From the Bellman optimality equation  Q(cid:63)
notation [ˆPki

h Vh+1](x  a) := Vh+1(xki

h(x  a) = (rh + PhV (cid:63)
i=0 αi

t = 1  we have

h+1)(x  a)  our

Q(cid:63)

h(x  a) = α0

t Q(cid:63)

h(x  a) +

h+1(x  a) + V (cid:63)

h+1(xki

h+1)

Subtracting the formula (4.3) from this equation  we obtain Lemma 4.2.

i=1

(cid:105)

.

(cid:3)

Next  using Lemma 4.2 and the Azuma-Hoeffding concentration bound  our next lemma shows that
Qk is always an upper bound on Q(cid:63) at any episode k  and the difference between Qk and Q(cid:63) can be
bounded by quantities from the next step.
Lemma 4.3 (bound on Qk − Q(cid:63)). There exists an absolute constant c > 0 such that  for any

p ∈ (0  1)  letting bt = c(cid:112)H 3ι/t  we have βt = 2(cid:80)t

tbi ≤ 4c(cid:112)H 3ι/t and  with probability

i=1 αi

7

at least 1 − p  the following holds simultaneously for all (x  a  h  k) ∈ S × A × [H] × [K]:

0 ≤ (Qk

h − Q(cid:63)

h)(x  a) ≤ α0

t H +

t(V ki
αi

h+1 − V (cid:63)

h+1)(xki

h+1) + βt  

t(cid:88)

i=1

h (x  a) and k1  . . .   kt < k are the episodes where (x  a) was taken at step h.

where t = N k
Proof of Lemma 4.3. For each ﬁxed (x  a  h) ∈ S × A × [H]  let us denote k0 = 0  and denote

h) = (x  a)(cid:9) ∪ {K + 1}(cid:1) .

h  ak

be the σ-ﬁeld generated by all the random variables until episode ki  step h. Then (cid:0)I[ki ≤ K] ·

That is  ki is the episode of which (x  a) was taken at step h for the ith time (or ki = K + 1
if it is taken for fewer than i times). The random variable ki is clearly a stopping time. Let Fi
i=1 is a martingale difference sequence w.r.t the ﬁltration {Fi}i≥0. By
[(ˆPki
Azuma-Hoeffding and a union bound  we have that with probability at least 1 − p/(SAH):

h − Ph)V (cid:63)

ki = min(cid:0)(cid:8)k ∈ [K] | k > ki−1 ∧ (xk
h+1](x  a)(cid:1)τ
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) τ(cid:88)

τ · I[ki ≤ K] · [(ˆPki
αi

h − Ph)V (cid:63)

i=1

∀τ ∈ [K] :

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ cH

2

(cid:118)(cid:117)(cid:117)(cid:116) τ(cid:88)

i=1

(cid:114)
τ )2 · ι ≤ c

(αi

H 3ι

τ

 

h+1](x  a)

(4.4)
for some absolute constant c. Because inequality (4.4) holds for all ﬁxed τ ∈ [K] uniformly  it also
h (x  a) ≤ K  which is a random variable  where k ∈ [K]. Also note I[ki ≤
holds for τ = t = N k
K] = 1 for all i ≤ N k
h (x  a). Putting everything together  and using a union bound  we see that with
least 1− p probability  the following holds simultaneously for all (x  a  h  k) ∈ S ×A× [H]× [K]:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) t(cid:88)
On the other hand  if we choose bt = c(cid:112)H 3ι/t for the same constant c in Eq. (4.4)  we have
tbi ∈ [c(cid:112)H 3ι/t  2c(cid:112)H 3ι/t(cid:3) according to Lemma 4.1.a. Then the right-hand side
βt/2 =(cid:80)t

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ c
(cid:114)

of Lemma 4.3 follows immediately from Lemma 4.2 and inequality (4.5). The left-hand side also
(cid:3)
follows from Lemma 4.2 and Eq. (4.5) and induction on h = H  H − 1  . . .   1.

h − Ph)V (cid:63)

t[(ˆPki
αi

h+1](x  a)

h (x  a) .

t = N k

i=1 αi

where

(4.5)

H 3ι

i=1

t

We are now ready to prove Theorem 2. The proof decomposes the regret in a recursive form  and
carefully controls the error propagation with repeated usage of Lemma 4.3.

Proof of Theorem 2. Denote by
δk
h := (V k

h − V πk

h := (V k
h ≥ Q(cid:63)

and φk
By Lemma 4.3  we have that with 1 − p probability  Qk
regret can be upper bounded:

h )(xk
h)

Regret(K) =(cid:80)K
h to(cid:80)K

k=1 δk

k=1(V (cid:63)

1 − V πk

The main idea of the rest of the proof is to upper bound(cid:80)K
relating(cid:80)K

k=1(V k

1 )(xk

1) ≤(cid:80)K

h.
k=1 φk

For any ﬁxed (k  h) ∈ [K] × [H]  let t = N k
h  ak
at step h of episodes k1  . . .   kt < k. Then we have:

h (xk

h − V (cid:63)
h )(xk
h and thus V k
1 − V πk

1 )(xk

h) .
h ≥ V (cid:63)

1) =(cid:80)K
h by the next step(cid:80)K

k=1 δk

1 .

h . Thus  the total

h+1 
thus giving a recursive formula to calculate total regret. We can obtain such a recursive formula by

k=1 δk

k=1 δk

h)  and suppose (xk

h  ak

h) was previously taken

h )(xk
h)
h  ak
h)(xk

h − V πk
h − Q(cid:63)

δk
h = (V k
= (Qk
y≤ α0
z
= α0

t H +(cid:80)t
t H +(cid:80)t
tbi ≤ O(1)(cid:112)H 3ι/t and ξk

h − Qπk
h )(xk
h − Qπk

x≤ (Qk
h  ak
h)
h) + (Q(cid:63)
h  ak
h )(xk
h)
h+1 − V πk
h+1 + βt + [Ph(V (cid:63)
tφki
h+1 + βt − φk
tφki
h+1 + ξk
h)(V (cid:63)
h+1)](xk
h) ≤ maxa(cid:48)∈A Qk

h+1  
h+1 − V k

h+1 := [(Ph − ˆPk
martingale difference sequence. Inequality x holds because V k
h (xk

where βt = 2(cid:80) αi

i=1 αi
i=1 αi

h+1 + δk

h+1)](xk

h  ak
h)

(4.6)
h) is a
h  ak
h  a(cid:48)) =
h(xk

8

h)  and inequality y holds by Lemma 4.3 and the Bellman equation (2.1). Finally  equality

h(xk

h  ak

h+1 = (V (cid:63)

Qk
z holds by deﬁnition δk

h+1 − φk
K(cid:88)

We turn to computing the summation(cid:80)K
h+1 − V πk
K(cid:88)
k=1 δk
h(cid:88)

K(cid:88)

α0
nk
h

H =

k=1

k=1

nk

h+1)(xk

h+1).

h. Denoting by nk
H · I[nk

h = N k
h = 0] ≤ SAH .

The key step is to upper bound the second term in (4.6)  which is:

αi

nk
h

φki(xk

h+1

h ak
h)

 

k=1

i=1

h (xk

h  ak

h)  we have:

h  ak

h) is the episode in which (xk

where ki(xk
the summands in a different way. For every k(cid:48) ∈ [K]  the term φk(cid:48)
k > k(cid:48) if and only if (xk
second time it appears we have nk

h) was taken at step h for the ith time. We regroup
h+1 appears in the summand with
h + 1  the

h ). The ﬁrst time it appears we have nk

h + 2  and so on. Therefore

h   sk(cid:48)
h = nk(cid:48)

h) = (xk(cid:48)

h = nk(cid:48)

h  ak

h  sk

t=i αi

t = 1+ 1

H from Lemma 4.1.c. Plugging these back into (4.6) 

αnk(cid:48)
t ≤

h

1 +

1
H

φk

h+1 

(cid:18)

K(cid:88)

(cid:19) K(cid:88)

k=1

K(cid:88)

φk

h+1 +

δk
h+1 +

(βnk

h

+ ξk

h+1)

k=1

k=1

h

k=1

k=1

(βnk

+ ξk

h+1)  

1
(4.7)
δk
h+1 +
H
h+1 ≤ δk
h+1 (owing to the fact that V (cid:63) ≥ V πk). Recursing the
(cid:16)
= O(1) ·(cid:88)

(cid:17)
H 3SAKι(cid:1) = O(cid:0)√

H+1 ≡ 0  we have:
K(cid:88)
H(cid:88)
(cid:114)
h (x a)(cid:88)

H 2SAT ι(cid:1)

x≤ O(cid:0)√

H 2SA +

h+1)

+ ξk

(βnk

1 ≤ O
δk

H 3ι

N K

h=1

k=1

.

h

n=1

n

K(cid:88)

h(cid:88)

nk

αi

φki(xk

h ak
h)

where the ﬁnal inequality uses(cid:80)∞

h+1

k=1

i=1

nk
h

we have:

K(cid:88)

k=1

h ≤ SAH +
δk

≤ SAH +

1 +

1
H

1 +

k(cid:48)=1

≤ K(cid:88)
(cid:19) K(cid:88)
(cid:19) K(cid:88)

k=1

∞(cid:88)

t=nk(cid:48)

h +1

φk(cid:48)

h+1

φk

h+1 − K(cid:88)
K(cid:88)

k=1

where the ﬁnal inequality uses φk
result for h = 1  2  . . .   H  and using the fact δK

(cid:18)
(cid:18)

K(cid:88)

k=1

Finally  by the pigeonhole principle  for any h ∈ [H]:

(cid:115)

h

x a

k=1

k=1

βnk

H 3ι
nk
h

when N K
1 − p  we have:

≤ O(1) · K(cid:88)

K(cid:88)
where inequality x is true because(cid:80)
(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12) H(cid:88)
K(cid:88)
1 ≤ O(cid:0)H 2SA +
1 ≤ O(cid:0)H 2SA +

(cid:12)(cid:12)(cid:12) H(cid:88)
K(cid:88)
This establishes (cid:80)K
In sum  we have(cid:80)K

√
we have

k=1 δk
p to p/2 ﬁnishes the proof.

k=1 δk

ξk
h+1

√

h=1

h=1

k=1

k=1

√

(4.8)
h (x  a) = K and the left-hand side of x is maximized
h (x  a) = K/SA for all x  a. Also  by the AzumaHoeffding inequality  with probability

x a N K

H 4SAT ι ≥ H 2SA  and when T ≤

k=1 δk
H 4SAT ι. Therefore  we can remove the H 2SA term in the regret upper bound.

[(Ph − ˆPk

√

√

h  ak
h)

h)(V (cid:63)

h+1)](xk

h+1 − V k

(cid:12)(cid:12)(cid:12) ≤ cH
H 4SAT ι(cid:1). We note that when T ≥
H 4SAT ι  we have(cid:80)K
H 4SAT ι(cid:1)  with probability at least 1 − 2p. Rescaling
(cid:3)

H 4SAT ι 
1 ≤ HK = T ≤

T ι.
√

√

9

Acknowledgements

We thank Nan Jiang  Sham M. Kakade  Greg Yang and Chicheng Zhang for valuable discussions.
This work was supported in part by the DARPA program on Lifelong Learning Machines  and
Microsoft Research Gratis Traveler program.

References

[1] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning:
In Conference on Neural Information Processing Systems  pages

worst-case regret bounds.
1184–1194. Curran Associates Inc.  2017.

[2] Mohammad Azar  Remi Munos  Mohammad Ghavamzadeh  and Hilbert J Kappen. Speedy Q-
learning. In Conference on Neural Information Processing Systems  pages 2411–2419. Curran
Associates Inc.  2011.

[3] Mohammad Azar  R´emi Munos  and Hilbert J. Kappen. On the sample complexity of reinforce-
ment learning with a generative model. In Proceedings of the 29th International Conference
on Machine Learning (ICML)  2012.

[4] Mohammad Azar  R´emi Munos  and Hilbert J. Kappen. Minimax PAC bounds on the sample
complexity of reinforcement learning with a generative model. Machine Learning  91(3):325–
349  2013.

[5] Mohammad Azar  Ian Osband  and R´emi Munos. Minimax regret bounds for reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning (ICML) 
pages 263–272  2017.

[6] S´ebastien Bubeck  Nicolo Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochas-
tic multi-armed bandit problems. Foundations and Trends in Machine Learning  5(1):1–122 
2012.

[7] Marc Deisenroth and Carl E Rasmussen. PILCO: A model-based and data-efﬁcient approach
to policy search. In Proceedings of the 28th International Conference on machine learning
(ICML)  pages 465–472  2011.

[8] Eyal Even-Dar and Yishay Mansour. Learning rates for Q-learning. Journal of Machine Learn-

ing Research  5(Dec):1–25  2003.

[9] Maryam Fazel  Rong Ge  Sham Kakade  and Mehran Mesbahi. Global convergence of policy

gradient methods for linearized control problems. arXiv preprint arXiv:1801.05039  2018.

[10] Thomas Jaksch  Ronald Ortner  and Peter Auer. Near-optimal regret bounds for reinforcement

learning. Journal of Machine Learning Research  11:1563–1600  2010.

[11] Nan Jiang  Akshay Krishnamurthy  Alekh Agarwal  John Langford  and Robert E Schapire.
Contextual decision processes with low Bellman rank are PAC-learnable. arXiv preprint
arXiv:1610.09512  2016.

[12] Sham Kakade  Mengdi Wang  and Lin F Yang. Variance reduction methods for sublinear

reinforcement learning. ArXiv e-prints  abs/1802.09184  April 2018.

[14] Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time.

Machine Learning  49(2-3):209–232  2002.

[15] Sven Koenig and Reid G Simmons. Complexity analysis of real-time reinforcement learning.

In AAAI Conference on Artiﬁcial Intelligence  pages 99–105  1993.

[16] Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In International Con-

ference on Algorithmic Learning Theory  pages 320–334  2012.

[17] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Alex Graves  Ioannis Antonoglou  Daan
Wierstra  and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv
preprint arXiv:1312.5602  2013.

[18] Volodymyr Mnih  Adria Puigdomenech Badia  Mehdi Mirza  Alex Graves  Timothy Lillicrap 
Tim Harley  David Silver  and Koray Kavukcuoglu. Asynchronous methods for deep reinforce-
ment learning. In International Conference on Machine Learning (ICML)  pages 1928–1937 
2016.

10

[19] Anusha Nagabandi  Gregory Kahn  Ronald S Fearing  and Sergey Levine. Neural network
dynamics for model-based deep reinforcement learning with model-free ﬁne-tuning. arXiv
preprint arXiv:1708.02596  2017.

[20] Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning.

ArXiv e-prints  abs/1608.02732  April 2016.

[] Ian Osband  Benjamin Van Roy  and Zheng Wen. Generalization and exploration via random-

ized value functions. arXiv preprint arXiv:1402.0635  2014.

[21] Vitchyr Pong  Shixiang Gu  Murtaza Dalal  and Sergey Levine. Temporal difference models:

Model-free deep RL for model-based control. arXiv preprint arXiv:1802.09081  2018.

[22] John Schulman  Sergey Levine  Pieter Abbeel  Michael Jordan  and Philipp Moritz. Trust
region policy optimization. In International Conference on Machine Learning (ICML)  pages
1889–1897  2015.

[23] Aaron Sidford  Mengdi Wang  Xian Wu  and Yinyu Ye. Variance reduced value iteration and
faster algorithms for solving Markov decision processes. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms  pages 770–787. SIAM  2018.

[24] David Silver  Aja Huang  Chris J Maddison  Arthur Guez  Laurent Sifre  George Van
Den Driessche  Julian Schrittwieser  Ioannis Antonoglou  Veda Panneershelvam  Marc Lanc-
tot  et al. Mastering the game of Go with deep neural networks and tree search. Nature  529
(7587):484–489  2016.

[25] Alexander L Strehl  Lihong Li  Eric Wiewiora  John Langford  and Michael L Littman. PAC
model-free reinforcement learning. In Proceedings of the 23rd International Conference on
Machine Learning  pages 881–888. ACM  2006.

[26] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press

Cambridge  1998.

[27] Christopher Watkins. Learning from delayed rewards. PhD thesis  King’s College  Cambridge 

1989.

11

,Chi Jin
Zeyuan Allen-Zhu
Sebastien Bubeck
Michael Jordan