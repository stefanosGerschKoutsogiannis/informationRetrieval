2018,Generalization Bounds for Uniformly Stable Algorithms,Uniform stability of a learning algorithm is a classical notion of algorithmic stability introduced to derive high-probability bounds on the generalization error (Bousquet and Elisseeff  2002).  Specifically  for a loss function with range bounded in $[0 1]$  the generalization error of $\gamma$-uniformly stable learning algorithm on $n$ samples is known to be at most $O((\gamma +1/n) \sqrt{n \log(1/\delta)})$ with probability at least $1-\delta$. Unfortunately  this bound does not lead to meaningful generalization bounds in many common settings where $\gamma \geq 1/\sqrt{n}$. At the same time the bound is known to be tight only when $\gamma = O(1/n)$.
  Here we prove substantially stronger generalization bounds for uniformly stable algorithms without any additional assumptions. First  we show that the generalization error in this setting is at most $O(\sqrt{(\gamma + 1/n) \log(1/\delta)})$ with probability at least $1-\delta$. In addition  we prove a tight bound of $O(\gamma^2 + 1/n)$ on the second moment of the generalization error. The best previous bound on the second moment of the generalization error is $O(\gamma + 1/n)$. Our proofs are based on new analysis techniques and our results imply substantially stronger generalization guarantees for several well-studied algorithms.,Generalization Bounds for Uniformly Stable

Algorithms

Vitaly Feldman
Google Brain

Jan Vondrak

Stanford University

Abstract

is known to be within O((γ + 1/n)(cid:112)n log(1/δ)) of the empirical error with

Uniform stability of a learning algorithm is a classical notion of algorithmic stability
introduced to derive high-probability bounds on the generalization error (Bousquet
and Elisseeff  2002). Speciﬁcally  for a loss function with range bounded in [0  1] 
the generalization error of a γ-uniformly stable learning algorithm on n samples
probability at least 1 − δ. Unfortunately  this bound does not lead to meaningful
√
generalization bounds in many common settings where γ ≥ 1/
n. At the same
time the bound is known to be tight only when γ = O(1/n).
We substantially improve generalization bounds for uniformly stable algorithms
without making any additional assumptions. First  we show that the bound in this

setting is O((cid:112)(γ + 1/n) log(1/δ)) with probability at least 1 − δ. In addition 

we prove a tight bound of O(γ2 + 1/n) on the second moment of the estimation
error. The best previous bound on the second moment is O(γ + 1/n). Our proofs
are based on new analysis techniques and our results imply substantially stronger
generalization guarantees for several well-studied algorithms.

1

Introduction

√

We consider the basic problem of estimating the generalization error of learning algorithms. Over
the last couple of decades  a remarkably rich and deep theory has been developed for bounding
the generalization error via notions of complexity of the class of models (or predictors) output by
the learning algorithm. At the same time  for a variety of learning algorithms this theory does not
provide satisfactory bounds (even as compared with other theoretical analyses). Most notable among
these are continuous optimization algorithms that play the central role in modern machine learning.
For example  the standard generalization error bounds for stochastic gradient descent (SGD) on
convex Lipschitz functions cannot be obtained by proving uniform convergence for all empirical risk
minimizers (ERM) [13  26]. Speciﬁcally  there exist empirical risk minimizing algorithms whose
generalization error is
d times larger than the generalization error of SGD  where d is the dimension
of the problem (without the Lipschitzness assumption the gap is inﬁnite even for d = 2) [13]. This
disparity stems from the fact that uniform convergence bounds largely ignore the way in which
the model output by the algorithm depends on the data. We note that in the restricted setting of
generalized linear models one can obtain tight generalization bounds via uniform convergence [15].
Another classical and popular approach to proving generalization bounds is to analyze the stability of
the learning algorithm to changes in the dataset. This approach has been used to obtain relatively
strong generalization bounds for several convex optimization algorithms. For example  the seminal
works of Bousquet and Elisseeff [4] and Shalev-Shwartz et al. [26] demonstrate that for strongly
convex losses the ERM solution is stable. The use of stability is also implicit in standard analyses
of online convex optimization [26] and online-to-batch conversion [5]. More recently  Hardt et al.
[14] showed that for convex smooth losses the solution obtained via (stochastic) gradient descent is

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

stable. They also conjectured that stability can be used to understand the generalization properties of
algorithms used for training deep neural networks.
While a variety of notions of stability have been proposed and analyzed  most only lead to bounds on
the expectation or the second moment of the estimation error over the random choice of the dataset
(where estimation error refers to the difference between the true generalization error and the empirical
error). In contrast  generalization bounds based on uniform convergence show that the estimation
error is small with high probability (more formally  the distribution of the error has exponentially
decaying tails). This discrepancy was ﬁrst addressed by Bousquet and Elisseeff [4] who deﬁned the
notion of uniform stability.
Deﬁnition 1.1. Let A : Z n → F be a learning algorithm mapping a dataset S to a model in F and
(cid:96) : F × Z → R be a function such that (cid:96)(f  z) measures the loss of model f on point z. Then A is
said to have uniform stability γn with respect to (cid:96) if for any pair of datasets S  S(cid:48) ∈ Z n that differ in
a single element and every z ∈ Z  |(cid:96)(A(S)  z) − (cid:96)(A(S(cid:48))  z)| ≤ γn.
(cid:80)n
We denote the empirical loss of the algorithm A on S = (S1  . . .   Sn) by ES[(cid:96)(A(S))]
i=1 (cid:96)(A(S)  Si) and its expected loss relative to distribution P over Z by EP [(cid:96)(A(S))]
Ez∼P [(cid:96)(A(S)  z)]. We denote the estimation error1 of A on S relative to P by

.
=
.
=

1
n

∆P−S((cid:96)(A))

= EP [(cid:96)(A(S))] − ES[(cid:96)(A(S))].
.

We summarize the generalization properties of uniform stability in the below (all proved in [4]
although properties (1) and (2) are implicit in earlier work and also hold under weaker stability
notions). Let A : Z n → F be a learning algorithm that has uniform stability γn with respect to a loss
function (cid:96) : F × Z → [0  1]. Then for every distribution P over Z and δ > 0:

S∼P n

[∆P−S((cid:96)(A))]

(cid:12)(cid:12)(cid:12)(cid:12) ≤ γn;
(cid:12)(cid:12)(cid:12)(cid:12) E
(∆P−S((cid:96)(A)))2(cid:105) ≤ 1
(cid:104)
(cid:19)(cid:114)
(cid:18)

2n

+ 6γn;

E

S∼P n

(cid:34)

Pr
S∼P n

∆P−S((cid:96)(A)) ≥

4γn +

1
n

n ln(1/δ)

2

+ 2γn

(1)

(2)

(3)

(cid:35)

≤ δ.

√

(cid:18)

(cid:19)

As can be readily seen from eq.(3) the high probability bound is at least a factor
n larger than
the expectation of the estimation error. In addition  the bound on the estimation error implied by
eq.(2) is quadratically worse than the stability parameter. We note that eq. (1) does not imply that
EP [(cid:96)(A(S))] ≤ ES[(cid:96)(A(S))] + O(γn/δ) with probability at least 1 − δ since ∆P−S((cid:96)(A)) can be
negative and Markov’s inequality cannot be used. Such “low-probability” result is known only for
ERM algorithms for which Shalev-Shwartz et al. [26] showed that

[|∆P−S((cid:96)(A))|] ≤ O

E

S∼P n

γn +

1√
n

(4)

Naturally  for most algorithms the stability parameter needs be balanced against the guarantees on the
empirical error. For example  ERM solution to convex learning problems can be made uniformly
stable by adding a strongly convex term to the objective [26]. This change in the objective introduces
an error. In the other example  the stability parameter of gradient descent on smooth objectives is
determined by the sum of the rates used for all the gradient steps [14]. Limiting the sum limits
√
the empirical error that can be achieved. In both of those examples the optimal expected error can
only be achieved when γn = θ(1/
n) (which is also the expected suboptimality of the solutions).
Unfortunately  in this setting  eq. (3) gives a vacuous bound and only “low-probability” generalization
bounds are known for the ﬁrst example (since it is ERM and eq. (4) applies).
This raises a natural question of whether the known bounds in eq. (2) and eq. (3) are optimal. In
particular  Shalev-Shwartz et al. [26] conjecture that better high probability bounds can be achieved.

1Also referred to as the generalization gap is several recent works.

2

It is easy to see that the expectation of the absolute value of the estimation error can be at least
γn + 1√
n. Consequently  as observed already in [4]  eq. (3) is optimal when γn = O(1/n). (Note
that this is the optimal level of stability for non-trivial learning algorithms with (cid:96) normalized to
[0  1].) Yet both bounds in eq. (2) and eq.(3) are signiﬁcantly larger than this lower bound whenever
γn = ω(1/n). At the same time  to the best of our knowledge  no other upper or lower bounds on the
estimation error of uniformly stable algorithms were previously known.

1.1 Our Results

√

n(γn + 1/n) to(cid:112)γn + 1/n. This rate is non-vacuous for any non-trivial

We give two new upper bounds on the estimation error of uniformly stable learning algorithms.
Speciﬁcally  our bound on the second moment of the estimation error is O(γ2
n + 1/n) matching (up
to a constant) the simple lower bound of γn + 1√
n on the ﬁrst moment. Our high probability bound
improves the rate from
stability parameter γn = o(1) and matches the rate that was previously known only for the second
moment (eq. (2)).
For convenience and generality we state our bounds on the estimation error for arbitrary data
dependent functions (and not just losses of models). Speciﬁcally  let M : Z n×Z → R be an algorithm
that is given a dataset S and a point z as an input. It can be thought of as computing a real-valued
(cid:80)n
function M (S ·) and then applying it to z. In the case of learning algorithms M (S  z) = (cid:96)(A(S)  z)
but this notion also captures other data statistics whose choice may depend on the data. We denote
i=1 M (S  Si)  expectation relative to distribution P over Z by
the empirical mean ES[M (S)]
EP [M (S)]

.
= Ez∼P [M (S  z)] and the estimation error by

.
= 1
n

∆P−S(M )

= EP [M (S)] − ES[M (S)].
.

Uniform stability for data-dependent functions is deﬁned analogously (Def. 2.1).
Theorem 1.2. Let M : Z n × Z → [0  1] be a data-dependent function with uniform stability γn.
Then for any probability distribution P over Z and any δ ∈ (0  1):

n +

2
n

;

· ln(8/δ)

(cid:35)

≤ δ.

(5)

(6)

(cid:104)

(∆P−S(M ))2(cid:105) ≤ 16γ2
(cid:19)

(cid:115)(cid:18)

∆P−S(M ) ≥ 8

2γn +

1
n

E

S∼P n

(cid:34)

Pr
S∼P n

The results in Theorem 1.2 are stated only for deterministic functions (or algorithms). They can be
extended to randomized algorithms in several standard ways [12  26]. If M is uniformly γ-stable
with high probability over the choice of its random bits then one can obtain a statement which holds
with high probability over the choice of both S and the random bits (e.g. [19]). Alternatively  one
can always consider the function M(cid:48)(S  z) = EM [M (S  z)]. If M(cid:48)(S  z) is uniformly γ-stable then
Thm. 1.2 can be applied to it. The resulting statement will be only about the expected value of the
estimation error with expectation taken over the randomness of the algorithm. Further  if M is used
with independent randomness in each evaluation of M (S  Si) then the empirical mean ES[M (S)]
will be strongly concentrated around ES[M(cid:48)(S)] (whenever the variance of each evaluation is not too
large). We note that randomized algorithms also allow to extend the notion of uniform stability to
binary classiﬁcation algorithms by considering the expectation of the 0/1 loss.
A natural and  we believe  important question left open by our work is whether the high probability
result in eq. (6) is tight.

Our techniques The high-probability generalization result in [4] (eq. (3)) is based on a simple
observation that as a function of S  ∆P−S(M ) has the bounded differences property. Replacing any
element of S can change ∆P−S(M ) by at most 2γn + 1/n (where γn comes from changing the
function M (S ·) to M (S(cid:48) ·) and 1/n comes the change in one of the points on which this function
is evaluated). Applying McDiarmid’s concentration inequality immediately implies concentration
with rate
n(2γn + 1/n) around the expectation. The expectation  in turn  is small by eq. (1). In
contrast  our approach uses stability itself as a tool for proving concentration inequalities. It is based
on ideas developed in [2] to prove generalization bounds for differentially private algorithms in the

√

3

context of adaptive data analysis [11]. It was recently shown that this proof approach can be used to
re-derive and extend several standard concentration inequalities [23  27].
At a high level  the ﬁrst step of the argument reduces the task of proving a bound on the tail of a
non-negative real-valued random variable to bounding the expectation of the maximum of multiple
independent samples of that random variable. We then show that from multiple executions of M
on independently chosen datasets it is possible to select the execution of M with approximately the
largest estimation error in a stable way. That is  uniform stability of M allows us to ensure that the
selection procedure is itself uniformly stable. The selection procedure is based on the exponential
mechanism [21] and satisﬁes differential privacy [9](Def. 2.3). The stability of this procedure allows
us to bound the expectation of the estimation error of the execution of M with approximately the
largest estimation error (among the multiple executions). This gives us the desired bound on the
expectation of the maximum of multiple independent samples of the estimation error random variable.
We remark that the multiple executions and an algorithm for selecting among them exist purely for
the purposes of the proof technique and do not require any modiﬁcations to the algorithm itself.
Our approach to proving the bound on the second moment of the estimation error is based on two
ideas. First we decouple the point on which each M (S) is estimated from S by observing that for
every dataset S the empirical mean is within 2γn of the “leave-one-out" estimate of the true mean.

Speciﬁcally  our leave-one-out estimator is deﬁned as Ez∼P(cid:2) 1

i=1 M (Si←z  Si)(cid:3)  where Si←z
(cid:80)n

denotes replacing the element in S at index i with z. We then bound the second moment of the
estimation error of the leave-one-out estimate by bounding the effect of dependence between the
random variables by O(γ2

n

n + 1/n).

Applications We now apply our bounds on the estimation error to several known uniformly stable
algorithms in a straightforward way. Our main focus are learning problems that can be formulated as
stochastic convex optimization. Speciﬁcally  these are problems in which the goal is to minimize the
= Ez∼P [(cid:96)(w  z)] over w ∈ K ⊂ Rd for some convex body K and a family of
.
expected loss: FP (w)
convex losses F = {(cid:96)(·  z)}z∈Z. The stochastic convex optimization problem for a family of losses
F over K is the problem of minimizing FP (w) for an arbitrary distribution P over Z.
For concreteness  we consider the well-studied setting in which F contains 1-Lipschitz convex
functions with range in [0  1] and K is included in the unit ball. In this case ERM with a strongly
2(cid:107)w(cid:107)2 has uniform stability of 1/(λn) [4  26]. From here  applying Markov’s
convex regularizer λ
√
inequality to eq. (4)  Shalev-Shwartz et al. [26] obtain a “low-probability" generalization bound for
the solution. Their bound on the true loss is within O(1/
δn) from the optimum with probability at
least 1− δ. Applying eq. (5) with Chebyshev’s inequality improves the dependence on δ quadratically 
sub-optimality of the solution is at most O((cid:112)log(1/δ)/n1/3).
that is to O(1/(δ1/4√
n)). Further  using eq. (5) we obtain that for an appropriate choice of λ  the

Another algorithm that was shown to be uniformly stable is gradient descent2 on sufﬁciently smooth
convex functions [14]. The generalization bounds we obtain for this algorithm are similar to those
√
we get for the strongly convex ERM. We note that for the stability-based analysis in this case even
“low-probability" generalization bounds were not known for the optimal error rate of 1/
Finally  we show that our results can be used to improve the recent bounds on estimation error of
learning algorithms with differentially private prediction. These are algorithms introduced to model
privacy-preserving learning in the settings where users only have black-box access to the learned
model via a prediction interface [10]. The properties of differential privacy imply that the expectation
over the randomness of a predictor K : (X × Y )n × X of the loss of K at any point x ∈ X is
uniformly stable. Speciﬁcally  for an -differentially private prediction algorithm  every loss function
(cid:96) : Y ×Y → [0  1]  two datasets S  S(cid:48) ∈ (X ×Y )n that differ in a single element and (x  y) ∈ X ×Y :

n.

(cid:12)(cid:12)(cid:12)(cid:12) ≤ e − 1.

(cid:12)(cid:12)(cid:12)(cid:12)E

K

[(cid:96)(K(S  x)  y)] − E

[(cid:96)(K(S(cid:48)  x)  y)]

M

Therefore  our generalization bounds can be directly applied to the data-dependent function
.
= EK[(cid:96)(K(S  x)  y)]. These bounds can  in turn  be used to get stronger genera-
M (S  (x  y))

2The analysis in [14] focuses on the stochastic gradient descent and derives uniform stability for the
expectation of the loss (over the randomness of the algorithm). However their analysis applies to gradient steps
on smooth functions more generally.

4

lization bounds for one of the learning algorithms proposed in [10] (that has unbounded model
complexity).
Additional details of these applications can be found in the supplemental material.

1.2 Additional related work

The use of stability for understanding of generalization properties of learning algorithms dates back
to the pioneering work of Rogers and Wagner [25]. They showed that expected sensitivity of a
classiﬁcation algorithm to changes of individual examples can be used to obtain a bound on the
variance of the leave-one-out estimator for the k-NN algorithm. Early work on stability focused
on extensions of these results to other “local” algorithms and estimators and focused primarily on
variance (a notable exception is [8] where high probability bounds on the generalization error of
k-NN are proved). See [7] for an overview. In a somewhat similar spirit  stability is also used for
analysis of the variance of the k-fold cross-validation estimator [3  16  17].
A long line of work focuses on the relationship between various notions of stability and learnability
in supervised setting (see [24  26] for an overview). This work employs relatively weak notions of
average stability and derives a variety of asymptotic equivalence results. The results in [4] on uniform
stability and their applications to generalization properties of strongly convex ERM algorithms
have been extended and generalized in several directions (e.g. [18  28  30]). Maurer [20] considers
generalization bounds for a special case of linear regression with a strongly convex regularizer and a
sufﬁciently smooth loss function. Their bounds are data-dependent and are potentially stronger for
large values of the regularization parameter (and hence stability). However the bound is vacuous when
the stability parameter is larger than n−1/4 and hence is not directly comparable to ours. Finally 
recent work of Abou-Moustafa and Szepesvári [1] gives high-probability generalization bounds
similar to those in [4] but using a bound on a high-order moment of stability instead of the uniform
stability. We also remark that all these works are based on techniques different from ours.
Uniform stability plays an important role in privacy-preserving learning since a differentially private
learning algorithm can usually be obtained one by adding noise to the output of a uniformly stable
one (e.g. [6  10  29]).

2 Preliminaries
For a domain Z  a dataset S ∈ Z n is an n-tuple of elements in Z. We refer to element with index i
by Si and by Si←z to the dataset obtained from S by setting the element with index i to z. We refer
to a function that takes as an input a dataset S ∈ Z n and a point z ∈ Z as a data-dependent function
over Z. We think of data-dependent functions as outputs of an algorithm that takes S as an input. For
example in supervised learning Z is the set of all possible labeled examples Z = X × Y and the
algorithm M is deﬁned as estimating some loss function (cid:96)Y : Y × Y → R+ of the model hS output
by a learning algorithm A(S) on example z = (x  y). That is M (S  z) = (cid:96)Y (hS(x)  y). Note that in
this setting EP [M (S)] is exactly the true loss of hS on data distribution P  whereas ES[M (S)] is the
empirical loss of hS.
Deﬁnition 2.1. A data-dependent function M : Z n × Z → R has uniform stability γ if for all
S ∈ Z n  i ∈ [n]  zi  z ∈ Z  |M (S  z) − M (Si←zi  z)| ≤ γ.
This deﬁnition is equivalent to having M (S  z) having sensitivity γ or γ-bounded differences for all
z ∈ Z.
Deﬁnition 2.2. A real-valued function f : Z n → R has sensitivity at most γ if for all S ∈ Z n 
i ∈ [n]  zi  z ∈ Z  |f (S) − f (Si←zi)| ≤ γ.
We will also rely on several elementary properties of differential privacy [9]. In this context differential
privacy is simply a form of uniform stability for randomized algorithms.
Deﬁnition 2.3 ([9]). An algorithm A : Z n → Y is -differentially private if  for all datasets
S  S(cid:48) ∈ Z n that differ on a single element 

∀E ⊆ Y

Pr[A(S) ∈ E] ≤ e Pr[A(S(cid:48)) ∈ E].

5

3 Generalization with Exponential Tails

Our approach to proving the high-probability generalization bounds is based on the technique
introduced by Nissim and Stemmer [22] (see [2]) to show that differentially private algorithm have
strong generalization properties. It has recently been pointed out by Steinke and Ullman [27] that
this approach can be used to re-derive the standard Bernstein  Hoeffding  and Chernoff concentration
inequalities. Nissim and Stemmer [23] used the same approach to generalize McDiarmid’s inequality
to functions with unbounded (or high) sensitivity.
We prove a bound on the tail of a random variable by bounding the expectation of the maximum of
multiple independent samples of the random variable. Speciﬁcally  the following simple lemma (see
[27] for proof):
Lemma 3.1. Let Q be a probability distribution over the reals. Then
v1 ... vm∼Q [max{0  v1  v2  . . .   vm}]

≤ ln(2)
m

v ≥ 2 ·

Pr
v∼Q

(cid:20)

(cid:21)

E

.

The second step relies on the relationship between the maximum of a set of values and the value
chosen by the soft-argmax  which we refer to as the stable-max. Speciﬁcally  we deﬁne

(cid:88)

i∈[m]

evi(cid:80)

vi ·

 

(cid:96)∈[m] ev(cid:96)

stablemax{v1  . . .   vm} .
=

evi(cid:80)
(cid:96)∈[m] ev(cid:96) should be thought of as the relative weight assigned to value vi. (We remark
where
that this vector of weights is commonly referred to as softmax and soft-argmax. We therefore use
stable-max to avoid confusion between the weights and the weighted sum of values.) The ﬁrst
property of the stable-max is that its value is close to the maximum:

stablemax{v1  . . .   vm} ≥ max{v1  . . .   vm} − ln m


.

The second property that we will use is that the weight (or probability) assigned to each value is stable:
it changes by a factor of at most e2γ whenever each of the values changes by at most γ. These two
properties are known properties of the exponential mechanism [21]. More formally  the exponential
mechanism is the randomized algorithm that given values {v1  . . .   vm} and   outputs the index i
evi(cid:80)
with probability
(cid:96)∈[m] ev(cid:96) . We state the properties of the exponential mechanism specialized to our
context below.
Theorem 3.2. [2  21] Let f1  . . .   fm : Z n → R be m scoring functions of a dataset each of
sensitivity at most ∆. Let A be the algorithm that given a dataset S ∈ Z n and a parameter  > 0
outputs an index (cid:96) ∈ [m] with probability proportional to e 
2∆·f(cid:96)(S). Then A is -differentially private
and  further  for every S ∈ Z n:

E

(cid:96)=A(S)

[f(cid:96)(S)] ≥ max
(cid:96)∈[m]

{f(cid:96)(S)} − 2∆


· ln m.

We now deﬁne the scoring functions designed to select the execution of M with the worst estimation
error. For these purposes our dataset will consist of m datasets each of size n. To avoid confusion 
we emphasize this by referring to it as multi-dataset and using S to denote it. That is S ∈ Z m×n and
we refer to each of the sub-datasets as S1  . . .  Sm and to an element i of sub-dataset (cid:96) as S(cid:96) i.
Lemma 3.3. Let M : Z n × Z → [0  1] be a data-dependent function with uniform stability γ. For
a probability distribution P over Z  multi-dataset S ∈ Z m×n and an index (cid:96) ∈ [m] we deﬁne the
scoring function

f(cid:96)(S)

= ∆P−S(cid:96)(M ) = EP [M (S(cid:96))] − ES(cid:96)[M (S(cid:96))].
.

Then f(cid:96) has sensitivity 2γ + 1/n.
Proof. Let S and S(cid:48) be two multi-datasets that differ in a single element at index i in sub-dataset k.
Clearly  if k (cid:54)= (cid:96) then S(cid:96) = S(cid:48)
(cid:96) differ in a single element.
Thus

(cid:96) and f(cid:96)(S) = f(cid:96)(S(cid:48)). Otherwise  S(cid:96) and S(cid:48)
z∼P[M (S(cid:96)  z) − M (S(cid:48)

(cid:96))]| =

|EP [M (S(cid:96))] − EP [M (S(cid:48)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ γ.

(cid:12)(cid:12)(cid:12)(cid:12) E

(cid:96)  z)]

6

and

(cid:12)(cid:12)(cid:12) =

[M (S(cid:48)
(cid:96))]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

n

(cid:88)

j∈[n]

M (S(cid:96) S(cid:96) j) − 1
n

(cid:88)
·(cid:12)(cid:12)M (S(cid:48)

j∈[n]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) +

1
n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

M (S(cid:48)

(cid:96) S(cid:48)

(cid:96) j)

(cid:96) i)(cid:12)(cid:12)

(M (S(cid:96) S(cid:96) j) − M (S(cid:48)

(cid:96) S(cid:96) j))

(cid:96) S(cid:96) i) − M (S(cid:48)

(cid:96) S(cid:48)

(cid:12)(cid:12)(cid:12)ES(cid:96)[M (S(cid:96))] − ES(cid:48)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

(cid:88)

j∈[n] j(cid:54)=i

≤

(cid:96)

n
≤ γ +

1
n

.

The ﬁnal (and new) ingredient of our proof is a bound on the expected estimation error of any
uniformly stable algorithm on a sub-dataset chosen in a differentially private way.
Lemma 3.4. For (cid:96) ∈ [m]  let M(cid:96) : Z n × Z → [0  1] be a data-dependent function with uniform
stability γ. Let A : Z n×m → [m] be an -differentially private algorithm. Then for any distribution
P over Z  we have that:

e−VS − γ ≤

E
= ES∼Pmn (cid:96)=A(S) [ES(cid:96)[M(cid:96)(S(cid:96))]] .

S∼Pmn (cid:96)=A(S)

where VS .

[EP [M(cid:96)(S(cid:96))]] ≤ eVS + γ 

Proof.

VS =

=

=



M(cid:96)(S(cid:96) S(cid:96) i)

1(A(S) = (cid:96)) · M(cid:96)(S(cid:96) S(cid:96) i)

 1
(cid:88)

n

i∈[n]

(cid:88)
(cid:88)
(cid:20)


(cid:21)

ES∼Pmn

[1(A(S) = (cid:96))] · M(cid:96)(S(cid:96) S(cid:96) i)
E
A

i∈[n]

(cid:96)∈[m]

A S∼Pmn

i∈[n]

(cid:96)∈[m]

E

S∼Pmn (cid:96)=A(S)

n

 1
(cid:88)
(cid:88)
(cid:88)

E

(cid:88)
(cid:88)
(cid:88)

≤ 1
n

1
n

1
n

=

=

E

S∼Pmn z∼P

i∈[n]

(cid:96)∈[m]

E

S∼Pmn z∼P

i∈[n]

(cid:96)∈[m]

(cid:20)
(cid:20)

e · E

A

e · E

A

(cid:21)

(cid:21)

[1(A(S (cid:96) i←z) = (cid:96))] · (M(cid:96)(S i←z

(cid:96)

 S(cid:96) i) + γ)

[1(A(S) = (cid:96))] · (M(cid:96)(S(cid:96)  z) + γ)

(cid:18)

S∼Pmn z∼P (cid:96)=A(S)

E

[e · (M(cid:96)(S(cid:96)  z) + γ)] = e ·

S∼Pmn z∼P (cid:96)=A(S)

E

[M(cid:96)(S(cid:96)  z)] + γ

This gives the left hand side of the stated inequality. The right hand side is obtained analogously.

We are now ready to put the ingredients together to prove the claimed result:

(cid:19)

.

Proof of eq. (6) in Theorem 1.2. We choose m = ln(2)/δ. Let f1  . . .   fm be the scoring functions
deﬁned in Lemma 3.3. Let fm+1(S) ≡ 0. Let A be the execution of the exponential mechanism with
∆ = 2γ + 1/n on scoring functions f1  . . .   fm+1 and  to be deﬁned later. Note that this corresponds
to the setting of Lemma 3.4 with M(cid:96) ≡ M for all (cid:96) ∈ [m] and Mm+1 ≡ 0. By Lemma 3.4 we have
that

(cid:20)

(cid:21)

ES∼P (m+1)n

E

(cid:96)=A(S)

[f(cid:96)(S)]

=

E

S∼P (m+1)n (cid:96)=A(S)

[EP [M(cid:96)(S(cid:96))] − ES(cid:96)[M(cid:96)(S(cid:96))]] ≤ e − 1 + γ.

7



(cid:113)(cid:0)2γ + 1
(cid:1) · ln(m + 1) =
(cid:115)(cid:18)
(cid:19)
(cid:19)

· ln(8/δ)

(cid:35)

2γ +

1
n

(cid:115)(cid:18)

(cid:12)(cid:12)(cid:12)(cid:12) E

By Theorem 3.2

(cid:20)

(cid:26)

max

(cid:20)

ES∼Pmn
≤ ES∼Pmn

0  max
(cid:96)∈[m]
[f(cid:96)(S)]

E

(cid:96)=A(S)

(cid:27)(cid:21)
(cid:21)
EP [M (S(cid:96))] − ES(cid:96) [M (S(cid:96))]

= ES∼Pmn
ln(m + 1) ≤ e − 1 + γ +

+

(cid:20)

max
(cid:96)∈[0.m]
4γ + 2/n

(cid:21)

f(cid:96)(S)

2∆


(cid:113)(cid:0)2γ + 1

To bound this expression we choose  =
Our bound is at least 2 and hence holds trivially if  ≥ 1/2. Otherwise (e − 1) ≤ 2 and we obtain
the following bound on the expectation of the maximum.

n

n

ln(m + 1).

(cid:1) · ln(e ln(2)/δ).

(cid:115)(cid:18)
where we used that γ ≤ √

4

2γ +

1
n

(cid:19)

(cid:34)

· ln(e ln(2)/δ) + γ ≤ 4

γ. Finally  plugging this bound into Lemma 3.1 we obtain that

EP [M (S)] − ES [M (S)] ≥ 8

PrS∼P n

2γ +

1
n

· ln(8/δ)

≤ ln(2)
m

≤ δ.

(cid:12)(cid:12)(cid:12)(cid:12) ≤ γ.

4 Second Moment of the Estimation Error

In this section we prove eq. (5) of Theorem 1.2. It will be more convenient to directly work with
= M (S  z) − EP [M (S)]. Clearly  L is
.
the unbiased version of M. Speciﬁcally  we deﬁne L(S  z)
unbiased with respect to P in the sense that for every S ∈ Z n  EP [L(S)] = 0. Note that if the range
of M is [0  1] then the range of L is [−1  1]. Further  L has uniform stability of at most 2γ since for
two datasets S and S(cid:48) that differ in a single element 

|EP [M (S)] − EP [M (S(cid:48))]| ≤

z∼P[M (S  z) − M (S(cid:48)  z)]

Observe that

∆P−S(M (S)) =

1
n

By eq. (7) we obtain that

(EP [M (S)] − M (S  Si)) =

n(cid:88)
(∆P−S(M (S)))2(cid:105)
(cid:104)

i=1

L(S  Si) = −ES[L(S)].

(7)

−1
n

n(cid:88)
(cid:104)
(ES[L(S)])2(cid:105)

i=1

.

= E

S∼P n

E

S∼P n

Therefore eq. (5) of Theorem 1.2 will follow immediately from the following lemma (by using it with
stability 2γ).
Lemma 4.1. Let L : Z n × Z → [−1  1] be a data-dependent function with uniform stability γ and
P be an arbitrary distribution over Z. If L is unbiased with respect to P then:

(cid:104)

(ES[L(S)])2(cid:105) ≤ 4γ2 +

2
n

.

E

S∼P n

Our proof starts by ﬁrst establishing this result for the leave-one-out estimate.
Lemma 4.2. For a data-dependent function L : Z n × Z → [−1  1]  a dataset S ∈ Z n and a
distribution P  deﬁne

= E
z∼P

L(Si←z  Si)

If L has uniform stability γ and is unbiased with respect to P then:
1
n

(cid:2)L(cid:0)S←P(cid:1)(cid:3)(cid:1)2(cid:105) ≤ γ2 +

S∼P n

E

.

ES

(cid:2)L(cid:0)S←P(cid:1)(cid:3) .
(cid:104)(cid:0)ES

 .

 1

n

(cid:88)

i∈[n]

8

Proof.

E

S∼P n

=

1
n2
≤ 1
n

(cid:104)(cid:0)ES
(cid:88)

i∈[n]
1
n2

+

(cid:2)L(cid:0)S←P(cid:1)(cid:3)(cid:1)2(cid:105) ≤

E

S∼P n z∼P

(cid:104)(cid:0)L(Si←z  Si)(cid:1)2(cid:105)


 1

n

1
n2

(cid:88)
(cid:88)

i∈[n]

S∼P n z∼P

E

(cid:88)

i j∈[n] i(cid:54)=j

E

S∼P n z∼P

+

(cid:2)L(Si←z  Si) · L(Sj←z  Sj)(cid:3)  

i j∈[n] i(cid:54)=j

E

S∼P n z∼P

L(Si←z  Si)

2
(cid:2)L(Si←z  Si) · L(Sj←z  Sj)(cid:3)

(8)

where we used convexity to obtain the ﬁrst line and the bound on the range of L to obtain the last
inequality. For a ﬁxed i (cid:54)= j and a ﬁxed setting of all the elements in S with other indices (which we
denote by S−i j) we now analyze the cross term

(cid:2)L(Si←z  Si) · L(Sj←z  Sj)(cid:3) .

.
=

vi j

E

Si Sj  z∼P

For z ∈ Z  deﬁne

g(z) = min
zi zj∈Z

L(Si j←zi zj   z) + γ.

(We remark that g implicitly depends on i  j and S−i j). Uniform stability of L implies that

max
zi zj∈Z

L(Si j←zi zj   z) ≤ min
zi zj∈Z

L(Si j←zi zj   z) + 2γ.

=

(9)

E

E

vi j =

Si Sj  z∼P

Si Sj  z∼P

Using this inequality we obtain

This means that for all zi  zj  z ∈ Z (cid:12)(cid:12)L(Si j←zi zj   z) − g(z)(cid:12)(cid:12) ≤ γ.
(cid:2)L(Si←z  Si) · L(Sj←z  Sj)(cid:3)
(cid:2)(L(Si←z  Si) − g(Si)) · (L(Sj←z  Sj) − g(Sj))(cid:3) +
Si Sj∼P [g(Si) · g(Sj)]

(cid:2)g(Si) · L(Sj←z  Sj)(cid:3)
(cid:18)
(cid:19)2
z(cid:48)∼P[g(z(cid:48))]
Note that L is unbiased and g does not depend on Si or Sj. Therefore  for every ﬁxed setting of Si
and z 

(cid:2)g(Sj) · L(Si←z  Si)(cid:3) − E
(cid:2)g(Sj) · L(Si←z  Si)(cid:3) −
(cid:2)g(Si) · L(Sj←z  Sj)(cid:3) +
(cid:2)g(Si) · L(Sj←z  Sj)(cid:3) = g(Si) · EP [L(Sj←z)] = 0.
(cid:2)g(Si) · L(Sj←z  Sj)(cid:3) +

Therefore 

E

Si Sj  z∼P

≤ γ2 +

Si Sj  z∼P

Si Sj  z∼P

Si Sj  z∼P

Si Sj  z∼P

E
Sj∼P

E

Si Sj  z∼P

E

E

E

.

+

E

E

implying that vi j ≤ γ2. Substituting this into eq.(8) we obtain the claim.

(cid:2)g(Sj) · L(Si←z  Si)](cid:3) = 0.
(cid:2)L(cid:0)S←P(cid:1)(cid:3)(see supplemental material for

We can now obtain the proof of Lemma 4.1 by observing that for every S  the empirical mean
ES[L(S)] is within γ of our leave-one-out estimator ES
the proof).

References
[1] Karim T. Abou-Moustafa and Csaba Szepesvári. An exponential tail bound for lq stable learning
rules. application to k-folds cross-validation. In ISAIM  2018. URL http://isaim2018.cs.
virginia.edu/papers/ISAIM2018_Abou-Moustafa_Szepesvari.pdf.

9

[2] Raef Bassily  Kobbi Nissim  Adam D. Smith  Thomas Steinke  Uri Stemmer  and Jonathan

Ullman. Algorithmic stability for adaptive data analysis. In STOC  pages 1046–1059  2016.

[3] Avrim Blum  Adam Kalai  and John Langford. Beating the hold-out: Bounds for k-fold and

progressive cross-validation. In COLT  pages 203–208  1999.

[4] Olivier Bousquet and André Elisseeff. Stability and generalization. JMLR  2:499–526  2002.

[5] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning

algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[6] Kamalika Chaudhuri  Claire Monteleoni  and Anand D. Sarwate. Differentially private empirical

risk minimization. Journal of Machine Learning Research  12:1069–1109  2011.

[7] L. Devroye  L. Györﬁ  and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer 

1996.

[8] Luc Devroye and Terry J. Wagner. Distribution-free inequalities for the deleted and holdout

error estimates. IEEE Trans. Information Theory  25(2):202–207  1979.

[9] C. Dwork  F. McSherry  K. Nissim  and A. Smith. Calibrating noise to sensitivity in private

data analysis. In TCC  pages 265–284  2006.

[10] Cynthia Dwork and Vitaly Feldman. Privacy-preserving prediction. CoRR  abs/1803.10266 

2018. URL http://arxiv.org/abs/1803.10266. Extended abstract in COLT 2018.

[11] Cynthia Dwork  Vitaly Feldman  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Aaron
Roth. Preserving statistical validity in adaptive data analysis. CoRR  abs/1411.2664  2014.
Extended abstract in STOC 2015.

[12] André Elisseeff  Theodoros Evgeniou  and Massimiliano Pontil. Stability of randomized
learning algorithms. Journal of Machine Learning Research  6:55–79  2005. URL http:
//www.jmlr.org/papers/v6/elisseeff05a.html.

[13] Vitaly Feldman. Generalization of ERM in stochastic convex optimization: The dimension
strikes back. CoRR  abs/1608.04414  2016. URL http://arxiv.org/abs/1608.04414.
Extended abstract in NIPS 2016.

[14] Moritz Hardt  Ben Recht  and Yoram Singer. Train faster  generalize better: Stability of
stochastic gradient descent. In ICML  pages 1225–1234  2016. URL http://jmlr.org/
proceedings/papers/v48/hardt16.html.

[15] S. Kakade  K. Sridharan  and A. Tewari. On the complexity of linear prediction: Risk bounds 

margin bounds  and regularization. In NIPS  pages 793–800  2008.

[16] Satyen Kale  Ravi Kumar  and Sergei Vassilvitskii. Cross-validation and mean-square stability.
In Innovations in Computer Science - ICS  pages 487–495  2011. URL http://conference.
itcs.tsinghua.edu.cn/ICS2011/content/papers/31.html.

[17] Ravi Kumar  Daniel Lokshtanov  Sergei Vassilvitskii  and Andrea Vattani. Near-optimal
In ICML  pages 27–35  2013. URL http:

bounds for cross-validation via loss stability.
//jmlr.org/proceedings/papers/v28/kumar13a.html.

[18] Tongliang Liu  Gábor Lugosi  Gergely Neu  and Dacheng Tao. Algorithmic stability and
hypothesis complexity. In ICML  pages 2159–2167  2017. URL http://proceedings.mlr.
press/v70/liu17c.html.

[19] Ben London. A pac-bayesian analysis of randomized learning with application to stochastic

gradient descent. In NIPS  pages 2935–2944  2017.

[20] Andreas Maurer. A second-order look at stability and generalization. In COLT  pages 1461–

1475  2017. URL http://proceedings.mlr.press/v65/maurer17a.html.

[21] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In FOCS  pages

94–103  2007.

10

[22] Kobbi Nissim and Uri Stemmer. On the generalization properties of differential privacy. CoRR 

abs/1504.05800  2015.

[23] Kobbi Nissim and Uri Stemmer. Concentration bounds for high sensitivity functions through
differential privacy. CoRR  abs/1703.01970  2017. URL http://arxiv.org/abs/1703.
01970.

[24] Tomaso Poggio  Ryan Rifkin  Sayan Mukherjee  and Partha Niyogi. General conditions for

predictivity in learning theory. Nature  428(6981):419–422  2004.

[25] W. H. Rogers and T. J. Wagner. A ﬁnite sample distribution-free performance bound for local

discrimination rules. The Annals of Statistics  6(3):506–514  1978.

[26] Shai Shalev-Shwartz  Ohad Shamir  Nathan Srebro  and Karthik Sridharan. Learnability 
stability and uniform convergence. The Journal of Machine Learning Research  11:2635–2670 
2010.

[27] Thomas Steinke and Jonathan Ullman. Subgaussian tail bounds via stability arguments. arXiv

preprint arXiv:1701.03493  2017. URL https://arxiv.org/abs/1701.03493.

[28] Rosasco Lorenzo Wibisono  Andre and Tomaso Poggio. Sufﬁcient conditions for uniform
stability of regularization algorithms. Technical Report MIT-CSAIL-TR-2009-060  MIT  2009.

[29] Xi Wu  Fengan Li  Arun Kumar  Kamalika Chaudhuri  Somesh Jha  and Jeffrey Naughton. Bolt-
on differential privacy for scalable stochastic gradient descent-based analytics. In (SIGMOD) 
pages 1307–1322  2017.

[30] Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation  15(6):1397–1437 

2003.

11

,Vitaly Feldman
Jan Vondrak