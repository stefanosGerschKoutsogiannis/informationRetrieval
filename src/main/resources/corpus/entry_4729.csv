2018,Revisiting Decomposable Submodular Function Minimization with Incidence Relations,We introduce a new approach to decomposable submodular function minimization (DSFM) that exploits incidence relations. Incidence relations describe which variables effectively influence the component functions  and when properly utilized  they allow for improving the convergence rates of DSFM solvers. Our main results include the precise parametrization of the DSFM problem based on incidence relations  the development of new scalable alternative projections and parallel coordinate descent methods and an accompanying rigorous analysis of their convergence rates.,Revisiting Decomposable Submodular Function

Minimization with Incidence Relations

Pan Li
UIUC

panli2@illinois.edu

Abstract

Olgica Milenkovic

UIUC

milenkov@illinois.edu

We introduce a new approach to decomposable submodular function minimiza-
tion (DSFM) that exploits incidence relations. Incidence relations describe which
variables effectively inﬂuence the component functions  and when properly uti-
lized  they allow for improving the convergence rates of DSFM solvers. Our
main results include the precise parametrization of the DSFM problem based on
incidence relations  the development of new scalable alternative projections and
parallel coordinate descent methods and an accompanying rigorous analysis of
their convergence rates.

Introduction

1
A set function F : 2[N ] → R over a ground set [N ] is termed submodular if for all pairs of sets
S1  S2 ⊆ [N ]  one has F (S1) + F (S2) ≥ F (S1 ∩ S2) + F (S1 ∪ S2). Submodular functions capture
the ubiquitous phenomenon of diminishing marginal costs [1] and they frequently arise as part of the
objective function of various machine learning optimization problems [2  3  4  5  6  7].
Among the various submodular function optimization problems  submodular function minimization
(SFM)  which may be stated as minS⊆[N ] F (S)  is one of the most important and commonly studied
questions. The current fastest known SFM algorithm has complexity O(N 4 logO(1) N + τ N 3) 
where τ denotes the time needed to evaluate the submodular function [8]. Although SFM solvers
operate in time polynomial in N  the high-degree of the underlying polynomial prohibits their use
in practical large-scale settings. For this reason  a recent line of work has focused on developing
scalable and parallelizable algorithms for solving the SFM problem by leveraging the property of
decomposability [9]. Decomposability asserts that the submodular function may be written as a sum
of “simpler” submodular functions that may be optimized sequentially or in parallel. Formally  the
underlying problem  referred to as decomposable SFM (DSFM)  may be stated as:

(cid:88)

r∈[R]

DSFM: min

S

Fr(S) 

(1)

where Fr : 2[N ] → R is a submodular function for all r ∈ [R]. Algorithmic solutions for the DSFM
problem fall into two categories  combinatorial optimization approaches [10  11] and continuous func-
tion optimization methods [12]. In the latter setting  a crucial concept is the Lov´asz extension of the
submodular function which is convex [13] and lends itself to a norm-regularized convex optimization
framework. Prior work in continuous DSFM has focused on devising efﬁcient algorithms for solving
the convex problem and deriving matching convergence results. The best known approaches include
the alternating projection (AP) methods [14  15] and the coordinate descent (CD) methods [16].
Despite some simpliﬁcations offered through decomposibility  DSFM algorithms still suffer from
scalability issues and have convergence guarantees that are suboptimal. To address the ﬁrst issue  one
needs to identify additional problem constraints that allow for parallel implementations. To resolve the
second issue and more precisely characterize and improve the convergence rates  one needs to better
understand how the individual submodular components jointly govern the global optimal solution.
In both cases  it is crucial to utilize incidence relations that describe which subsets of variables

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

directly affect the value of any given component function. Often  incidences involve relatively small
subsets of elements  which leads to desirable sparsity constraints. This is especially the case for
min-cut problems on graphs and hypergraphs (where each submodular component involves two or
several vertices) [17  18] and MAP inference with higher-order potentials (where each submodular
component involves variables corresponding to adjacent pixels) [9]. Although incidence relations
have been used to parametrize the algorithmic complexity of combinatorial optimization methods
for solving DSFM problems [10]  they have been largely overlooked in continuous optimization
methods. Some prior work considered merging decomposable parts with nonoverlapping support
into one submodular function  thereby creating a coarser decomposition that may be processed more
efﬁciently [14  15  16]  but the accompanying algorithms were neither designed in a form that can
optimally use this information nor analyzed precisely with respect to their convergence rates and
merging strategies. In an independent work  Djolonga and Krause found that the variational inference
problem in L-FIELD could be reduced to a DSFM problem with sparse incidence relations [19] 
while their analysis only worked for regular cases.
Here  we revisit two benchmark algorithms for continuous DSFM – AP and CD – and describe how
to modify them to exploit incidence relations that allow for signiﬁcantly improved computational
complexity. Furthermore  we provide a complete theoretical analysis of the algorithms parametrized
by incidence relations with respect to their convergence rates. AP-based methods that leverage
incidence relations achieve better convergence rates than classical AP algorithms both in the sequential
and parallel optimization scenario. The random CD method (RCDM) and accelerated CD method
(ACDM) that incorporate incidence information can be parallelized. The complexity of sequential
CD methods cannot be improved using incidence relations  but the convergence rate of parallel CD
methods strongly depends on how the incidence relations are used for coordinate sampling: while
a new specialized combinatorial sampling based on equitable coloring [20] is optimal  uniformly
at random sampling produces a 2-approximation. It also leads to a greedy method that empirically
outperforms random sampling. A summary of these and other ﬁndings is presented in Table 1.

Prior work

AP

Parallel

Sequential
Sequential
O(N 2R2) O(N 2R2/K) O(N(cid:107)µ(cid:107)1R)
O(N 2R)

-

RCDM O(N 2R)

ACDM O(N R)

-

O(N R)

O

This work

(cid:16)(cid:16) R−K
(cid:18)(cid:16) R−K

(cid:17)
Parallel
O(N(cid:107)µ(cid:107)1R/K)
(cid:17)1/2
R−1 N(cid:107)µ(cid:107)1
R−1 N(cid:107)µ(cid:107)1

R−1 N 2 + K−1
R−1 N 2 + K−1

O

(cid:17)

(cid:19)

R/K

R/K

Here  (cid:107)µ(cid:107)1 = (cid:80)

Table 1: Overview of known and new results: each entry contains the required number of iterations to
achieve an -optimal solution (the dependence on  is the same for all algorithms and hence omitted).
i∈[N ] µi  where for all i ∈ [N ]  µi equals the number of submodular functions
that involve element i; K is a parallelization parameter that equals the number of min-norm points
problems that have to be solved within each iteration.
2 Background  Notation and Problem Formulation
We start our exposition by reviewing several recent lines of work for solving the DSFM problem  and
focus on approaches that transform the DSFM problem into a continuous optimization problem. Such
approaches exploit the fact that the Lov´asz extension of a submodular function is convex. Without
loss of generality  we tacitly assume that all submodular functions Fr are normalized  i.e.  that
i∈S zi.

Fr(∅) = 0 for all r ∈ [R]. Also  we deﬁne given a vector z ∈ RN and S ⊆ [N ]  z(S) =(cid:80)

Then  the base polytope of the r-th submodular function Fr is deﬁned as

Br (cid:44) {yr ∈ RN|yr(S) ≤ Fr(S)  for any S ⊂ [N ]  and yr([N ]) = Fr([N ])}.

solved through continuous optimization  minx∈[0 1]N(cid:80)

The Lov´asz extention [13] fr(·) : RN → R of a submodular function Fr is deﬁned as fr(x) =
maxyr∈Br(cid:104)yr  x(cid:105)  where (cid:104)· ·(cid:105) denotes the inner product of two vectors. The DSFM problem can be
r fr(x). To counter the nonsmoothness of
the objective function  a proximal formulation of a generalization of the above optimization problem
is considered instead [14] 

(cid:88)

r∈[R]

min
x∈RN

fr(x) +

(cid:107)x(cid:107)2
2.

1
2

2

(2)

As the problem (2) is strongly convex  it has a unique optimal solution  denoted by x∗. The exact
discrete solution to the DSFM problem equals S∗ = {i ∈ [N ]| x∗
i > 0}.
For convenience  we denote the product of base polytopes as B = ⊗R
(y1  y2  ...  yR) ∈ B. Also  we let A be a simple linear mapping ⊗R
a point a = (a1  a2  ...  aR) ∈ ⊗R
solving (2) use the dual form of the problem  described in the next lemma.
Lemma 2.1 ([14]). The dual problem of (2) reads as

r=1Br  and write y =
RN → RN   which given
r∈[R] ar. The AP and CD algorithms for

RN outputs Aa =(cid:80)

r=1

r=1

(cid:107)a − y(cid:107)2

2

min
a y

s.t. Aa = 0  y ∈ B.

(3)

Moreover  problem (3) may be written in the more compact form

(4)
For both problems  the primal and dual variables are related according to x = −Ay. In what follows 
for notational simplicity  we write g(y) = 1

min

s.t.

y

2

2(cid:107)Ay(cid:107)2
2.

(cid:107)Ay(cid:107)2

y ∈ B.

r

r = y(k−1)

− Ay(k−1)/R  y(k)

The AP [15] and RCD algorithms [16] described below provide solutions to the problems (3) and (4) 
respectively. They both rely on repeated projections ΠBr (·) onto the base polytopes Br  r ∈ [R].
These projections are typically less computationally intense than projections onto the complete base
polytope of F as they involve fewer data dimensions. The projection operation ΠBr (·) requires one
to solve a min-norm problem by either exploiting the special forms of Fr or by using the general
purpose algorithm of Wolfe [21]. The complexity of the method is typically characterized by the
number of required projections ΠBr (·).
The AP algorithm. Starting with y = y(0)  iteratively compute a sequence (a(k)  y(k))k=1 2 ... such
that for all r ∈ [R]  a(k)
r )  until a stopping criteria is met.
The RCDM algorithm. In each iteration k  chose uniformly at random a subset of elements in y
associated with one atomic function in the decomposition (1)  say the one with index rk. Then 
r = y(k−1)
compute the sequence (y(k))k=1 2 ... according to y(k)
 
for r (cid:54)= rk.
Finding an -optimal solution for both the AP and RCD methods requires O(N 2R log( 1
 )) iterations.
In each iteration  the AP algorithm computes the projections onto all R base polytopes  while the
RCDM only computes one projection. Therefore  as may be seen from Table 1  the sequential
AP solver  which computes one projection in each iteration  requires O(N 2R2 log( 1
 )) iterations.
However  the projections within one iteration of the AP method can be generated in parallel  while
the projections performed in the RCDM have to be generated sequentially.
2.1

Incidence Relations and Related Notations

(cid:16)−(cid:80)

r = ΠBr (a(k)

rk = ΠBrk

y(k−1)

  y(k)

r(cid:54)=rk

(cid:17)

r

r

We next formally introduce one of the key concepts used in this work: incidence relations between
elements of the ground set and the component submodular functions.
We say that an element i ∈ [N ] is incident to a submodular function F iff there exists a S ⊆ [N ]/{i}
such that F (S ∪ {i}) (cid:54)= F (S); similarly  we say that the submodular function F is incident to an
element i iff i is incident to F . To verify whether an element i is incident to a submodular function
F   one needs to verify that F ({i}) = 0 and that F ([N ]) = F ([N ]/{i}) since for any S ⊆ [N ]/{i}

F ({i}) ≥ F (S ∪ {i}) − F (S) ≥ F ([N ]) − F ([N ]/{i}).

Furthermore  note that if i ∈ [N ] is not incident to Fr  then for any yr ∈ Br  one has yr i = 0. Let
Sr be the set of all elements incident to Fr. For each element i  denote the number of submodular
functions that are incident to i by µi = |{r ∈ [R] : i ∈ Sr}|. We also refer to µi as the degree of
element i. We ﬁnd it useful to partition the set of submodular functions into different groups. Given
a group C ⊆ [R] of submodular functions  we deﬁne the degree of the element i within C  µC
i   as
i = |{r ∈ C : i ∈ Sr}|.
µC
We also deﬁne a skewed norm involving two vectors w ∈ RN

i∈[N ] wiz2

i . With a slight abuse of notation  for two vectors θ = (θ1  θ2  ...  θR) ∈ ⊗R

>0 and z ∈ RN according to (cid:107)z(cid:107)2 w (cid:44)
RN
>0

r=1

(cid:113)(cid:80)

3

r=1

r=1

RN   we also deﬁne the norm (cid:107)y(cid:107)2 θ (cid:44)(cid:113)(cid:80)

r=1

2 θr

and y ∈ ⊗R

r∈[R] (cid:107)yr(cid:107)2

RN and a positive vector θ ∈ ⊗R

refer to should be clear from the context. In addition  we let (cid:107)θ(cid:107)1 ∞ =(cid:80)

. Which of the norms we
i∈[N ] maxr∈[R]:i∈Sr θr i.
For a closed set K ⊆ ⊗R
>0  the distance between y and K is
RN
deﬁned as dθ(y K) = min{(cid:107)y − z(cid:107)2 θ|z ∈ K}. Also  given a set Ω ⊆ RN   we let ΠΩ w(·) denote
the projection operation onto Ω with respect to the norm (cid:107) · (cid:107)2 w.
Given a vector w ∈ RN
RN whose r-th entry
satisﬁes (I(w))r = w. It is easy to check that (cid:107)I(w)(cid:107)1 ∞ = (cid:107)w(cid:107)1. Of special interest are induced
vectors based on pairs of N-dimensional vectors  µ = (µ1  µ2  ...  µN )  µC = (µC
N ).
2   ...  µC
Finally  for w  w(cid:48) ∈ RN   we denote the element-wise power of w by wα = (wα
N )  for
1   wα
some α ∈ R  and the element-wise product of w and w(cid:48) by w (cid:12) w(cid:48) = (w1w(cid:48)
1  w2w(cid:48)
N ).
Next  recall that x∗ is the unique optimal solution of the problem (2) and let Z = {ξ ∈
⊗R
RN|Aξ = −x∗  ξr i = 0 ∀i ∈ Sr ∀r ∈ [R]}. Then  due to the duality relationship of
Lemma 2.1  Ξ = Z ∩ B is the set of optimal solutions {y}.
3 Continuous DSFM Algorithms with Incidence Relations

>0  we also make use of an induced vector I(w) ∈ ⊗R

1   µC
2   ...  wα
2  ...  wN w(cid:48)

r=1

r=1

In what follows  we revisit the AP and CD algorithms and describe how to improve their performance
and analytically establish their convergence rates. Our ﬁrst result introduces a modiﬁcation of the
AP algorithm (3) that exploits incidence relations so as to decrease the required number of iterations
from O(N 2R) to O(N(cid:107)µ(cid:107)1). Our second result is an example that shows that the convergence rates
of CD algorithms [11] cannot be directly improved by exploiting the functions’ incidence relations
even when the incidence matrix is extremely sparse. Our third result is a new algorithm that relies of
coordinate descent steps but can be parallelized. In this setting  incidence relations are essential to
the parallelization process.
To analyze solvers for the continuous optimization problem (2) that exploit the incidence structure of
the functions  we make use of the skewed norm (cid:107) · (cid:107)2 w with respect to some positve vector w that
accounts for the fact that incidences are  in general  nonuniformly distributed. In this context  the
projection ΠBr w(·) reduces to solving a classical min-norm problem after a simple transformation
of the underlying space which does not incur signiﬁcant complexity overheads. To see this  note
that in order to solve a generic min-norm point problem  one typically uses either Wolfe’s algorithm
(continuous) or a divide-and-conquer procedure (combinatorial). The complexity of the former is at
most quadratic in Fr max (cid:44) maxv S |Fr(S ∪ {v}) − Fr(S)| [22]  while the complexity of the latter
merely depends on log Fr max [14] (see Section A in the Supplement). It is unclear if including the
weight vector w into the projection procedure increases or decreases Fr max. In either case  given that
in our derivations all elements of w are contained in [1  maxi∈[N ] µi] instead of N or R  we do not
expect to see signiﬁcant changes in the complexity of the projection operation. Hence  throughout
the remainder of our exposition  we regard the projection operation as an oracle and measure the
complexity of all algorithms in terms of the number of projections performed.
Also  observe that one may avoid computing projections in skewed-norm spaces by introducing in (2)
a weighted rather than an unweighted proximal term. This gives another continuous objective that
still provides a solution to the discrete problem (1). Even in this case  we can prove that the numbers
of iterations used in the different methods listed Table 1 remain the same. Furthermore  by combining
projections in skewed-norm spaces and weighted proximal terms  it is possible to actually reduce
the number of iterations given in Table 1. However  for simplicity  we focus on the objective (2)
and projections in skewed-norm spaces. Methods using weighted proximal terms with and without
skewed-norm projections are analyzed in a similar manner in Section L of the Supplement.
We make frequent use of the following result which generalizes Lemma 4.1 of [11].
Lemma 3.1. Let θ ∈ ⊗R
>0 be two positive vectors. Let y ∈ B and let z be in the
base polytope of the submodular function F . Then  there exists a point ξ ∈ B such that Aξ = z and

(cid:107)Ay − z(cid:107)1. Moreover  (cid:107)ξ − y(cid:107)2 θ ≤(cid:113)(cid:107)θ(cid:107)1 ∞(cid:107)w−1(cid:107)1

(cid:107)ξ − y(cid:107)2 θ ≤(cid:113)(cid:107)θ(cid:107)1 ∞

>0  w ∈ RN
RN

(cid:107)Ay − z(cid:107)2 w.

r=1

2

2

3.1 The Incidence Relation AP (IAP)

The following result establishes the basis of our improved AP method leveraging incidence structures.

4

Lemma 3.2. The following problem is equivalent to problem (3):

(cid:107)a − y(cid:107)2

2 I(µ)

min
a y

s.t.

y ∈ B  Aa = 0  and ar i = 0  ∀(r  i) : i /∈ Sr  r ∈ [R].

(5)

r=1

r=1

RN|Aa = 0  ar i = 0  ∀(r  i) : i /∈ Sr} and A(cid:48) = {a ∈ ⊗R

RN|Aa = 0}.
Let A = {a ∈ ⊗R
The AP algorithm for problem (5) consists of alternatively computing projections between A and
B  as opposed to those between A(cid:48) and B used in the problem (3). However  as already pointed out 
unlike for the classical AP problem (3)  the distance in (5) is not Euclidean  and hence the projections
may not be orthogonal.
The IAP method for solving (5) proceeds as follows. We begin with a = a(0) ∈ A  and iteratively
compute a sequence (a(k)  y(k))k=1 2 ... as follows: for all r ∈ [R]  y(k)
r i =
y(k−1)
(Ay(k−1))i  ∀ i ∈ Sr. The key difference between the AP and IAP algorithms is that
r i − µ−1
the latter effectively removes “irrelevant” components of yr by ﬁxing the irrelevant components of a
to 0. In the AP method of Nishihara [15]  these components are never zero as they may be “corrupted”
by other components during AP iterations. Removing irrelevant components results in projecting y
into a subspace of lower dimensions  which signiﬁcantly accelerates the convergence of IAP.

r = ΠBr µ(a(k)

r )  a(k)

i

B

y(0)(y(cid:48)(0))

y(cid:48)(1)

y(cid:48)(2)

y(1)

y∗

A(cid:48)

a(cid:48)(1)
A

a(cid:48)(2)
a(1)

Figure 1: Illustration of the IAP method for solving problem (5): The space A is a subspace of A(cid:48) 
which leads to faster convergence of the IAP method when compared to AP.

The analysis of the convergence rate of the IAP method follows a similar outline as that used to ana-
lyze (3) in [15]. Following Nishihara et al. [15]  we deﬁne the following parameter that plays a key role
in determining the rate of convergence of the AP algorithm  κ∗ (cid:44) sup
max{dI(µ)(y Z) dI(µ)(y B)}.
Lemma 3.3 ([15]). If κ∗ < ∞  the AP algorithm converges linearly with rate 1 − 1
iteration  the algorithm outputs a value y(k) that satisﬁes

κ2∗ . At the k-th

y∈Z∪B/Ξ

dI(µ)(y Ξ)

dI(µ)(y(k)  Ξ) ≤ 2dI(µ)(y(0)  Ξ)

(cid:18)

1 − 1
κ2∗

(cid:19)k

.

To apply the above lemma in the IAP setting  one ﬁrst needs to establish an upper bound on κ∗. This
bound is given in Lemma 3.4 below.

Lemma 3.4. The parameter κ∗ is upper bounded as κ∗ ≤(cid:112)N(cid:107)µ(cid:107)1/2 + 1.

By using the above lemma and the bound on κ∗  one can establish the following convergence rate for
the IAP method.
Theorem 3.5. After O(N(cid:107)µ(cid:107)1 log(1/)) iterations  the IAP algorithm for solving problem (5)
outputs a pair of points (a  y) that satisﬁes dI(µ)(y  Ξ) ≤ .
Note that in practice  one often has (cid:107)µ(cid:107)1 (cid:28) N R  which shows that the convergence rate of the AP
method for solving the DSBM problem may be signiﬁcantly improved.
3.2 Sequential Coordinate Descent Algorithms
Unlike the AP algorithm  the CD algorithms by Ene et al. [16] remain unchanged given (4). Our
ﬁrst goal is to establish whether the convergence rate of the CD algorithms can be improved using a
parameterization that exploits incidence relations.
The convergence rate of CD algorithms is linear if the objective function is component-wise smooth
and (cid:96)-strong convex. In our case  g(y) is component-wise smooth as for any y  z ∈ B that only differ

5

in the r-th block (i.e.  yr (cid:54)= zr  yr(cid:48) = zr(cid:48) for r(cid:48) (cid:54)= r)  one has

(cid:107)∇rg(y) − ∇rg(z)(cid:107)2 ≤ (cid:107)y − z(cid:107)2.
Here  ∇rg denotes the gradient vector associated with the r-th block.
Deﬁnition 3.6. We say that the function g(y) is (cid:96)-strongly convex in (cid:107) · (cid:107)2   if for any y ∈ B
g(y∗) ≥ g(y) + (cid:104)∇g(y)  y∗ − y(cid:105) +
where y∗ = arg min
z∈Ξ

2 ≥ (cid:96)(cid:107)y∗ − y(cid:107)2
2 
2. Moreover  we let (cid:96)∗ = sup{(cid:96) : g(y) is (cid:96)-strongly convex in (cid:107) · (cid:107)2}.

2  or equivalently  (cid:107)Ay − Ay∗(cid:107)2

(cid:107)y∗ − y(cid:107)2

(cid:107)z − y(cid:107)2

(cid:96)
2

(6)

Note that the above deﬁnition essentially establishes a form of weak-strong convexity [23]. Then 
using standard analytical tools for CD algorithms [24]  we can prove the following result [16].
Theorem 3.7. The RCDM for problem (4) outputs a point y that satisﬁes E[g(y)] ≤ g(y∗) +  after
(cid:96)∗ log(1/)) iterations. The ACDM applied to the problem (4) outputs a point y that satisﬁes
O( R
E[g(y)] ≤ g(y∗) +  after O( R√

(cid:96)∗ log(1/)) iterations.

More precisely  let N = 2n + 1  R = 2n  and (cid:107)µ(cid:107)1 =(cid:80)

To precisely characterize the convergence rate  we need to ﬁnd an accurate estimate of (cid:96)∗. Ene et
al. [11] derived (cid:96)∗ ≥ 1
N 2 without taking into account the incidence structure. As sparse incidence
side information improves the performance of the AP method  it is of interest to determine if the
same can be accomplished for the CD algorithms. Example 3.1 establishes that this is not possible in
general if one only relies on (cid:96)∗.
Example 3.1. Consider a DSFM problem with a extremely sparse incidence structure with |Sr| = 2.
r∈[R] |Sr| = 4n (cid:28) N R. Let Fr be incident
to the elements {r  r + 1}  for all r ∈ [R]  and be such that Fr({r}) = Fr({r + 1}) = 1  Fr(∅) =
Fr({r  r + 1}) = 0. Then  (cid:96)∗ < 7
N 2 .
Note that the optimal solution of problem (4) for this particular setting equals y∗ = 0. Let us consider
a point y ∈ B speciﬁed as follows. First  due to the given incidence relations  the block yr has two
components corresponding to the elements indexed by r and r + 1. For any r ∈ [R] 

(cid:26)

yr r = −yr r+1 =

r
n

2n+1−r

n

r ≤ n 
r ≥ n + 1.
2n2 ≤ 7

N 2 for all n ≥ 3.

(7)

n   (cid:107)y(cid:107)2

2 > 4

3 n  which results in (cid:96)∗ < 3

Therefore  g(y) = 1
Example 3.1 only illustrates that an important parameter of CDMs cannot be improved using incidence
information; but this does not necessarily imply that a sequential RCDM that uses incidence structures
cannot offer better convergence rates than O(N 2R). In Section E of the Supplement  we present
additional experimental evidence that supports our observation  using the setting of Example 3.1.
As a ﬁnal remark  note that Nishihara et al. [15] also proposed a lower bound that does not make use
of sparse incidence structures and only works for the AP method.

(cid:88)

3.3 New Parallel CD methods
In what follows  we propose two CDMs which rely on parallel projections and incidence relations.
The following observation is key to understanding the proposed approach. Suppose that we have a
nonempty group of blocks C ⊆ [R]. Let y  h ∈ ⊗R
RN . If hr i is nonzero only for block r ∈ C
and i ∈ Sr  then 

r=1

(cid:107)Ah(cid:107)2

2 ≤ g(y) +

1
2

1
2

(cid:107)hr(cid:107)2

(cid:104)∇rg(y)  hr(cid:105) +

g(y + h) = g(y) + (cid:104)∇g(y)  h(cid:105) +

r∈C

(8)
Hence  for all r ∈ C  if we perform projections onto Br with respect to the norm (cid:107) · (cid:107)2 µC simultane-
ously in each iteration of the CDM  convergence is guaranteed as the value of the objective function
remains bounded. The smaller the components of µC  the faster the convergence. Note that the
components of µC are the numbers of incidence relations of elements restricted to the set C. Hence 
in each iteration  blocks that ought to be updated in parallel are those that correspond to submodular
functions that have supports with smallest possible intersections.
One can select blocks that are to be updated in parallel in a combinatorially speciﬁed fashion or in a
randomized fashion  as dictated by what we call an α-proper distribution. To describe our parallel
RCDM  we ﬁrst introduce the notion of an α-proper distribution.

2 µC .

r∈C

(cid:88)

6

2   ...  θP

R) such that for r ∈ [R]  θP

Deﬁnition 3.8. Let P be a distribution used to sample a group of C blocks. Deﬁne θP =
(θP
1   θP
distribution  if for any r ∈ [R] and a given α ∈ (0  1)  we have P(r ∈ C) = α.
We are now ready to describe the parallel RCDM algorithm – Algorithm 1; the description of the
parallel ACDM is postponed to Section J of the Supplement.

(cid:2)µC|r ∈ C(cid:3). We say that P is an α-proper

(cid:44) EC∼P

r

Algorithm 1: Parallel RCDM for Solving (4)
Input: B  α
0: Initialize y(0) ∈ B  k ← 0
1: Do the following steps iteratively until the dual gap < :
2:
3:
y(k+1)
4:
r
Set y(k+1)
5:
6: Output y(k)

Sample Cik using some α-proper distribution P
For r ∈ Cik:

r − (θP
(y(k)
for r (cid:54)∈ Cik  k ← k + 1

r )−1 (cid:12) ∇rg(y(k)))

← ΠBr θP
← y(k)

r

r

r

Next  we establish strong convexity results for the space (cid:107) · (cid:107)2 θP by invoking Lemma 3.1.
Lemma 3.9. For any y ∈ B  let y∗ = arg minξ∈Ξ (cid:107)ξ − y(cid:107)2

(cid:107)Ay − Ay∗(cid:107)2

2

N(cid:107)θP(cid:107)1 ∞

2 ≥
(cid:20)

1 −

(cid:21)

≤

2 θP . Then 
(cid:107)y − y∗(cid:107)2

2 θP .

(cid:21)k(cid:20)

The convergence rate of Algorithm 1 is established in the next theorem.
Theorem 3.10. At each iteration of Algorithm 1  y(k) satisﬁes

(cid:20)

(cid:21)

.

4α

E

g(y(k)) − g(y∗) +

1
2

g(y(0)) − g(y∗) +

1
2

r∈[R]:i∈Sr

d2
θP (yk  ξ)

(N(cid:107)θP(cid:107)1 ∞ + 2)

θP (y0  ξ)
d2
The parameter N(cid:107)θP(cid:107)1 ∞ is obtained by combining the strong convexity constant and the properties
of the sampling distribution P . Small values of (cid:107)θP(cid:107)1 ∞ ensure better convergence rates  and we
next bound this value.
Lemma 3.11. For any α-proper distribution P and an element i ∈ [N ]  max
r i ≥
θP
max{αµi  1}. Consequently  (cid:107)θP(cid:107)1 ∞ ≥ max{α(cid:107)µ(cid:107)1  N}.
Without considering incidence relations  i.e.  by setting (cid:107)µ(cid:107)1 = N R  one always has (cid:107)θP(cid:107)1 ∞ ≥
αN R  which shows that parallelization cannot improve the convergence rate of the RCDM.
The next lemma characterizes an achievable (cid:107)θP(cid:107)1 ∞ obtained by choosing P to be a uniform
distribution  which  when combined with Theorem 3.10  proves the result of the last column in
Table 1.
Lemma 3.12. If C is a set of size 0 < K ≤ R obtained by sampling the K-subsets of [R] uniformly
at random  then θP
Comparing Lemma 3.11 and Lemma 3.12  we see that the (cid:107)θP(cid:107)1 ∞ achieved by sampling uniformly
at random is at most a factor of two of the lower bound since α = K/R. A natural question is if
it is possible to devise a better sampling strategy. This question is addressed in Section K of the
Supplement  where we related the sampling problem to equitable coloring [20]. By using Hajnal-
Szemerédi’s Theorem [25]  we derived a sufﬁcient condition under which an α-proper distribution P
that achieves the lower bound in Lemma 3.11 can be found in polynomial time. We also described
a greedy algorithm for minimizing (cid:107)θP(cid:107)1 ∞ that empirically convergences faster than sampling
uniformly at random.

R−1 1. Moreover  (cid:107)θP(cid:107)1 ∞ = K−1

R−1 (cid:107)µ(cid:107)1 + R−K

R−1 µ + R−K

r = K−1

R−1 N.

4 Experiments

In what follows  we illustrate the performance of the newly proposed DSFM algorithms on a
benchmark datasets used for MAP inference in image segmentation [9] and used for semi-supervised

7

Figure 2: Image segmentation example. First row: Gap vs the number of iterations ×α. Second row:
The number of iterations ×α vs α. Here  α is the parallelization parameter  while K = αR equals
the number of projections that have to be computed in each iteration.

learning over graphs 1. More experiments on semi-supervised learning over hypergraphs can be
found in Section M of the Supplement.
In all the experiments  we evaluated the convergence rate of the algorithms by using the smooth
duality gap νs and the discrete duality gap νd. The primal problem solution equals x = −Ay so
2(cid:107)Ay(cid:107)2).
Moreover  as the level set Sλ = {v ∈ [N ]|xv > λ} can be easily found based on x  the discrete

that the smooth duality gap can be computed according to νs =(cid:80)
duality gap can be written as νd = minλ F (Sλ) −(cid:80)

v∈[N ] min{−xv  0}.

r fr(x) + 1

2(cid:107)x(cid:107)2 − (− 1

MAP inference. We used two images – oct and smallplant – adopted from [14]2. The images
comprise 640 × 427 pixels so that N = 273  280. The decomposable submodular functions are
constructed following a standard procedure. The ﬁrst class of functions arises from the 4-neighbor
grid graph over the pixels. Each edge corresponds to a pairwise potential between two adjacent
pixels i  j that follows the formula exp(−(cid:107)vi − vj(cid:107)2
2)  where vi is the RGB color vector of pixel i.
We split the vertical and horizontal edges into rows and columns that result in 639 + 426 = 1065
components in the decomposition. Note that within each row or each column  the edges have no
overlapping pixels  so the projections of these submodular functions onto the base polytopes reduce
to projections onto the base polytopes of edge-like submodular functions. The second class of
submodular functions contain clique potentials corresponding to the superpixel regions; speciﬁcally 
for region r  Fr(S) = |S|(|Sr| − |S|) [26]. These functions give another 500 decomposition
components. We apply the divide and conquer method in [14] to compute the projections required for
this type of submodular functions. Note that in each experiment  all components of the submodular
function are of nearly the same size  and thus the projections performed for different components
incur similar computational costs. As the projections represent the primary computational units  for
comparative purposes we use the number of iterations (similarly to [14  16]).
We compared ﬁve algorithms: RCDM with a sampling distribution P found by the greedy algorithm
(RCDM-G)  RCDM with uniform sampling (RCDM-U)  ACDM with uniform sampling (ACDM-U) 
AP based on (5) (IAP) and AP based on (3) (AP). Figure 2 depicts the results. In the ﬁrst row  we
compared the convergence rates of different algorithms for a ﬁxed parallelization parameter α = 0.1.
The values on the horizontal axis correspond to # iterations × α  the total number of projections
performed divided by R. The results are averaged over 10 independent experiments. We observe
that the CD-based methods outperform AP-based methods  and that ACDM-U is the best performing
CD-based method. IAP signiﬁcantly outperforms AP. Similarly  RCDM-G outperforms RCDM-U.
We also investigated the relationship between the number of iterations and the parameter α. We
recorded the number of iterations needed to achieve a smooth and discrete gap below a certain given
threshold. The results are shown in the second row of Figure 2. We did not plot the curves for the
AP-based methods as they are essentially horizontal lines. Among the CD-based methods  ACDM-U
performs best. RCDM-G offers a much better convergence rate than RCDM-U since the sampling
probability P produced by the greedy algorithm leads to a smaller value of (cid:107)θP(cid:107)1 ∞ compared to

1The code for this work can be found in https://github.com/lipan00123/DSFM-with-incidence-relations.
2Downloaded from the website of Professor Stefanie Jegelka: http://people.csail.mit.edu/stefje/code.html

8

0100200300400500600#Iterations× α10-410-2100102104106108smooth gap (α = 0.1  oct)RCDM-GRCDM-UACDM-UIAPAP050100150200250300#Iterations × α10-810-610-410-2100102104106discrete gap (α = 0.1  oct)RCDM-GRCDM-UACDM-UIAPAP0100200300400500600#Iterations× α101102103104105106107108smooth gap (α = 0.1  smallplant)RCDM-GRCDM-UACDM-UIAPAP0100200300400500600#Iterations × α10-810-610-410-2100102104106discrete gap (α = 0.1  smallplant)RCDM-GRCDM-UACDM-UIAPAP00.050.10.150.2α300400500600700800900100011001200#Iterations × α (νs < 10-2  oct)RCDM-GRCDM-UACMD-U00.050.10.150.2α45505560657075#Iterations × α (νd < 10-3  oct)RCDM-GRCDM-UACMD-U00.050.10.150.2α2003004005006007008009001000#Iterations × α (νs < 102  smallplant)RCDM-GRCDM-UACMD-U00.050.10.150.2α100200300400500600700#Iterations × α (νd < 10-3  smallplant)RCDM-GRCDM-UACMD-UFigure 3: Zachary’s Karate Club. Left two: Gap vs the number of iterations ×α. Right two: The
number of iterations ×α vs α. Here  α is the parallelization parameter  while K = αR equals the
number of projections that have to be computed in each iteration.

uniform sampling. The reason behind this ﬁnding is that the supports of the components in the
decomposition are localized  which makes the sampling P obtained from the greedy algorithm highly
effective. For RCDM-U  the total number of iterations increases almost linearly with α (= K/R) 
which conﬁrms the results of Lemma 3.12.
Note that in the above examples of MAP inference  another way to decompose the submodular
functions is available: as there are three natural layers of non-overlapping incidence sets  we can
merge all vertical edges  all horizontal edges  and all superpixel regions into three components
respectively. Then  each of this component is incident to all pixels  and the derived results in this work
will reduce to those of the former works [14  16]. However  such a way to decompose submodular
function strongly depends on the particular structure and thus is not general for DSFM problems.
The following example on semi-supervised learning over graphs does not contain natural layers for
decomposition.
Semi-supervised learning. We tested our algorithms over the dataset of Zachary’s karate club [27].
This dataset is used as a benchmark example for evaluating semisupervised learning algorithms over
graphs [28]. It includes N = 34 vertices and R = 78 submodular functions in the decomposition 
each corresponding to one edge in the network. The objective function of both semi-supervised
learning problems may be written as

(cid:88)

r∈[R]

min

x

τ

fr(x) +

(cid:107)x − x0(cid:107)2

2

1
2

(9)

where τ is a parameter that needs to be tuned  and x0 ∈ {−1  0  1}N   so that the nonzero components
correspond to the labels that are known a priori. In our case  as we are only concerned with the
convergence rate of the algorithm  we ﬁx τ = 0.1. In the experiments for Zachary’s karate club  we
set x0(1) = 1  x0(34) = −1 and let all other components of x0 be equal to zero.
Figure 3 shows the results of the experiments pertaining to Zachary’s karate club. In the left two
subﬁgures  we compared the convergence rates of different algorithms for a ﬁxed parallelization
parameter α = 0.1. The values on the horizontal axis correspond to # iterations × α  the total number
of projections performed divided by R. In the right two subﬁgures  we controlled the numbers of
projections executed within one iteration by tuning the parameter α and recorded the number of
iterations needed to achieve smooth/discrete gaps below 10−3. The values depicted on the vertical
axis correspond to # iterations ×α  describing the total number of projections needed to achieve
the given accuracy. In all cases  we see the similar tendency to that of the MAP inference. As may
be seen  AP-based methods require more projections than CD-based methods  but IAP consistently
outperforms AP  which is consistent with our theoretical results. Among the CD-based methods 
ACDM-U offers the best performance in general  and RCDM-G slightly outperforms RCDM-U  since
the greedy algorithm used for sampling produces a smaller (cid:107)θP(cid:107)1 ∞ than uniform sampling. As the
AP-based methods are completely parallelizable  and increasing the parameter α does not increase the
total number of projections. However  for RCDM-U  the total number of iterations required increases
almost linearly with α  which is supported by the result in Lemma 3.12. The performance curve for
RCDM-G exhibits large oscillations due to the discrete problem component  needed for ﬁnding a
balanced partition.

9

0100200300400500#Iterations× α10-1010-810-610-410-2100102smooth gap (α = 0.1  Karate)RCDM-GRCDM-UACDM-UIAPAP0100200300400500#Iterations × α10-710-610-510-410-310-210-1100discrete gap (α= 0.1  Karate)RCDM-GRCDM-UACDM-UIAPAP00.10.20.30.4α303540455055606570#Iterations × α (smooth)RCDM-GRCDM-UACMD-U00.10.20.30.4α1015202530354045505560#Iterations × α (discrete)RCDM-GRCDM-UACMD-U5 Acknowledgement

The authors gratefully acknowledge many useful suggestions by the reviewers. This work was
supported in part by the NSF grant CCF 15-27636  the NSF Purdue 4101-38050 and the NFT STC
center Science of Information.

References

[1] S. Fujishige  Submodular functions and optimization. Elsevier  2005  vol. 58.
[2] K. Wei  R. Iyer  and J. Bilmes  “Submodularity in data subset selection and active learning ” in

Proceedings of the International Conference on Machine Learning  2015  pp. 1954–1963.

[3] P. Li and O. Milenkovic  “Inhomogeneous hypergraph clustering with applications ” in Advances

in Neural Information Processing Systems  2017  pp. 2305–2315.

[4] ——  “Submodular hypergraphs: p-laplacians  cheeger inequalities and spectral clustering ” in

Proceedings of the International Conference on Machine Learning  2018  pp. 3014–3023.

[5] P. Kohli  P. H. Torr et al.  “Robust higher order potentials for enforcing label consistency ”

International Journal of Computer Vision  vol. 82  no. 3  pp. 302–324  2009.

[6] H. Lin and J. Bilmes  “A class of submodular functions for document summarization ” in
Proceedings of the Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1. Association for Computational Linguistics  2011  pp. 510–520.

[7] A. Krause and C. Guestrin  “Near-optimal observation selection using submodular functions ”
in Proceedings of the AAAI Conference on Artiﬁcial Intelligence  vol. 7  2007  pp. 1650–1654.
[8] Y. T. Lee  A. Sidford  and S. C.-w. Wong  “A faster cutting plane method and its implications
for combinatorial and convex optimization ” in Foundations of Computer Science (FOCS)  2015
IEEE 56th Annual Symposium on.

IEEE  2015  pp. 1049–1065.

[9] P. Stobbe and A. Krause  “Efﬁcient minimization of decomposable submodular functions ” in

Advances in Neural Information Processing Systems  2010  pp. 2208–2216.

[10] V. Kolmogorov  “Minimizing a sum of submodular functions ” Discrete Applied Mathematics 

vol. 160  no. 15  pp. 2246–2258  2012.

[11] A. Ene  H. Nguyen  and L. A. Végh  “Decomposable submodular function minimization:
discrete and continuous ” in Advances in Neural Information Processing Systems  2017  pp.
2874–2884.

[12] F. Bach et al.  “Learning with submodular functions: A convex optimization perspective ”

Foundations and Trends R(cid:13) in Machine Learning  vol. 6  no. 2-3  pp. 145–373  2013.

[13] L. Lovász  “Submodular functions and convexity ” in Mathematical Programming The State of

the Art. Springer  1983  pp. 235–257.

[14] S. Jegelka  F. Bach  and S. Sra  “Reﬂection methods for user-friendly submodular optimization ”

in Advances in Neural Information Processing Systems  2013  pp. 1313–1321.

[15] R. Nishihara  S. Jegelka  and M. I. Jordan  “On the convergence rate of decomposable submod-
ular function minimization ” in Advances in Neural Information Processing Systems  2014  pp.
640–648.

[16] A. Ene and H. Nguyen  “Random coordinate descent methods for minimizing decomposable
submodular functions ” in Proceedings of the International Conference on Machine Learning 
2015  pp. 787–795.

[17] D. R. Karger  “Global min-cuts in RNC  and other ramiﬁcations of a simple min-cut algorithm.”
in Proceedings of the ACM-SIAM Symposium on Discrete Algorithms  vol. 93  1993  pp. 21–30.
[18] C. Chekuri and C. Xu  “Computing minimum cuts in hypergraphs ” in Proceedings of the ACM-
SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics 
2017  pp. 1085–1100.

[19] J. Djolonga and A. Krause  “Scalable variational inference in log-supermodular models.” in

Proceedings of the International Conference on Machine Learning  2015  pp. 1804–1813.

10

[20] W. Meyer  “Equitable coloring ” The American Mathematical Monthly  vol. 80  no. 8  pp.

920–922  1973.

[21] P. Wolfe  “Finding the nearest point in a polytope ” Mathematical Programming  vol. 11  no. 1 

pp. 128–149  1976.

[22] D. Chakrabarty  P. Jain  and P. Kothari  “Provable submodular minimization using Wolfe’s

algorithm ” in Advances in Neural Information Processing Systems  2014  pp. 802–809.

[23] H. Karimi  J. Nutini  and M. Schmidt  “Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition ” in Joint European Conference on Machine
Learning and Knowledge Discovery in Databases. Springer  2016  pp. 795–811.

[24] Y. Nesterov  “Efﬁciency of coordinate descent methods on huge-scale optimization problems ”

SIAM Journal on Optimization  vol. 22  no. 2  pp. 341–362  2012.

[25] A. Hajnal and E. Szemerédi  “Proof of a conjecture of Erdös ” Combinatorial Theory and Its

Applications  vol. 2  pp. 601–623  1970.

[26] A. Levinshtein  A. Stere  K. N. Kutulakos  D. J. Fleet  S. J. Dickinson  and K. Siddiqi  “Tur-
bopixels: fast superpixels using geometric ﬂows ” IEEE Transactions on Pattern Analysis and
Machine Intelligence  vol. 31  no. 12  pp. 2290–2297  2009.

[27] W. W. Zachary  “An information ﬂow model for conﬂict and ﬁssion in small groups ” Journal of

Anthropological Research  vol. 33  no. 4  pp. 452–473  1977.

[28] T. N. Kipf and M. Welling  “Semi-supervised classiﬁcation with graph convolutional networks ”

arXiv preprint arXiv:1609.02907  2016.

[29] S. Fujishige and X. Zhang  “New algorithms for the intersection problem of submodular systems ”

Japan Journal of Industrial and Applied Mathematics  vol. 9  no. 3  p. 369  1992.

[30] O. Fercoq and P. Richtárik  “Accelerated  parallel  and proximal coordinate descent ” SIAM

Journal on Optimization  vol. 25  no. 4  pp. 1997–2023  2015.

[31] H. A. Kierstead  A. V. Kostochka  M. Mydlarz  and E. Szemerédi  “A fast algorithm for equitable

coloring ” Combinatorica  vol. 30  no. 2  pp. 217–224  2010.

[32] A. Chambolle and J. Darbon  “On total variation minimization and surface evolution using
parametric maximum ﬂows ” International journal of computer vision  vol. 84  no. 3  p. 288 
2009.

[33] R. Albert and A.-L. Barabási  “Statistical mechanics of complex networks ” Reviews of modern

physics  vol. 74  no. 1  p. 47  2002.

[34] M. Hein  S. Setzer  L. Jost  and S. S. Rangapuram  “The total variation on hypergraphs-learning
on hypergraphs revisited ” in Advances in Neural Information Processing Systems  2013  pp.
2427–2435.

[35] N. Yadati  M. Nimishakavi  P. Yadav  A. Louis  and P. Talukdar  “Hypergcn: Hypergraph
convolutional networks for semi-supervised classiﬁcation ” arXiv preprint arXiv:1809.02589 
2018.

11

,Aviv Tamar
YI WU
Garrett Thomas
Sergey Levine
Pieter Abbeel
Pan Li
Olgica Milenkovic