2017,Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference,Semi-supervised learning methods using Generative adversarial networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well  we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process  we propose enhancements over existing methods for learning the inverse mapping (i.e.  the encoder)  which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines  particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.,Semi-supervised Learning with GANs: Manifold

Invariance with Improved Inference

Abhishek Kumar∗
IBM Research AI

Yorktown Heights  NY
abhishk@us.ibm.com

Prasanna Sattigeri∗
IBM Research AI

Yorktown Heights  NY
psattig@us.ibm.com

P. Thomas Fletcher
University of Utah
Salt Lake City  UT

fletcher@sci.utah.edu

Abstract

Semi-supervised learning methods using Generative adversarial networks (GANs)
have shown promising empirical success recently. Most of these methods use a
shared discriminator/classiﬁer which discriminates real examples from fake while
also predicting the class label. Motivated by the ability of the GANs generator to
capture the data manifold well  we propose to estimate the tangent space to the data
manifold using GANs and employ it to inject invariances into the classiﬁer. In the
process  we propose enhancements over existing methods for learning the inverse
mapping (i.e.  the encoder) which greatly improves in terms of semantic similarity
of the reconstructed sample with the input sample. We observe considerable
empirical gains in semi-supervised learning over baselines  particularly in the cases
when the number of labeled examples is low. We also provide insights into how
fake examples inﬂuence the semi-supervised learning procedure.

1

Introduction

Deep generative models (both implicit [11  23] as well as prescribed [16]) have become widely
popular for generative modeling of data. Generative adversarial networks (GANs) [11] in particular
have shown remarkable success in generating very realistic images in several cases [30  4]. The
generator in a GAN can be seen as learning a nonlinear parametric mapping g : Z → X to the data
manifold. In most applications of interest (e.g.  modeling images)  we have dim(Z) (cid:28) dim(X). A
distribution pz over the space Z (e.g.  uniform)  combined with this mapping  induces a distribution
pg over the space X and a sample from this distribution can be obtained by ancestral sampling  i.e. 
z ∼ pz  x = g(z). GANs use adversarial training where the discriminator approximates (lower
bounds) a divergence measure (e.g.  an f-divergence) between pg and the real data distribution px by
solving an optimization problem  and the generator tries to minimize this [28  11]. It can also be seen
from another perspective where the discriminator tries to tell apart real examples x ∼ px from fake
examples xg ∼ pg by minimizing an appropriate loss function[10  Ch. 14.2.4] [21]  and the generator
tries to generate samples that maximize that loss [39  11].
One of the primary motivations for studying deep generative models is for semi-supervised learning.
Indeed  several recent works have shown promising empirical results on semi-supervised learning
with both implicit as well as prescribed generative models [17  32  34  9  20  29  35]. Most state-of-
the-art semi-supervised learning methods using GANs [34  9  29] use the discriminator of the GAN
as the classiﬁer which now outputs k + 1 probabilities (k probabilities for the k real classes and one
probability for the fake class).
When the generator of a trained GAN produces very realistic images  it can be argued to capture
the data manifold well whose properties can be used for semi-supervised learning. In particular  the

∗Contributed equally.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

tangent spaces of the manifold can inform us about the desirable invariances one may wish to inject
in a classiﬁer [36  33]. In this work we make following contributions:
• We propose to use the tangents from the generator’s mapping to automatically infer the desired
invariances and further improve on semi-supervised learning. This can be contrasted with methods
that assume the knowledge of these invariances (e.g.  rotation  translation  horizontal ﬂipping  etc.)
[36  18  25  31].
• Estimating tangents for a real sample x requires us to learn an encoder h that maps from data to
latent space (inference)  i.e.  h : X → Z. We propose enhancements over existing methods for
learning the encoder [8  9] which improve the semantic match between x and g(h(x)) and counter
the problem of class-switching.
• Further  we provide insights into the workings of GAN based semi-supervised learning methods

[34] on how fake examples affect the learning.

2 Semi-supervised learning using GANs

Most of the existing methods for semi-supervised learning using GANs modify the regular GAN
discriminator to have k outputs corresponding to k real classes [38]  and in some cases a (k + 1)’th
output that corresponds to fake samples from the generator [34  29  9]. The generator is mainly
used as a source of additional data (fake samples) which the discriminator tries to classify under the
(k + 1)th label. We propose to use the generator to obtain the tangents to the image manifold and use
these to inject invariances into the classiﬁer [36].

2.1 Estimating the tangent space of data manifold

Earlier work has used contractive autoencoders (CAE) to estimate the local tangent space at each
point [33]. CAEs optimize the regular autoencoder loss (reconstruction error) augmented with an
additional (cid:96)2-norm penalty on the Jacobian of the encoder mapping. Rifai et al. [33] intuitively reason
that the encoder of the CAE trained in this fashion is sensitive only to the tangent directions and use
the dominant singular vectors of the Jacobian of the encoder as the tangents. This  however  involves
extra computational overhead of doing an SVD for every training sample which we will avoid in
our GAN based approach. GANs have also been established to generate better quality samples than
prescribed models (e.g.  reconstruction loss based approaches) like VAEs [16] and hence can be
argued to learn a more accurate parameterization of the image manifold.
The trained generator of the GAN serves as a parametric mapping from a low dimensional space Z to
a manifold M embedded in the higher dimensional space X  g : Z → X  where Z is an open subset
in Rd and X is an open subset in RD under the standard topologies on Rd and RD  respectively
(d (cid:28) D). This map is not surjective and the range of g is restricted to M.2 We assume g is a smooth 
injective mapping  so that M is an embedded manifold. The Jacobian of a function f : Rd → RD
at z ∈ Rd  Jzf  is the matrix of partial derivatives (of shape D × d). The Jacobian of g at z ∈ Z 
Jzg  provides a mapping from the tangent space at z ∈ Z into the tangent space at x = g(z) ∈ X 
i.e.  Jzg : TzZ → TxX. It should be noted that TzZ is isomorphic to Rd and TxX is isomorphic to
RD. However  this mapping is not surjective and the range of Jzg is restricted to the tangent space of
the manifold M at x = g(z)  denoted as TxM (for all z ∈ Z). As GANs are capable of generating
realistic samples (particularly for natural images)  one can argue that M approximates the true data
manifold well and hence the tangents to M obtained using Jzg are close to the tangents to the true
data manifold. The problem of learning a smooth manifold from ﬁnite samples has been studied in
the literature[5  2  27  6  40  19  14  3]  and it is an interesting problem in its own right to study the
manifold approximation error of GANs  which minimize a chosen divergence measure between the
data distribution and the fake distribution [28  23] using ﬁnite samples  however this is outside the
scope of the current work.
For a given data sample x ∈ X  we need to ﬁnd its corresponding latent representation z before we
can use Jzg to get the tangents to the manifold M at x. For our current discussion we assume the
availability of a so-called encoder h : X → Z  such that h(g(z)) = z ∀ z ∈ Z. By deﬁnition  the

2We write g as a map from Z to X to avoid the unnecessary (in our context) burden of manifold terminologies
and still being technically correct. This also enables us to get the Jacobian of g as a regular matrix in RD×d 
instead of working with the differential if g was taken as a map from Z to M.

2

Jacobian of the generator at z  Jzg  can be used to get the tangent directions to the manifold at a point
x = g(z) ∈ M. The following lemma speciﬁes the conditions for existence of the encoder h and
shows that such an encoder can also be used to get tangent directions. Later we will come back to the
issues involved in training such an encoder.
Lemma 2.1. If the Jacobian of g at z ∈ Z  Jzg  is full rank then g is locally invertible in the open
neighborhood g(S) (S being an open neighborhood of z)  and there exists a smooth h : g(S) → S
such that h(g(y)) = y ∀ y ∈ S. In this case  the Jacobian of h at x = g(z)  Jxh  spans the tangent
space of M at x.

Proof. We refer the reader to standard textbooks on multivariate calculus and differentiable manifolds
for the ﬁrst statement of the lemma (e.g.  [37]).
The second statement can be easily deduced by looking at the Jacobian of the composition of functions
h ◦ g. We have Jz(h ◦ g) = Jg(z)h Jzg = Jxh Jzg = Id×d  since h(g(z)) = z. This implies that the
row span of Jxh coincides with the column span of Jzg. As the columns of Jzg span the tangent
space Tg(z)M  so do the the rows of Jxh.
2.1.1 Training the inverse mapping (the encoder)
To estimate the tangents for a given real data point x ∈ X  we need its corresponding latent
representation z = h(x) ∈ Z  such that g(h(x)) = x in an ideal scenario. However  in practice
g will only learn an approximation to the true data manifold  and the mapping g ◦ h will act like
a projection of x (which will almost always be off the manifold M) to the manifold M  yielding
some approximation error. This projection may not be orthogonal  i.e.  to the nearest point on M.
Nevertheless  it is desirable that x and g(h(x)) are semantically close  and at the very least  the class
label is preserved by the mapping g ◦ h. We studied the following three approaches for training the
inverse map h  with regard to this desideratum :
• Decoupled training. This is similar to an approach outlined by Donahue et al. [8] where the
generator is trained ﬁrst and ﬁxed thereafter  and the encoder is trained by optimizing a suitable
reconstruction loss in the Z space  L(z  h(g(z))) (e.g.  cross entropy  (cid:96)2). This approach does not
yield good results and we observe that most of the time g(h(x)) is not semantically similar to
the given real sample x with change in the class label. One of the reasons as noted by Donahue
et al. [8] is that the encoder never sees real samples during training. To address this  we also
experimented with the combined objective minh Lz(z  h(g(z))) + Lh(x  g(h(x)))  however this
too did not yield any signiﬁcant improvements in our early explorations.
• BiGAN. Donahue et al. [8] propose to jointly train the encoder and generator using adversarial
training  where the pair (z  g(z)) is considered a fake example (z ∼ pz) and the pair (h(x)  x) is
considered a real example by the discriminator. A similar approach is proposed by Dumoulin
et al. [9]  where h(x) gives the parameters of the posterior p(z|x) and a stochastic sample from
the posterior paired with x is taken as a real example. We use BiGAN [8] in this work  with one
modiﬁcation: we use feature matching loss [34] (computed using features from an intermediate
layer (cid:96) of the discriminator f)  i.e.  (cid:107)Exf(cid:96)(h(x)  x) − Ezf(cid:96)(z  g(z))(cid:107)2
2  to optimize the generator
and encoder  which we found to greatly help with the convergence 3. We observe better results in
terms of semantic match between x and g(h(x)) than in the decoupled training approach  however 
we still observe a considerable fraction of instances where the class of g(h(x)) is changed (let us
refer to this as class-switching).
• Augmented-BiGAN. To address the still-persistent problem of class-switching of the recon-
structed samples g(h(x))  we propose to construct a third pair (h(x)  g(h(x)) which is also con-
sidered by the discriminator as a fake example in addition to (z  g(z)). Our Augmented-BiGAN
objective is given as
Ex∼px log f (h(x)  x) +
(1)
where f (· ·) is the probability of the pair being a real example  as assigned by the discriminator
f. We optimize the discriminator using the above objective (1). The generator and encoder are
again optimized using feature matching [34] loss on an intermediate layer (cid:96) of the discriminator 
i.e.  Lgh = (cid:107)Exf(cid:96)(h(x)  x) − Ezf(cid:96)(z  g(z))(cid:107)2
2  to help with the convergence. Minimizing Lgh
3Note that other recently proposed methods for training GANs based on Integral Probability Metrics

Ex∼px log(1 − f (h(x)  g(h(x))) 

1
2

Ez∼pz log(1 − f (z  g(z))) +

1
2

[1  13  26  24] could also improve the convergence and stability during training.

3

will make x and g(h(x)) similar (through the lens of f(cid:96)) as in the case of BiGAN  however
the discriminator tries to make the features at layer f(cid:96) more difﬁcult to achieve this by directly
optimizing the third term in the objective (1). This results in improved semantic similarity between
x and g(h(x)).

We empirically evaluate these approaches with regard to similarity between x and g(h(x)) both
quantitatively and qualitatively  observing that Augmented-BiGAN works signiﬁcantly better than
BiGAN. We note that ALI [9] also has the problems of semantic mismatch and class switching for
reconstructed samples as reported by the authors  and a stochastic version of the proposed third term
in the objective (1) can potentially help there as well  investigation of which is left for future work.

2.1.2 Estimating the dominant tangent space

Once we have a trained encoder h such that g(h(x)) is a good approximation to x and h(g(z)) is a
good approximation to z  we can use either Jh(x)g or Jxh to get an estimate of the tangent space.
Speciﬁcally  the columns of Jh(x)g and the rows of Jxh are the directions that approximately span
the tangent space to the data manifold at x. Almost all deep learning packages implement reverse
mode differentiation (to do backpropagation) which is computationally cheaper than forward mode
differentiation for computing the Jacobian when the output dimension of the function is low (and
vice versa when the output dimension is high). Hence we use Jxh in all our experiments to get the
tangents.
As there are approximation errors at several places (M ∼ data-manifold  g(h(x)) ∼ x  h(g(z)) ∼ z) 
it is preferable to only consider dominant tangent directions in the row span of Jxh. These can be
obtained using the SVD on the matrix Jxh and taking the right singular vectors corresponding to
top singular values  as done in [33] where h is trained using a contractive auto-encoder. However 
this process is expensive as the SVD needs to be done independently for every data sample. We
adopt an alternative approach to get dominant tangent direction: we take the pre-trained model with
encoder-generator-discriminator (h-g-f) triple and insert two extra functions p : Rd → Rdp and ¯p :
Rdp → Rd (with dp < d) which are learned by optimizing minp  ¯p Ex[(cid:107)g(h(x))− g(¯p(p(h(x))))(cid:107)1 +
(cid:107)f X−1(g(h(x))) − f X−1(g(¯p(p(h(x)))))(cid:107)] while g  h and f are kept ﬁxed from the pre-trained model.
Note that our discriminator f has two pipelines f Z and f X for the latent z ∈ Z and the data x ∈ X 
respectively  which share parameters in the last few layers (following [8])  and we use the last layer of
f X in this loss. This enables us to learn a nonlinear (low-dimensional) approximation in the Z space
such that g(¯p(p(h(x)))) is close to g(h(x)). We use the Jacobian of p ◦ h  Jx p ◦ h  as an estimate of
the dp dominant tangent directions (dp = 10 in all our experiments)4.

2.2

Injecting invariances into the classiﬁer using tangents

(cid:80)

labeled examples  it uses a regularizer of the form(cid:80)n

We use the tangent propagation approach (TangentProp) [36] to make the classiﬁer invariant to the
estimated tangent directions from the previous section. Apart form the regular classiﬁcation loss on
2  where Jxic ∈ Rk×D
is the Jacobian of the classiﬁer function c at x = xi (with the number of classes k). and Tx is the
set of tangent directions we want the classiﬁer to be invariant to. This term penalizes the linearized
variations of the classiﬁer output along the tangent directions. Simard et al. [36] get the tangent
directions using slight rotations and translations of the images  whereas we use the GAN to estimate
the tangents to the data manifold.
We can go one step further and make the classiﬁer invariant to small perturbations in all directions
emanating from a point x. This leads to the regularizer

(cid:107)(Jxic) v(cid:107)2

v∈Txi

i=1

j ≤ k(cid:88)

i=1

k(cid:88)

i=1

(cid:107)(Jxc) v(cid:107)j

sup

v:(cid:107)v(cid:107)p≤

|(Jxc)i: v|j = j

sup

v:(cid:107)v(cid:107)p≤

(cid:107)(Jxc)i:(cid:107)j
q 

(2)

where (cid:107)·(cid:107)q is the dual norm of (cid:107)·(cid:107)p (i.e.  1
j denotes jth power of (cid:96)j-norm. This
reduces to squared Frobenius norm of the Jacobian matrix Jxc for p = j = 2. The penalty in
4Training the GAN with z ∈ Z ⊂ Rdp results in a bad approximation of the data manifold. Hence we ﬁrst
learn the GAN with Z ⊂ Rd and then approximate the smooth manifold M parameterized by the generator
using p and ¯p to get the dominant dp tangent directions to M.

q = 1)  and (cid:107)·(cid:107)j

p + 1

4

Eq. (2) is closely related to the recent work on virtual adversarial training (VAT) [22] which uses a
regularizer (ref. Eq (1)  (2) in [22])

KL[c(x)||c(x + v)] 

sup

v:(cid:107)v(cid:107)2≤

(3)

where c(x) are the classiﬁer outputs (class probabilities). VAT[22] approximately estimates v∗ that
yields the sup using the gradient of KL[c(x)||c(x + v)]  calling (x + v∗) as virtual adversarial
example (due to its resemblance to adversarial training [12])  and uses KL[c(x)||c(x + v∗)] as the
regularizer in the classiﬁer objective. If we replace KL-divergence in Eq. 3 with total-variation
distance and optimize its ﬁrst-order approximation  it becomes equivalent to the regularizer in Eq. (2)
for j = 1 and p = 2.
In practice  it is computationally expensive to optimize these Jacobian based regularizers. Hence in all
our experiments we use stochastic ﬁnite difference approximation for all Jacobian based regularizers.
For TangentProp  we use (cid:107)c(xi + v) − c(xi)(cid:107)2
2 with v randomly sampled (i.i.d.) from the set of
tangents Txi every time example xi is visited by the SGD. For Jacobian-norm regularizer of Eq. (2) 
we use (cid:107)c(x + δ)− c(x)(cid:107)2
2 with δ ∼ N (0  σ2I) (i.i.d) every time an example x is visited by the SGD 
which approximates an upper bound on Eq. (2) in expectation (up to scaling) for j = 2 and p = 2.

2.3 GAN discriminator as the classiﬁer for semi-supervised learning: effect of fake examples

Recent works have used GANs for semi-supervised learning where the discriminator also serves as a
classiﬁer [34  9  29]. For a semi-supervised learning problem with k classes  the discriminator has
k + 1 outputs with the (k + 1)’th output corresponding to the fake examples originating from the
generator of the GAN. The loss for the discriminator f is given as [34]

Lf = Lf
and Lf

sup + Lf
unsup = −Ex∼pg(x) log(pf (y = k + 1|x)) − Ex∼pd(x) log(1 − pf (y = k + 1|x))).

sup = −E(x y)∼pd(x y) log pf (y|x  y ≤ k)

unsup  where Lf

(4)

The term pf (y = k + 1|x) is the probability of x being a fake example and (1 − pf (y = k + 1|x))
is the probability of x being a real example (as assigned by the model). The loss component
Lf
unsup is same as the regular GAN discriminator loss with the only modiﬁcation that probabilities
for real vs. fake are compiled from (k + 1) outputs. Salimans et al. [34] proposed training the
generator using feature matching where the generator minimizes the mean discrepancy between the
features for real and fake examples obtained from an intermediate layer (cid:96) of the discriminator f 
i.e.  Lg = (cid:107)Exf(cid:96)(x) − Ezf(cid:96)(g(z))(cid:107)2
2. Using feature matching loss for the generator was empirically
shown to result in much better accuracy for semi-supervised learning compared to other training
methods including minibatch discrimination and regular GAN generator loss [34].
Here we attempt to develop an intuitive understanding of how fake examples inﬂuence the learning
of the classiﬁer and why feature matching loss may work much better for semi-supervised learning
compared to regular GAN. We will use the term classiﬁer and discriminator interchangeably based
on the context however they are really the same network as mentioned earlier. Following [34] we
assume the (k + 1)’th logit is ﬁxed to 0 as subtracting a term v(x) from all logits does not change the
softmax probabilities. Rewriting the unlabeled loss of Eq. (4) in terms of logits li(x)  i = 1  2  . . .   k 
we have

(cid:32)

k(cid:88)

(cid:33)

(cid:34)

k(cid:88)

unsup = Exg∼pg log
Lf

1 +

eli(xg)

− Ex∼pd

log

eli(x) − log

1 +

eli(x)

(5)

(cid:32)

k(cid:88)

(cid:33)(cid:35)

i=1

i=1

i=1

Taking the derivative w.r.t. discriminator’s parameters θ followed by some basic algebra  we get
∇θLf

unsup =

k(cid:88)
k(cid:88)

i=1

i=1

(cid:124)

E

xg∼pg

pf (y = i|xg)∇li(xg) − E
x∼pd

= E
xg∼pg

pf (y = i|xg)

∇li(xg) − E
x∼pd

(cid:123)(cid:122)

ai(xg)

(cid:125)

(cid:34) k(cid:88)
pf (y = i|x  y ≤ k)∇li(x) − k(cid:88)
k(cid:88)
(cid:124)

pf (y = i|x  y ≤ k)pf (y = k + 1|x)

(cid:123)(cid:122)

(cid:125)

i=1

i=1

i=1

bi(x)

(cid:35)

pf (y = i|x)∇li(x)

∇li(x)

(6)

5

Minimizing Lf
unsup will move the parameters θ so as to decrease li(xg) and increase li(x) (i =
1  . . .   k). The rate of increase in li(x) is also modulated by pf (y = k + 1|x). This results in warping
of the functions li(x) around each real example x with more warping around examples about which
the current model f is more conﬁdent that they belong to class i: li(·) becomes locally concave
around those real examples x if xg are loosely scattered around x. Let us consider the following three
cases:
Weak fake examples. When the fake examples coming from the generator are very weak (i.e. 
very easy for the current discriminator to distinguish from real examples)  we will have pf (y =
k + 1|xg) ≈ 1  pf (y = i|xg) ≈ 0 for 1 ≤ i ≤ k and pf (y = k + 1|x) ≈ 0. Hence there is no
gradient ﬂow from Eq. (6)  rendering unlabeled data almost useless for semi-supervised learning.
Strong fake examples. When the fake examples are very strong (i.e.  difﬁcult for the current
discriminator to distinguish from real ones)  we have pf (k + 1|xg) ≈ 0.5 + 1  pf (y = imax|xg) ≈
0.5− 2 for some imax ∈ {1  . . .   k} and pf (y = k + 1|x) ≈ 0.5− 3 (with 2 > 1 ≥ 0 and 3 ≥ 0).
Note that bi(x) in this case would be smaller than ai(x) since it is a product of two probabilities. If
two examples x and xg are close to each other with imax = arg maxi li(x) = arg maxi li(xg) (e.g. 
x is a cat image and xg is a highly realistic generated image of a cat)  the optimization will push
limax(x) up by some amount and will pull limax(xg) down by a larger amount. We further want to
consider two cases here: (i) Classiﬁer with enough capacity: If the classiﬁer has enough capacity 
this will make the curvature of limax(·) around x really high (with limax(·) locally concave around x)
since x and xg are very close. This results in over-ﬁtting around the unlabeled examples and for a test
example xt closer to xg (which is quite likely to happen since xg itself was very realistic sample)  the
model will more likely misclassify xt. (ii) Controlled-capacity classiﬁer: Suppose the capacity of
the classiﬁer is controlled with adequate regularization. In that case the curvature of the function
limax(·) around x cannot increase beyond a point. However  this results in limax(x) being pulled down
by the optimization process since ai(xg) > bi(x). This is more pronounced for examples x on which
the classiﬁer is not so conﬁdent (i.e.  pf (y = imax|x  y ≤ k) is low  although still assigning highest
probability to class imax) since the gap between ai(xg) and bi(x) becomes higher. For these examples 
the entropy of the distribution {p(y = i|x  y ≤ k)}k
i=1 may actually increase as the training proceeds
which can hurt the test performance.
Moderate fake examples. When the fake examples from the generator are neither too weak nor too
strong for the current discriminator (i.e.  xg is a somewhat distorted version of x)  the unsupervised
gradient will push limax(x) up while pulling limax(xg) down  giving rise to a moderate curvature of
li(·) around real examples x since xg and x are sufﬁciently far apart (consider multiple distorted cat
images scattered around a real cat image at moderate distances). This results in a smooth decision
function around real unlabeled examples. Again  the curvatures of li(·) around x for classes i which
the current classiﬁer does not trust for the example x are not affected much. Further  pf (y = k + 1|x)
will be less than the case when fake examples are very strong. Similarly pf (y = imax|xg) (where
imax = arg max1≤i≤k li(xg)) will be less than the case of strong fake examples. Hence the norm
of the gradient in Eq. (6) is lower and the contribution of unlabeled data in the overall gradient of
Lf (Eq. (4) is lower than the case of strong fake examples. This intuitively seems beneﬁcial as the
classiﬁer gets ample opportunity to learn on supervised loss and get conﬁdent on the right class for
unlabeled examples  and then boost this conﬁdence slowly using the gradient of Eq. (6) as the training
proceeds.
We experimented with regular GAN loss (i.e.  Lg = Ex∼pg log(pf (y = k + 1|x)))  and feature
matching loss for the generator [34]  plotting several of the quantities of interest discussed above
for MNIST (with 100 labeled examples) and SVHN (with 1000 labeled examples) datasets in Fig.1.
Generator trained with feature matching loss corresponds to the case of moderate fake examples
discussed above (as it generates blurry and distorted samples as mentioned in [34]). Generator
trained with regular GAN loss corresponds to the case of strong fake examples discussed above.
We plot Exg aimax(xg) for imax = arg max1≤i≤k li(xg) and Exg [ 1
1≤i(cid:54)=imax≤k ai(xg)] separately
k−1
to look into the behavior of imax logit. Similarly we plot Exbt(x) separately where t is the true
label for unlabeled example x (we assume knowledge of the true label only for plotting these
quantities and not while training the semi-supervised GAN). Other quantities in the plots are self-
explanatory. As expected  the unlabeled loss Lf
unsup for regular GAN becomes quite high early on
implying that fake examples are strong. The gap between aimax(xg) and bt(x) is also higher for
regular GAN pointing towards the case of strong fake examples with controlled-capacity classiﬁer as
discussed above. Indeed  we see that the average of the entropies for the distributions pf (y|x) (i.e. 

(cid:80)

6

Figure 1: Plots of Entropy  Lf
GAN generator loss and feature-matching GAN generator loss.

unsup (Eq. (4))  ai(xg)  bi(x) and other probabilities (Eq. (6)) for regular

ExH(pf (y|x  y ≤ k))) is much lower for feature-matching GAN compared to regular GAN (seven
times lower for SVHN  ten times lower for MNIST). Test errors for MNIST for regular GAN and
FM-GAN were 2.49% (500 epochs) and 0.86% (300 epochs)  respectively. Test errors for SVHN
were 13.36% (regular-GAN at 738 epochs) and 5.89% (FM-GAN at 883 epochs)  respectively5.
It should also be emphasized that the semi-supervised learning heavily depends on the generator
dynamically adapting fake examples to the current discriminator – we observed that freezing the
training of the generator at any point results in the discriminator being able to classify them easily
(i.e.  pf (y = k + 1|xg) ≈ 1) thus stopping the contribution of unlabeled examples in the learning.
Our ﬁnal loss for semi-supervised learning. We use feature matching GAN with semi-supervised
loss of Eq. (4) as our classiﬁer objective and incorporate invariances from Sec. 2.2 in it. Our ﬁnal
objective for the GAN discriminator is

(cid:88)

v∈Tx

Lf = Lf

sup + Lf

unsup + λ1Ex∼pd(x)

(cid:107)(Jxf ) v(cid:107)2

2 + λ2Ex∼pd(x)(cid:107)Jxf(cid:107)2
F .

(7)

The third term in the objective makes the classiﬁer decision function change slowly along tangent
directions around a real example x. As mentioned in Sec. 2.2 we use stochastic ﬁnite difference
approximation for both Jacobian terms due to computational reasons.

3 Experiments
Implementation Details. The architecture of the endoder  generator and discriminator closely
follow the network structures in ALI [9]. We remove the stochastic layer from the ALI encoder (i.e. 
h(x) is deterministic). For estimating the dominant tangents  we employ fully connected two-layer
network with tanh non-linearly in the hidden layer to represent p ◦ ¯p. The output of p is taken from
the hidden layer. Batch normalization was replaced by weight normalization in all the modules to
make the output h(x) (similarly g(z)) dependent only on the given input x (similarly z) and not on
the whole minibatch. This is necessary to make the Jacobians Jxh and Jzg independent of other
examples in the minibatch. We replaced all ReLU nonlinearities in the encoder and the generator
with the Exponential Linear Units (ELU) [7] to ensure smoothness of the functions g and h. We
follow [34] completely for optimization (using ADAM optimizer [15] with the same learning rates as
in [34]). Generators (and encoders  if applicable) in all the models are trained using feature matching
loss.

5We also experimented with minibatch-discrimination (MD) GAN[34] but the minibatch features are not
suited for classiﬁcation as the prediction for an example x is adversely affected by features of all other examples
(note that this is different from batch-normalization). Indeed we notice that the training error for MD-GAN is
10x that of regular GAN and FM-GAN. MD-GAN gave similar test error as regular-GAN.

7

Figure 2: Comparing BiGAN with Augmented BiGAN based on the classiﬁcation error on the
reconstructed test images. Left column: CIFAR10  Right column: SVHN. In the images  the top row
corresponds to the original images followed by BiGAN reconstructions in the middle row and the
Augmented BiGAN reconstructions in the bottom row. More images can be found in the appendix.

Figure 3: Visualizing tangents. Top: CIFAR10  Bottom: SVHN. Odd rows: Tangents using our
method for estimating the dominant tangent space. Even rows: Tangents using SVD on Jh(x)g and
Jxh. First column: Original image. Second column: Reconstructed image using g ◦ h. Third column:
Reconstructed image using g ◦ ¯p ◦ p ◦ h. Columns 4-13: Tangents using encoder. Columns 14-23:
Tangents using generator.

Semantic Similarity. The image samples x and their reconstructions g(h(x)) for BiGAN and
Augemented-BiGAN can be seen in Fig. 2. To quantitatively measure the semantic similarity of the
reconstructions to the original images  we learn a supervised classiﬁer using the full training set and
obtain the classiﬁcation accuracy on the reconstructions of the test images. The architectures of the
classiﬁer for CIFAR10 and SVHN are similar to their corresponding GAN discriminator architectures
we have. The lower error rates with our Augmented-BiGAN suggest that it leads to reconstructions
with reduced class-switching.
Tangent approximations. Tangents for CIFAR10 and SVHN are shown in Fig. 3. We show visual
comparison of tangents from Jx(p ◦ h)  from Jp(h(x))g ◦ ¯p  and from Jxh and Jh(x)g followed
by the SVD to get the dominant tangents. It can be seen that the proposed method for getting
dominant tangent directions gives similar tangents as SVD. The tangents from the generator (columns
14-23) look different (more colorful) from the tangents from the encoder (columns 4-13) though they
do trace the boundaries of the objects in the image (just like the tangents from the encoder). We
also empirically quantify our method for dominant tangent subspace estimation against the SVD
estimation by computing the geodesic distances and principal angles between these two estimations.
These results are shown in Table 2.
Semi-supervised learning results. Table 1 shows the results for SVHN and CIFAR10 with various
number of labeled examples. For all experiments with the tangent regularizer for both CIFAR10
and SVHN  we use 10 tangents. The hyperparameters λ1 and λ2 in Eq. (7) are set to 1. We obtain
signiﬁcant improvements over baselines  particularly for SVHN and more so for the case of 500

8

–
–
–
–
–
–

SVHN

Nl = 1000
36.02 ± 0.10

23.56
24.63

–

16.61 ± 0.24
7.41 ± 0.65
8.11 ± 1.3
4.42 ± 0.16
4.74 ± 1.2
5.26 ± 1.1
4.39 ± 1.2

–
–
–
–

20.40

–
–
–
–
–

–

Model

Nl = 500

CIFAR-10

Nl = 1000

Nl = 4000

VAE (M1+M2) [17]

SWWAE with dropout [41]

VAT [22]

Skip DGM [20]

Ladder network [32]

ALI [9]

FM-GAN [34]

Temporal ensembling [18]

FM-GAN + Jacob.-reg (Eq. (2))

17.99 ± 1.62
18.63 ± 2.32
12.16 ± 0.24
16.84 ± 1.5
16.96 ± 1.4
16.20 ± 1.6
Table 1: Test error with semi-supervised learning on SVHN and CIFAR-10 (Nl is the number of
labeled examples). All results for the proposed methods (last 3 rows) are obtained with training the
model for 600 epochs for SVHN and 900 epochs for CIFAR10  and are averaged over 5 runs.

19.98 ± 0.89
21.83 ± 2.01
20.87 ± 1.7
20.23 ± 1.3
19.52 ± 1.5

18.44 ± 4.8
5.12 ± 0.13
10.28 ± 1.8
5.88 ± 1.5
4.87 ± 1.6

FM-GAN + Jacob.-reg + Tangents

FM-GAN + Tangents

d(S1 S2)

Rand-Rand

SVD-Approx. (CIFAR)
SVD-Approx. (SVHN)

4.5
2.6
2.3

θ1
14
2
1

θ2
83
15
7

θ3
85
21
12

θ4
86
26
16

θ5
87
34
22

θ6
87
40
30

θ7
88
50
41

θ8
88
61
51

θ9
88
73
67

θ10
89
85
82

Table 2: Dominant tangent subspace approximation quality: Columns show the geodesic distance
and 10 principal angles between the two subspaces. Top row shows results for two randomly sampled
10-dimensional subspaces in 3072-dimensional space  middle and bottom rows show results for
dominant subspace obtained using SVD of Jxh and dominant subspace obtained using our method 
for CIFAR-10 and SVHN  respectively. All numbers are averages 10 randomly sampled test examples.

labeled examples. We do not get as good results on CIFAR10 which may be due to the fact that
our encoder for CIFAR10 is still not able to approximate the inverse of the generator well (which
is evident from the sub-optimal reconstructions we get for CIFAR10) and hence the tangents we
get are not good enough. We think that obtaining better estimates of tangents for CIFAR10 has the
potential for further improving the results. ALI [9] accuracy for CIFAR (Nl = 1000) is also close to
our results however ALI results were obtained by running the optimization for 6475 epochs with a
slower learning rate as mentioned in [9]. Temporal ensembling [18] using explicit data augmentation
assuming knowledge of the class-preserving transformations on the input  while our method estimates
these transformations from the data manifold in the form of tangent vectors. It outperforms our
method by a signiﬁcant margin on CIFAR-10 which could be due the fact that it uses horizontal
ﬂipping based augmentation for CIFAR-10 which cannot be learned through the tangents as it is a
non-smooth transformation. The use of temporal ensembling in conjunction with our method has the
potential of further improving the semi-supervised learning results.
4 Discussion
Our empirical results show that using the tangents of the data manifold (as estimated by the generator
of the GAN) to inject invariances in the classiﬁer improves the performance on semi-supevised
learning tasks. In particular we observe impressive accuracy gains on SVHN (more so for the
case of 500 labeled examples) for which the tangents obtained are good quality. We also observe
improvements on CIFAR10 though not as impressive as SVHN. We think that improving on the
quality of tangents for CIFAR10 has the potential for further improving the results there  which is a
direction for future explorations. We also shed light on the effect of fake examples in the common
framework used for semi-supervised learning with GANs where the discriminator predicts real
class labels along with the fake label. Explicitly controlling the difﬁculty level of fake examples
(i.e.  pf (y = k + 1|xg) and hence indirectly pf (y = k + 1|x) in Eq. (6)) to do more effective
semi-supervised learning is another direction for future work. One possible way to do this is to have a
distortion model for the real examples (i.e.  replace the generator with a distorter that takes as input
the real examples) whose strength is controlled for more effective semi-supervised learning.

9

References
[1] Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein gan. arXiv preprint

arXiv:1701.07875  2017.

[2] Alexander V Bernstein and Alexander P Kuleshov. Tangent bundle manifold learning via

grassmann&stiefel eigenmaps. arXiv preprint arXiv:1212.6031  2012.

[3] AV Bernstein and AP Kuleshov. Data-based manifold reconstruction via tangent bundle
manifold learning. In ICML-2014  Topological Methods for Machine Learning Workshop 
Beijing  volume 25  pages 1–6  2014.

[4] David Berthelot  Tom Schumm  and Luke Metz. Began: Boundary equilibrium generative

adversarial networks. arXiv preprint arXiv:1703.10717  2017.

[5] Guillermo Canas  Tomaso Poggio  and Lorenzo Rosasco. Learning manifolds with k-means
and k-ﬂats. In Advances in Neural Information Processing Systems  pages 2465–2473  2012.

[6] Guangliang Chen  Anna V Little  Mauro Maggioni  and Lorenzo Rosasco. Some recent advances
in multiscale geometric analysis of point clouds. In Wavelets and Multiscale Analysis  pages
199–225. Springer  2011.

[7] Djork-Arné Clevert  Thomas Unterthiner  and Sepp Hochreiter. Fast and accurate deep network

learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289  2015.

[8] Jeff Donahue  Philipp Krähenbühl  and Trevor Darrell. Adversarial feature learning. arXiv

preprint arXiv:1605.09782  2016.

[9] Vincent Dumoulin  Ishmael Belghazi  Ben Poole  Alex Lamb  Martin Arjovsky  Olivier
arXiv preprint

Mastropietro  and Aaron Courville. Adversarially learned inference.
arXiv:1606.00704  2016.

[10] Jerome Friedman  Trevor Hastie  and Robert Tibshirani. The elements of statistical learning 

volume 1. Springer series in statistics Springer  Berlin  2001.

[11] Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil
Ozair  Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural
information processing systems  pages 2672–2680  2014.

[12] Ian J Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and harnessing adversar-

ial examples. arXiv preprint arXiv:1412.6572  2014.

[13] Ishaan Gulrajani  Faruk Ahmed  Martin Arjovsky  Vincent Dumoulin  and Aaron Courville.

Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028  2017.

[14] Kui Jia  Lin Sun  Shenghua Gao  Zhan Song  and Bertram E Shi. Laplacian auto-encoders: An

explicit learning of nonlinear data manifold. Neurocomputing  160:250–260  2015.

[15] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[16] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114  2013.

[17] Diederik P Kingma  Shakir Mohamed  Danilo Jimenez Rezende  and Max Welling. Semi-
supervised learning with deep generative models. In Advances in Neural Information Processing
Systems  pages 3581–3589  2014.

[18] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint

arXiv:1610.02242  2016.

[19] G Lerman and T Zhang. Probabilistic recovery of multiple subspaces in point clouds by

geometric (cid:96)p minimization. Preprint  2010.

[20] Lars Maaløe  Casper Kaae Sønderby  Søren Kaae Sønderby  and Ole Winther. Auxiliary deep

generative models. arXiv preprint arXiv:1602.05473  2016.

10

[21] Aditya Menon and Cheng Soon Ong. Linking losses for density ratio and class-probability

estimation. In International Conference on Machine Learning  pages 304–313  2016.

[22] Takeru Miyato  Shin-ichi Maeda  Masanori Koyama  Ken Nakae  and Shin Ishii. Distributional

smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677  2015.

[23] Shakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv

preprint arXiv:1610.03483  2016.

[24] Youssef Mroueh and Tom Sercu. Fisher gan. In NIPS  2017.
[25] Youssef Mroueh  Stephen Voinea  and Tomaso A Poggio. Learning with group invariant features:
A kernel perspective. In Advances in Neural Information Processing Systems  pages 1558–1566 
2015.

[26] Youssef Mroueh  Tom Sercu  and Vaibhava Goel. Mcgan: Mean and covariance feature

matching gan. In ICML  2017.

[27] Partha Niyogi  Stephen Smale  and Shmuel Weinberger. A topological view of unsupervised

learning from noisy data. SIAM Journal on Computing  40(3):646–663  2011.

[28] Sebastian Nowozin  Botond Cseke  and Ryota Tomioka. f-gan: Training generative neural sam-
plers using variational divergence minimization. In Advances in Neural Information Processing
Systems  pages 271–279  2016.

[29] Augustus Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint

arXiv:1606.01583  2016.

[30] Alec Radford  Luke Metz  and Soumith Chintala. Unsupervised representation learning with
deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434  2015.

[31] Anant Raj  Abhishek Kumar  Youssef Mroueh  P Thomas Fletcher  and Bernhard Schölkopf.

Local group invariant representations via orbit embeddings. In AISTATS  2017.

[32] Antti Rasmus  Mathias Berglund  Mikko Honkala  Harri Valpola  and Tapani Raiko. Semi-
supervised learning with ladder networks. In Advances in Neural Information Processing
Systems  pages 3546–3554  2015.

[33] Salah Rifai  Yann Dauphin  Pascal Vincent  Yoshua Bengio  and Xavier Muller. The manifold

tangent classiﬁer. In NIPS  2011.

[34] Tim Salimans  Ian Goodfellow  Wojciech Zaremba  Vicki Cheung  Alec Radford  and Xi Chen.
Improved techniques for training gans. In Advances in Neural Information Processing Systems 
2016.

[35] Cicero Nogueira dos Santos  Kahini Wadhawan  and Bowen Zhou. Learning loss func-
tions for semi-supervised learning via discriminative adversarial networks. arXiv preprint
arXiv:1707.02198  2017.

[36] Patrice Y Simard  Yann A LeCun  John S Denker  and Bernard Victorri. Transformation
invariance in pattern recognition—tangent distance and tangent propagation. In Neural networks:
tricks of the trade  pages 239–274. Springer  1998.

[37] M. Spivak. A Comprehensive Introduction to Differential Geometry  volume 1. Publish or

Perish  3rd edition  1999.

[38] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical genera-

tive adversarial networks. arXiv preprint arXiv:1511.06390  2015.

[39] Zhuowen Tu. Learning generative models via discriminative approaches. In Computer Vision

and Pattern Recognition  2007. CVPR’07. IEEE Conference on  pages 1–8. IEEE  2007.

[40] Rene Vidal  Yi Ma  and Shankar Sastry. Generalized principal component analysis (gpca). IEEE

Transactions on Pattern Analysis and Machine Intelligence  27(12):1945–1959  2005.

[41] Junbo Zhao  Michael Mathieu  Ross Goroshin  and Yann Lecun. Stacked what-where auto-

encoders. arXiv preprint arXiv:1506.02351  2015.

11

,Abhishek Kumar
Prasanna Sattigeri
Tom Fletcher
Ivan Stelmakh
Nihar Shah
Aarti Singh