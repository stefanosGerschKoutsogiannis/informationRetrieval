2012,Bayesian Probabilistic Co-Subspace Addition,For modeling data matrices  this paper introduces Probabilistic Co-Subspace Addition (PCSA) model by simultaneously capturing the dependent structures among both rows and columns. Briefly  PCSA assumes that each entry of a matrix is generated by the additive combination of the linear mappings of two features  which distribute in the row-wise and column-wise latent subspaces. Consequently  it captures the dependencies among entries intricately  and is able to model the non-Gaussian and heteroscedastic density. Variational inference is proposed on PCSA for  approximate Bayesian learning  where the updating for posteriors is formulated into the problem of solving Sylvester equations. Furthermore  PCSA is extended to tackling and filling missing values  to adapting its sparseness  and to modelling tensor data. In comparison with several state-of-art approaches  experiments demonstrate the effectiveness and efficiency of Bayesian (sparse) PCSA on modeling matrix (tensor) data and filling missing values.,Bayesian Probabilistic Co-Subspace Addition

Lei Shi

Baidu.com  Inc

shilei06@baidu.com

Abstract

For modeling data matrices  this paper introduces Probabilistic Co-Subspace Ad-
dition (PCSA) model by simultaneously capturing the dependent structures among
both rows and columns. Brieﬂy  PCSA assumes that each entry of a matrix is gen-
erated by the additive combination of the linear mappings of two low-dimensional
features  which distribute in the row-wise and column-wise latent subspaces re-
spectively. In consequence  PCSA captures the dependencies among entries in-
tricately  and is able to handle non-Gaussian and heteroscedastic densities. By
formulating the posterior updating into the task of solving Sylvester equations 
we propose an efﬁcient variational inference algorithm. Furthermore  PCSA is
extended to tackling and ﬁlling missing values  to adapting model sparseness  and
to modelling tensor data. In comparison with several state-of-art methods  experi-
ments demonstrate the effectiveness and efﬁciency of Bayesian (sparse) PCSA on
modeling matrix (tensor) data and ﬁlling missing values.

1 Introduction

This paper focuses on modeling data matrices by simultaneously capturing the dependent structures
among both rows and columns  which is especially useful for ﬁlling missing values. Using Gaussian
Process (GP)  Xu et al [25] modiﬁed the kernel to incorporate relational information and drew
outputs from GPs. Widely used in geostatistics  Linear Models of Corregionalization (LMC) [5]
learns the covariance structures over the vectorized data matrix. In [12  16]  Bayesian probabilistic
matrix factorization (PMF) is investigated via modeling the row-wise and column-wise speciﬁc
variances and inferred based on suitable priors. Probabilistic Matrix Addition (PMA) [1] describes
the covariance structures among rows and columns  showing promising results compared with GP
regression  PMF and LMC. However  both LMC and PMA are inefﬁcient on large scale matrices.
On high dimensional data  subspace structures are usually designed in statistical models with re-
duced numbers of free parameters  leading to improvement on both learning efﬁciency and accuracy
[3  11  24]. Equipping PMA with the subspace structures  this paper proposes a simple yet novel
generative Probabilistic Co-Subspace Addition (PCSA) model  which  as its name  assumes that all
entries in a matrix come from the sums of linear mappings of latent features in row-wise and column-
wise hidden subspaces. Including many existing models as its special cases (see Section 2.1)  PCSA
is able to capture the dependencies among entries intricately  ﬁt the non-Gaussian and heteroscedas-
tic density  and extract the hidden features in the co-subspaces.
We propose a variational Bayesian algorithm for inferring both the parameters and the latent di-
mensionalities of PCSA. For quick and stable convergence  we formulate the posterior updating
procedure into solving Sylvester equations [10]. Furthermore  Bayesian PCSA is implemented in
three extensions. First  missing values in data matrices are easily tackled and ﬁlled by iterating
with the variational inference. Second  with a Jeffreys prior  Bayesian sparse PCSA is implemented
with an adaptive model sparseness [4]. Finally  we extend the PCSA on matrix data (i.e.  2nd-order
tensor) to PCSA-k for modelling tensor data with an arbitrary order k.

1

On the task of ﬁlling missing values in matrix data  we compare (sparse) PCSA with several state-
of-art models/approaches  including PMA  Robust Bayesian PMF and Bayesian GPLVM [21]. The
datasets under consideration range from multi-label classiﬁcation data  user-item rating data for
collaborative ﬁltering  and face images. Further on tensor structured face image data  PCSA is com-
pared with the M2SA method [6] that uses consecutive SVDs on all modes of the tensor. Although
simple and not designed for any particular application  through experiments PCSA shows results
promisingly comparable to or better than the competing approaches.

2 PCSA Model and Variational Bayesian Inference

2.1 Probabilistic Co-Subspace Addition (PCSA)
The PCSA model deﬁnes distributions over real valued matrices. Letting X ∈ RD1×D2 be an
observed matrix with D1 ≤ D2 without loss of generality1  we start by outlining a generative model
for X. Consider two hidden variables y ∼ N (y|0d1  Id1 ) and z ∼ N (z|0d2   Id2) with d1 < D1
and d2 < D2  where 0d denotes a d-dim vector with all entries being zeros and Id denotes a
d × d identity matrix. Using the concatenation nomenclature of Matlab  two matrices of hidden
factors Y = [y∗1  . . .   y∗D2] ∈ Rd1×D2 and Z = [z∗1  . . .   z∗D1] ∈ Rd2×D1 are column-wise
independently generated  respectively. Through two linear mapping matrices A ∈ RD1×d1 and
B ∈ RD2×d2  each entry xij ∈ X is independent given Y and Z by xij = ai∗y∗j + bj∗z∗i + eij 
where ai∗ is the i-th row of A. Each eij ∼ N (eij|0  1/τ ) is independently Gaussian distributed and
independent from Y  Z. The generative process of X thus is:

• Get Y by independently drawing each vector y∗j ∼ N (y∗j|0d1  Id1 ) for j = 1  . . .   D2;
• Get Z by independently drawing each vector z∗i ∼ N (z∗i|0d2   Id2) for i = 1  . . .   D1;
• Get E ∈ RD1×D2 by independently drawing each element eij ∼ N (eij|0  1/τ ) for ∀i  j;
• Get X = AY + (BZ)
⊤
+ E given Y and Z  i.e.  additively combines the co-subspaces.
D2∏
D1∏

Given parameters θ = {A  B  τ}  the joint distribution of X  Y and Z is p(X  Y  Z|θ) =
N (xij|ai∗y∗j + bj∗z∗i  1/τ )].

N (y∗j|0d1   Id1 )] · [

D1∏

D2∏

[

N (z∗i|0d2  Id2 )] · [

(1)

j=1

i=1

i=1

j=1

Properties and relations to existing work. Albeit its simple generative process  PCSA owns
meaningful properties and can be viewed as an extension of several existing models.

• Intricate dependencies between entries in X. Although each entry xij ∈ X is indepen-
dent given Y and Z  the PCSA model captures the dependencies along rows as well as
columns in the joint X. Particularly  assuming D1 is the data dimensionality while D2 is
the sample size  the samples (column vectors) in X is dependent from each other by PC-
SA. When B is constrained as 0  PCSA will degenerate to Probabilistic PCA (PPCA) [3] 
which insists the sample i.i.d. assumption.
• Non-Gaussianity and heteroscedasticity. If we still consider D1 as the data dimensional-
ity and D2 as the sample size  the PCSA model handles the non-Gaussianity in samples of
⊤ are discretized to take values from a
X. As an extreme example  if all columns of Z
B
set of n vectors  PCSA degenerates to Mixture of PPCA [7  20] with n components  whose
subspace loadings are the same. That is  learning such a PCSA model actually implements
the group PPCA [24] throughout different components. Also  if marginalizing Z  we are
describing the column samples of X with a dependent heteroscedasticity.
• Co-subspace feature extraction. Although able to describe the row-wise and column-
wise covariances  PMA [1] requires estimating and inverting two (large) kernel matrices
with sizes D1 × D1 and D2 × D2 respectively  which is intractable for many real world
applications. In contrast  PCSA has (D1d1 + D2d2 + 1) free parameters and inverts smaller
matrices  and recovers PMA when d1 = D1 and d2 = D2. Moreover  PCSA is able to
extract the hidden features Y and Z simultaneously.

⊤

1Otherwise  we can transpose X. This assumption is for efﬁcient Sylvester equation solving in the sequel.

2

2.2 Variational Bayesian Inference

Given X and the hidden dimensionalities (d1  d2)  we can estimate PCSA’s parameters θ =
{A  B  τ} by maximizing the likelihood p(X|θ). However  the capacity control is essential to gen-
eralization ability  for which we proceed to deliver a variational Bayesian inference on PCSA.
By introducing hyper-parameters ς = [ς1  . . .   ςd1 ]
Normal-Gamma prior on (A  ς) and (B  φ) respectively [3  7  20]  we have the prior p(θ) as

⊤ for a hierarchical

⊤ and φ = [φ1  . . .   φd2]
d1∏
(cid:0)(ςi|uς
d2∏

d1∏
p(τ ) = (cid:0)(τ|uτ   vτ ) 
N (a∗i|0D1   ID1/ςi)  p(ς) =
d2∏

N (b∗i|0D2   ID2 /φi)  p(φ) =

i=1

i=1

i   vς

i ) 

p(θ  ς  φ) = p(τ )p(A  ς)p(B  φ) 
p(A|ς) =
p(A  ς) = p(A|ς)p(ς) 

D2∏

i=1

D1∏

i=1

(cid:0)(φi|uφ

p(B  φ) = p(B|φ)p(φ)  p(B|φ) =

i   vφ

i=1

i=1

i )  (2)
where (cid:0)(·|u  v) denotes a Gamma distribution with a shape parameter u and an inverse scale param-
eter v. Each column a∗i of the mapping matrix A priori independently follows a spherical Gaussian
with a precision scalar ςi  i.e.  an automatic relevance determination (ARD) type prior [14]. Each
precision ςi further follows a Gamma prior for completing the speciﬁcation of the Bayesian model.
p(X|(cid:2))p((cid:2))d(cid:2) 
It is computationally intractable to evaluate the marginal likelihood p(X) =
where (cid:2) = {Z  Y  θ  ς  φ} represents the set of all parameters and latent variables. Since MCMC
samplers are inefﬁcient for high dimensional data  this paper chooses variational inference instead
[11]  which introduces a distribution Q((cid:2)) and approximates maximizing the log marginal like-
lihood log p(X) by maximizing a lower bound L(Q) =
Q((cid:2)) d(cid:2). For tractability 
Q((cid:2)) is factorized into the following mean-ﬁeld form:

Q((cid:2)) log p(X (cid:2))

∫

∫

Q((cid:2)) = Q(Y)Q(Z)Q(A)Q(B)Q(τ )Q(ς)Q(φ)  Q(Y) =

d2∏

d1∏

Q(y∗i)  Q(Z) =

d2∏

Q(z∗i) 

d1∏

Q(A) =

Q(a∗i)  Q(B) =

Q(b∗i)  Q(ς) =

Q(ςi)  Q(φ) =

Q(φi).

(3)

i=1

i=1

i=1

i=1

i   (cid:22)vς

i   (cid:22)vφ
i ) 

Maximizing L(Q) w.r.t. the above Q(ϑ) for ∀ϑ ∈ (cid:2) leads to the following explicit conjugate forms

i )  Q(φi) = (cid:0)(φi|(cid:22)uφ

Q(y∗t) = N (y∗t|(cid:22)y∗t  (cid:22)(cid:6)Y )  Q(z∗i) = N (z∗i|(cid:22)z∗i  (cid:22)(cid:6)Z) 
Q(ςi) = (cid:0)(ςi|(cid:22)uς
Q(a∗i) = N (a∗i|(cid:22)a∗i  ψAID1)  Q(b∗i) = N (b∗i|(cid:22)b∗i  ψBID2 )  Q(τ ) = (cid:0)(τ|(cid:22)uτ   (cid:22)vτ ). (4)
]−1
[
For expression simplicity  we denote (cid:22)A = [(cid:22)a∗1  . . .   (cid:22)a∗d1 ] and similarly for (cid:22)B  (cid:22)Y and (cid:22)Z. During
maximizing L(Q)  the solutions of (cid:22)Y and (cid:22)Z are bundled and conditional on each other:
]−1
[
(1 + ⟨τ⟩D1ψA)Id1 + ⟨τ⟩ (cid:22)A
(1 + ⟨τ⟩D2ψB)Id2 + ⟨τ⟩ (cid:22)B
⊤)

 
  (5)
where ⟨·⟩ is expectation and ⟨τ⟩ = (cid:22)uτ /(cid:22)vτ . Directly updating by the above converges neither quickly
nor stably. Instead after putting one equation into the other  we attain a Sylvester equation [10] and
can efﬁciently solve it by many tools. Then (cid:22)Z is obtained by solving LZ
1

(cid:22)Y = ⟨τ⟩SA (cid:22)A
(X − (cid:22)Z
⊤ (cid:22)B
⊤
⊤
(cid:22)Z = ⟨τ⟩SB (cid:22)B
(X − (cid:22)A (cid:22)Y)
⊤
⊤

(6)
whose solution is further put into Eq.(5) to update (cid:22)Y2. Given (cid:22)Y and (cid:22)Z  updating ( (cid:22)A  (cid:22)B) is similar.
The remainders of Q((cid:2)) in Eq.(4) are updated as

(cid:22)(cid:6)Y = SA  SA =
(cid:22)(cid:6)Z = SB  SB =

− (cid:22)Z + LZ
− ⟨τ⟩ (cid:22)ASA (cid:22)A

3 = 0 
 

⊤ (cid:22)B  LZ

⊤ (cid:22)A
⊤ (cid:22)B

⊤(

with LZ

(cid:22)ZLZ
2

ID1

X

) 

 

ψA = d1/tr

ψB = d2/tr

SY =

+ D2 (cid:22)(cid:6)Y  
+ D1 (cid:22)(cid:6)Z 
2The choice of computing (cid:22)Z ﬁrst is based on the assumption D1 (cid:20) D2 for learning efﬁciency.

KY = (cid:22)Y (cid:22)Y
⊤
KZ = (cid:22)Z (cid:22)Z

SZ =

SZ

 

 

⊤

  LZ

⊤
2 = (cid:22)ASA (cid:22)A

3 = ⟨τ⟩SB (cid:22)B
⊤
[⟨τ⟩KY + diag(⟨ς⟩)
]−1
]−1
[⟨τ⟩KZ + diag(⟨φ⟩)

 

1 = ⟨τ⟩2SB (cid:22)B
)
(
)
(

SY

 

−1
−1

3

(cid:22)uτ = uτ +

(cid:22)vτ = vτ +

D1D2

 

2
D1
tr( (cid:22)(cid:6)Y (cid:22)A
2
1
diag( (cid:22)A
2

D1
2

(cid:22)uς = uς +
⊤ (cid:22)A + ψAKY ) +

1d1  
D2
2

(cid:22)uφ = uφ +

D2
2

1d2  
⊤ (cid:22)B + ψBKZ) +

tr( (cid:22)(cid:6)Z (cid:22)B

D1ψA

⊤ (cid:22)A) +

1/(cid:22)vς

2
/(cid:22)vς
d1

(cid:22)vφ = vφ +

1  . . .   (cid:22)uς
d1

1d1  
⊤  ⟨φ⟩ = [(cid:22)uφ
]

(cid:22)vς = vς +
where ⟨ς⟩ = [(cid:22)uς
inter-converts between a vector and a diagonal matrix  and || · ||F is the Frobenius norm.
In implementation  all Gamma priors in Eq.(2) are set to be vague as (cid:0)(·|10
−3). During
−3  10
learning  redundant columns of (cid:22)A and (cid:22)B will be pushed to approach zeros  which actually makes
Bayesian model selection on hidden dimensionalities d1 and d2.

/(cid:22)vφ
d2

1 /(cid:22)vς

diag( (cid:22)B

]

1
2
1  . . .   (cid:22)uφ
d2

)||2
F  

⊤
⊤ (cid:22)B

||(X − (cid:22)A (cid:22)Y − (cid:22)Z
D2ψB

1
2
⊤ (cid:22)B) +
⊤  tr(·) stands for trace  diag(·)

1d2  

2

(7)

3 Extensions

3.1 Filling Missing Values

In many real applications  X is usually partially observed with some missing entries. The goal here
is to infer not only the PCSA model but also the missing values in X based on the model structure.

Similar to the settings of PMA in [1]  let us begin with a full matrix eX  where the missing values
values therein. In each iteration  we “pretend” that eX is the observed matrix and update Q((cid:2)) by

are randomly ﬁlled. We denote M = {(i  j) : ~xij is missing} as the index set of the missing
Eqs.(6∼7). Then given Q((cid:2))  the missing entries {~xij : (i  j) ∈ M} are updated by maximizing
L(Q)  i.e.  ~xij = (cid:22)xij with (cid:22)X = arg maxX L(Q) = (cid:22)A (cid:22)Y + (cid:22)Z
⊤. This updating manner plays a
role of adaptive regularization [2]  and performs well in experiments as to be shown in Section 4.
Moreover  ﬁlling missing values in PMA [1] needs to infer the column and row factors by either
Gibbs sampling or MAP. In contrast  PCSA directly employs (cid:22)Y ∪ (cid:22)Z that were estimated already in
the variational inference  and thus saves the computing cost.

⊤ (cid:22)B

3.2 Bayesian Sparse PCSA

As discussed above  PCSA describes observations by mapping hidden features (Y and Z) in the
co-subspaces through A and B respectively  i.e.  A and B serve similarly to the transformation
matrix in Factor Analysis and PPCA. For high dimensional data  the parameters A and B probably
suffer from inaccurate estimations and are difﬁcult to interpret. Sparsiﬁcation is one popularly-
used method to improve model interpretability in the literature. In this part  we extend to provide a
Bayesian treatment on the sparse PCSA model.
LASSO [19] encourages model sparseness by adding an ℓ1 regularizer  which is equivalent to a
Laplacian prior. In [4]  the sparseness is adaptively controlled by assigning a hierarchical Normal-
Jeffreys (NJ) prior. Paper [9] showed that the NJ prior performs better than the Laplacian on sparse
PPCA. In this paper  we choose to adopt the NJ prior for learning a sparse PCSA model.
Different from Eq.(2)  each column of A and B follows a hierarchical Normal-Jeffreys prior:

p(A|αA) =

p(B|αB) =

N (a∗i|0  αA

i ID1 ) 

p(αA) =

N (b∗i|0  αB

i ID2 ) 

p(αB) =

i=1

i=1

  with αA = [αA

1   . . .   αA
d1

⊤

]

 

  with αB = [αB

1   . . .   αB
d2

⊤

]

 

(8)

1
αA
i

1
αB
i

which also encourages the variances in αA and αB of redundant dimensions to approach zeros.
The prior on τ remains the same as in Eq.(2). Still under the variational inference framework 
we now let (cid:2) = {Z  Y  θ} and Q((cid:2)) = Q(Y)Q(Z)Q(θ) takes the conjugate form same as in
Eq.(4). In consequence  we optimize L(Q; αA  αB) =
Q((cid:2)) log p(X (cid:2)|(cid:11)A (cid:11)B )
d(cid:2) w.r.t. Q((cid:2)) 
αA and αB  where L(Q; αA  αB) ≤ log p(X|αA  αB). Posterior inference remains the same
as above  except that all appearances of ⟨ς⟩ and ⟨φ⟩ are replaced with [1/αA
]T and

∫

Q((cid:2))

1   . . .   1/αA
d1

4

d1∏
d2∏

i=1

d1∏
d2∏

i=1

1   . . .   1/αB
d2

[1/αB
αA = 1

D1+2 [diag( (cid:22)A

]T respectively. Then given Q((cid:2))  the variances αA and αB are updated via

⊤ (cid:22)A) + ψA] and αB = 1

D2+2 [diag( (cid:22)B

⊤ (cid:22)B) + ψB].

3.3 Modeling High-Order Tensor Data

Up till now  we have been talking about modeling X when it is a matrix  and this part extends the
PCSA model and its Bayesian inference to cover the cases when X is structured as a tensor. Tensors
are higher-order generalizations of vectors (1st-order tensors) and matrices (2nd-order tensors) [6].
Each dimension of a tensor is called as a mode  and the order of a tensor is determined as the number
of its modes. Let us denote tensors with open-face uppercase letters (e.g.  X  Y  Z)  in comparison
with the bold uppercase letters (e.g.  X  Y  Z) for matrices. A kth-order tensor X can be denoted
by X ∈ RD1×D2×...×Dk  where its dimensionalities in each mode are D1  D2  . . .   Dk respectively.
An element and a (1st-mode) vector of X are denoted by xj1j2...jk and x∗j2...jk respectively  where
1 ≤ ji ≤ Di for each i = 1  . . .   k. Moreover  the 1st-mode ﬂattening transform of X  denoted by
F(X) ∈ RD1×(D2D3...Dk)  is obtained by concatenating all the (1st-mode) vectors of X. Vice versa 
a ([D1  . . .   Dk])-tensorization of a matrix X ∈ RD1×(D2...Dk) is deﬁned as T(X  [D1  . . .   Dk]) ∈
RD1×D2×...×Dk  so that T(F(X)  [D1  . . .   Dk]) = X. An ith mode-shift transform is deﬁned as
M(X  i) ∈ RDi×Di+1×...×Dk×D1×...×Di(cid:0)1  which shifts the modes sequentially in a cycle and until
the ith-mode in X becomes the 1st-mode in M(X  i).
Based on the above deﬁnitions  the PCSA model describes a kth-order tensor data XD1×...×Dk
through the following generative process: (i) for each mode i  all elements of the hidden tensor
Y(i) ∈ Rdi×Di+1×...×Dk×D1×...×Di(cid:0)1 are assumed i.i.d. drawn from N (y(i)
|0  1);
)
(ii) draw each element xj1...jk
  1/τ )  i.e.  X is actually
generated by a mode-shift co-subspace addition:

∼ N (xj1...jk
(

ji∗y(i)∗ji+1...jkj1...ji(cid:0)1

|∑

jiji+1...jkj1ji(cid:0)1

i=1 a(i)

)

(

k

k∑

 

T

i=1

M

X = E +

(cid:22)X(i)  [Di  . . .   Dk  D1  . . .   Di−1]

i=1 and parameters θ = {τ} ∪ {A(i)}k

(9)
where each (cid:22)X(i) = A(i)F(Y(i)) and the matrix A(i) ∈ RDi×di maps Y(i) to X. Shortly named as
PCSA-k  this model has latent tensors {Y(i)}k
i=1 with latent
scales {di}k
i=1. When k = 2  PCSA-2 is exactly the PCSA in Section 2.1 on matrix data. Also  it
can be imagined as a kind of group Factor Analysis [24].
Same as Eq.(2)  each column of A(i) takes a hierarchical Normal-Gamma prior  and the Bayesian
inference in Section 2.2 can be trivially extended for covering PCSA-k model. Please see the details
in the supplementary materials. Except the involvement of the tensor structure and its operators 
there is another difference compared with the variational posterior updating based on a matrix X.
Remembering (Q(Y)  Q(Z)) and (Q(A)  Q(B)) pairwise were decoupled and updated by solving
Sylvester equations  we can decouple neither {Q(Y(i))}i nor {Q(A(i))}i into Sylvester equations
for the general k > 2. Instead  sequentially for each i = 1  . . .   k  we update only Q(Y(i)) (or
Q(A(i))) and keep the remaining {Q(Y(u))}u̸=i (or {Q(A(u))}u̸=i) ﬁxed.

  k + 2 − i

4 Experimental Results

4.1 Predicting Missing Entries in Weight Matrices

On Emotions and CAL500 Data. The proposed PCSA model can be viewed as a rather direct
extension of the PMA model  which showed advantages over GPR  LMC and PMF in [1]. Following
[1]  the ﬁrst experiment compares PCSA with PMA in ﬁlling the missing entries of a truncated log-
odds matrix in multi-label classiﬁcation. For n samples and m classes  the class memberships can
be represented as an n × m binary matrix G. A truncated log-odds matrix X is constructed with
xij = c if gij = 1 and xij = −c if gij = 0  where c is nonzero constant. In experiments  certain
entries xij are assumed to be missing and ﬁlled as ~xij by an algorithm  and the performance is
evaluated by the class membership prediction accuracy based on sign(~xij).
Two multi-label classiﬁcation datasets are under consideration  namely Emotions [22] and
CAL500 [23]. Already used in [1]  the Emotions contains 593 samples with 72 numeric at-

5

tributes in 6 classes  and the number of classes that each sample belongs to ranges from 1 to 3. The
constructed X for Emotions is thus 593× 6. The CAL500 contains 502 samples with 68 numeric
attributes in 174 classes  and the min and max numbers of classes that each sample belongs to are
13 and 48 respectively. The constructed X for CAL500 is thus 502 × 174  i.e.  its size is larger and
more balanced than the one for Emotions.
To test the capability in dealing with missing values  the proportion of the missing labels is increased
from 10% to 50%  with 5% as a step size. Instead of Gibbs sampling  the MAP inference is used in
PMA implementation for a fair comparison. After 10 independent runs on each dataset  Fig.1 report-
s the error rates for recovering the missing labels in the truncated log-odds matrices  by Bayesian
PCSA  Bayesian sparse PCSA and PMA. On the relatively unbalanced Emotions data  PCSA out-
performs sparse PCSA when the missing proportion is no larger than 40%  while sparse PCSA takes
over the advantage when too many entries are missing due to the increasing importance of model
sparsity. On the more balanced CAL500 data  the sparse PCSA keeps a slight outperformance over
PCSA  again due to the sparsity. Moreover  PCSA and sparse PCSA always perform considerably
better than PMA on both datasets. Table 1 reports the average time cost  where sparse PCSA shows
a little quicker convergence than PCSA. Both are much quicker than PMA  since they do not need
to either invert large covariances or infer the factor during ﬁlling missing values (see Section 3.1).

dataset:
PCSA

sparse PCSA

PMA

Emotions CAL500

4.0
3.5
22.9

17.3
11.6
198.3

Figure 1: Error rates of 10 independent runs for
recovering the missing labels in Emotions (left)
and CAL500 (right) data.

Table 1: Average time cost (in seconds)
on each dataset throughout 10 indepen-
dent runs and all missing proportions.

On MovieLens and JesterJoke Data.
In many real applications  e.g. collaborative ﬁltering 
the size of the matrix X is much larger than the above. We proceed to consider on two larger
weight datasets: the MovieLens100K data3 and the JesterJoke3 data [8]. Particularly  the
MovieLens100K dataset contains 100K ratings of 943 users on 1682 movies  which are ordinal
values on the scale [1  5]. The JesterJoke3 data contains ratings of 24983 users who have rated
between 15 and 35 pieces of the total 100 jokes  where the ratings are continuous in [−10.0  10.0].
Recently in [12]  Robust Bayesian Matrix Factorization (RBMF) was proposed by adopting a
Student-t prior in probabilistic matrix factorization  and showed promising results on predicting
entries on both MovieLens100K and JesterJoke3 data. Following [12]  in each run we ran-
∑
domly choose 70% of the ratings for training  and use the remaining ratings as the missing values
t=1 and the predictions {~rt}T
for testing. Given the true test ratings {rt}T
t=1  the performance is eval-
t=1(rt − ~rt)2  and
uated based on the rooted mean squared error (RMSE)  i.e.  RMSE =
the mean absolute error (MAE)  i.e.  MAE = 1
T
After 10 independent runs  the average RMSE and MAE values obtained by (sparse) PCSA are
reported in Table 2  in comparison with the best results by RBMF (i.e.  RBMF-RR) collected from
[12]. Since PMA runs inefﬁciently on high dimensional data as in Table 1  it is not considered to ﬁll
the ratings in this experiment. It is observed that the performance by PCSA on predicting ratings is
comparable with RBMF. On both RMSE and MAE scores  the sparse PCSA further improves the
correctness and performs similarly to or better than RBMF.

|rt − ~rt|.

∑

√

T

1
T

T
t=1

Table 2: Average RMSE and MAE on MovieLens100K (left) and JesterJoke3 (right).

model
PCSA

sparse PCSA
RBMF-RR [12]

RMSE MAE
0.903
0.708
0.898
0.706
0.705
0.900

model
PCSA

sparse PCSA
RBMF-RR [12]

RMSE MAE
4.446
3.447
3.434
4.413
4.454
3.439

3Downloaded from www.grouplens.org/node/73.

6

4.2 Completing Partially Observed Images

We consider two greyscale face image datasets  namely Frey [15] and ORL [17]. Speciﬁcally  Frey
has 1965 images of size 28× 20 taken from one person  and the data X is thus a 560× 1965 matrix;
ORL has 400 images of size 64 × 64 taken from 40 persons (10 images per person)  and the data X
is thus a 4096 × 400 matrix. Applied on these matrices  the PCSA model is expected to extract the
latent correlations among pixels and images. In [13]  Neil Lawrence proposed a Gaussian process
latent variable model (GPLVM) for modeling and visualizing high dimensional data. Recently a
Bayesian GPLVM [21] was developed and showed much improved performance on ﬁlling pixels in
partially observed Frey faces. This experiment compares PCSA with Bayesian GPLVM4.
While PCSA can utilize the partial observed samples  the Bayesian GPLVM cannot. Thus in each
run  we randomly pick nf images as fully observed  and a half pixels of the remaining images are
further randomly chosen as missing values. Same as [21]  Bayesian GPLVM uses the nf images for
training and then infers the missing pixels. In contrast  (sparse) PCSA uses all images as a whole
matrix. In order to test the robustness  the nf for Frey is decreased gradually from 1000 to 200 
and for ORL is decreased gradually from 300 to 50. Performance is evaluated by the correlation
coefﬁcient (CORR) and the MAE between the ﬁlled pixels and the ground truth.
On Frey and ORL data respectively  Fig.2 and Fig.3 report the CORR and MAE values of 10
independent runs by PCSA  sparse PCSA and Bayesian GPLVM. Both PCSA and sparse PCSA
perform more accurately than Bayesian GPLVM in completing the missing pixels  and PCSA gives
the best matching. Also  (sparse) PCSA shows promising stability against the decreasing fully
observed sample size nf   and this tendency is kept even when we assign all images are partially
observed (i.e.  nf = 0)  as exempliﬁed by Fig.4. The results by Bayesian GPLVM deteriorate more
obviously  because the partially observed images have no contribution during learning. Furthermore 
the advantage of PCSA becomes more signiﬁcant  as we shift from the Frey data for a single
person  to the ORL data for multiple persons. It indirectly reﬂects the importance of extracting the
correlations among different images  rather than keeping them independent. Sparse PCSA performs
worse than PCSA in this task  mainly because it leads to a little too many sparse dimensions.

Figure 2: Results of 10 runs on Frey faces.

Figure 3: Results of 10 runs on ORL faces.

Figure 4: Reconstruction examples by PCSA when all images are partially observed: Frey (left)
and ORL (right). Three rows from top are true  observed  and reconstructed images  respectively.

4.3 Completing Partially Observed Image Tensor

We proceed to consider modeling the face image data arranged in a tensor. The dataset under consid-
eration is a subset of the CMU PIE database [18]  and totally has 5100 face images from 30 individ-
uals. Each person’s face exhibits 170 images corresponding to 170 different pose-and-illumination
combinations. Each normalized image has 32 × 32 greyscale pixels  and the dataset is thus a tensor
X ∈ R1024×30×170  whose three modes correspond to pixel  identity  and pose/illumination  respec-
tively. Figure 5 shows some image examples of two persons. The PCSA-k model (with k = 3 on the

4We use the code in http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/vargplvm/.

7

3rd-order tensor X) in Section 3.3 is expected to extract the co-subspace structures (i.e.  correlations
among pixels  identities  and poses/illuminations respectively) and ﬁll the missing values in X. In
[6]  an M2SA method was proposed to conduct multilinear subspace analysis with missing values
on the tensor data  via consecutive SVD dimension reductions on each mode.

Figure 5: Typical normalized face images from the CMU PIE database.

true:

ﬁlled:

true:

ﬁlled:

Figure 6: Typical missing images ﬁlled by PCSA-3. Original images (in the odd rows) are
randomly picked and removed  and PCSA-3 ﬁlls the images in the even rows.

Table 3: Average CORR (left) and MAE (right) of 10 runs
by PCSA-3 and M2SA on the CMU PIE data.

missing proportion:

PCSA-3
M2SA

missing proportion:

PCSA-3
M2SA

10% 20% 30%
0.937
0.908
0.893
0.928

0.926
0.914

10% 20% 30%
21.5
14.6
17.8
24.8

18.3
21.9

Here  the randomly drawn missing values are not pixels as in Section 4.2 but images. Compared with
the true missing images  the goodness of the ﬁlled missing images is evaluated again by CORR and
MAE. Still to test the capability in dealing with missing values  the proportion of the missing images
is considered as 10%  20% and 30%  respectively. After 10 independent runs for each proportion  the
averages CORR and average MAE of ﬁling the missing images by PCSA-3 and M2SA are compared
in Table 3. During implementing M2SA  the ratio of the subspace rank over the original rank is set
as 0.6 according to Fig.9 in [6]. As shown in Table 3  PCSA-3 achieves the better performance in
all cases. For demonstration  Fig.6 shows some ﬁlled missing images when the missing proportion
is 20%  which match the original images steadily well.

5 Concluding Remarks

We have introduced the Probabilistic Co-Subspace Addition (PCSA) model  which simultaneously
captures the dependent structures among both rows and columns in data matrices (tensors). Vari-
ational inference is proposed on PCSA for an approximate Bayesian learning  and the posteriors
can be efﬁciently and stably updated by solving Sylvester equations. Capable to ﬁll missing values 
PCSA is extended to not only sparse PCSA with the help of a Jeffreys prior  but also PCSA-k that
models arbitrary kth-order tensor data. Although somewhat simple and not designed for any partic-
ular application  the experiments demonstrate the effectiveness and efﬁciency of PCSA on modeling
matrix (tensor) data and ﬁlling missing values. The performance by PCSA may be further improved
by considering nonlinear mappings with the kernel trick  which however is not that direct due to the
coupling inner products between the co-subspaces.

Acknowledgments

The author would like to thank the anonymous reviewers for their useful comments on this paper.

8

References
[1] A. Agovic  A. Banerjee  and S. Chatterjee. Probabilistic matrix addition. In Proc. ICML  pages 1025–

1032  2011.

[2] C. M. Bishop. Training with noise is equivalent to Tikhonov regularization. Neural Computation 

7(1):108–116  1995.

[3] C. M. Bishop. Variational principal components. In Proc. ICANN’1999  volume 1  pages 509–514  1999.
[4] M. A. T. Figueiredo. Adaptive sparseness using Jeffreys prior. In Advances in NIPS  volume 14  pages

679–704. MIT Press  Cambridge  MA  2002.

[5] A. E. Gelfand and S. Banerjee. Multivariate spatial process models. In A. E. Gelfand  P. Diggle  P.Guttorp 

and M. Fuentes  editors  Handbook of Spatial Statistics. CRC Press  2010.

[6] X. Geng  K. Smith-Miles  Z.-H. Zhou  and L. Wang. Face image modeling by multilinear subspace

analysis with missing values. IEEE Trans. Syst.  Man  Cybern. B  Cybern.  41(3):881–892  2011.

[7] Z. Ghahramani and G. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report

CRG-TR-96-1  Department of Computer Science  University of Toronto  Toronto  Canada  1997.

[8] K. Goldberg  T. Roeder  D. Gupta  and C. Perkins. Eigentaset: A constant time collaborative ﬁltering

algorithm. Information Retrieval  4(2):133–151  2001.

[9] Y. Guan and J. Dy. Sparse probabilistic principal component analysis. In Proc. AISTATS’2009  JMLR

W&CP  volume 5  pages 185–192. 2009.

[10] D. Y. Hu and L. Reichel. Krylov-subspace methods for the Sylvester equation. Linear Algebra and Its

Applications  172:283–313  1992.

[11] M. I. Jordan  editor. Learning in graphical models. MIT Press  Cambridge MA  1999.
[12] B. Lakshimanarayan  G. Bouchard  and C. Archambeau. Robust Bayesian matrix factorisation. In Proc.

AISTATS’2011  JMLR W&CP  volume 15  pages 425–433. 2011.

[13] N. D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In

Advances in NIPS  volume 16  pages 329–336. MIT Press  Cambridge  MA  2003.

[14] R. M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag  New York  1996.
[15] S. Roweis  L. K. Saul  and G. Hinton. Global coordination of local linear models. In Advances in NIPS 

volume 14  pages 889–896. MIT Press  Cambridge  MA  2002.

[16] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in NIPS  volume 20  pages

1257–1264. MIT Press  Cambridge  MA  2008.

[17] F. Samaria and A. Harter. Parameterisation of a stochastic model for human face identiﬁcation. In Proc.

2nd IEEE Workshop on Applications of Computer Vision  pages 138–142  1994.

[18] T. Sim  S. Baker  and M. Bsat. The CMU pose  illumination  and expression database. IEEE Trans. Patten

Anal. Mach. Intell.  25(12):1615–1618  2003.

[19] R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. B  58(1):267–288  1996.
[20] M. E. Tipping and C. M. Bishop. Mixtures of probabilistic principal component analyzers. Neural

Computation  11(2):443–482  1999.

[21] M. Titsias and N. Lawrence. Bayesian Gaussian process latent variable model. In Proc. AISTATS’2009 

JMLR W&CP  volume 9  pages 844–851. 2010.

[22] K. Trohidis  G. Tsoumakas  G. Kalliris  and I. Vlahavas. Multilabel classiﬁcation of music into emotions.

In Proc. Intl. Conf. on Music Information Retrieval (ISMIR)  pages 325–330  2008.

[23] D. Turnbull  L. Barrington  D. Torres  and G. Lanckriet. Semantic annotation and retrieval of music and

sound effects. IEEE Trans. Audio  Speech and Lang. Process.  16(2):467–476  2008.

[24] S. Virtanen  A. Klami  S. A. Khan  and S. Kaski. Bayesian group factor analysis. In Proc. AISTATS’2012 

JMLR W&CP  volume 22  pages 1269–1277. 2012.

[25] Z. Xu  K. Kersting  and V. Tresp. Multi-relational learning with Gaussian processes. In Proc. IJCAI’2009 

pages 1309–1314  2009.

9

,Ying Yang
Elissa Aminoff
Michael Tarr
Kass Robert
Brett Daley
Christopher Amato