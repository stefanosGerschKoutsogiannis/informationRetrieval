2016,Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction,We consider a crowdsourcing model in which n workers are asked to rate the quality of n items previously generated by other workers. An unknown set of $\alpha n$ workers generate reliable ratings  while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items  and wishes to curate together almost all of the high-quality items with at most an fraction of low-quality items. Perhaps surprisingly  we show that this is possible with an amount of work required of the manager  and each worker  that does not scale with n: the dataset can be curated with $\tilde{O}(1/\beta\alpha\epsilon^4)$ ratings per worker  and $\tilde{O}(1/\beta\epsilon^2)$ ratings by the manager  where $\beta$ is the fraction of high-quality items. Our results extend to the more general setting of peer prediction  including peer grading in online classrooms.,Avoiding Imposters and Delinquents: Adversarial

Crowdsourcing and Peer Prediction

Jacob Steinhardt
Stanford University

Gregory Valiant
Stanford University

Moses Charikar
Stanford University

Abstract

We consider a crowdsourcing model in which n workers are asked to rate the quality
of n items previously generated by other workers. An unknown set of αn workers
generate reliable ratings  while the remaining workers may behave arbitrarily and
possibly adversarially. The manager of the experiment can also manually evaluate
the quality of a small number of items  and wishes to curate together almost all
of the high-quality items with at most an  fraction of low-quality items. Perhaps
surprisingly  we show that this is possible with an amount of work required of the
manager  and each worker  that does not scale with n: the dataset can be curated
ratings by the manager  where β
is the fraction of high-quality items. Our results extend to the more general setting
of peer prediction  including peer grading in online classrooms.

ratings per worker  and ˜O(cid:16) 1

(cid:17)

with ˜O(cid:16) 1

βα34

(cid:17)

β2

1

Introduction

How can we reliably obtain information from humans  given that the humans themselves are unreli-
able  and might even have incentives to mislead us? Versions of this question arise in crowdsourcing
(Vuurens et al.  2011)  collaborative knowledge generation (Priedhorsky et al.  2007)  peer grading
in online classrooms (Piech et al.  2013; Kulkarni et al.  2015)  aggregation of customer reviews
(Harmon  2004)  and the generation/curation of large datasets (Deng et al.  2009). A key challenge
is to ensure high information quality despite the fact that many people interacting with the system
may be unreliable or even adversarial. This is particularly relevant when raters have an incentive to
collude and cheat as in the setting of peer grading  as well as for reviews on sites like Amazon and
Yelp  where artists and ﬁrms are incentivized to manufacture positive reviews for their own products
and negative reviews for their rivals (Harmon  2004; Mayzlin et al.  2012).
One approach to ensuring quality is to use gold sets — questions where the answer is known  which
can be used to assess reliability on unknown questions. However  this is overly constraining — it
does not make sense for open-ended tasks such as knowledge generation on wikipedia  nor even for
crowdsourcing tasks such as “translate this paragraph” or “draw an interesting picture” where there
are different equally good answers. This approach may also fail in settings  such as peer grading in
massive online open courses  where students might collude to inﬂate their grades.
In this work  we consider the challenge of using crowdsourced human ratings to accurately and
efﬁciently evaluate a large dataset of content. In some settings  such as peer grading  the end goal
is to obtain the accurate evaluation of each datum; in other settings  such as the curation of a large
dataset  accurate evaluations could be leveraged to select a high-quality subset of a larger set of
variable-quality (perhaps crowd-generated) data.
There are several confounding difﬁculties that arise in extracting accurate evaluations. First  many
raters may be unreliable and give evaluations that are uncorrelated with the actual item quality;
second  some reliable raters might be harsher or more lenient than others; third  some items may be
harder to evaluate than others and so error rates could vary from item to item  even among reliable
30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

raters; ﬁnally  some raters may even collude or want to hack the system. This raises the question: can
we obtain information from the reliable raters  without knowing who they are a priori?
In this work  we answer this question in the afﬁrmative  under surprisingly weak assumptions:

• We do not assume that the majority of workers are reliable.
• We do not assume that the unreliable workers conform to any statistical model; they could
behave fully adversarially  in collusion with each other and with full knowledge of how the
reliable workers behave.
• We do not assume that the reliable worker ratings match the true ratings  but only that they
are “approximately monotonic” in the true ratings  in a sense that will be formalized later.
• We do not assume that there is a “gold set” of items with known ratings presented to each
user (as an adversary could identify and exploit this). Instead  we rely on a small number of
reliable judgments on randomly selected items  obtained after the workers submit their own
ratings; in practice  these could be obtained by rating those items oneself.

For concreteness  we describe a simple formalization of the crowdsourcing setting (our actual results
hold in a more general setting). We imagine that we are the dataset curator  so that “us” and “ourselves”
refers in general to whoever is curating the data. There are n raters and m items to evaluate  which
have an unknown quality level in [0  1]. At least αn workers are “reliable” in that their judgments
match our own in expectation  and they make independent errors. We assign each worker to evaluate
at most k randomly selected items. In addition  we ourselves judge k0 items. Our goal is to recover
the β-quantile: the set T ∗ of the βm highest-quality items. Our main result implies the following:
Theorem 1. In the setting above  suppose n = m. Then there is k = O(
βα34 )  and k0 = ˜O( 1
β2 )
such that  with probability 99%  we can identify βm items with average quality only  worse than T ∗.

1

Interestingly  the amount of work that each worker (and we ourselves) has to do does not grow with
n; it depends only on the fraction α of reliable workers and the desired accuracy . While the number
of evaluations k for each worker is likely not optimal  we note that the amount of work k0 required of
us is close to optimal: for α ≤ β  it is information theoretically necessary for us to evaluate Ω(1/β2)
items  via a reduction to estimating noisy coin ﬂips.
2  then an adversary
Why is it necessary to include some of our own ratings? If we did not  and α < 1
could create a set of dishonest raters that were identical to the reliable raters except with the item
indices permuted by a random permutation of {1  . . .   m}. In this case  there is no way to distinguish
the honest from the dishonest raters except by breaking the symmetry with our own ratings.
Our main result holds in a considerably more general setting where we require a weaker form of
inter-rater agreement — for example  our results hold even if some of the reliable raters are harsher
than others  as long as the expected ratings induce approximately the same ranking. The focus on
quantiles rather than raw ratings is what enables this. Note that once we estimate the quantiles  we
can approximately recover the ratings by evaluating a few items in each quantile.

r∗

true ratings

good raters

˜A

random

adversaries



.9
1

1

1
1



T ∗

=⇒ M∗



1

1
1

1

1
1

1

1
1

0

1
1

1

1
1

1

0
0

0

0
0

0

0
0

0

0
0

1

0
0



0

0
0

0

1
1

items

1

0.8

0.6

0.4

0.2

0.1

.8
.9

0

.8
.8

.7
.8

1

.6
.6

.6
.2

0

0
0

.5
.1

1

0
0

.4
0

0

1
1

Figure 1: Illustration of our problem setting. We observe a small number of ratings from each rater
(indicated in blue)  which we represent as entries in a matrix ˜A (unobserved ratings in red  treated as
zero by our algorithm). There is also a true rating r∗ that we would assign to each item; by rating
some items ourself  we observe some entries of r∗ (also in blue). Our goal is to recover the set T ∗
representing the top β fraction of items under r∗. As an intermediate step  we approximately recover
a matrix M∗ that indicates the top items for each individual rater.

2

Our technical tools draw on semideﬁnite programming methods for matrix completion  which have
been used to study graph clustering as well as community detection in the stochastic block model
(Holland et al.  1983; Condon and Karp  2001). Our setting corresponds to the sparse case of graphs
with constant degree  which has recently seen great interest (Decelle et al.  2011; Mossel et al.  2012;
2013b;a; Massoulié  2014; Guédon and Vershynin  2014; Mossel et al.  2015; Chin et al.  2015; Abbe
and Sandon  2015a;b; Makarychev et al.  2015). Makarychev et al. (2015) in particular provide an
algorithm that is robust to adversarial perturbations  but only if the perturbation has size o(n); see
also Cai and Li (2015) for robustness results when the degree of the graph is logarithmic.
Several authors have considered semirandom settings for graph clustering  which allow for some
types of adversarial behavior (Feige and Krauthgamer  2000; Feige and Kilian  2001; Coja-Oghlan 
2004; Krivelevich and Vilenchik  2006; Coja-Oghlan  2007; Makarychev et al.  2012; Chen et al. 
2014; Guédon and Vershynin  2014; Moitra et al.  2015; Agarwal et al.  2015). In our setting  these
semirandom models are unsuitable because they rule out important types of strategic behavior  such
as an adversary rating some items accurately to gain credibility. By allowing arbitrary behavior
from the adversary  we face a key technical challenge: while previous analyses consider errors
relative to a ground truth clustering  in our setting the ground truth only exists for rows of the matrix
corresponding to reliable raters  while the remaining rows could behave arbitrarily even in the limit
where all ratings are observed. This necessitates a more careful analysis  which helps to clarify what
properties of a clustering are truly necessary for identifying it.

2 Algorithm and Intuition

We now describe our recovery algorithm. To ﬁx notation  we assume that there are n raters and m
items  and that we observe a matrix ˜A ∈ [0  1]n×m: ˜Aij = 0 if rater i does not rate item j  and
otherwise ˜Aij is the assigned rating  which takes values in [0  1]. In the settings we care about ˜A is
very sparse — each rater only rates a few items. Remember that our goal is to recover the β-quantile
T ∗ of the best items according to our own rating.
Our algorithm is based on the following intuition: the reliable raters must (approximately) agree on
the ranking of items  and so if we can cluster the rows of ˜A appropriately  then the reliable raters
should form a single very large cluster (of size αn). There can be at most 1
α disjoint clusters of this
size  and so we can manually check the accuracy of each large cluster (by checking agreement with
our own rating on a few randomly selected items) and then choose the best one.
One major challenge in using the clustering intuition is the sparsity of ˜A: any two rows of ˜A will
almost certainly have no ratings in common  so we must exploit the global structure of ˜A to discover
clusters  rather than using pairwise comparisons of rows. The key is to view our problem as a form of
noisy matrix completion — we imagine a matrix A∗ in which all the ratings have been ﬁlled in and
all noise from individual ratings has been removed. We deﬁne a matrix M∗ that indicates the top βm
items in each row of A∗: M∗
ij = 0
otherwise (this differs from the actual deﬁnition of M∗ given in Section 4  but is the same in spirit).
If we could recover M∗  we would be close to obtaining the clustering we wanted.

ij = 1 if item j has one of the top βm ratings from rater i  and M∗

Algorithm 1 Algorithm for recovering β-quantile matrix ˜M using (unreliable) ratings ˜A.
1: Parameters: reliable fraction α  quantile β  tolerance   number of raters n  number of items m
2: Input: noisy rating matrix ˜A
3: Let ˜M be the solution of the optimization problem (1):

maximize (cid:104) ˜A  M(cid:105) 
(cid:80)
subject to 0 ≤ Mij ≤ 1 ∀i  j 
jMij ≤ βm ∀j 

where (cid:107) · (cid:107)∗ denotes nuclear norm.

4: Output ˜M.

3

(cid:112)αβnm 

(cid:107)M(cid:107)∗ ≤ 2
α

(1)

√

α

4 βk

Algorithm 2 Algorithm for recovering an accurate β-quantile T from the β-quantile matrix ˜M.
1: Parameters: tolerance   reliable fraction α
2: Input: matrix ˜M of approximate β-quantiles  noisy ratings ˜r
3: Select 2 log(2/δ)/α indices i ∈ [n] at random.
4: Let i∗ be the index among these for which (cid:104) ˜Mi  ˜r(cid:105) is largest  and let T0 ← ˜Mi∗. (cid:46) T0 ∈ [0  1]m
5: do T ← RANDOMIZEDROUND(T0) while (cid:104)T − T0  ˜r(cid:105) < − 
(cid:46) T ∈ {0  1}m
6: return T
The key observation that allows us to approximate M∗ given only the noisy  incomplete ˜A is that M∗
has low-rank structure: since all of the reliable raters agree with each other  their rows in M∗ are all
identical  and so there is an (αn)× m submatrix of M∗ with rank 1. This inspires the low-rank matrix
completion algorithm for recovering ˜M given in Algorithm 1. Each row of M is constrained to have
sum at most βm  and M as a whole is constrained to have nuclear norm (cid:107)M(cid:107)∗ at most 2
αβnm.
Recall that the nuclear norm is the sum of the singular values of M; in the same way that the (cid:96)1-norm
is a convex surrogate for the (cid:96)0-norm  the nuclear norm acts as a convex surrogate for the rank of M
(i.e.  number of non-zero singular values). The optimization problem (1) therefore chooses a set of
βm items in each row to maximize the corresponding values in ˜A  while constraining the item sets to
have low rank (where low rank is relaxed to low nuclear norm to obtain a convex problem). This
low-rank constraint acts as a strong regularizer that quenches the noise in ˜A.
Once we have recovered ˜M using Algorithm 1  it remains to recover a speciﬁc set T that approximates
the β-quantile according to our ratings. Algorithm 2 provides a recipe for doing so: ﬁrst  rate k0
items at random  obtaining the vector ˜r: ˜rj = 0 if we did not rate item j  and otherwise ˜rj is the
(possibly noisy) rating that we assign to item j. Next  score each row ˜Mi based on the noisy ratings
˜Mij ˜rj  and let T0 be the highest-scoring ˜Mi among O(log(2/δ)/α) randomly selected i. Finally 
randomly round the vector T0 ∈ [0  1]m to a discrete vector T ∈ {0  1}m  and treat T as the indicator
function of a set approximating the β-quantile (see Section 5 for details of the rounding algorithm).
In summary  given a noisy rating matrix ˜A  we will ﬁrst run Algorithm 1 to recover a β-quantile
matrix ˜M for each rater  and then run Algorithm 2 to recover our personal β-quantile using ˜M.
Possible attacks by adversaries. In our algorithm  the adversaries can inﬂuence ˜Mi for reliable
raters i via the nuclear norm constraint (note that the other constraints are separable across rows).
This makes sense because the nuclear norm is what causes us to pool global structure across raters
(and thus potentially pool bad information). In order to limit this inﬂuence  the constraint on the
nuclear norm is weaker than is typical by a factor of 2
 ; it is not clear to us whether this is actually
necessary or due to a loose analysis.
j Mij ≤ βm is not typical in the literature. For instance  (Chen et al.  2014) place
no constraint on the sum of each row in M (they instead normalize ˜M to lie in [−1  1]n×m  which
recovers the items with positive rating rather than the β-quantile). Our row normalization constraint
prevents an attack in which a spammer rates a random subset of items as high as possible and rates the
remaining items as low as possible. If the actual set of high-quality items has density much smaller
than 50%  then the spammer gains undue inﬂuence relative to honest raters that only rate e.g. 10% of
the items highly. Normalizing M to have a ﬁxed row sum prevents this; see Section B for details.

The constraint(cid:80)

(cid:80)

j

3 Assumptions and Approach
We now state our assumptions more formally  state the general form of our results  and outline the
key ingredients of the proof. In our setting  we can query a rater i ∈ [n] and item j ∈ [m] to obtain a
rating ˜Aij ∈ [0  1]. Let r∗ ∈ [0  1]m denote the vector of true ratings of the items. We can also query
an item j (by rating it ourself) to obtain a noisy rating ˜rj such that E[˜rj] = r∗
j .
Let C ⊆ [n] be the set of reliable raters  where |C| ≥ αn. Our main assumption is that the reliable
raters make independent errors:
Assumption 1 (Independence). When we query a pair (i  j) with i ∈ C  we obtain an output ˜Aij
whose value is independent of all of the other queries so far. Similarly  when we query an item j  we
obtain an output ˜rj that is independent of all of the other queries so far.

4

Algorithm 3 Algorithm for obtaining (unreliable) ratings matrix ˜A and noisy ratings ˜r  ˜r(cid:48).
1: Input: number of raters n  number of items m  and number of ratings k and k0.
2: Initially assign each rater to each item independently with probability k/m.
3: For each rater with more than 2k items  arbitrarily unassign items until there are 2k remaining.
4: For each item with more than 2k raters  arbitrarily unassign raters until there are 2k remaining.
5: Have the raters submit ratings of their assigned items  and let ˜A denote the resulting matrix of

ratings with missing entries ﬁll in with zeros.
6: Generate ˜r by rating items with probability k0
7: Output ˜A  ˜r

m (ﬁll in missing entries with zeros)

Note that Assumption 1 allows the unreliable ratings to depend on all previous ratings and also allows
arbitrary collusion among the unreliable raters. In our algorithm  we will generate our own ratings
after querying everyone else  which ensures that at least ˜r is independent of the adversaries.
We need a way to formalize the idea that the reliable raters agree with us. To this end  for i ∈ C
let A∗
ij = E[ ˜Aij] be the expected rating that rater i assigns to item j. We want A∗ to be roughly
increasing in r∗:
Deﬁnition 1 (Monotonic raters). We say that the reliable raters are (L  )-monotonic if

whenever r∗

j ≥ r∗

j(cid:48) ≤ L · (A∗
j(cid:48)  for all i ∈ C and all j  j(cid:48) ∈ [m].

j − r∗
r∗

ij − A∗

ij(cid:48)) + 

(2)

(3)

5. Then A∗

j with probability 3

j ∈ {0  1}) and that each rating ˜Ai j matches r∗

The (L  )-monotonicity property says that if we think that one item is substantially better than another
item  the reliable raters should think so as well. As an example  suppose that our own ratings are
binary (r∗
5 r∗
j  
and hence the ratings are (5  0)-monotonic. In general  the monotonicity property is fairly mild — if
the reliable ratings are not (L  )-monotonic  it is not clear that they should even be called reliable!
Algorithm for collecting ratings. Under the model given in Assumption 1  our algorithm for
collecting ratings is given in Algorithm 3. Given integers k and k0  Algorithm 3 assigns each rater
at most 2k ratings  and assigns us k0 ratings in expectation. The output is a noisy rating matrix
˜A ∈ [0  1]n×m as well as a noisy rating vector ˜r ∈ [0  1]m. Our main result states that we can use ˜A
and ˜r to estimate the β-quantile T ∗; throughout we will assume that m is at least n.
Theorem 2. Let m ≥ n. Suppose that Assumption 1 holds  that the reliable raters are (L  0)-

monotonic  and that we run Algorithm 3 to obtain noisy ratings. Then there is k = O(cid:16) log3(2/δ)
and k0 = O(cid:16) log(2/αβδ)

such that  with probability 1 − δ  Algorithms 1 and 2 output a set T with

i j = 2

5 + 1

(cid:17)

(cid:17)

βα34

m
n

β2

(cid:88)

j∈T ∗

j −(cid:88)

r∗

j∈T

1
βm

 ≤ (2L + 1) ·  + 20.

r∗

j

n . Some dependence on m

Note that the amount of work for the raters scales as m
we need to make sure that every item gets rated at least once.
The proof of Theorem 2 can be split into two parts: analyzing Algorithm 1 (Section 4)  and analyzing
Algorithm 2 (Section 5). At a high level  analyzing Algorithm 1 involves showing that the nuclear
norm constraint in (1) imparts sufﬁcient noise robustness while not allowing the adversary too much
inﬂuence over the reliable rows of ˜M. Analyzing Algorithm 2 is far more straightforward  and
requires only standard concentration inequalities and a standard randomized rounding idea (though
the latter is perhaps not well-known  so we will explain it brieﬂy in Section 5).

n is necessary  since

4 Recovering ˜M (Algorithm 1)

The goal of this section is to show that solving the optimization problem (1) recovers a matrix ˜M that
approximates the β-quantile of r∗ in the following sense:

5

(cid:88)

(cid:88)

Proposition 1. Under the conditions of Theorem 2 and the corresponding values of k and k0 
Algorithm 1 outputs a matrix ˜M satisfying

(T ∗

1
βm

1
|C|
with probability 1 − δ  where T ∗
j = 1 if j lies in the β-quantile of r∗  and is 0 otherwise.
Proposition 1 says that the row ˜Mi is good according to rater i’s ratings A∗
monotonicity then implies that ˜Mi is also good according to r∗. In particular (see A.2 for details)

j − ˜Mij)A∗

ij ≤ 

i . Note that (L  0)-

j∈[m]

i∈C

(4)

1
|C|

1
βm

(T ∗

j − ˜Mij)r∗

j ≤ L · 1
|C|

1
βm

(T ∗

j − ˜Mij)A∗

ij + 0 ≤ L ·  + 0.

(5)

(cid:88)

(cid:88)

i∈C

j∈[m]

(cid:88)

(cid:88)

i∈C

j∈[m]

Proving Proposition 1 involves two major steps: showing (a) that the nuclear norm constraint in (1)
imparts noise-robustness  and (b) that the constraint does not allow the adversaries to inﬂuence ˜MC
too much. (For a matrix X we let XC denote the rows indexed by C and XC the remaining rows.)
In a bit more detail  if we let M∗ denote the “ideal” value of ˜M  and B denote a denoised version
of ˜A  we ﬁrst show (Lemma 1) that (cid:104)B  ˜M − M∗(cid:105) ≥ −(cid:48) for some (cid:48) determined below. This is
established via the matrix concentration inequalities in Le et al. (2015). Lemma 1 would already
sufﬁce for standard approaches (e.g.  Guédon and Vershynin  2014)  but in our case we must grapple
with the issue that the rows of B could be arbitrary outside of C  and hence closeness according to
B may not imply actual closeness between ˜M and M∗. Our main technical contribution  Lemma 2 
shows that (cid:104)BC  ˜MC − M∗
C(cid:105) ≥ (cid:104)B  ˜M − M∗(cid:105)− (cid:48); that is  closeness according to B implies closeness
according to BC. We can then restrict attention to the reliable raters  and obtain Proposition 1.
Part 1: noise-robustness. Let B be the matrix satisfying BC = k
on C. The scaling k
Ideally  we would like to have MC = RC  i.e.  M matches T ∗ on all the rows of C. In light of this 
we will let M∗ be the solution to the following “corrected” program  which we don’t have access to
(since it involves knowledge of A∗ and C)  but which will be useful for analysis purposes:

m is chosen so that E[ ˜AC] ≈ BC. Also deﬁne R ∈ Rn×m by Rij = T ∗
j .

C  BC = ˜AC  which denoises ˜A

m A∗

(6)

(7)

maximize (cid:104)B  M(cid:105) 
subject to MC = RC 

(cid:80)
jMij ≤ βm ∀i 
ij = T ∗

0 ≤ Mij ≤ 1 ∀i  j 
(cid:107)M(cid:107)∗ ≤ 2
α

(cid:112)αβnm

Importantly  (6) enforces M∗

Lemma 1. Let m ≥ n. Suppose that Assumption 1 holds. Then there is a k = O(cid:16) log3(2/δ)

j for all i ∈ C. Lemma 1 shows that ˜M is “close” to M∗:

such
that the solution ˜M to (1) performs nearly as well as M∗ under B; speciﬁcally  with probability
1 − δ 

(cid:17)

βα34

m
n

(cid:104)B  ˜M(cid:105) ≥ (cid:104)B  M∗(cid:105) − αβkn.

Note that ˜M is not necessarily feasible for (6)  because of the constraint MC = RC; Lemma 1 merely
asserts that ˜M approximates M∗ in objective value. The proof of Lemma 1  given in Section A.3 
primarily involves establishing a uniform deviation result; if we let P denote the feasible set for (1) 
then we wish to show that |(cid:104) ˜A − B  M(cid:105)| ≤ 1
2 αβkn for all M ∈ P. This would imply that the
objectives of (1) and (6) are essentially identical  and so optimizing one also optimizes the other.
Using the inequality |(cid:104) ˜A − B  M(cid:105)| ≤ (cid:107) ˜A − B(cid:107)op(cid:107)M(cid:107)∗  where (cid:107) · (cid:107)op denotes operator norm  it
sufﬁces to establish a matrix concentration inequality bounding (cid:107) ˜A − B(cid:107)op. This bound follows
from the general matrix concentration result of Le et al. (2015)  stated in Section A.1.
Part 2: bounding the inﬂuence of adversaries. We next show that the nuclear norm constraint does
not give the adversaries too much inﬂuence over the de-noised program (6); this is the most novel
aspect of our argument.

6

MC

MC = RC

(cid:107)M(cid:107)∗≤

ρ


≤
(cid:105)

M
−
∗

M

 
C
Z
−
C
B
(cid:104)

˜M
M∗

B

BC−ZC

MC

C − MC(cid:105) ≤   which will contain ˜M.

Figure 2: Illustration of our Lagrangian duality argument  and of the role of Z. The blue region
represents the nuclear norm constraint and the gray region the remaining constraints. Where the blue
region slopes downwards  a decrease in MC can be offset by an increase in MC when measuring
(cid:104)B  M(cid:105). By linearizing the nuclear norm constraint  the vector B − Z accounts for this offset  and
the red region represents the constraint (cid:104)BC − ZC  M∗
Suppose that the constraint on (cid:107)M(cid:107)∗ were not present in (6). Then the adversaries would have no
inﬂuence on M∗
C   because all the remaining constraints in (6) are separable across rows. How can we
quantify the effect of this nuclear norm constraint? We exploit Lagrangian duality  which allows us to
replace constraints with appropriate modiﬁcations to the objective function.
To gain some intuition  consider Figure 2. The key is that the Lagrange multiplier ZC can bound the
amount that (cid:104)B  M(cid:105) can increase due to changing M outside of C. If we formalize this and analyze
Z in detail  we obtain the following result:

Lemma 2. Let m ≥ n. Then there is a k = O(cid:16) log3(2/δ)
1 − δ  there exists a matrix Z with rank(Z) = 1  (cid:107)Z(cid:107)F ≤ k(cid:112)αβn/m  and

such that  with probability at least

(cid:104)BC − ZC  M∗

C − MC(cid:105) ≤ (cid:104)B  M∗ − M(cid:105) for all M ∈ P.

(8)
By localizing (cid:104)B  M∗ − M(cid:105) to C via (8)  Lemma 2 bounds the effect that the adversaries can have
on ˜MC. It is therefore the key technical tool powering our results  and is proved in Section A.4.
Proposition 1 is proved from Lemmas 1 and 2 in Section A.5.

(cid:17)

αβ2

m
n

5 Recovering T (Algorithm 2)
In this section we show that if ˜M satisﬁes the conclusion of Proposition 1  then Algorithm 2 recovers
a set T that approximates T ∗ well. We represent the sets T and T ∗ as {0  1}-vectors  and use the

notation (cid:104)T  r(cid:105) to denote(cid:80)
Proposition 2. Suppose Assumption 1 holds. For some k0 = O(cid:16) log(2/αβδ)

j∈[m] Tjrj. Formally  we show the following:

  with probability 1− δ 

(cid:17)

β2

Algorithm 2 outputs a set T satisfying

(cid:104)T ∗ − T  r∗(cid:105) ≤ 2
|C|

(cid:16)(cid:88)

(cid:104)T ∗ − ˜Mi  r∗(cid:105)(cid:17)

i∈C

+ βm.

(9)

2  at least one of the 2 log(2/δ)

To establish Proposition 2  ﬁrst note that with probability 1− δ
randomly
selected i from Algorithm 2 will have cost (cid:104)T ∗ − ˜Mi  r∗(cid:105) within twice the average cost across i ∈ C.
This is because with probability α  a randomly selected i will lie in C  and with probability 1
2  an
i ∈ C will have cost at most twice the average cost (by Markov’s inequality).
The remainder of the proof hinges on two results. First  we establish a concentration bound showing
j for any ﬁxed i  and hence (by a union bound) also for
randomly selected i. This yields the following lemma  which is a straightforward

the 2 log(2/δ)
application of Bernstein’s inequality (see Section A.6 for a proof):

˜Mij ˜rj is close to k0
m

that(cid:80)

˜Mijr∗

(cid:80)

α

α

j

j

7

Lemma 3. Let i∗ be the row selected in Algorithm 2. Suppose that ˜r satisﬁes Assumption 1. For

some k0 = O(cid:16) log(2/αδ)

(cid:17)

β2

  with probability 1 − δ  we have

(cid:16)(cid:88)

(cid:104)T ∗ − ˜Mi  r∗(cid:105)(cid:17)

(cid:104)T ∗ − ˜Mi∗   r∗(cid:105) ≤ 2
|C|

i∈C

+


4

βm.

(10)

Having recovered a good row T0 = ˜Mi∗  we need to turn T0 into a binary vector so that Algorithm 2
can output a set; we do so via randomized rounding  obtaining a vector T ∈ {0  1}m such that E[T ] =
T0 (where the randomness is with respect to the choices made by the algorithm). Our rounding
procedure is given in Algorithm 4; the following lemma  proved in A.7  asserts its correctness:
Lemma 4. The output T of Algorithm 4 satisﬁes E[T ] = T0  (cid:107)T(cid:107)0 ≤ βm.
Algorithm 4 Randomized rounding algorithm.
1: procedure RANDOMIZEDROUND(T0)
2:
3:
4:
5:
6:
7:
8:
9: end procedure

Let s be the vector of partial sums of T0
Sample u ∼ Uniform([0  1]).
T ← [0  . . .   0] ∈ Rm
for z = 0 to βm − 1 do

Find j such that u + z ∈ [sj−1  sj)  and set Tj = 1. (cid:46) if no such j exists  skip this step

(cid:46) T0 ∈ [0  1]m satisﬁes (cid:107)T0(cid:107)1 ≤ βm
(cid:46) i.e.  sj = (T0)1 + ··· + (T0)j

end for
return T

The remainder of the proof involves lower-bounding the probability that T is accepted in each stage
of the while loop in Algorithm 2. We refer the reader to Section A.8 for details.

6 Open Directions and Related Work

αβ2

(cid:17)

α  β  and . It is tempting to hope that when m = n a tight result would have k = ˜O(cid:16) 1

Future Directions. On the theoretical side  perhaps the most immediate open question is whether it is
possible to improve the dependence of k (the amount of work required per worker) on the parameters
  in
loose analogy to recent results for the stochastic block model (Abbe and Sandon  2015b;a; Banks and
Moore  2016). For stochastic block models  there is conjectured to be a gap between computational
and information-theoretic thresholds  and it would be interesting to see if a similar phenomenon holds
here (the scaling for k given above is based on the conjectured computational threshold).
A second open question concerns the scaling in n: if n (cid:29) m  can we get by with much less work
per rater? Finally  it would be interesting to consider adaptivity: if the choice of queries is based on
previous worker ratings  can we reduce the amount of work?
Related work. Our setting is closely related to the problem of peer prediction (Miller et al.  2005) 
in which we wish to obtain truthful information from a population of raters by exploiting inter-rater
agreement. While several mechanisms have been proposed for these tasks  they typically assume that
rater accuracy is observable online (Resnick and Sami  2007)  that the dishonest raters are rational
agents maximizing a payoff function (Dasgupta and Ghosh  2013; Kamble et al.  2015; Shnayder
et al.  2016)  that the raters follow a simple statistical model (Karger et al.  2014; Zhang et al.  2014;
Zhou et al.  2015)  or some combination of the above (Shah and Zhou  2015; Shah et al.  2015).
Ghosh et al. (2011) allow o(n) adversaries to behave arbitrarily but require the rest to be stochastic.
The work closest to ours is Christiano (2014; 2016)  which studies online collaborative prediction in
the presence of adversaries; roughly  when raters interact with an item they predict its quality and
afterwards observe the actual quality; the goal is to minimize the number of incorrect predictions
among the honest raters. This differs from our setting in that (i) the raters are trying to learn the item
qualities as part of the task  and (ii) there is no requirement to induce a ﬁnal global estimate of the
high-quality items  which is necessary for estimating quantiles. It seems possible however that there
are theoretical ties between this setting and ours  which would be interesting to explore.
Acknowledgments. JS was supported by a Fannie & John Hertz Foundation Fellowship  an NSF Graduate
Research Fellowship  and a Future of Life Institute grant. GV was supported by NSF CAREER award CCF-
1351108  a Sloan Foundation Research Fellowship  and a research grant from the Okawa Foundation. MC was
supported by NSF grants CCF-1565581  CCF-1617577  CCF-1302518 and a Simons Investigator Award.

8

2016.

References
E. Abbe and C. Sandon. Community detection in general stochastic block models: fundamental limits and

efﬁcient recovery algorithms. arXiv  2015a.

E. Abbe and C. Sandon. Detection in the stochastic block model with multiple clusters: proof of the achievability

conjectures  acyclic BP  and the information-computation gap. arXiv  2015b.

N. Agarwal  A. S. Bandeira  K. Koiliaris  and A. Kolla. Multisection in the stochastic block model using

semideﬁnite programming. arXiv  2015.

J. Banks and C. Moore. Information-theoretic thresholds for community detection in sparse networks. arXiv 

T. T. Cai and X. Li. Robust and computationally feasible community detection in the presence of arbitrary outlier

nodes. The Annals of Statistics  43(3):1027–1059  2015.

Y. Chen  S. Sanghavi  and H. Xu. Improved graph clustering. IEEE Transactions on Information Theory  2014.
P. Chin  A. Rao  and V. Vu. Stochastic block model and community detection in the sparse graphs: A spectral

algorithm with optimal rate of recovery. In Conference on Learning Theory (COLT)  2015.

P. Christiano. Provably manipulation-resistant reputation systems. arXiv  2014.
P. Christiano. Robust collaborative online learning. arXiv  2016.
A. Coja-Oghlan. Coloring semirandom graphs optimally. Automata  Languages and Programming  2004.
A. Coja-Oghlan. Solving NP-hard semirandom graph problems in polynomial expected time. Journal of

A. Condon and R. M. Karp. Algorithms for graph partitioning on the planted partition model. Random Structures

Algorithms  62(1):19–46  2007.

and Algorithms  pages 116–140  2001.

A. Dasgupta and A. Ghosh. Crowdsourced judgement elicitation with endogenous proﬁciency. In WWW  2013.
A. Decelle  F. Krzakala  C. Moore  and L. Zdeborová. Asymptotic analysis of the stochastic block model for

modular networks and its algorithmic applications. Physical Review E  84(6)  2011.

J. Deng  W. Dong  R. Socher  L. Li  K. Li  and L. Fei-Fei. ImageNet: A large-scale hierarchical image database.

In Computer Vision and Pattern Recognition (CVPR)  pages 248–255  2009.

U. Feige and J. Kilian. Heuristics for semirandom graph problems. Journal of Computer and System Sciences 

U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semirandom graph. Random

Structures and Algorithms  16(2):195–208  2000.

A. Ghosh  S. Kale  and P. McAfee. Who moderates the moderators?: crowdsourcing abuse detection in

user-generated content. In 12th ACM conference on Electronic commerce  pages 167–176  2011.

O. Guédon and R. Vershynin. Community detection in sparse networks via Grothendieck’s inequality. arXiv 

63(4):639–671  2001.

2014.

A. Harmon. Amazon glitch unmasks war of reviewers. New York Times  2004.
P. W. Holland  K. B. Laskey  and S. Leinhardt. Stochastic blockmodels: Some ﬁrst steps. Social Networks  1983.
V. Kamble  N. Shah  D. Marn  A. Parekh  and K. Ramachandran. Truth serums for massively crowdsourced

evaluation tasks. arXiv  2015.

Research  62(1):1–24  2014.

D. R. Karger  S. Oh  and D. Shah. Budget-optimal task allocation for reliable crowdsourcing systems. Operations

M. Krivelevich and D. Vilenchik. Semirandom models as benchmarks for coloring algorithms. In Meeting on

Analytic Algorithmics and Combinatorics  pages 211–221  2006.

C. Kulkarni  P. W. Koh  H. Huy  D. Chia  K. Papadopoulos  J. Cheng  D. Koller  and S. R. Klemmer. Peer and

self assessment in massive online classes. Design Thinking Research  pages 131–168  2015.

C. M. Le  E. Levina  and R. Vershynin. Concentration and regularization of random graphs. arXiv  2015.
K. Makarychev  Y. Makarychev  and A. Vijayaraghavan. Approximation algorithms for semi-random partitioning

problems. In Symposium on Theory of Computing (STOC)  pages 367–384  2012.

K. Makarychev  Y. Makarychev  and A. Vijayaraghavan. Learning communities in the presence of errors. arXiv 

L. Massoulié. Community detection thresholds and the weak Ramanujan property. In STOC  2014.
D. Mayzlin  Y. Dover  and J. A. Chevalier. Promotional reviews: An empirical investigation of online review

manipulation. Technical report  National Bureau of Economic Research  2012.

N. Miller  P. Resnick  and R. Zeckhauser. Eliciting informative feedback: The peer-prediction method. Manage-

ment Science  51(9):1359–1373  2005.

A. Moitra  W. Perry  and A. S. Wein. How robust are reconstruction thresholds for community detection? arXiv 

models. arXiv  2013a.

E. Mossel  J. Neeman  and A. Sly. Stochastic block models and reconstruction. arXiv  2012.
E. Mossel  J. Neeman  and A. Sly. Belief propagation  robust reconstruction  and optimal recovery of block

E. Mossel  J. Neeman  and A. Sly. A proof of the block model threshold conjecture. arXiv  2013b.
E. Mossel  J. Neeman  and A. Sly. Consistency thresholds for the planted bisection model. In STOC  2015.
C. Piech  J. Huang  Z. Chen  C. Do  A. Ng  and D. Koller. Tuned models of peer assessment in MOOCs. arXiv 

2015.

2015.

2013.

R. Priedhorsky  J. Chen  S. T. K. Lam  K. Panciera  L. Terveen  and J. Riedl. Creating  destroying  and restoring

value in Wikipedia. In International ACM Conference on Supporting Group Work  pages 259–268  2007.

P. Resnick and R. Sami. The inﬂuence limiter: provably manipulation-resistant recommender systems. In ACM

Conference on Recommender Systems  pages 25–32  2007.

N. Shah  D. Zhou  and Y. Peres. Approval voting and incentives in crowdsourcing. In ICML  2015.
N. B. Shah and D. Zhou. Double or nothing: Multiplicative incentive mechanisms for crowdsourcing. In

Advances in Neural Information Processing Systems (NIPS)  2015.

V. Shnayder  R. Frongillo  A. Agarwal  and D. C. Parkes. Strong truthfulness in multi-task peer prediction  2016.
J. Vuurens  A. P. de Vries  and C. Eickhoff. How much spam can you take? An analysis of crowdsourcing results

to increase accuracy. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval  2011.

Y. Zhang  X. Chen  D. Zhou  and M. I. Jordan. Spectral methods meet EM: A provably optimal algorithm for

D. Zhou  Q. Liu  J. C. Platt  C. Meek  and N. B. Shah. Regularized minimax conditional entropy for crowdsourc-

crowdsourcing. arXiv  2014.

ing. arXiv  2015.

9

,Jacob Steinhardt
Gregory Valiant
Moses Charikar