2011,Divide-and-Conquer Matrix Factorization,This work introduces Divide-Factor-Combine (DFC)  a parallel divide-and-conquer framework for noisy matrix factorization.  DFC divides a large-scale matrix factorization task into smaller subproblems  solves each subproblem in parallel using an arbitrary base matrix factorization algorithm  and combines the subproblem solutions using techniques from randomized matrix approximation.  Our experiments with collaborative filtering  video background modeling  and simulated data demonstrate the near-linear to super-linear speed-ups attainable with this approach.  Moreover  our analysis shows that DFC enjoys high-probability recovery guarantees comparable to those of its base algorithm.,Divide-and-Conquer Matrix Factorization

Michael I. Jordana  b
Lester Mackeya
a Department of Electrical Engineering and Computer Science  UC Berkeley

Ameet Talwalkara

b Department of Statistics  UC Berkeley

Abstract

This work introduces Divide-Factor-Combine (DFC)  a parallel divide-and-
conquer framework for noisy matrix factorization. DFC divides a large-scale
matrix factorization task into smaller subproblems  solves each subproblem in par-
allel using an arbitrary base matrix factorization algorithm  and combines the sub-
problem solutions using techniques from randomized matrix approximation. Our
experiments with collaborative ﬁltering  video background modeling  and simu-
lated data demonstrate the near-linear to super-linear speed-ups attainable with
this approach. Moreover  our analysis shows that DFC enjoys high-probability
recovery guarantees comparable to those of its base algorithm.

1 Introduction
The goal in matrix factorization is to recover a low-rank matrix from irrelevant noise and corrup-
tion. We focus on two instances of the problem: noisy matrix completion  i.e.  recovering a low-rank
matrix from a small subset of noisy entries  and noisy robust matrix factorization [2  3  4]  i.e.  re-
covering a low-rank matrix from corruption by noise and outliers of arbitrary magnitude. Examples
of the matrix completion problem include collaborative ﬁltering for recommender systems  link pre-
diction for social networks  and click prediction for web search  while applications of robust matrix
factorization arise in video surveillance [2]  graphical model selection [4]  document modeling [17] 
and image alignment [21].
These two classes of matrix factorization problems have attracted signiﬁcant interest in the research
community. In particular  convex formulations of noisy matrix factorization have been shown to ad-
mit strong theoretical recovery guarantees [1  2  3  20]  and a variety of algorithms (e.g.  [15  16  23])
have been developed for solving both matrix completion and robust matrix factorization via convex
relaxation. Unfortunately  these methods are inherently sequential and all rely on the repeated and
costly computation of truncated SVDs  factors that limit the scalability of the algorithms.
To improve scalability and leverage the growing availability of parallel computing architectures  we
propose a divide-and-conquer framework for large-scale matrix factorization. Our framework  en-
titled Divide-Factor-Combine (DFC)  randomly divides the original matrix factorization task into
cheaper subproblems  solves those subproblems in parallel using any base matrix factorization al-
gorithm  and combines the solutions to the subproblem using efﬁcient techniques from randomized
matrix approximation. The inherent parallelism of DFC allows for near-linear to superlinear speed-
ups in practice  while our theory provides high-probabilityrecovery guarantees for DFC comparable
to those enjoyed by its base algorithm.
The remainder of the paper is organized as follows. In Section 2  we deﬁne the setting of noisy ma-
trix factorization and introduce the components of the DFC framework. To illustrate the signiﬁcant
speed-up and robustness of DFC and to highlight the effectiveness of DFC ensembling  we present
experimental results on collaborative ﬁltering  video background modeling  and simulated data in
Section 3. Our theoretical analysis follows in Section 4. There  we establish high-probability noisy
recovery guarantees for DFC that rest upon a novel analysis of randomized matrix approximation
and a new recovery result for noisy matrix completion.

1

M U"

Notation For M ∈ Rm×n  we deﬁne M(i) as the ith row vector and Mij as the ijth en-
try.
If rank(M) = r  we write the compact singular value decomposition (SVD) of M as
M  where ΣM is diagonal and contains the r non-zero singular values of M  and
UM ΣM V"
UM ∈ Rm×r and VM ∈ Rn×r are the corresponding left and right singular vectors of M. We
deﬁne M+ = VM Σ−1
M as the Moore-Penrose pseudoinverse of M and PM = MM+ as the
orthogonal projection onto the column space of M. We let "·"2  "·"F  and "·"∗ respectively denote
the spectral  Frobenius  and nuclear norms of a matrix and let "·" represent the !2 norm of a vector.
2 The Divide-Factor-Combine Framework
In this section  we present our divide-and-conquerframework for scalable noisy matrix factorization.
We begin by deﬁning the problem setting of interest.

2.1 Noisy Matrix Factorization (MF)
In the setting of noisy matrix factorization  we observe a subset of the entries of a matrix M =
L0 + S0 + Z0 ∈ Rm×n  where L0 has rank r # m  n  S0 represents a sparse matrix of outliers of
arbitrary magnitude  and Z0 is a dense noise matrix. We let Ω represent the locations of the observed
entries and PΩ be the orthogonal projection onto the space of m × n matrices with support Ω  so
that
Our goal is to recover the low-rank matrix L0 from PΩ(M) with error proportional to the noise level
∆ ! "Z0"F. We will focus on two speciﬁc instances of this general problem:

(PΩ(M))ij = Mij  if (i  j) ∈ Ω and (PΩ(M))ij = 0 otherwise.

• Noisy Matrix Completion (MC): s ! |Ω| entries of M are revealed uniformly without
replacement  along with their locations. There are no outliers  so that S0 is identically zero.
• Noisy Robust Matrix Factorization (RMF): S0 is identically zero save for s outlier en-
tries of arbitrary magnitude with unknown locations distributed uniformly without replace-
ment. All entries of M are observed  so that PΩ(M) = M.

2.2 Divide-Factor-Combine
Algorithms 1 and 2 summarize two canonical examples of the general Divide-Factor-Combine
framework that we refer to as DFC-PROJ and DFC-NYS. Each algorithm has three simple steps:
(D step) Divide input matrix into submatrices: DFC-PROJ randomly partitions PΩ(M) into t l-
column submatrices  {PΩ(C1)  . . .  PΩ(Ct)}1  while DFC-NYS selects an l-column sub-
matrix  PΩ(C)  and a d-row submatrix  PΩ(R)  uniformly at random.
(F step) Factor each submatrix in parallel using any base MF algorithm: DFC-PROJ performs
t parallel submatrix factorizations  while DFC-NYS performs two such parallel factoriza-
tions. Standard base MF algorithms output the low-rank approximations { ˆC1  . . .   ˆCt} for
DFC-PROJ and ˆC  and ˆR for DFC-NYS. All matrices are retained in factored form.
(C step) Combine submatrix estimates: DFC-PROJ generates a ﬁnal low-rank estimate ˆLproj by
projecting [ ˆC1  . . .   ˆCt] onto the column space of ˆC1  while DFC-NYS forms the low-
rank estimate ˆLnys from ˆC and ˆR via the generalized Nystr¨om method. These matrix
approximation techniques are described in more detail in Section 2.3.

2.3 Randomized Matrix Approximations
Our divide-and-conqueralgorithms rely on two methods that generate randomized low-rank approx-
imations to an arbitrary matrix M from submatrices of M.

1For ease of discussion  we assume that mod(n  t) = 0  and hence  l = n/t. Note that for arbitrary n and

t  PΩ(M) can always be partitioned into t submatrices  each with either !n/t" or #n/t$ columns.

2

Algorithm 1 DFC-PROJ
Input: PΩ(M)  t
{PΩ(Ci)}1≤i≤t = SAMPCOL(PΩ(M)  t)
do in parallel

ˆC1 = BASE-MF-ALG(PΩ(C1))
ˆCt = BASE-MF-ALG(PΩ(Ct))

end do
ˆLproj = COLPROJECTION( ˆC1  . . .   ˆCt)

...

Algorithm 2 DFC-NYSa
Input: PΩ(M)  l  d
PΩ(C)  PΩ(R) = SAMPCOLROW(PΩ(M)  l  d)
do in parallel

ˆC = BASE-MF-ALG(PΩ(C))
ˆR = BASE-MF-ALG(PΩ(R))

end do
ˆLnys = GENNYSTR¨OM ( ˆC  ˆR)
aWhen Q is a submatrix of M we abuse notation and
deﬁne PΩ(Q) as the corresponding submatrix of PΩ(M).

Column Projection This approximation  introduced by Frieze et al. [7]  is derived from column
sampling of M. We begin by sampling l < n columns uniformly without replacement and let C
be the m × l matrix of sampled columns. Then  column projection uses C to generate a “matrix
projection” approximation [13] of M as follows:

Lproj = CC+M = UC U"

CM.

CM.
In practice  we do not reconstruct Lproj but rather maintain low-rank factors  e.g.  UC and U"
Generalized Nystr¨om Method The standard Nystr¨om method is often used to speed up large-
scale learning applications involving symmetric positive semideﬁnite (SPSD) matrices [24] and has
been generalized for arbitrary real-valued matrices [8].
In particular  after sampling columns to
obtain C  imagine that we independently sample d < m rows uniformly without replacement. Let
R be the d × n matrix of sampled rows and W be the d × l matrix formed from the intersection
of the sampled rows and columns. Then  the generalized Nystr¨om method uses C  W  and R to
compute an “spectral reconstruction” approximation [13] of M as follows:

W U"

W R.

W and U"

Lnys = CW+R = CVW Σ+

W R .
As with Mproj  we store low-rank factors of Lnys  such as CVW Σ+
2.4 Running Time of DFC
Many state-of-the-art MF algorithms have Ω(mnkM ) per-iteration time complexity due to the rank-
kM truncated SVD performed on each iteration. DFC signiﬁcantly reduces the per-iteration com-
plexity to O(mlkCi) time for Ci (or C) and O(ndkR) time for R. The cost of combining the
submatrix estimates is even smaller  since the outputs of standard MF algorithms are returned in fac-
tored form. Indeed  the column projection step of DFC-PROJ requires only O(mk2 + lk2) time for
k ! maxi kCi: O(mk2 + lk2) time for the pseudoinversion of ˆC1 and O(mk2 + lk2) time for ma-
trix multiplication with each ˆCi in parallel. Similarly  the generalized Nystr¨om step of DFC-NYS
requires only O(l¯k2 + d¯k2 + min(m  n)¯k2) time  where ¯k ! max(kC  kR). Hence  DFC divides
the expensive task of matrix factorization into smaller subproblems that can be executed in parallel
and efﬁciently combines the low-rank  factored results.

2.5 Ensemble Methods
Ensemble methods have been shown to improve performance of matrix approximation algorithms 
while straightforwardly leveraging the parallelism of modern many-core and distributed architec-
tures [14]. As such  we propose ensemble variants of the DFC algorithms that demonstrably reduce
recovery error while introducing a negligible cost to the parallel running time. For DFC-PROJ-
ENS  rather than projecting only onto the column space of ˆC1  we project [ ˆC1  . . .   ˆCt] onto the
column space of each ˆCi in parallel and then average the t resulting low-rank approximations. For
DFC-NYS-ENS  we choose a random d-row submatrix PΩ(R) as in DFC-NYS and independently
partition the columns of PΩ(M) into {PΩ(C1)  . . .  PΩ(Ct)} as in DFC-PROJ. After running the

3

base MF algorithm on each submatrix  we apply the generalized Nystr¨om method to each ( ˆCi  ˆR)
pair in parallel and average the t resulting low-rank approximations. Section 3 highlights the empir-
ical effectiveness of ensembling.
3 Experimental Evaluation
We nowexplore the accuracyand speed-up of DFC on a variety of simulated and real-world datasets.
We use state-of-the-artmatrix factorization algorithms in our experiments: the Accelerated Proximal
Gradient (APG) algorithm of [23] as our base noisy MC algorithm and the APG algorithm of [15] as
our base noisy RMF algorithm. In all experiments  we use the default parameter settings suggested
by [23] and [15]  measure recovery error via root mean square error (RMSE)  and report parallel
running times for DFC. We moreover compare against two baseline methods: APG used on the full
matrix M and PARTITION  which performs matrix factorization on t submatrices just like DFC-
PROJ but omits the ﬁnal column projection step.
3.1 Simulations
For our simulations  we focused on square matrices (m = n) and generated random low-rank and
sparse decompositions  similar to the schemes used in related work  e.g.  [2  12  25]. We created
L0 ∈ Rm×m as a random product  AB"  where A and B are m × r matrices with indepen-
dent N (0 !1/r) entries such that each entry of L0 has unit variance. Z0 contained independent
N (0  0.1) entries. In the MC setting  s entries of L0 + Z0 were revealed uniformly at random. In
the RMF setting  the support of S0 was generated uniformly at random  and the s corrupted entries
took values in [0  1] with uniform probability. For each algorithm  we report error between L0 and
the recovered low-rank matrix  and all reported results are averages over ﬁve trials.

0.25

0.2

0.15

0.1

0.05

E
S
M
R

0
 
0

2

MC

 

Part−10%
Proj−10%
Nys−10%
Proj−Ens−10%
Nys−Ens−10%
Proj−Ens−25%
Base−MC

4

% revealed entries

6

8

10

E
S
M
R

0.25

0.2

0.15

0.1

0.05

0
 
0

RMF

 

Part−10%
Proj−10%
Nys−10%
Proj−Ens−10%
Nys−Ens−10%
Base−RMF
50
60

70

10

20

30
40
% of outliers

Figure 1: Recovery error of DFC relative to base algorithms.

We ﬁrst explored the recovery error of DFC as a function of s  using (m = 10K  r = 10) with
varying observation sparsity for MC and (m = 1K  r = 10) with a varying percentage of outliers
for RMF. The results are summarized in Figure 1.2 In both MC and RMF  the gaps in recovery
between APG and DFC are small when sampling only 10% of rows and columns. Moreover  DFC-
PROJ-ENS in particular consistently outperforms PARTITION and DFC-NYS-ENS and matches the
performance of APG for most settings of s.
We next explored the speed-up of DFC as a function of matrix size. For MC  we revealed 4% of
the matrix entries and set r = 0.001 · m  while for RMF we ﬁxed the percentage of outliers to 10%
and set r = 0.01 · m. We sampled 10% of rows and columns and observed that recovery errors
were comparable to the errors presented in Figure 1 for similar settings of s; in particular  at all
values of n for both MC and RMF  the errors of APG and DFC-PROJ-ENS were nearly identical.
Our timing results  presented in Figure 2  illustrate a near-linear speed-up for MC and a superlinear
speed-up for RMF across varying matrix sizes. Note that the timing curves of the DFC algorithms
and PARTITION all overlap  a fact that highlights the minimal computational cost of the ﬁnal matrix
approximation step.

2In the left-hand plot of Figure 1  the lines for Proj-10% and Proj-Ens-10% overlap.

4

3000

2500

2000

1500

1000

500

)
s
(
 
e
m

i
t

0
 
1.5

2

MC

Part−10%
Proj−10%
Nys−10%
Proj−Ens−10%
Nys−Ens−10%
Base−RMF

 

10000

RMF

)
s
(
 
e
m

i
t

8000

6000

4000

2000

 

Part−10%
Proj−10%
Nys−10%
Proj−Ens−10%
Nys−Ens−10%
Base−RMF

2.5

3

0

 

3.5

4

m

3000
m
Figure 2: Speed-up of DFC relative to base algorithms.

1000

2000

4.5

5
x 104

4000

5000

3.2 Collaborative Filtering
Collaborative ﬁltering for recommender systems is one prevalent real-world application of noisy
matrix completion. A collaborative ﬁltering dataset can be interpreted as the incomplete observation
of a ratings matrix with columns corresponding to users and rows corresponding to items. The goal
is to infer the unobserved entries of this ratings matrix. We evaluate DFC on two of the largest
publicly available collaborative ﬁltering datasets: MovieLens 10M3 (m = 4K  n = 6K  s > 10M)
and the Netﬂix Prize dataset4 (m = 18K  n = 480K  s > 100M). To generate test sets drawn
from the training distribution  for each dataset  we aggregated all available rating data into a single
training set and withheld test entries uniformly at random  while ensuring that at least one training
observation remained in each row and column. The algorithms were then run on the remaining
training portions and evaluated on the test portions of each split. The results  averaged over three
train-test splits  are summarized in Table 3.2. Notably  DFC-PROJ  DFC-PROJ-ENS  and DFC-
NYS-ENS all outperform PARTITION  and DFC-PROJ-ENS performs comparably to APG while
providing a nearly linear parallel time speed-up. The poorer performance of DFC-NYS can be in
part explained by the asymmetry of these problems. Since these matrices have many more columns
than rows  MF on column submatrices is inherently easier than MF on row submatrices  and for
DFC-NYS  we observe that ˆC is an accurate estimate while ˆR is not.

Table 1: Performance of DFC relative to APG on collaborative ﬁltering tasks.

MovieLens 10M
Method
RMSE
Time
APG
0.8005
294.3s
PARTITION-25%
77.4s
0.8146
36.0s
PARTITION-10%
0.8461
DFC-NYS-25%
0.8449
77.2s
53.4s
DFC-NYS-10%
0.8769
84.5s
DFC-NYS-ENS-25% 0.8085
63.9s
DFC-NYS-ENS-10% 0.8327
DFC-PROJ-25%
0.8061
77.4s
36.1s
DFC-PROJ-10%
0.8272
DFC-PROJ-ENS-25% 0.7944
77.4s
DFC-PROJ-ENS-10% 0.8119
36.1s

Netﬂix

RMSE
0.8433
0.8451
0.8492
0.8832
0.9224
0.8486
0.8613
0.8436
0.8484
0.8411
0.8433

Time
2653.1s
689.1s
289.2s
890.9s
487.6s
964.3s
546.2s
689.5s
289.7s
689.5s
289.7s

3.3 Background Modeling
Background modeling has important practical ramiﬁcations for detecting activity in surveillance
video. This problem can be framed as an application of noisy RMF  where each video frame is
a column of some matrix (M)  the background model is low-rank (L0)  and moving objects and

3http://www.grouplens.org/
4http://www.netflixprize.com/

5

background variations  e.g.  changes in illumination  are outliers (S0). We evaluate DFC on two
videos: ‘Hall’ (200 frames of size 176 × 144) contains signiﬁcant foreground variation and was
studied by [2]  while ‘Lobby’ (1546 frames of size 168×120)includes many changes in illumination
(a smaller video with 250 frames was studied by [2]). We focused on DFC-PROJ-ENS  due to its
superior performance in previous experiments  and measured the RMSE between the background
model recovered by DFC and that of APG. On both videos  DFC-PROJ-ENS recovered nearly the
same background model as the full APG algorithm in a small fraction of the time. On ‘Hall ’ the
DFC-PROJ-ENS-5% and DFC-PROJ-ENS-0.5% models exhibited RMSEs of 0.564 and 1.55  quite
small given pixels with 256 intensity values. The associated runtime was reduced from 342.5s for
APG to real-time (5.2s for a 13s video) for DFC-PROJ-ENS-0.5%. Snapshots of the results are
presented in Figure 3. On ‘Lobby ’ the RMSE of DFC-PROJ-ENS-4% was 0.64  and the speed-up
over APG was more than 20X  i.e.  the runtime reduced from 16557s to 792s.

Original frame

APG
(342.5s)

5% sampled

(24.2s)

0.5% sampled

(5.2s)

Figure 3: Sample ‘Hall’ recovery by APG  DFC-PROJ-ENS-5%  and DFC-PROJ-ENS-.5%.

4 Theoretical Analysis
Having investigated the empirical advantages of DFC  we now show that DFC admits high-
probability recovery guarantees comparable to those of its base algorithm.
4.1 Matrix Coherence
Since not all matrices can be recovered from missing entries or gross outliers  recent theoretical
advances have studied sufﬁcient conditions for accurate noisy MC [3  12  20] and RMF [1  25].
Most prevalent among these are matrix coherence conditions  which limit the extent to which the
singular vectors of a matrix are correlated with the standard basis. Letting ei be the ith column of
the standard basis  we deﬁne two standard notions of coherence [22]:
Deﬁnition 1 (µ0-Coherence). Let V ∈ Rn×r contain orthonormal columns with r ≤ n. Then the
µ0-coherence of V is:

Deﬁnition 2 (µ1-Coherence). Let L ∈ Rm×n have rank r. Then  the µ1-coherence of L is:

µ0(V) ! n

r max1≤i≤n $PV ei$2 = n
µ1(L) !! mn

r maxij |e#

r max1≤i≤n $V(i)$2 .

i ULV#

L ej| .

For any µ > 0  we will call a matrix L (µ  r)-coherent if rank(L) = r  max(µ0(UL)  µ0(VL)) ≤
µ  and µ1(L) ≤ √µ. Our analysis will focus on base MC and RMF algorithms that express their
recovery guarantees in terms of the (µ  r)-coherence of the target low-rank matrix L0. For such
algorithms  lower values of µ correspond to better recovery properties.
4.2 DFC Master Theorem
We now show that the same coherence conditions that allow for accurate MC and RMF also imply
high-probability recovery for DFC. To make this precise  we let M = L0 + S0 + Z0 ∈ Rm×n 
where L0 is (µ  r)-coherent and $PΩ(Z0)$F ≤ ∆. We further ﬁx any   δ ∈ (0  1] and deﬁne A(X)
1−/2   r)-coherent. Then  our Thm. 3 provides a generic recovery
as the event that a matrix X is ( rµ2
bound for DFC when used in combination with an arbitrary base algorithm. The proof requires a
novel  coherence-based analysis of column projection and random column sampling. These results
of independent interest are presented in Appendix A.

6

Theorem 3. Choose t = n/l and l ≥ crµ log(n) log(2/δ)/2  where c is a ﬁxed positive con-
stant  and ﬁx any ce ≥ 0. Under the notation of Algorithm 1  if a base MF algorithm yields
P!"C0 i − ˆCi"F > ce√ml∆ | A(C0 i)" ≤ δC for each i  where C0 i is the corresponding parti-
tion of L0  then  with probability at least (1 − δ)(1 − tδC)  DFC-PROJ guarantees
Under Algorithm 2  if a base MF algorithm yields P!"C0 − ˆC"F > ce√ml∆ | A(C)" ≤ δC
and P!"R0 − ˆR"F > ce√dn∆ | A(R)" ≤ δR for d ≥ clµ0( ˆC) log(m) log(1/δ)/2  then  with
probability at least (1 − δ)2(1 − δC − δR)  DFC-NYS guarantees

"L0 − ˆLproj"F ≤ (2 + )ce√mn∆.

"L0 − ˆLnys"F ≤ (2 + 3)ce√ml + dn∆.

To understand the conclusions of Thm. 3  consider a typical base algorithm which  when applied to
PΩ(M)  recovers an estimate ˆL satisfying "L0 − ˆL"F ≤ ce√mn∆ with high probability. Thm. 3
asserts that  with appropriately reduced probability  DFC-PROJ exhibits the same recovery error
scaled by an adjustable factor of 2 +   while DFC-NYS exhibits a somewhat smaller error scaled by
2+3.5 The key take-away then is that DFC introducesa controlled increase in error and a controlled
decrement in the probability of success  allowing the user to interpolate between maximum speed
and maximum accuracy. Thus  DFC can quickly provide near-optimal recovery in the noisy setting
and exact recovery in the noiseless setting (∆= 0)   even when entries are missing or grossly
corrupted. The next two sections demonstrate how Thm. 3 can be applied to derive speciﬁc DFC
recovery guarantees for noisy MC and noisy RMF. In these sections  we let ¯n ! max(m  n).
4.3 Consequences for Noisy MC
Our ﬁrst corollary of Thm. 3 shows that DFC retains the high-probability recovery guarantees of a
standard MC solver while operating on matrices of much smaller dimension. Suppose that a base
MC algorithm solves the following convex optimization problem  studied in [3]:
subject to "PΩ(M − L)"F ≤ ∆.

Then  Cor. 4 follows from a novel guarantee for noisy convex MC  proved in the appendix.
Corollary 4. Suppose that L0 is (µ  r)-coherent and that s entries of M are observed  with locations
Ω distributed uniformly. Deﬁne the oversampling parameter

minimizeL "L"∗

βs !

s(1 − /2)

32µ2r2(m + n) log2(m + n)

 

βs

βs

2

βs

and ﬁx any target rate parameter 1 <β ≤ βs. Then  if "PΩ(M) −P Ω(L0)"F ≤ ∆ a.s.  it sufﬁces
to choose t = n/l and
l ≥ max# nβ
to achieve

+$ n(β−1)
DFC-PROJ: "L0 − ˆLproj"F ≤ (2 + )c#
DFC-NYS: "L0 − ˆLnys"F ≤ (2 + 3)c#

d ≥ max# mβ
e√mn∆
e√ml + dn∆

  crµ log(n) log(2/δ)

+$ m(β−1)

βs

  clµ0( ˆC) log(m) log(1/δ)

2

% 

%

with probability at least

DFC-PROJ: (1 − δ)(1 − 5t log(¯n)¯n2−2β) ≥ (1 − δ)(1 − ¯n3−2β)
DFC-NYS: (1 − δ)2(1 − 10 log(¯n)¯n2−2β) 
e a positive constant.

respectively  with c as in Thm. 3 and c#

5Note that the DFC-NYS guarantee requires the number of rows sampled to grow in proportion to µ0( ˆC) 

a quantity always bounded by µ in our simulations.

7

m log2(m + n)) sampled columns and O( m

Notably  Cor. 4 allows for the fraction of columns and rows sampled to decrease as the oversampling
parameter βs increases with m and n. In the best case  βs =Θ( mn/[(m + n) log2(m + n)])  and
n log2(m + n)) sampled rows. In
Cor. 4 requires only O( n
the worst case  βs =Θ(1)   and Cor. 4 requires the number of sampled columns and rows to grow
linearly with the matrix dimensions. As a more realistic intermediate scenario  consider the setting
in which βs =Θ( √m + n) and thus a vanishing fraction of entries are revealed. In this setting 
only O(√m + n) columns and rows are required by Cor. 4.
4.4 Consequences for Noisy RMF
Our next corollary shows that DFC retains the high-probability recovery guarantees of a standard
RMF solver while operating on matrices of much smaller dimension. Suppose that a base RMF
algorithm solves the following convex optimization problem  studied in [25]:

minimizeL S

"L"∗ + λ"S"1

subject to "M − L − S"F ≤ ∆ 

with λ = 1/√¯n. Then  Cor. 5 follows from Thm. 3 and the noisy RMF guarantee of [25  Thm. 2].
Corollary 5. Suppose that L0 is (µ  r)-coherent and that the uniformly distributed support set of
S0 has cardinality s. For a ﬁxed positive constant ρs  deﬁne the undersampling parameter

βs !!1 −

s

mn"/ρs 

l ≥ max# r2µ2 log2(¯n)
(1 − /2)ρr
d ≥ max# r2µ2 log2(¯n)
(1 − /2)ρr

and ﬁx any target rate parameter β> 2 with rescaling β" ! β log(¯n)/ log(m) satisfying 4βs −
3/ρs ≤ β" ≤ βs. Then  if "M − L0 − S0"F ≤ ∆ a.s.  it sufﬁces to choose t = n/l and
  crµ log(n) log(2/δ)/2$
  clµ0( ˆC) log(m) log(1/δ)/2$
e√mn∆
e√ml + dn∆

DFC-PROJ: "L0 − ˆLproj"F ≤ (2 + )c""
DFC-NYS: "L0 − ˆLnys"F ≤ (2 + 3)c""

4 log(¯n)β(1 − ρsβs)
m(ρsβs − ρsβ")2
4 log(¯n)β(1 − ρsβs)
n(ρsβs − ρsβ")2

to have

 

 

with probability at least

DFC-PROJ: (1 − δ)(1 − tcp¯n−β) ≥ (1 − δ)(1 − cp¯n1−β)
DFC-NYS: (1 − δ)2(1 − 2cp¯n−β) 

e   and cp positive constants.

respectively  with c as in Thm. 3 and ρr  c""
Note that Cor. 5 places only very mild restrictions on the number of columns and rows to be sampled.
Indeed  l and d need only grow poly-logarithmically in the matrix dimensions to achieve high-
probability noisy recovery.
5 Conclusions
To improve the scalability of existing matrix factorization algorithms while leveraging the ubiquity
of parallel computing architectures  we introduced  evaluated  and analyzed DFC  a divide-and-
conquer framework for noisy matrix factorization with missing entries or outliers. We note that the
contemporaneous work of [19] addresses the computational burden of noiseless RMF by reformu-
lating a standard convex optimization problem to internally incorporate random projections. The
differences between DFC and the approach of [19] highlight some of the main advantages of this
work: i) DFC can be used in combination with any underlying MF algorithm  ii) DFC is trivially
parallelized  and iii) DFC provably maintains the recovery guarantees of its base algorithm  even in
the presence of noise.

8

References
[1] A. Agarwal  S. Negahban  and M. J. Wainwright. Noisy matrix decomposition via convex relaxation:

Optimal rates in high dimensions. In International Conference on Machine Learning  2011.

[2] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? Journal of the ACM  58

(3):1–37  2011.

[3] E.J. Cand`es and Y. Plan. Matrix completion with noise. Proceedings of the IEEE  98(6):925 –936  2010.
[4] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Sparse and low-rank matrix decompo-

sitions. In Allerton Conference on Communication  Control  and Computing  2009.

[5] Y. Chen  H. Xu  C. Caramanis  and S. Sanghavi. Robust matrix completion and corrupted columns. In

International Conference on Machine Learning  2011.

[6] P. Drineas  M. W. Mahoney  and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM

Journal on Matrix Analysis and Applications  30:844–881  2008.

[7] A. Frieze  R. Kannan  and S. Vempala. Fast Monte-Carlo algorithms for ﬁnding low-rank approximations.

In Foundations of Computer Science  1998.

[8] S. A. Goreinov  E. E. Tyrtyshnikov  and N. L. Zamarashkin. A theory of pseudoskeleton approximations.

Linear Algebra and its Applications  261(1-3):1 – 21  1997.

[9] D. Gross and V. Nesme. Note on sampling without replacing from a ﬁnite collection of matrices. CoRR 

abs/1001.2738  2010.

[10] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American

Statistical Association  58(301):13–30  1963.

[11] D. Hsu  S. M. Kakade  and T. Zhang. Dimension-free tail inequalities for sums of random matrices.

arXiv:1104.1672v3[math.PR]  2011.

[12] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from noisy entries. Journal of Machine

Learning Research  99:2057–2078  2010.

[13] S. Kumar  M. Mohri  and A. Talwalkar. On sampling-based approximate spectral decomposition.

International Conference on Machine Learning  2009.

In

[14] S. Kumar  M. Mohri  and A. Talwalkar. Ensemble Nystr¨om method. In NIPS  2009.
[15] Z. Lin  A. Ganesh  J. Wright  L. Wu  M. Chen  and Y. Ma. Fast convex optimization algorithms for exact

recovery of a corrupted low-rank matrix. UIUC Technical Report UILU-ENG-09-2214  2009.

[16] S. Ma  D. Goldfarb  and L. Chen. Fixed point and bregman iterative methods for matrix rank minimiza-

tion. Mathematical Programming  128(1-2):321–353  2011.

[17] K. Min  Z. Zhang  J. Wright  and Y. Ma. Decomposing background topics from keywords by principal

component pursuit. In Conference on Information and Knowledge Management  2010.

[18] M. Mohri and A. Talwalkar. Can matrix coherence be efﬁciently and accurately estimated? In Conference

on Artiﬁcial Intelligence and Statistics  2011.

[19] Y. Mu  J. Dong  X. Yuan  and S. Yan. Accelerated low-rank visual recovery by random projection. In

Conference on Computer Vision and Pattern Recognition  2011.

[20] S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal

bounds with noise. arXiv:1009.2118v2[cs.IT]  2010.

[21] Y. Peng  A. Ganesh  J. Wright  W. Xu  and Y. Ma. Rasl: Robust alignment by sparse and low-rank
decomposition for linearly correlated images. In Conference on Computer Vision and Pattern Recognition 
2010.

[22] B. Recht. A simpler approach to matrix completion. arXiv:0910.0651v2[cs.IT]  2009.
[23] K. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least squares

problems. Paciﬁc Journal of Optimization  6(3):615–640  2010.

[24] C.K. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In NIPS  2000.
[25] Z. Zhou  X. Li  J. Wright  E. J. Cand`es  and Y. Ma. Stable principal component pursuit. arXiv:

1001.2363v1[cs.IT]  2010.

9

,Kumar Avinava Dubey
Qirong Ho
Sinead Williamson
Eric Xing