2014,Distance-Based Network Recovery under Feature Correlation,We present an inference method for Gaussian graphical models when only pairwise distances of n objects are observed. Formally  this is a problem of estimating an n x n covariance matrix from the Mahalanobis distances dMH(xi  xj)  where object xi lives in a latent feature space. We solve the problem in fully Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-Gamma prior; the resulting Matrix-T posterior enables network recovery even under strongly correlated features. Hereby  we generalize TiWnet  which assumes Euclidean distances with strict feature independence. In spite of the greatly increased flexibility  our model neither loses statistical power nor entails more computational cost. We argue that the extension is highly relevant as it yields significantly better results in both synthetic and real-world experiments  which is successfully demonstrated for a network of biological pathways in cancer patients.,Distance-Based Network Recovery

under Feature Correlation

David Adametz  Volker Roth

Department of Mathematics and Computer Science
{david.adametz volker.roth}@unibas.ch

University of Basel  Switzerland

Abstract

We present an inference method for Gaussian graphical models when only pair-
wise distances of n objects are observed. Formally  this is a problem of esti-
mating an n × n covariance matrix from the Mahalanobis distances dMH(xi  xj) 
where object xi lives in a latent feature space. We solve the problem in fully
Bayesian fashion by integrating over the Matrix-Normal likelihood and a Matrix-
Gamma prior; the resulting Matrix-T posterior enables network recovery even
under strongly correlated features. Hereby  we generalize TiWnet [19]  which as-
sumes Euclidean distances with strict feature independence. In spite of the greatly
increased ﬂexibility  our model neither loses statistical power nor entails more
computational cost. We argue that the extension is highly relevant as it yields
signiﬁcantly better results in both synthetic and real-world experiments  which is
successfully demonstrated for a network of biological pathways in cancer patients.

1

Introduction

In this paper we introduce the Translation-invariant Matrix-T process (TiMT) for estimating Gaus-
sian graphical models (GGMs) from pairwise distances. The setup is particularly interesting  as
many applications only allow distances to be observed in the ﬁrst place. Hence  our approach is
capable of inferring a network of probability distributions  of strings  graphs or chemical structures.

We begin by stating the setup of classical GGMs: The basic building block is matrix (cid:101)X ∈ Rn×d

which follows the Matrix-Normal distribution [8]

(cid:101)X ∼ N (M  Ψ ⊗ Id).

(1)
The goal is to identify Ψ−1  which encodes the desired dependence structure. More speciﬁcally  two
objects (= rows) are conditionally independent given all others if and only if Ψ−1 has a correspond-
ing zero element. This is often depicted as an undirected graph (see Figure 1)  where the objects are
vertices and (missing) edges represent their conditional (in)dependencies.

Figure 1: Precision matrix Ψ−1 and its interpretation as a graph (self-loops are typically omitted).

Prabhakaran et al. [19] formulated the Translation-invariant Wishart Network (TiWnet)  which treats

(cid:101)X as a latent matrix and only requires their squared Euclidean distances Dij = dE((cid:101)xi (cid:101)xj)2  where

1

(cid:101)xi ∈ Rd is the ith row of (cid:101)X. Also  SE = (cid:101)X(cid:101)X(cid:62) refers to the n × n inner-product matrix  which is

linked via Dij = SE ii + SE jj − 2 SE ij. Importantly  the transition to distances implies that means
of the form M = 1nw(cid:62) with w ∈ Rd are not identiﬁable anymore. In contrast to the above  we
start off by assuming a matrix

(2)
where the columns (= features) are correlated as deﬁned by Σ ∈ Rd×d. Due to this change  the

inner-product becomes SMH = XX(cid:62) = (cid:101)XΣ(cid:101)X(cid:62). If we directly observed X as in classical GGMs 
then Σ could be removed to recover (cid:101)X  however  in the case of distances  the impact of Ψ and Σ is

inevitably mixed. A suitable assumption is therefore the squared Mahalanobis distance

1

2 ∼ N (M  Ψ ⊗ Σ) 

X := (cid:101)XΣ

Dij = dMH(xi  xj)2 = ((cid:101)xi −(cid:101)xj)(cid:62)Σ((cid:101)xi −(cid:101)xj) 

(3)

only D is observed and the following is latent: d  X  (cid:101)X  S := SMH  Σ and M = 1nw(cid:62).

which dramatically increases the degree of freedom for inference about Ψ. Recall that in our setting

The main difﬁculty comes from the inherent mixture effect of Ψ and Σ in the distances  which blurs
or obscures what is relevant in GGMs. For example  if we naively enforce Σ = Id  then all of the
information is solely attributed to Ψ. However  in applications where the true Σ (cid:54)= Id  we would
consequently infer false structure  up to a degree where the result is completely mislead by feature
correlation.
In pure Bayesian fashion  we specify a prior belief for Σ and average over all realizations weighted
by the Gaussian likelihood. For a conjugate prior  this leads to the Matrix-T distribution  which
forms the core part of our approach. The resulting model generalizes TiWnet and is ﬂexible enough
to account for arbitrary feature correlation.
In the following  we brieﬂy describe a practical application with all the above properties.

Example: A Network of Biological Pathways Using DNA microarrays  it is possible to mea-
sure the expression levels of thousands of genes in a patient simultaneously  however  each gene is
highly prone to noise and only weakly informative when analyzed on its own. To solve this problem 
the focus is shifted towards pathways [5]  which can be seen as (non-disjoint) groups of genes that
contribute to high-level biological processes. The underlying idea is that genes exhibit visible pat-
terns only when paired with functionally related entities. Hence  every pathway has a characteristic
distribution of gene expression values  which we compare via the so-called Bhattacharyya distance
[2  11]. Our goal is then to derive a network between pathways  but what if the patients (= features)
from whom we obtained the cells were correlated (sex  age  treatment  . . .)?

Figure 2: The big picture. Different assumptions about M and Σ lead to different models.

Related work Inference in GGMs is generally aimed at Ψ−1 and therefore every approach relies
on Eq. (1) or (2)  however  they differ in their assumptions about M and Σ. Figure 2 puts our setting
into a larger context and describes all possible conﬁgurations in a single scheme. Throughout the
paper  we assume there are n objects and an unknown number of d latent features. Since our inputs
are pairwise distances D  the mean is of the form M = 1nw(cid:62)  but at the same time  we do not

2

Σ=IdΣ=IdΣ=IdΣΣXDM=1nwtSXX=tM=v1tM=0n×ddTiWnetTiMTgLmodelinputmeansfeaturecorrelationgLTRCMimpose any restriction on Σ. A complementary assumption is made in TiWnet [19]  which enforces
strict feature independence.
For the models based on matrix X  the mean matrix is deﬁned as M = v1(cid:62)d with v ∈ Rn. This
choice is neither better nor worse—it does not rely on pairwise distances and hence addresses a
different question. By further assuming Σ = Id  we arrive at the graphical LASSO (gL) [7] that
optimizes the likelihood under an L1 penalty. The Transposable Regularized Covariance Model
(TRCM) [1] is closely related  but additionally allows arbitrary Σ and alternates between estimating
Ψ−1 and Σ−1. The basic conﬁguration for S  M = 0n×d and Σ = Id  also leads to the model of
gL  however this rarely occurs in practice.

2 Model

On the most fundamental level  our task deals with incorporating invariances into the Gaussian
model  meaning it must not depend on any unrecoverable feature information  i.e. Σ  M = 1nw(cid:62)
(vanishes for distances) and d. The starting point is the log-likelihood of Eq. (2)

2tr(cid:0)W (X − M )Σ−1(X − M )(cid:62)(cid:1) 

(4)

(cid:96)(W  Σ  M ; X) = d

2 log |W| − n

2 log |Σ| − 1

where we used the shorthand W := Ψ−1. In the literature  there exist two conceptually different
approaches to achieve invariances: the ﬁrst is the classical marginal likelihood [12]  closely related
to the proﬁle likelihood [16]  where a nuisance parameter is either removed by a suitable statistic
or replaced by its corresponding maximum likelihood estimate [9]. The second approach follows
the Bayesian marginal likelihood by introducing a prior and integrating over the product. Hereby 
the posterior is a weighted average  where the weights are distributed according to prior belief. The
following sections will discuss the required transformations of Eq. (4).

2.1 Marginalizing the Latent Feature Correlation

2.1.1 Classical Marginal Likelihood

Let us begin with the attempt to remove Σ by explicit reconstruction  as done in McCullagh [13].
Computing the derivative of Eq. (4) with respect to Σ and setting it to zero  we arrive at the maximum

likelihood estimate(cid:98)Σ = 1

(cid:96)(W  M ; X (cid:98)Σ) = d

n (X − M )(cid:62)W (X − M )  which leads to

2tr(W (X − M )(cid:98)Σ−1(X − M )(cid:62))

2 log |(cid:98)Σ| − 1
2 log |W (X − M )(X − M )(cid:62)|.

2 log |W| − n
2 log |W| − n

= d

(5)
(6)

(cid:98)Σ−1 only exists if(cid:98)Σ has full rank  or equivalently  if d ≤ n. Further  even d = n must be excluded 

Eq. (6) does not depend on Σ anymore  however  note that there is a hidden implication in Eq. (5):

since Eq. (6) would become independent of X otherwise. McCullagh [13] analyzed the Fisher
information for varying d and concluded that this model is “a complete success” for d (cid:28) n  but “a
spectacular failure” if d → n. Since distance matrices typically require d ≥ n  the approach does
not qualify.

2.1.2 Bayesian Marginal Likelihood

Iranmanesh et al. [10] analyzed the Matrix-Normal likelihood in Eq. (4) in conjunction with an
Inverse Matrix-Gamma (IMG) prior—the latter being a generalization of an inverse Wishart prior. It
is denoted by Σ ∼ IMG(α  β  Ω)  where α > 1
2 (d − 1) and β > 0 are shape and scale parameters 
respectively. Ω is a d × d positive-deﬁnite matrix reﬂecting the expectation of Σ. This combination
leads to the so-called (Generalized) Matrix T-distribution1 X ∼ T (α  β  M  W  Ω) with likelihood
(7)
Compared to the classical marginal likelihood  the obvious differences are In and scalar β  which
can be seen as regularization. The limit of β → ∞ implies that no regularization takes place
1Choosing an inverse Wishart prior for Σ results in the standard Matrix T-distribution  however its variance

2 W (X − M )Ω−1(X − M )(cid:62)|.

2 log |W| − (α + n

(cid:96)(W  M ; α  β  X  Ω) = d

2 ) log |In + β

can only be controlled by an integer. This is why the Generalized Matrix T-distribution is preferred.

3

2 W (X − M )Ω−1(X − M )(cid:62)  hence any d ≥ 1 is valid.

and  interestingly  this likelihood resembles Eq. (6). The other extreme β → 0 leads to a likeli-
hood that is independent of X. Another observation is that the regularization ensures full rank of
In + β
At this point  the Bayesian approach reveals a fundamental advantage: For TiWnet  the distance
matrix enforced independent features  but now  we are in a position to maintain the full model while
adjusting the hyperparameters instead. We propose Ω ≡ Id  meaning the prior of Σ will be centered
at independent latent features  which is a common and plausible choice before observing any data.
The ﬂexibility ultimately comes from α and β when deﬁning a ﬂat prior  which means deviations
from independent features are explicitly allowed.

2.2 Marginalizing the Latent Means

(cid:96)(Ψ ; α  β  LX) = − d

2 log |LΨL(cid:62)| − (α + n−1

2 ) log |In + β

2 L(cid:62)(LΨL(cid:62))−1LXX(cid:62)|.

(9)

The fact that we observe a distance matrix D implies that information about the (feature) coordinate
system is irrevocably lost  namely M = 1w(cid:62)  which is why the means must be marginalized. We
brieﬂy discuss the necessary steps  but for an in-depth review please refer to [19  14  17]. Following
the classical marginalization  it sufﬁces to deﬁne a projection L ∈ R(n−1)×n with property L1n =
0n−1. In other words  all biases of the form 1nw(cid:62) are mapped to the nullspace of L. The Matrix
T-distribution under afﬁne transformations [10  Theorem 3.2] reads LX ∼ T (α  β  LM  LΨL(cid:62)  Ω)
and in our case (Ω = Id  LM = L1nw(cid:62) = 0(n−1)×d)  we have

2 log |W| − d

(cid:96)(W ; α  β  D  1n) = d

(8)
Note that due to the statistic LX  the likelihood is constant over all X (or S) mapping to the same D.
As we are not interested in any speciﬁcs about L other than its nullspace  we replace the image with
the kernel of the projection and deﬁne matrix Q := In − (1(cid:62)n W 1n)−11n1(cid:62)n W . Using the identity
QSQ(cid:62) = − 1

2 QDQ(cid:62) and Q(cid:62)W Q = W Q  we can ﬁnally write the likelihood as
2 ) log |In − β

4 W QD| 
which accounts for arbitrary latent feature correlation Σ and all mean matrices M = 1nw(cid:62).
In hindsight  the combination of Bayesian and classical marginal likelihood might appear arbitrary 
but both strategies have their individual strengths. Mean matrix M  for example  is limited to a single
direction in an n dimensional space  therefore the statistic LX represents a convenient solution. In
contrast  the rank-d matrix Σ affects a much larger spectrum that cannot be handled in the same
fashion—ignoring this leads to a degenerate likelihood as previously shown. The problem is only
tractable when specifying a prior belief for Bayesian marginalization. On a side note  the Bayesian
posterior includes the classical marginal likelihood for the choice of an improper prior [4]  which
could be seen in the Matrix-T likelihood  Eq. (7)  in the limit of β → ∞.

2 log(1(cid:62)n W 1n) − (α + n−1

3

Inference

The previous section developed a likelihood for GGMs that conforms to all aspects of information
loss inherent to distance matrices. As our interest lies in the network-deﬁning W   the following will
discuss Bayesian inference using a Markov chain Monte Carlo (MCMC) sampler.

Hyperparameters α  β and d At some point in every Bayesian analysis  all hyperparameters
need to be speciﬁed in a sensible manner. Currently  the occurrence of d in Eq. (9) is particularly
problematic  since (i) the number of latent features is unknown and (ii) it critically affects the balance
2 (d − 1)  which can
between determinants. To resolve this issue  recall that α must satisfy α > 1
alternatively be expressed as α = 1
4 W QD| 

(10)
where d now inﬂuences the likelihood on a global level and can be used as temperature reminiscent
of simulated annealing techniques for optimization. In more detail  we initialize the MCMC sampler
with a small value of d and increase it slowly  until the acceptance ratio is below  say  1 percent. After
that event  all samples of W are averaged to obtain the ﬁnal network.
Parameter v and β still play a crucial role in the process of inference  as they distribute the probability
mass across all latent feature correlations and effectively control the scope of plausible Σ. Upon

2 (vd − n + 1) with v > 1 + n−2
2 log(1(cid:62)n W 1n) − vd
2 log |W| − d

d . Thereby  we arrive at
2 log |In − β

(cid:96)(W ; v  β  D  1n) = d

4

Algorithm 1 One loop of the MCMC sampler

ik from {−1  0  +1}

and W (p)

ii

kk accordingly

ki ← W (p)

(p) refers to proposal

Input: distance matrix D  temperature d and ﬁxed v > 1 + n−2
d
for i = 1 to n do
W (p) ← W  
Uniformly select node k (cid:54)= i and sample element W (p)
Set W (p)
ik and update W (p)
Compute posterior in Eq. (12) and acceptance of W (p)
if u ∼ U(0  1) < acceptance then
end if
end for
Sample proposal β(p) ∼ Γ(βshape  βscale)
Compute posterior in Eq. (12) and acceptance of β(p)
if u ∼ U(0  1) < acceptance then
end if

W ← W (p)

β ← β(p)

closer inspection  we gain more insight by the variance of the Matrix-T distribution 

2(Ψ ⊗ Ω)

β(v d − 2 n + 1)

 

(11)

which is maximal when β and v are jointly small. We aim for the most ﬂexible solution  thus v is
ﬁxed at the smallest possible value and β is stochastically integrated out in a Metropolis-Hastings
step. A suitable choice is a Gamma prior β ∼ Γ(βshape  βscale); its shape and scale must be chosen to
be sufﬁciently ﬂexible on the scale of the distance matrix at hand.

Priors for W The prior for W is ﬁrst and foremost required to be sparse and ﬂexible. There
are many valid choices  like spike and slab [15] or partial correlation [3]  but we adapt the two-
component scheme of TiWnet  which has computational advantages and enables symmetric random
walks. The following brieﬂy explains the construction:
Prior p1(W ) deﬁnes a symmetric random matrix  where off-diagonal elements Wij are uniform on
{−1  0  +1}  i.e. an edge with positive/negative weight or no edge. The diagonal is chosen such that
i=1(Wii − )(cid:1) and induces sparsity.
j(cid:54)=i |Wij|. Although this only allows 3 levels  it proved to be

W is positive deﬁnite: Wii ←  +(cid:80)
The second component is a Laplacian p2(W | λ) ∝ exp(cid:0) − λ(cid:80)n

sufﬁciently ﬂexible in practice. Replacing it with more levels is possible  but conceptually identical.

Here  the total number of edges in the network is penalized by parameter λ > 0. Combining the
likelihood of Eq. (10) and the above priors  the ﬁnal posterior reads:

p(W |• ) = p(D | W  β  1n) p1(W ) p2(W | λ) p3(β | βshape  βscale).

(12)

The full scheme of the MCMC sampler is reported in Algorithm 1.

Complexity Analysis The runtime of Algorithm 1 is primarily determined by the repeated evalu-
ation of the posterior in Eq. (12)  which would require O(n4) in the naive case of fully recomputing
the determinants. Every ﬂip of an edge  however  only changes a maximum of 4 elements2 in W  
which gives rise to an elegant update scheme building on the QR decomposition.
Theorem. One full loop in Algorithm 1 requires O(n3).
Proof. Due to the 3-level prior  there are only 6 possible ﬂip conﬁgurations depending on the
current edge between object i and j (2 examples depicted here for i = 1  j = 3):

∆W := W (p) − W ⇔

(cid:40)(cid:34)−1

0
+1

(cid:35)

(cid:34) 0

(cid:35)(cid:41)

0 +1
0
0
0 −1

  . . .  

0
+2

0 +2
0
0
0
0

(13)

An important observation is that ∆W can solely be expressed in terms of rank-1 matrices  in partic-
ular either uv(cid:62) or uv(cid:62) + ab(cid:62). If we know the QR decomposition of W   then the decomposition

2This also holds for more than 3 edge levels.

5

of W (p) can be found in O(n2). Consequently  its determinant is obtained by det(QR) =(cid:81)n

i=1 Rii
in O(n). Our goal is to exploit this property and express both determinants of the posterior as rank-1
updates to their existing QR decompositions. Restating the likelihood  we have
2 log |In − β

− d
2 log(1(cid:62)n W (p)1n) − vd

(cid:96)(W (p) ; •) = d

(14)

.

(cid:124)

(cid:125)
(cid:123)(cid:122)
4 W (p)QD|

=: det2

Updating det1 corresponds to either W (p) = W + uv(cid:62) or W (p) = W + uv(cid:62) + ab(cid:62) as explained
in Eq. (13)  thus leading to O(n2). We reformulate det2 to follow the same scheme:

(cid:12)(cid:12)(cid:12)In − β

4 W

det2 =

=: det1

(cid:124) (cid:123)(cid:122) (cid:125)
2 log |W (p)|
(cid:16)
(cid:17)
(cid:104)(cid:16)
u − γ(cid:0)1(cid:62)n u(cid:1)(cid:16)
(cid:104)
a − γ(cid:0)1(cid:62)n a(cid:1)(cid:16)
(cid:104)

1(cid:62)
n W 1n
− γ

In −

1(cid:62)
n W 1n

1

1

4

− β
− β
− β

4

4

D

1n1(cid:62)n W
W 1n − γ

(cid:17)
(cid:16)(cid:0)v(cid:62)1n
(cid:17)(cid:105)(cid:0)DW 1n
(cid:1)u +(cid:0)b(cid:62)1n
(cid:1)a
(cid:17)(cid:105)(cid:0)Dv(cid:1)(cid:62)
W 1n +(cid:0)v(cid:62)1n
(cid:1)u +(cid:0)b(cid:62)1n
(cid:1)a
(cid:12)(cid:12)(cid:12).
(cid:17)(cid:105)(cid:0)Db(cid:1)(cid:62)
W 1n +(cid:0)v(cid:62)1n
(cid:1)u +(cid:0)b(cid:62)1n
(cid:1)a

(cid:1)(cid:62)

(15)

For notational convenience  we deﬁned the shorthand

γ :=

1

1(cid:62)n W (p)1n

=

1

1(cid:62)n (W + uv(cid:62) + ab(cid:62))1n

=

1

1(cid:62)n W 1n + (1(cid:62)n u)(v(cid:62)1n) + (1(cid:62)n a)(b(cid:62)1n)

.

Note that the determinant of the ﬁrst line in Eq. (15) is already known (i.e. its QR decomposition)
and the following 3 lines are only rank-1 updates as indicated by parenthesis. Therefore  det2 is
computed in 3 steps  each consuming O(n2). For some of the 6 ﬂip conﬁgurations  we even have
a = b = 0n  which renders the last line in Eq. (15) obsolete and simpliﬁes the remaining terms.
Since the for loop covers n ﬂips  all updates contribute as n·O(n2). There is no shortcut to evaluate
proposal β(p) given β  thus its posterior is recomputed from scratch in O(n3). Therefore  Algorithm
1 has an overall complexity of O(n3)  which is the same as TiWnet.

4 Experiments

4.1 Synthetic Data

We ﬁrst look at synthetic data and compare how well the recovered network matches the true one.
Hereby  the accuracy is measured by the f-score using the edges (positive/negative/zero).

Independent Latent Features Since TiMT is a generalization for arbitrary Σ  it must also cover
Σ ≡ Id  thus  we generate a set of 100 Gaussian-distributed matrices X with known W and Σ = Id 
where n = 30 and d = 300. Next  we add column translations 1nw(cid:62) with elements in w ∈ Rd
being Gamma distributed  however these do not enter D by deﬁnition. As TRCM does not account
for column shifts  it is used in conjunction with the true  unshifted matrix X (hence TRCM.u).
All methods require a regularization parameter  which obviously determines the outcome. In par-
ticular  TiWnet and TiMT use the same  constant parameter throughout all 100 distance matrices
and obtain the ﬁnal W via annealing. Concerning TRCM and gL  we evaluate each X on a set of
parameters and only report the highest f-score per data set. This is in strong favor of the competition.
Boxplots of the achieved f-scores and the false positive rates are depicted in Figure 3  left. As
can be seen  TiMT and TiWnet score as high as TRCM.u without knowledge of features or feature
translations. We omit gL from the comparison due to a model mismatch regarding M  meaning it
will naturally fall short. Instead  the interested reader is pointed to extensive results in [19].
The gist of this experiment is that all methods work well when the model requirements are met.
Also  translating the individual features and obscuring them does not impair TiWnet and TiMT.

Correlated Latent Features The second experiment is similar to the ﬁrst one (n = 30  d =
300 and column shifts)  but it additionally introduces feature correlation. Here  Σ is generated
by sampling a matrix G ∼ N (0d×5d  Id ⊗ I5d) and adding Gamma distributed vector a ∈ R5d to
randomly selected rows of G. The ﬁnal feature covariance matrix is given by Σ = 1

5d GG(cid:62).

6

Figure 3: Results for synthetic data. Translations do not apply to TRCM.u. Models with violated
assumptions (M and/or Σ) are highlighted with gray bars.

Due to the dramatically increased degree of freedom  all methods are impacted by lower f-scores
(see Figure 3  right). As expected  TRCM.u performs best in terms of f-score  which is based on
the unshifted full data matrix X with an individually optimized regularization parameter. TiMT 
however  follows by a slim margin. On the contrary  TiWnet explains the similarities exclusively
by adding more (unnecessary) edges  which is reﬂected in its increased  but strongly consistent
false positive rate. This issue leads to a comparatively low f-score that is even below the remaining
contenders. Finally  Figure 4 shows an example network and its reconstruction. Keeping in mind
the drastic information loss between true X30×300 and D30×30  TiMT performs extremely well.

Figure 4: An example for synthetic data with feature correlation. The network inferred by TiMT
(center) is relatively close to ground truth (left)  however TiWnet (right) is apparently mislead by Σ.
Black/red edges refer to +/− edge weight.

4.2 Real-World Data: A Network of Biological Pathways

In order to demonstrate the scalability of TiMT  we apply it to the publicly available colon cancer
dataset of Sheffer et al. [20]  which is comprised of 13 437 genes measured across 182 patients.
Using the latest gene sets from the KEGG3 database  we arrive at n = 276 distinct pathways.
After learning the mean and variance of each pathway as the distribution of its gene expression
values across patients  the Bhattacharyya distances [11] are computed as a 276× 276 matrix D. The
pathways are allowed to overlap via common genes  thus leading to similarities  however it is unclear
how and to what degree the correlation of patients affects the inferred network. For this purpose  we
run TiMT alongside TiWnet with identical parameters for 20 000 samples and report the annealed
networks in Figure 5. Again  the difference in topology is only due to latent feature correlation.
Runtime on a standard 3 GHz PC was 3:10 hours for TiMT  while a naive implementation in O(n4)
ﬁnished after ∼20 hours. TiWnet performed slightly better at around 3 hours  since the model does
not have hyperparameter β to control feature correlation.

3http://www.genome.jp/kegg/  accessed in May 2014

7

MODEL MISMATCHTRCM.uTRCMTiWnetgLTiMTTRCM.uTRCMTiWnetgLTiMTMODEL MISMATCHF−score0.00.20.40.60.81.0False positive rate0.00.20.40.60.81.0F−score0.00.20.40.60.81.0False positive rateIndependent Latent FeaturesCorrelated Latent Features0.00.20.40.60.81.0TRCM.uTiWnetTiMTTRCM.uTiWnetTiMTTrue networkTiMTTiWnetFigure 5: A network of pathways in colon cancer patients  where each vertex represents one pathway.
From both results  we extract a subgraph of 3 pathways including all neighbors in reach of 2 edges.
The matrix on the bottom shows external information on pathway similarity based on their relative
number of protein-protein interactions. Black/red edges refer to +/− edge weight.

Without side information it is not possible to conﬁrm either result  hence we resort to expert knowl-
edge for protein-protein interactions from the BioGRID4 database and compute the strength of con-
nection between pathways as the number of interactions relative to their theoretical maximum. Using
this  we can easily check subnetworks for plausibility (see Figure 5  center): The black vertices 96 
98 and 114 correspond to base excision repair  mismatch repair and cell cycle  which are particu-
larly interesting as they play a key role in DNA mutation. These pathways are known to be strongly
dysregulated in colon cancer and indicate an elevated susceptibility [18  6]. The topology of these 3
pathways for TiMT is fully supported by protein interactions  i.e. 98 is the link between 114 and 96
and removing it renders 96 and 98 independent. TiWnet  on the contrary  overestimates the network
and produces a highly-connected structure contradicting the evidence. This is a clear indicator for
latent feature correlation.

5 Conclusion

We presented the Translation-invariant Matrix-T process (TiMT) as an elegant way to make in-
ference in Gaussian graphical models when only pairwise distances are available. Previously  the
inherent information loss about underlying features appeared to prevent any conclusive statement
about their correlation  however  we argue that neither assumed full independence nor maximum
likelihood estimation is reasonable in this context.
Our contribution is threefold: (i) A Bayesian relaxation solves the issue of strict feature indepen-
dence in GGMs. The assumption is now shifted into the prior  but ﬂat priors are possible. (ii) The
approach generalizes TiWnet  but maintains the same complexity  thus  there is no reason to retain
the simpliﬁed model. (iii) TiMT for the ﬁrst time accounts for all latent parameters of the Matrix
Normal without access to the latent data matrix X. The distances D are fully sufﬁcient.
In synthetic experiments  we observed a substantial improvement over TiWnet  which highly over-
estimated the networks and falsly attributed all information to the topological structure. At the same
time  TiMT performed almost on par with TRCM(.u)  which operates under hypothetical  optimal
conditions. This demonstrates that all aspects of information loss can be handled exceptionally well.
Finally  the network of biological pathways provided promising results for a domain of non-vectorial
objects  which effectively precludes all methods except for TiMT and TiWnet. Comparing these two 
the considerable difference in network topology only goes to show that invariance against latent
feature correlation is indispensable—especially pertaining to distances.

4http://thebiogrid.org  version 3.2

8

TiMTTiWnet3192582899196981141151149801969611498322336079828991969798114References
[1] G. Allen and R. Tibshirani. Transposable Regularized Covariance Models with an Application

to Missing Data Imputation. The Annals of Applied Statistics  4:764–790  2010.

[2] A. Bhattacharyya. On a Measure of Divergence between Two Statistical Populations Deﬁned
by Their Probability Distributions. Bulletin of the Calcutta Mathematical Society  35:99–109 
1943.

[3] M. Daniels and M. Pourahmadi. Modeling Covariance Matrices via Partial Autocorrelations.

Journal of Multivariate Analysis  100(10):2352–2363  2009.

[4] A. de Vos and M. Francke. Bayesian Unit Root Tests and Marginal Likelihood. Technical
report  Department of Econometrics and Operation Researchs  VU University Amsterdam 
2008.

[5] L. Ein-Dor  O. Zuk  and E. Domany. Thousands of Samples are Needed to Generate a Robust
In Proceedings of the National Academy of

Gene List for Predicting Outcome in Cancer.
Sciences  pages 5923–5928  2006.

[6] P. Fortini  B. Pascucci  E. Parlanti  M. D’Errico  V. Simonelli  and E. Dogliotti. The
Base Excision Repair: Mechanisms and its Relevance for Cancer Susceptibility. Biochimie 
85(11):1053–1071  2003.

[7] J. Friedman  T. Hastie  and R. Tibshirani. Sparse Inverse Covariance Estimation with the

Graphical Lasso. Biostatistics  9(3):432–441  2008.

[8] A. K. Gupta and D. K. Nagar. Matrix Variate Distributions. PMS Series. Addison-Wesley

Longman  1999.

[9] D. Harville. Maximum Likelihood Approaches to Variance Component Estimation and to

Related Problems. Journal of the American Statistical Association  72(358):320–338  1977.

[10] A. Iranmanesh  M. Arashi  and S. Tabatabaey. On Conditional Applications of Matrix Variate
Normal Distribution. Iranian Journal of Mathematical Sciences and Informatics  pages 33–43 
2010.

[11] T. Jebara and R. Kondor. Bhattacharyya and Expected Likelihood Kernels. In Conference on

Learning Theory  2003.

[12] J. Kalbﬂeisch and D. Sprott. Application of Likelihood Methods to Models Involving Large
Numbers of Parameters. Journal of the Royal Statistical Society. Series B (Methodological) 
32(2):175–208  1970.

[13] P. McCullagh. Marginal Likelihood for Parallel Series. Bernoulli  14:593–603  2008.
[14] P. McCullagh. Marginal Likelihood for Distance Matrices. Statistica Sinica  19:631–649 

2009.

[15] T. Mitchell and J. Beauchamp. Bayesian Variable Selection in Linear Regression. Journal of

the American Statistical Association  83(404):1023–1032  1988.

[16] S. Murphy and A. van der Vaart. On Proﬁle Likelihood. Journal of the American Statistical

Association  95:449–465  2000.

[17] H. Patterson and R. Thompson. Recovery of Inter-Block Information when Block Sizes are

Unequal. Biometrika  58(3):545–554  1971.

[18] P. Peltom¨aki. DNA Mismatch Repair and Cancer. Mutation Research  488(1):77–85  2001.
[19] S. Prabhakaran  D. Adametz  K. J. Metzner  A. B¨ohm  and V. Roth. Recovering Networks

from Distance Data. JMLR  92:251–283  2013.

[20] M. Sheffer  M. D. Bacolod  O. Zuk  S. F. Giardina  H. Pincas  F. Barany  P. B. Paty  W. L.
Gerald  D. A. Notterman  and E. Domany. Association of Survival and Disease Progression
with Chromosomal Instability: A Genomic Exploration of Colorectal Cancer. In Proceedings
of the National Academy of Sciences  pages 7131–7136  2009.

9

,David Adametz
Volker Roth
Eugene Ndiaye
Olivier Fercoq
Alexandre Gramfort
Joseph Salmon