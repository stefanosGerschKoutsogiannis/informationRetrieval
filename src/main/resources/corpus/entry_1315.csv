2017,Decomposition-Invariant Conditional Gradient for General Polytopes with Line Search,Frank-Wolfe (FW) algorithms with linear convergence rates have recently achieved great efficiency in many applications. Garber and Meshi (2016) designed a new decomposition-invariant pairwise FW variant with favorable dependency on the domain geometry. Unfortunately  it applies only to a restricted class of polytopes and cannot achieve theoretical and practical efficiency at the same time. In this paper  we show that by employing an away-step update  similar rates can be generalized to arbitrary polytopes with strong empirical performance. A new "condition number" of the domain is introduced which allows leveraging the sparsity of the solution. We applied the method to a reformulation of SVM  and the linear convergence rate depends  for the first time  on the number of support vectors.,Decomposition-Invariant Conditional Gradient for

General Polytopes with Line Search

Mohammad Ali Bashiri

Xinhua Zhang

Department of Computer Science  University of Illinois at Chicago

Chicago  Illinois 60607

{mbashi4 zhangx}@uic.edu

Abstract

Frank-Wolfe (FW) algorithms with linear convergence rates have recently achieved
great efﬁciency in many applications. Garber and Meshi (2016) designed a new
decomposition-invariant pairwise FW variant with favorable dependency on the
domain geometry. Unfortunately it applies only to a restricted class of polytopes
and cannot achieve theoretical and practical efﬁciency at the same time. In this
paper  we show that by employing an away-step update  similar rates can be
generalized to arbitrary polytopes with strong empirical performance. A new
“condition number” of the domain is introduced which allows leveraging the sparsity
of the solution. We applied the method to a reformulation of SVM  and the linear
convergence rate depends  for the ﬁrst time  on the number of support vectors.

1

Introduction

The Frank-Wolfe algorithm [FW  1] has recently gained revived popularity in constrained convex
optimization  in part because linear optimization on many feasible domains of interest admits efﬁcient
computational solutions [2]. It has been well known that FW achieves O(1/) rate for smooth convex
optimization on a compact domain [1  3  4]. Recently a number of works have focused on linearly
converging FW variants under various assumptions.
In the context of convex feasibility problem  [5] showed linear rates for FW where the condition
number depends on the distance of the optimum to the relative boundary [6]. Similar dependency
was derived in the local linear rate on polytopes using the away-step [6  7]. With a different analysis
approach  [8–10] derived linear rates when the Robinson’s condition is satisﬁed at the optimal solution
[11]  but it was not made clear how the rate depends on the dimension and other problem parameters.
To avoid the dependency on the location of the optimum  [12] proposed a variant of FW whose
rate depends on some geometric parameters of the feasible domain (a polytope). In a similar ﬂavor 
[13  14] analyzed four versions of FW including away-steps [6]  and their afﬁne-invariant rates depend
on the pyramidal width (Pw) of the polytope  which is hard to compute and can still be ill-conditioned.
Moreover  [15] recently gave a duality-based analysis for non-strongly convex functions. Some lower
bounds on the dependency of problem parameters for linear rates of FW are given in [12  16].
To get around the lower bound  one may tailor FW to speciﬁc objectives and domains (e.g. spectra-
hedron in [17]). [18] specialized the pairwise FW (PFW) to simplex-like polytopes (SLPs) whose
vertices are binary  and is deﬁned by equality constraints and xi ≥ 0. The advantages include: a) the
convergence rate depends linearly on the cardinality of the optimal solution and the domain diameter
square (D2)  which can be much better than the pyramidal width; b) it is decomposition-invariant 
meaning that it does not maintain a pool of atoms accumulated and the away-step is performed on the
face that the current iterate lies on. This results in considerable savings in computation and storage.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

PFW-1 [18] PFW-2 [18]

(SLP)

general

ns
ks
×
×

×
×
×
×

LJ [13]
general
n2
n (k = 1)
k · Pw−2
D2 · Pw−2

AFW-1
(SLP)

ns
ks
×
×

AFW-2
general

n2s
k2s

k2 min(sk  n)

Unit cube [0  1]n

Pk = {x ∈ [0  1]n : 1(cid:62)x = k}
Qk = {x ∈ [0  1]n : 1(cid:62)x ≤ k}

arbitrary polytope in Rn

D2nHs
 to get the
Table 1: Comparison of related methods. These numbers need to be multiplied with κ log 1
convergence rates  where κ is the condition number of the objective  D is the diameter of the domain 
s is the cardinality of the optimum  and Pw is the pyradimal width.. Our method is AFW. × means
inapplicable or no rate known. PFW-1 [18] and AFW-1 apply only to SLP  hence not covering Qk
(k≥ 2). [13] showed the pyramidal width for Pk only with k = 1.

However  [18] suffers from multiple inherent restrictions. First it applies only to SLPs  which although
encompass useful sets such as k-simplex Pk  do not cover its convex hull with the origin (Qk):
Pk = {x ∈ [0  1]n : 1(cid:62)x = k}  Qk = {x ∈ [0  1]n : 1(cid:62)x ≤ k}  where k ∈ {1  . . .   n}.

Here 1 = (1  . . .   1)(cid:62). Extending its analysis to general polytopes is not promising because it relies
fundamentally on the integrality of the vertices. Second  its rate is derived from a delicately designed
sequence of step size (PFW-1)  which exhibits no empirical competency. In fact  the experiments in
[18] resorted to line search (PFW-2). However no rate was proved for it. As shown in [13]  dimension
friendly bounds are intrinsically hard for PFW  and they settled for the factorial of the vertex number.
The goal of this paper is to address these two issues while at the same time retaining the computational
efﬁciency of decomposition invariance. Our contributions are four folds. First we generalize the
dimension friendly linear rates to arbitrary polytopes  and this is achieved by replacing the pairwise
PFW in [18] with the away-step FW (AFW  §2)  and setting the step sizes by line search instead of a
pre-deﬁned schedule. This allows us to avoid “swapping atoms” in PFW  and the resulting method
(AFW-2) delivers not only strong empirical performance (§5) but also strong theoretical guarantees
(§3.5)  improving upon PFW-1 and PFW-2 which are strong in either theory or practice  but not both.
Second  a new condition number Hs is introduced in §3.1 to characterize the dimension dependency of
AFW-2. Compared with pyramidal width  it not only provides a more explicit form for computation 
but also leverages the cardinality (s) of the optimal solution. This may lead to much smaller constants
considering the likely sparsity of the solution. Since pyramidal width is hard to compute [13]  we
leave the thorough comparison for future work  but they are comparable on simple polytopes. The
decomposition invariance of AFW-2 also makes each step much more efﬁcient than [13].
Third  when the domain is indeed an SLP  we provide a step size schedule (AFW-1  §3.4) yielding the
same rate as PFW-1. This is in fact nontrivial because the price for replacing PFW by AFW is the
much increased hardness in maintaining the integrality of iterates. The current iterate is scaled in
AFW  while PFW simply adds (scaled) new atoms (which on the other hand complicates the analysis
for line search [13]). Our solution relies on ﬁrst running a constant number of FW-steps.
Finally we applied AFW to a relaxed-convex hull reformulation of binary kernel SVM with bias (§4) 
obtaining O(nκ(#SV)3 log 1
 ) for
AFW-2. Here κ is the condition number of the objective  n is the number of training examples  and
#SV is the number of support vectors in the optimal solution. This is much better than the best known
result of O(n3κ log 1
 ) based on sequential minimal optimization [SMO  19  20]  because #SV is
typically much smaller than n. To the best of our knowledge  this is the ﬁrst linear convergence rate
for hinge-loss SVMs with bias where the rate leverages dual sparsity.
A brief comparison of our method (AFW) with [18] and [13] is given in Table 1. AFW-1 matches
the superior rates of PFW-1 on SLPs  and AFW-2 is more general and its rate is slightly worse than
AFW-1 on SLPs. PFW-2 has no rates available  and pyramidal width is hard to compute in general.

 ) computational complexity for AFW-1 and O(nκ(#SV)4 log 1

2 Preliminaries and Algorithms
Our goal is to solve minx∈P f (x)  where P is a polytope and f is both strongly convex and smooth. A
function f : P → R is α-strongly convex if f (y) ≥ f (x)+(cid:104)y − x ∇f (x)(cid:105)+ α
2 (cid:107)y − x(cid:107)2   ∀ x  y ∈

2

if(cid:10)dFW

Choose the FW-direction via v+
Choose the away-direction v−

 −∇f (xt)(cid:11) ≥(cid:10)dA

Algorithm 1: Decomposition-invariant Away-step Frank-Wolfe (AFW)
1 Initialize x1 by an arbitrary vertex of P. Set q0 = 1.
2 for t = 1  2  . . . do
3
4
5
6
7
8
9
10
12
11

Choose the step size ηt by using one of the following two options:
Option 1: Pre-deﬁned step size:
if t ≤ n0 then

t  −∇f (xt)(cid:11) then dt ← dFW

t   and revert dt = dFW

else

t

t

Set qt = t  ηt = 1
Find the smallest integer s ≥ 0 such that qt deﬁned as follows satisﬁes qt ≥ (cid:100)1/γt(cid:101):

(cid:46) Perform FW-step for the ﬁrst n0 steps

.

t

t ← arg minv∈P (cid:104)v ∇f (xt)(cid:105)  and set dFW
t by calling the away-oracle in (3)  and set dA

t − xt.
t ← v+
t ← xt − v−
t .

  else dt ← dA

t . (cid:46) Choose a direction

(cid:46) This is for SLP only. Need input arguments n0  γt.

2sqt−1 + 1

2sqt−1 − 1

qt ←

if line 5 adopts the FW-step
if line 5 adopts the away-step

and ηt ← q−1

t

.

 

(2)

Option 2: Line search: ηt ← arg min
η≥0

xt+1 ← xt + ηtdt. Return xt if(cid:10)−∇f (xt)  dFW

(cid:11) ≤ .

t

13

14

f (xt + ηdt)  s.t. xt + ηdt∈P. (cid:46) General purpose

Algorithm 2: Decomposition-invariant Pairwise Frank-Wolfe (PFW) (exactly the same as [18])
1 ... as in Algorithm 1  except replacing a) line 5 by dt = dPFW
Option 1: Pre-deﬁned step size: Find the smallest integer s ≥ 0 such that 2sqt−1 ≥ 1/γt.

:= v+
Set qt ← 2sqt−1 and ηt ← q−1

t   and b) line 8-11 by

t − v−
. (cid:46) This option is for SLP only.

t

t

P. In this paper  all norms are Euclidean  and we write vectors in bold lowercase letters. f is β-
smooth if f (y)≤ f (x) +(cid:104)y − x ∇f (x)(cid:105) + β
2 (cid:107)y−x(cid:107)2 ∀ x  y∈P. Denote the condition number as
κ = β/α  and the diameter of the domain P as D. We require D < ∞  i.e. the domain is bounded.
Let [m] := {1  . . .   m}. In general  a polytope P can be deﬁned as

P = {x ∈ Rn : (cid:104)ak  x(cid:105) ≤ bk  ∀ k ∈ [m]  Cx = d}.

(1)
Here {ak} is a set of “directions” and is ﬁnite (m < ∞) and bk cannot be reduced without changing P.
Although the equality constraints can be equivalently written as two linear inequalities  we separate
them out to improve the bounds below. Denoting A = (a1  . . .   am)(cid:62) and b = (b1  . . .   bm)(cid:62)  we
can simplify the representation into P = {x ∈ Rn : Ax ≤ b  Cx = d}.
In the sequel  we will ﬁnd highly efﬁcient solvers for a special class of polytope that was also studied
by [18]. We call a potytope as a simplex-like polytope (SLP)  if all vertices are binary (i.e. the set of
extreme points ext(P) are contained in {0  1}n)  and the only inequality constraints are x ∈ [0  1]n.1
Our decomposition-invariant Frank-Wolfe (FW) method with away-step is shown in Algorithm 1.
There are two different schemes of choosing the step size: one with ﬁxed step size (AFW-1) and one
with line search (AFW-2). Compared with [13]  AFW-2 enjoys decomposition invariance. Like [13] 
we also present a pairwise version in Algorithm 2 (PFW)  which is exactly the method given in [18].
The efﬁciency of line search in step 13 of Algorithm 1 depends on the polytope. Although in
general one needs a problem-speciﬁc procedure to compute the maximal step size  we will show in
experiments some examples where such procedures with high computational efﬁciency are available.
The idea of AFW is to compute a) the FW-direction in the conventional FW sense (call it FW-oracle) 
and b) the away-direction (call it away-oracle). Then pick the one that gives the steeper descent and
take a step along it. Our away-oracle adopts the decomposition-invariant approach in [18]  which
differs from [13] by saving the cost of maintaining a pool of atoms. To this end  our search space in
the away-oracle is restricted to the vertices that satisfy all the inequality constraints by equality if the
1Although [18] does not allow for x ≤ 1 constraints  we can add a slack variable yi: yi + xi = 1  yi ≥ 0.

3

t

current xt does so:
:= arg maxv (cid:104)v ∇f (xt)(cid:105)   s.t. Av≤ b  Cv = d  and (cid:104)ai  xt(cid:105) = bi ⇒ (cid:104)ai  v(cid:105) = bi ∀i. (3)
v−
Besides saving the space of atoms  this also dispenses with computing the inner product between
the gradient and all existing atoms. Same as [18]  it presumes efﬁcient solutions to the away-oracle 
which may preclude its applicability to problems where only the FW-oracle is efﬁciently solvable.
We will show some examples that admit efﬁcient away-oracle.
Before moving on to the analysis  we here make a new  albeit quick  observation that this selection
scheme is in fact decomposing xt implicitly. Speciﬁcally  it tries all possible decompositions of xt 
and for each of them it ﬁnds the best away-direction in the traditional sense. Then it picks the best of
the best over all proper convex decompositions of xt.
Property 1. Denote S(x) := {S ⊆ P : x is a proper convex combination of all elements in S} 
where proper means that all elements in S have a strictly positive weight. Then the away-step in (3)
is exactly equivalent to maxS∈S(xt) maxv∈S (cid:104)v ∇f (xt)(cid:105) . See the proof in Appendix A.

3 Analysis
We aim to analyze the rate by which the primal gap ht := f (xt) − f (x∗) decays. Here x∗ is the
minimizer of f  and we assume it can be written as the convex combination of s vertices of P.

3.1 A New Geometric “Condition Number” of a Polytope

Underlying the analysis of linear convergence for FW-style algorithms is the following inequality
that involves a geometric ”condition number” Hs of the polytope: (v+
t are the FW and

t and v−

away-directions) (cid:112)2Hsht/α(cid:10)v+

t  ∇f (xt)(cid:11) ≤ (cid:104)x∗ − xt ∇f (xt)(cid:105) .

t − v−

(4)
In Theorem 3 of [13]  this Hs is essentially the pyramidal width inverse. In Lemma 3 of [18]  it is the
cardinality of the optimal solution  which  despite being better than the pyramidal width  is restricted
to SLPs. Our ﬁrst key step here is to relax this restriction to arbitrary polytopes and deﬁne our Hs.
Let {ui} be the set of vertices of the polytope P  and this set must be ﬁnite. We do not assume ui is
binary. The following “margin” for each separating hyperplane directions ak will be important:

(cid:104)ak  ui(cid:105) − second max

(cid:104)ak  ui(cid:105) ≥ 0.

gk := max

(5)
Here the second max is the second distinct max in {(cid:104)ak  ui(cid:105) : i}. If (cid:104)ak  ui(cid:105) is invariant to i  then
this inequality (cid:104)ak  x(cid:105) ≤ bk is indeed an equality constraint ((cid:104)ak  x(cid:105) = maxz∈P (cid:104)ak  z(cid:105)) hence can
be moved to Cx = d. So w.l.o.g  we assume gk > 0. Now we state the generalized result.
Lemma 1. Let P be deﬁned as in (1). Suppose x can be written as some convex combination of s
i=1 γiui  where γi ≥ 0  1(cid:62)γ = 1. Then any y ∈ P can be written
Hs (cid:107)x − y(cid:107) where

i=1(γi − ∆i)ui + (1(cid:62)∆)z  such that z ∈ P  ∆i∈ [0  γi]  and 1(cid:62)∆ ≤√

number of vertices of P: x =(cid:80)s
as y =(cid:80)s

i

i

(cid:33)2

(cid:32)(cid:88)

n(cid:88)

j=1

k∈S

akj
gk

Hs := max

S⊆[m] |S|=s

.

(6)

In addition  Equation (4) holds with this deﬁnition of Hs. Note our Hs is deﬁned here  not in (4).

Some intuitive interpretations of Hs are in order. First the deﬁnition in (6) admits a much more
explicit characterization than pyramidal width. The maximization in (6) ranges over all possible
subsets of constraints with cardinality s  and can hence be much lower than if s = m (taking all
constraints). Recall that pyramidal width is oblivious to  hence not beneﬁting from  the sparsity of
the optimal solution. More comparisons are hard to make because [13] only provided an existential
proof of pyramidal width  along with its value for simplex and hypercube only.2
However  Hs is clearly not intrinsic of the polytope. For example  by deﬁnition Hs = n for Q2.
By contrast  we can introduce a slack variable y to Q2  leading to a polytope over [x; y] (vertical
2[21] showed pyramidal width is equivalent to a more interpretable quantity called ”facial distance”  and

they derived its value for more examples. But the evaluation of its value remains challenging in general.

4

concatenation)  with x ≥ 0  y ≥ 0  y + 1(cid:62)x = 2. The augmented polytope enjoys Hs = s.
Nevertheless  adding slack variables increases the diameter of the space and the vertices may no
longer be binary. It also incurs more computation.
Second  gk may approach 0 (tending Hs to inﬁnity) when more linear constraints are introduced and
vertices get closer neighbors. Hs is inﬁnity if the domain is not a polytope  requiring an uncountable
number of supporting hyperplanes. Third  due to the square in (6)  Hs grows more rapidly as one
variable participates in a larger number of constraints  than as a constraint involves a larger number
of variables. When all gk = 1 and all akj are nonnegative  Hs grows with the magnitude of akj.
However this is not necessarily the case when akj elements have mixed sign. Finally  Hs is relative
to the afﬁne subspace that P lies in  and is independent of linear equality constraints.
The proof of Lemma 1 utlizes the fact that the lowest value of 1(cid:62)∆ is the optimal objective value of
(7)

s.t. 0 ≤ ∆ ≤ γ  y = x − (u1  . . .   us)∆ + (1(cid:62)∆)z 

min∆ z 1(cid:62)∆ 

where the inequalities are both elementwise. To ensure z ∈ P  we require Az ≤ b  i.e.

z ∈ P 

(b1(cid:62) − AU )∆ ≥ A(y − x)  where U = (u1  . . .   us).

(8)
The rest of the proof utilizes the optimality conditions of ∆  and is relegated to Appendix A.
Compared with Lemma 2 of [18]  our Lemma 1 does not require ext(P) to be binary  and allows
arbitrary inequality constraints rather than only x ≥ 0. Note Hs depends on b indirectly  and employs
a more explicit form for computation than pyramidal width. Obviously Hs is non-decreasing in s.
Example 1. To get some idea  consider the k-simplex Pk or more general polytopes {x ∈ [0  1]n :
Cx = d}. In this case  the inequality constraints are exclusively xi ∈ [0  1]  meaning ak = ±ek for
all k ∈ [2n] in (1). Here ek stands for a canonical vector of straight 0 except a single 1 in the k-th
coordinate. Obviously all gk = 1. Therefore by Lemma 1  one can derive Hs = s  ∀ s ≤ n.
Example 2. To include inequality  let us consider Qk  the convex hull of a k-simplex. Lemma 1
implies its Hs = n + 3s − 3  independent of k. One might hope to get better Hs when k = 1  since
the constraint x ≤ 1 can be dropped in this case. Unfortunately  still Hs = n.
Remark 1. The L0 norm of the optimal x can be connected with s simply by Caratheodory’s theorem.
Obviously s = (cid:107)x(cid:107)0 (L0 norm) for P1 and Q1. In general  an x in P may be decomposed in multiple
ways  and Lemma 1 immediately applies to the lowest (best) possible value of s (which we will refer
to as the cardinality of x following [18]). For example  the smallest s for any x ∈ Pk (or Qk) must
be at most (cid:107)x(cid:107)0 + 1  because x must be in the convex hull of V := {y ∈ {0  1}n : 1(cid:62)y = k  xi =
0 ⇒ yi = 0 ∀ i}. Clearly its afﬁne hull has dimension (cid:107)x(cid:107)0  and V is a subset of ext(Pk) = ext(Qk).
3.2 Tightness of Hs under a Given Representation of the Polytope
We show some important examples that demonstrate the tightness of Lemma 1 with respect to the
dimensionality (n) and the cardinality of x (s). Note the tightness is in the sense of satisfying the
conditions in Lemma 1  not in the rate of convergence for the optimization algorithm.
Example 3. Consider Q2. u1 = e1 is a vertex and let x = u1 (hence s = 1) and y = (1    . . .   )(cid:62) 
where  > 0 is a small scalar. So in the necessary condition (8)  the row corresponding to 1(cid:62)x ≤ 2
becomes ∆1 ≥ (n − 1) =
Example 4. Let us see another example that is not simplex-like. Let ak = −ek + en+1 + en+2
for k ∈ [n]. Let A = (a1  . . .   an)(cid:62) = (−I  1  1) where I is the identity matrix. Deﬁne P as
i=1 iei + ren+1 + (1 − r)en+2  where r = n(n + 1)/2 and
 > 0 is a small positive constant. x can be represented as the convex combination of n + 1 vertices
(9)
x =
With U = (u1  . . .   un+1)  we have b1(cid:62) − AU = (I  0). Let y = x + en+1  which is clearly in P.
n2 (cid:107)y − x(cid:107). Applying Lemma 1 with s = n + 1 and
Then (8) becomes ∆ ≥ 1  and so 1(cid:62)∆ ≥
gk = 1 for all k  we get Hs = 2n2 + n − 1  which is of the same order of magnitude as n2.
3.3 Analysis for Pairwise Frank-Wolfe (PFW-1) on SLPs
Equipped with Lemma 1  we can now extend the analysis in [18] to SLPs where the constraint of
x ≤ 1 can be explicitly accommodated without having to introduce a slack variable which increases
the diameter D and costs more computations.

P =(cid:8)x ∈ [0  1]n+2 : Ax ≤ 1(cid:9)   i.e. b = 1. Since A is totally unimodular  all the vertices of P must
be binary. Let us consider x =(cid:80)n

iui + (1 − r)un+1  where ui = ei + en+1 for i ≤ n  and un+1 = en+2.

n − 1 · (cid:107)x − y(cid:107) . By Lemma 1  Hs = n which is almost n − 1.

(cid:88)n

√

5

√

i=1

1

α

(1−c1)

t−1
2   where c1 =

2 (1 − c1)t−1 if we
16βHsD2 . The proof just replaces all card(x∗) in [18] with Hs.

Theorem 1. Applying PFW-1 to SLP  all iterates must be feasible and ht ≤ βD2
set γt = c1/2
Slight effort is needed to guarantee the feasibility and we show it as Lemma 6 in Appendix A.
When P is not an SLP or general inequality constraints are present  we resort to line search (PFW-2) 
which is more efﬁcient than PFW-1 in practice. However  the analysis becomes challenging [13  18] 
because it is difﬁcult to bound the number of steps where the step size is clamped due to the feasibility
constraint (the swap step in [13]). So [13] appealed to a bound that is the factorial of the number of
vertices. Fortunately  we will show below that by switching to AFW  the line search version achieves
linear rates with improved dimension dependency for general polytopes  and the pre-deﬁned step
version preserves the strong rates of PFW-1 on SLPs. These are all facilitated by the Hs in Lemma 1.

3.4 Analysis for Away-step Frank-Wolfe with Pre-deﬁned Step Size (AFW-1) on SLPs

We ﬁrst show that AFW-1 achieves the same rate of convergence as PFW-1 on SLPs. Although this
does not appear surprising and the proof architecture is similar to [18]  we stress that the step size
needs delicate modiﬁcations because the descent direction dt in PFW does not rescale xt  while
AFW does. Our key novelty is to ﬁrst run a constant number of FW-steps (O( 1
t ) rate)  and start
accepting away-steps when the step size is small enough to ensure feasibility and linear convergence.
We ﬁrst establish the feasibility of iterates under the pre-deﬁned step sizes. Proofs are in Appendix A.
Lemma 2 (Feasibility of iterates for AFW-1). Suppose P is an SLP and the reference step sizes
{γt}t≥n0 are contained in [0  1]. Then the iterates generated by AFW-1 are always feasible.
Choosing the step size. Key to the AFW-1 algorithm is the delicately chosen sequence of step
sizes. For AFW-1  deﬁne (logarithms are natural basis)
c0(1 − c1)(t−1)/2  where M1 =

(cid:114) α

  M2 =

θ = 52

γt =

βD2

(10)

√

 

8Hs

2

3M2 log n0

(1 − c1)1−n0 .

(11)

 

c1

n0

c0 =

c1 =

  n0 =
t M2 log t for all t ∈ [2  n0]. Obviously n0 ≥ 200 by (11).
Lemma 3. In AFW-1  we have ht ≤ 3
This result is similar to Theorem 1 in [4]. However  their step size is 2/(t + 2) leading to a 2
t+2 M2
rate of convergence. Such a step size will break the integrality of the iterates  and hence we adjusted
the step size  at the cost of a log t term in the rates which can be easily handled in the sequel.
The condition number c1 gets better (bigger) when: the strongly convex parameter α is larger  the
smoothness constant β is smaller  the diameter D of the domain is smaller  and Hs is smaller.
Lemma 4. For all t ≥ n0  AFW-1 satisﬁes a) γt ≤ 1  b) γ−1
By Lemma 2 and Lemma 4a  we know that the iterates generated by AFW-1 are all feasible.
Theorem 2. Applying AFW-1 to SLP  the gap decays as ht ≤ c0(1 − c1)t−1 for all t ≥ n0.
Proof. By Lemma 3  hn0≤3M2

log n0 = c0(1 − c1)n0−1. Let the result hold for some t ≥ n0. Then

t ≥ 1  and c) ηt ∈ [ 1

t+1 − γ−1

4 γt  γt].

n0

M1
θM2
M 2
1
M2

θ − 4
4θ2 <

1
200

(cid:24) 1

(cid:25)

ht+1 ≤ ht + ηt (cid:104)dt ∇f (xt)(cid:105) +

β
2

η2
t D2

(cid:10)v+
(cid:114) α

t  ∇f (xt)(cid:11) +
t − v−
(cid:112)

ht +

η2
t D2

β
2

2Hs

(smoothness of f)

(by step 5 of Algorithm 1)

β
η2
t D2
2
(by (4) and the fact (cid:104)x∗ − xt ∇f (xt)(cid:105) ≤ −ht)

M1γth1/2

(Lemma 4c and the defn. of M1)

(12)

(13)

(14)

(15)

(16)

(17)

≤ ht +
ηt
2
≤ ht − ηt
2
≤ ht − 1
4
= ht − M 2
4θM2
≤ c0(1 − c1)t−1

√

1

β
2

γ2
t D2

(cid:18)

t +
c0(1 − c1)(t−1)/2h1/2
M 2
1 − M 2
1
θ2M2
4θM2

+

1

t +

(cid:19)

6

c0(1 − c1)t−1

M 2
1
θ2M2
= c0(1 − c1)t

(by defn. of c1).

(by defn. of γt)

Here the inequality in step (17) is by treating (16) as a quadratic of h1/2
assumption on ht. The last step completes the induction: the conclusion also holds for step t + 1.

and applying the induction

t

3.5 Analysis for Away-step Frank-Wolfe with Line Search (AFW-2)
We ﬁnally analyze AFW-2 on general polytopes with line search. Noting that f (xt + ηdt)− f (x∗) ≤
(14) (with ηt in (14) replaced by η)  we minimize both sides over η : xt + ηdt ∈ P. If none of the
inequality constraints are satisﬁed as equality at the optimal ηt of line search  then we call it a good
step and in this case

(cid:18)

(cid:19)

ht+1 ≤

1 −

α

256βD2Hs

ht 

(Eq 14 in η is minimized at η∗

t :=

1
βD2 M1h1/2

t

).

(18)

The only task left is to bound the number of bad steps (i.e. ηt clamped by its upper bound). In [13]
where the set of atoms is maintained  it is easily shown that up to step t there can be only at most t/2
bad steps  and so the overall rate of convergence is slowed down by at most a factor of two. This
favorable result no longer holds in our decomposition-invariant AFW. However  thanks to the special
property of AFW  it is still not hard to bound the number of bad steps between two good steps.
t ≤ 1 and for FW-steps 
First we notice that such clamping never happens for FW-steps  because η∗
xt + ηtdt ∈ P implicitly enforces ηt ≤ 1 only (after ηt ≥ 0 is imposed). For an away-step  if the
line search is blocked by some constraint  then at least one inequality constraint will turn into an
equality constraint if the next step is still away. Since AFW selects the away-direction by respecting
all equality constraints  the succession of away-steps (called an away epoch) must terminate when the
set of equalities deﬁne a singleton. For any index set of inequality constraints S ⊆ [m]  let P(S) :=
{x ∈ P : (cid:104)aj  x(cid:105) = bj  ∀ j ∈ S} be the set of points that satisfy these inequalities with equality. Let
(19)

n(P) := max{|S| : S ⊆ [m]  |P(S)| = 1  |P(S(cid:48))| = ∞ for all S(cid:48) (cid:40) S}

be the maxi-min number of constraints to deﬁne a singleton. Then obviously n(P) ≤ n  and so
Theorem 3. To ﬁnd an  accurate solution  AFW-2 requires at most O
steps.

(cid:16) nβD2Hs

α

(cid:17)

log 1


2 (cid:107)x + 1(cid:107)2 with P = [0  1]n. Clearly n(P) = n. Unfortunately
Example 5. Suppose f (x) = 1
we can construct an initial x1 as a convex combination of only O(log n) vertices  but AFW-2 will
then run O(n) number of away-steps consecutively. Hence our above analysis on the max length of
away epoch seems tight  although having n consecutive away-steps between two good steps once is
different than this happening multiple times. See the construction of x1 in Appendix A.
Tighter bounds. By reﬁning the analysis of the polytopes  we may improve upon the n(P) bound.
For example it is not hard to show that n(Pk) = n(Qk) = n. Let us consider the number of non-zeros
in the iterates xt. A bad step (which must be an away-step) will either a) set an entry to 1  which will
force the corresponding entry of v−
to be 1 in the future steps of the away epoch  hence can happen
at most k times; or b) set at least one nonzero entry of xt into 0  and will never switch a zero entry to
nonzero. But each FW-step may introduce at most k nonzeros. So the number of bad steps cannot be
over 2k times of that of FW-step  and the overall iteration complexity is at most O( kβD2Hs
 ).
We can now revisit Table 1 and observe the generality and efﬁciency of AFW-2. It is noteworthy that
on SLPs  we are not yet able to establish the same rate as AFW-1. We believe that the vertices being
binary is very special  making it hard to generalize the analysis.

log 1

α

t

4 Application to Kernel Binary SVM

As a concrete example  we apply AFW to the dual objective of a binary SVM with bias:

min

x

f (x) := 1

2 x(cid:62)Qx − 1

C 1(cid:62)x  s.t. x ∈ [0  1]n  y(cid:62)x = 0.

(SVM-Dual)
(20)
Here y = (y1  . . .   yn)(cid:62) is the label vector with yi ∈ {−1  1}  and Q is the signed kernel matrix
with Qij = yiyjk(xi  xj). Since the feasible region is an SLP with diameter O(
n)  we can use
both AFW-1 and PFW-1 to solve it with O(#SV · nκ log 1
 ) iterations  where κ is the ratio between
the maximum and minimum eigenvalues of Q (assume Q is positive deﬁnite)  and #SV stands for the
number of support vectors in the optimal solution.

√

7

t = Q(v+

t and v−

t − xt) = −∇f (xt) − 1

Computational efﬁciency per iteration. The key technique for computational efﬁciency is to keep
updating the gradient ∇f (x) over the iterations  exploiting the fact that v+
t might be sparse
and ∇f (x) = Qx − 1
C 1 is afﬁne in x. In particular  when AFW takes a FW-step in line 5  we have
Qdt = QdFW
(21)
Similar update formulas can be shown for away-step dA
t ) has k
non-zeros  all these three updates can be performed in O(kn) time. Based on them  we can update
the gradient by ∇f (xt+1) = ∇f (xt) + ηtQdt. The FW-oracle and away-oracle cost O(n) time
given the gradient  and the line search has a closed form solution. See more details in Appendix B.
t and v−
Major drawback. This approach unfortunately provides no control of the sparseness of v+
t .
As a result  each iteration may require evaluating the entire kernel matrix (O(n2) kernel evaluations) 
leading to an overall computational cost O(#SV · n3κ log 1

C 1 + Qv+
t .
. So if v+ (or v−

t and PFW-step dPFW

 ) . This can be prohibitive.

t

4.1 Reformulation by Relaxed Convex Hull

To ensure the sparsity of each update  we reformulate the SVM dual objective (20) by using the
reduced convex hull (RC-Hull  [22]). Let P and N be the set of positive and negative examples  resp.

1
K

(1(cid:62)ξ+ + 1(cid:62)ξ−) +

(cid:107)θ(cid:107)2 − α + β 

1
2

(RC-Margin)

min

θ  ξ+∈R|P |  ξ−∈R|N|  α  β
s.t. A(cid:62)θ − α1 + ξ+ ≥ 0  −B(cid:62)θ + β1 + ξ− ≥ 0  ξ+ ≥ 0  ξ− ≥ 0.

(22)

1

2 (cid:107)Au − Bv(cid:107)2  

s.t. u ∈ PK  v ∈ PK.

(RC-Hull)

min

u∈R|P | v∈R|N|

(23)
Here A (or B) is a matrix whose i-th column is the (implicit) feature representation of the i-th positive
(or negative) example. RC-Margin resembles the primal SVM formulation  except that the bias term
is split into two terms α and β. RC-Hull is the dual problem of RC-Margin  and it has a very intuitive
geometric meaning. When K = 1  RC-Hull tries to ﬁnd the distance between the convex hull of P
K Au is a reduced convex hull of the positive
and N. When the integer K is greater than 1  then 1
examples  and the objective ﬁnds the distance of the reduced convex hull of P and N.
Since the feasible region of RC-Hull is a simplex  dt in AFW and PFW have at most 2K and 4K
nonzeros respectively  and it costs O(nK) time to update the gradient (see Appendix B.1). Given
K  Appendix B.2 shows how to recover the corresponding C in (20)  and to translate the optimal
solutions. Although solving RC-Hull requires the knowledge of K (which is unknown a priori if we
are only given C)  in practice  it is equally justiﬁed to tune the value of K via model selection tools
in the ﬁrst place  which is approximately tuning the number of support vectors.

4.2 Discussion and Comparison of Rates of Convergence

Clearly  the feasible region of RC-Hull is an SLP  allowing us to apply AFW-1 and PFW-1 with
optimal linear convergence: O(#SV · κK log 1
 )  because K = 1(cid:62)u ≤ #SV.
So overall  the computational cost is O(nκ(#SV)3 log 1

 ) ≤ O(κ(#SV)2 log 1

 ).

 ) when #SV ≤ n2/3. [24] requires O(κ2n(cid:107)Q(cid:107)sp log 1

 ) computations. This
[20] shows sequential minimal optimization (SMO) [19  23] costs O(n3κ log 1
is greater than O(nκ(#SV)3 log 1
 ) iterations 
and each iteration costs O(n). SVRG [25]  SAGA [26]  SDCA [27] require losses to be decomposable
and smooth  which do not hold for hinge loss with a bias. SDCA can be extended to almost smooth
losses such as hinge loss  but still the dimension dependency is unclear and it cannot handle bias.
As a ﬁnal remark  despite the superior rates of AFW-1 and PFW-1  their pre-deﬁned step size makes
them impractical. With line search  AFW-2 is much more efﬁcient in practice  and at the same time
provides theoretical guarantees of O(nκ(#SV)4log 1
 ) computational cost  just slightly worse by #SV
times. Such an advantage in both theory and practice by one method is not available in PFW [18].

5 Experiments and Future Work

In this section we compare the empirical performance of AFW-2 against related methods. We ﬁrst
illustrate the performance on kernel binary SVM  then we investigate a problem whose domain is not
an SLP  and ﬁnally we demonstrate the scalability of AFW-2 on a large scale dataset.

8

(a) Breast-cancer (K = 10)

(b) a1a (K = 30)

(c) ijcnn1 (K = 20)

Figure 1: Comparison of SMO and AFW-2 on three different datasets

Binary SVM Our ﬁrst comparison is on solving kernel binary SVMs with bias. Three datasets are
used. breast-cancer and a1a are obtained from the UCI repository [28] with n = 568 and 1  605
training examples respectively  and ijcnn1 is from [29] with a subset of 5  000 examples.
As a competitor  we adopted the well established Sequential Minimal Optimization (SMO) algorithm
[19]. The implementation updates all cached errors corresponding to each examples if any variable is
being updated at each step. Using these cached error  the algorithm heuristically picks the best subset
of variable to update at each iteration.
We ﬁrst run AFW-2 on the RC-Hull objective in (23)  with the value of K set to optimize the test
accuracy (K shown in Figure 1). After obtaining the optimal solution  we compute the equivalent C
value based on the conversion rule in Appendix B.2  and then run SMO on the dual objective (20).
Figure 1 shows the decay of the primal SVM objective (hence
ﬂuctuation) as a function of (the number of kernel evaluations
divided by n). This avoids the complication of CPU frequency
and kernel caching. Clearly  AFW-2 outperforms SMO on breast-
cancer and ijcnn1  and overtakes SMO on a1a after a few iterations.
PFW-1 and PFW-2 are also applicable to the RC-Hull formulation.
Although the rate of PFW-1 is better than AFW-2  it is much slower
in practice. Although empirically we observed that PFW-2 is similar
to our AFW-2  unfortunately PFW-2 has no theoretical guarantee.
General Polytope Our next comparison uses Qk as the domain.
Since it is not an SLP  neither PFW-1 nor PFW-2 provides a bound.
Here we aim to show that AFP-2 is not only advantageous in pro-
viding a good rate of convergence  it is also comparable to (or better
than) PFW-2 in terms of practical efﬁciency. Our objective is a least
square (akin to lasso):

Figure 2: Least square w. Q375

minx f (x) = (cid:107)Ax − b(cid:107)2 

0 ≤ x ≤ 1 

1(cid:62)x ≤ 375.

Here A ∈ R100×1000  and both A and b were generated randomly.
Both the FW-oracle and away-oracle are simply based on sorting
the gradient. As shown in Figure 2  AFW-2 is indeed slightly faster than PFW-2.
Scalability To demonstrate the scalability of AFP-2  we plot its convergence curve (K = 100)
along with SMO on the full ijcnn1 dataset with 49  990 examples. In Figure 3  AFW-2 starts with
a higher primal objective value  but after a while it outperforms SMO near the optimum. In this
problem  kernel evaluation is the major computational bottleneck  hence used as the horizontal axis.
This also helps avoiding the complication of CPU speed (e.g. when wall-clock time is used).
6 Future work

Figure 3: Full ijcnn1

We will extend the decomposition invariant method to gauge regularized problems [30–32]  and
derive comparable linear convergence rates. Moreover  although it is hard to evaluate pyramidal
width  it will be valuable to compare it with Hs even in terms of upper/lower bounds.
Acknowledgements. We thank Dan Garber for very helpful discussions and clariﬁcations on [18].
Mohammad Ali Bashiri is supported in part by NSF grant RI-1526379.

9

050100150# Kernel evaluations / # of examples101102103104Primal ObjectiveAFW-2SMO0204060# Kernel evaluations / # of examples103104105106Primal ObjectiveAFW-2SMO03006009001200# Kernel evaluations / # of examples103104105106107Primal ObjectiveAFW-2SMO0204060# steps10-2010-101001010GapAFW-2PFW-22505007501000# Kernel evaluations / # of examples104105106107108Primal ObjectiveAFW-2SMOReferences

[1] M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics

Quarterly  3(1-2):95–110  1956.

[2] Z. Harchaoui  M. Douze  M. Paulin  M. Dudik  and J. Malick. Large-scale image classiﬁcation
with trace-norm regularization. In Proc. IEEE Conf. Computer Vision and Pattern Recognition.
2012.

[3] E. S. Levitin and B. T. Polyak. Constrained minimization methods. USSR Computational

Mathematics and Mathematical Physics  6(5):787–823  1966.

[4] M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In Proceedings

of International Conference on Machine Learning. 2013.

[5] A. Beck and M. Teboulle. A conditional gradient method with linear rate of convergence for
solving convex linear systems. Mathematical Methods of Operations Research  59(2):235–247 
2004.

[6] J. Gu´eLat and P. Marcotte. Some comments on Wolfe’s ‘away step’. Mathematical Program-

ming  35(1):110–119  1986.

[7] P. Wolfe. Convergence theory in nonlinear programming. In Integer and Nonlinear Program-

ming. North-Holland  1970.

[8] S. D. Ahipasaoglu  P. Sun  and M. J. Todd. Linear convergence of a modiﬁed Frank-Wolfe algo-
rithm for computing minimum-volume enclosing ellipsoids. Optimization Methods Software 
23(1):5–19  2008.

[9] R. ˜Nanculef  E. Frandi  C. Sartori  and H. Allende. A novel Frank-Wolfe algorithm. analysis

and applications to large-scale svm training. Information Sciences  285(C):66–99  2014.

[10] P. Kumar and E. A. Yildirim. A linearly convergent linear-time ﬁrst-order algorithm for support
vector classiﬁcation with a core set result. INFORMS J. on Computing  23(3):377–391  2011.
[11] S. M. Robinson. Generalized equations and their solutions  part II: Applications to nonlinear

programming. Springer Berlin Heidelberg  1982.

[12] D. Garber and E. Hazan. A linearly convergent variant of the conditional gradient algorithm
under strong convexity  with applications to online and stochastic optimization. SIAM Journal
on Optimization  26(3):1493–1528  2016.

[13] S. Lacoste-Julien and M. Jaggi. On the global linear convergence of Frank-Wolfe optimization

variants. In Neural Information Processing Systems. 2015.

[14] S. Lacoste-Julien and M. Jaggi. An afﬁne invariant linear convergence analysis for Frank-Wolfe

algorithms. In NIPS 2013 Workshop on Greedy Algorithms  Frank-Wolfe and Friends. 2013.

[15] A. Beck and S. Shtern. Linearly convergent away-step conditional gradient for non-strongly

convex functions. Mathematical Programming  pp. 1–27  2016.

[16] G. Lan. The complexity of large-scale convex programming under a linear optimization oracle.

Technical report  University of Florida  2014.

[17] D. Garber. Faster projection-free convex optimization over the spectrahedron.

Information Processing Systems. 2016.

In Neural

[18] D. Garber and O. Meshi. Linear-memory and decomposition-invariant linearly convergent
conditional gradient algorithm for structured polytopes. In Neural Information Processing
Systems. 2016.

[19] J. C. Platt. Sequential minimal optimization: A fast algorithm for training support vector

machines. Tech. Rep. MSR-TR-98-14  Microsoft Research  1998.

[20] N. List and H. U. Simon. SVM-optimization and steepest-descent line search. In S. Dasgupta

and A. Klivans  eds.  Proc. Annual Conf. Computational Learning Theory. Springer  2009.

[21] J. Pena and D. Rodriguez.
[22] K. P. Bennett and E. J. Bredensteiner. Duality and geometry in SVM classiﬁers. In Proceedings

of International Conference on Machine Learning. 2000.

10

[23] S. S. Keerthi  O. Chapelle  and D. DeCoste. Building support vector machines with reduced

classiﬁer complexity. Journal of Machine Learning Research  7:1493–1515  2006.

[24] Y. You  X. Lian  J. Liu  H.-F. Yu  I. S. Dhillon  J. Demmel  and C.-J. Hsieh. Asynchronous

parallel greedy coordinate descent. In Neural Information Processing Systems. 2016.

[25] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Neural Information Processing Systems. 2013.

[26] A. Defazio  F. Bach  and S. Lacoste-Julien. SAGA: A fast incremental gradient method with
In Neural Information Processing

support for non-strongly convex composite objectives.
Systems. 2014.

[27] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research  14:567–599  2013.

[28] M. Lichman. UCI machine learning repository  2013. URL http://archive.ics.uci.edu/

ml.

[29] D. Prokhorov. Ijcnn 2001 neural network competition. Slide presentation in IJCNN  1:97  2001.
[30] M. Jaggi and M. Sulovsky. A simple algorithm for nuclear norm regularized problems. In

Proceedings of International Conference on Machine Learning. 2010.

[31] X. Zhang  Y. Yu  and D. Schuurmans. Accelerated training for matrix-norm regularization: A

boosting approach. In Neural Information Processing Systems. 2012.

[32] Z. Harchaoui  A. Juditsky  and A. Nemirovski. Conditional gradient algorithms for norm-

regularized smooth convex optimization. Mathematical Programming  152:75–112  2015.

11

,Shixiang (Shane) Gu
Zoubin Ghahramani
Richard Turner
Mohammad Ali Bashiri
Xinhua Zhang