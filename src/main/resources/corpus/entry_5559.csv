2016,Threshold Bandits  With and Without Censored Feedback,We consider the \emph{Threshold Bandit} setting  a variant of the classical multi-armed bandit problem in which the reward on each round depends on a piece of side information known as a \emph{threshold value}. The learner selects one of $K$ actions (arms)  this action generates a random sample from a fixed distribution  and the action then receives a unit payoff in the event that this sample exceeds the threshold value. We consider two versions of this problem  the \emph{uncensored} and \emph{censored} case  that determine whether the sample is always observed or only when the threshold is not met. Using new tools to understand the popular UCB algorithm  we show that the uncensored case is essentially no more difficult than the classical multi-armed bandit setting. Finally we show that the censored case exhibits more challenges  but we give guarantees in the event that the sequence of threshold values is generated optimistically.,Threshold Bandit  With and Without Censored

Feedback

Jacob Abernethy

Kareem Amin

Department of Computer Science

Department of Computer Science

University of Michigan
Ann Arbor  MI 48109
jabernet@umich.edu

University of Michigan
Ann Arbor  MI 48109
amkareem@umich.edu

Ruihao Zhu

AeroAstro&CSAIL

MIT

Cambridge  MA 02139

rzhu@mit.edu

Abstract

We consider the Threshold Bandit setting  a variant of the classical multi-armed
bandit problem in which the reward on each round depends on a piece of side
information known as a threshold value. The learner selects one of K actions
(arms)  this action generates a random sample from a ﬁxed distribution  and the
action then receives a unit payoff in the event that this sample exceeds the threshold
value. We consider two versions of this problem  the uncensored and censored case 
that determine whether the sample is always observed or only when the threshold is
not met. Using new tools to understand the popular UCB algorithm  we show that
the uncensored case is essentially no more difﬁcult than the classical multi-armed
bandit setting. Finally we show that the censored case exhibits more challenges  but
we give guarantees in the event that the sequence of threshold values is generated
optimistically.

1

Introduction

The classical Multi-armed Bandit (MAB) problem provides a framework to reason about sequential
decision settings  but speciﬁcally where the learner’s chosen decision is intimately tied to information
content received as feedback. MAB problems have generated much interest in the Machine Learning
research literature in recent years  particularly as a result of the changing nature in which learning
and estimation algorithms are employed in practice. More and more we encounter scenarios in which
the procedure used to make and exploit algorithmic predictions is exactly the same procedure used to
capture new data to improve prediction performance. In other words it is increasingly harder to view
training and testing as distinct entities.
MAB problems generally involve repeatedly making a choice between one of a ﬁnite (or even inﬁnite)
set of actions  and these actions have historically been referred to as arms of the bandit. If we “pull”
arm i at round t  then we receive a reward Rt
i 2 [0 1] which is frequently assumed to be a stochastic
quantity that is drawn according to distribution Di. Typically we assume that Di are heterogeneous
across the arms i  whereas we assume the samples {Rt
i}t=1 ... T are independently and identically
distributed according to the ﬁxed Di across all times t.1 Of course  were the learner to have full
knowledge of the distributions Di from the outset  she would presumably choose to pull the arm
whose expected reward µi is highest. With that in mind  we tend to consider the (expected) regret of
the learner  deﬁned to be the (expected) reward of the best arm minus the (expected) reward of the
actual arms selected by the learner.
Early work on MAB problems (Robbins  1952; Lai and Robbins  1985; Gittins et al.  2011) tended to
be more focused on asymptotic guarantees  whereas more recent work (Auer et al.  2002; Auer  2003)

1Note that in much of our notation we use superscript t to denote the time period rather than as an exponent.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

has been directed towards a more “ﬁnite time” approach in which we can bound regret for ﬁxed time
horizons T . One of the best-known and well-studied techniques is known as the Upper Conﬁdence
Bound (UCB) algorithm (Auer et al.  2002; Auer and Ortner  2010). The magic of UCB relies on a
very intuitive policy framework  that a learner should select decisions by maximizing over rewards
estimated from previous data but only after biasing each estimate according to its uncertainty. Simply
put  one should choose the arm that maximizes the “mean plus conﬁdence interval ” hence the name
Upper Conﬁdence Bound.
In the present paper we focus on the Threshold Bandit setting  described as follows. On each round t 
a piece of side information is given to the learner in the form of a real number ct  the learner must
then choose arm i out of K arms  and this arm produces a value Xt
i drawn from a survival distribution
with survival function Fi(x) = Pr(Xt
i itself but is instead
i = I[Xt
the binary value Rt
i exceeds the
threshold value ct  and otherwise we receive no reward. For a ﬁxed value of ct  each arm i has
i] = Fi(ct). Notice  crucially  that the arm with the greatest expected payoff can
expected payoff E[Rt
vary signiﬁcantly across different threshold values.
This abstract model has a number of very natural applications:

i  ct]; that is  we receive a unit reward when the sample Xt

i  x). The reward to the learner is not Xt

1. Packet Delivery with Deadlines: FedEx receives a stream of packages that need to be
shipped from source to destination  and each package is supplied with a delivery deadline.
The goal of the FedEx routing system is to select a transportation route (via air or road or
ship  etc.) that has the highest probability of on-time arrival. Of course some transportation
schemes are often faster (e.g. air travel) but have higher volatility (e.g. due to poor weather).
2. Supplier Selection: Customers approach a manufacturing ﬁrm to produce a product with
speciﬁc quality demands. The ﬁrm must approach one of several suppliers to contract out
the work  but the ﬁrm is uncertain as to the capabilities and variabilities of the products each
supplier produces.

3. Dark Pool Brokerage: A ﬁnancial brokerage ﬁrm is asked to buy or sell various sized
bundles of shares  and the brokerage aims to ofﬂoad the transactions onto one of many
dark pools  i.e. ﬁnancial exchanges that match buyers and sellers in a conﬁdential manner
(Ganchev et al.  2010; Amin et al.  2012; Agarwal et al.  2010). A standard dark pool
mechanism will simply execute the transaction if there is suitable liquidity  or will reject the
transaction when no match is made. Of course the brokerage gets paid on commission  and
simply wants to choose the pool that has the highest probability of completion.

What distinguishes the Threshold Bandit problem from the standard stochastic multi-armed bandit
setting are two main features:

1. The regret of the learner will be measured in comparison to the best policy rather than to
simply the best arm. Note that the optimal ofﬂine policy may incorporate the threshold
value ct before selecting an arm I t.

2. Whereas the standard stochastic bandit setting assumes that we observe the reward Rt
I t of
the chosen arm I t  in the Threshold Bandit setting we consider two types of feedback.
(a) Uncensored Feedback: After playing arm I t  the learner observes the sample Xt
I t
regardless of the threshold value ct. This is a natural model for the FedEx routing
problem above  wherein one learns the travel time of a package regardless of the
deadline having been met.
I t  ct 
and otherwise observes Xt
I t . This is a natural model for the Supplier Selection problem
above  as we would only learn the product’s quality value when the customer rejects
what is received from the supplier.

(b) Censored Feedback: After playing I t  the learner observes a null value when Xt

In the present paper we present roughly three primary results. First  we provide a new perspective
on the classical UCB algorithm  giving an alternative proof that relies on an interesting potential
function argument; we believe this technique may be of independent interest. Second  we analyze
the Threshold Bandit setting when given uncensored feedback  and we give a novel algorithm called
DKWUCB based on the Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al.  1956). We show 
somewhat surprisingly  that with uncensored feedback the regret bound is no worse than the standard

2

stochastic MAB setting  suggesting that despite the much richer policy class one has nearly the same
learning challenge. Finally  we consider learning in the censored feedback setting  and propose an
algorithm KMUCB  akin to the Kaplan-Meier estimator (Kaplan and Meier  1958). Learning with
censored feedback is indeed more difﬁcult  as the performance can depend signiﬁcantly on the order
of the threshold values. In the worst case  when threshold values are chosen in an adversarial order 
the cost of learning scales with the number of unique threshold values  whereas one can perform
signiﬁcantly better under certain constraints on optimistic assumptions on the order or even a random
order.

2 A New Perspective on UCB

Before focusing on the Threshold Bandit problem  let us turn our attention to the classical stochastic
MAB setting and give another look at the UCB algorithm. We will now provide a modiﬁed proof
of the regret bound of UCB that relies on a potential function. Potential arguments have proved
quite popular in studying adversarial bandit problems (Auer et al.  2003; Audibert and Bubeck  2009;
Abernethy et al.  2012; Neu and Bartók  2013; Abernethy et al.  2015)  but have received less use
in the stochastic setting. This potential trick is the basis for forthcoming results on the Threshold
Bandit.
Let Di be a distribution on the reward Rt
i  with support on [0 1]. We imagine the rewards
R1
i   . . .  RT
i] = µi. A bandit algorithm is simply a procedure that chooses
i
a random arm/action I t on round t as a function of the set of past observed (action  reward) pairs 
t=1 I[I t = i] and deﬁne the empirical mean estimator
(I 1 R1
at time t to be ˆµt

I 1)  . . .   (I t1 Rt1
I t1). Finally  let Nt
t=1 I[I t=i]Rt
Ât1
I t

i.i.d.⇠ Di  whose mean E[Rt

i := Ât1

.

i :=

Nt
i

We assume we are given a particular deviation bound which provides the following guarantee 

Pr|µi  ˆµt

i| > e  Nt

i  N  f (N e) 

where f (·) is some function  continuous in e and monotonically decreasing in both parameters  that
controls the probability of a large deviation. While UCB relies speciﬁcally on the Hoeffding-Azuma
inequality (Cesa-Bianchi and Lugosi  2006)  for now we leave the deviation bound in generic form.
This will be useful in following sections.
Given f (· ·)  what is of interest to our present work is a pair of functions that allow us to convert
between values of e and N in order to guarantee that f (N e)  d for a given d > 0. To this end deﬁne

](e d)

V(N d)

:= min{N : f (N e/2)  d} 
:= ⇢

inf{e : f (N e)  d} if N > 0;
otherwise 
1

We will often omit the d in the argument to ](·) V(·). Note the key property that V(N d)  e/2 for
any N  ](e d).
We can now deﬁne our variant of the UCB algorithm for a ﬁxed choice of d > 0.
i + V(Nt

on round t play I t = argmax

UCB Algorithm:

(1)

ˆµt

i

i  d) 

We will make the simplifying assumption that the largest µi is unique and  without loss of generality  let
us assume that the coordinates are permuted in order that µ1 is the largest mean reward. Furthermore 
deﬁne Di := µ1  µi for i = 2  . . .  K.
A central piece of the analysis relies on the following potential function  which depends on the current
number of plays of each arm i = 2  . . .  K.

K

Nt
i 1
Â

N=0

V(N d)

(2)

F(Nt

Â
i=2
Lemma 1. The expected regret of UCB is bounded as
E[RegretT (UCB)]  E[F(NT +1

2  . . .  Nt

K) := 2

2

  . . .  NT +1

K

)] + O(Td)

3

1 d)

1 + V(Nt

µ1  ˆµt

Proof. The (random) additional regret suffered on round t of UCB is exactly µ1  µI t . By virtue of
our given deviation bound  we know that
ˆµt
I t  µI t + V(Nt
(3)
Also  let xt be the indicator variable that one of the above two inequalities fails to hold. Of course we
chose V(·) in order that E[xt]  2d via a simple union bound.
Note that  by virtue of using the UCB selection rule for I t  it is clear that we have
I t + V(Nt

each w.p. > 1 d.

ˆµt
1 + V(Nt

I t  d) 

I t  d)

and

(4)

If we combine Equations 3 and 4  and consider the event that xt = 0  then we obtain

1 d)  ˆµt
I t + V(Nt

µ1  ˆµt

1 + V(Nt

1 d)  ˆµt

I t  d)  µI t + 2V(Nt

I t  d).

I t  d) + xt.

Even in the event that xt = 1 we have that µ1µI t  1. Hence  it follows immediately that µ1µI t 
2V(Nt
Finally  we observe that
F(Nt

K ) 
I t  d). Recalling that F(0  . . .  0) = 0  a simple telescoping argument gives that

the potential function was chosen so that F(Nt+1

K) = 2V(Nt

  . . .  Nt+1

2  . . .  Nt

2

E[RegretT (UCB)]  E"F(NT +1

2

  . . .  NT +1

K

) +

T

Â

t=1

xt# = E[F(NT +1

2

  . . .  NT +1

K

)] + 2Td.

The ﬁnal piece we need to establish is that the number of pulls Nt
i of arm i  for i = 2  . . .  K  is unlikely
to exceed ](Di d). This result uses some more standard techniques from the original UCB analysis
(Auer et al.  2002)  and we defer it to the appendix.
Lemma 2. For any T > 0 we have E[F(NT +1
We are now able to combine the above results for the ﬁnal bound.
Theorem 1. If we set d = T2/2  the expected regret of UCB is bounded as

)]  F(](D2 d)  . . .  ] (DK d)) + O(T 2d).

  . . .  NT +1

K

2

E[RegretT (UCB)]  8

K

Â

i=2

log(T )

Di

+ O(1).

Proof. Note that a very standard deviation bound that holds for all distributions supported on
[0 1] is the Hoeffding-Azuma inequality (Cesa-Bianchi and Lugosi  2006)  where the bound is

given by f (N e) = 2exp(2Ne2). Utilizing Hoeffding-Azuma we have ](e d) =l 2log2/d
V(N d) =q log(2/d)
1py  2pY   then we see that

for N > 0. If we utilize the fact that ÂY

m and

y=1

2N

e2

F(](D2 d)  . . .  ] (DK d)) = 2

Â
i=2
Combining the Lemma 1 and Lemma 2  setting d = T2/2  we conclude the theorem.

V(N d) = 2

Â

Â

= 4

N=0

i=2

i=2

K

2rlog(2/d)](Di d)

2

K

log(2/d)

.

Di

K

](Di d)
Â

3 The Threshold Bandits Model

In the preceding  we described a potential-based proof for the UCB algorithm in the classic stochastic
bandit problem. We now return to the Threshold Bandit setting  our problem of interest.
A K-armed Threshold Bandit problem is deﬁned by random variables Xt
i and a sequence of threshold
values ct for 1  i  K and 1  t  T  where i is the index for arms. Successive pulling of arm
i generates the values X 1
i   which are drawn i.i.d. from an unknown distribution. The
threshold values c1 c2  . . .  cT are drawn from M = {1 2  . . .  m} (according to rules speciﬁed later).
The threshold value ct is observed at the beginning of round t  and the learner follows a policy P to
choose the arm to play based on its past selections and previously observed feedbacks. Suppose the
arm pulled at round t is I t  the observed reward is then Rt
I t  ct]; that is  we receive a unit
reward when the sample Xt
I t exceeds the threshold value ct  and otherwise we receive no reward. We
distinguish two different types of feedback.

I t = I[Xt

i   . . .  X T

i  X 2

4

of the threshold value ct.

1. Uncensored Feedback: After playing arm I t  the learner observes the sample Xt
if Xt
I t  ct 
otherwise

2. Censored Feedback: After playing I t  the learner observes2⇢/0

In this case  we refer to the threshold value as a censor value.

Xt
I t

.

I t regardless

Let Fi(x) denote the survival function of the distribution on arm i. That is  Fi(x) = Pr(Xt
measure regret against the optimal policy with full knowledge of F1  . . .  Fn i.e. 

i  x). We

Notice that for a ﬁxed value of ct  each arm i has expected payoff E[Rt
be written as

RegretT (P) = E" T
t=1✓max
Â
RegretT (P) = E⇥ÂT

I t◆# = E" T
i  Rt
Rt
t=1maxi2[n] Fi (ct) FI t (ct)⇤ .

Our goal is to design a policy that minimizes the regret.

t=1✓max
Â

IXt
i  ctIXt

i2[n]

i2[n]

I t  ct◆# .

i] = Fi(ct)  the regret can also

4 DKWUCB: Dvoretzky-Kiefer-Wolfowitz Inequality based Upper

Conﬁdence Bound algorithm

In this section  we study the uncensored feedback setting in which the value Xt
I t is always observed
regardless of ct. We assume that the largest Fi( j) is unique for all j 2 M  and deﬁne i⇤( j) =
argmaxi Fi( j) Di( j) = Fi⇤( j)( j) Fi( j) for all i = 1 2  . . .  K and j 2 M.
Under this setting  the algorithm will use the empirical distribution as an estimate for the true
distribution. Speciﬁcally  we want to estimate the true survival function Fi via:

ˆFt
i ( j) =

t=1 I[Xt
Ât1

I t  j I t = i]
Nt
i

8 j 2 M

(5)

i ( j) :

than 2exp2e2Nt

The key tool in our analysis is a deviation bound on the empirical CDF of a distribution  and we
note that this bound holds uniformly over the support of the distribution. The Dvoretzky-Kiefer-
Wolfowitz (DKW) inequality (Dvoretzky et al.  1956) allows us to bound the error on ˆFt
Lemma 3. At a time t  let ˆFt
probability that the maximum of the difference between ˆFt

i be the empirical distribution function of Fi as given in equation 5. The
i and Fi over all j 2 M is at least e is less
i  N  2exp2e2N .

i ( j) Fi( j)| e  Nt

The proof of the lemma can be found in Dvoretzky et al. (1956). The key insight is that the estimate ˆFi
converges to Fi point-wise at the same rate as the Hoeffding-Asumza inequality. That is  one does not
pay an additional M factor from applying a union bound. The fact that we have uniform convergence
of the CDF with the same rate as the Hoeffding-Azuma inequality allows us to immediately apply

the potential function argument from Section 2. In particular  we deﬁne f (N e) = 2exp2e2N   as

well as the pair of functions ](e d) and V(N d) exactly the same as the previous section  i.e. 

i   i.e. 
Prsup j2M | ˆFt

](e d)

V(N d)

e2

⇡  
:= ⇠2log2/d
:= ( q log(2/d)

2N

1

if N > 0;
otherwise.

We are now ready to deﬁne our DKWUCB algorithm for a ﬁxed choice of parameter d > 0 to solve
the problem.

DKWUCB Algorithm:

on round t play I t argmax

i

i (ct) + V(Nt

 ˆFt

i  d) .

(6)

2Existing literature often refers to this as right-censoring. With right-censored feedback  samples from

playing arms at high threshold values can inform decisions at low threshold values but not vice versa.

5

To analyze DKWUCB  we use a slight variant of the potential function deﬁned in Section 2. Let
i⇤( j) = argmaxi Fi( j) denote the optimal arm for threshold value j  and ˜Nt
i denote the number of
rounds arm i is pulled when it is not optimal  ˜Nt
i  Nt
i .
Deﬁne the potential function as:

t=1 I[I t = i I t 6= i⇤(ct)]. Notice that ˜Nt

i = Ât1

F( ˜Nt

1  . . .   ˜Nt

K) := 2

V(N d)

(7)

K

Â

i=1

˜Nt
i 1
Â

N=0

Theorem 2. Setting d = T2/2  the expected regret of DKWUCB is bounded as

logT

K

Â

i=1

D2

+ O(1) 

min j2M Di( j)

E[RegretT (DKWUCB)]  8
We defer the proof of this theorem to the appendix.
We pause now to comment on some of the strengths of this type of analysis. At a high-level  the
typical analysis to the UCB algorithm for the standard multi-armed bandit problem (Auer et al. 

with high probability  and (2) the regret suffered by any such pull is O(Di). The contribution of arm

2002) is the following: (1) at some ﬁnite time T   the number of pulls of a bad arm i is O⇣ log(T )
i ⌘
Di ⌘. In contrast  we analyzed the UCB algorithm in Section 2
i to total regret is therefore O⇣ log(T )
by observing that the expected regret suffered on round t is bounded by the difference between the
empirical mean estimator and the true mean for the payoff of arm It. Of course by design this quantity
is almost certainly (w.p. at least 1 d) less than V(Nt
It ). The potential function F(·  . . .  ·) tracks the
accumulation of these values V(Nt
i ) for each arm i  and the ﬁnal regret bound is a consequence of the
summation properties of V ] for the particular estimator being used.
While these two approaches lead to the same bound in the standard multi-armed bandit problem 
the potential function approach bears fruit in the Threshold Bandit setting. Because the uniform
convergence rate promised by the DKW inequality matches that of the Hoeffding-Azume inequality 
Theorem 2 should not be surprising; the ith arm’s contribution to DKWUCB’s regret should be
idenitical to UCB  but with the suboptimality gap now equal to min j Di( j).
However  following the program for the standard analysis of UCB  one would naively argue that

log(T )

(min j2M Di( j))2⌘ times. These pulls might come in the face of any
arm i is incorrectly pulled O⇣
number of threshold values ct  suffering as much as max j2M Di( j) regret  yielding a bound of
min j Di( j)⌘ worse than
O⇣ max j2M Di( j)log(T )

(min j2M Di( j))2 ⌘ on the ith arm’s regret contribution  which is a factor O⇣ max j Di( j)

the derived result. By tracking the convergence of the underlying estimator  we circumvent this
problem entirely.

5 KMUCB: Kaplan-Meier based Upper Conﬁdence Bound Algorithm
We now turn to the censored feedback setting  in which the feedback of pulling arm I t is observed
only when Xt
I t is less than ct. For ease of presentation  we assume that the largest Fi( j) is unique for
all j 2 M  and deﬁne i⇤( j) = argmaxi Fi( j) Di( j) = Fi⇤( j)( j)Fi( j) for all i = 1 2  . . .  K and j 2 M.
One prevalent non-parametric estimator for censored data is the Kaplan-Meier maximum likelihood
estimator Kaplan and Meier (1958); Peterson (1983). Most of existing works have studied the uniform
error bound of Kaplan-Meier estimator in the case that the threshold values are drawn i.i.d. from a
known distribution Foldes and Rejto (1981) or asymptotic error bound for the non-i.i.d. case Huh
et al. (2009). The only known uniform error bound of Kaplan-Meier estimator is proposed in Ganchev
et al. (2010).
Noting that for a given threshold value  all the feedbacks from larger threshold values are useful  we
propose a new estimator with tighter uniform error bound based on the Kaplan-Meier estimator as
following:

ˆFt
i =

Dt
i( j)
Nt
i ( j)

6

(8)

where Dt

i( j) and Nt
At := min{Xt

i ( j) is deﬁned as follows
t1Â
I t  ct} 

i( j) :=

Dt

t=1

I[At  j I t = i] 

Nt
i ( j) :=

t1Â

t=1

I[ct  j I t = i].

We ﬁrst present an error bound for the modiﬁed Kaplan-Meier estimate of Fi( j) :
Lemma 4. At time t  let ˆFt
For any j 2 M  the probability that the difference between ˆFt
2exp⇣

i be the modiﬁed Kaplan-Meier estimate of Fi as given in equation 8.
i ( j) and Fi( j) is at least e is less than

2 ⌘   i.e. 

e2Nt

i ( j)

Pr| ˆFt

i ( j) Fi( j)| e  2exp✓

e2Nt

i ( j)

2 ◆ .

We defer the proof of this lemma to the appendix.
Different to the stochastic uncensored MAB setting  we show that the cost of learning with censored
feedback depends signiﬁcantly on the order of the threshold values. To illustrate this point  we ﬁrst
show a comparison between the regret of adversarial setting and optimistic setting. In the adversarial
setting  the threshold values are chosen to arrive in a non-decreasing order 1 1  . . .  1 2  . . .  2 3  . . .  m 
the problem becomes playing m independent copies of bandits  and the regret scales with m;
while in the optimistic setting  the threshold values are chosen to arrive in a non-increasing or-
der m m  . . .  m m  1  . . .  m  1  . . .  1  . . .  1  which means the learner can make full use of the
samples  and can thus perform signiﬁcantly better. Afterwards  we show that if the order of the
threshold values is close to uniformly random  the regret only scales with logm.

5.1 Adversarial vs. Optimistic Setting
For the simplicity of presentation  we assume that in both settings  the time horizon could be divided
in to m stages  each with length bT /mc.. In the adversarial setting  threshold value j comes during
stage j; while in the optimistic setting threshold value m j + 1 comes during stage j.
For the adversarial setting  due to the censored feedback structure  only the samples observed within
the same stage can help to inform decision making. From the perspective of the learner  this is
equivalent to facing m independent copies of stochastic MAB problems  and thus  the regret scales
with m. Making use of the lower bound of stochastic MAB problems Lai and Robbins (1985)  we can
conclude the following theorem.
Theorem 3. If the threshold values arrive according to the adversarial order speciﬁed above  no
learning algorithm can achive a regret bound better than Âm
KL(B(Fi( j)||B(Fi⇤( j)( j)))   where
KL(·||·) is the Kullback-Leibler divergence Lai and Robbins (1985) and B(·) is the probability
distribution function of Bernoulli distribution.

j=1 ÂK
i=1

log(T /m)

For the optimistic setting  although the feedbacks are right censored  we note that every sample
observed in the previous rounds are useful in later rounds. This is because the threshold values arrive
in non-increasing order. Therefore  we can reduce the optimistic setting to the Threshold Bandit
problem with uncensored feedback  and use the DKWUCB proposed in Section 4 to solve it. More
speciﬁcally  we can set

f (N e)

](e d)

V(N d)

:= 2exp(e2N/2) 
:= ⇠8log2/d
⇡  
:= ( q 2log(2/d)

e2

1

N

if N  1;
otherwise.

 

and on every round  the learner plays the same strategy as DKWUCB. We call this strategy OPTIM.
Following the same procedure in Section 4  we can provide a regret for OPTIM.
Theorem 4. Let d = T2/2 and assume T  mK. The regret of the optimistic setting satisﬁes

E[RegretT (OPTIM )]  32

7

K

Â

i=1

logT

min j2M Di( j)

+ O(1).

5.2 Cyclic Permutation Setting
In this subsection  we show that if the order of threshold values is close to uniformly random  we can
perform signiﬁcantly better than the adversarial setting. To be precise  we assume that the threshold
values are a cyclic permutation order of 1 2  . . .  m. We deﬁne the set M = {ckm ckm+1  . . .  ck(m+1)1}
for any non-negative integer k  T /m.
We are now ready to present KMUCB  which is a modiﬁed Kaplan-Meier-based UCB algorithm.
KMUCB divides the time horizon into epochs of length Km and  for each epoch  pulls each arm once
for each threshold value. KMUCB then performs an “arm elimination” process  and once all but one
arm has been eliminated  it proceeds to pull the single remaining arm for the given threshold value.
KMUCB’s estimation procedure leverages information across threshold values  where observations
from higher thresholds are utilized to estimate mean payoffs for lower thresholds; information does
not ﬂow in the other direction  however  as a result of the censoring assumption. Speciﬁcally  for a
given threshold index j  KMUCB tracks the arm elimination process as follows: for any threshold
values below j  KMUCB believes that we have determined the best arm  and plays that arm constantly.
For threshold values greater than or equal to j  KMUCB explores all arms uniformly. Note that by
uniform exploration over all arms for threshold value j  all sub-optimal arms can be detected with

probability at least 1 O 1

T after O✓

logT

(m j+1)mini2[K] D2

i ( j)◆ epochs. KMUCB then removes all the

sub-optimal arms for threshold value j  and increments j by 1. Denoting the last time unit of epoch k
as tk = kKm  the detailed description of KMUCB is shown in Algorithm 1.

Algorithm 1 KMUCB
1: Input: A set of arms 1 2  . . .  K.
2: Initialization: L j [K] 8 j 2 M k 1  j 1
3: for epoch k = 1 2  . . .  T /Km do
count[ j0] 0 8 j0 2 M
4:
for t from (tk1 + 1) to tk do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Observe ct = j0 and set count[ j0] count[ j0] + 1
if j0 < j then
I t index of the single arm remaining in L j0
I t count[ j0].

end for
if j  m and maxi02[K] ˆFtk

end if

else

i0 ( j) ˆFtk
L j (argmax

i ( j) q 16log(T k)
(m j+1)k 8i 2 L j \{argmaxi02[K] ˆFtk
i0 ( j))  

j j + 1

ˆFtk

i02[K]

i0 ( j)} then

end if

15:
16: end for

Theorem 5. The expected regret of KMUCB is bounded as
128max j2M Di( j)logT
mini2[K]  j2M D2
i ( j)

logm

Â

i=1

K

+ O(1).

We defer the proof of this theorem to the appendix.
We note two directions of future research. First  we believe the above bound can likely be made
stronger by either improving upon the minimization in the denominator or the maximization in the
numerator. Second  we believe the “cyclic permutation” assumption can be weakened to “uniformly
randomly sequence of thresholds ” but we were unable to make progress in this direction. We
welcome further investigation along these lines.

8

References
Jacob Abernethy  Elad Hazan  and Alexander Rakhlin. 2012.

Interior-point methods for full-
information and bandit online learning. IEEE Transactions on Information Theory 58  7 (2012) 
4164–4175.

Jacob D Abernethy  Chansoo Lee  and Ambuj Tewari. 2015. Fighting Bandits with a New Kind of

Smoothness. In Advances in Neural Information Processing Systems. 2188–2196.

Alekh Agarwal  Peter L Bartlett  and Max Dama. 2010. Optimal Allocation Strategies for the Dark

Pool Problem. In International Conference on Artiﬁcial Intelligence and Statistics. 9–16.

Kareem Amin  Michael Kearns  Peter Key  and Anton Schwaighofer. 2012. Budget optimization for

sponsored search: Censored learning in mdps. arXiv preprint arXiv:1210.4847 (2012).

Jean-Yves Audibert and Sébastien Bubeck. 2009. Minimax policies for adversarial and stochastic

bandits. In COLT. 217–226.

Peter Auer. 2003. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of

Machine Learning Research 3 (2003)  397–422.

Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. 2002. Finite-time analysis of the multiarmed

bandit problem. Machine learning 47  2-3 (2002)  235–256.

Peter Auer  Nicolò Cesa-Bianchi  Yoav Freund  and Robert E. Schapire. 2003. The Nonstochastic

Multiarmed Bandit Problem. SIAM Journal of Computuataion 32  1 (2003)  48–77.

Peter Auer and Ronald Ortner. 2010. UCB revisited: Improved regret bounds for the stochastic

multi-armed bandit problem. Periodica Mathematica Hungarica 61  1-2 (2010)  55–65.

Nicolò Cesa-Bianchi and Gábor Lugosi. 2006. Prediction  Learning  and Games. Cambridge

University Press.

A. Dvoretzky  J. Kiefer  and J. Wolfowitz. 1956. Asymptotic Minimax Character of the Sample
Distribution Function and of the Classical Multinomial Estimator. In Annals of Mathematical
Statistics.

A. Foldes and L. Rejto. 1981. Strong uniform consistency for nonparametric survival curve estimators

from randomly censored data. In The Annals of Statistics. 9(1):122?129.

Kuzman Ganchev  Michael Kearns  Yuriy Nevmyvaka  and Jennifer Wortman Vaughan. 2010. Cen-

sored Exploration and the Dark Pool Problem. In UAI.

John Gittins  Kevin Glazebrook  and Richard Weber. 2011. Multi-armed bandit allocation indices.

John Wiley & Sons.

W. T. Huh  R. Levi  P. Rusmevichientong  and J. Orlin. 2009. Adaptive data-driven inventory control
policies based on Kaplan-Meier estimator. In http://legacy.orie.cornell.edu/ paatrus/ psﬁles/km-
myopic.pdf.

E. L. Kaplan and P. Meier. 1958. Nonparametric Estimation from Incomplete Observations. In JASA.
T. L. Lai and Herbert Robbins. 1985. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics 6 (1985)  4–22.

Gergely Neu and Gábor Bartók. 2013. An efﬁcient algorithm for learning with semi-bandit feedback.

In Algorithmic Learning Theory. Springer  234–248.

A. V. Peterson. 1983. Kaplan-Meier estimator. In Encyclopedia of Statistical Sciences.
Herbert Robbins. 1952. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc.

58  5 (1952)  527–535.

9

,Jacob Abernethy
Kareem Amin
Ruihao Zhu