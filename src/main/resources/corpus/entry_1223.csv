2008,Correlated Bigram LSA for Unsupervised Language Model Adaptation,We propose using correlated bigram LSA for unsupervised LM adaptation for automatic speech recognition. The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. Our approach can be scalable to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation  unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results show that applying unigram and bigram LSA together yields 6%--8% relative perplexity reduction and 0.6% absolute character error rates (CER) reduction compared to applying only unigram LSA on the Mandarin RT04 test set. Comparing with the unadapted baseline  our approach reduces the absolute CER by 1.2%.,Correlated Bigram LSA for Unsupervised Language

Model Adaptation

Yik-Cheung Tam∗

Tanja Schultz

InterACT  Language Technologies Institute

InterACT  Language Technologies Institute

Carnegie Mellon University

Pittsburgh  PA 15213
yct@cs.cmu.edu

Carnegie Mellon University

Pittsburgh  PA 15213
tanja@cs.cmu.edu

Abstract

We present a correlated bigram LSA approach for unsupervised LM adaptation for
automatic speech recognition. The model is trained using efﬁcient variational EM
and smoothed using the proposed fractional Kneser-Ney smoothing which handles
fractional counts. We address the scalability issue to large training corpora via
bootstrapping of bigram LSA from unigram LSA. For LM adaptation  unigram
and bigram LSA are integrated into the background N-gram LM via marginal
adaptation and linear interpolation respectively. Experimental results on the Man-
darin RT04 test set show that applying unigram and bigram LSA together yields
6%–8% relative perplexity reduction and 2.5% relative character error rate reduc-
tion which is statistically signiﬁcant compared to applying only unigram LSA.
On the large-scale evaluation on Arabic  3% relative word error rate reduction is
achieved which is also statistically signiﬁcant.

1 Introduction

Language model (LM) adaptation is crucial to automatic speech recognition (ASR) as it enables
higher-level contextual information to be effectively incorporated into a background LM improving
recognition performance. Exploiting topical context for LM adaptation has shown to be effective
for ASR using latent semantic analysis (LSA) such as LSA using singular value decomposition [1] 
Latent Dirichlet Allocation (LDA) [2  3  4] and HMM-LDA [5  6]. One issue in LSA is the bag-
of-word assumption which ignores word ordering. For document classiﬁcation  word ordering may
not be important. But in the LM perspective  word ordering is crucial since a trigram LM normally
performs signiﬁcantly better than a unigram LM for word prediction. In this paper  we investigate
whether relaxing the bag-of-word assumption in LSA helps improving the ASR performance via
LM adaptation.

We employ bigram LSA [7] which is a natural extension of LDA to relax the bag-of-word assump-
tion by connecting the adjacent words in a document together to form a Markov chain. There are
two main challenges in bigram LSA which are not addressed properly in [7] especially for large-
scale application. Firstly  the model can be very sparse since it covers topical bigrams in O(V 2 · K)
where V and K denote the vocabulary size and the number of topics. Therefore  model smoothing
becomes critical. Secondly  model initialization is important for EM training  especially for bigram
LSA due to the model sparsity. To tackle the ﬁrst challenge  we represent bigram LSA as a set
of K topic-dependent backoff LM. We propose fractional Kneser-Ney smoothing 1 which supports
∗This work is partly supported by the Defense Advanced Research Projects Agency (DARPA) under Con-
tract No. HR0011-06-2-0001. Any opinions  ﬁndings and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reﬂect the views of DARPA.

1This method was brieﬂy mentioned in [8] without detail. To the best of our knowledge  our formulation in

this paper is considered new to the research community.

Prior distribution over topic mixture weights

θ

Latent topics

z1

z2

Observed words

<s>

w1

w2

zN

wN

Figure 1: Graphical representation of bigram LSA. Adjacent words in a document are linked to-
gether to form a Markov chain from left to right.

fractional counts to smooth each backoff LM. We show that our formulation recovers the original
Kneser-Ney smoothing [9] which supports only integral counts. To address the second challenge 
we propose a bootstrapping approach for bigram LSA training using a well-trained unigram LSA as
an initial model.

During unsupervised LM adaptation  word hypotheses from the ﬁrst-pass decoding are used to es-
timate the topic mixture weight of each test audio to adapt both unigram and bigram LSA. The
adapted unigram and bigram LSA are combined with the background LM in two stages. Firstly 
marginal adaptation [10] is applied to integrate unigram LSA into the background LM. Then the in-
termediately adapted LM from the ﬁrst stage is combined with bigram LSA via linear interpolation
with the interpolation weights estimated by minimizing the word perplexity on the word hypotheses.
The ﬁnal adapted LM is employed for re-decoding.

Related work includes topic mixtures [11] which perform document clustering and train a trigram
LM for each document cluster as an initial model. Sentence-level topic mixtures are modeled so that
the topic label is ﬁxed within a sentence. Topical N-gram model [12] focuses on phrase discovery
and information retrieval. We do not apply this model because the phrase-based LM seems not
outperform the word-based LM.

The paper is organized as follows: In Section 2  we describe the bigram LSA training and the
fractional Kneser-Ney smoothing algorithm. In Section 3  we present the LM adaptation approach
based on marginal adaptation and linear interpolation. In Section 4  we report LM adaptation results
on Mandarin and Arabic ASR  followed by conclusions and future work in Section 5.

2 Correlated bigram LSA

Latent semantic analysis such as LDA makes a bag-of-word assumption that each word in a docu-
ment is generated irrespective of its position in a document. To relax this assumption  bigram LSA
has been proposed [7] to modify the graphical structure of LDA by connecting adjacent words in a
document together to form a Markov chain. Figure 1 shows the graphical representation of bigram
LSA where the top node represents the prior distribution over the topic mixture weights and the
middle layer represents the latent topic label associated to each observed word at the bottom layer.
The document generation procedure of bigram LSA is similar to LDA except that the previous word
is taken into consideration for generating the current word:

1. Sample θ from a prior distribution p(θ)
2. For each word wi at the i-th position of a document:

(a) Sample topic label: zi ∼ Multinomial(θ)
(b) Sample wi given the previous word wi−1 and the topic label zi: wi ∼ p(·|wi−1  zi)

Our incremental contributions for bigram LSA are three-folded: Firstly  we present a technique
for topic correlation modeling using Dirichlet-Tree prior in Section 2.1. Secondly  we propose
efﬁcient algorithm for bigram LSA training via variational Bayes approach and model bootstrapping
which are scalable to large settings in Section 2.2. Thirdly  we formulate the fractional Kneser-Ney
smoothing to generalize the original Kneser-Ney smoothing which supports only integral counts in
Section 2.3.

j=1
Dir(.)

j=2
Dir(.)

j=3
Dir(.)

j=J
Dir(.)

propagate

j=1
Dir(.)

0.1+0.2

0.3+0.4

j=2
Dir(.)

j=3
Dir(.)

Latent topics

topic 1

topic 2 topic 3

topic 4

topic K−1

topic K

q(z=k)

0.1

0.2

0.3

0.4

Figure 2: Left: Dirichlet-Tree prior of depth two. Right: Variational E-step as bottom-up propaga-
tion and summation of fractional topic counts.

2.1 Topic correlation

Modeling topic correlations is motivated by an observation that documents such as newspaper arti-
cles are usually organized into main-topic and sub-topic hierarchy for document browsing. From this
perspective  a Dirichlet prior is not appropriate since it assumes topic independence. A Dirichlet-
Tree prior [13  14] is employed to capture topic correlations. Figure 2 (Left) illustrates a depth-two
Dirichlet-Tree. A depth-one Dirichlet-tree is equivalent to a Dirichlet prior in LDA. The sampling
procedure for the topic mixture weight θ ∼ p(θ) can be described as follows:

1. Sample a vector of branch probabilities bj ∼ Dirichlet(·; {αjc}) for each node j = 1...J
where {αjc} denotes the parameter of the Dirichlet distribution at node j  i.e. the pseudo-
counts of the outgoing branch c at node j.

2. Compute the topic mixture weight as θk =Qjc b

where δjc(k) is an indicator function
which sets to unity when the c-th branch of the j-th node leads to the leaf node of topic k
and zero otherwise. The k-th topic weight θk is computed as the product of sampled branch
probabilities from the root node to the leaf node corresponding to topic k.

δjc(k)
jc

The structure and the number of outgoing branches of each Dirichlet node can be arbitrary. In this
paper  we employ a balanced binary Dirichlet-tree.

2.2 Model training

Gibbs sampling was employed for bigram LSA training [7]. Despite the simplicity  it can be slow
and inefﬁcient since it usually requires many sampling iterations for convergence. We present a
variational Bayes approach for model training. The joint likelihood of a document wN
1   the latent
topic sequence zN

1 and θ using the bigram LSA can be written as follows:

N

p(wN

1   zN

1   θ) = p(θ) ·

p(zi|θ) · p(wi|wi−1  zi)

(1)

By introducing a factorizable variational posterior distribution q(zN
i=1 q(zi)
over the latent variables and applying the Jensen’s inequality  the lower bound of the marginalized
document likelihood can be derived as follows:

1   θ; Γ) = q(θ) · QN

Yi=1

log p(wN

1 ; Λ  Γ) = logZθ Xz1...zN
≥ Zθ Xz1...zN

q(zN

1   θ; Γ) ·

1   θ; Λ)

p(wN

1   zN
q(zN

1   θ; Γ)

q(zN

1   θ; Γ) · log

1   θ; Λ)

p(wN

1   zN
q(zN

1   θ; Γ)

= Eq[log

p(θ)
q(θ)

] +

Eq[log

p(zi|θ)
q(zi)

] +

N

Xi=1

N

Xi=1

(2)

(By Jensen’s Inequality) (3)

Eq[log p(wi|wi−1  zi)]

(4)

= Q(wN

(5)
where the expectation is taken using the variational posterior q(zN
1   θ). For the E-step  we compute
the partial derivative of the auxiliary function Q(·) with respect to q(zi) and the parameter γjc in the
Dirichlet-Tree posterior q(θ). Setting the derivatives to zero yields:

1 ; Λ  Γ)

E-Steps:

q(zi = k) ∝ p(wi|wi−1  k) · eEq[log θk;{γjc}] for k = 1..K

N

N

K

γjc = αjc +

Eq[δjc(zi)] = αjc +

Xi=1
δjc(k) · Eq[log bjc] =Xjc

q(zi = k) · δjc(k)

Xk=1

Xi=1
δjc(k)ÃΨ(γjc) − Ψ(Xc

γjc)! (8)

where Eq[log θk] = Xjc

where Eqn 7 is motivated from the conjugate property that the Dirichlet-Tree posterior given the
topic sequence zN

1 has the same form as the Dirichlet-Tree prior:

p(bJ

1 |zN

1 ) ∝ p(zN

1 |bJ

1 ) · p(bJ

1 ; {αjc}) ∝

=Yjc

i=1 δjc(zi))−1

δjc(zi)

jc 

b

N

Yi=1Yjc

 ·Yjc
Yj=1

J

b

αjc−1
jc

jc−1

γ 0
jc

b

=

Dirichlet(bj; {γ0

jc})

b

(αjc+P N

jc

= Yjc

Figure 2 (Right) illustrates that Eqn 7 can be implemented as propagation of fractional topic counts
in a bottom-up fashion with each branch as an accumulator for γjc. Eqn 6 and Eqn 7 are applied
iteratively until convergence is reached. For the M-step  we compute the partial derivative of the aux-
iliary function Q(·) over all training documents d with respect to topic bigram probability p(v|u  k)
and set it to zero:
M-Step (unsmoothed):

(6)

(7)

(9)

(10)

(11)

(12)

q(zi = k|d) · δ(wi−1  u)δ(wi  v)

p(v|u  k) ∝ Xd

=

Nd

Xi=1
Pd Cd(u  v|k)
PdPV

v0=1 Cd(u  v0|k)

C(u  v|k)
v0=1 C(u  v0|k)

=

PV

where Nd denote the number of words in document d and δ(wi  v) is a 0-1 Kronecker Delta function
to test if the i-th word in document d is vocabulary v. Cd(u  v|k) denotes the fractional counts of a
bigram (u  v) belonging to topic k in document d. Intuitively  Eqn 12 simply computes the relative
frequency of the bigram (u  v). However  this solution is not practical since bigram LSA assigns
zero probability to unseen bigrams. Therefore  bigram LSA should be smoothed properly. One
simple approach is to use Laplace-smoothing by adding a small count δ to all bigrams. However 
this approach can lead to worse performance since it will bias the bigram probability towards a
uniform distribution when the vocabulary size V gets large. Our approach is to represent p(v|u  k)
as a standard backoff LM smoothed by fractional Kneser-Ney smoothing as described in Section 2.3.

Model initialization is crucial for variational EM training. We employ a bootstrapping approach
using a well-trained unigram LSA as an initial model for bigram LSA so that p(wi|wi−1  k) is
approximated by p(wi|k) in Eqn 6. It saves computation and avoids keeping the full initial bigram
LSA in memory during the EM training. To make the training procedure more practical  we apply
bigram pruning during statistics accumulation in the M-step when the bigram count in a document
is less than 0.1. This heuristic is reasonable since only a small number of topics are “active” to
a bigram. With the sparsity  there is no need to store K copies of accumulators for each bigram
and thus reducing the memory requirement signiﬁcantly. The pruned bigram counts are re-assigned
to the most likely topic of the current document so that the counts are conserved. For practical
implementation  accumulators are saved into the disk in batches for count merging. In the ﬁnal step 
each topic-dependent LM is smoothed individually using the merged count ﬁle.

2.3 Fractional Kneser-Ney smoothing

Standard backoff N-gram LM is widely used in the ASR community. The state-of-the-art smoothing
for the backoff LM is based on Kneser-Ney smoothing [9]. The belief of its success is due to the
preservation of marginal distributions. However  the original formulation only works for integral

counts which is not suitable for bigram LSA using fractional counts. Therefore  we propose the
fractional Kneser-Ney smoothing as a generalization of the original formulation. The interpolated
form using absolute discounting can be expressed as follows:
max{C(u  v) − D  0}

+ λ(u) · pKN (v)

(13)

pKN (v|u) =

C(u)

where D is a discounting factor. In the original formulation  D lies between 0 and 1. But in our
formulation  D can be any positive number. Intuitively  D controls the degree of smoothing. If D is
set to zero  the model is unsmoothed; If D is too big  bigrams with counts smaller than D are pruned
from the LM. λ(u) ensures the bigram probability sums to unity. After summing over all possible v
on both sides of Eqn 13 and re-arranging terms  λ(u) becomes:

1 = Xv

max{C(u  v) − D  0}

C(u)

+ λ(u)

=⇒ λ(u) = 1 −Xv

max{C(u  v) − D  0}

C(u)

= 1 − Xv:C(u v)>D
C(u) −Pv:C(u v)>D C(u  v) + DPv:C(u v)>D 1

C(u)

C(u  v) − D

C(u)

=

=

= Pv:C(u v)≤D C(u  v) + DPv:C(u v)>D 1

C(u)

C≤D(u  ·) + D · N>D(u  ·)

C(u)

where C≤D(u  ·) denotes the sum of bigram counts following u and smaller than D. N>D(u  ·)
denotes the number of word types following u with the bigram counts bigger than D.
In Kneser-Ney smoothing  the lower-order distribution pKN (v) is treated as unknown parameters
which can be estimated using the preservation of marginal distributions:

where ˆp(v) is the marginal distribution estimated from the background training data so that ˆp(v) =

ˆp(v) = Xu

pKN (v|u) · ˆp(u)

(19)

(14)

(15)

(16)

(17)

(18)

(22)

(23)

(24)

(25)

C(v)

P v0 C(v0) . Therefore  we substitute Eqn 13 into Eqn 19:
C(v) = Xu µ max{C(u  v) − D  0}

C(u)

+ λ(u) · pKN (v)¶ · C(u)

(20)

C(u) · λ(u)

(21)

=⇒ pKN (v) =

=

=

=

= ÃXu

C(v) −Pu max{C(u  v) − D  0}

max{C(u  v) − D  0}! + pKN (v) ·Xu
Pu C(u) · λ(u)
Pu C(u) · λ(u)

C(v) − C>D(·  v) + D · N>D(·  v)

C≤D(·  v) + D · N>D(·  v)

(using Eqn 18)

C≤D(·  v) + D · N>D(·  v)

Pu C≤D(u  ·) + D · N>D(u  ·)
Pv C≤D(·  v) + D · N>D(·  v)

Eqn 25 generalizes Kneser-Ney smoothing to integral and fractional counts. For the original formu-
lation  C≤D(u  ·) equals to zero since each observed bigram count must be at least one by deﬁnition
with D less than one. As a result  the D term cancels out yielding the original formulation which
counts the number of words preceding v and thus recovering the original formulation. Intuitively 
the numerator in Eqn 25 measures the total discounts of observed bigrams ending at v. In other
words  fractional Kneser-Ney smoothing estimates the lower-order probability distribution using the
relative frequency over discounts instead of word counts. With this approach  each topic-dependent
LM in bigram LSA can be smoothed using our formulation.

3 Unsupervised LM adaptation

Unsupervised LM adaptation is performed by ﬁrst inferring the topic distribution of each test audio
using the word hypotheses from the ﬁrst-pass decoding via variational inference in Eqn 6–7. Relative
frequency over the branch posterior counts γjc is applied on each Dirichlet node j. The MAP topic
mixture weight ˆθ and the adapted unigram and bigram LSA are computed as follows:

ˆθk ∝ Yjc µ γjc

Pc0 γjc0¶δjc(k)

p(v|k) · ˆθk and pa(v|u) =

K

pa(v) =

Xk=1

for k = 1...K

p(v|u  k) · ˆθk

K

Xk=1

(26)

(27)

The unigram LSA marginals are integrated into the background N-gram LM pbg(v|h) via marginal
adaptation [10] as follows:

p(1)

pbg(v)¶β
a (v|h) ∝ µ pa(v)

· pbg(v|h)

(28)

Marginal adaptation has a close connection to maximum entropy modeling since the marginal con-
straints can be encoded as unigram features. Intuitively  bigram LSA would be integrated in the same
fashion by introducing bigram marginal constraints. However  we found that integrating bigram
features via marginal adaptation did not offer further improvement compared to only integrating un-
igram features. Since marginal adaptation integrates a unigram feature as a likelihood ratio between
the adapted marginal pa(v) and the background marginal pbg(v) in Eqn 28  perhaps the unigram and
bigram likelihood ratios are very similar and thus the latter does not give extra information. Another
explanation is that marginal adaptation corresponds to only one iteration of generalized iterative
scaling (GIS). Due to the large number of bigram features in terms of millions  one GIS iteration
may not be sufﬁcient for convergence. On the other hand  simple linear LM interpolation is found
to be effective in our experiment. The ﬁnal LM adaptation formula is provided using results from
Eqn 27 and Eqn 28 as a two-stage process:

p(2)
a (v|h) = λ · p(1)

a (v|h) + (1 − λ) · pa(v|u)

(29)

where λ is tuned to optimize perplexity on word hypotheses from the ﬁrst-pass decoding on a per-
audio basis.

4 Experimental setup

Our LM adaptation approach was evaluated using the RT04 Mandarin Broadcast News evaluation
system. The system employed context-dependent Initial-Final acoustic models trained using 100-
hour broadcast news audio from the Mandarin HUB4 1997 training set and a subset of TDT4. 42-
dimension features were extracted after linear discriminant analysis projected from a window of
MFCC and energy features. The system employed a two-pass decoding strategy using speaker-
independent and speaker-adaptive acoustic models. For the second-pass decoding  we applied stan-
dard acoustic model adaptation such as vocal tract length normalization and maximum likelihood
linear regression on the feature and model spaces. The training corpora include Xinhua News 2002
(January–September) containing 13M words and 64k documents. A background 4-gram LM was
trained using modiﬁed Kneser-Ney smoothing using the SRILM toolkit [15]. The same training
corpora were used for unigram and bigram LSA training with 200 topics. The vocabulary size is
108k words. Discounting factor D for fractional Kneser-Ney smoothing was set to 0.4.
First-pass decoding was ﬁrst performed to obtain an automatic transcript for each audio show. Then
unsupervised LM adaptation was applied using the automatic transcript to obtain an adapted LM
for second-pass decoding using the approach described in Section 3. Word perplexity and character
error rates (CER) were measured on the Mandarin RT04 test set. Matched pairs sentence-segment
word error test was performed for signiﬁcance test using the NIST scoring tool.

Table 1: Correlated bigram topics extracted from bigram LSA.

Topic index

Top bigrams sorted by p(u  v|k)

“topic-61”

ሇ+ࣣᅴ(’s student)  ሇ+಑ᔯ(’s education)  ಑ᔯ+ሇ(education ’s)

ࣣච+ሇ(school ’s)  ळ৯+ᄪ(youth class)  ᐆᤌ+಑ᔯ(quality of education)

“topic-62”

Оୣ+߯Ԧ(expert cultivation)  ࠵ࣣ+චᮿ(university chancellor)

“topic-63”

“topic-64”

“topic-65”

቉+ٍ(famous)  ୛+ᴱච(high-school)  ሇ+ࣣᅴ(’s student)
ڔ+የѕұᰧ(and social security)  ሇ+ीχ(’s employment) 

࠼χ+Оٽ(unemployed ofﬁcer)  ीχ+७Ѭ(employment position)
ሇ+኎ፀ(’s research)  ρए+ࣣᓥ(expert people)  ᎋ+Ეߨ(etc area)

ᅴႪ+୼ഴ(biological technology)  ኎ፀ+ୄ൤(research result)

ОᏚ+߰ݿᐵ(Human DNA sequence)  ሇ+߰ݿ(’s DNA)

ᅴႪ+୼ഴ(biological technology)  ᕃᔿ+৭ᐷᕇ(embryo stem cell)

Table 2: Character Error Rates (Word perplexity) on the RT04 test set. Bigram LSA was applied in
addition to unigram LSA.

LM (13M)

CCTV

NTDTV

RFA

OVERALL

background LM
+unigram LSA

+bigram LSA (Kneser-Ney  30 topics)

+bigram LSA (Witten-Bell)
+bigram LSA (Kneser-Ney)

15.3% (748)
14.4 (629)
14.5 (604)
14.1 (594)
14.0 (587)

21.8 (1718)
21.5 (1547)
20.7 (1502)
20.9 (1452)
20.8 (1448)

39.5 (3655)
38.9 (3015)
39.0 (2736)
38.3 (2628)
38.2 (2586)

24.9
24.3
24.1
23.8
23.7

4.1 LM adaptation results

Table 1 shows the correlated bigram topics sorted by the joint bigram probability p(v|u  k) · p(u|k).
Most of the top bigrams appear either as phrases or words attached with a stopword such as ሇ(’s in
English). Table 2 shows the LM adaptation results in CER and perplexity. Applying both unigram
and bigram LSA yields consistent improvement over unigram LSA in the range of 6.4%–8.5%
relative reduction in perplexity and 2.5% relative reduction in the overall CER. The CER reduction is
statistically signiﬁcant at 0.1% signiﬁcance level. We compared our proposed fractional Kneser-Ney
smoothing with Witten-Bell smoothing which also supports fractional counts. The results showed
that Kneser-Ney smoothing performs slightly better than Witten-Bell smoothing.
Increasing the
number of topics in bigram LSA helps despite model sparsity. We applied extra EM iterations on
top of the bootstrapped bigram LSA but no further performance improvement was observed.

4.2 Large-scale evaluation

We evaluated our approach using the CMU-InterACT vowelized Arabic transcription system dis-
criminatively trained on 1500-hour transcribed audio using MMIE for the GALE Phase-3 evaluation.
A large background 4-gram LM was trained using 962M-word text corpora with 737k vocabulary.
Unigram and bigram LSA were trained on the same corpora and were applied to lattice rescoring on
Dev07 and unseen Dev08 test sets with 2.6-hour and 3-hour audio shows containing broadcast news
(BN) and broadcast conversation (BC) genre. Table 3 shows that bigram LSA rescoring reduces the
overall word error rate by more than 3.0% relative compared to the unadapted baseline on both sets
which are statistically signiﬁcant at 0.1% signiﬁcance level. However  degradation is observed using
trigram LSA compared to bigram LSA which may be due to data sparseness.

Table 3: Lattice rescoring results in word error rate on Dev07 (unseen Dev08) using the CMU-
InterACT Arabic transcription system for the GALE Phase-3 evaluation.

GALE LM (962M)

BN

background LM
+unigram LSA

+bigram LSA (Witten-Bell)
+bigram LSA (Kneser-Ney)
+trigram LSA (Kneser-Ney)

BC OVERALL
14.3 (16.4)
14.2 (16.3)
13.9 (15.9)
13.8 (15.9)

11.6% 19.4
19.2
11.5
11.0
19.0
11.0
18.9
18.8
11.3

14.0 (-)

5 Conclusion

We present a correlated bigram LSA approach for unsupervised LM adaptation for ASR. Our con-
tributions include efﬁcient variational EM for model training and fractional Kneser-Ney approach
for LM smoothing with fractional counts. Bigram LSA yields additional improvement in both per-
plexity and recognition performance in addition to unigram LSA. Increasing the number of topics
for bigram LSA helps despite the model sparsity. Bootstrapping bigram LSA from unigram LSA
saves computation and memory requirement during EM training. Our approach is scalable to large
training corpora and works well on different languages. The improvement from bigram LSA is
statistically signiﬁcant compared to the unadapted baseline. Future work includes applying the pro-
posed approach for statistical machine translation.

Acknowledgement

We would like to thank Mark Fuhs for help parallelizing the bigram LSA training via condor.

References

[1] J. R. Bellegarda  “Large Vocabulary Speech Recognition with Multispan Statistical Language
Models ” IEEE Transactions on Speech and Audio Processing  vol. 8  no. 1  pp. 76–84  Jan
2000.

[2] D. Blei  A. Ng  and M. Jordan  “Latent Dirichlet Allocation ” in Journal of Machine Learning

Research  2003  pp. 1107–1135.

[3] Y. C. Tam and T. Schultz  “Language model adaptation using variational Bayes inference ” in

Proceedings of Interspeech  2005.

[4] D. Mrva and P. C. Woodland  “Unsupervised language model adaptation for mandarin broad-

cast conversation transcription ” in Proceedings of Interspeech  2006.

[5] T. Grifﬁths  M. Steyvers  D. Blei  and J. Tenenbaum  “Integrating topics and syntax ” in

Advances in Neural Information Processing Systems  2004.

[6] B. J. Hsu and J. Glass  “Style and topic language model adaptation using HMM-LDA ” in

Proceedings of Empirical Methods on Natural Language Processing (EMNLP)  2006.

[7] Hanna M. Wallach  “Topic Modeling: Beyond Bag-of-Words ” in International Conference

on Machine Learning  2006.

[8] P. Xu  A. Emami  and F. Jelinek  “Training connectionist models for the structured language
model ” in Proceedings of Empirical Methods on Natural Language Processing (EMNLP) 
2003.

[9] R. Kneser and H. Ney  “Improved backing-off for M-gram language modeling ” in Proceedings
of the IEEE International Conference on Acoustics  Speech  and Signal Processing (ICASSP) 
1995  vol. 1  pp. 181–184.

[10] R. Kneser  J. Peters  and D. Klakow  “Language model adaptation using dynamic marginals ”
in Proceedings of European Conference on Speech Communication and Technology (EU-
ROSPEECH)  1997  pp. 1971–1974.

[11] R. Iyer and M. Ostendorf  “Modeling long distance dependence in language: Topic mixtures
versus dynamic cache models ” IEEE Transactions on Speech and Audio Processing  vol. 7 
no. 1  pp. 30–39  Jan 1999.

[12] X. Wang  A. McCallum  and X. Wei  “Topical N-grams: Phrase and topic discovery  with an
application to information retrieval ” in IEEE International Conference on Data Mining  2007.

[13] T. Minka  “The dirichlet-tree distribution ” 1999.
[14] Y. C. Tam and T. Schultz  “Correlated latent semantic model for unsupervised language model
adaptation ” in Proceedings of the IEEE International Conference on Acoustics  Speech  and
Signal Processing (ICASSP)  2007.

[15] A. Stolcke  “SRILM - an extensible language modeling toolkit ” in Proceedings of Interna-

tional Conference on Spoken Language Processing (ICSLP)  2002.

,Ohad Shamir