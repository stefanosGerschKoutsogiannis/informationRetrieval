2013,Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result,Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our first main result  we show that an important subset of the latter methodology is  in fact  a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods  and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and  as such  can be seen as dual in spirit to the continuum in TD($\lambda$)-style algorithms in policy evaluation. As our second main result  we show for a substantial subset of soft-greedy value function approaches that  while having the potential to avoid policy oscillation and policy chattering  this subset can never converge toward any optimal policy  except in a certain pathological case. Consequently  in the context of approximations  the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality.,Optimistic policy iteration and natural actor-critic:

A unifying view and a non-optimality result

Paul Wagner

Aalto University

Department of Information and Computer Science

FI-00076 Aalto  Finland

paul.wagner@aalto.fi

Abstract

Approximate dynamic programming approaches to the reinforcement learning
problem are often categorized into greedy value function methods and value-based
policy gradient methods. As our ﬁrst main result  we show that an important subset
of the latter methodology is  in fact  a limiting special case of a general formula-
tion of the former methodology; optimistic policy iteration encompasses not only
most of the greedy value function methods but also natural actor-critic methods 
and permits one to directly interpolate between them. The resulting continuum ad-
justs the strength of the Markov assumption in policy improvement and  as such 
can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy
evaluation. As our second main result  we show for a substantial subset of soft-
greedy value function approaches that  while having the potential to avoid policy
oscillation and policy chattering  this subset can never converge toward an opti-
mal policy  except in a certain pathological case. Consequently  in the context of
approximations (either in state estimation or in value function representation)  the
majority of greedy value function methods seem to be deemed to suffer either from
the risk of oscillation/chattering or from the presence of systematic sub-optimality.

1 Introduction

We consider the reinforcement learning problem in which one attempts to ﬁnd an approximately
optimal policy for controlling a stochastic nonlinear dynamical system. We focus on the setting in
which the target system is actively sampled during the learning process. Here the sampling policy
changes during the learning process in a manner that depends on the main policy being optimized.
This learning setting is often called interactive learning [e.g.  23  §3]. Many approaches to the
problem are value-based and build on the methodology of simulation-based approximate dynamic
programming [23  4  9  19  8  21]. The majority of these methods are often categorized into greedy
value function methods (critic-only) and value-based policy gradient methods (actor-critic) [e.g. 
23  13].
Within this interactive setting  the policy gradient approach has better convergence guarantees  with
the strongest case being for Monte Carlo evaluation with ‘compatible’ value function approximation.
In this case  convergence with probability one (w.p.1) to a local optimum can be established for
arbitrary differentiable policy classes under mild assumptions [22  13  19]. On the other hand  while
the greedy value function approach is often considered to possess practical advantages in terms of
convergence speed and representational ﬂexibility  its behavior in the proximity of an optimum is
currently not well understood. It is well known that interactively operated approximate hard-greedy

An extended version of this paper with full proofs and additional background material is available at

http://books.nips.cc/ and http://users.ics.aalto.fi/pwagner/.

1

value function methods can fail to converge to any single policy and instead become trapped in
sustained policy oscillation or policy chattering  which is currently a poorly understood phenomenon
[6  7]. This applies to both non-optimistic and optimistic policy iteration (value iteration being a
special case of the latter). In general  the best guarantees for this methodology exist in the form of
sub-optimality bounds [6  7]. The practical value of these bounds  however  is under question (e.g. 
[2; 7  §6.2.2])  as they can permit very bad solutions. Furthermore  it has been shown that these
bounds are tight [7  §6.2.3; 12  §3.2].
A hard-greedy policy is a discontinuous function of its parameters  which has been identiﬁed as a
key source of problems [18  10  17  22]. In addition to the observation that the class of stochastic
policies may often permit much simpler solutions [cf. 20]  it is known that continuously stochastic
policies can also re-gain convergence: both non-optimistic and optimistic soft-greedy approximate
policy iteration using  for example  the Gibbs/Boltzmann policy class  is known to converge with
enough softness  ‘enough’ being problem-speciﬁc. This has been shown by Perkins & Precup [18]
and Melo et al. [14]  respectively  although with no consideration of the quality of the obtained
solutions nor with an interpretation of how ‘enough’ relates to the problem at hand. Unfortunately 
the aforementioned sub-optimality bounds are also lost in this case (consider temperature τ → ∞);
while convergence is re-gained  the properties of the obtained solutions are rather unknown.
To summarize  there are considerable shortcomings in the current understanding of the learning
dynamics at the very heart of the approximate dynamic programming methodology. We share the
belief of Bertsekas [5  6]  expressed in the context of the policy oscillation phenomenon  that a
better understanding of these issues “has the potential to alter in fundamental ways our thinking
about approximate DP.”
In this paper  we provide insight into the convergence behavior and optimality of the generalized
optimistic form of the greedy value function methodology by reﬂecting it against the policy gradient
approach. While these two approaches are considered in the literature mostly separately  we are
motivated by the belief that it is eventually possible to fully unify them  so as to have the beneﬁts and
insights from both in a single framework with no artiﬁcial (or historical) boundaries  and that such a
uniﬁcation can eventually resolve the issues outlined above. These issues revolve mainly around the
greedy methodology  while at the same time  solid convergence results exist for the policy gradient
methodology; connecting these methodologies more ﬁrmly might well lead to a fuller understanding
of both.
After providing background in Section 2  we take the following steps in this direction. First  we
show that natural actor-critic methods from the policy gradient side are  in fact  a limiting special
case of optimistic policy iteration (Sec. 3). Second  we show that while having the potential to avoid
policy oscillation and chattering  a substantial subset of soft-greedy value function approaches can
never converge to an optimal policy  except in a certain pathological case (Sec. 4). We then conclude
with a discussion in a broader context and use the results to complete a high-level convergence and
optimality property map of the variants of the considered methodology (Sec. 5).

2 Background

A Markov decision process (MDP) is deﬁned by a tuple M = (S A P  r)  where S and A denote
the state and action spaces. St ∈ S and At ∈ A denote random variables at time t. s  s(cid:48) ∈ S
and a  b ∈ A denote state and action instances. P(s  a  s(cid:48)) = P(St+1 = s(cid:48)|St = s  At = a)
deﬁnes the transition dynamics and r(s  a) ∈ R deﬁnes the expected immediate reward function.
Non-Markovian aggregate states  i.e.  subsets of S  are denoted by y. A policy π(a|s  θk) ∈ Π is
a stochastic mapping from states to actions  parameterized by θk ∈ Θ. Improvement is performed
E[r(St  At)|π(θ)]. ∇θJ(θk) ∈ Θ denotes
(cid:80)
a parameter gradient at θk. ∇πJ(θk) ∈ Π denotes the corresponding policy gradient in the selected
policy space. We deﬁne the policy distance (cid:107)πu − πv(cid:107) as some p-norm of the action probability
a |πu(a|s)− πv(a|s)|p)1/p. Action value functions ¯Q(s  a  ˆwk) and Q(s  a  ˆwk) 
t γtE[r(St  At)|S0 =
s  A0 = a  π(θk)] for some (s  a) when following some policy π(θk). The state value function
V (s  ˆwk) is an estimator of such cumulative reward that follows some s. We use  to denote a small
positive inﬁnitesimal quantity.

with respect to the performance metric J(θ) = 1/H(cid:80)H
differences ((cid:80)
parameterized by ˆwk  are estimators of the γ-discounted cumulative reward(cid:80)

s

t

2

We focus on the Gibbs (Boltzmann) policy class with a linear combination of basis functions φ:

π(a|s  θk) =

(1)
We shall use the term ‘semi-uniformly stochastic policy’ for referring to a policy for which π(a|s) =
cs ∨ π(a|s) = 0  ∀s  a  ∀s ∃cs ∈ [0  1]. Note that both the uniformly stochastic policy and all
deterministic policies are special cases of semi-uniformly stochastic policies.
For the value function  we focus on least-squares linear-in-parameters approximation with the same
basis φ as in (1). We consider both advantage values [see 22  19]

k φ(s b)

.

k φ(s a)

(cid:80)

eθ(cid:62)
b eθ(cid:62)

¯Qk(s  a  ˆwk) = ˆw(cid:62)

k

π(b|s  θk)φ(s  b)

(cid:32)
φ(s  a) −(cid:88)

b

(cid:33)

(2)

(5)

and absolute action values

Qk(s  a  ˆwk) = ˆw(cid:62)

(3)
Evaluation can be based on either Monte Carlo or temporal difference estimation. We focus on
optimistic policy iteration  which contains both non-optimistic policy iteration and value iteration as
special cases  and on the policy gradient counterparts of these.
In the general form of optimistic approximate policy iteration (e.g.  [7  §6.4]; see also [6  §3.3])  a
value function parameter vector w is gradually interpolated toward the most recent evaluation ˆw:

k φ(s  a) .

wk+1 = wk + κk( ˆwk − wk)  

(4)
Non-optimistic policy iteration is obtained with κk = 1  ∀k and ‘complete’ evaluations ˆwk (see
below). The corresponding Gibbs soft-greedy policy is obtained by combining (1) and a temperature
(softness) parameter τ with

κk ∈ (0  1] .

θk+1 = wk+1/τk  

τk ∈ (0 ∞) .

Hard-greedy iteration is obtained in the limit as τ → 0.
In optimistic policy iteration  policy improvement is based on an incomplete evaluation. We distin-
guish between two dimensions of completeness  which are evaluation depth and evaluation accu-
racy. By evaluation depth  we refer to the look-ahead depth after which truncation with the previous
value function estimate occurs. For example  LSPE(0) and LSTD(0) [e.g.  15] implement shallow
and deep evaluation  respectively. With shallow evaluation  the current value function parameter
vector wk is required for look-ahead truncation when computing ˆwk+1. Inaccurate (noisy) evalua-
tion necessitates additional caution in the policy improvement process and is the usual motivation
for using (4) with κ < 1.
It is well known that greedy policy iteration can be non-convergent under approximations [4]. The
widely used projected equation approach can manifest convergence behavior that is complex and not
well understood  including bounded but potentially severe sustained policy oscillations [6  7] (see
the extended version for further details). Similar consequences arise in the context of partial ob-
servability for approximate or incomplete state estimation [e.g.  20  16]. A novel explanation to the
phenomenon in the non-optimistic case was recently proposed in [24  25]  where policy oscillation
was re-cast as sustained overshooting over an attractive stochastic policy. Policy convergence can
be established under various restrictions (see the extended version for further details). Most impor-
tantly to this paper  convergence can be established with continuously soft-greedy action selection
[18  14]  in which case  however  the quality of the obtained solutions is unknown.
In policy gradient reinforcement learning [22  13  19  8]  improvement is obtained via stochastic
gradient ascent:

θk+1 = θk + αkG(θk)−1 ∂J(θk)

(6)
where αk ∈ (0 ∞)  G is a Riemannian metric tensor that ideally encodes the curvature of the
policy parameterization  and ηk is some estimate of the gradient. With value-based policy gradient
methods  using (1) together with either (2) or (3) fulﬁlls the ‘compatibility condition’ [22  13]. With
(2)  the value function parameter vector ˆwk becomes the natural gradient estimate for the evaluated
policy π(θk)  leading to natural actor-critic algorithms [11  19]  for which

= θk + αkηk  

∂θ

ηk = ˆwk .

3

(7)

For policy gradient learning with a ‘compatible’ value function and Monte Carlo evaluation  con-
vergence w.p.1 to a local optimum is established under standard assumptions [22  13]. Temporal
difference evaluation can lead to sub-optimal results with a known sub-optimality bound [13  8].

3 Forgetful natural actor-critic

In this section  we show that an important subset of natural actor-critic algorithms is a limiting
special case of optimistic policy iteration. A related connection was recently shown in [24  25] 
where a modiﬁed form of the natural actor-critic algorithm by Peters & Schaal [19] was shown
to correspond to non-optimistic policy iteration. In the following  we generalize and simplify this
result: by starting from the more general setting of optimistic policy iteration  we arrive at a unifying
view that both encompasses a broader range of greedy methods and permits interpolation between
the approaches directly with existing (unmodiﬁed) methodology.
We consider the Gibbs policy class from (1) and the linear-in-parameters advantage function from
(2)  which form a ‘compatible’ actor-critic setup. We assume deep policy evaluation (cf. Section 2).
We begin with the natural actor-critic (NAC) algorithm by Peters & Schaal [19] (cf. (6) and (7)) and
generalize it by adding a forgetting term:

θk+1 = θk + αkηk − κkθk  

(8)
where αk ∈ (0 ∞)  κk ∈ (0  1]. We refer to this generalized algorithm as the forgetful natural
actor-critic algorithm  or NAC(κ).
In the following  we show that this algorithm is  within the
discussed context  equivalent to the general form of optimistic policy iteration in (4) and (5)  with
the following translation of the parameterization:

τk =

κk
αk

 

or αk =

κk
τk

.

(9)

Taking the forgetting factor κ in (8) toward zero leads back toward the original natural actor-critic
algorithm  with the implication that the original algorithm is a limiting special case of optimistic
policy iteration.
Theorem 1. For the case of deep policy evaluation (Section 2)  the natural actor-critic algorithm
for the Gibbs policy class ((6)  (7)  (1)  (2)) is a limiting special case of Gibbs soft-greedy optimistic
policy iteration ((4)  (5)  (1)  (2)).

Proof. The update rule for Gibbs soft-greedy optimistic policy iteration is given in (4) and (5). By
moving the temperature to scale ˆw (assume w0 to be scaled accordingly)  we obtain

(cid:26) w(cid:48)

k+1 = w(cid:48)
θk+1 = w(cid:48)

k + κk( ˆwk/τk − w(cid:48)
k)
k+1  

(10)
again with κk ∈ (0  1]  τk ∈ (0 ∞). Such a re-formulation effectively re-scales w and is possible
only with deep policy evaluation (cf. Section 2)  with which the non-scaled w is not needed by the
policy evaluation process. We can now remove the redundant second line and rename w(cid:48) to θ:

Finally  we open up the last term and encapsulate κ/τ into α:

θk+1 = θk + κk( ˆwk/τk − θk) .

(11)

θk+1 = θk + κk( ˆwk/τk) − κkθk

(12)
(13)
with αk = κk/τk. Based on (7)  we observe that (13) is equivalent to (8). The original natural
actor-critic algorithm is obtained in the limit as κk → 0  which causes the forgetting term κkθk to
vanish (the effective step size α can still be controlled with τ).

= θk + αk ˆwk − κkθk  

This result has some interesting implications. First  it becomes apparent that the implicit effective
step size in optimistic policy iteration is  in fact  α = κ/τ  i.e.  it is inversely related to the tem-
perature τ. If the interpolation factor κ is held ﬁxed  a low temperature  which can lead to policy

4

oscillation  equals a long effective step size. This agrees with the interpretation of policy oscillation
as overshooting in [24  25]. Likewise  a high temperature equals a short effective step size. In [18] 
convergence is established for a high enough constant temperature. This result now becomes trans-
lated to showing that convergence is established with a short enough constant effective step size 1
which creates an interesting and more direct connection to convergence results for (batch) steepest
descent methods with a constant step size [e.g.  1  3]. In addition  this connection might permit the
application of the results in the aforementioned literature to establish  in the considered context  a
constant step size convergence result for the natural actor-critic methodology.
Second  we see that the interpolation scheme in optimistic policy iteration  while originally intro-
duced for the sake of countering an inaccurate value function estimate  actually goes in the direction
of the policy gradient methodology. Smooth interpolation between policy gradient and greedy value
function learning turns out to be possible by simply adjusting the interpolation factor κ while treat-
ing the temperature τ as an inverse of the step size (we return to provide an interpretation of the role
of κ at a later point). Contrary to the related result in [24]  no modiﬁcations to existing algorithms
are needed. This connection also allows the convergence results from the policy gradient literature
to be brought in (see Section 2): convergence w.p.1  under standard assumptions from the referred
literature  to an optimal solution is established in the limit for this class of approximate optimistic
policy iteration as the interpolation factor κ is taken toward zero and the step size requirements are
inversely enforced on the temperature τ.
Third  we observe that in non-optimistic policy iteration (κ = 1)  the forgetting term resets the
parameter vector to the origin at the beginning of every iteration  with the implication that solutions
that are not within the range of a single step from the origin in the direction of the natural gradient
cannot be reached in any number of iterations. The choice of the effective step size  which is
inversely controlled by the temperature  becomes again decisive: a step size that is too short (the
temperature is too high) will cause the algorithm to permanently undershoot the desired optimum 
thus trapping it in sustained sub-optimality  while a step size that is too long (the temperature is too
low) will cause it to overshoot  which can additionally trap it in sustained oscillation. Unfortunately 
even hitting the target exactly with a perfect step size will fail to lead to convergence and optimality
at the same time. Our next section examines these issues more closely.

4 Systematic non-optimality of soft-greedy methods

For greedy value function methods  using the hard-greedy policy class trivially prevents conver-
gence to other than deterministic policies. Furthermore  the proximity of an attractive stochastic
policy can prevent convergence altogether and trap the process in oscillation (cf. Section 2). The
Gibbs soft-greedy policy class  on the other hand  can represent stochastic policies  ﬁxed points do
exist [10  17]  and convergence toward some policy is guaranteed with sufﬁcient softness [18  14].
While convergence toward deterministic optimal decisions is trivially lost as soon as any softness
is introduced (τ (cid:54)→ 0  and assuming a bounded value function)  one might hope that convergence
toward stochastic optimal decisions could still occur in some cases. Unfortunately  as we show in
the following  this is not the case: in the presence of any softness  this approach can never converge
toward any optimal policy (i.e.  convergence and optimality become mutually exclusive)  except in
a certain pathological case.
At this point  we wish to make clear that we are not arguing against the practical value of the greedy
value function methodology in (interactively) approximated problems; the methodology has some
clear merits  and the sub-optimality and oscillations could well be negligible in a given task. Instead 
we take the following result  together with existing literature on policy oscillations  as an indication
of a fundamental theoretical incompatibility of this methodology to this context: the way by which
this methodology deals with stochastic optima seems to be fundamentally ﬂawed  and we believe
that a thorough understanding of this ﬂaw will have  in addition to facilitating sound theoretical
advances  also immediate practical value by permitting correctly informed trade-off decisions.
Theorem 2. Assume an unbiased value function estimator (e.g.  Monte Carlo evaluation). Now 
for Gibbs soft-greedy policy iteration ((1)  (4) and (5)) using a linear-in-parameters value function
approximator ((2) or (3))  including optimistic and non-optimistic variants (any κ in (4))  there
cannot exist a ﬁxed point at an optimum  except for the uniformly stochastic policy.

1Note that the diminishing step size αt in [18  Fig. 1] concerns policy evaluation  not policy improvement.

5

Proof outline. A ﬁxed point of the update rule (4) must satisfy

(14)
i.e.  at a ﬁxed point  the policy evaluation step ˆwk := eval(π(wk/τk)) for the current parameter
vector must yield the same parameter vector as its result:

ˆwk = wk  

eval (π (wk/τk)) = wk .

(15)

By applying (14) and (7)  we have

wk = ˆwk = ηk = G(θk)−1∇θJ(θk)  

(16)
which shows that the ﬁxed-point policy π(wk/τk) in (15) is deﬁned solely by its own (scaled)
performance gradient.
For an optimal policy and an unbiased estimator  this parameter gradient must  by deﬁnition  map to
the zero policy gradient  i.e.  to ∇πJ(θk) = 0. Consequently  an optimal policy at a ﬁxed point is
deﬁned solely by the zero policy gradient  making the policy equal to π(0)  which is the uniformly
stochastic policy. For the full proof  see the extended version.

Theorem 3. Consider the family of methods from Theorem 2. Assume a smooth policy gradient ﬁeld
((cid:107)∇πJ(πu) − ∇πJ(πv)(cid:107) → 0 as (cid:107)πu − πv(cid:107) → 0) and τ (cid:54)→ 0. First  the policy distance between a
if the optimal policy π(cid:63) is a semi-uniformly stochastic policy. Second  for bounded returns (γ (cid:54)→ 1
and r(s  a) (cid:54)→ ±∞ ∀s  a)  the policy distance between a ﬁxed point policy πf and an optimal policy

ﬁxed point policy πf and an optimal policy π(cid:63) cannot be vanishingly small ((cid:13)(cid:13)πf − π(cid:63)(cid:13)(cid:13) (cid:54)< )  except
π(cid:63) cannot be vanishingly small ((cid:13)(cid:13)πf − π(cid:63)(cid:13)(cid:13) (cid:54)< )  except if the optimal policy π(cid:63) is the uniformly

stochastic policy.

Proof outline. For a policy ¯π = π(wk/τk) that is vanishingly close to an optimum  an unbiased
parameter gradient ηk must  assuming a smooth gradient ﬁeld  map to a policy gradient that is
vanishingly close to zero  i.e.  ηk must have a vanishingly small effect on ¯π with any ﬁnite step size:
(17)

(cid:107)π(wk/τk + αηk) − π(wk/τk)(cid:107) <   

∀α > 0  α (cid:54)→ ∞ .

If ¯π is also a ﬁxed point  then  by (16)  we can substitute both wk and ηk in (17) with ˆwk:

(cid:107)π( ˆwk/τk + α ˆwk) − π( ˆwk/τk)(cid:107) <   

∀α > 0  α (cid:54)→ ∞
⇔ (cid:107)π ((1/τk + α) ˆwk) − π((1/τk) ˆwk)(cid:107) <    ∀α > 0  α (cid:54)→ ∞ .

(18)

We now see that ¯π is deﬁned solely by a temperature-scaled version of a vanishingly small policy
gradient  and that the condition in (17) is equivalent to stating that any ﬁnite decrease of the tem-
perature must not have a non-vanishing effect on ¯π. As only semi-uniformly stochastic policies are
invariant to such temperature decreases  it follows that ¯π must be vanishingly close to such a policy.
Furthermore  if assuming bounded returns  then no dimension of the term ˆw(cid:62)φ(s  a) can approach
positive or negative inﬁnity when ˆw is estimated using (2) or (3). Consequently  for τ (cid:54)→ 0  the
uniformly stochastic policy π(0) becomes the only semi-uniformly stochastic policy that the Gibbs
policy class in (1) can approach  with the implication that ¯π must be vanishingly close to the uni-
formly stochastic policy. For the full proof  see the extended version.

To interpret the preceding theorems  we observe that the gist of them is that  assuming a well-
behaved gradient ﬁeld  the closer the evaluated policy is to an optimum  the closer the target point of
the next greedy update will be to the origin (in policy parameter space). At a ﬁxed point  the policy
parameter vector must equal the target point of the next update  causing convergence to or toward a
policy that is exactly optimal but not at the origin to be a contradiction (Theorem 2). Convergence to
or toward a policy that is vanishingly close to an optimum is also impossible  except if the optimum
is (semi-)uniformly stochastic (Theorem 3).
In practical terms  Theorem 2 states that even if the task at hand and the chosen hyperparameters
would allow convergence to some policy in a ﬁnite number of iterations  the resulting policy can

6

never contain optimal decisions  except for uniformly stochastic ones. Theorem 3 generalizes this
result to the case of asymptotic convergence toward some limiting policy: for unbounded returns
and any τ (cid:54)→ 0  it is impossible to have asymptotic convergence toward any optimal decision in any
state  except for semi-uniformly stochastic decisions  and for bounded returns and any τ (cid:54)→ 0  it is
impossible to have asymptotic convergence toward any non-uniform optimal decision in any state.
If convergence is to occur  then the limiting policy must reside “between” the origin and an optimum 
i.e.  the result must always undershoot the optimum that the learning process was inﬂuenced by.
However  we can see in (15) that by decreasing the temperature τ  it is possible to shift this point of
convergence further away from the origin and closer to the optimum: in the limit of τ → 0  (15) can
permit the parameter vector ˆw to converge toward a point that approaches the origin while  at the
same time  allowing the corresponding policy π( ˆw/τ ) to converge toward a policy that is arbitrarily
close to a distant optimum (one can also see that with τ → 0  the inequality in (18) becomes satisﬁed
for any ˆwk  due to α (cid:54)→ ∞). Unfortunately  as we already know  such manipulation of the distance of
the ﬁxed point from an optimum by adjusting τ can ruin convergence altogether in non-Markovian
problems. Perkins & Precup [18] report negative convergence results for non-optimistic iteration
(κ = 1) with a too low τ  while for optimistic iteration (κ < 1)  Melo et al. [14] report a lack
of positive results.
Interestingly  this latter case is exactly what Theorem 1 addressed  showing
that there actually is a way out and that it is by moving toward natural policy gradient iteration:
decreasing the temperature τ toward zero causes the sub-optimality to vanish  while decreasing the
interpolation factor κ at the same rate prevents the effective step size from exploding.
Finally  we provide a brief discussion on some questions that may have occurred to the reader by
now. First  how does the preceding ﬁt with the well-known soundness of greedy value function
methods in the Markovian case? The crucial difference between the Markovian case (fully observ-
able and tabular) and the non-Markovian case (partially observable or non-tabular) follows from the
standard result for MDPs that states that in the former  all optima must be deterministic (with the
possibility of redundant stochastic optima) [e.g.  23  §A.2]. For the Gibbs policy class  determin-
istic policies reside at inﬁnity in some direction in the parameter space  with two implications for
the Markovian case. First  the distance to an optimum never decreases. Consequently  the value
function  being a correction toward an optimum  never vanishes toward a ‘neutral’ state. Second 
only the direction of an optimum is relevant  as the distance can be always assumed to be inﬁnite.
This implies that in  and only in Markovian problems  the value function never ceases to retain all
necessary information about the current solution  while in non-Markovian problems  relying solely
on the value function can lead to losing track of the current solution.
Second  when moving toward an optimum at inﬁnity  how can the value function / natural gradient
(encoded by ˆw = η) stay non-zero and continue to properly represent action values while the corre-
sponding policy gradient ∇πJ(θ) must approach zero at the same time? We note that the equivalence
in (7) is between a value function and a natural gradient η. We then recall that the curvature of the
Gibbs policy class turns into a plateau at inﬁnity  onto which the policy becomes pushed when mov-
ing toward a deterministic optimum. The increasing discrepancy between η = G(θ)−1∇θJ(θ) (cid:54)→ 0
and ∇πJ(θ) → 0 can be consumed by G(θ)−1 as it captures the curvature of this plateau.

5 Common ground

Figure 1 shows a map of relevant variants of optimistic policy iteration  parameterized as in (4). As
is well known  the hard-greedy variants of this methodology (seen on the left edge on the map) can
become trapped in non-converging cycles over potentially non-optimal policies (see Section 2 for
references and exceptions). For a continuously soft-greedy policy class (toward right on the map) 
convergence can be established with enough softness [18  14]. The natural actor-critic algorithm 
which is convergent and optimal  is placed to the lower left corner by Theorem 1  while the in-
evitable non-optimality of soft-greedy variants toward right follows from Theorems 2 and 3. The
exact (problem-dependent) place and shape of the line separating non-convergent and convergent
soft-greedy variants (dashed line on the map) remains an open problem.
The main value of Theorem 1 is in bringing the greedy value function and policy gradient method-
ologies closer to each other. In our context  the unifying NAC(κ) formulation in (8) permits interpo-
lation between the methodologies using the κ parameter. As discussed at the end of Section 4  the
policy-forgetting term requires a Markovian problem for being justiﬁed: a greedy update implicitly

7

Non-optimistic soft-greedy (small τ)
 Non-convergence (Perkins & Precup)
 Non-optimality (Theorems 2–3)

Non-optimistic soft-greedy (large τ)
 Convergence (Perkins & Precup)
 Non-optimality (Theorems 2–3)

Non-optimistic hard-greedy
 Oscillation
 Non-optimality

(Bertsekas  . . . )

Optimistic hard-greedy
 Chattering
 Non-optimality

(Bertsekas  . . . )

1

κ

0

Natural actor-critic
 Convergence
 Optimality

(Theorem 1)

0

c

c f. F i g . 2

cf. Fig. 2b

Optimistic soft-greedy (large τ)
 Convergence (Melo et al.)
 Non-optimality (Theorems 2–3)

τ

∞

Figure 1: The hyperparameter space of the general form of (approximate) optimistic policy
iteration in (4)  with known convergence and optimality properties (see text for assumptions).

y1

s1

al

s2

al

1/4

ar

ar

1

0

(a) A non-Markovian
problem (adapted from

[24]). The incoming arrow

indicates the start state.

Arrows leading out indicate
termination with the shown

reward.

1

0.5

)
t
h
g
i
r
(
θ
−

)
t
f
e
l
(
θ

0

0

κ = 0.2  τ = 1
κ = 0.2  τ = 0.2
κ = 0.2  τ = 0.05

1

0.5

)
t
h
g
i
r
(
θ
−

)
t
f
e
l
(
θ

κ = τ = 0.2
κ = τ = 0.05
κ = τ = 0.01
NAC (α = 1)

5

10

15

20

iteration

0

0

5

10

15

20

iteration

(b) Non-optimality or oscillation
with κ (cid:54)→ 0. The variants are

marked with

in Fig. 1 (schematic).

(c) Interpolation toward NAC with
κ → 0 and τ → 0. The variants are
marked with in Fig. 1 (schematic).

Figure 2: Empirical illustration of the behavior of optimistic policy iteration ((1)  (2)  (4) and (5) 
with tabular φ) in the proximity of a stochastic optimum. The problem is shown in Fig. 2a. In
Figures 2b and 2c  the optimum at θ(left) − θ(right) = log(2) is denoted by a solid green line. The

uniformly stochastic policy is denoted by a dashed red line.

stands on a Markov assumption and the κ parameter in (8) can be interpreted as adjusting the strength
of this assumption. In this respect  the policy improvement parameter κ in NAC(κ) can be seen (in-
versely) as a dual in spirit to the policy evaluation parameter λ in TD(λ)-style algorithms. On the
policy evaluation side  having λ = 0 obtains variance reduction by assuming and exploiting Marko-
vianity of the problem  while λ = 1 obtains unbiased estimates also for non-Markovian problems.
On the policy improvement side  with κ = 1  we have strictly greedy updates that gain in speed as
the policy can respond instantly to new opportunities appearing in the value function (for empirical
observations of such a speed gain  see [11  25])  and in representational ﬂexibility due to the lack of
continuity constraints between successive policies (for a canonical example  consider ﬁtted Q itera-
tion). This comes at the price of either oscillation or non-optimality if the Markov assumption fails
to hold  which is illustrated in Figure 2b for the problem in 2a. With κ → 0  we approach natural
gradient updates that remain sound also in non-Markovian settings  which is illustrated in Figure 2c.
The possibility to interpolate between the approaches might turn out useful in problems with partial
Markovianity: a large κ in the NAC(κ) formulation can be used to quickly ﬁnd the rough direction of
the strongest attractors  after which gradually decreasing κ allows a convergent ﬁnal ascent toward
an optimum.

Acknowledgments

This work has been ﬁnancially supported by the Academy of Finland through project no. 254104 
and by the Foundation of Nokia Corporation.

8

References

[1] Armijo  L. (1966). Minimization of functions having Lipschitz continuous ﬁrst partial derivatives. Paciﬁc

Journal of Mathematics  16(1)  1–3.

[2] Baxter  J.  & Bartlett  P. L. (2000). Reinforcement learning in POMDP’s via direct gradient ascent. In

Proceedings of the Seventeenth International Conference on Machine Learning  (pp. 41–48).

[3] Bertsekas  D. P. (1997). A new class of incremental gradient methods for least squares problems. SIAM

Journal on Optimization  7(4)  913–926.

[4] Bertsekas  D. P. (2005). Dynamic Programming and Optimal Control. Athena Scientiﬁc.
[5] Bertsekas  D. P. (2010). Pathologies of temporal difference methods in approximate dynamic program-

ming. In 49th IEEE Conference on Decision and Control  (pp. 3034–3039).

[6] Bertsekas  D. P. (2011). Approximate policy iteration: A survey and some new methods. Journal of

Control Theory and Applications  9(3)  310–335.

[7] Bertsekas  D. P.  & Tsitsiklis  J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc.
[8] Bhatnagar  S.  Sutton  R. S.  Ghavamzadeh  M.  & Lee  M. (2009). Natural actor-critic algorithms. Auto-

matica  45(11)  2471–2482.

[9] Bus¸oniu  L.  Babuˇska  R.  De Schutter  B.  & Ernst  D. (2010). Reinforcement learning and dynamic

programming using function approximators. CRC Press.

[10] De Farias  D. P.  & Van Roy  B. (2000). On the existence of ﬁxed points for approximate value iteration

and temporal-difference learning. Journal of Optimization Theory and Applications  105(3)  589–608.

[11] Kakade  S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing Systems.
[12] Kakade  S. M. (2003). On the Sample Complexity of Reinforcement Learning. Ph.D. thesis  University

College London.

[13] Konda  V. R.  & Tsitsiklis  J. N. (2004). On actor-critic algorithms. SIAM Journal on Control and

Optimization  42(4)  1143–1166.

[14] Melo  F. S.  Meyn  S. P.  & Ribeiro  M. I. (2008). An analysis of reinforcement learning with function
approximation. In Proceedings of the 25th International Conference on Machine Learning  (pp. 664–671).
[15] Nedi´c  A.  & Bertsekas  D. P. (2003). Least squares policy evaluation algorithms with linear function

approximation. Discrete Event Dynamic Systems: Theory and Applications  13(1–2)  79–110.

[16] Pendrith  M. D.  & McGarity  M. J. (1998). An analysis of direct reinforcement learning in non-Markovian

domains. In Proceedings of the Fifteenth International Conference on Machine Learning.

[17] Perkins  T. J.  & Pendrith  M. D. (2002). On the existence of ﬁxed points for Q-learning and sarsa in
In Proceedings of the Nineteenth International Conference on Machine

partially observable domains.
Learning  (pp. 490–497).

[18] Perkins  T. J.  & Precup  D. (2003). A convergent form of approximate policy iteration. In Advances in

Neural Information Processing Systems.

[19] Peters  J.  & Schaal  S. (2008). Natural actor-critic. Neurocomputing  71(7-9)  1180–1190.
[20] Singh  S. P.  Jaakkola  T.  & Jordan  M. I. (1994). Learning without state-estimation in partially observable
In Proceedings of the Eleventh International Conference on Machine

Markovian decision processes.
Learning.

[21] Sutton  R. S.  & Barto  A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
[22] Sutton  R. S.  McAllester  D.  Singh  S.  & Mansour  Y. (2000). Policy gradient methods for reinforcement

learning with function approximation. In Advances in Neural Information Processing Systems.
[23] Szepesv´ari  C. (2010). Algorithms for Reinforcement Learning. Morgan & Claypool Publishers.
[24] Wagner  P. (2011). A reinterpretation of the policy oscillation phenomenon in approximate policy itera-

tion. In Advances in Neural Information Processing Systems 24  (pp. 2573–2581).

[25] Wagner  P. (to appear). Policy oscillation is overshooting. Neural Networks. Author manuscript available

at http://users.ics.aalto.fi/pwagner/.

9

,Paul Wagner