2013,Improved and Generalized Upper Bounds on the Complexity of Policy Iteration,Given a Markov Decision Process (MDP) with $n$ states and $m$ actions per state  we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal $\gamma$-discounted optimal policy. We consider two variations of PI: Howard's PI that changes the actions in all states with a positive advantage  and Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard's PI terminates after at most  $ O \left( \frac{ n m}{1-\gamma} \log \left( \frac{1}{1-\gamma} \right)\right) $ iterations  improving by a factor $O(\log n)$ a result by Hansen et al. (2013)  while Simplex-PI terminates after at most $ O \left(  \frac{n^2 m}{1-\gamma} \log \left( \frac{1}{1-\gamma} \right)\right) $ iterations  improving by a factor $O(\log n)$ a result by Ye (2011). Under some structural assumptions of the MDP  we then consider bounds that are independent of the discount factor~$\gamma$: given a measure of the maximal transient time $\tau_t$ and the maximal time $\tau_r$ to revisit states in recurrent classes under all policies  we show that Simplex-PI terminates after at most $ \tilde O \left( n^3 m^2 \tau_t \tau_r \right) $ iterations. This generalizes a recent result for deterministic MDPs by Post & Ye (2012)  in which $\tau_t \le n$ and $\tau_r \le n$. We explain why similar results seem hard to derive for Howard's PI. Finally  under the additional (restrictive) assumption that the state space is partitioned in two sets  respectively states that are transient and recurrent for all policies  we show that Simplex-PI and Howard's PI terminate after at most  $ \tilde O(nm (\tau_t+\tau_r))$ iterations.,Improved and Generalized Upper Bounds on

the Complexity of Policy Iteration

Universit´e de Lorraine  LORIA  UMR 7503  Vandoeuvre-l`es-Nancy  F-54506  France

Bruno Scherrer

Inria  Villers-l`es-Nancy  F-54600  France

bruno.scherrer@inria.fr

Abstract

Given a Markov Decision Process (MDP) with n states and m actions per
state  we study the number of iterations needed by Policy Iteration (PI)
algorithms to converge to the optimal “-discounted optimal policy. We con-
sider two variations of PI: Howard’s PI that changes the actions in all states
with a positive advantage  and Simplex-PI that only changes the action in
the state with maximal advantage. We show that Howard’s PI terminates

improving by a factor O(log n) a result by [3]  while Simplex-PI terminates

after at most n(m ≠ 1)Ï 1
1≠“ log1 1
after at most n2(m ≠ 1)11 + 2

1≠“2Ì = O1 nm
1≠“ log1 1

1≠“ log1 1
1≠“22 = O1 n2m

1≠“22 iterations 
1≠“ log1 1
1≠“22

iterations  improving by a factor O(log n) a result by [11]. Under some
structural assumptions of the MDP  we then consider bounds that are
independent of the discount factor “: given a measure of the maximal tran-
sient time ·t and the maximal time ·r to revisit states in recurrent classes
under all policies  we show that Simplex-PI terminates after at most n2(m≠
1) (Á·r log(n·r)Ë + Á·r log(n·t)Ë)#(m ≠ 1)Án·t log(n·t)Ë + Án·t log(n2·t)Ë$ =
˜O!n3m2·t·r" iterations. This generalizes a recent result for determin-
istic MDPs by [8] 
in which ·t Æ n and ·r Æ n. We explain why
similar results seem hard to derive for Howard’s PI. Finally  under
the additional (restrictive) assumption that the state space is parti-
tioned in two sets  respectively states that are transient and recurrent
for all policies  we show that Howard’s PI terminates after at most
n(m ≠ 1) (Á·t log n·tË + Á·r log n·rË) = ˜O(nm(·t + ·r)) iterations while
Simplex-PI
terminates after n(m ≠ 1) (Án·t log n·tË + Á·r log n·rË) =
˜O(n2m(·t + ·r)) iterations.

1 Introduction

We consider a discrete-time dynamic system whose state transition depends on a control.
We assume that there is a state space X of ﬁnite size n. At state i œ{ 1  ..  n}  the control is
chosen from a control space A of ﬁnite size1 m. The control a œ A speciﬁes the transition
probability pij(a) = P(it+1 = j|it = i  at = a) to the next state j. At each transition 
the system is given a reward r(i  a  j) where r is the instantaneous reward function. In
this context  we look for a stationary deterministic policy (a function ﬁ : X æ A that maps
1In the works of [11  8  3] that we reference  the integer “m” denotes the total number of actions 
that is nm with our notation. When we restate their result  we do it with our own notation  that
is we replace their ÕÕmÕÕ by ÕÕnmÕÕ.

1

states into controls2) that maximizes the expected discounted sum of rewards from any state
i  called the value of policy ﬁ at state i:

vﬁ(i) := EC Œÿk=0

“kr(ik  ak  ik+1)-----

i0 = i  ’k Ø 0  ak = ﬁ(ik)  ik+1 ≥ P(·|ik  ak)D

where “ œ (0  1) is a discount factor. The tuple ÈX  A  p  r  “Í is called a Markov Decision
Process (MDP) [9  1]  and the associated problem is known as optimal control.
The optimal value starting from state i is deﬁned as
vﬁ(i).

vú(i) := max

ﬁ

For any policy ﬁ  we write Pﬁ for the n ◊ n stochastic matrix whose elements are pij(ﬁ(i))
and rﬁ the vector whose components are qj pij(ﬁ(i))r(i  ﬁ(i)  j). The value functions vﬁ
and vú can be seen as vectors on X. It is well known that vﬁ is the solution of the following
Bellman equation:
that is vﬁ is a ﬁxed point of the ane operator Tﬁ : v ‘æ rﬁ + “Pﬁv. It is also well known
that vú satisﬁes the following Bellman equation:

vﬁ = rﬁ + “Pﬁvﬁ 

vú = max

ﬁ

(rﬁ + “Pﬁvú) = max

ﬁ

Tﬁvú

where the max operator is componentwise. In other words  vú is a ﬁxed point of the nonlinear
operator T : v ‘æ maxﬁ Tﬁv. For any value vector v  we say that a policy ﬁ is greedy with
respect to the value v if it satisﬁes:

ﬁ œ arg max

ﬁÕ

TﬁÕv

or equivalently Tﬁv = T v. With some slight abuse of notation  we write G(v) for any policy
that is greedy with respect to v. The notions of optimal value function and greedy policies
are fundamental to optimal control because of the following property: any policy ﬁú that is
greedy with respect to the optimal value vú is an optimal policy and its value vﬁú is equal
to vú.
Let ﬁ be some policy. We call advantage with respect to ﬁ the following quantity:

TﬁÕvﬁ ≠ vﬁ = T vﬁ ≠ vﬁ.
We call the set of switchable states of ﬁ the following set

aﬁ = max
ﬁÕ

Assume now that ﬁ is non-optimal (this implies that Sﬁ is a non-empty set). For any
non-empty subset Y of Sﬁ  we denote switch(ﬁ  Y ) a policy satisfying:

Sﬁ = {i  aﬁ(i) > 0}.

’i  switch(ﬁ  Y )(i) =; G(vﬁ)(i)

ﬁ(i)

if i œ Y
if i ”œ Y.

The following result is well known (see for instance [9]).
Lemma 1. Let ﬁ be some non-optimal policy. If ﬁÕ = switch(ﬁ  Y ) for some non-empty
subset Y of Sﬁ  then vﬁÕ Ø vﬁ and there exists at least one state i such that vﬁÕ(i) > vﬁ(i).
This lemma is the foundation of the well-known iterative procedure  called Policy Iteration
(PI)  that generates a sequence of policies (ﬁk) as follows.

ﬁk+1 Ω switch(ﬁk  Yk) for some set Yk such that ÿ ( Yk ™ Sﬁk .

The choice for the subsets Yk leads to dierent variations of PI. In this paper we will focus
on two speciﬁc variations:

2Restricting our attention to stationary deterministic policies is not a limitation. Indeed  for the
optimality criterion to be deﬁned soon  it can be shown that there exists at least one stationary
deterministic policy that is optimal [9].

2

• When for all iterations k  Yk = Sﬁk  that is one switches the actions in all states with
positive advantage with respect to ﬁk  the above algorithm is known as Howard’s
PI; it can be seen then that ﬁk+1 œG (vﬁk).
• When for all k  Yk is a singleton containing a state ik œ arg maxi aﬁk(i)  that is if
we only switch one action in the state with maximal advantage with respect to ﬁk 
we will call it Simplex-PI3.

Since it generates a sequence of policies with increasing values  any variation of PI converges
to the optimal policy in a number of iterations that is smaller than the total number of
policies mn. In practice  PI converges in very few iterations. On random MDP instances 
convergence often occurs in time sub-linear in n. The aim of this paper is to discuss existing
and provide new upper bounds on the number of iterations required by Howard’s PI and
Simplex-PI that are much sharper than mn.
In the next sections  we describe some known results—see [11] for a recent and comprehensive
review—about the number of iterations required by Howard’s PI and Simplex-PI  along with
some of our original improvements and extensions.4

2 Bounds with respect to a Fixed Discount Factor “< 1
A key observation for both algorithms  that will be central to the results we are about to
discuss  is that the sequence they generate satisﬁes some contraction property5. For any
vector u œ Rn  let ÎuÎŒ = max1ÆiÆn|u(i)| be the max-norm of u. Let 1 be the vector of
which all components are equal to 1.
Lemma 2 (Proof in Section A). The sequence (Îvú ≠ vﬁkÎŒ)kØ0 built by Howard’s PI is
contracting with coecient “.
Lemma 3 (Proof in Section B). The sequence (1T (vú ≠ vﬁk))kØ0 built by Simplex-PI is
contracting with coecient 1 ≠ 1≠“
n .
Though this observation is widely known for Howard’s PI  it was to our knowledge never
mentionned explicitly in the literature for Simplex-PI. These contraction properties have
the following immediate consequence6.
Corollary 1. Let Vmax = maxﬁ ÎrﬁÎŒ
be an upper bound on ÎvﬁÎŒ for all policies ﬁ. In
order to get an ‘-optimal policy  that is a policy ﬁk satisfying Îvú ≠ vﬁkÎŒ Æ ‘  Howard’s
Ì
‘1≠“ Ì iterations  while Simplex-PI requires at most Ï n log nVmax
PI requires at most Ï log Vmax
iterations.
These bounds depend on the precision term ‘  which means that Howard’s PI and Simplex-
PI are weakly polynomial for a ﬁxed discount factor “. An important breakthrough was
recently achieved by [11] who proved that one can remove the dependency with respect to ‘ 
and thus show that Howard’s PI and Simplex-PI are strongly polynomial for a ﬁxed discount
factor “.
Theorem 1 ([11]). Simplex-PI and Howard’s PI both terminate after at most n(m ≠
1)Ï n
1≠“ log1 n2

1≠“2Ì iterations.

3In this case  PI is equivalent to running the simplex algorithm with the highest-pivot rule on a

1≠“

1≠“

‘

linear program version of the MDP problem [11].

4For clarity  all proofs are deferred to the Appendix. The ﬁrst proofs about bounds for the
case “< 1 are given in the Appendix of the paper. The other proofs  that are more involved  are
provided in the Supplementary Material.
5A sequence of non-negative numbers (xk)kØ0 is contracting with coecient – if and only if for
all k Ø 0  xk+1 Æ –xk.
6For Howard’s PI  we have: Îvú≠vﬁkÎŒ Æ “kÎvú≠vﬁ0ÎŒ Æ “kVmax. Thus  a sucient condition
for Îvú≠vﬁkÎŒ <‘ is “kVmax <‘   which is implied by k Ø
. For Simplex-PI  we
‘
log 1
have Îvú ≠ vﬁkÎŒ Æ Îvú ≠ vﬁkÎ1 Æ!1 ≠ 1≠“
“
nVmax  and the conclusion
is similar to that for Howard’s PI.

n "k Îvú ≠ vﬁ0Î1 Æ!1 ≠ 1≠“
n "k

1≠“ >

log Vmax

log Vmax

‘

3

The proof is based on the fact that PI corresponds to the simplex algorithm in a linear
programming formulation of the MDP problem. Using a more direct proof  [3] recently
improved the result by a factor O(n) for Howard’s PI.

Theorem 2 ([3]). Howard’s PI terminates after at most (nm + 1)Ï 1

tions.
Our ﬁrst two results  that are consequences of the contraction properties (Lemmas 2 and
3)  are stated in the following theorems.
in Section C). Howard’s PI terminates after at most n(m ≠
Theorem 3 (Proof
in Section D). Simplex-PI
terminates after at most n(m ≠

1≠“2Ì itera-

1≠“ log1 n

Theorem 4 (Proof

1)Ï 1
1≠“ log1 1
1)Ï n
1≠“ log1 n

1≠“2Ì iterations.
1≠“2Ì iterations.

Our result for Howard’s PI is a factor O(log n) better than the previous best result of [3].
Our result for Simplex-PI is only very slightly better (by a factor 2) than that of [11]  and
uses a proof that is more direct. Using more reﬁned argument  we managed to also improve
the bound for Simplex-PI by a factor O(log n).
in Section E). Simplex-PI terminates after at most n2(m ≠
Theorem 5 (Proof

1)11 + 2

1≠“ log 1

1≠“2 iterations.

Compared to Howard’s PI  our bound for Simplex-PI is a factor O(n) larger. However  since
one changes only one action per iteration  each iteration may have a complexity lower by a
factor n: the update of the value can be done in time O(n2) through the Sherman-Morrisson
formula  though in general each iteration of Howard’s PI  which amounts to compute the
value of some policy that may be arbitrarily dierent from the previous policy  may require
O(n3) time. Overall  both algorithms seem to have a similar complexity.
It is easy to see that the linear dependency of the bound for Howard’s PI with respect to
n is optimal. We conjecture that the linear dependency of both bounds with respect to
m is also optimal. The dependency with respect to the term 1
1≠“ may be improved  but
removing it is impossible for Howard’s PI and very unlikely for Simplex-PI. [2] describes an
MDP for which Howard’s PI requires an exponential (in n) number of iterations for “ = 1
and [5] argued that this holds also when “ is in the vicinity of 1. Though a similar result
does not seem to exist for Simplex-PI in the literature  [7] consider four variations of PI
that all switch one action per iteration  and show through speciﬁcally designed MDPs that
they may require an exponential (in n) number of iterations when “ = 1.

3 Bounds for Simplex-PI that are independent of “
In this section  we will describe some bounds that do not depend on “ but that will be
based on some structural assumptions of the MDPs. On this topic  [8] recently showed the
following result for deterministic MDPs.
Theorem 6 ([8]). If the MDP is deterministic  then Simplex-PI terminates after at most
O(n5m2 log2 n) iterations.
Given a policy ﬁ of a deterministic MDP  states are either on cycles or on paths induced by
ﬁ. The core of the proof relies on the following lemmas that altogether show that cycles are
created regularly and that signiﬁcant progress is made every time a new cycle appears; in
other words  signiﬁcant progress is made regularly.
Lemma 4. If the MDP is deterministic  after at most nmÁ2(n≠ 1) log nË iterations  either
Simplex-PI ﬁnishes or a new cycle appears.
Lemma 5. If the MDP is deterministic  when Simplex-PI moves from ﬁ to ﬁÕ where ﬁÕ
involves a new cycle  we have

1T (vﬁú ≠ vﬁÕ) Æ31 ≠

1

n4 1T (vﬁú ≠ vﬁ).

4

suce

observations

to prove7
these
Indeed 
after
1≠“ ) = ˜O(n4m2). Removing completely the dependency with respect to the
O(n4m2 log n
discount factor “—the term in O(log 1
1≠“ )—requires a careful extra work described in [8] 
which incurs an extra term of order O(n log(n)).
At a more technical level  the proof of [8] critically relies on some properties of the vec-
ﬁ )≠11 that provides a discounted measure of state visitations along the
tor xﬁ = (I ≠ “P T
trajectories induced by a policy ﬁ starting from a uniform distribution:

that Simplex-PI

terminates

Œÿt=0

“tP(it = i | i0 ≥ U  at = ﬁ(it)) 

’i œ X  xﬁ(i) = n
i  we trivially have xﬁ(i) œ11  n

where U denotes the uniform distribution on the state space X. For any policy ﬁ and state

1≠“2. The proof exploits the fact that xﬁ(i) belongs to the
set (1  n) when i is on a path of ﬁ  while xﬁ(i) belongs to the set ( 1
1≠“ ) when i is on
a cycle of ﬁ. As we are going to show  it is possible to extend the proof of [8] to stochastic
MDPs. Given a policy ﬁ of a stochastic MDP  states are either in recurrent classes or
transient classes (these two categories respectively generalize those of cycles and paths).
We will consider the following structural assumption.
Assumption 1. Let ·t Ø 1 and ·r Ø 1 be the smallest constants such that for all policies
ﬁ and all states i 

1≠“   n

(1 Æ )xﬁ(i) Æ ·t
(1 ≠ “)·r Æ xﬁ(i)3 Æ

n

n

1 ≠ “4

if i is transient for ﬁ  and
if i is recurrent for ﬁ.

The constant ·t (resp. ·r) can be seen as a measure of the time needed to leave transient
states (resp. the time needed to revisit states in recurrent classes). In particular  when “
tends to 1  it can be seen that ·t is an upper bound of the expected time L needed to “Leave
the set of transient states”  since for any policy ﬁ 

1
n

lim

·t Ø

lim
“æ1

xﬁ(i) =

“æ1 ÿi transient for ﬁ

Œÿt=0
= E [ L | i0 ≥ U  at = ﬁ(it)] .
Similarly  when “ is in the vicinity of 1  1
is the minimal asymptotic frequency8 in recurrent
·r
states given that one starts from a random uniform state  since for any policy ﬁ and recurrent
state i:

P(it transient for ﬁ | i0 ≥ U  at = ﬁ(it))

n

1 ≠ “

lim
“æ1

“tP(it = i | i0 ≥ U  at = ﬁ(it))

xﬁ(i) = lim
“æ1

Œÿt=0
(1 ≠ “)
T≠1ÿt=0
1
P(it = i | i0 ≥ U  at = ﬁ(it)).
T
With Assumption 1 in hand  we can generalize Lemmas 4-5 as follows.
Lemma
n#(m ≠ 1)Án·t log(n·t)Ë + Án·t log(n2·t)Ë$
new recurrent class appears.

= lim
TæŒ

the MDP

satisﬁes

6.

If

Assumption

at most
iterations either Simplex-PI ﬁnishes or a

after

1 

7This can be done by using arguments similar to the proof of Theorem 4 in Section D.
8If the MDP is aperiodic and irreducible  and thus admits a stationary distribution ‹ﬁ for any

policy ﬁ  one can see that

1
·r

=

min

ﬁ  i recurrent for ﬁ

‹ﬁ(i).

5

Lemma 7. If the MDP satisﬁes Assumption 1  when Simplex-PI moves from ﬁ to ﬁÕ where
ﬁÕ involves a new recurrent class  we have

1T (vﬁú ≠ vﬁÕ) Æ31 ≠

1

·r4 1T (vﬁú ≠ vﬁ).

From these generalized observations  we can deduce the following original result.
Theorem 7 (Proof in Appendix F of the Supp. Material). If the MDP satisﬁes Assump-
tion 1  then Simplex-PI terminates after at most

n2(m ≠ 1) (Á·r log(n·r)Ë + Á·r log(n·t)Ë)#(m ≠ 1)Án·t log(n·t)Ë + Án·t log(n2·t)Ë$

iterations.
Remark 1. This new result is a strict generalization of the result for deterministic MDPs.
Indeed  in the deterministic case  we have ·t Æ n and ·r Æ n  and it is is easy to see that
Lemmas 6  7 and Theorem 7 respectively imply Lemmas 4  5 and Theorem 6.
An immediate consequence of the above result is that Simplex-PI is strongly polynomial for
sets of MDPs that are much larger than the deterministic MDPs mentionned in Theorem 6.
Corollary 2. For any family of MDPs indexed by n and m such that ·t and ·r are polyno-
mial functions of n and m  Simplex-PI terminates after a number of steps that is polynomial
in n and m.

4 Similar results for Howard’s PI?
One may then wonder whether similar results can be derived for Howard’s PI. Unfortunately 
and as quickly mentionned by [8]  the line of analysis developped for Simplex-PI does not
seem to adapt easily to Howard’s PI  because simultaneously switching several actions can
interfere in a way that the policy improvement turns out to be small. We can be more
precise on what actually breaks in the approach we have described so far. On the one hand 
it is possible to write counterparts of Lemmas 4 and 6 for Howard’s PI (see Appendix G of
the Supp. Material).
Lemma 8. If the MDP is deterministic  after at most n iterations  either Howard’s PI
ﬁnishes or a new cycle appears.
Lemma 9. If the MDP satisﬁes Assumption 1  after at most nmÁ·t log n·tË iterations 
either Howard’s PI ﬁnishes or a new recurrent class appears.
However  on the other hand  we did not manage to adapt Lemma 5 nor Lemma 7. In fact 
it is unlikely that a result similar to that of Lemma 5 will be shown to hold for Howard’s PI.
In a recent deterministic example due to [4] to show that Howard’s PI may require at most
O(n2) iterations  new cycles are created every single iteration but the sequence of values
satisﬁes9 for all iterations k < n2

4 + n

4 and states i 

vú(i) ≠ vﬁk+1(i) ØC1 ≠3 2

n4kD (vú(i) ≠ vﬁk(i)).

Contrary to Lemma 5  as k grows  the amount of contraction gets (exponentially) smaller and
smaller. With respect to Simplex-PI  this suggests that Howard’s PI may suer from subtle
speciﬁc pathologies. In fact  the problem of determining the number of iterations required
by Howard’s PI has been challenging for almost 30 years. It was originally identiﬁed as
an open problem by [10]. In the simplest—deterministic—case  the question is still open:
the currently best known lower bound is the O(n2) bound by [4] we have just mentionned 
while the best known upper bound is O( mn

n ) (valid for all MDPs) due to [6].

9This MDP has an even number of states n = 2p. The goal is to minimize the long term expected
cost. The optimal value function satisﬁes vú(i) = ≠pN for all i  with N = p2 + p. The policies
generated by Howard’s PI have values vﬁk(i) œ (pN≠k≠1  pN≠k). We deduce that for all iterations
k and states i  vú(i)≠vﬁk+1 (i)

1+p≠k Ø 1 ≠ p≠k(1 ≠ p≠2) Ø 1 ≠ p≠k.

vú(i)≠vﬁk (i) Ø 1+p≠k≠2

1+p≠k = 1 ≠ p≠k≠p≠k≠2

6

On the positive side  an adaptation of the line of proof we have considered so far can be
carried out under the following assumption.
Assumption 2. The state space X can be partitioned in two sets T and R such that for
all policies ﬁ  the states of T are transient and those of R are recurrent.
Indeed  under this assumption  we can prove for Howard’s PI a variation of Lemma 7
introduced for Simplex-PI.
Lemma 10. For an MDP satisfying Assumptions 1-2  suppose Howard’s PI moves from ﬁ
to ﬁÕ and that ﬁÕ involves a new recurrent class. Then

1T (vﬁú ≠ vﬁÕ) Æ31 ≠

1

·r4 1T (vﬁú ≠ vﬁ).

And we can deduce the following original bound (that also applies to Simplex-PI).
Theorem 8 (Proof in Appendix H of the Supp. Material). If the MDP satisﬁes Assump-
tions 1-2  then Howard’s PI terminates after at most n(m ≠ 1) (Á·t log n·tË + Á·r log n·rË)
iterations  while Simplex-PI terminates after at most n(m ≠ 1) (Án·t log n·tË + Á·r log n·rË)
iterations.
It should however be noted that Assumption 2 is rather restrictive. It implies that the algo-
rithms converge on the recurrent states independently of the transient states  and thus the
analysis can be decomposed in two phases: 1) the convergence on recurrent states and then
2) the convergence on transient states (given that recurrent states do not change anymore).
The analysis of the ﬁrst phase (convergence on recurrent states) is greatly facilitated by the
fact that in this case  a new recurrent class appears every single iteration (this is in contrast
with Lemmas 4  6  8 and 9 that were designed to show under which conditions cycles and
recurrent classes are created). Furthermore  the analysis of the second phase (convergence
on transient states) is similar to that of the discounted case of Theorems 3 and 4. In other
words  if this last result sheds some light on the practical eciency of Howard’s PI and
Simplex-PI  a general analysis of Howard’s PI is still largely open  and constitutes our main
future work.

A Contraction property for Howard’s PI (Proof of Lemma 2)
For any k  using the facts that {’ﬁ  Tﬁvﬁ = vﬁ}  {Tﬁú
{Lemma 1 and Pﬁk is positive deﬁnite}  we have

vﬁk≠1 Æ Tﬁk vﬁk≠1} and

vﬁú ≠ vﬁk = Tﬁú

vﬁú ≠ Tﬁú

vﬁk≠1 ≠ Tﬁk vﬁk≠1 + Tﬁk vﬁk≠1 ≠ Tﬁk vﬁk
Æ “Pﬁú(vﬁú ≠ vﬁk≠1) + “Pﬁk(vﬁk≠1 ≠ vﬁk) Æ “Pﬁú(vﬁú ≠ vﬁk≠1).

vﬁk≠1 + Tﬁú

Since vﬁú ≠ vﬁk is non negative  we can take the max norm and get: Îvﬁú ≠ vﬁkÎŒ Æ
“Îvﬁú ≠ vﬁk≠1ÎŒ.
B Contraction property for Simplex-PI (Proof of Lemma 3)
By using the fact that {vﬁ = Tﬁvﬁ ∆ vﬁ = (I ≠ “Pﬁ)≠1rﬁ}  we have that for all pairs of
policies ﬁ and ﬁÕ.

vﬁÕ ≠ vﬁ = (I ≠ “PﬁÕ)≠1rﬁÕ ≠ vﬁ = (I ≠ “PﬁÕ)≠1(rﬁÕ + “PﬁÕvﬁ ≠ vﬁ)

= (I ≠ “PﬁÕ)≠1(TﬁÕvﬁ ≠ vﬁ).

(1)
On the one hand  by using this lemma and the fact that {Tﬁk+1vﬁk ≠ vﬁk Ø 0}  we have for
any k: vﬁk+1 ≠ vﬁk = (I ≠ “Pk+1)≠1(Tﬁk+1vﬁk ≠ vﬁk) Ø Tﬁk+1vﬁk ≠ vﬁk   which implies that
(2)
On the other hand  using Equation (1) and the facts that {Î(I ≠ “Pﬁú)≠1ÎŒ =
1≠“ and (I ≠ “Pﬁú)≠1 is positive deﬁnite}  {maxs Tﬁk+1vﬁk(s) = maxs ˜ﬁ T˜ﬁvﬁk(s)} and

1T (vﬁk+1 ≠ vﬁk) Ø 1T (Tﬁk+1vﬁk ≠ vﬁk).

1

7

{’x Ø 0  maxs x(s) Æ 1T x}  we have:
vﬁú ≠ vﬁk = (I ≠ “Pﬁú)≠1(Tﬁú

vﬁk ≠ vﬁk) Æ

1
1 ≠ “
Tﬁk+1vﬁk(s) ≠ vﬁk(s) Æ

vﬁk(s) ≠ vﬁk(s)
Tﬁú
1T (Tﬁk+1vﬁk ≠ vﬁk) 

Æ

1
1 ≠ “

max
s

which implies (using {’x  1T x Æ nÎxÎŒ}) that

1T (Tﬁk+1vﬁk ≠ vﬁk) Ø (1 ≠ “)Îvﬁú ≠ vﬁkÎŒ Ø

1T (vﬁú ≠ vﬁk).

(3)

max
s
1
1 ≠ “
1 ≠ “

n

Combining Equations (2) and (3)  we get:
1T (vﬁú ≠ vﬁk+1) = 1T (vﬁú ≠ vﬁk) ≠ 1T (vﬁk+1 ≠ vﬁk)

Æ 1T (vﬁú ≠ vﬁk) ≠

1 ≠ “

n

1T (vﬁú ≠ vﬁk) =31 ≠

n 4 1T (vﬁú ≠ vﬁk).
1 ≠ “

C A bound for Howard’s PI when “< 1 (Proof of Theorem 3)
For any k  by using Equation (1) and the fact {vú ≠ vﬁk Ø 0 and Pﬁk positive deﬁnite}  we
have:

vú ≠ Tﬁk vú = (I ≠ “Pﬁk)(vú ≠ vﬁk) Æ vú ≠ vﬁk .

Since vú≠Tﬁk vú is non negative  we can take the max norm and  using Lemma 2  Equation (1)
and the fact that {Î(I ≠ “Pﬁ0)≠1ÎŒ = 1

1≠“}  we get:

Îvú ≠ Tﬁk vúÎŒ Æ Îvú ≠ vﬁkÎŒ Æ “kÎvﬁú ≠ vﬁ0ÎŒ

= “kÎ(I ≠ “Pﬁ0)≠1(vú ≠ Tﬁ0vú)ÎŒ Æ
(4)
By deﬁnition of the max-norm  there exists a state s0 such that vú(s0) ≠ [Tﬁ0vú](s0) =
Îvú ≠ Tﬁ0vúÎŒ. From Equation (4)  we deduce that for all k 
vú(s0) ≠ [Tﬁk vú](s0) Æ Îvú ≠ Tﬁk vúÎŒ Æ

1 ≠ “Îvú ≠ Tﬁ0vúÎŒ.

(vú(s0) ≠ [Tﬁ0vú](s0)).

“k

“k

1 ≠ “Îvú ≠ Tﬁ0vúÎŒ = “k
1 ≠ “
1≠“ Ì >Ï log 1

1≠“
log 1

1≠“

As a consequence  the action ﬁk(s0) must be dierent from ﬁ0(s0) when “k

1≠“ < 1  that is for
“ Ì . In other words  if some policy ﬁ

all values of k satisfying k Ø kú =Ï log 1
is not optimal  then one of its non-optimal actions will be eliminated for good after at most
kú iterations. By repeating this argument  one can eliminate all non-optimal actions (they
are at most n(m ≠ 1))  and the result follows.
D A bound for Simplex-PI when “< 1 (Proof of Theorem 4)
Using {’x Ø 0  ÎxÎŒ Æ 1T x}  Lemma 3  {’x  1T x Æ nÎxÎŒ}  Equation (1) and {Î(I ≠
“Pﬁ0)≠1ÎŒ = 1
1≠“}  we have for all k 
Îvﬁú ≠ Tﬁk vﬁúÎŒ Æ Îvﬁú ≠ vﬁkÎŒ Æ 1T (vﬁú ≠ vﬁk)
Æ31 ≠
n 4k
1T (vﬁú ≠ vﬁ0) Æ n31 ≠
n 4k
1 ≠ “
1 ≠ “
n 4k
= n31 ≠
1 ≠ “
Î(I ≠ “Pﬁ0)≠1(vú ≠ Tﬁ0vú)ÎŒ Æ
after at most kú =Ï n
obtained by noting that there are at most n(m ≠ 1) non optimal actions to eliminate.

n ):   and the overall number of iterations is

Similarly to the proof for Howard’s PI  we deduce that a non-optimal action is eliminated

1≠“Ì Ø9 log n

1 ≠ “31 ≠

n 4k
1 ≠ “

Îvﬁú ≠ Tﬁ0vﬁúÎŒ

Îvﬁú ≠ vﬁ0ÎŒ

1≠“ log n

1≠“
log(1≠ 1≠“

n

8

References
[1] D.P. Bertsekas and J.N. Tsitsiklis. Neurodynamic Programming. Athena Scientiﬁc 

1996.

[2] J. Fearnley. Exponential lower bounds for policy iteration. In Proceedings of the 37th
international colloquium conference on Automata  languages and programming: Part
II  ICALP’10  pages 551–562  Berlin  Heidelberg  2010. Springer-Verlag.

[3] T.D. Hansen  P.B. Miltersen  and U. Zwick. Strategy iteration is strongly polynomial
for 2-player turn-based stochastic games with a constant discount factor. J. ACM 
60(1):1:1–1:16  February 2013.

[4] T.D. Hansen and U. Zwick. Lower bounds for howard’s algorithm for ﬁnding minimum

mean-cost cycles. In ISAAC (1)  pages 415–426  2010.

[5] R. Hollanders  J.C. Delvenne  and R. Jungers. The complexity of policy iteration is
In 51st IEEE conference on

exponential for discounted markov decision processes.
Decision and control (CDC’12)  2012.

[6] Y. Mansour and S.P. Singh. On the complexity of policy iteration.

401–408  1999.

In UAI  pages

[7] M. Melekopoglou and A. Condon. On the complexity of the policy improvement algo-
rithm for markov decision processes. INFORMS Journal on Computing  6(2):188–192 
1994.

[8] I. Post and Y. Ye. The simplex method is strongly polynomial for deterministic markov

decision processes. Technical report  arXiv:1208.5083v2  2012.

[9] M. Puterman. Markov Decision Processes. Wiley  New York  1994.
[10] N. Schmitz. How good is howard’s policy improvement algorithm? Zeitschrift f¨ur

Operations Research  29(7):315–316  1985.

[11] Y. Ye. The simplex and policy-iteration methods are strongly polynomial for the markov

decision problem with a ﬁxed discount rate. Math. Oper. Res.  36(4):593–603  2011.

9

,Bruno Scherrer
Ming-Yu Liu
Oncel Tuzel