2017,A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning,This paper takes a step towards temporal reasoning in a dynamically changing video  not in the pixel space that constitutes its frames  but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder  a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation  coming from a recognition model  and a latent state describing its dynamics. As a result  the evolution of the world can be imagined and missing data imputed  both without the need to  generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems  and outperforms competing methods in generative and missing data imputation tasks.,A Disentangled Recognition and Nonlinear Dynamics

Model for Unsupervised Learning

Marco Fraccaro†∗

Simon Kamronn †∗
† Technical University of Denmark

Ulrich Paquet‡

‡ DeepMind

Ole Winther†

Abstract

This paper takes a step towards temporal reasoning in a dynamically changing video 
not in the pixel space that constitutes its frames  but in a latent space that describes
the non-linear dynamics of the objects in its world. We introduce the Kalman
variational auto-encoder  a framework for unsupervised learning of sequential data
that disentangles two latent representations: an object’s representation  coming
from a recognition model  and a latent state describing its dynamics. As a result  the
evolution of the world can be imagined and missing data imputed  both without the
need to generate high dimensional frames at each time step. The model is trained
end-to-end on videos of a variety of simulated physical systems  and outperforms
competing methods in generative and missing data imputation tasks.

1

Introduction

From the earliest stages of childhood  humans learn to represent high-dimensional sensory input
to make temporal predictions. From the visual image of a moving tennis ball  we can imagine
its trajectory  and prepare ourselves in advance to catch it. Although the act of recognising the
tennis ball is seemingly independent of our intuition of Newtonian dynamics [31]  very little of this
assumption has yet been captured in the end-to-end models that presently mark the path towards
artiﬁcial general intelligence. Instead of basing inference on any abstract grasp of dynamics that is
learned from experience  current successes are autoregressive: to imagine the tennis ball’s trajectory 
one forward-generates a frame-by-frame rendering of the full sensory input [5  7  23  24  29  30].
To disentangle two latent representations  an object’s  and that of its dynamics  this paper introduces
Kalman variational auto-encoders (KVAEs)  a model that separates an intuition of dynamics from
an object recognition network (section 3). At each time step t  a variational auto-encoder [18  25]
compresses high-dimensional visual stimuli xt into latent encodings at. The temporal dynamics in
the learned at-manifold are modelled with a linear Gaussian state space model that is adapted to
handle complex dynamics (despite the linear relations among its states zt). The parameters of the
state space model are adapted at each time step  and non-linearly depend on past at’s via a recurrent
neural network. Exact posterior inference for the linear Gaussian state space model can be preformed
with the Kalman ﬁltering and smoothing algorithms  and is used for imputing missing data  for
instance when we imagine the trajectory of a bouncing ball after observing it in initial and ﬁnal video
frames (section 4). The separation between recognition and dynamics model allows for missing data
imputation to be done via a combination of the latent states zt of the model and its encodings at only 
without having to forward-sample high-dimensional images xt in an autoregressive way. KVAEs are
tested on videos of a variety of simulated physical systems in section 5: from raw visual stimuli  it
“end-to-end” learns the interplay between the recognition and dynamics components. As KVAEs can
do smoothing  they outperform an array of methods in generative and missing data imputation tasks
(section 5).

∗Equal contribution.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

2 Background

Linear Gaussian state space models. Linear Gaussian state space models (LGSSMs) are widely
used to model sequences of vectors a = a1:T = [a1  ..  aT ]. LGSSMs model temporal correlations
through a ﬁrst-order Markov process on latent states z = [z1  ..  zT ]  which are potentially further
controlled with external inputs u = [u1  ..  uT ]  through the Gaussian distributions

pγt(zt|zt−1  ut) = N (zt; Atzt−1 + Btut  Q)  

(1)
Matrices γt = [At  Bt  Ct] are the state transition  control and emission matrices at time t. Q and R
are the covariance matrices of the process and measurement noise respectively. With a starting state
z1 ∼ N (z1; 0  Σ)  the joint probability distribution of the LGSSM is given by

pγt(at|zt) = N (at; Ctzt  R) .

pγ(a  z|u) = pγ(a|z) pγ(z|u) =(cid:81)T

t=1pγt(at|zt) · p(z1)(cid:81)T

(2)
where γ = [γ1  ..  γT ]. LGSSMs have very appealing properties that we wish to exploit: the ﬁltered
and smoothed posteriors p(zt|a1:t  u1:t) and p(zt|a  u) can be computed exactly with the classical
Kalman ﬁlter and smoother algorithms  and provide a natural way to handle missing data.

t=2 pγt(zt|zt−1  ut)  

Variational auto-encoders. A variational auto-encoder (VAE) [18  25] deﬁnes a deep generative
model pθ(xt  at) = pθ(xt|at)p(at) for data xt by introducing a latent encoding at. Given a
likelihood pθ(xt|at) and a typically Gaussian prior p(at)  the posterior pθ(at|xt) represents a
stochastic map from xt to at’s manifold. As this posterior is commonly analytically intractable  VAEs
approximate it with a variational distribution qφ(at|xt) that is parameterized by φ. The approximation
qφ is commonly called the recognition  encoding  or inference network.

3 Kalman Variational Auto-Encoders

The useful information that describes the movement and interplay of objects in a video typically lies
in a manifold that has a smaller dimension than the number of pixels in each frame. In a video of a
ball bouncing in a box  like Atari’s game Pong  one could deﬁne a one-to-one mapping from each
of the high-dimensional frames x = [x1  ..  xT ] into a two-dimensional latent space that represents
the position of the ball on the screen. If the position was known for consecutive time steps  for a
set of videos  we could learn the temporal dynamics that govern the environment. From a few new
positions one might then infer where the ball will be on the screen in the future  and then imagine the
environment with the ball in that position.
The Kalman variational auto-encoder (KVAE) is based
on the notion described above. To disentangle recognition
and spatial representation  a sensory input xt is mapped to
at (VAE)  a variable on a low-dimensional manifold that
encodes an object’s position and other visual properties. In
turn  at is used as a pseudo-observation for the dynamics
model (LGSSM). xt represents a frame of a video2 x =
[x1  ..  xT ] of length T . Each frame is encoded into a
point at on a low-dimensional manifold  so that the KVAE
contains T separate VAEs that share the same decoder
pθ(xt|at) and encoder qφ(at|xt)  and depend on each
other through a time-dependent prior over a = [a1  ..  aT ].
This is illustrated in ﬁgure 1.

LGSSM

ut−1

VAE

xt−1

at−1

zt−1

at

zt

ut

ut+1

xt

xt+1

at+1

zt+1

3.1 Generative model

factorizes as pθ(x|a) = (cid:81)T

Figure 1: A KVAE is formed by stack-
ing a LGSSM (dashed blue)  and a VAE
(dashed red). Shaded nodes denote ob-
served variables. Solid arrows represent
the generative model (with parameters θ)
while dashed arrows represent the VAE
inference network (with parameters φ).
2While our main focus in this paper are videos  the same ideas could be applied more in general to any

We assume that a acts as a latent representation of the
whole video  so that the generative model of a sequence
t=1 pθ(xt|at). In this paper
pθ(xt|at) is a deep neural network parameterized by θ 

sequence of high dimensional data.

2

(cid:90)

that emits either a factorized Gaussian or Bernoulli probability vector depending on the data type of
xt. We model a with a LGSSM  and following (2)  its prior distribution is

pγ(a|u) =

pγ(a|z) pγ(z|u) dz  

(3)
so that the joint density for the KVAE factorizes as p(x  a  z|u) = pθ(x|a) pγ(a|z) pγ(z|u). A
LGSSM forms a convenient back-bone to a model  as the ﬁltered and smoothed distributions
pγ(zt|a1:t  u1:t) and pγ(zt|a  u) can be obtained exactly. Temporal reasoning can be done in the
latent space of zt’s and via the latent encodings a  and we can do long-term predictions without
having to auto-regressively generate high-dimensional images xt. Given a few frames  and hence
their encodings  one could “remain in latent space” and use the smoothed distributions to impute
missing frames. Another advantage of using a to separate the dynamics model from x can be seen by
considering the emission matrix Ct. Inference in the LGSSM requires matrix inverses  and using
it as a model for the prior dynamics of at allows the size of Ct to remain small  and not scale with
the number of pixels in xt. While the LGSSM’s process and measurement noise in (1) are typically
formulated with full covariance matrices [26]  we will consider them as isotropic in a KVAE  as at
act as a prior in a generative model that includes these extra degrees of freedom.
What happens when a ball bounces against a wall  and the dynamics on at are not linear any more?
Can we still retain a LGSSM backbone? We will incorporate nonlinearities into the LGSSM by
regulating γt from outside the exact forward-backward inference chain. We revisit this central idea at
length in section 3.3.

(cid:90)

log likelihoods L =(cid:80)

3.2 Learning and inference for the KVAE
We learn θ and γ from a set of example sequences {x(n)} by maximizing the sum of their respective
n log pθγ(x(n)|u(n)) as a function of θ and γ. For simplicity in the exposition
we restrict our discussion below to one sequence  and omit the sequence index n. The log likelihood or
evidence is an intractable average over all plausible settings of a and z  and exists as the denominator
in Bayes’ theorem when inferring the posterior p(a  z|x  u). A more tractable approach to both
learning and inference is to introduce a variational distribution q(a  z|x  u) that approximates the
posterior. The evidence lower bound (ELBO) F is
log p(x|u) = log
p(x  a  z|u) ≥ Eq(a z|x u)
(4)
and a sum of F’s is maximized instead of a sum of log likelihoods. The variational distribution
q depends on φ  but for the bound to be tight we should specify q to be equal to the posterior
distribution that only depends on θ and γ. Towards this aim we structure q so that it incorporates the
exact conditional posterior pγ(z|a  u)  that we obtain with Kalman smoothing  as a factor of γ:

pθ(x|a)pγ(a|z)pγ(z|u)

= F(θ  γ  φ)  

q(a  z|x  u)

q(a  z|x  u) = qφ(a|x) pγ(z|a  u) =(cid:81)T

(5)
The beneﬁt of the LGSSM backbone is now apparent. We use a “recognition model” to encode each
xt using a non-linear function  after which exact smoothing is possible. In this paper qφ(at|xt) is a
deep neural network that maps xt to the mean and the diagonal covariance of a Gaussian distribution.
As explained in section 4  this factorization allows us to deal with missing data in a principled way.
Using (5)  the ELBO in (4) becomes

t=1qφ(at|xt) pγ(z|a  u) .

(cid:20)

(cid:21)

log

+ Epγ (z|a u)

F(θ  γ  φ) = Eqφ(a|x)

(6)

.

1
I

log

log

(cid:88)

pγ(z|a  u)

drawn from q 
ˆF(θ  γ  φ) =

The lower bound in (6) can be estimated using Monte Carlo integration with samples {(cid:101)a(i) (cid:101)z(i)}I
log pθ(x|(cid:101)a(i))+log pγ((cid:101)a(i) (cid:101)z(i)|u)−log qφ((cid:101)a(i)|x)−log pγ((cid:101)z(i)|(cid:101)a(i)  u) . (7)
Note that the ratio pγ((cid:101)a(i) (cid:101)z(i)|u)/pγ((cid:101)z(i)|(cid:101)a(i)  u) in (7) gives pγ((cid:101)a(i)|u)  but the formulation with
{(cid:101)z(i)} allows stochastic gradients on γ to also be computed. A sample from q can be obtained by
ﬁrst sampling(cid:101)a ∼ qφ(a|x)  and using(cid:101)a as an observation for the LGSSM. The posterior pγ(z|(cid:101)a  u)
can be tractably obtained with a Kalman smoother  and a sample(cid:101)z ∼ pγ(z|(cid:101)a  u) obtained from it.

Parameter learning is done by jointly updating θ  φ  and γ by maximising the ELBO on L  which
decomposes as a sum of ELBOs in (6)  using stochastic gradient ascent and a single sample to
approximate the intractable expectations.

i=1

i

pθ(x|a)
qφ(a|x)

(cid:20)

(cid:20)

(cid:21)(cid:21)

pγ(a|z)pγ(z|u)

3

3.3 Dynamics parameter network
The LGSSM provides a tractable way to structure pγ(z|a  u) into the variational approximation in
(5). However  even in the simple case of a ball bouncing against a wall  the dynamics on at are not
linear anymore. We can deal with these situations while preserving the linear dependency between
consecutive states in the LGSSM  by non-linearly changing the parameters γt of the model over time
as a function of the latent encodings up to time t − 1 (so that we can still deﬁne a generative model).
Smoothing is still possible as the state transition matrix At and others in γt do not have to be constant
in order to obtain the exact posterior pγ(zt|a  u).
Recall that γt describes how the latent state zt−1 changes from time t − 1 to time t. In the more
general setting  the changes in dynamics at time t may depend on the history of the system  encoded
in a1:t−1 and possibly a starting code a0 that can be learned from data. If  for instance  we see the ball
colliding with a wall at time t − 1  then we know that it will bounce at time t and change direction.
We then let γt be a learnable function of a0:t−1  so that the prior in (2) becomes

pγ(a  z|u) =(cid:81)T

t=1pγt(a0:t−1)(at|zt) · p(z1)(cid:81)T

t=2 pγt(a0:t−1)(zt|zt−1  ut) .

(8)

During inference  after all the frames are encoded in a  the
dynamics parameter network returns γ = γ(a)  the param-
eters of the LGSSM at all time steps. We can now use the
Kalman smoothing algorithm to ﬁnd the exact conditional
posterior over z  that will be used when computing the
gradients of the ELBO.
In our experiments the dependence of γt on a0:t−1 is
modulated by a dynamics parameter network αt =
αt(a0:t−1)  that is implemented with a recurrent neu-
ral network with LSTM cells that takes at each time
step the encoded state as input and recurses dt =
LSTM(at−1  dt−1) and αt = softmax(dt)  as illustrated
in ﬁgure 2. The output of the dynamics parameter network

is weights that sum to one (cid:80)K

k=1 α(k)

t

K different operating modes:

K(cid:88)

αt−1

αt

αt+1

dt−1

dt

dt+1

at−2

at−1

at

Figure 2: Dynamics parameter network
for the KVAE.

(a0:t−1) = 1. These weights choose and interpolate between

K(cid:88)

K(cid:88)

At =

α(k)

t

(a0:t−1)A(k)  Bt =

α(k)

t

(a0:t−1)B(k)  Ct =

α(k)

t

(a0:t−1)C(k) .

(9)

k=1

k=1

k=1

We globally learn K basic state transition  control and emission matrices A(k)  B(k) and C(k)  and
interpolate them based on information from the VAE encodings. The weighted sum can be interpreted
as a soft mixture of K different LGSSMs whose time-invariant matrices are combined using the time-
varying weights αt. In practice  each of the K sets {A(k)  B(k)  C(k)} models different dynamics 
that will dominate when the corresponding α(k)
is high. The dynamics parameter network resembles
the locally-linear transitions of [16  33]; see section 6 for an in depth discussion on the differences.

t

4 Missing data imputation

Let xobs be an observed subset of frames in a video sequence  for instance depicting the initial
movement and ﬁnal positions of a ball in a scene. From its start and end  can we imagine how
the ball reaches its ﬁnal position? Autoregressive models like recurrent neural networks can only
forward-generate xt frame by frame  and cannot make use of the information coming from the ﬁnal
frames in the sequence. To impute the unobserved frames xun in the middle of the sequence  we need
to do inference  not prediction.
The KVAE exploits the smoothing abilities of its LGSSM to use both the information from the past
and the future when imputing missing data. In general  if x = {xobs  xun}  the unobserved frames in
xun could also appear at non-contiguous time steps  e.g. missing at random. Data can be imputed
by sampling from the joint density p(aun  aobs  z|xobs  u)  and then generating xun from aun. We
factorize this distribution as

p(aun  aobs  z|xobs  u) = pγ(aun|z) pγ(z|aobs  u) p(aobs|xobs)  

(10)

4

(cid:90)
(cid:90)

(11)

(12)

and we sample from it with ancestral sampling starting from xobs. Reading (10) from right to left  a
sample from p(aobs|xobs) can be approximated with the variational distribution qφ(aobs|xobs). Then 
if γ is fully known  pγ(z|aobs  u) is computed with an extension to the Kalman smoothing algorithm
to sequences with missing data  after which samples from pγ(aun|z) could be readily drawn.
However  when doing missing data imputation the parameters γ of the LGSSM are not known at
all time steps. In the KVAE  each γt depends on all the previous encoded states  including aun  and
these need to be estimated before γ can be computed. In this paper we recursively estimate γ in the
following way. Assume that x1:t−1 is known  but not xt. We sample a1:t−1 from qφ(a1:t−1|x1:t−1)
using the VAE  and use it to compute γ1:t. The computation of γt+1 depends on at  which is missing 
distribution pγ(zt−1|a1:t−1  u1:t−1) can be computed as it depends only on γ1:t−1  and from it  we
sample

and an estimate(cid:98)at will be used. Such an estimate can be arrived at in two steps. The ﬁltered posterior

(cid:98)zt ∼ pγ(zt|a1:t−1  u1:t) =
(cid:98)at ∼ pγ(at|a1:t−1  u1:t) =

and sample(cid:98)at from the predictive distribution of at 
The parameters of the LGSSM at time t + 1 are then estimated as γt+1([a0:t−1 (cid:98)at]). The same

pγt(zt|zt−1  ut) pγ(zt−1|a1:t−1  u1:t−1) dzt−1
pγt(at|zt) pγ(zt|a1:t−1  u1:t) dzt ≈ pγt(at|(cid:98)zt) .

procedure is repeated at the next time step if xt+1 is missing  otherwise at+1 is drawn from the VAE.
After the forward pass through the sequence  where we estimate γ and compute the ﬁltered posterior
for z  the Kalman smoother’s backwards pass computes the smoothed posterior. While the smoothed
posterior distribution is not exact  as it relies on the estimate of γ obtained during the forward pass  it
improves data imputation by using information coming from the whole sequence; see section 5 for an
experimental illustration.

5 Experiments

We motivated the KVAE with an example of a bouncing ball  and use it here to demonstrate the
model’s ability to separately learn a recognition and dynamics model from video  and use it to impute
missing data. To draw a comparison with deep variational Bayes ﬁlters (DVBFs) [16]  we apply
the KVAE to [16]’s pendulum example. We further apply the model to a number of environments
with different properties to demonstrate its generalizability. All models are trained end-to-end with
stochastic gradient descent. Using the control input ut in (1) we can inform the model of known
quantities such as external forces  as will be done in the pendulum experiment. In all the other
experiments  we omit such information and train the models fully unsupervised from the videos only.
Further implementation details can be found in the supplementary material (appendix A) and in the
Tensorﬂow [1] code released at github.com/simonkamronn/kvae.

5.1 Bouncing ball

We simulate 5000 sequences of 20 time steps each of a ball moving in a two-dimensional box  where
each video frame is a 32x32 binary image. A video sequence is visualised as a single image in ﬁgure
4d  with the ball’s darkening color reﬂecting the incremental frame index. In this set-up the initial
position and velocity are randomly sampled. No forces are applied to the ball  except for the fully
elastic collisions with the walls. The minimum number of latent dimensions that the KVAE requires
to model the ball’s dynamics are at ∈ R2 and zt ∈ R4  as at the very least the ball’s position in the
box’s 2d plane has to be encoded in at  and zt has to encode the ball’s position and velocity. The
model’s ﬂexibility increases with more latent dimensions  but we choose these settings for the sake of
interpretable visualisations. The dynamics parameter network uses K = 3 to interpolate three modes 
a constant velocity  and two non-linear interactions with the horizontal and vertical walls.
We compare the generation and imputation performance of the KVAE with two recurrent neural
network (RNN) models that are based on the same auto-encoding (AE) architecture as the KVAE and
are modiﬁcations of methods from the literature to be better suited to the bouncing ball experiments.3

3We also experimented with the SRNN model from [8] as it can do smoothing. However  the model is

probably too complex for the task in hand  and we could not make it learn good dynamics.

5

(a) Frames xt missing completely at random.

(b) Frames xt missing in the middle of the sequence.

(c) Comparison of encoded (ground truth)  generated and smoothed trajectories of a KVAE in the latent space
a. The black squares illustrate observed samples and the hexagons indicate the initial state. Notice that the
at’s lie on a manifold that can be rotated and stretched to align with the frames of the video.

Figure 3: Missing data imputation results.

In the AE-RNN  inspired by the architecture from [29]  a pretrained convolutional auto-encoder 
identical to the one used for the KVAE  feeds the encodings to an LSTM network [13]. During
training the LSTM predicts the next encoding in the sequence and during generation we use the
previous output as input to the current step. For data imputation the LSTM either receives the previous
output or  if available  the encoding of the observed frame (similarly to ﬁltering in the KVAE). The
VAE-RNN is identical to the AE-RNN except that uses a VAE instead of an AE  similarly to the model
from [6].
Figure 3a shows how well missing frames are imputed in terms of the average fraction of incorrectly
guessed pixels. In it  the ﬁrst 4 frames are observed (to initialize the models) after which the next
16 frames are dropped at random with varying probabilities. We then impute the missing frames
by doing ﬁltering and smoothing with the KVAE. We see in ﬁgure 3a that it is beneﬁcial to utilize
information from the whole sequence (even the future observed frames)  and a KVAE with smoothing
outperforms all competing methods. Notice that dropout probability 1 corresponds to pure generation
from the models. Figure 3b repeats this experiment  but makes it more challenging by removing an
increasing number of consecutive frames from the middle of the sequence (T = 20). In this case
the ability to encode information coming from the future into the posterior distribution is highly
beneﬁcial  and smoothing imputes frames much better than the other methods. Figure 3c graphically
illustrates ﬁgure 3b. We plot three trajectories over at-encodings. The generated trajectories were
obtained after initializing the KVAE model with 4 initial frames  while the smoothed trajectories
also incorporated encodings from the last 4 frames of the sequence. The encoded trajectories were
obtained with no missing data  and are therefore considered as ground truth. In the ﬁrst three plots
in ﬁgure 3c  we see that the backwards recursion of the Kalman smoother corrects the trajectory
obtained with generation in the forward pass. However  in the fourth plot  the poor trajectory that is
obtained during the forward generation step  makes smoothing unable to follow the ground truth.
The smoothing capabilities of KVAEs make it also possible to train it with up to 40% of missing data
with minor losses in performance (appendix C in the supplementary material). Links to videos of
the imputation results and long-term generation from the models can be found in appendix B and at
sites.google.com/view/kvae.

Understanding the dynamics parameter network.
In our experiments the dynamics parameter
network αt = αt(a0:t−1) is an LSTM network  but we could also parameterize it with any differen-
tiable function of a0:t−1 (see appendix D in the supplementary material for a comparison of various

6

(a) k = 1

(b) k = 2

(c) k = 3

(d) Reconstruction of x

Figure 4: A visualisation of the dynamics parameter network α(k)
(at−1) for K = 3  as a function of
at−1. The three α(k)
’s sum to one at every point in the encoded space. The greyscale backgrounds in
a) to c) correspond to the intensity of the weights α(k)
  with white indicating a weight of one in the
dynamics parameter network’s output. Overlaid on them is the full latent encoding a. d) shows the
reconstructed frames of the video as one image.

t

t

t

architectures). When using a multi-layer perceptron (MLP) that depends on the previous encoding as
mixture network  i.e. αt = αt(at−1)  ﬁgure 4 illustrates how the network chooses the mixture of
learned dynamics. We see that the model has correctly learned to choose a transition that maintains a
constant velocity in the center (k = 1)  reverses the horizontal velocity when in proximity of the left
and right wall (k = 2)  the reverses the vertical velocity when close to the top and bottom (k = 3).

5.2 Pendulum experiment

Model

Test ELBO

We test the KVAE on the experiment of a dynamic torque-
controlled pendulum used in [16]. Training  validation and
test set are formed by 500 sequences of 15 frames of 16x16
pixels. We use a KVAE with at ∈ R2  zt ∈ R3 and K = 2 
and try two different encoder-decoder architectures for the
VAE  one using a MLP and one using a convolutional neural
network (CNN). We compare the performaces of the KVAE
to DVBFs [16] and deep Markov models4 (DMM) [19]  non-
linear SSMs parameterized by deep neural networks whose
intractable posterior distribution is approximated with an inference network. In table 1 we see that
the KVAE outperforms both models in terms of ELBO on a test set  showing that for the task in hand
it is preferable to use a model with simpler dynamics but exact posterior inference.

Table 1: Pendulum experiment.

KVAE (CNN)
KVAE (MLP)

810.08
807.02
798.56
784.70

DVBF
DMM

5.3 Other environments

To test how well the KVAE adapts to different environments  we trained it end-to-end on videos of (i)
a ball bouncing between walls that form an irregular polygon  (ii) a ball bouncing in a box and subject
to gravity  (iii) a Pong-like environment where the paddles follow the vertical position of the ball to
make it stay in the frame at all times. Figure 5 shows that the KVAE learns the dynamics of all three
environments  and generates realistic-looking trajectories. We repeat the imputation experiments of
ﬁgures 3a and 3b for these environments in the supplementary material (appendix E)  where we see
that KVAEs outperform alternative models.

6 Related work

Recent progress in unsupervised learning of high dimensional sequences is found in a plethora of
both deterministic and probabilistic generative models. The VAE framework is a common work-
horse in the stable of probabilistic inference methods  and it is extended to the temporal setting by
[2  6  8  16  19]. In particular  deep neural networks can parameterize the transition and emission
distributions of different variants of deep state-space models [8  16  19]. In these extensions  inference

4Deep Markov models were previously referred to as deep Kalman ﬁlters.

7

(a) Irregular polygon.

(b) Box with gravity.

(c) Pong-like environment.

Figure 5: Generations from the KVAE trained on different environments. The videos are shown as
single images  with color intensity representing the incremental sequence index t. In the simulation
that resembles Atari’s Pong game  the movement of the two paddles (left and right) is also visible.

networks deﬁne a variational approximation to the intractable posterior distribution of the latent states
at each time step. For the tasks in section 5  it is preferable to use the KVAE’s simpler temporal model
with an exact (conditional) posterior distribution than a highly non-linear model where the posterior
needs to be approximated. A different combination of VAEs and probabilistic graphical models has
been explored in [15]  which deﬁnes a general class of models where inference is performed with
message passing algorithms that use deep neural networks to map the observations to conjugate
graphical model potentials.
In classical non-linear extensions of the LGSSM like the extended Kalman ﬁlter and in the locally-
linear dynamics of [16  33]  the transition matrices at time t have a non-linear dependence on zt−1.
The KVAE’s approach is different: by introducing the latent encodings at and making γt depend
on a1:t−1  the linear dependency between consecutive states of z is preserved  so that the exact
smoothed posterior can be computed given a  and used to perform missing data imputation. LGSSM
with dynamic parameterization have been used for large-scale demand forecasting in [27]. [20]
introduces recurrent switching linear dynamical systems  that combine deep learning techniques and
switching Kalman ﬁlters [22] to model low-dimensional time series. [11] introduces a discriminative
approach to estimate the low-dimensional state of a LGSSM from input images. The resulting model
is reminiscent of a KVAE with no decoding step  and is therefore not suited for unsupervised learning
and video generation. Recent work in the non-sequential setting has focused on disentangling basic
visual concepts in an image [12]. [10] models neural activity by ﬁnding a non-linear embedding of a
neural time series into a LGSSM.
Great strides have been made in the reinforcement learning community to model how environments
evolve in response to action [5  23  24  30  32]. In similar spirit to this paper  [32] extracts a latent
representation from a PCA representation of the frames where controls can be applied. [5] introduces
action-conditional dynamics parameterized with LSTMs and  as for the KVAE  a computationally
efﬁcient procedure to make long term predictions without generating high dimensional images at
each time step. As autoregressive models  [29] develops a sequence to sequence model of video
representations that uses LSTMs to deﬁne both the encoder and the decoder. [7] develops an action-
conditioned video prediction model of the motion of a robot arm using convolutional LSTMs that
models the change in pixel values between two consecutive frames.
While the focus in this work is to deﬁne a generative model for high dimensional videos of simple
physical systems  several recent works have combined physical models of the world with deep learning
to learn the dynamics of objects in more complex but low-dimensional environments [3  4  9  34].

7 Conclusion

The KVAE  a model for unsupervised learning of high-dimensional videos  was introduced in this
paper. It disentangles an object’s latent representation at from a latent state zt that describes its
dynamics  and can be learned end-to-end from raw video. Because the exact (conditional) smoothed
posterior distribution over the states of the LGSSM can be computed  one generally sees a marked

8

improvement in inference and missing data imputation over methods that don’t have this property.
A desirable property of disentangling the two latent representations is that temporal reasoning  and
possibly planning  could be done in the latent space. As a proof of concept  we have been deliberate
in focussing our exposition to videos of static worlds that contain a few moving objects  and leave
extensions of the model to real world videos or sequences coming from an agent exploring its
environment to future work.

Acknowledgements

We would like to thank Lars Kai Hansen for helpful discussions on the model design. Marco Fraccaro
is supported by Microsoft Research through its PhD Scholarship Programme. We thank NVIDIA
Corporation for the donation of TITAN X GPUs.

References
[1] M. Abadi  A. Agarwal  P. Barham  E. Brevdo  Z. Chen  C. Citro  G. S. Corrado  A. Davis  J. Dean 
M. Devin  S. Ghemawat  I. Goodfellow  A. Harp  G. Irving  M. Isard  Y. Jia  R. Jozefowicz  L. Kaiser 
M. Kudlur  J. Levenberg  D. Mané  R. Monga  S. Moore  D. Murray  C. Olah  M. Schuster  J. Shlens 
B. Steiner  I. Sutskever  K. Talwar  P. Tucker  V. Vanhoucke  V. Vasudevan  F. Viégas  O. Vinyals  P. War-
den  M. Wattenberg  M. Wicke  Y. Yu  and X. Zheng. TensorFlow: Large-scale machine learning on
heterogeneous systems  2015. Software available from tensorﬂow.org.

[2] E. Archer  I. M. Park  L. Buesing  J. Cunningham  and L. Paninski. Black box variational inference for

state space models. arXiv:1511.07367  2015.

[3] P. W. Battaglia  R. Pascanu  M. Lai  D. J. Rezende  and K. Kavukcuoglu. Interaction networks for learning

about objects  relations and physics. In NIPS  2016.

[4] M. B. Chang  T. Ullman  A. Torralba  and J. B. Tenenbaum. A compositional object-based approach to

learning physical dynamics. In ICLR  2017.

[5] S. Chiappa  S. Racanière  D. Wierstra  and S. Mohamed. Recurrent environment simulators. In ICLR 

2017.

[6] J. Chung  K. Kastner  L. Dinh  K. Goel  A. C. Courville  and Y. Bengio. A recurrent latent variable model

for sequential data. In NIPS  2015.

[7] C. Finn  I. J. Goodfellow  and S. Levine. Unsupervised learning for physical interaction through video

prediction. In NIPS  2016.

[8] M. Fraccaro  S. K. Sønderby  U. Paquet  and O. Winther. Sequential neural models with stochastic layers.

In NIPS  2016.

[9] K. Fragkiadaki  P. Agrawal  S. Levine  and J. Malik. Learning visual predictive models of physics for

playing billiards. In ICLR  2016.

[10] Y. Gao  E. W. Archer  L. Paninski  and J. P. Cunningham. Linear dynamical neural population models

through nonlinear embeddings. In NIPS  2016.

[11] T. Haarnoja  A. Ajay  S. Levine  and P. Abbeel. Backprop KF: learning discriminative deterministic state

estimators. In NIPS  2016.

[12] I. Higgins  L. Matthey  A. Pal  C. Burgess  X. Glorot  M. Botvinick  S. Mohamed  and A. Lerchner.

beta-vae: Learning basic visual concepts with a constrained variational framework. 2017.

[13] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation  9(8):1735–1780  Nov.

1997.

[14] E. Jang  S. Gu  and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint

arXiv:1611.01144  2016.

[15] M. J. Johnson  D. Duvenaud  A. B. Wiltschko  S. R. Datta  and R. P. Adams. Composing graphical models

with neural networks for structured representations and fast inference. In NIPS  2016.

[16] M. Karl  M. Soelch  J. Bayer  and P. van der Smagt. Deep variational bayes ﬁlters: Unsupervised learning

of state space models from raw data. In ICLR  2017.

9

[17] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv:1412.6980  2014.

[18] D. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR  2014.

[19] R. Krishnan  U. Shalit  and D. Sontag. Structured inference networks for nonlinear state space models. In

AAAI  2017.

[20] S. Linderman  M. Johnson  A. Miller  R. Adams  D. Blei  and L. Paninski. Bayesian Learning and Inference

in Recurrent Switching Linear Dynamical Systems. In AISTATS  2017.

[21] C. J. Maddison  A. Mnih  and Y. W. Teh. The concrete distribution: A continuous relaxation of discrete

random variables. In ICLR  2017.

[22] K. P. Murphy. Switching Kalman ﬁlters. Technical report  1998.

[23] J. Oh  X. Guo  H. Lee  R. L. Lewis  and S. Singh. Action-conditional video prediction using deep networks

in atari games. In NIPS  2015.

[24] V. Patraucean  A. Handa  and R. Cipolla. Spatio-temporal video autoencoder with differentiable memory.

arXiv:1511.06309  2015.

[25] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in

deep generative models. In ICML  2014.

[26] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Computation 

11(2):305–45  1999.

[27] M. W. Seeger  D. Salinas  and V. Flunkert. Bayesian intermittent demand forecasting for large inventories.

In NIPS  2016.

[28] W. Shi  J. Caballero  F. Huszár  J. Totz  A. P. Aitken  R. Bishop  D. Rueckert  and Z. Wang. Real-time
single image and video super-resolution using an efﬁcient sub-pixel convolutional neural network. In
CVPR  2016.

[29] N. Srivastava  E. Mansimov  and R. Salakhudinov. Unsupervised learning of video representations using

LSTMs. In ICML  2015.

[30] W. Sun  A. Venkatraman  B. Boots  and J. A. Bagnell. Learning to ﬁlter with predictive state inference

machines. In ICML  2016.

[31] L. G. Ungerleider and L. G. Haxby. “What” and “where” in the human brain. Curr. Opin. Neurobiol. 

4:157–165  1994.

[32] N. Wahlström  T. B. Schön  and M. P. Deisenroth. From pixels to torques: Policy learning with deep

dynamical models. arXiv:1502.02251  2015.

[33] M. Watter  J. Springenberg  J. Boedecker  and M. Riedmiller. Embed to control: A locally linear latent

dynamics model for control from raw images. In NIPS  2015.

[34] J. Wu  I. Yildirim  J. J. Lim  W. T. Freeman  and J. B. Tenenbaum. Galileo: Perceiving physical object

properties by integrating a physics engine with deep learning. In NIPS  2015.

10

,Yin Cheng Ng
Pawel Chilinski
Ricardo Silva
Marco Fraccaro
Simon Kamronn
Ole Winther