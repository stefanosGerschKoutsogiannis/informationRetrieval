2013,Capacity of strong attractor patterns to model behavioural and cognitive prototypes,We solve the mean field equations for a stochastic Hopfield network with temperature (noise) in the presence of strong  i.e.  multiply stored patterns  and use this solution to obtain the storage capacity of such a network. Our result provides for the first time a rigorous solution of the mean field equations for the standard Hopfield model and is in contrast to the mathematically unjustifiable replica technique that has been hitherto used for this derivation. We show that the critical temperature for stability of a strong pattern is equal to its degree or multiplicity  when sum of the cubes of degrees of all stored patterns is negligible compared to the network size. In the case of a single strong pattern in the presence of simple patterns  when the ratio of the number of all stored patterns and the network size is a positive constant  we obtain the distribution of the overlaps of the patterns with the mean field and deduce that the storage capacity for retrieving a strong pattern exceeds that for retrieving a simple pattern by a multiplicative factor equal to the square of the degree of the strong pattern. This square law property provides justification for using strong patterns to model attachment types and behavioural prototypes in psychology and psychotherapy.,Capacity of strong attractor patterns to model

behavioural and cognitive prototypes

Abbas Edalat

Department of Computing
Imperial College London
London SW72RH  UK

ae@ic.ac.uk

Abstract

We solve the mean ﬁeld equations for a stochastic Hopﬁeld network with tem-
perature (noise) in the presence of strong  i.e.  multiply stored  patterns  and use
this solution to obtain the storage capacity of such a network. Our result provides
for the ﬁrst time a rigorous solution of the mean ﬁled equations for the standard
Hopﬁeld model and is in contrast to the mathematically unjustiﬁable replica tech-
nique that has been used hitherto for this derivation. We show that the critical
temperature for stability of a strong pattern is equal to its degree or multiplicity 
when the sum of the squares of degrees of the patterns is negligible compared
to the network size. In the case of a single strong pattern  when the ratio of the
number of all stored pattens and the network size is a positive constant  we obtain
the distribution of the overlaps of the patterns with the mean ﬁeld and deduce that
the storage capacity for retrieving a strong pattern exceeds that for retrieving a
simple pattern by a multiplicative factor equal to the square of the degree of the
strong pattern. This square law property provides justiﬁcation for using strong
patterns to model attachment types and behavioural prototypes in psychology and
psychotherapy.

1

Introduction: Multiply learned patterns in Hopﬁeld networks

The Hopﬁeld network as a model of associative memory and unsupervised learning was introduced
in [23] and has been intensively studied from a wide range of viewpoints in the past thirty years.
However  properties of a strong pattern  as a pattern that has been multiply stored or learned in
these networks  have only been examined very recently  a surprising delay given that repetition of an
activity is the basis of learning by the Hebbian rule and long term potentiation. In particular  while
the storage capacity of a Hopﬁeld network with certain correlated patterns has been tackled [13  25] 
the storage capacity of a Hopﬁeld network in the presence of strong as well as random patterns has
not been hitherto addressed.
The notion of a strong pattern of a Hopﬁeld network has been proposed in [15] to model attachment
types and behavioural prototypes in developmental psychology and psychotherapy. This sugges-
tion has been motivated by reviewing the pioneering work of Bowlby [9] in attachment theory and
highlighting how a number of academic biologists  psychiatrists  psychologists  sociologists and
neuroscientists have consistently regarded Hopﬁeld-like artiﬁcial neural networks as suitable tools
to model cognitive and behavioural constructs as patterns that are deeply and repeatedly learned by
individuals [11  22  24  30  29  10].
A number of mathematical properties of strong patterns in Hopﬁeld networks  which give rise to
strong attractors  have been derived in [15]. These show in particular that strong attractors are
strongly stable; a series of experiments have also been carried out which conﬁrm the mathematical

1

results and also indicate that a strong pattern stored in the network can be retrieved even in the pres-
ence of a large number of simple patterns  far exceeding the well-known maximum load parameter
or storage capacity of the Hopﬁeld network with random patterns (αc ≈ 0.138).
In this paper  we consider strong patterns in stochastic Hopﬁeld model with temperature  which ac-
counts for various types of noise in the network. In these networks  the updating rule is probabilistic
and depend on the temperature. Since analytical solution of such a system is not possible in general 
one strives to obtain the average behaviour of the network when the input to each node  the so-called
ﬁeld at the node  is replaced with its mean. This is the basis of mean ﬁeld theory for these networks.
Due to the close connection between the Hopﬁeld network and the Ising model in ferromagnetism [1 
8]  the mean ﬁeld approach for the Hopﬁeld network and its variations has been tackled using the
replica method  starting with the pioneering work of Amit  Gutfreund and Sompolinsky [3  2  4  19 
31  1  13]. Although this method has been widely used in the theory of spin glasses in statistical
physics [26  16] its mathematical justiﬁcation has proved to be elusive as we will discuss in the next
section; see for example [20  page 264]  [14  page 27]  and [7  page 9].
In [17] and independently in [27]  an alternative technique to the replica method for solving the
mean ﬁeld equations has been proposed which is reproduced and characterised as heuristic in [20 
section 2.5] since it relies on a number of assumptions that are not later justiﬁed and uses a number
of mathematical steps that are not validated.
Here  we use the basic idea of the above heuristic to develop a veriﬁable mathematical framework
with provable results grounded on elements of probability theory  with which we assume the reader
is familiar. This technique allows us to solve the mean ﬁeld equations for the Hopﬁeld network in
the presence of strong patterns and use the results to study  ﬁrst  the stability of these patterns in the
presence of temperature (noise) and  second  the storage capacity of the network with a single strong
pattern at temperature zero.
We show that the critical temperature for the stability of a strong pattern is equal to its degree (i.e. 
its multiplicity) when the ratio of the sum of the squares of degrees of the patterns to the network
size tends to zero when the latter tends to inﬁnity. In the case that there is only one strong pattern
present with its degree small compared to the number of patterns and the latter is a ﬁxed multiple of
the number of nodes  we ﬁnd the distribution of the overlap of the mean ﬁeld and the patterns when
the strong pattern is being retrieved. We use these distributions to prove that the storage capacity
for retrieving a strong pattern exceeds that for a simple pattern by a multiplicative factor equal to
the square of the degree of the strong attractor. This result matches the ﬁnding in [15] regarding the
capacity of a network to recall strong patterns as mentioned above. Our results therefore show that
strong patterns are robust and persistent in the network memory as attachment types and behavioural
prototypes are in the human memory system.
In this paper  we will several times use Lyapunov’s theorem in probability which provides a simple
sufﬁcient condition to generalise the Central Limit theorem when we deal with independent but
not necessarily identically distributed random variables. We require a general form of this theorem
i=1 Yni  for n ∈ IN  be a triangular array of random variables such
that for each n  the random variables Yni  for 1 ≤ i ≤ kn are independent with E(Yni) = 0
and E(Y 2
ni  where E(X) stands for the expected value of the random variable X. Let
ni. We use the notation X ∼ Y when the two random variables X and Y have the
s2
same distribution (for large n if either or both of them depend on n).

as follows. Let Yn = (cid:80)kn
n = (cid:80)kn

ni) = σ2
i=1 σ2

Theorem 1.1 (Lyapunov’s theorem [6  page 368]) If for some δ > 0  we have the condition:

E(|Yn|2+δ|) → 0

as n → ∞

1

s2+δ
n

Yn

d−→ denotes convergence in distribution  and we denote
then 1
sn
by N (a  σ2) the normal distribution with mean a and variance σ2. Thus  for large n we have
Yn ∼ N (0  s2

d−→ N (0  1) as n → ∞ where
n). (cid:3)

2

2 Mean ﬁeld theory
We consider a Hopﬁeld network with N neurons i = 1  . . .   N with values Si = ±1 and follow the
notations in [20]. As in [15]  we assume patterns can be multiply stored and the degree of a pattern
is deﬁned as its multiplicity. The total number of patterns  counting their multiplicity  is denoted by
p and we assume there are n patterns ξ1  . . .   ξn with degrees d1  . . .   dn ≥ 1 respectively and that
k=1 dk ≥ 0 patterns are simple  i.e.  each has degree one. Note that by our

the remaining p −(cid:80)n

assumptions there are precisely

distinct patterns  which we assume are independent and identically distributed with equal probability
of taking value ±1 for each node. More generally  for any non-negative integer k ∈ IN  we let

dk

p0 = p + n − n(cid:88)
p0(cid:88)

k=1

pk =

dk
µ.

µ=1

(cid:80)p0

N(cid:88)

j=1

j for i (cid:54)= j
We use the generalized Hebbian rule for the synaptic couplings: wij = 1
with wii = 0 for 1 ≤ i  j ≤ N. As in the standard stochastic Hopﬁeld model [20]  we use Glauber
N
dynamics [18] for the stochastic updating rule with pseudo-temperature T > 0  which accounts for
various types of noise in the network  and assume zero bias in the local ﬁeld. Putting β = 1/T
(i.e.  with the Boltzmann constant kB = 1) and letting fβ(h) = 1/(1 + exp(−2βh))  the stochastic
updating rule at time t is given by:

µ=1 dµξµ

i ξµ

Pr(Si(t + 1) = ±1) = fβ(±hi(t))  where hi(t) =

wijSj(t) 

(1)

is the local ﬁeld at i at time t. The updating is implemented asynchronously in a random way.
The energy of the network in the conﬁguration S = (Si)N

i=1 is given by

N(cid:88)

i j=1

H(S) = − 1
2

SiSjwij.

For large N  this speciﬁes a complex system  with an underlying state space of dimension 2N   which
in general cannot be solved exactly. However  mean ﬁeld theory has proved very useful in studying
Hopﬁeld networks. The average updated value of Si(t + 1) in Equation (1) is

(cid:104)Si(t + 1)(cid:105) = 1/(1 + e−2βhi(t)) − 1/(1 + e2βhi(t)) = tanh(βhi(t)) 

(2)
where (cid:104). . .(cid:105) denotes taking average with respect to the probability distribution in the updating rule
in Equation (1). The stationary solution for the mean ﬁeld thus satisﬁes:

(cid:104)Si(cid:105) = (cid:104)tanh(βhi)(cid:105) 

The average overlap of pattern ξµ with the mean ﬁeld at the nodes of the network is given by:

N(cid:88)

i=1

mν =

1
N

i (cid:104)Si(cid:105)
ξν

(3)

(4)

The replica technique for solving the mean ﬁeld problem  used in the case p/N = α > 0 as N → ∞ 
seeks to obtain the average of the overlaps in Equation (4) by evaluating the partition function of the
system  namely 

Z = TrS exp(−βH(S)) 

where the trace TrS stands for taking sum over all possible conﬁgurations S = (Si)N
i=1. As it
is generally the case in statistical physics  once the partition function of the system is obtained 

3

all required physical quantities can in principle be computed. However  in this case  the partition
function is very difﬁcult to compute since it entails computing the average (cid:104)(cid:104)log Z(cid:105)(cid:105) of log Z  where
(cid:104)(cid:104). . .(cid:105)(cid:105) indicates averaging over the random distribution of the stored patterns ξµ. To overcome this
problem  the identity

Z k − 1

log Z = lim
k→0

k

is used to reduce the problem to ﬁnding the average (cid:104)(cid:104)Z k(cid:105)(cid:105) of Z k  which is then computed for
positive integer values of k. For such k  we have:

Z k = TrS1TrS2 . . . TrSk exp(−β(H(S1) + H(S1) + . . . + H(Sk))) 

where for each i = 1  . . .   k the super-scripted conﬁguration Si is a replica of the conﬁguration
state. In computing the trace over each replica  various parameters are obtained and the replica
symmetry condition assumes that these parameters are independent of the particular replica under
consideration. Apart from this assumption  there are two basic mathematical problems with the tech-
nique which makes it unjustiﬁable [20  page 264]. Firstly  the positive integer k above is eventually
treated as a real number near zero without any mathematical justiﬁcation. Secondly  the order of
taking limits  in particular the order of taking the two limits k → 0 and N → ∞  are several times
interchanged again without any mathematical justiﬁcation.
Here  we develop a mathematically rigorous method for solving the mean ﬁeld problem  i.e.  com-
puting the average of the overlaps in Equation (4) in the case of p/N = α > 0 as N → ∞. Our
method turns the basic idea of the heuristic presented in [17] and reproduced in [20] for solving
the mean ﬁeld equation into a mathematically veriﬁable formalism  which for the standard Hopﬁeld
network with random stored patterns gives the same result as the replica method  assuming replica
symmetry. In the presence of strong patterns we obtain a set of new results as explained in the next
two sections.
The mean ﬁeld equation is obtained from Equation (3) by approximating the right hand side of
j=1 wij(cid:104)Sj(cid:105)  ignoring the sum

this equation by the value of tanh at the mean ﬁeld (cid:104)hi(cid:105) = (cid:80)N
(cid:80)N
j=1 wij(Sj − (cid:104)Sj(cid:105)) for large N [17  page 32]:
(cid:16) β

(cid:80)p0

(cid:80)N

N

j=1

ξµ (1 ≤ µ ≤ n) and p −(cid:80)n
ﬁrst case we assume p2 :=(cid:80)p0

Equation (5) gives the mean ﬁeld equation for the Hopﬁeld network with n possible strong patterns
µ=1 dµ simple patterns ξµ with n + 1 ≤ µ ≤ p0. As in the standard
Hopﬁeld model  where all patterns are simple  we have two cases to deal with. However  we now
have to account for the presence of strong attractors and our two cases will be as follows: (i) In the
µ = o(N )  which includes the simpler case p2 (cid:28) N when p2
is ﬁxed and independent of N. (ii) In the second case we assume we have a single strong attractor
with the load parameter p/N = α > 0.

µ=1 d2

(cid:104)Si(cid:105) = tanh(β(cid:104)hi(cid:105)) = tanh

j (cid:104)Sj(cid:105)(cid:17)

.

(5)

µ=1 dµξµ

i ξµ

3 Stability of strong patterns with noise: p2 = o(N )
The case of constant p and N → ∞ is usually referred to as α = 0 in the standard Hopﬁeld
model. Here  we need to consider the sum of degrees of all stored patterns (and not just the number
of patterns) compared to N. We solve the mean ﬁeld equation with T > 0 by using a method
similar in spirit to [20  page 33] for the standard Hopﬁeld model  but in our case strong patterns
induce a sequence of independent but non-identically distributed random variables in the crosstalk
term  where the Central Limit Theorem cannot be used; we show however that Lyapunov’s theorem
(Theorem (1.1) can be invoked. In retrieving pattern ξ1  we look for a solution of the mean ﬁled
equation of the form: (cid:104)Si(cid:105) = mξ1
i   where m > 0 is a constant. Using Equation (5) and separating
the contribution of ξ1 in the argument of tanh  we obtain:

 mβ

N

d1ξ1

i +

(cid:88)

j(cid:54)=i µ>1

mξ1

i = tanh

4

 .

dµξµ

i ξµ

j ξ1
j

(6)

For each N  µ > 1 and j (cid:54)= i  let

YN µj =

dµ
N

This gives (p0 − 1)(N − 1) independent random variables with E(YN µj) = 0  E(Y 2
and E(|Y 3

µ/N 3. We have:

N µj|) = d3

N µj) = d2

µ/N 2 

i ξµ
ξµ

j ξ1
j .

(7)

E(Y 2

N µj) =

s2
N :=

µ>1 j(cid:54)=i

N − 1
N 2

(cid:88)
(cid:88)

Thus  as N → ∞  we have:

(cid:88)
(cid:80)
N ((cid:80)
as N → ∞ since for positive numbers dµ we always have(cid:80)
(cid:32)
(cid:88)

N µj|) ∼

(cid:88)

E(|Y 3

µ>1 j(cid:54)=i

1
s3
N

√

µ>1

dµξµ

i ξµ

j ξ1

j ∼ N

1
N

µ>1 j(cid:54)=i

µ>1

(cid:88)

µ>1

d2
µ.

µ ∼ 1
d2
N

µ>1 d3
µ
µ>1 d2
µ)3/2

→ 0.

µ < ((cid:80)
(cid:33)

(8)

(9)

Lyapunov condition is satisﬁed for δ = 1. By Lyapunov’s theorem we deduce:

µ>1 d3

µ>1 d2

µ)3/2. Thus the

0 

d2
µ/N

(10)

Since we also have p2 = o(N )  it follows that we can ignore the second term  i.e.  the crosstalk
term  in the argument of tanh in Equation (6) as N → ∞; we thus obtain:

(11)
To examine the ﬁxed points of the Equation (11)  we let d = d1 for convenience and put x = βdm =
dm/T   so that tanh x = T x/d; see Figure 1. It follows that Tc = d is the critical temperature. If
T < d then there is a non-zero (non-trivial) solution for m  whereas for T > d we only have the
trivial solution. For d = 1 our solution is that of the standard Hopﬁeld network as in [20  page 34].

m = tanh βd1m.

Figure 1: Stability of strong attractors with noise

Theorem 3.1 The critical temperature for stability of a strong attractor is equal to its degree. (cid:3)

4 Mean ﬁeld equations for p/N = α > 0.

The case p/N = α  as for the standard Hopﬁeld model  is much harder and we here assume we
have only a single pattern ξ1 with d1 ≥ 1 and the rest of the patterns ξµ are simple with dµ = 1 for
2 ≤ µ ≤ p0. The case when there are more than one strong patterns is harder and will be dealt with
in a future paper. Moreover  we assume d1 (cid:28) p0 which is the interesting case in applications. If
d1 > 1 then we have a single strong pattern whereas if d1 = 1 the network is reduced to the standard
Hopﬁeld network. We recall that all patterns ξµ for 1 ≤ µ ≤ p0 are independent and random. Since

5

x                          y =   tanh x                y > xy = x  ( d  = T)   y < x ( T < d  )       ( d   <  T )p and N are assumed to be large and d1 (cid:28) p0  we will replace p0 with p and approximate terms like
p − 2 with p.
We again consider the mean ﬁeld equation (5) for retrieving pattern ξ1 but now the cross talk term
in (6) is large and can no longer be ignored. We therefore look at the overlaps  Equation (4)  of the
mean ﬁeld with all the stored patterns ξν and not just ξ1.
Combining Equation (5) and (4)  we eliminate the mean ﬁeld to obtain a recursive equation for the
overlaps as the new variables:

N(cid:88)

mν =

1
N

(cid:32)

p(cid:88)

(cid:33)

ξν
i tanh

β

dµξµ

i mµ

(12)

i=1

µ=1

We now have a family of p stochastic equations for the random variables mν with 1 ≤ ν ≤ p in
order to retrieve the random pattern ξ1. Formally  we assume we have a probability space (Ω F  P )
with the real-valued random variables mν : Ω → IR  which are measurable with respect to F and
the Borel sigma ﬁeld B over the real line and which take value mν(ω) ∈ IR for each sample point
ω ∈ Ω. The probability of an event A ∈ B is given by Pr{ω : mν(ω) ∈ A}. As usual Ω can itself
be taken to the real line with its Borel sigma ﬁeld and we will usually drop all references to Ω. We
a.s.−→ X for the almost sure convergence
need two lemmas to prove our main result. We write XN
d−→ X indicates convergence in
of the sequence of random variables XN to X  whereas XN
distribution [6]. Recall that almost sure convergence implies convergence in distribution. To help
us compute the right hand side of Equation (12)  we need the following lemma  which extends the
standard result for the Law of Large Numbers and its rate of convergence [5  pages 112 and 113].

Lemma 4.1 Let X be a random variable on IR such that its probability distribution F (x) =
Pr(X ≤ x) is differentiable with density F (cid:48)(x) = f (x). If g : IR → IR is a bounded measur-
able function and Xk (k ≥ 1) is a sequence of of independent and identically distributed random
variables with distribution X  then

(cid:90) ∞
(cid:33)

∞

g(Xi)

a.s.−→ Eg(X) =

g(x)f (x)dx 

(g(Xi) − kE(g)(X))

≥ 

(cid:33)

= o(1/N t−1) (cid:3)

(13)

(14)

and for all  > 0 and t > 1  we have:

1
N

(cid:32)

N(cid:88)

i=1

k(cid:88)

i=1

1
k

(cid:32)

Pr

sup
k≥N

The proof of the above lemma is given on-line in the supplementary material.
Assume p/N = α > 0 with d1 (cid:28) p0 and dµ = 1 for 1 < µ ≤ p0. In the following theorem  we use
the basic idea of the heuristic in [17] which is reproduced in [20  section 2.5] to develop a veriﬁable
mathematical method with provable results to solve the mean ﬁeld equation in the more general case
that we have a single strong pattern present in the network.

Theorem 4.2 There is a solution to the mean ﬁeld equations (12) for retrieving ξ1 with independent
random variables mν (for 1 ≤ ν ≤ p0)  where m1 ∼ N (m  s/N ) and mν ∼ N (0  r/N ) (for
ν (cid:54)= 1)  if the real numbers m  s and r satisfy the four simultaneous equations:



(i) m = (cid:82) ∞
q = (cid:82) ∞

−∞ dz√
s = q − m2
−∞ dz√
(1−β(1−q))2

r =

(iii)
(iv)

(ii)

2π

q

2π

e− z2

2 tanh(β(d1m +

√

αrz))

e− z2

2 tanh2(β(d1m +

√

αrz))

(15)

In the proof of this theorem  as given below  we seek a solution of the mean ﬁeld equations assuming
we have independent random variables mν (for 1 ≤ ν ≤ p0) such that for large N and p with

6

p/N = α  we have m1 ∼ N (m  s/N ) and mν ∼ N (0  r/N ) (ν (cid:54)= 1)  and then ﬁnd conditions in
terms of m  s and r to ensure that such a solution exists. These assumptions are in effect equivalent
to the replica symmetry approximation [17  page 262]  since they lead  as shown below  to the same
solution derived from the replica method when all stored patterns are simple. In analogy with the
replica technique  we call our solution symmetric. Since by our assumption about the distribution of
√
√
the overlaps mµ  the standard deviation of each overlap is O(1/
N )  we ignore terms of O(1/N )
and more generally terms of o(1/
N ) in the proof including in
N ) compared to terms of O(1/
the lemma below  which enables us to compute the argument of tanh in Equation (12) for large N.
Lemma 4.3 If mν ∼ N (0  r/N ) (for ν (cid:54)= 1)  then we have the equivalence of distributions:

√

i mµ ∼ N (0  αr) ∼(cid:88)

i ξµ
ξ1

(cid:88)

µ(cid:54)=1 ν

i mµ. (cid:3)
i ξµ
ξ1

µ(cid:54)=1

The proofs of the above lemma and Theorem (4.2) are given on-line in the supplementary material.
We note that in the heuristic described in [20] the distributions of m1 and mν (ν (cid:54)= 1) are not
eventually determined yet an initial assumption about the variance of mν is made. Moreover  the
heuristic has no assumption on how mν is distributed  and no valid justiﬁcation is provided for
computing the double summation to obtain mν  which is similar to the lack of justiﬁcation for the
interchange of limits in the replica technique mentioned in Section 2.
Comparing the equations for m  q and r in Equations (15) with those obtained by the replica
method [20  pages 263-4] or the heuristic in [20  page 37]  we see that m has been replaced by
d1m on the right hand side of the equations for m and q. It follows that for d1 = 1  we obtain the
solution for random patterns in the standard Hopﬁeld network produced by the replica method.
We can solve the simultaneous equations in (15) for m  q and r (and then for s) numerically. As
in [20  page 38]  we examine when these equations have non-trivial solutions (i.e.  m (cid:54)= 0) when
T → 0 corresponding to β → ∞  where we also have q → 1 but C := β(1 − q) remains ﬁnite:

Using the relations:(cid:40) (cid:82) ∞
(cid:82) ∞
C := β(1 − q) = (cid:112)2/παr exp(−(dm)2/2αr)

e−z2/2(1 − tanh2 β(az + b)) ≈ 2
aβ e−b2/2a2
√
β→∞−→ erf(b/
e−z2/2 tanh β(az + b)
2a) 

where erf is the error function  the three equations for m  q and r become:

−∞ dz√
−∞ dz√

(cid:40)

2π

2π

1

π

r = 1/(1 − C)2 

m = erf(dm/

√

2αr) 

where we have put d := d1. Let y = dm/
y
d

fα d(y) :=

√
2αr; then we obtain:
√
(

e−y2

2α +

2√
π

) = erf(y)

(16)

(17)

(18)

Figure 2  gives a schematic view of the solution of Equation (18). The dotted curve is the erf function
on the right hand side of the equation  whereas the three solid curves correspond to the graphs of the
function fα d on the left hand side of the equation for a given value of d and three different values
of α. The heights of these graphs increase with α.
The critical load parameter αc(d) is the threshold such that for α < αc(d) the strong pattern with
degree d can be retrieved whereas for αc(d) < α this memory is lost. Geometrically  αc(d) corre-
sponds to the curve that is tangent  say at yd  to the error function  i.e. 

f(cid:48)
αc(d) d(yd) = erf (cid:48)(yd).

For α < αc(d)  the function fα d has two non-trivial intersections (away from the origin) with erf
while for αc(d) < α there are no non-trivial intersections.
We can compare the storage capacity of strong patterns with that of simple patterns  assuming the
independence of mν (equivalently replica symmetry)  by ﬁnding a lower bound for αc(d) in terms

7

Figure 2: Capacity of strong attractors

of αc(1) as follows. We have:

fα d(y) = y((cid:112)2(α/d2) +

) ≤ y((cid:112)2(α/d2) +

√
2
d

π

e−y2

e−y2

)

2√
π

(19)

where equality holds iff d = 1. Putting α = d2αc(1) and y = y1  we have for d > 1:

fd2αc(1) d(y1) < fαc(1) 1(y1) = erf(y1) 

(20)
Therefore  for a strong pattern  the graphs of fd2αc(1) d and erf intersect in two non-trivial points and
thus αc(d) > d2αc(1). Since αc(1) = αc ≈ 0.138  this yields: αc(d)/0.138 > d2  i.e.  the relative
increase in the storage capacity exceeds the square of the degree of the strong pattern.
In the case of the standard Hopﬁeld network with simple patterns only  we have αc(1) = αc ≈
0.138  but simulation experiments show that for values in the narrow range 0.138 < α < 0.144
there are replica symmetry breaking solutions for which a stored pattern can still be retrieved [12].
We show that the square property holds when we take into account symmetry breaking solutions.
By [15  Theorem 1]  it follows that the error probability of retrieving a single strong attractor is:

Prer ≈ 1
2

(1 − erf(d/
√

√

2α) 

for α = p/N. Thus  this error will be constant if d/
value of the load parameter is proportional to the square of the degree of the strong attractor.

α remains ﬁxed  indicating that the critical

Corollary 4.4 The storage capacity for retrieving a single strong pattern exceeds that of a simple
pattern by the square of the degree of the strong pattern. (cid:3)

This square property shows that a multiply learned pattern is retained in the memory in the presence
of a large number of other random patterns  proportional to the square of its multiplicity.

5 Conclusion

We have developed a mathematically justiﬁable method to derive the storage capacity of the Hopﬁeld
network when the load parameter α = p/N remains a positive constant as the network size N → ∞.
For the standard model  our result conﬁrms that of the replica technique  i.e.  αc ≈ 0.138. However 
our method also computes the storage capacity when retrieving a single strong pattern of degree d
in the presence of other random patterns and we have shown that this capacity exceeds that of a
simple pattern by a multiplicative factor d2  providing further justiﬁcation for using strong patterns
of Hopﬁeld networks to model attachment types and behavioural prototypes in psychology.
The storage capacity of Hopﬁeld networks when there are more than a single strong pattern and in
networks with low neural activation will be addressed in future work. It is also of interest to examine
the behaviour of strong patterns in Boltzmann Machines [20]  Restricted Boltzmann Machines [28]
and Deep Learning Networks [21].

8

.yyd0fα ..derf(y) αcf dα  (  ) dα  dfReferences
[1] D. J. Amit. Modeling Brain Function: The World of Attractor Neural Networks. Cambridge  1989.
[2] D. J. Amit  H. Gutfreund  and H. Sompolinsky. Spin-glass models of neural networks. Phys. Rev. A 

32:1007–1018  1985.

[3] D. J. Amit  H. Gutfreund  and H. Sompolinsky. Storing inﬁnite numbers of patterns in a spin-glass model

of neural networks. Phys. Rev. Lett.  55:1530–1533  Sep 1985.

[4] D. J. Amit  H. Gutfreund  and H. Sompolinsky. Information storage in neural networks with low levels of

activity. Phys. Rev. A  35:2293–2303  Mar 1987.

[5] L. E. Baum and M. Katz. Convergence rates in the law of large numbers. Transactions of the American

Mathematical Society  120(1):108–123  1965.

[6] P. Billingsley. Probability and Measure. John Wiley & Sons  second edition  1986.
[7] E. Bolthausen. Random media and spin glasses: An introduction into some mathematical results and
In E. Bolthausen and A. Bovier  editors  Spin Glasses  volume 1900 of Lecture Notes in

problems.
Mathematics. Springer  2007.

[8] A. Bovier and V. Gayrard. Hopﬁeld models as generalized random mean ﬁeld models. In A. Bovier and
P. Picco  editors  Mathematical Aspects of Spin Glasses and Neural Networks  pages 3–89. Birkhuser 
1998.

[9] John Bowlby. Attachment: Volume One of the Attachment and Loss Trilogy. Pimlico  second revised

edition  1997.

[10] L. Cozolino. The Neuroscience of Human Relationships. W. W. Norton  2006.
[11] F. Crick and G. Mitchison. The function of dream sleep. Nature  304:111–114  1983.
[12] A. Crisanti  D. J. Amit  and H. Gutfreund. Saturation level of the hopﬁeld model for neural network.

Europhys. Lett.  2(337)  1986.

[13] L. F. Cugliandolo and M. V. Tsodyks. Capacity of networks with correlated attractors. Journal of Physics

A: Mathematical and General  27(3):741  1994.

[14] V. Dotsenko. An Introduction to the theory of spin glasses and neural networks. World Scientiﬁc  1994.
[15] A. Edalat and F. Mancinelli. Strong attractors of Hopﬁeld neural networks to model attachment types and

behavioural patterns. In IJCNN 2013 Conference Proceedings. IEEE  August 2013.

[16] K. H. Fischer and J. A. Hertz. Spin Glasses (Cambridge Studies in Magnetism). Cambridge  1993.
[17] T. Geszti. Physical Models of Neural Networks. World Scientiﬁc  1990.
[18] R. J. Glauber. Time–dependent statistics of the Ising model. J. Math. Phys.  294(4)  1963.
[19] H. Gutfreund. Neural networks with hierarchically correlated patterns. Phys. Rev. A  37:570–577  1988.
[20] J. A. Hertz  A. S. Krogh  and R. G. Palmer. Introduction To The Theory Of Neural Computation. Westview

Press  1991.

[21] G. E. Hinton  S. Osindero  and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural compu-

tation  18(7):1527–1554  2006.

[22] R. E. Hoffman. Computer simulations of neural information processing and the schizophrenia-mania

dichotomy. Arch Gen Psychiatry.  44(2):178–88  1987.

[23] J. J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities.

Proceedings of the National Academy of Science  USA  79:2554–2558  1982.

[24] T. Lewis  F. Amini  and R. Richard. A General Theory of Love. Vintage  2000.
[25] Matthias Lowe. On The Storage Capacity of Hopﬁeld Models with Correlated Patterns. Annals of App-

plied Probability  8(4):1216–1250  1998.

[26] M. Mezard  G. Parisi  and M. Virasoro  editors. Spin Glass Theory and Beyond. World Scientiﬁc  1986.
[27] P. Peretto. On learning rules and memory storage abilities of asymmetrical neural networks. J. Phys.

France  49:711–726  1998.

[28] A. Salakhutdinov  R.and Mnih and G. Hinton. Restricted Boltzmann machines for collaborative ﬁltering.

In Proceedings of the 24th international conference on Machine learning  pages 791–798  2007.

[29] A. N. Schore. Affect Dysregulation and Disorders of the Self. W. W. Norton  2003.
[30] T. S. Smith  G. T. Stevens  and S. Caldwell. The familiar and the strange: Hopﬁeld network models
for prototype-entrained. In T. S. Franks  D. D. and Smith  editor  Mind  brain  and society: Toward a
neurosociology of emotion  volume 5 of Social perspectives on emotion. Elsevier/JAI  1999.

[31] M. Tsodyks and M. Feigelman. Enhanced storage capacity in neural networks with low level of activity.

Europhysics Letters   6:101–105  1988.

9

,Abbas Edalat
Takahiro Omi
naonori ueda
Kazuyuki Aihara