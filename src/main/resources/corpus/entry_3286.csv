2013,Low-Rank Matrix and Tensor Completion via Adaptive Sampling,We study low rank matrix and tensor completion and propose novel algorithms that employ adaptive sampling schemes to obtain strong performance guarantees for these problems. Our algorithms exploit adaptivity to identify entries that are highly informative for identifying the column space of the matrix (tensor) and consequently  our results hold even when the row space is highly coherent  in contrast with previous analysis of matrix completion. In the absence of noise  we show that one can exactly recover a $n \times n$ matrix of rank $r$ using $O(r^2 n \log(r))$ observations  which is better than the best known bound under random sampling. We also show that one can recover an order $T$ tensor using $O(r^{2(T-1)}T^2 n \log(r))$. For noisy recovery  we show that one can consistently estimate a low rank matrix corrupted with noise using $O(nr \textrm{polylog}(n))$ observations. We complement our study with simulations that verify our theoretical guarantees and demonstrate the scalability of our algorithms.,Low-Rank Matrix and Tensor Completion via

Adaptive Sampling

Akshay Krishnamurthy

Computer Science Department
Carnegie Mellon University

Pittsburgh  PA 15213

akshaykr@cs.cmu.edu

Aarti Singh

Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA 15213

aartisingh@cs.cmu.edu

Abstract

We study low rank matrix and tensor completion and propose novel algorithms
that employ adaptive sampling schemes to obtain strong performance guarantees.
Our algorithms exploit adaptivity to identify entries that are highly informative
for learning the column space of the matrix (tensor) and consequently  our results
hold even when the row space is highly coherent  in contrast with previous analy-
ses. In the absence of noise  we show that one can exactly recover a n ⇥ n matrix
of rank r from merely ⌦(nr3/2 log(r)) matrix entries. We also show that one can
recover an order T tensor using ⌦(nrT1/2T 2 log(r)) entries. For noisy recov-
ery  our algorithm consistently estimates a low rank matrix corrupted with noise
using ⌦(nr3/2polylog(n)) entries. We complement our study with simulations
that verify our theory and demonstrate the scalability of our algorithms.

1

Introduction

Recently  the machine learning and signal processing communities have focused considerable atten-
tion toward understanding the beneﬁts of adaptive sensing. This theme is particularly relevant to
modern data analysis  where adaptive sensing has emerged as an efﬁcient alternative to obtaining
and processing the large data sets associated with scientiﬁc investigation. These empirical observa-
tions have lead to a number of theoretical studies characterizing the performance gains offered by
adaptive sensing over conventional  passive approaches. In this work  we continue in that direction
and study the role of adaptive data acquisition in low rank matrix and tensor completion problems.
Our study is motivated not only by prior theoretical results in favor of adaptive sensing but also
by several applications where adaptive sensing is feasible. In recommender systems  obtaining a
measurement amounts to asking a user about an item  an interaction that has been deployed in
production systems. Another application pertains to network tomography  where a network operator
is interested in inferring latencies between hosts in a communication network while injecting few
packets into the network. The operator  being in control of the network  can adaptively sample the
matrix of pair-wise latencies  potentially reducing the total number of measurements. In particular 
the operator can obtain full columns of the matrix by measuring from one host to all others  a
sampling strategy we will exploit in this paper.
Yet another example centers around gene expression analysis  where the object of interest is a matrix
of expression levels for various genes across a number of conditions. There are typically two types
of measurements:
low-throughput assays provide highly reliable measurements of single entries
in this matrix while high-throughput microarrays provide expression levels of all genes of interest
across operating conditions  thus revealing entire columns. The completion problem can be seen
as a strategy for learning the expression matrix from both low- and high-throughput data while
minimizing the total measurement cost.

1

1.1 Contributions

We develop algorithms with theoretical guarantees for three low-rank completion problems. The
algorithms ﬁnd a small subset of columns of the matrix (tensor) that can be used to reconstruct or
approximate the matrix (tensor). We exploit adaptivity to focus on highly informative columns  and
this enables us to do away with the usual incoherence assumptions on the row-space while achieving
competitive (or in some cases better) sample complexity bounds. Speciﬁcally our results are:

1. In the absence of noise  we develop a streaming algorithm that enjoys both low sample
requirements and computational overhead. In the matrix case  we show that ⌦(nr3/2 log r)
adaptively chosen samples are sufﬁcient for exact recovery  improving on the best known
bound of ⌦(nr2 log2 n) in the passive setting [21]. This also gives the ﬁrst guarantee for
matrix completion with coherent row space.

2. In the tensor case  we establish that ⌦(nrT1/2T 2 log r) adaptively chosen samples are
sufﬁcient for recovering a n ⇥ . . . ⇥ n order T tensor of rank r. We complement this
with a necessary condition for tensor completion under random sampling  showing that
our adaptive strategy is competitive with any passive algorithm. These are the ﬁrst sample
complexity upper and lower bounds for exact tensor completion.

3. In the noisy matrix completion setting  we modify the adaptive column subset selection
algorithm of Deshpande et al. [10] to give an algorithm that ﬁnds a rank-r approximation
to a matrix using ⌦(nr3/2polylog(n)) samples. As before  the algorithm does not require
an incoherent row space but we are no longer able to process the matrix sequentially.

4. Along the way  we improve on existing results for subspace detection from missing data 

the problem of testing if a partially observed vector lies in a known subspace.

2 Related Work

The matrix completion problem has received considerable attention in recent years. A series of
papers [6  7  13  21]  culminating in Recht’s elegent analysis of the nuclear norm minimization pro-
gram  address the exact matrix completion problem through the framework of convex optimization 
1} log2(n2)) randomly drawn samples are sufﬁcient to
establishing that ⌦((n1 + n2)r max{µ0  µ2
exactly identify an n1 ⇥ n2 matrix with rank r. Here µ0 and µ1 are parameters characterizing the
incoherence of the row and column spaces of the matrix  which we will deﬁne shortly. Candes and
Tao [7] proved that under random sampling ⌦(n1rµ0 log(n2)) samples are necessary  showing that
nuclear norm minimization is near-optimal.
The noisy matrix completion problem has also received considerable attention [5  17  20]. The
majority of these results also involve some parameter that quantiﬁes how much information a single
observation reveals  in the same vein as incoherence.
Tensor completion  a natural generalization of matrix completion  is less studied. One challenge
stems from the NP-hardness of computing most tensor decompositions  pushing researchers to study
alternative structure-inducing norms in lieu of the nuclear norm [11  22]. Both papers derive algo-
rithms for tensor completion  but neither provide sample complexity bounds for the noiseless case.
Our approach involves adaptive data acquisition  and consequently our work is closely related to
a number of papers focusing on using adaptive measurements to estimate a sparse vector [9  15].
In these problems  speciﬁcally  problems where the sparsity basis is known a priori  we have a
reasonable understanding of how adaptive sampling can lead to performance improvements. As a
low rank matrix is sparse in its unknown eigenbasis  the completion problem is coupled with learning
this basis  which poses a new challenge for adaptive sampling procedures.
Another relevant line of work stems from the matrix approximations literature. Broadly speaking 
this research is concerned with efﬁciently computing a structured matrix  i.e. sparse or low rank 
that serves as a good approximation to a fully observed input matrix. Two methods that apply to
the missing data setting are the Nystrom method [12  18] and entrywise subsampling [1]. While
the sample complexity bounds match ours  the analysis for the Nystrom method has focused on
positive-semideﬁnite kernel matrices and requires incoherence of both the row and column spaces.
On the other hand  entrywise subsampling is applicable  but the guarantees are weaker than ours.

2

It is also worth brieﬂy mentioning the vast body of literature on column subset selection  the task
of approximating a matrix by projecting it onto a few of its columns. While the best algorithms 
namely volume sampling [14] and sampling according to statistical leverages [3]  do not seem to be
readily applicable to the missing data setting  some algorithms are. Indeed our procedure for noisy
matrix completion is an adaptation of an existing column subset selection procedure [10].
Our techniques are also closely related to ideas employed for subspace detection – testing whether a
vector lies in a known subspace – and subspace tracking – learning a time-evolving low-dimensional
subspace from vectors lying close to that subspace. Balzano et al. [2] prove guarantees for subspace
detection with known subspace and a partially observed vector  and we will improve on their result
en route to establishing our guarantees. Subspace tracking from partial information has also been
studied [16]  but little is known theoretically about this problem.

rXk=1

3 Deﬁnitions and Preliminaries
Before presenting our algorithms  we clarify some notation and deﬁnitions. Let M 2 Rn1⇥n2 be a
rank r matrix with singular value decomposition U ⌃V T . Let c1  . . . cn2 denote the columns of M.
Let M 2 Rn1⇥...⇥nT denote an order T tensor with canonical decomposition:

M =

a(1)
k ⌦ a(2)

k ⌦ . . . ⌦ a(T )

k

(1)

i

i

k=1 need not be orthogonal  nor even linearly independent.

where ⌦ is the outer product. Deﬁne rank(M) to be the smallest value of r that establishes this
equality. Note that the vectors {a(t)
k }r
The mode-t subtensors of M  denoted M(t)
  are order T  1 tensors obtained by ﬁxing the ith
coordinate of the t-th mode. For example  if M is an order 3 tensor  then M(3)
are the frontal slices.
We represent a d-dimensional subspace U ⇢ Rn as a set of orthonormal basis vectors U = {ui}d
and in some cases as n ⇥ d matrix whose columns are the basis vectors. The interpretation will be
clear from context. Deﬁne the orthogonal projection onto U as PU v = U (U T U )1U T v.
For a set ⌦ ⇢ [n]1  c⌦ 2 R|⌦| is the vector whose elements are ci  i 2 ⌦ indexed lexicographically.
Similarly the matrix U⌦ 2 R|⌦|⇥d has rows indexed by ⌦ lexicographically. Note that if U is a
orthobasis for a subspace  U⌦ is a |⌦|⇥ d matrix with columns ui⌦ where ui 2 U  rather than a set
of orthonormal basis vectors. In particular  the matrix U⌦ need not have orthonormal columns.
These deﬁnitions extend to the tensor setting with slight modiﬁcations. We use the vec operation
to unfold a tensor into a single vector and deﬁne the inner product hx  yi = vec(x)T vec(y). For a
subspace U ⇢ R⌦ni  we write it as a (Q ni) ⇥ d matrix whose columns are vec(ui)  ui 2 U. We
can then deﬁne projections and subsampling as we did in the vector case.
As in recent work on matrix completion [7  21]  we will require a certain amount of incoherence
between the column space associated with M (M) and the standard basis.
Deﬁnition 1. The coherence of an r-dimensional subspace U ⇢ Rn is:

i=1

µ(U )   n
r

max

1jn||PU ej||2

(2)

where ej denotes the jth standard basis element.
In previous analyses of matrix completion  the incoherence assumption is that both the row and col-
umn spaces of the matrix have coherences upper bounded by µ0. When both spaces are incoherent 
each entry of the matrix reveals roughly the same amount of information  so there is little to be
gained from adaptive sampling  which typically involves looking for highly informative measure-
ments. Thus the power of adaptivity for these problems should center around relaxing the incoher-
ence assumption  which is the direction we take in this paper. Unfortunately  even under adaptive
sampling  it is impossible to identify a rank one matrix that is zero in all but one entry without ob-
serving the entire matrix  implying that we cannot completely eliminate the assumption. Instead  we
will retain incoherence on the column space  but remove the restrictions on the row space.

1We write [n] for {1  . . .   n}

3

Algorithm 1: Sequential Tensor Completion (M {mt}T

t=1)

t=1 [nt] uniformly with replacement w. p. mT /QT1
of M  i 2 [nT ]:
  {mt}T1
t=1 )

i

t=1 nt.

1. Let U = ;.
2. Randomly draw entries ⌦ ⇢QT1
3. For each mode-T subtensor M(T )
i⌦ P U⌦M(t)
(a) If ||M(T )
2 > 0:
i⌦||2
i recurse on (M(T )
i. ˆM(T )
ii. Ui PU?
||PU?
(b) Otherwise ˆM(T )

ˆM(T )
ˆM(T )

i U (U T

||

i

i

i

. U U[ Ui.

⌦U⌦)1U⌦M(T )

i⌦

4. Return ˆM with mode-T subtensors ˆMi

(T ).

4 Exact Completion Problems

In the matrix case  our sequential algorithm builds up the column space of the matrix by selecting a
few columns to observe in their entirety. In particular  we maintain a candidate column space ˜U and
test whether a column ci lives in ˜U or not  choosing to completely observe ci and add it to ˜U if it
does not. Balzano et al. [2] observed that we can perform this test with a subsampled version of ci 
meaning that we can recover the column space using few samples. Once we know the column space 
recovering the matrix  even from few observations  amounts to solving determined linear systems.
For tensors  the algorithm becomes recursive in nature. At the outer level of the recursion  the
algorithm maintains a candidate subspace U for the mode T subtensors M(T )
. For each of these
subtensors  we test whether M(T )
lives in U and recursively complete that subtensor if it does not.
Once we complete the subtensor  we add it to U and proceed at the outer level. When the subtensor
itself is just a column; we observe the columns in its entirety.
The pseudocode of the algorithm is given in Algorithm 1. Our ﬁrst main result characterizes the
performance of the tensor completion algorithm. We defer the proof to the appendix.

Theorem 2. Let M = Pr
span({a(t)
j }r
mt = 36rt1/2µt1
recovers M and has expected sample complexity

be a rank r order-T tensor with subspaces A(t) =
j=1). Suppose that all of A(1)  . . . A(T1) have coherence bounded above by µ0. Set
log(2r/) for each t. Then with probability  1 5T rT   Algorithm 1 exactly

i=1 ⌦T

t=1a(t)

0

j

i

i

36(

TXt=1

nt)rT1/2µT1

0

log(2r/)

(3)

0

In the special case of a n ⇥ . . . ⇥ n tensor of order T   the algorithm succeeds with high probability
using ⌦(nrT1/2µT1
T 2 log(T r/)) samples  exhibiting a linear dependence on the tensor dimen-
t=2 nt⌘ r⌘ samples are

sions. In comparison  the only guarantee we are aware of shows that ⌦⇣⇣QT1
dimension [23]. In the noiseless scenario  one can unfold the tensor into a n1 ⇥QT
structure  this approach will scale withQT

sufﬁcient for consistent estimation of a noisy tensor  exhibiting a much worse dependence on tensor
t=2 nt matrix
and apply any matrix completion algorithm. Unfortunately  without exploiting the additional tensor
t=2 nt  which is similarly much worse than our guarantee.
Note that the na¨ıve procedure that does not perform the recursive step has sample complexity scaling
with the product of the dimensions and is therefore much worse than the our algorithm.
The most obvious specialization of Theorem 2 is to the matrix completion problem:
Corollary 3. Let M := U ⌃V T 2 Rn1⇥n2 have rank r  and ﬁx > 0. Assume µ(U )  µ0. Setting
m   m2  36r3/2µ0 log( 2r
 )  the sequential algorithm exactly recovers M with probability at least
1  4r +  while using in expectation

36n2r3/2µ0 log(2r/) + rn1

(4)

4

observations. The algorithm runs in O(n1n2r + r3m) time.

A few comments are in order. Recht [21] guaranteed exact recovery for the nuclear norm minimiza-
1} log2(2n2)
tion procedure as long as the number of observations exceeds 32(n1+n2)r max{µ0  µ2
where  controls the probability of failure and ||U V T||1  µ1pr/(n1n2) with µ1 as another co-
herence parameter. Without additional assumptions  µ1 can be as large as µ0pr. In this case  our
bound improves on his in its the dependence on r  µ0 and logarithmic terms.
The Nystrom method can also be applied to the matrix completion problem  albeit under non-
uniform sampling. Given a PSD matrix  one uses a randomly sampled set of columns and the corre-
sponding rows to approximate the remaining entries. Gittens showed that if one samples O(r log r)
columns  then one can exactly reconstruct a rank r matrix [12]. This result requires incoherence of
both row and column spaces  so it is more restrictive than ours. Almost all previous results for exact
matrix completion require incoherence of both row and column spaces.
The one exception is a recent paper by Chen et al. that we became aware of while preparing the
ﬁnal version of this work [8]. They show that sampling the matrix according to statistical leverages
of the rows and columns can eliminate the need for incoherence assumptions. Speciﬁcally  when the
matrix has incoherent column space  they show that by ﬁrst estimating the leverages of the columns 
sampling the matrix according to this distribution  and then solving the nuclear norm minimization
program  one can recover the matrix with ⌦(nrµ0 log2 n) samples. Our result improves on theirs
when r is small compared to n  speciﬁcally when pr log r  log2 n  which is common.
Our algorithm is also very computationally efﬁcient. Existing algorithms involve successive singular
value decompositions (O(n1n2r) per iteration)  resulting in much worse running times.
The key ingredient in our proofs is a result pertaining to subspace detection  the task of testing if
a subsampled vector lies in a subspace. This result  which improves over the results of Balzano et
al. [2]  is crucial in obtaining our sample complexity bounds  and may be of independent interest.
Theorem 4. Let U be a d-dimensional subspace of Rn and y = x + v where x 2 U and v 2 U?.
 and let ⌦ be an index set with entries sampled uniformly with
Fix > 0  m  8
replacement with probability m/n. Then with probability at least 1  4:
||v||2
2  (1 + ↵)
Where ↵ = q2 µ(v)
m log(1/) + 2 µ(v)
3m log(1/)   = 6 log(d/) + 4
q 8dµ(U )
3m log(2d/) and µ(v) = n||v||2
2.
1/||v||2
This theorem shows that if m = ⌦(max{µ(v)  dµ(U )  dpµ(U )µ(v)} log d) then the orthogonal
projection from missing data is within a constant factor of the fully observed one.
In contrast 
Balzano et al. [2] give a similar result that requires m = ⌦(max{µ(v)2  dµ(U )  dµ(U )µ(v)} log d)
to get a constant factor approximation. In the matrix case  this improved dependence on incoherence
parameters brings our sample complexity down from nr2µ2
0 log r to nr3/2µ0 log r. We conjecture
that this theorem can be further improved to eliminate another pr factor from our ﬁnal bound.

3 dµ(U ) log 2d

2 || y⌦ P U⌦y⌦||2

m(1  ↵)  dµ(U )

m log2(d/)   =

(1)

m
n ||v||2

2

(5)

dµ(v)

3

n



4.1 Lower Bounds for Uniform Sampling

We adapt the proof strategy of Candes and Tao [7] to the tensor completion problem and establish
the following lower bound for uniform sampling:
Theorem 5 (Passive Lower Bound). Fix 1  m  r  mint nt and µ0 > 1. Fix 0 << 1/2 and
suppose that we do not have the condition:

QT
Then there exist inﬁnitely many pairs of distinct n1 ⇥ . . . ⇥ nT order-T tensors M 6= M0 of rank r
with coherence parameter  µ0 such that P⌦(M) = P⌦(M0) with probability at least . Each entry
is observed independently with probability T = mQT

i=1 ni

.

(6)

µT1
0

rT1
i=2 ni

log⇣ n1
2⌘

 log 1 

m

i=1 ni! 
QT

5

m  n1rT1µT1

0

log⇣ n1

2⌘ (1  ✏/2)

(7)

Theorem 5 implies that as long as the right hand side of Equation 6 is at most ✏< 1  and:

then with probability at least  there are inﬁnitely many matrices that agree on the observed entries.
This gives a necessary condition on the number of samples required for tensor completion. Note
that when T = 2 we recover the known lower bound for matrix completion.
Theorem 5 gives a necessary condition under uniform sampling. Comparing with Theorem 2 shows
that our procedure outperforms any passive procedure in its dependence on the tensor dimensions.
However  our guarantee is suboptimal in its dependence on r. The extra factor of pr would be
eliminated by a further improvement to Theorem 5  which we conjecture is indeed possible.
For adaptive sampling  one can obtain a lower bound via a parameter counting argument. Observing
k (it) = Mi1 ... iT . If

the (i1  . . .   iT )th entry leads to a polynomial equation of the formPkQt a(t)
m < r(Pt nt)  this system is underdetermined showing that ⌦((Pt nt)r) observations are neces-

sary for exact recovery  even under adaptive sampling. Thus  our algorithm enjoys sample complex-
ity with optimal dependence on matrix dimensions.

5 Noisy Matrix Completion

Our algorithm for noisy matrix completion is an adaptation of the column subset selection (CSS)
algorithm analyzed by Deshpande et al. [10]. The algorithm builds a candidate column space in
rounds; at each round it samples additional columns with probability proportional to their projection
on the orthogonal complement of the candidate column space.
To concretely describe the algorithm  suppose that at the beginning of the lth round we have a
candidate subspace Ul. Then in the lth round  we draw s additional columns according to the
distribution where the probability of drawing the ith column is proportional to ||PU?l
2. Observing
these s columns in full and then adding them to the subspace Ul gives the candidate subspace Ul+1
for the next round. We initialize the algorithm with U1 = ;. After L rounds  we approximate each
column c with ˆc = UL(U T
The challenge is that the algorithm cannot compute the sampling probabilities without observing
entries of the matrix. However  our results show that with reliable estimates  which can be computed
from few observations  the algorithm still performs well.
We assume that the matrix M 2 Rn1⇥n2 can be decomposed as a rank r matrix A and a random
gaussian matrix R whose entries are independently drawn from N (0  2). We write A = U ⌃V T
and assume that µ(U )  µ0. As before  the incoherence assumption is crucial in guaranteeing that
one can estimate the column norms  and consequently sampling probabilities  from missing data.
Theorem 6. Let ⌦ be the set of all observations over the course of the algorithm 
let UL
be the subspace obtained after L = log(n1n2) rounds and ˆM be the matrix whose columns
ˆci = UL(U T

L⌦c⌦ and concatenate these estimates to form ˆM.

L⌦c⌦i. Then there are constants c1  c2 such that:

L⌦UL⌦)1U T

L⌦UL⌦)1U T

ci||2

||A  ˆM||2

F 

c1

(n1n2)||A||2

F + c2||R⌦||2

F

ˆM can be computed from ⌦((n1 + n2)r3/2µ(U )polylog(n1n2)) observations.
F = 1 and Rij ⇠N (0  2/(n1n2))  then there is a constant c? for which:
||A||2

In particular  if

||A  ˆA||2

F 

c?

n1n2⇣1 + 2⇣(n1 + n2)r3/2µ(U )polylog(n1n2)⌘⌘

The main improvement in the result is in relaxing the assumptions on the underlying matrix A.
Existing results for noisy matrix completion require that the energy of the matrix is well spread
out across both the rows and the columns (i.e. incoherence)  and the sample complexity guarantees
deteriorate signiﬁcantly without such an assumption [5  17]. As a concrete example  Negahban and
Wainwright [20] use a notion of spikiness  measured as pn1n2 ||A||1
which can be as large as pn2
||A||F
in our setup  e.g. when the matrix is zero except for on one column and constant across that column.

6

(a)

(d)

(b)

(e)

(c)

(f)

(g)

(h)

Figure 1: Probability of success curves for our noiseless matrix completion algorithm (top) and
SVT (middle). Top: Success probability as a function of: Left: p  the fraction of samples per
column  Center: np  total samples per column  and Right: np log2 n  expected samples per column
for passive completion. Bottom: Success probability of our noiseless algorithm for different values
of r as a function of p  the fraction of samples per column (left)  p/r3/2 (middle) and p/r (right).

(i)

F = 1 and noise variance rescaled by

The choices of ||A||2
enable us to compare our results
with related work [20]. Thinking of n1 = n2 = n and the incoherence parameter as a constant  our
results imply consistent estimation as long as 2 = !⇣

r2polylog(n)⌘. On the other hand  thinking

of the spikiness parameter as a constant  [20] show that the error is bounded by 2nr log n
where
m is the total number of observations. Using the same number of samples as our procedure  their
results implies consistency as long as 2 = !(rpolylog(n)). For small r (i.e. r = O(1))  our noise
tolerance is much better  but their results apply even with fewer observations  while ours do not.

m

1

n1n2

n

6 Simulations

We verify Corollary 3’s linear dependence on n in Figure 1  where we empirically compute the
success probability of the algorithm for varying values of n and p = m/n  the fraction of entries
observed per column. Here we study square matrices of ﬁxed rank r = 5 with µ(U ) = 1. Figure 1(a)
shows that our algorithm can succeed with sampling a smaller and smaller fraction of entries as n
increases  as we expect from Corollary 3. In Figure 1(b)  we instead plot success probability against
total number of observations per column. The fact that the curves coincide suggests that the samples
per column  m  is constant with respect to n  which is precisely what Corollary 3 implies. Finally 
in Figure 1(c)  we rescale instead by n/ log2 n  which corresponds to the passive sample complexity
bound [21]. Empirically  the fact that these curves do not line up demonstrates that our algorithm
requires fewer than log2 n samples per column  outperforming the passive bound.
The second row of Figure 1 plots the same probability of success curves for the Singular Value
Thresholding (SVT) algorithm [4]. As is apparent from the plots  SVT does not enjoy a linear
dependence on n; indeed Figure 1(f) conﬁrms the logarithmic dependency that we expect for passive
matrix completion  and establishes that our algorithm has empirically better performance.

7

n

1000

5000

10000

Unknown M

r m/dr m/n2
0.07
10
0.33
50
100
0.61
0.01
10
0.07
50
0.14
100
10
0.01
0.03
50
100
0.07

3.4
3.3
3.2
3.4
3.5
3.4
3.4
3.5
3.5

Results
time (s)

16
29
45
3
27
104
10
84
283

Table 1: Computational results on large low-
rank matrices. dr = r(2n r) is the degrees of
freedom  so m/dr is the oversampling ratio.

Figure 2: Reconstruction error as a function of
row space incoherence for our noisy algorithm
(CSS) and the semideﬁnite program of [20].
In the third row  we study the algorithm’s dependence on r on 500 ⇥ 500 square matrices. In Fig-
ure 1(g) we plot the probability of success of the algorithm as a function of the sampling probability
p for matrices of various rank  and observe that the sample complexity increases with r. In Fig-
ure 1(h) we rescale the x-axis by r3/2 so that if our theorem is tight  the curves should coincide. In
Figure 1(i) we instead rescale the x-axis by r1 corresponding to our conjecture about the perfor-
mance of the algorithm. Indeed  the curves line up in Figure 1(i)  demonstrating that empirically  the
number of samples needed per column is linear in r rather than the r3/2 dependence in our theorem.
To conﬁrm the computational improvement over existing methods  we ran our matrix completion
algorithm on large-scale matrices  recording the running time and error in Table 1. To contrast with
SVT  we refer the reader to Table 5.1 in [4]. As an example  recovering a 10000 ⇥ 10000 matrix of
rank 100 takes close to 2 hours with the SVT  while it takes less than 5 minutes with our algorithm.
For the noisy algorithm  we study the dependence on row-space incoherence. In Figure 2  we plot the
reconstruction error as a function of the row space coherence for our procedure and the semideﬁnite
program of Negahban and Wainwright [20]  where we ensure that both algorithms use the same
number of observations. It’s readily apparent that the SDP decays in performance as the row space
becomes more coherent while the performance of our procedure is unaffected.

7 Conclusions and Open Problems
In this work  we demonstrate how sequential active algorithms can offer signiﬁcant improvements
in time  and measurement overhead over passive algorithms for matrix and tensor completion. We
hope our work motivates further study of sequential active algorithms for machine learning.
Several interesting theoretical questions arise from our work:

1. Can we tighten the dependence on rank for these problems? In particular  can we bring the

dependence on r down from r3/2 to linear? Simulations suggest this is possible.

2. Can one generalize the nuclear norm minimization program for matrix completion to the

tensor completion setting while providing theoretical guarantees on sample complexity?

We hope to pursue these directions in future work.

Acknowledgements

This research is supported in part by AFOSR under grant FA9550-10-1-0382 and NSF under grant
IIS-1116458. AK is supported in part by a NSF Graduate Research Fellowship. AK would like to
thank Martin Azizyan  Sivaraman Balakrishnan and Jayant Krishnamurthy for fruitful discussions.

References
[1] Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approximations.

Journal of the ACM (JACM)  54(2):9  2007.

8

[2] Laura Balzano  Benjamin Recht  and Robert Nowak. High-dimensional matched subspace
detection when data are missing. In Information Theory Proceedings (ISIT)  2010 IEEE Inter-
national Symposium on  pages 1638–1642. IEEE  2010.

[3] Christos Boutsidis  Michael W Mahoney  and Petros Drineas. An improved approximation
algorithm for the column subset selection problem. In Proceedings of the twentieth Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 968–977. Society for Industrial and
Applied Mathematics  2009.

[4] Jian-Feng Cai  Emmanuel J Cand`es  and Zuowei Shen. A singular value thresholding algorithm

for matrix completion. SIAM Journal on Optimization  20(4):1956–1982  2010.

[5] Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE 

98(6):925–936  2010.

[6] Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization.

Foundations of Computational mathematics  9(6):717–772  2009.

[7] Emmanuel J Cand`es and Terence Tao. The power of convex relaxation: Near-optimal matrix

completion. Information Theory  IEEE Transactions on  56(5):2053–2080  2010.

[8] Yudong Chen  Srinadh Bhojanapalli  Sujay Sanghavi  and Rachel Ward. Coherent matrix

completion. arXiv preprint arXiv:1306.2979  2013.

[9] Mark A Davenport and Ery Arias-Castro. Compressive binary search. In Information Theory
Proceedings (ISIT)  2012 IEEE International Symposium on  pages 1827–1831. IEEE  2012.
[10] Amit Deshpande  Luis Rademacher  Santosh Vempala  and Grant Wang. Matrix approximation

and projective clustering via volume sampling. Theory of Computing  2:225–247  2006.

[11] Silvia Gandy  Benjamin Recht  and Isao Yamada. Tensor completion and low-n-rank tensor

recovery via convex optimization. Inverse Problems  27(2):025010  2011.

[12] Alex Gittens. The spectral norm error of the naive nystrom extension.

arXiv:1110.5305  2011.

arXiv preprint

[13] David Gross. Recovering low-rank matrices from few coefﬁcients in any basis. Information

Theory  IEEE Transactions on  57(3):1548–1566  2011.

[14] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix re-
construction. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms  pages 1207–1214. SIAM  2012.

[15] Jarvis D Haupt  Richard G Baraniuk  Rui M Castro  and Robert D Nowak. Compressive
distilled sensing: Sparse recovery using adaptivity in compressive measurements. In Signals 
Systems and Computers  2009 Conference Record of the Forty-Third Asilomar Conference on 
pages 1551–1555. IEEE  2009.

[16] Jun He  Laura Balzano  and John Lui. Online robust subspace tracking from partial informa-

tion. arXiv preprint arXiv:1109.3827  2011.

[17] Raghunandan H Keshavan  Andrea Montanari  and Sewoong Oh. Matrix completion from

noisy entries. The Journal of Machine Learning Research  99:2057–2078  2010.

[18] Sanjiv Kumar  Mehryar Mohri  and Ameet Talwalkar. Sampling methods for the nystr¨om

method. The Journal of Machine Learning Research  98888:981–1006  2012.

[19] B´eatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model

selection. The annals of Statistics  28(5):1302–1338  2000.

[20] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix

completion: Optimal bounds with noise. The Journal of Machine Learning Research  2012.

[21] Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine Learning

Research  7777777:3413–3430  2011.

[22] Ryota Tomioka  Kohei Hayashi  and Hisashi Kashima. Estimation of low-rank tensors via

convex optimization. arXiv preprint arXiv:1010.0789  2010.

[23] Ryota Tomioka  Taiji Suzuki  Kohei Hayashi  and Hisashi Kashima. Statistical performance of
convex tensor decomposition. In Advances in Neural Information Processing Systems  pages
972–980  2011.

[24] Roman Vershynin.

Introduction to the non-asymptotic analysis of random matrices. arXiv

preprint arXiv:1011.3027  2010.

9

,Akshay Krishnamurthy
Aarti Singh