2011,Unifying Non-Maximum Likelihood Learning Objectives with Minimum KL Contraction,When used to learn high dimensional parametric probabilistic models  the clas- sical maximum likelihood (ML) learning often suffers from computational in- tractability  which motivates the active developments of non-ML learning meth- ods. Yet  because of their divergent motivations and forms  the objective func- tions of many non-ML learning methods are seemingly unrelated  and there lacks a unified framework to understand them. In this work  based on an information geometric view of parametric learning  we introduce a general non-ML learning principle termed as minimum KL contraction  where we seek optimal parameters that minimizes the contraction of the KL divergence between the two distributions after they are transformed with a KL contraction operator. We then show that the objective functions of several important or recently developed non-ML learn- ing methods  including contrastive divergence [12]  noise-contrastive estimation [11]  partial likelihood [7]  non-local contrastive objectives [31]  score match- ing [14]  pseudo-likelihood [3]  maximum conditional likelihood [17]  maximum mutual information [2]  maximum marginal likelihood [9]  and conditional and marginal composite likelihood [24]  can be unified under the minimum KL con- traction framework with different choices of the KL contraction operators.,Unifying Non-Maximum Likelihood Learning

Objectives with Minimum KL Contraction

Computer Science Department

University at Albany  State University of New York

Siwei Lyu

lsw@cs.albany.edu

Abstract

When used to learn high dimensional parametric probabilistic models  the clas-
sical maximum likelihood (ML) learning often suffers from computational in-
tractability  which motivates the active developments of non-ML learning meth-
ods. Yet  because of their divergent motivations and forms  the objective func-
tions of many non-ML learning methods are seemingly unrelated  and there lacks
a uniﬁed framework to understand them. In this work  based on an information
geometric view of parametric learning  we introduce a general non-ML learning
principle termed as minimum KL contraction  where we seek optimal parameters
that minimizes the contraction of the KL divergence between the two distributions
after they are transformed with a KL contraction operator. We then show that
the objective functions of several important or recently developed non-ML learn-
ing methods  including contrastive divergence [12]  noise-contrastive estimation
[11]  partial likelihood [7]  non-local contrastive objectives [31]  score match-
ing [14]  pseudo-likelihood [3]  maximum conditional likelihood [17]  maximum
mutual information [2]  maximum marginal likelihood [9]  and conditional and
marginal composite likelihood [24]  can be uniﬁed under the minimum KL con-
traction framework with different choices of the KL contraction operators.

1

Introduction

(cid:80)n

1
n

Z(θ) = (cid:82)

Fitting parametric probabilistic models to data is a basic task in statistics and machine learning.
Given a set of training data {x(1) ···   x(n)}  parameter learning aims to ﬁnd a member in a
parametric distribution family  qθ  to best represent the training data.
In practice  many useful
high dimensional parametric probabilistic models  such as Markov random ﬁelds [18] or products
of experts [12]  are deﬁned as qθ(x) = ˜qθ(x)/Z(θ)  where ˜qθ is the unnormalized model and
Rd ˜qθ(x)dx is the partition function. The maximum (log) likelihood (ML) estimation is
the most commonly used method for parameter learning  where the optimal parameter is obtained
by solving argmaxθ
k=1 log qθ(x(k)). The obtained ML estimators has many desirable proper-
ties  such as consistency and asymptotic normality [21]. However  because of the high dimensional
integration/summation  the partition function in qθ oftentimes makes ML learning computationally
intractable. For this reason  non-ML parameter learning methods that use “tricks” to obviate direct
computation of the partition function have experienced rapid developments  particularly in recent
years. While many computationally efﬁcient non-ML learning methods have achieved impressive
practical performances  with a few exceptions  their different learning objectives and numerical im-
plementations seem to suggest that they are largely unrelated.
In this work  based on the information geometric view of parametric learning  we elaborate on a gen-
eral non-ML learning principle termed as minimum KL contraction (MKC)  where we seek optimal
parameters that minimize the contraction of the KL divergence between two distributions after they
are transformed with a KL contraction operator. The KL contraction operator is a mapping between

1

probability distributions under which the KL divergence of two distributions tend to reduce unless
they are equal. We then show that the objective functions of a wide range of non-ML learning meth-
ods  including contrastive divergence [12]  noise-contrastive estimation [11]  partial likelihood [7] 
non-local contrastive objectives [31]  score matching [14]  pseudo-likelihood [3]  maximum con-
ditional likelihood [17]  maximum mutual information [2]  maximum marginal likelihood [9]  and
conditional and marginal composite likelihood [24]  can all be uniﬁed under the MKC framework
with different choices of the KL contraction operators and MKC objective functions.

2 Related Works

Similarities in the parameter updates among non-ML learning methods have been noticed in several
recent works. For instance  in [15]  it is shown that the parameter update in score matching [14] is
equivalent to the parameter update in a version of contrastive divergence [12] that performs Langevin
approximation instead of Gibbs sampling  and both are approximations to the parameter update of
pseudo-likelihood [3]. This connection is further generalized in [1]  which shows that parameter
update in another variant of contrastive divergence is equivalent to a stochastic parameter update
in conditional composite likelihood [24]. However  such similarities in numerical implementations
are only tangential to the more fundamental relationship among the objective functions of different
non-ML learning methods. On the other hand  the energy based learning [22] presents a general
framework that subsume most non-ML learning objectives  but its broad generality also obscures
their speciﬁc statistical interpretations.
At the objective function level  relations between some non-ML methods are known. For instance 
it is known that pseudo-likelihood is a special case of conditional composite likelihood [30]. In
[10]  several non-ML learning methods are uniﬁed under the framework of minimizing Bregman
divergence.

3 KL Contraction Operator

We base our discussion hereafter on continuous variables and probability density functions. Most
results can be readily extended to the discrete case by replacing integrations and probability density
functions with summations and probability mass functions. We denote Ωd as the set of all probability
(cid:82)
density functions over Rd. For two different probability distributions p  q ∈ Ωd  their Kulback-
Leibler (KL) divergence (also known as relative entropy or I-divergence) [6] is deﬁned as KL(p(cid:107)q) =
Rd p(x) log p(x)
q(x) dx. KL divergence is non-negative and equals to zero if and only if p = q almost
everywhere (a.e.). We deﬁne a distribution operator  Φ  as a mapping between a density function
p ∈ Ωd to another density function ˜p ∈ Ωd(cid:48)  and adopt the shorthand notation ˜p = Φ{p}. A
distribution p is a ﬁx point of a distribution operator Φ if p = Φ{p}.
A KL contraction operator is a distribution operator  Φ :
Ωd (cid:55)→ Ωd(cid:48)  such that for any p  q ∈ Ωd  there exist a
constant β ≥ 1 for the following condition to hold:
(1)
Subsequently  β is known as the contraction factor  and
LHS of Eq.(1) is the KL contraction of p and q under Φ.
Obviously  if p = q (a.e.)  their KL contraction  as well
as their KL divergence  is zero. In addition  a KL con-
traction operator is strict if the equality in Eq.(1) holds
only when p = q (a.e.). Intuitively  if the KL divergence
is regarded as a “distance” metric of probability distri-
butions1  then it is never increased after both distributions are transformed with a KL contraction
operator  a graphical illustration of which is shown in Fig.1. Furthermore  under a strict KL contrac-
tion operator  the KL divergence is always reduced unless the two distributions are equal (a.e.). The
KL contraction operators are analogous to the contraction operators in ordinary metric spaces  with
β having a similar role as the Lipschitz constant [19].

Figure 1: Illustration of a KL contraction
operator on two density functions p and q.

KL(p(cid:107)q) − βKL(Φ{p}(cid:107)Φ{q}) ≥ 0.

1Indeed  it is known that the KL divergence behaves like the squared Euclidean distance [6].

2

pq˜q˜pDKL(˜p￿˜q)DKL(p￿q)˜p=Φ{p}˜q=Φ{q}Eq.(1) gives the general and abstract deﬁnition of KL contraction operators. In the following  we
give several examples of KL contraction operators that are constructed from common operations of
probability distributions.

3.1 Conditional Distribution
We can form a family of KL contraction operators using conditional distributions. Consider x ∈ Rd
(cid:90)
with distribution p(x) ∈ Ωd and y ∈ Rd(cid:48)
  from a conditional distribution  T (y|x)  we can deﬁne a
distribution operator  as
(2)
Φc
T{p}(y) =

T (y|x)p(x)dx = ˜p(y).

The following result shows that Φc
Lemma 1 (Cover & Thomas [6]2) For two distributions p  q ∈ Ωd  with the distribution operator
deﬁned in Eq.(2)  we have

T is a strict KL contraction operator with β = 1.

Rd

KL(p(cid:107)q) − KL(Φc
where Tp(x|y) = T (y|x)p(x)
from p and q with T . Furthermore  the equality holds if and only if p = q (a.e.).
3.2 Marginalization and Marginal Grafting

T{p}(cid:107)Φc
and Tq(x|y) = T (y|x)q(x)

Rd(cid:48) ˜p(y)KL(Tp(x|y)(cid:107)Tq(x|y)) dy ≥ 0 

T{q}) =

˜p(y)

˜q(y)

are the induced conditional distributions

(cid:90)

Two related types of KL contraction operators can be constructed based on marginal distributions.
Consider x with distribution p(x) ∈ Ωd  and a nonempty index subset A ⊂ {1 ···   d}. Let \A
denote {1 ···   d}− A  the marginal distribution  pA(xA)  of sub-vector xA formed by components
whose indices are in A is obtained by integrating p(x) over sub-vector x\A. This marginalization
operation thus deﬁnes a distribution operator between p ∈ Ωd and pA ∈ Ω|A|  as:

(3)
Another KL contraction operator termed as marginal grafting can also be deﬁned based on pA. For
a distribution q(x) ∈ Ωd  the marginal grafting operator is deﬁned as:

Φm
A{p}(xA) =

p(x)dx\A = pA(xA)

Rd−|A|

(cid:90)

Φg
p A{q}(x) =

(4)
Φg
p A{q} can be understood as replacing qA in q(x) with pA. The last term in Eq.4 is nonnegative
and integrates to one over Rd  and is thus a proper probability distribution in Ωd. Furthermore  p is
the ﬁxed point of operator Φg

= q\A|A(x\A|xA)pA(xA) 

qA(xA)

p A  as Φg

q(x)pA(xA)

The following result shows that both Φm
in a sense complementary to each other.
Lemma 2 (Huber [13]) For two distributions p  q ∈ Ωd  with the distribution operators deﬁned in
Eq.(3) and Eq.(4)  we have

p A are KL contraction operators  and that they are

p A{p} = p.
A and Φmg

(cid:16)
(cid:17)

(cid:90)

(cid:17)

(cid:13)(cid:13)(cid:13)Φg
pA(xA)KL(cid:0)p\A|A(·|xA)(cid:13)(cid:13)q\A|A(·|xA)(cid:1) dxA 

A{p}(cid:107)Φm

A{q}) .

= KL(Φm

p A{q}

KL(Φm

where p\A|A(·|xA) and q\A|A(·|xA) are the conditional distributions induced from p(x) and q(x) 
and

A{p}(cid:107)Φm
Lemma 2 also indicates that neither Φm
p A is strict - the KL contraction of p(x) and q(x) for
the former is zero if p\A|A(x\A|xA) = q\A|A(x\A|xA) (a.e.)  even though they may differ on the
marginal distribution over xA. And for the latter  having pA(xA) = qA(xA) is sufﬁcient to make
their KL contraction zero.

A{q}) = KL(pA(xA)(cid:107)qA(xA)) .

A nor Φmg

2We cite the original reference to this and subsequent results  which are recast using the terminology in-
troduced in this work. Due to the limit of space  we defer formal proofs of these results to the supplementary
materials.

3

Furthermore 

(cid:16)

KL(p(cid:107)q) − KL

Φg
p A{p}

KL

Φg
p A{p}

p A{q}

=

Rd

(cid:13)(cid:13)(cid:13)Φg

3.3 Binary Mixture
For two different distributions p(x) and g(x) ∈ Ωd  we introduce a binary variable c ∈ {0  1} and
P r(c = i) = πi  with π0  π1 ∈ [0  1] and π0 + π1 = 1. We can then form a joint distribution
ˆp(x  c = 0) = π0g(x) and ˆp(x  c = 1) = π1p(x) over Rd × {0  1}. Marginalizing out c from
ˆp(x  c)  we obtain a binary mixture ˜p(x)  which induces a distribution operator:
(5)

Φb
g{p}(x) = π0g(x) + π1p(x) = ˜p(x).

g{q}

g{p}

(cid:1) =

(cid:13)(cid:13)Φb

KL(p(cid:107)q) −

KL(cid:0)Φb

g is a strict KL contraction operator with β = 1/π1.

The following result shows that Φb
Lemma 3 For two distributions p  q ∈ Ωd  with the distribution operator deﬁned in Eq.(5)  we have

˜p(x)(cid:2)KL(cid:0)pc|x(c|x)(cid:13)(cid:13)qc|x(c|x)(cid:1)(cid:3) dx ≥ 0 
Let S = (S1  S2 ···   Sm) be a partition of Rd such that Si ∩ Sj = ∅ for i (cid:54)= j  and(cid:83)m

where p(c|x) and q(c|x) are the induced posterior conditional distributions from ˆp(x  c) and ˆq(x  c) 
respectively. The equality holds if and only if p = q (a.e.).

i=1 Si = Rd 
the lumping [8] of a distribution p(x) ∈ Ωd over S yields a distribution over i ∈ {1  2 ···   m}  and
subsequently induces a distribution operator ΦlS  as:

3.4 Lumping

1
π1

1
π1

(cid:90)

Rd

ΦlS{p}(i) =

S
i   for i = 1 ···   m.
The following result shows that ΦlS is a KL contraction operator with β = 1.
Lemma 4 (Csisz`ar & Shields [8]) For two distributions p  q ∈ Ωd  with the distribution operator
deﬁned in Eq.(6)  we have

p(x)dx = P

x∈Si

(6)

(cid:90)

KL(p(cid:107)q) − KL(cid:0)ΦlS{p}

(cid:82)
p(x)×1[x∈Si]
x(cid:48)∈Si

m(cid:88)

i=1

(cid:13)(cid:13)ΦlS{p}
(cid:1) =
(cid:82)
q(x)×1[x∈Si]
x(cid:48)∈Si

P

S
i KL(˜pi(cid:107)˜qi) ≥ 0 

p(x(cid:48))dx(cid:48) and ˜qi(x) =

where ˜pi(x) =
q by restricting to Si  respectively  with 1[·] being the indicator function.
Note that ΦlS is in general not strict  as two distributions agree over all ˜pi and ˜qi will have a zero KL
contraction.

q(x(cid:48))dx(cid:48) are the distributions induced from p and

4 Minimizing KL Contraction for Parametric Learning

(cid:82)

(cid:80)n

In this work  we take the information geometric view of parameter learning - assuming training
data are samples from a distribution p ∈ Ωd  we seek an optimal distribution on the statistical
manifold corresponding to the parametric distribution family qθ that best approximates p [20]. In this
context  the maximum (log) likelihood learning is equivalent to ﬁnding parameter θ that minimizes
(cid:82)
the KL divergence of p and qθ [8]  as argminθ KL(p(cid:107)qθ) = argmaxθ
Rd p(x) log qθ(x)dx. The
data based ML objective is obtained when we approximate the expectation with sample average as
Rd p(x) log qθ(x)dx ≈ 1
The KL contraction operators suggest an alternative approach for parametric learning. In particular 
the KL contraction of p and qθ under a KL contraction operator is always nonnegative and reaches
zero when p and qθ are equal almost everywhere. Therefore  we can minimize their KL contrac-
tion under a KL contraction operator to encourage the matching of qθ to p. We term this general
approach of parameter learning as minimum KL contraction (MKC). Mathematically  minimum KL
contraction may be realized with three different but related types of objective functions.
- Type I: With a KL contraction operator Φ  we can ﬁnd optimal θ that directly minimizes the KL

k=1 log qθ(x(k)).

n

contraction between p and qθ  as:
argmin

KL(p(cid:107)qθ) − βKL(Φ{p}(cid:107)Φ{qθ}) .

θ

(7)

In practice  it may be desirable to use Φ with β = 1 that is also a linear operator for L2 bounded
functions over Rd [19]. To better see this  consider qθ(x) = ˜qθ(x)
Z(θ) as the model deﬁned with

4

n(cid:88)

1
n

n(cid:48)(cid:88)

1
n(cid:48)

the unnormalized model and its partition function. Furthermore  assuming that we can obtain
samples {x1 ···   xn} and {y1 ···   yn(cid:48)} from p and Φ{p}  respectively  the optimization of
Eq.(7) can be approximated as

θ

θ

k=1

k=1

log ˜qθ(x(k))−

KL(p(cid:107)qθ)−KL(Φ{p}(cid:107)Φ{qθ})≈ argmax

log Φ{˜qθ}(y(k)) 
argmin
where due to the linearity of Φ  the two terms of Z(θ) in qθ and L{qθ} cancel out each other.
Therefore  the optimization does not require the computation of the partition function  a highly
desirable property for ﬁtting parameters in high dimensional probabilistic models with intractable
partition functions. Type I MKC objective functions with KL contraction operators induced from
conditional distribution  marginalization  marginal grafting  linear transform  and lumping all fall
into this category. However  using nonlinear KL contraction operators  such as the one induced
from binary mixtures  may also be able to avoid computing the partition function (e.g.  Section
4.4). Furthermore  the KL contraction operator in Eq.(7) can have parameters  which can include
the model parameter θ (e.g.  Section 4.2). However  the optimization becomes more complicated
as Φ{p} cannot be ignored when optimizing θ. Last  note that when using Type I MKC objective
functions with a non-strict KL contraction operator  we cannot guarantee p = qθ even if their
corresponding KL contraction is zero.

(cid:12)(cid:12)(cid:12)t=0

- Type II: Consider a strict KL contraction operator with β = 1  denoted as Φt  is parameterized
by an auxiliary parameter t that is different from θ  and for any distribution p ∈ Ωd  we have
Φ0{p} = p and Φt{p} is continuously differentiable with regards to t. Then  the KL divergence
Φt{p} and Φt{qθ} can be regarded as a function of t and θ  as: L(t  θ) = KL(Φt{p}(cid:107)Φt{qθ).
Thus  the KL contraction in Eq.(7) can be approximated with a Taylor expansion near t = 0  as
KL(p(cid:107)qθ)− KL(Φδt{p}(cid:107)Φδt{qθ}) = KL(Φ0{p}(cid:107)Φ0{qθ})− KL(Φδt{p}(cid:107)Φδt{qθ}) = L(0  θ)−
L(δt  θ) ≈ −δt ∂L(t θ)
. If the derivative of KL contrac-
tion with regards to t is easier to work with than the KL contraction itself (e.g.  Section 4.5) 
we can ﬁx δt and equivalently maximizing the derivative  which is the Type II MKC objective
function  as

∂t KL(Φt{p}(cid:107)Φt{qθ})(cid:12)(cid:12)t=0
(cid:12)(cid:12)(cid:12)(cid:12)t=0

(8)
- Type III: In the case where we have access to a set of different KL contraction operators 
{Φ1 ···   Φm}  we can implement the minimum KL contraction principle by ﬁnding optimal
θ that minimizes their average KL contraction  as
(9)

KL(Φt{p}(cid:107)Φt{qθ})

= −δt ∂

m(cid:88)

argmax

∂
∂t

argmin

∂t

.

θ

(KL(p(cid:107)qθ) − βiKL(Φi{p}(cid:107)Φi{qθ})) .

1
m

θ

As each KL contraction in the sum is nonnegative  Eq.(9) is zero if and only if each KL contrac-
tion is zero. If the consistency of p and qθ with regards to Φi corresponds to certain constraints
on qθ  the objective function  Eq.(9)  represents the consistency of all such constraints. Under
some special cases  minimizing Eq.(9) to zero over a sufﬁcient number of certain types of KL
contraction operators may indeed ensure equality of p and qθ (e.g.  Section 4.6).

i=1

4.1 Fitting Gaussian Model with KL Contraction Operator from a Gaussian Distribution

We ﬁrst describe an instance of MKC learning under a very simple setting  where we approximate
0  with a Gaussian model qθ
a distribution p(x) for x ∈ R with known mean µ0 and variance σ2
(cid:19)
whose mean and variance are the parameters to be estimated as θ = (µ  σ2). Using the strict KL
contraction operator Φc

T constructed with a Gaussian conditional distribution

(cid:18)

1(cid:112)

T (y|x) =

exp

−

2πσ2
1

(y − x)2

2σ2
1

 

(cid:20) σ2

with known variance σ2
reduced to a closed form objective function  as:

1  we form the Type I MKC objective function. In this simple case  Eq.(7) is

0

µ σ2

argmin

2σ2 −

0 + σ2
σ2
1
2(σ2 + σ2
1)

1
2
whose optimal solution  µ = µ0 and σ2 = σ2
0  is obtained by direct differentiation. The detailed
derivation of this result is omitted due to the limit of space. Note that  the optimal parameters do
not rely on the parameter in the KL contraction operator (in this case  σ2
1)  and are the same as those
obtained by minimizing the KL divergence between p and qθ  or equivalently  maximizing the log
likelihood  when samples from p(x) are used to approximate the expectation.

1(µ − µ0)2
σ2
2σ2(σ2 + σ2
1)

σ2 + σ2
1

log

σ2

+

+

 

(cid:21)

5

4.2 Relation with Contrastive Divergence [12]

(cid:82)
Next  we consider the general strict KL contraction operator Φc
constructed from a conditional
Tθ
distribution  Tθ(y|x)  for x  y ∈ Rd  of which the parametric model qθ is a ﬁxed point  as qθ(y) =
In other words  qθ is the equilibrium distribution of the
Rd Tθ(y|x)qθ(x)dx = Φc
Markov chain whose transitional distribution is given by Tθ(y|x). The Type I objective function
of minimum KL contraction  Eq.(7)  for p  qθ ∈ Ωd under Φc

Tθ{qθ}(y).

is

θ

argmin

Tθ{p}
where pθ is the shorthand notation for Φc
Tθ{p}. Note that this is the objective function of the con-
trastive divergence learning [12]. However  the dependency of pθ on θ makes this objective function
difﬁcult to optimize. By ignoring this dependency  the practical parameter update in contrastive
divergence only approximately follows the gradient of this objective function [5].

KL(p(cid:107)qθ) − KL(pθ(cid:107)qθ)  

Tθ{qθ}

θ

KL(p(cid:107)qθ) − KL(cid:0)Φc

(cid:1) = argmin

Tθ

(cid:13)(cid:13)Φc

4.3 Relation with Partial Likelihood [7] and Non-local Contrastive Objectives [31]

Next  we consider the Type I MKC objective function  Eq.(7)  combined with the KL contraction
operator constructed from lumping. Using Lemma 4  we have

(cid:8)KL(p(cid:107)qθ) − KL(cid:0)ΦlS{p}
m(cid:88)

(cid:90)

˜pi(x) log ˜qθ

S
i

P

(cid:13)(cid:13)ΦlS{qθ}

(cid:1)

(cid:13)(cid:13)˜qθ

i

(cid:1)(cid:9) = argmin
n(cid:88)

θ

m(cid:88)

i=1

P

S

i KL(cid:0)˜pi
m(cid:88)

1[x(k)∈Si]

k=1

i=1

1
n

= argmax

θ

i=1

x∈Si

i (x)dx ≈ argmax

θ

S
i

log ˜qθ

i (x(k)) 

P

argmin

θ

where {x(1) ···   x(n)} are samples from p(x). Minimizing KL contraction in this case is equiv-
alent to maximizing the weighted sum of log likelihood of the probability distributions formed by
restricting the overall model to subsets of state space. The last step resembles the partial likelihood
objective function [7]  which is recently rediscovered in the context of discriminative learning as
non-local contrastive objectives [31]. In [31]  the partitions are required to overlap with each other 
while the above result shows that non-overlapping partitions of Rd can also be used for non-ML
parameter learning.

4.4 Relation with Noise Contrastive Estimation [11]

Next  we consider the Type I MKC objective function  Eq.(7)  combined with the strict KL contrac-
tion operator constructed from the binary mixture operation (Lemma 3). In particular  we simplify
g  as:
Eq.(7) using the deﬁnition of Φb
1
π1

KL(cid:0)Φb

(cid:13)(cid:13)Φb

g{qθ}

g{p}

argmin

(cid:1)

θ

(cid:90)

(cid:90)

KL(p(cid:107)qθ) −
1
π1

Rd

(cid:90)

(cid:90)

n−(cid:88)

k=1

= argmin

θ

(π0g(x) + π1p(x)) log (π0g(x) + π1qθ(x)) dx −

Rd

p(x) log qθ(x)dx

= argmax

θ

Rd

p(x) log

π1qθ(x)

π0g(x) + π1qθ(x)

dx +

π0
π1

g(x) log

π0g(x)

π0g(x) + π1qθ(x)

dx.

Rd

When the expectations in the above objective function are approximated with averages over samples
from p(x) and g(x)  {x(1) ···   x(n+)} and {y(1) ···   y(n−)}  the Type I MKC objective function
in this case reduces to

argmax

θ

1
n+

log

π1qθ(x(k))

π0g(x(k)) + π1qθ(x(k))

+

π0
π1

1
n−

log

π0g(y(k))

π0g(y(k)) + π1qθ(y(k))

.

If we set π0 = π1 = 1/2  and treat {x(1) ···   x(n+)} and {y(1) ···   y(n−)} as data of interest and
noise  respectively  the above objective function can also be interpreted as minimizing the Bayesian
classiﬁcation error of data and noise  which is the objective function of noise-contrastive estimation
[11].

6

n+(cid:88)

k=1

4.5 Relation with Score Matching [14]

Next  we consider the strict KL contraction operator  Φc
Tt
conditional distribution with a time decaying variance (i.e.  a Gaussian diffusion process):

  constructed from an isotropic Gaussian

(cid:18)

(cid:19)

 

Tt(y|x) =

1

(2πt)d/2 exp

−(cid:107)y − x(cid:107)2

2t

T0{p} = p for any p ∈ Ωd.
where t ∈ [0 ∞) is the continuous temporal index. Note that we have Φc
If both p(x) and qθ(x) are functions differentiable with regards to x  it is know that the temporal
derivative of the KL contraction of p and qθ under Φc
is in closed form  which is formally stated in
Tt
the following result.
Lemma 5 (Lyu [25]) For any two distributions p  q ∈ Ωd differentiable with regards to x  we have
(10)

Φc
Tt{p}(x)
where ∇x is the gradient operator with regards to x.
Setting t = 0 in Eq.(10)  we obtain a closed form for the Type II MKC objective function  Eq.(8) 
which can be further simpliﬁed [14]  as

(cid:1) = −

KL(cid:0)Φc

(cid:13)(cid:13)Φc

(cid:13)(cid:13)(cid:13)(cid:13)2

Tt{qθ}

Tt{p}

Ttqθ(x)

Ttqθ(x)

d
dt

(cid:90)

dx 

Rd

Φc

Φc

1
2

KL(Φt{p}(cid:107)Φt{qθ})

(cid:12)(cid:12)(cid:12)(cid:12)t=0
(cid:16)
(cid:18)(cid:13)(cid:13)(cid:13)∇x log qθ(x(k))
(cid:13)(cid:13)(cid:13)2

p(x)

n(cid:88)

d
dt

(cid:90)

Rd
1
n

k=1

= argmin

(cid:17)
Rd
(cid:107)∇x log qθ(x)(cid:107)2 + 2(cid:52)x log qθ(x)

θ

+ 2(cid:52)x log qθ(x(k))

 

argmax

θ

= argmin

θ

≈ argmin

θ

Ttp(x)

(cid:13)(cid:13)(cid:13)(cid:13)∇xΦc
Ttp(x) − ∇xΦc
(cid:13)(cid:13)(cid:13)(cid:13)∇xp(x)
(cid:90)
(cid:19)

p(x)

dx

(cid:13)(cid:13)(cid:13)(cid:13)2

p(x) − ∇xqθ(x)

qθ(x)

dx

where {x(1) ···   x(n)} are samples from p(x)  and (cid:52)x is the Laplacian operator with regards to x.
The last step is the objective function of score matching learning [14].

4.6 Relation with Conditional Composite Likelihood [24] and Pseudo-Likelihood [3]

(cid:82)

1
n

m(cid:88)

A{p}(cid:107)Φm

A{q}) = argmaxθ

(cid:80)n
k=1 log q\A|A(x(k)\A|x(k)

Next  we consider the Type I MKC objective function  Eq.(7)  combined with the KL
A   constructed from marginalization. According to Lemma 2  we
contraction operator  Φm
have argminθ KL(p(cid:107)q) − KL(Φm
A{p}(cid:107)Φm
Rd p(x) log q\A|A(x\A|xA)dx ≈
A )  where in the last step  expectation over p(x) is replaced
argmaxθ
with averages over samples from p(x)  {x(1) ···   x(n)}. This corresponds to the objective function
in maximum conditional likelihood [17] or maximum mutual information [2]  which are non-ML
learning objectives for discriminative learning of high dimensional probabilistic data models.
A{q}) = 0 is not sufﬁcient to guaran-
However  Lemma 2 also shows that KL(p(cid:107)q)− KL(Φm
tee p = qθ. Alternatively  we can use the Type III MKC objective function  Eq.(9)  to combine KL
m(cid:88)
contraction operators formed from marginalizations over m different index subsets A1 ···   Am:
Ai |x(k)\Ai
argmin
This is the objective function in conditional composite likelihood [24  30  23  1] (also rediscovered
under the name piecewise learning in [26]).
A special case of conditional composite likelihood is when Ai = \{i}  the resulting marginal-
ization operator  Φm\{i}  is known as the ith singleton marginalization operator. With the d dif-
(cid:80)d
ferent singleton marginalization operators  we can rewrite the objective function as KL(p(cid:107)q) −
1
d
this case  the average KL contraction is zero if and only if p(x) and qθ(x) agree on every singleton
conditional distribution  i.e.  pi|\i(xi|x\i) = qi|\i(xi|x\i) for all i and x. According the Brook’s
Lemma [4]  the latter condition is sufﬁcient for p(x) = qθ(x) (a.e.). Furthermore  when approxi-
mating the expectations with averages over samples from p(x)  we have

R pi(xi)KL(cid:0)pi|\i(xi|x\i)(cid:13)(cid:13)qi|\i(xi|x\i)(cid:1) dxi. Note that in
(cid:82)

(cid:13)(cid:13)(cid:13)Φm\iq
(cid:17)

KL(cid:0)Φm

log qAi|\Ai(x(k)

(cid:13)(cid:13)Φm

KL(p(cid:107)q)−

≈ argmax

(cid:80)d

n(cid:88)

Ai{p}

Ai{q}

i=1 KL

Φm\ip

= 1
d

(cid:16)

1
m

1
m

(cid:1)

1
n

k=1

i=1

i=1

i=1

).

θ

θ

7

d(cid:88)

(cid:16)

KL

Φm\{i}p

(cid:13)(cid:13)(cid:13)Φm\{i}q
(cid:17)

1
d

n(cid:88)

1
d

d(cid:88)

log qi|\i(x(k)

i

|x(k)\i ) 

≈ argmax

θ

1
n

argmin

θ

KL(p(cid:107)q) −

i=1
which is objective function in maximum pseudo-likelihood learning [3  29].

k=1

i=1

4.7 Relation with Marginal Composite Likelihood

(cid:16)

m(cid:88)

i=1

1
m

(cid:90)

m(cid:88)

Rd

i=1

m(cid:88)
n(cid:88)

i=1

1
m

1
n

m(cid:88)

1
m

k=1

i=1

We now consider combining Type III MKC objective function  Eq.(9)  with the KL contraction oper-
ator constructed from the marginal grafting operation. Speciﬁcally  with m different KL contraction
operators constructed from marginal grafting on index subsets A1 ···   Am  using Lemma 2  we can
expand the corresponding Type III minimum KL contraction objective function as:

argmin

θ

KL(p(cid:107)q) −

KL

Φg
p Ai{p}

p Ai{q}

= argmin

θ

KL(pAi(xAi)(cid:107)qAi(xAi))

(cid:13)(cid:13)(cid:13)Φg

(cid:17)

= argmax

θ

1
m

pAi(xAi) log qAi (xAi)dxAi ≈ argmax

θ

log qAi (x(k)
Ai )

The last step  which maximizes the log likelihood of a set of marginal distributions on training data 
corresponds to the objective function of marginal composite likelihood [30]. With m = 1  the
resulting objective is used in the maximum marginal likelihood or Type-II likelihood learning [9].

5 Discussions

In this work  based on an information geometric view of parameter learning  we have described
minimum KL contraction as a unifying principle for non-ML parameter learning  showing that the
objective functions of several existing non-ML parameter learning methods can all be understood as
instantiations of this principle with different KL contraction operators.
There are several directions that we would like to extend the current work. First  the proposed min-
imum KL contraction framework may be further generalized using the more general f-divergence
[8]  of which the KL divergence is a special case. With the more general framework  we hope to
reveal further relations among other types of non-ML learning objectives [16  25  28  27]. Second  in
the current work  we have focused on the idealization of parametric learning as matching probability
distributions. In practice  learning is most often performed on ﬁnite data set with an unknown under-
lying distribution. In such cases  asymptotic properties of the estimation as data volume increases 
such as consistency  become essential. While many non-ML learning methods covered in this work
have been shown to be consistent individually  the uniﬁcation based on the minimum KL contrac-
tion may provide a general condition for such asymptotic properties. Last  understanding different
existing non-ML learning objectives through minimizing KL contraction also provides a principled
approach to devise new non-ML learning methods  by seeking new KL contraction operators  or
new combinations of existing KL contraction operators.

Acknowledgement The author would like to thank Jascha Sohl-Dickstein  Michael DeWeese and
Michael Gutmann for helpful discussions on an early version of this work. This work is supported
by the National Science Foundation under the CAREER Award Grant No. 0953373.

References

[1] Arthur U. Asuncion  Qiang Liu  Alexander T. Ihler  and Padhraic Smyth. Learning with blocks:

Composite likelihood and contrastive divergence. In AISTATS  2010. 2  7

[2] L. Bahl  P. Brown  P. de Souza  and R. Mercer. Maximum mutual information estimation of

hidden markov model parameters for speech recognition. In ICASSP  1986. 1  2  7

[3] J. Besag. Statistical analysis of non-lattice data. The Statistician  24:179–95  1975. 1  2  7  8
[4] D. Brook. On the distinction between the conditional probability and the joint probability
approaches in the speciﬁcation of nearest-neighbor systems. Biometrika  3/4(51):481–483 
1964. 7

8

[5] M. ´A. Carreira-Perpi˜n´an and G. E. Hinton. On contrastive divergence learning. In AISTATS 

2005. 6

[6] T. Cover and J. Thomas. Elements of Information Theory. Wiley-Interscience  2nd edition 

2006. 2  3

[7] D. R. Cox. Partial likelihood. Biometrika  62(2):pp. 269–276  1975. 1  2  6
[8] I. Csisz´ar and P. C. Shields. Information theory and statistics: A tutorial. Foundations and

Trends in Communications and Information Theory  1(4):417–528  2004. 4  8

[9] I.J. Good. The Estimation of Probabilities: An Essay on Modern Bayesian Methods. MIT

Press  1965. 1  2  8

[10] M. Gutmann and J. Hirayama. Bregman divergence as general framework to estimate un-
normalized statistical models. In Conference on Uncertainty in Artiﬁcial Intelligence (UAI) 
Barcelona  Spain  2011. 2

[11] M. Gutmann and A. Hyv¨arinen. Noise-contrastive estimation: A new estimation principle for

unnormalized statistical models. In AISTATS  2010. 1  2  6

[12] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation  14:1771–1800  2002. 1  2  6

[13] P. J. Huber. Projection pursuit. The Annuals of Statistics  13(2):435–475  1985. 3
[14] A. Hyv¨arinen. Estimation of non-normalized statistical models using score matching. Journal

of Machine Learning Research  6:695–709  2005. 1  2  7

[15] A. Hyv¨arinen. Connections between score matching  contrastive divergence  and pseudolike-
lihood for continuous-valued variables. IEEE Transactions on Neural Networks  18(5):1529–
1531  2007. 2

[16] A. Hyv¨arinen. Some extensions of score matching. Computational Statistics & Data Analysis 

51:2499–2512  2007. 8

[17] T. Jebara and A. Pentland. Maximum conditional likelihood via bound maximization and the

CEM algorithm. In NIPS  1998. 1  2  7

[18] J. Laurie Kindermann  Ross; Snell. Markov Random Fields and Their Applications. American

Mathematical Society  1980. 1

[19] E. Kreyszig. Introductory Functional Analysis with Applications. Wiley  1989. 2  4
[20] Stefan L. Lauritzen. Statistical manifolds. In Differential Geometry in Statistical Inference 

pages 163–216  1987. 4

[21] Lucien Le Cam. Maximum likelihood — an introduction. ISI Review  58(2):153–171  1990. 1
[22] Y. LeCun  S. Chopra  R. Hadsell  M. Ranzato  and F. Huang. Tutorial on energy-based learning.

In Predicting Structured Data. MIT Press  2006. 2

[23] P. Liang and M. I Jordan. An asymptotic analysis of generative  discriminative  and pseudo-

likelihood estimators. In International Conference on Machine Learning  2008. 7

[24] B. G Lindsay. Composite likelihood methods. Contemporary Mathematics  80(1):22–39  1988.

1  2  7

[25] S. Lyu. Interpretation and generalization of score matching. In UAI  2009. 7  8
[26] A. McCallum  C. Pal  G. Druck  and X. Wang. Multi-conditional learning: Genera-
tive/discriminative training for clustering and classiﬁcation. In Association for the Advance-
ment of Artiﬁcial Intelligence (AAAI)  2006. 7

[27] M. Pihlaja  M. Gutmann  and A. Hyv¨arinen. A family of computationally efﬁcient and simple

estimators for unnormalized statistical models. In UAI  2010. 8

[28] J. Sohl-Dickstein  P. Battaglino  and M. DeWeese. Minimum probability ﬂow learning.

ICML  2011. 8

In

[29] D. Strauss and M. Ikeda. Pseudolikelihood estimation for social networks. Journal of the

American Statistical Association  85:204–212  1990. 8

[30] C. Varin and P. Vidoni. A note on composite likelihood inference and model selection.

Biometrika  92(3):519–528  2005. 2  7  8

[31] D. Vickrey  C. Lin  and D. Koller. Non-local contrastive objectives. In ICML  2010. 1  2  6

9

,Ali Borji
Laurent Itti
Jeremias Knoblauch
Jack Jewson
Theodoros Damoulas