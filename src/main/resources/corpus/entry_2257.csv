2019,Generalized Block-Diagonal Structure Pursuit: Learning Soft Latent Task Assignment against Negative Transfer,In multi-task learning  a major challenge springs from a notorious issue known as negative transfer  which refers to the phenomenon that sharing the knowledge with dissimilar and hard tasks often results in a worsened performance. To circumvent this issue  we propose a novel multi-task learning method  which simultaneously learns latent task representations and a block-diagonal Latent Task Assignment Matrix (LTAM). Different from most of the previous work  pursuing the Block-Diagonal structure of LTAM (assigning latent tasks to output tasks) alleviates negative transfer via collaboratively grouping latent tasks and output tasks such that inter-group knowledge transfer and sharing is suppressed. This goal is challenging  since 1) our notion of Block-Diagonal Property extends the traditional notion for square matrices where the $i$-th column and the $i$-th column represents the same concept; 2) marginal constraints on rows and columns are also required for avoiding isolated latent/output tasks. Facing such challenges  we propose  a novel regularizer by means of an equivalent spectral condition realizing this generalized block-diagonal property. Practically  we provide a relaxation scheme which improves the flexibility of the model. With the objective function given  we then propose an alternating optimization method  which not only tells how negative transfer is alleviated in our method but also reveals an interesting connection between our method and the optimal transport problem.  Finally  the method is demonstrated on a simulation dataset  three real-world benchmark datasets and further applied to personalized attribute predictions.,Generalized Block-Diagonal Structure Pursuit:
Learning Soft Latent Task Assignment against

Negative Transfer

Zhiyong Yang1 2 Qianqian Xu3

Xiaochun Cao1 2 6

Yangbangyan Jiang1 2

Qingming Huang3 4 5 6∗

1State Key Laboratory of Information Security  Institute of Information Engineering  CAS

2School of Cyber Security  University of Chinese Academy of Sciences

3Key Lab. of Intelligent Information Processing  Institute of Computing Technology  CAS

4School of Computer Science and Tech.  University of Chinese Academy of Sciences

5Key Laboratory of Big Data Mining and Knowledge Management  CAS

6Peng Cheng Laboratory

yangzhiyong@iie.ac.cn  xuqianqian@ict.ac.cn  jiangyangbangyan@iie.ac.cn

caoxiaochun@iie.ac.cn  qmhuang@ucas.ac.cn

Abstract

In multi-task learning  a major challenge springs from a notorious issue known as
negative transfer  which refers to the phenomenon that sharing the knowledge with
dissimilar and hard tasks often results in a worsened performance. To circumvent
this issue  we propose a novel multi-task learning method  which simultaneously
learns latent task representations and a block-diagonal Latent Task Assignment
Matrix (LTAM). Different from most of the previous work  pursuing the Block-
Diagonal structure of LTAM (assigning latent tasks to output tasks) alleviates
negative transfer via punishing inter-group knowledge transfer and sharing. This
goal is challenging since our notion of Block-Diagonal Property extends the tradi-
tional notion for homogeneous and square matrices. In this paper  we propose a
spectral regularizer which is proven to leverage the expected structure. Practically 
we provide a relaxation scheme which improves the ﬂexibility of the model. With
the objective function given  we then propose an alternating optimization method 
which reveals an interesting connection between our method and the optimal trans-
port problem. Finally  the method is demonstrated on a simulation dataset  three
real-world benchmark datasets and further applied to two personalized attribute
learning datasets.

1

Introduction

Multi-Task Learning (MTL) is a learning paradigm whose aim is to leverage useful information
contained in multiple related tasks to help improve the generalization performance of all the tasks
[Caruana  1997]. Nowadays  MTL has emerged as a fundamental building block for a wide range of
applications ranging from scene parsing [Xu et al.  2018]  attribute learning [Cao et al.  2018  Yang
et al.  2019a  2018]  text classiﬁcation [Liu et al.  2017]  sequence labeling [Lin et al.  2018]  to travel
time estimation [Li et al.  2018]  etc.
The fundamental belief of MTL lies in that sharing knowledge among multiple tasks often results
in an improvement in generalization performance  which is especially of great signiﬁcance in the

∗Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

presence of insufﬁcient data annotations [Heskes  1998]. Based on the belief  a great number of
studies have been carried out to explore the problem of how to share valuable knowledge across
different tasks. The early studies of MTL (e.g.[Argyriou et al.  2008a]) hold that all the tasks share
common and sparse features. However  [Kang et al.  2011] later points out that if not all the tasks
are indeed related  then sharing common features with dissimilar and hard tasks often results in
performance degradation  which is termed as negative transfer.
To address this issue  recent studies in the odyssey against negative transfer fall in two major
directions. One line of the researches leverages the grouping effect based on the latent-task-agnostic
idea which develops structural regularizers where only the original per-task parameters are utilized.
[Kang et al.  2011  Kshirsagar et al.  2017] directly formulate the tasking grouping as a mixed integer
programming (or a relaxation [Frecon et al.  2018])  which simultaneously learns the group index
and the model parameters. [Argyriou et al.  2008b  Zhou et al.  2011a  Jacob et al.  2009  Lee et al. 
2016  Liu and Pan  2017  McDonald et al.  2014] leverage the tasking grouping via enforcing a
speciﬁc structure  hopefully block-diagonal  on the task correlation matrix. As an extension of this
formulation [Zhong and Kwok  2012] resorts to feature-speciﬁc task clustering. The other line of
researches formulates the MTL based on the latent task  where the model parameter is represented as
a linear combination of latent task basis. [Kumar and III  2012] gives an early trial of this formulation
in search of a more ﬂexible MTL model. Similarly  in the work of [Maurer et al.  2013]  a sparse
coding model is proposed for MTL  where the dictionary is set as the latent task basis and the code is
set as the linear combination coefﬁcients of such basis. Recently  [Lee et al.  2018] also provides an
asymmetric learning framework based on the latent task representation where transferring knowledge
from unreliable tasks to reliable tasks is explicitly punished.
The two aforementioned directions  i.e.  learning grouped model structure and latent task represen-
tation provide complementary functions in a sense that the former one avoids inter-group negative
transfer  while the latter one focuses on learning a more ﬂexible model. However  the related studies
on how to bridge the efforts of these two directions are sparse. To the best of our knowledge  the only
two studies along this direction are [Crammer and Mansour  2012  Barzilai and Crammer  2015].
However  both studies adopt a strong assumption that each group of tasks is only assigned with one
latent task basis.
To leverage a ﬂexible grouping structure with latent task representations  we should allow each task
cluster to have multiple latent tasks. Motivated by this  we study the structural learning problem of
how to learn a block-diagonal Latent Task Assignment Matrix (LATM). With the block-diagonal
structure  tasks within each group share a subset (not necessarily one) of the latent task basis. Since
LATM is not a squared matrix and marginal constraints are also necessary to avoid isolated tasks/latent
tasks  our notion of block-diagonality generalizes the one adopted in the self-expression scenario
[Lu et al.  2019  Lee et al.  2016  Liu and Pan  2017]   which makes traditional structural regularizers
not available to solve our problem. Our ﬁrst contribution then comes as an equivalent spectral
condition that realizes our pursuit of the generalized block-diagonal structure. Then we propose a
new MTL method named Generalized Block-Diagonal Structural Pursuit (GBDSP)  which utilizes
the spectral condition as a novel regularizer with a relaxation scheme. In our optimization method 
the intermediate solution produced provides new insights into how negative transfer is alleviated in
our model. Theoretical studies show how the proposed regularizer guarantees the expected structure.
Finally  empirical studies demonstrate the effectiveness of the proposed method.

2 Generalized Block-Diagonal Structure Pursuit

Notations The notations adopted in this paper are enumerated as follows. Sm denotes the set of
all symmetric matrices in Rm×m. The eigenvalues of a symmetric matrix A ∈ Sm are denoted as
λ1(A) ···   λm(A) such that λ1(A) ≥ λ2(A) ≥ ··· ≥ λm(A). (cid:104)· ·(cid:105) denotes the inner product for
two matrices or two vectors. Given two vectors a and b  a ⊕ b denotes the outter sum a1(cid:62) + 1b(cid:62).
Given two matrices A and B  A⊕B denotes the direct sum of two matrices  i.e.  A⊕B =
 
0 B
and we say A (cid:23) B  if A − B is positive semi-deﬁnite. For distributions  N (µ  σ2) denotes the
normal distribution. Pm denotes the set of all permutation matrices in Rm×m. For two matrices
A and B having the same size  d(A  B) = (cid:107)A − B(cid:107)2
F . Given an event A  δ(A) denotes the
corresponding indicator function. Moreover  let us note two notations in our paper that are prone

(cid:20)A 0

(cid:21)

2

to be confused. k denotes the dimension of the latent task representation. K(K ≤ k) denotes the
number of given groups.

2.1 Model Formulation

j

j=1 (cid:96)(Y (i)

j

  ˆY (i)

j

1  ···   X (i)

ni ](cid:62)  where X (i)

1  ···   Y (i)

ni ](cid:62) ∈ Rni×1  where Y (i)

for the i-th task is deﬁned as J (i) =(cid:80)ni

as:(cid:8)(X (1)  Y (1)) ···   (X (T )  Y (T ))(cid:9). Here X (i) = [X (i)

Before entering our new method  we ﬁrst provide a brief introduction of the multi-task learning
setting we adopted in this paper. Here we adopt the latent task representation framework proposed
in [Kumar and III  2012]. Given T tasks to be learned simultaneously  we denote the training data
j ∈ Rd×1 is
the input feature for the j-th sample of the i-th task  ni denotes the number of instances and d
represents the feature dimension. Similarly Y (i) = [Y (i)
is the
corresponding response for the j-th sample of the i-th task. Following the standard multi-task learning
paradigm  we learn a model ˆY (i)(x) = W (i)(cid:62)
x to estimate the output response for each task i. Here
we call W = [W (1) ···   W (T )] ∈ Rd×T the per-task parameter matrix. Furthermore  to model
the relationship among the tasks  we assume that the per-task parameters lie in a low dimensional
subspace. To this end  we introduce a set of latent task basis L ∈ Rd×k  where k < T . For each given
task i  its parameter is then represented as a linear combination of the basis by letting W (i) = LS(i) 
where S(i) ∈ Rk×1 is the combination coefﬁcients. Given a loss function (cid:96)(y  ˆy)  the empirical risk
(cid:80)T
). Given proper regularizers Ω(L)  Ω(S) 
i=1 J (i) + α1· Ω(L) + α2· Ω(S).
[Kumar and III  2012] learns L  S from the problem argminL S
In this paper  we adopt the F -norm penalty for L  i.e.  we set Ω(L) = (cid:107)L(cid:107)2
F . And we seek new
solutions against negative transfer from Ω(S). In this setting  we must deal with both the latent task
representations and the true tasks. To differentiate the two  we refer the former ones to latent tasks
(l1 ···   lk) and the latter ones to output tasks (o1 ···   oT ).
With the latent task formulation W = LS  S then captures the importance of the latent tasks to the
output tasks. In a natural sense  we regard Si j as P(l = i|o = j)  namely the possibility of choosing
li to represent oj. In this probabilistic view  LS(i) now becomes El|o=i(L)  i.e.  the expectation of
the latent tasks representations assigned to task oi. We then call S the Latent Task Assignment Matrix
(LTAM)  since the conditional possibility could be considered as a soft assignment score. Before
developing a proper regularizer  we must ﬁrst answer the question that can we choose S arbitrarily?
Unfortunately  we will immediately see that the answer is negative. Let us denote S‡ ∈ Rk×T by
i j = P(l = i  o = j)  the joint distribution of l and o. Note that S‡1 and S‡(cid:62)
‡
1 are marginal
S
distributions on l and o  we come to two extreme cases that must be ruled out from consideration.
If (S‡1T )i = 0 then li becomes an isolated latent task which is irrelevant with all the output task.
Similarly  if (S‡(cid:62)
1k)j = 0 then oj becomes isolated with no latent tasks assigned to it. To remove
extreme cases of such kinds  we then pose normalization constraints on S‡ for each row and column
in the form: S‡1T = a > 0k  S‡(cid:62)
1k = b > 0T . To maintain fairness  we do not expect to introduce
extra bias from the choice of marginal distribution. Such a spirit guides us to put out a = 1k/k 
b = 1T /T . Moreover  this also simpliﬁes the relation between S and S‡ with S = T S‡. From all
above  we adopt the transportation polytopes Π(a  b) =
as
the feasible set for the parameter S‡.
So far we have known that S must satisfy the marginal constraints to make the solution non-trivial.
Now let us step further to seek out what else we should pose on S to suppress inter-group transfer.
In this paper  we adopt a basic assumption that the latent tasks and output tasks could be clustered
into K independent groups. In order to avoid negative transfer  we hope the possibility to assign
li to oj is nonzero if and only if (i  j) belongs to the same group. This leads to a block-diagonal
structure of S‡ up to row and column permutations. Next  we give a formal deﬁnition of the desired
block-diagonal structure with S‡ ∈ Π(a  b)  based on the following simple idea. If the columns and
rows of S‡ could be partitioned into K groups  S‡ could then be expressed as a direct sum of K
blocks up to proper column and row permutations. The maximum of such K then implies the number
of groups in the matrix.1 This motivates the following deﬁnition of the grouping structure  which is
termed as the Generalized K Block Diagonal Property (GKBDP) in our paper.

: S‡1 = a  S‡(cid:62)

S‡ ∈ Rk×T

(cid:111)

1 = b

(cid:110)

+

1If K is not the maximum of such numbers  we can always ﬁnd out more disjoint blocks.

3

such that: PrS‡Pc = (cid:76)K

Deﬁnition 1 (Generalized K Block Diagonal Property). Given a matrix S‡ ∈ Π(a  b)  if there
exists a permutation matrix over rows Pr ∈ Pk and a permutation matrix over columns Pc ∈ PT
i Ti = T  
then we deﬁne χS‡(Pr  Pc) = K. Moreover  we say S‡ is Generalized K Block Diagonal if
χS‡ =

ˆS(i) where ˆS(i) (cid:54)= 0  ˆS(i) ∈ Rki×Ti (cid:80)

i ki = k  (cid:80)

χS‡(Pr  Pc) = K.

max

i=1

Pr∈Pd Pc∈PT

c

(cid:21)

S‡(cid:62)

S‡
0

(cid:20) 0

Note that GKBDP extends the notion of block-diagonal property deployed in self-expression [Lee
et al.  2016  Liu and Pan  2017  Lu et al.  2019  Jiang et al.  2018  2019  Yang et al.  2019b] which is
only available for square matrices. Furthermore  the traditional self-expression-based block-diagonal
property requires Pr = P (cid:62)
[Lu et al.  2019]  i.e.  a simultaneous permutation on columns and rows
(the i-th row and the i-th column represent the same object) . However  this is not the case in our
notion of GKBDP since the columns and rows here represent heterogeneous concepts (the i-th row
is for li  i-th column is for oi). Moreover we also consider the marginal constraints S‡1 = a  and
S‡(cid:62)
1 = b to avoid isolated l or o. Based on the aforementioned facts  traditional regularization
schemes are thus not directly applicable to leverage the GKBDP.
Next  we derive an equivalent condition for GKBDP  which directly leads to the formulation of
our method. First  we deﬁne an auxiliary bipartite graph Gl∪o = (Vl∪o El∪o  Al∪o). The vertices
of Gl∪o include all l and o. Denote Vo as the set of all output tasks and Vl as the set of all latent
tasks  the vertex set Vl∪o is then deﬁned as Vl∪o = Vl ∪ V o. To deﬁne the edge set  we ﬁrst deﬁne
  where the (i  j) ∈ El∪o if and only if
an afﬁnity matrix Al∪o in the form Al∪o =
Al∪oij (cid:54)= 0. Then the well-known graph Laplacian follows as ∆(S‡) = diag(Al∪o1) − Al∪o.
With the deﬁnition of Gl∪o  we could derive the following theorem.
Theorem 1. If S‡ ∈ Π(a  b)  χS = K holds if and only if dim(N ull(∆(S‡))) = K  i.e  the 0
eigenvalue of ∆(S‡) has multiplicity K. Moreover  denote A(i) as the set of latent and output tasks
belonging to the i-th block of S  the eigenspace of 0 is spanned by ιA(1)  ιA(2) ···   ιA(K)  where
ιA(i) ∈ R(k+T )×1  [ιA(i) ]j = 1 if j ∈ A(i)  otherwise [ιA(i)]j = 0.
The proof can be found in Appendix B.1. With the theorem  we can now step further to seek a suitable
regularizer realizing GKBDP. It becomes straightforward that leveraging GKBDP requires the sum of
bottom K eigenvalues to be as small as possible. Let N = k + T denote the total number of nodes in
N−K+1 λi(∆(S‡)) with the constraint S‡ ∈ Π(a  b). Following
the variational characterization of eigenvalues  we could reformulate eigenvalue calculation as an
optimization problem with the following theorem.
Theorem 2. Let M = {U : U ∈ SN   I (cid:23) U (cid:23) 0  tr(U ) = K}  then ∀A ∈ SN :
K   where

Gl∪o  we then need to minimize(cid:80)N
(cid:80)N
N−K+1 λi(A) = minU∈M (cid:104)A  U(cid:105)   with an optimal value reached at U = VKV (cid:62)
Thm.2 provides a regularizer as Ω(S‡) = inf(cid:8)(cid:10)∆(S‡)  U(cid:11) : U ∈ M(cid:9). Denote (cid:101)J =(cid:80)T

The theorem slightly extends the results in [Overton and Womersley  1992b]  which considers top-K
eigenvalues. A proof for the theorem could be found in Appendix B.2. Back to our practical problem 
i=1 J (i) 
Ω1 = α1 · ||L||2
framework:

F /2  Ω2 = α3 ·(cid:10)∆(S‡)  U(cid:11)  we then reach an MTL model based on the latent task

VK represents the eigenvectors of the smallest K eigenvalues of A.

(cid:101)J + Ω1 + Ω2  s.t. S‡ ∈ Π(a  b)  U ∈ M  S = T S‡.

min

L S S‡ U

(Obj0)

This model exactly realizes GKBDP. However  this exact model is impractical in the following
sense. First  it is hard to solve (Obj0) directly since multiple constraints are wrapped together on S.
Moreover  it encourages a strict structural control of S  prohibiting overlapped subspaces even when
it beneﬁts the performance. These problems lead us to a relaxed implementation of (Obj0)   bringing
us possibilities to embrace a practical and a more ﬂexible solution.
To avoid directly controlling the structure of S  we relax the constraint S = T S‡ as a distance
penalty2 Ω3 = α2 · d(S  T S‡)/2. This brings us to the ﬁnal optimization problem:

2Let us note that  in the rest of the paper  S = T S‡ no longer holds

4

J = (cid:101)J + Ω1 + Ω2 + Ω3  s.t. S‡ ∈ Π(a  b)  U ∈ M.

min

L S S‡ U

(Obj)

(cid:104)
(cid:104)

where

The relaxation scheme improves the ﬂexibility of our model via leveraging a partial structural control 
which decomposes S into a structural component T S‡ and a dense component as the residual
S − T S‡. The new dense component allows S to slightly (controlled by the magnitude of α2) violate
the structural constraint  in search of a better performance.
Alternatively  we could also interpret (Obj) as a way to leverage hierarchical priors on the model.
This is speciﬁed by the generative model in the following:

(cid:17)

(cid:19)

(cid:104)
(cid:104)

X (i)LS(i)  I

 

T S‡(i) 

vec(L) | α1

S(i) | T S‡(i)  α2

Y (i) | X (i)  L  S(i)(cid:105) ∼ N(cid:16)
(cid:18)
(cid:105) ∼ N

(cid:105) ∼ N
(cid:105) ∼ g  U ∼ h 
g ∝ exp(cid:0)−α3 ·(cid:10)∆(S‡)  U(cid:11)(cid:1) · δ(cid:0)S‡ ∈ Π(a  b)(cid:1)   h ∝ δ(cid:0)U ∈ M(cid:1).
J = − log(cid:0)P(L  S  S‡  U |X  Y   α1  α2  α3)(cid:1) + const.

S‡ | U   α3

1
α2

I

 

Here g speciﬁes an exponential distribution restricted on the set Π(a  b)  and h speciﬁes a uniform
distribution on the set M. With this process  our objective function is equivalent to a Maximum A
Posterior (MAP) formulation in the following sense:

(cid:18)

(cid:19)

0 

I

 

1
α1

This fact gives us an alternative perspective on the relationship between S and S‡. With the relaxation
scheme  the constraints are moved to the mean of the prior distribution of S. This provides S with a
possibility to activate the overlapping off-diagonal block elements with a moderate variance α2.

2.2 Optimization
It is easy to see that J in (Obj) is not a jointly convex function. But fortunately  it is easy to show
that the four subproblems with respective to L  S  S‡  U are all convex. Instead of directly solving
the overall non-convex problem  this fact motivates us to adopt an alternating optimization scheme
where only one of the four parameters is updated each time and the others are ﬁxed as constants. Now
we elaborate the four subproblems  respectively.
L and S subroutine: Theoretically  both subroutines solve a strongly convex unconstrained quadratic
programming and enjoy a closed-form solution. However  calculating the closed form of the L
subproblem suffers from a heavy computational complexity. Instead of adopting the closed form
directly for the L subproblem  we adopt a gradient-based optimizer in our paper. Please see Appendix
C for more details.
U subroutine: According to Thm.2  U could be solved from: U = VKV (cid:62)
K   where VK denotes
eigenvectors associated with the smallest K eigenvalues of ∆(S‡). Denote VK = [f1 ···   fk+T ](cid:62) 
according to Thm.1  when χS‡ = K  up to some orthogonal transformation  fi ∈ RK×1 becomes an
indicator vector with only one non-zero entry where [fi]j = 1 only if the corresponding latent/output
task i is in group ιA(j). In this way  we see that fi is a strong group indicator. Consequently  we
name fi as the embedding vector for the latent (output) task.
S‡ subroutine: With U updated with U = VKV (cid:62)
this subproblem:
Proposition 1. The S‡ subproblem could be reformulated as:

K   the following proposition shows a way to solve

min

S‡∈Π(a b)

ϑ
2

(cid:107)S‡ − ¯S(cid:107)2

F +(cid:10)D  S‡(cid:11)  

(P rimal)

where ϑ =

α2 · T 2

α3

  ¯S =

S
T

and Dij = (cid:107)fi − fk+j(cid:107)2.

5

The proof could be found in Appendix A.1. From Prop.1  we see that the subproblem recovers a
smoothed Optimal Transport (OT) [Peyré et al.  2019] problem. More speciﬁcally  the calculation of
the Wasserstein-2 distance between l and o  based on the spectral embedding.
Similar to the recent results [Blondel et al.  2018  Peyré et al.  2019]  we can show that the regularized
OT problem has a close connection with the original OT problem.
Proposition 2. The following properties hold true:
(a) Denote S‡

ϑ0 as the solution of problem (P rimal) when ϑ = ϑ0. Then we have :

(cid:40)

(cid:10)D  S‡(cid:11)(cid:41)

.

S‡

ϑ0

ϑ0→0→ argmin

S‡

d(S‡  ¯S) : S‡ ∈ argmin
S‡∈Π(a b)

(b) Denote

JOT = min

S‡∈Π(a b)

(cid:10)D  S‡(cid:11)   JREG = min

S‡∈Π(a b)

(ϑ/2) · d(S  ¯S) +(cid:10)D  S‡(cid:11)  

we have:

2 (cid:107)b(cid:107)2

ϑ · max(cid:8)d( ¯S1 − a)/T  d( ¯S(cid:62)1  b)/k(cid:9) ≤ JREG − JOT ≤ ϑ · (min{(cid:107)a(cid:107)2

OT problem with a small ϑ. Moreover  if the regularizer (cid:10)∆(S‡)  U(cid:11) is sufﬁciently small  f

2} + (cid:107) ¯S(cid:107)2
F ).
The proof can be found in Appendix A.2. Prop.2-(a) shows that asymptotically  when ϑ → 0  the
solution of the regularized OT problem approaches a speciﬁc solution of the original OT problem.
More speciﬁcally  it will pick out an optimal coupling from the OT solution set with the smallest
regularization term d(S‡  ¯S). From a non-asymptotic perspective  Prop.2-(b) shows how fast this
approximation will take place. Consequently  we will get a reasonable approximation of the original
approaches the grouping indicator of a K-connected bipartite graph. At the same time  Dij  the
distance between the embedding vectors  approaches zero when li and oj belong to the same group
indicated by f. Under this circumstance  the transportation cost Di j is small only if li and oj
belong to the same group. By contrast  the inter-group negative transfer is suppressed with a large
transportation cost. Moreover  with α2 → +∞  LS(i) → El|o=i(L). This indicates that the
conditional expectation also embraces the idea of barycenter projection mapping [Seguy et al.  2018]
in the sense El|o=i(L) = argminz
El|o=i(d(L(i)  z)). Under this condition  the task parameter of
oi becomes a barycenter of the latent task embeddings. Finally  we show that this subproblem could
be solved efﬁciently from the dual formulation.
Proposition 3. The dual problem of (P rimal) could be solved from:

(h(cid:63)  g(cid:63)) = argmin

h  g

1
2ϑ

and the primal solution is given by S‡(cid:63)

=

·(cid:13)(cid:13)(h ⊕ g − D + ϑ ¯S)+
(cid:20) h(cid:63) ⊕ g(cid:63) − D

(cid:13)(cid:13)2

ϑ

+ ¯S

.

+

F − (cid:104)h  a(cid:105) − (cid:104)g  b(cid:105)  
(cid:21)

(Dual)

The proof can be found in Appendix A.3. From Prop.3  we can recover the primal solution from
(Dual)  which only involves O(k + T ) parameters instead of O(kT ). In this spirit  we ﬁrst solve
h(cid:63)  g(cid:63) from (Dual) with the L-BFGS [Zhu et al.  1997] method as the optimizer  and then recover
S‡(cid:63) from the dual parameters.
Summary Our optimization procedure then alternatively solves the four subproblems until a conver-
gence condition is reached  with irrelevant variables ﬁxed as their latest version. Moreover  since
all the subproblems are convex  it is easy to see that the iteration over subproblems then keeps the
overall loss function non-increasing.

2.3 Theoretical Analysis

In this section  we present theoretical analysis shedding light on how the hyperparameters α1  α2  α3
affect our proposed model. Let us start with deﬁning a proper hypothesis space H that covers the
solution returned by the optimization algorithm. Recall that (Obj) is non-increasing during the

6

F ≤ 2J0/α1  d(S  T S‡) ≤ 2J0/α2 (cid:10)∆(S‡)  U(cid:11) ≤ 2J0/α3  for all outputs from the

optimization procedure. This means that if we choose a feasible candidate L0  S0  S‡
0  U0 as the
initialization of the algorithm  denote by J0 the corresponding objective function value  we will
have (cid:107)L(cid:107)2
optimization algorithm. This naturally deﬁnes a hypothesis class H = H(L  S  S‡  U ):

(cid:26)(cid:110) ˆY (i)(X (t)
(cid:27)
d(S  T S‡) ≤ ξ2  (cid:10)∆(S‡)  U(cid:11) ≤ ξ3  S‡ ∈ Π(a  b)  U ∈ M

i ) = (LS(i))(cid:62)X (t)

F ≤ ξ1 

: (cid:107)L(cid:107)2

(cid:111)

ti

i

 

H(L  S  S‡  U ) =

as: ˆR(L  S) =(cid:80)T

i=1 J (i)/T R(L  S) =(cid:80)T

where ξ1 = 2J0/α1  ξ2 = 2J0/α2  ξ3 = 2J0/α3. Now we are ready to represent the theoretical
results based on H.
As the ﬁrst step  we explore how well a model learned from H generalizes to the overall population.
The empirical risk ˆR(L  S) over the observed dataset and the task-averaged risk R(L  S) are given
  Y (i)
are sampled from µi. We then bound the term ∆ = R(L  S) − ˆR(L  S). Following the spirit of
[Maurer et al.  2016]  we have the following bound for the hypothesis space:
Theorem 3. Suppose that n1 = n2 ··· = nT = n  the loss function (cid:96)(y ·) : ˆy (cid:55)→ [0  M ]  (cid:96)(y ·) is
M φ-Lipschitz continuous  and ∀(L  S) are chosen from H  the following bound holds with possibility
at least 1 − δ :

(cid:2)J (i)(cid:3)/T  where the training data X (i)

Eµi

i=1

j

j

(cid:32)

≤ κ1φℵ

ξ1k(cid:12)(cid:12)(cid:12)(cid:12)COV (X)(cid:12)(cid:12)(cid:12)(cid:12)1

(cid:33)1/2
deﬁned as (cid:104)COV (X)u  v(cid:105) = (1/nT ) ·(cid:80)

∆
M
where κ1 and κ2 are two universal constants  ℵ =

+ 2κ2φℵ ·

(cid:68)

nT

(cid:32)

√

u  X (t)

i

ti

ξ1

(cid:12)(cid:12)(cid:12)(cid:12)COV (X)(cid:12)(cid:12)(cid:12)(cid:12)∞
(cid:69)
(cid:69)(cid:68)

n

X (t)

i

  v

.

(cid:33)1/2

(cid:32)

+

9 ln (2/δ)

2nT

(cid:33)1/2

 

ξ2 + 1. COV (X) is the covariance operator

The proof can be found in Appendix B.3. With the sample complexity given  we narrow our focus to
the problem that how ξ2  ξ3 beneﬁt the hypothesis space. The following theorem shows that ξ2  ξ3
control the spectral properties of S and S‡.
Theorem 4 (Spectral Properties of S). Let k ≤ T   deﬁne the SVD of S as S = P ΛQ(cid:62)  where P =
[p1 ···   pk]  Q = [q1 ···   qk] are left and right singular vectors respectively  Λ = diag(σi (S))
with σ1 (S) ≥ σ2 (S)··· ≥ σk (S) ≥ 0. The following properties hold for all S ∈ H:
(cid:80)N
(a) The bottom K eigenvalues of
i=N−K+1 λi(∆(S)) ≤ T ξ3 +

√
the graph Laplacian induced by S is bounded by :
T )  where ∆(S) is obtained from

√
ξ2K(

k +

2 +

√

√

replacing S‡ in ∆(S‡) with S .

(b) Deﬁne M(P ) = Span{p1 ··· pK} and M(Q) = Span{q1 ··· qK}  if ξ3 +

and rank(S) ≥ K  then we have:

σ1 (S)
σK (S)

=

max

x y∈M(P )

(cid:107)x(cid:107)2=1 (cid:107)y(cid:107)2=1

(cid:107)Sx(cid:107)2
(cid:107)Sy(cid:107)2

=

max

x y∈M(Q)

(cid:107)x(cid:107)2=1 (cid:107)y(cid:107)2=1

(cid:107)S(cid:62)x(cid:107)2
(cid:107)S(cid:62)y(cid:107)2

≤ 1
k

·

√

ξ2/T < 1/T
√
1 − T ξ3 − √

T + k

ξ2

ξ2

.

The proof can be found in Appendix B.4. Thm.4.(a) implies that  with a small ξ2 and ξ3  the grouping
(cid:80)N
structure of S could also be controlled  even though the structural penalty is not directly exhibited
on S. More speciﬁcally  if we pick ξ3 = O(T −3/2) and ξ2 = O(1/T 2)  we can reach a small
i=N−K+1 λi(∆(S)) with O(T −1/2) if T >> k. Thm.4.(b) states that shrinking ξ2  ξ3 helps to
remain a smaller numerical perturbation of Sx (S(cid:62)x) over the principle subspaces  i.e.  the subspaces
spanned by principle left/right singular vectors. Besides numerical beneﬁts  the following theorem
shows that ξ3 guarantees good structure recovery in a non-asymptotic manner.
Theorem 5. Assume that k ≤ T   and that the ground-truth grouping is indicated by G = {(i  j) :
1 ≤ i ≤ k  1 ≤ j ≤ T  li and oi are in the same group} with K disjoint groups. Moreover  for a
matrix W   denote supp(W ) as {(i  j) : Wi j (cid:54)= 0}. For all S‡ obtained from the space H such

7

that λK+1(∆(S‡)) > λK(∆(S‡)) > 0 and inf S(cid:63)∈Π(a b) Supp(S(cid:63))=G ||∆(S‡) − ∆(S(cid:63))||F ≤   we
have:

(cid:107)S‡suppc(cid:107)1 ≤ 1
2

k + 6

T · 
λK+1(∆(S‡))

ξ3 +

· ξ3 +

√

4

kT λK+1(∆(S‡))

(cid:113) 2

·(cid:16)

(cid:17) ≤ 1

2

S‡suppc
diagonal structure in the sense that S‡suppc
otherwise.

denotes the projection of S‡ onto the complement of the support set of the expected block-

i j = 0 if i and j belong to the same group  S‡suppc

i j = S‡

i j

The proof can be found in Appendix B.5. Under the assumptions of Thm.5  a smaller ξ3 embraces a
better recovery of the true block-diagonal structure. More speciﬁcally  it shrinks (cid:107)S‡suppc(cid:107)1  i.e.  the
overall magnitude of the elements that violate the true grouping structure. Picking ξ3 = O(T −1/4) and
T )  if λK+1(∆(S‡)) = O(1/k)  under the worst case  we have (cid:107)S‡suppc(cid:107)1 = O(T −1/4).
k = O(

√

3 Empirical Study

3.1 Experiment Settings

For all the experiments  hyper-parameters are tuned based on the training and validation set  and the
results on the test set are recorded. The experiments are done with 5 repetitions for each involved
algorithm. Except for the Simulated Dataset  the train/valid/test ratio is ﬁxed as 70%/15%/15%.
For regression datasets (Simulated dataset and School)  we adopt the overall rmse on all samples
as the evaluation metric. For classiﬁcation datasets  we adopt the average of task-wise AUC as
the evaluation metric. For regression problem  J (·) in GBDSP is chosen as the square-loss. For
classiﬁcation problem  J (·) in GBDSP is chosen as the squared surrogate loss for AUC [Gao et al. 
2016]. All the experiments are run with MATLAB 2016b and a Ubuntu 16.04 system. In the
next subsection  we show our experimental results on a simulated dataset. More experiments for
real-world datasets could be found in Appendix D.

3.2 Simulated Dataset

L2 ∈ R300×20  L3 ∈ R300×10  L4 ∈ R300×30  L5 ∈ R300×20  and that S = (cid:76)5

To test the effectiveness of GBDSP we generate a simple simulated annotation dataset with T = 150
simulated tasks  where the dataset is produced according to the assumption in our model. For
each task  500 samples are generated with d = 300 features such that X (i) ∈ R500×300 and
k ∼ N (0  I80). Speciﬁcally  we generate latent task representations with k = 100 basis. This
x(i)
yields an L ∈ R300×100 and an S ∈ R100×150. To leverage the group structure  we split the
latent tasks and output tasks into 5 groups  in a way that L = [L1 ···   L5]  where L1 ∈ R300×20 
i=1 Si where
S1 ∈ R20×30  S2 ∈ R20×30  S3 ∈ R10×15  S4 ∈ R30×45  S5 ∈ R20×30. For the i-th group  the
elements in Li is sampled i.i.d from N (mi  0.01)  where mi = 5i. Si is generated as Si = si1 
i.e.  every element in Si shares the same value. Moreover  si is calculated from the constraint that
S ∈ Π(a  b). Then the task parameter is generated as W = LS. For each task  the outputs are
generated as Y (i) = X (i)(W (i) + (i))  where (i) ∈ R200×1  and (i) ∼ N (0  0.12I500). Based on
this setting  we compare GBDSP with GOMTL in the simulation dataset to see how the block-diagonal
structure beneﬁts the latent task representation based MTL.
First  we show how well could GOMTL and GBDSP recover the block-diagonal structure. We
compare S obtained from GOMTL and S‡ obtained from GBDSP  with the initial value of ˆL set as
ˆL = L + N (0  0.05I). As shown in Fig.1(a)- Fig.1(c)  GBDSP recovers a much clearer structure
than GOMTL. Moreover  we provide a closer look at the embedding vectors in GBDSP. To do this 
we visualize the spectral embeddings f in a 3d space with t-SNE [Maaten and Hinton  2008]  which
is shown in Fig.2(c). In this ﬁgure  the points with different colors represent latent/output tasks in
different groups. Clearly  we see that the clusters are well-separated in the spectral embedding space 
which again veriﬁes the grouping power of the proposed method.
Next  we check whether GBDSP could improve the performance with a structural LATM.
In Fig.2  we plot the performance of GOMTL and GBDSP with different training set ratio

8

(a)

(b)

(c)

Figure 1: Visualizations over the Simulated Dataset. (a)-(c) provide structural comparisons over the
LATM: (a) shows The true LATM; (b) shows the LATM recovered by GOMTL (c) shows the LATM
recovered by GBDSP.

(a)

(b)

(c)

Figure 2: (a-b) Performance curve with different training data ratio: (a) GOMTL (b) GBDSP(c)
shows the spectral group embedding of o and l in GBDSP.

(0.35  0.45  0.55  0.65  0.75). The corresponding results show that GBDSP consistently provides
a better performance with a smaller variance  which supports the idea that learning a block-diagonal
LATM structure improves the performance.

4 Conclusion

To simultaneously leverage a latent task representation and alleviate the inter-group negative transfer
issue  we develop a novel MTL method GBDSP  which simultaneously separates the latent tasks and
out tasks into a given number of groups. Moreover  we adopt an optimization method to solve the
model parameters  which gives an alternative update scheme for our multi-convex objective function.
The solution produced by the optimization method shows a close connection between our method
and the optimal transport problem  which brings new insight into how negative transfer could be
prevented across latent tasks and output tasks. Furthermore  we provide theoretical analysis on the
spectral properties of the model parameters. Empirical results on the simulated dataset show that
GBDSP could roughly recover the correct grouping structure with good performance  and results
on the real-world datasets further verify the effectiveness of our proposed model on the problem of
personalized attribute prediction.

5 Acknowledgements

This work was supported in part by National Natural Science Foundation of China: 61620106009 
U1636214  61836002  61861166002  61672514 and 61976202  in part by National Basic Research
Program of China (973 Program): 2015CB351800  in part by Key Research Program of Frontier
Sciences  CAS: QYZDJ-SSW-SYS013  in part by the Strategic Priority Research Program of Chinese
Academy of Sciences  Grant No. XDB28000000  in part by the Science and Technology Development
Fund of Macau SAR (File no. 0001/2018/AFJ) Joint Scientiﬁc Research Project  in part by Beijing
Natural Science Foundation (No. 61971016  L182057  and 4182079)  in part by Peng Cheng

9

0.360.370.380.390.4040506070ratiormse0.10250.10500.10750.11000.112540506070ratiormseLaboratory Project of Guangdong Province PCL2018KP004  and in part by Youth Innovation
Promotion Association CAS.

References
F. Alizadeh. Interior point methods in semideﬁnite programming with applications to combinatorial

optimization. SIAM journal on Optimization  5(1):13–51  1995.

G. P. andJames Hays. SUN attribute database: Discovering  annotating  and recognizing scene

attributes. In CVPR  pages 2751–2758  2012.

A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Machine Learning  73

(3):243–272  2008a.

A. Argyriou  M. Pontil  Y. Ying  and C. A. Micchelli. A spectral regularization framework for

multi-task structure learning. In Neurips  pages 25–32  2008b.

A. Barzilai and K. Crammer. Convex multi-task learning by clustering. In Artiﬁcial Intelligence and

Statistics  pages 65–73  2015.

M. Blondel  V. Seguy  and A. Rolet. Smooth and sparse optimal transport. In AISTATS  pages

880–889  2018.

J. Cao  Y. Li  and Z. Zhang. Partially shared multi-task convolutional neural network with local

constraint for face attribute learning. In CVPR  pages 4290–4299  2018.

R. Caruana. Multitask learnings. Machine Learning  28(1):41–75  1997.

K. Crammer and Y. Mansour. Learning multiple tasks using shared hypotheses. In Neurips  pages

1475–1483  2012.

J. Frecon  S. Salzo  and M. Pontil. Bilevel learning of the group lasso structure. In Neurips  pages

8301–8311  2018.

W. Gao  L. Wang  R. Jin  S. Zhu  and Z. Zhou. One-pass AUC optimization. AI  236:1–29  2016.

G. H. Golub and C. F. Van Loan. Matrix computations  volume 3. JHU press  2012.

L. Han and Y. Zhang. Multi-stage multi-task learning with reduced rank. In AAAI  pages 1638–1644 

2016.

T. Heskes. Solving a huge number of similar tasks: A combination of multi-task learning and a

hierarchical bayesian approach. In ICML  pages 233–241  1998.

R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press  2012.

L. Jacob  J.-p. Vert  and F. R. Bach. Clustered multi-task learning: A convex formulation. In Neurips 

pages 745–752  2009.

J.-Y. Jeong and C.-H. Jun. Variable selection and task grouping for multi-task learning. In KDD 

pages 1589–1598  2018.

Y. Jiang  Z. Yang  Q. Xu  X. Cao  and Q. Huang. When to learn what: Deep cognitive subspace

clustering. In ACM MM  pages 718–726  2018.

Y. Jiang  Q. Xu  Z. Yang  X. Cao  and Q. Huang. Duet robust deep subspace clustering. In ACM MM 

2019.

Z. Kang  K. Grauman  and F. Sha. Learning with whom to share in multi-task feature learning. In

ICML  pages 521–528  2011.

A. Kovashka and K. Grauman. Discovering attribute shades of meaning with the crowd. IJCV  114

(1):56–73  2015.

10

A. Kovashka  D. Parikh  and K. Grauman. Whittlesearch: Image search with relative attribute

feedback. In CVPR  pages 2973–2980  2012.

M. Kshirsagar  E. Yang  and A. C. Lozano. Learning task clusters via sparsity grouped multitask

learning. In ECML PKDD  pages 673–689  2017.

A. Kumar and H. D. III. Learning task grouping and overlap in multi-task learning. In ICML  pages

1723–1730  2012.

G. Lee  E. Yang  and S. Hwang. Asymmetric multi-task learning based on task relatedness and loss.

In ICML  pages 230–238  2016.

H. Lee  E. Yang  and S. J. Hwang. Deep asymmetric multi-task feature learning. In ICML  pages

2962–2970  2018.

Y. Li  K. Fu  Z. Wang  C. Shahabi  J. Ye  and Y. Liu. Multi-task representation learning for travel

time estimation. In KDD  pages 1695–1704  2018.

Y. Lin  S. Yang  V. Stoyanov  and H. Ji. A multi-lingual multi-task architecture for low-resource

sequence labeling. In ACL  pages 799–809  2018.

P. Liu  X. Qiu  and X. Huang. Adversarial multi-task learning for text classiﬁcation. In ACL  pages

1–10  2017.

S. Liu and S. J. Pan. Adaptive group sparse multi-task learning via trace lasso. In IJCAI  pages

2358–2364  2017.

C. Lu  J. Feng  Z. Lin  T. Mei  and S. Yan. Subspace clustering by block diagonal representation.

IEEE transactions on pattern analysis and machine intelligence  41(2):487–501  2019.

L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of machine learning research  9

(Nov):2579–2605  2008.

A. Maurer  M. Pontil  and B. Romera-Paredes. Sparse coding for multitask and transfer learning. In

ICML  pages 343–351  2013.

A. Maurer  M. Pontil  and B. Romera-Paredes. The beneﬁt of multitask representation learning. The

Journal of Machine Learning Research  17(1):2853–2884  2016.

A. M. McDonald  M. Pontil  and D. Stamos. Spectral k-support norm regularization. In Neurips 

pages 3644–3652  2014.

F. Nie  Z. Hu  and X. Li. Calibrated multi-task learning. In KDD  pages 2012–2021  2018.

M. L. Overton and R. S. Womersley. On the sum of the largest eigenvalues of a symmetric matrix.

SIAM Journal on Matrix Analysis and Applications  13(1):41–45  1992a.

M. L. Overton and R. S. Womersley. On the sum of the largest eigenvalues of a symmetric matrix.

SIAM Journal on Matrix Analysis and Applications  13(1):41–45  1992b.

G. Peyré  M. Cuturi  et al. Computational optimal transport. Foundations and Trends R(cid:13) in Machine

Learning  11(5-6):355–607  2019.

V. Seguy  B. B. Damodaran  R. Flamary  N. Courty  A. Rolet  and M. Blondel. Large-scale optimal

transport and mapping estimation. In (ICLR)  2018.

C. Szegedy  V. Vanhoucke  S. Ioffe  J. Shlens  and Z. Wojna. Rethinking the inception architecture

for computer vision. In CVPR  pages 2818–2826  2016.

U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing  17(4):395–416  2007.

Y. Xian  C. H. Lampert  B. Schiele  and Z. Akata. Zero-shot learning-a comprehensive evaluation of
the good  the bad and the ugly. IEEE transactions on pattern analysis and machine intelligence 
2018.

11

D. Xu  W. Ouyang  X. Wang  and N. Sebe. Pad-net: Multi-tasks guided prediction-and-distillation

network for simultaneous depth estimation and scene parsing. In CVPR  pages 675–684  2018.

L. Xu  A. Huang  J. Chen  and E. Chen. Exploiting task-feature co-clusters in multi-task learning. In

AAAI  pages 1931–1937  2015.

Z. Yang  Q. Xu  X. Cao  and Q. Huang. From common to special: When multi-attribute learning

meets personalized opinions. In AAAI  pages 515–522  2018.

Z. Yang  Q. Xu  X. Cao  and Q. Huang. Learning personalized attribute preference via multi-task

AUC optimization. In AAAI  pages 5660–5667  2019a.

Z. Yang  Q. Xu  W. Zhang  X. Cao  and Q. Huang. Split multiplicative multi-view subspace clustering.

IEEE Transactions on Image Processing  2019b.

Z. Yin and Y. Shen. On the dimensionality of word embedding. In NIPS  pages 887–898  2018.

Y. Yu  T. Wang  and R. J. Samworth. A useful variant of the davis–kahan theorem for statisticians.

Biometrika  102(2):315–323  2014.

W. Zhong and J. T. Kwok. Convex multitask learning with ﬂexible task clusters. In ICML  pages

483–490  2012.

J. Zhou  J. Chen  and J. Ye. Clustered multi-task learning via alternating structure optimization. In

Neurips  pages 702–710  2011a.

J. Zhou  J. Chen  and J. Ye. Malsar: Multi-task learning via structural regularization. Arizona State

University  21  2011b.

C. Zhu  R. H. Byrd  P. Lu  and J. Nocedal. Algorithm 778: L-bfgs-b: Fortran subroutines for large-
scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS)  23
(4):550–560  1997.

12

,Shiqi Wang
Kexin Pei
Justin Whitehouse
Junfeng Yang
Suman Jana
Zhiyong Yang
Qianqian Xu
Yangbangyan Jiang
Xiaochun Cao
Qingming Huang