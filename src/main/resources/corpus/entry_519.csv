2019,An Improved Analysis of Training Over-parameterized Deep Neural Networks,A recent line of research has shown that gradient-based algorithms with random initialization can converge to the global minima of the training loss for over-parameterized (i.e.  sufficiently wide) deep neural networks. However  the condition on the width of the neural network to ensure the global convergence is very stringent  which is often a high-degree polynomial in the training sample size $n$ (e.g.  $O(n^{24})$). In this paper  we provide an improved analysis of the global convergence of (stochastic) gradient descent for training deep neural networks  which only requires a milder over-parameterization condition than previous work in terms of the training sample size and other problem-dependent parameters. The main technical contributions of our analysis include (a) a tighter gradient lower bound that leads to a faster convergence of the algorithm  and (b) a sharper characterization of the trajectory length of the algorithm. By specializing our result to two-layer (i.e.  one-hidden-layer) neural networks  it also provides a milder over-parameterization condition than the best-known result in prior work.,An Improved Analysis of Training

Over-parameterized Deep Neural Networks

Difan Zou

Quanquan Gu

Department of Computer Science

University of California  Los Angeles

Department of Computer Science

University of California  Los Angeles

Los Angeles  CA 90095
knowzou@cs.ucla.edu

Los Angeles  CA 90095

qgu@cs.ucla.edu

Abstract

A recent line of research has shown that gradient-based algorithms with random
initialization can converge to the global minima of the training loss for over-
parameterized (i.e.  sufﬁciently wide) deep neural networks. However  the condi-
tion on the width of the neural network to ensure the global convergence is very
stringent  which is often a high-degree polynomial in the training sample size
n (e.g.  O(n24)). In this paper  we provide an improved analysis of the global
convergence of (stochastic) gradient descent for training deep neural networks 
which only requires a milder over-parameterization condition than previous work
in terms of the training sample size and other problem-dependent parameters. The
main technical contributions of our analysis include (a) a tighter gradient lower
bound that leads to a faster convergence of the algorithm  and (b) a sharper char-
acterization of the trajectory length of the algorithm. By specializing our result
to two-layer (i.e.  one-hidden-layer) neural networks  it also provides a milder
over-parameterization condition than the best-known result in prior work.

1

Introduction

Recent study [20] has revealed that deep neural networks trained by gradient-based algorithms can
ﬁt training data with random labels and achieve zero training error. Since the loss landscape of
training deep neural network is highly nonconvex or even nonsmooth  conventional optimization
theory cannot explain why gradient descent (GD) and stochastic gradient descent (SGD) can ﬁnd
the global minimum of the loss function (i.e.  achieving zero training error). To better understand
the training of neural networks  there is a line of research [18  5  10  16  23  8  22  12] studying
two-layer (i.e.  one-hidden-layer) neural networks  where it assumes there exists a teacher network
(i.e.  an underlying ground-truth network) generating the output given the input  and casts neural
network learning as weight matrix recovery for the teacher network. However  these studies not
only make strong assumptions on the training data (existence of ground-truth network with the
same architecture as the learned network)  but also need special initialization methods that are very
different from the commonly used initialization method [13] in practice. Li and Liang [15]  Du et al.
[11] advanced this line of research by proving that under much milder assumptions on the training
data  (stochastic) gradient descent can attain a global convergence for training over-parameterized
(i.e. sufﬁciently wide) two-layer ReLU network with widely used random initialization method [13].
More recently  Allen-Zhu et al. [2]  Du et al. [9]  Zou et al. [24] generalized the global convergence
results from two-layer networks to deep neural networks. However  there is a huge gap between the
theory and practice since all these work Li and Liang [15]  Du et al. [11]  Allen-Zhu et al. [2]  Du
et al. [9]  Zou et al. [24] require unrealistic over-parameterization conditions on the width of neural
networks  especially for deep networks. In speciﬁc  in order to establish the global convergence for
training two-layer ReLU networks  Du et al. [11] requires the network width  i.e.  number of hidden

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

0

nodes  to be at least Ω(n6/λ4
0)  where n is the training sample size and λ0 is the smallest eigenvalue
of the so-called Gram matrix deﬁned in Du et al. [11]  which is essentially the neural tangent kernel
[14  7] on the training data. Under the same assumption on the training data  Wu et al. [19] improved
Oymak and Soltanolkotabi [17] improved the over-parameterization condition to Ω(n(cid:107)X(cid:107)6
0) 
2/λ4
where  is the target error and X ∈ Rn×d is the input data matrix. For deep ReLU networks  the
best known result was established in Allen-Zhu et al. [2]  which requires the network width to be at

the iteration complexity of GD in Du et al. [11] from O(cid:0)n2 log(1/)/λ2
(cid:1) and
least(cid:101)Ω(kn24L12φ−8)1 to ensure the global convergence of GD and SGD  where L is the number of

(cid:1) to O(cid:0)n log(1/)/λ0

hidden layers  φ is the minimum data separation distance and k is the output dimension.
This paper continues the line of research  and improves the over-parameterization condition and
the global convergence rate of (stochastic) gradient descent for training deep neural networks. In
speciﬁc  under the same setting as in Allen-Zhu et al. [2]  we prove faster global convergence rates for
both GD and SGD under a signiﬁcantly milder condition on the neural network width. Furthermore 
when specializing our result to two-layer ReLU networks  it also outperforms the best-known result
proved in Oymak and Soltanolkotabi [17]. The improvement in our result is due to the following
two innovative proof techniques: (a) a tighter gradient lower bound  which leads to a faster rate of
convergence for GD/SGD; and (b) a sharper characterization of the trajectory length for GD/SGD
until convergence.
We highlight our main contributions as follows:
• We show that  with Gaussian random initialization [13] on each layer  when the number of hidden

nodes per layer is(cid:101)Ω(cid:0)kn8L12φ−4(cid:1)  GD can achieve  training loss within (cid:101)O(cid:0)n2L2 log(1/)φ−1(cid:1)
result [2]  our over-parameterization condition is milder by a factor of(cid:101)Ω(n16φ−4)  and our iteration
complexity is better by a factor of (cid:101)O(n4φ−1).
tion [13] on each layer  when the number of hidden nodes per layer is(cid:101)Ω(cid:0)kn17L12B−4φ−8(cid:1)  SGD
can achieve  expected training loss within (cid:101)O(cid:0)n5 log(1/)B−1φ−2(cid:1) iterations  where B is the
are strictly better by a factor of(cid:101)Ω(n7B5) and (cid:101)O(n2) respectively regarding over-parameterization

iterations  where L is the number of hidden layers  φ is the minimum data separation distance  n is
the number of training examples  and k is the output dimension. Compared with the state-of-the-art

• We also prove a similar convergence result for SGD. We show that with Gaussian random initializa-

minibatch size of SGD. Compared with the corresponding results in Allen-Zhu et al. [2]  our results

condition and iteration complexity.

• When specializing our results of training deep ReLU networks with GD to two-layer ReLU
networks  it also outperforms the corresponding results [11  19  17]. In addition  for training
two-layer ReLU networks with SGD  we are able to show much better result than training deep
ReLU networks with SGD.

For the ease of comparison  we summarize the best-known results [11  2  9  19  17] of training
overparameterized neural networks with GD and compare with them in terms of over-parameterization
condition and iteration complexity in Table 1. We will show in Section 3 that  under the assumption
that all training data points have unit (cid:96)2 norm  which is the common assumption made in all these
work [11  2  9  19  17]  λ0 > 0 is equivalent to the fact that all training data are separated by some
distance φ  and we have λ0 = O(n−2φ) [17]. Substituting λ0 = Ω(n−2φ) into Table 1  it is evident
that our result outperforms all the other results under the same assumptions.
Notation For scalars  vectors and matrices  we use lower case  lower case bold face  and upper case
bold face letters to denote them respectively. For a positive integer  we denote by [k] the set {1  . . .   k}.

For a vector x = (x1  . . .   xd)(cid:62) and a positive integer p  we denote by (cid:107)x(cid:107)p =(cid:0)(cid:80)d

the (cid:96)p norm of x. In addition  we denote by (cid:107)x(cid:107)∞ = maxi=1 ... d |xi| the (cid:96)∞ norm of x  and
(cid:107)x(cid:107)0 = |{xi : xi (cid:54)= 0  i = 1  . . .   d}| the (cid:96)0 norm of x. For a matrix A ∈ Rm×n  we denote
by (cid:107)A(cid:107)F the Frobenius norm of A  (cid:107)A(cid:107)2 the spectral norm (maximum singular value)  λmin(A)
the smallest singular value  (cid:107)A(cid:107)0 the number of nonzero entries  and (cid:107)A(cid:107)2 ∞ the maximum (cid:96)2
norm over all row vectors  i.e.  (cid:107)A(cid:107)2 ∞ = maxi=1 ... m (cid:107)Ai∗(cid:107)2. For a collection of matrices
W = {W1  . . .   WL}  we denote (cid:107)W(cid:107)F =
F   (cid:107)W(cid:107)2 = maxl∈[L] (cid:107)Wl(cid:107)2 and
1Here(cid:101)Ω(·) hides constants and the logarithmic dependencies on problem dependent parameters except .

i=1 |xi|p(cid:1)1/p

(cid:113)(cid:80)L

l=1 (cid:107)Wl(cid:107)2

2

Ω

Ω

O

O

Table 1: Over-parameterization conditions and iteration complexities of GD for training overparam-
terized neural networks. K(L) is the Gram matrix for L-hidden-layer neural network [9]. Note that
the dimension of the output is k = 1 in Du et al. [11  9]  Wu et al. [19]  Oymak and Soltanolkotabi
[17].

Oymak and Soltanolkotabi [17]

Du et al. [11]
Wu et al. [19]

Over-para. condition Iteration complexity Deep? ReLU?

(cid:17)
(cid:16) n6
(cid:17)
(cid:16) n6
(cid:16) n(cid:107)X(cid:107)6
(cid:17)
(cid:17)
(cid:16) 2O(L)·n4
(cid:16) kn24L12
(cid:17)
(cid:101)Ω
(cid:16) kn8L12
(cid:17)
(cid:101)Ω

(cid:16) n2 log(1/)
(cid:17)
(cid:17)
(cid:16) n log(1/)
(cid:16)(cid:107)X(cid:107)2
(cid:17)
(cid:16) 2O(L)·n2 log(1/)
(cid:17)
(cid:16) n6L2 log(1/)
(cid:17)
(cid:17)
(cid:16) n2L2 log(1/)
(cid:107)W(cid:107)2 ∞ = maxl∈[L] (cid:107)Wl(cid:107)2 ∞. Given two collections of matrices (cid:102)W = {(cid:102)W1  . . .  (cid:102)WL} and
(cid:99)W = {(cid:99)W1  . . .  (cid:99)WL}  we deﬁne their inner product as (cid:104)(cid:102)W (cid:99)W(cid:105) = (cid:80)L
l=1(cid:104)(cid:102)Wl (cid:99)Wl(cid:105). For two
addition  we use (cid:101)O(·) and(cid:101)Ω(·) to hide logarithmic factors.

sequences {an} and {bn}  we use an = O(bn) to denote that an ≤ C1bn for some absolute constant
C1 > 0  and use an = Ω(bn) to denote that an ≥ C2bn for some absolute constant C2 > 0. In

Allen-Zhu et al. [2]

yes
yes
yes

yes
yes
yes

no
yes
yes

This paper

λ0
2 log(1/)

Du et al. [9]

no
no
no

λ4
0

λ4
0

λ4
0

λ2

min(K(L))

λ4

min(K(L))

Ω

Ω

O

O

O

λ2
0

λ0

φ2

φ

φ8

φ4

2

O

2 Problem setup and algorithms

In this section  we introduce the problem setup and the training algorithms.
Following Allen-Zhu et al. [2]  we consider the training of an L-hidden layer fully connected neural
network  which takes x ∈ Rd as input  and outputs y ∈ Rk. In speciﬁc  the neural network is a
vector-valued function fW : Rd → Rk  which is deﬁned as

fW(x) = Vσ(WLσ(WL−1 ··· σ(W1x)··· )) 

where W1 ∈ Rm×d  W2  . . .   WL ∈ Rm×m denote the weight matrices for the hidden layers  and
V ∈ Rk×m denotes the weight matrix in the output layer  σ(x) = max{0  x} is the entry-wise
ReLU activation function. In addition  we denote by σ(cid:48)(x) = 1(x) the derivative of ReLU activation
function and wl j the weight vector of the j-th node in the l-th layer.
Given a training set {(xi  yi)}i=1 ... n where xi ∈ Rd and yi ∈ Rk  the empirical loss function for
training the neural network is deﬁned as

L(W) :=

where (cid:96)(· ·) is the loss function  and(cid:98)yi = fW(xi). In this paper  for the ease of exposition  we

follow Allen-Zhu et al. [2]  Du et al. [11  9]  Oymak and Soltanolkotabi [17] and consider square
loss as follows

i=1

(2.1)

n(cid:88)

1
n

(cid:96)((cid:98)yi  yi) 

(cid:96)((cid:98)yi  yi) =

(cid:107)yi −(cid:98)yi(cid:107)2

1
2

2 

where(cid:98)yi = fW(xi) ∈ Rk denotes the output of the neural network given input xi. It is worth noting

that our result can be easily extended to other loss functions such as cross entropy loss [24] as well.
We will study both gradient descent and stochastic gradient descent as training algorithms  which are
displayed in Algorithm 1. For gradient descent  we update the weight matrix W(t)
l using full partial
gradient ∇WlL(W(t)). For stochastic gradient descent  we update the weight matrix W(t)
l using

(cid:1)  where B(t) with |B(t)| = B denotes

stochastic partial gradient 1/B(cid:80)

s∈B(t) ∇Wl (cid:96)(cid:0)fW(t)(xs)  ys

the minibatch of training examples at the t-th iteration. Both algorithms are initialized in the same

3

Algorithm 1 (Stochastic) Gradient descent with Gaussian random initialization
1: input: Training data {xi  yi}i∈[n]  step size η  total number of iterations T   minibatch size B.
2: initialization: For all l ∈ [L]  each row of weight matrix W(0)
is independently generated from

l

N (0  2/mI)  each row of V is independently generated from N (0  I/k)

Gradient Descent

3: for t = 0  . . .   T do
4: W(t+1)
= W(t)
l
5: end for
6: output: {W(T )

l − η∇Wl L(W(t)) for all l ∈ [L]
}l∈[L]
7: for t = 0  . . .   T do
8:
9: W(t+1)
l
10: end for
11: output: {W(T )

s∈B(t) ∇Wl (cid:96)(cid:0)fW(t)(xs)  ys
(cid:80)

Uniformly sample a minibatch of training data B(t) ∈ [n]

l − η
}l∈[L]

= W(t)

Stochastic Gradient Descent

B

l

l

(cid:1) for all l ∈ [L]

way as Allen-Zhu et al. [2]  which is essentially the initialization method [13] widely used in practice.
In the remaining of this paper  we denote by
∇L(W(t)) = {∇Wl L(W(t))}l∈[L]

the collections of all partial gradients of L(W(t)) and (cid:96)(cid:0)fW(t) (xi)  yi

(cid:1) = {∇Wl (cid:96)(cid:0)fW(t)(xi)  yi

and ∇(cid:96)(cid:0)fW(t) (xi)  yi

(cid:1)}l∈[L]

(cid:1).

3 Main theory

In this section  we present our main theoretical results. We make the following assumptions on the
training data.
Assumption 3.1. For any xi  it holds that (cid:107)xi(cid:107)2 = 1 and (xi)d = µ  where µ is an positive constant.
The same assumption has been made in all previous work along this line [9  2  24  17]. Note that
requiring the norm of all training examples to be 1 is not essential  and this assumption can be relaxed
to be (cid:107)xi(cid:107)2 is lower and upper bounded by some constants.
Assumption 3.2. For any two different training data points xi and xj  there exists a positive constant
φ > 0 such that (cid:107)xi − xj(cid:107)2 ≥ φ.
This assumption has also been made in Allen-Zhu et al. [3  2]  which is essential to guarantee zero
training error for deep neural networks. It is a quite mild assumption for the regression problem as
studied in this paper. Note that Du et al. [9] made a different assumption on training data  which
requires the Gram matrix K(L) (See their paper for details) deﬁned on the L-hidden-layer networks
is positive deﬁnite. However  their assumption is not easy to verify for neural networks with more
than two layers.
Based on Assumptions 3.1 and 3.2  we are able to establish the global convergence rates of GD and
SGD for training deep ReLU networks. We start with the result of GD for L-hidden-layer networks.

3.1 Training L-hidden-layer ReLU networks with GD

The global convergence of GD for training deep neural networks is stated in the following theorem.
Theorem 3.3. Under Assumptions 3.1 and 3.2  and suppose the number of hidden nodes per layer
satisﬁes

Then if set the step size η = O(cid:0)k/(L2m)(cid:1)  with probability at least 1 − O(n−1)  gradient descent is

(3.1)

m = Ω(cid:0)kn8L12 log3(m)/φ4(cid:1).
T = O(cid:0)n2L2 log(1/)/φ(cid:1)

able to ﬁnd a point that achieves  training loss within

iterations.

4

Remark 3.4. The state-of-the-art results for training deep ReLU network are provided by Allen-Zhu

et al. [2]  where the authors showed that GD can achieve -training loss within O(cid:0)n6L2 log(1/)/φ2(cid:1)
iterations if the neural network width satisﬁes m = (cid:101)Ω(cid:0)kn24L12/φ8(cid:1). As a clear comparison 
parameterization condition is milder than theirs by a factor of(cid:101)Ω(n16/φ4). Du et al. [9] also proved

our result on the iteration complexity is better than theirs by a factor of O(n4/φ)  and our over-

the global convergence of GD for training deep neural network with smooth activation functions. As
shown in Table 1  the over-parameterization condition and iteration complexity in Du et al. [9] have
an exponential dependency on L  which is much worse than the polynomial dependency on L as in
Allen-Zhu et al. [2] and our result.

We now specialize our results in Theorem 3.3 to two-layer networks by removing the dependency on
the number of hidden layers  i.e.  L. We state this result in the following corollary.
Corollary 3.5. Under the same assumptions made in Theorem 3.3. For training two-layer ReLU
then with probability at least 1 − O(n−1)  GD is able to ﬁnd a point that achieves -training loss

networks  if set the number of hidden nodes m = Ω(cid:0)kn8 log3(m)/φ4(cid:1) and step size η = O(k/m) 
within T = O(cid:0)n2 log(1/)/φ(cid:1) iterations.

For training two-layer ReLU networks  Du et al. [11] made a different assumption on the training data
to establish the global convergence of GD. Speciﬁcally  Du et al. [11] deﬁned a Gram matrix  which
is also known as neural tangent kernel [14]  based on the training data {xi}i=1 ... n and assumed
that the smallest eigenvalue of such Gram matrix is strictly positive. In fact  for two-layer neural
networks  their assumption is equivalent to Assumption 3.2  as shown in the following proposition.
Proposition 3.6. Under Assumption 3.1  deﬁne the Gram matrix H ∈ Rn×n as follows

Hij = Ew∼N (0 I)[x(cid:62)

i xjσ(cid:48)(w(cid:62)xi)σ(cid:48)(w(cid:62)xj)] 

then the assumption λ0 = λmin(H) > 0 is equivalent to Assumption 3.2. In addition  there exists a
sufﬁciently small constant C such that λ0 ≥ Cφn−2.
Remark 3.7. According to Proposition 3.6  we can make a direct comparison between our con-
vergence results for two-layer ReLU networks in Corollary 3.5 with those in Du et al. [11]  Oy-
mak and Soltanolkotabi [17]. In speciﬁc  as shown in Table 1  the iteration complexity and over-
parameterization condition proved in Du et al. [11] can be translated to O(n6 log(1/)/φ2) and
Ω(n14/φ4) respectively under Assumption 3.2. Oymak and Soltanolkotabi [17] improved the
result in Du et al. [11] and the improved iteration complexity and over-parameterization con-
X = [x1  . . .   xn](cid:62) ∈ Rd×n is the input data matrix. Our iteration complexity for two-layer
ReLU networks is better than that in Oymak and Soltanolkotabi [17] by a factor of O((cid:107)X(cid:107)2
2) 3  and
the over-parameterization condition is also strictly milder than the that in Oymak and Soltanolkotabi
[17] by a factor of O(n(cid:107)X(cid:107)6
2).

dition can be translated to O(cid:0)n2(cid:107)X(cid:107)2

2 log(1/)/φ(cid:1) 2 and Ω(cid:0)n9(cid:107)X(cid:107)6

2/φ4(cid:1) respectively  where

3.2 Extension to training L-hidden-layer ReLU networks with SGD

Then we extend the convergence results of GD to SGD in the following theorem.
Theorem 3.8. Under Assumptions 3.1 and 3.2  and suppose the number of hidden nodes per layer
satisﬁes

Then if set the step size as η = O(cid:0)kBφ/(n3m log(m))(cid:1)  with probability at least 1 − O(n−1)  SGD

m = Ω(cid:0)kn17L12 log3(m)/(B4φ8)(cid:1).
T = O(cid:0)n5L2 log(m) log2 (1/)/(Bφ2)(cid:1)

(3.2)

is able to achieve  expected training loss within

iterations.

(cid:107)X(cid:107)2

2It is worth noting that (cid:107)X(cid:107)2
2 = O(n) in the worst case.
3Here we set k = 1 in order to match the problem setting in Du et al. [11]  Oymak and Soltanolkotabi [17].

2 = O(n/d) if X is randomly generated  and

2 = O(1) if d (cid:46) n  (cid:107)X(cid:107)2

5

Remark 3.9. We ﬁrst compare our result with the state-of-the-art proved in Allen-Zhu et al. [2] 

where they showed that SGD can ﬁnd a point with -training loss within (cid:101)O(cid:0)n7L2 log(1/)/(Bφ2)(cid:1)
iterations if m = (cid:101)Ω(cid:0)n24L12Bk/φ8(cid:1). In stark contrast  our result on the over-parameterization
condition is strictly better than it by a factor of(cid:101)Ω(n7B5)  and our result on the iteration complexity

is also faster by a factor of O(n2).

Moreover  we also characterize the convergence rate and over-parameterization condition of SGD for
training two-layer networks. Unlike the gradient descent  which has the same convergence rate and
over-parameterization condition for training both deep and two-layer networks in terms of training
data size n  we ﬁnd that the over-parameterization condition of SGD can be further improved for
training two-layer neural networks. We state this improved result in the following theorem.
Theorem 3.10. Under the same assumptions made in Theorem 3.8. For two-layer ReLU networks 
if set the number of hidden nodes and step size as

m = Ω(cid:0)k5/2n11 log3(m)/(φ5B)(cid:1) 

η = O(cid:0)kBφ/(n3m log(m))(cid:1) 

then with probability at least 1 − O(n−1)  stochastic gradient descent is able to achieve  training

loss within T = O(cid:0)n5 log(m) log(1/)/(Bφ2)(cid:1) iterations.
m = Ω(cid:0)kn17 log3(m)B−4φ−8(cid:1)  which is much worse than that in Theorem 3.10. This is because

Remark 3.11. From Theorem 3.8  we can also obtain the convergence results of SGD for two-layer
ReLU networks by choosing L = 1. However  the resulting over-parameterization condition is

for two-layer networks  the training loss enjoys nicer local properties around the initialization  which
can be leveraged to improve the convergence of SGD. Due to space limit  we defer more details to
Appendix A.

4 Proof sketch of the main theory

In this section  we provide the proof sketch for Theorems 3.3  and highlight our technical contributions
and innovative proof techniques.

4.1 Overview of the technical contributions

The improvements in our result are mainly attributed to the following two aspects: (1) a tighter
gradient lower bound leading to faster convergence; and (2) a sharper characterization of the trajectory
length of the algorithm.
We ﬁrst deﬁne the following perturbation region based on the initialization 

B(W(0)  τ ) = {W : (cid:107)Wl − W(0)

l (cid:107)2 ≤ τ for all l ∈ [L]} 

where τ > 0 is the preset perturbation radius for each weight matrix Wl.
Tighter gradient lower bound. By the deﬁnition of ∇L(W)  we have (cid:107)∇L(W)(cid:107)2

(cid:80)L
F =
l=1 (cid:107)∇Wl L(W)(cid:107)2
F . Therefore  we can focus on the partial gradient of
L(W) with respect to the weight matrix at the last hidden layer. Note that we further have
(cid:107)∇WLL(W)(cid:107)2

F ≥ (cid:107)∇WL L(W)(cid:107)2
j=1 (cid:107)∇wL j L(W)(cid:107)2

F =(cid:80)m

∇wL j L(W) =

1
n

2  where

(cid:104)fW(xi) − yi  vj(cid:105)σ(cid:48)(cid:0)(cid:104)wL j  xL−1 i(cid:105)(cid:1)xL−1 i 

n(cid:88)

i=1

and xL−1 i denotes the output of the (L − 1)-th hidden layer with input xi. In order to prove the
gradient lower bound  for each xL−1 i  we introduce a region namely “gradient region”  denoted by
Wj  which is almost orthogonal to xL−1 i. Then we prove two major properties of these n regions
{W1  . . .  Wn}: (1) Wi ∩ Wj = ∅ if i (cid:54)= j  and (2) if wL j ∈ Wi for any i  with probability at
least 1/2  (cid:107)∇wL j L(W)(cid:107)2 is sufﬁciently large. We visualize these “gradient regions” in Figure 1(a).
Since {wL j}j∈[m] are randomly generated at the initialization  in order to get a larger bound of
(cid:107)∇WLL(W)(cid:107)2
F   we hope the size of these “gradient regions” to be as large as possible. We take the
union of the “gradient regions” for all training data  i.e.  ∪n
i=1Wi  which is shown in Figure 1(a). As a

6

(a) “gradient region” for {xL−1 i}i∈[n]

(b) “gradient region” for xL−1 1

Figure 1: (a): “gradient region” for all training data (b): “gradient region” for one training example.

comparison  Allen-Zhu et al. [2]  Zou et al. [24] only leveraged the “gradient region” for one training
data point to establish the gradient lower bound  which is shown in Figure 1(b). Roughly speaking 
the size of “gradient regions” utilized in our proof is n times larger than those used in Allen-Zhu et al.
[2]  Zou et al. [24]  which consequently leads to an O(n) improvement on the gradient lower bound.
The improved gradient lower bound is formally stated in the following lemma.

Lemma 4.1 (Gradient lower bound). Let τ = O(cid:0)φ3/2n−3L−6 log
B(W(0)  τ )  with probability at least 1 − exp(cid:0)O(mφ/(dn)))  it holds that
F ≥ O(cid:0)mφL(W)/(kn2)(cid:1).

−3/2(m)(cid:1)  then for all W ∈

(cid:107)∇L(W)(cid:107)2

− W(t)

l

L(W(t)) −(cid:113)

l (cid:107)2 = η(cid:107)∇Wl L(W(t))(cid:107)2 ≤(cid:112)Ckn2/(mφ) ·(cid:16)(cid:113)

where C is an absolute constant. (4.1) enables the use of telescope sum  which yields (cid:107)W(t)
W(0)
as

Sharper characterization of the trajectory length. The improved analysis of the trajectory length
is motivated by the following observation: at the t-th iteration  the decrease of the training loss
after one-step gradient descent is proportional to the gradient norm  i.e.  L(W(t)) − L(W(t+1)) ∝
F . In addition  the gradient norm (cid:107)∇L(W(t))(cid:107)F determines the trajectory length in
(cid:107)∇L(W(t))(cid:107)2
(cid:17)
the t-th iteration. Putting them together  we can obtain
(cid:107)W(t+1)
l (cid:107)2 ≤(cid:112)Ckn2L(W(0))/mφ. In stark contrast  Allen-Zhu et al. [2] bounds the trajectory length
l (cid:107)2 ≤(cid:112)C(cid:48)kn6L2(W(0))/(mφ2) by taking summation over
−3/2(m)(cid:1)  if set the step size η = O(cid:0)k/(L2m)(cid:1)  with probability least

− W(t)
and further prove that (cid:107)W(t)
l − W(0)
t  where C(cid:48) is an absolute constant. Our sharp characterization of the trajectory length is formally
summarized in the following lemma.
iterates are staying inside the region B(W(0)  τ ) with τ =
Lemma 4.2. Assuming all
1 − O(n−1)  the following holds for all t ≥ 0 and l ∈ [L] 

O(cid:0)φ3/2n−3L−6 log

l (cid:107)2 = η(cid:107)∇Wl L(W(t))(cid:107)2 ≤ η

C(cid:48)mL(W(t))/k 

(cid:107)W(t+1)

l

l (cid:107)2 ≤ O(cid:0)(cid:112)kn2 log(n)/(mφ)(cid:1).

(cid:107)W(t)

l − W(0)

L(W(t+1))

 

(4.1)
l −

(cid:113)

4.2 Proof of Theorem 3.3

Our proof road map can be organized in three steps: (i) prove that the training loss enjoys good
curvature properties within the perturbation region B(W(0)  τ ); (ii) show that gradient descent is able
to converge to global minima based on such good curvature properties; and (iii) ensure all iterates
stay inside the perturbation region until convergence.
Step (i) Training loss properties. We ﬁrst show some key properties of the training loss within
B(W(0)  τ )  which are essential to establish the convergence guarantees of gradient descent.

7

Lemma 4.3. If m ≥ O(L log(nL))  with probability at least 1 − O(n−1) it holds that L(W(0)) ≤

(cid:101)O(1).

Lemma 4.3 suggests that the training loss L(W) at the initial point does not depend on the number
of hidden nodes per layer  i.e.  m.
Moreover  the training loss L(W) is nonsmooth due to the non-differentiable ReLU activation
function. Generally speaking  smoothness is essential to achieve linear rate of convergence for
gradient-based algorithms. Fortunately  Allen-Zhu et al. [2] showed that the training loss satisﬁes
locally semi-smoothness property  which is summarized in the following lemma.
Lemma 4.4 (Semi-smoothness [2]). Let

τ ∈(cid:2)Ω(cid:0)k3/2/(m3/2L3/2 log3/2(m))(cid:1)  O(cid:0)1/(L4.5 log3/2(m))(cid:1)(cid:3).

B(W(0)  τ )  with probability at least 1 − exp(−Ω(−mτ 3/2L))  there exist two constants C(cid:48) and C(cid:48)(cid:48)
such that

Then for any two collections (cid:99)W = {(cid:99)Wl}l∈[L] and (cid:102)W = {(cid:102)Wl}l∈[L] satisfying (cid:99)W (cid:102)W ∈
L((cid:102)W) ≤ L((cid:99)W) + (cid:104)∇L((cid:99)W) (cid:102)W −(cid:99)W(cid:105)
L((cid:99)W) · τ 1/3L2(cid:112)m log(m)

· (cid:107)(cid:102)W −(cid:99)W(cid:107)2 +

(cid:107)(cid:102)W −(cid:99)W(cid:107)2

+ C(cid:48)(cid:113)

C(cid:48)(cid:48)L2m

(4.2)

√

2.

k

k

(cid:18)

Lemma 4.4 is a rescaled version of Theorem 4 in Allen-Zhu et al. [2]  since the training loss L(W)
in (2.1) is divided by the training sample size n  as opposed to the training loss in Allen-Zhu et al. [2].
This lemma suggests that if the perturbation region is small  i.e.  τ (cid:28) 1  the non-smooth term (third
term on the R.H.S. of (4.2)) is small and dominated by the gradient term (the second term on the
the R.H.S. of (4.2)). Therefore  the training loss behaves like a smooth function in the perturbation
region and the linear rate of convergence can be proved.
Step (ii) Convergence rate of GD. Now we are going to establish the convergence rate for gradient
descent under the assumption that all iterates stay inside the region B(W(0)  τ )  where τ will be
speciﬁed later.
Lemma 4.5. Assume all

stay inside the region B(W(0)  τ )  where τ

O(cid:0)φ3/2n−3L−6 log
O(cid:0)k/(L2m)(cid:1)  with probability least 1 − exp(cid:0) − O(mτ 3/2L)(cid:1)  it holds that

−3/2(m)(cid:1). Then under Assumptions 3.1 and 3.2  if set the step size η =

iterates

=

L(W(t)) ≤

1 − O

L(W(0)).

(cid:18) mφη

(cid:19)(cid:19)t

kn2

Lemma 4.5 suggests that gradient descent is able to decrease the training loss to zero at a linear rate.
Step (iii) Verifying all iterates of GD stay inside the perturbation region. Then we are going
to ensure that all iterates of GD are staying inside the required region B(W(0)  τ ). Note that we
have proved the distance (cid:107)W(t)
l (cid:107)2 in Lemma 4.2. Therefore  it sufﬁces to verify that such
distance is smaller than the preset value τ. Thus  we can complete the proof of Theorem 3.3 by
verifying the conditions based on our choice of m. Note that we have set the required number of m
in (3.1)  plugging (3.1) into the result of Lemma 4.2  we have with probability at least 1 − O(n−1) 
the following holds for all t ≤ T and l ∈ [L]

l − W(0)

−3/2(m)(cid:1) 

l (cid:107)2 ≤ O(cid:0)φ3/2n−3L−6 log
T η = O(cid:0)kn2 log(cid:0)1/(cid:1)m−1φ−1(cid:1).

which is exactly in the same order of τ in Lemma 4.5. Therefore  our choice of m guarantees that all
iterates are inside the required perturbation region. In addition  by Lemma 4.5  in order to achieve 
accuracy  we require

Then substituting our choice of step size η = O(cid:0)k/(L2m)(cid:1) into (4.3) and applying Lemma 4.3  we

(4.3)

(cid:107)W(t)

l − W(0)

can get the desired result for T .

8

4.3 Optimizing both top and hidden layers

Here we would like to brieﬂy discuss the extension to the case where the top layer is also optimized.
The proof sketch is as follows: similar to our current proof  we can also deﬁne a small perturbation
region around the initialization  but the new deﬁnition involves a constraint on the top layer weights.
Speciﬁcally  such new perturbation region can be deﬁned as follows 

B(W(0)  τ ) = {W : (cid:107)Wl − W(0)

l (cid:107)2 ≤ τ for all l ∈ [L]  (cid:107)V − V(0)(cid:107)2 ≤ τ(cid:48)}.

Then  it can be proved that the neural network also enjoys good properties inside such region. Similar
to the proof in this paper  based on these good properties  we can prove that until convergence the
neural network weights  including the top layer weights  would not escape from such region. Note
that optimizing more parameter can lead to larger gradient  thus we can prove a larger gradient lower
bound during the training process which can potential speed up the convergence of optimization
algorithm (e.g.  GD  SGD).

5 Conclusions and future work

In this paper  we studied the global convergence of (stochastic) gradient descent for training over-
parameterized ReLU networks  and improved the state-of-the-art results. Our proof technique can
be also applied to prove similar results for other loss functions such as cross-entropy loss and other
neural network architectures such as convolutional neural networks (CNN) [2  11] and ResNet
[2  11  21]. One important future work is to investigate whether the over-parameterization condition
and the convergence rate can be further improved. It is promising that if we can further improve
the characterization of “gradient region”  as it may provide a tighter gradient lower bound and
consequently sharpen the over-parameterization condition. Another interesting future direction is to
explore the use of our proof technique to improve the generalization analysis of overparameterized
neural networks trained by gradient-based algorithms [1  6  4].

Acknowledgement

We thank the anonymous reviewers and area chair for their helpful comments. We also thank Jinshan
Zeng for his helpful comment on the proof in the earlier version of our work. This research was
sponsored in part by the National Science Foundation CAREER Award IIS-1906169  BIGDATA
IIS-1855099  and Salesforce Deep Learning Research Award. The views and conclusions contained
in this paper are those of the authors and should not be interpreted as representing any funding
agencies.

References
[1] ALLEN-ZHU  Z.  LI  Y. and LIANG  Y. (2018). Learning and generalization in overparameter-

ized neural networks  going beyond two layers. arXiv preprint arXiv:1811.04918 .

[2] ALLEN-ZHU  Z.  LI  Y. and SONG  Z. (2018). A convergence theory for deep learning via

over-parameterization. arXiv preprint arXiv:1811.03962 .

[3] ALLEN-ZHU  Z.  LI  Y. and SONG  Z. (2018). On the convergence rate of training recurrent

neural networks. arXiv preprint arXiv:1810.12065 .

[4] ARORA  S.  DU  S. S.  HU  W.  LI  Z. and WANG  R. (2019). Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. arXiv preprint
arXiv:1901.08584 .

[5] BRUTZKUS  A. and GLOBERSON  A. (2017). Globally optimal gradient descent for a convnet
with gaussian inputs. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70. JMLR. org.

[6] CAO  Y. and GU  Q. (2019). A generalization theory of gradient descent for learning over-

parameterized deep relu networks. arXiv preprint arXiv:1902.01384 .

9

[7] CHIZAT  L. and BACH  F. (2018). A note on lazy training in supervised differentiable program-

ming. arXiv preprint arXiv:1812.07956 .

[8] DU  S. S. and LEE  J. D. (2018). On the power of over-parametrization in neural networks

with quadratic activation. arXiv preprint arXiv:1803.01206 .

[9] DU  S. S.  LEE  J. D.  LI  H.  WANG  L. and ZHAI  X. (2018). Gradient descent ﬁnds global

minima of deep neural networks. arXiv preprint arXiv:1811.03804 .

[10] DU  S. S.  LEE  J. D. and TIAN  Y. (2017). When is a convolutional ﬁlter easy to learn? arXiv

preprint arXiv:1709.06129 .

[11] DU  S. S.  ZHAI  X.  POCZOS  B. and SINGH  A. (2018). Gradient descent provably optimizes

over-parameterized neural networks. arXiv preprint arXiv:1810.02054 .

[12] GAO  W.  MAKKUVA  A.  OH  S. and VISWANATH  P. (2019). Learning one-hidden-layer
neural networks under general input distributions. In The 22nd International Conference on
Artiﬁcial Intelligence and Statistics.

[13] HE  K.  ZHANG  X.  REN  S. and SUN  J. (2015). Delving deep into rectiﬁers: Surpassing
human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE international
conference on computer vision.

[14] JACOT  A.  GABRIEL  F. and HONGLER  C. (2018). Neural tangent kernel: Convergence and

generalization in neural networks. In Advances in neural information processing systems.

[15] LI  Y. and LIANG  Y. (2018). Learning overparameterized neural networks via stochastic

gradient descent on structured data. arXiv preprint arXiv:1808.01204 .

[16] LI  Y. and YUAN  Y. (2017). Convergence analysis of two-layer neural networks with ReLU

activation. arXiv preprint arXiv:1705.09886 .

[17] OYMAK  S. and SOLTANOLKOTABI  M. (2019). Towards moderate overparameteriza-
tion: global convergence guarantees for training shallow neural networks. arXiv preprint
arXiv:1902.04674 .

[18] TIAN  Y. (2017). An analytical formula of population gradient for two-layered ReLU network
and its applications in convergence and critical point analysis. arXiv preprint arXiv:1703.00560
.

[19] WU  X.  DU  S. S. and WARD  R. (2019). Global convergence of adaptive gradient methods

for an over-parameterized neural network. arXiv preprint arXiv:1902.07111 .

[20] ZHANG  C.  BENGIO  S.  HARDT  M.  RECHT  B. and VINYALS  O. (2016). Understanding

deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530 .

[21] ZHANG  H.  YU  D.  CHEN  W. and LIU  T.-Y. (2019). Training over-parameterized deep

resnet is almost as easy as training a two-layer network. arXiv preprint arXiv:1903.07120 .

[22] ZHANG  X.  YU  Y.  WANG  L. and GU  Q. (2018). Learning one-hidden-layer ReLU networks

via gradient descent. arXiv preprint arXiv:1806.07808 .

[23] ZHONG  K.  SONG  Z.  JAIN  P.  BARTLETT  P. L. and DHILLON  I. S. (2017). Recovery

guarantees for one-hidden-layer neural networks. arXiv preprint arXiv:1706.03175 .

[24] ZOU  D.  CAO  Y.  ZHOU  D. and GU  Q. (2018). Stochastic gradient descent optimizes

over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888 .

10

,Vincent Dutordoir
Hugh Salimbeni
James Hensman
Marc Deisenroth
Difan Zou
Quanquan Gu