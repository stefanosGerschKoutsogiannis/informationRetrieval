2014,Message Passing Inference for Large Scale Graphical Models with High Order Potentials,To keep up with the Big Data challenge  parallelized algorithms based on dual decomposition have been proposed to perform inference in Markov random fields. Despite this parallelization  current algorithms struggle when the energy has high order terms and the graph is densely connected. In this paper we propose a partitioning strategy followed by a message passing algorithm which is able to exploit pre-computations. It only updates the high-order factors when passing messages across machines. We demonstrate the effectiveness of our approach on the task of joint layout and semantic segmentation estimation from single images  and show that our approach is orders of magnitude faster than current methods.,Message Passing Inference for Large Scale Graphical

Models with High Order Potentials

Jian Zhang
ETH Zurich

Alexander G. Schwing
University of Toronto

Raquel Urtasun

University of Toronto

jizhang@ethz.ch

aschwing@cs.toronto.edu

urtasun@cs.toronto.edu

Abstract

To keep up with the Big Data challenge  parallelized algorithms based on dual de-
composition have been proposed to perform inference in Markov random ﬁelds.
Despite this parallelization  current algorithms struggle when the energy has high
order terms and the graph is densely connected. In this paper we propose a parti-
tioning strategy followed by a message passing algorithm which is able to exploit
pre-computations. It only updates the high-order factors when passing messages
across machines. We demonstrate the effectiveness of our approach on the task of
joint layout and semantic segmentation estimation from single images  and show
that our approach is orders of magnitude faster than current methods.

1

Introduction

Graphical models are a very useful tool to capture the dependencies between the variables of inter-
est. In domains such as computer vision  natural language processing and computational biology
they have been very widely used to solve problems such as semantic segmentation [37]  depth re-
construction [21]  dependency parsing [4  25] and protein folding [36].
Despite decades of research  ﬁnding the maximum a-posteriori (MAP) assignment or the minimi-
mum energy conﬁguration remains an open problem  as it is NP-hard in general. Notable exceptions
are specialized solvers such as graph-cuts [7  3] and dynamic programming [19  1]  which retrieve
the global optima for sub-modular energies and tree-shaped graphs. Algorithms based on message
passing [18  9]  a series of graph cut moves [16] or branch-and-bound techniques [5] are common
choices to perform approximate inference in the more general case. A task closely related to MAP
inference but typically harder is computation of the probability for a given conﬁguration. It requires
computing the partition function  which is typically done via message passing [18]  sampling or by
repeatedly using MAP inference to solve tasks perturbed via Gumbel distributions [8].
Of particular difﬁculty is the case where the involved potentials depend on more than two variables 
i.e.  they are high-order  or the graph is densely connected. Several techniques have been developed
to allow current algorithms to handle high-order potentials  but they are typically restricted to poten-
tials of a speciﬁc form  e.g.  a function of the cardinality [17] or piece-wise linear potentials [11  10].
For densely connected graphs with Gaussian potentials efﬁcient inference methods based on ﬁltering
have been proposed [14  33].
Alternating minimization approaches  which iterate between solving for subsets of variables have
also been studied [32  38  29]. However  most approaches loose their guarantees since related sub-
problems are solved independently. Another method to improve computational efﬁciency is to
divide the model into smaller tasks  which are solved in parallel using dual decomposition tech-
niques [13  20  22]. Contrasting alternating minimization  convergence properties are ensured.
However  these techniques are computationally expensive despite the division of computation  since
global and dense interactions are still present.

1

In this work we show that for many graphical models it is possible to devise a partitioning strat-
egy followed by a message passing algorithm such that efﬁciency can be improved signiﬁcantly.
In particular  our approach adds additional terms to the energy function (i.e.  regions to the Hasse
diagram) such that the high-order factors can be pre-computed and remain constant during local
message passing within each machine. As a consequence  high-order factors are only accessed once
before sending messages across machines. This contrasts tightening approaches [27  28  2  26] 
where additional regions are added to better approximate the marginal polytope at the cost of ad-
ditional computations  while we are mainly interested in computational efﬁciency. In contrast to
re-scheduling strategies [6  30  2]  our rescheduling is ﬁxed and does not require additional compu-
tation.
Our experimental evaluations show that state-of-the-art techniques [9  22] have difﬁculties optimiz-
ing energy functions that correspond to densely connected graphs with high-order factors. In con-
trast our approach is able to achieve more than one order of magnitude speed-ups while retrieving
the same solution in the complex task of jointly estimating 3D room layout and image segmentation
from a single RGB-D image.

2 Background: Dual Decomposition for Message Passing

order factors. To this end  we consider distributions deﬁned over a discrete domain S =(cid:81)N

We start by reviewing dual-decomposition approaches for inference in graphical models with high-
i=1 Si 
which is composed of a product of N smaller discrete spaces Si = {1  . . .  |Si|}. We model our dis-
tribution to depend log-linearly on a scoring function θ(s) deﬁned over the aforementioned discrete
product space S  i.e.  p(s) = 1
Z exp θ(s)  with Z the partition function. Given the scoring function
θ(s) of a conﬁguration s  it is unfortunately generally #P-complete to compute its probability since
the partition function Z is required. Its logarithm equals the following variational program [12]:

log Z = max
p∈∆

p(s)θ(s) + H(p) 

(1)

(cid:88)

s

r sr

where H denotes the entropy and ∆ indicates the probability simplex.
The variational program in Eq. (1) is challenging as it operates on the exponentially sized domain
S. However  we can make use of the fact that for many relevant applications the scoring function
r∈R θr(sr). These local scoring functions
θr depend on a subset of variables sr = (si)i∈r  deﬁned on a domain Sr ⊆ S  which is speciﬁed by
i∈r Si. We refer to R as the
set of all restriction required to compute the scoring function θ.

θ(s) is additively composed of local terms  i.e.  θ(s) =(cid:80)
the restriction often referred to as region r ⊆ {1  . . .   N}  i.e.  Sr =(cid:81)
Locality of the scoring function allows to equivalently rewrite the expected score via(cid:80)
pr(sr)θr(sr) by employing marginals pr(sr) = (cid:80)
(cid:80)
approximated by a weighted sum of local entropies H(p) ≈ (cid:80)
required to fulﬁll local marginalization constraints  i.e. (cid:80)

s p(s)θ(s) =
p(s). Unfortunately an exact de-
composition of the entropy H(p) using marginals is not possible. Instead  the entropy is typically
r crH(pr)  with cr the counting
numbers. The task remains intractable despite the entropy approximation since the marginals pr(sr)
are required to arise from a valid joint distribution p(s). However  if we require the marginals to be
consistent only locally  we obtain a tractable approximation [34]. We thus introduce local beliefs
br(sr) to denote the approximation  not to be confused with the true marginals pr. The beliefs are
bp(sp) = br(sr) ∀r  sr  p ∈ P (r) 
where the set P (r) subsumes the set of all parents of region r for which we want marginalization to
hold.
Putting all this together  we obtain the following approximation:

sp\sr

s\sr

(cid:88)

max

b

s.t.

(cid:88)
br ∈ ∆(cid:80)

r

sp\sr

(cid:26)

br(sr)θr(sr) +

crH(br)

r sr

∀r

br ∈ C =

br :

bp(sp) = br(sr) ∀sr  p ∈ P (r).

(2)

The computation and memory requirements can be too demanding when dealing with large graph-
ical models. To address this issue  [13  22] showed that this task can be distributed onto multiple

2

Algorithm: Distributed Message Passing Inference
Let a = 1/|M (r)| and repeat until convergence
1. For every κ in parallel: iterate T times over r ∈ R(κ):
∀p ∈ P (r)  sr

λp→p(cid:48)(sp) + (cid:80)

p(cid:48)∈P (p)

r(cid:48)∈C(p)∩κ\r

µp→r(sr) = ˆcp ln

λr→p(sr) ∝

sp\sr

(cid:88)
ˆcr +(cid:80)
(cid:88)

ˆcp
p∈P (r)

exp

ˆθp(sp) − (cid:80)
ˆθr(sr) +
(cid:88)
λc→r(sc) − (cid:88)

c∈C(r)∩κ

ˆcp

2. Exchange information by iterating once over r ∈ G ∀κ ∈ M (r)

νκ→r(sr) = a

λc→r(sc) +

λr→p(sr) − a

c∈C(r)

c∈C(r)∩κ

p∈P (r)

κ∈M (r) p∈P (r)

Figure 1: A block-coordinate descent algorithm for the distributed inference task.

λc→r(sc) + νκ→r(sr) +

µp→r(sr)

λr(cid:48)→p(sr(cid:48)) + νκ→p(sp)

ˆcp

(cid:88)

p∈P (r)

(cid:88)

(3)

− µp→r(sr)(4)
(cid:88)

λr→p(sr) (5)

computers κ by employing dual decomposition techniques. More speciﬁcally  the task is partitioned
into multiple independent tasks with constraints at the boundary ensuring consistency of the parts
r that
upon convergence. Hence  an additional constraint is added to make sure that all beliefs bκ
are assigned to multiple computers  i.e.  those at the boundary of the parts  are consistent upon
convergence and equal a single region belief br. The distributed program is then:

(cid:88)

(cid:88)
∀κ  r ∈ Rκ  sr  p ∈ P (r) (cid:80)

r (sr)ˆθr(sr) +
bκ

κ r

ˆcrH(bκ
r )

κ r sr

∀κ  r ∈ Rκ  sr

max
r ∈∆

br bκ

s.t.

p (sp) = bκ
bκ

r (sr)

sp\sr
bκ
r (sr) = br(sr) 

where Rκ refers to regions on comptuer κ. We uniformly distributed the scores θr(sr) and the
counting numbers cr of a region r to all overlapping machines. Thus ˆθr = θr/|M (r)| and ˆcr =
cr/|M (r)| with M (r) the set of machines that are assigned to region r.
Note that this program operates on the regions deﬁned by the energy decomposition. To derive an
efﬁcient algorithm making use of the structure incorporated in the constraints we follow [22] and
change to the dual domain. For the marginalization constraints we introduce Lagrange multipliers
r→p(sr) for every computer κ  all regions r ∈ Rκ assigned to that computer  all its states sr
λκ
and all its parents p. For the consistency constraint we introduce Lagrange multipliers νκ→r(sr)
for all computers  regions and states. The arrows indicate that the Lagrange multipliers can be
interpreted as messages sent between different nodes in a Hasse diagram with nodes corresponding
to the regions.
The resulting distributed inference algorithm [22] is summarized in Fig. 1. It consists of two parts 
the ﬁrst of which is a standard message passing on the Hasse-diagram deﬁned locally on each com-
puter κ. The second operation interrupts message passing occasionally to exchange information
between computers. This second task of exchanging messages is often visualized on a graph G with
nodes corresponding to computers and additional vertices denoting shared regions.
Fig. 2(a) depicts a region graph with four unary regions and two high-order ones  i.e.  R =
{{1} {2} {3} {4} {1  2  3} {1  2  3  4}}. We partition this region graph onto two computers
κ1  κ2 as indicated via the dashed rectangles. The graph G containing as nodes both computers
and the shared region is provided as well. The connections between all regions are labeled with the
corresponding message  i.e.  λ  µ and ν. We emphasize that the consistency messages ν are only
modiﬁed when sending information between computers κ. Investigating the provided example in
Fig. 2(a) more carefully we observe that the computation of µ as deﬁned in Eq. (3) in Fig. 1 in-
volves summing over the state-space of the third-order region {1  2  3} and the fourth-order region
{1  2  3  4}. The presence of those high-order regions make dual decomposition approaches [22]

3

(a)

(b)

Figure 2: Standard distributed message passing operating on an inference task partitioned to two
computers (left) is compared to the proposed approach (right) where newly introduced regions (yel-
low) ensure constant messages µ from the high-order regions.

impractical. In the next section we show how message passing algorithms can become orders of
magnitude faster when adding additional regions.

3 Efﬁcient Message Passing for High-order Models

The distributed message passing procedure described in the previous section involves summations
over large state-spaces when computing the messages µ.
In this section we derive an approach
that can signiﬁcantly reduce the computation by adding additional regions and performing message-
passing with a speciﬁc message scheduling. Our key observation is that computation can be greatly
reduced if the high-order regions are singly-connected since their outgoing message µ remains con-
stant. Generally  singly-connected high-order regions do not occur in graphical models. However  in
many cases we can use dual decomposition to distribute the computation in a way that the high-order
regions become singly-connected if we introduce additional intermediate regions located between
the high-order regions and the low-order ones (e.g.  unary regions).
At ﬁrst sight  adding regions increases computational complexity since we have to iterate over ad-
ditional terms. However  we add regions only if they result in constant messages from regions with
even larger state space. By pre-computing those constant messages rather than re-evaluating them at
every iteration  we hence decrease computation time despite augmenting the graph with additional
regions  i.e.  additional marginal beliefs br.
Speciﬁcally  we observe that there are no marginalization constraints for the singly-connected high-
order regions  subsumed in the set Hκ = {r ∈ ˆRκ : P (r) = ∅ |C(r)| = 1}  since their set
of parents is empty. An important observation made precise in Claim 1 is that the corresponding
messages µ are constant for high-order regions unless νκ→r changes. Therefore we can improve the
message passing algorithm discussed in the previous section by introducing additional regions to
increase the size of the set |Hκ| as much as possible while not changing the cost function. The latter
is ensured by requiring the additional counting numbers and potentials to equal zero. However  we
note that the program will change since the constraint set is augmented.
More formally  let ˆRκ be the set of all regions  i.e.  the regions Rκ of the original task on computer
κ in addition to the newly added regions ˆr ∈ ˆRκ\Rκ. Let Hκ = {r ∈ ˆRκ : P (r) = ∅ |C(r)| = 1}
be the set of high-order regions on computer κ that are singly connected and have no parent. Further 
let its complement Hκ = ˆRκ \ Hκ denote all remaining regions. The inference task is given by

(cid:88)

(cid:88)
∀κ  r ∈ Hκ  sr  p ∈ P (r) (cid:80)

r (sr)ˆθr(sr) +
bκ

κ r

ˆcrH(bκ
r )

κ r sr

∀κ  r ∈ ˆRκ  sr

max
r ∈∆

br bκ

s.t.

bκ
p (sp) = bκ

r (sr)

sp\sr
bκ
r (sr) = br(sr).

(9)

Even though we set θr(sr) ≡ 0 for all states sr  and ˆcr = 0 for all newly added regions r ∈ ˆRκ\Rκ 
the inference task is not identical to the original problem since the constraint set is not the same. Note
that new regions introduce new marginalization constraints. Next we show that messages leaving
singly-connected high-order regions are constant.

4

α = {1  2  3}β = {1  2  3  4}α = {1  2  3}β = {1  2  3  4}{1}{2}{3}{4}1αλ→1αµ→1βλ→1βµ→2αλ→2αµ→2βλ→2βµ→3αλ→3αµ→4βλ→4βµ→3βλ→3βµ→κ12κα = {1  2  3}β = {1  2  3  4}1κβυ→2καυ→2κβυ→1καυ→α = {1  2  3}β = {1  2  3  4}α = {1  2  3}β = {1  2  3  4}σ = {1  2}π = {3  4}{1}{2}{3}{4}κ12κ1σλ→1σµ→2σλ→2σµ→3πλ→3πµ→4πλ→4πµ→σαλ→ασµ→σβλ→βσµ→3αλ→3αµ→πβλ→βπµ→α = {1  2  3}β = {1  2  3  4}1κβυ→2καυ→2κβυ→1καυ→Algorithm: Message Passing for Large Scale Graphical Models with High Order Potentials
Let a = 1/|M (r)| and repeat until convergence
1. For every κ in parallel: Update singly-connected regions p ∈ Hκ: let r = C(p) ∀sr

ˆθp(sp) − (cid:80)

λp→p(cid:48)(sp) + (cid:80)

p(cid:48)∈P (p)

r(cid:48)∈C(p)∩κ\r

ˆcp

(cid:88)

sp\sr

µp→r(sr) = ˆcp ln

exp

λr(cid:48)→p(sr(cid:48)) + νκ→p(sp)

2. For every κ in parallel: iterate T times over r ∈ ˆRκ:
∀p ∈ P (r) \ Hκ  sr

λp→p(cid:48)(sp) + (cid:80)

p(cid:48)∈P (p)

r(cid:48)∈C(p)∩κ\r

λr(cid:48)→p(sr(cid:48)) + νκ→p(sp)

λc→r(sc) + νκ→r(sr) +

µp→r(sr)

ˆcp

(cid:88)

p∈P (r)

(cid:88)

(6)

− µp→r(sr)(7)
(cid:88)

λr→p(sr) (8)

λc→r(sc) +

λr→p(sr) − a

p∈P (r)

κ∈M (r) p∈P (r)

(cid:88)

sp\sr

exp

ˆθp(sp) − (cid:80)
ˆθr(sr) +
(cid:88)
λc→r(sc) − (cid:88)

c∈C(r)∩κ

c∈C(r)∩κ

ˆcp
p∈P (r)

ˆcp

ˆcr +(cid:80)
(cid:88)

c∈C(r)

µp→r(sr) = ˆcp ln
∀p ∈ P (r)  sr
λr→p(sr) ∝

νκ→r(sr) = a

3. Exchange information by iterating once over r ∈ G ∀κ ∈ M (r)

Figure 3: A block-coordinate descent algorithm for the distributed inference task.

Claim 1. During message passing updates deﬁned in Fig. 1 the multiplier µp→r(sr) is constant for
singly-connected high-order regions p.

Proof: More carefully investigating Eq. (3) which deﬁnes µ  it follows that(cid:80)
(cid:80)
p(cid:48)∈P (p) λp→p(cid:48)(sp) =
0 because P (p) = ∅ since p is assumed singly-connected. For the same reason we obtain
r(cid:48)∈C(p)∩κ\r λr(cid:48)→p(sr(cid:48)) = 0 because r(cid:48) ∈ C(p) ∩ κ \ r = ∅ and νκ→p(sp) is constant upon
each exchange of information. Therefore  µp→r(sr) is constant irrespective of all other messages
(cid:4)
and can be pre-computed upon exchange of information.
We can thus pre-compute the constant messages before performing message passing. Our approach
is summarized in Fig. 3. We now provide its convergence properties in the following claim.
Claim 2. The algorithm outlined in Fig. 3 is guaranteed to converge to the global optimum of the
program given in Eq. (9) for cr > 0 ∀r and is guaranteed to converge in case cr ≥ 0 ∀r.
Proof: The message passing algorithm is derived as a block-coordinate descent algorithm in the
dual domain. Hence it inherits the properties of block-coordinate descent algorithms [31] which are
guaranteed to converge to a single global optimum in case of strict concavity (cr > 0 ∀r) and which
are guaranteed to converge in case of concavity only (cr ≥ 0 ∀r)  which proves the claim.
(cid:4)
We note that Claim 1 nicely illustrates the beneﬁts of working with region graphs rather than factor
graphs. A bi-partite factor graph contains variable nodes connected to possibly high-order factors.
Assume that we distributed the task at hand such that every high-order region of size larger than two
is connected to at most two local variables. By adding a pairwise region in between the original
high-order factor node and the variable nodes we are able to reduce computational complexity since
the high-order factors are now singly connected. Therefore  we can guarantee that the complexity of
the local message-passing steps run in each machine reduces from the state-space size of the largest
factor to the size of the largest newly introduced region in each computer. This is summarized in the
following claim.
Claim 3. Assume we are given a high-order factor-graph representation of a graphical model. By
distributing the model onto multiple computers and by introducing additional regions we reduce the
complexity of the message passing iterations on every computer generally dominated by the state-

5

vp0

y1

y2

r3

r4

r1

r3

r4

vp2

y4

y3
vp1

vp0

y1

y2

r2

vp2

y4

y3
vp1

r1

r2

(a) Layout parameterization.

(b) Compatibility.

(c) Joint model.

Figure 4: Parameterization of the layout task is visualized in (a). Compatibility of a superpixel
labeling with a wall parameterization using third-order functions is outlined in (b) and the graphical
model for the joint layout-segmentation task is depicted in (c).

rel. duality gap

Ours [s]
cBP [s]
dcBP [s]

1
0.78
31.60
19.48
 = 0

0.1
5.92
986.54
1042.8

0.01
51.59
1736.6
1772.6

Ours [s]
cBP [s]
dcBP [s]

15.58
411.81
451.71
 = 1

rel. duality gap

1

0.1

448.26
4357.9
4506.6

0.01
1150.1
4479.9
4585.3

|Sr∩Hκ

|.

max =

max) with s(cid:48)

Table 1: Average time to achieve the speciﬁed relative duality gap for  = 0 (left) and  = 1 (right).
space size of the largest region smax = maxr∈Rκ |Sr| from O(smax) to O(s(cid:48)
maxr∈ ˆRκ
Proof: The complexity of standard message passing on a region graph is linear on the largest state-
space region  i.e.  O(smax). Since some operations can be pre-computed as per Claim 1 we em-
phasize that the largest newly introduced region on computer κ is of state-space size s(cid:48)
max which
(cid:4)
concludes the proof.
Claim 3 indicates that distributing computation in addition to message rescheduling is a powerful
tool to cope with high-order potentials. To gain some insight  we illustrate our idea with a speciﬁc
example. Suppose we distribute the inference computation on two computers κ1  κ2 as shown in
Fig. 2(a). We compare it to a task on ˆR regions  i.e.  we introduce additional regions ˆr ∈ ˆR\R. The
messages required in the augmented task are visualized in Fig. 2(b). Each computer (box highlighted
with dashed lines) is assigned a task speciﬁed by the contained region graph. As before we also
visualize the messages ν occasionally sent between the computers in a graph containing as nodes
the shared factors and the computers (boxes drawn with dashed lines). The algorithm proceeds by
passing messages λ  µ on each computer independently for T rounds. Afterwards messages ν are
exchanged between computers. Importantly  we note that messages for singly-connected high-order
regions within dashed boxes are only required to be computed once upon exchanging message ν.
This is the case for all high-order regions in Fig. 2(b) and for no high-order region in Fig. 2(a) 
highlighting the obtained computational beneﬁts.

4 Experimental Evaluation

We demonstrate the effectiveness of our approach in the task of jointly estimating the layout and
semantic labels of indoor scenes from a single RGB-D image. We use the dataset of [38]  which is
a subset of the NYU v2 dataset [24]. Following [38]  we utilize 202 images for training and 101 for
testing. Given the vanishing points (points where parallel lines meet at inﬁnity)  the layout task can
be formulated with four random variables s1  . . .   s4  each of which corresponds to angles for rays
originating from two distinct vanishing points [15]. We discretize each ray into |Si| = 25 states. To
deﬁne the segmentation task  we partition each image into super pixels. We then deﬁne a random
variable with six states for each super pixel si ∈ Si = {left  front  right  ceiling  ﬂoor  clutter} with
i > 4. We refer the reader to Fig. 4(a) and Fig. 4(b) for an illustration of the parameterization of the
problem. The graphical model for the joint problem is depicted in Fig. 4(c).
The score of the joint model is given by a sum of scores

θ(s) = θlay(s1  . . .   s4) + θlabel(s5  . . .   sM +4) + θcomp(s) 

where θlay is deﬁned as the sum of scores over the layout faces  which can be decomposed into a
sum of pairwise functions using integral geometry [23]. The labeling score θlabel contains unary

6

y4y3y2y1l4l5l3l2l1Layout NetworkSegmentation NetworkCompatibility Network(normalized primal/dual  = 0)

(normalized primal/dual  = 1)

(factor agreement  = 0)

(factor agreement  = 1)

Figure 5: Average normalized primal/dual and factor agreement for  = 1 and  = 0.

θcomp(s) =(cid:80)

potentials and pairwise regularization between neighboring superpixels. The third function  θcomp 
couples the two tasks and encourages the layout and the segmentation to agree in their labels  e.g.  a
superpixel on the left wall of the layout is more likely to be assigned the left-wall or the object label.
The compatibility score decomposes into a sum of ﬁfth-order scores  one for each superpixel  i.e. 
i>4 θcomp i(s1  . . .   s4  si). Using integral geometry [23]  we can further decompose
each superpixel score θcomp i into a sum of third-order energies. As illustrated in Fig. 4(c)  every
superpixel variable si  i > 4 is therefore linked to 4-choose-2 third order functions of state-space
size 6 · 252. These functions measure the overlap of each superpixel with a region speciﬁed by two
layout ray angles si  sj with i  j ∈ {1  . . .   4}  i (cid:54)= j. This is illustrated in Fig. 4(b) for the area
highlighted in purple and the blue region deﬁned by s2 and s3. Since a typical image has around
250 superpixels  there are approximately 1000 third-order factors.
Following Claim 3 we recognize that the third-order functions are connected to at most two vari-
ables if we distribute the inference such that the layout task is assigned to one computer while the
segmentation task is divided onto other machines. Importantly  this corresponds to a roughly equal
split of the problem when using our approach  since all tasks are pairwise and the state-space of
the layout task is higher than the one of the semantic-segmentation. Despite the third-order regions
involved in the original model  every local inference task contains at most pairwise factors.
We use convex BP [35  18  9] and distributed convex BP [22] as baselines. For our method  we assign
layout nodes to the ﬁrst machine and segmentation nodes to the second one. Without introducing
additional regions and pre-computations the workload of this split is highly unbalanced. This makes
distributed convex BP even slower than convex BP since many messages are exchanged over the
network. To be more fair to distributed convex BP  we split the nodes into two parts  each with 2
layout variables and half of the segmentation variables. For all experiments  we set cr = 1 and
evaluate the settings  = 1 and  = 0. For a fair comparison we employ a single core for our
approach and convex BP and two cores for distributed convex BP. Note that our approach can be run
in parallel to achieve even faster convergence.
We compare our method to the baselines using two metrics: Normalized primal/dual is a rescaled
version of the original primal and dual normalized by the absolute value of the optimal score. This
allow us to compare different images that might have fairly different energies.
In case none of
the algorithms converged we normalize all energies using the mean of the maximal primal and the
minimum dual. The second metric is the factor agreement  which is deﬁned as the proportion of
factors that agree with the connected node marginals.
Fig. 5 depicts the normalized primal/dual as well as the factor agreement for  = 0 (i.e.  MAP)
and  = 1 (i.e.  marginals). We observe that our proposed approach converges signiﬁcantly faster

7

10010110210300.511.522.5 ours primalcBP primaldcBP c = 1 primaldcBP c = 2 primalours dualcBP dualdcBP c = 1 dualdcBP c = 2 dual101102103−3−2−1012345 ours primalcBP primaldcBP c = 1 primaldcBP c = 2 primalours dualcBP dualdcBP c = 1 dualdcBP c = 2 dual010020030040050060070080090010000.650.70.750.80.850.90.951 ours agreementcBP agreementdcBP c = 1 agreementdcBP c = 2 agreement05001000150020002500300000.10.20.30.40.50.60.70.80.91 ours agreementcBP agreementdcBP c = 1 agreementdcBP c = 2 agreementlayout err: 0.90% segmentation err: 4.74% layout err: 1.15% segmentation err: 5.12% layout err: 1.75% segmentation err: 3.98%

layout err: 2.36% segmentation err: 4.06% layout err: 2.38% segmentation err: 3.77% layout err: 2.88% segmentation err: 6.01%

layout err: 2.89% segmentation err: 3.99% layout err: 4.20% segmentation err: 3.65% layout err: 4.79% segmentation err: 4.17%

layout err: 13.97% segmentation err: 32.08% layout err: 25.89% segmentation err: 16.70% layout err: 18.04% segmentation err: 5.34%
Figure 6: Qualitative Result ( = 0) : First column illustrates the inferred layout (blue) and layout
ground truth (red). The second and third columns are estimated and ground truth segmentations re-
spectively. Failure modes are shown in the last row. They are due to bad vanishing point estimation.

than the baselines. We additionally observe that for densely coupled tasks  the performance of
dcBP degrades when exchanging messages every other iteration (yellow curves). Importantly  in
our experiments we never observed any of the other approaches to converge when our approach
did not converge. Tab. 1 depicts the time in seconds required to achieve a certain relative duality
gap. We observe that our proposed approach outperforms all baselines by more than one order of
magnitude. Fig. 6 shows qualitative results for  = 0. Note that our approach manages to accurately
predict layouts and corresponding segmentations. Some failure cases are illustrated in the bottom
row. They are largely due to failures in the vanishing point detection which our approach can not
recover from.

5 Conclusions

We have proposed a partitioning strategy followed by a message passing algorithm which is able to
speed-up signiﬁcantly dual decomposition methods for parallel inference in Markov random ﬁelds
with high-order terms and dense connections. We demonstrate the effectiveness of our approach on
the task of joint layout and semantic segmentation estimation from single images  and show that our
approach is orders of magnitude faster than existing methods. In the future  we plan to investigate
the applicability of our approach to other scene understanding tasks.

References
[1] A. Amini  T. Wymouth  and R. Jain. Using Dynamic Programming for Solving Variational Problems in

Vision. PAMI  1990.

[2] D. Batra  S. Nowozin  and P. Kohli. Tighter Relaxations for MAP-MRF Inference: A Local Primal-Dual

Gap based Separation Algorithm. In Proc. AISTATS  2011.

[3] Y. Boykov  O. Veksler  and R. Zabih. Fast Approximate Energy Minimization via Graph Cuts. PAMI 

2001.

[4] M. Collins. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics 

2003.

[5] R. Dechter. Reasoning with Probabilistic and Deterministic Graphical Models: Exact Algorithms. Mor-

gan & Claypool  2013.

[6] G. Elidan  I. McGraw  and D. Koller. Residual belief propagation: Informed scheduling for asynchronous

message passing. In Proc. UAI  2006.

[7] L. R. Ford and D. R. Fulkerson. Maximal ﬂow through a network. Canadian Journal of Mathematics 

1956.

[8] T. Hazan and T. Jaakkola. On the Partition Function and Random Maximum A-Posteriori Perturbations.

In Proc. ICML  2012.

[9] T. Hazan and A. Shashua. Norm-Product Belief Propagation: Primal-Dual Message-Passing for LP-

Relaxation and Approximate-Inference. Trans. Information Theory  2010.

8

[10] P. Kohli and P. Kumar. Energy Minimization for Linear Envelope MRFs. In Proc. CVPR  2010.
[11] P. Kohli  L. Ladick`y  and P. H. S. Torr. Robust higher order potentials for enforcing label consistency.

IJCV  2009.

[12] D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT Press 

2009.

[13] N. Komodakis  N. Paragios  and G. Tziritas. MRF Optimization via Dual Decomposition: Message-

Passing Revisited. In Proc. ICCV  2007.

[14] P. Kr¨ahenb¨uhl and V. Koltun. Efﬁcient Inference in Fully Connected CRFs with Gaussian Edge Potentials.

In Proc. NIPS  2011.

[15] D. C. Lee  A. Gupta  M. Hebert  and T. Kanade. Estimating Spatial Layout of Rooms using Volumetric

Reasoning about Objects and Surfaces. In Proc. NIPS  2010.

[16] V. Lempitsky  C. Rother  S. Roth  and A. Blake. Fusion Moves for Markov Random Field Optimization.

PAMI  2010.

[17] Y. Li  D. Tarlow  and R. Zemel. Exploring compositional high order pattern potentials for structured

output learning. In Proc. CVPR  2013.

[18] T. Meltzer  A. Globerson  and Y. Weiss. Convergent Message Passing Algorithms: a unifying view. In

Proc. UAI  2009.

[19] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kauf-

mann  1988.

[20] M. Salzmann. Continuous Inference in Graphical Models with Polynomial Energies. In Proc. CVPR 

2013.

[21] M. Salzmann and R Urtasun. Beyond feature points: structured prediction for monocular non-rigid 3d

reconstruction. In Proc. ECCV  2012.

[22] A. G. Schwing  T. Hazan  M. Pollefeys  and R. Urtasun. Distributed Message Passing for Large Scale

Graphical Models. In Proc. CVPR  2011.

[23] A. G. Schwing  T. Hazan  M. Pollefeys  and R. Urtasun. Efﬁcient Structured Prediction for 3D Indoor

Scene Understanding. In Proc. CVPR  2012.

[24] N. Silberman  D. Hoiem  P. Kohli  and R. Fergus.

RGBD Images. In Proc. ECCV  2012.

Indoor Segmentation and Support Inference from

[25] D. A. Smith and J. Eisner. Dependency parsing by belief propagation. In Proc. EMNLP  2008.
[26] D. Sontag  D. K. Choe  and Y. Li. Efﬁciently Searching for Frustrated Cycles in MAP Inference. In Proc.

UAI  2012.

[27] D. Sontag and T. Jaakkola. New Outer Bounds on the Marginal Polytope. In Proc. NIPS  2007.
[28] D. Sontag  T. Meltzer  A. Globerson  and T. Jaakkola. Tightening LP Relaxations for MAP using Message

Passing. In Proc. NIPS  2008.

[29] D. Sun  C. Liu  and H. Pﬁster. Local Layering for Joint Motion Estimation and Occlusion Detection. In

Proc. CVPR  2014.

[30] C. Sutton and A. McCallum. Improved dynamic schedules for belief propagation. In Proc. UAI  2007.
[31] P. Tseng and D. P. Bertsekas. Relaxation Methods for Problems with Strictly Convex Separable Costs and

Linear Constraints. Mathematical Programming  1987.

[32] L. Valgaerts  A. Bruhn  H. Zimmer  J. Weickert  C. Stroll  and C. Theobalt. Joint Estimation of Motion 

Structure and Geometry from Stereo Sequences. In Proc. ECCV  2010.

[33] V. Vineet and P. H. S. Torr J. Warrell. Filter-based Mean-Field Inference for Random Fields with Higher

Order Terms and Product Label-Spaces. In Proc. ECCV  2012.

[34] M. J. Wainwright and M. I. Jordan. Graphical Models  Exponential Families and Variational Inference.

Foundations and Trends in Machine Learning  2008.

[35] Y. Weiss  C. Yanover  and T. Meltzer. MAP Estimation  Linear Programming and Belief Propagation with

Convex Free Energies. In Proc. UAI  2007.

[36] C. Yanover  O. Schueler-Furman  and Y. Weiss. Minimizing and Learning Energy Functions for Side-

Chain Prediction. J. of Computational Biology  2008.

[37] J. Yao  S. Fidler  and R. Urtasun. Describing the scene as a whole: Joint object detection  scene classiﬁ-

cation and semantic segmentation. In Proc. CVPR  2012.

[38] J. Zhang  K. Chen  A. G. Schwing  and R. Urtasun. Estimating the 3D Layout of Indoor Scenes and its

Clutter from Depth Sensors. In Proc. ICCV  2013.

9

,Jian Zhang
Alex Schwing
Raquel Urtasun
Gang Niu
Marthinus Christoffel du Plessis
Tomoya Sakai
Masashi Sugiyama
Hoda Heidari
Claudio Ferrari
Krishna Gummadi
Andreas Krause