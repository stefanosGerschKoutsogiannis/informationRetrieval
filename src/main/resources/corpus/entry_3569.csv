2009,Multi-Step Dyna Planning for Policy Evaluation and Control,We extend Dyna planning architecture for policy evaluation and control in two significant aspects. First  we introduce a multi-step Dyna planning that projects the simulated state/feature many steps into the future. Our multi-step Dyna is based on a multi-step model  which we call the {\em $\lambda$-model}. The $\lambda$-model interpolates between the one-step model and an infinite-step model  and can be learned efficiently online. Second  we use for Dyna control a dynamic multi-step model that is able to predict the results of a sequence of greedy actions and track the optimal policy in the long run. Experimental results show that Dyna using the multi-step model evaluates a policy faster than using single-step models; Dyna control algorithms using the dynamic tracking model are much faster than model-free algorithms; further  multi-step Dyna control algorithms enable the policy and value function to converge much faster to their optima than single-step Dyna algorithms.,Multi-step Linear Dyna-style Planning

Hengshuai Yao

Shalabh Bhatnagar

Department of Computing Science

Department of Computer Science

University of Alberta

Edmonton  AB  Canada T6G2E8

and Automation

Indian Institute of Science
Bangalore  India 560012

Dongcui Diao

School of Economics and Management

South China Normal University

Guangzhou  China 518055

Abstract

In this paper we introduce a multi-step linear Dyna-style planning algorithm. The
key element of the multi-step linear Dyna is a multi-step linear model that en-
ables multi-step projection of a sampled feature and multi-step planning based on
the simulated multi-step transition experience. We propose two multi-step linear
models. The ﬁrst iterates the one-step linear model  but is generally computa-
tionally complex. The second interpolates between the one-step model and the
inﬁnite-step model (which turns out to be the LSTD solution)  and can be learned
efﬁciently online. Policy evaluation on Boyan Chain shows that multi-step linear
Dyna learns a policy faster than single-step linear Dyna  and generally learns faster
as the number of projection steps increases. Results on Mountain-car show that
multi-step linear Dyna leads to much better online performance than single-step
linear Dyna and model-free algorithms; however  the performance of multi-step
linear Dyna does not always improve as the number of projection steps increases.
Our results also suggest that previous attempts on extending LSTD for online
control were unsuccessful because LSTD looks inﬁnite steps into the future  and
suffers from the model errors in non-stationary (control) environments.

1 Introduction

Linear Dyna-style planning extends Dyna to linear function approximation (Sutton  Szepesv´ari 
Geramifard & Bowling  2008)  and can be used in large-scale applications. However  existing Dyna
and linear Dyna-style planning algorithms are all single-step  because they only simulate sampled
features one step ahead. This is many times insufﬁcient as one does not exploit in such a case all
possible future results. We extend linear Dyna architecture by using a multi-step linear model of
the world  which gives what we call the multi-step linear Dyna-style planning. Multi-step linear
Dyna-style planning is more advantageous than existing linear Dyna  because a multi-step model of
the world can project a feature multiple steps into the future and give more steps of results from the
feature.

For policy evaluation we introduce two multi-step linear models. The ﬁrst is generated by iterating
the one-step linear model  but is computationally complex when the number of features is large. The
second  which we call the λ-model  interpolates between the one-step linear model and an inﬁnite-
step linear model of the world  and is computationally efﬁcient to compute online. Our multi-step
linear Dyna-style planning for policy evaluation  Dyna(k)  uses the multi-step linear models to gen-
erate k-steps-ahead prediction of the sampled feature  and applies a generalized TD (temporal dif-

1

ference  e.g.  see (Sutton & Barto  1998)) learning on the imaginary multi-step transition experience.
When k is equal to 1  we recover the existing linear Dyna-style algorithm; when k goes to inﬁnity 
we actually use the LSTD (Bradtke & Barto  1996; Boyan  1999) solution for planning.

For the problem of control  related work include least-squares policy iteration (LSPI) (Lagoudakis &
Parr  2001; Lagoudakis & Parr  2003; Li  Littman & Mansley  2009)  and linear Dyna-style planning
for control. LSPI is an ofﬂine algorithm  that learns a greedy policy out of a data set of experience 
through a number of iterations  each of which sweeps the data set and alternates between LSTD
and policy improvement. Sutton et al. (2008) explored the use of linear function approximation
with Dyna for control  which does planning using a set of linear action models built from state
to state. In this paper  we ﬁrst build a one-step model from state-action pair to state-action pair
through tracking the greedy policy. Using this tracking model for planning is in fact another way of
doing single-step linear Dyna-style planning. In a similar manner to policy evaluation  we also have
two multi-step models for control. We build the iterated multi-step model by iterating the one-step
tracking model. Also  we build a λ-model for control by interpolating the one-step tracking model
and the inﬁnite-step model (also built through tracking). As the inﬁnite-step model coincides with
the LSTD solution  we actually propose an online LSTD control algorithm.

2 Backgrounds

Policy evaluation on Boyan Chain shows that multi-step linear Dyna learns a policy faster than
single-step linear Dyna. Results on the Mountain-car experiment show that multi-step linear Dyna
can ﬁnd the optimal policy faster than single-step linear Dyna; however  the performance of multi-
step linear Dyna does not always improve as the number of projection steps increases. In fact  LSTD
control and the inﬁnite-step linear Dyna for control are both unstable  and some intermediate value
of k makes the k-step linear Dyna for control perform the best.

Given a Markov decision process (MDP) with a state spaceS = {1  2  . . .   N }  the problem of
policy evaluation is to predict the long-term reward of a policy π for every state s ∈S:
S 7→R  j = 1  . . .   n  the feature of state i is φ(i) = [ϕ1(i)  ϕ2(i)  . . .   ϕn(i)]T . Now V π can

where rt is the reward received by the agent at time t. Given n (n ≤ N ) feature functions ϕj :

0 < γ < 1 

V π(s) =

s0 = s 

γtrt 

∞

X

t=0

be approximated using ˆV π = Φθ  where θ is the weight vector  and Φ is the feature matrix whose
entries are Φi j = ϕj(i)  i = 1  . . .   N; j = 1  . . .   n. At time t  linear TD(0) updates the weights as

θt+1 = θt + αtδtφt 

δt = rt + γθT

t φt+1 − θT

t φt 

where αt is a positive step-size and φt corresponds to φ(st).
Most of earlier work on Dyna uses a lookup table representation of states (Sutton  1990; Sutton &
Barto  1998). Modern Dyna is more advantageous in the use of linear function approximation  which
is called linear Dyna-style planning (Sutton et al.  2008). We denote the state transition probability
matrix of policy π by P π  whose (i  j)th component is P π
i j = Eπ{st+1 = j|st = i}; and denote the
expected reward vector of policy π by Rπ  whose ith component is the expected reward of leaving
state i in one step. Linear Dyna tries to estimate a compressed model of policy π:

(F π)T = (ΦT DπΦ)−1 · ΦT DπP πΦ;

f π = (ΦT DπΦ)−1 · ΦT DπRπ 

where Dπ is the N × N matrix whose diagonal entries correspond to the steady distribution of states
under policy π. F π and f π constitute the world model of linear Dyna for policy evaluation  and are
estimated online through gradient descent:

t+1 = F π
F π

t + βt(φt+1 − F π

t φt)φT
t ;

t+1 = f π
f π

t + βt(rt − φT

t f π

t )φt 

(1)

respectively  where the features and reward are all from real world experience and βt is the modeling
step-size.

Dyna repeats some steps of planning in each of which it samples a feature  projects it using the world
model  and plans using linear TD(0) based on the imaginary experience. For policy evaluation  the

2

ﬁxed-point of linear Dyna is the same as that of linear TD(0) under some assumptions (Tsitsiklis &
Van Roy  1997; Sutton et al.  2008)  that satisﬁes

Aπθ∗ + bπ = 0 : Aπ = ΦT Dπ(γP π − I)Φ;

bπ = ΦT DπRπ 

where IN ×N is the identity matrix.

3 The Multi-step Linear Model

In the lookup table representation  (P π)T and Rπ constitute the one-step world model. The k-step
transition model of the world is obtained by iterating (P π)T   k times with discount (Sutton  1995):

At the same time we accumulate the rewards generated in the process of this iterating:

P (k) = (γ(P π)T )k 

∀k = 1  2  . . .

R(k) =

k−1

X

j=0

(γP π)jRπ 

∀k = 1  2  . . .  

where R(k) is called the k-step reward model. P (k) and R(k) predict a feature k steps into the
future. In particular  P (k)φ is the feature of the expected state after k steps from φ  and (R(k))T φ is
the expected accumulated rewards in k steps from φ. Notice that

V π = R(k) + (P (k))T V π 

∀k = 1  2  . . .  

(2)

which is a generalization of the Bellman equation  V π = Rπ + γP πV π.

3.1 The Iterated Multi-step Linear Model

In the linear function approximation  F π and f π constitute the one-step linear model. Similar to the
lookup table representation  we can iterate F π  k times  and accumulate the approximated rewards
along the way:

F (k) = (γF π)k;

f (k) =

k−1

X

j=0

(γ(F π)T )j f π.

We call (F (k)  f (k)) the iterated multi-step linear model. By this deﬁnition  we extend (2) to the
k-step linear Bellman equation:

ˆV π = Φθ∗ = Φf (k) + Φ(F (k))T θ∗ 

∀k = 1  2  . . .  

(3)

where θ∗ is the linear TD(0) solution.

3.2 The λ-model

The quantities F (k) and f (k) require powers of F π. One can ﬁrst estimate F π and f π  and then
estimate F (k) and f (k) using powers of the estimated F π. However  real life tasks require a lot
of features. Generally (F π)k requires O((k − 1)n3) computation  which is too complex when the
number of features (n) is large.

Rather than using F (k) and f (k)  we would like to explore some other multi-step model that is cheap
in computation but is still meaningful in some sense. First let us see how F (k) and f (k) are used
if they can be computed. Given an imaginary feature ˜φτ   we look k steps ahead to see our future
feature by applying F (k):

˜φ(k)
τ = F (k) ˜φτ .
As k grows  F (k) diminishes and thus ˜φ(k)
converges to 0. 1 This means that the more steps we look
into the future from a given feature  the more ambiguous is our resulting feature. It suggests that we

τ

1This is because γF π has a spectral radius smaller than one  cf. Lemma 9.2.2 of (Bertsekas  Borkar &

Nedich  2004).

3

can use a decayed one-step linear model to approximate the effects of looking multiple steps into
the future:

L(k) = (λγ)k−1γF π 

parameterized by a factor λ ∈ (0  1]. To guarantee that the optimality (3) still holds  we deﬁne

l(k) = (I − (L(k))T )(I − γ(F π)T )−1f π.

We call (L(k)  l(k)) the λ-model. When k = 1  we have L(1) = F (1) = γF π and l(1) = f (1) = f π 
recovering the one-step model used by existing linear Dyna. Notice that L(k) diminishes as k grows 
which is consistent with the fact that F (k) also diminishes as k grows. Finally  the inﬁnite-step
model reduces to a single vector  l(∞) = f (∞) = θ∗. The intermediate k interpolates between the
single-step model and inﬁnite-step model.

For intermediate k  computation of L(k) has the same complexity as the estimation of F π. Interest-
ingly  all l(k) can be obtained by shifting from l(∞) by an amount that shrinks l(∞) itself: 2

l(k) = (I − (L(k))T )(I − γ(F π)T )−1f π 

= l(∞) − (L(k))T l(∞).

(4)
The case of k = 1 is interesting. The linear Dyna algorithm (Sutton et al.  2008) takes advantage
of the fact that l(1) = f π and estimates it through gradient descent. On the other hand  in our Dyna
algorithm  we use (4) and estimate all l(k) from the estimation of l(∞)  which is generally no longer
a gradient-descent estimate.

4 Multi-step Linear Dyna-style Planning for Policy Evaluation

The architecture of multi-step linear Dyna-style planning  Dyna(k)  is shown in Algorithm 1. Gen-
erally any valid multi-step model can be used in the architecture. For example  in the algorithm we
can take M (k) = F (k) and m(k) = f (k)  giving us a linear Dyna architecture using the iterated
multi-step linear model  which we call the Dyna(k)-iterate.
In the following we present the family of Dyna(k) planning algorithms that use the λ-model. We ﬁrst
develop a planning algorithm for the inﬁnite-step model  and based on it we then present Dyna(k)
planning using the λ-model for any ﬁnite k.

4.1 Dyna(∞): Planning using the Inﬁnite-step Model

The inﬁnite-step model is preferable in computation because F (∞) diminishes and the model re-
duces to f (∞). It turns out that f (∞) can be further simpliﬁed to allow an efﬁcient online estimation:

f (∞) = (I − γ(F π)T )−1f π

= (ΦT DπΦ − γΦT DπP πΦ)−1 · ΦT DπΦf π
= −(Aπ)−1bπ.

We can accumulate Aπ and bπ online like LSTD (Bradtke & Barto  1996; Boyan  1999; Xu et al. 
2002) and solve f (∞) by matrix inversion methods or recursive least-square methods.
As with traditional Dyna  we initially sample a feature ˜φ from some distribution µ. We then apply
the inﬁnite-step model to get the expected future features and all the possible future rewards:

Next  a generalized linear TD(0) is applied on this simulated experience.

˜φ(∞) = F (∞) ˜φ;

˜r(∞) = (f (∞))T ˜φ.

˜θ := ˜θ + α(˜r(∞) + ˜θT ˜φ(∞) − ˜θT ˜φ) ˜φ.

Because ˜φ(∞) = 0  this simpliﬁes into

˜θ := ˜θ + α(˜r(∞) − ˜θT ˜φ) ˜φ.

We call this algorithm Dyna(∞)  which actually uses the LSTD solution for planning.

2Similarly f (k) can be obtained by shifting from f (∞) by an amount that shrinks itself.

4

Algorithm 1 Dyna(k) algorithm for evaluating policy π (using any valid k-step model).

Initialize θ0 and some model
Select an initial state
for each time step do

Take an action a according to π  observing rt and φt+1
θt+1 = θt + αt(rt + γφT
Update M (k) and m(k)
Set ˜θ0 = θt+1
repeat τ = 1 to p

t+1θt − φT

t θt)φt

/* linear TD(0) */

/*Planning*/

/* ˜φ(∞) = 0*/

Sample ˜φτ ∼ µ(·)
˜φ(k) = M (k) ˜φτ
˜r(k) = (m(k))T ˜φτ
˜θτ +1 := ˜θτ + ατ (˜r(k)

Set θt+1 = ˜θτ +1

end for

τ + ˜θT

τ

˜φ(k)
τ − ˜θT

τ

˜φτ ) ˜φτ

/*Generalized k-step linear TD(0) learning */

4.2 Planning using the λ-model

The k-step λ-model is efﬁcient to estimate  and can be directly derived from the single-step and
inﬁnite-step models:

L(k) = (λγ)k−1γF π

t+1;

l(k) = f (∞) − (L(k))T f (∞) 

respectively  where the inﬁnite-step model is estimated by f (∞) = (Aπ
nary feature ˜φ  we look k steps ahead to see the future features and rewards:

t+1)−1bπ

t+1. Given an imagi-

˜φ(k) = L(k) ˜φ;

˜r(k) = (l(k))T ˜φ.

Thus we obtain an imaginary k-step transition experience ˜φ → ( ˜φ(k) 
k-step version of linear TD(0):

˜r(k))  on which we apply a

˜θτ +1 = ˜θτ + α(˜r(k) + ˜θT

τ

˜φ(k) − ˜θT

τ

˜φ) ˜φ.

We call this algorithm the Dyna(k)-lambda planning algorithm. When k = 1  we obtain another
single-step Dyna  Dyna(1). Notice that Dyna(1) uses f (∞) while the linear Dyna uses f π. When
k → ∞  we obtain the Dyna(∞) algorithm.

5 Planning for Control

Planning for control is more difﬁcult than that for policy evaluation because in control the policy
changes from time step to time step. Linear Dyna uses a separate model for each action  and these
action models are from state to state (Sutton et al.  2008). Our model for control is different in that
it is from state-action pair to state-action pair. However  rather than building a model for all state-
action pairs  we build only one state-action model that tracks the sequence of greedy actions. Using
this greedy-tracking model is another way of doing linear Dyna-style planning. In the following we
ﬁrst build the single-step greedy-tracking model and the inﬁnite-step greedy-tracking model  and
based on these tracking models we build the iterated model and the λ-model.
Our extension of linear Dyna to control contains a TD control step (we use Q-learning)  and we
call it the linear Dyna-Q architecture. In the Q-learning step  the next feature is already implicitly
selected. Recall that Q-learning selects the largest next Q-function as the target for TD learning 
which is maxa′ ˆQt+1(st+1  a′) = maxa′ φ(st+1  a′)T θt. Alternatively  the greedy next state-action
feature

~φt+1 = arg max

φ′T θt

φ′=φ(st+1 ·)

is selected by Q-learning. We build a single-step projection matrix between state-action pairs  F   by
moving its projection of the current feature towards the greedy next state-action feature (tracking):

Ft+1 = Ft + βt(~φt+1 − Ftφt)φT
t .

(5)

5

Algorithm 2 Dyna-Q(k)-lambda: k-step linear Dyna-Q algorithm for control (using the λ-model).

Initialize F0  A0  b0 and θ0
Select an initial state
for each time step do

Take action a at st (using ǫ-greedy)  observing rt and st+1
Choose a′ that leads to the largest ˆQ(st+1  a′)
Set φ = φ(st  a)  ~φ = φ(st+1  a′)
θt+1 = θt + αt(rt + γ ~φT θt − φT θt)φ
At+1 = At + φt(γ ~φT − φ)T  
f (∞) = −(At+1)−1bt+1
Ft+1 = Ft + αt(~φ − Ftφ)φT  
L(k) = (λγ)k−1γFt+1
l(k) = f (∞) − (L(k))T f (∞)
Set ˜θ0 = θt+1
repeat τ times

bt+1 = bt + φtrt

/*Q-learning*/

/*Using matrix inversion or recursive least-squares */

/*Planning*/

Sample ˜φτ ∼ µ
˜φ(k) = L(k) ˜φτ
˜r(k) = (l(k))T ˜φτ
˜θτ +1 := ˜θτ + ατ (˜r(k)

Set θt+1 = ˜θτ +1

end for

τ + ˜θT

τ

˜φ(k)
τ − ˜θT

τ

˜φτ ) ˜φτ

Estimation of the single-step reward model  f   is the same as in policy evaluation.
In a similar manner  in the inﬁnite-step model  matrix A is updated using the greedy next feature 
while vector b is updated in the same way as in LSTD. Given A and b  we can solve them and get
f (∞). Once the one-step model and the inﬁnite-step model are available  we interpolate them and
compute the λ-model in a similar manner to policy evaluation. The complete multi-step Dyna-Q
control algorithm using the λ-model is shown in Algorithm 2. We noticed that f (∞) can be directly
used for control  giving an online LSTD control algorithm.

We can also extend the iterated multi-step model and Dyna(k)-iterate to control. Given the single-
step greedy-tracking model  we can iterate it and get the iterated multi-step linear model in a similar
way to policy evaluation. The linear Dyna for control using the iterated greedy-tracking model
(which we call Dyna-Q(k)-iterate) is straightforward and thus not shown.

6 Experimental Results

6.1 Boyan Chain Example

The problem we consider is exactly the same as that considered by Boyan (1999). The root mean
square error (RMSE) of the value function is used as a criterion. Previously it was shown that linear
Dyna can learn a policy faster than model-free TD methods in the beginning episodes (Sutton et al. 
2008). However  after some episodes  their implementation of linear Dyna became poorer than
TD. A possible reason leading to their results may be that the step-sizes of learning  modeling and
planning were set to the same value. Also  their step-size diminishes according to 1/(traj#)1.1 
which does not satisfy the standard step-size rule required for stochastic approximation. In our linear
Dyna algorithms  we used different step-sizes for learning  modeling and planning.

(1) Learning step-size. We used here the same step-size rule for TD as Boyan (1999)  where α =
0.1(1 + 100)/(traj# + 100) was found to be the best in the class of step-sizes and also used it
for TD in the learning sub-procedure of all linear Dyna algorithms. (2) Modeling step-size. For
Dyna(k)-lambda  we used βT = 0.5(1 + 10)/(10 + T ) for estimation of F π  where T is the number
of state visits across episodes. For linear Dyna  the estimation of F π and f π also used the same βT .
(3) Planning step-size. In our experiments all linear Dyna algorithms simply used ατ = 0.1.

6

15

10

)
g
o
L
(
 

E
S
M
R

1

0.1

 

100

 

15

10

 Dyna(10)−lambda

 Dyna(1)

TD

)
g
o
L
(
E
S
M
R

1

LSTD  A0=−0.1I

 Dyna(∞)

101

Episodes (Log)

102

0.1

100

101

Episodes (Log)

102

LSTD  A
=−0.1I
0
LSTD  A
=−I
0
LSTD  A
=−10I
0

Dyna(3)−iterate
Dyna(5)−iterate
Dyna(10)−iterate
Linear Dyna

Figure 1: Results on Boyan Chain. Left: comparison of RMSE of Dyna(k)-iterate with LSTD.
Right: comparison of RMSE of Dyna(k)-lambda with TD and LSTD.

The weights of various learning algorithms  f π for the linear Dyna  and bπ for Dyna(k) were all
initialized to zero. No eligibility trace is used for any algorithm. In the planning step  all Dyna
algorithms sampled a unit basis vector whose nonzero component was in a uniformly random loca-
tion. In the following we report the results of planning only once. All RMSEs of algorithms were
averaged over 30 (identical) sets of trajectories.
Figure 1 (left) shows the performance of Dyna(k)-iterate and LSTD  and Figure 1 (right) shows
the performance of Dyna(k)-lambda  LSTD and TD. All linear Dyna algorithms were found to be
signiﬁcantly and consistently faster than TD. Furthermore  multi-step linear Dyna algorithms were
much faster than single-step linear Dyna algorithms. Matrix A of LSTD and Dyna(k)-lambda needs
perturbation in initialization  which has a great impact on the performances of two algorithms. For
LSTD  we tried initialization of Aπ
0 to −10I  −I  −0.1I  and showed their effects in Figure 1 (left) 
0 = −0.1I was the best for LSTD. Similar to LSTD  Dyna(k)-lambda is also sensitive
in which Aπ
to Aπ
0 . F π was
initialized to 0 for Dyna(k) (k < ∞) and linear Dyna. In Figure 1 (right) LSTD and Dyna(k)-lambda
were compared under the same setting (Dyna(k)-lambda also used A0 = −0.1I). Dyna(k)-lambda
used λ = 0.9.

0 . Linear Dyna and Dyna(k)-iterate do not use Aπ and thus do not have to tune Aπ

6.2 Mountain-car

We used the same Mountain-car environment and tile coding as in the linear Dyna paper (Sutton
et al.  2008). The state feature has a dimension of 10  000. The state-action feature is shifted from
the state feature  and has a dimension of 30  000 because there are three actions of the car. Because
the feature and matrix are really large  we were not able to compute the iterated model  and hence
we only present here the results of Dyna-Q(k)-lambda.
Experimental setting. (1)Step-sizes. The Q-learning step-size was chosen to be 0.1  in both the
independent algorithm and the sub-procedure of Dyna-Q(k)-lambda. The planning step-size was
0.1. The matrix F is much more dense than A and leads to a very slow online performance. To
tackle this problem  we avoided computing F explicitly  and used a least-squares computation of
the projection  given in the supplementary material. In this implementation  there is no modeling
step-size. (2)Initialization. The parameters θ and b were both initialized to 0. A was initialized to
−I. (3)Other setting. The λ value for Dyna-Q(k)-lambda was 0.9. We recorded the state-action
pairs online and replayed the feature of a past state-action pair in planning. We also compared the
linear Dyna-style planning for control (with state features) (Sutton et al.  2008)  which has three
sets of action models for this problem. In linear Dyna-style planning for control we replayed a state
feature of a past time step  and projected it using the model of the action that was selected at that
time step. No eligibility trace or exploration was used. Results reported below were all averaged
over 30 independent runs  each of which contains 20 episodes.

7

Dyna−Q(5)−lambda

Dyna−Q(10)−lambda

−100

−150

−200

t

n
r
u
e
R
e
n

 

−250

i
l

n
O

−300

−350

5

Dyna−Q(1)

Dyna−Q(20)−lambda

Dyna−Q(∞)

Linear Dyna

Q−learning

10

Episode

15

20

Figure 2: Results on Mountain-car: comparison of online return of Dyna-Q(k)-lambda  Q-learning
and linear Dyna for control.

Results are shown in Figure 2. Linear Dyna-style planning algorithms were found to be signiﬁcantly
faster than Q-learning. Multi-step planning algorithms can be still faster than single-step planning
algorithms. The results also show that planning too many steps into the future is harmful  e.g. 
Dyna-Q(20)-lambda and Dyna-Q(∞) gave poorer performance than Dyna-Q(5)-lambda and Dyna-
Q(10)-lambda. This shows that some intermediate values of k trade off the model accuracy and the
depth of looking ahead  and performed best. In fact  Dyna-Q(∞) and LSTD control algorithm were
both unstable  and typically failed once or twice in 30 runs. The intuition is that in control the policy
changes from time step to time step and the model is highly non-stationary. By solving the model
and looking inﬁnite steps into the future  LSTD and Dyna-Q(∞) magnify the errors in the model.

7 Conclusion and Future Work

We have taken important steps towards extending linear Dyna-style planning to multi-step planning.
Multi-step linear Dyna-style planning uses multi-step linear models to project a simulated feature
multiple steps into the future. For control  we proposed a different way of doing linear Dyna-style
planning  that builds a model from state-action pair to state-action pair  and tracks the greedy ac-
tion selection. Experimental results show that multi-step linear Dyna-style planning leads to better
performance than existing single-step linear Dyna-style planning on Boyan chain and Mountain-
car problems. Our experimental results show that linear Dyna-style planning can achieve a better
performance by using different step-sizes for learning  modeling  and planning than using a uni-
form step-size for the three sub-procedures. While it is not clear from previous work  our results
fully demonstrate the advantages of linear Dyna over TD/Q-learning for both policy evaluation and
control.

Our work also sheds light on why previous attempts on developing independent online LSTD control
were not successful (e.g.  forgetting strategies (Sutton et al.  2008)). LSTD and Dyna-Q(∞) can
become unstable because they magnify the model errors by looking inﬁnite steps into the future.
Current experiments do not include comparisons with any other LSTD control algorithm because
we did not ﬁnd in the literature an independent LSTD control algorithm. LSPI is usually off-line  and
its extension to online control has to deal with online exploration (Li et al.  2009). Some researchers
have combined LSTD in critic within the Actor-Critic framework (Xu et al.  2002; Peters & Schaal 
2008); however  LSTD there is still not an independent control algorithm.

Acknowledgements

The authors received many feedbacks from Dr. Rich Sutton and Dr. Csaba Szepesv´ari. We gratefully
acknowledge their help in improving the paper in many aspects. We also thank Alborz Geramifard
for sending us Matlab code of tile coding. This research was supported by iCORE  NSERC and the
Alberta Ingenuity Fund.

8

References

Bertsekas  D. P.  Borkar  V.  & Nedich  A. (2004). Improved temporal difference methods with linear
function approximation. Learning and Approximate Dynamic Programming (pp. 231–255). IEEE
Press.

Boyan  J. A. (1999). Least-squares temporal difference learning. ICML-16.
Bradtke  S.  & Barto  A. G. (1996). Linear least-squares algorithms for temporal difference learning.

Machine Learning  22  33–57.

Li  L.  Littman  M. L.  & Mansley  C. R. (2009). Online exploration in least-squares policy iteration.

AAMAS-8.

Peters  J.  & Schaal  S. (2008). Natural actor-critic. Neurocomputing  71  1180–1190.
Sutton  R. S. (1990). Integrated architectures for learning  planning  and reacting based on approxi-

mating dynamic programming. ICML-7.

Sutton  R. S. (1995). TD models: modeling the world at a mixture of time scales. ICML-12.
Sutton  R. S.  & Barto  A. G. (1998). Reinforcement learning: An introduction. MIT Press.
Sutton  R. S.  Szepesv´ari  C.  Geramifard  A.  & Bowling  M. (2008). Dyna-style planning with

linear function approximation and prioritized sweeping. UAI-24.

Tsitsiklis  J. N.  & Van Roy  B. (1997). An analysis of temporal-difference learning with function

approximation. IEEE Transactions on Automatic Control  42  674–690.

Xu  X.  He  H.  & Hu  D. (2002). Efﬁcient reinforcement learning using recursive least-squares

methods. Journal of Artiﬁcial Intelligence Research  16  259–292.

9

,Yiming Ying
Longyin Wen
Siwei Lyu