2019,Prediction of Spatial Point Processes: Regularized Method with Out-of-Sample Guarantees,A spatial point process can be characterized by an intensity function which predicts the number of events that occur across space. In this paper  we develop a method to infer predictive intensity intervals by learning a spatial model using a regularized criterion. We prove that the proposed method exhibits out-of-sample prediction performance guarantees which  unlike standard estimators  are valid even when the spatial model is misspecified. The method is demonstrated using synthetic as well as real spatial data.,Prediction of Spatial Point Processes:

Regularized Method with Out-of-Sample Guarantees

Muhammad Osama˚

muhammad.osama@it.uu.se

Dave Zachariah˚

dave.zachariah@it.uu.se

Peter Stoica˚

peter.stoica@it.uu.se

*Division of System and Control  Department of Information Technology  Uppsala University

Abstract

A spatial point process can be characterized by an intensity function which predicts
the number of events that occur across space. In this paper  we develop a method to
infer predictive intensity intervals by learning a spatial model using a regularized
criterion. We prove that the proposed method exhibits out-of-sample prediction
performance guarantees which  unlike standard estimators  are valid even when the
spatial model is misspeciﬁed. The method is demonstrated using synthetic as well
as real spatial data.

1

Introduction

Spatial point processes can be found in a range of applications from astronomy and biology to ecology
and criminology. These processes can be characterized by a nonnegative intensity function λpxq
which predicts the number of events that occur across space parameterized by x P X [8  4].
A standard approach to estimate the intensity function of a process is to use nonparametric kernel
density-based methods [6  7]. These smoothing techniques require  however  careful tuning of kernel
bandwidth parameters and are  more importantly  subject to selection biases. That is  in regions
where no events have been observed  the intensity is inferred to be zero and no measure is readily
available for a user to assess the uncertainty of such predictions. More advanced methods infer the
intensity by assuming a parameterized model of the data-generating process  such as inhomogeneous
Poisson point process models. One popular model is the log-Gaussian Cox process (LGCP) model [9]
where the intensity function is modeled as a Gaussian process [22] via a logarithmic link function
to ensure non-negativity. However  the inﬁnite dimensionality of the intensity function makes this
model computationally prohibitive and substantial effort has been devoted to develop more tractable
approximation methods based on gridding [9  13]  variational inference [15  12]  Markov chain
Monte Carlo [2] and Laplace approximations [20] for the log and other link functions. A more
fundamental problem remains in that their resulting uncertainty measures are not calibrated to the
actual out-of-sample variability of the number of events across space. Poor calibration consequently
leads to unreliable inferences of the process.
In this paper  we develop a spatially varying intensity interval with provable out-of-sample perfor-
mance guarantees. Our contributions can be summarized as follows:

• the interval reliably covers out-of-sample events with a speciﬁed probability by building on

the conformal prediction framework [19] 

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

• it is constructed using a predictive spatial Poisson model with provable out-of-sample

accuracy 

• its size appropriately increases in regions with missing data to reﬂect inherent uncertainty

and mitigate sampling biases 

• the statistical guarantees remain valid even when the assumed Poisson model is misspeciﬁed.

Thus the proposed method yields both reliable and informative predictive intervals under a wider
range of conditions than standard methods which depend on the assumed model  e.g. LGCP [9]  to
match the unknown data-generating process.
Notations: Enras “ n´1
product is denoted d.

i“1 ai denotes the sample mean of a. The element-wise Hadamard

ř

n

2 Problem formulation

Figure 1: Unknown intensity function λpxq (solid) expressed in number of counts per unit of area 
across a one-dimensional spatial domain X “ r0  200s which is discretized into 50 regions. Intensity
interval Λαpxq with 1 ´ α “ 80% out-of-sample coverage (3) inferred using n “ 50 samples.

Estimated intensity functionpλpxq (dashed). Data is missing in the regions r30  80s and r160  200s

where the intensity interval increases appropriately.
Ť
The intensity function λpxq of a spatial process is expressed as the number of events per unit area
and varies over a spatial domain of interest  X   which we equipartition into R disjoint regions:
r“1 Xr Ă Rd and is a common means of modelling continuous inhomogeneous point
X “
processes  see [9  13]. The function λpxq determines the expected number of events y P t0  . . .   Y u
that occur in region Xr by

R

ż

Xr

Ery|rs “

λpxqdx 

(1)

(2)

where r is the region index and Y is the maximum number of counts.
We observe n independent samples drawn from the process 
pri  yiq „ pprqppy|rq 

where the data-generating distribution is unknown. Let the collection of pairwise datapoints be
denoted pr  yq “ tpr1  y1q  . . .  prn  ynqu. Given this dataset  our goal is to infer an intensity interval
Λpxq Ă r0  8q of the unknown spatial point process  which predicts the number of events per unit
area at location x. See Figure 1 for an illustration in one-dimensional space. A reliable interval
Λαpxq will cover a new out-of-sample observation y in a region r with a probability of at least 1 ´ α.
That is  for a speciﬁed level α the out-of-sample coverage is
y P Λαpxq|Xr|  @x P Xr

(3)
where |Xr| is the area of the rth region. Since the trivial noninformative interval r0  8q also satisﬁes
(3)  our goal is to construct Λαpxq that is both reliable and informative.

ě 1 ´ α 

!

)

Pr

2

Inference method

3
We begin by showing that an intensity interval Λαpxq with reliable out-of-sample coverage can
be constructed using the conformal prediction framework [19]. Note that obtaining tractable and
informative intervals in this approach requires learning an accurate predictor in a computationally
efﬁcient manner. We develop such a predictor and prove that it has ﬁnite-sample and distribution-
free performance guarantees. These guarantees are independent of the manner in which space is
discretized.

3.1 Conformal intensity intervals
Let Eθry|rs denote a predictor parameterized by a vector θ. For a given region r  consider a new data
conformal prediction is to quantify how well this new point conforms to the observed data pr  yq.

point pr ryq  wherery represents number of counts and takes a value between r0  Y s. The principle of
This is done by ﬁrst ﬁtting parameters θ1 to the augmented set pr  yqYpr ryq and then using the score
ˇˇ¯
ˇˇry ´ Eθ1ry|rs
ˇˇ is statistically indistinguishable from
ˇˇry ´ Eθ1ry|rs
the rest  πpryq corresponds to a p-value [19]. On this basis we construct an intensity interval Λαpxq
by including all pointsry that conform to the dataset with signiﬁcance level α  as summarized in

(4)
where Ip¨q is the indicator function and ei “ |yi ´ Eθ1ry|ris| are residuals for all observed data
points i “ 1  . . .   n. When a new residual

Algorithm 1. Using [14  thm. 2.1]  we can prove that Λαpxq satisﬁes the out-of-sample coverage (3).

πpryq “ 1

´
ei ď

I

n`1ÿ

i“1

P p0  1s 

n ` 1

Algorithm 1 Conformal intensity interval
1: Input: Location x  signiﬁcance level α  data pr  yq
3: Set r if x P Xr

2: for allry P t0  . . .   Y u do
4: Update predictor Eθry|rs using augmented data pr  yq Y pr ryq
5: Compute score πpryq in (4)
7: Output: Λαpxq “ try : pn ` 1qπpryq ď rpn ` 1qαsu{|Xr|

6: end for

While this approach yields reliable out-of-sample coverage guarantees  there are two possible limita-
tions:

1. The residuals can be decomposed as e “ pEry|rs ´ Eθry|rsq ` ε  where the term in
brackets is the model approximation error and ε is an irreducible zero-mean error. Obtaining
informative Λαpxq across space requires learned predictors with small model approximation
errors.
2. Learning methods that are computationally demanding render the computation of Λαpxq
intractable across space  since the conformal method requires re-ﬁtting the predictor multiple
times for each region.

Next  we focus on addressing both limitations.

3.2 Spatial model
We seek an accurate model pθpy|rq of ppy|rq  parameterized by θ. For a given r  we quantify the
out-of-sample accuracy of a model by the Kullback-Leibler divergence per sample 

„



Rpθq “ 1
n

Ey|r

ln

ppy|rq
pθpy|rq

ě 0 

for which Rpθq “ 0 ô pθpy|rq ” ppy|rq

(5)

In general  the unknown intensity function underlying ppy|rq has a local spatial structure and can
be modeled as smooth since we expect counts in neighbouring regions to be similar in real-world

3

applications. On this basis  we consider following the class of models 

!
pθpy|rq is Poisson with mean Eθry|rs “ exppφJprqθq  θ P RR

)

 

Pθ “

where φprq is R ˆ 1 spatial basis vector whose components are given by the cubic b-spline function
[21] (see supplementary material). The Poisson distribution is the maximum entropy distribution
for count data and is here parameterized via a latent ﬁeld tθ1  . . .   θRu across regions [4  ch. 4.3].
Using a cubic b-spline basis [21]  we model the mean in region r via a weighted average φprqJθ
of latent parameters from neighbouring regions  where the maximum weight in φprq is less than 1.
This parameterization yields locally smooth spatial structures and is similar to using a latent process
model for the mean as in the commonly used LGCP model [9  sec. 4.1].
The unknown optimal predictive Poisson model is given by
Rpθq

θ‹ “ arg min

(6)

and has an out-of-sample accuracy Rpθ‹q.

θ

pθ “ arg min

3.3 Regularized learning criterion
We propose learning a spatial Poisson model in Pθ using the following learning criterion

´ n´1 ln pθpy|rq ` n´γ||w d θ||1 

(7)
where ln pθpy|rq is the log-likelihood  which is convex [18]  and w is a given vector of regularization
weights. The regularization term in (7) not only mitigates overﬁtting of the model by penalizing
parameters in θ individually  it also yields the following ﬁnite sample and distribution-free result.
Theorem 1 Let γ P p0  1
2q  then the out-of-sample accuracy of the learned model is bounded as

θ

´

with a probability of at least

max

0  1 ´ 2R exp

Rppθq ď Rpθ‹q ` 2n´γ||w d θ‹||1
!

)¯
  where wo “ min
k“1 ... R

´ w2

on1´2γ
2Y 2

wk.

(8)

Material. The above theorem guarantees that the out-of-sample accuracy Rppθq of the learned model

We provide an outline of the proof in Section 3.3.1  while relegating the details to the Supplementary
(7) will be close to Rpθ‹q of the optimal model (6)  even if the model class (3.2) does not contain the
true data-generating process. As γ is increased  the bound tightens and the probabilistic guarantee
weakens  but for a given data set one can readily search for the value of γ P p0  0.5q which yields the
most informative interval Λαpxq.
The ﬁrst term of (7) contains inner products φJprqθ which are formed using a regressor matrix.
To balance ﬁtting with the regularizing term in (7)  it is common to rescale all columns of the
regressor matrix to unit norm. An equivalent way is to choose the following regularization weights
wk “

a
Enr|φkprq|2s  see e.g. [3]. We then obtain a predictor as

and predictive intensity interval Λαpxq via Algorithm 1. Setting wk ” 0 in (7) yields a maximum
likelihood model with less informative intervals  as we show in the numerical experiments section.

Epθry|rs “ exppφJprqpθq
The minimizerpθ in (7) satisﬁespRppθq ď pRpθ‹q ` ρfpθ‹q ´ ρfppθq 
where pRpθq “ n´1 ln ppy|rq

3.3.1 Proof of theorem

and ρ “ n´γ.

(9)
pθpy|rq is the in-sample divergence  corresponding to (5)  fpθq “ ||w d θ||1

4

ř

θ ` 1
n

Using the functional form of the Poisson distribution  we have

´ ln pθpyi|riq “ nÿ

´ ln pθpy|rq “ nÿ
”
Rpθq ´ pRpθq “ 1
”
ln pθpy|rq ´ Ey|rrln pθpy|rqs ` Ey|rrln ppy|rqs ´ ln ppy|rq
py ´ Ey|rrysqφprq

Eθryi|ris ´ yi lnpEθryi|risq ` lnpyi!q
ı

Then the gap between the out-of-sample and in-sample divergences for any given model θ is given
by

n
“ En

ıJ

(10)

i“1

i“1

K 

´

‰

n
i“1

where

and we can therefore relate the gaps for the optimal modelpθ with the learned model θ‹ as follows:

where the second line follows from using our Poisson model Pθ and K “ Ey|rrln ppy|rqs ´
Ey|rrlnpyi!qs ´ lnpyi!q is a constant. Note that the divergence gap is linear in θ 
ln ppy|rq `
“
“
‰
Rppθq ´ pRppθq
Rpθ‹q ´ pRpθ‹q
”
ˇˇ
g ” BθrRpθq ´ pRpθqs
θ“pθ “
Enrz1s  . . .   EnrzRs

“ gJpθ‹ ´pθq 
ıJ

is the gradient of (10) and we introduce the random variable zk “ py ´ Ey|rrysqφkprq P r´Y  Y s
for notational simplicity (see supplementary material).
Inserting (9) into (11) and re-arranging yields

Rppθq ď Rpθ‹q ´ gJpθ‹ ´pθq ` ρfpθ‹q ´ ρfppθq 

where the RHS is dependent onpθ. Next  we upper bound the RHS by a constant that is independent
ofpθ.

(11)

(12)

 

The weighted norm fpθq has an associated dual norm
gJθ ” ||g||8

rfpgq “ sup
´gJθ‹ ď rfpgqfpθ‹q

θ:fpθqď1

“ max
k“1 ... R

|Enrzks|

wo

wo

and gJpθ ď rfpgqfppθq

see the supplementary material. Using the dual norm  we have the following inequalities

and combining them with (12)  as in [23]  yields

Rppθq ď Rpθ‹q ` pρ ` rfpgqqfpθ‹q ` prfpgq ´ ρqfppθq ď Rpθ‹q ` 2ρfpθ‹q

when ρ ě rfpgq. The probability of this event is lower bounded by

`

˘
ρ ě rfpgq

Pr

ě 1 ´ 2R exp

We derive this bound using Hoeffding’s inequality  for which

“

´
and Erzks “ Er

Prp|Enrzks ´ Erzks| ď q ě 1 ´ 2 exp
¯
“ 0. Moreover 

‰
´ Rč
pEy|rrys ´ Ey|rrysqφkprq
“ Pr

|Enrzks| ď 

|Enrzks| ď 

¯

Pr

max

k“1 ... R

k“1

”

ı

 

´ n2
2Y 2

ě 1 ´ 2R exp

using DeMorgan’s law and the union bound (see supplementary material). Setting  “ woρ  we
obtain (14) Hence equation (13) and (14) prove Theorem 1. It can be seen that for γ P p0  1
2q  the
probability bound on the right hand side increases with n.

”

ı

on1´2γ
´ w2
2Y 2
”

´ n2
2Y 2

ı

 

(13)

(14)

(15)

5

3.3.2 Minimization algorithm

(16)

V pθq ` n´γfpθq ď Qpθ;rθq ` n´γfpθq 

To solve the convex minimization problem (7) in a computationally efﬁcient manner  we use
a majorization-minimization (MM) algorithm. Speciﬁcally  let V pθq “ ´n´1 ln pθpy|rq and
fpθq “ ||w d θ||1 then we bound the objective in (7) as

where Qpθ;rθq is a quadratic majorizing function of V pθq such that Qprθ;rθq “ V prθq  see [18  ch. 5].
when updating the predictor Eθry|rs with an augmented dataset pr  yq Y pr ryq. This renders the

Minimizing the right hand side of (16) takes the form of a weighted lasso regression and can therefore
be solved efﬁciently using coordinate descent. The pseudo-code is given in Algorithm 2  see the
supplementary material for details. The runtime of Algorithm 2 scales as OpnR2q i.e. linear in
number of datapoints n. This computational efﬁciency of Algorithm 2 is leveraged in Algorithm 1
computation of Λαpxq tractable across space.
Algorithm 2 Majorization-minimization method
1: Input: Data pr  yq  parameter γ P p0  1
2: Form weights wk “

a
Qpθ;rθq ` n´γ||w d θ||1 using coordinate descent

3: Setrθ :“ 0
5: Form quadratic approximation atrθ: Qpθ;rθq ` n´γ||w d θ||1
6: Solveqθ :“ arg min
7: rθ :“qθ
8: until ||pθ ´qθ|| ě 
9: Output:pθ “qθ and Epθry|rs “ exppφJprqpθq

Enr|φkprq|2s for k “ 1  . . .   R

2q and Y

4: while

θ

The code for algorithms 1 and 2 are provided on github.

4 Numerical experiments

We demonstrate the proposed method using both synthetic and real spatial data.

4.1 Synthetic data with missing regions

¯

´

λpxq “ 10 exp

To illustrate the performance of our learning criterion in (7)  we begin by considering a one-
dimensional spatial domain X “ r0  100s  equipartitioned into R “ 20 regions. Throughout we
use γ “ 0.499 in (7).
Comparison with log-Gaussian Cox process model
We consider a process described by the intensity function
´ x
50

and sample events using a spatial Poisson process model using inversion sampling [5]. The distribution
ppy|rq is then Poisson. Using a realization pr  yq  we compare our predictive intensity interval Λαpxq

with a p1 ´ αq%-credibility intervalrΛαpxq obtained by assuming an LGCP model for the λpxq [9]
r70  100s  respectively. Figures 2a and 2b show the intervals both cases. WhilerΛαpxq is tighter than
away from the observed data regions. By contrast rΛαpxq provides misleading inferences in this case.

and approximating its posterior belief distribution using integrated nested Laplace approximation
(INLA) [17  11]. For the cubic b-splines in Pθ  the spatial support of the weights in φprq was set to
cover all regions.
We consider interpolation and extrapolation cases where the data is missing across r30  80s and
Λαpxq in the missing data regions  it has no out-of-sample guarantees and therefore lacks reliability.
This is critically evident in the extrapolation case  where Λαpxq becomes noninformative further

(17)

 

6

(b) Extrapolation with data miss-
ing in r70  100s

(c) Average interval size with data
missing in r50  90s

(a) Interpolation with data miss-
ing in r30  80s

Figure 2: (a) Interpolation and (b) extrapolation with Λαpxq (grey) andrΛαpxq (green) with 1 ´ α “

80%  for a given realization of point data (black crosses). The unknown intensity function λpxq
(red) gives the expected number of events in a region  see (1). (c) Misspeciﬁed case with average
intensity interval size |Λαpxq|  using nonzero (blue) and zero (red) regularization weights in (7). Data
in r50  90s is missing. The different markers correspond to three different spatial processes  with
intensity functions λ1pxq  λ2pxq and λ3pxq. The out-of-sample coverage (3) was set to be at least
1 ´ α “ 80% and the empirical coverage is given in 1.

Empirical coverage of Λαpxq [%]

Proposed Unregularized

α “ 0.2

λ1
λ2
λ3

97.05
91.05
81.37

97.37
98.32
95.37

Table 1: Comparison of empirical coverage of Λαpxq  using the proposed regularized vs.
unregularized maximum likelihood method. We target ě 1 ´ α “ 80% coverage.

the

Comparison with unregularized maximum likelihood model
Next  we consider a three different spatial processes  described by intensity functions

“

‰

λ1pxq “

500?
2π252

exp

´ px ´ 50q2
2 ˆ 252

  λ2pxq “ 5 sinp 2π
50

xq ` 5  λ3pxq “ 3
8

?

x.

For the ﬁrst process  the intensity peaks at x “ 50  the second process is periodic with a period of 50
spatial units  and for the third process the intensity grows monotonically with space x. In all three
cases  the number of events in a given region is then drawn as y „ ppy|rq using a negative binomial
distribution  with mean given by (1) and number of failures set to 100  yielding a dataset pr  yq. Note
that the Poisson model class Pθ is misspeciﬁed here.
We set the nominal out-of-sample coverage ě 80% and compare the interval sizes |Λαpxq| across
space and the overall empirical coverage  when using regularized and unregularized criteria (7) 
respectively. The averages are formed using 50 Monte Carlo simulations.
Figure 2c and Table 1 summarize the results of comparison between the regularized and unregularized
approaches for the three spatial processes. While both intervals exhibit the same out-of-sample
coverage (table 1)  the unregularized method results in intervals that are nearly four times larger than
those of the proposed method (ﬁgure 2c) in the missing region.

4.2 Real data

In this section we demonstrate the proposed method using two real spatial data sets.
In two-
dimensional space it is challenging to illustrate a varying interval Λαpxq  so for clarity we show its
maximum value  minimium value and size as well as compare it with a point estimate obtained from
the predictor  i.e. 

pλpxq “ Rÿ

r“1

Ipx P XrqEpθry|rs
|Xr|

(18)

Throughout we use γ “ 0.4 in (7).

7

02040608010001020304050(a)pλpxq

Figure 3: # trees per m2. Nominal coverage set to 1 ´ α “ 80%. The dashed boxes mark missing
data regions.

(b) max Λαpxq

(c) min Λαpxq

(a) |Λαpxq|

(b) |rΛαpxq|

Figure 4: # trees per m2. Comparison between proposed intensity interval and credibility intensity
interval from approximate posterior of LGCP model.

Hickory tree data
First  we consider the hickory trees data set [1] which consists of coordinates of hickory trees in a
spatial domain X Ă R2  shown in Figure 3a  that is equipartitioned into a regular lattice of R “ 52
hexagonal regions. The dataset pr  yq contains the observed number of trees in each region. The
dashed boxes indicate regions data inside which is considered to be missing. For the cubic b-splines
in Pθ  the spatial support was again set to cover all regions.

We observe that the point predictorpλpxq interpolates and extrapolates smoothly across regions and
p1´ αq% credibility interval |rΛαpxq| using the LGCP model as above  cf. Figures 4a and 4b. We note
|rΛαpxq| is virtually unchanged in contrast to |Λαpxq|. While |rΛαpxq| appears relatively tighter than

appears to visually conform to the density of the point data. Figures 3b and 3c provide important
complementary information using Λαpxq  whose upper limit increases in the missing data regions 
especially when extrapolating in the bottom-right corner  and lower limit rises in the dense regions.
The size of the interval |Λαpxq| quantiﬁes the predictive uncertainty and we compare it to the

that the sizes increase in different ways for the missing data regions. For the top missing data region 
|Λαpxq| across the bottom-right missing data regions  the credible interval lacks any out-of-sample
guarantees that would make the prediction reliable.
Crime data
Next we consider crime data in Portland police districts [16  10] which consists of locations of
calls-of-service received by Portland Police between January and March 2017 (see ﬁgure 5a). The
spatial region X Ă R2 is equipartitioned into a regular lattice of R “ 494 hexagonal regions.
The dataset pr  yq contains the reported number of crimes in each region. The support of the cubic
b-spline is taken to be 12 km.

The point predictionpλpxq is shown in Figure 5a  while Figures 5b and 5c plot the upper and lower
limits of Λαpxq  respectively. We observe thatpλpxq follows the density of the point pattern well 

predicting a high intensity of approximately 60 crimes/km2 in the center. Moreover  upper and lower
limits of Λαpxq are both high where point data is dense. The interval tends to being noninformative
for regions far away from those with observed data  as is visible in the top-left corner when comparing
Figures 5b and 5c.

8

010020005010015020025000.0050.010.0150.02010020005010015020025000.0050.010.0150.020.0250.03010020005010015020025000.0050.010.0150.020.0250.03010020005010015020025000.0050.010.0150.020.0250.03010020005010015020025000.0050.010.0150.020.0250.03(a)pλpxq

(b) max Λαpxq

(c) min Λαpxq

Figure 5: # crimes per km2 in Portland  USA. Nominal coverage set to 1 ´ α “ 80%.

5 Conclusion

We have proposed a method for inferring predictive intensity intervals for spatial point processes. The
method utilizes a spatial Poisson model with an out-of-sample accuracy guarantee and the resulting
interval has an out-of-sample coverage guarantee. Both properties hold even when the model is
misspeciﬁed. The intensity intervals provide a reliable and informative measure of uncertainty of the
point process. Its size is small in regions with observed data and grows along missing regions further
away from data. The proposed regularized learning criterion also leads to more informative intervals
as compared to an unregularized maximum likelihood approach  while its statistical guarantees
renders it reliable in a wider range of conditions than standard methods such as LGCP inference. The
method was demonstrated using both real and synthetic data.

Acknowledgments
The work was supported by the Swedish Research Council (contract numbers 2017 ´ 04610 and
2018 ´ 05040).

References
[1] P. J. Diggle @ lancaster university. https://www.lancaster.ac.uk/staff/diggle/

pointpatternbook/datasets/.

[2] R. P. Adams  I. Murray  and D. J. MacKay. Tractable nonparametric bayesian inference
in poisson processes with gaussian process intensities. In Proceedings of the 26th Annual
International Conference on Machine Learning  pages 9–16. ACM  2009.

[3] A. Belloni  V. Chernozhukov  and L. Wang. Square-root lasso: pivotal recovery of sparse signals

via conic programming. Biometrika  98(4):791–806  2011.

[4] N. Cressie and C. K. Wikle. Statistics for spatio-temporal data. John Wiley & Sons  2015.

[5] L. Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th

conference on Winter simulation  pages 260–265. ACM  1986.

[6] P. J. Diggle. A kernel method for smoothing point process data. Journal of the Royal Statistical

Society: Series C (Applied Statistics)  34(2):138–147  1985.

[7] P. J. Diggle. Statistics analysis of spatial point patterns. Hodder Education Publishers  2003.

[8] P. J. Diggle. Statistical analysis of spatial and spatio-temporal point patterns. Chapman and

Hall/CRC  2013.

[9] P. J. Diggle  P. Moraga  B. Rowlingson  B. M. Taylor  et al. Spatial and spatio-temporal log-
gaussian cox processes: extending the geostatistical paradigm. Statistical Science  28(4):542–
563  2013.

[10] S. Flaxman  M. Chirico  P. Pereira  and C. Loefﬂer. Scalable high-resolution forecasting of
sparse spatiotemporal events with kernel methods: a winning solution to the nij" real-time crime
forecasting challenge". arXiv preprint arXiv:1801.02858  2018.

9

76357640764576506806816826836846850010203040506076357640764576506806816826836846850204060801001207635764076457650680681682683684685020406080100120[11] J. B. Illian  S. H. Sørbye  and H. Rue. A toolbox for ﬁtting complex spatial point process models
using integrated nested laplace approximation (inla). The Annals of Applied Statistics  pages
1499–1530  2012.

[12] S. John and J. Hensman. Large-scale cox process inference using variational fourier features.

2018.

[13] O. O. Johnson  P. J. Diggle  and E. Giorgi. A spatially discrete approximation to log-gaussian
cox processes for modelling aggregated disease count data. arXiv preprint arXiv:1901.09551 
2019.

[14] J. Lei  M. G’Sell  A. Rinaldo  R. J. Tibshirani  and L. Wasserman. Distribution-free predictive
inference for regression. Journal of the American Statistical Association  113(523):1094–1111 
2018.

[15] C. Lloyd  T. Gunter  M. Osborne  and S. Roberts. Variational inference for gaussian process
modulated poisson processes. In International Conference on Machine Learning  pages 1814–
1822  2015.

[16] National Institute of Justice. Real-time crime forecasting challenge posting. https://nij.

gov/funding/Pages/fy16-crime-forecasting-challenge-document.aspx#data.

[17] H. Rue  S. Martino  and N. Chopin. Approximate bayesian inference for latent gaussian models
by using integrated nested laplace approximations. Journal of the royal statistical society:
Series b (statistical methodology)  71(2):319–392  2009.

[18] R. Tibshirani  M. Wainwright  and T. Hastie. Statistical learning with sparsity: the lasso and

generalizations. Chapman and Hall/CRC  2015.

[19] V. Vovk  A. Gammerman  and G. Shafer. Algorithmic learning in a random world. Springer

Science & Business Media  2005.

[20] C. J. Walder and A. N. Bishop. Fast bayesian intensity estimation for the permanental process.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70  pages
3579–3588. JMLR. org  2017.

[21] L. Wasserman. All of nonparametric statistics. Springer Science & Business Media  2006.

[22] C. K. Williams and C. E. Rasmussen. Gaussian processes for machine learning  volume 2. MIT

Press Cambridge  MA  2006.

[23] R. Zhuang and J. Lederer. Maximum regularized likelihood estimators: A general prediction

theory and applications. Stat  7(1):e186  2018.

10

,Muhammad Osama
Dave Zachariah
Peter Stoica