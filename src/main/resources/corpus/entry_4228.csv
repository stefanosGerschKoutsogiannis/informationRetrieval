2010,Parametric Bandits: The Generalized Linear Case,We consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large  or even infinite. We pro- pose a new optimistic  UCB-like  algorithm for non-linearly parameterized bandit problems using the Generalized Linear Model (GLM) framework. We analyze the regret of the proposed algorithm  termed GLM-UCB  obtaining results similar to those recently proved in the literature for the linear regression case. The analysis also highlights a key difficulty of the non-linear case which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover  as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice  we provide an asymptotic argument leading to significantly faster convergence. Simulation studies on real data sets illustrate the performance and the robustness of the proposed GLM-UCB approach.,Parametric Bandits:

The Generalized Linear Case

Sarah Filippi

LTCI

Paris  France

Telecom ParisTech et CNRS

Olivier Capp´e

LTCI

Telecom ParisTech et CNRS

Paris  France

filippi@telecom-paristech.fr

cappe@telecom-paristech.fr

Aur´elien Garivier

LTCI

Telecom ParisTech et CNRS

Paris  France

Csaba Szepesv´ari
RLAI Laboratory

University of Alberta
Edmonton  Canada

garivier@telecom-paristech.fr

szepesva@ualberta.ca

Abstract

We consider structured multi-armed bandit problems based on the Generalized
Linear Model (GLM) framework of statistics. For these bandits  we propose a new
algorithm  called GLM-UCB. We derive ﬁnite time  high probability bounds on
the regret of the algorithm  extending previous analyses developed for the linear
bandits to the non-linear case. The analysis highlights a key difﬁculty in generaliz-
ing linear bandit algorithms to the non-linear case  which is solved in GLM-UCB
by focusing on the reward space rather than on the parameter space. Moreover  as
the actual effectiveness of current parameterized bandit algorithms is often poor in
practice  we provide a tuning method based on asymptotic arguments  which leads
to signiﬁcantly better practical performance. We present two numerical experi-
ments on real-world data that illustrate the potential of the GLM-UCB approach.
Keywords: multi-armed bandit  parametric bandits  generalized linear models 
UCB  regret minimization.

1

Introduction

In the classical K-armed bandit problem  an agent selects at each time step one of the K arms and
receives a reward that depends on the chosen action. The aim of the agent is to choose the sequence
of arms to be played so as to maximize the cumulated reward. There is a fundamental trade-off
between gathering experimental data about the reward distribution (exploration) and exploiting the
arm which seems to be the most promising.

In the basic multi-armed bandit problem  also called the independent bandits problem  the
rewards are assumed to be random and distributed independently according to a probability
distribution that is speciﬁc to each arm –see [1  2  3  4] and references therein. Recently  structured
bandit problems in which the distributions of the rewards pertaining to each arm are connected
by a common unknown parameter have received much attention [5  6  7  8  9]. This model is
motivated by the many practical applications where the number of arms is large  but the payoffs are
interrelated. Up to know  two different models were studied in the literature along these lines. In
one model  in each times step  a side-information  or context  is given to the agent ﬁrst. The payoffs
of the arms depend both on this side information and the index of the arm. Thus the optimal arm
changes with the context [5  6  9]. In the second  simpler model  that we are also interested in here 
there is no side-information  but the agent is given a model that describes the possible relations

1

between the arms’ payoffs. In particular  in “linear bandits” [10  8  11  12]  each arm a ∈ A is
associated with some d-dimensional vector ma ∈ Rd known to the agent. The expected payoffs
of the arms are given by the inner product of their associated vector and some ﬁxed  but initially
aθ∗  which is linear in θ∗.1
unknown parameter vector θ∗. Thus  the expected payoff of arm a is m′
In this article  we study a richer generalized linear model (GLM) in which the expectation of
the reward conditionally to the action a is given by µ(m′
aθ∗)  where µ is a real-valued  non-linear
function called the (inverse) link function. This generalization allows to consider a wider class
of problems  and in particular cases where the rewards are counts or binary variables using 
respectively  Poisson or logistic regression. Obviously  this situation is very common in the ﬁelds of
marketing  social networking  web-mining (see example of Section 5.2 below) or clinical studies.

Our ﬁrst contribution is an “optimistic” algorithm  termed GLM-UCB  inspired by the Upper Con-
ﬁdence Bound (UCB) approach [2]. GLM-UCB generalizes the algorithms studied by [10  8  12].
Our next contribution are ﬁnite-time bounds on the statistical performance of this algorithm. In
particular  we show that the performance depends on the dimension of the parameter but not on the
number of arms  a result that was previously known in the linear case. Interestingly  the GLM-UCB
approach takes advantage of the particular structure of the parameter estimate of generalized linear
models and operates only in the reward space. In contrast  the parameter-space conﬁdence region
approach adopted by [8  12] appears to be harder to generalize to non-linear regression models.
Our second contribution is a tuning method based on asymptotic arguments. This contribution
addresses the poor empirical performance of the current algorithms that we have observed for small
or moderate sample-sizes when these algorithms are tuned based on ﬁnite-sample bounds.

The paper is organized as follows. The generalized linear bandit model is presented in Section 2 
together with a brief survey of needed statistical results. Section 3 is devoted to the description
of the GLM-UCB algorithm  which is compared to related approaches. Section 4 presents our
regret bounds  as well as a discussion  based on asymptotic arguments  on the optimal tuning of the
method. Section 5 reports the results of two experiments on real data sets.

2 Generalized Linear Bandits  Generalized Linear Models

We consider a structured bandit model with a ﬁnite  but possibly very large  number of arms. At
each time t  the agent chooses an arm At from the set A (we shall denote the cardinality of A by K).
The prior knowledge available to the agent consists of a collection of vectors {ma}a∈A of features
which are speciﬁc to each arm and a so-called (inverse) link function µ : R → R.

The generalized linear bandit model investigated in this work is based on the assumption that
the payoff Rt received at time t is conditionally independent of the past payoffs and choices and it
satisﬁes

At θ∗)  

E [ Rt| At] = µ(m′

(1)
for some unknown parameter vector θ∗ ∈ Rd. This framework generalizes the linear bandit model
considered by [10  8  12]. Just like the linear bandit model builds on linear regression  our model
capitalizes on the well-known statistical framework of Generalized Linear Models (GLMs). The
advantage of this framework is that it allows to address various  speciﬁc reward structures widely
found in applications. For example  when rewards are binary-valued  a suitable choice of µ is
µ(x) = exp(x)/(1 + exp(x))  leading to the logistic regression model. For integer valued rewards 
the choice µ(x) = exp(x) leads to the Poisson regression model. This can be easily extended to the
case of multinomial (or polytomic) logistic regression  which is appropriate to model situations in
which the rewards are associated with categorical variables.

To keep this article self-contained  we brieﬂy review the main properties of GLMs [13]. A
univariate probability distribution is said to belong to a canonical exponential family if its density
with respect to a reference measure is given by

(2)
where β is a real parameter  c(·) is a real function and the function b(·) is assumed to be twice
continuously differentiable. This family contains the Gaussian and Gamma distributions when the
reference measure is the Lebesgue measure and the Poisson and Bernoulli distributions when the

pβ(r) = exp (rβ − b(β) + c(r))  

1Throughout the paper we use the prime to denote transposition.

2

reference measure is the counting measure on the integers. For a random variable R with density
deﬁned in (2)  E(R) = ˙b(β) and Var(R) = ¨b(β)  where ˙b and ¨b denote  respectively  the ﬁrst and
second derivatives of b. In addition  ¨b(β) can also be shown to be equal to the Fisher information
matrix for the parameter β. The function b is thus strictly convex.

Now  assume that  in addition to the response variable R  we have at hand a vector of covariates
X ∈ Rd. The canonical GLM associated to (2) postulates that pθ(r|x) = px′θ(r)  where θ ∈ Rd
is a vector of parameter. Denote by µ = ˙b the so-called inverse link function. From the properties
of b  we know that µ is continuously differentiable  strictly increasing  and thus one-to-one. The
maximum likelihood estimator ˆθt  based on observations (R1  X1)  . . . (Rt−1  Xt−1)  is deﬁned as
the maximizer of the function

t−1

t−1

Xk=1

log pθ(Rk|Xk) =

RkX ′

kθ − b(X ′

kθ) + c(Rk)  

Xk=1

a strictly concave function in θ.2 Upon differentiating  we obtain that ˆθt is the unique solution of
the following estimating equation

t−1

Xk=1

(Rk − µ(X ′

kθ)) Xk = 0  

(3)

where we have used the fact that µ = ˙b. In practice  the solution of (3) may be found efﬁciently
using  for instance  Newton’s algorithm.

A semi-parametric version of the above model is obtained by assuming only that Eθ[R|X] =
µ(X ′θ) without (much) further assumptions on the conditional distribution of R given X. In this
case  the estimator obtained by solving (3) is referred to as the maximum quasi-likelihood estimator.
It is a remarkable fact that this estimator is consistent under very general assumptions as long as the
k tends to inﬁnity [14]. As we will see  this matrix also plays a crucial
role in the algorithm that we propose for bandit optimization in the generalized linear bandit model.

k=1 XkX ′

design matrixPt−1

3 The GLM-UCB Algorithm

According to (1)  the agent receives  upon playing arm a  a random reward whose expected value is
aθ∗)  where θ∗ ∈ Θ is the unknown parameter. The parameter set Θ is an arbitrary closed subset
µ(m′
of Rd. Any arm with largest expected reward is called optimal. The aim of the agent is to quickly ﬁnd
ˆθt)
an optimal arm in order to maximize the received rewards. The greedy action argmaxa∈A µ(m′
a
may lead to an unreliable algorithm which does not sufﬁciently explore to guarantee the selection of
an optimal arm. This issue can be addressed by resorting to an “optimistic approach”. As described
by [8  12] in the linear case  an optimistic algorithm consists in selecting  at time t  the arm

At = argmax

a

max

θ

Eθ [ Rt | At = a] s.t. kθ − ˆθtkMt ≤ ρ(t)  

where ρ is an appropriate  “slowly increasing” function 

t−1

(4)

(5)

Mt =

mAk m′

Ak

Xk=1

is the design matrix corresponding to the ﬁrst t − 1 timesteps and kvkM = √v′M v denotes the
matrix norm induced by the positive semideﬁnite matrix M . The region kθ − ˆθtkMt ≤ ρ(t) is
a conﬁdence ellipsoid around the estimated parameter ˆθt. Generalizing this approach beyond the
case of linear link functions looks challenging. In particular  in GLMs  the relevant conﬁdence
regions may have a more complicated geometry in the parameter space than simple ellipsoids. As
a consequence  the beneﬁts of this form of optimistic algorithms appears dubious.3

2Here  and in what follows log denotes the natural logarithm.
3Note that maximizing µ(m′

aθ over the
same region since µ is strictly increasing. Thus  computationally  this approach is not more difﬁcult than it is
for the linear case.

aθ) over a convex conﬁdence region is equivalent to maximizing m′

3

An alternative approach consists in directly determining an upper conﬁdence bound for the

expected reward of each arm  thus choosing the action a that maximizes

Eˆθt

[ Rt | At = a] + ρ(t)kmakM −1

t

.

In the linear case the two approaches lead to the same solution [12]. Interestingly  for non-linear
bandits  the second approach looks more appropriate.

In the rest of this section  we apply this second approach to the GLM bandit model deﬁned in (1).
According to (3)  the maximum quasi-likelihood estimator of the parameter in the GLM is the
unique solution of the estimating equation

t−1

Xk=1(cid:16)Rk − µ(m′

Ak

ˆθt)(cid:17) mAk = 0  

where A1  . . .   At−1 denote the arms played so far and R1  . . .   Rt−1 are the corresponding rewards.
Ak θ)mAk be the invertible function such that the estimated parameter ˆθt
k=1 RkmAk. Since ˆθt might be outside of the set of admissible parameters Θ 

k=1 µ(m′

(6)

.

(7)

we “project it” to Θ  to obtain ˜θt:

Let gt(θ) = Pt−1
satisﬁes gt(ˆθt) = Pt−1
θ∈Θ (cid:13)(cid:13)(cid:13)

˜θt = argmin

gt(θ) − gt(ˆθt)(cid:13)(cid:13)(cid:13)M −1

t

= argmin

θ∈Θ (cid:13)(cid:13)(cid:13)

gt(θ) −

t−1

Xk=1

RkmAk(cid:13)(cid:13)(cid:13)M −1

t

Note that if ˆθt ∈ Θ (which is easy to check and which happened to hold always in the examples we
dealt with) then we can let ˜θt = ˆθt. This is important since computing ˜θt is non-trivial and we can
save this computation by this simple check. The proposed algorithm  GLM-UCB  is as follows:

Algorithm 1 GLM-UCB
1: Input: {ma}a∈A
2: Play actions a1  . . .   ad  receive R1  . . .   Rd.
3: for t > d do
4:
5:
6:
7: end for

Estimate ˆθt according to (6)
if ˆθt ∈ Θ let ˜θt = ˆθt else compute ˜θt according to (7)
Play the action At = argmaxanµ(m′

a

˜θt) + ρ(t)kmakM −1

t o  receive Rt

t = ρ(t)kmakM −1

At time t  for each arm a  an upper bound µ(m′
a

t is computed  where the “exploration
is the product of two terms. The quantity ρ(t) is a slowly increasing
bonus” βa
function; we prove in Section 4 that ρ(t) can be set to guarantee high-probability bounds on the
t is kmakM −1
expected regret (for the actual form used  see (8)). Note that the leading term of βa
which decreases to zero as t increases.

˜θt) + βa

t

t

As we are mostly interested in the case when the number of arms K is much larger than the
dimension d  the algorithm is simply initialized by playing actions a1  . . .   ad such that the vectors
ma1 . . .   mad form a basis of M = span(ma  a ∈ A). Without loss of generality  here and in what
follows we assume that the dimension of M is equal to d. Then  by playing a1  . . .   ad in the ﬁrst
d steps the agent ensures that Mt is invertible for all t. An alternative strategy would be to initialize
M0 = λ0I  where I is the d × d identify matrix.
3.1 Discussion

The purpose of this section is to discuss some properties of Algorithm 1  and in particular the
interpretation of the role played by kmakM −1
Generalizing UCB The standard UCB algorithm for K arms [2] can be seen as a special case of
GLM-UCB where the vectors of covariates associated with the arms form an orthogonal system and
µ(x) = x. Indeed  take d = K  A = {1  . . .   K}  deﬁne the vectors {ma}a∈A as the canonical basis
{ea}a∈A of Rd  and take θ ∈ Rd the vector whose component θa is the expected reward for arm a.

.

t

4

Then  Mt is a diagonal matrix whose a-th diagonal element is the number Nt(a) of times the
a-th arm has been played up to time t. Therefore  the exploration bonus in GLM-UCB is given by
t = ˆθt(a)
t = ρ(t)/pNt(a). Moreover  the maximum quasi-likelihood estimator ˆθt satisﬁes ¯Ra
βa
for all a ∈ A  where ¯Ra
I{At=a}Rk is the empirical mean of the rewards received
while playing arm a. Algorithm 1 then reduces to the familiar UCB algorithm. In this case  it
is known that the expected cumulated regret can be controlled upon setting the slowly varying

Nt(a)Pt−1

t = 1

k=1

function ρ to ρ(t) =p2 log(t)  assuming that the range of the rewards is bounded by one [2].

Generalizing linear bandits Obviously  setting µ(x) = x  we obtain a linear bandit model. In
this case  assuming that Θ = Rd  the algorithm will reduce to those described in the papers [8  12].
In particular  the maximum quasi-likelihood estimator becomes the least-squares estimator and as
noted earlier  the algorithm behaves identically to one which chooses the parameter optimistically
within the conﬁdence ellipsoid {θ : kθ − ˆθtkMt ≤ ρ(t)}.
Dependence in the Number of Arms
In contrast to an algorithm such as UCB  Algorithm 1
does not need that all arms be played even once.4 To understand this phenomenon  observe that 
= kmak2
) for
as Mt+1 = Mt + mAtm′
any arm a. Thus the exploration bonus βa
t+1 decreases for all arms  except those which are exactly
orthogonal to mAt (in the M −1
t metric). The decrease is most signiﬁcant for arms that are colinear
to mAt. This explains why the regret bounds obtained in Theorems 1 and 2 below depend on d but
not on K.

t mAt(cid:1)2(cid:14)(1 + kmAtk2

t −(cid:0)m′

At  kmak2

aM −1

M −1
t+1

M −1

M −1

t

4 Theoretical analysis

In this section we ﬁrst give our ﬁnite sample regret bounds and then show how the algorithm can be
tuned based on asymptotic arguments.

4.1 Regret Bounds

To quantify the performance of the GLM-UCB algorithm  we consider the cumulated (pseudo)
regret deﬁned as the expected difference between the optimal reward obtained by always playing
an optimal arm and the reward received following the algorithm:

T

RegretT =

µ(m′

a∗ θ∗) − µ(m′

At θ∗) .

Xt=1

For the sake of the analysis  in this section we shall assume that the following assumptions hold:
Assumption 1. The link function µ : R → R is continuously differentiable  Lipschitz with constant
kµ and such that cµ = inf θ∈Θ a∈A ˙µ(m′

aθ) > 0.

For the logistic function kµ = 1/4  while the value of cµ depends on supθ∈Θ a∈A |m′

Assumption 2. The norm of covariates in {ma : a ∈ A} is bounded: there exists cm < ∞ such
that for all a ∈ A  kmak2 ≤ cm.

aθ|.

Finally  we make the following assumption on the rewards:

Assumption 3. There exists Rmax > 0 such that for any t ≥ 1  0 ≤ Rt ≤ Rmax holds a.s. Let
ǫt = Rt − µ(m′

At θ∗). For all t ≥ 1  it holds that E [ǫt|mAt   ǫt−1  . . .   mA2   ǫ1  mA1] = 0 a.s.

As for the standard UCB algorithm  the regret can be analyzed in terms of the difference between

the expected reward received playing an optimal arm and that of the best sub-optimal arm:

∆(θ∗) =

min

a:µ(m′

aθ∗)<µ(m′

a∗ θ∗)

µ(m′

a∗ θ∗) − µ(m′

aθ∗) .

Theorem 1 establishes a high probability bound on the regret underlying using GLM-UCB with

2kµκRmax

ρ(t) =

cµ p2d log(t) log(2 d T /δ)  

4Of course  the linear bandit algorithms also share this property with our algorithm.

(8)

5

ai  which by our previous assumption is positive.

i=1 maim′

where T is the ﬁxed time horizon  κ = p3 + 2 log(1 + 2c2
eigenvalue ofPd
Theorem 1 (Problem Dependent Upper Bound). Let s = max(1  c2
1–3  for all T ≥ 1  the regret satisﬁes:
C d2
P(cid:18)RegretT ≤ (d + 1)Rmax +
∆(θ∗)

log2 [s T ] log(cid:20) 2d T

δ (cid:21)(cid:19) ≥ 1− δ with C =

m/λ0) and λ0 denotes the smallest

m/λ0). Then  under Assumptions

32κ2R2
maxk2
µ
c2
µ

.

Note that the above regret bound depends on the true value of θ∗ through ∆(θ∗). The following

theorem provides an upper-bound of the regret independently of the θ∗.
Theorem 2 (Problem Independent Upper Bound). Let s = max(1  c2
Assumptions 1–3  for all T ≥ 1  the regret satisﬁes
P RegretT ≤ (d + 1)Rmax + Cd log [s T ]sT log(cid:20) 2dT

δ (cid:21)! ≥ 1 − δ with C =

m/λ0).

Then  under

8Rmaxkµκ

cµ

.

The proofs of Theorems 1–2 can be found in the supplementary material. The main idea is to use

the explicit form of the estimator given by (6) to show that

µ(m′

At θ∗) − µ(m′

At

(cid:12)(cid:12)(cid:12)

ˆθt)(cid:12)(cid:12)(cid:12) ≤

kµ
cµ kmAtkM −1

t−1

Xk=1

t (cid:13)(cid:13)(cid:13)

.

mAk ǫk(cid:13)(cid:13)(cid:13)M −1

t

Bounding the last term on the right-hand side is then carried out following the lines of [12].

4.2 Asymptotic Upper Conﬁdence Bound

Preliminary experiments carried out using the value of ρ(t) deﬁned equation (8)  including the
case where µ is the identity function –i.e.  using the algorithm described by [8  12]  revealed poor
performance for moderate sample sizes. A look into the proof of the regret bound easily explains
this observation as the mathematical involvement of the arguments is such that some approximations
seem unavoidable  in particular several applications of the Cauchy-Schwarz inequality  leading
to pessimistic conﬁdence bounds. We provide here some asymptotic arguments that suggest to
choose signiﬁcantly smaller exploration bonuses  which will in turn be validated by the numerical
experiments presented in Section 5.

Consider the canonical GLM associated with an inverse link function µ and assume that the
vectors of covariates X are drawn independently under a ﬁxed distribution. This random design
model would for instance describe the situation when the arms are drawn randomly from a ﬁxed
distribution. Standard statistical arguments show that the Fisher information matrix pertaining to
this model is given by J = E[ ˙µ(X ′θ∗)XX ′] and that the maximum likelihood estimate ˆθt is such
that t−1/2(ˆθt − θ∗) D−→N (0  J −1)  where D−→ stands for convergence in distribution. Moreover 
t−1Mt

−→ Σ where Σ = E[XX ′]. Hence  using the delta-method and Slutsky’s lemma
J −1) .

(µ(m′
a

a.s.

ˆθt) − µ(m′

aθ∗)) D−→N (0  ˙µ(m′

aθ∗)km′

ak−2

Σ−1km′

ak2

The right-hand variance is smaller than kµ/cµ as J (cid:23) cµΣ. Hence  for any sampling distribution
such that J and Σ are positive deﬁnite and sufﬁciently large t and small δ 

kmak−1

M −1

t

P(cid:18)kmak−1

t

M −1

(µ(m′
a

ˆθt) − µ(m′

aθ∗)) >q2kµ/cµ log(1/δ)(cid:19)

is asymptotically bounded by δ. Based on the above asymptotic argument  we postulate that using

ρ(t) = p2kµ/cµ log(t)  i.e.  inﬂating the exploration bonus by a factor of pkµ/cµ compared to

the usual UCB setting  is sufﬁcient. This is the setting used in the simulations below.

5 Experiments

To the best of our knowledge  there is currently no public benchmark available to test bandit
methods on real world data. On simulated data  the proposed method unsurprisingly outperforms
its competitors when the data is indeed simulated from a well-speciﬁed generalized linear model.
In order to evaluate the potential of the method in more challenging scenarios  we thus carried out
two experiments using real world datasets.

6

5.1 Forest Cover Type Data

In this ﬁrst experiment  we test the performance of the proposed method on a toy problem using the
“Forest Cover Type dataset” from the UCI repository. The dataset (centered and normalized with
constant covariate added  resulting in 11-dimensional vectors  ignoring all categorical variables)
has been partitioned into K = 32 clusters using unsupervised k-means. The values of the response
variable for the data points assigned to each cluster are viewed as the outcomes of an arm while the
centroid of the cluster is taken as the 11-dimensional vector of covariates characteristic of the arm.
To cast the problem into the logistic regression framework  each response variable is binarized by
associating the ﬁrst class (“Spruce/Fir”) to a response R = 1 and all other six classes to R = 0.
The proportions of responses equal to 1 in each cluster (or  in other word  the expected reward
associated with each arm) ranges from 0.354 to 0.992  while the proportion on the complete set
of 581 012 data points is equal to 0.367. In effect  we try to locate as fast as possible the cluster
that contains the maximal proportion of trees from a given species. We are faced with a 32-arm
problem in a 11-dimensional space with binary rewards. Obviously  the logistic regression model
is not satisﬁed  although we do expect some regularity with respect to the position of the cluster’s
centroid as the logistic regression trained on all data reaches a 0.293 misclassiﬁcation rate.

t

t

e
r
g
e
R

2000

1500

1000

500

 

0
0

6000

4000

2000

0

 

UCB
GLM−UCB
ε−greedy

 

1000

2000

3000

4000

5000

t

6000

7000

8000

9000 10000

 

GLM−UCB
UCB

2

4

6

8

10
arm a

12

14

16

18

Figure 1: Top: Regret of the UCB  GLM-UCB and the ǫ-greedy algorithms. Bottom: Frequencies
of the 20 best arms draws using the UCB and GLM-UCB.

First 

We compare the performance of three algorithms.

the GLM-UCB algorithm  with
parameters tuned as indicated in Section 4.2. Second  the standard UCB algorithm that ignores
the covariates. Third  an ǫ-greedy algorithm that performs logistic regression and plays the best
ˆθt)  with probability 1 − ǫ (with ǫ = 0.1). We observe in
estimated action  At = argmaxa µ(m′
a
the top graph of Figure 1 that the GLM-UCB algorithm achieves the smallest average regret by a
large margin. When the parameter is well estimated  the greedy algorithm may ﬁnd the best arm
in little time and then leads to small regrets. However  the exploration/exploitation tradeoff is not
correctly handled by the ǫ-greedy approach causing a large variability in the regret. The lower plot
of Figure 1 shows the number of times each of the 20 best arms have been played by the UCB
and GLM-UCB algorithms. The arms are sorted in decreasing order of expected reward. It can be
observed that GML-UCB only plays a small subset of all possible arms  concentrating on the bests.
This behavior is made possible by the predictive power of the covariates: by sharing information
between arms  it is possible to obtain sufﬁciently accurate predictions of the expected rewards of all
actions  even for those that have never (or rarely) been played.

7

5.2 Internet Advertisement Data

In this experiment  we used a large record of the activity of internet users provided by a major ISP.
The original dataset logs the visits to a set of 1222 pages over a six days period corresponding to
about 5.108 page visits. The dataset also contains a record of the users clicks on the ads that were
presented on these pages. We worked with a subset of 208 ads and 3.105 users. The pages (ads)
were partitioned in 10 (respectively  8) categories using Latent Dirichlet Allocation [15] applied to
their respective textual content (in the case of ads  the textual content was that of the page pointed
to by the ad’s link). This second experiment is much more challenging  as the predictive power of
the sole textual information turns out to be quite limited (for instance  Poisson regression trained on
the entire data does not even correctly identify the best arm).

The action space is composed of the 80 pairs of pages and ads categories: when a pair is chosen 
it is presented to a group of 50 users  randomly selected from the database  and the reward is the
number of recorded clicks. As the average reward is typically equal to 0.15  we use a logarithmic
link function corresponding to Poisson regression. The vector of covariates for each pair is of
it is composed of an intercept followed by the concatenation of two vectors of
dimension 19:
dimension 10 and 8 representing  respectively  the categories of the pages and the ads.
In this
problem  the covariate vectors do not span the entire space; to address this issue  it is sufﬁcient to
consider the pseudo-inverse of Mt instead of the inverse.

On this data  we compared the GLM-UCB algorithm with the two alternatives described in
Section 5.1. Figure 2 shows that GLM-UCB once again outperforms its competitors  even though
the margin over UCB is now less remarkable. Given the rather limited predictive power of the
covariates in this example  this is an encouraging illustration of the potential of techniques which
use vectors of covariates in real-life applications.

3000

t

e
r
g
e
R

2000

1000

0
 
0

UCB
GLM−UCB
ε−greedy

 

1000

2000

t

3000

4000

5000

Figure 2: Comparison of the regret of the UCB  GLM-UCB and the ǫ-greedy (ǫ = 0.1) algorithm
on the advertisement dataset.

6 Conclusions

We have introduced an approach that generalizes the linear regression model studied by [10  8  12].
As in the original UCB algorithm  the proposed GLM-UCB method operates directly in the reward
space. We discussed how to tune the parameters of the algorithm to avoid exaggerated optimism 
which would slow down learning.
In the numerical simulations  the proposed algorithm was
shown to be competitive and sufﬁciently robust to tackle real-world problems. An interesting
open problem (already challenging in the linear case) consists in tightening the theoretical results
obtained so far in order to bridge the gap between the existing (pessimistic) conﬁdence bounds and
those suggested by the asymptotic arguments presented in Section 4.2  which have been shown to
perform satisfactorily in practice.

Acknowledgments

This work was supported in part by AICML  AITF  NSERC  PASCAL2 under no216886  the
DARPA GALE project under noHR0011-08-C-0110 and Orange Labs under contract no289365.

8

References

[1] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in

Applied Mathematics  6(1):4–22  1985.

[2] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit

problem. Machine Learning  47(2):235–256  2002.

[3] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge Univ Pr  2006.
[4] J. Audibert  R. Munos  and Cs. Szepesv´ari. Tuning bandit algorithms in stochastic environ-

ments. Lecture Notes in Computer Science  4754:150  2007.

[5] C.C. Wang  S.R. Kulkarni  and H.V. Poor. Bandit problems with side observations.

Transactions on Automatic Control  50(3):338–355  2005.

IEEE

[6] J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side

information. Advances in Neural Information Processing Systems  pages 817–824  2008.

[7] S. Pandey  D. Chakrabarti  and D. Agarwal. Multi-armed bandit problems with dependent

arms. International Conference on Machine learning  pages 721–728  2007.

[8] V. Dani  T.P. Hayes  and S.M. Kakade. Stochastic linear optimization under bandit feedback.

Conference on Learning Theory  2008.

[9] S.M. Kakade  S. Shalev-Shwartz  and A. Tewari. Efﬁcient bandit algorithms for online
In Proceedings of the 25th International Conference on Machine

multiclass prediction.
learning  pages 440–447. ACM  2008.

[10] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine

Learning Research  3:397–422  2002.

[11] Y. Abbasi-Yadkori  A. Antos  and Cs. Szepesv´ari. Forced-exploration based algorithms for
In COLT Workshop on On-line Learning with Limited

playing in stochastic linear bandits.
Feedback  2009.

[12] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research  35(2):395–411  2010.

[13] P. McCullagh and J. A. Nelder. Generalized Linear Models. Chapman and Hall  1989.
[14] K. Chen  I. Hu  and Z. Ying. Strong consistency of maximum quasi-likelihood estima-
tors in generalized linear models with ﬁxed and adaptive designs. Annals of Statistics 
27(4):1155–1163  1999.

[15] David M. Blei  Andrew Y. Ng  and Michael I. Jordan. Latent Dirichlet allocation. Advances

in Neural Information Processing Systems  14:601–608  2002.

[16] V.H. De La Pena  M.J. Klass  and T.L. Lai. Self-normalized processes: exponential inequal-
ities  moment bounds and iterated logarithm laws. Annals of Probability  32(3):1902–1933 
2004.

[17] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Arxiv preprint

arXiv:0812.3465v2  2008.

9

,Anshumali Shrivastava
Peng Jiang
Fanglin Gu
Yunhai Wang
Changhe Tu
Baoquan Chen
Mete Ozay