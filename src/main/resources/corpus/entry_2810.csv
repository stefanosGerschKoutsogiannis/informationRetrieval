2019,Large Scale Markov Decision Processes with Changing Rewards,We consider Markov Decision Processes (MDPs) where the rewards are unknown and may change in an adversarial manner.  We provide an algorithm that achieves a regret bound of $O( \sqrt{\tau (\ln|S|+\ln|A|)T}\ln(T))$  where $S$ is the state space  $A$ is the action space  $\tau$ is the mixing time of the MDP  and $T$ is the number of periods. The algorithm's computational complexity  is polynomial in $|S|$ and $|A|$. We then consider a setting often encountered in practice  where the state space of the MDP is too large to allow for exact solutions. By approximating the state-action occupancy measures with a linear architecture of dimension $d\ll|S|$  we propose a modified algorithm with a computational complexity polynomial in $d$ and independent of $|S|$. We also prove a regret bound for this modified algorithm  which to the best of our knowledge  is the first $\tilde{O}(\sqrt{T})$ regret bound in the large-scale MDP setting with adversarially changing rewards.,Large Scale Markov Decision Processes with

Changing Rewards

Adrian Rivera Cardoso  He Wang

School of Industrial and Systems Engineering

Georgia Institute of Technology

adrian.riv@gatech.edu  he.wang@isye.gatech.edu

Huan Xu

Alibaba Group

huan.xu@alibaba-inc.com

Abstract

We consider Markov Decision Processes (MDPs) where the rewards are unknown
and may change in an adversarial manner. We provide an algorithm that achieves a

regret bound of O((cid:112)τ (ln|S| + ln|A|)T ln(T ))  where S is the state space  A is

the action space  τ is the mixing time of the MDP  and T is the number of periods.
The algorithm’s computational complexity is polynomial in |S| and |A|. We then
consider a setting often encountered in practice  where the state space of the MDP is
too large to allow for exact solutions. By approximating the state-action occupancy
measures with a linear architecture of dimension d (cid:28) |S|  we propose a modiﬁed
algorithm with a computational complexity polynomial in d and independent of
|S|. We also prove a regret bound for this modiﬁed algorithm  which to the best
of our knowledge  is the ﬁrst ˜O(
T ) regret bound in the large-scale MDP setting
with adversarially changing rewards.

√

1

Introduction

In this paper  we study Markov Decision Processes (hereafter MDPs) with arbitrarily varying rewards.
MDP provides a general mathematical framework for modeling sequential decision making under
uncertainty [8  24  35]. In the standard MDP setting  if the process is in some state s  the decision
maker takes an action a and receives an expected reward of r(s  a). The process then randomly enters
a new state according to some known transition probability. In particular  the standard MDP model
assumes that the decision maker has complete knowledge of the reward function r(s  a)  which does
not change over time.
Over the past two decades  there has been much interest in sequential learning and decision making
in an unknown and possibly adversarial environment. A wide range of sequential learning problems
can be modeled using the framework of Online Convex Optimization (OCO) [45  20]. In the OCO
setting  the decision maker plays a repeated game against an adversary for a given number of rounds.
At the beginning of each round indexed by t  the decision maker chooses an action at from a convex
compact set A and the adversary chooses a concave reward function rt(·)  hence a reward of rt(at)
is received. After observing the realized reward function  the decision maker chooses its next action
at+1 and so on. Since the decision maker does not know the future reward functions  its goal is to
achieve a small regret; that is  the cumulative reward earned throughout the game should be close to
the cumulative reward if the decision maker had been given the beneﬁt of hindsight to choose a ﬁxed

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

action. We can express the regret for T rounds as

T(cid:88)

rt(a) − T(cid:88)

rt(at).

Regret(T ) = max
a∈A

t=1

t=1

The OCO model has many applications such as universal portfolios [13  27  23]  online shortest path
[38]  and online submodular minimization [22]. It is also closely related with areas such as convex
optimization [21  7] and game theory [10]. There are many algorithms that guarantee sublinear regret 
e.g.  Online Gradient Descent [45]  Perturbed Follow the Leader [28]  and Regularized Follow the
Leader [37  4]. Compared with the MDP setting  the main difference is that in OCO there is no notion
of states  however the payoffs may be chosen by an adversary.
In this work  we study a general problem framework that unites MDP and OCO  which we call the
Online MDP problem. More speciﬁcally  we consider MDPs where the transition probabilities are
known but the rewards are sequentially chosen by an adversary.
We list below some canonical motivating examples that can be modeled as Online MDPs.

• Adversarial Multi-Armed Bandits with Constraints [43]: We can generalize the adversarial
multi-armed bandits problem with k arms (see Auer et al. [5]) with various constraints
such as: restricting the number of times that an arm can be chosen in a given time interval 
limiting how we switch between arms  etc. These constraints can be captured easily by
deﬁning appropriate states in the Online MDP.
• The Paging Problem [17]: Suppose we are given n pages. A memory can hold at most
k (k < n) of them. An arbitrary sequence of paging request arrives. A page request is a
hit if the associated page is in memory  and is a miss otherwise. After each request  the
decision maker may swap any page in memory by paying some cost. Note that the state of
the memory and the swapping decisions can be modeled using MDP. The decision maker’s
goal is to maximize the number of hits minus the switching costs.
• The k-Server Problem [29  17]: In this classical problem in computer science  there are k
servers  represented as points in a metric space. Requests arrive to the metric space  which
are also represented as points. As each request arrives  the decision maker can choose to
move one of the servers to the requested point. The goal is to minimize the total distance all
servers move. If the arrivals of requests are adversarial  this problem can be modeled as an
Online MDP problem  where the state represents the position of servers.

Notice that in all of the problems above  the transition probabilities are known  while the adversarial
rewards/costs are observed by the decision maker sequentially after each decision epoch. Moreover 
in each of these Online MDP problems  the size of the state space may grow exponentially with the
number k. Some other noteworthy examples are the stochastic inventory control problem [35] and
some server queuing problems [14  3].

1.1 Main Results

We propose a new computationally efﬁcient algorithm that achieves near optimal regret for the
Online MDP problem. Our algorithm is based on the (dual) linear programming formulation of
inﬁnite-horizon average reward MDPs  which uses the occupancy measure of state-action pairs
as decision variables. This approach differs from other papers that have studied the Online MDP
problem previously  see review in §1.2.

We prove that the algorithm’s regret is bounded by O(τ +(cid:112)τ T (ln|S| + ln|A|) ln(T ))  where S

denotes the state space  A denotes the action space  τ is the mixing time of the MDP  and T is the
number of periods. Notice that this regret bound depends logarithmically on the size of the state and
action space. The algorithm solves a regularized linear program in each period with poly(|S||A|)
complexity. The regret bound and the computation complexity compares favorably to the existing
methods  which are summarized in §1.2.
We then extend our results to the case where the state space S is extremely large so that poly(|S||A|)
computational complexity is impractical. We assume the state-action occupancy measures associated
with stationary policies are approximated with a linear architecture of dimension d (cid:28) |S|. We design
an approximate algorithm combining several innovative techniques for solving large scale MDPs

2

inspired by [2  3]. A salient feature of this algorithm is that its computational complexity does not
√
depend on the size of the state-space but instead on the number of features d. The algorithm has a
regret bound O(cS A(ln|S| + ln|A|)
τ T ln T )  where cS A is a problem dependent constant. To the
best of our knowledge  this is the ﬁrst ˜O(

T ) regret result for large scale Online MDPs.

√

1.2 Related Work

The history of MDP goes back to the seminal work of Bellman [6] and Howard [24] from the
1950’s. Some classic algorithms for solving MDP include policy iteration  value iteration  policy
gradient  Q-learning and their approximate versions (see [35  8  9] for an excellent discussion). In
this paper  we will focus on a relatively less used approach  which is based on ﬁnding the occupancy
measure using linear programming  as done recently in [12  39  2] to solve MDPs with static rewards
(see more details in Section 3.1). To deal with the curse of dimensionality  Chen et al. [12] uses
bilinear functions to approximate the occupancy measures and Abbasi-Yadkori et al. [2] uses a linear
approximation.
The Online MDP problem was ﬁrst studied a decade ago by [43  17]. Even-Dar et al. [17] developed

no regret algorithms where the bound scales as O(τ 2(cid:112)T ln(|A|))  where τ is the mixing time deﬁned

in §2. Their method runs an expert algorithm (e.g. Weighted Majority [31]) on every state where the
actions are the experts. However  the authors did not consider the case with large state space in their
paper. Yu et al. [43] proposed a more computationally efﬁcient algorithm using a variant of Follow
the Perturbed Leader [28]  but unfortunately their regret bound becomes O(|S||A|2τ T 3/4+). They
also considered approximation algorithm for large state space  but did not establish an exact regret
bound. The work most closely related to ours is that from Dick et al. [15]  where the authors also use
a linear programming formulation of MDP similar to ours. However  there seem to be some gaps
in the proof of their results.1 That issue aside  in order to solve large-scale MDPs  their focus is to
efﬁciently solve the quadratic sub-problems that deﬁne their iterates efﬁciently. Instead  we leverage
the linear approximation scheme introduced in [2].
Ma et al. [32] also considers Online MDPs with large state space. Under some conditions  they show
sublinear regret using a variant of approximate policy iteration  but the regret rate is left unspeciﬁed
in their paper. Zimin and Neu [44] considered a special class of MDPs called episodic MDPs and
design algorithms using the occupancy measure LP formulation. Following this line of work  Neu
et al. [34] shows that several reinforcement learning algorithms can be viewed as variant of Mirror
Descent [25]  thus one can establish convergence properties of these algorithms. In [33]  the authors
considered Online MDPs with bandit feedback and provide an algorithm based on [17]’s with regret
of O(T 2/3). Some other related work can be found in [11  30  26].
A more general problem to the Online MDP setting considered here is where the MDP transition
probabilities also change in an adversarial manner  which is beyond the scope of this paper. It
is believed that this problem is much less tractable computationally; see discussion in [16]. Yu
and Mannor [42] studied MDPs with changing transition probabilities  although [33] questions the
correctness of their result  as the regret obtained seems to have broken a lower bound. In [19]  the
authors use a sliding window approach under a particular deﬁnition of regret. Abbasi-Yadkori et
al. [1] achieved sublinear regret with changing transition probabilities when compared against a
restricted policy class.

2 Problem Formulation: Online MDP

We consider a general Markov Decision Process (MDP) with known transition probabilities but
unknown and adversarially chosen rewards. Let S denote the set of possible states  and A denote
the set of actions. (For notational simplicity  we assume the set of actions a player can take is the
same for all states  but this assumption can be relaxed easily.) At each period t ∈ [T ]  if the system is
in state st ∈ S  the decision maker chooses an action at ∈ A and collects a reward rt(st  at). Here 
rt : S × A → [−1  1] denotes a reward function for period t. We assume that the sequence of reward
1In particular  we believe the proof of Lemma 1 in [15] is incorrect. Equation (8) in their paper states that
the regret relative to a policy is equal to the sum of a sequence of vector products; however  the dimensions
of vectors involved in these dot products are incompatible. By their deﬁnition  the variable νt is a vector of
dimension |S|  which is being multiplied with a loss vector with dimension |S||A|.

3

T(cid:88)

T(cid:88)

t=1

t=1

i=1 and {ai}t−1

functions {rt}T
t=1 is initially unknown to the decision maker. The function rt is revealed only after
the action at has been chosen. We allow the sequence {rt}T
t=1 to be chosen by an adaptive adversary 
meaning rt can be chosen using the history {si}t
i=1. In particular  the adversary does
not observe the action at when choosing rt. After at is chosen  the system then proceeds to state
st+1 in the next period with probability P (st+1|st  at). We assume the decision maker has complete
knowledge of the transition probabilities given by P (s(cid:48)|s  a) : S × A → S.
Suppose that the initial state of the MDP follows s1 ∼ ν1  where ν1 is a probability distribution
over S. The objective of the decision maker is to choose a sequence of actions based on the history
of states and rewards observed  such that the cumulative reward in T periods is close to that of
the optimal ofﬂine static policy. Formally  let π denote a stationary (possibly randomized) policy:
π : S → ∆A  where ∆A is the set of probability distributions over the action set A. Let Π denote the
set of all stationary policies. We aim to ﬁnd an algorithm that minimizes

MDP-Regret(T ) (cid:44) sup
π∈Π

R(T  π)  with R(T  π) (cid:44) E[

rt(sπ

t   aπ

t )] − E[

rt(st  at)] 

(1)

where the expectations are taken with respect to random transitions of MDP and (possibly) external
randomization of the algorithm.

3 Preliminaries

t+1 be the distribution over states at time t + 1 under policy π  given by νπ
st denote the stationary distribution for policy π  which satisﬁes the linear equation νπ

s s(cid:48) (cid:44) P (s(cid:48) | s  π(s)) be the probability
Next  we provide additional notations for the MDP. Let P π
of transitioning from state s to s(cid:48) given a policy π. Let P π be an |S| × |S| matrix with entries
s s(cid:48) (∀s  s(cid:48) ∈ S). We use row vector νt ∈ ∆S to denote the probability distribution over states at
P π
t+1 = νtP π.
time t. Let νπ
Let νπ
stP π.
st = νπ
We assume the following condition on the convergence to stationary distribution  which is commonly
used in the MDP literature [see 43  17  33].
Assumption 1. There exists a real number τ ≥ 0 such that for any policy π ∈ Π and any pair of
distributions ν  ν(cid:48) ∈ ∆S  it holds that (cid:107)νP π − ν(cid:48)P π(cid:107)1 ≤ e− 1
We refer to τ in Assumption 1 as the mixing time  which measures the convergence speed to the
stationary distribution. In particular  the assumption implies that νπ
st is unique for a given policy π.
We use µ(s  a) to denote the proportion of time that the MDP visits state-action pair (s  a) in
the long run. We call µπ ∈ R|S|×|A| the occupancy measure of policy π. Let ρπ
t be the long-
run average reward under policy π when the reward function is ﬁxed to be rt every period  i.e. 
t   where πt is the policy selected by the
ρπ
t
i=1
decision maker at time t.

i )]. We deﬁne ρt (cid:44) ρπt

(cid:44) limT→∞ 1

τ (cid:107)ν − ν(cid:48)(cid:107)1.

(cid:80)T

E[rt(sπ

i   aπ

T

3.1 Linear Programming Formulation for the Average Reward MDP
Given a reward function r : S × A → [−1  1]  suppose one wants to ﬁnd a policy π that maximizes
the long-run average reward: ρ∗ = supπ limT→∞ 1
t ). Under Assumption 1  the
Markov chain induced by any policy is ergodic and the long-run average reward is independent of
the starting state (see [8]). It is well known that the optimal policy can be obtained by solving the
Bellman equation  which in turn can be written as a linear program (in the dual form):

(cid:80)T

t=1 r(sπ

t   aπ

T

Let µ∗ be an optimal solution to the LP (2). We can construct an optimal policy of the MDP by
deﬁning π∗(s  a) (cid:44)
a∈A µ∗(s  a) > 0; for states where the

µ∗(s a)

(cid:80)

(2)

µ(s(cid:48)  a) ∀s(cid:48) ∈ S

µ(s  a) = 1  µ(s  a) ≥ 0 ∀s ∈ S  ∀a ∈ A.

ρ∗ = max

µ

s∈S

a∈A

(cid:88)

µ(s  a)r(s  a)

(cid:88)
(cid:88)
(cid:88)

µ(s  a)P (s(cid:48)|s  a) =

(cid:88)
s.t. (cid:88)
(cid:88)
a∈A µ∗(s a) for all s ∈ S such that(cid:80)

a∈A

a∈A

s∈S

s∈S

a∈A

4

denominator is zero  the policy may choose arbitrary actions  since those states will not be visited in
the stationary distribution. Let ν∗
st be the stationary distribution over states under this optimal policy.
For simplicity  we will write the ﬁrst constraint of LP (2) in the matrix form as µ(cid:62)(P − B) = 0 
where B is an appropriately chosen matrix with 0-1 entries. We denote the feasible set of the above
LP as ∆M (cid:44) {µ ∈ R : µ ≥ 0  µ(cid:62)1 = 1  µ(cid:62)(P − B) = 0}. The following deﬁnition will be used in
the analysis later.
Deﬁnition 1. Let δ0 ≥ 0 be the largest real number such that for all δ ∈ [0  δ0]  the set ∆M δ (cid:44)
{µ ∈ R|S|×|A| : µ ≥ δ  µ(cid:62)1 = 1  µ(cid:62)(P − B) = 0} is nonempty.

4 A Sublinear Regret Algorithm for Online MDP

In this section  we present an algorithm for the Online MDP problem. The algorithm is very intuitive
given the LP formulation (2) for the static problem. As the rewards may change each round  the
algorithm simply treats the Online MDP problem as an Online Convex Optimization (OCO) problem
with reward functions {rt}T

t=1 and decision set ∆M .

s∈S

a∈A µ(s  a) ln(µ(s  a))

Algorithm 1 (MDP-RFTL)

input: parameter δ > 0  η > 0  regularization term R(µ) =(cid:80)
if(cid:80)

initialization: choose any µ1 ∈ ∆M δ ⊂ R|S|×|A|
for t = 1  ...T do

observe current state st

(cid:80)

a∈A µt(st  a) > 0 then

(cid:80)
choose action a ∈ A with probability
a µt(st a).
else
choose action a ∈ A with probability 1|A|
(cid:104)(cid:104)ri  µ(cid:105) − 1
end if
observe reward function rt ∈ [−1  1]|S||A|
update µt+1 ← arg maxµ∈∆M δ

(cid:80)t

µt(st a)

i=1

(cid:105)

η R(µ)

end for

µt(st a)

far (cid:80)t

At the beginning of each round t ∈ [T ]  the algorithm starts with an occupancy measure µt. If the
(cid:80)
MDP is in state st  we play action a ∈ A with probability
a µt(st a). If the denominator is 0 
the algorithm picks any action in A with equal probability. After observing reward function rt and
collecting reward rt(st  at)  the algorithm changes the occupancy measure to µt+1.
The new occupancy measure is chosen according to the Regularized Follow the Leader (RFTL)
algorithm [37  4]. RFTL chooses the best occupancy measure for the cumulative reward observed so
i=1 ri  plus a regularization term R(µ). The regularization term forces the algorithm not to
drastically change the occupancy measure from round to round. In particular  we choose R(µ) to be
the entropy function. This choice will allow us to get ln(|S||A|) dependence in the regret bound.
The complete algorithm is shown in Algorithm 1. The main result of this section is the following.
Theorem 1. Suppose {rt}T
t=1 is an arbitrary sequence of rewards such that |rt(s  a)| ≤ 1 for all
s ∈ S and a ∈ A. For T ≥ ln2(1/δ0)  the MDP-RFTL algorithm with parameters η =
 
δ = e−√
The regret bound in Theorem 1 is near optimal: a lower bound of Ω((cid:112)T ln|A|) exists for the
O(τ + τ 2(cid:112)ln(|A|)T ). Compared to their result  our bound is better by a factor of τ 3/2. However 

problem of learning with expert advice [18  20]  a special case of Online MDP where the state
space is a singleton. We note that the bound only depends logarithmically on the size of the state
space and action space. The state-of-the-art regret bound for Online MDPs is that of [17]  which is

(cid:17)
τ + 4(cid:112)τ T (ln|S| + ln|A|) ln(T )

(cid:113) T ln(|S||A|)

MDP-Regret(T ) ≤ O

τ guarantees

(cid:16)

√

T /

.

τ

5

our bound has depends on(cid:112)ln|S| + ln|A|  whereas the bound in [17] depends on(cid:112)ln|A|. Both

algorithms require poly(|S||A|) computation time  but are based on different ideas: the algorithm of
[17] is based on expert algorithms and requires computing Q-functions at each time step  whereas
our algorithm is based on RFTL. In the next section  we will show how to extend our algorithm to the
case with large state space.

4.1 Proof Idea for Theorem 1
The key to analyze our algorithm is to decompose the regret with respect to policy π ∈ Π as follows

t )]− T(cid:88)

(cid:35)

(cid:34) T(cid:88)

t − T(cid:88)

ρπ

(cid:35)

(cid:34) T(cid:88)

ρt

+

T(cid:88)

(cid:34)

T(cid:88)

E[

(cid:35)

R(T  π) =

rt(sπ

t   aπ

ρπ
t

+

ρt−E[

rt(st  at)]

.

(3)

t=1

t=1

t=1

t=1

t=1

t=1

This decomposition was ﬁrst used by [17]. We now give some intuition on why R(T  π) should be
t at time t under a policy
sublinear. By the mixing condition in Assumption 1  the state distribution νπ
π differs from the stationary distribution νπ
st by at most O(τ ). This result can be used to bound the
ﬁrst term of (3).
The second term of (3) can be related to the online convex optimization (OCO) problem through
the linear programming formulation from §3.1. Notice that ρπ
a∈A µπ(s  a)r(s  a) =

(cid:80)
t (s  a)r(s  a) = (cid:104)µπt  r(cid:105). Therefore  we have

(cid:104)µπ  r(cid:105)  and ρt =(cid:80)

t =(cid:80)

a∈A µπ

s∈S

s∈S

(cid:80)
T(cid:88)

t=1

ρπ

(cid:104)µπ  rt(cid:105) − T(cid:88)
T(cid:88)

t − T(cid:88)
(cid:80)T
t=1(cid:104)µπ  rt(cid:105)−(cid:80)T

ρt =

t=1

t=1

t=1

(cid:104)µπt  rt(cid:105) 

(4)

which is exactly the regret quantity commonly studied in the OCO problem. We are thus seeking an
t=1(cid:104)µπt  rt(cid:105). In order to achieve logarithmic
algorithm that can bound maxµπ∈∆M
dependence on |S| and |A| in Theorem 1  we apply the RFTL algorithm  regularized by the negative
entropy function R(µ). A technical challenge we faced in the analysis is that R(µ) is not Lipschitz
continuous over the feasible set ∆M . So we design the algorithm to play in a shrunk set ∆M δ for
some δ > 0 (see Deﬁnition 1)  in which R(µ) is indeed Lipschitz continuous.
For the last term in (3)  note that it is similar to the ﬁrst term  albeit more complicated: the policy π
is ﬁxed in the ﬁrst term  but the policy πt used by the algorithm is varying over time. To solve this
challenge  the key idea is to show that the policies do not change too much from round to round  so
that the third term grows sublinearly in T . To this end  we use the property of the RFTL algorithm
with a carefully chosen regularization parameter η > 0. The complete proof of Theorem 1 can be
found in Appendix A.

5 Online MDPs with Large State Space

In the previous section  we designed an algorithm for Online MDP with sublinear regret. However 
the computational complexity of our algorithm is O(poly(|S||A|)) per round. MDPs in practice often
have extremely large state space S due to the curse of dimenionality [8]  so computing the exact
solution becomes impractical. In this section  we propose an approximate algorithm that can handle
large state space.

5.1 Approximating Occupancy Measures and Regret Deﬁnition

We consider an approximation scheme introduced in [3] for standard MDPs. The idea is to use d
feature vectors (with d (cid:28) |S||A|) to approximate occupancy measures µ ∈ R|S|×|A|. Speciﬁcally 
we approximate µ ≈ Φθ where Φ is a given matrix of dimension |S||A| × d  and θ ∈ Θ (cid:44) {θ ∈ Rd
+ :
(cid:107)θ(cid:107)∞ ≤ W} for some positive constant W . As we will restrict the occupancy measures chosen by
our algorithm to satisfy µ = Φθ  the deﬁnition of MDP-regret (1) is too strong as it compares against
all stationary policies. Instead  we restrict the benchmark to be the set of policies ΠΦ that can be
represented by matrix Φ  where

ΠΦ (cid:44) {π ∈ Π : there exists µπ ∈ ∆M such that µπ = Φθ for some θ ∈ Θ}.

6

Our goal will now be to achieve sublinear Φ-MDP-regret deﬁned as

Φ-MDP-Regret(T ) (cid:44) max
π∈ΠΦ

E[

rt(sπ

t   aπ

t )] − E[

T(cid:88)

t=1

T(cid:88)

t=1

rt(st  at)] 

(5)

where the expectation is taken with respect to random state transitions of the MDP and randomization
used in the algorithm. Additionally  we want to make the computational complexity independent of
|S| and |A|.
Choice of Matrix Φ and Computation Efﬁciency. The columns of matrix Φ ∈ R|S||A|×d represent
probability distributions over state-action pairs. The choice of Φ is problem-dependent  and a
detailed discussion is beyond the scope of this paper. Abbasi-Yadkori et al. [3] shows that for many
applications such as the game of Tetris and queuing networks  Φ can be naturally chosen as a sparse
matrix  which allows constant time access to entries of Φ and efﬁcient dot product operations. We
will assume such constant time access throughout our analysis. We refer readers to [3] for further
details.

5.2 The Approximate Algorithm

The algorithm we propose is built on MDP-RFTL  but is signiﬁcantly modiﬁed in several aspects.
We start with key ideas on how and why we need to modify the previous algorithm  and then formally
present the new algorithm. To aid our analysis  we make the following deﬁnition.
Deﬁnition 2. Let ˜δ0 ≥ 0 be the largest real number such that for all δ ∈ [0  ˜δ0] the set ∆Φ
(cid:44) {µ ∈
R|S||A| : there exists θ ∈ Θ such that µ = Φθ  µ ≥ δ  µ(cid:62)1 = 1  µ(cid:62)(P − B) = 0} is nonempty. We
also write ∆Φ
M

(cid:44) ∆Φ

M 0.

M δ

t+1 ← arg maxθ∈∆Φ

M δ deﬁned above. We then use occupancy measures µΦθ∗

(cid:80)t
As a ﬁrst attempt  one could replace the shrunk set of occupancy measures ∆M δ in Algorithm 1
with ∆Φ
t+1 given by the RFTL
i=1 [(cid:104)ri  µ(cid:105) − (1/η)R(µ)]. The same proof of Theorem 1
algorithm  i.e.  θ∗
would apply and guarantee a sublinear Φ-MDP-Regret. Unfortunately  replacing ∆M δ with ∆Φ
does not reduce the time complexity of computing the iterates {µΦθ∗
t=1  which is still poly(|S||A|).
To tackle this challenge  we will not apply the RFTL algorithm exactly  but will instead obtain an
approximate solution in poly(d) time. We relax the constraints µ ≥ δ and µ(cid:62)(P − B) = 0 that
deﬁne the set ∆Φ

M δ  and add the following penalty term to the objective function:

t+1 (cid:44) Φθ∗

t }T

M δ

M δ

V (θ) (cid:44) −Ht(cid:107)(Φθ)(cid:62)(P − B)(cid:107)1 − Ht(cid:107) min{δ  Φθ}(cid:107)1.

(6)
t=1 is a sequence of tuning parameters that will be speciﬁed in Theorem 2. Let ΘΦ (cid:44)

Here  {Ht}T
{θ ∈ Θ   1(cid:62)(Φθ) = 1}. Thus  the original RFTL step in Algorithm 1 now becomes

t(cid:88)

ct η(θ)  where ct η(θ) (cid:44) t(cid:88)

i=1

i=1

(cid:20)

max
θ∈ΘΦ

(cid:21)

(cid:104)ri  Φθ(cid:105) − 1
η

Rδ(Φθ)

+ V (θ).

(7)

In the above function  we use a modiﬁed entropy function Rδ(·) as the regularization term  because
the standard entropy function has an inﬁnite gradient at the origin. More speciﬁcally  let R(s a)(µ) (cid:44)

µ(s  a) ln(µ(s  a)) be the entropy function. We deﬁne Rδ(µ) =(cid:80)

(s a) Rδ

(cid:40)

Rδ

(s a)

(cid:44)

R(s a)(µ)
R(s a)(δ) + d

dµ(s a) R(s a)(δ)(µ(s  a) − δ)

(s a)(µ(s  a))  where
if µ(s  a) ≥ δ
otherwise.

(8)

Since computing an exact gradient for function ct η(·) would take O(|S||A|) time  we solve problem
(7) by stochastic gradient ascent. The following lemma shows how to efﬁciently generate stochastic
subgradients for function ct η via sampling.

7

Ht

Lemma 1. Let q1 be any probability distribution over state-action pairs  and q2 be any probability
distribution over all states. Sample a pair (s(cid:48)  a(cid:48)) ∼ q1 and s(cid:48)(cid:48) ∼ q2. The quantity
gs(cid:48) a(cid:48) s(cid:48)(cid:48) (θ) = Φ(cid:62)rt +
− Ht
q2(s(cid:48)(cid:48))

(s(cid:48) a(cid:48))(Φθ)
satisﬁes E(s(cid:48) a(cid:48))∼q1 s(cid:48)(cid:48)∼q2[gs(cid:48) a(cid:48) s(cid:48)(cid:48) (θ)|θ] = ∇θcη t(θ) for any θ ∈ Θ. Morever  we have (cid:107)g(θ)(cid:107)2 ≤
√
t

q1(s(cid:48)  a(cid:48))
[(P − B)(cid:62)Φ]s(cid:48)(cid:48) :sign([(P − B)(cid:62)Φ]s(cid:48)(cid:48) :θ) −

η (1 + ln(W d) + | ln(δ)|)C1  w.p.1  where

Φ(s(cid:48) a(cid:48)) :I{Φ(s(cid:48) a(cid:48)) :θ ≤ δ}

d + Ht(C1 + C2) + t

ηq1(s(cid:48)  a(cid:48))

∇θRδ

t

C1 = max

(s a)∈S×A

  C2 = max
s∈S

(cid:107)Φ(s a) :(cid:107)2
q1(s  a)

(cid:107)(P − B)(cid:62)
q2(s)

: sΦ(cid:107)2

.

(9)

Putting everything together  we present the complete approximate algorithm for large state online
MDPs in Algorithm 2. The algorithm uses Projected Stochastic Gradient Ascent (Algorithm 3) as a
subroutine  which uses the sampling method in Lemma 1 to generate stochastic sub-gradients.

Algorithm 2 (LARGE-MDP-RFTL)

input: matrix Φ  parameters: η  δ > 0  convex function Rδ(µ)  SGA step-size schedule {wt}T
penalty term parameters {Ht}T
initialize: ˜θ1 ← PSGA(−Rδ(Φθ) + V (θ)  ΘΦ  w0  K0)
for t = 1  ...  T do

t=1

t=0 

observe current state st; play action a with distribution
observe rt ∈ [−1  1]|S||A|

˜θt+1 ← PSGA((cid:80)t

i=1[(cid:104)ri  Φθ(cid:105) − 1

η Rδ(Φθ)] + V (θ)  ΘΦ  wt  Kt)

(cid:80)

[Φ˜θt]+(st a)
a∈A[Φ˜θt]+(st a)

end for

Algorithm 3 Projected Stochastic Gradient Ascent: PSGA(f  X  w  K)

input: concave objective function f  feasible set X  stepsize w  x1 ∈ X
for k = 1  ...K do

compute a stochastic subgradient gk such that E[gk] = ∇f (xk) using Lemma 1
set xk+1 ← PX (xk + wg(xk))

(cid:80)K

k=1 xk

end for
output: 1
K

5.3 Analysis of the Approximate Algorithm

We establish a regret bound for the LARGE-MDP-RFTL algorithm as follows.
Theorem 2. Suppose {rt}T
T   K(t) = (cid:2)W 3/2t2d3/2τ 4(C1 + C2)T 3/2 ln(W T d)(cid:3)2  wt =
all s ∈ S and a ∈ A. For T ≥ ln2( 1
e−√
guarantees that

t=1 is an arbitrary sequence of rewards such that |rt(s  a)| ≤ 1 for
τ   δ =

)  LARGE-MDP-RFTL with parameters η =

d+Ht(C1+C2)+ t

η C1)

K(t)(t

√

dW

√

√

δ0

(cid:113) T

√
Φ-MDP-Regret(T ) ≤ O(cS A ln(|S||A|)

τ T ln(T )).

Here cS A is a problem dependent constant. The constants C1  C2 are deﬁned in Lemma 1.

A salient feature of the LARGE-MDP-RFTL algorithm is that its computational complexity in each
period is independent of the size of state space |S| or the size of action space |A|  and thus is amenable
to large scale MDPs. In particular  in Theorem 2  the number of SGA iterations  K(t)  is O(d) and
independent of |S| and |A|.
Compared to Theorem 1  we achieve a regret with similar dependence on the number of periods T
and the mixing time τ. The regret bound also depends on ln(|S|) and ln(|A|)  with an additional

8

t }T

t }T

t=1 induced by following policies {Φθ∗

constant term cS A. The constant comes from a projection problem (see details in Appendix B) and
may grow with |S| and |A| in general. But for some MDP problems  cS A can be bounded by an
absolute constant: an example is the well-known (Markovian) multi-armed bandit problem [41]. For
a more detailed discussion of the constant cS A  we refer readers to Appendix C.
Proof Idea for Theorem 2. Consider the MDP-RFTL iterates  {θ∗
t=1  and the occupancy measures
{µΦθ∗
t = Φθ∗
M δ it holds that µΦθ∗
t
for all t. Thus  following the proof of Theorem 1  we can obtain the same Φ-MDP-Regret bound in
Theorem 1 if we follow policies {Φθ∗
t takes O(poly(|S||A|)) time.
The crux the proof of Theorem 2 is to show that the {Φ˜θt}T
occupancy measures {µΦ˜θt}T
constraints of ∆Φ
the distance between µΦθ∗

t=1 iterates in Algorithm 2 induce
t=1. Since the algorithm has relaxed
M δ and thus µΦ˜θt (cid:54)= Φ˜θt. So we need to show that

t+1  and µΦ˜θt+1 is small. Using triangle inequality we have

M δ  in general we have Φ˜θt /∈ ∆Φ

t=1 that are close to {µΦθ∗

t=1. However  computing θ∗

t }T
t ∈ ∆Φ

t=1. Since θ∗

t }T

t }T

t − µΦ˜θt(cid:107)1 ≤ (cid:107)µΦθ∗

(cid:107)µΦθ∗
where P∆Φ
individually. We defer the details to Appendix B as bounding each term requires lengthy proofs.

(Φ˜θt)(cid:107)1 + (cid:107)P∆Φ
(·) denotes the Euclidean projection onto ∆Φ

(Φ˜θt) − Φ˜θt(cid:107)1 + (cid:107)Φ˜θt − µΦ˜θt(cid:107)1 

M δ. We then proceed to bound each term

t − P∆Φ

M δ

M δ

M δ

6 Conclusion

We consider Markov Decision Processes (MDPs) where the transition probabilities are known but the
rewards are unknown and may change in an adversarial manner. We provide a simple online algorithm 
which applies Regularized Follow the Leader (RFTL) to the linear programming formulation of

the average reward MDP. The algorithm achieves a regret bound of O((cid:112)τ (ln|S| + ln|A|)T ln(T )) 

where S is the state space  A is the action space  τ is the mixing time of the MDP  and T is the number
of periods. The algorithm’s computational complexity is polynomial in |S| and |A| per period.
We then consider a setting often encountered in practice  where the state space of the MDP is
too large to allow for exact solutions. We approximate the state-action occupancy measures with
a linear architecture of dimension d (cid:28) |S||A|. We then propose an approximate algorithm that
relaxes the constraints in the RFTL algorithm  and then solve the relaxed problem using stochastic
gradient descent method. The computational complexity of this approximate algorithm is indepen-
dent of the size of state space |S| and the size of action space |A|. We prove a regret bound of
√
O(cS A ln(|S||A|)
τ T ln(T )) compared to the best static policy approximated by the linear archi-
√
tecture  where cS A is a problem dependent constant. To the best of our knowledge  this is the ﬁrst
˜O(

T ) regret bound for large scale MDPs with changing rewards.

References
[1] Y. Abbasi  P. L. Bartlett  V. Kanade  Y. Seldin  and C. Szepesvári. Online learning in Markov
decision processes with adversarially chosen transition probability distributions. In Advances in
Neural Information Processing Systems  pages 2508–2516  2013.

[2] Y. Abbasi-Yadkori  P. L. Bartlett  X. Chen  and A. Malek. Large-scale Markov decision problems

via the linear programming dual. arXiv preprint arXiv:1901.01992  2019.

[3] Y. Abbasi-Yadkori  P. L. Bartlett  and A. Malek. Linear programming for large-scale Markov
In International Conference on Machine Learning  volume 32  pages

decision problems.
496–504. MIT Press  2014.

[4] J. D. Abernethy  E. Hazan  and A. Rakhlin. Competing in the dark: An efﬁcient algorithm for

bandit linear optimization. In Conference on Learning Theory  2009.

[5] P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM Journal on Computing  32(1):48–77  2002.

[6] R. Bellman. A Markovian decision process. Journal of Mathematics and Mechanics  pages

679–684  1957.

9

[7] A. Ben-Tal  E. Hazan  T. Koren  and S. Mannor. Oracle-based robust optimization via online

learning. Operations Research  63(3):628–638  2015.

[8] D. P. Bertsekas. Dynamic programming and optimal control  volume 2. Athena Scientiﬁc 

Belmont  MA  4 edition  2012.

[9] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena Scientiﬁc  Belmont 

MA  1996.

[10] N. Cesa-Bianchi and G. Lugosi. Prediction  learning  and games. Cambridge university press 

2006.

[11] K. Chatterjee. Markov decision processes with multiple long-run average objectives.

In
International Conference on Foundations of Software Technology and Theoretical Computer
Science  pages 473–484. Springer  2007.

[12] Y. Chen  L. Li  and M. Wang. Scalable bilinear pi learning using state and action features. arXiv

preprint arXiv:1804.10328  2018.

[13] T. M. Cover. Universal portfolios. Mathematical ﬁnance  1(1):1–29  1991.

[14] D. P. De Farias and B. Van Roy. The linear programming approach to approximate dynamic

programming. Operations research  51(6):850–865  2003.

[15] T. Dick  A. Gyorgy  and C. Szepesvari. Online learning in Markov decision processes with
changing cost sequences. In International Conference on Machine Learning  pages 512–520 
2014.

[16] E. Even-Dar  S. M. Kakade  and Y. Mansour. Experts in a Markov decision process. In Advances

in Neural Information Processing Systems  pages 401–408  2005.

[17] E. Even-Dar  S. M. Kakade  and Y. Mansour. Online Markov decision processes. Mathematics

of Operations Research  34(3):726–736  2009.

[18] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. Games and

Economic Behavior  29(1-2):79–103  1999.

[19] P. Gajane  R. Ortner  and P. Auer. A sliding-window algorithm for Markov decision processes

with arbitrarily changing rewards and transitions. arXiv preprint arXiv:1805.10066  2018.

[20] E. Hazan. Introduction to online convex optimization. Foundations and Trends R(cid:13) in Optimiza-

tion  2(3-4):157–325  2016.

[21] E. Hazan and S. Kale. An optimal algorithm for stochastic strongly-convex optimization. arXiv

preprint arXiv:1006.2425  2010.

[22] E. Hazan and S. Kale. Online submodular minimization. Journal of Machine Learning Research 

13(Oct):2903–2922  2012.

[23] D. P. Helmbold  R. E. Schapire  Y. Singer  and M. K. Warmuth. On-line portfolio selection

using multiplicative updates. Mathematical Finance  8(4):325–347  1998.

[24] R. A. Howard. Dynamic programming and Markov processes. John Wiley  1960.

[25] A. Juditsky  A. Nemirovski  et al. First order methods for nonsmooth convex large-scale
optimization  i: general purpose methods. Optimization for Machine Learning  pages 121–148 
2011.

[26] S. Junges  N. Jansen  C. Dehnert  U. Topcu  and J.-P. Katoen. Safety-constrained reinforcement
learning for mdps. In International Conference on Tools and Algorithms for the Construction
and Analysis of Systems  pages 130–146. Springer  2016.

[27] A. Kalai and S. Vempala. Efﬁcient algorithms for universal portfolios. Journal of Machine

Learning Research  3(Nov):423–440  2002.

10

[28] A. Kalai and S. Vempala. Efﬁcient algorithms for online decision problems. Journal of

Computer and System Sciences  71(3):291–307  2005.

[29] E. Koutsoupias. The k-server problem. Computer Science Review  3(2):105–118  2009.

[30] J. Kˇretínsk`y  G. A. Pérez  and J.-F. Raskin. Learning-based mean-payoff optimization in an

unknown mdp under omega-regular constraints. arXiv preprint arXiv:1804.08924  2018.

[31] N. Littlestone and M. K. Warmuth. The weighted majority algorithm.

Computation  108(2):212–261  1994.

Information and

[32] Y. Ma  H. Zhang  and M. Sugiyama. Online Markov decision processes with policy iteration.

arXiv preprint arXiv:1510.04454  2015.

[33] G. Neu  A. György  C. Szepesvári  and A. Antos. Online markov decision processes under

bandit feedback. IEEE Transactions on Automatic Control  59(3):676–691  2014.

[34] G. Neu  A. Jonsson  and V. Gómez. A uniﬁed view of entropy-regularized Markov decision

processes. arXiv preprint arXiv:1705.07798  2017.

[35] M. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John

Wiley & Sons  2014.

[36] A. Schrijver. Theory of linear and integer programming. John Wiley & Sons  1998.
[37] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R(cid:13)

in Machine Learning  4(2):107–194  2012.

[38] E. Takimoto and M. K. Warmuth. Path kernels and multiplicative updates. Journal of Machine

Learning Research  4(Oct):773–818  2003.

[39] M. Wang. Primal-dual pi learning: Sample complexity and sublinear run time for ergodic

Markov decision problems. arXiv preprint arXiv:1710.06100  2017.

[40] R. Weber et al. On the Gittins index for multiarmed bandits. The Annals of Applied Probability 

2(4):1024–1033  1992.

[41] P. Whittle. Multi-armed bandits and the Gittins index. Journal of the Royal Statistical Society:

Series B (Methodological)  42(2):143–149  1980.

[42] J. Y. Yu and S. Mannor. Online learning in Markov decision processes with arbitrarily changing
rewards and transitions. In 2009 International Conference on Game Theory for Networks  pages
314–322. IEEE  2009.

[43] J. Y. Yu  S. Mannor  and N. Shimkin. Markov decision processes with arbitrary reward processes.

Mathematics of Operations Research  34(3):737–757  2009.

[44] A. Zimin and G. Neu. Online learning in episodic Markovian decision processes by relative
entropy policy search. In Advances in Neural Information Processing Systems  pages 1583–1591 
2013.

[45] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In
Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pages
928–936  2003.

11

,Ian Goodfellow
Jean Pouget-Abadie
Mehdi Mirza
Bing Xu
David Warde-Farley
Sherjil Ozair
Aaron Courville
Yoshua Bengio
Adrian Rivera Cardoso
He Wang
Huan Xu