2019,First order expansion of convex regularized estimators,We consider first order expansions of convex penalized estimators in
high-dimensional regression problems with random designs. Our setting includes
linear regression and logistic regression as special cases.  For a given
penalty function $h$ and the corresponding penalized estimator $\hbeta$  we
construct a quantity $\eta$  the first order expansion of $\hbeta$  such that
the distance between $\hbeta$ and $\eta$ is an order of magnitude smaller than
the estimation error $\|\hat{\beta} - \beta^*\|$.  In this sense  the first
order expansion $\eta$ can be thought of as a generalization of influence
functions from the mathematical statistics literature to regularized estimators
in high-dimensions.  Such first order expansion implies that the risk of
$\hat{\beta}$ is asymptotically the same as the risk of $\eta$ which leads to a
precise characterization of the MSE of $\hbeta$; this characterization takes a
particularly simple form for isotropic design.  Such first order expansion also
leads to inference results based on $\hat{\beta}$.  We provide sufficient
conditions for the existence of such first order expansion for three
regularizers: the Lasso in its constrained form  the lasso in its penalized
form  and the Group-Lasso.  The results apply to general loss functions under
some conditions and those conditions are satisfied for the squared loss in
linear regression and for the logistic loss in the logistic model.,First order expansion of convex regularized

estimators

Pierre C Bellec 

Department of Statistics 

Rutgers University 

501 Hill Center 

Piscataway  NJ 08854  USA.

pierre.bellec@rutgers.edu

Arun K Kuchibhotla 
Department of Statistics 

The Wharton School 

University of Pennsylvania 
Philadelphia  PA 19104  USA.

arunku@upenn.edu

Abstract

We consider ﬁrst order expansions of convex penalized estimators in high-
dimensional regression problems with random designs. Our setting includes linear
regression and logistic regression as special cases. For a given penalty function h
and the corresponding penalized estimator ˆβ  we construct a quantity η  the ﬁrst or-
der expansion of ˆβ  such that the distance between ˆβ and η is an order of magnitude
smaller than the estimation error (cid:107) ˆβ− β∗(cid:107). In this sense  the ﬁrst order expansion η
can be thought of as a generalization of inﬂuence functions from the mathematical
statistics literature to regularized estimators in high-dimensions. Such ﬁrst order
expansion implies that the risk of ˆβ is asymptotically the same as the risk of η
which leads to a precise characterization of the MSE of ˆβ; this characterization
takes a particularly simple form for isotropic design. Such ﬁrst order expansion
also leads to inference results based on ˆβ. We provide sufﬁcient conditions for
the existence of such ﬁrst order expansion for three regularizers: the Lasso in its
constrained form  the lasso in its penalized form  and the Group-Lasso. The results
apply to general loss functions under some conditions and those conditions are
satisﬁed for the squared loss in linear regression and for the logistic loss in the
logistic model.

ˆβ = argminβ∈Rp n−1(cid:80)n

consider

Introduction. We
observations
(X1  Y1)  ...  (Xn  Yn) with responses Yi and feature vectors Xi ∈ Rp. The literature of the
past two decades has demonstrated the great success of regularized estimators that are commonly
deﬁned as solutions to regularized optimization problems of the form

problems where

observes

learning

one

(1)
where (cid:96)(· ·) is referred to as the loss (e.g. squared loss  logistic loss) and h : Rp → R is a
regularization penalty (e.g. the (cid:96)1-norm for the Lasso  the (cid:96)2 1 norm for the Group-Lasso). All tuning
parameters are included in h(·). The performance of such regularized estimators is measured in terms
of prediction error or in terms estimation error (cid:107) ˆβ − β∗(cid:107) if the data comes from a model such as
Y = Xβ∗ + ε for some noise random variable ε in linear regression or

i=1 (cid:96)(Yi  X T

i β) + h(β) 

P(Y = 1|X = x) = 1/(1 + exp(xT β∗)) = 1 − P(Y = 0|X = x)

in logistic regression  where β∗ is the unknown coefﬁcient vector. For instance  if s = (cid:107)β∗(cid:107)0 is the
sparsity of β∗ in the above model  and (Xi  Yi)i=1 ... n are iid observations with the same distribution
as (X  Y )  both the Lasso in linear regression and the logistic Lasso in logistic regression enjoy rate
optimality: (cid:107) ˆβ − β∗(cid:107)2 ≤ s log(ep/s)/n; see [35  1] or the proof of Proposition 3.4 in Appendix F
for self-contained proofs. The latter estimation bound is optimal in a minimax sense and cannot

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

be improved  and the minimax rate s log(ep/s)/n represents the scale below which uncertainty is
unavoidable by information theoretic arguments  see for instance [36  Section 5].
We are interested in providing ﬁrst order expansion of ˆβ at scales negligible compared to the minimax
estimation rate  e.g. at scales negligible compared to s log(ep/s)/n in the aforementioned sparsity
contexts. To be more precise  the results below will construct random ﬁrst order expansion η such
that η is measurable w.r.t. a much smaller sigma algebra than that generated by (Xi  Yi)i=1 ... n  and
(2)
where op(1) is a quantity that converges to 0 in probability. In other words  we provide a ﬁrst-order
expansion of ˆβ similar to an inﬂuence function expansion  cf. Section 1. This allows for understanding
bias and standard deviation of ˆβ at a ﬁner scale than simply showing that ˆβ − β∗ converge to zero at
the minimax rate. The present paper intends to answer the two questions below regarding such ﬁrst
order expansion.

for some norm (cid:107) · (cid:107)K related to the problem at hand 

K = op(1)(cid:107) ˆβ − β∗(cid:107)2

(cid:107)η − ˆβ(cid:107)2

K

(Q1) How to construct η such that (2) holds for a given convex regularized estimator such as (1)?
(Q2) How are such ﬁrst order expansions useful in high-dimensional learning problems where

convex regularized estimators (1) are commonly used?

An expansion η satisfying (2) is interesting in and by itself because it describes phenomena at a ﬁner
scale than most of the literature in high-dimensional problems which focuses on minimax prediction
and estimation bounds. More importantly  we will see in Section 4 that such ﬁrst-order expansions
lead to exact identities for the loss of estimators  and in Section 5 that such ﬁrst-order expansions can
be used for inference (i.e.  uncertainty quantiﬁcation) about the unknown coefﬁcient vector β∗.

Notation. Throughout the paper  C1  C2  C3  ... denote positive absolute constants and we write
a (cid:46) b if a ≤ Cb for some absolute constant C > 0. The Euclidean norm in Rp or in Rn is denoted
by (cid:107) · (cid:107). For any positive deﬁnite matrix A  we write (cid:107)u(cid:107)A = (cid:107)A1/2u(cid:107) for the matrix square-root
A. For matrices  (cid:107) · (cid:107)op and (cid:107) · (cid:107)F denote the operator norm and Frobenius norm. For any real a 
a+ = max(0  a). If S ⊂ {1  ...  p}  v ∈ Rp  M ∈ Rp×p then vS is the restriction (vj  j ∈ S) and
MS S is the square submatrix of M made of entries indexed in S × S.

√

n

i=1

ψ(Xi  Yi)

n(cid:88)

i=1

ψ(Xi  Yi)√

n( ˆβ − β∗) =

Inﬂuence functions and Construction of η

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) = op(1)(cid:107) ˆβ − β∗(cid:107) ⇔ (1 + op(1))

1
To answer (Q1)  we start with a recap of unregularized estimators that correspond to h(·) ≡ 0  when
p is ﬁxed as n → +∞. In this case  it is well-known that certain smoothness assumptions on the
loss such as twice differentiability [25  19] or stochastic equicontinuity [42  41] imply (for any norm 
since all norms are equivalent in Rp for ﬁxed p):

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ˆβ − β∗ − n(cid:88)
we can take η = β∗ +(cid:80) ψ(Xi  Yi)/n in (2). This representation allows us to claim asymptotic

  (3)
for some target β∗ and a mean zero function ψ(· ·) sometimes referred to as the inﬂuence function.
See [25  Theorem 3.1]  [42  Page 52]  [41  Theorem 6.17]  [19  Lemma 5.4] for details. In this case
unbiasedness and ﬂuctuations of order n−1/2 for ˆβ around β∗. It also shows that estimator ˆβ behaves
like an average and hence allows transfer of results (e.g.  central limit theorems) for averages to study
of ˆβ in terms of variance estimation  conﬁdence intervals  hypothesis testing and bootstrap.
A general study of such representation for regularized problems is lacking in the literature. [23]
is the ﬁrst work that analyzed linear regression lasso when the number of covariates p is ﬁxed and
does not change with the sample size n. In the more challenging regime where p ≥ n  Theorem 5.1
of [22] provides a ﬁrst order expansion allowing for p to diverge (almost exponentially) with n. In
the present work  we simplify and present a uniﬁed derivation of such ﬁrst order expansion result 
generalizing [22  Theorem 5.1] beyond the squared loss  beyond the (cid:96)1 penalty and beyond certain
assumptions of [22] on E[XiX T
(cid:96)(cid:48)(Yi  X(cid:62)

i (β−β∗)}2+h(β)  (4)

i ]. The derivation of (3) can be motivated by deﬁning
i β∗)X(cid:62)

i (β−β∗)+

i β∗){X(cid:62)

(cid:96)(cid:48)(cid:48)(Yi  X(cid:62)

n(cid:88)

n

n(cid:88)

i=1

1
n

˜η := argmin
β∈Rp

1
2n

i=1

2

approximation of(cid:80)n
η := argminβ∈Rp n−1(cid:80)n
where K := n−1(cid:80)n
(cid:0)β∗ + n−1(cid:80)n

:= argminβ∈Rp

i=1

with h(·) ≡ 0. Here and throughtout (cid:96)(cid:48)(y  u) and (cid:96)(cid:48)(cid:48)(y  u) represent (ﬁrst and second) partial
derivatives of (cid:96) with respect to u. The right hand side of (4) (with h(·) ≡ 0) is the quadratic
i β)/n around β = β∗ (without the term independent of β). The
ﬁnal ﬁrst order expansion η is obtained by replacing the quadratic part of the approximation by its
expectation as in the next display. Following the intuitive construction of η for the unregularized
problem  we construct a ﬁrst order expansion for the regularized problem as

i=1 (cid:96)(Yi  X(cid:62)

1
2

i β∗)X(cid:62)

i=1 (cid:96)(cid:48)(Yi  X(cid:62)

2 (β − β∗)(cid:62)K(β − β∗) + h(β)

i (β − β∗) + 1
i=1 K−1Xi(cid:96)(cid:48)(Yi  X(cid:62)

(cid:13)(cid:13)K 1/2(cid:0)β − β∗ − n−1(cid:80)n
(cid:3) . From this deﬁnition  we can write η =
E(cid:2)(cid:96)(cid:48)(cid:48)(Yi  X(cid:62)
i β∗)(cid:1)   for a function ηK(·) (depending on h(·)  K). Our
i=1 K−1Xi(cid:96)(cid:48)(Yi  X(cid:62)
(cid:80)n
(cid:107) ˆβ − ηK(β∗ + 1
i=1 (cid:96)(cid:48)(Yi  X(cid:62)

i β∗)Xi)(cid:107)K = op(1)(cid:107) ˆβ − β∗(cid:107)K.

i β∗)(cid:1)(cid:13)(cid:13)2

i β∗)XiX(cid:62)

+ h(β).

(5)

i

n

ηK
main results prove under some mild assumptions that

Comparing this with (3) we note that for the unregularized problem  ηK(β) = β is the identity.

2 Main Results: Approximation Theorem
We introduce the notion of Gaussian complexity for the following results. For any set T ⊂ Rp and a
covariance matrix Σ  the Gaussian complexity of T is given by

γ(T  Σ) := E(cid:104)

supu∈T : (cid:107)Σ1/2u(cid:107)=1 |g(cid:62)Σ1/2u|(cid:105)

(6)
where the expectation is with respect to the standard normal vector g ∼ N (0  Ip). We also need
the notion of L-subGaussianity. A random vector X is said to be L-subGaussian with respect to a
(positive deﬁnite) matrix Σ if

supu∈T

 

= E(cid:104)

This implies supu
deﬁned by (cid:107)u(cid:107)2

∀u ∈ Rp  E[exp(uT X)] ≤ exp(L2(cid:107)u(cid:107)2

(7)
P(|u(cid:62)X| ≥ t(cid:107)u(cid:107)Σ) ≤ 2 exp(−t2/(2L2)). Recall that the scaled norm (cid:107) · (cid:107)K is

Σ/2)

where (cid:107)u(cid:107)Σ = (cid:107)Σ1/2u(cid:107).

i u)2]. Consider the following assumptions:
(A1) There exists constants 0 ≤ B  B2  B3 < ∞ such that the loss satisﬁes ∀u1  u2 ∈ R ∀y 

i=1

E[(cid:96)(cid:48)(cid:48)(Yi  X(cid:62)

i β∗)(X(cid:62)

K = n−1(cid:80)n

|g(cid:62)Σ1/2u|
(cid:107)Σ1/2u(cid:107)

(cid:105)

|(cid:96)(cid:48)(cid:48)(y  u1) − (cid:96)(cid:48)(cid:48)(y  u2)|

|u1 − u2|

≤ B 

|(cid:96)(cid:48)(cid:48)(y  u1)| ≤ B2 

sup
u∈Rp

(cid:107)Σ1/2u(cid:107)2
(cid:107)K 1/2u(cid:107)2

≤ B3.

(8)

(A2) The observations (X1  Y1)  . . .   (Xn  Yn) are iid. Further X1  . . .   Xn are mean zero and

L-subGaussian with respect to their covariance Σ  i.e.  (7) holds.

Note that L in (A2) is necessarily no smaller than one  i.e.  L ≥ 1. Deﬁne the error

where

E := (cid:107) ˆβ − β∗(cid:107)K + (cid:107)η − β∗(cid:107)K

(cid:107) · (cid:107)K is the norm (cid:107)u(cid:107)K = (cid:107)K 1/2u(cid:107).

(9)
The quantity E quantiﬁes the error made by ˆβ and η in estimating β∗ with respect to the norm (cid:107) · (cid:107)K.
Bounds on (cid:107) ˆβ − β∗(cid:107)K and (cid:107)η − β∗(cid:107)K follow from the existing literature; see [35] or Proposition 3.4
and its proof in Appendix F.
Theorem 2.1. Let rn := n−1/2γ(T  Σ) and assume that rn ≤ 1. Further assume (A1) and (A2)
hold true. Then with probability at least 1 − 2e−C4nr2

1. If { ˆβ− β∗  η− β∗} ⊆ T then (cid:107) ˆβ− η(cid:107)K (cid:46) LB2B3r1/2
2. If { ˆβ − η  ˆβ − β∗  η − β∗} ⊆ T then (cid:107) ˆβ − η(cid:107)K (cid:46) B2B3L2rnE + BB3/2

n − 2e−C5 log n we have the following:
√
n)E 3/2.
√
n)E 2.
The set T mentioned in Theorem 2.1(1) are available in the literature for many convex penalties.
In the following  we will ﬁnd this for constrained lasso  penalized lasso  and group lasso (with
non-overlapping groups) under sharp conditions. We refer to [7] for slope penalty  and Negahban
et al. [35  Lemma 1] and van de Geer [39  Def. 4.4 and Theorem 4.1] where set T is presented for a
general class of penalty functions including nuclear norm  group lasso (with overlapping groups).
Proofs of Theorem 2.1 and all following results are given in the supplement. An outline Theorem 2.1 is
given in Section 6. Although Theorem 2.1 is stated under assumption (A2)  we present a deterministic
version of the result (in Section 6) that replaces rn by suprema of different stochastic processes.

n E + B1/2(B3L)3/2(1 + r3
3 L3(1 + r3
n

n

3

Yi = X T

Squared loss in the linear model. Consider (cid:96)(y  u) = (y − u)2/2 and n iid observations

Then we have K = Σ = E[X1X T
satisﬁed with B = 0 and B2 = B3 = 1. The conclusions of the Theorem 2.1 can be rewritten as

(10)
1 ] and the second derivative (cid:96)(cid:48)(cid:48) is constant. Hence condition (8) is

i β∗ + εi  and Xi is independent of εi for i = 1  . . .   n 

{ ˆβ − β∗  η − β∗} ⊆ T ⇒ (cid:107) ˆβ − η(cid:107)K (cid:46) Lr1/2
n E.
{ ˆβ − η  ˆβ − β∗  η − β∗} ⊆ T ⇒ (cid:107) ˆβ − η(cid:107)K (cid:46) L2rnE 

(11)
(12)
where E = (cid:107)Σ1/2( ˆβ − β∗)(cid:107) + (cid:107)Σ1/2(η − β∗)(cid:107). Since rn ≤ 1 (and typically rn → 0 while L stays
bounded  as we will see in the examples below)  the inequality in (12) is stronger than the inequality
in (11). In the linear model  we thus refer to inequality (11) as the “slow rate” inequality  and to (12)
as the “fast rate” one. The set T encodes the low-dimensional structure and characterizes the rate
rn through the Gaussian complexity γ(T  Σ). The fast rate inequality is granted provided that T
contains the difference (η − ˆβ) additionally to the error vectors { ˆβ − β∗  η − β∗}. Conditions that
ensure the fast rate inequality will be made explicit in Section 3.2 for the Lasso.

Logistic loss in the logistic model. The following proposition shows that (8) is again satisﬁed.
Proposition 2.2. Consider the logistic loss (cid:96)(y  u) = yu − log(1 + eu) for y ∈ {0  1}  u ∈ R.
Assume that (Xi  Yi)i=1 ... n are iid satisfying the logistic regression model
P(Yi = 1|Xi) = 1 − P(Yi = 0|Xi) = 1/(1 + exp(Xi

for some β∗ ∈ Rp with (cid:107)Σ1/2β∗(cid:107) ≤ 1.1 Assume (A2) holds. Then (8) holds with B = 1/(6
B2 = 1 and an absolute constant B3 > 0.
In this logistic model  the conclusions of Theorem 2.1 present an extra term compared to the linear
model with squared loss because the Lipschitz constant B in (8) is non-zero: Theorem 2.1 reads that
with high probability

{ ˆβ − β∗  η − β∗} ⊂ T ⇒ (cid:107) ˆβ − η(cid:107)K (cid:46) Lr1/2

(13)
{ ˆβ − η  ˆβ − β∗  η − β∗} ⊂ T ⇒ (cid:107) ˆβ − η(cid:107)K (cid:46) L2rnE + BL3(1 + r3
(14)
Similar to the case of squared loss  inequality (14) is stronger than inequality (13) when ˆβ− η belongs
in T additionally to { ˆβ − β∗  η − β∗} ⊂ T .

√
n E + B1/2L3/2(1 + r3
n)E 2.

n)E 3/2 

(cid:62)β∗)) 

√

√

3) 

n

n

3 What is the low-dimensional set T ? Application to Lasso and Group-Lasso

We now provide applications of the above result to three different penalty functions commonly used
in high-dimensional settings. Throughout this section  for any cone T ⊆ Rp  let φ(T ) be the smallest
singular value of Σ1/2 restricted to T   i.e.  φ(T ) = minu∈T :(cid:107)u(cid:107)=1 (cid:107)Σ1/2u(cid:107). Further consider

(N1) The features are normalized such that Σjj ≤ 1 for all 1 ≤ j ≤ p.

3.1 Constrained Lasso

Let R > 0 be a ﬁxed parameter. Our ﬁrst example studies the constrained lasso penalty [38]

h(β) = +∞ if (cid:107)β(cid:107)1 > R

(15)
i.e.  h is the convex indicator function of the (cid:96)1-ball of radius R > 0. Applying the above result
requires two ingredients: proving that the error vectors { ˆβ − β∗  η − β∗} belong to some set T with
high probability  and proving that rn = n−1/2γ(T  Σ) is small. Deﬁne for any real k ≥ 1 

h(β) = 0

and

(16)
The parameter k above will typically be a constant times s = (cid:107)β∗(cid:107)0  the sparsity of β∗. If R = (cid:107)β∗(cid:107)1 
then the triangle inequality reveals that the error vectors of ˆβ and η satisfy

Tlasso(k) := {u ∈ Rp : (cid:107)u(cid:107)1 ≤

k(cid:107)u(cid:107)}.

√

if (cid:107)β(cid:107)1 ≤ R 

where S = {j = 1  ...  p : β∗
By the Cauchy-Schwarz inequality (cid:107)uS(cid:107)1 ≤ √

{ ˆβ − β∗  η − β∗} ⊆ T := {u ∈ Rp : (cid:107)uSc(cid:107)1 ≤ (cid:107)uS(cid:107)1} 

(17)
j (cid:54)= 0} is the support of the true β∗ and uS is the restriction of u to S.

s(cid:107)uS(cid:107)2  thus T in (17) satisﬁes T ⊂ Tlasso(4s).

1The constant 1 can be replaced by another absolute constant; this will only change B3 to a different constant.

4

Lemma 3.1. If (N1) holds and k ≥ 1  then we have γ(T  Σ) (cid:46) φ(T )−1(cid:112)k log(2p/k) for any cone
Hence under (N1) and by setting k = 4s and rn = φ(T )−1(cid:112)s log(ep/s)/n we have in the linear

T ⊂ Tlasso(k) where Tlasso(k) is deﬁned in (16).

model with squared loss that  with high probability 

(cid:107)Σ1/2(η − ˆβ)(cid:107) (cid:46) Lφ(T )−1/2(s log(ep/s)/n)1/4((cid:107)Σ1/2( ˆβ − β∗)(cid:107) + (cid:107)Σ1/2(η − β∗)(cid:107))

(18)
and we have established that η is a ﬁrst order expansion of ˆβ with respect to the norm (cid:107) · (cid:107)Σ if
s log(ep/s)/n → 0. It is informative to study the order of magnitude of the right hand side in (18).
For that purpose  the following Lemma gives explicit bounds on (cid:107)Σ1/2( ˆβ−β∗)(cid:107) and (cid:107)Σ1/2(η−β∗)(cid:107).
Lemma 3.2. Consider the linear model with squared loss (10) and assume (A2). Let ˆβ  η in (1) and
(5) with penalty (15). Then if R = (cid:107)β∗(cid:107)1  we have with probability at least 1 − 2e−nr2
n 
(cid:107)Σ1/2( ˆβ − β∗)(cid:107) (cid:46) Lσ∗rn(1 − C6L2rn)−1 

(cid:107)Σ1/2(η − β∗)(cid:107) (cid:46) Lσ∗rn 

where rn = φ(T )−1(cid:112)s log(ep/s)/n and (σ∗)2 = (ε2

1 + ... + ε2

n)/n.

(19)

and

The above lemma provides a slight improvement in the rate compared to [17  Theorem 11.1(a)].
Combined with inequality (18)  we have established that (cid:107)Σ1/2( ˆβ − η)(cid:107) (cid:46) L2σ∗r3/2
n . If rn → 0
(e.g.  if s log(ep/s)/n → 0 while φ(T ) stays bounded away from 0)  this means that the distance
(cid:107)Σ1/2( ˆβ − η)(cid:107) between ˆβ and η is an order of magnitude smaller than the risk bounds in (19).
Inclusion (17) is granted regardless of the loss (cid:96)  as soon as β∗ lies on the boundary of {β ∈ Rp :
(cid:107)β(cid:107)1 = R}. In logistic regression  i.e.  the setting of Proposition 2.2 with the constrained Lasso
penalty (15)  inequality (13) yields that with high probability  (cid:107)η − ˆβ(cid:107)K (cid:46) L[r1/2
n + L1/2(1 +
n)E 1/2]E. An extra term appears compared to the squared loss. In order to obtain a ﬁrst-order
r3
n
expansion as in (2) requires rn → 0 as well as (1 + r3
n)E 1/2 → 0. These conditions can be
obtained if risk bounds such as (19) are available  see [35  1] or Proposition 3.4 and its proof in
Appendix F for applicable general techniques. A more detailed discussion of Logistic Lasso is given
in the next subsection.

√

√

n

3.2 Penalized Lasso

h(β) = λ(cid:107)β(cid:107)1

λ ≥ 0.

We now consider the (cid:96)1-norm penalty

for some

(20)
Here  the fact that ˆβ − β∗  η − β∗ ∈ T for some low-dimensional cone T is not granted almost
surely  in that regard the situation differs from the constrained Lasso case in (17). We may ﬁnd
such low-dimensional cone T simultaneously for ˆβ  η for both the squared loss and logistic loss as
follows  using ideas from [35  11]. Let fn be the convex function so that the objective in (1) is equal
to fn(β) + h(β) and let gn be the convex function so that the objective in (5) is gn(β) + h(β). Since
ˆβ and η are solutions of the corresponding optimization problems (1) and (5) 
h( ˆβ) − h(β∗) ≤ fn(β∗) − fn( ˆβ) ≤ ∇fn(β∗)T (β∗ − ˆβ) 
h(η) − h(β∗) ≤ gn(β∗) − gn(η) ≤ ∇gn(β∗)T (β∗ − η).

(21)

Since ∇gn(β∗) = ∇fn(β∗)  both η and ˆβ belong to the set ˆT = {b ∈ Rp : h(b) − h(β∗) ≤
∇fn(β∗)T (b − β∗)}. Next  for both the squared loss and the logistic loss  ∇fn(β∗) has subGaussian
coordinates under (A2). Combining these remarks  we obtain the following  proved in supplement.
Lemma 3.3. Let h be as in (20). Consider the linear model (10) and assume (A2)  (N1). Let ξ > 0
n)/n and
(cid:107)β∗(cid:107)0 = s. Then

be a constant and let λ = Lσ∗(1 + 3ξ)(cid:112)2 log(p/s)/n where (σ∗)2 = (ε2
P(cid:104){ ˆβ − β∗  η − β∗} ⊂ T
(L/2)(1 + 3ξ)(cid:112)2 log(p/s)/n  then the previous display (22) also holds.

If instead the logistic regression model and assumptions of Proposition 2.2 are fulﬁlled and λ =

ξ2 log(p/s)(p/s)ξ where T = Tlasso

(cid:0)s(6 + 2ξ−1)2(cid:1) .

(cid:105) ≥ 1 −

1 + . . . + ε2

(22)

2

5

The set T above is the set Tlasso(k) in (16) with k = s(6 + 2ξ−1)2. Eq. (22) deﬁnes a low-
dimensional cone T that contains both error vectors ˆβ − β∗  η − β∗ for the squared loss and the
logistic loss. The Gaussian width of the set T in (18) is already bounded in Lemma 3.1. Hence
the Gaussian width of T in the previous lemma is bounded from above as in the previous section 

i.e.  γ(Σ  T ) (cid:46) φ(T )−1(6 + 2ξ−1)(cid:112)s log(2p/s) by Lemma 3.1  and the “slow rate” inequality (18)

again holds with high probability  where φ(T ) denotes the restricted eigenvalue of the set T of the
previous lemma. Risk bounds similar to (19) are given below. We emphasize here the fact that the
error vectors of the Lasso belong to the cone (22) with high probability is not new: this is a powerful
technique used throughout the literature on high-dimensional statistics starting from [11  35]. The
novelty of our results are inequalities such as (18) which shows that the distance (cid:107)Σ1/2( ˆβ − η)(cid:107) is an

order of magnitude faster than the minimax risk(cid:112)s log(ep/s)/n. We will now state a result similar

to Lemma 3.2 for linear and logistic lasso.
Proposition 3.4. Consider the penalized lasso estimator ˆβ given by

(cid:80)n
i=1 (cid:96)(Yi  X(cid:62)

i β) + λ(cid:107)β(cid:107)1 

ˆβ := argminβ∈Rp

1
n

where (cid:96) is either the squared or logistic loss and λ is chosen as in Lemma 3.3 for some ξ > 0.
Assume (A1)  (A2). With T deﬁned in (22)  assume that ∃θ > 0 s.t. for all u ∈ T with (cid:107)u(cid:107)K ≤ 1 
(23)

(cid:8)(cid:96)(Yi  X(cid:62)

i β∗) − u(cid:62)Xi(cid:96)(cid:48)(Yi  X(cid:62)

i β∗)(cid:9)  

i u) − (cid:96)(Yi  X(cid:62)

i β∗ + X(cid:62)

(cid:80)n

i=1

θ2(cid:107)u(cid:107)2

K ≤ 1

n

as well as

3 φ(T )θ2 ×
Then with probability at least 1 − 2/(ξ2 log(p/s)(p/s)ξ) 

L(2 + 5ξ)(cid:112)2s log(p/s)/n ≤ B1/2
(cid:114)

(cid:107) ˆβ − β∗(cid:107)K ≤ L(2 + 5ξ)
3 φ(T )θ2

B1/2

n

2s log(p/s)

×

(cid:26)1/σ∗ 
(cid:26)σ∗ 

2 

0.5 

for (cid:96)  the squared loss 
for (cid:96)  the logistic loss.

for (cid:96)  the squared loss 
for (cid:96)  the logistic loss.

(24)

(25)

The proof is given Appendix F. Assumption (23) is the classical restricted strong convexity condition
and we verify this for linear and logistic loss in Proposition F.1. Results similar to Proposition 3.4
are known in the literature [35] but the main novelty of our result is that the tuning parameter λ is of

order(cid:112)log(p/s)/n and not(cid:112)log(p)/n which proves the minimax optimal rate.

Faster rates for the penalized lasso. Fast rates for the Lasso can be obtained using the second
inequality of Theorem 2.1  which when specialized to the squared loss gives (12). To verify the main
additional assumption of ˆβ − η ∈ T   we prove sparsity of η and ˆβ. Since ˆβ  η are deﬁned through a
penalized quadratic problem  we can leverage existing results in the literature that imply that η  ˆβ
satisﬁes (cid:107)η(cid:107)0 ∨ (cid:107) ˆβ(cid:107)0 ≤ ˜Cs under suitable conditions on the design and as long as s log(ep/s)/n is
small enough  for some constant ˜C that depends on the restricted singular values of Σ; cf.  e.g. [44 
Lemma 1]  [9  Theorem 3] [22  Lemma 3.5]  [4  Lemma 6.1]. We prove such as result for the
Group-Lasso in Proposition 3.7 below. Now we deﬁne the cones T0 and T as the sets

T0 := {u ∈ Rp : (cid:107)u(cid:107)0 ≤ (2 ˜C + 1)s} ⊂ T = {u ∈ Rp : (cid:107)u(cid:107)1 ≤ (2 ˜C + 1)1/2√

(26)
where the inclusion is obtained thanks to the Cauchy-Schwarz inequality. Then {η − ˆβ  ˆβ − β∗  η −
β∗} ⊂ T with high probability  the Gaussian width γ(T  Σ) is bounded by Lemma 3.1 and the second
inequality of Theorem 2.1 yields

s(cid:107)u(cid:107)}.

(cid:107)Σ1/2(η − ˆβ)(cid:107) (cid:46) L2rnE  where

rn = φ(T )−1(s log(ep/s)/n)1/2.

Since E (cid:46) rn with high probability by known prediction bounds for the Lasso (see Proposition 3.4
and its proof in Appendix F for rates with squared and logistic loss)  we obtain that with high
probability 

(cid:107)Σ1/2(η − ˆβ)(cid:107) (cid:46) L3φ−2(T )s log(ep/s)/n = L3r2
n 

(27)
a rate that is the square of the minimax rate rn  hence much smaller. For squared loss  this rate
is also faster than the rate obtained in (18) which is of order r3/2
n . This faster rate is obtained

6

maxA⊂[p]:|A|≤c5s maxj∈A

(cid:80)
j∈Ac |Σij| ≤ c6.

thanks to the inclusion { ˆβ − η  ˆβ − β∗  η − β∗} ⊂ T   whereas in the setting of (18) we only had
{ ˆβ − β∗  η − β∗} ⊂ T but not ˆβ − η ∈ T . To our knowledge  the only result in the literature similar
to the above bounds is given by [22  Theorem 5.1]. This result from [22] shows that (27) holds for
squared loss  provided that the covariance Σ satisﬁes (a) the minimal singular value of Σ is at least
c3 > 0  (b) the maximal singular value of Σ is at most c4  and (c) the covariance matrix Σ satisﬁes
(28)
Our results show that a ﬁrst order expansion for the Lasso can be obtained using the slow rate bound
(11) without the requirement that the spectral norm of Σ is bounded  and for the fast rate without
the stringent assumption (28) on the correlations of Σ. Not only do our results generalize Theorem
5.1 from [22] to more general Σ  Theorem 2.1 shows how to obtain ﬁrst-order expansion η beyond
the squared loss (e.g. logistic loss) and beyond the (cid:96)1-penalty of the lasso: the previous subsection
tackles the constrained Lasso penalty (15) and the next subsection tackles the Group-Lasso penalty.
Sparsity of η for any general loss function is proved in Proposition 3.7. This alone does not imply
inclusion of η − ˆβ in a low-dimensional set without sparsity of ˆβ. Sparsity of ˆβ for general loss
function is not well-studied but for logistic loss function Section D.4 of the supplement of [10]
proves a sparsity bound of the form (cid:107) ˆβ(cid:107)0 ≤ ˜Cs  similar to the squared loss. Unfortunately the proof

there requires λ (cid:38)(cid:112)log p/n instead of condition λ (cid:38)(cid:112)log(p/s)/n used in Lemma 3.3 above and

in [26  37  7  4  2].

3.3 Group-Lasso
Consider now a partition of {1  ...  p} into M groups G1  ...  GM . For simplicity  we assume that the
groups have the same size d = p/M  which is typically the case in multitask learning with d tasks
and M shared features. The Group-Lasso penalty studied in this subsection is

h( ˆβ) = λ(cid:80)M

Gk

√

√
s(

k=1 (cid:107)βGk(cid:107)

(cid:54)= 0 (Lemma 3.6).

where βGk ∈ R|Gk| is the restriction (βj  j ∈ Gk).

d +(cid:112)2 log(M/s)) where s is the number of

2ξ)(cid:112)2 log(M/s)] where (σ∗)2 = ((cid:80)n
P(cid:16){ ˆβ − β∗  η − β∗} ⊂ T
for T = {δ ∈ Rp :(cid:80)M
k=1 (cid:107)δGk(cid:107) ≤ √

(29)
In both the linear model with squared loss and in logistic regression with the logistic loss  we now
show that ˆβ − β∗ and η − β∗ belong to a low-dimensional cone (Lemma 3.5)  and that the Gaussian
width of this cone is bounded from above by
groups with β∗
Lemma 3.5. Consider the linear model (10) and assume that maxk=1 ... M (cid:107)ΣGk Gk(cid:107)op ≤ 1 and
√
that each group has the same size |Gk| = d = p/M. Let ξ > 0 and set λ = Lσ∗(1 + ξ)[
d + (1 +
(cid:54)= 0. Then
i )/n and s is the number of groups with β∗
(30)
s(cid:107)δ(cid:107)2(2 + 3ξ−1)}. If instead the logistic regression model and
assumptions of Proposition 2.2 are fulﬁlled and λ is as above with σ∗ = 1/2  then (30) also holds.
The fact that the Group-Lasso belongs with high probability to a low-dimensional cone has been
used before to prove risk bounds  e.g.  [31  5]. However the tuning parameter in the above lemma is
smaller than that used in these works and using such cones to prove ﬁrst expansion as in the present
paper are  to our knowledge  novel.
Lemma 3.6. Assume that maxk=1 ... M (cid:107)ΣGk Gk(cid:107)op ≤ 1 and that each group has the same
size |Gk| = d = p/M. The set T deﬁned in the previous lemma satisﬁes γ(T  Σ) (cid:46)

C(ξ)φ(T )−1(cid:112)sd + s log(M/s) for some constant C(ξ) that depends only on ξ.

(cid:17) ≥ 1 − 2(cid:14)(cid:0)2ξ2 log(M/s)(M/s)ξ(cid:1) .

i=1 ε2

Gk

(cid:54)= 0) and
Hence if the number of groups M  the group-sparsity s (number of groups such that β∗
the group size d = p/M satisfy (sd + s log(M/s))/n → 0 while φ(T ) is bounded away from 0  the
above Lemmas combined with Theorem 2.1 imply that η is a ﬁrst-order expansion of ˆβ for both the
squared loss in linear regression and logistic loss in the logistic model. We leverage this result to
obtain an exact risk identity for the Group-Lasso in the next section.
Proposition 3.7. Assume (A1)  (A2). Let the setting of Lemma 3.6 be fulﬁlled. Fix λ as in Lemma 3.5
for both squared and logistic loss for some ξ > 0 and T be the cone deﬁned in Lemma 3.5. If
(cid:107)K(cid:107)op ≤ Cmax < ∞ and the assumptions of Proposition 3.4 hold  then

Gk

P(cid:16)|{k ∈ [M ] : ηGk (cid:54)= 0}| ≤ s ˜C

(cid:17) ≥ 1 − 2/(ξ2 log(M/s)(M/s)ξ) 

7

where ˜C := 1 + Cmax{2(3 + ξ)(1 + ξ−1)}2B2

with ˜C replaced by (1 + o(1)) ˜C provided φ(T )−1(cid:112)sd + s log(M/s)/

√

3 φ(T )−2. For the squared loss  the same holds for ˆβ

n → 0.

The proof is given in Appendix G. For the Lasso the assumption of (cid:107)K(cid:107)op ≤ Cmax can be relaxed to
a bound on the sparse maximal eigenvalue of K using devices from [44  Lemma 1]  [47  Corollary
2]  [8  Lemma 3] or [4  Proposition 7.4]. See also [31  Theorem 3.1] and [29  Lemma 6] for similar
results for the Group-Lasso  although with a larger tuning parameter than in Proposition 3.7.
For the squared loss  if the condition number of Σ stays bounded then Cmax/φ(T )−2 is also bounded.
n → 0  Proposition 3.7 yields that both ˆβ − η belongs to the
k=1 (cid:107)δGk(cid:107) ≤ (1 + o(1))(2 ˜Cs)1/2(cid:107)δ(cid:107)2}  which yields the “fast rate” bound (12).

Then if rn =(cid:112)sd + s log(M/s)/
cone {δ ∈ Rp :(cid:80)M

√

4 Application to exact risk identities

(cid:12)(cid:12)(cid:12)(cid:107) ˆβ − β∗(cid:107) − EZ
n1/2σ∗(cid:80)n

β∗ + n−1/2(cid:80)n

In the linear model with the squared loss and identity covariance (Σ = Ip)  the expansion η in
(5) is particularly simple: η becomes the proximal operator of the penalty h at the point z =
i=1 εiXi  i.e  η = proxh(z) where proxh(x) = argminb∈Rp (cid:107)x − b(cid:107)2/2 + h(b).
Hence the loss (cid:107)η − β∗(cid:107) of η has a simple form and if a ﬁrst-order expansion (2) is available  for
instance for the Lasso or Group-Lasso as a consequence of the Lemmas of the previous section  then
the loss (cid:107) ˆβ − β∗(cid:107) is exactly the loss of prox(z) up to a smaller order term. Let us emphasize that the
next result and following discussion provide exact risk identities for the loss (cid:107) ˆβ − β∗(cid:107) (as in (32)
below)  and not only upper bounds up to multiplicative constants.
Theorem 4.1. [Exact Risk Identity] Consider the linear model (10) and the regularized problem (1)
with an arbitrary proper convex function h(·). Assume that X1  . . .   Xn are iid N (0  Ip) independent
of ε1  ...  εn and set σ∗ = ( 1

i )1/2. Then with probability at least 1 − 2 exp(−t2/2) 

(cid:80)n
(cid:104)(cid:107)β∗ − proxh(β∗ + n−1/2σ∗Z)(cid:107)2(cid:105)1/2(cid:12)(cid:12)(cid:12) ≤ σ∗(t + 1)

i=1 ε2

n

n1/2

(31)

+ (cid:107) ˆβ − η(cid:107)
i=1 εiXi ∼ N (0  Ip) and EZ denotes the expectation with respect Z.

where Z = 1
Theorem 4.1 is a generalization of Corollary 5.2 of [22] where the result is stated for h(β) = λ(cid:107)β(cid:107)1

with λ (cid:38) σ∗(cid:112)2 log(p)/n. For the case of lasso  either in its constrained form with tuning parameter

(cid:107) ˆβ − β∗(cid:107) = (1 + op(1))EZ[(cid:107)β∗ − proxh(β∗ + n−1/2σ∗Z)(cid:107)2]1/2.

chosen as in Lemma 3.3 or the penalized Lasso with tuning parameter as in Lemma 3.3  inequality (18)
holds thanks to (17) and Lemma 3.1 for the constrained lasso  and thanks to Lemmas 3.1 and 3.3 for
the penalized lasso. Hence for both the constrained and penalized lasso  if Σ = Ip with Gaussian
√
n) + Op(s log(ep/s)/n)1/4)((cid:107)η −
design  the second term on the right hand side of (31) is Op(σ∗/
β∗(cid:107) + (cid:107) ˆβ − β∗(cid:107)). Hence if s  n  p → +∞ with s log(ep/s)/n → 0 and s/p → 0 then (31) implies
(32)
For the penalized lasso  since η represents a soft-thresholding operator which can be written in closed
form  Theorem 4.1 allows a reﬁned study of the risk of ˆβ; see [14  Theorem 5.1]. Similarly for the
group lasso  we have from Lemmas 3.5 and 3.6 that (cid:107)η− ˆβ(cid:107) = Op((sd+s log(M/s))1/4/n1/4)(cid:107) ˆβ−
β∗(cid:107) (slow rate) which is again negligible relative to (cid:107) ˆβ−β∗(cid:107) if (sd+s log(M/s))/n → 0  s/M → 0.
Thus  (32) again holds true. For the group lasso η = proxh(β∗ + n−1/2σ∗Z) represents the Block
James-Stein estimator in the sequence model; see [13  Section 2.1].
Extending Corollary 5.2 of [22] to more general loss/penalty functions  the above device lets us
characterize the risk (cid:107) ˆβ − β∗(cid:107): Up to a multiplicative constant of order 1 + op(1)  the risk is the same
as the risk of the proximal of h in the Gaussian sequence model where one observes N (β∗  (σ∗)2/n).

5 Application to inference

The second application we wish to mention is related to conﬁdence intervals in the linear model when
the squared loss is used and X1  ...  Xn are iid Gaussian N (0  Σ). Assume that one is interested
in constructing a conﬁdence interval for a speciﬁc linear combination aT β∗ for some a ∈ Rp.

8

√

nr3

n).

Further assume  for simplicity  that Σ is known and that a is normalized with (cid:107)Σ−1/2a(cid:107) = 1. Then
previous works on de-biasing [45  46  20  21  40  22  4] suggests  given an estimator ˆβ that may
be biased  to consider the bias-corrected estimator ˆθ deﬁned by ˆθ = aT ˆβ + (cid:107)za(cid:107)−2zT
a (y − X ˆβ) 
where y = (Y1  ...  Yn) is the response vector and X is the design matrix with rows X1  ...  Xn and
za = XΣ−1a ∼ N (0  In) is sometimes referred to as a score vector for the estimation of aT β∗.
Proposition 5.1. Assume that X1  ...  Xn are iid N (0  Σ) and is independent of ε = (ε1  ...  εn) ∼
√
N (0  In). Assume that for some cone T and rn = γ(T  Σ)/

n we have

nrn)(cid:107)Σ1/2(η − ˆβ)(cid:107) 

P((cid:107)Σ1/2( ˆβ − β∗)(cid:107) + (cid:107)Σ1/2(η − β∗)(cid:107) ≤ C7rn {η − ˆβ  η − β∗  ˆβ − β∗} ⊂ T ) ≥ 1 − α.

√
n(ˆθ − aT β∗) − Tn = Op((1 + rn))(cid:107)Σ1/2(η − β∗)(cid:107) + Op(

Then for some Tn with the t-distribution with n degrees-of-freedom  with probability 1−α−4e−nr2

(33)
n/2 
(34)
(35)
Because Tn has t distribution with n degrees of freedom  asymptotically P(|Tn| ≤ 1.96) → 0.95
and hence from (35)  we get that P(n1/2|ˆθ − a(cid:62)β∗| ≤ 1.96) → 0.95 if r3
n → 0. Therefore 
[ˆθ − 1.96/n1/2  ˆθ + 1.96/n1/2] represents a 95% conﬁdence interval for a(cid:62)β∗. Conclusion (34) is a
consequence of Theorem 2.1.

Lasso. Eq. (33) is satisﬁed for the penalized Lasso for rn =(cid:112)s log(ep/s)/n and the cone T in

√
= Op(rn(1 + rn)) + Op(

(26)  in situations where (cid:107) ˆβ(cid:107)0 ≤ ˜Cs with high probability as explained in the discussion surrounding
(26). In order to construct conﬁdence interval based on (34)  the right hand side of (35) needs to
converges to 0. This is the case if rn → 0 and
n → 0. For the Lasso with rn = s log(ep/s)/n 
this translates to the sparsity condition s3 log(ep/s)3/n2 → 0  i.e.  s = o(n2/3) up to logarithmic
for the Lasso beyond the condition s (cid:46) √
factors. Hence the ﬁrst order expansion results of the present paper lets us derive de-biasing results
(other recent approaches  [22  4] also allow to prove such result beyond s (cid:46) √
n required in the early results [46  20  40] on de-biasing
n). Moreover  the
above proposition is general and apply to any regularized estimator such that (33) holds  with suitable
bounds on the Gaussian complexity γ(T  Σ). For s ≫ n2/3  the estimator ˆθ requires an adjustment
for asymptotic normality in the form a degree-of-freedom adjustment [4].
Group-Lasso.
n and the
condition number of Σ is bounded  then (33) holds thanks to Proposition 3.7  the last paragraph of
Section 3.3 and the risk bound (86). Here  (35) is o(1) if and only if (sd + s log(M/s))/n2/3 → 0.
This improves the sample size requirement of [34]  although Σ is assumed known in Proposition 5.1.

If s is the number of non-zero groups  rn = (cid:112)sd + s log(M/s)/

nr3

√

n

√

√

(Detailed proofs are given in Appendix E)

i . Under assumption (A1)  we have

6 Proof sketch of Theorem 2.1

Theorem 6.1. Deﬁne ˆK := n−1(cid:80)n

i=1 (cid:96)(cid:48)(cid:48)(Yi  X(cid:62)
n E 3/2.
(i) If { ˆβ − β∗  η − β∗} ⊆ T then (cid:107) ˆβ − η(cid:107)K (cid:46) Q1/2
(ii) If { ˆβ − η  ˆβ − β∗  η − β∗} ⊆ T then (cid:107) ˆβ − η(cid:107)K (cid:46) Qn 2E + BZnE 2 
where

i β∗)XiX(cid:62)
n 1E + B1/2Z 1/2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) u(cid:62) ˆKu

(cid:107)u(cid:107)2

K

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   Qn 2 = sup

u v∈T

− 1

Qn 1 = sup
u∈T

|u(cid:62)( ˆK − K)v|
(cid:107)u(cid:107)K(cid:107)v(cid:107)K

and Zn = sup
u∈T

n(cid:88)

i=1

1
n

|X(cid:62)
i u|3
(cid:107)u(cid:107)3

K

.

Theorem 6.1 follows from the strong convexity of the objective function of η with respect to the
norm (cid:107) · (cid:107)K (cf. for instance  Lemma 1 of [6]) combined with Taylor expansions of the loss (cid:96).
Next  to prove Theorem 2.1  it remains to bound Qn 1(T )  Qn 2(T ) and Zn(T ). The quadratic
processes Qn 1(T )  Qn 2(T ) and cubic process Zn(T ) can be bounded in terms of γ(T  Σ) using
generic chaining results  Theorem 1.13 of Mendelson [33] and Eq. (3.9) of [32]  as follows.
Proposition 6.2. [Control of Qn 1  Qn 2 and Zn] Under assumptions (A1) and (A2)  we have
(i) With probability 1 − 2 exp(−C8t2γ2(T  Σ)) 

max{Qn 1(T )  Qn 2(T )} ≤ C9B2B3L2(cid:0)tn−1/2γ(T  Σ) + t2n−1γ2(T  Σ)(cid:1) .
3 L3(cid:0)1 + n−1γ3(T  Σ)(cid:1) t3.

(ii) With probability 1 − 2 exp(−C10t log n)  Zn(T ) ≤ C11B3/2

9

References
[1] Pierre Alquier  Vincent Cottet  and Guillaume Lecué. Estimation bounds and sharp oracle
inequalities of regularized procedures with lipschitz loss functions. The Annals of Statistics  47
(4):2117–2144  2019.

[2] Pierre C Bellec. The noise barrier and the large signal bias of the lasso and other convex
estimators. arXiv:1804.01230  2018. URL https://arxiv.org/pdf/1804.01230.
pdf.

[3] Pierre C Bellec. Sharp oracle inequalities for least squares estimators in shape restricted

regression. The Annals of Statistics  46(2):745–780  2018.

[4] Pierre C Bellec and Cun-Hui Zhang. De-biasing the lasso with degrees-of-freedom adjustment.

arXiv:1902.08885  2019. URL https://arxiv.org/pdf/1902.08885.pdf.

[5] Pierre C Bellec  Guillaume Lecué  and Alexandre B Tsybakov. Towards the study of least
squares estimators with convex penalty. In Seminaire et Congres  to appear  number 39. Societe
mathematique de France  2017. URL https://arxiv.org/pdf/1701.09120.pdf.

[6] Pierre C Bellec  Arnak S Dalalyan  Edwin Grappin  and Quentin Paris. On the prediction loss
of the lasso in the partially labeled setting. Electronic Journal of Statistics  12(2):3443–3472 
2018.

[7] Pierre C. Bellec  Guillaume Lecué  and Alexandre B. Tsybakov. Slope meets lasso: Improved
oracle bounds and optimality. Ann. Statist.  46(6B):3603–3642  2018. ISSN 0090-5364. doi:
10.1214/17-AOS1670. URL https://arxiv.org/pdf/1605.08651.pdf.

[8] Alexandre Belloni and Victor Chernozhukov. Least squares after model selection in high-

dimensional sparse models. Bernoulli  19(2):521–547  2013.

[9] Alexandre Belloni  Victor Chernozhukov  and Lie Wang. Pivotal estimation via square-root
lasso in nonparametric regression. Ann. Statist.  42(2):757–788  04 2014. URL http://dx.
doi.org/10.1214/14-AOS1204.

[10] Alexandre Belloni  Victor Chernozhukov  and Ying Wei. Post-selection inference for generalized
linear models with many controls. Journal of Business & Economic Statistics  34(4):606–619 
2016.

[11] Peter J. Bickel  Ya’acov Ritov  and Alexandre B. Tsybakov. Simultaneous analysis of lasso and
dantzig selector. Ann. Statist.  37(4):1705–1732  08 2009. doi: 10.1214/08-AOS620. URL
http://dx.doi.org/10.1214/08-AOS620.

[12] Stéphane Boucheron  Gábor Lugosi  and Pascal Massart. Concentration inequalities: A

nonasymptotic theory of independence. Oxford University Press  2013.

[13] T Tony Cai and Harrison H Zhou. A data-driven block thresholding approach to wavelet

estimation. The Annals of Statistics  37(2):569–595  2009.

[14] Emmanuel J Candes. Modern statistical estimation via oracle inequalities. Acta numerica  15:

257–325  2006.

[15] Antoine Dedieu. Error bounds for sparse classiﬁers in high-dimensions. arXiv preprint

arXiv:1810.03081  2018.

[16] Sjoerd Dirksen. Tail bounds via generic chaining. Electronic Journal of Probability  20  2015.

[17] Trevor Hastie  Robert Tibshirani  and Martin Wainwright. Statistical learning with sparsity: the

lasso and generalizations. CRC press  2015.

[18] Daniel Hsu  Sham Kakade  and Tong Zhang. A tail inequality for quadratic forms of subgaussian
random vectors. Electron. Commun. Probab.  17:no. 52  1–6  2012. doi: 10.1214/ECP.v17-2079.
URL http://ecp.ejpecp.org/article/view/2079.

10

[19] Hidehiko Ichimura. Semiparametric least squares (sls) and weighted sls estimation of single-

index models. Journal of Econometrics  58(1-2):71–120  1993.

[20] Adel Javanmard and Andrea Montanari. Conﬁdence intervals and hypothesis testing for high-
dimensional regression. The Journal of Machine Learning Research  15(1):2869–2909  2014.

[21] Adel Javanmard and Andrea Montanari. Hypothesis testing in high-dimensional regression
under the gaussian random design model: Asymptotic theory. IEEE Transactions on Information
Theory  60(10):6522–6554  2014.

[22] Adel Javanmard and Andrea Montanari. De-biasing the lasso: Optimal sample size for gaussian

designs. Annals of Statistics  to appear  2015.

[23] Keith Knight and Wenjiang Fu. Asymptotics for lasso-type estimators. Annals of statistics 

pages 1356–1378  2000.

[24] Vladimir Koltchinskii. Sparsity in penalized empirical risk minimization.

In Annales de
l’Institut Henri Poincaré  Probabilités et Statistiques  volume 45  pages 7–57. Institut Henri
Poincaré  2009.

[25] A. K. Kuchibhotla. Deterministic Inequalities for Smooth M-estimators.

prints:1809.05172  September 2018.

ArXiv e-

[26] Guillaume Lecué and Shahar Mendelson. Regularization and the small-ball method i: Sparse
recovery. Ann. Statist.  46(2):611–641  04 2018. doi: 10.1214/17-AOS1562. URL https:
//doi.org/10.1214/17-AOS1562.

[27] Jason D Lee  Yuekai Sun  and Michael A Saunders. Proximal newton-type methods for

minimizing composite functions. SIAM Journal on Optimization  24(3):1420–1443  2014.

[28] Christopher Liaw  Abbas Mehrabian  Yaniv Plan  and Roman Vershynin. A simple tool for
bounding the deviation of random matrices on geometric sets. In Geometric aspects of functional
analysis  pages 277–299. Springer  2017.

[29] Han Liu and Jian Zhang. Estimation consistency of the group lasso and its applications. In

Artiﬁcial Intelligence and Statistics  pages 376–383  2009.

[30] Po-Ling Loh. Statistical consistency and asymptotic normality for high-dimensional robust

m-estimators. The Annals of Statistics  45(2):866–896  2017.

[31] Karim Lounici  Massimiliano Pontil  Sara van de Geer  and Alexandre B. Tsybakov. Oracle
inequalities and optimal inference under group sparsity. Ann. Statist.  39(4):2164–2204  08
2011. doi: 10.1214/11-AOS896. URL http://dx.doi.org/10.1214/11-AOS896.

[32] Shahar Mendelson. Empirical processes with a bounded ψ1 diameter. Geometric and Functional

Analysis  20(4):988–1027  2010.

[33] Shahar Mendelson.

cesses.
10.1016/j.spa.2016.04.028.
v126y2016i12p3652-3680.html.

Stochastic Processes and their Applications  126(12):3652–3680  2016.

Upper bounds on product and multiplier empirical pro-
doi:
URL https://ideas.repec.org/a/eee/spapps/

[34] Ritwik Mitra and Cun-Hui Zhang. The beneﬁt of group sparsity in group inference with

de-biased scaled group lasso. Electronic Journal of Statistics  10(2):1829–1873  2016.

[35] Sahand Negahban  Bin Yu  Martin J Wainwright  and Pradeep K Ravikumar. A uniﬁed
framework for high-dimensional analysis of m-estimators with decomposable regularizers.
In Advances in Neural Information Processing Systems  pages 1348–1356  2009.

[36] Philippe Rigollet and Alexandre Tsybakov. Exponential screening and optimal rates of sparse

estimation. The Annals of Statistics  39(2):731–771  2011.

[37] Tingni Sun and Cun-Hui Zhang. Sparse matrix inversion with scaled lasso. Journal of Machine

Learning Research  14(1):3385–3418  2013.

11

[38] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological)  pages 267–288  1996.

[39] Sara van de Geer. Weakly decomposable regularization penalties and structured sparsity.

Scandinavian Journal of Statistics  41(1):72–86  2014.

[40] Sara Van de Geer  Peter Bühlmann  Ya’acov Ritov  and Ruben Dezeure. On asymptotically
optimal conﬁdence regions and tests for high-dimensional models. The Annals of Statistics  42
(3):1166–1202  2014.

[41] Aad van der Vaart. Part iii: Semiparameric statistics. Lectures on Probability Theory and

Statistics  pages 331–457  2002.

[42] Aad W Van der Vaart. Asymptotic statistics  volume 3. Cambridge university press  2000.

[43] Roman Vershynin. High-dimensional probability: An introduction with applications in data

science  volume 47. Cambridge University Press  2018.

[44] Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The

Annals of statistics  pages 894–942  2010.

[45] Cun-Hui Zhang. Statistical inference for high-dimensional data. Mathematisches Forschungsin-
stitut Oberwolfach: Very High Dimensional Semiparametric Models  Report  (48):28–31  2011.

[46] Cun-Hui Zhang and Stephanie S Zhang. Conﬁdence intervals for low dimensional parameters
in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  76(1):217–242  2014.

[47] Cun-Hui Zhang and Tong Zhang. A general theory of concave regularization for high-
doi:

dimensional sparse estimation problems.
10.1214/12-STS399. URL https://doi.org/10.1214/12-STS399.

Statist. Sci.  27(4):576–593  11 2012.

12

,Kai-Yang Chiang
Cho-Jui Hsieh
Inderjit Dhillon
Pierre-Alexandre Mattei
Jes Frellsen
Pierre Bellec
Arun Kuchibhotla