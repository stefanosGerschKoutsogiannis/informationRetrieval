2019,Self-Supervised Generalisation with Meta Auxiliary Learning,Learning with auxiliary tasks can improve the ability of a primary task to generalise. However  this comes at the cost of manually labelling auxiliary data. We propose a new method which automatically learns appropriate labels for an auxiliary task  such that any supervised learning task can be improved without requiring access to any further data. The approach is to train two neural networks: a label-generation network to predict the auxiliary labels  and a multi-task network to train the primary task alongside the auxiliary task. The loss for the label-generation network incorporates the loss of the multi-task network  and so this interaction between the two networks can be seen as a form of meta learning with a double gradient. We show that our proposed method  Meta AuXiliary Learning (MAXL)  outperforms single-task learning on 7 image datasets  without requiring any additional data. We also show that MAXL outperforms several other baselines for generating auxiliary labels  and is even competitive when compared with human-defined auxiliary labels. The self-supervised nature of our method leads to a promising new direction towards automated generalisation. Source code can be found at \url{https://github.com/lorenmt/maxl}.,Self-Supervised Generalisation with

Meta Auxiliary Learning

Shikun Liu

Andrew J. Davison

Edward Johns

Department of Computing  Imperial College London

{shikun.liu17  a.davison  e.johns}@imperial.ac.uk

Abstract

Learning with auxiliary tasks can improve the ability of a primary task to generalise.
However  this comes at the cost of manually labelling auxiliary data. We propose a
new method which automatically learns appropriate labels for an auxiliary task 
such that any supervised learning task can be improved without requiring access to
any further data. The approach is to train two neural networks: a label-generation
network to predict the auxiliary labels  and a multi-task network to train the
primary task alongside the auxiliary task. The loss for the label-generation network
incorporates the loss of the multi-task network  and so this interaction between the
two networks can be seen as a form of meta learning with a double gradient. We
show that our proposed method  Meta AuXiliary Learning (MAXL)  outperforms
single-task learning on 7 image datasets  without requiring any additional data.
We also show that MAXL outperforms several other baselines for generating
auxiliary labels  and is even competitive when compared with human-deﬁned
auxiliary labels. The self-supervised nature of our method leads to a promising
new direction towards automated generalisation. Source code can be found at
https://github.com/lorenmt/maxl.

1

Introduction

Auxiliary learning is a method to improve the ability of a primary task to generalise to unseen data 
by training on additional auxiliary tasks alongside this primary task. The sharing of features across
tasks results in additional relevant features being available  which otherwise would not have been
learned from training only on the primary task. The broader support of these features  across new
interpretations of input data  then allows for better generalisation of the primary task. Auxiliary
learning is similar to multi-task learning [5]  except that only the performance of the primary task is
of importance  and the auxiliary tasks are included purely to assist the primary task.
We now rethink this generalisation by considering that not all auxiliary tasks are created equal. In
supervised auxiliary learning [21  33]  auxiliary tasks can be manually chosen to complement the
primary task. However  this requires both domain knowledge to choose the auxiliary tasks  and
labelled data to train the auxiliary tasks. Unsupervised auxiliary learning [11  36  35  16  1] removes
the need for labelled data  but at the expense of a limited set of auxiliary tasks which may not
be beneﬁcial for the primary task. By combining the merits of both supervised and unsupervised
auxiliary learning  the ideal framework would be one with the ﬂexibility to automatically determine
the optimal auxiliary tasks  but without the need to manually label these auxiliary tasks.
In this paper  we propose to achieve such a framework with a simple and general meta-learning
algorithm  which we call Meta AuXiliary Learning (MAXL). We ﬁrst observe that in supervised
learning  deﬁning a task can equate to deﬁning the labels for that task. Therefore  for a given primary
task  an optimal auxiliary task is one which has optimal labels. The goal of MAXL is then to
automatically discover these auxiliary labels using only the labels for the primary task.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

The approach is to train two neural networks.
First  a multi-task network  which trains the
primary task and the auxiliary task  as in
standard auxiliary learning. Second  a label-
generation network  which learns the labels
for the auxiliary task. The key idea behind
MAXL is to then use the performance of the
primary task  when trained alongside the aux-
iliary task in one iteration  to improve the
auxiliary labels for the next iteration. This
is achieved by deﬁning the loss for the label-
generation network as a function of the multi-
task network’s performance on primary task
training data. In this way  the two networks are
tightly coupled and can be trained end-to-end.
In our experiments on image classiﬁcation  we show three key results. First  MAXL outperforms
single-task learning across seven image datasets  even though both methods use the same amount
of labelled data. Second  MAXL outperforms a number of baseline methods for creating auxiliary
labels. Third  when manually-deﬁned auxiliary labels exist  such as those from an image hierarchy 
MAXL is at least as competitive  despite not actually using the manually-deﬁned auxiliary labels.
This last result shows that MAXL is able to remove the need for manual labelling of auxiliary tasks 
which brings the advantages of auxiliary learning to new datasets previously not compatible with
auxiliary learning  due to the lack of auxiliary labels.

Figure 1: Illustration of MAXL framework. The
primary task is trained with ground-truth labels 
whereas the auxiliary task is trained with learned
labels.

2 Related Work

This work brings together ideas from a number of related areas of machine learning.
Multi-task & Transfer Learning
The aim of multi-task learning (MTL) is to achieve shared
representations by simultaneously training a set of related learning tasks. In this case  the learned
knowledge used to share across domains is encoded into the feature representations to improve
performance of each individual task  since knowledge distilled from related tasks are interdependent.
The success of deep neural networks has led to some recent methods advancing the multi-task
architecture design  such as applying a linear combination of task-speciﬁc features [25  8  17]. [23]
applied soft-attention modules as feature selectors  allowing learning of both task-shared and task-
speciﬁc features in an end-to-end manner. Transfer learning is another common approach to improve
generalisation  by incorporating knowledge learned from one or more related domains. Pre-training
a model with a large-scale dataset such as ImageNet [7] has become a standard practise in many
vision-based applications.
Auxiliary Learning Whilst in multi-task learning the goal is high test accuracy across all tasks 
auxiliary learning differs in that high test accuracy is only required for a single primary task  and the
role of the auxiliary tasks is to assist in generalisation of this primary task. Applying related learning
tasks is one straightforward approach to assist primary tasks. [33] applied auxiliary supervision
with phoneme recognition at intermediate low-level representations to improve the performance of
conversational speech recognition. [21] chose auxiliary tasks which can be obtained with low effort 
such as global descriptions of a scene  to boost the performance for single scene depth estimation
and semantic segmentation. By carefully choosing a pair of learning tasks  we may also perform
auxiliary learning without ground truth labels  in an unsupervised manner. [16] introduced a method
for improving agent learning in Atari games  by building unsupervised auxiliary tasks to predict
the onset of immediate rewards from a short historical context. [11  36] proposed image synthesis
networks to perform unsupervised monocular depth estimation by predicting the relative pose of
multiple cameras. [9] proposed to use cosine similarity as an adaptive task weighting to determine
when a deﬁned auxiliary task is useful. Differing from these works which require prior knowledge to
manually deﬁne suitable auxiliary tasks  our proposed method requires no additional task knowledge 
since it generates useful auxiliary knowledge in a purely unsupervised fashion. The most similar
work to ours is [35]  in which meta learning was used in auxiliary data selection. However  this still
requires manually-labelled data from which these selections are made  whilst our method is able to
generate auxiliary data from scratch.

2

InputDataNeuralNetworkPrimaryTaskPredictionAuxiliaryTaskPredictionGroundTruthLabelsdogGeneratedLabelsMeta Learning Meta learning (or learning to learn) aims to induce the learning algorithm itself.
Early works in meta learning explored automatically learning update rules for neural models [4  3  29].
Recent approaches have focussed on learning optimisers for deep networks based on LSTMs [26] or
synthetic gradients [2  15]. Meta learning has also been studied for ﬁnding optimal hyper-parameters
[20] and a good initialisation for few-shot learning [10]. [28] also investigated few shot learning
via an external memory module. [34  31] realised few shot learning in the instance space via a
differentiable nearest-neighbour approach. Related to meta learning  our framework is designed to
learn to generate useful auxiliary labels  which themselves are used in another learning procedure.

3 Meta Auxiliary Learning

We now introduce our method for automatically generating optimal labels for an auxiliary task  which
we call Meta AuXiliary Learning (MAXL). In this paper  we only consider a single auxiliary task 
although our method is general and could be modiﬁed to include several auxiliary tasks. We only
focus on classiﬁcation tasks for both the primary and auxiliary tasks  but the overall framework could
also be extended to regression. As such  the auxiliary task is deﬁned as a sub-class labelling problem 
where each primary class is associated with a number of auxiliary classes  in a two-level hierarchy.
For example  if manually-deﬁned labels were used  a primary class could be “Dog"  and one of the
auxiliary classes could be “Labrador".

3.1 Problem Setup

The goal of MAXL is to generate labels for the auxiliary task which  when trained alongside a primary
task  improve the performance of the primary task. To accomplish this  we train two networks: a
multi-task network  which trains on the primary and auxiliary task in a standard multi-task learning
setting  and a label-generation network  which generates the labels for the auxiliary task.
We denote the multi-task network as a function fθ1(x) with parameters θ1 which takes an input x 
and the label-generation network as a function gθ2 (x) with parameters θ2 which takes the same input
x. Parameters θ1 are updated by losses of both the primary and auxiliary tasks  as is standard in
auxiliary learning. However  θ2 is updated only by the performance of the primary task.
In the multi-task network  we apply a hard parameter sharing approach [27] in which we predict
both the primary and auxiliary classes using the shared set of features θ1. At the ﬁnal feature layer 
fθ1 (x)  we then further apply task-speciﬁc layers to output the corresponding prediction for each
task  using a SoftMax function. We denote the primary task predictions by f pri
(x)  and the auxiliary
θ1
task predictions by f aux
(x). And we denote the ground-truth primary task labels by ypri  and the
θ1
generated auxiliary task labels by yaux.
We found during experiments that training beneﬁted from assigning each primary class its own unique
set of possible auxiliary classes  rather than sharing all auxiliary classes across all primary classes.
In the label-generation network  we therefore deﬁne a hierarchical structure ψ which determines
the number of auxiliary classes for each primary class. At the output layer of the label-generation
network  we then apply a masked SoftMax function to ensure that each output node represents an
auxiliary class corresponding to only one primary class  as described further in Section 3.3. Given
input data x  the label-generation network then takes in the hierarchy ψ together with the ground-
truth primary task label ypri  and applies Mask SoftMax to predict the auxiliary labels  denoted by
yaux = ggen
(x  ypri  ψ). A visualisation of the overall MAXL framework is shown in Figure 2. Note
θ2
that we allow soft assignment for the generated auxiliary labels  rather than one-hot encoding  which
we found during experiments enables greater ﬂexibility to obtain optimal auxiliary labels.

3.2 Model Objectives

The multi-task network is trained alongside the label-generation network  with two stages per epoch.
In the ﬁrst stage  the multi-task network is trained using primary task ground-truth labels  and the
auxiliary labels from the label-generation network. In the second stage  the label-generation network
is updated by computing its gradients with respect to the multi-task network’s prediction accuracy on
the primary task. We train both networks in an iterative manner until convergence.
In the ﬁrst stage of each epoch  given target auxiliary labels as determined by the label-generation
network  the multi-task network is trained to predict these labels for the auxiliary task  alongside the

3

(a) Two networks applied in MAXL

(b) SoftMax versus Mask SoftMax

Figure 2: (a) Illustration of the two networks which make up MAXL. Dashed white boxes represent
data generated by neural networks  solid white boxes represent given data  and coloured boxes
represent functions. The double arrow represents equivalence. (b) Illustration of vanilla SoftMax and
Mask SoftMax with 2 primary classes. Vanilla SoftMax outputs over all 4 auxiliary classes  whereas
Mask SoftMax outputs over a hierarchical structure ψ = [2  2].

ground-truth labels for the primary task. For both the primary and auxiliary tasks  we apply the focal
loss [22] with a focusing parameter γ = 2  deﬁned as:

L(ˆy  y) = −y(1 − ˆy)γ log(ˆy) 

(1)
where ˆy is the predicted label and y is the target label. The focal loss helps to focus on the incorrectly
predicted labels  which we found improved performance during our experimental evaluation compared
with the regular cross-entropy log loss.
To update parameters θ1 of the multi-task network  we deﬁne the multi-task objective as follows:

(x(i))  ypri

(i)) + L(f aux

θ1

(x(i))  yaux
(i) )

(2)

(cid:16)L(f pri

θ1

arg min

θ1

(cid:17)

(cid:17)

(x(i)  ypri

(i) = ggen

where (i) represents the ith batch from the training data  and yaux
by the label-generation network.
In the second stage of each epoch  the label-generation network is then updated by encouraging
auxiliary labels to be chosen such that  if the multi-task network were to be trained using these
auxiliary labels  the performance of the primary task would be maximised on this same training data.
Leveraging the performance of the multi-task network to train the label-generation network can be
considered as a form of meta learning. Therefore  to update parameters θ2 of the label-generation
network  we deﬁne the meta objective as follows:
L(f pri

(i)  ψ) is generated

arg min

θ2

(3)

(x(i))  ypri

(i)) .

θ2

θ+
1

1 represents the weights of the multi-task network after one gradient update using the

Here  θ+
multi-task loss deﬁned in Equation 2:
1 = θ1 − α∇θ1
θ+

(cid:16)L(f

pri
θ1

(x(i))  y

pri

(i)) + L(f aux

θ1 (x(i))  yaux
(i) )

 

(4)

where α is the learning rate.
The trick in this meta objective is that we perform a derivative over a derivative (a Hessian matrix) to
update θ2  by using a retained computational graph of θ+
1 in order to compute derivatives with respect
to θ2. This second derivative trick was also proposed in several other meta-learning frameworks such
as [10] and [35].
However  we found that the generated auxiliary labels can easily collapse  such that the label-
generation network always generates the same auxiliary label. This leaves parameters θ2 in a local

4

Label-GenerationNetworkMulti-taskNetworkx<latexit sha1_base64="JAhJ72QyRF1aVPzO9TRxJqzPOlU=">AAACKHicbVDLSsNAFJ3UV42vVpdugkVwY0lE0GXRjcsW7APaUCaTm3ToZBJmJmoJ/QK3+hN+jTvp1i9x0kawrRcuHM65l3vu8RJGpbLtmVHa2Nza3invmnv7B4dHlepxR8apINAmMYtFz8MSGOXQVlQx6CUCcOQx6Hrj+1zvPoGQNOaPapKAG+GQ04ASrDTVehlWanbdnpe1DpwC1FBRzWHVqA78mKQRcEUYlrLv2IlyMywUJQym5iCVkGAyxiH0NeQ4Aulmc6dT61wzvhXEQjdX1pz9u5HhSMpJ5OnJCKuRXNVy8j+tn6rg1s0oT1IFnCwOBSmzVGzlb1s+FUAUm2iAiaDaq0VGWGCidDhLV8ALNR/F3F/5RTxf/ipuljvwQdKQT01Th+isRrYOOld1x647reta466Is4xO0Rm6QA66QQ30gJqojQgC9Ire0LvxYXwaX8ZsMVoyip0TtFTG9w/mMaY1</latexit><latexit sha1_base64="JAhJ72QyRF1aVPzO9TRxJqzPOlU=">AAACKHicbVDLSsNAFJ3UV42vVpdugkVwY0lE0GXRjcsW7APaUCaTm3ToZBJmJmoJ/QK3+hN+jTvp1i9x0kawrRcuHM65l3vu8RJGpbLtmVHa2Nza3invmnv7B4dHlepxR8apINAmMYtFz8MSGOXQVlQx6CUCcOQx6Hrj+1zvPoGQNOaPapKAG+GQ04ASrDTVehlWanbdnpe1DpwC1FBRzWHVqA78mKQRcEUYlrLv2IlyMywUJQym5iCVkGAyxiH0NeQ4Aulmc6dT61wzvhXEQjdX1pz9u5HhSMpJ5OnJCKuRXNVy8j+tn6rg1s0oT1IFnCwOBSmzVGzlb1s+FUAUm2iAiaDaq0VGWGCidDhLV8ALNR/F3F/5RTxf/ipuljvwQdKQT01Th+isRrYOOld1x647reta466Is4xO0Rm6QA66QQ30gJqojQgC9Ire0LvxYXwaX8ZsMVoyip0TtFTG9w/mMaY1</latexit><latexit sha1_base64="JAhJ72QyRF1aVPzO9TRxJqzPOlU=">AAACKHicbVDLSsNAFJ3UV42vVpdugkVwY0lE0GXRjcsW7APaUCaTm3ToZBJmJmoJ/QK3+hN+jTvp1i9x0kawrRcuHM65l3vu8RJGpbLtmVHa2Nza3invmnv7B4dHlepxR8apINAmMYtFz8MSGOXQVlQx6CUCcOQx6Hrj+1zvPoGQNOaPapKAG+GQ04ASrDTVehlWanbdnpe1DpwC1FBRzWHVqA78mKQRcEUYlrLv2IlyMywUJQym5iCVkGAyxiH0NeQ4Aulmc6dT61wzvhXEQjdX1pz9u5HhSMpJ5OnJCKuRXNVy8j+tn6rg1s0oT1IFnCwOBSmzVGzlb1s+FUAUm2iAiaDaq0VGWGCidDhLV8ALNR/F3F/5RTxf/ipuljvwQdKQT01Th+isRrYOOld1x647reta466Is4xO0Rm6QA66QQ30gJqojQgC9Ire0LvxYXwaX8ZsMVoyip0TtFTG9w/mMaY1</latexit><latexit sha1_base64="JAhJ72QyRF1aVPzO9TRxJqzPOlU=">AAACKHicbVDLSsNAFJ3UV42vVpdugkVwY0lE0GXRjcsW7APaUCaTm3ToZBJmJmoJ/QK3+hN+jTvp1i9x0kawrRcuHM65l3vu8RJGpbLtmVHa2Nza3invmnv7B4dHlepxR8apINAmMYtFz8MSGOXQVlQx6CUCcOQx6Hrj+1zvPoGQNOaPapKAG+GQ04ASrDTVehlWanbdnpe1DpwC1FBRzWHVqA78mKQRcEUYlrLv2IlyMywUJQym5iCVkGAyxiH0NeQ4Aulmc6dT61wzvhXEQjdX1pz9u5HhSMpJ5OnJCKuRXNVy8j+tn6rg1s0oT1IFnCwOBSmzVGzlb1s+FUAUm2iAiaDaq0VGWGCidDhLV8ALNR/F3F/5RTxf/ipuljvwQdKQT01Th+isRrYOOld1x647reta466Is4xO0Rm6QA66QQ30gJqojQgC9Ire0LvxYXwaX8ZsMVoyip0TtFTG9w/mMaY1</latexit>f✓1(x)<latexit sha1_base64="vaOzb09dIwUhNmkeDbmjyRzMELo=">AAACOHicbVDLSsNAFJ34Nj7a6tJNsAi6sCQi6FJ047KCVaENYTK5SQcnkzBzo5bQL3GrP+GfuHMnbv0Cpw/Bth4YOJxzL/fMCXPBNbruuzU3v7C4tLyyaq+tb2xWqrWtG50VikGLZSJTdyHVILiEFnIUcJcroGko4Da8vxj4tw+gNM/kNfZy8FOaSB5zRtFIQbUSB2UHu4A08Pr7TwdBte423CGcWeKNSZ2M0QxqVq0TZaxIQSITVOu25+bol1QhZwL6dqfQkFN2TxNoGyppCtovh8n7zp5RIifOlHkSnaH6d6Okqda9NDSTKcWunvYG4n9eu8D41C+5zAsEyUaH4kI4mDmDGpyIK2AoeoZQprjJ6rAuVZShKWviCoSJ0dNMRlN/UY+Hv45fDhJEoHki+7ZtSvSmK5slN0cNz214V8f1s/NxnStkh+ySfeKRE3JGLkmTtAgjBXkmL+TVerM+rE/razQ6Z413tskErO8fiQ+sAg==</latexit><latexit sha1_base64="vaOzb09dIwUhNmkeDbmjyRzMELo=">AAACOHicbVDLSsNAFJ34Nj7a6tJNsAi6sCQi6FJ047KCVaENYTK5SQcnkzBzo5bQL3GrP+GfuHMnbv0Cpw/Bth4YOJxzL/fMCXPBNbruuzU3v7C4tLyyaq+tb2xWqrWtG50VikGLZSJTdyHVILiEFnIUcJcroGko4Da8vxj4tw+gNM/kNfZy8FOaSB5zRtFIQbUSB2UHu4A08Pr7TwdBte423CGcWeKNSZ2M0QxqVq0TZaxIQSITVOu25+bol1QhZwL6dqfQkFN2TxNoGyppCtovh8n7zp5RIifOlHkSnaH6d6Okqda9NDSTKcWunvYG4n9eu8D41C+5zAsEyUaH4kI4mDmDGpyIK2AoeoZQprjJ6rAuVZShKWviCoSJ0dNMRlN/UY+Hv45fDhJEoHki+7ZtSvSmK5slN0cNz214V8f1s/NxnStkh+ySfeKRE3JGLkmTtAgjBXkmL+TVerM+rE/razQ6Z413tskErO8fiQ+sAg==</latexit><latexit sha1_base64="vaOzb09dIwUhNmkeDbmjyRzMELo=">AAACOHicbVDLSsNAFJ34Nj7a6tJNsAi6sCQi6FJ047KCVaENYTK5SQcnkzBzo5bQL3GrP+GfuHMnbv0Cpw/Bth4YOJxzL/fMCXPBNbruuzU3v7C4tLyyaq+tb2xWqrWtG50VikGLZSJTdyHVILiEFnIUcJcroGko4Da8vxj4tw+gNM/kNfZy8FOaSB5zRtFIQbUSB2UHu4A08Pr7TwdBte423CGcWeKNSZ2M0QxqVq0TZaxIQSITVOu25+bol1QhZwL6dqfQkFN2TxNoGyppCtovh8n7zp5RIifOlHkSnaH6d6Okqda9NDSTKcWunvYG4n9eu8D41C+5zAsEyUaH4kI4mDmDGpyIK2AoeoZQprjJ6rAuVZShKWviCoSJ0dNMRlN/UY+Hv45fDhJEoHki+7ZtSvSmK5slN0cNz214V8f1s/NxnStkh+ySfeKRE3JGLkmTtAgjBXkmL+TVerM+rE/razQ6Z413tskErO8fiQ+sAg==</latexit><latexit sha1_base64="vaOzb09dIwUhNmkeDbmjyRzMELo=">AAACOHicbVDLSsNAFJ34Nj7a6tJNsAi6sCQi6FJ047KCVaENYTK5SQcnkzBzo5bQL3GrP+GfuHMnbv0Cpw/Bth4YOJxzL/fMCXPBNbruuzU3v7C4tLyyaq+tb2xWqrWtG50VikGLZSJTdyHVILiEFnIUcJcroGko4Da8vxj4tw+gNM/kNfZy8FOaSB5zRtFIQbUSB2UHu4A08Pr7TwdBte423CGcWeKNSZ2M0QxqVq0TZaxIQSITVOu25+bol1QhZwL6dqfQkFN2TxNoGyppCtovh8n7zp5RIifOlHkSnaH6d6Okqda9NDSTKcWunvYG4n9eu8D41C+5zAsEyUaH4kI4mDmDGpyIK2AoeoZQprjJ6rAuVZShKWviCoSJ0dNMRlN/UY+Hv45fDhJEoHki+7ZtSvSmK5slN0cNz214V8f1s/NxnStkh+ySfeKRE3JGLkmTtAgjBXkmL+TVerM+rE/razQ6Z413tskErO8fiQ+sAg==</latexit>PrimaryTaskSoftMaxAuxiliaryTaskSoftMaxGeneratedTaskMaskSoftMaxfaux✓1(x)<latexit sha1_base64="aWqzEL6Ig/H4cJkqgq7ASsRiSZ8=">AAACRXicbVDLSsNAFJ34tr5aXeoiWARdWBIRdFl041LBVqGNYTK5aQcnkzBzoy0hG7/Grf6E3+BHuBO3On0IWj0wcOace7n3niAVXKPjvFpT0zOzc/MLi6Wl5ZXVtXJlvamTTDFosEQk6jqgGgSX0ECOAq5TBTQOBFwFt6cD/+oOlOaJvMR+Cl5MO5JHnFE0kl/eim7yNkIPc5r1isI3ny4g9d1it7fnl6tOzRnC/kvcMamSMc79ilVphwnLYpDIBNW65TopejlVyJmAotTONKSU3dIOtAyVNAbt5cMzCnvHKKEdJco8ifZQ/dmR01jrfhyYyphiV096A/E/r5VhdOzlXKYZgmSjQVEmbEzsQSZ2yBUwFH1DKFPc7GqzLlWUoUnu1xQIOkaPExlO3KLu978dLx9sEILmHVmUSiZEdzKyv6R5UHOdmntxWK2fjONcIJtkm+wSlxyROjkj56RBGHkgj+SJPFsv1pv1bn2MSqescc8G+QXr8wsItbJB</latexit><latexit sha1_base64="aWqzEL6Ig/H4cJkqgq7ASsRiSZ8=">AAACRXicbVDLSsNAFJ34tr5aXeoiWARdWBIRdFl041LBVqGNYTK5aQcnkzBzoy0hG7/Grf6E3+BHuBO3On0IWj0wcOace7n3niAVXKPjvFpT0zOzc/MLi6Wl5ZXVtXJlvamTTDFosEQk6jqgGgSX0ECOAq5TBTQOBFwFt6cD/+oOlOaJvMR+Cl5MO5JHnFE0kl/eim7yNkIPc5r1isI3ny4g9d1it7fnl6tOzRnC/kvcMamSMc79ilVphwnLYpDIBNW65TopejlVyJmAotTONKSU3dIOtAyVNAbt5cMzCnvHKKEdJco8ifZQ/dmR01jrfhyYyphiV096A/E/r5VhdOzlXKYZgmSjQVEmbEzsQSZ2yBUwFH1DKFPc7GqzLlWUoUnu1xQIOkaPExlO3KLu978dLx9sEILmHVmUSiZEdzKyv6R5UHOdmntxWK2fjONcIJtkm+wSlxyROjkj56RBGHkgj+SJPFsv1pv1bn2MSqescc8G+QXr8wsItbJB</latexit><latexit sha1_base64="aWqzEL6Ig/H4cJkqgq7ASsRiSZ8=">AAACRXicbVDLSsNAFJ34tr5aXeoiWARdWBIRdFl041LBVqGNYTK5aQcnkzBzoy0hG7/Grf6E3+BHuBO3On0IWj0wcOace7n3niAVXKPjvFpT0zOzc/MLi6Wl5ZXVtXJlvamTTDFosEQk6jqgGgSX0ECOAq5TBTQOBFwFt6cD/+oOlOaJvMR+Cl5MO5JHnFE0kl/eim7yNkIPc5r1isI3ny4g9d1it7fnl6tOzRnC/kvcMamSMc79ilVphwnLYpDIBNW65TopejlVyJmAotTONKSU3dIOtAyVNAbt5cMzCnvHKKEdJco8ifZQ/dmR01jrfhyYyphiV096A/E/r5VhdOzlXKYZgmSjQVEmbEzsQSZ2yBUwFH1DKFPc7GqzLlWUoUnu1xQIOkaPExlO3KLu978dLx9sEILmHVmUSiZEdzKyv6R5UHOdmntxWK2fjONcIJtkm+wSlxyROjkj56RBGHkgj+SJPFsv1pv1bn2MSqescc8G+QXr8wsItbJB</latexit><latexit sha1_base64="aWqzEL6Ig/H4cJkqgq7ASsRiSZ8=">AAACRXicbVDLSsNAFJ34tr5aXeoiWARdWBIRdFl041LBVqGNYTK5aQcnkzBzoy0hG7/Grf6E3+BHuBO3On0IWj0wcOace7n3niAVXKPjvFpT0zOzc/MLi6Wl5ZXVtXJlvamTTDFosEQk6jqgGgSX0ECOAq5TBTQOBFwFt6cD/+oOlOaJvMR+Cl5MO5JHnFE0kl/eim7yNkIPc5r1isI3ny4g9d1it7fnl6tOzRnC/kvcMamSMc79ilVphwnLYpDIBNW65TopejlVyJmAotTONKSU3dIOtAyVNAbt5cMzCnvHKKEdJco8ifZQ/dmR01jrfhyYyphiV096A/E/r5VhdOzlXKYZgmSjQVEmbEzsQSZ2yBUwFH1DKFPc7GqzLlWUoUnu1xQIOkaPExlO3KLu978dLx9sEILmHVmUSiZEdzKyv6R5UHOdmntxWK2fjONcIJtkm+wSlxyROjkj56RBGHkgj+SJPFsv1pv1bn2MSqescc8G+QXr8wsItbJB</latexit>g✓2(x)<latexit sha1_base64="wldqwOPRKbsEdsZiGouzX3HIIoE=">AAACOHicbVDLSsNAFJ34rPHV6tJNsAi6sCRF0KXoxmUFW4U2hMnkNh2cTMLMjVpCvsSt/oR/4s6duPULnD4ErR4YOJxzL/fMCTPBNbruqzU3v7C4tFxZsVfX1jc2q7Wtjk5zxaDNUpGqm5BqEFxCGzkKuMkU0CQUcB3eno/86ztQmqfyCocZ+AmNJe9zRtFIQXUzDooeDgBp0Cz3Hw6Cat1tuGM4f4k3JXUyRSuoWbVelLI8AYlMUK27npuhX1CFnAko7V6uIaPslsbQNVTSBLRfjJOXzp5RIqefKvMkOmP150ZBE62HSWgmE4oDPeuNxP+8bo79E7/gMssRJJsc6ufCwdQZ1eBEXAFDMTSEMsVNVocNqKIMTVm/rkAYGz1JZTTzF3V/+O34xShBBJrHsrRtU6I3W9lf0mk2PLfhXR7VT8+mdVbIDtkl+8Qjx+SUXJAWaRNGcvJInsiz9WK9We/Wx2R0zprubJNfsD6/AIyprAQ=</latexit><latexit sha1_base64="wldqwOPRKbsEdsZiGouzX3HIIoE=">AAACOHicbVDLSsNAFJ34rPHV6tJNsAi6sCRF0KXoxmUFW4U2hMnkNh2cTMLMjVpCvsSt/oR/4s6duPULnD4ErR4YOJxzL/fMCTPBNbruqzU3v7C4tFxZsVfX1jc2q7Wtjk5zxaDNUpGqm5BqEFxCGzkKuMkU0CQUcB3eno/86ztQmqfyCocZ+AmNJe9zRtFIQXUzDooeDgBp0Cz3Hw6Cat1tuGM4f4k3JXUyRSuoWbVelLI8AYlMUK27npuhX1CFnAko7V6uIaPslsbQNVTSBLRfjJOXzp5RIqefKvMkOmP150ZBE62HSWgmE4oDPeuNxP+8bo79E7/gMssRJJsc6ufCwdQZ1eBEXAFDMTSEMsVNVocNqKIMTVm/rkAYGz1JZTTzF3V/+O34xShBBJrHsrRtU6I3W9lf0mk2PLfhXR7VT8+mdVbIDtkl+8Qjx+SUXJAWaRNGcvJInsiz9WK9We/Wx2R0zprubJNfsD6/AIyprAQ=</latexit><latexit sha1_base64="wldqwOPRKbsEdsZiGouzX3HIIoE=">AAACOHicbVDLSsNAFJ34rPHV6tJNsAi6sCRF0KXoxmUFW4U2hMnkNh2cTMLMjVpCvsSt/oR/4s6duPULnD4ErR4YOJxzL/fMCTPBNbruqzU3v7C4tFxZsVfX1jc2q7Wtjk5zxaDNUpGqm5BqEFxCGzkKuMkU0CQUcB3eno/86ztQmqfyCocZ+AmNJe9zRtFIQXUzDooeDgBp0Cz3Hw6Cat1tuGM4f4k3JXUyRSuoWbVelLI8AYlMUK27npuhX1CFnAko7V6uIaPslsbQNVTSBLRfjJOXzp5RIqefKvMkOmP150ZBE62HSWgmE4oDPeuNxP+8bo79E7/gMssRJJsc6ufCwdQZ1eBEXAFDMTSEMsVNVocNqKIMTVm/rkAYGz1JZTTzF3V/+O34xShBBJrHsrRtU6I3W9lf0mk2PLfhXR7VT8+mdVbIDtkl+8Qjx+SUXJAWaRNGcvJInsiz9WK9We/Wx2R0zprubJNfsD6/AIyprAQ=</latexit><latexit sha1_base64="wldqwOPRKbsEdsZiGouzX3HIIoE=">AAACOHicbVDLSsNAFJ34rPHV6tJNsAi6sCRF0KXoxmUFW4U2hMnkNh2cTMLMjVpCvsSt/oR/4s6duPULnD4ErR4YOJxzL/fMCTPBNbruqzU3v7C4tFxZsVfX1jc2q7Wtjk5zxaDNUpGqm5BqEFxCGzkKuMkU0CQUcB3eno/86ztQmqfyCocZ+AmNJe9zRtFIQXUzDooeDgBp0Cz3Hw6Cat1tuGM4f4k3JXUyRSuoWbVelLI8AYlMUK27npuhX1CFnAko7V6uIaPslsbQNVTSBLRfjJOXzp5RIqefKvMkOmP150ZBE62HSWgmE4oDPeuNxP+8bo79E7/gMssRJJsc6ufCwdQZ1eBEXAFDMTSEMsVNVocNqKIMTVm/rkAYGz1JZTTzF3V/+O34xShBBJrHsrRtU6I3W9lf0mk2PLfhXR7VT8+mdVbIDtkl+8Qjx+SUXJAWaRNGcvJInsiz9WK9We/Wx2R0zprubJNfsD6/AIyprAQ=</latexit>fpri✓1(x)<latexit sha1_base64="NyKaTxuKybYh2fN99hg+QvfOhZ0=">AAACRHicbVBNb9NAEB2n0IYAJSlHOFitkMIhkc2lPVblUI5BIh9SYqz1epyssl5bu+O2keULx/4SrvALOPEf+A/cEFfE5gOpSRlppaf33szsvCiXwpDn/XBqew8e7h/UHzUeP3l6+KzZOhqYrNAc+zyTmR5FzKAUCvskSOIo18jSSOIwmr9d6sMr1EZk6gMtcgxSNlUiEZyRpcLmyyQsJzRDYqFffbQQb6jMtaiq9s3rsHnidb1VufeBvwEn553L228A0AtbTnMSZ7xIURGXzJix7+UUlEyT4BKrxqQwmDM+Z1McW6hYiiYoV2dU7ivLxG6SafsUuSv2bkfJUmMWaWSdKaOZ2dWW5P+0cUHJWVAKlReEiq8XJYV0KXOXmbix0MhJLixgXAv7V5fPmGacbHLbk/R1Z2qVNFNxUC43xWjEVG1dVmL0z1M1bIb+bmL3weBN1/e6/nsb5gWsqw4v4Bja4MMpnMM76EEfOHyCz/AFvjrfnZ/OL+f32lpzNj3PYaucP38BhLa0KA==</latexit><latexit sha1_base64="7TJTuUjm7A8V8knamWwgueSpGWc=">AAACRHicbVBNS8NAEN34bf2qetRDUAQ9WBIveix6sMcKVoU2hs1m0i5uNmF3opaQi0d/iUf1R4h/wf/gTbyK21bBqgMLj/fezOy8IBVco+O8WCOjY+MTk1PTpZnZufmF8uLSiU4yxaDBEpGos4BqEFxCAzkKOEsV0DgQcBpcHPT000tQmifyGLspeDFtSx5xRtFQfnk18vMWdgCp7xbnBsI15qniRbF5veWX152K0y/7L3C/wHp1+/D2/vm2VvcXrXIrTFgWg0QmqNZN10nRy6lCzgQUpVamIaXsgrahaaCkMWgv759R2BuGCe0oUeZJtPvsz46cxlp348A4Y4od/Vvrkf9pzQyjPS/nMs0QJBssijJhY2L3MrFDroCh6BpAmeLmrzbrUEUZmuSGJ6mr7bZR4kSGXt7bFILmbTl0WQ7Bt6comQzd34n9BSc7FdepuEcmzH0yqCmyQtbIJnHJLqmSGqmTBmHkhtyRB/JoPVmv1pv1PrCOWF89y2SorI9P9Ia1sg==</latexit><latexit sha1_base64="7TJTuUjm7A8V8knamWwgueSpGWc=">AAACRHicbVBNS8NAEN34bf2qetRDUAQ9WBIveix6sMcKVoU2hs1m0i5uNmF3opaQi0d/iUf1R4h/wf/gTbyK21bBqgMLj/fezOy8IBVco+O8WCOjY+MTk1PTpZnZufmF8uLSiU4yxaDBEpGos4BqEFxCAzkKOEsV0DgQcBpcHPT000tQmifyGLspeDFtSx5xRtFQfnk18vMWdgCp7xbnBsI15qniRbF5veWX152K0y/7L3C/wHp1+/D2/vm2VvcXrXIrTFgWg0QmqNZN10nRy6lCzgQUpVamIaXsgrahaaCkMWgv759R2BuGCe0oUeZJtPvsz46cxlp348A4Y4od/Vvrkf9pzQyjPS/nMs0QJBssijJhY2L3MrFDroCh6BpAmeLmrzbrUEUZmuSGJ6mr7bZR4kSGXt7bFILmbTl0WQ7Bt6comQzd34n9BSc7FdepuEcmzH0yqCmyQtbIJnHJLqmSGqmTBmHkhtyRB/JoPVmv1pv1PrCOWF89y2SorI9P9Ia1sg==</latexit><latexit sha1_base64="u7eg2qhV8ABOWEjIoF/okCfBDgA=">AAACRHicbVDLSsNAFJ3Ud31VXeoiWARdWBI3uhTduKxgW6GNYTK5aYdOJmHmRi0hG7/GrX6E/+A/uBO34vQhWPXCwOGcc++de4JUcI2O82qVZmbn5hcWl8rLK6tr65WNzaZOMsWgwRKRqOuAahBcQgM5CrhOFdA4ENAK+udDvXULSvNEXuEgBS+mXckjzigayq/sRH7ewR4g9d3ixkC4xzxVvCj27w/8StWpOaOy/wJ3AqpkUnV/w6p0woRlMUhkgmrddp0UvZwq5ExAUe5kGlLK+rQLbQMljUF7+eiMwt4zTGhHiTJPoj1if3bkNNZ6EAfGGVPs6d/akPxPa2cYnXg5l2mGINl4UZQJGxN7mIkdcgUMxcAAyhQ3f7VZjyrK0CQ3PUndHXaNEicy9PLhphA078qpy3IIvj1F2WTo/k7sL2ge1Vyn5l461dOzSZqLZJvskn3ikmNySi5InTQIIw/kkTyRZ+vFerPerY+xtWRNerbIVFmfX1cYsio=</latexit>y(cid:81)(cid:83)(cid:74)<latexit sha1_base64="pmpxilBbwbafOLqdHvy5+dcivQg=">AAACMnicbVDLSsNAFJ3UV62vVpdugkVwY0lE0GXRjcsK9gFtLJPJbTt0MgkzN2oJ+Q23+hH+jO7ErR/h9CHY1gMDh3Pua44fC67Rcd6t3Mrq2vpGfrOwtb2zu1cs7Td0lCgGdRaJSLV8qkFwCXXkKKAVK6ChL6DpD6/HfvMBlOaRvMNRDF5I+5L3OKNopM7ovoPwhGmseNYtlp2KM4G9TNwZKZMZat2SVewEEUtCkMgE1brtOjF6KVXImYCs0Ek0xJQNaR/ahkoagvbSydGZfWyUwO5FyjyJ9kT925HSUOtR6JvKkOJAL3pj8T+vnWDv0ku5jBMEyaaLeomwMbLHCdgBV8BQjAyhTHFzq80GVFGGJqf5SerxtG+cMJKBl443BaB5X879LAX/tyYrmAzdxcSWSeOs4joV9/a8XL2apZknh+SInBCXXJAquSE1UieMxOSZvJBX6836sD6tr2lpzpr1HJA5WN8/ZeqrVg==</latexit><latexit sha1_base64="pmpxilBbwbafOLqdHvy5+dcivQg=">AAACMnicbVDLSsNAFJ3UV62vVpdugkVwY0lE0GXRjcsK9gFtLJPJbTt0MgkzN2oJ+Q23+hH+jO7ErR/h9CHY1gMDh3Pua44fC67Rcd6t3Mrq2vpGfrOwtb2zu1cs7Td0lCgGdRaJSLV8qkFwCXXkKKAVK6ChL6DpD6/HfvMBlOaRvMNRDF5I+5L3OKNopM7ovoPwhGmseNYtlp2KM4G9TNwZKZMZat2SVewEEUtCkMgE1brtOjF6KVXImYCs0Ek0xJQNaR/ahkoagvbSydGZfWyUwO5FyjyJ9kT925HSUOtR6JvKkOJAL3pj8T+vnWDv0ku5jBMEyaaLeomwMbLHCdgBV8BQjAyhTHFzq80GVFGGJqf5SerxtG+cMJKBl443BaB5X879LAX/tyYrmAzdxcSWSeOs4joV9/a8XL2apZknh+SInBCXXJAquSE1UieMxOSZvJBX6836sD6tr2lpzpr1HJA5WN8/ZeqrVg==</latexit><latexit sha1_base64="pmpxilBbwbafOLqdHvy5+dcivQg=">AAACMnicbVDLSsNAFJ3UV62vVpdugkVwY0lE0GXRjcsK9gFtLJPJbTt0MgkzN2oJ+Q23+hH+jO7ErR/h9CHY1gMDh3Pua44fC67Rcd6t3Mrq2vpGfrOwtb2zu1cs7Td0lCgGdRaJSLV8qkFwCXXkKKAVK6ChL6DpD6/HfvMBlOaRvMNRDF5I+5L3OKNopM7ovoPwhGmseNYtlp2KM4G9TNwZKZMZat2SVewEEUtCkMgE1brtOjF6KVXImYCs0Ek0xJQNaR/ahkoagvbSydGZfWyUwO5FyjyJ9kT925HSUOtR6JvKkOJAL3pj8T+vnWDv0ku5jBMEyaaLeomwMbLHCdgBV8BQjAyhTHFzq80GVFGGJqf5SerxtG+cMJKBl443BaB5X879LAX/tyYrmAzdxcSWSeOs4joV9/a8XL2apZknh+SInBCXXJAquSE1UieMxOSZvJBX6836sD6tr2lpzpr1HJA5WN8/ZeqrVg==</latexit><latexit sha1_base64="pmpxilBbwbafOLqdHvy5+dcivQg=">AAACMnicbVDLSsNAFJ3UV62vVpdugkVwY0lE0GXRjcsK9gFtLJPJbTt0MgkzN2oJ+Q23+hH+jO7ErR/h9CHY1gMDh3Pua44fC67Rcd6t3Mrq2vpGfrOwtb2zu1cs7Td0lCgGdRaJSLV8qkFwCXXkKKAVK6ChL6DpD6/HfvMBlOaRvMNRDF5I+5L3OKNopM7ovoPwhGmseNYtlp2KM4G9TNwZKZMZat2SVewEEUtCkMgE1brtOjF6KVXImYCs0Ek0xJQNaR/ahkoagvbSydGZfWyUwO5FyjyJ9kT925HSUOtR6JvKkOJAL3pj8T+vnWDv0ku5jBMEyaaLeomwMbLHCdgBV8BQjAyhTHFzq80GVFGGJqf5SerxtG+cMJKBl443BaB5X879LAX/tyYrmAzdxcSWSeOs4joV9/a8XL2apZknh+SInBCXXJAquSE1UieMxOSZvJBX6836sD6tr2lpzpr1HJA5WN8/ZeqrVg==</latexit>y(cid:66)(cid:86)(cid:89)<latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit>y(cid:81)(cid:83)(cid:74)  <latexit sha1_base64="1Q7/7w/gGdd2NJVWnft8iJftUAQ=">AAACOXicbVDLSsNAFJ3UV62vVpdugkVwoSURQZdFNy4r2Ae0sUwmt+3QySTM3FhL6Ke41Y/wS1y6E7f+gNOHYFsPDBzOua85fiy4Rsd5tzIrq2vrG9nN3Nb2zu5evrBf01GiGFRZJCLV8KkGwSVUkaOARqyAhr6Aut+/Gfv1R1CaR/IehzF4Ie1K3uGMopHa+cLwoYXwhGms+Oi0FWvezhedkjOBvUzcGSmSGSrtgpVvBRFLQpDIBNW66ToxeilVyJmAUa6VaIgp69MuNA2VNATtpZPbR/axUQK7EynzJNoT9W9HSkOth6FvKkOKPb3ojcX/vGaCnSsv5TJOECSbLuokwsbIHgdhB1wBQzE0hDLFza0261FFGZq45iepwVnXOGEkAy8dbwpA866c+1kK/m/NKGcydBcTWya185LrlNy7i2L5epZmlhySI3JCXHJJyuSWVEiVMDIgz+SFvFpv1of1aX1NSzPWrOeAzMH6/gH1pa2N</latexit><latexit sha1_base64="1Q7/7w/gGdd2NJVWnft8iJftUAQ=">AAACOXicbVDLSsNAFJ3UV62vVpdugkVwoSURQZdFNy4r2Ae0sUwmt+3QySTM3FhL6Ke41Y/wS1y6E7f+gNOHYFsPDBzOua85fiy4Rsd5tzIrq2vrG9nN3Nb2zu5evrBf01GiGFRZJCLV8KkGwSVUkaOARqyAhr6Aut+/Gfv1R1CaR/IehzF4Ie1K3uGMopHa+cLwoYXwhGms+Oi0FWvezhedkjOBvUzcGSmSGSrtgpVvBRFLQpDIBNW66ToxeilVyJmAUa6VaIgp69MuNA2VNATtpZPbR/axUQK7EynzJNoT9W9HSkOth6FvKkOKPb3ojcX/vGaCnSsv5TJOECSbLuokwsbIHgdhB1wBQzE0hDLFza0261FFGZq45iepwVnXOGEkAy8dbwpA866c+1kK/m/NKGcydBcTWya185LrlNy7i2L5epZmlhySI3JCXHJJyuSWVEiVMDIgz+SFvFpv1of1aX1NSzPWrOeAzMH6/gH1pa2N</latexit><latexit sha1_base64="1Q7/7w/gGdd2NJVWnft8iJftUAQ=">AAACOXicbVDLSsNAFJ3UV62vVpdugkVwoSURQZdFNy4r2Ae0sUwmt+3QySTM3FhL6Ke41Y/wS1y6E7f+gNOHYFsPDBzOua85fiy4Rsd5tzIrq2vrG9nN3Nb2zu5evrBf01GiGFRZJCLV8KkGwSVUkaOARqyAhr6Aut+/Gfv1R1CaR/IehzF4Ie1K3uGMopHa+cLwoYXwhGms+Oi0FWvezhedkjOBvUzcGSmSGSrtgpVvBRFLQpDIBNW66ToxeilVyJmAUa6VaIgp69MuNA2VNATtpZPbR/axUQK7EynzJNoT9W9HSkOth6FvKkOKPb3ojcX/vGaCnSsv5TJOECSbLuokwsbIHgdhB1wBQzE0hDLFza0261FFGZq45iepwVnXOGEkAy8dbwpA866c+1kK/m/NKGcydBcTWya185LrlNy7i2L5epZmlhySI3JCXHJJyuSWVEiVMDIgz+SFvFpv1of1aX1NSzPWrOeAzMH6/gH1pa2N</latexit><latexit sha1_base64="1Q7/7w/gGdd2NJVWnft8iJftUAQ=">AAACOXicbVDLSsNAFJ3UV62vVpdugkVwoSURQZdFNy4r2Ae0sUwmt+3QySTM3FhL6Ke41Y/wS1y6E7f+gNOHYFsPDBzOua85fiy4Rsd5tzIrq2vrG9nN3Nb2zu5evrBf01GiGFRZJCLV8KkGwSVUkaOARqyAhr6Aut+/Gfv1R1CaR/IehzF4Ie1K3uGMopHa+cLwoYXwhGms+Oi0FWvezhedkjOBvUzcGSmSGSrtgpVvBRFLQpDIBNW66ToxeilVyJmAUa6VaIgp69MuNA2VNATtpZPbR/axUQK7EynzJNoT9W9HSkOth6FvKkOKPb3ojcX/vGaCnSsv5TJOECSbLuokwsbIHgdhB1wBQzE0hDLFza0261FFGZq45iepwVnXOGEkAy8dbwpA866c+1kK/m/NKGcydBcTWya185LrlNy7i2L5epZmlhySI3JCXHJJyuSWVEiVMDIgz+SFvFpv1of1aX1NSzPWrOeAzMH6/gH1pa2N</latexit>PredictionTargetg(cid:72)(cid:70)(cid:79)✓2(x y(cid:81)(cid:83)(cid:74)  )<latexit sha1_base64="Epms109afkLPDajCuu5zCsO0R7U=">AAACVnicbVDLSsQwFE3raxxfVZduioOgoEMrgi4H3bhUcFSYqSVN73SCaVqSW3Uo/Qu/xq1+hP6MmHkIjnogcHLOvbm5J8oF1+h5H5Y9Mzs3v1BbrC8tr6yuOesb1zorFIM2y0SmbiOqQXAJbeQo4DZXQNNIwE10fzb0bx5AaZ7JKxzkEKQ0kbzHGUUjhU4zuSu7CE9YJiCrKjSXPiAND6vdp/3B3djKFa/2u7nme6HT8JreCO5f4k9Ig0xwEa5bTjfOWJGCRCao1h3fyzEoqULOBFT1bqEhp+yeJtAxVNIUdFCOFqvcHaPEbi9T5kh0R+rPjpKmWg/SyFSmFPv6tzcU//M6BfZOgpLLvECQbDyoVwgXM3eYkhtzBQzFwBDKFDd/dVmfKsrQZDn9kno8SIyTZjIOyuGkGDRP5NRmJUTfNVXdZOj/TuwvuT5s+l7TvzxqtE4nadbIFtkmu8Qnx6RFzskFaRNGnskLeSVv1rv1ac/ZC+NS25r0bJIp2M4XalO3EQ==</latexit><latexit sha1_base64="Epms109afkLPDajCuu5zCsO0R7U=">AAACVnicbVDLSsQwFE3raxxfVZduioOgoEMrgi4H3bhUcFSYqSVN73SCaVqSW3Uo/Qu/xq1+hP6MmHkIjnogcHLOvbm5J8oF1+h5H5Y9Mzs3v1BbrC8tr6yuOesb1zorFIM2y0SmbiOqQXAJbeQo4DZXQNNIwE10fzb0bx5AaZ7JKxzkEKQ0kbzHGUUjhU4zuSu7CE9YJiCrKjSXPiAND6vdp/3B3djKFa/2u7nme6HT8JreCO5f4k9Ig0xwEa5bTjfOWJGCRCao1h3fyzEoqULOBFT1bqEhp+yeJtAxVNIUdFCOFqvcHaPEbi9T5kh0R+rPjpKmWg/SyFSmFPv6tzcU//M6BfZOgpLLvECQbDyoVwgXM3eYkhtzBQzFwBDKFDd/dVmfKsrQZDn9kno8SIyTZjIOyuGkGDRP5NRmJUTfNVXdZOj/TuwvuT5s+l7TvzxqtE4nadbIFtkmu8Qnx6RFzskFaRNGnskLeSVv1rv1ac/ZC+NS25r0bJIp2M4XalO3EQ==</latexit><latexit sha1_base64="Epms109afkLPDajCuu5zCsO0R7U=">AAACVnicbVDLSsQwFE3raxxfVZduioOgoEMrgi4H3bhUcFSYqSVN73SCaVqSW3Uo/Qu/xq1+hP6MmHkIjnogcHLOvbm5J8oF1+h5H5Y9Mzs3v1BbrC8tr6yuOesb1zorFIM2y0SmbiOqQXAJbeQo4DZXQNNIwE10fzb0bx5AaZ7JKxzkEKQ0kbzHGUUjhU4zuSu7CE9YJiCrKjSXPiAND6vdp/3B3djKFa/2u7nme6HT8JreCO5f4k9Ig0xwEa5bTjfOWJGCRCao1h3fyzEoqULOBFT1bqEhp+yeJtAxVNIUdFCOFqvcHaPEbi9T5kh0R+rPjpKmWg/SyFSmFPv6tzcU//M6BfZOgpLLvECQbDyoVwgXM3eYkhtzBQzFwBDKFDd/dVmfKsrQZDn9kno8SIyTZjIOyuGkGDRP5NRmJUTfNVXdZOj/TuwvuT5s+l7TvzxqtE4nadbIFtkmu8Qnx6RFzskFaRNGnskLeSVv1rv1ac/ZC+NS25r0bJIp2M4XalO3EQ==</latexit><latexit sha1_base64="Epms109afkLPDajCuu5zCsO0R7U=">AAACVnicbVDLSsQwFE3raxxfVZduioOgoEMrgi4H3bhUcFSYqSVN73SCaVqSW3Uo/Qu/xq1+hP6MmHkIjnogcHLOvbm5J8oF1+h5H5Y9Mzs3v1BbrC8tr6yuOesb1zorFIM2y0SmbiOqQXAJbeQo4DZXQNNIwE10fzb0bx5AaZ7JKxzkEKQ0kbzHGUUjhU4zuSu7CE9YJiCrKjSXPiAND6vdp/3B3djKFa/2u7nme6HT8JreCO5f4k9Ig0xwEa5bTjfOWJGCRCao1h3fyzEoqULOBFT1bqEhp+yeJtAxVNIUdFCOFqvcHaPEbi9T5kh0R+rPjpKmWg/SyFSmFPv6tzcU//M6BfZOgpLLvECQbDyoVwgXM3eYkhtzBQzFwBDKFDd/dVmfKsrQZDn9kno8SIyTZjIOyuGkGDRP5NRmJUTfNVXdZOj/TuwvuT5s+l7TvzxqtE4nadbIFtkmu8Qnx6RFzskFaRNGnskLeSVv1rv1ac/ZC+NS25r0bJIp2M4XalO3EQ==</latexit>y(cid:66)(cid:86)(cid:89)<latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit>y(cid:81)(cid:83)(cid:74)=0<latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit><latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit><latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit><latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit>y(cid:81)(cid:83)(cid:74)=1<latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit><latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit><latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit><latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit>y(cid:66)(cid:86)(cid:89)<latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit><latexit sha1_base64="wyD1LUrzWlVFLm63FXJsRtIZbkM=">AAACMnicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuiy6cVnBPqCNZTK5bYdOJmHmRltCf8OtfoQ/oztx60c4aSvY1gMDh3Pua44fC67Rcd6tldW19Y3N3FZ+e2d3b79QPKjrKFEMaiwSkWr6VIPgEmrIUUAzVkBDX0DDH9xkfuMRlOaRvMdRDF5Ie5J3OaNopPbooY0wxJQmw3GnUHLKzgT2MnFnpERmqHaKVqEdRCwJQSITVOuW68TopVQhZwLG+XaiIaZsQHvQMlTSELSXTo4e2ydGCexupMyTaE/Uvx0pDbUehb6pDCn29aKXif95rQS7V17KZZwgSDZd1E2EjZGdJWAHXAFDMTKEMsXNrTbrU0UZmpzmJ6mns55xwkgGXpptCkDznpz7WQr+b804bzJ0FxNbJvXzsuuU3buLUuV6lmaOHJFjckpcckkq5JZUSY0wEpNn8kJerTfrw/q0vqalK9as55DMwfr+AWsbq1k=</latexit>y(cid:81)(cid:83)(cid:74)=0<latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit><latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit><latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit><latexit sha1_base64="RMst+v0LzEh4uaL0NYIBsRIs6Ww=">AAACN3icbVDLSgMxFM34rPXV6tJNsAhuLDMi6EYQ3bisYB/QjiWTuW2DmcyQ3FHL0C9xqx/hp7hyJ279A9OHYFsPBA7n3FdOkEhh0HXfnYXFpeWV1dxafn1jc2u7UNypmTjVHKo8lrFuBMyAFAqqKFBCI9HAokBCPbi/Gvr1B9BGxOoW+wn4Eesq0RGcoZXahe3+XQvhCbNEi8E5dduFklt2R6DzxJuQEpmg0i46hVYY8zQChVwyY5qem6CfMY2CSxjkW6mBhPF71oWmpYpFYPxsdPmAHlglpJ1Y26eQjtS/HRmLjOlHga2MGPbMrDcU//OaKXbO/EyoJEVQfLyok0qKMR3GQEOhgaPsW8K4FvZWyntMM442rOlJ+vGoa50oVqGfDTeFYERXTf0sg+C3ZpC3GXqzic2T2nHZc8vezUnp4nKSZo7skX1ySDxySi7INamQKuEkJc/khbw6b86H8+l8jUsXnEnPLpmC8/0DXcGsMg==</latexit>y(cid:81)(cid:83)(cid:74)=1<latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit><latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit><latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit><latexit sha1_base64="35Cc4fhgKu0FHA1ZLgXhGmiIDMc=">AAACN3icbVDLSsNAFJ3UV62Ptrp0M1gEN5ZEBN0IRTcuFawKbSyTyW0dOpmEmRu1hHyJW/0IP8WVO3HrHzitEax6YOBwzn3NCRIpDLrui1OamZ2bXygvVpaWV1artfrahYlTzaHNYxnrq4AZkEJBGwVKuEo0sCiQcBkMj8f+5S1oI2J1jqME/IgNlOgLztBKvVp1dN1FuMcs0SI/pF6v1nCb7gT0L/EK0iAFTnt1p9YNY55GoJBLZkzHcxP0M6ZRcAl5pZsaSBgfsgF0LFUsAuNnk8tzumWVkPZjbZ9COlF/dmQsMmYUBbYyYnhjfntj8T+vk2L/wM+ESlIExb8W9VNJMabjGGgoNHCUI0sY18LeSvkN04yjDWt6kr7bGVgnilXoZ+NNIRgxUFM/yyD4rskrNkPvd2J/ycVu03Ob3tleo3VUpFkmG2STbBOP7JMWOSGnpE04SckDeSRPzrPz6rw571+lJafoWSdTcD4+AV+ErDM=</latexit>(cid:46)(cid:66)(cid:84)(cid:76)(cid:52)(cid:80)(cid:71)(cid:3005)(cid:46)(cid:66)(cid:89) =[2 2]<latexit sha1_base64="NVQbzrcan+IK7t0EDlSunHu7lAs=">AAACTHicbVDRShtBFJ2NrdpYNWkf+zI0FvqgYTcU9EWQ9qUvgqWNCtkl3J29G4fMzmxn7qphyQ/4NX1tP6Lv/Y++lUInMQWjXhg4nHPuvXNPWirpKAx/BY2VJ09X19afNTeeb25tt9ovTp2prMC+MMrY8xQcKqmxT5IUnpcWoUgVnqXjDzP97BKtk0Z/oUmJSQEjLXMpgDw1bO3EhNdUH4Mb888mp2O4jr9WkPEpj0snDwe93V4ybHXCbjgv/hBEC9BhizoZtoNWnBlRFahJKHBuEIUlJTVYkkLhtBlXDksQYxjhwEMNBbqknp8z5W88k/HcWP808Tl7t6OGwrlJkXpnAXTh7msz8jFtUFF+kNRSlxWhFreL8kpxMnyWDc+kRUFq4gEIK/1fubgAC4J8gsuT7NXeyCuF0VlSzzZl6ORIL11WY/rfM236DKP7iT0Ep71uFHajT+86R+8Xaa6zV+w1e8sits+O2Ed2wvpMsBv2jX1nP4Kfwe/gT/D31toIFj0v2VI1Vv8B5LezOA==</latexit><latexit sha1_base64="NVQbzrcan+IK7t0EDlSunHu7lAs=">AAACTHicbVDRShtBFJ2NrdpYNWkf+zI0FvqgYTcU9EWQ9qUvgqWNCtkl3J29G4fMzmxn7qphyQ/4NX1tP6Lv/Y++lUInMQWjXhg4nHPuvXNPWirpKAx/BY2VJ09X19afNTeeb25tt9ovTp2prMC+MMrY8xQcKqmxT5IUnpcWoUgVnqXjDzP97BKtk0Z/oUmJSQEjLXMpgDw1bO3EhNdUH4Mb888mp2O4jr9WkPEpj0snDwe93V4ybHXCbjgv/hBEC9BhizoZtoNWnBlRFahJKHBuEIUlJTVYkkLhtBlXDksQYxjhwEMNBbqknp8z5W88k/HcWP808Tl7t6OGwrlJkXpnAXTh7msz8jFtUFF+kNRSlxWhFreL8kpxMnyWDc+kRUFq4gEIK/1fubgAC4J8gsuT7NXeyCuF0VlSzzZl6ORIL11WY/rfM236DKP7iT0Ep71uFHajT+86R+8Xaa6zV+w1e8sits+O2Ed2wvpMsBv2jX1nP4Kfwe/gT/D31toIFj0v2VI1Vv8B5LezOA==</latexit><latexit sha1_base64="NVQbzrcan+IK7t0EDlSunHu7lAs=">AAACTHicbVDRShtBFJ2NrdpYNWkf+zI0FvqgYTcU9EWQ9qUvgqWNCtkl3J29G4fMzmxn7qphyQ/4NX1tP6Lv/Y++lUInMQWjXhg4nHPuvXNPWirpKAx/BY2VJ09X19afNTeeb25tt9ovTp2prMC+MMrY8xQcKqmxT5IUnpcWoUgVnqXjDzP97BKtk0Z/oUmJSQEjLXMpgDw1bO3EhNdUH4Mb888mp2O4jr9WkPEpj0snDwe93V4ybHXCbjgv/hBEC9BhizoZtoNWnBlRFahJKHBuEIUlJTVYkkLhtBlXDksQYxjhwEMNBbqknp8z5W88k/HcWP808Tl7t6OGwrlJkXpnAXTh7msz8jFtUFF+kNRSlxWhFreL8kpxMnyWDc+kRUFq4gEIK/1fubgAC4J8gsuT7NXeyCuF0VlSzzZl6ORIL11WY/rfM236DKP7iT0Ep71uFHajT+86R+8Xaa6zV+w1e8sits+O2Ed2wvpMsBv2jX1nP4Kfwe/gT/D31toIFj0v2VI1Vv8B5LezOA==</latexit><latexit sha1_base64="NVQbzrcan+IK7t0EDlSunHu7lAs=">AAACTHicbVDRShtBFJ2NrdpYNWkf+zI0FvqgYTcU9EWQ9qUvgqWNCtkl3J29G4fMzmxn7qphyQ/4NX1tP6Lv/Y++lUInMQWjXhg4nHPuvXNPWirpKAx/BY2VJ09X19afNTeeb25tt9ovTp2prMC+MMrY8xQcKqmxT5IUnpcWoUgVnqXjDzP97BKtk0Z/oUmJSQEjLXMpgDw1bO3EhNdUH4Mb888mp2O4jr9WkPEpj0snDwe93V4ybHXCbjgv/hBEC9BhizoZtoNWnBlRFahJKHBuEIUlJTVYkkLhtBlXDksQYxjhwEMNBbqknp8z5W88k/HcWP808Tl7t6OGwrlJkXpnAXTh7msz8jFtUFF+kNRSlxWhFreL8kpxMnyWDc+kRUFq4gEIK/1fubgAC4J8gsuT7NXeyCuF0VlSzzZl6ORIL11WY/rfM236DKP7iT0Ep71uFHajT+86R+8Xaa6zV+w1e8sits+O2Ed2wvpMsBv2jX1nP4Kfwe/gT/D31toIFj0v2VI1Vv8B5LezOA==</latexit>(cid:52)(cid:80)(cid:71)(cid:3005)(cid:46)(cid:66)(cid:89)<latexit sha1_base64="qq+2R9SQVnz+30b+YnJF4/j+xCY=">AAACNHicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuhTduBEqWhXaWCaTmzo4mQkzN2oJ/Q+3+hH+i+BO3PoNTtoKtnpg4HDOfc0JU8ENet6bMzU9Mzs3X1ooLy4tr6xWqmuXRmWaQZMpofR1SA0ILqGJHAVcpxpoEgq4Cu+OC//qHrThSl5gL4UgoV3JY84oWummjfCI+bmK8ZQ+9juVmlf3BnD/En9EamSERqfqVNqRYlkCEpmgxrR8L8Ugpxo5E9AvtzMDKWV3tAstSyVNwAT54Oy+u2WVyI2Vtk+iO1B/d+Q0MaaXhLYyoXhrJr1C/M9rZRgfBDmXaYYg2XBRnAkXlVtk4EZcA0PRs4Qyze2tLrulmjK0SY1P0g87XeskSkZBXmyKwPCuHPtZDuFPTb9sM/QnE/tLLnfrvlf3z/Zqh0ejNEtkg2ySbeKTfXJITkiDNAkjmjyRZ/LivDrvzofzOSydckY962QMztc31A+sCg==</latexit><latexit sha1_base64="qq+2R9SQVnz+30b+YnJF4/j+xCY=">AAACNHicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuhTduBEqWhXaWCaTmzo4mQkzN2oJ/Q+3+hH+i+BO3PoNTtoKtnpg4HDOfc0JU8ENet6bMzU9Mzs3X1ooLy4tr6xWqmuXRmWaQZMpofR1SA0ILqGJHAVcpxpoEgq4Cu+OC//qHrThSl5gL4UgoV3JY84oWummjfCI+bmK8ZQ+9juVmlf3BnD/En9EamSERqfqVNqRYlkCEpmgxrR8L8Ugpxo5E9AvtzMDKWV3tAstSyVNwAT54Oy+u2WVyI2Vtk+iO1B/d+Q0MaaXhLYyoXhrJr1C/M9rZRgfBDmXaYYg2XBRnAkXlVtk4EZcA0PRs4Qyze2tLrulmjK0SY1P0g87XeskSkZBXmyKwPCuHPtZDuFPTb9sM/QnE/tLLnfrvlf3z/Zqh0ejNEtkg2ySbeKTfXJITkiDNAkjmjyRZ/LivDrvzofzOSydckY962QMztc31A+sCg==</latexit><latexit sha1_base64="qq+2R9SQVnz+30b+YnJF4/j+xCY=">AAACNHicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuhTduBEqWhXaWCaTmzo4mQkzN2oJ/Q+3+hH+i+BO3PoNTtoKtnpg4HDOfc0JU8ENet6bMzU9Mzs3X1ooLy4tr6xWqmuXRmWaQZMpofR1SA0ILqGJHAVcpxpoEgq4Cu+OC//qHrThSl5gL4UgoV3JY84oWummjfCI+bmK8ZQ+9juVmlf3BnD/En9EamSERqfqVNqRYlkCEpmgxrR8L8Ugpxo5E9AvtzMDKWV3tAstSyVNwAT54Oy+u2WVyI2Vtk+iO1B/d+Q0MaaXhLYyoXhrJr1C/M9rZRgfBDmXaYYg2XBRnAkXlVtk4EZcA0PRs4Qyze2tLrulmjK0SY1P0g87XeskSkZBXmyKwPCuHPtZDuFPTb9sM/QnE/tLLnfrvlf3z/Zqh0ejNEtkg2ySbeKTfXJITkiDNAkjmjyRZ/LivDrvzofzOSydckY962QMztc31A+sCg==</latexit><latexit sha1_base64="qq+2R9SQVnz+30b+YnJF4/j+xCY=">AAACNHicbVDLSsNAFJ34rPXV6tJNsAhuLIkIuhTduBEqWhXaWCaTmzo4mQkzN2oJ/Q+3+hH+i+BO3PoNTtoKtnpg4HDOfc0JU8ENet6bMzU9Mzs3X1ooLy4tr6xWqmuXRmWaQZMpofR1SA0ILqGJHAVcpxpoEgq4Cu+OC//qHrThSl5gL4UgoV3JY84oWummjfCI+bmK8ZQ+9juVmlf3BnD/En9EamSERqfqVNqRYlkCEpmgxrR8L8Ugpxo5E9AvtzMDKWV3tAstSyVNwAT54Oy+u2WVyI2Vtk+iO1B/d+Q0MaaXhLYyoXhrJr1C/M9rZRgfBDmXaYYg2XBRnAkXlVtk4EZcA0PRs4Qyze2tLrulmjK0SY1P0g87XeskSkZBXmyKwPCuHPtZDuFPTb9sM/QnE/tLLnfrvlf3z/Zqh0ejNEtkg2ySbeKTfXJITkiDNAkjmjyRZ/LivDrvzofzOSydckY962QMztc31A+sCg==</latexit>minimum without producing any extra useful knowledge. Therefore  to encourage the network to
learn more complex and informative auxiliary tasks  we further apply an entropy loss H(yaux) as a
regularisation term in the meta objective. A detailed explanation of the entropy loss and the collapsing
label problem is given in Section 3.4. Finally  we update MAXL’s label generation network by

θ2 ← θ2 − β∇θ2

(x(i))  ypri

(i)) + λH(yaux
(i) )

.

(5)

(cid:16)L(f pri

θ+
1

(cid:17)

Overall  the entire MAXL algorithm is deﬁned as follows:

Algorithm 1: The MAXL algorithm
Initialise: Network parameters: θ1  θ2; Hierarchical structure: ψ
Initialise: Learning rate: α  β; Entropy weighting: λ
while not converged do

for each training iteration i do

(i)) ∈ (x  y)

# fetch one batch of training data
(x(i)  ypri
# auxiliary-training step
Update: θ1 ← θ1 − α∇θ1

(cid:16)L(f pri

θ1

end
for each training iteration i do

(x(i))  ypri

(i)) + L(f aux

θ1

(cid:17)

(x(i))  gθ2 (x(i)  ypri

(i)  ψ))

(i)) ∈ (x  y)

# fetch one batch of training data
(x(i)  ypri
# retain training computational graph
1 = θ1 − α∇θ1
(x(i))  ypri
Compute: θ+
# meta-training step (second derivative trick)
Update: θ2 ← θ2 − β∇θ2
(x(i))  ypri

(cid:16)L(f pri
(cid:16)L(f pri

θ1

θ+
1

(i)) + L(f aux
(i)) + λH(yaux
(i) )

θ1

(cid:17)

(cid:17)

(x(i))  gθ2(x(i)  ypri

(i)  ψ))

end

end

3.3 Mask SoftMax for Hierarchical Predictions

exp ˆyi(cid:80)

(cid:80)
exp M (cid:12) ˆyi
i exp M (cid:12) ˆyi

As previously discussed  we include a hierarchy ψ which deﬁnes the number of auxiliary classes
per primary class. To implement this  we designed a modiﬁed SoftMax function  which we call
Mask SoftMax  to predict auxiliary labels only for certain auxiliary classes. This takes ground-truth
primary task label y  and the hierarchy ψ  and creates a binary mask M = B(y  ψ). The mask is
zero everywhere  except for ones across the set of auxiliary classes associated with y. For example 
consider a primary task with 2 classes y = 0  1  and a hierarchy of ψ = [2  2] as in Figure 2b. In this
case  the binary masks are M = [1  1  0  0] for y = 0  and [0  0  1  1] for y = 1.
Applying this mask element-wise to the standard SoftMax function then allows the label-prediction
network to assign auxiliary labels only to relevant auxiliary classes:

SoftMax: p(ˆyi) =

(6)
where p(ˆyi) represents the probability of the generated auxiliary label ˆy over class i  and (cid:12) represents
element-wise multiplication. Note that no domain knowledge is required to deﬁne the hierarchy  and
MAXL performs well across a range of values for ψ as shown in Section 4.2.

Mask SoftMax: p(ˆyi) =

i exp ˆyi

 

 

3.4 The Collapsing Class Problem

As previously discussed  we introduce an additional regularisation loss  which we call the entropy
loss H(ˆy(i)). This encourages high entropy across the auxiliary class prediction space  which in
turn encourages the label-prediction network to fully utilise all auxiliary classes. The entropy
loss calculates the KL divergence between the predicted auxiliary label space ˆy(i)  and a uniform

5

distribution U  for each ith batch. This is equivalent to calculating the entropy of the predicted label
space  and is deﬁned as:

N(cid:88)

n=1

ˆyk
(i)[n].

(7)

K(cid:88)

k=1

H(ˆy(i)) =

(i) log ˆyk
ˆyk

(i) 

ˆyk
(i) =

1
N

where K is the total number of auxiliary classes  and N is the training batch size.

4 Experiments

In this section  we present experimental results to evaluate MAXL with respect to several baselines
and datasets on image classiﬁcation.

4.1 Experimental Setup

Datasets We evaluated on seven different datasets  with varying sizes and complexities. One
of these  CIFAR-100 [18]  contains a manually-deﬁned 2-level hierarchical structure  which we
expanded into a 4-level hierarchy by manually assigning data for the new levels  to create a hierarchy
of {3  10  20  100} classes . This hierarchy was then used for ground-truth auxiliary labels for the
Human baseline (see below). For the other six datasets: MNIST [19]  SVHN [12]  CIFAR-10 [18] 
ImageNet [7]  CINIC-10 [6] and UCF-101 [32]  a hierarchy is either not available or difﬁcult to
access  and so no ground-truth auxiliary labels exist. All larger datasets were rescaled to resolution
[32 × 32] to accelerate training.
Baselines We compare MAXL to a number of baselines. First  we compare with Single Task  which
trains only with the primary class label and does not employ auxiliary learning. This comparison was
done to determine whether MAXL could improve classiﬁcation performance without needing any
extra labelled data. Then  we compare to three baselines for generating auxiliary labels: Random 
K-Means  and Human  to evaluate the effectiveness of MAXL’s meta-learning for label generation.
Random assigns each training image to random auxiliary classes in a randomly generated (well-
balanced) hierarchy. K-Means determines auxiliary labels via unsupervised clustering using K-Means
[13]  performed on the latent representation of an auto-encoder  with clustering updated after every
training iteration. Human uses the human-deﬁned hierarchy of CIFAR-100  where the auxiliary
classes are at a lower (ﬁner-grained) level hierarchy to the primary classes. Note that in order to
compare these baselines to Human  they were only evaluated on CIFAR-100 because this is the only
dataset containing a human-deﬁned hierarchy (and hence ground-truth auxiliary labels).

4.2 Comparison to Single Task Learning

First  we compare MAXL to a single-task learning baseline  to determine whether MAXL can
improve recognition accuracy without needing access to any additional data. To test the robustness
of MAXL  we evaluate it on 3 different networks: a simple 4-layer ConvNet  VGG-16 [30]  and
ResNet-32 [14]. We used hyper-parameter search for all networks and applied regularisation methods
in order to achieve optimal performance . Since the power of MAXL lies in its ability to work without
domain knowledge  we tested MAXL across a range of hierarchies ψ  to study if it is effective without
needing to tune this hierarchy for each dataset. Here  the hierarchies are well balanced such that ψ[i]
is the same for all i (for all primary classes).

Datasets

Backbone

Single

4-layer ConvNet
4-layer ConvNet

MNIST
SVHN
CIFAR-10 VGG-16
VGG-16
ImageNet
ResNet-32
CINIC-10
UCF-101
ResNet-32

99.57 ± 0.02
94.05 ± 0.07
92.77 ± 0.13
46.67 ± 0.12
85.12 ± 0.08
53.15 ± 0.12

MAXL  ψ[i] =

2

99.56 ± 0.04
94.39 ± 0.08
93.27 ± 0.09
46.82 ± 0.14
85.66 ± 0.07
54.19 ± 0.18

3

99.71 ± 0.02
94.38 ± 0.07
93.47 ± 0.08
46.97 ± 0.10
85.72 ± 0.07
55.39 ± 0.16

5

99.59 ± 0.03
94.59 ± 0.12
93.49 ± 0.05
47.02 ± 0.11
85.83 ± 0.08
54.70 ± 0.12

10

99.57 ± 0.02
94.41 ± 0.09
93.10 ± 0.08
46.85 ± 0.11
85.80 ± 0.10
54.32 ± 0.18

Table 1: Comparison of MAXL with single-task learning  across a range of hierarchies. We reported
results from three individual runs  and the best performance for each dataset is marked with bold.

6

Table 1 shows the test accuracy of MAXL and single-task learning  with each accuracy averaged
over three individual runs. We see that MAXL consistently outperforms single-task learning across
all six datasets  despite both methods using exactly the same training data. We also see that MAXL
outperforms single-task learning across almost all tested values of ψ  showing the robustness of our
method without requiring domain knowledge or a manually-deﬁned hierarchy.

4.3 Comparison to Auxiliary Label Generation Baselines

Next  we compare MAXL to a number of baseline methods for generating auxiliary labels  on
CIFAR-100. Here  all the baselines were trained without any regularisation  to isolate the effect
of auxiliary learning and test generalisation ability purely from auxiliary tasks. This dataset has a
manually-deﬁned hierarchy  which is used in Human for ground-truth auxiliary labels. However 
MAXL  Random  and K-Means do not require any human knowledge or manually-deﬁned hierarchy
to generate auxiliary labels. Therefore  as in Section 4.2  a hierarchy ψ is deﬁned  assigning each
primary class a set of auxiliary classes. We created well-balanced hierarchies by assigning an equal
number of auxiliary classes per primary class. For cases where the hierarchy was unbalanced by one
auxiliary class  we randomly chose which primary classes are assigned each number of auxiliary
classes in ψ. We ran each experiment three times and averaged the results 

Figure 3: Learning curves for the CIFAR-100 test dataset  comparing MAXL with baseline methods
for generating auxiliary labels. Our version of CIFAR-100 has a four-level hierarchy of {3  10  20 
100} classes per level  and we use this to create the hierarchy ψ for auxiliary learning.

Test accuracy curves are presented in Figure 3  using all possible combinations of the numbers of
primary classes and total auxiliary classes in CIFAR-100 (where the auxiliary classes are at a lower
hierarchical level to the primary classes). We observe that MAXL outperforms Single Task  Random 
and K-Means. Note that K-Means required signiﬁcantly longer training time than MAXL due to the
need to run clustering after each iteration. Also note that the superior performance of MAXL over
these three baselines occurs despite all four methods using exactly the same data. Finally  we observe
that MAXL performs similarly to Human  despite this baseline requiring manually-deﬁned auxiliary
labels for the entire training dataset. With performance of MAXL similar to that of a system using
human-deﬁned auxiliary labels  we see strong evidence that MAXL is able to learn to generalise
effectively in a self-supervised manner.

4.4 Understanding the Utility of Auxiliary Labels

In [9]  the cosine similarity between gradients produced by the auxiliary and primary losses was used
to determine the task weighting in the overall loss function. We use this same idea to visualise the
utility of a set of auxiliary labels for improving the performance of the primary task. Intuitively  a
cosine similarity of -1 indicates that the auxiliary labels work against the primary task. A cosine
similarity of 0 indicates that the auxiliary labels have no impact on the primary task. And a cosine
similarity of 1 indicates that the auxiliary labels are learning the same features as the primary task 
and so offer no useful information. Therefore  the cosine similarity for the gradient produced from
optimal auxiliary labels should be between 0 and 1 to ensure that they assist the primary task.

7

0100200Epoch0.8500.8750.900TestPerformance(Acc.)0100200Epoch0.8500.8750.900TestPerformance(Acc.)0100200Epoch0.8500.8750.900TestPerformance(Acc.)0100200Epoch0.7250.7500.775TestPerformance(Acc.)0100200Epoch0.7250.7500.775TestPerformance(Acc.)0100200Epoch0.6750.7000.725TestPerformance(Acc.)HumanMAXLSingleTaskPRI3|AUX10PRI3|AUX20PRI20|AUX100PRI10|AUX100PRI10|AUX20PRI3|AUX100RandomK-MeansIn Figure 4  we show the cosine similarity measure-
ments of gradients in the shared layers of the multi-
task network  trained on 3 primary classes and 10  20
and 100 total auxiliary classes from CIFAR-100. We
observe that baseline methods Human and Random 
with ﬁxed auxiliary labels  reach their maximal sim-
ilarity at an early stage during training  which then
drops signiﬁcantly afterwards. K-Means produces
smooth auxiliary gradients throughout training  but
its similarity depends on the number of auxiliary
classes. In comparison  MAXL produces auxiliary
gradients with high similarity throughout the entire
training period  and consistently so across the num-
ber of auxiliary classes. Whilst we cannot say what
the optimal cosine similarity should be  it is clear
that MAXL’s auxiliary labels affect primary task
performance in a very different way to the other
baselines.
Due to MAXL’s cosine similarity measurements being greater than zero across the entire training
stage  a standard gradient update rule for shared feature space is then guaranteed to converge to a
local minima given a small learning rate [9].

Figure 4: Cosine similarity measurement be-
tween the auxiliary loss gradient and primary
loss gradient  on the shared representation in
the multi-task network.

4.5 Visualisations of Generated Knowledge

In Figure 5  we visualise 2D embeddings of examples from the CIFAR-100 test dataset  on two
different hierarchies. The visualisations are computed using t-SNE [24] on the ﬁnal feature layer of
the multi-task network  and compared across three methods: our MAXL method  the Human baseline 
and the Single Task baseline.

Figure 5: t-SNE visualisation of the learned ﬁnal layer of the multi-task network  trained on CIFAR-
100 with two different hierarchies. Colours represent the primary classes.

This visualisation shows the separability of primary classes after being trained with the multi-task
network. Qualitatively  we see that both MAXL and Human show better separation of the primary
classes than with Single Task  owing to the generalisation effect of the auxiliary learning. This again
shows the effectiveness of MAXL whilst requiring no additional human knowledge.
We also show examples of images assigned to the same auxiliary class through MAXL’s label-
generation network. Figure 6 shows example images with the highest prediction probabilities for
three random auxiliary classes from CIFAR-100  using the hierarchy of 20 primary classes and 100
total auxiliary classes (5 auxiliary classes per primary class)  which showed the best performance of
MAXL in Figure 3. In addition  we also present examples on MNIST  in which 3 auxiliary classes
were used for each of the 10 primary classes.
To our initial surprise  only part of the generated auxiliary labels visualised in both dataset show
human-understandable knowledge. We can observe that the auxiliary classes #1 and #2 of digit nine
are clustered by the direction of the ‘tail’  and auxiliary classes #2 and #3 of digit seven are clustered
by the distinction of the ‘horizontal line’. But in most cases  there are no obvious similarities within
each auxiliary class in terms of shape  colour  style  structure or semantic meaning. However  this
makes more sense when we re-consider the role of the label-generation network  which is to assign
auxiliary labels which assist the primary task  rather than grouping images in terms of semantic or
visual similarity. The label-generation network would therefore be more effective if it were to group

8

HumanMAXLRandomK-Means050100150200Epoch0.00.20.40.6CosineSimilarity050100150200Epoch0.00.20.40.6CosineSimilarity050100150200Epoch0.00.20.40.6CosineSimilarityPRI3|AUX20PRI3|AUX100PRI3|AUX10SingleTaskHumanMAXLSingleTaskHumanMAXLPRI3|AUX10PRI20|AUX100Auxiliary Class #1

Auxiliary Class #2

Auxiliary Class #3

Primary Class

aquatic
mammals
household
furnitures

invertebrates

nature
scenes

ﬂowers

digit 0

digit 3

digit 5

digit 7

digit 9

Figure 6: Visualisation of 5 test examples with the highest prediction probability  for each of 3
randomly selected auxiliary classes  for different primary classes. We present the visualisation for
CIFAR-100 (top) when trained with 20 primary classes and 5 auxiliary classes per primary class  and
for MNIST (bottom) when trained with 10 primary classes and 3 auxiliary classes per primary class.

images in terms of a shared aspect of reasoning which the primary task is currently struggling to
learn  which may not be human intepretable.
Furthermore  we discovered that the generated auxiliary knowledge is not deterministic  since the top
predicted candidates are different when we re-train the network from scratch. We therefore speculate
that using a human-deﬁned hierarchy is just one out of a potentially inﬁnite number of local optima 
and on each run of training  the label-generation network produces another of these local optima.

5 Conclusion & Future Work

In this paper  we have presented Meta AuXiliary Learning (MAXL) for generating optimal auxiliary
labels which  when trained alongside a primary task in a multi-task setup  improve the performance
of the primary task. Rather than employing domain knowledge and human-deﬁned auxiliary tasks
as is typically required  MAXL is self-supervised and  combined with its general nature  has the
potential to automate the process of generalisation to new levels.
Our evaluation on multiple datasets has shown the performance of MAXL in an image classiﬁcation
setup  where the auxiliary task is to predict sub-class  hierarchical labels for an image. We have
shown that MAXL signiﬁcantly outperforms other baselines for generating auxiliary labels  and is
competitive even when human-deﬁned knowledge is used to manually construct the auxiliary labels.
The general nature of MAXL also opens up questions about how self-supervised auxiliary learning
may be used to learn generic auxiliary tasks  beyond sub-class image classiﬁcation. During our
experiments  we also ran preliminary experiments on predicting arbitrary vectors such that the
auxiliary task becomes a regression  but results so far have been inconclusive. However  the ability of
MAXL to potentially learn ﬂexible auxiliary tasks which can automatically be tuned for the primary
task  now offers an exciting direction towards automated generalisation across a wide range of more
complex tasks.

Acknowledgements

We would like to thank Michael Bloesch  Fabian Falck  and Stephen James  for insightful discussions.

9

References

[1] Rishabh Agarwal  Chen Liang  Dale Schuurmans  and Mohammad Norouzi. Learning to generalize from
sparse and underspeciﬁed rewards. In International Conference on Machine Learning  pages 130–140 
2019.

[2] Marcin Andrychowicz  Misha Denil  Sergio Gomez  Matthew W Hoffman  David Pfau  Tom Schaul 
Brendan Shillingford  and Nando De Freitas. Learning to learn by gradient descent by gradient descent. In
Advances in Neural Information Processing Systems  pages 3981–3989  2016.

[3] Samy Bengio  Yoshua Bengio  Jocelyn Cloutier  and Jan Gecsei. On the optimization of a synaptic learning
rule. In Preprints Conf. Optimality in Artiﬁcial and Biological Neural Networks  pages 6–8. Univ. of Texas 
1992.

[4] Yoshua Bengio  Samy Bengio  and Jocelyn Cloutier. Learning a synaptic learning rule. Université de

Montréal  Département d’informatique et de recherche opérationnelle  1990.

[5] Rich Caruana. Multitask learning. In Learning to learn  pages 95–133. Springer  1998.
[6] Luke N Darlow  Elliot J Crowley  Antreas Antoniou  and Amos J Storkey. Cinic-10 is not imagenet or

cifar-10. arXiv preprint arXiv:1810.03505  2018.

[7] Jia Deng  Wei Dong  Richard Socher  Li-Jia Li  Kai Li  and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition  pages 248–255.
IEEE  2009.

[8] Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. In Proceedings of the

IEEE International Conference on Computer Vision  pages 2051–2060  2017.

[9] Yunshu Du  Wojciech M Czarnecki  Siddhant M Jayakumar  Razvan Pascanu  and Balaji Lakshminarayanan.

Adapting auxiliary losses using gradient similarity. arXiv preprint arXiv:1812.02224  2018.

[10] Chelsea Finn  Pieter Abbeel  and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep

networks. In International Conference on Machine Learning  pages 1126–1135  2017.

[11] John Flynn  Ivan Neulander  James Philbin  and Noah Snavely. Deepstereo: Learning to predict new
views from the world’s imagery. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 5515–5524  2016.

[12] Ian J Goodfellow  Yaroslav Bulatov  Julian Ibarz  Sacha Arnoud  and Vinay Shet. Multi-digit number recog-
nition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082 
2013.

[13] John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal of

the Royal Statistical Society. Series C (Applied Statistics)  28(1):100–108  1979.

[14] Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 770–778  2016.
[15] Max Jaderberg  Wojciech Marian Czarnecki  Simon Osindero  Oriol Vinyals  Alex Graves  David Sil-
ver  and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In International
Conference on Machine Learning  pages 1627–1635  2017.

[16] Max Jaderberg  Volodymyr Mnih  Wojciech Marian Czarnecki  Tom Schaul  Joel Z Leibo  David Silver  and
Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. International Conference
on Learning Representations  2017.

[17] Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-  mid-  and
high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition  pages 6129–6138  2017.

[18] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical

report  Citeseer  2009.

[19] Yann LeCun  Léon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to

document recognition. Proceedings of the IEEE  86(11):2278–2324  1998.

[20] Zhenguo Li  Fengwei Zhou  Fei Chen  and Hang Li. Meta-sgd: Learning to learn quickly for few shot

learning. arXiv preprint arXiv:1707.09835  2017.

[21] Lukas Liebel and Marco Körner. Auxiliary tasks in multi-task learning. arXiv preprint arXiv:1805.06334 

2018.

[22] Tsung-Yi Lin  Priya Goyal  Ross Girshick  Kaiming He  and Piotr Dollár. Focal loss for dense object
detection. In Proceedings of the IEEE international conference on computer vision  pages 2980–2988 

10

2017.

[23] Shikun Liu  Edward Johns  and Andrew J Davison. End-to-end multi-task learning with attention. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages 1871–1880 
2019.

[24] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning

research  9(Nov):2579–2605  2008.

[25] Ishan Misra  Abhinav Shrivastava  Abhinav Gupta  and Martial Hebert. Cross-stitch networks for multi-task
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
3994–4003  2016.

[26] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In International

Conference on Learning Representations  2016.

[27] Sebastian Ruder. An overview of multi-task learning in deep neural networks.

arXiv:1706.05098  2017.

arXiv preprint

[28] Adam Santoro  Sergey Bartunov  Matthew Botvinick  Daan Wierstra  and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International conference on machine learning 
pages 1842–1850  2016.

[29] Jürgen Schmidhuber. Learning complex  extended sequences using the principle of history compression.

Neural Computation  4(2):234–242  1992.

[30] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-

tion. In International Conference on Learning Representations  2015.

[31] Jake Snell  Kevin Swersky  and Richard Zemel. Prototypical networks for few-shot learning. In Advances

in Neural Information Processing Systems  pages 4077–4087  2017.

[32] Khurram Soomro  Amir Roshan Zamir  and Mubarak Shah. Ucf101: A dataset of 101 human actions

classes from videos in the wild. arXiv preprint arXiv:1212.0402  2012.

[33] Shubham Toshniwal  Hao Tang  Liang Lu  and Karen Livescu. Multitask learning with low-level auxiliary

tasks for encoder-decoder based speech recognition. Proc. Interspeech 2017  pages 3532–3536  2017.

[34] Oriol Vinyals  Charles Blundell  Tim Lillicrap  Daan Wierstra  et al. Matching networks for one shot

learning. In Advances in Neural Information Processing Systems  pages 3630–3638  2016.

[35] Yabin Zhang  Hui Tang  and Kui Jia. Fine-grained visual categorization using meta-learning optimization
with sample selection of auxiliary data. In Proceedings of the European Conference on Computer Vision 
pages 233–248  2018.

[36] Tinghui Zhou  Matthew Brown  Noah Snavely  and David G Lowe. Unsupervised learning of depth
and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition  pages 1851–1858  2017.

11

,Dominik Rothenhäusler
Christina Heinze
Jonas Peters
Nicolai Meinshausen
Mengdi Wang
Ji Liu
Ethan Fang
Shikun Liu
Andrew Davison
Edward Johns