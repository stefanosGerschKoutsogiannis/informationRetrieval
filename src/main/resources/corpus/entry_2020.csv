2009,Fast Learning from Non-i.i.d. Observations,We prove an oracle inequality for generic regularized empirical risk minimization algorithms learning from  $\a$-mixing processes. To illustrate this oracle inequality  we use it to derive learning rates for some learning methods including least squares SVMs. Since the proof of the oracle inequality uses recent localization ideas developed for independent and identically distributed (i.i.d.) processes  it turns out that these learning rates are close to the optimal rates known in the i.i.d. case.,Fast Learning from Non-i.i.d. Observations

Ingo Steinwart

Information Sciences Group CCS-3
Los Alamos National Laboratory
Los Alamos  NM 87545  USA

ingo@lanl.gov

Andreas.Christmann@uni-bayreuth.de

Andreas Christmann
University of Bayreuth

Department of Mathematics

D-95440 Bayreuth

Abstract

We prove an oracle inequality for generic regularized empirical risk minimization
algorithms learning from α-mixing processes. To illustrate this oracle inequality 
we use it to derive learning rates for some learning methods including least squares
SVMs. Since the proof of the oracle inequality uses recent localization ideas
developed for independent and identically distributed (i.i.d.) processes  it turns
out that these learning rates are close to the optimal rates known in the i.i.d. case.

1

Introduction

In the past  most articles investigating statistical properties of learning algorithms assumed that the
observed data was generated in an i.i.d. fashion. However  in many applications this assumption
cannot be strictly justiﬁed since the sample points are intrinsically temporal and thus often weakly
dependent. Typical examples for this phenomenon are applications where observations come from
(suitably pre-processed) time series  i.e.  for example  ﬁnancial predictions  signal processing  sys-
tem observation and diagnosis  and speech or text recognition. A set of natural and widely accepted
notions for describing such weak dependencies1 are mixing concepts such as α-  β-  and φ-mixing 
since a) they offer a generalization to i.i.d. processes that is satisﬁed by various types of stochastic
processes including Markov chains and many time series models  and b) they quantify the depen-
dence in a conceptionally simple way that is accessible to various types of analysis.
Because of these features  the machine learning community is currently in the process of appreciat-
ing and accepting these notions as the increasing number of articles in this direction shows. Prob-
ably the ﬁrst work in this direction goes back to Yu [20]  whose techniques for β-mixing processes
inspired subsequent work such as [18  10  11]  while the analysis of speciﬁc learning algorithms
probably started with [9  5  8]. More recently  [7] established consistency of regularized boosting
algorithms learning from β-mixing processes  while [15] established consistency of support vector
machines (SVMs) learning from α-mixing processes  which constitute the largest class of mixing
processes. For the latter  [21] established generalization bounds for empirical risk minimization
(ERM) and [19  17] analyzed least squares support vector machines (LS-SVMs).
In this work  we establish a general oracle inequality for generic regularized learning algorithms and
α-mixing observations by combining a Bernstein inequality for such processes [9] with localization
ideas for i.i.d. processes pioneered by [6] and reﬁned by e.g. [1]. To illustrate this oracle inequality 
we then use it to show learning rates for some algorithms including ERM over ﬁnite sets and LS-
SVMs. In the ERM case our results match those in the i.i.d. case if one replaces the number of
observations with the “effective number of observations”  while  for LS-SVMs  our rates are at
least quite close to the recently obtained optimal rates [16] for i.i.d. observations. However  the
latter difference is not surprising  when considering the fact that [16] used heavy machinery from

1For example  [4] write on page 71: “. . .

it is a common practice to assume a certain mild asymptotic

independence (such as α-mixing) as a precondition in the context of . . . nonlinear times series.”

1

empirical process theory such as Talagrand’s inequality and localized Rademacher averages  while
our results only use a light-weight argument based on Bernstein’s inequality.

2 Deﬁnitions  Results  and Examples
Let X be a measurable space and Y ⊂ R be closed. Furthermore  let (Ω A  µ) be a probability
space and Z := (Zi)i≥1 be a stochastic process such that Zi : Ω → X × Y for all i ≥ 1. For n ≥ 1 
we further write Dn := ((X1  Y1)  . . .   (Xn  Yn)) := (Z1  . . .   Zn) for a training set of length n
that is distributed according to the ﬁrst n components of Z. Throughout this work  we assume that
Z is stationary  i.e.  the (X × Y )n-valued random variables (Zi1  . . .   Zin) and (Zi1+i  . . .   Zin+i)
have the same distribution for all n  i  i1  . . .   in ≥ 1. We further write P for the distribution of one
(and thus all) Zi  i.e.  for all measurable A ⊂ X × Y   we have

(1)
To learn from stationary processes whose components are not independent  [15] suggests that it is
necessary to replace the independence assumption by a notion that still guarantees certain concen-
tration inequalities. We will focus on α-mixing  which is based on the α-mixing coefﬁcients

P (A) = µ(cid:0){ω ∈ Ω : Zi(ω) ∈ A}(cid:1) .
n(cid:12)(cid:12)µ(A ∩ B) − µ(A)µ(B)(cid:12)(cid:12) : i ≥ 1  A ∈ Ai

1 and B ∈ A∞

i+n

 

n ≥ 1 

α(Z  µ  n) := sup

o

where Ai
tively. Throughout this work  we assume that the process Z is geometrically α-mixing  that is

i+n are the σ-algebras generated by (Z1  . . .   Zi) and (Zi+n  Zi+n+1  . . . )  respec-

1 and A∞

α(Z  µ  n) ≤ c exp(−bnγ)  

(2)
for some constants b > 0  c ≥ 0  and γ > 0. Of course  i.i.d. processes satisfy (2) for c = 0 and all
b  γ > 0. Moreover  several time series models such as ARMA and GARCH  which are often used
to describe  e.g. ﬁnancial data  satisfy (2) under natural conditions [4  Chapter 2.6.1]  and the same
is true for many Markov chains including some dynamical systems perturbed by dynamic noise  see
e.g. [18  Chapter 3.5]. An extensive and thorough account on mixing concepts including stronger
mixing notions such as β- and φ-mixing is provided by [3].
Let us now describe the learning algorithms we are interested in. To this end  we assume that we
have a hypothesis set F consisting of bounded measurable functions f : X → R that is pre-compact
with respect to the supremum norm k · k∞  i.e.  for all ε > 0  the covering numbers

n ≥ 1 

(cid:26)

n ≥ 1 : ∃f1  . . .   fn ∈ F such that F ⊂ n[

N (F k · k∞  ε) := inf

(cid:27)

B(fi  ε)

i=1

are ﬁnite  where B(fi  ε) := {f ∈ ‘∞(X) : kf − fik∞ ≤ ε} denotes the ε-ball with center fi in the
space ‘∞(X) of bounded functions f : X → R. Moreover  we assume that we have a regularizer 
that is  a function Υ : F → [0 ∞). Following [13  Deﬁnition 2.22]  we further say that a function
L : X × Y × R → [0 ∞) is a loss that can be clipped at some M > 0  if L is measurable and

(x  y  t) ∈ X × Y × R 

L(x  y  ¯t) ≤ L(x  y  t)  

(3)
where ¯t denotes the clipped value of t at ±M  that is ¯t := t if t ∈ [−M  M]  ¯t := −M if t < −M 
and ¯t := M if t > M. Various often used loss functions can be clipped. For example  if Y :=
{−1  1} and L is a convex  margin-based loss represented by ϕ : R → [0 ∞)  that is L(y  t) =
ϕ(yt) for all y ∈ Y and t ∈ R  then L can be clipped  if and only if ϕ has a global minimum  see [13 
Lemma 2.23]. In particular  the hinge loss  the least squares loss for classiﬁcation  and the squared
hinge loss can be clipped  but the logistic loss for classiﬁcation and the AdaBoost loss cannot be
clipped. On the other hand  [12] established a simple technique  which is similar to inserting a
small amount of noise into the labeling process  to construct a clippable modiﬁcation of an arbitrary
convex  margin-based loss. Moreover  if Y := [−M  M] and L is a convex  distance-based loss
represented by some ψ : R → [0 ∞)  that is L(y  t) = ψ(y − t) for all y ∈ Y and t ∈ R  then L
can be clipped whenever ψ(0) = 0  see again [13  Lemma 2.23]. In particular  the least squares loss
and the pinball loss used for quantile regression can be clipped  if the space of labels Y is bounded.
Given a loss function L and an f : X → R  we often use the notation L ◦ f for the function
(x  y) 7→ L(x  y  f(x)). Moreover  the L-risk is deﬁned by

Z

RL P (f) :=

X×Y

L(x  y  f(x)) dP (x  y)  

2

and the minimal L-risk is R∗
L P := inf{RL P (f)| f : X → R}. In addition  a function f∗
satisfying RL P (f∗
L P ) = R∗
L P is called a Bayes decision function. Finally  we denote empirical
risks based on Dn by RL Dn(f)  that is  for a realization of Dn(ω) of the training set Dn we have

L(cid:0)Xi(ω)  Yi(ω)  f(Xi(ω))(cid:1) .
Given a regularizer Υ : F → [0 ∞)  a clippable loss  and an accuracy δ ≥ 0  we consider learning
(cid:17)
methods that  for all n ≥ 1  produce a decision function fDn Υ ∈ F satisfying
Υ(f) + RL Dn(f)

Υ(fDn Υ) + RL Dn( ¯fDn Υ) ≤ inf
f∈F

RL Dn(ω)(f) =

nX

+ δ .

(cid:16)

1
n

(4)

i=1

L P

Note that methods such as SVMs (see below) that minimize the right-hand side of (4) exactly  satisfy
(4)  because of (3). The following theorem  which is our main result  establishes an oracle inequality
for methods (4)  when the training data is generated by Z.
Theorem 2.1 Let L : X × Y × R → [0 ∞) be a loss that can be clipped at M > 0 and that
satisﬁes L(x  y  0) ≤ 1  L(x  y  t) ≤ B  and

(5)
for all (x  y) ∈ X × Y and t  t0 ∈ [−M  M]  where B > 0 is some constant. Moreover  let
Z := (Zi)i≥1 be an X × Y -valued process that satisﬁes (2)  and P be deﬁned by (1). Assume that
there exist a Bayes decision function f∗

L P and constants ϑ ∈ [0  1] and V ≥ B2−ϑ such that

(cid:12)(cid:12)L(x  y  t) − L(x  y  t0)(cid:12)(cid:12) ≤ |t − t0|
L P )(cid:1)ϑ
(cid:1)2 ≤ V ·(cid:0)EP (L ◦ ¯f − L ◦ f∗

(cid:0)L ◦ ¯f − L ◦ f∗

EP

L P

(6)
where F is a hypothesis set and L ◦ f denotes the function (x  y) 7→ L(x  y  f(x)). Finally  let
Υ : F → [0 ∞) be a regularizer  f0 ∈ F be a ﬁxed function and B0 ≥ B be a constant such that
kL ◦ f0k∞ ≤ B0. Then  for all ﬁxed ε > 0  δ ≥ 0  τ > 0  and n ≥ max{b/8  22+5/γb−1/γ}  every
learning method deﬁned by (4) satisﬁes with probability µ not less than 1 − 3Ce−τ :
Υ(fDn Υ) + RL P ( ¯fDn Υ) − R∗

L P < 3(cid:0)Υ(f0) + RL P (f0) − R∗

(cid:1) +
(cid:18)36cσV (τ + lnN (F k · k∞  ε))

(cid:19)1/(2−ϑ)
nα + 4ε + 2δ

4B0cBτ

L P

 

f ∈ F 

+

nα

 

where α := γ

γ+1   C := 1 + 4e−2c  cσ := ( 82+γ

b

)1/(1+γ)  and cB := cσ/3.

Before we illustrate this theorem by a few examples  let us brieﬂy discuss the variance bound (6).
For example  if Y = [−M  M] and L is the least squares loss  then it is well-known that (6) is
satisﬁed for V := 16M 2 and ϑ = 1  see e.g. [13  Example 7.3]. Moreover  under some assumptions
on the distribution P   [14] established a variance bound of the form (6) for the so-called pinball
loss used for quantile regression. In addition  for the hinge loss  (6) is satisﬁed for ϑ := q/(q + 1) 
if Tsybakov’s noise assumption holds for q  see [13  Theorem 8.24]. Finally  based on [2]  [12]
established a variance bound with ϑ = 1 for the earlier mentioned clippable modiﬁcations of strictly
convex  twice continuously differentiable margin-based loss functions.
One might wonder  why the constant B0 is necessary in Theorem 2.1  since appearently it only adds
further complexity. However  a closer look reveals that the constant B only bounds functions of the
form L ◦ ¯f  while B0 bounds the function L ◦ f0 for an unclipped f0 ∈ F. Since we do not assume
that all f ∈ F satisfy ¯f = f  we conclude that in general B0 is necessary. We refer to Examples 2.4
and 2.5 for situations  where B0 is signiﬁcantly larger than B.
Let us now consider a few examples of learning methods to which Theorem 2.1 applies. The ﬁrst
one is empirical risk minimization over a ﬁnite set.
Example 2.2 Let the hypothesis set F be ﬁnite and Υ(f) = 0 for all f ∈ F. Moreover  assume
that kfk∞ ≤ M for all f ∈ F. Then  for accuracy δ := 0  the learning method described by (4) is
ERM  and Theorem 2.1 provides  by some simple estimates  the oracle inequality
RL P (fDn Υ) − R∗

(cid:18)36cσV (τ + ln|F|)

(cid:0)RL P (f) − R∗

(cid:19)1/(2−ϑ)

(cid:1) +

4BcBτ

+

.

L P

L P < 3 inf
f∈F

nα

nα

3

Besides constants  this oracle inequality is an exact analogue to the standard oracle inequality for
C
ERM learning from i.i.d. processes  [13  Theorem 7.2].

Before we present another example  let us ﬁrst reformulate Theorem 2.1 for the case that the involved
covering numbers have a certain polynomial behavior.

Corollary 2.3 Consider the situation of Theorem 2.1 and additionally assume that there exist con-
stants a > 0 and p ∈ (0  1] such that

Then there is cp ϑ > 0 only depending on p and ϑ such that the inequality of Theorem 2.1 reduces to

lnN (F k · k∞  ε) ≤ a ε−2p  

ε > 0.

L P < 3(cid:0)Υ(f0) + RL P (f0) − R∗
(cid:19)1/(2−ϑ)

(cid:18)36cσV τ

L P

+

nα

+

4B0cBτ

nα + 2δ .

(cid:18)cσV a

(cid:19)1/(2+2p−ϑ)

(cid:1) + cp ϑ

nα

Υ(fDn Υ) + RL P ( ¯fDn Υ) − R∗

For the learning rates considered in the following examples  the exact value of cp ϑ is of no impor-
tance. However  a careful numerical analysis shows that cp ϑ ≤ 40 for all p ∈ (0  1] and ϑ ∈ [0  1].
Corollary 2.3 can be applied to various methods including e.g. SVMs with the hinge loss or the
pinball loss  and regularized boosting algorithms. For the latter  we refer to e.g. [2] for some learning
rates in the i.i.d. case and to [7] for a consistency result in the case of geometrically β-mixing
observations. Unfortunately  a detailed exposition of the learning rates resulting from Corollary 2.3
for all these algorithms is clearly out of scope this paper  and hence we will only discuss learning
rates for LS-SVMs. However  the only reason we picked LS-SVMs is that they are one of the few
methods for which both rates for learning from α-mixing processes and optimal rates in the i.i.d. case
are known. By considering LS-SVMs we can thus assess the sharpness of our results. Let us begin
by brieﬂy recalling LS-SVMs. To this end  let X be a compact metric space and k be a continuous
kernel on X with reproducing kernel Hilbert space (RKHS) H. Given a regularization parameter
λ > 0 and the least squares loss L(y  t) := (y − t)2  the LS-SVM ﬁnds the unique solution

(cid:0)λkfk2

H + RL Dn(f)(cid:1) .

To describe the approximation properties of H  we further need the approximation error function

A(λ) := inf
f∈H

H + RL P (f) − R∗

L P

λ > 0 .

Example 2.4 (Rates for least squares SVMs) Let X be a compact metric space  Y = [−1  1]  and
Z and P as above. Furthermore  let L be the least squares loss and H be the RKHS of a continuous
kernel k over X. Assume that the closed unit ball BH of H satisﬁes

(7)
where a > 0 and p ∈ (0  1] are some constants. In addition  assume that the approximation error
function satisﬁes A(λ) ≤ cλβ for some c > 0  β ∈ (0  1]  and all λ > 0. We deﬁne

ε > 0 

lnN (BH  k · k∞  ε) ≤ a ε−2p  

fDn λ = arg min
f∈H

(cid:0)λkfk2

(cid:1)  

o

.

n

ρ := min

β 

β

β + 2pβ + p

Then Corollary 2.3 applied to F := λ−1/2BH shows that the LS-SVM using λn := n−αρ/β learns
with rate n−αρ. Let us compare this rate with other recent results: [17] establishes the learning rate

− 2β

β+3  

n

whenever (2) is satisﬁed for some α. At ﬁrst glance  this rate looks stronger  since it is independent
of α. However  a closer look shows that it depends on the conﬁdence level 1− 3Ce−τ by a factor of
eτ rather than by the factor of τ appearing in our analysis  and hence these rates are not comparable.
Moreover  in the case α = 1  our rates are still faster whenever p ∈ (0  1/3]  which is e.g. satisﬁed

4

− αβ

for sufﬁciently smooth kernels  see e.g. [13  Theorem 6.26]. Moreover  [19] has recently established
the rate

n

(8)
2p+1  
which is faster than ours  if and only if β > 1+p
1+2p. In particular  for highly smooth kernels such
as the Gaussian RBF kernels  where p can be chosen arbitrarily close to 0  their rate is never faster.
Moreover  [19] requires knowing α  which  as we will brieﬂy discuss in Remark 2.6  is not the case
for our rates. In this regard  it is interesting to note that their iterative proof procedure  see [13 
Chapter 7.1] for a generic description of this technique  can also be applied to our oracle inequality.
The resulting rate is essentially n−α min{β β/(β+pβ+p)}  which is always faster than (8). Due to
space constraints and the fact that these rates require knowing α and β  we skip a detailed exposition.
Finally  both [19] and [17] only consider LS-SVMs  while Theorem 2.1 applies to various learning
C
methods.

Example 2.5 (Almost optimal rates for least squares SVMs) Consider the situation of Example
2.4  and additionally assume that there exists a constant Cp > 0 such that
f ∈ H.

kfk∞ ≤ Cp kfkp

Hkfk1−p

(9)

L2(PX )  

As in [16]  we can then bound B0 ≤ λ(β−1)p  and hence the SVM using λn := n
with rate

− αβ

β+2pβ+p  

n

−

α

β+2pβ+p learns

− β

β+p in the i.i.d. case  see [16]. In particular  if H = W m(X)
compared to the optimal rate n
is a Sobolev space over X ⊂ Rd with smoothness m > d/2  and the marginal distribution PX
is absolutely continuous with respect to the uniform distribution  where corresponding density is
bounded away from 0 and ∞  then (7) and (9) are satisﬁed for p := d
2m. Moreover  the assumption
L P ∈ W s(X) and
on the approximation error function is satisﬁed for β := s/m  whenever f∗
s ∈ (d/2  m]. Consequently  the resulting learning rate is

−

n

2sα

2s+d+2ds/m  

which in the i.i.d. case  where α = 1  is worse than the optimal rate n− 2s
2s+d by the term 2ds/m. Note
that this difference can be made arbitrarily small by picking a sufﬁciently large m. Unfortunately 
we do not know  whether the extra term 2ds/m is an artifact of our proof techniques  which are
relatively light-weighted compared to the heavy machinery used in the i.i.d. case. Similarly  we
do not know  whether the used Bernstein inequality for α-mixing processes  see Theorem 3.1  is
optimal  but it is the best inequality we could ﬁnd in the literature. However  if there is  or will be  a
better version of this inequality  our oracle inequalities can be easily improved  since our techniques
C
only require a generic form of Bernstein’s inequality.

Remark 2.6 In the examples above  the rates were achieved by picking particular regularization
sequences that depend on both α and β  which in turn  are almost never known in practice. Fortu-
nately  there exists an easy way to achieve the above rates without such knowledge. Indeed  let us
assume we pick a polynomially growing n−1/p-net Λn of (0  1]  split the training sample Dn into
n  λ for all λ ∈ Λn 
two (almost) equally sized and consecutive parts D(1)
and pick a λ∗ ∈ Λn whose fD
n  λ∗ minimizes the RL D
-risk over Λn. Then combining Example
2.2 with the oracle inequality of Corollary 2.3 for LS-SVMs shows that the learning rates of the
Examples 2.4 and 2.5 are also achieved by this training-validation approach. Although the proof is
C
a straightforward modiﬁcation of [13  Theorem 7.24]  it is out of the page limit of this paper.

n   compute fD

n and D(2)

(2)
n

(1)

(1)

3 Proofs
In the following  btc denotes the largest integer n satisfying n ≤ t  and similarly  dte denotes the
smallest integer n satisfying n ≥ t.
The key result we need to prove the oracle inequality of Theorem 2.1 is the following Bernstein type
inequality for geometrically α-mixing processes  which was established in [9  Theorem 4.3]:

5

Theorem 3.1 Let Z := (Zi)i≥1 be an X × Y -valued stochastic process that satisﬁes (2) and P be
deﬁned by (1). Furthermore  let h : X × Y → R be a bounded measurable function for which there
exist constants B > 0 and σ ≥ 0 such that EP h = 0  EP h2 ≤ σ2  and khk∞ ≤ B. For n ≥ 1 we
deﬁne

n(γ) :=
Then  for all n ≥ 1 and all ε > 0  we have

$

n

γ+1(cid:25)−1%
(cid:24)(cid:18)8n
(cid:19) 1
(cid:27)(cid:19)
≤ (cid:0)1 + 4e−2c(cid:1)e

b

.

ω ∈ Ω :

1
n

h(Zi(ω)) ≥ ε

− 3ε2n(γ)

6σ2+2εB .

(10)

(cid:18)(cid:26)

µ

ε2nα

(cid:18)(cid:26)

µ

nX

i=1

i=1

nX

i=1

Before we prove Theorem 2.1  we need to slightly modify (10). To this end  we ﬁrst observe that
dte ≤ 2t for all t ≥ 1 and btc ≥ t/2 for all t ≥ 2. From this it is easy to conclude that  for all n
satisfying n ≥ n0 := max{b/8  22+5/γb−1/γ}  we have

where α := γ

)1/(1+γ)  and cB := cσ/3  we thus obtain

n(γ) ≥ 2− 2γ+5
γ+1 b
(cid:18)(cid:26)
(cid:27)(cid:19)
γ+1. For C := 1 + 4e−2c  cσ := ( 82+γ
h(Zi(ω)) ≥ ε

ω ∈ Ω :

nX

µ

b

1
n

1
γ+1 nα  

≤ Ce−τ  

n ≥ n0 

r

(cid:27)(cid:19)

where τ :=

cσσ2+εcB B . Simple transformations and estimations then yield

ω ∈ Ω :

1
n

h(Zi(ω)) ≥

τ cσσ2

nα + cBBτ

nα

≤ Ce−τ

(11)

for all n ≥ max{b/8  22+5/γb−1/γ} and τ > 0. In the following  we will use only this inequality.
In addition  we will need the following simple and well-known lemma:
Lemma 3.2 For q ∈ (1 ∞)  deﬁne q0 ∈ (1 ∞) by 1/q + 1/q0 = 1. Then  for all a  b ≥ 0  we have
(qa)2/q(q0b)2/q0 ≤ (a + b)2 and ab ≤ aq/q + bq0
Proof of Theorem 2.1: For f : X → R we deﬁne hf := L ◦ f − L ◦ f∗
L P . By the deﬁnition of
fDn Υ  we then have Υ(fDn Υ) + EDnh ¯fDn Υ ≤ Υ(f0) + EDnhf0 + δ  and consequently we obtain

/q0.

Υ(fDn Υ) + RL P ( ¯fDn Υ) − R∗

L P

= Υ(fDn Υ) + EP h ¯fDn Υ
≤ Υ(f0) + EDnhf0 − EDnh ¯fDn Υ + EP h ¯fDn Υ + δ
= (Υ(f0) + EP hf0) + (EDnhf0 − EP hf0) + (EP h ¯fDn Υ − EDnh ¯fDn Υ) + δ .
(cid:1) .
EDnhf0 − EP hf0 =(cid:0)EDn(hf0 − h ¯f0) − EP (hf0 − h ¯f0)(cid:1) +(cid:0)EDnh ¯f0 − EP h ¯f0
Let us ﬁrst bound the term EDnhf0 − EP hf0. To this end  we further split this difference into
(cid:0)(hf0 − h ¯f0) − EP (hf0 − h ¯f0)(cid:1)2 ≤ EP (hf0 − h ¯f0)2 ≤ B0 EP (hf0 − h ¯f0) .

Now L ◦ f0 − L ◦ ¯f0 ≥ 0 implies hf0 − h ¯f0 = L ◦ f0 − L ◦ ¯f0 ∈ [0  B0]  and hence we obtain

EP

Inequality (11) applied to h := (hf0 − h ¯f0) − EP (hf0 − h ¯f0) thus shows that

(12)

(13)

r τ cσB0 EP (hf0 − h ¯f0)

nα
holds with probability µ not less than 1 − Ce−τ . Moreover  using

EDn(hf0 − h ¯f0) − EP (hf0 − h ¯f0) <
q
2 + b
n−ατ cσB0 EP (hf0 − h ¯f0) ≤ EP (hf0 − h ¯f0) + n−αcσB0τ /4  

ab ≤ a

√

+ cBB0τ
nα

2  we ﬁnd

6

and consequently we have with probability µ not less than 1 − Ce−τ that
EDn(hf0 − h ¯f0) − EP (hf0 − h ¯f0) < EP (hf0 − h ¯f0) +

(14)
In order to bound the remaining term in (13)  that is EDnh ¯f0 − EP h ¯f0  we ﬁrst observe that (5)
implies kh ¯f0k∞ ≤ B  and hence we have kh ¯f0 − EP h ¯f0k∞ ≤ 2B. Moreover  (6) yields

7cBB0τ

4nα

.

EP (h ¯f0 − EP h ¯f0)2 ≤ EP h2
¯f0
In addition  if ϑ ∈ (0  1]  Lemma 3.2 implies for q := 2
s
and b := (2ϑ−1EP h ¯f0)ϑ/2  that
(cid:19) 1
(cid:18)
(cid:18) cσV τ

(cid:19)(cid:18) cσ2−ϑϑϑV τ

cσV τ(EP h ¯f0)ϑ

1 − ϑ
2

EDnh ¯f0 − EP h ¯f0 < EP h ¯f0 +

≤

nα

nα

2−ϑ

≤ V (EP h ¯f0)ϑ .
2−ϑ  q0 := 2

+ EP h ¯f0 ≤
(cid:19) 1

2−ϑ

2cBBτ

Since EP h ¯f0 ≥ 0  this inequality also holds for ϑ = 0  and hence (11) shows that we have

ϑ  a := (n−αcσ2−ϑϑϑV τ)1/2 

(cid:18) cσV τ

(cid:19) 1

2−ϑ

nα

+ EP h ¯f0.

(15)
with probability µ not less than 1 − Ce−τ . By combining this estimate with (14) and (13)  we now
obtain that with probability µ not less than 1 − 2Ce−τ we have

nα

nα

+

EDnh ¯f0 − EP h ¯f0 < EP h ¯f0 +

+

2cBBτ
nα +

7cBB0τ

4nα

 

(16)

i.e.  we have established a bound on the second term in (12).
Let us now ﬁx a minimal ε-net C of F  that is  an ε-net of cardinality |C| = N (F k · k∞  ε). Let
us ﬁrst consider the case nα < 3cB(τ + ln|C|). Combining (16) with (12) and using B ≤ B0 
B2−ϑ ≤ V   3cB ≤ cσ  2 ≤ 41/(2−ϑ)  and EP h ¯fDn Υ − EDnh ¯fDn Υ ≤ 2B  we then ﬁnd

(cid:18) cσV τ

(cid:19) 1

2−ϑ

nα

(cid:19) 1
Υ(fDn Υ) + RL P (fDn Υ) − R∗

≤ Υ(f0) + 2EP hf0 +

L P

+

2−ϑ

nα

(cid:18) cσV τ
2cBBτ
(cid:19) 1
(cid:18) cσV (τ + ln|C|)
nα +
(cid:18)36cσV (τ + ln|C|)
(cid:19) 1

nα

2−ϑ

+

2−ϑ

nα

≤ Υ(f0) + 2EP hf0 +

≤ 3Υ(f0) + 3EP hf0 +

7cBB0τ
4nα + (EP h ¯fDn Υ − EDnh ¯fDn Υ) + δ
4cBB0τ

(cid:18) cσ(τ + ln|C|)

(cid:19) 1

2−ϑ

+ δ

nα

nα + 2B
4cBB0τ

nα + δ

+

with probability µ not less than 1−2e−τ . It thus remains to consider the case nα ≥ 3cB(τ + ln|C|).
To establish a non-trivial bound on the term EP h ¯fD − EDnh ¯fD in (12)  we deﬁne functions

gf r :=

EP h ¯f − h ¯f
EP h ¯f + r

 

f ∈ F  

2−ϑ  q0 := 2

where r > 0 is a real number to be ﬁxed later. For f ∈ F  we then have kgf rk∞ ≤ 2Br−1  and for
ϑ > 0  q := 2

ϑ  a := r  and b := EP h ¯f 6= 0  the ﬁrst inequality of Lemma 3.2 yields
f r ≤

(EP h ¯f + r)2 ≤ (2 − ϑ)2−ϑϑϑ EP h2
¯f = 0 by the variance bound (6)  which in
f r ≤ V rϑ−2. Finally  it is not hard to see that EP g2
f r ≤ V rϑ−2 also holds for
(cid:19)

4r2−ϑ(EP h ¯f )ϑ ≤ V rϑ−2 .

Moreover  for ϑ ∈ (0  1] and EP h ¯f = 0  we have EP h2
turn implies EP g2
ϑ = 0. Now  (11) together with a simple union bound yields

EP h2
¯f

r

EP g2

(cid:18)

(17)

¯f

µ

Dn ∈ (X × Y )n : sup
f∈C

EDngf r <

cσV τ
nαr2−ϑ +

2cBBτ
nαr

≥ 1 − C |C| e−τ  

7

(cid:19)

EP h ¯f − EDnh ¯f <(cid:0)EP h ¯f + r(cid:1)(cid:18)r

and consequently we see that  with probability µ not less than 1 − C |C| e−τ   we have

(18)
for all f ∈ C. Since fDn Υ ∈ F  there now exists an fDn ∈ C with kfDn Υ − fDnk∞ ≤ ε. By the
assumed Lipschitz continuity of L the latter implies

(x  y) − h ¯fDn Υ(x  y)(cid:12)(cid:12) ≤(cid:12)(cid:12) ¯fDn(x) − ¯fDn Υ(x)(cid:12)(cid:12) ≤(cid:12)(cid:12)fDn(x) − fDn Υ(x)(cid:12)(cid:12) ≤ ε

cσV τ
nαr2−ϑ +

2cBBτ
nαr

(cid:19)

+ 2ε
with probability µ not less than 1 − C e−τ . By combining this estimate with (12) and (16)  we then
obtain that

nαr2−ϑ

nαr

+

cσV (τ + ln|C|)

2cBB(τ + ln|C|)

for all (x  y) ∈ X × Y . Combining this with (18)  we obtain

(cid:12)(cid:12)h ¯fDn
EP h ¯fDn Υ − EDnh ¯fDn Υ <(cid:0)EP h ¯f + ε + r(cid:1)(cid:18)r
(cid:18) cσV τ
+(cid:0)EP h ¯fDn Υ + ε + r(cid:1)(cid:18)r
(cid:18)36cσV (τ + ln|C|)
r

Υ(fDn Υ) + EP h ¯fDn Υ < Υ(f0) + 2EP hf0 +

we obtain  since 6 ≤ 361/(2−ϑ) 

(cid:19) 1

r :=

nα

(cid:16) cσV τ

(cid:17) 1

2−ϑ ≤ r
6

nα

and

(cid:19)1/(2−ϑ)

 

2−ϑ

+

2cBBτ
nα +

nα
cσV (τ + ln|C|)

7cBB0τ
(cid:19)
4nα + 2ε + δ

2cBB(τ + ln|C|)

(19)
holds with probability µ not less than 1 − 3Ce−τ . Consequently  it remains to bound the various
terms. To this end  we ﬁrst observe that for

nαr2−ϑ

nαr

+

In addition  V ≥ B2−ϑ  cσ ≥ 3cB  6 ≤ 361/(2−ϑ)  and nα ≥ 3cB(τ + ln|C|) imply

2cBB(τ + ln|C|)

rnα

=

6
9

· 3cB(τ + ln|C|)

nα

· B
r

cσV (τ + ln|C|)

nαr2−ϑ

≤ 1
6 .
(cid:17) 1
·(cid:16)3cB(τ + ln|C|)
·(cid:16)36cσV (τ + ln|C|)
(cid:17) 1

nα

1

2−ϑ

2−ϑ · V

r
2−ϑ =

≤ 6
9
≤ 1
9

1
9 .

nαr2−ϑ
Using these estimates together with 1/6 + 1/9 ≤ 1/3 in (19)  we see that
Υ(fDn Υ) + EP h ¯fDn Υ < Υ(f0) + 2EP hf0 + r
3
holds with probability µ not less than 1 − 3Ce−τ . Consequently  we have

7cBB0τ
4nα +

+

+ 2ε + δ

3

EP h ¯fDn Υ + ε + r
(cid:19)1/(2−ϑ)

4cBB0τ

+

nα +4ε+2δ  

(cid:18)36cσV (τ + ln|C|

nα

Υ(fDn Υ)+EP h ¯fDn Υ < 3Υ(f0)+3EP hf0 +
i.e. we have shown the assertion.

Proof of Corollary 2.3: The result follows from minimizing the right-hand side of the oracle in-
equality of Theorem 2.1 with respect to ε.

References
[1] P. L. Bartlett  O. Bousquet  and S. Mendelson. Local Rademacher complexities. Ann. Statist. 

33:1497–1537  2005.

[2] G. Blanchard  G. Lugosi  and N. Vayatis. On the rate of convergence of regularized boosting

classiﬁers. J. Mach. Learn. Res.  4:861–894  2003.

8

[3] R. C. Bradley. Introduction to Strong Mixing Conditions. Vol. 1-3. Kendrick Press  Heber City 

UT  2007.

[4] J. Fan and Q. Yao. Nonlinear Time Series. Springer  New York  2003.
[5] A. Irle. On consistency in nonparametric estimation under mixing conditions. J. Multivariate

Anal.  60:123–147  1997.

[6] W. S. Lee  P. L. Bartlett  and R. C. Williamson. The importance of convexity in learning with

squared loss. IEEE Trans. Inform. Theory  44:1974–1980  1998.

[7] A. Lozano  S. Kulkarni  and R. Schapire. Convergence and consistency of regularized boosting
In Y. Weiss  B. Sch¨olkopf  and J. Platt 
algorithms with stationary β-mixing observations.
editors  Advances in Neural Information Processing Systems 18  pages 819–826. MIT Press 
Cambridge  MA  2006.

[8] R. Meir. Nonparametric time series prediction through adaptive model selection. Mach. Learn. 

39:5–34  2000.

[9] D. S. Modha and E. Masry. Minimum complexity regression estimation with weakly dependent

observations. IEEE Trans. Inform. Theory  42:2133–2145  1996.

[10] M. Mohri and A. Rostamizadeh. Stability bounds for non-i.i.d. processes.

In J.C. Platt 
D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Sys-
tems 20  pages 1025–1032. MIT Press  Cambridge  MA  2008.

[11] M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In
D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors  Advances in Neural Information
Processing Systems 21  pages 1097–1104. 2009.

[12] I. Steinwart. Two oracle inequalities for regularized boosting classiers. Statistics and Its

Interface  2:271284  2009.

[13] I. Steinwart and A. Christmann. Support Vector Machines. Springer  New York  2008.
[14] I. Steinwart and A. Christmann. Estimating conditional quantiles with the help of the pinball

loss. Bernoulli  accepted with minor revision.

[15] I. Steinwart  D. Hush  and C. Scovel. Learning from dependent observations. J. Multivariate

Anal.  100:175–194  2009.

[16] I. Steinwart  D. Hush  and C. Scovel. Optimal rates for regularized least squares regression. In
S. Dasgupta and A. Klivans  editors  Proceedings of the 22nd Annual Conference on Learning
Theory  pages 79–93. 2009.

[17] H. Sun and Q. Wu. Regularized least square regression with dependent samples. Adv. Comput.

Math.  to appear.

[18] M. Vidyasagar. A Theory of Learning and Generalization: With Applications to Neural Net-

works and Control Systems. Springer  London  2nd edition  2003.

[19] Y.-L. Xu and D.-R. Chen. Learning rates of regularized regression for exponentially strongly

mixing sequence. J. Statist. Plann. Inference  138:2180–2189  2008.

[20] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. Ann.

Probab.  22:94–116  1994.

[21] B. Zou and L. Li. The performance bounds of learning machines based on exponentially

strongly mixing sequences. Comput. Math. Appl.  53:1050–1058  2007.

9

,Aron Yu
Kristen Grauman
Matthew Chalk
Olivier Marre
Gasper Tkacik