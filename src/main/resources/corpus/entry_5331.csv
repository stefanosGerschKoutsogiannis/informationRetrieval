2018,Submodular Maximization via Gradient Ascent: The Case of Deep Submodular   Functions,We study the problem of maximizing deep submodular functions (DSFs) subject to a matroid constraint. DSFs are an expressive class of submodular functions that include  as strict subfamilies  the facility location  weighted coverage  and sums of concave composed with modular functions. We use a strategy similar to the continuous greedy approach  but we show that the multilinear extension of any DSF has a natural and computationally attainable concave relaxation that we can optimize using gradient ascent. Our results show a guarantee of $\max_{0<\delta<1}(1-\epsilon-\delta-e^{-\delta^2\Omega(k)})$ with a running time of $O(\nicefrac{n^2}{\epsilon^2})$ plus time for pipage rounding
to recover a discrete solution  where $k$ is the rank of the matroid constraint. This bound is often better than the standard $1-1/e$ guarantee of the continuous greedy algorithm  but runs much faster. Our bound also holds even for fully curved ($c=1$) functions where the guarantee of $1-c/e$ degenerates to $1-1/e$ where $c$ is the curvature of $f$.  We perform computational experiments that support our theoretical results.,Submodular Maximization via Gradient Ascent:

The Case of Deep Submodular Functions

Depts. of Electrical & Computer Engineering‡  Computer Science and Engineering$  and Genome Sciences∗

Wenruo Bai‡  William S Noble∗$  Jeff A. Bilmes‡$

Seattle  WA 98195

{wrbai wnoble bilmes}@uw.edu

Abstract

We study the problem of maximizing deep submodular functions (DSFs) [13  3]
subject to a matroid constraint. DSFs are an expressive class of submodular
functions that include  as strict subfamilies  the facility location  weighted coverage 
and sums of concave composed with modular functions. We use a strategy similar
to the continuous greedy approach [6]  but we show that the multilinear extension
of any DSF has a natural and computationally attainable concave relaxation that we
can optimize using gradient ascent. Our results show a guarantee of max0<δ<1(1−
− δ− e−δ2Ω(k)) with a running time of O(n2/2) plus time for pipage rounding [6]
to recover a discrete solution  where k is the rank of the matroid constraint. This
bound is often better than the standard 1 − 1/e guarantee of the continuous greedy
algorithm  but runs much faster. Our bound also holds even for fully curved (c = 1)
functions where the guarantee of 1 − c/e degenerates to 1 − 1/e where c is the
curvature of f [37]. We perform computational experiments that support our
theoretical results.

Introduction

1
A set function f : 2V → R+ is called submodular [15] if f (A) + f (B) ≥ f (A ∪ B) + f (A ∩ B) for
all A  B ⊆ V   where V = [n] is the ground set. An equivalent deﬁnition of submodularity states that
f (v|A) ≥ f (v|B) for all A ⊆ B ⊆ V and v ∈ V \B  where f (v|A) ≡ f ({v}∪A)−f (A) is the gain
of element v given A. This property of diminishing returns well models concepts such as information 
diversity  and representativeness. Recent studies have shown that submodularity is natural for a large
number of real world machine learning applications such as information gathering [23]  probabilistic
models [12]  image segmentation [22]  string alignment [28]  document and speech summarization
[27  26]  active learning [39]  genomic assay selection [40] and protein subset selection [25]  as well
as many others.
In addition to having a variety of natural applications in machine learning  the optimization properties
of submodular functions appear to be ever more auspicious. On one hand  the submodular minimiza-
tion problem can be exactly solved in polynomial time [29  11  15]. Recent studies mostly focus on
improving running times [24  7]. On the other hand  submodular maximization is harder  and the
optimal solution cannot be found by any polynomial time algorithm. A good approximate solution 
however  is usually acceptable  and a simple greedy algorithm can ﬁnd a constant factor 1 − 1/e
approximate solution for the monotone non-decreasing1 submodular maximization problem subject
to a k-cardinality constraint [32]. Although submodular maximization is a purely combinatorial
problem  there are also approaches to solve it via continuous relaxation (e.g. multilinear extension).
For example  [6] offers a randomized continuous greedy algorithm that offers the same 1 − 1/e
bound for monotone non-decreasing submodular maximization subject to a more general matroid
1A submodular function is said to be monotone non-decreasing if f (v|A) ≥ 0 for all v ∈ V and A ⊆ V .

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

independence constraint. If the function’s curvature c is taken into account  this approach yields
an improved guarantee of no worse than 1 − c/e [37]. Recent studies showed stochastic projected
gradient methods [18  31] can be useful on maximizing continuous DR-submodular function [35] 2.
The best guarantee is (1 − OP T/e − ) by 1/3 iterations in gradient methods [31].
The above results apply to any non-negative monotone submodular function. In practice  solving
a given problem requires applying the algorithm to a speciﬁc submodular function  for example 
set cover [16  20]  facility location [38]  feature-based [21]  graph cut [19] and deep submodular
functions (DSF) [13  3]. When working with a speciﬁc sub-class of functions  we can beneﬁt
from knowing the speciﬁc form and its mathematical properties. For example  in the simplest case 
maximizing a modular function (i.e.  a function f for which both f and −f are submodular) under a
matroid constraint can be exactly solved by a greedy algorithm. [20] showed beneﬁt for submodular
maximization in the speciﬁc case of weighted coverage functions.
In our work  we focus on DSF maximization under a matroid constraint. Introduced in [13  3]  DSFs
are a generalization of set coverage  facility location  and feature-based functions. Importantly  the
class of DSFs is a strict superset of the union of these three  which means that any method designed for
a general DSF can be applied to set coverage  facility location  and feature-based functions but not vice

versa. For example (cid:112)m(A) is concave over modular; a feature-based function has the form of a sum
of concave composed with modular functions  such as(cid:112)m1(A) + log(1 + m2(A))  while a two-layer
(cid:104)
(cid:105)1/4
m3(A) +(cid:112)m4(A)

(cid:113)(cid:112)m1(A) + arctan(m2(A))+

DSF has a nested composition of the form
In [3]  it was shown that the expressivity of DSFs strictly grows with the number of layers.
To our knowledge  there have been no studies on the speciﬁc problem of DSF maximization. On
the one hand  we can use the generic greedy or continuous greedy algorithms for DSF  since DSF is
monotone submodular  but we should not be surprised if better bounds than 1 − 1/e can be achieved
using the structure and properties of a DSF. The major contribution of the present work is to show
that a very natural and computationally easy-to-obtain concave extension of DSFs is a nearly tight
relaxation of the DSF’s multilinear extension. Therefore  given this extension  we can use projected
gradient ascent (Algorithm. 1 [4]) to maximize the concave extension and obtain a fractional solution 
and then use pipage rounding [2  6] to recover a discrete solution.
Our approach has the following advantages over the continuous greedy algorithm with only oracle
access to the submodular function:

.

(cid:21)

wmax

(cid:20)

− δ2wmin k

1 − |V (1)|e

multilinear extension which often itself needs to be approximated using sampling.

1. Easy concave extension: A natural concave extension of any DSF is easy to obtain  unlike the
2. Better guarantee for large k: Our method has a guarantee of max0<δ<1(1− − δ − e−δ2Ω(k))) 
where k is the rank of the matroid constraint (Corollary 2). A more complete formulation is
max0<δ<1(1 − )(1 − δ)
  where wmin/wmax is the ratio of the smallest
to the largest DSF element in the ﬁrst weight layer of a DSF and |V (1)| is the size of the feature
layer (see Figure 2). Importantly  this bound holds even when the curvature [37] of the DSF
is c = 1 so the 1 − c/e bound of [37] is at its worst at 1 − 1/e (Lemma 7 in Appendix). We
compare our bound with the traditional 1/2 (for the greedy) and 1 − 1/e (for the continuous
greedy) bounds in Figure 1. We show that our bound is better than the continuous greedy
algorithm (1 − 1/e) for large k (> 102 ∼ 104 depending on k and wmin/wmax).
3. Improved running time: Other than the fact that a natural concave extension of a DSF is readily
available  the running time of our method is O(n2−2) and is thus better than the O(n7) cost
for the continuous greedy algorithm. Most of the continuous greedy algorithm’s running time is
for estimating the multilinear extension (O(n5) [6])  while in our method  calculating the DSF
concave extension only needs one evaluation of the original function.

1.1 Background and Related work

[13  3] introduced deep submodular functions where [3] discussed their theoretical properties and
[13] their training in a fashion similar to how deep neural networks may be trained. Particularly
relevant to the present study  [3] showed that while DSFs cannot express all submodular functions 

2Multilinear extension is a special case of continuous DR-submodular.

2

(a)

(b)

Figure 1: Guarantee of propose methods stated in Theorem 3. Solid lines are the proposed guarantees
with respect to the rank of matroid constraint; dash lines are guarantees for the continuous greedy
algorithm and the greedy algorithm. Our guarantee is proportional to 1 −  and in the above ﬁgure 
we use  = 0.01 for illustration. (a) is ﬁxing |V (1)| = 10 and each trace is for different wmin/wmax 
which is the ratio of the smallest feature to the largest feature. (b) is ﬁxing wmin/wmax = 0.1 and each
trace is for different |V (1)|  which is the size of the features layer (see Figure 2).

are subsets of V   αi are non-negative numbers  and βi are non-negative integers.

below.

negative numbers. It is a subclass of weighted coverage functions [20].

k-layer DSFs strictly generalize k − 1-layer DSFs. Moreover  the following classes of functions are
all strict subclasses of DSFs [3].

functions [36]. These functions take the form f (A) =(cid:80)
2. Weighted cardinality truncation (WCT) functions. f (A) =(cid:80)
3. Weighted coverage (WC) functions which take the form f (A) =(cid:80)
4. Facility location (FL) functions. f (A) = (cid:80)

1. Sums of concave composed with non-negative modular functions plus an arbitrary modular
function (SCMM)  also called feature-based functions [21]  or “decomposable” submodular
i αiφi(mi(A))+m±(A) where αi are
non-negative numbers  φi are monotone non-decreasing concave functions  mi are non-negative
modular functions  and m± is an arbitrary modular function.
i αi min(|A ∩ Vi|  βi) where Vi
i αi min(|A ∩ Vi|  1). See
i∈V maxj∈A wij where wij is a matrix of non-
In particular  we have the following chain relationship between these classes of functions: FL ⊂
WC ⊂ WCT ⊂ SCMM ⊂ DSF ⊂ All-Submodular-Functions [3]. In the present paper  we address
any function that can be represented as a DSF.
In [20]  submodular maximization of the special case of weighted coverage (WC) functions was
studied  using an approach that took a concave relaxation of the multilinear extension of such
functions. Let U be a set and m : 2U → R+ be a non-negative modular function. The ground set V =
{B1  B2  . . .   Bn} is a collection of subsets of U. A weighted coverage function f (S) : 2V → R+ is
u∈U m(u) min(1 |S ∩ Cu|)
where Cu = {Bi|u ∈ Bi}  which reveals that the weighted coverage function is actually a simple
example of a one-layer DSF. In [20]  Karimi et al. show that the multilinear extension of f has a
u∈U m(u) min(1  1Cu · x) within a 1 − 1/e approximation.
They ﬁrst optimize the concave relaxation and claim that the solution is also good maximizer for the
multilinear extension by the 1 − 1/e approximation. They further show that their approach yields
solutions that match the 1 − 1/e guarantee of the continuous greedy algorithm  while reducing the
computational cost by several orders of magnitude  mostly because they do not need to compute the
multilinear extension.
Our framework in the present paper is a strict generalization of this previous method in the following
ways: (1) The weighted coverage function class is a subclass of DSFs  and Karimi et al.’s proposed
concave extension is a special case of a more general DSF concave extension; (2) We use a similar
algorithmic approach which thus also has the advantage of better running time over the continuous
greedy algorithm; and (3) We offer a still better bound for large k where k is the rank of the matroid.
(cid:80)
As an example application  we note that DSFs generalize feature-based functions which are useful
for various summarization tasks [21  41  17]. A feature-based function has the form f (A) =
u∈U wuφu(mu(A)) where U is a set of features  wu > 0 is a feature weight for u ∈ U  mu(A) =

deﬁned as f (S) = m(∪Bi∈SBi). An equivalent formula is f (S) =(cid:80)
natural concave relaxation ¯F (x) = (cid:80)

3

101102103104105106107rank of matroid constraint  k0.00.20.40.60.81.0guaranteecontinuous greedy algorithmgreedy algorithmwmin/wmax=0.5wmin/wmax=0.1wmin/wmax=0.01101102103104105106107rank of matroid constraint  k0.00.20.40.60.81.0guaranteecontinuous greedy algorithmgreedy algorithm|V(1)|=100|V(1)|=1000|V(1)|=10000|V(1)|=100000|V(1)|=1000000|V(1)|=10000000(cid:80)

x∈X mu(A) is a feature-speciﬁc non-negative modular function  and φu(x) is a feature-speciﬁc
monotone non-decreasing concave functions. Immediately  we have that the feature base functions
are DSFs. Our proposed methods  therefore  offer a good bound for maximizing such functions if
minu∈U

minv∈V mu(v)
maxv∈V mu(v) k is large  which is fairly common in practice.

2 Background and Problem Setup
We assume every set function f in this paper is normalized (i.e.  f (∅) = 0). A function m is modular
if and only if m and −m are both submodular. A normalized modular function m(A) always has
v∈V m(v) = w · 1A  where A ⊆ V   w and 1A are n-dimensional vectors 
w = (m(1)  m(2)  . . .   m(n)) and 1A ∈ RV

the form of m(A) =(cid:80)

+ is 0 for coordinate i /∈ A and 1 for i ∈ A.

2.1 Matroid and matroid polytopes
A matroid M = (V I) is a family of subsets of ground set V with the following three properties:

1. ∅ ∈ I.
2. If A ∈ I  then B ∈ I for all B ⊆ A.
3. For all A  B ∈ I  if |A| > |B|  then there exists an element v ∈ A \ B  s.t. B ∪ {v} ∈ I.

The sets I ∈ I are the independent sets of the matroid. The third property ensures that the maximal
independent sets always have the same size  equal to the rank rM = k of the matroid. Matroids can
be generalized to the continuous domain via the matroid polytope P = conv(1A : A ∈ I) where
“conv” means the convex hull.

2.2 Deep Submodular Function (DSFs)

A DSF [13  3] f is a natural generalization of
feature-based functions and can be deﬁned on
a directed graph (Figure 2). The graph has
K + 1 layers  where the ﬁrst layer V = V (0)
is the function’s ground set  and additional lay-
ers V (1)  V (2)  V (3)  . . .   V (K) are sets of “fea-
tures”  “meta features”  “meta-meta features” 
etc. The size of V (i) is di = |V (i)| for i =
0  1  2  . . .   K. Note that the size of the ﬁnal
layer V (K) is always 1 because a DSF maps a
set to a real number. For any i = 1  2  . . .   K  two successive layers V (i−1) and V (i) are con-
nected by a matrix w(i) ∈ Rdi×di−1
. Therefore  matrix w(i) is indexed by (vi  vi−1) for vi ∈ V (i)
and vi−1 ∈ V (i−1). w(i)
vi (vi−1) is an element from row vi and column vi−1. We may think of
: 2V (i−1) → R+ as a modular function deﬁned on subset of V (i−1). Further  let φvi : R+ → R+
w(i)
vi
be a non-negative  non-decreasing concave function. Thus  each element vi ∈ V (i) has a mod-
ular function w(i)
vi and concave function φvi for i = 1  2  . . .   K. In this setting  a K-layer DSF
f : 2V → R+ can be expressed  for any A ⊆ V   as follows:

Figure 2: A layered DSF with K = 3 layers.

+

 (cid:88)

vK−1∈V (k−1)

¯f (A) = φ

vK

. . .

(cid:88)

(K)

vK (vk−1)φ

w

vK−1

f (A) = ¯f (A) + m±(A) 

where

 (cid:88)

v1∈V (1)

(cid:88)

a∈A

(2)

v2 (v1)φv1
w



.


w

(1)
v1 (a)

(1)

(2)

(3)

v3 (v2)φv2
w

v2∈V (2)

2.2.1 Concave functions φvi and continuity
In a DSF  φvi is a normalized (i.e.  φvi(0) = 0) monotone non-decreasing concave function deﬁned
on [0  +∞). Via concavity  this implies that the function must also be continuous on (0  +∞). The
only point that need not be continuous is x = 0  i.e.  we may have limx→0+ φvi(x) > 0 = φvi(0).
When used in a DSF  however  the set of possible input values to φvi(x) is countable. Let β > 0
be the smallest strictly positive possible input to φvi(x). We deﬁne another φ0 vi : R+ → R+ s.t.

4

φ0 vi(x) ≡ φvi(x) for x ≥ β and φ0 vi(x) ≡ φvi (β)
β x for 0 ≤ x < β. φ0 vi is normalized  monotone
non-decreasing concave  and is continuous on [0  +∞). Moreover  replacing φvi(x) with φ0 vi(x)
leaves the DSF’s valuation uncharged for any set. Therefore  w.l.o.g. we assume that all concave
functions are also right-continuous at x = 0.

2.2.2 Final modular term m±
Recall that f (A) = ¯f (A) + m±(A)  where ¯f (A) has the form of nested concave over modular and
is always monotone non-decreasing  and m±(A) is a simple modular functions but can be negative.
Although [13  3] claim that the ﬁnal modular function is sometimes useful in applications  this ﬁnal
function will change the optimization properties of f  since ¯f is monotone non-decreasing but f is
non-monotone. In this work  we focus on the monotone non-decreasing DSF case where m± ≥ 0.3.

2.3 DSF maximization

The problem we consider is DSF maximization  i.e. 

(3)
where f is a DSF function and M is a matroid independence constraint. In this work  we focusing on
solving this problem with the knowledge that f is DSF.

Problem 1: max

A∈M f (A)

3 Continuous extension of submodular functions
Although a submodular function is discrete  providing one value to each A ⊆ V   it is often useful to
view such functions continuously. The bridge between the discrete and continuous worlds is made by
a continuous extension of a submodular function  which is some function from the hypercube [0  1]n
to R that agrees with f on the hypercube vertices [14]. This includes the Lovász extension [29] 
which is the convex closure of the function  and also the multilinear extension [6]  which is an
approximation of the concave closure. In general  most continuous methods [6  20] follow a similar
strategy: they ﬁrst ﬁnd a continuous extension of f  then optimize it to obtain a fractional solution 
and ﬁnally ﬁnish up by rounding the continuous solution back to a discrete ﬁnal solution set.
In our framework  we use an extension that is tailor-made for a DSF.

 (cid:88)

vK−1∈V (k−1)

. . .

(cid:88)

v2∈V (2)

 (cid:88)

v1∈V (1)

(cid:16)


(cid:17)
.

3.1 A DSF’s Natural Concave Extension
DSF functions have the form of nested sum of concave of modular (Equation (2)). [3] shows that
there exists a natural concave extension of f by replacing the discrete variables with real values in
the nested form F (x) = ¯F (x) + m± · x where

¯F (x) = φ

vK

(K)

vK (vk−1)φ

w

vK−1

(3)

v3 (v2)φv2
w

w

(2)

v2 (v1)φv1

w

(1)

v1 · x

(4)

Thus  f (A) in Equation (1) has f (A) = F (1A) for all A ⊆ V . In fact  we have the following:
Corollary 1 ([3]). The DSF concave extension F (x) : [0  1]n → R is an extension of a DSF f (x)
and is concave.

In [3]  it is claimed that the extension is potentially useful for maximizing DSFs  possibly in a
constrained fashion  followed by appropriate rounding methods  but the authors leave this as an open
question. In the present work  we address this claim and answer this question in the afﬁrmative.
Before presenting our algorithm  we ﬁrst discuss the relationship between DSF’s natural concave
extension and multilinear extension.

3.2 Multilinear extension

(cid:110)

:(cid:80)
S⊆V pS = 1  pS ≥ 0∀S ⊆ V  & (cid:80)

S⊆V pSf (S)  where
The concave closure of a submodular function f is deﬁned as minp∈(cid:52)n(x)
(cid:52)n(x) =
. The concave clo-
sure is NP-hard even to evaluate [14]; hence  the multilinear extension is often used. We ﬁrst specify
the following deﬁnition:

S⊆V pS1S = x

p ∈ R2n

(cid:80)
(cid:111)

3Note  if m± is non-negative  it can merge into ¯f (A) which is equivalent to m± = 0

5

Deﬁnition 1. For a given n-dimensional vector x ∈ [0  1]n  deﬁne Dx to be a distribution over sets
A  s.t. Pr(A) = Πv∈AxvΠv∈V \A(1 − xv).
If we sample a random set A from Dx  then the event v ∈ A is independent from u ∈ A if v (cid:54)= u and
Pr(v ∈ A) = xv. With these deﬁnitions  we may deﬁne the multlinear extension as:
Deﬁnition 2 (Multilinear Extension). Lf (x) = EA∼Dx f (A).
Calinescu et al. [6] showed that we may solve the following continuous problem instead of solving
Problem 1 directly.

Problem 2: max

x∈P Lf (x)

(5)

where P is a matroid polytope for M.
Unfortunately  two problems remain with the multilinear extension Lf (x). First  calculating the
exact value is not feasible in general  and even estimating it needs O(n5) time [6]. Second  it
is not concave. Therefore ﬁnding the global maximizer of Problem 2 is in general not feasible.
However  Calinescu et al. [6] developed a continuous greedy algorithm that ﬁnds ˆx s.t. Lf (ˆx) ≥
(1 − 1/e)Lf (x∗) where x∗ ∈ argmaxx∈P Lf (x). It is not hard to show that Lf (x∗) ≥ f (A∗) where
A∗ ∈ argmaxA∈M f (A)  since Lf (1A∗ ) = f (A∗). Therefore  Lf (ˆx) ≥ (1 − 1/e)f (A∗). Next  we
show how they round ˆx.

Rounding Rounding is a methodology that returns a discrete set from a fractional vector. “Pipage
rounding” was ﬁrst designed by Ageev et al. [2] and modiﬁed by Calinecu et al. [6] for submodular
modular maximization  using a convex property of the multilinear extension. It maintains the quality
of the solution in expectation  i.e.  E ˆA∼PIPAGE ROUNDING(ˆx)f ( ˆA) ≥ Lf (ˆx)  while satisfying the matroid
constraint  thus ﬁnishing the proof sketch of the 1 − 1/e bounds for the continuous greedy algorithm.
Another rounding technique is swap rounding [9] which can be seen as a replacement of pipage
rounding with better running time O(nk2). In the special case of the matroid constraint  e.g.  a simple
partition matroid [8  10]4  a simple rounding technique [5] is equivalent to pipage rounding with
much easier implementation and linear running time. In our work  we can use any proper rounding
techniques.
In this work  we show that given any DSF  it is not necessary to compute the multilinear extension at
all. This is based on the following theorem:
Theorem 1. For all f ∈ DSF  its DSF concave extension F   and for all x ∈ [0  1]n  we have
(1 − δ)
Proof. See Appendix A.

F (x) ≤ Lf (x) ≤ F (x) where ∆(x) = minv1∈V (1)

1 − |V (1)|e− δ2∆(x)

maxv∈V wv1 (v)

v1 ·x
w(1)

(cid:104)

(cid:105)

2

wmax

In Theorem 1  the term ∆(x) is fairly complex to interpret  but help can be gained by considering a
lower bound of ∆(x) offered by the following lemma:
Lemma 1. ∆(x) ≥ (cid:107)x(cid:107)1wmin
  where wmax = maxv1∈V (1) maxv∈V wv1 (v) and wmin =
minv1∈V (1) minv∈V wv1(v). If x is on the extreme point of a matroid polytope  then ∆(x) ≥ kwmin
where k is the rank of the matroid.
By applying Lemma 1 to Theorem 1 and ∆(x) = Ω(k) and noticing |V (1)|e−δ2Ω(k) ≥
e−δ2Ω(k)+log(|V (1)|) = e−δ2Ω(k)  we have the following results.
Proposition 1. maxδ(1 − δ)
In Figure 1  we show that the coefﬁcient of the lower bound converges to close to 1 as k → +∞.
Theorem 1 is one of the major results of the present work. It gives a concave relaxation (i.e.  the
natural concave extension of a DSF) of the non-concave multilinear extension Lf . In this sense 
we claim that multilinear extension Lf is closed to the DSF’s natural concave extension F . Not
surprisingly  maximizing a concave function is much easier than maximizing the multilinear extension
for a variety of reasons.

1 − e−δ2Ω(k)(cid:105)

F (x) ≤ Lf (x) ≤ F (x)

(cid:104)

wmax

4I = {A ⊆ V ||A ∪ Vi| ≤ 1 ∀i} where {Vi}s are a partition of V .

6

Lemma 2. Any concave problem solver that ﬁnds a solution ˆx such that F (ˆx) ≥ (1 − )F (x∗
satisfy Lf (ˆx) ≥ (1 − )(1 − δ)
of the corresponding function subject to the matroid polytope membership.
Proof. See Appendix B

1 − |V (1)|e− δ2∆(ˆx)

L)  where x∗

F ) will
L are the maximizer

F and x∗

2

(cid:105)L(x∗

(cid:104)

4 Projected Gradient Ascent

Following the general framework of [6  20]  we ﬁrst ﬁnd a fractional solution of the concave extension
and then employ pipage rounding to obtain a feasible set. This approach offers the aforementioned
guarantee for any member of the DSF family  regardless of its curvature.

4.1 Supergradient
For a concave function F : P → R  where P ⊆ Rn is a compact convex set  the set of supergradients
of f is deﬁned as

(6)

Given the formula of DSF concave extension F (x)  it is easy to compute supergradient as follows:

∂f (x) = {g ∈ Rn|f (y) − f (x) ≤ g · (y − x)∀y ∈ P}
(cid:88)

(cid:88)

(cid:88)

vK−1 (·) . . . φ
v2 (·)φ
v1 (·)w(K)
(cid:48)
(cid:48)
(cid:48)

. . .

φ

vK (vk−1) . . . w(2)

v2 (v1)w(1)

v1 (e)

g(x)e = φ

vK (·)
(cid:48)

(7)

v1∈V (1)

vK−1∈V (k−1)

:DSF concave extension F   matroid polytope P 
learning rate η  maximum number of iterations T

Algorithm 1: Projected Gradient Ascent [4]
input
Let x(0) ← argminx∈P (cid:107)x(cid:107)2
for t = 1  2  . . .   T do

v2∈V (2)
v1 (·)
where e ∈ [n] is a coordinate  φ(cid:48)
is the derivative of the concave func-
tion φv1 (x) at its current evaluation
if it is differentiable  or is any super-
gradient of φv1 (x) if it is not differ-
entiable. In fact  the way to calcu-
late the supergradient of a DSF is
exactly the same as what the back-
propagation algorithm needs in deep
neural network (DNN) training  and
this was used in [13] to train DSFs.
This is also one of the reasons for the
name deep submodular functions. Therefore  all of the toolkits available for DNN training  with
provisions for automatic symbolic differentiation (e.g.  PyTorch [33] and TensorFlow [1] ) can be
used to maximize a DSF. Since they are optimized for fast GPU computing  they can offer great
practical and computational advantages over traditional submodular maximization procedures.

compute a supergradient g(x(t−1)) ∈ ∂F (x(t−1)) ;
x(t) ← argminx∈P
2;
// This is done by projecting x(t−1) + ηg(x(t−1) to P

(cid:13)(cid:13)x −(cid:0)x(t−1) + ηg(x(t−1))(cid:1)(cid:13)(cid:13)2

end
return 1
T

(cid:80)T

t=1 x(t)

2

4.2 Projected gradient Ascent

We utilize the following theorem from [4  7] (modiﬁed for the concave  rather than convex  case) to
establish our bounds for DSF-based submodular maximization.
Theorem 2. [[4  7]] For any concave function F : Rn
supx∈P (cid:107)g(x)(cid:107)2
F (ˆx) ≥ maxx∈P F (x) − RB

+ → R  let R2 = supx∈P (cid:107)x(cid:107)2
2 and B2 =
BT will obtain a fractional solution ˆx s.t.

2  Algorithm 1 with learning rate η =

(cid:113) R

(cid:113) 2

T .

Applying Theorem 2 to Algorithm 1 and using our propose concave function F (x)  we have the
following result:
Lemma 3. For any 0 <  < 1  Algorithm 1 will obtain a fractional ˆx s.t. f (ˆx) ≥ (1 −
) maxx∈P f (x) with running time T = O(n2−2).
Proof. See Appendix C.

Thus  we have a approximate solution to the concave maximization problem and using this  in concert
with Lemma 2  we arrive at the following which offers a guarantee of our proposed method.

7

(a)

(b)

(c)

Figure 3: (a) DSF structure (b) performance comparison  solution value vs. k  (c) running time vs. k.

(cid:20)

(cid:21)

Theorem 3. Algorithm 1 with pipage rounding will give ˆX such that Ef ( ˆX) ≥ max0<δ<1(1 −
)(1 − δ)

maxX⊆M f (X) with running time T = O(n2−2)

1 − |V (1)|e

− δ2wmink

wmax

In Figure 1  we have a comparison of this bound with the traditional 1/2 and 1 − 1/e bounds. We ﬁnd
our proposed bound approaches 1 when k → +∞ and beats other bounds for large k (> 104 ∼ 106 
depending on wmin/wmax).
Corollary 2. Algorithm 1 with with pipage rounding will give ˆX such that Ef ( ˆX) ≥ max0<δ<1(1−
 − δ − e−δ2Ω(k)) maxX⊆M f (X) with running time T = O(n2−2)

5 Experiments

w(2)

i

i

(cid:113)

(1)f1 1(A) + w(2)

(2)f1 2(A) for i ∈ V (2)  where w(2)

In this section  we perform a number of synthetic dataset experiments in order to demonstrate proof
of concept and also to offer empirical evidence supporting our bounds above. While the results of the
paper are primarily theoretical  the results of this section show that our methods can yield practical
beneﬁt and also demonstrate the potential of the above methods for large-scale DSF-constrained
maximization.
Figure 3 shows the structure of the DSF f : 2V → R+ to be maximized.
It is a three-layer
DSF having ground set V = V (0) with |V | = n. We partition the ground set V into blocks
V1 ∪ V2 ∪ V3 s.t. |V1| = |V2| = |V3| = t  where t = |V |/3. In the next layer V (1)  the inner
part of f consists of two concave-composed-with-modular functions  f1 1(A) = min(|X ∩ [V1 ∪
V3]| + α|X ∩ V2|  t) and f1 2(A) = α|X ∩ [V1 ∪ V3]| + |X ∩ V2| where α = 0.1 is a parameter. In
the subsequent layer V (2)  every node is concave over the weighted sum of f1 1(A) and f1 2(A) 
(cid:80)
i.e.  f2 i(A) =
is a 2-dimensional
uniformly at random vector from [0  1]2. Finally  for the last layer V (3)  the entire function f (A) =
i∈V (2) w(3)(i)f2 i(A)  where w(3) is a |V (2)|−dimensional vector again uniformly at random from
[0  1]. The matroid constraint is a partition matroid s.t. X is independent if |X ∩ {v1 i  v2 i}| ≤ 1
for i = 1  2  . . .   t  where we label V1 = {v1 i}t
i=1. The rank of this matroid is
therefore k = 2t. We repeat the experiment on 30 random DSFs. For each DSF  we maximize it by
each algorithm and take the average function value  respectively. Figure 3(b) shows the performance
of our method compared to the combinatorial greedy algorithm using the lazy evaluation trick [30].
We see that our method offers a solution that is consistently better than the standard greedy for all
k. Regarding running time  we ﬁnd that while our method is slower than lazy greedy for small k  it
becomes faster than lazy greedy for large k (Figure 3c). For a fair comparison  both algorithms were
implemented in Python and run on a single CPU. We anticipate that our method will run even faster
on parallel GPU machines  which can be accomplished easily using any modern DNN toolkit (e.g. 
PyTorch [33] or TensorFlow [1]).
Acknowledgments: This material is based upon work supported by the National Science Foundation
under Grant No. IIS-1162606  the National Institutes of Health under award R01GM103544  and by
a Google  a Microsoft  and an Intel research award. This research is also supported by the CONIX
Research Center  one of six centers in JUMP  a Semiconductor Research Corporation (SRC) program
sponsored by DARPA.

i=1 and V1 = {v2 i}t

i

8

n=|V|V(1)V(2)V(3)V(4)101102103104k020406080100120140160performance projected gradient ascentgreedy101102103104k102101100101102103104105running timeprojected gradient ascentgreedyReferences
[1] Martín Abadi  Ashish Agarwal  Paul Barham  Eugene Brevdo  Zhifeng Chen  Craig Citro 
Greg S. Corrado  Andy Davis  Jeffrey Dean  Matthieu Devin  Sanjay Ghemawat  Ian Goodfellow 
Andrew Harp  Geoffrey Irving  Michael Isard  Yangqing Jia  Rafal Jozefowicz  Lukasz Kaiser 
Manjunath Kudlur  Josh Levenberg  Dandelion Mané  Rajat Monga  Sherry Moore  Derek
Murray  Chris Olah  Mike Schuster  Jonathon Shlens  Benoit Steiner  Ilya Sutskever  Kunal
Talwar  Paul Tucker  Vincent Vanhoucke  Vijay Vasudevan  Fernanda Viégas  Oriol Vinyals  Pete
Warden  Martin Wattenberg  Martin Wicke  Yuan Yu  and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems  2015. Software available from tensorﬂow.org.

[2] Alexander A Ageev and Maxim I Sviridenko. Pipage rounding: A new method of construct-
ing algorithms with proven performance guarantee. Journal of Combinatorial Optimization 
8(3):307–328  2004.

[3] Jeffrey Bilmes and Wenruo Bai. Deep Submodular Functions. Arxiv  abs/1701.08939  Jan 2017.

[4] Sébastien Bubeck et al. Convex optimization: Algorithms and complexity. Foundations and

Trends R(cid:13) in Machine Learning  8(3-4):231–357  2015.

[5] G. Calinescu  C. Chekuri  M. Pál  and J. Vondrák. Maximizing a monotone submodular function

subject to a matroid constraint. SIAM Journal on Computing  40(6):1740–1766  2011.

[6] Gruia Calinescu  Chandra Chekuri  Martin Pál  and Jan Vondrák. Maximizing a submodular set
function subject to a matroid constraint. In International Conference on Integer Programming
and Combinatorial Optimization  pages 182–196. Springer  2007.

[7] Deeparnab Chakrabarty  Yin Tat Lee  Aaron Sidford  and Sam Chiu-wai Wong. Subquadratic

submodular function minimization. arXiv preprint arXiv:1610.09800  2016.

[8] Chandra Chekuri and Amit Kumar. Maximum coverage problem with group budget con-
straints and applications. In Approximation  Randomization  and Combinatorial Optimization.
Algorithms and Techniques  pages 72–83. Springer  2004.

[9] Chandra Chekuri  Jan Vondrák  and Rico Zenklusen. Dependent randomized rounding for

matroid polytopes and applications. arXiv preprint arXiv:0909.4348  2009.

[10] Gerard Cornuejols  Marshall L Fisher  and George L Nemhauser. Exceptional paper location
of bank accounts to optimize ﬂoat: An analytic study of exact and approximate algorithms.
Management science  23(8):789–810  1977.

[11] W.H. Cunningham. On submodular function minimization. Combinatorica  5(3):185–192 

1985.

[12] J. Djolonga and A. Krause. From MAP to Marginals: Variational Inference in Bayesian Sub-
modular Models. In Neural Information Processing Society (NIPS)  Montreal  CA  December
2014.

[13] Brian W Dolhansky and Jeff A Bilmes. Deep submodular functions: Deﬁnitions and learning.

In Advances in Neural Information Processing Systems  pages 3396–3404  2016.

[14] Shaddin Dughmi. Submodular functions: Extensions  distributions  and algorithms. a survey.

arXiv preprint arXiv:0912.0322  2009.

[15] S. Fujishige. Submodular functions and optimization  volume 58. Elsevier Science  2005.

[16] Toshihiro Fujito. Approximation algorithms for submodular set cover with applications. IEICE

Transactions on Information and Systems  83(3):480–487  2000.

[17] Michael Gygli  Helmut Grabner  and Luc Van Gool. Video summarization by learning submod-

ular mixtures of objectives. In Proceedings CVPR 2015  pages 3090–3098  2015.

[18] Hamed Hassani  Mahdi Soltanolkotabi  and Amin Karbasi. Gradient methods for submodular
maximization. In Advances in Neural Information Processing Systems  pages 5841–5851  2017.

9

[19] Stefanie Jegelka and Jeff Bilmes. Submodularity beyond submodular energies: coupling edges
in graph cuts. In Computer Vision and Pattern Recognition (CVPR)  2011 IEEE Conference on 
pages 1897–1904. IEEE  2011.

[20] Mohammad Karimi  Mario Lucic  Hamed Hassani  and Andreas Krause. Stochastic submodular
maximization: The case of coverage functions. In Advances in Neural Information Processing
Systems  pages 6856–6866  2017.

[21] Katrin Kirchhoff and Jeff Bilmes. Submodularity for data selection in machine translation. In
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
(EMNLP)  pages 131–141  2014.

[22] Pushmeet Kohli  M Pawan Kumar  and Philip HS Torr. P3 & beyond: Move making algo-
rithms for solving higher order functions. Pattern Analysis and Machine Intelligence  IEEE
Transactions on  31(9):1645–1656  2009.

[23] Andreas Krause  Carlos Guestrin  Anupam Gupta  and Jon Kleinberg. Near-optimal sensor
placements: Maximizing information while minimizing communication cost. In Proceedings
of the 5th international conference on Information processing in sensor networks  pages 2–10.
ACM  2006.

[24] Yin Tat Lee  Aaron Sidford  and Sam Chiu-wai Wong. A faster cutting plane method and its
implications for combinatorial and convex optimization. In Foundations of Computer Science
(FOCS)  2015 IEEE 56th Annual Symposium on  pages 1049–1065. IEEE  2015.

[25] Maxwell W. Libbrecht  Jeffrey A. Bilmes  and William Stafford Noble. Choosing non-redundant
representative subsets of protein sequence data sets using submodular optimization. Proteins:
Structure  Function  and Bioinformatics  2018.

[26] H. Lin  J. Bilmes  and S. Xie. Graph-based submodular selection for extractive summarization.

In ASRU  2009.

[27] Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies-Volume 1  pages 510–520. Association for Computational
Linguistics  2011.

[28] Hui Lin and Jeff Bilmes. Word alignment via submodular maximization over matroids. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:
Human Language Technologies: short papers-Volume 2  pages 170–175. Association for
Computational Linguistics  2011.

[29] László Lovász. Submodular functions and convexity. In Mathematical Programming The State

of the Art  pages 235–257. Springer  1983.

[30] M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Opti-

mization Techniques  pages 234–243  1978.

[31] Aryan Mokhtari  Hamed Hassani  and Amin Karbasi. Conditional gradient method for stochas-
In International Conference on Artiﬁcial

tic submodular maximization: Closing the gap.
Intelligence and Statistics  pages 1886–1895  2018.

[32] George L Nemhauser  Laurence A Wolsey  and Marshall L Fisher. An analysis of approximations
for maximizing submodular set functions—i. Mathematical Programming  14(1):265–294 
1978.

[33] Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
pytorch. 2017.

[34] Prabhakar Raghavan. Probabilistic construction of deterministic algorithms: approximating

packing integer programs. Journal of Computer and System Sciences  37(2):130–143  1988.

[35] Tasuku Soma and Yuichi Yoshida. A generalization of submodular cover via the diminishing
return property on the integer lattice. In Advances in Neural Information Processing Systems 
pages 847–855  2015.

10

[36] P. Stobbe and A. Krause. Efﬁcient minimization of decomposable submodular functions. In

NIPS  2010.

[37] Maxim Sviridenko  Jan Vondrák  and Justin Ward. Optimal approximation for submodular and
supermodular optimization with bounded curvature. In Proceedings of the Twenty-Sixth Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 1134–1148. Society for Industrial and
Applied Mathematics  2015.

[38] Adrian Vetta. Nash equilibria in competitive societies  with applications to facility location 
trafﬁc routing and auctions. In Foundations of Computer Science  2002. Proceedings. The 43rd
Annual IEEE Symposium on  pages 416–425. IEEE  2002.

[39] Kai Wei  Rishabh Iyer  and Jeff Bilmes. Submodularity in data subset selection and active

learning. In International Conference on Machine Learning (ICML)  Lille  France  2015.

[40] Kai Wei  Maxwell W Libbrecht  Jeffrey A Bilmes  and William Noble. Choosing panels of

genomics assays using submodular optimization (tr). bioRxiv  2016.

[41] Kai Wei  Yuzong Liu  Katrin Kirchhoff  Chris Bartels  and Jeff Bilmes. Submodular subset

selection for large-scale speech training data. Proceedings of ICASSP  Florence  Italy  2014.

11

,Wenruo Bai
William Stafford Noble
Jeff Bilmes
Mingrui Zhang
Lin Chen
Hamed Hassani
Amin Karbasi