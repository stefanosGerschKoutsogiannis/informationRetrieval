2016,Dual Space Gradient Descent for Online Learning,One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal  projection  or merging strategies. Although projection and merging  in the literature  are known to be the most effective strategies  they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space  hence effectively resolve the curse of kernelization. However  this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently  it leads to a significant increase in the computational cost. To address all of these aforementioned challenges  we present in this paper the Dual Space Gradient Descent (DualSGD)  a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently  our approach permits the budget to be maintained in a simple  direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.,Dual Space Gradient Descent for Online Learning

Trung Le  Tu Dinh Nguyen  Vu Nguyen  Dinh Phung

Centre for Pattern Recognition and Data Analytics

{trung.l  tu.nguyen  v.nguyen  dinh.phung}@deakin.edu.au

Deakin University  Australia

Abstract

One crucial goal in kernel online learning is to bound the model size. Common
approaches employ budget maintenance procedures to restrict the model sizes using
removal  projection  or merging strategies. Although projection and merging  in the
literature  are known to be the most effective strategies  they demand extensive com-
putation whilst removal strategy fails to retain information of the removed vectors.
An alternative way to address the model size problem is to apply random features
to approximate the kernel function. This allows the model to be maintained directly
in the random feature space  hence effectively resolve the curse of kernelization.
However  this approach still suffers from a serious shortcoming as it needs to use a
high dimensional random feature space to achieve a sufﬁciently accurate kernel
approximation. Consequently  it leads to a signiﬁcant increase in the computational
cost. To address all of these aforementioned challenges  we present in this paper
the Dual Space Gradient Descent (DualSGD)  a novel framework that utilizes
random features as an auxiliary space to maintain information from data points
removed during budget maintenance. Consequently  our approach permits the
budget to be maintained in a simple  direct and elegant way while simultaneously
mitigating the impact of the dimensionality issue on learning performance. We
further provide convergence analysis and extensively conduct experiments on ﬁve
real-world datasets to demonstrate the predictive performance and scalability of
our proposed method in comparison with the state-of-the-art baselines.

Introduction

1
Online learning represents a family of effective and scalable learning algorithms for incrementally
building a predictive model from a sequence of data samples [1]. Unlike the conventional learning
algorithms  which usually require a costly procedure to retrain the entire dataset when a new instance
arrives [2]  the goal of online learning is to utilize new incoming instances to improve the model
given knowledge of the correct answers to previously processed data. The seminal line of work in
online learning  referred to as linear online learning [3  4]  aims to learn a linear predictor in the
input space. The key limitation of this approach lies in its oversimpliﬁed assumption in using a linear
hyperplane to represent data that could possibly possess nonlinear dependency as commonly seen
in many real-world applications. This inspires the work of kernel online learning [5  6] that uses a
linear model in the feature space to capture the nonlinearity of input data.
However  the kernel online learning approach suffers from the so-called curse of kernelization [7] 
that is  the model size linearly grows with the data size accumulated over time. A notable approach
to address this issue is to use a budget [8  9  7  10  11]. The work in [7] leveraged the budgeted
approach with stochastic gradient descent (SGD) [12  13] wherein the learning procedure employed
SGD and a budget maintenance procedure (e.g.  removal  projection  or merging) was employed to
maintain the model size. Although the projection and merging were shown to be effective [7]  their
associated computational costs render them impractical for large-scale datasets. An alternative way
to address the curse of kernelization is to use random features [14] to approximate a kernel function

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain
Acknowledgment: This work is partially supported by the Australian Research Council under the Discovery
Project DP160109394.

[15  16]. The work in [16] proposed to transform data from the input space to the random-feature
space  and then performed SGD in the feature space. However  in order for this approach to achieve
good kernel approximation  excessive number of random features is required  hence could lead to
serious computational issue.
In this paper  we propose the Dual Space Gradient Descent (DualSGD) to address the computational
problem encountered in the projection and merging strategies in the budgeted approach [8  9  17  7]
and the excessive number of random features in the random feature approach [15  16]. In particular 
the proposed DualSGD utilizes the random-feature space as an auxiliary space to store the information
of the vectors that have been discarded during the budget maintenance process. More speciﬁcally  the
DualSGD uses a provision vector in the random-feature space to store the information of all vectors
being removed. This allows us to propose a novel budget maintenance strategy  named k-merging 
which uniﬁes the removal  projection  and merging strategies.

Figure 1: Comparison of DualSGD with BSGD-M and FOGD on the cod-rna dataset. Left: DualSGD
vs. BSGD-M when B is varied. Right: DualSGD vs. FOGD when D is varied.
Our proposed DualSGD advances the existing works in the budgeted and random-feature approaches
in twofold. Firstly  since the goal of using random features is to approximate the original feature
space as much as possible  the proposed k-merging of DualSGD can preserve the information of
the removed vectors more effectively than the existing budget maintenance strategies. For example
comparing with the budgeted SGD using merging strategy (BSGD-M) [7]  as shown in Fig. 1 (left) 
the DualSGD with a small budget size (B = 5) can gain a signiﬁcant better mistake rate than that of
BSGD-M with a 80-fold larger budget size (B = 400). Secondly  since the core part of the model
(i.e.  the vectors in the support set) is stored in the feature space and the auxiliary part (i.e.  the
removed vectors) is stored in the random-feature space  our DualSGD can signiﬁcantly reduce the
inﬂuence of the number of random features to the learning performance. For example comparing
with the Fourier Online Gradient Descent (FOGD) [16]  as shown in Fig. 1 (right)  the DualSGD
with a small number of random features (D = 20) can achieve a comparable mistake rate to that of
FOGD with a 40-fold larger number of random features (D = 800) and the DualSGD with a medium
value of number of random features (D = 100) achieves a predictive performance that would not
be reached by FOGD (the detail of comparison in computational complexities of our DualSGD and
FOGD can be found in Section 3 in the supplementary material).
To provide theoretical foundation for DualSGD  we develop an extensive convergence analysis for a
wide spectrum of loss functions including Hinge  Logistic  and smooth Hinge [18] for classiﬁcation
task and (cid:96)1  ε-insensitive for regression. We conduct extensive experiments on ﬁve real-world datasets
to compare the proposed method with the state-of-the-art online learning methods. The experimental
results show that our proposed DualSGD achieves the most optimal predictive results in almost all
cases  whilst its execution time is much faster than the baselines.
2 Dual Space Gradient Descent for Online Learning
2.1 Problem Setting
We propose to solve the following optimization problem: min
w
deﬁned for online setting as follows:
J (w) ≡ λ
2

(cid:107)w(cid:107)2 + E(x y)∼pX  Y [l (w  x  y)]

J (w) whose objective function is

(1)

2

where x ∈ RM is the data vector  y the label  pX  Y denotes the joint distribution over X × Y with
the data domain X and label domain Y  l (w  x  y) is a convex loss function with parameters w 
and λ ≥ 0 is a regularization parameter. A kernelization of the loss function introduces a nonlinear
function Φ that maps x from the input space to a feature space. A classic example is the Hinge loss:

l (w  x  y) = max(cid:0)0  1 − yw(cid:62)Φ (x)(cid:1).

2.2 The Key Ideas of the Proposed DualSGD
Our key motivations come from the shortcomings of three current budget maintenance strategies:
removal  projection and merging. The removal strategy fails to retain information of the removed
vectors. Although the projection strategy can overcome this problem  it requires a costly procedure
to compute the inverse of an B × B matrix wherein B is the budget size  typically in the cubic
complexity of B. On the other hand  the merging strategy needs to estimate the preimage of a vector
in the feature space  leading to a signiﬁcant information loss and requiring extensive computation.
Our aim is to ﬁnd an approach to simultaneously retain the information of the removed vectors
accurately  and perform budget maintenance efﬁciently.
To this end  we introduce the k-merging  a new budget maintenance approach that uniﬁes three
aforementioned budget maintenance strategies under the following interpretation. For k = 1  the
proposed k-merging can be seen as a hybrid strategy of removal and projection. For k = 2  it
can be regarded as the standard merging. Moreover  our proposed k-merging strategy enables an
arbitrary number of vectors to be conveniently merged. Technically  we employ a vector in the
random-feature space [14]  called provision vector ˜w  to retain the information of all removed vectors.
When k-merging is invoked  the most redundant k vectors are sorted out  e.g.  xi1  . . .   xik and we

(cid:1) where αij is the coefﬁcient of support vector associated
(cid:1) denotes the mapping function from the input space to the random feature space.

increment ˜w as ˜w = ˜w +(cid:80)k
with xij   and z(cid:0)xij

j=1 αij z(cid:0)xij

(xt  yt) ∼ pX  Y
ˆwt+1 = t−1

The advantage of using the random-feature space as an auxiliary space is twofold: 1) the information
loss is negligible since the random-feature space is designed to approximate the original feature space 
and 2) the operations in budget maintenance strategy are direct and economic.
Algorithm 1 The learning of Dual Space Gradient Descent.
Input: Kernel K  regularization parameter λ  budget B  random feature dimension D.
1: ˆw1 = 0; ˜w1 = 0; b = 0; I0 = ∅
2: for t = 1  . . .   T do
3:
4:
5:
6:
7:
8:
9:
10:
end if
11:
12: end for

It = It−1∪ {t}
ˆwt+1 = ˆwt+1 − 1
if |It| > B then
end if

λt∇ol(cid:0)yt  oh

if ∇ol(cid:0)yt  oh

invokes k-merging(It  ˆwt+1  ˜wt+1)

t ˆwt; ˜wt+1 = t−1

(cid:1) (cid:54)= 0 then

(cid:1) Φ (xt)

t

t ˜wt

t

Output: wh

T +1 = ˆwT +1 ⊕ ˜wT +1 .

2.3 The Proposed Algorithm
In our proposed DualSGD  the model is distributed into two spaces: the feature and random-feature
(cid:44) ˆwt ⊕ ˜wt. Here we note that the kernel part ˆwt and
spaces with a hybrid vector wh
the provision part ˜wt lie in two different spaces  thus for convenience we deﬁne an abstract operator
⊕ to allow the addition between them  which implies that the decision function crucially depends on
both kernel and provision parts

t deﬁned as: wh
t

(cid:10)wh
t   x(cid:11) (cid:44) (cid:104)( ˆwt ⊕ ˜wt)   x(cid:105) (cid:44) ˆwT

t Φ (x) + ˜w(cid:62)

t z (x)

We employ one vector ˜wt in the random-feature space to preserve the information of the discarded vec-
tors  that are outside It – the set of indices of all support vectors in ˆwt. When an instance arrives and
the model size exceeds the budget B  the budget maintenance procedure k-merging(It  ˆwt+1  ˜wt+1)
is invoked to adjust ˆwt+1 and ˜wt+1  accordingly. Our proposed DualSGD is summarized in Algo-
rithm 1 where we note that  l (y  o) is another representation of convex loss function w.r.t the variable

3

o (e.g.  the Hinge loss given by l (y  o) = max (0  1 − yo))  and oh
hybrid objective value).
2.4 k-merging Budget Maintenance Strategy
Crucial to our proposed DualSGD in Algorithm 1 is the k-merging routine to allow efﬁcient merging
of k arbitrary vectors. We summarize the key steps for k-merging in Algorithm 2. In particular  we
ﬁrst select k support vectors whose corresponding coefﬁcients (αi1  αi2  ...  αik) have the smallest
absolute values (cf. line 1). We then approximate them by z (xi1 )   . . .   z (xik ) and merge them by

updating the provision vector as ˜wt+1 = ˜wt+1 +(cid:80)k

(cid:1) (cf. line 2). Finally  we remove

j=1 αij z(cid:0)xij

t Φ (x) + ˜w(cid:62)

t = ˆw(cid:62)

t z (x) (i.e. 

J (w). We then prove that if {wt}∞

the chosen vectors from the kernel part ˆwt+1 (cf. line 2).
2.5 Convergence Analysis
In this section  we present the convergence analysis for our proposed algorithm. We ﬁrst prove that
with a high probability f h
t (x) (i.e.  hybrid decision function and cf. 3) is a good approximation of
ft (x) for all x and t (cf. Theorem 1). Let w(cid:63) be the optimal solution of the optimization problem
deﬁned in Eq. (1): w(cid:63) = argmin
t=1 is constructed as in Eq. (2) 
this sequence rapidly converges to w(cid:63) or ft (x) = w(cid:62)
t Φ (x) rapidly approaches the optimal decision
function (cf. Theorems 2  3). Therefore  the decision function f h
t (x) also rapidly approaches the
optimal decision function. Our analysis can be generalized for the general k-merging strategy  but for
comprehensibility we present the analysis for the 1-merging case (i.e.  k = 1).
We assume that the loss function used in the analysis satisﬁes the condition |∇ol (y  o)| ≤ A  ∀y  o 
where A is a positive constant. A wide spectrum of loss functions including Hinge  logistic  smooth
Hinge [18]  (cid:96)1  and ε-insensitive satisfy this condition and hence are appropriate for this convergence
analysis. We further assume that (cid:107)Φ (x)(cid:107) = K (x  x)1/2 = 1  ∀x. Let βt be a binary random
variable which indicates whether the budget maintenance procedure is performed at the iteration t

(cid:1) (cid:54)= 0). We assume that if βt = 1  the vector Φ (xit) is selected to move to

(i.e.  the event ∇ol(cid:0)yt  oh

the random-feature space. Without loss of generality  we assume that it = t since we can arrange the
data instances so as to realize it. We deﬁne

w

t

t = λwt + ∇ol(cid:0)yt  f h

gh

ft (x) = w(cid:62)

t(cid:88)

t (xt)(cid:1) Φ (xt) and wt+1 = wt − ηtgh
t(cid:88)
t(cid:88)

αj (1 − βj) K (xj  x) +

αjK (xj  x)

t Φ (x) =

j=1

t

t (x) = ˆw(cid:62)
f h

t Φ (xt) + ˜w(cid:62)

t z (xt) =

αjβj ˜K (xj  x)

j=1

j=1

(cid:62)
where ˜K (x  x(cid:48)) = z (x)
z (x(cid:48)) is the approximated kernel induced by the random-feature space 
λt.
and the learning rate ηt = 1
Theorem 1 establishes that f h
t (.) is a good approximation of ft (x) with a high probability  followed
by Theorem 2 which establishes the bound on the regret.

(2)

(3)

(cid:1);

endprod

j∈It
|αj|;

It = It\ {i1  . . .   ik}

1: (i1  . . .   ik) =k-argmin
j∈It

j=1 αij z(cid:0)xij

Algorithm 2 k-merging Budget Maintenance Procedure.

procedure k-merging(It  ˆwt+1  ˜wt+1)
αjΦ (xj)

// Assume that ˆwt+1 =(cid:80)
ˆwt+1 = ˆwt+1 −(cid:80)k
2: ˜wt+1 = ˜wt+1 +(cid:80)k
Theorem 1. With a probability at least 1 − θ = 1 − 28(cid:16) σµAdX
i)(cid:12)(cid:12)ft (x) − f h
ii) E(cid:2)(cid:12)(cid:12)ft (x) − f h

t (x)(cid:12)(cid:12) ≤ ε for all t > 0 and x ∈ X .
t (x)(cid:12)(cid:12)(cid:3) ≤ A−1λε(cid:80)t
E(cid:2)α2
(cid:3)1/2 µ

1/2

j

j=1

j=1 αij Φ(cid:0)xij
(cid:17)

(cid:1)
(cid:16)− Dλ2ε2

(cid:17)

where M is
exp
the dimension of input space  D is the dimension of random feature space dX denotes the diameter of
the compact set X   and the constant σµ is deﬁned as in [14]  we have

4(M +2)A2

λε

j where µj = p (βj = 1).

4

It also indicates that to decrease the gap(cid:12)(cid:12)ft (x) − f h

Theorem 1 shows that with a high probability f h

we should choose the vectors whose coefﬁcients have smallest absolute values to move to the
random-feature space.
Theorem 2. The following statement guarantees for all T
E [J (wT )] − J (w(cid:63)) ≤ E

≤ 8A2 (log T + 1)

J (wt) − J (w(cid:63))

T(cid:88)

(cid:34)

W

+

t (x) can approximate ft (x) with an ε-precision.

t (x)(cid:12)(cid:12)  when performing budget maintenance 
(cid:35)
(cid:3)1/2
E(cid:2)M 2
5(cid:1) λ−1.

T(cid:88)
t (xt)(cid:1)  and W = 2A(cid:0)1 +

√

1
T

λT

t=1

t

If a smooth loss function is used  we can quantify the gap in more detail and with a high probability 
the gap is negligible and this is shown in Theorem 3.
Theorem 3. Assume that l (y  o) is a γ-strongly smooth loss function. With a probability at least

1
T

t=1

t=1 wt  Mt = ∇ol (yt  ft (xt))−∇ol(cid:0)yt  f h
(cid:80)T
(cid:16)− Dλ2ε2
(cid:17)
(cid:34)
T(cid:88)

  we have

4(M +2)A2

(cid:17)

(cid:35)

exp

J (wt) − J (w(cid:63))

where wT = 1
T

1 − 28(cid:16) σµAdX

λε

T(cid:88)

t=1

+

1
T

W γε

(cid:32)(cid:80)t

(cid:33)1/2

j=1 µj
t

E [J (wT )] − J (w(cid:63)) ≤ E

1
T

t=1

≤ 8A2 (log T + 1)

λT

≤ 8A2 (log T + 1)

λT

+ W γε

3 Experiments
In this section  we conduct comprehensive experiments to quantitatively evaluate the performance
of our proposed Dual Space Gradient Descent (DualSGD) on binary classiﬁcation  multiclass clas-
siﬁcation and regression tasks under online settings. Our main goal is to examine the scalability 
classiﬁcation and regression capabilities of DualSGDs by directly comparing them with those of
several recent state-of-the-art online learning approaches using a number of real-world datasets with
a wide range of sizes. In what follows  we present the data statistics  experimental setup  results and
our observations.
3.1 Data Statistics and Experimental Setup
We use 5 datasets which are ijcnn1  cod-rna  poker  year  and airlines. The datasets where purposely
are selected with various sizes in order to clearly expose the differences among scalable capabilities
of the models. Three of which are large-scale datasets with hundreds of thousands and millions of
data points (year: 515  345; poker: 1  025  010; and airlines: 5  929  413)  whilst the rest are medium
size databases (ijcnn1: 141  691 and cod-rna: 331  152). These datasets can be downloaded from
LIBSVM1 and UCI2 websites  except the airlines which was obtained from American Statistical
Association (ASA3). For the airlines dataset  our aim is to predict whether a ﬂight will be delayed or
not under binary classiﬁcation setting  and how long (in minutes) the ﬂight will be delayed in terms
of departure time under regression setting. A ﬂight is considered delayed if its delay time is above
15 minutes  and non-delayed otherwise. Following the procedure in [19]  we extract 8 features for
ﬂights in the year of 2008  and then normalize them into the range [0 1].
For each dataset  we perform 10 runs on each algorithm with different random permutations of the
training data samples. In each run  the model is trained in a single pass through the data. Its prediction
result and time spent are then reported by taking the average together with the standard deviation over
all runs. For comparison  we employ 11 state-of-the-art online kernel learning methods: perceptron
[5]  online gradient descent (OGD) [6]  randomized budget perceptron (RBP) [9]  forgetron [8]
projectron  projectron++ [20]  budgeted passive-aggressive simple (BPAS) [17]  budgeted SGD using
merging strategy (BSGD-M) [7]  bounded OGD (BOGD) [21]  Fourier OGD (FOGD) and Nystrom
OGD (NOGD) [16]. Their implementations are published as a part of LIBSVM  BudgetedSVM4 and
LSOKL5 toolboxes. We use a Windows machine with 3.46GHz Xeon processor and 96GB RAM to
conduct our experiments.

1https://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/
2https://archive.ics.uci.edu/ml/datasets.html
3http://stat-computing.org/dataexpo/2009/.
4http://www.dabi.temple.edu/budgetedsvm/index.html
5http://lsokl.stevenhoi.com/

5

3.2 Model Evaluation on the Effect of Hyperparameters
In the ﬁrst experiment  we investigate the effect of hyperparameters  i.e.  budget size B  merging
size k and random feature dimension D (cf. Section 2) on the performance behavior of DualSGD.
Particularly  we conduct an initial analysis to quantitatively evaluate the sensitivity of these hyperpa-
rameters and their impact on the predictive accuracy and wall-clock time. This analysis provides an
approach to ﬁnd the best setting of hyperparameters. Here the DualSGD with Hinge loss is trained
on the cod-rna dataset under the online classiﬁcation setting.

Figure 2: The effect of k-merging size on the mistake rate and running time (left). The effect of
budget size B and random feature dimension D on the mistake rate (middle) and running time (right).
First we set B = 200  D = 100  and vary k in the range of 1  2  10  20  50  100  150. For each
setting  we run our models and record the average mistake rates and running time as shown in Fig. 2
(left). There is a pattern that the classiﬁcation error increases for larger k whilst the wall-clock
time decreases. This represents the trade-off between model discriminative performance and model
computational complexity via the number of merging vectors. In this analysis  we can choose k = 20
to balance the performance and computational cost.
Fixing k = 20  we vary B and D in 4 values doubly increasing from 50 to 400 and from 100 to 800 
respectively  to evaluate the prediction performance and execution time. Fig. 2 depicts the average
mistake rates (middle) and running time in seconds (right) as a heat map of these values. These
visualizations indicate that the higher B and D produce better classiﬁcation results  but hurt the
training speed of the model. We found that increasing the dimension of random feature space from
100 to 800 at B = 50 signiﬁcantly reduces the mistake rates by 25%  at the same time increases the
wall-clock time by 76%. The same pattern with less effect is observed when increasing the budget
size B from 50 to 400 at D = 100 (mistake rate decreases by 1.5%  time increases by 54%). For a
good trade-off between classiﬁcation performance and computational cost  we select B = 100 and
D = 200 which achieves fairly comparable classiﬁcation result and running time.
3.3 Online Classiﬁcation
We now examine the performances of DualSGDs in the online classiﬁcation task. We use four
datasets: cod-rna  ijcnn1  poker and airlines (delayed and non-delayed labels). We create two
versions of our approach: DualSGD with Hinge loss (DualSGD-Hinge) and DualSGD with Logistic
loss (DualSGD-Logit). It is worth mentioning that the Hinge loss is not a smooth function with
undeﬁned gradient at the point that the classiﬁcation conﬁdence yf (x) = 1. Following the sub-
gradient deﬁnition  in our experiment  we compute the gradient given the condition that yf (x) < 1 
and set it to 0 otherwise.
Hyperparameters setting. There are a number of different hyperparameters for all methods. Each
method requires a different set of hyperparameters  e.g.  the regularization parameters (λ in DualSGD) 
the learning rates (η in FOGD and NOGD)  and the RBF kernel width (γ in all methods). Thus  for a
fair comparison  these hyperparameters are speciﬁed using cross-validation on a subset of data.
In particular  we further partition the training set into 80% for learning and 20% for valida-
tion. For large-scale databases  we use only 1% of dataset  so that the searching can ﬁn-
ish within an acceptable time budget. The hyperparameters are varied in certain ranges and
selected for the best performance on the validation set. The ranges are given as follows:
C ∈{2−5  2−3  ...  215}  λ ∈{2−4/N  2−2/N  ...  216/N}  γ ∈{2−8  2−4  2−2  20  22  24  28}  and
η ∈{2−4  2−3  ...  2−1  21  22...  24} where N is the number of data points. The budget size B 
merging size k and random feature dimension D of DualSGD are selected following the approach
described in Section 3.2. For the budget size ˆB in NOGD and Pegasos algorithm  and the feature
dimension ˆD in FOGD for each dataset  we use identical values to those used in Section 7.1.1 of [16].

6

Table 1: Mistake rate (%) and execution time (seconds). The notation [k; B; D; ˆB; ˆD] denotes the
merging size k  the budget sizes B and ˆB of DualSGD-based models and other budgeted algorithms 
and the number of random features D and ˆD of DualSGD and FOGD  respectively.

cod-rna

ijcnn1

[20 | 100 | 200 | 400 | 1  600]
Mistake Rate
9.79±0.04
7.81±0.03
26.02±0.39
28.56±2.22
11.16±3.61
17.97±15.60
11.97±0.09
5.33±0.04
38.13±0.11
7.15±0.03
7.83±0.06
4.92±0.25
4.83±0.21

1 393.56
2 804.01
85.84
102.64
97.38
1 799.93
92.08
184.58
104.60
53.45
105.18
28.29
31.96

[20 | 100 | 200 | 1  000 | 4  000]
Time
727.90
960.44
54.29
60.54
59.37
749.70
55.44
1 562.61
55.99
25.93
59.36
12.12
13.30

Time Mistake Rate
12.85±0.09
10.39±0.06
15.54±0.21
16.17±0.26
12.98±0.23
9.97±0.09
10.68±0.05
9.14±0.18
10.87±0.18
9.41±0.03
10.43±0.08
8.35±0.20
8.82±0.24

poker

airlines

[20 | 100 | 200 | 1  000 | 4  000]
Mistake Rate
52.28±0.04
44.90±0.16
46.73±0.22
46.65±0.14

[20 | 100 | 200 | 1  000 | 4  000]
Time
1 270.75
3 553.50
472.21
523.23

Time Mistake Rate
20.98±0.01
928.89
25.56±0.01
4 920.33
19.28±0.00
139.87
19.28±0.00
133.50

(cid:104)

Dataset

k | B | D | ˆB | ˆD

(cid:105)

Algorithm
Perceptron

OGD
RBP

Forgetron
Projectron
Projectron++

BPAS

BSGD-M
BOGD
FOGD
NOGD

(cid:104)

DualSGD-Hinge
DualSGD-Logit

Dataset [S]

k | B | D | ˆB | ˆD

(cid:105)

Algorithm

FOGD
NOGD

DualSGD-Hinge
DualSGD-Logit

Results. Table 1 reports the average classiﬁcation results and execution time after the methods see
all data samples. Note that for two biggest datasets (poker  airlines) that consist of millions of data
points  we only include the fast algorithms FOGD  NOGD and DualSGDs. The other methods would
exceed the time limit  which we set to two hours  when running on such data as they suffer from
serious computation issue. From these results  we can draw key observations below.
The budgeted online approaches show their effectiveness with substantially faster computation than
the ones without budgets. More speciﬁcally  the execution time of our proposed models is several
orders of magnitude (100 times) lower than that of regular online algorithms (e.g.  28.29 seconds
compared with 2  804 seconds for cod-rna dataset). Moreover  our models are twice as fast as the
recent fast algorithm FOGD for cod-rna and ijcnn1 datasets  and approximately eight and three times
for vast-sized data poker and airlines. This is because the DualSGDs maintain a sparse budget of
support vectors and a low random feature space  whose size and dimensionality are 10 times and 20
times smaller than those of other methods.
Second  in terms of classiﬁcation  the DualSGD-Hinge and DualSGD-Logit outperform other meth-
ods for almost all datasets except the poker data. In particular  the DualSGD-based methods achieve
the best mistake rates 4.83±0.21  8.35±0.20  19.28±0.00 for the cod-rna  ijcnn1 and airlines data 
that are  respectively  32.4%  11.3%  8.8% lower than the error rates of the second best models –
two recent approaches FOGD and NOGD. For poker dataset  our methods obtain fairly comparable
results with that of the NOGD  but still surpass the FOGD with a large margin. The reason is that the
DualSGD uses a dual space: a kernel space containing core support vectors and a random feature
space keeping the projections of the core vectors that are removed from the budget in kernel space.
This would minimize the information loss when the model performs budget maintenance.
Finally  two versions of DualSGDs demonstrate similar discriminative performances and computa-
tional complexities wherein the DualSGD-Logit is slightly slower due to the additional exponential
operators. All of these observations validate the effectiveness and efﬁciency of our proposed tech-
nique. Thus  we believe that our approximation machine is a promising technique for building
scalable online kernel learning algorithms for large-scale classiﬁcation tasks.
3.4 Online Regression
The last experiment addresses the online regression problem to evaluate the capabilities of our
approach with two proposed loss functions: (cid:96)1 and ε-insensitive losses. Incorporating these loss
functions creates two versions: DualSGD-ε  DualSGD-(cid:96)1. We use two datasets: year and airlines
(delay minutes)  and six baselines: RBP  Forgetron  Projectron  BOGD  FOGD and NOGD.

7

Table 2: Root mean squared error (RMSE) and execution time (seconds) of 6 baselines and 2 versions
of our DualSGDs. The notation [k; B; D; ˆB; ˆD] denotes the same meaning as those in Table 1.

(cid:104)

Dataset

k | B | D | ˆB | ˆD

(cid:105)

year

[20 | 100 | 200 | 400 | 1  600]
Time
RMSE
0.19±0.00
605.42
0.19±0.00
904.09
0.14±0.00
605.19
0.20±0.00
596.10
0.16±0.00
76.70
0.14±0.00
607.37
0.13±0.00
48.01
0.12±0.00
47.29

airlines

RMSE

[20 | 100 | 200 | 1  000 | 2  000]
Time
36.51±0.00
3 418.89
36.51±0.00
5 774.47
36.14±0.00
3 834.19
35.73±0.00
3 058.96
53.16±0.01
646.15
34.74±0.00
3 324.38
36.20±0.01
457.30
36.20±0.01
443.39

Algorithm

RBP

Forgetron
Projectron

BOGD
FOGD
NOGD

DualSGD-ε
DualSGD-(cid:96)1

Hyperparameters setting. We adopt the same hyperparameter searching procedure for online
classiﬁcation task as in Section 3.3. Furthermore  for the budget size ˆB and the feature dimension
ˆD in FOGD  we follow the same strategy used in Section 7.1.1 of [16]. More speciﬁcally  these
hyperparameters are separately set for different datasets as reported in Table 2. They are chosen
such that they are roughly proportional to the number of support vectors produced by the batch SVM
algorithm in LIBSVM running on a small subset. The aim is to achieve competitive accuracy using a
relatively larger budget size for tackling more challenging regression tasks.
Results. Table 2 reports the average regression errors and computation costs after the methods see
all data samples. From these results  we can draw some observations below.
Our proposed models enjoy a signiﬁcant advantage in computational efﬁcacy whilst achieve better
(for year dataset) or competitive regression results (for airlines dataset) with other methods. The
DualSGD  again  secures the best performance in terms of model sparsity. Among the baselines  the
FOGD is the fastest  that is  its time costs can be considered to compare with those of our methods 
but its regression performances are worse. The remaining algorithms usually obtain better results  but
is paid by the sacriﬁce of scalability.
Finally  comparing the capability of two DualSGD’s variants  both models demonstrate similar
regression capabilities and computational complexities wherein the DualSGD-(cid:96)1 is slightly faster due
to its simpler operator in computing the gradient. Besides  its regression scores are also lower or equal
to those of DualSGD-ε. These observations  once again  veriﬁes the effectiveness and efﬁciency of
our proposed techniques. Therefore the DualSGD is also a promising machine to perform online
regression task for large-scale datasets.
4 Conclusion
In this paper  we have proposed Dual Space Gradient Descent (DualSGD) that overcomes the
computational problem in the projection and merging strategies in Budgeted SGD (BSGD) and the
excessive number of random features in Fourier Online Gradient Descent (FOGD). More speciﬁcally 
we have employed the random features to form an auxiliary space for storing the vectors being
removed during the budget maintenance process. This makes the operations in budget maintenance
simple and convenient. We have further presented the convergence analysis that is appropriate for a
wide spectrum of loss functions. Finally  we have conducted the extensive experiments on several
benchmark datasets to prove the efﬁciency and accuracy of the proposed method.

8

References
[1] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization

in the brain. Psychological Review  65(6):386–408  1958.

[2] C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM Trans. Intell.

Syst. Technol.  2(3):27:1–27:27  May 2011.

[3] K. Crammer  O. Dekel  J. Keshet  S. Shalev-Shwartz  and Y. Singer. Online passive-aggressive

algorithms. J. Mach. Learn. Res.  7:551–585  2006.

[4] M. Dredze  K. Crammer  and F. Pereira. Conﬁdence-weighted linear classiﬁcation. In Interna-

tional Conference on Machine Learning 2008  pages 264–271  2008.

[5] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Mach.

Learn.  37(3):277–296  December 1999.

[6] J. Kivinen  A. J. Smola  and R. C. Williamson. Online Learning with Kernels. IEEE Transactions

on Signal Processing  52:2165–2176  August 2004.

[7] Z. Wang  K. Crammer  and S. Vucetic. Breaking the curse of kernelization: Budgeted stochastic

gradient descent for large-scale svm training. J. Mach. Learn. Res.  13(1):3103–3131  2012.

[8] O. Dekel  S. Shalev-Shwartz  and Y. Singer. The forgetron: A kernel-based perceptron on a

ﬁxed budget. In Advances in Neural Information Processing Systems  pages 259–266  2005.

[9] G. Cavallanti  N. Cesa-Bianchi  and C. Gentile. Tracking the best hyperplane with a simple

budget perceptron. Machine Learning  69(2-3):143–167  2007.

[10] T. Le  V. Nguyen  T. D. Nguyen  and Dinh Phung. Nonparametric budgeted stochastic gradient
descent. In The 19th International Conference on Artiﬁcial Intelligence and Statistics  May
2016.

[11] T. Le  P. Duong  M. Dinh  T. D. Nguyen  V. Nguyen  and D. Phung. Budgeted semi-supervised
support vector machine. In The 32th Conference on Uncertainty in Artiﬁcial Intelligence  June
2016.

[12] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical

Statistics  22:400–407  1951.

[13] S. Shalev-shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver for

svm. In ICML 2007  pages 807–814  2007.

[14] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in

Neural Infomration Processing Systems  2007.

[15] L. Ming  W. Shifeng  and Z. Changshui. On the sample complexity of random fourier features
for online learning: How many random fourier features do we need? ACM Trans. Knowl.
Discov. Data  8(3):13:1–13:19  June 2014.

[16] J. Lu  S. C.H. Hoi  J. Wang  P. Zhao  and Z.-Y. Liu. Large scale online kernel learning. J. Mach.

Learn. Res.  2015.

[17] Z. Wang and S. Vucetic. Online passive-aggressive algorithms on a budget.

volume 9  pages 908–915  2010.

In AISTATS 

[18] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized

loss. Journal of Machine Learning Research  14(1):567–599  2013.

[19] J. Hensman  N. Fusi  and N. D Lawrence. Gaussian processes for big data. In Uncertainty in

Artiﬁcial Intelligence  pages 282–290  2013.

[20] F. Orabona  J. Keshet  and B. Caputo. Bounded kernel-based online learning. J. Mach. Learn.

Res.  10:2643–2666  December 2009.

[21] P. Zhao  J. Wang  P. Wu  R. Jin  and S. C. H. Hoi. Fast bounded online gradient descent

algorithms for scalable kernel-based online learning. CoRR  2012.

9

,Trung Le
Tu Nguyen
Vu Nguyen
Dinh Phung