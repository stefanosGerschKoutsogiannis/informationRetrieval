2019,Implicit Semantic Data Augmentation for Deep Networks,In this paper  we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping  translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features  such that certain directions in the deep feature space correspond to meaningful semantic transformations  e.g.  adding sunglasses or changing backgrounds. As a consequence  translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently  we first perform an online estimate of the covariance matrix of deep features for each class  which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly  instead of augmenting the samples explicitly  we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set  leading to a highly efficient algorithm. In fact  we show that the proposed ISDA amounts to minimizing a novel robust CE loss  which adds negligible extra computational cost to a normal training procedure. Although being simple  ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets  e.g.  CIFAR-10  CIFAR-100 and ImageNet. Code for reproducing our results are available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.,Implicit Semantic Data Augmentation for Deep

Networks

Yulin Wang1∗ Xuran Pan1∗ Shiji Song1 Hong Zhang2 Cheng Wu1 Gao Huang1†

1Department of Automation  Tsinghua University  Beijing  China

Beijing National Research Center for Information Science and Technology (BNRist) 

{yulin.bh  fykalviny}@gmail.com  pxr18@mails.tsinghua.edu.cn 

{shijis  wuc  gaohuang}@tsinghua.edu.cn

2Baidu Inc.  China

Abstract

In this paper  we propose a novel implicit semantic data augmentation (ISDA)
approach to complement traditional augmentation techniques like ﬂipping  trans-
lation or rotation. Our work is motivated by the intriguing property that deep
networks are surprisingly good at linearizing features  such that certain directions
in the deep feature space correspond to meaningful semantic transformations  e.g. 
adding sunglasses or changing backgrounds. As a consequence  translating train-
ing samples along many semantic directions in the feature space can effectively
augment the dataset to improve generalization. To implement this idea effectively
and efﬁciently  we ﬁrst perform an online estimate of the covariance matrix of
deep features for each class  which captures the intra-class semantic variations.
Then random vectors are drawn from a zero-mean normal distribution with the
estimated covariance to augment the training data in that class. Importantly  instead
of augmenting the samples explicitly  we can directly minimize an upper bound
of the expected cross-entropy (CE) loss on the augmented training set  leading to
a highly efﬁcient algorithm. In fact  we show that the proposed ISDA amounts
to minimizing a novel robust CE loss  which adds negligible extra computational
cost to a normal training procedure. Although being simple  ISDA consistently
improves the generalization performance of popular deep models (ResNets and
DenseNets) on a variety of datasets  e.g.  CIFAR-10  CIFAR-100 and ImageNet.
Code for reproducing our results is available at https://github.com/blackfeather-
wang/ISDA-for-Deep-Networks.

1

Introduction

Data augmentation is an effective technique to alleviate the overﬁtting problem in training deep
networks [1  2  3  4  5]. In the context of image recognition  this usually corresponds to applying
content preserving transformations  e.g.  cropping  horizontal mirroring  rotation and color jittering 
on the input samples. Although being effective  these augmentation techniques are not capable of
performing semantic transformations  such as changing the background of an object or the texture
of a foreground object. Recent work has shown that data augmentation can be more powerful if
(class identity preserving) semantic transformations are allowed [6  7  8]. For example  by training
a generative adversarial network (GAN) for each class in the training set  one could then sample
an inﬁnite number of samples from the generator. Unfortunately  this procedure is computationally
intensive because training generative models and inferring them to obtain augmented samples are

∗Equal contribution.
†Corresponding author.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Forward

Deep Feature Space 

Deep Features 

Augmented Samples

…

Augmented Images

(Not Shown Explicitly) 

Training Data

Deep Networks

Corresponding to

Augment 

Semantically

ISDA Loss

Figure 1: An overview of ISDA. Inspired by the observation that certain directions in the feature space
correspond to meaningful semantic transformations  we augment the training data semantically by
translating their features along these semantic directions  without involving auxiliary deep networks.
The directions are obtained by sampling random vectors from a zero-mean normal distribution with
dynamically estimated class-conditional covariance matrices. In addition  instead of performing
augmentation explicitly  ISDA boils down to minimizing a closed-form upper-bound of the expected
cross-entropy loss on the augmented training set  which makes our method highly efﬁcient.

both nontrivial tasks. Moreover  due to the extra augmented data  the training procedure is also likely
to be prolonged.

In this paper  we propose an implicit semantic data augmentation (ISDA) algorithm for training deep
image recognition networks. The ISDA is highly efﬁcient as it does not require training/inferring
auxiliary networks or explicitly generating extra training samples. Our approach is motivated by the
intriguing observation made by recent work showing that the features deep in a network are usually
linearized [9  10]. Speciﬁcally  there exist many semantic directions in the deep feature space  such
that translating a data sample in the feature space along one of these directions results in a feature
representation corresponding to another sample with the same class identity but different semantics.
For example  a certain direction corresponds to the semantic translation of "make-bespectacled".
When the feature of a person  who does not wear glasses  is translated along this direction  the
new feature may correspond to the same person but with glasses (The new image can be explicitly
reconstructed using proper algorithms as shown in [9]). Therefore  by searching for many such
semantic directions  we can effectively augment the training set in a way complementary to traditional
data augmenting techniques.

However  explicitly ﬁnding semantic directions is not a trivial task  which usually requires extensive
human annotations [9]. In contrast  sampling directions randomly is efﬁcient but may result in
meaningless transformations. For example  it makes no sense to apply the "make-bespectacled"
transformation to the “car” class. In this paper  we adopt a simple method that achieves a good
balance between effectiveness and efﬁciency. In speciﬁc  we perform an online estimate of the
covariance matrix of the features for each class  which captures the intra-class variations. Then we
sample directions from a zero-mean multi-variate normal distribution with the estimated covariance 
and apply them to the features of training samples in that class to augment the dataset. In this way 
the chance of generating meaningless semantic transformations can be signiﬁcantly reduced.

To further improve the efﬁciency  we derive a closed-form upper bound of the expected cross-entropy
(CE) loss with the proposed data augmentation scheme. Therefore  instead of performing the
augmentation procedure explicitly  we can directly minimize the upper bound  which is  in fact  a
novel robust loss function. As there is no need to generate explicit data samples  we call our algorithm
implicit semantic data augmentation (ISDA). Compared to existing semantic data augmentation
algorithms  the proposed ISDA can be conveniently implemented on top of most deep models without
introducing auxiliary models or noticeable extra computational cost.

Although being simple  the proposed ISDA algorithm is surprisingly effective  and complements
existing non-semantic data augmentation techniques quite well. Extensive empirical evaluations
on several competitive image classiﬁcation benchmarks show that ISDA consistently improves the
generalization performance of popular deep networks  especially with little training data and powerful
traditional augmentation techniques.

2 Related Work

In this section  we brieﬂy review existing research on related topics.

2

Data augmentation is a widely used technique to alleviate overﬁtting in training deep networks. For
example  in image recognition tasks  data augmentation techniques like random ﬂipping  mirroring
and rotation are applied to enforce certain invariance in convolutional networks [4  5  3  11]. Recently 
automatic data augmentation techniques  e.g.  AutoAugment [12]  are proposed to search for a
better augmentation strategy among a large pool of candidates. Similar to our method  learning
with marginalized corrupted features [13] can be viewed as an implicit data augmentation technique 
but it is limited to simple linear models. Complementarily  recent research shows that semantic
data augmentation techniques which apply class identity preserving transformations (e.g. changing
backgrounds of objects or varying visual angles) to the training data are effective as well [14  15 
6  8]. This is usually achieved by generating extra semantically transformed training samples with
specialized deep structures such as DAGAN [8]  domain adaptation networks [15] or other GAN-
based generators [14  6]. Although being effective  these approaches are nontrivial to implement and
computationally expensive  due to the need to train generative models beforehand and infer them
during training.

Robust loss function. As shown in the paper  ISDA amounts to minimizing a novel robust loss
function. Therefore  we give a brief review of related work on this topic. Recently  several robust loss
functions are proposed for deep learning. For example  the Lq loss [16] is a balanced noise-robust
form for the cross entropy (CE) loss and mean absolute error (MAE) loss  derived from the negative
Box-Cox transformation. Focal loss [17] attaches high weights to a sparse set of hard examples to
prevent the vast number of easy samples from dominating the training of the network. The idea of
introducing large margin for CE loss has been proposed in [18  19  20]. In [21]  the CE loss and
the contrastive loss are combined to learn more discriminative features. From a similar perspective 
center loss [22] simultaneously learns a center for deep features of each class and penalizes the
distances between the samples and their corresponding class centers in the feature space  enhancing
the intra-class compactness and inter-class separability.

Semantic transformations in deep feature space. Our work is motivated by the fact that high-
level representations learned by deep convolutional networks can potentially capture abstractions
with semantics [23  10]. In fact  translating deep features along certain directions is shown to be
corresponding to performing meaningful semantic transformations on the input images. For example 
deep feature interpolation [9] leverages simple interpolations of deep features from pre-trained
neural networks to achieve semantic image transformations. Variational Autoencoder(VAE) and
Generative Adversarial Network(GAN) based methods [24  25  26] establish a latent representation
corresponding to the abstractions of images  which can be manipulated to edit the semantics of
images. Generally  these methods reveal that certain directions in the deep feature space correspond to
meaningful semantic transformations  and can be leveraged to perform semantic data augmentation.

3 Method

Deep networks are known to excel at forming high-level representations in the deep feature space
[4  5  9  27]  where the semantic relations between samples can be captured by the relative positions
of their features [10]. Previous work has demonstrated that translating features towards speciﬁc
directions corresponds to meaningful semantic transformations when the features are mapped to the
input space [9  28  10]. Based on this observation  we propose to directly augment the training data
in the feature space  and integrate this procedure into the training of deep models.

The proposed implicit semantic data augmentation (ISDA) has two important components  i.e.  online
estimation of class-conditional covariance matrices and optimization with a robust loss function.
The ﬁrst component aims to ﬁnd a distribution from which we can sample meaningful semantic
transformation directions for data augmentation  while the second saves us from explicitly generating
a large amount of extra training data  leading to remarkable efﬁciency compared to existing data
augmentation techniques.

3.1 Semantic Transformations in Deep Feature Space
As aforementioned  certain directions in the deep feature space correspond to meaningful semantic
transformations like “make-bespectacled” or ‘change-view-angle’. This motivates us to augment
the training set by applying such semantic transformations on deep features. However  manually
searching for semantic directions is infeasible for large scale problems. To address this problem 
we propose to approximate the procedure by sampling random vectors from a normal distribution
with zero mean and a covariance that is proportional to the intra-class covariance matrix  which
captures the variance of samples in that class and is thus likely to contain rich semantic information.

3

Intuitively  features for the person class may vary along the “wear-glasses” direction  while having
nearly zero variance along the “has-propeller” direction which only occurs for other classes like the
plane class. We hope that directions corresponding to meaningful transformations for each class are
well represented by the principal components of the covariance matrix of that class.
Consider training a deep network G with weights Θ on a training set D = {(xi  yi)}N
i=1  where
yi ∈ {1  . . .   C} is the label of the i-th sample xi over C classes. Let the A-dimensional vector
ai = [ai1  . . .   aiA]T = G(xi  Θ) denote the deep features of xi learned by G  and aij indicate the
jth element of ai.

To obtain semantic directions to augment ai  we randomly sample vectors from a zero-mean multi-
variate normal distribution N (0  Σyi )  where Σyi is the class-conditional covariance matrix estimated
from the features of all the samples in class yi. In implementation  the covariance matrix is computed
in an online fashion by aggregating statistics from all mini-batches. The online estimation algorithm
is given in Section A in the supplementary.

During training  C covariance matrices are computed  one for each class. The augmented feature ˜ai
is obtained by translating ai along a random direction sampled from N (0  λΣyi ). Equivalently  we
have

˜ai ∼ N (ai  λΣyi ) 

(1)
where λ is a positive coefﬁcient to control the strength of semantic data augmentation. As the
covariances are computed dynamically during training  the estimation in the ﬁrst few epochs are not
quite informative when the network is not well trained. To address this issue  we let λ = (t/T )×λ0
be a function of the current iteration t  thus to reduce the impact of the estimated covariances on our
algorithm early in the training stage.

Implicit Semantic Data Augmentation (ISDA)

3.2
A naive method to implement ISDA is to explicitly augment each ai for M times  forming an
augmented feature set {(a1
i is k-th copy of augmented
features for sample xi. Then the networks are trained by minimizing the cross-entropy (CE) loss:

i   yi)  . . .   (aM
i

  yi)}N

i=1 of size M N   where ak

LM (W   b  Θ) =

1
N

N

X

i=1

1
M

M

X

k=1

−log(

k
i +byi

T
yi

a

ew
j=1 ewT
PC

j

) 

ak

i +bj

(2)

where W = [w1  . . .   wC]T ∈ RC×A and b = [b1  . . .   bC]T ∈ RC are the weight matrix and
biases corresponding to the ﬁnal fully connected layer  respectively.

Obviously  the naive implementation is computationally inefﬁcient when M is large  as the feature
set is enlarged by M times. In the following  we consider the case that M grows to inﬁnity  and ﬁnd
that an easy-to-compute upper bound can be derived for the loss function  leading to a highly efﬁcient
implementation.

Upper bound of the loss function. In the case M → ∞  we are in fact considering the expectation
of the CE loss under all possible augmented features. Speciﬁcally  L∞ is given by:

L∞(W   b  Θ|Σ) =

1
N

N

X

i=1

E˜ai [−log(

T
yi

˜ai+byi

ew
j=1 ewT
PC

j ˜ai+bj

)].

(3)

If L∞ can be computed efﬁciently  then we can directly minimize it without explicitly sampling
augmented features. However  Eq. (3) is difﬁcult to compute in its exact form. Alternatively  we
ﬁnd that it is possible to derive an easy-to-compute upper bound for L∞  as given by the following
proposition.
Proposition 1. Suppose that ˜ai ∼ N (ai  λΣyi )  then we have an upper bound of L∞  given by

L∞(W   b  Θ|Σ) ≤

1
N

N

X

i=1

−log(

PC

j=1 ewT

j

ew
ai+bj + λ

T
yi

ai+byi

2 (wT

j −wT
yi

)Σyi (wj −wyi )

)   L∞.

(4)

Proof. According to the deﬁnition of L∞ in (3)  we have:

L∞(W   b  Θ|Σ) =

1
N

N

X

i=1

E˜ai [log(

C

X

j=1

e(w

T
j −w

T
yi

)˜ai+(bj −byi ))]

(5)

4

≤

=

1
N

1
N

N

X

i=1

N

X

i=1

log(

log(

C

X

j=1

C

X

j=1

E˜ai [e(w

T
j −w

T
yi

)˜ai+(bj −byi )])

(6)

e(w

T
j −w

T
yi

)ai+(bj −byi )+ λ

2 (w

T
j −w

T
yi

)Σyi (wj −wyi ))

(7)

(8)
In the above  the Inequality (6) follows from the Jensen’s inequality E[logX] ≤ logE[X]  as the
logarithmic function log(·) is concave. The Eq. (7) is obtained by leveraging the moment-generating
function:

= L∞.

E[etX ] = etµ+ 1

2 σ2t2

  X ∼ N (µ  σ2) 

due to the fact that (wT

j −wT

yi )˜ai +(bj −byi ) is a Gaussian random variable  i.e. 

(wT

j −wT

yi )˜ai +(bj −byi ) ∼ N (cid:0)(wT

j −wT
Essentially  Proposition 1 provides a surrogate
loss for our implicit data augmentation algo-
rithm. Instead of minimizing the exact loss func-
tion L∞  we can optimize its upper bound L∞
in a much more efﬁcient way. Therefore  the
proposed ISDA boils down to a novel robust
loss function  which can be easily adopted by
most deep models. In addition  we can observe
that when λ → 0  which means no features are
augmented  L∞ reduces to the standard CE loss.

yi )ai +(bj −byi )  λ(wT

j −wT

yi )Σyi (wj −wyi )(cid:1) .

Algorithm 1 The ISDA Algorithm.
1: Input: D  λ0
2: Randomly initialize W   b and Θ
3: for t = 0 to T do
4:
5:
6:

Sample a mini-batch {xi  yi}B
Compute ai = G(xi  Θ)
Estimate the covariance matrices Σ1  Σ2 
...  ΣC
Compute L∞ according to Eq. (4)
Update W   b  Θ with SGD

i=1 from D

7:
8:
9: end for
10: Output: W   b and Θ

In summary  the proposed ISDA can be simply
plugged into deep networks as a robust loss func-
tion  and efﬁciently optimized with the stochas-
tic gradient descent (SGD) algorithm. We present the pseudo code of ISDA in Algorithm 1. Details
of estimating covariance matrices and computing gradients are presented in Appendix A.

4 Experiments
In this section  we empirically validate the proposed algorithm on several widely used image clas-
siﬁcation benchmarks  i.e.  CIFAR-10  CIFAR-100 [1] and ImageNet[29]. We ﬁrst evaluate the
effectiveness of ISDA with different deep network architectures on these datasets. Second  we
apply several recent proposed non-semantic image augmentation methods in addition to the standard
baseline augmentation  and investigate the performance of ISDA. Third  we present comparisons
with state-of-the-art robust lost functions and generator-based semantic data augmentation algorithms.
Finally  ablation studies are conducted to examine the effectiveness of each component. We also
visualize the augmented samples in the original input space with the aid of a generative network.

4.1 Datasets and Baselines
Datasets. We use three image recognition benchmarks in the experiments. (1) The two CIFAR
datasets consist of 32x32 colored natural images in 10 classes for CIFAR-10 and 100 classes for
CIFAR-100  with 50 000 images for training and 10 000 images for testing  respectively. In our
experiments  we hold out 5000 images from the training set as the validation set to search for the
hyper-parameter λ0. These samples are also used for training after an optimal λ0 is selected  and
the results on the test set are reported. Images are normalized with channel means and standard
deviations for pre-procession. For the non-semantic data augmentation of the training set  we follow
the standard operation in [30]: 4 pixels are padded at each side of the image  followed by a random
32x32 cropping combined with random horizontal ﬂipping. (2) ImageNet is a 1 000-class dataset
from ILSVRC2012[29]  providing 1.2 million images for training and 50 000 images for validation.
We adopt the same augmentation conﬁgurations in [2  4  5].

Non-semantic augmentation techniques. To study the complementary effects of ISDA to tradi-
tional data augmentation methods  two state-of-the-art non-semantic augmentation techniques are
applied  with and without ISDA. (1) Cutout [31] randomly masks out square regions of input during
training to regularize the model. (2) AutoAugment [32] automatically searches for the best augmenta-
tion policies to yield the highest validation accuracy on a target dataset. All hyper-parameters are the
same as reported in the papers introducing them.

5

Table 1: Evaluation of ISDA on CIFAR with different models. The average test error over the last 10
epochs is calculated in each experiment  and we report mean values and standard deviations in three
independent experiments. The best results are bold-faced.

Method

ResNet-32 [4]

ResNet-32 + ISDA

ResNet-110 [4]

CIFAR-10

CIFAR-100

ResNet-110 + ISDA
SE-ResNet-110 [33]

SE-ResNet-110 + ISDA
Wide-ResNet-16-8 [34]

Params
7.39 ± 0.10% 31.20 ± 0.41%
0.5M
7.09 ± 0.12% 30.27 ± 0.34%
0.5M
6.76 ± 0.34% 28.67 ± 0.44%
1.7M
6.33 ± 0.19% 27.57 ± 0.46%
1.7M
6.14 ± 0.17% 27.30 ± 0.03%
1.7M
5.96 ± 0.21% 26.63 ± 0.21%
1.7M
11.0M 4.25 ± 0.18% 20.24 ± 0.27%
11.0M 4.04 ± 0.29% 19.91 ± 0.21%
36.5M 3.82 ± 0.15% 18.53 ± 0.07%
36.5M 3.58 ± 0.15% 17.98 ± 0.15%
34.4M 3.86 ± 0.14% 18.16 ± 0.13%
34.4M 3.67 ± 0.12% 17.43 ± 0.25%
4.90 ± 0.08% 22.61 ± 0.10%
0.8M
4.54 ± 0.07% 22.10 ± 0.34%
0.8M
25.6M
DenseNet-BC-190-40 + ISDA 25.6M

DenseNet-BC-100-12 + ISDA

Wide-ResNet-28-10 + ISDA

ResNeXt-29  8x64d + ISDA

Wide-ResNet-16-8 + ISDA

DenseNet-BC-100-12 [5]

DenseNet-BC-190-40 [5]

3.52%
3.24%

17.74%
17.42%

Wide-ResNet-28-10 [34]

ResNeXt-29  8x64d [35]

Table 2: Evaluation of ISDA with state-of-the-art non-semantic augmentation techniques. ‘AA’
refers to AutoAugment [32]. We report mean values and standard deviations in three independent
experiments. The best results are bold-faced.

Dataset

Networks

Cutout [31] Cutout + ISDA

AA [32]

AA + ISDA

CIFAR-10

Wide-ResNet-28-10 [34]

2.99 ± 0.06% 2.83 ± 0.04% 2.65 ± 0.07% 2.56 ± 0.01%
Shake-Shake (26  2x32d) [36] 3.16 ± 0.09% 2.93 ± 0.03% 2.89 ± 0.09% 2.68 ± 0.12%
Shake-Shake (26  2x112d) [36]

2.25%

1.82%

2.36%

2.01%

CIFAR-100

Wide-ResNet-28-10 [34]

18.05 ± 0.25% 17.02 ± 0.11% 16.60 ± 0.40% 15.62 ± 0.32%
Shake-Shake (26  2x32d) [36] 18.92 ± 0.21% 18.17 ± 0.08 % 17.50 ± 0.19% 17.21 ± 0.33%
Shake-Shake (26  2x112d) [36] 17.34 ± 0.28% 16.24 ± 0.20 % 15.21 ± 0.20% 13.87 ± 0.26%

Baselines. Our method is compared to several baselines including state-of-the-art robust loss func-
tions and generator-based semantic data augmentation methods. (1) Dropout [37] is a widely used
regularization approach which randomly mutes some neurons during training. (2) Large-margin
softmax loss [18] introduces large decision margin  measured by a cosine distance  to the standard CE
loss. (3) Disturb label [38] is a regularization mechanism that randomly replaces a fraction of labels
with incorrect ones in each iteration. (4) Focal loss [17] focuses on a sparse set of hard examples to
prevent easy samples from dominating the training procedure. (5) Center loss [22] simultaneously
learns a center of features for each class and minimizes the distances between the deep features
and their corresponding class centers. (6) Lq loss [16] is a noise-robust loss function  using the
negative Box-Cox transformation. (7) For generator-based semantic augmentation methods  we train
several state-of-the-art GANs [39  40  41  42]  which are then used to generate extra training samples
for data augmentation. For fair comparison  all methods are implemented with the same training
conﬁgurations when it is possible. Details for hyper-parameter settings are presented in Appendix B.

Training details. For deep networks  we implement the ResNet  SE-ResNet  Wide-ResNet  ResNeXt 
DenseNet and PyramidNet on CIFAR  and ResNet  ResNeXt on ImageNet. Detailed conﬁgurations
for these models are given in Appendix B. The hyper-parameter λ0 for ISDA is selected from the
set {0.1  0.25  0.5  0.75  1} according to the performance on the validation set. On ImageNet  due to
GPU memory limitation  we approximate the covariance matrices by their diagonals  i.e.  the variance
of each dimension of the features. The best hyper-parameter λ0 is selected from {1  2.5  5  7.5  10}.

4.2 Main Results
Table 1 presents the performance of several state-of-the-art deep networks with and without ISDA. It
can be observed that ISDA consistently improves the generalization performance of these models 
especially with fewer training samples per class. On CIFAR-100  for relatively small models like
ResNet-32 and ResNet-110  ISDA reduces test errors by about 1%  while for larger models like

6

28

26

24

22

20

18

16

)

%

(

r
o
r
r
E
t
s
e
T

WRN-28-10
WRN-28-10 + ISDA
WRN-28-10 + AA
WRN-28-10 + AA +ISDA

24

23

22

21

20

19

18

)

%

(

e
t
a
R
r
o
r
r
E

ResNet-152 (Test)
ResNet-152 + ISDA (Test)
ResNet-152 (Train)
ResNet-152 + ISDA (Train)

40

60

80

100

120

140

160

180

200

220

240

17

60

70

80

Epoch

90

Epoch

100

110

120

Figure 2: Curves of test errors on CIFAR-100
with Wide-ResNet (WRN).
Table 3: Comparisons with the state-of-the-art methods. We report mean values and standard
deviations of the test error in three independent experiments. Best results are bold-faced.

Figure 3: Training and test errors on ImageNet.

Method

Large Margin [18]
Disturb Label [38]
Focal Loss [17]
Center Loss [22]
Lq Loss [16]
WGAN [39]
CGAN [40]
ACGAN [41]
infoGAN [42]
Basic
Basic + Dropout
ISDA
ISDA + Dropout

ResNet-110

Wide-ResNet-28-10

CIFAR-10

6.46±0.20%
6.61±0.04%
6.68±0.22%
6.38±0.20%
6.69±0.07%
6.63±0.23%
6.56±0.14%
6.32±0.12%
6.59±0.12%
6.76±0.34%
6.23±0.11%
6.33±0.19%
5.98±0.20%

CIFAR-100

28.00±0.09%
28.46±0.32%
28.28±0.32%
27.85±0.10%
28.78±0.35%

-

28.25±0.36%
28.48±0.44%
27.64±0.14%
28.67±0.44%
27.11±0.06%
27.57±0.46%
26.35±0.30%

CIFAR-10

3.69±0.10%
3.91±0.10%
3.62±0.07%
3.76±0.05%
3.78±0.08%
3.81±0.08%
3.84±0.07%
3.81±0.11%
3.81±0.05%

-

CIFAR-100

18.48±0.05%
18.56±0.22%
18.22±0.08%
18.50±0.25%
18.43±0.37%

-

18.79±0.08%
18.54±0.05%
18.44±0.10%

-

3.82±0.15%

18.53±0.07%

-

-

3.58±0.15%

17.98±0.15%

Wide-ResNet-28-10 and ResNeXt-29  8x64d  our method outperforms the competitive baselines by
nearly 0.7%. Compared to ResNets  DenseNets generally suffer less from overﬁtting due to their
architecture design  thus appear to beneﬁt less from our algorithm.

Table 2 shows experimental results with recent proposed powerful traditional image augmentation
methods (i.e. Cutout [31] and AutoAugment [32]). Interestingly  ISDA seems to be even more
effective when these techniques exist. For example  when applying AutoAugment  ISDA achieves
performance gains of 1.34% and 0.98% on CIFAR-100 with the Shake-Shake (26  2x112d) and
the Wide-ResNet-28-10  respectively. Notice that these improvements are more signiﬁcant than the
standard situations. A plausible explanation for this phenomenon is that non-semantic augmentation
methods help to learn a better feature representation  which makes semantic transformations in the
deep feature space more reliable. The curves of test errors during training on CIFAR-100 with Wide-
ResNet-28-10 are presented in Figure 2. It is clear that ISDA achieves a signiﬁcant improvement
after the third learning rate drop  and shows even better performance after the fourth drop.

Table 4 presents the performance of ISDA on
the large scale ImageNet dataset. It can be ob-
served that ISDA reduces Top-1 error rate by
0.54% for the ResNeXt-50 model. The train-
ing and test error curves of ResNet-152 are
shown in Figure 3. Notably  ISDA achieves a
slightly higher training error but a lower test
error  indicating that ISDA performs effective
regularization on deep networks.

Table 4: Evaluation of ISDA on ImageNet.

Method

ResNet-50 [4]

ResNet-50 + ISDA

ResNet-152 [4]

ResNet-152 + ISDA

ResNeXt-50  32x4d [35]

ResNeXt-50  32x4d + ISDA

Top-5
Top-1
23.58%
6.92%
23.30% 6.82%
21.65%
6.01%
21.20% 5.67%
22.42%
6.42%
21.88% 6.23%

4.3 Comparison with Other Approaches
We compare ISDA with a number of competitive baselines described in Section 4.1  ranging from
robust loss functions to semantic data augmentation algorithms based on generative models. The

7

Initial Restored

Augmented

Initial Restored

Augmented

Figure 4: Visualization results of semantically augmented images.

results are summarized in Table 3  and the training curves are presented in Appendix D. One can
observe that ISDA compares favorably with all the competitive baseline algorithms. With ResNet-110 
the test errors of other robust loss functions are 6.38% and 27.85% on CIFAR-10 and CIFAR-100 
respectively  while ISDA achieves 6.23% and 27.11%  respectively.

Among all GAN-based semantic augmentation methods  ACGAN gives the best performance  espe-
cially on CIFAR-10. However  these models generally suffer a performance reduction on CIFAR-100 
which do not contain enough samples to learn a valid generator for each class. In contrast  ISDA
shows consistent improvements on all the datasets. In addition  GAN-based methods require addi-
tional computation to train the generators  and introduce signiﬁcant overhead to the training process.
In comparison  ISDA not only leads to lower generalization error  but is simpler and more efﬁcient.

4.4 Visualization Results
To demonstrate that our method is able to generate meaningful semantically augmented samples 
we introduce an approach to map the augmented features back to the pixel space to explicitly show
semantic changes of the images. Due to space limitations  we defer the detailed introduction of the
mapping algorithm and present it in Appendix C.

Figure 4 shows the visualization results. The ﬁrst and second columns represent the original images
and reconstructed images without any augmentation. The rest columns present the augmented images
by the proposed ISDA. It can be observed that ISDA is able to alter the semantics of images  e.g. 
backgrounds  visual angles  colors and type of cars  color of skins  which is not possible for traditional
data augmentation techniques.

Table 5: The ablation study for ISDA.

CIFAR-10

CIFAR-100

4.5 Ablation Study
To get a better understanding of the ef-
fectiveness of different components in
ISDA  we conduct a series of ablation
studies. In speciﬁc  several variants are
considered: (1) Identity matrix means
replacing the covariance matrix Σc by
the identity matrix. (2) Diagonal matrix
means using only the diagonal elements
of the covariance matrix Σc. (3) Single
covariance matrix means using a global covariance matrix computed from the features of all classes.
(4) Constant λ0 means using a constant λ0 without setting it as a function of the training iterations.

18.58±0.10%
3.82±0.15%
18.53±0.02%
3.63±0.12%
18.23±0.02%
3.70±0.15%
18.29±0.13%
3.67±0.07%
3.69±0.08%
18.33±0.16%
3.58±0.15% 17.98±0.15%

Setting
Basic
Identity matrix
Diagonal matrix
Single covariance matrix
Constant λ0
ISDA

Table 5 presents the ablation results. Adopting the identity matrix increases the test error by 0.05%
on CIFAR-10 and nearly 0.56% on CIFAR-100. Using a single covariance matrix greatly degrades
the generalization performance as well. The reason is likely to be that both of them fail to ﬁnd proper
directions in the deep feature space to perform meaningful semantic transformations. Adopting a
diagonal matrix also hurts the performance as it does not consider correlations of features.

5 Conclusion
In this paper  we proposed an efﬁcient implicit semantic data augmentation algorithm (ISDA) to
complement existing data augmentation techniques. Different from existing approaches leveraging
generative models to augment the training set with semantically transformed samples  our approach is
considerably more efﬁcient and easier to implement. In fact  we showed that ISDA can be formulated
as a novel robust loss function  which is compatible with any deep network with the cross-entropy loss.
Extensive results on several competitive image classiﬁcation datasets demonstrate the effectiveness
and efﬁciency of the proposed algorithm.

8

Acknowledgments

Gao Huang is supported in part by Beijing Academy of Artiﬁcial Intelligence (BAAI) under
grant BAAI2019QN0106 and Tencent AI Lab Rhino-Bird Focused Research Program under grant
JR201914.

References

[1] A. Krizhevsky and G. Hinton  “Learning multiple layers of features from tiny images ” Citeseer 

Tech. Rep.  2009.

[2] A. Krizhevsky  I. Sutskever  and G. E. Hinton  “Imagenet classiﬁcation with deep convolutional

neural networks ” in NeurIPS  2012  pp. 1097–1105.

[3] K. Simonyan and A. Zisserman  “Very deep convolutional networks for large-scale image

recognition ” in ICLR  2015.

[4] K. He  X. Zhang  S. Ren  and J. Sun  “Deep residual learning for image recognition ” in CVPR 

2016  pp. 770–778.

[5] G. Huang  Z. Liu  G. Pleiss  L. Van Der Maaten  and K. Weinberger  “Convolutional networks
with dense connectivity ” IEEE Transactions on Pattern Analysis and Machine Intelligence 
2019.

[6] A. J. Ratner  H. Ehrenberg  Z. Hussain  J. Dunnmon  and C. Ré  “Learning to compose domain-

speciﬁc transformations for data augmentation ” in NeurIPS  2017  pp. 3236–3246.

[7] C. Bowles  L. J. Chen  R. Guerrero  P. Bentley  R. N. Gunn  A. Hammers  D. A. Dickie  M. del
C. Valdés Hernández  J. M. Wardlaw  and D. Rueckert  “Gan augmentation: Augmenting
training data using generative adversarial networks ” CoRR  vol. abs/1810.10863  2018.

[8] A. Antoniou  A. J. Storkey  and H. A. Edwards  “Data augmentation generative adversarial

networks ” CoRR  vol. abs/1711.04340  2018.

[9] P. Upchurch  J. R. Gardner  G. Pleiss  R. Pless  N. Snavely  K. Bala  and K. Q. Weinberger 

“Deep feature interpolation for image content changes ” in CVPR  2017  pp. 6090–6099.

[10] Y. Bengio  G. Mesnil  Y. Dauphin  and S. Rifai  “Better mixing via deep representations ” in

ICML  2013  pp. 552–560.

[11] R. K. Srivastava  K. Greff  and J. Schmidhuber  “Training very deep networks ” in NeurIPS 

2015  pp. 2377–2385.

[12] E. D. Cubuk  B. Zoph  D. Mané  V. Vasudevan  and Q. V. Le  “Autoaugment: Learning

augmentation policies from data ” CoRR  vol. abs/1805.09501  2018.

[13] L. Maaten  M. Chen  S. Tyree  and K. Weinberger  “Learning with marginalized corrupted

features ” in ICML  2013  pp. 410–418.

[14] M. Jaderberg  K. Simonyan  A. Vedaldi  and A. Zisserman  “Reading text in the wild with
convolutional neural networks ” International Journal of Computer Vision  vol. 116  no. 1  pp.
1–20  2016.

[15] K. Bousmalis  N. Silberman  D. Dohan  D. Erhan  and D. Krishnan  “Unsupervised pixel-level

domain adaptation with generative adversarial networks ” in CVPR  2017  pp. 3722–3731.

[16] Z. Zhang and M. R. Sabuncu  “Generalized cross entropy loss for training deep neural networks

with noisy labels ” in NeurIPS  2018.

[17] T.-Y. Lin  P. Goyal  R. B. Girshick  K. He  and P. Dollár  “Focal loss for dense object detection ”

in ICCV  2017  pp. 2999–3007.

[18] W. Liu  Y. Wen  Z. Yu  and M. Yang  “Large-margin softmax loss for convolutional neural

networks.” in ICML  2016.

[19] X. Liang  X. Wang  Z. Lei  S. Liao  and S. Z. Li  “Soft-margin softmax for deep classiﬁcation ”

in ICONIP  2017.

[20] X. Wang  S. Zhang  Z. Lei  S. Liu  X. Guo  and S. Z. Li  “Ensemble soft-margin softmax loss

for image classiﬁcation ” in IJCAI  2018.

9

[21] Y. Sun  X. Wang  and X. Tang  “Deep learning face representation by joint identiﬁcation-

veriﬁcation ” in NeurIPS  2014.

[22] Y. Wen  K. Zhang  Z. Li  and Y. Qiao  “A discriminative feature learning approach for deep face

recognition ” in ECCV  2016  pp. 499–515.

[23] Y. Bengio et al.  “Learning deep architectures for ai ” Foundations and trends R(cid:13) in Machine

Learning  vol. 2  no. 1  pp. 1–127  2009.

[24] Y. Choi  M.-J. Choi  M. Kim  J.-W. Ha  S. Kim  and J. Choo  “Stargan: Uniﬁed generative
adversarial networks for multi-domain image-to-image translation ” in CVPR  2018  pp. 8789–
8797.

[25] J.-Y. Zhu  T. Park  P. Isola  and A. A. Efros  “Unpaired image-to-image translation using

cycle-consistent adversarial networks ” in ICCV  2017  pp. 2223–2232.

[26] Z. He  W. Zuo  M. Kan  S. Shan  and X. Chen  “Attgan: Facial attribute editing by only changing

what you want.” CoRR  vol. abs/1711.10678  2017.

[27] S. Ren  K. He  R. Girshick  and J. Sun  “Faster r-cnn: Towards real-time object detection with

region proposal networks ” in NeurIPS  2015  pp. 91–99.

[28] M. Li  W. Zuo  and D. Zhang  “Convolutional network for attribute-driven and identity-

preserving human face generation ” CoRR  vol. abs/1608.06434  2016.

[29] J. Deng  W. Dong  R. Socher  L. Li  K. Li  and L. Fei-Fei  “Imagenet: A large-scale hierarchical

image database ” in ICML  2009  pp. 248–255.

[30] A. G. Howard  “Some improvements on deep convolutional neural network based image

classiﬁcation ” CoRR  vol. abs/1312.5402  2014.

[31] T. DeVries and G. W. Taylor  “Improved regularization of convolutional neural networks with

cutout ” arXiv preprint arXiv:1708.04552  2017.

[32] E. D. Cubuk  B. Zoph  D. Mane  V. Vasudevan  and Q. V. Le  “Autoaugment: Learning

augmentation policies from data ” in CVPR  2019.

[33] J. Hu  L. Shen  and G. Sun  “Squeeze-and-excitation networks ” in CVPR  2018  pp. 7132–7141.

[34] S. Zagoruyko and N. Komodakis  “Wide residual networks ” in BMVC  2017.

[35] S. Xie  R. Girshick  P. Dollár  Z. Tu  and K. He  “Aggregated residual transformations for deep

neural networks ” in CVPR  2017  pp. 1492–1500.

[36] X. Gastaldi  “Shake-shake regularization ” arXiv preprint arXiv:1705.07485  2017.

[37] N. Srivastava  G. E. Hinton  A. Krizhevsky  I. Sutskever  and R. R. Salakhutdinov  “Dropout: a
simple way to prevent neural networks from overﬁtting ” Journal of Machine Learning Research 
vol. 15  pp. 1929–1958  2014.

[38] L. Xie  J. Wang  Z. Wei  M. Wang  and Q. Tian  “Disturblabel: Regularizing cnn on the loss

layer ” in CVPR  2016  pp. 4753–4762.

[39] M. Arjovsky  S. Chintala  and L. Bottou  “Wasserstein gan ” CoRR  vol. abs/1701.07875  2017.

[40] M. Mirza and S. Osindero  “Conditional generative adversarial nets ” CoRR  vol. abs/1411.1784 

2014.

[41] A. Odena  C. Olah  and J. Shlens  “Conditional image synthesis with auxiliary classiﬁer gans ”

in ICML  2017  pp. 2642–2651.

[42] X. Chen  Y. Duan  R. Houthooft  J. Schulman  I. Sutskever  and P. Abbeel  “Infogan: Inter-
pretable representation learning by information maximizing generative adversarial nets ” in
NeurIPS  2016  pp. 2172–2180.

10

,Yulin Wang
Xuran Pan
Shiji Song
Hong Zhang
Gao Huang
Cheng Wu