2019,Variational Structured Semantic Inference for Diverse Image Captioning,Despite the exciting progress in image captioning  generating diverse captions for a given image remains as an open problem. Existing methods typically apply generative models such as Variational Auto-Encoder to diversify the captions  which however neglect two key factors of diverse expression  i.e.  the lexical diversity and the syntactic diversity. To model these two inherent diversities in image captioning  we propose a Variational Structured Semantic Inferring model (termed VSSI-cap) executed in a novel structured encoder-inferer-decoder schema. VSSI-cap mainly innovates in a novel structure  i.e.  Variational Multi-modal Inferring tree (termed VarMI-tree). In particular  conditioned on the visual-textual features from the encoder  the VarMI-tree models the lexical and syntactic diversities by inferring their latent variables (with variations) in an approximate posterior inference guided by a visual semantic prior. Then  a reconstruction loss and the posterior-prior KL-divergence are jointly estimated to optimize the VSSI-cap model. Finally  diverse captions are generated upon the visual features and the latent variables from this structured encoder-inferer-decoder model. Experiments on the benchmark dataset show that the proposed VSSI-cap achieves significant improvements over the state-of-the-arts.,Variational Structured Semantic Inference for

Diverse Image Captioning

Fuhai Chen1  Rongrong Ji12∗  Jiayi Ji1  Xiaoshuai Sun1  Baochang Zhang3  Xuri Ge1 

Yongjian Wu4  Feiyue Huang4  Yan Wang5

1Department of Artiﬁcial Intelligence  School of Informatics  Xiamen University 

2Peng Cheng Lab  3Beihang University  4Tencent Youtu Lab  5Pinterest

{cfh3c.xmu jjyxmu xurigexmu}@gmail.com  {rrji xssun}@xmu.edu.cn  bczhang@buaa.edu.cn 

{littlekenwu garyhuang}@tencent.com  yanw@pinterest.com

Abstract

Despite the exciting progress in image captioning  generating diverse captions
for a given image remains as an open problem. Existing methods typically ap-
ply generative models such as Variational Auto-Encoder to diversify the captions 
which however neglect two key factors of diverse expression  i.e.  the lexical di-
versity and the syntactic diversity. To model these two inherent diversities in im-
age captioning  we propose a Variational Structured Semantic Inferring model
(termed VSSI-cap) executed in a novel structured encoder-inferer-decoder schema.
VSSI-cap mainly innovates in a novel structure  i.e.  Variational Multi-modal In-
ferring tree (termed VarMI-tree). In particular  conditioned on the visual-textual
features from the encoder  the VarMI-tree models the lexical and syntactic diver-
sities by inferring their latent variables (with variations) in an approximate poste-
rior inference guided by a visual semantic prior. Then  a reconstruction loss and
the posterior-prior KL-divergence are jointly estimated to optimize the VSSI-cap
model. Finally  diverse captions are generated upon the visual features and the
latent variables from this structured encoder-inferer-decoder model. Experiments
on the benchmark dataset show that the proposed VSSI-cap achieves signiﬁcant
improvements over the state-of-the-arts.

1 Introduction
Image captioning has recently attracted extensive research attention with broad application prospects.
Most state-of-the-art image captioning models adopt an encoder-decoder architecture [1  2  3]  which
encodes the image into a feature representation via Convolutional Neural Network (CNN) and then
decodes the feature into a caption via Recurrent Neural Networks with Long-Short Term Memory
units (LSTM). Despite the exciting progress  one common defect is that the generated captions
are semantically synonymous and syntactically similar  which goes against the inherent diversity
delivered by the image  i.e.  “A picture is worth a thousand words”. Nevertheless  generating diverse
captions from a given image remains as an open problem. As shown in Fig. 1 (Left-Top)  it is quite
intuitive to derive heterogeneous understanding from human being  while the traditional models
typically tend to generate homogeneous sentences due to the limited variation in the maximum
likelihood objective [4].
Several recent works have been proposed to investigate diverse image captioning [5  6  7  8  9]  which
typically employed a Generative Adversarial Network (GAN) or Variational Auto-Encoder (VAE) as
the generative model. For example  [5] designed an adversarial model trained with an approximate
sampler to implicitly match the generated distribution to the human caption. For another instance 
[8] proposed a conditional VAE based captioning model guided by an object-wise prior  as roughly

∗

Corresponding author

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Illustration of diverse image captioning. Left: Captions generated by traditional image captioning
model (Left-Top)  state-of-the-art generative model (Left-Middle)  and our scheme that explicitly models lexi-
cal and syntactic diversities (Left-Bottom) for diverse image captioning. Right: Captions with higher diversity
are generated when the lexical (light blue) and syntactic (purple) diversities are considered.

shown in Fig. 1 (Left-Middle). However  all these methods treated diverse image captioning as a
blackbox without explicitly modeling the key factors to diversify the expression  i.e.  the lexical and
syntactic diversities  as revealed in the natural language research [10  11  12]  which in principle
involves identifying content entities and then expressing their relationships. Fig. 1 (Left-Bottom
and Right) shows an example of the lexical and syntactic diversities  both of which should be taken
into account for generating diverse image captions.
In this paper  we aim at explicitly modeling the lexical and syntactic diversities from the visual
content towards diversiﬁed image caption generation. To this end  we tackle two fundamental chal-
lenges  i.e.  diversity modeling and diversity embedding. For diversity modeling  we infer the lex-
ical and syntactic variables from the visual content by leveraging the visual parsing tree (VP-tree)
[13  14  15  16]  which predicts the probability distributions of the lexical and syntactic categories
to weight the latent variables in variational inferences. For diversity embedding  we advance the
commonly-used encoder-decoder scheme into a new structured encoder-inferer-decoder scheme 
where the aforementioned variational inference is treated as an inferer and its outputs  i.e.  the lexi-
cal and syntactic latent variables (with variations)  are sampled together with visual features to feed
a LSTM-based caption generator.
In particular  we propose a novel Variational Structured Semantic Inferring model for diverse image
captioning  termed VSSI-cap as illustrated in Fig. 2  which is deployed over VAE2 to model and
embed the lexical and syntactic diversities. In general  towards diversity modeling  such diversities
are inferred in the designed variational multi-modal inferring tree (termed  VarMI-tree). Towards
diversity embedding  such diversities are integrated into diverse image captioning in a new structured
encoder-inferer-decoder scheme. In particular  the proposed model contains three components: 1)
encoder: Given an image and its corresponding caption  the visual and textual features are extracted
by CNN and a word embedding model  respectively. 2) inferer: Inspired by the recent work in visual
semantic parsing [13]  a VarMI-tree is proposed to infer the latent variables with variations for the
lexical and syntactic diversities. 3) decoder: The visual feature  the inferred lexical and syntactic
variables (from posterior/prior inference)  are decoded to output the caption by using LSTM.
The contributions of this paper are as follows: 1) We are the ﬁrst to explicitly model diverse im-
age captioning based on the lexical and syntactic diversities. We address two key issues in diverse
captioning  i.e. diversity modeling and diversity embedding. 2) For diversity modeling  we pro-
pose a novel variational multi-modal inferring tree (VarMI-tree) to model the lexical and syntactic
diversities. 3) For diversity embedding  we propose a structured encoder-inferer-decoder scheme
which explicitly integrates the lexical and syntactic diversities in caption generation. 4) The pro-
posed VSSI-Cap beats the state-of-the-arts [5  8] on the MSCOCO benchmark dataset in terms of
both accuracy metrics and diversity metrics.
2 Preliminary
Image Captioning. We adopt an encoder-decoder architecture as the basic image captioning
model  where CNN is employed to encode an image I into a deep visual feature v and LSTM
is used to decode this visual feature into a caption S. Many state-of-the-art methods [2  3]
adopt a maximum likelihood principle to train the models by using the image-caption pair set

2Compared to other generative models  VAE can represent richer latent variables  which can also be trained

more easily.

2

mana man with a hat and a bird in his handa man is sitting on a bench with a birda man is holding a parrot in his handa man is sitting on a chair with a parrota man is holding a white parrot in handhatwithandbirdhandinPREPCONJNOUNPREPmanholdinhand(cid:28742)VERBPREPbirdmansitwithbirdVERBPREPchairOur scheme modeling lexicon & syntaxa man is sitting on a bench with a birda man is sitting on a bench next to a birdTraditional modellessdiversediversemore diversedifferent lexicon  similar syntaxambiguous variationNOUNNOUNNOUNNOUNNOUNNOUNNOUNNOUNNOUN(cid:28742)(cid:28742)(cid:28742)(cid:28742)(cid:28742)(cid:28742)(cid:28742)Encoder-DecoderTree 1Tree 2Tree 3Visual semantic parsingTree 1Tree 2Tree 3similar lexicon  different syntaxRecent generative modelFigure 2: Overview of the proposed VSSI-cap model for diverse image captioning  which consists of encoder 
inferer  and decoder. Given an image and its corresponding caption during training  visual feature v and
textual feature e are extracted from CNN and word embedding model respectively in encoder (brief in Sec.
3.2). In inferer (Sec. 3.2)  to represent the lexical/syntactic diversity  a VarMI-tree is designed to infer the
latent lexical/syntactic variable z(ℓ)/z(s) upon an additive Gaussian distribution in each node  where the means
(cid:22)1:K and the square deviations (sds) (cid:27)1:K over different lexical/syntactic components are parameterized upon
the node feature h  and subsequently weighted by the corresponding probability distributions c1:K from the
VP-tree (Sec. 3.1) for the additive Gaussian parameters (cid:22) and (cid:27). In decoder (Sec. 3.3)  ~z is sampled from the
′
posterior inference and is used for training while ~z
is sampled from the prior inference (similar to the posterior
inference but with (cid:22)1:K and (cid:27)1:K initialized randomly  detailed in Sec. 3.2) and is used to generate captions.
′
Finally  v and ~z/~z

are fed into LSTM for the sequential caption outputs.

Dp = {I (i)  S(i) = {S(i)
respectively. The corresponding objective function can be formulated as follows:

}Np
i=0  where Np and T denote the pair number and the caption length 

t

}T (i)

t=0

∑Np

∑Ti

i=0

t=0

(

)

log P (S|I) =

1
Np

log p

|v(i)  S(i)

0:t−1

S(i)
t

.

(1)

However  the above schemes are unsuitable for generating multiple diverse caption candidates due
to the certainty of encoding in Eq. 1. Therefore  generative models  such as GAN and VAE  are
typically exploited to handle diverse image captioning [5  6  7  8]. Other related topics include:
personalized expression [17  18]  stylistic description [19  20]  online context-aware heuristic search
[21  22]  and word-speciﬁc discriminative captioning [15] etc.

Variational Auto-Encoder (VAE). We brieﬂy present the variational auto-encoder (VAE) [23  24]
and its conditional variant [25  26]  which serves as the fundamental framework of the proposed
structured encoder-inferer-decoder scheme. Given an observed variable x  VAEs aim at modeling
the data likelihood p(x) based on the assumption that x is generated from a latent variable z  i.e. 
the decoder p(x|z)  which is typically estimated via deep nets. Since the posterior inference p(z|x)
is not computably tractable  it is approximated with a posterior inference q(z|x) that is typically a
diagonal Gaussian N (µ  diag(σ2))  where the mean µ and the square deviation σ can be parameter-
ized in deep nets and serve as the encoder3. Thus  the encoder/inferer and decoder can be optimized
by maximizing the following lower bound:

LVAE(θ  ϕ; x  c) = Eqϕ(z|x c)

log pθ(x|z  c)

qϕ(z|x  c)∥pθ(z|c)

[

] − DKL

(

) ≤ log pθ(x) 

(2)

where E and DKL are the approximate expectation and Kullback-Leibler (KL) divergence  respec-
tively. c denotes the condition  which exists in the case of conditional VAE (CVAE). ϕ and θ denote
the parameters of the inferer and the decoder (e.g.  LSTM)  respectively. For diverse image cap-
tioning  it’s a straightforward thinking to represent the visual feature and the caption with c and x
respectively in a VAE model. However  the latent variable z in such VAE model has a very gen-
eral prior (standard Gaussians)  which does not consider any domain-speciﬁc knowledge. We argue
that it may waste model capacity  and one should consider the unique problem structures of image
captioning instead of using the VAE as is.

3

childcar seatin(cid:2246)(cid:2869)(cid:4666)(cid:3039)(cid:4667)(cid:3037)(cid:2246)(cid:3012)(cid:4666)(cid:2194)(cid:4667)(cid:4666)(cid:3039)(cid:4667)(cid:3037)(cid:28663)(cid:2252)(cid:4666)(cid:2194)(cid:4667)(cid:3037)a young child in the back car seat talks on a phoneLSTMDecoderGeneration: Sample on prior(cid:2246)(cid:4666)(cid:2194)(cid:4667)(cid:3037)(cid:1826)(cid:4666)(cid:3039)(cid:4667)(cid:3037)(cid:2252)(cid:2869)(cid:4666)(cid:3039)(cid:4667)(cid:3037)(cid:2252)(cid:3012)(cid:4666)(cid:2194)(cid:4667)(cid:4666)(cid:3039)(cid:4667)(cid:3037)(cid:1808)(cid:3037)(cid:2246)(cid:2869)(cid:4666)(cid:3046)(cid:4667)(cid:3037)(cid:2246)(cid:3012)(cid:4666)(cid:2201)(cid:4667)(cid:4666)(cid:3046)(cid:4667)(cid:3037)(cid:28663)(cid:2252)(cid:4666)(cid:2201)(cid:4667)(cid:3037)(cid:2246)(cid:4666)(cid:2201)(cid:4667)(cid:3037)(cid:1826)(cid:4666)(cid:2201)(cid:4667)(cid:3037)(cid:2252)(cid:2869)(cid:4666)(cid:3046)(cid:4667)(cid:3037)(cid:2252)(cid:3012)(cid:4666)(cid:2201)(cid:4667)(cid:4666)(cid:3046)(cid:4667)(cid:3037)(cid:3556)(cid:1826)(cid:4593)CNNSubject 1Object 1Subject 2Object 2Sub-relation 1Sub-relation 2Root-Relationtalk onNULLphoneNULLEncoderInferer(VariationalMulti-modal Inferring Tree: VarMI-tree)Lexical variableSyntactic variableplayinPREPVERBParsing & EmbeddingPosterior inference in the j-thnode(cid:1803)(cid:4666)(cid:2194)(cid:4667)(cid:3037)(cid:1803)(cid:4666)(cid:2201)(cid:4667)(cid:3037)(cid:1803)(cid:3012)(cid:4666)(cid:2194)(cid:4667)(cid:4666)(cid:3039)(cid:4667)(cid:3037)(cid:1803)(cid:2869)(cid:4666)(cid:3039)(cid:4667)(cid:3037)Probabilitydistributions from VP-treeTraining:Sample onposterior(cid:3556)(cid:1826)sequentialoutput(cid:1822)(cid:1805)SI(cid:1803)(cid:3012)(cid:4666)(cid:2201)(cid:4667)(cid:4666)(cid:3046)(cid:4667)(cid:3037)(cid:1803)(cid:2869)(cid:4666)(cid:3046)(cid:4667)(cid:3037)Notation
I
S
v
ej
hj
z(ℓ)j/z(s)j
c(ℓ)j/c(s)j
(cid:22)(ℓ)j/(cid:22)(s)j
(cid:27)(ℓ)j/(cid:27)(s)j
/(cid:22)(s)j
(cid:22)(ℓ)j
/(cid:27)(s)j
(cid:27)(ℓ)j
θ
ϕ(ℓ)/ϕ(s)
ψ
′

k

k

k

k

Table 1: Main notations and their deﬁnitions.

Deﬁnition
an image
a caption
the visual feature
the j-node word embedding feature
the feature of the j-th node in VarMI-tree
the j-node lexical/syntactic latent variable
the j-node lexical(word’s)/syntactic(POS’s) probability distribution in VP-tree
the additive mean of the j-node lexical/syntactic posterior Gaussian distribution
the additive squ. dev. of the j-node lexical/syntactic posterior Gaussian distribution
the k-component mean of the j-node lexical/syntactic posterior Gaussian distribution
the k-component squ. dev. of the j-node lexical/syntactic posterior Gaussian distribution
the parameter set of the decoder
the lexical/syntactic parameter set of the inferer
the parameter set of VarMI-tree trunk
the mark for the prior

3 The Proposed VSSI-Cap Model
The framework of the proposed VSSI-Cap model is illustrated in Fig. 2. Following Eq. 2  the model
is in principle optimized by maximizing the lower bound on the log-likelihood of pθ(S) as below:

[

]
log pθ(S|z(ℓ)  z(s)  v  c(ℓ)  c(s))

)

L(θ  ϕ(ℓ)  ϕ(s)  ψ; S  v  c(ℓ)  c(s)) = E
− DKL

(
z(ℓ)∼q
qϕ(ℓ) ψ(z(ℓ)|S  v  c(ℓ))∥p(z(ℓ)|c(ℓ))

) − DKL

ϕ(ℓ)  ψ

(

 z(s)∼q

ϕ(s) ψ

qϕ(s) ψ(z(s)|S  v  c(s))∥p(z(s)|c(s))

 

(3)
which consists of two components  i.e.  the approximate expectation E and the KL divergence DKL.
The former is maximized to reduce the reconstruction loss of the caption generation in decoder as Eq.
1  while the later measures the difference between the distributions of the posterior qϕ ψ(z|S  v  c)
and the prior p(z|c) for the prior guidance (detailed in Sec. 3.3). Firstly  we deﬁne the variables
and parameters as following: “(ℓ)” and “(s)” are the marks for the variables and parameters of the
lexicon and the syntax respectively. v  z  and c denote the visual feature of the image I  the lexi-
cal/syntactic latent variable  and the lexical(word’s)/syntactic(POS’s) probability distribution from
VP-tree (see Fig. 3)  respectively. S denotes the caption  which is parsed and embedded into the
textual feature e.4 θ is the parameter set in the decoder  while ϕ and ψ are the parameter sets of the
lexical/syntactic posterior inference and the VarMI-tree trunk  respectively  in the inferer. Secondly 
we introduce the posterior/prior (Sec. 3.2) based on the above deﬁnitions: we adopt an additive Gaus-
sian distribution for the posterior/prior to infer the latent variables. As shown in the middle of Fig.
′  are derived from
2  the additive parameters  i.e.  the mean (cid:22)/(cid:22)
multiple component parameters (means and sds of multiple Gaussian distributions  corresponding
to different word’s and POS’s components and weighted by the probability distributions). Thirdly 
we describe the posterior/prior inference (Sec. 3.2): In the posterior inference  the additive and
component parameters are both parameterized by a linear function  while the component parameters
are initialized randomly in the prior inference as shown in the middle of Fig. 2. Here we omit the
prior inference due to the similarity to the posterior inference. The corresponding lexical-syntactic
′. Finally 
latent variable ~z/~z
′ are
during training/generation (detailed in Sec. 3.3)  the visual feature v and the latent variable ~z/~z
fed into LSTM to generate sequential caption outputs.
In the following  we brieﬂy introduce the lexicon-syntax based VP-tree in Sec. 3.1. We then give the
details about the proposed VarMI-tree in Sec. 3.2. Finally  in Sec. 3.3  we introduce the proposed
structured encoder-inferer-decoder schema. For clarity  the main notations and their deﬁnitions
throughout the paper are shown in Tab. 1.

′ is sampled from the posterior/prior inference by reparameterizating z/z

′ and the square deviation (sd) (cid:27)/(cid:27)

3In some complex tasks  e.g.  image captioning  q(zjx) is commonly termed as inferer to differ from the

visual encoder CNN.

4Textual parsing and pruning preprocesses are conducted by following [13] to obtain the tree structure.

4

3.1 Visual Parsing Tree
Visual parsing tree (VP-tree) is ﬁrstly pro-
posed in [13]  which serves as a robust parser
to discover visual entities and their relations
from a given image. To parse them in the lexi-
con and the syntax  we modify VP-tree as Fig.
3 (a)  where the probability distributions of
K (ℓ) words and K (s) POSs  i.e.  c(ℓ) ∈ RK(ℓ)
and c(s) ∈ RK(s)  are estimated in each node
for weighting in the subsequent VarMI-tree.
Figure 3: The examplar subtrees of VP-tree and VarMI-
There are M (typically  M = 7) tree nodes
tree. The differences lie in: 1) Single vs. multi-modal
in these two binary trees. To distinguish these
Semantic mapping. 2) Whether inferring the lexical and
two trees  we deﬁne the variables and param-
syntactic latent variables z(ℓ) and z(s) or not. 3) Proba-
eters of VP-tree with “
”. As shown in Fig.
bility distributions c(ℓ) and c(s) of the optimized VP-tree
3 (a)  VP-tree consists of three operations  Se-
are utilized for weighting in the inference of VarMI-tree.
mantic mapping  Node combining  and Clas-
sifying  where the ﬁrst two adopt normal linear mapping and concatenating operations upon visual
feature v to obtain the node feature h
of each node is mapped into the
¯
word’s and POS’s category spaces  respectively according to their vocabularies. For the j-th node 
we obtain its word and POS probability distributions  i.e.  c(ℓ)j and c(s)j  as follows:

. In Classifying  the feature h
¯

¯

r = f (cl)(W
c(ℓ)j
¯

r h
(ℓ)

¯ j + b
¯

s.t.(j : r) ∈ {(1 : E)  (3 : E)  (5 : E)  (7 : E)  (2 : R)  (6 : R)  (4 : R)}  

r = f (cl)(W
c(s)j
¯

r h
(s)

¯ j + b
¯

(s)
r ) 

(ℓ)
r ) 

(4)

for the entity (r =“E”) or the
where f (cl) is a Softmax function with parameters W
(cl)
r
¯
relation (r =“R”) classiﬁcations. We unify cj
r (r =“E” “R”) into cj for simpliﬁcation. During
training  cj is used to compute the cross entropy loss with the lexical/syntactic category labels4. The
parameter set is ﬁnally optimized for automatical tree construction given an image feature  where
each node provides the optimal word’s and POS’s probability distributions c(ℓ) and c(s). Note that
one can replace VP-Tree with other alternative visual structured representations for the lexicon and
syntax. However  in order to directly demonstrate the effectiveness of the core idea  we intentionally
chose the straightforward assistance of VP-Tree.

and b
¯

(cl)
r

3.2 Variational Multi-modal Inferring Tree
The major challenge of VSSI-cap is to model the posterior inference of both the lexical and syntactic
latent variables  i.e.  qϕ(ℓ) ψ(z(ℓ)|S  v  c(ℓ)) and qϕ(s) ψ(z(s)|S  v  c(s))  in the tree structure. To this
end  we design a variational multi-modal inferring tree (VarMI-tree) to further innovate the VP-tree
as illustrated in Fig. 3 (b) and Fig. 2 (Middle). VarMI-tree consists of three operations  i.e.  Semantic
mapping  Node combining  and Inferring. We itemize them as follows:

Semantic Mapping.
In the encoder  the visual feature v is extracted from the last fully-connected
layer of CNN [27] while the j-th word’s feature ej (j ∈ {1  . . .   M} corresponds to the j-th tree
node) of the caption S is extracted by textual parsing and word embedding as aforementioned. In the
inferer  these features are mapped into different semantic spaces  i.e.  subjects  objects  and relations
in VarMI-tree as shown in Fig. 3 and Fig. 2  which can be formulated as:

(

)

 

s.t.(j : r) ∈ {(1 : Subj1)  (3 : Obj1)  (5 : Subj2)  (7 : Obj2)}  

r

r

hj = f (sm)

W(sm)

[v; ej] + b(sm)

(5)

where r represents one of four semantic entity items  i.e.  subject 1  object 1  subject 2  and object 2
as set up in VP-tree. [·;·] is the concatenation operation. f (sm) denotes a non-linear function with
the parameters W (sm)
for Semantic mapping in the j-th node (j = 1  3  5  7). For the
non-leaf nodes (j = 2  4  6)  similar operation is conducted as above  where  however  v is replaced
with the combination features (computed in next part) as shown in Fig. 3.

and b(sm)

r

r

Node Combining. The Node combining operation of VarMI-tree is the same as the one of VP-tree.
Correspondingly  we denote the parameters with W(nc) and b(nc).

5

(Subject 1)(Object 1)(Sub-relation 1)childcar seatin(Subject 1)(Object 1)(Sub-relation 1)(cid:1822)(cid:1822)(cid:1805)(cid:2870)(cid:1805)(cid:2869)(cid:1805)(cid:2871)ClassifyingNode combiningSemantic mappingInferring(cid:1822)(cid:1822)(a) VP-tree(b) VarMI-tree(cid:1803)(cid:4666)(cid:2194)(cid:4667)(cid:2870)(cid:352)(cid:346)(cid:347)(cid:348)(cid:349)(cid:350)(cid:351)Node Index(cid:346)(cid:348)(cid:347)(cid:346)(cid:348)(cid:347)(cid:1826)(cid:4666)(cid:2194)(cid:4667)(cid:2870)(cid:1826)(cid:4666)(cid:2201)(cid:4667)(cid:2870)(cid:1826)(cid:4666)(cid:2194)(cid:4667)(cid:2869)(cid:1826)(cid:4666)(cid:2201)(cid:4667)(cid:2869)(cid:1826)(cid:4666)(cid:2194)(cid:4667)(cid:2871)(cid:1826)(cid:4666)(cid:2201)(cid:4667)(cid:2871)(cid:1808)(cid:2870)(cid:1808)(cid:2871)(cid:1808)(cid:2869)(cid:1803)(cid:4666)(cid:2194)(cid:4667)(cid:2869)(cid:1803)(cid:4666)(cid:2201)(cid:4667)(cid:2870)(cid:1808)(cid:2870)(cid:1808)(cid:2869)(cid:1808)(cid:2871)(cid:1803)(cid:4666)(cid:2201)(cid:4667)(cid:2871)(cid:1803)(cid:4666)(cid:2194)(cid:4667)(cid:2871)(cid:1803)(cid:4666)(cid:2201)(cid:4667)(cid:2869)Inferring. For clarity  we deﬁne the function Hj as a uniﬁed operation of the above Semantic
mapping and Node combining for the j-th node feature  i.e  hj = Hj(v  e; ψ). In the j-th node  the
lexical and syntactic posterior inferences can be approximated upon an additive Gaussian distribu-
tion. For clarity  we only formulate it on the lexicon below:

qϕ(ℓ) ψ(z(ℓ)j|S  v  c(ℓ)j) = N(

z(ℓ)j|

∑K(ℓ)

k=1

k (cid:22)(ℓ)j
c(ℓ)j

k

(Hj)  (cid:6)(ℓ)j2I

 

(6)

where (cid:6)(ℓ)j2I is the spherical covariance matrix with (cid:6)(ℓ)j2 =
notes the length of the word’s vocabulary. The component Gaussian parameters can be obtained:

K(ℓ)
k=1 c(ℓ)j

k (cid:27)(ℓ)j

(Hj)2. K (ℓ) de-

k

∑

(cid:22)(ℓ)j

k

(Hj) = W(ℓ)j
µk

Hj + b(ℓ)j
µk

 

log (cid:27)(ℓ)j

k

(Hj)2 = W(ℓ)j
σk

Hj + b(ℓ)j
σk

 

(7)

To enable the differentiability in the end-to-end manner  we reparameterize z(ℓ)j into ~z(ℓ)j via the
reparameterization trick [23] as:
(8)
where "(ℓ) obeys a standard Gaussian distribution to introduce noise for the lexical diversity. ⊙ is
an element-wise product. Similar to the posterior  the prior p(z(ℓ)j|c(ℓ)j) can be formulated as:

~z(ℓ)j = (cid:22)(ℓ)j + (cid:27)(ℓ) ⊙ "(ℓ) 

)

)

p(z(ℓ)j|c(ℓ)j) = N(

z(ℓ)j|

∑K(ℓ)

k=1

′(ℓ)
where (cid:22)
k

and (cid:27)

′(ℓ)
k

are randomly initialized. z

∑K(ℓ)

′(ℓ)j
c(ℓ)j
c(ℓ)j
k (cid:22)
k (cid:27)
k
′(ℓ)j is reparameterized into ~z

k=1

  (

′(ℓ)j2
k

)I

′(ℓ)j as Eq. 8.

 

(9)

3.3 Structured Encoder-inferer-decoder
The structured encoder-inferer-decoder schema aims at integrating the lexical/syntactic latent vari-
ables in a tree structure to diversify the generated captions. Following Eq. 3  we give the ﬁnal
objective function as follows:
−∑
LVSSI-Cap(θ  ϕ(ℓ)  ϕ(s)  ψ; S  v  c(ℓ)  c(s)) = Ed(θ; S  v  c(ℓ)  c(s))

)]
qϕ(s) ψ(z(s)j|S  v  c(s)j)∥p(z(s)j|c(s)j)

(
qϕ(ℓ) ψ(z(ℓ)j|S  v  c(ℓ)j)∥p(z(ℓ)j|c(ℓ)j)

+ DKL

(10)

DKL

[

)

(

 

M
j=1

where most of the above notations are deﬁned in Eq. 3. DKL can be approximated following [28] (see
algorithm ﬂow in supplementary material). Ed is the approximate expectation on the log-likelihood
of pθ(S|I) in decoder. For the reconstruction loss  we use Monte Carlo method to approximate the
expectation Ed in Eq. 10 after sampling ~z(ℓ)j and ~z(s)j  which is formulated as:

Ed =

1
N

j=1  v  c(ℓ)  c(s)) 
s.t. ∀i  j z(ℓ)j(i) ∼ qϕ(ℓ) ψ(z(ℓ)j|S  v  c(ℓ)j)  z(s)j(i) ∼ qϕ(s) ψ(z(s)j|S  v  c(s)j) 

t=0

i=1

log p(St|S0:t−1 {z(ℓ)j(i)}M

j=1 {z(s)j(i)}M

(11)

∑N

∑T

where N and T denote the sample number of z(i) (sampled by Eq. 8) and the length of the caption 
respectively. Since the objective function in Eq. 10 is differentiable  we optimize the model param-
eter set θ  ϕ(ℓ)  ϕ(s)  and ψ jointly using stochastic gradient ascent method. To generate captions 
we use the above optimal parameters and choose the t-th word ~St over the dictionary according to
~St = arg maxSt p(St|S0:t−1  z
′ are concatenated to feed the decoder.

′(s)  v)  where v and z

′(ℓ)  z

4 Experiments
Dataset and Metrics. We conduct all the experiments on the MSCOCO dataset5 [30]  which is
widely used for image captioning [1  3] and diverse image captioning [5  8]. There are over 93K
images in MSCOCO  which has been split into training  testing and validating sets6. Each image
has at least ﬁve manual captions. The quality of captioning results lies in both accuracy (a basic
evaluation of captioning quality and has been used together with the subsequent diversity metrics in
[8  5  6]) and diversity. For accuracy  we use the MSCOCO caption evaluation tool7 by choosing

5http://cocodataset.org/#download
6https://github.com/karpathy/neuraltalk
7https://github.com/tylin/coco-caption

6

Metric
ErDr-cap [29]
Up-Down [3]
G-GAN [6]
Adv [5]
CAL [9]
GMM-CVAE [8]
AG-CVAE [8]
VSSI-cap-L
VSSI-cap-S
VSSI-cap

Bleu-1
69.9
79.8

-
-

66.5
70.0
70.2
69.9
70.4
70.4

Bleu-2
51.8

Bleu-3
36.6

-
-
-

48.4
52.0
52.2
51.9
52.7
52.7

-

-

30.5

33.2
37.1
37.1
37.3
37.9
38.1

-

” respectively.
Bleu-4 Meteor
23.1
25.6
27.7
36.3
20.7
22.4
23.9
22.6
23.2
23.4
23.5
23.8
23.9

21.8
26.0
26.0
26.1
27.1
27.3

Rouge-L

50.3
56.9
47.5

-

47.8
50.6
50.6
50.7
51.1
51.3

CIDEr
84.3
120.1
79.5

-

75.3
85.4
85.7
87.3
88.8
89.4

Spice
16.4
21.4
18.2
16.7
16.4
16.3
16.5
16.8
17.0
17.1

Table 2: Performance comparisons on accuracy of diverse image captioning. All values are in %. The ﬁrst and
the second places are marked with the bold font and “

” respectively.
mB.+
51.0
78.0
80.9

Table 3: Performance comparisons on diversity. “+” and “*” denote that lower and higher are better  respec-
tively. “n” denotes the number of generated captions (default 5). All values are in %. The ﬁrst and the second
places are marked with the bold font and “
Num.
n=5
n=5
n=5
n=5
n=5
n=5
n=5
n=10
n=5
n=5
n=5
n=10

Metric
human
ErDr-cap [29]
Up-Down [3]
G-GAN [6]
Adv [5]
CAL [9]
AG-CVAE [8]
AG-CVAE [8]
VSSI-cap-L
VSSI-cap-S
VSSI-cap
VSSI-cap

44.0
40.7
42.9
31.3
45.6
46.3
47.2
33.2

34.0
32.5
33.1
22.7
34.3
33.8
33.9
22.3

79.67
79.68
79.30
80.26
85.20
80.34

div2*
48.0
38.0
35.8

div1*
34.0
28.0
27.1

66.9
70.8
80.2
82.4
83.0
80.7

70.2
77.3
68.7
63.0
62.4
74.2

34.18
63.60
81.52
73.92

Uni.*
99.8

Nov.*

70.0

-

-

-

-

-
-
-
-
-

-

-

the best-performing one from the top-5 outputs  including Bleu  Meteor  Rouge-L  CIDEr [30] and
Spice [31]. For diversity  we use the benchmark metrics in [5  8]: 1) Div1  the ratio of unique
unigrams to words in the generated captions. Higher div1 means more diverse. 2) Div2  the ratio
of unique bigrams to words in the generated captions. Higher div2 means more diverse. 3) mBleu
(mB.)  the mean of Bleu scores  which are computed between each caption in the generated captions
against the rest. Lower mB. means more diverse. 4) Unique Sentence (Uni.)  the average percentage
of unique captions in candidates generated for each image. 5) Novel Sentence (Nov.)  the percentage
of the generated captions that do not appear in the training set. For uniformity  each output caption
corresponds to a sample of z.

Preprocessing  Parameter Settings  and Implementation Details.
In the proposed VarMI-tree 
we set the feature dimension of each node as 512. The dimensions of each mean  each sd  and each
latent variable are set as 150. We parse the captions by using the Stanford Parser [32] as well as
pruning the textual parsing results by using the pos-tag tool and the lemmatizer tool in NTLK [33] 
where the dynamic textual parsing trees are converted to a ﬁxed-structured  three-layer  complete
binary tree as designed in [13]. Only the words (including entities and relations) and the POSs (i.e. 
NOUN  VERB  PREP  and CONJ) with high frequency are left to form the vocabularies. Nouns are
regarded as entities and used as leaf nodes in the textual parsing tree  while others (verbs  coverbs 
prepositions  and conjunctions) are taken as relations for non-leaf nodes. The sizes of the entity’s 
relation’s and POS’s vocabularies  are 840  248  and 4  respectively.8 We extract the visual features
from VGG-16 network [25]. In LSTM  we use the same vector dimensions of the hidden states as
[29]  which is set as 512. We set the word vector dimension as 256 during word embedding. We
implement our model training based on the public code9 with the standard data split and the separate
z samples. KL annealing method [34] is adopted to reduce the KL vanishing (see the supplementary
material for the training details). All networks are trained with SGD with a learning rate 0.005 for
the ﬁrst 5 epochs  and is reduced by half every 5 epochs. On average  all models converge within 50
epochs. The overall process takes 37 hours on a NVIDIA GeForce GTX 1080 Ti GPU with 11GB
memory.

8https://github.com/cfh3c/NeurIPS19_VPtree_Dics
9https://github.com/yiyang92/vae_captioning

7

Baselines and Competing Methods. We compare the proposed VSSI-cap with four baselines: 1)
ErDr-cap: a caption generator trained based on encoder-decoder (beam search) [29] that represents
the mainstream of general image captioning. 2) AG-CVAE [8]: a recent generative model consider-
ing the variation over detected objects for diverse image captioning. 3) VSSI-cap-L: an alternative
version of VSSI-cap  which omits the syntax. 4) VSSI-cap-S: an alternative version of VSSI-cap 
which omits the lexicon. We also compare VSSI-cap with the state-of-the-art method Adv [5] and
AG-CVAE [8] (evaluated on the aforementioned universal split). Besides  we compare VSSI-cap
with 1) other recent diverse image captioning methods  including G-GAN [6]  GMM-CVAE [8]  and
CAL [9]  2) the state-of-the-art image captioning method  i.e.  Up-Down (beam search) [3]  and 3)
Human: a sentence randomly sampled from ground-truth/manually-labeled annotations of each im-
age is used as the output of this method. Note that comparing to pure image captioning methods
(only aiming at accuracy) seems far-fetched due to the mutual interference between accuracy and di-
versity (a more diverse caption tends to be more inconsistent with the ground truth caption) [9  6  5] 
where  therefore  the pure image captioning methods are taken as extraessential references.

Evaluation on Accuracy. Tab. 2 presents the accuracy comparisons of our VSSI-cap to the base-
lines and state-of-the-arts. Compared to others (except the state-of-the-art image captioning method) 
VSSI-cap achieves the best performance under most metrics. Specially  VSSI-cap outperforms AG-
CVAE under all metrics  e.g.  89.4% vs. 85.7% on CIDEr  which reﬂects the superiority of visual
semantic representation in the proposed VarMI-tree. Additionally  the propsoed structured encoder-
inferer-decoder schema also contributes to the improvement of accuracy according to the comparison
with ErDr-cap. Particularly  the gaps become larger from Bleu-1 to Bleu-4 (from 1-gram to 4-gram) 
manifesting the superiority of the structured semantic representation in VSSI-cap. In summary  al-
though VSSI-cap is designed for diverse captioning  the various but accurate visual semantic is well
captured in the lexical and syntactic parsing results  which promotes the accuracy of VSSI-cap in
the task of general image captioning.

Evaluation on Diversity. We compare the
proposed VSSI-cap to the baseline and state-
of-the-art methods on the diversity metrics in
Tab. 3 shows. Despite there is a gap on the di-
versity when compared to the human captions 
VSSI-cap achieves the best performance com-
pared to other learning methods under most
metrics  e.g.  the best 62.4% on mBleu (lower
is better)  which reﬂects the effectiveness of
considering both the lexical and syntactic di-
versities in diverse image captioning  as well
as the superiority of the proposed VarMI-tree
based inferer on modeling these diversities.
Specially  VSSI-cap-L and VSSI-cap-S also achieve competitive performance. This manifests the
signiﬁcant roles of the lexical and syntactic diversities respectively. We conduct additional compar-
isons on the results with 10 generated captions (5 is default)  where VSSI-cap (n=10) also outper-
forms AG-CVAE (n=10). We further retrieve the images of the generated captions in 5 000 images
(randomly selected) by taking the captions as queries. The recalls of the ranking results are shown
in Fig. 4  where our VSSI-cap is shown to provide more discriminative descriptions  outperforming
others by a large margin across all cases. To qualitatively compare the performances on diversity 
we output the results of VSSI-cap and the baselines of ErDr-cap and AG-CVAE (also a state-of-the-
art) in Fig. 5. Clearly  VSSI-cap generates more diverse captions  which further demonstrates the
superiority of the proposed VSSI-cap.

Figure 4: The recalls of image rankings for different
methods. Given the generated caption queries  R@k is
the ratio of correct images being ranked within the top k
results. The left is based on the similarity (Left) between
the generated caption ~S and each image I  while the right
is based on the log-likelihoods (Right) P ( ~SjI)  computed
in different methods.

Model Analysis.
It’s a challenge to analyze the internal mechanism of the VAE-based structured
encoder-inferer-decoder due to different vector spaces among 1) different node features  2) differ-
ent Gaussian functional parameters  and 3) different lexical/syntactic variables over different nodes.
Fortunately  the parsing results of VP-tree can be assigned with different probability distributions
(inputs of VarMI-tree) in each node to indirectly verify the effectiveness of the VarMI-tree  as shown
in Fig. 6. Highly diverse captions are generated derived from different visual parsing trees with
different lexical/syntactic probability distributions. This demonstrates the effectiveness of VarMI-

8

13 510k0102030405060R@kErDr-capAG-CVAEVSSI-cap13 510k0102030405060R@kErDr-capAG-CVAEVSSI-capFigure 5: Visualization of diverse captions (top 3) generated by ErDr-cap (blue)  AG-CVAE (green)  and our
VSSI-cap (red). More results are presented in the supplementary material.

Figure 6: Internal view on the effectiveness of the VarMI-tree by changing its inputs explicitly  i.e.  assigning
different lexical/syntactic probability distributions of each node from VP-tree (refer to Fig. 3 for the node index).
The histograms of each example reﬂect different visual parsing trees with different probability distributions
assigned in each node  where the middle is for the original parsing results (best-in-top3 is shown in each node)
from VP-tree  while the left/right is for the parsing results partly changed from the middle mainly on word/POS.
Captions are generated according to different visual parsing trees at the bottom.

tree on modeling the lexical/syntactic diversity and embedding them into caption generation in the
proposed structured encoder-inferer-decoder.

5 Conclusion
In this paper  we exploit the key factors of diverse image captioning  i.e.  the lexical and syntactic
diversities. To model these two diversities into image captioning  we propose a variational structured
semantic inferring model (VSSI-cap) with a novel variational multi-modal inferring tree (VarMI-
tree) on a structured encoder-inferer-decoder schema. Specially  conditioned on the visual-textual
features from encoder  VarMI-tree models the lexicon and the syntax  as well as inferring their latent
variables in approximate posterior inference guided by the visual prior. Reconstruction loss and
KL-divergence are jointly estimated to optimize the VSSI-cap model to generate diverse captions.
Experiments on benchmark dataset demonstrate that the proposed VSSI-cap achieves signiﬁcant
improvements over the state-of-the-arts.

Acknowledgments
This work is supported by the National Key R&D Program (No.2017YFC0113000  and
No.2016YFB1001503)  Nature Science Foundation of China (No.U1705262  No.61772443 
No.61572410  and No.61702136)  Post Doctoral Innovative Talent Support Program under Grant
BX201600094  China Post-Doctoral Science Foundation under Grant 2017M612134  Scientiﬁc Re-
search Project of National Language Committee of China (Grant No. YB135-49)  and Nature Sci-
ence Foundation of Fujian Province  China (No. 2017J01125 and No. 2018J01106).

9

a man sitting at a table with a plate of fooda man is eating a meal with a wine glassa man sitting at a table with a sandwicha man sitting at a table with a plate of fooda man sitting at a table with a slice of pizzaa man holding a plate of food in his handa man sitting at a table with a wine glassa man sitting at a table with a sandwich a cat sitting on a table with a laptop on ita black and white cat sitting on a beda cat sitting on a bed with a booka cat sitting on a bed with a laptop on ita cat sitting on a bed with a laptopa cat sitting on a table with a laptop on ita cat is sitting on a bed with a book and a phonea black and white cat sitting on a beda cat sitting on top of a bed with a phonea man and a woman are eating a sandwicha city street with people walking down the streeta man walking down a street with umbrellaa city street with people walking and umbrellasa man is walking down a street with umbrellaa man walking down a street with umbrellaa man walking down a street with a street signa street scene with people walking and umbrellasa person walking down a street with a street signa city street with people walking down the streeta couple of giraffes are standing in a zooa group of people standing around a fencetwo giraffes are standing in a fenced areaa couple of giraffes are standing in a zooa group of giraffes are standing in a zooa giraffe is standing in a zoo enclosuretwo giraffes standing next to a fence in a zootwo large black and white giraffes standing next to a mana giraffe is standing in the middle of a zooalargewhiteplatewithasandwichandaknifeaplateoffoodwithaforkandahotdogahotdogonaplatewithaforkandaknifeOriginal wordOriginal POSChanged word/POSacoupleofpeoplesmilingatthecameranexttoatableamanandawomanareusingalaptopnexttoatableawomaninadressnexttoatableOriginal wordOriginal POSChanged word/POSReferences
[1] Quanzeng You  Hailin Jin  Zhaowen Wang  Chen Fang  and Jiebo Luo. Image captioning with semantic

attention. In CVPR  pages 4651–4659  2016.

[2] Jiasen Lu  Caiming Xiong  Devi Parikh  and Richard Socher. Knowing when to look: Adaptive attention

via a visual sentinel for image captioning. In CVPR  pages 375–383  2017.

[3] Peter Anderson  Xiaodong He  Chris Buehler  Damien Teney  Mark Johnson  Stephen Gould  and Lei
Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR 
pages 6077–6086  2018.

[4] Jacob Devlin  Saurabh Gupta  Ross Girshick  Margaret Mitchell  and C Lawrence Zitnick. Exploring

nearest neighbor approaches for image captioning. arXiv preprint arXiv:1505.04467  2015.

[5] Rakshith Shetty  Marcus Rohrbach  Lisa Anne Hendricks  Mario Fritz  and Bernt Schiele. Speaking the
same language: Matching machine to human captions by adversarial training. In ICCV  pages 4155–4164 
2017.

[6] Bo Dai  Sanja Fidler  Raquel Urtasun  and Dahua Lin. Towards diverse and natural image descriptions

via a conditional gan. In ICCV  pages 2989–2998  2017.

[7] Unnat Jain  Ziyu Zhang  and Alexander Schwing. Creativity: Generating diverse questions using varia-

tional autoencoders. In CVPR  pages 5415–5424  2017.

[8] Liwei Wang  Alexander Schwing  and Svetlana Lazebnik. Diverse and accurate image description using a
variational auto-encoder with an additive gaussian encoding space. In NeurIPS  pages 5756–5766  2017.

[9] Dianqi Li  Qiuyuan Huang  Xiaodong He  Lei Zhang  and Ming-Ting Sun. Generating diverse and accu-
rate visual captions by comparative adversarial learning. In NeurIPS workshop on ViGIL  pages 27:1–6 
2018.

[10] Paul Martin Lester. Syntactic theory of visual communication. Retrieved December  3:1–14  2006.

[11] Pamela A Hadley  Megan M McKenna  and Matthew Rispoli. Sentence diversity in early language de-
velopment: Recommendations for target selection and progress monitoring. American journal of speech-
language pathology  27(2):553–565  2018.

[12] Marjorie Meecham and Janie Rees-Miller. Language in social contexts. Contemporary Linguistics  pages

537–590  2005.

[13] Fuhai Chen  Rongrong Ji  Jinsong Su  Yongjian Wu  and Yunsheng Wu. Structcap: Structured semantic

embedding for image captioning. In ACM MM  pages 46–54  2017.

[14] Xian Wu  Guanbin Li  Qingxing Cao  Qingge Ji  and Liang Lin.
trajectory structured localization. In CVPR  pages 6829–6837  2018.

Interpretable video captioning via

[15] Fuhai Chen  Rongrong Ji  Xiaoshuai Sun  Yongjian Wu  and Jinsong Su. Groupcap: Group-based image

captioning with structured relevance and diversity constraints. In CVPR  pages 1345–1353  2018.

[16] Bo Dai  Sanja Fidler  and Dahua Lin. A neural compositional paradigm for image captioning. In NeurIPS 

pages 656–666  2018.

[17] Cesc Chunseong Park  Byeongchang Kim  and Gunhee Kim. Attend to you: Personalized image caption-

ing with context sequence memory networks. In CVPR  pages 895–903  2017.

[18] Zhuhao Wang  Fei Wu  Weiming Lu  Jun Xiao  Xi Li  Zitong Zhang  and Yueting Zhuang. Diverse image

captioning via grouptalk. In IJCAI  pages 2957–2964  2016.

[19] Chuang Gan  Zhe Gan  Xiaodong He  Jianfeng Gao  and Li Deng. Stylenet: Generating attractive visual

captions with styles. In CVPR  pages 3137–3146  2017.

[20] Alexander Mathews  Lexing Xie  and Xuming He. Semstyle: Learning to generate stylised image captions

using unaligned text. In CVPR  pages 8591–8600  2018.

[21] Ashwin K Vijayakumar  Michael Cogswell  Ramprasath R Selvaraju  Qing Sun  Stefan Lee  David Cran-
dall  and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models.
arXiv preprint arXiv:1610.02424  2016.

10

[22] Ramakrishna Vedantam  Samy Bengio  Kevin Murphy  Devi Parikh  and Gal Chechik. Context-aware

captions from context-agnostic supervision. In CVPR  pages 251–260  2017.

[23] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 

2013.

[24] Danilo Jimenez Rezende  Shakir Mohamed  and Daan Wierstra. Stochastic backpropagation and approx-

imate inference in deep generative models. In ICML  pages 1278–1286  2014.

[25] Kihyuk Sohn  Honglak Lee  and Xinchen Yan. Learning structured output representation using deep

conditional generative models. In NeurIPS  pages 3483–3491  2015.

[26] Xinchen Yan  Jimei Yang  Kihyuk Sohn  and Honglak Lee. Attribute2image: Conditional image genera-

tion from visual attributes. In ECCV  pages 776–791  2016.

[27] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recog-

nition. arXiv preprint arXiv:1409.1556  2014.

[28] John R Hershey and Peder A Olsen. Approximating the kullback leibler divergence between gaussian

mixture models. In ICASSP  volume 4  pages IV–317  2007.

[29] Oriol Vinyals  Alexander Toshev  Samy Bengio  and Dumitru Erhan. Show and tell: A neural image

caption generator. In CVPR  pages 3156–3164  2015.

[30] Xinlei Chen  Hao Fang  Tsung-Yi Lin  Ramakrishna Vedantam  Saurabh Gupta  Piotr Dollár  and
C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325  2015.

[31] Peter Anderson  Basura Fernando  Mark Johnson  and Stephen Gould. Spice: Semantic propositional

image caption evaluation. In ECCV  pages 382–398  2016.

[32] Richard Socher  Cliff C Lin  Chris Manning  and Andrew Y Ng. Parsing natural scenes and natural

language with recursive neural networks. In ICML  pages 129–136  2011.

[33] Edward Loper and Steven Bird. Nltk: the natural language toolkit. arXiv preprint cs/0205028  2002.

[34] Samuel R Bowman  Luke Vilnis  Oriol Vinyals  Andrew M Dai  Rafal Jozefowicz  and Samy Bengio.

Generating sentences from a continuous space. In CoNLL  pages 10–21  2016.

11

,Chongxuan Li
Jun Zhu
Tianlin Shi
Bo Zhang
Fuhai Chen
Rongrong Ji
Jiayi Ji
Xiaoshuai Sun
Baochang Zhang
Xuri Ge
Yongjian Wu
Feiyue Huang
Yan Wang