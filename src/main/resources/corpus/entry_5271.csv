2012,Stochastic optimization and sparse statistical recovery: Optimal algorithms for high dimensions,We develop and analyze stochastic optimization algorithms for problems in which the expected loss is strongly convex  and the optimum is (approximately) sparse. Previous approaches are able to exploit only one of these two structures  yielding a $\order(\pdim/T)$ convergence rate for strongly convex objectives in $\pdim$ dimensions and $\order(\sqrt{\spindex( \log\pdim)/T})$ convergence rate when the optimum is $\spindex$-sparse. Our algorithm is based on successively solving a series of $\ell_1$-regularized optimization problems using Nesterov's dual averaging algorithm. We establish that the error of our solution after $T$ iterations is at most $\order(\spindex(\log\pdim)/T)$  with natural extensions to approximate sparsity. Our results apply to locally Lipschitz losses including the logistic  exponential  hinge and least-squares losses. By recourse to statistical minimax results  we show that our convergence rates are optimal up to constants. The effectiveness of our approach is also confirmed in numerical simulations where we compare to several baselines on a least-squares regression problem.,Stochastic optimization and sparse statistical

recovery: Optimal algorithms for high dimensions

Alekh Agarwal

Microsoft Research

New York NY

Sahand N. Negahban

Dept. of EECS

MIT

Martin J. Wainwright

Dept. of EECS and Statistics

UC Berkeley

alekha@microsoft.com

sahandn@mit.edu

wainwrig@stat.berkeley.edu

Abstract

We develop and analyze stochastic optimization algorithms for problems in which
the expected loss is strongly convex  and the optimum is (approximately) sparse.
Previous approaches are able to exploit only one of these two structures  yielding
a O(d/T ) convergence rate for strongly convex objectives in d dimensions and
O(ps(log d)/T ) convergence rate when the optimum is s-sparse. Our algorithm
is based on successively solving a series of ℓ1-regularized optimization problems
using Nesterov’s dual averaging algorithm. We establish that the error of our solu-
tion after T iterations is at most O(s(log d)/T )  with natural extensions to approx-
imate sparsity. Our results apply to locally Lipschitz losses including the logistic 
exponential  hinge and least-squares losses. By recourse to statistical minimax
results  we show that our convergence rates are optimal up to constants. The ef-
fectiveness of our approach is also conﬁrmed in numerical simulations where we
compare to several baselines on a least-squares regression problem.

1 Introduction

Stochastic optimization algorithms have many desirable features for large-scale machine learning 
and have been studied intensively in the last few years (e.g.  [18  4  8  22]). The empirical efﬁciency
of these methods is backed with strong theoretical guarantees on their convergence rates  which
depend on various structural properties of the objective function. More precisely  for an objective
function that is strongly convex  stochastic gradient descent enjoys a convergence rate ranging from
O(1/T )  when features vectors are extremely sparse  to O(d/T )  when feature vectors are dense [9 
14  10]. This strong convexity condition is satisﬁed for many common machine learning problems 
including boosting  least squares regression  SVMs and generalized linear models among others.

A complementary condition is that of (approximate) sparsity in the optimal solution. Sparse models
have proven useful in many applications (see e.g.  [6  5] and references therein)  and many statistical
procedures seek to exploit such sparsity. It has been shown [15  19] that when the optimal solution θ∗

Srebro et al. [20] exploit the smoothness of common loss functions  and obtain improved rates of

is s-sparse  appropriate versions of the mirror descent algorithm converge at a rate O(sp(log d)/T ).
the form O(ηp(s log d)/T )  where η is the noise variance. While the √log d scaling makes these
slow—namely  O(1/√T ) as opposed to O(1/T ) for strongly convex problems.

methods attractive in high dimensions  their scaling with respect to the iterations T is relatively

Many optimization problems encountered in practice exhibit both features: the objective function is
strongly convex  and the optimum is (approximately) sparse. This fact leads to the natural question:
is it possible to design algorithms for stochastic optimization that enjoy the best features of both
types of structure? More speciﬁcally  an algorithm should have a O(1/T ) convergence rate  as well
as a logarithmic dependence on dimension. The main contribution of this paper is to answer this
question in the afﬁrmative  and to analyze a new algorithm that has convergence rate O((s log d)/T )

1

for a strongly convex problem with an s-sparse optimum in d dimensions. This rate is unimprovable
(up to constants) in our setting  meaning that no algorithm can converge at a substantially faster rate.
Our analysis also yields optimal rates when the optimum is only approximately sparse.

The algorithm proposed in this paper builds off recent work on multi-step methods for strongly
convex problems [11  10  12]  but involves some new ingredients so as to obtain optimal rates for
statistical problems with sparse optima. In particular  we form a sequence of objective functions
by decreasing the amount of regularization as the optimization algorithm proceeds which is quite
natural from a statistical viewpoint. Each step of our algorithm can be computed efﬁciently  with a
closed form update rule in many common examples. In summary  the outcome of our development
is an optimal one-pass algorithm for many structured statistical problems in high dimensions  and
with computational complexity linear in the sample size. Numerical simulations conﬁrm our theo-
retical predictions regarding the convergence rate of the algorithm  and also establish its superiority
compared to regularized dual averaging [22] and stochastic gradient descent algorithms. They also
conﬁrm that a direct application of the multi-step method of Juditsky and Nesterov [11] is inferior
to our algorithm  meaning that our gradual decrease of regularization is quite critical. More details
on our results and their proofs can be found in the full-length version of this paper [2].

2 Problem set-up and algorithm description

Given a subset Ω ⊆ Rd and a random variable Z taking values in a space Z  we consider an
optimization problem of the form
(1)
where L : Ω × Z → R is a given loss function. As is standard in stochastic optimization  we do
not have direct access to the expected loss function L(θ) := E[L(θ; Z)]  nor to its subgradients.
Rather  for a given query point θ ∈ Ω  we observe a stochastic subgradient  meaning a random
vector g(θ) ∈ Rd such that E[g(θ)] ∈ ∂L(θ). The goal of this paper is to design algorithms that are
suitable for solving the problem (1) when the optimum θ∗ is (approximately) sparse.

θ∗ ∈ arg min
θ∈Ω

E[L(θ; Z)] 

Algorithm description:
consider a sequence of regularized problems of the form

In order to solve a sparse version of the problem (1)  our strategy is to

(2)

min

θ∈Ω′(cid:8)L(θ) + λkθk1(cid:9).

i=1  and

i=1  which specify the con-

i=1 and d-dimensional vectors {yi}KT

Our algorithm involves a sequence of KT different epochs  where the regularization parameter λ > 0
and the constraint set Ω′ ⊂ Ω change from epoch to epoch. The epochs are speciﬁed by:
• a sequence of natural numbers {Ti}KT
i=1  where Ti speciﬁes the length of the ith epoch 
• a sequence of positive regularization weights {λi}KT
• a sequence of positive radii {Ri}KT
straint set  Ω(Ri) :=(cid:8)θ ∈ Ω | kθ − yikp ≤ Ri(cid:9)  that is used throughout the ith epoch.
We initialize the algorithm in the ﬁrst epoch with y1 = 0  and with any radius R1 that is an up-
per bound on kθ∗k1. The norm k · kp used in deﬁning the constraint set Ω(Ri) is speciﬁed by
p = 2 log d/(2 log d − 1)  a choice that will be clariﬁed momentarily.
The goal of the ith epoch is to update yi
kyi+1 − θ∗k2
that upon termination  kyKT − θ∗k2
of the stochastic dual averaging algorithm [17] (henceforth DA) on the regularized objective

7→ yi+1  in such a way that we are guaranteed that
i /2  so
1/2KT−1. In order to update yi 7→ yi+1  we run Ti rounds
(3)

i+1 for each i = 1  2  . . .. We choose the radii such that R2

θ∈Ω(Ri)(cid:8)L(θ) + λikθk1(cid:9).
The DA method generates two sequences of vectors {µt}Ti
t=0 initialized as µ0 = 0
and θ0 = yi  using a sequence of step sizes {αt}Ti
t=0. At iteration t = 0  1  . . .   Ti  we let gt be a
stochastic subgradient of L at θt  and we let νt be any element of the subdifferential of the ℓ1-norm
k · k1 at θt. The DA update at time t maps (µt  θt) 7→ (µt+1  θt+1) via the recursions
(4)

µt+1 = µt + gt + λiνt  and θt+1 = arg min

t=0 and {θt}Ti

1 ≤ R2
min

1 ≤ R2

i+1 = R2

θ∈Ω(Ri)(cid:8)αt+1hµt+1  θi + ψyi Ri(θ)(cid:9) 

2

where the prox function ψ is speciﬁed below (5). The pseudocode describing the overall procedure
is given in Algorithm 1. In the stochastic dual averaging updates (4)  we use the prox function

ψyi Ri(θ) =

1

2R2

i (p − 1) kθ − yik2

p  where p =

.

(5)

2 log d
2 log d − 1

This particular choice of the prox-function and the speciﬁc value of p ensure that the function ψ
is strongly convex with respect to the ℓ1-norm  and has been previously used for sparse stochastic
optimization (see e.g. [15  19  7]). In most of our examples  Ω = Rd and owing to our choice of the
prox-function and the feasible set in the update (4)  we can compute θt+1 from µt+1 in closed form.
Some algebra yields that the update (4) with Ω = Rd is equivalent to

R2

i αt+1

θt+1 = yi +

kµt+1k(q−2)

q

(p − 1)(1 + ξ)

αt+1kµt+1kqRi

|µt+1|(q−1)sign(µt+1)

  where ξ = max(cid:26)0 

− 1(cid:27) .
Here |µt+1|(q−1) refers to elementwise operations and q = p/(p − 1) is the conjugate exponent to
p. We observe that our update (4) computes a subgradient of the ℓ1-norm rather than computing an
exact prox-mapping as in some previous methods [16  7  22]. Computing such a prox-mapping for
yi 6= 0 requires O(d2) computation  which is why we adopt the update (4) with a complexity O(d).
Algorithm 1 Regularization Annealed epoch Dual AveRaging (RADAR)
Require: Epoch length schedule {Ti}KT

i=1  initial radius R1  step-size multiplier α  prox-function

p − 1

ψ  initial prox-center y1  regularization parameters λi.
for Epoch i = 1  2  . . .   KT do

Initialize µ0 = 0 and θ0 = yi.
for Iteration t = 0  1  . . .   Ti − 1 do
Update (µt  θt) 7→ (µt+1  θt+1) according to rule (4) with step size αt = α/√t.
end for
Set yi+1 =
Update R2

PTi
t=1 θt
Ti

i+1 = R2

.
i /2.

end for
Return yKT +1

Conditions: Having deﬁned our algorithm  we now discuss the conditions on the objective func-
tion L(θ) and stochastic gradients that underlie our analysis.
Assumption 1 (Locally Lipschitz). For each R > 0  there is a constant G = G(R) such that

|L(θ) − L(˜θ)| ≤ Gkθ − ˜θk1

for all pairs θ  ˜θ ∈ Ω such that kθ − θ∗k1 ≤ R and k˜θ − θ∗k1 ≤ R.
We note that it sufﬁces to have k∇L(θ)k∞ ≤ G(R) for the above condition. As mentioned  our
goal is to obtain fast rates for objectives satisfying a local strong convexity condition  deﬁned below.
Assumption 2 (Local strong convexity (LSC)). The function L : Ω → R satisﬁes a R-local form
of strong convexity (LSC) if there is a non-negative constant γ = γ(R) such that

L(˜θ) ≥ L(θ) + h∇L(θ)  ˜θ − θi +

γ
2kθ − ˜θk2

2 ∀θ  ˜θ ∈ Ω with kθk1 ≤ R and k˜θk1 ≤ R.

(7)

Some of our results regarding stochastic optimization from a ﬁnite sample will use a weaker form of
the assumption  called local RSC  exploited in our recent work on statistics and optimization [1  13].
Our ﬁnal assumption is a tail condition on the error in stochastic gradients: e(θ) := g(θ) − E[g(θ)].
Assumption 3 (Sub-Gaussian stochastic gradients). There is a constant σ = σ(R) such that

E(cid:2) exp(ke(θ)k2

∞/σ2)(cid:3) ≤ exp(1) for all θ such that kθ − θ∗k1 ≤ R.

Clearly  this condition holds whenever the error vector e(θ) has bounded components. More gener-
ally  the bound (8) holds whenever each component of the error vector has sub-Gaussian tails.

(6)

(8)

3

Some illustrative examples: We now describe some examples that satisfy the above conditions to
illustrate how the various parameters of interest might be obtained in different scenarios.
Example 1 (Classiﬁcation under Lipschitz losses). In binary classiﬁcation  the samples consist of
pairs z = (x  y) ∈ Rd × {−1  1}. Common choices for the loss function L(θ; z) are the hinge loss
max(0  1− yhθ  xi) or the logistic loss log(1 + exp(−yhθ  xi). Given a distribution P over Z (either
the population or the empirical distribution)  a common strategy is to draw (xt  yt) ∼ P at iteration
t and use gt = ∇L(θ; (xt  yt)). We now illustrate how our conditions are satisﬁed in this setting.
• Locally Lipschitz: Both the above examples actually satisfy a stronger global Lipschitz condition
since we have the bound G ≤ k∇L(θ)k∞ ≤ Ekxk∞. Often  the data satisﬁes the normalization
kxk∞ ≤ B  in which case we get G ≤ B. More generally  tail conditions on the marginal
distribution of each coordinate of x ensure G = O(√log d)) is valid with high probability.
• LSC: When the expectation in the objective (1) is under the population distribution  the above
examples satisfy LSC. Here we focus on the example of the logistic loss  where we deﬁne the link
function ψ(α) = exp(α)/(1 + exp(α))2. We also deﬁne Σ = E[xxT ] to be the covariance matrix
and let σmin(Σ) denote its minimum singular value. Then a second-order Taylor expansion yields

L(˜θ) − L(θ) − h∇L(θ)  ˜θ − θi =
kθ − ˜θk2
2 
whereeθ = aθ + (1 − a)˜θ for some a ∈ (0  1). Hence γ ≥ ψ(BR)σmin(Σ) in this example.
• Sub-Gaussian gradients: Assuming the bound Ekxk∞ ≤ B  this condition is easily veriﬁed. A
simple calculation yields σ = 2B  since

kΣ1/2(θ − ˜θ)k2

2 ≥

2

ψ(BR)σmin(Σ)

ψ(heθ  xi)

2

ke(θ)k∞ = k∇L(θ; (x  y)) − ∇L(θ)k∞ ≤ k∇L(θ; (x  y))k∞ + k∇L(θ)k∞ ≤ 2B.

Example 2 (Least-squares regression). In the regression setup  we are given samples of the form
z = (x  y) ∈ Rd × R. The loss function of interest is L(θ; (x  y)) = (y − hθ  xi)2/2. To illustrate
the conditions more clearly  we assume that our samples are generated as y = hx  θ∗i + w  where
w ∼ N (0  η2) and ExxT = Σ so that EL(θ; (x  y)) = kΣ1/2(θ − θ∗)k2
• Locally Lipschitz: For this example  the Lipschitz parameter G(R) depends on the bound R.
If we deﬁne ρ(Σ) = maxi Σii to be the largest variance of a coordinate of x  then a direct
calculation yields the bound G(R) ≤ ρ(Σ)R.
• LSC: Again we focus on the case where the expectation is taken under the population distribu-
tion  where we have γ = σmin(Σ).
• Sub-Gaussian gradients: Once again we assume that kxk∞ ≤ B. It can be shown with some
work that Assumption 3 is satisﬁed with σ2(R) = 8ρ(Σ)2R2 + 4B4R2 + 10B2η2.

2/2.

3 Main results and their consequences

In this section we state our main results  regarding the convergence of Algorithm 1. We focus on
the cases where Assumptions 1 and 3 hold over the entire set Ω  and RSC holds uniformly for
all kθk1 ≤ R1; key examples being the hinge and logistic losses from Example 1. Extensions to
examples such as least-squares loss  which are not Lipschitz on all of Ω require a more delicate
treatment and these results as well the proofs of our results can be found in the long version [2].
Formally  we assume that G(R) ≡ G and σ(R) ≡ σ in Assumptions 1 and 3. We also use γ to
denote γ(R1) in Assumption 2. For a constant ω > 0 governing the error probability in our results 
we also deﬁne ω2

i = ω2 + 24 log i at epoch i. Our results assume that we run Algorithm 1 with

Ti ≥ c1 (cid:20) s2

γ2R2

i (cid:0)(G2 + σ2) log d + ω2

i σ2(cid:1) + log d(cid:21)  

(9)

where c1 is a universal constant. For a total of T iterations in Algorithm 1  we state our results for

the parameterbθT = y(KT +1) where KT is the last epoch completed in T iterations.

3.1 Main theorem and some remarks

We start with our main result which shows an overall convergence rate of O(1/T ) after T iterations.
This O(1/T ) convergence is analogous to earlier work on multi-step methods for strongly convex

4

objectives [11  12  10]. For each subset S ⊆ {1  2  . . .   d} of cardinality s  we deﬁne

(10)
This quantity captures the degree of sparsity in the optimum θ∗; for instance  ε2(θ∗; S) = 0 if and
only if θ∗ is supported on S. Given the probability parameter ω > 0  we also deﬁne the shorthand

ε2(θ∗; S) := kθ∗Sck2

1/s.

κT = log2(cid:20)

s2((G2 + σ2) log d + ω2σ2)(cid:21) log d.

γ2R2

1T

(11)

(14)

Theorem 1. Suppose the expected loss L satisﬁes Assumptions 1— 3 with parameters G(R) ≡ G 
γ and σ(R) ≡ σ  and we perform updates (4) with epoch lengths (9) and parameters
log d

λ2
i =

Riγ

s√Tiq(G2 + σ2) log d + ω2

i σ2

and α(t) = 5Ris

(G2 + λ2

i + σ2)t

.

(12)

Then for any subset S ⊆ {1  . . .   d} of cardinality s and any T ≥ 2κT   there is a universal constant
c0 such that with probability at least 1 − 6 exp(−ω2/12) we have
((G2 + σ2) log d + σ2(ω2 + log

(13)

κT
log d

)) + ε2(θ∗; S)(cid:21) .

2 ≤ c3 (cid:20) s

γ2T

kbθT − θ∗k2

Consequently  the theorem predicts a convergence rate of O(1/γ2T ) which is the best possible under
our assumptions. Under the setup of Example 1  the error bound of Theorem 1 further simpliﬁes to

2 = O(cid:18) sB2

γ2T

(log d + ω2) + ε2(θ∗; S)(cid:19) .

kbθT − θ∗k2

We note that for an approximately sparse θ∗  Theorem 1 guarantees convergence only to a toler-
ance ε2(θ∗; S) due to the error terms arising out of the approximate sparsity. Overall  the theorem
provides a family of upper bounds  one for each choice of S. The best bound can be obtained by
optimizing this choice  trading off the competing contributions of s and kθ∗Sck1.
At this point  we can compare the result of Theorem 1 to some of the previous work. One approach
to minimize the objective (1) is to perform stochastic gradient descent on the objective  which has a

scaling in the dimension d. An alternative is to perform mirror descent [15  19] or regularized dual
averaging [22] using the same prox-function as Algorithm 1 but without breaking it up into epochs.
As mentioned in the introduction  this single-step method fails to exploit the strong convexity of our

convergence rate of O((eG2 +eσ2)/(γ2T )) [10  14]  where k∇L(θ)k2 ≤ eG and E exp(cid:16) ke(θ)k2
eσ2 (cid:17) ≤
exp(1). In the setup of Example 1  eG2 = Bd and similarly foreσ; giving an exponentially worse
problem and obtains inferior convergence rates of O(splog d/T ) [19  22  7].
developed in this paper  it can be shown that setting λ = σplog d/T leads to an overall convergence
γ 2T (log d + ω2)(cid:17)  which exhibits the same scaling as Theorem 1. However  with this
rate of eO(cid:16) sB2

A proposal closer to our approach is to minimize the regularized objective (3)  but with a ﬁxed
value of λ instead of the decreasing schedule of λi used in Theorem 1. This amounts to using the
method of Juditsky and Nesterov [11] on the regularized problem  and by using the proof techniques

ﬁxed setting of λ  the initial epochs tend to be much longer than needed for halving the error. Indeed 
our setting of λi is based on minimizing the upper bound at each epoch  and leads to an improved
performance in our numerical simulations. The beneﬁts of slowly decreasing the regularization in
the context of deterministic optimization were also noted in the recent work of Xiao and Zhang [23].

2

3.2 Some illustrative corollaries

We now present some consequences of Theorem 1 by making speciﬁc assumptions regarding the
sparsity of θ∗. The simplest situation is when θ∗ is supported on some subset S of size s. More
generally  Theorem 1 also applies to the case when the optimum θ∗ is only approximately sparse.
One natural form of approximate sparsity is to assume that θ∗ ∈ Bq(Rq) for 0 < q ≤ 1  where

Bq(Rq) :=(θ ∈ Rd |

|θi|q ≤ Rq) .

dXi=1

5

For 0 < q ≤ 1  membership in the set Bq(Rq) enforces a decay rate on the components of the
vector θ. We now present a corollary of Theorem 1 under such an approximate sparsity condition.
To facilitate comparison with minimax lower bounds  we set ω2 = δ log d in the corollaries.
Corollary 1. Under the conditions of Theorem 1  for all T > 2κT with probability at least 1 −
6 exp(−δ log d/12)  there is a universal constant c0 such that

c0h G2+σ2(1+δ)
c0Rq(cid:20)n (G2+σ2(1+δ)) log d

γ 2T

γ 2

log di

s log d

T + sσ2

γ 2T log κT

2

o 2−q

2

γ 2T(cid:17) 2−q
+(cid:16) σ2

log κT

log d

((1+δ) log d)

θ∗ is s-sparse 

θ∗ ∈ Bq(Rq).

2(cid:21)

q

2 ≤
kbθT − θ∗k2

The ﬁrst part of the corollary follows directly from Theorem 1 by noting that ε2(θ∗; S) = 0 under
our assumptions. Note that as q ranges over the interval [0  1]  reﬂecting the degree of sparsity  the

convergence rate ranges from from eO(1/T ) (for q = 0 corresponding to exact sparsity) to eO(1/√T )

(for q = 1). This is a rather interesting trade-off  showing in a precise sense how convergence rates
vary quantitatively as a function of the underlying sparsity.

It is useful to note that the results on recovery for generalized linear models presented here exactly
match those that have been developed in the statistics literature [13  21]  which are optimal under our

assumptions on the design vectors. Concretely  ignoring factors of O(log T )  we get a parameterbθT
having error at most O(s log d/(γ2T ) with an error probability decyaing to zero with d. Moreover 
in doing so our algorithm only goes over at most T data samples  as each stochastic gradient can be
evaluated with one fresh data sample drawn from the underlying distribution. Since the statistical
minimax lower bounds [13  21] demonstrate that this is the smallest possible error that any method
can attain from T samples  our method is statistically optimal in the scaling of the estimation error
with the number of samples. We also observe that it is easy to instead set the error probability to
δ = ω2 log T   if an error probability decaying with T is desired  incurring at most additional log T
factors in the error bound. Finally  we also remark that our techniques extend to handle examples
such as the least-squares loss that are not uniformly Lipschitz. The details of this extension are
deferred to the long version of this paper [2].

Stochastic optimization over ﬁnite pools: A common setting for the application of stochastic op-
timization methods in machine learning is when one has a ﬁnite pool of examples  say {Z1  . . .   Zn} 
and the objective (1) takes the form

θ∗ = arg min
θ∈Ω

1
n

nXi=1

L(θ; Zi)

(15)

In this setting  a stochastic gradient g(θ) can be obtained by drawing a sample Zj at random with
replacement from the pool {Z1  . . .   Zn}  and returning the gradient ∇L(θ; Zj).
In high-dimensional problems where d ≫ n  the sample loss is not strongly convex. However  it has
been shown by many researchers [3  13  1] that under suitable conditions  this objective does satisfy
restricted forms of the LSC assumption  allowing us to appeal to a generalized form of Theorem 1.
We will present this corollary only for settings where θ∗ is exactly sparse and also specialize to the
logistic loss  L(θ; (x  y)) = log(1 + exp(−yhθ  xi)) to illustrate the key aspects of the result. We
recall the deﬁnition of the link function ψ(α) = exp(α)/(1 + exp(α))2. We will state the result
for sub-Gaussian data design with parameters (Σ  η2
i ] = Σ and hu  xii is
ηx-sub-Gaussian for any unit norm vector u ∈ Rd.
Corollary 2. Consider the ﬁnite-pool loss (15)  based on n i.i.d. samples from a sub-Gaussian
design with parameters (Σ  η2
x). Suppose that Assumptions 1-3 are satisﬁed and the optimum θ∗
of (15) is s-sparse. Then there are universal constants (c0  c1  c2  c3) such that for all T ≥ 2κT and
n ≥ c3

x)  meaning that the E[xixT

min(Σ) max(σ2

x)  we have

min(Σ)  η4

log d

σ2

2 ≤

kbθT − θ∗k2

c0

σ2
min(Σ)

s log d

T n

1

ψ2(2BR1)(cid:8)B2(1 + δ)(cid:9)o + c0

with probability at least 1 − 2 exp(−c1n min(σ2

min(Σ)/η4

sσ2

log

κT
log d

.

σ2
min(Σ)ψ2(2BR1)T
x  1)) − 6 exp(−δ log d/12).

6

We observe that the bound only holds when the number of samples n in the objective (15) is large
enough  which is necessary for the restricted form of the LSC condition to hold with non-trivial
parameters in the ﬁnite sample setting.

A modiﬁed method with constant epoch lengths: Algorithm 1 as described is efﬁcient and sim-
ple to implement. However  the convergence results critically rely on the epoch length Ti to be
set appropriately in a doubling manner. This could be problematic in practice  where it might be
tricky to know when an epoch should be terminated. Following Juditsky and Nesterov [11]  we
next demonstrate how a variant of our algorithm with constant epoch lengths enjoys similar rates of
convergence. The key challenge here is that unlike the previous set-up [11]  our objective function
changes at each epoch which leads to signiﬁcant technical difﬁculties. At a very coarse level  if
we have a total budget of T iterations  then this version of our algorithm allows us to set the epoch
lengths to O(log T )  and guarantees convergence rates that are O((log T )/T ).
Theorem 2. Suppose the expected loss satisﬁes Assumptions 1- 3 with parameters G  γ  and σ
resp. Let S be any subset of {1  . . .   d} of cardinality s. Suppose we run Algorithm 1 for a total of
T iterations with epoch length Ti ≡ T log d/κT and with parameters as in Equation 12. Assuming
that this setting ensures Ti = O(log d)  for any set S  with probability at least 1 − 3 exp(ω2/12)

2 = O(cid:18)s

kbθT − θ∗k2

(G2 + σ2) log d + (ω2 + log(κ/ log d))σ2

T

log d

κ (cid:19) .

The theorem shows that up to logarithmic factors in T   setting the epoch lengths optimally is not
critical. A similar result can also be proved for the case of least-squares regression.

4 Simulations

In this section we will present numerical simulations that back our theoretical convergence results.
We focus on least-squares regression  discussed in Example 2. Speciﬁcally  we generate samples
(xt  yt) with each coordinate of xt distributed as Unif[−B  B] and yt = hθ∗  xti + wt. We pick θ∗
to be s-sparse vector with s = ⌈log d⌉  and wt ∼ N (0  η2) with η2 = 0.5. Given an iterate θt  we
generate a stochastic gradient of the expected loss (1) at (xt  yt). For the ℓ1-norm  we pick the sign
vector of θt  with 0 for any component that is zero  a member of the ℓ1-sub-differential.
Our ﬁrst set of results evaluate Algorithm 1 against other stochastic optimization baselines assuming
a complete knowledge of problem parameters. Speciﬁcally  we epoch i is terminated once
p/2. This ensures that θ∗ remains feasible throughout  and tests the per-
kyi+1 − θ∗k2
formance of Algorithm 1 in the most favorable scenario. We compare the algorithm against two
baselines. The ﬁrst baseline is the regularized dual averaging (RDA) algorithm [22]  applied to the

p ≤ kyi − θ∗k2

rameter with T samples. We use the same prox-function ψ(θ) = kθk2

regularized objective (3) with λ = 4ηplog d/T   which is the statistically optimal regularization pa-
2(p−1)   so that the theory for RDA
predicts a convergence rate of O(splog d/T ) [22]. Our second baseline is the stochastic gradient
(SGD) algorithm which exploits the strong convexity but not the sparsity of the problem (1). Since
the squared loss is not uniformly Lipschitz  we impose an additional constraint kθk1 ≤ R1  without
which the algorithm does not converge. The results of this comparison are shown in Figure 1(a) 
where we present the error kθt − θ∗k2
2 averaged over 5 random trials. We observe that RADAR
comprehensively outperforms both the baselines  conﬁrming the predictions of our theory.

p

The second set of results focuses on evaluating algorithms better tailored for our assumptions. Our
ﬁrst baseline here is the approach that we described in our remarks following Theorem 1. In this
approach we use the same multi-step strategy as Algorithm 1 but keep λ ﬁxed. We refer to this as

Epoch Dual Averaging (henceforth EDA)  and again employ λ = 4ηplog d/T with this strategy.

Our epochs are again determined by halving of the squared ℓp-error measured relative to θ∗.
Finally  we also evaluate the version of our algorithm with constant epoch lengths that we analyzed in
Theorem 2 (henceforth RADAR-CONST)  using epochs of length log(T ). As shown in Figure 1(b) 
the RADAR-CONST has relatively large error during the initial epochs  before converging quite

7

rapidly  a phenomenon consistent with our theory.1 Even though the RADAR-CONST method does
not use the knowledge of θ∗ to set epochs  all three methods exhibit the same eventual convergence
rates  with RADAR (set with optimal epoch lengths) performing the best  as expected. Although
RADAR-CONST is very slow in initial iterations  its convergence rate remains competitive with
EDA (even though EDA does exploit knowledge of θ∗)  but is worse than RADAR as expected.
Overall  our experiments demonstrate that RADAR and RADAR-CONST have practical perfor-
mance consistent with our theoretical predictions. Although optimal epoch length setting is not
too critical for our approach  better data-dependent empirical rules for determining epoch lengths
remains an interesting question for future research. The relatively poorer performance of EDA
demonstrates the importance of our decreasing regularization schedule.

2 2
k
∗
θ
−

t
θ
k

6

5

4

3

2

1

 

0
0

Error vs. iterations

 

RADAR
SGD
RDA

0.5

1

Iterations

1.5

2
x 104

(a)

2 2
k
∗
θ
−

t
θ
k

5

4

3

2

1

 

0
0

Error vs. iterations

 

RADAR
EDA
RADAR-CONST

0.5

1

Iterations

1.5

2
x 104

(b)

Figure 1. A comparison of RADAR with other stochastic optimization algorithms for d = 40000 and
s = ⌈log d⌉. The left plot compares RADAR with the RDA and SGD algorithms  neither of which
exploits both the sparsity and the strong convexity structures simultaneously. The right one compares
RADAR with the EDA and RADAR-CONST algorithms  all of which exploit the problem structure
but with varying degrees of effectiveness. We plot kθt − θ∗k2
2 averaged over 5 random trials versus the
number of iterations.

5 Discussion

In this paper we present an algorithm that is able to take advantage of the strong convexity and spar-
sity conditions that are satsiﬁed by many common problems in machine learning. Our algorithm
is simple and efﬁcient to implement  and for a d-dimensional objective with an s-sparse optima  it
achieves the minimax-optimal convergence rate O(s log d/T ). We also demonstrate optimal con-
vergence rates for problems that have weakly sparse optima  with implications for problems such as
sparse linear regression and sparse logistic regression. While we focus our attention exclusively on
sparse vector recovery due to space constraints  the ideas naturally extend to other structures such as
group sparse vectors and low-rank matrices. It would be interesting to study similar developments
for other algorithms such as mirror descent or Nesterov’s accelerated gradient methods  leading to
multi-step variants of those methods with optimal convergence rates in our setting.

Acknowledgements

The work of all three authors was partially supported by ONR MURI grant N00014-11-1-0688 to
MJW. In addition  AA was partially supported by a Google Fellowship  and SNN was partially
supported by the Yahoo KSC award.

1 To clarify  the epoch lengths in RADAR-CONST are set large enough to guarantee that we can attain an overall error bound of O(1/T ) 
meaning that the initial epochs for RADAR-CONST are much longer than for RADAR. Thus  after roughly 500 iterations  RADAR-CONST
has done only 2 epochs and operates with a crude constraint set Ω(R1/4). During epoch i  the step size scales proportionally to Ri/√t 
where t is the iteration number within the epoch; hence the relatively large initial steps in an epoch can take us to a bad solution even when
we start with a good solution yi when Ri is large. As Ri decreases further with more epochs  this effect is mitigated and the error of
RADAR-CONST does rapidly decrease like our theory predicts.

8

References

[1] A. Agarwal  S. N. Negahban  and M. J. Wainwright. Fast global convergence rates of gradient methods
for high-dimensional statistical recovery. To appear in The Annals of Statistics  2012. Full-length version
http://arxiv.org/pdf/1104.4824v2.

[2] A. Agarwal  S. N. Negahban  and M. J. Wainwright. Stochastic optimization and sparse statistical recov-

ery: An optimal algorithm for high dimensions. 2012. URL http://arxiv.org/abs/1207.4421.

[3] P. J. Bickel  Y. Ritov  and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Ann.

Stat.  37(4):1705–1732  2009.

[4] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS  2007.
[5] P. B¨uhlmann and S. Van De Geer. Statistics for High-Dimensional Data: Methods  Theory and Applica-

tions. Springer Series in Statistics. Springer  2011.

[6] D. L. Donoho. High-dimensional data analysis: The curses and blessings of dimensionality  2000.
[7] J. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent. In Proceed-

ings of the 23rd Annual Conference on Learning Theory  pages 14–26. Omnipress  2010.

[8] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward-backward splitting. Journal of

Machine Learning Research  10:2873–2898  2009.

[9] E. Hazan  A. Kalai  S. Kale  and A. Agarwal. Logarithmic regret algorithms for online convex optimiza-

tion. In Proceedings of the Nineteenth Annual Conference on Computational Learning Theory  2006.

[10] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic
strongly-convex optimization. Journal of Machine Learning Research - Proceedings Track  19:421–436 
2011.

[11] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex func-

tions. Available online http://hal.archives-ouvertes.fr/docs/00/50/89/33/PDF/Strong-hal.pdf  2010.

[12] G. Lan and S. Ghadimi. Optimal stochastic approximation algorithms for strongly convex stochastic

composite optimization  part ii: shrinking procedures and optimal algorithms. 2010.

[13] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional
analysis of M-estimators with decomposable regularizers. In NIPS Conference  Vancouver  Canada  De-
cember 2009. Full length version arxiv:1010.2731v1.

[14] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[15] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization. Wiley  New

York  1983.

[16] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76  Center

for Operations Research and Econometrics (CORE)  Catholic University of Louvain (UCL)  2007.

[17] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming A 

120(1):261–283  2009.

[18] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In

Proceedings of the 24th International Conference on Machine Learning  2007.

[19] S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1 regularized loss minimization. Journal of

Machine Learning Research  12:1865–1892  June 2011.

[20] N. Srebro  K. Sridharan  and A. Tewari. Smoothness  low noise  and fast rates. In Advances in Neural

Information Processing Systems 23  pages 2199–2207  2010.

[21] S. A. van de Geer. High-dimensional generalized linear models and the lasso. The Annals of Statistics 

36:614–645  2008.

[22] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research  11:2543–2596  2010.

[23] L. Xiao and T. Zhang. A proximal-gradient homotopy method for the sparse least-squares problem.

ICML  2012. URL http://arxiv.org/abs/1203.3002.

9

,Kamalika Chaudhuri
Daniel Hsu
Shuang Song
James McInerney
Rajesh Ranganath
David Blei