2018,An Improved Analysis of Alternating Minimization for Structured Multi-Response Regression,Multi-response linear models aggregate a set of vanilla linear models by assuming correlated noise across them  which has an unknown covariance structure. To find the coefficient vector  estimators with a joint approximation of the noise covariance are often preferred than the simple linear regression in view of their superior empirical performance  which can be generally solved by alternating-minimization type procedures. Due to the non-convex nature of such joint estimators  the theoretical justification of their efficiency is typically challenging. The existing analyses fail to fully explain the empirical observations due to the assumption of resampling on the alternating procedures  which requires access to fresh samples in each iteration. In this work  we present a resampling-free analysis for the alternating minimization algorithm applied to the multi-response regression. In particular  we focus on the high-dimensional setting of multi-response linear models with structured coefficient parameter  and the statistical error of the parameter can be expressed by the complexity measure  Gaussian width  which is related to the assumed structure. More importantly  to the best of our knowledge  our result reveals for the first time that the alternating minimization with random initialization can achieve the same performance as the well-initialized one when solving this multi-response regression problem. Experimental results support our theoretical developments.,An Improved Analysis of Alternating Minimization

for Structured Multi-Response Regression

Sheng Chen ∗
The Voleon Group

chen2832@umn.edu

Dept. of Computer Science & Engineering

University of Minnesota  Twin Cities

Arindam Banerjee

banerjee@cs.umn.edu

Abstract

Multi-response linear models aggregate a set of vanilla linear models by assuming
correlated noise across them  which has an unknown covariance structure. To ﬁnd
the coefﬁcient vector  estimators with a joint approximation of the noise covariance
are often preferred than the simple linear regression in view of their superior
empirical performance  which can be generally solved by alternating-minimization-
type procedures. Due to the non-convex nature of such joint estimators  the
theoretical justiﬁcation of their efﬁciency is typically challenging. The existing
analyses fail to fully explain the empirical observations due to the assumption of
resampling on the alternating procedures  which requires access to fresh samples in
each iteration. In this work  we present a resampling-free analysis for the alternating
minimization algorithm applied to the multi-response regression. In particular 
we focus on the high-dimensional setting of multi-response linear models with
structured coefﬁcient parameter  and the statistical error of the parameter can be
expressed by the complexity measure  Gaussian width  which is related to the
assumed structure. More importantly  to the best of our knowledge  our result
reveals for the ﬁrst time that the alternating minimization with random initialization
can achieve the same performance as the well-initialized one when solving this
multi-response regression problem. Experimental results support our theoretical
developments.

1

Introduction

We consider the following multi-response linear model [1  5  18] with m real-valued outputs 

y = Xθ∗ + η   where η = Σ1/2∗ ˜η

(1)
where y ∈ Rm is the response vector and X ∈ Rm×p consists of m p-dimensional feature vectors 
and ˜η ∈ Rm is a zero-mean isotropic noise vector. The m responses share the same underlying
parameter θ∗ ∈ Rp  which corresponds to the so-called pooled model [17]. Without loss of generality 
the counterpart of (1) with response-speciﬁc parameters can be equivalently written in the above form 
by block-diagonalizing rows of X and concatenating different parameters into a single vector. What
makes this model different from vanilla linear models is the correlated noise η across responses  which
is assumed to be a linear transformation of ˜η. The noise covariance of η is given by Cov(η) = Σ∗.
This model has found numbers of real-world applications  such as econometrics [17]  computational
biology [22] and climate informatics [14  15]  just to name a few.
In practice  we are given n observations of (X  y)  denoted by D = {(Xi  yi)}n
i=1  while the
noise covariance structure Σ∗ between responses is typically unknown. Our goal is to estimate the

∗This work was done when the author studied at University of Minnesota  Twin Cities

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

parameter θ∗  ideally together with Σ∗. In this work  we additionally focus on the high-dimensional
regime  where the true parameter θ∗ is assumed to possess certain low-complexity structure measured
by some function f : Rp (cid:55)→ R+  which can be either convex (e.g.  norms) or non-convex (e.g.  L0
cardinality). For the low-dimensional setting  it has been shown  both empirically and theoretically 
that simultaneously estimating Σ and θ leads to better performance than ordinary least squares [20].
Inspired by this fact  we consider the joint estimator of (Σ  θ) in high dimension as follows 
f (θ) ≤ λ  

(cid:16) ˆθn  ˆΣn

2 (yi − Xiθ)

n(cid:88)

log |Σ|

(cid:17)

s.t.

(2)

+

(cid:13)(cid:13)(cid:13)Σ− 1

(cid:13)(cid:13)(cid:13)2

2

= argmin
θ∈Rp  Σ(cid:23)0

1
2

1
2n

i=1

which corresponds to the constrained maximum likelihood estimator (MLE) of (Σ  θ) when the noise
is multivariate Gaussian. The structural assumption on θ∗ is encoded by the inequality constraint.
Though the noise structure is accounted in this joint estimator  one challenge faced by the associated
optimization problem is the non-convexity of the objective function. In the light of the simplicity
of the marginal optimization over Σ and θ when the other is ﬁxed  a popular approach to dealing
with such problem is alternating minimization (AltMin)  i.e.  alternately solving for Σ (and θ) while
keeping θ (and Σ) ﬁxed. For problem (2)  the update of AltMin can be written as

n(cid:88)

(cid:16)

i=1

ˆΣ(t+1) =

1
n

(cid:17)(cid:16)

(cid:17)T
(cid:13)(cid:13)(cid:13)2

2

yi − Xi

ˆθ(t)

(cid:13)(cid:13)(cid:13) ˆΣ

n(cid:88)

i=1

yi − Xi

ˆθ(t)

− 1
(t+1) (yi − Xiθ)

2

ˆθ(t+1) = argmin
θ∈Rp

1
2n

 

s.t.

f (θ) ≤ λ  

(3)

(4)

which will be executed for a number of iterations  say T . Here the new ˆΣ(t+1) is obtained by
computing the empirical covariance of the residues estimated at ˆθ(t). Though f can potentially
be non-convex  the update of ˆθ(t+1) is merely solving a constrained least squares problem  for
which various algorithms are guaranteed to ﬁnd the global minimum under mild conditions on
data [21  3  33]. Generally speaking  both steps are easy to implement  which makes AltMin more
attractive compared with other optimization algorithms that jointly update θ and Σ. In the low-
dimensional setting  the AltMin algorithm for multi-response regression was initially proposed by
[28]. For the high-dimensional counterpart with sparse parameters  previous works [32  23  31]
considered the regularized MLE approaches  which are also solved by AltMin-type algorithms.
Unfortunately  none of those works provide ﬁnite-sample statistical guarantees for their algorithms.
The ﬁrst attempt to establish the non-asymptotic error bound of this AltMin approach is made by
[20] for low-dimensional regime  with a brief extension to sparse parameter setting using iterative
hard thresholding method [21]. But they did not allow more general structure of the parameter. One
of the closely related work is [10] with a focus on general parameter structures captured by norms.
They proposed an alternating estimation framework  in which the generalized Dantzig selector [8] is
used for θ-step as an alternative to the regularized and the constrained estimators.
The AltMin technique has also been applied to many other estimation problems  such as matrix
completion [19]  phase retrieval [27]  and mixed linear regression [44]. However  the current
theoretical understanding of AltMin is still incomplete. Including the aforementioned works  the
statistical guarantees for non-convex AltMin procedures are often shown under the resampling
assumption  which assumes that each iteration receives a fresh sample. Albeit this can be achieved by
partitioning the data into disjoint subsets and using different batches in each update  people seldom do
so in practice  as it usually results in worse performance than using all data in every iteration. From
the theoretical perspective  the resampling assumption oversimpliﬁes the analysis of the algorithm
used in practice  which may otherwise require sophisticated proof techniques [36].
In this paper  we aim at a better way to bound the statistical error of the above AltMin procedure for
general structure-inducing f. In principal  non-asymptotic statistical analyses for the high dimension
typically involve bounding suprema of stochastic processes [26  2  42  30]. The difﬁculty of analyzing
AltMin lies in the dependency between the data and the obtained iterates  and the lack of independence
prevents applications of various concentration inequalities to the suprema of the target processes. The
resampling assumption facilitates the analysis of AltMin by assuming access to new data that are
independent of previous iterates. In contrast to resampling  we here resort to uniform bounds to tackle
the dependency issue. That is  instead of dealing with the processes involving the speciﬁc iterates
generated by AltMin  we try to bound their worst-case counterparts that consider all possible iterates

2

before running AltMin. This solution to dependency ends up with more complicated stochastic
processes  which need careful treatment. By applying generic chaining [37]  an advanced tool from
probability theory  we are able to obtain the desired bounds for the processes under consideration 
and eventually express the error bound in terms of a complexity measure called Gaussian width
[16  7] (see Section 3.1). In particular  we analyze the AltMin procedure under two different choices
of initialization  one with an arbitrarily initialized iterate and the other starting at a point close to
θ∗. The L2-error for both types of AltMin is shown to converge geometrically to certain minimum
achievable error emin with overwhelming probability  i.e. 

where ρn < 1 is the contraction factor and emin is given by

(cid:13)(cid:13) ˆθ(T ) − θ∗(cid:13)(cid:13)2 ≤ emin + ρT
(cid:18) w(C) + m√
(cid:19)
(cid:18) w(C)√
(cid:19)

emin = O

n

n ·(cid:0)(cid:13)(cid:13) ˆθ(0) − θ∗(cid:13)(cid:13)2 − emin

(cid:1)

(arbitrary initialization)  

(5)

(6)

n

emin = O

(good initialization) .

(7)
Here w(C) is the Gaussian width of a set C related the structure of θ∗ (see Deﬁnition 2). Surprisingly
the error for good initializations matches the resampling-based result up to some constant  which
requires more fresh data to achieve such a bound. In general  our work improves the results in
[20  10] in several aspects. First  our analysis does not rely on the resampling assumption. Second 
our statistical guarantees work for general sub-Gaussian noise while [20] and [10] only considered
Gaussian noise. Third  we allow the complexity function f to be non-convex  whereas [10] required
f being a norm. Last but not least  our result suggests that when the amount of data is adequate
for the error bound (6) to meet the requirement of good initialization  the AltMin with arbitrary
initialization can achieve the same level of error as the well-initialized one. Although this type of
guarantee for arbitrary initializations was discovered for other problems [43]  it has not been revealed
for the multi-response regression  and our proof technique is also different from the existing ones.
The rest of the paper is organized as follows. In Section 2  we outline our strategy for combating non-
convexity and present the algorithmic details of the AltMin procedure for structured multi-response
regression. In Section 3  we present the statistical guarantees for the AltMin algorithm under suitable
probabilistic assumptions. We provide some experimental results in Section 4  and conclude in
Section 5. All proofs are deferred to the supplementary material.

2 Strategy to Conquer Non-Convexity

For many statistical estimation problems  we can construct the estimator of the underlying model
parameter w∗  by minimizing certain loss function on the given sample D 

ˆw = argmin
w∈W

L(w;D) .

(8)

In order to show the recovery guarantee for non-convex estimation  there are mainly two commonly-
used strategies. One strategy is to show certain local convergence in a neighborhood N of the global
minimizer ˆw of (8) [6  29  39  45  25]. With a proper initialization inside N   subsequent iterates
produced by some local search might be able to converge to ˆw  whose statistical error is expected to
be small. This strategy is particularly suitable for the noiseless setting  as ˆw is equal to w∗  and most
of the existing works use gradient descent type or its variants as workhorse algorithms. The other
strategy is to show that there is no spurious local minima of L under the assumed statistical models 
so that any optimization algorithms that provably converge to local minima will sufﬁce for a good
estimation [34  35  4  13  24  12].
For our multi-response regression problem  however  it is difﬁcult to apply the aforementioned
strategies. First  bounding the statistical error of the global minimizer is nontrivial in the noisy setting 
especially when the objective L(w) involves more than one set of variables like the multi-response
regression  let alone characterizing the equivalence of all local minima. Second  the gradient-based
local search is inefﬁcient for the problem (2)  since the update of Σ involves matrix inversion and
projection onto positive semideﬁnite (PSD) cone. In contrast  AltMin procedure has a closed-form
solution to Σ-step  which is preferred in this setting.

3

In this work  we consider another strategy for the non-convex estimation in which w (w∗) is composed
of two parameters  a and b (a∗ and b∗). The loss L is assumed to jointly non-convex over a and b 
but might be marginally convex w.r.t. a (b) when b (a) is ﬁxed. When the marginal subproblems are
easy to solve  alternating minimization procedure is appealing for the purpose of estimation  which is
true for the multi-response regression. The AltMin algorithm executes the following updates 

ˆa(t+1) = argmin

a∈A

L(a  ˆb(t);D)  

ˆb(t+1) = argmin

b∈B

L(ˆa(t+1)  b;D) .

(9)

The basic idea for showing the statistical guarantees of AltMin is to derive the statistical error bounds
for both the a- and b-steps when the other parameter is ﬁxed to the latest estimate. Since both
subproblems in (9) are usually simpler  the separate errors might be easier to characterize than
considered jointly  which are ideally of the form 

(cid:16)ˆb(t+1)  b∗

(cid:17) ≤ e2

(cid:0)d1

(cid:0)ˆa(t+1)  a∗(cid:1)(cid:1) .

(10)

(cid:0)ˆa(t+1)  a∗(cid:1) ≤ e1

d1

(cid:16)

d2

(cid:16)ˆb(t)  b∗

(cid:17)(cid:17)

 

d2

The function d1 (respectively d2) characterizes the closeness between ˆa(t+1) and a∗ (ˆb(t+1) and b∗) 
which is nonnegative with d1(a∗  a∗) = 0 (d2(b∗  b∗) = 0) but not necessarily a metric. The choice
of d1 and d2 depends on the goal of analysis for the problem under consideration  and a suitable
combination of d1 and d2 may facilitate the proof. The upper bound e1 (respectively e2) may depend
on other quantities such as n  but our emphasis is the dependence on the estimation accuracy of b
(a). It is natural to expect that e1 (e2) will shrink as ˆb(t) (ˆa(t)) moves closer to b∗ (a∗). Under this
condition  we can apply the bounds in (10) alternatingly and recursively

d1(ˆa(T )  a∗) ≤ e1

d2(ˆb(T )  b∗) ≤ e2

(cid:16)

d2

(cid:16)
(cid:0)d1

(cid:16)ˆb(T−1)  b∗
(cid:17)(cid:17) ≤ . . . . . . ≤ e1
(cid:124)
(cid:16)
(cid:0)ˆa(T )  a∗(cid:1)(cid:1) ≤ . . . . . . ≤ e2
(cid:124)

(cid:16)
(cid:16)

. . . e1

e2
composition of T e1(·) and T − 1 e2(·)

. . .

d2

(cid:16)
(cid:16)ˆb(0)  b∗
(cid:123)(cid:122)
(cid:16)ˆb(0)  b∗
(cid:17)(cid:17)
(cid:123)(cid:122)

(cid:17)(cid:17)
(cid:17)(cid:17)
(cid:125)

. . .

d2

. . . e1

e1
composition of T e2(·) and T e1(·)

(cid:16)

(cid:17)(cid:17)
(cid:125)

(11)

(12)

which may imply the error of ˆa(T ) and ˆb(T ) under other metrics of interest as well. Compared
with the previous strategies  one notable difference of our treatment is that we do not care about the
optimization convergence of AltMin  as we neither characterize the error of any local minimizers
of L(·) nor show any iterate convergence to those minimizers. Instead the ingredients we need
are simply the statistical error bounds in (10). Given this fact  our analysis can be extended to the
alternating estimation (AltEst) procedure [10] that need not optimize a joint objective over a and b
and certainly cannot be handled by the earlier strategies.
In order to get (10)  the analysis for each AltMin step is often confronted with a technical challenge
due to the dependency between data and the iterates obtained so far  which is bypassed by many
existing analyses via the resampling assumption. Essentially the resampling-based result states
that with any ﬁxed ˆb(t) (ˆa(t+1))  given a fresh sample D(t) independent of ˆb(t) (ˆa(t+1))  the next
iterate ˆa(t+1) (ˆb(t+1)) satisﬁes the corresponding bound in (10) with high probability. To avoid the
resampling  we leverage the idea of uniform bounds [40]  which aims to show that given a sample D 
the bounds in (10) hold uniformly with high probability for all possible value of the input ˆb(t) and
ˆa(t+1). This argument asks for no fresh data in each iteration  and the probability of the error bounds
being true does not deteriorate with growing number of iterations. For structured multi-response
regression  we will focus on the AltMin procedure shown in Algorithm 1. For the rest of the paper 
C0  C1  c0  c1 and so on are reserved for absolute constants.

3 Statistical Guarantees of Alternating Minimization

In this section  we apply the resampling-free analysis strategy introduced in Section 2 to the multi-
response regression problem  for which a = Σ and b = θ. First we introduce a few notations. Given
a set A ⊆ Rp  deﬁne coneA = {c · a | c ≥ 0  a ∈ A}. We denote the smallest and the largest
eigenvalue of Σ∗ as σ−
∗ and σ+∗   and assume Diag(Σ∗) = Im×m throughout the paper for simplicity.
In addition  we drop the subscripts indexing the iteration  and analyze both Σ-update and θ-update in

4

Algorithm 1 Alternating minimization for multi-response regression
Input: Number of iterations T   Data D = {(Xi  yi)}n
Output: Estimated ˆθ(T )
1: Initialize ˆθ(0) (e.g.  solving (4) with ˆΣ(0) = I)
2: for t:= 0 to T − 1 do
3:
4:
5: end for
6: return ˆθ(T )

Compute ˆΣ(t+1) according to (3)
Compute ˆθ(t+1) by solving (4)

i=1 and Tuning parameter λ

a broader setting  where the other parameter is ﬁxed as a generic input in certain regions  i.e. 

n(cid:88)

i=1

ˆΣ(θ) =

1
n

(yi − Xiθ) (yi − Xiθ)T  

ˆθ(Σ) = argmin
f (θ)≤f (θ∗)

1
2n

2 (yi − Xiθ)

(cid:13)(cid:13)(cid:13)Σ− 1

n(cid:88)

i=1

(13)

(14)

.

(cid:13)(cid:13)(cid:13)2

2

Note that here the tuning parameter λ in (4) for the θ-step is set as λ = f (θ∗)  which will be kept for
the rest of the analysis. Given the recent progress in non-convex optimization [3]  we also assume
that ˆθ(Σ) can be solved globally despite the potential non-convexity of f. The input regions we
consider for θ and Σ are respectively given by

R =(cid:8)θ ∈ Rp(cid:12)(cid:12) f (θ) ≤ f (θ∗)(cid:9)  
(cid:110) ˆΣ(θ) ∈ Rm×m(cid:12)(cid:12) θ ∈ R  (cid:107)θ − θ∗(cid:107)2 ≤ e0

(cid:111)

M(e0) =

 

(15)

(16)

in which e0 is the error tolerance to be speciﬁed for the initialization. Note that the input region
M(e0) implicitly depends on R as well as the sample D = {(x  y)}n
i=1 used for computing ˆΣ(θ).

3.1 Preliminaries

To apply the proof strategy for AltMin  we ﬁrst deﬁne the distance function d1 and d2.

Deﬁnition 1 (distance functions) The distance functions for Σ-step and θ-step are deﬁned as

d1(Σ  Σ∗) =

ξ(Σ)
ξ(Σ∗)

− 1  where ξ(Σ) =
d2(θ  θ∗) = (cid:107)θ − θ∗(cid:107)2 .

 

(17)

(18)

(cid:112)Tr(Σ−1Σ∗Σ−1)

Tr(Σ−1)

Although d1 may look odd at ﬁrst glance  it actually arises as a natural choice after we ﬁx d2  as the
L2-error of θ is our primary goal in the statistical analysis. It is worth noting that ξ(Σ) is minimized
at Σ = Σ∗. The following deﬁnition is critical to the analysis for general structures of θ∗ [7].

Deﬁnition 2 (error spherical cap) For a structure-inducing f  its error spherical cap is deﬁned as
(19)

C = cone(cid:8)u ∈ Rp(cid:12)(cid:12) f (θ∗ + u) ≤ f (θ∗)(cid:9) ∩ Sp−1  

where Sp−1 = {u | (cid:107)u(cid:107)2 = 1} is the unit sphere of Rp.

The probabilistic analysis of d1 and d2 is built upon the concept of sub-Gaussian vectors and matrices 
which are deﬁned below.
Deﬁnition 3 (sub-Gaussian vector and matrix) A vector x ∈ Rp is said to be sub-Gaussian if its
ψ2-norm satisﬁes 

|||x|||ψ2

= sup
u∈Sp−1

|||(cid:104)x  u(cid:105)|||ψ2

≤ κ < +∞  

(20)

5

where |||·|||ψ2
is deﬁned for a random variable x ∈ R as |||x|||ψ2
X ∈ Rm×p is sub-Gaussian if the following ψ2-norm for X is ﬁnite 

= supq≥1

1

(E|x|q)
q√
q

. A matrix

|||X|||ψ2

=

sup

u∈Sp−1 v∈Sm−1

− 1
v XT v

2

≤ κ < +∞  

(21)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)uT Γ

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)ψ2

where Γv = E[XT vvT X]. Further  Γv for any v ∈ Sm−1 is assumed to satisfy the condition
0 < µ− ≤ λmin(Γv) ≤ λmax(Γv) ≤ µ+ < +∞  for some constants µ− and µ+.

≤ κ  it is not difﬁcult to verify that |||X|||ψ2

This deﬁnition is adopted from [41  20]. If rows of X are i.i.d. copies of an isotropic sub-Gaussian
random vector x with |||x|||ψ2
≤ Cκ for a universal
constant C  and µ− = µ+ = 1. Our assumptions on {Xi} and { ˜ηi} are given below.
(A1) The designs X1  . . .   Xn are i.i.d. copies of a sub-Gaussian X with parameter κ  µ− and µ+.
(A2) The isotropic noises ˜η1  . . .   ˜ηn are i.i.d. copies of a sub-Gaussian ˜η with parameter τ.

Another key ingredient in the analysis is the complexity measure of the parameter structure captured
by C  which turns out to be the notion of Gaussian width [16].
Deﬁnition 4 (Gaussian width) The Gaussian width w(A) of a set A ⊆ Rp is deﬁned as

(cid:20)

(cid:21)

w(A) = Eg∼N (0 I)

(cid:104)g  u(cid:105)

sup
u∈A

.

(22)

Gaussian width is easy to calculate or bound for the error spherical caps induced by many f of
interest [7  9]. Based on Gaussian width  the proofs of the error bounds utilize a powerful tool
from probability theory  called generic chaining [37]. We refer the interested readers to the recent
monograph [38] and references therein.

3.2 Error Bound for Arbitrary Initializations

−
∗ µ−

σ

(cid:111)

(cid:26)

Given the deﬁnitions of distance function d1 and d2  we ﬁrst focus on the separate error bounds
for the Σ-step and the θ-step in (13) and (14). To allow arbitrary initializations  we consider the
tolerance of initialization error e0 = +∞  which appears in the deﬁnition of M(e0).

Lemma 1 (error bound for Σ-estimation) Under the assumptions (A1) and (A2)  if the sample size
n ≥ C0 max
  with probability at least 1−C2 exp (−C1m) 
ˆΣ(θ) given in (13) is invertible for any θ ∈ R and its error satisﬁes

(cid:17)2(cid:27)
1  τ 4  κ4(cid:16) σ+∗ µ+
(cid:16) ˆΣ(θ)  Σ∗
(cid:1) term in (23) is the typical statistical rate for covariance estimation.

Remark: If θ = θ∗  the Σ-step computes the sample covariance of the noise  for which d2(θ  θ∗) =

0  and the remaining O(cid:0)(cid:112) m

(cid:110)
(cid:17) ≤ C3τ 2

· d2 (θ  θ∗) .

(cid:114) m

m  w4(C)

·max

(cid:115)

µ+
σ−∗

n

(cid:17)2(cid:27)

Lemma 2 (error bound for θ-estimation) Under the assumptions (A1) and (A2)  if the sample
size n ≥ C0 max
  then with probability at least 1 −
C2 exp (−C1m)  the following bound holds for ˆθ(Σ) given in (14) with any input Σ ∈ M(+∞) 

(cid:26)
1  τ 4  κ4(cid:16) σ+∗ µ+
(cid:16) ˆθ(Σ)  θ∗

(cid:111)
(cid:17) ≤ (1 + d1 (Σ  Σ∗)) · C4κ(cid:112)µ+

· m + w(C)√

n

 

(24)

m  w4(C)

· max

(cid:110)

−
∗ µ−

d2

m

σ

+ C4

(23)

d1

m

n

µ−(cid:113)

Tr(Σ−1∗ )

where ξ(Σ) is given in Deﬁnition 1.

6

Remark: For Σ = Σ∗ and Σ = I  θ-step corresponds to the oracle estimator ˆθorc and the ordinary
least squares (OLS) estimator ˆθodn respectively  i.e. 

(cid:13)(cid:13)(cid:13)2

2

(yi − Xiθ)

(cid:107)yi − Xiθ(cid:107)2
2 .

 

(25)

(26)

An analysis similar to [10] shows that with high probability the L2-errors of ˆθorc and ˆθodn satisfy

ˆθorc = argmin
f (θ)≤f (θ∗)

ˆθodn = argmin
f (θ)≤f (θ∗)

(cid:13)(cid:13)(cid:13) ˆθorc − θ∗
(cid:13)(cid:13)(cid:13) ˆθodn − θ∗

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2

i=1

i=1

− 1
2∗

1
2n

1
2n

(cid:13)(cid:13)(cid:13)Σ

n(cid:88)
n(cid:88)
≤ C(cid:48)κ(cid:112)µ+
µ−(cid:113)
≤ C(cid:48)κ(cid:112)µ+
Tr(Σ−1∗ )
· w(C)√
µ−√
(cid:114) m

m

n

Tr(Σ−1∗ )

.

eorc
eodn

=

· w(C)√

n

(cid:44) eorc 

(cid:44) eodn 

which indicates that the oracle estimator improves the OLS by a factor of

(27)

(28)

(29)

(30)

(31)

(32)

In practice  this improvement can be signiﬁcant  especially when there is strong cross-correlation
among the responses  such that Σ∗ is close to singular.
By assembling Lemma 1 and 2  we obtain the following theorem for the error of AltMin  which
exhibits a geometrical convergence to certain minimum achievable error.

1  τ 4  κ4(cid:16) µ+σ+∗

Theorem 1 (error bound for arbitrarily-initialized AltMin) Under the assumptions (A1) and
(A2)  if the sample size n ≥ C0 · max
 
µ−
and ˆθ(0) is a feasible initialization (i.e.  f ( ˆθ(0)) ≤ f (θ∗)) 
1 − C2 exp(−C1m)  the following error bound holds for ˆθ(T ) returned by Algorithm 1

(cid:17)2
  κ2(cid:16) µ+
(cid:13)(cid:13)(cid:13)2
n ·(cid:16)(cid:13)(cid:13)(cid:13) ˆθ(0) − θ∗
in which ρn and emin satisfy the inequalities below with δn = C5τ 2(cid:112) m

(cid:17)2(cid:16) σ+∗
(cid:17)
n ≤ 1
4  

(cid:13)(cid:13)(cid:13) ˆθ(T ) − θ∗

then with probability at least

(cid:110) w4(C)

≤ emin + ρT

(cid:17)(cid:27)

− emin

· max

m   m

(cid:26)

−
µ−σ
∗

(cid:111)

−
∗
σ

 

(cid:13)(cid:13)(cid:13)2
µ−(cid:113)
µ−(cid:113)

C3κµ+
σ−∗ Tr(Σ−1∗ )

ρn ≤

emin ≤ C4κ(cid:112)µ+

Tr(Σ−1∗ )

· m + w(C)√

n

≤ 1
2

 

· m + w(C)√

n

· 1 + δn
1 − ρn

.

Remark: The inequality (30) indicates that the upper bound of the error for AltMin procedure will
decrease geometrically to the minimum achievable error emin with rate ρn. Though the initialization
condition f ( ˆθ(0)) ≤ f (θ∗) may not be true for arbitrary ˆθ(0)  it should be satisﬁed by the ﬁrst iterate
ˆθ(1)  from which Theorem 1 starts to apply.
Note that the ρn in (30) not only controls the convergence rate of error  but also affects the value of
emin. The emin is of the same order as the right-hand side of (24) with Σ = Σ∗  which has an extra
additive O
term compared with eorc. This is due to the uniformity considered for the θ-step
over all Σ ∈ M(+∞). To improve the bound for AltMin  we can consider a small e0 for M(e0).

(cid:16) m√

(cid:17)

n

Improved Bound with Good Initializations

3.3
As discussed above  we consider a smaller input region M(e0) for the θ-step with e0 =
Before presenting the results  we introduce the set called error spherical sector.

(cid:113) σ

−
∗
µ+ .

7

Deﬁnition 5 (error spherical sector) For a structure-inducing f  its error spherical sector is deﬁned
as

S = cone(cid:8)u ∈ Rp(cid:12)(cid:12) f (θ∗ + u) ≤ f (θ∗)(cid:9) ∩ Bp−1  

where Bp = {u | (cid:107)u(cid:107)2 ≤ 1} is the unit ball of Rp.
Geometrically S is closely related to the previously deﬁned C in (19)  and their Gaussian widths
satisfy w(S) ≤ w(C) + c for some universal constant c. Based on this deﬁnition  the following
theorem characterizes the sharpened error of AltMin under good initializations.

Theorem 2 (error bound for well-initialized AltMin) Under the assumptions (A1) and (A2)  if
the sample size n ≥ C0 · max
 

w2(C)   m2(cid:111)
(cid:17)2(cid:16) σ+∗
  κ2(cid:16) µ+
and a feasible initialization ˆθ(0) satisﬁes (cid:107) ˆθ(0) − θ∗(cid:107)2 ≤ (cid:113) σ
1 − C2 exp(cid:0)−C1 · min(cid:8)w2(C)  m(cid:9)(cid:1)  the error bound (30) holds for ˆθ(T ) returned by Algorithm 1

−
∗
σ
−
∗
µ+   then with probability at least

1  τ 4  κ4(cid:16) µ+σ+∗

(cid:110) w4(C)

m   m3

(cid:17)(cid:27)

· max

(cid:17)2

(cid:26)

(33)

µ−σ

−
∗

µ−

with ρn and emin satisfying

· w(S)√

n

≤ 1
2

 

(34)

C3κµ+
σ−∗ Tr(Σ−1∗ )

ρn ≤

emin ≤ C4κ(cid:112)µ+

· w(S)√

µ−(cid:113)
µ−(cid:113)

Tr(Σ−1∗ )
where δn is the same as the one given in Theorem 1.
Remark: Since w(S) only differs from w(C) by a constant  the above error bound matches the order
of the oracle error eorc. For instance  if θ∗ is s-sparse and f = (cid:107) · (cid:107)0  then w(S) and emin satisfy 

n

(35)

· 1 + δn
1 − ρn

 

(cid:16)(cid:112)s log p
(cid:17)

w(S) = O

(cid:32)(cid:114)

(cid:33)

=⇒

emin = O

s log p

n

The initialization condition is a result of setting a small value of e0  which yields an improved version
of Lemma 2 so that we can obtain a better bound in Theorem 2. A reasonably good initialization
of ˆθ(0) can be obtained by solving OLS ˆθodn  whose error bound is given (27). On the other hand 
the iterates obtained by running arbitrarily-initialized AltMin may also satisfy the initialization
requirements as Theorem 1 guarantees a moderate error. Once the requirements are met during the
iteration  the arbitrarily-initialized AltMin can attain this sharper bound as well as the well-initialized.

4 Experiments

In this section  we present some experimental results to support our theoretical analysis. Speciﬁcally
we focus on the sparsity structure of θ∗  and consider L0-cardinality as complexity function f.
Throughout the experiment  we ﬁx problem dimension p = 1000  sparsity level of θ∗ s = 20  and
number of iterations T = 10. Entries of X of ˜η are generated by i.i.d. standard Gaussian  and
θ∗ = [1  . . .   1

]T . Σ∗ is given as a block diagonal matrix with Σ(cid:48) =

 −1  . . .  −1

(cid:104) 1

  0  . . .   0

(cid:105)

a
1

a

(cid:124) (cid:123)(cid:122) (cid:125)

10

(cid:124)

(cid:123)(cid:122)

10

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

980

replicated along the diagonal. All the plots are obtained based on the average over 100 random trials.
First we set a = 0.9  m = 10  and vary sample size n from 30 to 80. We run the AltMin initialized
by both OLS and Gaussian random vector  where θ-step is solved by the hard-thresholding pursuit
(HTP) algorithm [11]. The error plots are shown in Figure 1. Second  we ﬁx m = 10  and vary the
parameter a in Σ∗ from 0.5 to 0.9 for n = 30  40  50 and 60. The plots in Figure 2(a) shows the
error of AltMin against a. As indicated by (29)  the improvement of the oracle least squares over the
ordinary one is ampliﬁed with increasingly large a. Figure 2(b) compares the actual ratio of eorc to
eodn and the suggested one. Finally we ﬁx a = 0.8  and the number of responses m ranges from 10
to 18 for n = 30  40  50 and 60. The results are presented in Figure 2(c) and 2(d).

8

(a) L2-error vs. n

(b) L2-error vs. t (rand initialization)

Figure 1: (a) A phase transition is observed for the randomly-initialized AltMin around n = 40  whose error is
on a par with the well-initialized for n ≥ 40. This coincides with the remark for Theorem 2. Also  the error of
AltMin is close to the oracle estimator  which is signiﬁcantly better than OLS. (b) Our theoretical results suggest
that a larger sample size leads to smaller ρn  thus AltMin converge faster as shown in the plots.

(a) L2-error vs. a

(b) eorc/eodn (n=60)

(c) L2-error vs. m

(d) L2-error vs. m (n=30)

Figure 2: (a) With a varying from 0.5 to 0.9  the responses become increasingly correlated and the error of
AltMin reduces more quickly. (b) The actual ratio of eorc to eodn is very close the predicted one given by (29).
(c) As m increases from 10 to 18  the error of AltMin does not decrease drastically. The main reason is the
increasingly large error in the estimation of Σ∗. (d) Compared with the error of OLS  the advantage of AltMin
becomes marginal with growing m  while its gap with the oracle estimator is widened.

5 Conclusions

In this paper  we investigate the alternating minimization (AltMin) algorithm for high-dimensional
multi-response linear models  which allow general structures of the underlying parameter. In particu-
lar  we present a resampling-free analysis for the statistical error of the non-convex AltMin procedure.
Our error bound matches the resampling-based result up to some constant  which is of the same order
as the oracle estimator. Above all  the error bounds suggest that the arbitrarily-initialized AltMin is
able to attain the same level of estimation error as the one with good initializations.

Acknowledgements
The research was supported by NSF grants IIS-1563950  IIS-1447566  IIS-1447574  IIS-1422557 
CCF-1451986  CNS- 1314560  IIS-0953274  IIS-1029711  NASA grant NNX12AQ39A  and gifts
from Adobe  IBM  and Yahoo.

9

30354045505560Samplesizen0.010.020.030.040.050.060.070.080.090.1NormalizedL2-errorforˆθTwell-initializedrandomly-initializedOLSoracle12345678910Iterationt0.10.20.30.40.50.60.7NormalizedL2-errorforˆθtn=30n=40n=50n=600.50.550.60.650.70.750.80.850.9a0.010.020.030.040.050.060.07NormalizedL2-errorforˆθTn=30n=40n=50n=600.50.550.60.650.70.750.80.850.9a0.40.50.60.70.80.9eorc/eodnactual ratiopredicted ratio101112131415161718Numberofrepsonsesm0.0150.020.0250.030.0350.040.0450.05NormalizedL2-errorforˆθTn=30n=40n=60n=80101112131415161718Numberofrepsonsesm0.0250.030.0350.040.0450.050.0550.06NormalizedL2-errorforˆθTAltMinOLSoracleReferences
[1] T. W. Anderson. An introduction to multivariate statistical analysis. Wiley-Interscience  2003.

[2] A. Banerjee  S. Chen  F. Fazayeli  and V. Sivakumar. Estimation with norm regularization. In

Advances in Neural Information Processing Systems (NIPS)  2014.

[3] R. Barber and W. Ha. Gradient descent with nonconvex constraints: local concavity determines

convergence. arXiv preprint arXiv:1703.07755  2017.

[4] S. Bhojanapalli  B. Neyshabur  and N. Srebro. Global optimality of local search for low rank
matrix recovery. In Advances in Neural Information Processing Systems  pages 3873–3881 
2016.

[5] L. Breiman and J. H. Friedman. Predicting multivariate responses in multiple linear regression.
Journal of the Royal Statistical Society: Series B (Statistical Methodology)  59(1):3–54  1997.

[6] E. J Candes  X. Li  and M. Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and

algorithms. IEEE Transactions on Information Theory  61(4):1985–2007  2015.

[7] V. Chandrasekaran  B. Recht  P. A. Parrilo  and A. S. Willsky. The convex geometry of linear

inverse problems. Foundations of Computational Mathematics  12(6):805–849  2012.

[8] S. Chatterjee  S. Chen  and A. Banerjee. Generalized dantzig selector: Application to the

k-support norm. In Advances in Neural Information Processing Systems (NIPS)  2014.

[9] S. Chen and A. Banerjee. Structured estimation with atomic norms: General bounds and

applications. In NIPS  pages 2908–2916  2015.

[10] S. Chen and A. Banerjee. Alternating estimation for structured high-dimensional multi-response

models. In Advances in Neural Information Processing Systems  pages 2835–2844  2017.

[11] S. Foucart. Hard thresholding pursuit: an algorithm for compressive sensing. SIAM Journal on

Numerical Analysis  49(6):2543–2563  2011.

[12] R. Ge  C. Jin  and Y. Zheng. No spurious local minima in nonconvex low rank problems: A

uniﬁed geometric analysis. arXiv preprint arXiv:1704.00708  2017.

[13] R. Ge  J. D Lee  and T. Ma. Matrix completion has no spurious local minimum. In Advances in

Neural Information Processing Systems  pages 2973–2981  2016.

[14] A. Goncalves  P. Das  S. Chatterjee  V. Sivakumar  F. J. Von Zuben  and A. Banerjee. Multi-task

sparse structure learning. In CIKM  pages 451–460  2014.

[15] A. Goncalves  F. J Von Zuben  and A. Banerjee. Multi-task sparse structure learning with
gaussian copula models. The Journal of Machine Learning Research  17(1):1205–1234  2016.

[16] Y. Gordon. Some inequalities for gaussian processes and applications.

Mathematics  50(4):265–289  1985.

Israel Journal of

[17] W. H. Greene. Econometric Analysis. Prentice Hall  7. edition  2011.

[18] A. J. Izenman. Modern Multivariate Statistical Techniques: Regression  Classiﬁcation  and

Manifold Learning. Springer  2008.

[19] P. Jain  P. Netrapalli  and S. Sanghavi. Low-rank matrix completion using alternating minimiza-

tion. In STOC  pages 665–674  2013.

[20] P. Jain and A. Tewari. Alternating minimization for regression problems with vector-valued
outputs. In Advances in Neural Information Processing Systems (NIPS)  pages 1126–1134 
2015.

[21] P. Jain  A. Tewari  and P. Kar. On iterative hard thresholding methods for high-dimensional

m-estimation. In NIPS  pages 685–693  2014.

10

[22] S. Kim and E. P. Xing. Tree-guided group lasso for multi-response regression with structured

sparsity  with an application to eqtl mapping. Ann. Appl. Stat.  6(3):1095–1117  2012.

[23] W. Lee and Y. Liu. Simultaneous multiple response regression and inverse covariance matrix
estimation via penalized gaussian maximum likelihood. J. Multivar. Anal.  111:241–255  2012.

[24] Q. Li and G. Tang. The nonconvex geometry of low-rank matrix optimizations with general

objective functions. arXiv preprint arXiv:1611.03060  2016.

[25] C. Ma  K. Wang  Y. Chi  and Y. Chen. Implicit regularization in nonconvex statistical esti-
mation: Gradient descent converges linearly for phase retrieval  matrix completion and blind
deconvolution. arXiv preprint arXiv:1711.10467  2017.

[26] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for the analysis

of regularized M-estimators. Statistical Science  27(4):538–557  2012.

[27] P. Netrapalli  P. Jain  and S. Sanghavi. Phase retrieval using alternating minimization. In NIPS 

2013.

[28] W. Oberhofer and J. Kmenta. A general procedure for obtaining maximum likelihood estimates
in generalized regression models. Econometrica: Journal of the Econometric Society  pages
579–590  1974.

[29] S. Oymak  B. Recht  and M. Soltanolkotabi. Sharp time–data tradeoffs for linear inverse

problems. arXiv preprint arXiv:1507.04793  2015.

[30] Y. Plan  R. Vershynin  and E. Yudovina. High-dimensional estimation with geometric constraints.

Information and Inference  2016.

[31] P. Rai  A. Kumar  and H. Daume. Simultaneously leveraging output and task structures for

multiple-output regression. In NIPS  pages 3185–3193  2012.

[32] A. J. Rothman  E. Levina  and J. Zhu. Sparse multivariate regression with covariance estimation.

Journal of Computational and Graphical Statistics  19(4):947–962  2010.

[33] J. Shen and P. Li. On the iteration complexity of support recovery via hard thresholding pursuit.

In International Conference on Machine Learning  pages 3115–3124  2017.

[34] J. Sun  Q. Qu  and J. Wright. A geometric analysis of phase retrieval. arXiv preprint arX-

iv:1602.06664  2016.

[35] J. Sun  Q. Qu  and J. Wright. Complete dictionary recovery over the sphere i: Overview and the

geometric picture. IEEE Transactions on Information Theory  63(2):853–884  2017.

[36] R. Sun and Z.-Q. Luo. Guaranteed matrix completion via nonconvex factorization. In FOCS 

2015.

[37] M. Talagrand. The Generic Chaining. Springer  2005.

[38] M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer  2014.

[39] S. Tu  R. Boczar  M. Simchowitz  M. Soltanolkotabi  and B. Recht. Low-rank solutions of

linear matrix equations via procrustes ﬂow. arXiv preprint arXiv:1507.03566  2015.

[40] V. Vapnik. Statistical learning theory. Wiley New York  1998.

[41] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and
G. Kutyniok  editors  Compressed Sensing  chapter 5  pages 210–268. Cambridge University
Press  2012.

[42] R. Vershynin. Estimation in High Dimensions: A Geometric Perspective  pages 3–66. Springer

International Publishing  2015.

[43] Irène Waldspurger. Phase retrieval with random gaussian sensing vectors by alternating projec-

tions. IEEE Trans. Information Theory  64(5):3301–3312  2018.

11

[44] X. Yi  C. Caramanis  and S. Sanghavi. Alternating minimization for mixed linear regression. In

ICML  pages 613–621  2014.

[45] Q. Zheng and J. Lafferty. A convergent gradient descent algorithm for rank minimization
and semideﬁnite programming from random linear measurements. In Advances in Neural
Information Processing Systems  pages 109–117  2015.

12

,Sheng Chen
Arindam Banerjee