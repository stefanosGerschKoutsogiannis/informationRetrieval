2018,Provable Variational Inference for Constrained Log-Submodular Models,Submodular maximization problems appear in several areas of machine learning and data science  as many useful modelling concepts such as diversity and coverage satisfy this natural diminishing returns property. Because the data defining these functions  as well as the decisions made with the computed solutions  are subject to statistical noise and randomness  it is arguably necessary to go beyond computing a single approximate optimum and quantify its inherent uncertainty. To this end  we define a rich class of probabilistic models associated with constrained submodular maximization problems. These capture log-submodular dependencies of arbitrary order between the variables  but also satisfy hard combinatorial constraints. Namely  the variables are assumed to take on one of — possibly exponentially many — set of states  which form the bases of a matroid. To perform inference in these models we design novel variational inference algorithms  which carefully leverage the combinatorial and probabilistic properties of these objects. In addition to providing completely tractable and well-understood variational approximations  our approach results in the minimization of a convex upper bound on the log-partition function. The bound can be efficiently evaluated using greedy algorithms and optimized using any first-order method. Moreover  for the case of facility location and weighted coverage functions  we prove the first constant factor guarantee in this setting — an efficiently certifiable e/(e-1) approximation of the log-partition function. Finally  we empirically demonstrate the effectiveness of our approach on several instances.,Provable Variational Inference for

Constrained Log-Submodular Models

Josip Djolonga

Dept. of Computer Science

ETH Zürich

Andreas Krause

Dept. of Computer Science

ETH Zürich

josipd@inf.ethz.ch

stefje@csail.mit.edu

krausea@ethz.ch

Stefanie Jegelka

CSAIL
MIT

Abstract

Submodular maximization problems appear in several areas of machine learning
and data science  as many useful modelling concepts such as diversity and coverage
satisfy this natural diminishing returns property. Because the data deﬁning these
functions  as well as the decisions made with the computed solutions  are subject to
statistical noise and randomness  it is arguably necessary to go beyond computing a
single approximate optimum and quantify its inherent uncertainty. To this end  we
deﬁne a rich class of probabilistic models associated with constrained submodular
maximization problems. These capture log-submodular dependencies of arbitrary
order between the variables  but also satisfy hard combinatorial constraints. Namely 
the variables are assumed to take on one of — possibly exponentially many — set
of states  which form the bases of a matroid. To perform inference in these models
we design novel variational inference algorithms  which carefully leverage the
combinatorial and probabilistic properties of these objects. In addition to providing
completely tractable and well-understood variational approximations  our approach
results in the minimization of a convex upper bound on the log-partition function.
The bound can be efﬁciently evaluated using greedy algorithms and optimized using
any ﬁrst-order method. Moreover  for the case of facility location and weighted
coverage functions  we prove the ﬁrst constant factor guarantee in this setting — an
efﬁciently certiﬁable e/(e− 1) approximation of the log-partition function. Finally 
we empirically demonstrate the effectiveness of our approach on several instances.

1

Introduction

Many real-world tasks can be modeled as distributions over combinatorial objects such as trees 
assignments or selections. As an illustrative example  let us consider the following scenario inspired
by the recent work of Celis et al. [1]. Assume that we are building a news aggregator and are faced
with the task of populating the limited number of slots on the front page with articles originating from
various news outlets. We furthermore assume that we have a function that  given a news article and a
slot  estimates how good of a match they are. Hence  if we decide that a certain subset of the articles
should be shown  we can compute their optimal assignment using a maximal bipartite matching.
Furthermore  to make sure that a diverse set of points of views are represented  we want the chosen
articles to not only have a high matching value  but to also come from different sources. This can
be enforced using a hard selection constraint — for example  we can require that each source j has
exactly kj articles on the front page. While the optimization problem has been well-studied as it is
that of submodular maximization  taking a probabilistic approach seems very challenging. Not only
the random variables have to satisfy complicated combinatorial requirements  but the utility function
is only implicitly deﬁned via optimal matchings and is very challenging for many approximate
inference techniques. Nevertheless  by exploiting the submodular properties of the objective and

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

the combinatorial and probabilistic properties of matroids we will develop a method that can easily
handle such models with combinatorial constraints and complex long-ranging variable interactions.
Another family of constraints that often appears is that of (directed) spanning trees. Namely  we are
only interested in subsets of edges of a graph that form a spanning tree. Such constraints can model
information cascades in network inference [2]  or non-projective dependency parse trees in natural
language processing [3  4]. Moreover  the 1-of-K encoding typically used for multi-label inference
tasks is perhaps the simplest and most frequent case of a hard combinatorial assignment constraint.
What all these applications have in common is that they give rise to joint distributions over a set of
dependent random variables  each of which is itself a combinatorial object (a spanning tree in network
inference and dependency parsing; a discrete selection in multi-label inference and slot allocation).
Inference in such combinatorial models is complex due to two sources of dependencies. First  the
distribution may express pairwise or higher-order dependencies between elements (in our previous
example  the value of the optimal matching). Second  we have strict combinatorial constraints on the
support of the distribution (e.g.  only trees are allowed) that implicitly induce further interactions.
In this work  we undertake a variational inference approach and approximate these rich distribu-
tions with simpler ones that respect the combinatorial constraints but are fully tractable. These
approximations posses very strong negative association properties  which we utilize in our theory.
To ﬁnd the optimal approximation we minimize a Rényi divergence over these distributions  which
results in efﬁciently minimizable convex upper bounds on the partition function. While variational
inference methods rarely provide any approximation guarantees  our approach yields provably good
approximations for certain model families. In summary  this paper makes the following contributions.
• Fast variational convex algorithms for a large family of probabilistic models with submodular
dependencies of arbitrarily high order in combination with hard combinatorial constraints.
• By combining results from approximate inference and submodular maximization  we prove
the ﬁrst constant factor approximation on the log-partition function for facility location and
weighted coverage functions under a family of matroid constraints. We speciﬁcally show
that our upper bound does not exceed the true value by more than a factor of (1 − 1/e).

• An empirical evaluation of the proposed techniques on several problem instances.

Related work. Bouchard-Côté and Jordan [5] introduce a class of variational techniques over
combinatorial spaces  but they make a different set of assumptions — they assume a product space
and models that are tractable when retaining only one of the constraints. There has also been interest
in applying belief propagation (BP) to structured problems such as dependency parsing [3]. Our
approach makes a different set of factorization assumptions  and in contrast to BP  provides a bound on
the partition function and is guaranteed to converge without any damping heuristics. Other methods
that provide upper bounds make factorization assumptions not satisﬁed by the models we consider
[6  7  8]  or have to repeatedly solve hard optimization problems [9  10]. MCMC sampling methods
for distributions over more general combinatorial objects have been addressed in a rich literature
[11]. Li et al. [12] consider distributions over partition and uniform matroids that also allow for
non-linear dependencies between the variables and develop Gibbs samplers whose mixing time grows
exponentially with the non-linearity of the model. In the unconstrained case  the mixing time as a
function of non-submodularity has been analyzed in [13  14].
Variational inference in unconstrained probabilistic submodular models was considered by Djolonga
and Krause [15]  whose inference method for log-supermodular models was shown to be equivalent
to the minimization of the inclusive Rényi divergence [16]  which we also use as the variational
objective in this paper. The minimization of this divergence for decomposable unconstrained models
has been studied in Djolonga et al. [17]  who also utilize the M (cid:92)-concavity of the terms. Inference
in multi-label log-supermodular models has been considered by Zhang et al. [18]. The tractable
distributions used in our variational framework have been already studied [19  20  21]. Some of them
are determinantal point processes (DPPs)  which have been already used in machine learning [22].
Risteski [23] has proved a constant factor approximation for the log-partition function of certain Ising
models using a variational approach  and is also leveraging the mean-ﬁeld bound in the proof.

2

2 Background — submodularity  matroids and continuous extensions

Submodularity [24  25] formalizes the concept of diminishing returns — the beneﬁt of adding an
element decreases with the growth of the context in which it is being included. Formally  a set
function F : 2V → [0 ∞) is said to be submodular if for all X ⊆ Y ⊆ V and i ∈ V \ Y it holds
that F (i | X) ≥ F (i | Y )  where the marginal gain F (i | X) is deﬁned as F ({i} ∪ X) − F (X). To
keep the notation as simple as possible   we will w.l.o.g. assume that V = {1  2  . . .   n}.
A classical family of submodular functions are set cover functions. If we associate to each i ∈ V a
set Ui ⊆ U of elements from some ﬁnite universe U  the function is given as the size of the union of
the chosen sets  i.e.  F (X) = | ∪i∈X Ui|. Another well-known function class are facility location
j=1 maxi∈X wi j for some non-negative weights wi j ≥ 0. The
name stems from the following scenario: a set of facilities V serve m customers such that customer j
receives a utility of wi j from facility i ∈ V   and F (X) measures the total utility from the facilities X
if each customer can be served by exactly one facility. Moreover  many problems  such as exemplar
clustering  which we use in the experimental section  can be modelled using this function class.

functions  deﬁned as F (X) =(cid:80)m

Maximization. As both examples above model utilities  a natural problem that arises is that of
ﬁnding a conﬁguration X ⊆ V that maximizes F — cover as much as possible from U  or serve
as many customers as possible from the opened facilities. Note that the above functions are not
only submodular  but also monotone — adding an item can never decrease the value. Moreover  we
typically want to ﬁnd the maximal X subject to some constraints. A classical problem is that of
maximizing over all sets of cardinality at most k. In this case  Nemhauser et al. [26] have proven that
a simple greedy algorithm results in a provably good solution. Speciﬁcally  we start with X0 = ∅ 
and construct the set Xj+1 as the union of Xj and any element in arg maxi∈V \Xj F (i | Xj). Then 
the guarantee is that F (Xk) ≥ (1 − 1/e) maxX : |X|≤k F (X)  which is also optimal unless P = NP.

M (cid:92)-concavity. There exists a subclass of submodular functions for which the above algorithm
exactly maximizes the function even when it is not monotone  if we stop once we see a negative
gain. These functions  known as M (cid:92)-concave [27  §4]  are deﬁned as follows: for all X  Y ⊆ V and
i ∈ X \ Y either (i) F (X) + F (Y ) ≤ F (X \ {i}) + F (Y ∪ {i})  or (ii) there exist some j ∈ Y \ X
such that F (X) + F (Y ) ≤ F (X \ {i} ∪ {j}) + F (Y \ {j} ∪ {i}). Moreover  it also holds that
Xk = arg maxX : |X|=k F (X) [28  Lem. 6.3  29  30]. This family contains (see e.g. [31  §3.6]) the
maximum function maxi∈X wi j  weighted matroid rank functions  the value of the optimal bipartite
j=1 φj(|X ∩ Bj|)
for any concave φj : R → R and laminar {Bj}m
j=1. While not M (cid:92)-concave themselves  many
submodular functions  such as facility location  can be written as sums of M (cid:92)-concave terms — a
fact that we will exploit later on in this paper.

matching used in the introduction  as well as functions of the form F (X) =(cid:80)m

Matroids. Submodular maximization has been studied not only under cardinality constraints  but
also under a broader set of structures that have particularly nice mathematical properties: matroids.
Deﬁnition 1 (Oxley [32]). A matroid M consists of a ground set V = {1  2  . . .   n} and a collection
I ⊆ 2V of subsets of V (called independent) that satisfy:
(i) ∅ ∈ I.
(ii) If X ∈ I and Y ⊆ X then Y ∈ I.
(iii) If X ∈ I and Y ∈ I and |X| < |Y | then there exists some y ∈ Y \ X such that X ∪ {y} ∈ I.
A set X ∈ I is maximal if for all e ∈ V \ X  we have that X ∪ {e} /∈ I. We will focus on the
case when M is the collection of all maximal sets in I. These maximal sets are the bases of the
matroid. This framework encompasses for instance both the cardinality constraints and spanning
trees. Namely  the set I = {X ⊆ V | |X| ≤ k} is known as the uniform matroid and its bases are all
subsets of cardinality exactly k  while the set of spanning trees form the bases of the graphic matroid 
deﬁned as the collection of edge subsets that are cycle-free. This latter example belongs to the family
of regular matroids that are deﬁned as follows. Let

U = [ u1 u2

··· un ] ∈ {0 ±1}r×n

be a totally unimodular (TU) matrix  meaning that every square submatrix of U has a determinant in
{0 ±1}. A subset X ⊆ V is said to be independent if the columns of U indexed by X are linearly

3

independent. The bases of this matroid are the subsets of the columns of U that form a basis of the
column space of U. We can think of the i-th column ui as the representation of element i. As a
concrete example  the graphic matroid of a graph G = (V E) is generated by the (arbitrarily oriented)
edge-vertex incidence matrix U ∈ {0 ±1}(|V|−1)×|E| of G after removing an arbitrary vertex.

3 The problem and our approach
Formally  we have a random variable X that takes values in a set of combinatorial objects M. For
example  X could be a random tree drawn from the collection M of all trees in some graph G. We can
think of the members of M as being the valid conﬁgurations among all possible sets in 2V   so that
any conﬁguration not in M should get a probability of zero. Speciﬁcally  we consider distributions
over the conﬁgurations M of the general form

P(X = X) =(cid:74)X ∈ M(cid:75)Z

(cid:16)

exp

F (X)

(cid:17)

 

where F : 2V → R is the objective function and(cid:74)·(cid:75) is the Iverson bracket. Note that the problem of
(i) It holds that F =(cid:80)m

computing the MAP conﬁguration reduces to maxX∈M F (X)  which is can be approximated within
a factor of a (1 − 1/e) [33] when F is monotone submodular and M are matroid bases. We make the
following additional assumptions about F and the set M constituting the support of the distribution.

j=1 Fj for some monotone M (cid:92)-concave functions Fj : 2V → [0 ∞).

(ii) The set M consists of the bases of a matroid  which is a direct sum of uniform and totally

unimodular matroids  which we will call normalizable.

(1)

j=1 maxi∈X wi j +(cid:80)

coverage  i.e.  of the form F (X) =(cid:80)

i∈U wi(cid:74)i ∈ ∪i∈X Ui(cid:75)  where Ui and U are deﬁned as in the

F (X) = (cid:80)m
to be included. In addition  there is the modular function(cid:80)

We would like to point out that the model class is closed under conditioning  as M (cid:92)-concave functions
are closed under restrictions  and both uniform and TU matroids are closed under taking minors. The
MAP problem under (i) has been studied in [34]. Note that  unlike many inference methods  we make
no assumption about the number of variables that each Fj depends on  also known as its order.
We will pay special attention to the case when F is a facility location  or equivalently  a weighted
unweighted case  and wi ≥ 0 are arbitrary weights. As a speciﬁc instantiation  let us consider
the FLID model of Tschiatschek et al. [35]  which has been successfully applied to the problem
of item set recommendation. Speciﬁcally  we have a set of items V = {1  2  . . .   n} that we want
to recommend to the user. Moreover  we assume that there are a total of m traits  and item i
expresses a level of wi j ≥ 0 for trait j ∈ {1  2  . . .   m}. Then  the idea is that the function
i∈X ui captures the classical notion of substitutes — once we
select an item that has a high expression level of some item  those items similar to it will be less likely
i∈X ui to model the quality of individual
items. Similarly to the example in the introduction  we can explicitly enforce the user to see a diverse
set of offers by for example presenting them with a ﬁxed number of items from each brand — if items
Xp are produced by producer p  then we can use M = {X ⊆ V | ∀p : |Xp ∩ X| ≤ kp}  also called
a partition matroid  which as a direct sum of uniform matroids  satisﬁes our modelling assumptions.
The central problem of interest in this paper is to compute marginal probabilities P(Y ⊆ X) for
any set Y ⊆ V . In its general form  this problem is hard  owing to the presence of the intractable
normalizer Z  whose computation is also important for the computation of likelihoods and model
selection. We therefore revert to approximate techniques for computing the marginal probabilities
and the partition function Z. Speciﬁcally  we will undertake a divergence minimization approach 
which will yield both an estimate of log Z and approximate marginals. Namely  we will ﬁrst deﬁne a
set of approximate distributions Q that are rich enough to capture some of the properties of the target
distribution P  but are computationally tractable. Then  we will ﬁnd the distribution Q in Q that is the
closest to P  as measured by some measure of distributional discrepancy  also called a divergence.

“Simple” distributions over matroid bases

4
We begin with a characterization of the distributions Q that will serve as approximations. These
distributions correspond to modular objective functions  so that for some θ ∈ Rn they are given as

4

at coordinates X. Formally  they belong to the exponential family and can be written as

e∈X θe = θ(cid:62)1X  where 1X ∈ {0  1}n is the characteristic vector of X with ones only

F (X) =(cid:80)
Qθ(X = X) = exp(θ(cid:62)1X − A(θ))(cid:74)X ∈ M(cid:75).
Here  A(θ) = log(cid:80)
(2)
X∈C exp(θ(cid:62)1X ) is the normalizing log-partition function  and M is the set of
bases of the considered matroid classes. Because of the constraint(cid:74)X ∈ M(cid:75)  the distribution is not a
product distribution  and the elements i ∈ V are not independent. Even though computing A(θ) can
be challenging for arbitrary constraints  it can be efﬁciently done for the considered classes. In what
follows we will assume that we have a single normalizable matroid  as the result for their direct sums
easily follows. In the uniform matroid case  (2) is known as a cardinality potential  and both A(θ)
and the unary marginals can be computed in O(nk) using the algorithm of Tarlow et al. [19]. If M is
a regular matroid  the model can be efﬁciently normalized via the celebrated matrix-tree theorem.

Theorem 1 (Maurer [36]). For regular matroids  it holds that A(θ) = log det(cid:80)n

i=1 eθiuiu(cid:62)
i .

Lyons [20] showed that the distribution (2) is a determinantal point process (DPP) with the scaled rep-
resentation Uθ = (U diag(exp(θ))U(cid:62))−1/2U diag(exp(θ/2))  and can be marginalized as follows.
Theorem 2 ([20  Remark 5.6]). The marginal probability of any Y ⊆ V is equal to

where K = U(cid:62)

(3)
θ Uθ ∈ Rn×n and KY is the submatrix formed by the rows and columns indexed by Y .

P (Y ⊆ X) = det KY  

e(cid:48)(cid:105)2 

e  uθ

e(cid:107)2(cid:107)uθ

e(cid:48)(cid:107)2 − (cid:104)uθ

For example  the ﬁrst and second order moments are given by
e(cid:107)2  and P ({e  e(cid:48)} ⊆ X) = (cid:107)uθ

P (e ∈ X) = (cid:107)uθ

which we now present. The marginals  i.e.  the vector µ ∈ [0  1]n with entries µi = EX∼Qθ [(cid:74)i ∈ X(cid:75)] 

(4)
which implies that the elements e  e(cid:48) are negatively correlated: their joint probability is smaller than if
they were independent. Moreover  an even stronger condition can be stated — both cases are strongly
Rayleigh [37  Coro. 4.18  Prop. 3.5]  so that for any Q ∈ Q we have that EA∼Q[G(A)H(A)] ≤
EA∼Q[G(A)]EA∼Q[H(A)] for any monotone functions G and H that depend on disjoint coordinates1.
As Q is an exponential family  it has many remarkable properties (for proofs see e.g. [38])  some of
can be easily computed from the log-partition function as µ = ∇A(θ). An important object
associated with Q is the marginal polytope  the set of all realizable unary marginals by any distribution
over M. In our case  it is equal to the convex hull of the bases  i.e.  M = conv{1A | A ∈ M}.
Remarkably  Q is rich enough to represent any marginal vector in relint M  i.e.  ∀µ ∈ relint M there
exists some θ(µ) ∈ Rn such that Ex∼Qθ(µ)[x] = µ. Furthermore  the convex conjugate A∗(µ) of
the log-partition function A evaluates to ∞ if µ /∈ M  and to −H[Qθ(µ)] otherwise  where H[·] is
Shannon’s entropy function. Moreover  we can optimize linear functions over M using Edmonds’
[25] celebrated algorithm in O(n log n). Namely  to solve maxµ∈M µ(cid:62)θ  ﬁrst sort θ in descending
order θσ(1) ≥ θσ(1) ≥ . . . ≥ θσ(n)  and deﬁne the chain

X0 = ∅  and Xi =

(cid:26)Xi−1 ∪ {σ(i)}

Xi−1

if Xi−1 ∪ {σ(i)} ∈ I
otherwise

.

Then  it can be shown that 1Xn is a maximizer. For spanning trees  this is exactly Kruskal’s algorithm.

5

Inference using the inclusive inﬁnite Rényi divergence

Having ﬁxed the approximation family  we turn to the choice of the function that will quantify the
distance between the distributions  and the analysis of resulting optimization problem. In this paper
we will use the inclusive Rényi ∞-divergence [39  40]  deﬁned as

D∞(P(cid:107) Q) = log max
X∈M

(5)
In other words  it evaluates to the worst-case log-ratio between P and Q. In the terminology of Minka
[41]  it is an inclusive (zero-avoiding) divergence — it prefers more conservative distributions that do
1The case M = {X | |X| ≤ k} can be also normalized and Q is again strongly Rayleigh [37  Cor. 4.18].

P(X)/Q(X).

5

(cid:98)f (cid:63)(y | M) = inf x∈M y(cid:62)1X − F (X)  which is easily seen to be concave. Then  by expanding the

not assign event probabilities close to zero or one. To better understand the optimization problem
that results from minimizing D∞  let us ﬁrst deﬁne for F : 2V → R and any M ⊆ 2V the function
divergence and minimizing with respect to Qθ ∈ Q we obtain the following upper bound2

log Z ≤ inf
θ∈Rn

A(θ) − (cid:98)f (cid:63)(θ | M) = sup

µ∈M(cid:98)f (µ | M) − A∗(µ) 

where (cid:98)f (µ | M) = (cid:98)f (cid:63)(cid:63)(µ | M) = inf y∈Rn µ(cid:62)y − (cid:98)f (y | M) is the concave conjugate of
(cid:98)f (cid:63)(y | M)  and the equality follows from Fenchel’s duality. Unfortunately  we can not evaluate the
above bound as we do not know how to compute (cid:98)f (cid:63)(µ | M)  which requires the maximization of a

non-monotone function over M. We do  however  know that F decomposes as a sum of M (cid:92)-concave
functions  which we can leverage to obtain a more tractable bound using dual decomposition [42  43].
Proposition 1. By applying dual decomposition to (6) we arrive at the following bound

(6)

(7)

log Z ≤ inf
{θj}m

j=1

m(cid:88)

j=1

θj) − m(cid:88)
(cid:123)(cid:122)

j=1

A(

(cid:124)

(cid:98)f (cid:63)
j (θj | M)
(cid:125)

R(θ1 ... θm|M)

m(cid:88)

j=1

= sup
µ∈M

(cid:98)fj(µ | M) − A∗(µ).

Now  instead of maximizing F over M  we only have to maximize only each component Fj. Because
we have assumed that each Fj is M (cid:92)-concave  if M is a uniform matroid remember that we can easily
solve the resulting problem max|X|=k F (X) − y(cid:62)1X using the greedy strategy. Even though the
general case seems much harder  it can be solved using Murota’s duality theorem [27  Thm. 8.21(i)]
by introducing a set of m auxiliary variables {λj ∈ Rn}m
j=1 over which we also have to minimize.
Proposition 2. For any set of parameters {θj ∈ Rn}m

j=1 it holds that

m(cid:88)

θj)− m(cid:88)

j=1

j=1

A(

(cid:98)f (cid:63)
j (θj | M) = inf
{λj}m

m(cid:88)

θj)− m(cid:88)

A(

(cid:98)f (cid:63)
j (λj | V ) +

m(cid:88)

j=1

j=1

j=1

j=1

µ(cid:62)(λj − θj). (8)

sup
µ∈M

Note that it is easy to both evaluate this bound and compute a subgradient. Namely  we can compute
both the log-partition function and its derivatives using the methods from Section 4. The computation

of both (cid:98)f (cid:63) and the linear maximization over M can be done using greedy algorithms  and the

computed maxima are members of the corresponding subdifferentials. Hence  we can easily employ
ﬁrst-order convex methods to optimize this bound to arbitrary precision in polynomial time.

The facility location case. We will now prove a strong theoretical guarantee for the quality of the
computed approximation for this important class. Speciﬁcally  we will show that the obtained upper
bound is no greater than (1 − 1/e)−1 log Z ≈ 1.582 log Z. To this end  we ﬁrst construct a lower
bound on log Z  and then show that the lower and upper bounds are within a multiplicative constant of
each other. Moreover  this lower bound can be easily evaluated  so that we can at any point return not
only a bound  but also a corresponding certiﬁcate. We begin by introducing the multi-linear extension
˜f : [0  1]n → R [33] of F   deﬁned as ˜f (µ) = Exi∼Bernoulli(µi)[F (x)]. It can be evaluated within any
accuracy using Monte-Carlo sampling  and also analytically for several cases such as facility location
functions (see e.g. [44]). To derive the bound  we start from the mean-ﬁeld bound [38] (details in
appendix) EX∼Q[F (X)] + H[Q] ≤ log Z  which holds for any distribution Q absolutely continuous
with respect to P. Then  we use a result by Chekuri et al. [45  Lem. VI.1]  which states that if F is a
weighted sum of coverage functions and Q is negatively associated with unary marginals µ ∈ [0  1]n
— both conditions satisﬁed for our model — then EX∼Q[F (X)] ≥ ˜f (µ).
Proposition 3. If F is a facility location function  then for any θ ∈ Rn it holds that

L(θ) = ˜f (∇A(θ)) + H[Qθ] = ˜f (∇A(θ)) + A(θ) − ∇A(θ)(cid:62)θ ≤ log Z.

(9)

replace (cid:98)f (cid:63)

j (y | M) by (cid:98)f (cid:63)

We will actually prove a stronger result that holds not only for (7)  but also if we relax the bound and
j (y | V )  i.e.  we ignore the constraints when we maximize. In other words 

2We defer the proofs of all results in this section to the appendix.

6

we will show that the bound

m(cid:88)

j=1

θj) − m(cid:88)
(cid:123)(cid:122)

j=1
R(θ1 ... θj )

f (cid:63)
j (θj)

(cid:125)

inf
{θj}m

j=1

A(

(cid:124)

m(cid:88)

j=1

(cid:98)fj(µ) − A∗(µ) 

= sup
µ∈M

(10)

immediately clear from their deﬁnitions  both (cid:98)f (µ | V ) and ˜f (µ) are extensions of F — if we see

is within a multiplicative constant of L evaluated at any optimizer of (10). Even though perhaps not
F as being deﬁned over {0  1}n instead of 2V using the natural bijection  then both of them agree
with F for binary vectors and continuously ﬁll in the rest of the unit cube. Moreover  they are closely
related via the following result known as the correlation gap inequality.
Theorem 3 ([46  Lem. 3.8  47]). If F : 2V → R is monotone submodular with F (∅) = 0  then

∀µ ∈ [0  1]n : (1 − 1/e)(cid:98)f (µ | V ) ≤ ˜f (µ) ≤ (cid:98)f (µ | V ).

m(cid:88)

j}m
m(cid:88)
m) ≤ (1 − 1/e)−1L(

By combining these two results  we can ﬁnally prove the approximation result claimed above.
Theorem 4. If F is a facility location function and {θ∗

j=1 minimizes (7) or (10)  then

L(

j ) ≤ log Z ≤ R(θ∗
θ∗

1  . . .   θ∗

j ) ≤ (1 − 1/e)−1 log Z.
θ∗

(11)

j=1

by computing C(θ1  . . .   θm) = R(θ1  . . .   θm)/L((cid:80)m

Furthermore  at any point during the optimization we can easily certify our approximation quality
j=1 θ∗
j )  as the true approximation factor
R(θ1  . . .   θm)/ log Z is guaranteed to be upper bounded by it.

j=1

6 Experiments

We perform numerical experiments to better understand the practical performance of the proposed
methods  namely how good is the approximation when compared to the theoretical e/(e − 1) factor
and how well are the marginals estimated. Moreover  we showcase the scalability of our approach
by performing inference on large real-world instances. The implementation was done in Python
using PyTorch  and we optimize the bound using subgradient descent. The computation of the
log-partition function and its gradients (building on the code from [48])  as well as the greedy oracle
were implemented in C++. We provide all details in the appendix.

6.1 Synthetic experiments

facility location models with objectives of the form F (X) =(cid:80)20

We begin by comparing the accuracy of the methods on a set of synthetic experiments. We consider
j=1 maxi∈X wi j  where we sample
wi j ∼ Uniform[0  α]. We vary the inverse temperature parameter α and show the results in Figure 1.
We ﬁrst used a uniform matroid constraint |X| = 5 over a ground set of size n = 40. For the same
models we then considered partition constraints by partitioning V into three sets V1  V2 and V3 of sizes
10  10  and 20 respectively and deﬁning M = {X ⊆ V | |X ∩ V1| = 2 |X ∩ V2| = 2 |X ∩ V3| = 4}.
Because the number of conﬁgurations is in the millions  we were able to compute the exact marginals
and log-partition functions. From the plots we can see that the approximation is much better than the
theoretical factor (≈ 1.582)  and close to exact in the small and high temperature regimes. Moreover 
even though the divergence we are optimizing does not necessarily target the marginals  we can see
that they are also approximated within a small error.

6.2 Real data

We consider two problems from data mining that can be written as facility location maximization
problems under cardinality constraints. For each function F (A) we perform inference in models with
objectives αF (A) for varying α ≥ 0. Moreover  to obtain statistical estimates on the approximation
factors  we repeat the experiments several times by taking random subsets of the data.
Exemplar clustering. Given a dataset X = {x1  x2  . . .   xn} of n points in Rd  we want to ﬁnd a small
i=1 minxj∈A (cid:107)xi−xj(cid:107).

subset of size k = 10 that is a good summary of X by minimizing G(A) =(cid:80)n

7

(a) Inference on synthetic models under a uniform matroid constraint |X| = 6.

(b) Inference on synthetic models under a partition matroid constraint with 3 blocks of sizes 10  10 and 15.

Figure 1: Results on synthetic facility location models on a ground set of size n = 40. The parameters are
sampled from UNIFORM(0  α)  and there are m = 10 components. The ordinates on plots in the ﬁrst column
have been centered so that zero corresponds to the true partition function. In the last column we plot both the
certiﬁed approximation factor (the ratio of the upper bound and the certiﬁcate) and the exact one (when dividing
by the exact partition function). The error bars indicate three standard deviations from 20 repetitions.

(a) Sensor placement under uniform (left) and partition (right) matroids.(b) Exemplar clustering (CIFAR10).

Figure 2: Results on large real-world datasets (full explanation in §6.2). The error bars indicate three standard
deviations from 20 repetitions. Note that the certiﬁcate is signiﬁcantly lower than the theoretical factor of 1.582.

detection time  and the total utility is naturally captured using F (A) = (cid:80)m

While −G is not submodular  it can be shown [49] that F (A) = G({x0}) − G(A ∪ {x0}) is
monotone submodular for carefully chosen x0  typically taken to be the origin. We show our results
in Figure 2(b)  on n = 1500 points from the CIFAR10 [50] dataset normalized as in [44].
Sensor placement. The second problem is that of placing sensors at pipe junctions in order to
effectively detect water contaminations. Namely  there a total of n locations where we can place
our sensors  and a set of m possible contamination scenarios. For each scenario j and sensor i
there is some utility wi j ≥ 0 if i detects contamination j  computed e.g. as a function of the the
j=1 maxi∈A wi j. We
use a subset of the data from [51]  and show the results in Figure 2(a). We consider two scenarios
— (i) n = 5000  m = 300 under a cardinality constraint M = {X ⊆ V | |X| = 50}  and (ii)
n = 1500  m = 100 under a partition matroid  constructed by splitting V into 3 blocks of equal size 
and consider only distributions that pick exactly 5  10 and 5 points from each block respectively.
Despite the fact that these models have a much larger number of variables and components in the
objective  in Figure 2 we see a behaviour similar to that of the synthetic instances — the certiﬁcate
of the approximation factor is close to one under high and low temperatures (large and small α
respectively)  while remaining always signiﬁcantly smaller than the theoretical guarantee.

8

10−1100101102103α−400−300−200−1000100200ValueofthefunctionminuslogZUpperboundRLowerboundL10−1100101102103α0.000.050.100.150.20Meanabsoluteerrorofthemarginals10−1100101102103α1.001.021.041.061.08ApproximationfactorCertiﬁcateCExact10−1100101102103α−400−300−200−1000100200ValueofthefunctionminuslogZUpperboundRLowerboundL10−1100101102103α0.000.050.100.150.20Meanabsoluteerrorofthemarginals10−1100101102103α1.001.011.021.031.041.05ApproximationfactorCertiﬁcateCExact10−1100101102103α1.001.011.021.031.041.051.061.07ApproximationfactorCertiﬁcateC10−1100101102103α1.0001.0251.0501.0751.1001.1251.150ApproximationfactorCertiﬁcateC10−1100101102103α1.001.011.021.031.04ApproximationfactorCertiﬁcateC7 Conclusion

We explored a new  rich class of probabilistic models  whose variables realize bases of a sum of
normalizable matroids. These models allow to capture high-order submodular dependencies between
complex combinatorial objects. We presented efﬁcient  convergent convex variational inference
algorithms that yield upper bounds on the partition function. Moreover  we proved the ﬁrst constant
factor approximation on the log-partition function of facility location and weighted models under
constraints. We also numerically showcased the quality of the estimated partition function and
the marginals. Our models and methods provide important steps towards exploiting combinatorial
structure for principled modeling and reasoning about complex real-world phenomena.

Acknowledgements. The research was partially supported by ERC StG 307036  Google European
PhD Fellowship  and NSF CAREER award 1553284.

References

[1] L. E. Celis  V. Keswani  D. Straszak  A. Deshpande  T. Kathuria  and N. K. Vishnoi. “Fair and

Diverse DPP-based Data Summarization”. arXiv preprint arXiv:1802.04023 (2018).

[2] M. Gomez-Rodriguez  J. Leskovec  and A. Krause. “Inferring networks of diffusion and

inﬂuence”. ACM Transactions on Knowledge Discovery from Data (TKDD) (2012).

[3] D. A. Smith and J. Eisner. “Dependency parsing by belief propagation”. Proceedings of
the Conference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics. 2008.

[4] T. Koo  A. Globerson  X. Carreras  and M. Collins. “Structured prediction models via the
matrix-tree theorem”. Proceedings of the 2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning (EMNLP-
CoNLL). 2007.

[5] A. Bouchard-Côté and M. I. Jordan. “Variational inference over combinatorial spaces”. Neural

Information Processing Systems (NIPS). 2010.

[6] M. J. Wainwright  T. S. Jaakkola  and A. S. Willsky. “A new class of upper bounds on the log
partition function”. IEEE Transactions on Information Theory 51.7 (2005)  pp. 2313–2335.
[7] M. J. Wainwright and M. I. Jordan. “Log-determinant relaxation for approximate inference
in discrete Markov random ﬁelds”. IEEE Transactions on Signal Processing 54.6 (2006) 
pp. 2099–2109.

[8] T. Hazan and A. Shashua. “Norm-product belief propagation: Primal-dual message-passing for
approximate inference”. IEEE Transactions on Information Theory 56.12 (2010)  pp. 6294–
6316.

[9] G. Papandreou and A. L. Yuille. “Perturb-and-MAP Random Fields: Using discrete opti-
mization to learn and sample from energy models”. Computer Vision (ICCV)  2011 IEEE
International Conference on. IEEE. 2011  pp. 193–200.

[10] T. Hazan and T. Jaakkola. “On the partition function and random maximum a-posteriori

perturbations”. arXiv preprint arXiv:1206.6410 (2012).

[11] M. Jerrum. “Probabilistic Methods for Algorithmic Discrete Mathematics”. 1998. Chap. Math-

ematical Foundations of the Markov Chain Monte Carlo Method  pp. 116–165.

[12] C. Li  S. Sra  and S. Jegelka. “Fast mixing Markov chains for strongly Rayleigh measures 
DPPs  and constrained sampling”. Advances in Neural Information Processing Systems. 2016 
pp. 4188–4196.

[13] A. Gotovos  S. H. Hassani  and A. Krause. “Sampling from Probabilistic Submodular Models”.

Neural Information Processing Systems (NIPS). Dec. 2015.

[14] P. Rebeschini and A. Karbasi. “Fast Mixing for Discrete Point Processes”. 28th Conference on

[15]

[16]

Learning Theory (COLT). 2015.
J. Djolonga and A. Krause. “From MAP to Marginals: Variational Inference in Bayesian
Submodular Models”. Neural Information Processing Systems (NIPS). 2014.
J. Djolonga and A. Krause. “Scalable Variational Inference in Log-supermodular Models”.
International Conference on Machine Learning (ICML). 2015.

9

[17]

[18]

J. Djolonga  S. Tschiatschek  and A. Krause. “Variational Inference in Mixed Probabilistic
Submodular Models”. Neural Information Processing Systems (NIPS). 2016.
J. Zhang  J. Djolonga  and A. Krause. “Higher-Order Inference for Multi-class Log-
supermodular Models”. International Conference on Computer Vision (ICCV). 2015.

[19] D. Tarlow  K. Swersky  R. S. Zemel  R. P. Adams  and B. J. Frey. “Fast exact inference for

recursive cardinality models”. arXiv preprint arXiv:1210.4899 (2012).

[20] R. Lyons. “Determinantal probability measures”. Publications Mathématiques de l’IHÉS 98

(2003)  pp. 167–212.

[21] R. Burton and R. Pemantle. “Local characteristics  entropy and limit theorems for spanning
trees and domino tilings via transfer-impedances”. The Annals of Probability (1993)  pp. 1329–
1371.

[22] A. Kulesza and B. Taskar. “Determinantal Point Processes for Machine Learning”. Foundations

and Trends in Machine Learning 5.2–3 (2012).

[23] A. Risteski. “How to calculate partition functions using convex programming hierarchies:

provable bounds for variational methods”. Conference on Learning Theory. 2016.

[24] S. Fujishige. Submodular functions and optimization. Annals of Discrete Mathematics vol. 58.

[25]

2005.
J. Edmonds. “Matroids and the greedy algorithm”. Mathematical programming 1.1 (1971) 
pp. 127–136.

[26] G. L. Nemhauser  L. A. Wolsey  and M. L. Fisher. “An analysis of approximations for
maximizing submodular set functions—I”. Mathematical Programming 14.1 (1978)  pp. 265–
294.

[27] K. Murota. Discrete convex analysis. SIAM  2003.
[28] R. P. Leme. “Gross substitutability: An algorithmic survey”. Games and Economic Behavior

106 (2017)  pp. 294–316.

[29] A. W. Dress and W. Terhalle. “Well-layered maps—A class of greedily optimizable set

functions”. Applied Mathematics Letters 8.5 (1995)  pp. 77–80.

[30] S. Fujishige and Z. Yang. “A note on Kelso and Crawford’s gross substitutes condition”.

Mathematics of Operations Research 28.3 (2003)  pp. 463–469.

[31] K. Murota et al. “Discrete convex analysis: A tool for economics and game theory”. Journal of

Mechanism and Institution Design 1.1 (2016)  pp. 151–273.
J. G. Oxley. Matroid theory. Vol. 3. Oxford University Press  USA  2006.

[32]
[33] G. Calinescu  C. Chekuri  M. Pál  and J. Vondrák. “Maximizing a monotone submodular
function subject to a matroid constraint”. SIAM Journal on Computing 40.6 (2011)  pp. 1740–
1766.

[34] A. Shioura. “On the pipage rounding algorithm for submodular function maximization—a
view from discrete convex analysis”. Discrete Mathematics  Algorithms and Applications 1.01
(2009)  pp. 1–23.

[35] S. Tschiatschek  J. Djolonga  and A. Krause. “Learning Probabilistic Submodular Diversity
Models Via Noise Contrastive Estimation.” International Conference on Artiﬁcial Intelligence
and Statistics (AISTATS). 2016.

[37]

[36] S. B. Maurer. “Matrix generalizations of some theorems on trees  cycles and cocycles in

graphs”. SIAM Journal on Applied Mathematics 30.1 (1976)  pp. 143–148.
J. Borcea  P. Brändén  and T. Liggett. “Negative dependence and the geometry of polynomials”.
Journal of the American Mathematical Society 22.2 (2009)  pp. 521–567.
inference”. Foundations and Trends R(cid:13) in Machine Learning 1.1-2 (2008)  pp. 1–305.

[38] M. J. Wainwright and M. I. Jordan. “Graphical models  exponential families  and variational

[39] A. Rényi. “On measures of entropy and information”. Fourth Berkeley symposium on mathe-

matical statistics and probability. Vol. 1. 1961  pp. 547–561.

[40] T. Van Erven and P. Harremoës. “Rényi Divergence and Kullback-Leibler Divergence”. arXiv

preprint arXiv:1206.2459 (2012).

[41] T. Minka. Divergence measures and message passing. Tech. rep. Microsoft Research  2005.
[42] D. P. Bertsekas. Nonlinear programming. Athena Scientiﬁc Belmont  1999.

10

[43] N. Komodakis  N. Paragios  and G. Tziritas. “MRF energy minimization and beyond via dual
decomposition”. IEEE Transactions on Pattern Analysis and Machine Intelligence 33.3 (2011) 
pp. 531–552.

[44] M. Karimi  M. Lucic  H. Hassani  and A. Krause. “Stochastic submodular maximization: The
case of coverage functions”. Neural Information Processing Systems (NIPS). 2017  pp. 6856–
6866.

[45] C. Chekuri  J. Vondrak  and R. Zenklusen. “Dependent randomized rounding via exchange
properties of combinatorial structures”. Foundations of Computer Science (FOCS)  2010 51st
Annual IEEE Symposium on. IEEE. 2010  pp. 575–584.
J. Vondrák. “Submodularity in combinatorial optimization”. PhD thesis. Univerzita Karlova 
Matematicko-fyzikální fakulta  2007.

[46]

[47] S. Agrawal  Y. Ding  A. Saberi  and Y. Ye. “Correlation robust stochastic optimization”.
Proceedings of the twenty-ﬁrst annual ACM-SIAM Symposium on Discrete Algorithms. Society
for Industrial and Applied Mathematics. 2010  pp. 1087–1096.

[48] K. Swersky  I. Sutskever  D. Tarlow  R. S. Zemel  R. R. Salakhutdinov  and R. P. Adams.
“Cardinality restricted boltzmann machines”. Neural Information Processing Systems (NIPS).
2012  pp. 3293–3301.

[49] R. Gomes and A. Krause. “Budgeted Nonparametric Learning from Data Streams.” Interna-

tional Conference on Machine Learning (ICML). 2010  pp. 391–398.

[50] A. Krizhevsky and G. Hinton. “Learning multiple layers of features from tiny images” (2009).
J. Leskovec  A. Krause  C. Guestrin  C. Faloutsos  J. VanBriesen  and N. Glance. “Cost-
[51]
effective outbreak detection in networks”. Proceedings of the 13th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM. 2007  pp. 420–429.

11

,Nathaniel Smith
Noah Goodman
Michael Frank
Josip Djolonga
Stefanie Jegelka
Andreas Krause