2019,Making the Cut: A Bandit-based Approach to Tiered Interviewing,Given a huge set of applicants  how should a firm allocate sequential resume screenings  phone interviews  and in-person site visits?  In a tiered interview process  later stages (e.g.  in-person visits) are more informative  but also more expensive than earlier stages (e.g.  resume screenings).  Using accepted hiring models and the concept of structured interviews  a best practice in human resources  we cast tiered hiring as a combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting. The goal is to select a subset of arms (in our case  applicants) with some combinatorial structure.  We present new algorithms in both the probably approximately correct (PAC) and fixed-budget settings that select a near-optimal cohort with provable guarantees.  We show via simulations on real data from one of the largest US-based computer science graduate programs that our algorithms make better hiring decisions or use less budget than the status quo.,A Bandit-based Approach to Tiered Interviewing

Making the Cut:

Candice Schumann?

Zhi Lang?

Jeffrey S. Foster†

John P. Dickerson?

{schumann zlang}@cs.umd.edu  jfoster@cs.tufts.edu  john@cs.umd.edu

?University of Maryland

†Tufts University

Abstract

Given a huge set of applicants  how should a ﬁrm allocate sequential resume
screenings  phone interviews  and in-person site visits? In a tiered interview process 
later stages (e.g.  in-person visits) are more informative  but also more expensive
than earlier stages (e.g.  resume screenings). Using accepted hiring models and
the concept of structured interviews  a best practice in human resources  we cast
tiered hiring as a combinatorial pure exploration (CPE) problem in the stochastic
multi-armed bandit setting. The goal is to select a subset of arms (in our case 
applicants) with some combinatorial structure. We present new algorithms in both
the probably approximately correct (PAC) and ﬁxed-budget settings that select a
near-optimal cohort with provable guarantees. We show via simulations on real
data from one of the largest US-based computer science graduate programs that
our algorithms make better hiring decisions or use less budget than the status quo.

‘... nothing we do is more important than hiring and developing people. At the end of the
day  you bet on people  not on strategies.” – Lawrence Bossidy  The CEO as Coach (1995)

1

Introduction

Hiring workers is expensive and lengthy. The average cost-per-hire in the United States is $4 129 [So-
ciety for Human Resource Management  2016]  and with over ﬁve million hires per month on average 
total annual hiring cost in the United States tops hundreds of billions of dollars [United States Bureau
of Labor Statistics  2018]. In the past decade  the average length of the hiring process has doubled to
nearly one month [Chamberlain  2017]. At every stage  ﬁrms expend resources to learn more about
each applicant’s true quality  and choose to either cut that applicant or continue interviewing with the
intention of offering employment.
In this paper  we address the problem of a ﬁrm hiring a cohort of multiple workers  each with
unknown true utility  over multiple stages of structured interviews. We operate under the assumption
that a ﬁrm is willing to spend an increasing amount of resources—e.g.  money or time—on applicants
as they advance to later stages of interviews. Thus  the ﬁrm is motivated to aggressively “pare down”
the applicant pool at every stage  culling low-quality workers so that resources are better spent in
more costly later stages. This concept of tiered hiring can be extended to crowdsourcing or ﬁnding a
cohort of trusted workers. At each successive stage  crowdsourced workers are given harder tasks.
Using techniques from the multi-armed bandit (MAB) and submodular optimization literature  we
present two new algorithms—in the probably approximately correct (PAC) (§3) and ﬁxed-budget
settings (§4)—and prove upper bounds that select a near-optimal cohort in this restricted setting. We
explore those bounds in simulation and show that the restricted setting is not necessary in practice
(§5). Then  using real data from admissions to a large US-based computer science Ph.D. program 
we show that our algorithms yield better hiring decisions at equivalent cost to the status quo—or
comparable hiring decisions at lower cost (§5).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

2 A Formal Model of Tiered Interviewing

In this section  we provide a brief overview of related work  give necessary background for our
model  and then formally deﬁne our general multi-stage combinatorial MAB problem. Each of our n
applicants is an arm a in the full set of arms A. Our goal is to select K < n arms that maximize some
objective w using a maximization oracle. We split up the review/interview process into m stages 
such that each stage i 2 [m] has per-interview information gain si  cost ji  and number of required
arms Ki (representing the size of the “short list” of applicants who proceed to the next round). We
want to solve this problem using either a conﬁdence constraint (  ✏)  or a budget constraint over each
stage (Ti). We rigorously deﬁne each of these inputs below.

Multi-armed bandits. The multi-armed bandit problem allows for modeling resource allocation
during sequential decision making. Bubeck et al. [2012] provide a general overview of historic
research in this ﬁeld. In a MAB setting there is a set of n arms A. Each arm a 2 A has a true
utility u(a) 2 [0  1]  which is unknown. When an arm a 2 A is pulled  a reward is pulled from
a distribution with mean u(a) and a -sub-Gaussian tail. These pulls give an empirical estimate
ˆu(a) of the underlying utility  and an uncertainty bound rad(a) around the empirical estimate  i.e. 
ˆu(a)  rad(a) < u(a) < ˆu(a) + rad(a) with some probability . Once arm a is pulled (e.g  an
application is reviewed or an interview is performed)  ˆu(a) and rad(a) are updated.

Top-K and subsets. Traditionally  MAB problems focus on selecting a single best (i.e.  highest
utility) arm. Recently  MAB formulations have been proposed that select an optimal subset of K arms.
Bubeck et al. [2013] propose a budgeted algorithm (SAR) that successively accepts and rejects arms.
We build on work by Chen et al. [2014]  which generalizes SAR to a setting with a combinatorial
objective. They also outline a ﬁxed-conﬁdence version of the combinatorial MAB problem. In the
Chen et al. [2014] formulation  the overall goal is to choose an optimal cohort M⇤ from a decision

class M. In this work  we use decision class MK(A) = {M ✓ A |M| = K}. A cohort is optimal

if it maximizes a linear objective function w : Rn ⇥M K(A) ! R. Chen et al. [2014] rely on a
maximization oracle  as do we  deﬁned as
(1)

Oracle K(ˆu  A) = argmax
M2MK (A)

w(ˆu  M ).

Chen et al. [2014] deﬁne a gap score for each arm a in the optimal cohort M⇤  which is the difference
in combinatorial utility between M⇤ and the best cohort without arm a. For each arm a not in the
optimal set M⇤  the gap score is the difference in combinatorial utility between M⇤ and the best set
with arm a. Formally  for any arm a 2 A  the gap score a is deﬁned as

a =(w(M⇤)  max{M | M2MK^a2M} w(M ) 
w(M⇤)  max{M | M2MK^a /2M} w(M ) 

if a /2 M⇤
if a 2 M⇤.

Using this gap score we estimate the hardness of a problem as the sum of inverse squared gaps:

(2)

(3)

This helps determine how easy it is to differentiate between arms at the border of accept/reject.

Objectives. Cao et al. [2015] tighten the bounds of Chen et al. [2014] where the objective function
is Top-K  deﬁned as

u(a).

(4)

In this setting the objective is to pick the K arms with the highest utility. Jun et al. [2016] look at the
Top-K MAB problem with batch arm pulls  and Singla et al. [2015a] look at the Top-K problem from
a crowdsourcing point of view.
In this paper  we explore a different type of objective that balances both individual utility and the
diversity of the set of arms returned. Research has shown that a more diverse workforce produces
better products and increases productivity [Desrochers  2001; Hunt et al.  2015]. Thus  such an
objective is of interest to our application of hiring workers. In the document summarization setting 
Lin and Bilmes [2011] introduced a submodular diversity function where the arms are partitioned
into q disjoint groups P1 ···   Pq:

wDIV(u  M ) =

u(a).

(5)

H =Xa2A

2
a

wTOP(u  M ) = Xa2M

qXi=0s Xa2Pi\M

2

Nemhauser et al. [1978] prove theoretical bounds for the simple greedy algorithm that selects a set
that maximizes a submodular  monotone function. Krause and Golovin [2014] overview submodular
optimization in general. Singla et al. [2015b] propose an algorithm for maximizing an unknown
function  and Ashkan et al. [2015] introduce a greedy algorithm that optimally solves the problem of
diversiﬁcation if that diversity function is submodular and monotone. Radlinski et al. [2008] learn a
diverse ranking from behavior patterns of different users by using multiple MAB instances. Yue and
Guestrin [2011] introduce the linear submodular bandits problem to select diverse sets of content
while optimizing for a class of feature-rich submodular utility models. Each of these papers uses
submodularity to promote some notion of diversity. Using this as motivation  we empirically show
that we can hire a diverse cohort of workers (Section 5).

Variable costs.
In many real-world settings  there are different ways to gather information  each
of which vary in cost and effectiveness. Ding et al. [2013] looked at a regret minimization MAB
problem in which  when an arm is pulled  a random reward is received and a random cost is taken
from the budget. Xia et al. [2016] extend this work to a batch arm pull setting. Jain et al. [2014] use
MABs with variable rewards and costs to solve a crowdsourcing problem. While we also assume
non-unit costs and rewards  our setting is different than each of these  in that we actively choose how
much to spend on each arm pull.
Interviews allow ﬁrms to compare applicants. Structured interviews treat each applicant the same
by following the same questions and scoring strategy  allowing for meaningful cross-applicant
comparison. A substantial body of research shows that structured interviews serve as better predictors
of job success and reduce bias across applicants when compared to traditional methods [Harris  1989;
Posthuma et al.  2002]. As decision-making becomes more data-driven  ﬁrms look to demonstrate
a link between hiring criteria and applicant success—and increasingly adopt structured interview
processes [Kent and McCarthy  2016; Levashina et al.  2014].
Motivated by the structured interview paradigm  Schumann et al. [2019] introduced a concept of
“weak” and “strong” pulls in the Strong Weak Arm Pull (SWAP) algorithm. SWAP probabilistically
chooses to strongly or weakly pull an arm. Inspired by that work we associate pulls with a cost
j  1 and information gain s  j  where a pull receives a reward pulled from a distribution with a
ps-sub-Gaussian tail  but incurs cost j. Information gain s relates to the conﬁdence of accept/reject
from an interview vs review. As stages get more expensive  the estimates of utility become more
precise - the estimate comes with a distribution with a lower variance. In practice  a resume review
may make a candidate seem much stronger than they are  or a badly written resume could severely
underestimate their abilities. However  in-person interviews give better estimates. A strong arm
pull with information gain s is equivalent to s weak pulls because it is equivalent to pulling from a
distribution with a /ps sub-Gaussian tail - it is equivalent to getting a (probably) closer estimate.
Schumann et al. [2019] only allow for two types of arm pulls and they do not account for the structure
of current tiered hiring frameworks; nevertheless  in Section 5  we extend (as best we can) their model
to our setting and compare as part of our experimental testbed.

Generalizing to multiple stages. This paper  to our knowledge  gives the ﬁrst computational
formalization of tiered structured interviewing. We build on hiring models from the behavioral
science literature [Vardarlier et al.  2014; Breaugh and Starke  2000] in which the hiring process
starts at recruitment and follows several stages  concluding with successful hiring. We model these m
successive stages as having an increased cost—in-person interviews cost more than phone interviews 
which in turn cost more than simple résumé screenings—but return additional information via the
score given to an applicant. For each stage i 2 [m] the user deﬁnes a cost ji and an information gain si
for the type of pull (type of interview) being used in that stage. During each stage  Ki arms move on
to the next stage (we cut off Ki1  Ki arms)  where n = K0 > K1 > ··· > Km1 > Km = K).
The user must therefore deﬁne Ki for each i 2 [m  1]. The arms chosen to move on to the next
stage are denoted as Am ⇢ Am1 ⇢···⇢ A1 ⇢ A0 = A.
Tiered MAB and interviewing stages. Our formulation was initially motivated by the graduate
admissions system run at our university. Here  at every stage  it is possible for multiple independent
reviewers to look at an applicant. Indeed  our admissions committee strives to hit at least two written
reviews per application package  before potentially considering one or more Skype/Hangouts calls
with a potential applicant. (In our data  for instance  some applicants received up to 6 independent
reviews per stage.)

3

While motivated by academic admissions  we believe our model is of broad interest to industry as
well. For example  in the tech industry  it is common to allocate more (or fewer) 30-minute one-on-
one interviews on a visit day  and/or multiple pre-visit programming screening teleconference calls.
Similarly  in management consulting Hunt et al. [2015]  it is common to repeatedly give independent
“case study” interviews to borderline candidates.

3 Probably Approximately Correct Hiring

In this section  we present Cutting Arms using a Combinatorial Oracle (CACO)  the ﬁrst of two
multi-stage algorithms for selecting a cohort of arms with provable guarantees. CACO is a probably
approximately correct (PAC) [Haussler and Warmuth  1993] algorithm that performs interviews over
m stages  for a user-supplied parameter m  before returning a ﬁnal subset of K arms.
Algorithm 1 provides pseudocode for CACO. The algorithm requires several user-supplied parameters
in addition to the standard PAC-style conﬁdence parameters ( - conﬁdence probability  ✏ - error) 
including the total number of stages m; pairs (si  ji) for each stage i 2 [m] representing the
information gain si and cost ji associated with each arm pull; the number Ki of arms to remain at
the end of each stage i 2 [m]; and a maximization oracle. After each stage i is complete  CACO
removes all but Ki arms. The algorithm tracks these “active” arms  denoted by Ai1 for each stage i 
the total cost Cost that accumulates over time when pulling arms  and per-arm a information such as
empirical utility ˆu(a) and total information gain T (a). For example  if arm a has been pulled once in
stage 1 and twice in stage 2  then T (a) = s1 + 2s2.

T (a)

number of stages m; (si  ji  Ki) for each stage i

CACO begins with all arms active
(line 1). Each stage i starts by pulling
Algorithm 1 Cutting Arms using a Combinatorial Oracle (CACO)
each active arm once using the given
Require: Conﬁdence  2 (0  1); Error ✏ 2 (0  1); Oracle;
(si  ji) pair to initialize or update em-
pirical utilities (line 3). It then pulls
1: A0 A
arms until a conﬁdence level is trig-
2: for stage i = 1  . . .   m do
gered  removes all but Ki arms  and
Pull each a 2 Ai1 once using the given si  ji pair
3:
continues to the next stage (line 13).
Update empirical means ˆu
4:
In a stage i  CACO proceeds in
5:
Cost Cost +Ki1 · ji
rounds indexed by t. In each round 
for t = 1  2  . . . do
6:
the algorithm ﬁrst ﬁnds a set Ai of
7:
Ai Oracle Ki(ˆu)
size Ki using the maximization ora-
for a 2 Ai1 do
8:
rad(a) q 2 log(4|A| Cost 3)/
cle and the current empirical means ˆu
9:
(line 7). Then  given a conﬁdence ra-
if a 2 Ai then ˜u(a) ˆu(a)  rad t(a)
10:
dius (line 9)  it computes pessimistic
else ˜u(a) ˆu(a) + rad(a)
11:
estimates ˜u(a) of the true utilities of
each arm a and uses the oracle to
˜Ai Oracle Ki(˜u)
12:
ﬁnd a set of arms ˜Ai under these
if |w( ˜Ai)  w(Ai)| <✏ then break
13:
pessimistic assumptions (lines 10-12).
14:
p arg maxa2( ˜Ai\Ai)[(Ai\ ˜Ai) rad(a)
If those two sets are “close enough”
Pull arm p using the given si  ji pair
15:
(✏ away)  CACO proceeds to the
Update ˆu(p) with the observed reward
16:
next stage (line 13). Otherwise  across
17:
T (p) T (p) + si
all arms a in the symmetric differ-
18:
Cost Cost + ji
ence between Ai and ˜Ai  the arm
19: Out Am; return Out
p with the most uncertainty over its
true utility—determined via rad(a)—
is pulled (line 14). At the end of the last stage m  CACO returns a ﬁnal set of K active arms that
approximately maximizes an objective function (line 19).
We prove a bound on CACO in Theorem 1. As a special case of this theorem  when only a single
stage of interviewing is desired  and as ✏ ! 0  then Algorithm 1 reduces to Chen et al. [2014]’s
CLUCB  and our bound then reduces to their upper bound for CLUCB. This bound provides insights
into the trade-offs of Cost  information gain s  problem hardness H (Equation 3)  and shortlist size
Ki. Given the Cost and information gain s parameters Theorem 1 provides a tighter bound than
those for CLUCB.

4

i

K 2
i

K 2
i

i t



 

2
a

Theorem 1. Given any  2 (0  1)  any ✏ 2 (0  1)  any decision classes Mi ✓ 2[n] for each stage
i 2 [m]  any linear function w  and any expected rewards u 2 Rn  assume that the reward distribution
'a for each arm a 2 [n] has mean u(a) with a -sub-Gaussian tail. Let M⇤i = argmaxM2Mi denote
the optimal set in stage i 2 [m]. Set radt(a) = q2 log(
)/Ti t(a) for all t > 0 and
a 2 [n]. Then  with probability at least 1    the CACO algorithm (Algorithm 1) returns the set Out
where w(Out)  w(M⇤m) <✏ and
✏2 1A log0@ 2j4
T O0@2 Xi2[m]

0@ ji
si0@ Xa2Ai1

✏2 1A1A1A .

si Xa2Ai1

min⇢ 1

min⇢ 1

4Ki1 Cost 3

T

Theorem 1 gives a bound relative to problem-speciﬁc pa-
rameters such as the gap scores a (Equation 2)  inter-stage
cohort sizes Ki  and so on. Figure 11 lends intuition as to
how CACO changes with respect to these inputs  in terms of
problem hardness (deﬁned in Eq. 3). When a problem is easy
(gap scores a are large and hardness H becomes small) 
the min parts of the bound are dominated by gap scores
a  and there is a smooth increase in total cost. When the
problem gets harder (gap scores a are small and hardness
H becomes large)  the mins are dominated by K2
i /✏2 and
the cost is noisy but bounded below. When ✏ or  increases 
the lower bounds of the noisy section decrease—with the
impact of ✏ dominating that of . A policymaker can use these high-level trade-offs to determine
hiring mechanism parameters. For example  assume there are two interview stages. As the number
K1 of applicants who pass the ﬁrst interview stage increases  so too does total cost T . However  if
K1 is too small (here  very close to the ﬁnal cohort size K)  then the cost also increases.

Figure 1: Hardness (H) vs theoretical cost
(T ) as user-speciﬁed parameters to the
CACO algorithm change.

Bounded 

H

105

103

 

2
a

4 Hiring on a Fixed Budget with BRUTAS

i 2 [m]  wherePi2[m]

In many hiring situations  a ﬁrm or committee has a ﬁxed budget for hiring (number of phone
interviews  total dollars to spend on hosting  and so on). With that in mind  in this section  we present
Budgeted Rounds Updated Targets Successively (BRUTAS)  a tiered-interviewing algorithm in the
ﬁxed-budget setting.
Algorithm 2 provides pseudocode for BRUTAS  which takes as input ﬁxed budgets ¯Ti for each stage
¯Ti = ¯T   the total budget. In this version of the tiered-interview problem  we
also know how many decisions—whether to accept or reject an arm—we need to make in each stage.
This is slightly different than in the CACO setting (§3)  where we need to remove all but Ki arms at
the conclusion of each stage i. We make this change to align with the CSAR setting of Chen et al.
[2014]  which BRUTAS generalizes. In this setting  let ˜Ki represent how many decisions we need
to make at stage i 2 [m]; thus Pi2[m]
˜Ki = n. The ˜Kis are independent of K  the ﬁnal number of
arms we want to accept  except that the total number of accept decisions across all ˜K must sum to K.
The budgeted setting uses a constrained oracle COracle : Rn ⇥ 2[n] ⇥ 2[n] ! M [ {?} deﬁned as

COracle(ˆu  A  B) =

argmax

w(ˆu  M ) 

{M2MK | A✓M ^ B\M =;}

where A is the set of arms that have been accepted and B is the set of arms that have been rejected.
In each stage i 2 [m]  BRUTAS starts by collecting the accept and reject sets from the previous stage.
It then proceeds through ˜Ki rounds  indexed by t  and selects a single arm to place in the accept
set A or the reject set B. In a round t  it ﬁrst pulls each active arm—arms not in A or B—a total of
˜Ti t  ˜Ti t1 times using the appropriate si and ji values. ˜Ti t is set according to Line 6; note that
˜Ti 0 = 0. Once all the empirical means for each active arm have been updated  the constrained oracle
is run to ﬁnd the empirical best set Mi t (Line 9). For each active arm a  a new pessimistic set ˜Mi t a
is found (Lines 11-15). a is placed in the accept set A if a is not in Mi t  or in the reject set B if a

1For detailed ﬁgures see Appendix E

5

is in Mi t. This is done to calculate the gap that arm a creates (Equation 2). The arm pi t with the
largest gap is selected and placed in the accept set A if pi t was included in Mi t  or placed in the
reject set B otherwise (Lines 16-20). Once all rounds are complete  the ﬁnal accept set A is returned.
Theorem 2  provides an lower bound on the conﬁdence that BRUTAS returns the optimal set. Note
that if there is only a single stage  then Algorithm 2 reduces to Chen et al. [2014]’s CSAR algorithm 
and our Theorem 2 reduces to their upper bound for CSAR. Again Theorem 2 provides tighter bounds
than those for CSAR given the parameters for information gain sb and arm pull cost jb.

Theorem 2. Given any ¯Tis such thatPi2[m]
¯Ti = ¯T > n  any decision class MK ✓ 2[n]  any
linear function w  and any true expected rewards u 2 Rn  assume that reward distribution 'a for
each arm a 2 [n] has mean u(a) with a -sub-Gaussian tail. Let (1)  . . .   (n) be a permutation
of 1  . . .   n (deﬁned in Eq. 2) such that (1)  . . .  (n). Deﬁne ˜H   maxi2[n] i2
(i) . Then 
Algorithm 2 uses at most ¯Ti samples per stage i 2 [m] and outputs a solution Out 2M K [ {?}
such that

Pr[Out 6= M⇤]  n2 exp Pm

b=1 sb( ¯Tb  ˜Kb)/(jbflog( ˜Kb))

722 ˜H

!

(6)

a=0 eKi)ji(eKit+1)⇡
¯Ti(nPi1
a=0 eKi)

flog(nPi1

where flog(n)  Pn

i=1 i1  and M⇤ = argmaxM2MK w(M ).
Algorithm 2 Budgeted Rounds Updated Targets Successively
(BRUTAS)
Require: Budgets ¯Ti 8i 2 [m]; (si  ji  ˜Ki) for each stage i; con-

strained oracle COracle
1
i=1
i
2: A0 1 ?; B0 1 ?
3: for stage i = 1  . . .   m do
4:
5:

1: Deﬁne flog(n)  Pn
Ai 1 Ai1  ˜Ki1+1; Bi 1 Bi1  ˜Ki1+1; ˜Ti 0 0
for t = 1  . . .   ˜Ki do
˜Ti t ⇠
foreach a 2 [n] \ (Ai t [ Bi t) do
Pull a ( ˜Ti t  ˜Ti t1) times; update ˆui t(a)
Mi t COracle(ˆui t  Ai t  Bi t)
if Mi t =? then return ?
foreach a 2 [n] \ (Ai t [ Bi t) do

When setting the budget for each
stage  a policymaker should en-
sure there is sufﬁcient budget
for the number of arms in each
stage i  and for the given ex-
ogenous cost values ji associ-
ated with interviewing at that
stage. There is also a balance
between the number of deci-
sions that must be made in a
given stage i and the ratio si
of
ji
interview information gain and
cost. Intuitively  giving higher
budget to stages with a higher
ratio makes sense—but one
si
ji
also would not want to make all
accept/reject decisions in those
stages  since more decisions cor-
responds to lower conﬁdence.
Generally  arms with high gap
scores a are accepted/rejected
in the earlier stages  while arms
with low gap scores a are ac-
cepted/rejected in the later stages.
17:
The policy maker should look
18:
at past decisions to estimate gap
19:
scores a (Equation 2) and hard-
20:
ness H (Equation 3). There is a
21: Out Am  ˜Km+1; return Out
clear trade-off between informa-
tion gain and cost. If the policy
maker assumes (based on past data) that the gap scores will be high (it is easy to differentiate between
applicants) then the lower stages should have a high Ki  and a budget to match the relevant cost ji. If
the gap scores are all low (it is hard to differentiate between applicants) then more decisions should
be made in the higher  more expensive stages. By looking at the ratio of small gap scores to high gap
scores  or by bucketing gap scores  a policy maker will be able to set each Ki.

˜Mi t a COracle( ˆwi t  Ai t  Bi t[{a})
˜Mi t a COracle( ˆwi t  Ai t[{a}  Bi t)
a2[n]\(Ai t[Bi t)

Ai t+1 Ai t [{ pi t}; Bi t+1 Bi t
Ai t+1 Ai t; Bi t+1 Bi t [{ pi t}

6:

7:
8:
9:
10:
11:
12:
13:
14:
15:
16:

if a 2 Mi t then
else

pi t argmax
if pi t 2 Mt then
else

w(Mi t)  w( ˜Mi t a)

6

=0.05

=0.075

=0.1

=0.2

=0.1

t
s
o
C

t
s
o
C

t
s
o
C

t
s
o
C

t
s
o
C

K1 =10
K1 =13
K1 =18
K1 =29

10

15
s

20

10

15
s

20

10

15
s

20

10

10

20

20

15
s

15
s

Figure 2: Comparison of Cost vs information gain (s) as ✏ increases for CACO. Here   = 0.05 and  = 0.2.
As ✏ increases  the cost of the algorithm also decreases. If the overall cost of the algorithm is low  then increasing
s (while keeping j constant) provides diminishing returns.

5 Experiments

In this section  we experimentally evaluate BRUTAS and CACO in two different settings. The ﬁrst
setting uses data from a toy problem of Gaussian distributed arms. The second setting uses real
admissions data from one of the largest US-based graduate computer science programs.

5.1 Gaussian Arm Experiments

We begin by using simulated data to test the tightness of our theoret-
ical bounds. To do so  we instantiate a cohort of n = 50 arms whose
true utilities  ua  are sampled from a normal distribution. We aim to
select a ﬁnal cohort of size K = 7. When an arm is pulled during
a stage with cost j and information gain s  the algorithm is charged
a cost of j and a reward is pulled from a distribution with mean ua
and standard deviation of /ps. For simplicity  we present results
in the setting of m = 2 stages.

Figure 3: Hardness (H) vs Cost 
comparing against Theorem 1.

CACO. To evaluate CACO  we vary   ✏    K1  and s2. We ﬁnd that as  increases  both cost
and utility decrease  as expected. Similarly  Figure 2 shows that as ✏ increases  both cost and utility
decrease. Higher values of  increase the total cost  but do not affect utility. We also ﬁnd diminishing
returns from high information gain s values (x-axis of Figure 2). This makes sense—as s tends to
inﬁnity  the true utility is returned from a single arm pull. We also notice that if many “easy” arms
(arms with very large gap scores) are allowed in higher stages  total cost rises substantially.
Although the bound deﬁned in Theorem 1 assumes a linear function w  we empirically tested CACO
using a submodular function wDIV. We ﬁnd that the cost of running CACO using this submodular
function is signiﬁcantly lower than the theoretical bound. This suggests that (i) the bound for CACO
can be tightened and (ii) CACO could be run with submodular functions w.

BRUTAS. To evaluate BRUTAS  we varied  and ( ˜Ki  Ti) pairs
for two stages. Utility varies as expected from Theorem 2: when 
increases  utility decreases. There is also a trade-off between ˜Ki and
Ti values. If the problem is easy  a low budget and a high ˜K1 value
is sufﬁcient to get high utility. If the problem is hard (high H value) 
a higher overall budget is needed  with more budget spent in the
second stage. Figure 4 shows this escalating relationship between
budget and utility based on problem hardness. Again we found that
BRUTAS performed well when using a submodular function wDIV.
Finally  we compare CACO and BRUTAS to two baseline algo-
rithms: UNIFORM and RANDOM  which uniformly and randomly
respectively  pulls arms in each stage. In both algorithms  the maximization oracle is run after each
stage to determine which arms should move on to the next stage. When given a budget of 2 750 
BRUTAS achieves a utility of 244.0  which outperforms both the UNIFORM and RANDOM baseline
utilities of 178.4 and 138.9  respectively. When CACO is run on the same problem  it ﬁnds a solution
(utility of 231.0) that beats both UNIFORM and RANDOM at a roughly equivalent cost of 2 609. This
qualitative behavior exists for other budgets.

Figure 4: Effect of an increas-
ing budget on the overall utility
of a cohort. As hardness (H) in-
creases  more budget is needed to
produce a high quality cohort.

7

103105H100102104106CostTheorySimulations800010000BudgetwH=2.7⇥1010H=8.0⇥103Figure 5: Utility vs Cost over four different algorithms (RANDOM  UNIFORM  SWAP  CACO  BRUTAS) and
the actual admissions decisions made at the university. Both CACO and BRUTAS produce equivalent cohorts
to the actual admissions process with lower cost  or produce high quality cohorts than the actual admissions
process with equivalent cost.

5.2 Graduate Admissions Experiment

We evaluate how CACO and BRUTAS might perform in the real world by applying them to a
graduate admissions dataset from one of the largest US-based graduate computer science programs.
These experiments were approved by the university’s Institutional Review Board and did not affect
any admissions decisions for the university. Our dataset consists of three years (2014–16) worth
of graduate applications. For each application we also have graduate committee review scores
(normalized to between 0 and 1) and admission decisions.

Experimental setup. Using information from 2014 and 2015  we used a random forest classi-
ﬁer [Pedregosa et al.  2011]  trained in the standard way on features extracted from the applications 
to predict probability of acceptance. Features included numerical information such as GPA and
GRE scores  topics from running Latent Dirichlet Allocation (LDA) on faculty recommendation
letters [Schmader et al.  2007]  and categorical information such as region of origin and undergraduate
school. In the testing phase  the classiﬁer was run on the set of applicants A from 2016 to produce a
probability of acceptance P (a) for every applicant a 2 A.
We mimic the university’s application process of two stages: a ﬁrst review stage where admissions
committee members review the application packet  and a second interview stage where committee
members perform a Skype interview for a select subset of applicants. The committee members follow
a structured interview approach. We determined that the time taken for a Skype interview is roughly 6
times as long as a packet review  and therefore we set the cost multiplier for the second stage j2 = 6.
We ran over a variety of s2 values  and we determined  by looking at the distribution of review
scores from past years. When an arm a 2 A is pulled with information gain s and cost j  a reward is
randomly pulled from the arm’s review scores (when s1 = 1 and j1 = 1  as in the ﬁrst stage)  or a
reward is pulled from a Gaussian distribution with mean P (a) and a standard deviation of ps.
We ran simulations for BRUTAS  CACO  UNIFORM  and RANDOM. In addition we compare
to an adjusted version of Schumann et al. [2019]’s SWAP. SWAP uses a strong pull policy to
probabilistically weak or strong pull arms. In this adjusted version we use a strong pull policy of
always weak pulling arms until some threshold time t and strong pulling for the remainder of the
algorithm. Note that this adjustment moves SWAP away from ﬁxed conﬁdence but not all the way to
a budgeted algorithm like BRUTAS but ﬁts into the tiered structure. For the budgeted algorithms
BRUTAS  UNIFORM  and RANDOM  (as well as the pseudo-budgeted SWAP) if there are Ki arms
in round i  the budget is Ki · xi where xi 2 N. We vary  and ✏ to control CACO’s cost.
We compare the utility of the cohort selected by each of the algorithms to the utility from the cohort
that was actually selected by the university. We maximize either objective wTOP or wDIV for each
of the algorithms. We instantiate wDIV  deﬁned in Equation 5  in two ways: ﬁrst  with self-reported
gender  and second  with region of origin. Note that since the graduate admissions process is run
entirely by humans  the committee does not explicitly maximize a particular function. Instead  the
committee tries to ﬁnd a good overall cohort while balancing areas of interest and general diversity.

Results. Figure 5 compares each algorithm to the actual admissions decision process performed by
the real-world committee. In terms of utility  for both wTOP and wDIV  BRUTAS and CACO achieve
similar gains to the actual admissions process (higher for wDIV over region of origin) when using less
cost/budget. When roughly the same amount of budget is used  BRUTAS and CACO are able to

8

500010000Cost406080wTOPTopK500010000Cost101112wDIVDiversityoverGender500010000Cost1820wDIVDiversityoverRegionprovide higher predicted utility than the true accepted cohort  for both wTOP and wDIV. As expected 
BRUTAS and CACO outperform the baseline algorithms RANDOM  UNIFORM. The adjusted SWAP
algorithm performs poorly in this restricted setting of tiered hiring. By limiting the strong pull policy
of SWAP  only small incremental improvements can be made as Cost is increased.

6 Conclusions & Discussion of Future Research

We provided a formalization of tiered structured interviewing and presented two algorithms  CACO
in the PAC setting and BRUTAS in the ﬁxed-budget setting  which select a near-optimal cohort
of applicants with provable bounds. We used simulations to quantitatively explore the impact of
various parameters on CACO and BRUTAS and found that behavior aligns with theory. We showed
empirically that both CACO and BRUTAS work well with a submodular function that promotes
diversity. Finally  on a real-world dataset from a large US-based Ph.D. program  we showed that
CACO and BRUTAS identify higher quality cohorts using equivalent budgets  or comparable cohorts
using lower budgets  than the status quo admissions process. Moving forward  we plan to incorporate
multi-dimensional feedback (e.g.  with respect to an applicant’s technical  presentation  and analytical
qualities) into our model; recent work due to Katz-Samuels and Scott [2018  2019] introduces that
feedback (in a single-tiered setting) as a marriage of MAB and constrained optimization  and we see
this as a fruitful model to explore combining with our novel tiered system.
Discussion. The results support the use of BRUTAS and CACO in a practical hiring scenario. Once
policymakers have determined an objective  BRUTAS and CACO could help reduce costs and
produce better cohorts of employees. Yet  we note that although this experiment uses real data  it
is still a simulation. The classiﬁer is not a true predictor of utility of an applicant. Indeed  ﬁnding
an estimate of utility for an applicant is a nontrivial task. Additionally  the data that we are using
incorporates human bias in admission decisions  and reviewer scores [Schmader et al.  2007; Angwin
et al.  2016]. Finally  deﬁning an objective function on which to run CACO and BRUTAS is a difﬁcult
task. Recent advances in human value judgment aggregation [Freedman et al.  2018; Noothigattu et
al.  2018] could ﬁnd use in this decision-making framework.

7 Acknowledgements

Schumann and Dickerson were supported by NSF IIS RI CAREER Award #1846237. We thank
Google for gift support  University of Maryland professors David Jacobs and Ramani Duraiswami
for helpful input  and the anonymous reviewers for helpful comments.

References
Julia Angwin  Jeff Larson  Surya Mattu  and Lauren Kirchner. Machine bias. www.propublica.org/

article/machine-bias-risk-assessments-in-criminal-sentencing  2016.

Azin Ashkan  Branislav Kveton  Shlomo Berkovsky  and Zheng Wen. Optimal greedy diversity for

recommendation. In IJCAI  2015.

James A Breaugh and Mary Starke. Research on employee recruitment. Journal of Management 

2000.

Sébastien Bubeck  Nicolo Cesa-Bianchi  et al. Regret analysis of stochastic and nonstochastic

multi-armed bandit problems. Foundations and Trends in Machine Learning  2012.

Séebastian Bubeck  Tengyao Wang  and Nitin Viswanathan. Multiple identiﬁcations in multi-armed

bandits. In ICML  2013.

Wei Cao  Jian Li  Yufei Tao  and Zhize Li. On top-k selection in multi-armed bandits and hidden

bipartite graphs. In NeurIPS  2015.

Andrew Chamberlain. How long does it take to hire? https://www.glassdoor.com/research/

time-to-hire-in-25-countries/  2017.

Shouyuan Chen  Tian Lin  Irwin King  Michael R Lyu  and Wei Chen. Combinatorial pure exploration

of multi-armed bandits. In NeurIPS  2014.

9

Pierre Desrochers. Local diversity  human creativity  and technological innovation. Growth and

Change  2001.

Wenkui Ding  Tao Qin  Xu-Dong Zhang  and Tie-Yan Liu. Multi-armed bandit with budget constraint

and variable costs. In AAAI  2013.

Rachel Freedman  J Schaich Borg  Walter Sinnott-Armstrong  J Dickerson  and Vincent Conitzer.

Adapting a kidney exchange algorithm to align with human values. In AAAI  2018.

Michael M Harris. Reconsidering the employment interview. Personal Psychology  1989.

David Haussler and Manfred Warmuth. The probably approximately correct (PAC) and other learning

models. In Foundations of Knowledge Acquisition. 1993.

Vivian Hunt  Dennis Layton  and Sara Prince. Diversity matters. McKinsey & Company  2015.

Shweta Jain  Sujit Gujar  Satyanath Bhat  Onno Zoeter  and Y Narahari. An incentive compatible

mab crowdsourcing mechanism with quality assurance. arXiv  2014.

Kwang-Sung Jun  Kevin Jamieson  Robert Nowak  and Xiaojin Zhu. Top arm identiﬁcation in

multi-armed bandits with batch arm pulls. In AISTATS  2016.

Julian Katz-Samuels and Clayton Scott. Feasible arm identiﬁcation. In ICML  2018.

Julian Katz-Samuels and Clayton Scott. Top feasible arm identiﬁcation. In AISTATS  2019.

Julia D. Kent and Maureen Terese McCarthy. Holistic Review in Graduate Admissions. Council of

Graduate Schools  2016.

Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability. 2014.

Julia Levashina  Christopher J Hartwell  Frederick P Morgeson  and Michael A Campion. The

structured employment interview. Personnel Psychology  2014.

Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In ACL 

2011.

George L Nemhauser  Laurence A Wolsey  and Marshall L Fisher. An analysis of approximations for

maximizing submodular set functions. Mathematical Programming  1978.

Ritesh Noothigattu  Snehalkumar Neil S. Gaikwad  Edmond Awad  Sohan D’Souza  Iyad Rahwan 
Pradeep Ravikumar  and Ariel D. Procaccia. A voting-based system for ethical decision making.
In AAAI  2018.

F. Pedregosa  G. Varoquaux  A. Gramfort  V. Michel  B. Thirion  O. Grisel  M. Blondel  P. Prettenhofer 
R. Weiss  V. Dubourg  J. Vanderplas  A. Passos  D. Cournapeau  M. Brucher  M. Perrot  and
E. Duchesnay. Scikit-learn. JMLR  2011.

Richard A Posthuma  Frederick P Morgeson  and Michael A Campion. Beyond employment interview

validity. Personal Psychology  2002.

Filip Radlinski  Robert Kleinberg  and Thorsten Joachims. Learning diverse rankings with multi-

armed bandits. In ICML  2008.

Toni Schmader  Jessica Whitehead  and Vicki H. Wysocki. A linguistic comparison of letters of
recommendation for male and female chemistry and biochemistry job applicants. Sex Roles  2007.

Candice Schumann  Samsara N. Counts  Jeffrey S. Foster  and John P. Dickerson. The diverse cohort

selection problem. AAMAS  2019.

Adish Singla  Eric Horvitz  Pushmeet Kohli  and Andreas Krause. Learning to hire teams. In HCOMP 

2015.

Adish Singla  Sebastian Tschiatschek  and Andreas Krause. Noisy submodular maximization via
adaptive sampling with applications to crowdsourced image collection summarization. AAAI  2015.

10

Society for Human Resource Management. Human capital benchmarking report  2016.
United States Bureau of Labor Statistics. Job openings and labor turnover. https://www.bls.gov/news.

release/pdf/jolts.pdf  2018.

Pelin Vardarlier  Yalcin Vural  and Semra Birgun. Modelling of the strategic recruitment process by

axiomatic design principles. In Social and Behavioral Sciences  2014.

Yingce Xia  Tao Qin  Weidong Ma  Nenghai Yu  and Tie-Yan Liu. Budgeted multi-armed bandits

with multiple plays. In IJCAI  2016.

Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversiﬁed

retrieval. In NeurIPS  2011.

11

,Candice Schumann
Zhi Lang
Jeffrey Foster
John Dickerson