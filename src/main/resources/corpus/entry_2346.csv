2012,Gradient Weights help Nonparametric Regressors,In regression problems over $\real^d$  the unknown function $f$ often varies more in some coordinates than in others. We show that weighting each coordinate $i$ with the estimated norm of the $i$th derivative of $f$  is an efficient way to significantly improve the performance of distance-based regressors  e.g. kernel and $k$-NN regressors.  We propose a simple estimator of these derivative norms and prove its consistency. Moreover  the proposed  estimator is efficiently learned online.,Gradient Weights help Nonparametric Regressors

Samory Kpotufe(cid:3)

Max Planck Institute for Intelligent Systems

Abdeslam Boularias

Max Planck Institute for Intelligent Systems

samory@tuebingen.mpg.de

boularias@tuebingen.mpg.de

Abstract

In regression problems over Rd  the unknown function f often varies more in
some coordinates than in others. We show that weighting each coordinate i with
the estimated norm of the ith derivative of f is an efﬁcient way to signiﬁcantly
improve the performance of distance-based regressors  e.g. kernel and k-NN re-
gressors. We propose a simple estimator of these derivative norms and prove its
consistency. Moreover  the proposed estimator is efﬁciently learned online.

1 Introduction

0
i

0
i

0
i

0
i

0
i

>
0
i = e
i

k
1;(cid:22) = EX jf

In regression problems over Rd  the unknown function f might vary more in some coordinates than
in others  even though all coordinates might be relevant. How much f varies with coordinate i can
be captured by the norm kf
rf of f. A simple
i (X)j of the ith derivative f
0
k
way to take advantage of the information in kf
1;(cid:22) is to weight each coordinate proportionally to an
k
estimate of kf
1;(cid:22). The intuition  detailed in Section 2  is that the resulting data space behaves as a
k
low-dimensional projection to coordinates with large norm kf
1;(cid:22)  while maintaining information
about all coordinates. We show that such weighting can be learned efﬁciently  both in batch-mode
and online  and can signiﬁcantly improve the performance of distance-based regressors in real-world
p
applications. In this paper we focus on the distance-based methods of kernel and k-NN regression.
For distance-based methods  the weights can be incorporated into a distance function of the form
(x (cid:0) x0)>W(x (cid:0) x0)  where each element Wi of the diagonal matrix W is an esti-
0
(cid:26)(x; x
) =
k
mate of kf
1;(cid:22). This is not metric learning [1  2  3  4] where the best (cid:26) is found by optimizing
over a sufﬁciently large space of possible metrics. Clearly metric learning can only yield better per-
formance  but the optimization over a larger space will result in heavier preprocessing time  often
O(n2) on datasets of size n. Yet  preprocessing time is especially important in many modern ap-
plications where both training and prediction are done online (e.g. robotics  ﬁnance  advertisement 
recommendation systems). Here we do not optimize over a space of metrics  but rather estimate a
single metric (cid:26) based on the norms kf
1;(cid:22). Our metric (cid:26) is efﬁciently obtained  can be estimated
online  and still signiﬁcantly improves the performance of distance-based regressors.
To estimate kf
0
i well everywhere  just well on average. While
many elaborate derivative estimators exist (see e.g. [5])  we have to keep in mind our need for
fast but consistent estimator of kf
k
1;(cid:22). We propose a simple estimator Wi which averages the
differences along i of an estimator fn;h of f. More precisely (see Section 3) Wi has the form
En jfn;h(X + tei) (cid:0) fn;h(X (cid:0) tei)j =2t where En denotes the empirical expectation over a sample
fXign
In this paper fn;h is a kernel estimator  although any regression method might be used in estimating
k
kf
1;(cid:22). We prove in Section 4 that  under mild conditions  Wi is a consistent estimator of the

1 . Wi can therefore be updated online at the cost of just two estimates of fn;h.

0
i

k
1;(cid:22)  one does not need to estimate f

k

0
i

0
i

0
i

(cid:3)

Currently at Toyota Technological Institute Chicago  and afﬁliated with the Max Planck Institute.

1

(a) SARCOS robot  joint 7.

Figure 1: Typical gradient weights

(b) Parkinson’s.

n
Wi (cid:25) kf

0
i

k

1;(cid:22)

o

(c) Telecom.

for some real-world datasets.

i2[d]

unknown norm kf
practical tuning of the two parameters t and h.

k
1;(cid:22). Moreover we prove ﬁnite sample convergence bounds to help guide the

0
i

Most related work

As we mentioned above  metric learning is closest in spirit to the gradient-weighting approach pre-
sented here  but our approach is different from metric learning in that we do not search a space
of possible metrics  but rather estimate a single metric based on gradients. This is far more time-
efﬁcient and can be implemented in online applications which require fast preprocessing.
There exists many metric learning approaches  mostly for classiﬁcation and few for regression (e.g.
[1  2]). The approaches of [1  2] for regression are meant for batch learning. Moreover [1] is limited
to Gaussian-kernel regression  and [2] is tuned to the particular problem of age estimation. For the
problem of classiﬁcation  the metric-learning approaches of [3  4] are meant for online applications 
but cannot be used in regression.
In the case of kernel regression and local polynomial regression  multiple bandwidths can be used 
one for each coordinate [6]. However  tuning d bandwidth parameters requires searching a d(cid:2)d grid 
which is impractical even in batch mode. The method of [6] alleviates this problem  however only
in the particular case of local linear regression. Our method applies to any distance-based regressor.
Finally  the ideas presented here are related to recent notions of nonparametric sparsity where it is
assumed that the target function is well approximated by a sparse function  i.e. one which varies
little in most coordinates (e.g. [6]  [? ]). Here we do not need sparsity  instead we only need the
target function to vary in some coordinates more than in others. Our approach therefore works even
in cases where the target function is far from sparse.

2 Technical motivation
k
In this section  we motivate the approach by considering the ideal situation where Wi = kf
1;(cid:22).
Let’s consider regression on (X ; (cid:26))  where the input space X (cid:26) Rd is connected. The prediction
performance of a distance-based estimator (e.g. kernel or k-NN) is well known to be the sum of its
variance and its bias [7]. Regression on (X ; (cid:26)) decreases variance while keeping the bias controlled.
Regression variance decreases on (X ; (cid:26)): The variance of a distance based estimate fn(x) is in-
versely proportional to the number of samples (and hence the mass) in a neighborhood of x (see
e.g. [8]). Let’s therefore compare the masses of (cid:26)-balls and Euclidean balls. Suppose some weights
largely dominate others  for instance in R2  let kf
k
1;(cid:22). A ball B(cid:26) in (X ; (cid:26)) then takes
the ellipsoidal shape below which we contrast with the dotted Euclidean ball inside.

(cid:29) kf

k

1;(cid:22)

0
1

0
i

0
2

2

d

Relative to a Euclidean ball  a ball B(cid:26) of similar1 radius has more mass in the direction e1 in which f
varies least. This intuition is made more precise in Lemma 1 below  which is proved in the appendix.
Essentially  let R (cid:26) [d] be the set of coordinates with larger weights Wi  then the mass of balls B(cid:26)
behaves like the mass of balls in RjRj. Thus  effectively  regression in (X ; (cid:26)) has variance nearly as
small as that for regression in the lower-dimensional space RjRj.
Note that the assumptions on the marginal (cid:22) in the lemma statement are veriﬁed for instance when
(cid:22) has a continuous lower-bounded density on X . For simplicity we let (X ;k(cid:1)k) have diameter 1.
p
Lemma 1 (Mass of (cid:26)-balls). Consider any R (cid:26) [d] such that maxi =2R Wi < mini2R Wi: Sup-
pose X (cid:17) 1p
[0; 1]d  and the marginal (cid:22) satisﬁes on (X ;k(cid:1)k)  for some C1; C2: 8x 2 X ;8r >
maxi2R Wi= mini2R Wi  (cid:15)6R   maxi =2R Wi (cid:1) p
0; C1rd (cid:20) (cid:22)(B(x; r)) (cid:20) C2rd. Let (cid:20)  
d 
and let (cid:26)(X )   supx;x02X (cid:26)(x; x
0
Then for any (cid:15)(cid:26)(X ) > 2(cid:15)6R  (cid:22)(B(cid:26)(x; (cid:15)(cid:26)(X ))) (cid:21) C(2(cid:20))
Ideally we would want jRj (cid:28) d and (cid:15)6R (cid:25) 0  which corresponds to a sparse metric.
Regression bias remains bounded on (X ; (cid:26)): The bias of distance-based regressors is controlled by
the smoothness of the unknown function f on (X ; (cid:26))  i.e. how much f might differ for two close
0 that were originally far from x
points. Turning back to our earlier example in R2  some points x
along e1 might now be included in the estimate fn(x) on (X ; (cid:26)). Intuitively  this should not add bias
to the estimate since f does not vary much in e1. We have the following lemma.
i is bounded on X
0
Lemma 2 (Change in Lipschitz smoothness for f). Suppose each derivative f
j
j
sup. Assume Wi > 0 whenever jf
by jf
0
0
sup > 0. Denote by R the largest subset of [d] such that
jf
j
0 2 X 
sup > 0 for i 2 R . We have for all x; x
i
i
0
i
jf (x) (cid:0) f (x
0

jRj  where C is independent of (cid:15).

 X

0
(cid:26)(x; x

(cid:0)jRj

).

):

(cid:15)

!
P

j
jf
0
supp
i
Wi

i2R

)j (cid:20)
q

0
i

j

0
i

jf

kf

i2R

0  is of the order

Applying the above lemma with Wi = 1  we see that in the original Euclidean space  the variation
sup. This variation in f is now
in f relative to distance between points x; x
increased in (X ; (cid:26)) by a factor of 1= inf i2R
k
1;(cid:22) in the worst case. In this sense  the space
(X ; (cid:26)) maintains information about all relevant coordinates. In contrast  information is lost under a
projection of the data in the likely scenario that all or most coordinates are relevant.
Finally  note that if all weights were close  the space (X ; (cid:26)) is essentially equivalent to the original
(X ;k(cid:1)k)  and we likely neither gain nor loose in performance  as conﬁrmed by experiments. How-
ever  we observed that in practice  even when all coordinates are relevant  the gradient-weights vary
sufﬁciently (Figure 1) to observe signiﬁcant performance gains for distance-based regressors.
3 Estimating kf
In all that follows we are given n i.i.d samples (X; Y) = f(Xi; Yi)gn
distribution with marginal (cid:22). The marginal (cid:22) has support X (cid:26) Rd while the output Y 2 R.
The kernel estimate at x is deﬁned using any kernel K(u)  positive on [0; 1=2]  and 0 for u > 1. If
nX
B(x; h) \ X = ;  fn;h(x) = EnY   otherwise

i=1  from some unknown

nX

k

1;(cid:22)

0
i

fn; (cid:22)(cid:26);h(x) =

i=1

K((cid:22)(cid:26)(x; Xi)=h)
n
j=1 K((cid:22)(cid:26)(x; Xj)=h)

(cid:1) Yi =

wi(x)Yi;

i=1

P

(1)

for some metric (cid:22)(cid:26) and a bandwidth parameter h.
For the kernel regressor fn;h used to learn the metric (cid:26) below  (cid:22)(cid:26) is the Euclidean metric. In the

analysis we assume the bandwidth for fn;h is set as h (cid:21) (cid:0)

(cid:1)1=d  given a conﬁdence

log2(n=(cid:14))=n

1Accounting for the scale change induced by (cid:26) on the space X .

3

parameter 0 < (cid:14) < 1. In practice we would learn h by cross-validation  but for the analysis we only
need to know the existence of a good setting of h.
The metric is deﬁned as
Wi   En
(2)
where An;i(X) is the event that enough samples contribute to the estimate (cid:1)t;ifn;h(X). For the
consistency result  we assume the following setting:

(cid:2)
(cid:1)t;ifn;h(X) (cid:1) 1fAn;i(X)g

jfn;h(X + tei) (cid:0) fn;h(X (cid:0) tei)j

(cid:1) 1fAn;i(X)g = En

(cid:3)

2t

;

An;i(X) (cid:17) min

s2f(cid:0)t;tg (cid:22)n(B(X + sei; h=2)) (cid:21) (cid:11)n where (cid:11)n   2d ln 2n + ln(4=(cid:14))

n

:

4 Consistency of the estimator Wi of kf
4.1 Theoretical setup

0
i

k

1;(cid:22)

4.1.1 Marginal (cid:22)
Without loss of generality we assume X has bounded diameter 1. The marginal is assumed to have
a continuous density on X and has mass everywhere on X : 8x 2 X ;8h > 0; (cid:22)(B(x; h)) (cid:21) C(cid:22)hd.
This is for instance the case if (cid:22) has a lower-bounded density on X . Under this assumption  for
samples X in dense regions  X (cid:6) tei is also likely to be in a dense region.
4.1.2 Regression function and noise
The output Y 2 R is given as Y = f (X) + (cid:17)(X)  where E(cid:17)(X) = 0. We assume the following
general noise model: 8(cid:14) > 0 there exists c > 0 such that supx2X PY jX=x (j(cid:17)(x)j > c) (cid:20) (cid:14):
We denote by CY ((cid:14)) the inﬁmum over all such c. For instance  suppose (cid:17)(X) has exponentially
decreasing tail  then 8(cid:14) > 0  CY ((cid:14)) (cid:20) O(ln 1=(cid:14)). A last assumption on the noise is that the
variance of (Y jX = x) is upper-bounded by a constant (cid:27)2
Deﬁne the (cid:28)-envelope of X as X +B(0; (cid:28) )   fz 2 B(x; (cid:28) ); x 2 Xg. We assume there exists (cid:28) such
that f is continuously differentiable on the (cid:28)-envelope X + B(0; (cid:28) ). Furthermore  each derivative
>
0
sup and is uniformly continuous on
f
i (x) = e
X + B(0; (cid:28) ) (this is automatically the case if the support X is compact).
i
4.1.3 Parameters varying with t
Our consistency results are expressed in terms of the following distributional quantities. For i 2 [d] 
deﬁne the (t; i)-boundary of X as @t;i(X )   fx : fx + tei; x (cid:0) teig * Xg. The smaller the mass
(cid:22)(@t;i(X )) at the boundary  the better we approximate kf
k
0
1;(cid:22).
i
i (x) (cid:0) f
jf
0
The second type of quantity is (cid:15)t;i   supx2X ; s2[(cid:0)t;t]
Since (cid:22) has continuous density on X and rf is uniformly continuous on X + B(0; (cid:28) )  we automat-
ically have (cid:22)(@t;i(X )) t!0(cid:0)(cid:0)(cid:0)! 0 and (cid:15)t;i

rf (x) is upper bounded on X + B(0; (cid:28) ) by jf

Y uniformly over all x 2 X .

i (x + sei)j.
0

t!0(cid:0)(cid:0)(cid:0)! 0.

0
i

j

4.2 Main theorem
k
Our main theorem bounds the error in estimating each norm kf
1;(cid:22) with Wi. The main technical
hurdles are in handling the various sample inter-dependencies introduced by both the estimates
fn;h(X) and the events An;i(X)  and in analyzing the estimates at the boundary of X .
Theorem 1. Let t + h (cid:20) (cid:28)  and let 0 < (cid:14) < 1. There exist C = C((cid:22); K((cid:1))) and N = N ((cid:22)) such
that the following holds with probability at least 1(cid:0) 2(cid:14). Deﬁne A(n)   Cd(cid:1) log(n=(cid:14))(cid:1) C 2
Y ((cid:14)=2n)(cid:1)
!
Y = log2(n=(cid:14)). Let n (cid:21) N  we have for all i 2 [d]:
(cid:27)2
+ (cid:22) (@t;i(X ))

(cid:12)(cid:12)(cid:12)Wi (cid:0) kf

1A + 2jf

(cid:12)(cid:12)(cid:12) (cid:20) 1

0@r

 r

X

ln 2d=(cid:14)

+ (cid:15)t;i:

jf

k

0
i

j

0
i

sup

0
i

1;(cid:22)

j

0
i

sup

A(n)

nhd + h (cid:1)

i2[d]

t

n

4

The bound suggest to set t in the order of h or larger. We need t to be small in order for (cid:22) (@t;i(X ))
and (cid:15)t;i to be small  but t need to be sufﬁciently large (relative to h) for the estimates fn;h(X + tei)
and fn;h(X (cid:0) tei) to differ sufﬁciently so as to capture the variation in f along ei.
The theorem immediately implies consistency for t n!1(cid:0)(cid:0)(cid:0)(cid:0)! 0  h n!1(cid:0)(cid:0)(cid:0)(cid:0)! 0  h=t n!1(cid:0)(cid:0)(cid:0)(cid:0)! 0  and
(n= log n)hdt2 n!1(cid:0)(cid:0)(cid:0)(cid:0)! 1. This is satisﬁed for many settings  for example t / p
h and h / 1= log n.

4.3 Proof of Theorem 1

(cid:12)(cid:12)(cid:12) is in circumventing certain depencies: both quanti-
(cid:12)(cid:12)(cid:12)  i 2 [d]  starting with:

The main difﬁculty in bounding
ties fn;h(X) and An;i(X) depend not just on X 2 X  but on other samples in X  and thus introduce
inter-dependencies between the estimates (cid:1)t;ifn;h(X) for different X 2 X.
To handle these dependencies  we carefully decompose
i (X)jj +
0

(cid:12)(cid:12)(cid:12)Wi (cid:0) kf
(cid:12)(cid:12)(cid:12)En jf
r
The following simple lemma bounds the second term of (3).
Lemma 3. With probability at least 1 (cid:0) (cid:14)  we have for all i 2 [d] 

(cid:12)(cid:12)(cid:12)Wi (cid:0) kf
(cid:12)(cid:12)(cid:12) (cid:20) jWi (cid:0) En jf
(cid:12)(cid:12)(cid:12)En jf

k
i (X)j (cid:0) kf
0

(cid:12)(cid:12)(cid:12)Wi (cid:0) kf

(cid:12)(cid:12)(cid:12) (cid:20) jf

i (X)j (cid:0) kf
0

(cid:12)(cid:12)(cid:12) :

(3)

k

k

k

j

0
i

k

0
i

1;(cid:22)

(cid:1)

sup

0
i

1;(cid:22)

0
i

1;(cid:22)

0
i

1;(cid:22)

1;(cid:22)

0
i

ln 2d=(cid:14)

n

:

Proof. Apply a Chernoff bound  and a union bound on i 2 [d].

Now the ﬁrst term of equation (3) can be further bounded as
i (X)j (cid:1) 1fAn;i(X)g
0
i (X)j (cid:1) 1fAn;i(X)g
0

i (X)jj (cid:20)(cid:12)(cid:12)Wi (cid:0) En jf
(cid:20)(cid:12)(cid:12)Wi (cid:0) En jf

jWi (cid:0) En jf

0

(cid:12)(cid:12) + En jf
(cid:12)(cid:12) + jf

j

0
i

sup

i (X)j (cid:1) 1f (cid:22)An;i(X)g
0
(cid:1) En1f (cid:22)An;i(X)g:

(4)

We will bound each term of (4) separately.
The next lemma bounds the second term of (4). It is proved in the appendix. The main technicality
in this lemma is that  for any X in the sample X  the event (cid:22)An;i(X) depends on other samples in X.
Lemma 4. Let @t;i(X ) be deﬁned as in Section (4.1.3). For n (cid:21) n((cid:22))  with probability at least
1 (cid:0) 2(cid:14)  we have for all i 2 [d] 

r

ln 2d=(cid:14)

+ (cid:22) (@t;i(X )) :

(cid:12)(cid:12). To this end we need to bring in f through the

(cid:21)

(cid:2)
(cid:1)t;if (X) (cid:1) 1fAn;i(X)g

(cid:3)

= En

P
(cid:1) 1fAn;i(X)g

i wi(x)f (xi).

n

En1f (cid:22)An;i(X)g (cid:20)

i (X)j (cid:1) 1fAn;i(X)g
0

fWi   En

It remains to bound
following quantities:

(cid:12)(cid:12)Wi (cid:0) En jf
(cid:20)jf (X + tei) (cid:0) f (X (cid:0) tei)j
The quantityfWi is easily related to En jf
quantity ~fn;h(x) is needed when relating Wi tofWi.
(cid:12)(cid:12)(cid:12)fWi (cid:0) En jf

and for any x 2 X   deﬁne ~fn;h(x)   EYjXfn;h(x) =

2t

i (X)j (cid:1) 1fAn;i(X)g. This is done in Lemma 5 below. The
0
Lemma 5. Deﬁne (cid:15)t;i as in Section (4.1.3). With probability at least 1 (cid:0) (cid:14)  we have for all i 2 [d] 

(cid:12)(cid:12)(cid:12) (cid:20) (cid:15)t;i:

i (X)j (cid:1) 1fAn;i(X)g
0

5

i (x) (cid:0) (cid:15)t;i) (cid:20) f (x + tei) (cid:0) f (x (cid:0) tei) (cid:20) 2t (f
0

0
i (x) + (cid:15)t;i) :

0
i (x + sei) ds and therefore

i (x)j(cid:12)(cid:12) (cid:20) (cid:15)t;i  therefore

0

jf (x + tei) (cid:0) f (x (cid:0) tei)j (cid:0) jf

i (x)j
0

(cid:12)(cid:12)(cid:12)(cid:12) (cid:20) (cid:15)t;i:

R

t(cid:0)t f

Proof. We have f (x + tei) (cid:0) f (x (cid:0) tei) =

2t

2t (f

It follows that

jf (x + tei) (cid:0) f (x (cid:0) tei)j (cid:0) jf
i (X)j (cid:1) 1fAn;i(X)g
0

(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12) (cid:20) En
(cid:12)(cid:12)(cid:12)fWi (cid:0) En jf
It remains to relate Wi tofWi. We have
(cid:12)(cid:12)(cid:12)Wi (cid:0)fWi

(cid:12)(cid:12)(cid:12) =2t

(cid:12)(cid:12)(cid:12)(cid:12) 1

2t

2t

(cid:20)2 max
s2f(cid:0)t;tg
(cid:20)2 max
s2f(cid:0)t;tg

(cid:12)(cid:12)
(cid:12)(cid:12)En((cid:1)t;ifn;h(X) (cid:0) (cid:1)t;if (X)) (cid:1) 1fAn;i(X)g
(cid:12)(cid:12)(cid:12) (cid:1) 1fAn;i(X)g
(cid:12)(cid:12)(cid:12)fn;h(X + sei) (cid:0) ~fn;h(X + sei)
(cid:12)(cid:12)(cid:12) (cid:1) 1fAn;i(X)g:
(cid:12)(cid:12)(cid:12) ~fn;h(X + sei) (cid:0) f (X + sei)
(cid:12)(cid:12)(cid:12) (cid:1) 1fAn;i(X)g (cid:20) h (cid:1)
We ﬁrst handle the bias term (6) in the next lemma which is given in the appendix.
X
Lemma 6 (Bias). Let t + h (cid:20) (cid:28). We have for all i 2 [d]  and all s 2 ft;(cid:0)tg:
jf

Enjfn;h(X + sei) (cid:0) f (X + sei)j (cid:1) 1fAn;i(X)g
En

(cid:12)(cid:12)(cid:12) ~fn;h(X + sei) (cid:0) f (X + sei)

+ 2 max
s2f(cid:0)t;tg

En

En

0
i

j

sup :

i2[d]

(5)

(6)

The variance term in (5) is handled in the lemma below. The proof is given in the appendix.
Lemma 7 (Variance terms). There exist C = C((cid:22); K((cid:1))) such that  with probability at least 1(cid:0) 2(cid:14) 
we have for all i 2 [d]  and all s 2 f(cid:0)t; tg:

(cid:12)(cid:12)(cid:12)fn;h(X + sei) (cid:0) ~fn;h(X + sei)

(cid:12)(cid:12)(cid:12) (cid:1) 1fAn;i(X)g (cid:20)

En

Cd (cid:1) log(n=(cid:14))C 2
n(h=2)d

Y ((cid:14)=2n) (cid:1) (cid:27)2

Y

:

s

The next lemma summarizes the above results:
Lemma 8. Let t + h (cid:20) (cid:28) and let 0 < (cid:14) < 1. There exist C = C((cid:22); K((cid:1))) such that the following
holds with probability at least 1 (cid:0) 2(cid:14). Deﬁne A(n)   Cd (cid:1) log(n=(cid:14)) (cid:1) C 2
Y = log2(n=(cid:14)).
X
We have

Y ((cid:14)=2n) (cid:1) (cid:27)2

(cid:12)(cid:12)Wi (cid:0) En jf

1A + (cid:15)t;i:

i (X)j (cid:1) 1fAn;i(X)g
0

A(n)

nhd + h (cid:1)

jf

0
i

j

sup

i2[d]

0@r

(cid:12)(cid:12) (cid:20) 1

t

Proof. Apply lemmas 5  6 and 7  in combination with equations 5 and 6.

To complete the proof of Theorem 1  apply lemmas 8 and 3 in combination with equations 3 and 4.

5 Experiments

5.1 Data description

We present experiments on several real-world regression datasets. The ﬁrst two datasets describe the
dynamics of 7 degrees of freedom of robotic arms  Barrett WAM and SARCOS [9  10]. The input
points are 21-dimensional and correspond to samples of the positions  velocities  and accelerations
of the 7 joints. The output points correspond to the torque of each joint. The far joints (1  5  7)

6

KR error
KR-(cid:26) error
KR time
KR-(cid:26) time

KR error
KR-(cid:26) error
KR time
KR-(cid:26) time

Barrett joint 5
0.50 (cid:6) 0.03
0.35 (cid:6) 0.02
0.37 (cid:6) 0.01
0.38 (cid:6) 0.02
Concrete Strength Wine Quality
0.75 (cid:6) 0.03
0.75 (cid:6) 0.02
0.19 (cid:6) 0.02
0.19 (cid:6) 0.02

Barrett joint 1
0.50 (cid:6) 0.02
0.38(cid:6) 0.03
0.39 (cid:6) 0.02
0.41 (cid:6) 0.03
0.42 (cid:6) 0.05
0.37 (cid:6) 0.03
0.14 (cid:6) 0.02
0.14 (cid:6) 0.01

k-NN error
k-NN-(cid:26) error
k-NN time
k-NN-(cid:26) time

k-NN error
k-NN-(cid:26) error
k-NN time
k-NN-(cid:26) time

Barrett joint 5
0.40 (cid:6) 0.02
0.30 (cid:6) 0.02
0.16 (cid:6) 0.03
0.16 (cid:6) 0.03
Concrete Strength Wine Quality
0.73 (cid:6) 0.04
0.72 (cid:6) 0.03
0.15 (cid:6) 0.01
0.15 (cid:6) 0.01

Barrett joint 1
0.41 (cid:6) 0.02
0.29 (cid:6) 0.01
0.21 (cid:6) 0.04
0.13 (cid:6) 0.04
0.40 (cid:6) 0.04
0.38 (cid:6) 0.03
0.10 (cid:6) 0.01
0.11 (cid:6) 0.01

SARCOS joint 1
0.16 (cid:6) 0.02
0.14 (cid:6) 0.02
0.28 (cid:6) 0.05
0.32 (cid:6) 0.05
Telecom
0.30(cid:6)0.02
0.23(cid:6)0.02
0.15(cid:6)0.01
0.16(cid:6)0.01

SARCOS joint 1
0.08 (cid:6) 0.01
0.07 (cid:6) 0.01
0.13 (cid:6) 0.01
0.14 (cid:6) 0.01
Telecom
0.13(cid:6)0.02
0.17(cid:6)0.02
0.16(cid:6)0.02
0.15(cid:6)0.01

SARCOS joint 5
0.14 (cid:6) 0.02
0.12 (cid:6) 0.01
0.23 (cid:6) 0.03
0.23 (cid:6) 0.02
Ailerons
0.40(cid:6)0.02
0.39(cid:6)0.02
0.20(cid:6)0.01
0.21(cid:6)0.01

SARCOS joint 5
0.08 (cid:6) 0.01
0.07 (cid:6) 0.01
0.13 (cid:6) 0.01
0.13 (cid:6) 0.01
Ailerons
0.37(cid:6)0.01
0.34(cid:6)0.01
0.12(cid:6)0.01
0.11(cid:6)0.01

Housing
0.37 (cid:6)0.08
0.25 (cid:6)0.06
0.10 (cid:6)0.01
0.11 (cid:6)0.01
Parkinson’s
0.38(cid:6)0.03
0.34(cid:6)0.03
0.30(cid:6)0.03
0.30(cid:6)0.03

Housing
0.28 (cid:6)0.09
0.22(cid:6)0.06
0.08 (cid:6)0.01
0.08 (cid:6)0.01
Parkinson’s
0.22(cid:6)0.01
0.20(cid:6)0.01
0.14(cid:6)0.01
0.15(cid:6)0.01

Table 1: Normalized mean square prediction errors and average prediction time per point (in mil-
liseconds). The top two tables are for KR vs KR-(cid:26) and the bottom two for k-NN vs k-NN-(cid:26).

(a) SARCOS  joint 7  with KR

(b) Ailerons with KR

(c) Telecom with KR

(d) SARCOS  joint 7  with k-NN

(e) Ailerons with k-NN

(f) Telecom with k-NN

Figure 2: Normalized mean square prediction error over 2000 points for varying training sizes.
Results are shown for k-NN and kernel regression (KR)  with and without the metric (cid:26).

correspond to different regression problems and are the only results reported. Expectedly  results for
the other joints are similarly good.
The other datasets are taken from the UCI repository [11] and from [12]. The concrete strength
dataset (Concrete Strength) contains 8-dimensional input points  describing age and ingredients of
concrete  the output points are the compressive strength. The wine quality dataset (Wine Quality)
contains 11-dimensional input points corresponding to the physicochemistry of wine samples  the
output points are the wine quality. The ailerons dataset (Ailerons) is taken from the problem of ﬂying
a F16 aircraft. The 5-dimensional input points describe the status of the aeroplane  while the goal is

7

1000200030004000500000.020.040.060.080.1number of training pointserror KR errorKR−ρ error100020003000400050000.320.340.360.380.40.420.44number of training pointserror KR errorKR−ρ error10002000300040005000600070000.10.150.20.250.30.35number of training pointserror KR errorKR−ρ error100020003000400050000.0050.010.0150.020.025number of training pointserror k−NN errork−NN−ρ error100020003000400050000.290.30.310.320.330.340.350.360.370.38number of training pointserror k−NN errork−NN−ρ error100020003000400050006000700000.050.10.150.2number of training pointserror k−NN errork−NN−ρ errorto predict the control action on the ailerons of the aircraft. The housing dataset (Housing) concerns
the task of predicting housing values in areas of Boston  the input points are 13-dimensional. The
Parkinson’s Telemonitoring dataset (Parkison’s) is used to predict the clinician’s Parkinson’s disease
symptom score using biomedical voice measurements represented by 21-dimensional input points.
We also consider a telecommunication problem (Telecom)  wherein the 47-dimensional input points
and the output points describe the bandwidth usage in a network.
For all datasets we normalize each coordinate with its standard deviation from the training data.

5.2 Experimental setup

To learn the metric  we set h by cross-validation on half the training points  and we set t = h=2
for all datasets. Note that in practice we might want to also tune t in the range of h for even
better performance than reported here. The event An;i(X) is set to reject the gradient estimate
(cid:1)n;ifn;h(X) at X if no sample contributed to one the estimates fn;h(X (cid:6) tei).
In each experiment  we compare kernel regression in the euclidean metric space (KR) and in the
learned metric space (KR-(cid:26))  where we use a box kernel for both. Similar comparisons are made
using k-NN and k-NN-(cid:26). All methods are implemented using a fast neighborhood search procedure 
namely the cover-tree of [13]  and we also report the average prediction times so as to conﬁrm that 
on average  time-performance is not affected by using the metric.
The parameter k in k-NN/k-NN-(cid:26)  and the bandwidth in KR/KR-(cid:26) are learned by cross-validation
on half of the training points. We try the same range of k (from 1 to 5 log n) for both k-NN and
k-NN-(cid:26). We try the same range of bandwidth/space-diameter (a grid of size 0:02 from 1 to 0:02 )
for both KR and KR-(cid:26): this is done efﬁciently by starting with a log search to detect a smaller range 
followed by a grid search on a smaller range.
Table 5 shows the normalized Mean Square Errors (nMSE) where the MSE on the test set is normal-
ized by variance of the test output. We use 1000 training points in the robotic datasets  2000 training
points in the Telecom  Parkinson’s  Wine Quality  and Ailerons datasets  and 730 training points in
Concrete Strength  and 300 in Housing. We used 2000 test points in all of the problems  except for
Concrete  300 points  and Housing  200 points. Averages over 10 random experiments are reported.
For the larger datasets (SARCOS  Ailerons  Telecom) we also report the behavior of the algorithms 
with and without metric  as the training size n increases (Figure 2).

5.3 Discussion of results

From the results in Table 5 we see that virtually on all datasets the metric helps improve the perfor-
mance of the distance based-regressor even though we did not tune t to the particular problem (re-
member t = h=2 for all experiments). The only exceptions are for Wine Quality where the learned
weights are nearly uniform  and for Telecom with k-NN. We noticed that the Telecom dataset has
a lot of outliers and this probably explains the discrepancy  besides from the fact that we did not
attempt to tune t. Also notice that the error of k-NN is already low for small sample sizes  making
it harder to outperform. However  as shown in Figure 2  for larger training sizes k-NN-(cid:26) gains on
k-NN. The rest of the results in Figure 2 where we vary n are self-descriptive: gradient weighting
clearly improves the performance of the distance-based regressors.
We also report the average prediction times in Table 5. We see that running the distance-based
methods with gradient weights does not affect estimation time. Last  remember that the metric can
be learned online at the cost of only 2d times the average kernel estimation time reported.

6 Final remarks

Gradient weighting is simple to implement  computationally efﬁcient in batch-mode and online  and
most importantly improves the performance of distance-based regressors on real-world applications.
In our experiments  most or all coordinates of the data are relevant  yet some coordinates are more
important than others. This is sufﬁcient for gradient weighting to yield gains in performance. We
believe there is yet room for improvement given the simplicity of our current method.

8

References
[1] Kilian Q. Weinberger and Gerald Tesauro. Metric learning for kernel regression. Journal of

Machine Learning Research - Proceedings Track  2:612–619  2007.

[2] Bo Xiao  Xiaokang Yang  Yi Xu  and Hongyuan Zha. Learning distance metric for regression
by semideﬁnite programming with application to human age estimation. In Proceedings of the
17th ACM international conference on Multimedia  pages 451–460  2009.

[3] Shai Shalev-shwartz  Yoram Singer  and Andrew Y. Ng. Online and batch learning of pseudo-

metrics. In ICML  pages 743–750. ACM Press  2004.

[4] Jason V. Davis  Brian Kulis  Prateek Jain  Suvrit Sra  and Inderjit S. Dhillon. Information-

theoretic metric learning. In ICML  pages 209–216  2007.

[5] W. H¨ardle and T. Gasser. On robust kernel estimation of derivatives of regression functions.

Scandinavian journal of statistics  pages 233–240  1985.

[6] J. Lafferty and L. Wasserman. Rodeo: Sparse nonparametric regression in high dimensions.

Arxiv preprint math/0506342  2005.

[7] L. Rosasco  S. Villa  S. Mosci  M. Santoro  and A. Verri. Nonparametric sparsity and regular-

ization. http://arxiv.org/abs/1208.2572  2012.

[8] L. Gyorﬁ  M. Kohler  A. Krzyzak  and H. Walk. A Distribution Free Theory of Nonparametric

Regression. Springer  New York  NY  2002.

[9] S. Kpotufe. k-NN Regression Adapts to Local Intrinsic Dimension. NIPS  2011.
[10] Duy Nguyen-Tuong  Matthias W. Seeger  and Jan Peters. Model learning with local gaussian

process regression. Advanced Robotics  23(15):2015–2034  2009.

[11] Duy Nguyen-Tuong and Jan Peters. Incremental online sparsiﬁcation for model learning in

real-time robot control. Neurocomputing  74(11):1859–1867  2011.

[12] A. Frank and A. Asuncion. UCI machine learning repository. http://archive.ics.
uci.edu/ml. University of California  Irvine  School of Information and Computer Sci-
ences  2012.

[13] Luis Torgo. Regression datasets. http://www.liaad.up.pt/˜ltorgo. University of

Porto  Department of Computer Science  2012.

[14] A. Beygelzimer  S. Kakade  and J. Langford. Cover trees for nearest neighbors. ICML  2006.

9

,Josh Merel
Roy Fox
Tony Jebara
Liam Paninski
Daniel Vainsencher
Han Liu
Tong Zhang
Yali Wan
Marina Meila