2013,More data speeds up training time in learning halfspaces over sparse vectors,The increased availability of data in recent years led several authors to ask whether it is possible to use data as  a {\em computational} resource. That is  if more data is available  beyond the sample complexity limit  is it possible to use the extra examples to speed up the computation time required to perform the learning task?  We give the first positive answer to this question for a {\em natural supervised learning problem} --- we consider agnostic PAC learning of halfspaces over $3$-sparse vectors in $\{-1 1 0\}^n$. This class is inefficiently learnable using $O\left(n/\epsilon^2\right)$ examples. Our main contribution is a novel  non-cryptographic  methodology for establishing computational-statistical gaps  which allows us to show that  under a widely believed assumption that refuting random $\mathrm{3CNF}$ formulas is hard  efficiently learning this class using $O\left(n/\epsilon^2\right)$ examples is impossible. We further show that under stronger hardness assumptions  even $O\left(n^{1.499}/\epsilon^2\right)$ examples do not suffice.  On the other hand  we show a new algorithm that learns this class efficiently using $\tilde{\Omega}\left(n^2/\epsilon^2\right)$ examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem.,More data speeds up training time in learning

halfspaces over sparse vectors

Amit Daniely

Department of Mathematics

The Hebrew University

Jerusalem  Israel

Nati Linial

School of CS and Eng.
The Hebrew University

Jerusalem  Israel

Shai Shalev-Shwartz
School of CS and Eng.
The Hebrew University

Jerusalem  Israel

Abstract

The increased availability of data in recent years has led several authors to ask
whether it is possible to use data as a computational resource. That is  if more
data is available  beyond the sample complexity limit  is it possible to use the
extra examples to speed up the computation time required to perform the learning
task?
We give the ﬁrst positive answer to this question for a natural supervised learning
problem — we consider agnostic PAC learning of halfspaces over 3-sparse vec-

Our main contribution is a novel  non-cryptographic  methodology for establish-
ing computational-statistical gaps  which allows us to show that  under a widely
believed assumption that refuting random 3CNF formulas is hard  it is impossible

tors in {−1  1  0}n. This class is inefﬁciently learnable using O(cid:0)n/2(cid:1) examples.
to efﬁciently learn this class using only O(cid:0)n/2(cid:1) examples. We further show that
under stronger hardness assumptions  even O(cid:0)n1.499/2(cid:1) examples do not suf-
using ˜Ω(cid:0)n2/2(cid:1) examples. This formally establishes the tradeoff between sample

ﬁce. On the other hand  we show a new algorithm that learns this class efﬁciently

and computational complexity for a natural supervised learning problem.

1

Introduction

In the modern digital period  we are facing a rapid growth of available datasets in science and
technology. In most computing tasks (e.g. storing and searching in such datasets)  large datasets
are a burden and require more computation. However  for learning tasks the situation is radically
different. A simple observation is that more data can never hinder you from performing a task. If
you have more data than you need  just ignore it!
A basic question is how to learn from “big data”. The statistical learning literature classically studies
questions like “how much data is needed to perform a learning task?” or “how does accuracy improve
as the amount of data grows?” etc. In the modern  “data revolution era”  it is often the case that the
amount of data available far exceeds the information theoretic requirements. We can wonder whether
this  seemingly redundant data  can be used for other purposes. An intriguing question in this vein 
studied recently by several researchers ([Decatur et al.  1998  Servedio.  2000  Shalev-Shwartz et al. 
2012  Berthet and Rigollet  2013  Chandrasekaran and Jordan  2013])  is the following

Question 1: Are there any learning tasks in which more data  beyond the in-
formation theoretic barrier  can provably be leveraged to speed up computation
time?

The main contributions of this work are:

1

of a natural supervised learning problem for which the answer to Question 1 is positive.

• Conditioning on the hardness of refuting random 3CNF formulas  we give the ﬁrst example
• To prove this  we present a novel technique to establish computational-statistical tradeoffs
in supervised learning problems. To the best of our knowledge  this is the ﬁrst such a result
that is not based on cryptographic primitives.

and 3-sparse vectors using ˜O(cid:0) n

(cid:1) and ˜O

2

(cid:17)

(cid:16) n2

2

examples respectively.

Additional contributions are non trivial efﬁcient algorithms for learning halfspaces over 2-sparse

The natural learning problem we consider is the task of learning the class of halfspaces over k-sparse
vectors. Here  the instance space is the space of k-sparse vectors 

Cn k = {x ∈ {−1  1  0}n | |{i | xi (cid:54)= 0}| ≤ k}  

and the hypothesis class is halfspaces over k-sparse vectors  namely

Hn k = {hw b : Cn k → {±1} | hw b(x) = sign((cid:104)w  x(cid:105) + b)  w ∈ Rn  b ∈ R}  

where (cid:104)· ·(cid:105) denotes the standard inner product in Rn.
We consider the standard setting of agnostic PAC learning  which models the realistic scenario
where the labels are not necessarily fully determined by some hypothesis from Hn k. Note that in
the realizable case  i.e. when some hypothesis from Hn k has zero error  the problem of learning
halfspaces is easy even over Rn.
In addition  we allow improper learning (a.k.a. representation independent learning)  namely  the
learning algorithm is not restricted to output a hypothesis from Hn k  but only should output a hy-
pothesis whose error is not much larger than the error of the best hypothesis in Hn k. This gives the
learner a lot of ﬂexibility in choosing an appropriate representation of the problem. This additional
freedom to the learner makes it much harder to prove lower bounds in this model. Concretely  it is
not clear how to use standard reductions from NP hard problems in order to establish lower bounds
for improper learning (moreover  Applebaum et al. [2008] give evidence that such simple reductions
do not exist).
The classes Hn k and similar classes have been studied by several authors (e.g. Long. and Servedio
[2013]). They naturally arise in learning scenarios in which the set of all possible features is very
large  but each example has only a small number of active features. For example:

• Predicting an advertisement based on a search query: Here  the possible features of each
instance are all English words  whereas the active features are only the set of words given
in the query.
• Learning Preferences [Hazan et al.  2012]: Here  we have n players. A ranking of the
players is a permutation σ : [n] → [n] (think of σ(i) as the rank of the i’th player). Each
ranking induces a preference hσ over the ordered pairs  such that hσ(i  j) = 1 iff i is ranked
higher that j. Namely 

(cid:26)1

hσ(i  j) =

σ(i) > σ(j)
−1 σ(i) < σ(j)

The objective here is to learn the class  Pn  of all possible preferences. The problem of
learning preferences is related to the problem of learning Hn 2: if we associate each pair
(i  j) with the vector in Cn 2 whose i’th coordinate is 1 and whose j’th coordinate is −1 
it is not hard to see that Pn ⊂ Hn 2: for every σ  hσ = hw 0 for the vector w ∈ Rn 
given by wi = σ(i). Therefore  every upper bound for Hn 2 implies an upper bound for
Pn  while every lower bound for Pn implies a lower bound for Hn 2. Since VC(Pn) = n

and VC(Hn 2) = n + 1  the information theoretic barrier to learn these classes is Θ(cid:0) n
(cid:16) n log3(n)

In Hazan et al. [2012] it was shown that Pn can be efﬁciently learnt using O
examples. In section 4  we extend this result to Hn 2.

(cid:1).
(cid:17)

2

2

We will show a positive answer to Question 1 for the class Hn 3. To do so  we show1 the following:
1In fact  similar results hold for every constant k ≥ 3. Indeed  since Hn 3 ⊂ Hn k for every k ≥ 3  it is
trivial that item 3 below holds for every k ≥ 3. The upper bound given in item 1 holds for every k. For item 2 

2

size ˜Ω

2. It is also possible to efﬁciently learn Hn 3 if we are provided with a larger training set (of

1. Ignoring computational issues  it is possible to learn the class Hn 3 using O(cid:0) n
(cid:16) n2
size O(cid:0) n

(cid:17)
(cid:1) under Feige’s assumption regarding the hardness of refuting random 3CNF

3. It is impossible to efﬁciently learn Hn 3  if we are only provided with a training set of
formulas [Feige  2002]. Furthermore  for every α ∈ [0  0.5)  it is impossible to learn
efﬁciently with a training set of size O
under a stronger hardness assumption. This
is formalized in Theorem 4.1.

(cid:1) examples.

). This is formalized in Theorem 3.1.

(cid:16) n1+α

(cid:17)

2

2

2

2

A graphical illustration of our main results is given below:

runtime

2O(n)

> poly(n)

nO(1)

n

n1.5

n2

examples

(cid:17)

(cid:16) n3

The proof of item 1 above is easy – simply note that Hn 3 has VC dimension n + 1.
Item 2 is proved in section 4  relying on the results of Hazan et al. [2012]. We note  however  that
a weaker result  that still sufﬁces for answering Question 1 in the afﬁrmative  can be proven using
a naive improper learning algorithm. In particular  we show below how to learn Hn 3 efﬁciently
examples. The idea is to replace the class Hn 3 with the class {±1}Cn 3
with a sample of Ω
containing all functions from Cn 3 to {±1}. Clearly  this class contains Hn 3.
In addition  we
can efﬁciently ﬁnd a function f that minimizes the empirical training error over a training set S
as follows: For every x ∈ Cn k  if x does not appear at all in the training set we will set f (x)
arbitrarily to 1. Otherwise  we will set f (x) to be the majority of the labels in the training set
that correspond to x. Finally  note that the VC dimension of {±1}Cn 3 is smaller than n3 (since
|Cn 3| < n3). Hence  standard generalization results (e.g. Vapnik [1995]) implies that a training set
size of Ω

sufﬁces for learning this class.

(cid:16) n3

(cid:17)

2

2

Item 3 is shown in section 3 by presenting a novel
computational tradeoffs.
The class Hn 2. Our main result gives a positive answer to Question 1 for the task of improp-
erly learning Hn k for k ≥ 3. A natural question is what happens for k = 2 and k = 1. Since
VC(Hn 1) = VC(Hn 2) = n + 1  the information theoretic barrier for learning these classes is

Θ(cid:0) n
(cid:1). In section 4  we prove that Hn 2 (and  consequently  Hn 1 ⊂ Hn 2) can be learnt using
(cid:16) n log3(n)

technique for establishing statistical-

examples  indicating that signiﬁcant computational-statistical tradeoffs start to mani-

(cid:17)

2

O
fest themselves only for k ≥ 3.

2

1.1 Previous approaches  difﬁculties  and our techniques

[Decatur et al.  1998] and [Servedio.  2000] gave positive answers to Question 1 in the realizable
PAC learning model. Under cryptographic assumptions  they showed that there exist binary learn-
ing problems  in which more data can provably be used to speed up training time. [Shalev-Shwartz
et al.  2012] showed a similar result for the agnostic PAC learning model. In all of these papers  the
main idea is to construct a hypothesis class based on a one-way function. However  the constructed
it is not hard to show that Hn k can be learnt using a sample of Ω
algorithm  similar to the algorithm we describe in this section for k = 3.

examples by a naive improper learning

(cid:16) nk

(cid:17)

2

3

classes are of a very synthetic nature  and are of almost no practical interest. This is mainly due
to the construction technique which is based on one way functions. In this work  instead of using
cryptographic assumptions  we rely on the hardness of refuting random 3CNF formulas. The sim-
plicity and ﬂexibility of 3CNF formulas enable us to derive lower bounds for natural classes such
as halfspaces.
Recently  [Berthet and Rigollet  2013] gave a positive answer to Question 1 in the context of unsuper-
vised learning. Concretely  they studied the problem of sparse PCA  namely  ﬁnding a sparse vector
that maximizes the variance of an unsupervised data. Conditioning on the hardness of the planted
clique problem  they gave a positive answer to Question 1 for sparse PCA. Our work  as well as
the previous work of Decatur et al. [1998]  Servedio. [2000]  Shalev-Shwartz et al. [2012]  studies
Question 1 in the supervised learning setup. We emphasize that unsupervised learning problems
are radically different than supervised learning problems in the context of deriving lower bounds.
The main reason for the difference is that in supervised learning problems  the learner is allowed
to employ improper learning  which gives it a lot of power in choosing an adequate representa-
tion of the data. For example  the upper bound we have derived for the class of sparse halfspaces
switched from representing hypotheses as halfspaces to representation of hypotheses as tables over
Cn 3  which made the learning problem easy from the computational perspective. The crux of the
difﬁculty in constructing lower bounds is due to this freedom of the learner in choosing a convenient
representation. This difﬁculty does not arise in the problem of sparse PCA detection  since there
the learner must output a good sparse vector. Therefore  it is not clear whether the approach given
in [Berthet and Rigollet  2013] can be used to establish computational-statistical gaps in supervised
learning problems.

2 Background and notation
For hypothesis class H ⊂ {±1}X and a set Y ⊂ X  we deﬁne the restriction of H to Y by
H|Y = {h|Y | h ∈ H}. We denote by J = Jn the all-ones n × n matrix. We denote the j’th vector
in the standard basis of Rn by ej.

2.1 Learning Algorithms
For h : Cn 3 → {±1} and a distribution D on Cn 3 × {±1} we denote the error of h w.r.t. D
by ErrD(h) = Pr(x y)∼D (h(x) (cid:54)= y). For H ⊂ {±1}Cn 3 we denote the error of H w.r.t. D by
ErrD(H) = minh∈H ErrD(h). For a sample S ∈ (Cn 3 × {±1})m we denote by ErrS(h) (resp.
ErrS(H)) the error of h (resp. H) w.r.t. the empirical distribution induces by the sample S.
A learning algorithm  L  receives a sample S ∈ (Cn 3 × {±1})m and return a hypothesis L(S) :
Cn 3 → {±1}. We say that L learns Hn 3 using m(n  ) examples if 2 for every distribution D on
Cn 3 × {±1} and a sample S of more than m(n  ) i.i.d. examples drawn from D 

(ErrD(L(S)) > ErrD(H3 n) + ) <

Pr
S

1
10

The algorithm L is efﬁcient if it runs in polynomial time in the sample size and returns a hypothesis
that can be evaluated in polynomial time.

2.2 Refuting random 3SAT formulas
We frequently view a boolean assignment to variables x1  . . .   xn as a vector in Rn. It is convenient 
therefore  to assume that boolean variables take values in {±1} and to denote negation by “ − ”
(instead of the usual “¬”). An n-variables 3CNF clause is a boolean formula of the form

C(x) = (−1)j1xi1 ∨ (−1)j2 xi2 ∨ (−1)j1xi3  x ∈ {±1}n

An n-variables 3CNF formula is a boolean formula of the form

φ(x) = ∧m

i=1Ci(x)  

2For simplicity  we require the algorithm to succeed with probability of at least 9/10. This can be easily
ampliﬁed to probability of at least 1 − δ  as in the usual deﬁnition of agnostic PAC learning  while increasing
the sample complexity by a factor of log(1/δ).

4

where every Ci is a 3CNF clause. Deﬁne the value  Val(φ)  of φ as the maximal fraction of clauses
that can be simultaneously satisﬁed. If Val(φ) = 1  we say the φ is satisﬁable. By 3CNFn m we
denote the set of 3CNF formulas with n variables and m clauses.
Refuting random 3CNF formulas has been studied extensively (see e.g. a special issue of TCS
Dubios et al. [2001]). It is known that for large enough ∆ (∆ = 6 will sufﬁce) a random formula in
3CNFn ∆n is not satisﬁable with probability 1 − o(1). Moreover  for every 0 ≤  < 1
4  and a large
enough ∆ = ∆()  the value of a random formula 3CNFn ∆n is ≤ 1 −  with probability 1 − o(1).
The problem of refuting random 3CNF concerns efﬁcient algorithms that provide a proof that a
random 3CNF is not satisﬁable  or far from being satisﬁable. This can be thought of as a game
between an adversary and an algorithm. The adversary should produce a 3CNF-formula. It can
either produce a satisﬁable formula  or  produce a formula uniformly at random. The algorithm
should identify whether the produced formula is random or satisﬁable.
Formally  let ∆ : N → N and 0 ≤  < 1
4. We say that an efﬁcient algorithm  A  -refutes random
3CNF with ratio ∆ if its input is φ ∈ 3CNFn n∆(n)  its output is either “typical” or “exceptional”
and it satisﬁes:

• Soundness: If Val(φ) ≥ 1 −   then

Pr

Rand. coins of A

(A(φ) = “exceptional”) ≥ 3
4

• Completeness: For every n 

Rand. coins of A  φ∼Uni(3CNFn n∆(n))

Pr

(A(φ) = “typical”) ≥ 1 − o(1)

4 can be ampliﬁed to 1− 2−n  while efﬁciency
By a standard repetition argument  the probability of 3
is preserved. Thus  given such an (ampliﬁed) algorithm  if A(φ) = “typical”  then with conﬁdence
of 1 − 2−n we know that Val(φ) < 1 − . Since for random φ ∈ 3CNFn n∆(n)  A(φ) = “typical”
with probability 1 − o(1)  such an algorithm provides  for most 3CNF formulas a proof that their
value is less that 1 − .
Note that an algorithm that -refutes random 3CNF with ratio ∆ also (cid:48)-refutes random 3CNF with
ratio ∆ for every 0 ≤ (cid:48) ≤ . Thus  the task of refuting random 3CNF’s gets easier as  gets smaller.
Most of the research concerns the case  = 0. Here  it is not hard to see that the task is getting easier
√
as ∆ grows. The best known algorithm [Feige and Ofek  2007] 0-refutes random 3CNF with ratio
n). In Feige [2002] it was conjectured that for constant ∆ no efﬁcient algorithm can
∆(n) = Ω(
provide a proof that a random 3CNF is not satisﬁable:
Conjecture 2.1 (R3SAT hardness assumption – [Feige  2002]). For every  > 0 and for every large
enough integer ∆ > ∆0() there exists no efﬁcient algorithm that -refutes random 3CNF formulas
with ratio ∆.
In fact  for all we know  the following conjecture may be true for every 0 ≤ µ ≤ 0.5.
Conjecture 2.2 (µ-R3SAT hardness assumption). For every  > 0 and for every integer ∆ > ∆0()
there exists no efﬁcient algorithm that -refutes random 3CNF with ratio ∆ · nµ.

Note that Feige’s conjecture is equivalent to the 0-R3SAT hardness assumption.
3 Lower bounds for learning Hn 3
Theorem 3.1 (main). Let 0 ≤ µ ≤ 0.5. If the µ-R3SAT hardness assumption (conjecture 2.2) is
true  then there exists no efﬁcient learning algorithm that learns the class Hn 3 using O
examples.

(cid:16) n1+µ

(cid:17)

2

In the proof of Theorem 3.1 we rely on the validity of a conjecture  similar to conjecture 2.2 for 3-
variables majority formulas. Following an argument from [Feige  2002] (Theorem 3.2) the validity
of the conjecture on which we rely for majority formulas follows the validity of conjecture 2.2.

5

Deﬁne

∀(x1  x2  x3) ∈ {±1}3  MAJ(x1  x2  x3) := sign(x1 + x2 + x3)

An n-variables 3MAJ clause is a boolean formula of the form

C(x) = MAJ((−1)j1 xi1   (−1)j2xi2  (−1)j1xi3)  x ∈ {±1}n

An n-variables 3MAJ formula is a boolean formula of the form

φ(x) = ∧m

i=1Ci(x)

where the Ci’s are 3MAJ clauses. By 3MAJn m we denote the set of 3MAJ formulas with n variables
and m clauses.
Theorem 3.2 ([Feige  2002]). Let 0 ≤ µ ≤ 0.5. If the µ-R3SAT hardness assumption is true  then
for every  > 0 and for every large enough integer ∆ > ∆0() there exists no efﬁcient algorithm
with the following properties.

• Its input is φ ∈ 3MAJn ∆n1+µ  and its output is either “typical” or “exceptional”.
• If Val(φ) ≥ 3

4 −   then

Pr

Rand. coins of A

(A(φ) = “exceptional”) ≥ 3
4

• For every n 

Rand. coins of A  φ∼Uni(3MAJn ∆n1+µ )

Pr

(A(φ) = “typical”) ≥ 1 − o(1)

Next  we prove Theorem 3.1.
In fact  we will prove a slightly stronger result. Namely  deﬁne
the subclass Hd
n 3 =
{hw 0 | w ∈ {±1}n}. As we show  under the µ-R3SAT hardness assumption  it is impossible to
efﬁciently learn this subclass using only O

n 3 ⊂ Hn 3  of homogenous halfspaces with binary weights  given by Hd

(cid:16) n1+µ

examples.

(cid:17)

2

n 3  that uses κ n

n 3) = 1 − Val(φ).

Proof idea: We will reduce the task of refuting random 3MAJ formulas with linear number of
clauses to the task of (improperly) learning Hd
n 3 with linear number of samples. The ﬁrst step will be
to construct a transformation that associates every 3MAJ clause with two examples in Cn 3×{±1} 
and every assignment with a hypothesis in Hd
n 3. As we will show  the hypothesis corresponding to
an assignment ψ is correct on the two examples corresponding to a clause C if and only if ψ satisﬁes
C. With that interpretation at hand  every 3MAJ formula φ can be thought of as a distribution Dφ
on Cn 3 × {±1}  which is the empirical distribution induced by ψ’s clauses. It holds furthermore
that ErrDφ(Hd
Suppose now that we are given an efﬁcient learning algorithm for Hd
2 examples 
for some κ > 0. To construct an efﬁcient algorithm for refuting 3MAJ-formulas  we simply feed
0.012 examples drawn from Dφ and answer “exceptional” if the error
the learning algorithm with κ n
of the hypothesis returned by the algorithm is small. If φ is (almost) satisﬁable  the algorithm is
guaranteed to return a hypothesis with a small error. On the other hand  if φ is far from being
satisﬁable  ErrDφ(Hd
n 3) is large. If the learning algorithm is proper  then it must return a hypothesis
from Hd
n 3 and therefore it would necessarily return a hypothesis with a large error. This argument
can be used to show that  unless N P = RP   learning Hd
n 3 with a proper efﬁcient algorithm is
impossible. However  here we want to rule out improper algorithms as well.
The crux of the construction is that if φ is random  no algorithm (even improper and even inefﬁcient)
can return a hypothesis with a small error. The reason for that is that since the sample provided
to the algorithm consists of only κ n
0.012 samples  the algorithm won’t see most of ψ’s clauses 
and  consequently  the produced hypothesis h will be independent of them. Since these clauses are
random  h is likely to err on about half of them  so that ErrDφ(h) will be close to half!
To summarize we constructed an efﬁcient algorithm with the following properties: if φ is almost
satisﬁable  the algorithm will return a hypothesis with a small error  and then we will declare “ex-
ceptional”  while for random φ  the algorithm will return a hypothesis with a large error  and we will
declare “typical”.

6

Our construction crucially relies on the restriction to learning algorithm with a small sample com-
plexity. Indeed  if the learning algorithm obtains more than n1+µ examples  then it will see most
of ψ’s clauses  and therefore it might succeed in “learning” even when the source of the formula is
random. Therefore  we will declare “exceptional” even when the source is random.

(cid:16) n1+µ

(cid:17)

Proof. (of theorem 3.1) Assume by way of contradiction that the µ-R3SAT hardness assumption is
true and yet there exists an efﬁcient learning algorithm that learns the class Hn 3 using O
100  we conclude that there exists an efﬁcient algorithm L and a constant
examples. Setting  = 1
κ > 0 such that given a sample S of more than κ · n1+µ examples drawn from a distribution D on
Cn 3 × {±1}  returns a classiﬁer L(S) : Cn 3 → {±1} such that

2

• L(S) can be evaluated efﬁciently.
• W.p. ≥ 3

4 over the choice of S  ErrD(L(S)) ≤ ErrD(Hn 3) + 1
100.

100. We
Fix ∆ large enough such that ∆ > 100κ and the conclusion of Theorem 3.2 holds with  = 1
will construct an algorithm  A  contradicting Theorem 3.2. On input φ ∈ 3MAJn ∆n1+µ consisting
of the 3MAJ clauses C1  . . .   C∆n1+µ  the algorithm A proceeds as follows

1. Generate a sample S consisting of ∆n1+µ examples as follows. For every clause  Ck =
MAJ((−1)j1xi1  (−1)j2 xi2   (−1)j3xi3)  generate an example (xk  yk) ∈ Cn 3 × {±1} by
choosing b ∈ {±1} at random and letting

(cid:32) 3(cid:88)

(cid:33)
(−1)jl eil   1

(xk  yk) = b ·

∈ Cn 3 × {±1} .

l=1

For example  if n = 6  the clause is MAJ(−x2  x3  x6) and b = −1  we generate the
example

((0  1 −1  0  0 −1) −1)

2. Choose a sample S1 consisting of ∆n1+µ

(with repetitions) examples from S.

100 ≥ κ · n1+µ examples by choosing at random

3. Let h = L(S1). If ErrS(h) ≤ 3

8  return “exceptional”. Otherwise  return “typical”.

We claim that A contradicts Theorem 3.2. Clearly  A runs in polynomial time. It remains to show
that

• If Val(φ) ≥ 3

4 − 1

100  then

Pr

Rand. coins of A

(A(φ) = “exceptional”) ≥ 3
4

• For every n 

Rand. coins of A  φ∼Uni(3MAJn ∆n1+µ )

Pr

(A(φ) = “typical”) ≥ 1 − o(1)

Assume ﬁrst that φ ∈ 3MAJn ∆n1+µ is chosen at random. Given the sample S1  the sample S2 :=
S \ S1 is a sample of |S2| i.i.d. examples which are independent from the sample S1  and hence also
from h = L(S1). Moreover  for every example (xk  yk) ∈ S2  yk is a Bernoulli random variable
2 which is independent of xk. To see that  note that an example whose instance is xk
with parameter 1
can be generated by exactly two clauses – one corresponds to yk = 1  while the other corresponds
to yk = −1 (e.g.  the instance (1 −1  0  1) can be generated from the clause MAJ(x1 −x2  x4) and
b = 1 or the clause MAJ(−x1  x2 −x4) and b = −1). Thus  given the instance xk  the probability
that yk = 1 is 1

2  independent of xk.

7

It follows that ErrS2(h) is an average of at least(cid:0)1 − 1
(cid:18)

variable. By Chernoff’s bound  with probability ≥ 1 − o(1)  ErrS2 (h) > 1
− 1
100

ErrS2(h) ≥

(cid:1) ∆n1+µ independent Bernoulli random
(cid:19)

(cid:19)
2 − 1

ErrS(h) ≥

1 − 1
100

1 − 1
100

100. Thus 

(cid:18) 1

(cid:18)

(cid:19)

3
8

100

>

2

·

And the algorithm will output “typical”.
Assume now that Val(φ) ≥ 3
100 and let ψ ∈ {±1}n be an assignment that indicates that. Let
Ψ ∈ Hn 3 be the hypothesis Ψ(x) = sign ((cid:104)ψ  x(cid:105)). It can be easily checked that Ψ(xk) = yk if and
only if ψ satisﬁes Ck. Since Val(φ) ≥ 3

4 − 1

100  it follows that

4 − 1
ErrS(Ψ) ≤ 1
4

+

1
100

.

Thus 

ErrS(Hn 3) ≤ 1
4

+

1
100

.

+

+

1
100

1
100

4 
4 = 3

By the choice of L  with probability ≥ 1 − 1
ErrS(h) ≤ 1
4
and the algorithm will return “exceptional”.
4 Upper bounds for learning Hn 2 and Hn 3
The following theorem derives upper bounds for learning Hn 2 and Hn 3. Its proof relies on results
from Hazan et al. [2012] about learning β-decomposable matrices  and due to the lack of space is
given in the appendix.
Theorem 4.1.

3
8

<

• There exists an efﬁcient algorithm that learns Hn 2 using O
• There exists an efﬁcient algorithm that learns Hn 3 using O

5 Discussion

(cid:17)
(cid:16) n log3(n)
(cid:17)
(cid:16) n2 log3(n)

2

2

examples

examples

We formally established a computational-sample complexity tradeoff for the task of (agnostically
and improperly) PAC learning of halfspaces over 3-sparse vectors. Our proof of the lower bound
relies on a novel  non cryptographic  technique for establishing such tradeoffs. We also derive a new
non-trivial upper bound for this task.
Open questions. An obvious open question is to close the gap between the lower and upper bounds.
We conjecture that Hn 3 can be learnt efﬁciently using a sample of ˜O
examples. Also  we
believe that our new proof technique can be used for establishing computational-sample complexity
tradeoffs for other natural learning problems.

(cid:16) n1.5

(cid:17)

2

Acknowledgements: Amit Daniely is a recipient of the Google Europe Fellowship in Learning
Theory  and this research is supported in part by this Google Fellowship. Nati Linial is supported
by grants from ISF  BSF and I-Core. Shai Shalev-Shwartz is supported by the Israeli Science Foun-
dation grant number 590-10.

References
Benny Applebaum  Boaz Barak  and David Xiao. On basing lower-bounds for learning on worst-
case assumptions. In Foundations of Computer Science  2008. FOCS’08. IEEE 49th Annual IEEE
Symposium on  pages 211–220. IEEE  2008.

8

Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal

component detection. In COLT  2013.

Nicolo Cesa-Bianchi  Alex Conconi  and Claudio Gentile. On the generalization ability of on-line

learning algorithms. IEEE Transactions on Information Theory  50:2050–2057  2001.

Venkat Chandrasekaran and Michael I. Jordan. Computational and statistical tradeoffs via convex

relaxation. Proceedings of the National Academy of Sciences  2013.

S. Decatur  O. Goldreich  and D. Ron. Computational sample complexity. SIAM Journal on Com-

puting  29  1998.

O. Dubios  R. Monasson  B. Selma  and R. Zecchina (Guest Editors). Phase Transitions in Combi-

natorial Problems. Theoretical Computer Science  Volume 265  Numbers 1-2  2001.
U. Feige. Relations between average case complexity and approximation complexity.

In STOC 

pages 534–543  2002.

Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. Theory of

Computing  3(1):25–43  2007.

E. Hazan  S. Kale  and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. In

COLT  2012.

P. Long. and R. Servedio. Low-weight halfspaces for sparse boolean vectors. In ITCS  2013.
R. Servedio. Computational sample complexity and attribute-efﬁcient learning. J. of Comput. Syst.

Sci.  60(1):161–178  2000.

Shai Shalev-Shwartz  Ohad Shamir  and Eran Tromer. Using more data to speed-up training time.

In AISTATS  2012.

V.N. Vapnik. The Nature of Statistical Learning Theory. Springer  1995.

9

,Amit Daniely
Nati Linial
Shai Shalev-Shwartz
Omar Besbes
Yonatan Gur
Assaf Zeevi