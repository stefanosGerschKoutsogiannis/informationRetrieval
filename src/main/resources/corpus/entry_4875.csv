2017,Scalable Levy Process Priors for Spectral Kernel Learning,Gaussian processes are rich distributions over functions  with generalization properties determined by a kernel function. When used for long-range extrapolation  predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a Levy process. The resulting distribution has support for all stationary covariances---including the popular RBF  periodic  and Matern kernels---combined with inductive biases which enable automatic and data efficient learning  long-range extrapolation  and state of the art predictive performance. The proposed model also presents an approach to spectral regularization  as the Levy process introduces a sparsity-inducing prior over mixture components  allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for O(n) training and O(1) predictions. We perform extrapolations having reasonable uncertainty estimates on several benchmarks  show that the proposed model can recover flexible ground truth covariances and that it is robust to errors in initialization.,Scalable L´evy Process Priors for Spectral Kernel

Learning

Phillip A. Jang Andrew E. Loeb Matthew B. Davidow Andrew Gordon Wilson

Cornell University

Abstract

Gaussian processes are rich distributions over functions  with generalization prop-
erties determined by a kernel function. When used for long-range extrapolation 
predictions are particularly sensitive to the choice of kernel parameters.
It is
therefore critical to account for kernel uncertainty in our predictive distributions.
We propose a distribution over kernels formed by modelling a spectral mixture
density with a L´evy process. The resulting distribution has support for all sta-
tionary covariances—including the popular RBF  periodic  and Mat´ern kernels—
combined with inductive biases which enable automatic and data efﬁcient learn-
ing  long-range extrapolation  and state of the art predictive performance. The
proposed model also presents an approach to spectral regularization  as the L´evy
process introduces a sparsity-inducing prior over mixture components  allowing
automatic selection over model order and pruning of extraneous components. We
exploit the algebraic structure of the proposed process for O(n) training and O(1)
predictions. We perform extrapolations having reasonable uncertainty estimates
on several benchmarks  show that the proposed model can recover ﬂexible ground
truth covariances and that it is robust to errors in initialization.

Introduction

1
Gaussian processes (GPs) naturally give rise to a function space view of modelling  whereby we
place a prior distribution over functions  and reason about the properties of likely functions under
this prior (Rasmussen & Williams  2006). Given data  we then infer a posterior distribution over
functions to make predictions. The generalisation behavior of the Gaussian process is determined
by its prior support (which functions are a priori possible) and its inductive biases (which functions
are a priori likely)  which are in turn encoded by a kernel function. However  popular kernels 
and even multiple kernel learning procedures  typically cannot extract highly expressive hidden
representations  as was envisaged for neural networks (MacKay  1998; Wilson  2014).
To discover such representations  recent approaches have advocated building more expressive ker-
nel functions. For instance  spectral mixture kernels (Wilson & Adams  2013b) were introduced for
ﬂexible kernel learning and extrapolation  by modelling a spectral density with a scale-location mix-
ture of Gaussians  with promising results. However  Wilson & Adams (2013b) specify the number of
mixture components by hand  and do not characterize uncertainty over the mixture hyperparameters.
As kernel functions become increasingly expressive and parametrized  it becomes natural to also
adopt a function space view of kernel learning—to represent uncertainty over the values of the
kernel function  and to reﬂect the belief that the kernel does not have a simple form. Just as we
use Gaussian processes over functions to model data  we can apply the function space view a step
further in a hierarchical model—with a prior distribution over kernels.
In this paper  we introduce a scalable distribution over kernels by modelling a spectral density  the
Fourier transform of a kernel  with a L´evy process. We consider both scale-location mixtures of
Gaussians and Laplacians as basis functions for the L´evy process  to induce a prior over kernels that

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

gives rise to the sharply peaked spectral densities that often occur in practice—providing a powerful
inductive bias for kernel learning. Moreover  this choice of basis functions allows our kernel func-
tion  conditioned on the L´evy process  to be expressed in closed form. This prior distribution over
kernels also has support for all stationary covariances—containing  for instance  any composition
of the popular RBF  Mat´ern  rational quadratic  gamma-exponential  or spectral mixture kernels.
And unlike the spectral mixture representation in Wilson & Adams (2013b)  this proposed process
prior allows for natural automatic inference over the number of mixture components in the spectral
density model. Moreover  the priors implied by popular L´evy processes such as the gamma process
and symmetric α-stable process result in even stronger complexity penalties than (cid:96)1 regularization 
yielding sparse representations and removing mixture components which ﬁt to noise.
Conditioned on this distribution over kernels  we model data with a Gaussian process. To form a
predictive distribution  we take a Bayesian model average of GP predictive distributions over a large
set of possible kernel functions  represented by the support of our prior over kernels  weighted by
the posterior probabilities of each of these kernels. This procedure leads to a non-Gaussian heavy-
tailed predictive distribution for modelling data. We develop a reversible jump MCMC (RJ-MCMC)
scheme (Green  1995) to infer the posterior distribution over kernels  including inference over the
number of components in the L´evy process expansion. For scalability  we pursue a structured kernel
interpolation (Wilson & Nickisch  2015) approach  in our case exploiting algebraic structure in the
L´evy process expansion  for O(n) inference and O(1) predictions  compared to the standard O(n3)
and O(n2) computations for inference and predictions with Gaussian processes. Flexible distri-
butions over kernels will be especially valuable on large datasets  which often contain additional
structure to learn rich statistical representations.
The key contributions of this paper are summarized as follows:

1. The ﬁrst fully probabilistic approach to inference with spectral mixture kernels — to incor-
porate kernel uncertainty into our predictive distributions  for a more realistic coverage of
extrapolations. This feature is demonstrated in Section 5.3.

2. Spectral regularization in spectral kernel learning. The L´evy process prior acts as a sparsity-
inducing prior on mixture components  automatically pruning extraneous components.
This feature allows for automatic inference over model order  a key hyperparameter which
must be hand tuned in the original spectral mixture kernel paper.

3. Reduced dependence on a good initialization  a key practical improvement over the original

spectral mixture kernel paper.

4. A conceptually natural and interpretable function space view of kernel learning.

2 Background
We provide a review of Gaussian and L´evy processes as models for prior distributions over functions.
2.1 Gaussian Processes
A stochastic process f (x) is a Gaussian process (GP) if for any ﬁnite collection of inputs X =
{x1 ···   xn} ⊂ RD  the vector of function values [f (x1) ···   f (xn)]T is jointly Gaussian.
The distribution of a GP is completely determined by its mean function m(x)  and covariance
kernel k(x  x(cid:48)). A GP used to specify a distribution over functions is denoted as f (x) ∼
GP(m(x)  k(x  x(cid:48)))  where E[f (xi)] = m(xi) and cov(f (x)  f (x(cid:48))) = k(x  x(cid:48)). The general-
ization properties of the GP are encoded by the covariance kernel and its hyperparameters.
By exploiting properties of joint Gaussian variables  we can obtain closed form expressions for
conditional mean and covariance functions of unobserved function values given observed function
values. Given that f (x) is observed at n training inputs X with values f = [f (x1) ···   f (xn)]T  
the predictive distribution of the unobserved function values f∗ at n∗ testing inputs X∗ is given by
(1)
(2)
(3)

f∗|X∗  X  θ ∼ N (¯f∗  cov(f∗)) 
¯f∗ = mX∗ + KX∗ X K−1

cov(f∗) = KX∗ X∗ − KX∗ X K−1

X X (f − mX ) 
X X KX X∗ .

where KX∗ X for example denotes the n∗ × n matrix of covariances evaluated at X∗ and X.

2

The popular radial basis function (RBF) kernel has the following form:
kRBF(x  x(cid:48)) = exp(−0.5(cid:107)x − x(cid:48)(cid:107)2 /(cid:96)2).

(4)

GPs with RBF kernels are limited in their expressiveness and act primarily as smoothing interpo-
lators  because the only covariance structure they can learn from data is the length scale (cid:96)  which
determines how quickly covariance decays with distance.
Wilson & Adams (2013b) introduce the more expressive spectral mixture (SM) kernel capable of ex-
tracting more complex covariance structures than the RBF kernel  formed by placing a scale-location
mixture of Gaussians in the spectrum of the covariance kernel. The RBF kernel in comparison can
only model a single Gaussian centered at the origin in frequency (spectral) space.
2.2 L´evy Processes
A stochastic process {L(ω)}ω∈R+ is a L´evy process if it has stationary  independent increments and
it is continuous in probability. In other words  L must satisfy

1. L(0) = 0 
2. L(ω0)  L(ω1) − L(ω0) ···   L(ωn) − L(ωn−1) are independent ∀ω0 ≤ ω1 ≤ ··· ≤ ωn 
3. L(ω2) − L(ω1) d= L(ω2 − ω1) ∀ω2 ≥ ω1 
4.

P(|L(ω + h) − L(ω)| ≥ ε) = 0 ∀ε > 0 ∀ω ≥ 0.

lim
h→0

By the L´evy-Khintchine representation  the dis-
tribution of a (pure jump) L´evy process is com-
pletely determined by its L´evy measure. That
is  the characteristic function of L(ω) is given
by:
log E[eiuL(ω)] =

(cid:0)eiu·β − 1 − iu · β1|β|≤1

(cid:1) ν(dβ).

(cid:90)

ω

Rd\{0}

where the L´evy measure ν(dβ) is any σ-ﬁnite
measure which satisﬁes the following integra-
bility condition

(cid:90)

(1 ∧ β2)ν(dβ) < ∞.

Rd\{0}

Figure 1: Annotated realization of a compound
Poisson process  a special case of a L´evy process.
The ωj represent jump locations  and βj represent
jump magnitudes.

A L´evy process can be viewed as a combination of a Brownian motion with drift and a superposition
of independent Poisson processes with differing jump sizes β. The L´evy measure ν(dβ) determines
the expected number of Poisson events per unit of time for any particular jump size β. The Brow-
nian component of a L´evy process will not be considered for this model. For higher dimension
input spaces ω ∈ Ω  one deﬁnes the more general notion of L´evy random measure  which is also
characterized by its L´evy measure ν(dβdω) (Wolpert et al.  2011) . We will show that the sample
realizations of L´evy processes can be used to draw sample parameters for adaptive basis expansions.
2.3 L´evy Process Priors over Adaptive Expansions
Suppose we wish
over

adaptive

class

prior
.

expansions:
Through a simple manipulation  we can rewrite

the

f : X → R(cid:12)(cid:12)(cid:12) f (x) =(cid:80)J
(cid:110)

a
j=1 βjφ(x  ωj)
f (x) into the form of a stochastic integral:

specify

of

to

(cid:111)
(cid:90)

J(cid:88)

J(cid:88)

(cid:90)

Ω

J(cid:88)
(cid:124)

j=1

(cid:123)(cid:122)

=dL(ω)

(cid:125)

f (x) =

βjφ(x  ωj) =

βj

φ(x  ω)δωj (ω)dω =

φ(x  ω)

βjδωj (ω)dω

.

j=1

j=1

Ω

Hence  by specifying a prior for the measure L(ω)  we can simultaneously specify a prior for all
of the parameters {J  (β1  ω1)  ...  (βJ   ωJ )} of the expansion. L´evy random measures provide a

3

0246810x-5051015f(x)β1β2β3ω1ω2ω3family of priors naturally suited for this purpose  as there is a one-to-one correspondence between
the jump behavior of the L´evy prior and the components of the expansion.
To illustrate this point  suppose the basis function parameters ωj are one-dimensional and consider
the integral of dL(ω) from 0 to ω.

(cid:90) ω
We see in Figure 1 that(cid:80)J

L(ω) =

0

(cid:90) ω

J(cid:88)

0

j=1

J(cid:88)

j=1

dL(ξ) =

βjδωj (ξ)dξ =

βj1[0 ω](ωj).

j=1 βj1[0 ω](ωj) resembles the sample path of a compound Poisson pro-
cess  with the number of jumps J  jump sizes βj  and jump locations ωj corresponding to the number
of basis functions  basis function weights  and basis function parameters respectively. We can use a
compound Poisson process to deﬁne a prior over all such piecewise constant paths. More generally 
we can use a L´evy process to deﬁne a prior for L(ω).
Through the L´evy-Khintchine representation  the jump behavior of the prior is characterized by a
L´evy measure ν(dβdω) which controls the mean number of Poisson events in every region of the
parameter space  encoding the inductive biases of the model. As the number of parameters in this
framework is random  we use a form of trans-dimensional reversible jump Markov chain Monte
Carlo (RJ-MCMC) to sample the parameter space (Green  2003).
Popular L´evy processes such as the gamma process  symmetric gamma process  and the symmetric
α-stable process each possess desirable properties for different situations. The gamma process is
able to produce strictly positive gamma distributed βj without transforming the output space. The
symmetric gamma process can produce both positive and negative βj  and according to Wolpert et al.
(2011) can achieve nearly all the commonly used isotropic geostatistical covariance functions. The
symmetric α-stable process can produce heavy-tailed distributions for βj and is appropriate when
one might expect the basis expansion to be dominated by a few heavily weighted functions.
While one could dispense with L´evy processes and place Gaussian or Laplace priors on βj to obtain
(cid:96)2 or (cid:96)1 regularization on the expansions  respectively  a key beneﬁt particular to these L´evy process
priors are that the implied priors on the coefﬁcients yield even stronger complexity penalties than
(cid:96)1 regularization. This property encourages sparsity in the expansions and permits scalability of
our MCMC algorithm. Refer to the supplementary material for an illustration of the joint priors
on coefﬁcients  which exhibit concave contours in contrast to the convex elliptical and diamond
contours of (cid:96)2 and (cid:96)1 regularization. Furthermore  in the log posterior for the L´evy process there
is a log(J!) complexity penalty term which further encourages sparsity in the expansions. Refer to
Clyde & Wolpert (2007) for further details.
3 L´evy Distributions over Kernels
In this section  we motivate our choice of prior over kernel functions and describe how to generate
samples from this prior distribution in practice.
3.1 L´evy Kernel Processes
By Bochner’s Theorem (1959)  a continuous stationary kernel can be represented as the Fourier dual
of a spectral density:

S(s)e2πis(cid:62)τ ds  S(s) =

k(τ )e−2πis(cid:62)τ dτ.

(5)

(cid:90)

k(τ ) =

RD

(cid:90)

RD

Hence  the spectral density entirely characterizes a stationary kernel. Therefore  it can be desirable
to model the spectrum rather than the kernel  since we can then view kernel estimation through the
lens of density estimation. In order to emulate the sharp peaks that characterize frequency spectra
of natural phenomena  we model the spectral density with a location-scale mixture of Laplacian
components:

φL(s  ωj) =

e−λj|s−χj|  ωj ≡ (χj  λj) ∈ [0  fmax] × R+.

Then the full speciﬁcation of the symmetric spectral mixture is

λj
2

(cid:104) ˜S(s) + ˜S(−s)
(cid:105)

S(s) =

1
2

(6)

(7)

J(cid:88)

j=1

 

˜S(s) =

4

βjφL(s  ωj).

As Laplacian spikes have a closed form inverse Fourier transform  the spectral density S(s) repre-
sents the following kernel function:

J(cid:88)

j=1

k(τ ) =

βj

λ2
j

j + 4π2τ 2 cos(2πχjτ ).
λ2

(8)

The parameters J  βj  χj  λj can be interpreted through Eq. (8). The total number of terms to the
mixture is J  while βj is the scale of the jth frequency contribution  χj is its central frequency  and λj
governs how rapidly the term decays (a high λ results in conﬁdent  long-term periodic extrapolation).
Other basis functions can be used in place of φL to model the spectrum as well. For example  if a
Gaussian mixture is chosen  along with maximum likelihood estimation for the learning procedure 
then we obtain the spectral mixture kernel (Wilson & Adams  2013b).
As the spectral density S(s) takes the form of an adaptive expansion  we can deﬁne a L´evy prior
over all such densities and hence all corresponding kernels of the above form. For a chosen basis
function φ(s  ω) and L´evy measure ν(dβdω) we say that k(τ ) is drawn from a L´evy kernel process
(LKP)  denoted as k(τ ) ∼ LKP(φ  ν). Wolpert et al. (2011) discuss the necessary regularity
conditions for φ and ν. In summary  we propose the following hierarchical model over functions

f (x)|k(τ ) ∼ GP(0  k(τ )) 

τ = x − x(cid:48) 

k(τ ) ∼ LKP(φ  ν).

(9)

Figure 2 shows three samples from the L´evy
process speciﬁed through Eq. (7) and their cor-
responding covariance kernels. We also show
one GP realization for each of the kernel func-
tions. By placing a L´evy process prior over
spectral densities  we induce a L´evy kernel pro-
cess prior over stationary covariance functions.
3.2 Sampling L´evy Priors
We now discuss how to generate samples
from the L´evy kernel process in practice.
In
short  the kernel parameters are drawn accord-
ing to {J {(βj  ωj)}J
j=1} ∼ L´evy(ν(dβdω)) 
and then Eq. (8) is used to evaluate k ∼
LKP(φL  ν) at values of τ.
Recall from Section 2.3 that
the choice of
L´evy measure ν is completely determined by
the choice of the corresponding L´evy process
and vice versa. Though the processes men-
tioned there produce sample paths with in-
ﬁnitely many jumps (and cannot be sampled
directly)  almost all jumps are inﬁnitesimally
small  and therefore these processes can be ap-
proximated in L2 by a compound Poisson pro-
cess with a jump size distribution truncated by
ε.
Once the desired L´evy process is chosen and the truncation bound is set  the basis expansion
parameters are generated by drawing J ∼ Poisson(ν+
samples
β1 ···   βJ ∼ πβ(dβ)  and J i.i.d. samples ω1 ···   ωJ ∼ πω(dω). Refer to the supplementary
ε = νε(R × Ω) for the gamma  symmetric gamma 
material for L2 error bounds and formulas for ν+
and symmetric α-stable processes.
The form of πβ(βj) also depends on the choice of L´evy process and can be found in the supplemen-
tary material  with further details in Wolpert et al. (2011). We choose to draw χ from an uninformed
uniform prior over a reasonable range in the frequency domain  and λ from a gamma distribution 
λ ∼ Gamma(aλ  bλ). The choices for aλ  bλ  and the frequency limits are left as hyperparame-
ters  which can have their own hyperprior distributions. After drawing the 3J values that specify

Figure 2: Samples from a L´evy kernel mix-
ture prior distribution.
(top) Three spectra with
Laplace components drawn from a L´evy process
prior. (middle) The corresponding stationary co-
variance kernel functions and the prior mean with
two standard deviations of the model  as deter-
mined by 10 000 samples. (bottom) GP samples
with the respective covariance kernel functions.

ε )  and then drawing J i.i.d.

5

00.050.10.150.2Frequency024Power00.050.10.150.2Frequency00.51Power00.050.10.150.2Frequency00.511.5Power00.20.40.60.811.21.41.61.82τ-0.100.10.2K(τ)02468101214161820X-0.500.5f(X)a L´evy process realization  the corresponding covariance function can be evaluated through the an-
alytical expression for the inverse Fourier transform (e.g. Eq. (8) for Laplacian frequency mixture
components).
4 Scalable Inference
Given observed data D = {xi  yi}N
x∗ for interpolation and extrapolation. We model observations y(x) with a hierarchical model:

i=1  we wish to infer p(y(x∗)|D  x∗) over some test set of inputs

y(x)|f (x) = f (x) + ε(x) 
f (x)|k(τ ) ∼ GP(0  k(τ )) 
k(τ ) ∼ LKP(φ  ν).

(10)
(11)
(12)
Computing the posterior distributions by marginalizing over the LKP will yield a heavy-tailed non-
Gaussian process for y(x∗) = y∗ given by an inﬁnite Gaussian mixture model:

ε(x)
τ = x − x(cid:48) 

iid∼ N (0  σ2) 

p(y∗|D) =

p(y∗|k D)p(k|D)dk ≈ 1
H

p(y∗|kh)  kh ∼ p(k|D).

(13)

(cid:90)

H(cid:88)

h=1

Initialization Considerations

We compute this approximating sum using H RJ-MCMC samples (Green  2003). Each sample
draws a kernel from the posterior kh ∼ p(k|D) distribution. Each sample of kh enables us to draw a
sample from the posterior predictive distribution p(y∗|D)  from which we can estimate the predictive
mean and variance.
Although we have chosen a Gaussian observation model in Eq. (10) (conditioned on f (x))  all of the
inference procedures we have introduced here would also apply to non-Gaussian likelihoods  such
as for Poisson processes with Gaussian process intensity functions  or classiﬁcation.
The sum in Eq. (13) requires drawing kernels from the distribution p(k|D). This is a difﬁcult dis-
tribution to approximate  particularly because there is not a ﬁxed number of parameters as J varies.
We employ RJ-MCMC  which extends the capability of conventional MCMC to allow sequential
samples of different dimensions to be drawn (Green  2003). Thus  a posterior distribution is not
limited to coefﬁcients and other parameters of a ﬁxed basis expansion  but can represent a chang-
ing number of basis functions  as required by the description of L´evy processes described in the
previous section. Indeed  RJ-MCMC can be used to automatically learn the appropriate number
of basis functions in an expansion. In the case of spectral kernel learning  inferring the number of
basis functions corresponds to automatically learning the important frequency contributions to a GP
kernel  which can lead to new interpretable insights into our data.
4.1
The choice of an initialization procedure is often an important practical consideration for machine
learning tasks due to severe multimodality in a likelihood surface (Neal  1996).
In many cases 
however  we ﬁnd that spectral kernel learning with RJ-MCMC can automatically learn salient fre-
quency contributions with a simple initialization  such as a uniform covering over a broad range
of frequencies with many sharp peaks. The frequencies which are not important in describing the
data are quickly attenuated or removed within RJ-MCMC learning. Typically only a few hundred
RJ-MCMC iterations are needed to discover the salient frequencies in this way.
Wilson (2014) proposes an alternative structured approach to initialization in previous spectral ker-
nel modelling work. First  pass the (squared) data through a Fourier transform to obtain an empirical
spectral density  which can be treated as observed. Next  ﬁt the empirical spectral density using a
standard Gaussian mixture density estimation procedure  assuming a ﬁxed number of mixture com-
ponents. Then  use the learned parameters of the Gaussian mixture as an initialization of the spectral
mixture kernel hyperparameters  for Gaussian process marginal likelihood optimization. We observe
successful adaptation of this procedure to our L´evy process method  replacing the approximation
with Laplacian mixture terms and using the result to initialize RJ-MCMC.
4.2 Scalability
As with other GP based kernel methods  the computational bottleneck lies in the evaluation of
the log marginal likelihood during MCMC  which requires computing (KX X + σ2I)−1y and

6

log |KX X + σ2I| for an n × n kernel matrix KX X evaluated at the n training points X. A di-
rect approach through computing the Cholesky decomposition of the kernel matrix requires O(n3)
computations and O(n2) storage  restricting the size of training sets to O(104). Furthermore  this
computation must be performed at every iteration of RJ-MCMC  compounding standard computa-
tional constraints.
However  this bottleneck can be readily overcome through the Structured Kernel Interpolation
approach introduced in Wilson & Nickisch (2015)  which approximates the kernel matrix as
˜KX X(cid:48) = MX KZ ZM(cid:62)
X(cid:48) for an exact kernel matrix KZ Z evaluated on a much smaller set of
m (cid:28) n inducing points  and a sparse interpolation matrix MX which facilitates fast computations.
The calculation reduces to O(n + g(m)) computations and O(n + g(m)) storage. As described
in Wilson & Nickisch (2015)  we can impose Toeplitz structure on KZ Z for g(m) = m log m 
allowing our RJ-MCMC procedure to train on massive datasets.
5 Experiments
We conduct four experiments in total.
In order to motivate our model for kernel learning in
later experiments  we ﬁrst demonstrate the ability of a L´evy process to recover—through direct
regression—an observed noise-contaminated spectrum that is characteristic of sharply peaked nat-
urally occurring spectra.
In the second experiment we demonstrate the robustness of our RJ-
MCMC sampler by automatically recovering the generative frequencies of a known kernel  even
in presence of signiﬁcant noise contamination and poor initializations.
In the third experiment
we demonstrate the ability of our method to infer the spectrum of airline passenger data  to per-
form long-range extrapolations on real data  and to demonstrate the utility of accounting for un-
certainty in the kernel.
In the ﬁnal experiment we demonstrate the scalability of our method
through training the model on a 100 000 data point sound waveform. Code is available at https:
//github.com/pjang23/levy-spectral-kernel-learning.
5.1 Explicit Spectrum Modelling
We begin by applying a L´evy process di-
rectly for function modelling (known as LARK
regression)  with inference as described in
Wolpert et al. (2011)  and Laplacian basis func-
tions. We choose an out of class test function
proposed by Donoho & Johnstone (1993) that
is standard in wavelet literature. The spatially
inhomogeneous function is deﬁned to represent
spectral densities that arise in scientiﬁc and en-
gineering applications. Gaussian i.i.d. noise is
added to give a signal-to-noise ratio of 7  to be
consistent with previous studies of the test func-
tion Wolpert et al. (2011).
The noisy test function and LARK regression ﬁt are shown in Figure 3. The synthetic spectrum
is well characterized by the L´evy process  with no “false positive” basis function terms ﬁtting the
noise owing to the strong regularization properties of the L´evy prior. By contrast  GP regression
with an RBF kernel learns a length scale of 0.07 through maximum marginal likelihood training:
the Gaussian process posterior can ﬁt the sharp peaks in the test function only if it also overﬁts to
the additive noise.
The point of this experiment is to show that the L´evy process with Laplacian basis functions forms
a natural prior over spectral densities. In other words  samples from this prior will typically look
like the types of spectra that occur in practice. Thus  this process will have a powerful inductive bias
when used for kernel learning  which we explore in the next experiments.

Figure 3: L´evy process regression on a noisy test
function (black). The ﬁt (red) captures the lo-
cations and scales of each spike while ignoring
noise  but falls slightly short at its modes since
the black spikes are parameterized as (1 + |x|)−4
rather than Laplacian.

7

012345678910x01020304050f(x)Figure 4: Ground truth recovery of known fre-
quency components.
(left) The spectrum of the
Gaussian process that was used to generate the
noisy training data is shown in black. From these
noisy data and the erroneous spectral initialization
shown in dashed blue  the maximum a posteriori
estimate of the spectral density (over 1000 RJ-
MCMC steps) is shown in red. A SM kernel also
identiﬁes the salient frequencies  but with broader
support  shown in magenta. (right) Noisy training
data are shown with a scatterplot  with withheld
testing data shown in green. The learned posterior
predictive distribution (mean in black  with 95%
credible set in grey) captures the test data.

5.2 Ground Truth Recovery
We next demonstrate the ability of our method
to recover the generative frequencies of a
known kernel and its robustness to noise and
poor initializations. Data are generated from a
GP with a kernel having two spectral Laplacian
peaks  and partitioned into training and testing
sets containing 256 points each. Moreover  the
training data are contaminated with i.i.d. Gaus-
sian noise (signal-to-noise ratio of 85%).
Based on these observed training data (depicted
as black dots in Figure 4  right)  we estimate
the kernel of the Gaussian process by inferring
its spectral density (Figure 4  left) using 1000
RJ-MCMC iterations. The empirical spectrum
initialization described in section 4.1 results in
the discovery of the two generative frequencies.
Critically  we can also recover these salient fre-
quencies even with a very poor initialization  as
shown in Figure 4 (left).
For comparison  we also train a Gaussian SM
kernel  initializing based on the empirical spec-
trum. The resulting kernel spectrum (Figure 4 
magenta curve) does recover the salient frequencies  though with less conﬁdence and higher over-
head than even a poor initialization and spectral kernel learning with RJ-MCMC.
5.3 Spectral Kernel Learning for Long-Range Extrapolation
We next demonstrate the ability of our method
to perform long-range extrapolation on real
data. Figure 5 shows a time series of monthly
airline passenger data from 1949 to 1961 (Hyn-
dman  2005). The data show a long-term ris-
ing trend as well as a short term seasonal wave-
form  and an absence of white noise artifacts.
As with Wilson & Adams (2013b)  the ﬁrst 96
monthly data points are used to train the model
and the last 48 months (4 years) are withheld as
testing data  indicated in green. With an initial-
ization from the empirical spectrum and 2500
RJ-MCMC steps  the model is able to automat-
ically learn the necessary frequencies and the
shape of the spectral density to capture both the
rising trend and the seasonal waveform  allow-
ing for accurate long-range extrapolations with-
out pre-specifying the number of model compo-
nents in advance.
This experiment also demonstrates the impact of accounting for uncertainty in the kernel  as the
withheld data often appears near or crosses the upper bound of the 95% predictive bands of the SM
ﬁt  whereas our model yields wider and more conservative predictive bands that wholly capture the
test data. As the SM extrapolations are highly sensitive to the choice of parameter values  ﬁxing
the parameters of the kernel will yield overconﬁdent predictions. The L´evy process prior allows us
to account for a range of possible kernel parameters so we can achieve a more realistically broad
coverage of possible extrapolations.
Note that the L´evy process over spectral densities induces a prior over kernel functions. Figure 6
shows a side-by-side comparison of covariance function draws from the prior and posterior distribu-
tions over kernels. We see that sample covariance functions from the prior vary quite signiﬁcantly 
but are concentrated in the posterior  with movement towards the empirical covariance function.

Figure 5: Learning of Airline passenger data.
Training data is scatter plotted  with withheld test-
ing data shown in green. The learned posterior
distribution with the proposed approach (mean in
black  with 95% credible set in grey) captures the
periodicity and the rising trend in the test data.
The analogous 95% interval using a GP with a SM
kernel is illustrated in magenta.

8

00.20.4Frequency100200300400Power01020304050X-10-505f(X)Figure 6: Covariance function draws from the kernel prior (left) and posterior (right) distributions 
with the empirical covariance function shown in black. After RJ-MCMC  the covariance distribution
centers upon the correct frequencies and order of magnitude.

Figure 7: Learning of a natural sound tex-
ture. A close-up of the training interval is
displayed with the true waveform data scat-
ter plotted. The learned posterior distribu-
tion (mean in black  with 95% credible set
in grey) retains the periodicity of the signal
within the corrupted interval. Three samples
are drawn from the posterior distribution.

5.4 Scalability Demonstration
A ﬂexible and fully Bayesian approach to kernel
learning can come with some additional computa-
tional overhead. Here we demonstrate the scalability
that is achieved through the integration of SKI (Wil-
son & Nickisch  2015) with our L´evy process model.
We consider a 100 000 data point waveform  taken
from the ﬁeld of natural sound modelling (Turner 
2010). A L´evy kernel process is trained on a sound
texture sample of howling wind with the middle
10% removed. Training involved initialization from
the signal empirical covariance and 500 RJ-MCMC
samples  and took less than one hour using an In-
tel i7 3.4 GHz CPU and 8 GB of memory. Four
distinct mixture components in the model were au-
tomatically identiﬁed through the RJ-MCMC proce-
dure. The learned kernel is then used for GP inﬁlling
with 900 training points  taken by down-sampling
the training data  which is then applied to the origi-
nal 44 100 Hz natural sound ﬁle for inﬁlling.
The GP posterior distribution over the region of interest is shown in Figure 7  along with sample
realizations  which appear to capture the qualitative behavior of the waveform. This experiment
demonstrates the applicability of our proposed kernel learning method to large datasets  and shows
promise for extensions to higher dimensional data.
6 Discussion
We introduced a distribution over covariance kernel functions that is well suited for modelling quasi-
periodic data. We have shown how to place a L´evy process prior over the spectral density of a sta-
tionary kernel. The resulting hierarchical model allows the incorporation of kernel uncertainty into
the predictive distribution. Through the spectral regularization properties of L´evy process priors  we
found that our trans-dimensional sampling procedure is suitable for automatically performing infer-
ence over model order  and is robust over initialization strategies. Finally  we incorporated structured
kernel interpolation into our training and inference procedures for linear time scalability  enabling
experiments on large datasets. The key advances over conventional spectral mixture kernels are in
being able to interpretably and automatically discover the number of mixture components  and in
representing uncertainty over the kernel. Here  we considered one dimensional inputs and station-
ary processes to most clearly elucidate the key properties of L´evy kernel processes. However  one
could generalize this process to multidimensional non-stationary kernel learning by jointly infer-
ring properties of transformations over inputs alongside the kernel hyperparameters. Alternatively 
one could consider neural networks as basis functions in the L´evy process  inferring distributions
over the parameters of the network and the numbers of basis functions as a step towards automating
neural network architecture construction.

9

0.0080.0090.010.0110.0120.0130.0140.015X (Seconds)-0.4-0.200.20.4f(X)Acknowledgements. This work is supported in part by the Natural Sciences and Engineering Re-
search Council of Canada (PGS-D 502888) and the National Science Foundation DGE 1144153 and
IIS-1563887 awards.
References
Bochner  S. Lectures on Fourier Integrals.(AM-42)  volume 42. Princeton University Press  1959.

Clyde  Merlise A and Wolpert  Robert L. Nonparametric function estimation using overcomplete

dictionaries. Bayesian Statistics  8:91–114  2007.

Donoho  D. and Johnstone  J.M. Ideal spatial adaptation by wavelet shrinkage. Biometrika  81(3):

425–455  1993.

Green  P.J.

Reversible jump monte carlo computation and bayesian model determination.

Biometrika  89(4):711–732  1995.

Green  P.J. Trans-dimensional Markov chain Monte Carlo  chapter 6. Oxford University Press 

2003.

Hyndman  R.J. Time series data library. 2005. http://www-personal.buseco.monash.

edu.au/˜hyndman/TSDL/.

MacKay  David J.C. Introduction to Gaussian processes. In Bishop  Christopher M. (ed.)  Neural

Networks and Machine Learning  chapter 11  pp. 133–165. Springer-Verlag  1998.

Micchelli  Charles A  Xu  Yuesheng  and Zhang  Haizhang. Universal kernels. Journal of Machine

Learning Research  7(Dec):2651–2667  2006.

Neal  R.M. Bayesian Learning for Neural Networks. Springer Verlag  1996. ISBN 0387947248.

Rasmussen  C. E. and Williams  C. K. I. Gaussian processes for Machine Learning. The MIT Press 

2006.

Turner  R. Statistical models for natural sounds. PhD thesis  University College London  2010.

Wilson  A.G. and Adams  R.P. Gaussian process kernels for pattern discovery and extrapola-
tion supplementary material and code. 2013a. http://mlg.eng.cam.ac.uk/andrew/
smkernelsupp.pdf.

Wilson  Andrew Gordon. Covariance kernels for fast automatic pattern discovery and extrapolation

with Gaussian processes. PhD thesis  University of Cambridge  2014.

Wilson  Andrew Gordon and Adams  Ryan Prescott. Gaussian process kernels for pattern discovery

and extrapolation. International Conference on Machine Learning (ICML)  2013b.

Wilson  Andrew Gordon and Nickisch  Hannes. Kernel interpolation for scalable structured Gaus-

sian processes (KISS-GP). International Conference on Machine Learning (ICML)  2015.

Wolpert  R.L.  Clyde  M.A.  and Tu  C. Stochastic expansions using continuous dictionaries: L´evy

adaptive regression kernels. The Annals of Statistics  39(4):1916–1962  2011.

10

,Phillip Jang
Andrew Loeb
Matthew Davidow
Andrew Wilson