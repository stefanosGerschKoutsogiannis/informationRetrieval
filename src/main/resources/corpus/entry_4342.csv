2009,Potential-Based Agnostic Boosting,We prove strong noise-tolerance properties of a potential-based boosting algorithm  similar to MadaBoost (Domingo and Watanabe  2000) and SmoothBoost (Servedio  2003). Our analysis is in the agnostic framework of Kearns  Schapire and Sellie (1994)  giving polynomial-time guarantees in presence of arbitrary noise. A remarkable feature of our algorithm is that it can be implemented without reweighting examples  by randomly relabeling them instead. Our boosting theorem gives  as easy corollaries  alternative derivations of two recent non-trivial results in computational learning theory: agnostically learning decision trees (Gopalan et al  2008) and agnostically learning halfspaces (Kalai et al  2005). Experiments suggest that the algorithm performs similarly to Madaboost.,Potential-Based Agnostic Boosting

Adam Tauman Kalai
Microsoft Research

adum@microsoft.com

Varun Kanade

Harvard University

vkanade@fas.harvard.edu

Abstract

We prove strong noise-tolerance properties of a potential-based boosting algo-
rithm  similar to MadaBoost (Domingo and Watanabe  2000) and SmoothBoost
(Servedio  2003). Our analysis is in the agnostic framework of Kearns  Schapire
and Sellie (1994)  giving polynomial-time guarantees in presence of arbitrary
noise. A remarkable feature of our algorithm is that it can be implemented with-
out reweighting examples  by randomly relabeling them instead. Our boosting
theorem gives  as easy corollaries  alternative derivations of two recent nontriv-
ial results in computational learning theory: agnostically learning decision trees
(Gopalan et al  2008) and agnostically learning halfspaces (Kalai et al  2005).
Experiments suggest that the algorithm performs similarly to MadaBoost.

1 Introduction

Boosting procedures attempt to improve the accuracy of general machine learning algorithms 
through repeated executions on reweighted data. Aggressive reweighting of data may lead to poor
performance in the presence of certain types of noise [1]. This has been addressed by a number of
“robust” boosting algorithms  such as SmoothBoost [2  3] and MadaBoost [4] as well as boosting
by branching programs [5  6]. Some of these algorithms are potential-based boosters  i.e.  natu-
ral variants on AdaBoost [7]  while others are perhaps less practical but have stronger theoretical
guarantees in the presence of noise.
The present work gives a simple potential-based boosting algorithm with guarantees in the (arbi-
trary noise) agnostic learning setting [8  9]. A unique feature of our algorithm  illustrated in Figure
1  is that it does not alter the distribution on unlabeled examples but rather it alters the labels. This
enables us to prove a strong boosting theorem in which the weak learner need only succeed for one
distribution on unlabeled examples. To the best of our knowledge  earlier weak-to-strong boosting
theorems have always relied on the ability of the weak learner to succeed under arbitrary distribu-
tions. The utility of our boosting theorem is demonstrated by re-deriving two non-trivial results
in computational learning theory  namely agnostically learning decision trees [10] and agnostically
learning halfspaces [11]  which were previously solved using very different techniques.
The main contributions of this paper are  ﬁrst  giving the ﬁrst provably noise-tolerant analysis of a
potential-based boosting algorithm  and  second  giving a distribution-speciﬁc boosting theorem that
does not require the weak learner to learn over all distributions on x ∈ X. This is in contrast to recent
work by Long and Servedio  showing that convex potential boosters cannot work in the presence of
random classiﬁcation noise [12]. The present algorithm circumvents that impossibility result in two
ways. First  the algorithm has the possibility of negating the current hypothesis and hence is not
technically a standard potential-based boosting algorithm. Second  weak agnostic learning is more
challenging than weak learning with random classiﬁcation noise  in the sense that an algorithm
which is a weak-learner in the random classiﬁcation noise setting need not be a weak-learner in the
agnostic setting.
Related work. There is a substantial literature on robust boosting algorithms  including algorithms
already mentioned  MadaBoost  SmoothBoost  as well as LogitBoost [13]  BrownBoost [14]  Nad-

1

Simpliﬁed Boosting by Relabeling Procedure
Inputs: (x1  y1)  . . .   (xm  ym) ∈ X × {−1  1}  T ≥ 1  and weak learner W .
Output: classiﬁer h : X → {−1  1}.

1. Let H 0 = 0
2. For t = 1  . . .   T :

i = min{1  exp(−H t−1(xi)yi)}

• wt
• With probability wt

i = yi  otherwise pick ˜yt

i ∈ {−1  1} randomly

m)(cid:1).

iyig(xi). /* possibly take negated hypothesis */

(a) For i = 1  . . .   m:

(b) gt = W(cid:0)(x1  ˆyt
(cid:80)m

(c) ht =

i  set ˜yt
1)  . . .   (xm  ˆyt
wt

(cid:88)

argmax

g∈{gt − sign(H t−1)}

i

(d) γt = 1
m
(e) H t(x) = H t−1(x) + γtht(x)

iyiht(xi)

i=1 wt

3. Output h = sign(H T ) as hypothesis.

Figure 1: Simpliﬁed Boosting by Relabeling Procedure. Each epoch  the algorithm runs the weak
learner on relabeled data (cid:104)(xi  ˜yt
i=1. In traditional boosting  on each epoch  H t is a linear com-
bination of weak hypotheses. For our agnostic analysis  we also need to include the negated current
hypothesis  − sign(H t−1) : X → {−1  1}  as a possible weak classiﬁer. ∗In practice  to avoid
adding noise  each example would be replaced with three weighted examples: (xi  yi) with weight
i  and (xi ±1) each with weight (1 − wt
wt

i)(cid:105)m

i)/2.

aBoost [15] and others [16  17]  including extensive experimentation [18  15  19]. These are all sim-
ple boosting algorithms whose output is a weighted majority of classiﬁers. Many have been shown
to have formal boosting properties (weak to strong PAC-learning) in a noiseless setting  or partial
boosting properties in noisy settings. There has also been a line of work on boosting algorithms that
provably boost from weak to strong learners either under agnostic or random classiﬁcation noise 
using branching programs [17  20  5  21  6]. Our results are stronger than those in the recent work
of Kalai  Mansour  Verbin [6]  for two main reasons. First  we propose a simple potential-based
algorithm that can be implemented efﬁciently. Second  since we don’t change the distribution over
unlabeled examples  we can boost distribution-speciﬁc weak learners. In recent work  using a simi-
lar idea of relabeling  Kalai  Kanade and Mansour[22] proved that the class of DNFs is learnable in
a one-sided error agnostic learning model. Their algorithm is essentially a simpler form of boosting.
Experiments. Our boosting procedure is quite similar to MadaBoost. The main differences are: (1)
there is the possibility of using the negation of the current hypothesis at each step  (2) examples are
relabeled rather than reweighted  and (3) the step size is slightly different. The goal of experiments
was to understand how signiﬁcant these differences may be in practice. Preliminary experimental
results  presented in Section 5  suggest that all of these modiﬁcations are less important in practice
than theory. Hence  the present simple analysis can be viewed as a theoretical justiﬁcation for the
noise-tolerance of MadaBoost and SmoothBoost.

1.1 Preliminaries
In the agnostic setting  we consider learning with respect to a distribution over X×Y . For simplicity 
we will take X be to ﬁnite or countable and Y = {−1  1}. Formally  learning is with respect to some
class of functions  C  where each c ∈ C is a binary classiﬁer c : X → {−1  1}. There is an arbitrary
distribution µ over X and an arbitrary target function f : X → [−1  1]. Together these determine
an arbitrary joint distribution D = (cid:104)µ  f(cid:105) over X × {−1  1} where D(x  y) = µ(x) 1+yf (x)
  i.e. 
f(x) = ED[y|x]. The error and correlation1 of a classiﬁer h : X → {−1  1} with respect to D  are

2

1This quantity is typically referred to as edge in the boosting literature. However  cor(h D) = 2 edge(h D)

according to the standard notation  hence we use the notation cor.

2

(cid:18)

We will omit D when understood from context. The goal of the learning algorithm is to achieve
error (equivalently correlation) arbitrarily close to that of the best classiﬁer in C  namely 

err(C) = err(C D) = inf

c∈C err(c D);

cor(C) = cor(C D) = sup
c∈C

cor(c D)

A γ-weakly accurate classiﬁer [23] for PAC (noiseless) learning is simply one whose correlation is
at least γ (for some γ ∈ (0  1)). A different deﬁnition of weakly accurate classiﬁer is appropriate in
the agnostic setting. Namely  for some γ ∈ (0  1)  h : X → {−1  1} is said to be γ-optimal for C
(and D) if 

cor(h D) ≥ γ cor(C D)

Hence  if the labels are totally random then a weak hypothesis need not have any correlation over
random guessing. On the other hand  in a noiseless setting  where cor(C) = 1  this is equivalent
to a γ-weakly accurate hypothesis. The goal is to boost from an algorithm capable of outputting
γ-optimal hypotheses to one which outputs a nearly 1-optimal hypothesis  even for small γ.
Let D be a distribution over X × {−1  1}. Let w : X × {−1  1} → [0  1] be a weighting function.
We now deﬁne the distribution D relabeled by w  RD w. Procedurally  one can think of generating a
sample from RD w by drawing an example (x  y) from D  then with probability w(x  y)  outputting
(x  y) directly  and with probability 1 − w(x  y)  outputting (x  y(cid:48)) where y(cid:48) is uniformly random in
{−1  1}. Formally 

RD w(x  y) = D(x  y)

w(x  y) +

1 − w(x  y)

2

+ D(x −y)

(cid:19)

(cid:18)1 − w(x −y)

(cid:19)

2

Note that D and RD w have the same marginal distributions over unlabeled examples x ∈ X. Also 
observe that  for any D  w  and h : X → R 

E

(x y)∼RD w

[h(x)y] = E

(x y)∼D

[h(x)yw(x  y)]

(1)

This can be seen by the procedural interpretation above. When (x  y) is returned directly  which
happens with probability w(x  y)  we get a contribution of h(x)y  but E[h(x)y(cid:48)] = 0 for uniform
y(cid:48) ∈ {−1  1}.
It is possible to describe traditional supervised learning and active (query) learning in the same
framework. A general (m  q)-learning algorithm is given m unlabeled examples (cid:104)x1  . . .   xm(cid:105)  and
may make q label queries to a query oracle L : X → {−1  1}  and it outputs a classiﬁer h : X →
{−1  1}. The queries may be active  meaning that queries may only be made to training examples
xi  or membership queries meaning that arbitrary examples x ∈ X may be queried. The active query
setting where q = m is the standard supervised learning setting where all m labels may be queried.
One can similarly model semi-supervised learning.
Since our boosting procedure does not change the distribution over unlabeled examples  it offers
two advantages: (1) Agnostic weak learning may be deﬁned with respect to a single distribution µ
over unlabeled examples  and (2) The weak learning algorithms may be active (or use membership
queries). In particular  the agnostic weak learning hypothesis for C and µ is that for any f : X →
[−1  1]  given examples from D = (cid:104)µ  f(cid:105)  the learner will output a γ-optimal classiﬁer for C. The
advantages of this new deﬁnition are: (a) it is not with respect to every distribution on unlabeled
examples (the algorithm may only have guarantees for certain distributions)  and (b) it is more
realistic as it does not assume noiseless data. Finding such a weak learner may be quite challenging
since it has to succeed in the agnostic model (where no assumption is made on f)  however it may
be a bit easier in the sense that the learning algorithm need only handle one particular µ.
Deﬁnition 1. A learning algorithm is a (γ  0  δ) agnostic weak learner for C and µ over X if 
for any f : X → [−1  1]  with probability ≥ 1 − δ over its random input  the algorithm outputs
h : X → [−1  1] such that  if D = (cid:104)µ  f(cid:105) 

respectively deﬁned as 

err(h D) = Pr
cor(h D) = E

(x y)∼D

(x y)∼D

[h(x) (cid:54)= y]
[h(x)y] = E
x∼µ

[h(x)f(x)] = 1 − 2 err(h D)

(cid:18)

(cid:19)

sup
c∈C

E
x∼µ

[c(x)f(x)]

− 0

cor(h D) = E
x∼µ

[h(x)f(x)] ≥ γ

3

The 0 parameter typically decreases quickly with the size of training data  e.g.  O(m−1/2). To see
why it is necessary  consider a class C = {c1  c2} consisting of only two classiﬁers  and one of them
has correlation 0 and the other has minuscule positive correlation. Then  one cannot even identify
which one has better correlation to within O(m−1/2) using m examples. Note that δ can easily
made exponentially small (boosting conﬁdence) using standard techniques.
Lastly  we deﬁne sign(z) to be 1 if z ≥ 0 and −1 if z < 0.

2 Formal boosting procedure and main results

The formal boosting procedure we analyze is given in Figure 2.

AGNOSTIC BOOSTER
Inputs: (cid:104)x1  . . .   xT m+s(cid:105)  T  s ≥ 1  label oracle L : X → {−1  1}  (m  q)-learner W .
Output: classiﬁer h : X → {−1  1}.
1. Let H 0 = 0
2. Query the labels of the ﬁrst s examples to get y1 = L(x1)  . . .   ys = L(xs).
3. For t = 1  . . .   T :

a) Deﬁne wt(x  y) = −φ(cid:48)(H t−1(x)y) = min{1  exp(−H t−1(x)y)}

Deﬁne Lt : X → {−1  1} by:
i) On input x ∈ X  let y = L(x).
ii) With probability wt(x  y)  return y.
iii) Otherwise return −1 or 1 with equal probability.

b) Let gt = W ((cid:104)xs+(t−1)m+1  . . .   xs+tm(cid:105)  Lt)
c) Let

s(cid:88)
s(cid:88)

i=1

i) αt =

ii) βt =

1
s
1
s
d) If αt ≥ βt 
ht = gt;
γt = αt;
ht = − sign(H t−1);

Else 

i=1

gt(xi)wt(xi  yi)

− sign(H t−1(xi))wt(xi  yi)

γt = βt;

e) H t(x) = H t−1(x) + γtht(x)
(cid:104)(x1  y1)  . . .   (xs  ys)(cid:105)

4. Output h = sign(H τ ) where τ is chosen so as to minimize empirical error on

Figure 2: Formal Boosting by Relabeling Procedure.

γ22 log(cid:0) 1

(cid:1)  T = 29

Theorem 1. If W is a (γ  0  δ) weak learner with respect to C and µ  s = 200
γ22  
Algorithm AGNOSTIC BOOSTER (Figure 2) with probability at least 1 − 4δT outputs a hypothesis
h satisfying:

δ

cor(h D) ≥ cor(C D) − 0
γ

− 

Recall that 0 is intended to be very small  e.g.  O(m−1/2). Also note that the number of calls to
the query oracle L is s plus T times the number of calls made by the weak learner (if the weak
learner is active  then so is the boosting algorithm). We show that two recent non-trivial results 
viz. agnostically learning decision trees and agnostically learning halfspaces follow as corollaries to
Theorem 1. The two results are stated below:
Theorem 2 ([10]). Let C be the class of binary decision trees on {−1  1}n with at most t leaves 
and let U be the uniform distribution on {−1  1}n. There exists an algorithm that when given
t  n    δ > 0  and a label oracle for an arbitrary f : {−1  1}n → [−1  1]  makes q = poly(nt/(δ))
membership queries and  with probability ≥ 1 − δ  outputs h : {−1  1}n → {−1  1} such that for
Uf = (cid:104)U  f(cid:105)  err(h Uf ) ≤ err(C Uf ) + .

4

Theorem 3 ([11]). For any ﬁxed  > 0  there exists a univariate polynomial p such that the following
holds: Let n ≥ 1  C be the class of halfspaces in n dimensions  let U be the uniform distribution
on {−1  1}n  and f : {−1  1}n → [−1  1] be an arbitrary function. There exists a polynomial-
time algorithm that  when given m = p(n log(1/δ)) labeled examples from Uf = (cid:104)U  f(cid:105)  outputs a
classiﬁer h : {−1  1}n → {−1  1} such that err(h Uf ) ≤ err(C Uf ) + . (The algorithm makes no
queries.)
Note that a related theorem was shown for halfspaces over log-concave distributions over X = Rn.
The boosting approach here similarly generalizes to that case in a straightforward manner. This
illustrates how  from the point of view of designing provably efﬁcient agnostic learning algorithms 
the current boosting procedure may be useful.

3 Analysis of Boosting Algorithm

This section is devoted to the analysis of algorithm AGNOSTIC BOOSTER (see Fig 2). As is standard 
the boosting algorithm can be viewed as minimizing a convex potential function. However  the proof
is signiﬁcantly different than the analysis of AdaBoost [7]  where they simply use the fact that the
potential is an upper-bound on the error rate.
Our analysis has two parts. First  we deﬁne a conservative relabeling  such as the one we use  to
be one which never relabels/downweights examples that the booster currently misclassiﬁes. We
show that for a conservative reweighting  either the weak learner will make progress  returning a
hypothesis correlated with the relabeled distribution or − sign(H t−1) will be correlated with the
relabeled distribution.
Second  if we ﬁnd a hypothesis correlated with the relabeled distribution  then the potential on round
t will be noticeably lower than that of round t − 1. This is essentially a simple gradient descent
analysis  using a bound on the second derivative of the potential. Since the potential is between 0
and 1  it can only drop so many rounds. This implies that sign(H t) must be a near-optimal classiﬁer
for some t (though the only sure way we have of knowing which one to pick is by testing accuracy
on held-out data).
The potential function we consider  as in MadaBoost  is deﬁned by φ : R → R 

(cid:26)1 − z

e−z

if z ≤ 0
if z > 0

φ(z) =

Φ(H D) = E

Deﬁne the potential of a (real-valued) hypothesis H with respect to a distribution D over X×{−1  1}
as:

(x y)∼D

[φ(yH(x))]

(2)
Note that Φ(H 0 D) = Φ(0 D) = 1. We will show that the potential decreases every round of
the algorithm. Notice that the weights in the boosting algorithm correspond to the derivative of the
potential  because −φ(cid:48)(z) = min{1  exp(−z)} ∈ [0  1]. In other words  the weak learning step is
essentially a gradient descent step.
We next state a key fact about agnostic learning in Lemma 1.
Deﬁnition 2. Let h : X → {−1  1} be a hypothesis. Then weighting function w : X × {−1  1} →
[0  1] is called conservative for h if w(x −h(x)) = 1 for all x ∈ X.
Note that  if the hypothesis is sign(H t(x))  then a weighting function deﬁned by −φ(cid:48)(H t(x)y) is
conservative if and only if φ(cid:48)(z) = −1 for all z < 0. We ﬁrst show that relabeling according to a
conservative weighting function is good in the sense that  if h is far from optimal according to the
original distribution  then after relabeling by w it is even further from optimal.
Lemma 1. For any distribution D over X × {−1  1}  classiﬁers c  h : X → {−1  1}  and any
weighting function w : X × {−1  1} → [0  1] conservative for h 

cor(c  RD w) − cor(h  RD w) ≥ cor(c D) − cor(h D)

5

Proof. By the deﬁnition of correlation and eq. (1)  cor(c  RD w) = ED[c(x)yw(x  y)]. Hence 
cor(c  RD w) − cor(h  RD w) = cor(c D) − cor(h D) − E
Finally  consider two cases. In the ﬁrst case  when 1 − w(x  y) > 0  we have h(x)y = 1 while
c(x)y ≤ 1. The second case is 1 − w(x  y) = 0. In either case  (c(x) − h(x))y(1 − w(x  y)) ≤ 0.
Thus the above equation implies the lemma.

[(c(x) − h(x))y(1 − w(x  y))]

(x y)∼D

We will use Lemma 1 to show that the weak learner will return a useful hypothesis. The case in
which the weak learner may not return a useful hypothesis is when cor(C  RD w) = 0  when the
optimal classiﬁer on the reweighted distribution has no correlation. This can happen  but in this case
it means that either our current hypothesis is close to optimal  or h = sign(H t−1) is even worse
than random guessing  and hence we can use its negation as a weak agnostic learner.
We next explain how a γ-optimal classiﬁer on the reweighted distribution decreases the potential.
We will use the following property linear approximation of φ.
Lemma 2. For any x  δ ∈ R  |φ(x + δ) − φ(x) − φ(cid:48)(x)δ| ≤ δ2/2.

Proof. This follows from Taylor’s theorem and the fact the function φ is differentiable everywhere 
and that the left and right second derivatives exist everywhere and are bounded by 1.
Let ht : X → {−1  1} be the weak hypothesis that the algorithm ﬁnds on round t. This may
either be the hypothesis returned by the weak learner W or − sign(H t−1). The following lemma
lower bounds the decrease in potential caused by adding γtht to H t−1. We will apply the following
Lemma on each round of the algorithm to show that the potential decreases on each round  as long
as the weak hypothesis ht has non-negligible correlation and γt is suitably chosen.
Lemma 3. Consider any function H : X → R  hypothesis h : X → [−1  1]  γ ∈ R 
and distribution D over X × {−1  1}. Let D(cid:48) = RD w be the distribution D relabeled by
w(x  y) = −φ(cid:48)(yH(x)). Then 

Φ(H D) − Φ(H + γh D) ≥ γ cor(h D(cid:48)) − γ2
2

Proof. For any (x  y) ∈ X × {−1  1}  using Lemma 2 we know that:

φ(H(x)y) − φ((H(x) + γh(x))y) ≥ γh(x)y(−φ(cid:48)(H(x)y)) − γ2
2

In the step above we use the fact that h(x)2y2 ≤ 1. Taking expectation over (x  y) from D 

Φ(H D) − φ(H + γh D) ≥ E

(x y)∼D

[h(x)y(−φ(cid:48)(H(x)y))] − γ2
2

In the above we have used Eq. (1). We are done  by deﬁnition of cor(h D(cid:48)).

=

E

(x y)∼D(cid:48)

[h(x)y] − γ2
2

Using all the above lemmas  we will show that the algorithm AGNOSTIC BOOSTER returns a hy-
pothesis with correlation (or error) close to that of the best classiﬁer from C. We are now ready to
prove the main theorem.
Proof of Theorem 1. Suppose ∃c ∈ C such that cor(c D) > cor(sign(H t−1) D) + 0
applying Lemma 1 to H t−1 and setting wt(x  y) = −φ(cid:48)(H t−1(x)y)  we get that

γ +   then

cor(c  RD wt) > cor(sign(H t−1)  RD wt) + 0
γ

+ 

(3)

In this case we want to show that the algorithm successfully ﬁnds ht with cor(ht  RD wt) ≥ γ
3 .

6

Let gt be the hypothesis returned by the weak learner W . From Step 3c) in the algorithm:

s(cid:88)
γ22 log(cid:0) 1
(cid:1)  by Chernoff-Hoeffding bounds we know that αt and βt are within an

− sign(H t−1)(xi)wt(xi  yi)

g(xi)wt(xi  yi); βt =

s(cid:88)

αt =

1
s

1
s

i=1

i=1

δ

γ + 

When s = 200
20 of cor(gt  RD wt) and cor(− sign(H t−1)  RD wt) respectively with probability at least
additive γ
1−2δ. As deﬁned in Step 3d) in the algorithm  let γt = max(αt  βt). We allow the algorithm to fail
with probability 3δ at this stage  possibly caused by the weak-learner and the estimation of αt  βt.
Consider two cases: First that cor(c  RD wt) ≥ 0
2  in this case by the weak learning assumption 
2 . In the second case  if this does not hold  then cor(− sign(H t−1)  RD wt) ≥ 
cor(gt  RD wt) ≥ γ
2
using (3). Thus  even after taking into account the fact that the empirical estimates may be off from
3 and that |γt − cor(ht  RD wt)| ≤ γ
the true correlations by γ
20.
Using this and Lemma 3  we get that by setting H t = H t−1 + γtht the potential decreases by at
least γ22
29 .
When t = 0 and H 0 = 0  Φ(H 0 D) = 1. Since for any H : X → R  Φ(H D) > 0; we can
have at most T = 29
γ22 rounds. This guarantees that when the algorithm is run for T rounds  on
. For

(cid:1) the empirical estimate of the correlation of the constructed hypothesis on each

some round t the hypothesis sign(H t) will have correlation at least sup
c∈C

s = 200
round is within an additive 
round. Thus the ﬁnal hypothesis H τ which has the highest empirical correlation satisﬁes 

6 of its true correlation  allowing a further failure probability of δ each

20  we get that cor(ht  RD wt) ≥ γ

γ22 log(cid:0) 1

cor(c D) − 0
γ

− 2
3

δ

cor(H τ  D) ≥ sup
c∈C

cor(c D) − 0
γ

− 

Since there is a failure probability of at most 4δ on each round  the algorithm succeeds with proba-
bility at least 1 − 4T δ.

4 Applications

We show that recent agnostic learning analyses can be dramatically simpliﬁed using our boosting
algorithm. Both of the agnostic algorithms are distribution-speciﬁc  meaning that they only work on
one (or a family) of distributions µ over unlabeled examples.

4.1 Agnostically Learning Decision Trees

Recent work has shown how to agnostically learn polynomial-sized decision trees using member-
ship queries  by an L1 gradient-projection algorithm [10]. Here  we show that learning decision
trees is quite simple using our distribution-speciﬁc boosting theorem and the Kushilevitz-Mansour
membership query parity learning algorithm as a weak learner [24].
Lemma 4. Running the KM algorithm  using q = poly(n  t  1/0) queries  and outputting the parity
with largest magnitude of estimated Fourier coefﬁcient  is a (γ = 1/t  0) agnostic weak learner for
size-t decision trees over the uniform distribution.

The proof of this Lemma is simple using results in [24] and is given in Appendix A. Theorem 2 now
follows easily from Lemma 4 and Theorem 1.

4.2 Agnostically Learning Halfspaces

In the case of learning halfspaces  the weak learner simply ﬁnds the degree-d term  χS(x) with
|S| ≤ d  with greatest empirical correlation 1
i=1 χS(xi)yi on a data set (x1  y1)  . . .   (xm  ym).
m
The following lemma is useful in analyzing it.
Lemma 5. For any  > 0  there exists d ≥ 1 such that the following holds. Let n ≥ 1  C be the class
of halfspaces in n dimensions  let U be the uniform distribution on {−1  1}n  and f : {−1  1}n →
[−1  1] be an arbitrary function. Then there exists a set S ⊆ [n] of size |S| ≤ d = 20
such that
4
| cor(χS Uf )| ≥ (cor(C Uf ) − 0)/nd.
0

(cid:80)m

7

Using results from [25] the proofs of Lemma 5 and Theorem 3 are straightforward and are given in
Appendix B.

5 Experiments

We performed preliminary experiments with the new boosting algorithm presented here on 8 datasets
from UCI repository [26]. We converted multi-class problems into binary classiﬁcation problems
by arbitrarily grouping classes  and ran Adaboost  Madaboost and Agnostic Boost on these datasets 
using stumps as weak learners. Since stumps can accept weighted examples  we passed the exact
weighted distribution to the weak learner.
Our experiments were performed with fractional relabeling  which means the following. Rather than
keeping the label with probability wt(x  y) and making it completely random with the remaining
probability  we added both (x  y) and (x −y) with weights (1 + wt(x  y))/2 and (1 − wt(x  y))/2
respectively. Experiments with random relabeling showed that random relabeling performs much
worse than fractional relabeling.
Table 1 summarizes the ﬁnal test error on the datasets. In the case of pima and german datasets 
we observed overﬁtting and the reported test errors are the minimum test error observed for all the
algorithms. In all other cases the test error rate at the end of round 500 is reported. Only pendigits
had a test dataset  for the rest of the datasets we performed 10-fold cross validation. We also added
random classiﬁcation noise of 5%  10% and 20% to the datasets and ran the boosting algorithms on
the modiﬁed dataset.

5% noise

Dataset

sonar

10% Noise

20% Noise

ionosphere 8.6

9.1

No Added Noise
Ada Mada Agn Ada Mada Agn Ada Mada Agn Ada Mada Agn
12.4 14.8 15.3 23.9 20.6 24.0 26.5 26.3 25.1 34.2 32.7 34.5
28.2 27.8
23.7 23.0 23.6 26.1 24.9 25.7 27.6 26.4 26.7 34.3 34.5
34
pima
german
23.1 23.6 23.1 28.5 27.7 27.5 29.0 29.5 30.0 35.0 34.5 35.1
waveform 10.4 10.2 10.3 14.9 15.0 13.9 20.1 19.2 19.1 27.9 27.3 27.1
14.7 14.9 14.5 18.2 18.3 18.1 21.9 22.0 21.5 29.4 29.1 28.7
17.4 18.2 18.3 20.9 21.4 21.5 24.6 24.9 25.2 31.4 31.8 31.6
8.2 12.1 12.0 13.0 16.8 16.3 16.9 25.5 25.2 25.3
7.4

8.1 15.8 17.2 14.4 24.2 23.8 21.8 32

magic
letter

pendigits

7.3

Table 1: Final test error rates of Adaboost  Madaboost and Agnostic Boosting on 8 datasets. The
ﬁrst column reports error rates on the original datasets  and the next three report errors on datasets
with 5%  10% and 20% classiﬁcation noise added.

6 Conclusion

We show that potential-based agnostic boosting is possible in theory  and also that this may be
done without changing the distribution over unlabeled examples. We show that non-trivial agnostic
learning results  for learning decision trees and halfspaces  can be viewed as simple applications of
our boosting theorem combined with well-known weak learners. Our analysis can be viewed as a
theoretical justiﬁcation of noise tolerance properties of algorithms like Madaboost and Smoothboost.
Preliminary experiments show that the performance of our boosting algorithm is comparable to that
of Madaboost and Adaboost. A more thorough empirical evaluation of our boosting procedure using
different weak learners is part of future research.

References
[1] T. G. Dietterich. An experimental comparison of three methods for constructing ensembles of decision

trees: bagging  boosting  and randomization. Machine Learning  40(2):139–158  2000.

[2] R. Servedio. Smooth boosting and learning with malicious noise. Journal of Machine Learning Research 

4:633–648  2003.

[3] D. Gavinsky. Optimally-smooth adaptive boosting and application to agnostic learning. Journal of Ma-

chine Learning Research  4:101–117  2003.

8

[4] C. Domingo and O. Watanabe. Madaboost: A modiﬁcation of adaboost.

In Proceedings of the Thir-
teenth Annual Conference on Learning Theory  pages 180–189  San Francisco  CA  USA  2000. Morgan
Kaufmann Publishers Inc.

[5] A. Kalai and R. Servedio. Boosting in the presence of noise. In Proceedings of the 35th Annual Symposium

on Theory of Computing (STOC)  pages 196–205  2003.

[6] A. T. Kalai  Y. Mansour  and E. Verbin. On agnostic boosting and parity learning. In STOC ’08: Proceed-
ings of the 40th annual ACM symposium on Theory of computing  pages 629–638  New York  NY  USA 
2008. ACM.

[7] Y. Freund and R. Schapire. Game theory  on-line prediction and boosting. In Proceedings of the Ninth

Annual Conference on Computational Learning Theory  pages 325–332  1996.

[8] D. Haussler. Decision theoretic generalizations of the pac model for neural net and other learning appli-

cations. Inf. Comput.  100(1):78–150  1992.

[9] M. Kearns  R. Schapire  and L. Sellie. Toward Efﬁcient Agnostic Learning. Machine Learning 

17(2):115–141  1994.

[10] P. Gopalan  A. T. Kalai  and A. R. Klivans. Agnostically learning decision trees. In Proceedings of the
40th annual ACM symposium on Theory of computing  pages 527–536  New York  NY  USA  2008. ACM.
[11] A. T. Kalai  A. R. Klivans  Y. Mansour  and R. Servedio. Agnostically learning halfspaces. In Proc. 46th

IEEE Symp. on Foundations of Computer Science (FOCS’05)  2005.

[12] P. M. Long and R. A. Servedio. Random classiﬁcation noise defeats all convex potential boosters. In

ICML  pages 608–615  2008.

[13] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: a statistical view of boosting.

Annals of Statistics  28:2000  1998.

[14] Y. Freund. An adaptive version of the boost-by-majority algorithm. In Proceedings of the Twelfth Annual

Conference on Computational Learning Theory  pages 102–113  1999.

[15] M. Nakamura  H. Nomiya  and K. Uehara. Improvement of boosting algorithm by modifying the weight-

ing rule. Annals of Mathematics and Artiﬁcial Intelligence  41(1):95–109  2004.

[16] T. Bylander and L. Tate. Using validation sets to avoid overﬁtting in adaboost.

R. Goebel  editors  FLAIRS Conference  pages 544–549. AAAI Press  2006.

In G. Sutcliffe and

[17] S. Ben-David  P. M. Long  and Y. Mansour. Agnostic boosting.

In Proceedings of the 14th Annual
Conference on Computational Learning Theory  COLT 2001  volume 2111 of Lecture Notes in Artiﬁcial
Intelligence  pages 507–516. Springer  2001.

[18] R. A. McDonald  D. J. Hand  and I. A. Eckley. An empirical comparison of three boosting algorithms on
real data sets with artiﬁcial class noise. In T. Windeatt and F. Roli  editors  Multiple Classiﬁer Systems 
volume 2709 of Lecture Notes in Computer Science  pages 35–44. Springer  2003.

[19] J. K. Bradley and R. Schapire. Filterboost: Regression and classiﬁcation on large datasets. In J.C. Platt 
D. Koller  Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 185–192. MIT Press  Cambridge  MA  2008.

[20] Y. Mansour and D. McAllester. Boosting using branching programs. Journal of Computer and System

Sciences  64(1):103–112  2002.

[21] P. M. Long and R. A. Servedio. Adaptive martingale boosting. In NIPS  pages 977–984  2008.
[22] A. T. Kalai  V. Kanade  and Y. Mansour. Reliable agnostic learning. In COLT ’09: Proceedings of the

22nd annual conference on learning theory  2009.

[23] M. Kearns and L. Valiant. Cryptographic limitations on learning Boolean formulae and ﬁnite automata.

Journal of the ACM  41(1):67–95  1994.

[24] E. Kushilevitz and Y. Mansour. Learning decision trees using the Fourier spectrum. SIAM J. on Comput-

ing  22(6):1331–1348  1993.

[25] A. Klivans  R. O’Donnell  and R. Servedio. Learning intersections and thresholds of halfspaces. Journal

of Computer & System Sciences  68(4):808–840  2004.

[26] A. Asuncion and D. J. Newman. UCI Machine Learning Repository [http://www.ics.uci.edu/
˜mlearn/MLRepository.html] Irvine  CA: University of California  School of Information and
Computer Science  2007.

9

,Boris Hanin