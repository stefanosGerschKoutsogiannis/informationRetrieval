2009,Inter-domain Gaussian Processes for Sparse Inference using Inducing Features,We present a general inference framework for inter-domain Gaussian Processes (GPs)  focusing on its usefulness to build sparse GP models. The state-of-the-art sparse GP model introduced by Snelson and Ghahramani in [1] relies on finding a small  representative pseudo data set of m elements (from the same domain as the n available data elements) which is able to explain existing data well  and then uses it to perform inference. This reduces inference and model selection computation time from O(n^3) to O(m^2n)  where m << n. Inter-domain GPs can be used to find a (possibly more compact) representative set of features lying in a different domain  at the same computational cost. Being able to specify a different domain for the representative features allows to incorporate prior knowledge about relevant characteristics of data and detaches the functional form of the covariance and basis functions. We will show how previously existing models fit into this framework and will use it to develop two new sparse GP models. Tests on large  representative regression data sets suggest that significant improvement can be achieved  while retaining computational efficiency.,Inter-domain Gaussian Processes for

Sparse Inference using Inducing Features

Miguel L´azaro-Gredilla and An´ıbal R. Figueiras-Vidal

Dep. Signal Processing & Communications
Universidad Carlos III de Madrid  SPAIN

{miguel arfv}@tsc.uc3m.es

Abstract

We present a general inference framework for inter-domain Gaussian Processes
(GPs) and focus on its usefulness to build sparse GP models. The state-of-the-art
sparse GP model introduced by Snelson and Ghahramani in [1] relies on ﬁnding
a small  representative pseudo data set of m elements (from the same domain as
the n available data elements) which is able to explain existing data well  and
then uses it to perform inference. This reduces inference and model selection
computation time from O(n3) to O(m2n)  where m (cid:28) n. Inter-domain GPs can
be used to ﬁnd a (possibly more compact) representative set of features lying in a
different domain  at the same computational cost. Being able to specify a different
domain for the representative features allows to incorporate prior knowledge about
relevant characteristics of data and detaches the functional form of the covariance
and basis functions. We will show how previously existing models ﬁt into this
framework and will use it to develop two new sparse GP models. Tests on large 
representative regression data sets suggest that signiﬁcant improvement can be
achieved  while retaining computational efﬁciency.

1 Introduction and previous work

Along the past decade there has been a growing interest in the application of Gaussian Processes
(GPs) to machine learning tasks. GPs are probabilistic non-parametric Bayesian models that com-
bine a number of attractive characteristics: They achieve state-of-the-art performance on supervised
learning tasks  provide probabilistic predictions  have a simple and well-founded model selection
scheme  present no overﬁtting (since parameters are integrated out)  etc.
Unfortunately  the direct application of GPs to regression problems (with which we will be con-
cerned here) is limited due to their training time being O(n3). To overcome this limitation  several
sparse approximations have been proposed [2  3  4  5  6]. In most of them  sparsity is achieved by
projecting all available data onto a smaller subset of size m (cid:28) n (the active set)  which is selected
according to some speciﬁc criterion. This reduces computation time to O(m2n). However  active
set selection interferes with hyperparameter learning  due to its non-smooth nature (see [1  3]).
These proposals have been superseded by the Sparse Pseudo-inputs GP (SPGP) model  introduced
in [1]. In this model  the constraint that the samples of the active set (which are called pseudo-
inputs) must be selected among training data is relaxed  allowing them to lie anywhere in the input
space. This allows both pseudo-inputs and hyperparameters to be selected in a joint continuous
optimisation and increases ﬂexibility  resulting in much superior performance.
In this work we introduce Inter-Domain GPs (IDGPs) as a general tool to perform inference across
domains. This allows to remove the constraint that the pseudo-inputs must remain within the same
domain as input data. This added ﬂexibility results in an increased performance and allows to encode
prior knowledge about other domains where data can be represented more compactly.

1

2 Review of GPs for regression

We will brieﬂy state here the main deﬁnitions and results for regression with GPs. See [7] for a
comprehensive review.
Assume we are given a training set with n samples D ≡ {xj  yj}n
j=1  where each D-dimensional
input xj is associated to a scalar output yj. The regression task goal is  given a new input x∗  predict
the corresponding output y∗ based on D.
The GP regression model assumes that the outputs can be expressed as some noiseless latent function
plus independent noise  y = f(x)+ε  and then sets a zero-mean1 GP prior on f(x)  with covariance
k(x  x(cid:48))  and a zero-mean Gaussian prior on ε  with variance σ2 (the noise power hyperparameter).
The covariance function encodes prior knowledge about the smoothness of f(x). The most common
choice for it is the Automatic Relevance Determination Squared Exponential (ARD SE):

(cid:34)

D(cid:88)

k(x  x(cid:48)) = σ2

(xd − x(cid:48)
(cid:96)2
d
0 (the latent function power) and {(cid:96)d}D

−1
2

0 exp

d=1

d)2

(cid:35)

 

(1)

0  {(cid:96)d}D

d=1}. We will omit the dependence on θ for the sake of clarity.

with hyperparameters σ2
d=1 (the length-scales  deﬁning how
rapidly the covariance decays along each dimension). It is referred to as ARD SE because  when
coupled with a model selection method  non-informative input dimensions can be removed automat-
ically by growing the corresponding length-scale. The set of hyperparameters that deﬁne the GP are
θ = {σ2  σ2
If we evaluate the latent function at X = {xj}n
j=1  we obtain a set of latent variables following a
joint Gaussian distribution p(f|X) = N (f|0  Kﬀ )  where [Kﬀ ]ij = k(xi  xj). Using this model
it is possible to express the joint distribution of training and test cases and then condition on the
observed outputs to obtain the predictive distribution for any test case
f∗(Kﬀ + σ2In)−1y  σ2 + k∗∗ − k(cid:62)

(2)
where y = [y1  . . .   yn](cid:62)  kf∗ = [k(x1  x∗)  . . .   k(xn  x∗)](cid:62)  and k∗∗ = k(x∗  x∗). In is used to
denote the identity matrix of size n. The O(n3) cost of these equations arises from the inversion of
the n × n covariance matrix. Predictive distributions for additional test cases take O(n2) time each.
These costs make standard GPs impractical for large data sets.
To select hyperparameters θ  Type-II Maximum Likelihood (ML-II) is commonly used. This
amounts to selecting the hyperparameters that correspond to a (possibly local) maximum of the
log-marginal likelihood  also called log-evidence.

pGP(y∗|x∗ D) = N (y∗|k(cid:62)

f∗(Kﬀ + σ2In)−1kf∗) 

3 Inter-domain GPs

In this section we will introduce Inter-Domain GPs (IDGPs) and show how they can be used as a
framework for computationally efﬁcient inference. Then we will use this framework to express two
previous relevant models and develop two new ones.

3.1 Deﬁnition
Consider a real-valued GP f(x) with x ∈ RD and some deterministic real function g(x  z)  with
z ∈ RH. We deﬁne the following transformation:

u(z) =

f(x)g(x  z)dx.

(3)

(cid:90)

RD

There are many examples of transformations that take on this form  the Fourier transform being
one of the best known. We will discuss possible choices for g(x  z) in Section 3.3; for the moment
we will deal with the general form. Since u(z) is obtained by a linear transformation of GP f(x) 

1We follow the common approach of subtracting the sample mean from the outputs and then assume a

zero-mean model.

2

it is also a GP. This new GP may lie in a different domain of possibly different dimension. This
transformation is not invertible in general  its properties being deﬁned by g(x  z).
IDGPs arise when we jointly consider f(x) and u(z) as a single  “extended” GP. The mean and
covariance function of this extended GP are overloaded to accept arguments from both the input and
transformed domains and treat them accordingly. We refer to each version of an overloaded function
as an instance  which will accept a different type of arguments. If the distribution of the original GP
is f(x) ∼ GP(m(x)  k(x  x(cid:48)))  then it is possible to compute the remaining instances that deﬁne the
distribution of the extended GP over both domains. The transformed-domain instance of the mean
is

m(z) = E[u(z)] =

m(x)g(x  z)dx.

The inter-domain and transformed-domain instances of the covariance function are:

(cid:90)

RD

E[f(x)]g(x  z)dx =

(cid:90)
f(x(cid:48))g(x(cid:48)  z(cid:48))dx(cid:48)(cid:21)

(cid:90)

RD

RD

(cid:90)

(cid:20)
(cid:20)(cid:90)

k(x  z(cid:48)) = E[f(x)u(z(cid:48))] = E

f(x)

k(z  z(cid:48)) = E[u(z)u(z(cid:48))] = E

(cid:90)

=

(cid:90)

RD

RD

f(x)g(x  z)dx

RD

k(x  x(cid:48))g(x  z)g(x(cid:48)  z(cid:48))dxdx(cid:48).

k(x  x(cid:48))g(x(cid:48)  z(cid:48))dx(cid:48)

=

f(x(cid:48))g(x(cid:48)  z(cid:48))dx(cid:48)(cid:21)

RD

(cid:90)

RD

(4)

(5)

Mean m(·) and covariance function k(· ·) are therefore deﬁned both by the values and domains of
their arguments. This can be seen as if each argument had an additional domain indicator used to
select the instance. Apart from that  they deﬁne a regular GP  and all standard properties hold. In
particular k(a  b) = k(b  a). This approach is related to [8]  but here the latent space is deﬁned as
a transformation of the input space  and not the other way around. This allows to pre-specify the
desired input-domain covariance. The transformation is also more general: Any g(x  z) can be used.
We can sample an IDGP at n input-domain points f = [f1  f2  . . .   fn](cid:62) (with fj = f(xj)) and m
transformed-domain points u = [u1  u2  . . .   um](cid:62) (with ui = u(zi)). With the usual assumption
of f(x) being a zero mean GP and deﬁning Z = {zi}m
i=1  the joint distribution of these samples is:

(cid:18)(cid:20)f

u

(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) X  Z
(cid:19)

p

(cid:18)(cid:20)f

(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) 0 

(cid:20) Kﬀ Kfu

(cid:21)(cid:19)

fu Kuu

= N

u

K(cid:62)
[Kfu]pq = k(xp  zq) 

with [Kﬀ ]pq = k(xp  xq) 

[Kuu]pq = k(zp  zq) 

which allows to perform inference across domains. We will only be concerned with one input
domain and one transformed domain  but IDGPs can be deﬁned for any number of domains.

3.2 Sparse regression using inducing features

 

(6)

In the standard regression setting  we are asked to perform inference about the latent function f(x)
from a data set D lying in the input domain. Using IDGPs  we can use data from any domain to
perform inference in the input domain. Some latent functions might be better deﬁned by a set of
data lying in some transformed space rather than in the input space. This idea is used for sparse
inference.
Following [1] we introduce a pseudo data set  but here we place it in the transformed domain:
D = {Z  u}. The following derivation is analogous to that of SPGP. We will refer to Z as the
inducing features and u as the inducing variables. The key approximation leading to sparsity is to
set m (cid:28) n and assume that f(x) is well-described by the pseudo data set D  so that any two samples
(either from the training or test set) fp and fq with p (cid:54)= q will be independent given xp  xq and D.
With this simplifying assumption2  the prior over f can be factorised as a product of marginals:

p(fj|xj  Z  u).

(7)

p(f|X  Z  u) ≈ n(cid:89)
ditional p(f|X  Z  u) ≈ q(f|X  Z  u) = (cid:81)n

j=1

2Alternatively  (7) can be obtained by proposing a generic factorised form for the approximate con-
j=1 qj(fj|xj  Z  u) and then choosing the set of func-
joint prior

j=1 so as to minimise the Kullback-Leibler (KL) divergence from the exact

tions {qj(·)}n
KL(p(f|X  Z  u)p(u|Z)||q(f|X  Z  u)p(u|Z))  as noted in [9]  Section 2.3.6.

3

Marginals are in turn obtained from (6): p(fj|xj  Z  u) = N (fj|kjK−1
uuu  λj)  where kj is the j-th
row of Kfu and λj is the j-th element of the diagonal of matrix Λf = diag(Kf f − KfuK−1
uuKuf ).
Operator diag(·) sets all off-diagonal elements to zero  so that Λf is a diagonal matrix.
Since p(u|Z) is readily available and also Gaussian  the inducing variables can be integrated out
from (7)  yielding a new  approximate prior over f(x):
p(f|X  Z) =

p(fj|xj  Z  u)p(u|Z)du = N (f|0  KfuK−1

p(f   u|X  Z)du ≈

(cid:90) n(cid:89)

uuKuf + Λf )

(cid:90)

j=1

uu)ku∗) 

u∗(Q−1 − K−1

u∗Q−1K(cid:62)
fuΛ−1

pIDGP(y∗|x∗ D  Z) = N (y∗|k(cid:62)

Using this approximate prior  the posterior distribution for a test case is:
y y  σ2 + k∗∗ + k(cid:62)

fuΛ−1
(8)
where we have deﬁned Q = Kuu + K(cid:62)
y Kfu and Λy = Λf + σ2In. The distribution (2)
is approximated by (8) with the information available in the pseudo data set. After O(m2n) time
precomputations  predictive means and variances can be computed in O(m) and O(m2) time per
test case  respectively. This model is  in general  non-stationary  even when it is approximating a
stationary input-domain covariance and can be interpreted as a degenerate GP plus heteroscedastic
white noise.
The log-marginal likelihood (or log-evidence) of the model  explicitly including the conditioning on
kernel hyperparameters θ can be expressed as
log p(y|X  Z  θ) = −1
y y−y(cid:62)Λ−1
2
which is also computable in O(m2n) time.
Model selection will be performed by jointly optimising the evidence with respect to the hyperpa-
rameters and the inducing features. If analytical derivatives of the covariance function are available 
conjugate gradient optimisation can be used with O(m2n) cost per step.

y y+log(|Q||Λy|/|Kuu|)+n log(2π)]

[y(cid:62)Λ−1

y KfuQ−1K(cid:62)

fuΛ−1

3.3 On the choice of g(x  z)

The feature extraction function g(x  z) deﬁnes the transformed domain in which the pseudo data set
lies. According to (3)  the inducing variables can be seen as projections of the target function f(x)
on the feature extraction function over the whole input space. Therefore  each of them summarises
information about the behaviour of f(x) everywhere. The inducing features Z deﬁne the concrete set
of functions over which the target function will be projected. It is desirable that this set captures the
most signiﬁcant characteristics of the function. This can be achieved either using prior knowledge
about data to select {g(x  zi)}m
i=1 or using a very general family of functions and letting model
selection automatically choose the appropriate set.
Another way to choose g(x  z) relies on the form of the posterior. The posterior mean of a GP is
often thought of as a linear combination of “basis functions”. For full GPs and other approximations
such as [1  2  3  4  5  6]  basis functions must have the form of the input-domain covariance function.
When using IDGPs  basis functions have the form of the inter-domain instance of the covariance
function  and can therefore be adjusted by choosing g(x  z)  independently of the input-domain
covariance function.
If two feature extraction functions g(· ·) and h(· ·) can be related by g(x  z) = h(x  z)r(z) for any
function r(·)  then both yield the same sparse GP model. This property can be used to simplify the
expressions of the instances of the covariance function.
In this work we use the same functional form for every feature  i.e. our function set is {g(x  zi)}m
i=1 
but it is also possible to use sets with different functional forms for each inducing feature  i.e.
{gi(x  zi)}m
i=1 where each zi may even have a different size (dimension). In the sections below
we will discuss different possible choices for g(x  z).

3.3.1 Relation with Sparse GPs using pseudo-inputs

The sparse GP using pseudo-inputs (SPGP) was introduced in [1] and was later renamed to Fully
Independent Training Conditional (FITC) model to ﬁt in the systematic framework of [10]. Since

4

the sparse model introduced in Section 3.2 also uses a fully independent training conditional  we
will stick to the ﬁrst name to avoid possible confusion.
IDGP innovation with respect to SPGP consists in letting the pseudo data set lie in a different do-
main. If we set gSPGP(x  z) ≡ δ(x − z) where δ(·) is a Dirac delta  we force the pseudo data set to
lie in the input domain. Thus there is no longer a transformed space and the original SPGP model is
retrieved. In this setting  the inducing features of IDGP play the role of SPGP’s pseudo-inputs.

3.3.2 Relation with Sparse Multiscale GPs

Sparse Multiscale GPs (SMGPs) are presented in [11]. Seeking to generalise the SPGP model with
ARD SE covariance function  they propose to use a different set of length-scales for each basis
function. The resulting model presents a defective variance that is healed by adding heteroscedastic
white noise. SMGPs  including the variance improvement  can be derived in a principled way as
IDGPs:

(cid:35)

(xd − µd)2
d − (cid:96)2
2(c2
d)

with z =

(cid:20)µ
(cid:21)

c

gSMGP(x  z) ≡

kSMGP(x  z(cid:48)) = exp

kSMGP(z  z(cid:48)) = exp

1

d=1

(cid:112)2π(c2
(cid:81)D
(cid:34)
− D(cid:88)
(cid:34)
− D(cid:88)

d=1

d − (cid:96)2
d)
(xd − µ(cid:48)
d)2
2c(cid:48)2
(µd − µ(cid:48)
d + c(cid:48)2

2(c2

d

d=1

exp

(cid:34)
− D(cid:88)
(cid:115)
(cid:35) D(cid:89)
(cid:35) D(cid:89)

d=1

d=1

(cid:96)2
d
c(cid:48)2

d

(cid:115)

d)2
d − (cid:96)2
d)

d=1

(cid:96)2
d
d − (cid:96)2
d + c(cid:48)2
c2

d

.

(9)

(10)

(11)

With this approximation  each basis function has its own centre µ = [µ1  µ2  . . .   µd](cid:62) and its
own length-scales c = [c1  c2  . . .   cd](cid:62)  whereas global length-scales {(cid:96)d}D
d=1 are shared by all
inducing features. Equations (10) and (11) are derived from (4) and (5) using (1) and (9). The
d ∀d  which suggests that other values 
integrals deﬁning kSMGP(· ·) converge if and only if c2
even if permitted in [11]  should be avoided for the model to remain well deﬁned.

d ≥ (cid:96)2

3.3.3 Frequency Inducing Features GP

If the target function can be described more compactly in the frequency domain than in the input
domain  it can be advantageous to let the pseudo data set lie in the former domain. We will pursue
that possibility for the case where the input domain covariance is the ARD SE. We will call the
resulting sparse model Frequency Inducing Features GP (FIFGP).
Directly applying the Fourier transform is not possible because the target function is not square
integrable (it has constant power σ2
0 everywhere  so (5) does not converge). We will workaround
this by windowing the target function in the region of interest. It is possible to use a square window 
but this results in the covariance being deﬁned in terms of the complex error function  which is very
slow to evaluate. Instead  we will use a Gaussian window3. Since multiplying by a Gaussian in
the input domain is equivalent to convolving with a Gaussian in the frequency domain  we will be
working with a blurred version of the frequency space. This model is deﬁned by:

cos

ω0 +

with z = ω

gFIF(x  z) ≡

kFIF(x  z(cid:48)) = exp

kFIF(z  z(cid:48)) = exp

exp

(cid:34)

d=1

x2
d
2c2
d

(cid:35)
− D(cid:88)
(cid:32)
(cid:35)
(cid:35)(cid:32)
(cid:35)

cos

d

d + c2
x2
2(c2

dω(cid:48)2
d + (cid:96)2
d)
d + ω(cid:48)2
d )
d(ω2
c2
d + (cid:96)2
d)
2(2c2
d(ωd + ω(cid:48)
d)2
c4
d)
2(2c2
d + (cid:96)2

d

d=1

(cid:112)2πc2
1(cid:81)D
(cid:34)
− D(cid:88)
(cid:34)
− D(cid:88)
(cid:34)
− D(cid:88)

d=1

d=1

d=1

ω(cid:48)
0 +

(cid:34)

exp

(cid:16)
D(cid:88)
− D(cid:88)

d=1

d=1

D(cid:88)

d=1

(cid:17)
(cid:33) D(cid:89)

xdωd

d

d=1

dω(cid:48)
c2
dxd
d + (cid:96)2
c2
d(ωd − ω(cid:48)
d)2
c4
(cid:115)
d)
2(2c2
d + (cid:96)2

(cid:33) D(cid:89)

(cid:115)
(cid:35)

(cid:96)2
d
d + (cid:96)2

d

2c2

d=1

cos(ω0 + ω(cid:48)
0)

+ exp

(12)

(13)

(cid:96)2
d
d + (cid:96)2
c2
cos(ω0 − ω(cid:48)
0)

d

.

(14)

3A mixture of m Gaussians could also be used as window without increasing the complexity order.

5

The inducing features are ω = [ω0  ω1  . . .   ωd](cid:62)  where ω0 is the phase and the remaining com-
ponents are frequencies along each dimension. In this model  both global length-scales {(cid:96)d}D
d=1 and
window length-scales {cd}D
d = cd. Instances (13) and (14) are induced by (12)
using (4) and (5).

d=1 are shared  thus c(cid:48)

3.3.4 Time-Frequency Inducing Features GP

Instead of using a single window to select the region of interest  it is possible to use a different
window for each feature. We will use windows of the same size but different centres. The re-
sulting model combines SPGP and FIFGP  so we will call it Time-Frequency Inducing Features
GP (TFIFGP). It is deﬁned by gTFIF(x  z) ≡ gFIF(x − µ  ω)  with z = [µ(cid:62) ω(cid:62)](cid:62). The implied
inter-domain and transformed-domain instances of the covariance function are:

kTFIF(x  z(cid:48)) = kFIF(x − µ(cid:48)  ω(cid:48))  

kTFIF(z  z(cid:48)) = kFIF(z  z(cid:48)) exp

(cid:34)

− D(cid:88)

d=1

(cid:35)

(µd − µ(cid:48)
d)2
d + (cid:96)2
2(2c2
d)

FIFGP is trivially obtained by setting every centre to zero {µi = 0}m
by setting window length-scales c  frequencies and phases {ωi}m
scales were individually adjusted  SMGP would be obtained.
While FIFGP has the modelling power of both FIFGP and SPGP  it might perform worse in prac-
tice due to it having roughly twice as many hyperparameters  thus making the optimisation problem
harder. The same problem also exists in SMGP. A possible workaround is to initialise the hyperpa-
rameters using a simpler model  as done in [11] for SMGP  though we will not do this here.

i=1  whereas SPGP is obtained
i=1 to zero. If the window length-

4 Experiments

In this section we will compare the proposed approximations FIFGP and TFIFGP with the current
state of the art  SPGP on some large data sets  for the same number of inducing features/inputs
and therefore  roughly equal computational cost. Additionally  we provide results using a full GP 
which is expected to provide top performance (though requiring an impractically big amount of
computation). In all cases  the (input-domain) covariance function is the ARD SE (1).
We use four large data sets: Kin-40k  Pumadyn-32nm4 (describing the dynamics of a robot arm 
used with SPGP in [1])  Elevators and Pole Telecomm5 (related to the control of the elevators of an
F16 aircraft and a telecommunications problem  and used in [12  13  14]). Input dimensions that
remained constant throughout the training set were removed. Input data was additionally centred for
use with FIFGP (the remaining methods are translation invariant). Pole Telecomm outputs actually
take discrete values in the 0-100 range  in multiples of 10. This was taken into account by using the
corresponding quantization noise variance (102/12) as lower bound for the noise hyperparameter6.
Hyperparameters are initialised as follows: σ2
d=1 to one half of
the range spanned by training data along each dimension. For SPGP  pseudo-inputs are initialised
to a random subset of the training data  for FIFGP window size c is initialised to the standard
deviation of input data  frequencies are randomly chosen from a zero-mean (cid:96)−2
d -variance Gaussian
distribution  and phases are obtained from a uniform distribution in [0 . . . 2π). TFIFGP uses the
same initialisation as FIFGP  with window centres set to zero. Final values are selected by evidence
maximisation.
Denoting the output average over the training set as y and the predictive mean and variance for test
sample y∗l as µ∗l and σ∗l respectively  we deﬁne the following quality measures: Normalized Mean
Square Error (NMSE) (cid:104)(y∗l − µ∗l)2(cid:105)/(cid:104)(y∗l − y)2(cid:105) and Mean Negative Log-Probability (MNLP)
2(cid:104)(y∗l − µ∗l)2/σ2∗l + log σ2∗l + log 2π(cid:105)  where (cid:104)·(cid:105) averages over the test set.

0/4  {(cid:96)d}D

j   σ2 = σ2

(cid:80)n

0 = 1

n

j=1 y2

1

4Kin-40k: 8 input dimensions  10000/30000 samples for train/test  Pumadyn-32nm: 32 input dimensions 
7168/1024 samples for train/test  using exactly the same preprocessing and train/test splits as [1  3]. Note that
their error measure is actually one half of the Normalized Mean Square Error deﬁned here.

5Pole Telecomm: 26 non-constant input dimensions  10000/5000 samples for train/test. Elevators:
17 non-constant input dimensions  8752/7847 samples for train/test. Both have been downloaded from
http://www.liaad.up.pt/∼ltorgo/Regression/datasets.html

6If unconstrained  similar plots are obtained; in particular  no overﬁtting is observed.

6

For Kin-40k (Fig. 1  top)  all three sparse methods perform similarly  though for high sparseness
(the most useful case) FIFGP and TFIFGP are slightly superior. In Pumadyn-32nm (Fig. 1  bottom) 
only 4 out the 32 input dimensions are relevant to the regression task  so it can be used as an ARD
capabilities test. We follow [1] and use a full GP on a small subset of the training data (1024 data
points) to obtain the initial length-scales. This allows better minima to be found during optimisation.
Though all methods are able to properly ﬁnd a good solution  FIFGP and especially TFIFGP are
better in the sparser regime. Roughly the same considerations can be made about Pole Telecomm
and Elevators (Fig. 2)  but in these data sets the superiority of FIFGP and TFIFGP is more dramatic.
Though not shown here  we have additionally tested these models on smaller  overﬁtting-prone data
sets  and have found no noticeable overﬁtting even using m > n  despite the relatively high number
of parameters being adjusted. This is in line with the results and discussion of [1].

(a) Kin-40k NMSE (log-log plot)

(b) Kin-40k MNLP (semilog plot)

(c) Pumadyn-32nm NMSE (log-log plot)

(d) Pumadyn-32nm MNLP (semilog plot)

Figure 1: Performance of the compared methods on Kin-40k and Pumadyn-32nm.

5 Conclusions and extensions

In this work we have introduced IDGPs  which are able combine representations of a GP in differ-
ent domains  and have used them to extend SPGP to handle inducing features lying in a different
domain. This provides a general framework for sparse models  which are deﬁned by a feature extrac-
tion function. Using this framework  SMGPs can be reinterpreted as fully principled models using a
transformed space of local features  without any need for post-hoc variance improvements. Further-
more  it is possible to develop new sparse models of practical use  such as the proposed FIFGP and
TFIFGP  which are able to outperform the state-of-the-art SPGP on some large data sets  especially
for high sparsity regimes.

7

255010020030050075012500.0010.0050.010.050.10.5Inducing features / pseudo−inputsNormalized Mean Squared Error SPGPFIFGPTFIFGPFull GP on 10000 data points25501002003005007501250−1.5−1−0.500.511.522.5Inducing features / pseudo−inputsMean Negative Log−Probability SPGPFIFGPTFIFGPFull GP on 10000 data points102550751000.040.050.1Inducing features / pseudo−inputsNormalized Mean Squared Error SPGPFIFGPTFIFGPFull GP on 7168 data points10255075100−0.2−0.15−0.1−0.0500.050.10.150.2Inducing features / pseudo−inputsMean Negative Log−Probability SPGPFIFGPTFIFGPFull GP on 7168 data points(a) Elevators NMSE (log-log plot)

(b) Elevators MNLP (semilog plot)

(c) Pole Telecomm NMSE (log-log plot)

(d) Pole Telecomm MNLP (semilog plot)

Figure 2: Performance of the compared methods on Elevators and Pole Telecomm.

Choosing a transformed space for the inducing features enables to use domains where the target
function can be expressed more compactly  or where the evidence (which is a function of the fea-
tures) is easier to optimise. This added ﬂexibility translates as a detaching of the functional form of
the input-domain covariance and the set of basis functions used to express the posterior mean.
IDGPs approximate full GPs optimally in the KL sense noted in Section 3.2  for a given set of
inducing features. Using ML-II to select the inducing features means that models providing a good
ﬁt to data are given preference over models that might approximate the full GP more closely. This 
though rarely  might lead to harmful overﬁtting. To more faithfully approximate the full GP and
avoid overﬁtting altogether  our proposal can be combined with the variational approach from [15] 
in which the inducing features would be regarded as variational parameters. This would result in
more constrained models  which would be closer to the full GP but might show reduced performance.
We have explored the case of regression with Gaussian noise  which is analytically tractable  but it
is straightforward to apply the same model to other tasks such as robust regression or classiﬁcation 
using approximate inference (see [16]). Also  IDGPs as a general tool can be used for other purposes 
such as modelling noise in the frequency domain  aggregating data from different domains or even
imposing constraints on the target function.

Acknowledgments

We would like to thank the anonymous referees for helpful comments and suggestions. This work
has been partly supported by the Spanish government under grant TEC2008- 02473/TEC  and by
the Madrid Community under grant S-505/TIC/0223.

8

10255010025050075010000.10.150.20.25Inducing features / pseudo−inputsNormalized Mean Squared Error SPGPFIFGPTFIFGPFull GP on 8752 data points1025501002505007501000−4.8−4.6−4.4−4.2−4−3.8Inducing features / pseudo−inputsMean Negative Log−Probability SPGPFIFGPTFIFGPFull GP on 8752 data points10255010025050010000.010.020.030.040.050.10.150.2Inducing features / pseudo−inputsNormalized Mean Squared Error SPGPFIFGPTFIFGPFull GP on 10000 data points10255010025050010002.533.544.555.5Inducing features / pseudo−inputsMean Negative Log−Probability SPGPFIFGPTFIFGPFull GP on 10000 data pointsReferences

[1] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in Neural

Information Processing Systems 18  pages 1259–1266. MIT Press  2006.

[2] A. J. Smola and P. Bartlett. Sparse greedy Gaussian process regression. In Advances in Neural Information

Processing Systems 13  pages 619–625. MIT Press  2001.

[3] M. Seeger  C. K. I. Williams  and N. D. Lawrence. Fast forward selection to speed up sparse Gaussian

process regression. In Proceedings of the 9th International Workshop on AI Stats  2003.
[4] V. Tresp. A Bayesian committee machine. Neural Computation  12:2719–2741  2000.
[5] L. Csat´o and M. Opper. Sparse online Gaussian processes. Neural Computation  14(3):641–669  2002.
[6] C. K. I. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In Advances

in Neural Information Processing Systems 13  pages 682–688. MIT Press  2001.

[7] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. Adaptive Computa-

tion and Machine Learning. MIT Press  2006.

[8] M. Alvarez and N. D. Lawrence. Sparse convolved Gaussian processes for multi-output regression. In

Advances in Neural Information Processing Systems 21  pages 57–64  2009.

[9] Ed. Snelson. Flexible and efﬁcient Gaussian process models for machine learning. PhD thesis  University

of Cambridge  2007.

[10] J. Qui˜nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process

regression. Journal of Machine Learning Research  6:1939–1959  2005.

[11] C. Walder  K. I. Kim  and B. Sch¨olkopf. Sparse multiscale Gaussian process regression. In 25th Interna-

tional Conference on Machine Learning. ACM Press  New York  2008.

[12] G. Potgietera and A. P. Engelbrecht. Evolving model trees for mining data sets with continuous-valued

classes. Expert Systems with Applications  35:1513–1532  2007.

[13] L. Torgo and J. Pinto da Costa. Clustered partial linear regression. In Proceedings of the 11th European

Conference on Machine Learning  pages 426–436. Springer  2000.

[14] G. Potgietera and A. P. Engelbrecht. Pairwise classiﬁcation as an ensemble technique. In Proceedings of

the 13th European Conference on Machine Learning  pages 97–110. Springer-Verlag  2002.

[15] M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceedings of

the 12th International Workshop on AI Stats  2009.

[16] A. Naish-Guzman and S. Holden. The generalized FITC approximation. In Advances in Neural Informa-

tion Processing Systems 20  pages 1057–1064. MIT Press  2008.

9

,Remi Lam
Karen Willcox
David Wolpert
Hadrien Hendrikx
Francis Bach
Laurent Massoulié