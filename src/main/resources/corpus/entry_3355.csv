2009,Matrix Completion from Noisy Entries,Given a matrix M of low-rank  we consider the problem of reconstructing it from noisy observations of a small  random subset of its entries. The problem arises in a variety of applications  from collaborative filtering (the ‘Netflix problem’) to structure-from-motion and positioning. We study a low complexity algorithm introduced in [1]  based on a combination of spectral techniques and manifold optimization  that we call here OPTSPACE. We prove performance guarantees that are order-optimal in a number of circumstances.,Matrix Completion from Noisy Entries

Raghunandan H. Keshavan∗  Andrea Montanari∗†  and Sewoong Oh∗

Abstract

Given a matrix M of low-rank  we consider the problem of reconstructing it from
noisy observations of a small  random subset of its entries. The problem arises
in a variety of applications  from collaborative ﬁltering (the ‘Netﬂix problem’)
to structure-from-motion and positioning. We study a low complexity algorithm
introduced in [1]  based on a combination of spectral techniques and manifold
optimization  that we call here OPTSPACE. We prove performance guarantees
that are order-optimal in a number of circumstances.

1 Introduction

Spectral techniques are an authentic workhorse in machine learning  statistics  numerical analysis 
and signal processing. Given a matrix M  its largest singular values –and the associated singular
vectors– ‘explain’ the most signiﬁcant correlations in the underlying data source. A low-rank ap-
proximation of M can further be used for low-complexity implementations of a number of linear
algebra algorithms [2].
In many practical circumstances we have access only to a sparse subset of the entries of an m × n
matrix M. It has recently been discovered that  if the matrix M has rank r  and unless it is too
‘structured’  a small random subset of its entries allow to reconstruct it exactly. This result was ﬁrst
proved by Cand´es and Recht [3] by analyzing a convex relaxation indroduced by Fazel [4]. A tighter
analysis of the same convex relaxation was carried out in [5]. A number of iterative schemes to solve
the convex optimization problem appeared soon thereafter [6  7  8] (also see [9] for a generalization).

In an alternative line of work  the authors of [1] attacked the same problem using a combination
of spectral techniques and manifold optimization: we will refer to their algorithm as OPTSPACE.
OPTSPACE is intrinsically of low complexity  the most complex operation being computing r sin-
gular values and the corresponding singular vectors of a sparse m × n matrix. The performance
roughly
guarantees proved in [1] are comparable with the information theoretic lower bound:
nr max{r  log n} random entries are needed to reconstruct M exactly (here we assume m of or-
der n). A related approach was also developed in [10]  although without performance guarantees for
matrix completion.

The above results crucially rely on the assumption that M is exactly a rank r matrix. For many
applications of interest  this assumption is unrealistic and it is therefore important to investigate
their robustness. Can the above approaches be generalized when the underlying data is ‘well ap-
proximated’ by a rank r matrix? This question was addressed in [11] within the convex relaxation
approach of [3]. The present paper proves a similar robustness result for OPTSPACE. Remark-
ably the guarantees we obtain are order-optimal in a variety of circumstances  and improve over the
analogus results of [11].

∗Department of Electrical Engineering  Stanford University
†Departments of Statistics  Stanford University

1

1.1 Model deﬁnition

Let M be an m × n matrix of rank r  that is

(1)
where U has dimensions m × r  V has dimensions n × r  and Σ is a diagonal r × r matrix. We
assume that each entry of M is perturbed  thus producing an ‘approximately’ low-rank matrix N 
with

M = U ΣV T .

Nij = Mij + Zij  

(2)

where the matrix Z will be assumed to be ‘small’ in an appropriate sense.
Out of the m × n entries of N  a subset E ⊆ [m] × [n] is revealed. We let N E be the m × n matrix
that contains the revealed entries of N  and is ﬁlled with 0’s in the other positions

(3)

ij =(cid:26) Nij

N E

if (i  j) ∈ E  

0 otherwise.

The set E will be uniformly random given its size |E|.
1.2 Algorithm

For the reader’s convenience  we recall the algorithm introduced in [1]  which we will analyze here.
The basic idea is to minimize the cost function F (X  Y )  deﬁned by

F (X  Y ) ≡ min
S∈Rr×r F(X  Y  S)  
2 X(i j)∈E
1

F(X  Y  S) ≡

(Nij − (XSY T )ij)2 .

(4)

(5)

Here X ∈ Rn×r  Y ∈ Rm×r are orthogonal matrices  normalized by X T X = m1  Y T Y = n1.
Minimizing F (X  Y ) is an a priori difﬁcult task  since F is a non-convex function. The key insight
is that the singular value decomposition (SVD) of N E provides an excellent initial guess  and that the
minimum can be found with high probability by standard gradient descent after this initialization.
Two caveats must be added to this decription: (1) In general the matrix N E must be ‘trimmed’
to eliminate over-represented rows and columns; (2) For technical reasons  we consider a slightly

0 ;

OPTSPACE( matrix N E )

modiﬁed cost function to be denoted by eF (X  Y ).
1: Trim N E  and let eN E be the output;
2: Compute the rank-r projection of eN E  Tr(eN E) = X0S0Y T
3: Minimize eF (X  Y ) through gradient descent  with initial condition (X0  Y0).
We may note here that the rank of the matrix M  if not known  can be reliably estimated from eN E.
The various steps of the above algorithm are deﬁned as follows.
Trimming. We say that a row is ‘over-represented’ if it contains more than 2|E|/m revealed entries
(i.e. more than twice the average number of revealed entries). Analogously  a column is over-
represented if it contains more than 2|E|/n revealed entries. The trimmed matrix eN E is obtained
from N E by setting to 0 over-represented rows and columns. fM E and eZ E are deﬁned similarly.
Hence  eN E = fM E + eZ E.

We refer to the journal version of this paper for further details.

Rank-r projection. Let

min(m n)Xi=1

eN E =

σixiyT
i  

2

(6)

be the singular value decomposition of eN E  with singular vectors σ1 ≥ σ2 ≥ . . . . We then deﬁne

(7)

Tr(eN E) =

mn
|E|

rXi=1

σixiyT
i .

norm.

Apart from an overall normalization  Tr(eN E) is the best rank-r approximation to eN E in Frobenius
Minimization. The modiﬁed cost function eF is deﬁned as
G1(cid:18)||X (i)||2

eF (X  Y ) = F (X  Y ) + ρ G(X  Y )

G1(cid:18)||Y (j)||2
3µ0r (cid:19)  

3µ0r (cid:19) + ρ

≡ F (X  Y ) + ρ

(8)

(9)

mXi=1

nXj=1

where X (i) denotes the i-th row of X  and Y (j) the j-th row of Y . See Section 1.3 below for the
deﬁnition of µ0. The function G1 : R+ → R is such that G1(z) = 0 if z ≤ 1 and G1(z) =
e(z−1)2
Let us stress that the regularization term is mainly introduced for our proof technique to work (and
a broad family of functions G1 would work as well). In numerical experiments we did not ﬁnd any
performance loss in setting ρ = 0.

− 1 otherwise. Further  we can choose ρ = Θ(nǫ).

the r-dimensional subspaces of Rm and Rn generated (respectively) by the columns of X and Y .
This interpretation is justiﬁed by the fact that F (X  Y ) = F (XA  Y B) for any two orthogonal

One important feature of OPTSPACE is that F (X  Y ) and eF (X  Y ) are regarded as functions of
matrices A  B ∈ Rr×r (the same property holds for eF ). The set of r dimensional subspaces of Rm
algorithm is applied to the function eF : M(m  n) ≡ G(m  r) × G(n  r) → R. For further details on

is a differentiable Riemannian manifold G(m  r) (the Grassman manifold). The gradient descent

optimization by gradient descent on matrix manifolds we refer to [12  13].

1.3 Main results

Our ﬁrst result shows that  in great generality  the rank-r projection of eN E provides a reasonable
approximation of M. Throughout this paper  without loss of generality  we assume α ≡ m/n ≥ 1.
Theorem 1.1. Let N = M + Z  where M has rank r and |Mij| ≤ Mmax for all (i  j) ∈ [m] × [n] 
and assume that the subset of revealed entries E ⊆ [m] × [n] is uniformly random with size |E|.
Then there exists numerical constants C and C ′ such that

1

|E| (cid:19)1/2
√mn||M − Tr(eN E)||F ≤ CMmax (cid:18) nrα3/2

+ C ′ n√rα
|E|

||eZ E||2  

(10)

with probability larger than 1 − 1/n3.
Projection onto rank-r matrices through SVD is pretty standard (although trimming is crucial for
achieving the above guarantee). The key point here is that a much better approximation is obtained

by minimizing the cost eF (X  Y ) (step 3 in the pseudocode above)  provided M satisﬁes an appro-

priate incoherence condition. Let M = U ΣV T be a low rank matrix  and assume  without loss of
generality  U T U = m1 and V T V = n1. We say that M is (µ0  µ1)-incoherent if the following
conditions hold.

A1. For all i ∈ [m]  j ∈ [n] we have Pr
A2. There exists µ1 such that |Pr

ik ≤ µ0r Pr

k=1 V 2
k=1 Uik(Σk/Σ1)Vjk| ≤ µ1r1/2.

k=1 U 2

ik ≤ µ0r.

Theorem 1.2. Let N = M + Z  where M is a (µ0  µ1)-incoherent matrix of rank r  and assume
that the subset of revealed entries E ⊆ [m] × [n] is uniformly random with size |E|. Further  let
Σmin = Σ1 ≤ ··· ≤ Σr = Σmax with Σmax/Σmin ≡ κ. Let cM be the output of OPTSPACE on
input N E. Then there exists numerical constants C and C ′ such that if

|E| ≥ Cn√ακ2 max(cid:8)µ0r√α log n ; µ2

0r2ακ4 ; µ2

1r2ακ4(cid:9)  

(11)

3

then  with probability at least 1 − 1/n3 

1

√mn ||cM − M||F ≤ C ′ κ2 n√αr

|E|

provided that the right-hand side is smaller than Σmin.

||Z E||2 .

(12)

Apart from capturing the effect of additive noise  these two theorems improve over the work of [1]
even in the noiseless case. Indeed they provide quantitative bounds in ﬁnite dimensions  while the
results of [1] were only asymptotic.

1.4 Noise models

In order to make sense of the above results  it is convenient to consider a couple of simple models
for the noise matrix Z:

Independent entries model. We assume that Z’s entries are independent random variables  with zero
mean E{Zij} = 0 and sub-gaussian tails. The latter means that

P{|Zij| ≥ x} ≤ 2 e− x

2σ

2

2  

(13)

for some bounded constant σ2.
Worst case model. In this model Z is arbitrary  but we have an uniform bound on the size of its
entries: |Zij| ≤ Zmax.
The basic parameter entering our main results is the operator norm of eZ E  which is bounded as

follows.
Theorem 1.3. If Z is a random matrix drawn according to the independent entries model  then
there is a constant C such that 

||eZ E||2 ≤ Cσ(cid:18)√α|E| log |E|

n

with probability at least 1 − 1/n3.
If Z is a matrix from the worst case model  then

for any realization of E.

2|E|
n√α

Zmax  

||eZ E||2 ≤

(cid:19)1/2

 

(14)

(15)

Note that for |E| = Ω(n log n)   no row or column is over-represented with high probability. It
follows that in the regime of |E| for which the conditions of Theorem 1.2 are satisﬁed  we have
Z E = eZ E. Then  among the other things  this result implies that for the independent entries model
the right-hand side of our error estimate  Eq. (12)  is with high probability smaller than Σmin  if
|E| ≥ Crα3/2n log n κ4(σ/Σmin)2. For the worst case model  the same statement is true if Zmax ≤
Σmin/C√rκ2.
Due to space constraints  the proof of Theorem 1.3 will be given in the journal version of this paper.

1.5 Comparison with related work

Let us begin by mentioning that a statement analogous to our preliminary Theorem 1.1 was proved
in [14]. Our result however applies to any number of revealed entries  while the one of [14] requires
|E| ≥ (8 log n)4n (which for n ≤ 5 · 108 is larger than n2).
As for Theorem 1.2  we will mainly compare our algorithm with the convex relaxation approach
recently analyzed in [11]. Our basic setting is indeed the same  while the algorithms are rather
different.

4

Convex Relaxation
Lower Bound
rank-r projection
OptSpace : 1 iteration
2 iterations
3 iterations
10 iterations

 1

 0.8

 0.6

 0.4

 0.2

E
S
M
R

 0

 0

 100

 200

 400

 500

 600

 300
|E|/n

Figure 1: Root mean square error achieved by OPTSPACE for reconstructing a random rank-2 matrix  as a
function of the number of observed entries |E|  and of the number of line minimizations. The performance of
nuclear norm minimization and an information theory lower bound are also shown.

Figure 1 compares the average root mean square error for the two algorithms as a function of |E|.
Here M is a random rank r = 2 matrix of dimension m = n = 600  generated by letting M = eUeV T
with eUij eVij i.i.d. N (0  20/√n). The noise is distributed according to the independent entries
model with Zij ∼ N (0  1). This example is taken from [11] Figure 2  from which we took the
data for the convex relaxation approach  as well as the information theory lower bound. After one
iteration  OPTSPACE has a smaller root mean square error than [11]  and in about 10 iterations it
becomes indistiguishable from the information theory lower bound.

Next let us compare our main result with the performance guarantee in [11]  Theorem 7. Let us
stress that we require some bound on the condition number κ  while the analysis of [11  5] requires
a stronger incoherence assumption. As far as the error bound is concerned  [11] proved

1

√mn ||cM − M||F ≤ 7r n

|E| ||Z E||F +

2
n√α ||Z E||F .

(16)

(The constant in front of the ﬁrst term is in fact slightly smaller than 7 in [11]  but in any case larger
than 4√2).
Theorem 1.2 improves over this result in several respects: (1) We do not have the second term on
the right hand side of (16)  that actually increases with the number of observed entries; (2) Our
error decreases as n/|E| rather than (n/|E|)1/2; (3) The noise enters Theorem 1.2 through the
operator norm ||Z E||2 instead of its Frobenius norm ||Z E||F ≥ ||Z E||2. For E uniformly random 
one expects ||Z E||F to be roughly of order ||Z E||2√n. For instance  within the intependent entries
model with bounded variance σ  ||Z E||F = Θ(p|E|) while ||Z E||2 is of order p|E|/n (up to

logarithmic terms).

2 Some notations

The matrix M to be reconstructed takes the form (1) where U ∈ Rm×r  V ∈ Rn×r. We write
U = [u1  u2  . . .   ur] and V = [v1  v2  . . .   vr] for the columns of the two factors  with ||ui|| = √m 
||vi|| = √n  and uT
i vj = 0 for i 6= j (there is no loss of generality in this  since

i uj = 0  vT

normalizations can be absorbed by redeﬁning Σ).

5

We shall write Σ = diag(Σ1  . . .   Σr) with Σ1 ≥ Σ2 ≥ ··· ≥ Σr > 0. The maximum and mini-
mum singular values will also be denoted by Σmax = Σ1 and Σmin = Σr. Further  the maximum
size of an entry of M is Mmax ≡ maxij |Mij|.
Probability is taken with respect to the uniformly random subset E ⊆ [m]× [n] given |E| and (even-
tually) the noise matrix Z. Deﬁne ǫ ≡ |E|/√mn. In the case when m = n  ǫ corresponds to the
average number of revealed entries per row or column. Then it is convenient to work with a model in
which each entry is revealed independently with probability ǫ/√mn. Since  with high probability
|E| ∈ [ǫ√α n − A√n log n  ǫ√α n + A√n log n]  any guarantee on the algorithm performances

that holds within one model  holds within the other model as well if we allow for a vanishing shift
in ǫ. We will use C  C ′ etc. to denote universal numerical constants.
Given a vector x ∈ Rn  ||x|| will denote its Euclidean norm. For a matrix X ∈ Rn×n′  ||X||F is
its Frobenius norm  and ||X||2 its operator norm (i.e. ||X||2 = supu6=0 ||Xu||/||u||). The standard
scalar product between vectors or matrices will sometimes be indicated by hx  yi or hX  Y i  respec-
tively. Finally  we use the standard combinatorics notation [N ] = {1  2  . . .   N} to denote the set of
ﬁrst N integers.

3 Proof of Theorem 1.1

As explained in the introduction  the crucial idea is to consider the singular value decomposition

of the trimmed matrix eN E instead of the original matrix N E. Apart from a trivial rescaling  these
singular values are close to the ones of the original matrix M.
Lemma 3.1. There exists a numerical constant C such that  with probability greater than 1− 1/n3 
(17)

σq

1

+

where it is understood that Σq = 0 for q > r.

ǫ − Σq(cid:12)(cid:12)(cid:12) ≤ CMmaxr α

ǫ

(cid:12)(cid:12)(cid:12)

ǫ||eZ E||2  

Proof. For any matrix A  let σq(A) denote the qth singular value of A. Then  σq(A+B) ≤ σq(A)+
σ1(B)  whence

(18)

σr+1(cid:19)

We will now prove Theorem 1.1.

1

Proof. (Theorem 1.1) For any matrix A of rank at most 2r  ||A||F ≤ √2r||A||2  whence
√mn||M − Tr(eN E)||F ≤
≤

√2r
√mn
√2r

√mn

M −

σixiyT

√mn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
i (cid:17)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
√mn(cid:18)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)M −
||eZ E||2 +
≤ 2CMmaxp2αr/ǫ + (2√2r/ǫ)||eZ E||2
|E| (cid:19) ||eZ E||2 .
≤ C ′Mmax (cid:18) nrα3/2
+ 2√2(cid:18) n√rα

ǫ (cid:16)eN E − Xi≥r+1
ǫ fM E(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
|E| (cid:19)1/2

√mn

This proves our claim.

+

ǫ

ǫ

√mn

6

where the second inequality follows from the following Lemma as shown in [1].
Lemma 3.2 (Keshavan  Montanari  Oh  2009 [1]). There exists a numerical constant C such that 
with probability larger than 1 − 1/n3 

(cid:12)(cid:12)(cid:12)

σq

ǫ − Σq(cid:12)(cid:12)(cid:12) ≤ (cid:12)(cid:12)(cid:12)σq(fM E)/ǫ − Σq(cid:12)(cid:12)(cid:12) + σ1(eZ E)/ǫ

1

≤ CMmaxr α

ǫ

+

ǫ||eZ E||2  
(cid:12)(cid:12)(cid:12)(cid:12)2 ≤ CMmaxr α

ǫ

.

1

√mn(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)M −

√mn

ǫ fM E(cid:12)(cid:12)(cid:12)(cid:12)

4 Proof of Theorem 1.2

Recall that the cost function is deﬁned over the Riemannian manifold M(m  n) ≡ G(m  r)×G(n  r).
The proof of Theorem 1.2 consists in controlling the behavior of F in a neighborhood of u = (U  V )
(the point corresponding to the matrix M to be reconstructed). Throughout the proof we let K(µ)
be the set of matrix couples (X  Y ) ∈ Rm×r × Rn×r such that ||X (i)||2 ≤ µr  ||Y (j)||2 ≤ µr for
all i  j

4.1 Preliminary remarks and deﬁnitions

Given x1 = (X1  Y1) and x2 = (X2  Y2) ∈ M(m  n)  two points on this manifold  their distance
is deﬁned as d(x1  x2) = pd(X1  X2)2 + d(Y1  Y2)2  where  letting (cos θ1  . . .   cos θr) be the
singular values of X T

1 X2/m 

Given S achieving the minimum in Eq. (4)  it is also convenient to introduce the notations

d(X1  X2) = ||θ||2 .

d−(x  u) ≡qΣ2
d+(x  u) ≡qΣ2

mind(x  u)2 + ||S − Σ||2
F  
maxd(x  u)2 + ||S − Σ||2
F .

4.2 Auxiliary lemmas and proof of Theorem 1.2

(19)

(20)

(21)

The proof is based on the following two lemmas that generalize and sharpen analogous bounds in
[1] (for proofs we refer to the journal version of this paper).
Lemma 4.1. There exists numerical constants C0  C1  C2 such that the following happens. Assume
ǫ ≥ C0µ0r√α max{ log n ; µ0r√α(Σmin/Σmax)4 } and δ ≤ Σmin/(C0Σmax). Then 
max d(x  u)2 + C2n√rα||Z E||2d+(x  u)  

(22)
(23)
for all x ∈ M(m  n) ∩ K(4µ0) such that d(x  u) ≤ δ  with probability at least 1 − 1/n4. Here
S ∈ Rr×r is the matrix realizing the minimum in Eq. (4).
Corollary 4.2. There exist a constant C such that  under the hypotheses of Lemma 4.1

F (x) − F (u) ≥ C1nǫ√α d−(x  u)2 − C1n√rα||Z E||2d+(x  u)  
F (x) − F (u) ≤ C2nǫ√α Σ2

||S − Σ||F ≤ CΣmaxd(x  u) + C

√r
ǫ ||Z E||2 .

Further  for an appropriate choice of the constants in Lemma 4.1  we have

σmax(S) ≤ 2Σmax + C
Σmin − C
σmin(S) ≥

1
2

√r
ǫ ||Z E||2  
√r
ǫ ||Z E||2 .

(24)

(25)

(26)

Lemma 4.3. There exists numerical constants C0  C1  C2 such that the following happens. Assume
ǫ ≥ C0µ0r√α (Σmax/Σmin)2 max{ log n ; µ0r√α(Σmax/Σmin)4 } and δ ≤ Σmin/(C0Σmax).

Then 

min(cid:20)d(x  u) − C2

√rΣmax
ǫΣmin

Σmin (cid:21)2
||Z E||2

+

 

(27)

||grad eF (x)||2 ≥ C1 nǫ2 Σ4

for all x ∈ M(m  n) ∩ K(4µ0) such that d(x  u) ≤ δ  with probability at least 1 − 1/n4. (Here
[a]+ ≡ max(a  0).)
We can now turn to the proof of our main theorem.

7

Proof. (Theorem 1.2). Let δ = Σmin/C0Σmax with C0 large enough so that the hypotheses of
Lemmas 4.1 and 4.3 are veriﬁed.
Call {xk}k≥0 the sequence of pairs (Xk  Yk) ∈ M(m  n) generated by gradient descent. By as-
sumption  the following is true with a large enough constant C:

||Z E||2 ≤

ǫ

Σmax(cid:19)2
C√r (cid:18) Σmin

Σmin .

Further  by using Corollary 4.2 in Eqs. (22) and (23) we get

F (x) − F (u) ≥ C1nǫ√αΣ2
F (x) − F (u) ≤ C2nǫ√αΣ2

min(cid:8)d(x  u)2 − δ2
max(cid:8)d(x  u)2 + δ2
√rΣmax
ǫΣmin

0 −(cid:9)  
0 +(cid:9)  
||Z E||2
Σmax

δ0 + ≡ C

where

√rΣmax
ǫΣmin

||Z E||2
Σmin

 

δ0 − ≡ C

By Eq. (28)  we can assume δ0 + ≤ δ0 − ≤ δ/10.
For ǫ ≥ Cαµ2
with the bound d(u  x0) ≤ ||M − X0SY T

0 ||F /n√αΣmin  we get
d(u  x0) ≤

δ
10

.

1r2(Σmax/Σmin)4 as per our assumptions  using Eq. (28) in Theorem 1.1  together

(28)

(29)
(30)

(31)

.

(32)

(33)

(34)

We make the following claims :

1. xk ∈ K(4µ0) for all k.
Indeed without loss of generality we can assume x0 ∈ K(3µ0) (because otherwise we can
rescale those lines of X0  Y0 that violate the constraint). Therefore eF (x0) = F (x0) ≤
4C2nǫ√αΣ2
maxδ2/100. On the other hand eF (x) ≥ ρ(e1/9 − 1) for x 6∈ K(4µ0).
Since eF (xk) is a non-increasing sequence  the thesis follows provided we take ρ ≥
C2nǫ√αΣ2
2. d(xk  u) ≤ δ/10 for all k.

min.

1r2(Σmax/Σmin)6  we have d(x0  u)2 ≤ (Σ2

Assuming ǫ ≥ Cαµ2
max)(δ/10)2.
Also assuming Eq. (28) with large enough C we can show the following. For all xk such
that d(xk  u) ∈ [δ/10  δ]  we have eF (x) ≥ F (x) ≥ F (x0). This contradicts the mono-
tonicity of eF (x)  and thus proves the claim.
Since the cost function is twice differentiable  and because of the above  the sequence {xk} con-
verges to

min/C ′Σ2

Ω =(cid:8)x ∈ K(4µ0) ∩ M(m  n) : d(x  u) ≤ δ   grad eF (x) = 0(cid:9) .

By Lemma 4.3 for any x ∈ Ω 

√rΣmax
ǫΣmin

||Z E||2
Σmin

d(x  u) ≤ C
which implies the thesis using Corollary 4.2.

Acknowledgements

This work was partially supported by a Terman fellowship  the NSF CAREER award CCF-0743978
and the NSF grant DMS-0806211.

8

References

[1] R. H. Keshavan  A. Montanari  and S. Oh. Matrix completion from a few entries.

arXiv:0901.3150  January 2009.

[2] A. Frieze  R. Kannan  and S. Vempala. Fast monte-carlo algorithms for ﬁnding low-rank

approximations. J. ACM  51(6):1025–1041  2004.

[3] E. J. Cand`es and B. Recht.

arxiv:0805.4471  2008.

Exact matrix completion via convex optimization.

[4] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis  Stanford University  2002.
[5] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion.

arXiv:0903.1476  2009.

[6] J-F Cai  E. J. Cand`es  and Z. Shen. A singular value thresholding algorithm for matrix com-

pletion. arXiv:0810.3286  2008.

[7] S. Ma  D. Goldfarb  and L. Chen. Fixed point and Bregman iterative methods for matrix rank

minimization. arXiv:0905.1643  2009.

[8] K. Toh and S. Yun. An accelerated proximal gradient algorithm for nuclear norm regularized

[9] J. Wright  A. Ganesh  S. Rao  and Y. Ma. Robust principal component analysis: Exact recovery

least squares problems. http://www.math.nus.edu.sg/∼matys  2009.
of corrupted low-rank matrices. arXiv:0905.0233  2009.

[10] K. Lee and Y. Bresler. Admira: Atomic decomposition for minimum rank approximation.

arXiv:0905.0044  2009.

[11] E. J. Cand`es and Y. Plan. Matrix completion with noise. arXiv:0903.3131  2009.
[12] A. Edelman  T. A. Arias  and S. T. Smith. The geometry of algorithms with orthogonality

constraints. SIAM J. Matr. Anal. Appl.  20:303–353  1999.

[13] P.-A. Absil  R. Mahony  and R. Sepulchrer. Optimization Algorithms on Matrix Manifolds.

Princeton University Press  2008.

[14] D. Achlioptas and F. McSherry. Fast computation of low-rank matrix approximations. J. ACM 

54(2):9  2007.

9

,Christian Kroer
Tuomas Sandholm