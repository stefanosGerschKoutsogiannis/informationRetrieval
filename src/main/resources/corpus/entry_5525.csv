2017,The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings,We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel  we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits  and empirical results which suggest that the approach is helpful in a wider range of applications.,The Unreasonable Effectiveness of Structured

Random Orthogonal Embeddings

Krzysztof Choromanski ∗
Google Brain Robotics
kchoro@google.com

Mark Rowland ∗

University of Cambridge
mr504@cam.ac.uk

Adrian Weller

University of Cambridge and Alan Turing Institute

aw665@cam.ac.uk

Abstract

We examine a class of embeddings based on structured random matrices with
orthogonal rows which can be applied in many machine learning applications
including dimensionality reduction and kernel approximation. For both the Johnson-
Lindenstrauss transform and the angular kernel  we show that we can select matrices
yielding guaranteed improved performance in accuracy and/or speed compared to
earlier methods. We introduce matrices with complex entries which give signiﬁcant
further accuracy improvement. We provide geometric and Markov chain-based
perspectives to help understand the beneﬁts  and empirical results which suggest
that the approach is helpful in a wider range of applications.

1

Introduction

Embedding methods play a central role in many machine learning applications by projecting feature
vectors into a new space (often nonlinearly)  allowing the original task to be solved more efﬁciently.
The new space might have more or fewer dimensions depending on the goal. Applications include
the Johnson-Lindenstrauss Transform for dimensionality reduction (JLT  Johnson and Lindenstrauss 
1984) and kernel methods with random feature maps (Rahimi and Recht  2007). The embedding can
be costly hence many fast methods have been developed  see §1.1 for background and related work.
We present a general class of random embeddings based on particular structured random matrices
with orthogonal rows  which we call random ortho-matrices (ROMs); see §2. We show that ROMs
may be used for the applications above  in each case demonstrating improvements over previous
methods in statistical accuracy (measured by mean squared error  MSE)  in computational efﬁciency
(while providing similar accuracy)  or both. We highlight the following contributions:
• In §3: The Orthogonal Johnson-Lindenstrauss Transform (OJLT) for dimensionality reduction.
We prove this has strictly smaller MSE than the previous unstructured JLT mechanisms. Further 
OJLT is as fast as the fastest previous JLT variants (which are structured).

• In §4: Estimators for the angular kernel (Sidorov et al.  2014) which guarantee better MSE. The
angular kernel is important for many applications  including natural language processing (Sidorov
et al.  2014)  image analysis (Jégou et al.  2011)  speaker representations (Schmidt et al.  2014)
and tf-idf data sets (Sundaram et al.  2013).

• In §5: Two perspectives on the effectiveness of ROMs to help build intuitive understanding.
In §6 we provide empirical results which support our analysis  and show that ROMs are effective for
a still broader set of applications. Full details and proofs of all results are in the Appendix.

∗equal contribution

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

1.1 Background and related work

Our ROMs can have two forms (see §2 for details): (i) a Gort is a random Gaussian matrix con-
ditioned on rows being orthogonal; or (ii) an SD-product matrix is formed by multiplying some
number k of SD blocks  each of which is highly structured  typically leading to fast computation
of products. Here S is a particular structured matrix  and D is a random diagonal matrix; see §2
for full details. Our SD block generalizes an HD block  where H is a Hadamard matrix  which
received previous attention. Earlier approaches to embeddings have explored using various structured
matrices  including particular versions of one or other of our two forms  though in different contexts.

For dimensionality reduction  Ailon and Chazelle (2006) used a single HD block as a way to spread
out the mass of a vector over all dimensions before applying a sparse Gaussian matrix. Choromanski
and Sindhwani (2016) also used just one HD block as part of a larger structure. Bojarski et al. (2017)
discussed using k = 3 HD blocks for locality-sensitive hashing methods but gave no concrete results
for their application to dimensionality reduction or kernel approximation. All these works  and other
earlier approaches (Hinrichs and Vybíral  2011; Vybíral  2011; Zhang and Cheng  2013; Le et al. 
2013; Choromanska et al.  2016)  provided computational beneﬁts by using structured matrices with
less randomness than unstructured iid Gaussian matrices  but none demonstrated accuracy gains.

Yu et al. (2016) were the ﬁrst to show that Gort-type matrices can yield improved accuracy  but their
theoretical result applies only asymptotically for many dimensions  only for the Gaussian kernel and
for just one speciﬁc orthogonal transformation  which is one instance of the larger class we consider.
Their theoretical result does not yield computational beneﬁts. Yu et al. (2016) did explore using a
number k of HD blocks empirically  observing good computational and statistical performance for
k = 3  but without any theoretical accuracy guarantees. It was left as an open question why matrices
formed by a small number of HD blocks can outperform non-discrete transforms.

In contrast  we are able to prove that ROMs yield improved MSE in several settings and for many of
them for any number of dimensions. In addition  SD-product matrices can deliver computational
speed beneﬁts. We provide initial analysis to understand why k = 3 can outperform the state-of-
the-art  why odd k yields better results than even k  and why higher values of k deliver decreasing
additional beneﬁts (see §3 and §5).

2 The family of Random Ortho-Matrices (ROMs)

Random ortho-matrices (ROMs) are taken from two main classes of distributions deﬁned below that
require the rows of sampled matrices to be orthogonal. A central theme of the paper is that this
orthogonal structure can yield improved statistical performance. We shall use bold uppercase (e.g.
M) to denote matrices and bold lowercase (e.g. x) for vectors.
Gaussian orthogonal matrices. Let G be a random matrix taking values in Rm×n with iid N (0  1)
elements  which we refer to as an unstructured Gaussian matrix. The ﬁrst ROM distribution we
consider yields the random matrix Gort  which is deﬁned as a random Rn×n matrix given by ﬁrst
taking the rows of the matrix to be a uniformly random orthonormal basis  and then independently
scaling each row  so that the rows marginally have multivariate Gaussian N (0  I) distributions. The
random variable Gort can then be extended to non-square matrices by either stacking independent
copies of the Rn×n random matrices  and deleting superﬂuous rows if necessary. The orthogonality
of the rows of this matrix has been observed to yield improved statistical properties for randomized
algorithms built from the matrix in a variety of applications.
SD-product matrices. Our second class of distributions is motivated by the desire to obtain similar
statistical beneﬁts of orthogonality to Gort  whilst gaining computational efﬁciency by employing
more structured matrices. We call this second class SD-product matrices. These take the more
n ∀i  j ∈
{1  . . .   n}; and the (Di)k
i=1 SDi  we
mean the matrix product (SDk) . . . (SD1). This class includes as particular cases several recently
introduced random matrices (e.g. Andoni et al.  2015; Yu et al.  2016)  where good empirical
performance was observed. We go further to establish strong theoretical guarantees  see §3 and §4.

i=1 are independent diagonal matrices described below. By(cid:81)k

structured form(cid:81)k

i=1 SDi  where S = {si j} ∈ Rn×n has orthogonal rows  |si j| = 1√

2

(cid:18)Hi−1 Hi−1

(cid:19)

2

Hi−1 −Hi−1

A prominent example of an S matrix is the normalized Hadamard matrix H  deﬁned recursively by
H1 = (1)  and then for i > 1  Hi = 1√
. Importantly  matrix-vector products
with H are computable in O(n log n) time via the fast Walsh-Hadamard transform  yielding large
computational savings. In addition  H matrices enable a signiﬁcant space advantage: since the
fast Walsh-Hadamard transform can be computed without explicitly storing H  only O(n) space is
i=1. Note that these Hn matrices are deﬁned only for
required to store the diagonal elements of (Di)k
n a power of 2  but if needed  one can always adjust data by padding with 0s to enable the use of ‘the
next larger’ H  doubling the number of dimensions in the worst case.
Matrices H are representatives of a much larger family in S which also attains computational savings.
These are L2-normalized versions of Kronecker-product matrices of the form A1 ⊗ ... ⊗ Al ∈ Rn×n
for l ∈ N  where ⊗ stands for a Kronecker product and blocks Ai ∈ Rd×d have entries of the
same magnitude and pairwise orthogonal rows each. For these matrices  matrix-vector products are
computable in O(n(2d − 1) logd(n)) time (Zhang et al.  2015).
n (−1)iN−1j0+...+i0jN−1
S includes also the Walsh matrices W = {wi j} ∈ Rn×n  where wi j = 1√
and iN−1...i0  jN−1...j0 are binary representations of i and j respectively.
For diagonal (Di)k
i=1  we mainly consider Rademacher entries leading to the following matrices.
Deﬁnition 2.1. The S-Rademacher random matrix with k ∈ N blocks is below  where (D(R)
are diagonal with iid Rademacher random variables [i.e. Unif({±1})] on the diagonals:

)k
i=1

i

k(cid:89)

M(k)

SR =

SD(R)

i

.

(1)

Having established the two classes of ROMs  we next apply them to dimensionality reduction.

i=1

2] = (cid:107)x(cid:107)2

3 The Orthogonal Johnson-Lindenstrauss Transform (OJLT)
Let X ⊂ Rn be a dataset of n-dimensional real vectors. The goal of dimensionality reduction via
random projections is to transform linearly each x ∈ X by a random mapping x F(cid:55)→ x(cid:48)  where:
F : Rn → Rm for m < n  such that for any x  y ∈ X the following holds: (x(cid:48))(cid:62)y(cid:48) ≈ x(cid:62)y. If
we furthermore have E[(x(cid:48))(cid:62)y(cid:48)] = x(cid:62)y then the dot-product estimator is unbiased. In particular 
this dimensionality reduction mechanism should in expectation preserve information about vectors’
norms  i.e. we should have: E[(cid:107)x(cid:48)(cid:107)2
m G  where G ∈ Rm×n is as
The standard JLT mechanism uses the randomized linear map F = 1√
in §2  requiring mn multiplications to evaluate. Several fast variants (FJLTs) have been proposed by
replacing G with random structured matrices  such as sparse or circulant Gaussian matrices (Ailon
and Chazelle  2006; Hinrichs and Vybíral  2011; Vybíral  2011; Zhang and Cheng  2013). The fastest
of these variants has O(n log n) time complexity  but at a cost of higher MSE for dot-products.
Our Orthogonal Johnson-Lindenstrauss Transform (OJLT) is obtained by replacing the unstructured
random matrix G with a sub-sampled ROM from §2: either Gort  or a sub-sampled version M(k) sub
of the S-Rademacher ROM  given by sub-sampling rows from the left-most S matrix in the product.
We sub-sample since m < n. We typically assume uniform sub-sampling without replacement. The
resulting dot-product estimators for vectors x  y ∈ X are given by:

2 for any x ∈ X .

(cid:98)K base
(cid:98)K ort
(MSE) for these three estimators. Precisely  the MSE of an estimator (cid:98)K(x  y) of the inner product
(cid:104)x  y(cid:105) for x  y ∈ X is deﬁned to be MSE((cid:98)K(x  y)) = E(cid:104)

(cid:17)(cid:62)(cid:16)
(cid:105)
((cid:98)K(x  y) − (cid:104)x  y(cid:105)2)

We contribute the following closed-form expressions  which exactly quantify the mean-squared error

(Gx)(cid:62)(Gy)
1
m
(Gortx)(cid:62)(Gorty) 
1
m

[unstructured iid baseline  previous state-of-the-art accuracy] 

. See the Appendix

(cid:98)K (k)

M(k) sub

SR y

M(k) sub

SR x

m (x  y) =

(cid:16)

1
m

m (x  y) =

m (x  y) =

SR

(cid:17)

.

(2)

for detailed proofs of these results and all others in this paper.

3

Lemma 3.1. The MSE of the unstructured JLT dot-product estimator (cid:98)K base
dimensional random feature maps is unbiased  with MSE((cid:98)K base
Theorem 3.2. The estimator (cid:98)K ort
MSE((cid:98)K ort
=MSE((cid:98)K base
(cid:18)(cid:18) 1
(cid:20)

m is unbiased and satisﬁes  for n ≥ 4:

m (x  y))
m (x  y)) +
2(cid:107)y(cid:107)2

m (x  y)) = 1

(cid:107)x(cid:107)2

(cid:19)

2n2

m
m − 1

4I(n − 3)I(n − 4)

− 1

n

n + 2

(I(n − 3) − I(n − 1))I(n − 4)

(cid:18) 1

(cid:19)(cid:20)

I(n − 1) (I(n − 4) − I(n − 2))

n − 2

− 1
n

cos2(θ) − 1
2

√

where I(n) =(cid:82) π
Theorem 3.3 (Key result). The OJLT estimator (cid:98)K (k)
MSE((cid:98)K (k)

(cid:18) n − m

0 sinn(x)dx =

πΓ((n+1)/2)
Γ(n/2+1)

(cid:19)(cid:18)

m (x  y)) =

.

1
m

n − 1

((x(cid:62)y)2 + (cid:107)x(cid:107)2(cid:107)y(cid:107)2) +

random feature maps and uniform sub-sampling policy without replacement  is unbiased with

m (x  y) with k blocks  using m-dimensional

m of x  y ∈ Rn using m-
2(cid:107)y(cid:107)2
m ((x(cid:62)y)2 +(cid:107)x(cid:107)2
2).

(cid:20)
(cid:21)(cid:19)

cos2(θ) +

+

1
2
− (cid:104)x  y(cid:105)2

(cid:21)
(cid:21)

 

(3)

k−1(cid:88)

r=1

(−1)r2r

nr

(2(x(cid:62)y)2 + (cid:107)x(cid:107)2(cid:107)y(cid:107)2) +

n(cid:88)

i=1

(−1)k2k
nk−1

(4)

(cid:19)

x2
i y2
i

.

Proof (Sketch). For k = 1  the random projection matrix is given by sub-sampling rows from SD1 
and the computation can be carried out directly. For k ≥ 1  the proof proceeds by induction.
The random projection matrix in the general case is given by sub-sampling rows of the matrix
SDk ··· SD1. By writing the MSE as an expectation and using the law of conditional expectations
conditioning on the value of the ﬁrst k − 1 random matrices Dk−1  . . .   D1  the statement of the
theorem for 1 SD block and for k − 1 SD blocks can be neatly combined to yield the result.

m

m (subsampling

m (x  y)) > MSE((cid:98)K (2k+1)

To our knowledge  it has not previously been possible to provide theoretical guarantees that
SD-product matrices outperform iid matrices. Combining Lemma 3.1 with Theorem 3.3 yields the
following important result.

m .
m ; we explore this empirically in §6.
Theorem 3.3 shows that there are diminishing MSE beneﬁts to using a large number k of SD
(x  y)) <
(x  y)). These observations  and those in §5  help to under-

Corollary 3.4 (Theoretical guarantee of improved performance). Estimators (cid:98)K (k)
without replacement) yield guaranteed lower MSE than (cid:98)K base
m is better or worse than (cid:98)K (k)
It is not yet clear when (cid:98)K ort
blocks. Interestingly  odd k is better than even: it is easy to observe that MSE((cid:98)K (2k−1)
MSE((cid:98)K (2k)
(cid:98)K (k)
a given datapoint x using (cid:98)K (k)

stand why empirically k = 3 was previously observed to work well (Yu et al.  2016).
If we take S to be a normalized Hadamard matrix H  then even though we are using sub-sampling 
and hence the full computational beneﬁts of the Walsh-Hadamard transform are not available  still
m achieves improved MSE compared to the base method with less computational effort  as follows.

Lemma 3.5. There exists an algorithm (see Appendix for details) which computes an embedding for
m with S set to H and uniform sub-sampling policy in expected time

min{O((k − 1)n log(n) + nm − (m−1)m
Note that for m = ω(k log(n)) or if k = 1  the time complexity is smaller than the brute force
Θ(nm). The algorithm uses a simple observation that one can reuse calculations conducted for the
upper half of the Hadamard matrix while performing computations involving rows from its other half 
instead of running these calculations from scratch (details in the Appendix).
An alternative to sampling without replacement is deterministically to choose the ﬁrst m rows. In our
experiments in §6  these two approaches yield the same empirical performance  though we expect

  kn log(n)}.

m

2

4

that the deterministic method could perform poorly on adversarially chosen data. The ﬁrst m rows
approach can be realized in time O(n log(m) + (k − 1)n log(n)) per datapoint.
Theorem 3.3 is a key result in this paper  demonstrating that SD-product matrices yield both statistical
and computational improvements compared to the base iid procedure  which is widely used in practice.
We next show how to obtain further gains in accuracy.

3.1 Complex variants of the OJLT

We show that the MSE beneﬁts of Theorem 3.3 may be markedly improved by using SD-product
matrices with complex entries M(k)
SH. Speciﬁcally  we consider the variant S-Hybrid random matrix
below  where D(U )
is a diagonal matrix with iid Unif(S1) random variables on the diagonal  inde-
pendent of (D(R)
)k−1
i=1   and S1 is the unit circle of C. We use the real part of the Hermitian product
between projections as a dot-product estimator; recalling the deﬁnitions of §2  we use:

k

i

M(k)

SH = SD(U )

k

SD(R)

i

 

k−1(cid:89)

i=1

(cid:98)KH (k)

m (x  y) =

(cid:20)(cid:16)

1
m

Re

(cid:17)(cid:62)(cid:16)

(cid:17)(cid:21)

M(k) sub

SH x

M(k) sub

SH y

.

(5)

2 MSE((cid:98)K (k)

Remarkably  this complex variant yields exactly half the MSE of the OJLT estimator.

Theorem 3.6. The estimator (cid:98)K
unbiased and satisﬁes: MSE((cid:98)K
This large factor of 2 improvement could instead be obtained by doubling m for (cid:98)K (k)

H (k)
m (x  y)  applying uniform sub-sampling without replacement  is
H (k)
m (x  y)) = 1

m . However 
this would require doubling the number of parameters for the transform  whereas the S-Hybrid
estimator requires additional storage only for the complex parameters in the matrix D(U )
k . Strikingly 
it is straightforward to extend the proof of Theorem 3.6 (see Appendix) to show that rather than
taking the complex random variables in M(k) sub
to be Unif(S1)  it is possible to take them to be
Unif({1 −1  i −i}) and still obtain exactly the same beneﬁt in MSE.

m (x  y)).

SH

H (k)
m

deﬁned in Equation (5): replacing the random matrix D(U )
(which has iid Unif(S1) elements on the diagonal) with instead a random diagonal matrix having iid
Unif({1 −1  i −i}) elements on the diagonal  does not affect the MSE of the estimator.
It is natural to wonder if using an SD-product matrix with more complex random variables (for all
SD blocks) would improve performance still further. However  interestingly  this appears not to be
the case; details are provided in the Appendix §8.7.

k

Theorem 3.7. For the estimator (cid:98)K

3.2 Sub-sampling with replacement

Our results above focus on SD-product matrices where rows have been sub-sampled without
replacement. Sometimes (e.g. for parallelization) it can be convenient instead to sub-sample with
replacement. As might be expected  this leads to worse MSE  which we can quantify precisely.

Theorem 3.8. For each of the estimators (cid:98)K (k)

m and (cid:98)K

H (k)
m   if uniform sub-sampling with (rather
than without) replacement is used then the MSE is worsened by a multiplicative constant of n−1
n−m .

4 Kernel methods with ROMs

ROMs can also be used to construct high-quality random feature maps for non-linear kernel
approximation. We analyze here the angular kernel  an important example of a Pointwise Nonlinear
Gaussian kernel (PNG)  discussed in more detail at the end of this section.
Deﬁnition 4.1. The angular kernel K ang is deﬁned on Rn by K ang(x  y) = 1 − 2θx y
is the angle between x and y.

π   where θx y

5

To employ random feature style approximations to this kernel  we ﬁrst observe it may be rewritten as

K ang(x  y) = E [sign(Gx)sign(Gy)]  

where G ∈ R1×n is an unstructured isotropic Gaussian vector. This motivates approximations of the
form:

(cid:98)K angm(x  y) =

sign(Mx)(cid:62)sign(My) 

1
m

m

m

is unbiased and MSE((cid:98)K ang base

(6)
where M ∈ Rm×n is a random matrix  and the sign function is applied coordinate-wise. Such
kernel estimation procedures are heavily used in practice (Rahimi and Recht  2007)  as they allow
fast approximate linear methods to be used (Joachims  2006) for inference tasks. If M = G  the
unstructured Gaussian matrix  then we obtain the standard random feature estimator. We shall contrast
this approach against the use of matrices from the ROMs family.
When constructing random feature maps for kernels  very often m > n. In this case  our structured
mechanism can be applied by concatenating some number of independent structured blocks. Our
theoretical guarantees will be given just for one block  but can easily be extended to a larger number
of blocks since different blocks are independent.

for approximating the angular kernel is
deﬁned by taking M to be G  the unstructured Gaussian matrix  in Equation (6)  and satisﬁes the
following.

The standard random feature approximation (cid:98)K ang base
Lemma 4.2. The estimator (cid:98)K ang base
The MSE of an estimator (cid:98)K ang(x  y) of the true angular kernel K ang(x  y) is deﬁned analogously
states that if we instead take M = Gort in Equation (6)  then we obtain an estimator (cid:98)K ang ort
Theorem 4.3. Estimator (cid:98)K ang ort
We also derive a formula for the MSE of an estimator (cid:98)K ang M

to the MSE of an estimator of the dot product  given in §3. Our main result regarding angular kernels
with

of the angular kernel which replaces G
with an arbitrary random matrix M and uses m random feature maps. The formula is helpful to see
how the quality of the estimator depends on the probabilities that the projections of the rows of M are
contained in some particular convex regions of the 2-dimensional space Lx y spanned by datapoints
x and y. For an illustration of the geometric deﬁnitions introduced in this Section  see Figure 1. The
formula depends on probabilities involving events Ai = {sgn((ri)T x) (cid:54)= sgn((ri)T y)}  where
ri stands for the ith row of the structured matrix. Notice that Ai = {ri
stands for the projection of ri into Lx y and Cx y is the union of two cones in Lx y  each of angle θx y.
satisﬁes the following  where: δi j = P[Ai ∩ Aj] − P[Ai]P[Aj]:

proj ∈ Cx y}  where ri

MSE((cid:98)K ang ort

is unbiased and satisﬁes:

(x  y)) < MSE((cid:98)K ang base

(x  y)) = 4θx y(π−θx y)

strictly smaller MSE  as follows.

(x  y)).

mπ2

proj

m

m

m

m

m

.

Theorem 4.4. Estimator (cid:98)K ang M
(cid:34)
MSE((cid:98)K ang M

(x  y)) =

m

m

m − m(cid:88)

1
m2

i=1

m

(cid:35)

 m(cid:88)

i=1

 .

(cid:88)

i(cid:54)=j

(1 − 2P[Ai])2

+

4
m2

(P[Ai] − θx y
π

)2 +

δi j

Note that probabilities P[Ai] and δi j depend on the choice of M. It is easy to prove that for
unstructured G and Gort we have: P[Ai] = θx y
π . Further  from the independence of the rows of
G  δi j = 0 for i (cid:54)= j. For unstructured G we obtain Lemma 4.2. Interestingly  we see that to
prove Theorem 4.3  it sufﬁces to show δi j < 0  which is the approach we take (see Appendix). If
we replace G with M(k)
π does not depend on i. Hence  the
angular kernel estimator based on Hadamard matrices gives smaller MSE estimator if and only if

SR  then the expression  = P[Ai] − θx y
i(cid:54)=j δi j + m2 < 0. It is not yet clear if this holds in general.

(cid:80)

As alluded to at the beginning of this section  the angular kernel may be viewed as a member of a wie
family of kernels known as Pointwise Nonlinear Gaussian kernels.

6

Figure 1: Left part: Left: g1 is orthogonal to Lx y. Middle: g1 ∈ Lx y. Right: g1 is close to orthogonal to
Lx y. Right part: Visualization of the Cayley graph explored by the Hadamard-Rademacher process in two
dimensions. Nodes are colored red  yellow  light blue  dark blue  for Cayley distances of 0  1  2  3 from the
identity matrix respectively. See text in §5.

deﬁned by K f (x  y) = E(cid:2)f (gT x)f (gT y)(cid:3)  where g is a Gaussian vector with i.i.d N (0  1) entries.

Deﬁnition 4.5. For a given function f  the Pointwise Nonlinear Gaussian kernel (PNG) K f is

Many prominent examples of kernels (Williams  1998; Cho and Saul  2009) are PNGs. Wiener’s
tauberian theorem shows that all stationary kernels may be approximated arbitrarily well by sums of
PNGs (Samo and Roberts  2015). In future work we hope to explore whether ROMs can be used to
achieve statistical beneﬁt in estimation tasks associated with a wider range of PNGs.

5 Understanding the effectiveness of orthogonality

Here we build intuitive understanding for the effectiveness of ROMs. We examine geometrically the
angular kernel (see §4)  then discuss a connection to random walks over orthogonal matrices.

Angular kernel. As noted above for the Gort-mechanism  smaller MSE than that for unstructured
G is implied by the inequality P[Ai ∩Aj] < P[Ai]P[Aj]  which is equivalent to: P[Aj|Ai] < P[Aj].
Now it becomes clear why orthogonality is crucial. Without loss of generality take: i = 1  j = 2  and
let g1 and g2 be the ﬁrst two rows of Gort.
Consider ﬁrst the extreme case (middle of left part of Figure 1)  where all vectors are 2-dimensional.
Recall deﬁnitions from just after Theorem 4.3. If g1 is in Cx y then it is much less probable for
g2 also to belong to Cx y. In particular  if θ < π
2 then the probability is zero. That implies the
inequality. On the other hand  if g1 is perpendicular to Lx y then conditioning on Ai does not have
any effect on the probability that g2 belongs to Cx y (left subﬁgure of Figure 1). In practice  with high
probability the angle φ between g1 and Lx y is close to π
2 . That again implies
p of g1 into Lx y to be in Cx y  the more probable directions of
that conditioned on the projection g1
p (see: ellipsoid-like shape in the right subﬁgure of Figure 1 which is the
p are perpendicular to g1
g2
projection of the sphere taken from the (n − 1)-dimensional space orthogonal to g1 into Lx y). This
makes it less probable for g2
2   but this is what
provides superiority of the orthogonal transformations over state-of-the-art ones in the angular kernel
approximation setting.

p to be also in Cx y. The effect is subtle since φ ≈ π

2   but is not exactly π

Markov chain perspective. We focus on Hadamard-Rademacher random matrices HDk...HD1 
a special case of the SD-product matrices described in Section 2. Our aim is to provide intuition
for how the choice of k affects the quality of the random matrix  following our earlier observations
just after Corollary 3.4  which indicated that for SD-product matrices  odd values of k yield greater
beneﬁts than even values  and that there are diminishing beneﬁts from higher values of k. We proceed
by casting the random matrices into the framework of Markov chains.
Deﬁnition 5.1. The Hadamard-Rademacher process in n dimensions is the Markov chain (Xk)∞
k=0
taking values in the orthogonal group O(n)  with X0 = I almost surely  and Xk = HDkXk−1
almost surely  where H is the normalized Hadamard matrix in n dimensions  and (Dk)∞
k=1 are iid
diagonal matrices with independent Rademacher random variables on their diagonals.
Constructing an estimator based on Hadamard-Rademacher matrices is equivalent to simulating
several time steps from the Hadamard-Rademacher process. The quality of estimators based on
Hadamard-Rademacher random matrices comes from a quick mixing property of the corresponding

7

(a) g50c - pointwise evalu-
ation MSE for inner product
estimation

(b) random - angular kernel (c) random - angular kernel

with true angle π/4

(d) g50c - inner product es-
timation MSE for variants of
3-block SD-product matri-
ces.

(e) LETTER - dot-product

(f) USPS - dot-product

(g) LETTER - angular kernel

(h) USPS - angular kernel

Figure 2: Top row: MSE curves for pointwise approximation of inner product and angular kernels on the
g50c dataset  and randomly chosen vectors. Bottom row: Gram matrix approximation error for a variety of
data sets  projection ranks  transforms  and kernels. Note that the error scaling is dependent on the application.

Markov chain. The following demonstrates attractive properties of the chain in low dimensions.

Proposition 5.2. The Hadamard-Rademacher process in two dimensions: explores a state-space of
16 orthogonal matrices  is ergodic with respect to the uniform distribution on this set  has period 2 
the diameter of the Cayley graph of its state space is 3  and the chain is fully mixed after 3 time steps.
This proposition  and the Cayley graph corresponding to the Markov chain’s state space (Figure 1
right)  illustrate the fast mixing properties of the Hadamard-Rademacher process in low dimensions;
this agrees with the observations in §3 that there are diminishing returns associated with using a large
number k of HD blocks in an estimator. The observation in Proposition 5.2 that the Markov chain
has period 2 indicates that we should expect different behavior for estimators based on odd and even
numbers of blocks of HD matrices  which is reﬂected in the analytic expressions for MSE derived in
Theorems 3.3 and 3.6 for the dimensionality reduction setup.

6 Experiments

We present comparisons of estimators introduced in §3 and §4  illustrating our theoretical results  and
further demonstrating the empirical success of ROM-based estimators at the level of Gram matrix
approximation. We compare estimators based on: unstructured Gaussian matrices G  matrices Gort 
S-Rademacher and S-Hybrid matrices with k = 3 and different sub-sampling strategies. Results
for k > 3 do not show additional statistical gains empirically. Additional experimental results 
including a comparison of estimators using different numbers of SD blocks  are in the Appendix §10.
Throughout  we use the normalized Hadamard matrix H for the structured matrix S.

6.1 Pointwise kernel approximation

Complementing the theoretical results of §3 and §4  we provide several salient comparisons of the
various methods introduced - see Figure 2 top. Plots presented here (and in the Appendix) compare
MSE for dot-product and angular and kernel. They show that estimators based on Gort  S-Hybrid
and S-Rademacher matrices without replacement  or using the ﬁrst m rows  beat the state-of-the-art
unstructured G approach on accuracy for all our different datasets in the JLT setup. Interestingly  the
latter two approaches give also smaller MSE than Gort-estimators. For angular kernel estimation 
where sampling is not relevant  we see that Gort and S-Rademacher approaches again outperform
the ones based on matrices G.

8

6.2 Gram matrix approximation

Moving beyond the theoretical guarantees established in §3 and §4  we show empirically that the
superiority of estimators based on ROMs is maintained at the level of Gram matrix approximation.
We compute Gram matrix approximations (with respect to both standard dot-product  and angular

kernel) for a variety of datasets. We use the normalized Frobenius norm error (cid:107)K − (cid:98)K(cid:107)2/(cid:107)K(cid:107)2

as our metric (as used by Choromanski and Sindhwani  2016)  and plot the mean error based on
1 000 repetitions of each random transform - see Figure 2 bottom. The Gram matrices are computed
on a randomly selected subset of 550 data points from each dataset. As can be seen  the S-Hybrid
estimators using the “no-replacement” or “ﬁrst m rows” sub-sampling strategies outperform even
the orthogonal Gaussian ones in the dot-product case. For the angular case  the Gort-approach and
S-Rademacher approach are practically indistinguishable.

7 Conclusion

We deﬁned the family of random ortho-matrices (ROMs). This contains the SD-product matrices 
which include a number of recently proposed structured random matrices. We showed theoretically
and empirically that ROMs have strong statistical and computational properties (in several cases
outperforming previous state-of-the-art) for algorithms performing dimensionality reduction and
random feature approximations of kernels. We highlight Corollary 3.4  which provides a theoretical
guarantee that SD-product matrices yield better accuracy than iid matrices in an important dimension-
ality reduction application (we believe the ﬁrst result of this kind). Intriguingly  for dimensionality
reduction  using just one complex structured matrix yields random features of much better quality.
We provided perspectives to help understand the beneﬁts of ROMs  and to help explain the behavior
of SD-product matrices for various numbers of blocks. Our empirical ﬁndings suggest that our
theoretical results might be further strengthened  particularly in the kernel setting.

Acknowledgements

We thank Vikas Sindhwani at Google Brain Robotics and Tamas Sarlos at Google Research for
inspiring conversations that led to this work. We thank Matej Balog  Maria Lomeli  Jiri Hron and
Dave Janz for helpful comments. MR acknowledges support by the UK Engineering and Physical
Sciences Research Council (EPSRC) grant EP/L016516/1 for the University of Cambridge Centre
for Doctoral Training  the Cambridge Centre for Analysis. AW acknowledges support by the Alan
Turing Institute under the EPSRC grant EP/N510129/1  and by the Leverhulme Trust via the CFI.

9

References
N. Ailon and B. Chazelle. Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform. In

STOC  2006.

A. Andoni  P. Indyk  T. Laarhoven  I. Razenshteyn  and L. Schmidt. Practical and optimal LSH for angular

distance. In NIPS  2015.

M. Bojarski  A. Choromanska  K. Choromanski  F. Fagan  C. Gouy-Pailler  A. Morvan  N. Sakr  T. Sarlos  and
J. Atif. Structured adaptive and random spinners for fast machine learning computations. In to appear in
AISTATS  2017.

Y. Cho and L. K. Saul. Kernel methods for deep learning. In NIPS  2009.

A. Choromanska  K. Choromanski  M. Bojarski  T. Jebara  S. Kumar  and Y. LeCun. Binary embeddings with

structured hashed projections. In ICML  2016.

K. Choromanski and V. Sindhwani. Recycling randomness with structure for sublinear time kernel expansions.

In ICML  2016.

A. Hinrichs and J. Vybíral. Johnson-Lindenstrauss lemma for circulant matrices. Random Structures &

Algorithms  39(3):391–398  2011.

H. Jégou  M. Douze  and C. Schmid. Product quantization for nearest neighbor search. IEEE Transactions on

Pattern Analysis and Machine Intelligence  33(1):117–128  2011.

Thorsten Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining  KDD ’06  pages 217–226  New York  NY  USA 
2006. ACM. ISBN 1-59593-339-5. doi: 10.1145/1150402.1150429. URL http://doi.acm.org/10.
1145/1150402.1150429.

W. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space. Contemporary

Mathematics  26:189–206  1984.

Q. Le  T. Sarlós  and A. Smola. Fastfood - approximating kernel expansions in loglinear time. In ICML  2013.

A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS  2007.

Y.-L. K. Samo and S. Roberts. Generalized spectral kernels. CoRR  abs/1506.02236  2015.

L. Schmidt  M. Shariﬁ  and I. Moreno. Large-scale speaker identiﬁcation. In Acoustics  Speech and Signal

Processing (ICASSP)  2014 IEEE International Conference on  pages 1650–1654. IEEE  2014.

G. Sidorov  A. Gelbukh  H. Gómez-Adorno  and D. Pinto. Soft similarity and soft cosine measure: Similarity of

features in vector space model. Computación y Sistemas  18(3)  2014.

N. Sundaram  A. Turmukhametova  N. Satish  T. Mostak  P. Indyk  S. Madden  and P. Dubey. Streaming
similarity search over one billion tweets using parallel locality-sensitive hashing. Proceedings of the VLDB
Endowment  6(14):1930–1941  2013.

J. Vybíral. A variant of the Johnson-Lindenstrauss lemma for circulant matrices. Journal of Functional Analysis 

260(4):1096–1105  2011.

C. Williams. Computation with inﬁnite neural networks. Neural Computation  10(5):1203–1216  1998.

F. Yu  A. Suresh  K. Choromanski  D. Holtmann-Rice  and S. Kumar. Orthogonal random features. In NIPS 

pages 1975–1983  2016.

H. Zhang and L. Cheng. New bounds for circulant Johnson-Lindenstrauss embeddings. CoRR  abs/1308.6339 

2013.

Xu Zhang  Felix X. Yu  Ruiqi Guo  Sanjiv Kumar  Shengjin Wang  and Shih-Fu Chang. Fast orthogonal
projection based on kronecker product. In 2015 IEEE International Conference on Computer Vision  ICCV
2015  Santiago  Chile  December 7-13  2015  pages 2929–2937  2015. doi: 10.1109/ICCV.2015.335. URL
http://dx.doi.org/10.1109/ICCV.2015.335.

10

,Krzysztof Choromanski
Mark Rowland
Adrian Weller