2017,Self-supervised Learning of Motion Capture,Current state-of-the-art solutions for motion capture from a single camera are optimization driven: they optimize the parameters of a 3D human model so that its re-projection matches measurements in the video (e.g. person segmentation  optical flow  keypoint detections etc.). Optimization models are susceptible to local minima. This has been the bottleneck that forced using clean green-screen like backgrounds at capture time  manual initialization  or switching to multiple cameras as input resource. In this work  we propose a learning based motion capture model for single camera input. Instead of optimizing mesh and skeleton parameters directly  our model optimizes neural network weights that predict 3D shape and skeleton configurations given a monocular RGB video. Our model is trained using a combination of strong supervision from synthetic data  and self-supervision from differentiable rendering of (a) skeletal keypoints  (b) dense 3D mesh motion  and (c) human-background segmentation  in an end-to-end framework. Empirically we show our model combines the best of both worlds of supervised learning and test-time optimization: supervised learning initializes the model parameters in the right regime  ensuring good pose and surface initialization at test time  without manual effort. Self-supervision by back-propagating through differentiable rendering allows (unsupervised) adaptation of the model to the test data  and offers much tighter fit than a pretrained fixed model. We show that the proposed model improves with experience and converges to low-error solutions where previous optimization methods fail.,Self-supervised Learning of Motion Capture

Hsiao-Yu Fish Tung 1  Hsiao-Wei Tung 2  Ersin Yumer 3  Katerina Fragkiadaki 1

1 Carnegie Mellon University  Machine Learning Department

2 University of Pittsburgh  Department of Electrical and Computer Engineering

{htung  katef}@cs.cmu.edu  hst11@pitt.edu yumer@adobe.com

3 Adobe Research

Abstract

Current state-of-the-art solutions for motion capture from a single camera are
optimization driven: they optimize the parameters of a 3D human model so that
its re-projection matches measurements in the video (e.g. person segmentation 
optical ﬂow  keypoint detections etc.). Optimization models are susceptible to
local minima. This has been the bottleneck that forced using clean green-screen
like backgrounds at capture time  manual initialization  or switching to multiple
cameras as input resource. In this work  we propose a learning based motion capture
model for single camera input. Instead of optimizing mesh and skeleton parameters
directly  our model optimizes neural network weights that predict 3D shape and
skeleton conﬁgurations given a monocular RGB video. Our model is trained using
a combination of strong supervision from synthetic data  and self-supervision from
differentiable rendering of (a) skeletal keypoints  (b) dense 3D mesh motion  and
(c) human-background segmentation  in an end-to-end framework. Empirically
we show our model combines the best of both worlds of supervised learning
and test-time optimization: supervised learning initializes the model parameters
in the right regime  ensuring good pose and surface initialization at test time 
without manual effort. Self-supervision by back-propagating through differentiable
rendering allows (unsupervised) adaptation of the model to the test data  and offers
much tighter ﬁt than a pretrained ﬁxed model. We show that the proposed model
improves with experience and converges to low-error solutions where previous
optimization methods fail.

1

Introduction

Detailed understanding of the human body and its motion from “in the wild" monocular setups
would open the path to applications of automated gym and dancing teachers  rehabilitation guidance 
patient monitoring and safer human-robot interactions. It would also impact the movie industry
where character motion capture (MOCAP) and retargeting still requires tedious labor effort of artists
to achieve the desired accuracy  or the use of expensive multi-camera setups and green-screen
backgrounds.
Most current motion capture systems are optimization driven and cannot beneﬁt from experience.
Monocular motion capture systems optimize the parameters of a 3D human model to match measure-
ments in the video (e.g.  person segmentation  optical ﬂow). Background clutter and optimization
difﬁculties signiﬁcantly impact tracking performance  leading prior work to use green screen-like
backdrops [5] and careful initializations. Additionally  these methods cannot leverage the data gener-
ated by laborious manual processes involved in motion capture  to improve over time. This means

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Self-supervised learning of motion capture. Given a video sequence and a set of 2D body
joint heatmaps  our network predicts the body parameters for the SMPL 3D human mesh model [25].
Neural networks weights are pretrained using synthetic data and ﬁnetuned using self-supervised losses
driven by differentiable keypoint  segmentation  and motion reprojection errors  against detected 2D
keypoints  2D segmentation and 2D optical ﬂow  respectively. By ﬁnetuning its parameters at test
time through self-supervised losses  the proposed model achieves signiﬁcantly higher level of 3D
reconstruction accuracy than pure supervised or pure optimization based models  which either do not
adapt at test time  or cannot beneﬁt from training data  respectively.

that each time a video needs to be processed  the optimization and manual efforts need to be repeated
from scratch.
We propose a neural network model for motion capture in monocular videos  that learns to map an
image sequence to a sequence of corresponding 3D meshes. The success of deep learning models lies
in their supervision from large scale annotated datasets [14]. However  detailed 3D mesh annotations
are tedious and time consuming to obtain  thus  large scale annotation of 3D human shapes in realistic
video input is currently unavailable. Our work bypasses lack of 3D mesh annotations in real videos
by combining strong supervision from large scale synthetic data of human rendered models  and self-
supervision from 3D-to-2D differentiable rendering of 3D keypoints  motion and segmentation  and
matching with corresponding detected quantities in 2D  in real monocular videos. Our self-supervision
leverages recent advances in 2D body joint detection [37; 9]  2D ﬁgure-ground segmentation [22] 
and 2D optical ﬂow [21]  each learnt using strong supervision from real or synthetic datasets  such as 
MPII [3]  COCO [24]  and ﬂying chairs [15]  respectively. Indeed  annotating 2D body joints is easier
than annotating 3D joints or 3D meshes  while optical ﬂow has proven to be easy to generalize from
synthetic to real data. We show how state-of-the-art models of 2D joints  optical ﬂow and 2D human
segmentation can be used to infer dense 3D human structure in videos in the wild  that is hard to
otherwise manually annotate. In contrast to previous optimization based motion capture works [8; 7] 
we use differentiable warping and differentiable camera projection for optical ﬂow and segmentation
losses  which allows our model to be trained end-to-end with standard back-propagation.
We use SMPL [25] as our dense human 3D mesh model. It consists of a ﬁxed number of vertices and
triangles with ﬁxed topology  where the global pose is controlled by relative angles between body
parts θ  and the local shape is controlled by mesh surface parameters β. Given the pose and surface
parameters  a dense mesh can be generated in an analytical (differentiable) form  which could then be
globally rotated and translated to a desired location. The task of our model is to reverse-engineer the
rendering process and predict the parameters of the SMPL model (θ and β)  as well as the focal length 
3D rotations and 3D translations in each input frame  provided an image crop around a detected
person.
Given 3D mesh predictions in two consecutive frames  we differentiably project the 3D motion
vectors of the mesh vertices  and match them against estimated 2D optical ﬂow vectors (Figure 1).
Differentiable motion rendering and matching requires vertex visibility estimation  which we perform
using ray casting integrated with our neural model for code acceleration. Similarly  in each frame 
3D keypoints are projected and their distances to corresponding detected 2D keypoints are penalized.
Last but not the least  differentiable segmentation matching using Chamfer distances penalizes under
and over ﬁtting of the projected vertices against 2D segmentation of the human foreground. Note that

2

1R1SMPLcamera re-projectionKeypoint re-projectionSegmentation re-projectionMotion re-projectiont1t21T12R22T2these re-projection errors are only on the shape rather than the texture by design  since our predicted
3D meshes are textureless.
We provide quantitative and qualitative results on 3D dense human shape tracking in SURREAL
[35] and H3.6M [22] datasets. We compare against the corresponding optimization versions  where
mesh parameters are directly optimized by minimizing our self-supervised losses  as well as against
supervised models that do not use self-supervision at test time. Optimization baselines easily get
stuck in local minima  and are very sensitive to initialization. In contrast  our learning-based MOCAP
model relies on supervised pretraining (on synthetic data) to provide reasonable pose initialization
at test time. Further  self-supervised adaptation achieves lower 3D reconstruction error than the
pretrained  non-adapted model. Last  our ablation highlights the complementarity of the three
proposed self-supervised losses.

2 Related Work

3D Motion capture 3D motion capture using multiple cameras (four or more) is a well studied
problem where impressive results are achieved with existing methods [17]. However  motion capture
from a single monocular camera is still an open problem even for skeleton-only capture/tracking.
Since ambiguities and occlusions can be severe in monocular motion capture  most approaches rely on
prior models of pose and motion. Earlier works considered linear motion models [16; 13]. Non-linear
priors such as Gaussian process dynamical models [34]  as well as twin Gaussian processes [6] have
also been proposed  and shown to outperform their linear counterparts. Recently  Bogo et al. [7]
presented a static image pose and 3D dense shape prediction model which works in two stages: ﬁrst  a
3D human skeleton is predicted from the image  and then a parametric 3D shape is ﬁt to the predicted
skeleton using an optimization procedure  during which the skeleton remains unchanged. Instead  our
work couples 3D skeleton and 3D mesh estimation in an end-to-end differentiable framework  via
test-time adaptation.

3D human pose estimation Earlier work on 3D pose estimation considered optimization methods
and hard-coded anthropomorphic constraints (e.g.  limb symmetry) to ﬁght ambiguity during 2D-
to-3D lifting [28]. Many recent works learn to regress to 3D human pose directly given an RGB
image [27] using deep neural networks and large supervised training sets [22]. Many have explored
2D body pose as an intermediate representation [11; 38]  or as an auxiliary task in a multi-task
setting [32; 38; 39]  where the abundance of labelled 2D pose training examples helps feature
learning and complements limited 3D human pose supervision  which requires a Vicon system and
thus is restricted to lab instrumented environments. Rogez and Schmid [29] obtain large scale
RGB to 3D pose synthetic annotations by rendering synthetic 3D human models against realistic
backgrounds [29]  a dataset also used in this work.

Deep geometry learning Our differentiable renderer follows recent works that integrate deep
learning and geometric inference [33]. Differentiable warping [23; 26] and backpropable camera
projection [39; 38] have been used to learn 3D camera motion [40] and joint 3D camera and 3D
object motion [30] in an end-to-end self-supervised fashion  minimizing a photometric loss. Garg et
al. [18]learns a monocular depth predictor  supervised by photometric error  given a stereo image
pair with known baseline as input. The work of [19] contributed a deep learning library with many
geometric operations including a backpropable camera projection layer  similar to the one used in
Yan et al. [39] and Wu et al. [38]’s cameras  as well as Garg et al.’s depth CNN [18].

3 Learning Motion Capture

The architecture of our network is shown in Figure 1. We use SMPL as the parametrized model of 3D
human shape  introduced by Loper et al. [25]. SMPL is comprised of parameters that control the yaw 
pitch and roll of body joints  and parameters that control deformation of the body skin surface. Let θ 
β denote the joint angle and surface deformation parameters  respectively. Given these parameters  a
ﬁxed number (n = 6890) of 3D mesh vertex coordinates are obtained using the following analytical
expression  where Xi ∈ R3 stands for the 3D coordinates of the ith vertex in the mesh:

Xi = ¯Xi +

βmsm i +

(Tn(θ) − Tn(θ∗))pn i

(1)

(cid:88)

(cid:88)

m

n

3

Figure 2: Differentiable rendering of body joints (left)  segmentation (middle) and mesh vertex
motion (right).

where ¯Xi ∈ R3 is the nominal rest position of vertex i  βm is the blend coefﬁcient for the skin surface
blendshapes  sm i ∈ R3 is the element corresponding to ith vertex of the mth skin surface blendshape 
pn i ∈ R3 is the element corresponding to ith vertex of the nth skeletal pose blendshape  Tn(θ) is a
function that maps the nth pose blendshape to a vector of concatenated part relative rotation matrices 
and Tn(θ∗) is the same for the rest pose θ∗. Note the expression in Eq. 1 is differentiable.
Our model  given an image crop centered around a person detection  predicts parameters β and θ of
the SMPL 3D human mesh. Since annotations of 3D meshes are very tedious and time consuming
to obtain  our model uses supervision from a large dataset of synthetic monocular videos  and self-
supervision with a number of losses that rely on differentiable rendering of 3d keypoints  segmentation
and vertex motion  and matching with their 2D equivalents. We detail supervision of our model
below.

Paired supervision from synthetic data We use the synthetic Surreal dataset [35] that contains
monocular videos of human characters performing activities against 2D image backgrounds. The
synthetic human characters have been generated using the SMPL model  and animated using Human
H3.6M dataset [22]. Texture is generated by directly coloring the mesh vertices  without actual
3D cloth simulation. Since values for β and θ are directly available in this dataset  we use them to
pretrain the θ and β branches of our network using a standard supervised regression loss.

3.1 Self-supervision through differentiable rendering

Self-supervision in our model is based on 3D-to-2D rendering and consistency checks against 2D
estimates of keypoints  segmentation and optical ﬂow. Self-supervision can be used at both train and
test time  for adapting our model’s weights to the statistics of the test set.

Keypoint re-projection error Given a static image  predictions of 3D body joints of the depicted
person should match  when projected  corresponding 2D keypoint detections. Such keypoint re-
projection error has been used already in numerous previous works [38; 39]. Our model predicts a
dense 3D mesh instead of a skeleton. We leverage the linear relationship that relates our 3D mesh
vertices to 3D body joints:

(2)
Let X ∈ R4×n denote the 3D coordinates of the mesh vertices in homogeneous coordinates (with
a small abuse of notation since it is clear from the context)  where n the number of vertices. For
estimating 3D-to-2D projection  our model further predicts focal length  rotation of the camera and

Xkpt

(cid:124)

= A · X

(cid:124)

4

camerat1t2Distance mapsThresholdSegmentation(SM)Chamferdistance maps(CM) (SI)(CI) x2d~x2du  vu  v~~matchmatch by differentiable interpolationSM x CI + SI x CM → 0xKPTxKPT~translation of the 3D mesh off the center of the image  in case the root node of the 3D mesh is
not exactly placed at the center of the image crop. We do not predict translation in the z direction
(perpendicular to the image plane)  as the predicted focal length accounts for scaling of the person
ﬁgure. For rotation  we predict Euler rotation angles α  β  γ so that the 3D rotation of the camera
reads R = Rx(α)Ry(β)Rz
t (γ)  where Rx(θ) denotes rotation around the x-axis by angle θ  here in
homogeneous coordinates. The re-projection equation for the kth keypoint then reads:

kpt = P ·(cid:16)

xk

R · Xk

kpt + T

(cid:17)

(3)

where P = diag([f
[Tx Ty
then reads:

0

is the predicted camera projection matrix and T =
0]T handles small perturbations in object centering. Keypoint reprojection error

1 0]

f

Lkpt = (cid:107)xkpt − ˜xkpt(cid:107)2
2 

(4)

and ˜xkpt are ground-truth or detected 2D keypoints. Since 3D mesh vertices are related to β  θ
predictions using Eq. 1  re-projection error minimization updates the neural parameters for β  θ
estimation.

Motion re-projection error Given a pair of frames  3D mesh vertex displacements from one frame
to the next should match  when projected  corresponding 2D optical ﬂow vectors  computed from
the corresponding RGB frames. All Structure-from-Motion (SfM) methods exploit such motion
re-projection error in one way or another: the estimated 3D pointcloud in time when projected should
match 2D optical ﬂow vectors in [2]  or multiframe 2D point trajectories in [31]. Though previous
SfM models use motion re-projection error to optimize 3D coordinates and camera parameters directly
[2]  here we use it to optimize neural network parameters  that predict such quantities  instead.
Motion re-projection error estimation requires visibility of the mesh vertices in each frame. We
implement visibility inference through ray casting for each example and training iteration in Tensor
Flow and integrate it with our neural network model  which accelerates by ten times execution time 
as opposed to interfacing with raycasting in OpenGL. Vertex visibility inference does not need to be
differentiable: it is used only to mask motion re-projection loss for invisible vertices. Since we are
only interested in visibility rather than complex rendering functionality  ray casting boils down to
detecting the ﬁrst mesh facet to intersect with the straight line from the image projected position of
the center of a facet to its 3D point. If the intercepted facet is the same as the one which the ray is
cast from  we denote that facet as visible since there is no occluder between that facet and the image
plane. We provide more details for the ray casting reasoning in the experiment section. Vertices that
constructs these visible facet are treated as visible. Let vi ∈ {0  1}  i = 1··· n denote visibilities of
mesh vertices.
Given two consecutive frames I1  I2  let β1  θ1  R1  T1  β2  θ2  R2  T2 denote corresponding predic-

   i = 1··· n  and
   i = 1··· n using Eq. 1. The 3D mesh vertices are mapped to corresponding pixel

tions from our model. We obtain corresponding 3D pointclouds  Xi

X i

X i

1
Y i
1
Z i
1

1 =

Xi

2 =

2 − yi

1  yi
2 − xi

2  yi
1)  i = 1··· n.

1)  i = 1··· n  (xi
1  yi

2)  i = 1··· n  using the camera projection equation (Eq.
coordinates (xi
3). Thus the predicted 2D body ﬂow resulting from the 3D motion of the corresponding meshes is
(ui  vi) = (xi
Let OF = (˜u  ˜v) denote the 2D optical ﬂow ﬁeld estimated with an optical ﬂow method  such as
the state-of-the-art deep neural ﬂow of [21]. Let OF(xi
1) denote the optical ﬂow at a potentially
1  obtained from the pixel centered optical ﬂow ﬁeld OF through differentiable
subpixel location xi
bilinear interpolation (differentiable warping) [23]. Then  the motion re-projection error reads:
n(cid:88)

1  yi

1  yi

1

(cid:1)

vi(cid:0)(cid:107)ui(xi

1) − ˜u(xi

1  yi

1)(cid:107)1 + (cid:107)vi(xi

1) − ˜v(xi

1  yi

1  yi

1)(cid:107)1

1  yi

Lmotion =

1T v

i

2
Y i
2
Z i
2

5

Segmentation re-projection error Given a static image  the predicted 3D mesh for the depicted
person should match  when projected  the corresponding 2D ﬁgure-ground segmentation mask.
Numerous 3D shape reconstruction methods have used such segmentation consistency constraint
[36; 2; 4]  but again  in an optimization as opposed to learning framework.
Let S I ∈ {0  1}w×h denote the 2D ﬁgure-ground binary image segmentation  supplied by ground-
truth  background subtraction or predicted by a ﬁgure-ground neural network segmenter [20]. Our
segmentation re-projection loss measures how well the projected mesh mask ﬁts the image seg-
mentation S I by penalizing non-overlapping pixels by the shortest distance to the projected model
segmentation SM = {x2d}. For this purpose Chamfer distance maps CI for the image segmentation
S I and Chamfer distance maps CM for the model projected segmentation SM are calculated. The
loss then reads:

Lseg = SM ⊗ CI + S I ⊗ CM  

where ⊗ denotes pointwise multiplication. Both terms are necessary to prevent under of over
coverage of the model segmentation over the image segmentation. For the loss to be differentiable
we cannot use distance transform for efﬁcient computation of Chamfer maps. Rather  we brute
force its computation by calculating the shortest distance of each pixel to the model segmentation
2d  i ∈ 1··· n denote the set of model projected vertex pixel coordinates and
and the inverse. Let xi
seg  p ∈ 1··· m denote the set of pixel centered coordinates that belong to the foreground of the 2D
xp
segmentation map S I:

Lseg-proj =

n(cid:88)
(cid:124)

i=1

m(cid:88)
(cid:124)

p

+

min

i

min

p

(cid:107)xi

2d − xp
(cid:123)(cid:122)

seg(cid:107)2
(cid:125)

2

prevent over-coverage

(cid:107)xp

seg − xi
(cid:123)(cid:122)

2d(cid:107)2
(cid:125)
2.

prevent under-coverage

(5)

The ﬁrst term ensures the model projected segmentation is covered by the image segmentation  while
the second term ensures that model projected segmentation covers well the image segmentation. To
lower the memory requirements we use half of the image input resolution.

4 Experiments

We test our method on two datasets: Surreal [35] and H3.6M [22]. Surreal is currently the largest
synthetic dataset for people in motion. It contains short monocular video clips depicting human
characters performing daily activities. Ground-truth 3D human meshes are readily available. We split
the dataset into train and test video sequences. Human3.6M (H3.6M) is the largest real video dataset
with annotated 3D human skeletons. It contains videos of actors performing activities and provides
annotations of body joint locations in 2D and 3D at every frame  recorded through a Vicon system. It
does not provide dense 3D ground-truth though.
Our model is ﬁrst trained using supervised skeleton and surface parameters in the training set of the
Surreal dataset. Then  it is self-supervised using differentiable rendering and re-projection error mini-
mization at two test sets  one in the Surreal dataset  and one in H3.6M. For self-supervision  we use
ground-truth 2D keypoints and segmentations in both datasets  Surreal and H3.6M. The segmentation
mask in Surreal is very accurate while in H3.6M is obtained using background subtraction and can be
quite inaccurate  as you can see in Figure 4. Our model reﬁnes such initially inaccurate segmentation
mask. The 2D optical ﬂows for dense motion matching are obtained using FlowNet2.0 [21] in both
datasets. We do not use any 3D ground-truth supervision in H3.6M as our goal is to demonstrate
successful domain transfer of our model  from SURREAL to H3.6M. We measure the quality of
the predicted 3D skeletons in both datasets  and we measure the quality of the predicted dense 3D
meshes in Surreal  since only there it is available.

Evaluation metrics Given predicted 3D body joint locations of K = 32 keypoints Xk
kpt  k =
1··· K and corresponding ground-truth 3D joint locations ˜Xk
kpt  k = 1··· K  we deﬁne the per-joint
kpt(cid:107)2 similar to previous works [41]. We also deﬁne
error of each example as 1
K
the reconstruction error of each example as the 3D per-joint error up to a 3D translation T (3D

(cid:80)K
k=1 (cid:107)Xk

kpt − ˜Xk

6

(cid:80)K
k=1 (cid:107)(Xk

1
K

kpt + T ) − ( ˜Xk

(cid:80)n
i=1 (cid:107)Xi − ˜Xi(cid:107)2.

kpt)(cid:107)2 We deﬁne the
rotation should still be predicted correctly): minT
surface error of each example to be the per-joint error when considering all the vertices of the 3D
mesh: 1
n
We compare our learning based model against two baselines: (1) Pretrained  a model that uses
only supervised training from synthetic data  without self-supervised adaptation. This baseline is
similar to the recent work of [12]. (2) Direct optimization  a model that uses our differentiable
self-supervised losses  but instead of optimizing neural network weights  optimizes directly over body
mesh parameters (θ  β)  rotation (R)  translation (T )  and focal length f. We use standard gradient
descent as our optimization method. We experiment with varying amount of supervision during
initialization of our optimization baseline: random initialization  using ground-truth 3D translation 
using ground-truth rotation and using ground-truth theta angles (to estimate the surface parameters).
Tables 1 and 2 show the results of our model and baselines for the different evaluation metrics. The
learning based self-supervised model outperforms both the pretrained model  that does not exploit
adaptation through differentiable rendering and consistency checks  as well as direct optimization
baselines  sensitive to initialization mistakes.

Ablation In Figure 3 we show the 3D keypoint reconstruction error after self-supervised ﬁnetuning
using different combinations of self-supervised losses. A model self-supervised by the keypoint
re-projection error (Lkpt) alone does worse than model using both keypoint and segmentation re-
projection error (Lkpt+Lseg). Models trained using all three proposed losses (keypoint  segmentation
and dense motion re-projection error (Lkpt+Lseg+Lmotion) outperformes the above two. This shows
the complementarity and importance of all the proposed losses.

Optimization

Optimization + ˜R

Optimization + ˜R + ˜T

Pretrained

Pretrained+Self-Sup

surface error (mm)

346.5
301.1
272.8
119.4
74.5

per-joint error (mm)

532.8
222.0
206.6
101.6
64.4

recon. error (mm)

1320.1
294.9
205.5
351.3
203.9

Table 1: 3D mesh prediction results in Surreal [35]. The proposed model (pretrained+self-
supervised) outperforms both optimization based alternatives  as well as pretrained models using
supervised regression  that do not adapt to the test data. We use a superscript ˜· to denote ground-truth
information provided at initialization of our optimization based baseline.

per-joint error

recon. error

(mm)
562.4
125.6
98.4

(mm)
883.1
303.5
145.8

Optimization
Pretrained

Pretrained+Self-Sup
Table 2: 3D skeleton prediction results on H3.6M
[22]. The proposed model (pretrained+self-supervised)
outperforms both an optimization based baseline  as
well as a pretrained model. Self-supervised learning
through differentiable rendering allows our model to
adapt effectively across domains (Surreal to H3.6M) 
while the ﬁxed pretrained baseline cannot. Dense 3D
surface ground-truth is not available and thus cannot be
measured in H3.6M

Figure 3: 3D reconstruction error dur-
ing purely unsupervised ﬁnetuning
under different self-supervised losses.
(Lk ≡ Lkpt: Keypoint re-projection
error; LS≡ Lseg: Segmentation re-
projection error LM≡ Lmotion: Dense
motion re-projection error ). All losses
contribute to 3D error reduction.

Discussion We have shown that a combination of supervised pretraining and unsupervised adapta-
tion is beneﬁcial for accurate 3D mesh prediction. Learning based self-supervision combines the best
of both worlds of supervised learning and test time optimization: supervised learning initializes the
learning parameters in the right regime  ensuring good pose initialization at test time  without manual

7

recon. errorLkLk + LsLk + Ls + LM0.280.260.240.220.2050.0keffort. Self-supervision through differentiable rendering allows adaptation of the model to test data 
thus allows much tighter ﬁtting that a pretrained model with “frozen" weights at test time. Note that
overﬁtting in that sense is desirable. We want our predicted 3D mesh to ﬁt as tight as possible to our
test set  and improve tracking accuracy with minimal human intervention.

Implementation details Our model architecture consists of 5 convolution blocks. Each block
contains two convolutional layers with ﬁlter size 5 × 5 (stride 2) and 3 × 3 (stride 1)  followed by

Figure 4: Qualitative results of 3D mesh prediction. In the top four rows  we show predictions in
Surreal and in the bottom four from H3.6M. Our model handles bad segmentation input masks in
H3.6M thanks to supervision from multiple rendering based losses. A byproduct of our 3D mesh
model is improved 2D person segmentation (column 6).

8

input 1input 2predicted meshpredicted 2d projectionsegmentation groundtruthpredicted ﬂowpredicted maskbatch normalization and leaky relu activation. The ﬁrst block contains 64 channels  and we double
size after each block. On top of these blocks  we add 3 fully connected layers and shrink the size of
the ﬁnal layer to match our desired outputs. Input image to our model is 128 × 128. The model is
trained with gradient descent optimizer with learning rate 0.0001 and is implemented in Tensorﬂow
v1.1.0 [1].
Chamfer distance: We obtain Chamfer distance map CI for an input image frame I using distance
transform with seed the image ﬁgure-ground segmentation mask S I. This assigns to every pixel in
CI the minimum distance to a pixel on the mask foreground. Next  we describe the differentiable
computation for CM used in our method. Let P = {x2d} denote a set of pixel coordinates for the
mesh’s visible projected points. For each pixel location p  we compute the minimum distance between
that pixel location and any pixel coordinate in P and obtain a distance map D ∈ Rw×h. Next  we
threshold the distance map D to get the Chamfer distance map CM and segmentation mask SM
where  for each pixel position p:

CM (p) = max(0.5  D(p))
SM (p) = min(0.5  D(p)) + δ(D(p) < 0.5) · 0.5 

(6)
(7)

and δ(·) is an indicator function.
Ray casting: We implemented a standard raycasting algorithm in TensorFlow to accelerate its
computation. Let r = (x  d) denote a casted ray  where x is the point where the ray casts from and d
is a normalized vector for the shooting direction. In our case  all the rays cast from the center of the
camera. For ease of explanation  we set x at (0 0 0). A facet f = (v0  v1  v2)  is determined as "hit" if
it satisﬁes the following three conditions : (1) the facet is not parallel to the casted ray  (2) the facet is
not behind the ray and (3) the ray passes through the triangle region formed by the three edges of the
facet. Given a facet f = (v0  v1  v2)  where vi denotes the ith vertex of the facet  the ﬁrst condition is
satisﬁed if the magnitude of the inner product between the ray cast direction d and the surface normal
of the facet f is large than some threshold . Here we set  to be 1e − 8. The second condition is
satisﬁed if the inner product between the ray cast direction d and the surface normal N  which is
deﬁned as the normalized cross product between v1 − v0 and v2 − v0  has the same sign as the inner
product between v0 on N. Finally  the last condition can be split into three sub-problems: given one
of the edges on the facet  whether the ray casts on the same side as the facet or not. First  we ﬁnd the
intersecting point p of the ray cast and the 2D plane expanded by the facet by the following equation:

p = x + d · < N  v0 >
< N  d >

(8)
where < · · > denotes inner product. Given an edge formed by vertices vi and vj  the ray casted is
determined to fall on the same side of the facet if the cross product between edge vi − vj and vector
p − vj has the same sign as the surface normal vector N. We examine this condition on all of the
three edges. If all the above conditions are satisﬁed  the facet is determined as hit by the ray cast.
Among the hit facets  we choose the one with the minimum distance to the origin as the visible facet
seen from the direction of the ray cast.

 

5 Conclusion

We have presented a learning based model for dense human 3D body tracking supervised by synthetic
data and self-supervised by differentiable rendering of mesh motion  keypoints  and segmentation 
and matching to their 2D equivalent quantities. We show that our model improves by using unlabelled
video data  which is very valuable for motion capture where dense 3D ground-truth is hard to annotate.
A clear direction for future work is iterative additive feedback [10] on the mesh parameters  for
achieving higher 3D reconstruction accuracy  and allowing learning a residual free form deformation
on top of the parametric SMPL model  again in a self-supervised manner. Extensions of our model
beyond human 3D shape would allow neural agents to learn 3D with experience as human do 
supervised solely by video motion.

9

References

[1] M. Abadi  A. Agarwal  P. Barham  E. Brevdo  Z. Chen  C. Citro  G. Corrado  A. Davis  J. Dean  M. Devin 
S. Ghemawat  I. Goodfellow  A. Harp  G. Irving  M. Isard  Y. Jia  R. Jozefowicz  L. Kaiser  M. Kudlur 
J. Levenberg  D. Mané  R. Monga  S. Moore  D. Murray  C. Olah  M. Schuster  J. Shlens  B. Steiner 
I. Sutskever  K. Talwar  P. Tucker  V. Vanhoucke  V. Vasudevan  F. Viégas  O. Vinyals  P. Warden  M. Wat-
tenberg  M. Wicke  Y. Yu  and X. Zheng. Tensorﬂow: Large-scale machine learning on heterogeneous
distributed systems  2015.

[2] T. Alldieck  M. Kassubeck  and M. A. Magnor. Optical ﬂow-based 3d human motion estimation from

monocular video. CoRR  abs/1703.00177  2017.

[3] M. Andriluka  L. Pishchulin  P. Gehler  and B. Schiele. 2d human pose estimation: New benchmark and
state of the art analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  June
2014.

[4] A. Balan  L. Sigal  M. J. Black  J. Davis  and H. Haussecker. Detailed human shape and pose from images.
In IEEE Conf. on Computer Vision and Pattern Recognition  CVPR  pages 1–8  Minneapolis  June 2007.
[5] L. Ballan and G. M. Cortelazzo. Marker-less motion capture of skinned models in a four camera set-up

using optical ﬂow and silhouettes. In 3DPVT  2008.

[6] L. Bo and C. Sminchisescu. Twin gaussian processes for structured prediction. International Journal of

Computer Vision  87(1):28–52  2010.

[7] F. Bogo  A. Kanazawa  C. Lassner  P. V. Gehler  J. Romero  and M. J. Black. Keep it SMPL: automatic

estimation of 3d human pose and shape from a single image. ECCV  2016  2016.

[8] T. Brox  B. Rosenhahn  D. Cremers  and H.-P. Seidel. High accuracy optical ﬂow serves 3-d pose tracking:

exploiting contour and ﬂow based constraints. In ECCV  2006.

[9] Z. Cao  T. Simon  S.-E. Wei  and Y. Sheikh. Realtime multi-person 2d pose estimation using part afﬁnity

ﬁelds. In CVPR  2017.

abs/1612.06524  2016.

[10] J. Carreira  P. Agrawal  K. Fragkiadaki  and J. Malik. Human pose estimation with iterative error feedback.

In arXiv preprint arXiv:1507.06550  2015.

[11] C. Chen and D. Ramanan. 3d human pose estimation = 2d pose estimation + matching. CoRR 

[12] W. Chen  H. Wang  Y. Li  H. Su  C. Tu  D. Lischinski  D. Cohen-Or  and B. Chen. Synthesizing training

images for boosting human 3d pose estimation. CoRR  abs/1604.02703  2016.

[13] K. Choo and D. J. Fleet. People tracking using hybrid monte carlo ﬁltering. In Computer Vision  2001.

ICCV 2001. Proceedings. Eighth IEEE International Conference on  volume 2  pages 321–328  2001.

[14] J. Deng  W. Dong  R. Socher  L.-J. Li  K. Li  and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image

Database. In CVPR09  2009.

[15] A. Dosovitskiy  P. Fischer  E. Ilg  P. Häusser  C. Hazirbas  V. Golkov  P. Smagt  D. Cremers  and T. Brox.

Flownet: Learning optical ﬂow with convolutional networks. In ICCV  2015.

[16] D. Fleet  A. Jepson  and T. El-Maraghi. Robust on-line appearance models for vision tracking. In Proc.

IEEE Conf. Computer Vision and Pattern Recognition  2001.

[17] J. Gall  C. Stoll  E. De Aguiar  C. Theobalt  B. Rosenhahn  and H.-P. Seidel. Motion capture using joint
skeleton tracking and surface estimation. In Computer Vision and Pattern Recognition  2009. CVPR 2009.
IEEE Conference on  pages 1746–1753. IEEE  2009.

[18] R. Garg  B. V. Kumar  G. Carneiro  and I. Reid. Unsupervised cnn for single view depth estimation:

Geometry to the rescue. In European Conference on Computer Vision  pages 740–756. Springer  2016.

[19] A. Handa  M. Bloesch  V. P˘atr˘aucean  S. Stent  J. McCormac  and A. Davison. gvnn: Neural network
library for geometric computer vision. In Computer Vision–ECCV 2016 Workshops  pages 67–82. Springer
International Publishing  2016.

[20] K. He  G. Gkioxari  P. Dollár  and R. B. Girshick. Mask R-CNN. CoRR  abs/1703.06870  2017.
[21] E. Ilg  N. Mayer  T. Saikia  M. Keuper  A. Dosovitskiy  and T. Brox. Flownet 2.0: Evolution of optical

ﬂow estimation with deep networks. CoRR  abs/1612.01925  2016.

[22] C. Ionescu  D. Papava  V. Olaru  and C. Sminchisescu. Human3.6m: Large scale datasets and predictive
methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and
Machine Intelligence  36(7):1325–1339  jul 2014.

[23] M. Jaderberg  K. Simonyan  A. Zisserman  and K. Kavukcuoglu. Spatial transformer networks. In NIPS 

2015.

2016.

[24] T. Lin  M. Maire  S. J. Belongie  L. D. Bourdev  R. B. Girshick  J. Hays  P. Perona  D. Ramanan  P. Dollár 

and C. L. Zitnick. Microsoft COCO: common objects in context. CoRR  abs/1405.0312  2014.

[25] M. Loper  N. Mahmood  J. Romero  G. Pons-Moll  and M. J. Black. SMPL: A skinned multi-person linear

model. ACM Trans. Graphics (Proc. SIGGRAPH Asia)  34(6):248:1–248:16  Oct. 2015.

[26] V. Patraucean  A. Handa  and R. Cipolla. Spatio-temporal video autoencoder with differentiable memory.

CoRR  abs/1511.06309  2015.

[27] G. Pavlakos  X. Zhou  K. G. Derpanis  and K. Daniilidis. Coarse-to-ﬁne volumetric prediction for

single-image 3d human pose. CoRR  abs/1611.07828  2016.

[28] V. Ramakrishna  T. Kanade  and Y. Sheikh. Reconstructing 3d Human Pose from 2d Image Landmarks.

Computer Vision–ECCV 2012  pages 573–586  2012.

[29] G. Rogez and C. Schmid. Mocap-guided data augmentation for 3d pose estimation in the wild. In NIPS 

[30] V. S.  R. S.  S. C.  S. R.  and F. K. Sfm-net: Learning of structure and motion from video. In arxiv  2017.

10

[31] C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: A factorization

method. Int. J. Comput. Vision  9(2):137–154  Nov. 1992.

[32] D. Tomè  C. Russell  and L. Agapito. Lifting from the deep: Convolutional 3d pose estimation from a

single image. CoRR  abs/1701.00295  2017.

[33] H. F. Tung  A. Harley  W. Seto  and K. Fragkiadaki. Adversarial inverse graphics networks: Learning

2d-to-3d lifting and image-to-image translation from unpaired supervision. ICCV  2017.

[34] R. Urtasun  D. Fleet  and P. Fua. Gaussian process dynamical models for 3d people tracking. In Proc. of

the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)  2006.

[35] G. Varol  J. Romero  X. Martin  N. Mahmood  M. J. Black  I. Laptev  and C. Schmid. Learning from

Synthetic Humans. In CVPR  2017.

[36] S. Vicente  J. Carreira  L. Agapito  and J. Batista. Reconstructing PASCAL VOC. In 2014 IEEE Conference
on Computer Vision and Pattern Recognition  CVPR 2014  Columbus  OH  USA  June 23-28  2014  pages
41–48  2014.

[37] S.-E. Wei  V. Ramakrishna  T. Kanade  and Y. Sheikh. Convolutional pose machines. In CVPR  2016.
[38] J. Wu  T. Xue  J. J. Lim  Y. Tian  J. B. Tenenbaum  A. Torralba  and W. T. Freeman. Single image 3D

interpreter network. In ECCV  2016.

[39] X. Yan  J. Yang  E. Yumer  Y. Guo  and H. Lee. Perspective transformer nets: Learning single-view 3d
object reconstruction without 3d supervision. In Advances in Neural Information Processing Systems 
pages 1696–1704  2016.

[40] T. Zhou  M. Brown  N. Snavely  and D. G. Lowe. Unsupervised learning of depth and ego-motion from

video. In arxiv  2017.

[41] X. Zhou  M. Zhu  G. Pavlakos  S. Leonardos  K. G. Derpanis  and K. Daniilidis. Monocap: Monocular

human motion capture using a CNN coupled with a geometric prior. CoRR  abs/1701.02354  2017.

11

,Hsiao-Yu Tung
Hsiao-Wei Tung
Ersin Yumer
Katerina Fragkiadaki