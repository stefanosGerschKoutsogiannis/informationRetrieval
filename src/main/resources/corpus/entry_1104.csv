2014,Consistent Binary Classification with Generalized Performance Metrics,Performance metrics for binary classification are designed to capture tradeoffs between four fundamental population quantities: true positives  false positives  true negatives and false negatives. Despite significant interest from theoretical and applied communities  little is known about either optimal classifiers or consistent algorithms for optimizing binary classification performance metrics beyond a few special cases. We consider a fairly large family of performance metrics given by ratios of linear combinations of the four fundamental population quantities. This family includes many well known binary classification metrics such as classification accuracy  AM measure  F-measure and the Jaccard similarity coefficient as special cases. Our analysis identifies the optimal classifiers as the sign of the thresholded conditional probability of the positive class  with a performance metric-dependent threshold. The optimal threshold can be constructed using simple plug-in estimators when the performance metric is a linear combination of the population quantities  but alternative techniques are required for the general case. We propose two algorithms for estimating the optimal classifiers  and prove their statistical consistency. Both algorithms are straightforward modifications of standard approaches to address the key challenge of optimal threshold selection  thus are simple to implement in practice. The first algorithm combines a plug-in estimate of the conditional probability of the positive class with optimal threshold selection. The second algorithm leverages recent work on calibrated asymmetric surrogate losses to construct candidate classifiers. We present empirical comparisons between these algorithms on benchmark datasets.,Consistent Binary Classiﬁcation with Generalized

Performance Metrics

Oluwasanmi Koyejo⇤
Department of Psychology 

Stanford University

sanmi@stanford.edu

Nagarajan Natarajan⇤

Department of Computer Science 

University of Texas at Austin
naga86@cs.utexas.edu

Pradeep Ravikumar

Department of Computer Science 

University of Texas at Austin

pradeepr@cs.utexas.edu

Inderjit S. Dhillon

Department of Computer Science 

University of Texas at Austin

inderjit@cs.utexas.edu

Abstract

Performance metrics for binary classiﬁcation are designed to capture tradeoffs be-
tween four fundamental population quantities: true positives  false positives  true
negatives and false negatives. Despite signiﬁcant interest from theoretical and
applied communities  little is known about either optimal classiﬁers or consis-
tent algorithms for optimizing binary classiﬁcation performance metrics beyond
a few special cases. We consider a fairly large family of performance metrics
given by ratios of linear combinations of the four fundamental population quanti-
ties. This family includes many well known binary classiﬁcation metrics such as
classiﬁcation accuracy  AM measure  F-measure and the Jaccard similarity coefﬁ-
cient as special cases. Our analysis identiﬁes the optimal classiﬁers as the sign of
the thresholded conditional probability of the positive class  with a performance
metric-dependent threshold. The optimal threshold can be constructed using sim-
ple plug-in estimators when the performance metric is a linear combination of
the population quantities  but alternative techniques are required for the general
case. We propose two algorithms for estimating the optimal classiﬁers  and prove
their statistical consistency. Both algorithms are straightforward modiﬁcations of
standard approaches to address the key challenge of optimal threshold selection 
thus are simple to implement in practice. The ﬁrst algorithm combines a plug-in
estimate of the conditional probability of the positive class with optimal threshold
selection. The second algorithm leverages recent work on calibrated asymmetric
surrogate losses to construct candidate classiﬁers. We present empirical compar-
isons between these algorithms on benchmark datasets.

1

Introduction

Binary classiﬁcation performance is often measured using metrics designed to address the short-
comings of classiﬁcation accuracy. For instance  it is well known that classiﬁcation accuracy is an
inappropriate metric for rare event classiﬁcation problems such as medical diagnosis  fraud detec-
tion  click rate prediction and text retrieval applications [1  2  3  4]. Instead  alternative metrics better
tuned to imbalanced classiﬁcation (such as the F1 measure) are employed. Similarly  cost-sensitive
metrics may useful for addressing asymmetry in real-world costs associated with speciﬁc classes. An
important theoretical question concerning metrics employed in binary classiﬁcation is the characteri-

⇤Equal contribution to the work.

1

zation of the optimal decision functions. For example  the decision function that maximizes the accu-
racy metric (or equivalently minimizes the “0-1 loss”) is well-known to be sign(P (Y = 1|x)1/2).
A similar result holds for cost-sensitive classiﬁcation [5]. Recently  [6] showed that the optimal de-
cision function for the F1 measure  can also be characterized as sign(P (Y = 1|x)  ⇤) for some
⇤ 2 (0  1). As we show in the paper  it is not a coincidence that the optimal decision function
for these different metrics has a similar simple characterization. We make the observation that the
different metrics used in practice belong to a fairly general family of performance metrics given by
ratios of linear combinations of the four population quantities associated with the confusion matrix.
We consider a family of performance metrics given by ratios of linear combinations of the four
population quantities. Measures in this family include classiﬁcation accuracy  false positive rate 
false discovery rate  precision  the AM measure and the F-measure  among others. Our analysis
shows that the optimal classiﬁers for all such metrics can be characterized as the sign of the thresh-
olded conditional probability of the positive class  with a threshold that depends on the speciﬁc
metric. This result uniﬁes and generalizes known special cases including the AM measure analysis
by Menon et al. [7]  and the F measure analysis by Ye et al. [6]. It is known that minimizing (con-
vex) surrogate losses  such as the hinge and the logistic loss  provably also minimizes the underlying
0-1 loss or equivalently maximizes the classiﬁcation accuracy [8]. This motivates the next question
we address in the paper: can one obtain algorithms that (a) can be used in practice for maximizing
metrics from our family  and (b) are consistent with respect to the metric? To this end  we propose
two algorithms for consistent empirical estimation of decision functions. The ﬁrst algorithm com-
bines a plug-in estimate of the conditional probability of the positive class with optimal threshold
selection. The second leverages the asymmetric surrogate approach of Scott [9] to construct candi-
date classiﬁers. Both algorithms are simple modiﬁcations of standard approaches that address the
key challenge of optimal threshold selection. Our analysis identiﬁes why simple heuristics such
as classiﬁcation using class-weighted loss functions and logistic regression with threshold search
are effective practical algorithms for many generalized performance metrics  and furthermore  that
when implemented correctly  such apparent heuristics are in fact asymptotically consistent.

Related Work. Binary classiﬁcation accuracy and its cost-sensitive variants have been studied
extensively. Here we highlight a few of the key results. The seminal work of [8] showed that mini-
mizing certain surrogate loss functions enables us to control the probability of misclassiﬁcation (the
expected 0-1 loss). An appealing corollary of the result is that convex loss functions such as the
hinge and logistic losses satisfy the surrogacy conditions  which establishes the statistical consis-
tency of the resulting algorithms. Steinwart [10] extended this work to derive surrogates losses for
other scenarios including asymmetric classiﬁcation accuracy. More recently  Scott [9] characterized
the optimal decision function for weighted 0-1 loss in cost-sensitive learning and extended the risk
bounds of [8] to weighted surrogate loss functions. A similar result regarding the use of a threshold
different than 1/2  and appropriately rebalancing the training data in cost-sensitive learning  was
shown by [5]. Surrogate regret bounds for proper losses applied to class probability estimation
were analyzed by Reid and Williamson [11] for differentiable loss functions. Extensions to the
multi-class setting have also been studied (for example  Zhang [12] and Tewari and Bartlett [13]).
Analysis of performance metrics beyond classiﬁcation accuracy is limited. The optimal classiﬁer
remains unknown for many binary classiﬁcation performance metrics of interest  and few results
exist for identifying consistent algorithms for optimizing these metrics [7  6  14  15]. Of particular
relevance to our work are the AM measure maximization by Menon et al. [7]  and the F measure
maximization by Ye et al. [6].

2 Generalized Performance Metrics

Let X be either a countable set  or a complete separable metric space equipped with the standard
Borel -algebra of measurable sets. Let X 2X and Y 2{ 0  1} represent input and output random
variables respectively. Further  let ⇥ represent the set of all classiﬁers ⇥= {✓ : X 7! [0  1]}.
We assume the existence of a ﬁxed unknown distribution P  and data is generated as iid. samples
(X  Y ) ⇠ P. Deﬁne the quantities: ⇡ = P(Y = 1) and (✓) = P(✓ = 1).
The components of the confusion matrix are the fundamental population quantities for binary classi-
ﬁcation. They are the true positives (TP)  false positives (FP)  true negatives (TN) and false negatives

2

(FN)  given by:

TP(✓  P) = P(Y = 1 ✓ = 1) 
FN(✓  P) = P(Y = 1 ✓ = 0) 

FP(✓  P) = P(Y = 0 ✓ = 1) 
TN(✓  P) = P(Y = 0 ✓ = 0).

(1)

These quantities may be further decomposed as:
FP(✓  P) = (✓)  TP(✓) 
Let L :⇥ ⇥ P 7! R be a performance metric of interest. Without loss of generality  we assume
that L is a utility metric  so that larger values are better. The Bayes utility L⇤ is the optimal value
of the performance metric  i.e.  L⇤ = sup✓2⇥ L(✓  P). The Bayes classiﬁer ✓⇤ is the classiﬁer that
optimizes the performance metric  so L⇤ = L(✓⇤)  where:

FN(✓  P) = ⇡  TP(✓)  TN(✓  P) = 1  (✓)  ⇡ + TP(✓).

(2)

✓⇤ = arg max

✓2⇥ L(✓  P).

We consider a family of classiﬁcation metrics computed as the ratio of linear combinations of these
fundamental population quantities (1). In particular  given constants (representing costs or weights)
{a11  a10  a01  a00  a0} and {b11  b10  b01  b00  b0}  we consider the measure:
a0 + a11TP + a10FP + a01FN + a00TN
b0 + b11TP + b10FP + b01FN + b00TN

L(✓  P) =

(3)

where  for clarity  we have suppressed dependence of the population quantities on ✓ and P. Examples
of performance metrics in this family include the AM measure [7]  the F measure [6]  the Jaccard
similarity coefﬁcient (JAC) [16] and Weighted Accuracy (WA):

AM =

1

2✓TP

⇡

+

TN

1  ⇡◆ =

TP

JAC =

TP + FN + FP =

(1  ⇡)TP + ⇡TN
TP
 + FN   WA =

2⇡(1  ⇡)
TP
⇡ + FP =

  F =

(1 + 2)TP

(1 + 2)TP + 2FN + FP =
w1TP + w2TN

(1 + 2)TP
2⇡ + 

 

w1TP + w2TN + w3FP + w4FN .

Note that we allow the constants to depend on P. Other examples in this class include commonly
used ratios such as the true positive rate (also known as recall) (TPR)  true negative rate (TNR) 
precision (Prec)  false negative rate (FNR) and negative predictive value (NPV):

TPR =

TP

TP + FN   TNR =

TN

FP + TN   Prec =

TP

TP + FP   FNR =

FN

FN + TP   NPV =

TN

TN + FN .

Interested readers are referred to [17] for a list of additional metrics in this class.
By decomposing the population measures (1) using (2) we see that any performance metric in the
family (3) has the equivalent representation:

L(✓) =

c0 + c1TP(✓) + c2(✓)
d0 + d1TP(✓) + d2(✓)

with the constants:

(4)

and

c0 = a01⇡ + a00  a00⇡ + a0 
d0 = b01⇡ + b00  b00⇡ + b0 

c1 = a11  a10  a01 + a00 
d1 = b11  b10  b01 + b00 

c2 = a10  a00
d2 = b10  b00.

Thus  it is clear from (4) that the family of performance metrics depends on the classiﬁer ✓ only
through the quantities TP(✓) and (✓).

Optimal Classiﬁer

We now characterize the optimal classiﬁer for the family of performance metrics deﬁned in (4). Let
⌫ represent the dominating measure on X . For the rest of this manuscript  we make the following
assumption:
Assumption 1. The marginal distribution P(X) is absolutely continuous with respect to the domi-
nating measure ⌫ on X so there exists a density µ that satisﬁes dP = µd⌫.

3

To simplify notation  we use the standard d⌫(x) = dx. We also deﬁne the conditional proba-
bility ⌘x = P(Y = 1|X = x). Applying Assumption 1  we can expand the terms TP(✓) =
Rx2X
✓(x)µ(x)dx  so the performance metric (4) may be repre-
sented as:

⌘x✓(x)µ(x)dx and (✓) = Rx2X
L(✓  P) =

(c1⌘x + c2)✓(x)µ(x)dx
(d1⌘x + d2)✓(x)µ(x)dx

.

Our ﬁrst main result identiﬁes the Bayes classiﬁer for all utility functions in the family (3)  showing
that they take the form ✓⇤(x) = sign(⌘x  ⇤)  where ⇤ is a metric-dependent threshold  and the
sign function is given by sign : R 7! {0  1} as sign(t) = 1 if t  0 and sign(t) = 0 otherwise.
Theorem 2. Let P be a distribution on X⇥ [0  1] that satisﬁes Assumption 1  and let L be a perfor-
mance metric in the family (3). Given the constants {c0  c1  c2} and {d0  d1  d2}  deﬁne:

c0 +Rx2X
d0 +Rx2X

⇤ =

d2L⇤  c2
c1  d1L⇤

.

(5)

1. When c1 > d1L⇤  the Bayes classiﬁer ✓⇤ takes the form ✓⇤(x) = sign(⌘x  ⇤)
2. When c1 < d1L⇤  the Bayes classiﬁer takes the form ✓⇤(x) = sign(⇤  ⌘x)

The proof of the theorem involves examining the ﬁrst-order optimality condition (see Appendix B).
Remark 3. The speciﬁc form of the optimal classiﬁer depends on the sign of c1  d1L⇤  and L⇤ is
often unknown. In practice  one can often estimate loose upper and lower bounds of L⇤ to determine
the classiﬁer.

A number of useful results can be evaluated directly as instances of Theorem 2. For the F measure 
we have that c1 = 1 + 2 and d2 = 1 with all other constants as zero. Thus  ⇤F = L⇤
1+2 . This
matches the optimal threshold for F1 metric speciﬁed by Zhao et al. [14]. For precision  we have that
c1 = 1  d2 = 1 and all other constants are zero  so ⇤Prec = L⇤. This clariﬁes the observation that in
practice  precision can be maximized by predicting only high conﬁdence positives. For true positive
rate (recall)  we have that c1 = 1  d0 = ⇡ and other constants are zero  so ⇤TPR = 0 recovering the
known result that in practice  recall is maximized by predicting all examples as positives. For the
Jaccard similarity coefﬁcient c1 = 1  d1 = 1  d2 = 1  d0 = ⇡ and other constants are zero  so
⇤JAC = L⇤
1+L⇤ .
When d1 = d2 = 0  the generalized metric is simply a linear combination of the four fundamental
quantities. With this form  we can then recover the optimal classiﬁer outlined by Elkan [5] for cost
sensitive classiﬁcation.
Corollary 4. Let P be a distribution on X⇥ [0  1] that satisﬁes Assumption 1  and let L be a
performance metric in the family (3). Given the constants {c0  c1  c2} and {d0  d1 = 0  d2 = 0}  the
optimal threshold (5) is ⇤ =  c2
Classiﬁcation accuracy is in this family  with c1 = 2  c2 = 1  and it is well-known that ⇤ACC = 1
2.
Another case of interest is the AM metric  where c1 = 1  c2 = ⇡  so ⇤AM = ⇡  as shown in Menon
et al. [7].

c1

.

3 Algorithms

The characterization of the Bayes classiﬁer for the family of performance metrics (4) given in The-
orem 2 enables the design of practical classiﬁcation algorithms with strong theoretical properties.
In particular  the algorithms that we propose are intuitive and easy to implement. Despite their
simplicity  we show that the proposed algorithms are consistent with respect to the measure of
interest; a desirable property for a classiﬁcation algorithm. We begin with a description of the
algorithms  followed by a detailed analysis of consistency. Let {Xi  Yi}n
training
instances drawn from a ﬁxed unknown distribution P. For a given ✓ : X!{ 0  1}  we deﬁne the
i=1 ✓(Xi)Yi 
following empirical quantities based on their population analogues: TPn(✓) = 1
nPn
i=1 ✓(Xi). It is clear that TPn(✓) n!1! TP(✓; P) and n(✓) n!1! (✓; P).
and n(✓) = 1

nPn

i=1 denote iid.

4

Consider the empirical measure:

Ln(✓) =

c1TPn(✓) + c2n(✓) + c0
d1TPn(✓) + d2n(✓) + d0

 

(6)

d1

d1

so ✓⇤(x) = sign(⌘x  ⇤). The case where L⇤ > c1

corresponding to the population measure L(✓; P) in (4). It is expected that Ln(✓) will be close to
the L(✓; P) when the sample is sufﬁciently large (see Proposition 8). For the rest of this manuscript 
is solved identically.
we assume that L⇤  c1
Our ﬁrst approach (Two-Step Expected Utility Maximization) is quite intuitive (Algorithm 1): Ob-
tain an estimator ˆ⌘x for ⌘x = P(Y = 1|x) by performing ERM on the sample using a proper loss
function [11]. Then  maximize Ln deﬁned in (6) with respect to the threshold  2 (0  1). The
optimization required in the third step is one dimensional  thus a global minimizer can be computed
efﬁciently in many cases [18]. In experiments  we use (regularized) logistic regression on a training
sample to obtain ˆ⌘.

Algorithm 1: Two-Step EUM

Input: Training examples S = {Xi  Yi}n
1. Split the training data S into two sets S1 and S2.
2. Estimate ˆ⌘x using S1  deﬁne ˆ✓ = sign(ˆ⌘x  )
3. Compute ˆ = arg max2(0 1) Ln(ˆ✓) on S2.
Return: ˆ✓ˆ

i=1 and the utility measure L.

Our second approach (Weighted Empirical Risk Minimization) is based on the observation that
empirical risk minimization (ERM) with suitably weighted loss functions yields a classiﬁer that
thresholds ⌘x appropriately (Algorithm 2). Given a convex surrogate `(t  y) of the 0-1 loss  where t
is a real-valued prediction and y 2{ 0  1}  the -weighted loss is given by [9]:

`(t  y) = (1  )1{y=1}`(t  1) + 1{y=0}`(t  0).

Denote the set of real valued functions as ; we then deﬁne ˆ✓ as:

ˆ = arg min
2

1
n

nXi=1

`((Xi)  Yi)

(7)

then set ˆ✓(x) = sign( ˆ(x)). Scott [9] showed that such an estimated ˆ✓ is consistent with ✓ =
sign(⌘x  ). With the classiﬁer deﬁned  maximize Ln deﬁned in (6) with respect to the threshold
 2 (0  1).
Algorithm 2: Weighted ERM

Input: Training examples S = {Xi  Yi}n
1. Split the training data S into two sets S1 and S2.
2. Compute ˆ = arg max2(0 1) Ln(ˆ✓) on S2.
Sub-algorithm: Deﬁne ˆ✓(x) = sign( ˆ(x)) where ˆ(x) is computed using (7) on S1.
Return: ˆ✓ˆ

i=1  and the utility measure L.

Remark 5. When d1 = d2 = 0  the optimal threshold does not depend on L⇤ (Corollary 4). We
may then employ simple sample-based plugin estimates ˆS.

A beneﬁt of using such plugin estimates is that the classiﬁcation algorithms can be simpliﬁed while
maintaining consistency. Given such a sample-based plugin estimate ˆS  Algorithm 1 then reduces
= sign(ˆ⌘x  ˆS)  Algorithm 2 reduces to a single ERM (7) to
to estimating ˆ⌘x  and then setting ˆ✓ˆS
estimate ˆˆS
(x)  and then setting ˆ✓ˆS
(x) = sign( ˆˆS
(x)). In the case of AM measure  the threshold
is given by ⇤ = ⇡. A consistent estimator for ⇡ is all that is required (see [7]).

5

3.1 Consistency of the proposed algorithms
p! 0 i.e.  for
An algorithm is said to be L-consistent if the learned classiﬁer ˆ✓ satisﬁes L⇤ L (ˆ✓)
every ✏> 0  P(|L⇤ L (ˆ✓)| <✏ )! 1  as n!1.
We begin the analysis from the simplest case when ⇤ is independent of L⇤ (Corollary 4). The
following proposition  which generalizes Lemma 1 of [7]  shows that maximizing L is equivalent to
minimizing ⇤-weighted risk. As a consequence  it sufﬁces to minimize a suitable surrogate loss `⇤
on the training data to guarantee L-consistency.
Proposition 6. Assume ⇤ 2 (0  1) and ⇤ is independent of L⇤  but may depend on the distribution
P. Deﬁne ⇤-weighted risk of a classiﬁer ✓ as

R⇤(✓) = E(x y)⇠P⇥(1  ⇤)1{y=1}1{✓(x)=0} + ⇤1{y=0}1{✓(x)=1}⇤ 

then  R⇤(✓)  min

✓

R⇤(✓) =

(L⇤ L (✓)).

1
c1

The proof is simple  and we defer it to Appendix B. Note that the key consequence of Proposition 6
is that if we know ⇤  then simply optimizing a weighted surrogate loss as detailed in the proposition
sufﬁces to obtain a consistent classiﬁer. In the more practical setting where ⇤ is not known exactly 
we can then compute a sample based estimate ˆS. We brieﬂy mentioned in the previous section
how the proposed Algorithms 1 and 2 simplify in this case. Using the plug-in estimate ˆS such
p! ⇤ in the algorithms directly guarantees consistency  under mild assumptions on P (see
that ˆS
Appendix A for details). The proof for this setting essentially follows the arguments in [7]  given
Proposition 6.
Now  we turn to the general case  i.e. when L is an arbitrary measure in the class (4) such that ⇤
is difﬁcult to estimate directly. In this case  both the proposed algorithms estimate  to optimize the
empirical measure Ln. We employ the following proposition which establishes bounds on L.
Proposition 7. Let the constants aij  bij for i  j 2{ 0  1}  a0  and b0 be non-negative and  without
loss of generality  take values from [0  1]. Then  we have:

1. 2  c1  d1  2 1  c2  d2  1  and 0  c0  d0  2(1 + ⇡).
2. L is bounded  i.e. for any ✓  0 L (✓)  L := a0+maxi j2{0 1} aij
b0+minij2{0 1} bij

.

  where r(n  ⇢) =q 1

The proofs of the main results in Theorem 10 and 11 rely on the following Lemmas 8 and 9 on how
the empirical measure converges to the population measure at a steady rate. We defer the proofs to
Appendix B.
Lemma 8. For any ✏> 0  limn ! 1 P(|Ln(✓) L (✓)| <✏ ) = 1. Furthermore  with probability at
least 1  ⇢ |Ln(✓) L (✓)| < (C+LD)r(n ⇢)
⇢  L is an upper bound on
BDr(n ⇢)
L(✓)  B  0  C  0  D  0 are constants that depend on L (i.e. c0  c1  c2  d0  d1 and d2).
Now  we show a uniform convergence result for Ln with respect to maximization over the threshold
 2 (0  1).
Lemma 9. Consider the function class of all thresholded decisions ⇥= {1{(x)>} 8 2 (0  1)}
for a [0  1]-valued function  : X! [0  1]. Deﬁne ˜r(n  ⇢) =q 32
⇢⇤. If ˜r(n  ⇢) < B
n⇥ ln(en) + ln 16
  then with prob. at least 1  ⇢ 
✓2⇥|Ln(✓) L (✓)| <✏.

(where B and D are deﬁned as in Lemma 8) and ✏ = (C+LD)˜r(n ⇢)
BD˜r(n ⇢)

2n ln 4

sup

D

We are now ready to state our main results concerning the consistency of the two proposed algo-
rithms.
Theorem 10. (Main Result 2) If the estimate ˆ⌘x satisﬁes ˆ⌘x

p! ⌘x  Algorithm 1 is L-consistent.

Note that we can obtain an estimate ˆ⌘x with the guarantee that ˆ⌘x
loss function [19] (e.g. logistic loss) (see Appendix B).

p! ⌘x by using a strongly proper

6

Theorem 11. (Main Result 3) Let ` : R : [0 1) be a classiﬁcation-calibrated convex (margin) loss
(i.e. `0(0) < 0) and let ` be the corresponding weighted loss for a given  used in the weighted
ERM (7). Then  Algorithm 2 is L-consistent.
Note that loss functions used in practice such as hinge and logistic are classiﬁcation-calibrated [8].

4 Experiments

We present experiments on synthetic data where we observe that measures from our family indeed
are maximized by thresholding ⌘x. We also compare the two proposed algorithms on benchmark
datasets on two speciﬁc measures from the family.

4.1 Synthetic data: Optimal decisions

We evaluate the Bayes optimal classiﬁers for common performance metrics to empirically verify the
results of Theorem 2. We ﬁx a domain X = {1  2  . . . 10}  then we set µ(x) by drawing random
values uniformly in (0  1)  and then normalizing these. We set the conditional probability using a
1+exp(wx)  where w is a random value drawn from a standard Gaussian.
sigmoid function as ⌘x =
As the optimal threshold depends on the Bayes risk L⇤  the Bayes classiﬁer cannot be evaluated
using plug-in estimates. Instead  the Bayes classiﬁer ✓⇤ was obtained using an exhaustive search
over all 210 possible classiﬁers. The results are presented in Fig. 1. For different metrics  we plot ⌘x 
the predicted optimal threshold ⇤ (which depends on P) and the Bayes classiﬁer ✓⇤. The results can
be seen to be consistent with Theorem 2 i.e. the (exhaustively computed) Bayes optimal classiﬁer
matches the thresholded classiﬁer detailed in the theorem.

1

(a) Precision

(b) F1

(c) Weighted Accuracy

(d) Jaccard

Figure 1: Simulated results showing ⌘x  optimal threshold ⇤ and Bayes classiﬁer ✓⇤.

4.2 Benchmark data: Performance of the proposed algorithms

2(T P +T N )

We evaluate the two algorithms on several benchmark datasets for classiﬁcation. We consider two
measures  F1 deﬁned as in Section 2 and Weighted Accuracy deﬁned as
2(T P +T N )+F P +F N . We
split the training data S into two sets S1 and S2: S1 is used for estimating ˆ⌘x and S2 for selecting .
For Algorithm 1  we use logistic loss on the samples (with L2 regularization) to obtain estimate ˆ⌘x.
Once we have the estimate  we use the model to obtain ˆ⌘x for x 2S 2  and then use the values ˆ⌘x as
candidate  choices to select the optimal threshold (note that the empirical best lies in the choices).
Similarly  for Algorithm 2  we use a weighted logistic regression  where the weights depend on the
threshold as detailed in our algorithm description. Here  we grid the space [0  1] to ﬁnd the best
threshold on S2. Notice that this step is embarrassingly parallelizable. The granularity of the grid
depends primarily on class imbalance in the data  and varies with datasets. We also compare the two
algorithms with the standard empirical risk minimization (ERM) - regularized logistic regression
with threshold 1/2.
First  we optimize for the F1 measure on four benchmark datasets: (1) REUTERS  consisting of
news 8293 articles categorized into 65 topics (obtained the processed dataset from [20]). For each
topic  we obtain a highly imbalanced binary classiﬁcation dataset with the topic as the positive
class and the rest as negative. We report the average F1 measure over all the topics (also known
as macro-F1 score). Following the analysis in [6]  we present results for averaging over topics that
had at least C positives in the training (5946 articles) as well as the test (2347 articles) data. (2)
LETTERS dataset consisting of 20000 handwritten letters (16000 training and 4000 test instances)

7

from the English alphabet (26 classes  with each class consisting of at least 100 positive training
instances). (3) SCENE dataset (UCI benchmark) consisting of 2230 images (1137 training and 1093
test instances) categorized into 6 scene types (with each class consisting of at least 100 positive
instances). (4) WEBPAGE binary text categorization dataset obtained from [21]  consisting of 34780
web pages (6956 train and 27824 test)  with only about 182 positive instances in the train. All the
datasets  except SCENE  have a high class imbalance. We use our algorithms to optimize for the
F1 measure on these datasets. The results are presented in Table 1. We see that both algorithms
perform similarly in many cases. A noticeable exception is the SCENE dataset  where Algorithm 1
is better by a large margin. In REUTERS dataset  we observe that as the number of positive instances
C in the training data increases  the methods perform signiﬁcantly better  and our results align with
those in [6] on this dataset. We also ﬁnd  albeit surprisingly  that using a threshold 1/2 performs
competitively on this dataset.

DATASET

REUTERS
(65 classes)

LETTERS (26 classes)
SCENE (6 classes)
WEB PAGE (binary)

C
1
10
50
100
1
1
1

ERM Algorithm 1 Algorithm 2
0.4855
0.5151
0.7449
0.7624
0.8560
0.8428
0.9670
0.9675
0.4827
0.5686
0.5916
0.3953
0.6254
0.6267

0.4980
0.7600
0.8510
0.9670
0.5742
0.6891
0.6269

Table 1: Comparison of methods: F1 measure. First three are multi-class datasets: F1 is computed
individually for each class that has at least C positive instances (in both the train and the test sets)
and then averaged over classes (macro-F1).
Next we optimize for the Weighted Accuracy measure on datasets with less class imbalance. In this
case  we can see that ⇤ = 1/2 from Theorem 2. We use four benchmark datasets: SCENE (same as
earlier)  IMAGE (2068 images: 1300 train  1010 test) [22]  BREAST CANCER (683 instances: 463
train  220 test) and SPAMBASE (4601 instances: 3071 train  1530 test) [23]. Note that the last three
are binary datasets. The results are presented in Table 2. Here  we observe that all the methods
perform similarly  which conforms to our theoretical guarantees of consistency.

DATASET
SCENE
IMAGE
BREAST CANCER
SPAMBASE

ERM Algorithm 1 Algorithm 2
0.9105
0.9000
0.9025
0.9060
0.9910
0.9860
0.9463
0.9430

0.9000
0.9063
0.9910
0.9550

Table 2: Comparison of methods: Weighted Accuracy deﬁned as
1/2. We observe that the two algorithms are consistent (ERM thresholds at 1/2).

2(T P +T N )

2(T P +T N )+F P +F N . Here  ⇤ =

5 Conclusions and Future Work
Despite the importance of binary classiﬁcation  theoretical results identifying optimal classiﬁers
and consistent algorithms for many performance metrics used in practice remain as open questions.
Our goal in this paper is to begin to answer these questions. We have considered a large family
of generalized performance measures that includes many measures used in practice. Our analysis
shows that the optimal classiﬁers for such measures can be characterized as the sign of the thresh-
olded conditional probability of the positive class  with a threshold that depends on the speciﬁc
metric. This result uniﬁes and generalizes known special cases. We have proposed two algorithms
for consistent estimation of the optimal classiﬁers. While the results presented are an important ﬁrst
step  many open questions remain. It would be interesting to characterize the convergence rates of
L(ˆ✓) p!L (✓⇤) as ˆ✓ p! ✓⇤  using surrogate losses similar in spirit to how excess 0-1 risk is controlled
through excess surrogate risk in [8]. Another important direction is to characterize the entire family
of measures for which the optimal is given by thresholded P (Y = 1|x). We would like to extend
our analysis to the multi-class and multi-label domains as well.
Acknowledgments: This research was supported by NSF grant CCF-1117055 and NSF grant CCF-1320746.
P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803  IIS-1320894.

8

References
[1] David D Lewis and William A Gale. A sequential algorithm for training text classiﬁers. In Proceedings
of the 17th annual international ACM SIGIR conference  pages 3–12. Springer-Verlag New York  Inc. 
1994.

[2] Chris Drummond and Robert C Holte. Severe class imbalance: Why better algorithms aren’t the answer?

In Machine Learning: ECML 2005  pages 539–546. Springer  2005.

[3] Qiong Gu  Li Zhu  and Zhihua Cai. Evaluation measures of the classiﬁcation performance of imbalanced

data sets. In Computational Intelligence and Intelligent Systems  pages 461–471. Springer  2009.

[4] Haibo He and Edwardo A Garcia. Learning from imbalanced data. Knowledge and Data Engineering 

IEEE Transactions on  21(9):1263–1284  2009.

[5] Charles Elkan. The foundations of cost-sensitive learning. In International Joint Conference on Artiﬁcial

Intelligence  volume 17  pages 973–978. Citeseer  2001.

[6] Nan Ye  Kian Ming A Chai  Wee Sun Lee  and Hai Leong Chieu. Optimizing F-measures: a tale of two

approaches. In Proceedings of the International Conference on Machine Learning  2012.

[7] Aditya Menon  Harikrishna Narasimhan  Shivani Agarwal  and Sanjay Chawla. On the statistical consis-
tency of algorithms for binary classiﬁcation under class imbalance. In Proceedings of The 30th Interna-
tional Conference on Machine Learning  pages 603–611  2013.

[8] Peter L Bartlett  Michael I Jordan  and Jon D McAuliffe. Convexity  classiﬁcation  and risk bounds.

Journal of the American Statistical Association  101(473):138–156  2006.

[9] Clayton Scott. Calibrated asymmetric surrogate losses. Electronic J. of Stat.  6:958–992  2012.
[10] Ingo Steinwart. How to compare different loss functions and their risks. Constructive Approximation  26

(2):225–287  2007.

[11] Mark D Reid and Robert C Williamson. Composite binary losses. The Journal of Machine Learning

Research  9999:2387–2422  2010.

[12] Tong Zhang. Statistical analysis of some multi-category large margin classiﬁcation methods. The Journal

of Machine Learning Research  5:1225–1251  2004.

[13] Ambuj Tewari and Peter L Bartlett. On the consistency of multiclass classiﬁcation methods. The Journal

of Machine Learning Research  8:1007–1025  2007.

[14] Ming-Jie Zhao  Narayanan Edakunni  Adam Pocock  and Gavin Brown. Beyond Fano’s inequality:
bounds on the optimal F-score  BER  and cost-sensitive risk and their implications. The Journal of Ma-
chine Learning Research  14(1):1033–1090  2013.

[15] Zachary Chase Lipton  Charles Elkan  and Balakrishnan Narayanaswamy. Thresholding classiers to max-

imize F1 score. arXiv  abs/1402.1892  2014.

[16] Marina Sokolova and Guy Lapalme. A systematic analysis of performance measures for classiﬁcation

tasks. Information Processing & Management  45(4):427–437  2009.

[17] Seung-Seok Choi and Sung-Hyuk Cha. A survey of binary similarity and distance measures. Journal of

Systemics  Cybernetics and Informatics  pages 43–48  2010.

[18] Yaroslav D Sergeyev. Global one-dimensional optimization using smooth auxiliary functions. Mathemat-

ical Programming  81(1):127–146  1998.

[19] Mark D Reid and Robert C Williamson. Surrogate regret bounds for proper losses. In Proceedings of the

26th Annual International Conference on Machine Learning  pages 897–904. ACM  2009.

[20] Deng Cai  Xuanhui Wang  and Xiaofei He. Probabilistic dyadic data analysis with local and global
consistency. In Proceedings of the 26th Annual International Conference on Machine Learning  pages
105–112. ACM  2009.

[21] John C Platt. Fast training of support vector machines using sequential minimal optimization. 1999.
[22] S. Mika  G. R¨atsch  J. Weston  B. Sch¨olkopf  and K.-R. M¨uller. Fisher discriminant analysis with kernels.
In Y.-H. Hu  J. Larsen  E. Wilson  and S. Douglas  editors  Neural Networks for Signal Processing IX 
pages 41–48. IEEE  1999.

[23] Steve Webb  James Caverlee  and Calton Pu. Introducing the webb spam corpus: Using email spam to

identify web spam automatically. In CEAS  2006.

[24] Stephen Poythress Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press 

2004.

[25] Luc Devroye. A probabilistic theory of pattern recognition  volume 31. springer  1996.
[26] Aditya Menon  Harikrishna Narasimhan  Shivani Agarwal  and Sanjay Chawla. On the statistical consis-
tency of algorithms for binary classiﬁcation under class imbalance: Supplementary material. In Proceed-
ings of The 30th International Conference on Machine Learning  pages 603–611  2013.

9

,Oluwasanmi Koyejo
Nagarajan Natarajan
Pradeep Ravikumar
Inderjit Dhillon
Ayan Chakrabarti
Benjamin Moseley