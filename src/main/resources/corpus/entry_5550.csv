2019,Thinning for Accelerating the Learning of Point Processes,This paper discusses one of the most fundamental issues about point processes that what is the best sampling method for point processes. We propose \textit{thinning} as a downsampling method for accelerating the learning of point processes. We find that the thinning operation preserves the structure of intensity  and is able to estimate parameters with less time and without much loss of accuracy. Theoretical results including intensity  parameter and gradient estimation on a thinned history are presented for point processes with decouplable intensities. A stochastic optimization algorithm based on the thinned gradient is proposed. Experimental results on synthetic and real-world datasets validate the effectiveness of thinning in the tasks of parameter and gradient estimation  as well as stochastic optimization.,Thinning for Accelerating the Learning of

Point Processes

Tianbo Li  Yiping Ke

School of Computer Science and Engineering
Nanyang Technological University  Singapore

tianbo001@e.ntu.edu.sg  ypke@ntu.edu.sg

Abstract

This paper discusses one of the most fundamental issues about point processes
that what is the best sampling method for point processes. We propose thinning
as a downsampling method for accelerating the learning of point processes. We
ﬁnd that the thinning operation preserves the structure of intensity  and is able to
estimate parameters with less time and without much loss of accuracy. Theoretical
results including intensity  parameter and gradient estimation on a thinned history
are presented for point processes with decouplable intensities. A stochastic opti-
mization algorithm based on the thinned gradient is proposed. Experimental re-
sults on synthetic and real-world datasets validate the effectiveness of thinning in
the tasks of parameter and gradient estimation  as well as stochastic optimization.

1

Introduction

Point processes are a powerful statistical tool for modeling event sequences and have drawn massive
attention from the machine learning community. Point processes have been widely used in ﬁnance
[6]  neuroscience [8]  seismology [28]  social network analysis [22] and many other disciplines.
Despite their popularity  applications related to point processes are often plagued by the scalability
issue. Some state-of-the-art models [37  35  33] have a time complexity of O(d2n3)  where n is the
number of events and d is the dimension. As the number of events increases  learning such a model
would be very time consuming  if not infeasible. This becomes a major obstacle in applying point
processes.
A simple strategy to address this problem is to use part of the dataset in the learning. For instance  in
mini-batch gradient descent  the gradient is computed at each iteration using a small batch instead
of full data. For point processes  however  to ﬁnd a suitable sampling method is not a easy task  at
least not as easily as it might seem. This is due to the special input data — event sequences. First
of all  event sequences are posets. An inappropriate sampling methods may spoil the order structure
of the temporal information. This is especially harmful when the intensity function depends on
its history. Second  many models built upon point processes utilize the arrival intervals between
two events. Such models are particularly useful as they take into account the interactions between
events or nodes. Examples include Hawkes processes and their variants [37  33  19]. An improper
sampling method may change the lengths of arrival intervals  leading to a poor estimation of model
parameters.
A commonly-used approach to the sampling of point processes is sub-interval sampling [34  32].
Sub-interval sampling is a piecewise sampling method  which splits an event sequence into small
pieces and learns the model on these sub-intervals. At each iteration  one or several sub-intervals
are selected to compute the gradient. This method  however  has an intrinsic limitation: it cannot
capture the panoramic view of a point process. Take self-excited event sequences as an example. An
important characteristic of such sequences is that events are not evenly distributed across the time

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

axis  but tend to be clumping in a short period of time. Sub-interval sampling  in this circumstance  is
like “a blind man appraising an elephant” — it can only see part of the information at each iteration 
prone to a large variance of the gradient.

Figure 1: Comparison of sub-interval sampling (a) and thinning sampling (b).

In this paper  we discuss “thinning sampling” as a downsampling method for accelerating the learn-
ing of point processes. A comparison between sub-interval and thinning are shown in Figure 1.
Conventionally  thinning is a classic technique for simulating point processes [18]. We borrow the
idea and adopt it as a downsampling method for fast learning of point processes. It is convenient to
implement and able to capture the entire landscape over the observation timeline.
The main contributions of this paper are summarized as follows.
• To the best of our knowledge  we are the ﬁrst to employ thinning as a downsampling method to
• We present theoretical results for intensity  parameter and gradient estimation on the thinned
history of a point process with decouplable intensities. We also apply thinning in stochastic
optimization for the learning of point processes.
• Experiments verify that thinning sampling can signiﬁcantly reduce the model learning time with-
out much loss of accuracy  and achieve the best performance when training a Hawkes process on
both synthetic and real-world datasets.

accelerate the learning of point processes.

2 Point Processes

∑

A point process N(t) can be viewed as a random measure from a probability space (Ω  F  P) onto a
simple point measure space (N  BN). We deﬁne a point process N(t) as follows.
Deﬁnition 2.1 (Point process). Let ti be the i-th arrival time of a point process N(t) deﬁned by 
ti = inft{N(t) > i}. A point process N(t) on R+ is deﬁned by N(t) =
i δti(t)  where δω is the
Dirac measure at ω.
The “information” available at time t is represented by a sub-σ-algebra Ht = σ(N(t) : t ∈ R+).
The ﬁltration H = (Ht)06t<∞ is called the internal history. A point process can be characterized
by its intensity function. It measures the probability that a point will arrive in an inﬁnitesimal period
of time given the history up to the current time. Herein we follow the deﬁnition of stochastic intensity
introduced in [7  15].
Deﬁnition 2.2 (Stochastic intensity). Let N(t) be an H-adapted point process and λ(t) be a non-
negative H-predictable process. λ(t) is called the H-intensity of N(t)  if for any nonegative H-
predictable process C(t)  the equality below holds 

E

C(s)dN(s)

= E

C(s)λ(s)ds

.

0

(1)

(cid:20)∫∞

0

(cid:21)

(cid:20)∫∞

(cid:21)

∫

The expectation of N(t) is called the H-compensator  which is the cumulative intensity Λ(t) =
t
0 λ(s)ds. Doob-Meyer decomposition yields that N(t) − Λ(t) is an H-adapted martingale. An-
other important result is that the conditional intensity function uniquely determines the probability
structure of a point process [9]. Similar results can be extended to compensators [10  15].

2

(a)(b)sub-intervalsamplesthinningsamplesM-estimator. Commonly-used parameter estimation methods for point processes include maximum
likelihood estimation (MLE) [22  33  37]  intensity-based least square estimation (LSE) [3  21] and
counting-based LSE [32]. These methods fall into a wider class called martingale estimator (M-
estimator) [15  2]. The gradient ∇R(θ)  θ ∈ Rd can be expressed as a stochastic integral: N(t) 

(full gradient)

∇R(θ) =

H(t; θ) [dN(t) − λ(t; θ)dt] .

(2)

∫

T

0

Here R(θ) is the loss function  which may be the log-likelihood  or the sum of the squares of
the residuals. λ(t; θ) is the intensity function to be estimated.
[0  T ] is the observation window.
H(t; θ) is a vector-valued function  or more generally  a predictable  boundedly ﬁnite  and square in-
tegrable process associated with λ(t; θ). Different choices of H(t; θ) instantiate different estimators:
H(t; θ) = −∇ log λ(t; θ) for MLE  H(t; θ) = ∇λ(t; θ) for intensity-based LSE  and H(t; θ) = 1
∇R(θ; ωi)  ωi ∈ Ω as the empirical gradient given real-
for counting-based LSE. We write
∗  ∇R(θ
∗
i=1
) is a martingale and its expectation is 0. Gradient
izations {ωi}. Under the true parameter θ
∇R( ˆθ; ωi) = 0. Note that in this paper 
descent methods are often used to ﬁnd ˆθ by solving
LSE refers to intensity-based LSE  as all the results can be easily extended to counting-based LSE.

∑

∑

i=1

3 Thinned Point Processes

In this section  we introduce a derivative process called p-thinned process. Intuitively  the p-thinned
process is obtained from a point process by retaining each point in it with probability p  and dropping
with probability 1 − p. We formally deﬁne the p-thinned process as follows.
Deﬁnition 3.1 (p-thinned process). The p-thinned process Np(t) associated with a point process
N(t) (called the ground process) is deﬁned by summing up the Dirac measure on the product space
Ω × K:

∑

Np(t) =

i=1

δ(ti Bi=1)(t) 

where Bi’s are independent Bernoulli distributed random variables with parameter p.

N(t)

Alternatively  the p-thinned process can be written in the form of a compound process as Np(t) =
i=1 Bi and its differential can be expressed as dNp(t) = BN(t)dN(t). In this way  the Rie-
mannStieltjes integral of a stochastic process with respect to a p-thinned process can be deﬁned
by 

∫

T

∫

T

N(T )∑

H(t)dNp(t) =

0

0

H(t)BN(t)dN(t) =

i=1

H(ti|Hti−)Bi.

Two types of histories. The differential deﬁned above implies that the intensity of thinned process
can be written as λp(t) = pλ(t). This relation between the intensities of a thinned process and
its ground process is intuitively plausible. The implicit condition  however  is that λp(t) must be
measurable with respect to the full history of N(t) and all the thinning marks Bi prior to the current
time t. Such a history can be expressed by the ﬁltration F = (Ft)  where Ft = Ht ⊗ Kt and Kt
is the cylinder σ-algebra of the markers. This history is called the full history  and its corresponding
F-intensity is denoted by λF
p (t)  we need to take into account all the points
prior to t  including those dropped ones.
The other type of history  called the thinned history  is the internal history of the thinned process 
denoted by G = (Gt)  where Gt = σ(Np(t)). In computing its G-intensity λG
p(t)  we only need to
consider all the retained points of the thinned process.
The following lemma describes the relationship between different intensities.
Lemma 3.2 (Relationship of intensities). Let F and G be the full history and thinned history with
respect to a p-thinned process Np(t). Let H be the internal history of N(t). The following equalities
hold:

p (t). When computing λF

∑

(cid:2)

(cid:3)

(1) λF
(2) λG

p (t) = pλH(t);
p(t) = pE

λH(t)|G

.

3

(cid:2)

(cid:3)

Due to space limit  we defer all the proofs to the supplementary material. Lemma 3.2 tells that the
intensity of a p-thinned process is a version of the conditional expectation E
. Provided
the information of the p-thinned process  the p-thinned intensity is an orthogonal projection of that
of the ground point process on L2. Therefore  1/pλG
p-thinned and sub-interval gradient. We deﬁne the p-thinned gradient  which is the stochastic
integral with respect to a p-thinned process:
(p-thinned gradient) ∇Rp(θ) =

p(t) is an unbiased estimation of λH(t).

dNp(t) − λ

λH(t)|G

∫

(cid:2)

(cid:3)

p(t; θ)dt

p(t; θ)

(3)

H

G

G

.

T

1
p

0

p(t; θ).

p(t; θ) is related to λG

Here HG
Sub-interval gradient is deﬁned as follows. Let τ0  τ1  τ2  . . .   τ⌈1/p⌉ be a partition of [0  T )  where
τ0 = 0  and τ⌈1/p⌉ = T. We cut the interval into ⌈1/p⌉ pieces so that the batch size is comparable
to that in the thinned gradient. At every step  one interval is selected with probability p. We deﬁne
the sub-interval gradient on [τi  τi+1) by 
∇Rℓ(θ) =

I{t ∈ [τi  τi+1)}H

(sub-interval gradient)

F(t; θ)dt

dN(t) − λ

F(t; θ)

∫

(cid:2)

(cid:3)

 

T

1
p

0

where I is an indicator representing whether the sub-interval is selected or not. Here we consider
the full history. It can be easily veriﬁed that ∇Rℓ(θ) is an unbiased estimation of the full gradient
in Eq. (2). The thinned gradient can be used as a estimator of full gradient  which will be illustrated
in Section 5. This deﬁnition also generalizes the stochastic optimization method proposed in [32] 
which splits the observation timeline at the arrival of each event.

4 Thinning for Parameter Estimation
In this section  we discuss how to estimate the parameter θ ∈ Rd of the intensity function λ(t; θ)
given the thinned history G. We ﬁrst deﬁne the notations used.
• θ
H: true parameter of H-intensity λH(t; θ)  such that E∇R(θ
∗
∗
H) = 0;
• θ
p(t; θ)  such that E∇Rp(θ
∗
∗
G: true parameter of G-intensity λG
G) = 0;
• ˆθH: estimate of θ
∇R( ˆθH; ωi) = 0;
∗
H  such that
• ˆθG: estimate of θ
∇Rp( ˆθG; ω
∗
G  such that

′
i is a realization of the p-thinned

′
i) = 0  where ω

∑
∑

i

i

process.

(cid:2)∇R( ˜θH)|G

(cid:3)

The task of parameter estimation on a thinned history is to ﬁnd ˜θH  such that E
is close
enough to 0. We refer to ˜θH as the M-estimator on thinned history. Here the expectation is over
the thinning operation. The tilde is used to indicate that ˜θH is a G-measurable estimator for the
parameter of H-intensity λH(t; θ)  whereas ˆθH  with a hat on it  is H-measurable. A notable result
P−→ θ
∗
is that M-estimators have asymptotic normality [2]  thus we have ˆθH
G  as the

P−→ θ
∗
H and ˆθG

number of realizations n →∞.

(cid:3)

(cid:2)∇R( ˜θH)|G

In the following  we ﬁrst present a method for parameter estimation of a non-homogeneous Poisson
process (NHPP) whose intensity is deterministic. We then derive a theorem that works for a more
general type of intensities.
Lemma 4.1 (Thinning for parameter estimation of NHPP). Consider an NHPP N(t) with determin-
istic intensity λ(t; θ)  t > 0  θ ∈ Rd. If there exists an invertible linear operator A : Rd → Rd satis-
fying λ(t; Aθ) = pλ(t; θ)  then the M-estimator on thinned history can be written as ˜θH = A−1 ˆθG
such that E
Example (Parameter estimation for NHPP). Let consider an NHPP with intensity λ(t; a  b  c  d) =
a + b sin(ct + d). We can ﬁnd a diagonal matrix A = diag(p  p  1  1) such that λ(t; A(a  b  c  d)) =
pa + pb sin(ct + d) = pλ(t; a  b  c  d). Thus the parameter given the thinned history can be
estimated by A−1( ˆa  ˆb  ˆc  ˆd) = (1/p ˆa  1/p ˆb  ˆc  ˆd)  where ˆa  ˆb  ˆc  ˆd are estimated on the thinned
history.

P−→ 0  as the number of realizations n →∞.

4

Next  we focus on a more general type of intensities  called decouplable intensity. Most commonly-
used point processes have decouplable intensities  including NHPPs  linear Hawkes processes  com-
pound Poisson process  etc.
Deﬁnition 4.2 (Decouplable intensity). An intensity function is said to be decouplable  if it can be
written in such a form:

H(t; θ) = g(t; θ)T m

(4)
Here g(t; θ) is a deterministic vector-valued function that is continuous with respect to θ and does
not contain any information regarding Ht. mH(t) is an H-predictable vector-valued measure
that does not contain any information regarding θ. Particularly  λH(t; θ) is said to be linear if
g(t; θ) = θ.

H(t).

λ

(cid:2)

mH(t)|G

= mG

p(t)  where mG

p(t) is the component of thinned intensity λG

(cid:3)
(cid:2)∇R( ˜θH)|G
(cid:2)

This category covers a multitude of state-of-the-art models  including Netcodec [30]  parametric
Hawkes [19]  MMEL model [35]  Granger causality for Hawkes [33]  and the sparse low-rank
Hawkes [36]. The next theorem demonstrates a similar result with Lemma 4.1 for decouplable
intensities.
Theorem 4.3 (Thinning for parameter estimation of decouplable intensities). Consider a point
process N(t) with decouplable intensity. If there exist invertible linear operators A and B satis-
fying BE
p(t)  and
pB−1g(t; θ) = g(t; Aθ)  then the M-estimator on thinned history can be written as ˜θH = A−1 ˆθG
such that E
linear  then A = pB−1.

P−→ 0  as the number of realizations n →∞. Particularly  if λH(t; θ) is
(cid:3)

Example (Parameter estimation for Hawkes processes). Consider a one-dimensional Hawkes pro-
cess with intensity λH(t; µ  α) = (µ  α)T
i=1 ϕ(t − ti). From
the fact that E
p(t)  we obtain B = diag(1  p). Thus Theorem 4.3 yields
A = pB−1 = diag(p  1) and consequently µ and α can be estimated by p ˆµ and ˆα  where ˆµ and
ˆα are estimated on the thinned history. Similar results can be obtained on multi-dimensional linear
Hawkes processes. This result reveals that the thinning operation does not change the endogenous
triggering pattern in linear Hawkes processes.
Remark (Parameter estimation for multi-dimensional Hawkes processes). The thinning estima-
tor is also valid for multi-dimensional Hawkes processes. Consider the i-th dimension of an m-
i (t; µi  αi1 ···   αim) =
dimensional Hawkes process. Its intensity function can be written as λH
(µi  αi1 ···   αim)T
  which complies with the deﬁnition of decouplable in-
tensity. Theorem 4.3 again yields a thinning estimator with the linear operator A = diag(p  1  ...  1).

1 (t) ···   mH

  where mH(t) =

= 1/pmG

1  mH(t)

mH(t)|G

∑

1  mH

m(t)

(cid:0)

(cid:1)

(cid:1)

(cid:3)

(cid:0)

5 Thinning for Gradient Estimation and Stochastic Optimization

So far we have discussed how to estimate the parameter given the thinned history.
In fact  the
gradient at any θ can also be recovered without knowing all the information about a point process.
The following theorem describes the gradient estimation on the thinned history for decoupleable
intensity.
Theorem 5.1 (Thinning for gradient estimation). Let N(t) be a point process with decouplable
intensity λH(t; θ) = g(t; θ)T mH(t) in Eq. (4). If there exist invertible linear operators A and B
satisfying BE
p(t)  and
pB−1g(t; θ) = g(t; Aθ)  then

p(t) is the component of thinned intensity λG

p(t)  where mG

mH(t)|G

= mG

(cid:3)

(cid:2)
(cid:2)∇R(θ)|G
(cid:2)∇R(θ)|G

(cid:3)
(cid:3)

(1) E
(2) E

6 1/pA−1∇Rp(Aθ)  for R is LSE;
6 A−1∇Rp(Aθ)  for R is MLE.

Particularly  if the intensity is deterministic  i.e.  mH(t) = 1  both equalities hold.

Remark. The thinned gradient can be transformed to a larger estimation of the full gradient  and
an unbiased estimation for deterministic intensity. More speciﬁcally  the gradient estimation is
unbiased if and only if E[H(t; θ)λ(t; θ)] = EH(t; θ)Eλ(t; θ)  as shown in the proof of Theo-
rem 5.1. Here H usually depends on the intensity function λ(t; θ)  such as MLE estimator has
H(t; θ) = −∇ log λ(t; θ). The condition may not hold under such circumstances. For stochastic

5

intensities  the thinned gradient may be biased  yielding an estimation larger than the ground truth.
Some empirical results on Hawkes processes are shown in Figure 3. The next theorem shows that
the thinned gradient has a smaller variance compared with the sub-interval gradient.
Theorem 5.2 (Variance comparison). Let ∇ ˜RG(θ) and ∇Rℓ(θ) be the p-thinned and sub-interval
gradient at θ  where ∇ ˜RG(θ) = 1/pA−1∇Rp(Aθ) for LSE and ∇ ˜RG(θ) = A−1∇Rp(Aθ) for MLE.
The variance of the p-thinned gradient is no greater than that of the sub-interval gradient  i.e. 

h
∇ ˜R

V

i

G(θ)

6 V

i

.

h
∇Rℓ(θ)
i

(cid:12)(cid:12)

p

=

> ϵ

i

(cid:17)

V

6

V

6

+

V

1
p

1 − p

G−E∇ ˜R

G(θ)

(cid:16)(cid:12)(cid:12)∇ ˜R

i2

h
E∇R(θ)
ϵ2

h
∇Rℓ(θ)
ϵ2

h
∇ ˜RG(θ)
ϵ2

Remark. A Chebyshev error bound can be easily obtained  as a result of Theorems 5.1 and 5.2:

(cid:3)
(cid:2)∇R(θ)
P
(cid:3)
(cid:2)∇R(θ)
 
for any ϵ > 0. Since ∇R(θ) is a martingale integral (Eq. 2)  we have E∇R(θ) → 0  as the number
of realizations increases. Hence  the left-hand side probability is bounded by O(ϵ−2p−1V
) 
which shows that the gradient estimation of deterministic intensities will not be far from its true
one  if the number of realizations is sufﬁciently large. Unfortunately  the result does not apply to
stochastic intensities. Nonetheless  its effectiveness on stochastic intensities is empirically validated
on real datasets with Hawkes processes in our experiments (See Figure 4).
Thinning for stochastic optimization. We have shown that thinning can be used for estimating
parameters and gradients with less data.This inspires us to employ it to stochastic optimization. We
propose a novel Thinning-SGD (TSGD) method for learning a point process with a parametric inten-
sity function  as shown in Algorithm 1. At each iteration  a thinned dataset is used for computing the
gradient. Compared with sub-interval variance  thinned gradient has a smaller variance  so that the
convergence curve may have less ﬂuctuations and ﬁnd a path to the optimal solution faster. Thinning
is also applicable to other gradient-based optimization algorithms such as Adam [16].

Algorithm 1: TSGD: Thinning Stochastic Gradient Descent
Input

:Event sequences {ti}  learning rate α  thinning size p  convergence criterion 
the objective function of a parametric point process model R(θ).

∗.
Output :Optimal parameter θ

1 Initialize θ;
2 Find A according to Theorem 4.3;
3 repeat
′
i from one of the sequences ti;
4
5
6
7 until Convergence criterion is satisﬁed;

Sample a p-thinning batch t
Compute the thinned gradient ˜RG(θ)  where ˜RG(θ) is deﬁned in Theorem 5.2;
θ ← θ − α ˜RG(θ) ;

6 Related Work

Learning of parametric point processes. Parametric point processes are the most conventional
and popular method in the study of point processes. For example  [37] designs an algorithm ADM4
for learning the parameter representing the hidden network of social inﬂuences. [19] parameterizes
the infectivity parameter in Hawkes processes and employs the technique of ADMM for parameter
estimation. [33] proposes a learning algorithm combining MLE with a sparse-group-lasso regular-
izer to learn the so-called “Granger causality graph”. All these models are decouplable  therefore
thinning is applicable to the learning of them.
Learning of non-parametric point processes. There has been an increasing amount of studies on
non-parametric point processes and their learning algorithms in recent years. Isotonic Hawkes pro-
cess [31] is an interesting and representative work among them  which combines isotonic regression
and Hawkes processes. [1] proposes a algorithm to learn the infectivity matrix without any para-
metric modeling and estimation of the kernels. Another category of non-parametric models related

6

to point processes is Bayesian non-parametric models  such as [4]  [12] and [25]. Besides  some
explorations of combining point processes and deep neural networks are emerging. Some typical
works include [11]  [26]  and [20].
Acceleration for the learning of point processes. [17] proposes a method of low rank approx-
imation of the kernel matrix for large-scale datasets. The online learning algorithm for Hawkes
[34] discretizes the time axis into small intervals for learning the triggering kernels. [13] designs
a hardware acceleration method for MLE of Hawkes processes. A recent work [32] introduces an
stochastic optimization method for Hawkes processes. Unfortunately  none of existing works con-
siders thinning as a sampling methods to reduce the time complexity.
Thinning for point processes. The thinning operation of point processes has been discussed mainly
in the statistics community. Thinning is ﬁrst used for the simulation of point processes [18  27].
Some limit results have been proposed [14  29  5]  among which the property of Cox process approx-
imation is often mentioned [10]. However  most  if not all  of these asymptotic results investigate
the behavior of a thinned process as the thinning level p → 0  which does not serve our purpose.

7 Experiments

In this section  we assess the performance of our proposed thinning sampling in three tasks: parame-
ter estimation  gradient estimation  and stochastic optimization. All the experiments were conducted
on a server with Intel Xeon CPU E5-2680 (2.80GHz) and 250GB RAM.

Figure 2: Parameter estimation on a 10-dimensional linear Hawkes process with LSE. (a): the RMSE
of estimated parameters. (b): trainning time. (c): RMSE v.s. thinning level p.
Parameter estimation. We conduct two experiments for this task on synthetic datasets. The ﬁrst
experiment is to test thinning on Hawkes processes. We simulate 100 sequences of 10-dimensional
linear Hawkes processes and use different number of events for training. The longest sequence
has around 14k events. The parameters of the process are randomly generated from a uniform
distribution. For each dataset  we perform LSE with different histories: full data and p-thinned data
with p = 0.2 and p = 0.5.
The results are shown in Figure 2. We can see that as the number of events training increases  the
error (measured by RMSE) in parameter estimation decreases  at the cost of longer running time. A

Table 1: Parameter estimation on state-of-the-art models.

Model

MMEL [37]

Granger

Causality for
Hawkes [33]

Sparse Low-rank

Hawkes [35]

Full
Thinned (p=0.5)
Thinned (p=0.2)
Full
Thinned (p=0.5)
Thinned (p=0.2)
Full
Thinned (p=0.5)
Thinned (p=0.2)

7

38.03 (4.19)
8.68 (1.06)
3.94 (0.47)

RMSE/Accuracy Training time (s)
0.0568 (0.0013)
0.0569 (0.0012)
0.0570 (0.0012)
0.0161 (0.0078)
0.0163 (0.0022)
0.0167 (0.0010)
97.46% (0.0133)
97.60% (0.0166)
96.63% (0.0243)

229.56 (17.87)
65.68 (4.67)
3.96 (1.80)
73.76 (42.24)
27.45 (17.51)
4.51 (2.65)

0.10.20.30500010000Number of eventsRMSE(a)0500100015000500010000Number of eventsElapsed time (sec)(b)llllllllllllllllllll0.020.030.040.050.000.250.500.751.00Thinning level pRMSE(c)Fullp=0.2p=0.5Figure 3: Gradient estimation for an NHPP and a linear Hawkes process using MLE and LSE. X-
axes represent the RMSE of the parameters  and Y-axes the l2-norm of gradient with corresponding
parameters.

∑

larger p value yields better estimations but also runs slower. When the number of events is large
enough  the estimation with 0.2-thinning is as accurate as that with full data  but runs an order of
magnitude faster. For a dataset of 14k events  0.2-thinning only took 2 minutes  whereas the training
on full data took 26.5 minutes  and the decrease of RMSE is less than 0.01. Figure 2 (c) shows that
RMSE decreases as the thinning level p increases.
The second experiment is to test thinning for learning various state-of-the-art models: MMEL [37] 
Granger Causality for Hawkes [33] and Sparse Low-rank Hawkes [35]. We generate 30 sequences
for each model and perform parameter estimation on different histories. The averages and standard
deviations of the quality metric and training time are presented in Table 1. We use RMSE as the
metric for MMEL and Granger Causality  and the accuracy of non-zero entries in the adjacency
matrix for Sparse Low-rank Hawkes. It can be seen that thinning signiﬁcantly reduces the training
time of all models without compromising much estimation quality.
Gradient estimation. We consider two types of point process: a non-homogeneous point process
with deterministic intensity λ(t; a  b  c  d) = a+b sin(ct+d); and a linear Hawkes process with H-
intensity λH(t) = (µ + α
ϕ(t − ti)). The gradient at different values of parameters is computed
and depicted in Figure 3.
The result shows three facts. First  every line in the ﬁgure touches X-axis at the origin  except for
NHPP(c) (indifferentiable). This phenomenon demonstrates that thinning sampling yields asymp-
totically unbiased parameter estimation  no matter for LSE or MLE. Second  we can see that red
and blue lines in the results of ﬁrst 6 sub-ﬁgures overlap signiﬁcantly  which conﬁrms that thinning
gives unbiased gradient estimation for deterministic intensities. Third  in the last two sub-ﬁgures 
blue lines tend to be on or above the red ones  which demonstrates that thinning makes gradient
estimation larger or equal to the ground truth for stochastic intensities.
Stochastic optimization. We test thinning sampling for stochastic optimization algorithms  includ-
ing SGD and Adam. The task is to learn a linear Hawkes process. We test Thinning (p=0.1)  sub-
interval sampling (SubInt)  the stochastic optimization learning algorithm (StoOpt) proposed in [32] 
combined with SGD  ADAM and the typical gradient descent (GD). We test on 4 datasets:
• Synthetic dataset: We simulate 10 realizations of a 5-dimensional linear Hawkes process  with
parameters generated from a uniform distribution. The dataset contains 20k events. We train the
model using the entire dataset and the RMSE between the estimated parameters and the ground
truth is shown as test error.
• IPTV dataset [24]: The dataset consists of IPTV viewing events  which records the timestamps
for multiple users watching a video  and the category that the video belongs to. Each user is

8

0.000.050.100.150.20−0.2−0.10.00.10.2NHPP(a  b) + MLE0255075100125−0.2−0.10.00.10.2NHPP(c) + MLE0.000.050.100.15−0.2−0.10.00.10.2NHPP(d) + MLE0.000.050.100.150.200.25−0.2−0.10.00.10.2Hawkes + MLE0.000.010.02−0.2−0.10.00.10.2NHPP(a  b) + LSE0100200300−0.2−0.10.00.10.2NHPP(c) + LSE0.00.10.20.3−0.2−0.10.00.10.2NHPP(d) + LSE0.00.10.20.30.40.5−0.2−0.10.00.10.2Hawkes + LSEFull dataThinning (p=0.25)Figure 4: The average convergence curves of different learning algorithms on different datasets.

treated as a realization and each category as a dimension. We select 7 and 3 realizations with
22k and 9k events as training and test datasets  respectively. The number of categories is 16.
• NYC taxi dataset: The data is from The New York City Taxi and Limousine Commission1 
which records ﬁelds capturing pick-up time  location and payment information of green taxis’
orders. We select those trips starting from Manhattan district in the ﬁrst 10 days of January 2018
and use the 14 areas as dimensions. The training and test datasets contain 60k and 12k events 
respectively.
• Weeplace dataset [23]: This dataset contains the check-in histories of users at different loca-
tions. The categories of events include food  education  outdoors  shops  and 10 others. The
check-in histories of 46 and 10 users are selected as training and test dataset  respectively. The
sizes of the datasets are 50k and 11k.

We ran each method on each dataset for 10 times. Figure 4 presents the average convergence curves
of each method on different datasets. Training of GD failed to ﬁnish the ﬁrst iteration given the
maximum time shown in Figure 4 for each dataset and thus its results are not presented. From the
learning curves  we can see that Thinning+Adam outperforms all the competitors in terms of test
error on all the datasets. When looking at the SGD group alone  Thinning also achieves the lowest
test error. From the bottom row  we see that Thinning+Adam tends to have less ﬂuctuated learning
curves. Especially on Weeplace and NYC taxi datasets  the ﬂuctuations of StoOpt and SubInt are
dramatic. This is due to the fact that thinning sampling can better capture the information of the
whole timeline  whereas other methods are prone to a zigzag of searching path.

8 Conclusion & Discussion

In this paper  we discussed thinning as a downsampling method for point processes. Thinning
operation uniformly compresses the intensity on time axis  but its structure is completely preserved.
In this way  for parameter estimation  similar performance can be achieved with less input data  as
shown in the experiments. We also demonstrated how to estimate gradient on the thinned history 
which leads to a novel stochastic optimization algorithm  called TSGD. Experimental results show
that TSGD converges faster and has a learning curve with less ﬂuctuations  which can be explained
by the theorem that the thinning estimator for gradient has a smaller variance.
In future work  it would be interesting to study other sampling methods  such as Jackknife resam-
pling  for point processes. This work focuses on point processes with decouplable intensities. It will
also be interesting to explore a broader assumption to serve more scenarios.

1https://www1.nyc.gov/site/tlc/index.page

9

0.0120.0150102030Elapsed time (sec)Test error (RMSE)Synthetic dataset51015200100200300Elapsed time (sec)Test error (NegLogLik)IPTV dataset−1.0−0.50.00.50200400600Elapsed time (sec)Test error (NegLogLik)NYC Taxi dataset−0.50−0.250.000.250100200300Elapsed time (sec)Test error (NegLogLik)Weeplace dataset3.94.24.54.85.10102030Elapsed time (sec)Training error (NegLogLik)Synthetic dataset−10010200100200300Elapsed time (sec)Training error (NegLogLik)IPTV dataset1.01.52.02.50200400600Elapsed time (sec)Training error (NegLogLik)NYC Taxi dataset2.53.03.54.00100200300Elapsed time (sec)Training error (NegLogLik)Weeplace datasetThinning+AdamThinning+SGDStoOpt+AdamStoOpt+SGDSubInt+AdamSubInt+SGDAcknowledgment

This work is partially supported by the Data Science and Artiﬁcial Intelligence Research Centre
(DSAIR) and the School of Computer Science and Engineering at Nanyang Technological Univer-
sity.

References
[1] Massil Achab  Emmanuel Bacry  Stéphane Gaïffas  Iacopo Mastromatteo  and Jean-François Muzy. Un-
covering causality from multivariate hawkes integrated cumulants. The Journal of Machine Learning
Research  18(1):6998–7025  2017.

[2] Per K Andersen  Ornulf Borgan  Richard D Gill  and Niels Keiding. Statistical models based on counting

processes. Springer Science & Business Media  2012.

[3] Emmanuel Bacry  Stéphane Gaïffas  and Jean-François Muzy. A generalization error bound for sparse

and low-rank multivariate hawkes processes. arXiv preprint arXiv:1501.00725  2015.

[4] Charles Blundell  Jeff Beck  and Katherine A Heller. Modelling reciprocating relationships with hawkes

processes. In Advances in Neural Information Processing Systems  pages 2600–2608  2012.

[5] Fred Böker. Convergence of thinning processes using compensators. Stochastic processes and their

applications  23(1):143–152  1986.

[6] Clive G Bowsher. Modelling security market events in continuous time: Intensity based  multivariate

point process models. Journal of Econometrics  141(2):876–912  2007.

[7] Pierre Brémaud. Point processes and queues: martingale dynamics  volume 50. Springer  1981.

[8] David R Brillinger et al. The identiﬁcation of point process systems. The Annals of Probability  3(6):909–

924  1975.

[9] Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume I: Elemen-

tary theory and methods. Springer Science & Business Media  2002.

[10] Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume II: general

theory and structure. Springer Science & Business Media  2007.

[11] Nan Du  Hanjun Dai  Rakshit Trivedi  Utkarsh Upadhyay  Manuel Gomez-Rodriguez  and Le Song. Re-
current marked temporal point processes: Embedding event history to vector. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 1555–1564.
ACM  2016.

[12] Nan Du  Mehrdad Farajtabar  Amr Ahmed  Alexander J Smola  and Le Song. Dirichlet-hawkes pro-
In Proceedings of the 21th
cesses with applications to clustering continuous-time document streams.
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining  pages 219–228.
ACM  2015.

[13] Ce Guo and Wayne Luk. Accelerating maximum likelihood estimation for hawkes point processes. In
2013 23rd International Conference on Field programmable Logic and Applications  pages 1–6. IEEE 
2013.

[14] Olav Kallenberg. Limits of compound and thinned point processes. Journal of Applied Probability 

12(2):269–278  1975.

[15] Alan Karr. Point Processes and Their Statistical Inference  volume 7. CRC Press  1991.

[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[17] Rémi Lemonnier  Kevin Scaman  and Argyris Kalogeratos. Multivariate hawkes processes for large-scale

inference. In Thirty-First AAAI Conference on Artiﬁcial Intelligence  2017.

[18] PA W Lewis and Gerald S Shedler. Simulation of nonhomogeneous poisson processes by thinning. Naval

research logistics quarterly  26(3):403–413  1979.

[19] Liangda Li and Hongyuan Zha. Learning parametric models for social infectivity in multi-dimensional

hawkes processes. In Twenty-Eighth AAAI Conference on Artiﬁcial Intelligence  2014.

10

[20] Shuang Li  Shuai Xiao  Shixiang Zhu  Nan Du  Yao Xie  and Le Song. Learning temporal point processes
via reinforcement learning. In Advances in Neural Information Processing Systems  pages 10781–10791 
2018.

[21] Tianbo Li  Pengfei Wei  and Yiping Ke. Transfer hawkes processes with content information. In 2018

IEEE International Conference on Data Mining (ICDM)  pages 1116–1121. IEEE  2018.

[22] Scott Linderman and Ryan Adams. Discovering latent network structure in point process data. In Inter-

national Conference on Machine Learning  pages 1413–1421  2014.

[23] Bin Liu  Yanjie Fu  Zijun Yao  and Hui Xiong. Learning geographical preferences for point-of-interest
In Proceedings of the 19th ACM SIGKDD international conference on Knowledge

recommendation.
discovery and data mining  pages 1043–1051. ACM  2013.

[24] Dixin Luo  Hongteng Xu  Yi Zhen  Xia Ning  Hongyuan Zha  Xiaokang Yang  and Wenjun Zhang. Multi-
task multi-dimensional hawkes processes for modeling event sequences. In Twenty-Fourth International
Joint Conference on Artiﬁcial Intelligence  2015.

[25] Charalampos Mavroforakis  Isabel Valera  and Manuel Gomez-Rodriguez. Modeling the dynamics of
learning activity on the web. In Proceedings of the 26th International Conference on World Wide Web 
pages 1421–1430. International World Wide Web Conferences Steering Committee  2017.

[26] Hongyuan Mei and Jason M Eisner. The neural hawkes process: A neurally self-modulating multivariate

point process. In Advances in Neural Information Processing Systems  pages 6754–6764  2017.

[27] Yosihiko Ogata. On lewis’ simulation method for point processes. IEEE Transactions on Information

Theory  27(1):23–31  1981.

[28] Yosihiko Ogata. Statistical models for earthquake occurrences and residual analysis for point processes.

Journal of the American Statistical association  83(401):9–27  1988.

[29] Richard Serfozo. Thinning of cluster processes: Convergence of sums of thinned point processes. Math-

ematics of operations research  9(4):522–533  1984.

[30] Long Tran  Mehrdad Farajtabar  Le Song  and Hongyuan Zha. Netcodec: Community detection from
individual activities. In Proceedings of the 2015 SIAM International Conference on Data Mining  pages
91–99. SIAM  2015.

[31] Yichen Wang  Bo Xie  Nan Du  and Le Song. Isotonic hawkes processes. In International conference on

machine learning  pages 2226–2234  2016.

[32] Hongteng Xu  Xu Chen  and Lawrence Carin. Superposition-assisted stochastic optimization for hawkes

processes. arXiv preprint arXiv:1802.04725  2018.

[33] Hongteng Xu  Mehrdad Farajtabar  and Hongyuan Zha. Learning granger causality for hawkes processes.

In International Conference on Machine Learning  pages 1717–1726  2016.

[34] Yingxiang Yang  Jalal Etesami  Niao He  and Negar Kiyavash. Online learning for multivariate hawkes

processes. In Advances in Neural Information Processing Systems  pages 4937–4946  2017.

[35] Ke Zhou  Hongyuan Zha  and Le Song. Learning social infectivity in sparse low-rank networks using

multi-dimensional hawkes processes. In Artiﬁcial Intelligence and Statistics  pages 641–649  2013.

[36] Ke Zhou  Hongyuan Zha  and Le Song. Learning social infectivity in sparse low-rank networks using

multi-dimensional hawkes processes. In Artiﬁcial Intelligence and Statistics  pages 641–649  2013.

[37] Ke Zhou  Hongyuan Zha  and Le Song. Learning triggering kernels for multi-dimensional hawkes pro-

cesses. In International Conference on Machine Learning  pages 1301–1309  2013.

11

,Elad Hazan
Holden Lee
Karan Singh
Cyril Zhang
Yi Zhang
Tianbo Li
Yiping Ke