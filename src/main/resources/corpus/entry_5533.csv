2016,The non-convex Burer-Monteiro approach works on smooth semidefinite programs,Semidefinite programs (SDP's) can be solved in polynomial time by interior point methods  but scalability can be an issue. To address this shortcoming  over a decade ago  Burer and Monteiro proposed to solve SDP's with few equality constraints via rank-restricted  non-convex surrogates. Remarkably  for some applications  local optimization methods seem to converge to global optima of these non-convex surrogates reliably. Although some theory supports this empirical success  a complete explanation of it remains an open question. In this paper  we consider a class of SDP's which includes applications such as max-cut  community detection in the stochastic block model  robust PCA  phase retrieval and synchronization of rotations. We show that the low-rank Burer-Monteiro formulation of SDP's in that class almost never has any spurious local optima.,The non-convex Burer–Monteiro approach works

on smooth semideﬁnite programs

Nicolas Boumal�

Department of Mathematics

Princeton University

Vladislav Voroninski�

Department of Mathematics

Massachusetts Institute of Technology

nboumal@math.princeton.edu

vvlad@math.mit.edu

Afonso S. Bandeira

Department of Mathematics and Center for Data Science

Courant Institute of Mathematical Sciences  New York University

bandeira@cims.nyu.edu

Abstract

Semideﬁnite programs (SDP’s) can be solved in polynomial time by interior point
methods  but scalability can be an issue. To address this shortcoming  over a
decade ago  Burer and Monteiro proposed to solve SDP’s with few equality con-
straints via rank-restricted  non-convex surrogates. Remarkably  for some applica-
tions  local optimization methods seem to converge to global optima of these non-
convex surrogates reliably. Although some theory supports this empirical success 
a complete explanation of it remains an open question. In this paper  we consider a
class of SDP’s which includes applications such as max-cut  community detection
in the stochastic block model  robust PCA  phase retrieval and synchronization of
rotations. We show that the low-rank Burer–Monteiro formulation of SDP’s in
that class almost never has any spurious local optima.

1

Introduction

We consider semideﬁnite programs (SDP’s) of the form

f∗ = min

X∈Sn×n �C  X�

subject to A(X) = b  X � 0 

(SDP)

where �C  X� = Tr(C�X)  C ∈ Sn×n is the symmetric cost matrix  A : Sn×n → Rm is a lin-
ear operator capturing m equality constraints with right hand side b ∈ Rm and the variable X is
symmetric  positive semideﬁnite. Interior point methods solve (SDP) in polynomial time [Nesterov 
2004]. In practice however  for n beyond a few thousands  such algorithms run out of memory (and
time)  prompting research for alternative solvers.
If (SDP) has a compact search space  then it admits a global optimum of rank at most r  where
r(r+1)
≤ m [Pataki  1998  Barvinok  1995]. Thus  if one restricts the search space of (SDP) to
matrices of rank at most p with p(p+1)
2 ≥ m  then the globally optimal value remains unchanged.
This restriction is easily enforced by factorizing X = Y Y � where Y has size n × p  yielding an
equivalent quadratically constrained quadratic program:
(P)

q∗ = min

2

subject to A(Y Y �) = b.

Y ∈Rn×p �CY   Y �

�The ﬁrst two authors contributed equally.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In general  (P) is non-convex  making it a priori unclear how to solve it globally. Still  the beneﬁts
are that it is lower dimensional than (SDP) and has no conic constraint. This has motivated Burer
and Monteiro [2003  2005] to try and solve (P) using local optimization methods  with surprisingly
good results. They developed theory in support of this observation (details below). About their
results  Burer and Monteiro [2005  §3] write (mutatis mutandis):

“How large must we take p so that the local minima of (P) are guaranteed to map
to global minima of (SDP)? Our theorem asserts that we need only1 p(p+1)
2 > m
(with the important caveat that positive-dimensional faces of (SDP) which are
‘ﬂat’ with respect to the objective function can harbor non-global local minima).”

2

The caveat—the existence or non-existence of non-global local optima  or their potentially adverse
effect for local optimization algorithms—was not further discussed.
In this paper  assuming p(p+1)
2 > m  we show that if the search space of (SDP) is compact and if
the search space of (P) is a smooth manifold  then  for almost all cost matrices C  if Y satisﬁes ﬁrst-
and second-order necessary optimality conditions for (P)  then Y is a global optimum of (P) and 
since p(p+1)
≥ m  X = Y Y � is a global optimum of (SDP); in other words  ﬁrst- and second-
order necessary optimality conditions for (P) are also sufﬁcient for global optimality—an unusual
theoretical guarantee in non-convex optimization.
Notice that this is a statement about the optimization problem itself  not about speciﬁc algorithms.
Interestingly  known algorithms for optimization on manifolds converge to second-order critical
points 2 regardless of initialization [Boumal et al.  2016].
For the speciﬁed class of SDP’s  our result improves on those of [Burer and Monteiro  2005] in
two important ways. Firstly  for almost all C  we formally exclude the existence of spurious local
optima.3 Secondly  we only require the computation of second-order critical points of (P) rather
than local optima (which is hard in general [Vavasis  1991]). Below  we make a statement about
computational complexity  and we illustrate the practical efﬁciency of the proposed methods through
numerical experiments.
SDP’s which satisfy the compactness and smoothness assumptions occur in a number of applica-
tions including Max-Cut  robust PCA  Z2-synchronization  community detection  cut-norm approx-
imation  phase synchronization  phase retrieval  synchronization of rotations and the trust-region
subproblem—see Section 4 for references.

A simple example: the Max-Cut problem

Given an undirected graph  Max-Cut is the NP-hard problem of clustering the n nodes of this graph
in two classes  +1 and −1  such that as many edges as possible join nodes of different signs. If C is
the adjacency matrix of the graph  Max-Cut is expressed as

max
x∈Rn

1
4

n�i j=1

Cij(1 − xixj)

s.t.

x2
1 = ··· = x2

n = 1.

(Max-Cut)

Introducing the positive semideﬁnite matrix X = xx�  both the cost and the constraints may be ex-
pressed linearly in terms of X. Ignoring that X has rank 1 yields the well-known convex relaxation
in the form of a semideﬁnite program (up to an afﬁne transformation of the cost):

min

X∈Sn×n �C  X�

s.t.

diag(X) = 1  X � 0.

(Max-Cut SDP)

If a solution X of this SDP has rank 1  then X = xx�for some x which is then an optimal cut. In the
general case of higher rank X  Goemans and Williamson [1995] exhibited the celebrated rounding
scheme to produce approximately optimal cuts (within a ratio of .878) from X.

1The condition on p and m is slightly  but inconsequentially  different in [Burer and Monteiro  2005].
2Second-order critical points satisfy ﬁrst- and second-order necessary optimality conditions.
3Before Prop. 2.3 in [Burer and Monteiro  2005]  the authors write: “The change of variables X = Y Y �
does not introduce any extraneous local minima.” This is sometimes misunderstood to mean (P) does not have
spurious local optima  when it actually means that the local optima of (P) are in exact correspondence with the
local optima of “(SDP) with the extra constraint rank(X) ≤ p ” which is also non-convex and thus also liable
to having local optima. Unfortunately  this misinterpretation has led to some confusion in the literature.

2

The corresponding Burer–Monteiro non-convex problem with rank bounded by p is:

min

Y ∈Rn×p �CY   Y �

s.t.

diag(Y Y �) = 1.

(Max-Cut BM)

The constraint diag(Y Y �) = 1 requires each row of Y to have unit norm; that is: Y is a point on the
Cartesian product of n unit spheres in Rp  which is a smooth manifold. Furthermore  all X feasible
for the SDP have identical trace equal to n  so that the search space of the SDP is compact. Thus 
our results stated below apply:

For p =�√2n�  for almost all C  even though (Max-Cut BM) is non-convex  any

local optimum Y is a global optimum (and so is X = Y Y �)  and all saddle points
have an escape (the Hessian has a negative eigenvalue).

We note that  for p > n/2  the same holds for all C [Boumal  2015].

Notation

Sn×n is the set of real  symmetric matrices of size n. A symmetric matrix X is positive semideﬁnite
(X � 0) if and only if u�Xu ≥ 0 for all u ∈ Rn. For matrices A  B  the standard Euclidean inner
product is �A  B� = Tr(A�B). The associated (Frobenius) norm is �A� = ��A  A�. Id is the
identity operator and In is the identity matrix of size n.

2 Main results

Our main result establishes conditions under which ﬁrst- and second-order necessary optimality
conditions for (P) are sufﬁcient for global optimality. Under those conditions  it is a fortiori true that
global optima of (P) map to global optima of (SDP)  so that local optimization methods on (P) can
be used to solve the higher-dimensional  cone-constrained (SDP).
We now specify the necessary optimality conditions of (P). Under the assumptions of our main
result below (Theorem 2)  the search space

(1)
is a smooth and compact manifold. As such  it can be linearized at each point Y ∈ M by a tangent
space  simply by differentiating the constraints [Absil et al.  2008  eq. (3.19)]:

M = Mp = {Y ∈ Rn×p : A(Y Y �) = b}

TY M = { ˙Y ∈ Rn×p : A( ˙Y Y � + Y ˙Y �) = 0}.

(2)
Endowing the tangent spaces of M with the (restricted) Euclidean metric �A  B� = Tr(A�B) turns
M into a Riemannian submanifold of Rn×p. In general  second-order optimality conditions can
be intricate to handle [Ruszczy´nski  2006]. Fortunately  here  the smoothness of both the search
space (1) and the cost function

f (Y ) = �CY   Y �

(3)
make for straightforward conditions. In spirit  they coincide with the well-known conditions for un-
constrained optimization. As further detailed in Appendix A  the Riemannian gradient gradf (Y ) is
the orthogonal projection of the classical gradient of f to the tangent space TY M. The Riemannian
Hessian of f at Y is a similarly restricted version of the classical Hessian of f to the tangent space.
Deﬁnition 1. A (ﬁrst-order) critical point for (P) is a point Y ∈ M such that

(1st order nec. opt. cond.)
where gradf (Y ) ∈ TY M is the Riemannian gradient at Y of f restricted to M. A second-order
critical point for (P) is a critical point Y such that

gradf (Y ) = 0 

(2nd order nec. opt. cond.)
where Hessf (Y ) : TY M → TY M is the Riemannian Hessian at Y of f restricted to M (a sym-
metric linear operator).

Hessf (Y ) � 0 

3

Proposition 1. All local (and global) optima of (P) are second-order critical points.

Proof. See [Yang et al.  2014  Rem. 4.2 and Cor. 4.2].

We can now state our main result. In the theorem statement below  “for almost all C” means po-
tentially troublesome cost matrices form at most a (Lebesgue) zero-measure subset of Sn×n  in the
same way that almost all square matrices are invertible. In particular  given any matrix C ∈ Sn×n 
perturbing C to C + σW where W is a Wigner random matrix results in an acceptable cost matrix
with probability 1  for arbitrarily small σ > 0.
Theorem 2. Given constraints A : Sn×n → Rm  b ∈ Rm and p satisfying p(p+1)

2 > m  if

(i) the search space of (SDP) is compact; and

(ii) the search space of (P) is a smooth manifold 

then for almost all cost matrices C ∈ Sn×n  any second-order critical point of (P) is globally
optimal. Under these conditions  if Y is globally optimal for (P)  then the matrix X = Y Y � is
globally optimal for (SDP).

The assumptions are discussed in the next section. The proof—see Appendix A—follows directly
from the combination of two intermediate results:

1. If Y is rank deﬁcient and second-order critical for (P)  then it is globally optimal and

X = Y Y � is optimal for (SDP); and

2. If p(p+1)

2 > m  then  for almost all C  every ﬁrst-order critical Y is rank-deﬁcient.

The ﬁrst step holds in a more general context  as previously established by Burer and Monteiro
[2003  2005]. The second step is new and crucial  as it allows to formally exclude the existence of
spurious local optima  generically in C  thus resolving the caveat mentioned in the introduction.
The smooth structure of (P) naturally suggests using Riemannian optimization to solve it [Absil et al. 
2008]  which is something that was already proposed by Journ´ee et al. [2010] in the same context.
Importantly  known algorithms converge to second-order critical points regardless of initialization.
We state here a recent computational result to that effect.
Proposition 3. Under the numbered assumptions of Theorem 2  the Riemannian trust-region method
(RTR) [Absil et al.  2007] initialized with any Y0 ∈ M returns in O(1/ε2
H ) iterations a
point Y ∈ M such that
f (Y ) ≤ f (Y0) 

Hessf (Y ) � −εH Id .

gεH + 1/ε3

�gradf (Y )� ≤ εg 

and

Proof. Apply the main results of [Boumal et al.  2016] using that f has locally Lipschitz continuous
gradient and Hessian in Rn×p and M is a compact submanifold of Rn×p.
Essentially  each iteration of RTR requires evaluation of one cost and one gradient  a bounded num-
ber of Hessian-vector applications  and one projection from Rn×p to M. In many important cases 
this projection amounts to Gram–Schmidt orthogonalization of small blocks of Y —see Section 4.
Proposition 3 bounds worst-case iteration counts for arbitrary initialization.
In practice  a good
initialization point may be available  making the local convergence rate of RTR more informative.
For RTR  one may expect superlinear or even quadratic local convergence rates near isolated local
minimizers [Absil et al.  2007]. While minimizers are not isolated in our case [Journ´ee et al.  2010] 
experiments show a characteristically superlinear local convergence rate in practice [Boumal  2015].
This means high accuracy solutions can be achieved  as demonstrated in Appendix B.
Thus  under the conditions of Theorem 2  generically in C  RTR converges to global optima. In
practice  the algorithm returns after a ﬁnite number of steps  and only approximate second-order
criticality is guaranteed. Hence  it is interesting to bound the optimality gap in terms of the approx-
imation quality. Unfortunately  we do not establish such a result for small p. Instead  we give an
a posteriori computable optimality gap bound which holds for all p and for all C. In the following
statement  the dependence of M on p is explicit  as Mp. The proof is in Appendix A.

4

Theorem 4. Let R < ∞ be the maximal trace of any X feasible for (SDP). For any p such that Mp
and Mp+1 are smooth manifolds (even if p(p+1)
2 ≤ m) and for any Y ∈ Mp  form ˜Y = [Y |0n×1]
in Mp+1. The optimality gap at Y is bounded as

(4)
If all feasible X have the same trace R and there exists a positive deﬁnite feasible X  then the bound
simpliﬁes to

0 ≤ 2(f (Y ) − f∗) ≤

√R�gradf (Y )� − Rλmin(Hessf ( ˜Y )).

0 ≤ 2(f (Y ) − f∗) ≤ −Rλmin(Hessf ( ˜Y ))

(5)

so that �gradf (Y )� needs not be controlled explicitly. If p > n  the bounds hold with ˜Y = Y .
In particular  for p = n + 1  the bound can be controlled a priori: approximate second-order critical
points are approximately optimal  for any C.4
Corollary 5. Under the assumptions of Theorem 4  if p = n + 1 and Y ∈ M satisﬁes both
�gradf (Y )� ≤ εg and Hessf (Y ) � −εH Id  then Y is approximately optimal in the sense that

Under the same condition as in Theorem 4  the bound can be simpliﬁed to RεH.

0 ≤ 2(f (Y ) − f∗) ≤

√Rεg + RεH .

This works well with Proposition 3. For any p  equation (4) also implies the following:

λmin(Hessf ( ˜Y )) ≤ −

2(f (Y ) − f∗) − √R�gradf (Y )�

.

R

That is  for any p and any C  an approximate critical point Y in Mp which is far from optimal maps
to a comfortably-escapable approximate saddle point ˜Y in Mp+1.
This suggests an algorithm as follows. For a starting value of p such that Mp is a manifold  use
RTR to compute an approximate second-order critical point Y . Then  form ˜Y in Mp+1 and test
the left-most eigenvalue of Hessf ( ˜Y ).5 If it is close enough to zero  this provides a good bound
on the optimality gap. If not  use an (approximate) eigenvector associated to λmin(Hessf ( ˜Y )) to
escape the approximate saddle point and apply RTR from that new point in Mp+1; iterate. In the
worst-case scenario  p grows to n + 1  at which point all approximate second-order critical points
are approximate optima. Theorem 2 suggests p =�√2m� should sufﬁce for C bounded away from

a zero-measure set. Such an algorithm already features with less theory in [Journ´ee et al.  2010]
and [Boumal  2015]; in the latter  it is called the Riemannian staircase  for it lifts (P) ﬂoor by ﬂoor.

Related work

Low-rank approaches to solve SDP’s have featured in a number of recent research papers. We
highlight just two which illustrate different classes of SDP’s of interest.
Shah et al. [2016] tackle SDP’s with linear cost and linear constraints (both equalities and inequal-
ities) via low-rank factorizations  assuming the matrices appearing in the cost and constraints are
positive semideﬁnite. They propose a non-trivial initial guess to partially overcome non-convexity
with great empirical results  but do not provide optimality guarantees.
Bhojanapalli et al. [2016a] on the other hand consider the minimization of a convex cost function
over positive semideﬁnite matrices  without constraints. Such problems could be obtained from
generic SDP’s by penalizing the constraints in a Lagrangian way. Here too  non-convexity is partially
overcome via non-trivial initialization  with global optimality guarantees under some conditions.
Also of interest are recent results about the harmlessness of non-convexity in low-rank matrix com-
pletion [Ge et al.  2016  Bhojanapalli et al.  2016b]. Similarly to the present work  the authors there
show there is no need for special initialization despite non-convexity.

4With p = n + 1  problem (P) is no longer lower dimensional than (SDP)  but retains the advantage of not

involving a positive semideﬁniteness constraint.

5It may be more practical to test λmin(S) (14) rather than λmin(Hessf ). Lemma 7 relates the two.

See [Journ´ee et al.  2010  §3.3] to construct escape tangent vectors from S.

5

3 Discussion of the assumptions

Our main result  Theorem 2  comes with geometric assumptions on the search spaces of both (SDP)
and (P) which we now discuss. Examples of SDP’s which ﬁt the assumptions of Theorem 2 are
featured in the next section.
The assumption that the search space of (SDP) 

C = {X ∈ Sn×n : A(X) = b  X � 0} 

(6)

is compact works in pair with the assumption p(p+1)
2 > m as follows. For (P) to reveal the global
optima of (SDP)  it is necessary that (SDP) admits a solution of rank at most p. One way to ensure
this is via the Pataki–Barvinok theorems [Pataki  1998  Barvinok  1995]  which state that all extreme
points of C have rank r bounded as r(r+1)
2 ≤ m. Extreme points are faces of dimension zero (such
as vertices for a cube). When optimizing a linear cost function �C  X� over a compact convex set C 
at least one extreme point is a global optimum [Rockafellar  1970  Cor. 32.3.2]—this is not true in
general if C is not compact. Thus  under the assumptions of Theorem 2  there is a point Y ∈ M such
that X = Y Y � is an optimal extreme point of (SDP); then  of course  Y itself is optimal for (P).
In general  the Pataki–Barvinok bound is tight  in that there exist extreme points of rank up to that
upper-bound (rounded down)—see for example [Laurent and Poljak  1996] for the Max-Cut SDP
and [Boumal  2015] for the Orthogonal-Cut SDP. Let C (the cost matrix) be the negative of such an
extreme point. Then  the unique optimum of (SDP) is that extreme point  showing that p(p+1)
2 ≥ m
is necessary for (SDP) and (P) to be equivalent for all C. We further require a strict inequality
because our proof relies on properties of rank deﬁcient Y ’s in M.
The assumption that M (eq. (1)) is a smooth manifold works in pair with the ambition that the re-
sult should hold for (almost) all cost matrices C. The starting point is that  for a given non-convex
smooth optimization problem—even a quadratically constrained quadratic program—computing lo-
cal optima is hard in general [Vavasis  1991]. Thus  we wish to restrict our attention to efﬁciently
computable points  such as points which satisfy ﬁrst- and second-order KKT conditions for (P)—
see [Burer and Monteiro  2003  §2.2] and [Ruszczy´nski  2006  §3]. This only makes sense if global
optima satisfy the latter  that is  if KKT conditions are necessary for optimality. A global optimum
Y necessarily satisﬁes KKT conditions if constraint qualiﬁcations (CQ’s) hold at Y [Ruszczy´nski 
2006]. The standard CQ’s for equality constrained programs are Robinson’s conditions or metric
regularity (they are here equivalent). They read as follows  assuming A(Y Y �)i =�Ai  Y Y �� for
some matrices A1  . . .   Am ∈ Sn×n:

CQ’s hold at Y if A1Y  . . .   AmY are linearly independent in Rn×p.

(7)
Considering almost all C  global optima could  a priori  be almost anywhere in M. To simplify 
we require CQ’s to hold at all Y ’s in M rather than only at the (unknown) global optima. This
turns out to be a sufﬁcient condition for M to be a smooth manifold of codimension m [Absil et al. 
2008  Prop. 3.3.3]. Indeed  tangent vectors ˙Y ∈ TY M (2) are exactly those vectors that satisfy
�AiY   ˙Y � = 0: under CQ’s  the AiY ’s form a basis of the normal space to the manifold at Y .
Once it is decided that M must be a manifold  we can step away from the speciﬁc representation
of it via the matrices A1  . . .   Am and reason about optimality conditions on the manifold directly.
Adding redundant constraints (for example  duplicating A1) would break the CQ’s  but not the
manifold structure. Hence  stating Theorem 2 in terms of manifolds better captures the role of M
than stating it in terms of CQ’s. See also [Andreani et al.  2010  Thm. 3.3] for a proof that requiring
M to be a manifold around Y is a type of CQ.
Finally  we note that Theorem 2 only applies for almost all C  rather than all C. To justify this
restriction  if indeed it is justiﬁed  one should exhibit a matrix C that leads to suboptimal second-
order critical points while other assumptions are satisﬁed. We do not have such an example. We do
observe that (Max-Cut SDP) on cycles of certain even lengths has a unique solution of rank 1  while
the corresponding (Max-Cut BM) with p = 2 has suboptimal local optima (strictly  if we quotient
out symmetries). This at least suggests it is not enough  for generic C  to set p just larger than
the rank of the solutions of the SDP. (For those same examples  at p = 3  we consistently observe
convergence to global optima.)

6

4 Examples of smooth SDP’s

The canonical examples of SDP’s which satisfy the assumptions in Theorem 2 are those where the
diagonal blocks of X or their traces are ﬁxed. We note that the algorithms and the theory continue
to hold for complex matrices  where the set of Hermitian matrices of size n is treated as a real
vector space of dimension n2 (instead of n(n+1)
in the real case) with inner product �H1  H2� =
�{Tr(H∗1 H2)}  so that occurrences of p(p+1)
2
Certain concrete examples of SDP’s include:

are replaced by p2.

2

X �C  X� s.t. Tr(X) = 1  X � 0;
min
X �C  X� s.t. diag(X) = 1  X � 0;
min
X �C  X� s.t. Xii = Id  X � 0.
min

(ﬁxed trace)

(ﬁxed diagonal)

(ﬁxed diagonal blocks)

Their rank-constrained counterparts read as follows (matrix norms are Frobenius norms):

min

min

min

Y : n×p�CY   Y � s.t. �Y � = 1;
Y : n×p�CY   Y � s.t. Y � = [y1
Y : qd×p�CY   Y � s.t. Y � = [Y1

(sphere)

···
···

yn] and �yi� = 1 for all i;
Yq] and Y �i Yi = Id for all i.

(product of spheres)

(product of Stiefel)

The ﬁrst example has only one constraint: the SDP always admits an optimal rank 1 solution  cor-
responding to an eigenvector associated to the left-most eigenvalue of C. This generalizes to the
trust-region subproblem as well.
For the second example  in the real case  p = 1 forces yi = ±1  allowing to capture combinatorial
problems such as Max-Cut [Goemans and Williamson  1995]  Z2-synchronization [Javanmard et al. 
2015] and community detection in the stochastic block model [Abbe et al.  2016  Bandeira et al. 
2016b]. The same SDP is central in a formulation of robust PCA [McCoy and Tropp  2011] and
is used to approximate the cut-norm of a matrix [Alon and Naor  2006]. Theorem 2 states that for

capture problems where phases must be recovered; in particular  phase synchronization [Bandeira
et al.  2016a  Singer  2011] and phase retrieval via Phase-Cut [Waldspurger et al.  2015]. For almost

almost all C  p = �√2n� is sufﬁcient. In the complex case  p = 1 forces |yi| = 1  allowing to
all C  it is then sufﬁcient to set p = �√n + 1�.
In the third example  Y of size n × p is divided in q slices of size d × p  with p ≥ d. Each
slice has orthonormal rows. For p = d  the slices are orthogonal (or unitary) matrices  allowing
to capture Orthogonal-Cut [Bandeira et al.  2016c] and the related problems of synchronization of
rotations [Wang and Singer  2013] and permutations. Synchronization of rotations is an important
step in simultaneous localization and mapping  for example. Here  it is sufﬁcient for almost all C to

let p =��d(d + 1)q�.

SDP’s with constraints that are combinations of the above examples can also have the smoothness
property; the right-hand sides 1 and Id can be replaced by any positive deﬁnite right-hand sides by a
change of variables. Another simple rule to check is if the constraint matrices A1  . . .   Am ∈ Sn×n
such that A(X)i = �Ai  X� satisfy AiAj = 0 for all i �= j (note that this is stronger than requiring
�Ai  Aj� = 0)  see [Journ´ee et al.  2010].
5 Conclusions

The Burer–Monteiro approach consists in replacing optimization of a linear function �C  X� over
the convex set {X � 0 : A(X) = b} with optimization of the quadratic function �CY   Y � over the
non-convex set {Y ∈ Rn×p : A(Y Y �) = b}. It was previously known that  if the convex set is
compact and p satisﬁes p(p+1)
2 ≥ m where m is the number of constraints  then these two problems
have the same global optimum. It was also known from [Burer and Monteiro  2005] that spurious
local optima Y   if they exist  must map to special faces of the compact convex set  but without
statement as to the prevalence of such faces or the risk they pose for local optimization methods. In

7

this paper we showed that  if the set of X’s is compact and the set of Y ’s is a smooth manifold  and
if p(p+1)
2 > m  then for almost all C  the non-convexity of the problem in Y is benign  in that all
Y ’s which satisfy second-order necessary optimality conditions are in fact globally optimal.
We further reference the Riemannian trust-region method [Absil et al.  2007] to solve the problem in
Y   as it was recently guaranteed to converge from any starting point to a point which satisﬁes second-
order optimality conditions  with global convergence rates [Boumal et al.  2016]. In addition  for p =
n + 1  we guarantee that approximate satisfaction of second-order conditions implies approximate
global optimality. We note that the 1/ε3 convergence rate in our results may be pessimistic. Indeed 
the numerical experiments clearly show that high accuracy solutions can be computed fast using
optimization on manifolds  at least for certain applications.
Addressing a broader class of SDP’s  such as those with inequality constraints or equality constraints
that may violate our smoothness assumptions  could perhaps be handled by penalizing those con-
straints in the objective in an augmented Lagrangian fashion. We also note that  algorithmically 
the Riemannian trust-region method we use applies just as well to nonlinear costs in the SDP. We
believe that extending the theory presented here to broader classes of problems is a good direction
for future work.

Acknowledgment

VV was partially supported by the Ofﬁce of Naval Research. ASB was supported by NSF Grant
DMS-1317308. Part of this work was done while ASB was with the Department of Mathematics at
the Massachusetts Institute of Technology. We thank Wotao Yin and Michel Goemans for helpful
discussions.

References
E. Abbe  A.S. Bandeira  and G. Hall. Exact recovery in the stochastic block model. Information Theory  IEEE

Transactions on  62(1):471–487  2016.

P.-A. Absil  C. G. Baker  and K. A. Gallivan. Trust-region methods on Riemannian manifolds. Foundations of

Computational Mathematics  7(3):303–330  2007. doi:10.1007/s10208-005-0179-9.

P.-A. Absil  R. Mahony  and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton University

Press  Princeton  NJ  2008. ISBN 978-0-691-13298-3.

N. Alon and A. Naor. Approximating the cut-norm via Grothendieck’s inequality. SIAM Journal on Computing 

35(4):787–803  2006. doi:10.1137/S0097539704441629.

R. Andreani  C. E. Echag¨ue  and M. L. Schuverdt. Constant-rank condition and second-order constraint qual-
iﬁcation. Journal of Optimization Theory and Applications  146(2):255–266  2010. doi:10.1007/s10957-
010-9671-8.

A.S. Bandeira  N. Boumal  and A. Singer. Tightness of the maximum likelihood semideﬁnite relaxation for
angular synchronization. Mathematical Programming  pages 1–23  2016a. doi:10.1007/s10107-016-1059-6.

A.S. Bandeira  N. Boumal  and V. Voroninski. On the low-rank approach for semideﬁnite programs arising
in synchronization and community detection. In Proceedings of The 29th Conference on Learning Theory 
COLT 2016  New York  NY  June 23–26  2016b.

A.S. Bandeira  C. Kennedy  and A. Singer. Approximating the little Grothendieck problem over the orthogonal

and unitary groups. Mathematical Programming  pages 1–43  2016c. doi:10.1007/s10107-016-0993-7.

A.I. Barvinok. Problems of distance geometry and convex properties of quadratic maps. Discrete & Computa-

tional Geometry  13(1):189–202  1995. doi:10.1007/BF02574037.

S. Bhojanapalli  A. Kyrillidis  and S. Sanghavi. Dropping convexity for faster semi-deﬁnite optimization.

Conference on Learning Theory (COLT)  2016a.

S. Bhojanapalli  B. Neyshabur  and N. Srebro. Global optimality of local search for low rank matrix recovery.

arXiv preprint arXiv:1605.07221  2016b.

N. Boumal. A Riemannian low-rank method for optimization over semideﬁnite matrices with block-diagonal

constraints. arXiv preprint arXiv:1506.00575  2015.

8

N. Boumal  B. Mishra  P.-A. Absil  and R. Sepulchre. Manopt  a Matlab toolbox for optimization on manifolds.

Journal of Machine Learning Research  15:1455–1459  2014. URL http://www.manopt.org.

N. Boumal  P.-A. Absil  and C. Cartis. Global rates of convergence for nonconvex optimization on manifolds.

arXiv preprint arXiv:1605.08101  2016.

S. Burer and R.D.C. Monteiro. A nonlinear programming algorithm for solving semideﬁnite programs via low-

rank factorization. Mathematical Programming  95(2):329–357  2003. doi:10.1007/s10107-002-0352-8.

S. Burer and R.D.C. Monteiro. Local minima and convergence in low-rank semideﬁnite programming. Math-

ematical Programming  103(3):427–444  2005.

CVX. CVX: Matlab software for disciplined convex programming. http://cvxr.com/cvx  August 2012.

R. Ge  J.D. Lee  and T. Ma. Matrix completion has no spurious local minimum.

arXiv:1605.07272  2016.

arXiv preprint

M.X. Goemans and D.P. Williamson.

Improved approximation algorithms for maximum cut and satisﬁa-
bility problems using semideﬁnite programming. Journal of the ACM (JACM)  42(6):1115–1145  1995.
doi:10.1145/227683.227684.

C. Helmberg  F. Rendl  R.J. Vanderbei  and H. Wolkowicz. An interior-point method for semideﬁnite program-

ming. SIAM Journal on Optimization  6(2):342–361  1996. doi:10.1137/0806020.

A. Javanmard  A. Montanari  and F. Ricci-Tersenghi. Phase transitions in semideﬁnite relaxations. arXiv

preprint arXiv:1511.08769  2015.

M. Journ´ee  F. Bach  P.-A. Absil  and R. Sepulchre. Low-rank optimization on the cone of positive semideﬁnite

matrices. SIAM Journal on Optimization  20(5):2327–2351  2010. doi:10.1137/080731359.

M. Laurent and S. Poljak. On the facial structure of the set of correlation matrices. SIAM Journal on Matrix

Analysis and Applications  17(3):530–547  1996. doi:10.1137/0617031.

M. McCoy and J.A. Tropp. Two proposals for robust PCA using semideﬁnite programming. Electronic Journal

of Statistics  5:1123–1160  2011. doi:10.1214/11-EJS636.

Y. Nesterov. Introductory lectures on convex optimization: A basic course  volume 87 of Applied optimization.

Springer  2004. ISBN 978-1-4020-7553-7.

G. Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of optimal eigenvalues.

Mathematics of operations research  23(2):339–358  1998. doi:10.1287/moor.23.2.339.

R.T. Rockafellar. Convex analysis. Princeton University Press  Princeton  NJ  1970.

A.P. Ruszczy´nski. Nonlinear optimization. Princeton University Press  Princeton  NJ  2006.

S. Shah  A. Kumar  D. Jacobs  C. Studer  and T. Goldstein. Biconvex relaxation for semideﬁnite programming

in computer vision. arXiv preprint arXiv:1605.09527  2016.

A. Singer. Angular synchronization by eigenvectors and semideﬁnite programming. Applied and Computa-

tional Harmonic Analysis  30(1):20–36  2011. doi:10.1016/j.acha.2010.02.001.

K.C. Toh  M.J. Todd  and R.H. T¨ut¨unc¨u. SDPT3–a MATLAB software package for semideﬁnite programming.

Optimization Methods and Software  11(1–4):545–581  1999. doi:10.1080/10556789908805762.

S.A. Vavasis. Nonlinear optimization: complexity issues. Oxford University Press  Inc.  1991.

I. Waldspurger  A. d’Aspremont  and S. Mallat. Phase recovery  MaxCut and complex semideﬁnite program-

ming. Mathematical Programming  149(1–2):47–81  2015. doi:10.1007/s10107-013-0738-9.

L. Wang and A. Singer. Exact and stable recovery of rotations for robust synchronization. Information and

Inference  2(2):145–193  2013. doi:10.1093/imaiai/iat005.

Z. Wen and W. Yin. A feasible method for optimization with orthogonality constraints. Mathematical Pro-

gramming  142(1–2):397–434  2013. doi:10.1007/s10107-012-0584-1.

W.H. Yang  L.-H. Zhang  and R. Song. Optimality conditions for the nonlinear programming problems on

Riemannian manifolds. Paciﬁc Journal of Optimization  10(2):415–434  2014.

9

,Nicolas Boumal
Vlad Voroninski