2014,A statistical model for tensor PCA,We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand  we use information theory  and recent results in probability theory to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio beta becomes larger than C\sqrt{k log k} (and in particular beta can remain bounded has the problem dimensions increase). On the other hand  we analyze several polynomial-time estimation algorithms  based on tensor unfolding  power iteration and message passing ideas from graphical models. We show that  unless the signal-to-noise ratio diverges in the system dimensions  none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. For moderate dimensions  we propose an hybrid approach that uses unfolding together with power iteration  and show that it outperforms significantly baseline methods. Finally  we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allow the iterative algorithms to converge to a good estimate.,A statistical model for tensor PCA

Andrea Montanari

Statistics & Electrical Engineering

Stanford University

Emile Richard

Electrical Engineering
Stanford University

Abstract

√

We consider the Principal Component Analysis problem for large tensors of ar-
bitrary order k under a single-spike (or rank-one plus noise) model. On the one
hand  we use information theory  and recent results in probability theory  to es-
tablish necessary and sufﬁcient conditions under which the principal component
can be estimated using unbounded computational resources. It turns out that this
is possible as soon as the signal-to-noise ratio β becomes larger than C
k log k
(and in particular β can remain bounded as the problem dimensions increase).
On the other hand  we analyze several polynomial-time estimation algorithms 
based on tensor unfolding  power iteration and message passing ideas from graph-
ical models. We show that  unless the signal-to-noise ratio diverges in the system
dimensions  none of these approaches succeeds. This is possibly related to a fun-
damental limitation of computationally tractable estimators for this problem.
We discuss various initializations for tensor power iteration  and show that a
tractable initialization based on the spectrum of the unfolded tensor outperforms
signiﬁcantly baseline methods  statistically and computationally. Finally  we con-
sider the case in which additional side information is available about the unknown
signal. We characterize the amount of side information that allows the iterative
algorithms to converge to a good estimate.

1

Introduction

Given a data matrix X  Principal Component Analysis (PCA) can be regarded as a ‘denoising’ tech-
nique that replaces X by its closest rank-one approximation. This optimization problem can be
solved efﬁciently  and its statistical properties are well-understood. The generalization of PCA to
tensors is motivated by problems in which it is important to exploit higher order moments  or data
elements are naturally given more than two indices. Examples include topic modeling  video pro-
cessing  collaborative ﬁltering in presence of temporal/context information  community detection
[1]  spectral hypergraph theory. Further  ﬁnding a rank-one approximation to a tensor is a bottle-
neck for tensor-valued optimization algorithms using conditional gradient type of schemes. While
tensor factorization is NP-hard [11]  this does not necessarily imply intractability for natural statis-
tical models. Over the last ten years  it was repeatedly observed that either convex optimization or
greedy methods yield optimal solutions to statistical problems that are intractable from a worst case
perspective (well-known examples include sparse regression and low-rank matrix completion).
In order to investigate the fundamental tradeoffs between computational resources and statistical
power in tensor PCA  we consider the simplest possible model where this arises  whereby an un-
known unit vector v0 is to be inferred from noisy multilinear measurements. Namely  for each
unordered k-uple {i1  i2  . . .   ik} ⊆ [n]  we measure

(1)
with Z Gaussian noise (see below for a precise deﬁnition) and wish to reconstruct v0. In tensor
notation  the observation model reads (see the end of this section for notations)

Xi1 i2 ... ik = β(v0)i1 (v0)i2 ··· (v0)ik + Zi1 i2 ... ik  

X = β v0

⊗k + Z .

Spiked Tensor Model

1

This is analogous to the so called ‘spiked covariance model’ used to study matrix PCA in high
dimensions [12].
It is immediate to see that maximum-likelihood estimator vML is given by a solution of the following
problem

maximize
subject to

(cid:104)X  v⊗k(cid:105) 
(cid:107)v(cid:107)2 = 1 .

Tensor PCA

Solving it exactly is –in general– NP hard [11].
We next summarize our results. Note that  given a completely observed rank-one symmetric tensor
⊗k (i.e. for β = ∞)  it is easy to recover the vector v0 ∈ Rn. It is therefore natural to ask the
v0
question for which signal-to-noise ratios can one still reliably estimate v0? The answer appears to
depend dramatically on the computational resources1.
Ideal estimation. Assuming unbounded computational resources  we can solve the Tensor PCA
results in probability theory to show that this approach is successful for β ≥ µk =
ok(1)). In particular  above this threshold2 we have  with high probability 

optimization problem and hence implement the maximum likelihood estimator(cid:98)vML. We use recent

k log k(1 +

√

(cid:107)(cid:98)vML − v0(cid:107)2

2

(cid:46) 2.01 µk

β

.

(2)

√
We use an information-theoretic argument to show that no approach can do signiﬁcantly better 
namely no procedure can estimate v0 accurately for β ≤ c

k (for c a universal constant).

Tractable estimators: Unfolding. We consider two approaches to estimate v0 that can be
implemented in polynomial time. The ﬁrst approach is based on tensor unfolding: starting from

the tensor X ∈(cid:78)k Rn  we produce a matrix Mat(X) of dimensions nq × nk−q. We then perform

matrix PCA on Mat(X). We show that this method is successful for β (cid:38) n((cid:100)k/2(cid:101)−1)/2. A heuristics
argument suggests that the necessary and sufﬁcient condition for tensor unfolding to succeed is
indeed β (cid:38) n(k−2)/4 (which is below the rigorous bound by a factor n1/4 for k odd). We can
indeed conﬁrm this conjecture for k even and under an asymmetric noise model.

Tractable estimators: Warm-start power iteration and Approximate Message Passing. We
prove that  initializing power iteration uniformly at random  it converges very rapidly to an accurate
estimate provided β (cid:38) n(k−1)/2. A heuristic argument suggests that the correct necessary and
sufﬁcient threshold is given by β (cid:38) n(k−2)/2. Motivated by the last observation  we consider a
‘warm-start’ power iteration algorithm  in which we initialize power iteration with the output of
tensor unfolding. This approach appears to have the same threshold signal-to-noise ratio as simple
unfolding  but signiﬁcantly better accuracy above that threshold. We extend power iteration to
an approximate message passing (AMP) algorithm [7  4]. We show that the behavior of AMP is
qualitatively similar to the one of naive power iteration. In particular  AMP fails for any β bounded
as n → ∞.

Side information. Given the above computational complexity barrier  it is natural to study weaker
version of the original problem. Here we assume that extra information about v0 is available. This
can be provided by additional measurements or by approximately solving a related problem  for
instance a matrix PCA problem as in [1]. We model this additional information as y = γv0 + g
(with g an independent Gaussian noise vector)  and incorporate it in the initial condition of AMP
algorithm. We characterize exactly the threshold value γ∗ = γ∗(β) above which AMP converges to
an accurate estimator. The thresholds for various classes of algorithms are summarized below.

1Here we write F (n) (cid:46) G(n) if there exists a constant c independent of n (but possibly dependent on n 

2Note that  for k even  v0 can only be recovered modulo sign. For the sake of simplicity  we assume here

such that F (n) ≤ c G(n)

that this ambiguity is correctly resolved.

2

Method

Tensor Unfolding

Tensor Power Iteration (with random init.)

Maximum Likelihood

Information-theory lower bound

O(n((cid:100)k/2(cid:101)−1)/2)
O(n(k−1)/2)

1
1

n(k−2)/4
n(k−2)/2

–
–

Required β (rigorous) Required β (heuristic)

We will conclude the paper with some insights that we believe provide useful guidance for tensor
factorization heuristics. We illustrate these insights through simulations.

1.1 Notations

Given X ∈ (cid:78)k Rn a real k-th order tensor  we let {Xi1 ... ik}i1 ... ik denote its coordinates and

deﬁne a map X : Rn → Rn  by letting  for v ∈ Rn 

X{v}i =

Xi j1 ···  jk−1 vj1 ··· vjk−1 .

j1 ···  jk−1∈[n]

The outer product of two tensors is X⊗Y  and  for v ∈ Rn  we deﬁne v⊗k = v⊗···⊗v ∈(cid:78)k Rn
as the k-th outer power of v. We deﬁne the inner product of two tensors X  Y ∈(cid:78)k Rn as
We deﬁne the Frobenius (Euclidean) norm of a tensor X  by (cid:107)X(cid:107)F =(cid:112)(cid:104)X  X(cid:105)  and its operator

Xi1 ···  ik Yi1 ···  ik .

(cid:104)X  Y(cid:105) =

(cid:88)

i1 ···  ik∈[n]

(4)

(3)

(cid:88)

norm by

(cid:107)X(cid:107)op ≡ max{(cid:104)X  u1 ⊗ ··· ⊗ uk(cid:105) : ∀i ∈ [k]   (cid:107)ui(cid:107)2 ≤ 1}.

(5)
For the special case k = 2  it reduces to the ordinary (cid:96)2 matrix operator norm. For π ∈ Sk  we
will denote by Xπ the tensor with permuted indices Xπ
= Xπ(i1) ···  π(ik). We call the tensor
X symmetric if  for any permutation π ∈ Sk  Xπ = X. It is proved [23] that  for symmetric
tensors  the value of problem Tensor PCA coincides with (cid:107)X(cid:107)op up to a sign. More precisely  for
symmetric tensors we have the equivalent representation max{|(cid:104)X  u⊗k(cid:105)| : (cid:107)u(cid:107)2 ≤ 1}. We denote

by G ∈ (cid:78)k Rn a tensor with independent and identically distributed entries Gi1 ···  ik ∼ N(0  1)
Z ∈(cid:78)k Rn by

(note that this tensor is not symmetric). We deﬁne the symmetric standard normal noise tensor

i1 ···  ik

Z =

1
k!

(cid:114)
(cid:16)(cid:107)(cid:98)v − v0(cid:107)2

k
n

Gπ .

(cid:88)
2 (cid:107)(cid:98)v + v0(cid:107)2

π∈Sk

2

(cid:17)

= 2 − 2|(cid:104)(cid:98)v  v0(cid:105)| .

(6)

(7)

We use the loss function

Loss((cid:98)v  v0) ≡ min

2

Ideal estimation

In this section we consider the problem of estimating v0 under the observation model Spiked Tensor
Model  when no constraint is imposed on the complexity of the estimator. Our ﬁrst result is a lower
bound on the loss of any estimator.
⊗kRn → Sn−1)  we have  for all n ≥ 4 

Theorem 1. For any estimator(cid:98)v = (cid:98)v(X) of v0 from data X  such that (cid:107)(cid:98)v(X)(cid:107)2 = 1 (i.e. (cid:98)v :

(cid:114)

β ≤

k
10

⇒ E Loss((cid:98)v  v0) ≥ 1

32

.

(8)

In order to establish a matching upper bound on the loss  we consider the maximum likelihood

estimator(cid:98)vML  obtained by solving the Tensor PCA problem. As in the case of matrix denoising  we

expect the properties of this estimator to depend on signal to noise ratio β  and on the ‘norm’ of the
noise (cid:107)Z(cid:107)op (i.e. on the value of the optimization problem Tensor PCA in the case β = 0). For the

3

matrix case k = 2  this coincides with the largest eigenvalue of Z. Classical random matrix theory
shows that –in this case– (cid:107)Z(cid:107)op concentrates tightly around 2 [10  6  3].
It turns out that tight results for k ≥ 3 follow immediately from a technically sophisticated analysis
of the stationary points of random Morse functions by Aufﬁnger  Ben Arous and Cerny [2].
Lemma 2.1. There exists a sequence of real numbers {µk}k≥2  such that

(cid:107)Z(cid:107)op ≤ µk
lim sup
n→∞
n→∞(cid:107)Z(cid:107)op = µk
lim

(k odd) 

(k even).

Further (cid:107)Z(cid:107)op concentrates tightly around its expectation. Namely  for any n  k

P(cid:0)(cid:12)(cid:12)(cid:107)Z(cid:107)op − E(cid:107)Z(cid:107)op

(cid:12)(cid:12) ≥ s(cid:1) ≤ 2 e−ns2/(2k) .

√

k log k(1 + ok(1)) for large k.

Finally µk =
For instance  a large order-3 Gaussian tensor should have (cid:107)Z(cid:107)op ≈ 2.87  while a large order 10
tensor has (cid:107)Z(cid:107)op ≈ 6.75. As a simple consequence of Lemma 2.1  we establish an upper bound on
the error incurred by the maximum likelihood estimator.

Theorem 2. Let µk be the sequence of real numbers introduced above. Letting (cid:98)vML denote the

maximum likelihood estimator (i.e. the solution of Tensor PCA)  we have for n large enough  and
all s > 0

(9)

(10)

(11)

β ≥ µk ⇒ Loss((cid:98)vML  v0) ≤ 2

β

(µk + s)  

(12)

with probability at least 1 − 2e−ns2/(16k).

The following upper bound on the value of the Tensor PCA problem is proved using Sudakov-
Fernique inequality. While it is looser than Lemma 2.1 (corresponding to the case β = 0)  we
expect it to become sharp for β ≥ βk a suitably large constant.
Lemma 2.2. Under Spiked Tensor Model model  we have

E(cid:107)Z(cid:107)op ≤ max
τ≥0

(cid:110)
P(cid:0)(cid:12)(cid:12)(cid:107)Z(cid:107)op − E(cid:107)Z(cid:107)op

β

(cid:19)k

τ√
1 + τ 2

(cid:18)
(cid:12)(cid:12) ≥ s(cid:1) ≤ 2 e−ns2/(2k) .

+

k√
1 + τ 2

(cid:111)

.

(13)

(14)

lim sup
n→∞

Further  for any s ≥ 0 

3 Tensor Unfolding

A simple and popular heuristics to obtain tractable estimators of v0 consists in constructing a suit-
able matrix with the entries of X  and performing PCA on this matrix.

3.1 Symmetric noise
For an integer k ≥ q ≥ k/2  we introduce the unfolding (also referred to as matricization or
reshape) operator Matq : ⊗kRn → Rnq×nk−q as follows. For any indices i1  i2 ···   ik ∈ [n] 

we let a = 1 +(cid:80)q

j=1(ij − 1)nj−1 and b = 1 +(cid:80)k

j=q+1(ij − 1)nj−q−1  and deﬁne

[Matq(X)]a b = Xi1 i2 ···  ik .

(15)

Standard convex relaxations of low-rank tensor estimation problem compute factorizations of
Matq(X)[22  15  17  19]. Not all unfoldings (choices of q) are equivalent. It is natural to expect that
this approach will be successful only if the signal-to-noise ratio exceeds the operator norm of the
unfolded noise (cid:107)Matq(Z)(cid:107)op. The next lemma suggests that the latter is minimal when Matq(Z) is
‘as square as possible’ . A similar phenomenon was observed in a different context in [17].

4

Lemma 3.1. For any integer k/2 ≤ q ≤ k we have  for some universal constant Ck 

n(q−1)/2(cid:16)

1(cid:112)(k − 1)!

1 −

Ck

nmax(q k−q))

n(q−1)/2 + n(k−q−1)/2(cid:17)

√

(cid:16)

(cid:17) ≤ E(cid:107)Matq(Z)(cid:107)op ≤
(cid:111) ≤ 2 e−nt2/(2k) .
(cid:12)(cid:12) ≥ t

k

.

(16)

(17)

For all n large enough  both bounds are minimized for q = (cid:100)k/2(cid:101). Further

P(cid:110)(cid:12)(cid:12)(cid:107)Matq(Z)(cid:107)op − E(cid:107)Matq(Z)(cid:107)op

The last lemma suggests the choice q = (cid:100)k/2(cid:101)  which we shall adopt in the following  unless stated
otherwise. We will drop the subscript from Mat.
Let us recall the following standard result derived directly from Wedin perturbation Theorem [24] 
and stated in the context of the spiked model.
T + Ξ ∈ Rm×p be a matrix with (cid:107)u0(cid:107)2 =
Theorem 3 (Wedin perturbation). Let M = βu0w0

(cid:107)w0(cid:107)2 = 1. Let (cid:98)w denote the right singular vector of M. If β > 2(cid:107)Ξ(cid:107)op  then

Loss((cid:98)w  w0) ≤ 8(cid:107)Ξ(cid:107)2

op

β2

.

(18)

Theorem 4. Letting w = w(X) denote the top right singular vector of Mat(X)  we have the
following  for some universal constant C = Ck > 0  and b ≡ (1/2)((cid:100)k/2(cid:101) − 1).
If β ≥ 5 k1/2 nb then  with probability at least 1 − n−2  we have

(cid:16)

w  vec(cid:0)v0

⊗(cid:98)k/2(cid:99)(cid:1)(cid:17) ≤ C kn2b

Loss

β2

.

(19)

3.2 Asymmetric noise and recursive unfolding

A technical complication in analyzing the random matrix Matq(X) lies in the fact that its entries are
not independent  because the noise tensor Z is assumed to be symmetric. In the next theorem we
consider the case of non-symmetric noise and even k. This allows us to leverage upon known results
in random matrix theory [18  8  5] to obtain: (i) Asymptotically sharp estimates on the critical
signal-to-noise ratio; (ii) A lower bound on the loss below the critical signal-to-noise ratio. Namely 
we consider observations

(cid:101)X = βv0

⊗k +

1√
n

G .

(20)

β ≤ (1 − ε)nb ⇒

where G ∈ ⊗kRn is a standard Gaussian tensor (i.e. a tensor with i.i.d. standard normal entries).

Let w = w((cid:101)X) ∈ Rnk/2 denote the top right singular vector of Mat(X). For k ≥ 4 even  and deﬁne

b ≡ (k − 2)/4  as above. By [18  Theorem 4]  or [5  Theorem 2.3]  we have the following almost
sure limits

lim

n→∞(cid:104)w((cid:101)X)  vec(v0
(cid:12)(cid:12)(cid:104)w((cid:101)X)  vec(v0

⊗(k/2))(cid:105) = 0  

⊗(k/2))(cid:105)(cid:12)(cid:12) ≥

(cid:114) ε

(21)

(22)

β ≥ (1 + ε)nb ⇒ lim inf
n→∞

1 + ε
⊗(k/2) if and only if β is larger than nb.

In other words w((cid:101)X) is a good estimate of v0
We can use w((cid:101)X) ∈ R2b+1 to estimate v0 as follows. Construct the unfolding Mat1(w) ∈ Rn×n2b
we then let(cid:98)v to be the left principal vector of Mat1(X). We refer to this algorithm as to recursive

(slight abuse of notation) of w by letting  for i ∈ [n]  and j ∈ [n2b] 

Mat1(w)i j = wi+(j−1)n  

(23)

.

unfolding.

5

Theorem 5. Let (cid:101)X be distributed according to the non-symmetric model (20) with k ≥ 4 even 
deﬁne b ≡ (k − 2)/4. and let(cid:98)v be the estimate obtained by two-steps recursive matricization.

If β ≥ (1 + ε)nb then  almost surely

n→∞ Loss((cid:98)v  v0) = 0 .

lim

(24)

We conjecture that the weaker condition β (cid:38) n(k−2)/4 is indeed sufﬁcient also for our original
symmetric noise model  both for k even and for k odd.

4 Power Iteration

Iterating over (multi-) linear maps induced by a (tensor) matrix is a standard method for ﬁnding
leading eigenpairs  see [14] and references therein for tensor-related results. In this section we will
consider a simple power iteration  and then its possible uses in conjunction with tensor unfolding.
Finally  we will compare our analysis with results available in the literature.

4.1 Naive power iteration

The simplest iterative approach is deﬁned by the following recursion

v0 =

y
(cid:107)y(cid:107)2

 

and vt+1 =

X{vt}
(cid:107)X{vt}(cid:107)2

.

Power Iteration

The following result establishes convergence criteria for this iteration  ﬁrst for generic noise Z and
then for standard normal noise (using Lemma 2.1).
Theorem 6. Assume

(26)
Then for all t ≥ t0(k)  the power iteration estimator satisﬁes Loss(vt  v0) ≤ 2e(cid:107)Z(cid:107)op/β. If Z is a
standard normal noise tensor  then conditions (25)  (26) are satisﬁed with high probability provided
(27)

.

(cid:104)y  v0(cid:105)
(cid:107)y(cid:107)2

β

β ≥ 2 e(k − 1)(cid:107)Z(cid:107)op  
≥

(cid:20) (k − 1)(cid:107)Z(cid:107)op
(cid:112)
(cid:21)1/(k−1)

(cid:21)1/(k−1)
k3 log k(cid:0)1 + ok(1)(cid:1)  
= β−1/(k−1)(cid:0)1 + ok(1)(cid:1) .

(cid:20) kµk

β ≥ 2ek µk = 6
≥

β

(cid:104)y  v0(cid:105)
(cid:107)y(cid:107)2

(25)

(28)

In Section 6 we discuss two aspects of this result: (i) The requirement of a positive correlation
between initialization and ground truth ; (ii) Possible scenarios under which the assumptions of
Theorem 6 are satisﬁed.

5 Asymptotics via Approximate Message Passing

Approximate message passing (AMP) algorithms [7  4] proved successful
in several high-
dimensional estimation problems including compressed sensing  low rank matrix reconstruction 
and phase retrieval [9  13  20  21]. An appealing feature of this class of algorithms is that their high-
dimensional limit can be characterized exactly through a technique known as ‘state evolution.’ Here
we develop an AMP algorithm for tensor data  and its state evolution analysis focusing on the ﬁxed
β  n → ∞ limit. Proofs follows the approach of [4] and will be presented in a journal publication.
In a nutshell  our AMP for Tensor PCA can be viewed as a sophisticated version of the power
iteration method of the last section. With the notation f (x) = x/(cid:107)x(cid:107)2  we deﬁne the AMP iteration
over vectors vt ∈ Rn by v0 = y  f (v−1) = 0  and

(cid:40) vt+1 = X{f (vt)} − bt f (vt−1)  
bt = (k − 1)(cid:0)(cid:104)f (vt)  f (vt−1)(cid:105)(cid:1)k−2

.

AMP

Our main conclusion is that the behavior of AMP is qualitatively similar to the one of power itera-
tion. However  we can establish stronger results in two respects:

6

1. We can prove that  unless side information is provided about the signal v0  the AMP esti-
mates remain essentially orthogonal to v0  for any ﬁxed number of iterations. This corre-
sponds to a converse to Theorem 6.

2. Since state evolution is asymptotically exact  we can prove sharp phase transition results

with explicit characterization of their locations.

We assume that the additional information takes the form of a noisy observation y = γ v0 + z 
where z ∼ N(0  In/n). Our next results summarize the state evolution analysis.
Proposition 5.1. Let k ≥ 2 be a ﬁxed integer. Let {v0(n)}n≥1 be a sequence of unit norm vectors
v0(n) ∈ Sn−1. Let also {X(n)}n≥1 denote a sequence of tensors X(n) ∈ ⊗kRn generated follow-
ing Spiked Tensor Model. Finally  let vt denote the t-th iterate produced by AMP  and consider its
orthogonal decomposition

(29)
where vt(cid:107) is proportional to v0  and vt⊥ is perpendicular. Then vt⊥ is uniformly random  conditional
on its norm. Further  almost surely

vt = vt(cid:107) + vt⊥  

(31)
where τt is given recursively by letting τ0 = γ and  for t ≥ 0 (we refer to this as to state evolution):

lim

n→∞(cid:104)vt(cid:107)  v0(cid:105) = τt  
n→∞(cid:104)vt  v0(cid:105) = lim
n→∞(cid:107)vt⊥(cid:107)2 = 1  
(cid:18) τ 2

(cid:19)k−1

lim

τ 2
t+1 = β2

t

1 + τ 2
t

.

(30)

(32)

The following result characterizes the minimum required additional information γ to allow AMP
to escape from those undesired local optima. We will say that {vt}t converges almost surely to a
desired local optimum if 

n→∞ Loss(vt/(cid:107)vt(cid:107)2  v0) ≤ 4
Theorem 7. Consider the Tensor PCA problem with k ≥ 3 and

lim
t→∞ lim

β2 .

√

β > ωk ≡(cid:113)

ek .

(k − 1)k−1/(k − 2)k−2 ∼

Then AMP converges almost surely to a desired local optimum if and only if γ > (cid:112)1/k(β) − 1
In the special case k = 3  and β > 2  assuming γ > β(1/2 −(cid:112)1/4 − 1/β2)  AMP tends to a
desired local optimum. Numerically β > 2.69 is enough for AMP to achieve (cid:104)v0 (cid:98)v(cid:105) ≥ 0.9 if

where k(β) is the largest solution of (1 − )(k−2) = β−2 

γ > 0.45.
As a ﬁnal remark  we note that the methods of [16] can be used to show that  under the assumptions
of Theorem 7  for β > βk a sufﬁciently large constant  AMP asymptotically solves the optimization
problem Tensor PCA. Formally  we have  almost surely 

(cid:12)(cid:12)(cid:12)(cid:104)X (cid:0)vt(cid:1)⊗k(cid:105) − (cid:107)X(cid:107)op

(cid:12)(cid:12)(cid:12) = 0.

t→∞ lim
lim
n→∞

(33)

6 Numerical experiments

6.1 Comparison of different algorithms

Our empirical results are reported in the appendix. The main ﬁndings are consistent with the theory
developed above:

• Tensor power iteration (with random initialization) performs poorly with respect to other
approaches that use some form of tensor unfolding. The gap widens as the dimension n
increases.

7

Figure 1: Simultaneous PCA at β = 3. Absolute correlation of the estimated principal component

with the truth |(cid:104)(cid:98)v  v0(cid:105)|  simultaneous PCA (black) compared with matrix (green) and tensor PCA

(blue).

• All algorithms based on initial unfolding (comprising PSD-constrained PCA and recursive
unfolding) have essentially the same threshold. Above that threshold  those that process the
singular vector (either by recursive unfolding or by tensor power iteration) have superior
performances over simpler one-step algorithms.

Our heuristic arguments suggest that tensor power iteration with random initialization will work for
β (cid:38) n1/2  while unfolding only requires β (cid:38) n1/4 (our theorems guarantee this for  respectively 

β (cid:38) n and β (cid:38) n1/2). We plot the average correlation |(cid:104)(cid:98)v  v0(cid:105)| versus (respectively) β/n1/2 and

β/n1/4. The curve superposition conﬁrms that our prediction captures the correct behavior already
for n of the order of 50.

6.2 The value of side information

Our next experiment concerns a simultaneous matrix and tensor PCA task: we are given a tensor
X ∈ ⊗3Rn of Spiked Tensor Model with k = 3 and the signal to noise ratio β = 3 is ﬁxed. In
T + N where N ∈ Rn×n is a symmetric noise matrix with upper
addition  we observe M = λv0v0
diagonal elements i < j iid Ni j ∼ N(0  1/n) and the value of λ ∈ [0  2] varies. This experiment
mimics a rank-1 version of topic modeling method presented in [1] where M is a matrix representing
pairwise co-occurences and X triples.
The analysis in previous sections suggests to use the leading eigenvector of M as the initial point
of AMP algorithm for tensor PCA on X. We performed the experiments on 100 randomly gener-

ated instances with n = 50  200  500 and report in Figure 1 the mean values of |(cid:104)v0 (cid:98)v(X)(cid:105)| with
Random matrix theory predicts limn→∞(cid:104)(cid:98)v1(M )  v0(cid:105) =
(cid:17)

1 − λ−2 and apply the theory of the previous section. In particular  Proposition 5.1 implies

conﬁdence intervals.
√

1 − λ−2 [8]. Thus we can set γ =

1/2 +(cid:112)1/4 − 1/β2

1/2 −(cid:112)1/4 − 1/β2

lim

and limn→∞(cid:104)(cid:98)v(X)  v0(cid:105) = 0 otherwise Simultaneous PCA appears vastly superior to simple PCA.

Our theory captures this difference quantitatively already for n = 500.

if γ > β

n→∞(cid:104)(cid:98)v(X)  v0(cid:105) = β

(cid:16)

√

(cid:16)

(cid:17)

Acknowledgements

This work was partially supported by the NSF grant CCF-1319979 and the grants AFOSR/DARPA
FA9550-12-1-0411 and FA9550-13-1-0036.

References
[1] A. Anandkumar  R. Ge  D. Hsu  S. M. Kakade  and M. Telgarsky. Tensor decompositions for

learning latent variable models. arXiv:1210.7559  2012.

8

00.511.5200.10.20.30.40.50.60.70.80.91| <v0   v> | n = 50λ00.511.5200.10.20.30.40.50.60.70.80.91| <v0   v> | n = 200λ00.511.5200.10.20.30.40.50.60.70.80.91| <v0   v> | n −> ∞ (theory)λ00.511.5200.10.20.30.40.50.60.70.80.91| <v0   v> | n = 500λ Pow. It. (init. y)Pow. It. (random init.)y = Matrix PCAPow. It. (unfold. init.) [2] A. Aufﬁnger  G. Ben Arous  and J. Cerny. Random matrices and complexity of spin glasses.

Communications on Pure and Applied Mathematics  66(2):165–201  2013.

[3] Z. Bai and J. Silverstein. Spectral Analysis of Large Dimensional Random Matrices (2nd

edition). Springer  2010.

[4] M. Bayati and A. Montanari. The dynamics of message passing on dense graphs  with appli-

cations to compressed sensing. IEEE Trans. on Inform. Theory  57:764–785  2011.

[5] Florent Benaych-Georges and Raj Rao Nadakuditi. The singular values and vectors of low
rank perturbations of large rectangular random matrices. Journal of Multivariate Analysis 
111:120–135  2012.

[6] K. R. Davidson and S. J. Szarek. Local operator theory  random matrices and Banach spaces.
In Handbook on the Geometry of Banach spaces  volume 1  pages 317–366. Elsevier Science 
2001.

[7] D. L. Donoho  A. Maleki  and A. Montanari. Message Passing Algorithms for Compressed

Sensing. Proceedings of the National Academy of Sciences  106:18914–18919  2009.

[8] D. F´eral and S. P´ech´e. The largest eigenvalues of sample covariance matrices for a spiked

population: diagonal case. Journal of Mathematical Physics  50:073302  2009.

[9] A. K. Fletcher  S. Rangan  L. R. Varshney  and A. Bhargava. Neural reconstruction with
approximate message passing (neuramp). In Neural Information Processing Systems (NIPS) 
pages 2555–2563  2011.

[10] S. Geman. A limit theorem for the norm of random matrices. Annals of Probability  8:252–261 

1980.

[11] C. Hillar and L. H. Lim. Most tensor problems are np-hard. Journal of the ACM  6  2009.
[12] I. M Johnstone and A. Y. Lu. On consistency and sparsity for principal components analysis

in high dimensions. Journal of the American Statistical Association  104(486)  2009.

[13] U. Kamilov  S. Rangan  A. K. Fletcher  and M. Unser. Approximate message passing with
In Neural Information

consistent parameter estimation and applications to sparse learning.
Processing Systems (NIPS)  pages 2447–2455  2012.

[14] T. Kolda and J. Mayo. Shifted power method for computing tensor eigenpairs. SIAM Journal

on Matrix Analysis and Applications  32(4):1095–1124  2011.

[15] J. Liu  P. Musialski  P. Wonka  and J. Ye. Tensor completion for estimating missing values in
visual data. IEEE Transactions on Pattern Analysis and Machine Intelligence  35(1):208–220 
2013.

[16] A. Montanari and E. Richard. Non-negative principal component analysis: Message passing

algorithms and sharp asymptotics. arXiv:1406.4775  2014.

[17] C. Mu  J. Huang  B. Wright  and D. Goldfarb. Square deal: Lower bounds and improved
In International Conference in Machine Learning (ICML) 

relaxations for tensor recovery.
2013.

[18] D. Paul. Asymptotics of sample eigenstructure for a large dimensional spiked covariance

model. Statistica Sinica  17(4):1617  2007.

[19] B. Romera-Paredes and M. Pontil. A new convex relaxation for tensor completion. In Neural

Information Processing Systems (NIPS)  2013.

[20] P. Schniter and V. Cevher. Approximate message passing for bilinear models. In Proc. Work-

shop Signal Process. Adaptive Sparse Struct. Repr.(SPARS)  page 68  2011.

[21] P. Schniter and S. Rangan. Compressive phase retrieval via generalized approximate message
passing. In Communication  Control  and Computing (Allerton)  2012 50th Annual Allerton
Conference on  pages 815–822. IEEE  2012.

[22] R. Tomioka  T. Suzuki  K. Hayashi  and H. Kashima. Statistical performance of convex tensor

decomposition. In Neural Information Processing Systems (NIPS)  2011.

[23] W. C. Waterhouse. The absolute-value estimate for symmetric multilinear forms. Linear Alge-

bra and its Applications  128:97–105  1990.

[24] P. A. Wedin. Perturbation bounds in connection with singular value decomposition. BIT Nu-

merical Mathematics  12(1):99–111  1972.

9

,Emile Richard
Andrea Montanari
Mark Ho
Michael Littman
James MacGlashan
Fiery Cushman
Joseph Austerweil