2019,Dimensionality reduction: theoretical perspective on practical measures,Dimensionality reduction plays a central role in real-world applications for Machine Learning  among many fields. In particular   metric dimensionality reduction where data from a general metric is mapped into low dimensional space  is often used as a first step before applying machine learning algorithms. In almost all these applications the quality of the embedding is measured by various average case criteria. Metric dimensionality reduction has also been studied in Math and TCS  within the extremely fruitful and influential field of metric embedding. Yet  the vast majority of theoretical research has been devoted to analyzing the worst case behavior of embeddings and therefore has little relevance to practical settings. The goal of this paper is to bridge the gap between theory and practice view-points of metric dimensionality reduction  laying the foundation for a theoretical study of more practically oriented analysis. This paper can be viewed as providing a comprehensive theoretical framework addressing a line of research initiated by VL [NeuroIPS' 18] who have set the goal of analyzing different distortion measurement criteria  with the lens of Machine Learning applicability  from both theoretical and practical perspectives.
We complement their work by considering some important and vastly used average case criteria  some of which originated within the well-known Multi-Dimensional Scaling framework.  While often studied in practice  no theoretical studies have thus far attempted at providing rigorous analysis of these criteria. In this paper we provide the first analysis of these  as well as the new distortion measure developed by [VL18] designed to possess Machine Learning desired properties. Moreover  we show that all measures considered can be adapted to possess similar qualities. The main consequences of our work are nearly tight bounds on the absolute values of all distortion criteria  as well as first approximation algorithms with provable guarantees.,Dimensionality reduction: theoretical perspective on

practical measures

Yair Bartal∗

Department of Computer Science
Hebrew University of Jerusalem

Jerusalem  Israel

yair@cs.huji.ac.il

Nova Fandina

fandina@cs.huji.ac.il

Department of Computer Science
Hebrew University of Jerusalem

Jerusalem  Israel

Ofer Neiman

Department of Computer Science
Ben Gurion University of the Negev

Beer-Sheva  Israel

neimano@cs.bgu.ac.il

Abstract

Dimensionality reduction plays a central role in real world applications for Machine
Learning  among many ﬁelds. In particular  metric dimensionality reduction  where
data from a general metric is mapped into low dimensional space  is often used
as a ﬁrst step before applying machine learning algorithms. In almost all these
applications the quality of the embedding is measured by various average case
criteria. Metric dimensionality reduction has also been studied in Math and TCS 
within the extremely fruitful and inﬂuential ﬁeld of metric embedding. Yet  the
vast majority of theoretical research has been devoted to analyzing the worst case
behavior of embeddings  and therefore has little relevance to practical settings. The
goal of this paper is to bridge the gap between theory and practice view-points of
metric dimensionality reduction  laying the foundation for a theoretical study of
more practically oriented analysis.
This paper can be viewed as providing a comprehensive theoretical framework for
analyzing different distortion measurement criteria  with the lens of practical appli-
cability  and in particular for Machine Learning. The need for this line of research
was recently raised by Chennuru Vankadara and von Luxburg in (13)[NeurIPS’ 18] 
who emphasized the importance of pursuing it from both theoretical and practical
perspectives.
We consider some important and vastly used average case criteria  some of which
originated within the well-known Multi-Dimensional Scaling framework. While
often studied in practice  no theoretical studies have thus far attempted at providing
rigorous analysis of these criteria. In this paper we provide the ﬁrst analysis of these 
as well as the new distortion measure developed in (13) designed to posses Machine
Learning desired properties. Moreover  we show that all measures considered can
be adapted to posses similar qualities. The main consequences of our work are
nearly tight bounds on the absolute values of all distortion criteria  as well as ﬁrst
approximation algorithms with provable guarantees.
All our theoretical results are backed by empirical experiments.

∗Author names are ordered alphabetically.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

1

Introduction

Metric Embedding plays an important role in a vast range of application areas such as machine
learning  computer vision  computational biology  networking  statistics  data mining  neuroscience
and mathematical psychology  to name a few. Perhaps the most signiﬁcant is the application of metric
dimensionality reduction for large data sets  where the data is represented by points in a metric space.
It is desirable to efﬁciently embed the data into low dimensional space which would allow compact
representation  efﬁcient use of resources  efﬁcient access and interpretation  and enable operations to
be carried out signiﬁcantly faster.
In machine learning  this task is often used as a preliminary step before applying various machine
learning algorithms  and sometimes refereed to as unsupervised metric dimensionality reduction.
Some studies of dimensionality reduction within ML include (10; 11; 13; 42; 49). Moreover  there are
numerous practical studies of metric embedding and dimensionality reduction appearing in a plethora
of papers ranging in a wide scope of research areas including work on Internet coordinate systems 
feature extraction  similarity search  visual recognition  and computational biology applications; the
papers (25; 41; 24; 5; 21; 17; 45; 48; 51; 52; 44; 32; 12; 11; 47) are just a small sample.
In nearly all practical applications of metric embedding and dimensionality reduction methods  the
fundamental criterion for measuring the quality of the embedding is its average performance over all
pairs  where the measure of quality per pair is often the distortion  the square distortion and similar
related notions. Such experimental results often indicate that the quality of metric embeddings and
dimensionality reduction techniques behave very well in practice.
In contrast  the classic theory of metric embedding has mostly failed to address this phenomenon.
Developed over the past few decades by both mathematicians and theoretical computer scientists
(see (26; 34; 27) for surveys)  it has been extremely fruitful in analyzing the worst case distortion of
embeddings. However  worst case analysis results often exhibit extremely high lower bounds. Indeed 
in most cases  the worst case bounds are growing  in terms of both distortion and dimension  as a
function of the size of the space. Such bounds are often irrelevant in practical terms.
These concerns were recently raised in the context of Machine Learning in (13) (NeurIPS’18) 
stressing the desire for embeddings into constant dimension with constant distortion. The authors of
(13) state the necessity for a systematic study of different average distortion measures. Their main
motivation is to examine the relevance of these measures for machine learning applications. Here  the
ﬁrst step is made to tackle this challenge.
The goal of this paper is to bridge between theory and practice outlook on metric embedding and
dimensionality reduction.
In particular  providing the ﬁrst comprehensive rigorous analysis of
the most basic practically oriented average case quality measurement criteria  using methods and
techniques developed within the classic theory of metric embedding  thereby providing new insights
for both theory and practice.
We focus on some of the most basic and commonly used average distortion measurement criteria:
Moments analysis: moments of distortion and Relative Error. The most basic average case
performance criterion is the average distortion. More generally  one could study all q-moments of
the distortion for every 1 ≤ q ≤ ∞. This notion was ﬁrst studied in (1). For a non-contractive
embedding f  whose distortion for a pair of points u  v is denoted distf (u  v):
Deﬁnition 1 ((cid:96)q-distortion). Let (X  dX ) and (Y  dY ) be any metric spaces  and f : X → Y be an

(cid:1) and q ≥ 1  the (cid:96)q-distortion of f with respect to Π is

embedding. For any distribution Π over(cid:0)X

2

deﬁned by: (cid:96)q-dist (Π)(f ) = (EΠ [(distf (u  v))q])

1

q   (cid:96)∞-dist (Π)(f ) = supΠ(u v)(cid:54)=0 {distf (u  v)}.

The most natural case is where Π is the uniform distribution (and will be omitted from the notation). In
order for this deﬁnition to extend to handle embeddings in their full generality and address important
applications such as dimensionality reduction  it turns out that one should remove the assumption
that the embedding is non-contractive.
We therefore naturally extend the above deﬁnition to deal with arbitrary embeddings by let-
ting distf (u  v) = max{expansf (u  v)  contrf (u  v)}  where expansf (u  v) = dY (f (u) f (v))
 

dX (u v)

2

dY (f (u) f (v)). In the full version (in supplementary materials) we provide justiﬁca-

contrf (u  v) = dX (u v)
tion of the necessity of this deﬁnition. Observe that this deﬁnition is not scale invariant2.
In many practical cases  where we may expect a near isometry for most pairs  the moments of
distortion may not be sensitive enough and more delicate measures of quality  which examine directly
the pairwise additive error  may be desired. The relative error measure (REM)  commonly used
in network applications (45; 44; 15) is the most natural choice. It turns out that this measure can
be viewed as the moment of distortion about 1. This gives rise to the following generalization of
Deﬁnition 1:
Deﬁnition 2 ((cid:96)q-distortion about c  REM). For c ≥ 0  the (cid:96)q-distortion of f about c is given by:

(cid:96)q-dist (Π)

(c) (f ) = (EΠ [|distf (u  v) − c|q])

1

q   REM (Π)

q

(f ) = (cid:96)q-dist (Π)

(1) (f ).

Additive distortion measures: Stress and Energy. Multi Dimensional Scaling (see (16; 8)) is a
well-established methodology aiming at embedding a metric representing the relations between
objects into (usually Euclidean) low-dimensional space  to allow feature extraction often used for
indexing  clustering  nearest neighbor searching and visualization in many application areas  including
machine learning (42). Several average additive error criteria for the embedding’s quality have been
suggested in the context of MDS over the years. Perhaps the most popular is the stress measure going
back to (30). For duv = dX (u  v) and ˆduv = dY (f (u)  f (v))  for normalized nonnegative weights
Π(u  v) (or distribution) we deﬁne the following natural generalizations  which include the classic
Kruskal stress Stress∗
2(f ) and normalized stress Stress2(f ) measures  as well as other common

(cid:18) EΠ[| ˆduv−duv|q]

(cid:19)1/q

EΠ[(duv)q]

  and

variants in the literature (e.g. (23; 46; 20; 9; 50; 11)): Stress(Π)

(f ) =

q

(cid:17)1/q

(cid:16) EΠ[| ˆduv−duv|q]
(cid:17)q(cid:105)(cid:17)1/q

(cid:16)

(cid:104)(cid:16)| ˆduv−duv|

duv

q

(f ) =

Stress∗(Π)
. Another popular and widely used additive error measure is
energy and its special case  Sammon cost (see e.g. (43; 7; 14; 36; 37; 12)). We deﬁne the following
generalizations  which include some common variants (e.g. (41; 45; 44; 33)): Energy(Π)
(f ) =

EΠ[( ˆduv)q]

q

(cid:16)

(cid:104)(cid:16) | ˆduv−duv|q

(cid:17)q(cid:105)(cid:17)1/q

EΠ

  and REM (Π)

(f ) =

q

EΠ

min{ ˆduv duv}

.
(f ) ≤ (cid:96)q-dist (Π)(f ).
(f ) ≤ REM (Π)
(f ) are equivalent via a simple transfor-

It immediately follows from the deﬁnitions that: Energy(Π)
and Energy(Π(cid:48))
Also it’s not hard to observe that Stress(Π)
mation of weights.
ML motivated measure: σ-Distortion. Recently published paper (13) studies various existing
and commonly used quality criteria in terms of their relevance in machine learning applications.
Particularly  the authors suggest a new measure  σ- distortion  which is claimed to possess all the
necessary properties for machine learning applications. We consider a generalized version of σ-

q

q

q

q

(cid:1) 

distortion3. Let (cid:96)r-expans(f ) = ((cid:0)n
(cid:1)−1(cid:80)
(cid:104)(cid:12)(cid:12)(cid:12) expansf (u v)

(Π)(f ) =

(cid:16)

EΠ

2

u(cid:54)=v(expansf (u  v))r)1/r. For a distribution Π over(cid:0)X
(cid:12)(cid:12)(cid:12)q(cid:105)(cid:17)1/q

2

(cid:96)r-expans (f )

let Φσ q r
(for q = 2  r = 1 this is the square root of
the measure deﬁned by (13)). We show that the tools we develop in this paper can be applied to
σ-distortion to obtain theoretical bounds on its value.
We further show (Section7)  generalizing (13)  that all other average distortion measures considered
here can be easily adapted to satisfy similar ML motivated properties.
A basic contribution of our paper is showing deeper tight relations between these different objective
functions  and further developing properties and tools for analyzing embeddings for these measures.
While these measures have been extensively studied from a practical point of view  and many
heuristics are known in the literature  almost nothing is known in terms of rigorous analysis and
absolute bounds. Moreover  many real-world misconceptions exist about what dimension may be
necessary for good embeddings. In this paper we present the ﬁrst theoretical analysis of all these

− 1

2We note that if one desires scale invariability it may always be achieved by deﬁning the scale-invariant
measure to be the minimization of the measure over all possible scaling of the embedding. For simplicity we
focus on the non-scalable version

3It is easy to verify that the general version satisﬁes all the properties considered in (13).

3

measures providing absolute bounds that shed light on these questions. We exhibit approximation
algorithms for optimizing these measures  and further applications.
In this paper we focus only on analyzing objective measures that attempt to preserve metric structure.
As a result  some popular objective measures used in applied settings are beyond the scope of this
paper  this includes the widely used t-SNE heuristic (which aims at reﬂecting the cluster structure of
the data  and generally does not preserve metric structure)  and various heuristics with local structure
objectives. When validating our theoretical ﬁndings experimentally (Section 6)  we chose to compare
our results with the most common in practice heuristics PCA/classical-MDS and Isomap amongst the
various methods that appear in the literature.
Moment analysis of dimensionality reduction. The main theoretical question our paper studies is:
Problem 1 ((k  q)-Dimension Reduction). Given a dimension bound k and 1 ≤ q ≤ ∞  what is
the least α(k  q) such that every ﬁnite subset of Euclidean space embeds into k dimensions with
Measureq ≤ α(k  q) ?
This question can be phrased for each Measureq of practical importance. A stronger demand would
be to require a single embedding to simultaneously achieve best possible bounds for all values of q.
We answer Problem 1 by providing (almost tight for most of the values of k and q) upper and
lower bounds on α(k  q). In particular we prove that the Johnson-Lindenstrauss (JL) dimensionality
reduction achieves bounds in terms of q and k that dramatically outperform a widely used in practice
PCA algorithm. Moreover  our experiments show that the same holds for the Isomap and classical
MDS methods.
The bounds we obtain provide several interesting conclusions regarding the expected behavior of
dimensionality reduction methods. As expected  the bound for the JL method is improving as k
grows  conﬁrming the intuition expressed in (13). Yet  countering their intuition  the bound does
not increase as a function of the original dimension d. A phase transition  exhibited in our bounds 
provides guidance on how to choose the target dimension k.
Another consequence arises by combining our result with the embedding of (1)  by composing it with
the JL: we obtain an embedding of general spaces into a constant dimensional Euclidean space with
constant distortion  for all discussed measures (presented in the full version). Here  the dimension is
constant even if the original space is not doubling  improving on the result obtained in (13).
Approximation algorithms. The bounds achieved for the Euclidean (k  q)-dimension reduction are
then applied to provide the ﬁrst approximation algorithms for embedding general metric spaces into
low dimensional Euclidean space  for all the various distortion criteria. This is based on composing
convex programming with the JL-transform. It should be stressed that such a composition may not
necessarily work in general  however  we are able to show that this yields efﬁcient approximation
algorithms for all the criteria considered in this paper.
The results on the JL transform yield bounds on distance oracles. In the full version  we provide
additional applications  including metric hyper sketching  a generalization of standard sketching.
Empirical Experiments. We validate our theoretical ﬁndings experimentally on various randomly
generated Euclidean and non-Euclidean metric spaces  in Section 6. In particular  as predicted by our
lower bounds  the phase transition is clearly seen in the JL  PCA and Isomap embeddings for all the
measurement criteria. Moreover  in our simulations the JL based approximation algorithm (as well as
the JL itself  when applied on Euclidean metrics) has shown dramatically better performance than the
PCA and Isomap heuristics for all distortion measures  indicating that the JL-based approximation
algorithm is a preferable choice when the preservation of metric properties is desirable.
Related work. For Euclidean embedding  it was shown in (35) that using SDP one can obtain
arbitrarily good approximation of the distortion. However  such a result is impossible when restricting
the target dimension to k  as in (39) it was shown that unless P=NP  the approximation factor must be
nΩ(1/k). Of all the measures studied in this paper  only Stressq was previously studied. In (10)  it
was shown that computing an embedding into R1 with optimal Stressq is NP-hard  for any given q.
The only approximation algorithms known for this problem are the following: a 2-approximation to
Stress∞ for embedding into R1 (22); an O(log1/q n)-approximation to Stressq for embedding into
R1 (19); an O(1)-approximation to Stress∞ for embedding into (cid:96)2
All proofs are omitted from this version and appear in the full version (in supplemental material).

1 (6).

4

2 On the limitations of classical MDS

Practitioners have developed various heuristics to cope with dimensionality reduction (see (49) for
a comprehensive overview). Most of the suggested methods are based on iterative improvement of
various objectives. All these strategies do not provide theoretical guarantees on convergence to the
global minimum and most of them even do not necessarily converge. Furthermore  classical MDS
or PCA  one of the widely used heuristics  is usually referred to as the method that computes the
optimal solution for minimizing Stress2. We show that this is in fact false: PCA can produce an
embedding with Stressq value being far from optimum  even for the space that can be efﬁciently
embedded into a line.4 Consider the following subset of Rd. For any α < 1/2  for all i ∈ [1  d]  for
any q ≥ 1  let si = 1/(αi)q. Let Xi ⊂ (cid:96)d
2 be the (multi) set of size 2si that contains si copies of
the vector αi · ei  denoted by X +
i   and si copies of the antipodal vector −αi · ei  denoted by X−
i  
where ei is the standard basis vector of Rd. Deﬁne X as the union of all Xi. In the full version of the
paper  we show that X can be embedded into a line with Stressq/Energyq(f ) = O(α/d1/q)  for
any q ≥ 1. Yet  for the PCA algorithm applied on X  into k ≤ β · d dimensions (β < 1)  it holds that
Stressq/Energyq(F ) = Ω(1)  and (cid:96)q-dist /REMq(F ) = ∞.
Moreover  our empirical experiments show that the PCA and Isomap methods have signiﬁcantly
worse performance than the JL on a variety of randomly generated families of metric spaces.

3 Euclidean dimension reduction: moment analysis of the JL transform

From a theoretical perspective  dimensionality reduction is known to be possible in Euclidean space
via the Johnson-Lindenstrauss Lemma (29)  a cornerstone of Banach space analysis and metric
embedding theory  playing a central role in a plethora of applications. The lemma states that every n
point subset of Euclidean space can be embedded in O(−2 log n) dimensions with worst case 1 + 
a ﬁxed dimension k  the worst case distortion becomes as bad as O(n2/k√
distortion. The dimension bound is shown to be tight in (31) (improving upon (4)). When applied in
log n). Moreover  in (40)
a lower bound of nΩ(1/k) on the worst case distortion of any embedding in k dimensions was proven.
However  as explained above  in many practical instances it is desirable to replace the demand for
worst case with average case guarantees. It should be noted yet that the JL transform does have good
properties  even when applied in k dimensions. The JL lemma in fact implies that in dimension k
for every pair there is some constant probability (≈ exp(−2k)) that a 1 +  distortion is achieved.
While in itself an appealing property  it should be stressed that standard tail bounds arguments cannot
imply that the average (or higher moments) distortion is bounded. Indeed  we show that for certain
specialized implementations of the JL embedding  such as those of (2) (e.g.  using Rademacher
entries matrix)  (3) (fast JL)  and (18) (sparse JL)  the (cid:96)q-dist and REM q are unbounded.
Observation 1. Let k ≥ 1  and d > k. Let Ed = {ei}1≤i≤d ⊆ (cid:96)d
vectors. Assume that a linear map f : (cid:96)d
for all i  j  P [i  j] ∈ U for some ﬁnite set U ⊂ R. If |U| < d 1
(cid:96)q-dist(f )  REMq(f ) = ∞.
The proof follows by volume argument: for matrix P the set f (Ed) = {P ei}1≤i≤d is exactly the
set of columns of P . Since the entries of P belong to U  there can be at most |U|k < d different
columns in the set f (Ed). Therefore  there is at least one pair of vectors in Ed that will be mapped
into the same image by f. This implies the observation as (cid:96)q-distortion and REM q measures depend
on the inverse of the embedded distance.
Yet  focusing on the Gaussian entries implementation by (28) we show that it behaves dramatically
better. Let X ⊂ (cid:96)d
2 be an n-point set  and k ≥ 1 be an integer. The JL transform of dimension k 
f : X → (cid:96)k
2 is deﬁned by generating a random matrix T of size k × d  with i.i.d. standard normal
entries  and setting f (x) = 1√
Theorem 1. Let X ⊂ (cid:96)d
transform f : X → (cid:96)k

2 be an n-point set  and let k ≥ 1. Given any distribution Π over(cid:0)X

2 be the set of standard basis
2 is given by a transformation matrix Pk×d  such that
k then for the set Ed  for all q ≥ 1 

2 is s.t. with probability at least 1/2  (cid:96)q-dist (Π)(f ) is bounded by:

(cid:1)  the JL

2

T x  for all x ∈ X.

k

2 → (cid:96)k

4We note that PCA is proven to minimize(cid:80)

but not over embeddings (not even linear maps).

u(cid:54)=v∈X (d2

uv − ˆd2

uv) over all projections into k dimensions (38) 

5

√

(cid:16) 1√

1 ≤ q <
1 + O

k

(cid:17)

k

√

(cid:16) q

k ≤ q ≤ k
1 + O

4
k−q

(cid:17)

k

(cid:17)O(1/q)
(cid:16) k
4 ≤ q (cid:47) k
k−q

q = k
(log n)O(1/k)

k (cid:47) q ≤ ∞
nO( 1

k − 1
q )

The bounds are asymptotically tight for most values of k and q when the embedding is required to

maintain all bounds simultaneously. For ﬁxed q tightness holds for most values of q ≥ √

k.

Note that for large q our theorem shows that a phase transition emerges around q = k. The necessity
of this phenomenon is implied by nearly tight lower bounds given in Section 4.1.
Additive distortion and σ-distortion measures analysis. The following theorem provides tight
upper bounds for all the additive distortion measures and for σ-distortion  for 1 ≤ q ≤ k − 1. This
follows from analyzing the REM (via similar approach to the raw moments analysis):
Theorem 2. Given a ﬁnite set X ⊂ (cid:96)d

(cid:1)  with constant probability  for all 1 ≤ r ≤ q ≤ k − 1:

dimension k. For any distribution Π over(cid:0)X

2 and an integer k ≥ 2  let f : X → (cid:96)k

2 be the JL transform of

2

REM (Π)

q

(f )  Energy(Π)

q

(f )  Φσ q r

(Π)(f )  Stress(Π)

q

(f )  Stress∗(Π)

q

(f ) = O

(cid:16)(cid:112)q/k

(cid:17)

.

The more challenging part of the analysis is ﬁguring out how good are the JL performance bounds.
Therefore our main goal is the task of establishing lower bounds for Problem 1.

4 Partially tight lower bounds: q < k

In the full version we show that JL is essentially optimal when simultaneous guarantees are required.
If that requirement is removed  it is still the case for most of the ranges of q. Providing lower bounds
lower bound of 1 + Ω(q/(k − q)) for the range 1 ≤ q ≤ k − 1. For q ≤ √
for each range requires a different technique. One of the most interesting cases  is the proof of the
k  this turns out to be a
consequence of the tightness for the additive distortion measures and σ-distortion  shown to be tight
for q ≥ 2. The proof is based on a delicate application of the technique of (4). We show that the
analysis of the JL transform for the additive measures and σ-distortion  provides tight bounds for all
values of 2 ≤ q ≤ k. Due to tight relations between the additive measures  the lower bounds for all
measures follow from Energy measure. Let En denote an n-point equilateral metric space.
  for any embedding f : En → (cid:96)k
Claim 3. For all k ≥ 2  k ≥ q ≥ 2  and n ≥ 4

(cid:17)q/2

9 · k

(cid:16)

2 it holds

q

that Energyq(f ) = Ω((cid:112) q
(cid:96)q-dist(f ) = 1 + Ω(cid:0) q

k

k ).

Claim 4. For all k ≥ 1  1 ≤ q < 2  and n ≥ 18k  for all f : En → (cid:96)k
A more involved argument shows that Claim 3 implies
Corollary 1. For any k ≥ 1 and any n ≥ 18k  for any embedding f : En → (cid:96)k

(cid:1)  for all 1 ≤ q ≤ √

k.

2  Energyq(f ) = Ω(cid:0) 1

k1/q

(cid:1).

2 it holds that

(cid:16) q

k log k(cid:1).

(cid:17)
  for all q = Ω(cid:0)√

2 it holds that (cid:96)q-dist(F ) ≥ 1 + Ω

Based on (31)  we also prove
Theorem 5. For all k ≥ 16  for all N large enough  there is a metric space Z ⊆ (cid:96)2 on N points 
such that for any F : Z → (cid:96)k

k−q
4.1 Phase transition: moment analysis lower bounds for q ≥ k
An important consequence of our analysis is that the q-moments of the distortion (including REM q) 
exhibit an impressive phase transition phenomenon occurring around q = k. This follows from lower
bounds for q ≥ k. The case q = k (and ≈ k) is of special interest where we obtain a tight bound:
Theorem 6. Any embedding f : En → (cid:96)k
log n)1/k/k1/4)  for any k ≥ 1.
Hence  for any q  the theorem tells that only k ≥ 1.01q may be suitable for dimensionality reduction.
This new consequence may serve an important guide for practical considerations  that seems to be
missing prior to our work. We also prove the following claim for large values of q:
Claim 7. For any embedding f : En → (cid:96)k
2  for all k ≥ 1  for all q > k  (cid:96)q-dist(f ) =
Ω(max{n(

2 has (cid:96)k-dist(f ) = Ω((

2(cid:100)k/2(cid:101)− 2

2q }).

2k − 1

q )  n

√

1

1

6

5 Approximate optimal embedding of general metrics

Perhaps the most basic goal in dimensionality reduction theory and essentially  the main problem
of MDS  is: Given an arbitrary metric space compute an embedding into k dimensional Euclidean
space which approximates the best possible embedding  in terms of minimizing a particular distortion
measure objective. Except for some very special cases no such approximation algorithms were
known prior to this work. Applying our moment analysis bounds for JL we are able to obtain the
ﬁrst general approximation guarantees to all the discussed measures. The bounds are obtained via
convex programming combined with the JL-transform. While the basic idea is quite simple  it is not
obvious that it can actually go through. The main obstacle is that all q-moment measures are not
associative. In fact  this is not generally the case that combining two embeddings results in a good
ﬁnal embedding. However  as we show  this is indeed true speciﬁcally for JL-type embeddings.

q = {(cid:96)q-dist (Π)  REM (Π)
Let OBJ (Π)
(cid:111)
q
of the objective measures. For Obj(Π)

(cid:110)

Obj(Π)

q

2

  Energy(Π)

(cid:110)
q } denote the set
q ∈ OBJ (Π)
Obj(Π)
  and
. Note that OP T (n) ≤ OP T . The ﬁrst step of the approximation

q
  denote OP T (n) = inf f :X→(cid:96)n

σ q 2  Stress(Π)

  Stress∗(Π)

  Φ(Π)

(cid:111)

(f )

q

q

q

2

q

q

q

q

q

q

(h)

q = Stress∗(Π)

2 with Stress∗(Π)

  without constraining the target dimension.

(cid:54)= Stress∗(Π)
2 such that Obj(Π)

OP T = inf h:X→(cid:96)k
algorithm is to compute OP T (n) for a given Obj(Π)
Theorem 8. Let (X  dX ) be an n-point metric space and Π be any distribution. Then for any
q ≥ 2 and for Obj(Π)
there exists a polynomial time algorithm that computes an
embedding f : X → (cid:96)n
(f ) approximates OP T (n) to within any level of precision.
For Obj(Π)
there exists a polynomial time algorithm that computes an embedding
f : X → (cid:96)n
The proof is based on formulating the appropriate convex optimization program  which can be solved
in polynomial time by interior-point methods.The exception is Stress∗
q which is inherently non-
convex. We show that Stress∗
q can be reduced to the case of Stressq  with an additional constant
factor loss  and that optimizing for Φσ q 2 can be reduced to the case of Energyq. The second step in
the algorithm is applying the JL to reduce the dimension to the desired number of dimensions k.

(f ) = O(cid:0)OP T (n)(cid:1).

Theorem 9. For any ﬁnite metric (X  dX )  any distribution Π over (cid:0)X
O(OP T ) + O((cid:112)q/k)  for Obj(Π)

2 ≤ q ≤ k−1  there is a randomized polynomial time algorithm that ﬁnds an embedding F : X → (cid:96)k
2 
such that with high probability: (cid:96)q-dist (Π)(F ) = (1 + O( 1√
k−q ))OP T ; and Obj(Π)
(F ) =
q }.
  Stress∗(Π)
  Energy(Π)

(cid:1)  for any k ≥ 3 and

+ q
k
σ q 2  Stress(Π)
  Φ(Π)

q ∈ {REM (Π)

2

q

q

q

q

6 Empirical experiments

In this section we provide experiments to demonstrate that the theoretical results are exhibited in
practical settings. We also compare in the experiments the bounds of the theoretical algorithms
(JL and the approximation algorithm based on it) to some of the most common heuristics. In all
the experiments  we use Normal distribution (with random variance) for sampling Euclidean input
spaces.5 Tests were made for a large range of parameters  averaging over at least 10 independent
tests. The results are consistent for all settings and measures.
√
We ﬁrst recall the main theoretical results to be veriﬁed. In Theorem 1 and Theorem 2 we showed
that for q < k the (cid:96)q-distortion is bounded by 1 + O(1/
k) + O(q/k)  and all the rest measures are

bounded by O((cid:112)q/k). Particularly  the bounds are independent of the size n and dimension d of the

input data set. In addition  our lower bounds in Section 4.1 show that for (cid:96)q-distortion and REMq
measures a phase transition must occur at q ∼ k for any dimensionality reduction method  where the
bounds dramatically increase from being bounded by a constant to grow with n as poly(n) for q < k.
Finally  in Section 5 we exhibited an approximation algorithm for all distortion measures.
The graphs in Fig.1 and Fig.2a describe the following setting: A random Euclidean space X of a
ﬁxed size n and dimension d = n = 800 was embedded into k ∈ [4  30] dimensions with q = 5 
5We note that (13) used similar settings with Normal/Gamma distributions. Most of our experimental results

hold also for the Gamma distribution.

7

by the JL/PCA/Isomap methods. We stress that we run many more experiments for a wide range
of parameter values of n ∈ [100  3000]  k ∈ [2  100]  q ∈ [1  10]  and obtained essentially identical
qualitative behavior. In Fig. 1a  the (cid:96)q-distortion as a function of k of the JL embedding is shown for
q = 8  10  12. The phase transitions are seen at around k ∼ q as predicted. In Fig. 1b the bounds
and the phase transitions of the PCA and Isomap methods are presented for the same setting  as
predicted. In Fig. 1c  (cid:96)q-distortion bounds are shown for increasing values of k > q. Note that the
(cid:96)q-distortion of the JL is a small constant close to 1  as predicted  compared to values signiﬁcantly
> 2 for the compared heuristics. Overall  Fig. 1 clearly shows the superiority of JL to the other
methods for all the range of values of k. The same conclusions as above hold for σ-distortion as well 

(a) Phase transition: JL.

(b) Phase transition: PCA  Isomap.

(c) Comparing (cid:96)q-dists for k > q.

Figure 1: Validating (cid:96)q-distortion behavior.

as shown in Fig. 2a. In the experiment shown in Fig. 2b  we tested the behavior of the σ-distortion
as a function of d-the dimension of the input data set  similarly to that of (13)(Fig. 2)  and tests are
shown for embedding dimension k = 20 and q = 2. According to our theorems  the σ-distortion of
the JL transform is bounded above by a constant independent of d  for q < k. Our experiment shows
that the σ-distortion is growing as d increases for both PCA/Isomap  whereas it is a constant for JL.
Moreover  JL obtains signiﬁcantly smaller value of σ-distortion.

(a) σ-distortion.

(b) σ-distortion as a function of d.

Figure 2: Validating σ-dist. behavior.

Figure 3: Non-Euclidean in-
put metric: (cid:96)q-distortion be-
havior.

In the last experiment  Fig.3  we tested the quality of our approximation algorithm on non-Euclidean
input spaces versus the classical MDS and Isomap methods (adapted for non-Euclidean input spaces).
The construction of the space is as follows: ﬁrst  a sampled Euclidean space X  of size and dimension
n = d = 100  is generated as above; second  the interpoint distances of X are distorted with a
noise factor 1 +   with  ∼ N (0  δ)  for δ < 1. We ensure that the resulting space is a valid
non-Euclidean metric. We then embed the ﬁnal space into k ∈ [10  30] dimensions with q = 5. Since
the non-Euclidean space is 1 +  far from being Euclidean  we expect a similar behavior to that shown
in Fig. 1c. The result clearly demonstrates the superiority of the JL-based approximation algorithm.

7 On relevance of distortion measures for ML

In (13) the authors developed a set of properties a distortion measure has to satisfy in order to be useful
for machine learning. Here we show that these properties can be generalized and that appropriate
modiﬁcations of all the measurement criteria discussed in this paper satisfy all of them.
For an embedding f : X → Y   let ρf (u  v) be an error function of a pair u (cid:54)= v ∈ X  which is a func-
tion of the embedded distance and original distance between u and v. Let ρ(f ) = (ρf (u  v))u(cid:54)=v∈X
denote the vector of ρf (u  v) for all pairs u (cid:54)= v ∈ X. Let M (Π)
: ρ(f ) → R+ be a measure function 
distf (u  v) and ρf (u  v) := distf (u  v) − 1  respectively; for Energyq  and Stressq measures 

(cid:1). For instance  for (cid:96)q-distortion measure and REMq  ρf (u  v) :=

q

for any distribution Π over(cid:0)X

2

8

q

(ρ(f )) := (EΠ[(cid:107)ρ(f )(cid:107)q

ρf (u  v) := |expansf (u  v) − 1|; for Φσ q r  ρf (u  v) := |expansf (u  v)/ (cid:96)r-expans(f ) − 1|. All
the measures are then deﬁned by M (Π)
q])1/q. In what follows we will omit
Π from the notation. We propose the generalizations of the ML motivated properties deﬁned in (13):
Scalability. Although a measurement criterion may not necessarily be scalable  it can be naturally
modiﬁed to a scalable version as follows. For every Mq  deﬁne ˆMq(ρ(f )) = minα>0 Mq(ρ(α · f )).
Note that the upper and lower bounds that hold for Mq also hold for its scalable version ˆMq.
Monotonicity. We generalize this property as follows. Let f  g : X → Y be any embeddings.
For a given measure Mq  let ˆf and ˆg be embeddings minimizing Mq(ρ(α · f )) and Mq(ρ(α · g)) 
respectively (over all scaling factors α > 0). If ˆf and ˆg are such that for every pair u (cid:54)= v ∈ X it
holds that ρ ˆf (u  v) ≥ ρˆg(u  v)  then the measure ˆMq is monotone iff Mq(ρ( ˆf )) ≥ Mq(ρ(ˆg)).
Robustness to outliers in data/in distances. The measure ˆMq is said to be robust to outliers if for any
embedding fn of an n-point space  any modiﬁcation ˜fn where a constant number of changes occurs
in either points or distances  it holds that limn→∞ Mq(ρ(fn)) = limn→∞ Mq(ρ( ˜f n)).
Incorporation of the probability distribution. Let h : X → Y be an embedding and let u (cid:54)= v ∈ X
and x (cid:54)= y ∈ X  such that Π(u  v) > Π(x  y) and ρh(u  v) = ρh(x  y). Assume that f : X → Y is
identical to h  except over (u  v)  and assume that g is identical to h  except over (x  y)  and assume
that ρf (u  v) = ρg(x  y). Now let ˆf and ˆh be deﬁned as above and assume ρ ˆf (u  v) ≥ ρˆh(u  v). Then 
the measure ˆM (Π)
(ρ(ˆg)).
Robustness to noise was not formally deﬁned in (13). Assuming the model of noise that affects the
error ρ by at most a factor of 1 +  (alternatively an additive error of ) for each pair  the requirement
is that the measure ˆMq will be changed by at most factor of 1 + O() (or additive O()).
It is easy to see that all distortion criteria (adapted to be scalable as in the ﬁrst property) discussed in
this paper obey all the above properties  implying their relevance to the ML applications.

is said to incorporate the probability distribution Π if M (Π)

q

(ρ( ˆf )) > M (Π)

q

q

8 Discussion

This work provides a new framework for theoretical analysis of embeddings in terms of performance
measures that are of practical relevance  initiating a theoretical study of a wide range of average case
quality measurement criteria  and providing the ﬁrst rigorous analysis of these criteria.
We use this framework to analyze the new distortion measure developed in (13) designed to posses
machine learning desired properties and show that all considered distortion measures can be adapted
to posses similar qualities.
We show nearly tight bounds on the absolute values of all distortion criteria  essentially showing that
the JL transform is near optimal for dimensionality reduction for most parameter regimes. When
considering other methods  the JL bound can serve as guidance and it would make sense to treat a
method useful only when it beats the JL bound. A phase transition exhibited in our bounds provides a
direction on how to choose the target dimension k  i.e. k should be greater than q by a factor > 1.
This means that the amount of outlier pairs is diminishing as k grows.
A major contribution of our paper is providing the ﬁrst approximation algorithms for embedding any
ﬁnite metric (possibly non-Euclidean) into k-dimensional Euclidean space with provable approxima-
tion guarantees. Since these approximation algorithms achieve near optimal distortion bounds they are
expected to beat most common heuristics in terms of the relevant distortion measures. Evidence exists
that there is correlation between lower distortion measures and quality of machine learning algorithms
applied on the resulting space  such as in (13)  where such correlation is experimentally shown
between σ-distortion and error bounds in classiﬁcation. This evidence suggests that the improvement
in distortion bounds should be reﬂected in better bounds for machine learning applications.
Our experiments show that the conclusions above hold in practical settings as well.

9

Acknowledgments

This work is supported by ISF grant #1817/17 and BSF grant #2015813.

References
[1] Ittai Abraham  Yair Bartal  and Ofer Neiman. Advances in metric embedding theory. Ad-
vances in Mathematics  228(6):3026 – 3126  2011. ISSN 0001-8708. doi: 10.1016/j.aim.
2011.08.003. URL http://www.sciencedirect.com/science/article/pii/
S000187081100288X.

[2] Dimitris Achlioptas. Database-friendly random projections: Johnson-lindenstrauss with binary
coins. Journal of Computer and System Sciences  66(4):671 – 687  2003. ISSN 0022-0000. doi:
http://dx.doi.org/10.1016/S0022-0000(03)00025-4. URL http://www.sciencedirect.
com/science/article/pii/S0022000003000254. Special Issue on {PODS} 2001.

[3] Nir Ailon and Edo Liberty. An almost optimal unrestricted fast johnson-lindenstrauss transform.
ACM Trans. Algorithms  9(3):21:1–21:12  June 2013. ISSN 1549-6325. doi: 10.1145/2483699.
2483701. URL http://doi.acm.org/10.1145/2483699.2483701.

[4] Noga Alon. Perturbed identity matrices have high rank: Proof and applications. Combinatorics 
Probability & Computing  18(1-2):3–15  2009. doi: 10.1017/S0963548307008917. URL
http://dx.doi.org/10.1017/S0963548307008917.

[5] Vassilis Athitsos and Stan Sclaroff. Database indexing methods for 3d hand pose estimation. In

Gesture Workshop  pages 288–299  2003.

[6] Mihai Badoiu. Approximation algorithm for embedding metrics into a two-dimensional space.
In Proceedings of the fourteenth annual ACM-SIAM symposium on Discrete algorithms  pages
434–443. Society for Industrial and Applied Mathematics  2003.

[7] Wojciech Basalaj. Proximity visualisation of abstract data. Technical Report UCAM-CL-TR-

509  University of Cambridge  Computer Laboratory  January 2001.

[8] I. Borg and P. J. F. Groenen. Modern Multidimensional Scaling: Theory and Applications

(Springer Series in Statistics). Springer  Berlin  2nd edition  2005.

[9] Alexander M. Bronstein  Michael M. Bronstein  and Ron Kimmel. Generalized multidimen-
sional scaling: A framework for isometry-invariant partial surface matching. Proceedings of the
National Academy of Sciences  103(5):1168–1172  2006.

[10] Lawrence Cayton and Sanjoy Dasgupta. Robust euclidean embedding. In Proceedings of
the 23rd International Conference on Machine Learning  ICML ’06  pages 169–176  New
York  NY  USA  2006. ACM. ISBN 1-59593-383-2. doi: 10.1145/1143844.1143866. URL
http://doi.acm.org/10.1145/1143844.1143866.

[11] A. Censi and D. Scaramuzza. Calibration by correlation using metric embedding from
IEEE Transactions on Pattern Analysis and Machine Intelligence 
ISSN 0162-8828. doi: 10.1109/TPAMI.2013.34. URL

nonmetric similarities.
35(10):2357–2370  Oct. 2013.
doi.ieeecomputersociety.org/10.1109/TPAMI.2013.34.

[12] Samidh Chatterjee  Bradley Neff  and Piyush Kumar. Instant approximate 1-center on road
networks via embeddings. In Proceedings of the 19th ACM SIGSPATIAL International Con-
ference on Advances in Geographic Information Systems  GIS ’11  pages 369–372  New York 
NY  USA  2011. ACM. ISBN 978-1-4503-1031-4. doi: 10.1145/2093973.2094025. URL
http://doi.acm.org/10.1145/2093973.2094025.

[13] Leena Chennuru Vankadara and Ulrike von Luxburg. Measures of distortion for ma-
chine learning.
In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi 
and R. Garnett  editors  Advances in Neural Information Processing Systems 31  pages
4891–4900. Curran Associates  Inc.  2018. URL http://papers.nips.cc/paper/
7737-measures-of-distortion-for-machine-learning.pdf.

10

[14] M. Costa  M. Castro  R. Rowstron  and P. Key. Pic: practical internet coordinates for dis-
tance estimation. In 24th International Conference on Distributed Computing Systems  2004.
Proceedings.  pages 178–187  2004. doi: 10.1109/ICDCS.2004.1281582.

[15] Russ Cox  Frank Dabek  Frans Kaashoek  Jinyang Li  and Robert Morris. Practical  distributed
network coordinates. SIGCOMM Comput. Commun. Rev.  34(1):113–118  January 2004.
ISSN 0146-4833. doi: 10.1145/972374.972394. URL http://doi.acm.org/10.1145/
972374.972394.

[16] T. F. Cox and M. A. A. Cox. Multidimensional Scaling (Monographs on Statistics and Applied

Probability). Chapman and Hall/CRC  2nd edition  2000.

[17] Frank Dabek  Russ Cox  M. Frans Kaashoek  and Robert Morris. Vivaldi: a decentralized
In Proceedings of the ACM SIGCOMM 2004 Conference on
network coordinate system.
Applications  Technologies  Architectures  and Protocols for Computer Communication  August
30 - September 3  2004  Portland  Oregon  USA  pages 15–26  2004. doi: 10.1145/1015467.
1015471. URL http://doi.acm.org/10.1145/1015467.1015471.

[18] Anirban Dasgupta  Ravi Kumar  and Tamás Sarlós. A sparse Johnson- Lindenstrauss transform.
In Proceedings of the forty-second ACM symposium on Theory of computing  pages 341–350.
ACM  2010.

[19] Kedar Dhamdhere. Approximating additive distortion of embeddings into line metrics. In
Approximation  Randomization  and Combinatorial Optimization. Algorithms and Techniques 
pages 96–104. Springer  2004.

[20] Patrick J. F. Groenen  Rudolf Mathar  and Willem J. Heiser. The majorization approach to
multidimensional scaling for minkowski distances. Journal of Classiﬁcation  12(1):3–19  1995.

[21] Eran Halperin  Jeremy Buhler  Richard M. Karp  Robert Krauthgamer  and B. Westover. Detect-
ing protein sequence conservation via metric embeddings. In ISMB (Supplement of Bioinfor-
matics)  pages 122–129  2003.

[22] Johan Håstad  Lars Ivansson  and Jens Lagergren. Fitting points on the real line and
its application to rh mapping.
ISSN 0196-
6774. doi: 10.1016/S0196-6774(03)00083-X. URL http://dx.doi.org/10.1016/
S0196-6774(03)00083-X.

J. Algorithms  49(1):42–62  October 2003.

[23] W. J Heiser. Multidimensional scaling with least absolute residuals. In In H. H. Bock (Ed.)

Classiﬁcation and related methods  pages 455–462. Amsterdam: NorthHolland  1988a.

[24] Gísli R. Hjaltason and Hanan Samet. Properties of embedding methods for similarity search-
ing in metric spaces. IEEE Trans. Pattern Anal. Mach. Intell.  25(5):530–549  2003. doi:
10.1109/TPAMI.2003.1195989. URL http://dx.doi.org/10.1109/TPAMI.2003.
1195989.

[25] Gabriela Hristescu and Martin Farach-Colton. Cofe: A scalable method for feature extrac-
tion from complex objects. In Proceedings of the Second International Conference on Data
Warehousing and Knowledge Discovery  DaWaK 2000  pages 358–371  London  UK  2000.
Springer-Verlag. ISBN 3-540-67980-4. URL http://portal.acm.org/citation.
cfm?id=646109.756709.

[26] P. Indyk. Algorithmic applications of low-distortion geometric embeddings. In Proceedings
42nd IEEE Symposium on Foundations of Computer Science  pages 10–33  Oct 2001. doi:
10.1109/SFCS.2001.959878.

[27] Piotr Indyk and Jiri Matoušek. Low-distortion embeddings of ﬁnite metric spaces. URL

citeseer.ist.psu.edu/672933.html.

[28] Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the
curse of dimensionality. In Proceedings of the Thirtieth Annual ACM Symposium on Theory
of Computing  STOC ’98  pages 604–613  New York  NY  USA  1998. ACM. ISBN 0-89791-
962-9. doi: 10.1145/276698.276876. URL http://doi.acm.org/10.1145/276698.
276876.

11

[29] William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert
space. In Conference in modern analysis and probability (New Haven  Conn.  1982)  pages
189–206. American Mathematical Society  Providence  RI  1984.

[30] J. B. Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis.

Psychometrika  29(1):1–27  1964.

[31] Kasper Green Larsen and Jelani Nelson. Optimality of the johnson-lindenstrauss lemma. arXiv

preprint arXiv:1609.02094  2016.

[32] Sanghwan Lee  Zhi-Li Zhang  Sambit Sahu  Debanjan Saha  and Mukund Srinivasan. Fun-
In NET-
damental effects of clustering on the euclidean embedding of internet hosts.
WORKING 2007. Ad Hoc and Sensor Networks  Wireless Networks  Next Generation In-
ternet  6th International IFIP-TC6 Networking Conference  Atlanta  GA  USA  May 14-18 
2007  Proceedings  pages 890–901  2007. doi: 10.1007/978-3-540-72606-7_76. URL
http://dx.doi.org/10.1007/978-3-540-72606-7_76.

[33] Sanghwan Lee  Zhi-Li Zhang  Sambit Sahu  and Debanjan Saha. On suitability of euclidean
embedding for host-based network coordinate systems. IEEE/ACM Trans. Netw.  18(1):27–
40  February 2010.
ISSN 1063-6692. doi: 10.1109/TNET.2009.2023322. URL http:
//dx.doi.org/10.1109/TNET.2009.2023322.

[34] N. Linial. Finite metric spaces- combinatorics  geometry and algorithms. In Proceedings of the

ICM  2002.

[35] Nathan Linial  Eran London  and Yuri Rabinovich. The geometry of graphs and some of
its algorithmic applications. Combinatorica  15(2):215–245  1995. ISSN 1439-6912. doi:
10.1007/BF01200757. URL http://dx.doi.org/10.1007/BF01200757.

[36] Eng Keong Lua  Timothy Grifﬁn  Marcelo Pias  Han Zheng  and Jon Crowcroft. On the accuracy
of embeddings for internet coordinate systems. In Proceedings of the 5th ACM SIGCOMM Con-
ference on Internet Measurement  IMC ’05  pages 11–11  Berkeley  CA  USA  2005. USENIX
Association. URL http://dl.acm.org/citation.cfm?id=1251086.1251097.

[37] C. Lumezanu and N. Spring. Measurement manipulation and space selection in network
coordinates. In 2008 The 28th International Conference on Distributed Computing Systems 
pages 361–368  June 2008. doi: 10.1109/ICDCS.2008.27.

[38] Kantilal Varichand Mardia  John T. Kent  and John M. Bibby. Multivariate analy-
sis. Probability and mathematical statistics. Acad. Press  London [u.a.]  1979.
ISBN
0124712509. URL http://gso.gbv.de/DB=2.1/CMD?ACT=SRCHA&SRT=YOP&
IKT=1016&TRM=ppn+02434995X&sourceid=fbw_bibsonomy.

[39] Jiri Matousek and Anastasios Sidiropoulos. Inapproximability for metric embeddings into
rd. In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer
Science  FOCS ’08  pages 405–413  Washington  DC  USA  2008. IEEE Computer Society.
ISBN 978-0-7695-3436-7. doi: 10.1109/FOCS.2008.21. URL http://dx.doi.org/10.
1109/FOCS.2008.21.

[40] Jiˇrí Matoušek. Bi-Lipschitz embeddings into low-dimensional Euclidean spaces. Commentat.

Math. Univ. Carol.  31(3):589–600  1990. ISSN 0010-2628.

[41] T. S. Eugene Ng and Hui Zhang. Predicting internet network distance with coordinates-based
approaches. In Proceedings IEEE INFOCOM 2002  The 21st Annual Joint Conference of the
IEEE Computer and Communications Societies  New York  USA  June 23-27  2002  2002. URL
http://www.ieee-infocom.org/2002/papers/785.pdf.

[42] Michael Quist  Golan Yona  and Bin Yu. Distributional scaling: An algorithm for structure-
preserving embedding of metric and nonmetric spaces. Journal of Machine Learning Research 
pages 399–420  2004.

[43] J. W. Sammon. A nonlinear mapping for data structure analysis.

IEEE Transactions on
Computers  C-18(5):401–409  May 1969. ISSN 0018-9340. doi: 10.1109/T-C.1969.222678.

12

[44] Puneet Sharma  Zhichen Xu  Sujata Banerjee  and Sung-Ju Lee. Estimating network proximity
and latency. Computer Communication Review  36(3):39–50  2006. doi: 10.1145/1140086.
1140092. URL http://doi.acm.org/10.1145/1140086.1140092.

[45] Yuval Shavitt and Tomer Tankel. Big-bang simulation for embedding network distances in
euclidean space. IEEE/ACM Trans. Netw.  12(6):993–1006  December 2004. ISSN 1063-6692.
doi: 10.1109/TNET.2004.838597. URL http://dx.doi.org/10.1109/TNET.2004.
838597.

[46] Ian Spence and Stephan Lewandowsky. Robust multidimensional scaling. Psychometrika  54

(3):501–513  1989.

[47] Sahaana Suri and Peter Bailis. DROP: dimensionality reduction optimization for time series.

CoRR  abs/1708.00183  2017. URL http://arxiv.org/abs/1708.00183.

[48] Liying Tang and Mark Crovella. Geometric exploration of the landmark selection problem. In
Passive and Active Network Measurement  5th International Workshop  PAM 2004  Antibes 
pages 63–72  2004.

[49] Laurens Van Der Maaten  Eric Postma  and Jaap Van den Herik. Dimensionality reduction: a

comparative review. J Mach Learn Res  10:66–71  2009.

[50] J. Fernando Vera  Willem J. Heiser  and Alex Murillo. Global optimization in any minkowski
metric: A permutation-translation simulated annealing algorithm for multidimensional scaling.
J. Classif.  24(2):277–301  September 2007. ISSN 0176-4268.

[51] Jason Tsong-Li Wang  Xiong Wang  Dennis E. Shasha  and Kaizhong Zhang. Metricmap:
an embedding technique for processing distance-based queries in metric spaces. IEEE Trans.
Systems  Man  and Cybernetics  Part B  35(5):973–987  2005. doi: 10.1109/TSMCB.2005.
848489. URL http://dx.doi.org/10.1109/TSMCB.2005.848489.

[52] Rongmei Zhang  Y. Charlie Hu  Xiaojun Lin  and Sonia Fahmy. A hierarchical approach to
internet distance prediction. In 26th IEEE International Conference on Distributed Computing
Systems (ICDCS 2006)  4-7 July 2006  Lisboa  Portugal  page 73  2006. doi: 10.1109/ICDCS.
2006.7. URL http://dx.doi.org/10.1109/ICDCS.2006.7.

13

,Yair Bartal
Nova Fandina
Ofer Neiman