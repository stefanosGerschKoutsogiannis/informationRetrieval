2017,Online to Offline Conversions  Universality and Adaptive Minibatch Sizes,We present an approach towards convex optimization that relies on a novel scheme which  converts  adaptive online algorithms into offline methods. In the offline optimization setting  our derived methods are shown to obtain favourable adaptive  guarantees which depend on the harmonic sum of the queried gradients.  We further  show that our methods implicitly adapt to the objective's structure: in the smooth case fast  convergence rates are ensured without any prior knowledge of the smoothness parameter  while  still maintaining guarantees in the non-smooth setting. Our approach has a natural extension to the stochastic setting  resulting in a lazy version of SGD  (stochastic GD)  where minibathces are chosen adaptively depending on the magnitude of  the gradients. Thus providing a principled approach towards choosing minibatch sizes.,Online to Ofﬂine Conversions  Universality and

Adaptive Minibatch Sizes

Kﬁr Y. Levy

Department of Computer Science  ETH Zürich.

yehuda.levy@inf.ethz.ch

Abstract

We present an approach towards convex optimization that relies on a novel scheme
which converts adaptive online algorithms into ofﬂine methods. In the ofﬂine
optimization setting  our derived methods are shown to obtain favourable adaptive
guarantees which depend on the harmonic sum of the queried gradients. We
further show that our methods implicitly adapt to the objective’s structure: in the
smooth case fast convergence rates are ensured without any prior knowledge of
the smoothness parameter  while still maintaining guarantees in the non-smooth
setting. Our approach has a natural extension to the stochastic setting  resulting in
a lazy version of SGD (stochastic GD)  where minibathces are chosen adaptively
depending on the magnitude of the gradients. Thus providing a principled approach
towards choosing minibatch sizes.

1

Introduction

Over the past years data adaptiveness has proven to be crucial to the success of learning algorithms.
The objective function underlying “big data" applications often demonstrates intricate structure:
the scale and smoothness are often unknown and may change substantially in between different
regions/directions  [1]. Learning methods that acclimatize to these changes may exhibit superior
performance compared to non adaptive procedures.
State-of-the-art ﬁrst order methods like AdaGrad  [1]  and Adam  [2]  adapt the learning rate on the
ﬂy according to the feedback (i.e. gradients) received during the optimization process. AdaGrad and
Adam are guaranteed to work well in the online convex optimization setting  where loss functions
may be chosen adversarially and change between rounds. Nevertheless  this setting is harder than the
stochastic/ofﬂine settings  which may better depict practical applications. Interestingly  even in the
ofﬂine convex optimization setting it could be shown that in several scenarios very simple schemes
may substantially outperform the output of AdaGrad/Adam. An example of such a simple scheme is
choosing the point with the smallest gradient norm among all rounds. In the ﬁrst part of this work we
address this issue and design adaptive methods for the ofﬂine convex optimization setting. At heart of
our derivations is a novel scheme which converts adaptive online algorithms into ofﬂine methods with
favourable guarantees1. Our shceme is inspired by standard online to batch conversions  [3].
A seemingly different issue is choosing the minibatch size  b  in the stochastic setting. Stochastic
optimization algorithms that can access a noisy gradient oracle may choose to invoke the oracle
b times in every query point  subsequently employing an averaged gradient estimate. Theory for
stochastic convex optimization suggests to use a minibatch of b = 1  and predicts a degradation of pb
factor upon using larger minibatch sizes2. Nevertheless in practice larger minibatch sizes are usually
found to be effective. In the second part of this work we design stochastic optimization methods in
1For concreteness we concentrate in this work on converting AdaGrad  [1]. Note that our conversion scheme
2A degradation by a pb factor in the general case and by a b factor in the strongly-convex case.

applies more widely to other adaptive online methods.

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

which minibatch sizes are chosen adaptively without any theoretical degradation. These are natural
extensions of the ofﬂine methods presented in the ﬁrst part.
Our contributions:
Ofﬂine setting: We present two (families of) algorithms AdaNGD (Alg. 2) and SC-AdaNGD
(Alg. 3) for the convex/strongly-convex settings which achieve favourable adaptive guarantees
(Thms. 2.1  2.2  3.1  3.2 ). The latter theorems also establish their universality  i.e.  their ability to
implicitly take advantage of the objective’s smoothness and attain rates as fast as GD would have
achieved if the smoothness parameter was known. In contrast to other universal approaches such as
line-search-GD  [4]  and universal gradient [5]  we do so without any line search procedure.
Concretely  without the knowledge of the smoothness parameter our algorithm ensures an O(1/pT )
rate in general convex case and an O(1/T ) rate if the objective is also smooth (Thms. 2.1  2.2). In
the strongly-convex case our algorithm ensures an O(1/T ) rate in general and an O(exp(T )) rate
if the objective is also smooth (Thm. 3.2 )  where  is the condition number.
Stochastic setting: We present Lazy-SGD (Algorithm 4) which is an extension of our ofﬂine
algorithms. Lazy-SGD employs larger minibatch sizes in points with smaller gradients  which
selectively reduces the variance in the “more important" query points. Lazy-SGD guarantees are
comparable with SGD in the convex/strongly-convex settings (Thms. 4.2  4.3).
On the technical side  our online to ofﬂine conversion schemes employ three simultaneous mech-
anisms: an adaptive online algorithm used in conjunction with gradient normalization and with
a respective importance weighting. To the best of our knowledge the combination of the above
techniques is novel  and we believe it might also ﬁnd use in other scenarios.
This paper is organized as follows.
In Sections 2 3  we present our methods for the ofﬂine
convex/strongly-convex settings. Section 4 describes our methods for the stochastic setting  and
Section 5 concludes. Extensions and a preliminary experimental study appear in the Appendix.

1.1 Related Work
The authors of [1] simultaneously to [6]  were the ﬁrst to suggest AdaGrad—an adaptive gradient
based method  and prove its efﬁciency in tackling online convex problems. AdaGrad was subsequently
adjusted to the deep-learning setting to yield the RMSprop  [7]  and Adadelta  [8]  heuristics. Adam 
[2]  is a popular adaptive algorithm which is often the method of choice in deep-learning applications.
It combines ideas from AdaGrad together with momentum machinery  [9].
An optimization procedure is called universal if it implicitly adapts to the objective’s smoothness. In
[5]  universal gradient methods are devised for the general convex setting. Concretely  without the
knowledge of the smoothness parameter  these methods attain the standard O(1/T )  an accelerated
O(1/T 2) rates for smooth objectives  and an O(1/pT ) rate in the non-smooth case. The core
technique in this work is a line search procedure which estimates the smoothness parameter in
every iteration. For strongly-convex and smooth objectives  line search techniques  [4]  ensure
linear convergence rate  without the knowledge of the smoothness parameter. However  line search
is not “fully universal"  in the sense that it holds no guarantees in the non-smooth case. For the
latter setting we present a method which is “fully universal" (Thm. 3.2)  nevertheless it requires the
strong-convexity parameter.
The usefulness of employing normalized gradients was demonstrated in several non-convex scenarios.
In the context of quasi-convex optimization  [10]  and [11]  established convergence guarantees for
the ofﬂine/stochastic settings. More recently  it was shown in [12]  that normalized gradient descent
is more appropriate than GD for saddle-evasion scenarios.
In the context of stochastic optimization  the effect of minibatch size was extensively investigated
throughout the past years  [13  14  15  16  17  18]. Yet  all of these studies: (i) assume a smooth
expected loss  (ii) discuss ﬁxed minibatch sizes. Conversely  our work discusses adaptive minibatch
sizes  and applies to both smooth/non-smooth expected losses.

1.2 Preliminaries
Notation: k·k denotes the `2 norm  G denotes a bound on the norm of the objective’s gradients  and
[T ] := {1  . . .   T}. For a set K2 Rd its diameter is deﬁned as D = supx y2K kx  yk. Next we

2

Algorithm 1 Adaptive Gradient Descent (AdaGrad)

Input: #Iterations T   x1 2 Rd  set K
Set: Q0 = 0
for t = 1 . . . T do

Calculate: gt = rft(xt)
Update:
Set: ⌘t = D/p2Qt
Update: xt+1 =⇧ K (xt  ⌘tgt)

end for

Qt = Qt1 + kgtk2

deﬁne H-strongly-convex/-smooth functions 

f (y)  f (x) + rf (x)>(y  x) +
f (y)  f (x) + rf (x)>(y  x) +

H
2 kx  yk2;

2kx  yk2;

8x  y 2K (H-strong-convexity)
8x  y 2K (-smoothness)

1.2.1 AdaGrad

The methods presented in this paper lean on AdaGrad (Alg. 1)  an online optimization method which
employs an adaptive learning rate. The following theorem states AdaGrad’s guarantees  [1] 
Theorem 1.1. Let K be a convex set with diameter D. Let {ft}T
convex loss functions. Then Algorithm 1 guarantees the following regret;
TXt=1

t=1 be an arbitrary sequence of

ft(xt)  min
x2K

ft(x)  vuut2D2

TXt=1

TXt=1

kgtk2 .

2 Adaptive Normalized Gradient Descent (AdaNGD)

In this section we discuss the convex optimization setting and introduce our AdaNGDk algorithm 
which depends on a parameter k 2 R. We ﬁrst derive a general convergence rate which holds for a
general k. Subsequently  we elaborate on the k = 1  2 cases which exhibit universality as well as
adaptive guarantees that may be substantially better compared to standard methods.
Our method AdaNGDk is depicted in Alg. 2. This algorithm can be thought of as an online to ofﬂine
conversion scheme which utilizes AdaGrad (Alg. 1) as a black box and eventually outputs a weighted
sum of the online queries. Indeed  for a ﬁxed k 2 R  it is not hard to notice that AdaNGDk is
equivalent to invoking AdaGrad with the following loss sequence { ˜ft(x) := g>t x/kgtkk}T
t=1. And
eventually weighting each query point inversely proportional to the k’th power norm of its gradient.
The reason behind this scheme is that in ofﬂine optimization it makes sense to dramatically reduce
the learning rate upon uncountering a point with a very small gradient. For k  1  this is achieved by
invoking AdaGrad with gradients normalized by their k’th power norm. Since we discuss constrained
optimization  we use the projection operator deﬁned as  ⇧K(y) := minx2K kx yk . The next lemma
states the guarantee of AdaNGD for a general k:
Lemma 2.1. Let k 2 R  K be a convex set with diameter D  and f be a convex function; Also let ¯xT
be the output of AdaNGDk (Algorithm 2)  then the following holds:

f (¯xT )  min
x2K

f (x)  q2D2PT
PT

t=1 1/kgtk2(k1)
t=1 1/kgtkk

Proof sketch. Notice that the AdaNGDk algorithm is equivalent to applying AdaGrad to the following
loss sequence: { ˜ft(x) := g>t x/kgtkk}T
t=1. Thus  applying Theorem 1.1  and using the deﬁnition of
¯xT together with Jensen’s inequality the lemma follows.

3

Algorithm 2 Adaptive Normalized Gradient Descent (AdaNGDk)

Input: #Iterations T   x1 2 Rd  set K   parameter k
Set: Q0 = 0
for t = 1 . . . T  1 do

Calculate: gt = rf (xt)  ˆgt = gt/kgtkk
Update:
Set ⌘t = D/p2Qt
Update: xt+1 =⇧ K (xt  ⌘tˆgt)
Return: ¯xT =PT

1/kgtkk
⌧ =1 1/kg⌧kk xt
PT

end for

t=1

Qt = Qt1 + 1/kgtk2(k1)

For k = 0  Algorithm 2 becomes AdaGrad (Alg. 1). Next we focus on the cases where k = 1  2 
showing improved adaptive rates and universality compared to GD/AdaGrad. These improved rates
are attained thanks to the adaptivity of the learning rate: when query points with small gradients are
encountered  AdaNGDk (with k  1) reduces the learning rate  thus focusing on the region around
these points. The hindsight weighting further emphasizes points with smaller gradients.

2.1 AdaNGD1
Here we show that AdaNGD1 enjoys a rate of O(1/pT ) in the non-smooth convex setting  and a
fast rate of O(1/T ) in the smooth setting. We emphasize that the same algorithm enjoys these rates
simultaneously  without any prior knowledge of the smoothness or of the gradient norms.
From Algorithm 2 it can be noted that for k = 1 the learning rate becomes independent of the
gradients  i.e. ⌘t = D/p2t  the update is made according to the direction of the gradients  and the
weighting is inversely proportional to the norm of the gradients. The following Theorem establishes
the guarantees of AdaNGD1 
Theorem 2.1. Let k = 1  K be a convex set with diameter D  and f be a convex function; Also let
¯xT be the outputs of AdaNGD1 (Alg. 2)  then the following holds:

Proof sketch. The data dependent bound is a direct corollary of Lemma 2.1. The general case bound

t=1 1/kgtk  ⌦(T 3/2)  which concludes the proof.

holds by using kgtk  G. The bound for the smooth case is proven by showingPT
This translates to a lower boundPT
The data dependent bound in Theorem 2.1 may be substantially better compared to the bound of
the GD/AdaGrad. As an example  assume that half of the gradients encountered during the run
of the algorithm are of O(1) norms  and the other gradient norms decay proportionally to O(1/t).
In this case the guarantee of GD/AdaGrad is O(1/pT )  whereas AdaNGD1 guarantees a bound
that behaves like O(1/T 3/2). Note that the above example presumes that all algorithms encounter
the same gradient magnitudes  which might be untrue. Nevertheless in the smooth case AdaNGD1
provably beneﬁts due to its adaptivity.

t=1 kgtk  O(pT ).

2.2 AdaNGD2
Here we show that AdaNGD2 enjoys comparable guarantees to AdaNGD1 in the general/smooth
case. Similarly to AdaNGD1 the same algorithm enjoys these rates simultaneously  without any
prior knowledge of the smoothness or of the gradient norms. The following Theorem establishes the
guarantees of AdaNGD2 

4

Moreover  if f is also -smooth and the global minimum x⇤ = arg minx2Rn f (x) belongs to K  then:

f (¯xT )  min
x2K

f (x) 

f (¯xT )  min
x2K

f (x) 

p2D2T
t=1 1/kgtk 
PT
DpT
t=1 1/kgtk 
PT

p2GD
pT

.

4D 2

.

T

Algorithm 3 Strongly-Convex AdaNGD (SC-AdaNGDk)

Input: #Iterations T   x1 2 Rd  set K  strong-convexity H  parameter k
Set: Q0 = 0
for t = 1 . . . T  1 do

Calculate: gt = rf (xt)  ˆgt = gt/kgtkk
Update:

Qt = Qt1 + 1/kgtkk

end for

Set ⌘t = 1/HQt
Update: xt+1 =⇧ K (xt  ⌘tˆgt)
Return: ¯xT =PT
Theorem 2.2. Let k = 2  K be a convex set with diameter D  and f be a convex function; Also let
¯xT be the outputs of AdaNGD2 (Alg. 2)  then the following holds:

1/kgtkk
⌧ =1 1/kg⌧kk xt
PT

t=1

Moreover  if f is also -smooth and the global minimum x⇤ = arg minx2Rn f (x) belongs to K  then:

f (¯xT )  min
x2K

f (x) 

f (¯xT )  min
x2K

f (x) 

p2GD
pT

4D 2

T

.

.

p2D2
qPT
t=1 1/kgtk2 
p2D2
qPT
t=1 1/kgtk2 
T PT

1
t=1 1/at

1

It is interesting to note that AdaNGD2 will have always performed better than AdaGrad  had both
algorithms encountered the same gradient norms. This is due to the well known inequality between
t=1 ⇢ R+   which directly
arithmetic and harmonic means  [19]  1

t=1 at 

8{at}T

 

implies 

1

t=1 1/kgtk2  1

pPT

T PT

t=1 kgtk2 .

TqPT

3 Adaptive NGD for Strongly Convex Functions

Here we discuss the ofﬂine optimization setting of strongly convex objectives. We introduce our
SC-AdaNGDk algorithm  and present convergence rates for general k 2 R. Subsequently  we
elaborate on the k = 1  2 cases which exhibit universality as well as adaptive guarantees that may be
substantially better compared to standard methods.
Our SC-AdaNGDk algorithm is depicted in Algorithm 3. Similarly to its non strongly-convex
counterpart  SC-AdaNGDk can be thought of as an online to ofﬂine conversion scheme which utilizes
an online algorithm which we denote SC-AdaGrad (we elaborate on the latter in the appendix). The
next Lemma states its guarantees 
Lemma 3.1. Let k 2 R  and K be a convex set. Let f be an H-strongly-convex function; Also let ¯xT
be the outputs of SC-AdaNGDk (Alg. 3)  then the following holds:

f (¯xT )  min
x2K

f (x) 

1
t=1 kgtkk

TXt=1

2HPT

kgtk2(k1)
Pt
⌧ =1 kg⌧kk

.

Proof sketch. In the appendix we present and analyze SC-AdaGrad. This is an online ﬁrst order algo-
⌧ =1 H⌧  
where H⌧ is the strong-convexity parameter of the loss function at time ⌧. Then we show that
SC-AdaNGDk is equivalent to applying SC-AdaGrad to the following loss sequence:

rithm for strongly-convex functions in which the learning rate decays according to ⌘t = 1/Pt

⇢ ˜ft(x) =

1

kgtkk g>t x +

H

2kgtkk kx  xtk2T

t=1

.

The lemma follows by combining the regret bound of SC-AdaGrad together with the deﬁnition of ¯xT
and with Jensen’s inequality.

5

For k = 0  SC-AdaNGD becomes the standard GD algorithm which uses learning rate of ⌘t = 1/Ht.
Next we focus on the cases where k = 1  2.

3.1 SC-AdaNGD1

Here we show that SC-AdaNGD1 enjoys a rate of ˜O(1/T ) for strongly-convex objectives  and a
faster rate of ˜O(1/T 2) assuming that the objective is also smooth. We emphasize that the same
algorithm enjoys these rates simultaneously  without any prior knowledge of the smoothness or of the
gradient norms. The following theorem establishes the guarantees of SC-AdaNGD1 
Theorem 3.1. Let k = 1  and K be a convex set. Let f be a G-Lipschitz and H-strongly-convex
function; Also let ¯xT be the outputs of SC-AdaNGD1 (Alg. 3)  then the following holds:

f (¯xT )  min
x2K

f (x) 

G⇣1 + log⇣PT
2HPT

t=1

t=1

1
kgtk

G

kgtk⌘⌘

G2(1 + log T )

2HT

.



Moreover  if f is also -smooth and the global minimum x⇤ = arg minx2Rn f (x) belongs to K  then 

f (¯xT )  min
x2K

f (x) 

(/H)G2 (1 + log T )2

HT 2

.

3.2 SC-AdaNGD2

Here we show that SC-AdaNGD2 enjoys the standard ˜O(1/T ) rate for strongly-convex objectives 
and a linear rate assuming that the objective is also smooth. We emphasize that the same algorithm
enjoys these rates simultaneously  without any prior knowledge of the smoothness or of the gradient
norms. In the case where k = 2 the guarantee of SC-AdaNGD is as follows 
Theorem 3.2. Let k = 2  K be a convex set  and f be a G-Lipschitz and H-strongly-convex function;
Also let ¯xT be the outputs of SC-AdaNGD2 (Alg. 3)  then the following holds:

f (¯xT )  min
x2K

f (x) 

1 + log(G2PT
2HPT

t=1 kgtk2)

t=1 kgtk2

G2(1 + log T )

2HT

.



Moreover  if f is also -smooth and the global minimum x⇤ = arg minx2Rn f (x) belongs to K  then 

f (¯xT )  min
x2K

f (x) 

3G2
2H

e H

 T✓1 +

H


T◆ .

Intuition: For strongly-convex objectives the appropriate GD algorithm utilizes two very extreme
learning rates of ⌘t / 1/t vs. ⌘t = 1/ for the general/smooth settings respectively. A possible
explanation to the universality of SCAdaNGD2 is that it implicitly interpolate between these rates.
kgtk2
Indeed the update rule of our algorithm can be written as follows  xt+1 = xt  1
⌧ =1 kg⌧k2 gt .
Pt
Thus  ignoring the hindsight weighting  SCAdaNGD2 is equivalent to GD with an adaptive learning
rate ˜⌘t := kgtk2/HPt
⌧ =1 kg⌧k2. Now  when all gradient norms are of the same magnitude  then
˜⌘t / 1/t  which boils down to the standard GD for strongly-convex objectives. Conversely  assume
that the gradients are exponentially decaying  i.e.  that kgtk / qt for some q < 1. In this case ˜⌘t is
approximately constant. We believe that the latter applies for strongly-convex & smooth case.

H

4 Adaptive NGD for Stochastic Optimization

Here we show that using data-dependent minibatch sizes  we can adapt our (SC-)AdaNGD2 algo-
rithms (Algs. 2  3 with k = 2) to the stochastic setting  and achieve the well know convergence rates
for the convex/strongly-convex settings. Next we introduce the stochastic optimization setting  and
then we present and discuss our Lazy SGD algorithm.
Setup: We consider the problem of minimizing a convex/strongly-convex function f : K 7! R 
where K2 Rd is a convex set. We assume that optimization lasts for T rounds; on each round

6

Algorithm 4 Lazy Stochastic Gradient Descent (LazySGD)

Input: #Oracle Queries T   x1 2 Rd  set K  ⌘0  p
Set: t = 0  s = 0
while t  T do

Algorithm 5 Adaptive Estimate (AE)

ni

i=1

i=1 ni = T )

end while

T xi . (Note thatPs

Update: s = s + 1
Set G = GradOracle(xs)  i.e.  G generates i.i.d. noisy samples of rf (xs)
Get: (˜gs  ns) = AE(G  T  t) % Adaptive Minibatch
Update: t = t + ns
Calculate: ˆgs = ns˜gs
Set: ⌘s = ⌘0/tp
Update: xs+1 =⇧ K (xs  ⌘sˆgs)
Return: ¯xT =Ps
Input: random vectors generator G  sample budget Tmax  sample factor m0
Set: i = 0  N = 0  ˜g0 = 0
while N < Tmax do
Take ⌧i = min{2i  Tmax  N} samples from G
Set N N + ⌧i
Update: ˜gN Average of N samples received so far from G
If k˜gNk > 3m0/pN then return (˜gN   N )
Update i i + 1
end while
Return: (˜gN   N )

t = 1  . . .   T   we may query a point xt 2K   and receive a feedback. After the last round  we choose
¯xT 2K   and our performance measure is the expected excess loss  deﬁned as 

E[f (¯xT )]  min
x2K

f (x) .

Here we assume that our feedback is a ﬁrst order noisy oracle G : K 7! Rd such that upon
querying G with a point xt 2K   we receive a bounded and unbiased gradient estimate  G(xt) 
such E[G(xt)|xt] = rf (xt); kG(xt)k  G. We also assume that the that the internal coin tosses
(randomizations) of the oracle are independent. It is well known that variants of Stochastic Gradient
Descent (SGD) are ensured to output an estimate ¯xT such that the excess loss is bounded by
O(1/pT )/O(1/T ) for the setups of convex/strongly-convex stochastic optimization  [20]  [21].
Notation: In this section we make a clear distinction between the number of queries to the gradient
oracle  denoted henceforth by T ; and between the number of iterations in the algorithm  denoted
henceforth by S. We care about the dependence of the excess loss in T .

4.1 Lazy Stochastic Gradient Descent

Data Dependent Minibatch sizes: The Lazy SGD (Alg. 4) algorithm that we present in this section 
uses a minibatch size that changes in between query points. Given a query point xs  Lazy SGD
invokes the noisy gradient oracle ˜O(1/kgsk2) times  where gs := rf (xs) 3. Thus  in contrast to
SGD which utilizes a ﬁxed number of oracle calls per query point  our algorithm tends to stall in
points with smaller gradients  hence the name Lazy SGD.
Here we give some intuition regarding our adaptive minibatch size rule: Consider the stochastic
optimization setting. However  imagine that instead of the noisy gradient oracle G  we may access an
improved (imaginary) oracle which provides us with unbiased estimates  ˜g(x)  that are accurate up
to some multiplicative factor  e.g.  E[˜g(x)|x] = rf (x)  and 1
2krf (x)k  k˜g(x)k  2krf (x)k .
Then intuitively we could have used these estimates instead of the exact normalized gradients inside
our (SC-)AdaNGD2 algorithms (Algs. 2  3 with k = 2)  and still get similar (in expectation) data

3Note that the gradient norm  kgsk  is unknown to the algorithm. Nevertheless it is estimated on the ﬂy.

7

dependent bounds. Quite nicely  we may use our original noisy oracle G to generate estimates
from this imaginary oracle. This can be done by invoking G for ˜O(1/kgsk2) times at each query
point. Using this minibatch rule  the total number of calls to G (along all iterations) is equal to
T =PS
s=1 1/kgsk2. Plugging this into the data dependent bounds of (SC-)AdaNGD2 (Thms. 2.2 
3.2)  we get the well known ˜O(1/pT )/ ˜O(1/T ) rates for the stochastic convex settings.
The imaginary oracle: The construction of the imaginary oracle from the original oracle appears in
Algorithm 5 (AE procedure) . It receives as an input  G  a generator of independent random vectors
with an (unknown) expected value g 2 Rd. The algorithm outputs two variables: N which is an
estimate of 1/kgk2  and ˜gN an average of N random vectors from G. Thus  it is natural to think of
N ˜gN as an estimate for g/kgk2. Moreover  it can be shown that E[N (˜gN  g)] = 0. Thus in a sense
we receive an unbiased estimate. The guarantees of Algorithm 5 appear below 
Lemma 4.1 (Informal). Let Tmax  1  2 (0  1). Suppose an oracle G : K 7! Rd that generates
G-bounded i.i.d. random vectors with an (unknown) expected value g 2 Rd. Then w.p. 1   
invoking AE (Algorithm 5)  with m0 =⇥( G log(1/))  it is ensured that:

N =⇥(min {m0/kgk2  Tmax})  and E[N (˜gN  g)] = 0 .

Lazy SGD: Now  plugging the output of the AE algorithm into our ofﬂine algorithms (SC-)AdaNGD2 
we get their stochastic variants which appears in Algorithm 4 (Lazy SGD). This algorithm is equivalent
to the ofﬂine version of (SC-)AdaNGD2  with the difference that we use ns instead of 1/krf (xs)k2
and ns˜gs instead of rf (xs)/krf (xs)k2.
Let T be a bound on the total number of queries to the the ﬁrst order oracle G  and  be the conﬁdence
parameter used to set m0 in the AE procedure. Next we present the guarantees of LazySGD 
Lemma 4.2. Let  = O(T 3/2); let K be a convex set with diameter D  and f be a convex function;
and assume kG(x)k  G w.p.1. Then using LazySGD with ⌘0 = D/p2G  p = 1/2  ensures:

E[f (¯xT )]  min
x2K

f (x)  O✓ GD log(T )
pT

◆ .

Lemma 4.3. Let  = O(T 2)  let K be a convex set  and f be an H-strongly-convex convex function;
and assume kG(x)k  G w.p.1. Then using LazySGD with ⌘0 = 1/H  p = 1  ensures:

E[f (¯xT )]  min
x2K

f (x)  O✓ G2 log2(T )

HT

◆ .

Note that LazySGD uses minibatch sizes that are adapted to the magnitude of the gradients  and still
maintains the optimal O(1/pT )/O(1/T ) rates. In contrast using a ﬁxed minibatch size b for SGD
might degrade the convergence rates  yielding O(pb/pT )/O(b/T ) guarantees. This property of
LazySGD may be beneﬁcial when considering distributed computations (see [13]).

5 Discussion

We have presented a new approach based on a conversion scheme  which exhibits universality and
new adaptive bounds in the ofﬂine convex optimization setting  and provides a principled approach
towards minibatch size selection in the stochastic setting. Among the many questions that remain
open is whether we can devise “accelerated" universal methods. Furthermore  our universality results
only apply when the global minimum is inside the constraints. Thus  it is natural to seek for methods
that ensure universality when this assumption is violated. Moreover  our algorithms depend on
a parameter k 2 R  but only the cases where k 2{ 0  1  2} are well understood. Investigating a
wider spectrum of k values is intriguing. Lastly  it is interesting to modify and test our methods in
non-convex scenarios  especially in the context of deep-learning applications.

Acknowledgments
I would like to thank Elad Hazan and Shai Shalev-Shwartz for fruitful discussions during the early
stages of this work.
This work was supported by the ETH Zürich Postdoctoral Fellowship and Marie Curie Actions for
People COFUND program.

8

References
[1] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research  12(Jul):2121–2159  2011.

[2] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980  2014.

[3] Nicolo Cesa-Bianchi  Alex Conconi  and Claudio Gentile. On the generalization ability of
on-line learning algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[4] Stephen Wright and Jorge Nocedal. Numerical optimization. Springer Science  35:67–68  1999.

[5] Yu Nesterov. Universal gradient methods for convex optimization problems. Mathematical

Programming  152(1-2):381–404  2015.

[6] H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex

optimization. COLT 2010  page 244  2010.

[7] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning  4(2) 
2012.

[8] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 

2012.

[9] Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of

convergence o (1/k2). In Doklady an SSSR  volume 269  pages 543–547  1983.

[10] Yu E Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions.

Matekon  29:519–531  1984.

[11] Elad Hazan  Kﬁr Levy  and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
optimization. In Advances in Neural Information Processing Systems  pages 1594–1602  2015.

[12] Kﬁr Y Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint

arXiv:1611.04831  2016.

[13] Ofer Dekel  Ran Gilad-Bachrach  Ohad Shamir  and Lin Xiao. Optimal distributed online
prediction using mini-batches. Journal of Machine Learning Research  13(Jan):165–202  2012.

[14] Andrew Cotter  Ohad Shamir  Nati Srebro  and Karthik Sridharan. Better mini-batch algorithms
via accelerated gradient methods. In Advances in neural information processing systems  pages
1647–1655  2011.

[15] Shai Shalev-Shwartz and Tong Zhang. Accelerated mini-batch stochastic dual coordinate ascent.

In Advances in Neural Information Processing Systems  pages 378–385  2013.

[16] Mu Li  Tong Zhang  Yuqiang Chen  and Alexander J Smola. Efﬁcient mini-batch training for
stochastic optimization. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining  pages 661–670. ACM  2014.

[17] Martin Takáˇc  Peter Richtárik  and Nathan Srebro. Distributed mini-batch sdca. arXiv preprint

arXiv:1507.08322  2015.

[18] Prateek Jain  Sham M Kakade  Rahul Kidambi  Praneeth Netrapalli  and Aaron Sidford. Par-
allelizing stochastic approximation through mini-batching and tail-averaging. arXiv preprint
arXiv:1610.03774  2016.

[19] Peter S Bullen  Dragoslav S Mitrinovic  and M Vasic. Means and their Inequalities  volume 31.

Springer Science & Business Media  2013.

[20] Arkadii Nemirovskii  David Borisovich Yudin  and ER Dawson. Problem complexity and

method efﬁciency in optimization. 1983.

9

[21] Elad Hazan  Amit Agarwal  and Satyen Kale. Logarithmic regret algorithms for online convex

optimization. Machine Learning  69(2-3):169–192  2007.

[22] Hongzhou Lin  Julien Mairal  and Zaid Harchaoui. A universal catalyst for ﬁrst-order optimiza-

tion. In Advances in Neural Information Processing Systems  pages 3384–3392  2015.

[23] Elad Hazan and Tomer Koren. Linear regression with limited observation. In Proceedings of

the 29th International Conference on Machine Learning (ICML-12)  pages 807–814  2012.

[24] Kenneth L Clarkson  Elad Hazan  and David P Woodruff. Sublinear optimization for machine

learning. Journal of the ACM (JACM)  59(5):23  2012.

[25] Sham Kakade. Lecture notes in multivariate analysis  dimensionality reduction  and spectral
methods. http://stat.wharton.upenn.edu/~skakade/courses/stat991_
mult/lectures/MatrixConcen.pdf  April 2010.

[26] Anatoli B Juditsky and Arkadi S Nemirovski. Large deviations of vector-valued martingales in

2-smooth normed spaces. arXiv preprint arXiv:0809.0813  2008.

[27] David Asher Levin  Yuval Peres  and Elizabeth Lee Wilmer. Markov chains and mixing times.

American Mathematical Soc.  2009.

10

,Kfir Levy
Lu Qi
Shu Liu
Jianping Shi
Jiaya Jia