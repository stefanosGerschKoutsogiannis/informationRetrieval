2016,An equivalence between high dimensional Bayes optimal inference and M-estimation,Due to the computational difficulty of performing MMSE (minimum mean squared error) inference  maximum a posteriori (MAP) is often used as a surrogate. However  the accuracy of MAP is suboptimal for high dimensional inference  where the number of model parameters is of the same order as the number of samples. In this work we demonstrate how MMSE performance is asymptotically achievable via optimization with an appropriately selected convex penalty and regularization function which are a smoothed version of the widely applied MAP algorithm. Our findings provide a new derivation and interpretation for recent optimal M-estimators discovered by El Karoui  et. al. PNAS 2013 as well as extending to non-additive noise models. We demonstrate the performance of these optimal M-estimators with numerical simulations.  Overall  at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration  and high dimensional convex optimization.  In essence we show that the former computationally difficult integral may be computed by solving the latter  simpler optimization problem.,An equivalence between high dimensional Bayes

optimal inference and M-estimation

Madhu Advani

Surya Ganguli

Department of Applied Physics  Stanford University

msadvani@stanford.edu

sganguli@stanford.edu

and

Abstract

When recovering an unknown signal from noisy measurements  the computational
difﬁculty of performing optimal Bayesian MMSE (minimum mean squared error)
inference often necessitates the use of maximum a posteriori (MAP) inference 
a special case of regularized M-estimation  as a surrogate. However  MAP is
suboptimal in high dimensions  when the number of unknown signal components
is similar to the number of measurements. In this work we demonstrate  when
the signal distribution and the likelihood function associated with the noise are
both log-concave  that optimal MMSE performance is asymptotically achievable
via another M-estimation procedure. This procedure involves minimizing convex
loss and regularizer functions that are nonlinearly smoothed versions of the widely
applied MAP optimization problem. Our ﬁndings provide a new heuristic derivation
and interpretation for recent optimal M-estimators found in the setting of linear
measurements and additive noise  and further extend these results to nonlinear
measurements with non-additive noise. We numerically demonstrate superior
performance of our optimal M-estimators relative to MAP. Overall  at the heart
of our work is the revelation of a remarkable equivalence between two seemingly
very different computational problems: namely that of high dimensional Bayesian
integration underlying MMSE inference  and high dimensional convex optimization
underlying M-estimation. In essence we show that the former difﬁcult integral may
be computed by solving the latter  simpler optimization problem.

1

Introduction

Modern technological advances now enable scientists to simultaneously record hundreds or thousands
of variables in ﬁelds ranging from neuroscience and genomics to health care and economics. For
example  in neuroscience  we can simultaneously record P = O(1000) neurons in behaving animals.
However  the number of measurements N we can make of these P dimensional neural activity
patterns can be limited in any given experimental condition due to constraints on recording time.
Thus a critical parameter is the measurement density α = N
P . Classical statistics focuses on the
limit of few variables and many measurements  so P is ﬁnite  N is large  and α → ∞. Here  we
instead consider the modern high dimensional limit where the measurement density α remains ﬁnite
as N  P → ∞. In this important limit  we ask what is the optimal way to recover signal from noise?
More precisely  we wish to recover an unknown signal vector s0 ∈ RP given N noisy measurements
(1)
Here  xµ and yµ are input-output pairs for measurement µ  r is a measurement nonlinearity  and
µ is a noise realization. For example  in a brain machine interface  xµ could be a neural activity
pattern  yµ a behavioral covariate  and s0 the unknown regression coefﬁcients of a decoder relating
neural activity to behavior. Alternatively  in sensory neuroscience  xµ could be an external stimulus 

yµ = r(xµ · s0  µ) where xµ ∈ RP

for µ = 1  . . .   N.

and

yµ ∈ R 

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

yµ a single neuron’s response to that stimulus  and s0 the unknown receptive ﬁeld relating stimulus
to neural response. We assume the noise µ is independent and identically distributed (iid) across
measurements  implying the outputs yµ are drawn iid from a noise distribution Py|z(yµ|zµ)  where
zµ = xµ · s0. Similarly  we assume the signal components s0
i are drawn iid from a prior signal
s. Finally  we denote by X ∈ RN×P the
distribution Ps(s0). We denote its variance below by σ2
input or measurement matrix  whose µ’th row is xµ  and by y ∈ RN the measurement output vector
whose µ’th component is yµ. In this paper  we will focus on the case of dense iid random Gaussian
measurements  normalized so that (cid:104) xµ · xν (cid:105) = γ δµ ν. In the case of systems identiﬁcation in
sensory neuroscience  this choice would correspond to an oft used white noise stimulus at contrast γ.
Now given measurement data (X  y)  as well as knowledge of the nonlinearity r(·) and the signal
Ps and noise Py|z distributions  what is the best way to infer an estimate ˆs of the unknown signal
s0? We characterize the performance of an estimate ˆs by its mean squared error (MSE)  (cid:107)ˆs − s0(cid:107)2
2 
averaged over noise realizations and measurements. The best minimal MSE (MMSE) estimator is
given by optimal Bayesian integration to compute the posterior mean:

ˆsMMSE =

s P (s|X  y) ds.

(2)

(cid:90)

Unfortunately  this integral is generally intractable in high dimensions  at large P ; both numerical
integration and Monte Carlo methods for estimating the integral require computational time growing
exponentially in P for high accuracy. Consequently  an often used surrogate for MMSE inference
is maximum a posteriori (MAP) inference  which computes the mode rather than the mean of the
posterior distribution. Thus MAP relies on optimization rather than integration:
[− log P (s|X  y)].

P (s|X  y) = arg min

ˆsMAP = arg max

(3)

Assuming inputs X are independent of the unkown signal s0  the above expression becomes

s

(cid:34) N(cid:88)

s

P(cid:88)

(cid:35)

ˆsMAP = arg min

s

− log Py|z(yµ|xµ · s) +

− log Ps(si)

.

(4)

µ=1

i=1

A related algorithm is maximum likelihood (ML)  which seeks to maximize the likelihood of the data
given a candidate signal s. ML is equivalent to MAP in (4) but without the second sum  i.e. without
prior information on the signal.
While ML is typically optimal amongst unbiased estimators in the classical statistical limit α → ∞
(see e.g. [1])  neither MAP nor ML are optimal in high dimensions  at ﬁnite α. Therefore  we consider
a broader class of estimators known as regularized M-estimators  corresponding to the optimization
problem

ˆs = arg min

s

L(yµ  xµ · s) +

σ(si)

.

(5)

Here L(y  η) is a loss function and σ is a regularizer. We assume both to be convex functions in η and
s respectively. Note that MAP inference corresponds to the choice L(y  η) = − log Py|z(y|η) and
σ(s) = − log Ps(s). ML inference corresponds to the same loss function but without regularization:
σ(s) = 0. Other well known M-estimators include LASSO [2]  corresponding to the choice
2 (y − η)2 and σ(s) ∝ |s|  or the elastic net [3]  which includes an addition quadratic term
L(y  η) = 1
on the LASSO regularizer. Such M-estimators are heuristically motivated as a convex relaxation of
MAP inference for sparse signal distributions  and have been found to be very useful in such settings.
However  a general theory for how to select the optimal M-estimator in (5) given the generative model
of data in (1) remains elusive. This is the central problem we address in this work.

(cid:35)

P(cid:88)

i=1

(cid:34) N(cid:88)

µ=1

1.1 Related work and Outline

Seminal work [4] found the optimal unregularized M-estimator using variational methods in the
special case of linear measurements and additive noise  i.e. r(z  ) = z +  in (1). In this same
setting  [5] characterized unregularized M-estimator performance via approximate message passing
(AMP) [6]. Following this  the performance of regularized M-estimators in the linear additive setting
was characterized in [7]  using non-rigorous statistical physics methods based on replica theory  and

2

in [8]  using rigorous methods different from [4  5]. Moreover  [7] found the optimal regularized
M-estimator and demonstrated  surprisingly  zero performance gap relative to MMSE. The goals of
this paper are to (1) interpret and extend previous work by deriving an equivalence between optimal
M-estimation and Bayesian MMSE inference via AMP and (2) to derive the optimal M-estimator in
the more general setting of nonlinear measurements and non-additive noise.
To address these goals  we begin in section 2 by describing a pair of AMP algorithms  derived
heuristically via approximations of belief propagation (BP). The ﬁrst algorithm  mAMP  is designed
to solve M-estimation in (5)  while the second  bAMP  is designed to solve Bayesian MMSE inference
in (2). In section 3 we derive a connection  via AMP  between M-estimation and MMSE inference:
we ﬁnd  for a particular choice of optimal M-estimator  that mAMP and bAMP have the same ﬁxed
points. To quantitatively determine the optimal M-estimator  which depends on some smoothing
parameters  we must quantitatively characterize the performance of AMP  which we do in section
4. We thereby recover optimal M-estimators found in recent works in the linear additive setting 
without using variational methods  and moreover ﬁnd optimal M-estimators in the nonlinear  non-
additive setting. Our non-variational approach through AMP also provides an intuitive explanation
for the form of the optimal M-estimator in terms of Bayesian inference. Intriguingly  the optimal
M-estimator resembles a smoothed version of MAP  with lower measurement density requiring
more smoothing. In Section 4  we also demonstrate  through numerical simulations  a substantial
performance improvement in inference accuracy achieved by the optimal M-estimator over MAP
under nonlinear measurements with non-additive noise. We end with a discussion in section 5.

2 Formulations of Bayesian inference and M-estimation through AMP

Both mAMP and bAMP  heuristically derived in the supplementary material 1 (SM) sections 2.2-2.4
though approximate BP applied to (5) and (2) respectively  can be expressed as special cases of a
generalized AMP (gAMP) algorithm [9]  which we ﬁrst describe. gAMP is a set of iterative equations 

ηt = Xˆst + λt

ηGy(λt−1

η

ˆst+1 = Gs

h  ˆst − λt
λt

hXT Gy(λt

η  y  ηt)

(6)

(cid:17)

(cid:16)
P(cid:88)

j=1

  y  ηt−1)

(cid:33)−1

(cid:32)

N(cid:88)

ν=1

λt
h =

γα
N

∂
∂η

Gy(λt

η  yν  ηt
ν)

λt+1
η =

γλt
h
P

Gs(λt

h  ˆst

j − λt

hXT

j Gy(λt

η  y  ηt)) 

∂
∂h

(7)
that depend on the scalar functions Gy(λη  y  η) and Gs(λh  h) which  in our notation  act component-
wise on vectors so that µth component Gy(λη  y  η)µ = Gy(λη  yµ  ηµ) and the ith component
η ∈ R+ and ηt=−1 ∈ RN .
Gs(λh  h)i = Gs(λh  hi). Initial conditions are given by ˆst=0 ∈ RP   λt=0
Intuitively  one can think of ηt as related to the linear part of the measurement outcome predicted by
the current guess ˆst  and Gy is a measurement correction map that uses the actual measurement data y
to correct ηt. Also  intuitively  we can think of Gs as taking an input ˆst− λt
η  y  ηt)  which
is a measurement based correction to ˆst  and yielding as output a further  measurement independent
correction ˆst+1  that could depend on either a regularizer or prior. We thus refer to the functions Gy
and Gs as the measurement and signal correctors respectively. gAMP is thus alternating measurement
and signal correction  with time dependent parameters λt
η. These equations were described in
[9]  and special cases of them were studied in various works (see e.g. [5  10]).

hXT Gy(λt

h and λt

2.1 From M-estimation to mAMP

Now  applying approximate BP to (5) when the input vectors xµ are iid Gaussian  again with
normalization (cid:104) xµ · xµ (cid:105) = γ  we ﬁnd (SM Sec. 2.3) that the resulting mAMP equations are a special
case of the gAMP equations  where the functions Gy and Gs are related to the loss L and regularizer
σ through

y (λη  y  η) = Mλη [L(y ·) ](cid:48)(η) 
GM

s (λh  h) = Pλh[ σ ](h).
GM

(8)

1Please see https://ganguli-gang.stanford.edu/pdf/16.Bayes.Mestimation.Supp.pdf for the supplementary

material.

3

The functional mappings M and P  the Moreau envelope and proximal map [11]  are deﬁned as

Mλ[ f ](x) = min

y

+ f (y)

 

Pλ[ f ](x) = arg min

y

(cid:20) (x − y)2

2λ

(cid:21)

(cid:20) (x − y)2

2λ

(cid:21)

+ f (y)

.

(9)

The proximal map maps a point x to another point that minimizes f while remaining close to x as
determined by a scale λ. This can be thought of as a proximal descent step on f starting from x with
step length λ. Perhaps the most ubiquitous example of a proximal map occurs for f (z) = |z|  in which
case the proximal map is known as the soft thresholding operator and takes the form Pλ[ f ](x) = 0
for |x| ≤ λ and Pλ[ f ](x) = x − sign(x)λ for |x| ≥ λ. This soft thresholding is prominent in AMP
approaches to compressed sensing (e.g. [10]). The Moreau envelope is a minimum convolution of f
with a quadratic  and as such  Mλ[ f ](x) is a smoothed lower bound on f with the same minima
[11]. Moreover  differentiating M with respect to x yields [11] the relation

(10)
Thus a proximal descent step on f is equivalent to a gradient descent step on the Moreau envelope of
f  with the same step length λ. This equality is also useful in proving (SM Sec. 2.1) that the ﬁxed
points of mAMP satisfy

Pλ[ f ](x) = x − λMλ[ f ](cid:48)(x).

XT ∂
∂η

L(y  Xˆs) + σ(cid:48)(ˆs) = 0.

(11)

ˆst+1 = Pλh [ σ ](ˆst − λhXT ∂

Thus ﬁxed points of mAMP are local minima of M-estimation in (5).
To develop intuition for the mAMP algorithm  we note that the ˆs update step in (6) is similar to
the more intuitive proximal gradient descent algorithm [11] which seeks to solve the M-estimation
problem in (5) by alternately performing a gradient descent step on the loss term and a proximal
descent step on the regularization term  both with the same step length. Thus one iteration of gradient
descent on L followed by proximal descent on σ in (5)  with both steps using step length λh  yields
(12)
By inserting (8) into (6)-(7)  we see that mAMP closely resembles proximal gradient descent  but
with three main differences: 1) the loss function is replaced with its Moreau envelope  2) the loss is
evaluated at ηt which includes an additional memory term  and 3) the step size λt
h is time dependent.
Interestingly  this additional memory term and step size evolution has been found to speed up
convergence relative to proximal gradient descent in certain special cases  like LASSO [10].
In summary  in mAMP the measurement corrector Gy implements a gradient descent on the Moreau
smoothed loss  while the signal corrector Gs implements a proximal descent step on the regularizer.
But because of (10)  this latter step can also be thought of as a gradient descent step on the Moreau
smoothed regularizer. Thus overall  the mAMP approach to M-estimation is intimately related to
Moreau smoothing of both the loss and regularizer.

∂ηL(y  Xˆst)).

2.2 From Bayesian integration to bAMP

(cid:82) sPs(s)e− (s−h)2
(cid:82) Ps(s)e− (s−h)2

Now  applying approximate BP to (2) when again the input vectors xµ are iid Gaussian  we ﬁnd (SM
Sec. 2.2) that the resulting bAMP equations are a special case of the gAMP equations  where the
functions Gy and Gs are related to the noise Py|z and signal Ps distributions through
s (λh  h) = ˆsmmse(λh  h) 

log (Py(y|η  λη)) 

(13)

GB

y (λη  y  η) = − ∂
GB
∂η

where

Py(y|η  λ) ∝

(cid:90)
(cid:10) s0|h(cid:11) where h = s0 +

2λ ds

Py|z(y|z)e− (η−z)2

(14)
as derived in SM section 2.2. Here Py(y|η  λ) is a convolution of the likelihood with a Gaussian of
variance λ (normalized so that it is a probability density in y) and ˆsmmse denotes the posterior mean
λw is a corrupted signal  w is a standard Gaussian random variable  and

2λ dz 

ˆsmmse(λ  h) =

2λ ds

√

 

s0 is a random variable drawn from Ps.
Inserting these equations into (6)-(7)  we see that bAMP performs a measurement correction step
through Gy that corresponds to a gradient descent step on the negative log of a Gaussian-smoothed
likelihood function. The subsequent signal correction step through Gs is simply the computation of a
posterior mean  assuming the input is drawn from the prior and corrupted by additive Gaussian noise
with a time-dependent variance λt
h.

4

3 An AMP equivalence between Bayesian inference and M-estimation

In the previous section  we saw intriguing parallels between mAMP and bAMP  both special cases of
gAMP. While mAMP performs its measurement and signal correction through a gradient descent
step on a Moreau smoothed loss and a Moreau smoothed regularizer respectively  bAMP performs its
measurement correction through a gradient descent step on the minus log of a Gaussian smoothed
likelihood  and its signal correction though an MMSE estimation problem. These parallels suggest we
may be able to ﬁnd a loss L and regularizer σ such that the corresponding mAMP becomes equivalent
to bAMP. If so  then assuming the correctness of bAMP as a solution to (2)  the resulting Lopt and
σopt will yield the optimal mAMP dynamics  achieving MMSE inference.
By comparing (8) and (13)  we see that bAMP and mAMP will have the same Gy if the Moreau-
smoothed loss equals the minus log of the Gaussian-smoothed likelihood function:

Mλη [Lopt(y ·) ](η) = − log (Py(y|η  λη)).

(15)
Before describing how to invert the above expression to determine Lopt  we would also like to ﬁnd a
relation between the two signal correction functions GM
s . This is a little more challenging
because the former implements a proximal descent step while the latter implements an MMSE
posterior mean computation. However  we can express the MMSE computation as gradient ascent on
the log of a Gaussian smoothed signal distribution (see SM):

s and GB

ˆsmmse(λh  h) = h + λh

log (Ps(h  λh)) 

Ps(h  λ) ∝

∂
∂h

Ps(s)e− (s−h)2

2λ ds.

(16)

(cid:90)

s as gradient descent on
Moreover  by applying (10) to the deﬁnition of GM
s
a Moreau smoothed regularizer. Then  comparing these modiﬁed forms of GB
s   we ﬁnd
a similar condition for σopt  namely that its Moreau smoothing should equal the minus log of the
Gaussian smoothed signal distribution:

in (8)  we can write GM

s with GM

Mλh [ σopt ](h) = − log (Ps(h  λh)) .

(17)

Our goal is now to compute the optimal loss and regularizer by inverting the Moreau envelope
relations (15  17) to solve for Lopt  σopt. A sufﬁcient condition [4] to invert these Moreau envelopes
to determine the optimal mAMP dynamics is that Py(y|z) and Ps(s) are log concave with respect
to z and s respectively. Under this condition the Moreau envelope will be invertible via the relation
Mq[−Mq[−f ](·) ](·) = f (·) (see SM Appendix A.3 for a derivation)  which yields:

σopt(h) = −Mλh[ log (Ps(·  λh)) ](h).

Lopt(y  η) = −Mλη [ log (Py(y|·  λη)) ](η) 

(18)
This optimal loss and regularizer form resembles smoothed MAP inference  with λη and λh being
scalar parameters that modify MAP through both Gaussian and Moreau smoothing. An example of
such a family of smoothed loss and regularizer functions is given in Fig. 1 for the case of a logistic
output channel with Laplacian distributed signal. Additionally  one can show that the optimal loss
and regularizer are convex when the signal and noise distributions are log-concave. Overall  this
analysis yields a dynamical equivalence between mAMP and bAMP as long as at each iteration time
t  the optimal loss and regularizer for mAMP are chosen through the smoothing operation in (18)  but
using time-dependent smoothing parameters λt

h whose evolution is governed by (7).

η and λt

4 Determining optimal smoothing parameters via state evolution of AMP

In the previous section  we have shown that mAMP and bAMP have the same dynamics  as long as  at
each iteration t of mAMP  we choose a time dependent optimal loss Lopt
through
(18)  where the time dependence is inherited from the time dependent smoothing parameters λt
η and
h. However  mAMP was motivated as an algorithmic solution to the M-estimation problem in (5)
λt
for a ﬁxed loss and regularizer  while bAMP was motivated as a method of performing the Bayesian
integral in (2). This then raises the question  is there a ﬁxed  optimal choice of Lopt and σopt in (5)
such the corresponding M-estimation problem yields the same answer as the Bayesian integral in (2)?
The answer is yes: simply choose a ﬁxed Lopt and σopt through (18) where the smoothing parameters
λη and λh are chosen to be those found at the ﬁxed points of bAMP. To see this  note that ﬁxed
points of mAMP with time dependent choices of Lopt
are equivalent to the minima of the

and regularizer σopt

t and σopt

t

t

t

5

Figure 1: Here we plot the optimal loss (A) and regularizer (B) in (18)  for a logistic output y ∈ {0  1}
with Py|z(y = 1|z) = 1
2 e−|s|. In (A) we plot the loss
for the measurement y = 1: Lopt(y = 1 ·). Both sets of curves from red to black (and bottom to top)
correspond to smoothing parameters λη = (0  2  4  6) in (A) and λh = (0  1/2  1  2) in (B). With
zero smoothing  the red curves at the bottom correspond to the MAP loss and regularizer.

1+e−z   and Laplacian signal s with Ps(s) = 1

η  λt

η and λ∞

h) approaches the ﬁxed point (ˆs∞  λ∞

M-estimation problem in (5)  with the choice of loss and regularizer that this time dependent sequence
converges to: Lopt∞ and σopt∞ (this follows from an extension of the argument that lead to (11)). In turn
the ﬁxed points of mAMP are equivalent to those of bAMP under the choice (18). These equivalences
η   λ∞
then imply that  if the bAMP dynamics for (ˆst  λt
h ) 
then ˆs∞ is the solution to both Bayesian inference in (2) and optimal M-estimation in (5)  with
η and λ∞
optimal loss and regularizer given by (18) with the choice of smoothing parameters λ∞
h .
We now discuss how to determine λ∞
h analytically  thereby completing our heuristic derivation
of an optimal M-estimator that matches Bayesian MMSE inference. An essential tool is state evolution
(SE) which characterizes the gAMP dynamics [12] as follows. First  let z = Xs0 be related to the
true measurements. Then (6) implies that ηt− z is a time-dependent residual. Remarkably  the gAMP
equations ensure that the components of the residual ηt − z  as well as ht = −λt
η  y  ηt)
are Gaussian distributed; the history term in the update of ηt in (6) crucially cancels out non-Gaussian
structure that would otherwise develop as the vectors ηt and ht propagate through the nonlinear
measurement and signal correction steps induced by Gy and Gs. We denote by qt
h the variance
of the components of ηt − z and ht respectively. Additionally  we denote by qt
P (cid:104)(cid:107) ˆst − s0(cid:107)2(cid:105)
the per component MSE at iteration t. SE is a set of analytical evolution equations for the quantities
(qt
h) that characterize the state of gAMP. A rigorous derivation both for dense [12]
s  qt
Gaussian measurements and sparse measurements [13] reveal that the SE equations accurately track
the gAMP dynamical state in the high dimensional limit N  P → ∞ with α = N
P O(1) that we
consider here.
We derive the speciﬁc form of the mAMP SE equations  yielding a set of 5 update equations (see SM
section 3.1 for further details). We also derive the SE equations for bAMP  which are simpler. First 
we ﬁnd the relations λt
h. Thus SE for bAMP reduces to a pair of update equations:

η and qt
s = 1

hXT Gy(λt

η = qt

h  λt

η  λt

η  qt

(cid:18)

(cid:68)(cid:0)GB

η  y  ηt)(cid:1)2(cid:69)

qt
h =

αγ

y (qt

w s0

(cid:19)−1

.

y z ηt

(cid:28)(cid:16)

η and λt

h  s0 +(cid:112)qt

h = qt

hw) − s0(cid:17)2(cid:29)

qt+1
η = γ

GB

s (qt

(19)
Here w is a zero mean unit variance Gaussian and s0 is a scalar signal drawn from the signal
distribution Ps. Thus the computation of the next residual qt+1
on the LHS of (19) involves
computing the MSE in estimating a signal s0 corrupted by Gaussian noise of variance qt
h  using
MMSE inference as an estimation prcoedure via the function GB deﬁned in (13). The RHS involves
an average over the joint distribution of scalar versions of the output y  true measurement z  and
estimated measurement ηt. These three scalars are the SE analogs of the gAMP variables y  z 
and ηt  and they model the joint distribution of single components of these vectors. Their joint
distribution is given by P (y  z  ηt) = Py|z(y|z)P (z  ηt). In the special case of bAMP  z and ηt
are jointly zero mean Gaussian with second moments given by (cid:104)(ηt)2(cid:105) = γσ2
η  (cid:104)z2(cid:105) = γσ2
s 

s − qt

η

6

-4-202401234-2-101200.511.52A Optimal loss B Optimal regularizer and (cid:104) zηt (cid:105) = γσ2

(cid:10) (z − ηt)2(cid:11) = qt

s − qt
η (see SM 3.2 for derivations). These moments imply the residual variance
η. Intuitively  when gAMP works well  that is reﬂected in the SE equations by the
reduction of the residual variance qt
η over time  as the time dependent estimated measurement ηt
converges to the true measurement z. The actual measurement outcome y  after the nonlinear part of
the measurement process  is always conditionally independent of the estimated measurement ηt  given
the true linear part of the measurement  z. Finally  the joint distribution of a single component of ˆst+1
and s0 in gAMP are predicted by SE to have the same distribution as ˆst+1 = GB
hw) 
after marginalizing out w. Comparing with the LHS of (19) then yields that the MSE per component
satisﬁes qt
Now  bAMP performance  upon convergence  is characterized by the ﬁxed point of SE  which satisﬁes

h  s0 +(cid:112)qt

s = qt

η/γ.

s (qt

qs = MMSE(s0|s0 +

√

qhw)

qh =

1

αγJ [ Py(y|η  γqs) ]

.

(20)

Here  the MMSE function denotes the minimal error in estimating the scalar signal s0 from a
measurement of s0 corrupted by additive Gaussian noise of variance qh via computation of the

posterior mean(cid:10) s0|s0 +

qhw(cid:11):

√

MMSE(s0|s0 +

√

qhw) =

(cid:68)(cid:0)(cid:10) s0|s0 +

qhw(cid:11) − s0(cid:1)2(cid:69)

√

.

(21)

s0 w

Also  the function J on the RHS of (20) denotes the average Fisher information that y retains about
an input  with some additional Gaussian input noise of variance q:
∂η2 log Py(y|η  q)

J [ Py(y|η  q) ] = −(cid:68) ∂2

(cid:69)

(22)

η y

These equations characterize the performance of bAMP  through qs. Furthermore  they yield the
optimal smoothing parameters λη = γqs and λh = qh. This choice of smoothing parameters 
when used in (18)  yield a ﬁxed optimal loss Lopt and regularizer σopt. When this optimal loss
and regularizer are used in the M-estimation problem in (5)  the resulting M-estimator should have
performance equivalent to that of MMSE inference in (2). This completes our heuristic derivation of
an equivalence between optimal M-estimation and Bayesian inference through message passing.
In Figure 2 we demonstrate numerically that the optimal M-estimator substantially outperforms MAP 
especially at low measurement density α  and has performance equivalent to MMSE inference  as
theoretically predicted by SE for bAMP.

Figure 2: For logistic output and Laplacian signal  as in Fig.
1  we plot the per component MSE  normalized by signal
variance. Smooth curves are theoretical predictions based
on SE ﬁxed points for mAMP for MAP inference (red) and
bAMP for MMSE inference (black). Error bars reﬂect stan-
dard deviation in performance obtained by solving (5)  via
mAMP  for MAP inference (red) and optimal M-estimation
(black)  using simulated data generated as in (1)  with dense
i.i.d Gaussian measurements. For these ﬁnite simulated data
N P ≈ 250. These
sets  we varied α = N
results demonstrate that optimal M-estimation both signif-
icantly outperforms MAP (black below red) and matches
Bayesian MMSE inference as predicted by SE for bAMP
(black error bars consistent with black curve).

P   while holding

√

5 Discussion

Overall we have derived an optimal M-estimator  or a choice of optimal loss and regularizer  such the
M-estimation problem in (5) has equivalent performance to that of Bayes optimal MMSE inference in
(2)  in the case of log-concave signal distribution and noise likelihood. Our derivation is heuristic in
that it employs the formalism of gAMP  and as such depends on the correctness of a few statements.
First  we assume that two special cases of the gAMP dynamics in (6)  namely mAMP in (8) and

7

0123450.30.40.50.60.70.80.9MAPOptimalOptimal vs MAP inference errorbAMP in (13) correctly solve the M-estimation problem in (5) and Bayesian MMSE inference in
(2)  respectively. We provide a heuristic derivation of both of these assumptions in the SM based on
approximations of BP. Second  we require that SE in (19) correctly tracks the performance of gAMP
in (13). We note that under mild conditions  the correctness of SE as a description of gAMP was
rigorously proven in [12].
While we have not presented a rigorous derivation that the bAMP dynamics correctly solves the
MMSE inference problem  we note several related rigorous results. First  it has been shown that
bAMP is equivalent to MMSE inference in the limit of large sparse measurement matrices in [13  14].
Also  in this same large sparse limit  the corresponding mAMP algorithm was shown to be equivalent
to MAP inference with additive Gaussian noise [15]. In the setting of dense measurements  the
correctness of bAMP has not yet been rigorously proven  but the associated SE is believed to be exact
in the dense iid Gaussian measurement setting based on replica arguments from statistical physics
(see e.g. section 4.3 in [16] for further discussion). For this reason  similar arguments have been
used to determine theoretical bounds on inference algorithms in compressed sensing [16]  and matrix
factorization [17].
There are further rigorous results in the setting of M-estimation: mAMP and its associated SE is
also provably correct in the large sparse measurement limit  and has additionally been rigorously
proven to converge in special cases [5] [6] for dense iid Gaussian measurements. We further expect
these results to generalize to a universality class of measurement matrices with iid elements and a
suitable condition on their moments. Indeed this generalization was demonstrated rigorously for a
subclass of M-estimators in [18]. In the setting of dense measurements  due to the current absence
of rigorous results demonstrating the correctness of bAMP in solving MMSE inference  we have
also provided numerical experiments in Fig. 2. This ﬁgure demonstrates that optimal M-estimation
can signiﬁcantly outperform MAP for high dimensional inference problems  again for the case of
log-concave signal and noise.
Additionally  we note that the per-iteration time complexity of the gAMP algorithms (6  7) scales
linearly in both the number of measurements and signal dimensions. Therefore the optimal algorithms
we describe are applicable to large-scale problems. Moreover  at lower measurement densities  the
optimal loss and regularizer are smoother. Such smoothing may accelerate convergence time. Indeed
smoother convex functions  with smaller Lipschitz constants on their derivative  can be minimized
faster via gradient descent. It would be interesting to explore whether a similar result may hold for
gAMP dynamics.
Another interesting future direction is the optimal estimation of sparse signals  which typically do not
have log-concave distributions. One potential strategy in such scenarios would be to approximate
the signal distribution with the best log-concave ﬁt and apply optimal smoothing to determine a
good regularizer. Alternatively  for any practical problem  one could choose the precise smoothing
parameters through any model selection procedure  for example cross-validation on held-out data.
Thus the combined Moreau and Gaussian smoothing in (18) could yield a family of optimization
problems  where one member of this family could potentially yield better performance in practice on
held-out data. For example  while LASSO performs very well for sparse signals  as demonstrated by
its success in compressed sensing [19  20]  the popular elastic net [3]  which sometimes outperforms
pure LASSO by combining L1 and L2 penalties  resembles a speciﬁc type of smoothing of an L1
regularizer. It would be interesting to see if combined Moreau and Gaussian smoothing underlying
our optimal M-estimators could signiﬁcantly out-perform LASSO and elastic net in practice  when
our distributional assumptions about signal and noise need not precisely hold. However  ﬁnding
optimal M-estimators for known sparse signal distributions  and characterizing the gap between their
performance and that of MMSE inference  remains a fundamental open question.

Acknowledgements

The authors would like to thank Lenka Zdeborova and Stephen Boyd for useful discussions and also
Chris Stock and Ben Poole for comments on the manuscript. M.A. thanks the Stanford MBC and
SGF for support. S.G. thanks the Burroughs Wellcome  Simons  Sloan  McKnight  and McDonnell
foundations  and the Ofﬁce of Naval Research for support.

8

References
[1] P. Huber and E. Ronchetti. Robust Statistics. Wiley  2009.

[2] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society. Series B (Methodological)  58:267–288  1996.

[3] H. Zou and T. Hastie. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 

67:301–320  2005.

[4] D. Bean  PJ Bickel  N. El Karoui  and B. Yu. Optimal M-estimation in high-dimensional

regression. PNAS  110(36):14563–8  2013.

[5] D Donoho and A Montanari. High dimensional robust m-estimation: Asymptotic variance via

approximate message passing. Probability Theory and Related Fields  pages 1–35  2013.

[6] M Bayati and A Montanari. The dynamics of message passing on dense graphs  with applications

to compressed sensing. Information Theory  IEEE Transactions  57(2):764–785  2011.

[7] M Advani and S Ganguli. Statistical mechanics of optimal convex inference in high dimensions.

Physical Review X  6:031034.

[8] C. Thrampoulidis  Abbasi E.  and Hassibi B. Precise high-dimensional error analysis of
regularized m-estimators. 2015 53rd Annual Allerton Conference on Communication  Control 
and Compting (Allerton)  pages 410–417  2015.

[9] S Rangan. Generalized approximate message passing for estimation with random linear mixing.
Information Theory Proceedings (ISIT)  2011 IEEE International Symposium  pages 2168–2172 
2011.

[10] D. L. Donoho  A. Maleki  and Montanari A. Message-passing algorithms for compressed

sensing. Proceedings of the National Academy of Sciences  pages 18914–18919  2009.

[11] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in Optimization  1(3):123–

231  2013.

[12] A Javanmard and A Montanari. State evolution for general approximate message passing
algorithms  with applications to spatial coupling. Information and Inference  page iat004  2013.

[13] S Rangan. Estimation with random linear mixing  belief propagation and compressed sensing.

Information Sciences and Systems (CISS)  2010 44th Annual Conference  2010.

[14] D Guo and CC Wang. Random sparse linear systems observed via arbitrary channels: A
decoupling principle. Information Theory  2007. ISIT 2007. IEEE International Symposium 
2007.

[15] CC Wang and D Guo. Belief propagation is asymptotically equivalent to map estimation for

sparse linear systems. Proc. Allerton Conf  pages 926–935  2006.

[16] F Krzakala  M M´ezard  F Sausset  Y. Sun  and L. Zdeborov´a. Probabilistic reconstruction in
compressed sensing: algorithms  phase diagrams  and threshold achieving matrices. Journal of
Statistical Mechanics: Theory and Experiment  (08):P08009  2012.

[17] Y. Kabashima  F. Krzakala  M. Mzard  A. Sakata  and L. Zdeborov´a. Phase transitions and
sample complexity in bayes-optimal matrix factorization. IEEE Transactions on Information
Theory  62:4228–4265  2016.

[18] M Bayati  M Lelarge  and A Montanari. Universality in polytope phase transitions and message

passing algorithms. The Annals of Applied Probability  25:753–822  2015.

[19] E. Candes and M. Wakin. An introduction to compressive sampling. IEEE Sig. Proc. Mag. 

25(2):21–30  2008.

[20] A.M. Bruckstein  D.L. Donoho  and M. Elad. From sparse solutions of systems of equations to

sparse modeling of signals and images. SIAM Review  51(1):34–81  2009.

9

,Madhu Advani
Surya Ganguli
Yannick Schroecker
Charles Isbell