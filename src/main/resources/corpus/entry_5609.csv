2019,Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks,Stochastic gradient descent with a large initial learning rate is widely used for training modern neural net architectures. Although a small initial learning rate allows for faster training and better test performance initially  the large learning rate achieves better generalization soon after the learning rate is annealed. Towards explaining this phenomenon  we devise a setting in which we can prove that a two layer network trained with large initial learning rate and annealing provably generalizes better than the same network trained with a small learning rate from the start. The key insight in our analysis is that the order of learning different types of patterns is crucial: because the small learning rate model first memorizes low-noise  hard-to-fit patterns  it generalizes worse on hard-to-generalize  easier-to-fit patterns than its large learning rate counterpart. This concept translates to a larger-scale setting: we demonstrate that one can add a small patch to CIFAR-10 images that is immediately memorizable by a model with small initial learning rate  but ignored by the model with large learning rate until after annealing. Our experiments show that this causes the small learning rate model's accuracy on unmodified images to suffer  as it relies too much on the patch early on.,TowardsExplainingtheRegularizationEffectofInitialLargeLearningRateinTrainingNeuralNetworksYuanzhiLiMachineLearningDepartmentCarnegieMellonUniversityyuanzhil@andrew.cmu.eduColinWeiComputerScienceDepartmentStanfordUniversitycolinwei@stanford.eduTengyuMaComputerScienceDepartmentStanfordUniversitytengyuma@stanford.eduAbstractStochasticgradientdescentwithalargeinitiallearningrateiswidelyusedfortrainingmodernneuralnetarchitectures.Althoughasmallinitiallearningrateallowsforfastertrainingandbettertestperformanceinitially thelargelearningrateachievesbettergeneralizationsoonafterthelearningrateisannealed.Towardsexplainingthisphenomenon wedeviseasettinginwhichwecanprovethatatwolayernetworktrainedwithlargeinitiallearningrateandannealingprovablygeneralizesbetterthanthesamenetworktrainedwithasmalllearningratefromthestart.Thekeyinsightinouranalysisisthattheorderoflearningdifferenttypesofpatternsiscrucial:becausethesmalllearningratemodelﬁrstmemorizeseasy-to-generalize hard-to-ﬁtpatterns itgeneralizesworseonhard-to-generalize easier-to-ﬁtpatternsthanitslargelearningratecounterpart.Thisconcepttranslatestoalarger-scalesetting:wedemonstratethatonecanaddasmallpatchtoCIFAR-10imagesthatisimmediatelymemorizablebyamodelwithsmallinitiallearningrate butignoredbythemodelwithlargelearningrateuntilafterannealing.Ourexperimentsshowthatthiscausesthesmalllearningratemodel’saccuracyonunmodiﬁedimagestosuffer asitreliestoomuchonthepatchearlyon.1IntroductionItisacommonlyacceptedfactthatalargeinitiallearningrateisrequiredtosuccessfullytrainadeepnetworkeventhoughitslowsdownoptimizationofthetrainloss.Modernstate-of-the-artarchitecturestypicallystartwithalargelearningrateandannealitatapointwhenthemodel’sﬁttothetrainingdataplateaus[25 32 17 42].Meanwhile modelstrainedusingonlysmalllearningrateshavebeenfoundtogeneralizepoorlydespiteenjoyingfasteroptimizationofthetrainingloss.Anumberofpapershaveproposedexplanationsforthisphenomenon suchassharpnessofthelocalminima[22 20 24] thetimeittakestomovefrominitialization[18 40] andthescaleofSGDnoise[38].However westillhavealimitedunderstandingofasurprisingandstrikingpartofthelargelearningratephenomenon:fromlookingatthesectionoftheaccuracycurvebeforeannealing itwouldappearthatasmalllearningratemodelshouldoutperformthelargelearningratemodelinbothtrainingandtesterror.Concretely inFig.1 themodeltrainedwithsmalllearningrateoutperforms33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019) Vancouver Canada.Figure1:CIFAR-10accuracyvs.epochforWideResNetwithweightdecay nodataaugmen-tation andinitiallrof0.1vs.0.01.Grayrepresentstheanneal-ingtime.Left:Train.Right:Validation.thelargelearningrateuntilepoch60whenthelearningrateisﬁrstannealed.Onlyafterannealingdoesthelargelearningratevisiblyoutperformthesmalllearningrateintermsofgeneralization.Inthispaper weproposetotheoreticallyexplainthisphenomenonviatheconceptoflearningorderofthemodel i.e. theratesatwhichitlearnsdifferenttypesofexamples.Thisisnotatypicalconceptinthegeneralizationliterature—learningorderisatraining-timepropertyofthemodel butmostanalysesonlyconsiderpost-trainingpropertiessuchastheclassiﬁer’scomplexity[8] orthealgorithm’soutputstability[9].Wewillconstructasimpledistributionforwhichthelearningorderofatwo-layernetworktrainedunderlargeandsmallinitiallearningratesdeterminesitsgeneralization.Informally consideradistributionovertrainingexamplesconsistingoftwotypesofpatterns(“pattern”referstoagroupingoffeatures).Theﬁrsttypeconsistsofasetofeasy-to-generalize(i.e. discrete)patternsoflowcardinalitythatisdifﬁculttoﬁtusingalow-complexityclassiﬁer buteasilylearnableviacomplexclassiﬁerssuchasneuralnetworks.Thesecondtypeofpatternwillbelearnablebyalow-complexityclassiﬁer butareinherentlynoisysoitisdifﬁcultfortheclassiﬁertogeneralize.Inourcase thesecondtypeofpatternrequiresmoresamplestocorrectlylearnthantheﬁrsttype.Supposewehavethefollowingsplitofexamplesinourdataset:20%containingonlyeasy-to-generalizeandhard-to-ﬁtpatterns20%containingonlyhard-to-generalizeandeasy-to-ﬁtpatterns60%containingbothpatterntypes(1.1)Thefollowinginformaltheoremscharacterizethelearningorderandgeneralizationofthelargeandsmallinitiallearningratemodels.TheyareadramaticsimpliﬁcationofourTheo-rems3.4and3.5meantonlytohighlighttheintuitionsbehindourresults.Theorem1.1(Informal largeinitialLR+anneal).ThereisadatasetwithsizeNoftheform(1.1)suchthatwithalargeinitiallearningrateandnoisygradientupdates atwolayernetworkwill:1)initiallyonlylearnhard-to-generalize easy-to-ﬁtpatternsfromthe0.8Nexamplescontainingsuchpatterns.2)learneasy-to-generalize hard-to-ﬁtpatternsonlyafterthelearningrateisannealed.Thus themodellearnshard-to-generalize easilyﬁtpatternswithaneffectivesamplesizeof0.8Nandstilllearnsalleasy-to-generalize hardtoﬁtpatternscorrectlywith0.2Nsamples.Theorem1.2(Informal smallinitialLR).Inthesamesettingasabove withsmallinitiallearningratethenetworkwill:1)quicklylearnalleasy-to-generalize hard-to-ﬁtpatterns.2)ignorehard-to-generalize easilyﬁtpatternsfromthe0.6Nexamplescontainingbothpatterntypes andonlylearnthemfromthe0.2Nexamplescontainingonlyhard-to-generalizepatterns.Thus themodellearnshard-to-generalize easilyﬁtpatternswithasmallereffectivesamplesizeof0.2Nandwillperformrelativelyworseonthesepatternsattesttime.Together thesetwotheoremscanjustifythephenomenonobservedinFigure1asfollows:inareal-worldnetwork thelargelearningratemodelﬁrstlearnshard-to-generalize easier-to-ﬁtpatternsandisunabletomemorizeeasy-to-generalize hard-to-ﬁtpatterns leadingtoaplateauinaccuracy.Oncethelearningrateisannealed itisabletoﬁtthesepatterns explainingthesuddenspikeinbothtrainandtestaccuracy.Ontheotherhand becauseofthelowamountofSGDnoisepresentineasy-to-generalize hard-to-ﬁtpatterns thesmalllearningratemodelquicklyoverﬁtstothembeforefullylearningthehard-to-generalizepatterns resultinginpoortesterroronthelattertypeofpattern.2Bothintuitivelyandinouranalysis thenon-convexityofneuralnetsiscrucialforthelearning-ordereffecttooccur.Stronglyconvexproblemshaveauniqueminimum sowhathappensduringtrainingdoesnotaffecttheﬁnalresult.Ontheotherhand weshowthenon-convexitycausesthelearningordertohighlyinﬂuencethecharacteristicsofthesolutionsfoundbythealgorithm.InSectionF.1 weproposeamitigationstrategyinspiredbyouranalysis.InthesamesettingasTheorems1.1and1.2 weconsidertrainingamodelwithsmallinitiallearningratewhileaddingnoisebeforetheactivationswhichgetsreducedbysomeconstantfactoratsomeparticularepochintraining.Weshowthatthisalgorithmprovidesthesametheoreticalguaranteesasthelargeinitiallearningrate andweempiricallydemonstratetheeffectivenessofthisstrategyinSection6.InSection6wealsoempiricallyvalidateTheorems1.1and1.2byaddinganartiﬁcialmemorizablepatchtoCIFAR-10images inamannerinspiredby(1.1).1.1RelatedWorkThequestionoftrainingwithlargerbatchsizesiscloselytiedwithlearningrate andmanypapershaveempiricallystudiedlargebatch/smallLRphenomena[22 18 35 34 11 41 16 38] particularlyfocusingonvisiontasksusingSGDastheoptimizer.1Keskaretal.[22]arguethattrainingwithalargebatchsizeorsmalllearningrateresultsinsharplocalminima.Hofferetal.[18]proposetrainingthenetworkforlongerandwithlargerlearningrateasawaytotrainwithalargerbatchsize.Wenetal.[38]proposeaddingFishernoisetosimulatetheregularizationeffectofsmallbatchsize.Adaptivegradientmethodsareapopularmethodfordeeplearning[14 43 37 23 29]thatadaptivelychoosedifferentstepsizesfordifferentparameters.Onemotivationforthesemethodsisreducingtheneedtotunelearningrates[43 29].However thesemethodshavebeenobservedtohurtgeneralizationperformance[21 10] andmodernarchitecturesoftenachievethebestresultsviaSGDandhand-tunedlearningrates[17 42].Wilsonetal.[39]constructatoyexampleforwhichADAM[23]generalizesprovablyworsethanSGD.Additionally thereareseveralalternativelearningrateschedulesproposedforSGD suchaswarm-restarts[28]and[33].Geetal.[15]analyzetheexponentiallydecayinglearningrateandshowthatitsﬁnaliterateachievesoptimalerrorinstochasticoptimizationsettings buttheyonlyanalyzeconvexsettings.Therearealsoseveralrecentworksonimplicitregularizationofgradientdescentthatestablishconvergencetosomeidealizedsolutionunderparticularchoicesoflearningrate[27 36 1 7 26].Incontrasttoouranalysis thegeneralizationguaranteesfromtheseworkswoulddependonlyonthecomplexityoftheﬁnaloutputandnotontheorderoflearning.Otherrecentpapershavealsostudiedtheorderinwhichdeepnetworkslearncertaintypesofexamples.MangalamandPrabhu[30]andNakkiranetal.[31]experimentallydemonstratethatdeepnetworksmayﬁrstﬁtexampleslearnableby“simpler”classiﬁers.Forourconstruction weprovethattheneuralnetwithlargelearningratefollowsthisbehavior initiallylearningaclassiﬁeronlinearlyseparableexamplesandlearningtheremainingexamplesafterannealing.However thephenomenonthatweanalyzeisalsomorenuanced:withasmalllearningrate weprovethatthemodelﬁrstlearnsacomplexclassiﬁeronlow-noiseexampleswhicharenotlinearlyseparable.Finally ourprooftechniquesandintuitionsarerelatedtorecentliteratureonglobalconvergenceofgradientdescentforover-parametrizednetworks[6 12 13 1 5 7 4 26 2].Theseworksshowthatgradientdescentlearnsaﬁxedkernelrelatedtotheinitializationundersufﬁcientover-parameterization.Inouranalysis theunderlyingkernelischangingovertime.TheamountofnoiseduetoSGDgovernsthespaceofpossiblelearnedkernels andasaresult regularizestheorderoflearning.2SetupandNotationsDatadistribution.Weformallyintroduceourdatadistribution whichcontainsexamplessupportedontwotypesofcomponents:aPcomponentmeanttomodelhard-to-generalize easier-to-ﬁtpatterns andaQcomponentmeanttomodeleasy-to-generalize hard-to-ﬁtpatterns(seethediscussioninourintroduction).Formally weassumethatthelabelyhasauniformdistributionover{−1 1} andthe1Whilethesepapersareframedasastudyoflarge-batchtraining anumberofthemexplicitlyacknowledgetheconnectionbetweenlargebatchsizeandsmalllearningrate.3Figure2:Avisualizationofthevectorsz z−ζ andz+ζusedtodeﬁnethedistributionQin2dimensions.z±ζwillhavelabel−1andzhaslabel+1.Notethatthenormofζismuchsmallerthanthenormofz.dataxisgeneratedasConditionedonthelabely(2.1)withprobabilityp0 x1∼Py andx2=0(2.2)withprobabilityq0 x1=0 andx2∼Qy(2.3)withprobability1−p0−q0 x1∼Py andx2∼Qy(2.4)whereP−1 P1areassumedtobetwohalfGaussiandistributionswithamarginγ0betweenthem:x1∼P1⇔x1=γ0w?+z|hw? zi≥0 wherez∼N(0 Id×d/d)x1∼P−1⇔x1=−γ0w?+z|hw? zi≤0 wherez∼N(0 Id×d/d)Therefore weseethatwhenx1ispresent thelinearclassiﬁersign(w?>x1)canclassifytheexamplecorrectlywithamarginofγ0.Tosimplifythenotation weassumethatγ0=1/√dandw?∈Rdhasaunit‘2norm.Intuitively Pislinearlyseparable thuslearnablebylowcomplexity(e.g.linear)classiﬁers.However becauseofthedimensionality Phashighnoiseandrequiresarelativelylargesamplecomplexitytolearn.ThedistributionQ−1andQ1aresupportedonlyonthreedistinctdirectionsz−ζ zandz+ζwithsomerandomscalingα andarethuslow-noiseandmemorizable.Concretely z−ζandz+ζhavenegativelabelsandzhaspositivelabels.x2∼Q1⇔x2=αzwithα∼[0 1]uniformlyx2∼Q−1⇔x2=α(z+bζ)withα∼[0 1] b∼{−1 1}uniformly(2.5)Hereforsimplicity wetakeztobeaunitvectorinRd.Weassumeζ∈Rdhasnormkζk2=randhz ζi=0.Wewillassumer(cid:28)1sothatz+ζ z z−ζarefairlyclosetoeachother.Wedepictz−ζ z z+ζinFigure2.WechoosethistypeofQtobetheeasy-to-generalize hard-to-ﬁtpattern.Notethatzisnotlinearlyseparablefromz+ζ z−ζ sonon-linearityisnecessarytolearnQ.Ontheotherhand itisalsoeasyforhigh-complexitymodelssuchasneuralnetworkstomemorizeQwithrelativelysmallsamplecomplexity.MemorizingQwithatwo-layernet.Itiseasyforatwo-layerrelunetworktomemorizethelabelsofx2usingtwoneuronswithweightsw vsuchthathw zi<0 hw z−ζi>0anhv zi<0 hv z+ζi>0.Inparticular wecanverifythat−hw x2i+−hv x2i+willoutputanegativevalueforx2∈{z−ζ z+ζ}andazerovalueforx2=z.Thuschoosingasmallenoughρ>0 theclassiﬁer−hw x2i+−hv x2i++ρgivesthecorrectsignforthelabely.WeassumethatwehaveatrainingdatasetwithNexamples{(x(1) y(1)) ··· (x(N) y(N))}drawni.i.dfromthedistributiondescribedabove.Weusepandqtodenotetheempiricalfractionofdatapointsthataredrawnfromequation(2.2)and(2.3).Two-layerneuralnetworkmodel.Wewilluseatwo-layerneuralnetworkwithreluactivationtolearnthedatadistributiondescribedabove.TheﬁrstlayerweightsaredenotedbyU∈Rm×2dandthesecondlayerweightisdenotedbyu∈Rm.Withreluactivation theoutputoftheneuralnetworkisu>(1(Ux)(cid:12)Ux)where(cid:12)denotestheelement-wisedotproductoftwovectorsand1(z)isthebinaryvectorthatcontains1(zi≥0)asentries.ItturnsoutthatwewilloftenbeconcernedwiththeobjectthatdisentanglesthetwooccurrencesofUintheformulau>(1(Ux)(cid:12)Ux).Wedeﬁnethefollowingnotationtofacilitatethereferencetosuchanobject.LetNA(u U;x) w>(1(Ax)(cid:12)Ux)(2.6)Thatis NA(w W;x)denotesthefunctionwherewecomputetheactivationpattern1(Ax)bythematrixAinsteadofU.Whenuisclearfromthecontext withslightabuseofnotation wewriteNA(U;x) u>(1(Ax)(cid:12)Ux).Inthisnotation ourmodelisdeﬁnedasf(u U;x)=NU(u U;x).WeconsiderseveraldifferentstructuresregardingtheweightmatricesU.Thesimplestversion4whichweconsiderinthemainbodyofthispaperisthatUcanbedecomposedintotwoU=(cid:20)WV(cid:21)whereWonlyoperatesontheﬁrstdcoordinates(thatis thelastdcolumnsofWarezero) andVonlyoperatesonthelastdcoordinates(thosecoordinatesofx2.)NotethatWoperatesonthePcomponentofexamples andVoperatesontheQcomponentofexamples.Inthiscase themodelcanbedecomposedintof(u U;x)=NU(u U;x)=NW(w W;x)+NV(v V;x)=NW(w W;x1)+NV(v V;x2)HereweslightlyabusethenotationtouseWtodenotebothamatrixof2dcolumnswithlastdcolumnsbeingzero oramatrixofdcolumns.WealsoextendourtheoremtootherUsuchasatwolayerconvolutionnetworkinSectionF.Trainingobjective.Let‘(f;(x y))bethelossoftheexample(x y)undermodelf.Throughoutthepaperweusethelogisticloss‘(f;(x y))=−log11+e−yf(x).WeusethestandardtraininglossfunctionbLdeﬁnedas:bL(u U)=1NPi∈[N]‘(cid:0)f(u U;·);(x(i) y(i))(cid:1)andletbLS(u U)denotetheaverageoversomesubsetSofexamplesinsteadoftheentiredataset.WeconsideraregularizedtrainingobjectivebLλ(u U)=bL(u U)+λ2kUk2F.Forthesimplicityofderivation thesecondlayerweightvectoruisrandominitializedandﬁxedthroughoutthispaper.ThuswithslightabuseofnotationthetrainingobjectivecanbewrittenasbLλ(U)=bL(u U)+λ2kUk2F.Notations.Herewecollectadditionalnotationsthatwillbeusefulthroughoutourproofs.Thesymbol⊕willrefertothesymmetricdifferenceoftwosetsortwobinaryvectors.Thesymbol\referstothesetdifference.LetusdeﬁneM1tobethesetofalli∈[N]suchthatx(i)16=0 let¯M1=[N]\M1.LetM2tobethesetofalli∈[N]suchthatx(i)26=0 let¯M2=[N]\M2.Wedeﬁneq=|¯M1|Nandp=|¯M2|NtobetheempiricalfractionofdatacontainingpatternsonlyfromQandP respectively.WewillsometimesusebEtodenoteanempiricalexpectationoverthetrainingsamples.Foravectorormatrixv weusesupp(v)todenotethesetofindicesofthenon-zeroentriesofv.ForU∈Rm×dandR⊂[m] letURbetherestrictionofUtothesubsetofrowsindexedbyR.Weuse[U]itodenotethei-throwofUasarowvectorinR1×d.Letthesymbol(cid:12)denotetheelement-wiseproductbetweentwovectorsormatrices.ThenotationIn×nwilldenotethen×nidentitymatrix and1theall1’svectorwheredimensionwillbeclearfromcontext.Wedeﬁne“withhighprobability”tomeanwithprobabilityatleast1−e−Clog2(d)forasufﬁcientlylargeconstantC.˜O ˜Ωwillbeusedtohidepolylogfactorsofd.3MainResultsThetrainingalgorithmthatweconsiderisstochasticgradientdescentwithsphericalGaussiannoise.WeremarkthatweanalyzethisalgorithmasasimpliﬁcationoftheminibatchSGDnoiseencounteredwhentrainingreal-worldnetworks.Thereareanumberofworkstheoreticallycharacterizingthisparticularnoisedistribution[19 18 38] andweleaveanalysisofthissettingtofuturework.WeinitializeU0tohavei.i.d.entriesfromaGaussiandistributionwithvarianceτ20 andateachiterationofgradientdescentweaddsphericalGaussiannoisewithcoordinate-wisevarianceτ2ξtothegradientupdates.Thatis thelearningalgorithmforthemodelisU0∼N(0 τ20Im×m⊗Id×d)Ut+1=Ut−γt∇U(bLλ(u Ut)+ξt)=(1−γtλ)Ut−γt(∇UbL(u Ut)+ξt)(3.1)whereξt∼N(0 τ2ξIm×m⊗Id×d)(3.2)whereγtdenotesthelearningrateattimet.Wewillanalyzetwoalgorithms:Algorithm1(L-S):Thelearningrateisη1fort0iterationsuntilthetraininglossdropsbelowthethresholdε1+qlog2.Thenweannealthelearningratetoγt=η2(whichisassumedtobemuchsmallerthanη1)andrununtilthetraininglossdropstoε2.Algorithm2(S):Weusedaﬁxedlearningrateofη2andstopattraininglossε02≤ε2.5Fortheconvenienceoftheanalysis wemakethefollowingassumptionthatwechooseτ0inawaysuchthatthecontributionofthenoisesinthesystemstabilizeattheinitialization:2Assumption3.1.Afterﬁxingλandτξ wechooseinitializationτ0andlargelearningrateη1sothat(1−η1λ)2τ20+η21τ2ξ=τ20(3.3)Asatechnicalassumptionforourproofs wewillalsorequireη1.ε1.Wealsorequiresufﬁcientover-parametrization.Assumption3.2(Over-parameterization).Weassumethroughoutthepaperthatτ0=1/poly(cid:0)dε(cid:1)andm≥poly(cid:16)dετ0(cid:17)wherepolyisasufﬁcientlylargeconstantdegreepolynomial.Wenotethatwecanchooseτ0arbitrarilysmall solongasitisﬁxedbeforewechoosem.Aswewillseesoon thepreciserelationbetweenN dimpliesthatthelevelofover-parameterizationispolynomialinN  whichﬁtswiththeconditionsassumedinpriorworks suchas[26 13].Assumption3.3.Throughoutthispaper weassumethefollowingdependenciesbetweentheparam-eters.WeassumethatN d→∞witharelationshipNd=1κ2whereκ∈(0 1)isasmallvalue.3Wesetr=d−3/4 p0=κ2/2 andq0=Θ(1).Theregularizerwillbechosentobeλ=d−5/4.Allofthesechoicesofhyper-parameterscanberelaxed butforsimplicityofexpositionweonlyworkthissetting.Wenotethatunderourassumptions forsufﬁcientlylargeN p≈p0andq≈q0uptoconstantmultiplicativefactors.Thuswewillmostlyworkwithpandq(theempiricalfractions)intherestofthepaper.Wealsonotethatourparameterchoicesatisﬁes(rd)−1 dλ λ/r≤κO(1)andλ≤r2/(κ2q3p2) whichareafewconditionsthatwefrequentlyuseinthetechnicalpartofthepaper.NowwepresentourmaintheoremsregardingthegeneralizationofmodelstrainedwiththeL-SandSalgorithms.TheﬁnalgeneralizationerrorofthemodeltrainedwiththeL-SalgorithmwillendupafactorO(κ)=O(p1/2)smallerthanthegeneralizationerrorofthemodeltrainedwithSalgorithm.Theorem3.4(AnalysisofAlgorithmL-S).UnderAssumption3.1 3.2 and3.3 thereexistsauniversalconstant0<c<1/16suchthatAlgorithm1(L-S)withannealingatlossε1+qlog2forε1∈(cid:0)d−c κ2p2q3(cid:1)andstoppingcriterionε2=pε1/qsatisﬁesthefollowing:1.ItannealsthelearningratewithineO(cid:16)dη1ε1(cid:17)iterations.2.Itstopsatatmostt=eO(cid:16)dη1ε1+1η2rε31(cid:17).Withprobabilityatleast0.99 thesolutionUthastest(classiﬁcation)errorandtestlossatmostO(cid:16)pκlog1ε1(cid:17).Roughly thelearningorderandgeneralizationoftheL-Smodelisasfollows:beforeannealingthelearningrate themodelonlylearnsaneffectiveclassiﬁerforPonthe≈(1−q)NsamplesinM1asthelargelearningratecreatestoomuchnoisetoeffectivelylearnQ(Lemma4.1andLemma4.2).Afterthelearningrateisannealed themodelmemorizesQandcorrectlyclassiﬁesexampleswithonlyaQcomponentduringtesttime(formallyshowninLemmas4.3and4.4).ForexampleswithonlyPcomponent thegeneralizationerroris(ignoringlogfactorsandothertechnicalities)pqdN=O(pκ)viastandardRademachercomplexity.ThefullanalysisoftheL-SalgorithmisclariﬁedinSection4.Theorem3.5(LowerboundforAlgorithmS).Letε2bechoseninTheorem3.4.UnderAssump-tion3.1 3.2and3.3 thereexistsauniversalconstantc>0suchthatw.h.p Algorithm2withanyη2≤η1d−candanystoppingcriterionε02∈(d−c ε2] achievestraininglossε02inatmosteO(cid:16)dη2ε02(cid:17)iterations andboththetesterrorandthetestlossoftheobtainedsolutionareatleastΩ(p).2Letτ00bethesolutionto(3.3)holdingτξ η1 λﬁxed.Ifthestandarddeviationoftheinitializationischosentobesmallerthanτ00 thenstandarddeviationofthenoisewillgrowtoτ00.Otherwiseiftheinitializationischosentobelarger thecontributionofthenoisewilldecreasetothelevelofτ00duetoregularization.IntypicalanalysisofSGDwithsphericalnoises oftenaslongaseitherthenoiseorthelearningrateissmallenough theproofgoesthrough.However herewewillmakeexplicituseofthelargelearningrateorthelargenoisetoshowbettergeneralizationperformance.3Orinanon-asymptoticlanguage weassumethatN daresufﬁcientlylargecomparedtoκ:N d(cid:29)poly(κ)6Weexplainthislowerboundasfollows:theSalgorithmwillquicklymemorizetheQcomponentwhichislownoiseandignorethePcomponentforthe≈1−p−qexampleswithbothPandQcomponents(showninLemma5.2).Thus itonlylearnsPon≈pNexamples.ItobtainsasmallmarginontheseexamplesandthereforemisclassiﬁesaconstantfractionofP-onlyexamplesattesttime.ThisresultsinthelowerboundofΩ(p).WeformalizetheanalysisinSection5.DecouplingtheIterates.ItwillbefruitfulforouranalysistoseparatelyconsiderthegradientsignalandGaussiannoisecomponentsoftheweightmatrixUt.WewilldecomposetheweightmatrixUtasfollows:Ut=Ut+eUt.Inthisformula Utdenotesthesignalsfromallthegradientupdatesaccumulatedovertime andeUtreferstothenoiseaccumulatedovertime:Ut=−tXs=1γs−1 t−1Yi=s(1−γiλ)!∇bL(Us−1)eUt= t−1Yi=0(1−γiλ)!U0−tXs=1γs−1 t−1Yi=s(1−γiλ)!ξs−1(3.4)Notethatwhenthelearningrateγtisalwaysη theformulasimpliﬁestoUt=Pts=1η(1−ηλ)t−s∇bL(Us−1)andeUt=(1−ηλ)tU0+Pts=1η(1−ηλ)t−sξs−1.Thedecouplingandourparticularchoiceofinitializationsatisﬁesthatthenoiseupdatesinthesystemstabilizeatinitialization sothemarginaldistributionofeUtisalwaysthesameastheinitialization.Anotherniceaspectofthesignal-noisedecompositionisasfollows:weusetoolsfrom[6]toshowthatifthesignaltermUissmall thenusingonlythenoisecomponenteUtocomputetheactivationsroughlypreservestheoutputofthenetwork.Thisfacilitatesouranalysisofthenetworkdynamics.SeeSectionA.1forfulldetails.DecompositionofNetworkOutputs.Forconvenience wewillexplicitlydecomposethemodelpredictionateachtimeintotwocomponents eachofwhichoperatesononepattern:wehaveNUt(u Ut;x)=gt(x)+rt(x) wheregt(x)=gt(x2) NVt(v Vt;x)=NVt(v Vt;x2)(3.5)rt(x)=rt(x1) NWt(w Wt;x)=NWt(w Wt;x1)(3.6)Inotherwords thenetworkgtactsontheQcomponentofexamples andthenetworkrtactsonthePcomponentofexamples.4CharacterizationofAlgorithm1(L-S)WecharacterizethebehaviorofalgorithmL-Swithlargeinitiallearningrate.WeprovideproofsketchesinSectionB.1withfullproofsinSectionD.PhaseI:initiallearningrateη1.Thefollowinglemmaboundstherateofconvergencetothepointwherethelossgetsannealed.Italsoboundsthetotalgradientsignalaccumulatedbythispoint.Lemma4.1.InthesettingofTheorem3.4 atsometimestept0≤eO(cid:16)dη1ε1(cid:17) thetraininglossbL(Ut0)becomessmallerthanqlog2+1.Moreover wehavekUt0k2F=O(cid:16)dlog21ε1(cid:17).OurproofofLemma4.1viewstheSGDdynamicsasoptimizationwithrespecttotheneuraltangentkernelinducedbytheactivationpatternswherethekernelisrapidlychangingduetothenoisetermsξ.ThisisincontrasttothestandardNTKregime wheretheactivationpatternsareassumedtobestable[13 26].OuranalysisextendstheNTKtechniquestodealwithasequenceofchangingkernelswhichshareacommonoptimalclassiﬁer(seeSectionB.1andTheoremB.2foradditionaldetails).Thenextlemmasaysthatwithlargeinitiallearningrate thefunctiongtdoesnotlearnanythingmeaningfulfortheQcomponentbeforethe1η1λ-timestep.Notethatbyourchoiceofparameters1/λ(cid:29)dandLemma4.1 weannealatthetimestepeO(cid:16)dη1ε1(cid:17)≤1η1λ.Therefore thefunctionhasnotlearnedanythingmeaningfulaboutthememorizablepatternondistributionQbeforeweanneal.Lemma4.2.InthesettingofTheorem3.4 w.h.p. foreveryt≤1η1λ |gt(z+ζ)+gt(z−ζ)−2gt(z)|≤eO(cid:18)r2λ(cid:19)=eO(d−1/4)(4.1)7PhaseII:afterannealingthelearningratetoη2.Afteriterationt0 wedecreasethelearningratetoη2.Thefollowinglemmaboundshowfastthelossconvergesafterannealing.Lemma4.3.InthesettingofTheorem3.4 thereexistst=eO(cid:16)1ε31η2r(cid:17) suchthataftert0+titerations wehavethatbL(Ut)=O(cid:16)pε1/q(cid:17)Moreover kUt0+t−Ut0k2F≤eO(cid:16)1ε21r(cid:17)≤O(d).ThefollowinglemmaboundsthetraininglossontheexamplesubsetsM1 ¯M1.Lemma4.4.InthesettingofLemma4.3usingthesamet=eO(cid:16)1ε31η2r(cid:17) theaveragetraininglossesonthesubsetsM1and¯M1arebothgoodinthesensethatbLM1(rt0+t)=O(pε1/q)andbL¯M1(gt0+t)=O(pε1/q3)(4.2)Intuitively lowtraininglossofgt0+ton¯M1immediatelyimpliesgoodgeneralizationonexamplescontainingpatternsfromQ.Meanwhile theclassiﬁerforP rt0+t haslowlosson(1−q)Nexamples.ThenthetesterrorboundfollowsfromstandardRademachercomplexitytoolsappliedtothese(1−q)Nexamples.5CharacterizationofAlgorithm2(S)Wepresentoursmalllearningratelemmas withproofssketchesinSectionB.2andfullproofsinSectionE.Traininglossconvergence.Thebelowlemmashowsthatthealgorithmwillconvergetosmalltrainingerrortooquickly.Inparticular thenormofWtisnotlargeenoughtoproducealargemarginsolutionforthosexsuchthatx2=0.Lemma5.1.InthesettingofTheorem3.5 thereexistsatimet0=˜O(cid:16)1η2ε032r(cid:17)suchthatbLM2(Ut0)≤ε02.Moreover thereexiststwitht=˜O(cid:16)1η2ε032r+Npη2ε02(cid:17)suchthatbL(Ut)≤ε02aftertiterations.Moreover wehavethatkUtk2F≤˜O(cid:16)1ε022r+Np(cid:17).Lowerboundonthegeneralizationerror.ThefollowingimportantlemmastatesthatourclassiﬁerforPdoesnotlearnmuchfromtheexamplesinM2.Intuitively underasmalllearningrate theclassiﬁerwillalreadylearnsoquicklyfromtheQcomponentoftheseexamplesthatitwillnotlearnfromthePcomponentofexamplesinM1∩M2.WemakethisprecisebyshowingthatthemagnitudeofthegradientsonM2issmall.Lemma5.2.Inthesettingoftheorem3.5 letW(2)t=1Nη2Xs≤t(1−η2λ)t−sXi∈M2∇WbL{i}(Us)(5.1)bethe(accumulated)gradientoftheweightW restrictedtothesubsetM2.Then foreveryt=O(d/η2ε02) wehave:(cid:13)(cid:13)(cid:13)W(2)t(cid:13)(cid:13)(cid:13)F≤˜O(cid:0)d15/32/ε022(cid:1).Fornotationsimplicity wewilldeﬁneε3=d−1/321ε022.Then (cid:13)(cid:13)(cid:13)W(2)t(cid:13)(cid:13)(cid:13)F≤˜O(cid:16)√dε3(cid:17).TheabovelemmaimpliesthatWdoesnotlearnmuchfromexamplesinM2 andthereforemustoverﬁttothepNexamplesin¯M2.AspN≤d/2byourchoiceofparameters wewillnothaveenoughsamplestolearnthed-dimensionaldistributionP.ThefollowinglemmaformalizestheintuitionthatthemarginwillbepooronsamplesfromP.Lemma5.3.Thereexistsα∈Rdsuchthatα∈span{x(i)1}i∈¯M2andkαk2=˜Ω(√Np)suchthatw.h.p.overarandomlychosenx1 wehavethatrt(x1)−rt(−x1)=2hα x1i±˜O(ε3)(5.2)Asthemarginispoor thepredictionswillbeheavilyinﬂuencedbynoise.WeusethisintuitiontoprovetheclassiﬁcationlowerboundforTheorem3.5.8Figure3:Accuracyvs.epochonpatch-augmentedCIFAR-10.Thegraylineindicatesannealingofactivationnoiseandlearningrate.Left:Cleanvalidationset.Right:Imagescontainingonlythepatch.6ExperimentsOurtheorysuggeststhataddingnoisetothenetworkcouldbeaneffectivestrategytoregularizeasmalllearningrateinpractice.WetestthisempiricallybyaddingsmallGaussiannoiseduringtrainingbeforeeveryactivationlayerinaWideResNet16[42]architecture asouranalysishighlightspre-activationnoiseasakeyregularizationmechanismofSGD.Thenoiselevelisannealedovertime.WedemonstrateonCIFAR-10imageswithoutdataaugmentationthatthisregularizationcanindeedcounteractthenegativeeffectsofsmalllearningrate aswereporta4.72%increaseinvalidationaccuracywhenaddingnoisetoasmalllearningrate.FulldetailsareinSectionH.1.Wewillalsoempiricallydemonstratethatthechoiceoflargevs.smallinitiallearningratecanindeedinvertthelearningorderofdifferentexampletypes.Weaddamemorizable7×7pixelpatchtoasubsetofCIFAR-10imagesfollowingthescenariopresentedin(1.1) suchthataround20%ofimageshavenopatch 16%ofimagescontainonlyapatch and64%containbothCIFAR-10dataandpatch.Wegeneratethepatchessothattheyarenoteasilyseparable asinourconstructedQ buttheyarelowinvariationandthereforeeasytomemorize.Precisedetailsonproducingthedata includingavisualizationofthepatch areinSectionH.2.WetrainonthemodiﬁeddatasetusingWideResNet16using3methods:largelearningratewithannealingatthe30thepoch smallinitiallearningrate andsmalllearningratewithnoiseannealedatthe30thepoch.Figure3depictsthevalidationaccuracyvs.epochonclean(nopatch)andpatch-onlyimages.Fromtheplots itisapparentthatthesmalllearningratepicksupthesignalinthepatchveryquickly whereastheothertwomethodsonlymemorizethepatchafterannealing.Fromthevalidationaccuracyoncleanimages wecandeducethatthesmalllearningratemethodisindeedlearningtheCIFARimagesusingasmallfractionofalltheavailabledata asthevalidationaccuracyofasmallLRmodelwhentrainingonthefulldatasetisaround83% butthevalidationoncleandataaftertrainingwiththepatchis70%.WeprovideadditionalargumentsinSectionH.2.7ConclusionInthiswork weshowthattheorderinwhichaneuralnetlearnstoﬁtdifferenttypesofpatternsplaysacrucialroleingeneralization.Todemonstratethis weconstructadistributiononwhichmodelstrainedwithlargelearningratesgeneralizeprovablybetterthanthosetrainedwithsmalllearningratesduetolearningorder.OuranalysisrevealsthatmoreSGDnoise orlargerlearningrate biasesthemodeltowardslearning“generalizing”kernelsratherthan“memorizing”kernels.WeconﬁrmonarticiﬁallymodiﬁedCIFAR-10datathatthescaleofthelearningratecanindeedinﬂuencelearningorderandgeneralization.Inspiredbytheseﬁndings weproposeamitigationstrategythatinjectsnoisebeforetheactivationsandworksboththeoreticallyforourconstructionandempirically.Thedesignofbetteralgorithmsforregularizinglearningorderisanexcitingquestionforfuturework.AcknowledgementsCWacknowledgessupportfromaNSFGraduateResearchFellowship.9References[1]ZeyuanAllen-ZhuandYuanzhiLi.CanSGDlearnrecurrentneuralnetworkswithprovablegeneralization?CoRR abs/1902.01028 2019.URLhttp://arxiv.org/abs/1902.01028.[2]ZeyuanAllen-ZhuandYuanzhiLi.Whatcanresnetlearnefﬁciently goingbeyondkernels?CoRR abs/1905.10337 2019.URLhttp://arxiv.org/abs/1905.10337.[3]ZeyuanAllen-Zhu YuanzhiLi andYingyuLiang.LearningandGeneralizationinOverpa-rameterizedNeuralNetworks GoingBeyondTwoLayers.arXivpreprintarXiv:1811.04918 November2018.[4]ZeyuanAllen-Zhu YuanzhiLi andZhaoSong.Ontheconvergencerateoftrainingrecurrentneuralnetworks.arXivpreprintarXiv:1810.12065 2018.[5]ZeyuanAllen-Zhu YuanzhiLi andZhaoSong.Ontheconvergencerateoftrainingrecurrentneuralnetworks.arXivpreprintarXiv:1810.12065 2018.[6]ZeyuanAllen-Zhu YuanzhiLi andZhaoSong.Aconvergencetheoryfordeeplearningviaover-parameterization.arXivpreprintarXiv:1811.03962 November2018.[7]SanjeevArora SimonS.Du WeiHu ZhiyuanLi andRuosongWang.Fine-grainedanalysisofoptimizationandgeneralizationforoverparameterizedtwo-layerneuralnetworks.CoRR abs/1901.08584 2019.URLhttp://arxiv.org/abs/1901.08584.[8]PeterLBartlettandShaharMendelson.Rademacherandgaussiancomplexities:Riskboundsandstructuralresults.JournalofMachineLearningResearch 3(Nov):463–482 2002.[9]OlivierBousquetandAndréElisseeff.Stabilityandgeneralization.Journalofmachinelearningresearch 2(Mar):499–526 2002.[10]JinghuiChenandQuanquanGu.Closingthegeneralizationgapofadaptivegradientmethodsintrainingdeepneuralnetworks.arXivpreprintarXiv:1806.06763 2018.[11]XiaowuDaiandYuhuaZhu.Towardstheoreticalunderstandingoflargebatchtraininginstochasticgradientdescent.arXivpreprintarXiv:1812.00542 2018.[12]SimonS.Du JasonD.Lee YuandongTian BarnabásPóczos andAartiSingh.Gradientdescentlearnsone-hidden-layerCNN:don’tbeafraidofspuriouslocalminima.InInternationalConferenceonMachineLearning(ICML).http://arxiv.org/abs/1712.00779 2018.[13]SimonS.Du XiyuZhai BarnabasPoczos andAartiSingh.GradientDescentProvablyOptimizesOver-parameterizedNeuralNetworks.ArXive-prints 2018.[14]JohnDuchi EladHazan andYoramSinger.Adaptivesubgradientmethodsforonlinelearningandstochasticoptimization.JournalofMachineLearningResearch 12(Jul):2121–2159 2011.[15]RongGe ShamM.Kakade RahulKidambi andPraneethNetrapalli.TheStepDecaySched-ule:ANearOptimal GeometricallyDecayingLearningRateProcedure.arXive-prints art.arXiv:1904.12838 Apr2019.[16]PriyaGoyal PiotrDollár RossGirshick PieterNoordhuis LukaszWesolowski AapoKyrola AndrewTulloch YangqingJia andKaimingHe.Accurate largeminibatchsgd:Trainingimagenetin1hour.arXivpreprintarXiv:1706.02677 2017.[17]KaimingHe XiangyuZhang ShaoqingRen andJianSun.Deepresiduallearningforimagerecognition.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition pages770–778 2016.[18]EladHoffer ItayHubara andDanielSoudry.Trainlonger generalizebetter:closingthegeneralizationgapinlargebatchtrainingofneuralnetworks.InAdvancesinNeuralInformationProcessingSystems pages1731–1741 2017.[19]WenqingHu ChrisJunchiLi LeiLi andJian-GuoLiu.Onthediffusionapproximationofnonconvexstochasticgradientdescent.arXivpreprintarXiv:1705.07562 2017.10[20]StanisławJastrz˛ebski ZacharyKenton NicolasBallas AsjaFischer YoshuaBengio andAmosStorkey.Dnn’ssharpestdirectionsalongthesgdtrajectory.arXivpreprintarXiv:1807.05031 2018.[21]NitishShirishKeskarandRichardSocher.Improvinggeneralizationperformancebyswitchingfromadamtosgd.arXivpreprintarXiv:1712.07628 2017.[22]NitishShirishKeskar DheevatsaMudigere JorgeNocedal MikhailSmelyanskiy andPingTakPeterTang.Onlarge-batchtrainingfordeeplearning:Generalizationgapandsharpminima.arXivpreprintarXiv:1609.04836 2016.[23]DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980 2014.[24]RobertKleinberg YuanzhiLi andYangYuan.Analternativeview:WhendoesSGDescapelocalminima?CoRR abs/1802.06175 2018.URLhttp://arxiv.org/abs/1802.06175.[25]AlexKrizhevsky IlyaSutskever andGeoffreyEHinton.Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems pages1097–1105 2012.[26]YuanzhiLiandYingyuLiang.Learningoverparameterizedneuralnetworksviastochasticgradientdescentonstructureddata.InAdvancesinNeuralInformationProcessingSystems pages8157–8166 2018.[27]YuanzhiLi TengyuMa andHongyangZhang.Algorithmicregularizationinover-parameterizedmatrixrecovery.CoRR abs/1712.09203 2017.URLhttp://arxiv.org/abs/1712.09203.[28]IlyaLoshchilovandFrankHutter.Sgdr:Stochasticgradientdescentwithwarmrestarts.arXivpreprintarXiv:1608.03983 2016.[29]LiangchenLuo YuanhaoXiong YanLiu andXuSun.Adaptivegradientmethodswithdynamicboundoflearningrate.arXivpreprintarXiv:1902.09843 2019.[30]KarttikeyaMangalamandVinayPrabhu.Dodeepneuralnetworkslearnshallowlearnableexamplesﬁrst?June2019.[31]PreetumNakkiran GalKaplun DimitrisKalimeris TristanYang BenjaminL.Edelman FredZhang andBoazBarak.SGDonNeuralNetworksLearnsFunctionsofIncreasingComplexity.arXive-prints art.arXiv:1905.11604 May2019.[32]KarenSimonyanandAndrewZisserman.Verydeepconvolutionalnetworksforlarge-scaleimagerecognition.arXivpreprintarXiv:1409.1556 2014.[33]LeslieNSmith.Cyclicallearningratesfortrainingneuralnetworks.In2017IEEEWinterConferenceonApplicationsofComputerVision(WACV) pages464–472.IEEE 2017.[34]SamuelLSmithandQuocVLe.Abayesianperspectiveongeneralizationandstochasticgradientdescent.arXivpreprintarXiv:1710.06451 2017.[35]SamuelLSmith Pieter-JanKindermans ChrisYing andQuocVLe.Don’tdecaythelearningrate increasethebatchsize.arXivpreprintarXiv:1711.00489 2017.[36]DanielSoudry EladHoffer MorShpigelNacson SuriyaGunasekar andNathanSrebro.Theimplicitbiasofgradientdescentonseparabledata.TheJournalofMachineLearningResearch 19(1):2822–2878 2018.[37]TijmenTielemanandGeoffreyHinton.Lecture6.5-rmsprop coursera:Neuralnetworksformachinelearning.UniversityofToronto TechnicalReport 2012.[38]YemingWen KevinLuk MaximeGazeau GuodongZhang HarrisChan andJimmyBa.Interplaybetweenoptimizationandgeneralizationofstochasticgradientdescentwithcovariancenoise.arXivpreprintarXiv:1902.08234 2019.11[39]AshiaCWilson RebeccaRoelofs MitchellStern NatiSrebro andBenjaminRecht.Themarginalvalueofadaptivegradientmethodsinmachinelearning.InAdvancesinNeuralInformationProcessingSystems pages4148–4158 2017.[40]ChenXing DevanshArpit ChristosTsirigotis andYoshuaBengio.Awalkwithsgd.arXivpreprintarXiv:1802.08770 2018.[41]YangYou IgorGitman andBorisGinsburg.Largebatchtrainingofconvolutionalnetworks.arXivpreprintarXiv:1708.03888 2017.[42]SergeyZagoruykoandNikosKomodakis.Wideresidualnetworks.arXivpreprintarXiv:1605.07146 2016.[43]MatthewDZeiler.Adadelta:anadaptivelearningratemethod.arXivpreprintarXiv:1212.5701 2012.12,Yuanzhi Li
Colin Wei
Tengyu Ma