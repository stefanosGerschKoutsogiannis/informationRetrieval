2019,Modelling the Dynamics of Multiagent Q-Learning in Repeated Symmetric Games: a Mean Field Theoretic Approach,Modelling the dynamics of multi-agent learning has long been an important research topic  but all of the previous works focus on 2-agent settings and mostly use evolutionary game theoretic approaches. In this paper  we study an n-agent setting with n tends to infinity  such that agents learn their policies concurrently over repeated symmetric bimatrix games with some other agents. Using mean field theory  we approximate the effects of other agents on a single agent by an averaged effect. A Fokker-Planck equation that describes the evolution of the probability distribution of Q-values in the agent population is derived. To the best of our knowledge  this is the first time to show the Q-learning dynamics under an n-agent setting can be described by a system of only three equations. We validate our model through comparisons with agent-based simulations on typical symmetric bimatrix games and different initial settings of Q-values.,Modelling the Dynamics of Multiagent Q-Learning in
Repeated Symmetric Games: a Mean Field Theoretic

Approach

Shuyue Hu  Chin-Wing Leung  Ho-fung Leung

The Chinese University of Hong Kong  Hong Kong  China

{syhu cwleung lhf}@cse.cuhk.edu.hk

Abstract

Modelling the dynamics of multi-agent learning has long been an important re-
search topic  but all of the previous works focus on 2-agent settings and mostly
use evolutionary game theoretic approaches. In this paper  we study an n-agent
setting with n tends to inﬁnity  such that agents learn their policies concurrently
over repeated symmetric bimatrix games with some other agents. Using the mean
ﬁeld theory  we approximate the effects of other agents on a single agent by an
averaged effect. A Fokker-Planck equation that describes the evolution of the
probability distribution of Q-values in the agent population is derived. To the best
of our knowledge  this is the ﬁrst time to show the Q-learning dynamics under an
n-agent setting can be described by a system of only three equations. We validate
our model through comparisons with agent-based simulations on typical symmetric
bimatrix games and different initial settings of Q-values.

1

Introduction

A multi-agent system concerns a set of autonomous agents interacting in a shared environment.
Learning in multi-agent systems has recently attracted much attention [3  13  15]  since multi-agent
systems ﬁnd application in a wide variety of domains  such as trafﬁc control [1]  energy management
[20]  robotic coordination [19]  and distributed sensing [16]. While single-agent reinforcement
learning has acquired a strong theoretical foundation [26]  there is a lack of a thorough understanding
of reinforcement learning under multi-agent settings [2]. Shoham [24] calls for more grounded
research in this area rather than designing arbitrary learning strategies that result in convergence to a
certain solution concept. Bloembergen et al. [2] point out that the modelling of multi-agent learning
dynamics may facilitate parameter tuning  systematic comparison of different learning algorithms 
and shedding light into the design of new learning algorithms.
Tuyls et al. [27  29] model the dynamics of Q-learning with Boltzmann exploration in repeated
2-player bimatrix games using a evolutionary game theoretic approach. They derive a differential
equation for each of the row and column player  and show that the learning process of each player can
be understood as the replicator dynamics of a strategy change in an inﬁnitely large agent population.
Extensions have been made to study the dynamics of other learning algorithms  such as FAQ-learning
[10]  lenient FAQ-learning [18]  gradient ascent [9] and regret minimization [12]  in a similar manner.
Gomes and Kowalczyk [22] construct a continuous time model for Q-learning  but focus on how
another exploration strategy  -greedy  affects the expected behaviours of agents. Wunder et al. [32]
use dynamical system methods to study an idealization of Q-learning with -greedy in repeated
2-player general-sum games. They show that the use of this learning method in certain subclasses of
general-sum games induces chaotic behaviour for some initial conditions.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In general  all of the aforementioned works focus on the dynamics of reinforcement learning under
2-agent settings. Many real-life multi-agent systems  however  involve a much greater number of
agents by nature. In this paper  we consider an n-agent setting with n tends to inﬁnity  such that 
concurrently  agents learn their policies over repeated 2-player symmetric bimatrix games with some
other agents in the population. The opponents that an agent interacts with will change from time to
time. Thus  instead of learning against some ﬁxed opponents  an agent learns to play with a wide
range of socially changeable opponents. We note that this scenario  which has not been considered in
the literature before  is a typical setting in norm emergence research [23].
One major difﬁculty of modelling multi-agent learning dynamics is to cope with non-stationarity 
i.e.  the fact that the interactions of agents leads to a highly dynamic shared environment [2  28  22].
One can expect that this non-stationarity will drastically increase as the total number of agents
increases. This makes directly applying previous models in aforementioned works to n-agent settings
inappropriate  because  in principle  the number of equations required to model the entire population
dynamics is proportional to the number of agents in the population. As n tends to inﬁnity  analyzing
or solving this system of equations becomes practically infeasible. We ﬁnd the mean ﬁeld theory
[31] in statistical mechanics sheds light on this kind of problems. According to this theory  all of
the effects of neighboring particles impose on a single particle can be approximated by an averaged
effect—mean ﬁeld—on that particle. This consequently reduces the degrees of freedom of the
problem  and may make the problem analytically solvable.
Here  we assume agents use Q-learning with Boltzmann exploration. Using the mean ﬁeld theory 
we approximate the effects of other agents on a single agent with an averaged effect  such that one
can conceive each agent in effect learns its policy over repeated interactions with a ﬁctitious agent
using the mean policy of the population. The Q-learning processes of individual agents will change
the environment shared by all the agents. To capture this effect  we derive a Fokker-Planck equation
that describes how the distribution of Q-values of the entire population evolves as time goes forward.
We show under the n-agent setting we consider  the population dynamics can be modelled by a
system of only three equations. For validation  we compare the behaviours obtained by our mean ﬁeld
theoretic model with the behaviours found in agent-based simulations. The comparison indicates our
model well describes the qualitatively different patterns of evolution resulting from different types of
symmetric bimatrix games and different initial settings of Q-values.
It is interesting to note that there is another line of research [17  25  33] on reinforcement learning
in mean-ﬁeld games [8  14]. In this line of research  novel learning algorithms that converge to
certain solution concepts (e.g.  Nash equilibria) in mean-ﬁeld games are proposed  however  the
actual process of convergence is not formally described. This paper  to the best of our knowledge 
is the ﬁrst time to formally show the reinforcement learning dynamics in an inﬁnitely large agent
population. In particular  the Fokker-Planck equation describing the evolution of the probability
distribution of Q-values in an agent population has not been reported elsewhere.
2 Preliminaries

In this paper  we focus on an inﬁnitely large Q-learning agent population  in which each agent learns
its policy concurrently over repeated symmetric bimatrix games with some other agents. Here we
present the n-agent learning framework we consider in Section 2.1. The necessary backgrounds on
symmetric bimatrix games and Q-learning are provided in Sections 2.2 and 2.3 respectively.

2.1 An n-Agent Concurrent Learning Framework
Consider a large population N “ t1  . . .   nu of n agents  where n tends to inﬁnity. Each agent has
the same set A “ ta1  . . .   aku of k available actions for a symmetric bimatrix game G. The learning
framework of these n agents is presented in Algorithm 1. Speciﬁcally  at each time step  an individual
agent ﬁrst independently selects an action to use according to its own policy (lines 3-5). Then  each
agent plays the game G with each of the m opponents that are randomly selected from the population
(lines 6-12). Note that the m opponents with whom an agent play games may change for different
time steps. A larger value of m suggests agents can learn their policies from the interactions with a
wider range of other agents.1 For normalization  we assume the received immediate payoff for an

1When m equals 1  our framework is in effect equivalent to the social learning [23]  which is a commonly

adopted framework in norm emergence research.

2

individual agent is averaged over all of the m games it plays at each time step. At the end of each
time step  each agent learns its policy independently and concurrently  so as to maximize its own
future payoff (lines 13-16).

time step  the maximum time step T

Agent i select an action a P A according to its policy

Algorithm 1 An n-Agent Concurrent Learning Framework
Require: a set N of agents  a set A of available actions  a symmetric game G  the number m of opponents per
1: while t ă T do
t Ð t ` 1
2:
for each agent i P N do
3:
4:
end for
5:
for each agent i P N do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end for
16:
17: end while

Randomly select agent j from the population N if θj ă m
Agents i and j play the game G using their selected actions respectively
θi Ð θi ` 1  θj Ð θj ` 1

Receive an immediate payoff  and update its policy using a learning method
θi Ð 0

Ź θi is the number of games that agent i has played

end for
for each agent i P N do

while θi ă m do

end while

2.2 Symmetric Bimatrix Games

Bimatrix games are typical mathematical modellings of strategic interactions between rational
decision-makers (agents). Conventionally  in such a game  there are two players: the row player and
the column player. The players play an action at the same time  and receive a payoff immediately.
A bimatrix game is symmetric if both the players have the same set of available actions  and the
resulting payoff of each player depends not on the role of the player  but only on their joint actions
[4]. For reasons of exposition  we focus on an action set size of 2. In Table 1  we present a general
form of 2-player-2-action symmetric bimatrix games. The ﬁrst number of each entry is the payoff of
the row player and the second number is the payoff of the column player. Clearly  the payoff matrix
of the row player is the transpose of the payoff matrix of the column player.

Action a1
Action a2

Action a1 Action a2

α  α
γ  β

β  γ
δ  δ

Table 1: A general form of 2-player-2-action symmetric bimatrix games.

2.3 Q-Learning for Bimatrix Games

Q-learning [30] is one of the most important algorithms in reinforcement learning research  and is
the basis of a number of multi-agent reinforcement learning algorithms [3  5  7]. Given that there
is a set S of states and a set A of available actions  such that an agent may transit to a new state
s1 P S as a result of using an action a P A under the current state s P S. Q-learning maintains a
Q-value for each state-action pair ps  aq to estimate the cumulative payoff over the successive time
steps after performing action a at state s. Consider an arbitrary agent i in the agent population N .
Suppose that at time t  it plays the jth action aj under state s  and receives an immediate payoff
t`1ps  ajq for the state-action pair ps  ajq
tps  ajq accordingly. This agent will update its Q-value Qi
ri
as follows:

t`1ps  ajq “ p1 ´ ηqQi
Qi

tps  ajq ` ηrri

(1)
where η is the learning rate  γ is the discounting factor  and s1 is the resulting state of using action a
tps1  a1q estimates the optimal discounted future payoff
under state s  so that the term γ maxa1PA Qi

tps  ajq ` γ max@a1PA

tps1  a1qs 
Qi

3

t “ rQi

after the state transition. For any bimatrix game  there is only one episode in the entire course (or
round) of the game: at a given time step t  the players each takes one action simultaneously  and
receives an immediate payoff; then  the game ends. At the next time step t ` 1  agents play another
round of the game. This means that from time t to t ` 1  there is no state transition for an agent in
bimatrix games  and the resulting state s1 does not exist at all. Therefore  for bimatrix games  it is a
tpakqs(cid:124) 
common practice to maintain a vector of Q-values for each action  i.e.  Qi
and remove the term γ maxa1PA Qi

tps1  a1q from the Q-value update function [11  22  32]:
t`1pajq “ p1 ´ ηqQi
Qi

tpa1q . . . Qi

(2)
We consider that each agent adopts a mixed-strategy policy  such that its Q-values are interpreted as
tpakqs(cid:124) represent the mixed-
Boltzmann probabilities for action selection. Let xi
tpajq @aj P A is its probability of
strategy policy of agent i at time t  in which each component xi
tpajq is given as follows:
ř
playing action aj at time t. The value of xi

(3)
where τ P r0 8q is the Boltzmann exploration temperature. A larger value of τ indicates the fewer
exploration for individual agents. When τ is 0  the probability of taking each action is uniform  which
means that agents take actions randomly. When τ Ñ 8  agents take the action with the highest
Q-value in probability 1.

tpajq
eτ Qi
@aPA eτ Qi

tpajq ` ηri
t “ rxi

tpajq.
tpa1q . . . xi

tpajq “
xi

tpaq  

3 A Mean Field Theoretic Model

In this section  we model the Q-learning dynamics under the n-agent setting presented in the last
section. In Section 3.1  taking the view of an individual agent  we model the dynamics of its Q-values
with mean ﬁeld approximation  such that  ﬁctitiously  an agent updates its Q-values in response to
the mean policy of the population. In Section 3.2  taking an bird eye’s view  we model how the
probability distribution of Q-values in the population evolves as time goes forward  and show the
population dynamics can be characterized by a system of three equations.

3.1 Dynamics of Q-values for Individual Agents
Consider an arbitrary agent i in the population N . By Equation 2  we can derive the difference
equation of its Q-values in terms of expected change. For any action aj  at time t  the expected
change of the corresponding Q-value is given as follows:

ErQi

t`1pajq ´ Qi

tpajqs “ xi
“ ηxi

tpajqrQi
tpajqrErri

t`1pajq ´ Qi
tpajqs ´ Qi

tpajqs ` r1 ´ xi
tpajqs.

tpajqs ˆ 0

On the right hand side of the ﬁrst line  the ﬁrst term represents the change in the Q-value if action
aj is used at time t  and the second term indicates that there should be no change in the Q-value if
action aj is not used at time t. In the continuous time limit  this difference equation corresponds to
the following differential equation:
tpajq

(4)

(5)

(6)

Er dQi
dt

s “ ηxi
tpajqrErri
ř
tpajq
eτ Qi
“ η
@aPA eτ Qi

tpajqs ´ Qi
tpaqrErri

tpajqs
tpajqs ´ Qi

tpajqs.

This differential equation governs the dynamics of the expected change in Q-values for individual
agents. By this equation  at a certain time step t  how fast an agent increases or decreases its Q-value
for a particular action is susceptible to the learning rate η  Boltzmann exploration temperature τ  the
current Q-values and the received payoff at this time step.
Remember that at each time step  an agent play games with m other agents that are randomly selected
from the population. Let us ﬁrst focus on one particular round of the game G and assume the opponent
in this round to be agent z. We denote the payoff matrix of the row player in game G by U. For agent
i  the expected payoff of taking action aj against agent z using the policy xz

t is determined as:

Erri

tpaj  xz

(cid:124)
tqs “ e
j Uxz
t  

4

ř

n

zPMi

t

Erri

Erri

tpaj  xz

ts.
∆xz

tpaj  ¯xtq(cid:124)

tpaj  xz
tpaj  ¯xt ` ∆xz
tqs
tpaj  ¯xtqs ` Er∇ri

t  where n is the total number of agents. The policy xz
t “ ¯xt ` ∆xz
t from the mean policy ¯xt  such that xz
tqs is approximated as:

Let Mi
expected received payoff of taking action aj at time t  that is  Erri
m rounds of games it plays with m opponents  is approximated as:

(7)
t Ă N be the set of m opponents that agent i plays games with at time t. For agent i  its
tpajqs  which is averaged over the

where ej is the unit vector  in which the jth component equals 1 and the other components equal
0. Let ¯xt “ rxtpa1q . . . xtpakqs(cid:124) be the mean policy of the population N at time t  such that
¯xt “ 1
t of agent z can be represented
@iPN xi
by a deviation ∆xz
t . With the ﬁrst-order Taylor
series expansion  the expected payoff Erri
tqs “ Erri
« Erri
ř
ř
Erri
tpaj  xz
tqs
tpajqs “ 1
m
« 1
rErri
tpaj  ¯xtqs ` Er∇ri
zPMi
m
tpaj  ¯xtqs ` ∇ri
“ Erri
tpaj  ¯xtq(cid:124)Er 1
ř
tpaj  ¯xtqs.
« Erri
ts will become closer to 0  and hence the
As the value of m increases  the term Er 1
approximation will become more accurate.2 By this approximation  for an individual agent  its
received payoff of playing with its opponents is approximately the payoff of playing against the mean
policy ¯xt averaged over all of the agents in the population. That is to say  although different agents
actually interact with different opponents  intuitively  one can conceive different agents face one same
ﬁctitious agent that uses the mean policy.
Substituting the term Erri
tpajqs with the approximation shown in Equation 8  Equation 5—the
equation that fundamentally governs the dynamics of the expected change in Q-values for individual
agents—is rewritten as follows:
tpajq

ř
tpaj  ¯xtq(cid:124)

tss
∆xz
ts
∆xz

zPMi

zPMi

t

∆xz

t

(8)

m

m

t

Er dQi
dt

s “ η

tpaqrErri

tpaj  ¯xtqs ´ Qi

tpajqs.

ř

tpajq
eτ Qi
@aPA eτ Qi

On the right hand side  the learning rate η and Boltzmann exploration temperature τ are a priori
given and the same for the entire agent population. Moreover  for symmetric bimatrix games  the
expected payoff of using action aj against the mean policy ¯xt is independent of the roles of individual
agents. Therefore  at time t  for any agent i  how fast it changes its Q-values should be attributed to
its current Q-values Qi
t and the mean policy ¯xt of the whole population. Dropping the agent index 
for any individual agent in the population  Equation 9 can be expressed as a function vj of its current
Q-values and the mean policy:

(9)

(10)

ř

vjpQt  ¯xtq ﬁ Er dQtpajq

dt

ř

eτ Qtpajq
@aPA eτ QtpaqrErrtpaj  ¯xtqs ´ Qtpajqs.
s “ η
ř
@iPNpeτ Qi

ř
tpajq{

@aPA eτ Qi

Note that the mean policy ¯xt is indeed given by the Q-values of all agents in the population  i.e. 
tpaqq. Therefore  the expected
@aj P A  ¯xtpajq “ 1
change in Q-values for any individual agent is determined by the joint Q-values of all the agents 
which include the Q-values of itself. This suggests that in long term  the trajectories of Q-values for
individual agents are uniquely determined by their joint initial Q-values.

tpajq “ 1

@iPN xi

n

n

3.2 Evolution of the Distribution of Q-values in a Population
Consider a Q-value space Rk with k axes Y1  . . .   Yk  where k is the number of available actions. At
time t  each agent i occupies a point Qt “ Qi
t in this space according to its current Q-values Qi
t.
tq is an analytic function. Given
t is between 0 and 1  we consider the second order and the higher order terms

2 The series in Equation 8 should be convergent  since the function upaj  xz

each element of the vector ∆xz
negligible. When m  n Ñ 8  Equation 8 holds.

5

Figure 1: A 3-dimensional illustration of the entry and departure of individual agents through facets
causing the change in the number of agents in the box B.
Let ppQt  tq be the function of agent density in the space at time t  such that the density ppQt  tq at
any point Qt is the proportion of agents in the population having their Q-values equal to Qt and
hence occupying the point Qt in the space at time t. Intuitively  ppQt  tq can also be considered as
the probability distribution of Q-values in the agent population. Note that agents will update their
Q-values during interactions. As a result  as time t moves forward  agents will change their positions
in the space  which will lead to the change in the density function ppQt  tq. In what follows  we shall
derive the differential equation that describes the time evolution of ppQt  tq.
Let us focus on an arbitrary point Qt in this space  and an inﬁnitesimal box (or hyperrectangle) B
around this point  such that B ﬁ tqt : Qtpajq ď qtpajq ď Qtpajq ` dQtpajq  @aj P Au. Basically 
the number of agents in this box at time t is nppQt  tqdV   where dV “ Π@ajPAdQtpajq is the
volume of the box. Given that there is no birth or death of individual agents over time  there is only
one cause for the change in the density ppQt  tq of agents in that box—some agents enter or leave the
box through its surface. Note that there are 2k facets for a k-dimensional box. Let FpQtpajqq denote
a facet of this box  in which the jth component of each vector in this facet is set to Qtpajq  such
that the Yj-axis is the normal of this facet. That is  FpQtpajqq ﬁ tqt : qtpajq “ Qtpajq  Qtpaiq ď
qtpaiq ď Qtpaiq ` dQtpaiq  @i P t1  . . .   kuztjuu. We deﬁne ψ`pQtpajq  tq and ψ´pQtpajq  tq 
respectively  to be the number of agents that travel through the facet FpQtpajqq in the positive and
negative direction of the Yj-axis from time t to t ` dt. A graphical demonstration with the number of
available actions k “ 3 is shown in Figure 1. By the conservation law of the number of agents in the
population  we shall have:

nppQt  t ` dtqdV ´ nppQt  tqdV “ kÿ

j“1

ψ`pQtpajq  tq ` ψ´pQtpajq ` dQtpajq  tq
´ ψ´pQtpajq  tq ´ ψ`pQtpajq ` dQtpajq  tq.

(11)

This equation expresses that the number of agents entering (or leaving) the box should be the sum of
the number of agents entering (or leaving) through every facets. The ﬁrst and the second term on the
left hand side represent the numbers of agents in this box at time t ` dt and at time t  respectively.
Thus  the left hand side corresponds to the change in the number of agents in the box from time t
to t ` dt. On the right hand side  since agents that travel through the facet FpQtpajq ` dQtpajqq
in the negative direction of the Yj-axis will in effect enter the box B (as shown in Figure 1)  the
ﬁrst two terms are the number of agents entering the box B along the Yj-axis. Symmetrically  the
last two terms are the number of agents leaving that box along the Yj-axis. Hence  the right hand
side corresponds to the sum of the net number of agents entering the box along every axes. Let
ψpQtpajq  tq ﬁ ψ`pQtpajq  tq ´ ψ´pQtpajq  tq  which denotes the ﬂow of agents travelling through
the facet FpQtpajqq. Equation 11 can be rewritten as:

nppQt  t ` dtqdV ´ nppQt  tqdV “

j“1 ψpQtpajq  tq ´ ψpQtpajq ` dQtpajq  tq.

(12)
We now derive the form of ψpQtpajq  tq. Remember that how fast an agent increases or decreases
its Q-value  i.e.  the velocity of this agent in the Q-value space  is given by the function vjpQt  ¯xtq
shown in Equation 10. From time t to t ` dt  the displacement that an agent around the point Qt
travels should be approximately vjpQt  ¯xtqdt. That is  roughly speaking  agents that travel through
the facet FpQtpajqq along the Yj-axis from time t to t ` dt should be located in the adjacent box
B1 ﬁ tqt : Qtpajq´ vjpQt  ¯xtqdt ď qtpajq ď Qtpajq  Qtpaiq ď qtpaiq ď Qtpaiq` dQtpaiq  @i P

ř

k

6

t1  . . .   kuztjuu. Therefore  the value of ψpQtpajq  tq should be:

ψpQtpajq  tq “ nppQt  tqvjpQt  ¯xtqdtdSj 

(13)
where dSj “ Π@aiPAztajudQtpaiq is the area of the facet FpQtpajqq  so that vjpQt  ¯xtqdtdSj is the
volume of of the box B1. Substituting ψpQtpajq  tq in Equation 12 with Equation 13  and dividing
both sides by dV dt  we have:
nppQt  t ` dtq ´ nppQt  tq

nppQt  tqvjpQt  ¯xtqdSj ´ nppQt ` dQt  tqvjpQt ` dQt  ¯xtqdSj

dV

1

dQtpajqrnppQt  tqvjpQt  ¯xtq ´ nppQt ` dQt  tqvjpQt ` dQt  ¯xtqs.

“ kÿ
“ kÿ

j“1

j“1

dt

“ ´ kÿ

j“1

BppQt  tq

Bt

(14)

(15)

(16)

(17)

This equation in the continuous limit corresponds to:

B

BQtpajqrppQt  tqvjpQt  ¯xtqs “ ´∇ ¨ pppQt  tqvpQt  ¯xtqq 

where ∇¨ is the divergence operator  and vpQt  ¯xtq is a vector ﬁeld (or the ﬂux) in which the jth
component is vjpQt  tq. This equation is the Fokker-Planck equation [6  21] with zero diffusion. By
this equation  the change in the density of agents occupying a certain point Qt in the space  which
is also the probability density of agents having certain Q-values Qt in the population  is jointly
determined by the current density ppQt  tq and the velocity vpQt  ¯xtq. Note that the velocity in Q-
values depends on the mean policy ¯xt. By the law of large numbers  each component ¯xtpajq @aj P A
of the mean policy ¯xt should be close to the expectation  which is given by:

Therefore  the Q-learning dynamics of an inﬁnitely large agent population can be modelling by the
following system of three equations:

ż

ż

¯xt “

. . .

ř
eτ Qtpajq
@aPA eτ Qtpaq ppQt  tqdQtpa1q . . . dQtpakq.
“ ´ kÿ
BQtpajqrppQt  tqvjpQt  ¯xtqs 
ř
eτ Qpajq
ż
vjpQt  ¯xtq “ η
@aPA eτ QtpaqrErrtpaj  ¯xtqs ´ Qtpajqs 
ř
eτ Qtpajq
@aPA eτ Qtpaq ppQt  tqdQtpa1q . . . dQtpakq.

BppQt  tq
Bt
ż

¯xt “

j“1

. . .

B

$’’’’’’’’’&’’’’’’’’’%

This system of equation by nature involves a backward-forward structure. For an individual agent 
at a certain time instant  it reasons backward and updates its Q-values towards a better estimation
of the best response action facing the current expected policy. Collectively  the current updates of
Q-values for individual agents may result in a future Q-value distribution that is different from the
current one. This will in the other way round cause a change in the expected policy  which will
make agents’ current best responses to the expected policy invalid in the future. Therefore  under the
n-agent setting we consider  the Q-learning agents are usually myopic.

4 Experimental Validation

In this section  we compare the behaviours obtained by our mean ﬁeld theoretic model with the
behaviours obtained from agent-based simulations. For the model  we employ ﬁnite difference
methods to solve the system of equations shown in Equation 17. For the agent-based simulations  we
set the number n of agents to 1  000  and consider two cases of the number m of opponents per time
step: m “ 0.05n and m “ n ´ 1. To smooth out the randomness  we run 100 simulations for each
setting. For comparison  the learning rate η is set to 0.1 and the exploration temperature τ is set to 2
in both the model and the simulations.

7

Figure 2: Evolution of the expected Q-values derived from our model and that of the mean Q-values
in agent-based simulations.

Figure 3: Evolution of the expected policy derived from our model and that of the mean policy in
agent-based simulations.

To validate if our model can well reﬂect the diverse population dynamics caused by different
game settings  we select four typical types of symmetric bimatrix games to experiment on  namely 
prisoner’s dilemma (PD)  choosing side (CS)  stag hunt (SH) and hawk dove (HD) games. The payoff
bimatrices of these games are shown in Table 2. In PD game  the dominant strategy is for both players
to play D  and hence pD  Dq is the unique Nash equilibrium. In CS game  there are two equally
good symmetric Nash equilibria pL  Lq and pR  Rq. In SH game  there are also two symmetric Nash
equilibria  i.e.  pS  Sq and pH  Hq. However  while pS  Sq Pareto dominates pH  Hq and maximizes
the social welfare  pH  Hq risk dominates pS  Sq. In HD game  the two Nash equilibria pD  Hq and
pH  Dq are asymmetric  such that it is unfair for the player taking H in these two equilibria.

C
C 3 3
D 0 5

D
0 5
1 1

L
1 1
L
R -1 -1

R
-1 -1
1 1

H
H 1 1
0 2
S

S
2 0
4 4

D
D 1 1
H 2 0

H
0 2
-1 -1

(b) Choosing Side  L: left 
R: right

(c) Stag Hunt  S: stag 
H: hare

(d) Hawk Dove  D: dove 
H: hawk

(a)
Dilemma 
C:
operate  D: defect

Prisoner’s
co-

Table 2: The typical symmetric bimatrix games that we experiment on.

Without loss of generality  for each game  we assume the initial Q-value of the ﬁrst action and the
second action follow Beta distributions Betap20  80  rmin  rmaxq and Betap10  90  rmin  rmaxq  respec-
tively.The ﬁrst two parameters control the shape of the probability density function  and the latter two
parameters prescribe the support to be rrmin  rmaxs  where rmin is the minimum payoff of the game
and rmax is the maximum payoff. Consequently  for every games  the initial expected Q-value of the
ﬁrst action is slightly higher than that of the second action.
In Figures 2 and 3  we compare the expected Q-values ErQts and the expected policy Erxts obtained
by our model with the counterparts ¯Qt and ¯xt that are averaged over all of the agents in the agent-
based simulations. It is clear that our model well captures the qualitatively different patterns of
evolution in agent populations playing different kinds of games. In particular  as shown in Figure 2 
the dynamics of the expected Q-values generally overlap the dynamics of the mean Q-values  which
suggest our model almost precisely describes how the Q-value distribution of the population evolves
over time. Moreover  we note that in agent-based simulations  the agent behaviours with m “ 0.05n
match those with m “ n ´ 1. This implies that  strictly speaking  Equation 8 holds if m  n Ñ 8 
however  our mean ﬁeld theoretic model should be practically valid if the values of m and n are
sufﬁciently large.

8

Figure 4: The probability density functions of different initial Q-value distribution that we experiment
on. Yellow (light) color indicates the ﬁrst action H and purple (dark) color indicates the second
action S.

Figure 5: Evolution of the expected Q-values derived from our model and that of the mean Q-values
in agent-based simulations.

Figure 6: Evolution of the expected policy derived from our model and that of the mean policy in
agent-based simulations.

We proceed to change the initial Q-value distribution for stag hunt games. Given the equilibrium
pS  Sq is Pareto dominant but the other equilibrium pH  Hq is risk dominant  the population dynamics
should be highly susceptible to the initial proportion of agents using each action. As shown in Figure
4  we consider three different cases of the initial Q-value distribution : 1) Q0pa1q„Betap80  20  0  3q
and Q0pa2q„Betap90  10  0  3q; 2) Q0pa1q„Betap80  20  0  3q and Q0pa2q„Betap20  80  0  3q; and
3) Q0pa1q„Betap50  50  0  3q and Q0pa2q„Betap5  5  0  3q. In Figures 5 and 6  we compare the
expected Q-values and policy obtained by our model with the mean Q-values and policy in agent-
based simulations. We can easily observe that the different settings of initial Q-value distribution
results in diverse patterns of evolution in agent populations. Under each setting  the dynamics
obtained by our model match those in agent-based simulations  which again validates our model well
describes the population dynamics under different settings.

5 Conclusions and Future Work

In this paper  we model the dynamics of Q-learning in symmetric bimatrix games under an n-agent
setting where n Ñ 8. Using the mean ﬁeld theory  we derive an equation that universally describes
the dynamics of Q-values for any individual agent. We also derive a Fokker-Planck equation that
describes the evolution of the distribution of Q-values in the agent population. We show the Q-
learning dynamics under the n-agent setting can be described by a system of only three equations.
The experiments on typical types of symmetric bimatrix games and different initial settings of Q-
values validate that the expected agent behaviours obtained by our model well match the counterparts
in agent-based simulations. As future work  we will extend our model to multiple-state games 
asymmetric games  and multiple populations. Other learning algorithms will also be investigated.

9

References
[1] Adrian K Agogino and Kagan Tumer. A multiagent approach to managing air trafﬁc ﬂow.

Autonomous Agents and Multi-Agent Systems  24(1):1–25  2012.

[2] Daan Bloembergen  Karl Tuyls  Daniel Hennes  and Michael Kaisers. Evolutionary dynamics of
multi-agent learning: A survey. Journal of Artiﬁcial Intelligence Research  53:659–697  2015.

[3] Lucian Bu  Robert Babu  Bart De Schutter  et al. A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems  Man  and Cybernetics  Part C (Applications
and Reviews)  38(2):156–172  2008.

[4] Shih-Fen Cheng  Daniel M Reeves  Yevgeniy Vorobeychik  and Michael P Wellman. Notes on

equilibria in symmetric games. 2004.

[5] C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent

systems. In Proc. of AAAI  1998.

[6] Adriaan Daniël Fokker. Die mittlere energie rotierender elektrischer dipole im strahlungsfeld.

Annalen der Physik  348(5):810–820  1914.

[7] Junling Hu  Michael P Wellman  et al. Multiagent reinforcement learning: theoretical framework

and an algorithm. In ICML  volume 98  pages 242–250. Citeseer  1998.

[8] Minyi Huang  Roland P Malhamé  Peter E Caines  et al. Large population stochastic dy-
namic games: closed-loop mckean-vlasov systems and the nash certainty equivalence principle.
Communications in Information & Systems  6(3):221–252  2006.

[9] Michael Kaisers  Daan Bloembergen  and Karl Tuyls. A common gradient in multi-agent
reinforcement learning. In Proceedings of the 11th International Conference on Autonomous
Agents and Multiagent Systems-Volume 3  pages 1393–1394. International Foundation for
Autonomous Agents and Multiagent Systems  2012.

[10] Michael Kaisers and Karl Tuyls. Frequency adjusted multi-agent q-learning. In Proceedings
of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume
1-Volume 1  pages 309–316. International Foundation for Autonomous Agents and Multiagent
Systems  2010.

[11] Ardeshir Kianercy and Aram Galstyan. Dynamics of boltzmann q learning in two-player

two-action games. Physical Review E  85(4):041145  2012.

[12] Tomas Klos  Gerrit Jan Van Ahee  and Karl Tuyls. Evolutionary dynamics of regret minimization.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases 
pages 82–96. Springer  2010.

[13] Marc Lanctot  Vinicius Zambaldi  Audrunas Gruslys  Angeliki Lazaridou  Karl Tuyls  Julien
Pérolat  David Silver  and Thore Graepel. A uniﬁed game-theoretic approach to multiagent
reinforcement learning. In Advances in Neural Information Processing Systems  pages 4190–
4203  2017.

[14] Jean-Michel Lasry and Pierre-Louis Lions. Mean ﬁeld games. Japanese journal of mathematics 

2(1):229–260  2007.

[15] Joel Z Leibo  Vinicius Zambaldi  Marc Lanctot  Janusz Marecki  and Thore Graepel. Multi-agent
reinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference
on Autonomous Agents and MultiAgent Systems  pages 464–473. International Foundation for
Autonomous Agents and Multiagent Systems  2017.

[16] Victor Lesser  Charles L Ortiz Jr  and Milind Tambe. Distributed sensor networks: A multiagent

perspective  volume 9. Springer Science & Business Media  2012.

[17] David Mguni  Joel Jennings  and Enrique Munoz de Cote. Decentralised learning in systems
with many  many strategic agents. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence 
2018.

10

[18] Liviu Panait  Karl Tuyls  and Sean Luke. Theoretical advantages of lenient learners: An
evolutionary game theoretic perspective. Journal of Machine Learning Research  9(Mar):423–
457  2008.

[19] Lynne E Parker. Current state of the art in distributed autonomous mobile robotics. In Distributed

Autonomous Robotic Systems 4  pages 3–12. Springer  2000.

[20] M. Pipattanasomporn  H. Feroze  and S. Rahman. Multi-agent systems in a distributed smart
grid: Design and implementation. In 2009 IEEE/PES Power Systems Conference and Exposition 
pages 1–8  2009.

[21] VM Planck. Über einen satz der statistischen dynamik und seine erweiterung in der quantenthe-

orie. Sitzungberichte der  1917.

[22] Eduardo Rodrigues Gomes and Ryszard Kowalczyk. Dynamic analysis of multiagent q-learning
with ε-greedy exploration. In Proceedings of the 26th Annual International Conference on
Machine Learning  pages 369–376. ACM  2009.

[23] Sandip Sen and Stéphane Airiau. Emergence of norms through social learning. In Proceedings
of 20th International Joint Conference on Artiﬁcial Intelligence  volume 1507  page 1512  2007.

[24] Yoav Shoham  Rob Powers  and Trond Grenager. If multi-agent learning is the answer  what is

the question? Artiﬁcial Intelligence  171(7):365–377  2007.

[25] Jayakumar Subramanian and Aditya Mahajan. Reinforcement learning in stationary mean-
ﬁeld games. In Proceedings of the 18th International Conference on Autonomous Agents and
MultiAgent Systems  pages 251–259. International Foundation for Autonomous Agents and
Multiagent Systems  2019.

[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

2018.

[27] Karl Tuyls  Pieter Jan’T Hoen  and Bram Vanschoenwinkel. An evolutionary dynamical
analysis of multi-agent learning in iterated games. Autonomous Agents and Multi-Agent Systems 
12(1):115–153  2006.

[28] Karl Tuyls and Simon Parsons. What evolutionary game theory tells us about multiagent

learning. Artiﬁcial Intelligence  171(7):406–416  2007.

[29] Karl Tuyls  Katja Verbeeck  and Tom Lenaerts. A selection-mutation model for q-learning in
multi-agent systems. In Proceedings of the second international joint conference on Autonomous
agents and multiagent systems  pages 693–700. ACM  2003.

[30] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine Learning  8(3-4)  1992.

[31] Pierre Weiss. L’hypothèse du champ moléculaire et la propriété ferromagnétique. J. Phys. Theor.

Appl.  6(1):661–690  1907.

[32] Michael Wunder  Michael L Littman  and Monica Babes. Classes of multiagent q-learning
dynamics with epsilon-greedy exploration. In Proceedings of the 27th International Conference
on Machine Learning (ICML-10)  pages 1167–1174. Citeseer  2010.

[33] Yaodong Yang  Rui Luo  Minne Li  Ming Zhou  Weinan Zhang  and Jun Wang. Mean ﬁeld
multi-agent reinforcement learning. In International Conference on Machine Learning  pages
5567–5576  2018.

11

,Shuyue Hu
Chin-wing Leung
Ho-fung Leung