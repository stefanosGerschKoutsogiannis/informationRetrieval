2019,A Composable Specification Language for Reinforcement Learning Tasks,Reinforcement learning is a promising approach for learning control policies for robot tasks. However  specifying complex tasks (e.g.  with multiple objectives and safety constraints) can be challenging  since the user must design a reward function that encodes the entire task. Furthermore  the user often needs to manually shape the reward to ensure convergence of the learning algorithm. We propose a language for specifying complex control tasks  along with an algorithm that compiles specifications in our language into a reward function and automatically performs reward shaping. We implement our approach in a tool called SPECTRL  and show that it outperforms several state-of-the-art baselines.,A Composable Speciﬁcation Language for

Reinforcement Learning Tasks

Kishor Jothimurugan  Rajeev Alur  Osbert Bastani

University of Pennsylvania

{kishor alur obastani}@cis.upenn.edu

Abstract

Reinforcement learning is a promising approach for learning control policies for
robot tasks. However  specifying complex tasks (e.g.  with multiple objectives
and safety constraints) can be challenging  since the user must design a reward
function that encodes the entire task. Furthermore  the user often needs to manually
shape the reward to ensure convergence of the learning algorithm. We propose
a language for specifying complex control tasks  along with an algorithm that
compiles speciﬁcations in our language into a reward function and automatically
performs reward shaping. We implement our approach in a tool called SPECTRL 
and show that it outperforms several state-of-the-art baselines.

1

Introduction

Reinforcement learning (RL) is a promising approach to learning control policies for robotics
tasks [5  21  16  15]. A key shortcoming of RL is that the user must manually encode the task as a
real-valued reward function  which can be challenging for several reasons. First  for complex tasks
with multiple objectives and constraints  the user must manually devise a single reward function
that balances different parts of the task. Second  the state space must often be extended to encode
the reward—e.g.  adding indicators that keep track of which subtasks have been completed. Third 
oftentimes  different reward functions can encode the same task  and the choice of reward function
can have a large impact on the convergence of the RL algorithm. Thus  users must manually design
rewards that assign “partial credit” for achieving intermediate goals  known as reward shaping [17].
For example  consider the task in Figure 1  where the state is the robot position and its remaining
fuel  the action is a (bounded) robot velocity  and the task is

“Reach target q  then reach target p  while maintaining positive fuel and avoiding
obstacle O”.

To encode this task  we would have to combine rewards for (i) reaching q  and then reaching p (where
“reach x” denotes the task of reaching an  box around x—the regions corresponding to p and q are
denoted by P and Q respectively)  (ii) avoiding region O  and (iii) maintaining positive fuel  into
a single reward function. Furthermore  we would have to extend the state space to keep track of
whether q has been reached—otherwise  the control policy would not know whether the current goal
is to move towards q or p. Finally  we might need to shape the reward to assign partial credit for
getting closer to q  or for reaching q without reaching p.
We propose a language for users to specify control tasks. Our language allows the user to specify
objectives and safety constraints as logical predicates over states  and then compose these primitives

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Figure 1: Example control task. The blue dashed trajectory satisﬁes the speciﬁcation φex (ignoring the
fuel budget)  whereas the red dotted trajectory does not satisfy φex as it passes through the obstacle.

sequentially or as disjunctions. For example  the above task can be expressed as

φex = achieve (reach q; reach p) ensuring (avoid O ∧ fuel > 0) 

(1)

where fuel is the component of the state space keeping track of how much fuel is remaining.
The principle underlying our approach is that in many applications  users have in mind a sequence
of high-level actions that are needed to accomplish a given task. For example  φex may encode the
scenario where the user wants a quadcopter to ﬂy to a location q  take a photograph  and then return
back to its owner at position p  while avoiding a building O and without running out of battery.
Alternatively  a user may want to program a warehouse robot to go to the next room  pick up a box 
and then bring this item back to the ﬁrst room. In addition to specifying sequences of tasks  users can
also specify choices between multiple tasks (e.g.  bring back any box).
Another key aspect of our approach is to allow the user to specify a task without providing the
low-level sequence of actions needed to accomplish the task. Instead  analogous to how a compiler
generates machine code from a program written by the user  we propose a compiler for our language
that takes the user-provided task speciﬁcation and generates a control policy that achieves the task.
RL is a perfect tool for doing so—in particular  our algorithm compiles the task speciﬁcation to a
reward function  and then uses state-of-the-art RL algorithms to learn a control policy. Overall  the
user provides the high-level task structure  and the RL algorithm ﬁlls in the low-level details.
A key challenge is that our speciﬁcations may encode rewards that are not Markov—e.g.  in φex 
the robot needs memory that keeps track of whether its current goal is reach q or reach p. Thus 
our compiler automatically extends the state space using a task monitor  which is an automaton
that keeps track of which subtasks have been completed.1 Furthermore  this automaton may have
nondeterministic transitions; thus  our compiler also extends the action space with actions for choosing
state transitions. Intuitively  there may be multiple points in time at which a subtask is considered
completed  and the robot must choose which one to use.
Another challenge is that the naïve choice of rewards—i.e.  reward 1 if the task is completed and
0 otherwise—can be very sparse  especially for complex tasks. Thus  our compiler automatically
performs two kinds of reward shaping based on the structure of the speciﬁcation—it assigns partial
credit for (i) partially accomplishing intermediate subtasks  and (ii) for completing more subtasks. For
deterministic MDPs  our reward shaping is guaranteed to preserve the optimal policy; we empirically
ﬁnd it also works well for stochastic MDPs.
We have implemented our approach in a tool called SPECTRL 2 and evaluated the performance of
SPECTRL compared to a number of baselines. We show that SPECTRL learns policies that solve each
task in our benchmark with a success rate of at least 97%. In summary  our contributions are:

• We propose a language for users to specify RL tasks (Section 2).
• We design an algorithm for compiling a speciﬁcation into an RL problem  which can be
• We have implemented SPECTRL  and empirically demonstrated its beneﬁts (Section 4).

solved using standard RL algorithms (Section 3).

Related work. Imitation learning enables users to specify tasks by providing demonstrations of the
desired task [18  1  26  20  10]. However  in many settings  it may be easier for the user to directly
specify the task—e.g.  when programming a warehouse robot  it may be easier to specify waypoints
describing paths the robot should take than to manually drive the robot to obtain demonstrations.

1Intuitively  this construction is analogous to compiling a regular expression to a ﬁnite state automaton.
2SPECTRL stands for SPECifying Tasks for Reinforcement Learning.

2

Also  unlike imitation learning  our language allows the user to specify global safety constraints on
the robot. Indeed  we believe our approach complements imitation learning  since the user can specify
some parts of the task in our language and others using demonstrations.
Another approach is for the user to provide a policy sketch—i.e.  a string of tokens specifying a
sequence of subtasks [2]. However  tokens have no meaning  except equal tokens represent the
same task. Thus  policy sketches cannot be compiled to a reward function  which must be provided
separately.
Our speciﬁcation language is based on temporal logic [19]  a language of logical formulas for
specifying constraints over (typically  inﬁnite) sequences of events happening over time. For example 
temporal logic allows the user to specify that a logical predicate must be satisﬁed at some point
in time (e.g.  “eventually reach state q”) or that it must always be satisﬁed (e.g.  “always avoid an
obstacle”). In our language  these notions are represented using the achieve and ensuring
operators  respectively. Our language restricts temporal logic in a way that enables us to perform
reward shaping  and also adds useful operators such as sequencing that allow the user to easily express
complex control tasks.
Algorithms have been designed for automatically synthesizing a control policy that satisﬁes a given
temporal logic formula; see [4] for a recent survey  and [12  25  6  9] for applications to robotic
motion planning. However  these algorithms are typically based on exhaustive search over control
policies. Thus  as with ﬁnite-state planning algorithms such as value iteration [22]  they cannot be
applied to tasks with continuous state and action spaces that can be solved using RL.
Reward machines have been proposed as a high-level way to specify tasks [11]. In their work  the
user provides a speciﬁcation in the form of a ﬁnite state machine along with reward functions for
each state. Then  they propose an algorithm for learning multiple tasks simultaneously by applying
the Q-learning updates across different speciﬁcations. At a high level  these reward machines are
similar to the task monitors deﬁned in our work. However  we differ from their approach in two ways.
First  in contrast to their work  the user only needs to provide a high-level logical speciﬁcation; we
automatically generate a task monitor from this speciﬁcation. Second  our notion of task monitor has
a ﬁnite set of registers that can store real values; in contrast  their ﬁnite state reward machines cannot
store quantitative information.
The most closely related work is [13]  which proposes a variant of temporal logic called truncated
LTL  along with an algorithm for compiling a speciﬁcation written in this language to a reward
function that can be optimized using RL. However  they do not use any analog of the task monitor 
which we demonstrate is needed to handle non-Markovian speciﬁcations. Finally  [24] allows the
user to separately specify objectives and safety constraints  and then using RL to learn a policy.
However  they do not provide any way to compose rewards  and do not perform any reward shaping.
Also  their approach is tied to a speciﬁc RL algorithm. We show empirically that our approach
substantially outperforms both these approaches.
Finally  an alternative approach is to manually specify rewards for sub-goals to improve perfor-
mance. However  many challenges arise when implementing sub-goal based rewards—e.g.  how
does achieving a sub-goal count compared to violating a constraint  how to handle sub-goals that
can be achieved in multiple ways  how to ensure the agent does not repeatedly obtain a reward for a
previously completed sub-goal  etc. As tasks become more complex and deeply nested  manually
specifying rewards for sub-goals becomes very challenging. Our system is designed to automatically
solve these issues.

2 Task Speciﬁcation Language

Markov decision processes. A Markov decision process (MDP) is a tuple (S  D  A  P  T )  where
S ⊆ Rn are the states  D is the initial state distribution  A ⊆ Rm are the actions  P : S × A × S →
[0  1] are the transition probabilities  and T ∈ N is the time horizon. A rollout ζ ∈ Z of length t
a0−→ . . .
at−1−−−→ st where si ∈ S and ai ∈ A. Given a (deterministic) policy
is a sequence ζ = s0
π : Z → A  we can generate a rollout using ai = π(ζ0:i). Optionally  an MDP can also include a
reward function R : Z → R. 3

3Note that we consider rollout-based rewards rather than state-based rewards. Most modern RL algorithms 

such as policy gradient algorithms  can use rollout-based rewards.

3

Speciﬁcation language. Intuitively  a speciﬁcation φ in our language is a logical formula specify-
ing whether a given rollout ζ successfully accomplishes the desired task—in particular  it can be
interpreted as a function φ : Z → B  where B = {true  false}  deﬁned by

φ(ζ) = I[ζ successfully achieves the task] 

every p ∈ P0 is associated with a function(cid:74)p(cid:75) : S → B such that(cid:74)p(cid:75)(s) indicates whether s satisﬁes

where I is the indicator function. Formally  the user ﬁrst deﬁnes a set of atomic predicates P0  where
p. For example  given x ∈ S  the atomic predicate

(cid:74)reach x(cid:75)(s) = ((cid:107)s − x(cid:107)∞ < 1)
(cid:74)avoid O(cid:75)(s) = (s (cid:54)∈ O)

indicates whether the robot is in a state near x  and given a rectangular region O ⊆ S  the atomic
predicate

b ::= p | (b1 ∧ b2) | (b1 ∨ b2) 

Finally  the syntax of our speciﬁcations is given by 5

predicates. In particular  the syntax of predicates is given by 4

indicates if the robot is avoiding O. In general  the user can deﬁne a new atomic predicate as an

arbitrary function(cid:74)p(cid:75) : S → B. Next  predicates b ∈ P are conjunctions and disjunctions of atomic
where p ∈ P0. Similar to atomic predicates  each predicate b ∈ P corresponds to a function(cid:74)b(cid:75) :
S → B  deﬁned recursively by(cid:74)b1 ∧ b2(cid:75)(s) =(cid:74)b1(cid:75)(s)∧(cid:74)b2(cid:75)(s) and(cid:74)b1 ∨ b2(cid:75)(s) =(cid:74)b1(cid:75)(s)∨(cid:74)b2(cid:75)(s).
that(cid:74)b(cid:75)(s) = true. The second construct says that the robot should try to satisfy φ1 while always
staying in states s such that(cid:74)b(cid:75)(s) = true. The third construct says the robot should try to satisfy
φ1 or task φ2. Formally  we associate a function(cid:74)φ(cid:75) : Z → B with φ recursively as follows:

where b ∈ P. Intuitively  the ﬁrst construct means that the robot should try to reach a state s such

task φ1 and then task φ2. The fourth construct means that the robot should try to satisfy either task

φ ::= achieve b | φ1 ensuring b | φ1; φ2 | φ1 or φ2 

(cid:74)achieve b(cid:75)(ζ) = ∃ i < t  (cid:74)b(cid:75)(si)
(cid:74)φ ensuring b(cid:75)(ζ) = (cid:74)φ(cid:75)(ζ) ∧ (∀i < t  (cid:74)b(cid:75)(si))
(cid:74)φ1; φ2(cid:75)(ζ) = ∃ i < t  ((cid:74)φ1(cid:75)(ζ0:i) ∧ (cid:74)φ2(cid:75)(ζi:t))
(cid:74)φ1 or φ2(cid:75)(ζ) = (cid:74)φ1(cid:75)(ζ) ∨ (cid:74)φ2(cid:75)(ζ) 
[(cid:74)φ(cid:75)(ζ) = true] 

π∗ ∈ arg max

Pr
ζ∼Dπ

where t is the length of ζ. A rollout ζ satisﬁes φ if(cid:74)φ(cid:75)(ζ) = true  which is denoted ζ |= φ.

Problem formulation. Given an MDP and a speciﬁcation φ  our goal is to compute

(2)
where Dπ is the distribution over rollouts generated by π. In other words  we want to learn a policy
π∗ that maximizes the probability that a generated rollout ζ satisﬁes φ.

π

3 Compilation and Learning Algorithms

In this section  we describe our algorithm for reducing the above problem (2) for a given MDP
(S  D  A  P  T ) and a speciﬁcation φ to an RL problem speciﬁed as an MDP with a reward function.
At a high level  our algorithm extends the state space S to keep track of completed subtasks and
constructs a reward function R : Z → R encoding φ. A key feature of our algorithm is that the user
has control over the compilation process—we provide a natural default compilation strategy  but the
user can extend or modify our approach to improve the performance of the RL algorithm. We give
proofs in Appendix B.
Quantitative semantics. So far  we have associated speciﬁcations φ with Boolean semantics (i.e. 

(cid:74)φ(cid:75)(ζ) ∈ B). A naïve strategy is to assign rewards to rollouts based on whether they satisfy φ:

(cid:26)1

0

R(ζ) =

if ζ |= φ
otherwise.

4Formally  a predicate is a string in the context-free language generated by this context-free grammar.
5Here  achieve and ensuring correspond to the “eventually” and “always” operators in temporal logic.

4

x1 ← 0
x2 ← 0
x3 ← ∞
x4 ← ∞

ρ : min{x1  x2  x3  x4}

u

q1

q4

u

Σ : s ∈ Q

x1 ← 1 − d∞(s  q)

Σ : s ∈ P

x2 ← 1 − d∞(s  p)

u

u

u

q2
Σ : min{x1  x3  x4} > 0
u
q3

u

Figure 2: An example of a task monitor. States are labeled with rewards (preﬁxed with “ρ :”).
Transitions are labeled with transition conditions (preﬁxed with “Σ :”)  as well as register update
rules. A transition from q2 to q4 is omitted for clarity. Also  u denotes the two updates x3 ←
min{x3  d∞(s  O)} and x4 ← min{x4  fuel(s)}.

However  it is usually difﬁcult to learn a policy to maximize this reward due to its discrete nature.
A common strategy is to provide a shaped reward that quantiﬁes the “degree” to which ζ satisﬁes
φ. Our algorithm uses an approach based on quantitative semantics for temporal logic [7  8  14].
In particular  we associate an alternate interpretation of a speciﬁcation φ as a real-valued function

(cid:74)φ(cid:75)q : Z → R. To do so  the user provides quantitative semantics for atomic predicates p ∈ P0—in
particular  they provide a function(cid:74)p(cid:75)q : S → R that quantiﬁes the degree to which p holds for s ∈ S.

For example  we can use

(cid:74)reach x(cid:75)q(s) = 1 − d∞(s  x)
(cid:74)avoid O(cid:75)q(s) = d∞(s  O) 

i<t

In principle  we could now deﬁne quantitative semantics for speciﬁcations φ:

to an increase in the “degree” to which p holds. Then  the quantitative semantics for predicates

where d∞ is the L∞ distance between points  with the usual extension to sets. These semantics

should satisfy(cid:74)p(cid:75)q(s) > 0 if and only if(cid:74)p(cid:75)(s) = true  and a larger value of(cid:74)p(cid:75)q should correspond
b ∈ P are(cid:74)b1 ∧ b2(cid:75)q(s) = min{(cid:74)b1(cid:75)q(s) (cid:74)b2(cid:75)q(s)} and(cid:74)b1 ∨ b2(cid:75)q(s) = max{(cid:74)b1(cid:75)q(s) (cid:74)b2(cid:75)q(s)}.
Assuming(cid:74)p(cid:75)q satisﬁes the above properties  then(cid:74)b(cid:75)q > 0 if and only if(cid:74)b(cid:75) = true.
(cid:74)achieve b(cid:75)q(ζ) = max
(cid:74)φ ensuring b(cid:75)q(ζ) = min{(cid:74)φ(cid:75)q(ζ)  (cid:74)b(cid:75)q(s0)  ...  (cid:74)b(cid:75)q(st−1)}
(cid:74)φ1; φ2(cid:75)q(ζ) = max
(cid:74)φ1 or φ2(cid:75)q(ζ) = max{(cid:74)φ1(cid:75)q(ζ)  (cid:74)φ2(cid:75)q(ζ)}.

i<t (cid:74)b(cid:75)q(si)
min{(cid:74)φ1(cid:75)q(ζ0:i)  (cid:74)φ2(cid:75)q(ζi:t)}

Then  it is easy to show that(cid:74)φ(cid:75)(ζ) = true if and only if(cid:74)φ(cid:75)q(ζ) > 0  so we could deﬁne a reward
function R(ζ) =(cid:74)φ(cid:75)q(ζ). However  one of our key goals is to extend the state space so the policy
knows which subtasks have been completed. On the other hand  the semantics(cid:74)φ(cid:75)q quantify over all
on(cid:74)φ(cid:75)q  but applied to a single choice of time steps on which each subtask is completed.

possible ways that subtasks could have been completed in hindsight (i.e.  once the entire trajectory is
known). For example  there may be multiple points in a trajectory when a subtask reach q could be
considered as completed. Below  we describe our construction of the reward function  which is based

Task monitor. Intuitively  a task monitor is a ﬁnite-state automaton (FSA) that keeps track of which
subtasks have been completed and which constraints are still satisﬁed. Unlike an FSA  its transitions
may depend on the state s ∈ S of a given MDP. Also  since we are using quantitative semantics 
the task monitor has to keep track of the degree to which subtasks are completed and the degree to
which constraints are satisﬁed; thus  it includes registers that keep track of the these values. A key
challenge is that the task monitor is nondeterministic; as we describe below  we let the policy resolve
the nondeterminism  which corresponds to choosing which subtask to complete on each step.
Formally  a task monitor is a tuple M = (Q  X  Σ  U  ∆  q0  v0  F  ρ). First  Q is a ﬁnite set of
monitor states  which are used to keep track of which subtasks have been completed. Also  X is a
ﬁnite set of registers  which are variables used to keep track of the degree to which the speciﬁcation

5

holds so far. Given an MDP (S  D  A  P  T )  an augmented state is a tuple (s  q  v) ∈ S × Q × V  
where V = RX—i.e.  an MDP state s ∈ S  a monitor state q ∈ Q  and a vector v ∈ V encoding the
value of each register in the task monitor. An augmented state is analogous to a state of an FSA.
The transitions ∆ of the task monitor depend on the augmented state; thus  they need to specify two
pieces of information: (i) conditions on the MDP states and registers for the transition to be enabled 
and (ii) how the registers are updated. To handle (i)  we consider a set Σ of predicates over S × V  
and to handle (ii)  we consider a set U of functions u : S × V → V . Then  ∆ ⊆ Q × Σ × U × Q is
a ﬁnite set of (nondeterministic) transitions  where (q  σ  u  q(cid:48)) ∈ ∆ encodes augmented transitions
(s  q  v) a−→ (s(cid:48)  q(cid:48)  u(s  v))  where s a−→ s(cid:48) is an MDP transition  which can be taken as long as
σ(s  v) = true. Finally  v0 ∈ RX is the vector of initial register values  F ⊆ Q is a set of ﬁnal
monitor states  and ρ is a reward function ρ : S × F × V → R.
Given an MDP (S  D  A  P  T ) and a speciﬁcation φ  our algorithm constructs a task monitor Mφ =
(Q  X  Σ  U  ∆  q0  v0  F  ρ) whose states and registers keep track which subtasks of φ have been
completed. Our task monitor construction algorithm is analogous to compiling a regular expression
to an FSA. More speciﬁcally  it is analogous to algorithms for compiling temporal logic formulas
to automata [23]. We detail this algorithm in Appendix A. The underlying graph of a task monitor
constructed from any given speciﬁcation is acyclic (ignoring self loops) and ﬁnal states correspond to
sink vertices with no outgoing edges (except a self loop).
As an example  the task monitor for φex is shown in Figure 2. It has monitor states Q = {q1  q2  q3  q4}
and registers X = {x1  x2  x3  x4}. The monitor states encode when the robot (i) has not yet reached
q (q1)  (ii) has reached q  but has not yet returned to p (q2 and q3)  and (iii) has returned to p (q4); q3
is an intermediate monitor state used to ensure that the constraints are satisﬁed before continuing.

Register x1 records(cid:74)reach q(cid:75)(s) = 1 − d∞(s  q) when transitioning from q1 to q2  and x2 records
(cid:74)reach p(cid:75)q = 1−d∞(s  p) when transitioning from q3 to q4. Register x3 keeps track of the minimum
value of(cid:74)avoid s(cid:75) = d∞(s  O) over states s in the rollout  and x4 keeps track of the minimum value
of(cid:74)fuel > 0(cid:75)(s) over states s in the rollout.

Augmented MDP. Given an MDP  a speciﬁcation φ  and its task monitor Mφ  our algorithm con-
structs an augmented MDP  which is an MDP with a reward function ( ˜S  ˜s0  ˜A  ˜P   ˜R  T ). Intuitively 
if ˜π∗ is a good policy (one that achieves a high expected reward) for the augmented MDP  then
rollouts generated using ˜π∗ should satisfy φ with high probability.
In particular  we have ˜S = S × Q × V and ˜s0 = (s0  q0  v0). The transitions ˜P are based on P and
∆. However  the task monitor transitions ∆ may be nonderministic. To resolve this nondeterminism 
we require that the policy decides which task monitor transitions to take. In particular  we extend
the actions ˜A = A × Aφ to include a component Aφ = ∆ indicating which one to take at each
step. An augmented action (a  δ) ∈ ˜A  where δ = (q  σ  u  q(cid:48))  is only available in augmented state
˜s = (s  q  v) if σ(s  v) = true. Then  the augmented transition probability is given by 

˜P ((s  q  v)  (a  (q  σ  u  q(cid:48))))  (s(cid:48)  q(cid:48)  u(s  v))) = P (s  a  s(cid:48)).
Next  an augmented rollout of length t is a sequence ˜ζ = (s0  q0  v0) a0−→ ...
augmented transitions. The projection proj(˜ζ) = s0
(normal) rollout. Then  the augmented rewards

a0−→ ...

at−1−−−→ (st  qt  vt) of
at−1−−−→ st of ˜ζ is the corresponding

(cid:26)ρ(sT   qT   vT )

˜R(˜ζ) =

−∞

if qT ∈ F
otherwise

are constructed based on F and ρ. The augmented rewards satisfy the following property.
Theorem 3.1. For any MDP  speciﬁcation φ  and rollout ζ of the MDP  ζ satisﬁes φ if and only if
there exists an augmented rollout ˜ζ such that (i) R(˜ζ) > 0  and (ii) proj(˜ζ) = ζ.
Thus  if we use RL to learn an optimal augmented policy ˜π∗ over augmented states  then ˜π∗ is more
likely to generate rollouts ˜ζ such that proj(˜ζ) satisﬁes φ.
Reward shaping. As discussed before  our algorithm constructs a shaped reward function that
provides “partial credit” based on the degree to which φ is satisﬁed. We have already described
one step of reward shaping—i.e.  using quantitative semantics instead of the Boolean semantics.

6

However  the augmented rewards ˜R are −∞ unless a run reaches a ﬁnal state of the task monitor.
Thus  our algorithm performs an additional step of reward shaping—in particular  it constructs a
reward function ˜Rs that gives partial credit for accomplishing subtasks in the MDP.
For a non-ﬁnal monitor state q  let α : S × Q × V → R be deﬁned by

α(s  q  v) =

(q σ u q(cid:48))∈∆  q(cid:48)(cid:54)=q(cid:74)σ(cid:75)q(s  v).

max

Intuitively  α quantiﬁes how “close” an augmented state ˜s = (s  q  v) is to transitioning to another
augmented state with a different monitor state. Then  our algorithm assigns partial credit to augmented
states where α is larger.
However  to ensure that a good policy according to the shaped rewards ˜Rs is also a good policy
according to ˜R  it does so in a way that preserves the ordering of the cumulative rewards for rollouts—
i.e.  for two length T rollouts ˜ζ and ˜ζ(cid:48)  it guarantees that if ˜R(˜ζ) > ˜R(˜ζ(cid:48))  then ˜Rs(˜ζ) > ˜Rs(˜ζ(cid:48)).
To this end  we assume that we are given a lower bound C(cid:96) on the ﬁnal reward achieved when
reaching a ﬁnal monitor state—i.e.  C(cid:96) < ˜R(˜ζ) for all ˜ζ with ﬁnal state ˜sT = (sT   qT   vT ) such that
qT ∈ F is a ﬁnal monitor state. Furthermore  we assume that we are given an upper bound Cu on the
absolute value of α over non-ﬁnal monitor states—i.e.  Cu ≥ |α(s  q  v)| for any augmented state
such that q (cid:54)∈ F .
Now  for any q ∈ Q  let dq be the length of the longest path from q0 to q in the graph of Mφ (ignoring
self loops in ∆) and D = maxq∈Q dq. Given an augmented rollout ˜ζ  let ˜si = (si  qi  vi) be the ﬁrst
augmented state in ˜ζ such that qi = qi+1 = ... = qT . Then  the shaped reward is

(cid:26)maxi≤j<T α(sj  qT   vj) + 2Cu · (dqT − D) + C(cid:96)

˜Rs(˜ζ) =

˜R(˜ζ)

if qT (cid:54)∈ F
otherwise.

If qT (cid:54)∈ F   then the ﬁrst term of ˜Rs(˜ζ) computes how close ˜ζ was to transitioning to a new monitor
state. The second term ensures that moving closer to a ﬁnal state always increases reward. Finally 
the last term ensures that rewards ˜R(˜ζ) for qT ∈ F are always higher than rewards for qT (cid:54)∈ F . The
following theorem follows straightforwardly.
Theorem 3.2. For two augmented rollouts ˜ζ  ˜ζ(cid:48)  (i) if ˜R(˜ζ) > ˜R(˜ζ(cid:48))  then ˜Rs(˜ζ) > ˜Rs(˜ζ(cid:48))  and
(ii) if ˜ζ and ˜ζ(cid:48) end in distinct non-ﬁnal monitor states qT and q(cid:48)
  then
˜Rs(˜ζ) ≥ ˜Rs(˜ζ(cid:48)).

T such that dqT > dq(cid:48)

T

Reinforcement learning. Once our algorithm has constructed an augmented MDP  it can use any
RL algorithm to learn an augmented policy ˜π : ˜S → ˜A for the augmented MDP:

˜π∗ ∈ arg max

˜π

E˜ζ∼D˜π

[ ˜Rs(˜ζ)]

where D˜π denotes the distribution over augmented rollouts generated by policy ˜π. We solve this RL
problem using augmented random search (ARS)  a state-of-the-art RL algorithm [15].
After computing ˜π∗  we can convert ˜π∗ to a projected policy π∗ = proj(˜π∗) for the original MDP by
integrating ˜π∗ with the task monitor Mφ  which keeps track of the information needed for ˜π∗ to make
decisions. More precisely  proj( ˜π∗) includes internal memory that keeps track of the current monitor
state and register value (qt  vt) ∈ Q × V . It initializes this memory to the initial monitor state q0 and
initial register valuation v0. Given an augmented action (a  (q  σ  u  q(cid:48))) = ˜π∗((st  qt  vt))  it updates
this internal memory using the rules qt+1 = q(cid:48) and vt+1 = u(st  vt).
Finally  we use a neural network architecture similar to neural module networks [3  2]  where different
neural networks accomplish different subtasks in φ. In particular  an augmented policy ˜π is a set
of neural networks {Nq | q ∈ Q}  where Q are the monitor states in Mφ. Each Nq takes as
input (s  v) ∈ S × V and outputs an augmented action Nq(s  v) = (a  a(cid:48)) ∈ Rk+2  where k is the
out-degree of the q in Mφ; then  ˜π(s  q  v) = Nq(s  v).

7

Figure 3: Learning curves for φ1  φ2  φ3 and φ4 (top  left to right)  and φ5  φ6 and φ7 (bottom  left
to right)  for SPECTRL (green)  TLTL (blue)  CCE (yellow)  and SPECTRL without reward shaping
(purple). The x-axis shows the number of sample trajectories  and the y-axs shows the probability of
satisfying the speciﬁcation (estimated using samples). To exclude outliers  we omitted one best and
one worst run out of the 5 runs. The plots are the average over the remaining 3 runs with error bars
indicating one standard deviation around the average.

4 Experiments

Setup. We implemented our algorithm in a tool SPECTRL6  and used it to learn policies for a variety
of speciﬁcations. We consider a dynamical system with states S = R2×R  where (x  r) ∈ S encodes
the robot position x and its remaining fuel r  actions A = [−1  1]2 where an action a ∈ A is the robot
velocity  and transitions f (x  r  a) = (x + a +   r − 0.1 · |x1| · (cid:107)a(cid:107))  where  ∼ N (0  σ2I) and the
fuel consumed is proportional to the product of speed and distance from the y-axis. The initial state
is s0 = (5  0  7)  and the horizon is T = 40.
In Figure 3  we consider the following speciﬁcations  where O = [4  6] × [4  6]:

• φ1 = achieve reach (5  10) ensuring (avoid O)
• φ2 = achieve reach (5  10) ensuring (avoid O ∧ (r > 0))
• φ3 = achieve (reach [(5  10); (5  0)]) ensuring avoid O
• φ4 = achieve (reach (5  10) or reach (10  0); reach (10  10)) ensuring avoid O
• φ5 = achieve (reach [(5  10); (5  0); (10  0)]) ensuring avoid O
• φ6 = achieve (reach [(5  10); (5  0); (10  0); (10  10)]) ensuring avoid O
• φ7 = achieve (reach [(5  10); (5  0); (10  0); (10  10); (0  0)]) ensuring avoid O

where the abbreviation achieve (b; b(cid:48)) denotes achieve b; achieve b(cid:48) and the abbreviation
reach [p1; p2] denotes reach p1; reach p2. For all speciﬁcations  each Nq has two fully con-
nected hidden layers with 30 neurons each and ReLU activations  and tanh function as its output
layer. We compare our algorithm to [13] (TLTL)  which directly uses the quantitative semantics of
the speciﬁcation as the reward function (with ARS as the learning algorithm)  and to the constrained
cross entropy method (CCE) [24]  which is a state-of-the-art RL algorithm for learning policies to
perform tasks with constraints. We used neural networks with two hidden layers and 50 neurons per
layer for both the baselines.
Results. Figure 3 shows learning curves of SPECTRL (our tool)  TLTL  and CCE. In addition  it
shows SPECTRL without reward shaping (Unshaped)  which uses rewards ˜R instead of ˜Rs. These
plots demonstrate the ability of SPECTRL to outperform state-of-the-art baselines. For speciﬁcations
φ1  ...  φ5  the curve for SPECTRL gets close to 100% in all executions  and for φ6 and φ7  it gets
close to 100% in 4 out of 5 executions. The performance of CCE drops when multiple constraints
(here  obstacle and fuel) are added (i.e.  φ2). TLTL performs similar to SPECTRL on tasks φ1  φ3 and
φ4 (at least in some executions)  but SPECTRL converges faster for φ1 and φ4.

6The implementation can be found at https://github.com/keyshor/spectrl_tool.

8

Figure 4: Sample complexity curves (left) with number of nested sequencing operators on the x-axis
and average number of samples to converge on the y-axis. Learning curve for cartpole example
(right).

Since TLTL and CCE use a single neural network to encode the policy as a function of state  they
perform poorly in tasks that require memory—i.e.  φ5  φ6  and φ7. For example  to satisfy φ5  the
action that should be taken at s = (5  0) depends on whether (5  10) has been visited. In contrast 
SPECTRL performs well on these tasks since its policy is based on the monitor state.
These results also demonstrate the importance of reward shaping. Without it  ARS cannot learn unless
it randomly samples a policy that reaches ﬁnal monitor state. Reward shaping is especially important
for speciﬁcations that include many sequencing operators (φ; φ(cid:48))—i.e.  speciﬁcations φ5  φ6  and φ7.
Figure 4 (left) shows how sample complexity grows with the number of nested sequencing operators
(φ1  φ3  φ5  φ6  φ7). Each curve indicates the average number of samples needed to learn a policy
that achieves a satisfaction probability ≥ τ. SPECTRL scales well with the size of the speciﬁcation.
Cartpole. Finally  we applied SPECTRL to a different control task—namely  to learn a policy for the
version of cart-pole in OpenAI Gym  in which we used continuous actions instead of discrete actions.
The speciﬁcation is to move the cart to the right and move back left without letting the pole fall. The
formal speciﬁcation is given by

φ = achieve (reach 0.5; reach 0.0) ensuring balance

where the predicate balance holds when the vertical angle of the pole is smaller than π/15 in
absolute value. Figure 4 (right) shows the learning curve for this task averaged over 3 runs of the
algorithm along with the three baselines. TLTL is able to learn a policy to perform this task  but it
converges slower than SPECTRL; CCE is unable to learn a policy satisfying this speciﬁcation.

5 Conclusion

We have proposed a language for formally specifying control tasks and an algorithm to learn policies
to perform tasks speciﬁed in the language. Our algorithm ﬁrst constructs a task monitor from the
given speciﬁcation  and then uses the task monitor to assign shaped rewards to runs of the system.
Furthermore  the monitor state is also given as input to the controller  which enables our algorithm
to learn policies for non-Markovian speciﬁcations. Finally  we implemented our approach in a tool
called SPECTRL  which enables the users to program what the agent needs to do at a high level;
then  it automatically learns a policy that tries to best satisfy the user intent. We also demonstrate
that SPECTRL can be used to learn policies for complex speciﬁcations  and that it can outperform
state-of-the-art baselines.
Acknowledgements. We thank the reviewers for their insightful comments. This work was partially
supported by NSF by grant CCF 1723567 and by AFRL and DARPA under Contract No. FA8750-18-
C-0090.

References
[1] Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning.

In Proceedings of the 21st International Conference on Machine Learning  pages 1–8  2004.

9

[2] Jacob Andreas  Dan Klein  and Sergey Levine. Modular multitask reinforcement learning with
policy sketches. In Proceedings of the 34th International Conference on Machine Learning 
pages 166–175  2017.

[3] Jacob Andreas  Marcus Rohrbach  Trevor Darrell  and Dan Klein. Neural module networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  pages
39–48  2016.

[4] Roderick Bloem  Krishnendu Chatterjee  and Barbara Jobstmann. Graph games and reactive

synthesis. In Handbook of Model Checking  pages 921–962. Springer  2018.

[5] Steve Collins  Andy Ruina  Russ Tedrake  and Martijn Wisse. Efﬁcient bipedal robots based on

passive-dynamic walkers. Science  307(5712):1082–1085  2005.

[6] Samuel Coogan  Ebru Aydin Gol  Murat Arcak  and Calin Belta. Trafﬁc network control from
temporal logic speciﬁcations. IEEE Transactions on Control of Network Systems  3(2):162–172 
2015.

[7] Jyotirmoy V. Deshmukh  Alexandre Donzé  Shromona Ghosh  Xiaoqing Jin  Garvit Juniwal 
and Sanjit A. Seshia. Robust online monitoring of signal temporal logic. Formal Methods in
System Design  51(1):5–30  2017.

[8] Georgios E. Fainekos and George J. Pappas. Robustness of temporal logic speciﬁcations for
continuous-time signals. Theoretical Computer Science  410(42):4262–4291  September 2009.

[9] Keliang He  Morteza Lahijanian  Lydia E. Kavraki  and Moshe Y. Vardi. Reactive synthesis for
ﬁnite tasks under resource constraints. In IEEE/RSJ International Conference on Intelligent
Robots and Systems  pages 5326–5332  2017.

[10] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in

Neural Information Processing Systems  pages 4565–4573  2016.

[11] Rodrigo Toro Icarte  Toryn Klassen  Richard Valenzano  and Sheila McIlraith. Using reward
machines for high-level task speciﬁcation and decomposition in reinforcement learning. In
Proceedings of the 35th International Conference on Machine Learning  pages 2112–2121 
2018.

[12] Hadas Kress-Gazit  Georgios E. Fainekos  and George J. Pappas. Temporal-logic-based reactive

mission and motion planning. IEEE Transactions on Robotics  25(6):1370–1381  2009.

[13] Xiao Li  Cristian-Ioan Vasile  and Calin Belta. Reinforcement learning with temporal logic
rewards. In IEEE/RSJ International Conference on Intelligent Robots and Systems  pages
3834–3839  2017.

[14] Oded Maler and Dejan Nivckovi´c. Monitoring properties of analog and mixed-signal circuits.

International Journal on Software Tools for Technology Transfer  15(3):247–268  2013.

[15] Horia Mania  Aurelia Guy  and Benjamin Recht. Simple random search of static linear policies
In Advances in Neural Information Processing

is competitive for reinforcement learning.
Systems  pages 1800–1809  2018.

[16] Volodymyr Mnih  Koray Kavukcuoglu  David Silver  Andrei A Rusu  Joel Veness  Marc G
Bellemare  Alex Graves  Martin Riedmiller  Andreas K Fidjeland  Georg Ostrovski  et al.
Human-level control through deep reinforcement learning. Nature  518(7540):529  2015.

[17] Andrew Y. Ng  Daishi Harada  and Stuart J. Russell. Policy invariance under reward transfor-
mations: Theory and application to reward shaping. In Proceedings of the 16th International
Conference on Machine Learning  pages 278–287  1999.

[18] Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning.

In
Proceedings of the 17th International Conference on Machine Learning  pages 663–670  2000.

[19] Amir Pnueli. The temporal logic of programs. In Proceedings of the 18th Annual Symposium

on Foundations of Computer Science  pages 46–57  1977.

10

[20] Stéphane Ross  Geoffrey Gordon  and Drew Bagnell. A reduction of imitation learning and
structured prediction to no-regret online learning. In Proceedings of the 14th International
Conference on Artiﬁcial Intelligence and Statistics  pages 627–635  2011.

[21] David Silver  Guy Lever  Nicolas Heess  Thomas Degris  Daan Wierstra  and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference
on Machine Learning  pages 387–395  2014.

[22] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press 

2018.

[23] M.Y. Vardi and P. Wolper. Reasoning about inﬁnite computations. Information and Computation 

115(1):1–37  1994.

[24] Min Wen and Ufuk Topcu. Constrained cross-entropy method for safe reinforcement learning.

In Advances in Neural Information Processing Systems  pages 7450–7460  2018.

[25] Tichakorn Wongpiromsarn  Ufuk Topcu  and Richard M. Murray. Receding horizon temporal

logic planning. IEEE Transactions on Automatic Control  57(11):2817–2830  2012.

[26] Brian D Ziebart  Andrew L Maas  J Andrew Bagnell  and Anind K Dey. Maximum entropy
inverse reinforcement learning. In Proceedings of the 23rd AAAI Conference on Artiﬁcial
Intelligence  pages 1433–1438  2008.

11

,Kishor Jothimurugan
Rajeev Alur
Osbert Bastani