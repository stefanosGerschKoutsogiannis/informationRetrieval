2019,The Implicit Bias of AdaGrad on Separable Data,We study the implicit bias of AdaGrad on separable linear classification problems. 
We show that AdaGrad converges to a direction that can be  characterized as the solution of a quadratic optimization problem with the same feasible set as the hard SVM problem. 
We also give a discussion about how different choices of the hyperparameters of AdaGrad may impact this direction. 
This provides a deeper understanding of why adaptive methods do not seem to have the generalization ability as good as gradient descent does in practice.,The Implicit Bias of AdaGrad on Separable Data

Qian Qian

Department of Statistics
Ohio State University

Columbus  OH 43210  USA

qian.216@osu.edu

Xiaoyuan Qian

School of Mathematical Sciences
Dalian University of Technology
Dalian  Liaoning 116024  China

xyqian@dlut.edu.cn

Abstract

We study the implicit bias of AdaGrad on separable linear classiﬁcation problems.
We show that AdaGrad converges to a direction that can be characterized as the
solution of a quadratic optimization problem with the same feasible set as the
hard SVM problem. We also give a discussion about how different choices of the
hyperparameters of AdaGrad might impact this direction. This provides a deeper
understanding of why adaptive methods do not seem to have the generalization
ability as good as gradient descent does in practice.

1

Introduction

In recent years  implicit regularization from various optimization algorithms plays a crucial role
in the generalization abilities in training deep neural networks (Salakhutdinov and Srebro [2015] 
Neyshabur et al. [2015]  Keskar et al. [2016]  Neyshabur et al. [2017]  Zhang et al. [2017]). For
example  in underdetermined problems where the number of parameters is larger than the number
of training examples  many global optima fail to exhibit good generalization properties  however  a
speciﬁc optimization algorithm (such as gradient descent) does converge to a particular optimum that
generalize well  although no explicit regularization is enforced when training the model. In other
words  the optimization technique itself "biases" towards a certain model in an implicit way (Soudry
et al. [2018]). This motivates a line of works to investigate the implicit biases of various algorithms
(Telgarsky [2013]  Soudry et al. [2018]  Gunasekar et al. [2017  2018a b]).

The choice of algorithms would affect the implicit regularization introduced in the learned
models. In underdetermined least squares problems  where the minimizers are ﬁnite  we know that
gradient descent yields the minimum L2 norm solution  whereas coordinate descent might give a
different solution. Another example is logistic regression with separable data. While gradient descent
converges in the direction of the hard margin support vector machine solution (Soudry et al. [2018]) 
coordinate descent converges to the maximum L1 margin solution (Telgarsky [2013]  Gunasekar
et al. [2018a]). Unlike the squared loss  the logistic loss does not admit a ﬁnite global minimizer
on separable data: the iterates will diverge in order to drive the loss to zero. As a result  instead of
characterizing the convergence of the iterates w(t)  it is the asymptotic direction of these iterates i.e. 
limt→∞ w(t)/(cid:107)w(t)(cid:107) that is important and therefore has been characterized (Soudry et al. [2018] 
Gunasekar et al. [2018b]).

Morevoer  it has attracted much attention that different adaptive methods of gradient descent and
hyperparameters of an adaptive method exhibit different biases  thus leading to different generalization
performance in deep learning (Salakhutdinov and Srebro [2015]  Keskar et al. [2016]  Wilson et al.
[2017]  Hoffer et al. [2017]). Among those ﬁndings is that the vanilla SGD algorithm demonstrates
better generalization than its adaptive variants (Wilson et al. [2017])  such as AdaGrad (Duchi et al.
[2010]) and Adam (Kingma and Ba [2015]). Therefore it is important to precisely characterize how
different adaptive methods induce difference biases. A natural question to ask is: can we explain
this observation by characterizing the implicit bias of AdaGrad  which is a paradigm of adaptive

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

methods  in a binary classiﬁcation setting with separable data using logistic regression? And how
does the implicit bias depend on the choice of the hyperparameters of this speciﬁc algorithm  such as
initialization  step sizes  etc?

1.1 Our Contribution

In this work we study Adagrad applied to logistic regression with separable data. Our contribution is
three-fold as listed as follows.

always converge.

• We prove that the directions of AdaGrad iterates  with a constant step size sufﬁciently small 
• We formulate the asymptotic direction as the solution of a quadratic optimization problem.
This achieves a theoretical characterization of the implicit bias of AdaGrad  which also
provides insights about why and how the factors involved  such as certain intrinsic properties
of the dataset  the initialization and the learning rate  affect the implicit bias.

• We introduce a novel approach to study the bias of AdaGrad.

It is mainly based on
a geometric estimation on the directions of the updates  which doesn’t depend on any
calculation on the convergence rate.

1.2 Paper Organization

This paper is organized as follows. In Section 2 we explain our problem setup. The main theory is
developed in Section 3  including convergence of the adaptive learning rates of AdaGrad  existence
of the asymptotic direction of AdaGrad iterates  and relations between the asymptotic directions of
Adagrad and gradient descent iterates. We conclude our paper in Section 4 with a review of our
results and some questions left to future research.

Problem Setup

2
Let {(xn  yn) : n = 1 ···   N} be a training dataset with features xn ∈ Rp and labels yn ∈
{−1  1} . To simplify the notation  we redeﬁne ynxn as xn   n = 1 ···   N  and consider learning
the logistic regression model over the empirical loss:

N(cid:88)

l(cid:0)wT xn

(cid:1)   w ∈ Rp  

L(w) =

where l : Rp → R . We focus on the following case  same as proposed in Soudry et al. [2018]:

n=1

Assumption 1. There exists a vector w∗ such that wT∗ xn > 0 for all n.
Assumption 2. l(u) is continuously differentiable  β−smooth  and strictly decreasing to zero.
Assumption 3. There exist positive constants a  b  c  and d such that

(cid:12)(cid:12)l(cid:48)(u) + ce−au(cid:12)(cid:12) ≤ e−(a+b)u  

for u > d .

It is easy to see that the exponential loss l(u) = e−u and the logistic loss l(u) = log (1 + e−u)

Given two hyperparameters    η > 0 and an initial point w(0) ∈ Rp   we consider the diagonal

w(t + 1) = w(t) − ηh(t) (cid:12) g(t)  

t = 0  1  2 ···  

(1)

both meet these assumptions.

AdaGrad iterates

where

hi(t) =

g(t) = (g1(t) ···   gp(t))  

gi(t) =

(w(t))  

h(t) = (h1(t) ···   hp(t))  

(cid:112)gi(0)2 + ··· + gi(t)2 + 

1

∂L
∂wi

2

i = 1 ···   p  

 

and (cid:12) is the element-wise multiplication of two vectors  e.g.

a (cid:12) b = (a1b1 ···   apbp)T

for a = (a1 ···   ap)T   b = (b1 ···   bp)T .

To analyze the convergence of the algorithm  we put an additional restriction on the hyperparam-

eter η .

Assumption 4. The hyperparameter η is not too large; speciﬁcally 

2 mini∈{1 ···  p}(cid:112)gi(0)2 + 

η <

β

.

(2)

We are interested in the asymptotic behavior of the AdaGrad iteration scheme in (1). The main

problem is: does there exists some vector wA such that

t→∞ w(t)/(cid:107)w(t)(cid:107) = wA ?

lim

We will provide an afﬁrmative answer to this question in the following section.

3 The Asymptotic Direction of AdaGrad Iterates

3.1 Convergence of the Adaptive Learning Rates

We ﬁrst provide some elementary facts about AdaGrad iterates (1) with all assumptions (1-4) proposed
in Section 2.
Lemma 3.1. L (w(t + 1)) < L (w(t)) ( t = 0  1 ··· ) .

Lemma 3.2. (cid:80)∞

t=0 (cid:107)g(t)(cid:107)2 < ∞ .

We notice that Gunasekar et al. [2018a] showed a similar result (Lemma 6  in Section 3.3 of
their work) for exponential loss only  under slightly different assumptions. However  their approach
depends on some speciﬁc properties of the exponential function  and thus cannot be extended to
Lemma 3.2 in a trivial manner.
Lemma 3.3. The following statements hold:

(i) (cid:107)g(t)(cid:107) → 0 (t → ∞).
(ii) (cid:107)w(t)(cid:107) → ∞ (t → ∞).
(iii) L(w(t)) → 0 (t → ∞).
(iv) ∀n  
limt→∞ w(t)T xn = ∞.
(v) ∃t0   ∀ t > t0   w(t)T xn > 0.
Theorem 3.1. The sequence {h(t)}∞

satisfying h∞ i > 0 (i = 1 ···   p) .

t=0 converges as t → ∞ to a vector
h∞ = (h∞ 1 ···   h∞ p)

3.2 Convergence of the Directions of AdaGrad Iterates
In the remainder of the paper we denote h∞ = limt→∞ h(t) and ξn = h1/2∞ (cid:12)xn (n = 1 ···   N ) .
Since  by Theorem 3.1  the components of h∞ have a positive lower bound  we can deﬁne

β(t) = h−1∞ (cid:12) h(t) (t = 0  1  2 ··· ) .

Here the squared root and the inverse of vectors are deﬁned element-wise. We call the function

Lind : Rp → R   Lind(v) =

N(cid:88)

l(cid:0)vT ξn

(cid:1)

n=1

3

the induced loss with respect to AdaGrad (1). Note that

L(w) =

=

Thus if we set

(cid:1) =

l(cid:0)wT xn
(cid:18)(cid:16)

l

h−1/2∞ (cid:12) w

N(cid:88)
(cid:17)T

n=1

l

(cid:18)(cid:16)
(cid:17)T(cid:16)
h−1/2∞ (cid:12) w
(cid:19)
(cid:16)

= Lind

ξn

h1/2∞ (cid:12) xn
(cid:17)

h−1/2∞ (cid:12) w

.

(cid:17)(cid:19)

N(cid:88)
N(cid:88)

n=1

n=1

v(t) = h−1/2∞ (cid:12) w(t) (t = 0  1  2 ··· )  

then v(0) = h

−1/2∞ (cid:12) w(0)   and

h1/2∞ (cid:12) v(t + 1) = w(t + 1) = w(t) − ηh(t) (cid:12) ∇L (w(t))

= h1/2∞ (cid:12) v(t) − ηh(t) (cid:12) h−1/2∞ (cid:12) ∇L(cid:16)

(cid:17)
h−1/2∞ (cid:12)(cid:16)
(cid:16)
h1/2∞ (cid:12) v(t)
= h1/2∞ (cid:12) v(t) − ηh(t) (cid:12) h−1/2∞ (cid:12) ∇Lind
= h1/2∞ (cid:12) (v(t) − ηβ(t) (cid:12) ∇Lind (v(t)))  

(cid:17)(cid:17)
h1/2∞ (cid:12) v(t)

or

v(t + 1) = v(t) − ηβ(t) (cid:12) ∇Lind(v(t)) (t = 0  1 ··· ) .

We refer to (4) as the induced form of AdaGrad (1).

The following result for the induced form is a simple corollary of Lemma 3.3.

Lemma 3.4. The following statements hold:

(i) (cid:107)∇Lind(t)(cid:107) → 0 (t → ∞).
(ii) (cid:107)v(t)(cid:107) → ∞ (t → ∞).
(iii) Lind(v(t)) → 0 (t → ∞).
(iv) ∀n  
limt→∞ v(t)T ξn = ∞.
(v) ∃t0   ∀ t > t0   v(t)T ξn > 0.
For the induced loss Lind  consider GD iterates

u(t + 1) = u(t) − η∇Lind(u(t)) (t = 0  1 ··· ) .

According to Theorem 3 in Soudry et.al.(2018)  we have

(3)

(4)

(5)

u(t)

lim
t→∞

(cid:107)u(t)(cid:107) = (cid:98)u
(cid:107)(cid:98)u(cid:107)  
(cid:98)u = arg min

(cid:107)u(cid:107)2.

uT ξn≥1  ∀n

where

Noting that β(t) → 1 (t → ∞) we can obtain GD iterates (5) by taking the limit of β(t) in
(4). Therefore it is reasonable to expect that these two iterative processes have similar asymptotic
behaviors  especially a common limiting direction.

Different from the case of GD method discussed in Soudry et al. [2018]  however  it is difﬁcult to
obtain an effective estimation about the convergence rate of w(t). Instead  we introduce an orthogonal
decomposition approach to obtain the asymptotic direction of the original Adagrad process (1).

In the remainder of the paper  we denote by P the projection onto the 1−dimensional subspace

spanned by(cid:98)u  and Q the projection onto the orthogonal complement. Without any loss of generality
we may assume (cid:107)(cid:98)u(cid:107) = 1 . Thus we have the orthogonal decomposition
where P v = (cid:107)P v(cid:107)(cid:98)u =(cid:0)vT(cid:98)u(cid:1)(cid:98)u. Moreover  we denote

v = P v + Qv (v ∈ Rp)  

δ(t) = −η∇Lind (v(t))   d(t) = β(t) (cid:12) δ(t).

(6)

4

Using this notation we can rewrite the iteration scheme (4) as

v(t + 1) = v(t) + d(t)

(t = 0  1 ··· ).

By reformulating (6) as
where β(t) − 1 → 0 as t → ∞  we regard δ(t) as the decisive part of d(t) and acquire properties of
d(t) through exploring analogues of δ(t).
First  we can show a basic estimation:

d(t) = δ(t) + (β(t) − 1) (cid:12) δ(t)  

δ(t)T(cid:98)u = (cid:107)P δ(t)(cid:107) ≥ (cid:107)δ(t)(cid:107)

max

n

(cid:107)ξn(cid:107) (t = 0  1  2··· ) .

The projection properties of δ(t) is easily passed on to d(t). In fact  for sufﬁciently large t  

d(t)T(cid:98)u = (cid:107)P d(t)(cid:107) ≥ (cid:107)d(t)(cid:107)

(cid:107)ξn(cid:107)  

4 max

n

(7)

Inequality (7) provides a cumulative effect on the projection of v(t) as t increases:

(cid:107)P v(t)(cid:107) ≥ (cid:107)v(t)(cid:107)

(cid:107)ξn(cid:107)  

8max

n

for sufﬁciently large t .

The following lemma reveals a crucial characteristic of the iterative process (4): as t tends to

inﬁnity  the contribution of δ(t) to the increment of the deviation from the direction of(cid:98)u   compared
to its contribution to the increment in the direction of(cid:98)u  becomes more and more insigniﬁcant.

Lemma 3.5. Given ε > 0 . Let a  b  c be positive numbers as deﬁned in Assumption 3 in Section 2.
If (cid:107)Qv(t)(cid:107) > 2N (c + 1)(aceε)−1   then for sufﬁciently large t 
Qv(t)T δ(t) < ε(cid:107)Qv(t)(cid:107)(cid:107)δ(t)(cid:107) .

This property can be translated into a more convenient version for d(t).

Lemma 3.6. For any ε > 0   there exist R > 0 such that for sufﬁciently large t and (cid:107)Qv(t)(cid:107) ≥ R 

Therefore  over a long period  the cumulative increment of v(t) in the direction of (cid:98)u will

(cid:107)Qv(t + 1)(cid:107) − (cid:107)Qv(t)(cid:107) ≤ ε(cid:107)d(t)(cid:107) .

overwhelm the deviation from it  yielding the existence of an asymptotic direction for v(t).
Lemma 3.7.

By the relation (3) between v(t) and w(t)  our main result directly follows from (8).

Theorem 3.2. AdaGrad iterates (1) has an asymptotic direction:

where

lim
t→∞

v(t)

(cid:107)v(t)(cid:107) =(cid:98)u .
(cid:107)w(t)(cid:107) = (cid:101)w
(cid:107)(cid:101)w(cid:107)  
(cid:13)(cid:13)(cid:13)(cid:13) 1√

w(t)

h∞

(cid:13)(cid:13)(cid:13)(cid:13)2

.

(cid:12) w

lim
t→∞

(cid:101)w = arg min

wT xn≥1  ∀n

(8)

(9)

3.3 Factors Affecting the Asymptotic Direction

Theorem 3.2 conﬁrms that AdaGrad iterates (1) have an asymptotic direction (cid:101)w/(cid:107)(cid:101)w(cid:107)   where (cid:101)w
(cid:13)(cid:13)(cid:13)2

is
is the solution to the optimization problem (9). Since the objective function
determined by the limit vector h∞   it is easy to see that the asymptotic direction may depend on the
choices of the dataset {(xn  yn)}N
n=1   the hyperparameters    η   and the initial point w(0) . In the
following we will discuss this varied dependency in several respects.

−1/2∞ (cid:12) w

(cid:13)(cid:13)(cid:13)h

5

3.3.1 Difference from the Asymptotic Direction of GD iterates

When the classic gradient descent method is applied to minimize the same loss  it is known (see
Theorem 3  Soudry et al. [2018]) that GD iterates

have an asymptotic direction (cid:98)w/(cid:107)(cid:98)w(cid:107)  where (cid:98)w is the solution of the hard max-margin SVM

wG(t + 1) = wG(t) − η∇L (wG(t))

(t = 0  1  2 ··· )  

(10)

problem

(cid:107)w(cid:107)2 .

arg min

wT xn≥1  ∀n

(11)

The two optimization problems (9) and (11) have the same feasible set

(cid:8)w ∈ Rp : wT xn ≥ 1  for n = 1 ···   N(cid:9)  

yield different directions  as shown in the following toy example.
Example 3.1. Let x1 = (cos θ  sin θ)T and L(w) = e−wT x1 . Suppose 0 < θ < π/2 . In this

but they take on different objective functions. It is natural to expect that their solutions (cid:101)w and (cid:98)w
setting we simply have (cid:98)w = x1 . Selecting w(0) = (a  b)T and  = 0  we have
(cid:19)T

−g(0) = e−w(0)T x1x1 = e−a cos θ−b sin θ (cos θ  sin θ)T  

(cid:18) 1

h(0) = (h1(0)  h2(0))T = ea cos θ+b sin θ

1

 

cos θ

sin θ

.

In general we can show there is a sequence of positive numbers p(t) such that

−g(t) = p(t) (cos θ  sin θ)T  

and

1

Now

h∞ = lim
t→∞

(cid:112)p(0)2 + p(1)2 + ··· + p(t)2
(cid:13)(cid:13)(cid:13)h−1/2∞ (cid:12) w
(cid:13)(cid:13)(cid:13)2
(cid:101)w = arg min
2 sin θ(cid:1) =
(cid:0)w2
2/2(cid:1)T
and we have (cid:101)w/(cid:107)(cid:101)w(cid:107) = (cid:0)√

= arg min
wT x1≥1

1 cos θ + w2

wT x1≥1

2/2 

√

= arg min
wT x1≥1

(cid:18)

(cid:19)T

.

 

1

sin θ

(cid:19)T

=

(cid:18) 1

cos θ

sin θ

1

 

ρ(cid:0)w2

1
ρ

(cid:18) 1
2 sin θ(cid:1)

cos θ

1 cos θ + w2

1

cos θ + sin θ

 

1

cos θ + sin θ

(cid:19)

 

between 0 and π/2  i.e.  irrelevant to x1. These two directions coincide only when θ = π/4.

. Note that this direction is invariant when θ ranges

Sensitivity to Small Coordinate System Rotations

direction (cid:101)w/(cid:107)(cid:101)w(cid:107) will become (cid:0)−√

3.3.2
If we consider the same setting as in Example 3.1  but taking θ ∈ (π/2  π) . Then the asymptotic
. This implies  however  if x1 is close to the
direction of y−axis  then a small rotation of the coordinate system may result in a large change of
the asymptotic direction reaching a right angle  i.e.  in this case the asymptotic direction is highly
unstable even for a small perturbation of its x−coordinate.

2/2(cid:1)T

2/2 

√

3.3.3 Effects of the Initialization and Hyperparameter η

It is reasonable to believe that the asymptotic direction of AdaGrad depends on the initial condi-
tions  including initialization and step size (see Section 3.3  Gunasekar et al. [2018a]). Theorem
3.2 yields a geometric interpretation for this dependency as shown in Figure 1  where the red
arrows indicate x1 = (cos (3π/8)   sin (3π/8)) and x2 = (cos (9π/20)   sin (9π/20))  and the
cyan arrow indicates the max-margin separator  which points at m  the corner of the feasible set

(cid:8)w(cid:12)(cid:12) wT xn ≥ 1  ∀n = 1  2(cid:9) (the yellow shadowed area).

6

(cid:13)(cid:13)(cid:13)h

−1/2∞ (cid:12) w

(cid:13)(cid:13)(cid:13)2

Figure 1: A case that the asymptotic directions of AdaGrad and GD are different.

Since the isolines of the function

are ellipses (the green dashed curves) centered
at the origin  the unique minimizer of the function in the feasible set must be the tangency point p
(pointed at by the magenta arrow) between the tangent ellipse and the boundary of the feasible set. If
h∞ varies  then the eccentricity of the tangent ellipses may change. It makes the tangency point
move along the boundary  indicating the change of the asymptotic direction.

Numerical simulations also reveal the differences among the asymptotic directions of Ada-
Grad iterates with various learning rates  as shown in Figure 2. On the left-hand diagram 
x1 = (cos (π/8)   sin (π/8)) and x2 = (cos (π/20)   sin (π/20)) are two support vectors. dm
denotes the direction of the max-margin separator. d01 and d05 denote the directions of AdaGrad
iterates computed after 108 steps  with η = 0.1 and 0.5  respectively. The small angle between the
two may indicates that the asymptotic direction depend on η. However  all the asymptotic directions
apparently diverge from the max-margin separator. On the right-hand diagram  the red and blue
curves plot
It illustrates that the two sequences of the directions of AdaGrad iterates slowly converge to their
own asymptotic directions  slightly different from each other.

(cid:13)(cid:13)(cid:13) vs. the number of the iterates with η = 0.1 and 0.5  respectively.

(cid:13)(cid:13)(cid:13)w(t)/(cid:107)w(t)(cid:107) − dm

Figure 2: Numerical simulations with η = 0.1 and 0.5.

3.3.4 Cases that the Asymptotic Direction is Stable

Above we have observed that the asymptotic direction of AdaGrad iterates can be very different
from the asymptotic direction of GD iterates  which is robust with respect to different choices of
initialization and learning rate η . It is natural to ask what are the conditions under which the two
asymptotic directions coincide. The following proposition provides a sufﬁcient one.

7

Proposition 3.1. Let a = (a1 ···   ap)T be a vector satisfying aT xn ≥ 1 (n = 1 ···   N ) and
a1 ··· ap (cid:54)= 0 . Suppose that w = (w1 ···   wp)T satisﬁes wT xn ≥ 1 (n = 1 ···   N ) and

ai (wi − ai) ≥ 0 (i = 1 ···   p) .

Then for any b = (b1 ···   bp)T such that b1 ··· bp (cid:54)= 0  

arg min

wT xn≥1  ∀n

(cid:107)b (cid:12) w(cid:107)2 = arg min
wT xn≥1  ∀n

(cid:107)w(cid:107)2 = a  

and therefore the asymptotic directions of AdaGrad (1) and GD (10) are equal.

Such a condition seems at ﬁst sight quite harsh to be satiﬁed. However  there is a signiﬁcant
proportion of the chances that a dataset {xn : n = 1 ···   N} meets the requirement  as shown in
the following result.
Proposition 3.2. Suppose N ≥ p and X = [x1 ···   xN ] ∈ Rp×N is sampled from any distri-
bution whose density function is nonzero almost everywhere. Then with a positive probability the
asymptotic directions of AdaGrad (1) and GD (10) are equal.

Example 3.2. Let r1  r2 > 0 

x1 = r1 (cos θ1  sin θ1)T  
x2 = r2 (cos θ2  sin θ2)T   θ1 − π < θ2 ≤ 0  

≤ θ1 < π  

π
2

and L(w) = l(wT x1) + l(wT x2) . The system of equations
wT xi = 1 (i = 1  2)

has a unique solution (α  β)T   where

r−1

2

α =

sin θ1 − r−1
sin (θ1 − θ2)

1

sin θ2

> 0  β =

1 cos θ2 − r−1
r−1
sin (θ1 − θ2)

2 cos θ1

> 0 .

It is easy to check that if w = (w1  w2)T satisﬁes wT xi ≥ 1 (i = 1  2)   then w1 ≥ α   w2 ≥ β.
Thus any quadratic form b1w2
2 (b1  b2 > 0) takes its minimum at (α  β)T over the feasible

set(cid:8)w : wT xi ≥ 1 (i = 1  2)(cid:9). Hence the asymptotic direction of AdaGrad (1) applying to this
problem is always equal to (α  β)T(cid:14)(cid:107)(α  β)(cid:107)  which is also the asymptotic direction of GD (10).

1 + b2w2

A geometric perspective of this example is given in Figure 2  where the red arrows indicate
x1 = (cos (5π/8)   sin (5π/8)) and x2 = (cos (−π/8)   sin (−π/8))   and the magenta arrow
indicates (α  β)T . It is easy to see that the isoline (the thick ellipse drawn in green) along which the
function
equals its minimum must intersect with the feasible set (the grey shadowed
area) at the corner (α  β)T   no matter what h∞ is.

−1/2∞ (cid:12) w

(cid:13)(cid:13)(cid:13)h

(cid:13)(cid:13)(cid:13)2

Figure 3: A case that the asymptotic directions of AdaGrad and GD are equal.

8

4 Conclusion

We proved that the basic diagonal AdaGrad  when minimizing a smooth monotone loss function
with an exponential tail  has an asymptotic direction  which can be characterized as the solution
of a quadratic optimization problem. In this respect AdaGrad is similar to GD  even though their
asymptotic directions are usually different. The difference between them also lies in the stability of
their asymptotic directions. The asymptotic direction of GD is uniquely determined by the predictors
xn’s and independent of initialization and the learning rate  as well as the rotation of coordinate
system  while the asymptotic direction of AdaGrad is likely to be affected by those factors.

In spite of all these ﬁndings  we still do not know whether the asymptotic direction of AdaGrad
will change for various initialization or different learning rates. Furthermore  we hope our approach
can be applied to the research on the implicit biases of other adaptive methods such as AdaDelta 
RMSProp  and Adam.

References
B. Neyshaburand R. R. Salakhutdinov and N. Srebro. Path-sgd: Path-normalized optimization in
deep neural networks. In Advances in Neural Information Processing Systems  page 2422–2430 
2015.

B. Neyshabur  R. Tomioka  and N. Srebro. In search of the real inductive bias: On the role of implicit
regularization in deep learning. In International Conference on Learning Representations  2015.

Nitish Shirish Keskar  Dheevatsa Mudigere  Jorge Nocedal  Mikhail Smelyanskiy  and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR 
2016.

B. Neyshabur  R. Tomioka  R. Salakhutdinov  and N. Srebro. Geometry of optimization and implicit

regularization in deep learning  2017. URL https://arxiv.org/pdf/1705.03071.pdf.

C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires

rethinking generalization. In International Conference on Learning Representations  2017.

D. Soudry  E. Hoffer  M. S. Nacson  S. Gunasekar  and N. Srebro. The implicit bias of gradient

descent on separable data  2018.

M. Telgarsky. Margins  shrinkage and boosting. Proceedings of the 30th International Conference on

Machine Learning  PMLR  28(2):307–315  2013.

Suriya Gunasekar  Blake E. Woodworth  Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro.

Implicit regularization in matrix factorization  2017.

Suriya Gunasekar  Jason Lee  Daniel Soudry  and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In Proceedings of the 35th International Conference on Machine
Learning  2018a.

S. Gunasekar  J. Lee  D. Soudry  and N. Srebro. Implicit bias of gradient descent on linear convo-
lutional networks. In Proceedings of the 35th International Conference on Machine Learning 
2018b.

Ashia C. Wilson  Rebecca Roelofs  Mitchell Stern  Nathan Srebro  and Benjamin Recht. The marginal

value of adaptive gradient methods in machine learning. arXiv  pages 1–14  2017.

E. Hoffer  I. Hubara  and D. Soudry. Train longer  generalize better: closing the generalization gap in
large batch training of neural networks. In Advances in Neural Information Processing Systems 
page 1–13  2017.

John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121 – 2159  2010.

Diederik P. Kingma and Jimmy Lei Ba. Adam: a method for stochastic optimization. International

Conference on Learning Representations  pages 1–13  2015.

9

,Qian Qian
Xiaoyuan Qian