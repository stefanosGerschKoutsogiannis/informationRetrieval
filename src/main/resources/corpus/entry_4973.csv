2011,Semi-supervised Regression via Parallel Field Regularization,This paper studies the problem of semi-supervised learning from the vector field perspective. Many of the existing work use the graph Laplacian to ensure the smoothness of the prediction function on the data manifold. However  beyond smoothness  it is suggested by recent theoretical work that we should ensure second order smoothness for achieving faster rates of convergence for semi-supervised regression problems. To achieve this goal  we show that the second order smoothness measures the linearity of the function  and the gradient field of a linear function has to be a parallel vector field. Consequently  we propose to find a function which minimizes the empirical error  and simultaneously requires its gradient field to be as parallel as possible. We give a continuous objective function on the manifold and discuss how to discretize it by using random points. The discretized optimization problem turns out to be a sparse linear system which can be solved very efficiently. The experimental results have demonstrated the effectiveness of our proposed approach.,Semi-supervised Regression via
Parallel Field Regularization

Binbin Lin

State Key Lab of CAD&CG  College of Computer Science  Zhejiang University
{binbinlinzju  chiyuan.zhang.zju  xiaofeihe}@gmail.com

Hangzhou 310058  China

Chiyuan Zhang

Xiaofei He

Abstract

This paper studies the problem of semi-supervised learning from the vector ﬁeld
perspective. Many of the existing work use the graph Laplacian to ensure the
smoothness of the prediction function on the data manifold. However  beyond
smoothness  it is suggested by recent theoretical work that we should ensure
second order smoothness for achieving faster rates of convergence for semi-
supervised regression problems. To achieve this goal  we show that the second
order smoothness measures the linearity of the function  and the gradient ﬁeld of
a linear function has to be a parallel vector ﬁeld. Consequently  we propose to
ﬁnd a function which minimizes the empirical error  and simultaneously requires
its gradient ﬁeld to be as parallel as possible. We give a continuous objective
function on the manifold and discuss how to discretize it by using random points.
The discretized optimization problem turns out to be a sparse linear system which
can be solved very efﬁciently. The experimental results have demonstrated the
effectiveness of our proposed approach.

1

Introduction

In many machine learning problems  one is often confronted with very high dimensional data. There
is a strong intuition that the data may have a lower dimensional intrinsic representation. Various
researchers have considered the case when the data is sampled from a submanifold embedded in
the ambient Euclidean space. Consequently  learning with the low dimensional manifold structure 
or speciﬁcally the intrinsic topological and geometrical properties of the data manifold  becomes a
crucial problem.
In the past decade  many geometrically motivated approaches have been developed. The early work
mainly considers the problem of dimensionality reduction. One hopes that the manifold structure
could be preserved in the much lower dimensional Euclidean space. For example  ISOMAP [1] is
a global approach which tries to preserve the pairwise geodesic distance on the manifold. Different
from ISOMAP  Hessian Eigenmaps (HLLE  [2]) is a local approach for similar purpose. Locally
Linear Embedding (LLE  [3]) and Laplacian Eigenmaps (LE  [4]) can be viewed as Laplacian oper-
ator based methods which mainly consider the local neighborhood structure of the manifold.
Besides dimensionality reduction  Laplacian based regularization has also been widely employed
in semi-supervised learning. These methods construct a nearest neighbor graph over the labeled
and unlabeled data to model the underlying manifold structure  and use the graph Laplacian [5]
to measure the smoothness of the learned function on the manifold. A variety of semi-supervised
learning approaches using the graph Laplacian have been proposed [6  7  8]. In semi-supervised
regression  some recent theoretical analysis [9] shows that using the graph Laplacian regularizer
does not lead to faster minimax rates of convergence. [9] also states that the Laplacian regularizer
is way too general for measuring the smoothness of the function. It is further suggested that we

1

should ensure second order smoothness to achieve faster rates of convergence for semi-supervised
regression problems. The Laplacian regularizer is the integral on the norm of the gradient of the
function  which is the ﬁrst order derivative on the function.
In this paper  we design regularization terms that penalize the second order smoothness of the func-
tion  i.e.  the linearity of the function. Estimating the second order covariant derivative of the func-
tion is a very challenging problem. We try to address this problem from vector ﬁelds perspective.
We show that the gradient ﬁeld of a linear function has to be a parallel vector ﬁeld (or parallel ﬁeld
in short). Consequently  we propose a novel approach called Parallel Field Regularization (PFR)
to simultaneously ﬁnd the function and its gradient ﬁeld  while requiring the gradient ﬁeld to be as
parallel as possible. Speciﬁcally  we propose to compute a function and a vector ﬁeld which satisfy
three conditions simultaneously: 1) the function minimizes the empirical error on the labeled data 
2) the vector ﬁeld is close to the gradient ﬁeld of the function  3) the vector ﬁeld should be as par-
allel as possible. A novel regularization framework from the vector ﬁled perspective is developed.
We give both the continuous and discrete forms of the objective function  and develop an efﬁcient
optimization scheme to solve this problem.

2 Regularization on the Vector Field

We ﬁrst brieﬂy introduce semi-supervised learning methods with regularization on the function.
i=1 on M  we
Let M be a d-dimensional submanifold in Rm. Given l labeled data points (xi  yi)l
aim to learn a function f : M → R based on the manifold M and the labeled points (xi  yi)l
i=1. A
framework of semi-supervised learning based on differential operators can be formulated as follows:

arg min
f∈C∞(M)

E(f ) =

1
l

R0(f (xi)  yi) + λ1R1(f )

with a differential operator  i.e.  R1(f ) = (cid:82)
the covariant derivative ∇ on the manifold  then R1(f ) = (cid:82)
Laplacian regularizer. If D is the Hessian operator on the manifold  then R1(f ) = (cid:82)

where C∞(M) denotes smooth functions on M  R0 : R × R → R is the loss function and R1(f ) :
C∞(M) → R is a regularization functional. R1 is often written as a functional norm associated
M (cid:107)Df(cid:107)2 where D is a differential operator. If D is
M f L(f ) becomes the
M (cid:107)Hessf(cid:107)2

M (cid:107)∇f(cid:107)2 = (cid:82)

becomes the Hessian regularizer.

l(cid:88)

i=1

2.1 Parallel Fields and Linear Functions

We ﬁrst show the relationship between a parallel ﬁeld and a linear function on the manifold.
Deﬁnition 2.1 (Parallel Field [10]). A vector ﬁeld X on manifold M is a parallel ﬁeld if

∇X ≡ 0 

where ∇ is the covariant derivative on M.
Deﬁnition 2.2 (Linear Function [10]). A continuous function f : M → R is said to be linear if

(f ◦ γ)(t) = f (γ(0)) + ct

(1)

for each geodesic γ.

A function f is linear means that it varies linearly along the geodesics of the manifold. It is a natural
extension of linear functions on Euclidean space.
Proposition 2.1. [10] Let V be a parallel ﬁeld on the manifold. If it is also a gradient ﬁeld for
function f  V = ∇f  then f is a linear function on the manifold.
This proposition tells us the relationship between a parallel ﬁeld and a linear function on the mani-
fold.

2.2 Objective Function

We aim to design regularization terms that penalize the second order smoothness of the function.
Following the above analysis  we ﬁrst approximate gradient ﬁeld of the prediction function by a

2

Figure 1: Covariant derivative demonstration. Let
V  Y be two vector ﬁelds on manifold M. Given
a point x ∈ M  we show how to compute the
vector ∇Y V |x. Let γ(t) be a curve on M:
γ : I → M which satisﬁes γ(0) = x and
γ(cid:48)(0) = Yx. Then the covariant derivative along
|t=0 can be computed by pro-
the direction dγ(t)
dt
dt |t=0 to the tangent space TxM at x.
jecting dV
dt |t=0)  where
In other words  ∇γ(cid:48)(0)V |x = Px( dV
Px : v ∈ Rm → Px(v) ∈ TxM is the projection
matrix. It is not difﬁcult to check that the compu-
tation of ∇Y V |x is independent to the choice of
the curve γ.

vector ﬁeld  then we require the vector ﬁeld to be as parallel as possible. Therefore  we try to learn
the function f and its gradient ﬁeld ∇f simultaneously. Formally  we propose to learn a function f
and a vector ﬁeld V on the manifold with two constraints:

• The vector ﬁeld V should be close to the gradient ﬁeld ∇f of f  which can be formularized

as follows:

M
• The vector ﬁeld V should be as parallel as possible:

min

f∈C∞ V

R1(f  V ) =

(cid:107)∇f − V (cid:107)2

min

V

R2(V ) =

M

(cid:107)∇V (cid:107)2

F

(cid:90)
(cid:90)

where ∇ is the covariant derivative on the manifold  (cid:107) · (cid:107)F denotes the Frobenius norm.

In the following  we provide some detailed explanation of R2(V ). ∇V measures the change of the
vector ﬁeld V . If ∇V vanishes  then V is a parallel ﬁeld. For a given direction Yx at x ∈ M  the
geometrical meaning of ∇Y V |x is demonstrated in Fig. 1. For a ﬁxed point x ∈ M  ∇V |x is a
linear map on the tangent space TxM. According to the deﬁnition of Frobenius norm  we have

d(cid:88)

(cid:107)∇V (cid:107)2

F =

(g(∇∂iV  ∂j))2 =

i j=1

i=1

d(cid:88)
(g(∇∂iV ∇∂iV ))

where g is the Riemannian metric on M and ∂1  . . .   ∂d is an orthonormal basis of TxM.
Naturally  we propose the following objective function based on vector ﬁeld regularization:

arg min
f∈C∞(M) V

E(f  V ) =

1
l

R0(xi  yi  f ) + λ1R1(f  V ) + λ2R2(V )

For the loss function R0  we use the squared loss R0(f (xi)  yi) = (f (xi) − yi)2 for simplicity.

Implementation

3
Since the manifold M is unknown  the function f which minimizes (5) can not be directly solved.
In this section  we discuss how to discretize the continuous objective function (5).

3.1 Vector Field Representation

i=1 and n − l unlabeled points xl+1  . . .   xn in Rm. Let fi =
Given l labeled data points (xi  yi)l
f (xi)  i = 1  . . .   n  our goal is to learn a function f = (f1  . . .   fn)T . We ﬁrst construct a nearest
neighbor graph by either -neighborhood or k nearest neighbors. Let xi ∼ xj denote that xi and

3

(2)

(3)

(4)

(5)

l(cid:88)

i=1

xj are neighbors. For each point xi  we estimate its tangent space TxiM by performing PCA on its
neighborhood. We choose the largest d eigenvectors as the bases since TxiM is d dimensional. Let
Ti ∈ Rm×d be the matrix whose columns constitute an orthonormal basis for TxiM. It is easy to
is the unique orthogonal projection from Rm onto the tangent space TxiM
show that Pi = TiT T
[11]. That is  for any vector a ∈ Rm  we have Pia ∈ TxiM and (a − Pia) ⊥ Pia.
i
Let V be a vector ﬁeld on the manifold. For each point xi  let Vxi denote the value of the vector ﬁeld
V at xi  and ∇V |xi denote the value of ∇V at xi. According to the deﬁnition of vector ﬁeld  Vxi
should be a vector in tangent space TxiM. Therefore  it can be represented by the local coordinates
V is a dn-dimensional big column vector which concatenates all the vi’s. In the following  we ﬁrst
discretize our objective function E(f  V )  and then minimize it to obtain f and V.

of the tangent space  Vxi = Tivi  where vi ∈ Rd. We deﬁne V =(cid:0)vT

(cid:1)T ∈ Rdn. That is 

1   . . .   vT
n

3.2 Gradient Field Computation

In order to discretize R1(f  V )  we ﬁrst discuss the Taylor expansion of f on the manifold. Let expx
denote the exponential map at x. The exponential map expx : TxM → M maps the tangent space
TxM to the manifold M. Let a ∈ TxM be a tangent vector. Then there is a unique geodesic γa
satisfying γa(0) = x with the initial tangent vector γ(cid:48)
a(0) = a. The corresponding exponential map
is deﬁned as expx(ta) = γa(t)  t ∈ [0  1]. Locally  the exponential map is a diffeomorphism.
Note that f ◦expx : TxM → R is a smooth function on TxM. Then the following Taylor expansion
of f holds:
(6)
where a ∈ TxM is a sufﬁciently small tangent vector. In the discrete case  let expxi denote the
exponential map at xi. Since expxi is a diffeomorphism  there exists a tangent vector aij ∈ TxiM
such that expxi(aij) = xj. According to the deﬁnition of exponential map  (cid:107)aij(cid:107) equals to the
geodesic distance between xi and xj  which can be denoted as dij. Let eij be the unit vector in the
direction of aij  i.e.  eij = aij/dij. We approximate aij by projecting the vector xj − xi to the
tangent space  i.e.  aij = Pi(xj − xi). Therefore  Eq. (6) can be rewritten as follows:

f (expx(a)) ≈ f (x) + (cid:104)∇f (x)  a(cid:105) 

f (xj) = f (xi) + (cid:104)∇f (xi)  Pi(xj − xi)(cid:105)

(7)
Since f is unknown  ∇f is also unknown. In the following  we discuss how to compute (cid:107)∇f (xi) −
Vxi(cid:107)2 discretely. We ﬁrst show that the vector norm can be computed by an integral on a unit sphere 
where the unit sphere can be discretely approximated by a neighborhood.
Let u be a unit vector on tangent space TxM  then we have (see the exercise 1.12 in [12])

(8)
where Sd−1 is the unit (d − 1)-sphere  dωd is its volume  and dδ is its volume form. Let ∂i 
i = 1  . . .   d  be an orthonormal basis of TxM. Then for any vector b ∈ TxM  it can be written as

Sd−1

(cid:104)X  u(cid:105)2dδ(X) = 1

(cid:90)

1
ωd

(cid:90)

(cid:104)X  ∂i(cid:105)2dδ(X) =

1
ωd

Sd−1

Sd−1

(cid:104)X  b(cid:105)2dδ(X)

b =(cid:80)d

i=1 bi∂i. Furthermore  we have
(bi)2 1
ωd

(cid:107)b(cid:107)2 =

(bi)2 =

d(cid:88)

i=1

d(cid:88)

i=1

(cid:90)

From Eq. (7)  we can see that

Thus  we have

(cid:107)∇f (xi) − Vxi(cid:107)2 =

(cid:104)∇f (xi)  Pi(xj − xi)(cid:105) = f (xj) − f (xi).

(cid:90)
Sd−1
(cid:104)eij ∇f (xi) − Vxi(cid:105)2 =

(cid:88)
(cid:104)X ∇f (xi) − Vxi(cid:105)2dδ(X)

wij(cid:104)aij ∇f (xi) − Vxi(cid:105)2

j∼i

wij(cid:104)Pi(xj − xi) ∇f (xi) − Vxi(cid:105)2

(cid:0)(Pi(xj − xi))T Vxi − f (xj) + f (xi)(cid:1)2

wij

4

(9)

.

1
ωd

≈ (cid:88)
(cid:88)
(cid:88)

j∼i

j∼i

=

=

j∼i

where wij = d−2
xj(cid:107)2/δ) or simply by 0 − 1 weight. Then R1 reduces to the following:

ij . The weight wij can be approximated either by heat kernel weight exp(−(cid:107)xi −

(cid:88)

(cid:88)

j∼i

i

(cid:0)(xj − xi)T Tivi − fj + fi

(cid:1)2

wij

R1(f  V) =

3.3 Parallel Field Computation

n(cid:88)

As discussed before  we hope the vector ﬁeld to be as parallel as possible on the manifold. In the
discrete case  R2 becomes

F

i=1

R2(V) =

(cid:107)∇V |xi(cid:107)2
In the following  we discuss how to approximate (cid:107)∇V |xi(cid:107)2
F for a given point xi. Since we do
not know ∇∂iV for a given basis ∂i  (cid:107)∇V |xi(cid:107)2
F cannot be computed according to Eq. (4). We
deﬁne a (0  2) symmetric tensor α as α(X  Y ) = g(∇X V ∇Y V )  where X and Y are vector ﬁelds
F   where ∂1  . . .   ∂d is
an orthonormal basis of the tangent space. For the trace of α  we have the following geometric
interpretation (see the exercise 1.12 in [12]):

on the manifold. We have Trace(α) = (cid:80)d

i=1 g(∇∂iV ∇∂iV ) = (cid:107)∇V (cid:107)2
(cid:90)

(11)

(10)

(12)

where Sd−1 is the unit (d− 1)-sphere  dωd its volume  and dδ its volume form. So for a given point
xi  we can approximate (cid:107)∇V |xi(cid:107) by the following

Trace(α) =

α(X  X)dδ(X)

1
ωd

Sd−1

(cid:90)

α(X  X)|xidδ(X) ≈(cid:88)

(cid:88)

j∼i

α(eij  eij) =

j∼i

(cid:107)∇eij V (cid:107)2

(cid:107)∇V |xi(cid:107)2

F = Trace(α)xi =

1
ωd

Sd−1

(13)
Then we discuss how to discretize ∇eij V . Given eij ∈ TxiM  there exists a unique geodesic γ(t)
which satisﬁes γ(0) = xi and γ(cid:48)(0) = eij. Then the covariant derivative of vector ﬁeld V along eij
is given by (please see Fig. 1)
∇eij V = Pi

V (γ(t)) − V (γ(0))

wij(PiVxj − Vxi)

(Vxj − Vxi)

(cid:18) dV

≈ Pi

|t=0

(cid:19)

√

=

dt

= Pi lim
t→0

t

dij

Combining Eq. (13)  R2 becomes:
R2(V) =

(cid:88)

(cid:88)

j∼i

i

wij (cid:107)PiTjvj − Tivi(cid:107)2

(14)

3.4 Objective Function in the Discrete Form
Let I denote a n × n diagonal matrix where Iii = 1 if xi is labeled and Iii = 0 otherwise. And let
y ∈ Rn be a column vector whose i-th element is yi if xi is labeled and 0 otherwise. Then

R0(f ) =

(f − y)T I(f − y)

1
l

(15)

Combining R1 in Eq. (10) and R2 in Eq. (14)  the ﬁnal objective function in the discrete form can
be written as follows:
E(f  V) =

(cid:0)(xj − xi)T Tivi − fj + fi

(f − y)T I(f − y) + λ1

(cid:88)

(cid:88)

(cid:1)2

wij

1
l

+ λ2

(cid:88)

(cid:88)

j∼i

i

j∼i
wij (cid:107)PiTjvj − Tivi(cid:107)2

i

5

(16)

3.5 Optimization

In this subsection  we discuss how to solve the optimization problem (16).
Let L denote the Laplacian matrix of the graph with weights wij. Then we can rewrite R1 as follows:

(cid:88)

(cid:88)

j∼i

i

(cid:0)(xj − xi)T Tivi

(cid:1)2 − 2

wij

(cid:88)

(cid:88)

j∼i

i

wij(xj − xi)T TivisT
ijf

R1(f  V) = 2f T Lf +

where sij ∈ Rn is a selection vector of all zero elements except for the i-th element being −1 and
the j-th element being 1. Then the partial derivative of R1 with respect to the variable vi is
i (xj − xi)sT
ijf

i (xj − xi)(xj − xi)T Tivi − 2

∂R1(f  V)

(cid:88)

(cid:88)

wijT T

wijT T

= 2

j∼i

∂vi

j∼i

Thus we get

where G is a dn × dn block diagonal matrix  and C = [C T
Denote the i-th d × d diagonal block of G by Gii and the i-th d × n block of C by Ci  we have

1   . . .   C T

(17)
n ]T is a dn × n block matrix.

∂R1(f  V)

∂V

= 2GV − 2Cf

(cid:88)
(cid:88)

j∼i

j∼i

Gii =

Ci =

wijT T

wijT T

i (xj − xi)(xj − xi)T Ti
i (xj − xi)sT

ij

(18)

(19)

(20)

(cid:1)

(22)

(23)

(24)

(25)

(26)

The partial derivative of R1 with respect to the variable f is

∂R1(f  V)

∂f

= 4Lf − 2C T V

Similarly  we can compute the partial derivative of R2 with respect to the variable vi:
∂R2(V)

i TjT T

j Ti + I)vi − 2T T

i Tjvj

= 2

wij

wij

(cid:0)(QijQT

(cid:0)(T T

ij + I)vi − 2Qijvj

(cid:1) = 2

(cid:88)

j∼i

(cid:88)

j∼i

∂vi

where Qij = T T

i Tj. Thus we obtain

(21)
where B is a dn × dn sparse block matrix. If we index each d × d block by Bij  then for i  j =
1  . . .   n  we have

∂R2

∂V = 2BV
(cid:88)
(cid:26)−2wijQij 

wij(QijQT

j∼i

ij + I)
if xi ∼ xj
otherwise

0 

Bii =

Bij =

Notice that ∂R0

I(f − y). Combining Eq. (17)  Eq. (20) and Eq. (21)  we have

∂f = 2 1
∂E(f  V)

l

∂E(f  V)

∂f

∂V

=

=

+ λ1

∂R0
∂f
∂R0
∂V + λ1

+ λ2

∂R1
∂f
∂R1
∂V + λ2

∂R2
∂f
∂R2

= 2(

1
l

I + 2λ1L)f − 2λ1C T V − 2

1
l

y

∂V = −2λ1Cf + 2(λ1G + λ2B)V

Requiring that the derivatives vanish  we ﬁnally get the following linear system

(cid:18) 1

l

I + 2λ1L
−λ1C

−λ1C T
λ1G + λ2B

6

(cid:19)(cid:18)f

(cid:19)

V

=

(cid:18) 1

(cid:19)

l y
0

(a) Ground truth

(b) Laplacian (3.65)

(c) Hessian (1.35)

(d) PFR (1.14)

Figure 2: Global temperature prediction. Regression on the satellite measurement of temperatures
in the middle troposphere. 1% samples are randomly selected as training data. The ground truth
is shown in (a). The colors indicate temperature values (in Kelvin). The regression results are
visualized in (b)∼(d). The numbers in the captions are the mean absolute prediction errors.

4 Related Work and Discussion

The approximation of the Laplacian operator using the graph Laplacian [5] has enjoyed a great
success in the last decade. Some theoretical results [13  14] also show the consistency of the ap-
proximation. One of the most important features of the graph Laplacian is that it is coordinate free.
That is  it does not depend on any special coordinate system.
The estimation of Hessian is very difﬁcult and there is few work on it. Previous approaches [2  15]
ﬁrst estimate normal coordinates in the tangent space  and then estimate the ﬁrst order derivative at
each point  which is a matrix pseudo-inversion problem. One major limitation of this is that when the
number of nearest neighbors k is larger than d + d(d+1)
  where d is the dimension of the manifold 
the estimation will be inaccurate and unstable [15]. This is contradictory to the asymptotic case 
since it is not desirable that k is bounded by a ﬁnite number when the data is sufﬁciently dense. In
contrast  our method is coordinate free. Also  we directly estimate the norm of the second order
derivative instead of trying to estimate its coefﬁcients  which turns out to be an integral problem
over the neighboring points. We only need to do simple matrix multiplications to approximate the
integral at each point  but do not have to solve matrix inversion problems. Therefore  asymptotically 
we would expect our method to be much more accurate and robust for the approximation of the norm
of the second order derivative.

2

5 Experiments

In this section  we compare our proposed Parallel Field Regularization (PFR) algorithm with two
state-of-the-art semi-supervised regression methods: Laplacian regularized transduction (Laplacian)
[8] and Hessian regularized transduction (Hessian)1 [15]  respectively. Our experiments are carried
out on two real-world data sets. Regularization parameters for all algorithms are chosen via cross-
validation.

5.1 Global Temperature

In this test  we perform regression on the earth surface  which is a 2D sphere manifold. We try to
predict the satellite measurement of temperatures in the middle troposphere in Dec. 20042  which
contains 9504 valid temperature measurements. The coordinates (latitude  longitude) of the mea-
surements are used as features and the corresponding temperature values are the responses. The
dimension of manifold is set to 2 and the number of nearest neighbors is set to 6 in graph construc-
tion. We randomly select 1% of the samples as labeled data  and compare the predicted temperature
values with the ground truth on the rest of the data.
The regression results are shown in Fig. 2. The numbers in the captions indicate the mean absolute
prediction errors generated by different algorithms. It can be seen from the visualization result that

1We use the code from the authors downloadable from http://www.ml.uni-saarland.de/code/

HessianSSR/HessianSSR.html.

2http://www.remss.com/msu/.

7

6
1
0

e
m
a
r
f

0
0
3

e
m
a
r
f

Laplacian

Hessian

PFR

Figure 4: The examples of regression results on the moving hand data
set. 60 labeled samples are used for training. Each row shows the re-
sults obtained via the three algorithms for a frame. In each image  the
red dots indicate the ground truth positions we labeled manually  and
the blue arrows show the positions predicted by different algorithms.

Figure 3: Results on the
moving hand dataset.

Hessian and PFR perform better than Laplacian. Furthermore  from the prediction error  we can see
that PFR outperforms Hessian.

5.2 Positions of Moving Hands

In this subsection  we perform experiments using a video of a subject sitting in a sofa and waving
his arms 1. Our goal is to predict the positions of the (left and right) elbows and wrists. We extract
the ﬁrst 500 frames of the video and manually label the positions of the elbows and wrists. We scale
each frame to size of 120 × 90 and use the raw pixels (10800-dimensional vectors) as the features.
The response for each frame is a 8-dimensional vector whose elements are the 2D coordinates of the
elbows and wrists. Since there are 8 free parameters  we set the dimension of manifold to 8. We use
18 nearest neighbors in graph construction.
We run the experiments with different numbers of labeled frames. For each given number of labeled
frames  we perform 10 tests with randomly selected labeled set. The average of the mean absolute
error (MAE) for each test is calculated. The ﬁnal result is shown in Fig. 3. As can be seen  PFR
consistently outperforms the other two algorithms. Laplacian yields high MAE. Hessian is very
unstable on this dataset  and the results vary drastically with different numbers of labels.
We also show some example frames in Fig. 4. The red dots in the ﬁgures indicate the ground truth
positions and the blue arrows are drawn by connecting the positions of elbows and wrists predicted
by different algorithms. Again we can verify that PFR performs better than the other two algorithms.

6 Conclusion

In this paper  we propose a novel semi-supervised learning algorithm from the vector ﬁeld perspec-
tive. We show the relationship between vector ﬁelds and functions on the manifold. The parallelism
of the vector ﬁeld is used to measure the linearity of the target prediction function. Parallel ﬁelds are
one kind of special vector ﬁelds on the manifold  which have very nice properties. It is interesting
to explore other kinds of vector ﬁelds to facilitate learning on manifolds. Moreover  vector ﬁelds
can also be used to study the geometry and topology of the manifold. For example  Poincar´e-Hopf
theorem tells us that the sum of the indices over all the isolated zeroes of a vector ﬁeld equals to the
Euler characteristic of the manifold  which is a very important topological invariant.

Acknowledgments

This work was supported by the National Natural Science Foundation of China under
Grant 61125203  the National Basic Research Program of China (973 Program) under Grant
2012CB316404  and the National Natural Science Foundation of China under Grants 90920303
and 60875044.

1The video is obtained from http://www.csail.mit.edu/˜rahimi/manif.

8

204060801005101520number of labelsMAE  PFRHessianLaplacianReferences
[1] J. Tenenbaum  V. de Silva  and J. Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500):2319–2323  2000.

[2] D. L. Donoho and C. E. Grimes. Hessian eigenmaps: Locally linear embedding techniques for
high-dimensional data. Proceedings of the National Academy of Sciences of the United States
of America  100(10):5591–5596  2003.

[3] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Sci-

ence  290(5500):2323–2326  2000.

[4] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and
clustering. In Advances in Neural Information Processing Systems 14  pages 585–591. 2001.
[5] Fan R. K. Chung. Spectral Graph Theory  volume 92 of Regional Conference Series in Math-

ematics. AMS  1997.

[6] X. Zhu and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions.

In Proc. of the 20th Internation Conference on Machine Learning  2003.

[7] D. Zhou  O. Bousquet  T.N. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global

consistency. In Advances in Neural Information Processing Systems 16  2003.

[8] Mikhail Belkin  Irina Matveeva  and Partha Niyogi. Regularization and semi-supervised learn-

ing on large graphs. In Conference on Learning Theory  pages 624–638  2004.

[9] John Lafferty and Larry Wasserman. Statistical analysis of semi-supervised regression.

Advances in Neural Information Processing Systems 20  pages 801–808  2007.

In

[10] P. Petersen. Riemannian Geometry. Springer  New York  1998.
[11] G. H. Golub and C. F. Van Loan. Matrix computations. Johns Hopkins University Press  3rd

edition  1996.

[12] B. Chow  P. Lu  and L. Ni. Hamilton’s Ricci Flow. AMS  Providence  Rhode Island  2006.
[13] Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for laplacian-based man-

ifold methods. In Conference on Learning Theory  pages 486–500  2005.

[14] Matthias Hein  Jean yves Audibert  and Ulrike Von Luxburg. From graphs to manifolds - weak
In Conference on Learning Theory 

and strong pointwise consistency of graph laplacians.
pages 470–485  2005.

[15] K. I. Kim  F. Steinke  and M. Hein. Semi-supervised regression using hessian energy with an
application to semi-supervised dimensionality reduction. In Advances in Neural Information
Processing Systems 22  pages 979–987. 2009.

9

,Robert Gower
Dmitry Koralev
Felix Lieder
Peter Richtarik