2019,Multivariate Sparse Coding of Nonstationary Covariances with Gaussian Processes,This paper studies statistical characteristics of multivariate observations with irregular changes in their covariance structures across input space. We propose a unified nonstationary modeling framework to jointly encode the observation correlations to generate a piece-wise representation with a hyper-level Gaussian process (GP) governing the overall contour of the pieces. In particular  we couple the encoding process with automatic relevance determination (ARD) to promote sparsity to account for the inherent redundancy. The hyper GP enables us to share statistical strength among the observation variables over a collection of GPs defined within the observation pieces to characterize the variables' respective local smoothness. Experiments conducted across domains show superior performances over the state-of-the-art methods.,Multivariate Sparse Coding of Nonstationary

Covariances with Gaussian Processes

Rui Li

Golisano College of Computing and Information Sciences

Rochester Institute of Technology

Rochester  NY 14623

rxlics@rit.edu

Abstract

This paper studies statistical characteristics of multivariate observations with ir-
regular changes in their covariance structures across input space. We propose
a uniﬁed nonstationary modeling framework to jointly encode the observation
correlations to generate a piece-wise representation with a hyper-level Gaussian
process (GP) governing the overall contour of the pieces. In particular  we couple
the encoding process with automatic relevance determination (ARD) to promote
sparsity to account for the inherent redundancy. The hyper GP enables us to share
statistical strength among the observation variables over a collection of GPs de-
ﬁned within the observation pieces to characterize the variables’ respective local
smoothness. Experiments conducted across domains show superior performances
over the state-of-the-art methods.

1

Introduction

In many real-world applications  multivariate observations exhibit critical irregular changes in their
covariance smoothness with sharp transitions. For example  a major challenge to accurately locate
a seizure-onset zone (SOZ) through intracranial electroencephalography (iEEG) recordings is to
detect different forms of sudden transient electrophysiologic events of SOZ signals [1  2  3]. Another
scenario is that regional outbursts and geographic features (e.g.  parks  rivers) lead to complex
tempo-spatial variations of crime occurrences across regions over time [4  5].
In these scenarios  some segments of the observations exhibit larger variability than others. The
stationary methods  assuming the same covariance structure throughout the entire input space  cannot
capture such change in covariance smoothness. Conventional nonstationary modeling methods are
limited to model univariate observation by two consecutive steps [6  7  8]. They ﬁrst recursively
partition the input space into regions  and then deﬁne separate local Gaussian processes (GPs)
within each region. The GP inference step cannot capture long-range dependence or share statistical
information among the independent local GPs. A solution to alleviate the problem is to combine
the local GPs with a global GP which is ﬁtted to the whole observation [9  10]. However  it tends to
over-smooth the local covariance variability.
To address these challenges  we propose a novel nonstationary modeling framework that jointly
infers a piece-wise representation of the multivariate observations and a hyper-level GP governing
the overall contour of the pieces. In particular  we employ multilogit regression function to encode
the observation correlations coupled with automatic relevance determination (ARD) priors over the
coefﬁcients to promote sparsity. This encoding process transforms the observations into a set of
disjoint pieces to model the variability in the covariance smoothness with the correlations providing
between-variable information. Since commonly only a portion of observations are informative for
such transformation  ARD shrinks the correlation dimensions towards zero to handle the inherent

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

redundancy. Regulated by the hyper GP through their mean functions  a collection of variable-speciﬁc
local GPs are deﬁned to model the variables’ respective smoothness within the pieces. The hyper GP
not only shares statistical strength across the local GPs while retaining their distinctive covariance
property  but also induces the observation variables’ conditional independence. The piece-wise
representation leads to efﬁcient posterior computation with the conjugate priors.
We evaluate our nonstationary modeling method across domains: for seizure onset localization 
we achieve robustly better performances than the state-of-the-art competing methods; for crime
occurrence prediction  by modeling the evolving covariances of weekly crime rates among the 179
census tracts in Washington D.C between 2015-2019  we outperform the state-of-the-art methods.

2 Related work

Although iEEG recordings provide critical information to locate areas of the brain to remove for
epilepsy patients  pre-surgical examination of between-seizure iEEG signals is a labor-intensive and
error-prone process [1  11]. It becomes increasingly essential to develop effective computational
methods to identify the iEEG channels that are most likely to be in the SOZs by identifying different
abrupt changes in neurophysiological events [2  3  12  13]. Empirical studies focus on identifying
biomarkers (e.g.  spectral features  high frequency oscillations (HFO)) related to sub-clinical epileptic
bursts [1  12]. Classical modeling methods make stationary assumption without considering covari-
ance change over time. In [13]  Markov switching process is coupled with a stochastic process prior
to analyze the iEEG signal dynamics. A factor graphical model is proposed to integrate temporal
and spatial information of iEEG channels to infer pathologic brain activity for SOZ localization
[3]. Speciﬁcally  the spatial property is deﬁned as correlations between channels  and the temporal
function measures correlations between a channel’s current state and the linear combination of its
previous states. GP with stationary covariance is applied to model nonlinearity in neonatal EEG sig-
nals for seizure detection  and shows high level of prediction performance [14]. For crime prediction 
an autoregressive mixture model with Poisson processes is proposed [5]. Its most recent extension
PoINAR incorporates a stochastic process prior to group spatial correlation modes across multiple
time series  and achieves the state-of-the-art performance [15].
Nonstationary covariance function modeling methods with designed or learned kernels typically
assume the same covariance structure as a function of distances from observations throughout the
input space [16  17]. This is a strong modeling assumption for the above applications where sharp
transitions in covariance smoothness play the key role. Partitioning including Bayesian trees  Voronoi
tessellation  and normalized cuts (N-cuts) is widely used for modeling nonstationarity with abrupt
changes [6  7  8  9]. The local GPs deﬁned within the recursively partitioned regions are independent.
To capture the long-range trend  some methods deﬁne a global stationary GP over the entire input
space  and combine it with the local GPs. This leads to over-smooth the complex covariances induced
by the local GPs  since the global GP is also independent of the partition inference procedure [9  10].
Additionally  these methods are subject to some constraints such as partition points having to be at
observation locations  and balanced binary trees. A mixture of GP experts models nonstationary
univariate observations by deﬁning each GP expert over the entire input space [18  19].

3 Uniﬁed nonstationary modeling framework

Our framework encodes observation correlations into a trending piece-wise representation with both
ARD and hyper GP priors. By coupling the relevance vectors with the hyper GP  we are able to share
statistical information among the pieces. Given the hyper GP  each observation variable is modeled
by a conditionally independent GP within the pieces for its local covariance smoothness.

3.1 Sparse coding for observation correlations
Let Y = {y1 ···   yN} denote a set of multivariate observations at locations {x1 ···   xN} with
xi ∈ X as a non-random covariate in the input space X and an observation yi ∈ RD×1. We encode
Y into K pieces with the corresponding inputs X = ∪kXk and Xk ∩ Xk(cid:48) = ∅  where k (cid:54)= k(cid:48).

2

(cid:80)K

k Q(·  yi))

exp(θT
k(cid:48)=1 exp(θT

Let Z denote a N × K indicator matrix. Its element zik = δ(xi ∈ Xk) is the one-of-K encoding of
xi  where zik turns on iff xi ∈ Xk. Its probability of being 1 is a multilogit regression function:

p(zik = 1|θk  Q) =

(1)
where θk denotes a N × 1 coefﬁcient vector for the k’th piece  and Q(·  yi) is the i’th column vector
of the observation correlation matrix Q.
We employ the sparse prior ARD to explore how the correlation between any two observations con-
tributes to the encoding. ARD eliminates the irrelevant correlations by encouraging their coefﬁcients
go to zero. Speciﬁcally  we deﬁne independent  zero-mean  spherically symmetric Gaussian priors on
θk:

k(cid:48)Q(·  yi))

p(θk|αk) = N (θk|0  A−1
k )

k = diag(α−1

k ) denotes a diagonal matrix with the components of vector α−1

(2)
where A−1
k on the
diagonal. Each component of precision parameter αk is given a Γ(a  b) prior. ARD method penalizes
non-zero coefﬁcient components by an amount determined by the precision parameters. Iterative
estimation of αk and θk leads to αk becoming large for components whose evidence in the correlations
is insufﬁcient for overcoming the penalty induced by the prior. Having αk → ∞ drives θk → 0 
which implies that the corresponding correlations do not contribute to the encoding. Therefore  ARD
identiﬁes a subset of the observations  known as relevance vectors  with non-zero coefﬁcients for
each piece.
Let vk denote the input in Xk whose corresponding observation is the relevance vector with the
maximum absolute value of non-zero component of θk  where vk ∈ V ⊂ X. We deﬁne a function:
g : V → R which describes the overall contour of the observation pieces by sharing statistical
information among them:

g(v) ∼ GP (0  κg(v  v(cid:48)))

(3)
where κg is a covariance function deﬁned on V . We use a squared-exponential kernel κg =
g exp(−lg||v − v(cid:48)||2
2) to encourage a smooth proﬁle of the pieces. We further deﬁne a local function
σ2
fk : Xk → R for each piece:

fk(x)|g ∼ GP (g(vk)  κk(x  x(cid:48)))

(4)
where g(vk) speciﬁes the mean function of the GP prior for the local function fk. κk is a squared-
exponential kernel κk = σ2
2) deﬁning a covariance function. We assume lk =
||Xk||2

to let the horizontal lengthscales of the local functions reﬂect the global smoothness.

k exp(−lk||x − x(cid:48)||2

lg

2

3.2 Piece-wise GPs for univariate observations
Let g = g(V ) ∈ RK×1 and f = [f1(X1)T  ···   fK(XK)T ]T ∈ RN×1  the hyper-level and local
GPs deﬁne two joint Gaussians for any ﬁnite set of observations  respectively:
p(f|g  Z) = N (f|Zg  Σf )

(5)
where Σg is the covariance matrix with κg(v  v(cid:48)) as the elements  and Σf is a diagonal block
covariance matrix in which the elements of the k’th block Σ(k)
are κk of the input pairs in the k’th
piece as [Σ(k)
One can analytically marginalize g conditioned on the piece-wise representation Z yielding

p(g|V ) = N (g|0  Σg)

f ]ij = κk(x(k)

j ∈ Xk.

)  where x(k)

  x(k)

  x(k)

f

j

i

i

(6)

p(f|Z) = N (f|0  ZΣgZ T + Σf )
A univariate observation y ∈ RN×1 with noise is thus generated as:

(7)
where I is a N × N identity matrix. Recalling (6)  the marginal likelihood conditioned on Z yields
(8)

p(y|f )p(f|Z)df = N (y|0  Σy)

p(y|Z) =

(cid:90)

p(y|f   σ2) = N (y|f   σ2I)

where Σy = ZΣgZ T + Σf + σ2I denotes the induced nonstationary covariance matrix. Σy captures
the varying covariance structures of the pieces  and the discontinuities between them.

3

3.3 Piece-wise GPs for multivariate observations
To extend to multivariate observations Y = {y(1) ···   y(D)} where y(d) ∈ RN×1 denotes the
observations of the dth variable (e.g.  a feature of iEEG recording  a census tract’s crime occurrences) 
we model each variable’s observations y(d) as a realization from a speciﬁc local function f (d) as

The variable-speciﬁc local functions {f (d)} are conditionally independent given g and Z:

p(y(d)|f (d)  σ2) = N (y(d)|f (d)  σ2I)

p(f (1:D)|g  Z) =

D(cid:89)

d=1

p(f (d)|g  Z)
D | 1
|2π Σf
|2πΣf| D

exp[− 1
2

2

2

=

(cid:88)

(f (d) − ¯f )T Σ−1

f (f (d) − ¯f ))]N (¯f|Zg 

tr(

d

Σf
D

)

(9)

(10)

(cid:80)

=

(cid:80)

d f (d)
where ¯f =
D . The assumption allows to share statistical strength among the observation vari-
ables through g while retaining variable-speciﬁc covariance variability  as each variable’s observations
can be derived by marginalizing over f (d):

p(y(d)|g  Z  σ2) =

p(y(d)|f (d)  σ2I)p(f (d)|g  Z)df (d) = N (y(d)|Zg  Σy|g)

(11)

(cid:90)

where Σy|g = Σf + σ2I. By exploiting the conditional independence of Y   the marginal likelihood
for the multivariate observations is:
p(Y |Z  σ2) =

p(y(d)|g  Z  σ2)p(g|V )dg

(cid:90) D(cid:89)

d=1

|2π Σy|g
D | 1
|2πΣy|g| D

2

2

exp[− 1
2

tr(

(cid:88)

d

(y(d) − ¯y)(y(d) − ¯y)T )Σ−1

y|g]N (¯y|0 

Σy|g
D

+ ZΣgZ T )

(12)

d y(d)
where ¯y =
D . The kth diagonal block of the covariance matrix is [Σ(k)]ij = κg(vk  vk) +
[κk(x(k)
) + σ2δ(i  j)]/D. The multivariate case in (12) can be reduced to (8) when D = 1.
The computation complexity for (12) is O(KM 3)  where M denotes the rough size of each piece.
By optimizing (12)  we can determine the settings of the hyperparameters {lg  σ2

  x(k)

j

i

f} 1.

g  σ2

3.4 Efﬁcient inference

We develop a Gibbs sampling solution to iteratively sample the GP functions and the piece-wise
representation given their priors and the observations  and then update the hyper-parameters given
the latent functions and the observations.
First  our model’s joint probability can be factorized as

∝ D(cid:89)

p(Y {f (1:D)}  g  Z {θk}  V  X  σ2  Q  α)
[p(y(d)|f (d)  σ2)p(f (d)|g  Z)]p(g|Z)

K(cid:89)

N(cid:89)

[
i=1
k=1

d=1

p(zik|θk  Q(·  yi))p(θk|αk)p(αk)]

(13)

We propose to adopt the Rao-Blackwellized sampling scheme through analytic marginalization from
the joint distribution of {f (1:D)} and g  and sample them from their respective posteriors. This
improves the efﬁciency of our Gibbs sampler by reducing the underlying sample space and the
variance of later estimates. The conjugate priors result in closed-form marginalization.
By Combining the likelihood marginalized over f (d) in (11) and the prior in (5)  we sample g from
its posterior as

p(g|Y  Z) ∝ N (g|µg|y  Σg|y)
Σ−1
g|y = Σ−1

g + Z T Σ−1
y|gZ

µg|y = Σg|yZ T Σ−1
y|g ¯y

(14)

1See the supplementary material for the derivation of 12 and its gradients.

4

(a)

(b)

(c)

Figure 1: (a) Plot of mean and ±1 std of the log marginal likelihood in (12) of the true positive
iEEG observations in the training set versus different K. (b) Empirical mean of the 8 PIB features
of a true positive iEEG observation’s heldout segment (blue)  our method’s predictive mean of the
corresponding y(1:D)
in (24) (red)  and the predictive mean of Heinonen et al. method [16] (green).
(c) Boxplots of the cross-validation RMSEs summarizing the true positive observations in the training
set versus K.

∗

By marginalizing over g  each f (d) has the following posterior distribution:

p(f (d)|y(d)  f (−d)  Z  σ2) ∝ p(y(d)|f (d)  σ2)p(f (d)|f (−d)  Z)

(15)
where f (−d) denote the set {f (1:D)} other than f (d). The ﬁrst term p(y(d)|f (d)  σ2) is as in (9)  and
for the second term  we have

(cid:90)

p(f (d)|f (−d)  Z) =

p(f (d)|g  Z)p(g|f (−d)  Z)dg

Recalling (10) and (5)  the conditional distribution of g in (16) is

where ¯f (−d) =

(cid:80)

)−1Z

g + Z T (

Σf
D − 1

p(g|f (−d)  Z) ∝ p(f (−d)|g  Z)p(g|Z) = N (g|µg|f (−d)  Σg|f (−d) )
Σ−1
g|f (−d) = Σ−1
µg|f (−d) = Σg|f (−d) Z T (
d(cid:48)(cid:54)=d f (d(cid:48) )
D−1
p(f (d)|f (−d)  Z) =

Σf
D − 1
. Thus  we have the conditional distribution of f (d) as

(cid:90)
= N (f (d)|Zµg|f (−d)   Σf + ZΣg|f (−d)Z T )

p(f (d)|g  Z)p(g|f (−d)  Z)dg

)−1¯f (−d)

and the posterior distribution of f (d) as

p(f (d)|y(d)  f (−d)  Z  σ2) = N (f (d)|µf (d)|f (−d)   Σf (d)|f (−d))
Σ−1
f (d)|f (−d) = (Σf + ZΣg|f (−d)Z T )−1 + (σ2I)−1
µf (d)|f (−d) = Σf (d)|f (−d) [(σ2I)−1y(d) + (Σf + ZΣg|f (−d)Z T )−1Zµg|f (−d)]

We marginalize over {f (1:D)} to sample zik from its posterior by combining the marginal likelihood
in (11) and the prior in (1):

p(zik = 1|Y  Z−ik  g  σ2 {θk}  Q) ∝ p(Y |g  Z  σ2)

(cid:89)

(cid:89)

∝(cid:89)

(cid:89)

p(zik = 1|θk  Q(·  yi))
k Q(·  yi))

(cid:89)

exp(θT

i

k

N (y(d)|Zg  Σy|g)

d

i

k

where Z−ik denotes the matrix Z other than element zik. For binary random variables  Metropolis-
Hastings (MH) algorithm is shown to mix faster and have greater statistical efﬁciency than standard
Gibbs samplers [20]. To update zik given Z−ik  we thus use the posterior of (20) to evaluate a MH
proposal which ﬂips the binary variable zik with the current value z to its complement value ¯z:

(16)

(17)

(18)

(19)

(20)

(21)

zik ∝ κ(¯z|z)δ(zik  ¯z) + (1 − κ(¯z|z))δ(zik  z)
κ(¯z|z) = min{ p(zik = ¯z|Y  Z−ik  σ2 {θk}  Q)
  1}
p(zik = z|Y  Z−ik  σ2 {θk}  Q)

5

(a)

(b)

(c)

Figure 2: (a) Absolute correlation matrix of a heldout iEEG observation with SOZ events. (b) The
corresponding posterior covariance matrix of f (1:D) with the diagonal blocks as the local covariance
matrices of the observation pieces averaged over the Gibbs samples. (c) The blue bands indicate the
epileptologist’s labels on the SOZ events of the iEEG signal (gray)  and the red segments are the
encoded pieces predicted to be SOZ events.

p(θk|Z {θ−k}  αk  Q) ∝(cid:89)

To compute the conditional posterior of a coefﬁcient vector θk  we ﬁx the set {θ−k} other than θk
and have
(1 − ηik)δ(zi(cid:54)=k)
(22)
k(cid:48)Q(·  yi)]. We adopt the logistic sampling technique

where ηik ∝ exp[θT
with auxiliary variable sampling for its efﬁciency [21].
Finally  given {θk} and recalling that each αk is gamma distributed  its posterior is

p(zik|{θk}  Q)p(θk|αk) ∝ N (θk|0  A−1
k )

k(cid:48)(cid:54)=k exp(θT

i

k Q(·  yi)−log(cid:80)

ηδ(zi=k)
ik

(cid:89)

i

(cid:80)

|Sk|
2

i k θ2
ik
2
The set Sk contains the indices for which θik has prior precision αk.
From (11) and (14)  the predictive distribution of new observations y(d)∗

p(αk|θk  a  b) = Γ(a +

  b +

)

(23)

for the d’th variable is

p(y(d)∗ |Y  Z) =

p(y(d)∗ |g  Z  σ2)p(g|Y  Z)dg = N (y(d)∗ |Zµg|y  ZΣg|yZ T + Σy|g)

(24)

(cid:90)

The computational complexity for the predictive is O(M 2N ) due to the block structure of the
covariance matrix. After precomputation  the per-iteration complexity is reduced to O(M 2).

4 Experiments

We test our method across two domains. For seizure onset localization  we leverage our model to
detect early seizure discharges characterized by irregular covariance changes in iEEG recordings. For
crime occurrence prediction  our model captures the sharp transitions in regional crime occurrence
covariances.

4.1

iEEG data description

The dataset of iEEG recordings for SOZ detection are from 83 epilepsy patients 2. The patients
with different SOZs are surgically implanted with different numbers of iEEG sensors in potentially
epileptogenic regions in the brains. Among 4966 electrodes in total  911 of them identiﬁed to be in
SOZs by clinical epileptologists are taken as true positive examples. The iEEG data are down-sampled
to 5 kHz  and ﬁltered to remove artifacts. We adopt power-in-band (PIB) features measuring iEEG
data’s spectral power in the 8 frequency bands: Delta (0-3Hz)  low-theta (3-6 Hz)  high-theta (6-9
Hz)  alpha (9-14 Hz)  beta (14-25 Hz)  low-gamma (30-55 Hz)  high-gamma (65-115 Hz)  and ripple
(125-150 Hz)  as in [3]. The PIB features extracted from every second in a 2-hour iEEG recording
construct an observation Y with D = 8 feature variables and length of N = 7200.

4.2 MCMC settings

2The dataset is available in ftp://msel.mayo.edu/EEG_Data/

6

For each observation  we simulate 3 chains of 7000 Gibbs iterations 
and discard the ﬁrst 3000 as burn-in phase. Each sampling chain is
initialized with parameters sampled from their priors. We set Γ(a  b)
prior on the ARD precisions as a = |Sk| and b = a/1000  where Sk is
deﬁned in (23). This prior speciﬁcation is equally informative for various
choices of effective coefﬁcient number |Sk| by ﬁxing the prior mean of
the prior distribution. Given the number of pieces K ﬁxed  the marginal
likelihood in (12) is a function of the hyperparameters {lg  σ2
f}. We
use empirical Bayes approach to determine the optimum hyperparameter
values by optimizing the log marginal likelihood 3. To determine K  we
evaluate the marginal likelihood of the true positive iEEG observations
in the training set as shown in Figure 1 (a). It suggests K ≈ 1010 is
sufﬁcient to capture the covariance variability. We perform the Gelman-Rubin diagnostic [22] to
assess convergence by calculating the within-chain and between-chain variances on the Gibbs samples
of the posteriors.

Figure 3: The mean and
±1 std of AUROC scores
for different lengthscale
lg and K.

g  σ2

4.3 SOZ localization

We evaluate SOZ localization as a binary classiﬁcation task in terms of SOZ abnormal events predicted
to be present or absent in an iEEG channel’s observation  and use standard performance metrics to
compare with the state-of-the-art methods in Table 1.
We use 10-fold cross-validation (CV) to evaluate predictions with 30% test set while keeping the
same proportion of SOZ and non-SOZ observations in both sets. We ﬁrst evaluate our model’s
regression performance as demonstrated in Figure 1 (b) and (c). Figure 1 (b) shows that both the
long-range trend and the changes in covariance smoothness are captured without over-smoothing
the local GPs. We summarize the regression performance to ﬁne-tune K in Figure 1 (c) based on
the discussion in Section 4.2. Our method encodes each observation’s correlations into a covariance
matrix with diagonal block structures  as illustrated in Figure 2 (a)-(b). In Figure 2 (c) the pieces
capturing SOZ abnormal events are identiﬁed through a clinical epileptologist’s visual inspection
of the true positive iEEG signals. We utilize the local posterior covariance matrices  illustrated
in Figure 2 (b)  associated with SOZ events as the features of the positive examples  and the
local covariances without SOZ events as the negative ones. The averaged similarities of the local
covariances in the test set to the positive and negative examples are calculated via the Wasserstein
metric: ||µf − µf(cid:48)||2
2 )  respectively  and the ratios are used to
predict whether an observation consists of SOZ related pieces.

2 + tr(Σf + Σf(cid:48) − 2(Σ

f(cid:48)Σf Σ

f(cid:48)) 1

1
2

1
2

Methods

Our method

Factor graph model [3]
HFO biomarker [12]

N-cuts based mGP [9]
Tree based method [6]

Paciorek et al. [17]
Heinonen et al. [16]

Table 1: Performance evaluation of the SOZ channel detection
Recall (Sensitivity)

AUROC
0.80 ± 0.05
0.72 ± 0.03
0.66 ± 0.07
Partition-based nonstationary models
0.65 ± 0.03
0.64 ± 0.08
0.63 ± 0.07
0.67 ± 0.05

Precision
0.41 ± 0.07
0.39 ± 0.05
0.34 ± 0.05
0.61 ± 0.02
0.59 ± 0.03
0.41 ± 0.05
0.43 ± 0.03

0.75 ± 0.06
0.74 ± 0.03
0.53 ± 0.08
0.35 ± 0.07
0.38 ± 0.05
0.43 ± 0.09
0.58 ± 0.03

Nonstationary covariance function models

F1 score
0.51 ± 0.07
0.46 ± 0.04
0.41 ± 0.04
0.43 ± 0.05
0.40 ± 0.03
0.39 ± 0.04
0.42 ± 0.07

Figure 3 further explores the interactions between K and lg around its optimum in terms of the
classiﬁcation performances. The K leading to the best performance is consistent with the regression
performance in Figure 1 (c). In Table 1  the factor graphical model method heuristically divides the
iEEG recordings into non-overlapping three-second epochs to accommodate SOZ events [3]  whereas
our method is more ﬂexible by learning the SOZ pieces with various lengths. We implement the
other partition-based methods with the same settings as ours. Since these methods can only model

3See the supplementary material for the implementation.

7

(a)

(b)

(c)

(d)

Figure 4: (a) RMSE of prediction performance to ﬁne-tune K. (b) Absolute correlation matrix of
the crime occurrence rates of 179 CTs in 2015-2019. (c) The corresponding posterior covariance
matrix of f (1:D) averaged over the Gibbs samples. (d) plots of observation mean (blue)  our method’s
posterior and predictive mean (red)  and N-cuts based mGP [9]’s mean (green).

univariate observations  we apply them on each PIB feature and take the average. For Heinonen et
al. method [16]  we run 3 chains of 5000 samples of HMC-NUTS sampling to infer the three sets
of hyperparameters (noise variance  signal variance  and lengthscale)  and initialize the method as
suggested. One key to the method is the balance between the signal variance and the nonstationary
lengthscale  which is intrinsically related to the partition-based idea. For Paciorek et al. method [17] 
we use the Matern covariance function described in the paper. The Matern kernel leads to less smooth
functions  but it still assumes the covariance structure is the same throughout the entire input space.

4.4 Crime event prediction

∗

We apply our method to model the nonstationary evolution of crime
occurrence rates in the 179 census tracts (CTs) in Washington  D.C.
between 2015-2019 for crime occurrence prediction 4. We analyze
the crime rates on a weekly basis  with totally 227 weeks. By denot-
ing the crime rates in a CT with a variable  we have the multivariate
observation Y with D = 179 and N = 227.
We follow the model setting strategy as in Section 4.2. In particular 
We ﬁnd K = 15 with lg = 0.5 sufﬁciently to account for the crime
rates’ nonstationary variations based on the predictive performance 
as shown in Figure 4 (a). The results in Figure 4 (b)-(d) indicate that
we are able to capture the abrupt changes in covariance structure of
the CTs’ crime rates over time via the posterior and the predictive
estimates of y(1:D)
. Figure 4) (d) shows that the classic nonsta-
tionary method mGP [9] tends to over-smooth the local covariance
variability for combining a global GP with the local GPs.
We predict the one-week-ahead crime rates in each tract for the ﬁrst
16 weeks in 2019 based on the posterior estimates in 2015-2018. We
estimate the posterior predictive of the 2019 weekly crime rate in
each CT y(d)∗
as in (24) by averaging over the Gibbs samples. Table 2 shows the monthly-averaged
prediction RMSEs  conditioned on the observations in 2015-2018. For PoINAR  we use the same
setting as in [15]. The implementation of Paciorek et al. method [17] and Heinonen et al. method
[16] are the same as in Section 4.3. One major challenge to implement Paciorek et al. method is that
the number of its hyperparameters increases fast in multivariate cases. In particular  computation of
the kernel matrices at each input location is slow because of the matrix decomposition (O(D3)). In
contrast  our method is more computationally efﬁcient by introducing the conditional independence
given the hyper-GP as in (10). The results indicate that our method produces lower RMSE. Figure 5
visualizes the RMSEs between our method’s predictions and the ground truth by CTs geographically.

Figure 5: 2019 monthly aver-
aged RMSE maps between the
ground truth and our model’s
predictive means of y(1:D)

∗

.

4The crime data are available on http://opendata.dc.gov

8

Jan. 2019

Table 2: Monthly average RMSE of one-week-ahead predictions of the crime rates in 2019.
RM SE ± error
April 2019
0.817 ± 0.027
Our method
1.071 ± 0.032
1.165 ± 0.006
1.462 ± 0.147
1.069 ± 0.014

Mar. 2019
0.815 ± 0.029
0.893 ± 0.033
0.912 ± 0.086
1.176 ± 0.209
0.931 ± 0.763

0.638 ± 0.025
0.657 ± 0.023
0.839 ± 0.017
0.949 ± 0.034
0.704 ± 0.031

Feb. 2019

0.707 ± 0.023
0.818 ± 0.019
0.825 ± 0.014
1.122 ± 0.055
0.875 ± 0.118

N-cuts based mGP [9]

PoINAR [15]

Paciorek et al. [17]
Heinonen et al. [16]

5 Conclusions

Our uniﬁed nonstationary modeling framework integrates a sparse encoding process that transforms
the observations into a piece-wise representation with a hyper GP deﬁned over its relevance vectors.
The hyper GP governs a set of local GPs ﬁtted to the pieces through their mean functions. The
framework efﬁciently extends to multivariate observations by inducing conditional independence
among variables and between their respective local GPs. It achieves superior performance over the
state-of-the-art competitors by effectively capturing both sharp changes in covariance smoothness
and long-range trend.

6 Acknowledgments

This work is funded in part by National Science Foundation (NSF-1850492).

References
[1] Christopher P. Warren  Sanqing Hu  Matt Stead  Benjamin H. Brinkmann  Mark R. Bower  and Gregory A.
Worrell. Synchrony in normal and focal epileptic brain: The seizure onset zone is functionally disconnected.
Journal of Neurophysiology  104(6):3530–3539  October 2010.

[2] Greg A. Worrell  Andrew B. Gardner  S. Matt Stead  Sanqing Hu  Steve Goerss  Gregory J. Cascino 
Fredric B. Meyer  Richard Marsh  and Brian Litt. High-frequency oscillations in human temporal lobe:
Simultaneous microwire and clinical macroelectrode recordings. Brain  131(4):928–937  October 2008.

[3] Yogatheesan Varatharahah  Min Jin Chong  Krishnakant Saboo  Brent Berry  Benjamin Brinkmann  Gregory
Worrell  and Ravishankar Iyer. Eeg-graph: A factor-graph-based model for capturing spatial  temporal  and
observantional relationships in electroencephalograms. In NIPS  pages 5377–5386  December 2017.

[4] David McDowall  Colin Loftin  and Matthew Pate. Seasonal cycles in crime  and their variability. Journal

of Quantitative Criminology  28(3):389–410  September 2012.

[5] Matthew A. Taddy. Autoregressive mixture models for dynamic spatial poisson processes: Application to
tracking intensity of violent crime. Journal of the American Statistical Association  105(492):1403–1417 
January 2010.

[6] Robert B. Gramacy and Herbert K. H. Lee. Bayesian treed gaussian process models with an application to
computer modeling. Journal of the American Statistical Association  103(483):1119–1130  March 2009.

[7] Hyoung-Moon Kim  Bani K. Mallick  and C. C. Holmes. Analyzing nonstationary spatial data using
piecewise gaussian processes. Journal of the American Statistical Association  100(470):653–668  June
2005.

[8] Sunho Park and Seungjin Choi. Hierarchical gaussian process regression. In ACML  pages 95–110 

November 2010.

[9] Emily B. Fox and David B. Dunson. Multiresolution gaussian processes.

December 2012.

In NIPS  pages 737–745 

[10] Edward Snelson and Zoubin Ghahramani. Local and global sparse gaussian process approximations. In

AISTATS  pages 524–531  March 2007.

[11] Nicholas Michael Wetjen  Greg Worrell  Jeffrey Britton  Gregory Cascino  W. R. Marsh  Fredric B. Meyer 
Cheolsu Shin  and Elson So. Intracranial electroencephalography seizure onset patterns and surgical
outcomes in nonlesional extratemporal epilepsy. Journal of Neurosurgery  61(1):1147–1152  July 2009.

9

[12] Su Liu  Zhiyi Sha  Altay Sencer  Aydin Aydoseli  Nerse Bebek  Aviva Abosch  Thomas Henry  Candan
Gurses  and Nuri Firat Ince. Exploring the time-frequency content of high frequency oscillations for
automated identiﬁcation of seizure onset zone in epilepsy. Journal of Neural Engineering  13(2):26026–
26041  Feburary 2016.

[13] Drausin F.Wulsina  Emily B.Fox  and BrianLitt. Modeling the complex dynamics and changing correlations

of epileptic events. Artiﬁcial Intelligence  216(1):55–75  November 2014.

[14] Stephen Faul  Gregor Gregorcic  Geraldine Boylan  William Marnane  Gordon Lightbody  and Sean
Connolly. Gaussian process modeling of eeg for the detection of neonatal seizures. IEEE Transactions on
Biomedical Engineering  54(12):2151–2162  December 2007.

[15] Sivan Aldor-Noiman  Lawrence D. Brown  Emily B. Fox  and Robert A. Stine. Spatio-temporal low count
processes with application to violent crime events. Statistica Sinica  26(8):1587–1610  December 2016.

[16] Markus Heinonen  Henrik Mannerstrom  Juho Rousu  Samuel Kaski  and Harri Lahdesmaki. Non-
stationary gaussian process regression with hamiltonian monte carlo. In AISTATS  pages 732–740  June
2016.

[17] Christopher J. Paciorek and Mark J. Schervish. Nonstationary covariance functions for gaussian process

regression. In NIPS  pages 273–280  December 2003.

[18] Carl Edward Rasmussen and Zoubin Ghahramani. Inﬁnite mixtures of gaussian process experts. In NIPS 

pages 881–888  December 2001.

[19] Edward Meeds and Simon Osindero. An alternative inﬁnite mixture of gaussian process experts. In NIPS 

pages 883–890  December 2005.

[20] Arnoldo Frigessi  Patrizia Di Stefano  Chii-Ruey Hwang  and Shuenn-Jyi Sheu. Convergence rates of the
gibbs sampler  the metropolis algorithm and other single-site updating dynamics. Journal of the Royal
Statistical Association  55(1):205–219  March 1993.

[21] Chris C. Holmes and Leonhard Held. Bayesian auxiliary variable models for binary and multinomial

regression. Bayesian Analysis  1(1):145–168  March 2006.

[22] Stephen P. Brooks and Andrew E. Gelman. General methods for monitoring convergence of iterative

simulations. Journal of Computational and Graphical Statistics  7(4):434–455  November 1998.

10

,Francesco Orabona
Michael Andersen
Ole Winther
Lars Hansen
Corinna Cortes
Giulia DeSalvo
Mehryar Mohri
Bo-Jian Hou
Lijun Zhang
Zhi-Hua Zhou
Horia Mania
Aurelia Guy
Benjamin Recht
Rui Li