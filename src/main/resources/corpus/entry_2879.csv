2017,Learning Chordal Markov Networks via Branch and Bound,We present a new algorithmic approach for the task of finding a chordal Markov network structure that maximizes a given scoring function. The algorithm is based on branch and bound and integrates dynamic programming for both domain pruning and for obtaining strong bounds for search-space pruning. Empirically  we show that the approach dominates in terms of running times a recent integer programming approach (and thereby also a recent constraint optimization approach) for the problem. Furthermore  our algorithm scales at times further with respect to the number of variables than a state-of-the-art dynamic programming algorithm for the problem  with the potential of reaching 20 variables and at the same time circumventing the tight exponential lower bounds on memory consumption of the pure dynamic programming approach.,Learning Chordal Markov Networks

via Branch and Bound

Kari Rantanen

HIIT  Dept. Comp. Sci. 
University of Helsinki

Antti Hyttinen

HIIT  Dept. Comp. Sci. 
University of Helsinki

Matti Järvisalo

HIIT  Dept. Comp. Sci. 
University of Helsinki

Abstract

We present a new algorithmic approach for the task of ﬁnding a chordal Markov
network structure that maximizes a given scoring function. The algorithm is
based on branch and bound and integrates dynamic programming for both domain
pruning and for obtaining strong bounds for search-space pruning. Empirically 
we show that the approach dominates in terms of running times a recent integer
programming approach (and thereby also a recent constraint optimization approach)
for the problem. Furthermore  our algorithm scales at times further with respect to
the number of variables than a state-of-the-art dynamic programming algorithm
for the problem  with the potential of reaching 20 variables and at the same time
circumventing the tight exponential lower bounds on memory consumption of the
pure dynamic programming approach.

1

Introduction

Graphical models offer a versatile and theoretically solid framework for various data analysis
tasks [1  30  17]. In this paper we focus on the structure learning task for chordal Markov networks
(or chordal/triangulated Markov random ﬁelds or decomposable graphs)  a central class of undirected
graphical models [7  31  18  17]. This problem  chordal Markov network structure learning (CMSL)  is
computationally notoriously challenging; e.g.  ﬁnding a maximum likelihood chordal Markov network
with bounded structure complexity (clique size) is known to be NP-hard [23]. Several Markov chain
Monte Carlo (MCMC) approaches have been proposed for this task in the literature [19  27  10  11].
Here we take on the challenge of developing a new exact algorithmic approach for ﬁnding an optimal
chordal Markov network structure in the score-based setting. Underlining the difﬁculty of this
challenge  ﬁrst exact algorithms for CMSL have only recently been proposed [6  12  13  14]  and
generally do no scale up to 20 variables. Speciﬁcally  the constraint optimization approach introduced
in [6] does not scale up to 10 variables within hours. A similar approach was also taken in [16] in the
form of a direct integer programming encoding for CMSL  but was not empirically evaluated in an
exact setting. Comparably better performance  scaling up to 10 (at most 15) variables  is exhibited
by the integer programming approach implemented in the GOBNILP system [2]  extending the core
approach of GOBNILP to CMSL by enforcing additional constraints. The true state-of-the-art exact
algorithm for CMSL  especially when the clique size of the networks to be learned is not restricted  is
Junctor  implementing a dynamic programming approach [13]. The method is based on recursive
characterization of clique trees and storing in memory the scores of already-solved subproblems.
Due to its nature  the algorithm has to iterate through every single solution candidate  although its
effective memoization technique helps to avoid revisiting solution candidates [13]. As typical for
dynamic programming algorithms  the worst-case and best-case performance coincide: Junctor is
guaranteed to use Ω(4n) time and Ω(3n) space.
In this work  we develop an alternative exact algorithm for CMSL. While a number of branch-
and-bound algorithms have been proposed in the past for Bayesian network structure learning

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

(BNSL) [25  28  20  29  26]  to the best of our knowledge our approach constitutes the ﬁrst non-trivial
branch-and-bound approach for CMSL. Our core search routine takes advantage of similar ideas
as a recently proposed approach for optimally solving BNSL [29]  and  on the other hand  like
GOBNILP  uses the tight connection between BNSL and CMSL by searching over the space of
chordal Markov network structures via considering decomposable directed acyclic graphs. Central to
the efﬁciency of our approach is the integration of dynamic programming over Bayesian network
structures for obtaining strong bounds for effectively pruning the search space during search  as
well as problem-speciﬁc dynamic programming for efﬁciently implementing domain ﬁltering during
search. Furthermore  we establish a condition which enables symmetry breaking for noticeably
pruning the search space over which we perform branch and bound. In comparison with Junctor 
a key beneﬁt of our approach is the potential of avoiding worst-case behavior  especially in terms
of memory usage  based on using strong bounds to rule out provably non-optimal solutions from
consideration during search.
Empirically  we show the approach dominates the integer programming approach of GOBNILP [2] 
and thereby also the constraint optimization approach [6  12]. Furthermore  our algorithm scales
at times further in terms of the number of variables than the DP-based approach implemented in
Junctor [13]  with the potential of reaching 20 variables within hours and at the same time circumvent-
ing the tight exponential lower bounds on memory consumption of the pure dynamic programming
approach  which is witnessed also in practice by noticeably lower memory consumption.1

2 Chordal Markov Network Structure Learning

A Markov network structure is represented by an undirected graph Gu = (V  Eu)  where V =
{v1  . . .   vn} is the set of vertices and Eu the set of undirected edges. This structure represents
independencies vi ⊥⊥ vj|S according to the undirected separation property: vi and vj are separated
given set S if and only if all paths between them go through a vertex in set S. The undirected graph is
chordal iff every (undirected) cycle of length greater than three contains a chord  i.e.  an edge between
two non-consecutive vertices in the cycle. Figure 1 a) shows an example. Here we focus on the task of
ﬁnding a chordal graph U that maximizes posterior probability P (Gu|D) = P (D|Gu)P (Gu)/P (D) 
where D denotes the i.i.d. data set. As we assume a uniform prior over chordal graphs  this boils
down to maximizing the marginal likelihood P (D|Gu).
Dawid et al. have shown that the marginal likelihood P (D|Gu) for chordal Markov networks can be
calculated using a clique tree representation [7  9]. A clique C is a fully connected subset of vertices.
A clique tree for an undirected graph Gu is an undirected tree such that

I. (cid:83)

II.
III.

s(Ci) −(cid:80)

Ci

Sj

i P (Ci)/(cid:81)

i Ci = V  
if {v(cid:96)  vk} ∈ Eu  then either {v(cid:96)  vk} ⊆ Ck or {v(cid:96)  vk} ⊆ C(cid:96)  and
the running intersection property holds: whenever vk ∈ Ci and vk ∈ Cj  then vk is also in
every clique on the unique path between Ci and Cj.

The marginal likelihood factorizes according to the clique tree: P (D|U ) =(cid:81)
(cid:80)

The separators are the intersections of adjacent cliques in a clique tree. Figure 1 b) shows an example.
j P (Sj)
(assuming positivity and that the prior factorizes) [6]. The marginal likelihood P (S) for a set
S of random variables can be calculated with suitable priors; in this paper we consider discrete
data using a Dirichlet prior. If we denote s(S) = log P (S)  CMSL can be cast as maximizing
s(Sj). For example  the marginal log-likelihood of the graph in Figure 1 a)
can be calculated using the clique tree presentation in Figure 1 b) as s({v1  v6}) + s({v1  v5}) +
s({v1  v2  v3}) + s( v2  v3  v4}) − s({v1}) − s({v1}) − s({v2  v3}).
In this paper  we view the chordal Markov network structure learning problem from the viewpoint
of directed graphs  making use of the fact that for each chordal Markov network structure there are
equivalent directed graph structures [15  7]  which we call here decomposable DAGs. A decomposable
DAG is a DAG G = (V  E) such that the set of directed edges E ⊂ V × V does not include any
immoralities  i.e.  structures of the form vi → vk ← vj with no edges between vi and vj. Due
to lack of immoralities  the d-separation property on a decomposable DAG corresponds exactly
to the separation property on the chordal undirected graph (the skeleton of the decomposable
DAG). Thus  decomposable graphs represent distributions that are representable by Markov and by

1Extended discussion and empirical results are available in [21].

2

v6

v2

v1

v4

v5

v3

{v1  v6}

{v1}

{v1  v5}

{v1}

{v2  v3}

{v1  v2  v3}

{v2  v3  v4}

v6

v2

v1

v4

v5

v3

a)
Figure 1: Three views on chordal Markov network structures: a) chordal undirected graph  b) clique
tree  (c) decomposable DAG.

b)

c)

Bayesian networks. Figure 1 c) shows a corresponding decomposable DAG for the chordal undirected
graph in a). Note that the decomposable DAG may not be unique; for example  v2 → v3 can be
directed also in the opposite direction. The score of the decomposable DAG can be calculated as
s(v1 ∅) + s(v5 {v1}) + s(v6 {v1}) + s(v2 {v1}) + s(v3 {v1  v2}) + s(v4 {v2  v3})  where s(vi  S)
are the local scores for BNSL using e.g. a Dirichlet prior. Because these local scores s(· ·) correspond
to s(·) through s(vi  S) = s({vi  S}) − s(S) (and s(∅) = 0)  we ﬁnd that this BNSL scoring gives
the same result as the clique tree based scoring rule.
Thus CMSL can also be cast as the optimization problem of ﬁnding a graph in

(cid:88)

vi∈V

arg max
G∈G

s(vi  paG(vi)) 

where G denotes the class of decomposable DAGs. (This formulation is used also in the GOBNILP
system [2].) The optimal chordal Markov network structure is the skeleton of the optimal G. This
problem is notoriously computationally difﬁcult in practice  emphasized by the fact that standard
score-pruning [3  8] used for BNSL is not generally applicable in the context of CMSL as it will often
prevent ﬁnding the true optimum: pruning parent sets for vertices often circumvents other vertices
achieving high scoring parents sets (as immoralities would be induced).

3 Hybrid Branch and Bound for CMSL

In this section we present details on our branch-and-bound approach to CMSL. We start with an
overview of the search algorithm  and then detail how we apply symmetry breaking and make use of
dynamic programming to dynamically update variable domains  i.e.  for computing parent set choices
during search  and to obtain tight bounds for pruning the search tree.

3.1 Branch and Bound over Ordered Decomposable DAGs

The search is performed over the space of ordered decomposable DAGs. While in general the order
of the vertices of a DAG can be ambiguous  this notion allows for differentiating the exact order of
the vertices  and allows for pruning the search space by identifying symmetries (see Section 3.2).
Deﬁnition 1. G = (V  E  π) is an ordered decomposable DAG if and only if (V  E) is a decomposable
DAG and π : {1...n} → {1...n} a total order over V such that (vi  vj) ∈ E only if π−1(i) < π−1(j)
for all vi  vj ∈ V .
Partial solutions during search are hence ordered decomposable DAGs  which are extended by adding
a parent set choice (v  P )  i.e.  adding the new vertex v and edges from each of its parents in P to v.
Deﬁnition 2. Let G = (V  E  π) be an ordered decomposable DAG. Given vk /∈ V and P ⊆ V   we
say that the ordered decomposable DAG G(cid:48) = (V (cid:48)  E(cid:48)  π(cid:48)) is G with the parent set choice (vk  P ) if
the following conditions hold.

2. E(cid:48) = E ∪(cid:83)

1. V (cid:48) = V ∪ {vk}
3. We have π(cid:48)(i) = π(i) for all i = 1...|V |  and π(cid:48)(|V | + 1) = k.

v(cid:48)∈P{(v(cid:48)  vk)}.

Algorithm 1 represents the core functionality of the branch and bound. The recursive function
takes two arguments: the remaining vertices of the problem instance  U  and the current partial
solution G = (V  E  π). In addition we keep stored a best lower bound solution G∗  which is the

3

Algorithm 1 The core branch-and-bound search.
1: function BRANCHANDBOUND(U  G = (V  E  π))
2:
3:
4:
5:
6:

if U = ∅ and s(G∗) < s(G) then G∗ ← G
if this branch cannot improve LB then return
for (vi  P ) ∈ PARENTSETCHOICES(U  G) do

Let G(cid:48) = (V (cid:48)  E(cid:48)  π(cid:48)) be G with the parent set choice (vi  P ).
BRANCHANDBOUND(U \ {vi}  G(cid:48))

(cid:46) Update LB if improved.
(cid:46) Backtrack
(cid:46) Iterate the current parent set choices.

(cid:46) Continue the search.

highest-scoring solution that has been found so far. Thus  at the end of the search  G∗ is an optimal
solution. During the search we use G∗ for bounding as further detailed in Section 3.3.
In the loop on line 4 we branch with all the parent set choices that we have deemed necessary to try
during the search. The method PARENTSETCHOICES(U  G) and the related symmetry breaking are
explained in Section 3.2. We sort the parent set choices into decreasing order based on their score  so
that (v  P ) is tried before (v(cid:48)  P (cid:48)) if s(v  P ) > s(v(cid:48)  P (cid:48))  where v  v(cid:48) ∈ U and P  P (cid:48) ⊆ V . This is
done to focus the search ﬁrst to the most promising branches for ﬁnding an optimal solution. When
U = ∅  we have PARENTSETCHOICES(U  G) = ∅  and so the current branch gets terminated.

3.2 Dynamic Branch Selection  Parent Set Pruning  and Symmetry Breaking

We continue by proposing symmetry breaking for the space of ordered decomposable DAGs  and
propose a dynamic programming approach for dynamic parent set pruning during search. We start
with symmetry breaking.
In terms of our branch-and-bound approach to CMSL  symmetry breaking is a vital part of the search 
as there can be exponentially many decomposable DAGs which correspond to a single undirected
chordal graph; for example  the edges of a complete graph can be directed arbitrarily without the
resulting DAG containing any immoralities. Hence symmetry breaking in terms of pruning out
symmetric solution candidates during search has potential for noticeably speeding up search.
Chickering [4  5] showed how so-called covered edges can be used to detect equivalencies between
Bayesian network structures. Later van Beek and Hoffmann [29] implemented covered edge based
symmetry breaking in their BNSL approach. Here we introduce the concept of preferred vertex
orders  which generalizes covered edges for CMSL based on the decomposability of the solution
graphs.
Deﬁnition 3. Let G = (V  E  π) be an ordered decomposable DAG. A pair vi  vj ∈ V violates the
preferred vertex order in G if the following conditions hold.

1. i > j.
2. paG(vi) ⊆ paG(vj).
3. There is a path from vi to vj in G.

Theorem 1 states that for any (partial) solution (i.e.  an ordered decomposable DAG)  there always
exists an equivalent solution that does not contain any violations of the preferred vertex order.
Mapping to practice  this theorem allows for very effectively pruning out all symmetric solutions but
the one not violating the preferred vertex order within our branch-and-bound approach. A detailed
proof is provided in Appendix A.
Theorem 1. Let G = (V  E  π) be an ordered decomposable DAG. There exists an ordered decom-
posable DAG G(cid:48) = (V  E(cid:48)  π(cid:48)) that is equivalent to G  but where for all vi  vj ∈ V the pair (vi  vj)
does not violate the preferred vertex order in G(cid:48).
It follows from Theorem 1 that for each solution (ordered decomposable DAG) there exists an
equivalent solution where the lexicographically smallest vertex is a source. Thus we can ﬁx it as the
ﬁrst vertex in the order at the beginning of the search.
Similarly as in [29] for BNSL  we deﬁne the depths of vertices as follows.
Deﬁnition 4. Let G = (V  E  π) be an ordered decomposable DAG. The depth of v ∈ V in G is

(cid:40) 0

d(G  v) =

max

v(cid:48)∈paG(v)

if paG(v) = ∅ 
otherwise.

d(G  v(cid:48)) + 1

4

The depths of G are ordered if for all vi  vj ∈ V   where π−1(i) < π−1(j)  the following hold.
1. d(G  vi) ≤ d(G  vj)  and 2. If d(G  vi) = d(G  vj)  then i < j.
Note that "violating the preferred vertex order" concerns the order in which the vertices are in
the underlying DAG  whereas "depths are ordered" concerns the order by which a solution was
constructed. We use the former to prune whole solution candidates from the search space  and the
latter to ensure that no solution candidate is seen twice during search.
We also propose a dynamic programming approach to branch selection and parent set pruning during
search  based on the following deﬁnition of valid parent sets.
Deﬁnition 5. Let G = (V  E  π) be an ordered decomposable DAG. Given vk /∈ V and P ⊆ V   let
G(cid:48) = (V (cid:48)  E(cid:48)  π(cid:48)) be G with the parent set choice (vk  P ). The parent set choice (vk  P ) is valid for
G if the following hold.

1. For all vi  vj ∈ P we have either (vi  vj) ∈ E or (vj  vi) ∈ E.
2. For all vi ∈ V   the pair (vi  vk) does not violate the preferred vertex order in G(cid:48).
3. The depths of G(cid:48) are ordered.

Given a partial solution G = (V  E  π)  a vertex v /∈ V   and a subset P ⊆ V   the function GETSU-
PERSETS in Algorithm 2 represents a dynamic programming method for determining valid parent set
choices (v  P (cid:48)) for G where P (cid:48) ⊇ P . An advantage of this formulation is that invalidating conditions
for a parent set  such as immoralities or violations of the preferred vertex order  automatically hold
for all the supersets of the parent set; this is applied on line 6 to avoid unnecessary branching.
On line 8 we require that a parent set P is added to the list only if none of its valid supersets P (cid:48) ∈ C
have a higher score. This pruning technique is based on the observation that P (cid:48) provides all the same
moralizing edges as P   and therefore it is sufﬁcient to only consider the parent set choice (v  P (cid:48)) in
the search when s(v  P ) ≤ s(v  P (cid:48)).
Given the set of remaining vertices U  the function PARENTSETCHOICES in Algorithm 2 constructs
all the available parent set choices for the current partial solution G = (V  E  π). The collection
M(G  vi) contains the subset-minimal parent sets for vertex vi ∈ U that satisfy the 3rd condition
of Deﬁnition 5. If V = ∅  then M(G  vi) = {∅}. Otherwise  let k be the maximum depth of the
vertices in G. Now M(G  vi) contains the subset-minimal parent sets that would insert vi on depth
k + 1. In addition  if i > j for all vj ∈ V where d(G  vj) = k  then M(G  vi) also contains the
subset-minimal parent sets that would insert vi on depth k. Note that the cardinality of any parent set
in M(G  vi) is at most one.

3.3 Computing Tight Bounds by Harnessing Dynamic Programming for BNSL

To obtain tight bounds during search  we make use of the fact that the score of the optimal BN
structures for the BNSL instance with same scores as in the CMSL instance at hand is guaranteed
to give an upper bound on the optimal solutions to the CMSL instance. To compute an optimal
BN structure  we use a variant of a standard dynamic programming algorithm by Silander and
Myllymäki [22]. While there are far more efﬁcient algorithms for BNSL [2  32  29]  we use BNSL
DP for obtaining an upper bound during the branch-and-bound search under the current partial

Algorithm 2 Constructing parent set choices via dynamic programming.
1: function PARENTSETCHOICES(U  G = (V  E  π))
2:

GETSUPERSETS(v  G  M )

return (cid:83)

(cid:83)

M∈M(G v)

v∈U
Let C = ∅
for v(cid:48) ∈ V \ P \ {v} do

3: function GETSUPERSETS(v  G = (V  E  π)  P )
4:
5:
6:
7:
8:
9:
10:

C ← C ∪ GETSUPERSETS(v  G  P ∪ {v(cid:48)})

if (v  P ) is valid parent set choice for G and s(v  P ) > s(v  P (cid:48)) for all P (cid:48) ∈ C then
return C

C ← C ∪ {(v  P )}

if (v  P (cid:48)) is a valid parent set choice for G with some P (cid:48) ⊇ P ∪ {v(cid:48)} then

5

CMSL solution (i.e.  under the current branch). Speciﬁcally  before the actual branch and bound  we
precompute a DP table which stores  for each subset of vertices V (cid:48) ⊂ V of the problem instance  the
score of the so-called BN extensions of V (cid:48)  i.e.  the optimal BN structures over U = V \ V (cid:48) where
we additionally allow the vertices in U to also take parents from V (cid:48). This guarantees that the BN
extensions are compatible with the vertex order in the current branch of the branch-and-bound search
tree  and thereby the sum of the score of the current partial CMSL solution over V (cid:48) and the score of
the optimal BN extensions of V (cid:48) is a valid upper bound. By spending O(n· 2n) time in the beginning
of the branch and bound for computing the scores of optimal BN extensions of every V (cid:48) ⊂ V   we
can then look up these scores during branch and bound in O(1) time.
With the DP table  it takes only low polynomial time to construct the optimal BN structure over the
set of all vertices [22]  i.e.  a BN extension of ∅. Thus  we can obtain an initial lower bound solution
G∗ for the branch and bound as follows.

1. Construct the optimal BN structure for the vertices of the problem instance
2. Try to make the BN decomposable by heuristically adding or removing edges.
3. Let G∗ be the highest-scoring decomposable DAG from step 2.

However  the upper bounds obtained via BNSL can be at times can be quite weak when the network
structures contain many immoralities. For this reason  in Algorithm 3  we introduce an additional
method for computing the upper bounds  taking immoralities “relaxedly” into consideration. The
algorithm takes four inputs: A ﬁxed partial solution G = (V  E  π)  a list of vertices A that we have
assigned during the upper bound computation  a list of remaining vertices U  and an integer d ≥ 0
which dictates the maximum recursion depth. As a fallback option  on line 3 we return the optimal
BN score for the remaining vertices if the maximum recursion depth is reached.
On line 4 we construct the collection of sets P that are the maximal sets that any vertex can take
as parent set during the upper bound computation. The sets in P take immoralities relaxedly into
consideration: For any vi  vj ∈ V   we have {vi  vj} ⊆ P for some P ∈ P if and only if (vi  vj) ∈ E
or (vj  vi) ∈ E. That is  when choosing parent sets during the upper bound computation  we allow
immoralities to appear  as long as they are not between vertices of the ﬁxed partial solution. In the
loop on line 6  we iterate through each vertex v ∈ U that is still remaining  and ﬁnd its highest-
scoring relaxedly-moral parent set according to P. Note that given any P (cid:48) ∈ P  we can ﬁnd the
highest-scoring parent set P ⊆ P (cid:48) in O(1) time when the scores are stored in a segment tree. For
information about constructing such data structure  see [22]. Thus line 7 takes O(|V |) time to execute.
Finally  on line 8 of the loop  we split the problem into subproblems to see which parent set choice
(v  P ) provides the highest local upper bound u to be returned.
Algorithm 3 requires O((n − m) · m · 2n−m) time  where m = |V | is the number of vertices in the
partial solution and n the number of vertices in the problem instance  assuming that the BN extensions
and the segment trees have been precomputed. (In the empirical evaluation  the total runtimes of our
branch-and-bound approach include these computations.) The collections P can exist implicitly.
We use the upper bounds within branch and bound as follows. Let G = (V  E  π) be the current
partial solution  let U be the set of remaining vertices  and let b be the score of optimal BN extensions
of V . We can close the current branch if s(G∗) ≥ s(G) + b. Otherwise  we can close the branch if
s(G∗) ≥ s(G) + UPPERBOUND(G ∅  U  d) for some d > 0. Our implementation uses d = 10.

Algorithm 3 Computing upper bounds for a partial solution via dynamic programming.
1: function UPPERBOUND(G = (V  E  π)  A  U  d)
2:
3:
4:

if U = ∅ then return 0
if d = 0 then return the score of optimal BN extensions of V ∪ A

Let P = (cid:83)

{{v} ∪ paG(v) ∪ A}

v∈V
Let u ← −∞
for v ∈ U do

5:
6:
7:

8:
9:

Let P = arg max
P⊆P (cid:48)∈P
u ← max(u  s(v  P ) + UPPERBOUND(G  A ∪ {v}  U \ {v}  d − 1))

s(v  P )

return u

6

4 Empirical Evaluation

We implemented the branch-and-bound algorithm in C++  and refer to this prototype as BBMarkov.
We compare the performance of BBMarkov to that of GOBNILP (the newest development version [24]
at the time of publication  using IBM CPLEX version 12.7.1 as the internal IP solver) as a state-of-
the-art BNSL system implementing a integer programming branch-and-cut approach to CMSL by
ruling out non-chordal graphs  and Junctor  implementing a state-of-the-art DP approach to CMSL.
We used a total of 54 real-world datasets used as standard benchmarks for exact approaches [32  29].
For investigating scalability of the algorithms in terms of the number of variables n  we obtained
from each dataset several benchmark instances by restricting to the ﬁrst n variables for increasing
values of n. We did not impose a bound on the treewidth of the chordal graphs of interest  i.e.  the
size of candidate parent sets was not limited. We used the BDeu score with equivalent sample size 1.
As standard practice in benchmarking exact structure learning algorithms  we focus on comparing the
running times of the considered approaches on precomputed input CMSL instances. The experiments
were run under Debian GNU/Linux on 2.83-GHz Intel Xeon E5440 nodes with 32-GB RAM.
Figure 2 compares BBMarkov to GOBNILP and Junctor under a 1-h per-instance time limit  with
different numbers n of variables distinguished using different point styles. BBMarkov clearly
dominates GOBNILP in runtime performance (Fig. 2 left); instances for n > 15 are not shown as
GOBNILP was unable to solve them. Compared to Junctor (Fig. 2 middle  Table 1)  BBMarkov
exhibits complementary performance. Junctor is noticeably strong on several datasets and lower
values of n  and exhibits fewer timeouts. For a ﬁxed n  Junctor’s runtimes have a very low variance
independent of the dataset  which is due to the Ω(4n) (both worst-case and best-case) runtime
guarantee. However  BBMarkov shows potential for scaling up for larger n than Junctor: at n = 17
Junctor’s runtimes are very close to 1 h on all instances  while BBMarkov’s bounds rule out at times
very effectively non-optimal solutions  resulting in noticeable lower runtimes on speciﬁc datasets
with increasing n. This is show-cased in Table 1 on the right  highlighting some of the best-case
performance of BBMarkov using per-instance time limit of 24 h for both BBMarkov and Junctor.
In terms of how the various search techniques implemented in BB-
Markov contribute to the running times of BBMarkov  we observed
that the running times for obtaining BNSL-based bounds (via the
use of exact BN dynamic programming and segment trees) tend to
be only a small fraction of the overall running times. For example 
at n = 20  these computations take less than minute in total. Most
of the time in the search is typically used in the optimization loop
and in computing the tighter upper bounds that take immoralities
"relaxedly" into consideration. While computing the tighter bounds
is more expensive than computing the exact BNs at the beginning of
search  the tighter bounds often pay off in terms of overall running
times as branches can be closed earlier during search.
Another beneﬁt of BBMarkov compared to Junctor is the observed
lower memory consumption (Figure 3). Junctor’s Ω(3n) memory

Figure 3: Memory usage

Figure 2: Per-instance runtime comparisons. Left: BBMarkov vs GOBNILP. Middle: BBMarkov vs
Junctor. Right: BBMarkov time to ﬁnding vs BBMarkov time to proving an optimal solution.

7

10MB100MB1GB10GB25GB891113151719Memory usageNumber of variablesJunctorBBMarkov<1s1m>1h<1s1m>1hRun time of GOBNILPRun time of BBMarkovn=15n=14n=13n=12n=11<1s1m>1h<1s1m>1hRun time of JunctorRun time of BBMarkov<1s1m>1h<1s1m>1hTo find an optimal solutionTo find and prove the solutionn=17n=16n=15n=14n=13Dataset
Wine
Adult
Letter
Voting
Zoo
Water100
Water1000
Water10000
Tumor

Running times (s)
BBMarkov
n
13
<1
14
58
16 >3600
17
281
17 >3600
17
100
17
2731
17 >3600
18
610

Table 1: BBMarkov v Junctor. Left: smaller datasets and for different sample sizes on the Water
dataset. Right: Examples of best-case performance of BBMarkov. to: timeout  mo: memout.
Running times (s)
BBMarkov
268
1462
10274
49610
41
162
1186
15501
225
2543
13749
33503
590
6581
61152

Junctor
2724
12477
52130
mo
3007
11179
50296
mo
2588
12422
53108
mo
12244
52575
mo

(62)
(315)
(2028)
(50)
(22)
(85)
(698)
(13845)
(108)
(1348)
(6418)
(25393)
(244)
(6187)
(54806)

(<1)
(35)
(>3600)
(207)
(>3600)
(49)
(279)
(>3600)
(268)

Junctor
6
29
592
3050
2690
2580
2592
2928
12019

Dataset
Alarm

Heart

Hailﬁnder500

Water100

n
17
18
19
20
17
18
19
20
17
18
19
20
18
19
20

usage results consistently in running out on memory for n ≥ 20. At n = 19  BBMarkov uses on
average approx. 1 GB of memory  while Junctor uses close to 30 GB. A further beneﬁt of BBMarkov
is its ability to provide “anytime” solutions during search. In fact  the bounds obtained during search
result at times in ﬁnding optimal solutions relatively fast: Figure 2 right shows the ratio of time
needed to ﬁnd an optimal solution (x-axis) from time needed to terminate search  i.e.  to ﬁnd a
solution and prove its optimality (y-axis)  and in Table 1  with the time needed to ﬁnd an optimal
solution given in parentheses.

5 Conclusions

We introduced a new branch-and-bound approach to learning optimal chordal Markov network
structures  i.e.  decomposable graphs. In addition to core branch-and-bound search  the approach
integrates dynamic programming for obtaining tight bounds and effective variable domain pruning
during search. In terms of practical performance  the approach has the potential of reaching 20
variables within hours of runtime  at which point the competing native dynamic programming
approach Junctor runs out of memory on standard modern computers. When approaching 20
variables  our approach is approximately 30 times as memory-efﬁcient as Junctor. Furthermore  in
contrast to Junctor  the approach is “anytime” as solutions can be obtained already before ﬁnishing
search. Efﬁcient parallelization of the approach is a promising direction for future work.

Acknowledgments

The authors gratefully acknowledge ﬁnancial support from the Academy of Finland under grants
251170 COIN Centre of Excellence in Computational Inference Research  276412  284591  295673 
and 312662; and the Research Funds of the University of Helsinki.

A Proofs

We give a proof for Theorem 1  central in enabling effective symmetric breaking in our branch-and-
bound approach. We start with a deﬁnition and lemma towards the proof.
Deﬁnition 6. Let V = {v1  ...  vn} be a set of vertices and let π and π(cid:48) be some total orders over V .
Let k = mini π(i)(cid:54)=π(cid:48)(i) i be the ﬁrst difference between the orders. If no such difference exists  we
denote π = π(cid:48). Otherwise we denote π < π(cid:48) if and only if π(k) < π(cid:48)(k).
Lemma 1. Let G = (V  E  π) be an ordered decomposable DAG. If there are vi  vj ∈ V such that
the pair (vi  vj) violates the preferred vertex order in G  then there exists an ordered decomposable
DAG G(cid:48) = (V  E(cid:48)  π(cid:48))  where 1. G(cid:48) belongs to the same equivalence class with G  2. the pair (vi  vj)
does not violate the preferred vertex order in G(cid:48)  and 3. π < π(cid:48).

8

Proof. We begin by deﬁning a directed clique tree C = (V E) over G.
Given vk ∈ V   let Ck = paG(vk) ∪ {vk} be the clique deﬁned by vk in G. The vertices of C are
these cliques; we also add an empty set as a clique to make sure the cliques form a tree (and not a
forest). Formally  V = {Ck | vk ∈ V } ∪ {∅}.
Given vk ∈ V   where paG(vk) (cid:54)= ∅  let φk = argmaxv(cid:96)∈paG(vk)π−1((cid:96)) denote the parent of vk in
G that is in the least signiﬁcant position in π. Now  the edges of C are

E = {(∅  Ck) | Ck = {vk}  vk ∈ V } ∪ {(C(cid:96)  Ck) | v(cid:96) = φk  Ck (cid:54)= {vk}  vk ∈ V }.

I.(cid:83)

In words  if vk ∈ V is a source vertex in G (i.e.  Ck = {vk})  then the parent of Ck is ∅ in C.
Otherwise (i.e.  Ck (cid:54)= {vk}) the parent of Ck is C(cid:96)  where v(cid:96) is the closest vertex to vk in order
π that satisﬁes C(cid:96) ∩ paG(vk) (cid:54)= ∅. We see that all the requirements for clique trees hold for C:
C∈V C = V   II. if {v(cid:96)  vk} ∈ E  then either {v(cid:96)  vk} ⊆ Ck or {v(cid:96)  vk} ⊆ C(cid:96)  and III. due to the
decomposability of G  we have Ca ∩ Cc ⊆ Cb on any path from Ca to Cc through Cb (the running
intersection property).
Now assume that there are vi  vj ∈ V such that the pair (vi  vj) violates the preferred vertex order in
G; that is  we have i > j  paG(vi) ⊆ paG(vj) and a path from vi to vj in G. This means that there is
a path from Ci to Cj in C as well.
Let P ∈ V be the parent vertex of Ci in C. We see that Cj exists in a subtree T of C that is separated
from rest of C by P   and where Ci is the root vertex. Let T (cid:48) be a new clique tree that is like T   but
redirected so that Cj is the root vertex of T (cid:48). Let C(cid:48) be a new clique tree that is like C  but T is
replaced with T (cid:48).
We show that C(cid:48) is a valid clique tree. First of all  the vertices (cliques) of C(cid:48) are exactly the same as in
C  so C(cid:48) clearly satisﬁes the requirements I and II. As for the requirement III  consider the non-trivial
case where Ca  Cb ∈ C have a path from Ca to Cb through Ci in C. This means vi /∈ Ca (due to the
way C was constructed)  and so we get

Ca ∩ Cb ⊆ Ci → Ca ∩ Cb ⊆ Ci \ {vi} → Ca ∩ Cb ⊆ paG(vi) ⊆

Def. 3 (2)

paG(vj) ⊆ Cj.

Therefore the running intersection property holds for C(cid:48).
Let ˆπ be the total order by which C(cid:48) is ordered. Let G(cid:48) = (V  E(cid:48)  ˆπ) be a new ordered decomposable
DAG that is equivalent to G  but where the edges E(cid:48) are arranged to follow the order ˆπ.
Finally  we see that G(cid:48) satisﬁes the conditions of the theorem: 1. The cliques of G(cid:48) are identical to
that of G  so G(cid:48) belongs to the same equivalence class with G. 2. We have ˆπ−1(j) < ˆπ−1(i)  and
therefore there is no path from vi to vj in G(cid:48). Thus the pair (vi  vj) does not violate the preferred
vertex order in G(cid:48). 3. Let o = π−1(i). We have ˆπ(o) = j < i = π(o). Furthermore  the change from
T to T (cid:48) in C(cid:48) did not affect any vertex whose position was earlier than o. Therefore ˆπ(k) = π(k) for
all k = 1...(o − 1). This implies ˆπ < π.

Proof of Theorem 1. Consider the following procedure for ﬁnding G(cid:48).

1. Select vi  vj ∈ V where the pair (vi  vj) violates the preferred vertex order in G. If there

are no such vertices  assign G(cid:48) ← G and terminate.
2. Let π be the total order of the vertices of G. Construct an ordered decomposable DAG
ˆG = (V  ˆE  π(cid:48)) such that I. the pair (vi  vj) does not violate the preferred vertex order in ˆG 
II. ˆG belongs to the same equivalent class with G  and III. π(cid:48) < π. By Lemma 1  ˆG can be
constructed from G.

3. Assign G ← ˆG and return to step 1.

It is clear that when the procedure terminates  G(cid:48) belongs to same equivalence class with G and there
are no violations of the preferred vertex order in G(cid:48). We also see that the total order of G (i.e.  π)
is lexicographically strictly decreasing every time the step 3 is reached. There are ﬁnite amount of
possible permutations (total orders) and therefore the procedure converges. The existence of this
procedure and its correctness proves that G(cid:48) exists.

9

References
[1] Haley J. Abel and Alun Thomas. Accuracy and computational efﬁciency of a graphical
modeling approach to linkage disequilibrium estimation. Statistical Applications in Genetics
and Molecular Biology  143(10.1)  2017.

[2] Mark Bartlett and James Cussens.

Integer linear programming for the Bayesian network

structure learning problem. Artiﬁcial Intelligence  244:258–271  2017.

[3] Cassio P. de Campos and Qiang Ji. Efﬁcient structure learning of Bayesian networks using

constraints. Journal of Machine Learning Research  12:663–689  2011.

[4] David Maxwell Chickering. A transformational characterization of equivalent Bayesian network

structures. In Proc. UAI  pages 87–98. Morgan Kaufmann  1995.

[5] David Maxwell Chickering. Learning equivalence classes of Bayesian network structures.

Journal of Machine Learning Research  2:445–498  2002.

[6] Jukka Corander  Tomi Janhunen  Jussi Rintanen  Henrik J. Nyman  and Johan Pensar. Learning
chordal Markov networks by constraint satisfaction. In Proc. NIPS  pages 1349–1357  2013.

[7] A. Philip Dawid and Steffen L. Lauritzen. Hyper Markov laws in the statistical analysis of

decomposable graphical models. Annals of Statistics  21(3):1272–1317  09 1993.

[8] Cassio P. de Campos and Qiang Ji. Properties of Bayesian Dirichlet scores to learn Bayesian

network structures. In Proc. AAAI  pages 431–436. AAAI Press  2010.

[9] Petros Dellaportas and Jonathan J. Forster. Markov chain Monte Carlo model determination for

hierarchical and graphical log-linear models. Biometrika  86(3):615–633  1999.

[10] Paolo Giudici and Peter J. Green. Decomposable graphical Gaussian model determination.

Biometrika  86(4):785  1999.

[11] Peter J. Green and Alun Thomas. Sampling decomposable graphs using a Markov chain on

junction trees. Biometrika  100(1):91  2013.

[12] Tomi Janhunen  Martin Gebser  Jussi Rintanen  Henrik Nyman  Johan Pensar  and Jukka Coran-
der. Learning discrete decomposable graphical models via constraint optimization. Statistics
and Computing  27(1):115–130  2017.

[13] Kustaa Kangas  Mikko Koivisto  and Teppo M. Niinimäki. Learning chordal Markov networks

by dynamic programming. In Proc. NIPS  pages 2357–2365  2014.

[14] Kustaa Kangas  Teppo Niinimäki  and Mikko Koivisto. Averaging of decomposable graphs by

dynamic programming and sampling. In Proc. UAI  pages 415–424. AUAI Press  2015.

[15] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques.

MIT press  2009.

[16] K. S. Sesh Kumar and Francis R. Bach. Convex relaxations for learning bounded-treewidth de-
composable graphs. In Proc. ICML  volume 28 of JMLR Workshop and Conference Proceedings 
pages 525–533. JMLR.org  2013.

[17] Steffen L. Lauritzen and David J. Spiegelhalter. Local computations with probabilities on
graphical structures and their application to expert systems. In Glenn Shafer and Judea Pearl 
editors  Readings in Uncertain Reasoning  pages 415–448. Morgan Kaufmann Publishers Inc. 
1990.

[18] Gérard Letac and Hélène Massam. Wishart distributions for decomposable graphs. The Annals

of Statistics  35(3):1278–1323  2007.

[19] David Madigan  Jeremy York  and Denis Allard. Bayesian graphical models for discrete data.

International Statistical Review/Revue Internationale de Statistique  pages 215–232  1995.

10

[20] Brandon M. Malone and Changhe Yuan. A depth-ﬁrst branch and bound algorithm for learning
optimal Bayesian networks. In GKR 2013 Revised Selected Papers  volume 8323 of Lecture
Notes in Computer Science  pages 111–122. Springer  2014.

[21] Kari Rantanen. Learning score-optimal chordal Markov networks via branch and bound.

Master’s thesis  University of Helsinki  Finland  2017.

[22] Tomi Silander and Petri Myllymäki. A simple approach for ﬁnding the globally optimal

Bayesian network structure. In Proc. UAI  pages 445–452. AUAI Press  2006.

[23] Nathan Srebro. Maximum likelihood bounded tree-width Markov networks. Artiﬁcial Intelli-

gence  143(1):123 – 138  2003.

[24] Milan Studený and James Cussens. Towards using the chordal graph polytope in learning
decomposable models. International Journal of Approximate Reasoning  88:259–281  2017.

[25] Joe Suzuki. Learning Bayesian belief networks based on the Minimum Description Length
principle: An efﬁcient algorithm using the B&B technique. In Proc. ICML  pages 462–470.
Morgan Kaufmann  1996.

[26] Joe Suzuki and Jun Kawahara. Branch and Bound for regular Bayesian network structure

learning. In Proc. UAI. AUAI Press  2017.

[27] Claudia Tarantola. MCMC model determination for discrete graphical models. Statistical

Modelling  4(1):39–61  2004.

[28] Jin Tian. A branch-and-bound algorithm for MDL learning Bayesian networks. In Proc. UAI 

pages 580–588. Morgan Kaufmann  2000.

[29] Peter van Beek and Hella-Franziska Hoffmann. Machine learning of Bayesian networks using
constraint programming. In Proc. CP  volume 9255 of Lecture Notes in Computer Science 
pages 429–445. Springer  2015.

[30] Claudio J. Verzilli  Nigel Stallard  and John C. Whittaker. Bayesian graphical models for
genomewide association studies. The American Journal of Human Genetics  79(1):100–112 
2006.

[31] Ami Wiesel  Yonina C. Eldar  and Alfred O. Hero III. Covariance estimation in decomposable
Gaussian graphical models. IEEE Transactions on Signal Processing  58(3):1482–1492  2010.

[32] Changhe Yuan and Brandon M. Malone. Learning optimal Bayesian networks: A shortest path

perspective. Journal of Artiﬁcial Intelligence Research  48:23–65  2013.

11

,Kari Rantanen
Antti Hyttinen
Matti Järvisalo
Mikhail Belkin
Daniel Hsu
Partha Mitra