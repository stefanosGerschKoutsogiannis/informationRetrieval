2015,Natural Neural Networks,We introduce Natural Neural Networks  a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular  we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer  while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG)  which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks  and showcase its scalability by training on the large-scale ImageNet Challenge dataset.,Natural Neural Networks

Guillaume Desjardins  Karen Simonyan  Razvan Pascanu  Koray Kavukcuoglu

{gdesjardins simonyan razp korayk}@google.com

Google DeepMind  London

Abstract

We introduce Natural Neural Networks  a novel family of algorithms that speed up
convergence by adapting their internal representation during training to improve
conditioning of the Fisher matrix. In particular  we show a speciﬁc example that
employs a simple and efﬁcient reparametrization of the neural network weights by
implicitly whitening the representation obtained at each layer  while preserving
the feed-forward computation of the network. Such networks can be trained efﬁ-
ciently via the proposed Projected Natural Gradient Descent algorithm (PRONG) 
which amortizes the cost of these reparametrizations over many parameter up-
dates and is closely related to the Mirror Descent online learning algorithm. We
highlight the beneﬁts of our method on both unsupervised and supervised learn-
ing tasks  and showcase its scalability by training on the large-scale ImageNet
Challenge dataset.

1

Introduction

Deep networks have proven extremely successful across a broad range of applications. While their
deep and complex structure affords them a rich modeling capacity  it also creates complex depen-
dencies between the parameters which can make learning difﬁcult via ﬁrst order stochastic gradient
descent (SGD). As long as SGD remains the workhorse of deep learning  our ability to extract high-
level representations from data may be hindered by difﬁcult optimization  as evidenced by the boost
in performance offered by batch normalization (BN) [7] on the Inception architecture [25].
Though its adoption remains limited  the natural gradient [1] appears ideally suited to these difﬁcult
optimization issues. By following the direction of steepest descent on the probabilistic manifold 
the natural gradient can make constant progress over the course of optimization  as measured by the
Kullback-Leibler (KL) divergence between consecutive iterates. Utilizing the proper distance mea-
sure ensures that the natural gradient is invariant to the parametrization of the model. Unfortunately 
its application has been limited due to its high computational cost. Natural gradient descent (NGD)
typically requires an estimate of the Fisher Information Matrix (FIM) which is square in the number
of parameters  and worse  it requires computing its inverse. Truncated Newton methods can avoid
explicitly forming the FIM in memory [12  15]  but they require an expensive iterative procedure to
compute the inverse. Such computations can be wasteful as they do not take into account the highly
structured nature of deep models.
Inspired by recent work on model reparametrizations [17  13]  our approach starts with a sim-
ple question: can we devise a neural network architecture whose Fisher is constrained to be
identity? This is an important question  as SGD and NGD would be equivalent in the resulting
model. The main contribution of this paper is in providing a simple  theoretically justiﬁed network
reparametrization which approximates via ﬁrst-order gradient descent  a block-diagonal natural gra-
dient update over layers. Our method is computationally efﬁcient due to the local nature of the
reparametrization  based on whitening  and the amortized nature of the algorithm. Our second con-
tribution is in unifying many heuristics commonly used for training neural networks  under the roof
of the natural gradient  while highlighting an important connection between model reparametriza-
tions and Mirror Descent [3]. Finally  we showcase the efﬁciency and the scalability of our method

1

across a broad-range of experiments  scaling our method from standard deep auto-encoders to large
convolutional models on ImageNet[20]  trained across multiple GPUs. This is to our knowledge the
ﬁrst-time a (non-diagonal) natural gradient algorithm is scaled to problems of this magnitude.

2 The Natural Gradient

This section provides the necessary background and derives a particular form of the FIM whose
structure will be key to our efﬁcient approximation. While we tailor the development of our method
to the classiﬁcation setting  our approach generalizes to regression and density estimation.

2.1 Overview
We consider the problem of ﬁtting the parameters ✓ 2 RN of a model p(y | x; ✓) to an empirical
distribution ⇡(x  y) under the log-loss. We denote by x 2X the observation vector and y 2Y its
associated label. Concretely  this stochastic optimization problem aims to solve:
(1)

✓⇤ 2 argmin✓ E(x y)⇠⇡ [ log p(y | x  ✓)] .

Deﬁning the per-example loss as `(x  y)  Stochastic Gradient Descent (SGD) performs the above
minimization by iteratively following the direction of steepest descent  given by the column vector
r = E⇡ [d`/d✓]. Parameters are updated using the rule ✓(t+1) ✓(t)  ↵(t)r(t)  where ↵ is a
learning rate. An equivalent proximal form of gradient descent [4] reveals the precise nature of ↵:

✓(t+1) = argmin✓⇢h✓  ri +

1

2↵(t)✓  ✓(t)

2

2

Namely  each iterate ✓(t+1) is the solution to an auxiliary optimization problem  where ↵ controls
the distance between consecutive iterates  using an L2 distance. In contrast  the natural gradient
relies on the KL-divergence between iterates  a more appropriate distance measure for probability
distributions. Its metric is determined by the Fisher Information matrix 

F✓ = Ex⇠⇡(Ey⇠p(y|x ✓)"✓ @ log p

@✓ ◆✓ @ log p

@✓ ◆T#)  

(2)

(3)

i.e. the covariance of the gradients of the model log-probabilities wrt. its parameters. The natural
gradient direction is then obtained as rN = F 1
✓ r. See [15  14] for a recent overview of the topic.
2.2 Fisher Information Matrix for MLPs

We start by deriving the precise form of the Fisher for a canonical multi-layer perceptron (MLP)
composed of L layers. We consider the following deep network for binary classiﬁcation  though our
approach generalizes to an arbitrary number of output classes.

p(y = 1 | x) ⌘ hL = fL(WLhL1 + bL)

···
h1 = f1 (W1x + b1)

(4)

The parameters of the MLP  denoted ✓ = {W1  b1 ···   WL  bL}  are the weights Wi 2 RNi⇥Ni1
connecting layers i and i  1  and the biases bi 2 RNi. fi is an element-wise non-linear function.
Let us deﬁne i to be the backpropagated gradient through the i-th non-linearity. We ignore the
off block-diagonal components of the Fisher matrix and focus on the block FWi  corresponding to
interactions between parameters of layer i. This block takes the form:

FWi = Ex⇠⇡

y⇠phvecihT

i1 vecihT

itTi  

where vec(X) is the vectorization function yielding a column vector from the rows of matrix X.
Assuming that i and activations hi1 are independent random variables  we can write:

FWi(km  ln) ⇡ Ex⇠⇡
y⇠p

[i(k)i(l)] E⇡ [hi1(m)hi1(n)]  

(5)

2

✓t

1
2

F (✓t)

✓t+T

F (✓t)1

2

⌦t

⌦t+1

⌦t+T

Figure 1: (a) A 2-layer natural neural network. (b) Illustration of the projections involved in PRONG.

where X(i  j) is the element at row i and column j of matrix X and x(i) is the i-th element of vector
x. FWi(km  ln) is the entry in the Fisher capturing interactions between parameters Wi(k  m)
and Wj(l  n). Our hypothesis  veriﬁed experimentally in Sec. 4.1  is that we can greatly improve

conditioning of the Fisher by enforcing that E⇡⇥hihT

i⇤ = I  for all layers of the network  despite

ignoring possible correlations in the ’s and off block diagonal terms of the Fisher.

3 Projected Natural Gradient Descent

(cid:42)(cid:82)(cid:82)(cid:74)(cid:79)(cid:72)(cid:3)(cid:70)(cid:82)(cid:81)(cid:73)(cid:76)(cid:71)(cid:72)(cid:81)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:83)(cid:85)(cid:82)(cid:83)(cid:85)(cid:76)(cid:72)(cid:87)(cid:68)(cid:85)(cid:92)

This section introduces Whitened Neural Networks (WNN)  which perform approximate whitening
of their internal representations. We begin by presenting a novel whitened neural layer  with the
assumption that the network statistics µi(✓) = E[hi] and ⌃i(✓) = E[hihT
i ] are ﬁxed. We then show
how these layers can be adapted to efﬁciently track population statistics over the course of training.
The resulting learning algorithm is referred to as Projected Natural Gradient Descent (PRONG). We
highlight an interesting connection between PRONG and Mirror Descent in Section 3.3.

3.1 A Whitened Neural Layer

The building block of WNN is the following neural layer 

hi = fi (ViUi1 (hi1  ci1) + di) .

(6)

Compared to Eq. 4  we have introduced an explicit centering parameter ci1 2 RNi1  equal to
µi1  which ensures that the input to the dot product has zero mean in expectation. This is anal-
ogous to the centering reparametrization for Deep Boltzmann Machines [13]. The weight matrix
Ui1 2 RNi1⇥Ni1 is a per-layer PCA-whitening matrix whose rows are obtained from an eigen-
decomposition of ⌃i1:

⌃i = ˜Ui · diag (i) · ˜U T

i =) Ui = diag (i + ✏) 1

2 · ˜U T
i .

(7)

The hyper-parameter ✏ is a regularization term controlling the maximal multiplier on the learning
rate  or equivalently the size of the trust region. The parameters Vi 2 RNi⇥Ni1 and di 2 RNi are
analogous to the canonical parameters of a neural network as introduced in Eq. 4  though operate
in the space of whitened unit activations Ui(hi  ci). This layer can be stacked to form a deep
neural network having L layers  with model parameters ⌦= {V1  d1 ··· VL  dL} and whitening
coefﬁcients = {U0  c0 ···   UL1  cL1}  as depicted in Fig. 1a.
Though the above layer might appear over-parametrized at ﬁrst glance  we crucially do not learn
the whitening coefﬁcients via loss minimization  but instead estimate them directly from the model
statistics. These coefﬁcients are thus constants from the point of view of the optimizer and simply
serve to improve conditioning of the Fisher with respect to the parameters ⌦  denoted F⌦. Indeed 
using the same derivation that led to Eq. 5  we can see that the block-diagonal terms of F⌦ now

involve terms E⇥(Uihi)(Uihi)T⇤  which equals identity by construction.

3.2 Updating the Whitening Coefﬁcients

As the whitened model parameters ⌦ evolve during training  so do the statistics µi and ⌃i. For our
model to remain well conditioned  the whitening coefﬁcients must be updated at regular intervals 

3

if mod(t  T ) = 0 then
for all layers i do

Algorithm 1 Projected Natural Gradient Descent
1: Input: training set D  initial parameters ✓.
2: Hyper-parameters: reparam. frequency T   number of samples Ns  regularization term ✏.
3: Ui I; ci 0; t 0
4: repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: until convergence

Compute canonical parameters Wi = ViUi1; bi = di  Wici1.
Estimate µi and ⌃i  using Ns samples from D.
Update ci from µi and Ui from eigen decomp. of ⌃i + ✏I.
Update parameters Vi WiU1
i1; di bi + ViUi1ci1.

end if
Perform SGD update wrt. ⌦ using samples from D.
t t + 1

end for

. amortize cost of lines [6-11]

. proj. P 1

 (⌦)

. update 
. proj. P(✓)

while taking care not to interfere with the convergence properties of gradient descent. This can be
achieved by coupling updates to  with corresponding updates to ⌦ such that the overall function
implemented by the MLP remains unchanged  e.g. by preserving the product ViUi1 before and
after each update to the whitening coefﬁcients (with an analoguous constraint on the biases).
Unfortunately  while estimating the mean µi and diag(⌃i) could be performed online over a mini-
batch of samples as in the recent Batch Normalization scheme [7]  estimating the full covariance
matrix will undoubtedly require a larger number of samples. While statistics could be accumulated
online via an exponential moving average as in RMSprop [27] or K-FAC [8]  the cost of the eigen-
decomposition required for computing the whitening matrix Ui remains cubic in the layer size.
In the simplest instantiation of our method  we exploit the smoothness of gradient descent by simply
amortizing the cost of these operations over T consecutive updates. SGD updates in the whitened
model will be closely aligned to NGD immediately following the reparametrization. The quality
of this approximation will degrade over time  until the subsequent reparametrization. The resulting
algorithm is shown in the pseudo-code of Algorithm 1. We can improve upon this basic amor-
tization scheme by updating the whitened parameters ⌦ using a per-batch diagonal natural gra-
dient update  whose statistics are computed online.
In our framework  this can be implemented
via the reparametrization Wi = ViDi1Ui1  where Di1 is a diagonal matrix updated such that
V [Di1Ui1hi1] = 1  for each minibatch. Updates to Di1 can be compensated for exactly and
cheaply by scaling the rows of Ui1 and columns of Vi accordingly. A simpler implementation of
this idea is to combine PRONG with batch-normalization  which we denote as PRONG+.

3.3 Duality and Mirror Descent

There is an inherent duality between the parameters ⌦ of our whitened neural layer and the param-
eters ✓ of a canonical model. Indeed  there exist linear projections P(✓) and P 1
 (⌦)  which map
from canonical parameters ✓ to whitened parameters ⌦  and vice-versa. P(✓) corresponds to line
10 of Algorithm 1  while P 1
 (⌦) corresponds to line 7. This duality between ✓ and ⌦ reveals a
close connection between PRONG and Mirror Descent [3].
Mirror Descent (MD) is an online learning algorithm which generalizes the proximal form of gra-
dient descent to the class of Bregman divergences B (q  p)  where q  p 2  and : ! R is a
strictly convex and differentiable function. Replacing the L2 distance by B   mirror descent solves
the proximal problem of Eq. 2 by applying ﬁrst-order updates in a dual space and then project-
ing back onto the primal space. Deﬁning ⌦= r✓ (✓) and ✓ = r⇤⌦ (⌦)  with ⇤ the complex
conjugate of   the mirror descent updates are given by:

⌦(t+1) = r✓ ⇣✓(t)⌘  ↵(t)r✓
✓(t+1) = r⌦ ⇤⇣⌦(t+1)⌘

4

(8)

(9)

(a)

(b)

(c)

Figure 2: Fisher matrix for a small MLP (a) before and (b) after the ﬁrst reparametrization. Best viewed in
colour. (c) Condition number of the FIM during training  relative to the initial conditioning. All models where
initialized such that the initial conditioning was the same  and learning rate where adjusted such that they reach
roughly the same training error in the given time.

It is well known [26  18] that the natural gradient is a special case of MD  where the distance
generating function 1 is chosen to be (✓) = 1
The mirror updates are somewhat unintuitive however. Why is the gradient r✓ applied to the dual
space if it has been computed in the space of parameters ✓ ? This is where PRONG relates to MD. It
2 ✓TpF✓   instead of the previously deﬁned (✓) 
is trivial to show that using the function ˜ (✓) = 1
enables us to directly update the dual parameters using r⌦  the gradient computed directly in the
dual space. Indeed  the resulting updates can be shown to implement the natural gradient and are
thus equivalent to the updates of Eq. 9 with the appropriate choice of (✓):

2 ✓T F✓ .

1

F  1

d✓

2 ✓(t)  ↵(t)E⇡ d`
˜⌦(t+1) = r✓ ˜ ⇣✓(t)⌘  ↵(t)r⌦ = F
˜✓(t+1) = r⌦ ˜ ⇤⇣ ˜⌦(t+1)⌘ = ✓(t)  ↵(t)F 1E⇡ d`
d✓
The operators ˜r and ˜r ⇤ correspond to the projections P(✓) and P 1
 (⌦) used by PRONG
to map from the canonical neural parameters ✓ to those of the whitened layers ⌦. As illustrated
in Fig. 1b  the advantage of this whitened form of MD is that one may amortize the cost of the
projections over several updates  as gradients can be computed directly in the dual parameter space.

2

(10)

3.4 Related Work

This work extends the recent contributions of [17] in formalizing many commonly used heuristics
for training MLPs: the importance of zero-mean activations and gradients [10  21]  as well as the
importance of normalized variances in the forward and backward passes [10  21  6]. More recently 
Vatanen et al. [28] extended their previous work [17] by introducing a multiplicative constant i
to the centered non-linearity. In contrast  we introduce a full whitening matrix Ui and focus on
whitening the feedforward network activations  instead of normalizing a geometric mean over units
and gradient variances.
The recently introduced batch normalization (BN) scheme [7] quite closely resembles a diagonal
version of PRONG  the main difference being that BN normalizes the variance of activations before
the non-linearity  as opposed to normalizing the latent activations by looking at the full covariance.
Furthermore  BN implements normalization by modifying the feed-forward computations thus re-
quiring the method to backpropagate through the normalization operator. A diagonal version of
PRONG also bares an interesting resemblance to RMSprop [27  5]  in that both normalization terms
involve the square root of the FIM. An important distinction however is that PRONG applies this
update in the whitened parameter space  thus preserving the natural gradient interpretation.

1As the Fisher and thus ✓ depend on the parameters ✓(t)  these should be indexed with a time superscript 

which we drop for clarity.

5

(a)

(b)

(c)

(d)

Figure 3: Optimizing a deep auto-encoder on MNIST. (a) Impact of eigenvalue regularization term ✏.
(b)
Impact of amortization period T showing that initialization with the whitening reparametrization is important
for achieving faster learning and better error rate. (c) Training error vs number of updates. (d) Training error
vs cpu-time. Plots (c-d) show that PRONG achieves better error rate both in number of updates and wall clock.

K-FAC [8] is closely related to PRONG and was developed concurrently to our method. It targets
the same layer-wise block-diagonal of the Fisher  approximating each block as in Eq. 5. Unlike
our method however  KFAC does not approximate the covariance of backpropagated gradients as
the identity  and further estimates the required statistics using exponential moving averages (un-
like our approach based on amortization). Similar techniques can be found in the preconditioning
of the Kaldi speech recognition toolkit [16]. By modeling the Fisher matrix as the covariance of
a sparsely connected Gaussian graphical model  FANG [19] represents a general formalism for
exploiting model structure to efﬁciently compute the natural gradient. One application to neural
networks [8] is in decorrelating gradients across neighbouring layers.
A similar algorithm to PRONG was later found in [23]  where it appeared simply as a thought
experiment  but with no amortization or recourse for efﬁciently computing F .

4 Experiments

We begin with a set of diagnostic experiments which highlight the effectiveness of our method at
improving conditioning. We also illustrate the impact of the hyper-parameters T and ✏  controlling
the frequency of the reparametrization and the size of the trust region. Section 4.2 evaluates PRONG
on unsupervised learning problems  where models are both deep and fully connected. Section 4.3
then moves onto large convolutional models for image classiﬁcation. Experimental details such as
model architecture or hyper-parameter conﬁgurations can be found in the supplemental material.

4.1

Introspective Experiments

Conditioning. To provide a better understanding of the approximation made by PRONG  we train
a small 3-layer MLP with tanh non-linearities  on a downsampled version of MNIST (10x10) [11].
The model size was chosen in order for the full Fisher to be tractable. Fig. 2(a-b) shows the FIM
of the middle hidden layers before and after whitening the model activations (we took the absolute
value of the entries to improve visibility). Fig. 2c depicts the evolution of the condition number
of the FIM during training  measured as a percentage of its initial value (before the ﬁrst whitening
reparametrization in the case of PRONG). We present such curves for SGD  RMSprop  batch nor-
malization and PRONG. The results clearly show that the reparametrization performed by PRONG
improves conditioning (reduction of more than 95%). These observations conﬁrm our initial as-
sumption  namely that we can improve conditioning of the block diagonal Fisher by whitening
activations alone.
Sensitivity of Hyper-Parameters. Figures 3a- 3b highlight the effect of the eigenvalue regular-
ization term ✏ and the reparametrization interval T . The experiments were performed on the best

6

(a)

(b)

(c)

(d)

Figure 4: Classiﬁcation error on CIFAR-10 (a-b) and ImageNet (c-d). On CIFAR-10  PRONG achieves better
test error and converges faster. On ImageNet  PRONG+ achieves comparable validation error while maintain-
ing a faster covergence rate.

performing auto-encoder of Section 4.2 on the MNIST dataset. Figures 3a- 3b plot the reconstruction
error on the training set for various values of ✏ and T . As ✏ determines a maximum multiplier on the
learning rate  learning becomes extremely sensitive when this learning rate is high2. For smaller step
sizes however  lowering ✏ can yield signiﬁcant speedups often converging faster than simply using a
larger learning rate. This conﬁrms the importance of the manifold curvature for optimization (lower
✏ allows for different directions to be scaled drastically different according to their corresponding
curvature). Fig 3b compares the impact of T for models having a proper whitened initialization
(solid lines)  to models being initialized with a standard “fan-in” initialization (dashed lines) [10].
These results are quite surprising in showing the effectiveness of the whitening reparametrization
as a simple initialization scheme. That being said  performance can degrade due to ill conditioning
when T becomes excessively large (T = 105).

4.2 Unsupervised Learning

Following Martens [12]  we compare PRONG on the task of minimizing reconstruction error of a
dense 8-layer auto-encoder on the MNIST dataset. Reconstruction error with respect to updates and
wallclock time are shown in Fig. 3 (c d). We can see that PRONG signiﬁcantly outperforms the
baseline methods  by up to an order of magnitude in number of updates. With respect to wallclock 
our method signiﬁcantly outperforms the baselines in terms of time taken to reach a certain error
threshold  despite the fact that the runtime per epoch for PRONG was 3.2x that of SGD  compared
to batch normalization (2.3x SGD) and RMSprop (9x SGD). Note that these timing numbers reﬂect
performance under the optimal choice of hyper-parameters  which in the case of batch normalization
yielded a batch size of 256  compared to 128 for all other methods. Further breaking down the
performance  34% of the runtime of PRONG was spent performing the whitening reparametrization 
compared to 4% for estimating the per layer means and covariances. This conﬁrms that amortization
is paramount to the success of our method.3

4.3 Supervised Learning

We now evaluate our method for training deep supervised convolutional networks for object recog-
nition. Following [7]  we perform whitening across feature maps only: that is we treat pixels in a
given feature map as independent samples. This allows us to implement the whitened neural layer
as a sequence of two convolutions  where the ﬁrst is by a 1x1 whitening ﬁlter. PRONG is compared
to SGD  RMSprop and batch normalization  with each algorithm being accelerated via momentum.
Results are presented on CIFAR-10 [9] and the ImageNet Challenge (ILSVRC12) datasets [20]. In
both cases  learning rates were decreased using a “waterfall” annealing schedule  which divided the
learning rate by 10 when the validation error failed to improve after a set number of evaluations.

2Unstable combinations of learning rates and ✏ are omitted for clarity.
3We note that our whitening implementation is not optimized  as it does not take advantage of GPU accel-

eration. Runtime is therefore expected to improve as we move the eigen-decompositions to GPU.

7

CIFAR-10 We now evaluate PRONG on CIFAR-10  using a deep convolutional model inspired
by the VGG architecture [22]. The model was trained on 24 ⇥ 24 random crops with random
horizontal reﬂections. Model selection was performed on a held-out validation set of 5k examples.
Results are shown in Fig. 4. With respect to training error  PRONG and BN seem to offer similar
speedups compared to SGD with momentum. Our hypothesis is that the beneﬁts of PRONG are more
pronounced for densely connected networks  where the number of units per layer is typically larger
than the number of maps used in convolutional networks. Interestingly  PRONG generalized better 
achieving 7.32% test error vs. 8.22% for batch normalization. This reﬂects the ﬁndings of [15] 
which showed how NGD can leverage unlabeled data for better generalization: the “unlabeled” data
here comes from the extra crops and reﬂections observed when estimating the whitening matrices.

ImageNet Challenge Dataset Our ﬁnal set of experiments aims to show the scalability of our
method. We applied our natural gradient algorithm to the large-scale ILSVRC12 dataset (1.3M im-
ages labelled into 1000 categories) using the Inception architecture [7]. In order to scale to problems
of this size  we parallelized our training loop so as to split the processing of a single minibatch (of
size 256) across multiple GPUs. Note that PRONG can scale well in this setting  as the estimation
of the mean and covariance parameters of each layer is also embarassingly parallel. Eight GPUs
were used for computing gradients and estimating model statistics  though the eigen decomposition
required for whitening was itself not parallelized in the current implementation. Given the difﬁculty
of the task  we employed the enhanced version of the algorithm (PRONG+)  as simple periodic
whitening of the model proved to be unstable. Figure 4 (c-d) shows that batch normalisation and
PRONG+ converge to approximately the same top-1 validation error (28.6% vs 28.9% respectively)
for similar cpu-time. In comparison  SGD achieved a validation error of 32.1%. PRONG+ however
exhibits much faster convergence initially: after 105 updates it obtains around 36% error compared
to 46% for BN alone. We stress that the ImageNet results are somewhat preliminary. While our
top-1 error is higher than reported in [7] (25.2%)  we used a much less extensive data augmentation
pipeline. We are only beginning to explore what natural gradient methods may achieve on these
large scale optimization problems and are encouraged by these initial ﬁndings.

5 Discussion

We began this paper by asking whether convergence speed could be improved by simple model
reparametrizations  driven by the structure of the Fisher matrix. From a theoretical and experimental
perspective  we have shown that Whitened Neural Networks can achieve this via a simple  scalable
and efﬁcient whitening reparametrization. They are however one of several possible instantiations
of the concept of Natural Neural Networks. In a previous incarnation of the idea  we exploited a
similar reparametrization to include whitening of backpropagated gradients4. We favor the simpler
approach presented in this paper  as we generally found the alternative less stable for deep networks.
This may be due to the difﬁculty in estimating gradient covariances in lower layers  a problem which
seems to mirror the famous vanishing gradient problem. [17].
Maintaining whitened activations may also offer additional beneﬁts from the point of view of model
compression and generalization. By virtue of whitening  the projection Uihi forms an ordered rep-
resentation  having least and most signiﬁcant bits. The sharp roll-off in the eigenspectrum of ⌃i
may explain why deep networks are ammenable to compression [2]. Similarly  one could envision
spectral versions of Dropout [24] where the dropout probability is a function of the eigenvalues.
Alternative ways of orthogonalizing the representation at each layer should also be explored  via al-
ternate decompositions of ⌃i  or perhaps by exploiting the connection between linear auto-encoders
and PCA. We also plan on pursuing the connection with Mirror Descent and further bridging the
gap between deep learning and methods from online convex optimization.

Acknowledgments

We are extremely grateful to Shakir Mohamed for invaluable discussions and feedback in the preparation of
this manuscript. We also thank Philip Thomas  Volodymyr Mnih  Raia Hadsell  Sergey Ioffe and Shane Legg
for feedback on the paper.

4The weight matrix can be parametrized as Wi = RT

i ViUi1  with Ri the whitening matrix for i.

8

References
[1] Shun-ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation  1998.
[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS. 2014.
[3] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex

optimization. Oper. Res. Lett.  2003.

[4] P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. ArXiv e-prints 

December 2009.

[5] John Duchi  Elad Hazan  and Yoram Singer. Adaptive subgradient methods for online learning and

stochastic optimization. In JMLR. 2011.

[6] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural

networks. In AISTATS  May 2010.

[7] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing

internal covariate shift. ICML  2015.

[8] Roger Grosse James Martens. Optimizing neural networks with kronecker-factored approximate curva-

ture. In ICML  June 2015.

[9] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis  University of

Toronto  2009.

[10] Yann LeCun  L´eon Bottou  Genevieve B. Orr  and Klaus-Robert M¨uller. Efﬁcient backprop. In Neural

Networks  Tricks of the Trade  Lecture Notes in Computer Science LNCS 1524. Springer Verlag  1998.

[11] Yann Lecun  Lon Bottou  Yoshua Bengio  and Patrick Haffner. Gradient-based learning applied to docu-

ment recognition. In Proceedings of the IEEE  pages 2278–2324  1998.

[12] James Martens. Deep learning via Hessian-free optimization. In ICML  June 2010.
[13] K.-R. M¨uller and G. Montavon. Deep boltzmann machines and the centering trick.

G. Montavon  and G. B. Orr  editors  Neural Networks: Tricks of the Trade. Springer  2013.

In K.-R. M¨uller 

[14] Yann Ollivier. Riemannian metrics for neural networks. arXiv  abs/1303.0818  2013.
[15] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In ICLR  2014.
[16] Daniel Povey  Xiaohui Zhang  and Sanjeev Khudanpur. Parallel training of deep neural networks with

natural gradient and parameter averaging. ICLR workshop  2015.

[17] T. Raiko  H. Valpola  and Y. LeCun. Deep learning made easier by linear transformations in perceptrons.

In AISTATS  2012.

[18] G. Raskutti and S. Mukherjee. The Information Geometry of Mirror Descent. arXiv  October 2013.
[19] Ruslan Salakhutdinov Roger B. Grosse. Scaling up natural gradient by sparsely factorizing the inverse

ﬁsher matrix. In ICML  June 2015.

[20] Olga Russakovsky  Jia Deng  Hao Su  Jonathan Krause  Sanjeev Satheesh  Sean Ma  Zhiheng Huang 
Andrej Karpathy  Aditya Khosla  Michael Bernstein  Alexander C. Berg  and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV)  2015.

[21] Nicol N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical Report

IDSIA-33-98  Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale  1998.

[22] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In

International Conference on Learning Representations  2015.

[23] Jascha Sohl-Dickstein. The natural gradient by analogy to signal whitening  and recipes and tricks for its

use. arXiv  2012.

[24] Nitish Srivastava  Geoffrey Hinton  Alex Krizhevsky  Ilya Sutskever  and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research  2014.
[25] Christian Szegedy  Wei Liu  Yangqing Jia  Pierre Sermanet  Scott Reed  Dragomir Anguelov  Dumitru

Erhan  Vincent Vanhoucke  and Andrew Rabinovich. Going deeper with convolutions. arXiv  2014.

[26] Philip S Thomas  William C Dabney  Stephen Giguere  and Sridhar Mahadevan. Projected natural actor-

critic. In Advances in Neural Information Processing Systems 26. 2013.

[27] Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average of its recent

magnitude. coursera: Neural networks for machine learning. 2012.

[28] Tommi Vatanen  Tapani Raiko  Harri Valpola  and Yann LeCun. Pushing stochastic gradient towards
second-order methods – backpropagation learning with transformations in nonlinearities. ICONIP  2013.

9

,Guillaume Desjardins
Karen Simonyan
Razvan Pascanu
koray kavukcuoglu
David Ha
Jürgen Schmidhuber