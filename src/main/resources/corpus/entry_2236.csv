2014,General Table Completion using a Bayesian Nonparametric Model,Even though heterogeneous databases can be found in a broad variety of applications  there exists a lack of tools for estimating missing data in such databases. In this paper  we provide an efficient and robust table completion tool  based on a Bayesian nonparametric latent feature model. In particular  we propose a general observation model for the Indian buffet process (IBP) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical  ordinal and count) observations. Then  we propose an inference algorithm that scales linearly with the number of observations. Finally  our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard IBP and the Bayesian probabilistic matrix factorization with Gaussian observations.,General Table Completion using a Bayesian

Nonparametric Model

Isabel Valera

Department of Signal Processing

and Communications

University Carlos III in Madrid
ivalera@tsc.uc3m.es

Zoubin Ghahramani

Department of Engineering
University of Cambridge

zoubin@eng.cam.ac.uk

Abstract

Even though heterogeneous databases can be found in a broad variety of applica-
tions  there exists a lack of tools for estimating missing data in such databases. In
this paper  we provide an efﬁcient and robust table completion tool  based on a
Bayesian nonparametric latent feature model. In particular  we propose a general
observation model for the Indian buffet process (IBP) adapted to mixed continuous
(real-valued and positive real-valued) and discrete (categorical  ordinal and count)
observations. Then  we propose an inference algorithm that scales linearly with
the number of observations. Finally  our experiments over ﬁve real databases show
that the proposed approach provides more robust and accurate estimates than the
standard IBP and the Bayesian probabilistic matrix factorization with Gaussian
observations.

1

Introduction

A full 90% of all the data in the world has been generated over the last two years and this expansion
rate will not diminish in the years to come [17]. This extreme availability of data explains the great
investment that both the industry and the research community are expending in data science. Data is
usually organized and stored in databases  which are often large  noisy  and contain missing values.
Missing data may occur in diverse applications due to different reasons. For example  a sensor in
a remote sensor network may be damaged and transmit corrupted data or even cease to transmit;
participants in a clinical study may drop out during the course of the study; or users of a recom-
mendation system rate only a small fraction of the available books  movies  or songs. The presence
of missing values can be challenging when the data is used for reporting  information sharing and
decision support  and as a consequence  missing data treatment has captured the attention in diverse
areas of data science such as machine learning  data mining  and data warehousing and management.
Several studies have shown that probabilistic modeling can help to estimate missing values  detect
errors in databases  or provide probabilistic responses to queries [19]. In this paper  we exclusively
focus on the use of probabilistic modeling for missing data estimation  and assume that the data
are missing completely at random (MCAR). There is extensive literature in probabilistic missing
data estimation and imputation in homogeneous databases  where all the attributes that describe
each object in the database present the same (continuous or discrete) nature. Most of the work
assumes that databases contain only either continuous data  usually modeled as Gaussian variables
[21]  or discrete  that can be either modeled by discrete likelihoods [9] or simply treated as Gaussian
variables [15  21]. However  there still exists a lack of work dealing with heterogeneous databases 
which in fact are common in real applications and where the standard approach is to treat all the
attributes  either continuous or discrete  as Gaussian variables. As a motivating example  consider a
database that contains the answers to a survey  including diverse information about the participants
such as age (count data)  gender (categorical data)  salary (continuous non negative data)  etc.

1

In this paper  we provide a general Bayesian approach for estimating and replacing the missing data
in heterogeneous databases (being the data MCAR)  where the attributes describing each object can
be either discrete  continuous or mixed variables. Speciﬁcally  we account for real-valued  positive
real-valued  categorical  ordinal and count data. To this end  we assume that the information in
the database can be stored in a matrix (or table)  where each row corresponds to an object and
the columns are the attributes that describe the different objects. We propose a novel Bayesian
nonparametric approach for general table completion based on feature modeling  in which each
object is represented by a set of latent variables and the observations are generated from a distribution
determined by those latent features. Since the number of latent variables needed to explain the data
depends on the speciﬁc database  we use the Indian buffet process (IBP) [8]  which places a prior
distribution over binary matrices where the number of columns (latent variables) is unbounded.
The standard IBP assumes real-valued observations combined with conjugate likelihood models
that allow for fast inference algorithms [4]. Here  we aim at dealing with heterogeneous databases 
which may contain mixed continuous and discrete observations.
We propose a general observation model for the IBP that accounts for mixed continuous and dis-
crete data  while keeping the properties of conjugate models. This allows us to propose an inference
algorithm that scales linearly with the number of observations. The proposed algorithm does not
only infer the latent variables for each object in the table  but it also provides accurate estimates for
its missing values. Our experiments over ﬁve real databases show that our approach for table com-
pletion outperforms  in terms of accuracy  the Bayesian probabilistic matrix factorization (BPMF)
[15] and the standard IBP which assume Gaussian observations. We also observe that the approach
based on treating mixed continuous and discrete data as Gaussian fails in estimating some attributes 
while the proposed approach provides robust estimates for all the missing values regardless of their
discrete or continuous nature.
The main contributions in this paper are: i) A general observation model (for mixed continuous and
discrete data) for the IBP that allows us to derive an inference algorithm that scales linearly with
the number of objects  and its application to build ii) a general and scalable tool to estimate missing
values in heterogeneous databases. An efﬁcient C-code implementation for Matlab of the proposed
table completion tool is also released on the authors website.

2 Related Work
In recent years  probabilistic modeling has become an attractive option for building database man-
agement systems since it allows estimating missing values  detecting errors  visualizing the data  and
providing probabilistic answers to queries [19]. BayesDB 1 for instance  is a database management
system that resorts to Crosscat [18]  which originally appeared as a Bayesian approach to model hu-
man categorization of objects. BayesDB provides missing data estimates and probabilistic answer
to queries  but it only considers Gaussian and multinomial likelihood functions.
In the literature  probabilistic low-rank matrix factorization approaches have been broadly applied to
table completion (see  e.g.  [14  15  21]). In these approaches  the table database X is approximated
by a low-rank matrix representation X ≈ ZB  where Z and B are usually assumed to be Gaussian
distributed. Most of the works in this area have focused on building automatic recommendation
systems  which appears as the most popular application of missing data estimation [14  15  21].
More speciﬁc models to build recommendation systems can be found in [7  22]  where the authors
assume that the rates each user assign to items are generated by a probabilistic generative model
which  based on the available data  accounts for similarities among users and among items to provide
good estimates of the missing rates.
Probabilistic matrix factorization can also be viewed as latent feature modeling  where each object
is represented by a vector of continuous latent variables. In contrast  the IBP and other latent feature
models (see  e.g.  [16]) assume binary latent features to represent each object. Latent feature models
usually assume homogeneous databases with either real [14  15  21] or categorical data [9  12  13] 
and only a few works consider heterogeneous data  such as mixed real and categorical data [16].
However  up to our knowledge  there are no general latent feature models (nor table completion
tools) to directly deal with heterogeneous databases. To ﬁll this gap  in this paper we provide a
general table completion approach for heterogeneous databases  based on a generalized IBP  that
allows for efﬁcient inference.

1http://probcomp.csail.mit.edu/bayesdb/

2

3 Model Description
Let us assume a table with N objects  where each object is deﬁned by D attributes. We can store
the data in an N × D observation matrix X  in which each D-dimensional row vector is denoted by
n. We consider that column vectors xd (i.e.  each
xn = [x1
dimension in the observation matrix X) may contain the following types of data:

n ] and each entry is denoted by xd

n  . . .   xD

• Continuous variables:

1. Real-valued  i.e.  xd
2. Positive real-valued  i.e.  xd

n ∈ (cid:60)

n ∈ (cid:60)+.

1. Categorical data  i.e.  xd

• Discrete variables:
‘red’  ‘black’}.
n takes values in a ﬁnite ordered set  e.g.  xd
times’  ‘often’  ‘usually’  ‘always’}.
n ∈ {0  . . .  ∞} 

2. Ordinal data  i.e.  xd

3. Count data  i.e.  xd

n takes values in a ﬁnite unordered set  e.g.  xd

n ∈ {‘blue’ 
n ∈ {‘never’  ‘some-

n can be explained by a K-length vector of latent variables
We assume that each observation xd
associated to the n-th data point zn = [zn1  . . .   znK] and a weighting vector2 Bd = [bd
K]
1  . . .   bd
k weight the contribution of k-th the
(being K the number of latent variables)  whose elements bd
latent feature to the d-th dimension of X. We gather the latent binary feature vectors zn in a N × K
matrix Z  which follows an IBP with concentration parameter α  i.e.  Z ∼ IBP(α) [8]. We place a
Gaussian distribution with zero mean and covariance matrix σ2
BIK over the weighting vectors Bd.
For convenience  zn is a K-length row vector  while Bd is a K-length column vector.
To accommodate for all kinds of observed random variables described above  we introduce an auxil-
n  such that when conditioned on the auxiliary variables  the latent variable
iary Gaussian variable yd
n is Gaus-
model behaves as a standard IBP with Gaussian observations. In particular  we assume yd
sian distributed with mean znBd and variance σ2

y  i.e. 
n|zn  Bd) = N (yd

p(yd

y) 
n|znBd  σ2

n to obtain the obser-
and assume that there exists a transformation function over the variables yd
n  mapping the real line (cid:60) into the observation space. The resulting generative model is
vations xd
shown in Figure 1  where Z is the IBP latent matrix  and Yd and Bd contain  respectively  the
k for the d-dimension of the data. Ad-
auxiliary Gaussian variables yd
ditionally  Ψd denotes the set of auxiliary random variables needed to obtain the observation vector
xd given Yd  and Hd contains the hyper-parameters associated to the random variables in Ψd. This
n are independent given the latent matrix Z  the weighting
model assumes that the observations xd
matrices Bd and the auxiliary variables Ψd. Therefore  the likelihood can be factorized as

n and the weighting factors bd

p(X|Z {Bd  Ψd}D

d=1) =

p(xd|Z  Bd  Ψd) =

p(xd

n|zn  Bd  Ψd).

D(cid:89)

d=1

D(cid:89)

N(cid:89)

d=1

n=1

Note that  if we assume Gaussian observations and set Yd = xd  this model resembles the standard
IBP with Gaussian observations [8]. In addition  conditioned on the variables Yd  we can infer the
latent matrix Z as in the standard IBP. We also remark that auxiliary Gaussian variables to link a
latent model with the observations have been previously used in Gaussian processes for multi-class
classiﬁcation [6] and for ordinal regression [2]. However  up to our knowledge  this simple approach
has not been used to account for mixed continuous and discrete data  and the existent approaches
for the IBP with discrete observations propose non-conjugate likelihood models and approximate
inference algorithms [12  13].

3.1 Likelihood Functions

n to the corre-
Now  we deﬁne the set of transformations that map from the Gaussian variables yd
sponding observations xd
n. We consider that each dimension in the table X may contain any of the
discrete or continuous variables detailed above  provide a likelihood function for each kind of data
and  in turn  also a likelihood function for mixed data.

2For convenience  we capitalized here the notation for the weighting vectors Bd.

3

Real-valued Data. In this case  we assume that xd = Yd in the model in Figure 1 and consider
the standard approach when dealing with real-valued observations  which consist of assuming a
Gaussian likelihood function. In particular  as in the standard linear-Gaussian IBP [8]  we assume
that each observation xd

n is distributed as

p(xd

n|zn  Bd) = N (xd

y).
n|znBd  σ2

Positive Real-valued Data. In order to obtain positive real-valued observations  i.e.  xd
apply a transformation over yd

n ∈ (cid:60)+  we
n that maps from the real numbers to the positive real numbers  i.e. 

n + ud
n is a Gaussian noise variable with variance σ2

n) 
where ud
u  and f : (cid:60) → (cid:60)+ is a monotonic differen-
tiable function. By change of variables  we obtain the likelihood function for positive real-valued
observations as

n = f(yd
xd

exp

1
y + σ2
u)

2(σ2

−

(f

−1(xd

n) − znBd)2

−1(xd
n)

f

(cid:27)(cid:12)(cid:12)(cid:12)(cid:12) d

dxd
n

(cid:12)(cid:12)(cid:12)(cid:12)   (1)

p(xd

n|zn  Bd) =

1(cid:113)

2π(σ2

y + σ2
u)

(cid:26)

where f−1 : (cid:60)+ → (cid:60) is the inverse function of the transformation f(·)  i.e  f−1(f(v)) = v. Note
n  and therefore 
that in this case we resort to the Gaussian variable ud
Ψd = ud

n in order to obtain xd

n from yd

u.
d and Hd = σ2

Categorical Data. Now we account for categorical observations  i.e.  each observation xd
n can take
values in the unordered index set {1  . . .   Rd}. Hence  assuming a multinomial probit model  we
can write
(2)

n = arg max
xd

r∈{1 ... Rd} yd
nr 

r  σ2

nr|znbd

y) where bd

nr ∼ N (yd

r denotes the K-length weighting vector  in which each bd
being yd
n taking value r. Note that  under this
weights the inﬂuence of the k-th feature for the observation xd
kr for
likelihood model  since we have a Gaussian auxiliary variable yd
each possible value of the observation r ∈ {1  . . .   Rd}  we need to gather all the weighting factors
nr in the N × Rd matrix Yd.
kr in a K × Rd matrix Bd  and all the Gaussian auxiliary variables yd
bd
nr is a Gaussian noise
Under this observation model  we can write yd
n taking
variable with variance σ2
value r ∈ {1  . . .   Rd} as [6]

y  and therefore  we can obtain the probability of each element xd

nr and a weighting factor bd

nr  where ud

nr = znbd

r + ud

kr

p(xd

n = r|zn  Bd) = E

p(u)

Φ

u + zn(bd

j )
r − bd

 

(3)

(cid:16)

(cid:34) Rd(cid:89)

j=1
j(cid:54)=r

(cid:17)(cid:35)

r states for the column in Bd (r ∈ {1  . . .   Rd})  Φ(·) denotes the cumulative
p(u)[·] denotes expectation with respect to

where subscript r in bd
density function of the standard normal distribution and E
the distribution p(u) = N (0  σ2
y).
Ordinal Data. Consider ordinal data  in which each element xd
set {1  . . .   Rd}. Then  assuming an ordered probit model  we can write

n takes values in the ordered index



1
2

if yd
if θd

n ≤ θd
1
1 < yd

n ≤ θd

2

n =
xd

...

Rd

if θd

Rd−1 < yd
n

(4)

n is Gaussian distributed with mean znBd and variance σ2

r for r ∈
where again yd
{1  . . .   Rd − 1} are the thresholds that divide the real line into Rd regions. We assume the thresh-
θ)I(θd
olds θd
r >
r−1)  where θd
Rd = +∞. As opposed to the categorical case  now we have a unique
θd

r are sequentially generated from the truncated Gaussian distribution θd

y  and θd
r|0  σ2

0 = −∞ and θd

r ∝ N (θd

4

n is determined by the region in which yd

weighting vector Bd and a unique Gaussian variable yd
of xd
Under the ordered probit model [2]  the probability of each element xd
can be written as

n falls.

(cid:32)
n = r|zn  Bd) = Φ

p(xd

n for each observation xd

n. Hence  the value

(cid:33)

(cid:32)

n taking value r ∈ {1  . . .   Rd}

(cid:33)

r − znBd
θd

σy

− Φ

r−1 − znBd
θd

σy

.

(5)

In count data each observation xd

Let us remark that  if the d-dimension of the observation matrix contains ordinal data  the set of
auxiliary variables reduces to the Gaussian thresholds Ψd = {θd
Count Data.
n ∈
{0  . . .  ∞}. Then  we assume
(6)
where (cid:98)v(cid:99) returns the ﬂoor of v  that is the largest integer that does not exceed v  and f : (cid:60) → (cid:60)+
is a monotonic differentiable function that maps from the real numbers to the positive real numbers.
We can therefore write the likelihood function as

n takes non-negative integer values  i.e.  xd

Rd−1} and Hd = σ2
θ.

n = (cid:98)f(yd
xd

1  . . .   θd

n)(cid:99) 
(cid:33)

(cid:32)

(cid:33)

f−1(xd

n + 1) − znBd

σy

− Φ

f−1(xd

n) − znBd
σy

(7)

(cid:32)
n|zn  Bd) = Φ

p(xd

where f−1 : (cid:60)+ → (cid:60) is the inverse function of the transformation f(·).

Figure 1: Generalized IBP for mixed continuous and discrete observations.

Inference Algorithm

n from which we can obtain estimates for xd

4
In this section we describe our algorithm for inferring the latent variables given the observation
matrix. Under our model  detailed in Section 3  the probability distribution over the observation
matrix is fully characterized by the latent matrices Z and {Bd}D
d=1 (as well as the auxiliary variables
Ψd). Hence  if we assume the latent vector zn for the n-th datapoint and the weighting factors
Bd (and the auxiliary variables Ψd) to be known  we have a probability distribution over missing
n by sampling from this distribution 3 or
observations xd
by simply taking either its mean  mode or median value. However  this procedure requires the latent
matrix Z and the latent weighting factors Bd (and Ψd) to be known.
We use Markov Chain Monte Carlo (MCMC) methods  which have been broadly applied to infer
the IBP matrix (see  e.g.  in [8  23  20]). The proposed inference algorithm is summarized in Algo-
rithm 1. This algorithm exploits the information in the available data to learn the similarities among
the objects (captured in our model by the latent feature matrix Z)  and how these latent features
show up in the attributes that describe the objects (captured in our model by Bd). In Algorithm 1 
we ﬁrst need to update the latent matrix Z. Note that conditioned on {Yd}D
d=1  both the latent
d=1 are independent of the observation matrix X. Ad-
matrix Z and the weighting matrices {Bd}D
d=1 are Gaussian distributed  we can analytically marginalize
ditionally  since {Bd}D
out the weighting matrices {Bd}D
d=1|Z). Therefore  to infer the matrix Z  we
can apply the collapsed Gibbs sampler which presents better mixing properties than the uncollapsed

d=1 to obtain p({Yd}D

d=1 and {Yd}D

3Note that sampling from this distribution might be computationally expensive. In this case  we can easily
n by exploiting the structure of our model. In particular  we can simply sample the auxiliary
n given zn and Bd  and then obtain an estimate for xd
n by applying the corresponding

obtain samples of xd
Gaussian variables yd
transformation  detailed in Section 3.1.

5

Z2B↵YdBdd=1 ... D2yX dHdd=1

Update Z given {Yd}D
for d = 1  . . .   D do

Algorithm 1 Inference Algorithm.
Input: X
Initialize: initialize Z and {Yd}D
1: for each iteration do
2:
3:
4:
5:
6:
7:
8: end for
Output: Z  {Bd}D

d=1 and {Ψd}D

end for

d=1.

d=1

Sample Bd given Z and Yd according to (8).
Sample Yd given X  Z and Bd (as shown in the Supplementary Material).
Sample Ψd if needed (as shown in the Supplementary Material).

Gibbs sampler and  in consequence  is the standard method of choice in the context of the standard
linear-Gaussian IBP [8]. However  this algorithm suffers from a high computational cost (being
complexity per iteration cubic with the number of data points N)  which is prohibitive when dealing
with large databases. In order to solve this limitation  we resort to the accelerated Gibbs sampler [4]
instead. This algorithm presents linear complexity with the number of datapoints and is detailed in
the Supplementary Material.
Second  we need to sample the weighting factors in Bd  which is a K × Rd matrix in the case of
categorical attributes  and a K-length column vector otherwise. We denote each column vector in
Bd by bd

r. The posterior over the weighting vectors are given by

p(bd

r|P−1λd

r  P−1) 

r   Z) = N (bd

r|yd
r = Z(cid:62)yd

(8)

BIk and λd

nr|znbd  σ2

y) if the observation xd

r. Note that the covariance matrix P−1 depend neither
where P = Z(cid:62)Z + 1/σ2
on the dimension d nor on r  so we only need to invert the K × K matrix P once at each iteration.
We describe in the Supplementary Material how to efﬁciently compute P after changes in the Z
matrix by rank one updates  without the need of computing the matrix product Z(cid:62)Z.
in Yd from the distribution
Once we have updated Z and Bd  we sample each element
n  zn  bd) spec-
N (yd
iﬁed in the Supplementary Material  otherwise. Finally  we sample the auxiliary variables in Ψd
from their posterior distribution (detailed in the Supplementary Material) if necessary. This two lat-
ter steps involve  in the worst case  sampling from a doubly truncated univariate normal distribution
(see the Supplementary Material for further details)  for which we make use of the algorithm in [11].
5 Experimental evaluation
We now validate the proposed algorithm for table completion on ﬁve real databases  which are
summarized in Table 1. The datasets contain different numbers of instances and attributes  which
cover all the discrete and continuous variables described in Section 3. We compare  in terms of
predictive log-likelihood  the following methods for table completion:

n is missing  and from the posterior p(yd

nr|xd

sian.

treats all the attributes in X as Gaussian distributed.

• The proposed general table completion approach denoted by GIBP (detailed in Section 3).
• The standard linear-Gaussian IBP [8] denoted by SIBP  treating all the attributes as Gaus-
• The Bayesian probabilistic matrix factorization approach [15] denoted by BPMF  that also
For the GIBP  we consider for the real positive and the count data the following transformation 
that maps from the real numbers to the real positive numbers  f(x) = log(exp(wx) + 1)  where
w is a user hyper-parameter. Before running the SIBP and the BPMF methods we normalize each
column in matrix X to have zero-mean and unit-variance. Then  in order to provide estimates for
the missing data  we denormalize the inferred Gaussian variable. Additionally  since both the SIBP
and the BPMF assume continuous observations  when dealing with discrete data  we estimate each
missing value as the closest integer value to the (denormalized) Gaussian variable.

6

Dataset
Statlog German credit dataset
[5]
QSAR biodegradation dataset
[10]
Internet usage survey dataset
[1]
Wine quality Dataset [3]

N
1 000

1 055

1 006

6 497

NESARC dataset [13]

43 000

55 C

D
20 (10 C + 4 O
+ 6 N)
41 (2 R + 17 P
+ 4 C + 18 N)
32 (23 C + 8 O
+ 1 N)
12 (11 P + 1 N) Contains the results of physicochemical tests re-

Description
Collects information about the credit risks of
the applicants.
Contains molecular descriptors of biodegrad-
able and non-biodegradable chemicals.
Contains the responses of the participants to a
survey related to the usage of internet.

alized to different wines.
Contains the responses of the participants to a
survey related to personality disorders.

Table 1: Description of datasets. ‘R’ states for real-valued variables  ‘P’ for positive real-valued
variables  ‘C’ for categorical variables  ‘O’ for ordinal variables and ‘N’ for count variables

(a) Statlog.

(b) QSAR biodegradation.

(c) Internet usage survey.

(d) Wine quality.

(e) Nesarc database

Figure 2: Average test log-likelihood per missing datum. The ‘whiskers’ show a standard deviations
from the average test log-likelihood.

In Figure 2  we plot the average predictive log-likelihood per missing value as a function of the
percentage of missing data. Each value in Figure 2 has been obtained by averaging the results in
20 independent sets where the missing values have been randomly chosen. In Figures 2a and 2b 
we cut the plot in 50% because  in these two databases  the discrete attributes present a mode value
that is present for more than 80% of the instances. As a consequence  the SIBP and the BPMF
algorithms assign probability close to one to the mode  which results in an artiﬁcial increase in the
average test log-likelihood for larger percentages of missing data. For the BPMF model  we have
used different numbers of latent features (in particular  10  20 and 50)  although we only show the
best results for each database  speciﬁcally  K = 10 for the NESARC and the wine databases  and
K = 50 for the remainder. Both the GIBP and the SIBP have not inferred a number of (binary)
latent features above 25 in any case. Note that in Figure 2e  we only plot the test log-likelihood for
the GIBP and the SIBP because the BPMF provides much lower values. As expected  we observe
in Figure 2 that the average test log-likelihood decreases for the three models when the number of
missing values increases (ﬂat shape of the curves are due to the y-axis scale). In this ﬁgure  we also
observe that the proposed general IBP model outperforms the SIBP and the BPMF for four of the
the databases  being the SIBP slightly better for the Internet database. The BPMF model presents
the lowest test-log-likelihood in all the databases.
Now  we analyze the performance of the three models for each kind of discrete and continuous
variables. Figure 3 shows average predictive likelihood per missing value for each attribute in the
table  i.e.  for each dimension in X. In this ﬁgure we have grouped the dimensions according to the
kind of data that they contain  showing in the x-axis the number of considered categories for the case
of categorical and ordinal data. In this ﬁgure  we observe that the GIBP presents similar performance

7

1020304050−6−5−4−3−2−1% of missing dataLog−likelihood GIBPSIBPBPMF1020304050−10−8−6−4−20% of missing dataLog−likelihood GIBPSIBPBPMF102030405060708090−2.5−2−1.5−1% of missing dataLog−likelihood GIBPSIBPBPMF102030405060708090−10−50% of missing dataLog−likelihood GIBPSIBPBPMF102030405060708090−0.8−0.7−0.6−0.5% of missing dataLog−likelihood GIBPSIBPfor all the attributes in the ﬁve databases  while for the SIBP and the BPMF models  the test-log-
likelihood falls drastically for some of the attributes  being this effect worse in the case of the BPMF
(it explains the low log-likelihood in Figure 2). This effect is even more evident in Figures 2b
and 2d. We also observe  in Figures 2 and 3  that both IBP based approaches (the GIBP and the
SIBP) outperform the BPMF  with the proposed GIBP being the one that best performs across all
the databases. We can conclude that  unlike to the BPMF and the GIBP  the GIBP provides accurate
estimates for the missing data regardless of their discrete or continuous nature.

6 Conclusions
In this paper  we have proposed a table completion approach for heterogeneous databases  based on
an IBP with a generalized likelihood that allows for mixed discrete and continuous data. We have
then derived an inference algorithm that scales linearly with the number of observations. Finally  our
experimental results over ﬁve real databases have shown than the proposed approach outperforms 
in terms of robustness and accuracy  approaches that treat all the attributes as Gaussian variables.

(a) Statlog.

(b) QSAR biodegradation.

(c) Internet usage survey.

(d) Wine quality.

(e) Nesarc database

Figure 3: Average test log-likelihood per missing datum in each dimension of the data with 50% of
missing data. In the x-axis ‘R’ states for real-valued variables  ‘P’ for positive real-valued variables 
‘C’ for categorical variables  ‘O’ for ordinal variables and ‘N’ for count variables. The number that
accompanies the ‘C’ or ‘O’ corresponds to the number of categories.
Acknowledgments
Isabel Valera acknowledge the support of Plan Regional-Programas I+D of Comunidad de Madrid
(AGES-CM S2010/BMD-2422)  Ministerio de Ciencia e Innovaci´on of Spain (project DEIPRO
TEC2009-14504-C02-00 and program Consolider-Ingenio 2010 CSD2008-00010 COMONSENS).
Zoubin Ghahramani is supported by the EPSRC grant EP/I036575/1 and a Google Focused Research
Award.

8

C5C10C5C3C4C3C3C4C2C2O4O5O5O2NNNNNN−30−20−100AttributeLog−likelihood GIBPSIBPBPMFRRPPPPPPPPPPPPPPPPPC2C2C4C2NNNNNNNNNNNNNNNNNN−30−20−10010AttributeLog−likelihood GIBPSIBPBPMFC3C3C3C3C3C3C4C4C4C5C5C6C6C6C6C6C5C5C3C2C2C2C9O6O7O7O7O7O7O8O6N−8−6−4−20AttributeLog−likelihood GIBPSIBPBPMFPPPPPPPPPPPN−30−20−10010AttributeLog−likelihood GIBPSIBPBPMFCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC−30−20−100AttributeLog−likelihood GIBPSIBPBPMFCentre.

25th

anniversary

of

the web.

Available

on:

References
[1] Pew Research

http://www.pewinternet.org/datasets/january-2014-25th-anniversary-of-the-web-omnibus/.

[2] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Mach. Learn. Res.  6:1019–

1041  December 2005.

[3] P. Cortez  A. Cerdeira  F. Almeida  T. Matos  and J. Reis. Modeling wine preferences by
data mining from physicochemical properties. Decision Support Systems. Dataset available on:
http://archive.ics.uci.edu/ml/datasets.html  47(4):547–553  2009.

[4] F. Doshi-Velez and Z. Ghahramani. Accelerated sampling for the indian buffet process. In Proceedings of
the 26th Annual International Conference on Machine Learning  ICML ’09  pages 273–280  New York 
NY  USA  2009. ACM.

[5] J. Eggermont  J. N. Kok  and W. A. Kosters. Genetic programming for data classiﬁcation: Partitioning
the search space. In In Proceedings of the 2004 Symposium on applied computing (ACM SAC04). Dataset
available on: http://archive.ics.uci.edu/ml/datasets.html  pages 1001–1005. ACM  2004.

[6] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian process

priors. Neural Computation  18:2006  2005.

[7] P. Gopalan  F. J. R. Ruiz  R. Ranganath  and D. M. Blei. Bayesian Nonparametric Poisson Factor-
ization for Recommendation Systems. nternational Conference on Artiﬁcial Intelligence and Statistics
(AISTATS)  2014.

[8] T. L. Grifﬁths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal of

Machine Learning Research  12:1185–1224  2011.

[9] X.-B. Li. A Bayesian approach for estimating and replacing missing categorical data. J. Data and

Information Quality  1(1):3:1–3:11  June 2009.

[10] K. Mansouri  T. Ringsted  D. Ballabio  R. Todeschini  and V. Consonni. Quantitative structureactivity
relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Mod-
eling. Dataset available on: http://archive.ics.uci.edu/ml/datasets.html.

[11] C. P. Robert. Simulation of truncated normal variables. Statistics and computing  5(2):121–125  1995.
[12] F. J. R. Ruiz  I. Valera  C. Blanco  and F. Perez-Cruz. Bayesian nonparametric modeling of suicide

attempts. Advances in Neural Information Processing Systems  25:1862–1870  2012.

[13] F. J. R. Ruiz  I. Valera  C. Blanco  and F. Perez-Cruz. Bayesian nonparametric comorbidity anal-
Journal of Machine Learning Research (To appear). Available on

ysis of psychiatric disorders.
http://arxiv.org/pdf/1401.7620v1.pdf  2013.

[14] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in Neural Information

Processing Systems  2007.

[15] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov Chain Monte
In Proceedings of the 25th International Conference on Machine Learning  ICML ’08  pages

Carlo.
880–887  New York  NY  USA  2008. ACM.

[16] E. Salazar  M. Cain  E. Darling  S. Mitroff  and L. Carin. Inferring latent structure from mixed real and
categorical relational data. In Proceedings of the 29th International Conference on Machine Learning
(ICML-12)  ICML ’12  pages 1039–1046  New York  NY  USA  July 2012. Omnipress.

[17] ScienceDaily. Big data  for better or worse: 90% of world’s data generated over last two years.
[18] P. Shafto  C. Kemp  Mansinghka V.  and Tenenbaum J. B. A probabilistic model of cross-categorization.

Cognition  120(1):1 – 25  2011.

[19] S. Singh and T. Graepel. Automated probabilistic modelling for relational data. In Proceedings of the

ACM of Information and Knowledge Management  CIKM ’13  New York  NY  USA  2013. ACM.

[20] M. Titsias. The inﬁnite gamma-Poisson feature model. Advances in Neural Information Processing

Systems  19  2007.

[21] A. Todeschini  F. Caron  and M. Chavent. Probabilistic low-rank matrix completion with adaptive spectral
regularization algorithms. In C.J.C. Burges  L. Bottou  M. Welling  Z. Ghahramani  and K.Q. Weinberger 
editors  Advances in Neural Information Processing Systems 26  pages 845–853. Curran Associates  Inc. 
Dec. 2013.

[22] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientiﬁc articles.

In Pro-
ceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining 
KDD ’11  pages 448–456  New York  NY  USA  2011. ACM.

[23] S. Williamson  C. Wang  K. Heller  and D. Blei. The IBP compound Dirichlet process and its applica-
tion to focused topic modeling. Proceedings of the 27th Annual International Conference on Machine
Learning  2010.

9

,Yuhong Guo
Isabel Valera
Zoubin Ghahramani
Ting-Chun Wang
Ming-Yu Liu
Jun-Yan Zhu
Guilin Liu
Andrew Tao
Jan Kautz
Bryan Catanzaro