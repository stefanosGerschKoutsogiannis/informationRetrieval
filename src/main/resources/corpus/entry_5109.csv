2019,Sparse High-Dimensional Isotonic Regression,We consider the problem of estimating an unknown coordinate-wise monotone function given noisy measurements  known as the isotonic regression problem. Often  only a small subset of the features affects the output. This motivates the sparse isotonic regression setting  which we consider here. We provide an upper bound on the expected VC entropy of the space of sparse coordinate-wise monotone functions  and identify the regime of statistical consistency of our estimator. We also propose a linear program to recover the active coordinates  and provide theoretical recovery guarantees. We close with experiments on cancer classification  and show that our method significantly outperforms several standard methods.,Sparse High-Dimensional Isotonic Regression

David Gamarnik ∗

Sloan School of Management

Massachusetts Institute of Technology

Cambridge  MA 02139
gamarnik@mit.edu

Julia Gaudio†

Operations Research Center

Massachusetts Institute of Technology

Cambridge  MA 02139

jgaudio@mit.edu

Abstract

We consider the problem of estimating an unknown coordinate-wise monotone
function given noisy measurements  known as the isotonic regression problem.
Often  only a small subset of the features affects the output. This motivates the
sparse isotonic regression setting  which we consider here. We provide an upper
bound on the expected VC entropy of the space of sparse coordinate-wise mono-
tone functions  and identify the regime of statistical consistency of our estimator.
We also propose a linear program to recover the active coordinates  and provide
theoretical recovery guarantees. We close with experiments on cancer classiﬁcation 
and show that our method signiﬁcantly outperforms several standard methods.

Introduction

1
Given a partial order (cid:22) on Rd  we say that a function f : Rd → R is monotone if for all x1  x2 ∈ Rd
such that x1 (cid:22) x2  it holds that f (x1) ≤ f (x2). In this paper  we study the univariate isotonic
regression problem under the standard Euclidean partial order. Namely  we deﬁne the partial order (cid:22)
on Rd as follows: x1 (cid:22) x2 if x1 i ≤ x2 i for all i ∈ {1  . . .   d}. If f is monotone according to the
Euclidean partial order  we say f is coordinate-wise monotone.
This paper introduces the sparse isotonic regression problem  deﬁned as follows. Write x1 (cid:22)A x2 if
x1 i ≤ x2 i for all i ∈ A. We say that a function f on Rd is s-sparse coordinate-wise monotone if for
some set A ⊆ [d] with |A| = s  it holds that x1 (cid:22)A x2 =⇒ f (x1) ≤ f (x2). We call A the set of
active coordinates. The sparse isotonic regression problem is to estimate the s-sparse coordinate-wise
monotone function f from samples  knowing the sparsity level s but not the set A. Observe that if x
and y are such that xi = yi for all i ∈ A  then x (cid:22)A y and y (cid:22)A x  so that f (x) = f (y). In other
words  the value of f is determined by the active coordinates.
We consider two different noise models. In the Noisy Output Model  the input X is a random
variable supported on [0  1]d  and W is zero-mean noise that is independent from X. The model is
Y = f (X) + W . Let R be the range of f and let supp(W ) be the support of W . We assume that
both R and supp(W ) are bounded. Without loss of generality  let R + supp(W ) ⊆ [0  1]  where +
is the Cartesian sum. In the Noisy Input Model  Y = f (X + W )  and we exclusively consider the
classiﬁcation problem  namely f : Rd → {0  1}. In either noise model  we assume that n independent
samples (X1  Y1)  . . .   (Xn  Yn) are given.
The goal of our paper is to produce an estimator ˆfn and give statistical guarantees for it. To our
knowledge  the only work that provides statistical guarantees on isotonic regression estimators in
the Euclidean partial order setting with d ≥ 3 is the paper of Han et al ([13]). The authors give

∗http://web.mit.edu/gamarnik/www/home.html
†http://web.mit.edu/jgaudio/www/index.html

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(cid:20)

(cid:80)n

(cid:16) ˆfn(Xi) − f (Xi)

(cid:17)2(cid:21)

guarantees of the empirical L2 loss  deﬁned as R( ˆfn  f ) = E
  where
the expectation is over the samples X1  . . . Xn. In this paper  we instead expand on the approach in
Gamarnik ([11])  to the high-dimensional sparse setting. It is shown in [11] that the expected Vapnik-
Chervonenkis entropy of the class of coordinate-wise monotone functions grows subexponentially.
The main result of [11] is a guarantee on the tail of (cid:107) ˆfn − f(cid:107)2. When X ∈ [0  1]2 and Y ∈ [0  1]
almost surely  it is claimed that

i=1

1
n

P(cid:16)(cid:107) ˆfn − f(cid:107)2 > 

(cid:17) ≤ e(cid:100) 4

2(cid:101)√

n− 4n
256  

(cid:111)

(cid:110)

(cid:111)

(cid:110)

eω(s2)  ω (s log d)

seω(s2)  ω(s3 log d))

where ˆfn is a coordinate-wise monotone function  estimated based on empirical mean squared error.
However  the constants of the result are incorrect due to a calculation error  which we correct. This
result shows that the estimated function converges to the true function in L2  almost surely ([11]).
In this paper  we extend the work of [11] to the sparse high-dimensional setting  where the problem
dimension d and the sparsity s may diverge to inﬁnity as the sample size n goes to inﬁnity.
We propose two algorithms for the estimation of the unknown s-sparse coordinate-wise monotone
function f. The simultaneous algorithm determines the active coordinates and the estimated function
values in a single optimization formulation based on integer programming. The two-stage algorithm
ﬁrst determines the active coordinates via a linear program  and then estimates function values.
The sparsity level is treated as constant or moderately growing. We give statistical consistency
and support recovery guarantees for the Noisy Output Model  analyzing both the simultaneous and
  the estimator ˆfn from the
two-stage algorithms. We show that when n = max
simultaneous procedure is statistically consistent. In particular  when the sparsity s level is of constant
order  the dimension d is allowed to be much larger than the sample size. We note that  remarkably 
when the maximum is dominated by ω(s log d)  our sample performance nearly matches the one of
high-dimensional linear regression [2  10]. For the two-stage approach  we show that if a certain
signal strength condition holds and n = max
  the estimator is consistent.
We also give statistical consistency guarantees for the simultaneous and two-stage algorithms in the
Noisy Input Model  assuming that the components of W are independent. We show that in the regime
where a signal strength condition holds  s is of constant order  and n = ω(log d)  the estimators from
both algorithms are consistent.
The isotonic regression problem has a long history in the statistics literature; see for example the
books [19] and [20]. The emphasis of most research in the area of isotonic regression has been the
design of algorithms: for example  the Pool Adjacent Violators algorithm ([15])  active set methods
([1]  [5])  and the Isotonic Recursive Partitioning algorithm ([16]). In addition to the univariate setting
(f : Rd → R)  the multivariate setting (f : Rd → Rq  q ≥ 2) has also been considered; see e.g. [21]
and [22]. In the multivariate setting  whenever x1 (cid:22) x2 according to some deﬁned partial order (cid:22)  it
holds that f (x1) ˜(cid:22)f (x2)  where ˜(cid:22) is some other deﬁned partial order. There are many applications
for the coordinate-wise isotonic regression problem. For example  Dykstra and Robertson (1982)
showed that isotonic regression could be used to predict college GPA from standardized test scores
and high school GPA. Luss et al (2012) applied isotonic regression to the prediction of baseball
players’ salaries  from the number of runs batted in and the number of hits. Isotonic regression has
found rich applications in biology and medicine  particularly to build disease models ([16]  [23]).
The rest of the paper is structured as follows. Section 2 gives the simultaneous and two-stage
algorithms for sparse isotonic regression. Section 3 and Section A of the supplementary material
provide statistical consistency and recovery guarantees for the Noisy Output and Noisy Input models.
All proofs can be found in the supplementary material. In Section 4  we provide experimental
evidence for the applicability of our algorithms. We test our algorithm on a cancer classiﬁcation
task  using gene expression data. Our algorithm achieves a success rate of about 96% on this task 
signiﬁcantly outperforming the k-Nearest Neighbors classiﬁer and the Support Vector Machine.

2 Algorithms for sparse isotonic regression

In this section  we present our two algorithmic approaches for sparse isotonic regression:
the
simultaneous and two-stage algorithms. Recall that R is the range of f. In the Noisy Output Model 
R ⊆ [0  1]  and in the Noisy Input Model  R = {0  1}. We assume the following throughout.

2

Assumption 1. For each i ∈ A  the function f (x) is not constant with respect to xi  i.e.

(cid:90)

x∈X

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)f (x) −

f (z)dz

z∈X

(cid:12)(cid:12)(cid:12)(cid:12) dx > 0.

2.1 The Simultaneous Algorithm

The simultaneous algorithm solves the following problem.

n(cid:88)

i=1

min
A F

(Yi − Fi)2

(1)

s.t. |A| = s
Fi ≤ Fj
Fi ∈ R

if Xi (cid:22)A Xj
∀i

(2)
(3)
(4)
The estimated function ˆfn is determined by interpolating from the pairs (X1  F1)  . . .   (Xn  Fn) in
a straightforward way. In particular  ˆfn(x) = max{Fi : Xi (cid:22) x}. In other words  we identify all
points Xi such that Xi (cid:22) x and select the smallest consistent function value. We call this the “min”
interpolation rule because it selects the smallest interpolation values. The “max” interpolation rule is
ˆfn(x) = min{Fi : Xi (cid:23) x}.
Deﬁnition 1. For inputs X1  . . .   Xn  let q(i  j  k) = 1 if Xi k > Xj k  and q(i  j  k) = 0 otherwise.
Problem (1)-(4) can be encoded as a single mixed-integer convex minimization. We refer to the
resulting Algorithm 1 as Integer Programming Isotonic Regression (IPIR) and provide its formulation
below. Binary variables vk indicate the estimated active coordinates; vk = 1 means that the
optimization program has determined that coordinate k is active. The variables Fi represent the
estimated function values at data points Xi.
Algorithm 1 Integer Programming Isotonic Regression (IPIR)
Input: Values (X1  Y1)  . . .   (Xn  Yn); sparsity level s
Output: An estimated function ˆfn
1: Solve the following optimization problem.

n(cid:88)

i=1

(Yi − Fi)2

vk = s

q(i  j  k)vk ≥ Fi − Fj

s.t.

min
v F

d(cid:88)
d(cid:88)

k=1

k=1

vk ∈ {0  1}
Fi ∈ R

(5)

(6)

(7)

(8)
(9)

∀i  j ∈ {1  . . .   n}
∀k ∈ {1  . . .   d}
∀i ∈ {1  . . .   n}

2: Return the function ˆfn(x) = max{Fi : Xi (cid:22) x}.
We claim that Problem (5)-(9) is equivalent to Problem (1)-(4). Indeed  the monotonicity requirement
is Xi (cid:22)A Xj =⇒ f (Xi) ≤ f (Xj). The contrapositive of this statement is f (Xi) > f (Xj) =⇒
Xi (cid:54)(cid:22)A Xj; alternatively  f (Xi) > f (Xj) =⇒ ∃k ∈ A s.t. Xik > Xjk. The contrapositive is
expressed by Constraints (7).
Recall that in the Noisy Input Model  the function f is binary-valued  i.e. R = {0  1}. Let
S + = {i : Yi = 1} and S− = {i : Yi = 0}. When {Fi}n
i=1 are binary-valued  it holds that
i∈S− Fi. Therefore  if we replace the objective function

i∈S+ (1 − Fi) +(cid:80)

(cid:80)n
i=1 (Yi − Fi)2 =(cid:80)
(5) by(cid:80)

i∈S+ (1 − Fi) +(cid:80)

i∈S− Fi  we obtain an equivalent linear integer program.

Algorithm 1 when applied to the Noisy Output Model is a mixed-integer convex optimization program.
When applied to the Noisy Input Model  it s a mixed integer linear optimization program. While both
are formally NP-hard in general  moderately-sized instances are solvable in practice.

3

2.2 The Two-Stage Algorithm

Algorithm 1 is slow  both in theory and in practice. Motivated by this  we propose an alternative
two-stage algorithm. The two-stage algorithm estimates the active coordinates through a linear
program  using these to then estimate the function values. The process of estimating the active
coordinates is referred to as support recovery. The active coordinates may be estimated all at once
(Algorithm 2) or sequentially (Algorithm 3). Algorithm 2 is referred to as Linear Programming
Support Recovery (LPSR) and Algorithm 3 is referred to as Sequential Linear Programming Support
Recovery (S-LPSR). The two-stage algorithm for estimating ˆfn ﬁrst estimates the set of active
coordinates using the LPSR or S-LPSR algorithm  and then estimates the function values. The results
algorithm is referred to as Two Stage Isotonic Regression (TSIR) (Algorithm 4).

Algorithm 2 Linear Programming Support Recovery (LPSR)
Input: Values (X1  Y1)  . . .   (Xn  Yn); sparsity level s
Output: The estimated support  ˆA
1: Solve the following optimization problem.

n(cid:88)

n(cid:88)

d(cid:88)

i=1

j=1

k=1

cij
k

min
v c

d(cid:88)
d(cid:88)

k=1

s.t.

vk = s

(cid:16)

vk + cij
k

(cid:17) ≥ 1

(10)

(11)

(12)

d(cid:88)

(13)
(14)
2: Determine the s largest values vi  breaking ties arbitrarily. Let ˆA be the set of the corresponding

if Yi > Yj and

q(i  j  k) ≥ 1
∀k ∈ {1  . . .   d}
∀i ∈ {1  . . .   n}  j ∈ {1  . . .   n}  k ∈ {1  . . .   p}

k=1

k=1

q(i  j  k)
0 ≤ vk ≤ 1
k ≥ 0
cij

s indices.

(12)  (cid:80)d

In Problem (10)-(14)  the vk variables are meant to indicate the active coordinates  while the cij
k
variables act as correction in the monotonicity constraints. For example  if for one of the constraints
k = 0.3 for some (i  j  k) such that

k=1 q(i  j  k)vk = 0.7  then we will need to set cij

q(i  j  k) = 1. The vk’s should therefore be chosen in a way to minimize the correction.
Algorithm 3 determines the active coordinates one at a time  setting s = 1 in Problem (10)-(14).
Once a coordinate i is included in the set of active coordinates  variable vi is set to zero in future
iterations.
Algorithm 3 Sequential Linear Programming Support Recovery (S-LPSR)
Input: Values (X1  Y1)  . . .   (Xn  Yn); sparsity level s
Output: The estimated support  ˆA
1: B ← ∅
2: while |B| < s do
3:

Solve the optimization problem in Algorithm 2 with s = 1:

n(cid:88)

n(cid:88)

d(cid:88)

i=1

j=1

k=1

vk = 1

cij
k

s.t.

min

d(cid:88)
d(cid:88)

k=1

k=1
vi = 0

q(i  j  k)

(15)

(16)
∀i ∈ B (17)

q(i  j  k) ≥ 1

(18)

d(cid:88)

k=1

(cid:16)

vk + cij
k

(cid:17) ≥ 1

if Yi > Yj and

4

(19)
(20)

0 ≤ vk ≤ 1
k ≥ 0
cij

∀k ∈ {1  . . .   d}
∀i ∈ {1  . . .   n}  j ∈ {1  . . .   n}  k ∈ {1  . . .   d}
Identify i(cid:63) such that vi(cid:63) = maxi{vi}  breaking ties arbitrarily. Set B ← B ∪ {imax}.

4:
5: end while
6: Return ˆA = B.
Algorithm 3’ is deﬁned to be the batch version of Algorithm 3. Namely  there are n samples in total 
divided into n
s batches. The ﬁrst iteration of the sequential procedure is performed on the ﬁrst batch 
the second iteration on the second batch  and so on.
We are now ready to state the two-stage algorithm for estimating the function ˆfn.
Algorithm 4 Two Stage Isotonic Regression (TSIR)
Input: Values (X1  Y1)  . . .   (Xn  Yn); sparsity level s
Output: The estimated function  ˆfn
1: Estimate ˆA by using Algorithm 2  3  or 3’. Let vk = 1 if k ∈ ˆA and vk = 0 otherwise.
2: Solve the following optimization problem.

n(cid:88)

i=1

min

d(cid:88)

(Yi − Fi)2

(21)

(22)

(23)

s.t.

q(i  j  k)vk ≥ Fi − Fj

In the Noisy Input Model  replace the objective with(cid:80)

Fi ∈ R

k=1

∀i  j ∈ {1  . . .   n}
∀i ∈ {1  . . .   n}

i∈S+ (1 − Fi) +(cid:80)

i∈S− Fi.

3: Return the function ˆfn(x) = max{Fi : Xi (cid:22) x}.
Both algorithms for support recovery are linear programs  which can be solved in polynomial time.
The second step of Algorithm 4 when applied to the Noisy Output Model is a linearly-constrained
quadratic minimization program that can be solved in polynomial time. The following lemma shows
that Step 2 of Algorithm 4 when applied to the Noisy Input Model can be reduced to a linear program.
Lemma 1. Under the Noisy Input Model  replacing the constraints Fi ∈ {0  1} with Fi ∈ [0  1] in
Problems (5)-(9) and (21)-(23) does not change the optimal value. Furthermore  there always exists
an integer optimal solution that can be constructed from an optimal solution in polynomial time.

3 Results on the Noisy Output Model

Recall the Noisy Output Model: Y = f (X) + W   where f is an s-sparse coordinate-wise monotone
function with active coordinates A. We assume throughout this section that X is a uniform random
variable on [0  1]d  W is a zero-mean random variable independent from X  and the domain of f
is [0  1]d. We additionally assume that Y ∈ [0  1] almost surely. Up to shifting and scaling  this is
equivalent to assuming that f has a bounded range and W has a bounded support.

3.1 Statistical consistency

In this section  we extend the results of [11]  in order to demonstrate the statistical consistency of the
estimator produced by Algorithm 1. The consistency will be stated in terms of the L2 norm error.
Deﬁnition 2 (L2 Norm Error). For an estimator ˆfn  deﬁne

(cid:44)

(cid:107) ˆfn − f(cid:107)2
We call (cid:107) ˆfn − f(cid:107)2 the L2 norm error.
Deﬁnition 3 (Consistent Estimator). Let ˆfn be a estimator for the function f. We say that ˆfn is
consistent if for all  > 0  it holds that

x∈[0 1]d

dx.

2

(cid:16) ˆfn(x) − f (x)
(cid:17)2

(cid:90)

P(cid:16)(cid:107) ˆfn − f(cid:107)2 ≥ 

(cid:17) → 0.

lim
n→∞

5

P(cid:16)(cid:107) ˆfn − f(cid:107)2 ≥ 
(cid:17) ≤ 8
(cid:110)

(cid:18)d

(cid:19)

s

(cid:20)(cid:18) 128 log(2)
(cid:111)

2

exp

(cid:19)

+ 2

64

2 2s

n

s−1

s − 4n
512

(cid:21)

.

Theorem 1. The L2 error of the estimator ˆfn obtained from Algorithm 1 is upper bounded as

Corollary 1. When n = max
  the estimator ˆfn from Algorithm 1 is consistent.
Namely  (cid:107) ˆfn − f(cid:107)2 → 0 in probability as n → ∞. In particular  if the sparsity level s is constant 
the sample complexity is only logarithmic in the dimension.

eω(s2)  ω(s log(d))

3.2 Support recovery

In this subsection  we give support recovery guarantees for Algorithm 3. The guarantees will be in
terms of the values pk  deﬁned below.
Deﬁnition 4. Let Y1 = f (X1) + W1 and Y2 = f (X2) + W2 be two independent samples from the
model. For k ∈ A  let

pk (cid:44) P (Y1 > Y2 | q(1  2  k) = 1) − P (Y1 < Y2 | q(1  2  k) = 1) .
Assume without loss of generality that A = {1  2  . . .   s} and p1 ≤ p2 ≤ ··· ≤ ps.
Lemma 2. It holds that pk > 0 for all k. In other words  when X1 is greater than X2 in at least one
active coordinate  the output corresponding to X1 is likely to be larger than the one corresponding to
X2.
Theorem 2. Let B be the set of indices corresponding to running Algorithm 3’ using n samples.
Then it holds that B = A with probability at least
1 − ds exp

(cid:18)

(cid:19)

.

− p2
1n
64s3

Corollary 2. Assume that p1 = Θ(1). Let n be the number of samples used by Algorithm 3’. If
n = ω(s3 log(d))  then Algorithm 3’ recovers the true support w.h.p. as n → ∞.
For x ∈ Rd  let xA denote x restricted to coordinates deﬁned by the set A. Suppose that s is
constant  and the sequence of functions {fd} extends a function on s variables  i.e. fd is deﬁned as
fd(x) = g(xA) for some g : [0  1]s → R. In that case  p1 = Θ(1).
We can now give a guarantee of the success of Algorithm 4  using Algorithm 3’ for support recovery.
Corollary 3. Assume that p1 = Θ(1). Consider running Algorithm 4 using n samples for se-
quential recovery. Let m = n
s . Consider using an additional m samples for function value
estimation  so that the total sample size is n + m. Let ˆfn+m be the estimated function.
If
n = max
Corollary 3 shows that if s is constant and the sequence of functions {fd} extends a function of s
variables  then Algorithm 4 produces a consistent estimator with n = ω(log(d)) samples. In the
supplementary material  we state the statistical consistency results for the Noisy Input Model.

(cid:110)
ω(s3 log(d))  seω(s2)(cid:111)

  then ˆfn+m is a consistent estimator.

4 Experimental results

All algorithms were implemented in Java version 8  using Gurobi version 6.0.0.

4.1 Support recovery
We test the support recovery algorithms on random synthetic instances. Let A = {1  . . .   s} without
loss of generality. First  randomly sample r “anchor points” in [0  1]d  calling them Z1  . . .   Zr. The
parameter r governs the complexity of the function produced. In our experiment  we set r = 10.
Next  randomly sample X1  . . .   Xn in [0  1]d. For i ∈ {1  . . .   n}  assign Yi = 1 + Wi if Zj (cid:22)A Xi
for some j ∈ {1  . . .   r}  and assign Yi = Wi otherwise. The linear programming based algorithms
for support recovery  LPSR and S-LPSR  are compared to the simultaneous approach  IPIR  which
estimates the active coordinates while also estimating the function values. Note that even though the

6

proof of support recovery using S-LPSR requires fresh data at each iteration  our experiments do not
use fresh data. We keep s = 3 ﬁxed and vary d and n. The error is Gaussian with mean 0 and variance
0.1  independent across coordinates. We report the percentages of successful recovery (see Table 1).
The IPIR algorithm performs the best on nearly all settings of (n  d). This suggests that the objective
of the IPIR algorithm- to minimize the number of misclassiﬁcations on the data- gives the algorithm
an advantage in selecting the true active coordinates. The LPSR algorithm outperforms the S-LPSR
algorithm when d = 5  but the situation reverses for d ∈ {10  20}. For n = 200 samples and d = 5 
the LPSR algorithm correctly recovers the coordinates all but one time  while S-LPSR succeeds
86% of the time. For d = 10  LPSR and S-LPSR succeed 46 and 75% of the time  respectively; for
d = 20  LPSR and S-LPSR succeed 30 and 63% of the time  respectively. It appears that determining
the coordinates one at a time provides implicit regularization for larger d.
We additionally compare the accuracy in function estimation (Table 2). We found these results to be
extremely encouraging. For n = 200 samples  the IPIR and S-LPSR algorithms had accuracy rates in
the range of 87 − 90%.

Table 1: Performance of support recovery algorithms on synthetic instances. Each line of the table
corresponds to 100 trials.

IPIR
d =
10
55
85
94
99

20
57
90
91
92

LPSR
d =
10
29
33
50
46

5
76
92
99
99

20
1
13
16
30

S-LPSR

d =
10
33
56
71
75

20
26
49
65
63

5
62
76
86
86

n

50
100
150
200

5
62
92
98
95

Table 2: Accuracy of isotonic regression on synthetic instances. Each line of the table corresponds to
100 trials.

IPIR
d =
10
77.8
85.8
87.8
89.8

LPSR
d =
10
74.2
77.6
81.3
83.6

S-LPSR

d =
10
76.1
83.9
86.6
88.9

20
74.3
81.7
85.9
87.5

20
65.9
75.0
77.9
83.4

5

77.1
84.2
87.1
89.0

20
77.6
84.6
86.8
88.3

5

77.4
84.1
87.8
89.1

n

50
100
150
200

5

78.2
85.1
87.9
89.2

4.2 Cancer classiﬁcation using gene expression data

The presence or absence of a disease is believed to follow a monotone relationship with respect to
gene expression. Similarly  classifying patients as having one of two diseases amounts to applying
the monotonicity principle to a subpopulation of individuals having one of the two diseases. In
order to assess the applicability of our sparse monotone regression approach  we apply it to cancer
classiﬁcation using gene expression data. The motivation for using a sparse model for disease
classiﬁcation is that certain genes should be more responsible for disease than others. Sparsity can
be viewed as a kind of regularization; to prevent overﬁtting  we allow the regression to explain the
results using only a small number of genes.
The data is drawn from the COSMIC database [9]  which is widely used in quantitative research
in cancer biology. Each patient in the database is identiﬁed as having a certain type of cancer. For
each patient  gene expressions of tumor cells are reported as a z-score. Namely  if µG and σG are
the mean and standard deviation of the gene expression of gene G and x is the gene expression of a
certain patient  then his or her z-score would be equal to x−µG
. We ﬁlter the patients by cancer type 
selecting those with skin and lung cancer  two common cancer types. There are 236698 people with
lung or skin cancer in the database  though the database only includes gene expression data for 1492
of these individuals. Of these  1019 have lung cancer and 473 have skin cancer. A classiﬁer always

σG

7

selecting “lung” would have an expected correct classiﬁcation rate of 1019/1492 ≈ 68%. Therefore
this rate should be regarded as the baseline classiﬁcation rate.
Our goal is to use gene expression data to classify the patients as having either skin or lung cancer.
We associate skin cancer as a “0” label and lung cancer as a “1” label. We only include the 20 most
associated genes for each of the two types  according to the COSMIC website. This leaves 31 genes 
since some genes appear on both lists. We additionally include the negations of the gene expression
values as coordinates  since a lower gene expression of certain genes may promote lung cancer over
skin cancer. The number of coordinates is therefore equal to 62. The number of active genes is ranged
between 1 and 5.
We perform both simultaneous and two-stage isotonic regression  comparing the IPIR and TSIR
algorithms  using S-LPSR to recover the coordinates in the two-stage approach. Since for every
gene  its negation also corresponds to a coordinate  we added additional constraints. In IPIR  we use
variables vk ∈ {0  1} to indicate whether coordinate k is in the estimated set of active coordinates.
In LPSR and S-LPSR  we use variables vk ∈ [0  1] instead. In order to incorporate the constraints
regarding negation of coordinates in IPIR  we included the constraint vi + vj ≤ 1 for pairs (i  j) such
that coordinate j is the negation of coordinate i. In S-LPSR  once a coordinate vi was selected  its
negation was set to zero in future iterations. The LPSR algorithm  however  could not be modiﬁed to
take this additional structure into account without using integer variables. Adding the constraints
vi + vj ≤ 1 when coordinate j is the negation of coordinate i proved to be insufﬁcient. Therefore 
we do not include the LPSR algorithm in our experiments on the COSMIC database.
We compare our isotonic regression algorithms to two classical algorithms: k-Nearest Neighbors ([8])
and the Support Vector Machine ([4]). Given a test sample x and an odd number k  the k-Nearest
Neighbors algorithm ﬁnds the k closest training samples to x. The label of x is chosen according
to the majority of the labels of the k closest training samples. The SVM algorithm used is the
soft-margin classiﬁer with penalty C and polynomial kernel given by K(x  y) = (1 + x · y)m. We
have additionally implemented a version of kNN with dimensionality reduction  in an effort to reduce
the curse-of-dimensionality suffered by kNN. Data points are compressed by Principal Component
Analysis ([18]) prior to nearest-neighbor classiﬁcation. However  this version of kNN performed
worse than the basic version  and we omit the results.
In Table 3  each row is based on 10 trials  with 1000 test data points chosen uniformly and separately
from the training points. The two-stage method was generally faster than the simultaneous method.
With 200 training points and s = 3  the simultaneous method took 260 seconds on average per
trial  while the two-stage method took only 42 seconds per trial. The simultaneous method became
prohibitively slow for higher values of n. The averages for k-Nearest Neighbors and Support
Vector Machine are taken as the best over parameter choices in hindsight. For k-Nearest Neighbors 
k ∈ {1  3  5  7  9  11  15}  and for SVM  C ∈ {10  100  500  1000} and m ∈ {1  2  3  4}. The fact
that the sparse isotonic regression method outperforms the k-NN classiﬁer and the polynomial kernel
SVM by such a large margin can be explained by a difference in structural assumptions; the results
suggest that monotonicity  rather than proximity or a polynomial functional relationship  is the correct
property to leverage.

Table 3: Comparison of classiﬁer success rates on COSMIC data. Top row data is according to the
“min” interpolation rule and bottom row data is according to the “max” interpolation rule.

n

100

200

300

400

1

83.1
83.9
85.4
85.8

-
-
-
-

2

84.6
91.8
88.1
92.6

-
-
-
-

IPIR
s =
3

76.8
91.0
84.3
96.4

-
-
-
-

4

66.2
85.7
73.9
88.9

-
-
-
-

5

53.8
75.7
62.7
83.9

-
-
-
-

TSIR + S-LPSR

k-NN SVM

2

84.6
90.4
89.3
94.5
91.7
94.2
91.8
94.0

s =
3

77.8
88.9
86.7
95.9
89.0
95.6
89.7
95.7

4

73.0
87.4
81.2
95.3
84.4
95.9
87.3
96.4

5

65.4
83.3
76.9
93.0
80.2
94.8
81.7
95.7

69.8

63.8

76.6

72.6

76.6

74.2

78.6

77.4

1

82.4
82.9
85.4
85.8
84.7
85.1
85.6
85.8

8

The results suggest that the correct sparsity level is s = 3. With n = 400 samples  the classiﬁcation
accuracy rate is 95.7%. When the sparsity level is too low  the monotonicity model is too simple to
accurately describe the monotonicity pattern. On the other hand  when the sparsity level is too high 
fewer points are comparable  which leads to fewer monotonicity constraints. For n ∈ {100  200} and
d ∈ {1  2  3  4  5}  TSIR + S-LPSR does at least as well as IPIR on 15 out of 20 of (n  d) pairs  and
outperforms on 12 of these. This result is surprising  because synthetic experiments show that IPIR
outperforms S-LPSR on support recovery.
We further investigate the TSIR + S-LPSR algorithm. Figure 1 shows how the two-stage procedure
labels the training points. The high success rate of the sparse isotonic regression method suggests
that this nonlinear picture is quite close to reality. The observed clustering of points may be a feature
of the distribution of patients  or could be due to a saturation in measurement. Figure 2 studies the
robustness of TSIR + S-LPSR. Additional synthetic zero-mean Gaussian noise is added to the inputs 
with varying standard deviation. The “max” classiﬁcation rule is used. 200 training points and 1000
test points were used. Ten trials were run  with one standard deviation error bars indicated in gray.
The results indicate that TSIR + S-LPSR is robust to moderate levels of noise.
We note that because the gene expression is measured from tumor cells  much of the variation between
the lung and skin cancer patients can be attributed to intrinsic differences between lung and skin
cells. Still  this classiﬁcation task is highly non-linear and challenging  as evidenced by the poor
performance of other classiﬁers. We view these experiments as a proof-of-concept  showing that our
algorithm can perform well on real data. An example of a more medically relevant application of our
algorithm would be identifying patients as having cancer or not  using blood protein levels [3].

(a) s = 2.

(b) s = 3.

Figure 1: Illustration of the TSIR + S-LPSR algorithm. Blue and red markers correspond to lung and
skin cancer  respectively.

)

%

(

y
c
a
r
u
c
c
A

100

90

80

70

60

0

0.1

0.2

0.3

0.4

0.5

Standard deviation of additional synthetic noise

Figure 2: Robustness to error of TSIR + S-LPSR.

5 Conclusion

In this paper  we have considered the sparse isotonic regression problem under two noise models:
Noisy Output and Noisy Input. We have formulated optimization problems to recover the active
coordinates  and then estimate the underlying monotone function. We provide explicit guarantees on
the performance of these estimators. We leave the analysis of Linear Programming Support Recovery
(Algorithm 2) as an open problem. Finally  we demonstrate the applicability of our approach to a
cancer classiﬁcation task  showing that our methods outperform widely-used classiﬁers. While the
task of classifying patients with two cancer types is relatively simple  the accuracy rates illustrate the
modeling power of the sparse monotone regression approach.

9

-6-5-4-3-2-101-6-5-4-3-2-101-41-30.50-20-1-0.50-1-11-1.5-2-2-2.5References
[1] Michael J. Best and Nilotpal Chakravarti. Active set algorithms for isotonic regression; a

unifying framework. Mathematical Programming  47:425–439  1990.

[2] Peter Bühlmann and Sara Van De Geer. Statistics for high-dimensional data: methods  theory

and applications. Springer Science & Business Media  2011.

[3] Joshua D Cohen  Lu Li  Yuxuan Wang  Christopher Thoburn  Bahman Afsari  Ludmila Danilova 
Christopher Douville  Ammar A Javed  Fay Wong  Austin Mattox  et al. Detection and localiza-
tion of surgically resectable cancers with a multi-analyte blood test. Science  359(6378):926–930 
2018.

[4] Corinna Cortes and Vladimir Vapnik. Support-Vector networks. Machine Learning  20:273–297 

1995.

[5] Jan de Leeuw  Kurt Hornik  and Patrick Mair.

Isotone optimization in R: Pool-Adjacent-
Violoators Algorithm (PAVA) and active set methods. Journal of Statistical Software  32(5):1–24 
2009.

[6] Luc Devroye  László Györﬁ  and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition.

Springer  1996.

[7] Richard L. Dykstra and Tim Robertson. An algorithm for isotonic regression for two or more

independent variables. The Annals of Statistics  10(3):708–716  1982.

[8] E. Fix and J.L. Hodges. Discriminatory analysis. nonparametric discrimination; consistency
properties. Technical Report Report Number 4  Project Number 21-49-004  USAF School of
Aviation Medicine  Randolph Field  Texas.  1951.

[9] Simon A. Forbes  Nidhi Bindal  Sally Bamford  Charlotte Cole  Chai Yin Kok  David Beare 
Mingming Jia  Rebecca Shepherd  Kenric Leung  Andrew Menzies  Jon W. Teague  Peter J.
Campbell  Michael R. Stratton  and P. Andrew Futreal. COSMIC: mining complete cancer
genomes in the Catalogue of Somatic Mutations in Cancer. Nucleic Acids Research  39(1):D945–
D950  2011.

[10] Simon Foucart and Holger Rauhut. A mathematical introduction to compressive sensing.

Birkhäuser Springer  2013.

[11] D. Gamarnik. Efﬁcient learning of monotone concepts via quadratic optimization. In COLT 

1999.

[12] Dimitris Bertsimas David Gamarnik and John N. Tsitsiklis. Estimation of time-varying pa-
rameters in statistical models: an optimization approach. Machine Learning  35(3):225–245 
1999.

[13] Qiyang Han  Tengyao Wang  Sabyasachi Chatterjee  and Richard J. Samworth.

regression in general dimensions. arXiv 1708.0946v1  2017.

Isotonic

[14] David Haussler. Overview of the Probably Approximately Correct (PAC) learning frame-
work. https://hausslergenomics.ucsc.edu/wp-content/uploads/2017/08/smo_0.
pdf  1995.

[15] J. B. Kruskal. Nonmetric multidimensional scaling: A numerical method. Psychometrika 

29(2):115–129  1964.

[16] Ronny Luss  Saharon Rosset  and Moni Shahar. Efﬁcient regularised isotonic regression with
application to gene-gene interaction search. The Annals of Applied Statistics  6(1):253–283 
2012.

[17] Guy Moshkovitz and Asaf Shapira. Ramsey theory  integer partitions and a new proof of the

Erdos-Szekeres theorem. Advances in Mathematics  262:1107–1129  2014.

[18] Karl Pearson. On lines and planes of closest ﬁt to systems of points in space. The London 
Edinburgh  and Dublin Philosophical Magazine and Journal of Science  2(11):559–572  1901.

10

[19] R. E. Barlow  D. J. Bartholomew  J. M. Bremner  and H. D. Brunk. Statistical inference under

order restrictions. John Wiley & Sons  1973.

[20] T. Robertson  F. T. Wright  and R. L. Dykstra. Order restricted statistical inference. John Wiley

& Sons  1988.

[21] S. Sasabuchi  M. Inutsuka  and D. D. S. Kulatunga. A multivariate version of isotonic regression.

Biometrika  70(2):465–472  1983.

[22] Syoichi Sasabuchi  Makoto Inutsuka  and D. D. Sarath Kulatunga. An algorithm for computing

multivariate isotonic regression. Hiroshima Mathematical Journal  22(551-560)  1992.

[23] Michael J. Schell and Bahadur Singh. The reduced monotonic regression method. Journal of

the American Statistical Association  92(437):128–135  1997.

[24] V. Vapnik. Nature of Learning Theory. Springer-Verlag  1996.

11

,David Gamarnik
Julia Gaudio