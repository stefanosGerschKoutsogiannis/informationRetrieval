2014,Optimal Regret Minimization in Posted-Price Auctions with Strategic Buyers,We study revenue optimization learning algorithms for posted-price auctions with strategic buyers. We analyze a very broad family of monotone regret minimization algorithms for this problem  which includes the previous best known algorithm  and show that no algorithm in that family admits a strategic regret more favorable than $\Omega(\sqrt{T})$. We then introduce a new algorithm that achieves a strategic regret differing from the lower bound only by a factor in $O(\log T)$  an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural analysis and simpler proofs  and the ideas behind its design are general. We also report the results of empirical evaluations comparing our algorithm with the previous best algorithm and show a consistent exponential improvement in several different scenarios.,Optimal Regret Minimization in Posted-Price

Auctions with Strategic Buyers

Mehryar Mohri

Courant Institute and Google Research

251 Mercer Street

New York  NY 10012
mohri@cims.nyu.edu

Andres Mu˜noz Medina

Courant Institute
251 Mercer Street

New York  NY 10012
munoz@cims.nyu.edu

Abstract

We study revenue optimization learning algorithms for posted-price auctions with
strategic buyers. We analyze a very broad family of monotone regret minimization
algorithms for this problem  which includes the previously best known algorithm 
√
and show that no algorithm in that family admits a strategic regret more favorable
than Ω(
T ). We then introduce a new algorithm that achieves a strategic regret
differing from the lower bound only by a factor in O(log T )  an exponential im-
provement upon the previous best algorithm. Our new algorithm admits a natural
analysis and simpler proofs  and the ideas behind its design are general. We also
report the results of empirical evaluations comparing our algorithm with the pre-
vious state of the art and show a consistent exponential improvement in several
different scenarios.

1

Introduction

Auctions have long been an active area of research in Economics and Game Theory [Vickrey  2012 
Milgrom and Weber  1982  Ostrovsky and Schwarz  2011]. In the past decade  however  the advent
of online advertisement has prompted a more algorithmic study of auctions  including the design of
learning algorithms for revenue maximization for generalized second-price auctions or second-price
auctions with reserve [Cesa-Bianchi et al.  2013  Mohri and Mu˜noz Medina  2014  He et al.  2013].
These studies have been largely motivated by the widespread use of AdExchanges and the vast
amount of historical data thereby collected – AdExchanges are advertisement selling platforms us-
ing second-price auctions with reserve price to allocate advertisement space. Thus far  the learning
algorithms proposed for revenue maximization in these auctions critically rely on the assumption
that the bids  that is  the outcomes of auctions  are drawn i.i.d. according to some unknown distri-
bution. However  this assumption may not hold in practice. In particular  with the knowledge that a
revenue optimization algorithm is being used  an advertiser could seek to mislead the publisher by
under-bidding. In fact  consistent empirical evidence of strategic behavior by advertisers has been
found by Edelman and Ostrovsky [2007]. This motivates the analysis presented in this paper of the
interactions between sellers and strategic buyers  that is  buyers that may act non-truthfully with the
goal of maximizing their surplus.
The scenario we consider is that of posted-price auctions  which  albeit simpler than other mech-
anisms  in fact matches a common situation in AdExchanges where many auctions admit a single
bidder. In this setting  second-price auctions with reserve are equivalent to posted-price auctions: a
seller sets a reserve price for a good and the buyer decides whether or not to accept it (that is to bid
higher than the reserve price). In order to capture the buyer’s strategic behavior  we will analyze an
online scenario: at each time t  a price pt is offered by the seller and the buyer must decide to either
accept it or leave it. This scenario can be modeled as a two-player repeated non-zero sum game with

1

2

incomplete information  where the seller’s objective is to maximize his revenue  while the advertiser
seeks to maximize her surplus as described in more detail in Section 2.
The literature on non-zero sum games is very rich [Nachbar  1997  2001  Morris  1994]  but much of
the work in that area has focused on characterizing different types of equilibria  which is not directly
relevant to the algorithmic questions arising here. Furthermore  the problem we consider admits a
particular structure that can be exploited to design efﬁcient revenue optimization algorithms.
From the seller’s perspective  this game can also be viewed as a bandit problem [Kuleshov and Pre-
cup  2010  Robbins  1985] since only the revenue (or reward) for the prices offered is accessible to
the seller. Kleinberg and Leighton [2003] precisely studied this continuous bandit setting under the
assumption of an oblivious buyer  that is  one that does not exploit the seller’s behavior (more pre-
cisely  the authors assume that at each round the seller interacts with a different buyer). The authors
presented a tight regret bound of Θ(log log T ) for the scenario of a buyer holding a ﬁxed valuation
and a regret bound of O(T
3 ) when facing an adversarial buyer by using an elegant reduction to a
discrete bandit problem. However  as argued by Amin et al. [2013]  when dealing with a strategic
buyer  the usual deﬁnition of regret is no longer meaningful. Indeed  consider the following exam-
ple: let the valuation of the buyer be given by v ∈ [0  1] and assume that an algorithm with sublinear
regret such as Exp3 [Auer et al.  2002b] or UCB [Auer et al.  2002a] is used for T rounds by the
seller. A possible strategy for the buyer  knowing the seller’s algorithm  would be to accept prices
only if they are smaller than some small value   certain that the seller would eventually learn to offer
only prices less than . If  (cid:28) v  the buyer would considerably boost her surplus while  in theory 
the seller would have not incurred a large regret since in hindsight  the best ﬁxed strategy would
have been to offer price  for all rounds. This  however is clearly not optimal for the seller. The
stronger notion of policy regret introduced by Arora et al. [2012] has been shown to be the appro-
priate one for the analysis of bandit problems with adaptive adversaries. However  for the example
just described  a sublinear policy regret can be similarly achieved. Thus  this notion of regret is also
not the pertinent one for the study of our scenario.
We will adopt instead the deﬁnition of strategic-regret  which was introduced by Amin et al. [2013]
precisely for the study of this problem. This notion of regret also matches the concept of learning
loss introduced by [Agrawal  1995] when facing an oblivious adversary. Using this deﬁnition  Amin
et al. [2013] presented both upper and lower bounds for the regret of a seller facing a strategic
buyer and showed that the buyer’s surplus must be discounted over time in order to be able to
achieve sublinear regret (see Section 2). However  the gap between the upper and lower bounds
they presented is in O(
T ). In the following  we analyze a very broad family of monotone regret
minimization algorithms for this problem (Section 3)  which includes the algorithm of Amin et al.
[2013]  and show that no algorithm in that family admits a strategic regret more favorable than
Ω(
T ). Next  we introduce a nearly-optimal algorithm that achieves a strategic regret differing
from the lower bound at most by a factor in O(log T ) (Section 4). This represents an exponential
improvement upon the existing best algorithm for this setting. Our new algorithm admits a natural
analysis and simpler proofs. A key idea behind its design is a method deterring the buyer from lying 
that is rejecting prices below her valuation.

√

√

2 Setup

We consider the following game played by a buyer and a seller. A good  such as an advertisement
space  is repeatedly offered for sale by the seller to the buyer over T rounds. The buyer holds a
private valuation v ∈ [0  1] for that good. At each round t = 1  . . .   T   a price pt is offered by the
seller and a decision at ∈ {0  1} is made by the buyer. at takes value 1 when the buyer accepts
to buy at that price  0 otherwise. We will say that a buyer lies whenever at = 0 while pt < v.
At the beginning of the game  the algorithm A used by the seller to set prices is announced to the
buyer. Thus  the buyer plays strategically against this algorithm. The knowledge of A is a standard
assumption in mechanism design and also matches the practice in AdExchanges.
For any γ ∈ (0  1)  deﬁne the discounted surplus of the buyer as follows:

Sur(A  v) =

γt−1at(v − pt).

(1)

T(cid:88)

t=1

2

The value of the discount factor γ indicates the strength of the preference of the buyer for current
surpluses versus future ones. The performance of a seller’s algorithm is measured by the notion of
strategic-regret [Amin et al.  2013] deﬁned as follows:

Reg(A  v) = T v − T(cid:88)

atpt.

(2)

t=1

The buyer’s objective is to maximize his discounted surplus  while the seller seeks to minimize his
regret. Note that  in view of the discounting factor γ  the buyer is not fully adversarial. The problem
consists of designing algorithms achieving sublinear strategic regret (that is a regret in o(T )).
The motivation behind the deﬁnition of strategic-regret is straightforward: a seller  with access to
the buyer’s valuation  can set a ﬁxed price for the good  close to this value. The buyer  having no
control on the prices offered  has no option but to accept this price in order to optimize his utility.
The revenue per round of the seller is therefore v−. Since there is no scenario where higher revenue
can be achieved  this is a natural setting to compare the performance of our algorithm.
To gain more intuition about the problem  let us examine some of the complications arising when
dealing with a strategic buyer. Suppose the seller attempts to learn the buyer’s valuation v by per-
forming a binary search. This would be a natural algorithm when facing a truthful buyer. However 
in view of the buyer’s knowledge of the algorithm  for γ (cid:29) 0  it is in her best interest to lie on the
initial rounds  thereby quickly  in fact exponentially  decreasing the price offered by the seller. The
seller would then incur an Ω(T ) regret. A binary search approach is therefore “too aggressive”. In-
deed  an untruthful buyer can manipulate the seller into offering prices less than v/2 by lying about
her value even just once! This discussion suggests following a more conservative approach. In the
next section  we discuss a natural family of conservative algorithms for this problem.

3 Monotone algorithms

√

√

The following conservative pricing strategy was introduced by Amin et al. [2013]. Let p1 = 1
and β < 1. If price pt is rejected at round t  the lower price pt+1 = βpt is offered at the next
round. If at any time price pt is accepted  then this price is offered for all the remaining rounds. We
will denote this algorithm by monotone. The motivation behind its design is clear: for a suitable
choice of β  the seller can slowly decrease the prices offered  thereby pressing the buyer to reject
many prices (which is not convenient for her) before obtaining a favorable price. The authors present
T ) regret bound for this algorithm  with Tγ = 1/(1 − γ). A more careful analysis shows
an O(Tγ
T ) when the discount factor γ is known to

that this bound can be further tightened to O((cid:112)TγT +

the seller.
Despite its sublinear regret  the monotone algorithm remains sub-optimal for certain choices of
γ. Indeed  consider a scenario with γ (cid:28) 1. For this setting  the buyer would no longer have an
√
incentive to lie  thus  an algorithm such as binary search would achieve logarithmic regret  while the
regret achieved by the monotone algorithm is only guaranteed to be in O(
One may argue that the monotone algorithm is too speciﬁc since it admits a single parameter
β and that perhaps a more complex algorithm with the same monotonic idea could achieve a more
favorable regret. Let us therefore analyze a generic monotone algorithm Am deﬁned by Algorithm 1.
Deﬁnition 1. For any buyer’s valuation v ∈ [0  1]  deﬁne the acceptance time κ∗ = κ∗(v) as the
ﬁrst time a price offered by the seller using algorithm Am is accepted.
Proposition 1. For any decreasing sequence of prices (pt)T
t=1  there exists a truthful buyer with
valuation v0 such that algorithm Am suffers regret of at least
√
T −

(cid:113)

T ).

Reg(Am  v0) ≥ 1
4

T .

Proof. By deﬁnition of the regret  we have Reg(Am  v) = vκ∗ + (T − κ∗)(v − pκ∗). We can
T for every v ∈ [1/2  1].
consider two cases: κ∗(v0) >
In the former case  we have Reg(Am  v0) ≥ v0
T   which implies the statement of the
proposition. Thus  we can assume the latter condition.

T for some v0 ∈ [1/2  1] and κ∗(v) ≤ √

T ≥ 1

√

√

√

2

3

Algorithm 1 Family of monotone algorithms.
Let p1 = 1 and pt ≤ pt−1 for t = 2  . . . T .
t ← 1
p ← pt
Offer price p
while (Buyer rejects p) and (t < T ) do
t ← t + 1
p ← pt
Offer price p
end while
while (t < T ) do

t ← t + 1
Offer price p

end while

Algorithm 2 Deﬁnition of Ar.

n = the root of T (T )
while Offered prices less than T do

Offer price pn
if Accepted then

n = r(n)

else

Offer price pn for r rounds
n = l(n)

end if
end while

Let v be uniformly distributed over [ 1
E[vκ∗] + E[(T − κ∗)(v − pκ∗)] ≥ 1
2

E[κ∗] + (T −

2   1]. In view of Lemma 4 (see Appendix 8.1)  we have

E[κ∗] + T − √
T
32E[κ∗] .

√

T )E[(v − pκ∗)] ≥ 1
2
√
T−√

T

.

4

4

T

√

T−√

The right-hand side is minimized for E[κ∗] =
E[Reg(Am  v)] ≥
  which implies the existence of v0 with Reg(Am  v0) ≥
√
We have thus shown that any monotone algorithm Am suffers a regret of at least Ω(
T )  even when
facing a truthful buyer. A tighter lower bound can be given under a mild condition on the prices
offered.
t=1 is said to be convex if it veriﬁes pt − pt+1 ≥ pt+1 − pt+2 for
Deﬁnition 2. A sequence (pt)T
t = 1  . . .   T − 2.

Plugging in this value yields

T−√

√

T

.

4

An instance of a convex sequence is given by the prices offered by the monotone algorithm. A
seller offering prices forming a decreasing convex sequence seeks to control the number of lies of
the buyer by slowly reducing prices. The following proposition gives a lower bound on the regret of
any algorithm in this family.
Proposition 2. Let (pt)T
√

for the buyer such that the regret of the monotone algorithm deﬁned by these prices is Ω((cid:112)T Cγ +

t=1 be a decreasing convex sequence of prices. There exists a valuation v0

T )  where Cγ = γ

2(1−γ) .

The full proof of this proposition is given in Appendix 8.1. The proposition shows that when the
discount factor γ is known  the monotone algorithm is in fact asymptotically optimal in its class.
The results just presented suggest that the dependency on T cannot be improved by any monotone
algorithm. In some sense  this family of algorithms is “too conservative”. Thus  to achieve a more
favorable regret guarantee  an entirely different algorithmic idea must be introduced. In the next
section  we describe a new algorithm that achieves a substantially more advantageous strategic regret
by combining the fast convergence properties of a binary search-type algorithm (in a truthful setting)
with a method penalizing untruthful behaviors of the buyer.

4 A nearly optimal algorithm
Let A be an algorithm for revenue optimization used against a truthful buyer. Denote by T (T ) the
tree associated to A after T rounds. That is  T (T ) is a full tree of height T with nodes n ∈ T (T )
labeled with the prices pn offered by A. The right and left children of n are denoted by r(n) and
l(n) respectively. The price offered when pn is accepted by the buyer is the label of r(n) while the
price offered by A if pn is rejected is the label of l(n). Finally  we will denote the left and right
subtrees rooted at node n by L (n) and R(n) respectively. Figure 1 depicts the tree generated by an
algorithm proposed by Kleinberg and Leighton [2003]  which we will describe later.

4

(a)

(b)

Figure 1: (a) Tree T (3) associated to the algorithm proposed in [Kleinberg and Leighton  2003]. (b) Modiﬁed
tree T (cid:48)(3) with r = 2.

Since the buyer holds a ﬁxed valuation  we will consider algorithms that increase prices only after a
price is accepted and decrease it only after a rejection. This is formalized in the following deﬁnition.
Deﬁnition 3. An algorithm A is said to be consistent if maxn(cid:48)∈L (n) pn(cid:48) ≤ pn ≤ minn(cid:48)∈R(n) pn(cid:48)
for any node n ∈ T (T ).
For any consistent algorithm A  we deﬁne a modiﬁed algorithm Ar  parametrized by an integer
r ≥ 1  designed to face strategic buyers. Algorithm Ar offers the same prices as A  but it is deﬁned
with the following modiﬁcation: when a price is rejected by the buyer  the seller offers the same
price for r rounds. The pseudocode of Ar is given in Algorithm 2. The motivation behind the
modiﬁed algorithm is given by the following simple observation: a strategic buyer will lie only if
she is certain that rejecting a price will boost her surplus in the future. By forcing the buyer to reject
a price for several rounds  the seller ensures that the future discounted surplus will be negligible 
thereby coercing the buyer to be truthful.
We proceed to formally analyze algorithm Ar.
In particular  we will quantify the effect of the
parameter r on the choice of the buyer’s strategy. To do so  a measure of the spread of the prices
offered by Ar is needed.
n := pr(n)− pn. Similarly 
Deﬁnition 4. For any node n ∈ T (T ) deﬁne the right increment of n as δr
deﬁne its left increment to be δl
The prices offered by Ar deﬁne a path in T (T ). For each node in this path  we can deﬁne time
t(n) to be the number of rounds needed for this node to be reached by Ar. Note that  since r may
be greater than 1  the path chosen by Ar might not necessarily reach the leaves of T (T ). Finally 
let S : n (cid:55)→ S(n) be the function representing the surplus obtained by the buyer when playing an
optimal strategy against Ar after node n is reached.
Lemma 1. The function S satisﬁes the following recursive relation:

n := maxn(cid:48)∈L (n) pn − pn(cid:48).

S(n) = max(γt(n)−1(v − pn) + S(r(n)) S(l(n))).

(3)
Proof. Deﬁne a weighted tree T (cid:48)(T ) ⊂ T (T ) of nodes reachable by algorithm Ar. We assign
weights to the edges in the following way: if an edge on T (cid:48)(T ) is of the form (n  r(n))  its weight
is set to be γt(n)−1(v − pn)  otherwise  it is set to 0. It is easy to see that the function S evaluates
the weight of the longest path from node n to the leafs of T (cid:48)(T ). It thus follows from elementary
graph algorithms that equation (3) holds.

The previous lemma immediately gives us necessary conditions for a buyer to reject a price.
Proposition 3. For any reachable node n  if price pn is rejected by the buyer  then the following
inequality holds:

v − pn <

(δl

n + γδr

n).

γr

(1 − γ)(1 − γr)

Proof. A direct implication of Lemma 1 is that price pn will be rejected by the buyer if and only if
(4)

γt(n)−1(v − pn) + S(r(n)) < S(l(n)).

5

1/21/43/41/165/169/1613/161/21/43/413/16However  by deﬁnition  the buyer’s surplus obtained by following any path in R(n) is bounded
above by S(r(n)). In particular  this is true for the path which rejects pr(n) and accepts every price
are the prices the seller would offer if price pr(n) were rejected. Furthermore  since algorithm Ar is

afterwards. The surplus of this path is given by(cid:80)T
consistent  we must have(cid:98)pt ≤ pr(n) = pn + δr

t=t(n)+r+1 γt−1(v −(cid:98)pt) where ((cid:98)pt)T

n. Therefore  S(r(n)) can be bounded as follows:

t=t(n)+r+1

γt−1(v − pn − δr

(v − pn − δr
n).

(5)

t=t(n)+r+1

We proceed to upper bound S(l(n)). Since pn − p(cid:48)
and

n for all n(cid:48) ∈ L (n)  v − pn(cid:48) ≤ v − pn + δl

n

γt−1(v − pn + δl

(v − pn + δl
n).

(6)

1 − γ

n) = γt(n)+r − γT
n ≤ δl
n) = γt(n)+r−1 − γT

1 − γ

S(r(n)) ≥

T(cid:88)
S(l(n)) ≤ T(cid:88)

t=tn+r

Combining inequalities (4)  (5) and (6) we conclude that

γt(n)−1(v − pn) + γt(n)+r − γT
(cid:18)

1 − γ
⇒ (v − pn)

(v − pn − δr
1 + γr+1 − γr
1 − γ

n) ≤ γt(n)+r−1 − γT
(cid:19)
1 − γ
n + γr+1δr
≤ γrδl
⇒ (v − pn)(1 − γr) ≤ γr(δl

.

n)
n + γδr
1 − γ

(v − pn + δl
n)
n − γT−t(n)+1(δr
1 − γ

n)
n + δl

Rearranging the terms in the above inequality yields the desired result.
Let us consider the following instantiation of algorithm A introduced in [Kleinberg and Leighton 
2003]. The algorithm keeps track of a feasible interval [a  b] initialized to [0  1] and an increment
parameter  initialized to 1/2. The algorithm works in phases. Within each phase  it offers prices
a +   a + 2  . . . until a price is rejected. If price a + k is rejected  then a new phase starts with
the feasible interval set to [a + (k − 1)  a + k] and the increment parameter set to 2. This process
continues until b − a < 1/T at which point the last phase starts and price a is offered for the
remaining rounds. It is not hard to see that the number of phases needed by the algorithm is less
than (cid:100)log2 log2 T(cid:101)+1. A more surprising fact is that this algorithm has been shown to achieve regret
O(log log T ) when the seller faces a truthful buyer. We will show that the modiﬁcation Ar of this
algorithm admits a particularly favorable regret bound. We will call this algorithm PFSr (penalized
fast search algorithm).
Proposition 4. For any value of v ∈ [0  1] and any γ ∈ (0  1)  the regret of algorithm PFSr admits
the following upper bound:

Reg(PFSr  v) ≤ (vr + 1)((cid:100)log2 log2 T(cid:101) + 1) +

(1 + γ)γrT

2(1 − γ)(1 − γr) .

(7)

Note that for r = 1 and γ → 0 the upper bound coincides with that of [Kleinberg and Leighton 
2003].

Proof. Algorithm PFSr can accumulate regret in two ways: the price offered pn is rejected  in which
case the regret is v  or the price is accepted and its regret is v − pn.
Let K = (cid:100)log2 log2 T(cid:101) + 1 be the number of phases run by algorithm PFSr. Since at most K
different prices are rejected by the buyer (one rejection per phase) and each price must be rejected
for r rounds  the cumulative regret of all rejections is upper bounded by vKr.
The second type of regret can also be bounded straightforwardly. For any phase i  let i and [ai  bi]
denote the corresponding search parameter and feasible interval respectively. If v ∈ [ai  bi]  the
regret accrued in the case where the buyer accepts a price in this interval is bounded by bi−ai =
i.
If  on the other hand v ≥ bi  then it readily follows that v − pn < v − bi +
i for all prices pn
offered in phase i. Therefore  the regret obtained in acceptance rounds is bounded by

√

√

K(cid:88)

(cid:16)
(v − bi)1v>bi +

Ni

(cid:17) ≤ K(cid:88)

√

i

i=1

i=1

6

(v − bi)1v>biNi + K 

denotes the number of prices offered during the i-th round.

where Ni ≤ 1√
Finally  notice that  in view of the algorithm’s deﬁnition  every bi corresponds to a rejected price.
Thus  by Proposition 3  there exist nodes ni (not necessarily distinct) such that pni = bi and

i

v − bi = v − pni ≤

γr

(1 − γ)(1 − γr)

(δl
ni

+ γδr
ni

).

It is immediate that δr

K(cid:88)

n ≤ 1/2 and δl
(v − bi)1v>biNi ≤

i=1

n ≤ 1/2 for any node n  thus  we can write
γr(1 + γ)

γr(1 + γ)

K(cid:88)

2(1 − γ)(1 − γr)

2(1 − γ)(1 − γr) T.

Ni ≤

i=1

The last inequality holds since at most T prices are offered by our algorithm. Combining the bounds
for both regret types yields the result.

When an upper bound on the discount factor γ is known to the seller  he can leverage this information
and optimize upper bound (7) with respect to the parameter r.
Theorem 1. Let 1/2 < γ < γ0 < 1 and r∗ =
if T > 4  the regret of PFSr∗ satisﬁes

. For any v ∈ [0  1] 

argminr≥1 r +

(1−γ0)(1−γr
0 )

γr
0 T

(cid:109)

(cid:108)

Reg(PFSr∗   v) ≤ (2vγ0Tγ0 log cT + 1 + v)(log2 log2 T + 1) + 4Tγ0 

where c = 4 log 2.

The proof of this theorem is fairly technical and is deferred to the Appendix. The theorem helps
us deﬁne conditions under which logarithmic regret can be achieved. Indeed  if γ0 = e−1/ log T =
O(1 − 1

log T )  using the inequality e−x ≤ 1 − x + x2/2 valid for all x > 0 we obtain

1

1 − γ0

≤ log2 T

2 log T − 1

≤ log T.

It then follows from Theorem 1 that

Reg(PFSr∗   v) ≤ (2v log T log cT + 1 + v)(log2 log2 T + 1) + 4 log T.

Let us compare the regret bound given by Theorem 1 with the one given by Amin et al. [2013]. The
above discussion shows that for certain values of γ  an exponentially better regret can be achieved
by our algorithm. It can be argued that the knowledge of an upper bound on γ is required  whereas
this is not needed for the monotone algorithm. However  if γ > 1 − 1/
T   the regret bound
on monotone is super-linear  and therefore uninformative. Thus  in order to properly compare
both algorithms  we may assume that γ < 1 − 1/
T in which case  by Theorem 1  the regret
T log T ) whereas only linear regret can be guaranteed by the monotone
of our algorithm is O(
T )  for any α < 1 and γ <
2 ) while a strictly better regret

algorithm. Even under the more favorable bound of O((cid:112)TγT +

1 − 1/T α  the monotone algorithm will achieve regret O(T
O(T α log T log log T ) is attained by ours.

√

√

√

√

α+1

5 Lower bound

The following lower bounds have been derived in previous work.
Theorem 2 ([Amin et al.  2013]). Let γ > 0 be ﬁxed. For any algorithm A  there exists a valuation
v for the buyer such that Reg(A  v) ≥ 1

12 Tγ.

This theorem is in fact given for the stochastic setting where the buyer’s valuation is a random
variable taken from some ﬁxed distribution D. However  the proof of the theorem selects D to be a
point mass  therefore reducing the scenario to a ﬁxed priced setting.
Theorem 3 ( [Kleinberg and Leighton  2003]). Given any algorithm A to be played against a
truthful buyer  there exists a value v ∈ [0  1] such that Reg(A  v) ≥ C log log T for some universal
constant C.

7

γ = .85  v = .75

γ = .95  v = .75

γ = .75  v = .25

γ = .80  v = .25

Figure 2: Comparison of the monotone algorithm and PFSr for different choices of γ and v. The regret of
each algorithm is plotted as a function of the number rounds when γ is not known to the algorithms (ﬁrst two
ﬁgures) and when its value is made accessible to the algorithms (last two ﬁgures).

Combining these results leads immediately to the following.
Corollary 1. Given any algorithm A  there exists a buyer’s valuation v ∈ [0  1] such that
Reg(A  v) ≥ max

  for a universal constant C.

12 Tγ  C log log T

(cid:16) 1

(cid:17)

We now compare the upper bounds given in the previous section with the bound of Corollary 1. For
γ > 1/2  we have Reg(PFSr  v) = O(Tγ log T log log T ). On the other hand  for γ ≤ 1/2  we may
choose r = 1  in which case  by Proposition 4  Reg(PFSr  v) = O(log log T ). Thus  the upper and
lower bounds match up to an O(log T ) factor.

6 Empirical results

In this section  we present the result of simulations comparing the monotone algorithm and our
algorithm PFSr. The experiments were carried out as follows: given a buyer’s valuation v  a discrete

set of false valuations(cid:98)v were selected out of the set {.03  .06  . . .   v}. Both algorithms were run
against a buyer making the seller believe her valuation is(cid:98)v instead of v. The value of(cid:98)v achieving

the best utility for the buyer was chosen and the regret for both algorithms is reported in Figure 2.
We considered two sets of experiments. First  the value of parameter γ was left unknown to both
algorithms and the value of r was set to log(T ). This choice is motivated by the discussion following
Theorem 1 since  for large values of T   we can expect to achieve logarithmic regret. The ﬁrst two
plots (from left to right) in Figure 2 depict these results. The apparent stationarity in the regret of
PFSr is just a consequence of the scale of the plots as the regret is in fact growing as log(T ). For
the second set of experiments  we allowed access to the parameter γ to both algorithms. The value
of r was chosen optimally based on the results of Theorem 1 and the parameter β of monotone
T ). It is worth noting that even though
our algorithm was designed under the assumption of some knowledge about the value of γ  the
experimental results show that an exponentially better performance over the monotone algorithm
is still attainable and in fact the performances of the optimized and unoptimized versions of our
algorithm are comparable. A more comprehensive series of experiments is presented in Appendix 9.

was set to 1 − 1/(cid:112)T Tγ to ensure regret in O((cid:112)T Tγ +

√

7 Conclusion

We presented a detailed analysis of revenue optimization algorithms against strategic buyers. In
doing so  we reduced the gap between upper and lower bounds on strategic regret to a logarithmic
factor. Furthermore  the algorithm we presented is simple to analyze and reduces to the truthful
scenario in the limit of γ → 0  an important property that previous algorithms did not admit. We
believe that our analysis helps gain a deeper understanding of this problem and that it can serve as a
tool for studying more complex scenarios such as that of strategic behavior in repeated second-price
auctions  VCG auctions and general market strategies.

Acknowledgments

We thank Kareem Amin  Afshin Rostamizadeh and Umar Syed for several discussions about the
topic of this paper. This work was partly funded by the NSF award IIS-1117591.

8

 0 200 400 600 800 1000 1200 2 2.5 3 3.5 4 4.5RegretNumber of rounds (log-scale)PFSmon 0 500 1000 1500 2000 2500 2 2.5 3 3.5 4 4.5RegretNumber of rounds (log-scale)PFSmon 0 20 40 60 80 100 120 2 2.5 3 3.5 4 4.5RegretNumber of rounds (log-scale)PFSmon 0 20 40 60 80 100 120 2 2.5 3 3.5 4 4.5RegretNumber of rounds (log-scale)PFSmonReferences
R. Agrawal. The continuum-armed bandit problem. SIAM journal on control and optimization  33

(6):1926–1951  1995.

K. Amin  A. Rostamizadeh  and U. Syed. Learning prices for repeated auctions with strategic buyers.

In Proceedings of NIPS  pages 1169–1177  2013.

R. Arora  O. Dekel  and A. Tewari. Online bandit learning against an adaptive adversary: from

regret to policy regret. In Proceedings of ICML  2012.

P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multiarmed bandit problem.

Machine Learning  47(2-3):235–256  2002a.

P. Auer  N. Cesa-Bianchi  Y. Freund  and R. E. Schapire. The nonstochastic multiarmed bandit

problem. SIAM J. Comput.  32(1):48–77  2002b.

N. Cesa-Bianchi  C. Gentile  and Y. Mansour. Regret minimization for reserve prices in second-price

auctions. In Proceedings of SODA  pages 1190–1204  2013.

B. Edelman and M. Ostrovsky. Strategic bidder behavior in sponsored search auctions. Decision

Support Systems  43(1)  2007.

D. He  W. Chen  L. Wang  and T. Liu. A game-theoretic machine learning approach for revenue

maximization in sponsored search. In Proceedings of IJCAI  pages 206–213  2013.

R. D. Kleinberg and F. T. Leighton. The value of knowing a demand curve: Bounds on regret for

online posted-price auctions. In Proceedings of FOCS  pages 594–605  2003.

V. Kuleshov and D. Precup. Algorithms for the multi-armed bandit problem. Journal of Machine

Learning  2010.

P. Milgrom and R. Weber. A theory of auctions and competitive bidding. Econometrica: Journal of

the Econometric Society  pages 1089–1122  1982.

M. Mohri and A. Mu˜noz Medina. Learning theory and algorithms for revenue optimization in

second-price auctions with reserve. In Proceedings of ICML  2014.

P. Morris. Non-zero-sum games. In Introduction to Game Theory  pages 115–147. Springer  1994.
J. Nachbar. Bayesian learning in repeated games of incomplete information. Social Choice and

Welfare  18(2):303–326  2001.

J. H. Nachbar. Prediction  optimization  and learning in repeated games. Econometrica: Journal of

the Econometric Society  pages 275–309  1997.

M. Ostrovsky and M. Schwarz. Reserve prices in internet advertising auctions: A ﬁeld experiment.

In Proceedings of EC  pages 59–60. ACM  2011.

H. Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Selected

Papers  pages 169–177. Springer  1985.

W. Vickrey. Counterspeculation  auctions  and competitive sealed tenders. The Journal of ﬁnance 

16(1):8–37  2012.

9

,Shahin Shahrampour
Sasha Rakhlin
Ali Jadbabaie
Mehryar Mohri
Andres Munoz