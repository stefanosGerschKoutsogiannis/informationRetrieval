2013,Generalized Random Utility Models with Multiple Types,We propose a model for demand estimation in multi-agent  differentiated product settings and present an estimation algorithm that uses reversible jump MCMC techniques to classify agents' types. Our model extends the popular setup in Berry  Levinsohn and Pakes (1995) to allow for the data-driven classification of agents' types using agent-level data. We focus on applications involving data on agents' ranking over alternatives  and present theoretical conditions that establish the identifiability of the model and uni-modality of the likelihood/posterior. Results on both real and simulated data provide support for the scalability of our approach.,Generalized Random Utility Models with Multiple

Types

Hossein Azari Souﬁani

Hansheng Diao

Zhenyu Lai

David C. Parkes

SEAS

Mathematics Department Economics Department

SEAS

Harvard University
azari@fas.harvard.edu

Harvard University
diao@fas.harvard.edu

Harvard University
zlai@fas.harvard.edu

Harvard University

parkes@eecs.harvard.edu

Abstract

We propose a model for demand estimation in multi-agent  differentiated prod-
uct settings and present an estimation algorithm that uses reversible jump MCMC
techniques to classify agents’ types. Our model extends the popular setup in Berry 
Levinsohn and Pakes (1995) to allow for the data-driven classiﬁcation of agents’
types using agent-level data. We focus on applications involving data on agents’
ranking over alternatives  and present theoretical conditions that establish the iden-
tiﬁability of the model and uni-modality of the likelihood/posterior. Results on
both real and simulated data provide support for the scalability of our approach.

1

Introduction

Random utility models (RUM)  which presume agent utility to be composed of a deterministic com-
ponent and a stochastic unobserved error component  are frequently used to model choices by in-
dividuals over alternatives. In this paper  we focus on applications where the data is rankings by
individuals over alternatives. Examples from economics include the popular random coefﬁcients
logit model [7] where the data may involve a (partial) consumer ranking of products [9]. In a RUM 
each agent receives an intrinsic utility that is common across all agents for a given choice of alter-
native  a pairwise-speciﬁc utility that varies with the interaction between agent characteristics and
the characteristics of the agent’s chosen alternative  as well as an agent-speciﬁc taste shock (noise)
for his chosen alternative. These ingredients are used to construct a posterior/likelihood function of
speciﬁc data moments  such as the fraction of agents of each type that choose each alternative.
To estimate preferences across heterogenous agents  one approach as allowed by prior work [20  24]
is to assume a mixture of agents with a ﬁnite number of types. We build upon this work by develop-
ing an algorithm to endogenously learn the classiﬁcation of agent types within this mixture. Empir-
ical researchers are increasingly being presented with rich data on the choices made by individuals 
and asked to classify these agents into different types [28  29] and to estimate the preferences of each
type [10  23]. Examples of individual-level data used in economics include household purchases
from supermarket-scanner data [1  21]  and patients’ hospital or treatment choices from healthcare
data [22].
The partitioning of agents into latent  discrete sets (or “types”) allows for the study of the underlying
distribution of preferences across a population of heterogeneous agents. For example  preferences
may be correlated with an agent characteristic  such as income  and the true classiﬁcation of each
agent’s type  such as his income bracket  may be unobserved. By using a model of demand to esti-
mate the elasticity in behavioral response of each type of agent and by aggregating these responses
over the different types of agents  it is possible to simulate the impact of a social or public policy [8] 
or simulate the counterfactual outcome of changing the options available to agents [19].

1

1.1 Our Contributions

This paper focuses on estimating generalized random utility models (GRUM1) when the observed
data is partial orders of agents’ rankings over alternatives and when latent types are present.
We build on recent work [3  4] on estimating GRUMs by allowing for an interaction between agent
characteristics and the characteristics of the agent’s chosen alternative.The interaction term helps us
to avoid unrealistic substitution patterns due to the independence of irrelevant alternatives [26] by
allowing agent utilities to be correlated across alternatives with similar characteristics. For example 
this prevents a situation where removing the top choices of both a rich household and a poor house-
hold lead them to become equally likely to substitute to the same alternative choice. Our model also
allows the marginal utilities associated with the characteristics of alternatives to vary across agent
types.
To classify agents’
types and
estimate the parameters associ-
ated with each type  we pro-
pose an algorithm involving a
novel application of
reversible
jump Markov Chain Monte Carlo
(RJMCMC) techniques. RJM-
CMC can be used for model se-
lection and learning a posterior
on the number of types in a mix-
ture model [31]. Here  we use
RJMCMC to cluster agents into
different types  where each type
exhibits demand for alternatives
based on different preferences;
i.e.  different interaction terms be-
tween agent and alternative char-
acteristics.
We apply the approach to a real-world dataset involving consumers’ preference rankings and also
conduct experiments on synthetic data to perform coverage analysis of RJMCMC. The results show
that our method is scalable  and that the clustering of types provides a better ﬁt to real world data.
The proposed learning algorithm is based on Bayesian methods to ﬁnd posteriors on the parameters.
This differentiates us from previous estimation approaches in econometrics rely on techniques based
on the generalized method of moments.2
The main theoretical contribution establishes identiﬁability of mixture models over data consisting
of partial orders. Previous theoretical results have established identiﬁability for data consisting of
vectors of real numbers [2  18]  but not for data consisting of partial orders. We establish conditions
under which the GRUM likelihood function is uni-modal for the case of observable types. We do
not provide results on the log concavity of the general likelihood problem with unknown types and
leave it for future studies.

Figure 1: A GRUM with multiple types of agents

1.2 Related work

Prior work in econometrics has focused on developing models that use data aggregated across types
of agents  such as at the level of a geographic market  and that allow heterogeneity by using random
coefﬁcients on either agents’ preference parameters [7  9] or on a set of dummy variables that deﬁne
types of agents [6  27]  or by imposing additional structure on the covariance matrix of idiosyncratic
taste shocks [16]. In practice  this approach typically relies on restrictive functional assumptions
about the distribution of consumer taste shocks that enter the RUM in order to reduce computational

1Deﬁned in [4] as a RUM with a generalized linear model for the regression of the mean parameters on the

interaction of characteristics data as in Figure 1

2There are alternative methods to RJMCMC  such as the saturation method [13]. However  the memory
required to keep track of former sampled memberships in the saturation method quickly becomes infeasible
given the combinatorial nature of our problem.

2

HeavinessSale VolumePrice GenderAgeAlternativesAgentsCharacteristics of AlternativesCharacteristics of AgentsCharacteristics RelationshipsCustomerLoyaltyExpectedUtilitiesAlternative 1Alternative 2Alternative 3Alternatives’ Intrinsic E(cid:31)ectSake SushiEbi SushiTako SushiAgent Type 1Agent Type 4Agent Type 3Agent Type 2= δ  + x  W (z )µij jijT1= δ  + x  W  (z )µij jijT2= δ  + x  W  (z )µij jijT3= δ  + x  W  (z )µij jijT4burden. For example  the logit model [26] assumes i.i.d. draws from a Type I extreme value dis-
tribution. This may lead to biased estimates  in particular when the number of alternatives grow
large [5].
Previous work on clustering ranking data for variations of the Placket-Luce (PL) model [28  29]
has been restricted to settings without agent and alternative characteristics. Morover  Gormley et
al. [28] and Chu et al. [14] performed clustering for RUMs with normal distributions  but this was
limited to pairwise comparisons. Inference of GRUMs for partial ranks involved the computational
hardness addressed in [3]. In mixture models  assuming an arbitrary number of types can lead to
biased results  and reduces the statistical efﬁciency of the estimators [15].
To the best of our knowledge  we are the ﬁrst to study the identiﬁability and inference of GRUMs
with multiple types. Inference for GRUMs has been generalized in [4]  However  Azari et al. [4]
do not consider existence of multiple types. Our method applies to data involving individual-level
observations  and partial orders with more than two alternatives. The inference method establishes
a posterior on the number of types  resolving the common issue of how the researcher should select
the number of types.
2 Model
Suppose we have N agents and M alternatives {c1  ..  cM}  and there are S types (subgroups) of
agents and s(n) is agent n’s type.
Agent characteristics are observed and deﬁned as an N ×K matrix X  and alternative characteristics
are observed and deﬁned as an L × M matrix Z  where K and L are the number of agent and
alternative characteristics respectively.
Let unm be agent n’s perceived utility for alternative m  and let W s(n) be a K × L real matrix that
models the linear relation between the attributes of alternatives and the attributes of agents. We have 

(1)
where (cid:126)xn is the nth row of the matrix X and (cid:126)zm is the mth column of the matrix Z. In words  agent
n’s utility for alternative m consists of the following three parts:

unm = δm + (cid:126)xnW s(n)((cid:126)zm)T + nm 

1. δm:gs The intrinsic utility of alternative m  which is the same across all agents;
2. (cid:126)xnW s(n)((cid:126)zm)T : The agent-speciﬁc utility  which is unique to all agents of type s(n)  and

where W s(n) has at least one nonzero element;

3. nm: The random noise (agent-speciﬁc taste shock)  which is generated independently

across agents and alternatives.

M×P   such that A(n)

KL+m m = 1 for 1 ≤ m ≤ M and A(n)

The number of parameters for each type is P = KL + M.
See Figure 2 for an illustration of the model. In order to write the model as a linear regression  we
KL+m m(cid:48) = 0 for m (cid:54)=
deﬁne matrix A(n)
(k−1)L+l m = (cid:126)xn(k)(cid:126)zm(l) for 1 ≤ l ≤ L and 1 ≤ k ≤ K. We also need to shufﬂe
m(cid:48) and A(n)
the parameters for all types into a P × S matrix Ψ  such that ΨKL+m s = δ and Ψ(k−1)L+l s =
S×1 to indicate the type of agent n  with
W s
kl
B(n)
s(n) 1 = 1 and B(n)
m 1 = unm.
We can now rewrite (1) as:

s 1 = 0 for all s (cid:54)= s(n). We also deﬁne an M × 1 matrix  U (n)  as U (n)

for 1 ≤ k ≤ K and 1 ≤ l ≤ L. We adopt B(n)

U (n) = A(n)ΨB(n) + 

(2)

be written as  Pr(U (n)|X (n)  Z  Ψ  Γ) = (cid:80)S

Suppose that an agent has type s with probability γs. Given this  the random utility model can
s=1 γs Pr(U (n)|X (n)  Z  Ψs)  where Ψs is the sth
column of the matrix Ψ. An agent ranks the alternatives according to her perceived utilities for
the alternatives. Deﬁne rank order πn as a permutation (πn(1)  . . .   πn(m)) of {1  . . .   M}. πn
represents the full ranking [cπi(1) (cid:31)i cπi(2) (cid:31)i
··· (cid:31)i cπi(m)] of the alternatives {c1  ..  cM}.
That is  for agent n  cm1 (cid:31)n cm2 if and only if unm1 > unm2 (In this model  situations with tied
perceived utilities have zero probability measure).

3

The model for observed data π(n)  can be written as:
Pr(π(n)|X (n)  Z  Γ  Ψ) =

Pr(U (n)|X (n)  Z  Ψ  Γ) =

(cid:90)

π(n)=order (U (n))

S(cid:88)

s=1

γs Pr(π(n)|X (n)  Z  Ψs)

Note that X (n) and Z are observed characteristics  while Γ and Ψ are unknown parameters. π =
order (U ) is the ranking implied by U  and π(i) is the ith largest utility in U. D = {π1  ..  πN}
denotes the collection of all data for different agents. We have that

N(cid:89)

Pr(D|X  Z  Ψ  Γ) =

Pr(π(n)|X (n)  Z  Ψ  Γ)

3 Strict Log-concavity and Identiﬁability

n=1

In this section  we establish conditions for identiﬁability of the types and parameters for the model.
Identiﬁability is a necessary property in order for researchers to be able to infer economically-
relevant parameters from an econometric model. Establishing identiﬁability in a model with multiple
types and ranking data requires a different approach from classical identiﬁability results for mixture
models [2  18  e.g.].
Moreover  we establish conditions for uni-modality of the
likelihood for the parameters Γ and Ψ  when the types
are observed. Although our main focus is on data with
unobservable types  establishing the conditions for uni-
modality conditioned on known types remains an essen-
tial step because of the sampling and optimization aspects
of RJMCMC. We sample from the parameters conditional
on the algorithm’s speciﬁcation of types.
The uni-modality result establishes that the sampling ap-
proach is exploring a uni-modal distribution conditional
on its speciﬁed types. Despite adopting a Bayesian point
of view in presenting the model  we adopt a uniform prior
on the parameter set  and only impose nontrivial priors on
the number of types in order to obtain some regulariza-
tion. Given this  we present the theory with regards to the
likelihood function from the data rather than the posterior
on parameters.

Figure 2: Graphical representation of
the multiple type GRUM generative
process.

3.1 Strict Log-concavity of the Likelihood Function

k l xn(k)W s(n)

kl

1 ( (cid:126)ψ (cid:126)) ≥ 0  ...  gn

M−1( (cid:126)ψ (cid:126)) ≥ 0). This is because gn

m = 1  ..  M − 1 where µnj = δj +(cid:80)

For agent n  we deﬁne a set Gn of function gn’s whose positivity is equivalent to giving an order
m( (cid:126)ψ (cid:126)) = [µnπn(m) + nπn(m)] − [µnπn(m+1) + nπn(m+1)] for
πn. More precisely  we deﬁne gn
zj(l) for 1 ≤ j ≤ M. Here  (cid:126)ψ is a
vector of KL + M variables consisting of all δj’s and Wkl’s. We have  L( (cid:126)ψ  πn) = L( (cid:126)ψ  Gn) =
m( (cid:126)ψ (cid:126)) ≥ 0 is equivalent to saying
Pr(gn
alternative πn(m) is preferred to alternative πn(m + 1) in the RUM sense.
Then using the result in [3] and [30]  L( (cid:126)ψ) = L( (cid:126)ψ  π) is logarithmic concave in the sense that
L(λ (cid:126)ψ + (1 − λ) (cid:126)ψ(cid:48)) ≥ L(ψ)λL(ψ(cid:48))1−λ for any 0 < λ < 1 and any two vectors (cid:126)ψ  (cid:126)ψ(cid:48) ∈ RLK+M .
The detailed statement and proof of this result are contained in the Appendix. Let’s consider all
n=1 log P r(πn| (cid:126)ψs(n)). By log-concavity
of L( (cid:126)ψ  π) and using the fact that sum of concave functions is concave  we know that l(Ψ  D) is
concave in Ψ  viewed as a vector in RSKL+M . To show uni-modality  we need to prove that this

n agents together. We study the function  l(Ψ  D) = (cid:80)N

4

XZδWγAψBuπN(n)(n)(n)(n)(n)(n)concave function has a unique maximum. Namely  we need to be able to establish the conditions for
when the equality holds. If our data is subject to some mild condition  which implies boundedness
of the parameter set that maximizes l(Ψ  D)  Theorem 1 bellow tells us when the equality holds.
This condition has been explained in [3] as condition (1).
Before stating the main result  we deﬁne the following auxiliary (M − 1)N(cid:48) × (SKL + M − 1)
(Here  let N(cid:48) ≤ N be a positive number that we will specify later.) such that 
to 0 if s (cid:54)= s(n)  for all 1 ≤ n ≤ N(cid:48)  1 ≤ m ≤ M − 1  1 ≤ s ≤ S  1 ≤ k ≤ K  and
for all 1 ≤ m  m(cid:48) ≤ M − 1 and 1 ≤ n ≤ N(cid:48).

matrix (cid:101)A = (cid:101)AN(cid:48)
(cid:101)A(M−1)(n−1)+m (s−1)KL+(K−1)l+k is equal to xn(k)(zm(l) − zM (l))if s = s(n) and is equal
1 ≤ l ≤ L. Also  (cid:101)A(M−1)(n−1)+m SKL+m(cid:48) is equal to 1 if m = m(cid:48) and is equal to 0 if m (cid:54)= m(cid:48) 
Theorem 1. Suppose there is an N(cid:48) ≤ N such that rank (cid:101)AN(cid:48)

= SKL + M − 1. Then l(Ψ) =

l(Ψ  D) is strictly concave up to δ-shift  in the sense that 

l(λΨ + (1 − λ)Ψ(cid:48)) ≥ λl(Ψ) + (1 − λ)l(Ψ(cid:48)) 

(3)
for any 0 < λ < 1 and any Ψ  Ψ(cid:48) ∈ RSKL+M   and the equality holds if and only if there exists
c ∈ R  such that:

(cid:26) δm = δ(cid:48)

m + c
kl = W (cid:48)s
kl

W s

for all 1 ≤ m ≤ M
for all s  k  l

The proof of this theorem is in the appendix.
Remark 1. We remark that the strictness “up to δ-shift” is natural. A δ-shift results in a shift in the
intrinsic utilities of all the products  which does not change the utility difference between products.
So such a shift does not affect our outcome. In practice  we may set one of the δ’s to be 0 and then
our algorithm will converge to a single maximum.
Remark 2. It’s easy to see that N(cid:48) must be larger than or equal to 1 + SKL
introduce N(cid:48) is to avoid cumbersome calculations involving N.

M−1 . The reason we

3.2

Identiﬁability of the Model

In this section  we show that  for the case of unobserved types  our model is identiﬁable for a certain
class of cdfs for the noise in random utility models. Let’s ﬁrst specify this class of “nice” cdfs:
Deﬁnition 1. Let φ(x) be a smooth pdf deﬁned on R or [0 ∞)  and let Φ(x) be the associated cdf.
For each i ≥ 1  we write φ(i)(x) for the i-th derivative of φ(x). Let gi(x) = φ(i+1)(x)
φ(i)(x) . The function
Φ is called nice if it satisﬁes one of the following two mutually exclusive conditions:

(a) φ(x) is deﬁned on R. For any x1  x2 ∈ R  the sequence gi(x1)
R (as i → ∞) only if either x1 = x2; or x1 = −x2 and gi(x1)

(b) φ(x) is deﬁned on [0 ∞). For any x1  x2 ≥ 0  the ratio φ(i)(x1)

sufﬁciently large. Moreover  we require that φ(x1) = φ(x2) if and only if x1 = x2.

gi(x2) converges to some value in
gi(x2) → −1 as i → ∞.
φ(i)(x2) is independent of i for i

S(cid:88)

S(cid:48)(cid:88)

This class of nice functions contains normal distributions and exponential distributions. A proof of
this fact is included in the appendix.
Identiﬁability is formalized as follows: Let C = {{γs}S
Suppose  for two sequences {γs}S

s=1 | S ∈ Z>0  γi ∈ R>0 (cid:80)S

s=1 γs = 1}.

s}S(cid:48)
s=1  we have:

s=1 and {γ(cid:48)

γs Pr(π|X (n)  Z  Ψ) =

s Pr(π|X (n)  Z  Ψ(cid:48))
γ(cid:48)

(4)

s=1

s=1

for all possible orders π of M products  and for all agents n. Then  we must have S = S(cid:48) and (up to
a permutation of indices {1 ···   S}) γs = γ(cid:48)

s and Ψ = Ψ(cid:48) (up to δ-shift).

5

S(cid:48)(cid:88)

S(cid:48)(cid:88)

S(cid:88)

For now  let’s ﬁx the number of agent characteristics  K. One observation is that the number xn(k) 
for any characteristic k  reﬂects certain characteristics of agent n. Varying the agent n  this amount
xn(k) is in a bounded interval in R. Suppose the collection of data D is sufﬁciently large. Based
on this  assuming that N can be be arbitrarily large  we can assume that the xn(k)’s form a dense
subset in a closed interval Ik ⊂ R. Hence  (4) should hold for any X ∈ Ik  leading to the following
theorem:

Theorem 2. Deﬁne an (M − 1)× L matrix (cid:101)Z by setting (cid:101)Zm l = zm(l)− zM (l). Suppose the matrix
(cid:101)Z has rank L  and suppose 
S(cid:88)

S(cid:48)(cid:88)

γs Pr(π|X  Z  Ψ) =

s Pr(π|X  Z  Ψ(cid:48)) 
γ(cid:48)

(5)

s=1

s=1

s and Ψ = Ψ(cid:48) (up to δ-shift).

for all x(k) ∈ Ik and all possible orders π of M products. Here  the probability measure is associ-
ated with a nice cdf. Then we must have S = S(cid:48) and (up to a permutation of indices {1 ···   S}) 
γs = γ(cid:48)
The proof of this theorem is provided in the appendix. Here  we illustrate the idea for the simple
case  with two alternatives (m = 2) and no agent or alternative characteristics (K = L = 1).
Equation (5) is merely a single identity. Unwrapping the deﬁnition  we obtain:

S(cid:88)

γs Pr(1−2 > δ1−δ2 +xW s(z1−z2)) =

s Pr(1−2 > δ(cid:48)
γ(cid:48)

1−δ(cid:48)

2 +xW (cid:48)s(z1−z2)). (6)

s=1

s=1

Without loss of generality  we may assume z1 = 1  z2 = 0  and δ2 = 0. We may further assume
that the interval I = I1 contains 0. (Otherwise  we just need to shift I and δ accordingly.) Given
this  the problem reduces to the following lemma:
Lemma 1. Let Φ(x) be a nice cdf. Suppose 

γsΦ(δ + xW s) =

γ(cid:48)
sΦ(δ(cid:48) + xW (cid:48)s) 

(7)

s=1

s=1

for all x in a closed interval I containing 0. Then we must have S = S(cid:48)  δ = δ(cid:48) and (up to a
permutation of {1 ···   S}) γs = γs  W s = W (cid:48)s.
The proof of this lemma is in the appendix. By applying this to (6)  we can show identiﬁablity for
the simple case of m = 2 and K = L = 1.
Theorem 2 guarantees identiﬁability in the limit case that we observe agents with characteristics
that are dense in an interval. Beyond the theoretical guarantee  we would in practice expect (6) to
have a unique solution with a enough agents with different characteristics. Lemma 1 itself is a new
identiﬁability result for scalar observations from a set of truncated distributions.

4 RJMCMC for Parameter Estimation

We are using a uniform prior for the parameter space and regularize the number of types with a
geometric prior. We use a Gibbs sampler  as detailed in the appendix (supplementary material
Algorithm (1)) to sample from the posterior. In each of T iterations  we sample utilities un for
each agent  matrix ψs for each type  and set of assignments of agents to alternatives Sn. The utility
of each agent for each alternative conditioned on the data and other parameters is sampled from
a truncated Exponential Family (e.g. Normal) distribution.
In order to sample agent i’s utility
for alternative j (uij)  we set thresholds for lower and upper truncation based on agent i’s former
samples of utility for the two alternatives that are ranked one below and one above alternative j 
respectively.
We use reversible-jump MCMC [17] for sampling from conditional distributions of the assignment
function (see Algorithm 1). We consider three possible moves for sampling from the assignment
function S(n):

6

a

1

1
S

of

by

S+1

.

one 

. p+1
p−1

Increasing

this move is:

Pr(S) Pr(M(t)|D)

number
its own.

through moving

(1)
the
types
random agent
to a new type of
The acceptance ratio for
Prsplit =
min{1  Pr(S+1) Pr(M(t+1)|D)
p(α) .J(t)→(t+1)}  where M(t) = {u  ψ  B  S  π}(t) 
. 1
and J(t)→(t+1) = 2P is the Jacobian of the transformation from the previous state to the proposed
state and Pr(S) is the prior (regularizer) for the number of types.
(2) Decrease the number of types by one  through merging two random types. The acceptance ratio
for the merge move is: Prmerge = min{1  Pr(S−1) Pr(M(t+1)|D)
(3) We do not change the number of types  and consider moving one random agent from one type to
another. This case reduces to a standard Metropolis-Hastings  where because of the normal symmet-
ric proposal distribution  the proposal is accepted with probability: Prmh = min{1  Pr(M(t+1)|D)
Pr(M(t)|D) }.
Algorithm 1 RJMCMC to update S(t+1)(n) from
S(t)(n)

.J(t)→(t+1)}.

Pr(S) Pr(M(t)|D)

5 Experimental Study

. p−1
p+1

.

1

S−1

1
S

We evaluate the performance of the algorithm
on synthetic data  and for a real world data
set in which we observe agents’ characteris-
tics and their orderings on alternatives. For the
synthetic data  we generate data with different
numbers of types and perform RJMCMC in or-
der to estimate the parameters and number of
types. The algorithm is implemented in MAT-
LAB and scales linearly in the number of sam-
It takes on average 60 ± 5
ples and agents.
seconds to generate 50 samples for N = 200 
M = 10  K = 4 and L = 3 on an i5 2.70GHz
Intel(R).

Set p−1  p0  p+1  Find S: number of distinct
types in S(t)(n)
Propose move ν from {−1  0  +1} with proba-
bilities p−1  p0  p+1  respectively.
case ν = +1:

Select random type Ms and agent n ∈ Ms
uniformly and Assign n to module Ms1 and
remainder to Ms2 and Draw vector α ∼
N (0  1) and Propose ψs1 = ψs − α and
ψs2 = ψs + α and Compute proposal
{un  πn}(t+1)
Accept
S + 1 
S(t+1)(Ms2) = s with Prsplit from up-
date S = S + 1

S(t+1)(Ms1 )

=

case ν = −1:

Select
two random types Ms1 and Ms2
and Merge into one type Ms and Propose
ψs = (ψs1 + ψs1)/2 and Compute proposed
{un  πn}(i+1)
Accept S(t+1)(n) = s1 for ∀n s.t. S(t)(n) =
s2 with Prmerge update S = S − 1

Coverage Analysis for the number of types S
for Synthetic Data:
In this experiment  the
data is generated from a randomly chosen num-
ber of clusters S for N = 200  K = 3  L = 3
and M = 10 and the posterior on S is es-
timated using RJMCMC. The prior is chosen
to be Pr(S) ∝ exp(−3SKL). We consider
a noisy regime by generating data from noise
level of σ = 1  where all the characteristics
(X Z) are generated from N (0  1). We repeat
the experiment 100 times. Given this  we esti-
mate 60%  90% and 95% conﬁdence intervals
for the number of types from the posterior sam-
ples. We also estimate the coverage percentage 
which is deﬁned to be the percentage of samples which include the true number of types in the
interval. The simulations show 61%  73%  88%  93% for the intervals 60%  75%  90%  95%
respectively  which indicates that the method is providing reliable intervals for the number of types.

Select two random types Ms1 and Ms2 and
Move a random agent n from Ms1 to Ms2
and Compute proposed {u(n)  π(n)}(t+1)
Accept S(t+1)(n) = s2 with probability
Prmh

case ν = 0:

end switch

Performance for Synthetic Data: We generate data randomly from a model with between 1 and
4 types. N is set to 200  and M is set to 10 for K = 4 and L = 3. We draw 10  000 samples from
the stationary posterior distribution. The prior for S has chosen to be exp(−αSKL) where α is
uniformly chosen in (0  10). We repeat the experiment 5 times. Table 1 shows that the algorithm
successfully provides larger log posterior when the number of types is the number of true types.

Clustering Performance for Real World Data: We have tested our algorithm on a sushi dataset 
where 5  000 users provide rankings on M = 10 different kinds of sushi [25]. We ﬁt the multi-type

7

GRUM for different number of types  on 100 randomly chosen subsets of the sushi data with size
N = 200   using the same prior we used in synthetic case and provide the performance on the Sushi
data in Table 1. It can be seen that GRUM with 3 types has signiﬁcantly better performance in terms
of log posterior (with the prior that we chose  log posterior can be seen as log likelihood penalized
for number of parameters) than GRUM with one  two or four types. We have taken non-categorical
features as K = 4 feature for agents (age  time for ﬁlling the questionnaire  region ID  prefecture
ID) and L = 3 features for sushi ( price heaviness  sales volume).
6 Conclusions

Figure 3: Left Panel: 10000 samples for S in Syn-
thetic data  where the true S is 5. Right Panel:
Histogram of the samples for S with max at 5 and
mean at 4.56.

Synthetic True types

Type

In this paper  we have proposed an extension of
GRUMs in which we allow agents to adopt het-
erogeneous types. We develop a theory estab-
lishing the identiﬁability of the mixture model
when we observe ranking data. Our theoreti-
cal results for identiﬁability show that the num-
ber of types and the parameters associated with
them can be identiﬁed. Moreover  we prove
uni-modality of the likelihood (or posterior)
function when types are observable. We pro-
pose a scalable algorithm for inference  which
can be parallelized for use on very large data
sets. Our experimental results show that models
with multiple types provide a signiﬁcantly bet-
ter ﬁt  in real-world data. By clustering agents
into multiple types  our estimation algorithm
allows choices to be correlated across agents
of the same type  without making any a priori
assumptions on how types of agents are to be
partitioned. This use of machine learning tech-
niques complements various approaches in economics [11  7  8] by allowing the researcher to have
additional ﬂexibility in dealing with missing data or unobserved agent characteristics. We expect
the development of these techniques to grow in importance as large  individual-level datasets be-
come increasingly available. In future research we intend to pursue applications of this method to
problems of economic interest.

Sushi
One two Three Four sushi
one type
-2069 -2631 -2780 -2907 -2880
-2755 -2522 -2545 -2692 -2849
two types
three types -2796 -2642 -2582 -2790 -2819
-2778 -2807 -2803 -2593 -2850
four types

Table 1: Performance of the method for different
number of true types and number of types in algorithm
in terms of log posterior. All the standard deviations
are between 15 and 20. Bold numbers indicate the
best performance in their column with statistical sig-
niﬁcance of 95%.

Acknowledgments

This work is supported in part by NSF Grants No. CCF- 0915016 and No. AF-1301976. We thank
Elham Azizi for helping in the design and implementation of RJMCMC algorithm. We thank Simon
Lunagomez for helpful discussion on RJMCMC. We thank Lirong Xia  Gregory Lewis  Edoardo
Airoldi  Ryan Adams and Nikhil Agarwal for comments on the modeling and algorithmic aspects of
this paper. We thank anonymous NIPS-13 reviewers  for helpful comments and suggestions.

References
[1] Daniel A. Ackerberg. Advertising  learning  and consumer choice in experience goods: An empirical

examination. International Economic Review  44(3):1007–1040  2003.

[2] N. Atienza  J. Garcia-Heras  and J.M. Muoz-Pichardo. A new condition for identiﬁability of ﬁnite mix-

ture distributions. Metrika  63(2):215–221  2006.

[3] Hossein Azari Souﬁani  David C. Parkes  and Lirong Xia. Random utility theory for social choice. In
Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS)  pages 126–
134  Lake Tahoe  NV  USA  2012.

[4] Hossein Azari Souﬁani  David C. Parkes  and Lirong Xia. Preference elicitation for generalized random
utility models. In Proceedings of the Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI) 
Bellevue  Washington  USA  2013.

[5] Patrick Bajari and C. Lanier Benkard. Discrete choice models as structural models of demand: Some

economic implications of common approaches. Technical report  Working Paper  2003.

8

020004000600080001000002468100246810Number of Subgroups (S)IterationsFrequencyBiometrika  82(4):711–732  1995.
[18] Bettina Grn and Friedrich Leisch.

Identiﬁability of ﬁnite mixtures of multinomial logit models with

[6] James Berkovec and John Rust. A nested logit model of automobile holdings for one vehicle households.

Transportation Research Part B: Methodological  19(4):275–285  1985.

[7] Steven Berry  James Levinsohn  and Ariel Pakes. Automobile prices in market equilibrium. Economet-

rica  63(4):841–890  1995.

[8] Steven Berry  James Levinsohn  and Ariel Pakes. Voluntary export restraints on automobiles: evaluating

a trade policy. The American Economic Review  89(3):400–430  1999.

[9] Steven Berry  James Levinsohn  and Ariel Pakes. Differentiated products demand systems from a com-
bination of micro and macro data: The new car market. Journal of Political Economy  112(1):68–105 
2004.

[10] Steven Berry and Ariel Pakes. Some applications and limitations of recent advances in empirical indus-

trial organization: Merger analysis. The American Economic Review  83(2):247–252  1993.

[11] Steven Berry. Estimating discrete-choice models of product differentiation. The RAND Journal of Eco-

nomics  pages 242–262  1994.

[12] Edwin Bonilla  Shengbo Guo  and Scott Sanner. Gaussian process preference elicitation. In Advances in

Neural Information Processing Systems 23  pages 262–270. 2010.

[13] Stephen P Brooks  Paulo Giudici  and Gareth O Roberts. Efﬁcient construction of reversible jump
Markov chain Monte Carlo proposal distributions. Journal of the Royal Statistical Society: Series B
(Statistical Methodology)  65(1):3–39  2003.

[14] Wei Chu and Zoubin Ghahramani. Gaussian processes for ordinal regression. In Journal of Machine

Learning Research  pages 1019–1041  2005.

[15] Chris Fraley and Adrian E. Raftery. How many clusters? which clustering method? answers via model-

based cluster analysis. THE COMPUTER JOURNAL  41(8):578–588  1998.

[16] John Geweke  Michael Keane  and David Runkle. Alternative computational approaches to inference in

the multinomial probit model. Review of Economics and Statistics  pages 609–632  1994.

[17] P.J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.

varying and ﬁxed effects. Journal of Classiﬁcation  25(2):225–247  2008.

[19] Jerry A. Hausman. Valuation of new goods under perfect and imperfect competition. In The economies

of new goods  pages 207–248. University of Chicago Press  1996.

[20] James J. Heckman and Burton Singer. Econometric duration analysis. Journal of Econometrics  24(1-

2):63–132  1984.

[21] Igal Hendel and Aviv Nevo. Measuring the implications of sales and consumer inventory behavior.

Econometrica  74(6):1637–1673  2006.

[22] Katherine Ho. The welfare effects of restricted hospital choice in the us medical care market. Journal of

Applied Econometrics  21(7):1039–1079  2006.

[23] Neil Houlsby  Jose Miguel Hernandez-Lobato  Ferenc Huszar  and Zoubin Ghahramani. Collaborative
gaussian processes for preference learning. In Proceedings of the Annual Conference on Neural Infor-
mation Processing Systems (NIPS)  pages 2105–2113. Lake Tahoe  NV  USA  2012.

[24] Kamel Jedidi  Harsharanjeet S. Jagpal  and Wayne S. DeSarbo. Finite-mixture structural equation models
for response-based segmentation and unobserved heterogeneity. Marketing Science  16(1):39–59  1997.
[25] Toshihiro Kamishima. Nantonac collaborative ﬁltering: Recommendation based on order responses. In
Proceedings of the Ninth International Conference on Knowledge Discovery and Data Mining (KDD) 
pages 583–588  Washington  DC  USA  2003.

[26] Daniel McFadden. The measurement of urban travel demand. Journal of Public Economics  3(4):303–

328  1974.

[27] Daniel McFadden. Modelling the choice of residential location.

In Daniel McFadden  A Karlqvist 
L Lundqvist  F Snickars  and J Weibull  editors  Spatial Interaction Theory and Planing Models  pages
75–96. New York: Academic Press  1978.

[28] Gormley-Claire McParland  Damien. Clustering ordinal data via latent variable models. IFCS 2013 Con-
ference of the International Federation of Classiﬁcation Societies  Tilburg University  The Netherlands 
2013.

[29] Marina Meila and Harr Chen. Dirichlet process mixtures of generalized Mallows models. arXiv preprint

arXiv:1203.3496  2012.

[30] Andr´as Pr´ekopa. Logarithmic concave measures and related topics. In Stochastic Programming  pages

63–82. Academic Press  1980.

[31] Mahlet G. Tadesse  Naijun Sha  and Marina Vannucci. Bayesian variable selection in clustering high-

dimensional data. Journal of the American Statistical Association  100(470):602–617  2005.

9

,Hossein Azari Soufiani
Hansheng Diao
Zhenyu Lai
David Parkes