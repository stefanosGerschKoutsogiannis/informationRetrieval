2011,On Strategy Stitching in Large Extensive Form Multiplayer Games,Computing a good strategy in a large extensive form game often demands an extraordinary amount of computer memory  necessitating the use of abstraction to reduce the game size.  Typically  strategies from abstract games perform better in the real game as the granularity of abstraction is increased.  This paper investigates two techniques for stitching a base strategy in a coarse abstraction of the full game tree  to expert strategies in fine abstractions of smaller subtrees.  We provide a general framework for creating static experts  an approach that generalizes some previous strategy stitching efforts.  In addition  we show that static experts can create strong agents for both 2-player and 3-player Leduc and Limit Texas Hold'em poker  and that a specific class of static experts can be preferred among a number of alternatives.  Furthermore  we describe a poker agent that used static experts and won the 3-player events of the 2010 Annual Computer Poker Competition.,On Strategy Stitching in Large Extensive Form

Multiplayer Games

Richard Gibson and Duane Szafron

Department of Computing Science  University of Alberta

Edmonton  Alberta  T6G 2E8  Canada

{rggibson | dszafron}@ualberta.ca

Abstract

Computing a good strategy in a large extensive form game often demands an ex-
traordinary amount of computer memory  necessitating the use of abstraction to
reduce the game size. Typically  strategies from abstract games perform better in
the real game as the granularity of abstraction is increased. This paper investi-
gates two techniques for stitching a base strategy in a coarse abstraction of the full
game tree  to expert strategies in ﬁne abstractions of smaller subtrees. We provide
a general framework for creating static experts  an approach that generalizes some
previous strategy stitching efforts. In addition  we show that static experts can cre-
ate strong agents for both 2-player and 3-player Leduc and Limit Texas Hold’em
poker  and that a speciﬁc class of static experts can be preferred among a number
of alternatives. Furthermore  we describe a poker agent that used static experts
and won the 3-player events of the 2010 Annual Computer Poker Competition.

1 Introduction

Many sequential decision-making problems are commonly modelled as an extensive form game.
Extensive games are very versatile due to their ability to represent multiple agents  imperfect infor-
mation  and stochastic events.

For many real-world problems  however  the extensive form game representation is too large to be
feasibly handled by current techniques. To address this limitation  strategies are often computed
in abstract versions of the game that group similar states together into single abstract states. For
very large games  these abstractions need to be quite coarse  leaving many different states indistin-
guishable. However  for smaller subtrees of the full game  strategies can be computed in much ﬁner
abstractions. Such “expert” strategies can then be pieced together  typically connecting to a “base
strategy” computed in the full coarsely-abstracted game. A disadvantage of this approach is that
we may make assumptions about the other agents’ strategies. In addition  by computing the base
strategy and the experts separately  we may lose “cohesion” among the different components.

We investigate stitched strategies in extensive form games  focusing on the trade-offs between the
sizes of the abstractions versus the assumptions made and the cohesion among the computed strate-
gies. We deﬁne two strategy stitching techniques: (i) static experts that are computed in very ﬁne
abstractions with varying degrees of assumptions and little cohesion  and (ii) dynamic experts that
are contained in abstractions with lower granularity  but make fewer assumptions and have perfect
cohesion. This paper generalizes previous strategy stitching efforts [1  2  11] under a more general
static expert framework. We use poker as a testbed to demonstrate that  despite recent mixed results 
static experts can create much stronger overall agents than the base strategy alone. Furthermore  we
show that under a ﬁxed memory limitation  a speciﬁc class of static experts are preferred to several

1

alternatives. As a ﬁnal validation of these results  we describe entries to the 2010 Annual Computer
Poker Competition1 (ACPC) that used static experts to win the 3-player events.

2 Background

An extensive form game [9] is a rooted directed tree  where nodes represent decision states  edges
represent actions  and terminal nodes hold end-game utility values for players. For each player  the
decision states are partitioned into information sets such that game states within an information set
are indistinguishable to the player. Non-singleton information sets arise due to hidden information
that is only available to a subset of the players  such as private cards in poker. More formally:

Deﬁnition 2.1 (Osborne and Rubenstein [9  p. 200]) A ﬁnite extensive game Γ with imperfect in-
formation has the following components:

• A ﬁnite set N of players.
• A ﬁnite set H of sequences  the possible histories of actions  such that the empty sequence is in
H and every preﬁx of a sequence in H is also in H. Z ⊆ H are the terminal histories (those
which are not a preﬁx of any other sequence). A(h) = {a | ha ∈ H} are the actions available
after a nonterminal history h ∈ H.

• A function P that assigns to each nonterminal history h ∈ H\Z a member of N ∪ {C}. P is
the player function. P (h) is the player who takes an action after the history h. If P (h) = C 
then chance determines the action taken after history h. Deﬁne Hi := {h ∈ H | P (h) = i}.

• A function fC that associates with every history h for which P (h) = C a probability measure
fC(·|h) on A(h) (fC(a|h) is the probability that a occurs given h)  where each such probability
measure is independent of every other such measure.

• For each player i ∈ N a partition Ii of Hi with the property that A(h) = A(h′) whenever h
and h′ are in the same member of the partition. For I ∈ Ii  we denote by A(I) the set of A(h)
and by P (I) the player P (h) for any h ∈ I. Ii is the information partition of player i; a set
I ∈ Ii is an information set of player i.

• For each player i ∈ N a utility function ui from the terminal histories Z to the real numbers
R. If N = {1  2} and u1 = −u2  it is a 2-player zero-sum extensive game. Deﬁne ∆u i :=
maxz ui(z) − minz ui(z) to be the range of the utilities for player i.

A strategy for player i  σi  is a function such that for each information set I ∈ Ii  σi(I) is a
probability distribution over A(I). Let Σi be the set of all strategies for player i. For h ∈ I  we
deﬁne σi(h) := σi(I). A strategy proﬁle σ consists of a strategy σi for each player i ∈ N . We let
σ−i refer to all the strategies in σ except σi  and denote ui(σ) to be the expected utility for player i
given that all players play according to σ.
In a 2-player zero-sum game  a best response to a player 1 strategy σ1 is a player 2 strategy
σBR
2 = argmaxσ2 u2(σ1  σ2) (similarly for a player 2 strategy σ2). The best response value of σ1
is u2(σ1  σBR
2 )  which measures the exploitability of σ1. The exploitability of a strategy tells us
how much that strategy loses to a worst-case opponent. Outside of 2-player zero-sum games  the
worst-case scenario for player i would be for all other players to minimize player i’s utility instead
of maximizing their own. In large games  this value is difﬁcult to compute since opponents cannot
share private information. Thus  we only investigate exploitability for 2-player zero-sum games.

Counterfactual regret minimization (CFR) [14] is an iterative procedure for computing strategy pro-
ﬁles in extensive form games. In 2-player zero-sum games  CFR produces an approximate Nash
equilibrium proﬁle. In addition  CFR strategies have also been found to compete very well in games
with more than 2 players [1]. CFR’s memory requirements are proportional to the number of infor-
mation sets in the game times the number of actions available at an information set.

The extensive form game representation of many real-world problems is too large to feasibly com-
pute a strategy directly. A common approach in these games is to ﬁrst create an abstract game by
combining information sets into single abstract states or by disallowing certain actions:

1http://www.computerpokercompetition.org

2





























































































































(a)

(b)

Figure 1: (a) An abstraction of an extensive game  where states connected by a bold curve are in the
same information set and thin curves denote merged abstract information sets. In the unabstracted
game  player 1 cannot distinguish between whether chance generated b or c and player 2 cannot
distinguish between a and b. In the abstract game  neither player can distinguish between any of
chance’s outcomes. (b) An example of a game Γ′ derived from the unabstracted game Γ in (a) for a
dynamic expert strategy. Here  the abstraction from (a) is used as the base abstraction  and the null
abstraction is employed on the subtree with G1 1 = ∅ and G2 1 = {al  bl  cl} (bold states).

Deﬁnition 2.2 (Waugh et al. [12]) An abstraction for player i is a pair αi = (cid:10)αI

is a partition of Hi deﬁning a set of abstract information sets coarser than Ii (i.e.  every

i (cid:11)  where

i   αA

• αI
i
I ∈ Ii is a subset of some set in αI

i )  and

is a function on histories where αA

• αA
i
h′ in the same abstract information set. We will call this the abstract action set.

i (h) ⊆ A(h) and αA

i (h) = αA

i (h′) for all histories h and

The null abstraction for player i is φi = hIi  Ai. An abstraction α is a set of abstractions αi 
one for each player. Finally  for any abstraction α  the abstract game  Γα  is the extensive game
obtained from Γ by replacing Ii with αI

i (h) when P (h) = i  for all i ∈ N .

i and A(h) with αA

Figure 1a shows an example of an abstracted extensive form game with no action abstraction. By
reducing the number of information sets  computing strategies in an abstract game with an algorithm
such as CFR requires less memory than computing strategies in the real game.
Intuitively  if a
strategy proﬁle for the abstract game σ performs well in Γα  and if αI
i is deﬁned such that merged
information sets are “strategically similar ” then σ is also likely to perform well in Γ. Identifying
strategically similar information sets can be delicate though and typically becomes a domain-speciﬁc
task. Nevertheless  we often would like to have as much granularity in our abstraction as will ﬁt in
memory to allow computed strategies to be as diverse as necessary.

3 Strategy Stitching

To achieve abstractions with ﬁner granularity  a natural approach is to break the game up into sub-
trees  abstract each of the subtrees  and compute a strategy for each abstract subtree independently.
We introduce a formalism for doing so that generalizes Waugh et al.’s strategy grafting [11] and two
poker-speciﬁc methods described in Section 5. First  select a subset S ⊆ N of players. Secondly 
for each i ∈ S  compute a base strategy σi for playing the full game. Next  divide the game into
subtrees:

Deﬁnition 3.1 (Waugh et al. [11]) Gi = {Gi 0  Gi 1  ...  Gi p} is a grafting partition for player i if

• Gi is a partition of Hi (possibly containing empty parts) 
• ∀I ∈ Ii  ∃j ∈ {0  1  ...  p} such that I ⊆ Gi j  and
• ∀j ∈ {1  2  ...  p}  h ∈ Gi j  and h′ ∈ Hi  if h is a preﬁx of h′  then h′ ∈ Gi j ∪ Gi 0.

For each i ∈ S  choose a grafting partition Gi so that each partition has an equal number of parts p.
Then  compute a strategy  or static expert  for each subtree using any strategy computation technique 
such as CFR. Finally  since the subtrees are disjoint  create a static expert strategy by combining the
static experts without any overlap to the base strategy in the undivided game:

3



























































































































































(a)

(b)

Figure 2: Two examples of a game Γj for a static expert derived from the unabstracted game Γ in
Figure 1a. In both (a) and (b)  G2 j = {al  bl  cl} (bold states). If player 1 takes action r  player 2
no longer controls his or her decisions. Player 2’s actions are instead generated by the base strategy
σ2  computed beforehand. In (a)  we have S = {2}. On the other hand  in (b)  S = N = {1  2} 
G1 j = ∅  and hence all of player 1’s actions are seeded by the base strategy σ1.

Deﬁnition 3.2 Let S ⊆ N be a nonempty subset of players. For each i ∈ S  let σi be a strategy for
player i and Gi = {Gi 0  Gi 1  ...  Gi p} be a grafting partition for player i. For j ∈ {1  2  ...  p} 
deﬁne Γj to be an extensive game derived from the original game Γ where  for all i ∈ S and
h ∈ Hi\Gi j  we set P (h) = C and fC (a|h) = σi(h  a). That is  each player i ∈ S only controls
actions for histories in Gi j and is forced to play according to σi elsewhere. Let the static expert of
{Gi j | i ∈ S}  σj  be a strategy proﬁle of the game Γj. Finally  deﬁne the static expert strategy for
player i  σS

i   as

σS

i (h  a) := (cid:26) σi(h  a)

σj
i (h  a)

if h ∈ Gi 0
if h ∈ Gi j.

We call {σi | i ∈ S} the base or seeding strategies and {Gi | i ∈ S} the grafting proﬁle for the
static expert strategy σS
i .

Figure 2 shows two examples of a game Γj for a single static expert. This may be the only subtree
for which a static expert is computed (p = 1)  or there could be more subtrees contained in the
grafting partition(s) (p > 1). Under a ﬁxed memory limitation  we can employ ﬁner abstractions for
the subtrees Γj than we can in the full game Γ. This is because Γj removes some of the information
sets belonging to players in S  freeing up memory for computing strategies on the subtrees.
When |S| = 1  the static expert approach is identical to strategy grafting [11  Deﬁnition 8]  with
the exception that each static expert need not be an approximate Nash equilibrium. We relax the
deﬁnition for static experts because Nash equilibria are difﬁcult to compute in multiplayer games 
and may not be the best solution concept outside of 2-player zero-sum games anyways. Choosing
|S| > 1  however  is dangerous because we ﬁx opponent probabilities and assume that our opponents
are “static” at certain locations. For example  in Figure 2b  it may not be wise for player 2 to assume
that player 1 must follow σ1. Doing so can dramatically skew player 2’s beliefs about the action
generated by chance and hurt the expert’s performance against opponents that do not follow σ1. As
we will see in Section 6  having more static experts with |S| > 1 can result in a more exploitable
static expert strategy. On the other hand  by removing information sets for multiple players  the static
expert approach creates smaller subtrees than strategy grafting does. As a result  we can employ even
ﬁner abstractions within the subtrees. Section 6 shows that despite the risks  the abstraction gains
often lead to static experts with S = N being preferred.
Regardless of the choice of S  the base strategy lacks “cohesion” with the static experts since its
computation is based on its own play at the subtrees rather than the experts’ play. Though the
experts are identically seeded  the base strategy may want to play towards the expert subtrees more
often to increase utility. This observation motivates our introduction of dynamic experts that are
computed concurrently with a base. The full extensive game is divided into subtrees and each
subtree is supplied its own abstraction:

4

Deﬁnition 3.3 Let α0  α1  ...  αp be abstractions for the game Γ and for each i ∈ N   let Gi =
{Gi 0  Gi 1  ...  Gi p} be a grafting partition for player i satisfying I ∩ Gi j ∈ {∅  I} for all j ∈
{0  ...  p} and I ∈ αj I
. Thus  each abstract information set is contained entirely in some part of
j=0{I ∈
(h) when P (h) = i and h ∈ Gi j  for all i ∈ N . Let the
i  be a strategy for player i of the game Γ′. Finally deﬁne the
i|Gi j . The abstraction α0

the grafting partition. Let Γ′ be the abstract game obtained from Γ by replacing Ii with Sp

i restricted to the histories in Gi j  σ′

i

i is denoted as the base strategy.

i

αj I
| I ⊆ Gi j} and A(h) with αj A
dynamic expert strategy for player i  σ′
dynamic expert of Gi j  σj
is denoted as the base abstraction and the dynamic expert σ0

i   to be σ′

i

Figure 1b contains an abstract game tree Γ′ for a dynamic expert strategy. We can view a dynamic
expert strategy as a strategy computed in an abstraction with differing granularity dependent on
the history of actions taken. Note that our deﬁnition is somewhat redundant to the deﬁnition of
abstraction as we are simply deﬁning a new abstraction for Γ based on the abstractions α0  α1  ...  αp.
Nonetheless  we supply Deﬁnition 3.3 to provide the terms in bold that we will use throughout.

Under memory constraints  a dynamic expert strategy typically sacriﬁces abstraction granularity
in the base strategy to achieve ﬁner granularity in the experts. We hope doing so achieves better
performance at parts of the game that we believe may be more important. For instance  importance
could depend on the predicted relative frequencies of reaching different subtrees.The base strategy’s
abstraction is reduced to guarantee perfect cohesion between the base and the experts; the base
strategy knows about the experts and can calculate its probabilities “dynamically” during strategy
computation based on the feedback from the experts. In Section 6  we contrast static and dynamic
experts to compare this trade-off between abstraction size and strategy cohesion.

4 Texas and Leduc Hold’em

A hand of Texas Hold’em poker (or simply Hold’em) begins with each player being dealt two private
cards  and two players posting mandatory bets or blinds. There are four betting rounds  the pre-ﬂop 
ﬂop  turn  and river where ﬁve community cards are successively revealed. Of the players that did
not fold  the player with the highest ranked poker hand wins all of the bets. Full rules can be found
on-line.2 We focus on the Limit Hold’em variant that ﬁxes the bet sizes and the number of bets
allowed per round. We denote the players’ actions as f (fold)  c (check or call)  and r (bet or raise).
Leduc Hold’em [10] (or simply Leduc) is a smaller version of Hold’em  played with a six card deck
consisting of two Jacks  two Queens  and two Kings with only two betting rounds  pre-ﬂop and ﬂop.
Rather than using blinds  antes are posted by all players at the beginning of a hand. Only one private
card is dealt to each player and one community card is dealt on the ﬂop.

While Leduc is small enough to bypass abstraction  Hold’em is a massive game in terms of the
number of information sets; 2-player Limit Hold’em has approximately 3 × 1014 information sets 
and 3-player has roughly 5 × 1017. Applying CFR to these enormous state spaces necessitates
abstraction. A common abstraction technique in poker is to group many different card dealings
into single abstract states or buckets. This is commonly done by ordering all possible poker hands
for a speciﬁc betting round according to some metric  such as expected hand strength (E[HS]) or
expected hand strength squared (E[HS2])  and then grouping hands with similar metric values into
the same bucket [7]. Percentile bucketing with N buckets and M hands puts the top M/N hands
into 1 bucket  the next best M/N into a second bucket  etc.  so that the buckets are approximately
equal in size. More advanced bucketing schemes that use multiple metrics and clustering techniques
are possible  but our experiments use simple percentile bucketing with no action abstraction.

5 Related Work

Our general framework for applying static experts to any extensive form game captures some previ-
ous poker-speciﬁc strategy stitching approaches. First  the PsOpti family of agents [2]  which play
2-player Limit Hold’em  contain a base strategy called the “pre-ﬂop model” and 7 static experts with
S = N   or “post-ﬂop models.” Due to resource and technology limitations  the abstractions used to

2http://en.wikipedia.org/wiki/Texas hold ’em

5

build the pre-ﬂop and post-ﬂop models were quite coarse  making the family no match for today’s
top agents. Secondly  Abou Risk and Szafron [1] attach 6 static experts with S = N (which they call
“heads-up experts”) to a base strategy for playing 3-player Limit Hold’em. Each expert focuses on
a subtree immediately following a fold action  allowing much ﬁner abstractions for these 2-player
scenarios. However  their results were mixed as the stitched strategy was not always better than the
base strategy alone. Nonetheless  our positive results for static experts with S = N in Section 6
provide evidence that the PsOpti approach and heads-up experts are indeed credible.

In addition  Gilpin and Sandholm [5] create a poker agent for 2-player Limit Hold’em that uses a
2-phase strategy different from the approaches discussed thus far. The ﬁrst phase is used to play the
pre-ﬂop and ﬂop rounds  and is computed similarly to the PsOpti pre-ﬂop model. For the turn and
river rounds  a second phase strategy is computed on-line. One drawback of this approach is that the
on-line computations must be quick enough to play in real time. Despite ﬁxing the ﬂop cards  this
constraint forced the authors to still employ a very coarse abstraction during the second phase.

Furthermore  there have been a few other related approaches to creating poker agents. While 2-
player poker is well studied  Ganzfried and Sandholm [3  4] developed algorithms for computing
Nash equilibria in multiplayer games and applied it to a small 3-player jam/fold poker game. Addi-
tionally  Gilpin et al. [6] use an automated abstraction building tool to dynamically bucket hands in
2-player Limit Hold’em. Here  we are not concerned with equilibrium properties or the abstraction
building process itself. In fact  strategy stitching is orthogonal to both strategy computation and
abstraction improvements  and could be used in conjunction with more sophisticated techniques.

6 Empirical Evaluation

In this section  we create several stitched strategies in both Leduc and Hold’em using the chance-
sampled variant of CFR [14]. CFR is state of the art in terms of memory efﬁciency for strategy
computation  allowing us to employ abstractions with higher granularity than otherwise possible.
Results may differ with other techniques for computing strategies and building abstractions. While
CFR requires iterations quadratic in the number of information sets to converge [14  Theorem 4] 
we restrict our resources only in terms of memory. Even though Leduc is small enough to not
necessitate strategy stitching  the Leduc experiments were conducted to evaluate our hypothesis that
static experts with S = N can improve play. We ran many experiments and for brevity  only a
representative sample of the results are summarized.

To be consistent with post-ﬂop models [2] and heads-up experts [1]  our grafting proﬁles are deﬁned
only in terms of the players’ actions. For each history h ∈ H  deﬁne b := b(h) to be the subsequence
of h obtained by removing all actions generated by chance. We refer to a b-expert for player i as an
expert constructed for the subtree Gi(b) := {h ∈ Hi | b is a preﬁx of b(h)} containing all histories
where the players initially follow b. For example  the experts for the games in Figures 1b  2a  and
2b are l-experts because the game is split after player 1 takes action l.

Leduc. Our Leduc experiments use three different base abstractions  one of which is simply the
null abstraction. The second and third abstractions are the “JQ-K” and “J-QK” abstractions that  on
the pre-ﬂop  cannot distinguish between whether the private card is a Jack or Queen  or whether the
private card is a Queen or King respectively. In addition  these two abstractions can only distinguish
between whether the ﬂop card pairs with the private card or not rather than knowing the identity of
the ﬂop card. Because Leduc is such a small game  we do not consider a ﬁxed memory restriction
and instead just compare the techniques within the same base abstraction.

For both 2-player and 3-player  for each of the three base abstractions  and for each player i  we
build a base strategy  a dynamic expert strategy  an S = {i} static expert strategy  and two S = N
static expert strategies. Recall choosing S = {i} means that during computation of each static
expert  we only ﬁx player i’s action probabilities outside of the expert subtree  whereas S = N
means that we ﬁx all players outside of the subtree. For 2-player Leduc  we use r  cr  ccr  and cccr-
experts for both players. Thus  the base strategy plays until the ﬁrst raise occurs  at which point
an expert takes over for the remainder of the hand. As an exception  only one of our two S = N
static expert strategies  named “All ” uses all four experts; the other  named “Pre-ﬂop ” just uses the
r and cr-experts. For 3-player Leduc  we use r  cr  ccr  cccr  ccccr  and cccccr-experts  except the
“Pre-ﬂop” static strategies use just the three experts r  cr  and ccr. The null abstraction is employed

6

Table 1: The size  earnings  and exploitability of the 2-player (2p) Leduc strategies in the JQ-K base
abstraction  and the size and earnings of the 3-player (3p) strategies in the J-QK base abstraction.
The sizes are measured in terms of the maximum number of information sets present within a single
CFR computation. Earnings  as described in the text  and exploitability are in milli-antes per hand.

Strategy (2p)

Base

Dynamic

Static.S={i}
Static.S=N .All

Static.S=N .Pre-ﬂop

Size Earns. Exploit.
496.31
132
159.84
444
167.61
226
186
432.74
214.44
186

24.73
45.75
28.87
29.20
37.77

Strategy (3p)

Base

Dynamic

Static.S={i}
Static.S=N .All

Static.S=N .Pre-ﬂop

Size
1890
6903
3017
2145
2145

Earns.
-68.46
113.04
96.14
117.01
119.73

on every expert subtree. Each run of CFR is stopped after 100 million iterations  which for 2-player
yields strategies within a milli-ante of equilibrium in the abstract game.

Each strategy is evaluated against all combinations and orderings of opponent strategies where all
strategies use different base abstractions  and the scores are averaged together. For example  for
each of our 2-player strategy proﬁles σ in the JQ-K base abstraction  we compute 1/2(u1(σ1  σ′
2) +
1  σ2))  averaged over all proﬁles σ′ that use either the null or J-QK base abstraction. Leduc is
u2(σ′
a small enough game that the utilities can be computed exactly. A selection of these scores  along
with 2-player exploitability values  are reported in Table 1.

Firstly  by increasing abstraction granularity  all of the JQ-K strategies employing experts earn
more than the base strategy alone. Secondly  Dynamic and Static.S=N earn more overall than
Static.S={i}  despite the 2-player Static.S=N being more exploitable due to the opponent action
assumptions. In fact  despite requiring much less memory to compute  Static.S=N surprisingly
earns more than Dynamic in 3-player Leduc. Finally  we see that only using two pre-ﬂop static
experts as opposed to all four reduces the number of dangerous assumptions to provide a stronger
and less exploitable strategy. However  as expected  Dynamic and Static.S={i} are less exploitable.

Hold’em. Our Hold’em experiments enforce a ﬁxed memory restriction per run of CFR  which
we artiﬁcially set to 24 million information sets for 2-player and 162 million information sets for
3-player. We compute stitched strategies of each type using as many percentile E[HS2] buckets as
possible within the restriction. Our 2-player abstractions distribute buckets as close to uniformly
as possible across the betting rounds while remembering buckets from previous rounds (known as
“perfect recall”). Our 3-player abstractions are similar  except they use 169 pre-ﬂop buckets that are
forgotten on later rounds (known as “imperfect recall;” see [1] and [13] for more regarding CFR and
imperfect recall).

For 2-player  our dynamic strategy has just an r-expert  our S = {i} static strategy uses r  cr  ccr 
and cccr-experts  and our S = N static strategy employs r and cr-experts. These choices were
based on preliminary experiments to make the most effective use of the limited memory available
for each stitching approach. Following Abou Risk and Szafron [1]  our 3-player stitched strategies
all have f   rf   rrf   and rcf -experts as these appear to be the most commonly reached 2-player
scenarios [1  Table 4]. Our abstractions range quite dramatically in terms of number of buckets. For
example  in 3-player  our dynamic strategy’s base abstraction has just 8 river buckets with 7290 river
buckets for each expert  whereas our static strategies have 16 river buckets in the base abstraction
with up to 194 481 river buckets for the S = N static rcf -expert abstraction. For reference  all of
the 2-player base and experts are built from 720 million iterations of CFR  while we run CFR for
100 million and 5 billion iterations for the 3-player base and experts respectively.

We evaluate our 2-player strategies by playing 500 000 duplicate hands (players play both sides of
the dealt cards) of poker between each pair of strategies. In addition to our base and stitched strate-
gies  we also included a base strategy called “Base.797M” in an abstraction with over 797 million
information sets that we expected to beat all of the strategies we were evaluating. Furthermore  using
a specialized best response tool [8]  we computed the exploitability of our 2-player strategies. For
3-player  we play 500 000 triplicate hands (each set of dealt cards played 6 times  one for each of the
player seatings) between each combination of 3 strategies. We also included two other strategies:
“ACPC-09 ” the 2009 ACPC 3-player event winner that did not use experts (Abou Risk and Szafron
[1] call it “IR16”)  and “ACPC-10 ” a static expert strategy that won a 3-player event at the 2010
ACPC and is outlined at the end of this section. The results are provided in Table 2.

7

Table 2: Earnings and 95% conﬁdence intervals over 500 000 duplicate hands of 2-player Hold’em
per pairing  and over 500 000 triplicate hands of 3-player Hold’em per combination. The exploitabil-
ity of the 2-player strategies is also provided. All values are in milli-big-blinds per hand.
Earnings

Exploitability

Strategy (2p)

Earnings

Strategy (3p)

Base

Dynamic

−10.47 ± 1.99
−4.43 ± 1.98
Static.S={i} −13.13 ± 2.00
Static.S=N
−4.57 ± 1.95
Base.797M
32.59 ± 2.14

310.04
307.76
301.00
288.82
135.43

Base

Dynamic

Static.S={i}
Static.S=N
ACPC-09
ACPC-10

−6.09 ± 0.71
−4.91 ± 0.75
−5.20 ± 0.70
3.06 ± 0.70
−14.15 ± 0.89
27.29 ± 0.86

Firstly  in 2-player  we see that Static.S=N and Dynamic outperform Static.S={i} considerably 
agreeing with the previous Leduc results. In fact  the Static.S={i} fails to even improve upon the
base strategy. For 3-player  Static.S=N is noticeably ahead of both Dynamic and Static.S={i} as it
is the only strategy  aside from ACPC-10  to win money. By forcing one player to fold  the static
experts with S = N essentially reduce the size of the game tree from a 3-player to a 2-player
game  allowing many more buckets to be used. This result indicates that at least for poker  the
gains in abstraction bucketing outweigh the risks of forced action assumptions and lack of cohesion
between the base strategy and the experts. Furthermore  Static.S=N is slightly less exploitable in
2-player than the base strategy and the other two stitched strategies. While there are one and two
opponent static actions assumed by the r and cr-experts respectively  trading these few assumptions
for an increase in abstraction granularity is beneﬁcial. In summary  static experts with S = N are
preferred to both dynamic and static experts with S = {i} in the experiments we ran.
An additional validation of the quality of the static expert approach was provided by the 2010 ACPC.
The winning entries in both 3-player events employed static experts with S = N . The base strategy 
computed from 70 million iterations of CFR  used 169  900  100  and 25 buckets on each of the
respective rounds. Four experts were used  f   rf   rrf   and rcf   computed from 10 billion iterations
of CFR  each containing 169  60 000  180 000  and 26 160 buckets on the respective rounds. In
addition  clustering techniques on strength distribution were used instead of percentile bucketing.
Two strategies were created  where one was trained to play slightly more aggressively for the total
bankroll event. Each version ﬁnished in ﬁrst place in its respective competition.

7 Conclusions

We discussed two strategy stitching techniques for extensive games  including static experts that
generalize strategy grafting and some previous techniques used in poker. Despite the accompanying
potential dangers and lack of cohesion  we have shown static experts with S = N outperform the
dynamic and static experts with S = {i} that we considered  especially when memory limitations are
present. However  additional static experts with several forced actions can lead to a more exploitable
strategy. Static experts with S = N is currently our preferred method for creating multiplayer poker
strategies and would be our ﬁrst option for playing other large extensive games.

Future work includes ﬁnding a way to create more cohesion between the base strategy and static
experts. One possibility is to rebuild the base strategy after the experts have been created so that the
base strategy’s play is more uniﬁed with the experts. In addition  we have yet to experiment with 3-
player “hybrid” static experts where |S| = 2. Finally  there are many ways to combine the stitching
techniques described in this paper. One possibility is to use a dynamic expert strategy as a base
strategy of a static expert strategy. In addition  static experts could themselves be dynamic expert
strategies for the appropriate subtrees. Such combinations may produce even stronger strategies than
those produced in this paper.

Acknowledgments

We would like to thank Westgrid and Compute Canada for their computing resources that were used
during this work. We would also like to thank the members of the Computer Poker Research Group
at the University of Alberta for their helpful pointers throughout this project. This research was
funded by NSERC and Alberta Ingenuity  now part of Alberta Innovates - Technology Futures.

8

References

[1] N. Abou Risk and D. Szafron. Using counterfactual regret minimization to create competitive

multiplayer poker agents. In AAMAS  pages 159–166  2010.

[2] D. Billings  N. Burch  A. Davidson  R. Holte  J. Schaeffer  T. Schauenberg  and D. Szafron.
Approximating game-theoretic optimal strategies for full-scale poker. In IJCAI  pages 661–
668  2003.

[3] S. Ganzfried and T. Sandholm. Computing an approximate jam/fold equilibrium for 3-agent

no-limit Texas Hold’em tournaments. In AAMAS  2008.

[4] S. Ganzfried and T. Sandholm. Computing equilibria in multiplayer stochastic games of im-

perfect information. In IJCAI  2009.

[5] A. Gilpin and T. Sandholm. Better automated abstraction techniques for imperfect information

games  with application to Texas Hold’em poker. In AAMAS  2007.

[6] A. Gilpin  T. Sandholm  and T.B. Sørensen. Potential-aware automated abstraction of sequen-

tial games  and holistic equilibrium analysis of Texas Hold’em poker. In AAAI  2007.

[7] M. Johanson. Robust strategies and counter-strategies: Building a champion level computer

poker player. Master’s thesis  University of Alberta  2007.

[8] M. Johanson  K. Waugh  M. Bowling  and M. Zinkevich. Accelerating best response calcula-

tion in large extensive games. In IJCAI  2011. To appear.

[9] M. Osborne and A. Rubenstein. A Course in Game Theory. The MIT Press  Cambridge 

Massachusetts  1994.

[10] F. Southey  M. Bowling  B. Larson  C. Piccione  N. Burch  D. Billings  and C. Rayner. Bayes’

bluff: Opponent modelling in poker. In UAI  pages 550–558  2005.

[11] K. Waugh  M. Bowling  and N. Bard. Strategy grafting in extensive games. In NIPS-22  pages

2026–2034  2009.

[12] K. Waugh  D. Schnizlein  M. Bowling  and D. Szafron. Abstraction pathologies in extensive

games. In SARA  pages 781–788  2009.

[13] Kevin Waugh  Martin Zinkevich  Michael Johanson  Morgan Kan  David Schnizlein  and

Michael Bowling. A practical use of imperfect recall. In SARA  pages 175–182  2009.

[14] M. Zinkevich  M. Johanson  M. Bowling  and C. Piccione. Regret minimization in games with

incomplete information. In NIPS-20  pages 905–912  2008.

9

,Maren Mahsereci
Philipp Hennig
Zequn Jie
Xiaodan Liang
Jiashi Feng
Xiaojie Jin
Wen Lu
Shuicheng Yan
El Mahdi El Mhamdi
Hadrien Hendrikx
Alexandre Maurer
Yixi Xu
Xiao Wang
Dinghuai Zhang
Tianyuan Zhang
Yiping Lu
Zhanxing Zhu
Bin Dong