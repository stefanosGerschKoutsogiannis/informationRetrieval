2018,Learning with SGD and Random Features,Sketching and stochastic gradient methods are arguably the most common  techniques to derive efficient large scale learning algorithms. In this paper  we investigate their application in the context of nonparametric statistical learning. More precisely  we study the estimator defined by stochastic gradient with mini batches and   random features. The latter can be seen as form of nonlinear sketching and  used to define approximate kernel methods. The considered estimator is not explicitly penalized/constrained and regularization is implicit. Indeed  our study highlights how different parameters  such as number of features  iterations  step-size and mini-batch size control the learning properties of the solutions. We do this by deriving optimal finite sample bounds  under standard  assumptions. The obtained results are corroborated and illustrated by numerical experiments.,Learning with SGD and Random Features

Luigi Carratino⇤
University of Genoa 

Genoa  Italy

Alessandro Rudi

INRIA – Sierra Project-team 

École Normale Supérieure  Paris

Lorenzo Rosasco
University of Genoa 
LCSL – IIT & MIT

Abstract

Sketching and stochastic gradient methods are arguably the most common tech-
niques to derive efﬁcient large scale learning algorithms. In this paper  we inves-
tigate their application in the context of nonparametric statistical learning. More
precisely  we study the estimator deﬁned by stochastic gradient with mini batches
and random features. The latter can be seen as form of nonlinear sketching and used
to deﬁne approximate kernel methods. The considered estimator is not explicitly
penalized/constrained and regularization is implicit. Indeed  our study highlights
how different parameters  such as number of features  iterations  step-size and
mini-batch size control the learning properties of the solutions. We do this by
deriving optimal ﬁnite sample bounds  under standard assumptions. The obtained
results are corroborated and illustrated by numerical experiments.

1

Introduction

The interplay between statistical and computational performances is key for modern machine learning
algorithms [1]. On the one hand  the ultimate goal is to achieve the best possible prediction error. On
the other hand  budgeted computational resources need be factored in  while designing algorithms.
Indeed  time and especially memory requirements are unavoidable constraints  especially in large-
scale problems.
In this view  stochastic gradient methods [2] and sketching techniques [3] have emerged as funda-
mental algorithmic tools. Stochastic gradient methods allow to process data points individually  or
in small batches  keeping good convergence rates  while reducing computational complexity [4].
Sketching techniques allow to reduce data-dimensionality  hence memory requirements  by random
projections [3]. Combining the beneﬁts of both methods is tempting and indeed it has attracted much
attention  see [5] and references therein.
In this paper  we investigate these ideas for nonparametric learning. Within a least squares frame-
work  we consider an estimator deﬁned by mini-batched stochastic gradients and random features
[6]. The latter are typically deﬁned by nonlinear sketching: random projections followed by a
component-wise nonlinearity [3]. They can be seen as shallow networks with random weights [7] 
but also as approximate kernel methods [8]. Indeed  random features provide a standard approach
to overcome the memory bottleneck that prevents large-scale applications of kernel methods. The
theory of reproducing kernel Hilbert spaces [9] provides a rigorous mathematical framework to study
the properties of stochastic gradient method with random features. The approach we consider is not
based on penalizations or explicit constraints; regularization is implicit and controlled by different
parameters. In particular  our analysis shows how the number of random features  iterations  step-size
and mini-batch size control the stability and learning properties of the solution. By deriving ﬁnite
sample bounds  we investigate how optimal learning rates can be achieved with different parameter
choices. In particular  we show that similarly to ridge regression [10]  a number of random features
proportional to the square root of the number of samples sufﬁces for O(1/pn) error bounds.

⇤Email: luigi.carratino@dibris.unige.it

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

The rest of the paper is organized as follows. We introduce problem  background and the proposed
algorithm in section 2. We present our main results in section 3 and illustrate numerical experiments
in section 4.
Notation: For any T 2 N+ we denote by [T ] the set {1  . . .   T}  for any a  b 2 R we de-
note by a _ b the maximum between a and b and with ^ the minimum. For any linear operator
A and  2 R we denote by A the operator (A + I) if not explicitly deﬁned differently. When
A is a bounded self-adjoint linear operator on a Hilbert space  we denote by max(A) the biggest
eigenvalue of A.

2 Learning with Stochastic Gradients and Random Features

f E(f ) 
min

In this section  we present the setting and discuss the learning algorithm we consider.
The problem we study is supervised statistical learning with squared loss [11]. Given a probability
space X ⇥ R with distribution ⇢ the problem is to solve

E(f ) =Z (f (x)  y)2d⇢(x  y) 
i 2 (X ⇥ R)n  n 2 N  sampled independently according to
given only a training set of pairs (xi  yi)n
⇢. Here the minimum is intended over all functions for which the above integral is well deﬁned and ⇢
is assumed ﬁxed but known only through the samples.
In practice  the search for a solution needs to be restricted to a suitable space of hypothesis to allow
efﬁcient computations and reliable estimation [12]. In this paper  we consider functions of the form
(2)
where w 2 RM and M : X ! RM  M 2 N  denotes a family of ﬁnite dimensional feature maps 
see below. Further  we consider a mini-batch stochastic gradient method to estimate the coefﬁcients
from data 

f (x) = hw  M (x)i 

8x 2 X 

(1)

1
b

t = 1  . . .   T. (3)

bwt+1 = bwt  t

btXi=b(t1)+1hbwt  M (xji)i yjiM (xji) 

bw1 = 0;
Here T 2 N is the number of iterations and J = {j1  . . .   jbT} denotes the strategy to select training
set points. In particular  in this work we assume the points to be drawn uniformly at random with
replacement. Note that given this sampling strategy  one pass over the data is reached on average
after d n
b e iterations. Our analysis allows to consider multiple as well as single passes. For b = 1 the
above algorithm reduces to a simple stochastic gradient iteration. For b > 1 it is a mini-batch version 
where b points are used in each iteration to compute a gradient estimate. The parameter t is the
step-size.
The algorithm requires specifying different parameters. In the following  we study how their choice
is related and can be performed to achieve optimal learning bounds. Before doing this  we further
discuss the class of feature maps we consider.

2.1 From Sketching to Random Features  from Shallow Nets to Kernels
In this paper  we are interested in a particular class of feature maps  namely random features [6]. A
simple example is obtained by sketching the input data. Assume X ✓ RD and

M (x) = (hx  s1i  . . .  hx  sMi)  

where s1  . . .   sM 2 RD is a set of identical and independent random vectors [13]. More generally 
we can consider features obtained by nonlinear sketching
(4)
M (x) = ((hx  s1i)  . . .   (hx  sMi))  
where  : R ! R is a nonlinear function  for example (a) = cos(a) [6]  (a) = |a|+ = max(a  0) 
a 2 R [7]. If we write the corresponding function (2) explicitly  we get
8x 2 X.

wj(hsj  xi) 

f (x) =

(5)

MXj=1

2

that is as shallow neural nets with random weights [7] (offsets can be added easily).
For many examples of random features the inner product 

hM (x)  M (x0)i =

(hx  sji)(hx0  sji) 

(6)

MXj=1

can be shown to converge to a corresponding positive deﬁnite kernel k as M tends to inﬁnity [6  14].
We now show some examples of kernels determined by speciﬁc choices of random features.
Example 1 (Random features and kernel). Let (a) = cos(a) and consider (hx  si + b) in place of
the inner product hx  si  with s drawn from a standard Gaussian distribution with variance 2  and b
uniformly from [0  2⇡]. These are the so called Fourier random features and recover the Gaussian
kernel k(x  x0) = ekxx0k2/22 [6] as M increases. If instead (a) = a  and the s is sampled
according to a standard Gaussian the linear kernel k(x  x0) = 2hx  x0i is recovered in the limit.
[15].

These last observations allow to establish a connection with kernel methods [10] and the theory of
reproducing kernel Hilbert spaces [9]. Recall that a reproducing kernel Hilbert space H is a Hilbert
space of functions for which there is a symmetric positive deﬁnite function2 k : X ⇥ X ! R called
reproducing kernel  such that k(x ·) 2H and hf  k(x ·)i = f (x) for all f 2H   x 2 X. It is also
useful to recall that k is a reproducing kernel if and only if there exists a Hilbert (feature) space F
and a (feature) map  : X !F such that
(7)

k(x  x0) = h(x)  (x0)i 

8x  x0 2 X 

where F can be inﬁnite dimensional.
The connection to RKHS is interesting in at least two ways. First  it allows to use results and
techniques from the theory of RKHS to analyze random features. Second  it shows that random
features can be seen as an approach to derive scalable kernel methods [10]. Indeed  kernel methods
have complexity at least quadratic in the number of points  while random features have complexity
which is typically linear in the number of points. From this point of view  the intuition behind random
features is to relax (7) considering

where M is ﬁnite dimensional.

k(x  x0) ⇡ hM (x)  M (x0)i 

8x  x0 2 X.

(8)

2.2 Computational complexity
If we assume the computation of the feature map M (x) to have a constant cost  the iteration (3)
requires O(M ) operations per iteration for b = 1  that is O(M n) for one pass T = n. Note that for
b > 1 each iteration cost O(M b) but one pass corresponds to d n
b e iterations so that the cost for one
pass is again O(M n). A main advantage of mini-batching is that gradient computations can be easily
parallelized. In the multiple pass case  the time complexity after T iterations is O(M bT ).
Computing the feature map M (x) requires to compute M random features. The computation of
one random feature does not depend on n  but only on the input space X. If for example we assume
X ✓ RD and consider random features deﬁned as in the previous section  computing M (x) requires
M random projections of D dimensional vectors [6]  for a total time complexity of O(M D) for
evaluating the feature map at one point. For different input spaces and different types of random
features computational cost may differ  see for example Orthogonal Random Features [16] or Fastfood
[17] where the cost is reduced from O(M D) to O(M log D). Note that the analysis presented in
his paper holds for random features which are independent  while Orthogonal and Fastfood random
features are dependent. Although it should be possible to extend our analysis for Orthogonal and
Fastfood random features  further work is needed. To simplify the discussion  in the following we
treat the complexity of M (x) to be O(M ).
One of the advantages of random features is that each M (x) can be computed online at each iteration 
preserving O(M bT ) as the time complexity of the algorithm (3). Computing M (x) online also
reduces memory requirements. Indeed the space complexity of the algorithm (3) is O(M b) if the
mini-batches are computed in parallel  or O(M ) if computed sequentially.

2For all x1  . . .   xn the matrix with entries k(xi  xj)  i  j = 1  . . .   n is positive semi-deﬁnite.

3

2.3 Related approaches

We comment on the connection to related algorithms. Random features are typically used within an
empirical risk minimization framework [18]. Results considering convex Lipschitz loss functions and
`1 constraints are given in [19]  while [20] considers `2 constraints. A ridge regression framework is
considered in [8]  where it is shown that it is possible to achieve optimal statistical guarantees with a
number of random features in the order of pn. The combination of random features and gradient
methods is less explored. A stochastic coordinate descent approach is considered in [21]  see also
[22  23]. A related approach is based on subsampling and is often called Nyström method [24  25].
Here a shallow network is deﬁned considering a nonlinearity which is a positive deﬁnite kernel  and
weights chosen as a subset of training set points. This idea can be used within a penalized empirical
risk minimization framework [26  27  28] but also considering gradient [29  30] and stochastic
gradient [31] techniques. An empirical comparison between Nyström method  random features and
full kernel method is given in [23]  where the empirical risk minimization problem is solved by block
coordinate descent. Note that numerous works have combined stochastic gradient and kernel methods
with no random projections approximation [32  33  34  35  36  5]. The above list of references is only
partial and focusing on papers providing theoretical analysis. In the following  after stating our main
results we provide a further quantitative comparison with related results.

3 Main Results

In this section  we ﬁrst discuss our main results under basic assumptions and then more reﬁned results
under further conditions.

3.1 Worst case results

Our results apply to a general class of random features described by the following assumption.
Assumption 1. Let (⌦ ⇡ ) be a probability space  : X ⇥ ⌦ ! R and for all x 2 X 

M (x) =

1
pM

( (x  !1)  . . .   (x  !M ))  

(9)

where !1  . . .  ! M 2 ⌦ are sampled independently according to ⇡.
The above class of random features cover all the examples described in section 2.1  as well as many
others  see [8  20] and references therein. Next we introduce the positive deﬁnite kernel deﬁned by
the above random features. Let k : X ⇥ X ! R be deﬁned by
k(x  x0) =Z (x  !) (x0 ! )d⇡(!) 

8  x  x0 2 X.

It is easy to check that k is a symmetric and positive deﬁnite kernel. To control basic properties of
the induced kernel (continuity  boundedness)  we require the following assumption  which is again
satisﬁed by the examples described in section 2.1 (see also [8  20] and references therein).
Assumption 2. The function is continuous and there exists   1 such that | (x  !)|  for any
x 2 X  ! 2 ⌦.
The kernel introduced above allows to compare random feature maps of different size and to express
the regularity of the largest function class they induce. In particular  we require a standard assumption
in the context of non-parametric regression (see [11])  which consists in assuming a minimum for the
expected risk  over the space of functions induced by the kernel.
Assumption 3. If H is the RKHS with kernel k  there exists fH 2H such that

E(fH) = inf

f2HE(f ).

To conclude  we need some basic assumption on the data distribution. For all x 2 X  we denote by
⇢(y|x) the conditional probability of ⇢ and by ⇢X the corresponding marginal probability on X. We
need a standard moment assumption to derive probabilistic results.

4

Assumption 4. For any x 2 X

ZY

y2ld⇢(y|x)  l!Blp 

8l 2 N

(10)

for costants B 2 (0 1) and p 2 (1 1)  ⇢X-almost surely.
The above assumption holds when y is bounded  sub-gaussian or sub-exponential.
The next theorem corresponds to our ﬁrst main result. Recall that  the excess risk for a given estimator

E(bf ) E (fH) 

and is a standard error measure in statistical machine learning [11  18]. In the following theorem  we
control the excess risk of the estimator with respect to the number of points  the number of RF  the

bf is deﬁned as
step size  the mini-batch size and the number of iterations. We let bft+1 = hbwt+1  M (·)i  with bwt+1

as in (3).
Theorem 1. Let n  M 2 N+   2 (0  1) and t 2 [T ]. Under Assumptions 1 to 4  for b 2 [n]  t = 
 and M & T the following holds with probability at
s.t.  
least 1  :

8(1+log T )  n  32 log2 2

 ^

9T log n

n

1

EJ⇥E(bft+1)⇤E(fH) . 

b

+✓ t

M

+ 1◆ t log 1

n



+

log 1

M

+

1
t

.

(11)

The above theorem bounds the excess risk with a sum of terms controlled by the different parameters.
The following corollary shows how these parameters can be chosen to derive ﬁnite sample bounds.
Corollary 1. Under the same assumptions of Theorem 1  for one of the following conditions
(c1.1). b = 1   ' 1
(c1.2). b = 1   ' 1pn  and T = n iterations (1 pass over the data);
(c1.3). b = pn   ' 1  and T = pn iterations (1 pass over the data);
(c1.4). b = n   ' 1  and T = pn iterations (pn passes over the data);

n  and T = npn iterations (pn passes over the data);

a number

of random features is sufﬁcient to guarantee with high probability that

(12)

(13)

M = eO(pn)

EJ⇥E(bfT )⇤ E (fH) . 1

pn

.

The above learning rate is the same achieved by an exact kernel ridge regression (KRR) estimator
[11  37  38]  which has been proved to be optimal in a minimax sense [11] under the same assumptions.
Further  the number of random features required to achieve this bound is the same as the kernel ridge
regression estimator with random features [8]. Notice that  for the limit case where the number of
random features grows to inﬁnity for Corollary 1 under conditions (c1.2) and (c1.3) we recover the
same results for one pass SGD of [39]  [40]. In this limit  our results are also related to those in [41].
Here  however  averaging of the iterates is used to achieve larger step-sizes.
Note that conditions (c1.1) and (c1.2) in the corollary above show that  when no mini-batches are
used (b = 1) and 1
n    1pn  then the step-size  determines the number of passes over the data
required for optimal generalization. In particular the number of passes varies from constant  when
 = 1pn  to pn  when  = 1
n. In order to increase the step-size over 1pn the algorithm needs to
be run with mini-batches. The step-size can then be increased up to a constant if b is chosen equal
to pn (condition (c1.3))  requiring the same number of passes over the data of the setting (c1.2).
Interestingly condition (c1.4) shows that increasing the mini-batch size over pn does not allow to
take larger step-sizes  while it seems to increase the number of passes over the data required to reach
optimality.
We now compare the time complexity of algorithm (3) with some closely related methods which

5

achieve the same optimal rate of 1pn. Computing the classical KRR estimator [11] has a complexity
of roughly O(n3) in time and O(n2) in memory. Lowering this computational cost is possible with
random projection techniques. Both random features and Nyström method on KRR [8  26] lower
the time complexity to O(n2) and the memory complexity to O(npn) preserving the statistical
accuracy. The same time complexity is achieved by stochastic gradient method solving the full kernel
method [33  36]  but with the higher space complexity of O(n2). The combination of the stochastic
gradient iteration  random features and mini-batches allows our algorithm to achieve a complexity
of O(npn) in time and O(n) in space for certain choices of the free parameters (like (c1.2) and
(c1.3)). Note that these time and memory complexity are lower with respect to those of stochastic
gradient with mini-batches and Nyström approximation which are O(n2) and O(n) respectively [31].
A method with similar complexity to SGD with RF is FALKON [30]. This method has indeed a time
complexity of O(npn log(n)) and O(n) space complexity. This method blends together Nyström
approximation  a sketched preconditioner and conjugate gradient.

3.2 Reﬁned analysis and fast rates
We next discuss how the above results can be reﬁned under an additional regularity assumption.
We need some preliminary deﬁnitions. Let H be the RKHS deﬁned by k  and L : L2(X  ⇢X) !
L2(X  ⇢X) the integral operator

Lf (x) =Z k(x  x0)f (x0)d⇢X(x0) 

8f 2 L2(X  ⇢X)  x 2 X 

⇢ =R |f|2d⇢X < 1}. The above operator is symmetric
where L2(X  ⇢X) = {f : X ! R : kfk2
and positive deﬁnite. Moreover  Assumption 1 ensures that the kernel is bounded  which in turn
ensures L is trace class  hence compact [18].
Assumption 5. For any > 0  deﬁne the effective dimension as N () = Tr((L + I)1L)  and
assume there exist Q > 0 and ↵ 2 [0  1] such that
(14)

N ()  Q2↵.

Moreover   assume there exists r  1/2 and g 2 L2(X  ⇢X) such that

fH(x) = (Lrg)(x).

(15)
Condition (14) describes the capacity/complexity of the RKHS H and the measure ⇢. It is equivalent
to classic entropy/covering number conditions  see e.g. [18]. The case ↵ = 1 corresponds to making
no assumptions on the kernel  and reduces to the worst case analysis in the previous section. The
smaller is ↵ the more stringent is the capacity condition. A classic example is considering X = RD
with d⇢X(x) = p(x)dx  where p is a probability density  strictly positive and bounded away from
zero  and H to be a Sobolev space with smoothness s > D/2. Indeed  in this case ↵ = D/2s and
classical nonparametric statistics assumptions are recovered as a special case. Note that in particular
the worst case is s = D/2. Condition (15) is a regularity condition commonly used in approximation
theory to control the bias of the estimator [42].
The following theorem is a reﬁned version of Theorem 1 where we also consider the above capacity
condition (Assumption 5).
Theorem 2. Let n  M 2 N+   2 (0  1) and t 2 [T ]  under Assumptions 1 to 4  for b 2 [n]  t = 
 and M & T the following holds with high probability:
s.t.  
+✓ 1
t◆2r

+ 1◆ N⇣ 1

8(1+log T )  n  32 log2 2

t⌘ log 1

+ N⇣ 1

t⌘2r1

M (t)2r1

+✓ t

M

log 1


EJ⇥E(bft+1)⇤ E (fH) . 

b

.
(16)

n

9T log n

 ^

1



n

The main difference is the presence of the effective dimension providing a sharper control of the
stability of the considered estimator. As before  explicit learning bounds can be derived considering
different parameter settings.
Corollary 2. Under the same assumptions of Theorem 2  for one of the following conditions
(c2.1). b = 1   ' n1  and T = n

2r+↵ passes over the data);

2r+↵ iterations (n

2r+↵+1

1

6

(17)

(19)

(20)

(21)

(22)

(23)

moreover

 

1

7

n

8(1+log T )

9T 1✓ log n

 ^( ✓^(1✓)
M 4 + 18T 1✓ log
btmin(✓ 1✓) (log t _ 1)



✓ 2]0  1[
otherwise 

12T 1✓

 



2r

2r+↵   and T = n
2r+↵    ' 1  and T = n

(c2.2). b = 1   ' n 2r
(c2.3). b = n
(c2.4). b = n   ' 1  and T = n
a number

1

2r+1

2r+↵ iterations (n

1↵
2r+↵ passes over the data);

1

2r+↵ iterations (n

1↵
2r+↵ passes over the data);

2r+↵ iterations (n

1

2r+↵ passes over the data);

1+↵(2r1)

2r+↵

)

M = eO(n

of random features sufﬁes to guarantee with high probability that
2r+↵ .

EJ⇥E(bwT )⇤ E (fH) . n 2r

(18)
The corollary above shows that multi-pass SGD achieves a learning rate that is the same as kernel
ridge regression under the regularity assumption 5 and is again minimax optimal (see [11]). Moreover 
we obtain the minimax optimal rate with the same number of random features required for ridge
regression with random features [8] under the same assumptions. Finally  when the number of random
features goes to inﬁnity we also recover the results for the inﬁnite dimensional case of the single-pass
and multiple pass stochastic gradient method [33].
It is worth noting that  under the additional regularity assumption 5  the number of both random
features and passes over the data sufﬁcient for optimal learning rates increase with respect to the one
required in the worst case (see Corollary 1). The same effect occurs in the context of ridge regression
with random features as noted in [8]. In this latter paper  it is observed that this issue tackled can be
using more reﬁned  possibly more costly  sampling schemes [20].
Finally  we present a general result from which all our previous results follow as special cases. We
consider a more general setting where we allow decreasing step-sizes.

Theorem 3. Let n  M  T 2 N  b 2 [n] and > 0. Let  2 (0  1) and bwt+1 be the estimator in

Eq. (3) with t = 2t✓ and ✓ 2 [0  1[. Under Assumptions 1 to 4  when n  32 log2 2

 and

EJ⇥E(bwt+1)⇤ inf

then  for any t 2 [T ] the following holds with probability at least 1  9
 t1✓ _ 1◆ N⇣ 2
t1✓⌘

1
M
t1✓ )2r1 log 2

w2F E(w)  c1
+✓c2 + c3
+ c4 N ( 2

M (t1✓2)2r1 log22r11t1✓ +✓ 1

log

M

n



log2(t) _ 1 log2 4
t1✓◆2r!  



with c1  c2  c3  c4 constants which do not depend on b    n  t  M  .
We note that as the number of random features M goes to inﬁnity  we recover the same bound of [33]
for decreasing step-sizes. Moreover  the above theorem shows that there is no apparent gain in using
a decreasing stepsize (i.e. ✓> 0) with respect to the regimes identiﬁed in Corollaries 1 and 2.

3.3 Sketch of the Proof

intermediate functions. In particular  the following iterations are useful 

In this section  we sketch the main ideas in the proof. We relate bft and fH introducing several
bv1 = 0;
ev1 = 0;

nXi=1hbvt  M (xi)i  yiM (xi) 
bvt+1 =bvt  t
evt+1 =evt  tZXhevt  M (x)i  yM (x)d⇢(x  y) 
vt+1 = vt  tZXhvt  M (x)i  fH(x)M (x)d⇢X(x) 

8t 2 [T ].

8t 2 [T ].

8t 2 [T ].

v1 = 0;

(24)

(25)

(26)

1
n

7

SUSY

HIGGS

r
o
r
r
e

 

n
o

r
o
r
r
e

 

n
o

i
t

a
c
i
f
i
s
s
a
c

l

i
t

a
c
i
f
i
s
s
a
c

l

n° of random features

n° of random features

Figure 1: Classiﬁcation error of SUSY (left) and HIGGS (right) datasets as the no of random features varies

Further  we let

eu = argmin

u = argmin

u2RM ZXhu  M (x)i  fH(x)2d⇢X(x) + kuk2 >
u2F ZXhu  (x)i  y2d⇢(x  y) + kuk2 >

0 

0 

(27)

(28)

where (F  ) are feature space and feature map associated to the kernel k. The ﬁrst three vectors are
deﬁned by the random features and can be seen as an empirical and population batch gradient descent
iterations. The last two vectors can be seen as a population version of ridge regression deﬁned by the
random features and the feature map   respectively.
Since the above objects (24)  (25)  (26)  (27)  (28) belong to different spaces  instead of comparing
them directly we compare the functions in L2(X  ⇢X) associated to them  letting

bgt = hbvt  M (·)i   egt = hevt  M (·)i   gt = hvt  M (·)i   eg = heu  M (·)i   g = hu  (·)i .

Since it is well known [11] that

we than can consider the following decomposition

E(f ) E (fH) = kf  fHk2
⇢ 

bft  fH = bft bgt
+bgt egt
+egt  gt
+ gt eg
+eg  g

(29)
(30)
(31)
(32)
(33)
(34)
The ﬁrst two terms control how SGD deviates from the batch gradient descent and the effect of noise
and sampling. They are studied in Lemma 1  2  3  4 5  6 in the Appendix  borrowing and adapting
ideas from [33  36  8]. The following terms account for the approximation properties of random
features and the bias of the algorithm. Here the basic idea and novel result is the study of how the
population gradient decent and ridge regression are related (32) (Lemma 9 in the Appendix). Then 
results from the the analysis of ridge regression with random features are used [8].

+ g  fH.

4 Experiments
We study the behavior of the SGD with RF algorithm on subsets of n = 2 ⇥ 105 points of the
SUSY 3 and HIGGS 4 datasets [43]. The measures we show in the following experiments are an
average over 10 repetitions of the algorithm. Further  we consider random Fourier features that

3https://archive.ics.uci.edu/ml/datasets/SUSY
4https://archive.ics.uci.edu/ml/datasets/HIGGS

8

SUSY - Classification Error

HIGGS - Classification Error

e
z
s
 

i

h
c
t

a
b
-
i
n
m

i

i

e
z
s
 
h
c
t
a
b
-
i
n
m

i

step-size

step-size

Figure 2: Classiﬁcation error of SUSY (left) and HIGGS (right) datasets as step-seize and mini-batch size vary

are known to approximate translation invariant kernels [6]. We use random features of the form
 (x  !) = cos(wT x + q)  with ! := (w  q)  w sampled according to the normal distribution and q
sampled uniformly at random between 0 and 2⇡. Note that the random features deﬁned this way
satisfy Assumption 2.
Our theoretical analysis suggests that only a number of RF of the order of pn sufﬁces to gain optimal
learning properties. Hence we study how the number of RF affect the accuracy of the algorithm on
test sets of 105 points. In Figure 3.3 we show the classiﬁcation error after 5 passes over the data of
SGD with RF as the number of RF increases  with a ﬁxed batch size of pn and a step-size of 1. We
can observe that over a certain threshold of the order of pn  increasing the number of RF does not
improve the accuracy  conﬁrming what our theoretical results suggest.
Further  theory suggests that the step-size can be increased as the mini-batch size increases to reach
an optimal accuracy  and that after a mini-batch size of the order of pn more than 1 pass over the
data is required to reach the same accuracy. We show in Figure 2 the classiﬁcation error of SGD with
RF after 1 pass over the data  with a ﬁxed number of random features pn  as mini-batch size and
step-size vary  on test sets of 105 points. As suggested by theory  to reach the lowest error as the
mini-batch size grows the step-size needs to grow as well. Further for mini-batch sizes bigger that
pn the lowest error can not be reached in only 1 pass even if increasing the step-size.

5 Conclusions

In this paper we investigate the combination of sketching and stochastic techniques in the context of
non-parametric regression. In particular we studied the statistical and computational properties of
the estimator deﬁned by stochastic gradient descent with multiple passes  mini-batches and random
features. We proved that the estimator achieves optimal statistical properties with a number of
random features in the order of pn (with n the number of examples). Moreover we analyzed possible
trade-offs between the number of passes  the step and the dimension of the mini-batches showing
that there exist different conﬁgurations which achieve the same optimal statistical guarantees  with
different computational impacts.
Our work can be extended in several ways: First  (a) we can study the effect of combining random
features with accelerated/averaged stochastic techniques as [32]. Second  (b) we can extend our
analysis to consider more reﬁned assumptions  generalizing [35] to SGD with random features.
Additionally  (c) we can study the statistical properties of the considered estimator in the context of
classiﬁcation with the goal of showing fast decay of the classiﬁcation error  as in [34]. Moreover 
(d) we can apply the proposed method in the more general context of least squares frameworks for
multitask learning [44  45] or structured prediction [46  47  48]  with the goal of obtaining faster
algorithms  while retaining strong statistical guarantees. Finally  (e) to integrate our analysis with
more reﬁned methods to select the random features analogously to [49  50] in the context of column
sampling.

9

Acknowledgments.
This material is based upon work supported by the Center for Brains  Minds and Machines (CBMM)  funded by
NSF STC award CCF-1231216  and the Italian Institute of Technology. We gratefully acknowledge the support
of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research.
L. R. acknowledges the support of the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007
(European Ofﬁce of Aerospace Research and Development)  and the EU H2020-MSCA-RISE project NoMADS
- DLV-777826. A. R. acknowledges the support of the European Research Council (grant SEQUOIA 724063).

References
[1] Alekh Agarwal  Sahand Negahban  and Martin J Wainwright. Stochastic optimization and
sparse statistical recovery: Optimal algorithms for high dimensions. In Advances in Neural
Information Processing Systems  pages 1538–1546  2012.

[2] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of

mathematical statistics  pages 400–407  1951.

[3] Haim Avron  Vikas Sindhwani  and David Woodruff. Sketching structured matrices for faster
nonlinear regression. In Advances in neural information processing systems  pages 2994–3002 
2013.

[4] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural

information processing systems  pages 161–168  2008.

[5] Francesco Orabona. Simultaneous model selection and optimization through parameter-free
stochastic learning. In Advances in Neural Information Processing Systems  pages 1116–1124 
2014.

[6] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances

in neural information processing systems  pages 1177–1184  2008.

[7] Youngmin Cho and Lawrence K Saul. Kernel methods for deep learning. In Advances in neural

information processing systems  pages 342–350  2009.

[8] Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random
features. In Advances in Neural Information Processing Systems 30  pages 3215–3225. 2017.
[9] Nachman Aronszajn. Theory of reproducing kernels. Transactions of the American mathemati-

cal society  68(3):337–404  1950.

[10] Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines 

regularization  optimization  and beyond. 2002.

[11] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares

algorithm. Foundations of Computational Mathematics  7(3):331–368  2007.

[12] Luc Devroye  László Györﬁ  and Gábor Lugosi. A probabilistic theory of pattern recognition 

volume 31. Springer Science & Business Media  2013.

[13] David P Woodruff et al. Sketching as a tool for numerical linear algebra. Foundations and

Trends in Theoretical Computer Science  10(1–2):1–157  2014.

[14] Bharath Sriperumbudur and Zoltán Szabó. Optimal rates for random fourier features.

Advances in Neural Information Processing Systems  pages 1144–1152  2015.

In

[15] Raffay Hamid  Ying Xiao  Alex Gittens  and Dennis DeCoste. Compact random feature maps.

In International Conference on Machine Learning  pages 19–27  2014.

[16] X Yu Felix  Ananda Theertha Suresh  Krzysztof M Choromanski  Daniel N Holtmann-Rice 
and Sanjiv Kumar. Orthogonal random features. In Advances in Neural Information Processing
Systems  pages 1975–1983  2016.

[17] Quoc Le  Tamás Sarlós  and Alex Smola. Fastfood-approximating kernel expansions in loglinear
time. In Proceedings of the international conference on machine learning  volume 85  2013.

10

[18] Ingo Steinwart and Andreas Christmann. Support vector machines. Springer Science & Business

Media  2008.

[19] Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimiza-
tion with randomization in learning. In Advances in neural information processing systems 
pages 1313–1320  2009.

[20] Francis Bach. On the equivalence between kernel quadrature rules and random feature expan-

sions. Journal of Machine Learning Research  18(21):1–38  2017.

[21] Bo Dai  Bo Xie  Niao He  Yingyu Liang  Anant Raj  Maria-Florina F Balcan  and Le Song.
Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information
Processing Systems  pages 3041–3049  2014.

[22] Junhong Lin and Lorenzo Rosasco. Generalization properties of doubly online learning algo-

rithms. arXiv preprint arXiv:1707.00577  2017.

[23] Stephen Tu  Rebecca Roelofs  Shivaram Venkataraman  and Benjamin Recht. Large scale kernel

learning using block coordinate descent. arXiv preprint arXiv:1602.05310  2016.

[24] Alex J Smola and Bernhard Schölkopf. Sparse greedy matrix approximation for machine

learning. 2000.

[25] Christopher KI Williams and Matthias Seeger. Using the nyström method to speed up kernel

machines. In Advances in neural information processing systems  pages 682–688  2001.

[26] Alessandro Rudi  Raffaello Camoriano  and Lorenzo Rosasco. Less is more: Nyström computa-
tional regularization. In Advances in Neural Information Processing Systems  pages 1657–1665 
2015.

[27] Yun Yang  Mert Pilanci  and Martin J Wainwright. Randomized sketches for kernels: Fast and

optimal non-parametric regression. arXiv preprint arXiv:1501.06195  2015.

[28] Ahmed Alaoui and Michael W Mahoney. Fast randomized kernel ridge regression with statistical

guarantees. In Advances in Neural Information Processing Systems  pages 775–783  2015.

[29] Raffaello Camoriano  Tomás Angles  Alessandro Rudi  and Lorenzo Rosasco. Nytro: When
subsampling meets early stopping. In Artiﬁcial Intelligence and Statistics  pages 1403–1411 
2016.

[30] Alessandro Rudi  Luigi Carratino  and Lorenzo Rosasco. FALKON: An optimal large scale
kernel method. In Advances in Neural Information Processing Systems  pages 3891–3901 
2017.

[31] Junhong Lin and Lorenzo Rosasco. Optimal rates for learning with nyström stochastic gradient

methods. arXiv preprint arXiv:1710.07797  2017.

[32] Aymeric Dieuleveut  Nicolas Flammarion  and Francis Bach. Harder  better  faster  stronger
convergence rates for least-squares regression. The Journal of Machine Learning Research 
18(1):3520–3570  2017.

[33] Junhong Lin and Lorenzo Rosasco. Optimal rates for multi-pass stochastic gradient methods.

Journal of Machine Learning Research  18(97):1–47  2017.

[34] Loucas Pillaud-Vivien  Alessandro Rudi  and Francis Bach. Exponential convergence of testing
error for stochastic gradient methods. In Proceedings of the 31st Conference On Learning
Theory  volume 75  pages 250–296  2018.

[35] Loucas Pillaud-Vivien  Alessandro Rudi  and Francis Bach. Statistical optimality of stochastic
gradient descent on hard learning problems through multiple passes. In S. Bengio  H. Wallach 
H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural
Information Processing Systems 31  pages 8125–8135. Curran Associates  Inc.  2018.

[36] Lorenzo Rosasco and Silvia Villa. Learning with incremental iterative regularization.

Advances in Neural Information Processing Systems  pages 1630–1638  2015.

In

11

[37] Ingo Steinwart  Don R Hush  Clint Scovel  et al. Optimal rates for regularized least squares

regression. In COLT  2009.

[38] Junhong Lin  Alessandro Rudi  Lorenzo Rosasco  and Volkan Cevher. Optimal rates for spectral
algorithms with least-squares regression over hilbert spaces. Applied and Computational
Harmonic Analysis  2018.

[39] Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization:
Convergence results and optimal averaging schemes. In International Conference on Machine
Learning  pages 71–79  2013.

[40] Ofer Dekel  Ran Gilad-Bachrach  Ohad Shamir  and Lin Xiao. Optimal distributed online
prediction using mini-batches. Journal of Machine Learning Research  13(Jan):165–202  2012.
[41] Aymeric Dieuleveut  Francis Bach  et al. Nonparametric stochastic approximation with large

step-sizes. The Annals of Statistics  44(4):1363–1399  2016.

[42] Steve Smale and Ding-Xuan Zhou. Estimating the approximation error in learning theory.

Analysis and Applications  1(01):17–41  2003.

[43] Pierre Baldi  Peter Sadowski  and Daniel Whiteson. Searching for exotic particles in high-energy

physics with deep learning. Nature communications  5:4308  2014.

[44] Andreas Argyriou  Theodoros Evgeniou  and Massimiliano Pontil. Convex multi-task feature

learning. Machine Learning  73(3):243–272  2008.

[45] Carlo Ciliberto  Alessandro Rudi  Lorenzo Rosasco  and Massimiliano Pontil. Consistent multi-
task learning with nonlinear output relations. In Advances in Neural Information Processing
Systems  pages 1986–1996  2017.

[46] Carlo Ciliberto  Lorenzo Rosasco  and Alessandro Rudi. A consistent regularization approach
for structured prediction. Advances in Neural Information Processing Systems 29 (NIPS)  pages
4412–4420  2016.

[47] Anton Osokin  Francis Bach  and Simon Lacoste-Julien. On structured prediction theory with
calibrated convex surrogate losses. In Advances in Neural Information Processing Systems 
pages 302–313  2017.

[48] Carlo Ciliberto  Francis Bach  and Alessandro Rudi. Localized structured prediction. arXiv

preprint arXiv:1806.02402  2018.

[49] Petros Drineas  Malik Magdon-Ismail  Michael W Mahoney  and David P Woodruff. Fast
approximation of matrix coherence and statistical leverage. Journal of Machine Learning
Research  13(Dec):3475–3506  2012.

[50] Alessandro Rudi  Daniele Calandriello  Luigi Carratino  and Lorenzo Rosasco. On fast leverage
score sampling and optimal learning. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman 
N. Cesa-Bianchi  and R. Garnett  editors  Advances in Neural Information Processing Systems
31  pages 5677–5687. Curran Associates  Inc.  2018.

[51] Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the

American mathematical society  39(1):1–49  2002.

[52] Ernesto De Vito  Lorenzo Rosasco  Andrea Caponnetto  Umberto De Giovannini  and Francesca
Odone. Learning from examples as an inverse problem. Journal of Machine Learning Research 
6(May):883–904  2005.

[53] Alessandro Rudi  Guillermo D Canas  and Lorenzo Rosasco. On the sample complexity of
subspace learning. In Advances in Neural Information Processing Systems  pages 2067–2075 
2013.

12

,Luigi Carratino
Alessandro Rudi
Lorenzo Rosasco