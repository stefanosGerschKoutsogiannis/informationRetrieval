2013,Convex Two-Layer Modeling,Latent variable prediction models  such as multi-layer networks  impose auxiliary latent variables between inputs and outputs to allow automatic inference of implicit features useful for prediction.  Unfortunately  such models are difficult to train because inference over latent variables must be performed concurrently with parameter optimization---creating a highly non-convex problem.  Instead of proposing another local training method  we develop a convex relaxation of hidden-layer conditional models that admits global training.  Our approach extends current convex modeling approaches to handle two nested nonlinearities separated by a non-trivial adaptive latent layer.  The resulting methods are able to acquire two-layer models that cannot be represented by any single-layer model over the same features  while improving training quality over local heuristics.,Convex Two-Layer Modeling

¨Ozlem Aslan
Dale Schuurmans
Department of Computing Science  University of Alberta

Hao Cheng

Edmonton  AB T6G 2E8  Canada

{ozlem hcheng2 dale}@cs.ualberta.ca

Xinhua Zhang

Machine Learning Research Group
National ICT Australia and ANU
xinhua.zhang@anu.edu.au

Abstract

Latent variable prediction models  such as multi-layer networks  impose auxil-
iary latent variables between inputs and outputs to allow automatic inference of
implicit features useful for prediction. Unfortunately  such models are difﬁcult
to train because inference over latent variables must be performed concurrently
with parameter optimization—creating a highly non-convex problem.
Instead
of proposing another local training method  we develop a convex relaxation of
hidden-layer conditional models that admits global training. Our approach ex-
tends current convex modeling approaches to handle two nested nonlinearities
separated by a non-trivial adaptive latent layer. The resulting methods are able
to acquire two-layer models that cannot be represented by any single-layer model
over the same features  while improving training quality over local heuristics.

1

Introduction

Deep learning has recently been enjoying a resurgence [1  2] due to the discovery that stage-wise
pre-training can signiﬁcantly improve the results of classical training methods [3–5]. The advan-
tage of latent variable models is that they allow abstract “semantic” features of observed data to be
represented  which can enhance the ability to capture predictive relationships between observed vari-
ables. In this way  latent variable models can greatly simplify the description of otherwise complex
relationships between observed variates. For example  in unsupervised (i.e.  “generative”) settings 
latent variable models have been used to express feature discovery problems such as dimensionality
reduction [6]  clustering [7]  sparse coding [8]  and independent components analysis [9]. More
recently  such latent variable models have been used to discover abstract features of visual data
invariant to low level transformations [1  2  4]. These learned representations not only facilitate
understanding  they can enhance subsequent learning.
Our primary focus in this paper  however  is on conditional modeling. In a supervised (i.e. “condi-
tional”) setting  latent variable models are used to discover intervening feature representations that
allow more accurate reconstruction of outputs from inputs. One advantage in the supervised case
is that output information can be used to better identify relevant features to be inferred. However 
latent variables also cause difﬁculty in this case because they impose nested nonlinearities between
the input and output variables. Some important examples of conditional latent learning approaches
include those that seek an intervening lower dimensional representation [10] latent clustering [11] 
sparse feature representation [8] or invariant latent representation [1  3  4  12] between inputs and
outputs. Despite their growing success  the difﬁculty of training a latent variable model remains
clear: since the model parameters have to be trained concurrently with inference over latent vari-
ables  the convexity of the training problem is usually destroyed. Only highly restricted models can
be trained to optimality  and current deep learning strategies provide no guarantees about solution
quality. This remains true even when restricting attention to a single stage of stage-wise pre-training:
simple models such as the two-layer auto-encoder or restricted Boltzmann machine (RBM) still pose
intractable training problems  even within a single stage (in fact  simply computing the gradient of
the RBM objective is currently believed to be intractable [13]).

1

Meanwhile  a growing body of research has investigated reformulations of latent variable learn-
ing that are able to yield tractable global training methods in special cases. Even though global
training formulations are not a universally accepted goal of deep learning research [14]  there are
several useful methodologies that have been been applied successfully to other latent variable mod-
els: boosting strategies [15–17]  semideﬁnite relaxations [18–20]  matrix factorization [21–23]  and
moment based estimators (i.e. “spectral methods”) [24  25]. Unfortunately  none of these approaches
has yet been able to accommodate a non-trivial hidden layer between an input and output layer while
retaining the representational capacity of an auto-encoder or RBM (e.g. boosting strategies embed
an intractable subproblem in these cases [15–17]). Some recent work has been able to capture re-
stricted forms of latent structure in a conditional model—namely  a single latent cluster variable
[18–20]—but this remains a rather limited approach.
In this paper we demonstrate that more general latent variable structures can be accommodated
within a tractable convex framework. In particular  we show how two-layer latent conditional models
with a single latent layer can be expressed equivalently in terms of a latent feature kernel. This
reformulation allows a rich set of latent feature representations to be captured  while allowing useful
convex relaxations in terms of a semideﬁnite optimization. Unlike [26]  the latent kernel in this
model is explicitly learned (nonparametrically). To cope with scaling issues we further develop
an efﬁcient algorithmic approach for the proposed relaxation. Importantly  the resulting method
preserves sufﬁcient problem structure to recover prediction models that cannot be represented by any
one-layer architecture over the same input features  while improving the quality of local training.
2 Two-Layer Conditional Modeling
We address the problem of training a two-layer latent
conditional model in the form of Figure 1; i.e.  where
there is a single layer of h latent variables    between
a layer of n input variables  x  and m output variables 
y. The goal is to predict an output vector y given an
input vector x. Here  a prediction model consists of
the composition of two nonlinear conditional models 
f1(W x) ;  and f2(V ) ; ˆy  parameterized by the
matrices W 2 Rh⇥n and V 2 Rm⇥h. Once the param-
eters W and V have been speciﬁed  this architecture
deﬁnes a point predictor that can determine ˆy from x
by ﬁrst computing an intermediate representation .
To learn the model parameters  we assume we are given
t training pairs {(xj  yj)}t
j=1  stacked in two matrices
X = (x1  ...  xt) 2 Rn⇥t and Y = (y1  ...  yt) 2
Rm⇥t  but the corresponding set of latent variable val-
ues = ( 1  ...  t) 2 Rh⇥t remains unobserved.
To formulate the training problem  we will consider two losses  L1 and L2  that relate the input
to the latent layer  and the latent to the output layer respectively. For example  one can think of
losses as negative log-likelihoods in a conditional model that generates each successive layer given
its predecessor; i.e.  L1(W x  ) =  log pW (|x) and L2(V   y) =  log pV (y|). (However 
a loss based formulation is more ﬂexible  since every negative log-likelihood is a loss but not vice
versa.) Similarly to RBMs and probabilistic networks (PFNs) [27] (but unlike auto-encoders and
classical feed-forward networks)  we will not assume  is a deterministic output of the ﬁrst layer;
instead we will consider  to be a variable whose value is the subject of inference during training.
Given such a set-up many training principles become possible. For simplicity  we consider a Viterbi
based training principle where the parameters W and V are optimized with respect to an optimal
imputation of the latent values . To do so  deﬁne the ﬁrst and second layer training objectives as

Figure 1:
Latent conditional model
f1(W x) ;   f2(V ) ; ˆy  where j is
a latent variable  xj is an observed input
vector  yj is an observed output vector 
W are ﬁrst layer parameters  and V are
second layer parameters.

j

xj

yj

W

f1

V

f2

t

2 kWk2
F  

F1(W  ) = L1(W X  ) + ↵

and F2(  V ) = L2(V   Y ) + 

(1)
where we assume the losses are convex in their ﬁrst arguments. Here it is typical to assume
j=1 L1( ˆ j  j) and L2(Z  Y ) =
j=1 L2(ˆzj  yj)  where ˆ j is the jth column of ˆ and ˆzj is the jth column of ˆZ respectively. This

that the losses decompose columnwise; that is  L1( ˆ   ) = Pt
Pt

2kV k2
F  

2

follows for example if the training pairs (xj  yj) are assumed I.I.D.  but such a restriction is not nec-
essary. Note that we have also introduced Euclidean regularization over the parameters (i.e. negative
log-priors under a Gaussian)  which will provide a useful representer theorem [28] we exploit later.
These two objectives can be combined to obtain the following joint training problem:

min
W V

min



F1(W  ) + F2(  V ) 

(2)

where > 0 is a trade off parameter that balances the ﬁrst versus second layer discrepancy. Unfor-
tunately (2) is not jointly convex in the unknowns W   V and .
A key modeling question concerns the structure of the latent representation . As noted  the ex-
tensive literature on latent variable modeling has proposed a variety of forms for latent structure.
Here  we follow work on deep learning and sparse coding and assume that the latent variables are
boolean   2{ 0  1}h⇥1; an assumption that is also often made in auto-encoders [13]  PFNs [27] 
and RBMs [5]. A boolean representation can capture structures that range from a single latent clus-
tering [11  19  20]  by imposing the assumption that 01 = 1  to a general sparse code  by imposing
the assumption that 01 = k for some small k [1  4  13].1 Observe that  in the latter case  one
can control the complexity of the latent representation by imposing a constraint on the number of
“active” variables k rather than directly controlling the latent dimensionality h.

2.1 Multi-Layer Perceptrons and Large-Margin Losses

To complete a speciﬁcation of the two-layer model in Figure 1 and the associated training problem
(2)  we need to commit to speciﬁc forms for the transfer functions f1 and f2 and the losses in (1). For
simplicity  we will adopt a large-margin approach over two-layer perceptrons. Although it has been
traditional in deep learning research to focus on exponential family conditional models (e.g. as in
auto-encoders  PFNs and RBMs)  these are not the only possibility; a large-margin approach offers
additional sparsity and algorithmic simpliﬁcations that will clarify the development below. Despite
its simplicity  such an approach will still be sufﬁcient to prove our main point.
First  consider the second layer model. We will conduct our primary evaluations on multi-
class classiﬁcation problems  where output vectors y encode target classes by indicator vectors
y 2{ 0  1}m⇥1 such that y01 = 1. Although it is common to adopt a softmax transfer for f2 in
such a case  it is also useful to consider a perceptron model deﬁned by f2(ˆz) = indmax(ˆz) such that
indmax(ˆz) = 1i (vector of all 0s except a 1 in the ith position) where ˆzi  ˆzl for all l. Therefore 
for multi-class classiﬁcation  we will simply adopt the standard large-margin multi-class loss [29]:
(3)
Intuitively  if yc = 1 is the correct label  this loss encourages the response ˆzc = y0ˆz on the correct
label to be a margin greater than the response ˆzi on any other label i 6= c.
Second  consider the ﬁrst layer model. Although the loss (3) has proved to be highly successful for
multi-class classiﬁcation problems  it is not suitable for the ﬁrst layer because it assumes there is
only a single target component active in any latent vector ; i.e. 01 = 1. Although some work
has considered learning a latent clustering in a two-layer architecture [11  18–20]  such an approach
is not able to capture the latent sparse code of a classical PFN or RBM in a reasonable way: using
clustering to simulate a multi-dimensional sparse code causes exponential blow-up in the number of
latent classes required. Therefore  we instead adopt a multi-label perceptron model for the ﬁrst layer 
deﬁned by the transfer function f1( ˆ ) = step( ˆ ) applied componentwise to the response vector ˆ ;
i.e. step( ˆ i) = 1 if ˆ i > 0  0 otherwise. Here again  instead of using a traditional negative log-
likelihood loss  we will adopt a simple large-margin loss for multi-label classiﬁcation that naturally
accommodates multiple binary latent classiﬁcations in parallel. Although several loss formulations
exist for multi-label classiﬁcation [30  31]  we adopt the following:

L2(ˆz  y) = max(1  y + ˆz  1y0ˆz).

L1( ˆ   ) = max(1   + ˆ 01  10 ˆ ) ⌘ max(1  )/(01) + ˆ  10 ˆ /(01). (4)

Intuitively  this loss encourages the average response on the active labels  0 ˆ /(01)  to exceed the
response ˆ i on any inactive label i  i = 0  by some margin  while also encouraging the response on
any active label to match the average of the active responses. Despite their simplicity  large-margin
multi-label losses have proved to be highly successful in practice [30  31]. Therefore  the overall
architecture we investigate embeds two nonlinear conditionals around a non-trivial latent layer.

1 Throughout this paper we let 1 denote the vector of all 1s with length determined by context.

3

3 Equivalent Reformulation

The main contribution of this paper is to show that the training problem (2) has a convex relaxation
that preserves sufﬁcient structure to transcend one-layer models. To demonstrate this relaxation  we
ﬁrst need to establish the key observation that problem (2) can be re-expressed in terms of a kernel
matrix between latent representation vectors. Importantly  this reformulation allows the problem to
be re-expressed in terms of an optimization objective that is jointly convex in all participating vari-
ables. We establish this key intermediate result in this section in three steps: ﬁrst  by re-expressing
the latent representation in terms of a latent kernel; second  by reformulating the second layer ob-
jective; and third  by reformulating the ﬁrst layer objective by exploiting large-margin formulation
outlined in Section 2.1. Below let K = X0X denote the kernel matrix over the input data  let Im(N )
denote the row space of N  and let and † denote Moore-Penrose pseudo-inverse.
First  simply deﬁne N = 0. Next  re-express the second layer objective F2 in (1) by the following.
Lemma 1. For any ﬁxed   letting N = 0  it follows that

min
V

F2(  V ) =

min

B2Im(N )

L2(B  Y ) + 

2 tr(BN†B0).

(5)

Proof. The result follows from the following sequence of equivalence preserving transformations:

min
V

L2(V   Y ) + 

2kV k2

F = min
A
min

=

B2Im(N )

L2(AN  Y ) + 

2 tr(AN A0)

L2(B  Y ) + 

2 tr(BN†B0) 

(6)

(7)

where  starting with the deﬁnition of F2 in (1)  the ﬁrst equality in (6) follows from the representer
theorem applied to kV k2
F   which implies that the optimal V must be in the form of V = A0 for
some A 2 Rm⇥t [28]; and ﬁnally  (7) follows by the change of variable B = AN.
Note that Lemma 1 holds for any loss L2. In fact  the result follows solely from the structure of the
regularizer. However  we require L2 to be convex in its ﬁrst argument to ensure a convex problem
below. Convexity is indeed satisﬁed by the choice (3). Moreover  the term tr(BN†B0) is jointly
convex in N and B since it is a perspective function [32]  hence the objective in (5) is jointly convex.
Next  we reformulate the ﬁrst layer objective F1 in (1). Since this transformation exploits speciﬁc
structure in the ﬁrst layer loss  we present the result in two parts: ﬁrst  by showing how the de-
sired outcome follows from a general assumption on L1  then demonstrating that this assumption
is satisﬁed by the speciﬁc large-margin multi-label loss deﬁned in (4). To establish this result we
will exploit the following augmented forms for the data and variables: let ˜= [   kI]  ˜N = ˜0 ˜ 
˜ = [ ˆ   0]  ˜X = [X  0]  ˜K = ˜X0 ˜X  and ˜t = t + h.
Lemma 2. For any L1 if there exists a function ˜L1 such that L1( ˆ   ) = ˜L1( ˜0 ˜   ˜0 ˜) for all
ˆ 2 Rh⇥t and  2{ 0  1}h⇥t  such that 01 = 1k  it then follows that

min
W

F1(W  ) =

min

D2Im( ˜N )

˜L1(D ˜K  ˜N ) + ↵

2 tr(D0 ˜N†D ˜K).

(8)

(9)

(10)

(11)

Proof. Similar to above  consider the sequence of equivalence preserving transformations:

L1(W X  ) + ↵

min
W

2 kWk2

F = min
W
= min

˜L1( ˜0W ˜X  ˜0 ˜) + ↵
˜L1( ˜0 ˜C ˜X0 ˜X  ˜0 ˜) + 

2 kWk2

F

2 tr( ˜XC0 ˜0 ˜C ˜X0)

=

C
min

D2Im( ˜N )

˜L1(D ˜K  ˜N ) + ↵

2 tr(D0 ˜N†D ˜K) 

where  starting with the deﬁnition of F1 in (1)  the ﬁrst equality (9) simply follows from the as-
sumption. The second equality (10) follows from the representer theorem applied to kWk2
F   which
implies that the optimal W must be in the form of W = ˜C ˜X0 for some C 2 R˜t⇥˜t (using the fact
that ˜ has full rank h) [28]. Finally  (11) follows by the change of variable D = ˜N C.

4

Observe that the term tr(D0 ˜N†D ˜K) is again jointly convex in ˜N and D (also a perspective func-
tion)  while it is easy to verify that ˜L1(D ˜K  ˜N ) as deﬁned in Lemma 3 below is also jointly convex
in ˜N and D [32]; therefore the objective in (8) is jointly convex.
Next  we show that the assumption of Lemma 2 is satisﬁed by the speciﬁc large-margin multi-label
formulation in Section 2.1; that is  assume L1 is given by the large-margin multi-label loss (4):

ˆ j

L1( ˆ   ) = Pj max1  j + ˆ j0j1  10j

= ⌧110  + ˆ diag(01)  1 diag(0 ˆ )0  such that ⌧ (⇥) :=Pj max(✓j)  (12)
where we use ˆ j  j and ✓j to denote the jth columns of ˆ    and ⇥ respectively.
Lemma 3. For the multi-label loss L1 deﬁned in (4)  and for any ﬁxed  2{ 0  1}h⇥t where
01 = 1k  the deﬁnition ˜L1( ˜0 ˜   ˜0 ˜) := ⌧ ( ˜0 ˜  ˜0 ˜/k) + t tr( ˜0 ˜ ) using the augmentation
above satisﬁes the property that L1( ˆ   ) = ˜L1( ˜0 ˜   ˜0 ˜) for any ˆ 2 Rh⇥t.
Proof. Since 01 = 1k we obtain a simpliﬁcation of L1:

(13)
It only remains is to establish that ⌧ (k ˆ  ) = ⌧ ( ˜0 ˜  ˜0 ˜/k). To do so  consider the sequence
of equivalence preserving transformations:
(14)

L1( ˆ   ) = ⌧110  + k ˆ  1 diag(0 ˆ )0 = ⌧ (k ˆ  ) + t  tr( ˜0 ˜ ).
⌧ (k ˆ  ) =
=

tr⇤0(k ˜  ˜)
k tr⌦0 ˜0(k ˜  ˜) = ⌧ ( ˜0 ˜  ˜0 ˜/k) 

max
⇤2Rh⇥˜t
+ :⇤01=1
max
+ :⌦01=1

where the equalities in (14) and (15) follow from the deﬁnition of ⌧ and the fact that linear maxi-
mizations over the simplex obtain their solutions at the vertices. To establish the equality between
+ there must exist an ⌦ 2 R˜t⇥˜t
(14) and (15)  since ˜ embeds the submatrix kI  for any ⇤ 2 Rh⇥˜t
+ sat-
isfying ⇤= ˜⌦/k. Furthermore  these matrices satisfy ⇤01 = 1 iff ⌦0 ˜01/k = 1 iff ⌦01 = 1.
Therefore  the result (8) holds for the ﬁrst layer loss (4)  using ˜L1 deﬁned in Lemma 3.
(The
same result can be established for other loss functions  such as the multi-class large-margin loss.)
Combining these lemmas yields the desired result of this section.
Theorem 1. For any second layer loss and any ﬁrst layer loss that satisﬁes the assumption of Lemma
2 (for example the large-margin multi-label loss (4))  the following equivalence holds:

⌦2R˜t⇥˜t

(15)

1

(2) =

min

{ ˜N :92{0 1}t⇥hs.t. 1=1k  ˜N = ˜0 ˜}

min

B2Im( ˜N )

min

˜L1(D ˜K  ˜N ) + ↵

2 tr(D0 ˜N†D ˜K)

D2Im( ˜N )
+L2(B  Y ) + 

2 tr(B ˜N†B0).

(16)

(Theorem 1 follows immediately from Lemmas 1 and 2.) Note that no relaxation has occurred thus
far: the objective value of (16) matches that of (2). Not only has this reformulation resulted in (2)
being entirely expressed in terms of the latent kernel matrix ˜N  the objective in (16) is jointly convex
in all participating unknowns  ˜N  B and D. Unfortunately  the constraints in (16) are not convex.

4 Convex Relaxation
We ﬁrst relax the problem by dropping the augmentation  7! ˜ and working with the t⇥ t variable
N = 0. Without the augmentation  Lemma 3 becomes a lower bound (i.e. (14)(15))  hence a
relaxation. To then achieve a convex form we further relax the constraints in (16). To do so  consider
(17)
(18)
(19)
where it is clear from the deﬁnitions that N0 ✓N 1 ✓N 2. (Here we use N ⌫ 0 to also encode
N0 = N.) Note that the set N0 corresponds to the original set of constraints from (16). The set

N0 = N : 9 2{ 0  1}t⇥h such that 1 = 1k and N = 0 
N1 = N : N 2{ 0  ...  k}t⇥t  N ⌫ 0  diag(N ) = 1k  rank(N )  h 

N2 = {N : N  0  N ⌫ 0  diag(N ) = 1k}  

5

Algorithm 1: ADMM to optimize F(N ) for N 2N 2.
1 Initialize: M0 = I  0 = 0.
2 while T = 1  2  . . . do
3
4

NT arg minN⌫0 L(N  MT1  T1)  by using the boosting Algorithm 2.
MT arg minM0 Mii=k L(NT   M  T1)  which has an efﬁcient closed form solution.
T T1 + 1

µ (MT  NT ); i.e. update the multipliers.

5
6 return NT .

Algorithm 2: Boosting algorithm to optimize G(N ) for N ⌫ 0.
1 Initialize: N0 0  H0 [ ] (empty set).
2 while T = 1  2  . . . do
3
4

Find the smallest arithmetic eigenvalue of rG(NT1)  and its eigenvector hT .
Conic search by LBFGS: (aT   bT ) mina0 b0 G(aNT1 + bhT h0T ).
Local search by LBFGS: HT local minHG(HH0) initialized by H = (paHT1 pbhT ).
Set NT HT H0T ; break if stopping criterion met.

5
6 return NT .

N1 simpliﬁes the characterization of this constraint set on the resulting kernel matrices N = 0.
However  neither N0 nor N1 are convex. Therefore  we need to adopt the further relaxed set N2 
which is convex. (Note that Nij  k has been implied by N ⌫ 0 and Nii = k in N2.) Since
dropping the rank constraint eliminates the constraints B 2 Im(N ) and D 2 Im(N ) in (16) when
N  0 [32]  we obtain the following relaxed problem  which is jointly convex in N  B and D:

min
N2N2

min
B2Rt⇥t

min
D2Rt⇥t

˜L1(DK  N ) + ↵

2 tr(D0N†DK) + L2(B  Y ) + 

2 tr(BN†B0).

(20)

5 Efﬁcient Training Approach

Unfortunately  nonlinear semideﬁnite optimization problems in the form (20) are generally thought
to be too expensive in practice despite their polynomial theoretical complexity [33  34]. Therefore 
we develop an effective training algorithm that exploits problem structure to bypass the main compu-
tational bottlenecks. The key challenge is that N2 contains both semideﬁnite and afﬁne constraints 
and the pseudo-inverse N† makes optimization over N difﬁcult even for ﬁxed B and D.
To mitigate these difﬁculties we ﬁrst treat (20) as the reduced problem  minN2N2 F(N )  where F
is an implicit objective achieved by minimizing out B and D. Note that F is still convex in N by
the joint convexity of (20). To cope with the constraints on N we adopt the alternating direction
method of multipliers (ADMM) [35] as the main outer optimization procedure; see Algorithm 1.
This approach allows one to divide N2 into two groups  N ⌫ 0 and {Nij  0  Nii = k}  yielding
the augmented Lagrangian
(21)
L(N  M  ) = F(N ) + (N ⌫ 0) + (Mij  0  Mii = k)  h  NMi + 1
where µ > 0 is a small constant  and  denotes an indicator such that (·) = 0 if · is true  and 1
otherwise. In this procedure  Steps 4 and 5 cost O(t2) time; whereas the main bottleneck is Step 3 
which involves minimizing GT (N ) := L(N  MT1  T1) over N ⌫ 0 for ﬁxed MT1 and T1.
Boosting for Optimizing over the Positive Semideﬁnite Cone. To solve the problem in Step 3
we develop an efﬁcient boosting procedure based on [36] that retains low rank iterates NT while
avoiding the need to determine N† when computing G(N ) and rG(N ); see Algorithm 2. The key
idea is to use a simple change of variable. For example  consider the ﬁrst layer objective and let
G1(N ) = minD ˜L1(DK  N ) + ↵
2 tr(D0N†DK). By deﬁning D = N C  we obtain G1(N ) =
minC ˜L1(N CK  N ) + ↵
2 tr(C0N CK)  which no longer involves N† but remains convex in C; this
problem can be solved efﬁciently after a slight smoothing of the objective [37] (e.g. by LBFGS).
Moreover  the gradient rG1(N ) can be readily computed given C⇤. Applying the same technique

2µ kNMk2
F  

6

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2
−2

−1.5

−1

−0.5

0

0.5

1

1.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

0.5

1

1.5

2

2.5

3

3.5

4

1

0.8

0.6

0.4

0.2

0

−0.2

−0.4

−0.6

−0.8

−1
−2

0

2

4

6

8

10

12

14

XOR
TJB2 49.8±0.7
TSS1 50.2±1.2
SVM1 50.3±1.1
4.2±0.9
LOC2
CVX2
0.2±0.1

BOXES
45.7 ±0.6
35.7 ±1.3
31.4 ±0.5
11.4 ±0.6
10.1 ±0.4

INTER
49.3±1.3
42.6±3.9
50.0±0.0
50.0±0.0
20.0±2.4

(a) “Xor” (2 ⇥ 400)
Figure 2: Synthetic experiments: three artiﬁcial data sets that cannot be meaningfully classiﬁed by
a one-layer model that does not use a nonlinear kernel. Table shows percentage test set error.

(c) “Interval” (2 ⇥ 200)

(b) “Boxes” (2 ⇥ 320)

(d) Synthetic results (% error)

to the second layer yields an efﬁcient procedure for evaluating G(N ) and rG(N ). Finally note that
many of the matrix-vector multiplications in this procedure can be further accelerated by exploiting
the low rank factorization of N maintained by the boosting algorithm; see the Appendix for details.
Additional Relaxation. One can further reduce computation cost by adopting additional relaxations
to (20). For example  by dropping N  0 and relaxing diag(N ) = 1k to diag(N )  1k  the
objective can be written as min{N⌫0 maxi Niik} F(N ). Since maxi Nii is convex in N  it is well
known that there must exist a constant c1 > 0 such that the optimal N is also an optimal solution
to minN⌫0 F(N ) + c1 (maxi Nii)2. While maxi Nii is not smooth  one can further smooth it
with a softmax  to instead solve minN⌫0 F(N ) + c1 (logPi exp(c2Nii))2 for some large c2. This
formulation avoids the need for ADMM entirely and can be directly solved by Algorithm 2.

6 Experimental Evaluation

To investigate the effectiveness of the proposed relaxation scheme for training a two-layer condi-
tional model  we conducted a number of experiments to compare learning quality against baseline
methods. Note that  given an optimal solution N  B and D to (20)  an approximate solution to the
original problem (2) can be recovered heuristically by ﬁrst rounding N to obtain   then recovering
W and V   as shown in Lemmas 1 and 2. However  since our primary objective is to determine
whether any convex relaxation of a two-layer model can even compete with one-layer or locally
trained two-layer models (rather than evaluate heuristic rounding schemes)  we consider a transduc-
tive evaluation that does not require any further modiﬁcation of N  B and D. In such a set-up  train-
ing data is divided into a labeled and unlabeled portion  where the method receives X = [X`  Xu]
and Y`  and at test time the resulting predictions ˆYu are evaluated against the held-out labels Yu.
Methods. We compared the proposed convex relaxation scheme (CVX2) against the following
methods: simple alternating minimization of the same two-layer model (2) (LOC2)  a one-layer
linear SVM trained on the labeled data (SVM1)  the transductive one-layer SVM methods of [38]
(TSJ1) and [39] (TSS1)  and the transductive latent clustering method of [18  19] (TJB2)  which
is also a two-layer model. Linear input kernels were used for all methods (standard in most deep
learning models) to control the comparison between one and two-layer models. Our experiments
were conducted with the following common protocol: First  the data was split into a separate training
and test set. Then the parameters of each procedure were optimized by a three-fold cross validation
on the training set. Once the optimal parameters were selected  they were ﬁxed and used on the test
set. For transductive procedures  the same three training sets from the ﬁrst phase were used  but then
combined with ten new test sets drawn from the disjoint test data (hence 30 overall) for the ﬁnal
evaluation. At no point were test examples used to select any parameters for any of the methods.
We considered different proportions between labeled/unlabeled data; namely  100/100 and 200/200.
Synthetic Experiments. We initially ran a proof of concept experiment on three binary labeled
artiﬁcial data sets depicted in Figure 2 (showing data set sizes n⇥ t) with 100/100 labeled/unlabeled
training points. Here the goal was simply to determine whether the relaxed two-layer training
method could preserve sufﬁcient structure to overcome the limits of a one-layer architecture. Clearly 
none of the data sets in Figure 2 are adequately modeled by a one-layer architecture (that does not
cheat and use a nonlinear kernel). The results are shown in the Figure 2(d) table.

7

TJB2
LOC2
SVM1
TSS1
TSJ1
CVX2

MNIST
19.3 ±1.2
19.3 ±1.0
16.2 ±0.7
13.7 ±0.8
14.6 ±0.7
9.2 ±0.6

USPS
53.2±2.9
13.9±1.1
11.6±0.5
11.1±0.5
12.1±0.4
9.2±0.5

Letter
20.4±2.1
10.4±0.6
6.2±0.4
5.9±0.5
5.6±0.5
5.1±0.5

COIL
30.6±0.8
18.0±0.5
16.9±0.6
17.5±0.6
17.2±0.6
13.8±0.6

CIFAR G241N
26.3±0.8
29.2±2.1
31.8±0.9
41.6±0.9
27.6±0.9
27.1±0.9
26.7±0.7
25.1±0.8
24.4±0.7
26.6±0.8
26.5±0.8
25.2±1.0

Table 1: Mean test misclassiﬁcation error % (± stdev) for 100/100 labeled/unlabeled.

TJB2
LOC2
SVM1
TSS1
TSJ1
CVX2

MNIST
13.7 ±0.6
16.3 ±0.6
11.2 ±0.4
11.4 ±0.5
12.3 ±0.5
8.8 ±0.4

USPS
46.6±1.0
9.7±0.5
10.7±0.4
11.3±0.4
11.8±0.4
6.6±0.4

Letter
14.0±2.6
8.5±0.6
5.0±0.3
4.4±0.3
4.8±0.3
3.8±0.3

COIL
45.0±0.8
12.8±0.6
15.6±0.5
14.9±0.4
13.5±0.4
8.2±0.4

CIFAR G241N
22.4±0.5
30.4±1.9
28.2±0.9
40.4±0.7
25.5±0.6
22.9±0.5
23.7±0.5
24.0±0.6
23.9±0.5
22.2±0.6
22.8±0.6
20.3±0.5

Table 2: Mean test misclassiﬁcation error % (± stdev) for 200/200 labeled/unlabeled.

As expected  the one-layer models SVM1 and TSS1 were unable to capture any useful classiﬁcation
structure in these problems. (TSJ1 behaves similarly to TSS1.) The results obtained by CVX2  on
the other hand  are encouraging. In these data sets  CVX2 is easily able to capture latent nonlin-
earities while outperforming the locally trained LOC2. Although LOC2 is effective in the ﬁrst two
cases  it exhibits weaker test accuracy while failing on the third data set. The two-layer method
TJB2 exhibited convergence difﬁculties on these problems that prevented reasonable results.
Experiments on “Real” Data Sets. Next  we conducted experiments on real data sets to deter-
mine whether the advantages in controlled synthetic settings could translate into useful results in
a more realistic scenario. For these experiments we used a collection of binary labeled data sets:
USPS  COIL and G241N from [40]  Letter from [41]  MNIST  and CIFAR-100 from [42]. (See
Appendix B in the supplement for further details.) The results are shown in Tables 1 and 2 for the
labeled/unlabeled proportions 100/100 and 200/200 respectively.
The relaxed two-layer method CVX2 again demonstrates effective results  although some data sets
caused difﬁculty for all methods. The data sets can be divided into two groups  (MNIST  USPS 
COIL) versus (Letter  CIFAR  G241N). In the ﬁrst group  two-layer modeling demonstrates a clear
advantage: CVX2 outperforms SVM1 by a signiﬁcant margin. Note that this advantage must be
due to two-layer versus one-layer modeling  since the transductive SVM methods TSS1 and TSJ1
demonstrate no advantage over SVM1. For the second group  the effectiveness of SVM1 demon-
strates that only minor gains can be possible via transductive or two-layer extensions  although some
gains are realized. The locally trained two-layer model LOC2 performed quite poorly in all cases.
Unfortunately  the convex latent clustering method TJB2 was also not competitive on any of these
data sets. Overall  CVX2 appears to demonstrate useful promise as a two-layer modeling approach.

7 Conclusion
We have introduced a new convex approach to two-layer conditional modeling by reformulating the
problem in terms of a latent kernel over intermediate feature representations. The proposed model
can accommodate latent feature representations that go well beyond a latent clustering  extend-
ing current convex approaches. A semideﬁnite relaxation of the latent kernel allows a reasonable
implementation that is able to demonstrate advantages over single-layer models and local training
methods. From a deep learning perspective  this work demonstrates that trainable latent layers can
be expressed in terms of reproducing kernel Hilbert spaces  while large margin methods can be use-
fully applied to multi-layer prediction architectures. Important directions for future work include
replacing the step and indmax transfers with more traditional sigmoid and softmax transfers  while
also replacing the margin losses with more traditional Bregman divergences; reﬁning the relaxation
to allow more control over the structure of the latent representations; and investigating the utility of
convex methods for stage-wise training within multi-layer architectures.

8

References
[1] Q. Le  M. Ranzato  R. Monga  M. Devin  G. Corrado  K. Chen  J. Dean  and A. Ng. Building high-level

features using large scale unsupervised learning. In Proceedings ICML  2012.

[2] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep Boltzmann machines. In NIPS  2012.
[3] Y. Bengio. Learning deep architectures for AI. Foundat. and Trends in Machine Learning  2:1–127  2009.
[4] G. Hinton. Learning multiple layers of representations. Trends in Cognitive Sciences  11:428–434  2007.
[5] G. Hinton  S. Osindero  and Y. Teh. A fast algorithm for deep belief nets. Neur. Comp.  18(7)  2006.
[6] N. Lawrence. Probabilistic non-linear principal component analysis. JMLR  6:1783–1816  2005.
[7] A. Banerjee  S. Merugu  I. Dhillon  and J. Ghosh. Clustering with Bregman divergences. J. Mach. Learn.

Res.  6:1705–1749  2005.

[8] M. Elad and M. Aharon. Image denoising via sparse and redundant representations over learned dictio-

naries. IEEE Trans. on Image Processing  15:3736–3745  2006.

[9] P. Comon. Independent component analysis  a new concept? Signal Processing  36(3):287–314  1994.
[10] M. Carreira-Perpi˜n´an and Z. Lu. dimensionality reduction by unsupervised regression. In CVPR  2010.
[11] N. Tishby  F. Pereira  and W. Bialek. The information bottleneck method. In Allerton Conf.  1999.
[12] S. Rifai  P. Vincent  X. Muller  X. Glorot  and Y. Bengio. Contractive auto-encoders: Explicit invariance

during feature extraction. In ICML  2011.

[13] K. Swersky  M. Ranzato  D. Buchman  B. Marlin  and N. de Freitas. On autoencoders and score matching

for energy based models. In Proceedings ICML  2011.

[14] Y. LeCun. Who is afraid of non-convex loss functions? http://videolectures.net/eml07 lecun wia  2007.
[15] Y. Bengio  N. Le Roux  P. Vincent  and O. Delalleau. Convex neural networks. In NIPS  2005.
[16] S. Nowozin and G. Bakir. A decoupled approach to exemplar-based unsupervised learning. In Proceed-

ings of the International Conference on Machine Learning  2008.

[17] D. Bradley and J. Bagnell. Convex coding. In UAI  2009.
[18] A. Joulin and F. Bach. A convex relaxation for weakly supervised classiﬁers. In Proc. ICML  2012.
[19] A. Joulin  F. Bach  and J. Ponce. Efﬁcient optimization for discrimin. latent class models. In NIPS  2010.
[20] Y. Guo and D. Schuurmans. Convex relaxations of latent variable training. In Proc. NIPS 20  2007.
[21] A. Goldberg  X. Zhu  B. Recht  J. Xu  and R. Nowak. Transduction with matrix completion: Three birds

with one stone. In NIPS 23  2010.

[22] E. Candes  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? arXiv:0912.3599  2009.
[23] X. Zhang  Y. Yu  and D. Schuurmans. Accelerated training for matrix-norm regularization: A boosting

approach. In Advances in Neural Information Processing Systems 25  2012.

[24] A. Anandkumar  D. Hsu  and S. Kakade. A method of moments for mixture models and hidden Markov

models. In Proc. Conference on Learning Theory  2012.

[25] D. Hsu and S. Kakade. Learning mixtures of spherical Gaussians: Moment methods and spectral decom-

positions. In Innovations in Theoretical Computer Science (ITCS)  2013.

[26] Y. Cho and L. Saul. Large margin classiﬁcation in inﬁnite neural networks. Neural Comput.  22  2010.
[27] R. Neal. Connectionist learning of belief networks. Artiﬁcial Intelligence  56(1):71–113  1992.
[28] G. Kimeldorf and G. Wahba. Some results on Tchebychefﬁan spline functions. JMAA  33:82–95  1971.
[29] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector ma-

chines. JMLR  pages 265–292  2001.

[30] J. Fuernkranz  E. Huellermeier  E. Mencia  and K. Brinker. Multilabel classiﬁcation via calibrated label

ranking. Machine Learning  73(2):133–153  2008.

[31] Y. Guo and D. Schuurmans. Adaptive large margin training for multilabel classiﬁcation. In AAAI  2011.
[32] A. Argyriou  T. Evgeniou  and M. Pontil. Convex multi-task feature learning. Mach. Learn.  73(3)  2008.
[33] Y. Nesterov and A. Nimirovskii. Interior-Point Polynomial Algorithms in Convex Programming. 1994.
[34] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge U. Press  2004.
[35] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundat. Trends in Mach. Learn.  3(1):1–123  2010.

[36] S. Laue. A hybrid algorithm for convex semideﬁnite optimization. In Proc. ICML  2012.
[37] O. Chapelle. Training a support vector machine in the primal. Neural Comput.  19(5):1155–1178  2007.
[38] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML  1999.
[39] V. Sindhwani and S. Keerthi. Large scale semi-supervised linear SVMs. In SIGIR  2006.
[40] http://olivier.chapelle.cc/ssl- book/benchmarks.html.
[41] http://archive.ics.uci.edu/ml/datasets.
[42] http://www.cs.toronto.edu/ kriz/cifar.html.

9

,Özlem Aslan
Hao Cheng
Xinhua Zhang
Dale Schuurmans
Eliya Nachmani
Lior Wolf