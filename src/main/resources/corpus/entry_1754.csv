2014,Efficient Sampling for Learning Sparse Additive Models in High Dimensions,We consider the problem of learning sparse additive models  i.e.  functions of the form: $f(\vecx) = \sum_{l \in S} \phi_{l}(x_l)$  $\vecx \in \matR^d$ from point queries of $f$. Here $S$ is an unknown subset of coordinate variables with $\abs{S} = k \ll d$. Assuming $\phi_l$'s to be smooth  we propose a set of points at which to sample $f$ and an efficient randomized algorithm that recovers a \textit{uniform approximation} to each unknown $\phi_l$. We provide a rigorous theoretical analysis of our scheme along with sample complexity bounds. Our algorithm utilizes recent results from compressive sensing theory along with a novel convex quadratic program for recovering robust uniform approximations to univariate functions  from point queries corrupted with arbitrary bounded noise. Lastly we theoretically analyze the impact of noise -- either arbitrary but bounded  or stochastic -- on the performance of our algorithm.,Efﬁcient Sampling for Learning Sparse Additive

Models in High Dimensions

Hemant Tyagi
ETH Z¨urich

htyagi@inf.ethz.ch

Andreas Krause

ETH Z¨urich

krausea@ethz.ch

Bernd G¨artner

ETH Z¨urich

gaertner@inf.ethz.ch

Abstract

form: f(x) = (cid:80)

We consider the problem of learning sparse additive models  i.e.  functions of the
l∈S φl(xl)  x ∈ Rd from point queries of f. Here S is an un-
known subset of coordinate variables with |S| = k (cid:28) d. Assuming φl’s to be
smooth  we propose a set of points at which to sample f and an efﬁcient random-
ized algorithm that recovers a uniform approximation to each unknown φl. We
provide a rigorous theoretical analysis of our scheme along with sample complex-
ity bounds. Our algorithm utilizes recent results from compressive sensing theory
along with a novel convex quadratic program for recovering robust uniform ap-
proximations to univariate functions  from point queries corrupted with arbitrary
bounded noise. Lastly we theoretically analyze the impact of noise – either arbi-
trary but bounded  or stochastic – on the performance of our algorithm.

Introduction

1
Several problems in science and engineering require estimating a real-valued  non-linear (and of-
ten non-convex) function f deﬁned on a compact subset of Rd in high dimensions. This chal-
lenge arises  e.g.  when characterizing complex engineered or natural (e.g.  biological) systems
[1  2  3]. The numerical solution of such problems involves learning the unknown f from point
evaluations (xi  f(xi))n
i=1. Unfortunately  if the only assumption on f is of mere smoothness  then
the problem is in general intractable. For instance  it is well known [4] that if f is C s-smooth then
n = Ω((1/δ)d/s) samples are needed for uniformly approximating f within error 0 < δ < 1. This
exponential dependence on d is referred to as the curse of dimensionality.
Fortunately  many functions arising in practice are much better behaved in the sense that they are
intrinsically low-dimensional  i.e.  depend on only a small subset of the d variables. Estimating
such functions has received much attention and has led to a considerable amount of theory along
with algorithms that do not suffer from the curse of dimensionality (cf.  [5  6  7  8]). Here we focus
on the problem of learning one such class of functions  assuming f possesses the sparse additive
structure:
(1.1)

f(x1  x2  . . .   xd) =(cid:88)

φl(xl); S ⊂ {1  . . .   d}  |S| = k (cid:28) d.

l∈S

Functions of the form (1.1) are referred to as sparse additive models (SPAMs) and generalize sparse
linear models to which they reduce to if each φl is linear. The problem of estimating SPAMs has
received considerable attention in the regression setting (cf.  [9  10  11] and references within) where
i=1 are typically i.i.d samples from some unknown probability measure P. This setting 
(xi  f(xi))n
however  does not consider the possibility of sampling f at speciﬁcally chosen points  tailored to
the additive structure of f. In this paper  we propose a strategy for querying f  together with an
efﬁcient recovery algorithm  with much stronger guarantees than known in the regression setting. In
particular  we provide the ﬁrst results guaranteeing uniformly accurate recovery of each individual
component φl of the SPAM. This can be crucial in applications where the goal is to not merely
approximate f  but gain insight into its structure.

1

Related work. SPAMs have been studied extensively in the regression setting  with observations
being corrupted with random noise. [9] proposed the COSSO method  which is an extension of the
Lasso to the reproducing kernel Hilbert space (RKHS) setting. A similiar extension was considered
in [10].
In [12]  the authors propose a least squares method regularized with smoothness  with
each φl lying in an RKHS and derive error rates for estimating f  in the L2(P) norm1. [13  14]
propose methods based on least squares loss regularized with sparsity and smoothness constraints.
[13] proves consistency of its method in terms of mean squared risk while [14] derives error rates
for estimating f in the empirical L2(Pn) norm 1. [11] considers the setting where each φl lies in
an RKHS. They propose a convex program for estimating f and derive error rates for the same 
in the L2(P)  L2(Pn) norms. Furthermore they establish the minimax optimality of their method
for the L2(P) norm. For instance  they derive an error rate of O((k log d/n) + kn− 2s
2s+1 ) in the
L2(P) norm for estimating C s smooth SPAMs. An estimator similar to the one in [11] was also
considered by [15]. They derive similar error rates as in [11]  albeit under stronger assumptions on
f. [16] proposes a method based on the adaptive group Lasso which asymptotically recovers S as n
increases. They also derive L2(P) error rates for the individual components of the SPAM.
There is further related work in approximation theory  where it is assumed that f can be sampled
at a desired set of points. [5] considers a setting more general than (1.1)  with f simply assumed
to depend on an unknown subset of k (cid:28) d-coordinate variables. They construct a set of sampling
points of size O(ck log d) for some constant c > 0  and present an algorithm that recovers a uniform
approximation2 to f. This model is generalized in [8]  with f assumed to be of the form f(x) =
g(Ax) for unknown A ∈ Rk×d; each row of A is assumed to be sparse.
[7] generalizes this 
by removing the sparsity assumption on A. While the methods of [5  8  7] could be employed
for learning SPAMs  their sampling sets will be of size exponential in k  and hence sub-optimal.
Furthermore  while these methods derive uniform approximations to f  they are unable to recover
the individual φl’s.

Our contributions. Our contributions are threefold:

1. We propose an efﬁcient algorithm that queries f at O(k log d) locations and recovers: (i) the
active set S exactly  along with (ii) a uniform approximation to each φl  l ∈ S. In contrast 
the existing error bounds in the statistics community [11  12  15] are in the much weaker
L2(P) sense. Furthermore  the existing results in approximation theory provide explicit
error bounds for recovering f and not the individual φl’s.

2. An important component of our algorithm is a novel convex quadratic program for estimat-
ing an unknown univariate function from point queries corrupted with arbitrary bounded
noise. We derive rigorous error bounds for this program in the L∞ norm that demonstrate
the robustness of the solution returned. We also explicitly demonstrate the effect of noise 
sampling density and the curvature of the function on the solution returned.

3. We theoretically analyze the impact of additive noise in the point queries on the perfor-
mance of our algorithm  for two noise models: arbitrary bounded noise and stochastic (iid)
noise. In particular for additive Gaussian noise  we show that our algorithm recovers a ro-
bust uniform approximation to each φl with at most O(k3(log d)2) point queries of f. We
also provide simulation results that validate our theoretical ﬁndings.

2 Problem statement

we assume f to be of the additive form: f(x1  . . .   xd) =(cid:80)

For any function g we denote its pth derivative by g(p) when p is large  else we use appropriate
number of prime symbols. (cid:107) g (cid:107)L∞[a b] denotes the L∞ norm of g in [a  b]. For a vector x we
denote its (cid:96)q norm for 1 ≤ q ≤ ∞ by (cid:107) x (cid:107)q. We consider approximating functions f : Rd → R
from point queries. In particular  for some unknown active S ⊂ {1  . . .   d} with |S| = k (cid:28) d 
l∈S φl(xl). Here φl : R → R are the
individual univariate components of the model. Our goal is to query f at suitably chosen points in
its domain in order to recover an estimate φest l of φl in a compact subset Ω ⊂ R for each l ∈ S.
We measure the approximation error in the L∞ norm. For simplicity  we assume that Ω = [−1  1] 

L2(P)=R |f (x)|2 dP(x) and (cid:107) f (cid:107)2

1 (cid:107) f (cid:107)2
2This means in the L∞ norm

L2(Pn)= 1

n

P

i f 2(xi)

2

meaning that we guarantee an upper bound on: (cid:107) φest l − φl (cid:107)L∞[−1 1] ;
l ∈ S. Furthermore  we
assume that we can query f from a slight enlargement: [−(1 + r)  (1 + r)]d of [−1  1]d for3 some
small r > 0. As will be seen later  the enlargement r can be made arbitrarily close to 0. We now list
our main assumptions for this problem.

l

max
l∈S

(cid:107) φ(p)

1. Each φl is assumed to be sufﬁciently smooth. In particular we assume that φl ∈ C 5[−(1 +
r)  (1 + r)] where C 5 denotes ﬁve times continuous differentiability. Since [−(1 + r)  (1 +
r)] is compact  this implies that there exist constants B1  . . .   B5 ≥ 0 so that

(cid:107)L∞[−(1+r) (1+r)] ≤ Bp;

2. We assume each φl to be centered in the interval [−1  1]  i.e. (cid:82) 1
replace each φl with φl + al for al ∈ R where(cid:80)

(2.1)
φl(t)dt = 0; l ∈ S.
Such a condition is necessary for unique identiﬁcation of φl. Otherwise one could simply
l al = 0 and unique identiﬁcation will not
be possible.
3. We require that for each φl  ∃Il ⊆ [−1  1] with Il being connected and µ(Il) ≥ δ so that
l(x)| ≥ D ; ∀x ∈ Il. Here µ(I) denotes the Lebesgue measure of I and δ  D > 0 are
|φ(cid:48)
constants assumed to be known to the algorithm. This assumption essentially enables us
l was zero or close to zero throughout [−1  1] for some
to detect the active set S. If say φ(cid:48)
l ∈ S  then due to Assumption 2 this would imply that φl is zero or close to zero.

p = 1  . . .   5.

−1

We remark that it sufﬁces to use estimates for our problem parameters instead of exact values. In
particular we can use upper bounds for: k  Bp; p = 1  . . .   5 and lower bounds for the parameters:
D  δ. Our methods and results stated in the coming sections will remain unchanged.

3 Our sampling scheme and algorithm

f(ξ + v) − f(ξ)

In this section  we ﬁrst motivate and describe our sampling scheme for querying f. We then outline
our algorithm and explain the intuition behind its different stages. Consider the Taylor expansion of
f at any point ξ ∈ Rd along the direction v ∈ Rd with step size:  > 0. For any C p smooth f;
p ≥ 2  we obtain for ζ = ξ + θv for some 0 < θ <  the following expression:
vT (cid:53)2 f(ζ)v.

(3.1)
Note that (3.1) can be interpreted as taking a noisy linear measurement of (cid:53)f(ξ) with the mea-
surement vector v and the noise being the Taylor remainder term. Importantly  due to the sparse
additive form of f  we have φl ≡ 0  l /∈ S  implying that (cid:53)f(ξ) = [φ(cid:48)
d(ξd)] is
at most k-sparse. Hence (3.1) actually represents a noisy linear measurement of the k-sparse vector
: (cid:53)f(ξ). For any ﬁxed ξ  we know from compressive sensing (CS) [17  18] that (cid:53)f(ξ) can be
recovered (with high probability) using few random linear measurements4.
This motivates the following sets of points using which we query f as illustrated in Figure 1. For
integers mx  mv > 0 we deﬁne

= (cid:104)v (cid:53)f(ξ)(cid:105) +

2(ξ2) . . . φ(cid:48)

1(ξ1) φ(cid:48)

1
2



i
mx

X :=
V :=

(1  1  . . .   1)T ∈ Rd : i = −mx  . . .   mx

(cid:27)
.
Using (3.1) at each ξi ∈ X and vj ∈ V for i = −mx  . . .   mx and j = 1  . . .   mv leads to:

w.p. 1/2 each; j = 1  . . .   mv and l = 1  . . .   d

 

(cid:27)

(cid:26)
(cid:26)

ξi =
vj ∈ Rd : vj l = ± 1√
mv
f(ξi + vj) − f(ξi)
(cid:125)
(cid:124)

(cid:123)(cid:122)



yi j

(3.2)

(3.3)

(3.4)

(cid:124) (cid:123)(cid:122) (cid:125)
(cid:105) +
= (cid:104)vj (cid:53)f(ξi)

xi

1
2

(cid:124)

vT

(cid:125)
(cid:123)(cid:122)
j (cid:53)2 f(ζi j)vj

 

ni j

2 ) =P

3In case f : [a  b]d → R we can deﬁne g : [−1  1]d → R where g(x) = f ( (b−a)

with ˜φl(xl) = φl( (b−a)
by querying f  and estimate ˜φl in [−1  1] which in turn gives an estimate to φl in [a  b].

˜φl(xl)
2 ). We then sample g from within [−(1 + r)  (1 + r)]d for some small r > 0

2 xl + b+a

2 x + b+a

4 Estimating sparse gradients via compressive sensing has been considered previously by Fornasier et al.
[8] albeit for a substantially different function class than us. Hence their sampling scheme differs considerably
from ours  and is not tailored for learning SPAMs.

l∈S

3

1(i/mx) φ(cid:48)

2(i/mx) . . . φ(cid:48)

where xi = (cid:53)f(ξi) = [φ(cid:48)
d(i/mx)] is k-sparse. Let us denote V =
[v1 . . . vmv]T   yi = [yi 1 . . . yi mv] and ni = [ni 1 . . . ni mv]. Then for each i  we can write (3.4) in
the succinct form:
(3.5)
Here V ∈ Rmv×d represents the linear measurement matrix  yi ∈ Rmv denotes the mea-
surement vector at ξi and ni represents “noise” on account of non-linearity of f. Note that
we query f at |X| (|V| + 1) = (2mx + 1)(mv + 1) many points. Given yi  V we can re-
cover a robust approximation to xi via (cid:96)1 minimization [17  18]. On account of the struc-
ture of (cid:53)f   we thus recover noisy estimates to φ(cid:48)
l at equispaced points along the interval
[−1  1]. We are now in a position to formally present our algorithm for learning SPAMs.

yi = Vxi + ni.

Our algorithm for learning SPAMs. The steps involved in our learn-
ing scheme are outlined in Algorithm 1. Steps 1-4 involve the CS-based
recovery stage wherein we use the aforementioned sampling sets to for-
mulate our problem as a CS one. Step 4 involves a simple thresholding
procedure where an appropriate threshold τ is employed to recover the
unknown active set S. In Section 4 we provide precise conditions on our

sampling parameters which guarantee exact recovery  i.e. (cid:98)S = S. Step
l(i/mx)  i.e.  (cid:98)xi l for each l ∈ (cid:98)S and i = −mx  . . .   mx  to return a
l for each l ∈ (cid:98)S. Hence our ﬁnal es-

5 leverages a convex quadratic program (P)  that uses noisy estimates of
φ(cid:48)
cubic spline estimate ˜φ(cid:48)
l. This program and its theoretical properties are
explained in Section 4. Finally  in Step 6 we derive our ﬁnal estimate
φest l via piecewise integration of ˜φ(cid:48)
timate of φl is a spline of degree 4. The performance of Algorithm 1 for
recovering S and the individual φl’s is presented in Theorem 1  which is
also our ﬁrst main result. All proofs are deferred to the appendix.

Algorithm 1 Algorithm for learning φl in the SPAM: f(x) =(cid:80)

Figure 1: The points ξi ∈
X (blue disks) and ξi + vj
(red arrows) for vj ∈ V.



l.

yi=Vz

for i = −mx  . . .   mx and j = 1  . . .   mv.
{l ∈ {1  . . .   d} : |(cid:98)xi l| > τ}.

l∈S φl(xl)
1: Choose mx  mv and construct sampling sets X and V as in (3.2)  (3.3).
2: Choose step size  > 0. Query f at f(ξi) f(ξi +vj) for i = −mx  . . .   mx and j = 1  . . .   mv.
(cid:107) z (cid:107)1. For τ > 0 compute (cid:98)S = ∪mx
4: Set(cid:98)xi := argmin
3: Construct yi where yi j = f (ξi+vj )−f (ξi)
5: For each l ∈ (cid:98)S  run (P) as deﬁned in Section 4 using ((cid:98)xi l)mx
6: For each l ∈ (cid:98)S  set φest l to be the piece-wise integral of ˜φ(cid:48)
parameter γ ≥ 0  to obtain ˜φ(cid:48)
then with high probability  (cid:98)S = S and for any γ ≥ 0 the estimate φest l
Theorem 1. There exist constants C  C1 > 0 such that if mx ≥ (1/δ)  mv ≥ C1k log d  0 <  <
D√mv
returned by Algorithm 1 satisﬁes for each l ∈ S:
CkB2
(cid:107) φest l − φl (cid:107)L∞[−1 1]≤ [59(1 + γ)]

l as explained in Section 4.

  τ and some smoothing

2√mv
and τ = CkB2

(cid:107)L∞[−1 1] .

(cid:107) φ(5)

i=−mx

i=−mx

(3.6)

CkB2√
mv

+

87
64m4
x

l

Recall that k  B2  D  δ are our problem parameters introduced in Section 2  while  is the step size
parameter from (3.4). We see that with O(k log d) point queries of f and with  < D√mv
  the active
set is recovered exactly. The error bound in (3.6) holds for all such choices of . It is a sum of two
terms in which the ﬁrst one arises during the estimation of (cid:53)f during the CS stage. The second
error term is the interpolation error bound for interpolating φ(cid:48)
√
l from its samples in the noise-free
setting. We note that our point queries lie in [−(1 + (/
mv))]d. For the stated
condition on  in Theorem 1 we have /
which can be made arbitrarily close to zero
by choosing an appropriately small . Hence we sample from only a small enlargement of [−1  1]d.

√
mv))  (1 + (/

mv < D

CkB2

CkB2

√

4

(−1−1...−1)(11...1)4 Analyzing the algorithm

We now describe and analyze in more detail the individual stages of Algorithm 1. We ﬁrst analyze
Steps 1-4 which constitute the compressive sensing (CS) based recovery stage. Next  we analyze
Step 5 where we also introduce our convex quadratic program. Lastly  we analyze Step 6 where we
derive our ﬁnal estimate φest l.

CkB2

l(i/mx)

(cid:12)(cid:12)(cid:12)(cid:98)φ(cid:48)

1(i/mx) . . . φ(cid:48)

2√mv
that the choice τ = CkB2

1 > 0 such that for mv satisfying c(cid:48)

l(i/mx) = (cid:98)xi l of φ(cid:48)

1mv − e−√
implies that (cid:98)S = S.

Compressive sensing-based recovery stage. This stage of Algorithm 1 involves solving a se-
quence of linear programs for recovering estimates of xi = [φ(cid:48)
d(i/mx)] for
i = −mx  . . .   mx. We note that the measurements yi are noisy linear measurements of xi with the
noise being arbitrary and bounded. For such a noise model  it is known that (cid:96)1 minimization results
recovery error (cid:107)(cid:98)xi − xi (cid:107)2 as speciﬁed in Lemma 1.
in robust recovery of the sparse signal [19]. Using this result in our setting allows us to quantify the
mvd that(cid:98)xi satisﬁes (cid:107)(cid:98)xi − xi (cid:107)2≤
3 ≥ 1 and C  c(cid:48)
Lemma 1. There exist constants c(cid:48)
3k log d < mv <
d/(log 6)2 we have with probability at least 1 − e−c(cid:48)
for all i = −mx  . . .   mx. Furthermore  given that this holds and mx ≥ 1/δ is satisﬁed we
2√mv
CkB2
then have for any  < D√mv
Thus upon using (cid:96)1 minimization based decoding at 2mx + 1 points  we recover robust estimates(cid:98)xi
to xi which immediately gives us estimates (cid:98)φ(cid:48)
l(i/mx) for i = −mx  . . .   mx
and l = 1  . . .   d.
In order to recover the active set S  we ﬁrst note that the spacing between
consecutive samples in X is 1/mx. Therefore the condition mx ≥ 1/δ implies on account of
Assumption 3 that the sample spacing is ﬁne enough to ensure that for each l ∈ S  there exists a
(cid:12)(cid:12)(cid:12) lies within a sufﬁciently small neighborhood of the origin in turn enabling
sample i for which |φ(cid:48)
l(i/mx)| ≥ D holds. The stated choice of the step size  essentially guarantees
∀l /∈ S  i that
set S along with the estimates: ((cid:98)φ(cid:48)
(cid:12)(cid:12)(cid:12)(cid:98)φ(cid:48)
(cid:12)(cid:12)(cid:12) ≤ τ = CkB2
detection of the active set. Therefore after this stage of Algorithm 1  we have at hand: the active
for each l ∈ S. Furthermore  it is easy to see that
l(i/mx) − φ(cid:48)
by using the noisy samples ((cid:98)φ(cid:48)

Robust estimation via cubic splines. Our aim now is to recover a smooth  robust estimate to φ(cid:48)
l
. Note that the noise here is arbitrary and bounded
2√mv
by τ = CkB2
. To this end we choose to use cubic splines as our estimates  which are essentially
piecewise cubic polynomials that are C 2 smooth [20]. There is a considerable amount of literature
in the statistics community devoted to the problem of estimating univariate functions from noisy
samples via cubic splines (cf.  [21  22  23  24])  albeit under the setting of random noise. Cubic
splines have also been studied extensively in the approximation theoretic setting for interpolating
samples (cf.  [20  25  26]).
We introduce our solution to this problem in a more general setting. Consider a smooth function

g : [t1  t2] → R and a uniform mesh5: (cid:81) : t1 = x0 < x1 < ··· < xn−1 < xn = t2 with
xi − xi−1 = h. We have at hand noisy samples: (cid:98)gi = g(xi) + ei  with noise ei being arbitrary
boundary conditions. Let H 2[t1  t2] denote the space of cubic splines deﬁned on [t1  t2] w.r.t(cid:81). We

In the noiseless scenario  the problem would be an interpolation one
and bounded:
for which a popular class of cubic splines are the “not-a-knot” cubic splines [25]. These achieve
optimal O(h4) error rates for C 4 smooth g without using any higher order information about g as

l(i/mx))mx
2√mv

  ∀l ∈ S ∀i.

i=mx

|ei| ≤ τ.

l(i/mx))mx

i=mx

l(i/mx)

then propose ﬁnding the cubic spline estimate as a solution of the following convex optimization
problem (in the 4n coefﬁcients of the n cubic polynomials) for some parameter γ ≥ 0:

(P)

min

L∈H2[t1 t2]

s.t. (cid:98)gi − γτ ≤ L(xi) ≤(cid:98)gi + γτ;
1 )  L(cid:48)(cid:48)(cid:48)(x−

1 ) = L(cid:48)(cid:48)(cid:48)(x+

L(cid:48)(cid:48)(cid:48)(x−

t1

L(cid:48)(cid:48)(x)2dx

i = 0  . . .   n 
n−1) = L(cid:48)(cid:48)(cid:48)(x+

n−1).

(4.1)

(4.2)
(4.3)

5We consider uniform meshes for clarity of exposition. The results in this section can be easily generalized

to non-uniform meshes.

5

(cid:90) t2



Note that (P) is a convex QP with linear constraints. The objective function can be veriﬁed to be
a positive deﬁnite quadratic form in the spline coefﬁcients6. Speciﬁcally  the objective measures
the total curvature of a feasible cubic spline in [t1  t2]. Each of the constraints (4.2)-(4.3) along

with the implicit continuity constraints of L(p); p = 0  1  2 at the interior points of(cid:81)  are linear
equalities/inequalities in the coefﬁcients of the piecewise cubic polynomials. (4.3) refers to the not-
a-knot boundary conditions [25] which are also linear equalities in the spline coefﬁcients. These
space of all not-a-knot cubic splines such that L(xi) lies within a ±γτ interval of(cid:98)gi  and returns
conditions imply that L(cid:48)(cid:48)(cid:48) is continuous7 at the knots x1  xn−1. Thus  (P) searches amongst the
the smoothest solution  i.e.  the one with the least total curvature. The parameter γ ≥ 0  controls
((cid:98)gi)n
i=0. As γ increases  the search interval: [(cid:98)gi − γτ (cid:98)gi + γτ] becomes larger for all i  leading to
the degree of smoothness of the solution. Clearly  γ = 0 implies interpolating the noisy samples
smoother feasible cubic splines. The following theorem formally describes the estimation properties
of (P) and is also our second main result.
Theorem 2. For g ∈ C 4[t1  t2] let L∗ : [t1  t2] → R be a solution of (P) for some parameter γ ≥ 0.
We then have that

τ +

29
64

h4 (cid:107) g(4) (cid:107)∞ .

(4.4)

We show in the appendix that if(cid:82) t2

t1

(L∗(cid:48)(cid:48)(x))2dx > 0  then L∗ is unique. Note that the error bound
(4.4) is a sum of two terms. The ﬁrst term is proportional to the external noise bound: τ  indicating
that the solution is robust to noise. The second term is the error that would arise even if perturbation
was absent i.e. τ = 0. Intuitively  if γτ is large enough  then we would expect the solution returned
by (P) to be a line. Indeed  a larger value of γτ would imply a larger search interval in (4.2)  which
if sufﬁciently large  would allow a line (that has zero curvature) to lie in the feasible region. More
formally  we show in the appendix  sufﬁcient conditions: τ = Ω( n1/2(cid:107)g(cid:48)(cid:48)(cid:107)∞
)  γ > 1  which if
satisﬁed  imply that the solution returned by (P) is a line. This indicates that if either n is small or g
has small curvature  then moderately large values of τ and/or γ will cause the solution returned by
(P) to be a line. If an estimate of (cid:107) g(cid:48)(cid:48) (cid:107)∞ is available  then one could for instance  use the upper
bound 1 + O(n1/2 (cid:107) g(cid:48)(cid:48) (cid:107)∞ /τ) to restrict the range of values of γ within which (P) is used.
l in the interval [−1  1]. The
Theorem 2 has the following Corollary for estimation of C 4 smooth φ(cid:48)
proof simply involves replacing: g with φ(cid:48)
2√mv
l  n + 1 with 2mx + 1  h with 1/mx and τ with CkB2
.
As the perturbation τ is directly proportional to the step size   we show in the appendix that if
additionally  = Ω(
l will be a
line.
Corollary 1. Let (P) be employed for each l ∈ S using noisy samples
with step size  satisfying 0 <  < D√mv
(P)  we then have for any γ ≥ 0 that:

)  γ > 1  holds  then the corresponding estimate ˜φ(cid:48)

l as the corresponding solution returned by

. Denoting ˜φ(cid:48)

(cid:110)(cid:98)φ(cid:48)

√mxmv(cid:107)φ(cid:48)(cid:48)(cid:48)

l (cid:107)∞

(cid:111)mx

l(i/mx)

γ−1

  and

i=−mx

γ−1

(cid:107) L∗ − g (cid:107)∞≤(cid:20)118(1 + γ)

(cid:21)

3

CkB2

l (cid:107)L∞[−1 1]≤(cid:20)59(1 + γ)

3

(cid:21) CkB2√

mv

(cid:107) ˜φ(cid:48)

l − φ(cid:48)

+

29
64m4
x

(cid:107) φ(5)

l

(cid:107)L∞[−1 1] .

(4.5)

The ﬁnal estimate. We now derive the ﬁnal estimate φest l of φl for each l ∈ S. Denote x0(=
−1) < x1 < ··· < x2mx−1 < x2mx(= 1) as our equispaced set of points on [−1  1]. Since
l : [−1  1] → R returned by (P) is a cubic spline  we have ˜φ(cid:48)
l i(x) for x ∈ [xi  xi+1]
˜φ(cid:48)
l(x) = ˜φ(cid:48)
where ˜φ(cid:48)
l i is a polynomial of degree at most 3. We then deﬁne φest l(x) := ˜φl i(x) + Fi for
x ∈ [xi  xi+1] and i = 0  . . .   2mx − 1. Here ˜φl i is a antiderivative of ˜φ(cid:48)
l i and Fi’s are constants
of integration. Denoting F0 = F   we have that φest l is continuous at x1  . . .   x2mx−1 for: Fi =
1 ≤ i ≤ 2mx − 1. Hence
i we obtain φest l(·) = ψl(·) + F where ψl(x) = ψl i(x) for
by denoting ψl i(·) := ˜φl i(·) + F (cid:48)

j=1( ˜φl j(xj+1) − ˜φl j(xj)) − ˜φl i(xi) + F = F (cid:48)

˜φl 0(x1) +(cid:80)i−1

i + F ;

6Shown in the appendix.
7f (x−) = limh→0− f (x + h) and f (x+) = limh→0+ f (x + h) denote left right hand limits respectively.

6

x ∈ [xi  xi+1]. Now on account of Assumption 2  we require φest l to also be centered implying
F = − 1

ψl(x)dx. Hence we output our ﬁnal estimate of φl to be:

(cid:82) 1

2

−1

(cid:90) 1

φest l(x) := ψl(x) − 1
2

(4.6)
Since φest l is by construction continuous in [−1  1]  is a piecewise combination of polynomials of
degree at most 4  and since φ(cid:48)
est l is a cubic spline  φest l is a spline function of order 4. Lastly  we
show in the proof of Theorem 1 that (cid:107) φest l − φl (cid:107)L∞[−1 1]≤ 3 (cid:107) ˜φ(cid:48)
l (cid:107)L∞[−1 1] holds. Using
Corollary 1  this provides us with the error bounds stated in Theorem 1.

l − φ(cid:48)

ψl(x)dx;

−1

x ∈ [−1  1].

5

Impact of noise on performance of our algorithm

i j

(cid:17)

 + kB2
2mv

Our third main contribution involves analyzing the more realistic scenario  when the point queries
are corrupted with additive external noise z(cid:48). Thus querying f in Step 2 of Algorithm 1 results in
i and f(ξi + vj) + z(cid:48)
noisy values: f(ξi) + z(cid:48)
i j respectively. This changes (3.5) to the noisy linear
i)/ for i = −mx  . . .   mx and j = 1  . . .   mv.
i j − z(cid:48)
system: yi = Vxi + ni + zi where zi j = (z(cid:48)
Notice that external noise gets scaled by (1/)  while |ni j| scales linearly with .

In this model  the external noise is arbitrary but bounded  so that

i|  (cid:12)(cid:12)z(cid:48)
(cid:12)(cid:12) < κ; ∀i  j. It can be veriﬁed along the lines of the proof of Lemma 1 that: (cid:107) ni + zi (cid:107)2≤
(cid:16) 2κ

Arbitrary bounded noise.
|z(cid:48)
√
. Observe that unlike the noiseless setting   cannot be made arbitrarily close to
mv
0  as it would blow up the impact of the external noise. The following theorem shows that if κ is
small relative to D2 < |φ(cid:48)
l(x)|2  ∀x ∈ Il  l ∈ S  then8 there exists an interval for choosing   within
which Algorithm 1 recovers exactly the active set S. This condition has the natural interpretation
[1 − A  1 + A] where A :=(cid:112)1 − (16C 2kB2κ)/D2
that if the signal-to-‘external noise’ ratio in Il is sufﬁciently large  then S can be detected exactly.
Theorem 3. There exist constants C  C1 > 0 such that if κ < D2/(16C 2kB2)  mx ≥ (1/δ)  and
(cid:16) 2κ
mv ≥ C1k log d hold  then for any  ∈ D√mv
  we have in Algorithm 1  with high probability  that(cid:98)S = S and for any
and τ =
(cid:18)4C
γ ≥ 0  for each l ∈ S:
(cid:107) φest l − φl (cid:107)L∞[−1 1]≤ [59(1 + γ)]

(cid:107)L∞[−1 1] .

 + kB2
2mv

(cid:107) φ(5)

(cid:19)

(5.1)

(cid:17)

2CkB2

mv

√

√

+

+

l

mvκ


CkB2√
mv

87
64m4
x

i j

i − z(cid:48)

Stochastic noise.
In this model  the external noise is assumed to be i.i.d. Gaussian  so that
z(cid:48)
i  z(cid:48)
In this setting we consider resampling f at the query point N
times and then averaging the noisy samples  in order to reduce σ. Given this  we now have that
N ); i.i.d. ∀i  j. Using standard tail-bounds for Gaussians  we can show that for
i  z(cid:48)
z(cid:48)

i j ∼ N (0  σ2); i.i.d. ∀i  j.
i j ∼ N (0  σ2

any κ > 0 if N is chosen large enough then: |zi j| =(cid:12)(cid:12)z(cid:48)

(cid:12)(cid:12) ≤ 2κ; ∀i  j with high probability.

Hence the external noise zi j would be bounded with high probability and the analysis for Theorem
3 can be used in a straightforward manner. Of course  an advantage that we have in this setting is
that κ can be chosen to be arbitrarily close to zero by choosing a correspondingly large value of N.
We state all this formally in the form of the following theorem.
Theorem 4. There exist constants C  C1 > 0 such that for κ < D2/(16C 2kB2)  mx ≥ (1/δ)  and
mv ≥ C1k log d  if we re-sample each query in Step 2 of Algorithm 1: N > σ2
(cid:112)1 − (16C 2kB2κ)/D2 and τ =
times for 0 < p < 1  and average the values  then for any  ∈ D√mv
least 1 − p − o(1)  that (cid:98)S = S and for any γ ≥ 0  for each l ∈ S:
(cid:19)
(cid:107) φest l − φl (cid:107)L∞[−1 1]≤ [59(1 + γ)]
8Il is the “critical” interval deﬁned in Assumption 3 for detecting l ∈ S.

[1 − A  1 + A] where A :=
  we have in Algorithm 1  with probability at

(cid:16)√
κp |X||V|(cid:17)

(cid:16) 2κ
(cid:18)4C

 + kB2
2mv
√

(cid:107)L∞[−1 1] .

CkB2√
mv

87
64m4
x

mvκ


(cid:107) φ(5)

κ2 log

(5.2)

(cid:17)

2CkB2

mv

√

+

+

2σ

l

7

Note that we query f now N |X| (|V| + 1) times. Also  |X| = (2mx + 1) = Θ(1)  and
κ = O(k−1)  as D  C  B2  δ are constants. Hence the choice |V| = O(k log d) gives us N =
O(k2 log(p−1k2 log d)) and leads to an overall query complexity of: O(k3 log d log(p−1k2 log d))
when the samples are corrupted with additive Gaussian noise. Choosing p = O(d−c) for any con-
stant c > 0 gives us a sample complexity of O(k3(log d)2)  and ensures that the result holds with
high probability. The o(1) term goes to zero exponentially fast as d → ∞.

Simulation results. We now provide simulation results on synthetic data to support our theoretical
ﬁndings. We consider the noisy setting with the point queries being corrupted with Gaussian noise.
For d = 1000  k = 4 and S = {2  105  424  782}  consider f : Rd → R where f = φ2(x2) +
φ105(x105) + φ424(x424) + φ782(x782) with: φ2(x) = sin(πx)  φ105(x) = exp(−2x)  φ424(x) =
(1/3) cos3(πx) + 0.8x2  φ782(x) = 0.5x4 − x2 + 0.8x. We choose δ = 0.3  D = 0.2 which
can be veriﬁed as valid parameters for the above φl’s. Furthermore  we choose mx = (cid:100)2/δ(cid:101) = 7
and mv = (cid:100)2k log d(cid:101) = 56 to satisfy the conditions of Theorem 4. Next  we choose constants
= 4.24 × 10−4 as required by Theorem 4. For the choice
C = 0.2  B2 = 35 and κ = 0.95 D2
 = D√mv
= 0.0267  we then query f at (2mx + 1)(mv + 1) = 855 points. The function values
are corrupted with Gaussian noise: N (0  σ2/N) for σ = 0.01 and N = 100. This is equivalent to
resampling and averaging the points queries N times. Importantly the sufﬁcient condition on N  as
stated in Theorem 4 is (cid:100) σ2
)(cid:101) = 6974 for p = 0.1. Thus we consider a signiﬁcantly
undersampled regime. Lastly we select the threshold τ =
= 0.2875 as stated
by Theorem 4  and employ Algorithm 1 for different values of the smoothing parameter γ.

 + kB2
2mv

(cid:16) 2κ

2σ|X||V|

κp

√

mv

(cid:17)

2CkB2

16C2kB2

√

κ2 log(

(a) Estimates of φ2

(b) Estimates of φ105

(c) Estimates of φ424

(d) Estimates of φ782

Figure 2: Estimates φest l of φl (black) for: γ = 0.3 (red)  γ = 1 (blue) and γ = 5 (green).

The results are shown in Figure 2. Over 10 independent runs of the algorithm we observed that
S was recovered exactly each time. Furthermore we see from Figure 2 that the recovery is quite
accurate for γ = 0.3. For γ = 1 we notice that the search interval γτ = 0.2875 becomes large
enough so as to cause the estimates φest 424  φest 782 to become relatively smoother. For γ = 5 
the search interval γτ = 1.4375 becomes wide enough for a line to ﬁt in the feasible region for
424  φ(cid:48)
φ(cid:48)
105  the
search interval is not sufﬁciently wide enough for a line to lie in the feasible region  even for γ = 5.
However we notice that the estimates φest 2  φest 105 become relatively smoother as expected.

782. This results in φest 424  φest 782 to be quadratic functions. In the case of φ(cid:48)

2  φ(cid:48)

6 Conclusion

We proposed an efﬁcient sampling scheme for learning SPAMs. In particular  we showed that with
only a few queries  we can derive uniform approximations to each underlying univariate function
of the SPAM. A crucial component of our approach is a novel convex QP for robust estimation of
univariate functions via cubic splines  from samples corrupted with arbitrary bounded noise. Lastly 
we showed how our algorithm can handle noisy point queries for both (i) arbitrary bounded and (ii)
i.i.d. Gaussian noise models. An important direction for future work would be to determine the op-
timality of our sampling bounds by deriving corresponding lower bounds on the sample complexity.
Acknowledgments. This research was supported in part by SNSF grant 200021 137528 and a
Microsoft Research Faculty Fellowship.

8

−1−0.500.51−1−0.500.51xφ2 φest 2−1−0.500.51−20246xφ105 φest 105−1−0.500.51−0.2−0.100.10.20.3xφ424 φest 424−1−0.500.51−1.5−1−0.500.51xφ782 φest 782References
[1] Th. Muller-Gronbach and K. Ritter. Minimal errors for strong and weak approximation of
stochastic differential equations. Monte Carlo and Quasi-Monte Carlo Methods  pages 53–82 
2008.

[2] M.H. Maathuis  M. Kalisch  and P. B¨uhlmann. Estimating high-dimensional intervention ef-

fects from observational data. Ann. Statist.  37(6A):3133–3164  2009.

[3] M.J. Wainwright. Information-theoretic limits on sparsity recovery in the high-dimensional

and noisy setting. IEEE Trans. Inform. Theory  55(12):5728–5741  2009.

[4] J.F. Traub  G.W. Wasilkowski  and H. Wozniakowski. Information-Based Complexity. Aca-

demic Press  New York  1988.

[5] R. DeVore  G. Petrova  and P. Wojtaszczyk. Approximation of functions of few variables in

high dimensions. Constr. Approx.  33:125–143  2011.

[6] A. Cohen  I. Daubechies  R.A. DeVore  G. Kerkyacharian  and D. Picard. Capturing ridge

functions in high dimensions from point queries. Constr. Approx.  pages 1–19  2011.

[7] H. Tyagi and V. Cevher. Active learning of multi-index function models. Advances in Neural

Information Processing Systems 25  pages 1475–1483  2012.

[8] M. Fornasier  K. Schnass  and J. Vyb´ıral. Learning functions of few arbitrary linear parameters

in high dimensions. Foundations of Computational Mathematics  12(2):229–262  2012.

[9] Y. Lin and H.H. Zhang. Component selection and smoothing in multivariate nonparametric

regression. Ann. Statist.  34(5):2272–2297  2006.

[10] M. Yuan. Nonnegative garrote component selection in functional anova models. In AISTATS 

volume 2  pages 660–666  2007.

[11] G. Raskutti  M.J. Wainwright  and B. Yu. Minimax-optimal rates for sparse additive models

over kernel classes via convex programming. J. Mach. Learn. Res.  13(1):389–427  2012.

[12] V. Koltchinskii and M. Yuan. Sparse recovery in large ensembles of kernel machines. In COLT 

pages 229–238  2008.

[13] P. Ravikumar  J. Lafferty  H. Liu  and L. Wasserman. Sparse additive models. Journal of the

Royal Statistical Society: Series B (Statistical Methodology)  71(5):1009–1030  2009.

[14] L. Meier  S. Van De Geer  and P. B¨uhlmann. High-dimensional additive modeling. Ann.

Statist.  37(6B):3779–3821  2009.

[15] V. Koltchinskii and M. Yuan. Sparsity in multiple kernel learning. Ann. Statist.  38(6):3660–

3695  2010.

[16] J. Huang  J.L. Horowitz  and F. Wei. Variable selection in nonparametric additive models. Ann.

Statist.  38(4):2282–2313  2010.

[17] E.J. Cand`es  J.K. Romberg  and T. Tao. Stable signal recovery from incomplete and inaccurate
measurements. Communications on Pure and Applied Mathematics  59(8):1207–1223  2006.

[18] D.L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory  52(4):1289–1306  2006.
[19] P. Wojtaszczyk. (cid:96)1 minimization with noisy data. SIAM J. Numer. Anal.  50(2):458–467  2012.
[20] J.H. Ahlberg  E.N. Nilson  and J.L. Walsh. The theory of splines and their applications. Aca-

demic Press (New York)  1967.

[21] I.J. Schoenberg. Spline functions and the problem of graduation. Proceedings of the National

Academy of Sciences  52(4):947–950  1964.

[22] C.M. Reinsch. Smoothing by spline functions. Numer. Math  10:177–183  1967.
[23] G. Wahba. Smoothing noisy data with spline functions. Numer. Math.  24(5):383–393  1975.
[24] P. Craven and G. Wahba. Smoothing noisy data with spline functions. Numer. Math. 

31(4):377–403  1978.

[25] C. de Boor. A practical guide to splines. Springer Verlag (New York)  1978.
[26] C.A. Hall and W.W. Meyer. Optimal error bounds for cubic spline interpolation. J. Approx.

Theory  16(2):105 – 122  1976.

9

,Sivan Sabato
Anand Sarwate
Nati Srebro
Hemant Tyagi
Bernd Gärtner
Andreas Krause
Xiangyu Wang
Fangjian Guo
Katherine Heller
David Dunson
Daniel Ritchie
Anna Thomas
Pat Hanrahan
Noah Goodman