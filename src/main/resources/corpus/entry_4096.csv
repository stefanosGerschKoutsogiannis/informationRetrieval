2019,Optimal Analysis of Subset-Selection Based L_p Low-Rank Approximation,We show that for the problem of $\ell_p$ rank-$k$ approximation of any given matrix over $R^{n\times m}$ and $C^{n\times m}$  the algorithm of column subset selection enjoys approximation ratio $(k+1)^{1/p}$ for $1\le p\le 2$ and $(k+1)^{1-1/p}$ for $p\ge 2$. This improves upon the previous $O(k+1)$ bound (Chierichetti et al. 2017) for $p\ge 1$. We complement our analysis with lower bounds; these bounds match our upper bounds up to constant 1 when $p\geq 2$. At the core of our techniques is an application of Riesz-Thorin interpolation theorem from harmonic analysis  which might be of independent interest to other algorithmic designs and analysis more broadly.

Our analysis results in improvements on approximation guarantees of several other algorithms with various time complexity. For example  to make the algorithm of column subset selection computationally efficient  we analyze a polynomial time bi-criteria algorithm which selects $O(k\log m)$ number of columns. We show that this algorithm has an approximation ratio of $O((k+1)^{1/p})$ for $1\le p\le 2$ and $O((k+1)^{1-1/p})$ for $p\ge 2$. This improves over the bound in (Chierichetti et al. 2017) with an $O(k+1)$ approximation ratio. Our bi-criteria algorithm also implies an exact-rank method in polynomial time with a slightly larger approximation ratio.,Optimal Analysis of Subset-Selection Based (cid:96)p

Low-Rank Approximation

Chen Dan

Carnegie Mellon University

cdan@cs.cmu.edu

Hong Wang∗

Princeton University

Hong.Wang1991@gmail.com

Hongyang Zhang∗

Toyota Technological Institute at Chicago

honyanz@ttic.edu

Yuchen Zhou∗

University of Wisconsin  Madison
yuchenzhou@stat.wisc.edu

Pradeep Ravikumar

Carnegie Mellon University
pradeepr@cs.cmu.edu

Abstract

We study the low rank approximation problem of any given matrix A over Rn×m
and Cn×m in entry-wise (cid:96)p loss  that is  ﬁnding a rank-k matrix X such that
(cid:107)A − X(cid:107)p is minimized. Unlike the traditional (cid:96)2 setting  this particular variant is
NP-Hard. We show that the algorithm of column subset selection  which was an
algorithmic foundation of many existing algorithms  enjoys approximation ratio
(k + 1)1/p for 1 ≤ p ≤ 2 and (k + 1)1−1/p for p ≥ 2. This improves upon the
previous O(k + 1) bound for p ≥ 1 [1]. We complement our analysis with lower
bounds; these bounds match our upper bounds up to constant 1 when p ≥ 2. At the
core of our techniques is an application of Riesz-Thorin interpolation theorem from
harmonic analysis  which might be of independent interest to other algorithmic
designs and analysis more broadly.
As a consequence of our analysis  we provide better approximation guarantees
for several other algorithms with various time complexity. For example  to make
the algorithm of column subset selection computationally efﬁcient  we analyze a
polynomial time bi-criteria algorithm which selects O(k log m) columns. We show
that this algorithm has an approximation ratio of O((k + 1)1/p) for 1 ≤ p ≤ 2 and
O((k + 1)1−1/p) for p ≥ 2. This improves over the best-known bound with an
O(k + 1) approximation ratio. Our bi-criteria algorithm also implies an exact-rank
method in polynomial time with a slightly larger approximation ratio.

1

Introduction

Low rank approximation has wide applications in compressed sensing  numerical linear algebra 
machine learning  and many other domains. In compressed sensing  low rank approximation serves
as an indispensable building block for data compression. In numerical linear algebra and machine
learning  low rank approximation is the foundation of many data processing algorithms  such as
PCA. Given a data matrix A ∈ Fn×m  low rank approximation aims at ﬁnding a low-rank matrix

∗Equal Contribution

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

X ∈ Fn×m such that

OPT =

min

X:rank(X)≤k

(cid:107)X − A(cid:107).

(1)

Here the ﬁeld F can be either R or C. The focus of this work is on the case when (cid:107)·(cid:107) is the entry-wise

(cid:96)p norm  and we are interested in an estimate (cid:98)X with a tight approximation ratio α so that we have
the guarantee: (cid:107)(cid:98)X − A(cid:107) ≤ α · OPT.

As noted earlier  such low-rank approximation is a fundamental workhorse of machine learning. The
key reason to focus on approximations with respect to general (cid:96)p norms  in contrast to the typical
(cid:96)2 norm  is that these general (cid:96)p norms are better able to capture a broader range of realistic noise
in complex datasets. For example  it is well-known that the (cid:96)1 norm is more robust to the sparse
outlier [2–4]. So the (cid:96)1 low-rank approximation problem is a robust version of the classic PCA
which uses the (cid:96)2 norm and has received tremendous attentions in machine learning  computer vision
and data mining [5]  [6]  [7]. A related problem (cid:96)p linear regression has also been studied extensively
in the statistics community  and these two problems share similar motivation. In particular  if we
assume a statistical model Aij = A(cid:63)
ij + εij  where A(cid:63) is a low rank matrix and εij are i.i.d. noise  the
different values of p correspond to the MLE of different noise distributions  say p = 1 for Laplacian
noise and p = 2 for Gaussian noise.
While it has better empirical and statistical properties  the key bottleneck to solving the problem in (1)
is computational  and is known to be NP-hard in general. For example  the (cid:96)1 low-rank approximation
is NP-hard to solve exactly even when k = 1 [8]  and is even hard to approximate with large error
under the Exponential Time Hypothesis [9]. [10] proved the NP-hardness of the problem when
p = ∞. A recent work [11] proves that the problem has no constant factor approximation algorithm
running in time O(2kδ
) for a constant δ > 0  assuming the correctness of Small Set Expansion
Hypothesis and Exponential Time Hypothesis. The authors also proposed a PTAS (Polynomial Time
Approximation Scheme) with (1 + ε) approximation ratio when 0 < p < 2. However  the running
time is as large as O(npoly(k/ε)).
Many other efforts have been devoted to designing approximation algorithms in order to alleviate the
computational issues of (cid:96)p low-rank approximation. One promising approach is to apply subgradient
descent based methods or alternating minimization [12]. Unfortunately  the loss surface of problem
(1) suffers from saddle points even in the simplest p = 2 case [13]  which might be arbitrarily worse
than OPT. Therefore  they may not work well for the low-rank approximation problem as these local
searching algorithms may easily get stuck at bad stationary points without any guarantee.
Instead  we consider another line of research—the heuristic algorithm of column subset selection
(CSS). Here  the algorithm proceeds by choosing the best k columns of A as an estimation of column
space of X and then solving an (cid:96)p linear regression problem in order to obtain the optimal row space
of X. See Algorithm 1 for the detailed procedure. Although the vanilla form of the subset selection
based algorithm also has an exponential time complexity in terms of the rank k  it can be slightly
modiﬁed to polynomial time bi-criteria algorithms which selects more than k columns [1]. Most
importantly  these algorithms are easy to implement and runs fast with nice empirical performance.
Thus  subset selection based algorithms might seem to effectively alleviate the computational issues
of problem (1). The caveat however is that CSS might seem like a simple heuristic  with potentially a
very large worst-case approximation ratio α.
In this paper  we show that CSS yields surprisingly reasonable approximation ratios  which we
also show to be tight by providing corresponding lower bounds  thus providing a strong theoretical
backing for the empirical observations underlying CSS.
Due in part to its importance  there has been a burgeoning set of recent analyses of column subset
selection. In the traditional low rank approximation problem with Frobenious norm error (the p = 2
case in our setting)  [14] showed that CSS achieves
k + 1 approximation ratio. The authors
also showed that the
k + 1 bound is tight (both upper and lower bounds can be recovered by
our analysis). [15–18] improved the running time of CSS with different sampling schemes while
preserving similar approximation bounds. The CSS algorithm and its variants are also applied and
analyzed under various different settings. For instance  [19] and [20] studied the CUR decomposition
with the Frobenius norm. [21] studied the CSS problem under the missing-data case. With (cid:96)1
error  [22] studied CSS for non-negative matrices in (cid:96)1 error. [23] gave tight approximation bounds

√

√

2

for CSS under ﬁnite-ﬁeld binary matrix setting. Furthermore  [9] considered the low rank tensor
approximation with the Frobenius norm.
Despite a large amount of work on the subset-selection algorithm and the (cid:96)p low rank approximation
problem  many fundamental questions remain unresolved. Probably one of the most important open
questions is: what is the tight approximation ratio α for the subset-selection algorithm in the (cid:96)p low
rank approximation problem  up to a constant factor? In [1]  the approximation ratio is shown to be
upper bounded by (k + 1) and lower bounded by O(k1− 2
p ) when p > 2. This problem becomes even
more challenging when one requires the approximation ratio to be tight up to factor 1  as little was
known about a direct tool to achieve this goal in general. In this work  we improve both upper and
lower bounds in [1] to optimal when p > 2. Note that our bounds are still applicable and improve
over [1] when 1 < p < 2  but there is an O(k

p−1) gap between the upper and lower bounds.

2

1.1 Our Results

The best-known approximation ratio of subset selection based algorithms for (cid:96)p low-rank approxima-
tion is O(k + 1) [1]. In this work  we give an improved analysis of this algorithm. In particular  we
show that the Column Subset Selection in Algorithm 1 is a cp k-approximation  where

(cid:40)

cp k =

1
(k + 1)
p  
(k + 1)1− 1
p  

1 ≤ p ≤ 2 
p ≥ 2.

This improves over Theorem 4 in [1] which proved that the algorithm is an O(k + 1)-approximation 
for all p ≥ 1. Below  we state our main theorem formally:
Theorem 1.1 (Upper bound). The subset selection algorithm in Algorithm 1 is a cp k-approximation.
Our proof of Theorem 1.1 is built upon novel techniques of Riesz-Thorin interpolation theorem. In
particular  with the proof of special cases for p = 1  2 ∞  we are able to interpolate the approximation
ratio of all intermediate p’s. Our techniques might be of independent interest to other (cid:96)p norm or
Schatten-p norm related problem more broadly. See Section 1.2 for more discussions.
We also complement our positive result of subset selection algorithm with a negative result. Surpris-
ingly  our upper bound matches our lower bound exactly up to constant 1 for p ≥ 2. Below  we state
our negative results formally:
Theorem 1.2 (Lower bound). There exist inﬁnitely many different values of k  such that approxima-
tion ratio of any k-subset-selection based algorithm is at least (k + 1)1− 1
p for (cid:96)p rank-k approxima-
tion.
Note that our lower bound strictly improves the (k + 1)1− 2
can be found in Section 1.2 and we put the whole proof of Theorem 1.2 in Appendix 3.
One drawback of Algorithm 1 is that the running time scales exponentially with the rank k. However 
it serves as an algorithmic foundation of many existing computationally efﬁcient algorithms. For
example  a bi-criteria variant of this algorithm runs in polynomial time  only requiring the rank
parameter to be a little over-parameterized. Our new analysis can be applied to this algorithm as well.
Below  we state our result informally:
Theorem 1.3 (Informal statement of Theorem 4.1). There is a bi-criteria algorithm which runs in
poly(m  n  k) time and selects O(k log m) columns of A. The algorithm is an O(cp k)-approximation
algorithm.

p bound in [1]. The main idea of the proof

Our next result is a computationally-efﬁcient  exact-rank algorithm with slightly larger approximation
ratio. Below  we state our result informally:
Theorem 1.4 (Informal statement of Theorem 4.2). There is an algorithm which solves problem
(1) and runs in poly(m  n) time with an O(c3
p kk log m)-approximation ratio  provided that k =
O( log n

log log n ).

1.2 Our Techniques

In this section  we give a detailed discussion about our techniques in the proofs. We start with the
analysis of approximation ratio of column subset selection algorithm.

3

Algorithm 1 A cp k approximation to problem (1) by column subset selection.
1: Input: Data matrix A and rank parameter k.
2: Output: X ∈ Rn×m such that rank(X) ≤ k and (cid:107)X − A(cid:107)p ≤ cp k · OPT.
3: for I ∈ {S ⊆ [m]; |S| = k} do
4:
5:
6:
7: end for
8: return XI which minimizes (cid:107)A − XI(cid:107)p for I ∈ {S ⊆ [m]; |S| = k}.

U ← AI.
Run (cid:96)p linear regression over V that minimizes the loss (cid:107)A − U V (cid:107)p.
Let XI = U V

Remark Throughout this paper  we state the theorems for real matrices. The results can be naturally
generalized for complex matrices as well.
Notations: We denote by A ∈ Rn×m the input matrix  and Ai the i-th column of A. A∗ = U V
is the optimal rank-k approximation  where U ∈ Rn×k  V ∈ Rk×m. ∆i = Ai − U Vi is the error
vector on the i-th column  and ∆li the l-th element of vector ∆i. For any X ∈ Rn×k  deﬁne the
error of projecting A onto X by Err(X) = minY ∈Rk×m (cid:107)A − XY (cid:107)p. Let J = (j1 ···   jk) ∈ [m]k
be a subset of [m] with cardinality k. We denote XJ as the following column subset in matrix X:
XJ = [Xj1  Xj2  ···   Xjk ]. Similarly  we denote by XdJ the following column subset in the d-th
dimension of matrix X: XdJ = [Xdj1   Xdj2  ···   Xdjk ]. Denote by J∗ the column subset which
gives smallest approximation error  i.e.  J∗ = argminJ∈[m]k Err(AJ ).
Analysis in Previous Work: In order to show that the column subset selection algorithm gives an
α-approximation  we need to prove that

(2)
Directly bounding Err(AJ∗ ) is prohibitive. In [1]  the authors proved an upper bound of (k + 1)
in two steps. First  the authors constructed a speciﬁc S ∈ [m]k  and upper bounded Err(AJ∗ ) by
Err(AS). Their construction is as follows: S is deﬁned as the minimizer of

Err(AJ∗ ) ≤ α(cid:107)∆(cid:107)p.

(cid:81)
| det(VJ )|
j∈J (cid:107)∆j(cid:107)p

.

S = argmin
J∈[m]k

In the second step  [1] upper bounded Err(AS) by considering the approximation error on each
column Ai  and upper bounded the (cid:96)p distance from Ai to the subspace spanned by AS using triangle
inequality of (cid:96)p distance. They showed that the distance is at most (k + 1) times of (cid:107)∆i(cid:107)  uniformly
for all columns i ∈ [m]. Therefore  the approximation ratio is bounded by (k + 1).Our approach is
different from the above analysis in both steps.

Weighted Average: In the ﬁrst step  we use a so-called weighted average technique  inspired by the
approach in [14  23]. Instead of using the error of one speciﬁc column subset as an upper bound  we
use a weighted average over all possible column subsets  i.e. 

Errp(AJ∗ ) ≤ (cid:88)

J∈[m]k

wJ Errp(AJ ) 

where the weight wJ’s are carefully chosen for each column subset J. This weighted average
technique captures more information from all possible column subsets  rather than only from one
speciﬁc subset  and leads to a tighter bound.

Riesz-Thorin Interpolation Theorem: In the second step  unlike [1] which simply used triangle
inequality to prove the upper bound  our technique leads to more reﬁned analysis of upper bounds for
the approximation error for each subset J ∈ [m]k. With the technique of weighted average in the ﬁrst
step  proving a technical inequality (Lemma 2.2) concerning the determinants sufﬁces to complete the
analysis of approximation ratio. In the proof of this lemma  we introduce several powerful tools from
harmonic analysis  the theory of interpolating linear operators. Riesz-Thorin theorem is a classical
result in interpolation theory that gives bounds for Lp to Lq operator norm. In general  it is easier
to prove estimates within spaces like L2  L1 and L∞. Interpolation theory enables us to generalize

4

results in those spaces to some Lp and Lq spaces with an explicit operator norm. By the Riesz-Thorin
interpolation theorem  we are able to prove the lemma by just checking the special cases p = 1  2 ∞ 
and then interpolate the inequality for all the intermediate value of p’s.

√

Lower Bounds: We now discuss the techniques in proving the lower bounds. Our proof is a
generalization of [14]  which shows that for the special case p = 2 
k + 1 is the best possible
approximation ratio. Their proof for the lower bound is constructive: they constructed a (k + 1) ×
(k + 1) matrix A  such that using any k-subset leads to a sub-optimal solution by a factor no less
than (1 − ε)
k + 1. However  since (cid:96)p norm is not rotationally-invariant in general  it is tricky to
generalize their analysis to other values of p’s. To resolve the problem  we use a specialized version
of their construction  the perturbed Hadamard matrices (see Section 3 for details)  as they have nice
symmetricity and are much easier to analyze. We give an example of special case k = 3 for better
intuition:

√

ε

A =

ε
ε
ε
1 −1 −1
1
1 −1
1 −1
1 −1 −1
1

 .

Here ε is a positive constant close to 0. We note that A is very close to a rank-3 matrix: if we replace
the ﬁrst row by four zeros  then it becomes rank-3. Thus  the optimal rank-3 approximation error is at
most (4εp)1/p = 41/pε. Now we consider the column subset selection algorithm. For example  we
use the ﬁrst three columns A1  A2  A3 to approximate the whole matrix — the error only comes from
the fourth column. We can show that when ε is small  the projection of A4 to span{A1  A2  A3} is
very close to

−A1 − A2 − A3 = (−3ε −1 −1  1)T .

Therefore  the column subset selection algorithm achieve about 4ε error on this matrix  which is a
p factor from being optimal. The similar construction works for any integer k = 2r − 1  r ∈ Z+ 
41− 1
where the lower bound is replaced by (k + 1)1− 1
p   also matches with our upper bound exactly when
p ≥ 2.

2 Analysis of Approximation Ratio

In this section  we will prove Theorem 1.1. Recall that our goal is to bound Err(AJ∗ ). We ﬁrst
introduce two useful lemmas. Lemma 2.1 gives an upper bound on approximation error by choosing
a single arbitrary column subset AJ. Lemma 2.2 is our main technical lemma.
Lemma 2.1. If J satisﬁes det(VJ ) (cid:54)= 0  then the approximation error of AJ can be upper bounded
by

Errp(AJ ) ≤ (cid:107)∆ − ∆J V −1

J V (cid:107)p
p.

Lemma 2.2. Let S = {sij} ∈ Ck×m be a complex matrix  r = (r1 ···   rm) be m-dimensional
complex vector  and T =

∈ C(k+1)×m  then we have

|det(TI )|p ≤ Cp k

|det(SJ )|p  

|rl|p (cid:88)

J∈[m]k

m(cid:88)
(cid:26)(k + 1) 

l=1

Cp k = cp

p k =

(k + 1)p−1 

1 ≤ p ≤ 2 
p ≥ 2.

(cid:21)

(cid:20)r
(cid:88)

S

I∈[m]k+1

where

We ﬁrst show that Theorem 1.1 has a clean proof using the two lemmas  as stated below.

Proof. of Theorem 1.1: We can WLOG assume that rank(V ) = k. In fact  if rank(A) < k  then
of course Errp(AJ∗ ) = 0 and there is nothing to prove. Otherwise if rank(A) ≥ k  then by the
deﬁnition of V   we know that rank(V ) = k.

5

In other words  we are going to choose a set of non-negative weights wJ such that(cid:80)

We will upper bound the approximation error of the best column by a weighted average of Errp(AJ ).
J∈[m]k wJ = 1 

and upper bound Errp(AJ∗ ) by

Errp(AJ∗ ) = min
J∈[m]k

wJ Errp(AJ ).

In the following analysis  our choice of wJ will be

Errp(AJ ) ≤ (cid:88)
(cid:80)
| det(VJ )|p
I∈[m]k | det(VI )|p .

J∈[m]k

Since rank(V ) = k  wJ are well-deﬁned. We ﬁrst prove

wJ =

| det(VJ )|pErrp(AJ∗ ) ≤ n(cid:88)

m(cid:88)

d=1

i=1

(cid:12)(cid:12)(cid:12)(cid:12)det

(cid:18)∆dL

VL

(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)p

 

(3)

where we denote L = (i  J) = (i  j1 ···   jk) ∈ [m]k+1.
In fact  when det(VJ ) = 0  of course LHS of (3) = 0 ≤ RHS. When det(VJ ) (cid:54)= 0  we know that VJ is
invertible. By Lemma 2.1 

| det(VJ )|pErrp(AJ∗ ) ≤| det(VJ )|p(cid:107)∆ − ∆J V −1

p

p

=

i=1

J Vi

J V (cid:107)p

=(cid:107) det(VJ )(cid:0)∆ − ∆J V −1
J V(cid:1)(cid:107)p
m(cid:88)
(cid:107) det(VJ )(cid:0)∆i − ∆J V −1
(cid:1)(cid:107)p
m(cid:88)
n(cid:88)
| det(VJ )(cid:0)∆di − ∆dJ V −1
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)p
(cid:12)(cid:12)(cid:12)(cid:12)det
(cid:18)∆di ∆dJ
m(cid:88)
n(cid:88)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)p
(cid:12)(cid:12)(cid:12)(cid:12)det
(cid:18)∆dL
m(cid:88)
n(cid:88)

VL

VJ

d=1

d=1

Vi

i=1

i=1

=

=

=

p

.

d=1

i=1

(cid:1)|p

J Vi

1

1

i=1

VL

J∈[m]k

(cid:80)
| det(VJ )|p
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)p
I∈[m]k | det(VI )|p Errp(AJ )
(cid:12)(cid:12)(cid:12)(cid:12)det
(cid:18)∆dL
m(cid:88)
(cid:88)
(cid:80)
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)p .
I∈[m]k | det(VI )|p
(cid:12)(cid:12)(cid:12)(cid:12)det
(cid:18)∆dL
(cid:88)
(cid:80)
I∈[m]k | det(VI )|p
(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)p ≤ Cp k
(cid:88)
m(cid:88)
m(cid:88)
m(cid:88)

(cid:12)(cid:12)(cid:12)(cid:12)det
(cid:18)∆dL
 = Cp k

|∆dj|p.

(cid:107)∆j(cid:107)p

|∆dj|p

L∈[m]k+1

L∈[m]k+1

VL

VL

j=1

p = Cp kOPTp 

d=1

j=1

j=1

Err(AJ∗ ) ≤ C 1/p

p k OPT = cp kOPT.

6

The second to last equality follows from the Schur’s determinant identity. Therefore (3) holds  and

J∈[m]k

Errp(AJ∗ ) ≤ (cid:88)

≤ n(cid:88)

n(cid:88)

d=1

=

d=1

1

(cid:80)
I∈[m]k | det(VI )|p
Errp(AJ∗ ) ≤ n(cid:88)

Cp k

By Lemma 2.2 

Therefore 

which means

Therefore  we only need to prove the two lemmas. Lemma 2.1 is relatively easy to prove.

Proof. of Lemma 2.1: Recall that by deﬁnition of ∆i  Ai = U Vi + ∆i 

Errp(AJ ) = min

Y ∈Rk×m

(cid:107)A − AJ Y (cid:107)p
J V (cid:107)p

p

p

≤ (cid:107)A − AJ V −1
= (cid:107)(U V + ∆) − (U VJ + ∆J )V −1
= (cid:107)∆ − ∆J V −1

J V (cid:107)p
p.

J V (cid:107)p

p

The main difﬁculty in our analysis comes from Lemma 2.2. The proof is based on Riesz-Thorin
interpolation theorem from harmonic analysis. Although the technical details in verifying a key
inequality (4) are quite complicated  the remaining part which connects Lemma 2.2 to the Riesz-
Thorin interpolation theorem is not that difﬁcult to understand. Below we give a proof to Lemma 2.2
without verifying (4)  and leave the complete proof of (4) in the appendix.

Proof. of Lemma 2.2: We ﬁrst state a simpliﬁed version of the Riesz-Thorin interpolation theorem 
which is the most convenient-to-use version for our proof. The general version can be found in the
Appendix.
Lemma 2.3. [Simpliﬁed version of Riesz-Thorin] Let Λ : Cn1 × Cn2 → Cn0 be a multi-linear
operator  such that the following inequalities

(cid:107)Λ(a  b)(cid:107)p0 ≤ Mp0(cid:107)a(cid:107)p0(cid:107)b(cid:107)p0 .
(cid:107)Λ(a  b)(cid:107)p1 ≤ Mp1(cid:107)a(cid:107)p1(cid:107)b(cid:107)p1 .

hold for all a ∈ Cn1  b ∈ Cn2  then we have

holds for all θ ∈ [0  1]  where

(cid:107)Λ(a  b)(cid:107)pθ ≤ M 1−θ

p0

M θ
p1

(cid:107)a(cid:107)pθ(cid:107)b(cid:107)pθ .

1
pθ

:=

1 − θ
p0

+

θ
p1

.

Riesz-Thorin theorem is a classical result in interpolation theory that gives bounds for Lp to Lq oper-
ator norm. In general  it is easier to prove estimates within spaces like L2  L1 and L∞. Interpolation
theory enables us to generalize results in those spaces to some Lp and Lq spaces in between with an
explicit operator norm. In our application  the Xi is a set of ni elements and Vi is Cni  the space of
functions on ni elements.
Now we prove Lemma 2.2. In fact  by symmetricity  Lemma 2.2 is equivalent to

(cid:88)

m(cid:88)

|rt|p (cid:88)

(k + 1)!

|det(TI )|p ≤ k!Cp k

|det(SJ )|p .

Here (cid:0)[m]

k
Taking 1

t=1

J∈([m]
k )

I∈( [m]
k+1)

p-th power on both sides  we have the following equivalent form

(cid:1) = {(i1 ···   ik)|1 ≤ i1 < i2 < ··· < ik ≤ m} denotes the k-subsets of [m].
 (cid:88)
 1

 (cid:88)

(cid:32) m(cid:88)

 1

|det(SJ )|p

|det(TI )|p

(cid:33) 1

|rt|p

p

p

p

.

≤ cp k
(k + 1)

1
p

I∈( [m]
k+1)

By Laplace expansion on the ﬁrst row of det(TI )  we have for every I = (i1 ···   ik+1) ∈(cid:0) [m]

J∈([m]
k )

t=1

(cid:1)

k+1

k+1(cid:88)
(−1)t+1rit det(SI−t).

det(TI ) =

t=1

7

Here  I−t = (i1 ···   it−1  it+1 ···   ik+1) ∈(cid:0)[m]
(cid:1).
This motivates us to deﬁne the following multilinear map Λ : C([m]
{at}t∈([m]
is deﬁned as

1 ) {bJ}J∈([m]

k ) ∈ C([m]

1 ) ∈ C([m]

k ) → C( [m]

k

1 ) × C([m]

k )  and index set I = (i1 ···   ik+1) ∈(cid:0) [m]
k+1(cid:88)
(−1)t+1aitbI−t.

k+1

k+1): for all

(cid:1)  [Λ(a  b)]I

[Λ(a  b)]I =

t=1

Now  by letting at = rt  bJ = det(SJ )  the inequality can be written as

(cid:16)

(cid:17)(cid:107)a(cid:107)p(cid:107)b(cid:107)p.

(4)

(cid:107)Λ(a  b)(cid:107)p ≤ cp k
(k + 1)

(cid:107)a(cid:107)p(cid:107)b(cid:107)p = max

1
p

1  (k + 1)1− 2

p

(cid:16)

(cid:17)

1  (k + 1)1− 2

p

  the inequality can be rewritten as (cid:107)Λ(a  b)(cid:107)p ≤ Mp(cid:107)a(cid:107)p(cid:107)b(cid:107)p. We

Let Mp = max
denote

1
p

=

1 − θ
p0

+

θ
p1

.

(cid:26)ε

here  when p ∈ [1  2)  we choose p0 = 1  p1 = 2; when p ∈ [2  +∞)  we choose p0 = 2  p1 = +∞.
Then  we can observe the following nice property about Mp:

Mp = M 1−θ

(5)
This is exactly the same form as Riesz-Thorin Theorem! Hence  we only need to show (4) holds
for p = 1  2 ∞  then applying Riesz-Thorin proves all the intermediate cases p ∈ (1  2) ∪ (2 ∞)
immediately.
We leave the complete proof of (4) in the appendix.

M θ
p1

p0

3 Lower Bounds

In this section  we give a proof sketch of Theorem 1.2. The proof is constructive: we prove the
theorem by showing for all ε > 0  we can construct a matrix A(ε)  such that selecting every k
columns of A(ε) leads to an approximation ratio at least (k+1)
1+oε(1) . Then  the theorem follows by
letting ε → 0+. Our choice of A(ε) is a perturbation of Hadamard matrices.
Throughout the proof  we assume that k = 2r − 1  for some r ∈ Z+  and ε > 0 is an arbitrarily small
constant. We consider the well known Hadamard matrix of order (k + 1) = 2r  deﬁned below:

1− 1
p

(cid:32)

H (1) = 1 

(cid:33)

H (2l) =

H (2l−1) H (2l−1)
H (2l−1) −H (2l−1)

  l ≥ 1.

Now we can deﬁne A(ε)  the construction of lower bound instance: it is a perturbation of H by
replacing all the entries on the ﬁrst row by ε  i.e. 

A(ε)ij =

when i = 1 
Hij when i (cid:54)= 1.

(6)

We can see that A(ε) is close to a rank-k matrix. In fact  A(0) has rank at most k. Therefore  we can
upper bound OPT by

OPT ≤ (cid:107)A(ε) − A(0)(cid:107)p = ((k + 1)εp)1/p = (k + 1)1/pε.

(7)
The remaining work is to give a lower bound on the approximation error using any k columns.
For simplicity of notations  we use A as shorthand for A(ε) when it’s clear from context. Say we

8

return all the columns of A

Algorithm 2 [1] SELECTCOLUMNS (k  A): Selecting O(k log m) columns of A.
1: Input: Data matrix A and rank parameter k.
2: Output: O(k log m) columns of A
3: if number of columns of A ≤ 2k then
4:
5: else
6:
7:
8:
9:
10:
11: end if

until at least (1/10)-fraction columns of A are λp-approximately covered
Let AR be the columns of A not approximately covered by R
return AR∪ SELECTCOLUMNS (k  AR)

Let R be uniform at random 2k columns of A

repeat

Algorithm 3 [1]An algorithm that transforms an O(k log m)-rank matrix factorization into a k-rank
matrix factorization without inﬂating the error too much.
1: Input: U ∈ Rn×O(k log m)  V ∈ RO(k log m)×m
2: Output: W ∈ Rn×k  Z ∈ Rk×m
3: Apply Lemma E.1 to U to obtain matrix W 0
4: Run (cid:96)p linear regression over Z 0  s.t. (cid:107)W 0Z 0 − U V (cid:107)p is minimized
5: Apply Algorithm 1 with input (Z 0)T ∈ Rn×O(k log m) and k to obtain X and Y
6: Set Z ← X T
7: Set W ← W 0Y T
8: Output W and Z

are using all (k + 1) columns except the j-th  i.e. the column subset is A[k+1]−{j}. Obviously  we
achieve zero error on all the columns other than the j-th. Therefore  the approximation error is

essentially the (cid:96)p distance from Aj to span(cid:8)A[k+1]−{j}(cid:9). We can show that the (cid:96)p projection from
Aj to span(cid:8)A[k+1]−{j}(cid:9) is very close to(cid:80)

i(cid:54)=j(−Ai)  in other words 

(−Ai)(cid:107)p = (1 − o(1))(k + 1)ε

(8)

Err(A[k+1]−{j}) = (1 − o(1))(cid:107)Aj −(cid:88)

i(cid:54)=j

The theorem follows by combining (7) and (8). The complete proof can be found in the appendix.

4 Analysis of Efﬁcient Algorithms

One drawback of the column subset selection algorithm is its time complexity - it requires
O(mk poly(n)) time  which is not desirable since it’s exponential in k. However  several more
efﬁcient algorithms [1] are designed based on it. Our tighter analysis on Algorithm 1 implies better
approximation guarantees on these algorithms as well. The improved bounds can be stated as follows:

Theorem 4.1. Algorithm 2  which runs in poly(m  n  k) time and selects k log m columns  is a
bi-criteria O(cp k) = O((k + 1)max(1/p 1−1/p)) approximation algorithm.
Theorem 4.2. Algorithm 3  which runs in poly(m  n) time as long as k = O( log n
O(c3

p kk log m) = O(kmax(1+3/p 4−3/p) log m) approximation algorithm.

log log n ) is an

These results improve the previous O(k) and O(k4 log(m)) bounds respectively. We include the
analysis of Algorithm 2 and Algorithm 3 in Appendix for completeness.

Acknowledgments

C.D. and P.R. acknowledge the support of Rakuten Inc.  and NSF via IIS1909816. The authors would
also like to acknowledge two MathOverﬂow users  known to us only by their usernames  ’fedja’ and
’Mahdi’  for informing us the Riesz-Thorin interpolation theorem.

9

References
[1] Flavio Chierichetti  Sreenivas Gollapudi  Ravi Kumar  Silvio Lattanzi  Rina Panigrahy  and
David P Woodruff. Algorithms for lp low rank approximation. arXiv preprint arXiv:1705.06730 
2017.

[2] Emmanuel J Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component

analysis? Journal of the ACM (JACM)  58(3):11  2011.

[3] Peter J Huber. Robust statistics. In International Encyclopedia of Statistical Science  pages

1248–1251. Springer  2011.

[4] Lei Xu and Alan L Yuille. Robust principal component analysis by self-organizing rules based
on statistical physics approach. IEEE Transactions on Neural Networks  6(1):131–143  1995.

[5] Deyu Meng and Fernando De La Torre. Robust matrix factorization with unknown noise. In
Proceedings of the IEEE International Conference on Computer Vision  pages 1337–1344 
2013.

[6] Naiyan Wang and Dit-Yan Yeung. Bayesian robust matrix factorization for image and video
processing. In Proceedings of the IEEE International Conference on Computer Vision  pages
1785–1792  2013.

[7] Liang Xiong  Xi Chen  and Jeff Schneider. Direct robust matrix factorizatoin for anomaly
In Data Mining (ICDM)  2011 IEEE 11th International Conference on  pages

detection.
844–853. IEEE  2011.

[8] Nicolas Gillis and Stephen A Vavasis. On the complexity of robust pca and l1-norm low-rank

matrix approximation. Mathematics of Operations Research  2018.

[9] Zhao Song  David P Woodruff  and Peilin Zhong. Low rank approximation with entrywise
l 1-norm error. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing  pages 688–701. ACM  2017.

[10] Nicolas Gillis and Yaroslav Shitov. Low-rank matrix approximation in the inﬁnity norm. arXiv

preprint arXiv:1706.00078  2017.

[11] Frank Ban  Vijay Bhattiprolu  Karl Bringmann  Pavel Kolev  Euiwoong Lee  and David P
Woodruff. A ptas for lp-low rank approximation. In Proceedings of the Thirtieth Annual
ACM-SIAM Symposium on Discrete Algorithms  pages 747–766. SIAM  2019.

[12] Anastasios Kyrillidis. Simple and practical algorithms for lp norm low-rank approximation.

arXiv preprint arXiv:1805.09464  2018.

[13] Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning

from examples without local minima. Neural networks  2(1):53–58  1989.

[14] Amit Deshpande  Luis Rademacher  Santosh Vempala  and Grant Wang. Matrix approximation
In Proceedings of the seventeenth annual
and projective clustering via volume sampling.
ACM-SIAM symposium on Discrete algorithm  pages 1117–1126. Society for Industrial and
Applied Mathematics  2006.

[15] Alan Frieze  Ravi Kannan  and Santosh Vempala. Fast monte-carlo algorithms for ﬁnding

low-rank approximations. Journal of the ACM (JACM)  51(6):1025–1041  2004.

[16] Amit Deshpande and Santosh Vempala. Adaptive sampling and fast low-rank matrix approxi-
mation. In Approximation  Randomization  and Combinatorial Optimization. Algorithms and
Techniques  pages 292–303. Springer  2006.

[17] Christos Boutsidis  Michael W Mahoney  and Petros Drineas. An improved approximation
algorithm for the column subset selection problem. In Proceedings of the twentieth annual
ACM-SIAM symposium on Discrete algorithms  pages 968–977. SIAM  2009.

10

[18] Amit Deshpande and Luis Rademacher. Efﬁcient volume sampling for row/column subset
selection. In Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on 
pages 329–338. IEEE  2010.

[19] Petros Drineas  Michael W Mahoney  and S Muthukrishnan. Relative-error cur matrix decom-

positions. SIAM Journal on Matrix Analysis and Applications  30(2):844–881  2008.

[20] Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions. SIAM Journal

on Computing  46(2):543–589  2017.

[21] Yining Wang and Aarti Singh. Column subset selection with missing data via active sampling.

In Artiﬁcial Intelligence and Statistics  pages 1033–1041  2015.

[22] Aditya Bhaskara and Silvio Lattanzi. Non-negative sparse regression and column subset
selection with l1 error. In LIPIcs-Leibniz International Proceedings in Informatics  volume 94.
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik  2018.

[23] Chen Dan  Kristoffer Arnsfelt Hansen  He Jiang  Liwei Wang  and Yuchen Zhou. Low Rank Ap-
proximation of Binary Matrices: Column Subset Selection and Generalizations. In Igor Potapov 
Paul Spirakis  and James Worrell  editors  43rd International Symposium on Mathematical Foun-
dations of Computer Science (MFCS 2018)  volume 117 of Leibniz International Proceedings in
Informatics (LIPIcs)  pages 41:1–41:16  Dagstuhl  Germany  2018. Schloss Dagstuhl–Leibniz-
Zentrum fuer Informatik. ISBN 978-3-95977-086-6. doi: 10.4230/LIPIcs.MFCS.2018.41. URL
http://drops.dagstuhl.de/opus/volltexte/2018/9623.

[24] Qifa Ke and Takeo Kanade. Robust l/sub 1/norm factorization in the presence of outliers and
missing data by alternative convex programming. In Computer Vision and Pattern Recognition 
2005. CVPR 2005. IEEE Computer Society Conference on  volume 1  pages 739–746. IEEE 
2005.

[25] Feiping Nie  Jianjun Yuan  and Heng Huang. Optimal mean robust principal component analysis.

In International conference on machine learning  pages 1062–1070  2014.

[26] Praneeth Netrapalli  UN Niranjan  Sujay Sanghavi  Animashree Anandkumar  and Prateek
Jain. Non-convex robust pca. In Advances in Neural Information Processing Systems  pages
1107–1115  2014.

[27] Kai-Yang Chiang  Cho-Jui Hsieh  and Inderjit Dhillon. Robust principal component analysis
with side information. In International Conference on Machine Learning  pages 2291–2299 
2016.

[28] Xinyang Yi  Dohyung Park  Yudong Chen  and Constantine Caramanis. Fast algorithms for
robust pca via gradient descent. In Advances in neural information processing systems  pages
4152–4160  2016.

[29] Karl Bringmann  Pavel Kolev  and David Woodruff. Approximation algorithms for l0-low rank
approximation. In Advances in Neural Information Processing Systems  pages 6648–6659 
2017.

[30] Ricardo Otazo  Emmanuel Candès  and Daniel K Sodickson. Low-rank plus sparse matrix
decomposition for accelerated dynamic mri with separation of background and dynamic compo-
nents. Magnetic Resonance in Medicine  73(3):1125–1136  2015.

[31] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust pca via outlier pursuit. In

Advances in Neural Information Processing Systems  pages 2496–2504  2010.

[32] Javad Mashreghi. Representation theorems in Hardy spaces  volume 74. Cambridge University

Press  2009.

[33] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to escape

saddle points efﬁciently. arXiv preprint arXiv:1703.00887  2017.

[34] Christos Boutsidis  Petros Drineas  and Malik Magdon-Ismail. Near-optimal column-based

matrix reconstruction. SIAM Journal on Computing  43(2):687–717  2014.

11

[35] Yining Wang and Aarti Singh. Provably correct algorithms for matrix column subset selection

with selectively sampled data. arXiv preprint arXiv:1505.04343  2015.

[36] Zhao Song  David P Woodruff  and Peilin Zhong. Relative error tensor low rank approximation.
In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms  pages
2772–2789. SIAM  2019.

[37] Zhao Song  David P Woodruff  and Peilin Zhong. Towards a zero-one law for entrywise low

rank approximation. arXiv preprint arXiv:1811.01442  2018.

[38] Amit Deshpande  Madhur Tulsiani  and Nisheeth K Vishnoi. Algorithms and hardness for
subspace approximation. In Proceedings of the twenty-second annual ACM-SIAM symposium
on Discrete Algorithms  pages 482–496. Society for Industrial and Applied Mathematics  2011.
[39] Ravindran Kannan  Santosh Vempala  et al. Spectral algorithms. Foundations and Trends R(cid:13) in

Theoretical Computer Science  4(3–4):157–288  2009.

12

,ALEXANDROS GEORGOGIANNIS
Chen Dan
Hong Wang
Hongyang Zhang
Yuchen Zhou
Pradeep Ravikumar