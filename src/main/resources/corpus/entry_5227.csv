2018,Stochastic Composite Mirror Descent: Optimal Bounds with High Probabilities,We study stochastic composite mirror descent  a class of scalable algorithms able to exploit the geometry and composite structure of a problem. We consider both convex and strongly convex objectives with non-smooth loss functions  for each of which we establish high-probability convergence rates optimal up to a logarithmic factor. We apply the derived computational error bounds to study the generalization performance of multi-pass stochastic gradient descent (SGD) in a non-parametric setting. Our high-probability generalization bounds enjoy a logarithmical dependency on the number of passes provided that the step size sequence is square-summable  which improves the existing bounds in expectation with a polynomial dependency and therefore gives a strong justification on the ability of multi-pass SGD to overcome overfitting. Our analysis removes boundedness assumptions on subgradients often imposed in the literature. Numerical results are reported to support our theoretical findings.,Stochastic Composite Mirror Descent: Optimal

Bounds with High Probabilities

Shenzhen Key Laboratory of Computational Intelligence  Department of Computer Science
and Engineering  Southern University of Science and Technology  Shenzhen 518055  China

Yunwen Lei and Ke Tang∗

leiyw@sustc.edu.cn tangk3@sustc.edu.cn

Abstract

We study stochastic composite mirror descent  a class of scalable algorithms able
to exploit the geometry and composite structure of a problem. We consider both
convex and strongly convex objectives with non-smooth loss functions  for each
of which we establish high-probability convergence rates optimal up to a loga-
rithmic factor. We apply the derived computational error bounds to study the
generalization performance of multi-pass stochastic gradient descent (SGD) in a
non-parametric setting. Our high-probability generalization bounds enjoy a loga-
rithmical dependency on the number of passes provided that the step size sequence
is square-summable  which improves the existing bounds in expectation with a
polynomial dependency and therefore gives a strong justiﬁcation on the ability
of multi-pass SGD to overcome overﬁtting. Our analysis removes boundedness
assumptions on subgradients often imposed in the literature. Numerical results are
reported to support our theoretical ﬁndings.

1

Introduction

Stochastic gradient descent (SGD) has found wide applications in machine learning problems due
to its simplicity in implementation  low memory requirement and low computational complexity
per iteration  as well as good practical behavior [2  6  28  32  41]. As an iterative method  SGD
minimizes empirical errors by moving iterates along the direction of a negative gradient calculated
based on a loss function on a single training example or a batch of few examples. This strategy of
processing few examples per iteration makes SGD particularly suitable for large scale applications
with very large data points [2  41]  which are becoming ubiquitous in the big data era.
Stochastic composite mirror descent (SCMD) is a powerful extension of SGD based on two moti-
vations [12]. Firstly  it relaxes the Hilbert space structure of SGD by using a mirror map to capture
geometric properties of data from a Banach space [4  25]. Secondly  it exploits the problem structure
by separating  at every iteration  a data-ﬁtting term and a regularization term in structured optimization
problems to obtain a desired regularization effect  which arise naturally since a regularizer is often
introduced to either avoid overﬁtting or impose a priori information [12  37].
Although much theoretical analysis has been performed to understand the practical behavior of
SGD and SCMD  the existing theoretical results are still not quite satisfactory. Firstly  most of
the existing theoretical results are stated in expectation which inevitably ignore some information
on high-order moments of the random variable we are interested in. In practice  we may be more
interested in high-probability bounds to understand the variability of the learned model which is
also an important factor we should take into account when measuring the quality of models [32].
Secondly  the existing generalization bounds  stated in expectation  for SGD either are suboptimal

∗Corresponding author

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

or require to impose a smoothness assumption on loss functions [13  21]. Thirdly  a non-trivial
assumption on the boundedness of subgradients is often imposed in the literature to proceed with
the analysis [11  12  28  32]  especially in the derivation of high-probability bounds. However  this
boundedness assumption may not hold if the optimization is conducted in an unbounded domain 
under which scenario the derived bounds may not be intuitive.
In this paper  we aim to contribute towards a reﬁned analysis on both convergence rates and gen-
eralization properties of SCMD. We consider both general convex and strongly convex objectives 
for each of which we show that SCMD can achieve almost optimal convergence rates with high
probability  which match the minimax lower rates for stochastic approximation up to a logarithmic
factor [1  25]. In particular  we identify a constraint on step sizes to guarantee the boundedness of
iterates with high probability (up to a logarithmic factor). Furthermore  we apply these convergence
rates related to computational errors to establish high-probability generalization bounds for the model
trained by SGD through multiple passes over the training examples  which is a typical way of using
SGD to process large datasets [20]. Our generalization bounds do not require to impose smoothness
assumptions on loss functions and can be optimal up to a logarithmic factor. Surprisingly  we show
that estimation errors scale logarithmically with respect to (w.r.t.) the number of passes provided that
the step size sequence is square-summable  which implies that SGD may be immune to overﬁtting.
As a contrast  estimation error bounds based on stability arguments [13] and uniform deviation
arguments [21] scale polynomially w.r.t. the number of passes  which may not justify well the ability
of SGD in overcoming overﬁtting in practice. All our theoretical results are derived without any
boundedness assumptions on subgradients based on two tricks. The ﬁrst trick is to use a self-bounding
property of loss functions (Assumption 1) to show that a (weighted) summation of function values
can be controlled by step sizes (Lemma 2). The second trick is to show that conditional variances of
martingales in a one-step progress inequality of SCMD can be partially offset by some other terms in
the one-step progress inequality.
The paper is organized as follows. We introduce SCMD and state convergence rates in Section 2 and
Section 3  respectively. We study generalization bounds of SGD in Section 4. Discussions are given
in Section 5. Simulation results and conclusions are given in Section 6 and Section 7  respectively.

2 Stochastic Composite Mirror Descent

min

w∈W φ(w) = Ez[f (w  z)] + r(w) 

Many machine learning problems involve optimization problems of a composite structure [12  37]
(2.1)
where W is a Banach space with a norm (cid:107) · (cid:107)  F (w) := Ez[f (w  z)] is a data-ﬁtting term and
r : W → R+ is a simple regularizer possibly inducing sparsity. Here f : W × Z (cid:55)→ R+ is a
function with f (w  z) measuring the quality of a model indexed by w ∈ W on a random example
z = (x  y) drawn from a probability measure ˜ρ deﬁned in a sample space Z = X × Y with an input
space X ⊂ W∗ and an output space Y ⊂ R. We denote by Ez the expectation w.r.t. z  and by W∗
the dual of W with the dual norm (cid:107) · (cid:107)∗. A typical choice of the data-ﬁtting term takes the form
f (w  z) = (cid:96)((cid:104)w  x(cid:105)  y)  where (cid:96) : R × Y (cid:55)→ R+ is a loss function and (cid:104)w  x(cid:105) is the dual element
x ∈ W∗ acting on w ∈ W. With speciﬁc instantiations of loss functions (cid:96) and regularizers r  the
formulation (2.1) covers many famous machine learning problems in a unifying framework  including
least squares  support vector machines  logistic regression  lasso and elastic-net  etc [12  37].
As an extension of SGD  SCMD uses a strongly convex and Fréchet differentiable mirror map Ψ
to generate an appropriate Bregman distance DΨ(w  ˜w) := Ψ(w) − Ψ( ˜w) − (cid:104)w − ˜w ∇Ψ( ˜w)(cid:105) to
capture the involved non-Euclidean geometry [4  25]  where ∇Ψ( ˜w) denotes the gradient of Ψ at
˜w. Let w1 = 0 ∈ W and {ηt}t∈N be a positive step size sequence. Upon the arrival of zt at the
t-th iteration  SCMD calculates a subgradient f(cid:48)(wt  zt) ∈ ∂wf (wt  zt) as an unbiased estimate of
F (cid:48)(wt) ∈ ∂F (wt)  and updates the model as follows

(cid:2)(cid:104)w − wt  f(cid:48)(wt  zt)(cid:105) + r(w)(cid:3) + DΨ(w  wt).

(2.2)

w∈W ηt

wt+1 = arg min

Here ∂wf (wt  zt) :=(cid:8)g : f (w  zt) − f (wt  zt) ≥ (cid:104)w − wt  g(cid:105) for all w(cid:9) denotes the subdifferential

of f (·  zt) at wt. Intuitively  SCMD uses f(cid:48)(wt  zt) to form a ﬁrst-order approximation of f (·  zt) at
wt and uses the Bregman distance DΨ(w  wt) to keep wt+1 not far away from the current iterate.
The regularizer r is kept intact here for a regularization effect [12  37]. A typical choice of Ψ is the

2

2(cid:107)w(cid:107)2

p-norm divergence Ψp(w) = 1

setting p close to 1 [12  37]. Here (cid:107) · (cid:107)p is the p-norm deﬁned by (cid:107)w(cid:107)p =(cid:0)(cid:80)d

p (1 < p ≤ 2)  which works favorably for sparse problems by

i=1 |w(i)|p(cid:1)1/p for

w = (w(1)  . . .   w(d)) ∈ Rd. SCMD recovers SGD by taking Ψ = Ψ2 and r(w) = 0  stochastic
forward-backward splitting by taking Ψ = Ψ2 [11]  stochastic mirror descent by taking r(w) = 0 [24]
and stochastic mirror descent algorithm made sparse by taking Ψ = Ψp and r(w) = λ(cid:107)w(cid:107)1 [30].

3 Convergence Rates

Before stating our high-probability convergence rates  we introduce some assumptions. Throughout
the paper  we assume that the mirror map Ψ is Fréchet differentiable and σΨ-strongly convex in the
sense that DΨ(w  ˜w) ≥ 2−1σΨ(cid:107)w − ˜w(cid:107)2 for all w  ˜w ∈ W ⊂ Rd (σΨ > 0)  and f (w  z) is convex
w.r.t. the ﬁrst argument. We also always assume that Assumption 1 and Assumption 2 hold  the
sample space Z is bounded and supz∈Z f (0  z) < ∞.
Assumption 1. We assume that there exist A and B ≥ 0 such that the following inequalities hold
for any w ∈ W  z ∈ Z and any f(cid:48)(w  z) ∈ ∂f (w  z)  r(cid:48)(w) ∈ ∂r(w)

(cid:107)f(cid:48)(w  z)(cid:107)2∗ ≤ Af (w  z) + B and

(cid:107)r(cid:48)(w)(cid:107)2∗ ≤ Ar(w) + B.

(3.1)

This is a standard assumption and satisﬁed in many practical problems [11  41]. For example  Lemma
A.5 shows that r(w) = λ(cid:107)w(cid:107)p
p satisﬁes the second inequality of (3.1) with (cid:107) · (cid:107) = (cid:107) · (cid:107)p(1 ≤ p ≤ 2) 
A = 2λp(p − 1) and B = λp(2 − p). Furthermore  if f (w  z) = (cid:96)((cid:104)w  x(cid:105)  y)  then Lemma A.4
shows that (cid:107)f(cid:48)(w  z)(cid:107)2∗ = |(cid:96)(cid:48)((cid:104)w  x(cid:105)  y)|2(cid:107)x(cid:107)2∗ would satisfy the ﬁrst inequality of (3.1) if

|(cid:96)(cid:48)(a  y)|2 ≤ ˜A(cid:96)(a  y) + ˜B 

∀a ∈ R  y ∈ Y

(3.2)
for some ˜A  ˜B > 0 [41]  where (cid:96)(cid:48)(a  y) denotes a subgradient of (cid:96) w.r.t. the ﬁrst argument. Many
popular loss functions satisfy (3.2)  including the p-norm hinge loss (cid:96)(a  y) = max{0  1 − ya}p
(1 ≤ p ≤ 2) [34]  the logistic loss (cid:96)(a  y) = log(1 + exp(−ya)) for classiﬁcation  and the p-th
power absolute distance loss (cid:96)(a  y) = |a − y|p (1 ≤ p ≤ 2)  the Huber loss (cid:96)(a  y) = (a − y)2 if
|a − y| ≤ 1 and (cid:96)(a  y) = 2|a − y| − 1 otherwise for regression [41]. We refer the interested readers
to [41] for constants ˜A  ˜B in (3.2) with different loss functions (cid:96).
Assumption 2. We assume the existence of σF   σr ≥ 0 such that

F (w) − F ( ˜w) − (cid:104)w − ˜w  F (cid:48)( ˜w)(cid:105) ≥ σF DΨ(w  ˜w) 
r(w) − r( ˜w) − (cid:104)w − ˜w  r(cid:48)( ˜w)(cid:105) ≥ σrDΨ(w  ˜w)

(3.3)

hold for all w  ˜w ∈ W and any F (cid:48)( ˜w) ∈ ∂F ( ˜w)  r(cid:48)( ˜w) ∈ ∂r( ˜w).
The case σφ := σF + σr = 0 corresponds to general convex objectives  while the case σφ > 0
corresponds to strongly convex objectives. Let w∗ = arg minw∈W φ(w) be the minimizer of φ in W
with the minimal norm. We always assume (cid:107)w∗(cid:107) < ∞ in this paper.
Our theoretical analysis is based on the following lemma quantifying the one-step progress of SCMD
measured by Bregman distance  which shows how DΨ(w  wt) would change in a single iteration.
Lemma 1. Let {wt}t∈N be generated by (2.2)  then the following inequality holds for any w ∈ W

DΨ(w  wt+1) − DΨ(w  wt) ≤ ηt(cid:104)w − wt  f(cid:48)(wt  zt)(cid:105) + ηt(r(w) − r(wt))

+ σ−1

Ψ η2

t

−σrηtDΨ(w  wt+1).

(3.4)

and a G ∈ R is imposed to control(cid:80)T
Assumption 1 to replace Bt with At. Equation (3.6) allows us to control(cid:80)T

Existing one-step progress inequality can be found in the literature with At replaced by Bt :=
(cid:107)f(cid:48)(wt  zt)(cid:107)2∗ + (cid:107)r(cid:48)(wt)(cid:107)2∗  see  e.g.  [12]. Then  a non-trivial assumption as Bt ≤ G for all t ∈ N
t ). We reﬁne these discussions by using
t=1 η2
t )
without imposing any boundedness assumptions on subgradients. In our discussion for strongly
convex objectives  we require to divide both sides of (3.4) by η2
t . In this way  Eq. (3.7) plays an
analogous role in removing boundedness assumptions in the strongly convex case. Both proofs of
Lemma 1 and Lemma 2 are given in Supplementary Material B.

t At by O((cid:80)T

t=1 η2

t=1 η2

t=1 η2

(cid:0)Af (wt  zt) + Ar(wt) + 2B(cid:1)
(cid:123)(cid:122)
(cid:124)
(cid:125)
t Bt by O((cid:80)T

:=At

3

Lemma 2. Let {wt}t∈N be the sequence produced by (2.2) with ηt ≤ (2A)−1σΨ. Then  we have

where C1 = supz∈Z f (0  z) + r(0) + A−1B. Furthermore  if ηt+1 ≤ ηt  then for all t ∈ N

t(cid:88)

k=1

(cid:107)wt+1(cid:107)2 ≤ 2C1σ−1

Ψ

∀t ∈ N 

ηk 

t(cid:88)
t(cid:88)
(cid:0)f (wk  zk) + r(wk)(cid:1) ≤ 2C1
(cid:0) t(cid:88)
(cid:0)f (wk  zk) + r(wk)(cid:1) ≤ 2C1t + 2C1

η2
k

k=1

k=1

η2
k 

(cid:1)η−1

t

.

ηk

k=1

t(cid:88)

k=1

(3.5)

(3.6)

(3.7)

3.1 Convex Objectives

We study the behavior of SCMD for convex objectives with σφ = 0. The assumption(cid:80)∞
martingale(cid:80)t
(cid:80)∞

t < ∞
is satisﬁed if ηt = η1t−θ with θ > 1/2 or ηt = η1(t logβ(et))− 1
2 with β > 1. Our idea is to take
a summation of Eq. (3.4) with w = w∗  and show that the conditional variance of the involved
k=1 ηk(cid:104)w∗ − wk  f(cid:48)(wk  zk) − Ezk [f(cid:48)(wk  zk)](cid:105) can be partially offset by some other
terms. The proofs of Theorems 3 and 4 are given in Supplementary Material C.
Theorem 3. Let {wt}t∈N be the sequence produced by (2.2) with ηt ≤ (2A)−1σΨ  ηt+1 ≤ ηt and
t < ∞. Then  there exists a constant C2 independent of T (explicitly given in the proof) such
that for any δ ∈ (0  1) the following inequality holds with probability at least 1 − δ

t=1 η2

t=1 η2

max
1≤t≤T

(3.8)
Remark 1. Although implemented in a possibly unbounded domain  Theorem 3 shows that {wt}t∈N
by (2.2) falls into a bounded ball (up to a logarithmic factor) with high probabilities. Intuitively  this
suggests that SCMD is immune to overﬁtting if we take appropriate step sizes. In this case  we can
run SCMD with many iterations without essentially harming the quality of the output model.

.

(cid:107)wt(cid:107)2 ≤ C2 log

T
δ

Based on Theorem 3  we establish high-probability convergence rates for a weighted average of
iterates without any assumptions on the boundedness of iterates. In Theorem 4 and Corollary 5  we
establish bounds on suboptimality of objectives w.r.t. any w and an optimal solution w∗  respectively.
Theorem 4. Let w ∈ W and δ ∈ (0  2/e). Let ¯w(1)
t=1 ηtwt be a weighted
average of the ﬁrst T iterates. Under the conditions of Theorem 3  with probability 1 − δ we have

T = (cid:0)(cid:80)T

t=1 ηt

T ) − φ(w) ≤(cid:16) T(cid:88)

(cid:17)−1(cid:0)2C3DΨ(w  0) + C4

ηt

φ( ¯w(1)

t=1

(cid:1)−1(cid:80)T
(cid:1) log

3
2

2T
δ

 

(3.9)

where C3 and C4 are two constants (explicitly given in the proof) independent of T .
Remark 2. A similar high-probability bound was established for SCMD in [12]. However  their
discussion needs to impose an additional almost-sure boundedness assumption on iterates  i.e. 
(cid:107)wt(cid:107)2 ≤ G for a G > 0 and all t ∈ N. These boundedness assumptions on either subgradients
or iterates are fundamental to the existing analysis but hard to check in practice. Moreover  the
high-probability analysis makes these assumptions non-trivial to remove since one also needs to
consider high-order moments of random variables.
Corollary 5. If δ ∈ (0  2/e) and conditions of Theorem 4 are satisﬁed  then (3.9) holds with
probability 1 − δ with w = w∗. Furthermore  if we choose ηt = η1t−θ with θ > 1/2  then with
probability 1−δ we have φ( ¯w(1)
with β > 1  then with probability 1 − δ we have φ( ¯w(1)

T )−φ(w∗) = O(cid:0)T θ−1 log
The convergence rate O(cid:0)(cid:0)T −1 logβ T(cid:1) 1
factor [1]  which follows directly from Theorem 4 and(cid:80)T

(cid:1); if we choose ηt = η1(t logβ(et))− 1
(cid:1).
T ) − φ(w∗) = O(cid:0)(cid:0)T −1 logβ T(cid:1) 1
(cid:1) in Corollary 5 is optimal up to a logarithmic

t=1 t−θ ≥ (1− θ)−1(T 1−θ − 1)  θ ∈ (0  1).

3
2 T
δ

3
2 T
δ

3
2 T
δ

2 log

2 log

2

We omit the proof for brevity.

4

t=1 ηt

In Theorem 6  we give sufﬁcient conditions for the almost sure ﬁniteness of limt→∞ DΨ(w∗  wt)

(cid:0)φ(wt) − φ(w∗)(cid:1). As a direct corollary  we also establish convergence rates with

probability one in Corollary 7. Theorem 6 is a part of Proposition E.3 to be presented and proved in
Supplementary Material E  while the proof of Corollary 7 is omitted for brevity.
t < ∞. Then {DΨ(w∗  wt)}t converges almost
surely (a.s.) to a non-negative random variable and limt→∞ DΨ(w∗  wt) < ∞ a.s.. Furthermore  if

and(cid:80)∞
Theorem 6. Consider {wt}t∈N by (2.2) with(cid:80)∞
ηt ≤ (2A)−1σΨ and ηt+1 ≤ ηt  then(cid:80)∞
θ > 1/2  then limT→∞ T 1−θ(cid:0)φ( ¯w(1)
T ) − φ(w∗)(cid:1) < ∞ a.s.. If we choose ηt = η1(t logβ(et))− 1
2(cid:0)φ( ¯w(1)
(cid:1) 1
with β > 1  then limT→∞(cid:0) T

(cid:0)φ(wt) − φ(w∗)(cid:1) < ∞ a.s..
T ) − φ(w∗)(cid:1) < ∞ a.s..

Corollary 7. Let {wt}t∈N be produced by (2.2) and η1 ≤ (2A)−1σΨ. If we choose ηt = η1t−θ with

t=1 η2

t=1 ηt

2

logβ T

3.2 Strongly Convex Objectives

t

t

) − φ(w∗) with ¯w(2)

martingale(cid:80)t

We now turn to strongly convex objectives with σφ > 0. In Theorem 8  we establish high-probability
bounds for both (cid:107)wt − w∗(cid:107)2 and φ( ¯w(2)
being another weighted average of
the ﬁrst t iterates  for each of which we derive optimal convergence rates up to a logarithmic factor
[1]. The optimality means that not only the dependency on t but also the dependency on the strong-
convexity parameter σφ can not be improved up to a logarithmic factor [16  28] (σφ is often chosen
to be very small in practical learning problems [28  31]). It should be mentioned that our analysis
removes boundedness assumptions on subgradients in the literature [28]. Our idea is to take a
weighted summation of (3.4) with w = w∗  and show that the conditional variance of an involved
k=1(k + t0 + 1)(cid:104)w∗ − wk  f(cid:48)(wk  zk) − Ezk [f(cid:48)(wk  zk)](cid:105) can be partially offset by
another term in this weighted summation of (3.4)  which is another trick to remove boundedness
assumptions on subgradients. We also give a sufﬁcient condition on the almost sure convergence of
wt to w∗ in Theorem 9. The proof of Theorem 8 is given in Supplementary Material D. Theorem 9 is
a part of Proposition E.3 to be presented in Supplementary Material E.
4 ). Let {wt}t∈N be produced by (2.2) with ηt =
Theorem 8. Assume σφ > 0 and δ ∈ (0  e− 1
k=1(k+t0+1)wk  t ∈ N.

t =(cid:0)(cid:80)t

  where t0 ≥ 16A log T

Then  the following inequalities hold with probability 1 − δ for all t = 1  . . .   T

. Let ¯w(2)

σφt+2σF +σφt0

σφσΨ

2

δ

k=1(k+t0+1)(cid:1)−1(cid:80)t
) − φ(w∗) ≤ (cid:101)CT

(cid:107)w∗ − wt(cid:107)2 ≤

CT

Moreover  the dependencies of CT and (cid:101)CT on T /δ are logarithmic. The dependencies of CT and (cid:101)CT
Theorem 9. Let {wt}t∈N be the sequence produced by (2.2) with σφ > 0. If(cid:80)∞
(cid:80)∞
t=1 ηt = ∞ and

φ are quadratic and linear  respectively.

t < ∞  then limt→∞ DΨ(w∗  wt) = 0 a.s..

and φ( ¯w(2)

on σ−1

t + t0 + 1

(3.10)

t=1 η2

t

.

t

4 Generalization Error Bounds

i=1 (cid:96)(cid:0)h(xi)  yi
(cid:80)n

(cid:2)(cid:96)(h(x)  y)(cid:3)
(cid:1). The best model minimizing the generalization error then becomes

Here we apply our high-probability convergence rates for SCMD to establish generalization error
bounds for SGD. In this setting  we assume a training sample z = {z1  . . .   zn} of size n ∈ N is drawn
independently from a probability measure ρ deﬁned on the sample space Z  and our aim is to learn a
hypothesis h : X (cid:55)→ R from a hypothesis space W with good generalization performance. The quality
of h at (x  y) is quantiﬁed by (cid:96)(h(x)  y)  where (cid:96) : R × Y (cid:55)→ R+ is convex w.r.t. the ﬁrst argument.
The generalization error and empirical error of h are deﬁned respectively by E(h) = Ez
and Ez(h) = 1
hρ = arg minh E(h). We consider a non-parametric learning setting with W being a reproducing
kernel Hilbert space (RKHS) associated to a Mercer kernel K : X × X (cid:55)→ R which is continuous 
symmetric and positive semi-deﬁnite [9  34]. In this learning setting  the candidate models take the
form hw(x) = (cid:104)w  Kx(cid:105) with w ∈ W. For brevity  we denote the norm in the RKHS W by (cid:107) · (cid:107)2
and introduce abbreviations E(w) = E(hw) Ez(w) = Ez(hw). We assume (3.2) and apply the SGD
scheme to minimize Ez(w). To be speciﬁc  we let w1 = 0. At the t-th iteration  we randomly choose
an index jt from the uniform distribution over {1  . . .   n} and produce wt+1 by

n

wt+1 = wt − ηt(cid:96)(cid:48)(cid:0)(cid:104)wt  Kxjt

(cid:1)Kxjt

 

(cid:105)  yjt

t ∈ N.

(4.1)

5

2(cid:107)w(cid:107)2

L2

ρX

K (L2

+ λ(cid:107)w(cid:107)2

ρX )  where LK : L2

It is clear that (4.1) is a speciﬁc instantiation of (2.2) with Ψ(w) = 1
2  f (w  z) =
(cid:96)((cid:104)w  Kx(cid:105)  y)  r(w) = 0 and ˜ρ in Section 2 being the uniform distribution over {z1  . . .   zn}2.
Therefore  the objective function to which SGD is applied becomes φ(w) = Ez(w).
To state our generalization bounds  we need to introduce an assumption on a polynomial decay rate
of approximation errors.
Assumption 3. We assume the approximation error D(λ) := inf w∈W E(w)−E(hρ)+λ(cid:107)w(cid:107)2
2 enjoys
a polynomial decay with exponent 0 < α ≤ 1 in the sense D(λ) ≤ cαλα ∀λ > 0  where cα > 0.
Remark 3. Assumption 3 is standard in learning theory and satisﬁed under some mild conditions
on the smoothness of the function hρ and the representation power of W [9  33]. If (cid:96) is smooth 
2  which quantiﬁes the
ρX (square-integrable function class with marginal measure

then D(λ) can be controlled by (cid:101)D(λ) := inf w∈W (cid:107)hw − hρ(cid:107)2
ρX ) and is well studied in approximation theory. (cid:101)D(λ) decays polynomially with α ∈ (0  1] if

approximation of hρ by RKHS in L2

hρ ∈ Lα/2
ρX is the integral operator associated to K [9  Proposition
8.5]. Similar results hold if (cid:96) is Lipschitz continuous. Assumption 3 also holds if we use Gaussian
kernels with ﬂexible variances and distributions with geometric noise conditions [35]. It should be
mentioned that kernels need not to be universal for Assumption 3 since it concerns the target function
hρ  which may admit more regularity (e.g.  expressed by LK) than continuity  while universality
means that D(λ) → 0 as λ → 0 for all continuous hρ [34].
We now establish a generalization error bound for a weighted average of iterates produced by (4.1) to
be proved in Supplementary Material F  which is derived by decomposing the excess generalization
T ) − E(hρ) into three components: an estimation error  an approximation error and a
error E( ¯w(1)

computational error. As we will see in the proof  the term(cid:0)(cid:80)T

(cid:1)−α is due to the approximation

and computational error  while the term n− α
1+α is due to the estimation and approximation error. The
bound becomes n− α
δ for sufﬁciently large T   which enjoys a logarithmic dependency on
T and demonstrates the ability of SGD to avoid overﬁtting.
Theorem 10. Let {wt}t∈N be the sequence produced by (4.1) with ηt ≤ (2A)−1σΨ  ηt+1 ≤ ηt
t=1 ηt ≥ 1 and
δ ∈ (0  2/e)  the following inequality holds with probability at least 1 − δ

t < ∞. Suppose Assumption 3 holds. Then  for any T satisfying(cid:80)T

and(cid:80)∞

ρX (cid:55)→ L2

t=1 η2

1+α log

t=1 ηt

3
2 8T

E( ¯w(1)

T ) − E(hρ) ≤ C5 max

  n− α

1+α

3
2

log

8T
δ

 

(4.2)

where C5 is a constant independent of T (explicitly given in the proof).

generalization bounds  as shown in Corollary 11. The bound O(cid:0)n− α

We consider speciﬁc step sizes in Theorem 10 and choose an appropriate time index to get concrete

(cid:1) coincides with

n
δ

O(n− α
1+α log n) (up to a logarithmic factor) in expectation for convex and smooth loss functions [21] 
and largely improves the bound O(n− α
1+2α log n) in expectation for convex and non-smooth loss
functions [21]. In particular  if α = 1 we derive the optimal bound O(n− 1
δ ) in a general
case with neither Bernstein conditions on variances nor capacity assumptions on hypothesis spaces
(up to a logarithmic factor). It is also clear that SGD with different step sizes can achieve similar
generalization bounds. However  the computational complexity to fulﬁll this statistical potential can
be signiﬁcantly different. Corollary 11  with the proof omitted  follows directly from Theorem 10

and(cid:80)T
Corollary 11. Consider {wt}t∈N by (4.1) and δ ∈ (0  2/e). Let Assumption 3 hold and(cid:80)T

t=1 t−θ ≥ (1 − θ)−1(T 1−θ − 1)  θ ∈ (0  1). Denote (cid:100)a(cid:101) the least integer no less than a.

t=1 ηt ≥ 1.

2 log

2 n

3+β

1+α log

3+αβ

2

(a) If we take ηt = η1t−θ with η1 ≤ (2A)−1 and θ ∈ (1/2  1)  then with probability 1 − δ that

(cid:26)(cid:16) T(cid:88)

(cid:17)−α

ηt

t=1

(cid:27)

E( ¯w(1)

If we further take T ∗ =(cid:6)n

T ) − E(hρ) = O

(1+α)(1−θ)(cid:7)  then we get E( ¯w(1)

T −α(1−θ) + n− α

1

(cid:18)(cid:16)

(cid:17)

(cid:19)
T ∗ ) − E(hρ) = O(cid:0)n− α

T
δ

log

1+α

3
2

.

(cid:1).

3
2 n
δ

1+α log

2ρ is related to the draw of training examples while ˜ρ is related to the draw of indices for SGD.

6

(cid:18)(cid:16)

(cid:17)

(cid:19)

.

(b) If we take ηt = η1(t logβ(et))− 1

2 with η1 ≤ (2A)−1 and β > 1  then with probability 1 − δ that

E( ¯w(1)

If we further take T ∗ =(cid:6)n

T ) − E(hρ) = O

T − α

1+α(cid:7)  then we get E( ¯w(1)

2 log

2

αβ

1+α

2 T + n− α

T ∗ ) − E(hρ) = O(cid:0)n− α

log

3
2

T
δ

1+α log

3+αβ

2

(cid:1).

n
δ

It should be noted that our discussions depend on the existence of a minimizer of Ez(·) over the RKHS
with a ﬁnite norm. This assumption can be relaxed to the existence of a minimizer of E(·) over the
RKHS with a ﬁnite norm to derive similar generalization bounds. Indeed  one can perform deductions
similar to the proof of Theorem 3 by taking w in (3.4) to be the minimizer of E(·). However  in this
case it becomes a challenge to derive estimation error bounds with a logarithmic dependency on T .

5 Related Work and Discussions

5.1 Convex Objectives

√

T ) were established for online gradient descent
For general convex objectives  regret bounds O(
with T iterations [44]  from which one can directly derive convergence rates O(T − 1
2 ) for SGD
with some averaging schemes. This result was extended to stochastic forward-backward split-
ting [11]. A convergence rate O(T − 1
2 log T ) was established for the T -th individual iterate of
SGD [32]. All the above mentioned rates were stated in expectation and derived based on an
assumption E[(cid:107)f(cid:48)(wt  zt)(cid:107)2∗ + (cid:107)r(cid:48)(wt)(cid:107)2∗] ≤ G for a G ≥ 0 and t ∈ N. This boundedness as-
sumption was successfully removed for studying convergence rates in expectation under some
smoothness assumption [23  40  42] or Assumption 1 [30]. As compared to these convergence
rates in expectation  high-probability convergence rates were much less studied and were often
based on a stronger assumption on the almost sure boundedness of subgradients. Under the assump-
tion max{DΨ(w∗  wt)  supz (cid:107)f(cid:48)(wt  z)(cid:107)∗} ≤ G for a G > 0 and all t ∈ N  it was shown with
probability 1 − δ that φ( ¯w(1)
T deﬁned in Theorem 4 [12  24].
High-probability bounds were also established for stochastic dual averaging under the boundedness as-
sumption on iterates and subgradients [37]. In our discussion  we show that the same high-probability
convergence rate (up to a logarithmic factor) holds without any boundedness assumptions on either
the iterates {wt} or the associated subgradients. In particular  we show that {wt}t≤T automatically
1 − δ that (cid:107)wt − w∗(cid:107)2
δ ) for the particular SGD [19]. However  the discussion
in [19] requires a stronger assumption on the Hölder continuity of loss functions which excludes
non-differentiable loss functions such as hinge loss and the absolute loss satisfying (3.2). Secondly 
they only consider the one-pass SGD where each training example is used only once.

falls into a ball with radius O((cid:112)log T /δ) with high probability. It was shown with probability
(cid:0)φ(wt) − φ(w∗)(cid:1)  while

We also give a sufﬁcient condition for almost sure ﬁniteness of(cid:80)∞

T ) − φ(w∗) = O(cid:0)T − 1

(cid:1) for ¯w(1)

2 = O((cid:107)w∗(cid:107)2

2 log T

2 log

1
2 1
δ

most results on almost sure convergence are achieved for strongly convex objectives.

t=1 ηt

5.2 Strongly Convex Objectives
For λ-exp-concave loss functions  a regret bound O(λ−1 log T ) was established for an online Newton

method [15]  which implies convergence rates O(cid:0)(λT )−1 log T(cid:1) for some average of iterates pro-

duced by the stochastic counterpart. This result was extended to online forward-backward splitting
[11] and SCMD [12] applied to λ-strongly convex objectives. Optimal convergence rates O((λT )−1)
for the suboptimality of objective values were derived based on a sufﬁx averaging scheme [28]  a
epoch-GD scheme based on a doubling trick [14] and a weighted averaging with a weight of t + 1 for
wt [16]. However  the above mentioned results are all associated to convergence rates in expectation
and require to impose boundedness assumptions on subgradients encountered during the iterations.
This boundedness assumption was relaxed as Ez[(cid:107)f(cid:48)(wt  z)(cid:107)2∗] ≤ A1 + B1(cid:107)F (cid:48)(wt)(cid:107)2∗ for SGD [6]
with A1  B1 ≥ 0  which was further removed for SGD [26] and stochastic mirror descent [17] by
imposing smoothness assumptions on loss functions. All the above mentioned results are stated in ex-

pectation. With probability 1−δ  it was shown (cid:107)wT −w∗(cid:107)2 = O(cid:0)(λ2T )−1+(λT )−1 log(δ−1 log T )(cid:1)
for SGD [28]. High-probability convergence rates O(cid:0)(λT )−1 log(δ−1 log T )(cid:1) were also established

for the suboptimality of objective values for the T -th iterate of the epoch-GD [14]. These two high-
probability rates were derived based on an assumption on almost sure boundedness of subgradients

7

which is more challenging to remove [14  28]. As a comparison  we establish the same convergence
rate (up to a logarithmic factor) for a more general SCMD without boundedness assumptions on
subgradients. Sufﬁcient conditions as in Theorem 9 were established for almost sure convergence of
SGD [5  26] and stochastic mirror descent [17]  which were extended to SCMD in Theorem 9.

5.3 Generalization Error Bounds

of SGD with T iterates scales as O(n−1(cid:80)T

While computational complexity of SGD has been extensively studied in the optimization community 
there is much less work on the generalization property of the model trained by SGD. Classical
generalization bounds only hold for one-pass SGD [24  27  28  32  36  38  39] where each training
example can be used at most once. In practice  however  multiple passes are often used to produce a
model with good generalization behavior [13]. The landmark work in [7] developed a framework
to analyze generalization performance of multi-pass stochastic learning algorithms by taking into
account the computational complexity of learning algorithms. Under this framework  the interplay
among estimation errors  computational errors and approximation errors can be studied  showing that
an implicit regularization can be achieved in the absence of penalization or constraints by tuning either
the step size or the number of passes (the iteration number divided by the training set size) [13  20  21 
29]. In a parametric setting  it was shown that SGD is algorithmically stable and the stability measure
t=1 ηt) [13]  based on which a generalization bound
E[E( ¯w(1)
n) and T = O(n) without
considering approximation errors. The discussion in [13] requires to impose a smoothness assumption
on loss functions. Generalization analysis was considered separately for smooth and non-smooth loss
functions [21]. For smooth loss functions  it was shown E[E( ¯w(1)
1+α log n) for
√
α+1(cid:101) [21]  based on the stability property of SGD established in [13]. For
√
ηt = η1/
non-smooth loss functions  it was shown E[E( ¯w(1)
2α+1 log n) for ηt = η1/
t
2α+1(cid:101)  by controlling estimation errors with Rademacher complexities [3  21]. Still 
and T = (cid:100)n
the bounds in [13  21] require to impose a boundedness assumption on subgradients and are stated
in expectation. As a comparison  we establish high-probability bounds without any boundedness
assumptions on subgradients. Furthermore  our generalization analysis extends the analysis in [13] to
non-smooth loss functions and substantially improve the bound O(n− α
2α+1 log n) [21] in this setting.

√
2 ) was established for ηt = O(1/

T )] − inf w∈W E(w) = O(n− 1

T )] − E(hρ) = O(n− α

T )] − E(hρ) = O(n− α

(cid:1) in Corollary 11 is optimal in the sense that it

t with T = (cid:100)n

2

2

The generalization error bound O(cid:0)n− α

1+α log

3+αβ

2

n
δ

T )−Ez( ¯w(1)

matches the best available bound for Tikhonov regularization (up to a logarithmic factor) [9  21  34].
We achieve this improvement by controlling better estimation errors. Speciﬁcally  estimation errors
were shown to scale polynomially w.r.t. the number of passes [13  21]  which dominate the other two
errors for large T . In this way  one needs to tune T to balance the estimation  approximation and
computational errors. As a comparison  we show bounds scaling logarithmically w.r.t. the number of
passes for E( ¯w(1)
T ) (Theorem 10). This implies that estimation errors will never essentially
dominate the other two errors and one can run SGD with a sufﬁcient number of passes with little
overﬁtting if step sizes are square-summable  due to the key observation on the almost boundedness
of iterates established in Theorem 3. Another trick in getting almost optimal bounds includes the
use of Assumption 3 to control E(wλ) − Ez(wλ) with a linear (instead of quadratic) function of
supz f (wλ  z) and to select a suitable λ  where wλ = arg minw∈W E(w) + λ(cid:107)w(cid:107)2
2. Optimal learning
rates were given for multi-pass SGD with the least squares loss function [10  20  29]. However 
their analysis is based on an integral operator approach and does not apply to general loss functions.
Generalization bounds for SGD were also studied from a PAC-Bayesian perspective [22]. However 
the high-probability bounds there require to impose Lipschitz continuity  smoothness and strong
convexity assumptions on loss functions  and ignore computational and approximation errors [22].

6 Simulations

Our analysis implies that SGD can be run with a sufﬁcient number of iterations with little overﬁtting
if step sizes are square-summable  which meanwhile can achieve similar generalization performance
with different computational complexities. In this section  we include some experimental results to
validate these theoretical ﬁndings. We apply SGD (4.1) with a linear kernel Kx = x and the hinge
loss (cid:96)(a  y) = max{0  1 − ya} to several binary classiﬁcation datasets (ADULT  GISETTE  IJCNN 
MUSHROOMS  PHISHING and SPLICE). All these datasets  described in Supplementary Material

8

(cid:1)−1(cid:80)t

t =(cid:0)(cid:80)t

G  can be download from the LIBSVM website [8]. We consider polynomially decaying step sizes of
the form ηt = 5t−θ with θ ∈ {0.25  0.51  0.75} (we consider θ = 0.51  instead of θ = 0.5  since the
associated step size sequence is square-summable). We repeat experiments 12 times and report the
average of results. In Figure 1  we plot test errors of ¯w(3)
k=˜t+1 ηkwk versus
the number of passes (the iteration number divided by the training set size)  where ˜t = 2(cid:98)log2 t(cid:99)−1.
returns an α-sufﬁx average of iterates [28] with α ∈ [1/2  3/4] and one can adapt the
Intuitively  ¯w(3)
proof of Theorem 4 to show that ¯w(3)
. Moreover  ¯w(3)
j=1 ηjwj with k = 20  21  22  . . .. From Figure 1 
we see that SGD is resistant to overﬁtting for appropriate step sizes. For example  we observe no
overﬁtting even if the number of passes exceeds 1000 for SGD with θ ∈ {0.51  0.75}. Moreover 
SGD with θ ∈ {0.51  0.75} can achieve similar generalization errors on ADULT  IJCNN  PHISHING
and SPLICE  towards which SGD with θ = 0.51 requires a signiﬁcantly smaller number of passes.
This is well consistent with Corollary 11.

is easily computable on-the-ﬂy by storing only(cid:80)k

k=˜t+1 ηk

t

enjoys similar generalization bounds as ¯w(1)

t

t

t

(a) ADULT.

(b) GISETTE.

(c) IJCNN.

(d) MUSHROOMS.

(e) PHISHING.

(f) SPLICE.

Figure 1: Test errors versus the number of passes.

7 Conclusions

In this paper  we establish a rigorous theoretical foundation for SCMD by providing optimal conver-
gence rates (up to a logarithmic factor) in the stochastic optimization setting without boundedness
assumptions on either subgradients or iterates  which in turn also shed new insights on the generaliza-
tion behavior of the multi-pass SGD in the statistical learning theory setting. In particular  we justify
the immunity of multi-pass SGD to overﬁtting by giving estimation error bounds with a logarithmic
dependency on the number of passes for square-summable step sizes  while existing bounds scale
polynomially [13  21]. This improvement is based on the key observation on the almost boundedness
of iterates with high probability. Our generalization analysis of SGD also substantially improves
learning rates in [21]  removes bounded subgradient assumptions in [13  21  22]  removes smoothness
assumptions in [13  22] and is performed in high probability instead of in expectation [13  21]. It
would be interesting to extend our results to a non-convex setting [43] and to general mirror descent
algorithms with a non-differentiable mirror map [18].

Acknowledgments

This work is supported in part by the National Key Research and Development Program of
China (Grant No. 2017YFB1003102)  the National Natural Science Foundation of China (Grant

9

10-210-1100101102Passes0.40.60.811.21.41.61.82Test Error10-1100101Passes101102Test Error10-210-1100101102Passes0.20.250.30.350.40.450.5Test Error10-1100101102103Passes100Test Error10-1100101102Passes0.20.250.30.350.40.450.50.550.60.65Test Error100101102103Passes100101102Test ErrorNos. 61806091 and 61672478)  the Science and Technology Innovation Committee Founda-
tion of Shenzhen (Grant No. ZDSYS201703031748284) and Shenzhen Peacock Plan (Grant No.
KQTD2016112514355531).

References

[1] A. Agarwal  M. J. Wainwright  P. L. Bartlett  and P. K. Ravikumar. Information-theoretic lower bounds on
the oracle complexity of convex optimization. In Advances in Neural Information Processing Systems 
pages 1–9  2009.

[2] F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate

O(1/n). In Advances in Neural Information Processing Systems  pages 773–781  2013.

[3] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results.

Journal of Machine Learning Research  3:463–482  2002.

[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex

optimization. Operations Research Letters  31(3):167–175  2003.

[5] L. Bottou. On-line learning and stochastic approximations. In D. Saad  editor  On-line Learning in Neural

Networks  pages 9–42. Cambridge University Press  New York  NY  USA  1998.

[6] L. Bottou  F. E. Curtis  and J. Nocedal. Optimization methods for large-scale machine learning. SIAM

Review  60(2):223–311  2018.

[7] O. Bousquet and L. Bottou. The tradeoffs of large scale learning. In Advances in Neural Information

Processing Systems  pages 161–168  2008.

[8] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support vector machines. ACM Transactions on

Intelligent Systems and Technology  2(3):27  2011.

[9] F. Cucker and D.-X. Zhou. Learning Theory: an Approximation Theory Viewpoint. Cambridge University

Press  2007.

[10] A. Dieuleveut and F. Bach. Nonparametric stochastic approximation with large step-sizes. Annals of

Statistics  44(4):1363–1399  2016.

[11] J. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. In Advances

in Neural Information Processing Systems  pages 495–503  2009.

[12] J. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite objective mirror descent. In Conference

on Learning Theory  pages 14–26  2010.

[13] M. Hardt  B. Recht  and Y. Singer. Train faster  generalize better: Stability of stochastic gradient descent.

In International Conference on Machine Learning  pages 1225–1234  2016.

[14] E. Hazan and S. Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-

convex optimization. Journal of Machine Learning Research  15(1):2489–2512  2014.

[15] E. Hazan  A. Agarwal  and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine

Learning  69(2):169–192  2007.

[16] S. Lacoste-Julien  M. Schmidt  and F. Bach. A simpler approach to obtaining an O(1/t) convergence rate

for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002  2012.

[17] Y. Lei and D.-X. Zhou. Convergence of online mirror descent. Applied and Computational Harmonic

Analysis  2018. doi: https://doi.org/10.1016/j.acha.2018.05.005.

[18] Y. Lei and D.-X. Zhou. Learning theory of randomized sparse Kaczmarz method. SIAM Journal on

Imaging Sciences  11(1):547–574  2018.

[19] Y. Lei  L. Shi  and Z.-C. Guo. Convergence of unregularized online learning algorithms. Journal of

Machine Learning Research  18(171):1–33  2018.

[20] J. Lin and L. Rosasco. Optimal learning for multi-pass stochastic gradient methods. In Advances in Neural

Information Processing Systems  pages 4556–4564  2016.

[21] J. Lin  R. Camoriano  and L. Rosasco. Generalization properties and implicit regularization for multiple

passes SGM. In International Conference on Machine Learning  pages 2340–2348  2016.

[22] B. London. A PAC-bayesian analysis of randomized learning with application to stochastic gradient

descent. In Advances in Neural Information Processing Systems  pages 2931–2940  2017.

[23] E. Moulines and F. Bach. Non-asymptotic analysis of stochastic approximation algorithms for machine

learning. In Advances in Neural Information Processing Systems  pages 451–459  2011.

[24] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach to

stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

[25] A.-S. Nemirovsky and D.-B. Yudin. Problem Complexity and Method Efﬁciency in Optimization. John

Wiley & Sons  1983.

[26] L. M. Nguyen  P. H. Nguyen  M. van Dijk  P. Richtárik  K. Scheinberg  and M. Takáˇc. SGD and hogwild!

convergence without the bounded gradients assumption. arXiv preprint arXiv:1802.03801  2018.

10

[27] F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. In

Advances in Neural Information Processing Systems  pages 1116–1124  2014.

[28] A. Rakhlin  O. Shamir  and K. Sridharan. Making gradient descent optimal for strongly convex stochastic

optimization. In International Conference on Machine Learning  pages 449–456  2012.

[29] L. Rosasco and S. Villa. Learning with incremental iterative regularization.

Information Processing Systems  pages 1630–1638  2015.

In Advances in Neural

[30] S. Shalev-Shwartz and A. Tewari. Stochastic methods for (cid:96)1-regularized loss minimization. Journal of

Machine Learning Research  12:1865–1892  2011.

[31] S. Shalev-Shwartz  Y. Singer  and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In

International Conference on Machine Learning  pages 807–814. ACM  2007.

[32] O. Shamir and T. Zhang. Stochastic gradient descent for non-smooth optimization convergence results and

optimal averaging schemes. In International Conference on Machine Learning  pages 71–79  2013.

[33] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Analysis and Applications 

1(01):17–41  2003.

[34] I. Steinwart and A. Christmann. Support Vector Machines. Springer Science & Business Media  2008.
[35] I. Steinwart and C. Scovel. Fast rates for support vector machines using gaussian kernels. Annals of

Statistics  35(2):575–607  2007.

[36] P. Tarres and Y. Yao. Online learning as stochastic approximation of regularization paths: optimality and

almost-sure convergence. IEEE Transactions on Information Theory  60(9):5716–5735  2014.

[37] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of

Machine Learning Research  11:2543–2596  2010.

[38] Y. Ying and M. Pontil. Online gradient descent learning algorithms. Foundations of Computational

Mathematics  8(5):561–596  2008.

[39] Y. Ying and D.-X. Zhou. Online regularized classiﬁcation algorithms. IEEE Transactions on Information

Theory  52(11):4775–4788  2006.

[40] Y. Ying and D.-X. Zhou. Unregularized online learning algorithms with general loss functions. Applied

and Computational Harmonic Analysis  42(2):224–244  2017.

[41] T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In

International Conference on Machine Learning  pages 919–926  2004.

[42] P. Zhao and T. Zhang. Stochastic optimization with importance sampling for regularized loss minimization.

In International Conference on Machine Learning  pages 1–9  2015.

[43] Z. Zhou  P. Mertikopoulos  N. Bambos  S. Boyd  and P. W. Glynn. Stochastic mirror descent in variationally
coherent optimization problems. In Advances in Neural Information Processing Systems  pages 7043–7052 
2017.

[44] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In International

Conference on Machine Learning  pages 928–936  2003.

11

,Trung Nguyen
Edwin Bonilla
Yunwen Lei
Ke Tang