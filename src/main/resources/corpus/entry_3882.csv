2013,On Flat versus Hierarchical Classification in Large-Scale Taxonomies,We study in this paper flat and hierarchical classification strategies in the context of large-scale taxonomies. To this end  we first propose a multiclass  hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature  related to the performance of flat and hierarchical classifiers. We then introduce another type of bounds targeting the approximation error of a family of classifiers  and derive from it features used in a meta-classifier to decide which nodes to prune (or flatten) in a large-scale taxonomy. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.,On Flat versus Hierarchical Classiﬁcation in

Large-Scale Taxonomies

Rohit Babbar  Ioannis Partalas  Eric Gaussier  Massih-Reza Amini

Université Joseph Fourier  Laboratoire Informatique de Grenoble

BP 53 - F-38041 Grenoble Cedex 9

firstname.lastname@imag.fr

Abstract

We study in this paper ﬂat and hierarchical classiﬁcation strategies in the context
of large-scale taxonomies. To this end  we ﬁrst propose a multiclass  hierarchi-
cal data dependent bound on the generalization error of classiﬁers deployed in
large-scale taxonomies. This bound provides an explanation to several empirical
results reported in the literature  related to the performance of ﬂat and hierarchical
classiﬁers. We then introduce another type of bound targeting the approximation
error of a family of classiﬁers  and derive from it features used in a meta-classiﬁer
to decide which nodes to prune (or ﬂatten) in a large-scale taxonomy. We ﬁnally
illustrate the theoretical developments through several experiments conducted on
two widely used taxonomies.

1

Introduction

Large-scale classiﬁcation of textual and visual data into a large number of target classes has been
the focus of several studies  from researchers and developers in industry and academia alike. The
target classes in such large-scale scenarios typically have an inherent hierarchical structure  usually
in the form of a rooted tree  as in Directory Mozilla1  or a directed acyclic graph  with a parent-
child relationship. Various classiﬁcation techniques have been proposed for deploying classiﬁers
in such large-scale taxonomies  from ﬂat (sometimes referred to as big bang) approaches to fully
hierarchical one adopting a complete top-down strategy. Several attempts have also been made in
order to develop new classiﬁcation techniques that integrate  at least partly  the hierarchy into the
objective function being optimized (as [3  5  10  11] among others). These techniques are however
costly in practice and most studies either rely on a ﬂat classiﬁer  or a hierarchical one either deployed
on the original hierarchy or a simpliﬁed version of it obtained by pruning some nodes (as [15  18])2.
Hierarchical models for large scale classiﬁcation however suffer from the fact that they have to make
many decisions prior to reach a ﬁnal category. This intermediate decision making leads to the error
propagation phenomenon causing a decrease in accuracy. On the other hand  ﬂat classiﬁers rely on a
single decision including all the ﬁnal categories  a single decision that is however difﬁcult to make as
it involves many categories  potentially unbalanced. It is thus very difﬁcult to assess which strategy
is best and there is no consensus  at the time being  on to which approach  ﬂat or hierarchical  should
be preferred on a particular category system.
In this paper  we address this problem and introduce new bounds on the generalization errors of
classiﬁers deployed in large-scale taxonomies. These bounds make explicit the trade-off that both
ﬂat and hierarchical classiﬁers encounter in large-scale taxonomies and provide an explanation to

1www.dmoz.org
2The study in [19] introduces a slightly different simpliﬁcation  through an embedding of both categories

and documents into a common space.

1

several empirical ﬁndings reported in previous studies. To our knowledge  this is the ﬁrst time that
such bounds are introduced and that an explanation of the behavior of ﬂat and hierarchical classiﬁers
is based on theoretical grounds. We also propose a well-founded way to select nodes that should be
pruned so as to derive a taxonomy better suited to the classiﬁcation problem. Contrary to [4] that
reweighs the edges in a taxonomy through a cost sensitive loss function to achieve this goal  we use
here a simple pruning strategy that modiﬁes the taxonomy in an explicit way.
The remainder of the paper is organized as follows: Section 2 introduces the notations used and
presents the generalization error bounds for classiﬁcation in large-scale taxonomies. It also presents
the meta-classiﬁer we designed to select those nodes that should be pruned in the original taxonomy.
Section 3 illustrates these developments via experiments conducted on several taxonomies extracted
from DMOZ and the International Patent Classiﬁcation. The experimental results are in line with
results reported in previous studies  as well as with our theoretical developments. Finally  Section 4
concludes this study.

2 Generalization Error Analyses
Let X ⊆ Rd be the input space and let V be a ﬁnite set of class labels. We further assume that
examples are pairs (x  v) drawn according to a ﬁxed but unknown distribution D over X × V . In
the case of hierarchical classiﬁcation  the hierarchy of classes H = (V  E) is deﬁned in the form of
a rooted tree  with a root ⊥ and a parent relationship π : V \ {⊥} → V where π(v) is the parent of
node v ∈ V \ {⊥}  and E denotes the set of edges with parent to child orientation. For each node
v ∈ V \{⊥}  we further deﬁne the set of its sisters S(v) = {v(cid:48) ∈ V \{⊥}; v (cid:54)= v(cid:48) ∧ π(v) = π(v(cid:48))}
and its daughters D(v) = {v(cid:48) ∈ V \ {⊥}; π(v(cid:48)) = v}. The nodes at the intermediary levels of
the hierarchy deﬁne general class labels while the specialized nodes at the leaf level  denoted by
Y = {y ∈ V : (cid:64)v ∈ V  (y  v) ∈ E} ⊂ V   constitute the set of target classes. Finally for each class
y in Y we deﬁne the set of its ancestors P(y) deﬁned as

P(y) = {vy

1   . . .   vy
ky

1 = π(y) ∧ ∀l ∈ {1  . . .   ky − 1}  vy
; vy

l+1 = π(vy

l ) ∧ π(vy

ky

) =⊥}

For classifying an example x  we consider a top-down classiﬁer making decisions at each level of the
hierarchy  this process sometimes referred to as the Pachinko machine selects the best class at each
level of the hierarchy and iteratively proceeds down the hierarchy. In the case of ﬂat classiﬁcation 
the hierarchy H is ignored  Y = V   and the problem reduces to the classical supervised multiclass
classiﬁcation problem.

2.1 A hierarchical Rademacher data-dependent bound

Our main result is the following theorem which provides a data-dependent bound on the generaliza-
tion error of a top-down multiclass hierarchical classiﬁer. We consider here kernel-based hypotheses 
with K : X ×X → R a PDS kernel and Φ : X → H its associated feature mapping function  deﬁned
as :

FB = {f : (x  v) ∈ X × V (cid:55)→ (cid:104)Φ(x)  wv(cid:105) | W = (w1 . . .   w|V |) ||W||H ≤ B}

where W = (w1 . . .   w|V |) is the matrix formed by the |V | weight vectors deﬁning the kernel-based

hypotheses  (cid:104).  .(cid:105) denotes the dot product  and ||W||H =(cid:0)(cid:80)

v∈V ||wv||2(cid:1)1/2 is the L2H group norm

of W. We further deﬁne the following associated function class:

GFB = {gf : (x  y) ∈ X × Y (cid:55)→ min
v∈P(y)

(f (x  v) − max
v(cid:48)∈S(v)

f (x  v(cid:48))) | f ∈ FB}

esis f from FB such that the generalization error of gf ∈ GFB   E(gf ) = E(x y)∼D(cid:2)1gf (x y)≤0

For a given hypothesis f ∈ FB  the sign of its associated function gf ∈ GFB directly deﬁnes a
hierarchical classiﬁcation rule for f as the top-down classiﬁcation scheme outlined before simply
amounts to: assign x to y iff gf (x  y) > 0. The learning problem we address is then to ﬁnd a hypoth-
minimal (1gf (x y)≤0 is the 0/1 loss  equal to 1 if gf (x  y) ≤ 0 and 0 otherwise).
The following theorem sheds light on the trade-off between ﬂat versus hierarchical classiﬁcation.
The notion of function class capacity used here is the empirical Rademacher complexity [1]. The
proof of the theorem is given in the supplementary material.

(cid:3)  is

2

Theorem 1 Let S = ((x(i)  y(i)))m
i=1 be a dataset of m examples drawn i.i.d. according to a
probability distribution D over X ×Y  and let A be a Lipschitz function with constant L dominating
the 0/1 loss; further let K : X × X → R be a PDS kernel and let Φ : X → H be the associated
feature mapping function. Assume that there exists R > 0 such that K(x  x) ≤ R2 for all x ∈ X .
Then  for all 1 > δ > 0  with probability at least (1 − δ) the following hierarchical multiclass
classiﬁcation generalization bound holds for all gf ∈ GFB :
(cid:88)

m(cid:88)

(cid:114)

|D(v)|(|D(v)| − 1) + 3

A(gf (x(i)  y(i))) +

ln(2/δ)

2m

(1)

E(gf ) ≤ 1
m

8BRL√
m

v∈V \Y
where |D(v)| denotes the number of daughters of node v.

i=1

i=1 n2

ones  as any split of a set of category of size n in k parts n1 ···   nk ((cid:80)k
(cid:80)k

For ﬂat multiclass classiﬁcation  we recover the bounds of [12] by considering a hierarchy containing
a root node with as many daughters as there are categories. Note that the deﬁnition of functions
in GFB subsumes the deﬁnition of the margin function used for the ﬂat multiclass classiﬁcation
problems in [12]  and that the factor 8L in the complexity term of the bound  instead of 4 in [12] 
is due to the fact that we are using an L-Lipschitz loss function dominating the 0/1 loss in the
empirical Rademacher complexity.
Flat vs hierarchical classiﬁcation on large-scale taxonomies. The generalization error is con-
trolled in inequality (1) by a trade-off between the empirical error and the Rademacher complexity
of the class of classiﬁers. The Rademacher complexity term favors hierarchical classiﬁers over ﬂat
i=1 ni = n) is such that
i ≤ n2. On the other hand  the empirical error term is likely to favor ﬂat classiﬁers vs
hierarchical ones  as the latter rely on a series of decisions (as many as the length of the path from
the root to the chosen category in Y) and are thus more likely to make mistakes. This fact is often
referred to as the propagation error problem in hierarchical classiﬁcation.
On the contrary  ﬂat classiﬁers rely on a single decision and are not prone to this problem (even
though the decision to be made is harder). When the classiﬁcation problem in Y is highly unbal-
anced  then the decision that a ﬂat classiﬁer has to make is difﬁcult; hierarchical classiﬁers still have
to make several decisions  but the imbalance problem is less severe on each of them. So  in this
case  even though the empirical error of hierarchical classiﬁers may be higher than the one of ﬂat
ones  the difference can be counterbalanced by the Rademacher complexity term  and the bound in
Theorem 1 suggests that hierarchical classiﬁers should be preferred over ﬂat ones.
On the other hand  when the data is well balanced  the Rademacher complexity term may not be
sufﬁcient to overcome the difference in empirical errors due to the propagation error in hierarchical
classiﬁers; in this case  Theorem 1 suggests that ﬂat classiﬁers should be preferred to hierarchical
ones. These results have been empirically observed in different studies on classiﬁcation in large-
scale taxonomies and are further discussed in Section 3.
Similarly  one way to improve the accuracy of classiﬁers deployed in large-scale taxonomies is to
modify the taxonomy by pruning (sets of) nodes [18]. By doing so  one is ﬂattening part of the
taxonomy and is once again trading-off the two terms in inequality (1): pruning nodes leads to
reduce the number of decisions made by the hierarchical classiﬁer while maintaining a reasonable
Rademacher complexity. Even though it can explain several empirical results obtained so far  the
bound displayed in Theorem 1 does not provide a practical way to decide on whether to prune a
node or not  as it would involve the training of many classiﬁers which is impractical with large-scale
taxonomies. We thus turn towards another bound in the next section that will help us design a direct
and simple strategy to prune nodes in a taxonomy.

2.2 Asymptotic approximation error bounds

We now propose an asymptotic approximation error bound for a multiclass logistic regression (MLR)
classiﬁer. We ﬁrst consider the ﬂat  multiclass case (V = Y)  and then show how the bounds can
be combined in a typical top-down cascade  leading to the identiﬁcation of important features that
control the variation of these bounds.

3

Considering a pivot class y(cid:63) ∈ Y  a MLR classiﬁer  with parameters β = {βy
{1  . . .   d}}  models the class posterior probabilities via a linear function in x = (xj)d
example [13] p. 96) :

j ; y ∈ Y \{y(cid:63)}  j ∈
j=1 (see for

0   βy

P (y|x; β)y(cid:54)=y(cid:63) =

P (y(cid:63)|x; β) =

0 +(cid:80)d

exp(βy

j=1 βy

j xj)

0 +(cid:80)d
0 +(cid:80)d

1

y(cid:48)∈Y y(cid:48)(cid:54)=y(cid:63) exp(βy(cid:48)

j=1 βy(cid:48)

j xj)

y(cid:48)∈Y y(cid:48)(cid:54)=y(cid:63) exp(βy(cid:48)

j=1 βy(cid:48)

j xj)

1 +(cid:80)
1 +(cid:80)

The parameters β are usually ﬁt by maximum likelihood over a training set S of size m (denoted by

(cid:98)βm in the following) and the decision rule for this classiﬁer consists in choosing the class with the

highest class posterior probability :

P (y|x (cid:98)βm)

hm(x) = argmax

y∈Y

√

∀y ∈ Y 

j and (σy

j=1 βy

j xj) <

R|Y|σ0
δm

where σ0 = maxj y σy

0 +(cid:80)d

The following lemma states to which extent the posterior probabilities with maximum likelihood es-

timates(cid:98)βm may deviate from their asymptotic values obtained with maximum likelihood estimates
when the training size m tends to inﬁnity (denoted by(cid:98)β∞).
Lemma 1 Let S be a training set of size m and let (cid:98)βm be the maximum likelihood estimates of
the MLR classiﬁer over S. Further  let (cid:98)β∞ be the maximum likelihood estimates of parameters

j )y j represent the components of the inverse (diagonal) Fisher infor-

of MLR when m tends to inﬁnity. For all examples x  let R > 0 be the bound such that ∀y ∈
Y\{y(cid:63)}  exp(βy
R; then for all 1 > δ > 0  with probability at least (1 − δ) we
have:

(cid:114)
(cid:12)(cid:12)(cid:12) < d
(cid:12)(cid:12)(cid:12)P (y|x (cid:98)βm) − P (y|x (cid:98)β∞)
mation matrix at(cid:98)β∞.
Proof (sketch) By denoting the sets of parameters (cid:98)βm = { ˆβy
and (cid:98)β∞ = {βy
components of the inverse (diagonal) Fisher information matrix at(cid:98)β∞. Let σ0 = maxj y σy
1 − σ0/2  |(cid:98)βy
(cid:113) R
(cid:12)(cid:12)(cid:12)P (y|x (cid:98)βm) − P (y|x (cid:98)β∞)
(cid:12)(cid:12)(cid:12) < d

j ; j ∈ {0  . . .   d}  y ∈ Y \{y(cid:63)}} 
j ; j ∈ {0  . . .   d}  y ∈ Y \{y(cid:63)}}  and using the independence assumption and the
asymptotic normality of maximum likelihood estimates (see for example [17]  p. 421)  we have 
for 0 ≤ j ≤ d and ∀y ∈ Y \ {y(cid:63)}:
j )y i represent the
j . Then
using Chebyshev’s inequality  for 0 ≤ j ≤ d and ∀y ∈ Y\{y(cid:63)} we have with probability at least
R; using a
Taylor development of the functions exp(x+) and (1+x+x)−1 and the union bound  one obtains
that  ∀ > 0 and y ∈ Y with probability at least 1 − |Y|σ0
m .
Setting |Y|σ0
Lemma 1 suggests that the predicted and asymptotic posterior probabilities are close to each other 
as the quantities they are based on are close to each other. Thus  provided that the asymptotic
posterior probabilities between the best two classes  for any given x  are not too close to each other 
the generalization error of the MLR classiﬁer and the one of its asymptotic version should be similar.
Theorem 2 below states such a relationship  using the following function that measures the confusion
between the best two classes for the asymptotic MLR classiﬁer deﬁned as :

m. Further ∀x and ∀y ∈ Y\{y(cid:63)}  exp(βy

to δ  and solving for  gives the result. (cid:3)

m((cid:98)βy
j − βy

0 +(cid:80)d

j ) ∼ N (0  σy

j − βy

j | < √

j ) where the (σy

j=1 βy

j xj) <

:

2

√

√

2

P (y|x (cid:98)β∞)

h∞(x) = argmax

y∈Y

(2)

(3)

For any given x ∈ X   the confusion between the best two classes is deﬁned as follows.

4

Deﬁnition 1 Let f 1∞(x) = maxy∈Y P (y|x (cid:98)β∞) be the best class posterior probability for x by the
asymptotic MLR classiﬁer  and let f 2∞(x) = maxy∈Y\h∞(x) P (y|x (cid:98)β∞) be the second best class

posterior probability for x. We deﬁne the confusion of the asymptotic MLR classiﬁer for a category
set Y as:

GY (τ ) = P(x y)∼D(|f 1∞(x) − f 2∞(x)| < 2τ )

for a given τ > 0.

The following theorem states a relationship between the generalization error of a trained MLR clas-
siﬁer and its asymptotic version.

Theorem 2 For a multi-class classiﬁcation problem in d dimensional feature space with a training
set of size m  {x(i)  y(i)}m
i=1  x(i) ∈ X   y(i) ∈ Y  sampled i.i.d. from a probability distribution D  let
hm and h∞ denote the multiclass logistic regression classiﬁers learned from a training set of ﬁnite
size m and its asymptotic version respectively  and let E(hm) and E(h∞) be their generalization
errors. Then  for all 1 > δ > 0  with probability at least (1 − δ) we have:

E(hm) ≤ E(h∞) + GY

0 +(cid:80)d

R is a bound on the function exp(βy

√
where
constant.
Proof (sketch) The difference E(hm) − E(h∞) is bounded by the probability that the asymptotic
MLR classiﬁer h∞ correctly classiﬁes an example (x  y) ∈ X × Y randomly chosen from D  while
hm misclassiﬁes it. Using Lemma 1  for all δ ∈ (0  1) ∀x ∈ X  ∀y ∈ Y  with probability at least
1 − δ  we have:

j xj)  ∀x ∈ X and ∀y ∈ Y  and σ0 is a

j=1 βy

(4)

(cid:33)

(cid:32)

(cid:114)

d

R|Y|σ0
δm

(cid:114)
(cid:12)(cid:12)(cid:12)P (y|x (cid:98)βm) − P (y|x (cid:98)β∞)
(cid:12)(cid:12)(cid:12) < d
(cid:19)
(cid:113) R|Y|σ0

(cid:18)

R|Y|σ0
δm

Thus  the decision made by the trained MLR and its asymptotic version on an example (x  y) differs
only if the distance between the two predicted classes of the asymptotic classiﬁer is less than two

times the distance between the posterior probabilities obtained with(cid:98)βm and(cid:98)β∞ on that example;

and the probability of this is exactly GY

d

δm

  which upper-bounds E(hm) − E(h∞). (cid:3)

and is related to the smallest amount of information one has on the estimation of each parameter(cid:98)βk
Note that the quantity σ0 in Theorem 2 represents the largest value of the inverse (diagonal) Fisher
information matrix ([17]). It is thus the smallest value of the (diagonal) Fisher information matrix 
j .
This smallest amount of information is in turn related to the length (in number of occurrences) of
the longest (resp. shortest) class in Y denoted respectively by nmax and nmin as  the smaller they
are  the larger σ0 is likely to be.

2.3 A learning based node pruning strategy

Let us now consider a hierarchy of classes and a top-down classiﬁer making decisions at each
level of the hierarchy. A node-based pruning strategy can be easily derived from the approxima-
Indeed  any node v in the hierarchy H = (V  E) is associated with three
tion bounds above.
its sister categories with the node itself S(cid:48)(v) = S(v) ∪ {v}  its daughter cate-
category sets:
gories  D(v)  and the union of its sister and daughter categories  denoted F(v) = S(v) ∪ D(v).
These three sets of categories
are the ones involved before
and after the pruning of node
Let us now denote the
v.
MLR classiﬁer by hS(cid:48)
m learned
from a set of sister categories
of node v and the node itself 
and by hDv
m a MLR classiﬁer
learned from the set of daughter categories of node v (hS(cid:48)
totic versions). The following theorem is a direct extension of Theorem 2 to this setting.

v∞ and hDv∞ respectively denote their asymp-

S(v) ∪ {v}

Pruning

⊥

⊥

...

v

...

F(v)

D(v)

...

...

...

...

v

5

Theorem 3 With the notations deﬁned above  for MLR classiﬁers  ∀ > 0  v ∈ V \ Y  one has  with
probability at least 1 −

+ Rd2|D(v)|σD(v)

Rd2|S(cid:48)(v)|σS(cid:48) (v)

:

0

0
mD(v)2

(cid:18)

(cid:19)

E(hS(cid:48)

m ) + E(hDv

v

mS(cid:48) (v)2
m ) ≤ E(hS(cid:48)

v∞ ) + E(hDv∞ ) + GS(cid:48)(v)() + GD(v)()

{|Y (cid:96)|  mY (cid:96)  σY (cid:96)
0 ;Y (cid:96) ∈ {S(cid:48)(v)  D(v)}} are constants related to the set of categories Y (cid:96) ∈
{S(cid:48)(v)  D(v)} and involved in the respective bounds stated in Theorem 2. Denoting by hFv
m the
MLR classiﬁer trained on the set F(v) and by hFv∞ its asymptotic version  Theorem 3 suggests that
one should prune node v if:

GF(v)() ≤ GS(cid:48)(v)() + GD(v)() and

|F(v)|σF(v)

0
mF(v)

≤ |S(cid:48)(v)|σS(cid:48)(v)

0
mS(cid:48)(v)

|D(v)|σD(v)

0
mD(v)

+

(5)

max  nY (cid:96)

Furthermore  the bounds obtained rely on the union bound and thus are not likely to be ex-
ploitable in practice. They nevertheless exhibit the factors that play an important role in as-
sessing whether a particular trained classiﬁer in the logistic regression family is close or not
to its asymptotic version. Each node v ∈ V can then be characterized by factors in the set
min  GY (cid:96) (.)|Y (cid:96) ∈ {S(cid:48)(v)  D(v)  F(v)}} which are involved in the estimation
{|Y (cid:96)|  mY (cid:96)  nY (cid:96)
of inequalities (5) above. We propose to estimate the confusion term GY (cid:96)(.) with two simple quan-
tities: the average cosine similarity of all the pairs of classes in Y (cid:96)  and the average symmetric
Kullback-Leibler divergences between all the pairs in Y (cid:96) of class conditional multinomial distribu-
tions.
The procedure for collecting training data associates a positive (resp. negative) class to a node if
the pruning of that node leads to a ﬁnal performance increase (resp. decrease). A meta-classiﬁer is
then trained on these features using a training set from a selected class hierarchy. After the learning
phase  the meta-classiﬁer is applied to each node of a new hierarchy of classes so as to identify
which nodes should be pruned. A simple strategy to adopt is then to prune nodes in sequence: start-
ing from the root node  the algorithm checks which children of a given node v should be pruned by
creating the corresponding meta-instance and feeding the meta-classiﬁer; the child that maximizes
the probability of the positive class is then pruned; as the set of categories has changed  we recalcu-
late which children of v can be pruned  prune the best one (as above) and iterate this process till no
more children of v can be pruned; we then proceed to the children of v and repeat the process.

3 Discussion

We start our discussion by presenting results on different hierarchical datasets with different char-
acteristics using MLR and SVM classiﬁers. The datasets we used in these experiments are two large
datasets extracted from the International Patent Classiﬁcation (IPC) dataset3 and the publicly avail-
able DMOZ dataset from the second PASCAL large scale hierarchical text classiﬁcation challenge
(LSHTC2)4. Both datasets are multi-class; IPC is single-label and LSHTC2 multi-label with an
average of 1.02 categories per class. We created 4 datasets from LSHTC2 by splitting randomly the
ﬁrst layer nodes (11 in total) of the original hierarchy in disjoint subsets. The classes for the IPC
and LSHTC2 datasets are organized in a hierarchy in which the documents are assigned to the leaf
categories only. Table 1 presents the characteristics of the datasets.
CR denotes the complexity ratio between hierarchical and ﬂat classiﬁcation  given by the
/ (|Y|(|Y| − 1)); the
Rademacher complexity term in Theorem 1:
same constants B  R and L are used in the two cases. As one can note  this complexity ratio al-
ways goes in favor of the hierarchal strategy  although it is 2 to 10 times higher on the IPC dataset 
compared to LSHTC2-1 2 3 4 5. On the other hand  the ratio of empirical errors (last column of
Table 1) obtained with top-down hierarchical classiﬁcation over ﬂat classiﬁcation when using SVM

v∈V \Y |D(v)|(|D(v)| − 1)

(cid:16)(cid:80)

(cid:17)

3http://www.wipo.int/classifications/ipc/en/support/
4http://lshtc.iit.demokritos.gr/

6

Dataset
LSHTC2-1
LSHTC2-2
LSHTC2-3
LSHTC2-4
LSHTC2-5
IPC

# Tr.
25 310
50 558
38 725
27 924
68 367
46 324

# Test
6 441
13 057
10 102
7 026
17 561
28 926

# Classes

1 789
4 787
3 956
2 544
7 212
451

# Feat.
145 859
271 557
145 354
123 953
192 259
1 123 497

Depth

6
6
6
6
6
4

CR
0.008
0.003
0.004
0.005
0.002
0.02

Error ratio

1.24
1.32
2.65
1.8
2.12
12.27

ratio of hierarchical over the ﬂat case ((cid:80)

Table 1: Datasets used in our experiments along with the properties: number of training examples 
test examples  classes and the size of the feature space  the depth of the hierarchy and the complexity
v∈V \Y |D(v)|(|D(v)| − 1)/|Y|(|Y| − 1))  the ratio of

empirical error for hierarchical and ﬂat models.

with a linear kernel is this time higher than 1  suggesting the opposite conclusion. The error ratio is
furthermore really important on IPC compared to LSHTC2-1 2 3 4 5. The comparison of the com-
plexity and error ratios on all the datasets thus suggests that the ﬂat classiﬁcation strategy may be
preferred on IPC  whereas the hierarchical one is more likely to be efﬁcient on the LSHTC datasets.
This is indeed the case  as is shown below.
To test our simple node pruning strategy  we learned binary classiﬁers aiming at deciding whether
to prune a node  based on the node features described in the previous section. The label associated
to each node in this training set is deﬁned as +1 if pruning the node increases the accuracy of the
hierarchical classiﬁer by at least 0.1  and -1 if pruning the node decreases the accuracy by more than
0.1. The threshold at 0.1 is used to avoid too much noise in the training set. The meta-classiﬁer
is then trained to learn a mapping from the vector representation of a node (based on the above
features) and the labels {+1;−1}. We used the ﬁrst two datasets of LSHTC2 to extract the training
data while LSHTC2-3  4  5 and IPC were employed for testing.
The procedure for collecting training data is repeated for the MLR and SVM classiﬁers resulting in
three meta-datasets of 119 (19 positive and 100 negative)  89 (34 positive and 55 negative) and 94 (32
positive and 62 negative) examples respectively. For the binary classiﬁers  we used AdaBoost with
random forest as a base classiﬁer  setting the number of trees to 20  50 and 50 for the MLR and SVM
classiﬁers respectively and leaving the other parameters at their default values. Several values have
been tested for the number of trees ({10  20  50  100 and 200})  the depth of the trees ({unrestricted 
5  10  15  30  60})  as well as the number of iterations in AdaBoost ({10  20  30}). The ﬁnal values
were selected by cross-validation on the training set (LSHTC2-1 and LSHTC2-2) as the ones that
maximized accuracy and minimized false-positive rate in order to prevent degradation of accuracy.
We compare the fully ﬂat classiﬁer (FL) with the fully hierarchical (FH) top-down Pachinko ma-
chine  a random pruning (RN) and the proposed pruning method (PR) . For the random pruning
we restrict the procedure to the ﬁrst two levels and perform 4 random prunings (this is the average
number of prunings that are performed in our approach). For each dataset we perform 5 indepen-
dent runs for the random pruning and we record the best performance. For MLR and SVM  we use
the LibLinear library [8] and apply the L2-regularized versions  setting the penalty parameter C by
cross-validation.
The results on LSHTC2-3 4 5 and IPC are reported in Table 2. On all LSHTC datasets ﬂat clas-
siﬁcation performs worse than the fully hierarchy top-down classiﬁcation  for all classiﬁers. These
results are in line with complexity and empirical error ratios for SVM estimated on different col-
lections and shown in table 1 as well as with the results obtained in [14  7] over the same type of
taxonomies. Further  the work by [14] demonstrated that class hierarchies on LSHTC datasets suf-
fer from rare categories problem  i.e.  80% of the target categories in such hierarchies have less than
5 documents assigned to them.
As a result  ﬂat methods on such datasets face unbalanced classiﬁcation problems which results in
smaller error ratios; hierarchical classiﬁcation should be preferred in this case. On the other hand 
for hierarchies such as the one of IPC  which are relatively well balanced and do not suffer from
the rare categories phenomenon  ﬂat classiﬁcation performs at par or even better than hierarchical

7

LSHTC2-3
MLR
SVM

LSHTC2-4
MLR
SVM

LSHTC2-5
MLR
SVM

SVM
MLR
FL 0.528↓↓ 0.535↓↓ 0.497↓↓ 0.501↓↓ 0.542↓↓ 0.547↓↓ 0.546
0.446
RN 0.493↓↓ 0.517↓↓ 0.478↓↓ 0.484↓↓ 0.532↓↓ 0.536↓ 0.547↓ 0.458↓↓
0.527 0.552↓ 0.465↓↓
FH 0.484↓↓ 0.498↓↓ 0.473↓↓ 0.476↓
PR 0.480
0.472
0.523
0.450

0.526↓
0.522

0.544

0.493

0.469

IPC

Table 2: Error results across all datasets. Bold typeface is used for the best results. Statistical
signiﬁcance (using micro sign test (s-test) as proposed in [20]) is denoted with ↓ for p-value<0.05
and with ↓↓ for p-value<0.01.

classiﬁcation. This is in agreement with the conclusions obtained in recent studies  as [2  9  16  6] 
in which the datasets considered do not have rare categories and are more well-balanced.
The proposed hierarchy pruning strategy aims to adapt the given taxonomy structure for better clas-
siﬁcation while maintaining the ancestor-descendant relationship between a given pair of nodes. As
shown in Table 2  this simple learning based pruning strategy leads to statistically signiﬁcant better
results for all three classiﬁers compared to both the original taxonomy and a randomly pruned one.
A similar result is reported in [18] through a pruning of an entire layer of the hierarchy  which can be
seen as a generalization  even though empirical in nature  of the pruning strategy retained here. An-
other interesting approach to modify the original taxonomy is presented in [21]. In this study  three
other elementary modiﬁcation operations are considered  again with an increase of performance.

4 Conclusion

We have studied in this paper ﬂat and hierarchical classiﬁcation strategies in the context of large-
scale taxonomies  through error generalization bounds of multiclass  hierarchical classiﬁers. The
ﬁrst theorem we have introduced provides an explanation to several empirical results related to the
performance of such classiﬁers. We have also introduced a well-founded way to simplify a taxonomy
by selectively pruning some of its nodes  through a meta-classiﬁer. The features retained in this
meta-classiﬁer derive from the error generalization bounds we have proposed. The experimental
results reported here (as well as in other papers) are in line with our theoretical developments and
justify the pruning strategy adopted.
This is the ﬁrst time  to our knowledge  that a data dependent error generalization bound is pro-
posed for multiclass  hierarchical classiﬁers and that a theoretical explanation is provided for the
performance of ﬂat and hierarchical classiﬁcation strategies in large-scale taxonomies. In particular 
there is  up to now  no consensus on which classiﬁcation scheme  ﬂat or hierarchical  to use on a
particular category system. One of our main conclusions is that top-down hierarchical classiﬁers
are well suited to unbalanced  large-scale taxonomies  whereas ﬂat ones should be preferred for
well-balanced taxonomies.
Lastly  our theoretical development also suggests possibilities to grow a hierarchy of classes from
a (large) set of categories  as has been done in several studies (e.g. [2]). We plan to explore this in
future work.

5 Acknowledgments

This work was supported in part by the ANR project Class-Y  the Mastodons project Garguantua  the
LabEx PERSYVAL-Lab ANR-11-LABX-0025 and the European project BioASQ (grant agreement
no. 318652).

8

References
[1] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and

structural results. Journal of Machine Learning Research  3:463–482  2002.

[2] S. Bengio  J. Weston  and D. Grangier. Label embedding trees for large multi-class tasks. In

Advances in Neural Information Processing Systems 23  pages 163–171  2010.

[3] L. Cai and T. Hofmann. Hierarchical document categorization with support vector machines. In
Proceedings 13th ACM International Conference on Information and Knowledge Management
(CIKM)  pages 78–87. ACM  2004.

[4] O. Dekel. Distribution-calibrated hierarchical classiﬁcation. In Advances in Neural Informa-

tion Processing Systems 22  pages 450–458. 2009.

[5] O. Dekel  J. Keshet  and Y. Singer. Large margin hierarchical classiﬁcation. In Proceedings of

the 21st International Conference on Machine Learning  pages 27–35  2004.

[6] J. Deng  S. Satheesh  A. C. Berg  and F.-F. Li. Fast and balanced: Efﬁcient label tree learning
for large scale object recognition. In Advances in Neural Information Processing Systems 24 
pages 567–575  2011.

[7] S. Dumais and H. Chen. Hierarchical classiﬁcation of web content. In Proceedings of the 23rd

annual international ACM SIGIR conference  pages 256–263  2000.

[8] R.-E. Fan  K.-W. Chang  C.-J. Hsieh  X.-R. Wang  and C.-J. Lin. LIBLINEAR: A library for

large linear classiﬁcation. Journal of Machine Learning Research  9:1871–1874  2008.

[9] T. Gao and D. Koller. Discriminative learning of relaxed hierarchy for large-scale visual recog-
nition. In IEEE International Conference on Computer Vision (ICCV)  pages 2072–2079  2011.
[10] S. Gopal and Y. Y. A. Niculescu-Mizil. Regularization framework for large scale hierarchical
classiﬁcation. In Large Scale Hierarchical Classiﬁcation  ECML/PKDD Discovery Challenge
Workshop  2012.

[11] S. Gopal  Y. Yang  B. Bai  and A. Niculescu-Mizil. Bayesian models for large-scale hierarchi-

cal classiﬁcation. In Advances in Neural Information Processing Systems 25  2012.

[12] Y. Guermeur. Sample complexity of classiﬁers taking values in Rq  application to multi-class

SVMs. Communications in Statistics - Theory and Methods  39  2010.

[13] T. Hastie  R. Tibshirani  and J. Friedman. The Elements of Statistical Learning. Springer New

York Inc.  2001.

[14] T.-Y. Liu  Y. Yang  H. Wan  H.-J. Zeng  Z. Chen  and W.-Y. Ma. Support vector machines

classiﬁcation with a very large-scale taxonomy. SIGKDD  2005.

[15] H. Malik. Improving hierarchical SVMs by hierarchy ﬂattening and lazy classiﬁcation. In 1st

Pascal Workshop on Large Scale Hierarchical Classiﬁcation  2009.

[16] F. Perronnin  Z. Akata  Z. Harchaoui  and C. Schmid. Towards good practice in large-scale
learning for image classiﬁcation. In Computer Vision and Pattern Recognition  pages 3482–
3489  2012.

[17] M. Schervish. Theory of Statistics. Springer Series in Statistics. Springer New York Inc.  1995.
[18] X. Wang and B.-L. Lu. Flatten hierarchies for large-scale hierarchical text categorization. In

5th International Conference on Digital Information Management  pages 139–144  2010.

[19] K. Q. Weinberger and O. Chapelle. Large margin taxonomy embedding for document cat-
In Advances in Neural Information Processing Systems 21  pages 1737–1744 

egorization.
2008.

[20] Y. Yang and X. Liu. A re-examination of text categorization methods. In Proceedings of the

22nd annual International ACM SIGIR conference  pages 42–49. ACM  1999.

[21] J. Zhang  L. Tang  and H. Liu. Automatically adjusting content taxonomies for hierarchical

classiﬁcation. In Proceedings of the 4th Workshop on Text Mining  2006.

9

,Rohit Babbar
Ioannis Partalas
Eric Gaussier
Massih R. Amini
Xiaodan Liang
Zhiting Hu
Hao Zhang
Liang Lin
Eric Xing
Shengyuan Hu
Tao Yu
Chuan Guo
Wei-Lun Chao
Kilian Weinberger