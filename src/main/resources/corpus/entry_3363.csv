2015,Max-Margin Deep Generative Models,Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However  little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs)  which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs  while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models.,Max-Margin Deep Generative Models

Chongxuan Li†  Jun Zhu†  Tianlin Shi‡  Bo Zhang†

†Dept. of Comp. Sci. & Tech.  State Key Lab of Intell. Tech. & Sys.  TNList Lab 

Center for Bio-Inspired Computing Research  Tsinghua University  Beijing  100084  China
{licx14@mails.  dcszj@  dcszb@}tsinghua.edu.cn; stl501@gmail.com

‡Dept. of Comp. Sci.  Stanford University  Stanford  CA 94305  USA

Abstract

Deep generative models (DGMs) are effective on learning multilayered represen-
tations of complex data and performing inference of input data by exploring the
generative ability. However  little work has been done on examining or empower-
ing the discriminative ability of DGMs on making accurate predictions. This pa-
per presents max-margin deep generative models (mmDGMs)  which explore the
strongly discriminative principle of max-margin learning to improve the discrim-
inative power of DGMs  while retaining the generative capability. We develop an
efﬁcient doubly stochastic subgradient algorithm for the piecewise linear objec-
tive. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-
margin learning can signiﬁcantly improve the prediction performance of DGMs
and meanwhile retain the generative ability; and (2) mmDGMs are competitive to
the state-of-the-art fully discriminative networks by employing deep convolutional
neural networks (CNNs) as both recognition and generative models.

1

Introduction

Max-margin learning has been effective on learning discriminative models  with many examples
such as univariate-output support vector machines (SVMs) [5] and multivariate-output max-margin
Markov networks (or structured SVMs) [30  1  31]. However  the ever-increasing size of complex
data makes it hard to construct such a fully discriminative model  which has only single layer of
adjustable weights  due to the facts that: (1) the manually constructed features may not well capture
the underlying high-order statistics; and (2) a fully discriminative approach cannot reconstruct the
input data when noise or missing values are present.
To address the ﬁrst challenge  previous work has considered incorporating latent variables into
a max-margin model  including partially observed maximum entropy discrimination Markov net-
works [37]  structured latent SVMs [32] and max-margin min-entropy models [20]. All this work
has primarily focused on a shallow structure of latent variables. To improve the ﬂexibility  learn-
ing SVMs with a deep latent structure has been presented in [29]. However  these methods do not
address the second challenge  which requires a generative model to describe the inputs. The re-
cent work on learning max-margin generative models includes max-margin Harmoniums [4]  max-
margin topic models [34  35]  and nonparametric Bayesian latent SVMs [36] which can infer the
dimension of latent features from data. However  these methods only consider the shallow structure
of latent variables  which may not be ﬂexible enough to describe complex data.
Much work has been done on learning generative models with a deep structure of nonlinear hidden
variables  including deep belief networks [25  16  23]  autoregressive models [13  9]  and stochastic
variations of neural networks [3]. For such models  inference is a challenging problem  but for-
tunately there exists much recent progress on stochastic variational inference algorithms [12  24].
However  the primary focus of deep generative models (DGMs) has been on unsupervised learning 

1

with the goals of learning latent representations and generating input samples. Though the latent
representations can be used with a downstream classiﬁer to make predictions  it is often beneﬁcial
to learn a joint model that considers both input and response variables. One recent attempt is the
conditional generative models [11]  which treat labels as conditions of a DGM to describe input
data. This conditional DGM is learned in a semi-supervised setting  which is not exclusive to ours.
In this paper  we revisit the max-margin principle and present a max-margin deep generative model
(mmDGM)  which learns multi-layer representations that are good for both classiﬁcation and in-
put inference. Our mmDGM conjoins the ﬂexibility of DGMs on describing input data and the
strong discriminative ability of max-margin learning on making accurate predictions. We formulate
mmDGM as solving a variational inference problem of a DGM regularized by a set of max-margin
posterior constraints  which bias the model to learn representations that are good for prediction. We
deﬁne the max-margin posterior constraints as a linear functional of the target variational distribu-
tion of the latent presentations. Then  we develop a doubly stochastic subgradient descent algorithm 
which generalizes the Pagesos algorithm [28] to consider nontrivial latent variables. For the varia-
tional distribution  we build a recognition model to capture the nonlinearity  similar as in [12  24].
We consider two types of networks used as our recognition and generative models: multiple layer
perceptrons (MLPs) as in [12  24] and convolutional neural networks (CNNs) [14]. Though CNNs
have shown promising results in various domains  especially for image classiﬁcation  little work has
been done to take advantage of CNN to generate images. The recent work [6] presents a type of
CNN to map manual features including class labels to RBG chair images by applying unpooling 
convolution and rectiﬁcation sequentially; but it is a deterministic mapping and there is no random
generation. Generative Adversarial Nets [7] employs a single such layer together with MLPs in a
minimax two-player game framework with primary goal of generating images. We propose to stack
this structure to form a highly non-trivial deep generative network to generate images from latent
variables learned automatically by a recognition model using standard CNN. We present the detailed
network structures in experiments part. Empirical results on MNIST [14] and SVHN [22] datasets
demonstrate that mmDGM can signiﬁcantly improve the prediction performance  which is competi-
tive to the state-of-the-art methods [33  17  8  15]  while retaining the capability of generating input
samples and completing their missing values.

2 Basics of Deep Generative Models
We start from a general setting  where we have N i.i.d. data X = {xn}N
n=1. A deep generative
model (DGM) assumes that each xn ∈ RD is generated from a vector of latent variables zn ∈ RK 
which itself follows some distribution. The joint probability of a DGM is as follows:

p(X  Z|α  β) =

p(zn|α)p(xn|zn  β) 

n=1

(1)
where p(zn|α) is the prior of the latent variables and p(xn|zn  β) is the likelihood model for gen-
erating observations. For notation simplicity  we deﬁne θ = (α  β). Depending on the structure
of z  various DGMs have been developed  such as the deep belief networks [25  16]  deep sigmoid
networks [21]  deep latent Gaussian models [24]  and deep autoregressive models [9]. In this paper 
we focus on the directed DGMs  which can be easily sampled from via an ancestral sampler.
However  in most cases learning DGMs is challenging due to the intractability of posterior inference.
The state-of-the-art methods resort to stochastic variational methods under the maximum likelihood
estimation (MLE) framework  ˆθ = argmaxθ log p(X|θ). Speciﬁcally  let q(Z) be the variational
distribution that approximates the true posterior p(Z|X  θ). A variational upper bound of the per
sample negative log-likelihood (NLL) − log p(xn|α  β) is:

L(θ  q(zn); xn) (cid:44) KL(q(zn)||p(zn|α)) − Eq(zn)[log p(xn|zn  β)] 

L(θ  q(Z); X)(cid:44)(cid:80)

(2)
where KL(q||p) is the Kullback-Leibler (KL) divergence between distributions q and p. Then 
nL(θ  q(zn); xn) upper bounds the full negative log-likelihood − log p(X|θ).
It is important to notice that if we do not make restricting assumption on the variational distribution
q  the lower bound is tight by simply setting q(Z) = p(Z|X  θ). That is  the MLE is equivalent to
solving the variational problem: minθ q(Z) L(θ  q(Z); X). However  since the true posterior is in-
tractable except a handful of special cases  we must resort to approximation methods. One common

N(cid:89)

2

assumption is that the variational distribution is of some parametric form  qφ(Z)  and then we opti-
mize the variational bound w.r.t the variational parameters φ. For DGMs  another challenge arises
that the variational bound is often intractable to compute analytically. To address this challenge  the
early work further bounds the intractable parts with tractable ones by introducing more variational
parameters [26]. However  this technique increases the gap between the bound being optimized and
the log-likelihood  potentially resulting in poorer estimates. Much recent progress [12  24  21] has
been made on hybrid Monte Carlo and variational methods  which approximates the intractable ex-
pectations and their gradients over the parameters (θ  φ) via some unbiased Monte Carlo estimates.
Furthermore  to handle large-scale datasets  stochastic optimization of the variational objective can
be used with a suitable learning rate annealing scheme. It is important to notice that variance reduc-
tion is a key part of these methods in order to have fast and stable convergence.
Most work on directed DGMs has been focusing on the generative capability on inferring the obser-
vations  such as ﬁlling in missing values [12  24  21]  while little work has been done on investigating
the predictive power  except the semi-supervised DGMs [11] which builds a DGM conditioned on
the class labels and learns the parameters via MLE. Below  we present max-margin deep generative
models  which explore the discriminative max-margin principle to improve the predictive ability of
the latent representations  while retaining the generative capability.

3 Max-margin Deep Generative Models
We consider supervised learning  where the training data is a pair (x  y) with input features x ∈ RD
and the ground truth label y. Without loss of generality  we consider the multi-class classiﬁcation 
where y ∈ C = {1  . . .   M}. A max-margin deep generative model (mmDGM) consists of two
components: (1) a deep generative model to describe input features; and (2) a max-margin classiﬁer
to consider supervision. For the generative model  we can in theory adopt any DGM that deﬁnes a
joint distribution over (X  Z) as in Eq. (1). For the max-margin classiﬁer  instead of ﬁtting the input
features into a conventional SVM  we deﬁne the linear classiﬁer on the latent representations  whose
learning will be regularized by the supervision signal as we shall see. Speciﬁcally  if the latent
representation z is given  we deﬁne the latent discriminant function F (y  z  η; x) = η(cid:62)f (y  z) 
where f (y  z) is an M K-dimensional vector that concatenates M subvectors  with the yth being z
and all others being zero  and η is the corresponding weight vector.
We consider the case that η is a random vector  following some prior distribution p0(η). Then
our goal is to infer the posterior distribution p(η  Z|X  Y)  which is typically approximated by a
variational distribution q(η  Z) for computational tractability. Notice that this posterior is different
from the one in the vanilla DGM. We expect that the supervision information will bias the learned
representations to be more powerful on predicting the labels at testing. To account for the uncertainty
of (η  Z)  we take the expectation and deﬁne the discriminant function F (y; x) = Eq
and the ﬁnal prediction rule that maps inputs to outputs is:

(cid:2)η(cid:62)f (y  z)(cid:3)  

ˆy = argmax

y∈C

F (y; x).

(3)

Note that different from the conditional DGM [11]  which puts the class labels upstream  the above
classiﬁer is a downstream model  in the sense that the supervision signal is determined by condi-
tioning on the latent representations.

3.1 The Learning Problem
We want to jointly learn the parameters θ and infer the posterior distribution q(η  Z). Based on the
equivalent variational formulation of MLE  we deﬁne the joint learning problem as solving:

(4)

N(cid:88)

n=1

L(θ  q(η  Z); X) + C

(cid:26)Eq[η(cid:62)∆fn(y)] ≥ ∆ln(y) − ξn

ξn

min

θ q(η Z) ξ

∀n  y ∈ C  s.t. :

ξn ≥ 0 

where ∆fn(y) = f (yn  zn) − f (y  zn) is the difference of the feature vectors; ∆ln(y) is the loss
function that measures the cost to predict y if the true label is yn; and C is a nonnegative regular-
ization parameter balancing the two components. In the objective  the variational bound is deﬁned

3

min

as L(θ  q(η  Z); X) = KL(q(η  Z)||p0(η  Z|α)) − Eq [log p(X|Z  β)]  and the margin constraints
are from the classiﬁer (3). If we ignore the constraints (e.g.  setting C at 0)  the solution of q(η  Z)
will be exactly the Bayesian posterior  and the problem is equivalent to do MLE for θ.
By absorbing the slack variables  we can rewrite the problem in an unconstrained form:

L(θ  q(η  Z); X) + CR(q(η  Z; X)) 

where the hinge loss is: R(q(η  Z); X) = (cid:80)N
ror of classiﬁer (3)  that is  R(q(η  Z); X) ≥(cid:80)

(5)
n=1 maxy∈C(∆ln(y) − Eq[η(cid:62)∆fn(y)]). Due to the
convexity of max function  it is easy to verify that the hinge loss is an upper bound of the training er-
n ∆ln(ˆyn). Furthermore  the hinge loss is a convex
functional over the variational distribution because of the linearity of the expectation operator. These
properties render the hinge loss as a good surrogate to optimize over. Previous work has explored
this idea to learn discriminative topic models [34]  but with a restriction on the shallow structure of
hidden variables. Our work presents a signiﬁcant extension to learn deep generative models  which
pose new challenges on the learning and inference.

θ q(η Z)

3.2 The Doubly Stochastic Subgradient Algorithm
The variational formulation of problem (5) naturally suggests that we can develop a variational
algorithm to address the intractability of the true posterior. We now present a new algorithm to
solve problem (5). Our method is a doubly stochastic generalization of the Pegasos (i.e.  Primal
Estimated sub-GrAdient SOlver for SVM) algorithm [28] for the classic SVMs with fully observed
input features  with the new extension of dealing with a highly nontrivial structure of latent variables.
First  we make the structured mean-ﬁeld (SMF) assumption that q(η  Z) = q(η)qφ(Z). Under the
assumption  we have the discriminant function as Eq[η(cid:62)∆fn(y)] = Eq(η)[η(cid:62)]E
qφ(z(n))[∆fn(y)].
Moreover  we can solve for the optimal solution of q(η) in some analytical form.
In fact 
by the calculus of variations  we can show that given the other parts the solution is q(η) ∝
  where ω are the Lagrange multipliers (See [34] for de-
p0(η) exp
If the prior is normal  p0(η) = N (0  σ2I)  we have the normal posterior: q(η) =
tails).
Eqφ[∆fn(y)]. Therefore  even though we did not make a para-
metric form assumption of q(η)  the above results show that the optimal posterior distribution of η
is Gaussian. Since we only use the expectation in the optimization problem and in prediction  we
can directly solve for the mean parameter λ instead of q(η). Further  in this case we can verify that
||λ||2
KL(q(η)||p0(η)) =
2σ2 and then the equivalent objective function in terms of λ can be written
as:

N (λ  σ2I)  where λ = σ2(cid:80)

η(cid:62)(cid:80)

Eqφ[∆fn(y)]

n y ωy
n

n y ωy
n

(cid:16)

(cid:17)

L(θ  φ; X) +

min
θ φ λ

||λ||2
2σ2 + CR(λ  φ; X) 

(6)

n=1 (cid:96)(λ  φ; xn) is the total hinge loss  and the per-sample hinge-loss is
(cid:62)Eqφ[∆fn(y)]). Below  we present a doubly stochastic subgra-

(cid:96)(λ  φ; xn) = maxy∈C(∆ln(y) − λ
dient descent algorithm to solve this problem.
The ﬁrst stochasticity arises from a stochastic estimate of the objective by random mini-batches.
Speciﬁcally  the batch learning needs to scan the full dataset to compute subgradients  which is
often too expensive to deal with large-scale datasets. One effective technique is to do stochastic
subgradient descent [28]  where at each iteration we randomly draw a mini-batch of the training
data and then do the variational updates over the small mini-batch. Formally  given a mini batch of
size m  we get an unbiased estimate of the objective:

where R(λ  φ; X) = (cid:80)N

˜Lm :=

N
m

L(θ  φ; xn) +

||λ||2
2σ2 +

N C
m

(cid:96)(λ  φ; xn).

m(cid:88)

n=1

m(cid:88)

n=1

The second stochasticity arises from a stochastic estimate of the per-sample variational bound
and its subgradient  whose intractability calls for another Monte Carlo estimator. Formally  let
n ∼ qφ(z|xn  yn) be a set of samples from the variational distribution  where we explicitly put the
(cid:17)
zl
conditions. Then  an estimate of the per-sample variational bound and the per-sample hinge-loss is
˜L(θ  φ; xn)=

n|β)−log qφ(zl

n); ˜(cid:96)(λ  φ; xn)=max

(cid:62)
∆fn(y  zl

log p(xn  zl

(cid:88)

(cid:88)

(cid:16)

n)

λ

 

1
L

∆ln(y)−1
L

y

l

l

4

n) = f (yn  zl

n) − f (y  zl

n). Note that ˜L is an unbiased estimate of L  while ˜(cid:96) is a
where ∆fn(y  zl
biased estimate of (cid:96). Nevertheless  we can still show that ˜(cid:96) is an upper bound estimate of (cid:96) under
expectation. Furthermore  this biasedness does not affect our estimate of the gradient.
In fact 
by using the equality ∇φqφ(z) = qφ(z)∇φ log qφ(z)  we can construct an unbiased Monte Carlo
estimate of ∇φ(L(θ  φ; xn) + (cid:96)(λ  φ; xn)) as:

gφ =

1
L

log p(zl

n  xn) − log qφ(zl

n) + Cλ

(cid:62)

∆fn(˜yn  zl

n)

l=1
term roots from the hinge loss with the loss-augmented prediction ˜yn =
n)). For θ and λ  the estimates of the gradient ∇θL(θ  φ; xn)

f (y  zl

(cid:62)

l λ

(cid:17)∇φ log qφ(zl

n) 

(7)

L(cid:88)

(cid:16)
(cid:80)
(cid:88)

l

where the last
argmaxy(∆ln(y) + 1
and the subgradient ∇λ(cid:96)(λ  φ; xn) are easier  which are:
L
1
L

∇θ log p(xn  zl

n|θ) 

gλ =

gθ =

1
L

(cid:88)

(cid:0)f (˜yn  zl

l

n)(cid:1) .

n) − f (yn  zl

until Converge
return θ  λ  and φ

Initialize θ  λ  and φ
repeat

n) only depend on the variational distribution 

Algorithm 1 Doubly Stochastic Subgradient Algorithm

draw a random mini-batch of m data points
draw random samples from noise distribution p()
compute subgradient g = ∇θ λ φ ˜L(θ  λ  φ; Xm  )
update parameters (θ  λ  φ) using subgradient g.

Notice that the sampling and the gradient ∇φ log qφ(zl
not the underlying model.
The above estimates consider the gen-
eral case where the variational bound is
intractable. In some cases  we can com-
pute the KL-divergence term analyti-
cally  e.g.  when the prior and the vari-
ational distribution are both Gaussian.
In such cases  we only need to estimate
the rest intractable part by sampling 
which often reduces the variance [12].
Similarly  we could use the expectation
of the features directly  if it can be computed analytically  in the computation of subgradients (e.g. 
gθ and gλ) instead of sampling  which again can lead to variance reduction.
With the above estimates of subgradients  we can use stochastic optimization methods such as
SGD [28] and AdaM [10] to update the parameters  as outlined in Alg. 1. Overall  our algorithm is
a doubly stochastic generalization of Pegasos to deal with the highly nontrivial latent variables.
Now  the remaining question is how to deﬁne an appropriate variational distribution qφ(z) to obtain
a robust estimate of the subgradients as well as the objective. Two types of methods have been devel-
oped for unsupervised DGMs  namely  variance reduction [21] and auto-encoding variational Bayes
(AVB) [12]. Though both methods can be used for our models  we focus on the AVB approach. For
continuous variables Z  under certain mild conditions we can reparameterize the variational distri-
bution qφ(z) using some simple variables . Speciﬁcally  we can draw samples  from some simple
distribution p() and do the transformation z = gφ(  x  y) to get the sample of the distribution
q(z|x  y). We refer the readers to [12] for more details. In our experiments  we consider the special
Gaussian case  where we assume that the variational distribution is a multivariate Gaussian with a
diagonal covariance matrix:

qφ(z|x  y) = N (µ(x  y; φ)  σ2(x  y; φ)) 

(8)
whose mean and variance are functions of the input data. This deﬁnes our recognition model. Then 
the reparameterization trick is as follows: we ﬁrst draw standard normal variables l ∼ N (0  I) and
n = µ(xn  yn; φ) + σ(xn  yn; φ) (cid:12) l to get a sample. For simplicity 
then do the transformation zl
we assume that both the mean and variance are function of x only. However  it is worth to emphasize
that although the recognition model is unsupervised  the parameters φ are learned in a supervised
manner because the subgradient (7) depends on the hinge loss. Further details of the experimental
settings are presented in Sec. 4.1.

4 Experiments
We now present experimental results on the widely adopted MNIST [14] and SVHN [22] datasets.
Though mmDGMs are applicable to any DGMs that deﬁne a joint distribution of X and Z  we

5

concentrate on the Variational Auto-encoder (VA) [12]  which is unsupervised. We denote our
mmDGM with VA by MMVA. In our experiments  we consider two types of recognition models:
multiple layer perceptrons (MLPs) and convolutional neural networks (CNNs). We implement all
experiments based on Theano [2]. 1

4.1 Architectures and Settings
In the MLP case  we follow the settings in [11] to compare both generative and discriminative
capacity of VA and MMVA. In the CNN case  we use standard convolutional nets [14] with convo-
lution and max-pooling operation as the recognition model to obtain more competitive classiﬁcation
results. For the generative model  we use unconvnets [6] with a “symmetric” structure as the recog-
nition model  to reconstruct the input images approximately. More speciﬁcally  the top-down gen-
erative model has the same structure as the bottom-up recognition model but replacing max-pooling
with unpooling operation [6] and applies unpooling  convolution and rectiﬁcation in order. The total
number of parameters in the convolutional network is comparable with previous work [8  17  15].
For simplicity  we do not involve mlpconv layers [17  15] and contrast normalization layers in our
recognition model  but they are not exclusive to our model. We illustrate details of the network
architectures in appendix A.
In both settings  the mean and variance of the latent z are transformed from the last layer of the
recognition model through a linear operation. It should be noticed that we could use not only the
expectation of z but also the activation of any layer in the recognition model as features. The only
theoretical difference is from where we add a hinge loss regularization to the gradient and back-
propagate it to previous layers. In all of the experiments  the mean of z has the same nonlinearity
but typically much lower dimension than the activation of the last layer in the recognition model 
and hence often leads to a worse performance. In the MLP case  we concatenate the activations of
2 layers as the features used in the supervised tasks. In the CNN case  we use the activations of the
last layer as the features. We use AdaM [10] to optimize parameters in all of the models. Although it
is an adaptive gradient-based optimization method  we decay the global learning rate by factor three
periodically after sufﬁcient number of epochs to ensure a stable convergence.
We denote our mmDGM with MLPs by MMVA. To perform classiﬁcation using VA  we ﬁrst learn
the feature representations by VA  and then build a linear SVM classiﬁer on these features using the
Pegasos stochastic subgradient algorithm [28]. This baseline will be denoted by VA+Pegasos. The
corresponding models with CNNs are denoted by CMMVA and CVA+Pegasos respectively.

4.2 Results on the MNIST dataset
We present both the prediction performance and the results on generating samples of MMVA and
VA+Pegasos with both kinds of recognition models on the MNIST [14] dataset  which consists of
images of 10 different classes (0 to 9) of size 28×28 with 50 000 training samples  10 000 validating
samples and 10 000 testing samples.
Table 1: Error rates (%) on MNIST dataset.
4.2.1 Predictive Performance
In the MLP case  we only use 50 000 train-
ing data  and the parameters for classiﬁcation are
optimized according to the validation set. We
choose C = 15 for MMVA and initialize it with
an unsupervised pre-training procedure in classi-
ﬁcation.
three rows in Table 1 compare
VA+Pegasos  VA+Class-condtionVA and MMVA 
where VA+Class-condtionVA refers to the best fully
supervised model in [11]. Our model outperforms the baseline signiﬁcantly. We further use the
t-SNE algorithm [19] to embed the features learned by VA and MMVA on 2D plane  which again
demonstrates the stronger discriminative ability of MMVA (See Appendix B for details).
In the CNN case  we use 60 000 training data. Table 2 shows the effect of C on classiﬁcation error
rate and variational lower bound. Typically  as C gets lager  CMMVA learns more discriminative
features and leads to a worse estimation of data likelihood. However  if C is too small  the super-
vision is not enough to lead to predictive features. Nevertheless  C = 103 is quite a good trade-off

MODEL
VA+Pegasos
VA+Class-conditionVA
MMVA
CVA+Pegasos
CMMVA
Stochastic Pooling [33]
Network in Network [17]
Maxout Network [8]
DSN [15]

First

ERROR RATE

1.04
0.96
0.90
1.35
0.45
0.47
0.47
0.45
0.39

1The source code is available at https://github.com/zhenxuan00/mmdgm.

6

(a) VA

(b) MMVA

(c) CVA

(d) CMMVA

Figure 1: (a-b): randomly generated images by VA and MMVA  3000 epochs; (c-d): randomly
generated images by CVA and CMMVA  600 epochs.
between the classiﬁcation performance and generative performance and this is the default setting
of CMMVA on MNIST throughout this paper. In this setting  the classiﬁcation performance of our
CMMVA model is comparable to the recent state-of-the-art fully discriminative networks (without
data augmentation)  shown in the last four rows of Table 1.
4.2.2 Generative Performance
We further investigate the generative capability of MMVA
on generating samples. Fig. 1 illustrates the images ran-
domly sampled from VA and MMVA models where we
output the expectation of the gray value at each pixel to
get a smooth visualization. We do not pre-train our model
in all settings when generating data to prove that MMVA
(CMMVA) remains the generative capability of DGMs.

Table 2: Effects of C on MNIST dataset
with a CNN recognition model.
C ERROR RATE (%) LOWER BOUND
0
1
10
102
103
104

-93.17
-95.86
-95.90
-96.35
-99.62
-112.12

1.35
1.86
0.88
0.54
0.45
0.43

4.3 Results on the SVHN (Street View House Numbers) dataset
SVHN [22] is a large dataset consisting of color images of size 32 × 32. The task is to recognize
center digits in natural scene images  which is signiﬁcantly harder than classiﬁcation of hand-written
digits. We follow the work [27  8] to split the dataset into 598 388 training data  6000 validating
data and 26  032 testing data and preprocess the data by Local Contrast Normalization (LCN).
We only consider the CNN recognition model here. The network structure is similar to that in
MNIST. We set C = 104 for our CMMVA model on SVHN by default.
Table 3 shows the predictive performance.
In
this more challenging problem  we observe a
larger improvement by CMMVA as compared to
CVA+Pegasos  suggesting that DGMs beneﬁt a lot
from max-margin learning on image classiﬁcation.
We also compare CMMVA with state-of-the-art re-
sults. To the best of our knowledge  there is no com-
petitive generative models to classify digits on SVHN
dataset with full labels.
We further compare the generative capability of CMMVA and CVA to examine the beneﬁts from
jointly training of DGMs and max-margin classiﬁers. Though CVA gives a tighter lower bound
of data likelihood and reconstructs data more elaborately  it fails to learn the pattern of digits in a
complex scenario and could not generate meaningful images. Visualization of random samples from
CVA and CMMVA is shown in Fig. 2. In this scenario  the hinge loss regularization on recognition
model is useful for generating main objects to be classiﬁed in images.

MODEL
CVA+Pegasos
CMMVA
CNN [27]
Stochastic Pooling [33]
Maxout Network [8]
Network in Network [17]
DSN [15]

Table 3: Error rates (%) on SVHN dataset.

ERROR RATE

25.3
3.09
4.9
2.80
2.47
2.35
1.92

4.4 Missing Data Imputation and Classiﬁcation
Finally  we test all models on the task of missing data imputation. For MNIST  we consider two types
of missing values [18]: (1) Rand-Drop: each pixel is missing randomly with a pre-ﬁxed probability;
and (2) Rect: a rectangle located at the center of the image is missing. Given the perturbed images 
we uniformly initialize the missing values between 0 and 1  and then iteratively do the following
steps: (1) using the recognition model to sample the hidden variables; (2) predicting the missing
values to generate images; and (3) using the reﬁned images as the input of the next round. For
SVHN  we do the same procedure as in MNIST but initialize the missing values with Guassian

7

(a) Training data

(b) CVA

(c) CMMVA (C = 103) (d) CMMVA (C = 104)

(a):

training data after LCN preprocessing; (b): random samples from CVA; (c-d):

Figure 2:
random samples from CMMVA when C = 103 and C = 104 respectively.
random variables as the input distribution changes. Visualization results on MNIST and SVHN are
presented in Appendix C and Appendix D respectively.
Intuitively  generative models with CNNs
could be more powerful on learning pat-
terns and high-level structures  while
generative models with MLPs lean more
to reconstruct the pixels in detail. This
conforms to the MSE results shown in
Table 4: CVA and CMMVA outperform
VA and MMVA with a missing rectan-
gle  while VA and MMVA outperform
CVA and CMMVA with random miss-
ing values. Compared with the baseline 
mmDGMs also make more accurate com-
pletion when large patches are missing. All of the models infer missing values for 100 iterations.
We also compare the classiﬁcation performance of CVA  CNN and CMMVA with Rect missing
values in testing procedure in Appendix E. CMMVA outperforms both CVA and CNN.
Overall  mmDGMs have comparable capability of inferring missing values and prefer to learn high-
level patterns instead of local details.

Table 4: MSE on MNIST data with missing values in
the testing procedure.
NOISE TYPE
RAND-DROP (0.2) 0.0109 0.0110 0.0111
RAND-DROP (0.4) 0.0127 0.0127 0.0127
RAND-DROP (0.6) 0.0168 0.0165 0.0175
RAND-DROP (0.8) 0.0379 0.0358 0.0453
RECT (6 × 6)
0.0637 0.0645 0.0585
RECT (8 × 8)
0.0850 0.0841 0.0754
RECT (10 × 10)
0.1100 0.1079 0.0978
RECT (12 × 12)
0.1450 0.1342 0.1299

VA MMVA CVA CMMVA
0.0147
0.0161
0.0203
0.0449
0.0597
0.0724
0.0884
0.1090

5 Conclusions
We propose max-margin deep generative models (mmDGMs)  which conjoin the predictive power
of max-margin principle and the generative ability of deep generative models. We develop a doubly
stochastic subgradient algorithm to learn all parameters jointly and consider two types of recognition
models with MLPs and CNNs respectively. In both cases  we present extensive results to demon-
strate that mmDGMs can signiﬁcantly improve the prediction performance of deep generative mod-
els  while retaining the strong generative ability on generating input samples as well as completing
missing values. In fact  by employing CNNs in both recognition and generative models  we achieve
low error rates on MNIST and SVHN datasets  which are competitive to the state-of-the-art fully
discriminative networks.
Acknowledgments
The work was supported by the National Basic Research Program (973 Program) of China (Nos.
2013CB329403  2012CB316301)  National NSF of China (Nos. 61322308  61332007)  Tsinghua TNList Lab
Big Data Initiative  and Tsinghua Initiative Scientiﬁc Research Program (Nos. 20121088071  20141080934).
References
[1] Y. Altun  I. Tsochantaridis  and T. Hofmann. Hidden Markov support vector machines. In ICML  2003.
[2] F. Bastien  P. Lamblin  R. Pascanu  J. Bergstra  I. Goodfellow  A. Bergeron  N. Bouchard  D. Warde-
Farley  and Y. Bengio. Theano: new features and speed improvements. In Deep Learning and Unsuper-
vised Feature Learning NIPS Workshop  2012.

[3] Y. Bengio  E. Laufer  G. Alain  and J. Yosinski. Deep generative stochastic networks trainable by back-

prop. In ICML  2014.

[4] N. Chen  J. Zhu  F. Sun  and E. P. Xing. Large-margin predictive latent subspace learning for multi-view

data analysis. IEEE Trans. on PAMI  34(12):2365–2378  2012.

8

[5] C. Cortes and V. Vapnik. Support-vector networks. Journal of Machine Learning  20(3):273–297  1995.
[6] A. Dosovitskiy  J. T. Springenberg  and T. Brox. Learning to generate chairs with convolutional neural

networks. arXiv:1411.5928  2014.

[7] I. J. Goodfellow  J. P. Abadie  M. Mirza  B. Xu  D. W. Farley  S.ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In NIPS  2014.

[8] I. J. Goodfellow  D.Warde-Farley  M. Mirza  A. C. Courville  and Y. Bengio. Maxout networks. In ICML 

2013.

[9] K. Gregor  I. Danihelka  A. Mnih  C. Blundell  and D. Wierstra. Deep autoregressive networks. In ICML 

2014.

[10] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR  2015.
[11] D. P. Kingma  D. J. Rezende  S. Mohamed  and M. Welling. Semi-supervised learning with deep genera-

tive models. In NIPS  2014.

[12] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR  2014.
[13] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS  2011.
[14] Y. Lecun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document recognition.

In Proceedings of the IEEE  1998.

[15] C. Lee  S. Xie  P. Gallagher  Z. Zhang  and Z. Tu. Deeply-supervised nets. In AISTATS  2015.
[16] H. Lee  R. Grosse  R. Ranganath  and A. Y. Ng. Convolutional deep belief networks for scalable unsu-

pervised learning of hierarchical representations. In ICML  2009.
[17] M. Lin  Q. Chen  and S. Yan. Network in network. In ICLR  2014.
[18] R. J. Little and D. B. Rubin. Statistical analysis with missing data. JMLR  539  1987.
[19] L. V. Matten and G. Hinton. Visualizing data using t-SNE. JMLR  9:2579–2605  2008.
[20] K. Miller  M. P. Kumar  B. Packer  D. Goodman  and D. Koller. Max-margin min-entropy models. In

AISTATS  2012.

[21] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML  2014.
[22] Y. Netzer  T. Wang  A. Coates  A. Bissacco  B. Wu  and A. Y. Ng. Reading digits in natural images with
unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning 
2011.

[23] M. Ranzato  J. Susskind  V. Mnih  and G. E. Hinton. On deep generative models with applications to

recognition. In CVPR  2011.

[24] D. J. Rezende  S. Mohamed  and D. Wierstra. Stochastic backpropagation and approximate inference in

deep generative models. In ICML  2014.

[25] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS  2009.
[26] L. Saul  T. Jaakkola  and M. Jordan. Mean ﬁeld theory for sigmoid belief networks. Journal of AI

Research  4:61–76  1996.

[27] P. Sermanet  S. Chintala  and Y. Lecun. Convolutional neural networks applied to house numbers digit

classiﬁcation. In ICPR  2012.

[28] S. Shalev-Shwartz  Y. Singer  N. Srebro  and A. Cotter. Pegasos: Primal estimated sub-gradient solver for

SVM. Mathematical Programming  Series B  2011.

[29] Y. Tang. Deep learning using linear support vector machines. In Challenges on Representation Learning

Workshop  ICML  2013.

[30] B. Taskar  C. Guestrin  and D. Koller. Max-margin Markov networks. In NIPS  2003.
[31] I. Tsochantaridis  T. Hofmann  T. Joachims  and Y. Altun. Support vector machine learning for interde-

pendent and structured output spaces. In ICML  2004.

[32] C. J. Yu and T. Joachims. Learning structural SVMs with latent variables. In ICML  2009.
[33] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks.

In ICLR  2013.

[34] J. Zhu  A. Ahmed  and E. P. Xing. MedLDA: Maximum margin supervised topic models. JMLR  13:2237–

2278  2012.

[35] J. Zhu  N. Chen  H. Perkins  and B. Zhang. Gibbs max-margin topic models with data augmentation.

JMLR  15:1073–1110  2014.

[36] J. Zhu  N. Chen  and E. P. Xing. Bayesian inference with posterior regularization and applications to

inﬁnite latent SVMs. JMLR  15:1799–1847  2014.

[37] J. Zhu  E.P. Xing  and B. Zhang. Partially observed maximum entropy discrimination Markov networks.

In NIPS  2008.

9

,Chongxuan Li
Jun Zhu
Tianlin Shi
Bo Zhang
Fuhai Chen
Rongrong Ji
Jiayi Ji
Xiaoshuai Sun
Baochang Zhang
Xuri Ge
Yongjian Wu
Feiyue Huang
Yan Wang