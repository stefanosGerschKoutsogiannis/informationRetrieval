2016,Agnostic Estimation for Misspecified Phase Retrieval Models,The goal of noisy high-dimensional phase retrieval is to estimate an $s$-sparse parameter $\boldsymbol{\beta}^*\in \mathbb{R}^d$ from $n$ realizations of the model $Y = (\boldsymbol{X}^{\top} \boldsymbol{\beta}^*)^2 + \varepsilon$. Based on this model  we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR)  in which $Y = f(\boldsymbol{X}^{\top}\boldsymbol{\beta}^*  \varepsilon)$ with unknown $f$ and $\operatorname{Cov}(Y  (\boldsymbol{X}^{\top}\boldsymbol{\beta}^*)^2) > 0$. For example  MPR encompasses $Y = h(|\boldsymbol{X}^{\top} \boldsymbol{\beta}^*|) + \varepsilon$ with increasing $h$ as a special case. Despite the generality of the MPR model  it eludes the reach of most existing semi-parametric estimators. In this paper  we propose an estimation procedure  which consists of solving a cascade of two convex programs and provably recovers the direction of $\boldsymbol{\beta}^*$. Our theory is backed up by thorough numerical results.,Agnostic Estimation for Misspeciﬁed

Phase Retrieval Models

Matey Neykov

Zhaoran Wang

Han Liu

Department of Operations Research and Financial Engineering

Princeton University  Princeton  NJ 08544

{mneykov  zhaoran  hanliu}@princeton.edu

Abstract

The goal of noisy high-dimensional phase retrieval is to estimate an s-sparse pa-
rameter β∗ ∈ Rd from n realizations of the model Y = (X(cid:62)β∗)2 + ε. Based
on this model  we propose a signiﬁcant semi-parametric generalization called mis-
speciﬁed phase retrieval (MPR)  in which Y = f (X(cid:62)β∗  ε) with unknown f and
Cov(Y  (X(cid:62)β∗)2) > 0. For example  MPR encompasses Y = h(|X(cid:62)β∗|) + ε
with increasing h as a special case. Despite the generality of the MPR model  it
eludes the reach of most existing semi-parametric estimators. In this paper  we pro-
pose an estimation procedure  which consists of solving a cascade of two convex
programs and provably recovers the direction of β∗. Our theory is backed up by
thorough numerical results.

i )(cid:62)}n

Introduction

1
In scientiﬁc and engineering ﬁelds researchers often times face the problem of quantifying the
relationship between a given outcome Y and corresponding predictor vector X  based on a sample
{(Yi  X(cid:62)
i=1 of n observations. In such situations it is common to postulate a linear “working”
model  and search for a d-dimensional signal vector β∗ satisfying the following familiar relationship:
(1.1)
When the predictor X is high-dimensional in the sense that d (cid:29) n  it is commonly assumed that
the underlying signal β∗ is s-sparse. In a certain line of applications  such as X-ray crystallography 
microscopy  diffraction and array imaging1  one can only measure the magnitude of X(cid:62)β∗ but not
its phase (i.e.  sign in the real domain). In this case  assuming model (1.1) may not be appropriate. To
cope with such applications in the high-dimensional setting  [7] proposed the thresholded Wirtinger
ﬂow (TWF)  a procedure which consistently estimates the signal β∗ in the following real sparse
noisy phase retrieval model:

Y = X(cid:62)β∗ + ε.

(1.2)
where one additionally knows that the predictors have a Gaussian random design X ∼ N (0  Id). In
the present paper  taking an agnostic point of view  we recognize that both models (1.1) and (1.2)
represent an idealized view of the data generating mechanism. In reality  the nature of the data could
be better reﬂected through the more ﬂexible viewpoint of a single index model (SIM):

Y = (X(cid:62)β∗)2 + ε 

(1.3)
where f is an unknown link function  and it is assumed that (cid:107)β∗(cid:107)2 = 1 for identiﬁability. A recent
line of work on high-dimensional SIMs [25  27]  showed that under Gaussian designs  one can apply
(cid:96)1 regularized least squares to successfully estimate the direction of β∗ and its support. The crucial
condition allowing for the above somewhat surprising application turns out to be:

Y = f (X(cid:62)β∗  ε) 

Cov(Y  X(cid:62)β∗) (cid:54)= 0.

(1.4)
While condition (1.4) is fairly generic  encompassing cases with a binary outcome  such as logistic
regression and one-bit compressive sensing [5]  it fails to capture the phase retrieval model (1.2).
1In such applications it is typically assumed that X ∈ Cd is a complex normal random vector. In this paper

for simplicity we only consider the real case X ∈ Rd.
30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

More generally  it is easy to see that when the link function f is even in its ﬁrst coordinate  condition
(1.4) fails to hold. The goal of the present manuscript is to formalize a class of SIMs  which includes
the noisy phase retrieval model as a special case in addition to various other additive and non-additive
models with even link functions  and develop a procedure that can successfully estimate the direction
of β∗ up to a global sign. Formally  we consider models (1.3) with Gaussian design that satisfy the
following moment assumption:

Cov(Y  (X(cid:62)β∗)2) > 0.

(1.5)
Unlike (1.4)  one can immediately check that condition (1.5) is satisﬁed by model (1.2). In §2 we give
multiple examples  both abstract and concrete  of SIMs obeying this constraint. Our second moment
constraint (1.5) can be interpreted as a semi-parametric robust version of phase-retrieval. Hence  we
will refer to the class of models satisfying condition (1.5) as misspeciﬁed phase retrieval (MPR)
models. In this point of view it is worth noting that condition (1.4) relates to linear regression in a
way similar to how condition (1.5) relates to the phase retrieval model. Our motivation for studying
SIMs under such a constraint can ultimately be traced to the vast sufﬁcient dimension reduction
(SDR) literature. In particular  we would like to point out [22] as a source of inspiration.
Contributions. Our ﬁrst contribution is to formulate a novel and easily implementable two-step
procedure  which consistently estimates the direction of β∗ in an MPR model. In the ﬁrst step

we solve a semideﬁnite program producing a unit vector(cid:98)v  such that |(cid:98)v(cid:62)β∗| is sufﬁciently large.
augmented outcome (cid:101)Yi = (Yi − Y )X(cid:62)
i (cid:98)v  where Y is the average of Yi’s  to produce a second
estimate(cid:98)b  which is then normalized to obtain the ﬁnal reﬁned estimator (cid:98)β =(cid:98)b/(cid:107)(cid:98)b(cid:107)2. In addition

Once such a pilot estimate is available  we consider solving an (cid:96)1 regularized least squares on the

to being universally applicable to MPR models  our procedure has an algorithmic advantage in that
it relies solely on convex optimization  and as a consequence we can obtain the corresponding global
minima of the two convex programs in polynomial time.
Our second contribution is to rigorously demonstrate that the above procedure consistently estimates
the direction of β∗. We prove that for a given MPR model  with high probability  one has:

minη∈{−1 1}(cid:107)(cid:98)β − ηβ∗(cid:107)2 (cid:46)(cid:112)s log d/n 

provided that the sample size n satisﬁes n (cid:38) s2 log d. While the same rates (with different constants)
hold for the TWF algorithm of [7] in the special case of noisy phase retrieval model  our procedure
provably achieves these rates over the broader class of MPR models.
Lastly  we propose an optional reﬁnement of our algorithm  which shows improved performance in
the numerical studies.
Related Work. The phase retrieval model has received considerable attention in the recent years by
statistics  applied mathematics as well as signal processing communities. For the non-sparse version
of (1.2)  efﬁcient algorithms have been suggested based on both semideﬁnite programs [8  10] and
non-convex optimization methods that extend gradient descent [9]. Additionally  a non-traditional
instance of phase retrieval model (which also happens to be a special case of the MPR model) was
considered by [11]  where the authors suggested an estimation procedure originally proposed for the
problem of mixed regression. For the noisy sparse version of model (1.2)  near optimal solutions
were achieved with a computationally infeasible program by [20]. Subsequently  a tractable gradient
descent approach achieving minimax optimal rates was developed by [7].
Abstracting away from the phase retrieval or linear model settings  we note that inference for SIMs
in the case when d is small or ﬁxed  has been studied extensively in the literature [e.g.  18  24  26  34 
among many others]. In another line of research on SDR  seminal insights shedding light on condition
(1.4) can be found in  e.g.  [12  21  23]. The modiﬁed condition (1.5) traces roots to [22]  where the
authors designed a procedure to handle precisely situations where (1.4) fails to hold. More recently 
there have been active developments for high-dimensional SIMs. [27] and later [31] demonstrated that
under condition (1.4)  running the least squares with (cid:96)1 regularization can obtain a consistent estimate
of the direction of β∗  while [25] showed that this procedure also recovers the signed support of the
direction. Excess risk bounds were derived in [14]. Very recently  [16] extended this observation to
other convex loss functions under a condition corresponding to (1.4) depending implicitly on the loss
function of interest. [28] proposed a non-parametric least squares with an equality (cid:96)1 constraint to
handle simultaneous estimation of β∗ as well as f. [17] considered a smoothed-out U-process type
of loss function with (cid:96)1 regularization  and proved their approach works for a sub-class of functions
satisfying condition (1.4). None of the aforementioned works on SIMs can be directly applied to
tackle the MPR class (1.5). A generic procedure for estimating sparse principal eigenvectors was

2

developed in [37]. While in principle this procedure can be applied to estimate the direction in MPR
models  it requires proper initialization  and in addition  it requires knowledge of the sparsity of the
vector β∗. We discuss this approach in more detail in §4.
Regularized procedures have also been proposed for speciﬁc choices of f and Y . For example  [36]
studied consistent estimation under the model P(Y = 1|X) = (h(X(cid:62)β∗) + 1)/2 with binary Y  
where h : R (cid:55)→ [−1  1] is possibly unknown. Their procedure is based on taking pairs of differences
in the outcome  and therefore replaces condition (1.4) with a different type of moment conditon. [35]
considered the model Y = h(X(cid:62)β∗) + ε with a known continuously differentiable and monotonic
h  and developed estimation and inferential procedures based on the (cid:96)1 regularized quadratic loss  in
a similar spirit to the TWF algorithm suggested by [7]. In conclusion  although there exists much
prior related work  to the best of our knowledge  none of the available literature discusses the MPR
models in the generality we attempt in the present manuscript.
Notation. We now brieﬂy outline some commonly used notations. Other notations will be deﬁned as
needed throughout the paper. For a (sparse) vector v = (v1  . . .   vp)(cid:62)  we let Sv := supp(v) = {j :
vj (cid:54)= 0} denote its support  (cid:107)v(cid:107)p denote the (cid:96)p norm (with the usual extension when p = ∞) and
v⊗2 := vv(cid:62) is a shorthand for the outer product. With a standard abuse of notation we will denote
by (cid:107)v(cid:107)0 = |supp(v)| the cardinality of the support of v. We often use Id to denote a d × d identity
matrix. For a real random variable X  deﬁne

(cid:107)X(cid:107)ψ2 = sup
p≥1

p−1/2(E|X|p)1/p  (cid:107)X(cid:107)ψ1 = sup
p≥1

p−1(E|X|p)1/p.

Recall that a random variable is called sub-Gaussian if (cid:107)X(cid:107)ψ2 < ∞ and sub-exponential if (cid:107)X(cid:107)ψ1 <
∞ [e.g.  32]. For any integer k ∈ N we use the shorthand notation [k] = {1  . . .   k}. We also use
standard asymptotic notations. Given two sequences {an} {bn} we write an = O(bn) if there exists
a constant C < ∞ such that an ≤ Cbn  and an (cid:16) bn if there exist positive constants c and C such
that c < an/bn < C.
Organization. In §2 and §3 we introduce the MPR model class and our estimation procedure  and
§3.1 is dedicated to stating the theoretical guarantees of our proposed algorithm. Simulation results
are given in §4. A brief discussion is provided in §5. We defer the proofs to the appendices due to
space limitations.
2 MPR Models
In this section we formally introduce MPR models. In detail  we argue that the class of such models
is sufﬁciently rich  including numerous models of interest. Motivated by the setup in the sparse noisy
phase retrieval model (1.2)  we assume throughout the remainder of the paper that X ∼ N (0  Id).
We begin our discussion with a formal deﬁnition.
Deﬁnition 2.1 (MPR Models). Assume that we are given model (1.3)  where X ∼ N (0  Id)  ε ⊥⊥ X
and β∗ ∈ Rd is an s-sparse unit vector  i.e.  (cid:107)β∗(cid:107)2 = 1. We call such a model misspeciﬁed phase
retrieval (MPR) model  if the link function f and noise ε further satisfy  for Z ∼ N (0  1) and K > 0 
(2.2)
Both assumptions (2.1) and (2.2) impose moment restrictions on the random variable Y . Assumption
(2.1) states that Y is positively correlated with the random variable (X(cid:62)β∗)2  while assumption
(2.2) requires Y to have somewhat light-tails. Also  as mentioned in the introduction  the unit norm
constraint on the vector β∗ is required for the identiﬁability of model (1.3). We remark that the class of
MPR models is convex in the sense that if we have two MPR models f1(X(cid:62)β∗  ε) and f2(X(cid:62)β∗  ε) 
all models generated by their convex combinations λf1(X(cid:62)β∗  ε)+(1−λ)f2(X(cid:62)β∗  ε) (λ ∈ [0  1])
are also MPR models. It is worth noting the > direction in (2.1) is assumed without loss of generality.
If Cov(Y  (X(cid:62)β∗)2) < 0 one can apply the same algorithm to −Y . However  the knowledge of the
direction of the inequality is important.
In the following  we restate condition (2.1) in a more convenient way  enabling us to easily calculate
the explicit value of the constant c0 in several examples.
Proposition 2.2. Assume that there exists a version of the map ϕ(z) : z (cid:55)→ E[f (Z  ε)|Z = z] such
that ED2ϕ(Z) > 0  where D2 is the second distributional derivative of ϕ and Z ∼ N (0  1). Then
the SIM (1.3) satisﬁes assumption (2.1) with c0 = ED2ϕ(Z).
We now provide three concrete MPR models as warm up examples for the more general examples
discussed in Proposition 2.3 and Remark 2.3. Consider the models:

c0 := Cov(f (Z  ε)  Z 2) > 0 

(cid:107)Y (cid:107)ψ1 ≤ K.

(2.1)

3

Y = (X(cid:62)β∗)2 + ε 

Y = |X(cid:62)β∗| + ε 

(2.4)

Y = |X(cid:62)β∗ + ε| 

√

(2.3)

(2.5)
where ε ⊥⊥ X is sub-exponential noise  i.e.  (cid:107)ε(cid:107)ψ1 ≤ Kε for some Kε > 0. Model (2.3) is the
noisy phase retrieval model considered by [7]  while models (2.4) and (2.5) were both discussed
in [11]  where the authors proposed a method to solve model (2.5) in the low-dimensional regime.
Below we brieﬂy explain why these models satisfy conditions (2.1) and (2.2). First  observe that in
all three models we have a sum of two sub-exponential random variables  and hence by the triangle
inequality it follows that the random variable Y is also sub-exponential  which implies (2.2). To
understand why (2.1) holds  by applying Proposition 2.2 we have c0 = E2 = 2 > 0 for model (2.3) 
c0 = E2δ0(Z) = 2/
2π > 0 for model (2.4)  and c0 = E2δ0(Z + ε) = 2Eφ(ε) > 0 for model
(2.5)  where δ0(·) is the Dirac delta function centered at zero  and φ is the density of the standard
normal distribution.
Admittedly  calculating the second distributional derivative could be a laborious task in general. In
the remainder of this section we set out to ﬁnd a simple to check generic sufﬁcient condition on the
link function f and error term ε  under which both (2.1) and (2.2) hold. Before giving our result note
that the condition ED2ϕ(Z) > 0 is implied whenever ϕ is strictly convex and twice differentiable.
However  strictly convex functions ϕ may violate assumption (2.2) as they can inﬂate the tails of Y
arbitrarily (consider  e.g.  f (x  ε) = x4 + ε). Moreover  the functions in example (2.4) and (2.5) fail
to be twice differentiable. In the following result we handle those two problems  and in addition we
provide a more generic condition than convexity  which sufﬁces to ensure the validity of (2.1).
Proposition 2.3. The following statements hold.

(i) Let the function ϕ deﬁned in Proposition 2.2 be such that the map z (cid:55)→ ϕ(z) + ϕ(−z)
0 and and there exist z1 > z2 > 0 such that ϕ(z1) + ϕ(−z1) >
is non-decreasing on R+
ϕ(z2) + ϕ(−z2). Then (2.1) holds.
(ii) A sufﬁcient condition for (i) to hold  is that z (cid:55)→ ϕ(z) is convex and sub-differentiable at
0 satisfying ϕ(z0) + ϕ(−z0) > 2ϕ(0).
every point z ∈ R  and there exists a point z0 ∈ R+
(iii) Assume that there exist functions g1  g2 such that f (Z  ε) ≤ g1(Z) + g2(ε)  and g1 is
essentially quadratic in the sense that there exists a closed interval I = [a  b] with 0 ∈ I 
such that for all z satisfying g1(z) ∈ I c we have |g1(z)| ≤ Cz2 for a sufﬁciently large
constant C > 0  and let g2(ε) be sub-exponential. Then (2.2) holds.

Remark 2.4. Proposition 2.3 shows that the class of MPR models is sufﬁciently broad. By (i) and
(ii) it immediately follows that the additive models

Y = h(X(cid:62)β∗) + ε 

(2.6)
where the link function h is even and increasing on R+
0 or convex  satisfy the covariance condition
(2.1) by (i) and (ii) of Proposition 2.3 respectively. If h is also essentially quadratic and ε is sub-
exponentially distributed  using (iii) we can deduce that Y in (2.6) is a sub-exponential random
variable  and hence under these restrictions model (2.6) is an MPR model. Both examples (2.3) and
(2.4) take this form.
Additionally  Proposition 2.3 implies that the model

Y = h(X(cid:62)β∗ + ε)

(2.7)
satisﬁes (2.1)  whenever the link h is a convex sub-differentiable function  such that h(z0)+h(−z0) >
2h(0) for some z0 > 0  E|h(z + ε)| < ∞ for all z ∈ R and E|h(Z + ε)| < ∞. This conclusion
follows because under the latter conditions the function ϕ(z) = Eh(z + ε) satisﬁes (ii)  which is
proved in Appendix C under Lemma C.1. Moreover  if it turns out that h is essentially quadratic and
h(2ε) is sub-exponential  then by Jensen’s inequality we have 2h(Z +ε) ≤ h(2Z)+h(2ε) and hence
(iii) implies that (2.2) is also satisﬁed. Model (2.5) is of the type (2.7). Unlike the additive noise
models (2.6)  models (2.7) allow noise corruption even within the argument of the link function. On
the negative side  it should be apparent that (2.1) fails to hold in cases where ϕ is an odd function  i.e. 
ϕ(z) = −ϕ(−z). In many such cases (e.g. when ϕ is monotone or non-constant and non-positive/non-
negative on R+)  one would have Cov(Y  X(cid:62)β∗) = E[ϕ(Z)Z] (cid:54)= 0  and hence direct application
of the (cid:96)1 regularized least squares algorithm is possible as we discussed in the introduction.
3 Agnostic Estimation for MPR
In this section we describe and motivate our two-step procedure  which consists of a convex relaxation
and an (cid:96)1 regularized least squares program  for performing estimation in the MPR class of models

4

described by Deﬁnition 2.1. We begin our motivation by noting that any MPR model satisﬁes the
following inequality

Cov((Y − µ)X(cid:62)β∗  X(cid:62)β∗) = E{(Y − µ)(X(cid:62)β∗)2} = Cov(f (Z  ε)  Z 2) = c0 > 0 

to sharpen the convergence rate. At ﬁrst glance it might appear counter-intuitive that introducing a

(3.1)
where we have denoted µ := EY . This simple observation plays a major role in the motivation
of our procedure. Notice that in view of condition (1.4)  inequality (3.1) implies that if instead of
observing Y we had observed ˘Y = g(X(cid:62)β∗  ε) = (Y − µ)X(cid:62)β∗. However  there is no direct way
of generating the random variable ˘Y   as doing so would require the knowledge of β∗ and the mean µ.

Here  we propose to roughly estimate β∗ by a vector(cid:98)v ﬁrst  use an empirical estimate Y of µ  and
then obtain the (cid:96)1 regularized least squares estimate on the augmented variable (cid:101)Y = (Y − Y )X(cid:62)(cid:98)v
noisy estimate of β∗ can lead to consistent estimates  as the so-deﬁned (cid:101)Y variable depends on the
projection of X on span{β∗ (cid:98)v}. Decompose
where (cid:98)β⊥ ⊥ β∗. To better motivate this proposal  in the following we analyze the population least
squares ﬁt  based on the augmented variable ˇY = (Y − µ)X(cid:62)(cid:98)v for some ﬁxed unit vector(cid:98)v with

(cid:98)v = ((cid:98)v(cid:62)β∗)β∗ + (cid:98)β⊥ 
[EX⊗2]−1E[X ˇY ] = E[X(Y − µ)X(cid:62)((cid:98)v(cid:62)β∗)β∗]
(cid:125)

+ E[X(Y − µ)X(cid:62)(cid:98)β⊥]
(cid:124)
(cid:125)

decomposition (3.2). Writing out the population solution for least squares yields:

We will now argue that left hand side of (3.3) is proportional to β∗. First  we observe that

I1 = c0((cid:98)v(cid:62)β∗)β∗  since multiplying by any vector b ⊥ β∗ yields b(cid:62)I1 = 0 by independence.
tor b ∈ span{β∗ (cid:98)β⊥}⊥. Since the three variables b(cid:62)X  Y − µ and (cid:98)β⊥X are independent  we have
X(Y − µ) is independent of X(cid:62)(cid:98)β⊥.
Finally  multiplying by (cid:98)β⊥ yields I(cid:62)

Second  and perhaps more importantly  we have that I2 = 0. To see this  we ﬁrst take a vec-
b(cid:62)I2 = 0. Multiplying by β∗ we have β∗(cid:62)

2 (cid:98)β⊥ = 0  since (X(cid:62)(cid:98)β⊥)2 is independent of Y − µ.

I2 = 0 since β∗(cid:62)

.

(3.3)

(cid:123)(cid:122)

I1

(3.2)

(cid:124)

(cid:123)(cid:122)

I2

(b) Second Step

(a) Initialization

1. After the ﬁrst step we can guarantee that the vector β∗ belongs to one of two spherical caps which
n (cid:38) s2 log d is sufﬁciently large. After the second step we can guarantee that the vector β∗ belongs
to one of two spherical caps in (b)  which are shrinking with (n  s  d) at a faster rate.
It is noteworthy to mention that the above derivation crucially relies on the fact that the Y variable

Figure 1: An illustration of the estimates(cid:98)v and (cid:98)β produced by the ﬁrst and second steps of Algorithm
contain all vectors w such that |(cid:98)v(cid:62)w| ≥ κ for some constant κ > 0  provided that the sample size
was centered  and the vector(cid:98)v was ﬁxed. In what follows we formulate a pilot procedure which
produces an estimate(cid:98)v such that |(cid:98)v(cid:62)β∗| ≥ κ > 0. A proper initialization algorithm can be achieved
by using a spectral method  such as the Principal Hessian Directions (PHD) proposed by [22]. Cast
into the framework of SIM  the PHD framework implies the following simple observation:
Lemma 3.1. If we have an MPR model  then argmax(cid:107)v(cid:107)2=1 v(cid:62)E[Y (X⊗2 − I)]v = ±β∗.
A proof of this fact can be found in Appendix C. Lemma 3.1 encourages us to look into the following
sample version maximization problem

argmax(cid:107)v(cid:107)2=1 (cid:107)v(cid:107)0=sn−1v(cid:62)(cid:80)n
by [13]. Instead of solving (3.4)  deﬁne(cid:98)Σ = n−1(cid:80)n

(3.4)
which targets a restricted (s-sparse) principal eigenvector. Unfortunately  solving such a problem is a
computationally intensive task  and requires knowledge of s. Here we take a standard route of relaxing
the above problem to a convex program  and solving it efﬁciently via semideﬁnite programming
(SDP). A similar in spirit SDP relaxation for solving sparse PCA problems  was originally proposed
i −I)  and solve the following convex

i=1[Yi(X⊗2

i=1 Yi(X⊗2

i − I)]v 

5

program:

(cid:98)A = argmaxtr(A)=1 A∈Sd

tr((cid:98)ΣA) − λn

(cid:80)d
i j=1|Aij| 

(3.5)
where Sd
+ is the convex cone of non-negative semideﬁnite matrices  and λn is a regularization param-
eter encouraging element-wise sparsity in the matrix A. The hopes of introducing the optimization

program above are that (cid:98)A will be a good ﬁrst estimate of β∗⊗2. In practice it could turn out that the
matrix (cid:98)A is not rank one  hence we suggest taking(cid:98)v as the principal eigenvector of (cid:98)A. In theory we
show that with high probability the matrix (cid:98)A will indeed be of rank one. Observation (3.3)  Lemma

+

i∈S1
i∈S2

i b)2 + νn(cid:107)b(cid:107)1.

(3.6)

i (cid:98)v − X(cid:62)

i=1: data  λn  νn: tuning parameters

(cid:98)b = argminb(2|S2|)

Yi(X⊗2
Yi. Solve the following program:
((Yi − Y )X(cid:62)

3.1 and the SDP formulation motivate the agnostic two-step estimation procedure for misspeciﬁed
phase retrieval in Algorithm 1.
Algorithm 1
input :(Yi  Xi)n
1. Split the sample into two approximately equal sets S1  S2  with |S1| = (cid:98)n/2(cid:99)  |S2| = (cid:100)n/2(cid:101).

i − Id). Solve (3.5). Let(cid:98)v be the ﬁrst eigenvector of (cid:98)A.
−1(cid:80)

2. Let (cid:98)Σ := |S1|−1(cid:80)
3. Let Y = |S2|−1(cid:80)
4. Return (cid:98)β :=(cid:98)b/(cid:107)(cid:98)b(cid:107)2.
The sample split is required to ensure that after decomposition (3.2)  the vector (cid:98)β⊥ and the value
(cid:98)v(cid:62)β∗ are independent of the remaining sample. In §3.1 we demonstrate that Algorithm 1 succeeds
with optimal (in the noisy regime) (cid:96)2 rate(cid:112)s log d/n  provided that s2 log d (cid:46) n. The latter require-
ment on the sample size sufﬁces to guarantee that the solution (cid:98)A of optimization program (3.5) is
1 on the full dataset using the output vector (cid:98)β of Algorithm 1. Doing so can potentially result in
5. Let Y = n−1(cid:80)
6. Return (cid:98)β(cid:48) :=(cid:98)b/(cid:107)(cid:98)b(cid:107)2.

additional stability and further reﬁnements of the rate constant.
Algorithm 2 Optional Reﬁnement
input :(Yi  Xi)n

rank one. Figure 1 illustrates the two steps of Algorithm 1. In addition to our main procedure  we
propose an optional reﬁnement step (Algorithm 2) in which one applies steps 3. and 4. of Algorithm

n: tuning parameter  output (cid:98)β from the Algorithm 1

(cid:98)b = argminb(2n)−1(cid:80)n

i∈[n] Yi. Solve the following program:
i=1((Yi − Y )X(cid:62)

i (cid:98)β − X(cid:62)

i b)2 + ν(cid:48)

n(cid:107)b(cid:107)1.

i=1: data  ν(cid:48)

i∈S2

(3.7)

3.1 Theoretical Guarantees
In this section we present our main theoretical results  which consist of theoretical justiﬁcation of our
procedures  as well as lower bounds for certain types of SIM (1.3). To simplify the presentation for
this section  we slightly change the notation and assume that the sample size is 2n and S1 = [n] and
S2 = {n + 1  . . .   2n}. Of course this abuse of notation does not restrict our analysis to only even
sample size cases.
close to the vector β∗.

Our ﬁrst result shows that the optimization program (3.5) succeeds in producing a vector(cid:98)v which is
Proposition 3.2. Assume that n is large enough so that s(cid:112)log d/n < (1/6 − κ/4)c0/(C1 + C2)
value of λn (cid:16)(cid:112)log d/n such that the principal eigenvector(cid:98)v of (cid:98)A  the solution of (3.5)  satisﬁes
of β∗ to a union of two spherical caps (i.e.  the estimate(cid:98)v satisﬁes |(cid:98)v(cid:62)β∗| ≥ κ for some constant

with probability at least 1 − 4d−1 − O(n−1).
Proposition 3.2 shows that the ﬁrst step of Algorithm 1 narrows down the search for the direction

for some small but ﬁxed κ > 0 and constants C1  C2 (depending on f and ε). Then there exists a

|(cid:98)v(cid:62)β∗| ≥ κ > 0 

κ > 0  see also Figure 1a). Our main result below  demonstrates that in combination with program
(3.6) this sufﬁces to recover the direction of β∗ at an optimal rate with high probability.

6

(cid:18)

Theorem 3.3. There exist values of λn  νn (cid:16)(cid:112)log d/n and a constant R > 0 depending on f and
ε  such that if s(cid:112)log d/n < R and log(d) log2(n)/n = o(1)  the output of Algorithm 1 satisﬁes:
We remark that although the estimation rate is of the order(cid:112)s log d/n  our procedure still requires
that s(cid:112)log d/n is sufﬁciently small. This phenomenon is similar to what has been observed by [7] 
failure  it is less clear whether the estimate (cid:98)β is universally consistent (i.e.  whether the sup can be

and it is our belief that this requirement cannot be relaxed for computationally feasible algorithms.
We would further like to mention that while in bound (3.8) we control the worst case probability of

(cid:114)
(cid:107)(cid:98)β − ηβ∗(cid:107)2 > L

where L is a constant depending solely on f and ε.

≤ O(d−1 ∨ n−1) 

(cid:107)β∗(cid:107)2=1 (cid:107)β∗(cid:107)0≤s

η∈{1 −1}

(cid:19)

s log d

Pβ∗

(3.8)

min

sup

n

k∈K

i∈S2

The TPM is ran for 2000 iterations. In the case of phase retrieval  the TWF algorithm was also ran
at a total number of 2000 iterations  using the tuning parameters originally suggested in [7]. As
expected the TWF algorithm which targets the sparse phase retrieval model in particular outperforms
our approach in the case when the sample size n is small  however our approach performs very
comparatively to the TWF  and in fact even slightly better once we increase the sample size. It is
possible that the TWF algorithm can perform better if it is ran for a longer than 2000 iterations 
though in most cases it appeared to have converged to its ﬁnal value. The results are visualized on
Figure 2 above. The TPM algorithm  has performance comparable to that of Algorithm 1  is always

7

s

(cid:125)

(cid:124)

d−s

  0  . . . 0

(cid:123)(cid:122)

(cid:124) (cid:123)(cid:122) (cid:125)

moved inside the probability in (3.8)).
4 Numerical Experiments
In this section we provide numerical experiments based on the three models (2.3)  (2.4) and (2.5)
where the random variable ε ∼ N (0  1). All models are compared with the Truncated Power Method
(TPM)  proposed in [37]. For model (2.3) we also compare the results of our approach to the ones
given by the TWF algorithm of [7]. Our setup is as follows. In all scenarios the vector β∗ was held
ﬁxed at β∗ = (−s−1/2  s−1/2  . . .   s−1/2
). Since our theory requires that n (cid:38) s2 log d  we
have setup four different sample sizes n ≈ θs2 log d  where θ ∈ {4  8  12  16}. We let s depend on
the dimension d and we take s ≈ log d. In addition to the suggested approach in Algorithm 1  we
also provide results using the reﬁnement procedure (see Algorithm 3.7). We also provide the values
of two “warm” starts of our algorithm  produced by solving program (3.5) with half and full data
correspondingly. It is evident that for all scenarios the second step of Algorithms 1 and 2 outperform
the warm start from SDP  except in Figure 2 (b)  (c)  when the sample size is simply two small to for
the warm start on half of the data to be accurate. All values we report are based on an average over
100 simulations.
The SDP parameter was kept at a constant value (0.015) throughout all simulations  and we observed
that varying this parameter had little inﬂuence on the ﬁnal SDP solution. To select the νn parameter
for (3.6) a pre-speciﬁed grid of parameters {ν1  . . .   νl} was selected  and the following heuristic
equally sized non-intersecting sets S2 = ∪j∈[K](cid:101)Sj
procedure based on K-fold cross-validation was used. We divide S2 into K = 5 approximately
2 with a tuning parameter νn = νk to obtain an estimate (cid:98)βk −(cid:101)Sj
set ∪r∈[K] r(cid:54)=j(cid:101)Sr
2. For each j ∈ [K] and k ∈ [l] we run (3.6) on the
justiﬁes the following criteria to select the optimal index for selecting(cid:98)νn = ν(cid:98)l where
. Lemma 3.1 then
(cid:88)
(cid:88)
i∈(cid:101)Sj
are selected within appropriate range and are of the magnitude(cid:112)log d/n.
estimate (cid:98)βk  and the ﬁnal estimate is taken to be (cid:98)β(cid:98)k where(cid:98)k is given by
i − Id)(cid:98)βk.

Since the TPM algorithm requires an estimate of the sparsity s  we tuned it as suggested in Section
4.1.2 of [37]. In particular  for each scenario we considered the set of possible sparsities K =
{s  2s  4s  8s}. For each k ∈ K the algorithm is ran on the ﬁrst part of the data S1  to obtain an

Our experience suggests this approach works well in practice provided that the values {ν1  . . .   νl}

k |S2|−1(cid:88)
(cid:98)β(cid:62)

(cid:98)k = argmax

(cid:98)l = argmax

i (cid:98)βk −(cid:101)Sj

Yi(X⊗2

Yi(X(cid:62)

j∈[K]

k∈[l]

)2.

2

2

2

(a) Model (2.3)  d = 200

(b) Model (2.4)  d = 200

(c) Model (2.5)  d = 200

(d) Model (2.3)  d = 400

(e) Model (2.4)  d = 400

(f) Model (2.5)  d = 400

Figure 2: Simulation results for the three examples considered in §2  in two different settings for the
dimension d = 200  400. Here the parameter θ ≈ n
s2 log d describes the relationship between sample
size  dimension and sparsity of the signal. Algorithm 2 dominates in most settings  with exceptions
when θ is too small  in which case none of the approaches provides meaningful results.

worse than the estimate produced by Algorithm 2  and it needs an initialization (for the ﬁrst step
of Algorithm 1 is used) and further requires a rough knowledge of the sparsity s  whereas both
Algorithms 1 and 2 do not require an estimate of s.
5 Discussion
In this paper we proposed a two-step procedure for estimation of MPR models with standard Gaussian
designs. We argued that the MPR models form a rich class including numerous additive SIMs (i.e. 
Y = h(X(cid:62)β∗) + ε) with an even and increasing on R+ link function h. Our algorithm is based
solely on convex optimization  and achieves optimal rates of estimation.
Our procedure does require that the sample size n (cid:38) s2 log d to ensure successful initialization. The
same condition has been exhibited previously  e.g.  in [7] for the phase retrieval model  and in works
on sparse principal components analysis [see  e.g.  3  15  33]. We anticipate that for a certain subclass
of MPR models  the sample size requirement n (cid:38) s2 log d is necessary for computationally efﬁcient
algorithms to exist. We conjecture that models (2.3)-(2.5) are such models. It is however certainly
not true that this sample size requirement holds for all models from the MPR class. For example  the
following model can be solved efﬁciently by applying the Lasso algorithm  without requiring the
sample size scaling n (cid:38) s2 log d

Y = sign(X(cid:62)β∗ + c) 

where c < 0 is ﬁxed. This discussion leads to the important question under what conditions of the
(known) link and error distribution (f  ε) one can efﬁciently solve the SIM Y = f (X(cid:62)β∗  ε) with
an optimal sample complexity. We would like to investigate this issue further in our future work.
Acknowledgments: The authors would like to thank the reviewers and meta-reviewers for carefully
reading the manuscript and their helpful suggestions which improved the presentation. The authors
would also like to thank Professor Xiaodong Li for kindly providing the code for the TWF algorithm.
References
[1] Adamczak  R. and Wolff  P. (2015). Concentration inequalities for non-Lipschitz functions with bounded

derivatives of higher order. Probability Theory and Related Fields  162 531–586.

[2] Amini  A. A. and Wainwright  M. J. (2008). High-dimensional analysis of semideﬁnite relaxations for

sparse principal components. In IEEE International Symposium on Information Theory.

[3] Berthet  Q. and Rigollet  P. (2013). Complexity theoretic lower bounds for sparse principal component

[4] Bickel  P. J.  Ritov  Y. and Tsybakov  A. B. (2009). Simultaneous analysis of Lasso and dantzig selector.

detection. In Conference on Learning Theory.

The Annals of Statistics 1705–1732.

8

lll0.00.51.01.52.0||b^-b*||2llllllllllllInitSecond StepInit full dataRefinedTPMTWF full dataq=4q=8q=12q=16ll0.00.51.01.52.0||b^-b*||2llllllllInitSecond StepInit full dataRefinedTPMq=4q=8q=12q=16ll0.00.51.01.52.0||b^-b*||2llllllllInitSecond StepInit full dataRefinedTPMq=4q=8q=12q=16lll0.00.51.01.52.0||b^-b*||2llllllllllllInitSecond StepInit full dataRefinedTPMTWF full dataq=4q=8q=12q=16ll0.00.51.01.52.0||b^-b*||2llllllllInitSecond StepInit full dataRefinedTPMq=4q=8q=12q=16ll0.00.51.01.52.0||b^-b*||2llllllllInitSecond StepInit full dataRefinedTPMq=4q=8q=12q=16[5] Boufounos  P. T. and Baraniuk  R. G. (2008). 1-bit compressive sensing. In Annual Conference on Infor-

[6] Bühlmann  P. and van de Geer  S. (2011). Statistics for high-dimensional data: Methods  theory and

mation Sciences and Systems.

applications. Springer.

[7] Cai  T. T.  Li  X. and Ma  Z. (2015). Optimal rates of convergence for noisy sparse phase retrieval via

thresholded Wirtinger ﬂow. arXiv:1506.03382.

[8] Candès  E. J.  Li  X. and Soltanolkotabi  M. (2015). Phase retrieval from coded diffraction patterns. Ap-

plied and Computational Harmonic Analysis  39 277–299.

[9] Candès  E. J.  Li  X. and Soltanolkotabi  M. (2015). Phase retrieval via Wirtinger ﬂow: Theory and algo-

rithms. IEEE Transactions on Information Theory  61 1985–2007.

[10] Candès  E. J.  Strohmer  T. and Voroninski  V. (2013). Phaselift: Exact and stable signal recovery from
magnitude measurements via convex programming. Communications on Pure and Applied Mathematics 
66 1241–1274.

[11] Chen  Y.  Yi  X. and Caramanis  C. (2013). A convex formulation for mixed regression with two compo-

[12] Cook  R. D. and Ni  L. (2005). Sufﬁcient dimension reduction via inverse regression. Journal of the

nents: Minimax optimal rates. arXiv:1312.7006.

American Statistical Association  100.

[13] d’Aspremont  A.  El Ghaoui  L.  Jordan  M. I. and Lanckriet  G. R. (2007). A direct formulation for sparse

PCA using semideﬁnite programming. SIAM review  49 434–448.

[14] Ganti  R.  Rao  N.  Willett  R. M. and Nowak  R. (2015). Learning single index models in high dimensions.

[15] Gao  C.  Ma  Z. and Zhou  H. H. (2014). Sparse CCA: Adaptive estimation and computational barriers.

arXiv preprint arXiv:1506.08910.

arXiv:1409.8565.

[16] Genzel  M. (2016). High-dimensional estimation of structured signals from non-linear observations with

general convex loss functions. arXiv:1602.03436.

[17] Han  F. and Wang  H. (2015). Provable smoothing approach in high dimensional generalized regression

[18] Horowitz  J. L. (2009). Semiparametric and nonparametric methods in econometrics. Springer.
[19] Laurent  B. and Massart  P. (2000). Adaptive estimation of a quadratic functional by model selection.

[20] Lecué  G. and Mendelson  S. (2013). Minimax rate of convergence and the performance of erm in phase

[21] Li  K.-C. (1991). Sliced inverse regression for dimension reduction. Journal of the American Statistical

model. arXiv:1509.07158.

Annals of Statistics 1302–1338.

recovery. arXiv:1311.5024.

Association  86 316–327.

on information theory.

266–282.

1135–1151.

1052.

[22] Li  K.-C. (1992). On principal Hessian directions for data visualization and dimension reduction: Another

application of Stein’s lemma. Journal of the American Statistical Association  87 1025–1039.

[23] Li  K.-C. and Duan  N. (1989). Regression analysis under link violation. The Annals of Statistics 1009–

[24] McCullagh  P. and Nelder  J. (1989). Generalized linear models. Chapman & Hall/CRC.
[25] Neykov  M.  Liu  J. S. and Cai  T. (2016). L1-regularized least squares for support recovery of high dimen-

sional single index models with Gaussian designs. Journal of Machine Learning Research  17 1–37.

[26] Peng  H. and Huang  T. (2011). Penalized least squares for single index models. Journal of Statistical

Planning and Inference  141 1362–1379.

[27] Plan  Y. and Vershynin  R. (2015). The generalized Lasso with non-linear observations. IEEE Transactions

[28] Radchenko  P. (2015). High dimensional single index models. Journal of Multivariate Analysis  139

[29] Raskutti  G.  Wainwright  M. J. and Yu  B. (2010). Restricted eigenvalue properties for correlated Gaussian

designs. Journal of Machine Learning Research  11 2241–2259.

[30] Stein  C. M. (1981). Estimation of the mean of a multivariate normal distribution. The Annals of Statistics

[31] Thrampoulidis  C.  Abbasi  E. and Hassibi  B. (2015). Lasso with non-linear measurements is equivalent

to one with linear measurements. arXiv preprint  arXiv:1506.02181.

[32] Vershynin  R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027.
[33] Wang  Z.  Gu  Q. and Liu  H. (2015). Sharp computational-statistical phase transitions via oracle compu-

tational model. arXiv:1512.08861.

Statistical Association  94 1275–1285.

[34] Xia  Y. and Li  W. (1999). On single-index coefﬁcient regression models. Journal of the American

[35] Yang  Z.  Wang  Z.  Liu  H.  Eldar  Y. C. and Zhang  T. (2015). Sparse nonlinear regression: Parameter

estimation and asymptotic inference. arXiv; 1511:04514.

[36] Yi  X.  Wang  Z.  Caramanis  C. and Liu  H. (2015). Optimal linear estimation under unknown nonlinear

transform. In Advances in Neural Information Processing Systems.

[37] Yuan  X.-T. and Zhang  T. (2013). Truncated power method for sparse eigenvalue problems. Journal of

Machine Learning Research  14 899–925.

9

,Matey Neykov
Zhaoran Wang
Han Liu