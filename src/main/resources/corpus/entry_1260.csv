2013,Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions,We investigate three related and important problems connected to machine learning  namely approximating a submodular function everywhere  learning a submodular function (in a PAC like setting [26])  and constrained minimization of submodular functions. In all three problems  we provide improved bounds which depend on the “curvature” of a submodular function and improve on the previously known best results for these problems [9  3  7  25] when the function is not too curved – a property which is true of many real-world submodular functions. In the former two problems  we obtain these bounds through a generic black-box transformation (which can potentially work for any algorithm)  while in the case of submodular minimization  we propose a framework of algorithms which depend on choosing an appropriate surrogate for the submodular function. In all these cases  we provide almost matching lower bounds. While improved curvature-dependent bounds were shown for monotone submodular maximization [4  27]  the existence of similar improved bounds for the aforementioned problems has been open. We resolve this question in this paper by showing that the same notion of curvature provides these improved results. Empirical experiments add further support to our claims.,Curvature and Optimal Algorithms for Learning and

Minimizing Submodular Functions

Rishabh Iyer†  Stefanie Jegelka∗  Jeff Bilmes†

† University of Washington  Dept. of EE  Seattle  U.S.A.
∗ University of California  Dept. of EECS  Berkeley  U.S.A.

rkiyer@uw.edu  stefje@eecs.berkeley.edu  bilmes@uw.edu

Abstract

We investigate three related and important problems connected to machine learning:
approximating a submodular function everywhere  learning a submodular function
(in a PAC-like setting [28])  and constrained minimization of submodular functions.
We show that the complexity of all three problems depends on the “curvature” of the
submodular function  and provide lower and upper bounds that reﬁne and improve
previous results [2  6  8  27]. Our proof techniques are fairly generic. We either
use a black-box transformation of the function (for approximation and learning) 
or a transformation of algorithms to use an appropriate surrogate function (for
minimization). Curiously  curvature has been known to inﬂuence approximations
for submodular maximization [3  29]  but its effect on minimization  approximation
and learning has hitherto been open. We complete this picture  and also support
our theoretical claims by empirical results.

1

Introduction

Submodularity is a pervasive and important property in the areas of combinatorial optimization 
economics  operations research  and game theory. In recent years  submodularity’s use in machine
learning has begun to proliferate as well. A set function f : 2V → R over a ﬁnite set V =
{1  2  . . .   n} is submodular if for all subsets S  T ⊆ V   it holds that f(S) + f(T ) ≥ f(S ∪ T ) +
f(S ∩ T ). Given a set S ⊆ V   we deﬁne the gain of an element j /∈ S in the context S as
f(j|S) (cid:44) f(S ∪ j) − f(S). A function f is submodular if it satisﬁes diminishing marginal returns 
namely f(j|S) ≥ f(j|T ) for all S ⊆ T  j /∈ T   and is monotone if f(j|S) ≥ 0 for all j /∈ S  S ⊆ V .
While submodularity  like convexity  occurs naturally in a wide variety of problems  recent studies
have shown that in the general case  many submodular problems of interest are very hard: the
problems of learning a submodular function or of submodular minimization under constraints do
not even admit constant or logarithmic approximation factors in polynomial time [2  7  8  10  27].
These rather pessimistic results however stand in sharp contrast to empirical observations  which
suggest that these lower bounds are speciﬁc to rather contrived classes of functions  whereas much
better results can be achieved in many practically relevant cases. Given the increasing importance
of submodular functions in machine learning  these observations beg the question of qualifying and
quantifying properties that make sub-classes of submodular functions more amenable to learning and
optimization. Indeed  limited prior work has shown improved results for constrained minimization
and learning of sub-classes of submodular functions  including symmetric functions [2  25]  concave
functions [7  18  24]  label cost or covering functions [9  31].
In this paper  we take additional steps towards addressing the above problems and show how the
generic notion of the curvature – the deviation from modularity– of a submodular function determines
both upper and lower bounds on approximation factors for many learning and constrained optimization
problems. In particular  our quantiﬁcation tightens the generic  function-independent bounds in [8  2 
27  7  10] for many practically relevant functions. Previously  the concept of curvature has been used to

1

tighten bounds for submodular maximization problems [3  29]. Hence  our results complete a unifying
picture of the effect of curvature on submodular problems. Moreover  curvature is still a fairly generic
concept  as it only depends on the marginal gains of the submodular function. It allows a smooth
transition between the ‘easy’ functions and the ‘really hard’ subclasses of submodular functions.

2 Problem statements  deﬁnitions and background

Before stating our main results  we provide some necessary deﬁnitions and introduce a new concept 
the curve normalized version of a submodular function. Throughout this paper  we assume that
the submodular function f is deﬁned on a ground set V of n elements  that it is nonnegative and
f(∅) = 0. We also use normalized modular (or additive) functions w : 2V → R which are those that
i∈S w(i). We are concerned with the following three

can be written as a sum of weights  w(S) =(cid:80)

problems:
Problem 1. (Approximation [8]) Given a submodular function f in form of a value oracle  ﬁnd an
approximation ˆf (within polynomial time and representable within polynomial space)  such that for
all X ⊆ V   it holds that ˆf(X) ≤ f(X) ≤ α1(n) ˆf(X) for a polynomial α1(n).
Problem 2. (PMAC-Learning [2]) Given i.i.d training samples {(Xi  f(Xi)}m
i=1 from a distribution
D  learn an approximation ˆf(X) that is  with probability 1 − δ  within a multiplicative factor of
α2(n) from f.
Problem 3. (Constrained optimization [27  7  10  16]) Minimize a submodular function f over a
family C of feasible sets  i.e.  minX∈C f(X).
In its general form  the approximation problem was ﬁrst studied by Goemans et al. [8]  who approx-
imate any monotone submodular function to within a factor of O(
n log n)  with a lower bound
√
n/ log n). Building on this result  Balcan and Harvey [2] show how to PMAC-learn
of α1(n) = Ω(
a monotone submodular function within a factor of α2(n) = O(
n)  and prove a lower bound of
Ω(n1/3) for the learning problem. Subsequent work extends these results to sub-additive and frac-
tionally sub-additive functions [1]. Better learning results are possible for the subclass of submodular
shells [23] and Fourier sparse set functions [26]. Both Problems 1 and 2 have numerous applications
in algorithmic game theory and economics [2  8] as well as machine learning [2  22  23  26  15].
Constrained submodular minimization arises in applications such as power assignment or transporta-
tion problems [19  30  13]. In machine learning  it occurs  for instance  in the form of MAP inference
in high-order graphical models [17] or in size-constrained corpus extraction [21]. Recent results
show that almost all constraints make it hard to solve the minimization even within a constant factor
[27  6  16]. Here  we will focus on the constraint of imposing a lower bound on the cardinality  and
on combinatorial constraints where C is the set of all s-t paths  s-t cuts  spanning trees  or perfect
matchings in a graph.
A central concept in this work is the total curvature κf of a submodular function f and the curvature
κf (S) with respect to a set S ⊆ V   deﬁned as [3  29]

√

√

κf = 1 − min
j∈V

f(j | V \ j)

f(j|S\j)

.

κf (S) = 1 − min
j∈S

 

f(j)

f(j)

(1)
Without loss of generality  assume that f(j) > 0 for all j ∈ V . It is easy to see that κf (S) ≤
κf (V ) = κf   and hence κf (S) is a tighter notion of curvature. A modular function has curvature
κf = 0  and a matroid rank function has maximal curvature κf = 1. Intuitively  κf measures how
far away f is from being modular. Conceptually  curvature is distinct from the recently proposed
submodularity ratio [5] that measures how far a function is from being submodular. Curvature has
served to tighten bounds for submodular maximization problems  e.g.  from (1−1/e) to 1
(1−e−κf )
for monotone submodular maximization subject to a cardinality constraint [3] or matroid constraints
[29]  and these results are tight. For submodular minimization  learning  and approximation  however 
the role of curvature has not yet been addressed (an exception are the upper bounds in [13] for
minimization). In the following sections  we complete the picture of how curvature affects the
complexity of submodular maximization and minimization  approximation  and learning.
The above-cited lower bounds for Problems 1–3 were established with functions of maximal curvature
(κf = 1) which  as we will see  is the worst case. By contrast  many practically interesting functions
have smaller curvature  and our analysis will provide an explanation for the good empirical results

κf

2

vision [17]. This class comprises  for instance  functions of the form f(X) =(cid:80)k

observed with such functions [13  22  14]. An example for functions with κf < 1 is the class
of concave over modular functions that have been used in speech processing [22] and computer
i=1(wi(X))a  for
some a ∈ [0  1] and a nonnegative weight vectors wi. Such functions may be deﬁned over clusters
Ci ⊆ V   in which case the weights wi(j) are nonzero only if j ∈ Ci [22  17  11].

Curvature-dependent analysis. To analyze Problems 1 – 3  we introduce the concept of a curve-
normalized polymatroid1. Speciﬁcally  we deﬁne the κf -curve-normalized version of f as

f(X) − (1 − κf )(cid:80)

κf

j∈X f(j)

f κ(X) =

(2)

fdifﬁcult(X) = κf f κ(X) and measy(X) = (1− κf )(cid:80)

If κf = 0  then we set f κ ≡ 0. We call f κ the curve-normalized version of f because its curvature
is κf κ = 1. The function f κ allows us to decompose a submodular function f into a “difﬁ-
cult” polymatroid function and an “easy” modular part as f(X) = fdifﬁcult(X) + measy(X) where
j∈X f(j). Moreover  we may modulate the cur-
vature of given any function g with κg = 1  by constructing a function f(X) (cid:44) cg(X) + (1 − c)|X|
with curvature κf = c but otherwise the same polymatroidal structure as g.
Our curvature-based decomposition is different from decompositions such as that into a totally
normalized function and a modular function [4]. Indeed  the curve-normalized function has some
speciﬁc properties that will be useful later on (proved in [12]):
j∈X f(j) and f(X) ≥

Lemma 2.1. If f is monotone submodular with κf > 0  then f(X) ≤(cid:80)
(1 − κf )(cid:80)
submodular function. Furthermore  f κ(X) ≤(cid:80)

Lemma 2.2. If f is monotone submodular  then f κ(X) in Eqn. (2) is a monotone non-negative

j∈X f(j).

j∈X f(j).

The function f κ will be our tool for analyzing the hardness of submodular problems. Previous
information-theoretic lower bounds for Problems 1–3 [6  8  10  27] are independent of curvature and
use functions with κf = 1. These curvature-independent bounds are proven by constructing two
essentially indistinguishable matroid rank functions h and f R  one of which depends on a random
set R ⊆ V . One then argues that any algorithm would need to make a super-polynomial number of
queries to the functions for being able to distinguish h and f R with high enough probability. The
lower bound will be the ratio maxX∈C h(X)/f R(X). We extend this proof technique to functions
with a ﬁxed given curvature. To this end  we deﬁne the functions

κ (X) = κf f R(X) + (1 − κf )|X|
f R

and

hκ(X) = κf h(X) + (1 − κf )|X|.

(3)

Both of these functions have curvature κf . This construction enables us to explicitly introduce the
effect of curvature into information-theoretic bounds for all monotone submodular functions.

n log n) [8] by an improved curvature-dependent O(

Main results. The curve normalization (2) leads to reﬁned upper bounds for Problems 1–3  while
the curvature modulation (3) provides matching lower bounds. The following are some of our
main results: for approximating submodular functions (Problem 1)  we replace the known bound
√
√
of α1(n) = O(
n log n−1)(1−κf )). We
√
complement this with a lower bound of ˜Ω(
n−1)(1−κf )). For learning submodular functions
√
(Problem 2)  we reﬁne the known bound of α2(n) = O(
n) [2] in the PMAC setting to a curvature
1+(n1/3−1)(1−κf )). Finally 
dependent bound of O(
Table 1 summarizes our curvature-dependent approximation bounds for constrained minimization
(Problem 3). These bounds reﬁne many of the results in [6  27  10  16]. In general  our new curvature-
dependent upper and lower bounds reﬁne known theoretical results whenever κf < 1  in many cases
replacing known polynomial bounds by a curvature-dependent constant factor 1/(1 − κf ). Besides
making these bounds precise  the decomposition and the curve-normalized version (2) are the basis
for constructing tight algorithms that (up to logarithmic factors) achieve the lower bounds.

√
n−1)(1−κf ))  with a lower bound of ˜Ω(

n log n

n1/3

1+(

√

1+(

√

1+(

√

n

n

1A polymatroid function is a monotone increasing  nonnegative  submodular function satisfying f (∅) = 0.

3

Constraint
Card. LB
Spanning Tree
Matchings
s-t path
s-t cut

Modular approx. (MUB)

k

n

n

1+(k−1)(1−κf )
1+(n−1)(1−κf )
2+(n−2)(1−κf )
1+(n−1)(1−κf )
1+(m−1)(1−κf )

m

n

1+(

n log n

m log m

Ellipsoid approx. (EA)
√
√
n log n−1)(1−κf ))
O(
√
1+(
√
O(
m log m−1)(1−κf ))
√
√
m log m−1)(1−κf ))
O(
√
√
m log m−1)(1−κf ))
O(
√
m−1)(1−κf ))
O(

√
m log m

1+(log m

m log m

m log m

1+(

1+(

n

n1/2)

Lower bound
˜Ω(
1+(n1/2−1)(1−κf ))
˜Ω(
1+(n−1)(1−κf ))
˜Ω(
1+(n−1)(1−κf ))
˜Ω(
n2/3
1+(n2/3−1)(1−κf ))
√
˜Ω(
n−1)(1−κf ))

1+(

√

n

n

Table 1: Summary of our results for constrained minimization (Problem 3).

3 Approximating submodular functions everywhere

We ﬁrst address improved bounds for the problem of approximating a monotone submodular function
everywhere. Previous work established α-approximations g to a submodular function f satisfying
g(S) ≤ f(S) ≤ αg(S) for all S ⊆ V [8]. We begin with a theorem showing how any algorithm
computing such an approximation may be used to obtain a curvature-speciﬁc  improved approximation.
Note that the curvature of a monotone submodular function can be obtained within 2n + 1 queries to
f. The key idea of Theorem 3.1 is to only approximate the curved part of f  and to retain the modular
part exactly. The full proof is in [12].
Theorem 3.1. Given a polymatroid function f with κf < 1  let f κ be its curve-normalized version
deﬁned in Equation (2)  and let ˆf κ be a submodular function satisfying ˆf κ(X) ≤ f κ(X) ≤
α(n) ˆf κ(X)  for some X ⊆ V . Then the function ˆf(X) (cid:44) κf
j∈X f(j) satisﬁes

ˆf(X) ≤ f(X) ≤

α(n)

1 + (α(n) − 1)(1 − κf )

(4)

ˆf κ(X) + (1− κf )(cid:80)
ˆf(X) ≤ ˆf(X)
1 − κf

.

Theorem 3.1 may be directly applied to tighten recent results on approximating submodular functions
everywhere. An algorithm by Goemans et al. [8] computes an approximation to a polymatroid
function f in polynomial time by approximating the submodular polyhedron via an ellipsoid. This
n log n)  and has
approximation (which we call the ellipsoidal approximation) satisﬁes α(n) = O(

the form(cid:112)wf (X) for a certain weight vector wf . Corollary 3.2 states that a tighter approximation
Corollary 3.2. Let f be a polymatroid function with κf < 1  and let (cid:112)wf κ(X) be the ellip-
(cid:112)wf κ(X) + (1 − κf )(cid:80)

is possible for functions with κf < 1.

soidal approximation to the κ-curve-normalized version f κ(X) of f. Then the function f ea(X) =
κf

j∈X f(j) satisﬁes

√

(cid:18)

√

(cid:19)

f ea(X) ≤ f(X) ≤ O

√

1 + (

n log n

n log n − 1)(1 − κf )

f ea(X).

(5)

If κf = 0  then the approximation is exact. This is not surprising since a modular function can be
inferred exactly within O(n) oracle calls. The following lower bound (proved in [12]) shows that
Corollary 3.2 is tight up to logarithmic factors. It reﬁnes the lower bound in [8] to include κf .
Theorem 3.3. Given a submodular function f with curvature κf   there does not exist a (possibly
randomized) polynomial-time algorithm that computes an approximation to f within a factor of
1+(n1/2−−1)(1−κf )   for any  > 0.
The simplest alternative approximation to f one might conceive is the modular function ˆf m(X) (cid:44)

n1/2−

j∈X f(j) which can easily be computed by querying the n values f(j).

Lemma 3.1. Given a monotone submodular function f  it holds that2

(cid:80)

f(X) ≤ ˆf m(X) =(cid:88)

f(j) ≤

j∈X

1 + (|X| − 1)(1 − κf (X)) f(X)

|X|

(6)

.

P
P
j∈X f (j|X\j)

j∈X f (j)

2In [12]  we show this result with a stronger notion of curvature: ˆκf (X) = 1 −

4

The form of Lemma 3.1 is slightly different from Corollary 3.2. However  there is a straightforward
correspondence: given ˆf such that ˆf(X) ≤ f(X) ≤ α(cid:48)(n) ˆf(X)  by deﬁning ˆf(cid:48)(X) = α(cid:48)(n) ˆf(X) 
we get that f(X) ≤ ˆf(cid:48)(X) ≤ α(cid:48)(n)f(X). Lemma 3.1 for the modular approximation is comple-
mentary to Corollary 3.2: First  the modular approximation is better whenever |X| ≤ √
n. Second 
the bound in Lemma 3.1 depends on the curvature κf (X) with respect to the set X  which is stronger
than κf . Third  ˆf m is extremely simple to compute. For sets of larger cardinality  however  the
ellipsoidal approximation of Corollary 3.2 provides a better approximation  in fact  the best possible
one (Theorem 3.3). In a similar manner  Lemma 3.1 is tight for any modular approximation to a
submodular function:
Lemma 3.2. For any κ > 0  there exists a monotone submodular function f with curvature κ such
that no modular upper bound on f can approximate f(X) to a factor better than
1+(|X|−1)(1−κf ) .
The improved curvature dependent bounds immediately imply better bounds for the class of concave
over modular functions used in [22  17  11].
Corollary 3.4. Given weight vectors w1 ···   wk ≥ 0  and a submodular function f(X) =

(cid:80)k
i=1 λi[wi(X)]a  λi ≥ 0  for a ∈ (0  1)  it holds that f(X) ≤(cid:80)
modular functions by a factor of(cid:112)|X|.

In particular  when a = 1/2  the modular upper bound approximates the sum of square-root over

j∈X f(j) ≤ |X|1−af(X)

|X|

4 Learning Submodular functions

(cid:104)

We next address the problem of learning submodular functions in a PMAC setting [2]. The PMAC
(Probably Mostly Approximately Correct) framework is an extension of the PAC framework [28]
to allow multiplicative errors in the function values from a ﬁxed but unknown distribution D over
2V . We are given training samples {(Xi  f(Xi)}m
i=1 drawn i.i.d. from D. The algorithm may take
time polynomial in n  1/  1/δ to compute a (polynomially-representable) function ˆf that is a good
approximation to f with respect to D. Formally  ˆf must satisfy that

PrX1 X2 ···  Xm∼D

PrX∈D[ ˆf(X) ≤ f(X) ≤ α(n) ˆf(X)] ≥ 1 − 
√

(7)
for some approximation factor α(n). Balcan and Harvey [2] propose an algorithm that PMAC-learns
any monotone  nonnegative submodular function within a factor α(n) =
n + 1 by reducing the
problem to that of learning a binary classiﬁer. If we assume that we have an upper bound on the
curvature κf   or that we can estimate it 3  and have access to the value of the singletons f(j)  j ∈ V  
then we can obtain better learning results with non-maximal curvature:
Lemma 4.1. Let f be a monotone submodular function for which we know an upper bound on its
curvature and the singleton weights f(j) for all j ∈ V . For every   δ > 0 there is an algorithm
that uses a polynomial number of training examples  runs in time polynomial in (n  1/  1/δ) and
. If D is a product distribution  then there exists
PMAC-learns f within a factor of

√

(cid:105) ≥ 1 − δ

√
n+1−1)(1−κf )

n+1

1+(

an algorithm that PMAC-learns f within a factor of O(

1+(log 1

log 1
 −1)(1−κf )).


ˆf κ(X) + (1 − κf )(cid:80)

The algorithm of Lemma 4.1 uses the reduction of Balcan and Harvey [2] to learn the κf -curve-
normalized version f κ of f. From the learned function ˆf κ(X)  we construct the ﬁnal estimate
ˆf(X) (cid:44) κf
j∈X f(j). Theorem 3.1 implies Lemma 4.1 for this ˆf(X).
Moreover  no polynomial-time algorithm can be guaranteed to PMAC-learn f within a factor of
n1/3−(cid:48)
  for any (cid:48) > 0 [12]. We end this section by showing how we can learn with a
1+(n1/3−(cid:48)−1)(1−κf )
construction analogous to that in Lemma 3.1.
Lemma 4.2. If f is a monotone submodular function with known curvature (or a known upper
bound) ˆκf (X) ∀X ⊆ V   then for every   δ > 0 there is an algorithm that uses a polynomial number
of training examples  runs in time polynomial in (n  1/  1/δ) and PMAC learns f(X) within a factor
of 1 +

|X|

1+(|X|−1)(1− ˆκf (X)) .

3note that κf can be estimated from a set of 2n + 1 samples {(j  f (j))}j∈V   {(V  f (V ))}  and

{(V \j  f (V \j)}j∈V included in the training samples

5

Compare this result to Lemma 4.1. Lemma 4.2 leads to better bounds for small sets  whereas
Lemma 4.1 provides a better general bound. Moreover  in contrast to Lemma 4.1  here we only need an
upper bound on the curvature and do not need to know the singleton weights {f(j)  j ∈ V }. Note also
corollary is that the class of concave over modular functions f(X) =(cid:80)k
that  while κf itself is an upper bound of ˆκf (X)  often one does have an upper bound on ˆκf (X) if one
knows the function class of f (for example  say concave over modular). In particular  an immediate
i=1 λi[wi(X)]a  λi ≥ 0  for
a ∈ (0  1) can be learnt within a factor of min{√

n + 1  1 + |X|1−a}.

5 Constrained submodular minimization

Next  we apply our results to the minimization of submodular functions under constraints. Most
algorithms for constrained minimization use one of two strategies: they apply a convex relaxation [10 
16]  or they optimize a surrogate function ˆf that should approximate f well [6  8  16]. We follow
the second strategy and propose a new  widely applicable curvature-dependent choice for surrogate
functions. A suitable selection of ˆf will ensure theoretically optimal results. Throughout this section 
we refer to the optimal solution as X∗ ∈ argminX∈C f(X).
f(X) ≤ α(n) ˆf1(X)  for all X ⊆ V . Then any minimizer (cid:98)X1 ∈ argminX∈C ˆf(X) of ˆf satisﬁes
Lemma 5.1. Given a submodular function f  let ˆf1 be an approximation of f such that ˆf1(X) ≤
f((cid:98)X) ≤ α(n)f(X∗). Likewise  if an approximation of f is such that f(X) ≤ ˆf2(X) ≤ α(X)f(X)
for a set-speciﬁc factor α(X)  then its minimizer ˜X2 ∈ argminX∈C ˆf2(X) satisﬁes f((cid:98)X2) ≤

α(X∗)f(X∗). If only β-approximations4 are possible for minimizing ˆf1 or ˆf2 over C  then the ﬁnal
bounds are βα(n) and βα(X∗) respectively.
For Lemma 5.1 to be practically useful  it is essential that ˆf1 and ˆf2 be efﬁciently optimizable
over C. We discuss two general curvature-dependent approximations that work for a large class of
combinatorial constraints. In particular  we use Theorem 3.1: we decompose f into f κ and a modular
part f m  and then approximate f κ while retaining f m  i.e.  ˆf = ˆf κ + f m. The ﬁrst approach uses a
simple modular upper bound (MUB) and the second relies on the Ellipsoidal approximation (EA) we
used in Section 3.
MUB: The simplest approximation to a submodular function is the modular approximation
j∈X f(j) ≥ f(X). Since here  ˆf κ happens to be equivalent to f m  we obtain the
overall approximation ˆf = ˆf m. Lemmas 5.1 and 3.1 directly imply a set-dependent approximation
factor for ˆf m:

ˆf m(X) (cid:44) (cid:80)
Corollary 5.1. Let (cid:98)X ∈ C be a β-approximate solution for minimizing(cid:80)
(cid:80)
j∈bX f(j) ≤ β minX∈C(cid:80)
1 + (|X∗| − 1)(1 − κf (X∗)) f(X∗).

j∈X f(j) over C  i.e.

j∈X f(j). Then

(8)

f( ˆX) ≤

β|X∗|

Corollary 5.1 has also been shown in [13]. Similar to the algorithms in [13]  MUB can be extended
to an iterative algorithm yielding performance gains in practice. In particular  Corollary 5.1 implies
improved approximation bounds for practically relevant concave over modular functions  such
j∈X wi(j)  we obtain a worst-case
n. This is signiﬁcantly better than the worst case factor of |X∗|

as those used in [17]. For instance  for f(X) = (cid:80)k
approximation bound of(cid:112)|X∗| ≤ √

(cid:113)(cid:80)

for general submodular functions.
EA: Instead of employing a modular upper bound  we can approximate f κ using the construction
by Goemans et al. [8]  as in Corollary 3.2. In that case  ˆf(X) = κf
has a special form: a weighted sum of a concave function and a modular function. Minimizing
such a function over constraints C is harder than minimizing a merely modular function  but with
the algorithm in [24] we obtain an FPTAS5 for minimizing ˆf over C whenever we can minimize a
nonnegative linear function over C.

(cid:112)wf κ(X) + (1 − κf )f m(X)

i=1

4A β-approximation algorithm for minimizing a function g ﬁnds set X : g(X) ≤ β minX∈C g(X)
5The FPTAS will yield a β = (1 + )-approximation through an algorithm polynomial in 1
 .

6

Corollary 5.2. For a submodular function with curvature κf < 1  algorithm EA will return a

solution (cid:98)X that satisﬁes

(cid:18)

f((cid:98)X) ≤ O

√

n log n

n log n − 1)(1 − κf ) + 1)

√

(

(cid:19)

f(X∗).

(9)

k

n log n

√

1+(

n1/2−

Next  we apply the results of this section to speciﬁc optimization problems  for which we show
(mostly tight) curvature-dependent upper and lower bounds. We just state our main results; a more
extensive discussion along with the proofs can be found in [12].
Cardinality lower bounds (SLB). A simple constraint is a lower bound on the cardinality of the
solution  i.e.  C = {X ⊆ V : |X| ≥ k}. Svitkina and Fleischer [27] prove that for monotone
submodular functions of arbitrary curvature  it is impossible to ﬁnd a polynomial-time algorithm

with an approximation factor better than(cid:112)n/ log n. They show an algorithm which matches this

1+(k−1)(1−κf ) and O(

1+(n1/2−−1)(1−κf ) for any  > 0.

approximation factor. Corollaries 5.1 and 5.2 immediately imply curvature-dependent approximation
√
n log n−1)(1−κf )). These bounds are improvements over the
bounds of
results of [27] whenever κf < 1. Here  MUB is preferable to EA whenever k is small. Moreover 
the bound of EA is tight up to poly-log factors  in that no polynomial time algorithm can achieve a
general approximation factor better than
In the following problems  our ground set V consists of the set of edges in a graph G = (V E) with
two distinct nodes s  t ∈ V and n = |V|  m = |E|. The submodular function is f : 2E → R.
Shortest submodular s-t path (SSP). Here  we aim to ﬁnd an s-t path X of minimum (submodular)
length f(X). Goel et al. [6] show a O(n2/3)-approximation with matching curvature-independent
lower bound Ω(n2/3). By Corollary 5.1  the curvature-dependent worst-case bound for MUB is
1+(n−1)(1−κf ) since any minimal s-t path has at most n edges. Similarly  the factor for EA is
√
m log m−1)(1−κf )). The bound of EA will be tighter for sparse graphs while MUB provides
O(
better results for dense ones. Our curvature-dependent lower bound for SSP is
1+(n2/3−−1)(1−κf ) 
for any  > 0  which reduces to the result in [6] for κf = 1.
Minimum submodular s-t cut (SSC): This problem  also known as the cooperative cut problem [16 
17]  asks to minimize a monotone submodular function f such that the solution X ⊆ E is a set of
edges whose removal disconnects s from t in G. Using curvature reﬁnes the We can also show a
1+(n1/2−−1)(1−κf )  for any  > 0. Corollary 5.1 implies an approximation
lower bound of [16] to
1+(m−1)(1−κf ) for MUB  where m = |E|
factor of O(
is the number of edges in the graph. Hence the factor for EA is tight for sparse graphs. Speciﬁcally
for cut problems  there is yet another useful surrogate function that is exact on local neighborhoods.
Jegelka and Bilmes [16] demonstrate how this approximation may be optimized via a generalized
maximum ﬂow algorithm that maximizes a polymatroidal network ﬂow [20]. This algorithm still
ˆf κ + (1 − κf )f m  where we only approximate f κ. We refer to
applies to the combination ˆf = κf
this approximation as Polymatroidal Network Approximation (PNA).
Corollary 5.3. Algorithm PNA achieves a worst-case approximation factor of
cooperative cut problem.

m log m−1)(1−κf )+1) for EA and a factor of

2+(n−2)(1−κf ) for the

n1/2−

n2/3−

√

m log m

n

√

1+(

m log m

√

(

m

n

For dense graphs  this factor is theoretically tighter than that of the EA approximation.
Minimum submodular spanning tree (SST). Here  C is the family of all spanning trees in a given
graph G. Such constraints occur for example in power assignment problems [30]. Goel et al. [6] show
a curvature-independent optimal approximation factor of O(n) for this problem. Corollary 5.1
reﬁnes this bound to
1+(n−1)(1−κf ) when using MUB; Corollary 5.2 implies a slightly worse bound
for EA. We also show that the bound of MUB is tight: no polynomial-time algorithm can guarantee a
factor better than
Minimum submodular perfect matching (SPM): Here  we aim to ﬁnd a perfect matching in
a graph that minimizes a monotone submodular function. Corollary 5.1 implies that an MUB
approximation will achieve an approximation factor of at most
2+(n−2)(1−κf ). Similar to the
spanning tree case  the bound of MUB is also tight [12].

1+(n1−−1)(1−κf )+δκf

  for any   δ > 0.

n1−

n

n

7

(a) ﬁxed κ = 0  α =
Figure 1: Minimization of gκ for cardinality lower bound constraints.
n1/2+  β = n2 for varying ; (b) ﬁxed  = 0.1  but varying κ; (c) different choices of α for β = 1;
(d) varying κ with α = n/2  β = 1. Dashed lines: MUB  dotted lines: EA  solid lines: theoretical
bound. The results of EA are not visible in some instances since it obtains a factor of 1.

5.1 Experiments

κ (X) = κf(X) + (1 − κ)|X| as in Equation (3).

We end this section by empirically demonstrating the performance of MUB and EA and their precise
dependence on curvature. We focus on cardinality lower bound constraints  C = {X ⊆ V : |X| ≥ α}
and the “worst-case” class of functions that has been used throughout this paper to prove lower
bounds  f R(X) = min{|X ∩ ¯R| + β |X|  α} where ¯R = V \R and R ⊆ V is random set such that
|R| = α. We adjust α = n1/2+ and β = n2 by a parameter . The smaller  is  the harder the
problem. This function has curvature κf = 1. To obtain a function with speciﬁc curvature κ  we
deﬁne f R
In all our experiments  we take the average over 20 random draws of R. We ﬁrst set κ = 1 and
vary . Figure 1(a) shows the empirical approximation factors obtained using EA and MUB  and the
theoretical bound. The empirical factors follow the theoretical results very closely. Empirically  we
also see that the problem becomes harder as  decreases. Next we ﬁx  = 0.1 and vary the curvature
κ in f R
κ . Figure 1(b) illustrates that the theoretical and empirical approximation factors improve
signiﬁcantly as κ decreases. Hence  much better approximations than the previous theoretical lower
bounds are possible if κ is not too large. This observation can be very important in practice. Here 
too  the empirical upper bounds follow the theoretical bounds very closely.
Figures 1(c) and (d) show results for larger α and β = 1. In Figure 1(c)  as α increases  the empirical
factors improve. In particular  as predicted by the theoretical bounds  EA outperforms MUB for large
α and  for α ≥ n2/3  EA ﬁnds the optimal solution. In addition  Figures 1(b) and (d) illustrate the
theoretical and empirical effect of curvature: as n grows  the bounds saturate and approximate a
constant 1/(1 − κ) – they do not grow polynomially in n. Overall  we see that the empirical results
quite closely follow our theoretical results  and that  as the theory suggests  curvature signiﬁcantly
affects the approximation factors.

6 Conclusion and Discussion

In this paper  we study the effect of curvature on the problems of approximating  learning and
minimizing submodular functions under constraints. We prove tightened  curvature-dependent upper
bounds with almost matching lower bounds. These results complement known results for submodular
maximization [3  29]. Given that the functional form and effect of the submodularity ratio proposed
in [5] is similar to that of curvature  an interesting extension is the question of whether there is a
single unifying quantity for both of these terms. Another open question is whether a quantity similar
to curvature can be deﬁned for subadditive functions  thus reﬁning the results in [1] for learning
subadditive functions. Finally it also seems that the techniques in this paper could be used to provide
improved curvature-dependent regret bounds for constrained online submodular minimization [15].
Acknowledgments: Special thanks to Kai Wei for pointing out that Corollary 3.4 holds and for
other discussions  to Bethany Herwaldt for reviewing an early draft of this manuscript  and to
the anonymous reviewers. This material is based upon work supported by the National Science
Foundation under Grant No. (IIS-1162606)  a Google and a Microsoft award  and by the Intel Science
and Technology Center for Pervasive Computing. Stefanie Jegelka’s work is supported by the Ofﬁce
of Naval Research under contract/grant number N00014-11-1-0688  and gifts from Amazon Web
Services  Google  SAP  Blue Goji  Cisco  Clearstory Data  Cloudera  Ericsson  Facebook  General
Electric  Hortonworks  Intel  Microsoft  NetApp  Oracle  Samsung  Splunk  VMware and Yahoo!.

8

501001502002502468(a) varying ε  κ= 0emp. approx. factorn ε= 0.1ε= 0.2ε= 0.3ε= 0.4501001502002502345(b) varying κ  ε= 0.1emp. approx. factorn κ= 0.7κ= 0.5κ= 0.3κ= 0.15010015020020406080100(c) with varying α and β = 1emp. approx. factorn α= n/2α= n3/4α= n1/2501001502002468(d) varying κ with α = n/2 and β = 1emp. approx. factorn κ= 0.9κ= 0.6κ= 0.3κ= 0.1References
[1] M. F. Balcan  F. Constantin  S. Iwata  and L. Wang. Learning valuation functions. COLT  2011.
[2] N. Balcan and N. Harvey. Submodular functions: Learnability  structure  and optimization. In Arxiv

preprint  2012.

[3] M. Conforti and G. Cornuejols. Submodular set functions  matroids and the greedy algorithm: tight worst-
case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Mathematics  7(3):
251–274  1984.

[4] W. H. Cunningham. Decomposition of submodular functions. Combinatorica  3(1):53–68  1983.
[5] A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection  sparse

approximation and dictionary selection. In ICML  2011.

[6] G. Goel  C. Karande  P. Tripathi  and L. Wang. Approximability of combinatorial problems with multi-agent

submodular cost functions. In FOCS  2009.

[7] G. Goel  P. Tripathi  and L. Wang. Combinatorial problems with discounted price functions in multi-agent

systems. In FSTTCS  2010.

[8] M. Goemans  N. Harvey  S. Iwata  and V. Mirrokni. Approximating submodular functions everywhere. In

SODA  pages 535–544  2009.

[9] R. Hassin  J. Monnot  and D. Segev. Approximation algorithms and hardness results for labeled connectivity

problems. J Combinatorial Optimization  14(4):437–453  2007.

[10] S. Iwata and K. Nagano. Submodular function minimization under covering constraints. In In FOCS 

pages 671–680. IEEE  2009.

[11] R. Iyer and J. Bilmes. Algorithms for approximate minimization of the difference between submodular

functions  with applications. In UAI  2012.

[12] R. Iyer  S. Jegelka  and J. Bilmes. Curvature and Optimal Algorithms for Learning and Optimization of

Submodular Functions: Extended arxiv version  2013.

[13] R. Iyer  S. Jegelka  and J. Bilmes. Fast semidifferential based submodular function optimization. In ICML 

2013.

[14] S. Jegelka. Combinatorial Problems with submodular coupling in machine learning and computer vision.

PhD thesis  ETH Zurich  2012.

[15] S. Jegelka and J. Bilmes. Online submodular minimization for combinatorial structures. ICML  2011.
[16] S. Jegelka and J. A. Bilmes. Approximation bounds for inference using cooperative cuts. In ICML  2011.
[17] S. Jegelka and J. A. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In

CVPR  2011.

[18] P. Kohli  A. Osokin  and S. Jegelka. A principled deep random ﬁeld for image segmentation. In CVPR 

2013.

[19] A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models.

Proceedings of Uncertainity in Artiﬁcial Intelligence. UAI  2005.

In

[20] E. Lawler and C. Martel. Computing maximal “polymatroidal” network ﬂows. Mathematics of Operations

Research  7(3):334–347  1982.

[21] H. Lin and J. Bilmes. Optimal selection of limited vocabulary speech corpora. In Interspeech  2011.
[22] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In The 49th Meeting

of the Assoc. for Comp. Ling. Human Lang. Technologies (ACL/HLT-2011)  Portland  OR  June 2011.

[23] H. Lin and J. Bilmes. Learning mixtures of submodular shells with application to document summarization.

In UAI  2012.

[24] E. Nikolova. Approximation algorithms for ofﬂine risk-averse combinatorial optimization  2010.
[25] J. Soto and M. Goemans. Symmetric submodular function minimization under hereditary family constraints.

arXiv:1007.2140  2010.

[26] P. Stobbe and A. Krause. Learning fourier sparse set functions. In International Conference on Artiﬁcial

Intelligence and Statistics (AISTATS)  2012.

[27] Z. Svitkina and L. Fleischer. Submodular approximation: Sampling-based algorithms and lower bounds.

In FOCS  pages 697–706  2008.

[28] L. G. Valiant. A theory of the learnable. Communications of the ACM  27(11):1134–1142  1984.
[29] J. Vondr´ak. Submodularity and curvature: the optimal algorithm. RIMS Kokyuroku Bessatsu  23  2010.
[30] P.-J. Wan  G. Calinescu  X.-Y. Li  and O. Frieder. Minimum-energy broadcasting in static ad hoc wireless

networks. Wireless Networks  8:607–617  2002.

[31] P. Zhang  J.-Y. Cai  L.-Q. Tang  and W.-B. Zhao. Approximation and hardness results for label cut and

related problems. Journal of Combinatorial Optimization  21(2):192–208  2011.

9

,Rishabh Iyer
Stefanie Jegelka
Jeff Bilmes