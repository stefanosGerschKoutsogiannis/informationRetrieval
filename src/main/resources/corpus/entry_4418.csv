2015,Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process,Learning of low dimensional structure in multidimensional data is a canonical problem in machine learning.  One common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold.  There are a rich variety of manifold learning methods available  which allow mapping of data points to the manifold.  However  there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data.  The best attempt is the Gaussian process latent variable model (GP-LVM)  but identifiability issues lead to poor performance.  We solve these issues by proposing a novel Coulomb repulsive process (Corp) for locations of points on the manifold  inspired by physical models of electrostatic interactions among particles.  Combining this process with a GP prior for the mapping function yields a novel electrostatic GP (electroGP) process.  Focusing on the simple case of a one-dimensional manifold  we develop efficient inference algorithms  and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video.,Probabilistic Curve Learning: Coulomb Repulsion

and the Electrostatic Gaussian Process

Ye Wang

Department of Statistics

Duke University

Durham  NC  USA  27705

eric.ye.wang@duke.edu

David Dunson

Department of Statistics

Duke University

Durham  NC  USA  27705

dunson@stat.duke.edu

Abstract

Learning of low dimensional structure in multidimensional data is a canonical
problem in machine learning. One common approach is to suppose that the ob-
served data are close to a lower-dimensional smooth manifold. There are a rich
variety of manifold learning methods available  which allow mapping of data
points to the manifold. However  there is a clear lack of probabilistic methods
that allow learning of the manifold along with the generative distribution of the
observed data. The best attempt is the Gaussian process latent variable model
(GP-LVM)  but identiﬁability issues lead to poor performance. We solve these
issues by proposing a novel Coulomb repulsive process (Corp) for locations of
points on the manifold  inspired by physical models of electrostatic interactions
among particles. Combining this process with a GP prior for the mapping function
yields a novel electrostatic GP (electroGP) process. Focusing on the simple case
of a one-dimensional manifold  we develop efﬁcient inference algorithms  and il-
lustrate substantially improved performance in a variety of experiments including
ﬁlling in missing frames in video.

1

Introduction

There is broad interest in learning and exploiting lower-dimensional structure in high-dimensional
data. A canonical case is when the low dimensional structure corresponds to a p-dimensional smooth
Riemannian manifold M embedded in the d-dimensional ambient space Y of the observed data yyy.
Assuming that the observed data are close to M  it becomes of substantial interest to learn M along
with the mapping µ from M Ñ Y. This allows better data visualization and for one to exploit the
lower-dimensional structure to combat the curse of dimensionality in developing efﬁcient machine
learning algorithms for a variety of tasks.
The current literature on manifold learning focuses on estimating the coordinates xxx P M corre-
sponding to yyy by optimization  ﬁnding xxx’s on the manifold M that preserve distances between the
corresponding yyy’s in Y. There are many such methods  including Isomap [1]  locally-linear em-
bedding [2] and Laplacian eigenmaps [3]. Such methods have seen broad use  but have some clear
limitations relative to probabilistic manifold learning approaches  which allow explicit learning of
M  the mapping µ and the distribution of yyy.
There has been some considerable focus on probabilistic models  which would seem to allow learn-
ing of M and µ. Two notable examples are mixtures of factor analyzers (MFA) [4  5] and Gaussian
process latent variable models (GP-LVM)
[6]. Bayesian GP-LVM [7] is a Bayesian formulation
of GP-LVM which automatically learns the intrinsic dimension p and handles missing data. Such
approaches are useful in exploiting lower-dimensional structure in estimating the distribution of yyy 
but unfortunately have critical problems in terms of reliable estimation of the manifold and mapping

1

function. MFA is not smooth in approximating the manifold with a collage of lower dimensional
hyper-planes  and hence we focus further discussion on Bayesian GP-LVM. Similar problems occur
for MFA and other probabilistic manifold learning methods.
In general form for the ith data vector  Bayesian GP-LVM lets yyyi “ µpxxxiq ` i  with µ assigned
a Gaussian process prior  xxxi generated from a pre-speciﬁed Gaussian or uniform distribution over
a p-dimensional space  and the residual i drawn from a d-dimensional Gaussian centered on zero
with diagonal or spherical covariance. While this model seems appropriate to manifold learning 
identiﬁability problems lead to extremely poor performance in estimating M and µ. To give an
intuition for the root cause of the problem  consider the case in which xxxi are drawn independently
from a uniform distribution over r0  1sp. The model is so ﬂexible that we could ﬁt the training data
yyyi  for i “ 1  . . .   n  just as well if we did not use the entire hypercube but just placed all the xxxi values
in a small subset of r0  1sp. The uniform prior will not discourage this tendency to not spread out the
latent coordinates  which unfortunately has disasterous consequences illustrated in our experiments.
The structure of the model is just too ﬂexible  and further constraints are needed. Replacing the
uniform with a standard Gaussian does not solve the problem. Constrained likelihood methods [8  9]
mitigate the issue to some extent  but do not correspond to a proper Bayesian generative model.
To make the problem more tractable  we focus on the case in which M is a one-dimensional smooth
compact manifold. Assume yyyi “ µµµpxiq ` i  with i Gaussian noise  and µµµ : p0  1q ÞÑ M a smooth
mapping such that µjp¨q P C8 for j “ 1  . . .   d  where µµµpxq “ pµ1pxq  . . .   µdpxqq. We focus on
ﬁnding a good estimate of µµµ  and hence the manifold  via a probabilistic learning framework. We
refer to this problem as probabilistic curve learning (PCL) motivated by the principal curve literature
[10]. PCL differs substantially from the principal curve learning problem  which seeks to estimate a
non-linear curve through the data  which may be very different from the true manifold.
Our proposed approach builds on GP-LVM; in particular  our primary innovation is to generate the
latent coordinates xxxi from a novel repulsive process. There is an interesting literature on repulsive
point process modeling ranging from various Matern processes [11] to the determinantal point pro-
cess (DPP) [12]. In our very different context  these processes lead to unnecessary complexity —
computationally and otherwise — and we propose a new Coulomb repulsive process (Corp) moti-
vated by Coulomb’s law of electrostatic interaction between electrically charged particles. Using
Corp for the latent positions has the effect of strongly favoring spread out locations on the manifold 
effectively solving the identiﬁability problem mentioned above for the GP-LVM. We refer to the GP
with Corp on the latent positions as an electrostatic GP (electroGP).
The remainder of the paper is organized as follows. The Coulomb repulsive process is proposed
in § 2 and the electroGP is presented in § 3 with a comparison between electroGP and GP-LVM
demonstrated via simulations. The performance is further evaluated via real world datasets in § 4.
A discussion is reported in § 5.

2 Coulomb repulsive process

2.1 Formulation

Deﬁnition 1. A univariate process is a Coulomb repulsive process (Corp) if and only if for every
ﬁnite set of indices t1  . . .   tk in the index set N` 

Xt1 „ unifp0  1q 
ppXti|Xt1  . . .   Xti´1q9Πi´1

j“1 sin2r

`

˘

πXti ´ πXtj

1XtiPr0 1s  i ą 1 

(1)

where r ą 0 is the repulsive parameter. The process is denoted as Xt „ Corpprq.
The process is named by its analogy in electrostatic physics where by Coulomb law  two electro-
static positive charges will repel each other by a force proportional to the reciprocal of their square
distance. Letting dpx  yq “ sin|πx ´ πy|  the above conditional probability of Xti given Xtj is
proportional to d2rpXti  Xtjq  shrinking the probability exponentially fast as two states get closer to
each other. Note that the periodicity of the sine function eliminates the edges of r0  1s  making the
electrostatic energy ﬁeld homogeneous everywhere on r0  1s.
Several observations related to Kolmogorov extension theorem can be made immediately  ensuring
Corp to be well deﬁned. Firstly  the conditional density deﬁned in (1) is positive and integrable 

2

Figure 1: Each facet consists of 5 rows  with each row representing an 1-dimensional scatterplot of
a random realization of Corp under certain n and r.

since Xt’s are constrained in a compact interval  and sin2rp¨q is positive and bounded. Hence  the
ﬁnite distributions are well deﬁned.
Secondly  the joint ﬁnite p.d.f. for Xt1  . . .   Xtk can be derived as

(2)
As can be easily seen  any permutation of t1  . . .   tk will result in the same joint ﬁnite distribution 
hence this ﬁnite distribution is exchangeable.
Thirdly  it can be easily checked that for any ﬁnite set of indices t1  . . .   tk`m 

ppXt1   . . .   Xtkq9Πiąj sin2r
ż

πXti ´ πXtj

ż

˘

.

`

ppXt1  . . .   Xtk   Xtk`1   . . .   Xtk`mqdXtk`1 . . . dXtk`m  

ppXt1   . . .   Xtkq “

1

1

. . .

0

0

by observing that

ppXt1   . . .   Xtk   Xtk`1   . . .   Xtk`mq “ ppXt1  . . .   XtkqΠm

j“1ppXtk`j|Xt1  . . .   Xtk`j´1q.

2.2 Properties
Assuming Xt  t P N` is a realization from Corp  then the following lemmas hold.
Lemma 1. For any n P N`  any 1 ď i ă n and any  ą 0  we have

ppXn P BpXi  q|X1  . . .   Xn´1q ă 2π22r`1
2r ` 1

where BpXi  q “ tX P p0  1q : dpX  Xiq ă u.
Lemma 2. For any n P N`  the p.d.f. (2) of X1  . . .   Xn (due to the exchangeability  we can assume
`
X1 ă X2 ă ¨¨¨ ă Xn without loss of generality) is maximized when and only when

˘

dpXi  Xi´1q “ sin

1
n ` 1

for all 2 ď i ď n.

According to Lemma 1 and Lemma 2  Corp will nudge the x’s to be spread out within r0  1s  and
penalizes the case when two x’s get too close. Figure 1 presents some simulations from Corp.
This nudge becomes stronger as the sample size n grows  or as the repulsive parameter r grows.
The properties of Corp makes it ideal for strongly favoring spread out latent positions across the
manifold  avoiding the gaps and clustering in small regions that plague GP-LVM-type methods. The
proofs for the lemmas and a simulation algorithm based on rejection sampling can be found in the
supplement.

2.3 Multivariate Corp

Deﬁnition 2. A p-dimensional multivariate process is a Coulomb repulsive process if and only if for
every ﬁnite set of indices t1  . . .   tk in the index set N` 

„ p`1ÿ

m“1

3

Xm t1 „ unifp0  1q  for m “ 1  . . .   p
ppXXX ti|XXX t1  . . .   XXX ti´1q9Πi´1
j“1

pYm ti ´ Ym tjq2

r

1XtiPp0 1q  i ą 1



where the p-dimensional spherical coordinates XXX t’s have been converted into the pp ` 1q-
dimensional Cartesian coordinates YYY t:

Y1 t “ cosp2πX1 tq
Y2 t “ sinp2πX1 tq cosp2πX2 tq
...
Yp t “ sinp2πX1 tq sinp2πX2 tq . . . sinp2πXp´1 tq cosp2πXp tq
Yp`1 t “ sinp2πX1 tq sinp2πX2 tq . . . sinp2πXp´1 tq sinp2πXp tq.

The multivariate Corp maps the hyper-cubic p0  1qp through a spherical coordinate system to a unit
hyper-ball in (cid:60)p`1. The repulsion is then deﬁned as the reciprocal of the square Euclidean distances
between these mapped points in (cid:60)p`1. Based on this construction of multivariate Corp  a straight-
foward generalization of the electroGP model to a p-dimensional manifold could be made  where
p ą 1.

3 Electrostatic Gaussian Process

3.1 Formulation and Model Fitting

In this section  we propose the electrostatic Gaussian process (electroGP) model. Assuming n d-
dimensional data vectors yyy1  . . .   yyyn are observed  the model is given by
i j „ Np0  σ2
jq 
i “ 1  . . .   n 
j “ 1  . . .   d 
(

where yyyi “ pyi 1  . . .   yi dq for i “ 1  . . .   n and GPp0  K jq denotes a Gaussian process prior with
covariance function K jpx  yq “ φj exp
Letting Θ “ pσ2
ﬁtted by maximizing the joint posterior distribution of xxx “ px1  . . . .xnq and Θ 

yi j “ µjpxiq ` i j 
xi „ Corpprq 
µj „ GPp0  K jq 
(cid:32)

´ αjpx ´ yq2

d  αd  φdq denote the model hyperparameters  model (3) could be
pˆxxx  ˆΘq “ arg max

ppxxx|yyy1:n  Θ  rq 

(4)

1  α1  φ1  . . .   σ2

(3)

.

xxx.Θ

where the repulsive parameter r is ﬁxed and can be tuned using cross validation. Based on our
experience  setting r “ 1 always yields good results  and hence is used as a default across this
paper. For the simplicity of notations  r is excluded in the remainder. The above optimization
problem can be rewritten as

“
‰
πpxxxq

 

pˆxxx  ˆΘq “ arg max

xxx.Θ

(cid:96)pyyy1:n|xxx  Θq ` log
‰
πpxi “ xjq

“

where (cid:96)p¨q denotes the log likelihood function and πp¨q denotes the ﬁnite dimensional pdf of Corp.
Hence the Corp prior can also be viewed as a repulsive constraint in the optimization problem.
“ ´8  for any i and j. Starting at initial values
It can be easily checked that log
x0  the optimizer will converge to a local solution that maintains the same order as the initial x0’s.
We refer to this as the self-truncation property. We ﬁnd that conditionally on the starting order 
the optimization algorithm converges rapidly and yields stable results. Although the x’s are not
identiﬁable  since the target function (4) is invariant under rotation  a unique solution does exist
conditionally on the speciﬁed order.
Self-truncation raises the necessity of ﬁnding good initial values  or at least a good initial ordering
for x’s. Fortunately  in our experience  simply applying any standard manifold learning algorithm
to estimate x0 in a manner that preserves distances in Y yields good performance. We ﬁnd very
similar results using LLE  Isomap and eigenmap  but focus on LLE in all our implementations. Our
algorithm can be summarized as follows.

1. Learn the one dimensional coordinate xxx0 by your favorite distance-preserving manifold

learning algorithm and rescale xxx0 into p0  1q;

4

Figure 2: Visualization of three simulation experiments where the data (triangles) are simulated
from a bivariate Gaussian (left)  a rotated parabola with Gaussian noises (middle) and a spiral with
Gaussian noises (right). The dotted shading denotes the 95% posterior predictive uncertainty band
of py1  y2q under electroGP. The black curve denotes the posterior mean curve under electroGP and
the red curve denotes the P-curve. The three dashed curves denote three realizations from GP-LVM.
The middle panel shows a zoom-in region and the full ﬁgure is shown in the embedded box.

2. Solve Θ0 “ arg maxΘ ppyyy1:n|xxx0  Θ  rq using scaled conjugate gradient descent (SCG);
3. Using SCG  setting xxx0 and Θ0 to be the initial values  solve ˆxxx and ˆΘ w.r.t. (4).

3.2 Posterior Mean Curve and Uncertainty Bands

In this subsection  we describe how to obtain a point estimate of the curve µµµ and how to charac-
terize its uncertainty under electroGP. Such point and interval estimation is as of yet unsolved in
the literature  and is of critical importance. In particular  it is difﬁcult to interpret a single point
estimate without some quantiﬁcation of how uncertain that estimate is. We use the posterior mean
curve ˆµµµ “ Epµµµ|ˆxxx  yyy1:n  ˆΘq as the Bayes optimal estimator under squared error loss. As a curve  ˆµµµ
has inﬁnite dimensions. Hence  in order to store and visualize it  we discretize r0  1s to obtain nµ
nµ´1 for i “ 1  . . .   nµ. Using basic multivariate Gaussian theory 
equally-spaced grid points xµ
`
the following expectation is easy to compute.
µµµpxµ
“ E

˘
nµq
1q  . . .   ˆµµµpxµ

`
ˆµµµpxµ

i “ i´1

˘

.

(cid:32)
(
nµq|ˆxxx  yyy1:n  ˆΘ
1q  . . .   µµµpxµ
i q
i   ˆµµµpxµ
xµ

nµ

i“1. For ease of notation  we use
Then ˆµµµ is approximated by linear interpolation using
ˆµµµ to denote this interpolated piecewise linear curve later on. Examples can be found in Figure 2
where all the mean curves (black solid) were obtained using the above method.
Estimating an uncertainty region including data points with η probability is much more challenging.
We addressed this problem by the following heuristic algorithm.
Step 1. Draw x˚
Step 2. Sample the corresponding yyy˚
1   . . .   yyy˚

i from the posterior predictive distribution conditional on these
n1|x˚

i ’s from Unif(0 1) independently for i “ 1  . . .   n1;

latent coordinates ppyyy˚

  ˆxxx  yyy1:n  ˆΘq;

Step 3. Repeat steps 1-2 n2 times  collecting all n1 ˆ n2 samples yyy˚’s;
Step 4. Find the shortest distances from these yyy˚’s to the posterior mean curve ˆµµµ  and ﬁnd the

1:n1

η-quantile of these distances denoted by ρ;

Step 5. Moving a radius-ρ ball through the entire curve ˆµµµpr0  1sq  the envelope of the moving trace

deﬁnes the η% uncertainty band.

Note that step 4 can be easily solved since ˆµµµ is a piecewise linear curve. Examples can be found in
Figure 2  where the 95% uncertainty bands (dotted shading) were found using the above algorithm.

5

Figure 3: The zoom-in of the spiral case 3 (left) and the corresponding coordinate function  µ2pxq 
of electroGP (middle) and GP-LVM (right). The gray shading denotes the heatmap of the posterior
distribution of px  y2q and the black curve denotes the posterior mean.

3.3 Simulation

In this subsection  we compare the performance of electroGP with GP-LVM and principal curves (P-
curve) in simple simulation experiments. 100 data points were sampled from each of the following
three 2-dimensional distributions: a Gaussian distribution  a rotated parabola with Gaussian noises
and a spiral with Gaussian noises. ElectroGP and GP-LVM were ﬁtted using the same initial values
obtained from LLE  and the P-Curve was ﬁtted using the princurve package in R.
The performance of the three methods is compared in Figure 2. The dotted shading represents a
95% posterior predictive uncertainty band for a new data point yyyn`1 under the electroGP model.
This illustrates that electroGP obtains an excellent ﬁt to the data  provides a good characterization of
uncertainty  and accurately captures the concentration near a 1d manifold embedded in two dimen-
sions. The P-curve is plotted in red. The extremely poor representation of P-curve is as expected
based on our experience in ﬁtting principal curve in a wide variety of cases; the behavior is highly
unstable. In the ﬁrst two cases  the P-Curve corresponds to a smooth curve through the center of
the data  but for the more complex manifold in the third case  the P-Curve is an extremely poor
representation. This tendency to cut across large regions of near zero data density for highly curved
manifolds is common for P-Curve.
For GP-LVM  we show three random realizations (dashed) from the posterior in each case. It is
clear the results are completely unreliable  with the tendency being to place part of the curve through
where the data have high density  while also erratically adding extra outside the range of the data.
The GP-LVM model does not appropriately penalize such extra parts  and the very poor performance
shown in the top right of Figure 2 is not unusual. We ﬁnd that electroGP in general performs
dramatically better than competitors. More simulation results can be found in the supplement. To
better illustrate the results for the spiral case 3  we zoom in and present some further comparisons
of GP-LVM and electroGP in Figure 3.
As can be seen the right panel  optimizing x’s without any constraint results in “holes” on r0  1s.
The trajectories of the Gaussian process over these holes will become arbitrary  as illustrated by the
three realizations. This arbitrariness will be further projected into the input space Y  resulting in
the erratic curve observed in the left panel. Failing to have well spread out x’s over r0  1s not only
causes trouble in learning the curve  but also makes the posterior predictive distribution of yyyn`1
overly diffuse near these holes  e.g.  the large gray shading area in the right panel. The middle panel
shows that electroGP ﬁlls in these holes by softly constraining the latent coordinates x’s to spread
out while still allowing the ﬂexibility of moving them around to ﬁnd a smooth curve snaking through
them.

3.4 Prediction

Broad prediction problems can be formulated as the following missing data problem. Assume m new
data zzzi  for i “ 1  . . .   m  are partially observed and the missing entries are to be ﬁlled in. Letting
i denote the observed data vector and zzzM
i denote the missing part  the conditional distribution of
zzzO

6

Original

Observed

electroGP GP-LVM

Figure 4: Left Panel: Three randomly selected reconstructions using electroGP compared with
those using Bayesian GP-LVM; Right Panel: Another three reconstructions from electroGP  with
the ﬁrst row presenting the original images  the second row presenting the observed images and the
third row presenting the reconstructions.

the missing data is given by

ż
ż
1:m  ˆxxx  yyy1:n  ˆΘq
1:m|zzzO
ppzzzM
¨¨¨
1:m|xz

ppzzzM

xz
1

1:m  ˆxxx  yyy1:n  ˆΘq ˆ ppxz

“
i is the corresponding latent coordinate of zzzi  for i “ 1  . . .   n. However  dealing with
mq jointly is intractable due to the high non-linearity of the Gaussian process  which

1:m  ˆxxx  yyy1:n  ˆΘqdxz

1 ¨¨¨ dxz
m 

1:m|zzzO

xz
m

where xz
pxz
motivates the following approximation 

1  . . .   xz

ppxz

i“1ppxz

1:m|zzzO
i|zzzO
mq to be conditionally independent. This assumption is more
The approximation assumes pxz
accurate if ˆxxx is well spread out on p0  1q  as is favored by Corp.
The univariate distribution ppxz
i   yyy1:n  ˆuuu  ˆΘq  though still intractable  is much easier to deal with.
Depending on the purpose of the application  either a Metropolis Hasting algorithm could be adopted
to sample from the predictive distribution  or a optimization method could be used to ﬁnd the MAP
of xz’s. The details of both algorithms can be found in the supplement.

1:m  ˆxxx  yyy1:n  ˆΘq « Πm
1  . . .   xz
i|xxxO

i   ˆxxx  yyy1:n  ˆΘq.

4 Experiments

200 consecutive frames (of size 76 ˆ 101 with RGB color) [13] were collected
Video-inpainting
from a video of a teapot rotating 1800. Clearly these images roughly lie on a curve. 190 of the frames
were assumed to be fully observed in the natural time order of the video  while the other 10 frames
were given without any ordering information. Moreover  half of the pixels of these 10 frames were
missing. The electroGP was ﬁtted based on the other 190 frames and was used to reconstruct the
broken frames and impute the reconstructed frames into the whole frame series with the correct
order. The reconstruction results are presented in Figure 4. As can be seen  the reconstructed
images are almost indistinguishable from the original ones. Note that these 10 frames were also
correctly imputed into the video with respect to their latent position x’s. ElectroGP was compared
with Bayesian GP-LVM [7] with the latent dimension set to 1. The reconstruction mean square
error (MSE) using electroGP is 70.62  compared to 450.75 using GP-LVM. The comparison is
also presented in Figure 4. It can be seen that electroGP outperforms Bayesian GP-LVM in high-
resolution precision (e.g.  how well they reconstructed the handle of the teapot) since it obtains a
much tighter and more precise estimate of the manifold.
100 consecutive frames (of size 100 ˆ 100 with gray color) were
Super-resolution & Denoising
collected from a video of a shrinking shockwave. Frame 51 to 55 were assumed completely missing
and the other 95 frames were observed with the original time order with strong white noises. The
shockwave is homogeneous in all directions from the center; hence  the frames roughly lie on a
curve. The electroGP was applied for two tasks: 1. Frame denoising; 2. Improving resolution by
interpolating frames in between the existing frames. Note that the second task is hard since there are

7

Original

Noisy

electroGP

NLM

IsD

electroGP

LI

Figure 5: Row 1: From left to right are the original 95th frame  its noisy observation  its denoised
result by electroGP  NLM and IsD; Row 2: From left to right are the original 53th frame  its regen-
eration by electroGP  the residual image (10 times of the absolute error between the imputation and
the original) of electroGP and LI. The blank area denotes its missing observation.

5 consecutive frames missing and they can be interpolated only if the electroGP correctly learns the
underlying manifold.
The denoising performance was compared with non-local mean ﬁlter (NLM) [14] and isotropic
diffusion (IsD) [15]. The interpolation performance was compared with linear interpolation (LI).
The comparison is presented in Figure 5. As can be clearly seen  electroGP greatly outperforms
other methods since it correctly learned this one-dimensional manifold. To be speciﬁc  the denoising
MSE using electroGP is only 1.8ˆ 10´3  comparing to 63.37 using NLM and 61.79 using IsD. The
MSE of reconstructing the entirely missing frame 53 using electroGP is 2 ˆ 10´5 compared to 13
using LI. An online video of the super-resolution result using electroGP can be found in this link1.
The frame per second (fps) of the generated video under electroGP was tripled compared to the
original one. Though over two thirds of the frames are pure generations from electroGP  this new
video ﬂows quite smoothly. Another noticeable thing is that the 5 missing frames were perfectly
regenerated by electroGP.

5 Discussion

Manifold learning has dramatic importance in many applications where high-dimensional data are
collected with unknown low dimensional manifold structure. While most of the methods focus on
ﬁnding lower dimensional summaries or characterizing the joint distribution of the data  there is (to
our knowledge) no reliable method for probabilistic learning of the manifold. This turns out to be
a daunting problem due to major issues with identiﬁability leading to unstable and generally poor
performance for current probabilistic non-linear dimensionality reduction methods. It is not obvious
how to incorporate appropriate geometric constraints to ensure identiﬁability of the manifold without
also enforcing overly-restrictive assumptions about its form.
We tackled this problem in the one-dimensional manifold (curve) case and built a novel electro-
static Gaussian process model based on the general framework of GP-LVM by introducing a novel
Coulomb repulsive process. Both simulations and real world data experiments showed excellent
performance of the proposed model in accurately estimating the manifold while characterizing un-
certainty. Indeed  performance gains relative to competitors were dramatic. The proposed electroGP
is shown to be applicable to many learning problems including video-inpainting  super-resolution
and video-denoising. There are many interesting areas for future study including the development
of efﬁcient algorithms for applying the model for multidimensional manifolds  while learning the
dimension.

1https://youtu.be/N1BG220J5Js This online video contains no information regarding the authors.

8

References
[1] J.B. Tenenbaum  V. De Silva  and J.C. Langford. A global geometric framework for nonlinear

dimensionality reduction. Science  290(5500):2319–2323  2000.

[2] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding.

Science  290(5500):2323–2326  2000.

[3] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and

clustering. In NIPS  volume 14  pages 585–591  2001.

[4] M. Chen  J. Silva  J. Paisley  C. Wang  D.B. Dunson  and L. Carin. Compressive sensing
on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance
bounds. Signal Processing  IEEE Transactions on  58(12):6140–6155  2010.

[5] Y. Wang  A. Canale  and D.B. Dunson. Scalable multiscale density estimation. arXiv preprint

arXiv:1410.7692  2014.

[6] N. Lawrence. Probabilistic non-linear principal component analysis with gaussian process

latent variable models. The Journal of Machine Learning Research  6:1783–1816  2005.

[7] M. Titsias and N. Lawrence. Bayesian gaussian process latent variable model. The Journal of

Machine Learning Research  9:844–851  2010.

[8] Neil D Lawrence and Joaquin Qui˜nonero-Candela. Local distance preservation in the GP-LVM
In Proceedings of the 23rd international conference on Machine

through back constraints.
learning  pages 513–520. ACM  2006.

[9] Raquel Urtasun  David J Fleet  Andreas Geiger  Jovan Popovi´c  Trevor J Darrell  and Neil D
Lawrence. Topologically-constrained latent variable models. In Proceedings of the 25th inter-
national conference on Machine learning  pages 1080–1087. ACM  2008.

[10] T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association 

84(406):502–516  1989.

[11] V. Rao  R.P. Adams  and D.B. Dunson. Bayesian inference for mat´ern repulsive processes.

arXiv preprint arXiv:1308.1136  2013.

[12] J.B. Hough  M. Krishnapur  Y. Peres  et al. Zeros of Gaussian analytic functions and determi-

nantal point processes  volume 51. American Mathematical Soc.  2009.

[13] K.Q. Weinberger and L.K. Saul. An introduction to nonlinear dimensionality reduction by

maximum variance unfolding. In AAAI  volume 6  pages 1683–1686  2006.

[14] A. Buades  B. Coll  and J.M. Morel. A non-local algorithm for image denoising. In Computer
Vision and Pattern Recognition  2005. CVPR 2005. IEEE Computer Society Conference on 
volume 2  pages 60–65. IEEE  2005.

[15] P. Perona and J. Malik. Scale-space and edge detection using anisotropic diffusion. Pattern

Analysis and Machine Intelligence  IEEE Transactions on  12(7):629–639  1990.

9

,Ye Wang
David Dunson