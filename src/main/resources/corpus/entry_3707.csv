2019,Necessary and Sufficient Geometries for Gradient Methods,We study the impact of the constraint set and gradient geometry on the convergence of online and stochastic methods for convex optimization  providing a characterization of the geometries for which stochastic gradient and adaptive gradient methods are (minimax) optimal. In particular  we show that when the constraint set is quadratically convex  diagonally pre-conditioned stochastic gradient methods are minimax optimal. We further provide a converse that shows that when the constraints are not quadratically convex---for example  any $\ell_p$-ball for $p < 2$---the methods are far from optimal. Based on this  we can provide concrete recommendations for when one should use adaptive  mirror or stochastic gradient methods.,Necessary and Sufﬁcient Geometries

for Gradient Methods

Daniel Levy

Stanford University

danilevy@stanford.edu

John C. Duchi

Stanford University

jduchi@stanford.edu

Abstract

We study the impact of the constraint set and gradient geometry on the convergence
of online and stochastic methods for convex optimization  providing a charac-
terization of the geometries for which stochastic gradient and adaptive gradient
methods are (minimax) optimal. In particular  we show that when the constraint
set is quadratically convex  diagonally pre-conditioned stochastic gradient methods
are minimax optimal. We further provide a converse that shows that when the
constraints are not quadratically convex—for example  any `p-ball for p < 2—the
methods are far from optimal. Based on this  we can provide concrete recommen-
dations for when one should use adaptive  mirror or stochastic gradient methods.

1

Introduction

✓2⇥

minimize

fP (✓) := EP [F (✓  X )] =Z F (✓  x)dP (x) 

We study stochastic and online convex optimization in the following setting: for a collection
{F (·  x)  x 2X} of convex functions F (·  x) : Rd ! R and distribution P on X   we wish to
solve
(1)
where ⇥ ⇢ Rd is a closed convex set. The geometry of the underlying constraint set ⇥ and structure
of subgradients @F (·  x) of course impact the performance of algorithms for problem (1). Thus  while
stochastic subgradient methods are a de facto choice for their simplicity and scalability [22  19  5] 
their convergence guarantees depend on the `2-diameter of ⇥ and @F (·  x)  so that for non-Euclidean
geometries (e.g. when ⇥ is an `1-ball) one can obtain better convergence guarantees using mirror
descent  dual averaging or the more recent adaptive gradient methods [18  19  4  20  13]. We revisit
these ideas and precisely quantify optimal rates and gaps between the methods.
Our main contribution is to show that the geometry of the constraint set and gradients interact in
a way completely analogous to Donoho et al.’s classical characterization of optimal estimation
in Gaussian sequence models [10]  where one observes a vector ✓ 2 ⇥ corrupted by Gaussian
noise  Y = ✓ + N(0  2I). For such problems  one can consider linear estimators—b✓ = AY for
a A 2 Rd⇥d—or potentially non-linear estimators—b✓ =( Y ) where : Rd ! ⇥. When ⇥ is
j ) | ✓ 2 ⇥} is convex  Donoho et al. show there
quadratically convex  meaning the set ⇥2 := {(✓2
exists a minimax rate optimal linear estimator; conversely  there are non-quadratically convex ⇥ for
which minimax rate optimal estimatorsb✓ must be nonlinear in Y .
To build our analogy  we turn to stochastic and online convex optimization. Consider Nesterov’s
dual averaging  where for a strongly convex h :⇥ ! R  one iterates for k = 1  2  . . . by receiving a
(random) Xk 2X   choosing gk 2 @F (✓k  Xk)  and for a stepsize ↵k > 0 updating

✓2⇥ ⇢Xik

1
↵k

h(✓).

✓k+1 := argmin

g>i ✓ +

(2)

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

When ⇥= Rd and h is Euclidean  that is  h(✓) = 1

2 ✓>A✓ for some A  0  the updates are linear in
the observed gradients gi  as ✓k = ↵kA1Pik gi. Drawing a parallel between  in the Gaussian
sequence model [10] and h in dual averaging (2)  a natural conjecture is that a dichotomy similar
to that for the Gaussian sequence model holds for stochastic and online convex optimization: if
⇥ is quadratically convex  there is a Euclidean h (yielding “linear” updates) that is minimax rate
optimal  while there exist non-quadratically convex ⇥ for which Euclidean distance-generating h are
arbitrarily suboptimal. We show that this analogy holds almost completely  with the caveat that we
fully characterize minimax rates when the subgradients lie in a quadratically convex set or a weighted
`r ball  r  1. (This issue does not arise for the Gaussian sequence model  as the observations Y
come from a ﬁxed distribution  so there is no notion of alternative norms on Y .)
More precisely  we prove that for compact  convex  quadratically convex  orthosymmetric constraint
sets ⇥  subgradient methods with a ﬁxed diagonal re-scaling are minimax rate optimal. This
guarantees that for a large collection of constraints (e.g. `2 balls  weighted `p-bodies for p  2  or
hyperrectangles) a diagonal re-scaling sufﬁces. This is important in machine learning problems of
appropriate geometry  for example  in linear classiﬁcation problems where the data (features) are
sparse  so using a dense predictor ✓ is natural [13  14]. Conversely  we show that if the constraint
set ⇥ is a (scaled) `p ball  1  p < 2  then  considering unconstrained updates (2)  the regret of
the best method of linear type can bepd/ log d times larger than the minimax rate. As part of this 
we provide new information-theoretic lower bounds on optimization for general convex constraints
⇥. In contrast to the frequent practice in literature of comparing regret upper bounds—prima facie
illogical—we demonstrate the gap between linear and non-linear methods must hold.
Our conclusions relate to the growing literature in adaptive algorithms [3  13  21  9]. Our results
effectively prescribe that these adaptive algorithms are useful when the constraint set is quadratically
convex as then there is a minimax optimal diagonal pre-conditioner. Even more  different sets suggest
different regularizers. For example  when the constraint set is a hyperrectangle  AdaGrad has regret
at most p2 times that of the best post-hoc pre-conditioner  which we show is minimax optimal  while
(non-adaptive) standard gradient methods can be pd suboptimal on such problems. Conversely  our
results strongly recommend against those methods for non-quadratically convex constraint sets. Our
results thus clarify and explicate the work of Wilson et al. [28]: when the geometry of ⇥ and @F is
appropriate for adaptive gradient methods or Euclidean algorithms  one should use them; when it is
not—the constraints ⇥ are not quadratically convex—one should not.

✓

Notation d always refers to dimension and n to sample size. For a norm  : Rd ! R+ 
B(x0  r) := {x | (x  x0)  r} denotes the ball of radius r around x0 in the  norm. For
p 2 [1 1] we use the shorthand Bp(x0  r) := Bk·kp(x0  r). The dual norm of  is ⇤(z) =
sup(x)1 x>z. For ✓  ⌧ 2 Rd  we abuse notation and deﬁne ✓2 := (✓2
j )jd  |✓| := (|✓j|)jd 
⌧ := (✓j/⌧j)jd and ✓  ⌧ := (✓j⌧j)jd. The function h : Rd ! R denotes a distance generating
function  i.e. a function strongly convex w.r.t. a norm k·k ; Dh(x  y) = h(x) h(y)rh(y)>(x y)
denotes the Bregman divergence  where h is strongly convex w.r.t. k·k if and only if Dh(x  y) 
2 kx  yk2. The subdifferential of F (·  x) at ✓ is @✓F (✓  x). I(X; Y ) is the (Shannon) mutual infor-
mation between random variables X and Y . For a set ⌦ and f  g :⌦ ! R  we write f . g if there
exists a ﬁnite numerical constant C such that f (t)  Cg(t) for t 2 ⌦  and f ⇣ g if g . f . g.
2 Preliminaries

1

We begin by deﬁning the minimax framework in which we analyze procedures  review standard
stochastic subgradient methods  and introduce the relevant geometric notions of convexity we require.

Minimax rate for convex stochastic optimization We measure the complexity of families of
problems in two familiar ways: stochastic minimax complexity and regret [18  1  6]. Let ⇥ ⇢ Rd be
a closed convex set  X a sample space  and F a collection of functions F : Rd ⇥X ! R. For a
collection P of distributions over X   recall (1) that fP (✓) :=R F (✓  x)dP (x) is the expected loss of
the point ✓. Then the minimax stochastic risk is

MS

n(⇥ F P) := inf
b✓n

sup
F2F

sup
P2P

EfP (b✓n(X n

2

1 ))  inf
✓2⇥

fP (✓)  

1

1

1
n

MR

F2F xn

  so that

iid⇠ P and the inﬁmum ranges over all measurable functionsb✓n

where the expectation is taken over X n
1
of X n. A related notion is the average minimax regret  which instead takes a supremum over samples
1 2X n and measures losses instantaneously. In this case  an algorithm consists of a sequence of
xn
decisionsb✓1 b✓2  . . .  b✓n  whereb✓i is chosen conditional on samples xi1
nXi=1hF⇣b✓ixi1

In the regret case we may of course identify xi with individual functions F   so this corresponds to

   xi⌘  F (✓  xi)i .

n(⇥ F X ) := inf
b✓1:n

sup
1 2X n ✓2⇥

n; thus we typically provide lower bounds on MS

the standard regret. In both of these deﬁnitions  we do not constrain the point estimatesb✓ to lie in the

constraint sets—in language of learning theory  improper predictions—but in our cases  this does
not change regret by more than a constant factor. As online-to-batch conversions make clear [7]  we
always have MS
n and upper bounds on MR
n.
We study functions whose continuity properties are speciﬁed by a norm  over Rd  deﬁning

n  MR
F  r :=F : Rd ⇥X ! R | for all ✓ 2 Rd  g 2 @✓F (✓  x)  (g)  r  

(3)
which is equivalent to the Lipschitz condition |F (✓  x)  F (✓0  x)| r⇤(✓  ✓0)  where ⇤ is the
dual norm to . For a given norm  ( as a mnemonic for gradient)  we use the shorthands
n(⇥ F  1 P)
as the Lipschitzian properties of F in relation to ⇥ determine the minimax regret and risk.
Stochastic gradient methods  mirror descent  and regret Let us brieﬂy review the canonical
algorithms for solving the problem (1) and their associated convergence guarantees. For an algorithm
outputing points ✓1  . . .  ✓ n  the regret on the sequence F (·  xi) with respect to a point ✓ is

n(⇥ F  1 X ) and MS

n(⇥  ) := sup
X

n(⇥  ) := sup
X

P⇢P (X )

MR

MR

MS

sup

Regretn(✓) :=

[F (✓i  xi)  F (✓  xi)].

nXi=1

Recalling the deﬁnition Dh(✓  ✓0) = h(✓)  h(✓0)  rh(✓0)>(✓  ✓0) of the Bregman divergence 
the mirror descent algorithm [18  4] iteratively sets

gi 2 @✓F (✓i  xi) and updates ✓MD

i+1 := argmin

✓2⇥ ⇢g>i ✓ +

1
↵

Dh(✓  ✓i)

(4)

where ↵> 0 is a stepsize. When the function h is 1-strongly convex with respect to a norm k·k with
dual norm k·k⇤

  the iterates (4) and the iterates (2) of dual averaging satisfy (cf. [4  6  20])

Regretn(✓) 

↵

Dh(✓  ✓0)

+

↵

2Xin

kgik2
⇤

for any ✓ 2 ⇥.

(5)

2(p1) k✓k2

p  which is strongly convex with respect to the `p-norm k·kp.

One recovers the classical stochastic gradient method with the choice h(✓) = 1
2  which is
strongly convex with respect to the `2-norm  while the p-norm algorithms [15  23]  deﬁned for
1 < p  2  use h(✓) = 1
As we previously stated in our deﬁnitions of minimax risk and regret  we do not constrain the point
estimates to lie in the constraint set ⇥  which is equivalent to taking ⇥= Rd in the updates (4)
or (2). The regret bound (5) still holds when considering unconstrained updates  whenever ✓ 2 ⇥ 
and the regret of the algorithm with respect to a constraint set ⇥ is simply sup✓2⇥ Regretn(✓). Even
with unconstrained updates  the form (5) still captures small regret for all common constraint sets
log(2d) 
⇥ [23]. To make clear  let ⇥ ⇢ Rd be the `1-ball; taking h(✓) = 1
q = p

p for p = 1 + 1

2(p1) k✓k2

2 k✓k2

p1 = 1 + log(2d)  and ✓0 = 0 guarantees
↵
h(✓) +

sup
k✓k11

Regretn(✓) 

2Xin
kgik2
eplog(2d)/n gives the familiar O(1) · pn log d
Assuming kgik1  1 for all i and taking ↵ = 2
regret.

kgik2
1 .

sup
k✓k11

nXi=1

2 log(2d)

e2↵
2

q 

2
↵

+

↵

3

We frequently focus on distance generating functions of the form h(✓) = 1
2 ✓>A✓ for a ﬁxed positive
semi-deﬁnite matrix A. For an arbitrary A  we will refer to these methods as Euclidean gradient
methods and for a diagonal A as diagonally-scaled gradient methods. It is important to note that 
in this case  the mirror descent update is the stochastic gradient update with A1g  where g is a
stochastic subgradient. We shall refer to all such methods as methods of linear type.
Quadratic convexity and orthosymmetry For a set ⇥  we let ⇥2 := {✓2 ✓ 2 ⇥} denote its
square. The set ⇥ is quadratically convex if ⇥2 is convex; typical examples of quadratically convex
sets are weighted `p bodies for p  2 or hyperrectangles. We let QHull(⇥) be the quadratic convex
hull of ⇥  meaning the smallest convex and quadratically convex set containing ⇥. The set ⇥ ⇢ Rd
is orthosymmetric if it is invariant to ﬂipping the signs of any coordinate. Formally  if ✓ 2 ⇥ then
s 2 {±1}d implies (sj✓j)jd 2 ⇥. We extend this notion to norms: we say that a norm  is
orthosymmetric if (g) = (|g|) for all g. Similarly  we will say that a norm  is quadratically
convex if  induces a quadratically convex unit ball.

3 Minimax optimality and quadratically convex constraint sets

We begin our contributions by considering quadratically convex constraint sets  providing lower
bounds on the minimax risk and matching upper bounds on the minimax regret of convex optimization
over such sets. We further show that these are attained by diagonally-scaled gradient methods.
While the analogy with the Gaussian sequence model is nearly complete  in distinction to the work
of Donoho et al. (where results depend solely on the constraints ⇥)  our results necessarily depend on
the geometry of the subdifferential. Consequently  we distinguish throughout this section between
quadratically and non-quadratically convex geometry of the gradients. To set the stage and preview
our contributions  we begin our study with the familiar case of ⇥= Bp(0  1) and norm on the
subgradients  = k·kr (mnemonically   for gradients)  with p 2 [2 1] (so that ⇥ is quadratically
convex) and r  1. We then turn to arbitrary quadratically convex constraint sets and ﬁrst show
results in the case of general quadratically convex norms on the subgradients. We conclude the
section by proving that  when the subgradients do not lie in a quadratically convex set but lie in a
weighted `r ball (for r 2 [1  2])  diagonally-scaled gradient methods are still minimax rate optimal.
3.1 A warm-up: p-norm constraint sets for p  2
The results for the basic case that the constraints ⇥ are an `p-ball while the gradients belong to a
different `r-ball are special cases of the theorems to come  the proofs (appendicized) are simpler and
provide intuition for the later results. We distinguish between two cases depending on the value of r
in the gradient norm. The case that r 2 [1  2] corresponds roughly to “sparse” gradients  while the
case r  2 corresponds to harder problems with dense gradients. We provide information theoretic
proofs of the following two results in Appendices B.1 and B.2  respectively.
Proposition 1 (Sparse gradients). Let ⇥= Bp(0  1) with p  2 and (·) = k·kr where r 2 [1  2].
Then

1

p

2 1
d
pn . MS

1 ^

n(⇥  )  MR

n(⇥  ) . 1 ^

p

1

2 1
d
pn

.

Proposition 2 (Dense gradients). Let ⇥= Bp(0  1) with p  2 and (·) = k·k r with r  2. Then

d

1 ^

2 1

r

1

2 1
p d 1
pn

. MS

n(⇥  )  MR

n(⇥  ) . 1 ^

d

2 1

r

1

2 1
p d 1
pn

.

In both cases  the stochastic gradient method achieves the regret upper bound via a straightforward
2. That is  a method of linear type is optimal.
optimization of the regret bounds (5) with h(✓) = 1

2 k✓k2

3.2 General quadratically convex constraints
We now turn to the more general case that ⇥ is an arbitrary convex  compact  quadratically convex
and orthosymmetric set. We combine two techniques to develop the results. The ﬁrst essentially
builds out of the ideas of Donoho et al. [10] in Gaussian sequence estimation  which shows that the
largest hyperrectangle in ⇥ governs the performance of linear estimators; this gives us a lower bound.

4

The key second technique is in the upper bound  where a strong duality result holds because of the
quadratic convexity of ⇥—allowing us to prove minimax optimality of diagonally scaled Euclidean
procedures. As in the previous section  we divide our analysis into cases depending on whether the
gradient norm  is quadratically convex or not (the analogs of r 7 2 in Propositions 1 and 2).
We begin with the lower bound  which relies on rectangular structures in the primal ⇥ and dual
gradient spaces. For the proposition  we use a specialization of the function families (3) to rectangular
sets  where for M 2 Rd

+ we deﬁne

F M :=⇢F : Rd ⇥X ! R | for all ✓ 2 Rd  g 2 @✓f (✓  x)  max
j=1[aj  aj] ⇢ ⇥. Then

Proposition 3 (Duchi et al. [14]  Proposition 1). Let M 2 Rd
and assume the hyperrectangular containmentQd
dXj=1
n(⇥ F M ) 

8pn log 3

Mjaj.

MS

jd

1

|gj|

Mj  1 .

+ and F M be as above. Let a 2 Rd

+

We begin the analysis of the general case by studying the rates of diagonally-scaled gradient methods.

3.2.1 Diagonal re-scaling in gradient methods
As we discuss in Section 2  diagonally-scaled gradient methods (componentwise re-scaling of the
2 ✓>⇤✓ for ⇤ = diag() ⌫ 0 in the mirror descent
subgradients) are equivalent to using h⇤(✓) := 1
update (4). In this case  for any norm  on the gradients  the minimax regret bound (5) becomes
g>⇤1g# .

sup
✓2⇥
The rightmost term of course upper bounds the minimax regret  so we may take an inﬁmum over ⇤ 
yielding

2n24sup

✓>⇤✓ +Xin

2n"sup

Regretn ⇤(✓) 

✓>⇤✓ + n

g2B (0 1)

✓2⇥

✓2⇥

sup

1

1

MR

n(⇥  ) 

1
2n

inf
⌫0

sup
✓2⇥

j✓2

j + nXjd

1
j

g2

j

(6)

g>i ⇤1gi35 
g2B (0 1)Xjd

sup

The regret bound (6) holds without assumptions on ⇥ or . However  in the case when ⇥ is
quadratically convex  strong duality allows us to simplify this quantity:
Proposition 4. Let V  ⇥ ⇢ Rd be convex  quadratically convex and compact sets. Then
v2) .

✓2⇥ v2V(>✓2 +✓ 1
◆>

0(>✓2 +✓ 1
◆>

v2) = sup

✓2⇥ v2V

inf
0

sup

inf

+ ⇥ Rd

+ ! R  J(⌧  w   ) := >⌧ + ( 1

Proof. The quadratic convexity of the sets ⇥ and V implies that a (weighted) squared 2-norm
becomes a linear functional when lifted to the squared sets ⇥2 := {✓2 | ✓ 2 ⇥} and V 2. Indeed 
deﬁning J : R2d
 )>w  the function J is concave-convex: it is
linear (a fortiori concave) in (⌧  w) and convex in . Thus  using that the set { 2 Rd
+} is convex and
⇥2 ⇥ V 2 is convex compact (because ⇥ and V are quadratically convex compact)  Sion’s minimax
theorem [25] implies
w)
⌧2⇥2 w2V 2(>⌧ +✓ 1
◆>
w) .
0(>⌧ +✓ 1
◆>

✓2⇥ v2V(>✓2 +✓ 1
◆>

v2) = inf

⌧2⇥2 w2V 2

inf
0

0

sup

sup

sup

inf

=

Replacing ⌧ with ✓2 and w with v2 gives the result.

Proposition 4 provides a powerful hammer for diagonally scaled Euclidean optimization algorithms 
as we can choose an optimal scaling for any ﬁxed pair ✓  g  taking a worst case over such pairs:

5

Corollary 1. Let ⇥ be a convex  quadratically convex  compact set. Then

MR

n(✓   ) 

1
pn

sup

✓>g 

g2QHull(B (0 1)) ✓2⇥

and diagonally-scaled gradient methods achieves this regret.

Proof. We upper bound the minimax regret (6) by taking a supremum over the quadratic hull

g 2 QHull (B(0  1))  which contains B(0  1). Using that for a  b > 0  inf >0 a + b/ = 2pab

and applying Proposition 4 gives the proof.

The corollary allows us to provide concrete upper and lower bounds on minimax risk and regret  with
the results differing slightly based on whether the gradient norms are quadratically convex.

3.2.2 Orthosymmetric and quadratically convex gradient norms
We now provide lower bounds on minimax risk complementary to Corollary 1  focusing ﬁrst on the
case that the gradient norm  is quadratically convex.
Assumption A1. The norm  is orthosymmetric and quadratically convex  meaning (s v) = (v)
for all s 2 {±1}d and B(0  1) is quadratically convex.
With this  we have the following theorem  which shows that diagonally-scaled gradient methods are
minimax rate optimal  and that the constants are sharp up to a factor of 9  whenever the gradient
norms are quadratically convex. While the constant 9 is looser than that Donoho et al. [10] provide
for Gaussian sequence models  this theorem highlights the essential structural similarity between the
sequence model case and stochastic optimization methods.
Theorem 1. Let Assumption A1 hold and let ⇥ be quadratically convex  orthosymmetric  and
compact. Then

1

1
pn

8plog 3
There exists ⇤ 2 Rd
We present the proof in Appendix C.1.

sup
✓2⇥

⇤(✓)  MS

n(⇥  )  MR

n(⇥  ) 

1
pn

sup
✓2⇥

⇤(✓).

+ such that diagonally-scaled gradient methods with ⇤ achieve this rate.

3.2.3 Arbitrary gradient norms
When the norm  on the gradients deﬁnes a non-quadratically convex norm ball B(0  1)—for
example  when the gradients belong to an `r-norm ball for r 2 [1  2]—our results become slightly
less general. Nonetheless  when  is a weighted `r-norm ball (for r 2 [1  2])  diagonally-scaled
gradient methods are minimax rate optimal  as Corollary 2 will show; when the norms  are arbitrary
we have a slightly more complex result.
Theorem 2. Let ⇥ be an orthosymmetric  quadratically convex  convex and compact set and  an
arbitrary norm. Recall the deﬁnition ( ✓

(e.) )j = ✓j/(ej). Then for any k 2 N 

1

8pn log 3✓1 

k

n log 3◆

sup

✓2⇥ k✓k0k

✓

(e·)2  MS

 MR

n(⇥  )

n(⇥  ) 

1
pn

sup
✓2⇥

sup

✓>g.

g2QHull(B (0 1))

(7)

Corollary 1 gives the upper bound in the theorem. The lower bound consists of an application
of Assouad’s method [2]  but  in parallel to the warm-up examples  we construct well-separated
functions with “sparse” gradients. See Appendix C.2 for a proof.
We can develop a corollary of this result when the norm  is a weighted-`r norm (for r 2 [1  2]).
While these do not induce quadratically convex norm balls  meaning the results of the previous
section do not apply  the previous theorem still guarantees that diagonally-scaled gradient methods
are minimax rate optimal.

6

sup

✓2⇥

✓

(e·)2

.

1
16

sup

✓2⇥

✓

(e·)2  MS

Corollary 2. Let the conditions of Theorem 2 hold and assume that (g) = k  gkr with r 2 [1  2] 
j > 0 and (  g)j = jgj. Then for n  2d 

1
pn

n(⇥  ) 

n(⇥  )  MR

+ such that diagonally-scaled gradient methods with ⇤ achieve this rate.

1
pn
There exists ⇤ 2 Rd
A minor modiﬁcation of Theorem 2 gives the lower bound  while we obtain the upper bound by
noting that the quadratic hull of a weighted-`r norm ball for r 2 [1  2] is the weighted-`2 norm ball.
The dual norm of (g) = k  gk2 being ⇤(g) = kg/k2  the upper bound holds by duality. See
Appendix C.3 for the (short) precise proof.
Theorem 1 and Corollary 2 show that for a large collection of norms  on the gradients  diagonally-
scaled gradient methods is minimax rate optimal. Arguing that diagonally-scaled gradient methods
are minimax rate optimal when  is neither a weighted-`r norm nor induces a quadratically convex
unit ball remains an open question  though weighted-`r norms for r 2 [1 1] cover the majority of
practical applications of stochastic gradient methods.
We conclude this section by generalizing our results to constraint sets that are rotations of orthosym-
metric and quadratically convex sets. This is for example the case when features are sparse in an
appropriate basis (e.g. wavelets [17]). Unsurprisingly  methods of linear type retain their optimality
properties.
Corollary 3. Let ⇥0 be a compact  orthosymmetric  convex and quadratically convex set. Let
U 2O n(R) be a rotation matrix and ⇥:= U ⇥0 = {U✓ | ✓ 2 ⇥0}. Consider the collection

F := {F : Rd ⇥X ! R 8x 2X  8✓ 2 Rd 8g 2 @✓f (✓  x)  (U T g)  1}.

A method of linear type is minimax rate optimal for the pair (⇥ F).
We present the proof in Appendix C.4.

4 Beyond quadratic convexity – the necessity of non-linear methods
For ⇥ ⇢ Rd quadratically convex  the results in Section 3 show that methods of linear type achieve
optimal rates of convergence. When the constraint set is not quadratically convex  it is unclear
whether methods of linear type are sufﬁcient to achieve optimal rates. As we now show  they are not:
we exhibit a collection of problem instances where the constraint set is orthosymmetric  compact 
and convex but not quadratically convex. On such problems  the constraint set has substantial
consequences; for some non-quadratically convex sets ⇥  methods of linear type (e.g. the stochastic
gradient method) can be minimax rate-optimal  while for other constraint sets  all methods of linear

type must have regret at least a factorpd/ log d worse than the minimax optimal rate  which
(non-linear) mirror descent with appropriate distance generating function achieves.
To construct these problem instances  we turn to simple non-quadratically convex constraint sets: `p
balls for p 2 [1  2]. We measure subgradient norms in the dual `p⇤ norm  p⇤ = p1
p . Our analysis
consists of two steps: we ﬁrst prove sharp minimax rates on these problem instances and show
that mirror descent with the right (non-linear) distance generating function is minimax rate optimal.
These results extend those of Agarwal et al. [1]  who provide matching lower and upper bounds for
p  1 + c for a ﬁxed numerical constant c > 0. In contrast  we prove sharp minimax rates for all
p  1. To precisely characterize the gap between linear and non-linear methods  we show that for any
linear pre-conditioner  we can exhibit functions for which the regret of Euclidean gradient methods
2 k✓k2
is nearly the simple upper regret bound of standard gradient methods  Eq. (5) with h(✓) = 1
2.
Thus  when p is very close to 2 (nearly quadratically convex)  the gap remains within a constant

factor  whereas when p is close to 1  the gap can be as large aspd/ log d.
4.1 Minimax rates for p-norm constraint sets  p 2 [1  2]
For p 2 [1  2]  we consider the constraint set ⇥= Bp(0  1) and bound gradients with norm  = k·k p⇤.
We begin by proving sharp minimax rates on this collection of problems and show that  in these cases 
non-linear mirror descent is minimax optimal.

7

n . MS
Mirror descent (4) with h(✓) := 1

Theorem 3. Let p 2 [1  2]  ⇥= Bp(0  1) and  = k·k p⇤.
(i) If 1  p  1 + 1/ log(2d)  then
1 ^q log(2d)
(ii) If 1 + 1/ log(2d) < p  2  then
1 ^q 1

n(⇥  )  MR

n(⇥  )  MR

2(a1)k✓k2

Mirror descent with h(✓) := 1

n(p1) . MS
2(p1)k✓k2

n(⇥  ) . 1 ^q log(2d)

n

.

a for a = 1 + 1

log(2d) achieves the optimal rate.

n(⇥  ) . 1 ^q 1

n(p1) .

p achieves the optimal rate.

To prove the theorem  we upper bound the regret of mirror descent with norm-based distance
generating functions (cf. [24  Corollary 2.18])  which follows immediately from the regret bound (5).
Proposition 5. Let ⇥ be closed convex   a norm  and 1 < a  2  a⇤ = a
a1. Mirror descent with
sup✓2⇥ k✓✓0ka
distance generating function h(✓) := 1
achieves
regret

pn supg2B (0 1) kgka⇤

a and stepsize ↵ =

2(a1)k✓k2
sup✓2⇥ k✓ka supg2B (0 1) kgka⇤

.

MR

n(⇥  ) 

pn(a  1)

We present the full proof of Theorem 3 in Appendix D.1. We obtain the lower bound with the familiar
reduction from estimation to testing and Assouad’s method (see Appendix A.2).

4.2 Exhibiting hard problems for Euclidean gradient methods
Theorem 3 shows that (non-linear) mirror descent methods are minimax rate-optimal for `p-ball con-
straint sets  p 2 [1  2]  with gradients contained in the corresponding dual `p⇤-norm ball (p⇤ = p
p1).
For problems and p  standard subgradient methods achieve worst-case regret O(d1/21/p⇤/pn).
This is sharp: in the next theorem  we show that for any method of linear type  we can construct a
sequence of (linear) functions such that the method’s regret is at least this familiar upper bound of
standard subgradient methods  precisely quantifying the gap between linear and non-linear methods
for this problem class.

Theorem 4. Let Regretn A(✓) =Pn
descent method with distance generating function hA(✓) = 1
For any A ⌫ 0 and p 2 [1  2] with q = p
and point ✓ 2 Rd with k✓kp  1 such that
1
Regretn A(✓) 
2

i=1 g>i (✓i ✓) denote the regret of the (Euclidean) online mirror
2 ✓>A✓ for linear functions Fi(✓) = g>i ✓.
p1  there exists a sequence of vectors gi 2 Rd  kgikq  1 
minnn/2 p2n · d1/21/qo .

We provide the proof in Appendix D.2. These results explicitly exhibit a gap between methods
of linear type and non-linear mirror descent methods for this problem class. In contrast to the
frequent practice in literature of simply comparing regret upper bounds—prima facie illogical—we
demonstrate the gap indeed must hold.
In combination with Theorem 4  Proposition 5 precisely characterizes the gap between linear and
non-linear mirror descent on these problems for all values of p 2 [1  2]. Indeed  when p = 1  for
any pre-conditioner A  there exists a problem on which Euclidean gradient methods has regret at
least ⌦(1)pd/n. On the same problem  non-linear mirror descent has regret at most O(1)plog d/n 
showing the advertisedpd/ log d gap. When p  2  1/ log d (so ⇥ is nearly quadratically convex) 

the gap reduces to at most a constant factor.

5 The need for adaptive methods

We have so far demonstrated that diagonal re-scaling is sufﬁcient to achieve minimax optimal rates
for problems over quadratically convex constraint sets. In practice  however  it is often the case

8

that we do not know the geometry of the problem in advance  precluding selection of the optimal
linear pre-conditioner. To address this problem  adaptive gradient methods choose  at each step 
a (usually diagonal) matrix ⇤i conditional on the subgradients observed thus far  {gl}li. The
algorithm then updates the iterate based on the distance generating function hi(✓) := 1
2 ✓>⇤i✓. In
this section  we present a problem instance showing that when the “scale” of the subgradients varies
across dimensions  adaptive gradient methods are crucial to achieve low regret. While there exists an
optimal pre-conditioner  if we do not assume knowledge of the geometry in advance  AdaGrad [13]
achieves the minimax optimal regret while standard (non-adaptive) subgradient methods can be pd
suboptimal on the same problem.
We consider the following setting: ⇥= B1(0  1) and (g) = k  gk1  for an arbitrary  2
Rd   0. Intuitively  j corresponds to the “scale” of the j-th dimension. On this problem  a
straightforward optimization of the regret bound (5) guarantees that stochastic gradient methods
achieve regret pdn/ minj j. We exhibit a problem instance (in Appendix E) such that  for any
stepsize ↵  online gradient descent attains this worst-case regret.
Theorem 5. Let Regretn ↵(✓) =Pin g>i (✓i  ✓) denote the regret of the online gradient descent
method with stepsize ↵  0 for linear functions Fi(✓) = g>i ✓. For any choice of ↵  0 and   0 
there exists a sequence of vectors {gi}in ⇢ Rd  (gi)  1 and point ✓ 2 ⇥ such that

Regretn ↵(✓) 

1
2

min( dn

2kk1

 

p2dn

minjd j) .

In contrast  AdaGrad [13] achieves regret pnk1/k2  demonstrating suboptimality gap as large as
pd for some choices of . Indeed  let Regretn AdaGrad(✓) be the regret of AdaGrad. Then

(see [13  Corollary 6])  and by Cauchy-Schwarz 

g2
i j.

Regretn AdaGrad(✓)  2p2XjdsXin
i j  k1/k2sXin
jsXin

2
j g2

1

XjdsXin

g2

i j =Xjd

k  gik2

2  pnk1/k2 .

To concretely consider different scales across dimensions  we choose j = j. Theorem 5
guarantees that there exists a collection of linear functions such that stochastic gradient meth-
ods suffer regret ⌦(1)pdn. Given that k1/k2  p⇣(2)  ⇡/p6  AdaGrad achieves regret
O(1)pn—amounting to a suboptimality gap of order pd—exhibiting the need for adaptivity.
This pd gap is also the largest possible over subgradient methods  which may achieve regret
qdPin kgik2
i j for ⇥= B1(0  1). Finally  we note in passing that Ada-
Grad is minimax optimal on this class of problems via a straightforward application of Theorem 1.

2  pdPjdqPin g2

6 Discussion

In this paper  we provide concrete recommendations for when one should use adaptive  mirror or
standard gradient methods depending on the geometry of the problem. While we emphasize the
importance of adaptivity  the picture is not fully complete: for example  in the case of quadratically
convex constraint sets  while the best diagonal pre-conditioner achieves optimal rates  the extent
to which adaptive gradient algorithms ﬁnd this optimal pre-conditioner remains an open question.
Another avenue to explore involves the many ﬂavors of adaptivity—while the minimax framework
assumes knowledge of the problem setting (e.g. a bound on the domain or the gradient norms)  it is
often the case that such parameters are unknown to the practitioner. To what extent can adaptivity
mitigate this and achieve optimal rates  and is minimax (i.e. worst-case) optimality truly the right
measure of performance? Finally  we close with a parting message about the value and costs of
adaptive and related methods. One should turn to adaptive gradient methods (at most) in settings
where methods of linear type are optimal. It is as our mothers told us when we were children: if you
want steak  don’t order chicken.

9

Acknowledgments This work was supported by NSF-CAREER Award 1553086  ONR-YIP
N00014-19-1-2288  and the Stanford DAWN Project. We thank Aditya Grover  Annie Marsden and
Hongseok Namkoong for valuable comments on the draft as well as Quentin Guignard for pointing
us to the Banach-Mazur distance for Theorem 4.

References
[1] A. Agarwal  P. L. Bartlett  P. Ravikumar  and M. J. Wainwright. Information-theoretic lower
bounds on the oracle complexity of convex optimization. IEEE Transactions on Information
Theory  58(5):3235–3249  2012.

[2] P. Assouad. Deux remarques sur l’estimation. Comptes Rendus des Séances de l’Académie des

Sciences  Série I  296(23):1021–1024  1983.

[3] P. L. Bartlett  E. Hazan  and A. Rakhlin. Adaptive online gradient descent. In Advances in

Neural Information Processing Systems 20  2007.

[4] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for

convex optimization. Operations Research Letters  31:167–175  2003.

[5] L. Bottou  F. Curtis  and J. Nocedal. Optimization methods for large-scale learning. SIAM

Review  60(2):223–311  2018.

[6] N. Cesa-Bianchi and G. Lugosi. Prediction  Learning  and Games. Cambridge University Press 

2006.

[7] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of on-line learning

algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  September 2004.

[8] T. M. Cover and J. A. Thomas. Elements of Information Theory  Second Edition. Wiley  2006.

[9] A. Cutcosky and T. Sarlos. Matrix-free preconditioning in online learning. In Proceedings of

the 36th International Conference on Machine Learning  2019.

[10] D. L. Donoho  R. C. Liu  and B. MacGibbon. Minimax risk over hyperrectangles  and implica-

tions. Annals of Statistics  18(3):1416–1437  1990.

[11] J. C. Duchi. Introductory lectures on stochastic convex optimization. In The Mathematics of

Data  IAS/Park City Mathematics Series. American Mathematical Society  2018.

[12] J. C. Duchi. Information theory and statistics. Lecture Notes for Statistics 311/EE 377  Stanford
University  2019. URL http://web.stanford.edu/class/stats311/lecture-notes.
pdf. Accessed May 2019.

[13] J. C. Duchi  E. Hazan  and Y. Singer. Adaptive subgradient methods for online learning and

stochastic optimization. Journal of Machine Learning Research  12:2121–2159  2011.

[14] J. C. Duchi  M. I. Jordan  and H. B. McMahan. Estimation  optimization  and parallelism when

data is sparse. In Advances in Neural Information Processing Systems 26  2013.

[15] C. Gentile. The robustness of the p-norm algorithms. Machine Learning  53(3):265–299  2003.

[16] J. Hiriart-Urruty and C. Lemaréchal. Convex Analysis and Minimization Algorithms I. Springer 

New York  1993.

[17] S. Mallat. A Wavelet Tour of Signal Processing: The Sparse Way (Third Edition). Academic

Press  2008.

[18] A. Nemirovski and D. Yudin. Problem Complexity and Method Efﬁciency in Optimization.

Wiley  1983.

[19] A. Nemirovski  A. Juditsky  G. Lan  and A. Shapiro. Robust stochastic approximation approach

to stochastic programming. SIAM Journal on Optimization  19(4):1574–1609  2009.

10

[20] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Program-

ming  120(1):261–283  2009.

[21] F. Orabona and K. Crammer. New adaptive algorithms for online classiﬁcation. In Advances in

Neural Information Processing Systems 23  2010.

[22] H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical

Statistics  22:400–407  1951.

[23] S. Shalev-Shwartz. Online Learning: Theory  Algorithms  and Applications. PhD thesis  The

Hebrew University of Jerusalem  2007.

[24] S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in

Machine Learning  4(2):107–194  2012.

[25] M. Sion. On general minimax theorems. Paciﬁc Journal of Mathematics  8(1):171–176  1958.
[26] R. Vershynin. Lectures in geometric functional analysis. Unpublished manuscript  2009. URL

https://www.math.uci.edu/~rvershyn/papers/GFA-book.pdf.

[27] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge

University Press  2019.

[28] A. C. Wilson  R. Roelofs  M. Stern  N. Srebro  and B. Recht. The marginal value of adaptive
gradient methods in machine learning. In Advances in Neural Information Processing Systems
30  2017.

[29] B. Yu. Assouad  Fano  and Le Cam.

Springer-Verlag  1997.

In Festschrift for Lucien Le Cam  pages 423–435.

11

,Daniel Levy
John Duchi