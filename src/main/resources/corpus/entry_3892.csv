2011,On Learning Discrete Graphical Models using Greedy Methods,In this paper  we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency  or consistency in sparsity pattern recovery  properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case  we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result  we derive sufficient conditions on the number of samples n  the maximum node-degree d and the problem size p  as well as other conditions on the model parameters  so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d log(p))  in contrast to existing convex-optimization based algorithms that require a sample complexity of Omega(d^2 log(p)). Further  the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.,On Learning Discrete Graphical Models Using

Greedy Methods

Ali Jalali

University of Texas at Austin
alij@mail.utexas.edu

Christopher C. Johnson

University of Texas at Asutin

cjohnson@cs.utexas.edu

Pradeep Ravikumar

University of Texas at Asutin

pradeepr@cs.utexas.edu

Abstract

In this paper  we address the problem of learning the structure of a pairwise graph-
ical model from samples in a high-dimensional setting. Our ﬁrst main result stud-
ies the sparsistency  or consistency in sparsity pattern recovery  properties of a
forward-backward greedy algorithm as applied to general statistical models. As
a special case  we then apply this algorithm to learn the structure of a discrete
graphical model via neighborhood estimation. As a corollary of our general result 
we derive sufﬁcient conditions on the number of samples n  the maximum node-
degree d and the problem size p  as well as other conditions on the model param-
eters  so that the algorithm recovers all the edges with high probability. Our result
guarantees graph selection for samples scaling as n = Ω(d2 log(p))  in contrast to
existing convex-optimization based algorithms that require a sample complexity
of Ω(d3 log(p)). Further  the greedy algorithm only requires a restricted strong
convexity condition which is typically milder than irrepresentability assumptions.
We corroborate these results using numerical simulations at the end.

1 Introduction
Undirected graphical models  also known as Markov random ﬁelds  are used in a variety of domains 
including statistical physics  natural language processing and image analysis among others. In this
paper we are concerned with the task of estimating the graph structure G of a Markov random ﬁeld
(MRF) over a discrete random vector X = (X1  X2  . . .   Xp)  given n independent and identically
distributed samples {x(1)  x(2)  . . .   x(n)}. This underlying graph structure encodes conditional in-
dependence assumptions among subsets of the variables  and thus plays an important role in a broad
range of applications of MRFs.

Existing approaches: Neighborhood Estimation  Greedy Local Search. Methods for estimating such
graph structure include those based on constraint and hypothesis testing [22]  and those that estimate
restricted classes of graph structures such as trees [8]  polytrees [11]  and hypertrees [23]. A recent
class of successful approaches for graphical model structure learning are based on estimating the lo-
cal neighborhood of each node. One subclass of these for the special case of bounded degree graphs
involve the use of exhaustive search so that their computational complexity grows at least as quickly
as O(pd)  where d is the maximum neighborhood size in the graphical model [1  4  9]. Another
subclass use convex programs to learn the neighborhood structure: for instance [20  17  16] estimate
the neighborhood set for each vertex r ∈ V by optimizing its ℓ1-regularized conditional likelihood;
[15  10] use ℓ1/ℓ2-regularized conditional likelihood. Even these methods  however need to solve
regularized convex programs with typically polynomial computational cost of O(p4) or O(p6)  are
still expensive for large problems. Another popular class of approaches are based on using a score
metric and searching for the best scoring structure from a candidate set of graph structures. Ex-

1

act search is typically NP-hard [7]; indeed for general discrete MRFs  not only is the search space
intractably large  but calculation of typical score metrics itself is computationally intractable since
they involve computing the partition function associated with the Markov random ﬁeld [26]. Such
methods thus have to use approximations and search heuristics for tractable computation. Question:
Can one use local procedures that are as inexpensive as the heuristic greedy approaches  and yet
come with the strong statistical guarantees of the regularized convex program based approaches?

High-dimensional Estimation; Greedy Methods. There has been an increasing focus in recent years
on high-dimensional statistical models where the number of parameters p is comparable to or even
larger than the number of observations n. It is now well understood that consistent estimation is pos-
sible even under such high-dimensional scaling if some low-dimensional structure is imposed on the
model space. Of relevance to graphical model structure learning is the structure of sparsity  where
a sparse set of non-zero parameters entail a sparse set of edges. A surge of recent work [5  12]
has shown that ℓ1-regularization for learning such sparse models can lead to practical algorithms
with strong theoretical guarantees. A line of recent work (cf. paragraph above) has thus leveraged
this sparsity inducing nature of ℓ1-regularization  to propose and analyze convex programs based on
regularized log-likelihood functions. A related line of recent work on learning sparse models has
focused on “stagewise” greedy algorithms. These perform simple forward steps (adding parameters
greedily)  and possibly also backward steps (removing parameters greedily)  and yet provide strong
statistical guarantees for the estimate after a ﬁnite number of greedy steps. The forward greedy vari-
ant which performs just the forward step has appeared in various guises in multiple communities: in
machine learning as boosting [13]  in function approximation [24]  and in signal processing as basis
pursuit [6]. In the context of statistical model estimation  Zhang [28] analyzed the forward greedy
algorithm for the case of sparse linear regression; and showed that the forward greedy algorithm is
sparsistent (consistent for model selection recovery) under the same “irrepresentable” condition as
that required for “sparsistency” of the Lasso. Zhang [27] analyzes a more general greedy algorithm
for sparse linear regression that performs forward and backward steps  and showed that it is spar-
sistent under a weaker restricted eigenvalue condition. Here we ask the question: Can we provide
an analysis of a general forward backward algorithm for parameter estimation in general statistical
models? Speciﬁcally  we need to extend the sparsistency analysis of [28] to general non-linear mod-
els  which requires a subtler analysis due to the circular requirement of requiring to control the third
order terms in the Taylor series expansion of the log-likelihood  that in turn requires the estimate to
be well-behaved. Such extensions in the case of ℓ1-regularization occur for instance in [20  25  3].
Our Contributions. In this paper  we address both questions above. In the ﬁrst part  we analyze the
forward backward greedy algorithm [28] for general statistical models. We note that even though we
consider the general statistical model case  our analysis is much simpler and accessible than [28] 
and would be of use even to a reader interested in just the linear model case of Zhang [28]. In the
second part  we use this to show that when combined with neighborhood estimation  the forward
backward variant applied to local conditional log-likelihoods provides a simple computationally
tractable method that adds and deletes edges  but comes with strong sparsistency guarantees. We
reiterate that the our ﬁrst result on the sparsistency of the forward backward greedy algorithm for
general objectives is of independent interest even outside the context of graphical models. As we
show  the greedy method is better than the ℓ1-regularized counterpart in [20] theoretically  as well
as experimentally. The sufﬁcient condition on the parameters imposed by the greedy algorithm
is a restricted strong convexity condition [19]  which is weaker than the irrepresentable condition
required by [20]. Further  the number of samples required for sparsistent graph recovery scales as
O(d2 log p)  where d is the maximum node degree  in contrast to O(d3 log p) for the ℓ1-regularized
counterpart. We corroborate this in our simulations  where we ﬁnd that the greedy algorithm requires
fewer observations than [20] for sparsistent graph recovery.

2 Review  Setup and Notation
2.1 Markov Random Fields
Let X = (X1  . . .   Xp) be a random vector  each variable Xi taking values in a discrete set X
of cardinality m. Let G = (V  E) denote a graph with p nodes  corresponding to the p variables
{X1  . . .   Xp}. A pairwise Markov random ﬁeld over X = (X1  . . .   Xp) is then speciﬁed by
nodewise and pairwise functions θr : X 7→ R for all r ∈ V   and θrt : X ×X 7→ R for all (r  t) ∈ E:
(1)

P(x) ∝ exp(cid:8)Xr∈V

θr(xr) + X(r t)∈E

θrt(xr  xt)(cid:9).

2

In this paper  we largely focus on the case where the variables are binary with X = {−1  +1} 
where we can rewrite (1) to the Ising model form [14] for some set of parameters {θr} and {θrt} as
(2)

P(x) ∝ exp(cid:8)Xr∈V

θrxr + X(r t)∈E

θrtxrxt(cid:9).

2.2 Graphical Model Selection

Let D := {x(1)  . . .   x(n)} denote the set of n samples  where each p-dimensional vector x(i) ∈
{1  . . .   m}p is drawn i.i.d. from a distribution Pθ∗ of the form (1)  for parameters θ∗ and graph
G = (V  E∗) over the p variables. Note that the true edge set E∗ can also be expressed as a function
of the parameters as

E∗ = {(r  t) ∈ V × V : θ∗st 6= 0}.

(3)

The graphical model selection task consists of inferring this edge set E∗ from the samples D. The
goal is to construct an estimator ˆEn for which P[ ˆEn = E∗] → 1 as n → ∞. Denote by N ∗(r)
the set of neighbors of a vertex r ∈ V   so that N ∗(r) = {t : (r  t) ∈ E∗}. Then the graphical
model selection problem is equivalent to that of estimating the neighborhoods ˆNn(r) ⊂ V   so that
P[ ˆNn(r) = N ∗(r);∀r ∈ V ] → 1 as n → ∞.
For any pair of random variables Xr and Xt  the parameter θrt fully characterizes whether there is
an edge between them  and can be estimated via its conditional likelihood. In particular  deﬁning
Θr := (θr1  . . .   θrp)  our goal is to use the conditional likelihood of Xr conditioned on XV \r to
estimate Θr and hence its neighborhood N (r). This conditional distribution of Xr conditioned on
XV \r generated by (2) is given by the logistic model

Given the n samples D  the corresponding conditional log-likelihood is given by

P(cid:16)Xr = xr(cid:12)(cid:12)(cid:12)XV \r = xV \r(cid:17) =

log1+ expθrx(i) +Xt∈V \r
nXi=1

.

exp(θrxr +Pt∈V \r θrtxrxt)
1 + exp(θr +Pr∈V \r θrtxr)
t −θrx(i)
r −Xt∈V \r

θrtx(i)

r x(i)

θrtx(i)

r x(i)

.

(4)

t 

L(Θr; D) =

1
n

In Section 4  we study a greedy algorithm (Algorithm 2) that ﬁnds these node neighborhoods

ˆNn(r) = Supp(bΘr) of each random variable Xr separately by a greedy stagewise optimization
neighborhoods to obtain a graph estimate bE using an “OR” rule: bEn = ∪r{(r  t) : t ∈ ˆNn(r)}.

of the conditional log-likelihood of Xr conditioned on XV \r. The algorithm then combines these
Other rules such as the “AND” rule  that add an edge only if it occurs in each of the respective node
neighborhoods  could be used to combine the node-neighborhoods to a graph estimate. We show
in Theorem 2 that the neighborhood selection by the greedy algorithm succeeds in recovering the
exact node-neighborhoods with high probability  so that by a union bound  the graph estimates using
either the AND or OR rules would be exact with high probability as well.

Before we describe this greedy algorithm and its analysis in Section 4 however  we ﬁrst consider
the general statistical model case in the next section. We ﬁrst describe the forward backward greedy
algorithm of Zhang [28] as applied to general statistical models  followed by a sparsistency analysis
for this general case. We then specialize these general results in Section 4 to the graphical model
case. The next section is thus of independent interest even outside the context of graphical models.

3 Greedy Algorithm for General Losses

1 := {Z1  . . .   Zn} denote n obser-
Consider a random variable Z with distribution P  and let Z n
vations drawn i.i.d. according to P. Suppose we are interested in estimating some parameter
θ∗ ∈ Rp of the distribution P that is sparse; denote its number of non-zeroes by s∗ := kθ∗k0.
Let L : Rp × Z n 7→ R be some loss function that assigns a cost to any parameter θ ∈ Rp  for a
given set of observations Z n
1 . For ease of notation  in the sequel  we adopt the shorthand L(θ) for
L(θ; Z n

1 ). We assume that θ∗ satisﬁes EZ [∇L(θ∗)] = 0.

3

Algorithm 1 Greedy forward-backward algorithm for ﬁnding a sparse optimizer of L(·)
Input: Data D := {x(1)  . . .   x(n)}  Stopping Threshold ǫS  Backward Step Factor ν ∈ (0  1)
Output: Sparse optimizerbθ
bθ(0) ←− 0 and bS(0) ←− φ and k ←− 1
while true do {Forward Step}
; αL(bθ(k−1) +αej ; D)
j∈( bS(k−1))c
bS(k) ←− bS(k−1) ∪ {j∗}
f ←− L(bθ(k−1); D) − L(bθ(k−1) + α∗ej∗ ; D)
f ≤ ǫS then
break

(j∗  α∗) ←− arg

δ(k)
if δ(k)

min

end if

θ L(cid:0)θ bS(k) ; D(cid:1)
bθ(k) ←− arg min
k ←− k + 1
while true do {Backward Step}
j∗ ←− arg min

j

j∗

ej; D)

end if

break

j∈ bS(k−1) L(bθ(k−1) −bθ(k−1)

if L(cid:0)bθ(k−1) −bθ(k−1)
bS(k−1) ←− bS(k) − {j∗}
bθ(k−1) ←− arg min

ej∗ ; D(cid:1) − L(cid:0)bθ(k−1); D(cid:1) > νδ(k)
θ L(cid:0)θ bS(k−1) ; D(cid:1)

k ←− k − 1

f

end while

then

end while

We now consider the forward backward greedy algorithm in Algorithm 1 that rewrites the algorithm
in [27] to allow for general loss functions. The algorithm starts with an empty set of active variables

bS(0) and gradually adds (and removes) vairables to the active set until it meets the stopping criterion.

This algorithm has two major steps: the forward step and the backward step. In the forward step 
the algorithm ﬁnds the best next candidate and adds it to the active set as long as it improves the loss
function at least by ǫS  otherwise the stopping criterion is met and the algorithm terminates. Then 
in the backward step  the algorithm checks the inﬂuence of all variables in the presence of the newly
added variable. If one or more of the previously added variables do not contribute at least νǫS to
the loss function  then the algorithm removes them from the active set. This procedure ensures that
at each round  the loss function is improved by at least (1 − ν)ǫS and hence it terminates within a
ﬁnite number of steps.

We state the assumptions on the loss function such that sparsistency is guaranteed. Let us ﬁrst recall
the deﬁnition of restricted strong convexity from Negahban et al. [18]. Speciﬁcally  for a given set S 
the loss function is said to satisfy restricted strong convexity (RSC) with parameter κl with respect
to the set S if

L(θ + ∆; Z n

1 ) − L(θ; Z n

1 ) − h∇L(θ; Z n

1 )  ∆i ≥

κl
2 k∆k2

2

for all ∆ ∈ S.

(5)

We can now deﬁne sparsity restricted strong convexity as follows. Speciﬁcally  we say that the
loss function L satisﬁes RSC(k) with parameter κl if it satisﬁes RSC with parameter κl for the set
{∆ ∈ Rp : k∆k0 ≤ k}.
In contrast  we say that the loss function satisﬁes restricted strong smoothness (RSS) with parameter
κu with respect to a set S if

L(θ + ∆; Z n

1 ) − L(θ; Z n

1 ) − h∇L(θ; Z n

1 )  ∆i ≤

κu
2 k∆k2

2

for all ∆ ∈ S.

4

We can deﬁne RSS(k) similarly. The loss function L satisﬁes RSS(k) with parameter κu if it
satisﬁes RSS with parameter κu for the set {∆ ∈ Rp : k∆k0 ≤ k}. Given any constants κl and κu 
and a sample based loss function L  we can typically use concentration based arguments to obtain
bounds on the sample size required so that the RSS and RSC conditions hold with high probability.
Another property of the loss function that we require is an upper bound λn on the ℓ∞ norm of the
gradient of the loss at the true parameter θ∗  i.e.  λn ≥ k∇L(θ∗)k∞. This captures the “noise level”
of the samples with respect to the loss. Here too  we can typically use concentration arguments to
show for instance that λn ≤ cn(log(p)/n)1/2  for some constant cn > 0 with high probability.
Theorem 1 (Sparsistency). Suppose the loss function L(·) satisﬁes RSC (η s∗) and RSS (η s∗)
with parameters κl and κu for some η ≥ 2 + 4ρ2(p(ρ2 − ρ)/s∗ + √2)2 with ρ = κu/κl. Moreover 
suppose that the true parameters θ∗ satisfy minj∈S ∗ |θ∗j| >p32ρǫS/κl. Then if we run Algorithm 1
with stopping threshold ǫS ≥ (8ρη/κl) s∗λ2

n  the outputbθ with support bS satisﬁes:
√s∗ (λn√η + √ǫS

√2κu).

κl

(a) Error Bound: kbθ − θ∗k2 ≤ 2
(b) No False Exclusions: S∗ −bS = ∅.
(c) No False Inclusions: bS − S∗ = ∅.

Proof. The proof theorem hinges on three main lemmas: Lemmas 1 and 2 which are simple conse-
quences of the forward and backward steps failing when the greedy algorithm stops  and Lemma 3
which uses these two lemmas and extends techniques from [21] and [19] to obtain an ℓ2 error bound
on the error. Provided these lemmas hold  we then show below that the greedy algorithm is sparsis-
tent. However  these lemmas require apriori that the RSC and RSS conditions hold for sparsity size

Lemmas 1  2 and Lemma 3 to complete the proof as detailed below.

|S∗ ∪ bS|. Thus  we use the result in Lemma 4 that if RSC(ηs∗) holds  then the solution when the
algorithm terminates satisﬁes |bS| ≤ (η − 1)s∗  and hence |bS ∪ S∗| ≤ ηs∗. Thus  we can then apply
(a) The result follows directly from Lemma 3  and noting that |bS ∪ S∗| ≤ ηs∗. In this Lemma  we
show that the upper bound holds by drawing from ﬁxed point techniques in [21] and [19]  and by
using a simple consequence of the forward step failing when the greedy algorithm stops.
(b) We follow the chaining argument in [27]. For any τ ∈ R  we have
2 ≤ kθ∗ −bθk2

τ|{j ∈ S∗ − bS : |θ∗

S ∗− bSk2
8ηs∗λ2
n
+

j |2 > τ}| ≤ kθ∗

16κuǫS

2

≤

κ2
l

κ2
l

where the last inequality follows from part (a) and the inequality (a + b)2 ≤ 2a2 + 2b2. Now 
setting τ = 32κuǫS

  and dividing both sides by τ /2 we get

κ2

l

|S∗ −bS| 

ηs∗λ2
n
2κuǫS

j |2 > τ}| ≤

2|{j ∈ S∗ − bS : |θ∗
j |2 ≤ τ}| +

+ |S∗ −bS|.
Substituting |{j ∈ S∗ −bS : |θ∗
j|2 > τ}| = |S∗ − bS| − |{j ∈ S∗ −bS : |θ∗
2κuǫS ≤ |{j ∈ S∗ − bS : |θ∗
due to the setting of the stopping threshold ǫS. This in turn entails that
|S∗ −bS| ≤ |{j ∈ S∗ −bS : |θ∗j|2 ≤ τ}| = 0 

|S∗ − bS| ≤ |{j ∈ S∗ − bS : |θ∗

by our assumption on the size of the minimum entry of θ∗.

ηs∗λ2
n

j|2 ≤ τ}|  we get

j |2 ≤ τ}| + 1/2 

(c) From Lemma 2  which provides a simple consequence of the backward step failing when the
2  so that
≤ 1/2  due to the

greedy algorithm stops  for b∆ =bθ − θ∗  we have ǫS/κu|bS − S∗| ≤ kb∆ bS−S ∗k2
using Lemma 3 and that |S∗ − bS| = 0  we obtain that |bS − S∗| ≤ 4ηs

setting of the stopping threshold ǫS.

2 ≤ kb∆k2

λ2
ǫS κ2

nκu

∗

l

5

Algorithm 2 Greedy forward-backward algorithm for pairwise discrete graphical model learning
Input: Data D := {x(1)  . . .   x(n)}  Stopping Threshold ǫS  Backward Step Factor ν ∈ (0  1)
Output: Estimated Edges bE
for r ∈ V do
Run Algorithm 1 with the loss L(·) set as in (4)  to obtain bΘr with support cNr
end for
Output bE =Srn(r  t) : t ∈ cNro

3.1 Lemmas for Theorem 1

We list the simple lemmas that characterize the solution obtained when the algorithm terminates 
and on which the proof of Theorem 1 hinges.

we have

(cid:12)(cid:12)(cid:12)L(cid:16)bθ(cid:17) − L (θ∗)(cid:12)(cid:12)(cid:12) <q2|S∗ −bS| κu ǫS (cid:13)(cid:13)(cid:13)bθ − θ∗(cid:13)(cid:13)(cid:13)2

Lemma 1 (Stopping Forward Step). When the algorithm 1 stops with parameterbθ supported on bS 
Lemma 2 (Stopping Backward Step). When the algorithm 1 stops with parameterbθ supported on
bS  we have
Lemma 3 (Stopping Error Bound). When the algorithm 1 stops with parameterbθ supported on bS 

we have

ǫS

.

2

2 ≥

κu(cid:12)(cid:12)(cid:12)bS − S∗(cid:12)(cid:12)(cid:12) .

(cid:13)(cid:13)(cid:13)b∆ bS−S ∗(cid:13)(cid:13)(cid:13)
κl λnr(cid:12)(cid:12)(cid:12)S∗ ∪ bS(cid:12)(cid:12)(cid:12) +r2(cid:12)(cid:12)(cid:12)S∗ −bS(cid:12)(cid:12)(cid:12) κuǫS! .
κu (cid:16)q 2

η(cid:17)−2
η−1 −q 2

2

and RSC (ηs∗) holds for some η ≥ 2 +

(cid:13)(cid:13)(cid:13)bθ − θ∗(cid:13)(cid:13)(cid:13)2 ≤

Lemma 4 (Stopping Size). If ǫS > λ2

n

s∗ + √2(cid:19)2

  then the algorithm 1 stops with k ≤ (η − 1)s∗.

4ρ2(cid:18)q ρ2−ρ
Notice that if ǫS ≥ (8ρη/κl) (η2/(4ρ2)) λ2
n.
for large value of s∗ ≥ 8ρ2 > η2/(4ρ2)  it sufﬁces to have ǫS ≥ (8ρη/κl) s∗λ2
4 Greedy Algorithm for Pairwise Graphical Models

n  then  the assumption of this lemma is satisﬁed. Hence

Suppose we are given set of n i.i.d. samples D := {x(1)  . . .   x(n)}  drawn from a pairwise Ising
model as in (2)  with parameters θ∗  and graph G = (V  E∗). It will be useful to denote the maximum
node-degree in the graph E∗ by d. As we will show  our model selection performance depends
critically on this parameter d. We propose Algorithm 2 for estimating the underlying graphical
model from the n samples D.
Theorem 2 (Pairwise Sparsistency). Suppose we run Algorithm 2 with stopping threshold ǫS ≥
  where  d is the maximum node degree in the graphical model  and the true parameters θ∗
c1
satisfy c3√d

> minj∈S ∗ |θ∗j| > c2√ǫS  and further that number of samples scales as

d log p

n

n > c4 d2 log p 

for some constants c1  c2  c3  c4. Then  with probability at least 1 − c′ exp(−c′′n)  the output bθ
supported on bS satisﬁes:

(a) No False Exclusions: E∗ − bE = ∅.

6

(b) No False Inclusions: bE − E∗ = ∅.

Proof. This theorem is a corollary to our general Theorem 1. We ﬁrst show that the conditions of
Theorem 1 hold under the assumptions in this corollary.

RSC  RSS. We ﬁrst note that the conditional log-likelihood loss function in (4) corresponds to a lo-
gistic likelihood. Moreover  the covariates are all binary  and bounded  and hence also sub-Gaussian.
[19  2] analyze the RSC and RSS properties of generalized linear models  of which logistic models
are an instance  and show that the following result holds if the covariates are sub-Gaussian. Let
∂L(∆; θ∗) = L(θ∗ + ∆) − L(θ∗) − h∇L(θ∗)  ∆i be the second order Taylor series remainder.
Then  Proposition 2 in [19] states that that there exist constants κl
2  independent of n  p such
that with probability at least 1 − c1 exp(−c2n)  for some constants c1  c2 > 0 

1 and κl

∂L(∆; θ∗) ≥ κl

1k∆k2(k∆k2 − κl
n k∆k1)
2r log(p)
Thus  if k∆k0 ≤ k := ηd  then k∆k1 ≤ √kk∆k2  so that
n ! ≥
2 κl
2r k log p
∂L(∆; θ∗) ≥ k∆k2
if n > 4(κl
function L satisﬁes RSC(k) with parameter κl
follows from [19  2] that there exist constants κu
c′1 exp(−c′2n) 

1 − κl

2/κl

1)2 ηd log(p). In other words  with probability at least 1 − c1 exp(−c2n)  the loss
1)2 ηd log(p). Similarly  it
2 such that with probability at least 1 −

1 provided n > 4(κl

1 and κu

2/κl

for all ∆ : k∆k2 ≤ 1.

κl
1
2 k∆k2
2 

∂L(∆; θ∗) ≤ κu

for all ∆ : k∆k2 ≤ 1 

1k∆k2{k∆k2 − κu
2 /κu

2k∆k1}
1 )2 ηd log(p).

rt   where Z (i)

1 provided n > 4(κu

so that by a similar argument  with probability at least 1−c′1 exp(−c′2n)  the loss function L satisﬁes
RSS(k) with parameter κu
Noise Level. Next  we obtain a bound on the noiselevel λn ≥ k∇L(θ∗)k∞ following simi-
lar arguments to [20]. Let W denote the gradient ∇L(θ∗) of the loss function (4). Any en-
nPn
i=1 Z (i)
try of W has the form Wt = 1
\s )) are
and bounded |Z (i)
zero-mean  i.i.d.
rt | ≤ 1. Thus  an application of Hoeffding’s inequality
yields that P[|Wt| > δ] ≤ 2 exp(−2nδ2). Applying a union bound over indices in W   we get
P[kWk∞ > δ] ≤ 2 exp(−2nδ2 + log(p)). Thus  if λn = (log(p)/n)1/2  then kWk∞ ≤ λn with
probability at least 1 − exp(−nλ2
We can now verify that under the assumptions in the corollary  the conditions on the stopping size ǫS
and the minimum absolute value of the non-zero parameters minj∈S ∗ |θ∗j| are satisﬁed. Moreover 
from the discussion above  under the sample size scaling in the corollary  the required RSC and
RSS conditions hold as well. Thus  Theorem 1 yields that each node neighborhood is recovered
with no false exclusions or inclusions with probability at least 1 − c′ exp(−c′′n). An application of
a union bound over all nodes completes the proof.

r − P(xr = 1|x(i)

rt = x(i)

n + log(p)).

t (x(i)

Remarks. The sufﬁcient condition on the parameters imposed by the greedy algorithm is a restricted
strong convexity condition [19]  which is weaker than the irrepresentable condition required by
[20]. Further  the number of samples required for sparsistent graph recovery scales as O(d2 log p) 
where d is the maximum node degree  in contrast to O(d3 log p) for the ℓ1 regularized counterpart.
We corroborate this in our simulations  where we ﬁnd that the greedy algorithm requires fewer
observations than [20] for sparsistent graph recovery.

We also note that the result can also be extended to the general pairwise graphical model case  where
each random variable takes values in the range {1  . . .   m}. In that case  the conditional likelihood
of each node conditioned on the rest of the nodes takes the form of a multiclass logistic model  and
the greedy algorithm would take the form of a “group” forward-backward greedy algorithm  which
would add or remove all the parameters corresponding to an edge as a group. Our analysis however
naturally extends to such a group greedy setting as well. The analysis for RSC and RSS remains the
same and for bounds on λn  see equation (12) in [15]. We defer further discussion on this due to the
lack of space.

7

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
S

 
f

o
 
y
t
i
l
i

b
a
b
o
r
P

   Greedy
Algorithm

 

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
S

 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

   Greedy
Algorithm

  Logistic
Regression

p = 36
p = 64
p = 100

 

p = 36
p = 64
p = 100

  Logistic
Regression

 

0
0

0.5

1

1.5
2.5
Control Parameter

2

3

3.5

4

 

0
0

0.5

1

1.5
2.5
Control Parameter

2

3

3.5

4

(a) Chain (Line Graph)

(b) 4-Nearest Neighbor (Grid Graph)

1

0.8

0.6

0.4

0.2

s
s
e
c
c
u
S

 
f

o

 
y
t
i
l
i

b
a
b
o
r
P

 

0
0

  Greedy
Algorithm

 

  Logistic
Regression

p = 36
p = 64
p = 100

0.25

0.5

0.75

Control Parameter

1

1.25

1.51.5

(c) Star

(d) Chain  4-Nearest Neighbor and Star
Graphs

Fig 1: Plots of success probability P[bN±(r) = N ∗(r) ∀r ∈ V ] versus the control parameter
β(n  p  d) = n/[20d log(p)] for Ising model on (a) chain (d = 2)  (b) 4-nearest neighbor (d = 4)
and (c) Star graph (d = 0.1p). The coupling parameters are chosen randomly from θ∗st = ±0.50
for both greedy and node-wise ℓ1-regularized logistic regression methods. As our theorem suggests
and these ﬁgures show  the greedy algorithm requires less samples to recover the exact structure of
the graphical model.

5 Experimental Results

We now present experimental results that illustrate the power of Algorithm 2 and support our theo-
retical guarantees. We simulated structure learning of different graph structures and compared the
learning rates of our method to that of node-wise ℓ1-regularized logistic regression as outlined in
[20].

We performed experiments using 3 different graph structures: (a) chain (line graph)  (b) 4-nearest
neighbor (grid graph) and (c) star graph. For each experiment  we assumed a pairwise binary Ising
model in which each θ∗rt = ±1 randomly. For each graph type  we generated a set of n i.i.d.
samples {x(1)  ...  x(n)} using Gibbs sampling. We then attempted to learn the structure of the
model using both Algorithm 2 as well as node-wise ℓ1-regularized logistic regression. We then
compared the actual graph structure with the empirically learned graph structures.
If the graph
structures matched completely then we declared the result a success otherwise we declared the result
a failure. We compared these results over a range of sample sizes (n) and averaged the results for
each sample size over a batch of size 10. For all greedy experiments we set the stopping threshold
ǫS = c log(np)
  where c is a tuning constant  as suggested by Theorem 2  and set the backwards
step threshold ν = 0.5. For all logistic regression experiments we set the regularization parameter

λn = c′plog(p)/n  where c′ was set via cross-validation.
Figure 1 shows the results for the chain (d = 2)  grid (d = 4) and star (d = 0.1p) graphs using
both Algorithm 2 and node-wise ℓ1-regularized logistic regression for three different graph sizes
p ∈ {36  64  100} with mixed (random sign) couplings. For each sample size  we generated a batch
of 10 different graphical models and averaged the probability of success (complete structure learned)
over the batch. Each curve then represents the probability of success versus the control parameter
β(n  p  d) = n/[20d log(p)] which increases with the sample size n. These results support our
theoretical claims and demonstrate the efﬁciency of the greedy method in comparison to node-wise
ℓ1-regularized logistic regression [20].

n

6 Acknowledgements

We would like to acknowledge the support of NSF grant IIS-1018426.

8

References
[1] P. Abbeel  D. Koller  and A. Y. Ng. Learning factor graphs in polynomial time and sample complexity.

Jour. Mach. Learning Res.  7:1743–1788  2006.

[2] A. Agarwal  S. Negahban  and M. Wainwright. Convergence rates of gradient methods for high-

dimensional statistical recovery. In NIPS  2010.

[3] F. Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics  4:384–414 

2010.

[4] G. Bresler  E. Mossel  and A. Sly. Reconstruction of markov random ﬁelds from samples: Some easy

observations and algorithms. In RANDOM 2008.

[5] E. Candes and T. Tao. The Dantzig selector: Statistical estimation when p is much larger than n. Annals

of Statistics  2006.

[6] S. Chen  D. L. Donoho  and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM J. Sci.

Computing  20(1):33–61  1998.

[7] D. Chickering. Learning Bayesian networks is NP-complete. Proceedings of AI and Statistics  1995.
[8] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. IEEE Trans.

Info. Theory  14(3):462–467  1968.

[9] I. Csisz´ar and Z. Talata. Consistent estimation of the basic neighborhood structure of Markov random

ﬁelds. The Annals of Statistics  34(1):123–145  2006.

[10] C. Dahinden  M. Kalisch  and P. Buhlmann. Decomposition and model selection for large contingency

tables. Biometrical Journal  52(2):233–252  2010.

[11] S. Dasgupta. Learning polytrees. In Uncertainty on Artiﬁcial Intelligence  pages 134–14  1999.
[12] D. Donoho and M. Elad. Maximal sparsity representation via ℓ1 minimization. Proc. Natl. Acad. Sci. 

100:2197–2202  March 2003.

[13] J. Friedman  T. Hastie  and R. Tibshirani. Additive logistic regression: A statistical view of boosting.

Annals of Statistics  28:337–374  2000.

[14] E. Ising. Beitrag zur theorie der ferromagnetismus. Zeitschrift f¨ur Physik  31:253–258  1925.
[15] A. Jalali  P. Ravikumar  V. Vasuki  and S. Sanghavi. On learning discrete graphical models using group-

sparse regularization. In Inter. Conf. on AI and Statistics (AISTATS) 14  2011.

[16] S.-I. Lee  V. Ganapathi  and D. Koller. Efﬁcient structure learning of markov networks using l1-

regularization. In Neural Information Processing Systems (NIPS) 19  2007.

[17] N. Meinshausen and P. B¨uhlmann. High dimensional graphs and variable selection with the lasso. Annals

of Statistics  34(3)  2006.

[18] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional
In Neural Information Processing Systems

analysis of m-estimators with decomposable regularizers.
(NIPS) 22  2009.

[19] S. Negahban  P. Ravikumar  M. J. Wainwright  and B. Yu. A uniﬁed framework for high-dimensional

analysis of m-estimators with decomposable regularizers. In Arxiv  2010.

[20] P. Ravikumar  M. J. Wainwright  and J. Lafferty. High-dimensional ising model selection using ℓ1-

regularized logistic regression. Annals of Statistics  38(3):1287–1319.

[21] A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance estimation. 2:

494–515  2008.

[22] P. Spirtes  C. Glymour  and R. Scheines. Causation  prediction and search. MIT Press  2000.
[23] N. Srebro. Maximum likelihood bounded tree-width Markov networks. Artiﬁcial Intelligence  143(1):

123–138  2003.

[24] V. N. Temlyakov. Greedy approximation. Acta Numerica  17:235–409  2008.
[25] S. van de Geer. High-dimensional generalized linear models and the lasso. The Annals of Statistics  36:

614–645  2008.

[26] D. J. A. Welsh. Complexity: Knots  Colourings  and Counting. LMS Lecture Note Series. Cambridge

University Press  Cambridge  1993.

[27] T. Zhang. Adaptive forward-backward greedy algorithm for sparse learning with linear models. In Neural

Information Processing Systems (NIPS) 21  2008.

[28] T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal of

Machine Learning Research  10:555–568  2009.

9

,Elad Hoffer
Itay Hubara
Daniel Soudry
Runsheng Yu
Wenyu Liu
Yasen Zhang
Zhi Qu
Deli Zhao
Bo Zhang