2013,Analyzing the Harmonic Structure in Graph-Based Learning,We show that either explicitly or implicitly  various well-known graph-based models exhibit a common significant \emph{harmonic} structure in its target function -- the value of a vertex is approximately the weighted average of the values of its adjacent neighbors. Understanding of such structure and analysis of the loss defined over such structure help reveal important properties of the target function over a graph. In this paper  we show that the variation of the target function across a cut can be upper and lower bounded by the ratio of its harmonic loss and the cut cost. We use this to develop an analytical tool and analyze 5 popular models in graph-based learning: absorbing random walks  partially absorbing random walks  hitting times  pseudo-inverse of graph Laplacian  and eigenvectors of the Laplacian matrices. Our analysis well explains several open questions of these models reported in the literature. Furthermore  it provides theoretical justifications and guidelines for their practical use. Simulations on synthetic and real datasets support our analysis.,Analyzing the Harmonic Structure

in Graph-Based Learning

Xiao-Ming Wu1  Zhenguo Li3  and Shih-Fu Chang1 2

1Department of Electrical Engineering  Columbia University

2Department of Computer Science  Columbia University

3Huawei Noah’s Ark Lab  Hong Kong

{xmwu  sfchang}@ee.columbia.edu 

li.zhenguo@huawei.com

Abstract

We ﬁnd that various well-known graph-based models exhibit a common important
harmonic structure in its target function – the value of a vertex is approximately
the weighted average of the values of its adjacent neighbors. Understanding of
such structure and analysis of the loss deﬁned over such structure help reveal im-
portant properties of the target function over a graph. In this paper  we show that
the variation of the target function across a cut can be upper and lower bounded by
the ratio of its harmonic loss and the cut cost. We use this to develop an analytical
tool and analyze ﬁve popular graph-based models: absorbing random walks  par-
tially absorbing random walks  hitting times  pseudo-inverse of the graph Lapla-
cian  and eigenvectors of the Laplacian matrices. Our analysis sheds new insights
into several open questions related to these models  and provides theoretical justi-
ﬁcations and guidelines for their practical use. Simulations on synthetic and real
datasets conﬁrm the potential of the proposed theory and tool.

1 Introduction

Various graph-based models  regardless of application  aim to learn a target function on graphs that
well respects the graph topology. This has been done under different motivations such as Laplacian
regularization [4  5  6  14  24  25  26]  random walks [17  19  23  26]  hitting and commute times
[10]  p-resistance distances [1]  pseudo-inverse of the graph Laplacian [10]  eigenvectors of the
Laplacian matrices [18  20]  diffusion maps [8]  to name a few. Whether these models can capture
the graph structure faithfully  or whether their target functions possess desirable properties over
the graph  remain unclear. Understanding of such issues can be of great value in practice and has
attracted much attention recently [16  22  23].

Several important observations about learning on graphs have been reported. Nadler et al. [16]
showed that the target functions of Laplacian regularized methods become ﬂat as the number of
unlabeled points increases  but they also observed that a good classiﬁcation can still be obtained
if an appropriate threshold is used. An explanation to this would be interesting. Von Luxburg
et al. [22] proved that commute and hitting times are dominated by the local structures in large
graphs  ignoring the global patterns. Does this mean these metrics are ﬂawed? Interestingly  despite
this ﬁnding  the pseudo-inverse of graph Laplacian  known as the kernel matrix of commute times 
consistently performs superior in collaborative ﬁltering [10]. In spectral clustering  the eigenvectors
of the normalized graph Laplacian are more desired than those of the un-normalized one [20  21].
Also for the recently proposed partially absorbing random walks [23]  certain setting of absorption
rates seems better than others. While these issues arise from seemingly unrelated contexts  we will
show in this paper that they can be addressed in a single framework.

1

Our starting point is the discovery of a common structure hidden in the target functions of various
graph models. That is  the value of a vertex is approximately the weighted average of the values
of its adjacent neighbors. We call this structure the harmonic structure for its resemblance to the
harmonic function [9  26]. It naturally arises from the ﬁrst step analysis of random walk models 
and  as will be shown in this paper  implicitly exists in other methods such as pseudo-inverse of the
graph Laplacian and eigenvectors of the Laplacian matrices. The target functions of these models
are characterized by their harmonic loss  a quantitative notion introduced in this paper to measure
the discrepancy of a target function f on cuts of graphs. The variations of f across cuts can then be
upper and lower bounded by the ratio of its harmonic loss and the cut cost. As long as the harmonic
loss varies slowly  the graph conductance dominates the variations of f – it will remain smooth in
a dense area but vary sharply otherwise. Models possessing such properties successfully capture
the cluster structures  and as shown in Sec. 4  lead to superior performance in practical applications
including classiﬁcation and retrieval.

This novel perspective allows us to give a uniﬁed treatment of graph-based models. We use this tool
to study ﬁve popular models: absorbing random walks  partially absorbing random walks  hitting
times  pseudo-inverse of the graph Laplacian  and eigenvectors of the Laplacian matrices. Our
analysis provides new theoretical understandings into these models  answers related open questions 
and helps to correct and justify their practical use. The key message conveyed in our results is that
various existing models enjoying the harmonic structure are actually capable of capturing the global
graph topology  and understanding of this structure can guide us in applying them properly.

2 Analysis

Let us ﬁrst deﬁne some notations. In this paper  we consider graphs which are connected  undirected 
weighted  and without self-loops. Denote by G = (V  W ) a graph with n vertices V and a symmetric
non-negative afﬁnity matrix W = [wij ] ∈ Rn×n (wii = 0). Denote by di = Pj wij the degree of
vertex i  by D = diag(d1  d2  . . .   dn) the degree matrix  and by L = D − W the graph Laplacian
[7]. The conductance of a subset S ⊂ V of vertices is deﬁned as Φ(S) =
min(d(S) d( ¯S))  where
w(S  ¯S) = Pi∈S j∈ ¯S wij is the cut cost between S and its complement ¯S  and d(S) = Pi∈S di is
the volume of S. For any i /∈ S  denote by i ∼ S if there is an edge between vertex i and the set S.
Deﬁnition 2.1 (Harmonic loss). The harmonic loss of f : V → R on any S ⊆ V is deﬁned as:

w(S  ¯S)

Lf (S) := X

i∈S

di


f (i) − X

j∼i

wij
di

f (j)

 = X

i∈S


dif (i) − X

j∼i

wijf (j)
 .

(1)

Note that Lf (S) = Pi∈S(Lf )(i). By deﬁnition  the harmonic loss can be negative. However  as
we shall see below  it is always non-negative on superlevel sets.
The following lemma shows that the harmonic loss couples the cut cost and the discrepancy of the
function across the cut. This observation will serve as the foundation of our analysis in this paper.
Lemma 2.2. Lf (S) = Pi∈S j∈ ¯S wij (f (i) − f (j)). In particular  Lf (V) = 0.
In practice  to examine the variation of f on a graph  one does not necessarily examine on every
subset of vertices  which will be exponential in the number of vertices. Instead  it sufﬁces to consider
its variation on the superlevel sets deﬁned as follows.
Deﬁnition 2.3 (Superlevel set). For any function f : V → R on a graph and a scalar c ∈ R  the
set {i | f (i) ≥ c} is called a superlevel set of f with level c.

W.l.o.g.  we assume the vertices are sorted such that f (1) ≥ f (2) ≥ · · · ≥ f (n − 1) ≥ f (n). The
subset Si := {1  . . .   i} is the superlevel set with level f (i) if f (i) > f (i + 1). For convenience  we
still call Si a superlevel set of f even if f (i) = f (i + 1). In this paper  we will mainly examine the
variation of f on its n superlevel sets S1  . . .   Sn. Our ﬁrst observation is that the harmonic loss on
each superlevel set is non-negative  stated as follows.
Lemma 2.4. Lf (Si) ≥ 0  i = 1  . . .   n.

2

Based on the notion of superlevel sets  it becomes legitimate to talk about the continuity of a function
on graphs  which we formally deﬁne as follows.
Deﬁnition 2.5 (Continuity). For any function f : V → R  we call it left-continuous if i ∼ Si−1 
i = 2  . . .   n; we call it right-continuous if i ∼ ¯Si  i = 1  . . .   n − 1; we call it continuous if
i ∼ Si−1 and i ∼ ¯Si  i = 2  . . .   n − 1. Particularly  f is called left-continuous  right-continuous 
or continuous at vertex i if i ∼ Si−1  i ∼ ¯Si  or i ∼ Si−1 and i ∼ ¯Si  respectively.
Proposition 2.6. For any function f : V → R and any vertex 1 < i < n  1) if Lf (i) < 0  then
i ∼ Si−1  i.e.  f is left-continuous at i; 2) if Lf (i) > 0  then i ∼ ¯Si  i.e.  f is right-continuous at i;
3) if Lf (i) = 0 and f (i − 1) > f (i) > f (i + 1)  then i ∼ Si−1 and i ∼ ¯Si  i.e.  f is continuous at i.

The variation of f can be characterized by the following upper and lower bounds.
Theorem 2.7 (Dropping upper bound). For i = 1  . . .   n − 1 

f (i) − f (i + 1) ≤

Lf (Si)
w(Si  ¯Si)

=

Lf (Si)

Φ(Si) min(d(Si)  d( ¯Si))

.

Theorem 2.8 (Dropping lower bound). For i = 1  . . .   n − 1 

f (u) − f (v) ≥

Lf (Si)
w(Si  ¯Si)

=

Lf (Si)

Φ(Si) min(d(Si)  d( ¯Si))

 

where u := arg max

j∈Si j∼ ¯Si

f (j) and v := arg min

j∈ ¯Si j∼Si

f (j).

(2)

(3)

The key observations are two-fold. First  for any function f on a graph  as long as its harmonic
loss Lf (Si) varies slowly on the superlevel sets  i.e.  f is harmonic almost everywhere  the graph
conductance Φ(Si) will dominate the variation of f . In particular  by Theorem 2.7  f (i + 1) drops
little if Φ(Si) is large  whereas by Theorem 2.8  a big gap exists across the cut if Φ(Si) is small (see
Sec. 3.1 for illustration). Second  the continuity (either left  right  or both) of f ensures that its varia-
tions conform with the graph connectivity  i.e.  points with similar values on f tend to be connected.
It is a desired property because a “discontinuous” function that changes alternatively among differ-
ent clusters can hardly describe the graph. These observations can guide us in identifying “good”
functions that encode the global structure of graphs  as will be shown in the next section.

3 Examples

With the tool developed in Sec. 2  in this section  we study ﬁve popular graph models arising from
different contexts including SSL  retrieval  recommendation  and clustering. For each model  we
show its target function in harmonic forms  quantify its harmonic loss  analyze its dropping bounds 
and provide corrections or justiﬁcations for its use.

3.1 Absorbing Random Walks

The ﬁrst model we examine is the seminal Laplacian regularization method [26] proposed for SSL.
While it has a nice interpretation in terms of absorbing random walks  with the labeled points being
absorbing states  it was argued in [16] that this method might be ill-posed for large unlabeled data
in high dimension (≥ 2) because the target function is extremely ﬂat and thus seems problematic
for classiﬁcation. [1] further connected this argument with the resistance distance on graphs  point-
ing out that the classiﬁcation biases to the labeled points with larger degrees. Here we show that
Laplacian regularization can actually capture the global graph structure and a simple normalization
scheme would resolve the raised issue.

For simplicity  we consider the binary classiﬁcation setting with one label in each class. Denote by
f : V → R the absorption probability vector from every point to the positive labeled point. Assume
the vertices are sorted such that 1 = f (1) > f (2) ≥ · · · ≥ f (n − 1) > f (n) = 0 (vertex 1 is labeled
positive and vertex n is labeled negative). By the ﬁrst step analysis of the random walk 

f (i) = X

k∼i

wik
di

f (k)  for i = 2  . . .   n − 1.

(4)

Our ﬁrst observation is that the harmonic loss of f is constant w.r.t. Si  as shown below.

3

f

(2) 0.97

=

f

(1) 1

=

2

1

1

S2

1

3

1

S

3

f

(3) 0.94

=

0.1

f

(4) 0.06

=

1

4

f

(6) 0

=

6

1

5

f

(5) 0.03

=

1

Figure 1: Absorbing random walks on a 6-point graph.

Corollary 3.1. Lf (Si) = Pk∼1 w1k(1 − f (k))  i = 1  . . .   n − 1.
The following statement shows that f changes continuously on graphs under general condition.
Corollary 3.2. Suppose f is mutually different on unlabeled data. Then f is continuous.

Since the harmonic loss of f is a constant on the superlevel sets Si (Corollary 3.1)  by Theorems
2.7 and 2.8  the variation of f depends solely on the cut value w(Si  ¯Si)  which indicates that it will
drop slowly when the cut is dense but drastically when the cut is sparse. Also by Corollary 3.2  f is
continuous. Therefore  we conclude that f is a good function on graphs.
This can be illustrated by a toy example in Fig. 1  where the graph consists of 6 points in 2 classes
denoted by different colors  with 3 points in each. The edge weights are all 1 except for the edge
between the two cluster  which is 0.1. Vertices 1 and 6 (black edged) are labeled. The absorption
probabilities from all the vertices to vertex 1 are computed and shown. We can see that since the
cut w(S2  ¯S2) = 2 is quite dense  the drop between f (2) and f (3) is upper bounded by a small
number (Theorem 2.7)  so f (3) must be very close to f (2)  as observed. In contrast  since the cut
w(S3  ¯S3) = 0.1 is very weak  Theorem 2.8 guarantees that there will be a huge gap between f (3)
and f (4)  as also veriﬁed. The bound in Theorem 2.8 is now tight as there is only 1 edge in the cut.
Now let f1 and f2 denote the absorption probability vectors to the two labeled points respectively.
To classify an unlabeled point i  the usual way is to compare f1(i) and f2(i)  which is equivalent to
setting the threshold as 0 in f0 = f1 − f2. It was observed in [16] that although f0 can be extremely
ﬂat in the presence of large unlabeled data in high dimension  setting the “right” threshold can
produce sensible results. Our analysis explains this – it is because both f1 and f2 are informative of
the cluster structures. Our key argument is that Laplacian regularization actually carries sufﬁcient
information about the graph structure  but how to exploit it can really make a difference.

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

−2

−1

0

1

2

(a)

1

0.5

0
0

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

600

300
(b)

−2

−1

0

1

2

(c)

6x 10−3

4

2
0

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5

600

300
(d)

−2

−1

0

1

2

(e)

Figure 2: (a) Two 20-dimensional Gaussians with the ﬁrst two dimensions plotted. The magenta
triangle and the green circle denote labeled data. The blue cross denotes a starting vertex indexed
by i for later use.
(c) Classiﬁcation by
comparing the absorption probabilities. (d) Normalized absorption probabilities. (e) Classiﬁcation
by comparing the normalized absorption probabilities.

(b) Absorption probabilities to the two labeled points.

We illustrate this point by using a mixture of two 20-dimensional Gaussians of 600 points  with one
label in each Gaussian (Fig. 2(a)). The absorption probabilities to both labeled points are shown in
Fig. 2(b)  in magenta and green respectively. The green vector is well above the the magenta vector 
indicating that every unlabeled point has larger absorption probability to the green labeled point.
Comparing them classiﬁes all the unlabeled points to the green Gaussian (Fig. 2(c)). Since the green
labeled point has larger degree than the magenta one1  this result is expected from the analysis in
[1]. However  the probability vectors are informative  with a clear gap between the clusters in each

1The degrees are 1.4405 and 0.1435. We use a weighted 20-NN graph (see Supplement).

4

vector. To use the information  we propose to normalize each vector by its probability mass  i.e. 
f ′(i) = f (i)/Pj f (j) (Fig. 2(d)). Comparing them leads to a perfect classiﬁcation (Fig. 2(e)).
This idea is based on two observations from our analysis: 1) the variance of the probabilities within
each cluster is small; 2) there is a gap between the clusters. The small variance indicates that
comparing the probabilities is essentially the same as comparing their means within clusters. The
gap between the clusters ensures that the normalization makes the vectors align well (this point is
made precise in Supplement). Our above analysis applies to multi-class problems and allows more
than one labeled points in one class. In this general case  the classiﬁcation rule is as follows: 1)
compute the absorption probability vector fi : U → R for each labeled point i by taking all other
labeled points as negative  where U denotes the set of unlabeled points; 2) normalize fi by its mass 
i(j)}. We denote
denoted by f ′
this algorithm as ARW-N-1NN.

i; 3) assign each unlabeled point j to the class of j∗ := arg maxi{f ′

3.2 Partially Absorbing Random Walks

αλi+di

Here we revisit the recently proposed partially absorbing random walks (PARW) [23]  which gener-
alizes absorbing random walks by allowing partial absorption at each state. The absorption rate pii
at state i is deﬁned as pii = αλi
  where α > 0  λi > 0 are regularization parameters. Given cur-
rent state i  a PARW in the next step will get absorbed at i with probability pii and with probability
(1 − pii) × wij
moves to state j. Let aij be the probability that a PARW starting from state i gets
di
absorbed at state j within ﬁnite steps  and denote by A = [aij] ∈ Rn×n the absorption probability
matrix. Then A = (αΛ + L)−1αΛ  where Λ = diag(λ1  . . .   λn) is the regularization matrix.
PARW is a uniﬁed framework with several popular SSL methods and PageRank [17] as its special
cases  corresponding to different Λ. Particularly  the case Λ = I has been justiﬁed in capturing the
cluster structures [23]. In what follows  we extend this result to show that the columns of A obtained
by PARW with almost arbitrary Λ (not just Λ = I) actually exhibit strong harmonic structures and
should be expected to work equally well.

Our ﬁrst observation is that while A is not symmetric for arbitrary Λ  AΛ−1 = (αΛ + L)−1α is.
Lemma 3.3. aij = λj
λi
Lemma 3.4. aii is the only largest entry in the i-th column of A  i = 1  . . .   n.

aji.

Our second observation is that the harmonic structure exists in the probabilities of PARW from every
vertex getting absorbed at a particular vertex  i.e.  in the columns of A. W.l.o.g.  consider the ﬁrst
column of A and denote it by p. Assume that the vertices are sorted such that p(1) > p(2) ≥ · · · ≥
p(n − 1) ≥ p(n)  where p(1) > p(2) is due to Lemma 3.4. By the ﬁrst step analysis of PARW  we
can write p in a recursive form:
+ X

p(i) = X

p(k)  i = 2  . . .   n 

p(1) =

p(k) 

αλ1

w1k

(5)

d1 + αλ1

d1 + αλ1

k∼1

wik

di + αλi

k∼i

which is equivalent to the following harmonic form:

p(1) =

αλ1
d1

(1 − p(1)) + X

k∼1

w1k
d1

p(k) 

p(i) = −

αλi
di

p(i) + X

k∼i

wik
di

p(k)  i = 2  . . .   n.

(6)

a1k  i = 1  . . .   n − 1.

a1k) = αλ1 Pk∈ ¯Si

The harmonic loss of p can be computed from Eq. (6).
Corollary 3.5. Lp(Si) = αλ1(1 − Pk∈Si
Corollary 3.6. p is left-continuous.
Now we are ready to examine the variation of p. Note that Pk a1k = 1 and a1k → λk/Pi λi
as α → 0 [23]. By Theorem 2.7  the drop of p(i) is upper bounded by αλ1/w(Si  ¯Si)  which is
small when the cut w(Si  ¯Si) is dense and α is small. Now let k be the largest number such that
d(Sk) ≤ 1
2 Pi λi. By Theorem 2.8  for 1 ≤ i ≤ k  the drop of p(i)
3 αλ1/w(Si  ¯Si)  if α is sufﬁciently small. This shows
across the cut {Si  ¯Si} is lower bounded by 1
that p(i) will drop a lot when the cut w(Si  ¯Si) is weak. The comparison between the corresponding
row and column of A is shown in Figs. 3(a–b)2  which conﬁrms our analysis.

2 d(V)  and assume Pi∈ ¯Sk

λi ≥ 1

2λi’s are sampled from the uniform distribution on the interval [0  1] and α = 1e − 6  as used in Sec. 4.

5

4x 10−3

2

0
0

0.2

0.1

0

−0.1

3.418x 10−3

3.417

1

0.5

0

600

3.416
0

300
(a)

600

−0.5
0

300
(b)

600

300
(c)

0.5

0

−0.5

0.3

0.2

0.1

0

2x 104

1

0
0

0.1

0.05

0

−0.05

1500

1400

1300

600

1200
0

300
(d)

600

300
(e)

0.2

0.1

0

−0.1

300

300

300

−1
0
600
(g) λu = 0.0172

300

−0.1
0
600
(h) λ = 0.0304

300

−0.2
0
600
(f) λu = 0.0144

−0.2
0
600
(j) λv = 0.3845
Figure 3: (a) Absorption probabilities that a PAWR gets absorbed at other points when starting from
i (see Fig. 2). (b) Absorption probabilities that PAWR gets absorbed at i when starting from other
points. (c) The i-th row of L†. (d) Hitting times from i to hit other points. (e) Hitting times from
other points to hit i. (f) and (g) Eigenvectors of L (mini{di} = 0.0173). (h) An eigenvector of
Lsym. (i) and (j) Eigenvectors of Lrw. The values in (f–j) denote eigenvalues.

−0.1
0
600
(i) λv = 0.0304

It is worth mentioning that our analysis substantially extends the results in [23] by showing that the
setting of Λ is not really necessary – a random Λ can perform equally well if using the columns
instead of the rows of A. In addition  our result includes the seminal local clustering model [2] as a
special case  which corresponds to Λ = D in our analysis.

3.3 Pseudo-inverse of the Graph Laplacian

The pseudo-inverse L† of the graph Laplacian is a valid kernel corresponding to commute times
[10  12]. While commute times may fail to capture the global topology in large graphs [22]  L†  if
used directly as a similarity measure  gives superior performance in practice [10]. Here we provide
a formal analysis and justiﬁcation for L† by revealing the strong harmonic structure hidden in it.
Lemma 3.7. (L†L)ij = − 1

n   i 6= j; and (L†L)ii = 1 − 1
n .

Note that L† is symmetric since L is symmetric. W.l.o.g.  we consider the ﬁrst row of L† and denote
it by ℓ. The following lemma shows the harmonic form of ℓ.
Lemma 3.8. ℓ has the following harmonic form:

ℓ(1) =

1 − 1
n

d1

+ X

k∼1

w1k
d1

ℓ(k)  ℓ(i) = −

1
n
di

+ X

k∼i

wik
di

ℓ(k)  i = 2  . . .   n.

(7)

W.l.o.g.  assume the vertices have been sorted such that ℓ(1) > ℓ(2) ≥ · · · ≥ ℓ(n − 1) ≥ ℓ(n)3.
Then the harmonic loss of ℓ on the set Si admits a very simple form  as shown below.
Corollary 3.9. Lℓ(Si) = | ¯Si|
Corollary 3.10. ℓ is left-continuous.

n   i = 1  . . .   n − 1.

By Corollary 3.9  Lℓ(Si) < 1 and decreases very slowly in large graphs since Lℓ(Si) − Lℓ(Si+1) =
1
n for any i. From the analysis in Sec. 2  we can immediately conclude that the variation of ℓ(i) is
dominated by the cut cost on the superlevel set Si. Fig. 3(c) illustrates this argument.

3.4 Hitting Times

The hitting time hij from vertex i to j is the expected number of steps it takes a random walk starting
from i to reach j for the ﬁrst time. While it was proven in [22] that hitting times are dominated by
the local structure of the target  we show below that the hitting times from other points to the same
target admit a harmonic structure  and thus are still able to capture the global structure of graphs.
Our result is complementary to the analysis in [22]  and provides a justiﬁcation of using hitting times
in information retrieval where the query is taken as the target to be hit by others [15].

3ℓ(1) > ℓ(2) since one can show that any diagonal entry in L† is the only largest in the corresponding row.

6

Let h : V → R be the hitting times from every vertex to a particular vertex. W.l.o.g.  assume the
vertices have been sorted such that h(1) ≥ h(2) ≥ · · · ≥ h(n − 1) > h(n) = 0  where vertex n is
the target vertex. Applying the ﬁrst step analysis  we obtain the harmonic form of h:

h(i) = 1 + X

k∼i

wik
di

h(k) 

for i = 1  . . .   n − 1.

(8)

The harmonic loss on the set Si turns out to be the volume of the set  as stated below.
Corollary 3.11. Lh(Si) = X
Corollary 3.12. h is right-continuous.

dk  i = 1  . . .   n − 1.

1≤k≤i

Now let us examine the variation of h across any cut {Si  ¯Si}. Note that

Lh(Si)
w(Si  ¯Si)

=

αi

Φ(Si)

  where αi =

d(Si)

min(d(Si)  d( ¯Si))

.

(9)

− 1 could be quite large. As i decreases from d(Si) > 1

First  by Theorem 2.8  there could be a signiﬁcant gap between the target and its neighbors  since
αn−1 = d(V)
2 d(V)  the variation of αi
dn
becomes slower and slower (αi = 1 when d(Si) ≤ 1
2 d(V))  so the variation of h will depend on the
variation of the conductance of Si  i.e.  Φ(Si)  according to Theorems 2.7 and 2.8. Fig. 3(e) shows
that h is ﬂat within the clusters  but there is a large gap presented between them. In contrast  there
are no gaps exhibited in the hitting times from the target to other vertices (Fig. 3(d)).

3.5 Eigenvectors of the Laplacian Matrices

The eigenvectors of the Laplacian matrices play a key role in graph partitioning [20]. In practice  the
eigenvectors with smaller (positive) eigenvalues are more desired than those with larger eigenvalues 
and the ones from a normalized Laplacian are preferred than those from the un-normalized one.
These choices are usually justiﬁed from the relaxation of the normalized cuts [18] and ratio cuts
[11]. However  it has been known that these relaxations can be arbitrarily loose [20]. It seems more
interesting if one can draw conclusions by analyzing the eigenvectors directly. Here we address
these issues by examining the harmonic structures in these eigenvectors.

We follow the notations in [20] to denote two normalized graph Laplacians: Lrw := D−1L and
Lsym := D− 1
2 . Denote by u and v two eigenvectors of L and Lrw with eigenvalues λu > 0
and λv > 0  respectively  i.e.  Lu = λuu and Lrwv = λvv. Then we have

2 LD− 1

u(i) = X

k∼i

wik

di − λu

u(k) 

v(i) = X

k∼i

wik

di(1 − λv)

v(k) 

for i = 1  . . .   n.

(10)

We can see that the smaller λu and λv  the stronger the harmonic structures of u and v. This explains
why in practice the eigenvector with the second4 smallest eigenvalues gives superior performance.
As long as λu ≪ mini{di}  we are safe to say that u will have a signiﬁcant harmonic structure  and
thus will be informative for clustering. However  if λu is close to mini{di}  no matter how small λu
is  the harmonic structure of u will be weaker  and thus u is less useful. In contrast  from Eq. (10) 
v will always enjoy a signiﬁcant harmonic structure as long as λv is much smaller than 1. This
explains why eigenvectors of Lrw are preferred than those of L for clustering. These arguments are
validated in Figs. 3(f–j)  where we also include an eigenvector of Lsym for comparison.

4 Experiments

In the ﬁrst experiment5  we test absorbing random walks (ARW) for SSL  with the class mass nor-
malization suggested in [26] (ARW-CMN)  our proposed normalization (ARW-N-1NN  Sec. 3.1) 
and without any normalization (ARW-1NN) – where each unlabeled instance is assigned the class of
the labeled instance at which it most likely gets absorbed. We also compare with the local and global

4Note that the smallest one is zero in either L or Lrw.
5Please see Supplement for parameter settings  data description  graph construction  and experimental setup.

7

Table 1: Classiﬁcation accuracy on 9 datasets.

ARW-N-1NN
ARW-1NN
ARW-CMN

LGC

PARW (Λ = I)

USPS
.879
.445
.775
.821
.880

YaleB
.892
.733
.847
.884
.906

satimage

imageseg

ionosphere

.777
.650
.741
.725
.781

.673
.595
.624
.638
.665

.771
.699
.724
.731
.752

iris
.918
.902
.894
.903
.928

protein
.589
.440
.511
.477
.572

spiral
.830
.754
.726
.729
.835

soybean

.916
.889
.856
.816
.905

consistency (LGC) method [24] and the PARW with Λ = I in [23]. The results are summarized in
Table 1. We can see that ARW-N-1NN and PARW (Λ = I) consistently perform the best  which
veriﬁes our analysis in Sec. 3. The results of ARW-1NN are unsatisfactory due to its bias to the
labeled instance with the largest degree [1]. Although ARW-CMN does improve over ARW-1NN in
many cases  it does not perform as well as ARW-N-1NN  mainly because of the artifacts induced by
estimating the class proportion from limited labeled data. The results of LGC are not comparable to
ARW-N-1NN and PARW (Λ = I)  which is probably due to the lack of a harmonic structure.

Table 2: Ranking results (MAP) on USPS.

Digits

Λ = R (column)

Λ = R (row)

Λ = I

0

.981
.169
.981

1

.988
.143
.988

2

.875
.114
.876

3

.892
.096
.893

4

.647
.092
.646

5

.780
.076
.778

6

.941
.093
.940

7

.918
.093
.919

8

.746
.075
.746

9

.731
.086
.730

All
.850
.103
.850

In the second experiment  we test PARW on a retrieval task on USPS (see Supplement). We compare
the cases with Λ = I and Λ = R  where R is a random diagonal matrix with positive diagonal
entries. For Λ = R  we also compare the uses of columns and rows for retrieval. The results are
shown in Table 2. We observe that the columns in Λ = R give signiﬁcantly better results compared
with rows  implying that the harmonic structure is vital to the performance. Λ = R (column) and
Λ = I perform very similarly. This suggests that it is not the special setting of absorbing rates but
the harmonic structure that determines the overall performance.

Table 3: Classiﬁcation accuracy on USPS.

k-NN unweighted graphs

HT(L → U)
HT(U → L)

L†

10

.8514
.1518
.8512

20

.8361
.1454
.8359

50

.7822
.1372
.7816

100
.7500
.1209
.7493

200
.7071
.1131
.7062

500
.6429
.1113
.6426

In the third experiment  we test hitting times and pseudo-inverse of the graph Laplacian for SSL on
USPS. We compare two different uses of hitting times  the case of starting from the labeled data L
to hit the unlabeled data U (HT(L → U))  and the case of the opposite direction (HT(U → L)).
Each unlabeled instance j is assigned the class of labeled instance j∗  where j∗ = arg mini∈L{hij}
in HT(L → U)  j∗ = arg mini∈L{hji} in (HT(U → L))  and j∗ = arg maxi∈L{ℓji} in L† = (ℓij).
The results averaged over 100 trials are shown in Table 3  where we see that HT(L → U) performs
much better than HT(U → L)  which is expected as the former admits a desired harmonic structure.
Note that HT(L → U) is not lost as the number of neighbors increases (i.e.  the graph becomes
more connected). The slight performance drop is due to the inclusion of more noisy edges. In
contrast  HT(U → L) is completely lost [20]. We also observe that L† produces very competitive
performance  which again supports our analysis.

5 Conclusion

In this paper  we explore the harmonic structure that widely exists in graph models. Different
from previous research [3  13] of harmonic analysis on graphs  where the selection of canonical
basis on graphs and the asymptotic convergence on manifolds are studied  here we examine how
functions on graphs deviate from being harmonic and develop bounds to analyze their theoretical
behavior. The proposed harmonic loss quantiﬁes the discrepancy of a function across cuts  allows a
uniﬁed treatment of various models from different contexts  and makes them easy to analyze. Due
to its resemblance with standard mathematical concepts such as divergence and total variation  an
interesting line of future work is to make their connections clear. Other future works include deriving
more rigorous bounds for certain functions and extending our analysis to more graph models.

8

References

[1] M. Alamgir and U. von Luxburg. Phase transition in the family of p-resistances. In NIPS.

2011.

[2] R. Andersen  F. Chung  and K. Lang. Local graph partitioning using pagerank vectors. In

FOCS  pages 475–486  2006.

[3] M. Belkin. Problems of Learning on Manifolds. PhD thesis  The University of Chicago  2003.
[4] M. Belkin  I. Matveeva  and P. Niyogi. Regularization and semi-supervised learning on large

graphs. In COLT  pages 624–638  2004.

[5] M. Belkin  Q. Que  Y. Wang  and X. Zhou. Toward understanding complex spaces: Graph

laplacians on manifolds with singularities and boundaries. In COLT  2012.

[6] O. Bousquet  O. Chapelle  and M. Hein. Measure based regularization. In NIPS  2003.
[7] F. Chung. Spectral Graph Theory. American Mathematical Society  1997.
[8] R. Coifman and S. Lafon. Diffusion maps. Applied and Computational Harmonic Analysis 

21(1):5–30  2006.

[9] P. G. Doyle and J. L. Snell. Random walks and electric networks. Mathematical Association

of America  1984.

[10] F. Fouss  A. Pirotte  J.-M. Renders  and M. Saerens. Random-walk computation of similarities
between nodes of a graph with application to collaborative recommendation. IEEE Transac-
tions on Knowledge and Data Engineering  19(3):355–369  2007.

[11] L. Hagen and A. B. Kahng. New spectral methods for ratio cut partitioning and clustering.
IEEE transactions on Computer-aided design of integrated circuits and systems  11(9):1074–
1085  1992.

[12] D. J. Klein and M. Randi´c. Resistance distance. Journal of Mathematical Chemistry  12(1):81–

95  1993.

[13] S. S. Lafon. Diffusion maps and geometric harmonics. PhD thesis  Yale University  2004.
[14] M. H. G. Lever and M. Herbster. Predicting the labelling of a graph via minimum p-seminorm

interpolation. In COLT  2009.

[15] Q. Mei  D. Zhou  and K. Church. Query suggestion using hitting time. In CIKM  pages 469–

478  2008.

[16] B. Nadler  N. Srebro  and X. Zhou. Statistical analysis of semi-supervised learning: The limit

of inﬁnite unlabelled data. In NIPS  pages 1330–1338  2009.

[17] L. Page  S. Brin  R. Motwani  and T. Winograd. The pagerank citation ranking: Bringing order

to the web. 1999.

[18] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. PAMI  22(8):888–

905  2000.

[19] M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with Markov random walks. In

NIPS  pages 945–952  2002.

[20] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing  17(4):395–416 

2007.

[21] U. Von Luxburg  M. Belkin  and O. Bousquet. Consistency of spectral clustering. The Annals

of Statistics  pages 555–586  2008.

[22] U. Von Luxburg  A. Radl  and M. Hein. Hitting and commute times in large graphs are often

misleading. Arxiv preprint arXiv:1003.1266  2010.

[23] X.-M. Wu  Z. Li  A. M.-C. So  J. Wright  and S.-F. Chang. Learning with partially absorbing

random walks. In NIPS  2012.

[24] D. Zhou  O. Bousquet  T. Lal  J. Weston  and B. Sch¨olkopf. Learning with local and global

consistency. In NIPS  2004.

[25] X. Zhou and M. Belkin. Semi-supervised learning by higher order regularization. In AISTATS 

2011.

[26] X. Zhu  Z. Ghahramani  and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and

harmonic functions. In ICML  2003.

9

,Xiao-Ming Wu
Zhenguo Li
Shih-Fu Chang
Dohyung Park
Constantine Caramanis
Sujay Sanghavi
Mandar Dixit
Nuno Vasconcelos