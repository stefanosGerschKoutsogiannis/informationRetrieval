2012,Learning curves for multi-task Gaussian process regression,We study the average case performance of multi-task Gaussian process (GP)   regression as captured in the learning curve  i.e.\ the average Bayes error   for a chosen task versus the total number of examples $n$ for all   tasks. For GP covariances that are the product of an   input-dependent covariance function and a free-form inter-task   covariance matrix  we   show that accurate approximations for the learning curve can be   obtained for an arbitrary number of tasks $T$.  We use   these to study the asymptotic learning behaviour for large   $n$. Surprisingly  multi-task learning can be asymptotically essentially   useless: examples from other tasks only help when the   degree of inter-task correlation  $\rho$  is near its maximal value   $\rho=1$. This effect is most extreme for learning of smooth target   functions as described by e.g.\ squared exponential kernels. We also   demonstrate that when learning {\em many} tasks  the learning curves   separate into an initial phase  where the Bayes error on each task   is reduced down to a plateau value by ``collective learning''    even though most tasks have not seen examples    and a final decay that occurs only once the number of examples is   proportional to the number of tasks.,Learning curves for multi-task Gaussian process

regression

Simon R F Ashton

King’s College London

Department of Mathematics

Strand  London WC2R 2LS  U.K.

Peter Sollich

King’s College London

Department of Mathematics

Strand  London WC2R 2LS  U.K.
peter.sollich@kcl.ac.uk

Abstract

We study the average case performance of multi-task Gaussian process (GP) re-
gression as captured in the learning curve  i.e. the average Bayes error for a chosen
task versus the total number of examples n for all tasks. For GP covariances that
are the product of an input-dependent covariance function and a free-form inter-
task covariance matrix  we show that accurate approximations for the learning
curve can be obtained for an arbitrary number of tasks T . We use these to study
the asymptotic learning behaviour for large n. Surprisingly  multi-task learning
can be asymptotically essentially useless  in the sense that examples from other
tasks help only when the degree of inter-task correlation  ρ  is near its maximal
value ρ = 1. This effect is most extreme for learning of smooth target functions
as described by e.g. squared exponential kernels. We also demonstrate that when
learning many tasks  the learning curves separate into an initial phase  where the
Bayes error on each task is reduced down to a plateau value by “collective learn-
ing” even though most tasks have not seen examples  and a ﬁnal decay that occurs
once the number of examples is proportional to the number of tasks.

1

Introduction and motivation

Gaussian processes (GPs) [1] have been popular in the NIPS community for a number of years
now  as one of the key non-parametric Bayesian inference approaches. In the simplest case one can
use a GP prior when learning a function from data. In line with growing interest in multi-task or
transfer learning  where relatedness between tasks is used to aid learning of the individual tasks (see
e.g. [2  3])  GPs have increasingly also been used in a multi-task setting. A number of different
choices of covariance functions have been proposed [4  5  6  7  8]. These differ e.g. in assumptions
on whether the functions to be learned are related to a smaller number of latent functions or have
free-form inter-task correlations; for a recent review see [9].
Given this interest in multi-task GPs  one would like to quantify the beneﬁts that they bring compared
to single-task learning. PAC-style bounds for classiﬁcation [2  3  10] in more general multi-task sce-
narios exist  but there has been little work on average case analysis. The basic question in this setting
is: how does the Bayes error on a given task depend on the number of training examples for all tasks 
when averaged over all data sets of the given size. For a single regression task  this learning curve
has become relatively well understood since the late 1990s  with a number of bounds and approxi-
mations available [11  12  13  14  15  16  17  18  19] as well as some exact predictions [20]. Already
two-task GP regression is much more difﬁcult to analyse  and progress was made only very recently
at NIPS 2009 [21]  where upper and lower bounds for learning curves were derived. The tightest of
these bounds  however  either required evaluation by Monte Carlo sampling  or assumed knowledge
of the corresponding single-task learning curves. Here our aim is to obtain accurate learning curve
approximations that apply to an arbitrary number T of tasks  and that can be evaluated explicitly
without recourse to sampling.

1

We begin (Sec. 2) by expressing the Bayes error for any single task in a multi-task GP regression
problem in a convenient feature space form  where individual training examples enter additively.
This requires the introduction of a non-trivial tensor structure combining feature space components
and tasks. Considering the change in error when adding an example for some task leads to partial
differential equations linking the Bayes errors for all tasks. Solving these using the method of
characteristics then gives  as our primary result  the desired learning curve approximation (Sec. 3). In
Sec. 4 we discuss some of its predictions. The approximation correctly delineates the limits of pure
transfer learning  when all examples are from tasks other than the one of interest. Next we compare
with numerical simulations for some two-task scenarios  ﬁnding good qualitative agreement. These
results also highlight a surprising feature  namely that asymptotically the relatedness between tasks
can become much less useful. We analyse this effect in some detail  showing that it is most extreme
for learning of smooth functions. Finally we discuss the case of many tasks  where there is an
unexpected separation of the learning curves into a fast initial error decay arising from “collective
learning”  and a much slower ﬁnal part where tasks are learned almost independently.

2 GP regression and Bayes error

τ(cid:96). This setup allows the noise level σ2

We consider GP regression for T functions fτ (x)  τ = 1  2  . . .   T . These functions have to be
learned from n training examples (x(cid:96)  τ(cid:96)  y(cid:96))  (cid:96) = 1  . . .   n. Here x(cid:96) is the training input  τ(cid:96) ∈
{1  . . .   T} denotes which task the example relates to  and y(cid:96) is the corresponding training output.
We assume that the latter is given by the target function value fτ(cid:96) (x(cid:96)) corrupted by i.i.d. additive
τ to depend on
Gaussian noise with zero mean and variance σ2
the task.
In GP regression the prior over the functions fτ (x) is a Gaussian process. This means that for any
set of inputs x(cid:96) and task labels τ(cid:96)  the function values {fτ(cid:96)(x(cid:96))} have a joint Gaussian distribution.
As is common we assume this to have zero mean  so the multi-task GP is fully speciﬁed by the
covariances (cid:104)fτ (x)fτ(cid:48)(x(cid:48))(cid:105) = C(τ  x  τ(cid:48)  x(cid:48)). For this covariance we take the ﬂexible form from [5] 
(cid:104)fτ (x)fτ(cid:48)(x(cid:48))(cid:105) = Dτ τ(cid:48)C(x  x(cid:48)). Here C(x  x(cid:48)) determines the covariance between function values
at different input points  encoding “spatial” behaviour such as smoothness and the lengthscale(s)
over which the functions vary  while the matrix D is a free-form inter-task covariance matrix.
One of the attractions of GPs for regression is that  even though they are non-parametric models
with (in general) an inﬁnite number of degrees of freedom  predictions can be made in closed form 
see e.g. [1]. For a test point x for task τ  one would predict as output the mean of fτ (x) over
the (Gaussian) posterior  which is yTK−1kτ (x). Here K is the n × n Gram matrix with entries
δ(cid:96)m  while kτ (x) is a vector with the n entries kτ (cid:96) = Dτ(cid:96)τ C(x(cid:96)  x).
K(cid:96)m = Dτ(cid:96)τmC(x(cid:96)  xm) + σ2
τ(cid:96)
The error bar would be taken as the square root of the posterior variance of fτ (x)  which is

(1)
The learning curve for task τ is deﬁned as the mean-squared prediction error  averaged over the
location of test input x and over all data sets with a speciﬁed number of examples for each task  say
n1 for task 1 and so on. As is standard in learning curve analysis we consider a matched scenario
where the training outputs y(cid:96) are generated from the same prior and noise model that we use for
inference. In this case the mean-squared prediction error ˆτ is the Bayes error  and is given by the
average posterior variance [1]  i.e. ˆτ = (cid:104)Vτ (x)(cid:105)x. To obtain the learning curve this is averaged
over the location of the training inputs x(cid:96): τ = (cid:104)ˆτ(cid:105). This average presents the main challenge
for learning curve prediction because the training inputs feature in a highly nonlinear way in Vτ (x).
Note that the training outputs  on the other hand  do not appear in the posterior variance Vτ (x) and
so do not need to be averaged over.
We now want to write the Bayes error ˆτ in a form convenient for performing  at least approxi-
mately  the averages required for the learning curve. Assume that all training inputs x(cid:96)  and also the
(cid:80)
test input x  are drawn from the same distribution P (x). One can decompose the input-dependent
part of the covariance function into eigenfunctions relative to P (x)  according to C(x  x(cid:48)) =
i λiφi(x)φi(x(cid:48)). The eigenfunctions are deﬁned by the condition (cid:104)C(x  x(cid:48))φi(x(cid:48))(cid:105)x(cid:48) = λiφi(x)
and can be chosen to be orthonormal with respect to P (x)  (cid:104)φi(x)φj(x)(cid:105)x = δij. The sum over i
here is in general inﬁnite (unless the covariance function is degenerate  as e.g. for the dot product
kernel C(x  x(cid:48)) = x · x(cid:48)). To make the algebra below as simple as possible  we let the eigenvalues
λi be arranged in decreasing order and truncate the sum to the ﬁnite range i = 1  . . .   M; M is then
some large effective feature space dimension and can be taken to inﬁnity at the end.

Vτ (x) = Dτ τ C(x  x) − kT

τ (x)K−1kτ (x)

2

In terms of the above eigenfunction decomposition  the Gram matrix has elements

K(cid:96)m = Dτ(cid:96)τm

λiφi(x(cid:96))φi(xm)+σ2
τ(cid:96)

δ(cid:96)m =

δτ(cid:96) τ φi(x(cid:96))λiδijDτ τ(cid:48)φj(xm)δτ(cid:48) τm+σ2
τ(cid:96)

δ(cid:96)m

(cid:88)

i τ j τ(cid:48)

(cid:88)

i

or in matrix form K = ΨLΨT + Σ where Σ is the diagonal matrix from the noise variances and

Ψ(cid:96) iτ = δτ(cid:96) τ φi(x(cid:96)) 

Liτ jτ(cid:48) = λiδijDτ τ(cid:48)

(2)

Here Ψ has its second index ranging over M (number of kernel eigenvalues) times T (number of
tasks) values; L is a square matrix of this size. In Kronecker (tensor) product notation  L = D ⊗ Λ
if we deﬁne Λ as the diagonal matrix with entries λiδij. The Kronecker product is convenient for
the simpliﬁcations below; we will use that for generic square matrices  (A ⊗ B)(A(cid:48) ⊗ B(cid:48)) =
(AA(cid:48)) ⊗ (BB(cid:48))  (A ⊗ B)−1 = A−1 ⊗ B−1  and tr (A ⊗ B) = (tr A)(tr B). In thinking about
the mathematical expressions  it is often easier to picture Kronecker products over feature spaces
and tasks as block matrices. For example  L can then be viewed as consisting of T × T blocks  each
of which is proportional to Λ.
To calculate the Bayes error  we need to average the posterior variance Vτ (x) over the test input x.
The ﬁrst term in (1) then becomes Dτ τ(cid:104)C(x  x)(cid:105) = Dτ τ tr Λ. In the second one  we need to average

(cid:104)kτ (cid:96)(x)kτ m(cid:105)x = Dτ τ(cid:96)(cid:104)C(x(cid:96)  x)C(x  xm)(cid:105)xDτmτ

= Dτ τ(cid:96)

λiλjφi(x(cid:96))(cid:104)φi(x)φj(x)(cid:105)xφj(xm)Dτmτ

(cid:88)
(cid:88)

ij

i τ(cid:48) j τ(cid:48)(cid:48)

=

Dτ τ(cid:48)Ψl iτ(cid:48)λiλjδijΨm jτ(cid:48)(cid:48)Dτ(cid:48)(cid:48)τ

In matrix form this is (cid:104)kτ (x)kT
τ D) ⊗ Λ2]ΨT = ΨMτ ΨT Here the last
equality deﬁnes Mτ   and we have denoted by eτ the T -dimensional vector with τ-th component
equal to one and all others zero. Multiplying by the inverse Gram matrix K−1 and taking the trace
gives the average of the second term in (1); combining with the ﬁrst gives the Bayes error on task τ

τ (x)(cid:105)x = Ψ[(Deτ eT

ˆτ = (cid:104)Vτ (x)(cid:105)x = Dτ τ tr Λ − tr ΨMτ ΨT(ΨLΨT + Σ)−1

Applying the Woodbury identity and re-arranging yields

ˆτ = Dτ τ tr Λ − tr Mτ ΨTΣ−1Ψ(I + LΨTΣ−1Ψ)−1
= Dτ τ tr Λ − tr Mτ L−1[I − (I + LΨTΣ−1Ψ)−1]

But

tr Mτ L−1 = tr{[(Deτ eT
= tr{[Deτ eT

τ D) ⊗ Λ2][D ⊗ Λ]−1}
τ ] ⊗ Λ} = eT

τ Deτ tr Λ = Dτ τ tr Λ

so the ﬁrst and second terms in the expression for ˆτ cancel and one has

ˆτ = tr Mτ L−1(I + LΨTΣ−1Ψ)−1 = tr L−1Mτ L−1(L−1 + ΨTΣ−1Ψ)−1

= tr [D ⊗ Λ]−1[(Deτ eT
= tr [eτ eT

τ ⊗ I](L−1 + ΨTΣ−1Ψ)−1

τ D) ⊗ Λ2][D ⊗ Λ]−1(L−1 + ΨTΣ−1Ψ)−1

The matrix in square brackets in the last line is just a projector Pτ onto task τ; thought of as a matrix
of T × T blocks (each of size M × M)  this has an identity matrix in the (τ  τ ) block while all other
blocks are zero. We can therefore write  ﬁnally  for the Bayes error on task τ 

ˆτ = tr Pτ (L−1 + ΨTΣ−1Ψ)−1

(3)
Because Σ is diagonal and given the deﬁnition (2) of Ψ  the matrix ΨTΣ−1Ψ is a sum of contri-
butions from the individual training examples (cid:96) = 1  . . .   n. This will be important for deriving the
τ Pτ = I  the sum of the
τ ˆτ = tr (L−1 +ΨTΣ−1Ψ)−1  in close analogy to the corresponding

learning curve approximation below. We note in passing that  because(cid:80)
Bayes errors on all tasks is(cid:80)

expression for the single-task case [13].

3

response or resolvent matrix G = (L−1 + ΨTΣ−1Ψ +(cid:80)

3 Learning curve prediction
To obtain the learning curve τ = (cid:104)ˆτ(cid:105)  we now need to carry out the average (cid:104). . .(cid:105) over the training
inputs. To help with this  we can extend an approach for the single-task scenario [13] and deﬁne a
τ vτ Pτ )−1 with auxiliary parameters vτ
that will be set back to zero at the end. One can then ask how G = (cid:104)G(cid:105) and hence τ(cid:48) = tr Pτ(cid:48)G
changes with the number nτ of training points for task τ. Adding an example at position x for task
τ increases ΨTΣ−1Ψ by σ−2
τ   where φτ has elements (φτ )iτ(cid:48) = φi(x)δτ τ(cid:48). Evaluating the
τ φτ φT
difference (G−1 + σ−2
τ )−1 − G with the help of the Woodbury identity and approximating it
τ φτ φT
with a derivative gives

∂G
∂nτ

τ G
= − Gφτ φT
τ Gφτ
σ2
τ + φT

This needs to be averaged over the new example and all previous ones.
averaging numerator and denominator separately we get

If we approximate by

∂G
∂nτ

=

(4)
Here we have exploited for the average over x that the matrix (cid:104)φτ φT
τ (cid:105)x has (i  τ(cid:48))  (j  τ(cid:48)(cid:48))-entry
τ (cid:105)x = Pτ . We have also used the
(cid:104)φi(x)φj(x)(cid:105)xδτ τ(cid:48)δτ τ(cid:48)(cid:48) = δijδτ τ(cid:48)δτ τ(cid:48)(cid:48)  hence simply (cid:104)φτ φT
auxiliary parameters to rewrite −(cid:104)GPτG(cid:105) = ∂(cid:104)G(cid:105)/∂vτ = ∂G/∂vτ . Finally  multiplying (4) by Pτ(cid:48)
and taking the trace gives the set of quasi-linear partial differential equations

σ2
τ + tr Pτ G

1

∂G
∂vτ

∂τ(cid:48)
∂nτ

=

1

σ2
τ + τ

∂τ(cid:48)
∂vτ

(5)

The remaining task is now to ﬁnd the functions τ (n1  . . .   nT   v1  . . .   vT ) by solving these differ-
ential equations. We initially attempted to do this by tracking the τ as examples are added one
task at a time  but the derivation is laborious already for T = 2 and becomes prohibitive beyond.
Far more elegant is to adapt the method of characteristics to the present case. We need to ﬁnd a
2T -dimensional surface in the 3T -dimensional space (n1  . . .   nT   v1  . . .   vT   1  . . .   T )  which is
speciﬁed by the T functions τ (. . .). A small change (δn1  . . .   δnT   δv1  . . .   δvT   δ1  . . .   δT ) in
all 3T coordinates is tangential to this surface if it obeys the T constraints (one for each τ(cid:48))

(cid:88)

(cid:18) ∂τ(cid:48)

∂nτ

τ

δτ(cid:48) =

δnτ +

∂τ(cid:48)
∂vτ

δvτ

From (5)  one sees that this condition is satisﬁed whenever δτ = 0 and δnτ = −δvτ (σ2
τ + τ )
It follows that all the characteristic curves given by τ (t) = τ 0 = const.  vτ (t) = vτ 0(1 − t) 
τ + τ 0) t for t ∈ [0  1] are tangential to the solution surface for all t  so lie within
nτ (t) = vτ 0(σ2
this surface if the initial point at t = 0 does. Because at t = 0 there are no training examples
(nτ (0) = 0)  this initial condition is satisﬁed by setting

(cid:19)

(cid:33)−1

τ 0 = tr Pτ

L−1 +

vτ(cid:48) 0Pτ(cid:48)

(cid:88)

τ(cid:48)

(cid:32)

(cid:33)−1

(cid:32)

(cid:88)

τ(cid:48)

Because τ (t) is constant along the characteristic curve  we get by equating the values at t = 0 and
t = 1

τ 0 = tr Pτ

L−1 +

vτ(cid:48) 0Pτ(cid:48)

= τ ({nτ(cid:48) = vτ(cid:48) 0(σ2

Expressing vτ(cid:48) 0 in terms of nτ(cid:48) gives then

(cid:32)

τ = tr Pτ

L−1 +

(cid:88)

τ(cid:48)

nτ(cid:48)
σ2
τ(cid:48) + τ(cid:48)

Pτ(cid:48)

τ(cid:48) + τ(cid:48) 0)} {vτ(cid:48) = 0})
(cid:33)−1

(6)

This is our main result: a closed set of T self-consistency equations for the average Bayes errors
τ . Given L as deﬁned by the eigenvalues λi of the covariance function  the noise levels σ2
τ and the

4

number of examples nτ for each task  it is straightforward to solve these equations numerically to
ﬁnd the average Bayes error τ for each task.
The r.h.s. of (6) is easiest to evaluate if we view the matrix inside the brackets as consisting of
M × M blocks of size T × T (which is the reverse of the picture we have used so far). The matrix is
then block diagonal  with the blocks corresponding to different eigenvalues λi. Explicitly  because
L−1 = D−1 ⊗ Λ−1  one has

τ =

i D−1 + diag({
λ−1

nτ(cid:48)
σ2
τ(cid:48) + τ(cid:48)

})

(7)

(cid:19)−1

τ τ

(cid:18)

(cid:88)

4 Results and discussion

i

We now consider the consequences of the approximate prediction (7) for multi-task learning curves
in GP regression. A trivial special case is the one of uncorrelated tasks  where D is diagonal. Here
one recovers T separate equations for the individual tasks as expected  which have the same form as
for single-task learning [13].

4.1 Pure transfer learning

Consider now the case of pure transfer learning  where one is learning a task of interest (say τ = 1)
purely from examples for other tasks. What is the lowest average Bayes error that can be obtained?
Somewhat more generally  suppose we have no examples for the ﬁrst T0 tasks  n1 = . . . = nT0 = 0 
but a large number of examples for the remaining T1 = T − T0 tasks. Denote E = D−1 and write
this in block form as

(cid:18) E00 E01

ET

01 E11

(cid:19)

E =

i

00 + E−1

01E−1

00 E01)−1ET
00 )11 = (cid:104)C(x  x)(cid:105)(E−1

and add in the lower right block a diagonal matrix N = diag({nτ /(σ2

Now multiply by λ−1
τ +
τ )}τ =T0+1 ... T ). The matrix inverse in (7) then has top left block λi[E−1
00 E01(λiN +
E11 − ET
01E−1
00 ]. As the number of examples for the last T1 tasks grows  so do all
(diagonal) elements of N. In the limit only the term λiE−1
00 survives  and summing over i gives
1 = tr Λ(E−1
00 )11. The Bayes error on task 1 cannot become lower than this 
placing a limit on the beneﬁts of pure transfer learning. That this prediction of the approximation
(7) for such a lower limit is correct can also be checked directly: once the last T1 tasks fτ (x)
(τ = T0 + 1  . . . T ) have been learn perfectly  the posterior over the ﬁrst T0 functions is  by standard
Gaussian conditioning  a GP with covariance C(x  x(cid:48))(E00)−1. Averaging the posterior variance of
f1(x) then gives the Bayes error on task 1 as 1 = (cid:104)C(x  x)(cid:105)(E−1
This analysis can be extended to the case where there are some examples available also for the ﬁrst
T0 tasks. One ﬁnds for the generalization errors on these tasks the prediction (7) with D−1 replaced
by E00. This is again in line with the above form of the GP posterior after perfect learning of the
remaining T1 tasks.

00 )11  as found earlier.

4.2 Two tasks

We next analyse how well the approxiation (7) does in predicting multi-task learning curves for
T = 2 tasks. Here we have the work of Chai [21] as a baseline  and as there we choose

(cid:19)

(cid:18) 1 ρ

ρ 1

D =

The diagonal elements are ﬁxed to unity  as in a practical application where one would scale both
task functions f1(x) and f2(x) to unit variance; the degree of correlation of the tasks is controlled
by ρ. We ﬁx π2 = n2/n and plot learning curves against n. In numerical simulations we ensure
integer values of n1 and n2 by setting n2 = (cid:98)nπ2(cid:99)  n1 = n − n2; for evaluation of (7) we use
directly n2 = nπ2  n1 = n(1 − π2). For simplicity we consider equal noise levels σ2
2 = σ2.
As regards the covariance function and input distribution  we analyse ﬁrst the scenario studied
in [21]: a squared exponential (SE) kernel C(x  x(cid:48)) = exp[−(x − x(cid:48))2/(2l2)] with lengthscale
l  and one-dimensional inputs x with a Gaussian distribution N (0  1/12). The kernel eigenvalues λi

1 = σ2

5

Figure 1: Average Bayes error for task 1 for two-task GP regression with kernel lengthscale l = 0.01 
noise level σ2 = 0.05 and a fraction π2 = 0.75 of examples for task 2. Solid lines: numerical
simulations; dashed lines: approximation (7). Task correlation ρ2 = 0  0.25  0.5  0.75  1 from top to
bottom. Left: SE covariance function  Gaussian input distribution. Middle: SE covariance  uniform
inputs. Right: OU covariance  uniform inputs. Log-log plots (insets) show tendency of asymptotic
uselessness  i.e. bunching of the ρ < 1 curves towards the one for ρ = 0; this effect is strongest for
learning of smooth functions (left and middle).

are known explicitly from [22] and decay exponentially with i. Figure 1(left) compares numerically
simulated learning curves with the predictions for 1  the average Bayes error on task 1  from (7).
Five pairs of curves are shown  for ρ2 = 0  0.25  0.5  0.75  1. Note that the two extreme values
represent single-task limits  where examples from task 2 are either ignored (ρ = 0) or effectively
treated as being from task 1 (ρ = 1). Our predictions lie generally below the true learning curves 
but qualitatively represent the trends well  in particular the variation with ρ2. The curves for the dif-
ferent ρ2 values are fairly evenly spaced vertically for small number of examples  n  corresponding
to a linear dependence on ρ2. As n increases  however  the learning curves for ρ < 1 start to bunch
together and separate from the one for the fully correlated case (ρ = 1). The approximation (7)
correctly captures this behaviour  which is discussed in more detail below.
Figure 1(middle) has analogous results for the case of inputs x uniformly distributed on the interval
[0  1]; the λi here decay exponentially with i2 [17]. Quantitative agreement between simulations
and predictions is better for this case. The discussion in [17] suggests that this is because the
approximation method we have used implicitly neglects spatial variation of the dataset-averaged
posterior variance (cid:104)Vτ (x)(cid:105); but for a uniform input distribution this variation will be weak except
near the ends of the input range [0  1]. Figure 1(right) displays similar results for an OU kernel
C(x  x(cid:48)) = exp(−|x − x(cid:48)|/l)  showing that our predictions also work well when learning rough
(nowhere differentiable) functions.

4.3 Asymptotic uselessness

The two-task results above suggest that multi-task learning is less useful asymptotically: when the
number of training examples n is large  the learning curves seem to bunch towards the curve for
ρ = 0  where task 2 examples are ignored  except when the two tasks are fully correlated (ρ = 1).
We now study this effect.
When the number of examples for all tasks becomes large  the Bayes errors τ will become small
τ in (7). One then has an explicit
and eventually be negligible compared to the noise variances σ2
If we write  for T tasks 
prediction for each τ   without solving T self-consistency equations.
nτ = nπτ with πτ the fraction of examples for task τ  and set γτ = πτ /σ2
τ   then for large n
(Γ1/2DΓ1/2)−1 + nI]−1Γ−1/2)τ τ

(cid:0)λ−1
i D−1 + nΓ(cid:1)−1

τ =(cid:80)

τ τ =(cid:80)

(8)

i

where Γ = diag(γ1  . . .   γT ). Using an eigendecomposition of the symmetric matrix Γ1/2DΓ1/2 =

(cid:80)T

a=1 δavavT

a   one then shows in a few lines that (8) can be written as

τ ≈ γ−1

τ

a(va τ )2δag(nδa)

(9)

i

i(Γ−1/2[λ−1
(cid:80)

6

0100200300400500n00.20.40.60.81ε10100200300400500n00.20.40.60.81ε10100200300400500n00.20.40.60.81ε1110000n0.0011ε1110000n0.0011ε111000n0.011ε1Figure 2: Left: Bayes error (parameters as in Fig. 1(left)  with n = 500) vs ρ2. To focus on the
error reduction with ρ  r = [1(ρ) − 1(1)]/[1(0) − 1(1)] is shown. Circles: simulations; solid
line: predictions from (7). Other lines: predictions for larger n  showing the approach to asymptotic
uselessness in multi-task learning of smooth functions. Inset: Analogous results for rough functions
(parameters as in Fig. 1(right)). Right: Learning curve for many-task learning (T = 200  parameters
otherwise as in Fig. 1(left) except ρ2 = 0.8). Notice the bend around 1 = 1 − ρ = 0.106. Solid
line: simulations (steps arise because we chose to allocate examples to tasks in order τ = 1  . . .   T
rather than randomly); dashed line: predictions from (7). Inset: Predictions for T = 1000  with
asymptotic forms  = 1− ρ + ρ˜ and  = (1− ρ)¯ for the two learning stages shown as solid lines.

where g(h) = tr (Λ−1 + h)−1 = (cid:80)

i(λ−1

i + h)−1 and va τ is the τ-th component of the a-th
eigenvector va. This is the general asymptotic form of our prediction for the average Bayes error
for task τ.
To get a more explicit result  consider the case where sample functions from the GP prior have
(mean-square) derivatives up to order r. The kernel eigenvalues λi then decay as1 i−(2r+2) for large
i  and using arguments from [17] one deduces that g(h) ∼ h−α for large h  with α = (2r +1)/(2r +
2). In (9) we can then write  for large n  g(nδa) ≈ (δa/γτ )−αg(nγτ ) and hence

τ ≈ g(nγτ ){(cid:80)

a(va τ )2(δa/γτ )1−α}

(10)

When there is only a single task  δ1 = γ1 and this expression reduces to 1 = g(nγ1) = g(n1/σ2
1).
τ ) is the error we would get by ignoring all examples from tasks other than τ 
Thus g(nγτ ) = g(nτ /σ2
and the term in {. . .} in (10) gives the “multi-task gain”  i.e. the factor by which the error is reduced
because of examples from other tasks. (The absolute error reduction always vanishes trivially for
n → ∞  along with the errors themselves.) One observation can be made directly. Learning of very
smooth functions  as deﬁned e.g. by the SE kernel  corresponds to r → ∞ and hence α → 1  so
the multi-task gain tends to unity: multi-task learning is asymptotically useless. The only exception
occurs when some of the tasks are fully correlated  because one or more of the eigenvalues δa of
Γ1/2DΓ1/2 will then be zero.
Fig. 2(left) shows this effect in action  plotting Bayes error against ρ2 for the two-task setting of
Fig. 1(left) with n = 500. Our predictions capture the nonlinear dependence on ρ2 quite well 
though the effect is somewhat weaker in the simulations. For larger n the predictions approach a
curve that is constant for ρ < 1  signifying negligible improvement from multi-task learning except
at ρ = 1. It is worth contrasting this with the lower bound from [21]  which is linear in ρ2. While
this provides a very good approximation to the learning curves for moderate n [21]  our results here
show that asymptotically this bound can become very loose.
When predicting rough functions  there is some asymptotic improvement to be had from multi-task
learning  though again the multi-task gain is nonlinear in ρ2: see Fig. 2(left  inset) for the OU case 
which has r = 1). A simple expression for the gain can be obtained in the limit of many tasks  to
which we turn next.

1See the discussion of Sacks-Ylvisaker conditions in e.g. [1]; we consider one-dimensional inputs here

though the discussion can be generalized.

7

00.20.40.60.81ρ200.51r101001000n0.11ε1101001000n0.11εn=5005000500004.4 Many tasks
We assume as for the two-task case that all inter-task correlations  Dτ τ(cid:48) with τ (cid:54)= τ(cid:48)  are equal to
ρ  while Dτ τ = 1. This setup was used e.g. in [23]  and can be interpreted as each task having a
component proportional to
ρ of a shared latent function  with an independent task-speciﬁc signal
in addition. We assume for simplicity that we have the same number nτ = n/T of examples for
each task  and that all noise levels are the same  σ2
τ = σ2. Then also all Bayes errors τ =  will be
the same. Carrying out the matrix inverses in (7) explicitly  one can then write this equation as

√

where gT (h  ρ) is related to the single-task function g(h) from above by

(cid:18)

(cid:19)

 = gT (n/(σ2 + )  ρ)

(11)

gT (h  ρ) =

T − 1
T

(1 − ρ)g(h(1 − ρ)/T ) +

1 − ρ
T

ρ +

g(h[ρ + (1 − ρ)/T ])

(12)

Now consider the limit T → ∞ of many tasks.
If n and hence h = n/(σ2 + ) is kept
ﬁxed  gT (h  ρ) → (1 − ρ) + ρg(hρ); here we have taken g(0) = 1 which corresponds to
tr Λ = (cid:104)C(x  x)(cid:105)x = 1 as in the examples above. One can then deduce from (11) that the Bayes
error for any task will have the form  = (1− ρ) + ρ˜  where ˜ decays from one to zero with increas-
ing n as for a single task  but with an effective noise level ˜σ2 = (1 − ρ + σ2)/ρ. Remarkably  then 
even though here n/T → 0 so that for most tasks no examples have been seen  the Bayes error for
each task decreases by “collective learning” to a plateau of height 1− ρ. The remaining decay of  to
zero happens only once n becomes of order T . Here one can show  by taking T → ∞ at ﬁxed h/T
in (12) and inserting into (11)  that  = (1 − ρ)¯ where ¯ again decays as for a single task but with
an effective number of examples ¯n = n/T and effective noise level ¯σ2/(1 − ρ). This ﬁnal stage of
learning therefore happens only when each task has seen a considerable number of exampes n/T .
Fig. 2(right) validates these predictions against simulations  for a number of tasks (T = 200) that
is in the same ballpark as in the many-tasks application example of [24]. The inset for T = 1000
shows clearly how the two learning curve stages separate as T becomes larger.
Finally we can come back to the multi-task gain in the asymptotic stage of learning. For GP priors
with sample functions with derivatives up to order r as before  the function ¯ from above will decay
as (¯n/¯σ2)−α; since  = (1 − ρ)¯ and ¯σ2 = σ2/(1 − ρ)  the Bayes error  is then proportional
to (1 − ρ)1−α. This multi-task gain again approaches unity for ρ < 1 for smooth functions (α =
(2r + 1)/(2r + 2) → 1). Interestingly  for rough functions (α < 1)  the multi-task gain decreases

for small ρ2 as 1 − (1 − α)(cid:112)ρ2 and so always lies below a linear dependence on ρ2 initially. This

shows that a linear-in-ρ2 lower error bound cannot generally apply to T > 2 tasks  and indeed one
can verify that the derivation in [21] does not extend to this case.

5 Conclusion

We have derived an approximate prediction (7) for learning curves in multi-task GP regression  valid
for arbitrary inter-task correlation matrices D. This can be evaluated explicitly knowing only the
kernel eigenvalues  without sampling or recourse to single-task learning curves. The approximation
shows that pure transfer learning has a simple lower error bound  and provides a good qualitative
account of numerically simulated learning curves. Because it can be used to study the asymptotic
behaviour for large training sets  it allowed us to show that multi-task learning can become asymp-
totically useless: when learning smooth functions it reduces the asymptotic Bayes error only if tasks
are fully correlated. For the limit of many tasks we found that  remarkably  some initial “collective
learning” is possible even when most tasks have not seen examples. A much slower second learning
stage then requires many examples per task. The asymptotic regime of this also showed explicitly
that a lower error bound that is linear in ρ2  the square of the inter-task correlation  is applicable
only to the two-task setting T = 2.
In future work it would be interesting to use our general result to investigate in more detail the con-
sequences of speciﬁc choices for the inter-task correlations D  e.g. to represent a lower-dimensional
latent factor structure. One could also try to deploy similar approximation methods to study the case
of model mismatch  where the inter-task correlations D would have to be learned from data. More
challenging  but worthwhile  would be an extension to multi-task covariance functions where task
and input-space correlations to not factorize.

8

References
[1] C K I Williams and C Rasmussen. Gaussian Processes for Machine Learning. MIT Press  Cambridge 

MA  2006.

[2] J Baxter. A model of inductive bias learning. J. Artif. Intell. Res.  12:149–198  2000.
[3] S Ben-David and R S Borbely. A notion of task relatedness yielding provable multiple-task learning

guarantees. Mach. Learn.  73(3):273–287  December 2008.

[4] Y W Teh  M Seeger  and M I Jordan. Semiparametric latent factor models. In Workshop on Artiﬁcial

Intelligence and Statistics 10  pages 333–340. Society for Artiﬁcial Intelligence and Statistics  2005.

[5] E V Bonilla  F V Agakov  and C K I Williams. Kernel multi-task learning using task-speciﬁc features.
In Proceedings of the 11th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS).
Omni Press  2007.

[6] E V Bonilla  K M A Chai  and C K I Williams. Multi-task Gaussian process prediction. In J C Platt 
D Koller  Y Singer  and S Roweis  editors  NIPS 20  pages 153–160  Cambridge  MA  2008. MIT Press.
In
D Koller  D Schuurmans  Y Bengio  and L Bottou  editors  NIPS 21  pages 57–64  Cambridge  MA 
2009. MIT Press.

[7] M Alvarez and N D Lawrence. Sparse convolved Gaussian processes for multi-output regression.

[8] G Leen  J Peltonen  and S Kaski. Focused multi-task learning using Gaussian processes. In Dimitrios
Gunopulos  Thomas Hofmann  Donato Malerba  and Michalis Vazirgiannis  editors  Machine Learning
and Knowledge Discovery in Databases  volume 6912 of Lecture Notes in Computer Science  pages 310–
325. Springer Berlin  Heidelberg  2011.

[9] M A ´Alvarez  L Rosasco  and N D Lawrence. Kernels for vector-valued functions: a review. Foundations

and Trends in Machine Learning  4:195–266  2012.

[10] A Maurer. Bounds for linear multi-task learning. J. Mach. Learn. Res.  7:117–139  2006.
[11] M Opper and F Vivarelli. General bounds on Bayes errors for regression with Gaussian processes. In
M Kearns  S A Solla  and D Cohn  editors  NIPS 11  pages 302–308  Cambridge  MA  1999. MIT Press.
[12] G F Trecate  C K I Williams  and M Opper. Finite-dimensional approximation of Gaussian processes. In
M Kearns  S A Solla  and D Cohn  editors  NIPS 11  pages 218–224  Cambridge  MA  1999. MIT Press.
[13] P Sollich. Learning curves for Gaussian processes. In M S Kearns  S A Solla  and D A Cohn  editors 

NIPS 11  pages 344–350  Cambridge  MA  1999. MIT Press.

[14] D Malzahn and M Opper. Learning curves for Gaussian processes regression: A framework for good
approximations. In T K Leen  T G Dietterich  and V Tresp  editors  NIPS 13  pages 273–279  Cambridge 
MA  2001. MIT Press.

[15] D Malzahn and M Opper. A variational approach to learning curves. In T G Dietterich  S Becker  and

Z Ghahramani  editors  NIPS 14  pages 463–469  Cambridge  MA  2002. MIT Press.

[16] D Malzahn and M Opper. Statistical mechanics of learning: a variational approach for real data. Phys.

Rev. Lett.  89:108302  2002.

[17] P Sollich and A Halees. Learning curves for Gaussian process regression: approximations and bounds.

Neural Comput.  14(6):1393–1428  2002.

[18] P Sollich. Gaussian process regression with mismatched models.

In T G Dietterich  S Becker  and

Z Ghahramani  editors  NIPS 14  pages 519–526  Cambridge  MA  2002. MIT Press.

[19] P Sollich. Can Gaussian process regression be made robust against model mismatch? In Deterministic
and Statistical Methods in Machine Learning  volume 3635 of Lecture Notes in Artiﬁcial Intelligence 
pages 199–210. Springer Berlin  Heidelberg  2005.

[20] M Urry and P Sollich. Exact larning curves for Gaussian process regression on large random graphs. In
J Lafferty  C K I Williams  J Shawe-Taylor  R S Zemel  and A Culotta  editors  NIPS 23  pages 2316–2324 
Cambridge  MA  2010. MIT Press.

[21] K M A Chai. Generalization errors and learning curves for regression with multi-task Gaussian processes.
In Y Bengio  D Schuurmans  J Lafferty  C K I Williams  and A Culotta  editors  NIPS 22  pages 279–287 
2009.

[22] H Zhu  C K I Williams  R J Rohwer  and M Morciniec. Gaussian regression and optimal ﬁnite dimensional

linear models. In C M Bishop  editor  Neural Networks and Machine Learning. Springer  1998.

[23] E Rodner and J Denzler. One-shot learning of object categories using dependent Gaussian processes.
In Michael Goesele  Stefan Roth  Arjan Kuijper  Bernt Schiele  and Konrad Schindler  editors  Pattern
Recognition  volume 6376 of Lecture Notes in Computer Science  pages 232–241. Springer Berlin  Hei-
delberg  2010.

[24] T Heskes. Solving a huge number of similar tasks: a combination of multi-task learning and a hierarchi-
cal Bayesian approach. In Proceedings of the Fifteenth International Conference on Machine Learning
(ICML’98)  pages 233–241. Morgan Kaufmann  1998.

9

,Hongteng Xu
Hongyuan Zha