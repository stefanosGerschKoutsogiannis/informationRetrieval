2019,Controlling Neural Level Sets,The level sets of neural networks represent fundamental properties such as decision boundaries of classifiers and are used to model non-linear manifold data such as curves and surfaces. Thus  methods for controlling the neural level sets could find many applications in machine learning.

In this paper we present a simple and scalable approach to directly control level sets of a deep neural network. Our method consists of two parts: (i) sampling of the neural level sets  and (ii) relating the samples' positions to the network parameters. The latter is achieved by a sample network that is constructed by adding a single fixed linear layer to the original network. In turn  the sample network can be used to incorporate the level set samples into a loss function of interest.
 
We have tested our method on three different learning tasks: improving generalization to unseen data  training networks robust to adversarial attacks  and curve and surface reconstruction from point clouds. For surface reconstruction  we produce high fidelity surfaces directly from raw 3D point clouds. When training small to medium networks to be robust to adversarial attacks we obtain robust accuracy comparable to state-of-the-art methods.,Controlling Neural Level Sets

Matan Atzmon  Niv Haim  Lior Yariv  Ofer Israelov  Haggai Maron  Yaron Lipman

Weizmann Institute of Science

Rehovot  Israel

Abstract

The level sets of neural networks represent fundamental properties such as decision
boundaries of classiﬁers and are used to model non-linear manifold data such as
curves and surfaces. Thus  methods for controlling the neural level sets could ﬁnd
many applications in machine learning.
In this paper we present a simple and scalable approach to directly control level
sets of a deep neural network. Our method consists of two parts: (i) sampling of the
neural level sets  and (ii) relating the samples’ positions to the network parameters.
The latter is achieved by a sample network that is constructed by adding a single
ﬁxed linear layer to the original network. In turn  the sample network can be used
to incorporate the level set samples into a loss function of interest.
We have tested our method on three different learning tasks: improving generaliza-
tion to unseen data  training networks robust to adversarial attacks  and curve and
surface reconstruction from point clouds. For surface reconstruction  we produce
high ﬁdelity surfaces directly from raw 3D point clouds. When training small to
medium networks to be robust to adversarial attacks we obtain robust accuracy
comparable to state-of-the-art methods.

1

Introduction

The level sets of a Deep Neural Network (DNN) are known to capture important characteristics
and properties of the network. A popular example is when the network F (x; θ) : Rd × Rm → Rl
represents a classiﬁer  θ are its learnable parameters  fi(x; θ) are its logits (the outputs of the ﬁnal
linear layer)  and the level set

S(θ) =

(cid:27)

{fi} = 0

(cid:26)

x ∈ Rd(cid:12)(cid:12)(cid:12) fj − max
x ∈ Rd(cid:12)(cid:12)(cid:12) F = 0
(cid:110)
(cid:111)

i(cid:54)=j

S(θ) =

(1)

(2)

represents the decision boundary of the j-th class. Another recent example is modeling a manifold
(e.g.  a curve or a surface in R3) using a level set of a neural network (e.g.  [24]). That is 

represents (generically) a manifold of dimension d − l in Rd.
The goal of this work is to provide practical means to directly control and manipulate neural level
sets S(θ)  as exempliﬁed in Equations 1  2. The main challenge is how to incorporate S(θ) in a
differentiable loss function. Our key observation is that given a sample p ∈ S(θ)  its position can be
associated to the network parameters: p = p(θ)  in a differentiable and scalable way. In fact  p(θ) is
itself a neural network that is obtained by an addition of a single linear layer to F (x; θ); we call these
networks sample networks. Sample networks  together with an efﬁcient mechanism for sampling the
level set  {pj(θ)} ⊂ S(θ)  can be incorporated in general loss functions as a proxy for the level set
S(θ).

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

(a)

(b)

(c)

(d)

Figure 1: Our method applied to binary classiﬁcation in 2D. Blue and red dots represent positive
and negative examples respectively. (a) standard cross-entropy loss baseline; (b) using our method
to move the decision boundary at least ε away from the training set in L∞ norm; (c) same for L2
norm; (d) a geometrical adaptation of SVM soft-margin loss to neural networks using our method  the
+1 −1 level sets are marked in light red and blue  respectively. Note that in (b) (c) (d) we achieve
decision boundaries that seem to better explain the training examples compared to (a).

We apply our method of controlling the neural level sets to two seemingly different learning tasks:
controlling decision boundaries of classiﬁers (Equation 1) and reconstructing curves and surfaces
from point cloud data (Equation 2).
At ﬁrst  we use our method to improve the generalization and adversarial robustness in classiﬁcation
tasks. In these tasks  the distance between the training examples and the decision boundary is an
important quantity called the margin. Margins are traditionally desired to be as large as possible to
improve generalization [7  10] and adversarial robustness [10]. Usually  margins are only controlled
indirectly by loss functions that measure network output values of training examples (e.g.  cross
entropy loss). Recently  researchers have been working on optimizations with more direct control of
the margin using linearization techniques [13  21  10]  regularization [26]  output-layer margin [27] 
or using margin gradients [9]. We suggest controlling the margin by sampling the decision boundary 
constructing the sample network  and measuring distances between samples and training examples
directly. By applying this technique to train medium to small-size networks against adversarial
perturbations we achieved comparable robust accuracy to state-of-the-art methods.
To improve generalization when learning from small amounts of data  we devise a novel geometrical
formulation of the soft margin SVM loss to neural networks. This loss aims at directly increasing
the input space margin  in contrast to standard deep network hinge losses that deal with output
space margin [28  27]. The authors of [10] also suggest to increase input space margin to improve
generalization. Figure 1 shows 2D examples of training our adversarial robustness and geometric
SVM losses for networks.
In a different application  we use our method for the reconstruction of manifolds such as curves and
surfaces in R3 from point cloud data. The usage of neural networks for the modeling of surfaces
has recently become popular [12  29  6  2  24  22]. There are two main approaches: parametric and
implicit. The parametric approach uses networks as parameterization functions to the manifolds
[12  29]. The implicit approach represents the manifold as a level set of a neural network [24  6  22].
So far  implicit representations were learned using regression  given a signed distance function or
occupancy function computed directly from a ground truth surface. Unfortunately  for raw point
clouds in R3  computing the signed distance function or an occupancy function is a notoriously
difﬁcult task [4]. In this paper we show that by using our sample network to control the neural level
sets we can reconstruct curves and surfaces directly from point clouds in R3.
Lastly  to theoretically justify neural level sets for modeling manifolds or arbitrary decision boundaries 
we prove a geometric version of the universality property of MLPs [8  14]: any piecewise linear
hyper-surface in Rd (i.e.  a d− 1 manifold built out of a ﬁnite number of linear pieces  not necessarily
bounded) can be precisely represented as a neural level set of a suitable MLP.

2

2 Sample Network
Given a neural network F (x; θ) : Rd × Rm → Rl its 0 ∈ Rl level set is deﬁned by

(cid:110)

(cid:12)(cid:12)(cid:12) F (x; θ) = 0
(cid:111)

.

S(θ) :=

x

(3)
We denote by DxF (p; θ) ∈ Rl×d the matrix of partial derivatives of F with respect to x. Assuming
that θ is ﬁxed  F (p; θ) = 0 and that DxF (p; θ) is of full rank l (l (cid:28) d)  a corollary of the Implicit
Function Theorem [15] implies that S(θ) is a d − l dimensional manifold in the vicinity of p ∈ S(θ).
Our goal is to incorporate the neural level set S(θ) in a differentiable loss. We accomplish that by
performing the following procedure at each training iteration: (i) Sample n points on the level set:
pi ∈ S(θ)  i ∈ [n]; (ii) Build the sample network pi(θ)  i ∈ [n]  by adding a ﬁxed linear layer to the
network F (x; θ); and (iii) Incorporate the sample network in a loss function as a proxy for S(θ).
2.1 Level set sampling
To sample S(θ) at some θ = θ0  we start with a set of n points pi  i ∈ [n] sampled from some
probability measure in Rd. Next  we perform generalized Newton iterations [3] to move each point p
towards S(θ0):

pnext = p − DxF (p; θ0)†F (p; θ0) 

(4)
where DxF (p  θ0)† is the Moore-Penrose pseudo-inverse of DxF (p  θ0). The generalized Newton
step solves the under-determined (l (cid:28) d) linear system F (p; θ0) + DxF (p; θ0)(pnext − p) = 0. To
motivate this particular solution choice we show that the generalized Newton step applied to a linear
function is an orthogonal projection onto its zero level set (see proof in the supplementary material):
Lemma 1. Let (cid:96)(x) = Ax + b  A ∈ Rl×d  b ∈ Rl  (cid:96) < d  and A is of full rank l. Then
Equation 4 applied to F (x) = (cid:96)(x) is an orthogonal projection on the zero level set of (cid:96)  namely  on
{x | (cid:96)(x) = 0}.
For l > 1 the computation of DxF (p; θ0)† requires inverting an l × l matrix; in this paper l ∈ {1  2}.
The directions DxF (pi; θ0) can be computed efﬁciently using back-propagation where the points pi 
i ∈ [n] are grouped into batches. We performed 10 − 20 iterations of Equation 4 for each pi  i ∈ [n].
Scalar networks. For scalar networks  i.e.  l = 1  DxF ∈ R1×d  a direct computation shows that

DxF (p; θ0)† =

DxF (p; θ0)T
(cid:107)DxF (p; θ0)(cid:107)2 .

(5)

That is  the point p moves towards the level set S(θ0) by going in the direction of the steepest descent
(or ascent) of F .
It is worth mentioning that the projection-on-level-sets formula in the case of l = 1 has already
been developed in [23] and was used to ﬁnd adversarial perturbations; our result generalizes to the
intersection of several level sets and shows that this procedure is an instantiation of the generalized
Newton algorithm.
The generalized Newton method (similarly to Newton method) is not guaranteed to ﬁnd a point on the
zero level set. We denote by ci = F (pi; θ0) the level set values of the ﬁnal point pi; in the following 
we use also points that failed to be projected with their level set ci. Furthermore  we found that the
generalized Newton method usually does ﬁnd zeros of neural networks but these can be far from the
initial projection point. Other  less efﬁcient but sometimes more robust ways to project on neural
level sets could be using gradient descent on (cid:107)F (x; θ)(cid:107) or zero ﬁnding in the direction of a successful
PGD attack [17  20] as done in [9]. In our robust networks application (Section 3.2) we have used a
similar approach with the false position method.

Relation to Elsayed et al. [10]. The authors of [10] replace the margin distance with distance to
the linearized network  while our approach is to directly sample the actual network’s level set and
move it explicitly. Speciﬁcally  for L2 norm (p = 2) Elsayed’s method is similar to our method using
a single generalized Newton iteration  Equation 4.

3

2.2 Differentiable sample position
Next  we would like to relate each sample p  belonging to some level set F (p; θ0) = c  to the network
parameters θ. Namely  p = p(θ). The functional dependence of a sample p on θ is deﬁned by
p(θ0) = p and F (p(θ); θ) = c  for θ in some neighborhood of θ0. The latter condition ensures that p
stays on the c level set as the network parameters θ change. As only ﬁrst derivatives are used in the
optimization of neural networks  it is enough to replace this condition with its ﬁrst-order version. We
get the following two equations:

(cid:12)(cid:12)(cid:12)θ=θ0

p(θ0) = p ;

∂
∂θ

F (p(θ); θ) = 0.

(6)

Using the chain rule  the second condition in Equation 6 reads:

(7)
This is a system of linear equations with d × m unknowns (the components of Dθp(θ0)) and l × m
equations. When d > l  this linear system is under-determined. Similarly to what is used in the
generalized Newton method  a minimal norm solution is given by the Moore-Penrose inverse:

DxF (p  θ0)Dθp(θ0) + DθF (p  θ0) = 0.

Dθp(θ0) = −DxF (p  θ0)†DθF (p  θ0).

(8)
The columns of the matrix Dθp(θ0) ∈ Rd×m describe the velocity of p(θ) w.r.t. each of the parameters
in θ. The pseudo-inverse selects the minimal norm solution that can be shown to represent  in this
case  a movement in the orthogonal direction to the level set (see supplementary material for a proof).
We reiterate that for scalar networks  where l = 1  DxF (pi; θ0)† has a simple closed-form expression 
as shown in Equation 5.

The sample network. A possible simple solution to Equation 6 would be to use the linear function
p(θ) = p + Dθp(θ0)(θ − θ0)  with Dθp(θ0) as deﬁned in Equation 8. Unfortunately  this would
require storing Dθp(θ0)  using at-least O(m) space (i.e.  the number of network parameters)  for
every projection point p. (We assume the number of output channels l of F is constant  which is the
case in this paper.) A much more efﬁcient solution is

p(θ) = p − DxF (p; θ0)† [F (p; θ) − c]  

(9)
that requires storing DxF (p; θ0)†  using only O(d) space  where d is the input space dimension  for
every projection point p. Furthermore  Equation 9 allows an efﬁcient implementation with a single
network
We call G the sample network. Note that a collection of samples pi  i ∈ [n] can be treated as a batch
input to G.

G(p  DxF (p; θ0)†; θ) := p(θ).

Incorporation of samples in loss functions

3
Once we have the sample network pi(θ)  i ∈ [n]  we can incorporate it in a loss function to control
the neural level set S(θ) in a desired way. We give three examples in this paper.
3.1 Geometric SVM
Support-vector machine (SVM) is a model which aims to train a linear binary classiﬁer that would
generalize well to new data by combining the hinge loss and a large margin term. It can be interpreted
as encouraging large distances between training examples and the decision boundary. Speciﬁcally 
the soft SVM loss takes the form [7]:

loss(w  b) =

1
N

max{0  1 − yj(cid:96)(xj; w  b)} + λ(cid:107)w(cid:107)2
2  

(10)

where (xj  yj) ∈ Rd × {−1  1}  j ∈ [N ] is the binary classiﬁcation training data  and (cid:96)(x; w  b) =
wT x + b  w ∈ Rd  b ∈ R is the linear classiﬁer. We would like to generalize Equation 10 to
a deep network F : Rd × Rm → R towards the goal of increasing the network’s input space
margin  which is deﬁned as the minimal distance of training examples to the decision boundary

S(θ) =(cid:8)x ∈ Rd | F (x; θ) = 0(cid:9). Note that this is in strong contrast to standard deep network hinge

4

N(cid:88)

j=1

loss that works with the output space margin [28  27]  namely  measuring differences of output logits
when evaluating the network on training examples. For that reason  this type of loss function does
not penalize small input space margin  so long as it doesn’t damage the output-level classiﬁcation
performance on the training data. Using the input margin over the output margin may also provide
robustness to perturbations [10].
We now describe a new  geometric formulation of Equation 10  and use it to deﬁne the soft-SVM loss
for general neural networks. In the linear case  the following quantity serves as the margin:

(cid:107)w(cid:107)−1

where St(θ) =(cid:8)x ∈ Rd | F (x; θ) = t(cid:9)  and d(S(θ) St(θ)) is the distance between the level sets 

2 = d(S(θ) S1(θ)) = d(S(θ) S−1(θ))

which are two parallel hyper-planes. In the general case  however  level sets are arbitrary hyper-
surfaces which are not necessarily equidistant (i.e.  the distance when traveling from S to St does
not have to be constant across S). Hence  for each data sample x  we deﬁne the following margin
function:

d(cid:0)p(θ) S1(θ)(cid:1)  d(cid:0)p(θ) S−1(θ)(cid:1)(cid:111)
(cid:110)

∆(x; θ) = min

where p(θ) is the sample network of the projection of x onto S(θ0). Additionally  note that in the

linear case:(cid:12)(cid:12)wT x + b(cid:12)(cid:12) = d(x S(θ))/∆(x; θ). With these deﬁnitions in mind  Equation 10 can be

 

given the geometric generalized form:

loss(w  b) =

1
N

max

0  1 − sign(yjF (xj; θ))

d(xj  pj)
∆(xj; θ)

+

λ
N

∆(xj; θ)α 

(11)

(cid:27)

N(cid:88)

j=1

(cid:26)

N(cid:88)

j=1

N(cid:88)

1
N

(cid:110)

where F (x; θ) is a general classiﬁer (such as a neural network  in our applications). Note that in the
case where F (x; θ) is afﬁne  α = −2 and d = L2  Equation 11 reduces back to the regular SVM
loss  Equation 10. Figure 1d depicts the result of optimizing this loss in a 2D case  i.e.  xj ∈ R2; the
light blue and red curves represent S−1 and S1.

3.2 Robustness to adversarial perturbations

The goal of robust training is to prevent a change in a model’s classiﬁcation result when small
perturbations are applied to the input. Following [20] the attack model is speciﬁed by some set
S ⊂ Rd of allowed perturbations; in this paper we focus on the popular choice of L∞ perturbations 
that is S is taken to be the ε-radius L∞ ball  {x | (cid:107)x(cid:107)∞ ≤ ε}. Let (xj  yj) ∈ Rd ×L denote training
the decision boundary of label j. We deﬁne the loss

examples and labels  and let S j(θ) =(cid:8)x ∈ Rd | F j(x; θ) = 0(cid:9)  where F j(x; θ) = fj − maxi(cid:54)=j fi 

loss(θ) =

λj max

0  εj − sign(F yj (xj; θ))d(xj S yj (θ))

(12)

j=1

(cid:82)
where d(x S j) is some notion of a distance between x and S j  e.g.  miny∈S j (cid:107)x − y(cid:107)p or
S j (θ) (cid:107)x − y(cid:107)p dµ(y)  dµ is some probability measure on S j(θ). The parameter λj controls the
weighting between correct (i.e.  F yj (xj; θ) > 0) and incorrect (i.e.  F yj (xj; θ) < 0) classiﬁed
samples. We ﬁx λj = 1 for incorrectly classiﬁed samples and set λj to be the same for correctly
classiﬁed samples; The parameter εj controls the desired target distances; Similarly to [10]  the idea
of this loss is: (i) if xj is incorrectly classiﬁed  pull the decision boundary S yj toward xj; (ii) if xj is
classiﬁed correctly  push the decision boundary S yj to be within a distance of at-least εj from xj.
In our implementation we have used d(x S j) = ρ(x  p(θ))  where p = p(θ0) ∈ S j is a
of x  p (resp.)  i∗ = arg maxi∈[d] |DxF (p; θ0)|. This loss encourages p(θ) to move
in the direction of the axis (i.e.  i∗) that corresponds to the largest component in the
gradient DxF (p; θ0). Intuitively  as ρ(x  p) ≤ (cid:107)x − p(cid:107)∞  the loss pushes the level set S j to leave
the εj-radius L∞-ball in the direction of the axis that corresponds to the maximal speed of p(θ). The
inset depicts an example where this distance measure is more effective than the standard L∞ distance:
DxF (p; θ) is shown as black arrow and the selected axis i∗ as dashed line.

sample of this level set; ρ(x  p) =(cid:12)(cid:12)xi∗ − pi∗(cid:12)(cid:12)  and xi∗   pi∗ denote the i∗-th coordinates

(cid:111)

 

5

(cid:34)(cid:90)

(cid:88)

t∈T

St(θ)

(cid:12)(cid:12)(cid:12)d(x X ) − |t|(cid:12)(cid:12)(cid:12)p

loss(θ) =

(cid:35) 1

p

N(cid:88)

j=1

λ
N

dv(x)

+

|F (xj; θ)|  

(13)

In the case where the distance function miny∈S(θ) (cid:107)x − y(cid:107)p is used  the computation of its gradient
using Equation 8 coincides with the gradient derivation of [9] up to a sign difference. Still  our
derivation allows working with general level set points (i.e.  not just the closest) on the decision
boundary S j  and our sample network offers an efﬁcient implementation of these samples in a loss
function. Furthermore  we use the same loss for both correctly and incorrectly classiﬁed examples.

3.3 Manifold reconstruction
j=1 ⊂ Rd that samples  possibly with
Surface reconstruction. Given a point cloud X = {xj}N
noise  some surface M ⊂ R3  our goal is to ﬁnd parameters θ of a network F : R3 × Rm → R  so
that the neural level set S(θ) approximates M. Even more desirable is to have F approximate the
signed distance function to the unknown surface sampled by X . To that end  we would like the neural
level set St(θ)  t ∈ T to be of distance |t| to X   where T ⊂ R is some collection of desired level
set values. Let d(x X ) = minj∈[N ] (cid:107)x − xj(cid:107)2 be the distance between x and X . We consider the
reconstruction loss

where dv(x) is the normalized volume element on St(θ) and λ > 0 is a parameter. The ﬁrst part of
the loss encourages the t level set of F to be of distance |t| to X ; note that for t = 0 this reconstruction
error was used in level set surface reconstruction methods [33]. The second part of the loss penalizes
samples X outside the zero level set S(θ).

In case of approximating a manifold M ⊂ Rd with co-dimension greater
Curve reconstruction.
than 1  e.g.  a curve in R3  one cannot expect F to approximate the signed distance function as no
such function exists. Instead  we model the manifold via the level set of a vector-valued network
F : Rd × Rm → Rl whose zero level set is an intersection of l hyper-surfaces. As explained in
Section 2  this generically deﬁnes a d − l manifold. In that case we used the loss in Equation 13 with
T = {0}  namely  only encouraging the zero level set to be as close as possible to the samples X .
4 Universality
To theoretically support the usage of neural level sets for modeling manifolds or controlling decision
boundaries we provide a geometric universality result for multilayer perceptrons (MLP) with ReLU
activations. That is  the level sets of MLPs can represent any watertight piecewise linear hyper-surface
(i.e.  manifolds of co-dimension 1 in Rd that are boundaries of d-dimensional polytopes). More
speciﬁcally  we prove:
Theorem 1. Any watertight  not necessarily bounded  piecewise linear hypersurface M ⊂ Rd can
be exactly represented as the neural level set S of a multilayer perceptron with ReLU activations 
F : Rd → R.
The proof of this theorem is given in the supplementary material. Note that this theorem is a
geometrical version of Theorem 2.1 in [1]  asserting that MLPs with ReLU activations can represent
any piecewise linear continuous function.

5 Experiments
5.1 Classiﬁcation generalization

In this experiment  we show that when
training on small amounts of data  our
geometric SVM loss (see Equation 11)
generalizes better than the cross en-
tropy loss and the hinge loss. Exper-
iments were done on three datasets:
MNIST [18]  Fashion-MNIST [31]
and CIFAR10 [16]. For all datasets we arbitrarily merged the labels into two classes  resulting
in a binary classiﬁcation problem. We randomly sampled a fraction of the original training examples
and evaluated on the original test set.

Figure 2: Generalization from small fractions of the data.

(b)

(a)

(c)

6

0.10.20.30.512580859095HingeXentOursFraction of Training Data (%)Test accuracy (%)0.10.20.30.512591929394959697HingeXentOursFraction of Training Data (%)Test accuracy (%)0.10.20.30.512570758085HingeXentOursFraction of Training Data (%)Test accuracy (%)Attack

εtrain Test Acc. Rob. Acc. Xent Rob. Acc. Margin

Dataset Arch.
Method
99.34%
Standard
MNIST ConvNet-4a PGD40(εattack = 0.3)
99.35%
Madry et al. [20] MNIST ConvNet-4a PGD40(εattack = 0.3)
99.16%
Madry et al. [20] MNIST ConvNet-4a PGD40(εattack = 0.3)
98.97%
TRADES [32] MNIST ConvNet-4a PGD40(εattack = 0.3)
98.62%
TRADES [32] MNIST ConvNet-4a PGD40(εattack = 0.3)
99.35%
MNIST ConvNet-4a PGD40(εattack = 0.3)
Ours
Standard
83.67%
CIFAR10 ConvNet-4b PGD20 (εattack = 0.031)
Madry et al. [20] CIFAR10 ConvNet-4b PGD20 (εattack = 0.031) 0.031 71.86%
Madry et al. [20] CIFAR10 ConvNet-4b PGD20 (εattack = 0.031) 0.045 63.66%
TRADES [32]
CIFAR10 ConvNet-4b PGD20 (εattack = 0.031) 0.031 71.24%
CIFAR10 ConvNet-4b PGD20 (εattack = 0.031) 0.045 68.24%
TRADES [32]
CIFAR10 ConvNet-4b PGD20 (εattack = 0.031) 0.045 71.96%
Ours
Standard
CIFAR10 ResNet-18 PGD20 (εattack = 0.031)
93.18%
Madry et al. [20] CIFAR10 ResNet-18 PGD20 (εattack = 0.031) 0.031 81.0%
Madry et al. [20] CIFAR10 ResNet-18 PGD20 (εattack = 0.031) 0.045 74.97%
CIFAR10 ResNet-18 PGD20 (εattack = 0.031) 0.031 83.04%
TRADES [32]
CIFAR10 ResNet-18 PGD20 (εattack = 0.031) 0.045 79.52%
TRADES [32]
Ours
CIFAR10 ResNet-18 PGD20 (εattack = 0.031) 0.045 81.3%
Table 1: Results of different L∞-bounded attacks on models trained using our method (described in
Section 3.2) compared to other methods.

0.00%
96.11%
96.53%
96.74%
96.76%
97.35%
0.00%
38.18%
39.13%
38.4%
38.18%
38.54%
0.00%
46.58%
48.02%
51.36%
51.22%
43.17%

13.59%
96.04%
96.54%
96.75%
96.78%
99.23%
0.00%
39.84%
41.53%
41.89%
42.04%
38.45%
0.00%
47.29%
49.84%
53.31%
53.49%
79.74%

-
0.3
0.4
0.3
0.4
0.4
-

-

Due to the variability in the results  we rerun the experiment 100 times for MNIST and 20 times
for Fashion-MNIST and CIFAR10. We report the mean accuracy along with the standard deviation.
Figure 2 shows the test accuracy of our loss compared to the cross-entropy loss and hinge loss over
different training set sizes for MNIST (a)  Fashion-MNIST (b) and CIFAR10 (c). Our loss function
outperforms the standard methods.
For the implementation of Equation 11 we used α = −1  d = L∞  and approximated d(x St) ≈
(cid:107)x − pt(cid:107)∞  where pt denotes the projection of p on the level set St  t ∈ {−1  0  1} (see Section
2.1). The approximation of the term ∆(x; θ)  where x = xj is a train example  is therefore

min(cid:8)(cid:107)p0 − p−1(cid:107)∞ (cid:107)p0 − p1(cid:107)∞(cid:9). See supplementary material for further implementation details.

5.2 Robustness to adversarial examples

In this experiment we used our method with the loss in Equation 12 to train robust models on MNIST
[18] and CIFAR10 [16] datasets. For MNIST we used ConvNet-4a (312K params) used in [32]
and for CIFAR10 we used two architectures: ConvNet-4b (2.5M params) from [30] and ResNet-18
(11.2M params) from [32]. We report results using the loss in Equation 12 with the choice of εj ﬁxed
as εtrain in Table 1  λj to be 1  11 for MNIST and CIFAR10 (resp.)  and d = ρ as explained in Section
3.2. We evaluated our trained networks on L∞ bounded attacks with εattack radius using Projected
Gradient Descent (PGD) [17  20] and compared to networks with the same architectures trained
using the methods of Madry et al. [20] and TRADES [32]. We found our models to be robust to
PGD attacks based on the Xent loss; during the revision of this paper we discovered weakness of our

trained models to PGD attack based on the margin loss  i.e.  min(cid:8)F j(cid:9)  where F j = fj − maxi(cid:54)=j fi;

we attribute this fact to the large gradients created at the level set. Consequently  we added margin
loss attacks to our evaluation. The results are summarized in Table 1. Note that although superior in
robustness to Xent attacks we are comparable to baseline methods for margin attacks. Furthermore 
we believe the relatively low robust accuracy of our model when using the ResNet-18 architecture is
due to the fact that we didn’t speciﬁcally adapt our method to Batch-Norm layers.
In the supplementary material we provide tables summarizing robustness of our trained models
(MNIST ConvNet-4a and CIFAR10 ConvNet-4b) to black-box attacks; we log black-box attacks of
our and baseline methods [20  32] in an all-versus-all fashion. In general  we found that all black-box
attacks are less effective than the relevant white-box attacks  our method performs better when using
standard model black-box attacks  and that all three methods compared are in general similar in their
black-box robustness.

7

Figure 3: Point cloud reconstruction. Surface: (a) input point cloud; (b) AtlasNet [12] reconstruction;
(c) our result; (d) blow-ups; (e) more examples of our reconstruction. Curve: (f) bottom image shows
a curve reconstructed (black line) from a point cloud (green points) as an intersection of two scalar
level sets (top image); (g) bottom shows curve reconstruction from a point cloud with large noise 
where top image demonstrates the graceful completion of an open curve point cloud data.

Chamfer L1
23.56 ± 2.91
18.67 ± 3.45
11.54 ± 0.53
10.71 ± 0.63

Chamfer L2
17.69 ± 2.45
13.38 ± 2.66
7.89 ± 0.42
7.32 ± 0.46

Table 2: Surface reconstruction results.

AtlasNet-1 sphere
AtlasNet-1 patch
AtlasNet-25 patches
Ours

5.3 Surface and curve reconstruction
In this experiment we used our method to reconstruct curves and surfaces in R3 using only incomplete
point cloud data X ⊂ R3  which is an important task in 3D shape acquisition and processing. Each
point cloud is processed independently using the loss function described in Equation 13  which
encourages the zero level set of the network to pass through the point cloud. For surfaces  it also
moves other level sets to be of the correct distance to the point cloud.
For surface reconstruction  we trained on 10
human raw scans from the FAUST dataset [5] 
where each scan consists of ∼ 170K points
in R3. The scans include partial connectivity
information which we do not use. After con-
vergence  we reconstruct the mesh using the
marching cubes algorithm [19] sampled at a
resolution of [100]3. Table 2 compares our method with the recent method of [12] which also works
directly with point clouds. Evaluation is done using the Chamfer distance [11] computed between
30K uniformly sampled points from our and [12] reconstructed surfaces and the ground truth reg-
istrations provided by the dataset  with both L1  L2 norms. Numbers in the table are multiplied
by 103. We can see that our method outperforms its competitor; Figure 3b-3e show examples of
surfaces reconstructed from a point cloud (a batch of 10K points is shown in 3a) using our method
(in 3c  3d-right  3e)  and the method of [12] (in 3b  3d-left). Importantly  we note that there are
recent methods for implicit surface representation using deep neural networks [6  24  22]. These
methods use signed distance information and/or the occupancy function of the ground truth surfaces
and perform regression on these values. Our formulation  in contrast  allows working directly on the
more common  raw input of point clouds.
For curve reconstruction  we took a noisy sample of parametric curves in R3 and used similar network
to the surface case  except its output layer consists of two values. We trained the network with the
loss Equation 13  where T = {0}  using similar settings to the surface case. Figure 3f shows an
example of the input point cloud (in green) and the reconstructed curve (in black) (see bottom image) 
as well as the two hyper-surfaces of the trained network  the intersection of which deﬁnes the ﬁnal
reconstructed curve (see top image); 3g shows two more examples: reconstruction of a curve from
higher noise samples (see bottom image)  and reconstruction of a curve from partial curve data (see
top image); note how the network gracefully completes the curve.

6 Conclusions
We have introduced a simple and scalable method to incorporate level sets of neural networks into
a general family of loss functions. Testing this method on a wide range of learning tasks we found
the method particularly easy to use and applicable in different settings. Current limitations and
interesting venues for future work include: applying our method with the batch normalization layer
(requires generalization from points to batches); investigating control of intermediate layers’ level
sets; developing sampling conditions to ensure coverage of the neural level sets; and employing
additional geometrical regularization to the neural level sets (e.g.  penalize curvature).

8

Acknowledgments

This research was supported in part by the European Research Council (ERC Consolidator Grant 
"LiftMatch" 771136) and the Israel Science Foundation (Grant No. 1830/17).

References
[1] R. Arora  A. Basu  P. Mianjy  and A. Mukherjee. Understanding deep neural networks with rectiﬁed linear

units. arXiv preprint arXiv:1611.01491  2016.

[2] H. Ben-Hamu  H. Maron  I. Kezurer  G. Avineri  and Y. Lipman. Multi-chart generative surface modeling.

In SIGGRAPH Asia 2018 Technical Papers  page 215. ACM  2018.

[3] A. Ben-Israel. A newton-raphson method for the solution of systems of equations. Journal of Mathematical

analysis and applications  15(2):243–252  1966.

[4] M. Berger  A. Tagliasacchi  L. M. Seversky  P. Alliez  G. Guennebaud  J. A. Levine  A. Sharf  and C. T.
Silva. A survey of surface reconstruction from point clouds. In Computer Graphics Forum  volume 36 
pages 301–329. Wiley Online Library  2017.

[5] F. Bogo  J. Romero  M. Loper  and M. J. Black. FAUST: Dataset and evaluation for 3D mesh registration.
In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)  Piscataway  NJ  USA 
June 2014. IEEE.

[6] Z. Chen and H. Zhang. Learning implicit ﬁelds for generative shape modeling.

arXiv preprint

arXiv:1812.02822  2018.

[7] C. Cortes and V. Vapnik. Support-vector networks. Machine learning  20(3):273–297  1995.
[8] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control  signals

and systems  2(4):303–314  1989.

[9] G. W. Ding  Y. Sharma  K. Y. C. Lui  and R. Huang. Max-margin adversarial (mma) training: Direct input

space margin maximization through adversarial training. arXiv preprint arXiv:1812.02637  2018.

[10] G. Elsayed  D. Krishnan  H. Mobahi  K. Regan  and S. Bengio. Large margin deep networks for classiﬁca-

tion. In Advances in Neural Information Processing Systems  pages 842–852  2018.

[11] H. Fan  H. Su  and L. J. Guibas. A point set generation network for 3d object reconstruction from a single
image. In Proceedings of the IEEE conference on computer vision and pattern recognition  pages 605–613 
2017.

[12] T. Groueix  M. Fisher  V. G. Kim  B. C. Russell  and M. Aubry. A papier-mâché approach to learning 3d
surface generation. In Proceedings of the IEEE conference on computer vision and pattern recognition 
pages 216–224  2018.

[13] M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classiﬁer against adversarial

manipulation. In Advances in Neural Information Processing Systems  pages 2266–2276  2017.

[14] K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal approximators.

[15] S. G. Krantz and H. R. Parks. The implicit function theorem: history  theory  and applications. Springer

Neural networks  2(5):359–366  1989.

Science & Business Media  2012.

Citeseer  2009.

arXiv:1611.01236  2016.

[16] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report 

[17] A. Kurakin  I. Goodfellow  and S. Bengio. Adversarial machine learning at scale. arXiv preprint

[18] Y. LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/  1998.
[19] W. E. Lorensen and H. E. Cline. Marching cubes: A high resolution 3d surface construction algorithm. In

ACM siggraph computer graphics  volume 21  pages 163–169. ACM  1987.

[20] A. Madry  A. Makelov  L. Schmidt  D. Tsipras  and A. Vladu. Towards deep learning models resistant to

adversarial attacks. arXiv preprint arXiv:1706.06083  2017.

[21] A. Matyasko and L.-P. Chau. Margin maximization for robust classiﬁcation using deep learning. In 2017

International Joint Conference on Neural Networks (IJCNN)  pages 300–307. IEEE  2017.

[22] L. Mescheder  M. Oechsle  M. Niemeyer  S. Nowozin  and A. Geiger. Occupancy networks: Learning 3d

reconstruction in function space. arXiv preprint arXiv:1812.03828  2018.

[23] S.-M. Moosavi-Dezfooli  A. Fawzi  and P. Frossard. Deepfool: a simple and accurate method to fool deep
neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition  pages
2574–2582  2016.

[24] J. J. Park  P. Florence  J. Straub  R. Newcombe  and S. Lovegrove. Deepsdf: Learning continuous signed

distance functions for shape representation. arXiv preprint arXiv:1901.05103  2019.

[25] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison  L. Antiga  and

A. Lerer. Automatic differentiation in pytorch. 2017.

[26] J. Sokoli´c  R. Giryes  G. Sapiro  and M. R. Rodrigues. Robust large margin deep neural networks. IEEE

Transactions on Signal Processing  65(16):4265–4280  2017.

[27] S. Sun  W. Chen  L. Wang  and T.-Y. Liu. Large margin deep neural networks: Theory and algorithms.

arXiv preprint arXiv:1506.05232  148  2015.

[28] Y. Tang. Deep learning using support vector machines. CoRR  abs/1306.0239  2  2013.
[29] F. Williams  T. Schneider  C. Silva  D. Zorin  J. Bruna  and D. Panozzo. Deep geometric prior for surface

reconstruction. arXiv preprint arXiv:1811.10943  2018.

9

[30] E. Wong  F. Schmidt  J. H. Metzen  and J. Z. Kolter. Scaling provable adversarial defenses. In Advances in

Neural Information Processing Systems  pages 8400–8409  2018.

[31] H. Xiao  K. Rasul  and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine

learning algorithms. arXiv preprint arXiv:1708.07747  2017.

[32] H. Zhang  Y. Yu  J. Jiao  E. P. Xing  L. E. Ghaoui  and M. I. Jordan. Theoretically principled trade-off

between robustness and accuracy. arXiv preprint arXiv:1901.08573  2019.

[33] H.-K. Zhao  S. Osher  and R. Fedkiw. Fast surface reconstruction using the level set method. In Proceedings
IEEE Workshop on Variational and Level Set Methods in Computer Vision  pages 194–201. IEEE  2001.

10

,Alfredo Kalaitzis
Ricardo Silva
Scott Yang
Mehryar Mohri
Matan Atzmon
Niv Haim
Lior Yariv
Ofer Israelov
Haggai Maron
Yaron Lipman