2010,Efficient Relational Learning with Hidden Variable Detection,Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However  this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge  we propose a novel relational learning approach  which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN)  and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand  the restricted treeRMN only considers simple (e.g.  unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand  the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore  the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches  but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.,Efﬁcient Relational Learning with

Hidden Variable Detection

Ni Lao  Jun Zhu  Liu Liu  Yandong Liu  William W. Cohen

Carnegie Mellon University

{nlao junzhu liuliu yandongl wcohen}@cs.cmu.edu

5000 Forbes Avenue  Pittsburgh  PA 15213

Abstract

Markov networks (MNs) can incorporate arbitrarily complex features in modeling
relational data. However  this ﬂexibility comes at a sharp price of training an expo-
nentially complex model. To address this challenge  we propose a novel relational
learning approach  which consists of a restricted class of relational MNs (RMNs)
called relation tree-based RMN (treeRMN)  and an efﬁcient Hidden Variable De-
tection algorithm called Contrastive Variable Induction (CVI). On one hand  the
restricted treeRMN only considers simple (e.g.  unary and pairwise) features in re-
lational data and thus achieves computational efﬁciency; and on the other hand  the
CVI algorithm efﬁciently detects hidden variables which can capture long range
dependencies. Therefore  the resultant approach is highly efﬁcient yet does not
sacriﬁce its expressive power. Empirical results on four real datasets show that the
proposed relational learning method can achieve similar prediction quality as the
state-of-the-art approaches  but is signiﬁcantly more efﬁcient in training; and the
induced hidden variables are semantically meaningful and crucial to improve the
training speed and prediction qualities of treeRMNs.

1 Introduction
Statistical relational learning has attracted ever-growing interest in the last decade  because of widely
available relational data  which can be as complex as citation graphs  the World Wide Web  or rela-
tional databases. Relational Markov Networks (RMNs) are excellent tools to capture the statistical
dependency among entities in a relational dataset  as has been shown in many tasks such as col-
lective classiﬁcation [22] and information extraction [18][2]. Unlike Bayesian networks  RMNs
avoid the difﬁculty of deﬁning a coherent generative model  thereby allowing tremendous ﬂexibility
in representing complex patterns [21]. For example  Markov Logic Networks [10] can be auto-
matically instantiated as a RMN  given just a set of predicates representing attributes and relations
among entities. The algorithm can be applied to tasks in different domains without any change.
Relational Bayesian networks [22]  in contrary  would require expert knowledge to design proper
model structures and parameterizations whenever the schema of the domain under consideration is
changed. However  this ﬂexibility of RMN comes at a high price in training very complex models.
For example  work by Kok and Domingos [10][11][12] has shown that a prominent problem of re-
lational undirected models is how to handle the exponentially many features  each of which is an
conjunction of several neighboring variables (or “ground atoms” in terms of ﬁrst order logic). Much
computation is spent on proposing and evaluating candidate features.
The main goal of this paper is to show that instead of learning a very expressive relational model 
which can be extremely expensive  an alternative approach that explores Hidden Variable Detection
(HVD) to compensate a family of restricted relational models (e.g.  treeRMNs) can yield a very
efﬁcient yet competent relational learning framework. First  to achieve efﬁcient inference  we intro-
duce a restricted class of RMNs called relation tree-based RMNs (treeRMNs)  which only considers
unary (single variable assignment) and pairwise (conjunction of two variable assignments) features.

1

Since the Markov blanket of a variable is concisely deﬁned by a relation tree on the schema  we
can easily control the complexities of treeRMN models. Second  to compensate for the restricted
expressive power of treeRMNs  we further introduce a hidden variable induction algorithm called
Contrastive Variable Induction (CVI)  which can effectively detect latent variables capturing long
range dependencies. It has been shown in relational Bayesian networks [24] that hidden variables
can help propagating information across network structures  thus reducing the burden of extensive
structural learning. In this work  we explore the usefulness of hidden variables in learning RMNs.
Our experiments on four real datasets show that the proposed relational learning framework can
achieve similar prediction quality to the state-of-the-art RMN models  but is signiﬁcantly more ef-
ﬁcient in training. Furthermore  the induced hidden variables are semantically meaningful and are
crucial to improving training speed of treeRMN.
In the remainder of this paper  we ﬁrst brieﬂy review related work and training undirected graphical
models with mean ﬁeld contrastive divergence. Then we present the treeRMN model and the CVI
algorithm for variable induction. Finally  we present experimental results and conclude this paper.

2 Related Work
There has been a series of work by Kok and Domingos [10][11][12] developing Markov Logic
Networks (MLNs) and showing their ﬂexibility in different applications. The treeRMN model we
introduced in this work is intended to be a simpler model than MLNs  which can be trained more
efﬁciently  yet still be able to capture complex dependencies. Most of the existing RMN models
construct Markov networks by applying templates to entity relation graphs [21][8]. The treeRMN
model that we are going to introduce uses a type of template called a relation tree  which is very
general and applicable to a wide range of applications. This relation tree template resembles the
path-based feature generation approach for relational classiﬁers developed by Huang et al. [7].
Recently  much work has been done to induce hidden variables for generative Bayesian networks
[5][4][16][9][20][14]. However  previous studies [6][19] have pointed out that the generality of
Bayesian Networks is limited by their need for prior knowledge on the ordering of nodes. On the
other hand  very little progress has been made in the direction of non-parametric hidden variable
models based on discriminative Markov networks (MNs). One recent attempt is the Multiple Re-
lational Clustering (MRC) [11] algorithm  which performs top-down clustering of predicates and
symbols. However  it is computationally expensive because of its need for parameter estimation
when evaluating candidate structures. The CVI algorithm introduced in this work is most similar to
the “ideal parent” algorithm [16] for Gaussian Bayesian networks. The “ideal parent” evaluates can-
didate hidden variables based on the estimated gain of log-likelihood they can bring to the Bayesian
network. Similarly  the CVI algorithm evaluates candidate hidden variables based on the estimated
gain of an regularized RMN log-likelihood  thus avoids the costly step of parameter estimation.

3 Preliminaries
Before describing our model  let’s brieﬂy review undirected graphical models (a.k.a  Markov net-
works). Since our goal is to develop an efﬁcient RMN model  we use the simple but very efﬁcient
mean ﬁeld contrastive divergence [23] method. Our empirical results show that even the simplest
naive mean ﬁeld can yield very promising results. Extension to using more accurate (but also more
expensive) inference methods  such as loopy BP [15] or structured mean ﬁelds can be done similarly.
∑
Here we consider the general case that Markov networks have observed variables O  labeled vari-
ables Y  and hidden variables H. Let X = (Y  H) be the joint of hidden and labeled variables. The
conditional distribution of X given observations O is p(x|o; θ) = exp(θ
f (x  o))/Z(θ)  where f
is a vector of feature functions fk; θ is a vector of weights; Z(θ) =
f (x  o)) is a nor-
malization factor; and fk(x  o) counts the number of times the k-th feature ﬁres in (x  o). Here we
assume that the range of each variable is discrete and ﬁnite. Many commonly used graphical mod-
els have tied parameters  which allow a small number of parameters to govern a large number of
features. For example  in a linear chain CRF  each parameter is associated with a feature template:
e.g. “the current node having label yt = 1 and the immediate next neighbor having label yt+1 = 1”.
After applying each template to all the nodes in a graph  we get a graphical model with a large
number of features (i.e.  instantiations of feature templates). In general  a model’s order of Markov
dependence is determined by the maximal number of neighboring steps considered by any one of

x exp(θ

⊤

⊤

2

its feature templates. In the context of relational learning  the templates can be deﬁned similarly 
except having richer representations–with multiple types of entities and neighboring relations.
Given a set of training samples D = {(ym  om)}M
formulated as maximizing the following regularized log-likelihood
(cid:12)∥(cid:18)∥2
2;

m=1  the parameter estimation of MN can be

M∑

L((cid:18)) =

(1)

lm((cid:18)) − (cid:21)∥(cid:18)∥1 − 1
2

m=1

where λ and β are non-negative regularization constants for the ℓ1 and ℓ2-norm respectively. Be-
cause of its singularity at the origin  the ℓ1-norm can yield a sparse estimate  which is a desired
property for hidden variable discovery  as we shall see. The differentiable ℓ2-norm is useful when
there are strongly correlated features. The composite ℓ1/ℓ2-norm is known as ElasticNet [27]  which
has been shown to have nice properties. The log-likelihood for a single sample is

(2)

∑

h

l((cid:18)) = log p(y|o; (cid:18)) = log
p  where ⟨·⟩

− ⟨f⟩

p(h; y|o; (cid:18));

py

p is the expectation under the distribution p. To

and its gradient is ∇θl(θ) = ⟨f⟩
simplify notation  we use p to denote the distribution p(h  y|o; θ) and py to denote p(h|y  o; θ).
For simple (e.g. tree-structured) MNs  message passing algorithms can be used to infer the marginal
probabilities as required in the gradients exactly. For general MNs  however  we need approxi-
mate strategies like variational or Monte Carlo methods. Here we use simple mean ﬁeld variational
method [23]. By analogy with statistical physics  the free energy of any distribution q is deﬁned as
F (q) = ⟨−(cid:18)
⊤
(3)
f (y  h  o))  and l(θ) = F (p) − F (py).
Therefore  F (p) = − log Z(θ)  F (py) = − log
Let q0 be the mean ﬁeld approximation of p(h  y|o; θ) with y clamped to their true values  and qt
be the approximation of p(h  y|o; θ) obtained by applying t steps of mean ﬁeld updates to q0 with
y free. Then F (q0) ≥ F (qt) ≥ F (q∞) ≥ F (p). As in [23]  we set t = 1  and use

f⟩q − H(q):
⊤
h exp(θ

∑

lCD1((cid:18))   F (q1) − F (q0)
(4)
to approximate l(θ)  and its gradient is ∇θlCD1(θ) = ⟨f⟩q0
− ⟨f⟩q1. The new objective function
LCD1(θ) uses lCD1(θ) to replace l(θ). One advantage of CD is that it avoids q being trapped in a
possible multimodal distribution of p(h  y|o; θ) [25][3]. With the above approximation  we can use
orthant-wise L-BFGS [1] to estimate the parameters θ.

4 Relation Tree-Based RMNs
In the following  we formally deﬁne the treeRMN model with relation tree templates  which is very
general and applicable to a wide range of applications.
A schema S (Figure 1 left) is a pair (T  R). T = {Ti} is a set of entity types which include
both basic entity types (e.g.  P erson  Class) and composite entity types (e.g.  ⟨P erson  P erson⟩ 
⟨P erson  Class⟩). Each entity type is associated with a set of attributes A(T ) = {T.Ai}: e.g. 
A(P erson) = {P erson.gender}. R = {R} is a set of binary relations. We use dom(R) to denote
the domain type of R and range(R) to denote its range. For each argument of a composite entity
type  we deﬁne two relations  one with outward direction (e.g. P P 1 means from a Person-Person
−1). Here we use −1 to denote
pair to its ﬁrst argument) and another with inward direction (e.g. P P 1
the inverse of a relation. We further introduce a T win relation  which connects a composite entity
type to itself. Its semantics will be clear later. In principle  we can deﬁne other types of relations
such as those corresponding to functions in second order logic (e.g. P erson
An entity relation graph G = IE(S) (Figure 1 right)  is the instantiation of schema S on a set of
basic entities E = {ei}. We deﬁne the instantiation of a basic entity type T as IE(T ) = {e :
e.T = T}  and similarly for a composite type IE(T = ⟨T1  ...  Tk⟩) = {⟨e1  ...  ek⟩ : ei.T = Ti}.
In the given example  IE(P erson) = {p1  p2} is the set of persons; IE(Class) = {c1} is the
set of classes; IE(⟨P erson  P erson⟩) = {⟨p1  p2⟩ ⟨p2  p1⟩} is the set of person-person pairs; and
IE(⟨P erson  Class⟩) = {⟨p1  c1⟩ ⟨p2  c1⟩} is the set of person-class pairs. Each entity e has a set
of variables {e.Xi} that correspond to the set of attributes of its entity type A(e.T ). For a composite
entity that consists of two entities of the same type  we’d like to capture its correlation with its twin–
the composite entity made of the same basic entities but in reversed order. Therefore  we add the
T win relation between all pairs of twin entities: e.g.  from ⟨p1  p2⟩ to ⟨p2  p1⟩  and vice versa.

−−−−−−−→ P erson).

F atherOf

3

Figure 1: (Left) is a schema  where round and rectangular boxes represent basic and composite
entity types respectively. (Right) is a corresponding entity relation graph with three basic entities:
p1  p2  c1. For clarity we only show one direction of the relations and omit their labels.

Figure 2: Two-level relation trees for the P erson type (left) and the ⟨P erson  P erson⟩ type (right).

Given a schema  we can conveniently express how one entity can reach another entity by the con-
cept of a relation path. A relation path P is a sequence of relations R1 . . . Rℓ for which the do-
mains and ranges of adjacent relations are compatible–i.e.  range(Ri) = dom(Ri+1). We deﬁne
dom(R1 . . . Rℓ) ≡ dom(R1) and range(R1 . . . Rℓ) ≡ range(Rℓ)  and when we wish to em-
phasize the types associated with each step in a path  we will write the path P = R1 . . . Rℓ as
R1−−→ . . . Rℓ−−→ Tℓ  where T0 = dom(R1) = dom(P )  T1 = range(R1) = dom(R2) and
T0
so on. Note that  because some of the relations reﬂect one-to-one mappings  there are groups of
(cid:0)1
−−−−→
paths that are equivalent–e.g.  the path P erson is actually equivalent to the path P erson P C1
⟨P erson  Class⟩ P C1−−−→ P erson. To avoid creating these uninteresting paths  we add a constraint
to outward composite relations (e.g. P P 1 P C1) that they cannot be immediately preceded by their
inverse. We also constrain that the T win relation should not be combined with any other relations.
Now  the Markov blanket of an entity e ∈ T can be concisely deﬁned by the set of all relation paths
with domain T and of length ≤ ℓ (as shown in Figure 2). We call this set the relation tree of type
T   and denote it as T ree(T  ℓ) = {P}. We deﬁne a unary template as T.Ai = a  where Ai is an
attribute of type T   and a ∈ range(Ai). This template can be applied to any entity e of type T
in the entity relation graph. We deﬁne a pairwise template as T.Ai = a
P.Bj = b  where Ai
is an attribute of type T   a ∈ range(Ai)  P.Bj is an attribute of type range(P )  dom(P ) = T  
and b ∈ range(Bj). This template can be applied to any entity pair (e1  e2)  where e1.T = T and
e2 ∈ e1.P . Here we deﬁne e.P as the set of entities reach able from entity e ∈ T through the
relation path P . For example  the following template

∧

∧

pp:coauthor = 1

pp P P 1−−−→ p P P 1

−−−−−→ pp:advise = 1

(cid:0)1

can be applied to any person-person pair  and it ﬁres whenever co-author=1 for this person pair  and
the ﬁrst person (identiﬁed as pp P P 1−−−→ p ) also have advise=1 with another person. Here we use
p as a shorthand for the type P erson  and pp a shorthand for ⟨P erson  P erson⟩. In our current
implementation  we systematically enumerate all possible unary and pairwise templates.
Given the above concepts  we deﬁne a treeRMN model M = (G  f   θ) as the tuple of an entity rela-
tion graph G  a set of feature functions f  and their weights θ. Each feature function fk counts the
number of times the k-th template ﬁres in G. Generally  the complexity of inference is exponential
in the depth of the relation trees  because both the number of templates and their sizes of Markov
blankets grow exponentially w.r.t. the depth ℓ. TreeRMN provides us a very convenient way to con-
trol the complexity by the single parameter ℓ. Since treeRMN only considers pairwise and unary
features  it is less expressive than Markov Logic Networks [10]  which can deﬁne higher order
features by conjunction of predicates; and treeRMN is also less expressive than relational Bayesian
networks [9][20][14]  which have factor functions with three arguments. However  the limited ex-
pressive power of treeRMN can be effectively compensated for by detecting hidden variables  which
is another key component of our relational learning approach  as explained in the next section.

4

PP1<Person>gender<Class>isGrduateCourse<Person  Person>adviseco-auther<Person Class>givetakePP1PC2PP2PP2PC1PC1PC2-1-1-1-1Twin<p1>gender=M<p2>gender=F<c1>isGrduateCourse=0<p1 p2>advise=1co-auther=1<p2 p1>advise=0co-auther=1<p1 c1>give=1take=0<p2 c1>give=0take=1TwinTwinPP1<Person><Class><Person  Person><Person Class>PP2PC2<Person><Person  Person><Person>PP2-1PP1-1PC1-1<Person  Person><Person  Person>PP2<Person>Twin<Person  Person><Person Class><Person  Person>PP2-1PP1-1PC1-1PP1<Person><Person  Person><Person Class><Person  Person>PP2-1PP1-1PC1-1Algorithm 1 Contrastive Variable Induction
initialize a treeRMN M = (G  f   θ)
while true do

′

′

estimate parameters θ by L-BFGS
(f
if no hidden variable is induced then

) = induceHiddenVariables(M)

  θ

break

end if
end while
return M

Algorithm 2 Bottom Up Clustering of Entities
initialize clustering Γ = {Ii = {i}}
while true do
for any pair of clusters I1 I2 ∈ Γ do
− ∆I2
inc(I1  I2) = ∆I1∪I2
end for
if the largest increment ≤ 0 then
end if
merge the pair with the largest increment

− ∆I1

break

end while
return Γ

5 Contrastive Variable Induction (CVI)
As we have explained in the previous section  in order to compensate for the limited expressive
power of a shallow treeRMN and capture long-range dependencies in complex relational data  we
propose to introduce hidden variables. These variables are detected effectively with the Contrastive
Variable Induction (CVI) algorithm as explained below.
The basic procedure (Algorithm 1) starts with a treeRMN model on observed variables  which can
be manually designed or automatically learned [13]; then it iteratively introduces new HVs to the
model and estimate its parameters. The key to making this simple procedure highly efﬁcient is a
fast algorithm to evaluate and select good candidate HVs. We give closed-form expressions of the
likelihood gain and the weights of newly added features under contrastive divergence approximation
[23] (other type of inference can be done similarly). Therefore  the CVI process can be very efﬁcient 
only adding small overhead to the training of a regular treeRMN.
Consider introducing a new HV H to the entity type T . In order for H to inﬂuence the model  it
needs to be connected to the existing model. This is done by deﬁning additional feature templates:
we can denote a HV candidate by a tuple ({q(i)(H)}  fH   θH )  where {q(i)(H)} is the set of distri-
butions of the hidden variable H on all entities of type T   fH is a set of pairwise feature templates
that connect H to the existing model  and θH is a vector of feature weights. Here we assume that
any feature f ∈ fH is in the pairwise form fH=1
A=a  where a is the assignment to one of the
existing variables A in the relation tree of type T . Ideally  we would like to identify the candidate
HV  which gives the maximal gain in the regularized objective function LCD1(θ).
For easy evaluation of H  we set its mean ﬁeld variational parameters µH to either 0 or 1 on the
entities of type T . This yields a lower bound to the gain of LCD1(θ). Therefore  a candidate HV
can be represented as (I  fH   θH )  where I is the set of indices to the entities with µH = 1. Using
second order Taylor expansion  we can show that for a particular feature f ∈ fH the maximal gain
(5)

∧

∆I f =

is achieved at

(cid:18)f =

(6)

⌊−eI [f ]⌋2
1
λ
2
(cid:14)I [f ] + (cid:12)
⌊−eI [f ]⌋λ
(cid:14)I [f ] + (cid:12)

;

q1 I

− ⟨f⟩

q0 I is the difference of f’s expectations  and δI [f ] = V ar

b = a−b  if a > b; a+b  if a < −b; 0  otherwise. Error eI [f ] =
where ⌊⌋ is a truncation operator: ⌊a⌋
⟨f⟩
∗
q0 I [f ] is the
differences of f’s variances1. Here we use q  I to denote the distribution q of the existing variables
augmented by the distribution of H parameterized by the index set I. q0 and q1 are the wake
and sleep distributions estimated by 1-step mean-ﬁeld CD. The estimations in Eq. (5) and (6) are
simple  yet have nice intuitive explanations about the effects of the ℓ1 and ℓ2 regularizer as used in
Eq. (1): a large ℓ2-norm (i.e. large β) smoothly shrinks both the (estimated) likelihood gain and the
feature weights; while the non-differentiable ℓ1-norm not only shrink the estimated gain and feature
weights  but also drive features to have zero gains  therefore  can automatically select the features.
If we assume that the gains of individual features are independent  then the estimated gain for H is

q1 I [f ] − V ar
∗

1V arq I [f ] is intractable when we have tied parameters. Therefore  we approximate it by assuming
V ∈V V arq I [f (V )] =

that the occurrences of f are independent to each other:

∗
q I [f ] =

i.e. V ar

q I )  where V is any speciﬁc subset of variables that f can be applied to.

⟨f (V )⟩

q I (1 − ⟨f (V )⟩

V ∈V

∑

∑

5

∑

f∈fI

∆I ≈

∆I f ;

where fI = {f : ∆I f > 0} is the set of features that are expected to improve the objective function.
However  ﬁnding the index set I that maximizes ∆I is still non-trivial—an NP-hard combinatory
optimization problem  which is often tackled by top-down or bottom-up procedures in the clustering
literature. Algorithm 2 uses a simple bottom up clustering algorithm to build a hierarchy of clusters.
It starts with each sample as an individual cluster  and then repeatedly merges the two clusters that
lead to the best increment of gain. The merging is stopped if the best increment ≤ 0.
After clustering  we introduce a single categorical variable that treats each cluster with positive gain
as a category  and the remaining useless clusters are merged into a separate category. Introducing
this categorical variable is equivalent to introducing a set of binary variables–one for each cluster
with positive gain. From the above derivation  we can see that the essential part of the CVI algorithm
is to compute the expectations and variances of RMN features  both of which can be done by any
inference procedures  including the mean ﬁeld as we have used. Therefore  in principle  the CVI
algorithm can be extended to use other inference methods like belief propagation or exact inference.

Remark 1 after the induction step  the introduced HVs are treated as observations: i.e. their vari-
ational parameters are ﬁxed to their initial 0 or 1 values. In the future  we’d like to treat the HVs as
free variables. This can potentially correct the errors made by the greedy clustering procedure. The
cardinalities of HVs may be adapted by operators like deleting  merging  or splitting of categories.
Remark 2 currently  we only induce HVs to basic entity types. Extension to composite types can
show interesting tenary relations such as “Abnormality can be PartOf Animals”. However  this
requires clustering over a much larger number of entities  which cannot be done by our simple
implementation of bottom up clustering.

6 Experiment
In this section  we present both qualitative and quantitative results of treeRMN model. We demon-
strate that CVI can discover semantically meaningful hidden variables  which can signiﬁcantly im-
prove the speed and quality of treeRMN models.

Basic Composite
#E #A
#E #A
50
80
0
196 56
14 111
0 18 225 49
0 10 816 1*

0

Animal
Nation
UML 135
Kinship 104

6.1 Datasets
Table 1 shows the statistics of the four datasets used in our ex-
periments. These datasets are commonly used by previous work
in relational learning [9][11][20][14]. The Animal dataset con-
tains a set of animals and their attributes. It consists exclusively
of unary predicates of the form A(a) where A is an attribute and
a is an animal (e.g.  Swims(Dolphin)). This is a simple proposi-
tional dataset with no relational structure  but is useful as a base case
for comparison. The Nation dataset contains attributes of nations
and relations among them. The binary predicates are of the form
R(n1  n2)  where n1  n2 are nations and R is a relation between
them (e.g.  ExportsTo  GivesEconomicAidTo). The unary predicates
are of the form A(n)  where n is a nation and A is a attribute (e.g. 
Communist(China)). The UML dataset is a biomedical ontology called Uniﬁed Medical Lan-
guage System. It consists of binary predicates of the form R(c1  c2)  where c1 and c2 are biomedical
concepts and R is a relation between them (e.g. Treats(Antibiotic Disease)). The Kinship dataset
contains kinship relationships among members of the Alyawarra tribe from Central Australia. Pred-
icates are of the form R(p1  p2)  where R is a kinship term and p1  p2 are persons. Except for the
animal data  the number of composite entities is the square of the number of basic entities.

Table 1: Number of entities
(#E) and attributes (#A) for
∗The kinship
four datasets.
data has only one attribute
which has 26 possible values.

6.2 Characterization of treeRMN and CVI
In this section  we analyze the properties of the discovered hidden variables and demonstrate the
behavior of the CVI algorithm. For the simple non-relational Animal data  if we start with a full
model with all pairwise features  CVI will decide not to introduce any hidden variables. If we run
CVI starting from a model with only unary features  however  CVI decides to introduce one hidden
variable H0 with 8 categories. Table 2 shows the associated entities and features for the ﬁrst four
categories. We can see that they nicely identify marine mammals  predators  rodents  and primates.

6

Entities

C0 KillerWhale Seal Dolphin BlueWhale

Walrus HumpbackWhale

C1 GrizzlyBear Tiger GermanShepherd
Leopard Wolf Weasel Raccoon Fox
Bobcat Lion

C2 Hamster Skunk Mole Rabbit Rat Rac-

coon Mouse
SpiderMonkey Gorilla Chimpanzee

C3

Positive Features
Flippers Ocean Water Swims
Fish Hairless Coastal Arctic ...
Stalker Fierce Meat Meatteeth
Claws Hunter Nocturnal Paws
Smart Pads ...
Hibernate Buckteeth Weak
Small Fields Nestspot Paws ...
Tree Jungle Bipedal Hands
Vegetation Forest ...

Negative Features
Quadrapedal Ground Furry
Strainteeth Walks ...
Timid Vegetation Weak
Grazer Toughskin Hooves
Domestic ...
Strong Muscle Big Tough-
skin ...
Plains Fields Patches ...

Table 2: The associated entities and features (sorted by decreasing magnitude of feature weights)
for the ﬁrst four categories of the induced hidden variable a.H0 on the Animal data. The features
are in the form a.H0 = Ci

a.A = 1  where A is any of the variables in the last two columns.

∧

∧

Entities

C0 AcquiredAbnormality AnatomicalAb-

normality CongenitalAbnormality

C1 Alga Plant
C2 Amphibian Animal Bird Invertebrate

Fish Mammal Reptile Vertebrate

(cid:0)1
(cid:0)1

−−−−−→cc.PartOf
c CC1

Positive Features
(cid:0)1
−−−−−→cc.Causes
c CC2
−−−−−→cc.CooccursWith ...
−−−−−→cc.Complicates
c CC2
c CC2
−−−−−→cc.LocationOf ...
−−−−−→cc.InteractsWith
c CC1
c CC1
−−−−−→cc.PropertyOf
−−−−−→cc.InteractsWith c CC2
c CC1
−−−−−→cc.InteractsWith c CC2
−−−−−→cc.PartOf ...
c CC2

(cid:0)1

(cid:0)1

(cid:0)1

(cid:0)1

(cid:0)1

(cid:0)1

(cid:0)1

Table 3: The associated entities and features (sorted by decreasing magnitude of feature weights)
for the ﬁrst three categories of the induced hidden variable c.H0 on the UML data. The features are
in the form c.H0 = Ci

A = 1  where A is any of the variables in the last column.

For the three relational datasets  we use UML as an example. The
induction process of Nation and Kinship datasets are similar  and
we omit their details due to space limitation. For the UML task 
CVI induces two multinomial hidden variables H0 and H1. As we
can see from Figure 3  the inclusion of each hidden variable sig-
niﬁcantly improves the conditional log likelihood of the model.
The ﬁrst hidden variable C.H0 has 43 categories  and Table 3
shows the top three of them. We can see that these categories
represent the hidden concepts Abnormalities  Animals and Plants
respectively. Abnormalities can be caused or treated by other con-
cepts  and it can also be a part of other concepts. Plants can be
the location of some other concepts; and some other concepts can
be part of or the property of Animals. These grouping of concepts
are similar to those reported by Kok and Domingos [11].

Figure 3: change of the con-
ditional
log likelihood during
training for the UML data.

6.3 Overall Performance
Now we present quantitative evaluation of the treeRMN model  and compare it with other relational
learning methods including MLN structure learning (MLS) [10]  Inﬁnite Relational Models (IRM)
[9] and Multiple Relational Clustering (MRC) [11]. Following the methodology of [11]  we situate
our experiment in prediction tasks. We perform 10 fold cross validation by randomly splitting all
the variables into 10 sets. At each run  we treat one fold as hidden during training  and then evaluate
the prediction of these variables conditioned on the observed variables during testing. The overall
performance is measured by training time  average Conditional Log-Likelihood (CLL)  and Area
Under the precision-recall Curve (AUC) [11]. All implementation is done with Java 6.0.
Table 4 compares the overall performance of treeRMN (RMN)  treeRMN with hidden variable dis-
covery (RMNCV I)  and other relational models (MSL  IRM and MRC) as reported in [11]. We
use subscripts (0  1  2) to indicate the order of Markov dependency (depth of relation trees)  and
dimθ for the number of parameters. First  we can see that  without HVs  the treeRMNs with higher
Markov orders generally perform better in terms of CLL and AUC. However  due to the complex-
ity of high-order treeRMNs  this comes with large increases in training time. In some cases (e.g. 
Kinship data)  a high order treeRMN can perform worse than a low order treeRMN probably due to
the difﬁculty of inference with a large number of features. Second  training a treeRMN with CVI

7

-0.7-0.6-0.5-0.4-0.3-0.2-0.100102030405060L-BFGS IterationCLLInitial modelIntroduce c.H0Introduce c.H1RMN0

1

†
†
†

Animal  (cid:21)=0.01  (cid:12)=1

Nation  (cid:21)=0.01  (cid:12)=1

†
†
†

†
†
†

AUC

dimθ Time

CLL
-0.34±0.03

1

†
†
†

24h MSL
10h MRC
10h IRM

5s RMN0
RMN1
RMN2

AUC
0.88±0.02 3 655

dimθ Time

0

RMNCV I⋆
MSL
MRC
IRM

1

9s RMNCV I
24h MSL
10h MRC
10h IRM

-0.33±0.02
-0.54±0.04
-0.43±0.04
-0.43±0.06

0.89±0.02 4 349
0.68±0.04
0.80±0.04
0.79±0.08
UML  (cid:21)=0.01  (cid:12)=10

CLL
-0.40±0.01
-0.33±0.02
-0.38±0.03
-0.31±0.02
-0.33±0.04
-0.31±0.02
-0.32±0.02

CLL
-0.056±0.005 0.70±0.02 1 081 0.3h RMN0
-0.044±0.002 0.68±0.04 2 162 1.0h RMN1
-0.028±0.003 0.71±0.02 6 440 14.5h RMN2
-0.005±0.001 0.94±0.01 6 946 453s RMNCV I
-0.025±0.002 0.47±0.06
-0.004±0.000 0.97±0.00
-0.011±0.001 0.79±0.01

AUC
dimθ Time
0.63±0.04 7 812
15s
0.72±0.04 21 840
70s
0.71±0.04 40 489 446s
0.83±0.04 22 191 104s
0.77±0.04
24h
0.75±0.03
10h
0.75±0.03
10h
Kinship  (cid:21)=0.01  (cid:12)=10
AUC
CLL
dimθ Time
0.08±0.00
-2.95±0.01
§
RMN0
6s
25
-1.36±0.05
0.66±0.03
§
RMN1
350 107s
-2.34±0.01
0.33±0.00 1 625 2.1h
§
RMN2
-1.04±0.03
0.81±0.01
§
RMNCV I⋆
900 402s
-0.066±0.006 0.59±0.08
MSL
24h
-0.048±0.002 0.84±0.01
MRC
10h
-0.063±0.002 0.68±0.01
IRM
10h
Table 4: Overall performance. Bold identiﬁes the best performance  and ± marks the standard
deviations. Experiments are conducted with Intel Xeon 2.33GHz CPU (E5410). ⋆These results were
started with a treeRMN that only has unary features. §The CLL of kinship data is not comparable to
previous approaches  because we treat each of its labels as one variable with 26 categories instead
of 26 binary variables. †The results of existing methods were run on different machines (Intel Xeon
2.8GHz CPU)  and their 10-fold data splits are independent to those used for the RMN models.
They were allowed to run up to 10-24 hours  and here we assumes that these methods cannot achieve
similar accuracy when the amount of training time is signiﬁcantly reduced.
is only 2∼4 times slower than training a treeRMN of the same order of Markov dependency. On
all three relational datasets  treeRMNs with CVI can signiﬁcantly improve CLL and AUC. For the
simple Animal dataset  the improvement is less signiﬁcant because there is no long range depen-
dency to be captured in this data. Although the CVI models have similar number features as the
second order treeRMNs  their inferences are much faster due to their much smaller Markov blan-
kets. Finally  on all datasets  the treeRMNs with CVI can achieve similar prediction quality as the
existing methods (i.e.  MSL  IRM and MRC)  but is about two orders of magnitude more efﬁcient
in training. Speciﬁcally  it achieves signiﬁcant improvements on the Animal and Nation data  but
moderately worse results on the UML and Kinship data. Since both UML and Kinship data have
no attributes in basic entity types  composite entities become more important to model. Therefore 
we suspect that the MRC model achieves better performance because it can perform clustering on
two-argument predicates which corresponds to composite entities.

7 Conclusions and Future Work

We have presented a novel approach for efﬁcient relational learning  which consists of a restricted
class of Relational Markov Networks (RMN) called relation tree-based RMN (treeRMN) and an
efﬁcient hidden variable induction algorithm called Contrastive Variable Induction (CVI). By using
simple treeRMNs  we achieve computational efﬁciency  and CVI can effectively detect hidden vari-
ables  which compensates for the limited expressive power of treeRMNs. Experiments on four real
datasets show that the proposed relational learning approach can achieve state-of-the-art prediction
accuracy and is much faster than existing relational Markov network models.
We can improve the presented approach in several aspects. First  to further speedup the treeRMN
model we can apply efﬁcient Markov network feature selection methods [17][26] instead of sys-
tematically enumerating all possible feature templates. Second  as we have explained at the end
of section 5  we’d like to apply HVD on composite entity types. Third  we’d also like to treat the
introduced hidden variables as free variables and to make their cardinalities adaptive. Finally  we
would like to explore high order features which involves more than two variable assignments.
Acknowledgements.
We gratefully acknowledge the support of NSF grant IIS-0811562 and NIH grant R01GM081293.

8

References
[1] Galen Andrew and Jianfeng Gao. Scalable training of ℓ1-regularized log-linear models. In

ICML  2007.

[2] Razvan C. Bunescu and Raymond J. Mooney. Collective information extraction with relational

Markov networks. In ACL  2004.

[3] Miguel A. Carreira-Perpinan and Geoffrey E. Hinton. On contrastive divergence learning. In

AISTATS  2005.

[4] Gal Elidan and Nir Friedman. The information bottleneck em algorithm. In UAI  2003.
[5] Gal Elidan  Noam Lotner  Nir Friedman  and Daphne Koller. Discovering hidden variables: A

structure-based approach. In NIPS  2000.

[6] Nir Friedman  Lise Getoor  Daphne Koller  and Avi Pfeffer. Learning probabilistic relational

models. In IJCAI  1999.

[7] Yi Huang  Volker Tresp  and Stefan Hagen Weber. Predictive modeling using features derived

from paths in relational graphs. In Technical report  2007.

[8] Ariel Jaimovich  Ofer Meshi  and Nir Friedman. Template-based inference in symmetric rela-

tional Markov random ﬁelds. In UAI  2007.

[9] Charles Kemp  Joshua B. Tenenbaum  Thomas L. Grifﬁths  Takeshi Yamada  and Naonori

Ueda. Learning systems of concepts with an inﬁnite relational model. In AAAI  2006.

[10] Stanley Kok and Pedro Domingos. Learning the structure of Markov logic networks. In ICML 

2005.

[11] Stanley Kok and Pedro Domingos. Statistical predicate invention. In ICML  2007.
[12] Stanley Kok and Pedro Domingos. Learning Markov logic networks using structural motifs.

In ICML  2010.

[13] Su-In Lee  Varun Ganapathi  and Daphne Koller. Efﬁcient structure learning of Markov net-

works using ℓ1-regularization. In NIPS  2006.

[14] Kurt T. Miller  Thomas L. Grifﬁths  and Michael I. Jordan. Nonparametric latent feature mod-

els for link prediction. In NIPS  2009.

[15] Kevin P. Murphy  Yair Weiss  and Michael I. Jordan. Loopy belief propagation for approximate

inference: An empirical study. In UAI  1999.

[16] Iftach Nachman  Gal Elidan  and Nir Friedman. “Ideal parent” structure learning for continu-

ous variable networks. In UAI  2004.

[17] Simon Perkins  Kevin Lacker  and James Theiler. Grafting: Fast  incremental feature selection

by gradient descent in function spaces. In JMLR  2003.

[18] Hoifung Poon and Pedro Domingos. Joint inference in information extraction. In AAAI  2007.
[19] Karen Sachs  Omar Perez  Dana Peer  Douglas A. Lauffenburger  and Garry P. Nolan. Causal

protein-signaling networks derived from multiparameter single-cell data. In Science  2005.

[20] Ilya Sutskever  Ruslan Salakhutdinov  and Josh Tenenbaum. Modelling relational data using

Bayesian clustered tensor factorization. In NIPS  2009.

[21] Benjamin Taskar  Pieter Abbeel  and Daphne Koller. Discriminative probabilistic models for

relational data. In UAI  2002.

[22] Benjamin Taskar  Eran Segal  and Daphne Koller. Probabilistic classiﬁcation and clustering in

relational data. In IJCAI  2001.

[23] Max Welling and Geoffrey E. Hinton. A new learning algorithm for mean ﬁeld Boltzmann

machines. In ICANN  2001.

[24] Zhao Xu  Volker Tresp  Kai Yu  and Hans-Peter Kriegel. Inﬁnite hidden relational models. In

UAI  2006.

[25] Alan Yuille. The convergence of contrastive divergence. In NIPS  2004.
[26] Jun Zhu  Ni Lao  and Eric P. Xing. Grafting-light: Fast  incremental feature selection and

structure learning of Markov random ﬁelds. In KDD  2010.

[27] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. In Journal

Of The Royal Statistical Society Series B  2005.

9

,Damien Scieur
Vincent Roulet
Francis Bach
Alexandre d'Aspremont
Yuxin Chen
Adish Singla
Oisin Mac Aodha
Pietro Perona
Yisong Yue