2016,Global Analysis of Expectation Maximization for Mixtures of Two Gaussians,Expectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models.  However  EM  which is an iterative algorithm based on the maximum likelihood principle  is generally only guaranteed to find stationary points of the likelihood objective  and these points may be far from any maximizer.  This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties.  Specifically  it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians.  This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit  and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis  establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.,Global Analysis of Expectation Maximization

for Mixtures of Two Gaussians

Ji Xu

Columbia University

jixu@cs.columbia.edu

Daniel Hsu

Columbia University

djhsu@cs.columbia.edu

Arian Maleki

Columbia University

arian@stat.columbia.edu

Abstract

Expectation Maximization (EM) is among the most popular algorithms for estimat-
ing parameters of statistical models. However  EM  which is an iterative algorithm
based on the maximum likelihood principle  is generally only guaranteed to ﬁnd
stationary points of the likelihood objective  and these points may be far from any
maximizer. This article addresses this disconnect between the statistical principles
behind EM and its algorithmic properties. Speciﬁcally  it provides a global analysis
of EM for speciﬁc models in which the observations comprise an i.i.d. sample
from a mixture of two Gaussians. This is achieved by (i) studying the sequence of
parameters from idealized execution of EM in the inﬁnite sample limit  and fully
characterizing the limit points of the sequence in terms of the initial parameters;
and then (ii) based on this convergence analysis  establishing statistical consistency
(or lack thereof) for the actual sequence of parameters produced by EM.

1

Introduction

Since Fisher’s 1922 paper (Fisher  1922)  maximum likelihood estimators (MLE) have become one
of the most popular tools in many areas of science and engineering. The asymptotic consistency
and optimality of MLEs have provided users with the conﬁdence that  at least in some sense  there
is no better way to estimate parameters for many standard statistical models. Despite its appealing
properties  computing the MLE is often intractable. Indeed  this is the case for many latent variable
models {f (Y  z; η)}  where the latent variables z are not observed. For each setting of the parameters
η  the marginal distribution of the observed data Y is (for discrete z)

(cid:88)

f (Y; η) =

f (Y  z; η) .

It is this marginalization over latent variables that typically causes the computational difﬁculty.
Furthermore  many algorithms based on the MLE principle are only known to ﬁnd stationary points
of the likelihood objective (e.g.  local maxima)  and these points are not necessarily the MLE.

z

1.1 Expectation Maximization

Among the algorithms mentioned above  Expectation Maximization (EM) has attracted more attention
for the simplicity of its iterations  and its good performance in practice (Dempster et al.  1977; Redner
and Walker  1984). EM is an iterative algorithm for climbing the likelihood objective starting from
an initial setting of the parameters ˆη

(cid:104)0(cid:105). In iteration t  EM performs the following steps:
ˆQ(η | ˆη

) log f (Y  z; η)  

) (cid:44) (cid:88)

f (z | Y; ˆη

(cid:104)t(cid:105)

(cid:104)t(cid:105)

E-step:

M-step:

z

(cid:104)t+1(cid:105) (cid:44) arg max

ˆη

ˆQ(η | ˆη

(cid:104)t(cid:105)

)  

η

(1)

(2)

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

In many applications  each step is intuitive and can be performed very efﬁciently.
Despite the popularity of EM  as well as the numerous theoretical studies of its behavior  many im-
portant questions about its performance—such as its convergence rate and accuracy—have remained
unanswered. The goal of this paper is to address these questions for speciﬁc models (described in
Section 1.2) in which the observation Y is an i.i.d. sample from a mixture of two Gaussians.
Towards this goal  we study an idealized execution of EM in the large sample limit  where the E-step
is modiﬁed to be computed over an inﬁnitely large i.i.d. sample from a Gaussian mixture distribution
)  we replace the observed data Y with a random
in the model. In effect  in the formula for ˆQ(η | ˆη
variable Y ∼ f (y; η(cid:63)) for some Gaussian mixture parameters η(cid:63) and then take its expectation. The
resulting E- and M-steps in iteration t are

(cid:104)t(cid:105)

E-step:

Q(η | η(cid:104)t(cid:105)) (cid:44) EY

f (z | Y ; η(cid:104)t(cid:105)) log f (Y   z; η)
Q(η | η(cid:104)t(cid:105)) .

 

(3)

η(cid:104)t+1(cid:105) (cid:44) arg max

η

M-step:

(4)
This sequence of parameters (η(cid:104)t(cid:105))t≥0 is fully determined by the initial setting η(cid:104)0(cid:105). We refer to
this idealization as Population EM  a procedure considered in previous works of Srebro (2007) and
Balakrishnan et al. (2014). Not only does Population EM shed light on the dynamics of EM in
the large sample limit  but it can also reveal some of the fundamental limitations of EM. Indeed  if
Population EM cannot provide an accurate estimate for the parameters η(cid:63)  then intuitively  one would
not expect the EM algorithm with a ﬁnite sample size to do so either. (To avoid confusion  we refer
the original EM algorithm run with a ﬁnite sample as Sample-based EM.)

(cid:34)(cid:88)

z

(cid:35)

1.2 Models and Main Contributions

In this paper  we study EM in the context of two simple yet popular and well-studied Gaussian
mixture models. The two models  along with the corresponding Sample-based EM and Population
EM updates  are as follows:
Model 1. The observation Y is an i.i.d. sample from the mixture distribution 0.5N (−θ(cid:63)  Σ) +
0.5N (θ(cid:63)  Σ); Σ is a known covariance matrix in Rd  and θ(cid:63) is the unknown parameter of interest.
1. Sample-based EM iteratively updates its estimate of θ(cid:63) according to the following equation:

(cid:104)t+1(cid:105)

ˆθ

=

1
n

(cid:0)yi  ˆθ

(cid:17)
(cid:104)t(cid:105)(cid:1) − 1

yi 

2wd

n(cid:88)

(cid:16)

i=1

where y1  . . .   yn are the independent draws that comprise Y 

wd(y  θ) (cid:44)

φd(y − θ)

φd(y − θ) + φd(y + θ)

 

and φd is the density of a Gaussian random vector with mean 0 and covariance Σ.
2. Population EM iteratively updates its estimate according to the following equation:

where Y ∼ 0.5N (−θ(cid:63)  Σ) + 0.5N (θ(cid:63)  Σ).

(cid:104)t+1(cid:105)

θ

= E(2wd(Y   θ

(cid:104)t(cid:105)

) − 1)Y  

(5)

(6)

Model 2. The observation Y is an i.i.d. sample from the mixture distribution 0.5N (µ(cid:63)
0.5N (µ(cid:63)

2) are the unknown parameters of interest.

2  Σ). Again  Σ is known  and (µ(cid:63)

1  µ(cid:63)

1  Σ) +

1. Sample-based EM iteratively updates its estimate of µ(cid:63)

2 at every iteration according

to the following equations:

1 and µ(cid:63)

(cid:80)n
(cid:80)n
(cid:104)t(cid:105)
(cid:104)t(cid:105)
i=1 vd(yi  ˆµ
1   ˆµ
2 )yi
(cid:80)n
(cid:104)t(cid:105)
(cid:104)t(cid:105)
1   ˆµ
i=1 vd(yi  ˆµ
2 )
(cid:80)n
(cid:104)t(cid:105)
i=1(1 − vd(yi  ˆµ
1   ˆµ
(cid:104)t(cid:105)
i=1(1 − vd(yi  ˆµ
1   ˆµ

 

(cid:104)t(cid:105)
2 ))yi
(cid:104)t(cid:105)
2 ))

(cid:104)t+1(cid:105)
ˆµ
1

(cid:104)t+1(cid:105)
ˆµ
2

=

=

(7)

(8)

 

2

where y1  . . .   yn are the independent draws that comprise Y  and

vd(y  µ1  µ2) (cid:44)

φd(y − µ1)

φd(y − µ1) + φd(y − µ2)

.

2. Population EM iteratively updates its estimates according to the following equations:

(cid:104)t+1(cid:105)
1

µ

(cid:104)t+1(cid:105)
2

µ

=

=

 

(cid:104)t(cid:105)
(cid:104)t(cid:105)
Evd(Y   µ
2 )Y
1   µ
(cid:104)t(cid:105)
(cid:104)t(cid:105)
Evd(Y   µ
1   µ
2 )
(cid:104)t(cid:105)
E(1 − vd(Y   µ
1   µ
(cid:104)t(cid:105)
E(1 − vd(Y   µ
1   µ
2  Σ).

(cid:104)t(cid:105)
2 ))Y
(cid:104)t(cid:105)
2 ))

where Y ∼ 0.5N (µ(cid:63)

1  Σ) + 0.5N (µ(cid:63)

(9)

(10)

 

Our main contribution in this paper is a new characterization of the stationary points and dynamics of
EM in both of the above models.

1. We prove convergence for the sequence of iterates for Population EM from each model:
(cid:104)t(cid:105)
2 ))t≥0
2)/2). We also fully

the sequence (θ
converges to either (µ(cid:63)
1 + µ(cid:63)
characterize the initial parameter settings that lead to each limit point.

)t≥0 converges to either θ(cid:63)  −θ(cid:63)  or 0; the sequence ((µ

1 + µ(cid:63)

2)/2  (µ(cid:63)

1  µ(cid:63)

2)  (µ(cid:63)

2  µ(cid:63)

1)  or ((µ(cid:63)

(cid:104)t(cid:105)
1   µ

(cid:104)t(cid:105)

2. Using this convergence result for Population EM  we also prove that the limits of the Sample-
based EM iterates converge in probability to the unknown parameters of interest  as long
as Sample-based EM is initialized at points where Population EM would converge to these
parameters as well.

Formal statements of our results are given in Section 2.

1.3 Background and Related Work

The EM algorithm was formally introduced by Dempster et al. (1977) as a general iterative method
for computing parameter estimates from incomplete data. Although EM is billed as a procedure for
maximum likelihood estimation  it is known that with certain initializations  the ﬁnal parameters
returned by EM may be far from the MLE  both in parameter distance and in log-likelihood value (Wu 
1983). Several works characterize convergence of EM to stationary points of the log-likelihood
objective under certain regularity conditions (Wu  1983; Tseng  2004; Vaida  2005; Chrétien and
Hero  2008). However  these analyses do not distinguish between global maximizers and other
stationary points (except  e.g.  when the likelihood function is unimodal). Thus  as an optimization
algorithm for maximizing the log-likelihood objective  the “worst-case” performance of EM is
somewhat discouraging.
For a more optimistic perspective on EM  one may consider a “best-case” analysis  where (i) the
data are an iid sample from a distribution in the given model  (ii) the sample size is sufﬁciently
large  and (iii) the starting point for EM is sufﬁciently close to the parameters of the data generating
distribution. Conditions (i) and (ii) are ubiquitous in (asymptotic) statistical analyses  and (iii) is a
generous assumption that may be satisﬁed in certain cases. Redner and Walker (1984) show that
in such a favorable scenario  EM converges to the MLE almost surely for a broad class of mixture
models. Moreover  recent work of Balakrishnan et al. (2014) gives non-asymptotic convergence
guarantees in certain models; importantly  these results permit one to quantify the accuracy of a
pilot estimator required to effectively initialize EM. Thus  EM may be used in a tractable two-stage
estimation procedures given a ﬁrst-stage pilot estimator that can be efﬁciently computed.
Indeed  for the special case of Gaussian mixtures  researchers in theoretical computer science and
machine learning have developed efﬁcient algorithms that deliver the highly accurate parameter
estimates under appropriate conditions. Several of these algorithms  starting with that of Dasgupta
(1999)  assume that the means of the mixture components are well-separated—roughly at distance
either dα or kβ for some α  β > 0 for a mixture of k Gaussians in Rd (Dasgupta  1999; Arora
and Kannan  2005; Dasgupta and Schulman  2007; Vempala and Wang  2004; Kannan et al.  2008;
Achlioptas and McSherry  2005; Chaudhuri and Rao  2008; Brubaker and Vempala  2008; Chaudhuri
et al.  2009a). More recent work employs the method-of-moments  which permit the means of the

3

mixture components to be arbitrarily close  provided that the sample size is sufﬁciently large (Kalai
et al.  2010; Belkin and Sinha  2010; Moitra and Valiant  2010; Hsu and Kakade  2013; Hardt and
Price  2015). In particular  Hardt and Price (2015) characterize the information-theoretic limits of
parameter estimation for mixtures of two Gaussians  and that they are achieved by a variant of the
original method-of-moments of Pearson (1894).
Most relevant to this paper are works that speciﬁcally analyze EM (or variants thereof) for Gaussian
mixture models  especially when the mixture components are well-separated. Xu and Jordan (1996)
show favorable convergence properties (akin to super-linear convergence near the MLE) for well-
separated mixtures. In a related but different vein  Dasgupta and Schulman (2007) analyze a variant
of EM with a particular initialization scheme  and proves fast convergence to the true parameters 
again for well-separated mixtures in high-dimensions. For mixtures of two Gaussians  it is possible to
exploit symmetries to get sharper analyses. Indeed  Chaudhuri et al. (2009b) uses these symmetries to
prove that a variant of Lloyd’s algorithm (MacQueen  1967; Lloyd  1982) (which may be regarded as
a hard-assignment version of EM) very quickly converges to the subspace spanned by the two mixture
component means  without any separation assumption. Lastly  for the speciﬁc case of our Model
1  Balakrishnan et al. (2014) proves linear convergence of EM (as well as a gradient-based variant
of EM) when started in a sufﬁciently small neighborhood around the true parameters  assuming a
minimum separation between the mixture components. Here  the permitted size of the neighborhood
grows with the separation between the components  and a recent result of Klusowski and Brinda
(2016) quantitatively improves this aspect of the analysis (but still requires a minimum separation).
Remarkably  by focusing attention on the local region around the true parameters  they obtain non-
asymptotic bounds on the parameter estimation error. Our work is complementary to their results
in that we focus on asymptotic limits rather than ﬁnite sample analysis. This allows us to provide a
global analysis of EM without separation or initialization conditions  which cannot be deduced from
the results of Balakrishnan et al. or Klusowski and Brinda by taking limits.
Finally  two related works have appeared following the initial posting of this article (Xu et al.  2016).
First  Daskalakis et al. (2016) concurrently and independently proved a convergence result comparable
to our Theorem 1 for Model 1; for this case  they also provide an explicit rate of linear convergence.
Second  Jin et al. (2016) show that similar results do not hold in general for uniform mixtures of
three or more spherical Gaussian distributions: common initialization schemes for (Population or
Sample-based) EM may lead to local maxima that are arbitrarily far from the global maximizer.
Similar results were well-known for Lloyd’s algorithm  but were not previously established for
Population EM (Srebro  2007).

2 Analysis of EM for Mixtures of Two Gaussians

In this section  we present our results for Population EM and Sample-based EM under both Model
1 and Model 2  and also discuss further implications about the expected log-likelihood function.
Without loss of generality  we may assume that the known covariance matrix Σ is the identity matrix
I d. Throughout  we denote the Euclidean norm by (cid:107) · (cid:107)  and the signum function by sgn(·) (where
sgn(0) = 0  sgn(z) = 1 if z > 0  and sgn(z) = −1 if z < 0).

2.1 Main Results for Population EM

We present results for Population EM for both models  starting with Model 1.
Theorem 1. Assume θ(cid:63) ∈ Rd \ {0}. Let (θ
and suppose (cid:104)θ

  θ(cid:63)(cid:105) (cid:54)= 0. There exists κθ ∈ (0  1)—depending only on θ(cid:63) and θ
(cid:104)t+1(cid:105) − sgn((cid:104)θ

  θ(cid:63)(cid:105))θ(cid:63)(cid:13)(cid:13)(cid:13) ≤ κθ ·(cid:13)(cid:13)(cid:13)θ

  θ(cid:63)(cid:105))θ(cid:63)(cid:13)(cid:13)(cid:13) .

(cid:13)(cid:13)(cid:13)θ

(cid:104)t(cid:105) − sgn((cid:104)θ

(cid:104)0(cid:105)

(cid:104)0(cid:105)

(cid:104)t(cid:105)

)t≥0 denote the Population EM iterates for Model 1 

(cid:104)0(cid:105)—such that

(cid:104)0(cid:105)

The proof of Theorem 1 and all other omitted proofs are given in the full version of this article (Xu
(cid:104)0(cid:105) is not on the hyperplane {x ∈ Rd : (cid:104)x  θ(cid:63)(cid:105) = 0}  then
et al.  2016). Theorem 1 asserts that if θ
the sequence (θ
Our next result shows that if (cid:104)θ

)t≥0 converges to either θ(cid:63) or −θ(cid:63).
  θ(cid:63)(cid:105) = 0  then (θ
(cid:104)t(cid:105)

)t≥0 still converges  albeit to 0.

(cid:104)0(cid:105)

(cid:104)t(cid:105)

4

Theorem 2. Let (θ

(cid:104)t(cid:105)

)t≥0 denote the Population EM iterates for Model 1. If (cid:104)θ

(cid:104)0(cid:105)

  θ(cid:63)(cid:105) = 0  then

(cid:104)t(cid:105) → 0 as t → ∞ .

θ

Theorems 1 and 2 together characterize the ﬁxed points of Population EM for Model 1  and fully
specify the conditions under which each ﬁxed point is reached. The results are simply summarized in
the following corollary.
Corollary 1. If (θ

)t≥0 denote the Population EM iterates for Model 1  then

(cid:104)t(cid:105)

(cid:104)t(cid:105) → sgn((cid:104)θ

θ

(cid:104)0(cid:105)

  θ(cid:63)(cid:105))θ(cid:63)

as t → ∞ .

We now discuss Population EM with Model 2. To state our results more concisely  we use the
following re-parameterization of the model parameters and Population EM iterates:

a(cid:104)t(cid:105) (cid:44) µ

(cid:104)t(cid:105)
(cid:104)t(cid:105)
1 + µ
2

2

− µ(cid:63)

1 + µ(cid:63)
2

2

 

(cid:104)t(cid:105) (cid:44) µ
b

(cid:104)t(cid:105)
(cid:104)t(cid:105)
2 − µ
1

2

 

θ(cid:63) (cid:44) µ(cid:63)

2 − µ(cid:63)
2

1

.

(11)

If the sequence of Population EM iterates ((µ
(cid:104)t(cid:105) → θ(cid:63). Hence  we also deﬁne β(cid:104)t(cid:105) as the angle between b
b

(cid:104)t(cid:105)
2 ))t≥0 converges to (µ(cid:63)
(cid:104)t(cid:105) and θ(cid:63)  i.e. 

1  µ(cid:63)

2)  then we expect

(cid:104)t(cid:105)
1   µ

(cid:32) (cid:104)b

  θ(cid:63)(cid:105)
(cid:104)t(cid:105)
(cid:104)t(cid:105)(cid:107)(cid:107)θ(cid:63)(cid:107)

(cid:107)b

(cid:33)

β(cid:104)t(cid:105) (cid:44) arccos

∈ [0  π] .

(cid:104)t(cid:105) (cid:54)= 0 and θ(cid:63) (cid:54)= 0.)

(This is well-deﬁned as long as b
We ﬁrst present results on Population EM with Model 2 under the initial condition (cid:104)b
Theorem 3. Assume θ(cid:63) ∈ Rd \ {0}. Let (a(cid:104)t(cid:105)  b
EM iterates for Model 2  and suppose (cid:104)b
exist κa ∈ (0  1)—depending only on (cid:107)θ(cid:63)(cid:107) and |(cid:104)b
only on (cid:107)θ(cid:63)(cid:107)  (cid:104)b

  θ(cid:63)(cid:105) (cid:54)= 0.
)t≥0 denote the (re-parameterized) Population
(cid:104)t(cid:105) (cid:54)= 0 for all t ≥ 0. Furthermore  there
(cid:104)0(cid:105)(cid:107)|—and κβ ∈ (0  1)—depending

  θ(cid:63)(cid:105)/(cid:107)b
(cid:104)0(cid:105)(cid:107)—such that

(cid:104)0(cid:105)(cid:107)  (cid:107)a(cid:104)0(cid:105)(cid:107)  and (cid:107)b

  θ(cid:63)(cid:105) (cid:54)= 0. Then b

  θ(cid:63)(cid:105)/(cid:107)b

(cid:104)0(cid:105)

(cid:104)0(cid:105)

(cid:104)0(cid:105)

(cid:104)0(cid:105)

(cid:104)t(cid:105)

(cid:107)a(cid:104)t+1(cid:105)(cid:107)2 ≤ κ2
sin(β(cid:104)t+1(cid:105)) ≤ κt

By combining the two inequalities from Theorem 3  we conclude

(cid:107)a(cid:104)t+1(cid:105)(cid:107)2 = κ2t

a (cid:107)a(cid:104)0(cid:105)(cid:107)2 +

a · sin2(β(cid:104)t−τ(cid:105))
κ2τ

(cid:107)θ(cid:63)(cid:107)2 sin2(β(cid:104)t(cid:105))

 

4

(cid:107)θ(cid:63)(cid:107)2

a · (cid:107)a(cid:104)t(cid:105)(cid:107)2 +
β · sin(β(cid:104)0(cid:105)) .
t(cid:88)
t(cid:88)
(cid:16)
max(cid:8)κa  κβ

a κ2(t−τ )
κ2τ

(cid:107)θ(cid:63)(cid:107)2

(cid:107)θ(cid:63)(cid:107)2

τ =0

τ =0

4

4

t

β

4

· sin2(β(cid:104)0(cid:105))

(cid:9)(cid:17)t

sin2(β(cid:104)0(cid:105)) .

≤ κ2t

a (cid:107)a(cid:104)0(cid:105)(cid:107)2 +

≤ κ2t

a (cid:107)a(cid:104)0(cid:105)(cid:107)2 +

1 + µ(cid:63)

Theorem 3 shows that the re-parameterized Population EM iterates converge  at a linear rate  to the
2)/2  as well as the line spanned by θ(cid:63). The theorem  however 
average of the two means (µ(cid:63)
(cid:104)t(cid:105) to the magnitude of θ(cid:63).
does not provide any information on the convergence of the magnitude of b
This is given in the next theorem.
Theorem 4. Assume θ(cid:63) ∈ Rd \ {0}. Let (a(cid:104)t(cid:105)  b
EM iterates for Model 2  and suppose (cid:104)b
cb > 0—all depending only on (cid:107)θ(cid:63)(cid:107)  |(cid:104)b
(cid:104)0(cid:105)

)t≥0 denote the (re-parameterized) Population
  θ(cid:63)(cid:105) (cid:54)= 0. Then there exist T0 > 0  κb ∈ (0  1)  and
(cid:104)0(cid:105)
(cid:104)0(cid:105)(cid:107)|  (cid:107)a(cid:104)0(cid:105)(cid:107)  and (cid:107)b
  θ(cid:63)(cid:105)/(cid:107)b

(cid:104)t(cid:105)

(cid:13)(cid:13)(cid:13)b

(cid:104)t+1(cid:105) − sgn((cid:104)b

(cid:104)0(cid:105)

b ·(cid:13)(cid:13)(cid:13)b
  θ(cid:63)(cid:105))θ(cid:63)(cid:13)(cid:13)(cid:13)2 ≤ κ2

  θ(cid:63)(cid:105))θ(cid:63)(cid:13)(cid:13)(cid:13)2

(cid:104)0(cid:105)(cid:107)—such that
+ cb · (cid:107)a(cid:104)t(cid:105)(cid:107) ∀t > T0 .

(cid:104)t(cid:105) − sgn((cid:104)b

(cid:104)0(cid:105)

5

(cid:104)0(cid:105)

If (cid:104)b
degenerate solution (0  0).
(cid:104)t(cid:105)
Theorem 5. Let (a(cid:104)t(cid:105)  b
If (cid:104)b

  θ(cid:63)(cid:105) = 0  then

(cid:104)0(cid:105)

  θ(cid:63)(cid:105) = 0  then we show convergence of the (re-parameterized) Population EM iterates to the

)t≥0 denote the (re-parameterized) Population EM iterates for Model 2.

(a(cid:104)t(cid:105)  b

(cid:104)t(cid:105)

) → (0  0) as t → ∞ .

Theorems 3  4  and 5 together characterize the ﬁxed points of Population EM for Model 2  and fully
specify the conditions under which each ﬁxed point is reached. The results are simply summarized in
the following corollary.
Corollary 2. If (a(cid:104)t(cid:105)  b
then

)t≥0 denote the (re-parameterized) Population EM iterates for Model 2 

(cid:104)t(cid:105)

1 + µ(cid:63)
2

a(cid:104)t(cid:105) → µ(cid:63)
2
(cid:104)t(cid:105) → sgn((cid:104)b

b

(cid:104)0(cid:105)

as t → ∞  
2 − µ(cid:63)
µ(cid:63)
2 − µ(cid:63)
1(cid:105))
2

1

  µ(cid:63)

as t → ∞ .

2.2 Main Results for Sample-based EM

Using the results on Population EM presented in the above section  we can now establish consistency
of (Sample-based) EM. We focus attention on Model 2  as the same results for Model 1 easily follow
as a corollary. First  we state a simple connection between the Population EM and Sample-based EM
iterates.
Theorem 6. Suppose Population EM and Sample-based EM for Model 2 have the same initial
parameters: ˆµ

(cid:104)0(cid:105)
1 = µ

(cid:104)0(cid:105)
1 and ˆµ

(cid:104)0(cid:105)
2 = µ
(cid:104)t(cid:105)
(cid:104)t(cid:105)
1 → µ
ˆµ
1
where convergence is in probability.

(cid:104)0(cid:105)
2 . Then for each iteration t ≥ 0 
as n → ∞  
and

(cid:104)t(cid:105)
2 → µ
ˆµ

(cid:104)t(cid:105)
2

(cid:104)0(cid:105)
1   µ

(cid:104)0(cid:105)
1   ˆµ

(cid:104)0(cid:105)
2 ) = (µ

Note that Theorem 6 does not necessarily imply that the ﬁxed point of Sample-based EM (when
(cid:104)0(cid:105)
2 )) is the same as that of Population EM. It is conceivable that
initialized at ( ˆµ
as t → ∞  the discrepancy between (the iterates of) Sample-based EM and Population EM increases.
We show that this is not the case: the ﬁxed points of Sample-based EM indeed converge to the ﬁxed
points of Population EM.
Theorem 7. Suppose Population EM and Sample-based EM for Model 2 have the same initial
parameters: ˆµ

(cid:104)0(cid:105)
2 . If (cid:104)µ
and

(cid:104)0(cid:105)
(cid:104)0(cid:105)
1   θ(cid:63)(cid:105) (cid:54)= 0  then
2 − µ
(cid:104)t(cid:105)
(cid:104)t(cid:105)
| ˆµ
2 − µ
2 | → 0
lim sup
t→∞

as n → ∞  

(cid:104)0(cid:105)
(cid:104)0(cid:105)
(cid:104)0(cid:105)
1 and ˆµ
2 = µ
1 = µ
(cid:104)t(cid:105)
(cid:104)t(cid:105)
1 − µ
| ˆµ
1 | → 0
where convergence is in probability.

lim sup
t→∞

2.3 Population EM and Expected Log-likelihood

Do the results we derived in the last section regarding the performance of EM provide any information
on the performance of other ascent algorithms  such as gradient ascent  that aim to maximize the log-
likelihood function? To address this question  we show how our analysis can determine the stationary
points of the expected log-likelihood and characterize the shape of the expected log-likelihood in a
neighborhood of the stationary points. Let G(η) denote the expected log-likelihood  i.e. 

(cid:90)

G(η) (cid:44) E(log fη(Y )) =

f (y; η∗) log f (y; η) dy 

where η∗ denotes the true parameter value. Also consider the following standard regularity conditions:
R1 The family of probability density functions f (y; η) have common support.
R2 ∇η

(cid:82) f (y; η∗) log f (y; η) dy =(cid:82) f (y; η∗)∇η log f (y; η) dy  where ∇η denotes the gradient

with respect to η.

6

R3 ∇η(E(cid:80)

z f (z | Y ; η(cid:104)t(cid:105))) log f (Y   z; η) = E(cid:80)

z f (z | Y ; η(cid:104)t(cid:105))∇η log f (Y   z; η).

These conditions can be easily conﬁrmed for many models including the Gaussian mixture models.
The following theorem connects the ﬁxed points of the Population EM and the stationary points of
the expected log-likelihood.
Lemma 1. Let ¯η ∈ Rd denote a stationary point of G(η). Also assume that Q(η | η(cid:104)t(cid:105)) has a unique
and ﬁnite stationary point in terms of η for every η(cid:104)t(cid:105)  and this stationary point is its global maxima.
Then  if the model satisﬁes conditions R1–R3  and the Population EM algorithm is initialized at ¯η  it
will stay at ¯η. Conversely  any ﬁxed point of Population EM is a stationary point of G(η).
Proof. Let ¯η denote a stationary point of G(η). We ﬁrst prove that ¯η is a stationary point of Q(η | ¯η).

∇ηQ(η | ¯η)(cid:12)(cid:12)η=¯η =

∇ηf (y  z; η)(cid:12)(cid:12)η=¯η

f (y  z; ¯η)

f (y; η∗) dy

f (z | y; ¯η)

(cid:90) (cid:88)
(cid:90) (cid:88)
(cid:90) ∇ηf (y  η)(cid:12)(cid:12)η=¯η

∇ηf (y  z; η)(cid:12)(cid:12)η=¯η

f (y; ¯η)

z

z

f (y; ¯η)

f (y; η∗) dy = 0  

=

=

f (y; η∗) dy

where the last equality is using the fact that ¯η is a stationary point of G(η). Since Q(η | ¯η) has a
unique stationary point  and we have assumed that the unique stationary point is its global maxima 
then Population EM will stay at that point. The proof of the other direction is similar.
Remark 1. The fact that η∗ is the global maximizer of G(η) is well-known in the statistics and
machine learning literature (e.g.  Conniffe  1987). Furthermore  the fact that η∗ is a global maximizer
of Q(η | η∗) is known as the self-consistency property (Balakrishnan et al.  2014).
It is straightforward to conﬁrm the conditions of Lemma 1 for mixtures of Gaussians. This lemma
conﬁrms that Population EM may be trapped in every local maxima. However  less intuitively it may
get stuck at local minima or saddle points as well. Our next result characterizes the stationary points
of G(θ) for Model 1.
Corollary 3. G(θ) has only three stationary points. If d = 1 (so θ = θ ∈ R)  then 0 is a local
minima of G(θ)  while θ∗ and −θ∗ are global maxima. If d > 1  then 0 is a saddle point  and θ(cid:63) and
−θ(cid:63) are global maxima.
The proof is a straightforward result of Lemma 1 and Corollary 1. The phenomenon that Population
EM may stuck in local minima or saddle points also happens in Model 2. We can employ Corollary 2
and Lemma 1 to explain the shape of the expected log-likelihood function G. To simplify the notation 
we consider the re-parametrization a (cid:44) µ1+µ2
Corollary 4. G(a  b) has three stationary points:

and b (cid:44) µ2−µ1

.

2

2

(cid:19)

(cid:18) µ(cid:63)

1 + µ(cid:63)
2

2

1 − µ(cid:63)
µ(cid:63)
2

2

 

 

and

1 + µ(cid:63)
2

2

 

µ(cid:63)

1 + µ(cid:63)
2

2

(cid:18) µ(cid:63)

1 + µ(cid:63)
2

2

2 − µ(cid:63)
µ(cid:63)
2

1

 

(cid:19)

 

(cid:18) µ(cid:63)

(cid:19)

.

The ﬁrst two points are global maxima. The third point is a saddle point.

3 Concluding Remarks

Our analysis of Population EM and Sample-based EM shows that the EM algorithm can  at least
for the Gaussian mixture models studied in this work  compute statistically consistent parameter
estimates. Previous analyses of EM only established such results for speciﬁc methods of initializing
EM (e.g.  Dasgupta and Schulman  2007; Balakrishnan et al.  2014); our results show that they are not
really necessary in the large sample limit. However  in any real scenario  the large sample limit may
not accurately characterize the behavior of EM. Therefore  these speciﬁc methods for initialization 
as well as non-asymptotic analysis  are clearly still needed to understand and effectively apply EM.
There are several interesting directions concerning EM that we hope to pursue in follow-up work.
The ﬁrst considers the behavior of EM when the dimension d = dn may grow with the sample size

7

is of the order(cid:112)d/n as t → ∞. Therefore  we conjecture that the theorem still holds as long as

n. Our proof of Theorem 7 reveals that the parameter error of the t-th iterate (in Euclidean norm)

dn = o(n). This would be consistent with results from statistical physics on the MLE for Gaussian
mixtures  which characterize the behavior when dn ∝ n as n → ∞ (Barkai and Sompolinsky  1994).
Another natural direction is to extend these results to more general Gaussian mixture models (e.g. 
with unequal mixing weights or unequal covariances) and other latent variable models.

Acknowledgements. The second named author thanks Yash Deshpande and Sham Kakade for
many helpful initial discussions. JX and AM were partially supported by NSF grant CCF-1420328.
DH was partially supported by NSF grant DMREF-1534910 and a Sloan Fellowship.

References
D. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In Eighteenth

Annual Conference on Learning Theory  pages 458–469  2005.

S. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. The Annals of

Applied Probability  15(1A):69–92  2005.

S. Balakrishnan  M. J. Wainwright  and B. Yu. Statistical guarantees for the EM algorithm: From

population to sample-based analysis. arXiv preprint arXiv:1408.2156  August 2014.

N. Barkai and H. Sompolinsky. Statistical mechanics of the maximum-likelihood density estimation.

Physical Review E  50(3):1766–1769  Sep 1994.

M. Belkin and K. Sinha. Polynomial learning of distribution families. In Fifty-First Annual IEEE

Symposium on Foundations of Computer Science  pages 103–112  2010.

S. C. Brubaker and S. Vempala. Isotropic PCA and afﬁne-invariant clustering. In Forty-Ninth Annual

IEEE Symposium on Foundations of Computer Science  2008.

K. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations and indepen-

dence. In Twenty-First Annual Conference on Learning Theory  pages 9–20  2008.

K. Chaudhuri  S. M. Kakade  K. Livescu  and K. Sridharan. Multi-view clustering via canonical

correlation analysis. In ICML  2009a.

K. Chaudhuri  S. Dasgupta  and A. Vattani. Learning mixtures of Gaussians using the k-means

algorithm. CoRR  abs/0912.0086  2009b.

S. Chrétien and A. O. Hero. On EM algorithms and their proximal generalizations. ESAIM:

Probability and Statistics  12:308–326  May 2008.

D. Conniffe. Expected maximum log likelihood estimation. Journal of the Royal Statistical Society.

Series D  36(4):317–329  1987.

S. Dasgupta. Learning mixutres of Gaussians. In Fortieth Annual IEEE Symposium on Foundations

of Computer Science  pages 634–644  1999.

S. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated  spherical

Gaussians. Journal of Machine Learning Research  8(Feb):203–226  2007.

C. Daskalakis  C. Tzamos  and M. Zampetakis. Ten steps of EM sufﬁce for mixtures of two Gaussians.

arXiv preprint arXiv:1609.00368  September 2016.

A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum-likelihood from incomplete data via the

EM algorithm. J. Royal Statist. Soc. Ser. B  39:1–38  1977.

R. A. Fisher. On the mathematical foundations of theoretical statistics. Philosophical Transactions of

the Royal Society  London  A.  222:309–368  1922.

M. Hardt and E. Price. Tight bounds for learning a mixture of two Gaussians. In Proceedings of the

Forty-Seventh Annual ACM on Symposium on Theory of Computing  pages 753–760  2015.

D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and spectral

decompositions. In Fourth Innovations in Theoretical Computer Science  2013.

C. Jin  Y. Zhang  S. Balakrishnan  M. J. Wainwright  and M. Jordan. Local maxima in the likelihood
of Gaussian mixture models: Structural results and algorithmic consequences. arXiv preprint
arXiv:1609.00978  September 2016.

8

A. T. Kalai  A. Moitra  and G. Valiant. Efﬁciently learning mixtures of two Gaussians. In Forty-second

ACM Symposium on Theory of Computing  pages 553–562  2010.

R. Kannan  H. Salmasian  and S. Vempala. The spectral method for general mixture models. SIAM

Journal on Computing  38(3):1141–1156  2008.

J. M. Klusowski and W. D. Brinda. Statistical guarantees for estimating the centers of a two-

component Gaussian mixture by EM. arXiv preprint arXiv:1608.02280  August 2016.

S. P. Lloyd. Least squares quantization in PCM. IEEE Trans. Information Theory  28(2):129–137 

1982.

J. B. MacQueen. Some methods for classiﬁcation and analysis of multivariate observations. In
Proceedings of the ﬁfth Berkeley Symposium on Mathematical Statistics and Probability  volume 1 
pages 281–297. University of California Press  1967.

A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In Fifty-First

Annual IEEE Symposium on Foundations of Computer Science  pages 93–102  2010.

K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the

Royal Society  London  A.  185:71–110  1894.

R. A. Redner and H. F. Walker. Mixture densities  maximum likelihood and the EM algorithm. SIAM

Review  26(2):195–239  1984.

N. Srebro. Are there local maxima in the inﬁnite-sample likelihood of Gaussian mixture estimation?

In 20th Annual Conference on Learning Theory  pages 628–629  2007.

P. Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of

Operations Research  29(1):27–44  Feb 2004.

F. Vaida. Parameter convergence for EM and MM. Statistica Sinica  15  2005.
S. Vempala and G. Wang. A spectral algorithm for learning mixtures models. Journal of Computer

and System Sciences  68(4):841–860  2004.

C. F. J. Wu. On the convergence properties of the EM algorithm. The Annals of Statistics  11(1):

95–103  Mar 1983.

J. Xu  D. Hsu  and A. Maleki. Global analysis of Expectation Maximization for mixtures of two

Gaussians. arXiv preprint arXiv:1608.07630  2016.

L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures.

Neural Computation  8:129–151  1996.

9

,Chongjie Zhang
Julie Shah
Ji Xu
Daniel Hsu
Arian Maleki