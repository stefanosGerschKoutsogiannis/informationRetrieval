2008,An interior-point stochastic approximation method and an L1-regularized delta rule,The stochastic approximation method is behind the solution to many important  actively-studied problems in machine learning. Despite its far-reaching application  there is almost no work on applying stochastic approximation to learning problems with constraints. The reason for this  we hypothesize  is that no robust  widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically  and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization.,An interior-point stochastic approximation

method and an L1-regularized delta rule

Peter Carbonetto
pcarbo@cs.ubc.ca

Mark Schmidt

schmidtm@cs.ubc.ca

Nando de Freitas
nando@cs.ubc.ca

Department of Computer Science

University of British Columbia

Vancouver  B.C.  Canada V6T 1Z4

Abstract

The stochastic approximation method is behind the solution to many im-
portant  actively-studied problems in machine learning. Despite its far-
reaching application  there is almost no work on applying stochastic ap-
proximation to learning problems with general constraints. The reason for
this  we hypothesize  is that no robust  widely-applicable stochastic ap-
proximation method exists for handling such problems. We propose that
interior-point methods are a natural solution. We establish the stability
of a stochastic interior-point approximation method both analytically and
empirically  and demonstrate its utility by deriving an on-line learning al-
gorithm that also performs feature selection via L1 regularization.

1 Introduction
The stochastic approximation method supplies the theoretical underpinnings behind many
well-studied algorithms in machine learning  notably policy gradient and temporal dif-
ferences for reinforcement learning 
inference for tracking and ﬁltering  on-line learn-
ing [1  17  19]  regret minimization in repeated games  and parameter estimation in prob-
abilistic graphical models  including expectation maximization (EM) and the contrastive
divergences algorithm. The main idea behind stochastic approximation is simple yet pro-
found. It is simple because it is only a slight modiﬁcation to the most basic optimization
method  gradient descent. It is profound because it suggests a fundamentally diﬀerent way
of optimizing a problem—instead of insisting on making progress toward the solution at
every iteration  it only requires that progress be achieved on average.

Despite its successes  people tend to steer clear of constraints on the parameters. While
there is a sizable body of work on treating constraints by extending established optimization
techniques to the stochastic setting  such as projection [14]  subgradient (e.g. [19  27]) and
penalty methods [11  24]  existing methods are either unreliable or suited only to speciﬁc
types of constraints. We argue that a reliable stochastic approximation method that handles
constraints is needed because constraints routinely arise in the mathematical formulation of
learning problems  and the alternative approach—penalization—is often unsatisfactory.

Our main contribution is a new stochastic approximation method in which each step is the
solution to the primal-dual system arising in interior-point methods [7]. Our method is easy
to implement  dominates other approaches  and provides a general solution to constrained
learning problems. Moreover  we show interior-point methods are remarkably well-suited to
stochastic approximation  a result that is far from trivial when one considers that stochastic
algorithms do not behave like their deterministic counterparts (e.g. Wolfe conditions [13]
do not apply). We derive a variant of Widrow and Hoﬀ’s classic “delta rule” for on-line
learning (Sec. 5). It achieves feature selection via L1 regularization (known to statisticians

as the Lasso [22] and to signal processing engineers as basis pursuit [3])  so it is well-suited
to learning problems with lots of data in high dimensions  such as the problem of ﬁltering
spam from your email account (Sec. 5.2). To our knowledge  no method has been proposed
that reliably achieves L1 regularization in large-scale problems when data is processed on-
line or on-demand. Finally  it is important that we establish convergence guarantees for our
method (Sec. 4). To do so  we rely on math from stochastic approximation and optimization.

2 Overview of algorithm

In their 1952 research paper  Robbins and Monro [15] examined the problem of tuning a
control variable x (e.g. amount of alkaline solution) so that the expected outcome of the
experiment F (x) (pH of soil) attains a desired level α (so your Hydrangea have pink blos-
soms). When the distribution of the experimental outcomes is unknown to the statistician
or gardener  it may be still possible to take observations at x. In such case  Robbins and
Monro showed that a particularly eﬀective way to achieve a response level α = 0 is to take
a (hopefully unbiased) measurement yk ≈ F (xk)  adjust the control variable according to

(1)
for step size ak > 0  then repeat. Provided the sequence {ak} behaves like the harmonic
series (see Sec. 4.1)  this algorithm converges to the solution F (x⋆) = 0.

xk+1 = xk − akyk

Since the original publication  mathematicians have extended  generalized  and further weak-
ened the convergence conditions; see [11] for some of these developments. Kiefer and Wol-
fowitz re-interpreted the stochastic process as one of optimizing an unconstrained objective
(F (x) acts as the gradient vector) and later Dvoretsky pointed out that each measurement
y is actually the gradient F (x) plus some noise ξ(x). Hence  the stochastic gradient algo-
rithm. In this paper  we introduce a convergent sequence of nonlinear systems Fµ(x) = 0 and
interpret the Robbins-Monro process {xk} as solving a constrained optimization problem.

procedure IP–SG (Interior-point stochastic gradient)

for k = 1  2  3  . . .

• Set max. step size ˆak and centering parameter σk.
• Set barrier parameter µk = σkzT
• Run simulation to obtain gradient observation yk.
• Compute primal-dual search direction (∆xk  ∆zk)

k c(xk)/m.

by solving equations (6 7) with ∇f (x) = yk.

• Run backtracking line search to ﬁnd largest

We focus on convex optimization
problems [2] of the form

minimize
subject to c(x) ≤ 0 

f (x)

(2)

where c(x) is a vector of inequality
constraints  f (x) and c(x) have con-
tinous partial derivatives  and mea-
surements yk of the gradient at xk are
noisy. The feasible set  by contrast 
should be known exactly. To simplify
our exposition  we do not consider
equality constraints; techniques for
handling them are discussed in [13].
Convexity is a standard assumption made to simplify analysis of stochastic approximation
algorithms and  besides  constrained  non-convex optimization raises unresolved complica-
tions. We assume standard constraint qualiﬁcations so we can legitimately identify optimal
solutions via the Karush-Kuhn-Tucker (KKT) conditions [2  13].

ak ≤ min{ˆak  0.995 mini(−zk i/∆zk i)} such
that c(xk−1 + ak∆xk) < 0  and mini( · ) is
over all i such that ∆zk i < 0.

Figure 1: Proposed stochastic gradient algorithm.

• Set xk = xk−1 + ak∆xk and zk = zk−1 + ak∆zk.

Following the standard barrier approach [7]  we frame the constrained optimization problem
as a sequence of unconstrained objectives. This in turn is cast as a sequence of root-ﬁnding
problems Fµ(x) = 0  where µ > 0 controls for the accuracy of the approximate objective
and should tend toward zero. As we explain  a dramatically more eﬀective strategy is to
solve for the root of the primal-dual equations Fµ(x  z)  where z represents the set of dual
variables. This is the basic formula of the interior-point stochastic approximation method.

Fig. 1 outlines our main contribution. Provided x0 is feasible and z0 > 0  every subsequent
iterate (xk  zk) will be a feasible or “interior” point as well. Notice the absence of a suﬃ-
cient decrease condition on kFµ(x  z)k or suitable merit function; this is not needed in the
stochastic setting. Our stochastic approximation algorithm requires a slightly non-standard
treatment because the target Fµ(x  z) moves as µ changes. Fortunately  convergence under
non-stationarity has been studied in the literature on tracking and adaptive ﬁltering. The
next section is devoted to deriving the primal-dual search direction (∆x  ∆z).

3 Background on interior-point methods
We motivate and derive primal-dual interior-point methods starting from the logarithmic
barrier method. Barrier methods date back to the work of Fiacco and McCormick [6] in
the 1960s  but they lost favour due to their unreliable nature.
Ill-conditioning was long
considered their undoing. However  careful analysis [7] has shown that poor conditioning is
not the problem—rather  it is a deﬁciency in the search direction. In the next section  we
exploit this very analysis to show that every iteration of our algorithm produces a stable
iterate in the face of: 1) ill-conditioned linear systems  2) noisy observations of the gradient.

The logarithmic barrier approach for the constrained optimization problem (2) amounts to
solving a sequence of unconstrained subproblems of the form

minimize

(3)
where µ > 0 is the barrier parameter  and m is the number of inequality constraints.
As µ becomes smaller  the barrier function fµ(x) acts more and more like the objective.
The philosophy of barrier methods diﬀers fundamentally from “exterior” penalty methods
that penalize points violating the constraints [13  Chapter 17] because the logarithm in (3)
prevents iterates from violating the constraints at all  hence the word “barrier”.

fµ(x) ≡ f (x) − µPm

i=1 log(−ci(x)) 

The central thrust of the barrier method is to progressively push µ to zero at a rate which
allows the iterates to converge to the constrained optimum x⋆. Writing out a ﬁrst-order
Taylor-series expansion to the optimality conditions ∇fµ(x) = 0 about a point x  the Newton
step ∆x is the solution to the linear equations ∇2fµ(x) ∆x = −∇fµ(x). The barrier Hessian
has long been known to be incredibly ill-conditioned—this fact becomes apparent by writing
out ∇2fµ(x) in full—but an analysis by Wright [25] shows that the ill-conditioning is not
harmful under the right conditions. The “right conditions” are that x be within a small
distance1 from the central path or barrier trajectory  which is deﬁned to be the sequence of
isolated minimizers x⋆
µ) < 0. The bad news: the barrier
method is ineﬀectual at remaining on the barrier trajectory—it pushes iterates too close to
the boundary where they are no longer well-behaved [7]. Ordinarily  a convergence test is
conducted for each value of µ  but this is not a plausible option for the stochastic setting.

µ satisfying ∇fµ(x⋆

µ) = 0 and c(x⋆

Primal-dual methods form a Newton search direction for both the primal variables and the
Lagrange multipliers. Like classical barrier methods  they fail catastrophically outside the
central path. But their virtue is that they happen to be extremely good at remaining on
the central path (even in the stochastic setting; see Sec. 4.2). Primal-dual methods are also
blessed with strong results regarding superlinear and quadratic rates of convergence [7].

The principal innovation is to introduce Lagrange multiplier-like variables zi ≡ −µ/ci(x).
By setting ∇xfµ(x) to zero  we recover the “perturbed” KKT optimality conditions:

Fµ(x  z) ≡(cid:20) ∇xf (x) + J T Z1

CZ1 + µ1

(cid:21) = 0 

where Z and C are matrices with z and c(x) along their diagonals  and J ≡ ∇xc(x). Forming
a ﬁrst-order Taylor expansion about (x  z)  the primal-dual Newton step is the solution to

(4)

(5)

(cid:20) W J T
ZJ C (cid:21)(cid:20) ∆x

∆z (cid:21) = −(cid:20) ∇xf (x) + J T Z1

CZ1 + µ1

(cid:21)  

where W = H +Pm

xci(x) is the Hessian of the Lagrangian (as written in any textbook
on constrained optimization)  and H is the Hessian of the objective or an approximation.
Through block elimination  the Newton step ∆x is the solution to the symmetric system

i=1 zi∇2

(W − J T ΣJ)∆x = −∇xfµ(x) 

(6)

where Σ ≡ C −1Z. The dual search direction is then recovered according to

∆z = −(z + µ/c(x) + ΣJ∆x).

(7)
Because (2) is a convex optimization problem  we can derive a sensible update rule for the
barrier parameter by guessing the distance between the primal and dual objectives [2]. This
guess is typically µ = −σzT c(x)/m  where σ > 0 is a centering parameter. This update is
supported by the convergence theory (Sec. 4.1) so long as σk is pushed to zero.

1See Sec. 4.3.1 of [7] for the precise meaning of a “small distance”. Since x must be close to the

central path but far from the boundary  the favourable neighbourhood shrinks as µ nears 0.

4 Analysis of convergence

First we establish conditions upon which the sequence of iterates generated by the algorithm
converges almost surely to the solution (x⋆  z⋆) as the amount of data or iteration count goes
to inﬁnity. Then we examine the behaviour of the iterates under ﬁnite-precision arithmetic.

4.1 Asymptotic convergence

A convergence proof from ﬁrst principles is beyond the scope of this paper; we build upon
the martingale convergence proof of Spall and Cristion for non-stationary systems [21].

Assumptions: We establish convergence under the following conditions. They may be
weakened by applying results from the stochastic approximation and optimization literature.
1. Unbiased observations: yk is a discrete-time martingale diﬀerence with respect to

the true gradient ∇f (xk); that is  E(yk | xk  history up to time k) = ∇f (xk).

2. Step sizes: The maximum step sizes ˆak bounding ak (see Fig. 1) must approach

k=1 ˆak = ∞).

zero (ˆak → 0 as k → ∞ and P∞

k=1 ˆa2

k < ∞) but not too quickly (P∞

3. Bounded iterates: lim supk kxkk < ∞ almost surely.
4. Bounded gradient estimates: for some ρ and for every k  E(kykk) < ρ.
5. Convexity: The objective f (x) and constraints c(x) are convex.
6. Strict feasibility: There must exist an x that is strictly feasible; i.e. c(x) < 0.
7. Regularity assumptions: There exists a feasible minimizer x⋆ to the problem (2)
such that ﬁrst-order constraint qualiﬁcation and strict complementarity hold  and
∇xf (x)  ∇xc(x) are Lipschitz-continuous. These conditions allow us to directly apply
standard theorems on constrained optimization for convex programming [2  6  7  13].
Proposition: Suppose Assumptions 1–7 hold. Then θ⋆ ≡ (x⋆  z⋆) is an isolated (locally
unique within a δ-neighbourhood) solution to (2)  and the iterates θk ≡ (xk  zk) of the
feasible interior-point stochastic approximation method (Fig. 1) converge to θ⋆ almost surely;
that is  as k approaches the limit  ||θk − θ⋆|| = 0 with probability 1.
Proof: See Appendix A.

4.2 Considerations regarding the central path

The object of this section is to establish that computing the stochastic primal-dual search
direction is numerically stable. (See Part III of [23] for what we mean by “stable”.) The
concern is that noisy gradient measurements will lead to wildly perturbed search directions.
As we mentioned in Sec. 3  interior-point methods are surprisingly stable provided the
iterates remain close to the central path  but the prospect of keeping close to the path
seems particularly tenuous in the stochastic setting. A key observation is that the central
path is itself perturbed by the stochastic gradient estimates. Following arguments similar
to those given in Sec. 5 of [7]  we show that the stochastic Newton step (6 7) stays on target.

We deﬁne the noisy central path as θ(µ  ε) = (x  z)  where (x  z) is a solution to Fµ(x  z) = 0
with gradient estimate y ≡ ∇f (x) + ε. Suppose we are currently at point θ(µ  ε) = (x  z)
along the path  and the goal is to move closer to θ(µ⋆  ε⋆) = (x⋆  z⋆) by solving (5) or (6 7).
One way to assess the quality of the Newton step is to compare it to the tangent line of the
noisy central path at (µ  ε). Taking implicit partial derivatives at (x  z)  the tangent line is

θ(µ⋆  ε⋆) ≈ θ(µ  ε) + (µ⋆ − µ) ∂θ(µ ε)

∂µ + (y⋆ − y) ∂θ(µ ε)

∂ε

 

such that

ZJ C (cid:21)" (µ⋆ − µ) ∂x
(cid:20) H J T

(µ⋆ − µ) ∂z

∂µ + (y⋆ − y) ∂x
∂µ + (y⋆ − y) ∂z

∂ε # = −(cid:20)

∂ε

y⋆ − y

(µ⋆ − µ)1  (cid:21) .

(8)

(9)

with y⋆ ≡ ∇f (x) + ε⋆. Since we know that Fµ(x  z) = 0  the Newton step (5) at (x  z) with
perturbation µ⋆ and stochastic gradient estimate y⋆ is the solution to

(cid:20) H J T
ZJ C (cid:21)(cid:20) ∆x

∆z (cid:21) = −(cid:20)

y⋆ − y

(µ⋆ − µ)1. (cid:21) .

(10)

In conclusion  if the tangent line (8) is a fairly reasonable approximation to the central path 
then the stochastic Newton step (10) will make good progress toward θ(µ⋆  ε⋆).

Having established that the stochastic gradient algorithm closely follows the noisy central
path  the analysis of M. H. Wright [26] directly applies  in which round-oﬀ error (ǫmachine)
is occasionally replaced by gradient noise (ε). Since stability is of fundamental concern—
particularly in computing the values of W − J T ΣJ  the right-hand side of (6)  and the
solution to ∆x and ∆z—we elaborate on the signiﬁcance of Wright’s results in Appendix B.

5 On-line L1 regularization
In this section  we apply our ﬁndings to the problem of computing an L1-regularized least
squares estimator in an “on-line” manner; that is  by making adjustments to each new
example without having to review all the previous training instances. While this problem
only involves simple bound constraints  we can use it to compare our method to existing
approaches such as gradient projection. We start with some background behind the L1 
motivate the on-line learning approach  draw some experimental comparisons with existing
methods  then show that our algorithm can be used to ﬁlter spam.
Suppose we have n training examples xi ≡ (xi1  . . .   xim)T paired with real-valued responses
yi. (The notation here is separate from previous sections.) Assuming a linear model and
centred coordinates  the least squares estimate β minimizes the mean squared error (MSE).
Linear regression based on the maximum likelihood estimator is one of the basic statistical
tools of science and engineering and  while primitive  generalizes to many popular statistical
estimators  including linear discriminant analysis [9]. Because the least squares estimator is
unstable when m is large  it can generalize poorly to unseen examples. The standard cure
is “regularization ” which introduces bias  but typically produces estimators that are better
at predicting the outputs of unseen examples. For instance  the MSE with an L1-penalty 

MSE(L1) ≡ 1

i=1(yi − xT

i β)2 + λ

n kβk1 

(11)

2nPn

not only prevents overﬁtting but tends to produce estimators that shrink many of the
components βj to zero  resulting in sparse codes. Here  k · k1 is the L1 norm and λ > 0
controls for the level of regularization. This approach has been independently studied for
many problems  including statistical regression [22] and sparse signal reconstruction [3  10] 
precisely because it is eﬀective at choosing useful features for prediction.

We can treat the gradient of MSE as a sample expectation over responses of the form
−xi(yi − xT

i β)  so the on-line or stochastic update

β(new) = β + axi(yi − xT

i β) 

(12)

improves the linear regression with only a single data point (a is the step size).2 This is the
famed “delta rule” of Widrow and Hoﬀ [12]. Since standard “batch” learning requires a full
pass through the data for each gradient evaluation  the on-line update (12) may be the only
viable option when faced with  for instance  a collection of 80 million images [16]. On-line
learning for regression and classiﬁcation—including L2 regularization—is a well-researched
topic  particularly for neural networks [17] and support vector machines (e.g. [19]). On-line
learning with L1 regularization  despite its ascribed beneﬁts  has strangely avoided study.
(The only known work that has approached the problem is [27] using subgradient methods.)

We derive an on-line  L1-regularized learning rule of the form

z(new)
β(new)
pos = zpos + a(µ/βpos − zpos − ∆βposzpos/βpos)
pos = βpos + a∆βpos
β(new)
z(new)
neg = zneg + a(µ/βneg − zneg − ∆βnegzneg/βneg) 
neg = βneg + a∆βneg
such that ∆βpos = (xi(yi − xT

n + µ/βpos)/(1 + zpos/βpos)

i β) − λ

∆βneg = (−xi(yi + xT

i β) − λ

n + µ/βneg)/(1 + zneg/βneg) 

(13)

and where µ > 0 is the barrier parameter  β = βpos − βneg  zpos and zneg are the Lagrange
multipliers associated with the lower bounds βpos ≥ 0 and βneg ≥ 0  respectively  and a is a
step size ensuring the variables remain in the positive quadrant. Multiplication and division
in (13) are component-wise. The remainder of the algorithm (Fig. 1) consists of choosing µ
and feasible step size a at each iteration. Let us brieﬂy explain how we arrived at (13).

2The gradient descent direction can be a poor choice because it ignores the scaling of the problem.
Much work has focused on improving the delta rule  but we shall not discuss these improvements.

 

 
 

 

 
 

Figure 2: (left) Performance of constrained stochastic gradient methods for diﬀerent step size
sequences. (right) Performance of methods for increasing levels of variance in the dimensions
of the training data. Note the logarithmic scale in the vertical axis.

It is diﬃcult to ﬁnd a collection of regression coeﬃcients β that directly minimizes MSE(L1)
because the L1 norm is not diﬀerentiable near zero. The trick is to separate the coeﬃcients
into their positive (βpos) and negative (βneg) components following [3]  thereby transform-
ing the non-smooth  unconstrained optimization problem (11) into a smooth problem with
convex  quadratic objective and bound constraints βpos  βneg ≥ 0. The regularized delta
rule (13) is then obtained from direct application of the primal-dual interior-point Newton
search direction (6 7) with a stochastic gradient (see Eq. 12)  and identity in place of H.

5.1 Experiments

We ran four small experiments to assess the reliability and shrinkage eﬀect of the interior-
point stochastic gradient method for linear regression with L1 regularization; refer to Fig. 1
and Eq. 13.3 We also studied four alternatives to our method: 1) a subgradient method 
2) a smoothed  unconstrained approximation to (11)  3) a projected gradient method  and
4) the augmented Lagrangian approach described in [24]. See [18] for an in-depth discussion
of the merits of applying the ﬁrst three optimization approaches to L1 regularization. All
these methods have a per-iteration cost on the order of the number of features.

Method. For the ﬁrst three experiments  we simulated 20 data sets following the procedure
described in Sec. 7.5 of [22]. Each data set had n = 100 observations with m = 40 features.
We deﬁned observations by xij = zij + zi  where zi was drawn from the standard normal
and zij was drawn i.i.d. from the normal with variance σ2
j   which in turn was drawn from
the inverse Gamma with shape 2.5 and scale ν = 1. (The mean of σ2
j is proportional to ν.)
The regression coeﬃcients were β = (0  . . .   0  2  . . .   2  0  . . .   0  2  . . .   2)T with 10 repeats in
each block [22]. Outputs were generated according to yi = βT xi + ǫ with standard Gaussian
noise ǫ. Each method was executed with a single pass on the data (100 iterations) with
step sizes ˆak = 1/(k0 + k)  where k0 = 50 by default. We chose L1 penalty λ/n = 1.25 
which tended to produce about 30% zero coeﬃcients at the solution to (11). The augmented
Lagrangian required a sequence of penalty terms rk → 0; after some trial and error  we chose
rk = 50/(k0 + k)0.1. The control variables of Experiments 1  2 and 3 were  respectively  the
step size parameter k0  the inverse Gamma scale parameter ν  and the L1 penalty parameter
λ. In Experiment 4  each example yi in the training set xi had 8 features  and we set the
true coeﬃcients were set to β = (0  0  2  −4  0  0  −1  3)T .
Results. Fig. 2 shows the results of Experiments 1 and 2  with error 1
n kβexact − βon-linek1
averaged over the 20 data sets  in which βexact is the solution to (11)  and βon-line is the esti-
mate obtained after 100 iterations of the on-line or stochastic gradient method. With a large
enough step size  almost all the methods converged close to βexact. The stochastic interior-
point method  however  always came closest to βexact and  for the range of values we tried  its
solution was by far the least sensitive to the step size sequence and level of variance in the ob-
servations. Experiment 3 (Fig. 3) shows that even with well-chosen step sizes for all methods 

3The Matlab code for all our experiments is on the Web at http://www.cs.ubc.ca/∼pcarbo.

Figure 4: Shrinkage eﬀect for diﬀerent choices of the L1 penalty parameter.

the stochastic interior-point method still best approximated the exact solution  and its per-
formance did not degrade when λ was small. (The dashed vertical line at λ/n = 1.25 in Fig. 3
corresponds to k0 = 50 and E(σ2) = 2/3 in the left and right plots of Fig. 2.) Fig. 4 shows the
regularized estimates of Experiment 4. After one
pass through the data (middle)—equivalent to a
single iteration of an exact solver—the interior-
point stochastic gradient method shrank some
of the data components  but didn’t quite dis-
card irrelevant features altogether. After 10 vis-
its to the training data (right)  the stochastic al-
gorithm exhibited feature selection close to what
we would normally expect from the Lasso (left).

 

 

5.2 Filtering spam

 

Classifying email as spam or not is most faith-
fully modeled as an on-line learning problem in
which supervision is provided after each email
has been designated for the inbox or trash [5]. An eﬀective ﬁlter is one that minimizes mis-
classiﬁcation of incoming messages—throwing away a good email being considerably more
deleterious than incorrectly placing a spam in the inbox. Without any prior knowledge as
to what spam looks like  any ﬁlter will be error-prone at initial stages of deployment.

Figure 3: Performance of the methods
for various choices of the L1 penalty.

Spam ﬁltering necessarily involves lots of data and an even larger number of features  so
a sparse  stable model is essential. We adapted the L1-regularized delta rule to the spam
ﬁltering problem by replacing the linear regression with a binary logistic regression [9]. The
on-line updates are similar to (13)  only xT
i β)  with φ(u) ≡ 1/(1+e−u).
To our knowledge  no one has investigated this approach for on-line spam ﬁltering  though
there is some work on logistic regression plus the Lasso for batch classiﬁcation in text
corpora [8]. Needless to say  batch learning is completely impractical in this setting.

i β is replaced by φ(xT

2   ˆai = 1

Method. We simulated the on-line spam ﬁltering task on the trec2005 corpus [4] contain-
ing emails from the legal investigations of Enron corporation. We compared our on-line clas-
siﬁer (λ = 10  σ = 1
1+i ) with two open-source software packages  SpamBayes 1.0.3
and Bogoﬁlter 0.93.4. (These packages are publicly available at spambayes.sourceforge.net
and bogoﬁlter.sourceforge.net.) A full comparison is certainly beyond the scope of this paper;
see [5] for a comprehensive evaluation. We represented each email as a vector of normalized
word frequencies  and used the word tokens extracted by SpamBayes. In the end  we had
an on-line learning problem involving n = 92189 documents and m = 823470 features.

true

not spam spam

true

not spam spam

true

not spam spam

. not spam 39382

spam

17

d
e
r
p

3291
49499

. not spam 39393

spam

3

d
e
r
p

5515
47275

. not spam 39389

spam

10

d
e
r
p

2803
49987

Results for SpamBayes

Results for Bogoﬁlter

Results for Logistic + L1

Table 1: Contingency tables for on-line spam ﬁltering task on the trec2005 data set.

Results. Following [5]  we use contingency tables to present results of the on-line spam
ﬁltering experiment (Table 1). The top-right/bottom-left entry of each table is the number
of misclassiﬁed spam/non-spam. Everything was evaluated on-line. We tagged an email
for deletion only if p(yi = spam) ≥ 97%. Our spam ﬁlter dominated SpamBayes on the
trec2005 corpus  and performed comparably to Bogoﬁlter—one of the best spam ﬁlters to
date [5]. Our model’s expense was slightly greater than the others. As we found in Sec. 5.1 
assessing sparsity of the on-line solution is more diﬃcult than in the exact case  but we can
say that removing the 41% smallest entries of β resulted in almost no (< 0.001) change.

6 Conclusions
Our experiments on a learning problem with noisy gradient measurements and bound con-
straints show that the interior-point stochastic approximation algorithm is a signiﬁcant
improvement over other methods. The interior-point approach also has the virtue of being
much more general  and our analysis guarantees that it will be numerically stable.

Acknowledgements. Thanks to Ewout van den Berg  Matt Hoﬀman and Firas Hamze.

References

[1] L. Bottou and O. Bousquet  The tradeoﬀs of large scale learning  in Advances in Neural Infor-

mation Processing Systems  vol. 20  1998.

[2] S. Boyd and L. Vandenberghe  Convex optimization  Cambridge University Press  2004.
[3] S. Chen  D. Donoho  and M. Saunders  Atomic decomposition by basis pursuit  SIAM Journal

on Scientiﬁc Computing  20 (1999)  pp. 33–61.

[4] G. V. Cormack and T. R. Lynam  Spam corpus creation for TREC  in Proc. 2nd CEAS  2005.
  Online supervised spam ﬁlter evaluation  ACM Trans. Information Systems  25 (2007).
[5]
[6] A. V. Fiacco and G. P. McCormick  Nonlinear programming: sequential unconstrained mini-

mization techniques  John Wiley and Sons  1968.

[7] A. Forsgren  P. E. Gill  and M. H. Wright  Interior methods for nonlinear optimization  SIAM

Review  44 (2002)  pp. 525–597.

[8] A. Genkin  D. D. Lewis  and D. Madigan  Large-scale Bayesian logistic regression for text

categorization  Technometrics  49 (2007)  pp. 291–304.

[9] T. Hastie  R. Tibshirani  and J. Friedman  The elements of statistical learning  Springer  2001.
[10] S.-J. Kim  K. Koh  M. Lustig  S. Boyd  and D. Gorinevsky  An interior-point method for
large-scale L1-regularized least squares  IEEE J. Selected Topics in Signal Processing  1 (2007).
[11] H. J. Kushner and D. S. Clark  Stochastic approximation methods for constrained and uncon-

strained systems  Springer-Verlag  1978.

[12] T. M. Mitchell  Machine Learning  McGraw-Hill  1997.
[13] J. Nocedal and S. J. Wright  Numerical Optimization  Springer  2nd ed.  2006.
[14] B. T. Poljak  Nonlinear programming methods in the presence of noise  Mathematical Pro-

gramming  14 (1978)  pp. 87–97.

[15] H. Robbins and S. Monro  A stochastic approximation method  Annals Math. Stats.  22 (1951).
[16] B. C. Russell  A. Torralba  K. P. Murphy  and W. T. Freeman  LabelMe: a database and
web-based tool for image annotation  Intl. Journal of Computer Vision  77 (2008)  pp. 157–173.

[17] D. Saad  ed.  On-line learning in neural networks  Cambridge University Press  1998.
[18] M. Schmidt  G. Fung  and R. Rosales  Fast optimization methods for L1 regularization  in

Proceedings of the 18th European Conference on Machine Learning  2007  pp. 286–297.

[19] S. Shalev-Shwartz  Y. Singer  and N. Srebro  Pegasos: primal estimated sub-gradient solver for

SVM  in Proceedings of the 24th Intl. Conference on Machine learning  2007  pp. 807–814.

[20] J. C. Spall  Adaptive stochastic approximation by the simultaneous perturbation method  IEEE

Transactions on Automatic Control  45 (2000)  pp. 1839–1853.

[21] J. C. Spall and J. A. Cristion  Model-free control of nonlinear stochastic systems with discrete-

time measurements  IEEE Transactions on Automatic Control  43 (1998)  pp. 1148–1210.

[22] R. Tibshirani  Regression shrinkage and selection via the Lasso  Journal of the Royal Statistical

Society  58 (1996)  pp. 267–288.

[23] L. N. Trefethen and D. Bau  Numerical linear algebra  SIAM  1997.
[24] I. Wang and J. C. Spall  Stochastic optimization with inequality constraints using simultaneous

perturbations and penalty functions  in Proc. 42nd IEEE Conf. Decision and Control  2003.

[25] M. H. Wright  Some properties of the Hessian of the logarithmic barrier function  Mathematical

Programming  67 (1994)  pp. 265–295.

[26]

  Ill-conditioning and computational error in interior methods for nonlinear programming 

SIAM Journal on Optimization  9 (1998)  pp. 84–111.

[27] A. Zheng  Statistical software debugging  PhD thesis  University of California  Berkeley  2005.

,Yichuan Zhang
Charles Sutton