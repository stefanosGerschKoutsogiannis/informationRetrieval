2011,Multi-Bandit Best Arm Identification,We study the problem of identifying the best arm in each of the bandits in a multi-bandit multi-armed setting. We first propose an algorithm called Gap-based Exploration (GapE) that focuses on the arms whose mean is close to the mean of the best arm in the same bandit (i.e.  small gap). We then introduce an algorithm  called GapE-V  which takes into account the variance of the arms in addition to their gap. We prove an upper-bound on the probability of error for both algorithms. Since GapE and GapE-V need to tune an exploration parameter that depends on the complexity of the problem  which is often unknown in advance  we also introduce variations of these algorithms that estimate this complexity online. Finally  we evaluate the performance of these algorithms and compare them to other allocation strategies on a number of synthetic problems.,Multi-Bandit Best Arm Identiﬁcation

Victor Gabillon

Mohammad Ghavamzadeh

INRIA Lille - Nord Europe  Team SequeL

Alessandro Lazaric

{victor.gabillon mohammad.ghavamzadeh alessandro.lazaric}@inria.fr

Department of Operations Research and Financial Engineering  Princeton University

S´ebastien Bubeck

sbubeck@princeton.edu

Abstract

We study the problem of identifying the best arm in each of the bandits in a multi-
bandit multi-armed setting. We ﬁrst propose an algorithm called Gap-based Ex-
ploration (GapE) that focuses on the arms whose mean is close to the mean of
the best arm in the same bandit (i.e.  small gap). We then introduce an algorithm 
called GapE-V  which takes into account the variance of the arms in addition to
their gap. We prove an upper-bound on the probability of error for both algo-
rithms. Since GapE and GapE-V need to tune an exploration parameter that de-
pends on the complexity of the problem  which is often unknown in advance  we
also introduce variations of these algorithms that estimate this complexity online.
Finally  we evaluate the performance of these algorithms and compare them to
other allocation strategies on a number of synthetic problems.

1 Introduction
Consider a clinical problem with M subpopulations  in which one should decide between Km op-
tions for treating subjects from each subpopulation m. A subpopulation may correspond to patients
with a particular gene biomarker (or other risk categories) and the treatment options are the available
treatments for a disease. The main objective here is to construct a rule  which recommends the best
treatment for each of the subpopulations. These rules are usually constructed using data from clin-
ical trials that are generally costly to run. Therefore  it is important to distribute the trial resources
wisely so that the devised rule yields a good performance. Since it may take signiﬁcantly more
resources to ﬁnd the best treatment for one subpopulation than for the others  the common strategy
of enrolling patients as they arrive may not yield an overall good performance. Moreover  applying
treatment options uniformly at random in a subpopulation could not only waste trial resources  but
also it might run the risk of ﬁnding a bad treatment for that subpopulation. This problem can be for-
mulated as the best arm identiﬁcation over M multi-armed bandits [1]  which itself can be seen as
the problem of pure exploration [4] over multiple bandits. In this formulation  each subpopulation is
considered as a multi-armed bandit  each treatment as an arm  trying a medication on a patient as a
pull  and we are asked to recommend an arm for each bandit after a given number of pulls (budget).
The evaluation can be based on 1) the average over the bandits of the reward of the recommended
arms  or 2) the average probability of error (not selecting the best arm)  or 3) the maximum prob-
ability of error. Note that this setting is different from the standard multi-armed bandit problem in
which the goal is to maximize the cumulative sum of rewards (see e.g.  [13  3]).

The pure exploration problem is about designing strategies that make the best use of the limited bud-
get (e.g.  the total number of patients that can be admitted to the clinical trial) in order to optimize the
performance in a decision-making task. Audibert et al. [1] proposed two algorithms to address this
problem: 1) a highly exploring strategy based on upper conﬁdence bounds  called UCB-E  in which
the optimal value of its parameter depends on some measure of the complexity of the problem  and
2) a parameter-free method based on progressively rejecting the arms which seem to be suboptimal 
called Successive Rejects. They showed that both algorithms are nearly optimal since their probabil-
ity of returning the wrong arm decreases exponentially at a rate. Racing algorithms (e.g.  [10  12])

1

and action-elimination algorithms [7] address this problem under a constraint on the accuracy in
identifying the best arm and they minimize the budget needed to achieve that accuracy. However 
UCB-E and Successive Rejects are designed for a single bandit problem  and as we will discuss later 
cannot be easily extended to the multi-bandit case studied in this paper. Deng et al. have recently
proposed an active learning algorithm for resource allocation over multiple bandits [5]. However 
they do not provide any theoretical analysis for their algorithm and only empirically evaluate its per-
formance. Moreover  the target of their proposed algorithm is to minimize the maximum uncertainty
in estimating the value of the arms for each bandit. Note that this is different than our target  which
is to maximize the quality of the arms recommended for each bandit.

In this paper  we study the problem of best-arm identiﬁcation in a multi-armed multi-bandit setting
under a ﬁxed budget constraint  and propose an algorithm  called Gap-based Exploration (GapE)  to
solve it. The allocation strategy implemented by GapE focuses on the gap of the arms  i.e.  the differ-
ence between the mean of the arm and the mean of the best arm (in that bandit). The GapE-variance
(GapE-V) algorithm extends this approach taking into account also the variance of the arms. For
both algorithms  we prove an upper-bound on the probability of error that decreases exponentially
with the budget. Since both GapE and GapE-V need to tune an exploration parameter that depends
on the complexity of the problem  which is rarely known in advance  we also introduce their adaptive
version. Finally  we evaluate the performance of these algorithms and compare them with Uniform
and Uniform+UCB-E strategies on a number of synthetic problems. Our empirical results indicate
that 1) GapE and GapE-V have a better performance than Uniform and Uniform+UCB-E  and 2) the
adaptive version of these algorithms match the performance of their non-adaptive counterparts.

2 Problem Setup
In this section  we introduce the notation used throughout the paper and formalize the multi-bandit
best arm identiﬁcation problem. Let M be the number of bandits and K be the number of arms for
each bandit (we use indices m  p  q for the bandits and k  i  j for the arms). Each arm k of a bandit
m is characterized by a distribution νmk bounded in [0  b] with mean µmk and variance σ2
mk. In the
following  we assume that each bandit has a unique best arm. We denote by µ∗m and k∗m the mean and
the index of the best arm of bandit m (i.e.  µ∗m = max1≤k≤K µmk  k∗m = arg max1≤k≤K µmk). In
each bandit m  we deﬁne the gap for each arm as ∆mk = | maxj6=k µmj − µmk|.
The clinical trial problem described in Sec. 1 can be formalized as a game between a stochastic multi-
bandit environment and a forecaster  where the distributions {νmk} are unknown to the forecaster.
At each round t = 1  . . .   n  the forecaster pulls a bandit-arm pair I(t) = (m  k) and observes
a sample drawn from the distribution νI(t) independent from the past. The forecaster estimates
the expected value of each arm by computing the average of the samples observed over time. Let
Tmk(t) be the number of times that arm k of bandit m has been pulled by the end of round t 
s=1 Xmk(s)  where Xmk(s) is the
s-th sample observed from νmk. Given the previous deﬁnitions  we deﬁne the estimated gaps as

then the mean of this arm is estimated asbµmk(t) = 1
b∆mk(t) = | maxj6=kbµmj(t)−bµmk(t)|. At the end of round n  the forecaster returns for each bandit
m the arm with the highest estimated mean  i.e.  Jm(n) = arg maxkbµmk(n)  and incurs a regret
MXm=1(cid:0)µ∗

As discussed in the introduction  other performance measures can be deﬁned for this problem. In
some applications  returning the wrong arm is considered as an error independently from its regret 
and thus  the objective is to minimize the average probability of error

Tmk(t)PTmk(t)

m − µmJm (n)(cid:1).

1
M

MXm=1

r(n) =

rm(n) =

1
M

e(n) =

1
M

em(n) =

1
M

MXm=1

MXm=1

P(cid:0)Jm(n) 6= k∗
m(cid:1).

Finally  in problems similar to the clinical trial  a reasonable objective is to return the right treatment
for all the genetic proﬁles and not just to have a small average probability of error. In this case  the
global performance of the forecaster can be measured as

ℓ(n) = max

m

ℓm(n) = max

m

P(cid:0)Jm(n) 6= k∗
m(cid:1).

It is interesting to note the relationship between these three performance measures: minm ∆m ×
e(n) ≤ Er(n) ≤ b×e(n) ≤ b×ℓ(n)  where the expectation in the regret is w.r.t. the random samples.
As a result  any algorithm minimizing the worst case probability of error  ℓ(n)  also controls the
average probability of error  e(n)  and the simple regret Er(n). Note that the algorithms introduced
in this paper directly target the problem of minimizing ℓ(n).

2

Parameters: number of rounds n  exploration parameter a  maximum range b

for t = 1  2  . . .   n do

Draw I(t) ∈ arg maxm k Bmk(t)

Initialize: Tmk(0) = 0  b∆mk(0) = 0 for all bandit-arm pairs (m  k)
Compute Bmk(t) = −b∆mk(t − 1) + bq a
Observe XI(t)(cid:0)TI(t)(t − 1) + 1(cid:1) ∼ νI(t)
Update TI(t)(t) = TI(t)(t − 1) + 1 and b∆mk(t) ∀k of the selected bandit
Return Jm(n) ∈ arg maxk∈{1 ... K}bµmk(n)  ∀m ∈ {1 . . . M }

Figure 1: The pseudo-code of the gap-based Exploration (GapE) algorithm.

for all bandit-arm pairs (m  k)

end for

Tmk (t−1)

3 The Gap-based Exploration Algorithm
Fig. 1 contains the pseudo-code of the gap-based exploration (GapE) algorithm. GapE ﬂattens the
bandit-arm structure and reduces it to a single-bandit problem with M K arms. At each time step t 
the algorithm relies on the observations up to time t − 1 to build an index Bmk(t) for each bandit-
arm pair  and then selects the pair I(t) with the highest index. The index Bmk consists of two
terms. The ﬁrst term is the negative of the estimated gap for arm k in bandit m. Similar to other
upper-conﬁdence bound (UCB) methods [3]  the second part is an exploration term which forces the
algorithm to pull arms that have been less explored. As a result  the algorithm tends to pull arms
with small estimated gap and small number of pulls. The exploration parameter a tunes the level
of exploration of the algorithm. As it is shown by the theoretical analysis of Sec. 3.1  if the time
horizon n is known  a should be set to a = 4
mk is the complexity of
9
the problem (see Sec. 3.1 for further discussion). Note that GapE differs from most standard bandit
strategies in the sense that the B-index for an arm depends explicitly on the statistics of the other
arms. This feature makes the analysis of this algorithm much more involved.

H   where H =Pm k b2/∆2

n−K

As we may notice from Fig. 1  GapE resembles the UCB-E algorithm [1] designed to solve the pure
exploration problem in the single-bandit setting. Nonetheless  the use of the negative estimated gap

(−b∆mk) instead of the estimated mean (bµmk) (used by UCB-E) is crucial in the multi-bandit setting.

In the single-bandit problem  since the best and second best arms have the same gap (∆mk∗
m =
m ∆mk)  GapE considers them equivalent and tends to pull them the same amount of time 
mink6=k∗
while UCB-E tends to pull the best arm more often than the second best one. Despite this difference 
the performance of both algorithms in predicting the best arm after n pulls would be the same. This is
due to the fact that the probability of error depends on the capability of the algorithm to distinguish
optimal and suboptimal arms  and this is not affected by a different allocation over the best and
second best arms as long as the number of pulls allocated to that pair is large enough w.r.t. their gap.
Despite this similarity  the two approaches become completely different in the multi-bandit case. In
this case  if we run UCB-E on all the M K arms  it tends to pull more the arm with the highest mean
over all the bandits  i.e.  k∗ = arg maxm k µmk. As a result  it would be accurate in predicting the
best arm k∗ over bandits  but may have an arbitrarily bad performance in predicting the best arm for
each bandit  and thus  may incur a large error ℓ(n). On the other hand  GapE focuses on the arms
with the smallest gaps. This way  it assigns more pulls to bandits whose optimal arms are difﬁcult
to identify (i.e.  bandits with arms with small gaps)  and as shown in the next section  it achieves a
high probability in identifying the best arm in each bandit.

3.1 Theoretical Analysis
In this section  we derive an upper-bound on the probability of error ℓ(n) for the GapE algorithm.
Theorem 1. If we run GapE with parameter 0 < a ≤ 4

H   then its probability of error satisﬁes

n−M K

9

in particular for a = 4
9

ℓ(n) ≤ P(cid:0)∃m : Jm(n) 6= k∗m(cid:1) ≤ 2M Kn exp(−
H   we have ℓ(n) ≤ 2M Kn exp(− 1

a
64
n−M K
H ).

n−M K

144

) 

Remark 1 (Analysis of the bound). If the time horizon n is known in advance  it would be possible
to set the exploration parameter a as a linear function of n  and as a result  the probability of error of
GapE decreases exponentially with the time horizon. The other interesting aspect of the bound is the

3

complexity term H appearing in the optimal value of the exploration parameter a (i.e.  a = 4
H ).
n−K
9
mk  the complexity of arm k in bandit m  it is clear from the deﬁnition
If we denote by Hmk = b2/∆2
of H that each arm has an additive impact on the overall complexity of the multi-bandit problem.
mk (similar to the

Moreover  if we deﬁne the complexity of each bandit m as Hm = Pk b2/∆2
deﬁnition of complexity for UCB-E in [1])  the GapE complexity may be rewritten as H =Pm Hm.

This means that the complexity of GapE is simply the sum of the complexities of all the bandits.
Remark 2 (Comparison with the static allocation strategy). The main objective of GapE is to
tradeoff between allocating pulls according to the gaps (more precisely  according to the complex-
ities Hmk) and the exploration needed to improve the accuracy of their estimates. If the gaps were
known in advance  a nearly-optimal static allocation strategy assigns to each bandit-arm pair a num-
ber of pulls proportional to its complexity. Let us consider a strategy that pulls each arm a ﬁxed
number of times over the horizon n. The probability of error for this strategy may be bounded as

ℓStatic(n) ≤ P(cid:0)∃m : Jm(n) 6= k∗

MXm=1 Xk6=k∗
MXm=1
m (n) ≤ ˆµmk(n)(cid:1)
P(cid:0)ˆµmk∗
m(cid:1) ≤
P(cid:0)Jm(n) 6= k∗
m(cid:1) ≤
MXm=1 Xk6=k∗
exp(cid:0) − Tmk(n)
exp(cid:0) − Tmk(n)H −1
mk(cid:1).
b2 (cid:1) =

∆2

mk

m

m

(1)

≤

MXm=1 Xk6=k∗

m

Given the constraint Pmk Tmk(n) = n  the allocation minimizing the last term in Eq. 1 is
T ∗mk(n) = nHmk/H. We refer to this ﬁxed strategy as StaticGap. Although this is not neces-
sarily the optimal static strategy (T ∗mk(n) minimizes an upper-bound)  this allocation guarantees
a probability of error smaller than M K exp(−n/H). Theorem 1 shows that  for n large enough 
GapE achieves the same performance as the static allocation StaticGap.
Remark 3 (Comparison with other allocation strategies). At the beginning of Sec. 3  we dis-
cussed the difference between GapE and UCB-E. Here we compare the bound reported in Theo-
rem 1 with the performance of the Uniform and combined Uniform+UCB-E allocation strategies. In
the uniform allocation strategy  the total budget n is uniformly split over all the bandits and arms.
As a result  each bandit-arm pair is pulled Tmk(n) = n/(M K) times. Using the same derivation as
in Remark 2  the probability of error ℓ(n) for this strategy may be bounded as

ℓUnif(n) ≤

MXm=1 Xk6=k∗

m

exp(cid:0) −

n

M K

∆2

mk

b2 (cid:1) ≤ M K exp(cid:16) −

n

M K maxm k Hmk(cid:17).

In the Uniform+UCB-E allocation strategy  i.e.  a two-level algorithm that ﬁrst selects a bandit
uniformly and then pulls arms within each bandit using UCB-E  the total number of pulls for each

bandit m isPk Tmk(n) = n/M   while the number of pulls Tmk(n) over the arms in bandit m is

determined by UCB-E. Thus  the probability of error of this strategy may be bounded as

n/M − K

MXm=1

ℓUnif+UCB-E(n) ≤

n/M − K

18Hm (cid:17) ≤ 2nM K exp(cid:16) −

2nK exp(cid:16) −

18 maxm Hm(cid:17) 
where the ﬁrst inequality follows from Theorem 1 in [1] (recall that Hm =Pk b2/∆2
mk). Let b = 1
(i.e.  all the arms have distributions bounded in [0  1])  up to constants and multiplicative factors in
front of the exponentials  and if n is large enough compared to M and K (so as to approximate
n/M − K and n − K by n)  the probability of error for the three algorithms may be bounded as
ℓUnif(n) ≤ exp(cid:16)O(cid:0) −n/M K
Hmk(cid:1)(cid:17).
Hm(cid:1)(cid:17)  ℓGapE(n) ≤ exp(cid:16)O(cid:0) −n
Pm k
M K maxm k Hmk ≥ M maxmPk Hmk ≥Pm k Hmk  which implies that the upper bound on the

Hmk(cid:1)(cid:17)  ℓU+UCBE(n) ≤ exp(cid:16)O(cid:0) −n/M

probability of error of GapE is usually signiﬁcantly smaller. This relationship  which is conﬁrmed
by the experiments reported in Sec. 4  shows that GapE is able to adapt to the complexity H of
the overall multi-bandit problem better than the other two allocation strategies. In fact  while the
performance of the Uniform strategy depends on the most complex arm over the bandits and the
strategy Unif+UCB-E is affected by the most complex bandit  the performance of GapE depends on
the sum of the complexities of all the arms involved in the pure exploration problem.

By comparing the arguments of the exponential terms  we have the trivial sequence of inequalities

max
m k

max

m

4

Proof of Theorem 1. Step 1. Let us consider the following event:

E =(cid:26)∀m ∈ {1  . . .   M }  ∀k ∈ {1  . . .   K}  ∀t ∈ {1  . . .   n}  (cid:12)(cid:12)bµmk(t) − µmk(cid:12)(cid:12) < bcr a
Tmk(t)(cid:27).
From Chernoff-Hoeffding’s inequality and a union bound  we have P(ξ) ≥ 1−2M Kn exp(−2ac2).
Now we would like to prove that on the event E  we ﬁnd the best arm for all the bandits  i.e.  Jm(n) =
k∗m  ∀m ∈ {1 . . . M}. Since Jm(n) is the empirical best arm of bandit m  we should prove that for
any k ∈ {1  . . .   K}  bµmk(n) ≤ bµmk∗
m(n). By upper-bounding the LHS and lower-bounding the
RHS of this inequality  we note that it would be enough to prove bcpa/Tmk(n) ≤ ∆mk/2 on the
event E  or equivalently  to prove that for any bandit-arm pair m  k  we have Tmk(n) ≥ 4ab2c2
Step 2. In this step  we show that in GapE  for any bandits (m  q) and arms (k  j)  and for any
t ≥ M K  the following dependence between the number of pulls of the arms holds

∆2

mk

.

−∆mk + (1 + d)br

a

max(cid:0)Tmk(t) − 1  1(cid:1) ≥ −∆qj + (1 − d)br a

Tqj(t)

 

(2)

where d ∈ [0  1]. We prove this inequality by induction.
Base step. We know that after the ﬁrst M K rounds of the GapE algorithm  all the arms have been
pulled once  i.e.  Tmk(t) = 1  ∀m  k  thus if a ≥ 1/4d2  the inequality (2) holds for t = M K.
Inductive step. Let us assume that (2) holds at time t − 1 and we pull arm i of bandit p at time t 
i.e.  I(t) = (p  i). So at time t  the inequality (2) trivially holds for every choice of m  q  k  and
j  except when (m  k) = (p  i). As a result  in the inductive step  we only need to prove that the
following holds for any q ∈ {1  . . . M} and j ∈ {1  . . . K}

Since arm i of bandit p has been pulled at time t  we have that for any bandit-arm pair (q  j)

. (5)

Tqj(t)

Tpi(t) − 1

1−c and d ≥ 2√2c

2√2bc

1 − d r a

and −b∆qj(t−1) ≥ −∆qj−

We report the proofs of the inequalities in (5) in App. B of [8]. The inequality (3)  and as a result 

To prove (3)  we ﬁrst prove an upper-bound for −b∆pi(t − 1) and a lower-bound for −b∆qj(t − 1)
−b∆pi(t−1) ≤ −∆pi+
the inductive step is proved by replacing −b∆pi(t− 1) and −b∆qj(t− 1) in (4) from (5) and under the
1−d . These conditions are satisﬁed by d = 1/2 and c = √2/16.
conditions that d ≥ 2c
Step 3. In order to prove the condition of Tmk(n) in step 1  we need to ﬁnd a lower-bound on the
number of pulls of all the arms at time t = n (at the end). Let us assume that arm k of bandit m has
been pulled less than ab2(1−d)2
Tmk(n) > 0. From this
+ 1

result and (2)  we have −∆qj + (1 + d)bq a
for any pair (q  j). We also know thatPq j Tqj(n) = n. From these  we deduce that n − M K <
ab2(1 + d)2Pq j
the ﬁrst assumption that Tmk(n) < ab2(1−d)2
for any pair
(m  k)  when 1 − d ≥ 2c. This concludes the proof. The condition for a in the statement of the
theorem comes from our choice of a in this step and the values of c and d from the inductive step.

. So  if we select a such that n− M K ≥ ab2(1 + d)2Pq j

  which indicates that −∆mk + (1 − d)bq a

Tqj (n)−1 > 0  or equivalently Tqj(n) < ab2(1+d)2

∆2
qj

  which means that Tmk(n) ≥ 4ab2c2

∆2

  we contradict

1
∆2
qj

1
∆2
qj

∆2

mk

∆2

mk

mk

3.2 Extensions
In this section we propose two variants on the GapE algorithm with the objective of extending its
applicability and improving its performance.

5

a

max(cid:0)Tpi(t) − 1  1(cid:1) ≥ −∆qj + (1 − d)br a
Tpi(t − 1) ≥ −b∆qj(t − 1) + br a

Tqj(t − 1)

.

.

Tqj(t)

(3)

(4)

−∆pi + (1 + d)br
−b∆pi(t − 1) + br a
1 − cr a

2bc

GapE with variance (GapE-V). The allocation strategy implemented by GapE focuses only on the
arms with small gap and does not take into consideration their variance. However  it is clear that the
arms with small variance  even if their gap is small  just need a few pulls to be correctly estimated. In
order to take into account both the gaps and variances of the arms  we introduce the GapE-variance
mk(t) be the estimated variance

s=1 X 2

1

for arm k of bandit m at the end of round t. GapE-V uses the following B-index for each arm:

(GapE-V) algorithm. Letbσ2

mk(s) −bµ2

mk(t) =

Tmk(t)−1PTmk(t)
Bmk(t) = −b∆mk(t − 1) +s 2abσ2

mk(t − 1)

Tmk(t − 1)

+

Note that the exploration term in the B-index has now two components: the ﬁrst one depends on the
empirical variance and the second one decreases as O(1/Tmk). As a result  arms with low variance
will be explored much less than in the GapE algorithm. Similar to the difference between UCB [3]
and UCB-V [2]  while the B-index in GapE is motivated by Hoeffding’s inequalities  the one for
GapE-V is obtained using an empirical Bernstein’s inequality [11  2]. The following performance
bound can be proved for GapE-V algorithm. We report the proof of Theorem 2 in App. C of [8].

7ab

3(cid:0)Tmk(t − 1) − 1(cid:1) .

in particular for a = 8
9

Theorem 2. If GapE-V is run with parameter 0 < a ≤ 8

9

n−2M K

Hσ

  then it satisﬁes

In Theorem 2  H σ is the complexity of the GapE-V algorithm and is deﬁned as

Hσ

n−2M K

ℓ(n) ≤ P(cid:0)∃m : Jm(n) 6= k∗m(cid:1) ≤ 6nM K exp(cid:18) −
  we have ℓ(n) ≤ 6nM K exp(cid:0) − 1
mk + (16/3)b∆mk(cid:1)2
MXm=1

KXk=1(cid:0)σmk +pσ2

H σ =

64×8

∆2

mk

.

9a

64 × 64(cid:19)
(cid:1).

n−2M K

Hσ

Although the variance-complexity H σ could be larger than the complexity H used in GapE  when-
ever the variances of the arms are small compared to the range b of the distribution  we expect H σ to
be smaller than H. Furthermore  if the arms have very different variances  then GapE-V is expected
to better capture the complexity of each arm and allocate the pulls accordingly. For instance  in the
case where all the gaps are the same  GapE tends to allocate pulls proportionally to the complex-
ity Hmk and it would perform an almost uniform allocation over bandits and arms. On the other
hand  the variances of the arms could be very heterogeneous and GapE-V would adapt the allocation
strategy by pulling more often the arms whose values are more uncertain.
Adaptive GapE and GapE-V. A drawback of GapE and GapE-V is that the exploration parameter
a should be tuned according to the complexities H and H σ of the multi-bandit problem  which are
rarely known in advance. A straightforward solution to this issue is to move to an adaptive version

b2

UCB∆i (t)2  

the adaptive GapE and GapE-V algorithms  we estimate these complexities as

of these algorithms by substituting H and H σ with suitable estimates bH and bH σ. At each step t of
bH(t) =Xm k
UCB∆i (t) = b∆i(t − 1) +s
Ti(t − 1) − 1(cid:19).
Similar to the adaptive version of UCB-E in [1]  bH and bH σ are lower-conﬁdence bounds on the true

bH σ(t) =Xm k(cid:0)LCBσi (t) +pLCBσi (t)2 + (16/3)b × UCB∆i (t)(cid:1)2

complexities H and H σ. Note that the GapE and GapE-V bounds written for the optimal value of
a indicate an inverse relation between the complexity and the exploration. By using a lower-bound
on the true H and H σ  the algorithms tend to explore arms more uniformly and this allows them to
increase the accuracy of their estimated complexities. Although we do not analyze these algorithms 
we empirically show in Sec. 4 that they are in fact able to match the performance of the GapE and
GapE-V algorithms.

LCBσi (t) = max(cid:18)0 bσi(t − 1) −s

UCB∆i (t)2

2Ti(t − 1)

  where

and

1

2

4 Numerical Simulations
In this section  we report numerical simulations of the gap-based algorithms presented in this paper 
GapE and GapE-V  and their adaptive versions A-GapE and A-GapE-V  and compare them with Unif

6

0 Uniform + UCBE

3
0

.

5
2
0

.

0
2

.

0

5
1

.

0

r
o
r
r
e
 
f
o
 
y
t
i
l
i

b
a
b
o
r
p
 
m
u
m
x
a
M

i

GapE

Adapt GapE

GapE

GapE−V

Adapt GapE−V

r
o
r
r
e
 
f
o
 
y
t
i
l
i

b
a
b
o
r
p
 
m
u
m
x
a
M

i

5
2
0

.

0
2

.

0

5
1

.

0

4

8

16 32

4

8
2
Parameter h

16

1/8 1/4 1/2

1

8

16 32 64

4

8
2
Parameter h

16

1/4 1/2

1

2

Figure 2: (left) Problem 1: Comparison between GapE  adaptive GapE  and the uniform strategies.
(right) Problem 2: Comparison between GapE  GapE-V  and adaptive GapE-V algorithms.

Unif + UCBE

Unif + A UCBE

Unif + UCBE−V

Unif + A UCBE−V

GapE

A GapE

GapE−V

A GapE−V

r
o
r
r
e

 
f

o

 
y
t
i
l
i

b
a
b
o
r
p
 
m
u
m
x
a
M

i

5
4
.
0

5
3
0

.

5
2
0

.

5
1
0

.

4

8 16 32

1

2

4

8

2

4

8 16

1/4 1/2 1

2

4
Parameter h

8 16 32

1/4 1/2 1

2

1/2 1

2

4

1/4 1/2 1

2

Figure 3: Performance of the algorithms in Problem 3.

and Unif+UCB-E algorithms introduced in Sec. 3.1. The results of our experiments both those in
the paper and those in App. A of [8] indicate that 1) GapE successfully adapts its allocation strategy
to the complexity of each bandit and outperforms the uniform allocation strategies  2) the use of
the empirical variance in GapE-V can signiﬁcantly improve the performance over GapE  and 3) the
adaptive versions of GapE and GapE-V that estimate the complexities H and H σ online attain the
same performance as the basic algorithms  which receive H and H σ as an input.
Experimental setting. We use the following three problems in our experiments. Note that b = 1
and that a Rademacher distribution with parameters (x  y) takes value x or y with probability 1/2.
• Problem 1. n = 700  M = 2  K = 4. The arms have Bernoulli distribution with parameters:
bandit 1 = (0.5  0.45  0.4  0.3)  bandit 2 = (0.5  0.3  0.2  0.1).
• Problem 2. n = 1000  M = 2  K = 4. The arms have Rademacher distribution with
parameters (x  y): bandit 1 = {(0  1.0)  (0.45  0.45)  (0.25  0.65)  (0  0.9)} and in bandit 2 =
{(0.4  0.6)  (0.45  0.45)  (0.35  0.55)  (0.25  0.65)}.
The arms have Rademacher distri-
• Problem 3.
bution with parameters (x  y): bandit 1 = {(0  1.0)  (0.45  0.45)  (0.25  0.65)  (0  0.9)}  ban-
dit 2 = {(0.4  0.6)  (0.45  0.45)  (0.35  0.55)  (0.25  0.65)}  bandit 3 = {(0  1.0)  (0.45  0.45) 
(0.25  0.65)  (0  0.9)}  and bandit 4 = {(0.4  0.6)  (0.45  0.45)  (0.35  0.55)  (0.25  0.65)}.
All the algorithms  except the uniform allocation  have an exploration parameter a. The theoretical
analysis suggests that a should be proportional to n
H . Although a could be optimized according to the
H  
bound  since the constants in the analysis are not accurate  we will run the algorithms with a = η n
where η is a parameter which is empirically tuned (in the experiments we report four different values
for η). If H correctly deﬁnes the complexity of the exploration problem (i.e.  the number of samples
to ﬁnd the best arms with high probability)  η should simply correct the inaccuracy of the constants
in the analysis  and thus  the range of its nearly-optimal values should be constant across different
problems. In Unif+UCB-E  UCB-E is run with the budget of n/M and the same parameter η for all
the bandits. Finally  we set n ≃ H σ  since we expect H σ to roughly capture the number of pulls
necessary to solve the pure exploration problem with high probability. In Figs. 2 and 3  we report
the performance l(n)  i.e. the probability to identify the best arm in all the bandits after n rounds 
of the gap-based algorithms as well as Unif and Unif+UCB-E strategies. The results are averaged

n = 1400  M = 4  K = 4.

7

over 105 runs and the error bars correspond to three times the estimated standard deviation. In all
the ﬁgures the performance of Unif is reported as a horizontal dashed line.

The left panel of Fig. 2 displays the performance of Unif+UCB-E  GapE  and A-GapE in Problem 1.
As expected  Unif+UCB-E has a better performance (23.9% probability of error) than Unif (29.4%
probability of error)  since it adapts the allocation within each bandit so as to pull more often the
nearly-optimal arms. However  the two bandit problems are not equally difﬁcult.
In fact  their
complexities are very different (H1 ≃ 925 and H2 ≃ 67)  and thus  much less samples are needed
to identify the best arm in the second bandit than in the ﬁrst one. Unlike Unif+UCB-E  GapE
adapts its allocation strategy to the complexities of the bandits (on average only 19% of the pulls are
allocated to the second bandit)  and at the same time to the arm complexities within each bandit (in
the ﬁrst bandit the averaged allocation of GapE is (37%  36%  20%  7%)). As a result  GapE has a
probability of error of 15.7%  which represents a signiﬁcant improvement over Unif+UCB-E.
The right panel of Fig. 2 compares the performance of GapE  GapE-V  and A-GapE-V in Problem 2.
In this problem  all the gaps are equals (∆mk = 0.05)  thus all the arms (and bandits) have the same
complexity Hmk = 400. As a result  GapE tends to implement a nearly uniform allocation  which
results in a small difference between Unif and GapE (28% and 25% accuracy  respectively). The
reason why GapE is still able to improve over Unif may be explained by the difference between static
and dynamic allocation strategies and it is further investigated in App. A of [8]. Unlike the gaps 
the variance of the arms is extremely heterogeneous. In fact  the variance of the arms of bandit 1 is
bigger than in bandit 2  thus making it harder to solve. This difference is captured by the deﬁnition
2 ≃ 600). Note also that H σ ≤ H. As discussed in Sec. 3.2  since
of H σ (H σ
GapE-V takes into account the empirical variance of the arms  it is able to adapt to the complexity
mk of each bandit-arm pair and to focus more on uncertain arms. GapE-V improves the ﬁnal
H σ
accuracy by almost 10% w.r.t. GapE. From both panels of Fig. 2  we also notice that the adaptive
algorithms achieve similar performance to their non-adaptive counterparts. Finally  we notice that
a good choice of parameter η for GapE-V is always close to 2 and 4 (see also [8] for additional
experiments)  while GapE needs η to be tuned more carefully  particularly in Problem 2 where the
large values of η try to compensate the fact that H does not successfully capture the real complexity
of the problem. This further strengthens the intuition that H σ is a more accurate measure of the
complexity for the multi-bandit pure exploration problem.

1 ≃ 1400 > H σ

While Problems 1 and 2 are relatively simple  we report the results of the more complicated Prob-
lem 3 in Fig. 3. The experiment is designed so that the complexity w.r.t. the variance of each bandit
and within each bandit is strongly heterogeneous. In this experiment  we also introduce UCBE-V
that extends UCB-E by taking into account the empirical variance similarly to GapE-V. The re-
sults conﬁrm the previous ﬁndings and show the improvement achieved by introducing empirical
estimates of the variance and allocating non-uniformly over bandits.
5 Conclusion
In this paper  we studied the problem of best arm identiﬁcation in a multi-bandit multi-armed setting.
We introduced a gap-based exploration algorithm  called GapE  and proved an upper-bound for its
probability of error. We extended the basic algorithm to also consider the variance of the arms and
proved an upper-bound for its probability of error. We also introduced adaptive versions of these
algorithms that estimate the complexity of the problem online. The numerical simulations conﬁrmed
the theoretical ﬁndings that GapE and GapE-V outperform other allocation strategies  and that their
adaptive counterparts are able to estimate the complexity without worsening the global performance.

Although GapE does not know the gaps  the experimental results reported in [8] indicate that it
might outperform a static allocation strategy  which knows the gaps in advance  thus suggesting
that an adaptive strategy could perform better than a static one. This observation asks for further
investigation. Moreover  we plan to apply the algorithms introduced in this paper to the problem of
rollout allocation for classiﬁcation-based policy iteration in reinforcement learning [9  6]  where the
goal is to identify the greedy action (arm) in each of the states (bandit) in a training set.
Acknowledgments Experiments presented in this paper were carried out using the Grid’5000 ex-
perimental testbed (https://www.grid5000.fr). This work was supported by Ministry of Higher Edu-
cation and Research  Nord-Pas de Calais Regional Council and FEDER through the “contrat de pro-
jets ´etat region 2007–2013”  French National Research Agency (ANR) under project LAMPADA
n◦ ANR-09-EMER-007  European Community’s Seventh Framework Programme (FP7/2007-2013)
under grant agreement n◦ 231495  and PASCAL2 European Network of Excellence.

8

References

[1] J.-Y. Audibert  S. Bubeck  and R. Munos. Best arm identiﬁcation in multi-armed bandits. In
Proceedings of the Twenty-Third Annual Conference on Learning Theory  pages 41–53  2010.
[2] Jean-Yves Audibert  R´emi Munos  and Csaba Szepesv´ari. Tuning bandit algorithms in stochas-
tic environments. In Marcus Hutter  Rocco Servedio  and Eiji Takimoto  editors  Algorith-
mic Learning Theory  volume 4754 of Lecture Notes in Computer Science  pages 150–165.
Springer Berlin / Heidelberg  2007.

[3] P. Auer  N. Cesa-Bianchi  and P. Fischer. Finite-time analysis of the multi-armed bandit prob-

lem. Machine Learning  47:235–256  2002.

[4] S. Bubeck  R. Munos  and G. Stoltz. Pure exploration in multi-armed bandit problems. In
Proceedings of the Twentieth International Conference on Algorithmic Learning Theory  pages
23–37  2009.

[5] K. Deng  J. Pineau  and S. Murphy. Active learning for personalizing treatment.
Symposium on Adaptive Dynamic Programming and Reinforcement Learning  2011.

In IEEE

[6] C. Dimitrakakis and M. Lagoudakis. Rollout sampling approximate policy iteration. Machine

Learning Journal  72(3):157–171  2008.

[7] Eyal Even-Dar  Shie Mannor  and Yishay Mansour. Action elimination and stopping conditions
for the multi-armed bandit and reinforcement learning problems. Journal of Machine Learning
Research  7:1079–1105  2006.

[8] V. Gabillon  M. Ghavamzadeh  A. Lazaric  and S. Bubeck. Multi-bandit best arm identiﬁcation.

Technical Report 00632523  INRIA  2011.

[9] M. Lagoudakis and R. Parr. Reinforcement learning as classiﬁcation: Leveraging modern
classiﬁers. In Proceedings of the Twentieth International Conference on Machine Learning 
pages 424–431  2003.

[10] O. Maron and A. Moore. Hoeffding races: Accelerating model selection search for classiﬁca-
tion and function approximation. In Proceedings of Advances in Neural Information Process-
ing Systems 6  1993.

[11] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In

22th annual conference on learning theory  2009.

[12] V. Mnih  Cs. Szepesv´ari  and J.-Y. Audibert. Empirical Bernstein stopping. In Proceedings of

the Twenty-Fifth International Conference on Machine Learning  pages 672–679  2008.

[13] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American

Mathematics Society  58:527–535  1952.

9

,Fang Zhao
Yongzhen Huang
Liang Wang
Tieniu Tan
Chaoyue Liu
Mikhail Belkin