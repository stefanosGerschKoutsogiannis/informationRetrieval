2019,Differentially Private Algorithms for Learning Mixtures of Separated Gaussians,Learning the parameters of Gaussian mixture models is a fundamental and widely studied problem with numerous applications. In this work  we give new algorithms for learning the parameters of a high-dimensional  well separated  Gaussian mixture model subject to the strong constraint of differential privacy. In particular  we give a differentially private analogue of the algorithm of Achlioptas and McSherry. Our algorithm has two key properties not achieved by prior work: (1) The algorithm’s sample complexity matches that of the corresponding non-private algorithm up to lower order terms in a wide range of parameters. (2) The algorithm requires very weak a priori bounds on the parameters of the mixture components.,Differentially Private Algorithms for Learning

Mixtures of Separated Gaussians

Gautam Kamath

David R. Cheriton School of Computer Science

University of Waterloo

Waterloo  ON  Canada N2L 3G1

g@csail.mit.edu

Department of Computer Science  Faculty of Exact Sciences

Or Sheffet

Bar-Ilan University

Ramat-Gan  5290002 Israel
or.sheffet@biu.ac.il

Vikrant Singhal

Jonathan Ullman

Khoury College of Computer Sciences

Khoury College of Computer Sciences

Northeastern University

Northeastern University

360 Huntington Ave.  Boston  MA 02115

360 Huntington Ave.  Boston  MA 02115

singhal.vi@husky.neu.edu

jullman@ccs.neu.edu

Abstract

Learning the parameters of Gaussian mixture models is a fundamental and widely
studied problem with numerous applications. In this work  we give new algorithms
for learning the parameters of a high-dimensional  well separated  Gaussian mixture
model subject to the strong constraint of differential privacy. In particular  we
give a differentially private analogue of the algorithm of Achlioptas and McSherry
(COLT 2005). Our algorithm has two key properties not achieved by prior work: (1)
The algorithm’s sample complexity matches that of the corresponding non-private
algorithm up to lower order terms in a wide range of parameters. (2) The algorithm
requires very weak a priori bounds on the parameters of the mixture.

1

Introduction

The Gaussian mixture model is one of the most important and widely studied models in Statistics—
with roots going back over a century [58]—and has numerous applications in the physical  life  and
social sciences. In a Gaussian mixture model  we suppose that each sample is drawn by randomly
selecting from one of k distinct Gaussian distributions G1  . . .   Gk in Rd and then drawing a sample
from that distribution. The problem of learning a Gaussian mixture model asks us to take samples
from this distribution and approximately recover the parameters (mean and covariance) of each of the
underlying Gaussians. The past decades have seen tremendous progress towards understanding both
the sample complexity and computational complexity of learning Gaussian mixtures [20  21  5  64  3 
17  16  52  7  59  41  27  51  46  54  10  42  4  11  40  38  66  23  6  35  36  22  63  26  53  15].

Due to signiﬁcant space restrictions  a full version of the paper  with additional details and all proofs 

appears in the supplementary material [48].

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

In many of the applications of Gaussian mixtures models  especially those in the social sciences  the
sample consists of sensitive data belonging to individuals. In these cases  it is crucial that we not only
learn the parameters of the mixture model  but do so while preserving these individuals’ privacy. In
this work  we study algorithms for learning Gaussian mixtures subject to differential privacy [32] 
which has become the de facto standard for individual privacy in statistical analysis of sensitive
data. Intuitively  differential privacy guarantees that the output of the algorithm does not depend
signiﬁcantly on any one individual’s data  which in this case means any one sample. Differential
privacy is used as a measure of privacy for data analysis systems at Google [34]  Apple [28]  and the
U.S. Census Bureau [19]. Differential privacy and related notions of algorithmic stability are also
crucial for statistical validity even when individual privacy is not a primary concern  as they provide
generalization guarantees in an adaptive setting [31  9].
The ﬁrst differentially private algorithm for learning Gaussian mixtures comes from the work of
Nissim  Raskhodnikova  and Smith [55] as an application of their inﬂuential subsample-and-aggregate
framework. However  their algorithm is a reduction from private estimation to non-private estimation
that blows up the sample complexity by at least a poly(d) factor.

The contribution of this work is new differentially private algorithms for recovering the parameters
of an unknown Gaussian mixture provided that the components are sufﬁciently well separated. In
particular we give differentially private analogues of the algorithm of Achlioptas and McSherry [3] 
which requires that the means are separated by a factor proportional to
k  but independent of the
dimension d. Our algorithms have two main features not shared by previous methods:

√

• The sample complexity of the algorithm matches that of the corresponding non-private

algorithm up to lower order additive terms for a wide range of parameters.

• The algorithm requires very weak a priori bounds on the parameters of the mixture com-
ponents. That is  like many algorithms  we require that the algorithm is seeded with some
information about the range of the parameters  but the algorithm’s sample complexity
depends only mildly (polylogarithmically) on the size of this range.

1.1 Problem Formulation

There are a plethora of algorithms for (non-privately) learning Gaussian mixtures  each with different
learning guarantees under different assumptions on the parameters of the underlying distribution.1 In
this section we describe the version of the problem that our work studies and give some justiﬁcation
for these choices.
We assume that the underlying distribution D is a mixture of k Gaussians in high dimension d.
The mixture is speciﬁed by k components  where each component Gi is selected with probability
wi ∈ [0  1] and is distributed as a Gaussian with mean µi ∈ Rd and covariance Σi ∈ Rd×d. Thus the
mixture is speciﬁed by the tuple {(wi  µi  Σi)}i∈[k].
Our goal is to accurately α-recover this tuple of parameters. Intuitively  with probability 1 − β  we

would like to recover a tuple {((cid:98)wi (cid:98)µi (cid:98)Σi)}i∈[k]  specifying a mixture ˜G such that (cid:107)(cid:98)w − w(cid:107)1 is small
and (cid:107)(cid:98)µi − µi(cid:107)Σi and (cid:107)(cid:98)Σi − Σi(cid:107)Σi are small for every i ∈ [k] (speciﬁcally  we require them all to be
O(α)). Here  (cid:107) · (cid:107)Σ is the appropriate vector/matrix norm that ensures N (µi  Σi) and N ((cid:98)µi (cid:98)Σi) are
close in total variation distance and we also compare (cid:98)w and w in (cid:107) · (cid:107)1 to ensure that the mixtures are
i.i.d. samples from an unknown mixture D and outputs the parameters of a mixture (cid:98)D satisﬁes these

close in total variation distance. Of course  since the labeling of the components is arbitrary  we can
actually only hope to recover the parameters up to some unknown permutation π : [k] → [k] on the
components. We say that an algorithm learns a mixture of Gaussian using n samples if it takes n

conditions  which will imply the resulting mixtures are α-close in total variation distance.2

1We remark that there are also many popular heurstics for learning Gaussian mixtures  notably the EM

algorithm [24]  but in this work we focus on algorithms with provable guarantees.

2To provide context  one might settle for a weaker goal of proper learning where the goal is merely to learn
some Gaussian mixture  possibly with a different number of components  that is close to the true mixture  or
improper learning where it sufﬁces to learn any such distribution.

2

In this work  we consider mixtures that satisfy the separation condition

∀i (cid:54)= j (cid:107)µi − µj(cid:107)2 ≥(cid:101)Ω

(cid:18)(cid:113)

(cid:19)

(cid:110)(cid:107)Σ1/2

j (cid:107)(cid:111)

k + 1/wi + 1/wj

· max

i (cid:107) (cid:107)Σ1/2

(1)

Note that the separation condition does not depend on the dimension d  only on the number of
mixture components. However  (1) is not the weakest possible condition under which one can learn a
mixture of Gaussians. We focus on (1) because in this regime we can learn the mixture components
using statistical properties of the data  such as the large principal components of the data and the
centers of a good clustering  which are amenable to privacy. In contrast  algorithms that learn with
separation proportional to k1/4 [64]  kε [41  51  27]  or even
log k [59] use algorithmic machinery
such as the sum-of-squares algorithm that has not been studied from the perspective of privacy  or
are not computationally efﬁcient. In particular  a barrier to learning with separation k1/4 is that the
non-private algorithms are based on single-linkage clustering  which is not amenable to privacy due
to its crucial dependence on the distance between individual points. We remark that one can also learn
without any separation conditions  but only with exp(k) many samples from the distributions [54].

√

In this work  our goal is to design learning algorithms for mixtures of Gaussians that are also
differentially private. An (ε  δ)-differentially private [32] randomized algorithm A for learning
mixtures of Gaussians is an algorithm that takes a dataset X of samples and:

• A is (ε  δ)-differentially private in the worst-case. That is  for every pair of samples
X  X(cid:48) ∈ X n differing on one sample  A(X) and A(X(cid:48)) are (ε  δ)-close in the following
sense: for b ∈ {0  1}  Pr[A(X) = b] ≤ eε Pr[A(X(cid:48)) = b] + δ
• If n is sufﬁciently large and X1  . . .   Xn ∼ D for a mixture D satisfying our assumptions 
then A(X) outputs an approximation to the parameters of G.

Note that  while the learning guarantees necessarily rely on the assumption that the data is drawn i.i.d.
from some mixture of Gaussians  the privacy guarantee is worst-case. It is important for privacy not
to rely on distributional assumptions because we have no way of verifying that the data was truly
drawn from a mixture of Gaussians  and if our assumption turns out to be wrong we cannot recover
privacy once it is lost.
Furthermore  our algorithms assume certain boundedness of the mixture components. Speciﬁcally 
we assume that there are known quantities R  σmax  σmin  wmin such that

min ≤ (cid:107)Σi(cid:107)2 ≤ σ2

max and wmin ≤ wi.

∀i ∈ [k] (cid:107)µi(cid:107)2 ≤ R and σ2

(2)
For brevity  we deﬁne G((cid:96)  k  R  σmin  σmax  wmin  s) to be the family of mixtures of k Gaussians
in (cid:96) dimensions satisfying 2 and a separation condition deﬁned by s (in our case  s conforms to 1).
These assumptions are to some extent necessary  as even the state-of-the-art algorithms for learning a
single multivariate normal [47] require boundedness.3 However  since R and σmax/σmin can be quite
large—and even if they are not we cannot expect the user of the algorithm to know these parameter a
priori—the algorithm’s sample complexity should depend only mildly on these parameters so that
they can be taken to be quite large.

1.2 Our Contributions

The main contribution of our work is an algorithm with improved sample complexity for learning
mixtures of Gaussians that are separated and bounded.
Theorem 1.1 (Main  Informal). There is an (ε  δ)-differentially private algorithm that takes

(cid:18) d2

n =

α2wmin

+

d2

αwminε

+

poly(k)d3/2

wminε

· polylog

(cid:19)

(cid:18) dkR(σmax/σmin)

(cid:19)

αβεδ

samples from an unknown mixture of k Gaussians D in Rd satisfying (1) and (2)  where wmin =
mini wi  and  with probability at least 1 − β  learns the parameters of D up to error α.

3These boundedness conditions are also provably necessary to learn even a single univariate Gaussian for
pure differential privacy  concentrated differential privacy  or Rényi differential privacy  by the argument of [50].
One could only hope to avoid boundedness using the most general formulation of (ε  δ)-differential privacy.

3

We remark that the sample complexity in Theorem 1.1 compares favorably to the sample complexity
of methods based on subsample-and-aggregate. In particular  when ε ≥ α and k is a small polynomial
in d  the sample complexity is dominated by d2/α2wmin  which is optimal even for non-private
In Section 5 we give an optimized version of the subsample-and-aggregate-based
algorithms.
√
reduction from [55] and show that we can learn mixtures of Gaussians with sample complexity
roughly ˜O(
d/ε) times the sample complexity of the corresponding non-private algorithm. In
contrast the sample complexity of our algorithm does not grow by dimension-dependent factors
compared to the non-private algorithm on which it is based.

At a high level  our algorithm mimics the approach of Achlioptas and McSherry [3]  which is to
use PCA to project the data into a low-dimensional space  which has the effect of projecting out
much of the noise  and then recursively clustering the data points in that low-dimensional space.
However  where their algorithm uses a Kruskal-based clustering algorithm  we have to use alternative
clustering methods that are more amenable to privacy. We develop our algorithm in two distinct
phases addressing different aspects of the problem:

In Section 3 we consider an “easy case” of Theorem 1.1  where we assume that: all components are
spherical Gaussians  such that variances of each component lie in a small  known range (such that
their ratio is bounded by a constant factor) and that the means of the Gaussians lie in a small ball
around the origin. Under these assumptions  it is fairly straight-forward to make the PCA-projection
step [64  3] private. The key piece of the algorithm that needs to be private is computing the principal
components of the data’s covariance matrix. We can make this step private by adding appropriate
noise to this covariance  and the key piece of the analysis is to analyze the effect of this noise
on the principal components  extending the work of Dwork et al. [33] on private PCA. Using the
assumptions we make in this easy case  we can show that the projection shifts each component’s
mean by O(
kσmax)  which preserves the separation of the data because all variances are within
constant factor of one another. Then  we iteratively cluster the data using the 1-cluster technique
of [57  56]. Lastly  we apply a simpliﬁed version of [47] to learn each component.

√

We then consider the general case where the Gaussians can be non-spherical and wildly different
from each other. In this case  if we directly add noise to the covariance matrix to achieve privacy  then
the noise will scale polynomially with σmax/σmin  which is undesirable. For the general case  we
use a recursive algorithm  which repeatedly identiﬁes a secluded cluster in the data  and then recurses
on this isolated cluster and the points outside of the cluster. To that end we develop in Section 4.1 a
variant of the private clustering algorithm of [57  56] that ﬁnds a secluded ball—a set of points that
lie inside of some ball Br (p) such that the annulus B10r (p) \ Br (p) is (essentially) empty.4
We can obtain a recursive algorithm in the following way. First we try to ﬁnd a secluded ball in the
unprojected data. If we ﬁnd one then we can split and recurse on the inside and outside of the ball.
If we cannot ﬁnd a ball  then we can argue that the diameter of the dataset is poly(d  k  σmax). In
√
the latter case  we can ensure that with poly(d  k) samples  the PCA-projection of the data preserves
the mean of each component up to O(
kσmax)  which guarantees that the cluster with the largest
variance is secluded  in which case we can ﬁnd the secluded ball and recurse.

1.3 Related Work

There has also been a great deal of work on learning mixtures of distribution classes  particularly
mixtures of Gaussians. There are many ways the objective can be deﬁned  including clustering [20  21 
5  64  3  17  16  52  7  59  41  27  51]  parameter estimation [46  54  10  42  4  11  40  38  66  23  6] 
proper learning [35  36  22  63  26  53]  and improper learning [15].
There has recently been a great deal of interest in differentially private distribution learning  the
most relevant being [50  47]  which focus on learning a single Gaussian. There are also algorithms
for learning structured univariate distributions in TV-distance [25]  and learning arbitrary univariate
distributions in Kolomogorov distance [13]. Upper and lower bounds for learning the mean of a
product distribution over the hypercube in (cid:96)∞-distance include [12  14  32  61]. [1] focuses on
estimating properties of a distribution  rather than the distribution itself. [60] gives an algorithm
which allows one to estimate asymptotically normal statistics with minimal increase in the sample

4Since [57  56] call the ball found by their algorithm a good ball  we call ours a terriﬁc ball.

4

complexity. There has also been a great deal of work on distribution learning in the local model of
differential privacy [29  65  45  2  30  44  67  37].
Within differential privacy  there are many algorithms for tasks that are related to learning mixtures
of Gaussians  notably PCA [12  49  18  33] and clustering [55  39  57  56  8  62  43]. Applying
these algorithms naïvely to the problem of learning Gaussian mixtures would necessarily introduce
a polynomial dependence on the range of the data  which we seek to avoid. Nonetheless  private
algorithms for PCA and clustering feature in our solution  which builds directly on these works.

2 Robustness of PCA-Projection to Noise

One of the main tools used in learning mixtures of Gaussians under separation is principal component
analysis (PCA). Speciﬁcally  we project onto the subspace spanned by the top k principal components 
which has the effect of preserving the means of the components while eliminating the directions
that are purely noise. In this section  we show that PCA achieves a similar effect even when adding
additional noise for privacy.
Before showing the result for perturbed PCA  we reiterate the very simple analysis of Achlioptas
and McSherry [3]. Let X ∈ Rn×d be a matrix of samples and A ∈ Rn×d be the rank-k matrix of
the corresponding cluster centers. Fixing a cluster i  denoting its empirical mean of as ¯µi  the mean
of the resulting projection as ˆµi  Π as the k-PCA projection matrix and ui ∈ {0  1}n as the vector
indicating which datapoint was sampled from cluster i  we have

(cid:0)X T − (XΠ)T(cid:1) ui(cid:107)2 ≤ (cid:107)X − XΠ(cid:107)2

(cid:107)ui(cid:107)2
ni

≤ 1√

ni

(cid:107)X − A(cid:107)2 

(cid:107)¯µi − ˆµi(cid:107)2 = (cid:107) 1

ni

where the last inequality follows from the XΠ being the best k-rank approximation of X whereas
A is any rank-k matrix. We extend this analysis to a perturbed k-PCA projection as given by the
following lemma.
Lemma 2.1. Let X ∈ Rn×d be a collection of n datapoints from k clusters each centered at
µ1  µ2  ...  µk. Let A ∈ Rn×d be the corresponding matrix of (unknown) centers (for each j we place
the center µc(j) with c(j) denoting the clustering point Xj belongs to). Let ΠVk ∈ Rd×d denote the
k-PCA projection of X’s rows. Let ΠU ∈ Rd×d be a projection such that for some bound B ≥ 0
it holds that (cid:107)X T X − (XΠU )T (XΠU )(cid:107)2 ≤ (cid:107)X T X − (XΠVk )T (XΠVk )(cid:107)2 + B. Denote ¯µi as
the empirical mean of all points in cluster i and denote ˆµi as the projection of the empirical mean
ˆµi = ΠU ¯µi. Then (cid:107) ¯µi − ˆµi(cid:107)2 ≤ 1√

(cid:107)X − A(cid:107)2 +(cid:112)B/ni.

ni

We can instantiate in the following lemma for mixtures of Gaussians.
Lemma 2.2. Let X ∈ Rn×d be a sample from a mixture of Gaussians D  and let A ∈ Rn×d be the
matrix where each row i is the (unknown) mean of the Gaussian from which Xi was sampled. For each
i denote the maximum directional variance of component i  and wi denote its mixing weight.
i  let σ2
Deﬁne σ2 = max
(ξ1d + ξ2 log(2k/β))  where ξ1  ξ2 are

{σ2

{wi}. If n ≥ 1
wmin
√
universal constants  then with probability at least 1 − β 

i } and wmin = min

(cid:115)
≤ (cid:107)X − A(cid:107)2 ≤ 4

i .
wiσ2

n

nwminσ

4

i

i

k(cid:80)

i=1

3 A Warm Up: Strongly Bounded Spherical Gaussian Mixtures

We ﬁrst give an algorithm to learn mixtures of spherical Gaussians  whose variances are within a
constant factor of one another  and means lie in a ball of radius k
dσ  where σ is the largest variance 
and all mixing weights are uniform. We denote such a family of spherical Gaussians by S(d  k  κ  s) 
where κ is an upper bound on the ratio of maxiumum and minimum variances (a constant)  and s
deﬁnes the separation condition. Now  we present the main theorem of this section.
Theorem 3.1. There exists an (ε  δ)-differentially private algorithm  which if given n independent
samples from D ∈ S(d  k  κ  C
κ  where ξ  κ ∈ Θ(1) and ξ is a universal
constant  (cid:96) = max{512 ln(nk/β)  k}  and

√
(cid:96))  such that C = ξ + 16

√

√

3

(cid:96) 5
9 k 5
ε 10

9

dk ln(k/β)

αε

+

k2
α2 +

5

(cid:32)

n ≥

dk
α2 +

d 3
2 k3
ε

+

dk
αε

+

√

(cid:33)

(cid:18)

· polylog

(cid:96)  k 

1
ε

 

1
δ

 

1
β

(cid:19)

then it (α  β)-learns D.
We assume that our algorithm is given samples X ∈ Rn×d  a parameter σmin that is the exact smallest
variance in the mixture and the parameter κ. The algorithm proceeds as follows:
√
(1) We ﬁrst truncate the dataset so that all points lie within a ball of radius 2k
origin. So with high probability  no points in X are lost in this step.
(2) Next  we do privacy-preserving (cid:96)-PCA. By the guarantees in the previous section and from
Theorem 9 of [33]  since all variances are almost identical  the distances between projected means
are changed by at most O(
(3) Now  that the Gaussians have shrunk down  and are still far apart  we can ﬁnd individual
components using an extension of the 1-cluster algorithm of [56] given below.
Theorem 3.2 (Extension of [56]). There is an (ε  δ)-DP algorithm PGLOC(X  t; ε  δ  R  σmin  σmax)
with the following guarantee. Let X = (X1  . . .   Xn) ∈ Rn×d be a set of n points drawn from a
mixture of Gaussians D ∈ G((cid:96)  k  R  σmin  σmax  wmin  s). Let S ⊆ X such that |S| ≥ t  and let
0 < a < 1 be any small absolute constant (say  one can take a = 0.1). If t = γn  where 0 < γ ≤ 1 
and

(cid:96)σmin)  which maintains the separation.

dκσmin around the

√

· polylog

(cid:96) 

1
ε

 

1
δ

 

1
β

 

1
γ

+ O

(cid:19)

(cid:18) (cid:96) + log(k/β)

(cid:19)

wmin

(cid:32)√

(cid:96)
γε

(cid:33) 1
1−a · 9

n ≥

log∗(cid:18)√

(cid:96)

(cid:16) Rσmax

σmin

(cid:17)(cid:96)(cid:19)

(cid:18)

(cid:96)σmin).

4

then for some absolute constant c > 4 that depends on a  with probability at least 1 − β  the
algorithm outputs (r  (cid:126)c) such that the following hold: (1) Br((cid:126)c) contains at least t
2 points in S  that
is  |Br((cid:126)c) ∩ S| ≥ t
√
2 . (2) If ropt is the radius of the smallest ball containing at least t points in S 
then r ≤ c(ropt + 1
For simplicity  we set the constant a = 0.1. Since we have balls that isolate individual components in
the lower-dimensional space and that give us a constant factor approximation of each variance  we
can partition the original dataset to ﬁnd a set of nearly optimal balls containing the points from each
cluster.
(4) We can ﬁnally learn the parameters of each individual Gaussian using a simpliﬁed version of the
Gaussian learner of [47] that is tailored speciﬁcally for spherical Gaussians.
Theorem 3.3. There exists an (ε  δ)-differentially private algorithm PSGE(X; (cid:126)c  r  αµ  ασ  β  ε  δ)
with the following guarantee. If Br((cid:126)c) ⊆ R(cid:96) is a ball  X1  . . .   Xn ∼ N (µ  σ2I(cid:96)×(cid:96))  and n ≥

(cid:33)
then with probability at least 1−β  the algorithm returns(cid:98)µ (cid:98)σ such that if X is contained in Br((cid:126)c) (that
is  Xi ∈ Br((cid:126)c) for every i) and (cid:96) ≥ 8 ln(10/β)  then (cid:107)µ−(cid:98)µ(cid:107)2 ≤ αµσ and (1−ασ) ≤ (cid:98)σ2
σ2 ≤ (1+ασ).

   nσ = O

(cid:32) ln( 1

 (cid:96)

r2 ln( 1
β )
ασεσ2(cid:96)

+ nµ + nσ  where

r ln( 1
β )
αµεσ

ln( 1
β )
α2
µ

ln( 1
β )
ασε

r

(cid:96) ln( 1
δ )

β )
α2
σ(cid:96)

+

α2
µ

6 ln(5/β)

ε

+

+

(cid:113)

nµ = O

+

+

αµεσ

 

4 An Algorithm for Privately Learning Mixtures of Gaussians

In this section  we present our main algorithm for privately learning mixtures of Gaussians and prove
Theorem 1.1 from the introduction.

4.1 Finding a Terriﬁc Ball

In this section we mention a key building block in our algorithm for learning Gaussian mixtures. This
particular subroutine is an adaptation of the work of Nissim and Stemmer [56] (who in turn built
on [57]) that ﬁnds a ball that contains many datapoints. In this section we show how to tweak their
algorithm so that it now produces a ball with a few more additional properties. More speciﬁcally  our
goal in this section is to privately locate a ball Br (p) that (i) contains many datapoints  (ii) leaves out
many datapoints and (iii) is secluded in the sense that B2r (p) \ Br (p) holds very few (and ideally
zero) datapoints. More speciﬁcally  we are using the following deﬁnition.

6

Deﬁnition 4.1. Given a dataset X and an integer t > 0  we say a ball Br (p) is (c  Γ)-terriﬁc for a
constant c > 1 and parameter Γ ≥ 0 if all of the following three properties hold: (i) The number of
datapoints in Br (p) is at least t − Γ; (ii) The number of datapoints outside the ball Bcr (p) is least
t − Γ; and (iii) The number of datapoints in the annulus Bcr (p) \ Br (p) is at most Γ.
We say a ball is c-terriﬁc if it is (c  0)-terriﬁc  and when c is clear from context we call a ball terriﬁc.
In this section  we provide a differentially private algorithm that locates a terriﬁc ball.
The algorithm of [56] is composed of two subroutines. The ﬁrst  GoodRadius privately computes
some radius ˜r such that ˜r ≤ 4ropt with ropt denoting the radius of the smallest ball that contains t
datapoints. Their next subroutine  GoodCenter  takes ˜r as an input and produces a ball of radius γ ˜r
that holds (roughly) t datapoints with γ denoting some constant > 2. The GoodCenter procedure
works by ﬁrst cleverly combining locality-sensitive hashing (LSH) and randomly-chosen axes-aligned
boxed to retrieve a poly-length list of candidate centers  then applying ABOVETHRESHOLD to ﬁnd a
center point p such that the ball B˜r (p) satisﬁes the required (holding enough datapoints).
In our modiﬁcation  the latter procedure is essentially unchanged  with the single condition before
replaced by the three conditions which we require. The previous procedure also requires the same
modiﬁcation  but since our new scoring function (which takes into account all three conditions) is no
longer monotone or quasi-concave  we perform a capping of the scores to facilitate our search. Our
modiﬁcation to GoodRadius is called TerrificRadius  to reﬂect its stronger guarantees.
The resulting combination of these tools gives the following lemma.
Lemma 4.2. The TerrificBall procedure is a (2ε  δ)-DP algorithm which  if run using size-
parameter t ≥ 1000c2
d log(nd/β) log(1/δ) log log(U/L) for some arbitrary small constant
a > 0 (say a = 0.1)  and is set to ﬁnd a c-terriﬁc ball with c > γ (γ being the parameter fed into the
LSH in the GoodCenter procedure)  then the following holds. With probability ≥ 1 − 2β if it returns
a ball Bp (r(cid:48))  and furthermore this ball is a (c  2Γ)-terriﬁc ball of radius r(cid:48) ≤ (1 + c

ε na

√

10 )r.

4.2 The Algorithm

(cid:17)

√

√
wi + 1/

(cid:16)(cid:112)k log(n) + 1/

We ﬁnally introduce our differentially private version of the Achlioptas-McSherry algorithm. Recall
that we assume the separation condition

(cid:112)log(n/β) ≤ 1

∀i  j 

(cid:107)µi − µj(cid:107) ≥ C(σi + σj)

(3)
for some constant C > 0  and that n ≥ poly(d  k) (we note that our constant C is larger than that
of [3]. We also make one additional technical assumption that the Gaussians are not “too skinny.”

wj

∀i  (cid:107)Σi(cid:107)F

and (cid:107)Σi(cid:107)2log(n/β) ≤ 1

√

8 tr(Σi)

d and (cid:107)Σi(cid:107)2 = σ2

i Id×d) we have that tr(Σi) = dσ2

(4)
i   while (cid:107)Σi(cid:107)F =
i   thus the above condition translates to requiring a sufﬁciently large dimension 

8 tr(Σi) 
Note the for a spherical Gaussian (where Σi = σ2
σ2
i
as assumption made explicit in the work regarding learning spherical Gaussians [64].
We now detail the main components of our algorithm. Algorithm 1 takes the dataset X and returns
a k-partition of X into subsets corresponding to different mixture components. There are two key
points to note about this algorithm. First  the parameter k is an upper bound on the number of mixture
components that have points in X  so in every recursive call  even though we specify some upper
bound k(cid:48)  the number of clusters returned will match the exact number of components. Second  the
partition itself cannot be private (since it is a list of points in the dataset). So once this k-partition
is done  one must apply the existing (ε  δ)-DP procedure (called  "PGE"  which is an adaptation of
the private learner for high-dimensional Gaussians from [47] for the case  where a tiny fraction of
the points may be lost  as described in the full version of this paper) for estimating the mean and
the covariance of each cluster  as well as apply the ε-DP histogram to ﬁnd the cluster weights. The
overall algorithm (PGME) is in Algorithm 2.
The main theorem of our section is as follows.
Theorem 4.3. There is an (ε  δ)-differentially private algorithm that takes

(cid:32) √

(cid:33) 1

1−a

7

 d2

α2wmin

n =

+

d2

αwminε

+

k9.06d3/2

wminε

+

dk
wminε

+

k 3

2

αwminε

 · polylog

(cid:32) dkR( σmax

σmin

(cid:33)

)

αβεδ

(where a > 0 is an arbitrarily small constant as in Theorem 3.2) samples from an unknown mixture
of k Gaussians D in Rd satisfying (1) and (2)  where wmin = mini wi  and (α  β)-learns D.

Algorithm 1: Private Gaussian Mixture Partitioning RPGMP(X; k  R  wmin  σmin  σmax  ε  δ)
Input: Dataset X ∈ Rn×d coming from a mixture of at most k Gaussians  such that each xi ∈ X

d). Privacy parameters ε  δ > 0; failure probability β > 0.

√
Output: Partition of X into clusters.

has length ≤ O(R + σmax

1. If k = 1 Skip to last step (#8)
2. Find a small ball that contains X  and bound the range of points to within that ball:

Set n(cid:48) ← |X| + Lap(2/ε) − nwmin
Br(cid:48)(cid:48) (p) ← PGLOC(X  n(cid:48); ε
Set r ← 12r(cid:48)(cid:48).
Set X ← X ∩ Br (p).

20

.

2   δ  R  σmin  σmax).

3. Find 5-TerrificBall in X with t = nwmin

:

2

4. If the data is separable already  we recurse on each part.

Br(cid:48) (p(cid:48)) ← PTERRIFICBALL(X  nwmin
If Br(cid:48) (p(cid:48)) (cid:54)= ⊥ then partition X into A = X ∩ Br(cid:48) (p(cid:48)) and B = X \ B5r(cid:48) (p(cid:48)) and return
RPGMP(A; k − 1  r  wmin  σmin  σmax  ε  δ) ∪ RPGMP(B; k − 1  r  wmin  σmin  σmax  ε  δ)

  c = 5  largest = FALSE; ε  δ  r 

dσmin

).

2

2

5. Find a private k-PCA of X: Π ← k-PCA of X T X + N where N is a symmetric matrix

√

√

whose entries are taken from N (0  4r4 ln(2/δ)
).
6. Find 5-TerrificBall in XΠ with t = nwmin
:

ε2

2

7. If the projected data is separable  we recurse on each part.

Br(cid:48) (p(cid:48)) ← PTERRIFICBALL(XΠ  wmin
If Br(cid:48) (p(cid:48)) (cid:54)= ⊥ then partition X into A = {xi ∈ X : Πxi ∈ Br(cid:48) (p(cid:48))} and
B = {xi ∈ X : Πxi (cid:54)∈ B5r(cid:48) (p(cid:48))} and return
RPGMP(A; k − 1  r  wmin  σmin  σmax  ε  δ) ∪ RPGMP(B; k − 1  r  wmin  σmin  σmax  ε  δ)

  c = 5  largest = TRUE; ε  δ  r 

kσmin

).

2

2

8. Since the data isn’t separable  we treat it as a single Gaussian.

Set a single cluster: C ← {i : xi ∈ X} and return the singleton {C}.

Algorithm 2: Privately Learn Gaussian Mixture PGME(X; k  R  wmin  σmin  σmax  ε  δ  β)
Input: Dataset X ∈ Rn×d coming from a k-Gaussian mixture model. Privacy parameters ε  δ > 0;

Output: Model Parameters Estimation

failure probability β > 0.
1. Truncate the dataset so that for all points  (cid:107)Xi(cid:107)2 ≤ O(R + σmax
2. {C1  ..  Ck} ← RPGMP(X; k  R  wmin  σmin  σmax  ε  δ)
3. For j from 1 to k: let (µj  Σj) ← PGE({xi : i ∈ Cj}; R  σmin  σmax  ε  δ) and

√

d)

4. Set weights such that for all j  wj ← ˜nj/((cid:80)

˜nj ← |Cj| + Lap(1/ε).

j ˜nj).

5. Return (cid:104)µj  Σj  wj(cid:105)k

j=1

5 Sample and Aggregate

In this section  we detail methods based on sample and aggregate  and derive their sample complexity.
This will serve as a baseline for comparison with our methods. In short  the method repeatedly runs a
non-private learning algorithm  and aggregates the results using the 1-cluster algorithms of [57  56].
A similar sample and aggregate method was considered in [55]  but they focused on a restricted case

8

√

(when all mixing weights are equal  and all components are spherical with a known variance)  and did
not explore certain considerations (i.e.  how to minimize the impact of a large domain). We provide a
more in-depth exploration and attempt to optimize the sample complexity.
The main advantage of the sample and aggregate method we describe here is that it is extremely
ﬂexible: given any non-private algorithm for learning mixtures of Gaussians  it can immediately
be converted to a private method. However  there are a few drawbacks  which our main algorithm
avoids. First  by the nature of the approach  it will increase the sample complexity multiplicatively by
d/ε)  thus losing any chance of the non-private sample complexity being the dominating term in
Ω(
any parameter regime. Second  it is not clear on how to adapt this method to non-spherical Gaussians 
since the methods of [57  56] can only ﬁnd (cid:96)2-balls containing many points  rather than the ellipsoids
as required by non-spherical Gaussians. We consider aggregation methods which can handle settings
where the required metric is unknown to be a very interesting direction for further study. Our main
sample-and-aggregate meta-theorem is the following.
Theorem 5.1 (Informal). Let m = ˜Θ(
))). Given a non-private
algorithm which learns a mixture of separated spherical Gaussians with n samples  there exists an
(ε  δ)-differentially private algorithm which learns the same mixture with O(mn) samples.

log2(1/δ) · 2O(log∗( dRσmax

kd+k1.5

ασmin

√

ε

Combining with results from [64]  this implies the following private learning algorithm.
Theorem 5.2 (Informal). There exists a (ε  δ)-differentially private algorithm learns a mixture
of spherical Gaussians with the separation condition that (cid:107)µi − µj(cid:107)2 ≥ (σi + σj) · ˜Ω(k1/4 ·
poly log(k  d  1/ε  log(1/δ)  log

))) . The number of samples it requires is

∗

( Rσmax
ασmin

(cid:32)√

n = ˜O

kd + k1.5

ε

log2(1/δ) · 2O(log∗( dRσmax

ασmin

))

+

d

wminα2 +

k2
α2

(cid:18) d3

w2

min

(cid:19)(cid:33)

.

Acknowledgments

Part of this work was done while the authors were visiting the Simons Institute for Theoretical
Computer Science. Parts of this work were done while GK was supported as a Microsoft Research
Fellow  as part of the Simons-Berkeley Research Fellowship program  while visiting Microsoft
Research  Redmond  and while supported by a University of Waterloo startup grant. This work
was done while OS was afﬁliated with the University of Alberta. OS gratefully acknowledges the
Natural Sciences and Engineering Research Council of Canada (NSERC) for its support through
grant #2017-06701. JU and VS were supported by NSF grants CCF-1718088  CCF-1750640  and
CNS-1816028.

References
[1] Jayadev Acharya  Gautam Kamath  Ziteng Sun  and Huanyu Zhang.

Inspectre: Privately
In Proceedings of the 35th International Conference on Machine

estimating the unseen.
Learning  ICML ’18  pages 30–39. JMLR  Inc.  2018.

[2] Jayadev Acharya  Ziteng Sun  and Huanyu Zhang. Hadamard response: Estimating distributions
privately  efﬁciently  and with little communication. In Proceedings of the 22nd International
Conference on Artiﬁcial Intelligence and Statistics  AISTATS ’19  pages 1120–1129. JMLR 
Inc.  2019.

[3] Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In
Proceedings of the 18th Annual Conference on Learning Theory  COLT ’05  pages 458–469.
Springer  2005.

[4] Joseph Anderson  Mikhail Belkin  Navin Goyal  Luis Rademacher  and James R. Voss. The
more  the merrier: the blessing of dimensionality for learning large Gaussian mixtures. In
Proceedings of the 27th Annual Conference on Learning Theory  COLT ’14  pages 1135–1164 
2014.

[5] Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary Gaussians. In Proceedings
of the 33rd Annual ACM Symposium on the Theory of Computing  STOC ’01  pages 247–257 
New York  NY  USA  2001. ACM.

9

[6] Hassan Ashtiani  Shai Ben-David  Nicholas Harvey  Christopher Liaw  Abbas Mehrabian  and
Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures of Gaussians via
sample compression schemes. In Advances in Neural Information Processing Systems 31 
NeurIPS ’18  pages 3412–3421. Curran Associates  Inc.  2018.

[7] Pranjal Awasthi and Or Sheffet. Improved spectral-norm bounds for clustering. In Approxima-
tion  Randomization  and Combinatorial Optimization. Algorithms and Techniques.  APPROX
’12  pages 37–49. Springer  2012.

[8] Maria-Florina Balcan  Travis Dick  Yingyu Liang  Wenlong Mou  and Hongyang Zhang.
Differentially private clustering in high-dimensional euclidean spaces. In Proceedings of the
34th International Conference on Machine Learning  ICML ’17  pages 322–331. JMLR  Inc. 
2017.

[9] Raef Bassily  Kobbi Nissim  Adam Smith  Thomas Steinke  Uri Stemmer  and Jonathan Ullman.
In Proceedings of the 48th Annual ACM
Algorithmic stability for adaptive data analysis.
Symposium on the Theory of Computing  STOC ’16  pages 1046–1059  New York  NY  USA 
2016. ACM.

[10] Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Proceedings
of the 51st Annual IEEE Symposium on Foundations of Computer Science  FOCS ’10  pages
103–112  Washington  DC  USA  2010. IEEE Computer Society.

[11] Aditya Bhaskara  Moses Charikar  Ankur Moitra  and Aravindan Vijayaraghavan. Smoothed
analysis of tensor decompositions. In Proceedings of the 46th Annual ACM Symposium on the
Theory of Computing  STOC ’14  pages 594–603  New York  NY  USA  2014. ACM.

[12] Avrim Blum  Cynthia Dwork  Frank McSherry  and Kobbi Nissim. Practical privacy: The
SuLQ framework. In Proceedings of the 24th ACM SIGMOD-SIGACT-SIGART Symposium on
Principles of Database Systems  PODS ’05  pages 128–138  New York  NY  USA  2005. ACM.

[13] Mark Bun  Kobbi Nissim  Uri Stemmer  and Salil Vadhan. Differentially private release and
In Proceedings of the 56th Annual IEEE Symposium on
learning of threshold functions.
Foundations of Computer Science  FOCS ’15  pages 634–649  Washington  DC  USA  2015.
IEEE Computer Society.

[14] Mark Bun  Jonathan Ullman  and Salil Vadhan. Fingerprinting codes and the price of approxi-
mate differential privacy. In Proceedings of the 46th Annual ACM Symposium on the Theory of
Computing  STOC ’14  pages 1–10  New York  NY  USA  2014. ACM.

[15] Siu On Chan  Ilias Diakonikolas  Rocco A. Servedio  and Xiaorui Sun. Efﬁcient density
estimation via piecewise polynomial approximation. In Proceedings of the 46th Annual ACM
Symposium on the Theory of Computing  STOC ’14  pages 604–613  New York  NY  USA 
2014. ACM.

[16] Kamalika Chaudhuri and Satish Rao. Beyond Gaussians: Spectral methods for learning mixtures
of heavy-tailed distributions. In Proceedings of the 21st Annual Conference on Learning Theory 
COLT ’08  pages 21–32  2008.

[17] Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using correla-
tions and independence. In Proceedings of the 21st Annual Conference on Learning Theory 
COLT ’08  pages 9–20  2008.

[18] Kamalika Chaudhuri  Anand D. Sarwate  and Kaushik Sinha. A near-optimal algorithm
Journal of Machine Learning Research 

for differentially-private principal components.
14(Sep):2905–2943  2013.

[19] Aref N. Dajani  Amy D. Lauger  Phyllis E. Singer  Daniel Kifer  Jerome P. Reiter  Ashwin
Machanavajjhala  Simson L. Garﬁnkel  Scot A. Dahl  Matthew Graham  Vishesh Karwa  Hang
Kim  Philip Lelerc  Ian M. Schmutte  William N. Sexton  Lars Vilhuber  and John M. Abowd.
The modernization of statistical disclosure limitation at the U.S. census bureau  2017. Presented
at the September 2017 meeting of the Census Scientiﬁc Advisory Committee.

10

[20] Sanjoy Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual IEEE
Symposium on Foundations of Computer Science  FOCS ’99  pages 634–644  Washington  DC 
USA  1999. IEEE Computer Society.

[21] Sanjoy Dasgupta and Leonard J. Schulman. A two-round variant of EM for Gaussian mixtures.
In Proceedings of the 16th Conference in Uncertainty in Artiﬁcial Intelligence  UAI ’00  pages
152–159. Morgan Kaufmann  2000.

[22] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms
for proper learning mixtures of Gaussians. In Proceedings of the 27th Annual Conference on
Learning Theory  COLT ’14  pages 1183–1213  2014.

[23] Constantinos Daskalakis  Christos Tzamos  and Manolis Zampetakis. Ten steps of EM sufﬁce
for mixtures of two Gaussians. In Proceedings of the 30th Annual Conference on Learning
Theory  COLT ’17  pages 704–710  2017.

[24] A. P. Dempster  N. M. Laird  and D. B. Rubin. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological) 
39(1):1–38  1977.

[25] Ilias Diakonikolas  Moritz Hardt  and Ludwig Schmidt. Differentially private learning of
structured discrete distributions. In Advances in Neural Information Processing Systems 28 
NIPS ’15  pages 2566–2574. Curran Associates  Inc.  2015.

[26] Ilias Diakonikolas  Gautam Kamath  Daniel M. Kane  Jerry Li  Ankur Moitra  and Alistair
Stewart. Robust estimators in high dimensions without the computational intractability. In
Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science  FOCS
’16  pages 655–664  Washington  DC  USA  2016. IEEE Computer Society.

[27] Ilias Diakonikolas  Daniel M. Kane  and Alistair Stewart. List-decodable robust mean estima-
tion and learning mixtures of spherical Gaussians. In Proceedings of the 50th Annual ACM
Symposium on the Theory of Computing  STOC ’18  pages 1047–1060  New York  NY  USA 
2018. ACM.

[28] Differential

Privacy

Team 

at

scale.
https://machinelearning.apple.com/docs /learning-with-privacy-at-scale/
appledifferentialprivacysystem.pdf  December 2017.

Learning with

privacy

Apple.

[29] John C. Duchi  Michael I. Jordan  and Martin J. Wainwright. Local privacy and statistical
minimax rates. In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer
Science  FOCS ’13  pages 429–438  Washington  DC  USA  2013. IEEE Computer Society.

[30] John C. Duchi and Feng Ruan. The right complexity measure in locally private estimation: It is

not the ﬁsher information. arXiv preprint arXiv:1806.05756  2018.

[31] Cynthia Dwork  Vitaly Feldman  Moritz Hardt  Toniann Pitassi  Omer Reingold  and Aaron Roth.
The reusable holdout: Preserving validity in adaptive data analysis. Science  349(6248):636–638 
2015.

[32] Cynthia Dwork  Frank McSherry  Kobbi Nissim  and Adam Smith. Calibrating noise to sensi-
tivity in private data analysis. In Proceedings of the 3rd Conference on Theory of Cryptography 
TCC ’06  pages 265–284  Berlin  Heidelberg  2006. Springer.

[33] Cynthia Dwork  Kunal Talwar  Abhradeep Thakurta  and Li Zhang. Analyze Gauss: Optimal
bounds for privacy-preserving principal component analysis. In Proceedings of the 46th Annual
ACM Symposium on the Theory of Computing  STOC ’14  pages 11–20  New York  NY  USA 
2014. ACM.

[34] Úlfar Erlingsson  Vasyl Pihur  and Aleksandra Korolova. RAPPOR: Randomized aggregatable
privacy-preserving ordinal response. In Proceedings of the 2014 ACM Conference on Computer
and Communications Security  CCS ’14  pages 1054–1067  New York  NY  USA  2014. ACM.

11

[35] Jon Feldman  Ryan O’Donnell  and Rocco A. Servedio. PAC learning axis-aligned mixtures of
Gaussians with no separation assumption. In Proceedings of the 19th Annual Conference on
Learning Theory  COLT ’06  pages 20–34  Berlin  Heidelberg  2006. Springer.

[36] Jon Feldman  Ryan O’Donnell  and Rocco A. Servedio. Learning mixtures of product distribu-

tions over discrete domains. SIAM Journal on Computing  37(5):1536–1564  2008.

[37] Marco Gaboardi  Ryan Rogers  and Or Sheffet. Locally private conﬁdence intervals: Z-test and
tight conﬁdence intervals. In Proceedings of the 22nd International Conference on Artiﬁcial
Intelligence and Statistics  AISTATS ’19  pages 2545–2554. JMLR  Inc.  2019.

[38] Rong Ge  Qingqing Huang  and Sham M. Kakade. Learning mixtures of Gaussians in high
dimensions. In Proceedings of the 47th Annual ACM Symposium on the Theory of Computing 
STOC ’15  pages 761–770  New York  NY  USA  2015. ACM.

[39] Anupam Gupta  Katrina Ligett  Frank McSherry  Aaron Roth  and Kunal Talwar. Differentially
private combinatorial optimization. In Proceedings of the 21st Annual ACM-SIAM Symposium
on Discrete Algorithms  SODA ’10  pages 1106–1125  Philadelphia  PA  USA  2010. SIAM.

[40] Moritz Hardt and Eric Price. Sharp bounds for learning a mixture of two Gaussians.

In
Proceedings of the 47th Annual ACM Symposium on the Theory of Computing  STOC ’15  pages
753–760  New York  NY  USA  2015. ACM.

[41] Samuel B. Hopkins and Jerry Li. Mixture models  robustness  and sum of squares proofs. In
Proceedings of the 50th Annual ACM Symposium on the Theory of Computing  STOC ’18  pages
1021–1034  New York  NY  USA  2018. ACM.

[42] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical Gaussians: Moment methods
and spectral decompositions. In Proceedings of the 4th Conference on Innovations in Theoretical
Computer Science  ITCS ’13  pages 11–20  New York  NY  USA  2013. ACM.

[43] Zhiyi Huang and Jinyan Liu. Optimal differentially private algorithms for k-means clustering. In
Proceedings of the 37th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database
Systems  PODS ’18  pages 395–408  New York  NY  USA  2018. ACM.

[44] Matthew Joseph  Janardhan Kulkarni  Jieming Mao  and Zhiwei Steven Wu. Locally private

gaussian estimation. arXiv preprint arXiv:1811.08382  2018.

[45] Peter Kairouz  Keith Bonawitz  and Daniel Ramage. Discrete distribution estimation under
local privacy. In Proceedings of the 33rd International Conference on Machine Learning  ICML
’16  pages 2436–2444. JMLR  Inc.  2016.

[46] Adam Tauman Kalai  Ankur Moitra  and Gregory Valiant. Efﬁciently learning mixtures of two
Gaussians. In Proceedings of the 42nd Annual ACM Symposium on the Theory of Computing 
STOC ’10  pages 553–562  New York  NY  USA  2010. ACM.

[47] Gautam Kamath  Jerry Li  Vikrant Singhal  and Jonathan Ullman. Privately learning high-
dimensional distributions. In Proceedings of the 32nd Annual Conference on Learning Theory 
COLT ’19  2019.

[48] Gautam Kamath  Or Sheffet  Vikrant Singhal  and Jonathan Ullman. Differentially private

algorithms for learning mixtures of separated gaussians  2019.

[49] Michael Kapralov and Kunal Talwar. On differentially private low rank approximation. In
Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms  SODA ’13 
pages 1395–1414  Philadelphia  PA  USA  2013. SIAM.

[50] Vishesh Karwa and Salil Vadhan. Finite sample differentially private conﬁdence intervals. In
Proceedings of the 9th Conference on Innovations in Theoretical Computer Science  ITCS ’18 
pages 44:1–44:9  Dagstuhl  Germany  2018. Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik.

[51] Pravesh Kothari  Jacob Steinhardt  and David Steurer. Robust moment estimation and improved
clustering via sum of squares. In Proceedings of the 50th Annual ACM Symposium on the
Theory of Computing  STOC ’18  pages 1035–1046  New York  NY  USA  2018. ACM.

12

[52] Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm.
In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science 
FOCS ’10  pages 299–308  Washington  DC  USA  2010. IEEE Computer Society.

[53] Jerry Li and Ludwig Schmidt. Robust proper learning for mixtures of Gaussians via systems of
polynomial inequalities. In Proceedings of the 30th Annual Conference on Learning Theory 
COLT ’17  pages 1302–1382  2017.

[54] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians.
In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science  FOCS
’10  pages 93–102  Washington  DC  USA  2010. IEEE Computer Society.

[55] Kobbi Nissim  Sofya Raskhodnikova  and Adam Smith. Smooth sensitivity and sampling in
private data analysis. In Proceedings of the 39th Annual ACM Symposium on the Theory of
Computing  STOC ’07  pages 75–84  New York  NY  USA  2007. ACM.

[56] Kobbi Nissim and Uri Stemmer. Clustering algorithms for the centralized and local models. In

Algorithmic Learning Theory  ALT ’18  pages 619–653. JMLR  Inc.  2018.

[57] Kobbi Nissim  Uri Stemmer  and Salil Vadhan. Locating a small cluster privately. In Proceedings
of the 35th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems 
PODS ’16  pages 413–427  New York  NY  USA  2016. ACM.

[58] Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions

of the Royal Society of London. A  pages 71–110  1894.

[59] Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians.
In Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science 
FOCS ’17  pages 85–96  Washington  DC  USA  2017. IEEE Computer Society.

[60] Adam Smith. Privacy-preserving statistical estimation with optimal convergence rates. In
Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing  STOC ’11 
pages 813–822  New York  NY  USA  2011. ACM.

[61] Thomas Steinke and Jonathan Ullman. Between pure and approximate differential privacy. The

Journal of Privacy and Conﬁdentiality  7(2):3–22  2017.

[62] Uri Stemmer and Haim Kaplan. Differentially private k-means with constant multiplicative error.
In Advances in Neural Information Processing Systems 31  NeurIPS ’18  pages 5431–5441.
Curran Associates  Inc.  2018.

[63] Ananda Theertha Suresh  Alon Orlitsky  Jayadev Acharya  and Ashkan Jafarpour. Near-
optimal-sample estimators for spherical Gaussian mixtures. In Advances in Neural Information
Processing Systems 27  NIPS ’14  pages 1395–1403. Curran Associates  Inc.  2014.

[64] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixtures of distributions.
In Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science 
FOCS ’02  pages 113–123  Washington  DC  USA  2002. IEEE Computer Society.

[65] Shaowei Wang  Liusheng Huang  Pengzhan Wang  Yiwen Nie  Hongli Xu  Wei Yang  Xiang-
Yang Li  and Chunming Qiao. Mutual information optimally local private discrete distribution
estimation. arXiv preprint arXiv:1607.08025  2016.

[66] Ji Xu  Daniel J. Hsu  and Arian Maleki. Global analysis of expectation maximization for
mixtures of two Gaussians. In Advances in Neural Information Processing Systems 29  NIPS
’16  pages 2676–2684. Curran Associates  Inc.  2016.

[67] Min Ye and Alexander Barg. Optimal schemes for discrete distribution estimation under locally

differential privacy. IEEE Transactions on Information Theory  64(8):5662–5676  2018.

13

,Gautam Kamath
Or Sheffet
Vikrant Singhal
Jonathan Ullman