2015,Weighted Theta Functions and Embeddings with Applications to Max-Cut  Clustering and Summarization,We introduce a unifying generalization of the Lovász theta function  and the associated geometric embedding  for graphs with weights on both nodes and edges. We show how it can be computed exactly by semidefinite programming  and how to approximate it using SVM computations. We show how the theta function can be interpreted as a measure of diversity in graphs and use this idea  and the graph embedding in algorithms for Max-Cut  correlation clustering and document summarization  all of which are well represented as problems on weighted graphs.,Weighted Theta Functions and Embeddings with

Applications to Max-Cut  Clustering and

Summarization

Fredrik D. Johansson

Computer Science & Engineering
Chalmers University of Technology

G¨oteborg  SE-412 96  Sweden
frejohk@chalmers.se

Chiranjib Bhattacharyya

Computer Science and Automation

Indian Institute of Science

Bangalore 560012  Karnataka  India
chiru@csa.iisc.ernet.in

Ankani Chattoraj∗

Brain & Cognitive Sciences

University of Rochester

Rochester  NY 14627-0268  USA

achattor@ur.rochester.edu

Devdatt Dubhashi

Computer Science & Engineering
Chalmers University of Technology

G¨oteborg  SE-412 96  Sweden
dubhashi@chalmers.se

Abstract

We introduce a unifying generalization of the Lov´asz theta function  and the as-
sociated geometric embedding  for graphs with weights on both nodes and edges.
We show how it can be computed exactly by semideﬁnite programming  and how
to approximate it using SVM computations. We show how the theta function can
be interpreted as a measure of diversity in graphs and use this idea  and the graph
embedding in algorithms for Max-Cut  correlation clustering and document sum-
marization  all of which are well represented as problems on weighted graphs.

1

Introduction

Embedding structured data  such as graphs  in geometric spaces  is a central problem in machine
learning. In many applications  graphs are attributed with weights on the nodes and edges – infor-
mation that needs to be well represented by the embedding. Lov´asz introduced a graph embedding
together with the famous theta function in the seminal paper [19]  giving his celebrated solution to
the problem of computing the Shannon capacity of the pentagon. Indeed  Lov´asz’s embedding is a
very elegant and powerful representation of unweighted graphs  that has come to play a central role
in information theory  graph theory and combinatorial optimization [10  8]. However  despite there
being at least eight different formulations of ϑ(G) for unweighted graphs  see for example [20]  there
does not appear to be a version that applies to graphs with weights on the edges. This is surprising 
as it has a natural interpretation in the information theoretic problem of the original deﬁnition [19].
A version of the Lov´asz number for edge-weighted graphs  and a corresponding geometrical rep-
resentation  could open the way to new approaches to learning problems on data represented as
similarity matrices. Here we propose such a generalization for graphs with weights on both nodes
and edges  by combining a few key observations. Recently  Jethava et al. [14] discovered an inter-
esting connection between the original theta function and a central problem in machine learning 
namely the one class Support Vector Machine (SVM) formulation [14]. This kernel based method
gives yet another equivalent characterization of the Lov´asz number. Crucially  it is easily modiﬁed
to yield an equivalent characterization of the closely related Delsarte version of the Lov´asz number
∗This work was performed when the author was afﬁliated with CSE at Chalmers University of Technology.

1

introduced by Schrijver [24] which is more ﬂexible and often more convenient to work with. Using
this kernel characterization of the Delsarte version of Lov´asz number  we deﬁne a theta function and
embedding of weighted graphs  suitable for learning with data represented as similarity matrices.
The original theta function is limited to applications on small graphs  because of its formulation as a
semideﬁnite program (SDP). In [14]  Jethava et al. showed that their kernel characterization can be
used to compute a number and an embedding of a graph that are often good approximations to the
theta function and embedding  and that can be computed fast  scaling to very large graphs. Here we
give the analogous approximate method for weighted graphs. We use this approximation to solve
the weighted maximum cut problem faster than the classical SDP relaxation.
Finally  we show that our edge-weighted theta function has a natural interpretation as a measure of
diversity in graphs. We use this intuition to deﬁne a centroid-based correlation clustering algorithm
that automatically chooses the number of clusters and initializes the centroids. We also show how to
use the support vectors  computed in the kernel characterization with both node and edge weights 
to perform extractive document summarization.
To summarize our main contributions:

with weights on both nodes and edges.

weighted theta function and the corresponding embeddings using SVM computations.

• We introduce a unifying generalization of the famous Lov´asz number applicable to graphs
• We show that via our characterization  we can compute a good approximation to our
• We show that the weighted version of the Lov´asz number can be interpreted as a measure
of diversity in graphs  and we use this to deﬁne a correlation clustering algorithm dubbed
ϑ-means that automatically a) chooses the number of clusters  and b) initializes centroids.
• We apply the embeddings corresponding to the weighted Lov´asz numbers to solve weighted
• We apply the weighted kernel characterization of the theta function to document summa-

maximum cut problems faster than the classical SDP methods  with similar accuracy.

rization  exploiting both node and edge weights.

2 Extensions of Lov´asz and Delsarte numbers for weighted graphs

Background Consider embeddings of undirected graphs G = (V  E). Lov´asz introduced an el-
egant embedding  implicit in the deﬁnition of his celebrated theta function ϑ(G) [19]  famously an
upper bound on the Shannon capacity and sandwiched between the independence number and the
chromatic number of the complement graph.

i

1

max

ϑ(G) = min{ui} c

(c(cid:62)ui)2   u(cid:62)

i uj = 0  ∀(i  j) (cid:54)∈ E  (cid:107)ui(cid:107) = (cid:107)c(cid:107) = 1 .

(1)
The vectors {ui}  c are so-called orthonormal representations or labellings  the dimension of which
is determined by the optimization. We refer to both {ui}  and the matrix U = [u1  . . .   un] as an
embedding G  and use the two notations interchangeably. Jethava et al. [14] introduced a characteri-
zation of the Lov´asz ϑ function that established a close connection with the one-class support vector
machine [23]. They showed that  for an unweighted graph G = (V  E) 

min

K∈K(G)

ω(K)  where

ϑ(G) =
K(G)
ω(K)

:= {K (cid:23) 0 | Kii = 1  Kij = 0 ∀(i  j) (cid:54)∈ E} 
:= max
α≥0

f (α; K) := 2

f (α; K) 

(cid:88)

αi −(cid:88)

i

i j

(2)

(3)
(4)

Kijαiαj

is the dual formulation of the one-class SVM problem  see [16]. Note that the conditions on K only
refer to the non-edges of G. In the sequel  ω(K) and f (α; K) always refer to the deﬁnitions in (4).

2.1 New weighted versions of ϑ(G)

A key observation in proving (2)  is that the set of valid orthonormal representations is equivalent
to the set of kernels K. This equivalence can be preserved in a natural way when generalizing the

2

deﬁnition to weighted graphs: any constraint on the inner product uT
constraints on the elements Kij of the kernel matrix.
To deﬁne weighted extensions of the theta function  we need to ﬁrst pass to the closely related
Delsarte version of the Lov´asz number introduced by Schrijver [24]. In the Delsarte version  the
i uj ≤ 0  (i  j) (cid:54)∈ E. With reference to the
orthogonality constraint for non-edges is relaxed to uT
formulation (2) it is easy to observe that the Delsarte version is given by

i uj may be represented as

ϑ1(G) = min

K∈K1(G)

ω(K)  where K1(G) := {K (cid:23) 0 | Kii = 1  Kij ≤ 0 ∀(i  j) (cid:54)∈ E}

(5)

In other words  the Lov´asz number corresponds to orthogonal labellings of G with orthogonal vec-
tors on the unit sphere assigned to non–adjacent nodes whereas the Delsarte version corresponds to
obtuse labellings  i.e. the vectors corresponding to non–adjacent nodes are vectors on the unit sphere
meeting at obtuse angles. In both cases  the corresponding number is essentially the half-angle of
the smallest spherical cap containing all the vectors assigned to the nodes. Comparing (2) and (5) it
follows that ϑ1(G) ≤ ϑ(G). In the sequel  we will use the Delsarte version and obtuse labellings to
deﬁne weighted generalizations of the theta function.
We observe in passing  that for any K ∈ K1  and for any independent set I in the graph  taking
αi = 1 if i ∈ I and 0 otherwise 

(cid:88)

αi −(cid:88)

αiαjKij ≥ (cid:88)

αi = |I|

(6)

(cid:88)

αi −(cid:88)

ω(K) ≥ 2

αiαjKij =

i

i j

i

i

i(cid:54)=j

since for each term in the second sum  either (i  j) is an edge  in which case either αi or αj is zero 
or (i  j) is a non–edge in which case Kij ≤ 0. Thus  like ϑ(G)  the Delsarte version ϑ1(G) is also
an upper bound on the stability or independence number α(G).

Kernel characterization of theta functions on node-weighted graphs Lov´asz number has a
classical extension to graphs with node weights σ = [σ1  . . .   σn](cid:62)  see for example [17]. The
generalization  in the Delsarte version (note the inequality constraint)  is the following

ϑ(G  σ) = min{ui} c

max

i

σi

(c(cid:62)ui)2   u(cid:62)

i uj ≤ 0  ∀(i  j) (cid:54)∈ E  (cid:107)ui(cid:107) = (cid:107)c(cid:107) = 1 .

(7)

By passing to the dual of (7)  see section 2.1 and [16]  we may  as for unweighted graphs  character-
ize ϑ(G  σ) by a minimization over the set of kernels 

K(G  σ) := {K (cid:23) 0 | Kii = 1/σi  Kij ≤ 0 ∀(i  j) (cid:54)∈ E}

(8)
and  just like in the unweighted case  ϑ1(G  σ) = minK∈K(G σ) ω(K). When σi = 1 ∀i ∈ V   this
reduces to the unweighted case. We also note that for any K ∈ K(G  σ) and for any independent
set I in the graph  taking αi = σi if i ∈ I and 0 otherwise 

(cid:88)

αi −(cid:88)

i

i j

ω(K) ≥ 2

αiαjKij = 2

(cid:88)

i∈I

σi −(cid:88)

i∈I

σ2
i
σi

−(cid:88)

αiαjKij ≥(cid:88)

i(cid:54)=j

i∈I

σi  

(9)

since Kij ≤ 0 ∀(i  j) (cid:54)∈ E. Thus  ϑ1(G  σ) ≥ ω(K) is an upper bound on the weight of the
maximum-weight independent set.

Extension to edge-weighted graphs The kernel characterization of ϑ1(G) allows one to deﬁne a
natural extension to data given as similarity matrices represented in the form of a weighted graph
G = (V  S). Here  S is a similarity function on (unordered) node pairs  and S(i  j) ∈ [0  1] with +1
representing complete similarity and 0 complete dissimilarity. The obtuse labellings corresponding
to the Delsarte version are somewhat more ﬂexible even for unweighted graphs  but is particularly
well suited for weighted graphs. We deﬁne

ϑ1(G  S) := min

K∈K(G S)

ω(K) where K(G  S) := {K (cid:23) 0 | Kii = 1  Kij ≤ Sij}

(10)

In the case of an unweighted graph  where Sij ∈ {0  1}  this reduces exactly to (5).

3

Table 1: Characterizations of weighted theta functions. In the ﬁrst row are characterizations follow-
ing the original deﬁnition. In the second are kernel characterizations. The bottom row are versions
of the LS-labelling [14]. In all cases  (cid:107)ui(cid:107) = (cid:107)c(cid:107) = 1. A refers to the adjacency matrix of G.

Unweighted

1

i

c

u

max

min{ui} min
(c(cid:62)ui)2
i uj ≤ 0  ∀(i  j) (cid:54)∈ E
(cid:62)
KG = {K (cid:23) 0 | Kii = 1 
Kij = 0 ∀(i  j) (cid:54)∈ E}

Node-weighted
σi

c

max

min{ui} min
(c(cid:62)ui)2
i uj = 0  ∀(i  j) (cid:54)∈ E
(cid:62)
u

i

Edge-weighted

1

max

min{ui} min
(c(cid:62)ui)2
c
i uj ≤ Sij  i (cid:54)= j
(cid:62)
u

i

KG σ = {K (cid:23) 0 | Kii = 1/σi 
Kij = 0 ∀(i  j) (cid:54)∈ E}

KG S = {K (cid:23) 0 | Kii = 1 
Kij ≤ Sij  i (cid:54)= j}

KLS =

A

|λn(A)| + I

K σ

LS =

A

σmax|λn(A)| +diag(σ)

−1

K S

LS =

S

|λn(S)| + I

Unifying weighted generalization We may now combine both node and edge weights to form a
fully general extension to the Delsarte version of the Lov´asz number 

(cid:26)

(cid:27)

(11)

ϑ1(G  σ  S) =

min

K∈K(G σ S)

ω(K)  K(G  σ  S) :=

K (cid:23) 0 | Kii =

1
σi

  Kij ≤ Sij√
σiσj

It is easy to see that for unweighted graphs  Sij ∈ {0  1}  σi = 1  the deﬁnition reduces to the
Delsarte version of the theta function in (5). ϑ1(G  σ  S) is hence a strict generalization of ϑ1(G).
All the proposed weighted extensions are deﬁned by the same objective  ω(K). The only difference
is the set K  specialized in various ways  over which the minimum  minK∈K ω(K)  is computed.
It also is important to note  that with the generalization of the theta function comes an implicit
generalization of the geometric representation of G. Speciﬁcally  for any feasible K in (11)  there
σiσj ≤ Sij 
is an embedding U = [u1  . . .   un] such that K = U(cid:62)U with the properties u(cid:62)
(cid:107)ui(cid:107)2 = 1/
σiσj is
i uj
exactly the cosine similarity between ui and uj  which is a very natural choice when Sij ∈ [0  1].
The original deﬁnition of the (Delsarte) theta function and its extensions  as well as their kernel
characterizations  can be seen in table 1. We can prove the equivalence of the embedding (top) and
kernel characterizations (middle) using the following result.
Proposition 2.1. For any embedding U ∈ Rd×n with K = U(cid:62)U  and f in (4)  the following holds

√
σi  which can be retrieved using matrix decomposition. Note that u(cid:62)

i uj

√

√

min
c∈S d−1

max

i

1

(c(cid:62)ui)2 = max
αi≥0

f (α; K) .

(12)

Proof. The result is given as part of the proof of Theorem 3 in Jethava et al. [14]. See also [16].

As we have already established in section 2 that any set of geometric embeddings have a characteri-
zation as a set of kernel matrices  it follows that the minimizing the LHS in (12) over a (constrained)
set of orthogonal representations  {ui}  is equivalent to minimizing the RHS over a kernel set K.

3 Computation and ﬁxed-kernel approximation

The weighted generalization of the theta function  ϑ1(G  σ  S)  deﬁned in the previous section  may
be computed as a semideﬁnite program. In fact ϑ1(G  σ  S) = 1/(t∗)2 for t∗  the solution to the
following problem. For details  see [16]. With Sk

+ the set of k × k symmetric p.s.d. matrices 

maximize

X

t

subject to X ∈ Sn+1
Xi n+1 ≥ t  Xii = 1/σi 
√
Xij ≤ Sij/

σiσj 

+

i ∈ [n]
i (cid:54)= j  i  j ∈ [n] .

(13)

4

While polynomial in time complexity [13]  solving the SDP is too slow in many cases. To address
this  Jethava et al. [14] introduced a fast approximation to (the unweighted) ϑ(G)  dubbed SVM-
theta. They showed that in some cases  the minimization over K in (2) can be replaced by a ﬁxed
choice of K  while causing just a constant-factor error. Speciﬁcally  for unweighted graphs with
adjacency matrix A  Jethava et al. [14] deﬁned the so called LS-labelling  KLS(G) = A/|λn(A)| +
I  and showed that for large families of graphs ϑ(G) ≤ ω(KLS(G)) ≤ γϑ(G)  for a constant γ.
We extend the LS-labelling to weighted graphs. For graphs with edge weights  represented by
a similarity matrix S  the original deﬁnition may be used  with S substituted for A. For node
weighted graphs we also must satisfy the constraint Kii = 1/σi  see (8). A natural choice  still
ensuring positive semideﬁniteness is 

KLS(G  σ) =

A

σmax|λn(A)| + diag(σ)−1

(14)

where diag(σ)−1 is the diagonal matrix Σ with elements Σii = 1/σi  and σmax = maxn
i=1 σi. Both
weighted versions of the LS-labelling are presented in table 1. The fully generalized labelling  for
graphs with weights on both nodes and edges  KLS(G  σ  S) can be obtained by substituting S for
A in (14). As with the exact characterization  we note that KLS(G  σ  S) reduces to KLS(G) for
the uniform case  Sij ∈ {0  1}  σi = 1. For all versions of the LS-labelling of G  as with the exact
characterization  a geometric embedding U may be obtained from KLS using matrix decompotion.

3.1 Computational complexity

Solving the full problem in the kernel characterization (11)  is not faster than the computing the
SDP characterization (13). However  for a ﬁxed K  the one-class SVM can be solved in O(n2)
time [12]. Retrieving the embedding U : K = U T U may be done using Cholesky or singular value
decomposition (SVD). In general  algorithms for these problems have complexity O(n3). However 
in many cases  a rank d approximation to the decomposition is sufﬁcient  see for example [9]. A thin
(or truncated) SVD corresponding to the top d singular values may be computed in O(n2d) time [5]
for d = O(
n). The remaining issue is the computation of K. The complexity of computing the
LS-labelling discussed in the previous section is dominated by the computation of the minimum
eigenvalue λn(A). This can be done approximately in ˜O(m) time  where m is the number of edges
of the graph [1]. Overall  the complexity of computing both the embedding U and ω(K) is O(dn2).

√

4 The theta function as diversity in graphs: ϑ-means clustering

In section 2  we deﬁned extensions of the Delsarte version of the Lov´asz number  ϑ1(G) and the
associated geometric embedding  for weighted graphs. Now we wish to show how both ϑ(G) and
the geometric embedding are useful for solving common machine learning tasks. We build on an
intuition of ϑ(G) as a measure of diversity in graphs  illustrated here by a few simple examples. For
complete graphs Kn  it is well known that ϑ(Kn) = 1  and for empty graphs K n  ϑ(K n) = n.
We may interpret these graphs as having 1 and n clusters respectively. Graphs with several disjoint
clusters make a natural middle-ground. For a graph G that is a union of k disjoint cliques  ϑ(G) = k.
Now  consider the analogue of (6) for graphs with edge weights Sij. For any K ∈ K(G  S) and for
any subset H of nodes  let αi = 1 if i ∈ H and 0 otherwise. Then  since Kij ≤ Sij 

(cid:88)

αi −(cid:88)

αiαjKij ≥ |H| − (cid:88)

(cid:88)

αi −(cid:88)

2

αiαjKij =

i

ij

i

i(cid:54)=j

Sij .

i(cid:54)=j i j∈H

Maximizing this expression may be viewed as the trade-off of ﬁnding a subset of nodes that is both
large and diverse; the objective function is the size of the set subjected to a penalty for non–diversity.
In general support vector machines  non-zero support values αi correspond to support vectors  deﬁn-
ing the decision boundary. As a result  nodes i ∈ V with high values αi may be interpreted as an
important and diverse set of nodes.

4.1 ϑ-means clustering

A common problem related to diversity in graphs is correlation clustering [3]. In correlation clus-
tering  the task is to cluster a set of items V = {1  . . .   n}  based on their similarity  or correlation 

5

Algorithm 1 ϑ-means clustering
1: Input: Graph G  with weight matrix S and node weights σ.
2: Compute kernel K ∈ K(G  σ  S)
i ← arg maxαi f (α; K)  as in (4)
3: α∗
4: Sort alphas according to ji such that αj1 ≥ αj2 ≥ ... ≥ αjn
5: Let k = (cid:100) ˆϑ(cid:101) where ˆϑ ← ω(K) = f (α∗; K)
6: either a)
7:
8: Output: result of kernel k-means with kernel K  k = (cid:100) ˆϑ(cid:101) and Z as initial labels
9: or b)
10:
11: Output: result of k-means with k = (cid:100) ˆϑ(cid:101) and C as initial cluster centroids

Compute U : K = U T U  with columns Ui  and let C ← {Uji : i ≤ k}

Initialize labels Zi = arg maxj∈{j1 ... jk} Kij

S : V × V → Rn×n  without specifying the number of clusters beforehand. This is naturally posed
as a problem of clustering the nodes of an edge-weighted graph. In a variant called overlapping
correlation clustering [4]  items may belong to several  overlapping  clusters. The usual formulation
of correlation clustering is an integer linear program [3]. Making use of geometric embeddings  we
may convert the graph clustering problem to the more standard problem of clustering a set of points
i=1 ∈ Rd×n  allowing the use of an arsenal of established techniques  such as k-means cluster-
{ui}n
ing. However  we remind ourselves of two common problems with existing clustering algorithms.

Problem 1: Number of clusters Many clustering algorithms relies on the user making a good
choice of k  the number of clusters. As this choice can have dramatic effect on both the accuracy
and speed of the algorithm  heuristics for choosing k  such as Pham et al. [22]  have been proposed.

Problem 2: Initialization Popular clustering algorithms such as Lloyd’s k-means  or expectation-
maximization for Gaussian mixture models require an initial guess of the parameters. As a result 
these algorithms are often run repeatedly with different random initializations.
We propose solutions to both problems based on ϑ1(G). To solve Problem 1  we choose k =
(cid:100)ϑ1(G)(cid:101). This is motivated by ϑ1(G) being a measure of diversity. For Problem 2  we propose
initializing parameters based on the observation that the non-zero αi are support vectors. Speciﬁ-
cally  we let the initial clusters by represented by the set of k nodes  I ⊂ V   with the largest αi. In
k-means clustering  this corresponds to letting the initial centroids be {ui}i∈I. We summarize these
ideas in algorithm 1  comprising both ϑ-means and kernel ϑ-means clustering.
√
In section 3.1  we showed that computating the approximate weighted theta function and embedding 
n) approximation to the SVD. As is well-known 
can be done in O(dn2) time for a rank d = O(
Lloyd’s algorithm has a very high worst-case complexity and will dominate the overall complexity.

5 Experiments

5.1 Weighted Maximum Cut

The maximum cut problem (Max-Cut)  a fundamental problem in graph algorithms  with applica-
tions in machine learning [25]  has famously been solved using geometric embeddings deﬁned by
semideﬁnite programs [9]. Here  given a graph G  we compute an embedding U ∈ Rd×n  the
SVM-theta labelling in [15]  using the LS-labelling  KLS. To reduce complexity  while preserving
accuracy [9]  we use a rank d =
2n truncated SVD  see section 3.1. We apply the Goemans-
Williamson random hyperplane rounding [9] to partition the embedding into two sets of points 
representing the cut. The rounding was repeated 5000 times  and the maximum cut is reported.
Helmberg & Rendl [11] constructed a set of 54 graphs  24 of which are weighted  that has since
often been used as benchmarks for Max-Cut. We use the six of the weighted graphs for which there
are multiple published results [6  21]. Our approach is closest to that of the SDP-relaxation  which

√

6

Table 2: Weighted maximum cut. c is the weight of the produced cut.

Graph
G11
G12
G13
G32
G33
G34

SDP [6]
c
528
522
542
1280
1248
1264

Time
165s
145s
145s
1318s
1417s
1295s

SVM-ϑ
c
522
518
540
1286
1260
1268

Time
3.13s
2.94s
2.97s
35.5s
36.4s
37.9s

c
564
556
580
1398
1376
1372

Best known [21]

Time
171.8s
241.5s
227.5s
900.6s
925.6s
925.6s

Table 3: Clustering of the (mini) newsgroup dataset. Average (and std. deviation) over 5 splits. ˆk is
the average number of clusters predicted. The true number is k = 16.

VOTE/BOEM
PIVOT/BOEM
BEST/BOEM
FIRST/BOEM
k-MEANS+RAND
k-MEANS+INIT
ϑ-MEANS+RAND
ϑ-MEANS

F1

31.29 ± 4.0
30.07 ± 3.4
29.67 ± 3.4
26.76 ± 3.8
17.31 ± 1.3
20.06 ± 6.8
35.60 ± 4.3
36.20 ± 4.9

ˆk
124
120
112
109
2
3
25
25

Time
8.7m
14m
13m
14m
15m
5.2m
45s
11s

has time complexity O(mn log2 n/3) [2]. In comparison  our method takes O(n2.5) time  see sec-
tion 3.1. The results are presented in table 2. For all graphs  the SVM approximation is comparable
to or better than the SDP solution  and considerably faster than the best known method [21].1

5.2 Correlation clustering

We evaluate several different versions of algorithm 1 in the task of correlation clustering  see sec-
tion 4.1. We consider a) the full version (ϑ-MEANS)  b) one with k = (cid:100) ˆϑ(cid:101) but random initialization
of centroids (ϑ-MEANS+RAND)  c) one with α-based initialization but choosing k according to Pham
et al. [22] (k-MEANS+INIT) and d) k according to [22] and random initialization (k-MEANS+RAND).
For the randomly initialized versions  we use 5 restarts of k-means++. In all versions  we cluster the
points of the embedding deﬁned by the ﬁxed kernel (LS-labelling) K = KLS(G  S).
Elsner & Schudy [7] constructed ﬁve afﬁnity matrices for a subset of the classical 20-newsgroups
dataset. Each matrix  corresponding to a different split of the data  represents the similarity between
messages in 16 different newsgroups. The task is to cluster the messages by their respective news-
group. We run algorithm 1 on every split  and compute the F1-score [7]  reporting the average and
standard deviation over all splits  as well as the predicted number of clusters  ˆk. We compare our
results to several greedy methods described by Elsner & Schudy [7]  see table 3. We only compare
to their logarithmic weighting schema  as the difference to using additive weights was negligible [7].
The results are presented in table 3. We observe that the full ϑ-means method achieves the highest
F1-score  followed by the version with random initialization (instead of using embeddings of nodes
with highest αi  see algorithm 1). We note also that choosing k by the method of Pham et al. [22]
consistently results in too few clusters  and with the greedy search methods  far too many.

5.3 Overlapping Correlation Clustering

Bonchi et al. [4] constructed a benchmark for overlapping correlation clustering based on two
datasets for multi-label classiﬁcation  Yeast and Emotion. The datasets consist of 2417 and 593
items belonging to one or more of 14 and 6 overlapping clusters respectively. Each set can be repre-
sented as an n × k binary matrix L  where k is the number of clusters and n is the number of items 

1Note that the timing results for the SDP method are from the original paper  published in 2001.

7

Table 4: Clustering of the Yeast and Emotion datasets. †The total time for ﬁnding the best solution.
The times for OCC-ISECT for a single k was 2.21s and 80.4s respectively.

OCC-ISECT [4]
ϑ-means (no k-means)

1

1
1

1

Prec. Rec.
0.98

Emotion
F1
0.99

Time
12.1†
0.34s

Prec. Rec.
0.99
1.00
0.94

1

Yeast
F1
1.00
0.97

Time
716s†
6.67s

such that Lic = 1 iff item i belongs to cluster c. From L  a weight matrix S is deﬁned such that Sij
is the Jaccard coefﬁcient between rows i and j of L. S is often sparse  as many of the pairs do not
share a single cluster. The correlation clustering task is to reconstruct L from S.
Here  we use only the centroids C = {uj1  ...  ujk} produced by algorithm 1  without running k-
means. We let each centroid c = 1  ...  k represent a cluster  and assign a node i ∈ V to that cluster 
i.e. ˆLic = 1  iff uT
i ujc > 0. We compute the precision and recall following Bonchi et al. [4]. For
comparison with Bonchi et al. [4]  we run their algorithm called OCC-ISECT with the parameter ¯k 
bounding the number of clusters  in the interval 1  ...  16 and select the one resulting in lowest cost.
The results are presented in table 4. For Emotion and Yeast  ϑ-means estimated the number of
clusters  k to be 6 (the correct number) and 8 respectively. For OCC-Isect  the k with the lowest
cost were 10 and 13. We note that while very similar in performance  the ϑ-means algorithms is
considerably faster than OCC-ISECT  especially when k is unknown.

5.4 Document summarization

Finally  we brieﬂy examine the idea of using αi to select a both relevant and diverse set of items  in a
very natural application of the weighted theta function – extractive summarization [18]. In extractive
summarization  the goal is to automatically summarize a text by picking out a small set of sentences
that best represents the whole text. We may view the sentences of a text as the nodes of a graph  with
edge weights Sij  the similarity between sentences  and node weights σi representing the relevance
of the sentence to the text as a whole. The trade-off between brevity and relevance described above
can then be viewed as ﬁnding a set of nodes that has both high total weight and high diversity. This is
naturally accomplished using our framework by computing [α∗
n](cid:62) = arg maxαi>0 f (α; K)
for ﬁxed K = KLS(G  σ  S) and picking the sentences with the highest α∗
i .
We apply this method to the multi-document summarization task of DUC-042. We let Sij be the
j Sij)2. State-of-the-
art systems  purpose-built for summarization  achieve around 0.39 in recall and F1 score [18]. Our
method achieves a score of 0.33 on both measures which is about the same as the basic version
of [18]. This is likely possible to improve by tuning the trade-off between relevance and diversity 
such as a making a more sophisticated choice of S and σ. However  we leave this to future work.

TF-IDF sentence similarity described by Lin & Bilmes [18]  and let σi = ((cid:80)

1  . . .   α∗

6 Conclusions

We have introduced a unifying generalization of Lov´asz’s theta function and the corresponding
geometric embedding to graphs with node and edge weights  characterized as a minimization over
a constrained set of kernel matrices. This allows an extension of a fast approximation of the Lov´asz
number to weighted graphs  deﬁned by an SVM problem for a ﬁxed kernel matrix. We have shown
that the theta function has a natural interpretation as a measure of diversity in graphs  a useful
function in several machine learning problems. Exploiting these results  we have deﬁned algorithms
for weighted maximum cut  correlation clustering and document summarization.

Acknowledgments

This work is supported in part by the Swedish Foundation for Strategic Research (SSF).

2http://duc.nist.gov/duc2004/

8

References
[1] S. Arora  E. Hazan  and S. Kale. Fast algorithms for approximate semideﬁnite programming using the
In Foundations of Computer Science  2005. FOCS 2005. 46th

multiplicative weights update method.
Annual IEEE Symposium on  pages 339–348. IEEE  2005.

[2] S. Arora  E. Hazan  and S. Kale. The multiplicative weights update method: a meta-algorithm and

applications. Theory of Computing  8(1):121–164  2012.

[3] N. Bansal  A. Blum  and S. Chawla. Correlation clustering. Machine Learning  56(1-3):89–113  2004.
[4] F. Bonchi  A. Gionis  and A. Ukkonen. Overlapping correlation clustering. Knowledge and information

systems  35(1):1–32  2013.

[5] M. Brand. Fast low-rank modiﬁcations of the thin singular value decomposition. Linear algebra and its

applications  415(1):20–30  2006.

[6] S. Burer and R. D. Monteiro. A projected gradient algorithm for solving the maxcut sdp relaxation.

Optimization methods and Software  15(3-4):175–200  2001.

[7] M. Elsner and W. Schudy. Bounding and comparing methods for correlation clustering beyond ilp. In
Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing  pages
19–27. Association for Computational Linguistics  2009.

[8] M. X. Goemans. Semideﬁnite programming in combinatorial optimization. Math. Program.  79:143–161 

1997.

[9] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and sat-
isﬁability problems using semideﬁnite programming. Journal of the ACM (JACM)  42(6):1115–1145 
1995.

[10] M. Gr¨otschel  L. Lov´asz  and A. Schrijver. Geometric Algorithms and Combinatorial Optimization 

volume 2 of Algorithms and Combinatorics. Springer  1988.

[11] C. Helmberg and F. Rendl. A spectral bundle method for semideﬁnite programming. SIAM Journal on

Optimization  10(3):673–696  2000.

[12] D. Hush  P. Kelly  C. Scovel  and I. Steinwart. Qp algorithms with guaranteed accuracy and run time for

support vector machines. Journal of Machine Learning Research  7:733–769  2006.

[13] G. Iyengar  D. J. Phillips  and C. Stein. Approximating semideﬁnite packing programs. SIAM Journal on

Optimization  21(1):231–268  2011.

[14] V. Jethava  A. Martinsson  C. Bhattacharyya  and D. Dubhashi. Lov´asz ϑ function  svms and ﬁnding

dense subgraphs. The Journal of Machine Learning Research  14(1):3495–3536  2013.

[15] V. Jethava  J. Sznajdman  C. Bhattacharyya  and D. Dubhashi. Lovasz ϑ  svms and applications.

Information Theory Workshop (ITW)  2013 IEEE  pages 1–5. IEEE  2013.

In

[16] F. D. Johanson  A. Chattoraj  C. Bhattacharyya  and D. Dubhashi. Supplementary material  2015.
[17] D. E. Knuth. The sandwich theorem. Electr. J. Comb.  1  1994.
[18] H. Lin and J. Bilmes. A class of submodular functions for document summarization.

In Proc. of the
49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-
Volume 1  pages 510–520. Association for Computational Linguistics  2011.

[19] L. Lov´asz. On the shannon capacity of a graph. IEEE Transactions on Information Theory  25(1):1–7 

1979.

[20] L. Lov´asz and K. Vesztergombi. Geometric representations of graphs. Paul Erdos and his Mathematics 

1999.

[21] R. Mart´ı  A. Duarte  and M. Laguna. Advanced scatter search for the max-cut problem.

Journal on Computing  21(1):26–38  2009.

INFORMS

[22] D. T. Pham  S. S. Dimov  and C. Nguyen. Selection of k in k-means clustering. Proceedings of the
Institution of Mechanical Engineers  Part C: Journal of Mechanical Engineering Science  219(1):103–
119  2005.

[23] B. Sch¨olkopf  J. C. Platt  J. Shawe-Taylor  A. J. Smola  and R. C. Williamson. Estimating the support of

a high-dimensional distribution. Neural computation  13(7):1443–1471  2001.

[24] A. Schrijver. A comparison of the delsarte and lov´asz bounds. Information Theory  IEEE Transactions

on  25(4):425–429  1979.

[25] J. Wang  T. Jebara  and S.-F. Chang. Semi-supervised learning using greedy max-cut. The Journal of

Machine Learning Research  14(1):771–800  2013.

9

,Fredrik Johansson
Ankani Chattoraj
Chiranjib Bhattacharyya
Devdatt Dubhashi