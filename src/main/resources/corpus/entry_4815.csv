2019,SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points,We analyze stochastic gradient algorithms for optimizing nonconvex problems.
In particular  our goal is to find local minima (second-order stationary points) instead of just finding first-order stationary points which may be some bad unstable saddle points.
We show that a simple perturbed version of stochastic recursive gradient descent algorithm (called SSRGD) can find an $(\epsilon \delta)$-second-order stationary point with $\widetilde{O}(\sqrt{n}/\epsilon^2 + \sqrt{n}/\delta^4 + n/\delta^3)$ stochastic gradient complexity for nonconvex finite-sum problems.
As a by-product  SSRGD finds an $\epsilon$-first-order stationary point with $O(n+\sqrt{n}/\epsilon^2)$ stochastic gradients. These results are almost optimal since Fang et al. [2018] provided a lower bound $\Omega(\sqrt{n}/\epsilon^2)$ for finding even just an $\epsilon$-first-order stationary point.
We emphasize that SSRGD algorithm for finding second-order stationary points is as simple as for finding first-order stationary points just by adding a uniform perturbation sometimes  while all other algorithms for finding second-order stationary points with similar gradient complexity need to combine with a negative-curvature search subroutine (e.g.  Neon2 [Allen-Zhu and Li  2018]).
Moreover  the simple SSRGD algorithm gets a simpler analysis.
Besides  we also extend our results from nonconvex finite-sum problems to nonconvex online (expectation) problems  and prove the corresponding convergence results.,SSRGD: Simple Stochastic Recursive Gradient

Descent for Escaping Saddle Points

Zhize Li

Tsinghua University  China and KAUST  Saudi Arabia

zhizeli.thu@gmail.com

Abstract

√

n/2 +

with (cid:101)O(

We analyze stochastic gradient algorithms for optimizing nonconvex problems. In
particular  our goal is to ﬁnd local minima (second-order stationary points) instead
of just ﬁnding ﬁrst-order stationary points which may be some bad unstable saddle
points. We show that a simple perturbed version of stochastic recursive gradient
descent algorithm (called SSRGD) can ﬁnd an (  δ)-second-order stationary point
√
n/δ4 + n/δ3) stochastic gradient complexity for nonconvex
√
ﬁnite-sum problems. As a by-product  SSRGD ﬁnds an -ﬁrst-order stationary
n/2) stochastic gradients. These results are almost optimal
point with O(n +
since Fang et al. [11] provided a lower bound Ω(
n/2) for ﬁnding even just an
-ﬁrst-order stationary point. We emphasize that SSRGD algorithm for ﬁnding
second-order stationary points is as simple as for ﬁnding ﬁrst-order stationary
points just by adding a uniform perturbation sometimes  while all other algorithms
for ﬁnding second-order stationary points with similar gradient complexity need to
combine with a negative-curvature search subroutine (e.g.  Neon2 [4]). Moreover 
the simple SSRGD algorithm gets a simpler analysis. Besides  we also extend our
results from nonconvex ﬁnite-sum problems to nonconvex online (expectation)
problems  and prove the corresponding convergence results.

√

1

Introduction

Nonconvex optimization is ubiquitous in machine learning applications especially for deep neural
networks. For convex optimization  every local minimum is a global minimum and it can be achieved
by any ﬁrst-order stationary point  i.e.  ∇f (x) = 0. However  for nonconvex problems  the point
with zero gradient can be a local minimum  a local maximum or a saddle point. To avoid converging
to bad saddle points (including local maxima)  we want to ﬁnd a second-order stationary point 
i.e.  ∇f (x) = 0 and ∇2f (x) (cid:23) 0 (this is a necessary condition for x to be a local minimum). All
second-order stationary points indeed are local minima if function f satisﬁes strict saddle property
[12]. Note that ﬁnding the global minimum in nonconvex problems is NP-hard in general. Also
note that it was shown that all local minima are also global minima for some nonconvex problems 
e.g.  matrix sensing [5]  matrix completion [13]  and some neural networks [14]. Thus  our goal
in this paper is to ﬁnd an approximate second-order stationary point (local minimum) with proved
convergence.
There has been extensive research for ﬁnding -ﬁrst-order stationary point (i.e.  (cid:107)∇f (x)(cid:107) ≤ )  e.g. 
GD  SGD and SVRG. See Table 1 for an overview. Although Xu et al. [33] and Allen-Zhu and Li
[4] independently proposed reduction algorithms Neon/Neon2 that can be combined with previous
-ﬁrst-order stationary points ﬁnding algorithms to ﬁnd an (  δ)-second-order stationary point (i.e. 
(cid:107)∇f (x)(cid:107) ≤  and λmin(∇2f (x)) ≥ −δ). However  algorithms obtained by this reduction are very
complicated in practice  and they need to extract negative curvature directions from the Hessian
to escape saddle points by using a negative curvature search subroutine: given a point x  ﬁnd an

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

approximate smallest eigenvector of ∇2f (x). This also involves a more complicated analysis. Note
that in practice  standard ﬁrst-order stationary point ﬁnding algorithms can often work (escape bad
saddle points) in nonconvex setting without a negative curvature search subroutine. The reason may
be that the saddle points are usually not very stable. So there is a natural question:
“Is there any simple modiﬁcation to allow ﬁrst-order stationary point ﬁnding algorithms to get a
theoretical second-order guarantee?".

Table 1: Gradient complexity of optimization algorithms for nonconvex ﬁnite-sum problem (1)

Algorithm
GD [24]

SVRG [28  3] 
SCSG [22] 
SVRG+ [23]
SNVRG [35] 
SPIDER [11] 

SpiderBoost [32] 

SARAH [27]

SSRGD (this paper)

PGD [18]

Neon2+FastCubic/CDHS [1  6]

Neon2+SVRG [4]

Stabilized SVRG [15]
SNVRG++Neon2 [34]

SPIDER-SFO+(+Neon2) [11]

SSRGD (this paper)

Stochastic gradient

complexity

O( n
2 )

Guarantee Negative-curvature
search subroutine
1st-order

No

O(n + n2/3
2 )

1st-order

O(n + n1/2
2 )

1.5 + n

(cid:101)O( n
(cid:101)O( n
(cid:101)O( n2/3
(cid:101)O( n2/3
(cid:101)O( n1/2
(cid:101)O( n1/2
(cid:101)O( n1/2

O(n + n1/2
2 )
2 + n
δ4 )
1.75 + n3/4
δ3 + n3/4
δ3.5 )
δ3 + n3/4
δ3.5 )
δ3 + n2/3
δ4 )
δ3 + n3/4
δ3.5 )
δ3 + 1
δ5 )
δ4 + n
δ3 )

2 + n
2 + n
2 + n
2 + n1/2

2 + n1/2

δ2 + 1

1st-order

1st-order
2nd-order
2nd-order
2nd-order
2nd-order
2nd-order
2nd-order
2nd-order

No

No

No
No

Needed
Needed

No

Needed
Needed

No

Table 2: Gradient complexity of optimization algorithms for nonconvex online problem (2)
Guarantee Negative-curvature
search subroutine
1st-order
1st-order

Stochastic gradient

complexity

No
No

O( 1

O( 1

4 )
3.5 )

Note: 1. Guarantee (see Deﬁnition 1): -ﬁrst-order stationary point (cid:107)∇f (x)(cid:107) ≤ ; (  δ)-second-
order stationary point (cid:107)∇f (x)(cid:107) ≤  and λmin(∇2f (x)) ≥ −δ.
2. In the classical setting where δ = O(
) [25  18]  our simple SSRGD is always (no matter what n
and  are) not worse than all other algorithms (in both Table 1 and 2) except FastCubic/CDHS (which
need to compute Hessian-vector product) and SPIDER-SFO+. Moreover  our simple SSRGD is not
worse than FastCubic/CDHS if n ≥ 1/ and is better than SPIDER-SFO+ if δ is very small (e.g. 
√
δ ≤ 1/

n) in Table 1.

√

2

Algorithm
SGD [16]
SCSG [22];
SVRG+ [23]
SNVRG [35];
SPIDER [11];

SpiderBoost [32];

SARAH [27]

SSRGD (this paper)
Perturbed SGD [12]

CNC-SGD [8]

Neon2+SCSG [4]

Neon2+Natasha2 [2]
SNVRG++Neon2 [34]

SPIDER-SFO+(+Neon2) [11]

SSRGD (this paper)

O( 1

3 )

1st-order

1

(cid:101)O( 1
(cid:101)O(
(cid:101)O( 1
(cid:101)O( 1
(cid:101)O( 1
(cid:101)O( 1

O( 1
3 )
poly(d  1
   1
δ )
4 + 1
δ10 )
10/3 + 1
3.25 + 1
3 + 1
3 + 1
3 + 1

2δ3 + 1
δ5 )
3δ + 1
δ5 )
2δ3 + 1
δ5 )
2δ2 + 1
δ5 )
2δ3 + 1
δ4 )

1st-order
2nd-order
2nd-order
2nd-order
2nd-order
2nd-order
2nd-order
2nd-order

No

No
No
No

Needed
Needed
Needed
Needed

No

Algorithm 1 Simple Stochastic Recursive Gradient Descent (SSRGD)
Input: initial point x0  epoch length m  minibatch size b  step size η  perturbation radius r  threshold

gradient gthres

1: for s = 0  1  2  . . . do
2:
3:

if not currently in a super epoch and (cid:107)∇f (xsm)(cid:107) ≤ gthres then

xsm ← xsm + ξ  where ξ uniformly ∼ B0(r)  start a super epoch
// we use super epoch since we do not want to add the perturbation too often near a saddle
point
end if
vsm ← ∇f (xsm)
for k = 1  2  . . .   m do
t ← sm + k
xt ← xt−1 − ηvt−1
vt ← 1
// Ib are i.i.d. uniform samples with |Ib| = b
if meet stop condition then stop super epoch

(cid:0)∇fi(xt) − ∇fi(xt−1)(cid:1) + vt−1

(cid:80)

b

i∈Ib

4:
5:
6:
7:
8:
9:

10:
11:
12: end for

end for

For gradient descent (GD)  Jin et al. [18] showed that a simple perturbation step is enough to escape
saddle points for ﬁnding a second-order stationary point  and this is necessary [10]. Very recently  Ge
et al. [15] showed that a simple perturbation step is also enough to ﬁnd a second-order stationary
point for SVRG algorithm [23]. Moreover  Ge et al. [15] also developed a stabilized trick to further
improve the dependency of Hessian Lipschitz parameter.

1.1 Our Contributions

In this paper  we propose a simple SSRGD algorithm (described in Algorithm 1) showed that a simple
perturbation step is enough to ﬁnd a second-order stationary point for stochastic recursive gradient
descent algorithm. Our results and previous results are summarized in Table 1 and 2. We would like
to highlight the following points:

• We improve the result in [15] to the almost optimal one (i.e.  from n2/3/2 to n1/2/2) since
Fang et al. [11] provided a lower bound Ω(
n/2) for ﬁnding even just an -ﬁrst-order
stationary point. Note that for the other two n1/2 algorithms (i.e.  SNVRG+ and SPIDER-
SFO+)  they both need the negative curvature search subroutine (e.g. Neon2) thus are more
complicated in practice and in analysis compared with their ﬁrst-order guarantee algorithms
(SNVRG and SPIDER)  while our SSRGD is as simple as its ﬁrst-order guarantee algorithm
just by adding a uniform perturbation sometimes.

√

• For more general nonconvex online (expectation) problems (2)  we obtain the ﬁrst algorithm
which is as simple as ﬁnding ﬁrst-order stationary points for ﬁnding a second-order stationary
point with similar state-of-the-art convergence result. See the last column of Table 2.

• Our simple SSRGD algorithm gets simpler analysis. Also  the result for ﬁnding a ﬁrst-order
stationary point is a by-product from our analysis. We also give a clear interpretation to show
why our analysis for SSRGD algorithm can improve the original SVRG from n2/3 to n1/2
in Section 5.1. We believe it is very useful for better understanding these two algorithms.

2 Preliminaries
Notation: Let [n] denote the set {1  2 ···   n} and (cid:107) · (cid:107) denote the Eculidean norm for a vector
and the spectral norm for a matrix. Let (cid:104)u  v(cid:105) denote the inner product of two vectors u and v. Let
λmin(A) denote the smallest eigenvalue of a symmetric matrix A. Let Bx(r) denote a Euclidean ball

with center x and radius r. We use O(·) to hide the constant and (cid:101)O(·) to hide the polylogarithmic

factor.

3

In this paper  we consider two types of nonconvex problems. The ﬁnite-sum problem has the form

min
x∈Rd

f (x) :=

1
n

n(cid:88)

i=1

fi(x) 

(1)

where f (x) and all individual fi(x) are possibly nonconvex. This form usually models the empirical
risk minimization in machine learning problems.
The online (expectation) problem has the form

f (x) := Eζ∼D[F (x  ζ)] 

min
x∈Rd

(2)

where f (x) and F (x  ζ) are possibly nonconvex. This form usually models the population risk
minimization in machine learning problems.
Now  we make standard smoothness assumptions for these two problems.

Assumption 1 (Gradient Lipschitz)

1. For ﬁnite-sum problem (1)  each fi(x) is differen-

tiable and has L-Lipschitz continuous gradient  i.e. 

(cid:107)∇fi(x1) − ∇fi(x2)(cid:107) ≤ L(cid:107)x1 − x2(cid:107) 

∀x1  x2 ∈ Rd.

(3)

2. For online problem (2)  F (x  ζ) is differentiable and has L-Lipschitz continuous gradient 

i.e. 

(cid:107)∇F (x1  ζ) − ∇F (x2  ζ)(cid:107) ≤ L(cid:107)x1 − x2(cid:107) 

∀x1  x2 ∈ Rd.

(4)

Assumption 2 (Hessian Lipschitz)

1. For ﬁnite-sum problem (1)  each fi(x) is twice-

differentiable and has ρ-Lipschitz continuous Hessian  i.e. 
(cid:107)∇2fi(x1) − ∇2fi(x2)(cid:107) ≤ ρ(cid:107)x1 − x2(cid:107) 

∀x1  x2 ∈ Rd.

(5)

2. For online problem (2)  F (x  ζ) is twice-differentiable and has ρ-Lipschitz continuous

Hessian  i.e. 

(cid:107)∇2F (x1  ζ) − ∇2F (x2  ζ)(cid:107) ≤ ρ(cid:107)x1 − x2(cid:107) 

∀x1  x2 ∈ Rd.

(6)

These two assumptions are standard for ﬁnding ﬁrst-order stationary points (Assumption 1) and
second-order stationary points (Assumption 1 and 2) for all algorithms in both Table 1 and 2.
Now we deﬁne the approximate ﬁrst-order stationary points and approximate second-order stationary
points.

Deﬁnition 1 x is an -ﬁrst-order stationary point for a differentiable function f if

(cid:107)∇f (x)(cid:107) ≤ .

x is an (  δ)-second-order stationary point for a twice-differentiable function f if

(cid:107)∇f (x)(cid:107) ≤  and λmin(∇2f (x)) ≥ −δ.

(7)

(8)

The deﬁnition of (  δ)-second-order stationary point is the same as [4  8  34  11] and it generalizes
the classical version where δ =

ρ used in [25  18  15].

√

3 Simple Stochastic Recursive Gradient Descent

In this section  we propose the simple stochastic recursive gradient descent algorithm called SSRGD.
The high-level description (which omits the stop condition details in Line 10) of this algorithm is
in Algorithm 1 and the full algorithm (containing the stop condition) is described in Algorithm 2.
Compared with the high-level Algorithm 1  the only difference is that Algorithm 2 contains the stop
condition of super epoch (Line 13–14 of Algorithm 2) and the random stop of epoch (Line 15–16 of
Algorithm 2). Note that we call each outer loop an epoch  i.e.  iterations t from sm to (s + 1)m for
an epoch s. We call the iterations between the beginning of perturbation and end of perturbation a
super epoch.

4

super_epoch ← 1

// start a super epoch near a saddle point

gradient gthres  threshold function value fthres  super epoch length tthres

if super_epoch = 0 and (cid:107)∇f (xsm)(cid:107) ≤ gthres then

Algorithm 2 Simple Stochastic Recursive Gradient Descent (SSRGD)
Input: initial point x0  epoch length m  minibatch size b  step size η  perturbation radius r  threshold
1: super_epoch ← 0
2: for s = 0  1  2  . . . do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

(cid:101)x ← xsm  tinit ← sm
xsm ←(cid:101)x + ξ  where ξ uniformly ∼ B0(r)
(cid:0)∇fi(xt) − ∇fi(xt−1)(cid:1) + vt−1

end if
vsm ← ∇f (xsm)
for k = 1  2  . . .   m do
t ← sm + k
xt ← xt−1 − ηvt−1
vt ← 1
// Ib are i.i.d. uniform samples with |Ib| = b

if super_epoch = 1 and (f ((cid:101)x) − f (xt) ≥ fthres or t − tinit ≥ tthres) then

(cid:80)

b

i∈Ib

13:
14:
15:
16:

super_epoch ← 0; break
else if super_epoch = 0 then
1

break with probability
// we use random stop since we want to randomly choose a point as the starting point of
the next epoch

m−k+1

end if
end for
x(s+1)m ← xt

17:
18:
19:
20: end for

The SSRGD algorithm is based on the stochastic recursive gradient descent which is introduced
in [26] for convex optimization. In particular  Nguyen et al. [26] want to save the storage of past
gradients in SAGA [9] by using the recursive gradient. However  this stochastic recursive gradient
descent is widely used in recent work for nonconvex optimization such as SPIDER [11]  SpiderBoost
[32] and some variants of SARAH (e.g.  ProxSARAH [27]).
Recall that in the well-known SVRG algorithm  Johnson and Zhang [19] reused a ﬁxed snapshot full

gradient ∇f ((cid:101)x) (which is computed at the beginning of each epoch) in the gradient estimator:

vt ← 1
b

(cid:88)
(cid:0)∇fi(xt) − ∇fi((cid:101)x)(cid:1) + ∇f ((cid:101)x) 
(cid:88)
(cid:0)∇fi(xt) − ∇fi(xt−1)(cid:1) + vt−1.

i∈Ib

(9)

(10)

while the stochastic recursive gradient descent uses a recursive update form (more timely update):

vt ← 1
b

4 Convergence Results

i∈Ib

Similar to the perturbed GD [18] and perturbed SVRG [15]  we add simple perturbations to the
stochastic recursive gradient descent algorithm to escape saddle points efﬁciently. Besides  we also
consider the more general online case. In the following theorems  we provide the convergence results
of SSRGD for ﬁnding an -ﬁrst-order stationary point and an (  δ)-second-order stationary point for
both nonconvex ﬁnite-sum problem (1) and online problem (2). The detailed proofs are provided in
Appendix C. We give an overview of the proofs in next Section 5.

4.1 Nonconvex Finite-sum Problem
Theorem 1 Under Assumption 1 (i.e. (3))  let ∆f := f (x0) − f∗  where x0 is the initial point and
f∗ is the optimal value of f. By letting step size η ≤ √
n and minibatch

5−1
2L   epoch length m =

√

5

√

size b =

n  SSRGD will ﬁnd an -ﬁrst-order stationary point in expectation using

(cid:16)

O

n +

L∆f
2

(cid:17)

√

n

stochastic gradients for nonconvex ﬁnite-sum problem (1).
Theorem 2 Under Assumption 1 and 2 (i.e. (3) and (5))  let ∆f := f (x0) − f∗  where x0 is the
√
n 

n  perturbation radius r = (cid:101)O(cid:0) min( δ3

initial point and f∗ is the optimal value of f. By letting step size η = (cid:101)O( 1
threshold function value fthres = (cid:101)O( δ3
(cid:16) L∆f

ρ2 ) and super epoch length tthres = (cid:101)O( 1

L )  epoch length m =

)(cid:1)  threshold gradient gthres =  
(cid:17)

ηδ )  SSRGD will at

least once get to an (  δ)-second-order stationary point with high probability using

minibatch size b =

ρ2∆f n

√
ρ2   δ3/2

ρ

L

√

√

n

√

n

+

Lρ2∆f
δ4

2

+

δ3

(cid:101)O

stochastic gradients for nonconvex ﬁnite-sum problem (1).

4.2 Nonconvex Online (Expectation) Problem

For nonconvex online problem (2)  one usually needs the following bounded variance assumption.
For notational convenience  we also consider this online case as the ﬁnite-sum form by letting
∇fi(x) := ∇F (x  ζi) and thinking of n as inﬁnity (inﬁnite data samples). Although we try to write
it as ﬁnite-sum form  the convergence analysis of optimization methods in this online case is a little
different from the ﬁnite-sum case.
Assumption 3 (Bounded Variance) For ∀x ∈ Rd  Ei[(cid:107)∇fi(x)−∇f (x)(cid:107)2] := Eζi[(cid:107)∇F (x  ζi)−
∇f (x)(cid:107)2] ≤ σ2  where σ > 0 is a constant.

Note that this assumption is standard and necessary for this online case since the full gradients are not
available (see e.g.  [16  22  23  21  20  35  11  32  27]). Moreover  we need to modify the full gradient
computation step at the beginning of each epoch to a large batch stochastic gradient computation step
(similar to [22  23])  i.e.  change vsm ← ∇f (xsm) (Line 8 of Algorithm 2) to

∇fj(xsm) 

(11)

(cid:88)

j∈IB

vsm ← 1
B

where IB are i.i.d. samples with |IB| = B. We call B the batch size and b the minibatch size. Also 
we need to change (cid:107)∇f (xsm)(cid:107) ≤ gthres (Line 3 of Algorithm 2) to (cid:107)vsm(cid:107) ≤ gthres.
Theorem 3 Under Assumption 1 (i.e. (4)) and Assumption 3  let ∆f := f (x0) − f∗  where x0 is
the initial point and f∗ is the optimal value of f. By letting step size η ≤ √
5−1
2L   batch size B = 4σ2
2  
minibatch size b =
 and epoch length m = b  SSRGD will ﬁnd an -ﬁrst-order stationary
point in expectation using

B = σ

√

(cid:17)

L∆f σ

(cid:16) σ2

O

2 +

3
stochastic gradients for nonconvex online problem (2).

For achieving a high probability result of ﬁnding second-order stationary points in this online case
(i.e.  Theorem 4)  we need a stronger version of Assumption 3 as in the following Assumption 4.
Assumption 4 (Bounded Variance) For ∀i  x  (cid:107)∇fi(x)−∇f (x)(cid:107)2 := (cid:107)∇F (x  ζi)−∇f (x)(cid:107)2 ≤
σ2  where σ > 0 is a constant.
We want to point out that Assumption 4 can be relaxed such that (cid:107)∇fi(x)−∇f (x)(cid:107) has sub-Gaussian
tail  i.e.  E[exp(λ(cid:107)∇fi(x) − ∇f (x)(cid:107))] ≤ exp(λ2σ2/2)  for ∀λ ∈ R. Then it is sufﬁcient for us to
get a high probability bound by using Hoeffding bound on these sub-Gaussian variables. Note that
Assumption 4 (or the relaxed sub-Gaussian version) is also standard in online case for second-order
stationary point ﬁnding algorithms (see e.g.  [4  34  11]).

6

g2

thres

√

Theorem 4 Under Assumption 1  2 (i.e. (4) and (6)) and Assumption 4  let ∆f := f (x0) − f∗ 
L )  batch
 )  epoch length m = b  perturbation

where x0 is the initial point and f∗ is the optimal value of f. By letting step size η = (cid:101)O( 1
B = (cid:101)O( σ
) = (cid:101)O( σ2
size B = (cid:101)O( σ2
radius r = (cid:101)O(cid:0) min( δ3
ρ2 ) and super epoch length tthres = (cid:101)O( 1
fthres = (cid:101)O( δ3

)(cid:1)  threshold gradient gthres =  ≤ δ2/ρ  threshold function value
(cid:16) L∆f σ
(cid:101)O

2 )  minibatch size b =
√
ρ2   δ3/2
ρ

(  δ)-second-order stationary point with high probability using

ηδ )  SSRGD will at least once get to an

ρ2∆f σ2
2δ3 +
stochastic gradients for nonconvex online problem (2).

3 +

Lρ2∆f σ

(cid:17)

δ4

L

5 Overview of the Proofs

5.1 Finding First-order Stationary Points

In this section  we ﬁrst show that why SSRGD algorithm can improve previous SVRG type algorithm
(see e.g.  [23  15]) from n2/3/2 to n1/2/2. Then we give a simple high-level proof for achieving
the n1/2/2 convergence result (i.e.  Theorem 1).

Why it can be improved from n2/3/2 to n1/2/2: First  we need a key relation between f (xt) and
f (xt−1)  where xt := xt−1 − ηvt−1 
f (xt) ≤ f (xt−1) − η
2

(cid:107)∇f (xt−1)(cid:107)2 −(cid:0) 1

(cid:1)(cid:107)xt − xt−1(cid:107)2 +

(cid:107)∇f (xt−1) − vt−1(cid:107)2 

− L
2

(12)

η
2

2η

2

2η − L

is large. The second term −(cid:0) 1

(cid:1)(cid:107)xt− xt−1(cid:107)2 indicates that the function value will also decrease

where (12) holds since f has L-Lipschitz continuous gradient (Assumption 1). The details for
obtaining (12) can be found in Appendix C.1 (see (27)).
Note that (12) is very meaningful and also very important for the proofs. The ﬁrst term
− η
2(cid:107)∇f (xt−1)(cid:107)2 indicates that the function value will decrease a lot if the gradient ∇f (xt−1)
a lot if the moving distance xt − xt−1 is large (note that here we require the step size η ≤ 1
L). The
2(cid:107)∇f (xt−1) − vt−1(cid:107)2 exists since we use vt−1 as an estimator of the actual
additional third term + η
gradient ∇f (xt−1) (i.e.  xt := xt−1 − ηvt−1). So it may increase the function value if vt−1 is a bad
direction in this step.
To get an -ﬁrst-order stationary point  we want to cancel the last two terms in (12). Firstly  we want
to bound the last variance term. Recall the variance bound (see Equation (29) in [23]) for SVRG
algorithm  i.e.  estimator (9):

In order to connect the last two terms in (12)  we use Young’s inequality for the second term
(cid:107)xt − xt−1(cid:107)2  i.e.  −(cid:107)xt − xt−1(cid:107)2 ≤ 1
this Young’s inequality and (13) into (12)  we can cancel the last two terms in (12) by summing up
(12) for each epoch  i.e.  for each epoch s (i.e.  iterations sm + 1 ≤ t ≤ sm + m)  we have (see
Equation (35) in [23])

E(cid:2)(cid:107)∇f (xt−1) − vt−1(cid:107)2(cid:3) ≤ L2
E[(cid:107)xt−1 −(cid:101)x(cid:107)2].
α(cid:107)xt−1 −(cid:101)x(cid:107)2 − 1
1+α(cid:107)xt −(cid:101)x(cid:107)2 (for any α > 0). By plugging
sm+m(cid:88)

(13)

b

E[(cid:107)∇f (xj−1)(cid:107)2].

(14)

E[f (x(s+1)m)] ≤ E[f (xsm)] − η
2

j=sm+1

However  due to the Young’s inequality  we need to let b ≥ m2 to cancel the last two terms in (12) for
obtaining (14)  where b denotes minibatch size and m denotes the epoch length. According to (14)  it
is not hard to see that ˆx is an -ﬁrst-order stationary point in expectation (i.e.  E[(cid:107)∇f (ˆx)(cid:107)] ≤ ) if ˆx is
chosen uniformly randomly from {xt−1}t∈[T ] and the number of iterations T = Sm = 2(f (x0)−f∗)
.
Note that for each iteration we need to compute b+ n
m stochastic gradients  where we amortize the full
gradient computation of the beginning point of each epoch (n stochastic gradients) into each iteration

η2

7

m ) ≥ n2/3
in its epoch (i.e.  n/m) for simple presentation. Thus  the convergence result is T (b + n
since b ≥ m2  where equality holds if b = m2 = n2/3. Note that here we ignore the factors of
f (x0) − f∗ and η = O(1/L).
However  for stochastic recursive gradient descent estimator (10)  we can bound the last variance
term in (12) as (see Equation (33) in Appendix C.1):

2

E(cid:2)(cid:107)∇f (xt−1) − vt−1(cid:107)2(cid:3) ≤ L2

t−1(cid:88)

b

j=sm+1

E[(cid:107)xj − xj−1(cid:107)2].

(15)

sm+m(cid:88)

j=sm+1

Now  the advantage of (15) compared with (13) is that it is already connected to the second term in
(12)  i.e.  moving distances {(cid:107)xt−xt−1(cid:107)2}t. Thus we do not need an additional Young’s inequality to
transform the second term as before. This makes the function value decrease bound tighter. Similarly 
we plug (15) into (12) and sum it up for each epoch to cancel the last two terms in (12)  i.e.  for each
epoch s  we have (see Equation (35) in Appendix C.1)

E[f (x(s+1)m)] ≤ E[f (xsm)] − η
2

E[(cid:107)∇f (xj−1)(cid:107)2].

(16)

Compared with (14) (which requires b ≥ m2)  here (16) only requires b ≥ m due to the tighter
function value decrease bound since it does not involve the additional Young’s inequality.

High-level proof for achieving n1/2/2 result: Now  according to (16)  we can use the same
above SVRG arguments to show the n1/2/2 convergence result of SSRGD  i.e.  ˆx is an -ﬁrst-
order stationary point in expectation (i.e.  E[(cid:107)∇f (ˆx)(cid:107)] ≤ ) if ˆx is chosen uniformly randomly
from {xt−1}t∈[T ] and the number of iterations T = Sm = 2(f (x0)−f∗)
. Also  for each iteration 
we compute b + n
m stochastic gradients. The only difference is that now the convergence result
) since b ≥ m (rather than b ≥ m2)  where we let b = m = n1/2 
is T (b + n
η = O(1/L) and ∆f := f (x0) − f∗. Moreover  it is optimal since it matches the lower bound
√
Ω( L∆f
2

) provided by [11].

m ) = O( L∆f

√
2

η2

n

n

5.2 Finding Second-order Stationary Points

In this section  we only discuss some high-level proof ideas for ﬁnding a second-order stationary point
with high probability due to the space limit. We provide a more detailed proof sketch in Appendix
A. We have discussed the difference of the ﬁrst-order guarantee analysis between estimator (9) and
estimator (10) in previous Section 5.1. For the second-order analysis  since the estimator (10) in our
SSRGD is more correlated than (9)  thus we will use martingales to handle it. Besides  different
estimators will incur more differences in the detailed proofs of second-order guarantee analysis than
that of ﬁrst-order guarantee analysis.
We divide the proof into two situations  i.e.  large gradients and around saddle points. According
to (16)  a natural way to prove the convergence result is that the function value will decrease at a
desired rate with high probability. Note that the total amount for function value decrease is at most
∆f := f (x0) − f∗.
Large gradients: (cid:107)∇f (x)(cid:107) ≥ gthres.
In this situation  due to the large gradients  it is sufﬁcient to adjust the ﬁrst-order analysis to show that
the function value will decrease a lot in an epoch with high probability. Concretely  we want to show
that the function value decrease bound (16) holds with high probability by using Azuma-Hoeffding
inequality to bound the variance term (15) with high probability. Then  according to (16)  it is not
L ) per iteration in
hard to see that the desired rate of function value decrease is O(ηg2

this situation (recall the parameters gthres =  and η = (cid:101)O(1/L) in our Theorem 2). Also note

thres) = (cid:101)O( 2

that we compute b + n
n in our
Theorem 2). Here we amortize the full gradient computation of the beginning point of each epoch
(n stochastic gradients) into each iteration in its epoch (i.e.  n/m) for simple presentation (we will
analyze this more rigorously in the detailed proofs in appendices). Thus the number of stochastic

n stochastic gradients at each iteration (recall m = b =

m = 2

√

√

gradient computation is at most (cid:101)O(

√

2/L ) = (cid:101)O( L∆f

√
2

n ∆f

n

) for this large gradients situation.

8

Note that (16) only guarantees function value decrease when the summation of gradients in this epoch
is large. However  in order to connect the guarantees between ﬁrst situation (large gradients) and
second situation (around saddle points)  we need to show guarantees that are related to the gradient
of the starting point of each epoch (see Line 3 of Algorithm 2). Similar to [15]  we achieve this by
stopping the epoch at a uniformly random point (see Line 16 of Algorithm 2). Then  we will know
that either the function value already decreases a lot in this epoch s or the starting point of the next
epoch x(s+1)m is around a saddle point (or x(s+1)m is already a second-order stationary point).

Around saddle points: (cid:107)∇f ((cid:101)x)(cid:107) ≤ gthres and λmin(∇2f ((cid:101)x)) ≤ −δ at the initial point(cid:101)x of a super
initial point(cid:101)x. To simplify the presentation  we use x0 :=(cid:101)x + ξ to denote the starting point of the

epoch.
In this situation  we want to show that the function value will decrease a lot in a super epoch (instead
of an epoch as in the ﬁrst situation) with high probability by adding a random perturbation at the
super epoch after the perturbation  where ξ uniformly ∼ B0(r) (see Line 6 in Algorithm 2).
Firstly  we show that if function value does not decrease a lot  then all iteration points are not far from
the starting point with high probability (localization). Concretely  we have

∀t  (cid:107)xt − x0(cid:107) ≤

where C(cid:48) = (cid:101)O(1). Then we show that the stuck region is relatively small in the random perturbation

ball  i.e.  xt will go far away from the perturbed starting point x0 with high probability (small stuck
region). Concretely  we have

(17)

C(cid:48)L

 

where C1 = (cid:101)O(1). Based on (17) and (18)  we can prove that

∃t ≤ tthres  (cid:107)xt − x0(cid:107) ≥ δ

C1ρ  

∃t ≤ tthres  f ((cid:101)x) − f (xt) ≥ fthres

(18)

(cid:113) 4t(f (x0)−f (xt))

√

√

√

n

√

n

√
m = 2

) for this around saddle points situation.

n stochastic gradients
n in our Theorem 2). Thus the number of stochastic gradient

holds with high probability.
Now  we can obtain that the desired rate of function value decrease in this situation is fthres/tthres =

Lρ2 ) per iteration (recall the parameters fthres = (cid:101)O(δ3/ρ2)  tthres = (cid:101)O(1/(ηδ))

at each iteration (recall m = b =
n ∆f

(cid:101)O( δ3/ρ2
1/(ηδ) ) = (cid:101)O( δ4
and η = (cid:101)O(1/L) in our Theorem 2). Same as before  we compute b + n
computation is at most (cid:101)O(
In sum  the number of stochastic gradient computation is at most (cid:101)O( L∆f
situation and is at most (cid:101)O( Lρ2∆f
) = (cid:101)O( L∆f

δ4/(Lρ2) ) = (cid:101)O( Lρ2∆f
ρ [25  18]  then (cid:101)O( Lρ2∆f

) for the large gradients
) for the around saddle points situation. Moreover  for the
classical version where δ =
)  i.e.  both situations get
the same stochastic gradient complexity. This also matches the convergence result for ﬁnding ﬁrst-
order stationary points (see our Theorem 1) if we ignore the logarithmic factor. More importantly  it
√
also almost matches the lower bound Ω( L∆f
) provided by [11] for ﬁnding even just an -ﬁrst-order
2
stationary point.
Finally  we point out that there is an extra term ρ2∆f n
in Theorem 2 beyond these two terms obtained
from the above two situations. The reason is that we amortize the full gradient computation of the
beginning point of each epoch (n stochastic gradients) into each iteration in its epoch (i.e.  n/m)
for simple presentation. We will analyze this more rigorously in the appendices  which incurs the
term ρ2∆f n
. For the more general online problem (2)  the high-level proofs are almost the same as
the ﬁnite-sum problem (1). The difference is that we need to use more concentration bounds in the
detailed proofs since the full gradients are not available in online case.

√
2
√
2

√

n

√

δ4

n

n

δ3

n

δ3

δ4

δ4

Acknowledgments

This work was supported by Ofﬁce of Sponsored Research of KAUST  through the Baseline Research
Fund of Prof. Peter Richtárik. The author would like to thank Rong Ge (Duke)  Jian Li (Tsinghua)
and the anonymous reviewers for their useful discussions/suggestions.

9

References
[1] Naman Agarwal  Zeyuan Allen-Zhu  Brian Bullins  Elad Hazan  and Tengyu Ma. Find-
ing approximate local minima for nonconvex optimization in linear time. arXiv preprint
arXiv:1611.01146  2016.

[2] Zeyuan Allen-Zhu. Natasha 2: Faster non-convex optimization than sgd. In Advances in Neural

Information Processing Systems  pages 2680–2691  2018.

[3] Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In

International Conference on Machine Learning  pages 699–707  2016.

[4] Zeyuan Allen-Zhu and Yuanzhi Li. Neon2: Finding local minima via ﬁrst-order oracles. In

Advances in Neural Information Processing Systems  pages 3720–3730  2018.

[5] Srinadh Bhojanapalli  Behnam Neyshabur  and Nati Srebro. Global optimality of local search
for low rank matrix recovery. In Advances in Neural Information Processing Systems  pages
3873–3881  2016.

[6] Yair Carmon  John C Duchi  Oliver Hinder  and Aaron Sidford. Accelerated methods for

non-convex optimization. arXiv preprint arXiv:1611.00756  2016.

[7] Fan Chung and Linyuan Lu. Concentration inequalities and martingale inequalities: a survey.

Internet Mathematics  3(1):79–127  2006.

[8] Hadi Daneshmand  Jonas Kohler  Aurelien Lucchi  and Thomas Hofmann. Escaping saddles

with stochastic gradients. arXiv preprint arXiv:1803.05999  2018.

[9] Aaron Defazio  Francis Bach  and Simon Lacoste-Julien. Saga: A fast incremental gradient
method with support for non-strongly convex composite objectives. In Advances in Neural
Information Processing Systems  pages 1646–1654  2014.

[10] Simon S Du  Chi Jin  Jason D Lee  Michael I Jordan  Aarti Singh  and Barnabas Poczos.
Gradient descent can take exponential time to escape saddle points. In Advances in Neural
Information Processing Systems  pages 1067–1077  2017.

[11] Cong Fang  Chris Junchi Li  Zhouchen Lin  and Tong Zhang. Spider: Near-optimal non-
convex optimization via stochastic path-integrated differential estimator. In Advances in Neural
Information Processing Systems  pages 687–697  2018.

[12] Rong Ge  Furong Huang  Chi Jin  and Yang Yuan. Escaping from saddle points — online
stochastic gradient for tensor decomposition. In Conference on Learning Theory  pages 797–842 
2015.

[13] Rong Ge  Jason D Lee  and Tengyu Ma. Matrix completion has no spurious local minimum. In

Advances in Neural Information Processing Systems  pages 2973–2981  2016.

[14] Rong Ge  Jason D Lee  and Tengyu Ma. Learning one-hidden-layer neural networks with

landscape design. arXiv preprint arXiv:1711.00501  2017.

[15] Rong Ge  Zhize Li  Weiyao Wang  and Xiang Wang. Stabilized svrg: Simple variance reduction

for nonconvex optimization. In Conference on Learning Theory  2019.

[16] Saeed Ghadimi  Guanghui Lan  and Hongchao Zhang. Mini-batch stochastic approximation
methods for nonconvex stochastic composite optimization. Mathematical Programming  155(1-
2):267–305  2016.

[17] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of

the American Statistical Association  58(301):13–30  1963.

[18] Chi Jin  Rong Ge  Praneeth Netrapalli  Sham M Kakade  and Michael I Jordan. How to escape
saddle points efﬁciently. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70  pages 1724–1732. JMLR. org  2017.

10

[19] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance

reduction. In Advances in neural information processing systems  pages 315–323  2013.

[20] Guanghui Lan  Zhize Li  and Yi Zhou. A uniﬁed variance-reduced accelerated gradient method

for convex optimization. In Advances in Neural Information Processing Systems  2019.

[21] Guanghui Lan and Yi Zhou. Random gradient extrapolation for distributed and stochastic

optimization. SIAM Journal on Optimization  28(4):2753–2782  2018.

[22] Lihua Lei  Cheng Ju  Jianbo Chen  and Michael I Jordan. Non-convex ﬁnite-sum optimization
via scsg methods. In Advances in Neural Information Processing Systems  pages 2345–2355 
2017.

[23] Zhize Li and Jian Li. A simple proximal stochastic gradient method for nonsmooth nonconvex
optimization. In Advances in Neural Information Processing Systems  pages 5569–5579  2018.

[24] Yurii Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer  2004.

[25] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global

performance. Mathematical Programming  108(1):177–205  2006.

[26] Lam M Nguyen  Jie Liu  Katya Scheinberg  and Martin Takáˇc. Sarah: A novel method for
machine learning problems using stochastic recursive gradient. In Proceedings of the 34th
International Conference on Machine Learning-Volume 70  pages 2613–2621. JMLR. org  2017.

[27] Nhan H Pham  Lam M Nguyen  Dzung T Phan  and Quoc Tran-Dinh. Proxsarah: An efﬁ-
cient algorithmic framework for stochastic composite nonconvex optimization. arXiv preprint
arXiv:1902.05679  2019.

[28] Sashank J Reddi  Ahmed Hefny  Suvrit Sra  Barnabás Póczos  and Alex Smola. Stochastic
variance reduction for nonconvex optimization. In International conference on machine learning 
pages 314–323  2016.

[29] Terence Tao and Van Vu. Random matrices: Universality of local spectral statistics of non-

hermitian matrices. The Annals of Probability  43(2):782–874  2015.

[30] Joel A Tropp. User-friendly tail bounds for matrix martingales. Technical report  CALIFORNIA

INST OF TECH PASADENA  2011.

[31] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computa-

tional mathematics  12(4):389–434  2012.

[32] Zhe Wang  Kaiyi Ji  Yi Zhou  Yingbin Liang  and Vahid Tarokh. Spiderboost: A class of faster
variance-reduced algorithms for nonconvex optimization. arXiv preprint arXiv:1810.10690 
2018.

[33] Yi Xu  Jing Rong  and Tianbao Yang. First-order stochastic algorithms for escaping from saddle
points in almost linear time. In Advances in Neural Information Processing Systems  pages
5535–5545  2018.

[34] Dongruo Zhou  Pan Xu  and Quanquan Gu. Finding local minima via stochastic nested variance

reduction. arXiv preprint arXiv:1806.08782  2018.

[35] Dongruo Zhou  Pan Xu  and Quanquan Gu. Stochastic nested variance reduction for nonconvex

optimization. arXiv preprint arXiv:1806.07811  2018.

11

,Jan-Matthis Lueckmann
Pedro Goncalves
Giacomo Bassetto
Kaan Öcal
Marcel Nonnenmacher
Jakob Macke
Zhize Li