2012,On the Sample Complexity of Robust PCA,We estimate the sample complexity of a recent robust estimator for a generalized version of the inverse covariance matrix. This estimator is used in a convex algorithm for robust subspace recovery (i.e.  robust PCA). Our model assumes a sub-Gaussian underlying distribution and an i.i.d.~sample from it. Our main result shows with high probability that the norm of the difference between the generalized inverse covariance of the underlying distribution and its estimator from an i.i.d.~sample of size $N$ is of order $O(N^{-0.5+\eps})$ for arbitrarily small $\eps>0$ (affecting the probabilistic estimate); this rate of convergence is close to one of direct covariance and inverse covariance estimation  i.e.  $O(N^{-0.5})$. Our precise probabilistic estimate implies for some natural settings that the sample complexity of the generalized inverse covariance estimation when using the Frobenius norm is $O(D^{2+\delta})$ for arbitrarily small $\delta>0$ (whereas the sample complexity of direct covariance estimation with Frobenius norm is $O(D^{2})$). These results provide similar rates of convergence and sample complexity for the corresponding robust subspace recovery algorithm  which are close to those of PCA. To the best of our knowledge  this is the only work analyzing the sample complexity of any robust PCA algorithm.,On the Sample Complexity of Robust PCA

Department of Electrical Engineering and Computer Science

Massachusetts Institute of Technology

Matthew Coudron

Cambridge  MA 02139
mcoudron@mit.edu

Gilad Lerman

School of Mathematics
University of Minnesota
Minneapolis  MN 55455

lerman@umn.edu

Abstract

We estimate the rate of convergence and sample complexity of a recent robust es-
timator for a generalized version of the inverse covariance matrix. This estimator
is used in a convex algorithm for robust subspace recovery (i.e.  robust PCA). Our
model assumes a sub-Gaussian underlying distribution and an i.i.d. sample from
it. Our main result shows with high probability that the norm of the difference
between the generalized inverse covariance of the underlying distribution and its
estimator from an i.i.d. sample of size N is of order O(N−0.5+) for arbitrar-
ily small  > 0 (affecting the probabilistic estimate); this rate of convergence is
close to the one of direct covariance estimation  i.e.  O(N−0.5). Our precise prob-
abilistic estimate implies for some natural settings that the sample complexity
of the generalized inverse covariance estimation when using the Frobenius norm
is O(D2+δ) for arbitrarily small δ > 0 (whereas the sample complexity of di-
rect covariance estimation with Frobenius norm is O(D2)). These results provide
similar rates of convergence and sample complexity for the corresponding robust
subspace recovery algorithm. To the best of our knowledge  this is the only work
analyzing the sample complexity of any robust PCA algorithm.

1

Introduction

A fundamental problem in probability and statistics is to determine with overwhelming probability
the rate of convergence of the empirical covariance (or inverse covariance) of an i.i.d. sample of
increasing size N to the covariance (or inverse covariance) of the underlying random variable (see
e.g.  [17  3] and references therein). Clearly  this problem is also closely related to estimating
with high probability the sample complexity  that is  the number of samples required to obtain a
given error of approximation . In the case of a compactly supported (or even more generally sub-
Gaussian) underlying distribution  it is a classical exercise to show that this rate of convergence is
O(N−0.5) (with a comparability constant depending on properties of µ  in particular D  as well as
on the threshold probability  see e.g.  [17  Proposition 2.1]). The precise estimate for this rate of
convergence implies that the sample complexity of covariance estimation is O(D) when using the
spectral norm and O(D2) when using the Frobenius norm. The rate of convergence and sample
complexity of PCA immediately follow from these estimates (see e.g.  [15]).
While such estimates are theoretically fundamental  they can be completely useless in the presence
of outliers. That is  direct covariance or inverse covariance estimation and its resulting PCA are very
sensitive to outliers. Many robust versions of covariance estimation  PCA and dimension reduction
have been developed in the last three decades (see e.g.  the standard textbooks [8  10  14]). In the last
few years new convex algorithms with provable guarantees have been suggested for robust subspace
recovery and its corresponding dimension reduction [5  4  19  20  11  7  2  1  21  9].
Most of these works minimize a mixture of an (cid:96)1-type norm (depending on the application) and the
nuclear norm. Their algorithmic complexity is not as competitive as PCA and their sample com-

1

plexity is hard to estimate due to the problem of extending the nuclear norm out-of-sample. On
the other hand  Zhang and Lerman [21] have proposed a novel M-estimator for robust PCA  which
is based on a convex relaxation of the sum of Euclidean distances to subspaces (which is origi-
nally minimized over the non-convex Grassmannian). This procedure suggests an estimator for a
generalized version of the inverse covariance matrix and uses it to robustly recover an underlying
low-dimensional subspace. This idea was extended in [9] to obtain an even more accurate method
for subspace recovery  though it does not estimate the generalized inverse covariance matrix (in par-
ticular  it has no analogous notion of singular values or their inverses). The algorithmic complexity
of the algorithms solving the convex formulations of [21] and [9] is comparable to that of full PCA.
Here we show that for the setting of sub-Gaussian distributions the sample complexity of the robust
PCA algorithm in [21] (or its generalized inverse covariance estimation) is close to that of PCA (or
to sample covariance estimation). Our analysis immediately extends to the robust PCA algorithm of
[9].

1.1 The Generalized Inverse Covariance and its Corresponding Robust PCA

Zhang and Lerman [21] formed the set

(1.1)
as a convex relaxation for the orthoprojectors (from RD to RD)  and deﬁned the following energy
function on H (with respect to a data set X in RD):

H := {Q ∈ RD×D : Q = QT   tr(Q) = 1} 

(cid:88)

x∈X

FX (Q) :=

(cid:107)Qx(cid:107) 

(1.2)

where (cid:107) · (cid:107) denotes the Euclidean norm of a vector in RD. Their generalized empirical inverse
covariance is

FX (Q).

ˆQX = arg min

(1.3)
They showed that when replacing the term (cid:107)Qx(cid:107) by (cid:107)Qx(cid:107)2 in (1.2) and when Sp{X} = RD 
then the minimization (1.3) results in a scaled version of the empirical inverse covariance matrix.
It is thus clear why we can refer to ˆQX as a generalized empirical inverse covariance (or (cid:96)1-type
version of it). We describe the absolute notion of generalized inverse covariance matrix  i.e.  non-
empirical  in §1.2. Zhang and Lerman [21] did not emphasize the empirical generalized inverse
covariance  but the robust estimate of the underlying low-dimensional subspace by the span of the
bottom eigenvectors of this matrix. They rigorously proved that such a procedure robustly recovers
the underlying subspace under some conditions.

Q∈H

1.2 Main Result of this Paper

We focus on computing the sample complexity of the estimator ˆQX . This problem is practically
equivalent with estimating the rate of convergence of ˆQX of an i.i.d. sample X to the “generalized
inverse covariance” of the underlying distribution µ. We may assume that µ is a sub-Gaussian
probability measure on RD (see §2.1 and the extended version of this paper). However  in order
to easily express the dependence of our probabilistic estimates on properties of the measure µ  we
assume for simplicity that µ is compactly supported and denote by Rµ the minimal radius among all
balls containing the support of µ  that is 

Rµ = min{r > 0 : supp(µ) ⊆ B(0  r)} 

where B(0  r) is the ball around the origin 0 with radius r. We further assume that for some
0 < γ < 1  µ satisﬁes the following condition  which we refer to as the “two-subspaces criterion”
(for γ): For any pair of (D − 1)-dimensional subspaces of RD  L1 and L2:

µ((L1 ∪ L2)c) ≥ γ.

(1.4)

We note that if µ satisﬁes the two-subspaces criterion for any particular 0 < γ < 1  then its support
cannot be a union of two hyperplanes of RD. The use of this assumption is clariﬁed below in §3.2 
though it is possible that one may weaken it.

2

(cid:18)

P

ˆQ & ˆQN are u.d. and (cid:107) ˆQ − ˆQN(cid:107)F ≤ 2
α0

≥ 1 − C0N D2

exp

where

(cid:19)
(cid:19)

N− 1

2 +

(cid:18) −N 2
(cid:32)

D · R2

µ

(cid:19)2

(cid:18) N

D − 1

− 2

(1 − γ)N−2(D−1) 

(1.8)

(cid:33) D(D+1)

2

.

(1.9)

We ﬁrst formulate the generalized inverse covariance of the underlying measure as follows:

where

F (Q) =

(cid:107)Qx(cid:107) dµ(x).

ˆQ = arg min

Q∈H F (Q) 

(cid:90)

(1.5)

(1.6)

Let {xi}∞
bution µ). Let XN := {xi}N

i=1 and denote

i=1 be a sequence of i.i.d. random variables sampled from µ (i.e.  each variable has distri-

ˆQN := ˆQXN and FN := FXN .

(1.7)

Our main result shows with high probability that ˆQ and ˆQN are uniquely deﬁned (which we denote
by u.d. from now on) and that { ˆQN}N∈N converges to ˆQ in the following speciﬁed rate. It uses the
common notation: a ∨ b := max(a  b). We explain its implications in §2.
Theorem 1.1. If µ is a compactly supported distribution satisfying the two-subspaces criterion for
γ > 0  then there exists a constant α0 ≡ α0(µ  D  ) > 0 such that for any  > 0 and N > 2(D−1)
the following estimate holds:

C0 ≡ C0(α0  D) := 4 · ((4α0) ∨ 2) ·

10 D

2α0 + 4((4α0) ∨ 2)Rµ

1 − 2α0
(4α0)∨2

Intuitively  α0 represents a lower bound on the directional second derivatives of F . Therefore 
α0 should affect sample complexity because the number of random samples taken to approximate a
minimum of F should be affected by how sharply F increases about its minimum. It is an interesting
and important open problem to ﬁnd lower bounds on α0 for general µ.

2

Implication and Extensions of the Main Result

2.1 Generalization to Sub-Gaussian Measures

We can remove the assumption that the support of µ is bounded (with radius Rµ) and assume instead
that µ is sub-Gaussian. In this case  instead of Hoeffding’s inequality  we apply [18  Proposition
5.10] with ai = 1 for all 1 ≤ i ≤ n. When formulating the corresponding inequality  one may
note that supp≥1 p−1/2(Eµ|x|p)1/p (where x represents a random variable sampled from µ) can be
regarded as a substitute for Rµ (see [21] for more details of a similar analysis).

2.2 Sample Complexity

The notion of sample complexity arises in the framework of Probably-Approximately-Correct
Learning of Valiant [16]. Generally speaking  the sample complexity in our setting is the mini-
mum number of samples N required  as a function of dimension D  to achieve a good estimation
of ˆQ with high probability. We recall that in this paper we use the Frobenius norm for the estima-
tion error. The following calculation will show that under some assumptions on µ it sufﬁces to use
N = Ω(Dη) samples for any η > 2 (we recall that f (x) = Ω(g(x)) as x → ∞ if and only if
√
g(x) = O(f (x))). In our analysis we will have to assume that γ is a ﬁxed constant  and α0 goes
D. These assumptions are placing additional restrictions on the measure µ  which we expect
as 1/
to be reasonable in practice as we later clarify. We further assume that Rµ = O(D−0.5) and also
explain later why it makes sense for the setting of robust subspace recovery.

3

To bound the sample complexity we set C1 := 4 · ((4α0) ∨ 2) and C2 := 10 · (2α0 + 4((4α0) ∨
2)Rµ)/(1−2α0/(4α0) ∨ 2) so that C0 ≤ C1·(C2·D)D2 (see (1.9)). Applying this bound and (1.8)
we obtain that if η > 2 is ﬁxed  1/η <  < 1

P

2 and N ≥ Dη  then
N− 1

(cid:18)
(cid:19)
(cid:18) −N 2
≥ 1 − C1 exp(cid:0)log(C2 · D1+η)D2 − D2η(cid:1)

≥ 1 − C1(C2 · D · N )D2

ˆQ & ˆQN are u.d. and (cid:107) ˆQ − ˆQN(cid:107)F ≤ 2
α0
− 2 N 2(D−1)(1 − γ)N−2(D−1)

D · R2

µ

(cid:19)

exp

2 +

− 2 exp (2η(D − 1) log(D) + log(1 − γ)(Dη − 2(D − 1))) .

(2.1)

2 + ≤ Dη(− 1

Since  > 1/η the ﬁrst term in the RHS of (2.1) decays exponentially as a function of D (or 
equivalently  as a function of N ≥ Dη). Similarly  since 0 < γ < 1 and η > 1 the second term in
the RHS of (2.1) decays exponentially as a function of D. Furthermore  since  < 1
2 it follows that
the error term for the minimizer  i.e.  N− 1
2 )  decays polynomially in D. Thus  in order
to achieve low error estimation with high probability it is sufﬁcient to take N = Ω(Dη) samples for
any η > 2. The exact guarantees on error estimation and probability of error can be manipulated by
changing the constant hidden in the Ω term.
We would like to point out the expected tradeoff between the sample complexity and the rate of
convergence. If  approaches 0  then the rate of convergence becomes optimal but the sample com-
plexity deteriorates. On the other hand  if  approaches 0.5  then the sample complexity becomes
optimal  but the rate of convergence deteriorates.
To motivate our assumption on Rµ  γ and α0  we recall the needle-haystack and syringe-haystack
models of [9] as a prototype for robust subspace recovery. These models assume a mixtures of outlier
and inliers components. The distribution of the outliers component is normal N (0  (σout2/D)ID)
and the distribution of the inliers component is a mixture of N (0  (σin2/d)PL) (where L is a d-
subspace) and N (0  (σin2/(CD))ID)  where C (cid:29) 1 (the latter component has coefﬁcient zero in
the needle-haystack model).
The underlying distribution of the syringe-haystack (or needle-haystack) model is not compactly
supported  but clearly sub-Gaussian (as discussed in §2.1) and its standard deviation is of order
O(D−0.5). We also note that γ here is the coefﬁcient of the outlier component in the needle-haystack
model  which we denote by ν0. Indeed  the only non-zero measure that can be contained in a (D-1)-
√
dimensional subspace is the measure associated with N (0  (σin2/d)PL)  and that has total weight
at most (1 − ν0). It is also possible to verify explicitly that α0 is lower bounded by 1/
D in this
case (though our argument is currently rather lengthy and will appear in the extended version of this
paper).

2.3 From Generalized Covariances to Subspace Recovery

We recall that the underlying d-dimensional subspace can be recovered from the bottom d eigen-
vectors of ˆQN . Therefore  the rate of convergence of the subspace recovery (or its corresponding
sample complexity) follows directly from Theorem 1.1 and the Davis-Kahan Theorem [6]. To for-
mulate this  we assume here for simplicity that ˆQ and ˆQN are u.d. (recall Theorems 3.1 and 3.2).
Theorem 2.1. If d < D   > 0  α0 ≡ α0(µ  D  ) is the positive constant guaranteed by Theo-
rem 2.1  ˆQ and ˆQN are u.d. and ˆLd  ˆLd N are the subspaces spanned by the bottom d eigenvectors
(i.e.  with lowest d eigenvalues) of ˆQ and ˆQN respectively  P ˆLd
are the orthoprojectors
on these subspaces and νD−d is the (D − d)th eigengap of ˆQ  then

and P ˆLd N

P

(cid:107)P ˆLd

− P ˆLd N

(cid:107)F ≤

4

α0 · νD−d

N− 1

2 +

≥ 1 − C0N D2

exp

.

(2.2)

(cid:19)

(cid:18) −N 2

D · R2

µ

(cid:18)

(cid:19)

2.4 Nontrivial Robustness to Noise

We remark that (2.2) implies nontrivial robustness to noise for robust PCA. Indeed  assume for ex-
ample an underlying d-subspace L∗
d and a mixture distribution (representing noisy inliers/outliers

4

components) whose inliers component is symmetric around L∗
d with relatively high level of variance
in the orthogonal component of ˆLd and its outliers component is spherically symmetric with sufﬁ-
ciently small mixture coefﬁcient. One can show that in this case ˆLd = L∗
d. Combining this observa-
tion and (2.2)  we can verify robustness to nontrivial noise when recovering L∗
d from i.i.d. samples
of such distributions.

2.5 Convergence Rate of the REAPER Estimator

G := {Q ∈ RD×D : Q = QT   tr(Q) = D − d and Q (cid:52) I} 

The REAPER and S-REAPER Algorithms [9] are variants of the robust PCA algorithm of [21]. The
objective of the REAPER algorithm can be formulated as aiming to minimize the energy FX (Q)
over the set
(2.3)
where (cid:52) denotes the semi-deﬁnite order. The d-dimensional subspace can then be recovered by
the bottom d eigenvectors of Q (in [9] this minimization is formulated with P = I − Q  whose
top d eigenvectors are found). The rate of convergence of the minimizer of FX (Q) over G to the
minimizer of F (Q) over G is similar to that in Theorem 1.1. The proof of Theorem 1.1 must be
modiﬁed to deal with the boundary of the set G. If the minimizer ˆQ lies on the interior of G then the
proof is the same. If ˆQ is on the boundary of G we must only consider the directional derivatives
which point towards the interior of G  or tangent to the boundary. Other than that the proof is the
same.

2.6 Convergence Rate with Additional Sparsity Term

Rothman et al. [13] and Ravikumar et al. [12] have analyzed an estimator for sparse inverse covari-
ance. This estimator minimizes over all Q (cid:31) 0 the energy

(cid:104)Q  (cid:98)ΣN(cid:105)F − log det(Q) + λN(cid:107)Q(cid:107)(cid:96)1 
where (cid:98)ΣN is the empirical covariance matrix based on sample of size N  (cid:104)·  ·(cid:105)F is the Frobenius
inner product (i.e.  sum of elementwise products) and (cid:107)Q(cid:107)(cid:96)1 =(cid:80)D

i j=1 |Qi j|.

(2.4)

Zhang and Zou [22] have suggested a similar minimization  which replaces the ﬁrst two terms
in (2.4) (corresponding to λN = 0) with

(cid:104)Q2  (cid:98)ΣN(cid:105)F /2 − tr(Q).

(2.5)
N (assuming that the

Sp({xi}N

Indeed  the minimizers of (2.4) when λN = 0 and of (2.5) are both equal to (cid:98)Σ−1
Using the deﬁnition of (cid:98)ΣN   i.e.  (cid:98)ΣN =(cid:80)N
(cid:104)Q2  (cid:98)ΣN(cid:105)F =

i=1) = RD so that the inverse empirical covariance exists).
i /N  we note that

(cid:107)Qxi(cid:107)2.

N(cid:88)

i=1 xixT

1
N

(2.6)
Therefore  the minimizer of (2.5) over all Q (cid:31) 0 is the same up to a multiplicative constant as
the minimizer of the RHS of (2.6) over all Q (cid:31) 0 with tr(Q) = 1. Teng Zhang suggested to us
replacing the RHS of (2.6) with FX and modifying the original problem of (2.4) (or more precisely
its variant in [22]) with the minimization over all Q ∈ H of the energy

i=1

FX (Q) + λN(cid:107)Q(cid:107)(cid:96)1.

(2.7)

The second term enforces sparseness and we expect the ﬁrst term to enforce robustness.
By choosing λN = O(N−0.5) we can obtain similar rates of convergence for the minimizer of (2.7)
as the one when λN = 0 (see extended version of this paper)  namely  rate of convergence of order
O(N−0.5+) for any  > 0. The dependence on D is also the same. That is  the minimum sample
size when using the Frobenius norm is O(Dη) for any η > 2. Nevertheless  Ravikumar et al. [12]
show that under some assumptions (see e.g.  Assumption 1 in [12])  the minimal sample size is
O(log(D)r2)  where r is the maximum node degree for a graph  whose edges are the nonzero entries
of the inverse covariance. It will be interesting to generalize such estimates to the minimization
of (2.7).

5

3 Overview of the Proof of Theorem 1.1

3.1 Structure of the Proof

We ﬁrst discuss in §3.2 conditions for uniqueness of ˆQ and ˆQN (with high probability). In §3.3 and
§3.4 we explain in short the two basic components of the proof of Theorem 1.1. The ﬁrst of them is
that (cid:107) ˆQ − ˆQN(cid:107)F can be controlled from above by differences of directional derivatives of F . The
second component is that the rate of convergence of the derivatives of {FN}∞
N =1 to the derivative
of F is easily obtained by Hoeffding’s inequality. In §3.5 we gain some intuition for the validity
of Theorem 1.1 in view of these two components and also explain why they are not sufﬁcient to
conclude the proof. In §3.6 we describe the construction of “nets” of increasing precision; using
these nets we conclude the proof of Theorem 1.1 in §3.7. Throughout this section we only provide
the global ideas of the proof  whereas in the extended version of this paper we present the details.

3.2 Uniqueness of the Minimizers

The two-subspaces criterion for µ guarantees that ˆQ is u.d. and that ˆQN is u.d. with overwhelming
probability for sufﬁciently large N as follows.
Theorem 3.1. If µ satisﬁes the two-subspaces criterion for some γ > 0  then F is strictly convex.
Theorem 3.2. If µ satisﬁes the two-subspaces criterion for some γ > 0 and N > 2(D − 1)  then

P (FN is not strictly convex) ≤ 2

(1 − γ)N−2(D−1).

(3.1)

(cid:19)2

(cid:18) N

D − 1

3.3 From Energy Minimizers to Directional Derivatives of Energies
We control the difference (cid:107)Q − ˆQ(cid:107)F from above by differences of derivatives of energies at Q and
ˆQ. Here Q is an arbitrary matrix in Br( ˆQ) for some r > 0 (where Br( ˆQ) is the ball in H with
center ˆQ and radius r w.r.t. the Frobenius norm)  but we will later apply it with Q = ˆQN for some
N ∈ N.

3.3.1 Preliminary Notation and Deﬁnitions

The “directions” of the derivatives  which we deﬁne below  are elements in the unit sphere of the
tangent space of H  i.e. 

D := {D ∈ RD×D | D = DT   tr(D) = 0 (cid:107)D(cid:107)F = 1}.

Throughout the paper  directions in D are often determined by particular points Q1  Q2 ∈ H  where
Q1 (cid:54)= Q2. We denote the direction from Q1 to Q2 by DQ1 Q2  that is 

DQ1 Q2 :=

Q2 − Q1
(cid:107)Q2 − Q1(cid:107)F

.

(3.2)

Directional derivatives with respect to an element of D may not exist and therefore we use directional
derivatives from the right. That is  for Q ∈ H and D ∈ D  the directional derivative (from the right)
of F at Q in the direction D is

F (Q + tD)(cid:12)(cid:12)t=0+ .

∇+
DF (Q) :=

d
dt

3.3.2 Mathematical Statement
We use the above notation to formulate the desired bound on (cid:107)Q − ˆQ(cid:107)F . It involves the constant
α0  which is also used in Theorem 1.1. The proof of this lemma clariﬁes the existence of α0  though
it does not suggest an explicit approximation for it.
Lemma 3.3. For r > 0 there exists a constant α0 ≡ α0(r  µ  D) > 0 such that for all Q ∈
Br( ˆQ) \ { ˆQ}:

∇+
D ˆQ Q

F (Q) − ∇+

D ˆQ Q

F ( ˆQ) ≥ α0(cid:107)Q − ˆQ(cid:107)F

(3.3)

(3.4)

6

and consequently

∇+
D ˆQ Q

F (Q) ≥ α0(cid:107)Q − ˆQ(cid:107)F .

(3.5)

3.4 N−1/2 Convergence of Directional Derivatives

We formulate the following convergence rate of the directional derivatives of FN from the right:
Theorem 3.4. For Q ∈ H and D ∈ D 
DF (Q) − ∇+

DFN (Q)(cid:12)(cid:12) ≥ N − 1

(cid:17) ≤ 2 exp

P(cid:16)(cid:12)(cid:12)∇+

(cid:18) −N 2

(cid:19)

.

2

(3.6)

D · R2

µ

It will be desirable to replace ∇+
DF (Q)  though it is impossible
in general. We will later use the following observation to implicitly obtain a result in this direction.
Lemma 3.5. If Q ∈ H \ { ˆQ}  then

DFN (Q) in (3.6) with ∇+

DF (Q)−∇+

∇+
D ˆQ Q

F (Q) ≥ 0.

(3.7)

3.5 An Incomplete Idea for Proving Theorem 1.1

At this point we can outline the basic intuition behind the proof of Theorem 1.1. We assume for
simplicity that ˆQN is u.d. Suppose  for the moment  that we could use (3.6) of Theorem 3.4 with
Q := ˆQN . This is actually not mathematically sound  as we will discuss shortly  but if we could do
it then we would have from (3.6) that
F ( ˆQN ) − ∇+

(cid:17) ≤ 2 exp

FN ( ˆQN )| ≥ N − 1

(cid:18) −N 2

P(cid:16)|∇+

(cid:19)

(3.8)

.

2

D ˆQ  ˆQN

D ˆQ  ˆQN

D · R2

µ

We note that (3.7) as well as both the convexity of FN and the deﬁnition of ˆQN imply that

F ( ˆQN ) ≥ 0 and ∇+

FN ( ˆQN ) ≤ 0.

D ˆQ  ˆQN

(cid:17) ≤ 2 exp

(cid:18) −N 2

(cid:19)

D · R2

µ

(3.9)

.

(3.10)

F ( ˆQN ) ≥ N − 1

2

Combining (3.8) and (3.9)  we obtain that

∇+
D ˆQ  ˆQN

P(cid:16)∇+

D ˆQ  ˆQN

At last  combining (3.5)  (3.10) and Theorem 3.2 we can formally prove Theorem 1.1.
However  as mentioned above  we cannot legally use Theorem 3.4 with Q = ˆQN . This is because
ˆQN is a function of the samples (random variables) {xi}N
i=1  but for our proof to be valid  Q needs
to be ﬁxed before the sampling begins.
Therefore  our new goal is to utilize the intuition described above  but modify the proof to make
it mathematically sound. This is accomplished by creating a series of “nets” (subsets of H) of
increasing precision. Each matrix in each of the nets is determined before the sampling begins  so
it can be used in Theorem 3.4. However  the construction of the nets guarantees that the Nth net
contains a matrix Q which is sufﬁciently close to ˆQN to be used as a substitute for ˆQN in the above
process.

3.6 The Missing Component: Adaptive Nets

We describe here a result on the existence of a sequence of nets as suggested in §3.5. They are
constructed in several stages  which cannot ﬁt in here (see careful explanation in the extended version
of this paper). We recall that B2( ˆQ) denotes a ball in H with center ˆQ and radius 2 w.r.t. the
Frobenius norm.
Lemma 3.6. Given κ ≥ 2 and τ > 0  there exists a sequence of sets {Sn}∞
n=1 such that ∀n ∈ N
2   ∃Q(cid:48) ∈ Sn with
Sn ⊂ B2( ˆQ) and for any Q ∈ B2( ˆQ) with (cid:107)Q − ˆQ(cid:107)F > n− 1

(cid:107)Q(cid:48) − ˆQ(cid:107)F ≤ (cid:107)Q − ˆQ(cid:107)F  

(3.11)

7

2 (τ + κ−1) ≥ (cid:107)Q(cid:48) − Q(cid:107)F ≥ n− 1

2n− 1
(cid:107)D ˆQ Q(cid:48) − D ˆQ Q(cid:107)F ≤ τ n−1 .

2 κ−1 and

Furthermore 

|Sn| ≤ 2κn

1
2

(cid:18) 10Dn

(cid:19) D(D+1)

2

.

τ

The following lemma shows that we can use SN to guarantee good approximation of ˆQ by ˆQN as
long as the differences of partial derivatives are well-controlled for elements of SN (it uses the ﬁxed
constants κ and τ for SN ; see Lemma 3.6).
Lemma 3.7. If for some  > 0  FN is strictly convex and

(cid:12)(cid:12)(cid:12)∇+

F (Q) − ∇+

DQ  ˆQ

DQ  ˆQ

FN (Q)

2 + ∀Q ∈ SN  

(cid:12)(cid:12)(cid:12) ≤ N− 1

then ˆQN is u.d. and

(cid:107) ˆQ − ˆQN(cid:107)F ≤ 1 + 2α0(τ + 1
α0

κ ) + 4Rµκτ

N− 1

2 +.

(3.12)
(3.13)

(3.14)

(3.15)

(3.16)

(3.17)

(3.18)

(3.19)

(3.20)

3.7 Completing the Proof of Theorem 1.1
Let us ﬁx κ0 = (4α0) ∨ 2  τ0 := (1 − 2α0/κ0)/(2α0 + 4Rµκ0) and N > 2(D − 1). We note that

We rewrite (3.14) using κ := κ0 and τ := τ0 and then bound its RHS from above as follows

1 + 2α0(τ0 +

1
κ0

) + 4Rµκ0τ0 = 2.

(cid:32)

|SN| ≤ 2((4α0) ∨ 2)N

D2+D+1

2

10D

2α0 + 4Rµ((4α0) ∨ 2)

1 − 2α0
(4α0)∨2

(cid:33) D(D+1)

2

≤ C0
2

N D2

.

Combining (3.6) (applied to any Q ∈ SN ) and (3.18) we obtain that

F (Q) − ∇+

DQ  ˆQ

DQ  ˆQ

FN (Q)

(cid:12)(cid:12)(cid:12)∇+

P(cid:16)∃Q ∈ SN with
P(cid:16)(cid:12)(cid:12)(cid:12)∇+

Furthermore  (3.1) and (3.19) imply that

F (Q) − ∇+

DQ  ˆQ

≥ 1 − C0N D2

exp

FN (Q)

(cid:18) −N 2

DQ  ˆQ

(cid:19)

D · R2

µ

− 2

D − 1

≤ C0N D2

(cid:12)(cid:12)(cid:12) ≥ N− 1
2 +(cid:17)
exp(cid:0)−N 2/(D · R2
µ)(cid:1) .
(cid:12)(cid:12)(cid:12) ≤ N− 1
(cid:17)
(cid:18) N
(cid:19)2

2 + ∀Q ∈ SN and ˆQN is u.d.

(1 − γ)N−2(D−1).

Theorem 1.1 clearly concludes from Lemma 3.7 (applied with κ := κ0 and τ := τ0)  (3.20)
and (3.17).

Acknowledgment

This work was supported by NSF grants DMS-09-15064 and DMS-09-56072. Part of this work was
performed when M. Coudron attended the University of Minnesota (as an undergraduate student).
We thank T. Zhang for valuable conversations and forwarding us [22].

8

References
[1] A. Agarwal  S. Negahban  and M. Wainwright. Fast global convergence of gradient methods

for high-dimensional statistical recovery. Technical Report arXiv:1104.4824  Apr 2011.

[2] A. Agarwal  S. Negahban  and M. Wainwright. Noisy matrix decomposition via convex relax-

ation: Optimal rates in high dimensions. In ICML  pages 1129–1136  2011.

[3] T. T. Cai  C.-H. Zhang  and H. H. Zhou. Optimal rates of convergence for covariance matrix

estimation. Ann. Statist.  38(4):2118–2144  2010.

[4] E. J. Cand`es  X. Li  Y. Ma  and J. Wright. Robust principal component analysis? J. ACM 

58(3):11  2011.

[5] V. Chandrasekaran  S. Sanghavi  P. A. Parrilo  and A. S. Willsky. Rank-sparsity incoherence

for matrix decomposition. Arxiv  02139:1–24  2009.

[6] C. Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii. SIAM J. on

Numerical Analysis  7:1–46  1970.

[7] D. Hsu  S. Kakade  and T. Zhang. Robust matrix decomposition with sparse corruptions.

Information Theory  IEEE Transactions on  57(11):7221 –7234  nov. 2011.

[8] P. J. Huber and E. Ronchetti. Robust statistics. Wiley series in probability and mathematical

statistics. Probability and mathematical statistics. Wiley  2009.

[9] G. Lerman  M. McCoy  J. A. Tropp  and T. Zhang. Robust computation of linear models  or

How to ﬁnd a needle in a haystack. ArXiv e-prints  Feb. 2012.

[10] R. A. Maronna  R. D. Martin  and V. J. Yohai. Robust statistics: Theory and methods. Wiley

Series in Probability and Statistics. John Wiley & Sons Ltd.  Chichester  2006.

[11] M. McCoy and J. Tropp. Two proposals for robust PCA using semideﬁnite programming. Elec.

J. Stat.  5:1123–1160  2011.

[12] P. Ravikumar  M. J. Wainwright  G. Raskutti  and B. Yu. High-dimensional covariance esti-
mation by minimizing (cid:96)1-penalized log-determinant divergence. Electron. J. Stat.  5:935–980 
2011.

[13] A. J. Rothman  P. J. Bickel  E. Levina  and J. Zhu. Sparse permutation invariant covariance

estimation. Electron. J. Stat.  2:494–515  2008.

[14] P. J. Rousseeuw and A. M. Leroy. Robust regression and outlier detection. Wiley Series in
Probability and Mathematical Statistics: Applied Probability and Statistics. John Wiley & Sons
Inc.  New York  1987.

[15] J. Shawe-taylor  C. Williams  N. Cristianini  and J. Kandola. On the eigenspectrum of the
Gram matrix and the generalisation error of kernel PCA. IEEE Transactions on Information
Theory  51(1):2510–2522  2005.

[16] L. G. Valiant. A theory of the learnable. Commun. ACM  27(11):1134–1142  Nov. 1984.
[17] R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix? to

appear.

[18] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. C. Eldar
and G. Kutyniok  editors  Compressed Sensing: Theory and Applications. Cambridge Univ
Press  to appear.

[19] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. In NIPS  pages 2496–

2504  2010.

[20] H. Xu  C. Caramanis  and S. Sanghavi. Robust pca via outlier pursuit. Information Theory 

IEEE Transactions on  PP(99):1  2012.

[21] T. Zhang and G. Lerman. A novel m-estimator for robust pca. Submitted  available at

arXiv:1112.4863.

[22] T. Zhang and H. Zou. Sparse precision matrix estimation via positive deﬁnite constrained

minimization of (cid:96)1 penalized d-trace loss. Personal Communication  2012.

9

,Chaitanya Ryali
Gautam Reddy
Angela Yu