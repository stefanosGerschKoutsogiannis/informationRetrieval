2019,Manifold denoising by Nonlinear Robust Principal Component Analysis,This paper extends robust principal component analysis (RPCA) to nonlinear manifolds. Suppose that the observed data matrix is the sum of a sparse component and a component drawn from some low dimensional manifold. Is it possible to separate them by using similar ideas as RPCA? Is there any benefit in treating the manifold as a whole as opposed to treating each local region independently? We answer these two questions affirmatively by proposing and analyzing an optimization framework that separates the sparse component from the manifold under noisy data. Theoretical error bounds are provided when the tangent spaces of the manifold satisfy certain incoherence conditions. We also provide a near optimal choice of the tuning parameters for the proposed optimization formulation with the help of a new curvature estimation method. The efficacy of our method is demonstrated on both synthetic and real datasets.,Manifold Denoising by Nonlinear Robust Principal

Component Analysis

He Lyu  Ningyu Sha  Shuyang Qin  Ming Yan  Yuying Xie  Rongrong Wang

Department of Computational Mathematics  Science and Engineering

Michigan State University

{lyuhe shaningy qinshuya myan xyy wangron6}@msu.edu

Abstract

This paper extends robust principal component analysis (RPCA) to nonlinear mani-
folds. Suppose that the observed data matrix is the sum of a sparse component and
a component drawn from some low dimensional manifold. Is it possible to separate
them by using similar ideas as RPCA? Is there any beneﬁt in treating the manifold
as a whole as opposed to treating each local region independently? We answer
these two questions afﬁrmatively by proposing and analyzing an optimization
framework that separates the sparse component from the manifold under noisy data.
Theoretical error bounds are provided when the tangent spaces of the manifold
satisfy certain incoherence conditions. We also provide a near optimal choice of
the tuning parameters for the proposed optimization formulation with the help of a
new curvature estimation method. The efﬁcacy of our method is demonstrated on
both synthetic and real datasets.

1

Introduction

Manifold learning and graph learning are nowadays widely used in computer vision  image processing 
and biological data analysis on tasks such as classiﬁcation  anomaly detection  data interpolation 
and denoising. In most applications  graphs are learned from the high dimensional data and used
to facilitate traditional data analysis methods such as PCA  Fourier analysis  and data clustering
[7  8  9  15  12]. However  the quality of the learned graph may be greatly jeopardized by outliers
which cause instabilities in all the aforementioned graph assisted applications.
In recent years  several methods have been proposed to handle outliers in nonlinear data [11  21  3].
Despite the success of those methods  they only aim at detecting the outliers instead of correcting them.
In addition  very few of them are equipped with theoretical analysis of the statistical performances.
In this paper  we propose a novel non-task-driven algorithm for the mixed noise model in (1) and
provide theoretical guarantees to control its estimation error. Speciﬁcally  we consider the mixed
noise model as

i = 1  . . .   n 

˜Xi = Xi + Si + Ei 

(1)
where Xi ∈ Rp is the noiseless data independently drawn from some manifold M with an intrinsic
dimension d (cid:28) p  Ei is the i.i.d. Gaussian noise with small magnitudes  and Si is the sparse
noise with possibly large magnitudes. If Si has a large entry  then the corresponding ˜Xi is usually
considered as an outlier. The goal of this paper is to simultaneously recover Xi and Si from ˜Xi 
i = 1  ..  n.
There are several beneﬁts in recovering the noise term Si along with the signal Xi. First  the support
of Si indicates the locations of the anomaly  which is informative in many applications. For example 
if Xi is the gene expression data from the ith patient  the nonzero elements in Si indicate the
differentially expressed genes that are the candidates for personalized medicine. Similarly  if Si is a

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

result of malfunctioned hardware  its nonzero elements indicate the locations of the malfunctioned
parts. Secondly  the recovery of Si allows the “outliers” to be pulled back to the data manifold instead
of simply being discarded. This prevents a waste of information and is especially beneﬁcial in cases
where data is insufﬁcient. Thirdly  in some applications  the sparse Si is a part of the clean data rather
than a noise term  then the algorithm provides a natural decomposition of the data into a sparse and a
non-sparse component that may carry different pieces of information.
Along a similar line of research  Robust Principle Component Analysis (RPCA) [2] has received
considerable attention and has demonstrated its success in separating data from sparse noise in many
applications. However  its assumption that the data lies in a low dimensional subspace is somewhat
strict. In this paper  we generalize the Robust PCA idea to the non-linear manifold setting. The major
new components in our algorithm are: 1) an incorporation of the manifold curvature information into
the optimization framework  and 2) a uniﬁed way to apply RPCA to a collection of tangent spaces of
the manifold.

2 Methodology
Let ˜X = [ ˜X1  . . .   ˜Xn] ∈ Rp×n be the noisy data matrix containing n samples. Each sample is a
vector in Rp independently drawn from (1). The overall data matrix ˜X has the representation

˜X = X + S + E

where X is the clean data matrix  S is the matrix of the sparse noise  and E is the matrix of the
Gaussian noise. We further assume that the clean data X lies on some manifold M embedded in Rp
with a small intrinsic dimension d (cid:28) p and the samples are sufﬁcient (n ≥ p). The small intrinsic
dimension assumption ensures that data is locally low dimensional so that the corresponding local
data matrix is of low rank. This property allows the data to be separated from the sparse noise.
The key idea behind our method is to handle the data locally. We use the k Nearest Neighbors (kNN)
to construct local data matrices  where k is larger than the intrinsic dimension d. For a data point
Xi ∈ Rp  we deﬁne the local patch centered at it to be the set consisted of its kNN and itself  and
a local data matrix X (i) associated with this patch is X (i) = [Xi1  Xi2   . . .   Xik   Xi]  where Xij is
the jth-nearest neighbor of Xi. Let Pi be the restriction operator to the ith patch  i.e.  Pi(X) = XPi
where Pi is the n× (k + 1) matrix that selects the columns of X in the ith patch. Then X (i) = Pi(X).
Similarly  we deﬁne S(i) = Pi(S)  E(i) = Pi(E) and ˜X (i) = Pi( ˜X).
Since each local data matrix X (i) is nearly of low rank and S is sparse  we can decompose the noisy
data matrix into low-rank parts and sparse parts through solving the following optimization problem
{ ˆS { ˆS(i)}n

F (S {S(i)}n

i=1} = arg min

i=1 { ˆL(i)}n

S S(i) L(i)

≡ arg min

S S(i) L(i)

n(cid:88)

i=1

i=1 {L(i)}n

i=1)

(cid:0)λi(cid:107) ˜X (i) − L(i) − S(i)(cid:107)2

F + (cid:107)C(L(i))(cid:107)∗ + β(cid:107)S(i)(cid:107)1

(cid:1)

subject to S(i) = Pi(S) 

(2)
here we take β = max{k + 1  p}−1/2 as in RPCA  ˜X (i) = Pi( ˜X) is the local data matrix on the
ith patch and C is the centering operator that subtracts the column mean: C(Z) = Z(I − 1
k+1 11T ) 
where 1 is the (k + 1)-dimensional column vector of all ones. Here we are decomposing the data
on each patch into a low-rank part L(i) and a sparse part S(i) by imposing the nuclear norm and
entry-wise (cid:96)1 norm on L(i) and S(i)  respectively. There are two key components in this formulation:
1). the local patches are overlapping (for example  the ﬁrst data point X1 may belong to several
patches). Thus  the constraint S(i) = Pi(S) is particularly important because it ensures copies of
the same point on different patches (and those of the sparse noise on different patches) remain the
same. 2). we do not require L(i) to be restrictions of a universal L to the ith patch  because the L(i)s
correspond to the local afﬁne tangent spaces  and there is no reason for a point on the manifold to
have the same projection on different tangent spaces. This seemingly subtle difference has a large
impact on the ﬁnal result.
If the data only contains sparse noise  i.e.  E = 0  then ˆX ≡ ˜X − ˆS is the ﬁnal estimation for X.
If E (cid:54)= 0  we apply Singular Value Hard Thresholding [6] to truncate C( ˜X (i) − Pi(S)) and remove

2

the Gaussian noise (See §6)  and use the resulting ˆL(i)
squares ﬁtting

τ∗ to construct a ﬁnal estimate ˆX of X via least

λi(cid:107)Pi(Z) − ˆL(i)
τ∗(cid:107)2
F .

(3)

n(cid:88)

i=1

ˆX = arg min
Z∈Rp×n

The following discussion revolves around (2) and (3)  and the structure of the paper is as follows. In
§3  we explain the geometric meaning of each term in (2). In §4  we establish theoretical recovery
guarantees for (2) which justiﬁes our choice of β and allows us to theoretically choose λ. The
calculation of λ uses the curvature of the manifold  so in §5  we provide a simple method to estimate
the average manifold curvature and the method is robust to sparse noise. The optimization algorithms
that solve (2) and (3) are presented in §6 and the numerical experiments are in §7.

3 Geometric explanation

We provide a geometric intuition for the formulation (2). Let us write the clean data matrix X (i) on
the ith patch in its Taylor expansion along the manifold 

X (i) = Xi1T + T (i) + R(i) 

(4)
where the Taylor series is expanded at Xi (the center point of the ith patch)  T (i) stores the ﬁrst order
term and its columns lie in the tangent space of the manifold at Xi  and R(i) contains all the higher
order terms. The sum of the ﬁrst two terms Xi1T + T (i) is the linear approximation to X (i) that is
unknown if the tangent space is not given. This linear approximation precisely corresponds to the
L(i)s in (2)  i.e.  L(i) = Xi1T + T (i). Since the tangent space has the same dimensionality d as the
manifold  with randomly chosen points  we have with probability one  that rank(T (i)) = d. As a
result  rank(L(i)) = rank(Xi1T + T (i)) ≤ d + 1. By the assumption that d < min{p  k}  we know
that L(i) is indeed low rank.
Combing (4) with ˜X (i) = X (i) + S(i) + E(i)  we ﬁnd the misﬁt term ˜X (i) − L(i) − S(i) in (2)
equals E(i) + R(i). This implies that the misﬁt contains the high order residues (i.e.  the linear
approximation error) and the Gaussian noise.

4 Theoretical choice of tuning parameters

To establish the error bound  we need a coherence condition on the tangent spaces of the manifold.
Deﬁnition 4.1 Let U ∈ Rm×r (m ≥ r) be a matrix with U∗U = I  the coherence of U is deﬁned as

µ(U ) =

m
r

max

k∈{1 ... m}

(cid:107)U∗ek(cid:107)2
2 

where ek is the kth element of the canonical basis. For a subspace T   its coherence is deﬁned as

µ(V ) =

m
r

max

k∈{1 ... m}

(cid:107)V ∗ek(cid:107)2
2 

where V is an orthonormal basis of T . The coherence is independent of the choice of basis.

The following theorem is proved for local patches constructed using the -neighborhoods. We use
kNN in the experiments because kNN is more robust to insufﬁcient samples. The full version of
Theorem 4.2 can be found in the supplementary material.
Theorem 4.2 [succinct version] Let each Xi ∈ Rp  i = 1  ...  n  be independently drawn from a
compact manifold M ⊆ Rp with an intrinsic dimension d and endowed with the uniform distribution.
Let Xij   j = 1  . . .   ki be the ki points falling in an η-neighborhood of Xi with radius η  where η > 0
is some ﬁxed small constant. These points form the matrix X (i) = [Xi1   . . .   Xiki
  Xi]. For any
q ∈ M  let Tq be the tangent space of M at q and deﬁne ¯µ = supq∈M µ(Tq). Suppose the support
of the noise matrix S(i) is uniformly distributed among all sets of cardinality mi. Then as long as
d < ρr min{k  p}¯µ−1 log
−2 max{¯k  p}  and mi ≤ 0.4ρspk (here ρr and ρs are positive constants 

3

λi =

has the error bound

min{ki + 1  p}1/2

i

(cid:88)

i

(cid:107)Pi( ˆS) − S(i)(cid:107)2 1 ≤ C

√

pn¯k(cid:107)(cid:107)2.

¯k = maxi ki  and k = mini ki)   then with probability over 1− c1n max{k  p}−10 − e−c2k for some
constants c1 and c2  the minimizer ˆS to (2) with weights

βi = max{ki + 1  p}−1/2

 

(5)

Here i = (cid:107) ˜X (i) − Xi1T − T (i) − S(i)(cid:107)F will be estimated in the next section   = [1  ...  n] 
(cid:107) · (cid:107)2 1 stands for taking (cid:96)2 norm along columns and (cid:96)1 norm along rows  and T (i) is the projection
of X (i) − Xi1T to the tangent space TXi.
Remark. We can interpret  as the total noise in the data. As explained in §3  (cid:107) ˜X (i) − Xi1T − T (i) −
S(i)(cid:107)F = (cid:107)R(i) + E(i)(cid:107)F   thus  = 0 if the manifold is linear and the Gaussian noise is absent. The
n in front of (cid:107)(cid:107)2 takes into account the use of different norms on the two hand sides (the
factor
right hand side is the Frobenius norm of the noise matrix {R(i) + E(i)}n
i=1 obtained by stacking the
√
p is due to the small weight
R(i) + E(i) associated with each patch into one big matrix). The factor
βi of (cid:107)S(i)(cid:107)1 compared to the weight 1 on (cid:107) ˜X (i) − L(i) − S(i)(cid:107)2
F . The factor ¯k appears because on
average  each column of ˆS − S is added about k := 1

i ki times on the left hand side.

(cid:80)

√

n

5 Estimating the curvature

F ≡
The deﬁnition λi in (5) involves an unknown quantity 2
(cid:107)R(i) + E(i)(cid:107)2
F . We assume the standard deviation σ of the i.i.d. Gaussian entries of E(i) is known 
F can be approximated. Since R(i) is independent of E(i)  the cross term (cid:104)R(i)  E(i)(cid:105) is
so (cid:107)E(i)(cid:107)2
small. Our main task is estimating (cid:107)R(i)(cid:107)2
F   the linear approximation error deﬁned in §3. At local
regions  second order terms dominates the linear approximation residue  hence estimating (cid:107)R(i)(cid:107)2
requires the curvature information.

i = (cid:107) ˜X (i) − Xi1T − T (i) − S(i)(cid:107)2

F

5.1 A short review of related concepts in Riemannian geometry

The principal curvatures at a point on a high dimensional manifold are deﬁned as the singular values
of the second fundamental forms [10]. As estimating all the singular values from the noisy data may
not be stable  we are only interested in estimating the mean curvature  that is the root mean squares
of the principal curvatures.
For the simplicity of illustration  we review the
related concepts using the 2D surface M embed-
ded in R3 (Figure 1). For any curve γ(s) in M
parametrized by arclength with unit tangent vec-
tor tγ(s)  its curvature is the norm of the covari-
ant derivative of tγ: (cid:107)dtγ(s)/ds(cid:107) = (cid:107)γ(cid:48)(cid:48)(s)(cid:107). In
particular  we have the following decomposition

γ(cid:48)(cid:48)(s) = kg(s)ˆv(s) + kn(s)ˆn(s) 

where ˆn(s) is the unit normal direction of the
manifold at γ(s) and ˆv is the direction perpendic-
ular to ˆn(s) and tγ(s)  i.e.  ˆv = ˆn × tγ(s). The
coefﬁcient kn(s) along the normal direction is
called the normal curvature  and the coefﬁcient kg(s) along the perpendicular direction ˆv is called the
geodesic curvature. The principal curvatures purely depend on kn. In particular  in 2D  the principal
curvatures are precisely the maximum and minimum of kn among all possible directions.
A natural way to compute the normal curvature is through geodesic curves. The geodesic curve
between two points is the shortest curve connecting them. Therefore geodesic curves are usually
viewed as “straight lines” on the manifold. The geodesic curves have the favorable property that their
curvatures have 0 contribution from kg. That is to say  the second order derivative of the geodesic
curve parameterized by the arclength is exactly kn.

Figure 1: Local manifold geometry

4

Algorithm 1: Estimate the mean curvature ¯Γ(p) at some point p
Input: Distance matrix D  adjacency matrix A  some proper constants r1 < r2  number of pairs m
Output: the estimated mean curvature ¯Γ(p)

Randomly pick some point qi ∈ B(p  r2)\B(p  r1);
Calculate the geodesic distance dg(p  qi) using A;
Solve for the radius Ri based on (7);

1 for i = 1: m do
2
3
4
5 end
6 Compute estimated curvature ¯Γ(p) = ( 1
m

(cid:80)m
i=1 R−2

i

)1/2.

Algorithm 2: Estimate the overall curvature ¯Γ(Ω) for some region Ω
Input: Distance matrix D  adjacency matrix A  some proper constants r1 < r2  number of pairs m
Output: the estimated overall curvature ¯Γ(Ω)

Randomly pick a pair of points pi  qi ∈ Ω such that r1 ≤ d(pi  qi) ≤ r2 ;
Calculate the geodesic distance dg(pi  qi) using A;
Solve for the radius Ri based on (7);

1 for i = 1: m do
2
3
4
5 end
6 Compute estimated curvature ¯Γ(Ω) = ( 1
m

(cid:80)m
i=1 R−2

)1/2.

i

5.2 The proposed method

All existing curvature estimation methods we are aware of are in the ﬁeld of computer vision where
the objects are 2D surfaces in 3D [5  4  19  14]. Most of these methods are difﬁcult to generalize to
high (> 3) dimensions with the exception of the integral invariant based approaches [17]. However 
the integral invariant based approaches is not robust to sparse noise and is unsuited to our problem.
We propose a new method to estimate the mean curvature from the noisy data. Although the graphic
illustration is made in 3D  the method is dimension independent. To compute the average normal
curvature at a point p ∈ M  we randomly pick m points qi ∈ M on the manifold lying within a
proper distance to p as speciﬁed in Algorithm 1. Let γi be the geodesic curve between p and qi. For
each i  we compute the pairwise Euclidean distance (cid:107)p − qi(cid:107)2 and the pairwise geodesic distance
dg(p  qi) using the Dijkstra’s algorithm. Through a circular approximation of the geodesic curve as
drawn in Figure 1  we can compute the curvature of the geodesic curve as the inverse of the radius
(6)
is the radius of the
through the

where γ(cid:48)
circular approximation to the curve γ at p  which can be solved along with the angle θγ(cid:48)
geometric relations

i is the tangent direction along which the curvature is calculated and Rγ(cid:48)

i (p)(cid:107) = 1/Rγ(cid:48)

(cid:107)γ(cid:48)(cid:48)

 

i

i

i

2Rγ(cid:48)

i

sin(θγ(cid:48)

i

/2) = (cid:107)p − qi(cid:107)2  Rγ(cid:48)

θγ(cid:48)

i

= dg(p  qi) 

(7)

i

as indicated in Figure 1. Finally  we deﬁne the average curvature ¯Γ(p) at p to be

¯Γ(p) := (Eqi(cid:107)γ(cid:48)(cid:48)

i (p)(cid:107)2)1/2 ≡ (EqiR−2

(8)
To estimate the mean curvature from the data  we construct two matrices D and A. D ∈ Rn×n is the
pairwise distance matrix  where Dij denotes the Euclidean distance between two points Xi and Xj.
A is a type of adjacency matrix deﬁned as follows and is to be used to compute the pairwise geodesic
distances from the data 

)1/2.

γi

if Xj is in the k nearest neighbors of Xi
elsewhere.

(9)

(cid:26)Dij

0

Aij =

Algorithm 1 estimates the mean curvature at some point p and Algorithm 2 estimates the overall
curvature within some region Ω on the manifold.
The geodesic distance is computed using the Dijkstra’s algorithm  which is not accurate when p and
q are too close to each other. The constant r1 in Algorithm 1 and 2 is thus used to make sure that p
and q are sufﬁciently apart. The constant r2 is to make sure that q is not too far away from p  as after
all we are computing the mean curvature around p.

5

5.3 Estimating λi from the mean curvature

We provide a way to approximate λi when the number of points n is ﬁnite. In the asymptotic limit
(k → ∞  k/n → 0)  all the approximate sign “≈” below become “=”.
Fix a point p ∈ M and another point qi in the η-neighborhood of p. Let γi be the geodesic curve
between them. With the computed curvature ¯Γ(p)  we can estimate the linear approximation error
of expanding qi at p: qi ≈ p + PTp (qi − p)  where PTp is the projection onto the tangent space at
p. Let E be the error of this linear approximation E(qi  p) = qi − p − PTp (qi − p) = PT ⊥
(qi − p)
where T ⊥
p is the orthogonal complement of the tangent space. From Figure 1  the relation between
(cid:107)E(p  qi)(cid:107)2  (cid:107)p − qi(cid:107)2  and θγ(cid:48)

is

p

θγ(cid:48)
2 =

i

(cid:107)p−qi(cid:107)2
2Rγ(cid:48)

2

(cid:107)E(p  qi)(cid:107)2 ≈ (cid:107)p − qi(cid:107)2 sin

.

(10)
To obtain a closed-form formula for E  we assume that for the ﬁxed p and a randomly chosen qi in an
ξ neighborhood of p  the projection PTp (qi − p) follows a uniform distribution in a ball with radius η(cid:48)
(in fact η(cid:48) ≈ η since when η is small  the projection of q − p is almost q − p itself  therefore the radius
of the projected ball almost equal to the radius of the original neighborhood). Under this assumption 
let ri = (cid:107)PTp (qi − p)(cid:107)2 be the magnitude of the projection and φi = PTp (qi − p)/(cid:107)PTp (qi − p)(cid:107)2
be the direction  by [20]  ri and φi are independent of each other. As the curvature Rγi only depends
on the direction  the numerator and the denominator of the right hand side of (10) are independent of
each other. Therefore 

i

i

E(cid:107)E(p  qi)(cid:107)2

2 ≈ E(cid:107)p−qi(cid:107)4
4R2
γ(cid:48)

2

i

=

E(cid:107)p−qi(cid:107)4

2

4

ER−2
γ(cid:48)

i

=

E(cid:107)p−qi(cid:107)4

2

4

· ¯Γ2(p) 

(11)

where the ﬁrst equality used the independence and the last equality used the deﬁnition of the mean
curvature in the previous subsection.
Now we apply this estimation to the neighborhood of Xi. Let p = Xi  and qj = Xij be the neighbors
of Xi. Using (11)  the average linear approximation error on this patch is
k→∞−−−−→ E(cid:107)Xi−Xij (cid:107)4

(cid:107)R(i)(cid:107)2

k(cid:80)

(12)

¯Γ2(Xi) 

2

(cid:107)E(Xij   Xi)(cid:107)2
where the right hand side can also be estimated with

F := 1
k

1
k

j=1

2

4

(cid:107)Xi − Xij(cid:107)4

2

¯Γ2(Xi) k→∞−−−−→ E(cid:107)Xi − Xij(cid:107)4

2

¯Γ2(Xi)

(13)

k(cid:88)

1
k

j=1

4

4

k(cid:80)

(cid:107)Xi−Xij (cid:107)4

2

so when k is sufﬁcient large  1

¯Γ2(Xi)  which can be
completely computed from the data. Combining this with the argument at the beginning of §5 we get 

F is also close to 1

j=1

k

4

k(cid:107)R(i)(cid:107)2

F ) ≈(cid:16)

F + (cid:107)E(i)(cid:107)2

(k + 1)pσ2 +

k(cid:88)

(cid:107)Xi − Xij(cid:107)4

2

j=1

4

(cid:17)1/2

¯Γ2(Xi)

=: ˆ.

due to (5). We show in the supplementary material that

i = (cid:107)R(i)+E(i)(cid:107)F ≈(cid:113)(cid:107)R(i)(cid:107)2
(cid:12)(cid:12)(cid:12) ˆλi−λ∗
(cid:12)(cid:12)(cid:12) k→∞−−−−→ 0  where λ∗

Thus we can set ˆλi = min{k+1 p}1/2

λ∗

ˆi

i

i

i = min{k+1 p}1/2

i

as in (5).

6 Optimization algorithm

To solve the convex optimization problem (2) in a memory-economic way  we ﬁrst write L(i) as a
function of S and eliminate them from the problem. We can do so by ﬁxing S and minimizing the
objective function with respect to L(i)

ˆL(i) = arg min

L(i)

= arg min

L(i)

λi(cid:107) ˜X (i) − L(i) − S(i)(cid:107)2
λi(cid:107)C(L(i)) − C( ˜X (i) − S(i))(cid:107)2

F + (cid:107)C(L(i))(cid:107)∗

F + (cid:107)C(L(i))(cid:107)∗ + λi(cid:107)(I − C)(L(i) − ( ˜X (i) − S(i)))(cid:107)2
F .

6

(14)

Notice that L(i) can be decomposed as L(i) = C(L(i)) + (I − C)(L(i))  set A = C(L(i))  B =
(I − C)(L(i))  then (14) is equivalent to

( ˆA  ˆB) = arg min

A B

λi(cid:107)A − C( ˜X (i) − S(i))(cid:107)2

F + (cid:107)A(cid:107)∗ + λi(cid:107)B − (I − C)( ˜X∗(i) − S(i)))(cid:107)2
F  

which decouples into
ˆA = arg min

A

λi(cid:107)A − C( ˜X (i) − S(i))(cid:107)2

F + (cid:107)A(cid:107)∗  ˆB = arg min

λi(cid:107)B − (I − C)( ˜X (i) − S(i))(cid:107)2
F .

B

The problems above have closed form solutions

ˆA = T1/2λi(C( ˜X (i) − Pi(S)))  ˆB = (I − C)( ˜X (i) − Pi(S))

where Tµ is the soft-thresholding operator on the singular values

Tµ(Z) = U max{Σ − µI  0}V ∗  where U ΣV ∗ is the SVD of Z.

Combing ˆA and ˆB  we have derived the closed form solution for ˆL(i)

ˆL(i)(S) = T1/2λi(C( ˜X (i) − Pi(S))) + (I − C)( ˜X (i) − Pi(S)).

(15)

(16)

Plugging (16) into F in (2)  the resulting optimization problem solely depends on S. Then we apply
FISTA [1  18] to ﬁnd the optimal solution ˆS with
ˆS = arg min

(17)
Once ˆS is found  if the data has no Gaussian noise  then the ﬁnal estimation for X is ˆX ≡ ˜X − ˆS; if
there is Gaussian noise  we use the following denoised local patches ˆL(i)
τ∗

F ( ˆL(i)(S)  S).

S

τ∗ = Hτ∗ (C( ˜X (i) − Pi( ˆS))) + (I − C)( ˜X (i) − Pi( ˆS)) 
ˆL(i)

(18)
where Hτ∗ is the Singular Value Hard Thresholding Operator with the optimal threshold as deﬁned
in [6]. This optimal thresholding removes the Gaussian noise from ˆL(i)
τ∗   we
solve (3) to obtain the denoised data

τ∗ . With the denoised ˆL(i)

ˆX = (

λi ˆL(i)

τ∗ P T

i )(

λiPiP T

i )−1.

(19)

The proposed Nonlinear Robust Principle Component Analysis (NRPCA) algorithm is summarized
in Algorithm 3. There is one caveat in solving (2): the strong sparse noise may result in a wrong

i=1

i=1

n(cid:88)

n(cid:88)

Algorithm 3: Nonlinear Robust PCA
Input: Noisy data matrix ˜X  k (number of neighbors in each local patch)  T (number of

neighborhood updates iterations)

Output: the denoised data ˆX  the estimated sparse noise ˆS

1 Estimate the curvature using (8);
2 Estimate λi  i = 1  . . .   n as in §5  set β as in (2);
3 ˆS ← 0;
4 for iter = 1: T do
5
6

Find the kNN for each point using ˜X − ˆS and construct the restriction operators {Pi}n
Construct the local data matrices ˜X (i) = Pi( ˜X) using Pi and the noisy data ˜X;
ˆS ← minimizer of (17) iteratively using FISTA;

i=1;

7
8 end
9 Compute each ˆL(i)

τ∗ from (18) and assign ˆX from (19).

neighborhood assignment when constructing the local patches. Therefore  once ˆS is obtained and
removed from the data  we update the neighborhood assignment and re-compute ˆS. This procedure is
repeated T times.

7

7 Numerical experiment

Simulated Swiss roll: We demonstrate the superior performance of NRPCA on a synthetic dataset
following the mixed noise model (1). We sampled 2000 noiseless data Xi uniformly from a 3D Swiss
roll and generated the Gaussian noise matrix with i.i.d. entries obeying N (0  0.25). The sparse noise
matrix S is generated by randomly replacing 100 entries of a zero p × n matrix with i.i.d. samples
generated from (−1)y · z where y ∼ Bernoulli(0.5) and z ∼ N (5  0.09). We applied NRPCA to the
simulated data with patch size k = 15. Figure 2 reports the denoising results in the original space
(3D) looking down from above. We compare two ways of using the outputs of NRPCA: 1). only
remove the sparse noise from the data ˜X − ˆS; 2). remove both the sparse and Gaussian noise from
the data: ˆX. In addition  we plotted ˜X − ˆS with and without the neighbourhood update. These results
are all superior to an ad-hoc application of the Robust PCA on the individual local patches.

Figure 2: NRPCA applied to the noisy 3D Swiss roll dataset. ˜X − ˆS is the result after subtracting
the sparse noise estimated by setting T = 1 in NRPCA  i.e.  no neighbour update; “ ˜X − ˆS with one
neighbor update” used the ˆS obtained by setting T = 2 in NRPCA; clearly  the neighbour update
helped to remove more sparse noise; ˆX is the data obtained via ﬁtting the denoised tangent spaces as
in (3). Compared to“ ˜X − ˆS with one neighbor update”  it further removed the Gaussian noise from
the data; ”Patch-wise Robust PCA” refers to the ad-hoc application of the vanilla Robust PCA to each
local patch independently  whose performance is worse than the proposed joint-recovery formulation.

The MNIST datasest: We observed some interesting dimension reduction result of MNIST with the
help of NRPCA. It is well-known that the handwritten digits 4 and 9 are so similar that the popular
dimension reduction methods Isomap and Laplacian Eigenmaps fail to separate them into two clusters
(ﬁrst column of Figure 3). We conjecture that the similarity between the two clusters is caused by
personalized writing styles of the beginning and ﬁnishing strokes. As this type of variation can be
better modeled by sparse noise than Gaussian or Poisson noises  we applied NRPCA to the raw
MNIST images. The right column of Figure 3 shows that after the NRPCA denoising (with k = 11) 
the separability of the two clusters in the ﬁrst two coordinates of Isomap and Laplacian Eigenmaps
increases. In addition  these new embeddings seem to suggest that some trajectory patterns exist in
the data. We provide additional plots in the supplementary material to support this observation.
Biological data: We illustrate the potential usefulness of NRPCA algorithm on an embryoid body
(EB) differentiation dataset over a 27-day time course  which consists of gene expressions for 31 000
cells measured with single-cell RNA-sequencing technology (scRNAseq) [13  16]. This EB data
comprising expression measurement for cells originated from embryoid at different stages is hence
developmental in nature  which should exhibit a progressive type of characters such as tree structure
because all cells arise from a single oocyte and then develop into different highly-differentiated
tissues. This progression character is often missing when we directly apply dimension reduction

8

Figure 3: Laplacian eigenmaps and Isomap results for the original and the NRPCA denoised digits 4
and 9 from the MNIST dataset.

methods to the data as shown in Figure 4 because biological data including scRNAseq is highly noisy
and often is contaminated with outliers from different sources including environmental effects and
measurement error. In this case  we aim to reveal the progressive nature of the single-cell data from
transcript abundance as measured by scRNAseq.
We ﬁrst normalized the scRNAseq data following the procedure described in [16] and randomly
selected 1000 cells using the stratiﬁed sampling framework to maintain the ratios among different
developmental stages. We applied our NRPCA method to the normalized subset of EB data and
then applied Locally Linear Embedding (LLE) to the denoised results. The two-dimensional LLE
results are shown in Figure 4. Our analysis demonstrated that although LLE is unable to show the
progression structure using noisy data  after the NRPCA denoising  LLE successfully extracted the
trajectory structure in the data  which reﬂects the underlying smooth differentiating processes of
embryonic cells. Interestingly  using the denoised data from ˜X − ˆS with neighbor update  the LLE
embedding showed a branching at around day 9 and increased variance in later time points  which
was conﬁrmed by manual analysis using 80 biomarkers in [16].

Figure 4: LLE results for denoised scRNAseq data set.

8 Conclusion

In this paper  we proposed the ﬁrst outlier correction method for nonlinear data analysis that can
correct outliers caused by the addition of large sparse noise. The method is a generalization of the
Robust PCA method to the nonlinear setting. We provided procedures to treat the non-linearity
by working with overlapping local patches of the data manifold and incorporating the curvature
information into the denoising algorithm. We established a theoretical error bound on the denoised
data that holds under conditions only depending on the intrinsic properties of the manifold. We tested
our method on both synthetic and real dataset that were known to have nonlinear structures and
reported promising results.

9

-0.01-0.008-0.006-0.004-0.00200.0020.0040.0060.0080.01Laplacian1-0.01-0.00500.0050.010.015Laplacian2Original  Laplacian-0.01-0.00500.0050.010.015Laplacian1-0.015-0.01-0.00500.0050.010.015Laplacian2Denoised Laplacian-20-15-10-50510152025Isomap1-15-10-505101520Isomap2Original Isomap-15-10-5051015Isomap1-10-50510Isomap2Denoised IsomapAcknowledgements The authors would like to thank Shuai Yuan  Hongbo Lu  Changxiong Liu 
Jonathan Fleck  Yichen Lou  and Lijun Cheng for useful discussions. This work was supported
in part by the NIH grants U01DE029255  5RO3DE027399 and the NSF grants DMS-1902906 
DMS-1621798  DMS-1715178  CCF-1909523 and NCS-1630982.

References
[1] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear

inverse problems. SIAM journal on imaging sciences  2(1):183–202  2009.

[2] Emmanuel J. Candes  Xiaodong Li  Yi Ma  and John Wright. Robust Principal Component

Analysis? J. ACM  58(3):11:1–11:37  June 2011.

[3] Chun Du  Jixiang Sun  Shilin Zhou  and Jingjing Zhao. An Outlier Detection Method for Robust
Manifold Learning. In Zhixiang Yin  Linqiang Pan  and Xianwen Fang  editors  Proceedings of
The Eighth International Conference on Bio-Inspired Computing: Theories and Applications
(BIC-TA)  2013  Advances in Intelligent Systems and Computing  pages 353–360. Springer
Berlin Heidelberg  2013.

[4] Sagi Eppel. Using curvature to distinguish between surface reﬂections and vessel contents
in computer vision based recognition of materials in transparent vessels. arXiv preprint
arXiv:1602.00177  2006.

[5] Patrick J Flynn and Anil K Jain. On reliable curvature estimation. Computer Vision and Pattern

Recognition  89:110–116  1989.

√

3. IEEE

[6] M. Gavish and D. L. Donoho. The optimal hard threshold for singular values is 4/

Transactions on Information Theory  60(8):5040–5053  Aug 2014.

[7] David K. Hammond  Pierre Vandergheynst  and Rémi Gribonval. Wavelets on graphs via
spectral graph theory. Applied and Computational Harmonic Analysis  30(2):129–150  March
2011.

[8] Jianbo Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on

Pattern Analysis and Machine Intelligence  22(8):888–905  August 2000.

[9] Bo. Jiang  Chris. Ding  Bin Luo  and Jin. Tang. Graph-Laplacian PCA: Closed-Form Solution
and Robustness. In 2013 IEEE Conference on Computer Vision and Pattern Recognition  pages
3492–3498  June 2013.

[10] Shoshichi Kobayashi and Katsumi Nomizu. Foundations of differential geometry. 2  1996.

[11] Xiang-Ru Li  Xiao-Ming Li  Hai-Ling Li  and Mao-Yong Cao. Rejecting Outliers Based on

Correspondence Manifold. Acta Automatica Sinica  35(1):17–22  January 2009.

[12] Anna Little  Yuying Xie  and Qiang Sun. An analysis of classical multidimensional scaling.

arXiv preprint arXiv:1812.11954  2018.

[13] G. R. Martin and M. J. Evans. Differentiation of clonal lines of teratocarcinoma cells: formation
of embryoid bodies in vitro. Proceedings of the National Academy of Sciences  72(4):1441–1445 
April 1975.

[14] Dereck S. Meek and Desmond J. Walton. On surface normal and gaussian curvature approx-
imations given data sampled from a smooth surface. Computer Aided Geometric Design 
17(6):521–543  2000.

[15] Marina Meila and Jianbo Shi. Learning Segmentation by Random Walks. In T. K. Leen  T. G.
Dietterich  and V. Tresp  editors  Advances in Neural Information Processing Systems 13  pages
873–879. MIT Press  2001.

[16] Kevin Moon  David van Dijk  Zheng Wang  Scott Gigante  Daniel B. Burkhardt  William S.
Chen  Kristina Yim  Antonia van den Elzen  Matthew J. Hirn  Ronald R. Coifman  Natalia B.
Ivanova  Guy Wolf  and Smita Krishnaswamy. Visualizing Structure and Transitions for
Biological Data Exploration. bioRxiv  page 120378  April 2019.

10

[17] Helmut Pottmann  Johannes Wallner  Yong-Liang Yang  Yu-Kun Lai  and Shi-Min Hu. Principal
curvatures from the integral invariant viewpoint. Computer Aided Geometric Design  24(8):428–
442  2007.

[18] Ningyu Sha  Ming Yan  and Youzuo Lin. Efﬁcient seismic denoising techniques using robust
principal component analysis. In SEG Technical Program Expanded Abstracts 2019  pages
2543–2547. Society of Exploration Geophysicists  2019.

[19] Wai-Shun Tong and Chi-Keung Tang. Robust estimation of adaptive tensors of curvature by
tensor voting. Pattern Analysis and Machine Intelligence  IEEE Transactions on  27(3):434–449 
2005.

[20] Roman Vershynin. High-dimensional probability: An introduction with applications in data

science  volume 47. Cambridge University Press  2018.

[21] Zhigang Tang  Jun Yang  and Bingru Yang. A new Outlier detection algorithm based on
Manifold Learning. In 2010 Chinese Control and Decision Conference  pages 452–457  May
2010.

11

,He Lyu
Ningyu Sha
Shuyang Qin
Ming Yan
Yuying Xie
Rongrong Wang