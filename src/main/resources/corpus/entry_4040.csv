2012,Online L1-Dictionary Learning with Application to Novel Document Detection,Given their pervasive use  social media  such as Twitter  have become a leading source of breaking news. A key task in the automated identification of such news is the detection of novel documents from a voluminous stream of text documents in a scalable manner. Motivated by this challenge  we introduce the problem of online L1-dictionary learning where unlike traditional dictionary learning  which uses squared loss  the L1-penalty is used for measuring the reconstruction error. We present an efficient online algorithm for this problem based on alternating directions method of multipliers  and establish a sublinear regret bound for this algorithm. Empirical results on news-stream and Twitter data  shows that this online L1-dictionary learning algorithm for novel document detection gives more than an order of magnitude speedup over the previously known batch algorithm  without any significant loss in quality of results. Our algorithm for online L1-dictionary learning could be of independent interest.,Online (cid:96)1-Dictionary Learning with Application to

Novel Document Detection

Shiva Prasad Kasiviswanathan∗
General Electric Global Research

kasivisw@gmail.com

Huahua Wang†

University of Minnesota
huwang@cs.umn.edu

Arindam Banerjee†
University of Minnesota

banerjee@cs.umn.edu

Prem Melville

IBM T.J. Watson Research Center

pmelvil@us.ibm.com

Abstract

Given their pervasive use  social media  such as Twitter  have become a leading
source of breaking news. A key task in the automated identiﬁcation of such news
is the detection of novel documents from a voluminous stream of text documents
in a scalable manner. Motivated by this challenge  we introduce the problem of
online (cid:96)1-dictionary learning where unlike traditional dictionary learning  which
uses squared loss  the (cid:96)1-penalty is used for measuring the reconstruction error.
We present an efﬁcient online algorithm for this problem based on alternating
directions method of multipliers  and establish a sublinear regret bound for this
algorithm. Empirical results on news-stream and Twitter data  shows that this
online (cid:96)1-dictionary learning algorithm for novel document detection gives more
than an order of magnitude speedup over the previously known batch algorithm 
without any signiﬁcant loss in quality of results.

1

Introduction

The high volume and velocity of social media  such as blogs and Twitter  have propelled them to
the forefront as sources of breaking news. On Twitter  it is possible to ﬁnd the latest updates on
diverse topics  from natural disasters to celebrity deaths; and identifying such emerging topics has
many practical applications  such as in marketing  disease control  and national security [14]. The
key challenge in automatic detection of breaking news  is being able to detect novel documents in
a stream of text; where a document is considered novel if it is “unlike” documents seen in the past.
Recently  this has been made possible by dictionary learning  which has emerged as a powerful data
representation framework. In dictionary learning each data point y is represented as a sparse linear
combination Ax of dictionary atoms  where A is the dictionary and x is a sparse vector [1  12]. A
dictionary learning approach can be easily converted into a novel document detection method: let A
be a dictionary representing all documents till time t− 1  for a new data document y arriving at time
t  if one does not ﬁnd a sparse combination x of the dictionary atoms  and the best reconstruction
Ax yields a large loss  then y clearly is not well represented by the dictionary A  and is hence novel
compared to documents in the past. At the end of timestep t  the dictionary is updated to represent
all the documents till time t.
Kasiviswanathan et al. [10] presented such a (batch) dictionary learning approach for detecting novel
documents/topics. They used an (cid:96)1-penalty on the reconstruction error (instead of squared loss com-

∗Part of this wok was done while the author was a postdoc at the IBM T.J. Watson Research Center.
†H. Wang and A. Banerjee was supported in part by NSF CAREER grant IIS-0953274  NSF grants IIS-

0916750  1029711  IIS-0812183  and NASA grant NNX12AQ39A.

1

monly used in the dictionary learning literature) as the (cid:96)1-penalty has been found to be more effective
for text analysis (see Section 3). They also showed this approach outperforms other techniques  such
as a nearest-neighbor approach popular in the related area of First Story Detection [16]. We build
upon this work  by proposing an efﬁcient algorithm for online dictionary learning with (cid:96)1-penalty.
Our online dictionary learning algorithm is based on the online alternating directions method which
was recently proposed by Wang and Banerjee [19] to solve online composite optimization problems
with additional linear equality constraints. Traditional online convex optimization methods such
as [25  8  5  6  22] require explicit computation of the subgradient making them computationally
expensive to be applied in our high volume text setting  whereas in our algorithm the subgradients
are computed implicitly. The algorithm has simple closed form updates for all steps yielding a fast
√
and scalable algorithm for updating the dictionary. Under suitable assumptions (to cope with the
non-convexity of the dictionary learning problem)  we establish an O(
T ) regret bound for the ob-
jective  matching the regret bounds of existing methods [25  5  6  22]. Using this online algorithm
for (cid:96)1-dictionary learning  we obtain an online algorithm for novel document detection  which we
empirically validate on traditional news-streams as well as streaming data from Twitter. Experi-
mental results show a substantial speedup over the batch (cid:96)1-dictionary learning based approach of
Kasiviswanathan et al. [10]  without a loss of performance in detecting novel documents.
Related Work. Online convex optimization is an area of active research and for a detailed survey
on the literature we refer the reader to [18]. Online dictionary learning was recently introduced
by Mairal et al. [12] who showed that it provides a scalable approach for handling large dynamic
datasets. They considered an (cid:96)2-penalty and showed that their online algorithm converges to the
minimum objective value in the stochastic case (i.e.  with distributional assumptions on the data).
However  the ideas proposed in [12] do not translate to the (cid:96)1-penalty. The problem of novel docu-
ment/topics detection was also addressed by a recent work of Saha et al. [17]  where they proposed a
non-negative matrix factorization based approach for capturing evolving and novel topics. However 
their algorithm operates over a sliding time window (does not have online regret guarantees) and
works only for (cid:96)2-penalty.

2 Preliminaries

its norm  (cid:107)Z(cid:107)1 = (cid:80)

F = (cid:80)

ij z2

i j |zij| and (cid:107)Z(cid:107)2

Notation. Vectors are always column vectors and are denoted by boldface letters. For a matrix Z
ij. For arbitrary real matrices the standard inner
product is deﬁned as (cid:104)Y  Z(cid:105) = Tr(Y (cid:62)Z). We use Ψmax(Z) to denote the largest eigenvalue of
Z(cid:62)Z. For a scalar r ∈ R  let sign(r) = 1 if r > 0  −1 if r < 0  and 0 if r = 0. Deﬁne
soft(r  T ) = sign(r) · max{|r| − T  0}. The operators sign and soft are extended to a matrix by
applying it to every entry in the matrix. 0m×n denotes a matrix of all zeros of size m × n and the
subscript is omitted when the dimension of the represented matrix is clear from the context.
Dictionary Learning Background. Dictionary learning is the problem of estimating a collection
of basis vectors over which a given data collection can be accurately reconstructed  often with sparse
encodings. It falls into a general category of techniques known as matrix factorization. Classic dic-
tionary learning techniques for sparse representation (see [1  15  12] and references therein) consider
a ﬁnite training set of signals P = [p1  . . .   pn] ∈ Rm×n and optimize the empirical cost function
i=1 l(pi  A)  where l(· ·) is a loss function such that l(pi  A) should
be small if A is “good” at representing the signal pi in a sparse fashion. Here  A ∈ Rm×k is referred
to as the dictionary. In this paper  we use a (cid:96)1-loss function with an (cid:96)1-regularization term  and our

which is deﬁned as f (A) =(cid:80)n

(cid:107)pi − Ax(cid:107)1 + λ(cid:107)x(cid:107)1  where λ is the regularization parameter.

l(pi  A) = min

x

We deﬁne the problem of dictionary learning as that of minimizing the empirical cost f (A). In other
words  the dictionary learning is the following optimization problem

min

A

f (A) = f (A  X)

def
= min
A X

l(pi  A) = min
A X

(cid:107)P − AX(cid:107)1 + λ(cid:107)X(cid:107)1.

n(cid:88)

i=1

For maintaining interpretability of the results  we would additionally require that the A and X ma-
trices be non-negative. To prevent A from being arbitrarily large (which would lead to arbitrarily
small values of X)  we add a scaling constant on A as follows. Let A be the convex set of matrices
deﬁned as
A = {A ∈ Rm×k : A ≥ 0m×k ∀j = 1  . . .   k  (cid:107)Aj(cid:107)1 ≤ 1}  where Aj is the jth column in A.

2

We use ΠA to denote the Euclidean projection onto the nearest point in the convex set A. The
resulting optimization problem can be written as

min

A∈A X≥0

(cid:107)P − AX(cid:107)1 + λ(cid:107)X(cid:107)1

(1)

The optimization problem (1) is in general non-convex. But if one of the variables  either A or X is
known  the objective function with respect to the other variable becomes a convex function (in fact 
can be transformed into a linear program).

3 Novel Document Detection Using Dictionary Learning

In this section  we describe the problem of novel document detection and explain how dictionary
learning could be used to tackle this problem. Our problem setup is similar to [10].
Novel Document Detection Task. We assume documents arrive in streams. Let {Pt
: Pt ∈
Rmt×nt  t = 1  2  3  . . .} denote a sequence of streaming matrices where each column of Pt repre-
sents a document arriving at time t. Here  Pt represents the term-document matrix observed at time
t. Each document is represented is some conventional vector space model such as TF-IDF [13].
The t could be at any granularity  e.g.  it could be the day that the document arrives. We use nt to
represent the number of documents arriving at time t. We normalize Pt such that each column (doc-
ument) in Pt has a unit (cid:96)1-norm. For simplicity in exposition  we will assume that mt = m for all
t.1 We use the notation P[t] to denote the term-document matrix obtained by vertically concatenating
the matrices P1  . . .   Pt  i.e.  P[t] = [P1|P2| . . .|Pt]. Let Nt be the number of documents arriving at
time ≤ t  then P[t] ∈ Rm×Nt. Under this setup  the goal of novel document detection is to identify
documents in Pt that are “dissimilar” to the documents in P[t−1].
Sparse Coding to Detect Novel Documents. Let At ∈ Rm×k represent the dictionary matrix after
time t− 1; where dictionary At is a good basis to represent of all the documents in P[t−1]. The exact
construction of the dictionary is described later. Now  consider a document y ∈ Rm appearing at
time t. We say that it admits a sparse representation over At  if y could be “well” approximated as
a linear combination of few columns from At. Modeling a vector with such a sparse decomposition
is known as sparse coding. In most practical situations it may not be possible to represent y as Atx 
e.g.  if y has new words which are absent in At. In such cases  one could represent y = Atx + e
where e is an unknown noise vector. We consider the following sparse coding formulation

(cid:107)y − Atx(cid:107)1 + λ(cid:107)x(cid:107)1.

l(y  At) = min
x≥0

(2)
The formulation (2) naturally takes into account both the reconstruction error (with the (cid:107)y − Atx(cid:107)1
term) and the complexity of the sparse decomposition (with the (cid:107)x(cid:107)1 term).
It is quite easy to
transform (2) into a linear program. Hence  it can be solved using a variety of methods. In our
experiments  we use the alternating directions method of multipliers (ADMM) [2] to solve (2).
ADMM has recently gathered signiﬁcant attention in the machine learning community due to its
wide applicability to a range of learning problems with complex objective functions [2].
We can use sparse coding to detect novel documents as follows. For each document y arriving at
time t  we do the following. First  we solve (2) to check whether y could be well approximated as a
sparse linear combination of the atoms of At. If the objective value l(y  At) is “big” then we mark
the document as novel  otherwise we mark the document as non-novel. Since  we have normalized
all documents in Pt to unit (cid:96)1-length  the objective values are in the same scale.
Choice of the Error Function. A very common choice of reconstruction error is the (cid:96)2-penalty. In
fact  in the presence of isotopic Gaussian noise the (cid:96)2-penalty on e = y − Atx gives the maximum
likelihood estimate of x [21  23]. However  for text documents  the noise vector e rarely satisﬁes the
Gaussian assumption  as some of its coefﬁcients contain large  impulsive values. For example  in
ﬁelds such as politics and sports  a certain term may become suddenly dominant in a discussion [10].
In such cases imposing an (cid:96)1-penalty on the error is a better choice than imposing an (cid:96)2-penalty
(e.g.  recent research [21  24  20] have successfully shown the superiority of (cid:96)1 over (cid:96)2 penalty for a

1As new documents come in and new terms are identiﬁed  we expand the vocabulary and zero-pad the
previous matrices so that at the current time t  all previous and current documents have a representation over
the same vocabulary space.

3

different but related application domain of face recognition). We empirically validate the superiority
of using the (cid:96)1-penalty for novel document detection in Section 5.
Size of the Dictionary. Ideally  in our application setting  changing the size of the dictionary (k)
dynamically with t would lead to a more efﬁcient and effective sparse coding. However  in our
theoretical analysis  we make the simplifying assumption that k is a constant independent of t. In
our experiments  we allow for small increases in the size of the dictionary over time when required.
Batch Algorithm for Novel Document Detection. We now describe a simple batch algorithm
(slightly modiﬁed from [10]) for detecting novel documents. The Algorithm BATCH alternates be-
tween a novel document detection and a batch dictionary learning step.

Algorithm 1 : BATCH

Input: P[t−1] ∈ Rm×Nt−1  Pt = [p1  . . .   pnt] ∈ Rm×nt  At ∈ Rm×k  λ ≥ 0  ζ ≥ 0
Novel Document Detection Step:
for j = 1 to nt do

Solve: xj = argminx≥0 (cid:107)pj − Atx(cid:107)1 + λ(cid:107)x(cid:107)1
if (cid:107)pj − Atxj(cid:107)1 + λ(cid:107)xj(cid:107)1 > ζ
Batch Dictionary Learning Step:
Set P[t] ← [P[t−1] | p1  . . .   pnt]
Solve: [At+1  X[t]] = argminA∈A X≥0 (cid:107)P[t] − AX(cid:107)1 + λ(cid:107)X(cid:107)1

Mark pj as novel

Batch Dictionary Learning. We now describe the batch dictionary learning step. At time t  the
dictionary learning step is2

[At+1  X[t]] = argminA∈A X≥0 (cid:107)P[t] − AX(cid:107)1 + λ(cid:107)X(cid:107)1.

(3)

Even though conceptually simple  Algorithm BATCH is computationally inefﬁcient. The bottleneck
comes in the dictionary learning step. As t increases  so does the size of P[t]  so solving (3) becomes
prohibitive even with efﬁcient optimization techniques. To achieve computational efﬁciency  in [10] 
the authors solved an approximation of (3) where in the dictionary learning step they only update
the A’s and not the X’s.3 This leads to faster running times  but because of the approximation  the
quality of the dictionary degrades over time and the performance of the algorithm decreases. In this
paper  we propose an online learning algorithm for (3) and show that this online algorithm is both
computationally efﬁcient and generates good quality dictionaries under reasonable assumptions.

4 Online (cid:96)1-Dictionary Learning

In this section  we introduce the online (cid:96)1-dictionary learning problem and propose an efﬁcient al-
gorithm for it. The standard goal of online learning is to design algorithms whose regret is sublinear
in time T   since this implies that “on the average” the algorithm performs as well as the best ﬁxed
strategy in hindsight [18]. Now consider the (cid:96)1-dictionary learning problem deﬁned in (3). Since
this problem is non-convex  it may not be possible to design efﬁcient (i.e.  polynomial running time)
algorithms that solves it without making any assumptions on either the dictionary (A) or the sparse
code (X). This also means that it may not be possible to design efﬁcient online algorithm with
sublinear regret without making any assumptions on either A or X because an efﬁcient online al-
gorithm with sublinear regret would imply an efﬁcient algorithm for solving (1) in the ofﬂine case.
Therefore  we focus on obtaining regret bounds for the dictionary update  assuming that the at each
timestep the sparse codes given to the batch and online algorithms are “close”. This motivates the
following problem.
Deﬁnition 4.1 (Online (cid:96)1-Dictionary Learning Problem). At time t  the online algorithm picks
ˆAt+1 ∈ A. Then  the nature (adversary) reveals (Pt+1  ˆXt+1) with Pt+1 ∈ Rm×n and ˆXt+1 ∈
2In our algorithms  it is quite straightforward to replace the condition A ∈ A by some other condition
3In particular  deﬁne (recursively) (cid:101)X[t] = [(cid:101)X[t−1] | x1  . . .   xnt ] where xj’s are coming from the novel
document detection step at time t. In [10]  the dictionary learning step is At+1 = argminA∈A (cid:107)P[t] − A(cid:101)X[t](cid:107)1.

A ∈ C  where C is some closed non-empty convex set.

4

Rk×n. The problem is to pick the ˆAt+1 sequence such that the following regret function is mini-
mized4

R(T ) =

(cid:107)Pt − ˆAt ˆXt(cid:107)1 − min
A∈A

(cid:107)Pt − AXt(cid:107)1  

T(cid:88)

t=1

T(cid:88)

t=1

where ˆXt = Xt + Et and Et is an error matrix dependent on t.

The regret deﬁned above admits the discrepancy between the sparse coding matrices supplied to the
batch and online algorithms through the error matrix. The reason for this generality is because in
our application setting  the sparse coding matrices used for updating the dictionaries of the batch
and online algorithms could be different. We will later establish the conditions on Et’s under which
we can achieve sublinear regret. All missing proofs and details appear in the full version of the
paper [11].

4.1 Online (cid:96)1-Dictionary Algorithm

In this section  we design an algorithm for the online (cid:96)1-dictionary learning problem  which we
call Online Inexact ADMM (OIADMM)5 and bound its regret. Firstly note that because of the
non-smooth (cid:96)1-norms involved it is computationally expensive to apply standard online learning
algorithms like online gradient descent [25  8]  COMID [6]  FOBOS [5]  and RDA [22]  as they
require computing a costly subgradient at every iteration. The subgradient of (cid:107)P − AX(cid:107)1 at A = ¯A
is (X · sign(X(cid:62) ¯A(cid:62) − P (cid:62)))(cid:62).
Our algorithm for online (cid:96)1-dictionary learning is based on the online alternating direction method
which was recently proposed by Wang et al. [19]. Our algorithm ﬁrst performs a simple variable
substitution by introducing an equality constraint. The update for each of the resulting variable has
a closed-form solution without the need of estimating the subgradients explicitly.

Algorithm 2 : OIADMM

Input: Pt ∈ Rm×n  ˆAt ∈ Rm×k  ∆t ∈ Rm×n  ˆXt ∈ Rk×n  βt ≥ 0  τt ≥ 0

(cid:101)Γt ←− Pt − ˆAt ˆXt
Γt+1 = argminΓ (cid:107)Γ(cid:107)1 + (cid:104)∆t (cid:101)Γt − Γ(cid:105) + (βt/2)(cid:107)(cid:101)Γt − Γ(cid:107)2
Gt+1 ←− −(∆t/βt +(cid:101)Γt − Γt+1) ˆX(cid:62)
ˆAt+1 = argminA∈A βt((cid:104)Gt+1  A − ˆAt(cid:105) + (1/2τt)(cid:107)A − ˆAt(cid:107)2
F )
∆t+1 = ∆t + βt(Pt − ˆAt+1 ˆXt − Γt+1)
Return ˆAt+1 and ∆t+1

t

(⇒ Γt+1 = soft((cid:101)Γt + ∆t/βt  1/βt))

F

(⇒ ˆAt+1 = ΠA(max{0  ˆAt − τtGt+1}))

The Algorithm OIADMM is simple. Consider the following minimization problem at time t

We can rewrite this above minimization problem as:

A∈A (cid:107)Pt − A ˆXt(cid:107)1.

min

(cid:107)Γ(cid:107)1

such that Pt − A ˆXt = Γ.

min
A∈A Γ

The augmented Lagrangian of (4) is:

L(A  Γ  ∆) = min
A∈A Γ

(cid:107)Γ(cid:107)1 + (cid:104)∆  Pt − A ˆXt − Γ(cid:105) +
where ∆ ∈ Rm×n is a multiplier and βt > 0 is a penalty parameter.

(cid:13)(cid:13)(cid:13)Pt − A ˆXt − Γ
(cid:13)(cid:13)(cid:13)2

F

βt
2

(4)

 

(5)

4For ease of presentation and analysis  we will assume that m and n don’t vary with time. One could allow

for changing m and n by carefully adjusting the size of the matrices by zero-padding.

5The reason for naming it OIADMM is because the algorithm is based on alternating directions method of

multipliers (ADMM) procedure.

5

OIADMM is summarized in Algorithm 2. The algorithm generates a sequence of iterates
{Γt  At  ∆t}∞
t=1. At each time t  instead of solving (4) completely  it only runs one step ADMM
update of the variables (Γt  At  ∆t). The complete analysis of Algorithm 2 is presented in the full
version of the paper [11]. Here  we just summarize the main result in the following theorem.
Theorem 4.2. Let {Γt  ˆAt  ∆t} be the sequences generated by the OIADMM procedure and R(T )
be the regret as deﬁned above. Assume the following conditions hold: (i) ∀t  the Frobenius norm
of ∂(cid:107)Γt(cid:107)1 is upper bounded by Φ  (ii) ˆA1 = 0m×k (cid:107)Aopt(cid:107)F ≤ D  (iii) ∆1 = 0m×n  and (iv) ∀t 
1/τt ≥ 2Ψmax( ˆXt). Setting ∀t  βt = Φ

τmT where τm = maxt {1/τt}  we have

√

R(T ) ≤ ΦD

+

(cid:107)AoptEt(cid:107)1.

D

√
T√
τm

T(cid:88)

t=1

More generally  as long as(cid:80)T

In the above theorem one could replace τm by any upper bound on it (i.e.  we don’t need to know
τm exactly).
Condition on Et’s for Sublinear Regret. In a standard online learning setting  the (Pt  ˆXt) made
available to the online learning algorithm will be the same as (Pt  Xt) made available to the batch
dictionary learning algorithm in hindsight  so that ˆXt = Xt ⇒ Et = 0  yielding a O(
T ) regret.
t=1 (cid:107)Et(cid:107)p = o(T ) for some suitable p-norm  we get a sublinear regret
bound.6 For example  if {Zt} is a sequence of matrices such that for all t  (cid:107)Zt(cid:107)p = O(1)  then
setting Et = t−Zt   > 0 yields a sublinear regret. This gives a sufﬁcient condition for sublinear
regret  and it is an interesting open problem to extend the analysis to other cases.
Running Time. For the ith column in the dictionary matrix the projection onto A can be done in
O(si log m) time where si is the number of non-zero elements in the ith column using the projec-
tion onto (cid:96)1-ball algorithm of Duchi et al. [4]. The simplest implementation of OIADMM takes
O(mnk) time at each timestep because of the matrix multiplications involved.

√

5 Experimental Results

In this section  we present experiments to compare and contrast the performance of (cid:96)1-batch and
(cid:96)1-online dictionary learning algorithms for the task of novel document detection. We also present
results highlighting the superiority of using an (cid:96)1- over an (cid:96)2-penalty on the reconstruction error for
this task (validating the discussion in Section 3).
Implementation of BATCH.
In our implementation  we grow the dictionary size by η in each
timestep. Growing the dictionary size is essential for the batch algorithm because as t increases the
number of columns of P[t] also increases  and therefore  a larger dictionary is required to compactly
represent all the documents in P[t]. For solving (3)  we use alternative minimization over the vari-
ables. The pseudo-code description is given in the full version of the paper [11]. The optimization
problems arising in the sparse coding and dictionary learning steps are solved using ADMM’s.
Online Algorithm for Novel Document Detection. Our online algorithm7 uses the same novel
document detection step as Algorithm BATCH  but dictionary learning is done using OIADMM. For
a pseudo-code description  see full version of the paper [11]. Notice that the sparse coding matrices
of the Algorithm BATCH  X1  . . .   Xt could be different from ˆX1  . . .   ˆXt. If these sequence of
matrices are close to each other  then we have a sublinear regret on the objective function.8
Evaluation of Novel Document Detection. For performance evaluation  we assume that documents
in the corpus have been manually identiﬁed with a set of topics. For simplicity  we assume that each
document is tagged with the single  most dominant topic that it associates with  which we call the
true topic of that document. We call a document y arriving at time t novel if the true topic of
y has not appeared before the time t. So at time t  given a set of documents  the task of novel
t=1 (cid:107)Et(cid:107)p) for 1 ≤
p  q ≤ ∞ and 1/p + 1/q = 1  and by the assuming (cid:107)Aopt(cid:107)q is bounded. Here  (cid:107)·(cid:107)p denotes Schatten p-norm.
7In our experiments  the number of documents introduced in each timestep is almost of the same order  and

6This follows from H¨older’s inequality which gives(cid:80)T

t=1 (cid:107)AoptEt(cid:107)1 ≤ (cid:107)Aopt(cid:107)q((cid:80)T

hence there is no need to change the size of the dictionary across timesteps for the online algorithm.

8As noted earlier  we can not do a comparison without making any assumptions.

6

document detection is to classify each document as either novel (positive) or non-novel (negative).
For evaluating this classiﬁcation task  we use the standard Area Under the ROC Curve (AUC) [13].
Performance Evaluation for (cid:96)1-Dictionary Learning. We use a simple reconstruction error mea-
sure for comparing the dictionaries produced by our (cid:96)1-batch and (cid:96)1-online algorithms. We want the
dictionary at time t to be a good basis to represent all the documents in P[t] ∈ Rm×Nt. This leads
us to deﬁne the sparse reconstruction error (SRE) of a dictionary A at time t as

(cid:19)

(cid:107)P[t] − AX(cid:107)1 + λ(cid:107)X(cid:107)1

.

(cid:18)

SRE(A)

def
=

1
Nt

min
X≥0

A dictionary with a smaller SRE is better on average at sparsely representing the documents in P[t].
Novel Document Detection using (cid:96)2-dictionary learning. To justify the choice of using an (cid:96)1-
penalty (on the reconstruction error) for novel document detection  we performed experiments com-
paring (cid:96)1- vs. (cid:96)2-penalty for this task. In the (cid:96)2-setting  for the sparse coding step we used a fast
implementation of the LARS algorithm with positivity constraints [7] and the dictionary learning
was done by solving a non-negative matrix factorization problem with additional sparsity constraints
(also known as the non-negative sparse coding problem [9]). A complete pseudo-code description
is given in the full version of the paper [11].9
Experimental Setup. All reported results are based on a Matlab implementation running on a quad-
core 2.33 GHz Intel processor with 32GB RAM. The regularization parameter λ is set to 0.1 which
yields reasonable sparsities in our experiments. OIADMM parameters τt is set 1/(2Ψmax( ˆXt))
(chosen according to Theorem 4.2) and βt is ﬁxed to 5 (obtained through tuning). The ADMM
parameters for the sparse coding and batch dictionary learning steps are set as suggested in [10]
(refer to the full version [11]). In the batch algorithms  we grow the dictionary sizes by η = 10 in
each timestep. The threshold value ζ is treated as a tunable parameter.

5.1 Experiments on News Streams

Our ﬁrst dataset is drawn from the NIST Topic Detection and Tracking (TDT2) corpus which con-
sists of news stories in the ﬁrst half of 1998. In our evaluation  we used a set of 9000 documents
represented over 19528 terms and distributed into the top 30 TDT2 human-labeled topics over a
period of 27 weeks. We introduce the documents in groups. At timestep 0  we introduce the ﬁrst
1000 documents and these documents are used for initializing the dictionary. We use an alternative
minimization procedure over the variables of (1) to initialize the dictionary. In these experiments
the size of the initial dictionary k = 200. In each subsequent timestep t ∈ {1  . . .   8}  we provide
the batch and online algorithms the same set of 1000 documents. In Figure 1  we present novel
document detection results for those timesteps where at least one novel document was introduced.
Table 1 shows the corresponding AUC numbers. The results show that using an (cid:96)1-penalty on the
reconstruction error is better for novel document detection than using an (cid:96)2-penalty.

Figure 1: ROC curves for TDT2 for timesteps where novel documents were introduced.

Comparison of the (cid:96)1-online and (cid:96)1-batch Algorithms. The (cid:96)1-online and (cid:96)1-batch algorithms
have almost identical performance in terms of detecting novel documents (see Table 1). However 
the online algorithm is much more computationally efﬁcient. In Figure 2(a)  we compare the running
times of these algorithms. As noted earlier  the running time of the batch algorithm goes up as
t increases (as it has to optimize over the entire past). However  the running time of the online
algorithm is independent of the past and only depends on the number of documents introduced
in each timestep (which in this case is always 1000). Therefore  the running time of the online

9We used the SPAMS package http://spams-devel.gforge.inria.fr/ in our implementation.

7

00.5100.51False Positive RateTrue Positive RateTimestep 1  ONLINEBATCH−IMPLL2−BATCH00.5100.51False Positive RateTrue Positive RateTimestep 2  ONLINEBATCH−IMPLL2−BATCH00.5100.51False Positive RateTrue Positive RateTimestep 5  ONLINEBATCH−IMPLL2−BATCH00.5100.51False Positive RateTrue Positive RateTimestep 6  ONLINEBATCH−IMPLL2−BATCH00.5100.51False Positive RateTrue Positive RateTimestep 8  ONLINEBATCH−IMPLL2−BATCHTimestep No. of Novel Docs. No. of Nonnovel Docs.

981
947
884
934
935

AUC (cid:96)1-online

AUC (cid:96)1-batch

AUC (cid:96)2-batch

0.791
0.694
0.732
0.881
0.757
0.771

0.815
0.704
0.764
0.898
0.760
0.788

0.674
0.586
0.601
0.816
0.701
0.676

1
2
5
6
8

Avg.

19
53
116
66
65

Table 1: AUC Numbers for ROC Plots in Figure 1.

algorithm is almost the same across different timesteps. As expected the run-time gap between the
(cid:96)1-batch and (cid:96)1-online algorithms widen as t increases – in the ﬁrst timestep the online algorithm is
5.4 times faster  and this rapidly increases to a factor of 11.5 in just 7 timesteps.
In Figure 2(b)  we compare the dictionaries produced by the (cid:96)1-batch and (cid:96)1-online algorithms
under the SRE metric. In the ﬁrst few timesteps  the SRE of the dictionaries produced by the online
algorithm is slightly lower than that of the batch algorithm. However  this gets corrected after a few
timesteps and as expected later on the batch algorithm produces better dictionaries.

(a)

(b)

(c)

(d)

Figure 2: Running time and SRE plots for TDT2 and Twitter datasets.

5.2 Experiments on Twitter

Our second dataset is from an application of monitoring Twitter for Marketing and PR for smart-
phone and wireless providers. We used the Twitter Decahose to collect a 10% sample of all tweets
(posts) from Sept 15 to Oct 05  2011. From this  we ﬁltered the tweets relevant to “Smartphones”
using a scheme presented in [3] which utilizes the Wikipedia ontology to do the ﬁltering. Our dataset
comprises of 127760 tweets over these 21 days and the vocabulary size is 6237 words. We used the
tweets from Sept 15 to 21 (34292 in number) to initialize the dictionaries. Subsequently  at each
timestep  we give as input to both the algorithms all the tweets from a given day (for a period of 14
days between Sept 22 to Oct 05). Since this dataset is unlabeled  we do a quantitative evaluation of
(cid:96)1-batch vs. (cid:96)1-online algorithms (in terms of SRE) and do a qualitative evaluation of the (cid:96)1-online
algorithm for the novel document detection task. Here  the size of the initial dictionary k = 100.
Figure 2(c) shows the running times on the Twitter dataset. At ﬁrst timestep the online algorithm is
already 10.8 times faster  and this speedup escalates to 18.2 by the 14th timestep. Figure 2(d) shows
the SRE of the dictionaries produced by these algorithms. In this case  the SRE of the dictionaries
produced by the batch algorithm is consistently better than that of the online algorithm  but as the
running time plots suggests this improvement comes at a very steep price.

Date

2011-09-26
2011-09-29
2011-10-03
2011-10-04
2011-10-05

Sample Novel Tweets Detected Using our Online Algorithm

Android powered 56 percent of smartphones sold in the last three months. Sad thing is it can’t lower the rating of ios!
How Windows 8 is faster  lighter and more efﬁcient: WP7 Droid Bionic Android 2.3.4 HP TouchPad white ipods 72

U.S. News: AT&T begins sending throttling warnings to top data hogs: AT&T did away with its unlimited da... #iPhone

Can’t wait for the iphone 4s #letstalkiphone

Everybody put an iPhone up in the air one time #ripstevejobs

Table 2: Sample novel documents detected by our online algorithm.

Table 2 below shows a representative set of novel tweets identiﬁed by our online algorithm. Using
a completely automated process (refer to the full version [11])  we are able to detect breaking news
and trending relevant to the smartphone market  such as AT&T throttling data bandwidth  launch of
IPhone 4S  and the death of Steve Jobs.

8

024680100200300400TimestepCPU Running Time (in mins)Running Time Plot for TDT2  ONLINEBATCH−IMPLL2−BATCH024680.60.70.80.91TimestepSparse Reconstruction Error (SRE)Sparse Reconstruction Error Plot for TDT2  ONLINEBATCH−IMPL05100100200300400TimestepCPU Running Time (in mins)Run Time Plot for Twitter  ONLINEBATCH−IMPL05100.50.60.70.80.91TimestepSparse Reconstruction Error (SRE)Sparse Reconstruction Error Plot for Twitter  ONLINEBATCH−IMPLReferences

[1] M. Aharon  M. Elad  and A. Bruckstein. The K-SVD: An Algorithm for Designing Overcomplete Dic-

tionaries for Sparse Representation. IEEE Transactions on Signal Processing  54(11)  2006.

[2] S. Boyd  N. Parikh  E. Chu  B. Peleato  and J. Eckstein. Distributed Optimization and Statistical Learning
via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning  2011.
[3] V. Chenthamarakshan  P. Melville  V. Sindhwani  and R. D. Lawrence. Concept Labeling: Building Text

Classiﬁers with Minimal Supervision. In IJCAI  pages 1225–1230  2011.

[4] J. Duchi  S. Shalev-Shwartz  Y. Singer  and T. Chandra. Efﬁcient Projections onto the l1-ball for Learning

in High Dimensions. In ICML  pages 272–279  2008.

[5] J. Duchi and Y. Singer. Efﬁcient Online and Batch Learning using Forward Backward Splitting. JMLR 

10:2873–2898  2009.

[6] J. C. Duchi  S. Shalev-Shwartz  Y. Singer  and A. Tewari. Composite Objective Mirror Descent. In COLT 

pages 14–26  2010.

[7] J. Friedman  T. Hastie  H. Hﬂing  and R. Tibshirani. Pathwise Coordinate Optimization. The Annals of

Applied Statistics  1(2):302–332  2007.

[8] E. Hazan  A. Agarwal  and S. Kale. Logarithmic Regret Algorithms for Online Convex Optimization.

Machine Learning  69(2-3):169–192  2007.

[9] P. O. Hoyer. Non-Negative Sparse Coding. In IEEE Workshop on Neural Networks for Signal Processing 

pages 557–565  2002.

[10] S. P. Kasiviswanathan  P. Melville  A. Banerjee  and V. Sindhwani. Emerging Topic Detection using

Dictionary Learning. In CIKM  pages 745–754  2011.

[11] S. P. Kasiviswanathan  H. Wang  A. Banerjee  and P. Melville. Online (cid:96)1-Dictionary Learning
http://www.cse.psu.edu/˜kasivisw/

with Application to Novel Document Detection.
fullonlinedict.pdf.

[12] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online Learning for Matrix Factorization and Sparse Coding.

JMLR  11:19–60  2010.

[13] C. Manning  P. Raghavan  and H. Sch¨utze. Introduction to Information Retrieval. Cambridge University

Press  2008.

[14] P. Melville  J. Leskovec  and F. Provost  editors. Proceedings of the First Workshop on Social Media

Analytics. ACM  2010.

[15] B. Olshausen and D. Field. Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by

V1? Vision Research  37(23):3311–3325  1997.

[16] S. Petrovi´c  M. Osborne  and V. Lavrenko. Streaming First Story Detection with Application to Twitter.

In HLT ’10  pages 181–189. ACL  2010.

[17] A. Saha and V. Sindhwani. Learning Evolving and Emerging Topics in Social Media: A Dynamic NMF

Approach with Temporal Regularization. In WSDM  pages 693–702  2012.

[18] S. Shalev-Shwartz. Online Learning and Online Convex Optimization. Foundations and Trends in Ma-

chine Learning  4(2)  2012.

[19] H. Wang and A. Banerjee. Online Alternating Direction Method. In ICML  2012.
[20] J. Wright and Y. Ma. Dense Error Correction Via L1-Minimization. IEEE Transactions on Information

Theory  56(7):3540–3560  2010.

[21] J. Wright  A. Yang  A. Ganesh  S. Sastry  and Y. Ma. Robust Face Recognition via Sparse Representation.

IEEE Transactions on Pattern Analysis and Machine Intelliegence  31(2):210–227  Feb. 2009.

[22] L. Xiao. Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization. JMLR 

11:2543–2596  2010.

[23] A. Y. Yang  S. S. Sastry  A. Ganesh  and Y. Ma. Fast L1-minimization Algorithms and an Application
in Robust Face Recognition: A Review. In International Conference on Image Processing  pages 1849–
1852  2010.

[24] J. Yang and Y. Zhang. Alternating Direction Algorithms for L1-Problems in Compressive Sensing. SIAM

Journal of Scientiﬁc Computing  33(1):250–278  2011.

[25] M. Zinkevich. Online Convex Programming and Generalized Inﬁnitesimal Gradient Ascent. In ICML 

pages 928–936  2003.

9

,Philip Thomas
William Dabney
Stephen Giguere
Sridhar Mahadevan
Songbai Yan
Kamalika Chaudhuri
Tara Javidi