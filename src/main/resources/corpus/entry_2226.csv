2017,Gaussian process based nonlinear latent structure discovery in multivariate spike train data,A large body of recent work focuses on methods for extracting low-dimensional latent structure from multi-neuron spike train data. Most such methods employ either linear latent dynamics or linear mappings from latent space to log spike rates. Here we propose a doubly nonlinear latent variable model that can identify low-dimensional structure underlying apparently high-dimensional spike train data. We introduce the Poisson Gaussian-Process Latent Variable Model (P-GPLVM)  which consists of Poisson spiking observations and two underlying Gaussian processes—one governing a temporal latent variable and another governing a set of nonlinear tuning curves. The use of nonlinear tuning curves enables discovery of low-dimensional latent structure even when spike responses exhibit high linear dimensionality (e.g.  as found in hippocampal place cell codes). To learn the model from data  we introduce the decoupled Laplace approximation  a fast approximate inference method that allows us to efficiently optimize the latent path while marginalizing over tuning curves. We show that this method outperforms previous Laplace-approximation-based inference methods in both the speed of convergence and accuracy. We apply the model to spike trains recorded from hippocampal place cells and show that it compares favorably to a variety of previous methods for latent structure discovery  including variational auto-encoder (VAE) based methods that parametrize the nonlinear mapping from latent space to spike rates with a deep neural network.,Gaussian process based nonlinear latent structure

discovery in multivariate spike train data

Anqi Wu  Nicholas A. Roy  Stephen Keeley  & Jonathan W. Pillow

Princeton Neuroscience Institute

Princeton University

Abstract

A large body of recent work focuses on methods for extracting low-dimensional
latent structure from multi-neuron spike train data. Most such methods employ
either linear latent dynamics or linear mappings from latent space to log spike
rates. Here we propose a doubly nonlinear latent variable model that can identify
low-dimensional structure underlying apparently high-dimensional spike train data.
We introduce the Poisson Gaussian-Process Latent Variable Model (P-GPLVM) 
which consists of Poisson spiking observations and two underlying Gaussian
processes—one governing a temporal latent variable and another governing a set
of nonlinear tuning curves. The use of nonlinear tuning curves enables discovery
of low-dimensional latent structure even when spike responses exhibit high linear
dimensionality (e.g.  as found in hippocampal place cell codes). To learn the model
from data  we introduce the decoupled Laplace approximation  a fast approxi-
mate inference method that allows us to efﬁciently optimize the latent path while
marginalizing over tuning curves. We show that this method outperforms previous
Laplace-approximation-based inference methods in both the speed of convergence
and accuracy. We apply the model to spike trains recorded from hippocampal place
cells and show that it compares favorably to a variety of previous methods for latent
structure discovery  including variational auto-encoder (VAE) based methods that
parametrize the nonlinear mapping from latent space to spike rates with a deep
neural network.

Introduction

1
Recent advances in multi-electrode array recording techniques have made it possible to measure
the simultaneous spiking activity of increasingly large neural populations. These datasets have
highlighted the need for robust statistical methods for identifying the latent structure underlying
high-dimensional spike train data  so as to provide insight into the dynamics governing large-scale
activity patterns and the computations they perform [1–4].
Recent work has focused on the development of sophisticated model-based methods that seek to
extract a shared  low-dimensional latent process underlying population spiking activity. These
methods can be roughly categorized on the basis of two basic modeling choices: (1) the dynamics of
the underlying latent variable; and (2) the mapping from latent variable to neural responses. For choice
of dynamics  one popular approach assumes the latent variable is governed by a linear dynamical
system [5–11]  while a second assumes that it evolves according to a Gaussian process  relaxing the
linearity assumption and imposing only smoothness in the evolution of the latent state [1  12–14].
For choice of mapping function  most previous methods have assumed a ﬁxed linear or log-linear
relationship between the latent variable and the mean response level [1  5–8  11  12]. These methods
seek to ﬁnd a linear embedding of population spiking activity  akin to PCA or factor analysis. In many
cases  however  the relationship between neural activity and the quantity it encodes can be highly
nonlinear. Hippocampal place cells provide an illustrative example: if each discrete location in a 2D

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Figure 1: Schematic diagram of the Poisson Gaussian Process Latent Variable Model (P-GPLVM) 
illustrating multi-neuron spike train data generated by the model with a one-dimensional latent
process.

environment has a single active place cell  population activity spans a space whose dimensionality is
equal to the number of neurons; a linear latent variable model cannot ﬁnd a reduced-dimensional
representation of population activity  despite the fact that the underlying latent variable (“position”)
is clearly two-dimensional.
Several recent studies have introduced nonlinear coupling between latent dynamics and ﬁring rate
[7  9  10  15]. These models use deep neural networks to parametrize the nonlinear mapping from
latent space to spike rates  but often require repeated trials or long training sets. Table 1 summarizes
these different model structures for latent neural trajectory estimation (including the original Gaussian
process latent variable model (GPLVM) [16]  which assumes Gaussian observations and does not
produce spikes).

linear

neural net
neural net

output nonlinearity

model
Poisson
PLDS [8]
Poisson
PfLDS [9  10]
Poisson
LFADS [15]
Gaussian
GPFA [1]
Poisson
P-GPFA [13  14]
Gaussian
GPLVM [16]
P-GPLVM
Poisson
Table 1: Modeling assumptions of various latent variable models for spike trains.

latent mapping function
LDS
LDS
RNN
GP
GP
GP
GP

linear
linear
GP
GP

exp
exp
exp

exp

observation

identity

identity

exp

In this paper  we propose the Poisson Gaussian process latent variable model (P-GPLVM) for spike
train data  which allows for nonlinearity in both the latent state dynamics and in the mapping from
the latent states to the spike rates. Our model posits a low-dimensional latent variable that evolves
in time according to a Gaussian process prior; this latent variable governs ﬁring rates via a set
of non-parametric tuning curves  parametrized as exponentiated samples from a second Gaussian
process  from which spikes are then generated by a Poisson process (Fig. 1).
The paper is organized as follows: Section 2 introduces the P-GPLVM; Section 3 describes the
decoupled Laplace approximation for performing efﬁcient inference for the latent variable and
tuning curves; Section 4 describes tuning curve estimation; Section 5 compares P-GPLVM to other
models using simulated data and hippocampal place-cell recordings  demonstrating the accuracy and
interpretability of P-GPLVM relative to other methods.

2 Poisson-Gaussian process latent variable model (P-GPLVM)
Suppose we have simultaneously recorded spike trains from N neurons. Let Y ∈ RN×T denote the
matrix of spike count data  with neurons indexed by i ∈ (1  . . .   N ) and spikes counted in discrete

2

1001230spike ratelatent processlog tuning curvestuning curvestime (s)spike trains100time (s)time bins indexed by t ∈ (1  . . .   T ). Our goal is to construct a generative model of the latent structure
underlying these data  which will here take the form of a P -dimensional latent variable x(t) and a set
of mapping functions or tuning curves {hi(x)}  i ∈ (1  . . .   N ) which map the latent variable to the
spike rates of each neuron.
Latent dynamics
Let x(t) denote a (vector-valued) latent process  where each component xj(t)  j ∈ (1  . . .   P ) 
evolves according to an independent Gaussian process (GP) 
xj(t) ∼ GP (0  kt)  

(1)
with covariance function kt(t  t(cid:48)) (cid:44) cov(xj(t)  xj(t(cid:48))) governing how each scalar process varies
over time. Although we can select any valid covariance function for kt  here we use the exponential
covariance function  a special case of the Matérn kernel  given by k(t  t(cid:48)) = r exp (−|t − t(cid:48)|/l) 
which is parametrized by a marginal variance r > 0 and length-scale l > 0. Samples from this GP
are continuous but not differentiable  equivalent to a Gaussian random walk with a bias toward the
origin  also known as the Ornstein-Uhlenbeck process [17].
The latent state x(t) at any time t is a P -dimensional vector that we will write as xt ∈ RP×1. The
collection of such vectors over T time bins forms a matrix X ∈ RP×T . Let xj denote the jth row
of X  which contains the set of states in latent dimension j. From the deﬁnition of a GP  xj has a
multivariate normal distribution 
(2)
with a T × T covariance matrix Kt generated by evaluating the covariance function kt at all time
bins in (1  . . .   T ).
Nonlinear mapping
Let h : RP −→ R denote a nonlinear function mapping from the latent vector xt to a ﬁring rate λt.
We will refer to h(x) as a tuning curve  although unlike traditional tuning curves  which describe
ﬁring rate as a function of some externally (observable) stimulus parameter  here h(x) describes
ﬁring rate as a function of the (unobserved) latent vector x. Previous work has modeled h with a
parametric nonlinear function such as a deep neural network [9  10]. Here we develop a nonparametric
approach using a Gaussian process prior over the log of h. The logarithm assures that spike rates are
non-negative.
Let fi(x) = log hi(x) denote the log tuning curve for the i’th neuron in our population  which we
model with a GP 

xj ∼ N (0  Kt)

fi(x) ∼ GP (0  kx)  

(3)
where kx is a (spatial) covariance function that governs smoothness of the function over its P -
dimensional input space. For simplicity  we use the common Gaussian or radial basis function (RBF)

covariance function: kx(x  x(cid:48)) = ρ exp(cid:0)−||x − x(cid:48)||2

2/2δ2(cid:1)  where x and x(cid:48) are arbitrary points in

latent space  ρ is the marginal variance and δ is the length scale. The tuning curve for neuron i is then
given by hi(x) = exp(fi(x)).
Let fi ∈ RT×1 denote a vector with the t’th element equal to fi(xt). From the deﬁnition of a GP  fi
has a multivariate normal distribution given latent vectors at all time bins x1:T = {xt}T

t=1 

fi|x1:T ∼ N (0  Kx)

(4)
with a T × T covariance matrix Kx generated by evaluating the covariance function kx at all pairs of
latent vectors in x1:T . Stacking fi for N neurons  we will formulate a matrix F ∈ RN×T with f(cid:62)
i on
the i’th row. The element on the i’th row and the t’th column is fi t = fi(xt).
Poisson spiking
Lastly  we assume Poisson spiking given the latent ﬁring rates. We assume that spike rates are in
units of spikes per time bin. Let λi t = exp(fi t) = exp(fi(xt)) denote the spike rate of neuron i at
time t. The spike-count of neuron i at t given the log tuning curve fi and latent vector xt is Poisson
distributed as

(5)
In summary  our model is as a doubly nonlinear Gaussian process latent variable model with Poisson
observations (P-GPLVM). One GP is used to model the nonlinear evolution of the latent dynamic
x  while a second GP is used to generate the log of the tuning curve f as a nonlinear function of x 
which is then mapped to a tuning curve h via a nonlinear link function  e.g. exponential function.
Fig. 1 provides a schematic of the model.

yi t|fi  xt ∼ Poiss(exp(fi(xt))).

3

Inference using the decoupled Laplace approximation

3
For our inference procedure  we estimate the log of the tuning curve  f  as opposed to attempting
to infer the tuning curve h directly. Once f is estimated  h can be obtained by exponentiating f.
Given the model outlined above  the joint distribution over the observed data and all latent variables
is written as 

p(Y  F  X  θ) = p(Y|F)p(F|X  ρ  δ)p(X|r  l) =

p(yi t|fi t)

p(fi|X  ρ  δ)

p(xj|r  l) 

(6)

N(cid:89)

T(cid:89)

i=1

t=1

N(cid:89)

i=1

P(cid:89)

j=1

where θ = {ρ  δ  r  l} is the hyperparameter set  references to which will now be suppressed for
simpliﬁcation. This is a Gaussian process latent variable model (GPLVM) with Poisson observations
and a GP prior  and our goal is to now estimate both F and X. A standard Bayesian treatment of the
GPLVM requires the computation of the log marginal likelihood associated with the joint distribution
(Eq.6). Both F and X must be marginalized out 

log p(Y) = log

p(Y  F  X)dXdF = log

p(Y|F)

p(F|X)p(X)dX dF.

(7)

(cid:90)

(cid:90)

(cid:90) (cid:90)

However  propagating the prior density p(X) through the nonlinear mapping makes this inference
difﬁcult. The nested integral in (Eq. 7) contains X in a complex nonlinear manner  making analytical
integration over X infeasible. To overcome these difﬁculties  we can use a straightforward MAP
training procedure where the latent variables F and X are selected according to

FMAP  XMAP = argmaxF X p(Y|F)p(F|X)p(X).

(8)
Note that point estimates of the hyperparameters θ can also be found by maximizing the same
objective function. As discussed above  learning X remains a challenge due to the interplay of the
latent variables  i.e. the dependency of F on X. For our MAP training procedure  ﬁxing one latent
variable while optimizing for the other in a coordinate descent approach is highly inefﬁcient since the
strong interplay of variables often means getting trapped in bad local optima. In variational GPLVM
[18]  the authors introduced a non-standard variational inference framework for approximately
integrating out the latent variables X then subsequently training a GPLVM by maximizing an analytic
lower bound on the exact marginal likelihood. An advantage of the variational framework is the
introduction of auxiliary variables which weaken the strong dependency between X and F. However 
the variational approximation is only applicable to Gaussian observations; with Poisson observations 
the integral over F remains intractable. In the following  we will propose using variations of the
Laplace approximation for inference.
3.1 Standard Laplace approximation
We ﬁrst use Laplace’s method to ﬁnd a Gaussian approximation q(F|Y  X) to the true posterior
p(F|Y  X)  then do MAP estimation for X only. We employ the Laplace approximation for each fi
individually. Doing a second order Taylor expansion of log p(fi|yi  X) around the maximum of the
posterior  we obtain a Gaussian approximation

q(fi|yi  X) = N (ˆfi  A−1) 

(9)

p(fi|yi  X) and A = −∇∇ log p(fi|yi  X)|fi=ˆfi

where ˆfi = argmaxfi
is the Hessian of the neg-
ative log posterior at that point. By Bayes’ rule  the posterior over fi is given by p(fi|yi  X) =
p(yi|fi)p(fi|X)/p(yi|X)  but since p(yi|X) is independent of fi  we need only consider the unnor-
malized posterior  deﬁned as Ψ(fi)  when maximizing w.r.t. fi. Taking the logarithm gives
log |Kx| + const.

Ψ(fi) = log p(yi|fi) + log p(fi|X) = log p(yi|fi) − 1
2

(10)

f(cid:62)
i K−1

x fi − 1
2

Differentiating (Eq. 10) w.r.t. fi we obtain

∇Ψ(fi) = ∇ log p(yi|fi) − K−1
x fi
∇∇Ψ(fi) = ∇∇ log p(yi|fi) − K−1
x = −Wi − K−1
x  

(11)
(12)
where Wi = −∇∇ log p(yi|fi). The approximated log conditional likelihood on X (see Sec. 3.4.4
in [17]) can then be written as

log q(yi|X) = log p(yi|ˆfi) − 1
2

ˆf(cid:62)
i K−1

x

ˆfi − 1
2

log |IT + KxWi|.

(13)

4

We can then estimate X as

N(cid:88)

q(yi|X)p(X).

XMAP = argmaxX

(14)

i=1

When using standard LA  the gradient of log q(yi|X) w.r.t. X should be calculated for a given
posterior mode ˆfi. Note that not only is the covariance matrix Kx an explicit function of X  but also
ˆfi and Wi are also implicitly functions of X — when X changes  the optimum of the posterior ˆfi
changes as well. Therefore  log q(yi|X) contains an implicit function of X which does not allow for
a straightforward closed-form gradient expression. Calculating numerical gradients instead yields a
very inefﬁcient implementation empirically.
3.2 Third-derivative Laplace approximation
One method to derive this gradient explicitly is described in [17] (see Sec. 5.5.1). We adapt their
procedure to our setting to make the implicit dependency of ˆfi and Wi on X explicit. To solve (Eq.
14)  we need to determine the partial derivative of our approximated log conditional likelihood (Eq.
13) w.r.t. X  given as

∂ log q(yi|X)

∂X

=

∂ log q(yi|X)

∂X

T(cid:88)

+

∂ log q(yi|X)

t=1

∂ ˆfi t

∂ ˆfi t
∂X

(15)

∂ˆfi
∂X · ∂

by the chain rule. When evaluating the second term  we use the fact that ˆfi is the posterior maximum 
so ∂Ψ(fi)/∂fi = 0 at fi = ˆfi  where Ψ(fi) is deﬁned in (Eq. 11). Thus the implicit derivatives of the
ﬁrst two terms in (Eq. 13) vanish  leaving only
∂ log q(yi|X)
x + Wi)−1 ∂Wi
∂ ˆfi t

(cid:2)(K−1
x + Wi)−1(cid:3)

log p(yi|ˆfi). (16)

= − 1
2

= − 1
2

(K−1

(cid:32)

∂ ˆfi t

tr

∂3
∂ ˆf 3
i t

tt

To evaluate ∂ ˆfi t/∂X  we differentiate the self-consistent equation ˆfi = Kx∇ log p(yi|ˆfi) (setting
(Eq. 11) to be 0 at ˆfi) to obtain
∂ˆfi
∂X

= (IT + KxWi)−1 ∂Kx
∂X

∇ log p(yi|ˆfi)  (17)

∇ log p(yi|ˆfi) + Kx

∇ log p(yi|ˆfi)

∂Kx
∂X

∂ˆfi
∂X

=

∂ˆfi

∂X = ∂ˆfi

and ∂∇ log p(yi|ˆfi)/∂ˆfi = −Wi from (Eq. 12). The
where we use the chain rule ∂
desired implicit derivative is obtained by multiplying (Eq. 16) and (Eq. 17) to formulate the second
term in (Eq. 15).
We can now estimate XMAP with (Eq. 14) using the explicit gradient expression in (Eq. 15). We
call this method third-derivative Laplace approximation (tLA)  as it depends on the third derivative
of the data likelihood term (see [17] for further details). However  there is a big computational
drawback with tLA: for each step along the gradient we have just derived  the posterior mode ˆfi
must be reevaluated. This method might lead to a fast convergence theoretically  but this nested
optimization makes for a very slow computation empirically.
3.3 Decoupled Laplace approximation
We propose a novel method to relax the Laplace approximation  which we refer to as the decoupled
Laplace approximation (dLA). Our relaxation not only decouples the strong dependency between X
and F  but also avoids the nested optimization of searching for the posterior mode of F within each
update of X. As in tLA  dLA also assumes ˆfi to be a function of X. However  while tLA assumes ˆfi
to be an implicit function of X  dLA constructs an explicit mapping between ˆfi and X.
The standard Laplace approximation uses a Gaussian approximation for the posterior p(fi|yi  X) ∝
p(yi|fi)p(fi|X) where  in this paper  p(yi|fi) is a Poisson distribution and p(fi|X) is a multivariate
Gaussian distribution. We ﬁrst do the same second order Taylor expansion of log p(fi|yi  X) around
the posterior maximum to ﬁnd q(fi|yi  X) as in (Eq. 9). Now if we approximate the likelihood
distribution p(yi|fi) as a Gaussian distribution q(yi|fi) = N (m  S)  we can derive its mean m and
covariance S. If p(fi|X) = N (0  Kx) and q(fi|yi  X) = N (ˆfi  A−1)  the relationship between
two Gaussian distributions and their product allow us to solve for m and S from the relationship
N (ˆfi  A−1) ∝ N (m  S)N (0  Kx):

(cid:12)(cid:12)(cid:12)(cid:12)explicit
(cid:33)

5

Algorithm 1 Decoupled Laplace approximation at iteration k

Input: data observation yi  latent variable Xk−1 from iteration k − 1
1. Compute the new posterior mode ˆf k

i and the precision matrix Ak by solving (Eq. 10) to obtain

q(fi|yi  Xk−1) = N (ˆf k

i   Ak−1

).

A(X) = Sk−1

2. Derive mk and Sk (Eq. 18): Sk = (Ak − K−1
3. Fix mk and Sk and derive the new mean and covariance for q(fi|yi  Xk−1) as functions of X:
x   we have Wi = Sk−1  and can obtain the new approximated conditional
(cid:80)N
i=1 q(yi|X)p(X).

x )−1  mk = SkAkˆf k
i .
−1Sk−1
−1Akˆf k
i .

distribution q(yi|X) (Eq. 13) with ˆfi replaced by ˆfi(X).

4. Since A = Wi + K−1

−1  ˆfi(X) = A(X)

mk = A(X)

+ Kx(X)

5. Solve Xk = argmaxX
Output: new latent variable Xk

A = S−1 + K−1

x )−1  m = SAˆfi.

x   ˆfi = A−1S−1m =⇒ S = (A − K−1

(18)
m and S represent the components of the posterior terms  ˆfi and A  that come from the likelihood.
Now when estimating X  we ﬁx these likelihood terms m and S  and completely relax the prior 
p(fi|X). We are still solving (Eq. 14) w.r.t. X  but now q(fi|yi  X) has both mean and covariance
approximated as explicit functions of X. Alg. 1 describes iteration k of the dLA algorithm  with which
we can now estimate XMAP. Step 3 indicates that the posterior maximum for the current iteration
ˆfi(X) = A(X)
is now explicitly updated as a function of X  avoiding the computationally
demanding nested optimization of tLA. Intuitively  dLA works by ﬁnding a Gaussian approximation
i such that the approximated posterior of fi  q(fi|yi  X)  is now a closed-form
to the likelihood at ˆf k
Gaussian distribution with mean and covariance as functions of X  ultimately allowing for the explicit
calculation of q(yi|X).

−1Akˆf k

i

4 Tuning curve estimation
Given the estimated ˆX and ˆf from the inference  we can now calculate the tuning curve h for each
neuron. Let x1:G = {xg}G
g=1 be a grid of G latent states  where xg ∈ RP×1. Correspondingly 
for each neuron  we have the log of the tuning curve vector evaluated on the grid of latent states 
fgrid ∈ RG×1  with the g’th element equal to f (xg). Similar to (Eq. 4)  we can write down its
distribution as
(19)
with a G × G covariance matrix Kgrid generated by evaluating the covariance function kx at all pairs
of vectors in x1:G. Therefore we can write a joint distribution for [ˆf   fgrid] as

fgrid|x1:G ∼ N (0  Kgrid)

(cid:35)

(cid:34) ˆf

fgrid

(cid:18)

(cid:20) Kˆx

∼ N

0 

kgrid
k(cid:62)
grid Kgrid

(cid:21)(cid:19)

.

(20)

Kˆx ∈ RT×T is a covariance matrix with elements evaluated at all pairs of estimated latent vectors
ˆx1:T = {ˆxt}T
t=1 in ˆX  and kgridt g = kx(ˆxt  xg). Thus we have the following posterior distribution
over fgrid:

fgrid|ˆf   ˆx1:T   x1:G ∼ N (µ(x1:G)  Σ(x1:G))

µ(x1:G) = k(cid:62)

gridK−1

ˆx

ˆf

  Σ(x1:G) = diag(Kgrid) − k(cid:62)

gridK−1

ˆx kgrid

(21)

(22)

where diag(Kgrid) denotes a diagonal matrix constructed from the diagonal of Kgrid. Setting ˆfgrid =
µ(x1:G)  the spike rate vector

ˆλgrid = exp(ˆfgrid)

describes the tuning curve h evaluated on the grid x1:G.
5 Experiments
5.1 Simulation data
We ﬁrst examine performance using two simulated datasets generated with different kinds of tuning
curves  namely sinusoids and Gaussian bumps. We will compare our algorithm (P-GPLVM) with

6

Figure 2: Results from the sinusoid and Gaussian bump simulated experiments. A) and C) are
estimated latent processes. B) and D) display the tuning curves estimated by different methods. E)
shows the R2 performances with error bars. F) shows the convergence R2 performances of three
different Laplace approximation inference methods with error bars. Error bars are plotted every 10
seconds.

PLDS  PfLDS  P-GPFA and GPLVM (see Table 1)  using the tLA and dLA inference methods. We
also include an additional variant on the Laplace approximation  which we call the approximated
Laplace approximation (aLA)  where we use only the explicit (ﬁrst) term in (Eq. 15) to optimize over
X for multiple steps given a ﬁxed ˆfi. This allows for a coarse estimation for the gradient w.r.t. X for
a few steps in X before estimation is necessary  partially relaxing the nested optimization so as to
speed up the learning procedure.
For comparison between models in our simulated experiments  we compute the R-squared (R2)
values from the known latent processes and the estimated latent processes. In all simulation studies 
we generate 1 single trial per neuron with 20 simulated neurons and 100 time bins for a single
experiment. Each experiment is repeated 10 times and results are averaged across 10 repeats.
Sinusoid tuning curve: This simulation generates a "grid cell" type response. A grid cell is a type of
neuron that is activated when an animal occupies any point on a grid spanning the environment [19].
When an animal moves in a one-dimensional space (P = 1)  grid cells exhibit oscillatory responses.
Motivated by the response properties of grid cells  the log ﬁring rate of each neuron i is coupled to
the latent process through a sinusoid with a neuron-speciﬁc phase Φi and frequency ωi 

fi = sin(ωix + Φi).

(23)

We randomly generated Φi uniformly from the region [0  2π] and ωi uniformly from [1.0  4.0].
An example of the estimated latent processes versus the true latent process is presented in Fig. 2A. We
used least-square regression to learn an afﬁne transformation from the latent space to the space of the
true locations. Only P-GPLVM ﬁnds the global optimum by ﬁtting the valley around t = 70. Fig. 2B
displays the true tuning curves and the estimated tuning curves for neuron 4  10  & 9 with PLDS 
PfLDS  P-GPFA and P-GPLVM-dLA. For PLDS  PfLDS and P-GPFA  we replace the estimated ˆf
with the observed spike count y in (Eq. 21)  and treat the posterior mean as the tuning curve on a
grid of latent representations. For P-GPLVM  the tuning curve is estimated via (Eq. 22). The R2
performance is shown in the ﬁrst column of Fig. 2E.
Deterministic Gaussian bump tuning curve: For this simulation  each neuron’s tuning curve is
modeled as a unimodal Gaussian bump in a 2D space such that the log of the tuning curve  f  is a
deterministic Gaussian function of x. Fig. 2C shows an example of the estimated latent processes.
PLDS ﬁts an overly smooth curve  while P-GPLVM can ﬁnd the small wiggles that are missed by
other methods. Fig. 2D displays the 2D tuning curves for neuron 1  4  & 12 estimated by PLDS 
PfLDS  P-GPFA and P-GPLVM-dLA. The R2 performance is shown in the second column of Fig. 2E.
Overall  P-GPFA has a quite unstable performance due to the ARD kernel function in the GP prior 
potentially encouraging a bias for smoothness even when the underlying latent process is actually

7

PfLDSP-GPFAP-GPLVMPLDSTrueneuron 1neuron 4neuron 12D)PLDSPfLDSTrue locationP-GPFAP-GPLVM-dLA-1.2-0.40.401.53-1.2-0.40.401.531.531.5300-1.2-0.40.4-1.2-0.40.4PLDSPfLDSP-GPFAP-GPLVM-dLAlocationspike rateneuron 4neuron 10neuron 19TrueEstimatedB)-2021st dimension022nd dimension2060100timeGaussian bump tuning curveC)time2060100-1.2-0.40.41st dimension Sinusoid tuning curveA)0.80.9Gaussian bumptuning curveE)F)P-GPLVM-dLAPfLDSP-GPFAGPLVMP-GPLVM-tLAP-GPLVM-aLASinusoidtuning curve0.90.60.3PLDS0.90.80.70.60400800time (sec)P-GPLVM-tLAP-GPLVM-aLAP-GPLVM-dLA value valueFigure 3: Results from the hippocampal data of two rats. A) and B) are estimated latent processes
during a 1s recording period for two rats. C) and D) show R2 and PLL performance with error bars.
E) and F) display the true tuning curves and the tuning curves estimated by P-GPLVM-dLA.

quite non-smooth. PfLDS performs better than PLDS in the second case  but when the true latent
process is highly nonlinear (sinusoid) and the single-trial dataset is small  PfLDS losses its advantage
to stochastic optimization. GPLVM has a reasonably good performance with the nonlinearities  but is
worse than P-GPLVM which demonstrates the signiﬁcance of using the Poisson observation model.
For P-GPLVM  the dLA inference algorithm performs best overall w.r.t. both convergence speed and
R2 (Fig. 2F).
5.2 Application to rat hippocampal neuron data
Next  we apply the proposed methods to extracellular recordings from the rodent hippocampus.
Neurons were recorded bilaterally from the pyramidal layer of CA3 and CA1 in two rats as they
performed a spatial alternation task on a W-shaped maze [20]. We conﬁne our analyses to simultane-
ously recorded putative place cells during times of active navigation. Total number of simultaneously
recorded neurons ranged from 7-19 for rat 1 and 24-38 for rat 2. Individual trials of 50 seconds were
isolated from 15 minute recordings  and binned at a resolution of 100ms.
We used this hippocampal data to identify a 2D latent space using PLDS  PfLDS  P-GPFA  GPLVM
and P-GPLVMs (Fig. 3)  and compared these to the true 2D location of the rodent. For visualization
purposes  we linearized the coordinates along the arms of the maze to obtain 1D representations.

8

PLDSTrue locationP-GPLVM-dLA20060010001802201st dimension2006001000501502nd dimensiontime (ms)rat 2A)B)C)D)010020004010020005010020004010020005010020004010020005010020004010020005locationspike rateneuron 1neuron 12neuron 2neuron 10True locationEstimated location01002003000.51.501002003000501002003000501002003000501002003000.51.5010020030005010020030005010020030005neuron 19neuron 1neuron 9neuron 28True locationEstimated locationlocationrat 120060010001002001st dimension201002nd dimension2006001000time (ms)E)F) value0.80.750.70.650.40.30.2PLL value0.80.750.70.65PLDSPfLDSP-GPFAGPLVMP-GPLVM-aLAP-GPLVM-tLAP-GPLVM-dLA0.150.1PLLFig. 3A & B present two segments of 1s recordings for the two animals. The P-GPLVM results are
smoother and recover short time-scale variations that PLDS ignores. The average R2 performance
for all methods for each rodent is shown in Fig. 3C & D where P-GPLVM-dLA consistently performs
the best.
We also assessed the model ﬁtting quality by doing prediction on a held-out dataset. We split all
the time bins in each trial into training time bins (the ﬁrst 90% time bins) and held-out time bins
(the last 10% time bins). We ﬁrst estimated the parameters for the mapping function or the tuning
curve in each model using spike trains from all the neurons within training time bins. Then we ﬁxed
the parameters and inferred the latent process using spike trains from 70% neurons within held-out
time bins. Finally  we calculated the predictive log likelihood (PLL) for the other 30% neurons
within held-out time bins given the inferred latent process. We subtracted the log-likelihood of the
population mean ﬁring rate model (single spike rate) from the predictive log likelihood divided by
number of observations  shown in Fig. 3C & D. Both P-GPLVM-aLA and P-GPLVM-dLA perform
well. GPLVM has very negative PLL  omitted in the ﬁgures.
Fig. 3E & F present the tuning curves learned by P-GPLVM-dLA where each row corresponds to
a neuron. For our analysis we have the true locations xtrue  the estimated locations xP-GPLVM  a grid
of G locations x1:G distributed with a shape of the maze  the spike count observation yi  and the
estimated log of the tuning curves ˆfi for each neuron i. The light gray dots in the ﬁrst column of
Fig. 3E & F are the binned spike counts when mapping from the space of xtrue to the space of x1:G.
The second column contains the binned spike counts mapped from the space of xP-GPLVM to the space
of x1:G. The black curves in the ﬁrst column are achieved by replacing ˆx and ˆf with xtrue and y
respectively using the predictive posterior in (Eq. 21) and (Eq. 22). The yellow curves in the second
column are the estimated tuning curves by using (Eq. 22) to get ˆλgrid for each neuron. We can tell that
the estimated tuning curves closely match the true tuning curves from the observations  discovering
different responsive locations for different neurons as the rat moves.
6 Conclusion
We proposed a doubly nonlinear Gaussian process latent variable model for neural population spike
trains that can identify nonlinear low-dimensional structure underlying apparently high-dimensional
spike train data. We also introduced a novel decoupled Laplace approximation  a fast approximate
inference method that allows us to efﬁciently maximize marginal likelihood for the latent path
while integrating over tuning curves. We showed that this method outperforms previous Laplace-
approximation-based inference methods in both the speed of convergence and accuracy. We applied
the model to both simulated data and spike trains recorded from hippocampal place cells and showed
that it outperforms a variety of previous methods for latent structure discovery.

9

References
[1] BM Yu  JP Cunningham  G Santhanam  SI Ryu  KV Shenoy  and M Sahani. Gaussian-process factor
analysis for low-dimensional single-trial analysis of neural population activity. In Adv neur inf proc sys 
pages 1881–1888  2009.

[2] L. Paninski  Y. Ahmadian  Daniel G. Ferreira  S. Koyama  Kamiar R. Rad  M. Vidne  J. Vogelstein  and

W. Wu. A new look at state-space models for neural data. J comp neurosci  29(1-2):107–126  2010.

[3] John P Cunningham and B M Yu. Dimensionality reduction for large-scale neural recordings. Nature

neuroscience  17(11):1500–1509  2014.

[4] SW Linderman  MJ Johnson  MA Wilson  and Z Chen. A bayesian nonparametric approach for uncovering

rat hippocampal population codes during spatial navigation. J neurosci meth  263:36–47  2016.

[5] JH Macke  L Buesing  JP Cunningham  BM Yu  KV Shenoy  and M Sahani. Empirical models of spiking

in neural populations. In Adv neur inf proc sys  pages 1350–1358  2011.

[6] L Buesing  J H Macke  and M Sahani. Spectral learning of linear dynamics from generalised-linear
observations with application to neural population data. In Adv neur inf proc sys  pages 1682–1690  2012.

[7] EW Archer  U Koster  JW Pillow  and JH Macke. Low-dimensional models of neural population activity in

sensory cortical circuits. In Adv neur inf proc sys  pages 343–351  2014.

[8] JH Macke  L Buesing  and M Sahani. Estimating state and parameters in state space models of spike trains.

Advanced State Space Methods for Neural and Clinical Data  page 137  2015.

[9] Evan Archer  Il Memming Park  Lars Buesing  John Cunningham  and Liam Paninski. Black box variational

inference for state space models. arXiv preprint arXiv:1511.07367  2015.

[10] Y Gao  EW Archer  L Paninski  and JP Cunningham. Linear dynamical neural population models through

nonlinear embeddings. In Adv neur inf proc sys  pages 163–171  2016.

[11] JC Kao  P Nuyujukian  SI Ryu  MM Churchland  JP Cunningham  and KV Shenoy. Single-trial dynamics

of motor cortex and their applications to brain-machine interfaces. Nature communications  6  2015.

[12] David Pfau  Eftychios A Pnevmatikakis  and Liam Paninski. Robust learning of low-dimensional dynamics

from large neural ensembles. In Adv neur inf proc sys  pages 2391–2399  2013.

[13] Hooram Nam. Poisson extension of gaussian process factor analysis for modeling spiking neural pop-
ulations. Master’s thesis  Department of Neural Computation and Behaviour  Max Planck Institute for
Biological Cybernetics  Tubingen  8 2015.

[14] Y. Zhao and I. M. Park. Variational latent gaussian process for recovering single-trial dynamics from

population spike trains. arXiv preprint arXiv:1604.03053  2016.

[15] David Sussillo  Rafal Jozefowicz  LF Abbott  and Chethan Pandarinath. Lfads-latent factor analysis via

dynamical systems. arXiv preprint arXiv:1608.06315  2016.

[16] Neil D Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In

Adv neur inf proc sys  pages 329–336  2004.

[17] Carl Rasmussen and Chris Williams. Gaussian Processes for Machine Learning. MIT Press  2006.

[18] AC Damianou  MK Titsias  and ND Lawrence. Variational inference for uncertainty on the inputs of

gaussian process models. arXiv preprint arXiv:1409.2287  2014.

[19] T Hafting  M Fyhn  S Molden  MB Moser  and EI Moser. Microstructure of a spatial map in the entorhinal

cortex. Nature  436(7052):801–806  2005.

[20] M Karlsson  M Carr  and Frank LM. Simultaneous extracellular recordings from hippocampal areas ca1 and
ca3 (or mec and ca1) from rats performing an alternation task in two w-shapped tracks that are geometrically
identically but visually distinct. crcns.org. http://dx.doi.org/10.6080/K0NK3BZJ  2005.

10

,Anqi Wu
Nicholas Roy
Stephen Keeley
Jonathan Pillow