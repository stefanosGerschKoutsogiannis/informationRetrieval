2018,Stochastic Chebyshev Gradient Descent for Spectral Optimization,A large class of machine learning techniques requires the solution of optimization problems involving spectral functions of parametric matrices  e.g. log-determinant and nuclear norm. Unfortunately  computing the gradient of a spectral function is generally of cubic complexity  as such gradient descent methods are rather expensive for optimizing objectives involving the spectral function. Thus  one naturally turns to stochastic gradient methods in hope that they will provide a way to reduce or altogether avoid the computation of full gradients. However  here a new challenge appears: there is no straightforward way to compute unbiased stochastic gradients for spectral functions. In this paper  we develop unbiased stochastic gradients for spectral-sums  an important subclass of spectral functions. Our unbiased stochastic gradients are based on combining randomized trace estimators with stochastic truncation of the Chebyshev expansions. A careful design of the truncation distribution allows us to offer distributions that are variance-optimal  which is crucial for fast and stable convergence of stochastic gradient methods. We further leverage our proposed stochastic gradients to devise stochastic methods for objective functions involving spectral-sums  and rigorously analyze their convergence rate. The utility of our methods is demonstrated in numerical experiments.,Stochastic Chebyshev Gradient Descent

for Spectral Optimization

Insu Han1  Haim Avron2 and Jinwoo Shin1 3

1School of Electrical Engineering  Korea Advanced Institute of Science and Technology

2Department of Applied Mathematics  Tel Aviv University

3AItrics
{insu.han jinwoos}@kaist.ac.kr

haimav@post.tau.ac.il

Abstract

A large class of machine learning techniques requires the solution of optimization
problems involving spectral functions of parametric matrices  e.g. log-determinant
and nuclear norm. Unfortunately  computing the gradient of a spectral function
is generally of cubic complexity  as such gradient descent methods are rather
expensive for optimizing objectives involving the spectral function. Thus  one
naturally turns to stochastic gradient methods in hope that they will provide a way
to reduce or altogether avoid the computation of full gradients. However  here
a new challenge appears: there is no straightforward way to compute unbiased
stochastic gradients for spectral functions. In this paper  we develop unbiased
stochastic gradients for spectral-sums  an important subclass of spectral functions.
Our unbiased stochastic gradients are based on combining randomized trace esti-
mators with stochastic truncation of the Chebyshev expansions. A careful design
of the truncation distribution allows us to offer distributions that are variance-
optimal  which is crucial for fast and stable convergence of stochastic gradient
methods. We further leverage our proposed stochastic gradients to devise stochastic
methods for objective functions involving spectral-sums  and rigorously analyze
their convergence rate. The utility of our methods is demonstrated in numerical
experiments.

1

Introduction

A large class of machine learning techniques involves spectral optimization problems of the form 

min
θ∈C F (A(θ)) + g(θ) 

(1)
where C is some ﬁnite-dimensional parameter space  A is a function that maps a parameter vector
θ to a symmetric matrix A(θ)  F is a spectral function (i.e.  a real-valued function on symmetric
matrices that depends only on the eigenvalues of the input matrix)  and g : C → R. Examples include
hyperparameter learning in Gaussian process regression with F (X) = log det X [22]  nuclear norm

regularization with F (X) = tr(cid:0)X 1/2(cid:1) [20]  phase retrieval with F (X) = tr (X) [8]  and quantum

state tomography with F (X) = tr (X log X) [15]. In the aforementioned applications  the main
difﬁculty in solving problems of the form (1) is in efﬁciently addressing the spectral component
F (A(·)). While explicit formulas for the gradients of spectral functions can be derived [17]  it is
typically computationally expensive. For example  for F (X) = log det X and A(θ) ∈ Rd×d  the
exact computation of ∇θF (A(θ)) can take as much as O(d3k)  where k is the number of parameters
in θ. Therefore  it is desirable to avoid computing  or at the very least reduce the number of times we
compute  the gradient of F (A(θ)) exactly.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

It is now well appreciated in the machine learning literature that the use of stochastic gradients is
effective in alleviating costs associated with expensive exact gradient computations. Using cheap
stochastic gradients  one can avoid computing full gradients altogether by using Stochastic Gradient
Descent (SGD). The cost is  naturally  a reduced rate of convergence. Nevertheless  many machine
learning applications require only mild suboptimality  in which case cheap iterations often outweigh
the reduced convergence rate. When nearly optimal solutions are sought  more recent variance
reduced methods (e.g. SVRG [14]) are effective in reducing the number of full gradient computations
to O(1). For non-convex objectives  the stochastic methods are even more attractive to use as they
allow to avoid a bad local optimum. However  closed-form formulas for computing the full gradients
of spectral functions do not lead to efﬁcient stochastic gradients in a straightforward manner.
Contribution.
In this paper  we propose stochastic methods for solving (1) when the spectral
function F is a spectral-sum. Formally  spectral-sums are spectral functions that can be expressed as
F (X) = tr (f (X)) where f is a real-valued function that is lifted to the symmetric matrix domain by
applying it to the eigenvalues. They constitute an important subclass of spectral functions  e.g.  in all
of the aforementioned applications of spectral optimization  the spectral function F is a spectral-sum.
Our algorithms are based on recent biased estimators for spectral-sums that combine stochastic trace
estimation with Chebyshev expansion [11]. The technique used to derive these estimators can also be
used to derive stochastic estimators for the gradient of spectral-sums (e.g.  see [7])  but the resulting
estimator is biased. To address this issue  we propose an unbiased estimator for spectral-sums  and
use it to derive unbiased stochastic gradients. Our unbiased estimator is based on randomly selecting
the truncation degree in the Chebyshev expansion  i.e.  the truncated polynomial degree is drawn
under some distribution. We remark that similar ideas of sampling unbiased polynomials have been
studied in the literature  but for different setups [4  16  28  25]  and none of which are suitable for
use in our setup.
While deriving unbiased estimators is very useful for ensuring stable convergence of stochastic
gradient methods  it is not sufﬁcient: convergence rates of stochastic gradient descent methods
depend on the variance of the stochastic gradients  and this can be rather large for naïve choices
of degree distributions. Thus  our main contribution is in establishing a provably optimal degree
distribution minimizing the estimators’ variances with respect to the Chebyshev series. The proposed
distribution gives order-of-magnitude smaller variances compared to other popular ones (Figure 1) 
which leads to improved convergence of the downstream optimization (Figure 2(c)).
We leverage our proposed unbiased estimators to design two stochastic gradient descent methods  one
using the SGD framework and the other using the SVRG one. We rigorously analyze their convergence
rates  showing sublinear and linear rate for SGD and SVRG  respectively. It is important to stress that
our fast convergence results crucially depend on the proposed optimal degree distributions. Finally 
we apply our algorithms to two machine learning tasks that involve spectral optimization: matrix
completion and learning Gaussian processes. Our experimental results conﬁrm that the proposed
algorithms are signiﬁcantly faster than other competitors under large-scale real-world instances. In
particular  for learning Gaussian process under Szeged humid dataset  our generic method runs up to
six times faster than the state-of-art method [7] specialized for the purpose.

2 Preliminaries
We denote the family of real symmetric matrices of dimension d by S d×d. For A ∈ S d×d  we use
(cid:107)A(cid:107)mv to denote the time-complexity of multiplying A with a vector  i.e.  (cid:107)A(cid:107)mv = O(d2). For some
structured matrices  e.g. low-rank  sparse or Toeplitz matrices  it is possible to have (cid:107)A(cid:107)mv = o(d2).

2.1 Chebyshev expansion
Let f : R → R be an analytic function on [a  b] for a  b ∈ R. Then  the Chebyshev series of f is
given by

(cid:18) 2

∞(cid:88)

j=0

f (x) =

bjTj

b − a

x − b + a
b − a

  bj =

(cid:19)

(cid:90) 1

f(cid:0) b−a

2 − 1j=0

π

−1

√

2 x + b+a
1 − x2

2

(cid:1) Tj(x)

dx.

In the above  1j=0 = 1 if j = 0 and 0 otherwise and Tj(x) is the Chebyshev polynomial (of the ﬁrst
kind) of degree j. An important property of the Chebyshev polynomials is the following recursive

2

j=0 bjTj( 2

(cid:80)n
Speciﬁcally  if f is analytic with(cid:12)(cid:12)f ( b−a

formula: Tj+1(x) = 2xTj(x) − Tj−1(x)  T1(x) = x  T0(x) = 1. The Chebyshev series can
be used to approximate f (x) via simply truncating the higher order terms  i.e.  f (x) ≈ pn(x) :=
b−a ). We call pn(x) the truncated Chebyhshev series of degree n. For analytic
functions  the approximation error (in the uniform norm) is known to decay exponentially [26].
the ellipse with foci +1 −1 and sum of major and minor semi-axis lengths equals to ρ > 1  then

2 )(cid:12)(cid:12) ≤ U for some U > 0 in the region bounded by

b−a x − b+a

2 z + b+a

|bj| ≤ 2U
ρj  

∀ j ≥ 0 

sup
x∈[a b]

|f (x) − pn(x)| ≤

4U

(ρ − 1) ρn .

(2)

2.2 Spectral-sums and their Chebyshev approximations
Given a matrix A ∈ S d×d and a function f : R → R  the spectral-sum of A with respect to f is

Σf (A) := tr (f (A)) =

f (λi) 

d(cid:88)

i=1

where tr (·) is the matrix trace and λ1  λ2  . . .   λd are the eigenvalues of A. Spectral-sums constitute
an important subclass of spectral functions  and many applications of spectral optimization involve
spectral-sums. This is fortunate since spectral-sums can be well approximated using Chebyshev
approximations.
For a general f  one needs all eigenvalues to compute Σf (A)  while for some functions  simpler
types of decomposition might sufﬁce (e.g.  log det A = Σlog(A) can be computed using the Cholesky
decomposition). Therefore  the general complexity of computing spectral-sums is O(d3)  which is
clearly not feasible when d is very large  as is common in many machine learning applications. Hence 
it is not surprising that recent literature proposed methods to approximate the large-scale spectral-
sums  e.g.  [11] recently suggested a fast randomized algorithm for approximating spectral-sums
based on Chebyshev series and Monte-Carlo trace estimators (i.e.  Hutchinson’s method [13]):

M(cid:88)

(cid:2)v(cid:62)pn(A)v(cid:3) ≈ 1
(cid:16) 2
b−a A − b+a
b−a I

k=1

M

v(k)(cid:62)

 n(cid:88)
(cid:17)

j=0

 (3)

bjw(k)

j

Σf (A) = tr (f (A)) ≈ tr (pn(A)) = Ev

(cid:16) 2
b−a A − b+a
b−a I

(cid:17)

j − w(k)
w(k)

1 =

j+1 = 2

v  w(k)

j−1  w(k)

where w(k)
0 = v(k)  and
{v(k)}M
k=1 are Rademacher random vectors  i.e.  each coordinate of v(k) is an i.i.d. random variable
in {−1  1} with equal probability 1/2 [13  2  24]. The approximation (3) can be computed using only
matrix-vector multiplications  vector-vector inner-products and vector-vector additions O(M n) times
each. Thus  the time-complexity becomes O(M n(cid:107)A(cid:107)mv + M nd) = O(M n(cid:107)A(cid:107)mv). In particular 
when M n (cid:28) d and (cid:107)A(cid:107)mv = o(d2)  the cost can be signiﬁcantly cheaper than O(d3) of exact
computation. We further note that to apply the approximation (3)  a bound on the eigenvalues is
necessary. For an upper bound  one can use fast power methods [6]; this does not hurt the total
algorithm complexity (see [10]). A lower bound can be encforced by substituting A with The lower
bound can typically be ensured A + εI for some small ε > 0. We use these techniques in our
numerical experiments.
We remark that one may consider other polynomial approximation schemes  e.g. Taylor  but we focus
on the Chebyshev approximations since they are nearly optimal in approximation among polynomial
series [19]. Another recently suggested powerful technique is stochastic Lanczos quadrature [27] 
however it is not suitable for our needs (our bias removal technique is not applicable for it).

3 Stochastic Chebyshev gradients of spectral-sums

Our main goal is to develop scalable methods for solving the following optimization problem:

min
θ∈C⊆Rd(cid:48) Σf (A(θ)) + g(θ) 

(4)

where C ⊆ Rd(cid:48)
parameter θ = [θi] ∈ Rd(cid:48)

is a non-empty  closed and convex domain  A : Rd(cid:48) → S d×d is a function of
and g : Rd(cid:48) → R is some function whose derivative with respect to

3

any parameter θ is computationally easy to obtain. Gradient-descent type methods are natural
candidates for tackling such problems. However  while it is usually possible to compute the gradient
of Σf (A(θ))  this is typically very expensive. Thus  we turn to stochastic methods  like (projected)
SGD [3  31] and SVRG [14  30]. In order to apply stochastic methods  one needs unbiased estimators
of the gradient. The goal of this section is to propose a computationally efﬁcient method to generate
unbiased stochastic gradients of small variance for Σf (A(θ)).

3.1 Stochastic Chebyshev gradients

approximation is exact  i.e.  f (x) = pn(x) =(cid:80)n

Biased stochastic gradients. We begin by observing that if f is a polynomial itself or the Chebyshev

∂
∂θi

Σpn (A) =

∂
∂θi

≈ 1
M

M(cid:88)

k=1

tr (pn(A)) =

∂
∂θi

Ev

v(k)(cid:62)pn(A)v(k) =

∂
∂θi

b−a )  we have

j=0 bjTj( 2

(cid:20) ∂
(cid:2)v(cid:62)pn(A)v(cid:3) = Ev
 n(cid:88)

b−a x − b+a
M(cid:88)

v(k)(cid:62)

∂θi

1
M

k=1

j=0

(cid:21)
 1 

v(cid:62)pn(A)v

∂w(k)
j
∂θi

bj

(5)

where {v(k)}M
recursive formula:

k=1 are i.i.d. Rademacher random vectors and ∂w(k)

j /∂θi are given by the following

j + 2(cid:101)A

∂w(k)
0
∂θi

∂w(k)
1
∂θi

∂w(k)
j+1
∂θi

∂w(k)
j
∂θi

− ∂w(k)
j−1
∂θi

 

=

=

∂θi

(6)

= 0 

v(k) 

w(k)

∂A
∂θi

2
b − a

4
b − a

i=1 (cid:107) ∂A

and (cid:101)A = 2
degree n can be computed in O(M n((cid:107)A(cid:107)mv d(cid:48) +(cid:80)d(cid:48)

∂A
∂θi
b−a A − b+a
b−a I. We note that in order to compute (6) only matrix-vector products with
A and ∂A/∂θi are needed. Thus  stochastic gradients of spectral-sums involving polynomials of
(cid:107)mv)). As we shall see in Section 5 
the complexity can be further reduced in certain cases. The above estimator can be leveraged to
approximate gradients for spectral-sums of analytic functions via the truncated Chebyshev series:
∇θΣf (A(θ)) ≈ ∇θΣpn (A(θ)). Indeed  [7] recently explored this in the context of Gaussian process
kernel learning. However  if f is not a polynomial  the truncated Chebyshev series pn is not equal
to f  so the above estimator is biased  i.e. ∇θΣf (A) (cid:54)= E[∇θv(cid:62)pn(A)v]. The biased stochastic
gradients might hurt iterative stochastic optimization as biased errors accumulate over iterations.
Unbiased stochastic gradients. The estimators (3) and (5) are biased since they approximate an
analytic function f via a polynomial pn of ﬁxed degree. Unless f is a polynomial itself  there
exists an x0 (usually uncountably many) for which f (x0) (cid:54)= pn(x0)  so if A has an eigenvalue at
x0 we have Σf (A) (cid:54)= Σpn(A). Thus  one cannot hope that the estimator (3)  let alone the gradient
estimator (5)  to be unbiased for all matrices A. To avoid deterministic truncation errors  we simply
randomize the degree  i.e.  design some distribution D on polynomials such that for every x we have
Ep∼D [p(x)] = f (x). This guarantees Ep∼D [tr (p(A))] = Σf (A) from the linearity of expectation.
We propose to build such a distribution on polynomials by using truncated Chebyshev expansions
where the truncation degree is stochastic. Let {qi}∞
i=0 ⊆ [0  1] be a set of numbers such that

i=0 qi = 1 and(cid:80)∞
(cid:80)∞

(cid:19)
i=r qi > 0 for all r ≥ 0. We now deﬁne for r = 0  1  . . .
(cid:98)pr (x) :=

(cid:18) 2

1 −(cid:80)j−1

x − b + a
b − a

r(cid:88)

b − a

bj
i=0 qi

Tj

.

j=0

i=0.
Next  let n be a random variable taking non-negative integer values  and deﬁned according to
of Σf (A) and ∇θΣf (A) as stated in the following lemma.

Note that(cid:98)pr (x) can be obtained from pr(x) by re-weighting each coefﬁcient according to {qi}∞
Pr(n = r) = qr. Under certain conditions on {qi} (cid:98)pn (·) can be used to derive unbiased estimators
Lemma 1 Suppose that f is an analytic function and(cid:98)pn is the randomized Chebyshev series of f in

(7). Assume that the entries of A are differentiable for θ ∈ C(cid:48)  where C(cid:48) is an open set containing
C  and that for a  b ∈ R all the eigenvalues of A(θ) for θ ∈ C(cid:48) are in [a  b]. For any degree
1We assume that all partial derivatives ∂Aj k/∂θi for j  k = 1  . . .   d  i = 1  . . .   d(cid:48) exist and are continuous.

(7)

4

distribution on non-negative integers {qi ∈ (0  1) :(cid:80)∞
limn→∞(cid:80)∞
i=n+1 qi(cid:98)pn (x) = 0 for all x ∈ [a  b]  it holds

(cid:2)v(cid:62)(cid:98)pn (A) v(cid:3) = Σf (A) 

Ev n

i=0 qi = 1 (cid:80)∞
(cid:2)∇θv(cid:62)(cid:98)pn (A) v(cid:3) = ∇θΣf (A).

Ev n

r=i qr > 0 ∀i ≥ 0} satisfying

(8)

where the expectations are taken over the joint distribution on random degree n and Rademacher
random vector v (other randomized probing vectors can be used as well).

The proof of Lemma 1 is given in the supplementary material. We emphasize that (8) holds for any
distribution {qi}∞
i=0 on non-negative integers for which the conditions stated in Lemma 1 hold  e.g. 
geometric  Poisson or negative binomial distribution.

3.2 Main result: optimal unbiased Chebyshev gradients

It is a well-known fact that stochastic gradient methods converge faster when the gradients have
smaller variances. The variance of our proposed unbiased estimators crucially depends on the choice
of the degree distribution  i.e.  {qi}∞
i=0. In this section  we design a degree distribution that is variance-
optimal in some formal sense. The variance of our proposed degree distribution decays exponentially
with the expected degree  and this is crucial for for the convergence analysis (Section 4).
The degrees-of-freedoms in choosing {qi}∞
i=0 is inﬁnite  which poses a challenge for devising low-
variance distributions. Our approach is based on the following simpliﬁed analytic approach studying
the scalar function f in such a way that one can naturally expect that the resulting distribution
{qi}∞
i=0 also provides low-variance for the matrix cases of (8). We begin by deﬁning the variance of
randomized Chebyshev expansion (7) via the Chebyshev weighted norm as
g( b−a
2 x + b+a
√
1 − x2

(cid:3)   where (cid:107)g(cid:107)2

VarC ((cid:98)pn) := En

(cid:2)(cid:107)(cid:98)pn − f(cid:107)2

(cid:90) 1

2 )2

C :=

(9)

dx.

C

−1

The primary reason why we consider the above variance is because by utilizing the orthogonality of
Chebyshev polynomials we can derive an analytic expression for it.
Lemma 2 Suppose {bj}∞

j=0 are coefﬁcients of the Chebyshev series for analytic function f and(cid:98)pn is
(cid:17)
(cid:16) (cid:80)j−1
1−(cid:80)j−1

its randomized Chebyshev expansion (7). Then  it holds that VarC ((cid:98)pn) = π

(cid:80)∞

j=1 b2
j

i=0 qi

2

.

i=0 qi

The proof of Lemma 2 is given in the supplementary material. One can observe from this result that the
variance reduces as we assign larger masses to to high degrees (due to exponentially decaying property
of bj (2)). However  using large degrees increases the computational complexity of computing the
estimators. Hence  we aim to design a good distribution given some target complexity  i.e.  the

expected polynomial degree N. Namely  the minimization of VarC ((cid:98)pn) should be constrained by
(cid:80)∞
i=1 iqi = N for some parameter N ≥ 0.
However  minimizing VarC ((cid:98)pn) subject to the aforementioned constraints might be generally in-
i=0 is inﬁnite and the algebraic structure of {bj}∞
tractable as the number of variables {qi}∞
i=0 is
arbitrary. Hence  in order to derive an analytic or closed-form solution  we relax the optimization.
In particular  we suggest the following optimization to minimize an upper bound of the variance by
utilizing |bj| ≤ 2U ρ−j from (2) as follows:

subject to

iqi = N 

qi = 1 and qi ≥ 0.

(10)

∞(cid:88)

j=1

ρ−2j

(cid:33)

(cid:32) (cid:80)j−1
1 −(cid:80)j−1

i=0 qi

i=0 qi

min
{qi}∞
i=0

∞(cid:88)

∞(cid:88)

i=1

i=0

j ≈ cρ−2j for constant c > 0 under f (x) = log x 
Figure 1(d) empirically demonstrates that b2
in which case the above relaxed optimization (10) is nearly tight. The next theorem establishes
that (10) has a closed-form solution  despite having inﬁnite degrees-of-freedom. The theorem
is applicable when knowing a ρ > 1 and a bound U such that the function f is analytic with

(cid:1)(cid:12)(cid:12) ≤ U in the complex region bounded by the ellipse with foci +1 −1 and sum of

(cid:12)(cid:12)f(cid:0) b−a

2 z + b+a

2

major and minor semi-axis lengths is equal to ρ > 1.

5

(a) f (x) = log(x)

(b) f (x) = x0.5

(c) f (x) = exp(x)

(d) coefﬁcients of log(x)

√
Figure 1: Chebyshev weighted variance for three distinct distributions: negative binomial (neg) 
Poisson (pois) and the optimal distribution (11) (opt) with the same mean N under (a) log x  (b)
x
on [0.05  0.95] and (c) exp(x) on [−1  1]  respectively. Observe that “opt” has the smallest variance
j and cρ−2j for some constant c > 0 and log x.
among all distributions. (d) Comparison between b2

Theorem 3 Let K = max{0  N −(cid:106) ρ
0

q∗
i =

(cid:107)}. The optimal solution {q∗

i }∞
for i < K
1 − (N − K) (ρ − 1)ρ−1
for i = K
(N − K)(ρ − 1)2ρ−i−1+N−K for i > K 

and it satisﬁes the unbiasedness condition in Lemma 1  i.e.  limn→∞(cid:80)∞

ρ−1

i(cid:98)pn (x) = 0.

i=n+1 q∗

i=0 of (10) is

(11)

i }   large degrees will be sampled with exponentially small probability.

The proof of Theorem 3 is given in the supplementary material. Observe that a degree smaller than K
is never sampled under {q∗
i }  which means that the corresponding unbiased estimator (7) combines
deterministic series of degree K with randomized ones of higher degrees. Due to the geometric decay
of {q∗
The optimality of the proposed distribution (11) (labeled opt) is illustrated by comparing it numerically
to other distributions: negative binomial (labeled neg) and Poisson (labeled pois)  on three analytic
functions: log x 
x and exp(x). Figures 1(a) to 1(c) show the weighted variance (9) of these
distributions where their means are commonly set from N = 5 to 100. Observe that the proposed
distribution has order-of-magnitude smaller variance compared to other tested distributions.

√

4 Stochastic Chebyshev gradient descent algorithms

In this section  we leverage unbiased gradient estimators based on (8) in conjunction with our optimal
degree distribution (11) to design computationally efﬁcient methods for solving (4). In particular  we
propose to randomly sample a degree n from (11) and estimate the gradient via Monte-Carlo method:

(cid:20) ∂

∂θi

(cid:21)

v(cid:62)(cid:98)pn (A) v

∂
∂θi

Σf (A) = E

M(cid:88)

≈ 1
M

v(k)(cid:62)

k=1

j=0

1 −(cid:80)j−1

bj
i=0 q∗

i

∂w(k)
j
∂θi

 n(cid:88)



(12)

where ∂w(k)

j /∂θi can be computed using a Rademacher vector v(k) and the recursive relation (6).

4.1 Stochastic Gradient Descent (SGD)

In this section  we consider the use of projected SGD in conjunction with (12) to numerically solve the
optimization (4). In the following  we provide a pseudo-code description of our proposed algorithm.

Algorithm 1 SGD for solving (4)
1: Input: number of iterations T   number of Rademacher vectors M  expected degree N and θ(0)
2: for t = 0 to T − 1 do
3:
4:
5:
6:
7: end for

(cid:0)ψ(t) + ∇g(θ(t))(cid:1)(cid:1)  where ΠC (·) is the projection mapping into C

Draw M Rademacher random vectors {v(k)}M
Compute ψ(t) from (12) at θ(t) using {v(k)}M
Obtain a proper step-size ηt

k=1 and a random degree n from (11) given N
k=1 and n

θ(t+1) ← ΠC(cid:0)θ(t) − ηt

6

020406080100expected degree N10-2010-10100weighted variancenegpoisopt020406080100expected degree N10-2010-10100weighted variancenegpoisopt020406080100expected degree N10-2010-10100weighted variancenegpoisopt020406080degree j10-4010-20100In order to analyze the convergence rate  we assume that (A0) all eigenvalues of A(θ) for θ ∈ C(cid:48) are in
the interval [a  b] for some open C(cid:48) ⊇ C  (A1) Σf (A(θ)) + g(θ) is continuous and α-strongly convex
with respect to θ and (A2) A(θ) is LA-Lipschitz for (cid:107)·(cid:107)F   g(θ) is Lg-Lipschitz and βg-smooth. The
formal deﬁnitions of the assumptions are in the supplementary material. These assumptions hold
for many target applications  including the ones explored in Section 5. In particular  we note that
assumption (A0) can be often satisﬁed with a careful choice of C. It has been studied that (projected)
SGD has a sublinear convergence rate for a smooth strongly-convex objective if the variance of
gradient estimates is uniformly bounded [23  21]. Motivated by this  we ﬁrst derive the following
upper bound on the variance of gradient estimators under the optimal degree distribution (11).
Lemma 4 Suppose that assumptions (A0)-(A2) hold and A(θ) is Lnuc-Lipschitz for (cid:107)·(cid:107)nuc. Let ψ
be the gradient estimator (12) at θ ∈ C using Rademacher vectors {v(k)}M
k=1 and degree n drawn
from the optimal distribution (11). Then  Ev n[(cid:107)ψ(cid:107)2
A/M + d(cid:48)L2
where C1  C2 > 0 are some constants independent of M  N.

(cid:1)(cid:0)C1 + C2N 4ρ−2N(cid:1)

2] ≤(cid:0)2L2

nuc

(cid:18)

(cid:18) 2L2

A
M

The above lemma allows us to provide a sublinear convergence rate for Algorithm 1.
Theorem 5 Suppose that assumptions (A0)-(A2) hold and A(θ) is Lnuc-Lipschitz for (cid:107)·(cid:107)nuc. If one
chooses the step-size ηt = 1/αt  then it holds that

(cid:19)(cid:18)

(cid:19)(cid:19)

E[(cid:107)θ(T ) − θ∗(cid:107)2

2] ≤ 4
α2T

max

L2
g 

+ d(cid:48)L2

nuc

C1 +

C2N 4
ρ2N

where C1  C2 > 0 are constants independent of M  N  and θ∗ ∈ C is the global optimum of (4).
The proofs of Lemma 4 and Theorem 5 are given in the supplementary material. Note that larger
M  N provide better convergence but they increase the computational complexity. The convergence
is also faster with smaller d(cid:48)  which is also evident in our experiments (see Section 5).
4.2 Stochastic Variance Reduced Gradient (SVRG)
In this section  we introduce a more advanced stochastic method using a further variance reduction
technique  inspired by the stochastic variance reduced gradient method (SVRG) [14]. The full
description of the proposed SVRG scheme for solving the optimization (4) is given below.

2: (cid:101)θ(1) ← θ(0)

Algorithm 2 SVRG for solving (4)
1: Input: number of inner/outer iterations T  S  number of Rademacher vectors M  expected degree

N  step-size η and initial parameter θ(0) ∈ C

for t = 0 to T − 1 do

Draw M Rademacher random vectors {v(k)}M

(cid:101)µ(s) ← ∇Σf (A((cid:101)θ(s))) and θ(0) ←(cid:101)θ(s)
Compute ψ(t) (cid:101)ψ(s) from (12) at θ(t) and(cid:101)θ(s)  respectively using {v(k)}M
(cid:101)θ(s+1) ← 1

(cid:17)(cid:17)
ψ(t) − (cid:101)ψ(s) +(cid:101)µ(s) + ∇g(θ(t))

3: for s = 1 to S do
4:
5:
6:
7:
8:
9:
10:
11: end for

(cid:16)
(cid:80)T

θ(t+1) ← ΠC

θ(t) − η

t=1 θ(t)

T

end for

(cid:16)

k=1 and a random degree n from (11)
k=1 and n

designed for optimizing ﬁnite-sum objectives  i.e. (cid:80)

The main idea of SVRG is to subtract a mean-zero random variable to the original stochastic gradient
estimator  where the randomness between them is shared. The SVRG algorithm was originally
i fi(x)  whose randomness is from the index
i. On the other hand  the randomness in our case is from polynomial degrees and trace probing
vectors for optimizing objectives of spectral-sums. This leads us to use the same randomness in
{v(k)}M
SGD  Algorithm 2 requires the expensive computation of exact gradients every T iterations. The next
theorem establishes that if one sets T correctly only O(1) gradient computations are required (for a
ﬁxed suboptimality) since we have a linear convergence rate.

k=1 and n for estimating both ψ(t) and (cid:101)ψ(s) in line 7 of Algorithm 2. We remark that unlike

7

(a)

(b)

(c)

(d)

Figure 2: Matrix completion results under (a) MovieLens 1M and (b) MovieLens 10M. (c) Algorithm
1 (SGD) in MovieLens 1M under other distributions such as negative binomial (neg) and Poisson
(pois). (d) SGD and SGD-DET under N = 10  30.
(cid:16) L4
Theorem 6 Suppose that assumptions (A0)-(A2) hold and A(θ) is βA-smooth for (cid:107)·(cid:107)F . Let β2 =
for some constants D1  D2 > 0 independent of M  N. Choose
2β2
g +
7β2 and T ≥ 25β2/α2. Then  it holds that
E[(cid:107)(cid:101)θ(S) − θ∗(cid:107)2
η = α

2] ≤ rSE[(cid:107)θ(0) − θ∗(cid:107)2
2] 

D1 + D2N 8
ρ2N

(cid:17)(cid:16)

A+β2
M + L4

A

A

(cid:17)

where 0 < r < 1 is some constant and θ∗ ∈ C is the global optimum of (4).

The proof of the above theorem is given in the supplementary material  where we utilize the recent
analysis of SVRG for the sum of smooth non-convex objectives [9  1]. The key additional component
in our analysis is to characterize β > 0 in terms of M  N so that the unbiased gradient estimator (12)
is β-smooth in expectation under the optimal degree distribution (11).

5 Applications

In this section  we apply the proposed methods to two machine learning tasks: matrix completion and
learning Gaussian processes. These correspond to minimizing spectral-sums Σf with f (x) = x1/2
and log x  respectively. We evaluate our methods under real-world datasets for both experiments.

5.1 Matrix completion
The goal is to recover a low-rank matrix θ ∈ [0  5]d×r when a few of its entries are given. Since the
rank function is neither differentiable nor convex  its relaxation such as Schatten-p norm has been
used in respective optimization formulations. In particular  we consider the smoothed nuclear norm
(i.e.  Schatten-1 norm) minimization [18  20] that corresponds to

min

θ∈[0 5]d×r

tr(A1/2) + λ

(θi j − Ri j)2

(cid:88)

(i j)∈Ω

where A = θθ(cid:62) + εI  R ∈ [0  5]d×r is a given matrix with missing entries  Ω indicates the positions
of known entries and λ is a weight parameter and ε > 0 is a smoothing parameter. Observe that
(cid:107)A(cid:107)mv = (cid:107)θ(cid:107)mv = O(dr)  and the derivative estimation in this case can be amortized to compute
using O(dM (N 2 + N r)) operations. More details on this and our experimental settings are given in
the supplementary material.
We use the MovieLens 1M and 10M datasets [12] (they correspond to d = 3  706 and 10  677 
respectively) and benchmark the gradient descent (GD)  Algorithm 1 (SGD) and Algorithm 2
(SVRG). We also consider a variant of SGD using a deterministic polynomial degree  referred as
SGD-DET  where it uses biased gradient estimators. We report the results for MovieLens 1M in
Figure 2(a) and 10M in 2(b). For both datasets  SGD-DET performs badly due to its biased gradient
estimators. On the other hand  SGD converges much faster and outperforms GD  where SGD for
10M converges much slower than that for 1M due to the larger dimension d(cid:48) = dr (see Theorem 5).
Observe that SVRG is the fastest one  e.g.  compared to GD  about 2 times faster to achieve RMSE
1.5 for MovieLens 1M and up to 6 times faster to achieve RMSE 1.8 for MovieLens 10M as shown
in Figure 2(b). The gap between SVRG and GD is expected to increase for larger datasets. We also
test SGD under other degree distributions: negative binomial (neg) and Poisson (pois) by choosing

8

0200400600elapsed time [sec]1.522.53test RMSEGDSGD-DETSGDSVRG050001000015000elapsed time [sec]1.522.53test RMSEGDSGD-DETSGDSVRG0204060number of iterations1.61.822.22.42.6test RMSESGD (neg)SGD (pois)SGD (opt)0204060number of iterations1.21.41.61.822.22.42.6test RMSESGD-DET  N=10SGD-DET  N=30SGD  N=10SGD  N=30parameters so that their means equal to N = 15. As reported in Figure 2(c)  other distributions have
relatively large variances so that they converge slower than the optimal distribution (opt). In Figure
2(d)  we compare SGD-DET with SGD of the optimal distribution under the (mean) polynomial
degrees N = 10  30. Observe that a larger degree (N = 30) reduces the bias error in SGD-DET 
while SGD achieves similar error regardless of the degree. The above results conﬁrm that the unbiased
gradient estimation and our degree distribution (11) are crucial for SGD.

5.2 Learning for Gaussian process regression

training data(cid:8)xi ∈ R(cid:96)(cid:9)d

Next  we apply our method to hyperparameter learning for Gaussian process (GP) regression. Given
i=1 with corresponding outputs y ∈ Rd  the goal of GP regression is to learn
a hyperparameter θ for predicting the output of a new/test input. The hyperparameter θ constructs
the kernel matrix A(θ) ∈ S d×d of the training data {xi}d
i=1 (see [22]). One can ﬁnd a good
hyperparameter by minimizing the negative log-marginal likelihood with respect to θ:

L := − log p(cid:0)y|{xi}d

i=1

(cid:1) =

y(cid:62)A(θ)−1y +

1
2

1
2

log det A(θ) +

n
2

log 2π.

For handling large-scale datasets  [29] proposed the structured kernel interpolation framework
assuming θ = [θi] ∈ R3 and

2 exp(cid:0)(cid:107)xi − xj(cid:107)2

2/2θ2
3

(cid:1)  

A(θ) = W KW (cid:62) + θ2

1I  Ki j = θ2

∂θi

where W ∈ Rd×r is some sparse matrix and K ∈ Rr×r is a dense kernel with r (cid:28) d. Speciﬁcally 
in [29]  r “inducing” points are selected and entries of W are computed via interpolation with the
inducing points. Under the framework  matrix-vector multiplications with A can be performed even
faster  requiring (cid:107)A(cid:107)mv = (cid:107)W(cid:107)mv + (cid:107)K(cid:107)mv = O(d + r2) operations. From (cid:107)A(cid:107)mv = (cid:107) ∂A
(cid:107)mv and
d(cid:48) = 3  the complexity for computing gradient estimation (12) becomes O(M N (d + r2)). If we
choose M  N  r = O(1)  the complexity reduces to O(d). The more detailed problem description
and our experimental settings are given in the supplementary material.
We benchmark GP regression under natural sound
dataset used in [29] and Szeged humid dataset [5]
where they correspond to d = 35  000 and 16  930 
respectively. Recently  [7] utilized an approximation
to derivatives of log-determinant based on stochastic
Lanczos quadrature [27] (LANCZOS). We compare
it with Algorithm 1 (SGD) which utilizes with un-
biased gradient estimators while SVRG requires the
exact gradient computation at least once which is in-
tractable to run in these cases. As reported in Figure
3  SGD converges faster than LANCZOS for both
datasets and it runs 2 times faster to achieve RMSE
0.0375 under sound dataset and under humid dataset
LANCZOS can be often stuck at a local optimum 
while SGD avoids it due to the use of unbiased gradi-
ent estimators.

Figure 3: Hyperparameter learning for Gaus-
sian process in modeling (a) sound dataset and
(b) Szeged humid dataset comparing SGD to
stochastic Lanczos quadrature (LANCZOS).

(b)

(a)

6 Conclusion

We proposed an optimal variance unbiased estimator for spectral-sums and their gradients. We
applied our estimator in the SGD and SVRG frameworks  and analyzed convergence. The proposed
optimal degree distribution is a crucial component of the analysis. We believe that the proposed
stochastic methods are of broader interest in many machine learning tasks involving spectral-sums.

Acknowledgement

This work was supported by the National Research Foundation of Korea(NRF) grant funded by the
Korea government(MSIT) (2018R1A5A1059921). Haim Avron acknowledges the support of the
Israel Science Foundation (grant no. 1272/17).

9

0100200elapsed time [sec]0.0380.0390.040.041test RMSELANCZOSSGD050100150elapsed time [sec]0.1670.1680.1690.170.171test RMSELANCZOSSGDReferences
[1] Allen-Zhu  Zeyuan and Yuan  Yang. Improved SVRG for non-strongly-convex or sum-of-non-
convex objectives. In International Conference on Machine Learning (ICML)  pp. 1080–1089 
2016.

[2] Avron  H. and Toledo  S. Randomized algorithms for estimating the trace of an implicit

symmetric positive semi-deﬁnite matrix. Journal of the ACM  58(2):8  2011.

[3] Bottou  Léon. Large-scale machine learning with stochastic gradient descent. In Proceedings of

COMPSTAT’2010  pp. 177–186. Springer  2010.

[4] Broniatowski  Michel and Celant  Giorgio. Some overview on unbiased interpolation and

extrapolation designs. arXiv preprint arXiv:1403.5113  2014.

[5] Budincsevity  Norbert. Weather in Szeged 2006-2016.

budincsevity/szeged-weather/data  2016.

https://www.kaggle.com/

[6] Davidson  Ernest R. The iterative calculation of a few of the lowest eigenvalues and correspond-
ing eigenvectors of large real-symmetric matrices. Journal of Computational Physics  17(1):
87–94  1975.

[7] Dong  Kun  Eriksson  David  Nickisch  Hannes  Bindel  David  and Wilson  Andrew G. Scalable
log determinants for Gaussian process kernel learning. In Advances in Neural Information
Processing Systems  pp. 6330–6340  2017.

[8] Friedlander  Michael P. and Macêdo  Ives. Low-rank spectral optimization via gauge duality.
SIAM Journal on Scientiﬁc Computing  38(3):A1616–A1638  2016. doi: 10.1137/15M1034283.
URL https://doi.org/10.1137/15M1034283.

[9] Garber  Dan and Hazan  Elad. Fast and simple PCA via convex optimization. arXiv preprint

arXiv:1509.05647  2015.

[10] Han  Insu  Malioutov  Dmitry  and Shin  Jinwoo. Large-scale log-determinant computation
through stochastic chebyshev expansions. In International Conference on Machine Learning 
pp. 908–917  2015.

[11] Han  Insu  Malioutov  Dmitry  Avron  Haim  and Shin  Jinwoo. Approximating spectral sums of
large-scale matrices using stochastic chebyshev approximations. SIAM Journal on Scientiﬁc
Computing  39(4):A1558–A1585  2017.

[12] Harper  F Maxwell and Konstan  Joseph A. The movielens datasets: History and context. Acm

transactions on interactive intelligent systems (tiis)  5(4):19  2016.

[13] Hutchinson  M.F. A stochastic estimator of the trace of the inﬂuence matrix for Laplacian
smoothing splines. Communications in Statistics-Simulation and Computation  18(3):1059–
1076  1989.

[14] Johnson  Rie and Zhang  Tong. Accelerating stochastic gradient descent using predictive
variance reduction. In Advances in Neural Information Processing Systems  pp. 315–323  2013.

[15] Koltchinskii  Vladimir and Xia  Dong. Optimal estimation of low rank density matrices. J.
Mach. Learn. Res.  16(1):1757–1792  January 2015. ISSN 1532-4435. URL http://dl.acm.
org/citation.cfm?id=2789272.2886806.

[16] Lee  Yin Tat  Sidford  Aaron  and Wong  Sam Chiu-wai. A faster cutting plane method and its
implications for combinatorial and convex optimization. In Foundations of Computer Science
(FOCS)  2015 IEEE 56th Annual Symposium on  pp. 1049–1065. IEEE  2015.

[17] Lewis  A. S. Derivatives of spectral functions. Mathematics of Operations Research  21(3):576–
588  1996. ISSN 0364765X  15265471. URL http://www.jstor.org/stable/3690298.

[18] Lu  Canyi  Lin  Zhouchen  and Yan  Shuicheng. Smoothed low rank and sparse matrix recovery
by iteratively reweighted least squares minimization. IEEE Transactions on Image Processing 
24(2):646–654  2015.

10

[19] Mason  John C and Handscomb  David C. Chebyshev polynomials. CRC Press  2002.

[20] Mohan  Karthik and Fazel  Maryam. Iterative reweighted algorithms for matrix rank minimiza-

tion. Journal of Machine Learning Research  13(Nov):3441–3473  2012.

[21] Nemirovski  Arkadi  Juditsky  Anatoli  Lan  Guanghui  and Shapiro  Alexander. Robust
stochastic approximation approach to stochastic programming. SIAM Journal on Optimization 
19(4):1574–1609  2009.

[22] Rasmussen  Carl Edward. Gaussian processes in machine learning. In Advanced Lectures on

Machine Learning  pp. 63–71. Springer  2004.

[23] Robbins  Herbert and Monro  Sutton. A stochastic approximation method. The Annals of

Mathematical Statistics  pp. 400–407  1951.

[24] Roosta-Khorasani  Farbod and Ascher  Uri. Improved bounds on sample size for implicit matrix

trace estimators. Foundations of Computational Mathematics  15(5):1187–1212  2015.

[25] Ryan P Adams  Jeffrey Pennington  Matthew J Johnson Jamie Smith Yaniv Ovadia Brian Patton
James Saunderson. Estimating the spectral density of large implicit matrices. arXiv preprint
arXiv:1802.03451  2018.

[26] Trefethen  Lloyd N. Approximation theory and approximation practice. SIAM  2013.

[27] Ubaru  Shashanka  Chen  Jie  and Saad  Yousef. Fast estimation of tr(f (a)) via stochastic
Lanczos quadrature. SIAM Journal on Matrix Analysis and Applications  38(4):1075–1099 
2017.

[28] Vinck  Martin  Battaglia  Francesco P  Balakirsky  Vladimir B  Vinck  AJ Han  and Pennartz 
Cyriel MA. Estimation of the entropy based on its polynomial representation. Physical Review
E  85(5):051139  2012.

[29] Wilson  Andrew and Nickisch  Hannes. Kernel interpolation for scalable structured Gaussian
processes (KISS-GP). In International Conference on Machine Learning  pp. 1775–1784  2015.

[30] Xiao  Lin and Zhang  Tong. A proximal stochastic gradient method with progressive variance

reduction. SIAM Journal on Optimization  24(4):2057–2075  2014.

[31] Zinkevich  Martin. Online convex programming and generalized inﬁnitesimal gradient ascent.
In Proceedings of the 20th International Conference on Machine Learning (ICML-03)  pp.
928–936  2003.

11

,Insu Han
Haim Avron
Jinwoo Shin