2009,AUC optimization and the two-sample problem,The purpose of the paper is to explore the connection between multivariate homogeneity tests and $\auc$ optimization. The latter problem has recently received much attention in the statistical learning literature. From the elementary observation that  in the two-sample problem setup  the null assumption corresponds to the situation where the area under the optimal ROC curve is equal to 1/2  we propose a two-stage testing method based on data splitting. A nearly optimal scoring function in the AUC sense is first learnt from one of the two half-samples. Data from the remaining half-sample are then projected onto the real line and eventually ranked according to the scoring function computed at the first stage. The last step amounts to performing a standard Mann-Whitney Wilcoxon  test in the one-dimensional framework. We show that the learning step of the procedure does not affect the consistency of the test as well as its properties in terms of power  provided the ranking produced is accurate enough in the AUC sense. The results of a numerical experiment are eventually displayed in order to show the efficiency of the method.,AUC optimization and the two-sample problem

St´ephan Cl´emenc¸on

Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141

stephan.clemencon@telecom-paristech.fr

Marine Depecker

Telecom Paristech (TSI) - LTCI UMR Institut Telecom/CNRS 5141

marine.depecker@telecom-paristech.fr

Nicolas Vayatis

ENS Cachan & UniverSud - CMLA UMR CNRS 8536

nicolas.vayatis@cmla.ens-cachan.fr

Abstract

The purpose of the paper is to explore the connection between multivariate ho-
mogeneity tests and AUC optimization. The latter problem has recently received
much attention in the statistical learning literature. From the elementary observa-
tion that  in the two-sample problem setup  the null assumption corresponds to the
situation where the area under the optimal ROC curve is equal to 1/2  we pro-
pose a two-stage testing method based on data splitting. A nearly optimal scoring
function in the AUC sense is ﬁrst learnt from one of the two half-samples. Data
from the remaining half-sample are then projected onto the real line and eventu-
ally ranked according to the scoring function computed at the ﬁrst stage. The last
step amounts to performing a standard Mann-Whitney Wilcoxon test in the one-
dimensional framework. We show that the learning step of the procedure does
not affect the consistency of the test as well as its properties in terms of power 
provided the ranking produced is accurate enough in the AUC sense. The results
of a numerical experiment are eventually displayed in order to show the efﬁciency
of the method.

1 Introduction

The statistical problem of testing homogeneity of two samples arises in a wide variety of appli-
cations  ranging from bioinformatics to psychometrics through database attribute matching for in-
stance. Practitioners may rely upon a wide range of nonparametric tests for detecting differences in
distribution (or location) between two one-dimensional samples  among which tests based on lin-
ear rank statistics  such as the celebrated Mann-Whitney Wilcoxon test. Being a (locally) optimal
procedure  the latter is the most widely used in homogeneity testing. Such rank statistics were orig-
inally introduced because they are distribution-free under the null hypothesis  thus permitting to set
critical values in a non asymptotic fashion for any given level. Beyond this simple fact  the cru-
cial advantage of rank-based tests relies in their asymptotic efﬁciency in a variety of nonparametric
situations. We refer for instance to [15] for an account of asymptotically (locally) uniformly most
powerful tests and a comprehensive treatment of asymptotic optimality of R-statistics.
In a different context  consider data sampled from a feature space X ⊂ Rd of high dimension with
binary label information in {−1  +1}. The problem of ranking such data  also known as the bipartite
ranking problem  has recently gained an increasing attention in the machine-learning literature  see

1

[5  10  19]. Here  the goal is to learn  based on a pooled set of labeled examples  how to rank
novel data with unknown labels  by means of a scoring function s : X → R  in order that positive
ones appear on top of the list. Over the last few years  this global learning problem has been the
subject of intensive research  involving issues related to the design of appropriate criteria reﬂecting
ranking performance or valid extensions of the Empirical Risk Minimization approach (ERM) to
this framework [2  6  11]. In most applications  the gold standard for measuring the capacity of a
scoring function s to discriminate between the class populations however remains the area under
the ROC curve criterion (AUC) and most ranking/scoring methods boil down to maximizing its
empirical counterpart. The empirical AUC may be viewed as the Mann-Whitney statistic based on
the images of the multivariate samples by s  see [13  9  12  18].
The purpose of this paper is to investigate how ranking methods for multivariate data with binary
labels may be exploited in order to extend the rank-based test approach for testing homogeneity
between two samples to a multidimensional setting. Precisely  the testing principle promoted in this
paper is described through an extension of the Mann-Whitney Wilcoxon test  based on a preliminary
ranking of the data through empirical AUC maximization. The consistency of the test is proved to
hold  as soon as the learning procedure is consistent in the AUC sense and its capacity to detect
”small” deviations from the homogeneity assumption is illustrated by a simulation example.
The rest of the paper is organized as follows. In Section 2  the homogeneity testing problem is
formulated and standard approaches are recalled  with focus on the one-dimensional case. Section
3 highlights the connection of the two-sample problem with optimal ROC curves and gives some
insight to our appproach. In Section 4  we describe the testing procedure proposed and set prelimi-
nary grounds for its theoretical validity. Simulation results are presented in Section 5 and technical
details are deferred to the Appendix.

2 The two-sample problem

. . .   X +

. . .   X−

We start off by setting out the notations needed throughout the paper and formulate the two-sample
problem precisely. We recall standard approaches to homogeneity testing. In particular  special
attention is paid to the one-dimensional case  for which two-sample linear rank statistics allow for
constructing locally optimal tests in a variety of situations.
Probabilistic setup. The problem considered in this paper is to test the hypothesis that two inde-
pendent i.i.d. random samples  valued in Rd with d ≥ 1  X +
n and X−
m are
1  
1  
identical in distributions. We denote by G(dx) the distribution function of the X +
i ’s  while the one
of the X−
j ’s is denoted by H(dx). We also denote by P(G H) the probability distribution on the
underlying space. The testing problem is tackled here from a nonparametric perspective  meaning
that the distributions G(dx) and H(dx) are assumed to be unknown. We suppose in addition that
G(dx) and H(dx) are continuous distributions and the asymptotics are described as follows: we set
N = m + n and suppose that n/N → p ∈ (0  1) as n  m tend to inﬁnity. Formally  the problem
is to test the null hypothesis H0 : G = H against the alternative H1 : G (cid:54)= H  based on the two
data sets. In this paper  we place ourselves in the difﬁcult case where G and H have same support 
X ⊂ Rd say.
Measuring dissimilarity. A possible approach is to consider a probability (pseudo)-metric D on the
space of probability distributions on Rd. Based on the simple observation that D(G  H) = 0 under

the null hypothesis  possible testing procedures consist of computing estimates (cid:98)Gn and (cid:98)Hm of the
underlying distributions and rejecting H0 for ”large” values of the statistic D((cid:98)Gn  (cid:98)Hm)  see [3] for

instance. Beyond computational difﬁculties and the necessity of identifying a proper standardization
in order to make the statistic asymptotically pivotal (i.e. its limit distribution is parameter free)  the
major issue one faces when trying to implement such plug-in procedures is related to the curse of
dimensionality. Indeed  plug-in procedures involve the consistent estimation of distributions on a
feature space of possibly very large dimension d ∈ N∗.
Various metrics or pseudo-metrics can be considered for measuring dissimilarity between two proba-
bility distributions. We refer to [17] for an excellent account of metrics in spaces of probability mea-
sures and their applications. Typical examples include the chi-square distance  the Kullback-Leibler
divergence  the Hellinger distance  the Kolmogorov-Smirnov distance and its generalizations of the

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)  

following type

f(x)G(dx) −

x∈X

f(x)H(dx)

MMD(G  H) = sup
f∈F

(1)
where F denotes a supposedly rich enough class of functions f : X ⊂ Rd → R  so that
MMD(G  H) = 0 if and only if G = H. The quantity (1) is called the Maximum Mean Dis-
crepancy in [1]  where a unit ball of a reproducing kernel Hilbert space H is chosen for F in order
to allow for efﬁcient computation of the supremum (1)  see also [23]. The view promoted in the
present paper for the two-sample problem is very different in nature and is inspired from traditional
procedures in the particular one-dimensional case.
The one-dimensional case. A classical approach to the two-sample problem in the one-dimensional
setup lies in ordering the observed data using the natural order on the real line R and then basing the
decision depending on the ranks of the positive instances among the pooled sample:

∀i ∈ {1  . . .   n}  Ri = N Fn m(X +
i ) 

where Fn m(t) = (n/N)(cid:98)Gn(t) + (m/N)(cid:98)Hm(t)  and denoting by (cid:98)Gn(t) = n−1(cid:80)
and (cid:98)Hn(t) = m−1(cid:80)

i ≤ t}
i ≤ t} the empirical counterparts of the cumulative distribution
functions G and H respectively. This approach is grounded in invariance considerations  practical
simplicity and optimality of tests based on R-estimates for this problem  depending on the class
of alternative hypotheses considered. Assuming the distributions G and H continuous  the idea
underlying such tests lies in the simple fact that  under the null hypothesis  the ranks of positive
instances are uniformly distributed over {1  . . .   N}. A popular choice is to consider the sum of
”positive ranks”  leading to the well-known rank-sum Wilcoxon statistic [22]

I{X−

I{X +

i≤n

i≤n

n(cid:88)

(cid:99)Wn m =

Ri 

i=1

which is distribution-free under H0  see Section 6.9 in [15] for further details. We also recall that 
the validity framework of the rank-sum test classically extends to the case where some observations
are tied (i.e. when G and/or H may be degenerate at some points)  by assigning the mean rank to ties
[4]. We shall denote by Wn m the distribution of the (average rank version of the) Wilcoxon statistic

(cid:99)Wn m under the homogeneity hypothesis. Since tables for the distributions Wn m are available  no
recalled below  the test based on the R-statistic(cid:99)Wn m has appealing optimality properties for certain

asymptotic approximation result is thus needed for building a test of appropriate level. As it will be
classes of alternatives. Although R-estimates (i.e. functions of the Ri’s) form a very rich collection
of statistics  but  for lack of space  we restrict our attention to the two-sample Wilcoxon statistic in
this paper.
Heuristics. We may now give a ﬁrst insight into the way we shall tackle the problem in the multi-
dimensional case. Suppose that we are able to ”project” the multivariate sampling data onto the real
line through a certain scoring function s : Rd → R in order to preserve the possible dissimilarity
(considered in a certain speciﬁc sense  which we shall discuss below) between the two populations 
leading then to ”large” values of the score s(x) for the positive instances and ”small” values for the
negative ones with high probability. Now that the dimension of the problem has been brought down
to 1  observations can be ranked and one may perform for instance a basic two-sample Wilcoxon
test based on the data sets s(X +

1 )  . . .   s(X−
m).

n ) and s(X−

1 )  . . .   s(X +

Remark 1 (LEARNING A STUDENT t TEST.) We point out that it is precisely the task Linear
Discriminant Analysis (LDA) tries to performs  in a restrictive Gaussian framework however (when
univariate Student t test based on the ”projected” data {(cid:98)δ(X +
G and H are normal distributions with same covariance structure namely). In order to test deviations
from the homogeneity hypothesis on the basis of the original samples  one may consider applying a
i ≤ m}  where(cid:98)δ denotes the empirical discriminant function  this may be shown as an appealing
i ) : 1 ≤

i ) : 1 ≤ i ≤ n} and {(cid:98)δ(X−

alternative to multivariate extensions of the standard t test [14].

The goal of this paper is to show how to exploit recent advances in ROC/AUC optimization for
extending this heuristics to more general situations than the parametric one mentioned above.

3

3 Connections with bipartite ranking

ROC curves are among the most widely used graphical tools for visualizing the dissimilarity be-
tween two one-dimensional distributions in a large variety of applications such as anomaly detection
in signal analysis  medical diagnosis  information retrieval  etc. As this concept is at the heart of
the ranking issue in the binary setting  which forms the ﬁrst stage of the testing procedure sketched
above  we recall its deﬁnition precisely.
Deﬁnition 1 (ROC curve) Let g and h be two cumulative distribution functions on R. The ROC
curve related to the distributions g(dt) and h(dt) is the graph of the mapping:

ROC ((g  h)  ·) : α ∈ [0  1] (cid:55)→ 1 − g ◦ h−1(1 − α) 

denoting by f−1(u) = inf{t ∈ R : f(t) ≥ u} the generalized inverse of any c`ad-l`ag function
f : R → R. When the distributions g(dt) and h(t) are continuous  it can alternatively be deﬁned as
the parametric curve t ∈ R (cid:55)→ (1 − h(t)  1 − g(t)).
One may show that ROC ((g  h)  ·) is above the diagonal ∆ : α ∈ [0  1] (cid:55)→ α of the ROC space if
and only if the distribution g is stochastically larger than h and it is concave as soon as the likelihood
ratio dg/dh is increasing. When g(dt) and h(dt) are both continuous  the curves ROC((g  h)  .) and
ROC((h  g)  .) are symmetric with respect to the diagonal of the ROC space with slope equal to one.
Refer to [9] for a detailed list of properties of ROC curves.
The notion of ROC curve provides a functional measure of dissimilarity between distributions on
R: the closer to the corners of the unit square the curve ROC ((g  h)  ·) is  the more dissimilar the
distributions g and h are. For instance  it exactly coincides with the upper left-hand corner of the unit
square  namely the curve α ∈ [0  1] (cid:55)→ I{α ∈]0  1]}  when there exists l ∈ R such that the support
of distribution g(dt) is a subset of [l  ∞[  while ]l −∞  ] contains the support of h. In contrast  it
merges with the diagonal ∆ when g = h. Hence  distance of ROC ((g  h)  ·) to the diagonal may
be naturally used to quantify departure from the homogeneous situation. The L1-norm provides a
convenient way of measuring such a distance  leading to the classical AUC criterion (AUC standing
for area under the ROC curve):

AUC(g  h) =

ROC ((g  h)  α) dα.

α=0

The popularity of this summary quantity arises from the fact that it can be interpreted in a proba-
bilistic fashion  and may be viewed as a distance between the locations of the two distributions. In
this respect  we recall the following result.
Proposition 1 Let g and h be two distributions on R. We have:
1
P{Z = Z(cid:48)} =
2

AUC(g  h) = P{Z > Z(cid:48)} +

+ E[h(Z)] − E[g(Z(cid:48))] 

1
2

where Z and Z(cid:48) denote independent random variables  drawn from g(dt) and h(dt) respectively.

We recall that the homogeneous situation corresponds to the case where AUC(g  h) = 1/2 and the
Mann-Withney statistic [16]

(cid:90) 1

n(cid:88)

m(cid:88)

(cid:18)

i=1

j=1

Un m =

1
nm

I{X−

j < X +

i } +

1
2

I{X−

(cid:19)
i }
j = X +

is exactly the empirical counterpart of AUC(g  h). It yields exactly the same statistical decisions as
the two-sample Wilcoxon statistic  insofar they are related as follows:

Wn m = nm(cid:98)Un m + n(n + 1)/2.

For this reason  the related test of hypotheses is called Mann-Whitney Wilcoxon test (MWW).
Multidimensional extension. In the multivariate setup  the notion of ROC curve can be extended
the following way. Let H(dx) and G(dx) be two given distributions on Rd and S = {s : X → R |

4

s Borel measurable}. For any scoring function s ∈ S  we denote by Hs(dt) and Gs(t) the images
of H(dx) and G(x) by the mapping s(x). In addition  we set for all s ∈ S:

ROC(s  .) = ROC((Gs  Hs)  .) and AUC(s) = AUC(Gs  Hs).

Clearly  the families of univariate distributions {Gs}s∈S and {Hs}s∈S entirely characterize the
multivariate probability measures G and H. One may thus consider evaluating the dissimilarity
between H(dx) and G(dx) on Rd through the family of curves {ROC(s  .)}s∈S or through the
collection of scalar values {AUC(s)}s∈S. Going back to the homogeneity testing problem  the null
assumption may be reformulated as

”H0 : ∀s ∈ S  AUC(s) = 1/2” versus ”H1 : ∃s ∈ S such that AUC(s) > 1/2”.

The next result  following from standard Neyman-Pearson type arguments  shows that the supremum
sups∈S AUC(s) is attained by increasing transforms of the likelihood ratio φ(x) = dG/dH(x) 
x ∈ X . Scoring functions with largest AUC are natural candidates for detecting the alternative H1.
Theorem 1 (OPTIMAL ROC CURVE.) The set of S∗ = {T ◦ φ | T : R → R strictly increasing }
deﬁnes the collection of optimal scoring functions in the sense that: ∀s ∈ S 

∀α ∈ [0  1]  ROC(s  α) ≤ ROC∗(α) and AUC(s) ≤ AUC∗ 
with the notations ROC∗(.) = ROC(s∗  .) and AUC∗ = AUC(s∗) for s∗ ∈ S∗.

Refer to Proposition 4’s proof in [9] for a detailed argument. Notice that  as dG/dH(X) =
dGφ(X)/dHφ(φ(X))  replacing X by s∗(X) with s∗ ∈ S∗ leaves the optimal ROC curve un-
touched. The following corollary is straightforward.
Corollary 1 For any s ∈ S∗  we have: sups∈S |AUC(s) − 1/2| = AUC(s∗) − 1/2.
Consequently  the homogeneity testing problem may be seen as closely related to the problem of
estimating the optimal AUC∗  since it may be re-formulated as follows:

”H0 : AUC∗ = 1/2” versus ”H1 : AUC∗ > 1/2”.

Knowing how a single optimal scoring function s∗ ∈ S∗ ranks observations drawn from a mixture
of G and H is sufﬁcient for detecting departure from the homogeneity hypothesis in an optimal fash-
ion  the MWW statistic computed from the (s∗(X +
j ))’s being an asymptotically efﬁcient
estimate of AUC∗ and thus yields an asymptotically (locally) uniformly most powerful test.
Let F (dx) = pG(dx) + (1 − p)H(dx) and denote by Fs(dt) the image of the distribution F by
s ∈ S. Notice that  for any s∗ ∈ S∗  the scoring function S∗ = Fs∗ ◦ s∗ is still optimal and the
score variable S∗(X) is uniformly distributed on [0  1] under the mixture distribution F (in addition 
it may be easily shown to be independent from s∗ ∈ S∗). Observe in addition that AUC∗ − 1/2
may be viewed as the Earth Mover’s distance between the class distributions HS∗ and GS∗ for this
”normalization”:

i )  s∗(X−

(cid:90) 1

AUC∗ − 1/2 =

{HS∗(t) − GS∗(t)} dt.

t=0

Empirical AUC maximization. A natural way of inferring the value of AUC∗ and/or selecting
a scoring function ˆs with AUC nearly as large as AUC∗ is to maximize an empirical version of
the AUC criterion over a set S0 of scoring function candidates. We assume that the class S0 is
sufﬁciently rich in order to guarantee that the bias AUC∗ − sups∈S0 AUC(s) is small  and its com-
plexity is controlled (when measured for instance by the VC dimension of the collection of sets
{{x ∈ X : s(x) ≥ t}  (s  t) ∈ S0 × R} as in [7] or by the order of magnitude of conditional
Rademacher averages as in [6]). We recall that  under such assumptions  universal consistency
results have been established for empirical AUC maximizers  together with distribution-free gener-
alization bounds  see [2  6] for instance. We point out that this approach can be extended to other
relevant ranking criteria. The contours of a theory guaranteeing the statistical performance of the
ERM approach for empirical risk functionals deﬁned by R-estimates have been sketched in [8].

5

“Pn1

i=1

−1
1

where bFˆs(t) = N
wherecWn1 m1 =Pn1

bRi = N1 ˆS(X +

I{ˆs(X +

n0+i) for 1 ≤ i ≤ n1 
−

n0+i) ≤ t} +Pm1
cWn1 m1 ≥ Qn1 m1 (α) 

I{ˆs(X

j=1

2. Rank-sum Wilcoxon test. Reject the homogeneity hypothesis H0 when:

m0+j) ≤ t}”

and ˆS = bFˆs ◦ ˆs.

4 The two-stage testing procedure

. . .   X +

n0} ∪ {X−
1  

the ﬁrst data set Dn0 m0 =
into two subsamples:
Assume that data have been split
{X +
m0} will be used for deriving a scoring function on X and
. . .   X−
1  
m0+m1} will serve to
n0+n1} ∪ {X−
n1 m1 = {X +
the second data set D(cid:48)
compute a pseudo- two-sample Wilcoxon test statistic from the ranked data. We set N0 = n0 + m0
and N1 = n1 + m1 and suppose that ni/Ni → p as ni and mi tend to inﬁnity for i ∈ {0  1}.
Let α ∈ (0  1). The testing procedure at level α is then performed in two steps  as follows.

m0+1  . . .   X−

n0+1  . . .   X +

SCORE-BASED RANK-SUM WILCOXON TEST

1. Ranking. From dataset Dn0 m0  perform empirical AUC maximization over S0 ⊂ S  yielding
the scoring function ˆs(x) = ˆsn0 m0 (x). Compute the ranks of data with positive labels among
the sample D(cid:48)

n1 m1  once sorted by increasing order of magnitude of their score:

i=1 bRi and Qn1 m1 (α) denotes the (1−α)-quantile of distribution Wn1 m1.

The next result shows that the learning step does not affect the consistency property  provided it
outputs a universally consistent scoring rule.
Theorem 2 Let α ∈ (0  1/2) and suppose that the ranking/scoring method involved at step 1 yields
a universally consistent scoring rule ˆs in the AUC sense. The score-based rank-sum Wilcoxon test

Φ = I(cid:110)(cid:99)Wn1 m1 ≥ Qn1 m1(α)

(cid:111)

is universally consistent as ni and mi tend to ∞ for i ∈ {0  1} at level α  in the following sense.

1. It is of level α for all ni and mi  i ∈ {0  1}: P(H H) {Φ = +1} ≤ α for any H(dx).
2. Its power converges to 1 as ni and mi  i ∈ {0  1}  tend to inﬁnity for every alternative:

limni  mi→∞ P(G H) {Φ = +1} = 1 for every pair of distinct distributions (G  H).

Remark 2 (CONVERGENCE RATES.) Under adequate complexity assumptions on the set S0 over
which empirical AUC maximization or one of its variants is performed  distribution-free rate bounds
for the generalization ability of scoring rules may be established in terms of AUC  see Corollary 6 in
[2] or Corollary 3 in [6]. As shown by a careful examination of Theorem 2  this permits to derive a
convergence rate for the decay of the score-based type II error of MWW under any given alternative
√
(G  H)  when combined with the Berry-Esseen theorem for two-sample U-statistics. For instance 
N0 rate bound holds for ˆs(x)  one may show that choosing N1 ∼ N0 then yields a
√
if a typical 1/
rate of order OP(G H)(1/
Remark 3 (INFINITE-DIMENSIONAL FEATURE SPACE.) We point out that the method presented
here is by no means restricted to the case where X is of ﬁnite dimension  but may be applied to
functional input data  provided an AUC-consistent ranking procedure can be applied in this context.

N0).

5 Numerical examples

The procedure proposed above is extremely simple once the delicate AUC maximization stage is
performed. A stunning property is the fact that critical thresholds are set automatically  with no ref-
erence to the data. We ﬁrts consider a low-dimensional toy experiment and display some numerical
results. Two independent i.i.d. samples of equal size m = n = N/2 have been generated from
two conditional 4-dimensional gaussian distributions on the hypercube [−2  2]4. Their parameters

6

are denoted by µ+ and µ− for the means and Γ is their common covariance matrix. Three cases
have been considered. The ﬁrst example corresponds to a homogeneous situation: µ+ = µ− = µ1
where µ1 = (−0.96 −0.83  0.29 −1.34) and the upper diagonals of Γ1 are (6.52  3.84  4.72  3.1) 
(−1.89  3.56  1.52)  (−3.2  0.2) and (−2.6). In the second example  we test homogeneity under an
alternative  ”fairly far” from H0  where µ− = µ1  µ+ = (0.17 −0.24  0.04 −1.02) and Γ as before.
Eventually  the third example corresponds to a much more difﬁcult problem  ”close” to H0  where
µ− = (1.19 −1.20 −0.02 −0.16)  µ+ = (1.08 −1.18 −0.1 −0.06) and the upper diagonals of
Γ are (1.83  6.02  0.69  4.99)  (−0.65 −0.31  1.03)  (−0.54 −0.03) and (−1.24). The difﬁculty of
each of these examples is illustrated by Fig. 2 in terms of (optimal) ROC curve. The table in Fig.
2 gives Monte-Carlo estimates of the power of three testing procedures when α = 0.05 (averaged
over B = 150 replications): 1) the score-based MWW test  where ranking is performed using the
scoring function output by a run of the TREERANK algorithm [9] on a training sample Dn0 m0  2)
the LDA-based Student test sketched in Remark 1 and 3) a bootstrap version of the MMD-test with
a Gaussian RBF Kernel proposed in [1].

DataSet
Ex. 1
Ex. 2

Ex. 3

Sample size (m0 m1)

LDA-Student

(500 500)
(500 500)
(2000 1000)
(3000 2000)

6%
99%
75%
98%

Score-based MWW MMD
5%
99%
30%
65%

1%
99%
45%
73%

Figure 1: Powers and ROC curves describing the ”distance” to H0 for each situation: example 1
(red)  example 2 (black) and example 3 (blue).

In the second series of experimental results  gaussian distributions with same covariance matrix
on Rd are generated  with larger values for the input space dimension d ∈ {10  30}. We
considered several problems at given toughness. The increasing difﬁculty of the testing problems
considered is controlled through the euclidian distance between the means ∆µ = ||µ+ − µ−|| and
is described by Fig. 2  which depicts the related ROC curves  corresponding to situations where
∆µ ∈ {0.2  0.1  0.08  0.05}. On these examples  we compared the performance of four methods at
level α = 0.05: the score-based MWW test  where ranking is again performed using the scoring
function output by a run of the TREERANK algorithm on a training sample Dn0 m0  the KFDA
test proposed in [23]  a bootstrap version of the MMD-test with a Gaussian RBF Kernel (M M D)
and another version  with moment matching to Pearson curves (M M Dmom)  using also with a
Gaussian RBF kernel (see [1]). Monte-Carlo estimates of the corresponding powers are given in the
Table displayed in Fig. 2.

6 Conclusion

We have provided a sound strategy  involving a preliminary bipartite ranking stage  to extend clas-
sical approaches for testing homogeneity based on ranks to a multidimensional setup. Consistency
of the extended version of the popular MWW test has been established  under the assumption of
universal consistency of the ranking method in the AUC sense. This principle can be applied to
other R-statistics  standing as natural criteria for the bipartite ranking problem [8]. Beyond the illus-
trative preliminary simulation example displayed in this paper  we intend to investigate the relative
efﬁciency of such tests with respect to other tests standing as natural candidates in this setup.

Appendix - Proof of Theorem 2

Observe that  conditioned upon the ﬁrst sample Dn0 m0  the statistic(cid:99)Wn1 m1 is distributed according
to Wn1 m1 under the null hypothesis. For any distribution H  we thus have: ∀α ∈ (0  1/2) 

(cid:110)(cid:99)Wn1 m1 > Qn1 m1(α) | Dn0 m0

(cid:111) ≤ α.

P(H H)

Taking the expectation  we obtain that the test is of level α for all n  m.

7

Dim. d M M Dboot M M Dmom Kfda

Sc.based MWW

d = 10
d = 30

d = 10
d = 30

d = 10
d = 30

d = 10
d = 30

case 1 :∆µ = 0.2

86%
58%

64%
36%

case 1 :∆µ = 0.1

20%
7%

20%
15%

case 3 :∆µ = 0.08

19%
7%

16%
9%

case 4 :∆µ = 0.05

13%
6%

13%
8%

86%
54%

20%
9%

19%
5%

11%
6%

90%
85%

58%
47%

42%
32%

18%
16%

Figure 2: Power estimates and ROC curves describing the ”distance” to H0 for each situation: case
1 (black)  case 2 (blue)  case 3 (green) and case 4 (red).

For any s ∈ S  denote by Un1 m1(s) the empirical AUC of s evaluated on the sample D(cid:48)
Recall ﬁrst that it follows from the two-sample U-statistic theorem (see [20]) that:

n1 m1.

i+n0)) − E[Hs(s(X+

j+m0)) − E[Gs(s(X−

1 ))](cid:9)
1 ))](cid:9) + oP(G H)(1) 

√
N{Un1 m1(s) − AUC(s)} =

√
N1
n1
√
N1
m1

−

n1(cid:88)
(cid:8)Hs(s(X+
m1(cid:88)
(cid:8)Gs(s(X−

i=1

j=1

√
N{Un1 m1(s) − AUC(s)} is asymptotically normal with limit variance σ2
1 )))/p+Var(Gs(s(X−

as n  m tend to inﬁnity. In particular  for any pair of distributions (G  H)  the centered random
s(G  H) =
variable
s(H  H) = 1/(12p(1−
Var(Hs(s(X +
p)) for any s ∈ S such that the distribution Hs(dt) is continuous. Refer to Theorem 12.4 in [21] for
further details.
(G  H)  so that AUC∗ > 1/2. Setting (cid:98)Un1 m1 = Un1 m1(ˆs) and decomposing AUC∗ −(cid:98)Un1 m1 as
We now place ourselves under an alternative hypothesis described by a pair of distinct distribution
the sum of the deﬁcit of AUC of ˆs(x)  AUC∗−AUC(ˆs) namely  and the deviation AUC(ˆs)−(cid:98)Un1 m1

1 )))/(1−p) under P(G H). Notice that σ2

(cid:110)(cid:99)Wn1 m1 ≤ Qn1 m1(α)
(cid:111)

may

n1 m1  type II error of Φ given by P(G H)

evaluated on the sample D(cid:48)
be bounded by:

P(G H)

N1

where

(cid:110)(cid:112)
(cid:16)(cid:98)Un1 m1 − AUC((cid:98)s)
n1 m1(α) =(cid:112)

(cid:17) ≤ n1 m1(α)
(cid:111)
(cid:110)(cid:112)
N1 (AUC((cid:98)s) − AUC∗) ≤ n1 m1(α)
(cid:18) Qn1 m1(α)
− 1
2

converges to zα/(cid:112)12p(1 − p). Now  the fact that type II error of Φ converges to zero as ni and

− n1 + 1
2m1
√
N1(Qn1 m1(α)/(n1m1) − (n1 + 1)/(2m1))
Observe that  by virtue of the CLT recalled above 
mi tend to ∞ for i ∈ {0  1} immediately follows from the assumption in regards to the AUC of
ˆs(x) universal consistency and the CLT for two-sample U-statistics combined with the theorem of
dominated convergence. Due to space limitations  details are omitted.

N1(AUC∗ − 1
2

+ P(G H)

(cid:19)

−(cid:112)

(cid:111)

 

n1m1

N1

).

8

References
[1] M.J. Rasch B. Scholkopf A. Smola A. Gretton  K.M. Borgwardt. A kernel method for the two-sample
problem. In Advances in Neural Information Processing Systems 19. MIT Press  Cambridge  MA  2007.
[2] S. Agarwal  T. Graepel  R. Herbrich  S. Har-Peled  and D. Roth. Generalization bounds for the area under

the ROC curve. J. Mach. Learn. Res.  6:393–425  2005.

[3] G. Biau and L. Gyorﬁ. On the asymptotic properties of a nonparametric l1-test statistic of homogeneity.

IEEE Transactions on Information Theory  51(11):3965–3973  2005.

[4] Y.K. Cheung and J.H. Klotz. The Mann Whitney Wilcoxon distribution using linked list. Statistica Sinica 

7:805–813  1997.

[5] S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and scoring using empirical risk minimization. In
P. Auer and R. Meir  editors  Proceedings of COLT 2005  volume 3559 of Lecture Notes in Computer
Science  pages 1–15. Springer  2005.

[6] S. Cl´emenc¸on  G. Lugosi  and N. Vayatis. Ranking and empirical risk minimization of U-statistics. The

Annals of Statistics  36(2):844–874  2008.

[7] S. Cl´emenc¸on and N. Vayatis. Ranking the best instances. Journal of Machine Learning Research 

8:2671–2699  2007.

[8] S. Cl´emenc¸on and N. Vayatis. Empirical performance maximization based on linear rank statistics. In
Advances in Neural Information Processing Systems  volume 3559 of Lecture Notes in Computer Science 
pages 1–15. Springer  2009.

[9] S. Cl´emenc¸on and N. Vayatis. Tree-based ranking methods. IEEE Transactions on Information Theory 

55(9):4316–4336  2009.

[10] W.W. Cohen  R.E. Schapire  and Y. Singer. Learning to order things. In NIPS ’97: Proceedings of the
1997 conference on Advances in neural information processing systems 10  pages 451–457  Cambridge 
MA  USA  1998. MIT Press.

[11] C. Cortes and M. Mohri. AUC optimization vs. error rate minimization.

In S. Thrun  L. Saul  and
B. Sch¨olkopf  editors  Advances in Neural Information Processing Systems 16. MIT Press  Cambridge 
MA  2004.

[12] C. Ferri  P.A. Flach  and J. Hern´andez-Orallo. Learning decision trees using the area under the roc curve.
In ICML ’02: Proceedings of the Nineteenth International Conference on Machine Learning  pages 139–
146  2002.

[13] Y. Freund  R. D. Iyer  R. E. Schapire  and Y. Singer. An efﬁcient boosting algorithm for combining

preferences. Journal of Machine Learning Research  4:933–969  2003.

[14] S. Kotz and S. Nadarajah. Multivariate t Distributions and Their Applications. Cambridge University

Press  2004.

[15] E.L. Lehmann and J. P. Romano. Testing Statistical Hypotheses. Springer  2005.
[16] H.B. Mann and D.R. Whitney. On a test of whether one of two random variables is stochastically larger

than the other. Ann. Math. Stat.  18:50–60  1947.

[17] A. Rachev. Probability Metrics and the Stability of Stochastic Models. Wiley  1991.
[18] A. Rakotomamonjy. Optimizing Area Under Roc Curve with SVMs. In Proceedings of the First Workshop

on ROC Analysis in AI  2004.

[19] C. Rudin  C. Cortes  M. Mohri  and R. E. Schapire. Margin-based ranking and boosting meet in the
middle. In P. Auer and R. Meir  editors  Proceedings of COLT 2005  volume 3559 of Lecture Notes in
Computer Science  pages 63–78. Springer  2005.

[20] R.J. Serﬂing. Approximation theorems of mathematical statistics. Wiley  1980.
[21] A.K. van der Vaart. Asymptotic Analysis. Cambridge University Press  1998.
[22] F. Wilcoxon. Individual comparisons by ranking methods. Biometrics  1:80–83  1945.
[23] E. Moulines Z. Harchaoui  F. Bach. Testing for homogeneity with kernel Fischer discriminant analysis.

In Advances in Neural Information Processing Systems 20. MIT Press  Cambridge  MA  2008.

9

,Sivan Sabato
Anand Sarwate
Nati Srebro
Hemant Tyagi
Bernd Gärtner
Andreas Krause
Xiangyu Wang
Fangjian Guo
Katherine Heller
David Dunson
Daniel Ritchie
Anna Thomas
Pat Hanrahan
Noah Goodman