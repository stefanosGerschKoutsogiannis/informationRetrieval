2019,Riemannian batch normalization for SPD neural networks,Covariance matrices have attracted attention for machine learning applications due
to their capacity to capture interesting structure in the data. The main challenge
is that one needs to take into account the particular geometry of the Riemannian
manifold of symmetric positive definite (SPD) matrices they belong to. In the con-
text of deep networks  several architectures for these matrices have recently been
proposed. In our article  we introduce a Riemannian batch normalization (batch-
norm) algorithm  which generalizes the one used in Euclidean nets. This novel
layer makes use of geometric operations on the manifold  notably the Riemannian
barycenter  parallel transport and non-linear structured matrix transformations. We
derive a new manifold-constrained gradient descent algorithm working in the space
of SPD matrices  allowing to learn the batchnorm layer. We validate our proposed
approach with experiments in three different contexts on diverse data types: a
drone recognition dataset from radar observations  and on emotion and action
recognition datasets from video and motion capture data. Experiments show that
the Riemannian batchnorm systematically gives better classification performance
compared with leading methods and a remarkable robustness to lack of data.,Riemannian batch normalization for SPD neural

networks

Daniel Brooks

Thales Land and Air Systems  BU ARC

Limours  FRANCE

Sorbonne Université  CNRS  LIP6

Paris  FRANCE

Sorbonne Université  CNRS  LIP6

Thales Land and Air Systems  BU ARC

Thales Land and Air Systems  BU ARC

Sorbonne Université  CNRS  LIP6

Olivier Schwander

Paris  FRANCE

Jean-Yves Schneider

Limours  FRANCE

Frédéric Barbaresco

Limours  FRANCE

Matthieu Cord

Paris  FRANCE

Abstract

Covariance matrices have attracted attention for machine learning applications due
to their capacity to capture interesting structure in the data. The main challenge
is that one needs to take into account the particular geometry of the Riemannian
manifold of symmetric positive deﬁnite (SPD) matrices they belong to. In the con-
text of deep networks  several architectures for these matrices have recently been
proposed. In our article  we introduce a Riemannian batch normalization (batch-
norm) algorithm  which generalizes the one used in Euclidean nets. This novel
layer makes use of geometric operations on the manifold  notably the Riemannian
barycenter  parallel transport and non-linear structured matrix transformations. We
derive a new manifold-constrained gradient descent algorithm working in the space
of SPD matrices  allowing to learn the batchnorm layer. We validate our proposed
approach with experiments in three different contexts on diverse data types: a
drone recognition dataset from radar observations  and on emotion and action
recognition datasets from video and motion capture data. Experiments show that
the Riemannian batchnorm systematically gives better classiﬁcation performance
compared with leading methods and a remarkable robustness to lack of data.

1

Introduction and related works

Covariance matrices are ubiquitous in any statistical related ﬁeld but their direct usage as a representa-
tion of the data for machine learning is less common. However  it has proved its usefulness in a variety
of applications: object detection in images [46]  analysis of Magnetic Resonance Imaging (MRI)
data [41]  classiﬁcation of time-series for Brain-Computer Interfaces [8] (BCI). It is particularly
interesting in the case of temporal data since a global covariance matrix is a straightforward way
to capture and represent the temporal ﬂuctuations of data points of different lengths. The main
difﬁculty is that these matrices  which are symmetric positive deﬁnite (SPD)  cannot be seen as
points in a Euclidean space: the set of SPD matrices is a curved Riemannian manifold  thus tools
from non-Euclidean geometry must be used; see [10] for a plethora of theoretical justiﬁcations and

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

properties on the matter. For this reason most of classiﬁcation methods (which implicitly make the
hypothesis of a Euclidean input space) cannot be used successfully.
Interestingly  relatively simple machine learning techniques can produce state-of-art results as soon as
the particular Riemannian geometry is taken into account. This is the case for BCI: [8  7] use nearest
barycenter (but with Riemannian barycenter) and SVM (but on the tangent space of the barycenter of
the data points) to successfully classify covariances matrices computed on electroencephalography
multivariate signals (EEG); in the same ﬁeld [51] propose kernel methods for metric learning on the
SPD manifold . Another example is in MRI  where [41  4] develop a k-nearest neighbors algorithm
using a Riemannian distance. Motion recognition from motion skeletal data also beneﬁts from
Riemannian geometry  as exposed in [16]  [30] and [29]. In the context of neural networks  an
architecture (SPDNet) speciﬁcally adapted for these matrices has been proposed [28]. The overall
aspect is similar to a classical (Euclidean) network (transformations  activations and a ﬁnal stage of
classiﬁcation) but each layer processes a point on the SPD manifold; the ﬁnal layer transforms the
feature manifold to a Euclidean space for further classiﬁcation. More architectures have followed 
proposing alternatives to the basic building blocks: in [23] and [27]  a more lightweight transformation
layer is proposed; in [52] and [18]  the authors propose alternate convolutional layers  respectively
based on multi-channel SPD representation and Riemannian means; a recurrent model is further
proposed in [19]; in [37] and [36]  an approximate matrix square-root layer replaces the ﬁnal
Euclidean projection to lighten computational complexity. In [15]  a SPD neural network is appended
to a fully-convolutional net to improve on performance and robustness to data scarcity. All in all 
most of the developments focus on improving or modifying existing blocks in an effort to converge to
their most relevant form  both theoretically and practically; in this work  we propose a new building
block for SPD neural networks  inspired by the well-known and well-used batch normalization
layer [31]. This layer makes use of batch centering and biasing  operations which need to be
deﬁned on the SPD manifold. As an additional  independent SPD building block  this novel layer
is agnostic to the particular way the other layers are computed  and as such can ﬁt into any of the
above architectures. Throughout the paper we choose to focus on the original architecture proposed
in [28]. Although the overall structure of the original batchnorm is preserved  its generalization to
SPD matrices requires geometric tools on the manifold  both for the forward and backward pass. In
this study  we further assess the particular interest of batch-normalized SPD nets in the context of
learning on scarce data with lightweight models: indeed  many ﬁelds are faced with costly  private
or evasive data  which strongly motivates the exploration of architectures naturally resilient to such
challenging situations. Medical imagery data is well-known to face these issues [41]  as is the ﬁeld
of drone radar classiﬁcation [14]  which we study in this work: indeed  radar signal acquisition is
prohibitively expensive  the acquired data is usually of conﬁdential nature  and drone classiﬁcation
in particular is plagued with an ever-changing pool of targets  which we can never reasonably hope
to encapsulate in comprehensive datasets. Furthermore  hardware integration limitations further
motivate the development of lightweight models based on a powerful representation of the data. As
such  our contributions are the following:

• a Riemannian batch normalization layer for SPD neural networks  respecting the manifold’s

geometry;

• a generalized gradient descent allowing to learn the batchnorm layer;
• extensive experimentations on three datasets from three different ﬁelds  (experiments are
made reproducible with our open-source PyTorch library  released along with the article).

Our article is organized as follows: we ﬁrst recall the essential required tools of manifold geometry;
we then proceed to describe our proposed Riemannian batchnorm algorithm; next  we devise the
projected gradient descent algorithm for learning the batchnorm; ﬁnally  we validate experimentally
our proposed architecture.

2 Geometry on the manifold of SPD matrices

We start by recalling some useful geometric notions on the SPD manifold  noted S +∗ in the following.

2

2.1 Riemannian metrics on SPD matrices

In a general setting  a Riemannian distance δR(P1  P2) between two points P1 and P2 on a manifold
is deﬁned as the length of the geodesic γP1→P2  i.e. the shortest parameterized curve ξ(t)  linking
them:

(cid:90) 1

δR(P1  P2) =

ξ | (ξ(0)=P1 ξ(1)=P2)

inf

0

ds(t)dt

(1)

ds(t)2 = ˙ξ(t)T Fξ(t) ˙ξ(t)

In the equation above  ds is the inﬁnitesimal distance between two close points and F is the metric
˙ξ is the velocity of the curve 
tensor  which deﬁnes a local metric at each point on the manifold.
sometimes noted dξ. For manifolds of exponential family distributions  F is none other than the
Fisher information matrix (FIM) (the inverse of which deﬁnes well-known Cramer-Rao bound) 
which is the Hessian matrix of the entropy. This connection between entropy and differential metrics
was ﬁrst made in 1945 by C.R. Rao [42] and in 1943 by M. Fréchet [26]  and further axiomatized
in 1965 by N.N. Chentsov [17]. Then  in a 1976 conﬁdential report cited in [5]  S.T. Jensen derived
2 tr(ξ−1 ˙ξξ−1 ˙ξ).
the inﬁnitesimal distance between two centered multivariate distributions ds(ξ)2 = 1
Such distributions being deﬁned entirely by the covariance matrix  they are isomorphic to the SPD
manifold  so the integration of ds along the geodesic leads to the globally-deﬁned natural distance on
S +∗ [38]  also called afﬁne-invariant Riemannian metric (AIRM) [41]  which can be expressed using
the standard Frobenius norm || · ||F :

δR(P1  P2) =

||log(P

− 1
1 P2P

2

− 1
1

2

)||F

1
2

(2)

The interested reader may note that while the above metric is the correct one from the information
geometric viewpoint  it is notoriously computation-heavy. Other metrics or divergences  either closely
approximate it or provide an alternate theoretical apporach  while contributing the highly desirable
property of lightweight computational complexity  especially in the modern context of machine
learning. Notable examples may include the usage of the Fisher-Bures metric [45]  the Bregman
divergence [11  44  6]  and optimal transport [3].
Another matter of importance is the deﬁnition of the natural mappings to and from the manifold and
its tangent bundle  which groups the tangent Euclidean spaces at each point in the manifold. At any
given reference point P0 ∈ S +∗   we call logarithmic mapping LogP0 of another point P ∈ S +∗ at P0
the corresponding vector S in the tangent space TP0 at P0. The inverse operation is the exponential
mapping ExpP0. In S +∗   both mappings (not to be confused with the matrix log and exp functions)
are known in closed form [2]:

∀S ∈ TP0   ExpP0(S) = P
∀P ∈ S +∗   LogP0 (P ) = P

1
2

0 exp(P

1
2

0 log(P

2

2

− 1
− 1
0 SP
0
− 1
− 1
0 P P
0

2

2

1
2

0 ∈ S +∗
)P
0 ∈ TP0

)P

1
2

(3a)

(3b)

2.2 Riemannian barycenter

(cid:80)

The ﬁrst step of the batchnorm algorithm is the computation of batch means; it may be possible to
i≤N Pi of a batch B of N SPD matrices {Pi}i≤N   we will rather use
use the arithmetic mean 1
N
the more geometrically appropriate Riemannian barycenter G   also known as the Fréchet mean [48]  
which we note Bar({Pi}i≤N ) or Bar(B). The Riemannian barycenter has shown strong theoretical
and practical interest in Riemannian data analysis [41]  which justiﬁes its usage in this context. By
deﬁnition  G is the point on the manifold that minimizes inertia in terms of the Riemannian metric
deﬁned in equation 2. The deﬁnition is trivially extensible to a weighted Riemannian barycenter  noted
Barw({Pi}i≤N ) or Barw(B)  where the weights w := {wi}i≤N respect the convexity constraint:

G = Barw({Pi}i≤N ) := arg min
G∈S+∗

wi δ2

R(G  Pi)   with

i≤N wi = 1

(4)

(cid:26)wi ≥ 0(cid:80)

N(cid:88)

i=1

3

Figure 1: Illustration of one iteration of the Karcher ﬂow [34].

When N = 2  i.e. when w = {w  1 − w}  a closed-form solution exists  which exactly corresponds
to the geodesic between two points P1 and P2  parameterized by w ∈ [0  1] [12]:

Bar(w 1−w)(P1  P2) = P

1
2
2

− 1
2 P1P

2

− 1
2

2

1
2

2   with w ≥ 0

P

(5)

(cid:0)P

(cid:1)w

Unfortunately  when N > 2  the solution to the minimization problem is not known in closed-form:
thus G is usually computed using the so-called Karcher ﬂow algorithm [34  49]   which we illustrate
in Figure 1 . In short  the Karcher ﬂow is an iterative process in which data points projected using the
logarithmic mapping (equation 3b) are averaged in tangent space and mapped back to the manifold
using the exponential mappings (equation 3a)   with a guaranteed convergence on a manifold with
constant negative curvature  which is the case for S +∗ [34]. The initialization of G is arbitrary  but
a reasonable choice is the arithmetic mean. A key point is that convergence is guaranteed on a
manifold with constant negative curvature  which is the case for the SPD manifold S +∗ [34]. Another
point of interest is that selecting K = 1 (that is  only one iteration of the ﬂow) and α = 1 (unit
step size) in the Karcher algorithm   corresponds exactly to the barycenter from the Log-Euclidean
metric viewpoint [41]. We actually use this setting in the layer: as the batch barycenter is but a noisy
estimation of the true barycenter  a lax approximation is sufﬁcient  and also allows for much faster
inference.

2.3 Centering SPD matrices using parallel transport
The Euclidean batchnorm involves centering and biasing the batch B  which is done via subtraction
and addition. However on a curved manifold  there is no such group structure in general  so these
seemingly basic operations are ill-deﬁned. To shift SPD matrices around their mean G or towards
a bias parameter G  we propose to rather use parallel transport on the manifold [2]. In short  the
parallel transport (PT) operator ΓP1→P2(S) of a vector S ∈ TP1 in the tangent plane at P1  between
P1  P2 ∈ S +∗ deﬁnes the path from P1 to P2 such that S remains parallel to itself in the tangent
planes along the path. The geodesic γP1→P2 is itself a special case of the PT  when S is chosen to be
the direction vector γ(cid:48)

P1→P2 (0) from P1 to P2. The expression for PT is known on S +∗ :

∀S ∈ TP1  ΓP1→P2(S) = (P2P −1

1

)

1

2 S (P2P −1

1

1

2 ∈ TP2

)

(6)

The equation above deﬁnes PT for tangent vectors  while we wish to transport points on the manifold.
To do so  we simply project the data points to the tangent space using the logarithmic mapping  
parallel transport the resulting vector from Eq. 6 which we then map back to the manifold using
exponential mapping . It can be shown (see [47]  appendix C for a full proof) that the resulting
operation  which we call SPD transport  turns out to be exactly the same as the formula above 
which is not an obvious result in itself. By abuse of notation  we also use ΓP1→P2 to denote the
SPD transport. Therefore  we can now deﬁne the centering of a batch of matrices {Pi}i≤N with
Riemannian barycenter G as the PT from G to the identity Id  and the biasing of the batch towards a
parametric SPD matrix G as the PT from Id to G.

4

Batch centering and biasing We now have the tools to deﬁne the batch centering and biasing:

Centering from G := Bar(B): ∀i ≤ N  ¯Pi = ΓG→Id (Pi) = G− 1
Biasing towards parameter G: ∀i ≤ N  ˜Pi = ΓId→G( ¯Pi) = G

2 Pi G− 1
2 ¯Pi G

2

1

1
2

(7a)
(7b)

3 Batchnorm for SPD data

In this section we introduce the Riemannian batch normalization (Riemannian BN  or RBN) algorithm
for SPD matrices. We ﬁrst brieﬂy recall the basic architecture of an SPD neural network.

3.1 Basic layers for SPD neural network

The SPDNet architecture mimics that of classical neural networks with a ﬁrst stage devoted to
compute a pertinent representation of the input data points and a second stage which allows to
perform the ﬁnal classiﬁcation. The particular structure of S +∗   the manifold of SPD matrices  is taken
into account by layers crafted to respect and exploit this geometry. The layers introduced in [28] are
threefold:
The BiMap (bilinear transformation) layer  analogous to the usual dense layer; the induced dimension
reduction eases the computational burden often found in learning algorithms on SPD data:

X (l) = W (l)T

P (l−1)W (l) with W (l) semi-orthogonal

(8)

The ReEig (rectiﬁed eigenvalues activation) layer  analogous to the ReLU activation; it can also be
seen as a eigen-regularization  protecting the matrices from degeneracy:

X (l) = U (l) max(Σ(l)  In)U (l)T   with P (l) = U (l)Σ(l)U (l)T

(9)

The LogEig (log eigenvalues Euclidean projection) layer:

X (l) = vec( U (l) log(Σ(l))U (l)T

)   with again U (l) the eigenspace of P (l)

(10)

This ﬁnal layer has no Eucidean counterpart: its purpose is the projection and vectorization of the
output feature manifold to a Euclidean space  which allows for further classiﬁcation with a traditional
dense layer. As stated previously  it is possible to envision different formulations for each of the layers
deﬁned above (see [23  52  37] for varied examples). Our following deﬁnition of the batchnorm can
ﬁt any formulation as it remains an independent layer.

3.2 Statistical distribution on SPD matrices

In traditional neural nets  batch normalization is deﬁned as the centering and standardization of the
data within one batch  followed by the multiplication and addition by parameterized variance and bias 
to emulate the data sampling from a learnt Gaussian distribution. In order to generalize to batches of
SPD matrices  we must ﬁrst deﬁne the notion of Gausian density on S +∗ . Although this deﬁnition has
not yet been settled for good  several approaches have been proposed. In [33]  the authors proceed
by introducing mean and variance as second- and fourth-order tensors. On the other hand  [43]
derive a scalar variance. In another line of work synthesized in [9]  which we adopt in this work 
the Gaussian density is derived from the deﬁnition of maximum entropy on exponential families
using information geometry on the cone of SPD matrices. In this setting  the natural parameter of the
resulting exponential family is simply the Riemannian mean; in other words  this means the notion of
variance  which appears in the Eucidean setting  takes no part in this deﬁnition of a Gaussian density
on S +∗ . Speciﬁcally  such a density p on SPD matrices P of dimension n writes:

p(P ) ∝ det(α G−1)e−tr(α G−1P )   with α =

n + 1

2

(11)

In the equation above  G is the Riemannian mean of the distribution. Again  there is no notion of
variance: the main consequence is that a Riemannian BN on SPD matrices will only involve centering
and biasing of the batch.

5

3.3 Final batchnorm algorithm

While the normalization is done on the current batch during training time  the statistics used in
inference are computed as running estimations. For instance  the running mean over the training set 
noted GS  is iteratively updated at each batch. In a Euclidean setting  this would amount to a weighted
average between the batch mean and the current running mean  the weight being a momentum typically
set to 0.9. The same concept holds for SPD matrices  but the running mean should be a Riemannian
mean weighted by η  i.e. Bar(η 1−η)(GS   GB)  which amounts to transporting the running mean
towards the current batch mean by an amount (1 − η) along the geodesic. We can now write the full
RBN algorithm 1. In practice  Riemannian BN is appended after each BiMap layer in the network.
Algorithm 1 Riemannian batch normalization on S +∗   training and testing phase

TRAINING PHASE
Require: batch of N SPD matrices {Pi}i≤N  running mean GS  bias G  momentum η
1: GB ← Bar({Pi}i≤N )
2: GS ← Barη(GS   GB)
3: for i ≤ N do
¯Pi ← ΓGB→Id (Pi)
4:
˜Pi ← ΓId→G( ¯Pi)
5:
6: end for

// compute batch mean
// update running mean

// center batch
// bias batch

return normalized batch { ˜Pi}i≤N

TESTING PHASE

Require: batch of N SPD matrices {Pi}i≤N  ﬁnal running mean GS  learnt bias G
1: for i ≤ N do
2:
3:
4: end for

// center batch using set statistics
// bias batch using learnt parameter

¯Pi ← ΓGS→Id (Pi)
˜Pi ← ΓId→G( ¯Pi)

return normalized batch { ˜Pi}i≤N

4 Learning the batchnorm

The speciﬁcities of a the proposed batchnorm algorithm are the non-linear manipulation of manifold
values in both inputs and parameters and the use of a Riemannian barycenter. Here we present the two
results necessary to correctly ﬁt the learning of the RBN in a standard back-propagation framework.

4.1 Learning with SPD constraint

The bias parameter matrix G of the RBN is by construction constrained to the SPD manifold.
However  noting L the network’s loss function  the usual Euclidean gradient ∂L
∂G  which we note
∂Geucl  has no particular reason to respect this constraint. To enforce it  ∂Geucl is projected to the
tangent space of the manifold at G using the manifold’s tangential projection operator ΠTG  resulting
in the tangential gradient ∂Griem. The update is then obtained by computing the geodesic on the
SPD manifold emanating from G in the direction ∂Griem  using the exponential mapping deﬁned in
equation 3a. Both operators are known in S +∗ [50]:

∀P  ΠTG(P ) = G

P + P T

2

G ∈ TG ⊂ S +

(12)

We illustrate this two-step process in Figure 2  explained in detail in [24]  which allows to learn the
parameter in a manifold-constrained fashion. However  this is still not enough for the optimization
of the layer  as the BN involves not simply G and G  but G 1
2   which are structured matrix
functions of G  i.e. which act non-linearly on the matrices’ eigenvalues without affecting its associated
eigenspace. The next subsection deals with the backpropagation through such functions.

2 and G− 1

6

Figure 2: Illustration of manifold-constrained gradient update. The Euclidean gradient is projected to
the tangent space  then mapped to the manifold.

4.2 Structured matrix backpropagation
Classically  the functions involved in the chain rule are vector functions in Rn [35]  whereas we deal
here with structured (symmetric) matrix functions in the S +∗   speciﬁcally the square root (·)
2 for the
bias and the inverse square root (·)− 1
2 for the barycenter (in equations 7b 7a). A generalization of the
chain rule to S∗
+ is thus required for the backpropagation through the RBN layer to be correct. Note
that a similar requirement applies to the ReEig and LogEig layers  respectively with a threshold and
log function. We generically note f a monotonous non-linear function; both (·)
2 check
out this hypothesis. A general formula for the gradient of f  applied on a SPD matrix’ eigenvalues
(σi)i≤n grouped in Σ’s diagonal  was independently developed by [32] and [13]. In short: given the
function P (cid:55)−→ X := f (P ) and the succeeding gradient ∂L(l+1)

∂X   the output gradient ∂L(l)

2 and (·)− 1

∂P is:

1

1

(cid:18)

(cid:19)

∂L(l)
∂P

= U

L (cid:12) (U T (

∂L(l+1)

∂X

)U )

U T

(13)

The equation above  also decribed in [39]  is called the Daleck˘ii-Kre˘in formula and dates back to
1956  (but was translated from Russian 9 years later)  predating the other formulation by 60 years.
It involves the eigenspace U of the input matrix P   and the Loewner matrix L  or ﬁnite difference
matrix deﬁned by:

In the case at hand 
2 . We credit [25] for ﬁrst
showing the equivalence between the two cited formulations  of which we expose the most concise.

2 and

= 1

(·)− 1

2

= − 1

2 (·)− 3

2 (·)− 1

In summary  the Riemannian barycenter (approximation via the Karcher ﬂow for a batch of matrices 
or exact formulation for two matrices)  the parallel transport and its extension on the SPD manifold  the
SPD-constrained gradient descent and the derivation of a non-linear SPD-valued structured function’s
gradient allow for training and inference of the proposed Riemannian batchnorm algorithm.

7

(cid:40) f (σi)−f (σj )
(cid:18)

σi−σj
f(cid:48)(σi)

if σi (cid:54)= σj
otherwise

(cid:19)(cid:48)

(·)

1
2

Lij =

(cid:18)

(cid:19)(cid:48)

(14)

Table 1: Accuracy comparison of SPDNet  SPDNetBN and FCNs on NATO radar data  in function of
amount of training data.

Model
# Parameters
Acc. (all data)
Acc. (10% data)

SPDNet
∼ 500

72.6% ± 0.61
69.1% ± 0.97

∼ 500

SPDNetBN
82.3% ± 0.80
77.777.777.7% ± 0.95

5 Experiments

FCN

∼ 10000

88.788.788.7% ± 0.83
65.6% ± 2.74

∼ 500

73.4% ± 3.66
61.1% ± 3.50

-

MRDRM
69.7% ± 1.12
67.1% ± 2.17

Here we evaluate the gain in performance of the RBN against the baseline SPDNet on different tasks:
radar data classiﬁcation  emotion recognition from video  and action recognition from motion capture
data. We call the depth L of an SPDNet the number of BiMap layers in the network  and denote the
dimensions as {n0 ···   nL}. The vectorized input to the ﬁnal classiﬁcation layer is thus of length
L. All networks are trained for 200 epochs using SGD with momentum set to 0.9 with a batch size
n2
of 30 and learning rate 5e−3  1e−2 or 5e−2. We provide the data in a pre-processed form alongside
the PyTorch [40] code for reproducibility purposes. We call SPDNetBN an SPDNet using RBN after
each BiMap layer. Finally  we also report performances of shallow learning method on SPD data 
namely a minimum Riemannian distance to Riemannian mean scheme (MRDRM)  described in [7] 
in order to bring elements of comparison between shallow and deep learning on SPD data.

5.1 Drones recognition

Our ﬁrst experimental target focuses on drone micro-Doppler [21] radar classiﬁcation. First we
validate the usage of our proposed method over a baseline SPDNet  and also compare to state-of-
the-art deep learning methods. Then  we study the models’ robustness to lack of data  a challenge
which  as stated previously  plagues the task of radar classiﬁcation and also a lot of different tasks.
Experiments are conducted on a conﬁdential dataset of real recordings issued from the NATO
organization 1 . To spur reproducibility  we also experiment on synthetic  publicly available data.

Radar data description A radar signal is the result of an emitted wave reﬂected on a target; as
such  one data point is a time-series of N values  which can be considered as multiple realizations
of a locally stationary centered Gaussian process  as done in [20]. The signal is split in windows
of length n = 20  the series of which a single covariance matrix of size 20 ∗ 20 is sampled from 
which represents one radar data point. The NATO data features 10 classes of drones  whereas the
synthetic data is generated by a realistic simulator of 3 different classes of drones following the
protocol described in [14]. We chose here to mimick the real dataset’s conﬁguration  i.e. we consider
a couple of minutes of continuous recordings per class  which correspond to 500 data points per class.

Comparison of SPDNetBN against SPDNet and radar state-of-the-art We test the two
SPD-based models in a {20  16  8}  2-layer conﬁguration for the synthetic data  and in a
{20  16  14  12  10  8}  5-layer conﬁguration for the NATO data  over a 5-fold cross-validation  split
in a train-test of 75% − 25%. We also wish to compare the Riemannian models to the common
Euclidean ones  which currently consitute the state-of-the-art in micro-Doppler classiﬁcation. We
compare two fully convolutional networks (FCN): the ﬁrst one is used as given in [14]; for the
second one  the number of parameters is set to approximately the same number as for the SPD neural
networks  which amounts to an unusually small deep net. All in all  the SPDNet  SPDNetBN and
small FCN on the one hand  and the full-size FCN on the other hand respectively have approximately
500 and 10000 parameters. Table 1 reports the average accuracies and variances on the NATO data.
We observe a strong gain in performance on the SPDNetBN over the SPDNet and over the small FCN 
which validates the usage of the batchnorm along with the exploitation of the geometric structure
underlying the data. All in all  we reach better performance with much fewer parameters.
Finally  in the interest of convergence analysis  we also report learning curves for the model’s accuracy
with and without Riemannian batchnorm in ﬁgure 3.

1We would like to thank the NATO working group SET245 for providing the drone micro-Doppler database

and allowing for publication of classiﬁcation results.

8

Figure 3: Test accuracy on the NATO dataset of the SPDNet with and without RBN  measured in
hours. The RBN exhibits a steeper learning curve. For the same number of epochs  it does take more
time overall  but reaches better accuracy much faster  allowing to reduce the number of epochs.

Figure 4: Performance of all models in function of the amount of synthetic radar data. The SPDNetBN
model outperforms the other ones and continues to work even with a little fraction of the train data.

Robustness to lack of data As stated previously  it is of great interest to consider the robustness of
learning algorithms when faced with a critically low amount of data. The last line in table 1 shows
that when given only 10% of available training data  the SPD-based models remain highly robust to
the lack of data while the FCNs plummet. Further  we study robustness on synthetic data  artiﬁcially
varying the amount of training data while comparing performance over the same test set. As the
simulator is unbounded on potential training data  we also increase the initial training set up to double
its original size. Results are reported in Figure 4. We can conclude from these that the SPDNetBN
both exhibits higher robustness to lack of data and performs much better than the state-of-the-art
deep method with much fewer parameters. When the available training data allowed skyrockets 
we do observe that the FCN comes back to par with the SPDNetBN to the point of outperforming
it by a small margin in the extremal scenario; in the meantime  the SPDNet lags behind by a large
margin to the SPDNetBN  which thus seems to beneﬁt strongly from the normalization. In any case 
the manifold framework seems well suited in a scarce data learning context  especially considering
the introduced normalization layers  which again pinpoints the interest of taking into account the
geometric structure of the data  all the while without introducing prior knowledge during training.

5.2 Other experiments

Here we validate the use of the RBN on a broader set of tasks. We ﬁrst clarify we do not necessarily
seek state-of-the-art in the general sense for the following tasks  but rather in the speciﬁc case of
the family of SPD-based methods. Our own implementation (as an open PyTorch library) of the
SPDNet’s performances match that in [28]  ensuring a fair comparison.

9

0.60.650.70.750.80.85-0.100.10.20.30.40.50.60.70.SDPNet with RBNSPDNet without RBNValidationaccuracyTime (hours)100020003000Numberoftrainingexamples708090Accuracy(%)Referenceamountofdata(1500)SPDNetSPDNetBNFCNTable 2: Accuracy comparison of SPDNet with and without Riemannian BN on the AFEW dataset.
{400  300  200  100  50}
Model architecture
SPDNet
SPDNetBN (ours)

{400  50}
29.9%
34.934.934.9%

{400  200  100  50}

{400  100  50}

33.7%
37.137.137.1%

34.5%
36.236.236.2%

31.2%
35.235.235.2%

Table 3: Accuracy comparison of SPDNet with and without Riemannian BN on the HDM05 dataset.

Model architecture
{93  30}

SPDNet
61.6%±1.35

SPDNetBN (ours)
65.265.265.2% ± 1.15

Emotion recognition In this section we experiment on the AFEW dataset [22]  which consists
of videos depicting 7 classes of emotions; we follow the setup and protocol in [28]. Results for
4 architectures are summarized in table 2. In comparison  the MRDRM yields a 20.5% accuracy.
We observe a consistent improvement using our normalization scheme. This dataset being our
largest-scale experiment  we also report the increase in computation time using the RBN  speciﬁcally
for the deepest net: one training lasted on average 81s for SPDNet  and 88s (+8.6%) for SPDNetBN.

Action recognition In this section we experiment on the HDM05 motion capture dataset. We use
the same experimental setup as in [28] results are shown in table 3. Note that all tested models
exhibit noticeable variance depending on the weights initialization and the initial random split of
the dataset; the results displayed were obtained by setting a ﬁxed seed of 0 for both. In comparison 
the MRDRM yields a 27.3% ± 1.06 accuracy. Again  we validate a better performance using the
batchnorm.

Conclusion

We proposed a batch normalization algorithm for SPD neural networks  mimicking the orginal
batchnorm in Euclidean neural networks. The algorithm makes use of the SPD Riemannian manifold’s
geometric structure  namely the Riemannian barycenter  parallel transport  and manifold-constrained
backpropagation through non-linear structured functions on SPD matrices. We demonstrate a
systematic  and in some cases considerable  performance increase across a diverse range of data
types. An additional observation is the better robustness to lack of data compared to the baseline
SPD neural network and to a state-of-the-art convolutional network  as well as better performance
than a well-used  more traditional Riemannian learning method (the closest-barycenter scheme). The
overall performances of our proposed SPDNetBN makes it a suitable candidate in learning scenarios
where data is structured  scarce  and where model size is a relevant issue.

References
[1] Motion Database HDM05.
[2] S.-i. Amari. Information Geometry and Its Applications. Applied Mathematical Sciences.

Springer Japan  2016.

[3] M. Arjovsky  S. Chintala  and L. Bottou. Wasserstein Generative Adversarial Networks. In

International Conference on Machine Learning  pages 214–223  July 2017.

[4] V. Arsigny  P. Fillard  X. Pennec  and N. Ayache. Log-Euclidean metrics for fast and simple

calculus on diffusion tensors. Magnetic Resonance in Medicine  56(2):411–421  Aug. 2006.

[5] C. Atkinson and A. F. S. Mitchell. Rao’s Distance Measure. Sankhy¯a: The Indian Journal of

Statistics  Series A (1961-2002)  43(3):345–365  1981.

[6] A. Banerjee  S. Merugu  I. S. Dhillon  and J. Ghosh. Clustering with Bregman Divergences.

Journal of Machine Learning Research  6(Oct):1705–1749  2005.

[7] A. Barachant  S. Bonnet  M. Congedo  and C. Jutten. Multiclass Brain–Computer Interface
IEEE Transactions on Biomedical Engineering 

Classiﬁcation by Riemannian Geometry.
59(4):920–928  Apr. 2012.

[8] A. Barachant  S. Bonnet  M. Congedo  and C. Jutten. Classiﬁcation of covariance matrices
using a Riemannian-based kernel for BCI applications. Neurocomputing  112:172–178  July
2013.

10

[9] F. Barbaresco. Jean-Louis Koszul and the Elementary Structures of Information Geometry. In
F. Nielsen  editor  Geometric Structures of Information  Signals and Communication Technology 
pages 333–392. Springer International Publishing  Cham  2019.

[10] R. Bhatia. Positive Deﬁnite Matrices. Princeton University Press  Princeton  NJ  USA  2015.
[11] J.-D. Boissonnat  F. Nielsen  and R. Nock. Bregman Voronoi diagrams. Discrete and Computa-

tional Geometry  page 200  2010.

[12] S. Bonnabel and R. Sepulchre. Riemannian Metric and Geometric Mean for Positive Semideﬁ-
nite Matrices of Fixed Rank. SIAM Journal on Matrix Analysis and Applications  31(3):1055–
1070  Jan. 2010.

[13] M. Brodski˘ı  J. Dalecki˘ı  O. È˘ıdus  I. Iohvidov  M. Kre˘ın  O. Ladyženskaja  V. Lidski˘ı  J. Ljubiˇc 
V. Macaev  A. Povzner  L. Sahnoviˇc  J. Šmuljan  I. Suharevski˘ı  and N. Uralceva. Thirteen
Papers on Functional Analysis and Partial Differential Equations  volume 47 of American
Mathematical Society Translations: Series 2. American Mathematical Society  Dec. 1965.

[14] D. A. Brooks  O. Schwander  F. Barbaresco  J. Schneider  and M. Cord. Temporal Deep
Learning for Drone Micro-Doppler Classiﬁcation. In 2018 19th International Radar Symposium
(IRS)  pages 1–10  June 2018.

[15] D. A. Brooks  O. Schwander  F. Barbaresco  J. Schneider  and M. Cord. Exploring Complex
Time-series Representations for Riemannian Machine Learning of Radar Data. In ICASSP 2019
- 2019 IEEE International Conference on Acoustics  Speech and Signal Processing (ICASSP) 
pages 3672–3676  May 2019.

[16] J. Cavazza  P. Morerio  and V. Murino. When Kernel Methods Meet Feature Learning: Log-
Covariance Network for Action Recognition From Skeletal Data. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition Workshops (CVPRW)  pages 1251–1258  July 2017.
[17] N. N. Cencov. Statistical Decision Rules and Optimal Inference. American Mathematical Soc. 

Apr. 2000. Google-Books-ID: 63CPCwAAQBAJ.

[18] R. Chakraborty  J. Bouza  J. Manton  and B. C. Vemuri. ManifoldNet: A Deep Network
Framework for Manifold-valued Data. arXiv:1809.06211 [cs]  Sept. 2018. arXiv: 1809.06211.
[19] R. Chakraborty  C.-H. Yang  X. Zhen  M. Banerjee  D. Archer  D. Vaillancourt  V. Singh  and
B. Vemuri. A Statistical Recurrent Model on the Manifold of Symmetric Positive Deﬁnite Ma-
trices. In S. Bengio  H. Wallach  H. Larochelle  K. Grauman  N. Cesa-Bianchi  and R. Garnett 
editors  Advances in Neural Information Processing Systems 31  pages 8883–8894. Curran
Associates  Inc.  2018.

[20] N. Charon and F. Barbaresco. A new approach for target detection in radar images based on

geometric properties of covariance matrices’ spaces  2009.

[21] V. C. Chen  F. Li  S.-S. Ho  and H. Wechsler. Micro-Doppler effect in radar: phenomenon  model 
and simulation study. IEEE Transactions on Aerospace and electronic systems  42(1):2–21 
2006.

[22] A. Dhall  R. Goecke  S. Lucey  and T. Gedeon. Static facial expression analysis in tough
conditions: Data  evaluation protocol and benchmark. In 2011 IEEE International Conference
on Computer Vision Workshops (ICCV Workshops)  pages 2106–2112  Nov. 2011.

[23] Z. Dong  S. Jia  V. C. Zhang  M. Pei  and Y. Wu. Deep Manifold Learning of Symmetric

Positive Deﬁnite Matrices with Application to Face Recognition. In AAAI  2017.

[24] A. Edelman  T. Arias  and S. Smith. The Geometry of Algorithms with Orthogonality Con-

straints. SIAM Journal on Matrix Analysis and Applications  20(2):303–353  Jan. 1998.

[25] M. Engin  L. Wang  L. Zhou  and X. Liu. DeepKSPD: Learning Kernel-matrix-based SPD
Representation for Fine-grained Image Recognition. arXiv:1711.04047 [cs]  Nov. 2017. arXiv:
1711.04047.

[26] M. Fréchet. Sur l’extension de certaines evaluations statistiques au cas de petits echantillons.
Revue de l’Institut International de Statistique / Review of the International Statistical Institute 
11(3/4):182–205  1943.

[27] Z. Gao  Y. Wu  X. Bu  and Y. Jia. Learning a Robust Representation via a Deep Network on
Symmetric Positive Deﬁnite Manifolds. arXiv:1711.06540 [cs]  Nov. 2017. arXiv: 1711.06540.
[28] Z. Huang and L. Van Gool. A Riemannian Network for SPD Matrix Learning. arXiv:1608.04233

[cs]  Aug. 2016. arXiv: 1608.04233.

[29] Z. Huang  C. Wan  T. Probst  and L. Van Gool. Deep Learning on Lie Groups for Skeleton-based

Action Recognition. arXiv:1612.05877 [cs]  Dec. 2016. arXiv: 1612.05877.

[30] Z. Huang  J. Wu  and L. Van Gool. Building Deep Networks on Grassmann Manifolds.

arXiv:1611.05742 [cs]  Nov. 2016. arXiv: 1611.05742.

11

[31] S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by

Reducing Internal Covariate Shift. arXiv:1502.03167 [cs]  Feb. 2015. arXiv: 1502.03167.

[32] C. Ionescu  O. Vantzos  and C. Sminchisescu. Matrix Backpropagation for Deep Networks with
Structured Layers. In 2015 IEEE International Conference on Computer Vision (ICCV)  pages
2965–2973  Santiago  Chile  Dec. 2015. IEEE.

[33] N. Jaquier and S. Calinon. Gaussian mixture regression on symmetric positive deﬁnite matrices
manifolds: Application to wrist motion estimation with sEMG. In 2017 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)  pages 59–64  Vancouver  BC  Sept. 2017.
IEEE.

[34] H. Karcher. Riemannian center of mass and molliﬁer smoothing. Communications on Pure and

Applied Mathematics  30(5):509–541  Sept. 1977.

[35] Y. Lecun  L. Bottou  Y. Bengio  and P. Haffner. Gradient-based learning applied to document

recognition. Proceedings of the IEEE  86(11):2278–2324  Nov. 1998.

[36] P. Li  J. Xie  Q. Wang  and Z. Gao. Towards Faster Training of Global Covariance Pooling
Networks by Iterative Matrix Square Root Normalization. In 2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition  pages 947–955  Salt Lake City  UT  June 2018. IEEE.
[37] Y. Mao  R. Wang  S. Shan  and X. Chen. COSONet: Compact Second-Order Network for Video

Face Recognition. page 16.

[38] G. Marceau-Caron and Y. Ollivier. Natural Langevin Dynamics for Neural Networks. In
F. Nielsen and F. Barbaresco  editors  Geometric Science of Information  volume 10589  pages
451–459. Springer International Publishing  Cham  2017.

[39] F. Nielsen and R. Bhatia  editors. Matrix Information Geometry. Springer-Verlag  Berlin

Heidelberg  2013.

[40] A. Paszke  S. Gross  S. Chintala  G. Chanan  E. Yang  Z. DeVito  Z. Lin  A. Desmaison 

L. Antiga  and A. Lerer. Automatic differentiation in PyTorch. Oct. 2017.

[41] X. Pennec  P. Fillard  and N. Ayache. A Riemannian Framework for Tensor Computing.

International Journal of Computer Vision  66(1):41–66  Jan. 2006.

[42] C. R. Rao. Information and the Accuracy Attainable in the Estimation of Statistical Parameters.
In Breakthroughs in Statistics  Springer Series in Statistics  pages 235–247. Springer  New York 
NY  1992.

[43] S. Said  L. Bombrun  Y. Berthoumieu  and J. Manton. Riemannian Gaussian Distributions on
the Space of Symmetric Positive Deﬁnite Matrices. arXiv:1507.01760 [math  stat]  July 2015.
arXiv: 1507.01760.

[44] A. Siahkamari  V. Saligrama  D. Castanon  and B. Kulis. Learning Bregman Divergences.

arXiv:1905.11545 [cs  stat]  May 2019. arXiv: 1905.11545.

[45] K. Sun  P. Koniusz  and Z. Wang. Fisher-Bures Adversary Graph Convolutional Networks.

arXiv:1903.04154 [cs  stat]  Mar. 2019. arXiv: 1903.04154.

[46] O. Tuzel  F. Porikli  and P. Meer. Region Covariance: A Fast Descriptor for Detection and
Classiﬁcation. In Computer Vision – ECCV 2006  Lecture Notes in Computer Science  pages
589–600. Springer  Berlin  Heidelberg  May 2006.

[47] O. Yair  M. Ben-Chen  and R. Talmon. Parallel Transport on the Cone Manifold of SPD Matrices

for Domain Adaptation. July 2018.

[48] L. Yang  M. Arnaudon  and F. Barbaresco. Riemannian median  geometry of covariance
matrices and radar target detection. In The 7th European Radar Conference  pages 415–418 
Sept. 2010.

[49] L. Yang  M. Arnaudon  and F. Barbaresco. Riemannian median  geometry of covariance

matrices and radar target detection. pages 415–418  Nov. 2010.

[50] F. Yger. A review of kernels on covariance matrices for BCI applications. In 2013 IEEE
International Workshop on Machine Learning for Signal Processing (MLSP)  pages 1–6  Sept.
2013.

[51] F. Yger and M. Sugiyama. Supervised LogEuclidean Metric Learning for Symmetric Positive

Deﬁnite Matrices. arXiv:1502.03505 [cs]  Feb. 2015. arXiv: 1502.03505.

[52] T. Zhang  W. Zheng  Z. Cui  and C. Li. Deep Manifold-to-Manifold Transforming Network. In
2018 25th IEEE International Conference on Image Processing (ICIP)  pages 4098–4102  Oct.
2018.

12

,Daniel Brooks
Olivier Schwander
Frederic Barbaresco
Jean-Yves Schneider
Matthieu Cord