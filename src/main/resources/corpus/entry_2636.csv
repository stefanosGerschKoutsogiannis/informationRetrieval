2011,Inferring Interaction Networks using the IBP applied to microRNA Target Prediction,Determining interactions between entities and the overall organization and clustering of nodes in networks is a major challenge when analyzing biological and social network data. Here we extend the Indian Buffet Process (IBP)  a nonparametric Bayesian model  to integrate noisy interaction scores with properties of individual entities for inferring interaction networks and clustering nodes within these networks. We present an application of this method to study how microRNAs regulate mRNAs in cells. Analysis of synthetic and real data indicates that the method improves upon prior methods  correctly recovers interactions and clusters  and provides accurate biological predictions.,Inferring Interaction Networks using the IBP applied

to microRNA Target Prediction

Hai-Son Le

Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA  USA
hple@cs.cmu.edu

Ziv Bar-Joseph

Machine Learning Department
Carnegie Mellon University

Pittsburgh  PA  USA

zivbj@cs.cmu.edu

Abstract

Determining interactions between entities and the overall organization and clus-
tering of nodes in networks is a major challenge when analyzing biological and
social network data. Here we extend the Indian Buffet Process (IBP)  a nonpara-
metric Bayesian model  to integrate noisy interaction scores with properties of
individual entities for inferring interaction networks and clustering nodes within
these networks. We present an application of this method to study how microR-
NAs regulate mRNAs in cells. Analysis of synthetic and real data indicates that the
method improves upon prior methods  correctly recovers interactions and clusters 
and provides accurate biological predictions.

1

Introduction

Determining interactions between entities based on observations is a major challenge when analyz-
ing biological and social network data [1  12  15]. In most cases we can obtain information regarding
each of the entities (individuals in social networks and proteins in biological networks) and some
information about possible relationships between them (friendships or conversation data for social
networks and motif or experimental data for biology). The goal is then to integrate these datasets
to recover the interaction network between the entities being studied. To simplify the analysis of
the data it is also beneﬁcial to identify groups  or clusters  within these interaction networks. Such
groups can then be mapped to speciﬁc demographics or interests in the case of social networks or to
modules and pathways in biological networks [2].
A large number of generative models were developed to represent entities as members of a number of
classes. Many of these models are based on the stochastic blockmodel introduced in [19]. While the
number of classes in such models could be ﬁxed  or provided by the user  nonparametric Bayesian
methods have been applied to allow this number to be inferred based on the observed data [9]. The
stochastic blockmodel was also further extended in [1] to allow mixed membership of entities within
these classes. An alternate approach is to use latent features to describe entities. [10] proposed a
nonparametric Bayesian matrix factorization method to learn the latent factors in relational data
whereas [12] presented a nonparametric model to study binary link data. All of these methods rely
on the pairwise link and interaction data and in most cases do not utilize properties of the individual
entities when determining interactions.
Here we present a model that extends the Indian Buffet Process (IBP) [7]  a nonparametric Bayesian
prior over inﬁnite binary matrices  to learn the interactions between entities with an unbounded
number of groups. Speciﬁcally  we represent each group as a latent feature and deﬁne interactions
between entities within each group. Such latent feature representation has been used in the past
to describe entities [7  10  12] and IBP is an appropriate nonparametric prior to infer the number
of latent features. However  unlike IBP our model utilizes interaction scores as priors and so the

1

model is not exchangeable anymore. We thus extend IBP by integrating it with Markov random
ﬁeld (MRF) constraints  speciﬁcally pairwise potentials as in Ising model. MRF priors has been
combined with Dirichlet Process mixture models for image segmentation in a related work of Orbanz
and Buhmann [13]. Pairwise information is also used in the distance dependent Chinese restaurant
process [4] to encourage similar objects to be clustered. Our model is well suited for cases in
which we are provided with information on both link structure and the outcome of the underlying
interactions.
In social networks such data can come from observations of conversation between
individuals followed by actions of the speciﬁc individuals (for example  travel)  whereas in biology
it is suited for regulatory networks as discussed below.
We apply our model to study the microRNA (miRNA) target prediction problem. miRNAs were
recently discovered as a class of regulatory RNA molecules that regulate the levels of messenger
RNAs (mRNAs) (which are later translated to proteins) by binding and inhibiting their speciﬁc
targets [15]. They were shown to play an important role in a number of diseases including cancer 
and determining the set of genes that are targeted by each miRNA is an important question when
studying these diseases. Several methods were proposed to predict targets of miRNAs based on
their sequence1. While these predictions are useful  due to the short length of miRNAs  they lead
to many false positives and some false negatives [8].
In addition to sequence information  it is
now possible to obtain the expression levels of miRNAs and their predicted mRNA targets using
microarrays. Since miRNAs inhibit their direct targets  integrating sequence and expression data
can improve predictions regarding the interactions between miRNAs and their targets. A number of
methods based on regression analysis were suggested for this task [8  17]. While methods utilizing
expression data improved upon methods that only used sequence data  they often treated each target
mRNA in isolation. In contrast  it has now been shown that each miRNA often targets hundreds
of genes  and that miRNAs often work in groups to achieve a larger impact [14]. Thus  rather than
trying to infer a separate regression model for each mRNA we use our IBP extended model to infer
a joint regression model for a cluster of mRNAs and the set of miRNAs that regulate them. Such a
model would provide statistical conﬁdence (since it combines several observations) while adhering
more closely to the underlying biology. In addition to inferring the interactions in the dataset such a
model would also provide a grouping for genes and miRNAs which can be used to improve function
prediction.

2 Computational model

Firstly  we derive a distribution on inﬁnite binary matrices starting with a ﬁnite model and taking the
limit as the number of features goes to inﬁnity. Secondly  we describe the application of our model
to the miRNA target prediction problem using a Gaussian additive model.

2.1

Interaction model

Let zik denote the (i  k) entry of a matrix Z and let zk denote the kth column of Z. The group
membership of N entities is deﬁned by a (latent) binary matrix Z where zik = 1 if entity i belongs
to group k. Given Z  we say that entity i interacts with entity j if zikzjk = 1 for some k. Note that
two entities can interact through many groups where each group represents one type of interaction.
In many cases  a prior on such interactions can be obtained. Assume we have a N × N symmetric
matrix W  where wij indicates the degree that we believe that entity i and j interact: wij > 0 if
entities i and j are more likely to interact and wij < 0 if they are less likely to do so.
Nonparametric prior for Z: Grifﬁths and Ghahramani [7] proposed the Indian Buffet Process (IBP)
as a nonparametric prior distribution on sparse binary matrices Z. The IBP can be derived from a
simple stochastic process  described by a culinary metaphor. In this metaphor  there are N customers
(entities) entering a restaurant and choosing from an inﬁnite array of dishes (groups). The ﬁrst
customer tries Poisson(α) dishes  where α is a parameter. The remaining customers enter one after
the others. The ith customer tries a previously sampled dish k with probability mk
i   where mk is the
number of previous customers who have sampled this dish. He then samples a Poisson( α
i ) number of
new dishes. This process deﬁnes an exchangeable distribution on the equivalence classes of Z  which
are the set of binary matrices that map to the same left-ordered binary matrices. [7]. Exchangeability

1Genes that are targets of miRNAs contain the reverse complement of part of the miRNA sequence.

2

means that the order of the customers does not affect the distribution and that permutation of the data
does not change the resulting likelihood.
The prior knowledge on interactions discussed above (encoded by W) violates the exchangeability
of the IBP since the group membership probability depends on the identities of the entities whereas
exchangeability means that permutation of entities does not change the probability. In [11]  Miller
et al. presented the phylogenetic Indian Buffet Process (pIBP)  where they used a tree representation
to express non-exchangeability. In their model  the relationships among customers are encoded as
a tree allowing them to exploit the sum-product algorithm in deﬁning the updates for an MCMC
sampler  without signiﬁcantly increasing the computational burden when performing inference.
We combine the IBP with pairwise potentials using W  constraining the dish selection of customers.
Similar to the pIBP  the entries in zk are not chosen independently given πk but rather depend on
the particular assignment of the remaining entries. In the following sections  we start with a model
with a ﬁnite number of groups and consider the limit as the number of groups grows to derive the
nonparametric prior. Note that in our model  as in the original IBP [7]  while the number of rows are
ﬁnite  the number of columns (features) could be inﬁnite. We can thus deﬁne a prior on interactions
between entities (since their number is known in advance) while still allowing for an inﬁnite number
of groups. This ﬂexibility allows the group parameters to be drawn from an inﬁnite mixtures of
priors which may lead to identical groups of entities each with a different set of parameters.

P (zk|πk) = exp

πk|α ∼ Beta(cid:0) α

  1(cid:1)

K

2.1.1 Prior on ﬁnite matrices Z
We have an N × K binary matrix Z where N is the number of entities and K is a ﬁxed  ﬁnite
number of groups. In the IBP  each group/column k is associated with a parameter πk  chosen from
a Beta(α/K  1) prior distribution where α is a hyperparameter:

(cid:16)(cid:88)

(cid:1)(cid:17)
(cid:0)(1 − zik) log(1 − πk) + zik log πk
(cid:17)
− 1(cid:1) log πk

(cid:1) +(cid:0) α

(cid:16)(cid:88)

i

1
B( α
K   1)

The joint probability of a column k and πk in the IBP is:
P (zk  πk|α) =
exp
where B(·) is the Beta function.
For our model  we add the new pairwise potentials on memberships of entities. Deﬁning Φzk =

(cid:0)(1 − zik) log(1 − πk) + zik log πk
(cid:1)  the joint probability of a column k and πk is:
(cid:0)(1 − zik) log(1 − πk) + zik log πk

− 1(cid:1) log πk

exp(cid:0)(cid:80)

(cid:1) +(cid:0) α

i<j wijzikzjk
1
Z(cid:48) Φzk exp

P (zk  πk|α) =

(cid:16)(cid:88)

(cid:17)

(1)

(2)

K

K

i

where Z(cid:48) is the partition function. Note that IBP is a special case of our model when all w’s are
zeros (W = 0).
Following [7]  we deﬁne the lof-equivalence classes [Z] as the sets of binary matrices mapped to
the same left-ordered binary matrices. The history hi of a feature k at an entity i is deﬁned as
(z1k  . . .   z(i−1)k). When no object is speciﬁed  h refers to the full history. mk and mh denote the
number of non-zero entries of a feature k and a history h respectively. Kh is the number of features
h=1 Kh is

possessing the history h while K0 is the number of features having mk = 0. K+ =(cid:80)2N−1

the number of features for which mk > 0.
By integrating over all values of πk  we get the marginal probability of a binary matrix Z.

i

(cid:90) 1

k=1

K(cid:89)
K(cid:89)
K(cid:89)

k=1

k=1

P (Z) =

=

=

P (zk  πk|α) dπk

0

1
Z(cid:48) Φzk

(cid:90) 1
Z(cid:48) Φzk B(cid:0) α

1

0

K

exp

(cid:16)(cid:0) α
+ mk  N − mk + 1(cid:1)

K

3

(cid:17)
+ mk − 1(cid:1) log πk + (N − mk) log(1 − πk)

(3)

dπk

(4)

(5)

The partition function Z(cid:48) could be written as: Z(cid:48) =(cid:80)2N−1

h=0 ΦhB(cid:0) α

K + mh  N − mh + 1(cid:1).

2.1.2 Taking the inﬁnite limit

(cid:88)

The probability of a particular lof-equivalence class of binary matrices  [Z]  is:

Z(cid:48) Φzk B(cid:0)mk +
Taking the limit when K → ∞  we can show that with Ψ =(cid:80)2N−1

h=0 Kh!

P ([Z]) =

P (Z) =

α
K

k=1

K(cid:89)

1

Z

  N − mk + 1(cid:1)

(N−mh)!(mh−1)!

K!(cid:81)2N−1
K+(cid:89)

h=0 Kh!

αK+(cid:81)2N−1

=

h=1 Kh!

k=1

h=1 Φh
K   N − mk + 1)
K   N + 1)

K(cid:89)
exp(cid:0) − αΨ)

k=1

N !

1
Z(cid:48) B(

α
K

Φzk

B(mk + α
B( α

(N − mk)!(mk − 1)!

N !

Φzk

K!(cid:81)2N−1
K+(cid:89)

k=1

K→∞ P ([Z]) = lim
lim
K→∞

(6)

:

  N + 1) (7)

(8)

The detailed derivations are shown in Appendix.

2.1.3 The generative process

We now describe a generative stochastic process for Z. It can be understood by a culinary metaphor 
where each row of Z corresponds to a customer and each column corresponds to a dish. We denote
by h(i) the value of zik in the complete history h. With ¯Φh = Φh
  we deﬁne

(N−mh)!(mh−1)!

i=1 Ψi. Finally  let z<ik be entries 1  . . .   (i − 1) of zk.

N !

¯Φh so that Ψ =(cid:80)N

Ψi =(cid:80)

h:hi=0 h(i)=1

customers. The probability that a dish would be selected is(cid:80)

Assume that we are provided with a compatibility score between pairs of customers. That is  we have
a value wij for the food preference similarity between customer i and customer j. Higher values of
wij indicate similar preferences and customers with such values are more likely to select the same
dish. Therefore  the dishes a customer selects may depend on the choices of previous customers. The
ﬁrst customer tries Poisson(αΨ1) dishes. The remaining customers enter one after the others. The
ith customer selects dishes with a probability that partially depends on the selection of the previous
¯Φh.
He then samples a Poisson(αΨi) number of new dishes. This process repeats until all customers
have made their selections. Although this process is not exchangeable  the sequential order of cus-
tomers is not important. This means that we get the same marginal distribution for any particular
order of customers. Let K (i)
1 denote the number of new dishes sampled by customer i  the probabil-
ity of a particular matrix generated by this process is:

¯Φh/(cid:80)

h:hi=z<ik h(i)=1

h:hi=z<ik

K+(cid:89)

¯Φzk exp(cid:0) − αΨ)

αK+(cid:81)N

i=1 K (i)

1

k=1

P (Z) =

(9)

(cid:81)N
(cid:81)2N −1

i=1 K(i)
h=1 Kh!

matrices gen-
If we only pay attention to the lof-equivalence classes [Z]  since there are
erated by this process mapped to the same equivalence classes  multiplying P (Z) by this quantity
recovers Equation (8). We show in Appendix that in the case of the IBP where Φh = 1 for all
histories h (when W = 0)  this generative process simpliﬁes to the Indian Buffet Process.

1

2.2 Regression model for mRNA expression

In this section  we describe the application using the nonparametric prior to the miRNA target pre-
diction problem. However  the method is applicable in general settings where there is a way to model
properties of one entity from properties of its interacting entities. Our input data are expression pro-
ﬁles of M messenger RNA (mRNA) transcripts and R miRNA transcript across T samples. Let
M )T   where each row vector xi is the expression proﬁle of mRNA i in all samples.
X = (xT
Similarly  let Y = (yT
R)T represent the expression proﬁles of R miRNAs. Furthermore 
suppose we are given a M × R matrix C where cij is the prior likelihood score for the interaction of

1   . . .   xT

1   . . .   yT

4

mRNA i and miRNA j. Such matrix C could be obtained from sequence-based miRNA target pre-
dictions as discussed above. Applying our interaction model to this problem  the set of N = M + R
entities are divided into two disjoint sets of mRNAs and miRNAs. Let Z = (UT   VT )T where U
and V are the group membership matrices for mRNAs and miRNAs respectively  W is given by
. Therefore  mRNA i and miRNA j interact through all groups k such that uikvjk = 1.

(cid:18) 0 C
(cid:19)

CT

0

2.2.1 Gaussian additive model

In the interaction model suggested by GenMiR++ [8]  each miRNA expression proﬁle is used to
explain the downregulation of the expression of its targeted mRNAs. Our model uses a group speciﬁc
and miRNA speciﬁc coefﬁcients ( s = [s1  . . .   s∞]T   with sk > 0 for groups and r = [r1  . . .   rR]T
for all miRNAs) to model the downregulation effect. These coefﬁcients represent the baseline effect
of group members and the strength of speciﬁc miRNAs  respectively. Using these parameters the
expression level of a speciﬁc mRNA could be explained by summing over expression proﬁles of all
miRNAs targeting the mRNA:

(cid:88)

(rj +

k:uikvjk=1

xi ∼ N(cid:0)µ −(cid:88)
(cid:16) − 1

j

k:uikvjk=1 sk) yj.

sk) yj  σ2I(cid:1)
(cid:17)

(10)

(11)

r ∼

where µ represents baseline expression for this mRNA and σ is used to represent measurement noise.
Thus  under this model  the expression of a mRNA are reduced from their baseline values by a linear
combination of expression values of the miRNAs that target them. The probability of the observed
data given Z is: P (X  Y|Z  Θ) ∝ exp
  with Θ = {µ  σ2  s  r}

(cid:80)
i(xi − ¯xi)T (xi − ¯xi)

2σ2

and ¯xi = µ −(cid:80)

j(rj +(cid:80)

2.2.2 Priors for model variables

We use the following as prior distributions for the variables in our model:
r ∼ N (0  σ2

r I)

sk ∼ Gamma(αs  βs)
µ ∼ N (0  σ2

µI)

1/σ2 ∼ Gamma(αv  βv)

where the α and β are the shape and scale parameters. The parameters are given hyperpriors: 1/σ2
µ ∼ Gamma(aµ  bµ). αs  βs  αv  βv are also given Gamma hyperpriors.
Gamma(ar  br) and 1/σ2

3

Inference by MCMC

As with many nonparametric Bayesian models  exact inference is intractable.
Instead we use a
Markov Chain Monte Carlo (MCMC) method to sample from the posterior distribution of Z and Θ.
Although  our model allows Z to have inﬁnite number of columns  we only need to keep track of
non-zero columns of Z  an important aspect which is exploited by several nonparametric Bayesian
models [7]. Our sampling algorithm involves a mix of Gibbs and Metropolis-Hasting steps which
are used to generate the new sample.

3.1 Sampling from populated columns of Z

Let m−ik is the number of one entries not including zik in zk. Also let z−ik denote the entries of
zk except zik and let Z−(ik) be the entire matrix Z except zik. The probability of an entry given
the remaining entries in a column can be derived by considering an ordering of customers such that
customer i is the last person in line and using the generative process in Section 2.1.3:

P (zik = 1|z−ik) =

=

=

¯Φz<ik zik=1 + ¯Φz<ik zik=0

¯Φz<ik zik=1

j(cid:54)=i wijzjk

(cid:1)(N − m−ik − 1)!m−ik!

exp(cid:0)(cid:80)
(cid:1)(N − m−ik − 1)!m−ik! + (N − m−ik)!(m−ik − 1)!
(cid:1)m−ik + (N − m−ik)

(cid:1)m−ik

j(cid:54)=i wijzjk

exp(cid:0)(cid:80)

j(cid:54)=i wijzjk

j(cid:54)=i wijzjk

exp(cid:0)(cid:80)
exp(cid:0)(cid:80)

5

We could also get the result using the limiting probability in Equation (8). The probability of each
zik given all other variables is: P (zik|X  Y  Z−(ik)) ∝ P (X  Y|Z−(ik)  zik)P (zik|z−ik). We need
only to condition on z−ik since columns of Z are generated independently.

3.2 Sampling other variables

Sampling a new column of Z: New columns are columns that do not yet have any entries equal
to 1 (empty groups). When sampling for an entity i  we assume this is the last customer in line.
Therefore  based on the generative process described in Section 2.1.3  the number of new features
are Poisson( α
N ). For each new column  we need to sample a new group speciﬁc coefﬁcient variable
sk. We can simply sample from the prior distribution given in Equation (11) since the probability
P (X  Y|Z  Θ) is not affected by these new columns since no interactions are currently represented
by these columns.
Sampling sk for populated columns: Since we do not have a conjugate prior on s  we cannot com-
pute the conditional likelihood directly. We turn to Metropolis-Hasting to sample s. The proposed
distribution of a new value s∗
h ) where h is the
shape parameter. The mean of this distribution is the old value sk. The acceptance ratio is

k given the old value sk is q(s∗

(cid:104)

A(sk → s∗

k) = min

1 

k|sk) = Gamma(h  sk
k|αs  βs) q(sk|s∗
k)

k) p(s∗

(cid:105)

P (X  Y|Z  Θ \ {sk}  s∗

P (X  Y|Z  Θ) p(sk|αs  βs) q(s∗

k|sk)

In our experiments  h is selected so that the average acceptance rate is around 0.25 [5].
Sampling r  µ  σ2 and prior parameters: Closed-form formulas for the posterior distributions of
r µ and σ2 can be derived due to their conjugacy. For example  the posterior distribution of 1/σ2

2  (cid:0) 1
given the other variables is: Gamma(cid:0)αv + M T

(cid:1)−1(cid:1).Equations for updates

i(xi−¯xi)T (xi−¯xi)

µ since we
of r and µ are omitted due to lack of space. Gibbs sampling steps are used for σ2
can compute the posterior distribution with conjugate priors. For prior parameters {αs  βs  αv  βv} 
we use Metropolis-Hasting steps discussed previously.

r and σ2

+

βv

(cid:80)

2

4 Results

We name our method GroupMiR (Group MiRNA target prediction). In this section we compare the
performance of GroupMiR with GenMiR++ [8]  which is one of the popular methods for predict-
ing miRNA-mRNA interactions. However  unlike our method it does not use grouping of mRNAs
and attempts to predict each one separately. Besides  there are two other important differences of
GenMiR++ from our method: 1) GenMiR++ only consider interactions in the candidate set while
our method consider all possible interactions. 2) GenMiR++ accepts a binary matrix as a candi-
date set while our method allows continuous valued scores. To our best knowledge  GenMiR++ 
which uses the regression model for interaction between entities  is the only appropriate method2
for comparison.

4.1 Synthetic data

We generated 9 synthetic datasets. Each dataset contains 20 miRNAs and 200 mRNAs. We set
the number of groups to K = 5 and T = 10 for all datasets. The miRNA membership V is a
random matrix with at most 5 ones in each column. The mRNA membership U is a random matrix
with density of 0.1. The expression of mRNAs are generated from the model in Equation (10) with
σ2 = 1. The remaining random variables are sampled as follows: y ∼ N (0  1)  s ∼ N (1  0.1)
and r ∼ N (0  0.1). Since the sequence based predictions of miRNA-mRNA interactions are based
on short complementary regions they often result in many more false positives than false negatives.
We thus introduce noise to the true binary interaction matrix C(cid:48) by probabilistically changing each
zero value in that matrix to 1. We tested different noise probabilities: 0.1  0.2  0.4 and 0.8. We use
C = 2C(cid:48) − 1.8  α = 1 and the hyperprior parameters are set to generic values. Our sampler is ran
for 2000 iterations and 1000 iterations are discarded as burn-in.

2We also tested with the original IBP (by setting W = 0). The results for both the synthetic and real data

were too weak to be comparable with GenMIR++. See Appendix.

6

(a) Truth (b) 0.1

(c) 0.2

(d) 0.4

(e) 0.8

Figure 1: The posterior distribution of K.

Figure 2: An example synthetic dataset.

Figure 1 plots the estimated posterior distribution of K from the samples of the 9 datasets for all
noise levels. As can be seen  when the noise level is small (0.1)  the distributions are correctly
centered around K = 5. With increasing noise levels  the number of groups is overestimated.
However  GroupMiR still does very well at a noise level of 0.4 and estimates for the higher noise
level are also within a reasonable range.
We estimated a posterior mean for the interaction matrix Z by ﬁrst ordering the columns of each
sampled Z and then selecting the mode from the set of Z matrices. GenMiR++ returns a score value
in [0  1] for each potential interaction. To convert these to binary interactions we tested a number
of different threshold cutoffs: 0.5  0.7 and 0.9. Figure 3 presents a number of quality measures
for the recovered interactions by the two methods. GroupMiR achieves the best F1 score across all
noise levels greatly improving upon GenMiR++ when high noise levels are considered (a reasonable
biological scenario). In general  while the precision is very high for all noise levels  recall drops to
a lower rate. From a biological point of view  precision is probably more important than recall since
each of the predictions needs to be experimentally tested  a process that is often time consuming and
expensive.
In addition to accurately recovering interactions between miRNAs and mRNAs  GroupMiR also
correctly recovers the groupings of mRNA and miRNAs. Figure 2 presents a graphical view of
the group membership in both the true model and the model recovered by GroupMiR for one of
the synthetic datasets. As can be seen  our method is able to accurately recover the groupings of
both miRNAs and mRNAs with moderate noise levels (up to 0.4). For the higher noise level (0.8)
the method assigns more groups than in the underlying model. However  most interactions are still
correctly recovered. These results hold for all datasets we tested (not shown due to lack of space).

(a) Recall

(b) Accuracy

(c) Precision

(d) F1 Score

Figure 3: Performance of GroupMiR versus GenMiR++: Each data point is a synthetic dataset.

4.2 Application to mouse lung development

To test our method on real biological data  we used a mouse lung developmental dataset [6]. In this
study  the authors used microarrays to proﬁle both miRNAs and mRNAs at 7 time points  which
include all recognized stages of lung development. We downloaded the log ratio normalized data
collected in this study. Duplicate samples were averaged and median values of all probes were
assigned to genes. As suggested in the paper  we used ratios to the last time point resulting in
6 values for each mRNA and miRNA. Priors for interaction between miRNA and mRNA were
downloaded from the MicroCosm Target3 database. Selecting genes with variance in the top 10% 
led to 219 miRNAs and 1498 mRNAs which were used for further analysis.
We collected 5000 samples of the interaction matrix Z following a 5000 iteration burn-in period.
Convergence of the MCMC chain is determined by monitoring trace plots of K in multiple chains.

3http://www.ebi.ac.uk/enright-srv/microcosm/

7

0.10.20.40.800.20.40.60.81NoiseRate0.10.20.40.80.40.60.81NoiseRate0.10.20.40.80.20.40.60.81NoiseRate0.10.20.40.800.20.40.60.81NoiseScore(a)

(b)

(c)

(d)

(e)

(f)

Figure 4: Interaction network recovered by GroupMiR: Each node is a pie chart corresponding to
its expression values in the 6 time points (red: up-regulation  green: down-regulation).
Since there are many more entries for real data compared to synthetic data we computed a consensus
for Z by reordering columns in each sample and averaging the entries across all matrices.
We further analyzed the network constructed from groups with at least 90% posterior probability.
The network recovered by GroupMiR is more connected (89 nodes and 208 edges) when compared
to the network recovered by GenMiR++ (using equivalent 0.9 threshold) with 37 nodes and 34 edges
(Appendix). We used Cytoscape [16] to visualize the 6 groups of interactions in Figure 4. The
network contains several groups of co-expressed miRNAs controlling sets of mRNA  in agreement
with previous biological studies [20].
To test the function of the clusters identiﬁed  we performed Gene Ontology (GO) enrichment anal-
ysis for the mRNAs using GOstat [3]. The full results (Bonferroni corrected) are presented in Ap-
pendix. As can be seen  several cell division categories are enriched in cluster (b) which is expected
when dealing with a developing organ (which undergoes several rounds of cell division). Other
signiﬁcant functions include organelle organization and apoptosis which also are associated with
development (cluster (c)). We performed similar GO enrichment analysis for the GenMiR++ results
and for K-means when using the same set of mRNAs (setting k = 6 as in our model). In both cases
we did not ﬁnd any signiﬁcant enrichment indicating that only by integrating sets of miRNAs with
the mRNAs for this data we can ﬁnd functional biological groupings. See Appendix for details.
We have also looked at the miRNAs controlling the different clusters and found that in a number of
cases these agreed with prior knowledge. Cluster (a) includes 2 members of the miR 17-92 cluster 
which is known to be critical to lung organogenesis [18]. MiRNA families miR-30  miR-29  miR-20
and miR-16  all identiﬁed by our method  were also reported to play roles in the early stages of lung
organogenesis [6]. It is important to point out that we did not ﬁlter miRNAs explicitly based on
expression but these miRNAs came in the results based on their strong effect on mRNA expression.

5 Conclusions

We have described an extension to IBP that allows us to integrate priors on interactions between
entities with measured properties for individual entities when constructing interaction networks.
The method was successfully applied to predict miRNA-mRNA interactions and we have shown
that it works well on both synthetic and real data. While our focus in this paper was on a biolog-
ical problem  several other datasets provide similar information including social networking data.
Our method is appropriate for such datasets and can help when attempting to construct interaction
networks based on observations.

Acknowledgments

This work is supported in part by NIH grants 1RO1GM085022  1U01HL108642 and NSF grant
DBI-0965316 to Z.B.J.

8

Myh14Slc6a14mmu-miR-106bAfpEar11Ear2Cyp2f2mmu-miR-93Ear1mmu-miR-106ammu-miR-20aTmem100Lgals3mmu-miR-17Lilrb3Parp3Dapk2Fmo1Brca1AurkbH19BlmCol24a14932413O14Rik2610318N02RikCcna2Pdgfcmmu-miR-29cTmem48Agtr2mmu-miR-29aPafah1b3Col11a16720463M24RikLin7aLmnb1Dbf4Mki67Kntc1Fkbp3Etaa1Wdr75Dhx9Diap3Tubb3PpihMsh6Fancd2Tcfl54732471D19Rikmmu-miR-30cmmu-miR-30bZranb3Igsf9Ppa1mmu-miR-30dmmu-miR-30emmu-miR-30aCenpqBrca2Ube2tMthfd1lRad54lAbcb7mmu-miR-30e*mmu-miR-30a*C330027C09RikHmmrMcm8Cdca8Gins1Dlk1mmu-miR-195Palb2ClspnWnk3mmu-miR-16Shcbp1Top2aDusp9mmu-miR-27bAurkaKif2cRrm2mmu-miR-27aNrkReferences
[1] E.M. Airoldi  D.M. Blei  S.E. Fienberg  and E.P. Xing. Mixed membership stochastic block-

models. The Journal of Machine Learning Research  9:1981–2014  2008.

[2] Z. Bar-Joseph  G.K. Gerber  T.I. Lee  et al. Computational discovery of gene modules and

regulatory networks. Nature biotechnology  21(11):1337–1342  2003.

[3] T. Beißbarth and T.P. Speed. GOstat: ﬁnd statistically overrepresented Gene Ontologies within

a group of genes. Bioinformatics  20(9):1464  2004.

[4] David M. Blei and Peter Frazier. Distance dependent chinese restaurant processes. In Johannes

Frnkranz and Thorsten Joachims  editors  ICML  pages 87–94. Omnipress  2010.

[5] S. Chib and E. Greenberg. Understanding the metropolis-hastings algorithm. Amer. Statisti-

cian  49(4):327–335  1995.

[6] J. Dong  G. Jiang  Y.W. Asmann  S. Tomaszek  et al. MicroRNA Networks in Mouse Lung

Organogenesis. PloS one  5(5):4645–4652  2010.

[7] T. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In

Advances in Neural Information Processing Systems  18:475  2006.

[8] J.C. Huang  T. Babak  T.W. Corson  et al. Using expression proﬁling data to identify human

microRNA targets. Nature methods  4(12):1045–1049  2007.

[9] C. Kemp  J.B. Tenenbaum  T.L. Grifﬁths    et al. Learning systems of concepts with an inﬁnite

relational model. In Proc. 21st Natl Conf. Artif. Intell.(1)  page 381  2006.

[10] E. Meeds  Z. Ghahramani  R.M. Neal  and S.T. Roweis. Modeling dyadic data with binary

latent factors. In Advances in NIPS  19:977  2007.

[11] K.T. Miller  T.L. Grifﬁths  and M.I. Jordan. The phylogenetic indian buffet process: A non-

exchangeable nonparametric prior for latent features. In UAI  2008.

[12] K.T. Miller  T.L. Grifﬁths  and M.I. Jordan. Nonparametric latent feature models for link

prediction. In Advances in Neural Information Processing Systems  2009.

[13] P. Orbanz and J.M. Buhmann. Nonparametric bayesian image segmentation.

Journal of Computer Vision  77(1):25–45  2008.

International

[14] ME Peter. Targeting of mrnas by multiple mirnas: the next step. Oncogene  29(15):2161–2164 

2010.

[15] N. Rajewsky. microRNA target predictions in animals. Nature genetics  38:S8–S13  2006.
[16] P. Shannon  A. Markiel  O. Ozier  et al. Cytoscape: a software environment for integrated

models of biomolecular interaction networks. Genome research  13(11):2498  2003.

[17] F. Stingo  Y. Chen  M. Vannucci  et al. A Bayesian graphical modeling approach to microRNA

regulatory network inference. Ann. Appl. Statist  2010.

[18] A. Ventura  A.G. Young  M.M. Winslow  et al. Targeted Deletion Reveals Essential and Over-

lapping Functions of the miR-17-92 Family of miRNA Clusters. Cell  132:875–886  2008.

[19] Y.J. Wang and G.Y. Wong. Stochastic blockmodels for directed graphs. Journal of the Ameri-

can Statistical Association  82(397):8–19  1987.

[20] C. Xiao and K. Rajewsky. MicroRNA control in the immune system: basic principles. Cell 

136(1):26–36  2009.

9

,Ofer Meshi
Mehrdad Mahdavi
Alex Schwing
Sebastian Nowozin
Botond Cseke
Ryota Tomioka
Paroma Varma
Bryan He
Payal Bajaj
Nishith Khandwala
Imon Banerjee
Daniel Rubin
Christopher Ré