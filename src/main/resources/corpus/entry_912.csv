2007,Catching Change-points with Lasso,We propose a new approach for dealing with the estimation of the location of change-points in one-dimensional piecewise constant signals observed in white noise. Our approach consists in reframing this task in a variable selection context. We use a penalized least-squares criterion with a l1-type penalty for this purpose. We prove that  in an appropriate asymptotic framework  this method provides consistent estimators of the change-points. Then  we explain how to implement this method in practice by combining the LAR algorithm and a reduced version of the dynamic programming algorithm and we apply it to synthetic and real data.,Catching Change-points with Lasso

Zaid Harchaoui  C´eline L´evy-Leduc
LTCI  TELECOM ParisTech and CNRS
37/39 Rue Dareau  75014 Paris  France
{zharchao levyledu}@enst.fr

Abstract

We propose a new approach for dealing with the estimation of the location of
change-points in one-dimensional piecewise constant signals observed in white
noise. Our approach consists in reframing this task in a variable selection con-
text. We use a penalized least-squares criterion with a `1-type penalty for this
purpose. We prove some theoretical results on the estimated change-points and
on the underlying piecewise constant estimated function. Then  we explain how
to implement this method in practice by combining the LAR algorithm and a re-
duced version of the dynamic programming algorithm and we apply it to synthetic
and real data.

1 Introduction

Change-points detection tasks are pervasive in various ﬁelds  ranging from audio [10] to EEG seg-
mentation [5]. The goal is to partition a signal into several homogeneous segments of variable
durations  in which some quantity remains approximately constant over time. This issue was ad-
dressed in a large literature (see [20] [11])  where the problem was tackled both from an online
(sequential) [1] and an off-line (retrospective) [5] points of view. Most off-line approaches rely on a
Dynamic Programming algorithm (DP)  allowing to retrieve K change-points within n observations
of a signal with a complexity of O(Kn2) in time [11]. Such a feature refrains practitioners from
applying these methods to large datasets. Moreover  one often observes a sub-optimal behavior of
the raw DP algorithm on real datasets.

We suggest here to slightly depart from this line of research  by focusing on a reformulation of
change-point estimation in a variable selection framework. Then  estimating change-point loca-
tions off-line turns into performing variable selection on dummy variables representing all possible
change-point locations. This allows us to take advantage of the latest theoretical [23]  [3] and prac-
tical [7] advances in regression with Lasso penalty. Indeed  Lasso provides us with a very efﬁcient
method for selecting potential change-point locations. This selection is then reﬁned by using the DP
algorithm to estimate the change-point locations.

Let us outline the paper. In Section 2  we ﬁrst describe our theoretical reformulation of off-line
change-point estimation as regression with a Lasso penalty. Then  we show that the estimated mag-
nitude of jumps are close in mean  in a sense to be precized  to the true magnitude of jumps. We
also give a non asymptotic inequality to upper-bound the `2-loss of the true underlying piecewise
constant function and the estimated one. We describe our algorithm in Section 3. In Section 4  we
discuss related works. Finally  we provide experimental evidence of the relevance of our approach.

1

2 Theoretical approach

2.1 Framework

k ’s in the following model:
Yt = µ?

We describe  in this section  how off-line change-point estimation can be cast as a variable selection
problem. Off-line estimation of change-point locations within a signal (Yt) consists in estimating
the τ ?

k + εt 

(1)
where εt are i.i.d zero-mean random variables with ﬁnite variance. This problem can be reformulated
as follows. Let us consider:

k   1 ≤ k ≤ K ? with τ ?

k−1 + 1 ≤ t ≤ τ ?

t = 1  . . .   n such that τ ?

0 = 0 

Yn = Xnβn + εn

(2)
where Yn is a n × 1 vector of observations  Xn is a n × n lower triangular matrix with nonzero
j ’s
n)0 is a zero-mean random vector such that the εn
elements equal to one and εn = (εn
are i.i.d with ﬁnite variance. As for βn  it is a n × 1 vector having all its components equal to
zero except those corresponding to the change-point instants. The above multiple change-point
estimation problem (1) can thus be tackled as a variable selection one:

1   . . .   εn

Minimize

β

kYn − Xnβk2

n subject to kβk1 ≤ s  

(3)

where kuk1 and kukn are deﬁned for a vector u = (u1  . . .   un) ∈ Rn by kuk1 = Pn
and kuk2
following counterpart objective in model (1):

j=1 |uj|
j respectively. Indeed  the above formulation amounts to minimize the

j=1 u2

n = n−1Pn

Minimize
µ1 ... µn

1
n

Xt=1

n

n−1

(Yt − µt)2

subject to

|µt+1 − µt| ≤ s 

(4)

Xt=1

which consists in imposing an `1-constraint on the magnitude of jumps. The underpinning insight
is the sparsity-enforcing property of the `1-constraint  which is expected to give a sparse vector 
whose non-zero components would match with those of βn and thus with change-point locations.
It is related to the popular Least Absolute Shrinkage eStimatOr (LASSO) in least-square regression
of [21]  used for efﬁcient variable selection.

In the next section  we provide two results supporting the use of the formulation (3) for off-line
multiple change-point estimation. We show that estimates of jumps minimizing (3) are consistent
in mean  and we provide a non asymptotic upper bound for the `2 loss of the underlying estimated
piecewise constant function and the true underlying piecewise function. This inequality shows that 
at a precized rate  the estimated piecewise constant function tends to the true piecewise constant
function with a probability tending to one.

2.2 Main results

In this section  we shall study the properties of the solutions of the problem (3) deﬁned by

(5)

(6)

ˆβn(λ) = Arg min

β

nkYn − Xnβk2

n + λkβk1o .

Let us now introduce the notation sign. It maps positive entry to 1  negative entry to -1 and a null
entry to zero. Let

and let C n the covariance matrix be deﬁned by

A = {k  βn

k 6= 0} and A = {1  . . .   n}\A

In a general regression framework  [18] recall that  with probability tending to one  ˆβn(λ) and βn
have the same sign for a well-chosen λ  only if the following condition holds element-wise:

C n = n−1X 0

nXn .

(7)

(8)
IJ is a sub-matrix of C n obtained by keeping rows with index in the set I and columns with
k )k∈A. The condition (8) is not fulﬁlled in the

A)¯¯ < 1 

where C n
index in J. The vector βn

A is deﬁned by βn

AA)−1sign(βn

¯¯C n

A = (βn

AA(C n

2

change-point framework implying that we cannot have a perfect estimation of the change-points as
it is already known  see [13]. But  following [18] and [3]  we can prove some consistency results 
see Propositions 1 and 2 below.

In the following  we shall assume that the number of break points is equal to K ?.
The following proposition ensures that for a large enough value of n the estimated change-point
locations are close to the true change-points.
Proposition 1. Assume that the observations (Yn) are given by (2) and that the εn
If λ = λn is such that λn√n → 0 as n tends to inﬁnity then

j ’s are centered.

kE( ˆβn(λn)) − βnkn → 0 .

Proof. We shall follow the proof of Theorem 1 in [18]. For this  we denote βn(λ) the estimator
ˆβn(λ) under the absence of noise and γn(λ) the bias associated to the Lasso estimator: γn(λ) =
βn(λ) − βn. For notational simplicity  we shall write γ instead of γn(λ). Note that γ satisﬁes the
following minimization: γ = Arg minζ∈Rn f (ζ)   where

f (ζ) = ζ 0C nζ + λXk∈A

k + ζk| + λXk∈ ¯A
|βn

|ζk| .

Since f (γ) ≤ f (0)  we get

γ0C nγ + λXk∈A

k + γk| + λXk∈ ¯A
|βn

We thus obtain using the Cauchy-Schwarz inequality the following upper bound

|βn
k | .

|γk| ≤ λXk∈A
|γk|2!1/2

.

|γk| ≤ λ√K ?Ã n
Xk=1

γ0C nγ ≤ λXk∈A
k=1 |γk|2  we obtain: kγkn ≤ λ√nK ?.

Using that γ0C nγ ≥ n−1Pn

The following proposition ensures  thanks to a non asymptotic result  that the estimated underlying
piecewise function is close to the true piecewise constant function.
Proposition 2. Assume that the observations (Yn) are given by (2) and that the εn
Gaussian random variables with variance σ2 > 0. Assume also that (βn

j ’s are centered iid
k )k∈A belong to (βmin  βmax)
where βmin > 0. For all n ≥ 1 and A > √2 then  with a probability larger than 1 − n1−A2/2  if
λn = Aσplog n/n 

kXn( ˆβn(λn) − βn)k2

n ≤ 2AσβmaxK ?r log n

n

.

Proof. By deﬁnition of ˆβn(λ) in (5) as a minimizer of a criterion  we have

kYn − Xn ˆβn(λ)k2

n + λk ˆβn(λ)k1 ≤ kYn − Xnβnk2

n + λkβnk1 .

Using (2)  we get

kXn(βn − ˆβn(λ))k2

n +

2
n

(βn − ˆβn(λ))0X 0

nεn + λ

n

Xj=1

| ˆβn
j (λ)| ≤ λ

n

Xj=1

|βn
j | .

Thus 

kXn(βn − ˆβn(λ))k2

n ≤

2
n

Observe that

( ˆβn(λ) − βn)0X 0

nεn + λXj∈A

(|βn

j | − | ˆβn

j (λ)|) − λXj∈ ¯A

| ˆβn
j (λ)| .

2
n

( ˆβn(λ) − βn)0X 0

nεn = 2

n

Xj=1

( ˆβn

j )
j (λ) − βn


1
n

n

Xi=j

εn

i
 .

3

n

n

n

εn

Xj=1

P( ¯E) ≤

i=j εn
zero-mean Gaussian random variables  we obtain

i¯¯¯ ≤ λo. Then  using the fact that the εn
j=1nn−1¯¯¯Pn
Let us deﬁne the event E = Tn
i¯¯¯¯¯¯
n−1¯¯¯¯¯¯
> λ
P
2σ2(n − j + 1)¶ .
Xj=1
Xi=j
 ≤
Thus  if λ = λn = Aσplog n/n 
With a probability larger than 1 − n1−A2/2  we get
| ˆβn
j (λ) − βn

j |) − λnXj∈ ¯A
We thus obtain with a probability larger than 1 − n1−A2/2 the following upper bound

kXn(βn − ˆβn(λ))k2

j | + λnXj∈A

P( ¯E) ≤ n1−A2/2 .

expµ−

j | − | ˆβn

n ≤ λn

Xj=1

(|βn

n2λ2

n

j | = 2Aσr log n
|βn

n Xj∈A

j | ≤ 2AσβmaxK ?r log n
|βn

n

.

i ’s are iid

| ˆβn
j | .

kXn(βn − ˆβn(λ))k2

n ≤ 2λnXj∈A

3 Practical approach

The previous results need to be efﬁciently implemented to cope with ﬁnite datasets. Our algorithm 
called Cachalot (CAtching CHAnge-points with LassO)  can be split into the following three steps
described hereafter.

Estimation with a Lasso penalty We compute the ﬁrst Kmax non-null coefﬁcients ˆβτ1  . . .   ˆβτKmax
on the regularization path of the LASSO problem (3). The LAR/LASSO algorithm  as described in
[7]  provides an efﬁcient algorithm to compute the entire regularization path for the LASSO problem.

SincePj |βj| ≤ s is a sparsity-enforcing constraint  the set {j  ˆβj 6= 0} = {τj} becomes larger as

we run through the regularization path. We shall denote by S the Kmax-selected variables:

(9)
The computational complexity of the Kmax-long regularization path of LASSO solutions is
maxn). Most of the time  we can see that the Lasso effectively catches the true change-
O(K 3
point but also irrelevant change-points at the vicinity of the true ones. Therefore  we propose to
reﬁne the set of change-points caught by the Lasso by performing a post-selection.

S = {τ1  . . .   τKmax} .

max + K 2

Reduced Dynamic Programming algorithm One can consider several strategies to remove ir-
relevant change-points from the ones retrieved by the Lasso. Among them  since usually in appli-
cations  one is only interested in change-point estimation up to a given accuracy  we could launch
the Lasso on a subsample of the signal. Here  we suggest to perform post-selection by using the
standard Dynamic Programming algorithm (DP) thoroughly described in [11] (Chapter 12  p. 450)
but on the reduced set S instead of {1  . . .   n}. This algorithm allows one to efﬁciently minimize
the following objective for each K in {1  . . .   Kmax}:
Xk=1

Xi=τk−1+1

(Yi − ˆµk)2 

s.t τ1 ... τK ∈S

J(K) =

τ1<···<τK

(10)

Min

S being deﬁned in (9) and outputs for each K 
the corresponding subset of change-points
(ˆτ1  . . .   ˆτK). The DP algorithm has a computational complexity of O(Kmax n2) if we look for
at most Kmax change-points within the signal. Here  our reduced DP calculations (rDP) scales
as O(Kmax K 2
max) where Kmax is the maximum number of change-points/variables selected by
LAR/LASSO algorithm. Since typically Kmax ¿ n  our method thus provides a reduction of the
computational burden associated with the classical change-points detection approach which consists
in running the DP algorithm over all the n observations.

τk

K

4

Selecting the number of change-points The point is now to select the adequate number of
change-points. As n → ∞  according to [15]  the ratio ρk = J(k + 1)/J(k) should show different
qualitative behavior when k 6 K ? and when k > K ?  K ? being the true number of change-points.
In particular  ρk ≥ Cn for k > K ?  where Cn → 1 as n → ∞. Actually we found out that Cn was
close to 1  even in small-sample settings  for various experimental designs in terms of noise variance
and true number of change-points. Hence  conciliating theoretical guidance in large-sample setting
and experimental ﬁndings in ﬁxed-sample setting  we suggest the following rule of thumb for select-
ing the number of change-points ˆK : ˆK = Mink≥1 {ρk ≥ 1 − ν}   where ρk = J(k + 1)/J(k).
Cachalot Algorithm
Input

• Vector of observations Y ∈ Rn
• Upper bound Kmax on the number of change-points
• Model selection threshold ν

Processing

1. Compute the ﬁrst Kmax non-null coefﬁcients (βτ1   . . .   βτKmax ) on the regularization path

with the LAR/LASSO algorithm.

2. Launch the rDP algorithm on the set of potential change-points (τ1  . . .   τKmax).
3. Select the smallest subset of the potential change-points (τ1  . . .   τKmax ) selected by the rDP

algorithm for which ρk ≥ 1 − ν.

Output Change-point locations estimates ˆτ1  . . .   ˆτ ˆK.

(Yn)

To illustrate our algorithm  we consider observations
(2) with
(β30  β50  β70  β90) = (5 −3  4 −2)  the other βj being equal to zero  n = 100 and εn a Gaus-
sian random vector with a covariance matrix equal to Id  Id being a n × n identity matrix. The
set of the ﬁrst nine active variables caught by the Lasso along the regularization path  i.e. the set
{k  ˆβk 6= 0} is given in this case by: S = {21  23  28  29  30  50  69  70  90}. The set S contains
the true change-points but also irrelevant ones close to the true change-points. Moreover the most
signiﬁcant variables do not necessarily appear at the beginning. This supports the use of the re-
duced version of the DP algorithm hereafter. Table 1 gathers the J(K)  K = 1  . . .   Kmax and the
corresponding (ˆτ1  . . .   ˆτK).

satisfying model

Table 1: Toy example: The empirical risk J and the estimated change-points as a function of the
possible number of change-points K

K
0
1
2
3
4
5
6
7
8
9

J(K)
696.28
249.24
209.94
146.29
120.21
118.22
116.97
116.66
116.65
116.64

(ˆτ1  . . .   ˆτK )

∅
30

(30 70)

(30 50 69)

(30 50 70 90)

(30 50 69 70 90)

(21 30 50 69 70 90)

(21 29 30 50 69 70 90)

(21 23 29 30 50 69 70 90)

(21 23 28 29 30 50 69 70 90)

The different values of the ratio ρk for k = 0  . . .   8 of the model selection procedure are given in
Table 2. Here we took ν = 0.05. We conclude  as expected  that ˆK = 4 and that the change-points
are (30  50  70  90)  thanks to the results obtained in Table 1.

4 Discussion

Off-line multiple change-point estimation has recently received much attention in theoretical works 
both in a non-asymptotic and in an asymptotic setting by [17] and [13] respectively. From a practi-
cal point of view  retrieving the set of change-point locations {τ ?
K} is challenging  since it is

1   . . .   τ ?

5

Table 2: Toy example: The values of the ratio (ρk = J(k + 1)/J(k)  k = 0  . . .   8)

k
ρk

0
0.3580

1
0.8423

2
0.6968

3
0.8218

4
0.9834

5
0.9894

6
0.9974

7
0.9999

8
1.0000

plagued by the curse of dimensionality. Indeed  all of the n observation times have to be considered
as potential change-point instants. Yet  a dynamic programming algorithm (DP)  proposed by [9]
and [2]  allows to explore all the conﬁgurations with a complexity of O(n3) in time. Then selecting
the number of change-points is usually performed thanks to a Schwarz-like penalty λnK  where
λn has to be calibrated on data [13] [12]  or a penalty K(a + b log(n/K)) as in [17] [14]  where
a and b are data-driven as well. We should also mention that an abundant literature tackles both
change-point estimation and model selection issues from a Bayesian point of view (see [20] [8] and
references therein). All approaches cited above rely on DP  or variants in Bayesian settings  and
hence yield a computational complexity of O(n3)  which makes them inappropriate for very large-
scale signal segmentation. Moreover  despite its theoretical optimality in a maximum likelihood
framework  raw DP may sometimes have poor performances when applied to very noisy obser-
vations. Our alternative framework for multiple change-point estimation was previously elusively
mentioned several times  e.g.
in [16] [4] [19]. However up to our knowledge neither successful
practical implementation nor theoretical grounding was given so far to support such an approach
for change-point estimation. Let us also mention [22]  where the Fused Lasso is applied in a simi-
lar yet different way to perform hot-spot detection. However  this approach includes an additional
penalty  penalizing departures from the overall mean of the observations  and should thus rather be
considered as an outlier detection method.

5 Comparison with other methods

5.1 Synthetic data

We propose to compare our algorithm with a recent method based on a penalized least-squares crite-
rion studied by [12]. The main difﬁculty in such approaches is the choice of the constants appearing
in the penalty. In [12]  a very efﬁcient approach to overcome this difﬁculty has been proposed: the
choice of the constants is completely data-driven and has been implemented in a toolbox available
online at http://www.math.u-psud.fr/˜lavielle/programs/index.html.
In the following  we benchmark our algorithm: A together with the latter method: B. We shall
use Recall and Precision as relevant performance measures to analyze the previous two algorithms.
More precisely  the Recall corresponds to the ratio of change-points retrieved by a method with
those really present in the data. As for the Precision  it corresponds to the number of change-points
retrieved divided by the number of suggested change-points. We shall also estimate the probability
of false alarm corresponding to the number of suggested change-points which are not present in the
signal divided by the number of true change-points.
To compute the precision and the recall of methods A and B  we ran Monte-Carlo experiments. More
precisely  we sampled 30 conﬁgurations of change-points for each real number of change-points K ?
equal to 5  10  15 and 20 within a signal containing 500 observations. Change-points were at least
distant of 10 observations. We sampled 30 conﬁgurations of levels from a Gaussian distribution.
We used the following setting for the noise: for each conﬁguration of change-points and levels 
we synthesized a Gaussian white noise such that the standard deviation is set to a multiple of the
minimum magnitude jump between two contiguous segments  i.e. σ = m Mink(µ∗
k)  µ?
k
being the level of the kth segment. The number of noise replications was set to 10.
As shown in Tables 3  4 and 5 below  our method A yields competitive results compared to method
B with 1 − ν = 0.99 and Kmax = 50. Performances in recall are comparable whereas method A
provides better results than method B in terms of precision and false alarm rate.
5.2 Real data

k+1 − µ∗

In this section  we propose to apply our method previously described to real data which have already
been analyzed by Bayesian methods: the well-log data which are described in [20] and [6] and

6

Table 3: Precision of methods A and B
K ? = 15

K ? = 10

K ? = 20

K ? = 5
B

A

A
A
0.95±0.05 0.86±0.13 0.97±0.03 0.91±0.09
0.81±0.15 0.71±0.29 0.89±0.08 0.8±0.22
0.8±0.16
0.95±0.05 0.86±0.13 0.97±0.03 0.92±0.09
0.73±0.29 0.89±0.08 0.8±0.21
0.78±0.17 0.71±0.27 0.88±0.09 0.78±0.21 0.93±0.06 0.85±0.13 0.96±0.04 0.9±0.09
0.73±0.19 0.66±0.28 0.84±0.1
0.93±0.06 0.84±0.13 0.95±0.04 0.9±0.1

0.79±0.2

B

B

A

B

Table 4: Recall of methods A and B

K ? = 5
B

K ? = 10

K ? = 15

K ? = 20

A

A
B
0.99±0.02 0.99±0.02 1±0
1±0
0.98±0.04 0.99±0.03 0.99±0.01 0.99±0.01 0.99±0.01 0.99±0.01 0.99±0.01 1±0
0.95±0.08 0.94±0.08 0.96±0.06 0.96±0.05 0.97±0.03 0.97±0.04 0.97±0.03 0.98±0.02
0.85±0.16 0.87±0.15 0.92±0.07 0.91±0.09 0.94±0.06 0.94±0.06 0.95±0.04 0.96±0.04

A
0.99±0

A
0.99±0

B
0.99±0

B
1±0

Table 5: False alarm rate of methods A and B

K ? = 5
B

K ? = 10

K ? = 15

K ? = 20

A
0.13±0.03 0.23±0.2
0.13±0.03 0.22±0.2
0.13±0.03 0.21±0.18 0.23±0.03 0.32±0.18 0.33±0.02 0.4±0.13
0.13±0.03 0.21±0.2
0.23±0.03 0.29±0.16 0.31±0.03 0.4±0.15

A
0.24±0.03 0.33±0.19 0.34±0.02 0.42±0.13 0.44±0.02 0.51±0.12
0.23±0.03 0.32±0.18 0.33±0.02 0.41±0.13 0.44±0.02 0.5±0.11
0.43±0.03 0.5±0.12
0.42±0.03 0.48±0.11

B

B

B

A

A

Method
m = 0.1
m = 0.5
m = 1.0
m = 1.5

Method
m = 0.1
m = 0.5
m = 1.0
m = 1.5

Method
m = 0.1
m = 0.5
m = 1.0
m = 1.5

displayed in Figure 1. They consist in nuclear magnetic response measurements expected to carry
information about rock structure and especially its stratiﬁcation.

One distinctive feature of these data is that they typically contain a non-negligible amount of outliers.
The multiple change-point estimation method should then  either be used after a data cleaning step
(median ﬁltering [6])  or explicitly make heavy-tailed noise distribution assumption. We restricted
ourselves to a median ﬁltering pre-processing. The results given by our method applied to the well-
log data processed with a median ﬁlter are displayed in Figure 1 for Kmax = 200 and 1 − ν = 0.99.
The vertical lines locate the change-points. We can note that they are close to those found out by [6]
(P. 206) who used Bayesian techniques to perform change-points detection.

x 105

1.5

1.4

1.3

1.2

1.1

1

0.9

0.8

0.7

0.6

0

500

1000

1500

2000

2500

3000

3500

4000

4500

x 105

1.4

1.35

1.3

1.25

1.2

1.15

1.1

1.05

1

0.95

0.9
0

500

1000

1500

2000

2500

3000

3500

4000

4500

Figure 1: Left: Raw well-log data  Right: Change-points locations obtained with our method in
well-log data processed with a median ﬁlter

7

6 Conclusion and prospects

We proposed here to cast the multiple change-point estimation as a variable selection problem. A
least-square criterion with a Lasso-penalty yields an efﬁcient primary estimation of change-point
locations. Yet these change-point location estimates can be further reﬁned thanks to a reduced
dynamic programming algorithm. We obtained competitive performances on both artiﬁcial and real
data  in terms of precision  recall and false alarm. Thus  Cachalot is a computationally efﬁcient
multiple change-point estimation method  paving the way for processing large datasets.

References

[1] M. Basseville and N. Nikiforov. The detection of abrupt changes. Information and System sciences series.

Prentice-Hall  1993.

[2] R. Bellman. On the approximation of curves by line segments using dynamic programming. Communi-

cations of the ACM  4(6)  1961.

[3] P. Bickel  Y. Ritov  and A. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Preprint 2007.
[4] L. Boysen  A. Kempe  A. Munk  V. Liebscher  and O. Wittich. Consistencies and rates of convergence of

jump penalized least squares estimators. Annals of Statistics  In revision.

[5] B. Brodsky and B. Darkhovsky. Non-parametric statistical diagnosis: problems and methods. Kluwer

Academic Publishers  2000.

[6] O. Capp´e  E. Moulines  and T. Ryden. Inference in Hidden Markov Models (Springer Series in Statistics).

Springer-Verlag New York  Inc.  2005.

[7] B. Efron  T. Hastie  and R. Tibshirani. Least angle regression. Annals of Statistics  32:407–499  2004.
[8] P. Fearnhead. Exact and efﬁcient bayesian inference for multiple changepoint problems. Statistics and

Computing  16:203–213  2006.

[9] W. D. Fisher. On grouping for maximum homogeneity. Journal of the American Statistical Society 

53:789–798  1958.

[10] O. Gillet  S. Essid  and G. Richard. On the correlation of automatic audio and visual segmentation of

music videos. IEEE Transactions on Circuits and Systems for Video Technology  2007.

[11] S. M. Kay. Fundamentals of statistical signal processing: detection theory. Prentice-Hall  Inc.  1993.
[12] M. Lavielle. Using penalized contrasts for the change-points problems. Signal Processing  85(8):1501–

1510  2005.

[13] M. Lavielle and E. Moulines. Least-squares estimation of an unknown number of shifts in a time series.

Journal of time series analysis  21(1):33–59  2000.

[14] E. Lebarbier. Detecting multiple change-points in the mean of a gaussian process by model selection.

Signal Processing  85(4):717–736  2005.

[15] C.-B. L. Lee. Estimating the number of change-points in a sequence of independent random variables.

Statistics and Probability Letters  25:241–248  1995.

[16] E. Mammen and S. Van De Geer. Locally adaptive regression splines. Annals of Statistics  1997.
[17] P. Massart. A non asymptotic theory for model selection. pages 309–323. European Mathematical Society 

2005.

[18] N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data.

Preprint 2006.

[19] S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics  35  2007.
[20] J. Ruanaidh and W. Fitzgerald. Numerical Bayesian Methods Applied to Signal Processing. Statistics and

Computing. Springer  1996.

[21] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society 

Series B  58(1):267–288  1996.

[22] R. Tibshirani and P. Wang. Spatial smoothing and hot spot detection for cgh data using the fused lasso.

Biostatistics  9(1):18–29  2008.

[23] P. Zhao and B. Yu. On model selection consistency of lasso. Journal Of Machine Learning Research  7 

2006.

8

,Hajin Shim
Sung Ju Hwang
Eunho Yang