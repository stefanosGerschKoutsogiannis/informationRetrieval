2009,Modeling Social Annotation Data with Content Relevance using a Topic Model,We propose a probabilistic topic model for analyzing and extracting content-related annotations from noisy annotated discrete data such as web pages stored in social bookmarking services. In these services  since users can attach annotations freely  some annotations do not describe the semantics of the content  thus they are noisy   i.e.  not content-related. The extraction of content-related annotations can be used as a preprocessing step in machine learning tasks such as text classification and image recognition  or can improve information retrieval performance. The proposed model is a generative model for content and annotations  in which the annotations are assumed to originate either from topics that generated the content or from a general distribution unrelated to the content. We demonstrate the effectiveness of the proposed method by using synthetic data and real social annotation data for text and images.,Modeling Social Annotation Data

with Content Relevance using a Topic Model

Tomoharu Iwata

Takeshi Yamada

Naonori Ueda

NTT Communication Science Laboratories

2-4 Hikaridai  Seika-cho  Soraku-gun  Kyoto  Japan

fiwata yamada uedag@cslab.kecl.ntt.co.jp

Abstract

We propose a probabilistic topic model for analyzing and extracting content-
related annotations from noisy annotated discrete data such as web pages stored
in social bookmarking services. In these services  since users can attach annota-
tions freely  some annotations do not describe the semantics of the content  thus
they are noisy  i.e. not content-related. The extraction of content-related annota-
tions can be used as a preprocessing step in machine learning tasks such as text
classiﬁcation and image recognition  or can improve information retrieval perfor-
mance. The proposed model is a generative model for content and annotations  in
which the annotations are assumed to originate either from topics that generated
the content or from a general distribution unrelated to the content. We demonstrate
the effectiveness of the proposed method by using synthetic data and real social
annotation data for text and images.

1 Introduction

Recently there has been great interest in social annotations  also called collaborative tagging or
folksonomy  created by users freely annotating objects such as web pages [7]  photographs [9] 
blog posts [23]  videos [26]  music [19]  and scientiﬁc papers [5]. Delicious [7]  which is a social
bookmarking service  and Flickr [9]  which is an online photo sharing service  are two representative
social annotation services  and they have succeeded in collecting huge numbers of annotations. Since
users can attach annotations freely in social annotation services  the annotations include those that do
not describe the semantics of the content  and are  therefore  not content-related [10]. For example 
annotations such as ’nikon’ or ’canon’ in a social photo service often represent the name of the
manufacturer of the camera with which the photographs were taken  or annotations such as ’2008’
or ’november’ indicate when they were taken. Other examples of content-unrelated annotations
include those designed to remind the annotator such as ’toread’  those identifying qualities such as
’great’  and those identifying ownership.
Content-unrelated annotations can often constitute noise if used for training samples in machine
learning tasks  such as automatic text classiﬁcation and image recognition. Although the perfor-
mance of a classiﬁer can generally be improved by increasing the number of training samples  noisy
training samples have a detrimental effect on the classiﬁer. We can improve classiﬁer performance
if we can employ huge amounts of social annotation data from which the content-unrelated annota-
tions have been ﬁltered out. Content-unrelated annotations may also constitute noise in information
retrieval. For example  a user may wish to retrieve a photograph of a Nikon camera rather than a
photograph taken by a Nikon camera.
In this paper  we propose a probabilistic topic model for analyzing and extracting content-related
annotations from noisy annotated data. A number of methods for automatic annotation have been
proposed [1  2  8  16  17]. However  they implicitly assume that all annotations are related to content 

1

Table 1: Notation

Symbol Description
D
W
T
K
Nd
Md
wdn
zdn
tdm
cdm
rdm

number of documents
number of unique words
number of unique annotations
number of topics
number of words in the dth document
number of annotations in the dth document
nth word in the dth document  wdn 2 f1;(cid:1)(cid:1)(cid:1) ; Wg
topic of the nth word in the dth document  zdn 2 f1;(cid:1)(cid:1)(cid:1) ; Kg
mth annotation in the dth document  tdm 2 f1;(cid:1)(cid:1)(cid:1) ; Tg
topic of the mth annotation in the dth document  cdm 2 f1;(cid:1)(cid:1)(cid:1) ; Kg
relevance to the content of the mth annotation of the dth document 
rdm = 1 if relevant  rdm = 0 otherwise

and to the best of our knowledge  no attempt has been made to extract content-related annotations
automatically. The extraction of content-related annotations can improve performance of machine
learning and information retrieval tasks. The proposed model can also be used for the automatic
generation of content-related annotations.
The proposed model is a generative model for content and annotations. It ﬁrst generates content  and
then generates the annotations. We assume that each annotation is associated with a latent variable
that indicates whether it is related to the content or not  and the annotation originates either from
the topics that generated the content or from a content-unrelated general distribution depending on
the latent variable. The inference can be achieved based on collapsed Gibbs sampling. Intuitively
speaking  this approach considers an annotation to be content-related when it is almost always at-
tached to objects in a speciﬁc topic. As regards real social annotation data  the annotations are not
explicitly labeled as content related/unrelated. The proposed model is an unsupervised model  and
so can extract content-related annotations without content relevance labels.
The proposed method is based on topic models. A topic model is a hierarchical probabilistic model 
in which a document is modeled as a mixture of topics  and where a topic is modeled as a proba-
bility distribution over words. Topic models are successfully used for a wide variety of applications
including information retrieval [3  13]  collaborative ﬁltering [14]  and visualization [15] as well as
for modeling annotated data [2].
The proposed method is an extension of the correspondence latent Dirichlet allocation (Corr-
LDA) [2]  which is a generative topic model for contents and annotations. Since Corr-LDA assumes
that all annotations are related to the content  it cannot be used for separating content-related an-
notations from content-unrelated ones. A topic model with a background distribution [4] assumes
that words are generated either from a topic-speciﬁc distribution or from a corpus-wide background
distribution. Although this is a generative model for documents without annotations  the proposed
model is related to the model in the sense that data may be generated from a topic-unrelated distri-
bution depending on a latent variable.
In the rest of this paper  we assume that the given data are annotated document data  in which the
content of each document is represented by words appearing in the document  and each document
has both content-related and content-unrelated annotations. The proposed model is applicable to a
wide range of discrete data with annotations. These include annotated image data  where each image
is represented with visual words [6]  and annotated movie data  where each movie is represented by
user ratings.

2 Proposed method

Suppose that  we have a set of D documents  and each document consists of a pair of words and
annotations (wd; td)  where wd = fwdngNd
n=1 is the set of words in a document that represents the
content  and td = ftdmgMd
m=1 is the set of assigned annotations  or tags. Our notation is summarized
in Table 1.

2

Figure 1: Graphical model representation of the proposed topic model with content relevance.

The proposed topic model ﬁrst generates the content  and then generates the annotations. The gen-
erative process for the content is the same as basic topic models  such as latent Dirichlet allocation
(LDA) [3]. Each document has topic proportions (cid:18)d that are sampled from a Dirichlet distribution.
For each of the Nd words in the document  a topic zdn is chosen from the topic proportions  and then
word wdn is generated from a topic-speciﬁc multinomial distribution (cid:30)zdn. In the generative pro-
cess for annotations  each annotation is assessed as to whether it is related to the content or not. In
particular  each annotation is associated with a latent variable rdm with value rdm = 0 if annotation
tdm is not related to the content; rdm = 1 otherwise. If the annotation is not related to the content 
rdm = 0  annotation tdm is sampled from general topic-unrelated multinomial distribution  0. If
the annotation is related to the content  rdm = 1  annotation tdm is sampled from topic-speciﬁc
multinomial distribution  cdm  where cdm is the topic for the annotation. Topic cdm is sampled
uniform randomly from topics zd = fzdngNd
n=1 that have previously generated the content. This
means that topic cdm is generated from a multinomial distribution  in which P (cdm = k) = Nkd
 
Nd
where Nkd is the number of words that are assigned to topic k in the dth document.
In summary  the proposed model assumes the following generative process for a set of annotated
documents f(wd; td)gD

d=1 

1. Draw relevance probability (cid:21) (cid:24) Beta((cid:17))
2. Draw content-unrelated annotation probability  0 (cid:24) Dirichlet((cid:13))
3. For each topic k = 1;(cid:1)(cid:1)(cid:1) ; K:

(a) Draw word probability (cid:30)k (cid:24) Dirichlet((cid:12))
(b) Draw annotation probability  k (cid:24) Dirichlet((cid:13))
4. For each document d = 1;(cid:1)(cid:1)(cid:1) ; D:
(a) Draw topic proportions (cid:18)d (cid:24) Dirichlet((cid:11))
(b) For each word n = 1;(cid:1)(cid:1)(cid:1) ; Nd:

(c) For each annotation m = 1;(cid:1)(cid:1)(cid:1) ; Md:

i. Draw topic zdn (cid:24) Multinomial((cid:18)d)
ii. Draw word wdn (cid:24) Multinomial((cid:30)zdn)
i. Draw topic cdm (cid:24) Multinomial(f Nkd
gK
k=1)
ii. Draw relevance rdm (cid:24) Bernoulli((cid:21))
iii. Draw annotation tdm (cid:24)

{

Nd

Multinomial( 0)
Multinomial( cdm)

if rdm = 0
otherwise

where (cid:11)  (cid:12) and (cid:13) are Dirichlet distribution parameters  and (cid:17) is a beta distribution parameter. Fig-
ure 1 shows a graphical model representation of the proposed model  where shaded and unshaded
nodes indicate observed and latent variables  respectively.
As with Corr-LDA  the proposed model ﬁrst generates the content and then generates the annotations
by modeling the conditional distribution of latent topics for annotations given the topics for the
content. Therefore  it achieves a comprehensive ﬁt of the joint distribution of content and annotations
and ﬁnds superior conditional distributions of annotations given content [2].
The joint distribution on words  annotations  topics for words  topics for annotations  and relevance
given parameters is described as follows:

P (W ; T ; Z; C; Rj(cid:11); (cid:12); (cid:13); (cid:17)) = P (Zj(cid:11))P (WjZ; (cid:12))P (TjC; R; (cid:13))P (Rj(cid:17))P (CjZ);

(1)

3

αθzNcMDλrηtwφψK+1Kβγd

D
d=1

(cid:0)((cid:11)K)
(cid:0)((cid:11))K

(cid:0)((cid:12)W )
(cid:0)((cid:12))W

d=1  Z = fzdgD

d=1  C = fcdgD

d=1  cd = fcdmgMd

m=1. We can integrate out multinomial distribution parameters  f(cid:18)dgD

d=1  T = ftdgD
where W = fwdgD
m=1  R =
d=1  and rd = frdmgMd
frdgD
d=1 
∫
∏
k=1 and f k0gK
f(cid:30)kgK
k0=0  because we use Dirichlet distributions for their priors  which are conjugate
to multinomial distributions. The ﬁrst term on the right hand side of (1) is calculated by P (Zj(cid:11)) =
)D∏
(
P (zdj(cid:18)d)P ((cid:18)dj(cid:11))d(cid:18)d  and we have the following equation by integrating out f(cid:18)dgD
Qk (cid:0)(Nkd+(cid:11))
d=1 
(
P (Zj(cid:11)) =
(cid:0)(Nd+(cid:11)K)   where (cid:0)((cid:1)) is the gamma function. Similarly  the second
term is given as follows  P (WjZ; (cid:12)) =
)K+1∏
(
  where Nkw is the number
of times word w has been assigned to topic k  and Nk =
w Nkw. The third term is given as
follows  P (TjC; R; (cid:13)) =
0 = 0
indicates irrelevant to the content. Mk0t is the number of times annotation t has been identiﬁed
t Mk0t.
as content-unrelated if k
The Bernoulli parameter (cid:21) can also be integrated out because we use a beta distribution for the
prior  which is conjugate to a Bernoulli distribution. The fourth term is given as follows  P (Rj(cid:17)) =
  where M is the number of annotations  and M0 is the number of content-

)K∏
k0 Qt (cid:0)(Mk0t+(cid:13))
0 = 0  or as content-related topic k

(cid:0)(Mk0 +(cid:13)T )   where k
0 if k
0

∑
Qw (cid:0)(Nkw+(cid:12))

∑
0 2 f0;(cid:1)(cid:1)(cid:1) ; Kg  and k
6= 0  and Mk0 =
∏

)M

∏

(

unrelated annotations. The ﬁfth term is given as follows  P (CjZ) =
0
kd is the number of annotations that are assigned to topic k in the dth document.
M
The inference of the latent topics Z given content W and annotations T can be efﬁciently computed
using collapsed Gibbs sampling [11]. Given the current state of all but one variable  zj  where
j = (d; n)  the assignment of a latent topic to the nth word in the dth document is sampled from 

Nkd
Nd

(cid:0)(M0+(cid:17))(cid:0)(M(cid:0)M0+(cid:17))

0
kd  where

k

(cid:0)(Nk+(cid:12)W )

(cid:0)((cid:13)T )
(cid:0)((cid:13))T

(cid:0)(2(cid:17))
(cid:0)((cid:17))2

(cid:0)(M +2(cid:17))

d

k

(

)M

0
kd

;

P (zj = kjW ; T ; Znj; C; R) / Nkdnj + (cid:11)
Ndnj + (cid:11)K

Nkwjnj + (cid:12)
Nknj + (cid:12)W

Nkdnj + 1

Nkdnj

Nd (cid:0) 1
Nd

where nj represents the count when excluding the nth word in the dth document. Given the current
state of all but one variable  ri  where i = (d; m)  the assignment of either relevant or irrelevant to
the mth annotation in the dth document is estimated as follows 
P (ri = 0jW ; T ; Z; C; Rni) / M0ni + (cid:17)
Mni + 2(cid:17)
P (ri = 1jW ; T ; Z; C; Rni) / Mni (cid:0) M0ni + (cid:17)

M0tini + (cid:13)
M0ni + (cid:13)T

(2)

:

;

Mni + 2(cid:17)

Mcitini + (cid:13)
Mcini + (cid:13)T

The assignment of a topic to a content-unrelated annotation is estimated as follows 

P (ci = kjri = 0; W ; T ; Z; Cni; Rni) / Nkd
Nd

;

and the assignment of a topic to a content-related annotation is estimated as follows 

P (ci = kjri = 1; W ; T ; Z; Cni; Rni) / Mktini + (cid:13)
Mkni + (cid:13)T

Nkd
Nd

:

(3)

(4)

The parameters (cid:11)  (cid:12)  (cid:13)  and (cid:17) can be estimated by maximizing the joint distribution (1) by the
ﬁxed-point iteration method described in [21].

3 Experiments

3.1 Synthetic content-unrelated annotations

We evaluated the proposed method quantitatively by using labeled text data from the 20 Newsgroups
corpus [18] and adding synthetic content-unrelated annotations. The corpus contains about 20 000
articles categorized into 20 discussion groups. We considered these 20 categories as content-related
annotations  and we also randomly attached dummy categories to training samples as content-
unrelated annotations. We created two types of training data  20News1 and 20News2  where the

4

^ kt  where ^(cid:18)dk = Nkd

Nd

k

document given the training samples as follows  P (tjd;D) (cid:25) ∑

former was used for evaluating the proposed method when analyzing data with different numbers
of content-unrelated annotations per document  and the latter was used with different numbers of
unique content-unrelated annotations. Speciﬁcally  in the 20News1 data  the unique number of
content-unrelated annotations was set at ten  and the number of content-unrelated annotations per
document was set at f1;(cid:1)(cid:1)(cid:1) ; 10g. In the 20News2 data  the unique number of content-unrelated
annotations was set at f1;(cid:1)(cid:1)(cid:1) ; 10g  and the number of content-unrelated annotations per document
was set at one. We omitted stop-words and words that occurred only once. The vocabulary size was
52 647. We sampled 100 documents from each of the 20 categories  for a total of 2 000 documents.
We used 10 % of the samples as test data.
We compared the proposed method with MaxEnt and Corr-LDA. MaxEnt represents a maximum
entropy model [22] that estimates the probability distribution that maximizes entropy under the
constraints imposed by the given data. MaxEnt is a discriminative classiﬁer and achieves high per-
formance as regards text classiﬁcation. In MaxEnt  the hyper-parameter that maximizes the perfor-
mance was chosen from f10(cid:0)3; 10(cid:0)2; 10(cid:0)1; 1g  and the input word count vector was normalized so
that the sum of the elements was one. Corr-LDA [2] is a topic model for words and annotations that
does not take the relevance to content into consideration. For the proposed method and Corr-LDA 
we set the number of latent topics  K  to 20  and estimated latent topics and parameters by using
collapsed Gibbs sampling and the ﬁxed-point iteration method  respectively.
We evaluated the predictive performance of each method using the perplexity of held-out content-
related annotations given the content. A lower perplexity represents higher predictive performance.
In the proposed method  we calculated the probability of content-related annotation t in the dth
^(cid:18)dk
is a
point estimate of the topic proportions for annotations  and ^ kt = Mkt+(cid:13)
Mk+(cid:13)T is a point estimate of the
annotation multinomial distribution. Note that no content-unrelated annotations were attached to the
test samples. The average perplexities and standard deviations over ten experiments on the 20News1
and 20News2 data are shown in Figure 2 (a). In all cases  when content-unrelated annotations were
included  the proposed method achieved the lowest perplexity  indicating that it can appropriately
predict content-related annotations. Although the perplexity achieved by MaxEnt was slightly lower
than that of the proposed method without content-unrelated annotations  the performance of MaxEnt
deteriorated greatly when even one content-unrelated annotation was attached. Since MaxEnt is a
supervised classiﬁer  it considers all attached annotations to be content-related even if they are not.
Therefore  its perplexity is signiﬁcantly high when there are fewer content-related annotations per
document than unrelated annotations as with the 20News1 data. In contrast  since the proposed
method considers the relevance to the content for each annotation  it always offered low perplexity
even if the number of content-unrelated annotations was increased. The perplexity achieved by
Corr-LDA was high because it does not consider the relevance to the content as in MaxEnt.
We evaluated the performance in terms of extracting content-related annotations. We considered ex-
traction as a binary classiﬁcation problem  in which each annotation is classiﬁed as either content-
related or content-unrelated. As the evaluation measurement  we used F-measure  which is the
harmonic mean of precision and recall. We compared the proposed method to a baseline method
in which the annotations are considered to be content-related if any of the words in the annotations
appear in the document. In particular  when the category name is ’comp.graphics’  if ’computer’ or
’graphics’ appears in the document  it is considered to be content-related. We assume that the base-
line method knows that content-unrelated annotations do not appear in any document. Therefore 
the precision of the baseline method is always one  because the number of false positive samples is
zero. Note that this baseline method does not support image data  because words in the annotations
never appear in the content. F-measures for the 20News1 and 20News2 data are shown in Fig-
ure 2 (b). A higher F-measure represents higher classiﬁcation performance. The proposed method
achieved high F-measures with a wide range of ratios of content-unrelated annotations. All of the
F-measures achieved by the proposed method exceeded 0.89  and the F-measure without unrelated
annotations was one. This result implies that it can ﬂexibly handle cases with different ratios of
content-unrelated annotations. The F-measures achieved by the baseline method were low because
annotations might be related to the content even if the annotations did not appear in the document.
On the other hand  the proposed method considers that annotations are related to the content when
the topic  or latent semantics  of the content and the topic of the annotations are similar even if the
annotations did not appear in the document.

5

20News1

20News2

(a) Perplexity

(b) F-measure

(c) ^(cid:21)

Figure 2: (a) Perplexities of the held-out content-related annotations  (b) F-measures of content
relevance  and (c) Estimated content-related annotation ratios in 20News data.

Figure 2 (c) shows the content-related annotation ratios as estimated by the following equation 
^(cid:21) = M(cid:0)M0+(cid:17)
  with the proposed method. The estimated ratios are about the same as the true
ratios.

M +2(cid:17)

3.2 Social annotations

We analyzed the following three sets of real social annotation data taken from two social bookmark-
ing services and a photo sharing service  namely Hatena  Delicious  and Flickr.
From the Hatena data  we used web pages and their annotations in Hatena::Bookmark [12]  which
is a social bookmarking service in Japan  that were collected using a similar method to that used
in [25  27]. Speciﬁcally  ﬁrst  we obtained a list of URLs of popular bookmarks for October 2008.
We then obtained a list of users who had bookmarked the URLs in the list. Next  we obtained a new
list of URLs that had been bookmarked by the users. By iterating the above process  we collected
a set of web pages and their annotations. We omitted stop-words and words and annotations that
occurred in fewer than ten documents. We omitted documents with fewer than ten unique words
and also omitted those without annotations. The numbers of documents  unique words  and unique
annotations were 39 132  8 885  and 43 667  respectively. From the Delicious data  we used web
pages and their annotations [7] that were collected using the same method used for the Hatena data.
The numbers of documents  unique words  and unique annotations were 65 528  30 274  and 21 454 
respectively. From the Flickr data  we used photographs and their annotations Flickr [9] that were
collected in November 2008 using the same method used for the Hatena data. We transformed photo
images into visual words by using scale-invariant feature transformation (SIFT) [20] and k-means as
described in [6]. We omitted annotations that were attached to fewer than ten images. The numbers
of images  unique visual words  and unique annotations were 12 711  200  and 2 197  respectively.
For the experiments  we used 5 000 documents that were randomly sampled from each data set.
Figure 3 (a)(b)(c) shows the average perplexities over ten experiments and their standard deviation
for held-out annotations in the three real social annotation data sets with different numbers of topics.
Figure 3 (d) shows the result with the Patent data as an example of data without content unrelated
annotations. The Patent data consist of patents published in Japan from January to March in 2004 
to which International Patent Classiﬁcation (IPC) codes were attached by experts according to their
content. The numbers of documents  unique words  and unique annotations (IPC codes) were 9 557 

6

 5 10 15 20 25 30 35 40 45 50 0 2 4 6 8 10perplexitynumber of content-unrelated annotations per documentProposedCorr-LDAMaxEnt 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10F-measurenumber of content-unrelated annotations per documentProposedBaseline 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10lambdanumber of content-unrelated annotations per documentEstimatedTrue 6 8 10 12 14 16 18 0 2 4 6 8 10perplexitynumber of unique content-unrelated annotationsProposedCorr-LDAMaxEnt 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10F-measurenumber of unique content-unrelated annotationsProposedBaseline 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10lambdanumber of unique content-unrelated annotationsEstimatedTrue(a) Hatena

(b) Delicious

(c) Flickr

(d) Patent

Figure 3: Perplexities of held-out annotations with different numbers of topics in social annotation
data (a)(b)(c)  and in data without content unrelated annotations (d).

Figure 4: Examples of content-related annotations in the Delicious data extracted by the proposed
method. Each row shows annotations attached to a document; content-unrelated annotations are
shaded.

104 621  and 6 117  respectively. With the Patent data  the perplexities of the proposed method
and Corr-LDA were almost the same. On the other hand  with the real social annotation data  the
proposed method achieved much lower perplexities than Corr-LDA. This result implies that it is
important to consider relevance to the content when analyzing noisy social annotation data. The
perplexity of Corr-LDA with social annotation data gets worse as the number of topics increases
because Corr-LDA overﬁts noisy content-unrelated annotations.
The upper half of each table in Table 2 shows probable content-unrelated annotations in the leftmost
column  and probable annotations for some topics  which were estimated with the proposed method
using 50 topics. The lower half in (a) and (b) shows probable words in the content for each topic.
With the Hatena data  we translated Japanese words into English  and we omitted words that had
the same translated meaning in a topic. For content-unrelated annotations  words that seemed to
be irrelevant to the content were extracted  such as ’toread’  ’later’  ’*’  ’?’  ’imported’  ’2008’ 
’nikon’  and ’cannon’. Each topic has characteristic annotations and words  for example  Topic1
in the Hatena data is about programming  Topic2 is about games  and Topic3 is about economics.
Figure 4 shows some examples of the extraction of content-related annotations.

7

 1000 1500 2000 2500 3000 3500 4000 0 20 40 60 80 100perplexitynumber of topicsProposedCorrLDA 2000 3000 4000 5000 6000 7000 8000 9000 0 20 40 60 80 100perplexitynumber of topicsProposedCorrLDA 800 1000 1200 1400 1600 1800 2000 2200 0 20 40 60 80 100perplexitynumber of topicsProposedCorrLDA 600 800 1000 1200 1400 1600 1800 2000 0 20 40 60 80 100perplexitynumber of topicsProposedCorrLDAcanada   banking   toreadLondon   river   london   history   reference   imported   Englandblog   ruby   rails   cell   person   misc   Ruby   plugin   cpu   ajax   javascript   exif   phpfuture   distribution   internet   prediction   Internet   computer   computers   no_tag   bandwidthfilm   Art   good   mindfuck   movies   list   blogricette   cucina   cooking   italy   search   recipes   italian   food   cook   news   reference   searchengine   list   italiano   linksruby   git   diff   useful   triage   imported   BookmarksBar   blogSSD   toread   ssdc#   interview   programming   C#   .net   todo   language   tips   microsoftgoogle   gmail   googlecalendar   Web-2.0   Gmail   via:mento.infoTable 2: The ten most probable content-unrelated annotations (leftmost column)  and the ten most
probable annotations for some topics (other columns)  estimated with the proposed method using 50
topics. Each column represents one topic. The lower half in (a) and (b) shows probable words in the
content.

(a) Hatena

Topic2
programming game
development
dev
webdev

unrelated Topic1
toread
web
later
great
document php
java
troll
software
*
ruby
?
opensource
summary
softwaredev
memo
development
web
series
hp
technology
management
source
usage
project
system

Topic6
linux
tips

Topic3
economics

Topic5
food
cooking
gourmet windows
recipe
security
server
cook
life
network

Topic4
science
animation ﬁnance
research
society
movie
biology
Nintendo
business
study
economy
movie
psychology
reading
event
mathematics
investment
xbox360
pseudoscience fooditem unix
japan
DS
knowledge
money
PS3
education
company
animation
math
game
year
science
article
animation
researcher
ﬁnance
movie
answer
economics
story
spirit
investment
work
question
company
create
human
PG
day
ehara
mr
management proof
interesting information mind
world
brain

foods
alcohol mail
foodie
eat
use
omission ﬁle
water
decision
broil
face
input
miss
food

Apache
in
setting

nikkei

mysql

Topic8
pc

Topic9
medical
health
lie
government
agriculture
food
mentalhealth

Topic7
politics
international apple
iphone
oversea
society
hardware
gadget
history
mac
china
cupidity
world
technology mental
international
ipod
usa
electronics
news
japan
yen
product
country
digital
usa
pc
china
support
politics
in
aso
mr
note
price
equipment
model

environment
science
rice
banana
medical
diet
hospital
poison
eat
incident
korea
jelly

server
case
mail
address
connection korea
human
access
security
people

reference money
web
ﬁnance
imported economics
design
internet
online
cool
toread
tools
blog

video
music
videos
fun
entertainment
funny
movies
media
Video
ﬁlm
music
video
link
tv
movie
itunes
ﬁlm
amazon

business
economy
Finance
ﬁnancial
investing
bailout
ﬁnances
money
ﬁnancial
credit
market
economic
october
economy
banks
government play
bank

interview

home

inspiration

art
photo

iphone
mobile

shopping
shop

education
learning

Photography wishlist

security
computer Art
microsoft
network music

(b) Delicious
food
opensource
software
recipes
programming recipe
development
linux
tools
rails
ruby
webdev
rubyonrails
project
code
server
ruby
rails
source
ﬁle
version
ﬁles
development

windows
linux
sysadmin photography Shopping hardware books
book
language
library
school
teaching
Education
research
book
legal
theory
books
law
university
students
learning
education
language

cooking Windows photos
Food
Recipes
baking
health
vegetarian Linux
ubuntu
diy
windows
recipe
system
food
recipes
microsoft photos
camera
linux
make
vol
software
wine
digital
ﬁle
made
images
server
add
2008
user
love
eat
ﬁles
photo
tracks
ubuntu
good

foto
fotograﬁa
art
photography online
price
cheap
product
order
free
products
rating
card

games
iPhone
apple
tech
gaming
mac
game
iphone
apple
ipod
mobile
game
games
pc
phone
mac
touch

buy
store
fashion
gifts
house
buy

2008
nikon
canon
white
yellow
red
photo
italy
california
color

dance
bar
dc
digital
concert
bands
music
washingtondc
dancing
work

sea
sunset
sky
clouds
mountains
ocean
panorama
south
ireland
oregon

(c) Flickr
rock
house
party
park
inn
coach
creature
halloween
mallory
night

autumn
trees
tree
mountain
fall
garden
bortescristian
geotagged
mud
natura

beach
travel
vacation
camping
landscape
texas
lake
cameraphone
md
sun

family
portrait
cute
baby
boy
kids
brown
closeup
08
galveston

island
asia
landscape
rock
blue
tour
plant
tourguidesoma
koh
samui

4 Conclusion

We have proposed a topic model for extracting content-related annotations from noisy annotated
data. We have conﬁrmed experimentally that the proposed method can extract content-related anno-
tations appropriately  and can be used for analyzing social annotation data. In future work  we will
determine the number of topics automatically by extending the proposed model to a nonparamet-
ric Bayesian model such as the Dirichlet process mixture model [24]. Since the proposed method
is  theoretically  applicable to various kinds of annotation data  we will conﬁrm this in additional
experiments.

8

References
[1] K. Barnard  P. Duygulu  D. Forsyth  N. de Freitas  D. M. Blei  and M. I. Jordan. Matching words and

pictures. Journal of Machine Learning Research  3:1107–1135  2003.

[2] D. M. Blei and M. I. Jordan. Modeling annotated data. In SIGIR ’03: Proceedings of the 26th Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval  pages
127–134  2003.

[3] D. M. Blei  A. Y. Ng  and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning

Research  3:993–1022  2003.

[4] C. Chemudugunta  P. Smyth  and M. Steyvers. Modeling general and speciﬁc aspects of documents
with a probabilistic topic model. In B. Sch¨olkopf  J. Platt  and T. Hoffman  editors  Advances in Neural
Information Processing Systems 19  pages 241–248. MIT Press  2007.

[5] CiteULike. http://www.citeulike.org.
[6] G. Csurka  C. Dance  J. Willamowski  L. Fan  and C. Bray. Visual categorization with bags of keypoints.

In ECCV International Workshop on Statistical Learning in Computer Vision  2004.

[7] Delicious. http://delicious.com.
[8] S. Feng  R. Manmatha  and V. Lavrenko. Multiple Bernoulli relevance models for image and video
annotation. In CVPR ’04: Proceedings of the IEEE Computer Society Conference on Computer Vision
and Pattern Recognition  volume 2  pages 1002–1009  2004.

[9] Flickr. http://ﬂickr.com.
[10] S. Golder and B. A. Huberman. Usage patterns of collaborative tagging systems. Journal of Information

Science  32(2):198–208  2006.

[11] T. L. Grifﬁths and M. Steyvers. Finding scientiﬁc topics. Proceedings of the National Academy of

Sciences  101 Suppl 1:5228–5235  2004.
[12] Hatena::Bookmark. http://b.hatena.ne.jp.
[13] T. Hofmann. Probabilistic latent semantic analysis.

Uncertainty in Artiﬁcial Intelligence  pages 289–296  1999.

In UAI ’99: Proceedings of 15th Conference on

[14] T. Hofmann. Collaborative ﬁltering via Gaussian probabilistic latent semantic analysis. In Proceedings
of the 26th Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval  pages 259–266. ACM Press  2003.

[15] T. Iwata  T. Yamada  and N. Ueda. Probabilistic latent semantic visualization: topic model for visualizing
documents. In KDD ’08: Proceeding of the 14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining  pages 363–371. ACM  2008.

[16] J. Jeon  V. Lavrenko  and R. Manmatha. Automatic image annotation and retrieval using cross-media
relevance models. In SIGIR ’03: Proceedings of the 26th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval  pages 119–126. ACM  2003.

[17] J. Jeon and R. Manmatha. Using maximum entropy for automatic image annotation.

In CIVR ’04:

Proceedings of the 3rd International Conference on Image and Video Retrieval  pages 24–32  2004.

[18] K. Lang. NewsWeeder: learning to ﬁlter netnews. In ICML ’95: Proceedings of the 12th International

Conference on Machine Learning  pages 331–339  1995.

[19] Last.fm. http://www.last.fm.
[20] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer

Vision  60(2):91–110  2004.

[21] T. Minka. Estimating a Dirichlet distribution. Technical report  M.I.T.  2000.
[22] K. Nigam  J. Lafferty  and A. McCallum. Using maximum entropy for text classiﬁcation. In Proceedings

of IJCAI-99 Workshop on Machine Learning for Information Filtering  pages 61–67  1999.

[23] Technorati. http://technorati.com.
[24] Y. W. Teh  M. I. Jordan  M. J. Beal  and D. M. Blei. Hierarchical Dirichlet processes. Journal of the

American Statistical Association  101(476):1566–1581  2006.

[25] X. Wu  L. Zhang  and Y. Yu. Exploring social annotations for the semantic web. In WWW ’06: Proceed-

ings of the 15th International Conference on World Wide Web  pages 417–426. ACM  2006.

[26] YouTube. http://www.youtube.com.
[27] D. Zhou  J. Bian  S. Zheng  H. Zha  and C. L. Giles. Exploring social annotations for information retrieval.
In WWW ’08: Proceeding of the 17th International Conference on World Wide Web  pages 715–724.
ACM  2008.

9

,Junqi Tang
Mohammad Golbabaee
Francis Bach
Mike davies
Qiangeng Xu
Weiyue Wang
Duygu Ceylan
Radomir Mech
Ulrich Neumann