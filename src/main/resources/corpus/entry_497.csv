2019,An Adaptive Empirical  Bayesian Method for Sparse Deep Learning,We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep learning  where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). The convergence of the proposed method to the asymptotically correct distribution is established under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks (CNN) and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.,An Adaptive Empirical Bayesian Method for Sparse

Deep Learning

Wei Deng

Xiao Zhang

Department of Mathematics

Department of Computer Science

Purdue University

West Lafayette  IN 47907
deng106@purdue.edu

Purdue University

West Lafayette  IN 47907
zhang923@purdue.edu

Faming Liang

Department of Statistics

Purdue University

West Lafayette  IN 47907
fmliang@purdue.edu

Guang Lin

Departments of Mathematics  Statistics
and School of Mechanical Engineering

Purdue University

West Lafayette  IN 47907
guanglin@purdue.edu

Abstract

We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep
learning  where the sparsity is ensured via a class of self-adaptive spike-and-slab
priors. The proposed method works by alternatively sampling from an adaptive
hierarchical posterior distribution using stochastic gradient Markov Chain Monte
Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic
approximation (SA). We further prove the convergence of the proposed method to
the asymptotically correct distribution under mild conditions. Empirical applica-
tions of the proposed method lead to the state-of-the-art performance on MNIST
and Fashion MNIST with shallow convolutional neural networks (CNN) and the
state-of-the-art compression performance on CIFAR10 with Residual Networks.
The proposed method also improves resistance to adversarial attacks.

1

Introduction

MCMC  known for its asymptotic properties  has not been fully investigated in deep neural networks
(DNNs) due to its unscalability in dealing with big data. Stochastic gradient Langevin dynamics
(SGLD) [Welling and Teh  2011]  the ﬁrst stochastic gradient MCMC (SG-MCMC) algorithm  tackled
this issue by adding noise to the stochastic gradient  smoothing the transition between optimization
and sampling and making MCMC scalable. Chen et al. [2014] proposed using stochastic gradient
Hamiltonian Monte Carlo (SGHMC)  the second-order SG-MCMC  which was shown to converge
faster. In addition to modeling uncertainty  SG-MCMC also has remarkable non-convex optimization
abilities. Raginsky et al. [2017]  Xu et al. [2018] proved that SGLD  the ﬁrst-order SG-MCMC 
is guaranteed to converge to an approximate global minimum of the empirical risk in ﬁnite time.
Zhang et al. [2017] showed that SGLD hits the approximate local minimum of the population risk
in polynomial time. Mangoubi and Vishnoi [2018] further demonstrated SGLD with simulated
annealing has a higher chance to obtain the global minima on a wider class of non-convex functions.
However  all the analyses fail when DNN has too many parameters  and the over-speciﬁed model
tends to have a large prediction variance  resulting in poor generalization and causing over-ﬁtting.
Therefore  a proper model selection is on demand at this situation.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

A standard method to deal with model selection is variable selection. Notably  the best variable
selection based on the L0 penalty is conceptually ideal for sparsity detection but is computationally
slow. Two alternatives emerged to approximate it. On the one hand  penalized likelihood approaches 
such as Lasso [Tibshirani  1994]  induce sparsity due to the geometry that underlies the L1 penalty. To
better handle highly correlated variables  Elastic Net was proposed [Zou and Hastie  2005] and makes
a compromise between L1 penalty and L2 penalty. On the other hand  spike-and-slab approaches
to Bayesian variable selection originates from probabilistic considerations. George and McCulloch
[1993] proposed to build a continuous approximation of the spike-and-slab prior to sample from a
hierarchical Bayesian model using Gibbs sampling. This continuous relaxation inspired the efﬁcient
EM variable selection (EMVS) algorithm in linear models [Roˇrková and George  2014  2018].
Despite the advances of model selection in linear systems  model selection in DNNs has received
less attention. Ghosh et al. [2018] proposed to use variational inference (VI) based on regularized
horseshoe priors to obtain a compact model. Liang et al. [2018] presented the theory of posterior
consistency for Bayesian neural networks (BNNs) with Gaussian priors  and Ye and Sun [2018]
applied a greedy elimination algorithm to conduct group model selection with the group Lasso penalty.
Although these works only show the performance of shallow BNNs  the experimental methodologies
imply the potential of model selection in DNNs. Louizos et al. [2017] studied scale mixtures of
Gaussian priors and half-Cauchy scale priors for the hidden units of VGG models [Simonyan and
Zisserman  2014] and achieved good model compression performance on CIFAR10 [Krizhevsky 
2009] using VI. However  due to the limitation of VI in non-convex optimization  the compression is
still not sparse enough and can be further optimized.
Over-parameterized DNNs often demand for tremendous memory use and heavy computational
resources  which is impractical for smart devices. More critically  over-parametrization frequently
overﬁts the data and results in worse performance [Lin et al.  2017]. To ensure the efﬁciency of the
sparse sampling algorithm without over-shrinkage in DNN models  we propose an AEB method to
adaptively sample from a hierarchical Bayesian DNN model with spike-and-slab Gaussian-Laplace
(SSGL) priors and the priors are learned through optimization instead of sampling. The AEB method
differs from the full Bayesian method in that the priors are inferred from the empirical data and the
uncertainty of the priors is no longer considered to speed up the inference. In order to optimize the
latent variables without affecting the convergence to the asymptotically correct distribution  stochastic
approximation (SA) [Benveniste et al.  1990]  a standard method for adaptive sampling [Andrieu
et al.  2005  Liang  2010]  naturally ﬁts to train the adaptive hierarchical Bayesian model. As a
result  the asymptotic property allows us to combine simulated annealing and/or parallel tempering to
accelerate the non-convex learning.
In this paper  we propose a sparse Bayesian deep learning algorithm  SG-MCMC-SA  to adaptively
learn the hierarchical Bayes mixture models in DNNs. This algorithm has four main contributions:
• We propose a novel AEB method to efﬁciently train hierarchical Bayesian mixture DNN
models  where the parameters are learned through sampling while the priors are learned
through optimization.
• We prove the convergence of this approach to the asymptotically correct distribution  and
it can be further generalized to a class of adaptive sampling algorithms for estimating
state-space models in deep learning.
• We apply this adaptive sampling algorithm in the DNN compression problems ﬁrstly  with
• It achieves the state of the art in terms of compression rates  which is 91.68% accuracy on

potential extension to a variety of model compression problems.

CIFAR10 using only 27K parameters (90% sparsity) with Resnet20 [He et al.  2016].

2 Stochastic Gradient MCMC

We denote the set of model parameters by β  the learning rate at time k by (k)  the entire data by
D = {di}N
i=1  where di = (xi  yi)  the log of posterior by L(β). The mini-batch of data B is of size
n with indices S = {s1  s2  ...  sn}  where si ∈ {1  2  ...  N}. Stochastic gradient ∇β ˜L(β) from a
mini-batch of data B randomly sampled from D is used to approximate ∇βL(β):

∇β log P(di|β).

(1)

(cid:88)

i∈S

∇β ˜L(β) = ∇β log P(β) +

N
n

2

 dβ = rdt 

SGLD (no momentum) is formulated as follows:

β(k+1) = β(k) + (k)∇β ˜L(β(k)) + N (0  2(k)τ−1) 

(2)
where τ > 0 denotes the inverse temperature. It has been shown that SGLD asymptotically converges
to a stationary distribution π(β|D) ∝ eτ L(β) [Teh et al.  2016  Zhang et al.  2017]. As τ increases
and  decreases gradually  the solution tends towards the global optima with a higher probability.
Another variant of SG-MCMC  SGHMC [Chen et al.  2014  Ma et al.  2015]  proposes to generate
samples as follows:

dr = ∇β ˜L(β)dt − Crdt + N (0  2Bτ−1dt) + N (0  2(C − ˆB)τ−1dt) 

(3)

where r is the momentum item  ˆB is an estimate of the stochastic gradient variance  C is a user-
speciﬁed friction term. Regarding the discretization of (3)  we follow the numerical method proposed
by Saatci and Wilson [2017] due to its convenience to import parameter settings from SGD.

3 Empirical Bayesian via Stochastic Approximation

3.1 A hierarchical formulation with deep SSGL priors

Inspired by the hierarchical Bayesian formulation for sparse inference [George and McCulloch  1993] 
we assume the weight βlj in sparse layer l with index j follows the SSGL prior

βlj|σ2  γlj ∼ (1 − γlj)L(0  σv0) + γljN (0  σ2v1).

(4)
where γlj ∈ {0  1}  βl ∈ Rpl  σ2 ∈ R  L(0  σv0) denotes a Laplace distribution with mean 0 and
scale σv0  and N (0  σ2v1) denotes a Normal distribution with mean 0 and variance σ2v1. The sparse
layer can be the fully connected layers (FC) in a shallow CNN or Convolutional layers in ResNet. If
we have γlj = 0  the prior behaves like Lasso  which leads to a shrinkage effect; when γlj = 1  the
L2 penalty dominates. The likelihood follows



(cid:26)

(cid:80)
i∈S (yi − ψ(xi; β))2

(cid:27)

exp

−

2σ2
(2πσ2)n/2
(cid:80)K
exp{ψyi(xi; β)}
t=1 exp{ψt(xi; β)}

(cid:81)

i∈S

π(B|β  σ2) =

(regression) 

(classiﬁcation) 

(5)

where ψ(xi; β) is a linear or non-linear mapping  and yi ∈ {1  2  . . .   K} is the response value of the
i-th example. In addition  the variance σ2 follows an inverse gamma prior π(σ2) = IG(ν/2  νλ/2).
The i.i.d. Bernoulli prior is used for γ  namely π(γl|δl) = δ
(1 − δl)pl−|γl| where δl ∈ R follows
(1 − δl)b−1. The use of self-adaptive penalty enables the model to
Beta distribution π(δl) ∝ δa−1
learn the level of sparsity automatically. Finally  our posterior follows

|γl|
l

l

π(β  σ2  δ  γ|B) ∝ π(B|β  σ2)

N

n π(β|σ2  γ)π(σ2|γ)π(γ|δ)π(δ).

(6)

3.2 Empirical Bayesian with approximate priors

To speed up the inference  we propose the AEB method by sampling β and optimizing σ2  δ  γ  where
uncertainty of the hyperparameters are not considered. Because the binary variable γ is harder to
Due to the limited memory  which restricts us from sampling directly from D  we choose to sample

optimize directly  we consider optimizing the adaptive posterior Eγ|· D(cid:2)π(β  σ2  δ  γ|D)(cid:3) ∗ instead.
β from Eγ|· D(cid:2)EB(cid:2)π(β  σ2  δ  γ|B)(cid:3)(cid:3) †. By Fubini’s theorem and Jensen’s inequality  we have

log Eγ|· D(cid:2)EB(cid:2)π(β  σ2  δ  γ|B)(cid:3)(cid:3) = log EB(cid:2)Eγ|· D(cid:2)π(β  σ2  δ  γ|B)(cid:3)(cid:3)
≥EB(cid:2)log Eγ|· D(cid:2)π(β  σ2  δ  γ|B)(cid:3)(cid:3) ≥ EB(cid:2)Eγ|· D(cid:2)log π(β  σ2  δ  γ|B)(cid:3)(cid:3) .

(7)

∗Eγ|· D[·] is short for E

†EB[π(β  σ2  δ  γ|B)] denotes(cid:82)

γ|β(k) σ(k) δ(k) D[·].

D π(β  σ2  δ  γ|B)dB

3

Instead of tackling π(β  σ2  δ  γ|D) directly  we propose to iteratively update the lower bound Q

Q(β  σ  δ|β(k)  σ(k)  δ(k)) = EB(cid:2)Eγ|D(cid:2)log π(β  σ2  δ  γ|B)(cid:3)(cid:3) .

(8)

Given (β(k)  σ(k)  δ(k)) at the k-th iteration  we ﬁrst sample β(k+1) from Q  then optimize Q with
respect to σ  δ and Eγl|· D via SA  where Eγl|· D is used since γ is treated as unobserved variable. To
make the computation easier  we decompose our Q as follows:

Q(β  σ  δ|β(k)  σ(k)  δ(k)) = Q1(β  σ|β(k)  σ(k)  δ(k)) + Q2(δ|β(k)  σ(k)  δ(k)) + C 

Denote X and C as the sets of the indices of sparse and non-sparse layers  respectively. We have:

(9)

Q1(β|β(k)  σ(k)  δ(k)) =

log π(B|β)

(cid:122)

Eγl|· D

(cid:88)
−(cid:88)
(cid:123)(cid:122)
(cid:124)
(cid:122)

j∈pl

log likelihood

N
n

(cid:124)
(cid:20)

(cid:125)(cid:124)

κlj0

(cid:123)(cid:122)

2

log(σ2)

β2
lj
2σ2
0

− p + ν + 2

(cid:125)
(cid:125)(cid:124)
(cid:20) 1

l∈C
non-sparse layers C
κlj1

(cid:125)
(cid:123)
(cid:21)
(cid:123)(cid:122)
(cid:125)(cid:124)
(cid:122)
Eγl|· Dγlj +(a − 1) log(δl) + (pl + b − 1) log(1 − δl) 
(11)

Eγl|· D
2σ2

(cid:123)
(cid:21)
(cid:125)

− νλ
2σ2

v1γlj

(10)

(cid:123)

ρlj

]

1

(cid:19)

β2
lj

v0(1 − γlj)
σ
deep SSGL priors in sparse layers X

+

(cid:18) δl

−(cid:88)

l∈X

(cid:88)

j∈pl

|βlj|

[

(cid:124)
(cid:88)

(cid:88)

Q2(δl|β(k)

l

  δ(k)

l

) =

log

1 − δl

l∈X

j∈pl

where ρ  κ  σ and δ are to be estimated in the next section.

3.3 Empirical Bayesian via stochastic approximation

To simplify the notation  we denote the vector (ρ  κ  σ  δ) by θ. Our interest is to obtain the optimal
θ∗ based on the asymptotically correct distribution π(β  θ∗). This implies that we need to obtain

an estimate θ∗ that solves a ﬁxed-point formulation(cid:82) gθ∗ (β)π(β  θ∗)dβ = θ∗ [Shimkin  2011] 

where gθ(β) is inspired by EMVS to obtain the optimal θ based on the current β. Deﬁne the random
output gθ(β) − θ as H(β  θ) and the mean ﬁeld function h(θ) := E[H(β  θ)]. The stochastic
approximation algorithm can be used to solve the ﬁxed-point iterations:

(1) Sample β(k+1) from a transition kernel Πθ(k)(β)  which yields the distribution π(β  θ(k)) 
(2) Update θ(k+1) = θ(k) + ω(k+1)H(θ(k)  β(k+1)) = θ(k) + ω(k+1)(h(θ(k)) + Ω(k)).

where ω(k+1) is the step size. The equilibrium point θ∗ is obtained when the distribution of β
converges to the invariant distribution π(β  θ∗). The stochastic approximation [Benveniste et al. 
1990] differs from the Robbins–Monro algorithm in that sampling β from a transition kernel instead
of a distribution introduces a Markov state-dependent noise Ω(k) [Andrieu et al.  2005]. In addition 
since variational technique is only used to approximate the priors  and the exact likelihood doesn’t
change  the algorithm falls into a class of adaptive SG-MCMC instead of variational inference.
Regarding the updates of gθ(β) with respect to ρ  we denote the optimal ρ based on the current β
and δ by ˜ρ. We have that ˜ρ(k+1)

  the probability of βlj being dominated by the L2 penalty is

= Eγl|· Bγlj = P(γlj = 1|β(k)

  δ(k)

where alj = π(β(k)
choice of Bernoulli prior enables us to use P(γlj = 1|δ(k)
Similarly  as to gθ(β) w.r.t. κ  the optimal ˜κlj0 and ˜κlj1 based on the current ρlj are given by:

lj |γlj = 1)P(γlj = 1|δ(k)
(cid:21)

) =
lj |γlj = 0)P(γlj = 0|δ(k)
) and blj = π(β(k)
) = δ(k)
.

alj + blj

(cid:20)

(cid:21)

 

l

l

l

l

l

l

alj

(cid:20) 1

1 − ρlj

˜κlj0 = Eγl|· B

1

v0(1 − γlj)

=

; ˜κlj1 = Eγl|· B

=

ρlj
v1

.

(13)

v1γlj

(12)

). The

lj
˜ρ(k+1)
lj

v0

4

To optimize Q1 with respect to σ  by denoting diag{κ0li}pl

i=1 as V 0l  diag{κ1li}pl

i=1 as V 1l we have:

where Ra = N +(cid:80)

||1 
||2.†
Rc = I +J +νλ  Cc = J +νλ  I = N
n
To optimize Q2  a closed-form update can be derived from Eq.(11) and Eq.(12) given batch data B:

l∈X ||V 0lβ(k+1)
1l β(k+1)

l∈X ||V 1/2

i∈S

l

l

(14)

b + 4RaRc

˜σ(k+1) =


Rb +(cid:112)R2
Cb +(cid:112)C 2
l∈X pl + ν  Ca = (cid:80)
(cid:80)

(regression) 

2Ra
b + 4CaCc
2Ca

(classiﬁcation) 

l∈X pl + ν + 2  Rb = Cb = (cid:80)
(cid:0)yi − ψ(xi; β(k+1))(cid:1)2  J =(cid:80)

(cid:80)pl
j=1 ρlj + a − 1
a + b + pl − 2

˜δ(k+1)
l

= arg max

δl∈R

Q2(δl|β(k)

l

  δ(k)

l

) =

3.4 Pruning strategy

.

(15)

There are quite a few methods for pruning neural networks including the oracle pruning and the
easy-to-use magnitude-based pruning [Molchanov et al.  2017]. Although the magnitude-based unit
pruning shows more computational savings [Gomez et al.  2018]  it doesn’t demonstrate robustness
under coarser pruning [Han et al.  2016  Gomez et al.  2018]. Pruning based on the probability
ρ is also popular in the Bayesian community  but achieving the target sparsity in sophisticated
networks requires extra ﬁne-tuning. We instead apply the magnitude-based weight-pruning to our
Resnet compression experiments and refer to it as SGLD-SA  which is detailed in Algorithm 1. The
corresponding variant of SGHMC with SA is referred to as SGHMC-SA.

4 Convergence Analysis

The key to guaranteeing the convergence of the adaptive SGLD algorithm is to use Poisson’s equation
to analyze additive functionals. By decomposing the Markov state-dependent noise Ω into martingale
difference sequences and perturbations  where the latter can be controlled by the regularity of the
solution of Poisson’s equation  we can guarantee the consistency of the latent variable estimators.
Theorem 1 (L2 convergence rate). For any α ∈ (0  1]  under assumptions in Appendix B.1  the
algorithm satisﬁes: there exists a constant λ and an optimum θ∗ such that

E(cid:104)(cid:107)θ(k) − θ∗(cid:107)2(cid:105) ≤ λk−α.

SGLD with adaptive latent variables forms a sequence of inhomogenous Markov chains and the weak
convergence of β to the target posterior is equivalent to proving the weak convergence of SGLD with
biased estimations of gradients. Inspired by Chen et al. [2015]  we have:
Corollary 1. Under assumptions in Appendix B.2  the random vector β(k) from the adaptive transi-
tion kernel Πθ(k−1) converges weakly to the invariant distribution eτ L(β θ∗) as  → 0 and k → ∞.
The smooth optimization of the priors makes the algorithm robust to bad initialization and avoids
entrapment in poor local optima. In addition  the convergence to the asymptotically correct distribution
allows us to combine simulated annealing to obtain better point estimates in non-convex optimization.

5 Experiments

5.1 Simulation of Large-p-Small-n Regression

We conduct the linear regression experiments with a dataset containing n = 100 observations
and p = 1000 predictors. Np(0  Σ) is chosen to simulate the predictor values X (training set)
i j=1 with Σi j = 0.6|i−j|. Response values y are generated from Xβ + η  where
where Σ = (Σ)p
c )  β2 ∼ N (2  σ2
β = (β1  β2  β3  0  0  ...  0)(cid:48) and η ∼ Nn(0  3In). We assume β1 ∼ N (3  σ2
c ) 
†The quadratic equation has only one unique positive root. (cid:107) · (cid:107) refers to L2 norm  (cid:107) · (cid:107)1 represents L1 norm.

5

Algorithm 1 SGLD-SA with SSGL priors

Initialize: β(1)  ρ(1)  κ(1)  σ(1) and δ(1) from scratch  set target sparse rates D  (cid:102) and S
for k ← 1 : kmax do

Sampling
β(k+1) ← β(k) + (k)∇βQ(·|B(k)) + N (0  2(k)τ−1)
Stochastic Approximation for Latent Variables
SA: ρ(k+1) ← (1 − ω(k+1))ρ(k) + ω(k+1) ˜ρ(k+1) following Eq.(12)
SA: κ(k+1) ← (1 − ω(k+1))κ(k) + ω(k+1)˜κ(k+1) following Eq.(13)
SA: σ(k+1) ← (1 − ω(k+1))σ(k) + ω(k+1) ˜σ(k+1) following Eq.(14)
SA: δ(k+1) ← (1 − ω(k+1))δ(k) + ω(k+1)˜δ(k+1) following Eq.(15)
if Pruning then

Prune the bottom-s% lowest magnitude weights
Increase the sparse rate s ← S(1 − Dk/(cid:102)

)

end if

end for

Table 1: Predictive errors in linear regression based on a test set considering different v0 and σ

MAE / MSE
SGLD-SA
SGLD-EM
SGLD

v0=0.01  σ=2
1.89 / 5.56
3.49 / 19.31
15.85 / 416.39

v0=0.1  σ=2
1.72 / 5.64
2.23 / 8.22

15.85 / 416.39

v0=0.01  σ=1
1.48 / 3.51
2.23 / 19.28
11.86 / 229.38

v0=0.1  σ=1
1.54 / 4.42
2.07 / 6.94
7.72 / 88.90

β3 ∼ N (1  σ2
c )  σc = 0.2. We introduce some hyperparameters  but most of them are uninformative.
We ﬁx τ = 1  λ = 1  ν = 1  v1 = 10  δ = 0.5  b = p and set a = 1. The learning rate follows
(k) = 0.001 × k− 1
3   and the step size is given by ω(k) = 10 × (k + 1000)−0.7. We vary v0 and σ to
show the robustness of SGLD-SA to different initializations. In addition  to show the superiority of
the adaptive update  we compare SGLD-SA with the intuitive implementation of the EMVS to SGLD
and refer to this algorithm as SGLD-EM  which is equivalent to setting ω(k) := 1 in SGLD-SA.
To obtain the stochastic gradient  we randomly select 50 observations and calculate the numerical
gradient. SGLD is sampled from the same hierarchical model without updating the latent variables.
We simulate 500  000 samples from the posterior distribution  and also simulate a test set with 50
observations to evaluate the prediction. As shown in Fig.1 (d)  all three algorithms are ﬁtted very
well in the training set  however  SGLD fails completely in the test set (Fig.1 (e))  indicating the
over-ﬁtting problem of SGLD without proper regularization when the latent variables are not updated.
Fig.1 (f) shows that although SGLD-EM successfully identiﬁes the right variables  the estimations are
lower biased. The reason is that SGLD-EM fails to regulate the right variables with L2 penalty  and
L1 leads to a greater amount of shrinkage for β1  β2 and β3 (Fig. 1 (a-c))  implying the importance
of the adaptive update via SA in the stochastic optimization of the latent variables. In addition  from
Fig. 1(a)  Fig. 1(b) and Fig.1(c)  we see that SGLD-SA is the only algorithm among the three that
quantiﬁes the uncertainties of β1  β2 and β3 and always gives the best prediction as shown in Table.1.
We notice that SGLD-SA is fairly robust to various hyperparameters.
For the simulation of SGLD-SA in logistic regression and the evaluation of SGLD-SA on UCI
datasets  we leave the results in Appendix C and D.

5.2 Classiﬁcation with Auto-tuning Hyperparameters

The following experiments are based on non-pruning SG-MCMC-SA  the goal is to show that auto-
tuning sparse priors are useful to avoid over-ﬁtting. The posterior average is applied to each Bayesian
model. We implement all the algorithms in Pytorch [Paszke et al.  2017]. The ﬁrst DNN is a standard
2-Conv-2-FC CNN model of 670K parameters (see details in Appendix D.1).
The ﬁrst set of experiments is to compare methods on the same model without using data augmentation
(DA) and batch normalization (BN) [Ioffe and Szegedy  2015]. We refer to the general CNN without
dropout as Vanilla  with 50% dropout rate applied to the hidden units next to FC1 as Dropout.

6

(a) Posterior estimation of β1.

(b) Posterior estimation of β2.

(c) Posterior estimation of β3.

(d) Training performance.

(e) Testing performance.

(f) Posterior mean vs truth.

Figure 1: Linear regression simulation when v0 = 0.1 and σ = 1.

Vanilla and Dropout models are trained with Adam [Kingma and Ba  2014] and Pytorch default
parameters (with learning rate 0.001). We use SGHMC as a benchmark method as it is also sampling-
based and has a close relationship with the popular momentum based optimization approaches
in DNNs. SGHMC-SA differs from SGHMC in that SGHMC-SA keeps updating SSGL priors
for the ﬁrst FC layer while they are ﬁxed in SGHMC. We set the training batch size n = 1000 
a  b = p and ν  λ = 1000. The hyperparameters for SGHMC-SA are set to v0 = 1  v1 = 0.1
and σ = 1 to regularize the over-ﬁtted space. The learning rate is set to 5 × 10−7  and the step
size is ω(k) = 1 × (k + 1000)− 3
4 . We use a thinning factor 500 to avoid a cumbersome system.
Fixed temperature can also be powerful in escaping “shallow" local traps [Zhang et al.  2017]  our
temperatures are set to τ = 1000 for MNIST and τ = 2500 for FMNIST.
The four CNN models are tested on MNIST and Fashion MNIST (FMNIST) [Xiao et al.  2017]
dataset. Performance of these models is shown in Tab.2. Compared with SGHMC  our SGHMC-SA
outperforms SGHMC on both datasets. We notice the posterior averages from SGHMC-SA and
SGHMC obtain much better performance than Vanilla and Dropout. Without using either DA or
BN  SGHMC-SA achieves 99.59% which outperforms some state-of-the-art models  such as Maxout
Network (99.55%) [Goodfellow et al.  2013] and pSGLD (99.55%) [Li et al.  2016] . In F-MNIST 
SGHMC-SA obtains 93.01% accuracy  outperforming all other competing models.
To further test the performance  we apply DA and BN to the following experiments (see details in
Appendix D.2) and refer to the datasets as DA-MNIST and DA-FMNIST. All the experiments are
conducted using a 2-Conv-BN-3-FC CNN of 490K parameters. Using this model  we obtain the
state-of-the-art 99.75% on DA-MNIST (200 epochs) and 94.38% on DA-FMNIST (1000 epochs) as
shown in Tab. 2. The results are noticeable  because posterior average is only conducted on a single
shallow CNN.

5.3 Defenses against Adversarial Attacks

Continuing with the setup in Sec. 5.2  the third set of experiments focuses on evaluating model
robustness. We apply the Fast Gradient Sign method [Goodfellow et al.  2014] to generate the

7

0.51.01.52.02.53.03.54.0SGLD−SASGLD−EMSGLDTrue value−2−10123SGLD−SASGLD−EMSGLDTrue value−10123SGLD−SASGLD−EMSGLDTrue value020406080100−15−10−50510lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllly^lllSGLD−SASGLD−EMSGLDTrue value01020304050−50510lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllly^lllSGLD−SASGLD−EMSGLDTrue valuelll0.00.51.01.52.02.53.00.00.51.01.52.02.53.0lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllb^llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllSGLD−SASGLD−EMSGLDTrue valueTable 2: Classiﬁcation accuracy using shallow networks

DATASET
VANILLA
DROPOUT
SGHMC
SGHMC-SA

MNIST DA-MNIST
99.31
99.38
99.47
99.59

99.54
99.56
99.63
99.75

FMNIST DA-FMNIST

92.73
92.81
92.88
93.01

93.14
93.35
94.29
94.38

adversarial examples with one single gradient step as in Papernot et al. [2016]’s study:

xadv ← x − ζ · sign{δx max

log P(y |x)} 

y

where ζ ranges from 0.1  0.2  . . .   0.5 to control the different levels of adversarial attacks.
Similar to the setup in Li and Gal [2017]  we normalize the adversarial images by clipping to the
range [0  1]. In Fig. 2(b) and Fig.2(d)  we see no signiﬁcant difference among all the four models in
the early phase. As the degree of adversarial attacks arises  the images become vaguer as shown in
Fig.2(a) and Fig.2(c). The performance of Vanilla decreases rapidly  reﬂecting its poor defense against
adversarial attacks  while Dropout performs better than Vanilla. But Dropout is still signiﬁcantly
worse than the sampling based methods. The advantage of SGHMC-SA over SGHMC becomes
more signiﬁcant when ζ > 0.25. In the case of ζ = 0.5 in MNIST where the images are hardly
recognizable  both Vanilla and Dropout models fail to identify the right images and their predictions
are as worse as random guesses. However  SGHMC-SA model achieves roughly 11% higher than
these two models and 1% higher than SGHMC  which demonstrates the robustness of SGHMC-SA.

(a) ζ = ....

(b) MNIST

(c) ζ = ....

(d) FMNIST

Figure 2: Adversarial test accuracies based on adversarial images of different levels

5.4 Residual Network Compression

Our compression experiments are conducted on the CIFAR-10 dataset [Krizhevsky  2009] with DA.
SGHMC and the non-adaptive SGHMC-EM are chosen as baselines. Simulated annealing is used to
enhance the non-convex optimization and the methods with simulated annealing are referred to as
A-SGHMC  A-SGHMC-EM and A-SGHMC-SA  respectively. We report the best point estimate.
We ﬁrst use SGHMC to train a Resnet20 model and apply the magnitude-based criterion to prune
weights to all convolutional layers (except the very ﬁrst one). All the following methods are evaluated
based on the same setup except for different step sizes to learn the latent variables. The sparse training
takes 1000 epochs. The mini-batch size is 1000. The learning rate starts from 2e-9 † and is divided by
10 at the 700th and 900th epoch. We set the inverse temperature τ to 1000 and multiply τ by 1.005
every epoch . We ﬁx ν = 1000 and λ = 1000 for the inverse gamma prior. v0 and v1 are tuned based
on different sparsity to maximize the performance. The smooth increase of the sparse rate follows the
pruning rule in Algorithm 1  and D and (cid:102) are set to 0.99 and 50  respectively. The increase in the
sparse rate s is faster in the beginning and slower in the later phase to avoid destroying the network
structure. Weight decay in the non-sparse layers C is set as 25.
As shown in Table 3  A-SGHMC-SA doesn’t distinguish itself from A-SGHMC-EM and A-SGHMC
when the sparse rate S is small  but outperforms the baselines given a large sparse rate. The pretrained
model has accuracy 93.90%  however the prediction performance can be improved to the state-of-the-
art 94.27% with 50% sparsity. Most notably  we obtain 91.68% accuracy based on 27K parameters

†It is equivalent to setting the learning rate to 1e-4 when we don’t multiply the likelihood with N
n .

8

0.00.10.20.30.40.50%20%40%60%80%100%SGHMC-SASGHMCDropoutVanilla0.00.10.20.30.40.50%20%40%60%80%100%SGHMC-SASGHMCDropoutVanillaTable 3: Resnet20 Compression on CIFAR10. When S = 0.9  we ﬁx v0 = 0.005  v1 =1e-5; When
S = 0.7  we ﬁx v0 = 0.1  v1 =5e-5; When S = 0.5  we ﬁx v0 = 0.1  v1 =5e-4; When S = 0.3  we
ﬁx v0 = 0.5  v1 =1e-3.

METHODS \ S
A-SGHMC

30%
94.07
A-SGHMC-EM 94.18
94.13
A-SGHMC-SA 94.23

SGHMC-SA

50%
94.16
94.19
94.11
94.27

70%
93.16
93.41
93.52
93.74

90%
90.59
91.26
91.45
91.68

(90% sparsity) in Resnet20. By contrast  targeted dropout obtained 91.48% accuracy based on 47K
parameters (90% sparsity) of Resnet32 [Gomez et al.  2018]  BC-GHS achieves 91.0% accuracy
based on 8M parameters (94.5% sparsity) of VGG models [Louizos et al.  2017]. We also notice
that when simulated annealing is not used as in SGHMC-SA  the performance will decrease by
0.2% to 0.3%. When we use batch size 2000 and inverse temperature schedule τ (k) = 20 × 1.01k 
A-SGHMC-SA still achieves roughly the same level  but the prediction of SGHMC-SA can be 1%
lower than A-SGHMC-SA.

6 Conclusion

We propose a novel AEB method to adaptively sample from hierarchical Bayesian DNNs and optimize
the spike-and-slab priors  which yields a class of scalable adaptive sampling algorithms in DNNs.
We prove the convergence of this approach to the asymptotically correct distribution. By adaptively
searching and penalizing the over-ﬁtted parameters  the proposed method achieves higher prediction
accuracy over the traditional SG-MCMC methods in both simulated examples and real applications
and shows more robustness towards adversarial attacks. Together with the magnitude-based weight
pruning strategy and simulated annealing  the AEB-based method  A-SGHMC-SA  obtains the
state-of-the-art performance in model compression.
Acknowledgments
We would like to thank Prof. Vinayak Rao  Dr. Yunfan Li and the reviewers for their insightful
comments. We acknowledge the support from the National Science Foundation (DMS-1555072 
DMS-1736364  DMS-1821233 and DMS-1818674) and the GPU grant program from NVIDIA.

References
Christophe Andrieu  Éric Moulines  and Pierre Priouret. Stability of stochastic approximation under

veriﬁable conditions. SIAM J. Control Optim.  44(1):283–312  2005.

Albert Benveniste  Michael Métivier  and Pierre Priouret. Adaptive Algorithms and Stochastic

Approximations. Berlin: Springer  1990.

Changyou Chen  Nan Ding  and Lawrence Carin. On the Convergence of Stochastic Gradient
MCMC Algorithms with High-order Integrators. In Proc. of the Conference on Advances in Neural
Information Processing Systems (NIPS)  pages 2278–2286  2015.

Tianqi Chen  Emily B. Fox  and Carlos Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In

Proc. of the International Conference on Machine Learning (ICML)  2014.

Edward I. George and Robert E. McCulloch. Variable Selection via Gibbs Sampling. Journal of the

American Statistical Association  88(423):881–889  1993.

Soumya Ghosh  Jiayu Yao  and Finale Doshi-Velez. Structured Variational Learning of Bayesian
Neural Networks with Horseshoe Priors. In Proc. of the International Conference on Machine
Learning (ICML)  2018.

Aidan N. Gomez  Ivan Zhang  Kevin Swersky  Yarin Gal  and Geoffrey E. Hinton. Targeted Dropout.

In NIPS 2018 workshop on Compact Deep Neural Networks with industrial applications  2018.

9

Ian J. Goodfellow  David Warde-Farley  Mehdi Mirza  Aaron Courville  and Yoshua Bengio. Maxout
networks. In Proc. of the International Conference on Machine Learning (ICML)  pages III–1319–
III–1327  2013.

Ian J. Goodfellow  Jonathon Shlens  and Christian Szegedy. Explaining and Harnessing Adversarial

Examples. ArXiv e-prints  December 2014.

Song Han  Huizi Mao  and William J Dally. Deep compression: Compressing deep neural network
with pruning  trained quantization and huffman coding. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)  2016.

Kaiming He  Xiangyu Zhang  Shaoqing Ren  and Jian Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2016.

Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by
Reducing Internal Covariate Shift. In Proc. of the International Conference on Machine Learning
(ICML)  pages 448–456  2015.

Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In Proc. of the

International Conference on Learning Representation (ICLR)  2014.

Alex Krizhevsky. Learning Multiple Layers of Features from tiny images. In Tech Report  2009.

Chunyuan Li  Changyou Chen  David Carlson  and Lawrence Carin. Preconditioned Stochastic
Gradient Langevin Dynamics for Deep Neural Networks. In Proc. of the National Conference on
Artiﬁcial Intelligence (AAAI)  pages 1788–1794  2016.

Yingzhen Li and Yarin Gal. Dropout inference in Bayesian neural networks with alpha-divergences.

In Proc. of the International Conference on Machine Learning (ICML)  2017.

Faming Liang. Trajectory averaging for stochastic approximation MCMC algorithms. The Annals of

Statistics  38:2823–2856  2010.

Faming Liang  Bochao Jia  Jingnan Xue  Qizhai Li  and Ye Luo. Bayesian Neural Networks for
Selection of Drug Sensitive Genes. Journal of the American Statistical Association  113(5233):
955–972  2018.

Ji Lin  Yongming Rao  Jiwen Lu  and Jie Zhou. Runtime Neural Pruning. In Proc. of the Conference

on Advances in Neural Information Processing Systems (NIPS)  2017.

Christos Louizos  Karen Ullrich  and Max Welling. Bayesian Compression for Deep learning. In

Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS)  2017.

Yi-An Ma  Tianqi Chen  and Emily B. Fox. A complete recipe for stochastic gradient MCMC. In

Proc. of the Conference on Advances in Neural Information Processing Systems (NIPS)  2015.

Oren Mangoubi and Nisheeth K. Vishnoi. Convex Optimization with Unbounded Nonconvex Oracles

using Simulated Annealing. In Proc. of Conference on Learning Theory (COLT)  2018.

Pavlo Molchanov  Stephen Tyree  Tero Karras  Timo Aila  and Jan Kautz. Pruning Convolutional
Neural Networks for Resource Efﬁcient Inference. In Proc. of the International Conference on
Learning Representation (ICLR)  2017.

Nicolas Papernot  Nicholas Carlini  Ian Goodfellow  Reuben Feinman  Fartash Faghri  Alexander
Matyasko  Karen Hambardzumyan  Yi-Lin Juang  Alexey Kurakin  Ryan Sheatsley  Abhibhav
Garg  and Yen-Chen Lin. cleverhans v2.0.0: an adversarial machine learning library. ArXiv
e-prints  October 2016.

Adam Paszke  Sam Gross  Soumith Chintala  Gregory Chanan  Edward Yang  Zachary DeVito 
Zeming Lin  Alban Desmaison  Luca Antiga  and Adam Lerer. Automatic differentiation in
PyTorch. In NIPS Autodiff Workshop  2017.

Maxim Raginsky  Alexander Rakhlin  and Matus Telgarsky. Non-convex learning via Stochastic
Gradient Langevin Dynamics: a nonasymptotic analysis. In Proc. of Conference on Learning
Theory (COLT)  June 2017.

10

Veronika Roˇrková and Edward I. George. EMVS: The EM Approach to Bayesian variable selection.

Journal of the American Statistical Association  109(506):828–846  2014.

Veronika Roˇrková and Edward I. George. The Spike-and-Slab Lasso. Journal of the American

Statistical Association  113:431–444  2018.

Yunus Saatci and Andrew G Wilson. Bayesian GAN. In Proc. of the Conference on Advances in

Neural Information Processing Systems (NIPS)  pages 3622–3631  2017.

Nahum Shimkin. Introduction to Stochastic Approximation Algorithms  2011. URL http://webee.

technion.ac.il/shimkin/LCS11/ch5_SA.pdf.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  2014.

Yee Whye Teh  Alexandre Thiéry  and Sebastian Vollmer. Consistency and Fluctuations for Stochastic

Gradient Langevin Dynamics. Journal of Machine Learning Research  17:1–33  2016.

Robert Tibshirani. Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical

Society  Series B  58:267–288  1994.

Max Welling and Yee Whye Teh. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In

Proc. of the International Conference on Machine Learning (ICML)  pages 681–688  2011.

Han Xiao  Kashif Rasul  and Roland Vollgraf. Fashion-MNIST: a Novel Image Dataset for Bench-

marking Machine Learning Algorithms. ArXiv e-prints  August 2017.

Pan Xu  Jinghui Chen  Difan Zou  and Quanquan Gu. Global Convergence of Langevin Dynamics
Based Algorithms for Nonconvex Optimization. In Proc. of the Conference on Advances in Neural
Information Processing Systems (NIPS)  December 2018.

Mao Ye and Yan Sun. Variable Selection via Penalized Neural Network: a Drop-Out-One Loss
Approach. In Proc. of the International Conference on Machine Learning (ICML)  volume 80 
pages 5620–5629  10–15 Jul 2018.

Yuchen Zhang  Percy Liang  and Moses Charikar. A Hitting Time Analysis of Stochastic Gradient
Langevin Dynamics. In Proc. of Conference on Learning Theory (COLT)  pages 1980–2022  2017.

Hui Zou and Trevor Hastie. Regularization and Variable Selection via the Elastic Net. Journal of the

Royal Statistical Society  Series B  67(2):301–320  2005.

11

,Wei Deng
Xiao Zhang
Faming Liang
Guang Lin