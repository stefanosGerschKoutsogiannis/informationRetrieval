2016,Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow,This paper puts forth a novel algorithm  termed \emph{truncated generalized gradient flow} (TGGF)  to solve for $\bm{x}\in\mathbb{R}^n/\mathbb{C}^n$ a system of $m$ quadratic equations $y_i=|\langle\bm{a}_i \bm{x}\rangle|^2$  $i=1 2 \ldots m$  which even for $\left\{\bm{a}_i\in\mathbb{R}^n/\mathbb{C}^n\right\}_{i=1}^m$ random is known to be \emph{NP-hard} in general. We prove that as soon as the number of equations $m$ is on the order of the number of unknowns $n$  TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data $\left\{\left(\bm{a}_i;\ y_i\right)\right\}_{i=1}^m$. Specifically  TGGF proceeds in two stages: s1) A novel \emph{orthogonality-promoting} initialization that is obtained with simple power iterations; and  s2) a refinement of the initial estimate by successive updates of scalable \emph{truncated generalized gradient iterations}. The former is in sharp contrast to the existing spectral initializations  while the latter handles the rather challenging nonconvex and nonsmooth \emph{amplitude-based} cost function. Numerical tests demonstrate that: i) The novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts; and ii) even with the same initialization  our refinement/truncation outperforms Wirtinger-based alternatives  all corroborating the superior performance of TGGF over state-of-the-art algorithms.,Solving Random Systems of Quadratic Equations via

Truncated Generalized Gradient Flow

Gang Wang∗ † and Georgios B. Giannakis†

∗ ECE Dept. and Digital Tech. Center  Univ. of Minnesota  Mpls  MN 55455  USA
† School of Automation  Beijing Institute of Technology  Beijing 100081  China

{gangwang  georgios}@umn.edu

Abstract

This paper puts forth a novel algorithm  termed truncated generalized gradi-
ent ﬂow (TGGF)  to solve for x ∈ Rn/Cn a system of m quadratic equations
yi = |(cid:104)ai  x(cid:105)|2  i = 1  2  . . .   m  which even for {ai ∈ Rn/Cn}m
i=1 random is
known to be NP-hard in general. We prove that as soon as the number of equations
m is on the order of the number of unknowns n  TGGF recovers the solution
exactly (up to a global unimodular constant) with high probability and complexity
growing linearly with the time required to read the data {(ai; yi)}m
i=1. Speciﬁcally 
TGGF proceeds in two stages: s1) A novel orthogonality-promoting initialization
that is obtained with simple power iterations; and  s2) a reﬁnement of the initial es-
timate by successive updates of scalable truncated generalized gradient iterations.
The former is in sharp contrast to the existing spectral initializations  while the
latter handles the rather challenging nonconvex and nonsmooth amplitude-based
cost function. Empirical results demonstrate that: i) The novel orthogonality-
promoting initialization method returns more accurate and robust estimates relative
to its spectral counterparts; and  ii) even with the same initialization  our reﬁne-
ment/truncation outperforms Wirtinger-based alternatives  all corroborating the
superior performance of TGGF over state-of-the-art algorithms.

1

Introduction

Consider a system of m quadratic equations
yi = |(cid:104)ai  x(cid:105)|2  

i ∈ [m] := {1  2  . . .   m}

(1)
T and feature vectors ai ∈ Rn/Cn  collected in the m× n matrix
where data vector y := [y1 ··· ym]
H are known  whereas vector x ∈ Rn/Cn is the wanted unknown. When {ai}m
A := [a1 ··· am]
i=1
and/or x are complex  their amplitudes are given but phase information is lacking; whereas in the real
case only the signs of {(cid:104)ai  x(cid:105)} are unknown. Supposing that the system of equations in (1) admits
a unique solution x (up to a global unimodular constant)  our objective is to reconstruct x from m
phaseless quadratic equations  or equivalently  recover the missing signs/phases of (cid:104)ai  x(cid:105) in the
real-/complex-valued settings. Indeed  it has been established that m ≥ 2n−1 or m ≥ 4n−4 generic
data {(ai; yi)}m
i=1 as in (1) sufﬁce for uniqueness of an n-dimensional real- or complex-valued vector
x [1  2]  respectively  and the former with equality has also been shown to be necessary [1].
The problem in (1) constitutes an instance of nonconvex quadratic programming  that is generally
known to be NP-hard [3]. Speciﬁcally for real-valued vectors  this can be understood as a com-
binatorial optimization since one seeks a series of signs si = ±1  such that the solution to the
system of linear equations (cid:104)ai  x(cid:105) = siψi  where ψi :=
yi  obeys the given quadratic system (1).
T   apparently there are a
Concatenating all amplitudes {ψi}m
total of 2m different combinations of {si}m
i=1  among which only two lead to x up to a global sign.

i=1 to form the vector ψ := [ψ1 ··· ψm]

√

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

The complex case becomes even more complicated  where instead of a set of signs {si}m
i=1  one must
specify for uniqueness a collection of unimodular complex scalars {σi ∈ C}m
i=1. In many ﬁelds of
physical sciences and engineering  the problem of recovering the phase from intensity/magnitude-only
measurements is commonly referred to as phase retrieval [4  5]. The plethora of applications include
X-ray crystallography  optics  as well as array imaging  where due to physical limitations  optical
detectors can record only (squared) modulus of the Fresnel or Fraunhofer diffraction pattern  while
losing the phase of the incident light reaching the object [5]. It has been shown that reconstructing a
discrete  ﬁnite-duration signal from its Fourier transform magnitude is NP-complete [6]. Despite its
simple form and practical relevance across various ﬁelds  tackling the quadratic system (1) under
real-/complex-valued settings is challenging and NP-hard in general.

1.1 Nonconvex Optimization

Adopting the least-squares criterion  the task of recovering x can be recast as that of minimizing the
following intensity-based empirical loss

i z(cid:12)(cid:12)2(cid:17)2
(cid:16)
yi −(cid:12)(cid:12)aH
(cid:0)ψi −(cid:12)(cid:12)aH
i z(cid:12)(cid:12)(cid:1)2

.

m(cid:88)
m(cid:88)

i=1

i=1

or  the amplitude-based one

min
z∈Cn

f (z) :=

min
z∈Cn

(cid:96)(z) :=

1
2m

1
2m

(2)

(3)

Unfortunately  both cost functions (2) and (3) are nonconvex. Minimizing nonconvex objectives 
which may exhibit many stationary points  is in general NP-hard [7]. In a nutshell  solving problems
of the form (2) or (3) is challenging.
Existing approaches to solving (2) (or related ones using the Poisson likelihood; see  e.g.  [8]) or
(3) fall under two categories: nonconvex and convex ones. Popular nonconvex solvers include the
alternating projection such as Gerchberg-Saxton [9] and Fineup [10]  AltMinPhase [11]  and (Trun-
cated) Wirtinger ﬂow (WF/TWF) [12  8]  as well as trust-region methods [13]. Convex approaches
on the other hand rely on the so-called matrix-lifting technique to obtain the solvers abbreviated as
PhaseLift [14] and PhaseCut [15].
In terms of sample complexity for Gaussian {ai} designs  convex approaches enable exact recovery
from1 O(n) noiseless measurements [16]  while they require solving a semideﬁnite program of a
matrix variable with size n × n  thus incurring worst-case computational complexity on the order of
O(n4.5) [15]  that does not scale well with dimensionality n. Upon exploiting the underlying problem
structure  O(n4.5) can be reduced to O(n3) [15]. Solving for vector variables  nonconvex approaches
achieve signiﬁcantly improved computational performance. Using formulation (3)  AltMinPhase
adopts a spectral initialization and establishes exact recovery with sample complexity O(n log3 n)
under Gaussian {ai} designs with resampling [11]. Concerning formulation (2)  WF iteratively reﬁnes
the spectral initial estimate by means of a gradient-like update [12]. The follow-up TWF improves
upon WF through a truncation procedure to separate gradient components of excessively extreme
sizes. Likewise  at the initialization stage  since the term (aT
responsible for the spectral
initialization is heavy-tailed  data {yi}m
i=1 are pre-screened in the truncated spectral initialization to
yield improved initial estimates [8]. Under Gaussian sampling models  WF allows exact recovery
from O(n log n) measurements in O(mn2 log(1/)) time/ﬂops to yield an -accurate solution for
any given  > 0 [12]  while TWF advances these to O(n) measurements and O(mn log(1/))
time [8]. Interestingly  the truncation procedure in the gradient stage turns out to be useful in avoiding
spurious stationary points in the context of nonconvex optimization. Although for large-scale linear
regressions  similar ideas including censoring have been studied [17  18]. It is worth mentioning
that when m ≥ Cn log3 n for sufﬁciently large C > 0  the objective function in (3) admits benign
geometric structure that allows certain iterative algorithms (e.g.  trust-region methods) to efﬁciently
ﬁnd a global minimizer with random initializations [13].
Although achieving a linear (in the number of unknowns n) sample and computational complexity 
the state-of-the-art TWF scheme still requires at least 4n ∼ 5n equations to yield a stable empirical
success rate (e.g.  ≥ 99%) under the real Gaussian model [8  Section 3]  which are more than twice
the known information-limit of m = 2n − 1 [1]. Similar though less obvious results hold also in

i x)2aiaH

i

1The notation φ(n) = O(g(n)) means that there is a constant c > 0 such that |φ(n)| ≤ c|g(n)|.

2

the complex-valued scenario. Even though the truncated spectral initialization improves upon the
“plain vallina” spectral initialization  its performance still suffers when the number of measurements
is relatively small and its advantage (over the untruncated version) narrows as the number of mea-
surements grows. Further  it is worth stressing that extensive numerical and experimental validation
conﬁrms that the amplitude-based cost function performs better than the intensity-based one; that is 
formulation (3) is superior over (2) [19]. Hence  besides enhancing initialization  markedly improved
performance in the gradient stage could be expected by re-examining the amplitude-based cost
function and incorporating judiciously designed truncation rules.

2 Algorithm: Truncated Generalized Gradient Flow

Along the lines of suitably initialized nonconvex schemes  and building upon the amplitude-based
formulation (3)  this paper develops a novel linear-time (in both m and n) algorithm  referred to
as truncated generalized gradient ﬂow (TGGF)  that provably recovers x ∈ Rn/Cn exactly from a
near-optimal number of noise-free measurements  while also featuring a near-perfect statistical per-
formance in the noisy setup. Our TGGF proceeds in two stages: s1) A novel orthogonality-promoting
initialization that relies on simple power iterations to markedly improve upon spectral initialization;
and  s2) a reﬁnement of the initial estimate by successive updates of truncated generalized gradient
iterations. Stages s1) and s2) are delineated next in reverse order. For concreteness  our analysis
will focus on the real Gaussian model with x ∈ Rn and independently and identically distributed
(i.i.d.) design vectors ai ∈ Rn ∼ N (0  In)  whereas numerical implementations for the complex
Gaussian model having x ∈ Cn and i.i.d. ai ∼ CN (0  In) := N (0  In/2) + jN (0  In/2) will
be discussed brieﬂy. To start  deﬁne the Euclidean distance of any estimate z to the solution set:
dist(z  x) := min(cid:107)z ± x(cid:107) for real signals  and dist(z  x) := minφ∈[0 2π) (cid:107)z − xeiφ(cid:107) for complex
ones [12]. Deﬁne also the indistinguishable global phase constant in real-valued settings as

(cid:26) 0 

π 

φ(z) :=

(cid:107)z − x(cid:107) ≤ (cid:107)z + x(cid:107) 
otherwise.

(4)

Henceforth  ﬁxing x to be any solution of the given quadratic system (1)  we always assume that
φ (z) = 0; otherwise  z is replaced by e−jφ(z)z  but for simplicity of presentation  the constant
phase adaptation term e−jφ(z) is dropped whenever it is clear from the context.
Numerical tests comparing TGGF  TWF  and WF will be presented throughout our analysis  so let us
ﬁrst describe our basic test settings. Simulated estimates will be averaged over 100 independent Monte
Carlo (MC) realizations without mentioning this explicitly each time. Performance is evaluated in
terms of the relative root mean-square error  i.e.  Relative error := dist(z  x)/(cid:107)x(cid:107)  and the success
rate among 100 trials  where a success will be claimed for a trial if the resulting estimate incurs
relative error less than 10−5 [8]. Simulated tests under both noiseless and noisy Gaussian models
with i.i.d. ai ∼ N (0  In) or ai ∼ CN (0  In).

(cid:12)(cid:12) with ηi = 0 and ηi ∼ N (0  σ2) [11]  respectively 

are performed  corresponding to ψi =(cid:12)(cid:12)aH

i x + ηi

2.1 Truncated generalized gradient stage

Let us rewrite the amplitude-based cost function in a matrix-vector form as

(cid:13)(cid:13)ψ − |Az|(cid:13)(cid:13)2

(cid:96)(z) =

1
2m

(5)

where |Az| :=(cid:2)|aT

min
z∈Rn

mz|(cid:3)T

1 z| ··· |aT

. Apart from being nonconvex  (cid:96)(z) is nondiffentiable. In the
presence of smoothness or convexity  convergence analysis of iterative algorithms relies either on
continuity of the gradient (gradient methods) [20]  or  on the convexity of the objective functional
(subgradient methods) [20]. Although subgradient methods have found widespread applicability in
nonsmooth optimization  they are limited to the class of convex functions [20  Page 4]. In nonconvex
nonsmooth optimization  the so-termed generalized gradient broadens the scope of the (sub)gradient
to the class of almost everywhere differentiable functions [21]. Consider a continuous function
h(z) ∈ R deﬁned over an open region S ⊆ Rn.
Deﬁnition 1 [22  Deﬁnition 1.1] The generalized gradient of a function h at z  denoted by ∂h  is the
convex hull of the set of limits of the form lim∇h(zk)  where zk → z as k → +∞  i.e. 

3

(cid:110)

∂h(z) := conv

k→+∞∇h(zk) : zk → z  zk /∈ G(cid:96)

lim

(cid:111)

where the symbol ‘conv’ signiﬁes the convex hull of a set  and G(cid:96) denotes the set of points in S at
which h fails to be differentiable.

Having introduced the notion of generalized gradient  and with t denoting the iteration number  our
approach to solving (5) amounts to iteratively reﬁning the initial guess z0 by means of the ensuing
truncated generalized gradient iterations

(6)
where µt > 0 is the stepsize  and a piece of the (truncated) generalized gradient ∂(cid:96)tr(zt) is given by

zt+1 = zt − µt∂(cid:96)tr(zt)

∂(cid:96)tr(zt) :=

ai

(7)

(cid:88)

(cid:16)

i∈It+1

(cid:17)

i zt − ψi
aT

aT
i zt
i zt|
|aT

for some index set It+1 ⊆ [m] to be designed shortly; and the convention aT
i zt
i zt| := 0 is adopted  if
|aT
aT
i zt = 0. Further  it is easy to verify that the update in (6) monotonically decreases the objective
value in (5).
Recall that since they offer descent iterations  the alter-
nating projection variants are guaranteed to converge
to a stationary point of (cid:96)(z)  and any limit point z∗
adheres to the following ﬁxed-point equation [23]

AT(cid:16)

(cid:17)

Az∗ − ψ (cid:12) Az∗
|Az∗|

= 0

(8)
for entry-wise product (cid:12)  which may have many so-
lutions. Clearly  if z∗ is a solution  so is −z∗. Fur-
ther  both solutions/global minimizers x and −x sat-
isfy (8) due to Ax − ψ (cid:12) Ax|Ax| = 0. Consider-
ing any stationary point z∗ (cid:54)= ±x that has been
adapted such that φ(z∗) = 0  one can write z∗ =

x+(AT A)−1AT(cid:2)ψ(cid:12)(cid:0) Az∗

(cid:1)(cid:3). A necessary

Figure 1: Empirical success rate for WF 
TWF  and TGGF with the same truncated
spectral initialization under the noiseless real
Gaussian model.

|Az∗| − Ax|Ax|
condition for z∗ (cid:54)= x is Az∗
|Az∗| (cid:54)= Ax|Ax|. Expressed dif-
ferently  there must be sign differences between Az∗
|Az∗|
and Ax|Ax| whenever one gets stuck with an undesirable
stationary point z∗. Building on this observation  it is reasonable to devise algorithms that can detect
and separate out the generalized gradient components corresponding to mistakenly estimated signs
aT
i zt| along the iterates {zt}. Precisely  if zt and x lie in different sides of the hyperplane aT
i zt
i z = 0 
|aT
i x; that is  aT
i x| (cid:54)= aT
then the sign of aT
i z
i z|. Speciﬁcally  one
|aT
|aT
(cid:17)
(cid:16) aT
can write the i-th generalized gradient component
i x| − aT
i z
|aT
|aT
i z|

i zt will be different than that of aT

∂(cid:96)i(z) =

(cid:16)

(cid:17)

ai +

ψiai

i x

i x

(cid:16)

(cid:17)
aT
i z − ψi
aT
i z
(cid:16) aT
i z|
|aT
i x| − aT
i z
i x
|aT
|aT
i z|

i h +

ai =

= aiaT

(9)
where h := z − x. Apparently  the strong law of large numbers (SLLN) asserts that averaging the
ﬁrst term aiaT
i h over m instances approaches h  which qualiﬁes it as a desirable search direction.
However  certain generalized gradient entries involve erroneously estimated signs of aT
i x; hence 
nonzero ri terms exert a negative inﬂuence on the search direction h by dragging the iterate away
from x  and they typically have sizable magnitudes. To see why  recall that quantities maxi∈[m] ψi
(cid:107)h(cid:107) ≤ ρ(cid:107)x(cid:107) for some small constant 0 < ρ ≤ 1/10  to be discussed shortly. To maintain a
meaningful search direction  those ‘bad’ generalized gradient entries should be detected and excluded
from the search direction.

m(cid:107)x(cid:107) and(cid:112)π/2(cid:107)x(cid:107)  respectively  whereas

and (1/m)(cid:80)m

i=1 ψi have magnitudes on the order of

√

aT
i z − ψi
aT
i x
(cid:17)
|aT
i x|
(cid:52)
= aiaT
i h + ri

ψiai

4

m/n for x∈ R1 0001234567Empirical success rate00.20.40.60.81WFTWFTAFNevertheless  it is difﬁcult or even impossible to check whether the sign of aT
i zt equals that of
aT
i x. Fortunately  when the initialization is accurate enough  most spurious gradient entries (those
corrupted by nonzero ri terms) provably hover around the watershed hyperplane aT
i zt = 0. For this
reason  TGGF includes only those components having zt sufﬁciently away from its watershed

(cid:110)

(cid:12)(cid:12)(cid:12)|aT
i zt|
i x| ≥ 1
|aT

1 + γ

(cid:111)

 

It+1 :=

1 ≤ i ≤ m

t ≥ 0

(10)

i /∈ Tt+1) introduces large bias into (1/m)(cid:80)m

for an appropriately selected threshold γ > 0. It is worth stressing that our novel truncation rule
deviates from the intuition behind TWF. Among its complicated truncation procedures  TWF also
throws away large-size gradient components corresponding to (10)  which is not the case with TGGF.
As demonstrated by our analysis  it rarely happens that a generalized gradient component having a
large |aT
i x. Further  discarding too many samples (those
i ht  thus rendering TWF less effective
i∈Tt+1
when m/n is small. Numerical comparison depicted in Fig. 1 suggests that even starting with
the same truncated spectral initialization  TGGF’s reﬁnement outperforms those of TWF and WF 
corroborating the merits of our novel truncation and update rule over TWF/WF.

i zt|/(cid:107)zt(cid:107) yields an incorrect sign of aT

aiaT

2.2 Orthogonality-promoting initialization stage

(cid:80)

i∈T0

yiaiaT

Leveraging the SLLN  spectral methods estimate x
using the (appropriately scaled) leading eigenvector
i   where T0 is an index
of Y := 1
m
set accounting for possible truncation. As asserted
in [8]  each summand (aT
i x)2aiaT
i follows a heavy-
tail probability density function lacking a moment
generating function. This causes major performance
degradation especially when the number of measure-
ments is limited. Instead of spectral initialization  we
shall take another route to bypass this hurdle. To gain
intuition for selecting our alternate route  a motivat-
ing example is presented ﬁrst that reveals fundamen-
tal characteristics among high-dimensional random
vectors.
Example: Fixing any nonzero vector x ∈ Rn  gen-
erate data ψi = |(cid:104)ai  x(cid:105)| using i.i.d. ai ∼ N (0  In) 
∀i ∈ [m]  and evaluate the squared normalized inner-
product

Figure 2: Ordered squared normalized inner-
product for pairs x and ai  ∀i ∈ [m] with
m/n varying by 2 from 2 to 10  and n = 103.

(11)

cos2 θi :=

|(cid:104)ai  x(cid:105)|2
(cid:107)ai(cid:107)2(cid:107)x(cid:107)2 =

ψ2
i

(cid:107)ai(cid:107)2(cid:107)x(cid:107)2   ∀i ∈ [m]

(cid:3)T

and collectively denote them as ξ :=(cid:2)cos2 θ[m] ··· cos2 θ[1]

where θi is the angle between ai and x. Consider ordering all cos2 θi’s in an ascending fashion 
with cos2 θ[1] ≥ ··· ≥ cos2 θ[m].
Fig. 2 plots the ordered entries in ξ for m/n varying by 2 from 2 to 10 with n = 103. Observe that
almost all {ai} vectors have a squared normalized inner-product smaller than 10−2  while half of the
inner-products are less than 10−3  which implies that x is nearly orthogonal to many ai’s.
This example corroborates that random vectors in high-dimensional spaces are almost always nearly
orthogonal to each other [24]. This inspired us to pursue an orthogonality-promoting initialization
method. Our key idea is to approximate x by a vector that is most orthogonal to a subset of vectors
{ai}i∈I0  where I0 is a set with cardinality |I0| < m that includes indices of the smallest squared
not inﬂuence their ordering. Henceforth  we assume without loss of generality that (cid:107)x(cid:107) = 1.
Using {(ai; ψi)}  evaluate cos2 θi according to (11) for each pair x and ai. Instrumental for the
ensuing derivations is noticing that the summation of cos2 θi over indices i ∈ I0 is very small  while
rigorous justiﬁcation is deferred to Section 3 and supplementary materials. Thus  a meaningful
approximation denoted by z0 ∈ Rn can be obtained by solving

(cid:9). Since (cid:107)x(cid:107) appears in all inner-products  its exact value does

normalized inner-products(cid:8)cos2 θi

5

Number of points100101102103104Squared normalized inner-product10-1410-1210-1010-810-610-410-2100m=2nm=4nm=6nm=8nm=10n(cid:32)

(cid:88)

i∈I0

1
|I0|

(cid:33)

z

aiaT
(cid:107)ai(cid:107)2

i

zT

min(cid:107)z(cid:107)=1

(cid:80)

1|I0|

(12)

i

aiaT
(cid:107)ai(cid:107)2 .
which amounts to ﬁnding the smallest eigenvalue and the associated eigenvector of
Yet ﬁnding the smallest eigenvalue calls for eigen-decomposition or matrix inversion  each requiring
computational complexity O(n3). Such a computational burden can be intractable when n grows
large. Applying a standard concentration result simpliﬁes greatly those computations next [25].
Since ai/(cid:107)ai(cid:107) has unit norm and is uniformly distributed on the unit sphere  it is uniformly spheri-
cally distributed.2 Spherical symmetry implies that ai/(cid:107)ai(cid:107) has zero mean and covariance matrix
aiaT
(cid:107)ai(cid:107)2 approaches
In/n [25]. Appealing again to the SLLN  the sample covariance matrix 1
m
aiaT
aiaT
(cid:107)ai(cid:107)2 (cid:117)
1

n In as m grows. Simple derivations lead to (cid:80)
n In −(cid:80)

aiaT
(cid:107)ai(cid:107)2   where I0 is the complement of I0 in the set [m].

(cid:80)m
(cid:107)ai(cid:107)2 −(cid:80)

i∈I0

i∈I0

i∈I0

(cid:107)ai(cid:107)2 = (cid:80)m

aiaT

i

i=1

i=1

i

i∈I0

i

m

Deﬁne S := [a1/(cid:107)a1(cid:107) ··· am/(cid:107)am(cid:107)]
indices do not belong to I0. The task of seeking the smallest eigenvalue of Y0 := 1|I0| ST
to computing the largest eigenvalue of Y0 := 1

T ∈ Rm×n  and form S0 by removing the rows of S if their
0 S0 reduces

0 S0  namely 

i

i

|I0| ST
˜z0 := arg max
(cid:107)z(cid:107)=1

zT Y0z

(13)

i=1 yi)  thus yielding z0 =(cid:112)(cid:80)m
(cid:80)m

which can be efﬁciently solved using simple power iterations. If  on the other hand  (cid:107)x(cid:107) (cid:54)= 1  the
estimate ˜z0 from (13) is further scaled so that its norm matches approximately that of x (which
is estimated to be 1
i=1 yi/m˜z0. It is worth stressing that the
m
constructed matrix Y0 does not depend on {yi} explicitly  saving our initialization from suffering
heavy-tails of the fourth order of {ai} in spectral initialization schemes.
Fig. 3 compares three initialization schemes showing
their relative errors versus the measurement/unknown
ratio m/n under the noise-free real Gaussian model 
where x ∈ R1 000 and m/n increases by 2 from 2 to
20. Apparently  all schemes enjoy improved perfor-
mance as m/n increases. In particular  the proposed
initialization method outperforms its spectral alterna-
tives. Interestingly  the spectral and truncated spectral
schemes exhibit similar performance when m/n is
sufﬁciently large (e.g.  m/n ≥ 14). This conﬁrms
that truncation helps only if m/n is relatively small.
Indeed  truncation is effected by discarding measure-
ments of excessively large sizes emerging from the
heavy tails of the data distribution. Hence  its advan-
tage over the untruncated one narrows as the number
of measurements increases  thus straightening out
the heavy tails. On the contrary  the orthogonality-
promoting initialization method achieves consistently
superior performance over its spectral alternatives.

Figure 3: Relative error versus m/n for: i)
the spectral method; ii) the truncated spectral
method; and iii) our orthogonality-promoting
method for noiseless real Gaussian model.

3 Main results

TGGF is summarized in Algorithm 1 with default values set for pertinent algorithmic parameters.
Postulating independent samples {(ai; ψi)}  the following result establishes the performance of our
TGGF approach.

2A random vector z ∈ Rn is said to be spherical (or spherically symmetric) if its distribution does not change
under rotations of the coordinate system; that is  the distribution of P z coincides with that of z for any given
orthogonal n × n matrix P .

6

m/n for x∈R1 00025101520Relative error0.30.40.50.60.70.80.911.11.21.3SpectralTruncated spectralOrthogonality-promotingAlgorithm 1 Truncated generalized gradient ﬂow (TGGF) solvers
1: Input: Data {ψi}m
i=1; the maximum number of iterations T =
1  000; by default  take constant step size µ = 0.6/1 for real/complex Gaussian models  truncation
thresholds |I0| = (cid:100) 1
(ψi/(cid:107)ai(cid:107))’s.

i=1 and feature vectors {ai}m
6 m(cid:101) ((cid:100)·(cid:101) the ceil operation)  and γ = 0.7.

2: Evaluate ψi/(cid:107)ai(cid:107)  ∀i ∈ [m]  and ﬁnd I0 comprising indices corresponding to the |I0| largest

i=1 ψ2

i /m˜z0  where ˜z0 is the unit leading eigenvector of Y0

:=

1
|I0|

3: Initialize z0 to (cid:112)(cid:80)m

(cid:80)
where It+1 :=(cid:8)1 ≤ i ≤ m(cid:12)(cid:12)|aT

4: Loop: for t = 0 to T − 1

aiaT
(cid:107)ai(cid:107)2 .

i∈I0

i

(cid:88)
zt+1 = zt − µ
m
i zt| ≥ 1

i∈It+1
1+γ ψi

(cid:18)
(cid:9).

5: Output: zT

(cid:19)

ai

i zt − ψi
aT

aT
i zt
i zt|
|aT

Theorem 1 Let x ∈ Rn be an arbitrary signal vector  and consider (noise-free) measurements
ψi = |aT
i.i.d.∼ N (0  In)  1 ≤ i ≤ m. Then with probability at least 1 − (m +
5)e−n/2 − e−c0m − 1/n2 for some universal constant c0 > 0  the initialization z0 returned by the
orthogonality-promoting method in Algorithm 1 satisﬁes

i x|  in which ai

dist(z0  x) ≤ ρ(cid:107)x(cid:107)

(14)
with ρ = 1/10 (or any sufﬁciently small positive constant)  provided that m ≥ c1|I0| ≥ c2n for
some numerical constants c1  c2 > 0  and sufﬁciently large n. Further  choosing a constant step size
µ ≤ µ0 along with a ﬁxed truncation level γ ≥ 1/2  and starting from any initial guess z0 satisfying
(14)  successive estimates of the TGGF solver (tabulated in Algorithm 1) obey

(15)
for some 0 < ν < 1  which holds with probability exceeding 1 − (m + 5)e−n/2 − 8e−c0m − 1/n2.
Typical parameters are µ = 0.6  and γ = 0.7.

t = 0  1  . . .

dist (zt  x) ≤ ρ (1 − ν)t (cid:107)x(cid:107)  

Theorem 1 asserts that: i) TGGF recovers the solution x exactly as soon as the number of equations is
about the number of unknowns  which is theoretically order optimal. Our numerical tests demonstrate
that for the real Gaussian model  TGGF achieves a success rate of 100% when m/n is as small as 3 
which is slightly larger than the information limit of m/n = 2 (Recall that m ≥ 2n − 1 is necessary
for a unique solution); this is a signiﬁcant reduction in the sample complexity ratio  which is 5 for
TWF and 7 for WF. Surprisingly  TGGF enjoys also a success rate of over 50% when m/n is 2  which
has not yet been presented for any existing algorithm under Gaussian sampling models and thus  our
TGGF bridges the gap; see further discussion in Section 4; and  ii) TGGF converges exponentially fast.
Speciﬁcally  TGGF requires at most O(log(1/)) iterations to achieve any given solution accuracy
 > 0 (a.k.a.  dist(zt  x) ≤ (cid:107)x(cid:107))  with iteration cost O(mn). Since truncation takes time on the
order of O(m)  the computational burden of TGGF per iteration is dominated by evaluating the
generalized gradients. The latter involves two matrix-vector multiplications that are computable in
O(mn) ﬂops  namely  Azt yields ut  and AT vt the generalized gradient  where vt := ut−ψ(cid:12) ut|ut|.
Hence  the total running time of TGGF is O(mn log(1/))  which is proportional to the time taken
to read the data O(mn). The proof of Theorem 1 can be found in the supplementary material.
4 Simulated tests and conclusions

Additional numerical tests evaluating performance of the proposed scheme relative to TWF/WF
are presented in this section. For fairness  all pertinent algorithmic parameters involved in each
scheme are set to their default values. The Matlab implementations of TGGF are available at
http://www.tc.umn.edu/˜gangwang/TAF. The initial estimate was found based on 50
power iterations  and was subsequently reﬁned by T = 103 gradient-like iterations in each scheme.
Left panel in Fig. 4 presents average relative error of three initialization methods on a series of
noiseless/noisy real Gaussian problems with m/n = 6 ﬁxed  and n varying from 500 to 104 

7

Figure 4: The average relative error using: i) the spectral method [11  12]; ii) the truncated spectral
method [8]; and iii) the proposed orthogonality-promoting method on noise-free (solid) and noisy
(dotted) instances with m/n = 6  and n varying from 500/100 to 10  000/5  000 for real/complex
vectors. Left: Real Gaussian model with x ∼ N (0  In)  ai ∼ N (0  In)  and σ2 = 0.22 (cid:107)x(cid:107)2. Right:
Complex Gaussian model with x ∼ CN (0  In)  ai ∼ CN (0  In)  and σ2 = 0.22 (cid:107)x(cid:107)2.

Figure 5: Empirical success rate for WF  TWF  and TGGF with n = 1  000 and m/n varying from 1
to 7. Left: Noiseless real Gaussian model with x ∼ N (0  In) and ai ∼ N (0  In); Right: Noiseless
complex Gaussian model with x ∼ CN (0  In) and ai ∼ CN (0  In).

while those for the corresponding complex Gaussian instances are shown in the right panel. Fig. 5
compares empirical success rate of three schemes under both real and complex Gaussian models
with n = 103 and m/n varying by 1 from 1 to 7. Apparently  the proposed initialization method
returns more accurate and robust estimates than the spectral ones. Moreover  for real-valued vectors 
TGGF achieves a success rate of over 50% when m/n = 2  and guarantees perfect recovery from
about 3n measurements; while for complex-valued ones  TGGF enjoys a success rate of 95% when
m/n = 3.4  and ensures perfect recovery from about 4.5n measurements. Regarding running times 
TGGF converges slightly faster than TWF  while both are markedly faster than WF. Curves in Fig. 5
clearly corroborate the merits of TGGF over Wirtinger alternatives.
This paper developed a linear-time algorithm termed TGGF for solving random systems of quadratic
equations. TGGF builds on three key ingredients: a novel orthogonality-promoting initialization 
along with a simple yet effective truncation rule  as well as simple scalable gradient-like iterations.
Numerical tests corroborate the superior performance of TGGF over state-of-the-art solvers.

Acknowledgements

Work in this paper was supported in part by NSF grants 1500713 and 1514056.

8

Real signal dimension n5002 0004 0006 0008 00010 000Relative error0.60.70.80.911.11.2SpectralTruncated spectralOrthogonality-promotingSpectral (noisy)Truncated spectral (noisy)Orthogonality-promoting (noisy)Complex signal dimension n1001 0002 0003 0004 0005 000Relative error0.750.80.850.90.9511.051.11.151.21.25SpectralTruncated spectralOrthogonality-promotingSpectral (noisy)Truncated spectral (noisy)Orthogonality-promoting (noisy)m/n for x∈R1 0001234567Empirical success rate00.20.40.60.81WFTWFTGGFm/n for x∈C1 0001234567Empirical success rate00.20.40.60.81WFTWFTGGFReferences
[1] R. Balan  P. Casazza  and D. Edidin  “On signal reconstruction without phase ” Appl. Comput. Harmon.

Anal.  vol. 20  no. 3  pp. 345–356  May 2006.

[2] A. Conca  D. Edidin  M. Hering  and C. Vinzant  “An algebraic characterization of injectivity in phase

retrieval ” Appl. Comput. Harmon. Anal.  vol. 38  no. 2  pp. 346–356  Mar. 2015.

[3] P. M. Pardalos and S. A. Vavasis  “Quadratic programming with one negative eigenvalue is NP-hard ” J.

Global Optim.  vol. 1  no. 1  pp. 15–22  1991.

[4] H. A. Hauptman  “The phase problem of X-ray crystallography ” Rep. Prog. Phys.  vol. 54  no. 11  p. 1427 

1991.

[5] E. J. Cand`es  Y. C. Eldar  T. Strohmer  and V. Voroninski  “Phase retrieval via matrix completion ” SIAM

Rev.  vol. 57  no. 2  pp. 225–251  May 2015.

[6] H. Sahinoglou and S. D. Cabrera  “On phase retrieval of ﬁnite-length sequences using the initial time

sample ” IEEE Trans. Circuits and Syst.  vol. 38  no. 8  pp. 954–958  Aug. 1991.

[7] K. G. Murty and S. N. Kabadi  “Some NP-complete problems in quadratic and nonlinear programming ”

Math. Prog.  vol. 39  no. 2  pp. 117–129  1987.

[8] Y. Chen and E. J. Cand`es  “Solving random quadratic systems of equations is nearly as easy as solving

linear systems ” Comm. Pure Appl. Math.  2016 (to appear).

[9] R. W. Gerchberg and W. O. Saxton  “A practical algorithm for the determination of phase from image and

diffraction ” Optik  vol. 35  pp. 237–246  Nov. 1972.

[10] J. Fienup  “Phase retrieval algorithms: A comparison ” Appl. Opt.  vol. 21  no. 15  pp. 2758–2769  1982.
[11] P. Netrapalli  P. Jain  and S. Sanghavi  “Phase retrieval using alternating minimization ” IEEE Trans. Signal

Process.  vol. 63  no. 18  pp. 4814–4826  Sept. 2015.

[12] E. J. Cand`es  X. Li  and M. Soltanolkotabi  “Phase retrieval via Wirtinger ﬂow: Theory and algorithms ”

IEEE Trans. Inf. Theory  vol. 61  no. 4  pp. 1985–2007  Apr. 2015.

[13] J. Sun  Q. Qu  and J. Wright  “A geometric analysis of phase retrieval ” arXiv:1602.06664  2016.

[14] E. J. Cand`es  T. Strohmer  and V. Voroninski  “PhaseLift: Exact and stable signal recovery from magnitude
measurements via convex programming ” Appl. Comput. Harmon. Anal.  vol. 66  no. 8  pp. 1241–1274 
Nov. 2013.

[15] I. Waldspurger  A. d’Aspremont  and S. Mallat  “Phase recovery  maxcut and complex semideﬁnite

programming ” Math. Prog.  vol. 149  no. 1-2  pp. 47–81  2015.

[16] E. J. Cand`es and X. Li  “Solving quadratic equations via PhaseLift when there are about as many equations

as unknowns ” Found. Comput. Math.  vol. 14  no. 5  pp. 1017–1026  2014.

[17] G. Wang  D. Berberidis  V. Kekatos  and G. B. Giannakis  “Online reconstruction from big data via
compressive censoring ” in IEEE Global Conf. Signal and Inf. Process.  Atlanta  GA  2014  pp. 326–330.

[18] D. K. Berberidis  V. Kekatos  G. Wang  and G. B. Giannakis  “Adaptive censoring for large-scale regres-
sions ” in IEEE Intl. Conf. Acoustics  Speech and Signal Process.  South Brisbane  QLD  Australia  2015 
pp. 5475–5479.

[19] L.-H. Yeh  J. Dong  J. Zhong  L. Tian  M. Chen  G. Tang  M. Soltanolkotabi  and L. Waller  “Experimental
robustness of Fourier ptychography phase retrieval algorithms ” Opt. Express  vol. 23  no. 26  pp. 33 214–
33 240  Dec. 2015.

[20] N. Z. Shor  K. C. Kiwiel  and A. Ruszcay`nski  Minimization Methods for Non-differentiable Functions.

Springer-Verlag New York  Inc.  1985.

[21] F. H. Clarke  Optimization and Nonsmooth Analysis. SIAM  1990  vol. 5.
[22] ——  “Generalized gradients and applications ” T. Am. Math. Soc.  vol. 205  pp. 247–262  1975.

[23] P. Chen  A. Fannjiang  and G.-R. Liu  “Phase retrieval with one or two diffraction patterns by alternating

projections of the null vector ” arXiv:1510.07379v2  2015.

[24] T. Cai  J. Fan  and T. Jiang  “Distributions of angles in random packing on spheres ” J. Mach. Learn. Res. 

vol. 14  no. 1  pp. 1837–1864  Jan. 2013.

[25] R. Vershynin  “Introduction to the non-asymptotic analysis of random matrices ” arXiv:1011.3027  2010.

9

,Gang Wang
Georgios Giannakis