2015,Empirical Localization of Homogeneous Divergences on Discrete Sample Spaces,In this paper  we propose a novel parameter estimator for probabilistic models on discrete space. The proposed estimator is derived from minimization of homogeneous divergence and can be constructed without calculation of the normalization constant  which is frequently infeasible for models in the discrete space. We investigate statistical properties of the proposed estimator such as consistency and asymptotic normality  and reveal a relationship with the alpha-divergence. Small experiments show that the proposed estimator attains comparable performance to the MLE with drastically lower computational cost.,Empirical Localization of Homogeneous Divergences

on Discrete Sample Spaces

Takashi Takenouchi

Department of Complex and Intelligent Systems

Future University Hakodate

116-2 Kamedanakano  Hakodate  Hokkaido  040-8655  Japan

ttakashi@fun.ac.jp

Department of Computer Science and Mathematical Informatics

Takafumi Kanamori

Nagoya University

Furocho  Chikusaku  Nagoya 464-8601  Japan

kanamori@is.nagoya-u.ac.jp

Abstract

In this paper  we propose a novel parameter estimator for probabilistic models on
discrete space. The proposed estimator is derived from minimization of homoge-
neous divergence and can be constructed without calculation of the normalization
constant  which is frequently infeasible for models in the discrete space. We in-
vestigate statistical properties of the proposed estimator such as consistency and
asymptotic normality  and reveal a relationship with the information geometry.
Some experiments show that the proposed estimator attains comparable perfor-
mance to the maximum likelihood estimator with drastically lower computational
cost.

1 Introduction

Parameter estimation of probabilistic models on discrete space is a popular and important issue in
the ﬁelds of machine learning and pattern recognition. For example  the Boltzmann machine (with
hidden variables) [1] [2] [3] is a very popular probabilistic model to represent binary variables  and
attracts increasing attention in the context of Deep learning [4]. A training of the Boltzmann ma-
chine  i.e.  estimation of parameters is usually done by the maximum likelihood estimation (MLE).
The MLE for the Boltzmann machine cannot be explicitly solved and the gradient-based optimiza-
tion is frequently used. A difﬁculty of the gradient-based optimization is that the calculation of the
gradient requires calculation of a normalization constant or a partition function in each step of the op-
timization and its computational cost is sometimes exponential order. The problem of computational
cost is common to the other probabilistic models on discrete spaces and various kinds of approxi-
mation methods have been proposed to solve the difﬁculty. One approach tries to approximate the
probabilistic model by a tractable model by the mean-ﬁeld approximation  which considers a model
assuming independence of variables [5]. Another approach such as the contrastive divergence [6]
avoids the exponential time calculation by the Markov Chain Monte Carlo (MCMC) sampling.
In the literature of parameters estimation of probabilistic model for continuous variables  [7] em-
ploys a score function which is a gradient of log-density with respect to the data vector rather than
parameters. This approach makes it possible to estimate parameters without calculating the normal-
ization term by focusing on the shape of the density function. [8] extended the method to discrete
variables  which deﬁnes information of “neighbor” by contrasting probability with that of a ﬂipped

1

variable. [9] proposed a generalized local scoring rules on discrete sample spaces and [10] proposed
an approximated estimator with the Bregman divergence.
In this paper  we propose a novel parameter estimator for models on discrete space  which does not
require calculation of the normalization constant. The proposed estimator is deﬁned by minimization
of a risk function derived by an unnormalized model and the homogeneous divergence having a weak
coincidence axiom. The derived risk function is convex for various kind of models including higher
order Boltzmann machine. We investigate statistical properties of the proposed estimator such as the
consistency and reveal a relationship between the proposed estimator and the (cid:11)-divergence [11].

2 Settings
Let X be a d-dimensional vector of random variables in a discrete space X (typically f+1;(cid:0)1gd)
x2X f (x). Let M and
and a bracket ⟨f⟩ be summation of a function f (x) on X   i.e.  ⟨f⟩ =
P be a space of all non-negative ﬁnite measures on X and a subspace consisting of all probability
measures on X   respectively.

∑

M = ff (x)j⟨f⟩ < 1; f (x) (cid:21) 0g ; P = ff (x)j⟨f⟩ = 1; f (x) (cid:21) 0g :

In this paper  we focus on parameter estimation of a probabilistic model (cid:22)q(cid:18)(x) on X   written as

q(cid:18)(x)

(cid:22)q(cid:18)(x) =

(1)
where (cid:18) is an m-dimensional vector of parameters  q(cid:18)(x) is an unnormalized model in M and
Z(cid:18) = ⟨q(cid:18)⟩ is a normalization constant. A computation of the normalization constant Z(cid:18) sometimes
requires calculation of exponential order and is sometimes difﬁcult for models on the discrete space.
Note that the unnormalized model q(cid:18)(x) is not normalized and ⟨q(cid:18)⟩ =
x2X q(cid:18)(x) = 1 does not
necessarily hold. Let (cid:18)(x) be a function on X and throughout the paper  we assume without loss
of generality that the unnormalized model q(cid:18)(x) can be written as

∑

Z(cid:18)

q(cid:18)(x) = exp( (cid:18)(x)):

(2)
Remark 1. By setting (cid:18)(x) as (cid:18)(x) (cid:0) log Z(cid:18)  the normalized model (1) can be written as (2).
Example 1. The Bernoulli distribution on X = f+1;(cid:0)1g is a simplest example of the probabilistic
model (1) with the function (cid:18)(x) = (cid:18)x.
Example 2. With a function (cid:18);k(x) = (x1; : : : ; xd; x1x2; : : : ; xd(cid:0)1xd; x1x2x3; : : :)(cid:18)  we can de-
ﬁne a k-th order Boltzmann machine [1  12].
Example 3. Let xo 2 f+1;(cid:0)1gd1 and xh 2 f+1;(cid:0)1gd2 be an observed vector and hidden vector 
respectively  and x =
catenated vector. A function h;(cid:18)(xo) for the Boltzmann machine with hidden variables is written
as

) 2 f+1;(cid:0)1gd1+d2 where T indicates the transpose  be a con-

o ; xT
h

(

xT

∑

 h;(cid:18)(xo) = log

exp( (cid:18);2(x));

xh

(3)

xh

is the summation with respect to the hidden variable xh.

where
Let us assume that a dataset D = fxign
i=1 generated by an underlying distribution p(x)  is given and
Z be a set of all patterns which appear in the dataset D. An empirical distribution ~p(x) associated
with the dataset D is deﬁned as

{

~p(x) =

nx
n
0

x 2 Z;
otherwise;

∑

∑

n

i=1 I(xi = x) is a number of pattern x appeared in the dataset D.

where nx =
Deﬁnition 1. For the unnormalized model (2) and distributions p(x) and ~p(x) in P   probability
functions r(cid:11);(cid:18)(x) and ~r(cid:11);(cid:18)(x) on X are deﬁned by

⟨

⟩

p(x)(cid:11)q(cid:18)(x)1(cid:0)(cid:11)

p(cid:11)q1(cid:0)(cid:11)

(cid:18)

r(cid:11);(cid:18)(x) =

⟨

~p(x)(cid:11)q(cid:18)(x)1(cid:0)(cid:11)

~p(cid:11)q1(cid:0)(cid:11)

(cid:18)

:

⟩

; ~r(cid:11);(cid:18)(x) =

2

∑

The distribution r(cid:11);(cid:18) (~r(cid:11);(cid:18)) is an e-mixture model of the unnormalized model (2) and p(x) (~p(x))
with ratio (cid:11) [11].
Remark 2. We observe that r0;(cid:18)(x) = ~r0;(cid:18)(x) = (cid:22)q(cid:18)(x)  r1;(cid:18)(x) = p(x)  ~r1;(cid:18)(x) = ~p(x). Also if
p(x) = (cid:22)q(cid:18)0 (x)  r(cid:11);(cid:18)0 (x) = (cid:22)q(cid:18)0(x) holds for an arbitrary (cid:11).

n

To estimate the parameter (cid:18) of probabilistic model (cid:22)q(cid:18)  the MLE deﬁned by ^(cid:18)mle = argmax(cid:18) L((cid:18)) is
frequently employed  where L((cid:18)) =
i=1 log (cid:22)q(cid:18)(xi) is the log-likelihood of the parameter (cid:18) with
the model (cid:22)q(cid:18). Though the MLE is asymptotically consistent and efﬁcient estimator  a main drawback
of the MLE is that computational cost for probabilistic models on the discrete space sometimes
becomes exponential. Unfortunately the MLE does not have an explicit solution in general  the
⟩(cid:0)
estimation of the parameter can be done by the gradient based optimization with a gradient ⟨~p 
⟨(cid:22)q(cid:18) 
@(cid:18) . While the ﬁrst term can be easily calculated  the second
term includes calculation of the normalization term Z(cid:18)  which requires 2d times summation for
X = f+1;(cid:0)1gd and is not feasible when d is large.

⟩ of log-likelihood  where 

′
(cid:18) = @ (cid:18)

′
(cid:18)

′
(cid:18)

3 Homogeneous Divergences for Statistical Inference

Divergences are an extension of the squared distance and are often used in statistical inference. A
formal deﬁnition of the divergence D(f; g) is a non-negative valued function on M(cid:2)M or on P(cid:2)P
such that D(f; f ) = 0 holds for arbitrary f. Many popular divergences such as the Kullback-Leilber
(KL) divergence deﬁned on P (cid:2) P enjoy the coincidence axiom  i.e.  D(f; g) = 0 leads to f = g.
The parameter in the statistical model (cid:22)q(cid:18) is estimated by minimizing the divergence D(~p; (cid:22)q(cid:18))  with
respect to (cid:18).
In the statistical inference using unnormalized models  the coincidence axiom of the divergence is
not suitable  since the probability and the unnormalized model do not exactly match in general. Our
purpose is to estimate the underlying distribution up to a constant factor using unnormalized models.
Hence  divergences having the property of the weak coincidence axiom  i.e.  D(f; g) = 0 if and only
if g = cf for some c > 0  are good candidate. As a class of divergences with the weak coincidence
axiom  we focus on homogeneous divergences that satisfy the equality D(f; g) = D(f; cg) for any
f; g 2 M and any c > 0.
A representative of homogeneous divergences is the pseudo-spherical (PS) divergence [13]  or in
other words  (cid:13)-divergence [14]  that is deﬁned from the H¨older inequality. Assume that (cid:13) is a
positive constant. For all non-negative functions f; g in M  the H¨older inequality

⟨
g(cid:13)+1
holds. The inequality becomes an equality if and only if f and g are linearly dependent. The PS-
divergence D(cid:13)(f; g) for f; g 2 M is deﬁned by

⟩ (cid:13)
(cid:13)+1 (cid:0) ⟨f g(cid:13)⟩ (cid:21) 0

⟩ 1

f (cid:13)+1

⟨

(cid:13)+1

⟨

⟩

⟨

⟩ (cid:0) log ⟨f g(cid:13)⟩ ;

D(cid:13)(f; g) =

1

1 + (cid:13)

log

f (cid:13)+1

+

(cid:13)

1 + (cid:13)

log

g(cid:13)+1

(cid:13) > 0:

(4)

The PS divergence is homogeneous  and the H¨older inequality ensures the non-negativity and the
weak coincidence axiom of the PS-divergence. One can conﬁrm that the scaled PS-divergence 
(cid:0)1D(cid:13)  converges to the extended KL-divergence deﬁned on M(cid:2)M  as (cid:13) ! 0. The PS-divergence
(cid:13)
is used to obtain a robust estimator [14].
As shown in (4)  the standard PS-divergence from the empirical distribution ~p to the unnormalized
⟩  that may be infeasible in our setup. To circumvent
model q(cid:18) requires the computation of ⟨q(cid:13)+1
such an expensive computation  we employ a trick and substitute a model ~pq(cid:18) localized by the
empirical distribution for q(cid:18)  which makes it possible to replace the total sum in ⟨q(cid:13)+1
⟩ with the
empirical mean. More precisely  let us consider the PS-divergence from f = (p(cid:11)q1(cid:0)(cid:11))
1+(cid:13) to
1+(cid:13) for the probability distribution p 2 P and the unnormalized model q 2 M 
q1(cid:0)(cid:11)
g = (p(cid:11)
′ are two distinct real numbers. Then  the divergence vanishes if and only if p(cid:11)q1(cid:0)(cid:11) /
where (cid:11); (cid:11)
  i.e.  q / p. We deﬁne the localized PS-divergence S(cid:11);(cid:11)′;(cid:13)(p; q) by
q1(cid:0)(cid:11)
p(cid:11)
q1(cid:0)(cid:11)
S(cid:11);(cid:11)′;(cid:13)(p; q) = D(cid:13)((p(cid:11)q1(cid:0)(cid:11))1=(1+(cid:13)); (p(cid:11)
(cid:13)

⟨

⟩

⟩

⟨

1

)

(cid:18)

(cid:18)

′

′

′

1

′

′

′

1

′⟩ (cid:0) log

p(cid:12)q1(cid:0)(cid:12)

;

(5)

)1=(1+(cid:13)))
log⟨p(cid:11)
q1(cid:0)(cid:11)

′

p(cid:11)q1(cid:0)(cid:11)

+

log

=

1 + (cid:13)

1 + (cid:13)

3

⟨

⟩

∑

(

)(cid:11)

′

)=(1 + (cid:13)). Substituting the empirical distribution ~p into p  the total sum over
where (cid:12) = ((cid:11) + (cid:13)(cid:11)
X is replaced with a variant of the empirical mean such as
q1(cid:0)(cid:11)(x)
′
for a non-zero real number (cid:11). Since S(cid:11);(cid:11)′;(cid:13)(p; q) = S(cid:11)′;(cid:11);1=(cid:13)(p; q) holds  we can assume (cid:11) > (cid:11)
without loss of generality. In summary  the conditions of the real parameters (cid:11); (cid:11)
; (cid:13) are given by

~p(cid:11)q1(cid:0)(cid:11)

x2Z
′

nx
n

=

′

; (cid:11) ̸= 0; (cid:11)

′ ̸= 0; (cid:11) + (cid:13)(cid:11)

′ ̸= 0;

(cid:13) > 0; (cid:11) > (cid:11)
where the last condition denotes (cid:12) ̸= 0.
Let us consider another aspect of the computational issue about the localized PS-divergence. For the
probability distribution p and the unnormalized exponential model q(cid:18)  we show that the localized
′ and (cid:13) are properly chosen.
PS-divergence S(cid:11);(cid:11)′;(cid:13)(p; q(cid:18)) is convex in (cid:18)  when the parameters (cid:11); (cid:11)
Theorem 1. Let p 2 P be any probability distribution  and let q(cid:18) be the unnormalized exponential
model q(cid:18)(x) = exp((cid:18)T ϕ(x))  where ϕ(x) is any vector-valued function corresponding to the sufﬁ-
cient statistic in the (normalized) exponential model (cid:22)q(cid:18). For a given (cid:12)  the localized PS-divergence
S(cid:11);(cid:11)′;(cid:13)(p; q(cid:18)) is convex in (cid:18) for any (cid:11); (cid:11)
)=(1 + (cid:13)) if and only if (cid:12) = 1.
Proof. After some calculation  we have @2 log⟨p(cid:11)q1(cid:0)(cid:11)
= (1 (cid:0) (cid:11))2Vr(cid:11);(cid:18) [ϕ]  where Vr(cid:11);(cid:18) [ϕ] is the
covariance matrix of ϕ(x) under the probability r(cid:11);(cid:18)(x). Thus  the Hessian matrix of S(cid:11);(cid:11)′;(cid:13)(p; q(cid:18))
is written as
@2

; (cid:13) satisfying (cid:12) = ((cid:11) + (cid:13)(cid:11)

(cid:18)
@(cid:18)@(cid:18)T

)2

⟩

′

′

′

Vr(cid:11)′;(cid:18) [ϕ] (cid:0) (1 (cid:0) (cid:12))2Vr(cid:12);(cid:18) [ϕ]:

@(cid:18)@(cid:18)T S(cid:11);(cid:11)′;(cid:13)(p; q(cid:18)) =

(1 (cid:0) (cid:11))2
1 + (cid:13)

Vr(cid:11);(cid:18) [ϕ] +

(cid:13)(1 (cid:0) (cid:11)
1 + (cid:13)

The Hessian matrix is non-negative deﬁnite if (cid:12) = 1. The converse direction is deferred to the
supplementary material.

Up to a constant factor  the localized PS-divergence with (cid:12) = 1 characterized by Theorem 1 is
denotes as S(cid:11);(cid:11)′(p; q) that is deﬁned by
1
(cid:11) (cid:0) 1
′ ̸= 0. The parameter (cid:11)

′ can be negative if p is positive on X . Clearly  S(cid:11);(cid:11)′ (p; q)

for (cid:11) > 1 > (cid:11)
satisﬁes the homogeneity and the weak coincidence axiom as well as S(cid:11);(cid:11)′;(cid:13)(p; q).

1 (cid:0) (cid:11)′ log⟨p(cid:11)

S(cid:11);(cid:11)′ (p; q) =

p(cid:11)q1(cid:0)(cid:11)

q1(cid:0)(cid:11)

log

′⟩

+

1

′

⟩

⟨

4 Estimation with the localized pseudo-spherical divergence

Given the empirical distribution ~p and the unnormalized model q(cid:18)  we deﬁne a novel estimator with
the localized PS-divergence S(cid:11);(cid:11)′;(cid:13) (or S(cid:11);(cid:11)′). Though the localized PS-divergence plugged-in
the empirical distribution is not well-deﬁned when (cid:11)
< 0  we can formally deﬁne the following
estimator by restricting the domain X to the observed set of examples Z  even for negative (cid:11)

′:

′

^(cid:18) = argmin

(cid:18)

S(cid:11);(cid:11)′;(cid:13)(~p; q(cid:18))

= argmin

(cid:18)

1

1 + (cid:13)
(cid:0) log

log

∑

x2Z

(
)(cid:12)

∑
(

x2Z
nx
n

)(cid:11)

nx
n
q(cid:18)(x)1(cid:0)(cid:12):

q(cid:18)(x)1(cid:0)(cid:11) +

(cid:13)

1 + (cid:13)

log

)(cid:11)

∑

(

x2Z

nx
n

(6)

′

q(cid:18)(x)1(cid:0)(cid:11)

′

Remark 3. The summation in (6) is deﬁned on Z and then is computable even when (cid:11); (cid:11)
Also the summation includes only Z((cid:20) n) terms and its computational cost is O(n).
Proposition 1. For the unnormalized model (2)  the estimator (6) is Fisher consistent.

′

; (cid:12) < 0.

Proof. We observe

@
@(cid:18)

S(cid:11);(cid:11)′;(cid:13)((cid:22)q(cid:18)0 ; q(cid:18))

implying the Fisher consistency of ^(cid:18).

(cid:12)(cid:12)(cid:12)(cid:12)

=

(cid:18)=(cid:18)0

(
(cid:12) (cid:0) (cid:11) + (cid:13)(cid:11)
1 + (cid:13)

′

)⟨

⟩

′
(cid:18)0

(cid:22)q(cid:18)0 

= 0

4

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

Theorem 2. Let q(cid:18)(x) be the unnormalized model (2)  and (cid:18)0 be the true parameter of underlying
distribution p(x) = (cid:22)q(cid:18)0(x). Then an asymptotic distribution of the estimator (6) is written as

p
n( ^(cid:18) (cid:0) (cid:18)0) (cid:24) N (0; I((cid:18)0)

(cid:0)1)

where I((cid:18)0) = V(cid:22)q(cid:18)0

[ 

′
(cid:18)0

] is the Fisher information matrix.

Proof. We shall sketch a proof and the detailed proof is given in supplementary material. Let us
assume that the empirical distribution is written as
Note that ⟨ϵ⟩ = 0 because ~p; (cid:22)q(cid:18)0
the estimator (6) around (cid:18) = (cid:18)0 leads to

2 P. The asymptotic expansion of the equilibrium condition for

~p(x) = (cid:22)q(cid:18)0 (x) + ϵ(x):

0 =

@
@(cid:18)

=

@
@(cid:18)

S(cid:11);(cid:11)′;(cid:13)(~p; q(cid:18))

(cid:18)= ^(cid:18)

S(cid:11);(cid:11)′;(cid:13)(~p; q(cid:18))

(cid:18)=(cid:18)0
By the delta method [15]  we have

@2

+

@(cid:18)@(cid:18)T S(cid:11);(cid:11)′;(cid:13)(~p; q(cid:18))

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:18)=(cid:18)0

( ^(cid:18) (cid:0) (cid:18)0) + O(jj ^(cid:18) (cid:0) (cid:18)0jj2)
⟩

⟨

≃ (cid:0)

(cid:13)

(1 + (cid:13))2 ((cid:11) (cid:0) (cid:11)

′

)2

′
(cid:18)0

ϵ

 

@
@(cid:18)

S(cid:11);(cid:11)′;(cid:13)(~p; q(cid:18))

S(cid:11);(cid:11)′;(cid:13)(p; q(cid:18))

and from the central limit theorem  we observe that

(cid:0) @
@(cid:18)

⟩

′
(cid:18)0

ϵ

 

=

(cid:18)=(cid:18)0

⟨

p
n

n∑

(

(cid:18)=(cid:18)0

(xi) (cid:0)⟨

′
(cid:18)0

 

i=1

p
n

1
n

⟩)

′
(cid:18)0

(cid:22)q(cid:18)0 

(cid:12)(cid:12)(cid:12)(cid:12)

asymptotically follows the normal distribution with mean 0  and variance I((cid:18)0) = V(cid:22)q(cid:18)0
[ 
is known as the Fisher information matrix. Also from the law of large numbers  we have

@2

@(cid:18)@(cid:18)T S(cid:11);(cid:11)′;(cid:13)(~p; q(cid:18))

( ^(cid:18) (cid:0) (cid:18)0) ! (cid:13)

(1 + (cid:13))2 ((cid:11) (cid:0) (cid:11)

′

)2I((cid:18)0);

(cid:18)=(cid:18)0

in the limit of n ! 1. Consequently  we observe that (2).
Remark 4. The asymptotic distribution of (6) is equal to that of the MLE  and its variance does not
depend on (cid:11); (cid:11)
Remark 5. As shown in Remark 1  the normalized model (1) is a special case of the unnormalized
model (2) and then Theorem 2 holds for the normalized model.

; (cid:13).

′

′
(cid:18)0

]  which

5 Characterization of localized pseudo-spherical divergence S(cid:11);(cid:11)′

Throughout this section  we assume that (cid:12) = 1 holds and investigate properties of the localized PS-
′ and characterization of the localized
divergence S(cid:11);(cid:11)′. We discuss inﬂuence of selection of (cid:11); (cid:11)
PS-divergence S(cid:11);(cid:11)′ in the following subsections.

5.1

Inﬂuence of selection of (cid:11); (cid:11)

′

We investigate inﬂuence of selection of (cid:11); (cid:11)
the estimating equation. The estimator ^(cid:18) derived from S(cid:11);(cid:11)′ satisﬁes
′
^(cid:18)

@S(cid:11);(cid:11)′(~p; q(cid:18))

~r(cid:11)′; ^(cid:18) 

~r(cid:11); ^(cid:18) 

(cid:0)

′
^(cid:18)

⟩

⟨

@(cid:18)

′ for the localized PS-divergence S(cid:11);(cid:11)′ with a view of

which is a moment matching with respect to two distributions ~r(cid:11);(cid:18) and ~r(cid:11)′;(cid:18) ((cid:11); (cid:11)
other hand  the estimating equation of the MLE is written as

/

(cid:12)(cid:12)(cid:12)(cid:12)
⟨
⟩ (cid:0) ⟨(cid:22)q(cid:18)mle (cid:18)mle

(cid:18)= ^(cid:18)

⟨

(7)
′ ̸= 0; 1). On the
⟩

= 0;

(8)

′
(cid:18)mle

(cid:12)(cid:12)(cid:12)(cid:12)

/⟨

@L((cid:18))

@(cid:18)

(cid:18)=(cid:18)mle

′
(cid:18)mle

~p 

which is a moment matching with respect to the empirical distribution ~p = ~r1;(cid:18)mle and the
normalized model (cid:22)q(cid:18) = ~r0;(cid:18)mle. While the localized PS-divergence S(cid:11);(cid:11)′ is not deﬁned with
) = (0; 1)  comparison of (7) with (8) implies that behavior the estimator ^(cid:18) becomes simi-
((cid:11); (cid:11)
lar to that of the MLE in the limit of (cid:11) ! 1 and (cid:11)

′ ! 0.

′

= 0:

⟩
⟩ (cid:0)⟨

⟩ =

~r1;(cid:18)mle 

′
(cid:18)mle

~r0;(cid:18)mle 

5

5.2 Relationship with the (cid:11)-divergence
The (cid:11)-divergence between two positive measures f; g 2 M is deﬁned as
(cid:11)f + (1 (cid:0) (cid:11))g (cid:0) f (cid:11)g1(cid:0)(cid:11)

D(cid:11)(f; g) =

⟨

1

⟩

;

(cid:11)(1 (cid:0) (cid:11))

where (cid:11) is a real number. Note that D(cid:11)(f; g) (cid:21) 0 and 0 if and only if f = g  and the (cid:11)-divergence
reduces to KL(f; g) and KL(g; f ) in the limit of (cid:11) ! 1 and 0  respectively.
Remark 6. An estimator deﬁned by minimizing (cid:11)-divergence D(cid:11)(~p; (cid:22)q(cid:18)) between the empirical
distribution and normalized model  satisﬁes

and requires calculation proportional to jXj which is infeasible. Also the same hold for an estimator
deﬁned by minimizing (cid:11)-divergence D(cid:11)(~p; q(cid:18)) between the empirical distribution and unnormalized
model  satisfying @D(cid:11)( ~p;q(cid:18))
′

̸= 0; 1 and consider a trick to cancel out the term ⟨g⟩ by mixing two

(cid:0) ~p(cid:11)q1(cid:0)(cid:11)

= 0.

′
(cid:18)

@(cid:18)

(cid:18)

Here  we assume that (cid:11); (cid:11)
(cid:11)-divergences as follows.

⟩
⟩)

= 0

′
(cid:18)

(cid:0) ⟨(cid:22)q(cid:18) 
⟩

@D(cid:11)(~p; (cid:22)q(cid:18))

~p(cid:11)q1(cid:0)(cid:11)

(cid:18)

′
(cid:18)

( 

/⟨

@(cid:18)

/⟨
((cid:0)(cid:11)

(1 (cid:0) (cid:11))q(cid:18) 
)

′

)

⟩

⟨(

D(cid:11);(cid:11)′ (f; g) =D(cid:11)(f; g) +
(cid:0)

=

1
1 (cid:0) (cid:11)

D(cid:11)′(f; g)
f (cid:0)

′

(cid:11)
(cid:11)(1 (cid:0) (cid:11)′)

(cid:11)

1

(cid:11)(1 (cid:0) (cid:11))

′

′

:

1

f (cid:11)

g1(cid:0)(cid:11)

(cid:11)(1 (cid:0) (cid:11)′)

f (cid:11)g1(cid:0)(cid:11) +
< 0 holds  i.e.  D(cid:11);(cid:11)′(f; g) (cid:21) 0 and
′ for
(

}

)(cid:11)

Remark 7. D(cid:11);(cid:11)′(f; g) (cid:21) 0 is divergence when (cid:11)(cid:11)
D(cid:11);(cid:11)′ (f; g) = 0 if and only if f = g. Without loss of generality  we assume (cid:11) > 0 > (cid:11)
D(cid:11);(cid:11)′.

′

{

∑

(

)(cid:11)

′

(cid:18)

1

min

x2Z

Firstly  we consider an estimator deﬁned by the minmizer of
′ (cid:0) 1
1 (cid:0) (cid:11)

q(cid:18)(x)1(cid:0)(cid:11)

1 (cid:0) (cid:11)′

:
Note that the summation in (9) includes only Z((cid:20) n) terms. We remark the following.
Remark 8. Let (cid:22)q(cid:18)0(x) be the underlying distribution and q(cid:18)(x) be the unnormalized model (2).
Then an estimator deﬁned by minimizing D(cid:11);(cid:11)′ ((cid:22)q(cid:18)0 ; q(cid:18)) is not in general Fisher consistent  i.e. 
@D(cid:11);(cid:11)′((cid:22)q(cid:18)0 ; q(cid:18))

)⟨

q(cid:18)(x)1(cid:0)(cid:11)

nx
n

nx
n

⟩

⟨

(

(9)

′

′

/

(cid:18)0 q1(cid:0)(cid:11)
(cid:22)q(cid:11)

(cid:18)0

′
(cid:18)0

 

(cid:0) (cid:22)q(cid:11)

(cid:18)0q1(cid:0)(cid:11)

(cid:18)0

′
(cid:18)0

 

⟩ ̸= 0:

⟨q(cid:18)0

⟩(cid:0)(cid:11)

′ (cid:0) ⟨q(cid:18)0

⟩(cid:0)(cid:11)

=

′
(cid:18)0

q(cid:18)0 

(cid:12)(cid:12)(cid:12)(cid:12)

@(cid:18)

(cid:18)=(cid:18)0

This remark shows that an estimator associated with D(cid:11);(cid:11)′ (~p; q(cid:18)) does not have suitable properties
such as (asymptotic) unbiasedness and consistency while required computational cost is drastically
reduced. Intuitively  this is because the (mixture of) (cid:11)-divergence satisﬁes the coincidence axiom.
To overcome this drawback  we consider the following minimization problem for estimation of the
parameter (cid:18) of model (cid:22)q(cid:18)(x).

( ^(cid:18); ^r) = argmin

D(cid:11);(cid:11)′(~p; rq(cid:18))

where r is a constant corresponding to an inverse of the normalization term Z(cid:18) = ⟨q(cid:18)⟩.
Proposition 2. Let q(cid:18)(x) be the unnormalized model (2). For (cid:11) > 1 and 0 > (cid:11)
of D(cid:11);(cid:11)′ (~p; rq(cid:18)) is equivalent to the minimization of

(cid:18);r

′  the minimization

Proof. For a given (cid:18)  we observe that

^r(cid:18) = argmin

r

D(cid:11);(cid:11)′(~p; rq(cid:18)) =

6

S(cid:11);(cid:11)′ (~p; q(cid:18)):

( ⟨
⟨

~p(cid:11)q1(cid:0)(cid:11)
q1(cid:0)(cid:11)′
~p(cid:11)′

(cid:18)

(cid:18)

⟩) 1
⟩

(cid:11)(cid:0)(cid:11)′

:

(10)

Note that computation of (10) requires only sample order O(n) calculation. By plugging (10) into
D(cid:11);(cid:11)′ (~p; rq(cid:18))  we observe

^(cid:18) = argmin

(cid:18)

D(cid:11);(cid:11)′(~p; ^r(cid:18)q(cid:18)) = argmin

(cid:18)

S(cid:11);(cid:11)′ (~p; q(cid:18)):

(11)

′

If (cid:11) > 1 and (cid:11)
< 0 hold  the estimator (11) is equivalent to the estimator associated with the
localized PS-divergence S(cid:11);(cid:11)′  implying that S(cid:11);(cid:11)′ is characterized by the mixture of (cid:11)-divergences.
Remark 9. From a viewpoint of the information geometry [11]  a metric (information geometrical
structure) induced by the (cid:11)-divergence is the Fisher metric induced by the KL-divergence. This im-
plies that the estimation based on the (mixture of) (cid:11)-divergence is Fisher efﬁcient and is an intuitive
explanation of the Theorem 2. The localized PS divergence S(cid:11);(cid:11)′;(cid:13) and S(cid:11);(cid:11)′ with (cid:11)(cid:11)
> 0 can be
interpreted as an extension of the (cid:11)-divergence  which preserves Fisher efﬁciency.

′

6 Experiments

We especially focus on a setting of (cid:12) = 1  i.e.  convexity of the risk function with the unnormalized
model exp((cid:18)T ϕ(x)) holds (Theorem 1) and examined performance of the proposed estimator.

6.1 Fully visible Boltzmann machine

′

′

(cid:3)

In the ﬁrst experiment  we compared the proposed estimator with parameter settings ((cid:11); (cid:11)
) =
(1:01; 0:01); (1:01;(cid:0)0:01); (2;(cid:0)1)  with the MLE and the ratio matching method [8]. Note that
the ratio matching method also does not require calculation of the normalization constant  and the
) = (1:01;(cid:6)0:01) may behave like the MLE as discussed in section
proposed method with ((cid:11); (cid:11)
5.1.
All methods were optimized with the optim function in R language [16]. The dimension d of input
was set to 10 and the synthetic dataset was randomly generated from the second order Boltzmann
(cid:3) (cid:24) N (0; I). We repeated comparison 50 times and
machine (Example 2) with a parameter (cid:18)
observed averaged performance. Figure 1 (a) shows median of the root mean square errors (RMSEs)
and ^(cid:18) of each method over 50 trials  against the number n of examples. We observe that
between (cid:18)
the proposed estimator works well and is superior to the ratio matching method. In this experiment 
the MLE outperforms the proposed method contrary to the prediction of Theorem 2. This is because
observed patterns were only a small portion of all possible patterns  as shown in Figure 1 (b). Even
in such a case  the MLE can take all possible patterns (210 = 1024) into account through the
normalization term log Z(cid:18) ≃ Const + 1
jj(cid:18)jj2 that works like a regularizer. On the other hand  the
proposed method genuinely uses only the observed examples  and the asymptotic analysis would not
be relevant in this case. Figure 1 (c) shows median of computational time of each method against
n. The computational time of the MLE does not vary against n because the computational cost is
dominated by the calculation of the normalization constant. Both the proposed estimator and the
ratio matching method are signiﬁcantly faster than the MLE  and the ratio matching method is faster
than the proposed estimator while the RMSE of the proposed estimator is less than that of the ratio
matching.

2

6.2 Boltzmann machine with hidden variables

′

In this subsection  we applied the proposed estimator for the Boltzmann machine with hidden vari-
ables whose associated function is written as (3). The proposed estimator with parameter settings
) = (1:01; 0:01); (1:01;(cid:0)0:01); (2;(cid:0)1) was compared with the MLE. The dimension d1 of
((cid:11); (cid:11)
(cid:3)
observed variables was ﬁxed to 10 and d2 of hidden variables was set to 2  and the parameter (cid:18)
(cid:3) (cid:24) N (0; I) including parameters corresponding to hidden variables. Note
was generated as (cid:18)
that the Boltzmann machine with hidden variables is not identiﬁable and different values of the pa-
rameter do not necessarily generate different probability distributions  implying that estimators are
inﬂuenced by local minimums. Then we measured performance of each estimator by the averaged

7

∑

n

Figure 1:
(a) Median of RMSEs of each method against n  in log scale. (b) Box-whisker plot of
number jZj of unique patterns in the dataset D against n. (c) Median of computational time of each
method against n  in log scale.

i=1 log (cid:22)q ^(cid:18)(xi) rather than the RMSE. An initial value of the parameter was set
log-likelihood 1
by N (0; I) and commonly used by all methods. We repeated the comparison 50 times and ob-
n
served the averaged performance. Figure 2 (a) shows median of averaged log-likelihoods of each
method over 50 trials  against the number n of example. We observe that the proposed estimator is
comparable with the MLE when the number n of examples becomes large. Note that the averaged
log-likelihood of MLE once decreases when n is samll  and this is due to overﬁtting of the model.
Figure 2 (b) shows median of averaged log-likelihoods of each method for test dataset consists of
10000 examples  over 50 trials. Figure 2 (c) shows median of computational time of each method
against n  and we observe that the proposed estimator is signiﬁcantly faster than the MLE.

Figure 2: (a) Median of averaged log-likelihoods of each method against n. (b) Median of averaged
log-likelihoods of each method calculated for test dataset against n. (c) Median of computational
time of each method against n  in log scale.

7 Conclusions

We proposed a novel estimator for probabilistic model on discrete space  based on the unnormal-
ized model and the localized PS-divergence which has the homogeneous property. The proposed
estimator can be constructed without calculation of the normalization constant and is asymptotically
efﬁcient  which is the most important virtue of the proposed estimator. Numerical experiments show
that the proposed estimator is comparable to the MLE and required computational cost is drastically
reduced.

8

05000100001500020000250000.10.20.51.0nRMSEMLERatio matchinga1=1.01 a2=0.01a1=1.01 a2=−0.01a1=2 a2=−11002004008001600320064001280025600050100150200250300nNumber |Z| of unique patterns050001000015000200002500025102050100200500nTime[s]MLERatio matchinga1=1.01 a2=0.01a1=1.01 a2=−0.01a1=2 a2=−10500010000150002000025000−15−10−5nAveraged Log likelihoodMLEa1=1.01 a2=0.01a1=1.01 a2=−0.01a1=2 a2=−10500010000150002000025000−15−10−5nAveraged Log likelihoodMLEa1=1.01 a2=0.01a1=1.01 a2=−0.01a1=2 a2=−1050001000015000200002500051020501002005001000nTime[s]MLEa1=1.01 a2=0.01a1=1.01 a2=−0.01a1=2 a2=−1References
[1] Hinton  G. E. & Sejnowski  T. J. (1986) Learning and relearning in boltzmann machines. MIT

Press  Cambridge  Mass  1:282–317.

[2] Ackley  D. H.  Hinton  G. E. & Sejnowski  T. J. (1985) A learning algorithm for boltzmann

machines. Cognitive Science  9(1):147–169.

[3] Amari  S.  Kurata  K. & Nagaoka  H. (1992) Information geometry of Boltzmann machines.

In IEEE Transactions on Neural Networks  3: 260–271.

[4] Hinton  G. E. & Salakhutdinov  R. R. (2012) A better way to pretrain deep boltzmann ma-
chines. In Advances in Neural Information Processing Systems  pp. 2447–2455 Cambridge 
MA: MIT Press.

[5] Opper  M. & Saad  D. (2001) Advanced Mean Field Methods: Theory and Practice. MIT

Press  Cambridge  MA.

[6] Hinton  G.E. (2002) Training Products of Experts by Minimizing Contrastive Divergence.

Neural Computation  14(8):1771–1800.

[7] Hyv¨arinen  A. (2005) Estimation of non-normalized statistical models by score matching.

Journal of Machine Learning Research  6:695–708.

[8] Hyv¨arinen  A. (2007) Some extensions of score matching. Computational statistics & data

analysis  51(5):2499–2512.

[9] Dawid  A. P.  Lauritzen  S. & Parry  M. (2012) Proper local scoring rules on discrete sample

spaces. The Annals of Statistics  40(1):593–608.

[10] Gutmann  M. & Hirayama  H. (2012) Bregman divergence as general framework to estimate

unnormalized statistical models. arXiv preprint arXiv:1202.3727.

[11] Amari  S & Nagaoka  H. (2000) Methods of Information Geometry  volume 191 of Transla-

tions of Mathematical Monographs. Oxford University Press.

[12] Sejnowski  T. J. (1986) Higher-order boltzmann machines. In American Institute of Physics

Conference Series  151:398–403.

[13] Good  I. J. (1971) Comment on “measuring information and uncertainty ” by R. J. Buehler.
In Godambe  V. P. & Sprott  D. A. editors  Foundations of Statistical Inference  pp. 337–339 
Toronto: Holt  Rinehart and Winston.

[14] Fujisawa  H. & Eguchi  S. (2008) Robust parameter estimation with a small bias against heavy

contamination. Journal of Multivariate Analysis  99(9):2053–2081.

[15] Van der Vaart  A. W. (1998) Asymptotic Statistics. Cambridge University Press.
[16] R Core Team. (2013) R: A Language and Environment for Statistical Computing. R Foundation

for Statistical Computing  Vienna  Austria.

9

,Lijun Zhang
Mehrdad Mahdavi
Rong Jin
Takashi Takenouchi
Takafumi Kanamori