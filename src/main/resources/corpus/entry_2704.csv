2019,Intrinsically Efficient  Stable  and Bounded Off-Policy Evaluation for Reinforcement Learning,Off-policy evaluation (OPE) in both contextual bandits and reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration  which is often costly or otherwise infeasible. The problem's importance has attracted many proposed solutions  including importance sampling (IS)  self-normalized IS (SNIS)  and doubly robust (DR) estimates. DR and its variants ensure semiparametric local efficiency if Q-functions are well-specified  but if they are not they can be worse than both IS and SNIS. It also does not enjoy SNIS's inherent stability and boundedness. We propose new estimators for OPE based on empirical likelihood that are always more efficient than IS  SNIS  and DR and satisfy the same stability and boundedness properties as SNIS. On the way  we categorize various properties and classify existing estimators by them. Besides the theoretical guarantees  empirical studies suggest the new estimators provide advantages.,Intrinsically Efﬁcient  Stable  and Bounded

Off-Policy Evaluation for Reinforcement Learning

Nathan Kallus
Cornell University

New York  NY

kallus@cornell.edu

Masatoshi Uehara ∗
Harvard University
Cambrdige  MA

uehara_m@g.harvard.edu

Abstract

Off-policy evaluation (OPE) in both contextual bandits and reinforcement learning
allows one to evaluate novel decision policies without needing to conduct explo-
ration  which is often costly or otherwise infeasible. The problem’s importance
has attracted many proposed solutions  including importance sampling (IS)  self-
normalized IS (SNIS)  and doubly robust (DR) estimates. DR and its variants
ensure semiparametric local efﬁciency if Q-functions are well-speciﬁed  but if they
are not they can be worse than both IS and SNIS. It also does not enjoy SNIS’s
inherent stability and boundedness. We propose new estimators for OPE based
on empirical likelihood that are always more efﬁcient than IS  SNIS  and DR and
satisfy the same stability and boundedness properties as SNIS. On the way  we
categorize various properties and classify existing estimators by them. Besides
the theoretical guarantees  empirical studies suggest the new estimators provide
advantages.

1

Introduction

Off-policy evaluation (OPE) is the problem of evaluating a given policy (evaluation policy) using data
generated by the log of another policy (behavior policy). OPE is a key problem in both reinforcement
learning (RL) [7  13–15  17  23  32] and contextual bandits (CB) [5  19  28] and it ﬁnds applications
as varied as healthcare [18] and education [16].
Methods for OPE can be roughly categorized into three types. The ﬁrst approach is the direct
method (DM)  wherein we directly estimate the Q-function using regression and use it to directly
estimate the value of the evaluation policy. The problem of this approach is that if the model is wrong
(misspeciﬁed)  the estimator is no longer consistent.
The second approach is importance sampling (IS; aka Horvitz-Thompson)  which averages the data
weighted by the density ratio of the evaluation and behavior policies. Although IS gives an unbiased
and consistent estimate  its variance tends to be large. Therefore self-normalized IS (SNIS; aka
Hájek) is often used [28]  which divides IS by the average of density ratios. SNIS has two important
properties: (1) its value is bounded in the support of rewards and (2) its conditional variance given
action and state is bounded by the conditional variance of the rewards. This leads to increased stability
compared with IS  especially when the density ratios are highly variable due to low overlap.
The third approach is the doubly robust (DR) method  which combines DM and IS and is given by
adding the estimated Q-function as a control variate [5  7  25]. If the Q-function is well speciﬁed  DR
is locally efﬁcient in the sense that its asymptotic MSE achieves the semiparametric lower bound
[33]. However  if the Q-function is misspeciﬁed  DR can actually have worse MSE than IS and/or
SNIS [12]. In addition  it does not have the boundedness property.
To address these deﬁciencies  we propose novel OPE estimators for both CB and RL that are
guaranteed to improve over both (SN)IS and DR in terms of asymptotic MSE (termed intrinsic

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

Table 1: Comparison of policy evaluation methods. The notation (*) means proposed estimator.
The notation # means partially satisﬁed  as discussed in the text. (S)IS and SN(S)IS refer either to
stepwise or non-stepwise.

Consistency
Local efﬁciency
Intrinsic efﬁciency
Boundedness
Stability

DM (S)IS SN(S)IS DR SNDR MDR REG(*) SNREG(*) EMP(*)

1
#

1

#

2
#

#
2

1

efﬁciency) and at the same time also satisfy the same boundedness and stability properties of SNIS 
in addition to the consistency and local efﬁciency of existing DR methods. See Table 1. Our general
strategy to obtain these estimators is to (1) make a parametrized class of estimators that includes IS 
SNIS  and DR and (2) choose the parameter using either a regression way (REG) or an empirical
likelihood way (EMP). The beneﬁt of these new properties in practice is conﬁrmed by experiments in
both CB and RL settings.
2 Sequential Decision Processes and Off Policy Evaluation
A sequential decision process is deﬁned by a tuple (X  A  P  R  P0  γ)  where X and A are the state
and action spaces  Pr(x  a) is the distribution of the bounded random variable r(x  a) ∈ [0  Rmax]
being the immediate reward of taking action a in state x  P (·|x  a) is the transition probability
distribution  P0 is the initial state distribution  and γ ∈ [0  1] is the discounting factor. A policy
π : X × A → [0  1] assigns each state x ∈ X a distribution over actions with π(a|x) being the
probability of taking actions a into x. We denote HT−1 = (x0  a0  r0 ···   xT−1  aT−1  rT−1) as a
t=0 γtrt  which is the return

T-step trajectory generated by policy π  and deﬁne RT−1(HT−1) =(cid:80)T−1

of trajectory. Our task is to estimate

T = E[RT−1(HT−1)]
βπ

(policy value).

We further deﬁne the value function V π(x) and Q-function Qπ(x  a) of a policy π  respectively  as
the expectation of the return of a T -step trajectory generated by starting at state x and state-action
pair (x  a). Note that the contextual bandit setting is a special case when T = 1.
The off-policy evaluation (OPE) problem is to estimate β∗ = βπe
T for the evaluation policy πe from n
observation of T -step trajectories D = {H(i)
i=1 independently generated by the behavior policy
πb. Here  we assume an overlap condition: for all state-action pair (x  a) ∈ X × A if πb(a|x) = 0
then πe(a|x) = 0. Throughout  expectations E[·] are taken with respect to a behavior policy. For any
function of the trajectory  we let

T−1}n

En[f (HT−1)] = n−1(cid:80)n
ωt1:t2 =(cid:81)t2

t=t1

i=1 f (H(i)

T−1).

Asmse[·] denotes asymptotic MSE in terms of the ﬁrst order; i.e.  Asmse[ ˆβ] = MSE[ ˆβ] + o(n−1).
The cumulative importance ratio from time step t1 to time step t2 is
πe(at|xt)/πb(at|xt) 

where the empty product is 1. We assume that this weight is bounded for simplicity.
2.1 Existing Estimators and Properties

We summarize three types of estimators. Some estimators depend on a model q(x  a; τ ) with
parameter τ ∈ Θτ for the Q-function Qπe(x  a). We say the model is correct or well-speciﬁed if
there is some τ0 such that Qπe(x  a) = q(x  a; τ0) and otherwise we say it is wrong or misspeciﬁed.
Throughout  we make the following assumption about the model
Assumption 2.1. (a1) Θτ is compact  (a2) |q(x  a; τ )| ≤ Rmax.
Direct estimator: DM is given by ﬁtting ˆτ  e.g.  by least squares  and then plugging this into

ˆβdm = En

.

(1)

(cid:34)(cid:88)

a∈A

(cid:35)

πe(a|x(i)

0 )q(x(i)

0   a; ˆτ )

2

When this model is correct  ˆβdm is both consistent for β∗ and locally efﬁcient in that its asymptotic
MSE is minimized among the class of all estimators consistent for β∗ [19  33].
Deﬁnition 2.1 (Local efﬁciency). When the model q(x  a; τ ) is well-speciﬁed  the estimator achieves
the efﬁciency bound.

However  all of models are wrong to some extent. In this sense  even if the sample size goes to
inﬁnity  ˆβdm might not be consistent.
Deﬁnition 2.2 (Consistency). The estimator is consistent for β∗ irrespective of model speciﬁcation.
Importance sampling estimators: IS and step-wise IS (SIS) are deﬁned
respectively as
ˆβis = En

(cid:104)(cid:80)T−1

(cid:80)T−1

EMP = REG = DR

  ˆβsis = En

ω0:T−1

(cid:69)

t=0 ω0:tγtrt

.

t=0 γtrt

IS  SNIS

(cid:104)

(cid:105)

(cid:105)

The weights ω0:t are assumed known here as is common in RL; otherwise
they can either be estimated directly or chosen by optimal balance [1  8  9].
Both IS and SIS satisfy consistency but the MSE of SIS estimator is
smaller than regular IS estimator by the law of total variance [27]. The
self-normalized versions of these estimators are:

(cid:104)

(cid:80)T−1

(cid:105)

(cid:35)

En

ˆβsnis =

ω0:T−1

t=0 γtrt

En[ω0:T−1]

  ˆβsnsis = En

ω0:t

En[ω0:t]

γtrt

.

SN(S)IS have two advantages over (S)IS. First  they are both 1-bounded
in that they are bounded by the theoretical upper bound of reward.

Deﬁnition 2.3 (α-Boundedness). The estimator is bounded by α(cid:80)T−1

(cid:34)T−1(cid:88)

t=0

(a) Well-speciﬁed

EMP = REG

(cid:69)

IS  SNIS  DR
(b) Misspeciﬁed

Figure 1:
asymptotic MSEs

Order of

t=0 γtRmax.

data. If the conditional variance of(cid:80)T−1

1-boundedness is the best we can achieve where α-boundedness for any α > 1 is a weaker property.
Second  their conditional variance given state and action data are no larger than the conditional
variance of any reward  to which we refer as stability.
Deﬁnition 2.4 (Stability). Let Dx a = {(x(i)
t=1 γtr(i)
variance of the estimator  given Dx a  is also bounded by σ2.
Unlike efﬁciency  boundedness and stability are ﬁnite-sample properties. Notably (S)IS lacks both of
these properties  which explains its unstable performance in practice  especially when density ratios
can be very large. While boundedness can be achieved by a simple truncation  stability cannot.
Doubly robust estimators: A DR estimator for RL [7  32] is given by ﬁtting ˆτ and plugging it into

t ) : i ≤ n  t ≤ T − 1} denote that action-state
  a(i)
  given Dx a  is bounded by σ2  then the conditional

t

t

ˆβdr = ˆβd({q(x  a; ˆτ )}T−1
t=0 ) 

where for any collection of functions {mt}T−1
ˆβd({mt}T−1

γtω0:trt − γt

t=0 ) = En

t=0 (known as control variates) we let
ω0:tmt(xt  at) − ω0:t−1

mt(xt  a)πe(a|xt)

(cid:32)

(cid:33)(cid:33)(cid:35)

.

(cid:34)T−1(cid:88)

t=0

(cid:32)(cid:88)

a∈A

(2)
The DR estimator is both consistent and locally efﬁcient. Recently  this approach to efﬁcient OPE
has been extended to Markov decision process [10] and inﬁnite-horizon problems [11]; in this paper
we focus on the more general sequential decision process. Instead of using a plug-in estimate of τ
in eq. 2  [4  6  26] further suggest that to pick ˆτ to minimize an estimate of the asymptotic variance
of ˆβd({q(x  a; τ )}T−1
t=0 )  leading to the MDR estimator [6] for OPE. However  DR and MDR satisfy
neither boundedness nor stability. Replacing  ω0:t with its self-normalized version ω0:t/En [ω0:t]
in (2) leads to SNDR [24  32] (aka WDR)  but it only satisﬁes these properties partially: it’s only
2-bounded and partially stable (see Appendix B).
Moreover  if the model is incorrectly speciﬁed  (M)DR may have MSE that is worse than any of the
four (SN)(S)IS estimators. [12] also experimentally showed that the performance of ˆβdr might be
very bad in practice when the model is wrong.
We therefore deﬁne intrinsic efﬁciency as an additional desiderata  which prohibits this from occurring.

3

Deﬁnition 2.5 (Intrinsic efﬁciency). The asymptotic MSE of the estimator is smaller than that of any
of ˆβsis  ˆβis  ˆβsnsis  ˆβsnis  ˆβdr  irrespective of model speciﬁcation.
MDR can be seen as motivated by a variant of intrinsic efﬁciency against only DR (hence the #
in Table 1). Although this is not precisely proven in [6]  this arises as a corollary of our results.
Nonetheless  MDR does not achieve full intrinsic efﬁciency against all above estimators.
3 REG and EMP for Contextual Bandit

None of the estimators above simultaneously satisfy all desired properties  Deﬁnitions 2.1–2.5. In the
next sections  we develop new estimators that do. For clarity we ﬁrst consider the simpler CB setting 
where we write (x  a  r) and w instead of (x0  a0  r0) and w0:0. We then start by showing how a
modiﬁcation to MDR ensures intrinsic efﬁciency. To obtain the other desiderata  we have to change
how we choose the parameters. Regarding the intuitive detailed explanation  refer to Appendix A.
3.1 REG: Intrinsic Efﬁciency

When T = 1  ˆβd(m) in (2) becomes simply

where F(m(x  a)) = wm(x  a) −(cid:8)(cid:80)

ˆβd(m) = En [wr − F(m)]  

a∈A m(x  a)πe(a|x)(cid:9). By construction  E[F(m)] = 0 for

(3)

every m. (M)DR  for example  use m(x  a; τ ) = q(x  a; τ ).
Instead  we let

m(x  a; ζ1  ζ2  τ ) = ζ1 + ζ2q(x  a; τ ) 

for parameters τ and ζ = (ζ1  ζ2). This new choice has a special property: it includes both IS and DR
estimators. Given any τ  setting ζ1 = 0  ζ2 = 0 yields IS and setting ζ1 = 0  ζ2 = 1 gives (M)DR.
This gives a simple recipe for intrinsic efﬁciency: estimate the variance of ˆβd(ζ1 + ζ2q(x  a; τ )) and

minimize it over τ  ζ. Because ˆβd(m) is unbiased  its variance is simply E(cid:2){wr − F(m)}2(cid:3) − β∗2.

Therefore  over the parameter spaces Θτ the (unknown) minimal variance choice is

(cid:104){wr − F(ζ1 + ζ2q(x  a; τ ))}2(cid:105)
(cid:104){wr − F(ζ1 + ζ2q(x  a; τ ))}2(cid:105)

.

(ζ∗  τ∗) = arg min
ζ∈R2 τ∈Θτ

E

(ˆζ  ˆτ ) = arg min
ζ∈R2 τ∈Θτ

En

We let the REG estimator be ˆβreg = ˆβd(ˆζ1 + ˆζ2q(x  a; ˆτ )) where we choose the parameters by
minimizing the estimated variance:

.

(4)

(5)

Asmse[ ˆβreg] = n−1 min

ζ∈R2 τ∈Θτ

E

To establish desired efﬁciencies  we prove the following theorem indicating that our choice of
parameters does not inﬂate the variance. Note that it is not obvious because the plug-in some
parameters generally causes an inﬂation of the variance.
Theorem 3.1. When the optimal solution (ζ∗  τ∗) in (4) is unique 

(cid:104){wr − F(ζ1 + ζ2q(x  a; τ ))}2 − β∗2(cid:105)

.

(cid:104){wr − F(ζ1 + ζ2q(x  a; τ ))}2 − β∗2(cid:105)

Remark 3.1. For ζ = (β∗  0)  this asymptotic MSE is the same as the one of SNIS  var[w(r − β∗)].
From Theorem 3.1 we obtain the desired efﬁciencies.
Importantly  to prove this  we note
how the asymptotic MSEs of each of (SN)(S)IS and DR can be represented in the form
n−1E
Corollary 3.1. The estimator ˆβreg has local and intrinsic efﬁciency.
Remark 3.2 (Comparison to MDR). REG is like MDR with an expanded model class. This class is
carefully chosen to guarantee intrinsic efﬁciency. In addition  as another corollary  we have proven
partial intrinsic efﬁciency for MDR against DR (just ﬁx ζ = (0  1) in (5)) where [6] only proved
consistency of MDR. However  neither MDR nor REG satisﬁes boundedness and stability.
Remark 3.3 (SNREG). Replacing weights w by their self-normalized version w/En[w] in REG
leads to SNREG. We explore this estimator in Appendix A and show it only gives 2-boundedness  does
not give stability  and limits REG’s intrinsic efﬁciency to be only against SN(S)IS and SNDR.

for some ζ and τ.

4

3.2 EMP: Intrinsic Efﬁciency  Boundedness  and Stability

We next construct an estimator satisfying intrinsic efﬁciency as well as boundedness and stability. The
key idea is to use empirical likelihood to choose the parameters [29–31]. Empirical likelihood is a
nonparametric MLE commonly used in statistics [21]. We consider the control variate m(x  a; ξ; τ ) =
ξ + q(x  a; τ ) with parameters ξ  τ and q(x  a; τ ) = t(x  a)(cid:62)τ  where t(x  a) is a dτ -dimensional
vector of linear independent basis functions not including a constant. Then  an estimator for β is
deﬁned as

(cid:2)ˆc−1ˆκ(x  a)πe(a|x)r(cid:3)   where

ˆβemp = En
ˆκ(x  a) = {πb(a|x)[1 + F(m(x  a; ˆξ  ˆτ ))]}−1  ˆc = En

(cid:104){1 + F(m(x  a; ˆξ  ˆτ ))}−1(cid:105)

 

(6)

ˆξ  ˆτ = arg max
ξ∈R τ∈Θτ

En[log{1 + F(m(x  a; ξ  τ ))}].

n(cid:88)

n(cid:88)

n(cid:88)

This is motivated by solving the dual problem of the following optimization problem formulated by
the empirical likelihood:

max

κ

log κ(i)  s.t.

i=1

i=1

i=1

κ(i)πb(a(i)|x(i)) = 1 

κ(i)πb(a(i)|x(i))F(m(x(i)  a(i); ξ  τ )) = 0.

The objective in an optimization problem (6) is a convex function; therefore  it is easy to solve. Then 
the estimator ˆβemp has all the desirable ﬁnite-sample and asymptotic properties.
Lemma 3.1. The estimator ˆβemp satisﬁes 1-boundedness and stability.
Theorem 3.2. The estimator ˆβemp has local and intrinsic efﬁciency  and

(cid:104){wr − F(ζ + q(x  a; τ ))}2 − β∗2(cid:105)

.

(7)

Asmse[ ˆβemp] = n−1 min

ζ∈R τ∈Rdτ

E

Here  we have assumed the model is linear in τ. Without this assumption  Theorem 3.2 may not
hold. In the following section  we consider how to relax this assumption while maintaining local and
intrinsic efﬁciency.
3.3 Practical REG and EMP

While REG and EMP have desirable theoretical properties  both have some practical issues. First  for
REG  the optimization problem in (5) may be non-convex if q(x  a; τ ) is not linear in τ  as is the case
in our experiment in Sec. 5.1 where we use a logistic model with 216 parameters. (The same issue
exists for MDR.) Similarly  EMP estimator has the problem that there is no theoretical guarantee for
intrinsic efﬁciency when q(x  a; τ ) is not linear in τ. Therefore  we suggest the following uniﬁed
practical approach to selecting τ in a way that maintains the desired properties.
First  we estimate a parameter τ in q(x  a; τ ) as in DM to obtain ˆτ  which we assume has a limit 
p→ τ† . Then  we consider solving the following optimization problems instead of (5) and (6) for
ˆτ
REG and EMP  respectively

(cid:104){wr − F(m(x  a; ζ  ˆτ ))}2(cid:105)

  ˆξ = arg max

ξ∈R2

En[log{1 + F(m(x  a; ξ  ˆτ ))}] 

ˆζ = arg min

ζ∈R2

En

where m(x  a; ζ  ˆτ ) = ζ1 + ζ2q(x  a; ˆτ ) or m(x  a; ξ  ˆτ ) = ξ1 + ξ2q(x  a; ˆτ ). This is a convex
optimization problem with two dimensional parameters; thus  it is easy to solve.
Here  the asymptotic MSE of practical ˆβreg and ˆβemp are as follows.
Theorem 3.3. The above plug-in-τ versions of ˆβreg and ˆβemp still satisfy local and intrinsic efﬁciency 
and ˆβemp satisﬁes 1-boundedness and partial stability. Their asymptotic MSEs are

(cid:104)(cid:8)wr − F(ζ1 + ζ2q(x  a; τ†))(cid:9)2 − β∗2(cid:105)

.

(8)

n−1 min
ζ∈R2

E

As a simple extension  we may consider multiple models for the Q-function. E.g  we can have two
models q1(x  a; τ1) and q2(x  a; τ1) and let m(x  a; ζ  ˆτ ) = ζ1 + ζ2q1(x  a; ˆτ1) + ζ3q2(x  a; ˆτ2). Our
results easily extend to provide intrinsic efﬁciency with respect to DR using any of these models.

5

4 REG and EMP for Reinforcement learning

We next present how REG and EMP extend to the RL setting. Some complications arise because of
the multi-step horizon. For example  IS and SIS are different as opposed to the case T = 1.

4.1 REG for RL
We consider an extension of REG to a RL setting. First  we derive the variance of ˆβd({mt}T−1
t=0 ).
Theorem 4.1. The variance of ˆβd({mt}T−1

t=0 ) is n−1E[v({mt}T−1

(cid:40)

ωt:tmt(xt  at) −(cid:88)

t=0 )]  where v({mt}T−1
t=0 ) is
mt(xt  a)πe(a|xt)

(cid:41)

γk−tωt:krk−t|Ht] −

(cid:32)

T−1(cid:88)

T−1(cid:88)

γ2tω2

0:t−1var

E[

t=0

k=t

a∈A

(cid:33)

.

|Ht−1

(9)

To derive REG  we consider the class of estimators ˆβd({mt}T−1
ζ1t + ζ2tq(xt  at; ˆτ ) for all 0 ≤ t ≤ T − 1. Then  we deﬁne an estimator ˆζ and the optimal ζ∗ as

t=0 ) where mt is mt(xt  at; ζ) =

E[v({mt(xt  at; ζ)}T−1

t=0 )].

(10)

ˆζ = arg min

ζ∈R2

En[v({mt(xt  at; ζ)}T−1

ζ∗ = arg min
ζ∈R2
reg = ˆβd({ˆζ1t + ˆζ2tq(x  a; ˆτ )}T−1

t=0 )] 

REG is then deﬁned as ˆβT−1
t=0 )  where following our discussion in
Section 3.3  ˆτ is given by ﬁtting as in DM/DR. Theoretically  we could also choose τ to minimize
eq. (9)  but that can be computationally intractable.
A similar argument to that in Section 3.1 shows that a data-driven parameter choice induces no
inﬂation in asymptotic MSE. Therefore  the asymptotic MSE of the estimator ˆβreg is minimized
among the class of estimators ˆβd({ζ1t + ζ2tq(xt  at; ˆτ )}T−1
t=0 )). This implies that the asymptotic
MSE of ˆβreg is smaller than ˆβsis and ˆβdr because ˆβsis corresponds to the case ζt = (0  0) and ˆβdr
corresponds to the case ζt = (0  1). In addition  we can prove that the estimator ˆβT−1
is more
efﬁcient than ˆβsnsis. To prove this  we introduce the following lemma.
Lemma 4.1.
Asmse[ ˆβsnis] = n−1

γk−tωt+1:krk−t|Ht

(cid:34)T−1(cid:88)

T−1(cid:88)

(cid:33)(cid:35)

|Ht−1

− β∗

(cid:33)

γ2tω2

0:t−1var

ωt:t

E

(cid:32)

(cid:32)

(cid:34)

(cid:35)

reg

 

t

k=t

t = E[ω0:trt].

t   0) in eq. (9) recovers the above. This suggests the following theorem.

where β∗
We note that setting ζt = (β∗
Theorem 4.2. The estimator ˆβT−1
Remark 4.1. Practically  when the horizon is long  there may be too many parameters to optimize 
which can causes overﬁtting. That is  although there is no inﬂation in MSE asymptotically  there
may be issues in ﬁnite samples. To avoid this problem  some constraint or regularization should be
reg (0 ≤ k ≤ T − 1) given by
imposed on the parameters. Here we will consider the estimator ˆβk
ˆβd({mt(xt  at; ˆζ)}T−1

t=0 ) for the constrained control variates:

is locally and intrinsically efﬁcient.

reg

E

t=0

(cid:26)ζt1 + ζt2q(xt  at; ˆτ ) (0 ≤ t < k) 

ζk1 + ζk2q(xt  at; ˆτ ) (k ≤ t ≤ T − 1).

mt(xt  at; ζ) =

The estimator ˆβT−1
guarantees of ˆβk

reg corresponds to the originally introduced estimator. We can also obtain theoretical
reg for k (cid:54)= T − 1. For details  see Appendix C.

4.2 EMP for RL

First  we deﬁne a control variate:

(cid:32)

T−1(cid:88)

g(Dx a; ξ  ˆτ ) =

γt

ω0:tmt(xt  at; ξ  ˆτ ) − ω0:t−1

t=0

6

(cid:41)(cid:33)

mt(xt  a; ξ  ˆτ )πe(a|xt)

.

(cid:40)(cid:88)

a∈A

Table 2: SatImage (RMSE×1000 )

DM2
12.2
30.5
71.7

IS
6.7
12.0
26.0

SNIS
4.0
5.6
12.7

DR
3.0
5.0
18.0

MDR
3.8
5.3
14.4

Table 3: Pageblock (RMSE×1000 )

DM2
2.6
5.6
16.0

IS
8.5
13.4
27.2

SNIS
3.4
4.0
6.5

DR
1.4
2.7
7.2

Table 4: PenDigits (RMSE×1000 )

DM2
8.2
17.4
56.0

IS
6.1
10.7
29.6

SNIS
2.8
3.9
9.9

DR
1.5
2.2
11.1

MDR
2.3
3.4
6.4

MDR
2.2
3.4
9.4

DM1
18.1
49.2
128.6

DM1
21.8
32.4
62.0

DM1
8.1
19.4
58.6

REG
2.8
4.4
13.6

REG
1.5
2.5
4.9

REG
1.4
2.1
9.4

EMP
2.8
4.4
13.7

EMP
1.4
2.4
4.9

EMP
1.4
2.0
9.5

Behavior policy

0.7πd + 0.3πu
0.4πd + 0.6πu
0.0πd + 1.0πu

Behavior policy

0.7πd + 0.3πu
0.4πd + 0.6πu
0.0πd + 1.0πu

Behavior policy

0.7πd + 0.3πu
0.4πd + 0.6πu
0.0πd + 1.0πu

By setting mt(xt  at; ξ  ˆτ ) = ξ1t + ξ2tq(xt  at; ˆτ )  deﬁne ˆξ;

ˆξ(ˆτ ) = arg max

ξ∈R2

En[log{1 + g(Dx a; ξ  ˆτ )}].

Then  an estimator ˆβT−1

emp is deﬁned as

(cid:34)T−1(cid:88)

t=0

ˆβT−1
emp = En

ω0:tγtrt

ˆc−1

1 + g(Dx a; ˆξ  ˆτ )

(cid:35)

(cid:34)

  ˆc = En

(cid:35)

.

1

1 + g(Dx a; ˆξ  ˆτ )

This estimator has the same efﬁciencies as ˆβT−1
tantly  the estimator ˆβT−1
Theorem 4.3. The asymptotic MSE of the estimator ˆβT−1
also locally and intrinsically efﬁcient. It also satisﬁes 1-boundeness and stability.

emp also satisﬁes a 1-boundedness and stability.

emp is the same as that of ˆβT−1

reg . Hence  it is

reg because the asymptotic MSE is the same. Impor-

5 Experiments

5.1 Contextual Bandit

We evaluate the OPE algorithms using the standard classiﬁcation data-sets from the UCI repository.
Here  we follow the same procedure of transforming a classiﬁcation data-set into a contextual bandit
data set as in [5  6]. Additional details of the experimental setup are given in Appendix E.
We ﬁrst split the data into training and evaluation. We make a deterministic policy πd by training
a logistic regression classiﬁer on the training data set. Then  we construct evaluation and behavior
policies as mixtures of πd and the uniform random policy πu. The evaluation policy πe is ﬁxed at
0.9πd + 0.1πu. Three different behavior policies are investigated by changing a mixture parameter.
Here  we compare the (practical) REG and EMP with DM  SIS  SNIS  DR  and MDR on the evaluation
data set. First  two Q-functions ˆq1(x  a)  ˆq2(x  a) are constructed by ﬁtting a logistic regression in
two ways with a l1 or l2 regularization term. We refer them as DM1 and DM2. Then  in DR  we
use a mixture of Q-functions 0.5ˆq1 + 0.5ˆq2 as m(x  a). For MDR  we use a logistic function as
m(x  a) and we use SGD to solve the resulting non-convex high-dimensional optimization (e.g. 
for SatImage we have 6(number of actions) × 36(number of covariates) parameters). We use
m(x  a; ζ) = ζ(cid:62)(1  ˆq1  ˆq2) in REG and m(x  a; ξ) = ξ(cid:62)(1  ˆq1  ˆq2) in EMP.
The resulting estimation RMSEs (root mean square error) over 200 replications of each experiment
are given in Tables 2–4  where we highlight in bold the best two methods in each case. We ﬁrst
ﬁnd that REG and EMP generally have overall the best performance. Second we see that this arises

7

Table 5: Windy GridWorld (RMSE)

SIS
0.64
0.53
0.39

SNSIS
0.49
0.34
0.29

MDR
0.28
0.21
0.14
Table 6: Cliff Walking (RMSE)

DR
0.17
0.11
0.09

SNSIS

2.9
2.4
2.2

SIS
3.6
3.2
3.1
Table 7: Mountain Car (RMSE)

MDR
2.3
2.2
2.0

DR
2.5
2.3
2.2

SIS
4.2
3.3
2.4

SNSIS

3.7
2.9
1.8

DR
1.9
1.6
1.4

MDR
1.9
1.6
1.5

REG
0.09
0.06
0.05

REG
2.1
1.6
1.2

REG
1.7
1.2
1.0

EMP
0.09
0.06
0.05

EMP
2.1
1.5
1.1

EMP
1.7
1.2
1.0

Size
250
500
750

DM
2.9
2.8
2.6

Size
1000
2000
3000

Size
1000
2000
3000

DM
7.7
6.0
6.8

DM
9.8
10.6
8.2

because they achieve similar RMSE to SNIS when SNIS performs well and similar RMSE to (M)DR
when (M)DR performs well  which is thanks to the intrinsic efﬁciency property. Whereas REG’s and
EMP’s intrinsic efﬁciency is visible  MDR still often does slightly worse than DR despites its partial
intrinsic efﬁciency  which can be attributed to optimizing too many parameters leading to overﬁtting
in the sample size studied.

5.2 Reinforcement Learning

We next compare the OPE algorithms in three standard RL setting from OpenAI Gym [3]: Windy
GridWorld  Cliff Walking  and Mountain Car. For further detail on each see Appendix E. We again
split the data into training and evaluation. In each setting we consider varying evaluation dataset
sizes. In each setting  a policy πd is computed as the optimal policy based on the training data using
Q-learning. The evaluation policy πe is then set to be (1− α)πd + απu  where α = 0.1. The behavior
policy is deﬁned similarly with α = 0.2 for Windy GridWorld and Cliff Walking and with α = 0.15
for Mountain Car. We set the discounting factor to be 1.0 as in [6].
We compare the (practical) REG  EMP with k = 2 with DM  SIS  SNSIS  DR  MDR on the evaluation
data set generated by a behavior policy. A Q-function model is constructed using an off-policy TD
learning [27]. This is used in DM  DR  REG  and EMP. For MDR  we use a linear function for
m(x  a) in order to enable tractable optimization given the many parameters due to long horizons.
We report the resulting estimation RMSEs over 200 replications of each experiment in Tables 5–7.
We ﬁnd that the modest beneﬁts we gained in one time step in the CB setting translate to signiﬁcant
outright beneﬁts in the longer horizon RL setting. REG and EMP consistently outperform other
methods. Their RMSEs are indistinguishable except for one setting where EMP has slightly better
RMSE. These results highlight how the theoretical properties of intrinsic efﬁciency  stability  and
boundedness can translate to improved performance in practice.
6 Conclusion and Discussion

We studied various desirable properties for OPE in CB and RL. Finding that no existing estimator
satisﬁes all of them  we proposed two new estimators  REG and EMP  that satisfy consistency 
local efﬁciency  intrinsic efﬁciency  1-boundedness  and stability. These theoretical properties also
translated to improved comparative performance in a variety of CB and RL experiments.
In practice  there may be additional modiﬁcations that can further improve these estimators. For
example  [32  35] propose hybrid estimators that blend or switch to DM when importance weights are
very large. This reportedly works very well in practice but may make the estimator inconsistent under
misspeciﬁcation unless blending vanishes with n. In this paper  we focused on consistent estimators.
Also these do not satisfy intrinsic efﬁciency  1-boudedness  or stability. Achieving these properties
with blending estimators remains an important next step.

8

Acknowledgements

This material is based upon work supported by the National Science Foundation under Grant No.
1846210.

References

[1] A. Bennett and N. Kallus. Policy evaluation with latent confounders via optimal balance. In

Advances in Neural Information Processing Systems  2019.

[2] C. G. Bowsher and P. S. Swain. Identifying sources of variation and the ﬂow of information in

biochemical networks. Proceedings of the National Academy of Sciences  109  2012.

[3] G. Brockman  V. Cheung  L. Pettersson  J. Schneider  J. Schulman  J. Tang    and W. Zaremba.

Openai gym. arXiv preprint arXiv:1606.01540  2016.

[4] W. Cao  A. A. Tsiatis  and M. Davidian. Improving efﬁciency and robustness of the doubly
robust estimator for a population mean with incomplete data. Biometrika  96:723–734  2009.

[5] M. Dudík  D. Erhan  J. Langford  and L. Li. Doubly robust policy evaluation and optimization.

Statistical Science  29:485–511  2014.

[6] M. Farajtabar  Y. Chow  and M. Ghavamzadeh. More robust doubly robust off-policy evaluation.
In Proceedings of the 35th International Conference on Machine Learning  pages 1447–1456 
2018.

[7] N. Jiang and L. Li. Doubly robust off-policy value evaluation for reinforcement learning. In
Proceedings of the 33rd International Conference on International Conference on Machine
Learning-Volume  pages 652–661  2016.

[8] N. Kallus. Balanced policy evaluation and learning.

Processing Systems  pages 8895–8906  2018.

In Advances in Neural Information

[9] N. Kallus. Discussion: “entropy learning for dynamic treatment regimes”. Statistica Sinica  29

(4):1697–1705  2019.

[10] N. Kallus and M. Uehara. Double reinforcement learning for efﬁcient off-policy evaluation in

markov decision processes. arXiv preprint arXiv:1908.08526  2019.

[11] N. Kallus and M. Uehara. Efﬁciently breaking the curse of horizon: Double reinforcement

learning in inﬁnite-horizon processes. arXiv preprint arXiv:1909.05850  2019.

[12] J. D. Y. Kang and J. L. Schafer. Demystifying double robustness: A comparison of alternative
strategies for estimating a population mean from incomplete data. Statistical Science  22:
523–539  2007.

[13] L. Li  R. Munos  and C. Szepesvari. Toward minimax off-policy value estimation. In Proceedings
of the 18th International Conference on Artiﬁcial Intelligence and Statistics  pages 608–616 
2015.

[14] Q. Liu  L. Li  Z. Tang  and D. Zhou. Breaking the curse of horizon: Inﬁnite-horizon off-policy
estimation. In Advances in Neural Information Processing Systems 31  pages 5356–5366. 2018.

[15] A. R. Mahmood  H. P. van Hasselt  and R. S. Sutton. Weighted importance sampling for
off-policy learning with linear function approximation. In Advances in Neural Information
Processing Systems 27  pages 3014–3022. 2014.

[16] T. Mandel  Y. Liu  S. Levine  E. Brunskill  and Z. Popovic. Off-policy evaluation across
representations with applications to educational games. In Proceedings of the 13th International
Conference on Autonomous Agentsand Multi-agent Systems  page 1077–1084  2014.

[17] R. Munos  T. Stepleton  A. Harutyunyan  and M. Bellemare. Safe and efﬁcient off-policy
reinforcement learning. In Advances in Neural Information Processing Systems 29  pages
1054–1062. 2016.

9

[18] S. A. Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical Society:

Series B (Statistical Methodology)  65:331–355  2003.

[19] Y. Narita  S. Yasui  and K. Yata. Efﬁcient counterfactual learning from bandit feedback. AAAI 

2019.

[20] W. K. Newey and D. L. Mcfadden. Large sample estimation and hypothesis testing. Handbook

of Econometrics  IV:2113–2245  1994.

[21] A. Owen. Empirical likelihood. Monographs on statistics and applied probability (Series); 92.

Chapman & Hall/CRC  2001.

[22] A. Owen and Y. Zhou. Safe and effective importance sampling. Journal of the American

Statistical Association  95(449):135–143  2000.

[23] D. Precup  R. Sutton  and S. Singh. Eligibility traces for off-policy policy evaluation. In
Proceedings of the 17th International Conference on Machine Learning  pages 759–766  2000.

[24] J. Robins  M. Sued  Q. Lei-Gomez  and A. Rotnitzky. Comment: Performance of double-robust
estimators when "inverse probability" weights are highly variable. Statistical Science  22:
544–559  2007.

[25] J. M. Robins  A. Rotnitzky  and L. P. Zhao. Estimation of regression coefﬁcients when some
regressors are not always observed. Journal of the American Statistical Association  89:846–866 
1994.

[26] D. B. Rubin and M. J. V. der Laan. Empirical efﬁciency maximization: Improved locally
efﬁcient covariate adjustment in randmized experiments and survival analysis. International
Journal of Biostatistics  4:Article 5  2008.

[27] R. S. Sutton. Reinforcement learning : an introduction. MIT Press  Cambridge  Mass.  2018.

[28] A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In

Advances in Neural Information Processing Systems 28  pages 3231–3239. 2015.

[29] Z. Tan. On likelihood approach for monte carlo integration. Journal of the American Statistical

Association  99:1027–1036  2004.

[30] Z. Tan. A distributional approach for causal inference using propensity scores. Journal of the

American Statistical Association  101:1619–1637  2006.

[31] Z. Tan. Bounded  efﬁcient and doubly robust estimation with inverse weighting. Biometrika 

97:661–682  2010.

[32] P. Thomas and E. Brunskill. Data-efﬁcient off-policy policy evaluation for reinforcement
learning. In Proceedings of the 33rd International Conference on Machine Learning  pages
2139–2148  2016.

[33] A. Tsiatis. Semiparametric Theory and Missing Data. Springer  New York  2006.

[34] A. W. van der Vaart. Asymptotic statistics. Cambridge University Press  Cambridge  UK  1998.

[35] Y.-X. Wang  A. Agarwal  and M. Dudik. Optimal and adaptive off-policy evaluation in contextual
bandits. In Proceedings of the 34th International Conference on Machine Learning  pages
3589–3597  2017.

10

,Nathan Kallus
Masatoshi Uehara