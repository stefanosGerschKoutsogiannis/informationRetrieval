2017,Convergence of Gradient EM on Multi-component Mixture of Gaussians,In this paper  we study convergence properties of the gradient variant of Expectation-Maximization algorithm~\cite{lange1995gradient} for Gaussian Mixture Models for arbitrary number of clusters and mixing coefficients. We derive the convergence rate depending on the mixing coefficients  minimum and maximum pairwise distances between the true centers  dimensionality and number of components; and obtain a near-optimal local contraction radius. While there have been some recent notable works that derive local convergence rates for EM in the two symmetric mixture of Gaussians  in the more general case  the derivations need structurally different and non-trivial arguments. We use recent tools from learning theory and empirical processes to achieve our theoretical results.,Convergence of Gradient EM on Multi-component

Mixture of Gaussians

Bowei Yan

University of Texas at Austin

boweiy@utexas.edu

Mingzhang Yin

University of Texas at Austin

mzyin@utexas.edu

Purnamrita Sarkar

University of Texas at Austin

purna.sarkar@austin.utexas.edu

Abstract

In this paper  we study convergence properties of the gradient variant of
Expectation-Maximization algorithm [11] for Gaussian Mixture Models for ar-
bitrary number of clusters and mixing coefﬁcients. We derive the convergence
rate depending on the mixing coefﬁcients  minimum and maximum pairwise dis-
tances between the true centers  dimensionality and number of components; and
obtain a near-optimal local contraction radius. While there have been some recent
notable works that derive local convergence rates for EM in the two symmetric
mixture of Gaussians  in the more general case  the derivations need structurally
different and non-trivial arguments. We use recent tools from learning theory and
empirical processes to achieve our theoretical results.

1

Introduction

Proposed by [7] in 1977  the Expectation-Maximization (EM) algorithm is a powerful tool for sta-
tistical inference in latent variable models. A famous example is the parameter estimation problem
under parametric mixture models. In such models  data is generated from a mixture of a known fam-
ily of parametric distributions. The mixture component from which a datapoint is generated from
can be thought of as a latent variable.
Typically the marginal data log-likelihood (which integrates the latent variables out) is hard to op-
timize  and hence EM iteratively optimizes a lower bound of it and obtains a sequence of estima-
tors. This consists of two steps. In the expectation step (E-step) one computes the expectation of
the complete data likelihood with respect to the posterior distribution of the unobserved mixture
memberships evaluated at the current parameter estimates. In the maximization step (M-step) one
this expectation is maximized to obtain new estimators. EM always improves the objective func-
tion. While it is established in [4] that the true parameter vector is the global maximizer of the
log-likelihood function  there has been much effort to understand the behavior of the local optima
obtained via EM.
When the exact M-step is burdensome  a popular variant of EM  named Gradient EM is widely
used. The idea here is to take a gradient step towards the maxima of the expectation computed in
the E-step. [11] introduces a gradient algorithm using one iteration of Newton’s method and shows
the local properties of the gradient EM are almost identical with those of the EM.
Early literature [22  24] mostly focuses on the convergence to the stationary points or local optima.
In [22] it is proven that the sequence of estimators in EM converges to stationary point when the
lower bound function from E-step is continuous. In addition  some conditions are derived under

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

which EM converges to local maxima instead of saddle points; but these are typically hard to check.
A link between EM and gradient methods is forged in [24] via a projection matrix and the local
convergence rate of EM is obtained. In particular  it is shown that for GMM with well-separated
centers  the EM achieves faster convergence rates comparable to a quasi-Newton algorithm. While
the convergence of EM deteriorates under worse separations  it is observed in [20] that the mixture
density determined by estimator sequence of EM reﬂects the sample data well.
In recent years  there has been a renewed wave of interest in studying the behavior of EM especially
in GMMs. The global convergence of EM for a mixture of two equal-proportion Gaussian distribu-
tions is fully characterized in [23]. For more than two clusters  a negative result on EM and gradient
EM being trapped in local minima arbitrarily far away from the global optimum is shown in [9].
For high dimensional GMMs with M components  the parameters are learned via reducing the
dimensionality via a random projection in [5]. In [6] the two-round method is proposed  where one
ﬁrst initializes with more than M points  then prune to get one point in every cluster. It is pointed
out in this paper that in high dimensional space  when the clusters are well separated  the mixing
weight will go to either 0 or 1 after one single update. It is showed in [25  17] that one can cluster
high dimensional sub-gaussian mixtures by semi-deﬁnite programming relaxations.
For the convergence rate of EM algorithm  it is observed in [19] that a very small mixing proportion
for one mixture component compared to others leads to slow convergence. [2] gives non-asymptotic
convergence guarantees in isotropic  balanced  two-component GMM; their result proves the linear
convergence of EM if the center is initialized in a small neighborhood of the true parameters. The
local convergence result in this paper has a sub-optimal contraction region.
K-means clustering is another widely used clustering method. Lloyd’s algorithm for k-means clus-
tering has a similar ﬂavor as EM. At each step  it recomputes the centroids of each cluster and
updates the membership assignments alternatively. While EM does soft clustering at each step 
Lloyd’s algorithm obtains hard clustering. The clustering error of Lloyd’s algorithm for arbitrary
number of clusters is studied in [13]. The authors also show local convergence results where the
contraction region is less restrictive than [2].
We would like to point out that there are many notable algorithms [10  1  21] with provable guaran-
tees for estimating mixture models. In [14  8] the authors propose polynomial time algorithms which
achieve epsilon approximation to the k-means loss. A spectral algorithm for learning mixtures of
gaussians is proposed in [21]. We want to point out that our aim is not to come up with a new algo-
rithm for mixture models  but to understand the interplay of model parameters in the convergence of
gradient EM for a mixture of Gaussians with M components. As we discuss later  our work also im-
mediately leads to convergence guarantees of Stochastic Gradient EM. Another important difference
is that the aim of these works is recovering the hidden mixture component memberships  whereas
our goal is completely different: we are interested in understanding how well EM can estimate the
mean parameters under a good initialization.
In this paper  we study the convergence rate and local contraction radius of gradient EM under
GMM with arbitrary number of clusters and mixing weights which are assumed to be known. For
simplicity  we assume that the components share the same covariance matrix  which is known. Thus
it sufﬁces to carry out our analysis for isotropic GMMs with identity as the shared covariance matrix.
We obtain a near-optimal condition on the contraction region in contrast to [2]’s contraction radius
for the mixture of two equal weight Gaussians. We want to point out that  while the authors of [2]
provide a general set of conditions to establish local convergence for a broad class of mixture models 
the derivation of speciﬁc results and conditions on local convergence are tailored to the balance and
symmetry of the model.
We follow the same general route: ﬁrst we obtain conditions for population gradient EM  where
all sample averages are replaced by their expected counterpart. Then we translate the population
version to the sample one. While the ﬁrst part is conceptually similar  the general setting calls for
more involved analysis. The second step typically makes use of concepts from empirical processes 
by pairing up Ledoux-Talagrand contraction type arguments with well established symmetrization
results. However  in our case  the function is not a contraction like in the symmetric two component
case  since it involves the cluster estimates of all M components. Furthermore  the standard analy-
sis of concentration inequalities by McDiarmid’s inequality gets complicated because the bounded
difference condition is not satisﬁed in our setting. We overcome these difﬁculties by taking advan-

2

tage of recent tools in Rademacher averaging for vector valued function classes  and variants of
McDiarmid type inequalities for functions which have bounded difference with high probability.
The rest of the paper is organized as follows. In Section 2  we state the problem and the notations.
In Section 3  we provide the main results in local convergence rate and region for both population
and sample-based gradient EM in GMMs. Section 4 and 5 provide the proof sketches of population
and sample-based theoretical results  followed by the numerical result in Section 6. We conclude
the paper with some discussions.

µi ∈ Rd be the mean of cluster i. Without loss of generality  we assume EX =(cid:80)
p(x|µ) =(cid:80)M
(cid:17)
i=1 πiφ(X|µi  Id)
to lower bound the log likelihood. Deﬁne Q(µ|µt) = EX [(cid:80)

2 Problem Setup and Notations
Consider a GMM with M clusters in d dimensional space  with weights π = (π1 ···   πM ). Let
i πiµi = 0 and
the known covariance matrix for all components is Id. Let µ ∈ RM d be the vector stacking the
µis vertically. We represent the mixture as X ∼ GMM(π  µ  Id)  which has the density function
i=1 πiφ(x|µi  Id). where φ(x; µ  Σ) is the PDF of N (µ  Σ). Then the population log-
likelihood function as L(µ) = EX log
. The Maximum Likelihood Estima-
tor is then deﬁned as ˆµML = arg max p(X|µ). EM algorithm is based on using an auxiliary function
i p(Z = i|X; µt) log φ(X; µi  Id)] 
where Z denote the unobserved component membership of data point X. The standard EM update
is µt+1 = arg maxµ Q(µ|µt). Deﬁne

(cid:16)(cid:80)M

wi(X; µ) =

(cid:80)M
πiφ(X|µi  Id)
j=1 πjφ(X|µj  Id)

The update step for gradient EM  deﬁned via the gradient operator G(µt) : RM d → RM d  is

i + s[∇Q(µt|µt)]i = µt

i + sEX

i = µt

G(µt)(i) := µt+1

(2)
where s > 0 is the step size and (.)(i) denotes the part of the stacked vector corresponding to the ith
mixture component. We will also use Gn(µ) to denote the empirical counterpart of the population
gradient operator G(µ) deﬁned in Eq (2). We assume we are given an initialization µ0
i and the true
mixing weight πi for each component.

(1)

(cid:2)πiwi(X; µt)(X − µt
i)(cid:3) .

2.1 Notations

i − µ∗

j(cid:107)  Rmin = mini(cid:54)=j (cid:107)µ∗

Deﬁne Rmax and Rmin as the largest and smallest distance between cluster centers i.e.  Rmax =
maxi(cid:54)=j (cid:107)µ∗
j(cid:107). Let πmax and πmin be the maximal and
. Standard complexity analysis notation
minimal cluster weights  and deﬁne κ as κ = πmax
πmin
o(·)  O(·)  Θ(·)  Ω(·) will be used. f (n) = ˜Ω(g(n)) is short for Ω(g(n)) ignoring logarithmic
factors  equivalent to f (n) ≥ Cg(n) logk(g(n))  similar for others. We use ⊗ to represent the
kronecker product.

i − µ∗

3 Main Results

Despite being a non-convex problem  EM and gradient EM algorithms have been shown to exhibit
good convergence behavior in practice  especially with good initializations. However  existing local
convergence theory only applies for two-cluster equal-weight GMM. In this section  we present our
main result in two parts. First we show the convergence rate and present a near-optimal radius for
contraction region for population gradient EM. Then in the second part we connect the population
version to ﬁnite sample results using concepts from empirical processes and learning theory.

3.1 Local contraction for population gradient EM
Intuitively  when µt equals the ground truth µ∗  then the Q(µ|µ∗) function will be well-behaved.
This function is a key ingredient in [2]  where the curvature of the Q(·|µ) function is shown to
be close to the curvature of Q(·|µ∗) when the µ is close to µ∗. This is a local property that only
requires the gradient to be stable at one point.

3

Deﬁnition 1 (Gradient Stability). The Gradient Stability (GS) condition  denoted by GS(γ  a)  is
satisﬁed if there exists γ > 0  such that for µt

i   a) with some a > 0  for ∀i ∈ [M ].

i ∈ B(µ∗

(cid:107)∇Q(µt|µ∗) − ∇Q(µt|µt)(cid:107) ≤ γ(cid:107)µt − µ∗(cid:107)

The GS condition is used to prove contraction of the sequence of estimators produced by population
gradient EM. However  for most latent variable models  it is typically challenging to verify the GS
condition and obtain a tight bound on the parameter γ. We derive the GS condition under milder
conditions (see Theorem 4 in Section 4)  which bounds the deviation of the partial gradient evaluated
i uniformly over all i ∈ [M ]. This immediately implies the global GS condition deﬁned in
at µt
Deﬁnition 1. Equipped with this result  we achieve a nearly optimal local convergence radius for
general GMMs in Theorem 1. The proof of this theorem can be found in Appendix B.2.
Theorem 1 (Convergence for Population gradient EM). Let d0 := min{d  M}. If Rmin = ˜Ω(
with initialization µ0 satisfying  (cid:107)µ0

d0) 

√

(cid:32)(cid:115)
i − µ∗
i (cid:107) ≤ a ∀i ∈ [M ]  where

(cid:26) M 2κ

(cid:18)

d0O

log

max

−(cid:112)

  Rmax  d0

πmin

(cid:27)(cid:19)(cid:33)

a ≤ Rmin
2

then the Population EM converges:

(cid:107)µt − µ∗(cid:107) ≤ ζ t(cid:107)µ0 − µ∗(cid:107) 

πmax − πmin + 2γ

πmax + πmin

< 1

ζ =

(cid:16)−(cid:0) Rmin

2 − a(cid:1)2 √

(cid:17)

where γ = M 2(2κ + 4) (2Rmax + d0)2 exp
Remark 1. The local contraction radius is largely improved compared to that in [2]  which has
Rmin/8 in the two equal sized symmetric GMM setting. It can be seen that in Theorem 1  a/Rmin
2 as the signal to noise ratio goes to inﬁnity. We will show in simulations that when initial-
goes to 1
ized from some point that lies Rmin/2 away from the true center  gradient EM only converges to a
stationary point which is not a global optimum. More discussion can be found in Section 6.

< πmin.

d0/8

3.2 Finite sample bound for gradient EM

In the ﬁnite sample setting  as long as the deviation of the sample gradient from the population
gradient is uniformly bounded  the convergence in the population setting implies the convergence
in ﬁnite sample scenario. Thus the key ingredient in the proof is to get this uniform bound over all
n (µ)(cid:107)  where G and Gn
parameters in the contraction region A  i.e. bound supµ∈A (cid:107)G(i)(µ) − G(i)
are deﬁned in Section 2.
To prove the result  we expand the difference and deﬁne the following function for i ∈ [M ]  where
u is a unit vector on a d dimensional sphere S d−1. This appears because we can write the Euclidean
norm of any vector B  as (cid:107)B(cid:107) = supu∈S d−1(cid:104)B  u(cid:105).

w1(Xi; µ)(cid:104)Xi − µ1  u(cid:105) − Ew1(X; µ)(cid:104)X − µ1  u(cid:105).

(3)

gu
i (X) = sup
µ∈A

1
n

n(cid:88)

i=1

We will drop the super and subscript and prove results for gu
The outline of the proof is to show that g(X) is close to its expectation. This expectation can be
further bounded via the Rademacher complexity of the corresponding function class (deﬁned below
in Eq (4)) by the tools like the symmetrization lemma [18].
Consider the following class of functions indexed by µ and some unit vector on d dimensional
sphere u ∈ S d−1:

1 without loss of generality.

F u
i = {f i : X → R|f i(X; µ  u) = wi(X; µ)(cid:104)X − µi  u(cid:105)}

(4)
We need to bound the M functions classes separately for each mixture. Given a ﬁnite n-sample
(X1 ···   Xn)  for each class  we deﬁne the Rademacher complexity as the expectation of empirical

4

Rademacher complexity.

ˆRn(F u

i ) = E

sup

µ∈A

n(cid:88)

j=1

1
n

 ;

if i(Xj; µ  u)

Rn(F u

i ) = EX ˆRn(F u
i )

where i’s are the i.i.d. Rademacher random variables.
For many function classes  the computation of the empirical Rademacher complexity can be hard.
For complicated functions which are Lipschitz w.r.t functions from a simpler function class  one
can use Ledoux-Talagrand type contraction results [12]. In order to use the Ledoux-Talagrand con-
traction  one needs a 1-Lipschitz function  which we do not have  because our function involves µi 
i ∈ [M ]. Also  the weight functions wi are not separable in terms of the µi’s. Therefore the classical
contraction lemma does not apply. In our analysis  we need to introduce a vector-valued function 
with each element involving only one µi  and apply a recent result of vector-versioned contraction
lemma [15]. With some careful analysis  we get the following. The details are deferred to Section 5.
Proposition 1. Let F u

i as deﬁned in Eq. (4) for ∀i ∈ [M ]  then for some universal constant c 
Rn(F u

√
i ) ≤ cM 3/2(1 + Rmax)3
√

d max{1  log(κ)}

n

After getting the Rademacher complexity  one needs to use concentration results like McDiarmid’s
inequality [16] to achieve the ﬁnite-sample bound. Unfortunately for the functions deﬁned in Eq. (4) 
the martingale difference sequence does not have bounded differences. Hence it is difﬁcult to apply
McDiarmid’s inequality in its classical form. To resolve this  we instead use an extension of Mc-
Diarmid’s inequality which can accommodate sequences which have bounded differences with high
probability [3].
Theorem 2 (Convergence for sample-based gradient EM). Let ζ be the contraction parameter in
Theorem 1  and

unif(n) = ˜O(max{n−1/2M 3(1 + Rmax)3

d max{1  log(κ)}  (1 + Rmax)d/

n}).

(5)

√

√

If unif(n) ≤ (1 − ζ)a  then sample-based gradient EM satisﬁes

(cid:13)(cid:13) ˆµt

i − µ∗

i

(cid:13)(cid:13) ≤ ζ t(cid:13)(cid:13)µ0 − µ∗(cid:13)(cid:13)2 +

1
1 − ζ

unif(n);
with probability at least 1 − n−cd  where c is a positive constant.
Remark 2. When data is observed in a streaming fashion  the gradient update can be modiﬁed
into a stochastic gradient update  where the gradient is evaluated based on a single observation
or a small batch. By the GS condition proved in Theorem 1  combined with Theorem 6 in [2]  we
immediately extend the guarantees of gradient EM into the guarantees for the stochastic gradient
EM.

∀i ∈ [M ]

3.3

Initialization

Appropriate initialization for EM is the key to getting good estimation within fewer restarts in prac-
tice. There have been a number of interesting initialization algorithms for estimating mixture mod-
els. It is pointed out in [9] that in practice  initializing the centers by uniformly drawing from the
data is often more reasonable than drawing from a ﬁxed distribution. Under this initialization strat-
egy  we can bound the number of initializations required to ﬁnd a “good” initialization that falls
in the contraction region in Theorem 1. The exact theorem statement and a discussion of random
initialization can be found in Appendix D. More sophisticated strategy includes  an approximate
solution to k-means on a projected low-dimensional space used in [1] and [10]. While it would be
interesting to study different initialization schemes  that is part of future work.

4 Local Convergence of Population Gradient EM

In this section we present the proof sketch for Theorem 1. The complete proofs in this section are
deferred to Appendix B. To start with  we calculate the closed-form characterization of the gradient
of q(µ) as stated in the following lemma.

5

Lemma 1. Deﬁne q(µ) = Q(µ|µ∗). The gradient of q(µ) is ∇q(µ) = (diag(π) ⊗ Id) (µ∗ − µ).
If we know the parameter γ in the gradient stability condition  then the convergence rate depends
only on the condition number of the Hessian of q(·) and γ.
Theorem 3 (Convergence rate for population gradient EM). If Q satisﬁes the GS condition with
parameter 0 < γ < πmin  denote dt := (cid:107)µt − µ∗(cid:107)  then with step size s =

  we have:

2

πmin+πmax

(cid:18) πmax − πmin + 2γ

(cid:19)t

πmax + πmin

d0

dt+1 ≤

The proof uses an approximation on gradient and standard techniques in analysis of gradient descent.
Remark 3. It can be veriﬁed that the convergence rate is equivalent to that shown in [2] when
applied to GMMs. The convergence slows down as the proportion imbalance κ = πmax/πmin
increases  which matches the observation in [19].

2 − (cid:112)min{d  M} max(4(cid:112)2[log(Rmin/4)]+  8
(cid:80)M
i   a) ∀i ∈ [M ] where a ≤ Rmin
(cid:17)
2 − a(cid:1)2(cid:112)min{d  M}/8
i=1 (cid:107)µt

Now to verify the GS condition  we have the following theorem.
Theorem 4 (GS condition for general GMM).
B(µ∗
(cid:107)∇µiQ(µ|µt) − ∇µiq(µ)(cid:107) ≤ γ
where γ = M 2(2κ + 4) (2Rmax + min{d  M})2 exp
Furthermore  (cid:107)∇Q(µ|µt) − ∇q(µ)(cid:107) ≤ γ(cid:107)µt − µ∗(cid:107).

If Rmin = ˜Ω((cid:112)min{d  M})  and µi ∈
(cid:16)−(cid:0) Rmin

(cid:107)µt − µ∗(cid:107) 

i (cid:107) ≤ γ√

i − µ∗

3)  then

√

M

M

.

Proof sketch of Theorem 4. W.l.o.g. we show the proof with the ﬁrst cluster  consider the difference
of the gradient corresponding to µ1.

∇µ1Q(µt|µt) − ∇µ1 q(µt) =E(w1(X; µt) − w1(X; µ∗))(X − µt
1)

For any given X  consider the function µ → w1(X; µ)  we have

w1(X; µ)(1 − w1(X; µ))(X − µ1)T

−w1(X; µ)w2(X; µ)(X − µ2)T

−w1(X; µ)wM (X; µ)(X − µM )T
i  (cid:107)µt

B(µ∗

i=1



∇µw1(X; µ) =

Let µu = µ∗ + u(µt − µ∗) ∀u ∈ [0  1]  obviously µu ∈ ΠM
By Taylor’s theorem 
(cid:107)E(w1(X; µt
≤U1(cid:107)µt
1 − µ∗

1) − w1(X; µ∗
1(cid:107)2 +
Ui(cid:107)µt

1))(X − µt
i − µ∗

1)(cid:107) =
i (cid:107)2 ≤ max
i∈[M ]

(cid:88)

i(cid:54)=1

...
(cid:13)(cid:13)(cid:13)(cid:13)E
(cid:20)(cid:90) 1
{Ui}(cid:88)

u=0

i

(6)

(7)

(8)

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)

i − µ∗

i (cid:107)) ⊂ ΠM

i=1

B(µ∗

i   a).

∇uw1(X; µu)du(X − µt
1)
(cid:107)µt

i − µ∗
i (cid:107)2

where

U1 = sup
u∈[0 1]

Ui = sup
u∈[0 1]

(cid:107)Ew1(X; µu)(1 − w1(X; µu))(X − µt
(cid:107)Ew1(X; µu)wi(X; µu)(X − µt

1)(X − µu

1)(X − µu
2 )T(cid:107)op

1 )T(cid:107)op

Bounding them with careful analysis on Gaussian distribution yields the result. The technical details
are deferred to Appendix B.

5 Sample-based Convergence

In this section we present the proof sketch for sample-based convergence of gradient EM. The main
ingredient in proof of Theorem 2 is the result of the following theorem  which develops an uniform
upper bound for the differences between sample-based gradient and population gradient on each
cluster center.

6

Theorem 5 (Uniform bound for sample-based gradient EM). Denote A as the contraction region
ΠM
i=1

i   a). Under the condition of Theorem 1  with probability at least 1 − exp (−cd log n) 

B(µ∗

(cid:13)(cid:13)(cid:13)G(i)(µ) − G(i)

n (µ)

(cid:13)(cid:13)(cid:13) < unif(n);

sup
µ∈A

∀i ∈ [M ]

where unif(n) is as deﬁned in Eq. (5).
Remark 4. It is worth pointing out that  the ﬁrst part of the bound on unif(n) in Eq. (5) comes from
the Rademacher complexity  which is optimal in terms of the order of n and d. However the extra
factor of
d and log(n) comes from the altered McDiarmid’s inequality  tightening which will be
left for future work.

√

(cid:13)(cid:13)(cid:13)G(i)(µ) − G(i)

(cid:13)(cid:13)(cid:13). Recall gu

Proof sketch of Theorem 5. Denote Zi = supµ∈A
i (X) deﬁned
in Eq. (3)  then it is not hard to see that Zi = supu∈S d−1 gu
2-covering
{u(1) ···   u(K)} of the unit sphere S d−1  where K is the covering number of an unit sphere in
d dimensions. We can show that Zi ≤ 2 maxj=1 ···  K gu(j)
As we state below in Lemma 2  we have for each u  with probability at least 1 − exp (−cd log n) 
n}). Plugging in the Rademacher complexity from
i (X) = ˜O(max{Rn(F u
gu
Proposition 1  standard bounds on K  and applying union bound  we have
√

n (µ)
i (X). Consider a 1

i )  (1 + Rmax)d/

(X).

√

i

Zi ≤ ˜O(max{n−1/2M 3(1 + Rmax)3

d max{1  log(κ)}  (1 + Rmax)d/

√

n})

with probability at least 1 − exp (2d − cd log n) = 1 − exp (−c(cid:48)d log n).

n}).

√
1 )  (1 + Rmax)d/

Iteratively applying Theorem 5  we can bound the error in µ for the sample updates. The full proof
is deferred to Appendix C. The key step is the following lemma  which bounds the gu
i (X) for any
given u ∈ S d−1. Without loss of generality we prove the result for i = 1.
Lemma 2. Let u be a unit vector. Xi  i = 1 ···   n are i.i.d. samples from GMM(π  µ∗  Id).
1 (X) as deﬁned in Eq. (3). Then with probability 1 − exp(−cd log n) for some constant c > 0 
gu
1 (X) = ˜O(max{Rn(F u
gu
The quantity gu
1 (X) depends on the sample  the idea for proving Lemma 2 is to show it concentrates
around its expectation when sample size is large. Note that when the function class has bounded
differences (changing one data point changes the function by a bounded amount almost surely)  as
in the case in many risk minimization problems in supervised learning  the McDiarmid’s inequality
can be used to achieve concentration. However the function class we deﬁne in Eq. (4) is not bounded
almost everywhere  but with high probability  hence the classical result does not apply. Conditioning
on the event where the difference is bounded  we use an extension of McDiarmid’s inequality by [3].
For convenience  we restate a weaker version of the theorem using our notation below.
Theorem 6 ([3]). Consider independent random variable X = (X1 ···   Xn) in the product prob-
i=1 Xi  where Xi is the probability space for Xi. Also consider a function
g : X → R. If there exists a subset Y ⊂ X   and a scalar c > 0  such that
|g(x) − g(y)| ≤ L ∀x  y ∈ Y  xj = yj ∀j (cid:54)= i.

ability space X = (cid:81)n

Denote p = 1 − P (X ∈ Y)  then P (g(X) − E[g(X)|X ∈ Y] ≥ ) ≤ p + exp

(cid:16)− 2(−npL)2

nL2

+

(cid:17)

.

It is worth pointing out that in Theorem 6  the concentration is shown in reference to the conditional
expectation E[g(X)|X ∈ Y] when the data points are in the bounded difference set. So to fully
achieve the type of bound given by McDiarmid’s inequality  we need to further bound the difference
of the conditional expectation and the full expectation. Combining the two parts we will be able to
show that  the empirical difference is upper bounded using the Rademacher complexity.
Now it remains to derive the Rademacher complexity under the given function class. Note that
when the function class is a contraction  or Lipschitz with respect to another function (usually of a
simpler form)  one can use the Ledoux-Talagrand contraction lemma [12] to reduce the Rademacher
complexity of the original function class to the Rademacher complexity of the simpler function
class. This is essential in getting the Rademacher complexities for complicated function classes. As

7

we mention in Section 3  our function class in Eq. (4) is unfortunately not Lipschitz due to the fact
that it involves all cluster centers even for the gradient on one cluster. We get around this problem
by introducing a vector valued function  and show that the functions in Eq. (4) are Lipschitz in
terms of the vector-valued function. In other words  the absolute difference in the function when
the parameter changes is upper bounded by the norm of the vector difference of the vector-valued
function. Then we build upon the recent vector-contraction result from [15]  and prove the following
lemma under our setting.
Lemma 3. Let X be nontrivial  symmetric and sub-gaussian. Then there exists a constant C < ∞ 
depending only on the distribution of X  such that for any subset S of a separable Banach space and
function hi : S → R  fi : S → Rk  i ∈ [n] satisfying ∀s  s(cid:48) ∈ S |hi(s)−hi(s(cid:48))| ≤ L(cid:107)f (s)−f (s(cid:48))(cid:107).
If ik is an independent doubly indexed Rademacher sequence  we have 

(cid:88)

i

E sup
s∈S

√
ihi(s) ≤ E

(cid:88)

i k

2L sup
s∈S

ikfi(s)k 

where fi(s)k is the k-th component of fi(s).
Remark 5. In contrast to the original form in [15]  we have a S as a subset of a separable Banach
Space. The proof uses standard tools from measure theory  and is to be found in Appendix C.

This equips us to prove Proposition 1.
Proof sketch of Proposition 1. For any unit vector u  the Rademacher complexity of F u

1 is

Rn(F u

1 ) =EXE sup
µ∈A
≤ EXE sup
µ∈A

1
n

1
n

(cid:124)

n(cid:88)
n(cid:88)

i=1

i=1

(cid:123)(cid:122)

(D)

iw1(Xi; µ)(cid:104)Xi − µ1  u(cid:105)

iw1(Xi; µ)(cid:104)Xi  u(cid:105)

(cid:125)

+ EXE sup
µ∈A

(cid:124)

iw1(Xi; µ)(cid:104)µ1  u(cid:105)

(9)

(cid:125)

n(cid:88)

i=1

1
n

(E)

(cid:123)(cid:122)
(cid:18) πk

π1

(cid:19)

We bound the two terms separately. Deﬁne ηj(µ) : RM d → RM to be a vector valued function with
the k-th coordinate

[ηj(µ)]k =

(cid:107)µ1(cid:107)2

2

− (cid:107)µk(cid:107)2

2

+ (cid:104)Xj  µk − µ1(cid:105) + log

√

M
4

It can be shown that

|w1(Xj; µ) − w1(Xj; µ(cid:48))| ≤

(cid:107)ηj(µ) − ηj(µ(cid:48))(cid:107)

(10)

Now let ψ1(Xj; µ) = w1(Xj; µ)(cid:104)Xj  u(cid:105). With Lipschitz property (10) and Lemma C.1  we have

sup

µ∈A

E

n(cid:88)

j=1

1
n

jwi(Xj; µ)(cid:104)Xj  u(cid:105)

 ≤ E

√

√
2
4n

M

sup
µ∈A

n(cid:88)

M(cid:88)

j=1

k=1



jk[ηj(µ)]k

The right hand side can be bounded with tools regarding independent sum of sub-gaussian random
variables. Similar techniques apply to the (E) term. Adding things up we get the ﬁnal bound.

6 Experiments

In this section we collect some numerical results. In all experiments we set the covariance matrix
for each mixture component as identity matrix Id and deﬁne signal-to-noise ratio (SNR) as Rmin.
Convergence Rate We ﬁrst evaluate the convergence rate and compare with those given in Theorem
3 and Theorem 4. For this set of experiments  we use a mixture of 3 Gaussians in 2 dimensions. In
both experiments Rmax/Rmin = 1.5. In different settings of π  we apply gradient EM with varying
SNR from 1 to 5. For each choice of SNR  we perform 10 independent trials with N = 12  000

8

data points. The average of log (cid:107)µt − ˆµ(cid:107) and the standard deviation are plotted versus iterations. In
Figure 1 (a) and (b) we plot balanced π (κ = 1) and unbalanced π (κ > 1) respectively.
All settings indicate the linear convergence rate as shown in Theorem 3. As SNR grows  the pa-
rameter γ in GS condition decreases and thus yields faster convergence rate. Comparing left two
panels in Figure 1  increasing imbalance of cluster weights κ slows down the local convergence rate
as shown in Theorem 3.

(a)

(b)

(c)

(d)

Figure 1: (a  b): The inﬂuence of SNR on optimization error in different settings. The ﬁgures
represent the inﬂuence of SNR when the GMMs have different cluster centers and weights: (a)
π = (1/3  1/3  1/3). (b) π = (0.6  0.3  0.1). (c) plots statistical error with different initializations
arbitrarily close to the boundary of the contraction region. (d) shows the suboptimal stationary point
when two centers are initialized from the midpoint of the respective true cluster centers.

Contraction Region To show the tightness of the contraction region  we generate a mixture with
2 +µ∗
M = 3  d = 2  and initialize the clusters as follows. We use µ0
2 +   for
shrinking   i.e. increasing a/Rmin and plot the error on the Y axis. Figure 1-(c) shows that gradient
EM converges when initialized arbitrarily close to the boundary  thus conﬁrming our near optimal
contraction region. Figure 1-(d) shows that when  = 0  i.e. a = Rmin
  gradient EM can be trapped
at a sub-optimal stationary point.

2 +µ∗
2 −   µ0

3 = µ∗

2

2 = µ∗

3

3

7 Concluding Remarks

In this paper  we obtain local convergence rates and a near optimal contraction radius for popu-
lation and sample-based gradient EM for multi-component GMMs with arbitrary mixing weights.
For simplicity  we assume that the the mixing weights are known  and the covariance matrices are
the same across components and known. For our sample-based analysis  we face new challenges
where bears structural differences from the two-component  equal-weight setting  which are allevi-
ated via the usage of non-traditional tools like a vector valued contraction argument and martingale
concentrations bounds where bounded differences hold only with high probability.

Acknowledgments

PS was partially supported by NSF grant DMS 1713082.

References
[1] Pranjal Awasthi and Or Sheffet. Improved spectral-norm bounds for clustering. In APPROX-

RANDOM  pages 37–49. Springer  2012.

[2] Sivaraman Balakrishnan  Martin J. Wainwright  and Bin Yu. Statistical guarantees for the em

algorithm: From population to sample-based analysis. Ann. Statist.  45(1):77–120  02 2017.

[3] Richard Combes. An extension of mcdiarmid’s inequality. arXiv preprint arXiv:1511.05240 

2015.

[4] Denis Conniffe. Expected maximum log likelihood estimation. The Statistician  pages 317–

329  1987.

9

[5] Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science  1999.

40th Annual Symposium on  pages 634–644. IEEE  1999.

[6] Sanjoy Dasgupta and Leonard J Schulman. A two-round variant of em for gaussian mixtures.
In Proceedings of the Sixteenth conference on Uncertainty in artiﬁcial intelligence  pages 152–
159. Morgan Kaufmann Publishers Inc.  2000.

[7] Arthur P Dempster  Nan M Laird  and Donald B Rubin. Maximum likelihood from incomplete
data via the em algorithm. Journal of the royal statistical society. Series B (methodological) 
pages 1–38  1977.

[8] Zachary Friggstad  Mohsen Rezapour  and Mohammad R Salavatipour. Local search yields
a ptas for k-means in doubling metrics. In Foundations of Computer Science (FOCS)  2016
IEEE 57th Annual Symposium on  pages 365–374. IEEE  2016.

[9] Chi Jin  Yuchen Zhang  Sivaraman Balakrishnan  Martin J Wainwright  and Michael I Jordan.
Local maxima in the likelihood of gaussian mixture models: Structural results and algorithmic
In Advances in Neural Information Processing Systems  pages 4116–4124 
consequences.
2016.

[10] Amit Kumar and Ravindran Kannan. Clustering with spectral norm and the k-means algorithm.
In Foundations of Computer Science (FOCS)  2010 51st Annual IEEE Symposium on  pages
299–308. IEEE  2010.

[11] Kenneth Lange. A gradient algorithm locally equivalent to the em algorithm. Journal of the

Royal Statistical Society. Series B (Methodological)  pages 425–437  1995.

[12] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and pro-

cesses. Springer Science & Business Media  2013.

[13] Yu Lu and Harrison H Zhou. Statistical and computational guarantees of lloyd’s algorithm and

its variants. arXiv preprint arXiv:1612.02099  2016.

[14] Jirı Matouˇsek. On approximate geometric k-clustering. Discrete & Computational Geometry 

24(1):61–84  2000.

[15] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International

Conference on Algorithmic Learning Theory  pages 3–17. Springer  2016.

[16] Colin McDiarmid. On the method of bounded differences.

141(1):148–188  1989.

Surveys in combinatorics 

[17] Dustin G Mixon  Soledad Villar  and Rachel Ward. Clustering subgaussian mixtures by

semideﬁnite programming. arXiv preprint arXiv:1602.06612  2016.

[18] Mehryar Mohri  Afshin Rostamizadeh  and Ameet Talwalkar. Foundations of machine learn-

ing. MIT press  2012.

[19] Iftekhar Naim and Daniel Gildea. Convergence of the em algorithm for gaussian mixtures with

unbalanced mixing coefﬁcients. arXiv preprint arXiv:1206.6427  2012.

[20] Richard A Redner and Homer F Walker. Mixture densities  maximum likelihood and the em

algorithm. SIAM review  26(2):195–239  1984.

[21] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal

of Computer and System Sciences  68(4):841–860  2004.

[22] CF Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics  pages

95–103  1983.

[23] Ji Xu  Daniel J Hsu  and Arian Maleki. Global analysis of expectation maximization for mix-
tures of two gaussians. In D. D. Lee  M. Sugiyama  U. V. Luxburg  I. Guyon  and R. Garnett 
editors  Advances in Neural Information Processing Systems 29  pages 2676–2684. Curran
Associates  Inc.  2016.

10

[24] Lei Xu and Michael I Jordan. On convergence properties of the em algorithm for gaussian

mixtures. Neural computation  8(1):129–151  1996.

[25] Bowei Yan and Purnamrita Sarkar. On robustness of kernel clustering. In Advances in Neural

Information Processing Systems  pages 3090–3098  2016.

11

,Bowei Yan
Mingzhang Yin
Purnamrita Sarkar