2018,Size-Noise Tradeoffs in Generative Networks,This paper investigates the ability of generative networks to convert their input noise distributions into other distributions. Firstly  we demonstrate a construction that allows ReLU networks to increase the dimensionality of their noise distribution by implementing a ``space-filling'' function based on iterated tent maps. We show this construction is optimal by analyzing the number of affine pieces in functions computed by multivariate ReLU networks. Secondly  we provide efficient ways (using polylog$(1/\epsilon)$ nodes) for networks to pass between univariate uniform and normal distributions  using a Taylor series approximation and a binary search gadget for computing function inverses. Lastly  we indicate how high dimensional distributions can be efficiently transformed into low dimensional distributions.,Size-Noise Tradeoffs in Generative Networks

Bolton Bailey
Matus Telgarsky
{boltonb2 mjt}@illinois.edu

University of Illinois  Urbana-Champaign

Abstract

This paper investigates the ability of generative networks to convert their input
noise distributions into other distributions. Firstly  we demonstrate a construction
that allows ReLU networks to increase the dimensionality of their noise distribution
by implementing a “space-ﬁlling” function based on iterated tent maps. We show
this construction is optimal by analyzing the number of afﬁne pieces in functions
computed by multivariate ReLU networks. Secondly  we provide efﬁcient ways
(using polylog(1/) nodes) for networks to pass between univariate uniform and
normal distributions  using a Taylor series approximation and a binary search
gadget for computing function inverses. Lastly  we indicate how high dimensional
distributions can be efﬁciently transformed into low dimensional distributions.

1

Introduction

This paper focuses on the representational capabilities of generative networks. A generative network
models a complex target distribution by taking samples from some efﬁciently-sampleable noise
distribution and mapping them to the target distribution using a neural network. What distributions
can a generative net approximate  and how well? Larger neural networks or networks with more noise
given as input have greater power to model distributions  but it is unclear how the use of one resource
can make up for the lack of the other. We seek to describe the relationship between these resources.
In our analysis  we make a few assumptions on the structure of the network and the noise. We focus
on the two most standard choices for noise distributions: The normal distribution  and the uniform
distribution on the unit hypercube [Arjovsky et al.  2017]. Henceforth  we will use the term “uniform
distribution” to refer to the uniform distribution on unit hypercubes  unless otherwise speciﬁed. We
look speciﬁcally at the case where the generative network is a fully-connected network with the ReLU
activation function (without weight sharing). The notion of approximation we use is the Wasserstein
distance  introduced for generative networks in Arjovsky et al. [2017]  which is deﬁned as follows:
Deﬁnition 1. For two distributions µ and ν on Rd  their Wasserstein distance is deﬁned as

(cid:90)

W (µ  ν) := inf

π∈Π(µ ν)

|x − y|dπ(x  y) 

where Π(µ  ν) is the set of joint distributions having µ and ν as marginals.

Our results fall into three regimes  each covered in its own section:

Section 2: The case where the input dimension is less than the output dimension.

In this regime  we prove tight upper and lower bounds for the task of approximating higher
dimensional uniform distributions with lower dimensional distributions in terms of the
average width W and depth (number of layers) L of the network. The bounds are tight in
the sense that both give an accuracy of the form  = O(W )−O(L) (keeping input and output
dimensions ﬁxed). Thus  this gives a good idea of the asymptotic behavior in this regime:
Error exponentially decays with the number of layers  and polynomially decays with the
number of nodes in the network.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

Section 3: The case where the input and output dimensions are equal.

In this regime  we give constructions for networks that can translate between single dimen-
sional uniform and normal distributions. These constructions incur  error in Wasserstein
distance using only polylog(1/) nodes.

Section 4: The case where the input dimension is greater than the output dimension.

In this regime  we show that even with trivial networks  increased input dimension can
sometimes improve accuracy.

In the course of proving the above results  we show several lemmas of independent interest.

Multivariable afﬁne complexity lemma.

For a function f : Rn0 → Rd computed by a neural network with N nodes and L layers
and ReLU nonlinearities  the domain of f can be partitioned into O
convex
(polyhedral) pieces such that f is afﬁne on each piece. This is extends prior work  which
considered networks with only univariate input [Telgarsky  2016].

n0L

(cid:16) N

(cid:17)n0L

Taylor series approximation.

Univariate functions with quickly decaying Taylor series  such as exp  cos  and the CDF
of the standard normal  can be approximated on domains of length M with networks of
size poly(M  ln(1/)). This idea was been explored before by Yarotsky [2017]; the key
difference between this work and the prior is that our results apply directly to arbitrary
domains.

Function inversion through binary search.

The inverses of increasing functions with large enough slope can be approximated efﬁciently 
provided that the functions themselves can be approximated efﬁciently. While this technique
does not provide uniform bounds on the error  we show that it provides approximations that
are good enough for generative networks to have low error.

Detailed proofs of most theorems and lemmas can be found in the appendix.

1.1 Related Work

Generative networks have become popular in the form of Generative Adversarial Networks (GANs) 
introduced by Goodfellow et al. [2014]; see for instance [Creswell et al.  2018] for a survey of various
GAN architectures. GANs are trained using a discriminator network  an auxiliary neural network
which tries to prove the distance from the simulated distribution to the true data distribution is large.
The generator is trained by gradient descent to minimize the distance given by the adversary network.
Wasserstein GANs (or WGANs) are GANs which use an approximation of the Wasserstein distance
as this notion of distance. The concept of Wasserstein distance comes out of the theory of optimal
transport  as discussed in Villani [2003]  and its use as a performance metric is expounded in Arjovsky
et al. [2017]. WGANs have shown success in various generation tasks [Osokin et al.  2017  Donahue
et al.  2018  Chen and Tong  2017]. While this paper uses the Wasserstein distance as a performance
metric  we are not concerned with the training process  only the representational capabilities of the
networks.
Many of the results in this paper build out of the results on the representational power of neural nets
as function approximators. These results ﬁrst focused upon approximating continuous functions
with a single hidden layer [Hornik et al.  1989  Cybenko  1989]  but recently branched out to deeper
networks [Telgarsky  2016  Eldan and Shamir  2016  Yarotsky  2017  Montufar et al.  2014]. A
concurrent work in this area is Zhang et al. [2018]  which uses tropical geometry to analyze deep
networks. This work produced a result on the number of afﬁne pieces of deep networks [Zhang et al. 
2018  Theorem 6.3]  which matches our bound in Lemma 1. This bound was originally suggested
in Montufar et al. [2014]. The present work relies upon some of these recent works (e.g.  afﬁne
piece counting bounds  approximation via Taylor series)  but develops nontrivial extensions (e.g. 
multivariate inputs and outputs with tight dimension dependence  less benign Taylor series).
The representational capabilities of generative networks have previously been studied by Lee et al.
[2017]. That paper provides a result for the representation capabilities of deep neural networks
in terms of “Barron functions”  ﬁrst described in Barron [1993]  which are functions with certain
constraints on their Fourier transform. Lee et al. [2017] showed that compositions of these Barron

2

functions could be approximated well by deep neural networks. Their main result with respect to the
representation of distributions was that the result of mapping a noise distribution through a Barron
function composition could be approximated in Wasserstein distance by mapping the same noise
distribution through the neural network approximation to the Barron function composition. These
techniques do not readily permit the analysis of target distributions which are not images of the input
space under these Barron functions.
The Box-Muller transform [Box et al.  1958] is a computational method for simulating bivariate
normal distributions using uniform distributions on the unit (2-dimensional) square. The method is a
general algorithm  but it is possible to simulate the transform with specially constructed neural nets 
to prove theorems similar to those in section 3. In fact this was our original approach; an overview of
the Box-Muller implementation can be found in section 3.

1.2 Notation for Neural Networks

We deﬁne a neural network with L layers and ni nodes in the ith layer as a composition of functions
of the form

AL ◦ σnL−1 ◦ AL−1 ◦ σnL−2 ◦ ··· ◦ σn1 ◦ A1.

the map x (cid:55)→ max{x  0}. The total number of nodes N in a network is the sum(cid:80)L

Here Ai : Rni−1 → Rni is an afﬁne function. That is  Ai is the sum of a linear function and a
constant vector. σk : Rk → Rk is the k-component pointwise ReLU function  where the ReLU is
i=0 ni. We will
sometimes use n = n0 to refer to the input dimension and d = nL to refer to the output dimension.
Since a neural network is a composition of piecewise afﬁne functions  it is piecewise afﬁne. The
number of afﬁne pieces of a function f will be denoted NA(f ) or just NA.
When µ is a distribution  we will adopt the notation of Villani [2003] and use f #µ to denote the
pushforward of µ under f  i.e.  the distribution obtained by applying f to samples from µ. We will
use U (A) to denote the uniform distribution on a set A ⊂ Rn  and m(A) to denote the Lebesgue
measure of that set. We will use N to denote a normal distribution  which will always be centered on
the origin and have unit covariance matrix.

2

Increasing the Dimensionality of Noise

How easy is it to create a generator network that can output more dimensions of noise than it receives?
It is common in practice to use a far greater output dimension. Here  we give both upper and lower
bounds showing that an increase in dimension can require a large  complicated network.

2.1 Constructions for the Uniform Hypercube

For this section  we restrict ourselves to the case of input and output distributions which are uniform.
To motivate our techniques  we can simplify our problem even further: We could ask how one
might approximate a uniform distribution on the unit square using the uniform distribution on the
unit interval. We see that we are limited by the fact that the range of the generator net is some
one-dimensional curve in R2  and so the distribution that the generator net produces will have to be
supported on this curve. We will want each point of the unit square to be close to some point on the
curve so that the mass of the square can be transported to the generated distribution. We are therefore
led to consider some kind of (almost) space ﬁlling curve. An excellent candidate is the graph of the
iterated tent map  shown in Figure 2.1. This function has been useful in the past [Montufar et al. 
2014  Telgarsky  2016] since it is highly nonlinear and it can be shown that neural networks must be
large to approximate it well. We can create a construction for the univariate to multivariate network
which uses tent maps of varying frequencies to ﬁll space.
The tentmap construction  which appears in [Montufar et al.  2014] and is given in full in the appendix 
achieves the following error:
Theorem 1. Let µ and ν respectively denote uniform distributions on [0  1] and [0  1]d. Given any
number of nodes N and number of layers L satisfying N > dL  we can construct a generative

3

Figure 1: Examples of paths that come near every point in the unit square and the unit cube.

network f : [0  1] → [0  1]d such that

W (f #µ  ν) ≤

√

d

(cid:22) N − dL

(cid:23)−(cid:98) L

d−1(cid:99)

L

.

(1)

Thus  as the size of the network grows  the base in Equation 1 grows proportionally to the average
width of the network  and the exponent grows proportionally to the depth of the network (while being
inversely proportional to the number of outputs). The N > dL requirement comes from using some
nodes to carry values forward — If we were to allow connections between non-adjacent layers  this
requirement would go away and N would replace N − dL in the theorem statement.
We now consider the case where our input noise dimension is larger than 1. In this case  we see
that one possible construction involves dividing the output dimensions evenly amongst the input
dimensions and then placing multiple copies of the above described construction in parallel. This
produces the following bound:
Theorem 2. Let µ and ν respectively denote uniform distributions on [0  1]n and [0  1]d. Given any
number of nodes N and number of layers L satisfying N > dL  we can construct a generative
network f : [0  1]n → [0  1]d such that

W (f #µ  ν) ≤

√

d

(cid:22) N − dL

(cid:36)

(cid:23)−

nL

(cid:37)

L

(cid:100) d−n

n

(cid:101)

= O

(cid:18) N

nL

(cid:19)−O( nL

d )

 

where the big-O hides factors of d in the base  and constant factors in the exponent.

Note that this generalizes Theorem 1. The proof can be found in the appendix. This bound is at
its tightest when d is a multiple of n  in which case d−n
n is an integer  and the exponent matches
exactly that in the lower bound determined later. The construction works more smoothly with this
even divisibility because the output nodes can be evenly split among the inputs  and it is easier to
parallelize the construction.

2.2 Lower Bounds for the Uniform Box

We now provide matching lower bounds. For this  it sufﬁces to count the afﬁne pieces. Bounds on the
number of afﬁne pieces have been proved before  but only with univariate input [Telgarsky  2016];
here we allow the network input to be multidimensional.
Lemma 1. Let f : Rn0 → Rd be a function computed by a neural network with at most N total
nodes and L layers. Then the domain of f can be divided into NA convex pieces on which the
function is afﬁne  where

(cid:19)n0L

NA ≤

e

+ e

.

(cid:18)

N
n0L

4

This lemma has also been proven in concurrent work [Zhang et al.  2018] using the techniques of
tropical geometry. Our proof works essentially by induction on the number of layers: We look at the
set of possible activations of the ith layer  we see that it is a union of convex afﬁne sets of dimension
at most n0. The application of the ReLU maps each of these convex afﬁne sets into O(ni)n0 convex
afﬁne sets  where ni is the number of nodes in the ith layer.
The one-dimensional tent map construction tells us that for a given number of nodes and number
of layers  we can construct a function with a number of afﬁne pieces bounded by the size of the
network. When constructing multidimensional input networks with a high number of afﬁne pieces 
we can always parallelize several of these tentmaps to get a map with the product of the number of
pieces in the individual networks. What this lemma guarantees is that  up to a constant factor in
the number of nodes  this construction is optimal for producing as many afﬁne pieces as possible.
This gives us conﬁdence that our parallelized tent map construction for low-dimensional uniform to
high-dimensional uniform may be close to optimal.
To show that our construction is optimal  we need to show that it approximates the high-dimensional
uniform distribution about as accurately as any piecewise afﬁne function with the same number of
pieces NA. To do this  we will use the fact that the range of a piecewise afﬁne function is a subset of
the union of ranges of its constituent afﬁne functions. We then show that any distribution on a union
like this is necessarily distant from the target uniform distribution.
Theorem 3. Let B be a bounded measurable subset of Rd of radius l  let f : Rn → Rd be piecewise
afﬁne with n < d  and let P be any distribution on Rn. The Wasserstein distance between f #P and
the uniform distribution UB on B has the following lower bound:

W (UB  f #P ) ≥ k

where k depends on n and d.

l−n m(B)
NA

(cid:18)

(cid:19) 1

d−n

 

Note that our technique can produce bounds not just for the unit cube on n-dimensions but for any
uniform distribution on any bounded subset of Rn such as the sphere. When we combine this with
our analysis of NA in Lemma 1  we get a lower bound result for a given number of nodes and layers.
Theorem 4. Let µ and ν respectively denote uniform distributions on [0  1]n and [0  1]d. Given any
number of nodes N and number of layers L  for any generative network f : [0  1]n → [0  1]d  we
have

(cid:18)

(cid:19)− nL

d−n

(cid:18) N

(cid:19)− nL

d−n

= O

nL

 

W (f #µ  ν) ≥ k

e

N
nL

+ e

where the big-O hides factors of n and d in the base.

Proof. This follows from applying Theorem 3 taking f as a neural network with the afﬁne piece
bound from Lemma 1  and P as µ  the uniform distribution on [0  1]n.

3 Transporting between Univariate Distributions

The two most common distributions used in generative networks in practice are the uniform distribu-
tion  and the normal. How easily can one of these distributions can be used to approximate the other?
We will deal with the simplest case where our input and output distributions are one-dimensional. If
we can construct a neural net for this case  we can parallelize multiple copies of the net if we want to
move between normal and uniform distributions in higher dimensions.

3.1 Approximation of a Uniform Distribution by a Normal

Perhaps the simplest idea for approximating a uniform distribution with a generative network with
normal noise is to let the network approximate Φ  the cumulative distribution function of the normal.
To approximate Φ  we will approximate its Maclaurin series:

Φ(z) =

1
2

+

1√
2π

(−1)nz2n+1
n!(2n + 1)2n .

∞(cid:88)

n=0

5

This series has convergence properties which allow a network based on its truncation to work.
Yarotsky [2017  Proposition 3c] showed that f : (x  y) (cid:55)→ xy over [−M  M ]2 can be efﬁciently
approximated by neural networks  in the sense that there is a network with O(ln(1/) + ln(M ))
nodes and layers computing a function ˆf with |f − ˆf| ≤ . Yarotsky [2017] uses this to show that
certain functions with small derivatives could be approximated well. We will show a similar result 
which depends on the good behavior of the Taylor series of Φ.
Naturally  if ˜f is a neural network approximating f and ˜g approximates g  then we can compose these
approximations to get an approximation of the composition. In particular  if g has a Lipschitz constant 
then the composition approximation will depend on this Lipschitz constant and the accuracies of
the individual approximations. A “composition lemma” to this effect is included in the appendix as
Lemma 8. We will use this idea several times to construct a variety of function approximations.
We will now consider the method of approximating functions by approximating their Taylor series
with neural networks. To do this  we ﬁrst demonstrate a network which takes a univariate input x in
[−M  M ] and returns the multivariate output (x0  x1  x2  . . .   xn).
Theorem 5. The function f : x (cid:55)→ (x0  . . .   xn) on [−M  M ] can be computed uniformly to within
 by a neural network of size poly(n  ln(M )  ln(1/)).
The proof relies on iteratively composing the multiplication function xi = xi−1 · x using the
“composition lemma” to get each of the xi.
Now that we know the size required to approximate the powers of x  we may use this to approximate
the Maclaurin series of Φ.
Theorem 6. The function Φ can be approximated uniformly to within  by a network of size
poly(ln(1/)).

To show this we apply Theorem 5 with a suitable choice of M and n and then use the monomial
approximations to get a Taylor approximation of Φ. Knowing that we can approximate Φ well  we
can give a precise bound on the Wasserstein distance of this construction.
Theorem 7. We can construct a generative network with polylog(1/) nodes and univariate normal
noise that can output a distribution with Wasserstein distance  from uniform.

Proof. Using Theorem 6  let ˜Φ : R → [0  1] approximate Φ uniformly to within 
2. Consider the
coupling π between the output of this network and the true uniform distribution which consists of
pairs (Φ(z)  ˜Φ(z))  where z is normally distributed:

(cid:16)

U ([0  1])  ˜Φ#N(cid:17)

W

(cid:16)

Φ#N   ˜Φ#N(cid:17) ≤

(cid:90)

= W

|Φ(z) − ˜Φ(z)| ·

e−z2/2dz.

1√
2π

R

But since |Φ − ˜Φ| is less than  everywhere  this integral is no more than   so we can indeed create a
generative network of polylog(1/) nodes for this task.

3.2 Approximation of a Normal Distribution by Uniform

Having shown that normal distributions can approximate uniform distributions with polylog(1/)
nodes  let’s see if the reverse is true. For this we’ll need a few lemmas.
For analytic convenience  a few of our intermediate constructions will use networks with both ReLU
activations  as well as step functions H(x) = 1[x > 0]. Networks with these two allowed activations
have a convenient property which allows them to be used to study vanilla ReLU networks: If there is
a ReLU/Step network approximating a function f uniformly  then f can be uniformly approximated
by a comparably-sized network on all but an arbitrarily small positive subset of its domain.
Lemma 2. Let µ be a measure  and A a measurable set with µ(A) < ∞. Suppose f : Rn → Rd can
be approximated uniformly to within  on A by a function ˜f computed by a ReLU/Step network with
N nodes. Then for any ζ > 0  there exists a ReLU network with O(N ) nodes which approximates f
to within 2 on a set B where A \ B has measure less than ζ.

6

Proof. Note that while a ReLU neural network cannot implement the step function  it can implement
the following approximation:

0

x/δ
1

sδ(x) =

if x ≤ 0 
if 0 ≤ x ≤ δ 
if x > δ.

In fact  this approximation to the step function can be implemented with a 4-node ReLU network.
If we replace every step function activation node in our architecture with a copy of this four node
network  we get an architecture of size O(N ). With this architecture  we can compute each of a
sequence (fn) of functions  where in fn  all step functions from our old network are replaced by
s1/n. For any x in A  consider the minimum positive input to the step function which occurs in the
computation graph. If δ = 1/n is less than this minimum  then fn(x) = f (x)  so this sequence
converges pointwise to ˜f. Egorov’s theorem [Kolmogorov and Fomin  1975  pp. 290  Theorem
12] now tells us that (fn) converges to ˜f uniformly on a set B that satisﬁes the µ(A \ B) < ζ
requirement. Thus  there is an fn that approximates ˜f to within  uniformly on this B  and fn
therefore approximates f uniformly on B to within 2.

This lemma has a useful application to generative networks: If we make ζ sufﬁciently small  the mass
of the noise distribution on A \ B is arbitrarily small. Therefore  we can make ζ small enough that
the impact of the mistake region on the Wasserstein distance is negligible.
We now would like to approximate some function that maps the uniform distribution to the normal
in this powerful format. Complementing the use of the normal CDF Φ in the previous subsection 
here we will use its inverse Φ−1. Since we conveniently have already proved that Φ is efﬁciently
approximable  we would like a general lemma that allows us to invert this.
Lemma 3. Let f : [a  b] → [c  d] be a strictly increasing differentiable function with f(cid:48) greater than
a constant L everywhere  and let f be approximated to within  by a network of size N. Then (for
any ζ > 0)  f−1 can be approximated to within (b − a)2−t + L on (all but a measure ζ subset of)
[c  d] by a network of size O(tN ).

The proof of this lemma constructs a neural network that executes t iterations of a binary search on
[a  b]  using t copies of the approximation to f to decide which subinterval to narrow in on. Applying
this lemma to our approximation theorem for the normal CDF gives us an approximation of the
inverse of the normal CDF.
Theorem 8. For any ζ > 0  the function Φ−1 can be approximated to within  by a network of size
polylog(1/) on [Φ(− ln(1/))  Φ(ln(1/))] \ A where A is of measure ζ.

Proof. By Theorem 6 we can get the normal CDF Φ to within ln(1/)+1 with polylog(1/) nodes.
Using Lemma 3 with t = O(ln(1/))  if we choose a = − ln(1/)  b = ln(1/) then the Lipschitz
constant of Φ−1 on this interval is

Φ(cid:48)(ln(1/))−1 = O(eln(1/)2/2) = O(− ln(1/)) 

and so Lemma 3 gives a total error on the order of .

With this approximation  we can get a generative network approximation. Since the tails of the
normal distribution are small  we can ignore them by collapsing the mass of the tails into a bounded
interval. Then  by setting ζ sufﬁciently small that the Wasserstein distance contributed by the error
region is negligible  our approximation can be shown to be within  of the normal.
As a ﬁnal lemma  we note the following observation
Proposition 1. For two distributions on R  their Wasserstein distance is equal to the L1 integral of
the difference of their CDFs.

For a proof  see [Villani  2003  remark 2.19.ii]. For an intuition  note that moving a mass m from a
to b on a one-dimensional distribution changes the CDF of the distribution on [a  b] by m.
With these in place  we use get a bound for the uniform to normal construction.

7

Theorem 9. A generative network with polylog(1/) nodes and univariate uniform noise can output
a distribution with Wasserstein distance  from a normal distribution.

The proof is an application of Theorem 8 and Proposition 1.

3.2.1 The Box-Muller Transform

We’ve established the bound we sought (approximation of a normal distribution via uniform)  but in
this section we’ll show that a curious classical construction also ﬁts the bill  albeit in two dimensions.
The Box-Muller transform [Box et al.  1958] comes from the observation that if X1 and X2 are two
independent uniform distributions on the unit interval  then if we deﬁne

Z2 :=(cid:112)−2 ln(X1) sin(2πX2) 

Z1 :=(cid:112)−2 ln(X1) cos(2πX2)

10.

ζ

>

0 

and

(2)
then Z1  Z2 are independent and normally distributed. Equation 2 comes from the interpretation of

(cid:112)−2 ln(X1) and 2πX2 as r and θ in a polar-coordinate representation of the pair of normals. While
(cid:112)−2 ln(X1) cos(2πX2) (cid:112)−2 ln(X1) sin(2πX2) can be approximated to within  by a net-

this method is not as powerful as the CDF approximation method  in that it requires two dimensions
of uniform noise in order to work  it still suggests an idea for a similar theorem to Theorem 8.
(cid:55)→
Theorem
the
work of size polylog(1/) on [0  1]2 \ A where A is of measure ζ.
We provide the following proof sketch:

function

For

any

X1  X2

same reason that Φ can: Their Taylor expansion coefﬁcients decay rapidly.

• The cos and sin functions (and the exp function) can be efﬁciently computed for much the
• The ln function can be approximated in [1/2  3/2] using the Taylor series for ln(1 + x). For
inputs outside this interval  we can repeatedly multiply double/halve the input until we reach
[1/2  3/2]  use the approximation we have  then add in a constant depending on the number
of times we doubled or halved.
• The square root function  and in fact all functions of the form x (cid:55)→ xα for α > 0  can be
approximated using the approximations for exp and ln and the identity xα = exp(α ln(x)).
• Putting these together  as well as the approximation for products from Yarotsky [2017]  we

get the result.

4 From Many Dimensions to One

This section will complete the story by seeing what is gained in transporting many dimensions into
one.
To begin  let’s ﬁrst reﬂect on the bounds we have. So far  we have shown upper bounds on neural
network sizes that are polylogarithmic in 1/. A careful analysis of the previous subsection shows
that the construction uses O(ln5(1/)) for normal to uniform and O(ln18(1/)) for the uniform to
normal. We would like to know how close to optimal these exponents are. The goal of this subsection
is to quickly establish that the lower bound for this exponent is at least 1. To do this  we will make
some use of the afﬁne piece analysis from Section 2.
Note that piecewise afﬁne functions acting on the uniform distribution have structure in their CDF 
since they are a mixture of distributions induced by each individual afﬁne piece:
Proposition 2. For a piecewise afﬁne function f : [0  1] → R with NA pieces  the CDF of the a
distribution f #U ([0  1]) is a piecewise afﬁne function with at most NA + 2 pieces.

So if we can establish a bound on the accuracy with which a piecewise afﬁne function can approximate
the normal CDF  we can use the univariate afﬁne pieces lemma above to lower bound the accuracy of
any uniform univariate noise approximation of the normal. A helpful bound is given in Safran and
Shamir [2016]  from which we get:
Lemma 4. Let f be a univariate piecewise afﬁne function with NA pieces. Then

(cid:90) b

a

|Φ(x) − f (x)|dx ≥ K
N 4
A

8

for some constant K.

Putting this together with Proposition 2 and Lemma 1  we see:
Theorem 11. A generative network taking uniform noise can approximate a normal with Wasserstein
accuracy exponential in the number of nodes.

Or in other words  approximation to accuracy  requires at least O(log(1/)) nodes.
Clearly  if we wish to approximate a low-dimensional uniform distribution with a higher-dimensional
one  all we need to do is ignore some of the inputs and spit the others back out unchewed. The same
goes for normal distributions. Is there any beneﬁt at all to additional dimensions on input noise when
the target distribution is a lower dimension?
Interestingly  the answer is yes. Considering the case of approximating a univariate normal distribution
with a high dimensional distribution  we note that there is the simplistic approach which involves
summing the inputs and reasoning that the output is close to a normal distribution by the Berry-Esseen
theorem.
Theorem 12. The distribution given by summing n uniform random variables on [0  1] and normal-
izing the result has a Wasserstein distance of O( 1√

n ) from the standard normal distribution.

Note that the above approach does not use any nonlinearity at all. It simply takes advantage of the
fact that projecting a hypercube onto a line results in an approximately normal distribution. This
theorem suggests another way of approaching Theorem 9: Use the results of section 2 to increase a
1-dimensional uniform distribution to a d-dimensional uniform distribution  then apply Theorem 12
as the ﬁnal layer of that construction to get an approximately normal distribution. Unfortunately  this
≈   which means the size of
technique does not prove the polylog(1/) size: it is necessary for 1√
the network (indeed  even the size of the ﬁnal layer of the network) is polynomial in 1/.

d

5 Conclusions and Future Work

One might ask with regards to Section 3 if there are more efﬁcient constructions than the ones found
in this section  since there is a gap between the upper and lower bounds. There are other approaches
to the uniform to normal transformation  such as the Box-Muller method [Box et al.  1958] we discuss.
Future work could modify this or other methods to tighten the bounds found in this section.
An interesting open question is whether the results of Section 3 can be applied more generally to
multidimensional distributions. Suppose for example that we have a neural network that pushes a uni-
variate uniform distribution into a univariate normal distribution. We can take d copies of this network
in parallel to get a network which takes d-dimensional uniform noise  and outputs d-dimensional
normal noise. Is a parallel construction of the form described here the most efﬁcient way to create a
network that pushes forward a d-dimensional uniform distribution to a d-dimensional normal? For
that matter  if f : Rd → Rd is of the form of a univariate function evaluated componentwise on the
input  is the best neural network approximation for f of a given size a parallel construction?
Another future direction is: To what extent do training methods for generative networks relate to
these results? The results in this paper are only representational; they provide proof of what is
possible with hand-chosen weights. One could experiment with training methods to see whether
they create the “space-ﬁlling” property that is necessary for optimal increase of noise dimension.
Alternatively  one could experiment with real-world datasets to see if changing the noise distributions
while simultaneously growing or shrinking the network leaves the accuracy of the method unchanged.
We ran some simple initial experiments measuring how well GANs of different architectures and
noise distributions learned MNIST generation  and we found them inconclusive; in particular  we
could not be certain if our empirical observations were a consequence purely of representation  or
some combination of representation and training.

Acknowledgements

The authors are grateful for support from the NSF under grant IIS-1750051  and for a GPU grant
from NVIDIA.

9

References
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge
University Press  New York  NY  USA  1st edition  2009. ISBN 052111862X  9780521118620.

Martin Arjovsky  Soumith Chintala  and Léon Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th International Conference
on Machine Learning  volume 70 of Proceedings of Machine Learning Research  pages 214–
223  International Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR. URL http:
//proceedings.mlr.press/v70/arjovsky17a.html.

A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory  39(3):930–945  May 1993. ISSN 0018-9448. doi: 10.1109/
18.256500.

George EP Box  Mervin E Muller  et al. A note on the generation of random normal deviates. The

annals of mathematical statistics  29(2):610–611  1958.

Zhimin Chen and Yuguang Tong. Face super-resolution through wasserstein gans. arXiv preprint

arXiv:1705.02438  2017.

Antonia Creswell  Tom White  Vincent Dumoulin  Kai Arulkumaran  Biswa Sengupta  and Anil A
Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine  35(1):
53–65  2018.

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control 

Signals and Systems  2(4):303–314  1989.

Chris Donahue  Julian McAuley  and Miller Puckette. Synthesizing audio with generative adversarial

networks. arXiv preprint arXiv:1802.04208  2018.

Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In COLT 

2016.

Ian Goodfellow  Jean Pouget-Abadie  Mehdi Mirza  Bing Xu  David Warde-Farley  Sherjil Ozair 
Aaron Courville  and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-
tion processing systems  pages 2672–2680  2014.

K. Hornik  M. Stinchcombe  and H. White. Multilayer feedforward networks are universal approxi-

mators. Neural Networks  2(5):359–366  july 1989.

A.N. Kolmogorov and S.V. Fomin. Introductory Real Analysis. Dover Books on Mathematics. Dover
Publications  1975. ISBN 9780486612263. URL https://books.google.com/books?id=
z8IaHgZ9PwQC.

Holden Lee  Rong Ge  Tengyu Ma  Andrej Risteski  and Sanjeev Arora. On the ability of neural
nets to express distributions. 65:1271–1296  07–10 Jul 2017. URL http://proceedings.mlr.
press/v65/lee17a.html.

Guido F Montufar  Razvan Pascanu  Kyunghyun Cho  and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in neural information processing systems  pages
2924–2932  2014.

Anton Osokin  Anatole Chessel  Rafael E Carazo Salas  and Federico Vaggi. Gans for biological
image synthesis. In 2017 IEEE International Conference on Computer Vision (ICCV)  pages
2252–2261. IEEE  2017.

Iosif Pinelis. On the nonuniform berry–esseen bound. https://arxiv.org/pdf/1301.2828.pdf 

May 2013. (Accessed on 05/15/2018).

Itay Safran and Ohad Shamir. Depth separation in relu networks for approximating smooth non-linear

functions. CoRR  abs/1610.09887  2016. URL http://arxiv.org/abs/1610.09887.

10

Matus Telgarsky. beneﬁts of depth in neural networks. In Vitaly Feldman  Alexander Rakhlin  and
Ohad Shamir  editors  29th Annual Conference on Learning Theory  volume 49 of Proceedings of
Machine Learning Research  pages 1517–1539  Columbia University  New York  New York  USA 
23–26 Jun 2016. PMLR. URL http://proceedings.mlr.press/v49/telgarsky16.html.

C. Villani. Topics in Optimal Transportation. Graduate studies in mathematics. American Mathe-
matical Society  2003. ISBN 9780821833124. URL https://books.google.com/books?id=
GqRXYFxe0l0C.

Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks  94:

103–114  2017.

Liwen Zhang  Gregory Naitzat  and Lek-Heng Lim. Tropical geometry of deep neural networks.

arXiv preprint arXiv:1805.07091  2018.

11

,Bolton Bailey
Matus Telgarsky
Jiong Zhang
Hsiang-Fu Yu
Inderjit Dhillon