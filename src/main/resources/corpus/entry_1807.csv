2010,Empirical Risk Minimization with Approximations of Probabilistic Grammars,Probabilistic grammars are generative statistical models that are  useful for compositional and sequential structures. We present a framework  reminiscent of structural risk minimization  for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting.,Empirical Risk Minimization

with Approximations of Probabilistic Grammars

Shay B. Cohen

Language Technologies Institute

School of Computer Science
Carnegie Mellon University
Pittsburgh  PA 15213  USA
scohen@cs.cmu.edu

Noah A. Smith

Language Technologies Institute

School of Computer Science
Carnegie Mellon University
Pittsburgh  PA 15213  USA
nasmith@cs.cmu.edu

Abstract

Probabilistic grammars are generative statistical models that are useful for compo-
sitional and sequential structures. We present a framework  reminiscent of struc-
tural risk minimization  for empirical risk minimization of the parameters of a
ﬁxed probabilistic grammar using the log-loss. We derive sample complexity
bounds in this framework that apply both to the supervised setting and the un-
supervised setting.

1

Introduction

Probabilistic grammars are an important statistical model family used in natural language processing
[7]  computer vision [16]  computational biology [19] and more recently  in human activity analysis
[12]. They are commonly estimated using maximum likelihood estimate or variants. Such estima-
tion can be viewed as minimizing empirical risk with the log-loss [21]. The log-loss is not bounded
when applied to probabilistic grammars  and that makes it hard to obtain uniform convergence re-
sults. Such results would help in deriving sample complexity bounds  that is  bounds on the number
of training examples required to obtain accurate estimation.
To overcome this problem  we derive distribution-dependent uniform convergence results for proba-
bilistic grammars. In that sense  our learning framework relates to previous work about learning in a
distribution-dependent setting [15] and structural risk minimization [21]. Our work is also related to
[8]  which discusses the statistical properties of estimation of parsing models in a distribution-free
setting. Based on the notion of bounded approximations [1  9]  we deﬁne a sequence of increasingly
better approximations for probabilistic grammars  which we call “proper approximations.” We then
derive sample complexity bounds in our framework  for both the supervised case and the unsuper-
vised case.
Our results rely on an exponential decay in probabilities with respect to the length of the derivation
(number of derivation steps the grammar takes when generating a structure). This means that most
of the probability mass for such a distribution is concentrated on a small number of grammatical
derivations. We formalize this notion  and use it in many of our results. For applications involving
real-world data of ﬁnite size (as in natural language processing  computational biology  and so on) 
we believe this is a reasonable assumption.
The rest of the paper is organized as follows. §2 gives an overview of probabilistic grammars. §3
gives an overview of the learning setting. §4 presents proper approximations  which are approximate
concept spaces that permit the derivation of sample complexity bounds for probabilistic grammars.
§5 describes the main sample complexity results. We discuss our results in §6 and conclude in §7.

1

2 Probabilistic Grammars

(cid:81)Nk

A probabilistic grammar deﬁnes a probability distribution over grammatical derivations generated
through a step-by-step process. For example  probabilistic context-free grammars (PCFGs) generate
phrase-structure trees by recursively rewriting nonterminal symbols as sequences of “child” symbols
according to a ﬁxed set of production rules. Each rewrite of a PCFG is conditionally independent
of previous ones given one PCFG state; this Markov property permits efﬁcient inference for the
probability distribution deﬁned by the probabilistic grammar.
In this paper  we will assume that any grammatical derivation z fully determines a string x  denoted
yield(z). There may be many derivations z for a given string (perhaps inﬁnitely many for some
kinds of grammars; we assume that the number of derivations is ﬁnite). In general  a probabilistic
grammar deﬁnes the probability of a grammatical derivation z as:

hθ(z) = (cid:81)K
events. We let N = (cid:80)K
|x| denote the length of the string x  and |z| =(cid:80)K

(1)
ψk i is a function that “counts” the number of times the kth distribution’s ith event occurs in the
derivation. The θ are a collection of K multinomials (cid:104)θ1  ...  θK(cid:105)  the kth of which includes Nk
k=1 Nk denote the total number of derivation event types. D(G) denotes
the set of all possible derivations of G. We deﬁne Dx(G) = {z ∈ D(G) | yield(z) = x}. We let
i=1 ψk i(z) denote the “length” (number of

= exp(cid:80)K

event tokens) of the derivation z.
Parameter estimation for probabilistic grammars means choosing θ from complete data (“super-
vised”) or incomplete data (“semi-supervised” or “unsupervised ” the latter usually implying that
strings x are evidence but all derivations z are missing). We can view parameter estimation as iden-
tifying a hypothesis from H(G) = {hθ(z) | θ} or  equivalently  from F(G) = {− log hθ(z) |
θ}. For simplicity of notation  we assume that there is a ﬁxed grammar and use H to re-
fer to H(G) and F to refer to F(G).1 For every fθ ∈ F we have parameters θ such that

i=1 ψk i(z) log θk i

(cid:80)Nk

(cid:80)Nk

k=1

i=1 θψk i(z)

k i

k=1

k=1

fθ(z) = −(cid:80)K

(cid:80)Nk

k=1

i=1 ψk i(z) log θk i.

|z| ≥ |x|.
P(z) ≤ Lr|z|.

We will make a few assumptions about G and P(z)  the distribution that generates derivations from
D(G) (note that P does not have to be a probabilistic grammar):
• Bounded derivation length: There is an α ≥ 1 such that  for all z  |z| ≤ α|yield(z)|. Further 
• Exponential decay of derivations: There is a constant r < 1 and a constant L ≥ 0 such that
• Exponential decay of strings: Let Λ(k) = |{z ∈ D(G) | |z| = k}| be the number derivations
of length k in G. Taking r as above  then we assume there exists a constant q < 1  such that
Λ(k)rk ≤ qk. This implies that the number of derivations of length k may be exponentially large
(e.g.  as with many PCFGs)  but is bounded by (q/r)k.

• Bounded expectations of rules: There is a B < ∞ such that E[ψk i(z)] ≤ B for all k and i.
We note that  for example  these assumptions must hold for any P whose support consists of a
ﬁnite set. These assumptions also hold in many cases when P itself is a probabilistic grammar.
See supplementary material for a note about these assumptions  their empirical justiﬁcation and the
relationship to Tsybakov noise [20  15].

3 The Learning Setting

In the supervised learning setting  a set of grammatical derivations z1  . . .   zn is used to estimate θ 
implying a choice of h ∈ H that “agrees” with the training data. MLE chooses h∗ ∈ H to maximize
the likelihood of the data:

h∗ = argmax
h∈H

1
n

log h(zi) = argmin

h∈H

˜P(z) (− log h(z))

(2)

n(cid:88)

i=1

(cid:88)

(cid:124)

z∈D(G)

(cid:123)(cid:122)

Remp n(− log h)

(cid:125)

1Learning the rules in a grammar is another important problem that has received much attention [11].

2

As shown  this equates to minimizing the empirical risk  or the expected value of a particular loss
function known as log-loss. The expected risk  under P  is the (unknowable) quantity

R(− log h) =

P(z) (− log h(z)) = EP[− log h]

(cid:88)

z∈D(G)

Showing convergence of the form suph∈H |Remp n(− log h) − R(− log h)| −→
n→∞ 0 (in probability) 
(We note that suph∈H |Remp n(− log h) −
is referred to as double-sided uniform convergence.
R(− log h)| = supf∈F |Remp n(f ) − R(f )|). This kind of uniform convergence is the driving
force in showing that the empirical risk minimizer is consistent  i.e.  the minimized empirical risk
converges to the minimized expected risk. We assume familiarity with the relevant literature about
empirical risk minimization; see [21].

4 Proper Approximations
The log-loss is unbounded  so that there is no function F : D(G) → R such that  ∀f ∈ F  ∀z ∈
D(G)  f (z) ≤ F (z); i.e.  there is no envelope to uniformly bound F. This makes it difﬁcult to
obtain a uniform convergence result of supf∈F |Remp n(f ) − R(f )|. Vapnik [21  page 93] shows
that we can still get consistency for the maximum likelihood estimator  if we bound from below and
above the family of probability distributions at hand.
Instead of making this restriction  which is heavy for probabilistic grammars  we revise the learning
model according to well-known results about the convergence of stochastic processes. The revision
approximates the concept space using a sequence F1  F2  . . . and replaces two-sided uniform con-
vergence with convergence on the sequence of concept spaces. The concept spaces in the sequence
vary as a function of the number of samples we have. We next construct the sequence of concept
spaces  and in §5 we return to the learning model. Our approximations are based on the concept of
bounded approximations [1  9].
Let Fm (for m ∈ {1  2  . . .}) be a sequence of concept spaces contained in F. We will require that
as m grows larger  Fm becomes a better approximation of the original concept space F. We say that
the sequence “properly approximates” F if there exists a non-increasing function tail(m) such that
tail(m) −→
m→∞ 0  and an operator
Cm: F → Fm such that for all m larger than some M:

m→∞ 0  a non-increasing function bound(m) such that bound(m) −→
Boundedness: ∃Km ≥ 0  ∀f ∈ Fm  E(cid:2)|f| × I(|f| ≥ Km)(cid:3) ≤ bound(m)
(cid:111) ≤ tail(m)

z | Cm(f )(z) − f (z) ≥ tail(m)

Containment: Fm ⊆ F

Tightness:

(cid:110)

(cid:91)

f∈F

P

The second requirement bounds the expected values of Fm on values larger than Km. This is
required to obtain uniform convergence results in the revised model [18]. Note that Km can grow
arbitrarily large. The third requirement ensures that our approximation actually converges to the
original concept space F. We will show in §4.2 this is actually a well-motivated characterization of
convergence for probabilistic grammars in the supervised setting.
We note that a good approximation would have Km increasing fast as a function of m and tail(m)
and bound(m) decreasing fast as a function of m. As we will see in §5  we cannot have an arbitrarily
fast convergence rate (by  for example  taking a subsequence of Fm)  because the size of Km has a
great effect on the number of samples required to obtain accurate estimation.

4.1 Constructing Proper Approximations for Probabilistic Grammars

We now focus on constructing proper approximations for probabilistic grammars. We make an as-
sumption about the probabilistic grammar that ∀k  Nk = 2. For most common grammar formalisms 
this does not change the expressive power: any grammar that can be expressed using Nk > 2 can be
expressed using a grammar that has Nk ≤ 2. See supplementary material and [6].

3

We now construct Fm. For each f ∈ F we deﬁne the transformation T (f  γ) that shifts every
θk = (cid:104)θk 1  θk 2(cid:105) in the probabilistic grammar by γ:
(cid:104)1 − γ  γ(cid:105)
(cid:104)θk 1 

θk 1 < γ
θk 1 > 1 − γ

if
if
otherwise

1 − γ(cid:105)
θk 2(cid:105)

(cid:104)θk 1  θk 2(cid:105) ←

(cid:40) (cid:104)γ 

(3)

Note that T (f  γ) ∈ F for any γ ≤ 1/2. Fix a constant p > 1. For each m ∈ N  deﬁne Fm =
{T (f  m−p) | f ∈ F}.
Proposition 4.1. There exists a constant β = β(L  q  p  N ) > 0 such that Fm has the boundedness
property with Km = pN log3 m and bound(m) = m−β log m.
Proof. Let f ∈ Fm. Let Z(m) = {z | |z| ≤ log2 m}. Then  for all z ∈ Z(m) we have f (z) =
i k ψ(k  i)(p log m) ≤ pN log3 m = Km  where the ﬁrst inequality
follows from f ∈ Fm (θk i ≥ m−p) and the second from |z| ≤ log2 m. In addition  from the
requirements on P we have:

−(cid:80)
i k ψ(k  i) log θk i ≤ (cid:80)
E(cid:104)|f| × I(|f| ≥ Km)

(cid:105) ≤ pN log m

(cid:17) ≤ κ log m

qlog2 m(cid:17)

(cid:16)(cid:80)

k>log2 m LΛ(k)rkk

(cid:16)

for some constant κ > 0. Finally  for some β(L  q  p  N ) = β > 0 and some constant M  if m > M
then κ log m

(cid:16)

qlog2 m(cid:17) ≤ m−β log m.

We show now that Fm is tight with respect to F with tail(m) =
Proposition 4.2. There

f∈F{z | Cm(f )(z) − f (z) ≥ tail(m)}(cid:17) ≤ tail(m) for tail(m) =

exists an M such that

P(cid:16)(cid:83)

:

N log2 m
mp − 1

for any m > M we have:

N log2 m
mp − 1

and

Cm(f ) = T (f  m−p).

Proof. See supplementary material.

We now have proper approximations for probabilistic grammars. From this point  we use Fm to
denote the proper approximation constructed for G. We use bound(m) and tail(m) as in Proposi-
tion 4.1 and Proposition 4.2  and assume that p > 1 is ﬁxed  for the rest of the paper.

4.2 Asymptotic Empirical Risk Minimization

It would be compelling to know that the empirical risk minimizer over Fn is an asymptotic empirical
risk minimizer (in the log-loss case  this means it converges to the maximum likelihood estimate).
As a conclusion to this section about proper approximations  we motivate the three requirements
that we posed on proper approximations by showing that this is indeed true. We now unify n  the
number of samples  and m  the index of the approximation of the concept space F. Let f∗
n be the
minimizer of the empirical risk over F  (f∗
n = argminf∈F Remp n(f )) and let gn be the minimizer
of the empirical risk over Fn (gn = argminf∈Fn Remp n(f )).
Let D = {z1  ...  zn} be a sample from P(z). The operator (gn =) argminf∈Fn Remp n(f ) is an
asymptotic empirical risk minimizer if E[Remp n(gn) − Remp n(f∗
n)] → 0. Then  we have the
following:
Proposition 4.3. Let D = {z1  ...  zn} be a sample of derivations for G.
argminf∈Fn Remp n(f ) is an asymptotic empirical risk minimizer.
“one of zi ∈ D is in Z n.” Then if Fn properly approximates F then:

Then gn =
f∈F{z | Cn(f )(z) − f (z) ≥ }. Denote by A n the event

Lemma 4.4. Denote by Z n the set(cid:83)

E [Remp n(gn) − Remp n(f∗
n)]

≤ (cid:12)(cid:12)E(cid:2)Remp n(Cn(f∗

n)) | A n

(cid:3)(cid:12)(cid:12) P(A n) +(cid:12)(cid:12)E(cid:2)Remp n(f∗

n) | A n

(cid:3)(cid:12)(cid:12) P(A n) + tail(n)

(4)

where the expectations are taken with respect to the dataset D. (See the supplementary material for
a proof.)

4

Proof of Proposition 4.3. Let f0 ∈ F be the concept that puts uniform weights over θ  i.e.  θk =
(cid:104) 1
2   1

2(cid:105) for all k. Note that |E[Remp n(f∗

≤ |E[Remp n(f0) | A n]|P(A n) = log 2

Let Aj  n for j ∈ {1  . . .   n} be the event “zj ∈ Z n”. Then A n =(cid:83)

k i

E[ψk i(zl) | A n]P(A n)

j Aj  n. We have that:

(cid:80)

(cid:80)n
n) | A n]|P(A n)
(cid:80)

l=1

n

E[ψk i(zl) | A n]P(A n) ≤(cid:80)
≤ (cid:80)
≤ (cid:16)(cid:80)

P(zl)P(Aj  n)|zl| +(cid:80)

(cid:80)

(cid:17)

j(cid:54)=l

zl

zl

j

(8)
(9)
where Eq. 7 comes from zl being independent and B is the constant from §2. Therefore  we have:

B + E[ψk i(z) | z ∈ Z n]P(z ∈ Z n)
≤ (n − 1)BP(z ∈ Z n) + E[ψk i(z) | z ∈ Z n]P(z ∈ Z n)

P(Aj  n)

j(cid:54)=l

n(cid:88)

(cid:88)

E[ψk i(zl) | A n]P(A n) ≤(cid:88)

(cid:0)E[ψk i(z) | z ∈ Z n]P(z ∈ Z n) + n2BP(z ∈ Z n)(cid:1)

P(zl  Aj  n)|zl|

P(zl  Al  n)|zl|

zl

(5)

(6)
(7)

1
n

l=1

k i

(10)
From the construction of our proper approximations (Proposition 4.2)  we know that only derivations
of length log2 n or greater can be in Z n. Therefore:

k i

E[ψk i | Z n]P(Z n) ≤ (cid:88)

z:|z|>log2 n

∞(cid:88)

l>log2 n

P(z)ψk i(z) ≤

LΛ(l)rll ≤ κqlog2 n = o(1)

(11)

where κ > 0 is a constant. Similarly  we have P(z ∈ Z n) = o(n−2). This means that
n) | A n]|P(A n) −→
|E[Remp n(f∗
n)) |
A n]|P(A n) −→
n→∞ 0 using the same proof technique we used above  while relying on the fact
that Cn(f∗

In addition  it can be shown |E[Remp n(Cn(f∗

n) ∈ Fn  and therefore Cn(f∗

n)(z) ≤ pN|z| log n.

n→∞ 0.

5 Sample Complexity Results

We now give our main sample complexity results for probabilistic grammars. These results hinge
on the convergence of supf∈Fn |Remp n(f )− R(f )|. The rate of this convergence can be fast  if the
covering numbers for Fn do not grow too fast.
We next give a brief overview of covering numbers. A cover gives a way to reduce a class of
functions to a much smaller (ﬁnite  in fact) representative class such that each function in the original
class is represented using a function in the smaller class. Let G be a class of functions. Let d(f  g)
be a distance measure between two functions f  g from G. An -cover is a subset of G  denoted by
G(cid:48)  such that for every f ∈ G there exists an f(cid:48) ∈ G(cid:48) such that d(f  f(cid:48)) < . The covering number
N(  G  d) is the size of the smallest -cover of G using with respect to the distance measure d.
We will be interested in a speciﬁc distance measure that is dependent on the empirical distribution
˜P that describes the data z1  ...  zn. Let f  g ∈ G. We will use:

(f  g) = E˜P[|f − g|] = (cid:80)

d˜P

(cid:80)n
i=1 |f (zi) − g(zi)|

z∈D(G) |f (z) − g(z)| ˜P(z) = 1

n

(12)
Instead of using N(  G  d˜P
) directly  we are going to bound this quantity with N(  G) =
sup˜P N(  G  d˜P
)  where we consider all possible samples (yielding ˜P). The following is the key
result about the connection between covering numbers and the double-sided convergence of the
empirical process supf∈Fn |Remp n(f ) − R(f )| as n → ∞:
Lemma 5.1. Let Fn be a permissible class2 of functions such that for every f ∈ Fn we have
E[|f|I(|f| ≤ Kn)] ≤ bound(n). Let Ftruncated n = {f × I(f ≤ Kn) | f ∈ Fm}  i.e.  the set of
2The “permissible class” requirement is a mild regularity condition about measurability that holds for proper

approximations. We refer the reader to [18] for more details.

5

functions from Fn after being truncated by Kn. Then for  > 0 we have 
≤ 8N(/8  Ftruncated n) exp

|Remp n(f ) − R(f )| > 2

P

(cid:33)

(cid:32)

sup
f∈Fn

(cid:18)

(cid:19)

− 1
128

n2/K 2
n

+ 2bound(n)/

provided n ≥ K 2

n/42.

Proof. See [18] (chapter 2  pages 30–31). See supplementary material for an explanation.

Covering numbers are rather complex combinatorial quantities that are hard to compute directly.
Fortunately  they can be bounded by using the pseudo dimension [3]  a generalization of VC di-
mension for real functions. In the case of our “binomialized” probabilistic grammars  the pseudo
dimension of Fn is bounded by N  because we have Fn ⊆ F  and the functions in F are linear with
N parameters. Hence  Ftruncated n has also pseudo dimension that is at most N. We have:
Lemma 5.2. (From [18  13].) Let Fn be the proper approximations for probabilistic grammars  for
any 0 <  < Kn we have:

(cid:18) 2eKn

log

2eKn





(cid:19)N

N(  Ftruncated n) < 2

5.1 Supervised Case

(13)

(14)

(15)

Lemmas 5.1 and 5.2 can be combined to get our main sample complexity result:
Theorem 5.3. Let G be a grammar. Let Fn be a proper approximation for the corresponding family
of probabilistic grammars. Let P(x  z) be a distribution over derivations that satisﬁes the require-
ments in §2. Let z1  ...  zn be a sample of derivations. Then there exists a constant β(L  q  p  N ) and
constant M such that for any 0 < δ < 1 and 0 <  < 1 and any n > M and if

(cid:27)

n ≥ max

then we have

2N log(16eKn/) + log

32
δ

 

log 4/δ + log 1/

β(L  q  p  N )

|Remp n(f ) − R(f )| ≤ 2

≥ 1 − δ

(cid:18)

(cid:26) 128K 2
(cid:32)

2

n

P

sup
f∈Fn

(cid:19)
(cid:33)

where Kn = pN log3 n.

Proof. Omitted for space. β(L  q  p  N ) is the constant from Proposition 4.1. The proof is based on
simple algebraic manipulation of the right side of Eq. 13 while relying on Lemma 5.2.

5.2 Unsupervised Case

f(cid:48)

θ as:

(cid:16)(cid:80)K

In the unsupervised setting  we have n yields of derivations from the grammar  x1  ...  xn  and our
goal again is to identify grammar parameters θ from these yields. Our concept classes are now the
sets of log marginalized distributions from Fn. For each fθ ∈ Fn  we deﬁne f(cid:48)

z∈Dx(G) exp(−fθ(z)) = − log(cid:80)

(cid:80)Nk
θ(x) = − log(cid:80)
unsupervised setting. Let f(cid:48) ∈ F(cid:48). Let f be the concept in F such that f(cid:48)(x) =(cid:80)

n(f(cid:48))(x) =(cid:80)

n(f(cid:48)) as a ﬁrst step towards deﬁning F(cid:48)

(16)
n. We deﬁne analogously F(cid:48). Note that we also need to deﬁne
n as proper approximations (for F(cid:48)) in the
z f (z  x). Then

We denote the set of {f(cid:48)
the operator C(cid:48)
we deﬁne C(cid:48)
It is not immediate to show that F(cid:48)
n is a proper approximation for F(cid:48). It is not hard to show that the
boundedness property is satisﬁed with the same Kn and the same form of bound(n) as in Proposi-
tion 4.1 (we would have (cid:48)
bound(m) = m−β(cid:48) log m for some β(cid:48)(L  q  p  N ) = β(cid:48) > 0). This relies
on the property of bounded derivation length of P. See the supplementary material for a proof. The
following result shows that we have tightness as well:

z Cn(f )(x  z).

i=1 ψi k(z)θi k

θ} by F(cid:48)

z∈Dx(G) exp

(cid:17)

k=1

6

i bi ≥  then there exists an i such that

> M we have:

N log2 n
np − 1

and the

n(f(cid:48))(x) − f(cid:48)(x) ≥ tail(n)}(cid:17) ≤ tail(n) for tail(n) =

exists an M such that

for any n

Sketch of proof of Proposition 5.4. From Utility Lemma 5.5 we have:

f(cid:48)∈F(cid:48){x | C(cid:48)

n(f ) as deﬁned above.

Proposition 5.4. There

operator C(cid:48)
− log ai + log bi ≥ .

P(cid:16)(cid:83)
i ai + log(cid:80)
Utility Lemma 5.5. For ai  bi ≥ 0  if − log(cid:80)
 (cid:91)
(cid:91)
 ≤ P
P(cid:16)(cid:83)
f∈F{x | ∃z s.t. Cn(f )(z) − f (z) ≥ tail(n)}(cid:17)
≤ (cid:88)

n(f(cid:48))(x) − f(cid:48)(x) ≥ tail(n)}

P(x) ≤

∞(cid:88)

{x | C(cid:48)

f(cid:48)∈F(cid:48)

f∈F

P

x∈X(n)
LΛ(k)rk ≤ tail(n)

(17)
Deﬁne X(n) to be all x such that there exists a z with yield(z) = x and |z| ≥ log2 n. From the
proof of Proposition 4.2 and the requirements on P  we know that there exists an α ≥ 1 such that

≤ (cid:88)

P(x)

{x | ∃zCn(f )(z) − f (z) ≥ tail(n)}



(18)

x:|x|≥log2 n/α

k=(cid:98)log2 n/α(cid:99)

where the last inequality happens for some n larger than a ﬁxed M.
Computing either the covering number or the pseudo dimension of F(cid:48)
n is a hard task  because the
function in the classes includes the “log-sum-exp.” In [9]  Dasgupta overcomes this problem for
Bayesian networks with ﬁxed structure by giving a bound on the covering number for (his respective)
F(cid:48) that depends on the covering number of F.
Unfortunately  we cannot fully adopt this approach  because the derivations of a probabilistic gram-
mar can be arbitrarily large. We overcome this problem using the following restriction. We assume
that |Dx(G)| < d(n)  where d is a function mapping n  the size of our sample  to a real number.
The more samples we have  the more permissive (for large derivation set) the grammar can be. On
the other hand  the more accuracy we desire  the more restricted we are in choosing grammars that
have a large derivation set. We refer to this restriction as the “derivational condition.” With the
derivational condition  we can show the following result:
Proposition 5.6. (Hidden Variable Rule for Probabilistic Grammars) Under the derivational condi-
tion  N(  F(cid:48)

truncated n) ≤ N(/d(n)  Ftruncated n).

The proof of Proposition 5.6 is almost identical to the proof of the hidden variable rule in [9]. For
the unsupervised case  then  we get the following sample complexity result:
Theorem 5.7. Let G be a grammar. Let F(cid:48)
n be a proper approximation for the corresponding
family of probabilistic grammars. Let P(x  z) be a distribution over derivations that satisﬁes the
requirements in §2. Let x1  ...  xn be a sample of strings from P(x). Then there exists a constant
β(cid:48)(L  q  p  N ) and constant M such that for any 0 < δ < 1 and 0 <  < 1 and any n > M and if

n ≥ max

2N log(16eKnd(n)/) + log

 

log 4/δ + log 1/
β(cid:48)(L  q  p  N )

and |Dx(G)| < d(n)  we have that

(cid:18)

(cid:26) 128K 2
(cid:32)

2

n

P

sup
f∈F(cid:48)

n

(cid:19)

32
δ

(cid:33)

(cid:27)

(19)

(20)

|Remp n(f ) − R(f )| ≤ 2

≥ 1 − δ

where Kn = pN log3 n.

For this sample complexity bound to be non-trivial  for example  we can restrict Dx(G)  through
d(n)  to have a polynomial size in the number of our samples. Enlarging d(n) is possible even to an
exponential function of nρ for ρ < 1  e.g. d(n) = 2

n.

√

7

criterion
tightness of proper approx-
imation
sample complexity bound

as Kn increases . . .
improves

as d(n) increases . . .
no effect

as p increases . . .
improves

degrades

degrades

degrades

Table 1: Trade-off between quantities in our learning model and effectiveness of different criteria.
d(n) is the function that gives the derivational condition  i.e.  |Dx(G)| ≤ d(n).

6 Discussion

Our framework can be specialized to improve the two main criteria that have a trade-off: the tight-
ness of the proper approximation and the sample complexity. For example  we can improve the
tightness of our proper approximations by taking a subsequence of Fn. However  this will make the
sample complexity bound degrade  because Kn will grow faster. Table 1 gives the different trade-
offs between parameters in our model and the effectiveness of learning. In general  we would want
the derivational condition to be removed (choose d(n) = ∞  or at least allow d(n) = Ω(tn) for
some t  for small samples)  but in that case our sample complexity bounds become trivial.
In the supervised case  our result states that the number of samples we require (as an upper bound)
grows mostly because of a term that behaves O(N 3 log N ) (for a ﬁxed δ and ). If our grammar  for
example  is a PCFG  then N depends on the total number of rules. When the PCFG is in Chomsky
normal form and lexicalized [10  7]  then N grows by an order of V 2  where V is the vocabulary size.
This means that the bound grows by an order of O(V 6 log V ). This is consistent with conventional
wisdom that lexicalized grammars require much more data for accurate learning.
The dependence of the bound on N suggests that it is easier to learn models with a smaller grammar
size. This may help explain the success of recent advances in supervised parsing [4  22  17] that
have “coarse” models (with a much smaller size of nontermimals) as a ﬁrst pass. Those models are
easier to learn and require less data to be accurate  and can serve as base models for later phases.
The sample complexity bound for the unsupervised case suggests that we need log d(n) times as
much data to achieve estimates as good as those for supervised learning. Interestingly  with unsu-
pervised grammar learning  available training sentences longer than a maximum length (e.g.  10) are
often ignored; see [14].
We note that sample complexity is not the only measure for the complexity of estimating probabilis-
tic grammars. In the unsupervised setting  for example  the computational complexity of ERM is
NP hard for PCFGs [5] or probabilistic automata [2].

7 Conclusion

We presented a framework for learning the parameters of a probabilistic grammar under the log-loss
and derived sample complexity bounds for it. We motivated this framework by showing that the
empirical risk minimizer for our approximate framework is an asymptotic empirical risk minimizer.
Our framework uses a sequence of approximations to a family of probabilistic grammars  which
improves as we have more data  to give distribution dependent sample complexity bounds in the
supervised and unsupervised settings.

Acknowledgements

We thank the anonymous reviewers for their comments and Avrim Blum  Steve Hanneke  and Dan
Roth for useful conversations. This research was supported by NSF grant IIS-0915187.

References
[1] N. Abe  J. Takeuchi  and M. Warmuth. Polynomial learnability of probabilistic concepts with
respect to the Kullback-Leiber divergence. In ACM Conference on Computational Learning
Theory  1990.

8

[2] N. Abe and M. Warmuth. On the computational complexity of approximating distributions by

probabilistic automata. Machine Learning  2:205–260  1992.

[3] M. Anthony and P. L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge

University Press  1999.

[4] E. Charniak and M. Johnson. Coarse-to-ﬁne n-best parsing and maxent discriminative rerank-

ing. In Proc. of ACL  2005.

[5] S. B. Cohen and N. A. Smith. Viterbi training for PCFGs: Hardness results and competitiveness

of uniform initialization. In Proceedings of ACL  2010.

[6] S. B. Cohen and N. A. Smith. Empirical risk minimization for probabilistic grammars: Sample

complexity and hardness of learning  in preparation.

[7] M. Collins. Head-driven statistical models for natural language processing. Computational

Linguistics  29:589–637  2003.

[8] M. Collins. Parameter estimation for statistical parsing models:

theory and practice of
distribution-free methods. Text  Speech and Language Technology (new developments in pars-
ing technology)  pages 19–55  2004.

[9] S. Dasgupta. The sample complexity of learning ﬁxed-structure Bayesian networks. Machine

Learning  29(2-3):165–180  1997.

[10] J. Eisner. Three new probabilistic models for dependency parsing: An exploration. In Proc. of

COLING  1996.

[11] E. M. Gold. Language identiﬁcation in the limit. Information and Control  10(5):447–474 

1967.

[12] G. Guerra and Y. Aloimonos. Discovering a language for human activity. In AAAI Workshop

on Anticipation in Cognitive Systems  2005.

[13] D. Haussler. Decision-theoretic generalizations of the PAC model for neural net and other

learning applications. Information and Computation  100:78–150  1992.

[14] D. Klein and C. D. Manning. Corpus-based induction of syntactic structure: Models of depen-

dency and constituency. In Proc. of ACL  2004.

[15] V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization.

The Annals of Statistics  34(6):2593–2656  2006.

[16] L. Lin  T. Wu  J. Porway  and Z. Xu. A stochastic graph grammar for compositional object

representation and recognition. Pattern Recognition  8  2009.

[17] S. Petrov and D. Klein. Improved inference for unlexicalized parsing. In Proc. of HLT-NAACL 

2007.

[18] D. Pollard. Convergence of Stochastic Processes. New York: Springer-Verlag  1984.
[19] Y. Sakakibara  M. Brown  R. Hughey  S. Mian  K. Sj¨olander  R. C. Underwood  and D. Haus-
sler. Stochastic context-free grammars for tRNA modeling. Nucleic Acids Research  22  1994.
[20] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics 

32(1):135–166  2004.

[21] V. N. Vapnik. Statistical Learning Theory. Wiley-Interscience  1998.
[22] D. Weiss and B. Taskar. Structured prediction cascades. In Proceedings of AISTATS  2010.

9

,Xiaoqin Zhang
Di Wang
Zhengyuan Zhou
Yi Ma
Balamurugan Palaniappan
Francis Bach