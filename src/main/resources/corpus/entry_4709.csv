2018,Rest-Katyusha: Exploiting the Solution's Structure via Scheduled Restart Schemes,We propose a structure-adaptive variant of the state-of-the-art stochastic variance-reduced gradient algorithm Katyusha for  regularized empirical risk minimization. The proposed method is able to exploit the intrinsic low-dimensional structure of the solution  such as sparsity or low rank which is enforced by a non-smooth regularization  to achieve even faster convergence rate. This provable algorithmic improvement is done by restarting the Katyusha algorithm according to restricted strong-convexity constants. We demonstrate the effectiveness of our approach via numerical experiments.,Rest-Katyusha: Exploiting the Solution’s Structure

via Scheduled Restart Schemes

Junqi Tang

School of Engineering

University of Edinburgh  UK

J.Tang@ed.ac.uk

Mohammad Golbabaee

Department of Computer Science

University of Bath  UK

M.Golbabaee@bath.ac.uk

Francis Bach
INRIA - ENS

PSL Research University  France

Francis.Bach@inria.fr

Mike Davies

School of Engineering

University of Edinburgh  UK
Mike.Davies@ed.ac.uk

Abstract

We propose a structure-adaptive variant of a state-of-the-art stochastic variance-
reduced gradient algorithm Katyusha for regularized empirical risk minimization.
The proposed method is able to exploit the intrinsic low-dimensional structure
of the solution  such as sparsity or low rank which is enforced by a non-smooth
regularization  to achieve even faster convergence rate. This provable algorithmic
improvement is done by restarting the Katyusha algorithm according to restricted
strong-convexity (RSC) constants. We also propose an adaptive-restart variant
which is able to estimate the RSC on the ﬂy and adjust the restart period automati-
cally. We demonstrate the effectiveness of our approach via numerical experiments.

1

Introduction

(cid:80)n

Many applications in supervised machine learning and signal processing share the same goal  which
is to estimate the minimizer of a population risk via minimizing the empirical risk 1
i=1 fi(ai  x) 
where ai  x ∈ Rd  each fi is a convex and smooth function [1]. In supervised machine learning  ai is
n
often referred to as the training data sample  while in signal/image processing applications it is the
representation of measurements. In practice the number of data samples or measurements is limited 
and from them we attempt to infer x† ∈ Rd which is the unique minimizer of the population risk:
(1)
The ultimate goal is to get a vector x(cid:63) which is a good approximation of x† from the empirical risk.
Since in many interesting applications  the dimension of parameter space d is of the same order or
even larger than the number of data samples n  minimizing the empirical risk alone will introduce
overﬁtting and hence leads to poor estimation of the true parameter x† [2  3]. In general  avoiding
overﬁtting is a key issue in both machine learning and signal processing  and the most common
approach is to add some regularization while minimizing the empirical risk [4  5  6]:

x† = arg min

x

Ea

¯f (a  x).

F (x) := f (x) + λg(x)

fi(x) 

(2)

where for the sake of compactness of notation  we denote fi(x) := fi(ai  x). Each fi is assumed to
be convex and have L-Lipschitz continuous gradient  while the regularization term g(x) is assumed
to be a simple convex function and possibly non-smooth.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

(cid:26)

x(cid:63) ∈ arg min
x∈Rd

(cid:27)

  f (x) :=

1
n

n(cid:88)

i=1

1.1 Accelerated Stochastic Variance-Reduced Optimization

To handle the empirical risk minimization in the “big data” and “big dimension” regimes  stochastic
gradient-based iterative algorithms are most often considered. The most basic one is often referred to
as stochastic gradient descent (SGD) [7  8]  in every iteration of which only one or a few functions fi
are randomly selected  and only their gradients are calculated as an estimation of the full gradient.
However  the convergence rate of SGD is sub-linear even when the loss function F is strongly-convex.
To further accelerate the stochastic gradient descent algorithm  researchers have recently developed
techniques which progressively reduce the variance of stochastic gradient estimators  starting from
SAG [9  10]  SDCA [11]  then SVRG [12  13] and SAGA [14]. Such methods enjoy a linear conver-
gence rate when the cost function F is µ-strongly-convex and each fi has L-Lipschitz continuous
gradients  that is  to achieve an output ˆx which satisﬁes F (ˆx) − F (x(cid:63)) ≤ δ  the total number of
δ . Nesterov’s acceleration [15  16  17]
has also been successfully applied to construct variance-reduced methods which have an accelerated
linear-convergence rate [18  19  20  21  22  23  24  25]:

stochastic gradient evaluations needed is O(cid:0)n + L/µ(cid:1) log 1

(cid:18)

(cid:115)

(cid:19)

O

n +

nL
µ

log

1
δ

.

(3)

It is worth noting that this convergence rate has been shown to be worst-case optimal [21]. However 
all of these algorithms need explicit knowledge of the strong-convexity parameter µ. Very recently 
[26] has shown theoretically that it is impossible for an accelerated incremental gradient method to
achieve this ideal linear rate without the knowledge of µ. Since in general the strong-convexity is
hard to be estimated accurately  researchers propose adaptive restart schemes [27  28  29  30  25  31]
for accelerated ﬁrst-order methods  either by the means of enforcing monotonicity on functional
decay  or by estimating the strong-convexity on the ﬂy.

1.2 Solution’s Structure  Restricted Strong Convexity  and Faster Convergence

In many interesting large-scale optimization problems in machine learning  the solution x(cid:63) in (2) has
some low-dimensional structure such as sparsity [4]  group-sparsity [32]  low-rank [33] or piece-wise
smoothness [6]  enforced by the non-smooth regularization. It is intuitive that an optimal algorithm
for this type of problem should take into account and exploit such solution’s structure. We believe
that  when being utilized properly  this prior information of the solution will facilitate the convergence
of an iterative algorithm.
One important theoretical cornerstone is the modiﬁed restricted strong convexity framework presented
by Agarwal et al. [34]. In the context of statistical estimation with high-dimensional data where
the usual strong-convexity assumption is vacuous  these authors have shown that the proximal
gradient descent method is able to achieve global linear convergence up to a point x which satisﬁes
(cid:107)x − x(cid:63)(cid:107)2 = o((cid:107)x(cid:63) − x†(cid:107)2)  the accuracy level of statistical precision. Moreover  the results based
on this restricted strong-convexity framework indicate that the convergence rate of the proximal
gradient become faster when the model complexity of the solution is lower.
Inspired by Agarwal et al. [34]  Qu and Xu [35] extend this framework to analyse some variance-
reduced stochastic gradient methods such as proximal SVRG [13]. Most recently  based on the same
framework  researchers proposed a two-stage APCG algorithm [36  37]  an accelerated coordinate
descent method able to exploit the solution’s structure for faster convergence. Moreover  in the context
of constrained optimization  researchers have also proposed efﬁcient sketching-based algorithms
[38  39  40  41  42] under a similar notion of conic restricted strong-convexity.

1.3 This Work

In this paper we extend the theoretical framework for randomized ﬁrst order methods established
in [36] to design and analyse a structure-adaptive variant of Katyusha [23]. Our proposed method Rest-
Katyusha is a restarted version of the original Katyusha method for non-strongly convex functions 
where the restart period is determined by the modiﬁed restricted strong-convexity (RSC). The
convergence analysis of Rest-Katyusha algorithm is provided  wherein we prove linear convergence
up to a statistical accuracy with an accelerated convergence rate characterized by the RSC property.

2

Like all other accelerated gradient methods which require the explicit knowledge of strong-convexity
parameter to achieve accelerated linear convergence  the vanilla Rest-Katyusha method also need
to explicitly know the RSC parameter. We therefore propose a practical heuristic (adaptive Rest-
Katyusha) which estimates the RSC parameter on the ﬂy and adaptively tune the restart period  and
we show that this adaptive scheme mimics the convergence behavior of the vanilla Rest-Katyusha.
Finally we validate the effectiveness of our approach via numerical experiments.

2 Restarted Katyusha Algorithm

The Katyusha algorithm [23] listed in Algorithm 1 is an accelerated stochastic variance-reduced gra-
dient method extended from the linear-coupling framework for constructing accelerated methods [43].
Its main loop (denoted as A in Algorithm 1) at iteration s is described as the following:

For k = 0  1  2  ...  m

 xk+1 = θzk + 1

2 ˆxs + ( 1

→ Linear coupling
(cid:79)k+1 = (cid:79)f (ˆxs) + (cid:79)fi(xk+1) − (cid:79)fi(ˆxs); → Variance reduced stochastic gradient
2 + (cid:104)(cid:79)k+1  z(cid:105) + λg(z); → Proximal mirror descent
zk+1 = arg minz
2 + (cid:104)(cid:79)k+1  y(cid:105) + λg(y);→ Proximal gradient descent
yk+1 = arg miny

2 − θ)yk;
2 (cid:107)z − zk(cid:107)2
2 (cid:107)y − xk+1(cid:107)2

3θL

3L

m

The output sequence of A is deﬁned as ˆxs+1 = 1
j=1 yj  ys+1 = ym  zs+1 = zm. It is one of the
state-of-the-art methods for empirical risk minimization and matches the complexity lower-bound
for minimizing smooth-convex ﬁnite-sum functions  proven by Lan and Zhou [21]. Most notably  it
is a primal method which directly1 accelerates stochastic variance-reduction methods. To achieve
acceleration in the sense of Nesterov  Katyusha introduces the three-point coupling strategy which
includes a combination of Nesterov’s momentum and a stabilizing negative momentum which cancels
the effect of noisy updates due to stochastic gradients. However  its accelerated linear convergence is
only established when the regularization term g(x) is strongly-convex  and fails to beneﬁt from the
strong convexity from the data-ﬁdelity term [31]  or the intrinsic restricted strong-convexity [36].

(cid:80)m

Algorithm 2 Rest-Katyusha (x0  µc  S0  β  T  L)

(cid:24)

(cid:113)

(cid:25)

;

β

32 + 24L
mµc

Initialize: m = 2n  S =
First stage —- warm start:
x1 = Katyusha (x0  m  S0  L)
Second stage —- exploit the restricted strong-
convexity via periodic restart:
for t = 1  ...  T do

xt+1 = Katyusha (xt  m  S  L)

end for

Algorithm 1 Katyusha (x0  m  S  L)

Initialize: y0 = z0 = ˆx0;
for s = 0  . . .   S − 1 do

θ ← 2

s+4  calculate (cid:79)f (ˆxs) 
(ˆxs+1  ys+1  zs+1)
= A(ˆxs  ys  zs  θ  (cid:79)f (ˆxs)  m)

end for
Output: ˆxS

(cid:108)
(cid:109)
4(cid:112)L/µ
F (xk) − F (cid:63) ≤ 4L(cid:107)x0 − x(cid:63)(cid:107)2
(cid:25)
(cid:113) L

(cid:24)

k2

2

(cid:108)
(cid:109)
4(cid:112)L/µ

Restart to rescue: it is well-known that if the cost function F (x) is µ-strongly convex  one can
periodically restart the accelerated full gradient method [16]  and improve it from a sublinear
convergence rate F (xk) − F (cid:63) ≤ 4L(cid:107)x0−x(cid:63)(cid:107)2
to a linearly convergent algorithm. For instance if we
set k =

4:
  then one can show that the suboptimality can be reduced by 1

k2

2

≤ 4L[F (x0) − F (cid:63)]

µk2

≤ 1
4

[F (x0) − F (cid:63)].

(4)

iteration)  and only k ≥

Then we can recursively apply this statement (algorithmically speaking  we restart the algorithm every
δ iterations are needed to make F (xk) − F (cid:63) ≤ δ 
and hence an accelerated linear rate is achieved. The restart scheme has been recently applied to
improve the convergence of the accelerated coordinate descent method [45  30] and accelerated
variance-reduced dual-averaging method [25] for strongly-convex functions.

log4

4

µ

1

1On the other hand  one can indirectly accelerate SVRG/SAGA via Catalyst [44].

3

Inspired by Nesterov [16] we ﬁrst propose the Katyusha method with periodic restarts  and meanwhile
demonstrate that when the restart period is appropriately chosen  the proposed method is able to
exploit the restricted strong-convexity property to achieve an accelerated linear convergence  even
when the cost function itself is not strongly-convex. We propose to warm start the algorithm prior to
the periodic restart stage  by running the Katyusha algorithm for a number of epochs  which in theory
should be proportional to the suboptimality of the starting point x0. We present our Rest-Katyusha
method as Algorithm 2.

3 Convergence Analysis of Rest-Katyusha

3.1 Generic Assumptions

We start by listing out the assumptions we may engage with in our analysis:
A. 1. (Decomposable regularizer) [34] Given a orthogonal subspace pair (M M⊥) in Rd  g(.) is
decomposable which means:

g(a + b) = g(a) + g(b) ∀a ∈ M  b ∈ M⊥.

(5)

In this paper we focus on cases where the regularizer is decomposable  which includes many popular
regularization which can enforce low-dimensional structure  such as (cid:96)1 norm  (cid:96)1 2 norm and nuclear
norm penalty. The subspace M is named the model subspace  while its orthogonal complement M⊥
is called perturbation subspace. Similar notion of decomposition would extend the scope of this
work to more general gauge functions g(.)  such as the so-called analysis priors  e.g.  total variation
regularization (for more details see Vaiter et al. [46]).
A. 2. (Restricted strong convexity) [34] The function f (.) satisﬁes restricted strong convexity with
respect to g(.) with parameters (γ  τ) if the following inequality holds true:

f (x) − f (x(cid:63)) − (cid:104)(cid:79)f (x(cid:63))  x − x(cid:63)(cid:105) ≥ γ
2

(cid:107)x − x(cid:63)(cid:107)2

2 − τ g2(x − x(cid:63))  ∀x ∈ Rd.

(6)

In [34]  γ is referred as the lower curvature parameter  while τ is named the tolerance parameter. It is
clear that if τ = 0  A.2 reduces to usual strong-convexity assumption. While in the high-dimensional
setting  the strong-convexity does not hold  but it has been shown in literature that such milder
assumption of RSC does hold in many situations. This notion of RSC distinguishes from other forms
of weak strong-convexity assumption based on the Polyak-Lojasiewicz inequality [47] for the purpose
of this work  because it encodes the direction-restricting effect of the regularization  and hence has
been shown to have a direct connection with the low-dimensional structure of x(cid:63). Next we deﬁne a
crucial property for our structure-driven analysis  which is called the subspace compatibility:
Deﬁnition 3.1. [34] With predeﬁned g(x)  we deﬁne the subspace compatibility of a model sub-
space M as:

when M (cid:54)= {0}.
The subspace compatibility Φ(M) captures the model complexity of subspace M. For example if
g(.) = (cid:107).(cid:107)1 and M is a subspace which is on a s-sparse support in Rd  we will have Φ(M) =
s.
A. 3. Each fi(.) has L-Lipschitz continuous gradient:

√

(cid:107)(cid:79)fi(x) − (cid:79)fi(x(cid:48))(cid:107)2 ≤ L(cid:107)x − x(cid:48)(cid:107)2 ∀x  x(cid:48) ∈ Rd.

(8)

This form of smoothness assumption is classic for variance-reduced stochastic gradient methods.
A. 4. The regularization parameter λ and x† satisﬁes:

with constant c ≥ 1.

λ ≥ (1 +

1
c

)g∗((cid:79)f (x†)) 

(9)

4

Φ(M) := sup

v∈M\{0}

g(v)
(cid:107)v(cid:107)2

 

(7)

Assumption A.4 with the choice of c = 1 is the fundamental assumption of the analytical framework
developed by Negahban et al. [3]. We relax the requirement to c ≥ 1 for more general results. It is
seemly a sophisticated and demanding assumption but indeed is reasonable and suits well the purpose
of this work  which is to develop fast algorithms to speedily solve structured problems (which is
always the result of sufﬁcient regularization). Moreover  recall that the goal of ﬁnding the solution
x(cid:63) via optimizing the regularized empirical risk is to get a meaningful approximation of the true
parameter x† which is the unique minimizer of the population risk. Especially in the high dimensional
setting where d > n  the choice of regularization is rather important since there is no good control
over the statistical error (cid:107)x† − x(cid:63)(cid:107) for an arbitrarily chosen λ. Because of this issue  in this work we
only focus on the “meaningful” regularized ERM problems which are able to provide trustworthy
approximation. Similar to A.4  Negahban et al.[3] has shown that λ ≥ 2g∗((cid:79)f (x†)) provides a
sufﬁcient condition to bound the statistical error (cid:107)x(cid:63) − x†(cid:107)2
2:
Proposition 3.2. [3  Theorem 1  informal] Under A.1  A.2  A.4  with c = 1  if furthermore
the curvature parameter γ  tolerance parameter τ and the subspace compatibility Φ(M) satisfy
τ Φ2(M) < γ

64   then for any optima x(cid:63)  the following inequality holds:

(cid:18) λ2
γ2 Φ2(M) +

λ
γ

g(x

†
M⊥ )

(cid:107)x(cid:63) − x†(cid:107)2

2 ≤ O

 

(10)

(cid:19)

(cid:19)

where O(.) hides deterministic constants for the simplicity of notation.

Such a bound reveals desirable properties of the regularized ERM when the range of λ satisﬁes
assumption A.4. For instance  if x† is the s-sparse ground truth vector of a noisy linear measurement
system y = Ax† + w  where w denotes the zero-mean sub-Gaussian noise (with variance σ2) and
the measurement matrix A satisﬁes a certain restricted eigenvalue condition [3  48]  and we use a
Lasso estimator x(cid:63) ∈ arg minx
2 + λ(cid:107)x(cid:107)1. In such case  let M be a subspace in Rd on
2n(cid:107)Ax − y(cid:107)2
s-sparse support where x† ∈ M and hence g(x

†
M⊥) = 0  this proposition implies that:

1

(cid:18) λ2s

(cid:19)

γ2

(cid:18) σ2

(cid:107)x(cid:63) − x†(cid:107)2

2 ≤ O

≈ O

s log d

γ2

n

 

(11)

which implies the optimal convergence of the statistical error in terms of sample size and dimension
for M-estimators. The details of this claim are presented in [3  Corollary 2].

3.2 Main Results

Base on the assumption of the restricted strong convexity on f (.) w.r.t g(.)  and also with the deﬁnition
of subspace compatibility  one can further derive a more expressive form of RSC  which is named
Effective RSC [34] which has a directly link to the structure of solution.
Lemma 3.3. (Effective RSC) [36  Lemma 3.3] Under A.1  A.2   A.4  while x satisﬁes F (x)−F (x(cid:63)) ≤
†
η for a given value η > 0 and any minimizer x(cid:63)  with ε := 2Φ(M)(cid:107)x† − x(cid:63)(cid:107)2 + 4g(x
M⊥) we have:
(12)

F (x) − F (cid:63) ≥ µc(cid:107)x − x(cid:63)(cid:107)2

2 − 2τ (1 + c)2v2 

where µc = γ

2 − 8τ (1 + c)2Φ2(M) and v = η

λ + ε.

γ

Here we refer µc as the effective restricted strong convexity parameter  which will provide us a direct
link between the convergence speed of an algorithm and the low-dimensional structure of the solution.
Note that this lemma relaxes the condition on λ in [34  Lemma 11]  which is restricted to c = 1. Our
main theorem is presented as the following:
Theorem 3.4. Under A.1 - 4  if further A.2 holds with parameter (γ  τ ) such that τ Φ2(M) <
(cid:25)
(cid:17)(cid:113) 6Lτ (1+c)2D(x0 x(cid:63))
†
n (cid:107)x0−x(cid:63)(cid:107)2
16(1+c)2   denote ε := 2Φ(M)(cid:107)x†−x(cid:63)(cid:107)2+4g(x
M⊥ )  D(x0  x(cid:63)) := 16(F (x0)−F (cid:63))+ 6L
2 
µc = γ
 
(cid:27)

(cid:24)
(cid:113)
2 − 8τ (1 + c)2Φ2(M)  if we run Rest-Katyusha with S0 ≥

with β ≥ 2  then the following inequality holds:

(cid:19)T D(x0  x(cid:63))

(cid:18) 1

32 + 12L
nµc

(cid:24)(cid:16)

1 + 2
ρλ

(cid:26)

S =

β

8nµc+3L

(cid:25)

E[F (xT +1) − F (cid:63)] ≤ max

ε 

(13)

β2

(S0 + 3)2

with probability at least 1 − ρ.

5

Corollary 3.5. Under the same assumptions  parameter choices and notations as Theorem 3.4  the
total number of stochastic gradient evaluations required by Rest-Katyusha to get an δ > ε-accuracy
is:

O

n +

nL
µc

log

1
δ

+ O(n)S0.

(14)

(cid:18)

(cid:115)

(cid:19)

Proof technique. We extend the proof technique of Agarwal et al. [34] to the proximal gradient
descent and also Qu and Xu [35] for SVRG which are both based on applying induction statements
to roll up the residual term of (12) which is the second term at the RHS. The complete proofs of
Theorem 3.4 and Corollary 3.5 can be found in the supplementary material.
Accelerated linear convergence. Under the RSC assumption  Theorem 3.4 and Corollary 3.5
demonstrate a local accelerated linear convergence rate of Rest-Katyusha up to a statistical accuracy
δ > ε. We derive this result based on extending the framework provided by Agarwal et al. [34] 
by which they established fast structure-dependent linear convergence of proximal gradient descent
method up to a statistical accuracy of δ > ε. To the best of our knowledge  this is the ﬁrst structure-
adaptive convergence result for an accelerated incremental gradient algorithm. Note that  this result
can be trivially extended to a global accelerated linear convergence result (with S0 = S) with the
same setting of Agarwal et al. [34] where a side constraint g(x) ≤ R for some radii R is added
to restrict the early iterations with additional re-projections unto this constraint set2. Start from
the objective-gap convergence result (13)  with some additional algebra one can easily derive the
accelerated linear convergence on the iterate (optimization variable) using the RSC condition as well.
2 − 8τ (1 + c)2Φ2(M) links the
Structure-adaptive convergence. The effective RSC µc = γ
convergence speed of Rest-Katyusha with the intrinsic low-dimensional structure of the solution
2 + λ(cid:107)x(cid:107)1  (cid:107)x(cid:63)(cid:107)0 = s and
which is due to the regularization. For instance  if F (x) := 1
2 − 32τ s  meanwhile for a wide class of random design
(A.4) holds c = 1  then we have µc = γ
matrices we have τ = O( log d
n ) and γ > 0. More speciﬁcally  if the rows of the random design matrix
A are drawn i.i.d. from N (0  Σ) with covariance matrix Σ ∈ Rd×d which has largest singular value
rmax(Σ) and smallest singular value rmin(Σ)  then γ ≥ rmin(Σ)
n with high
probability as shown by Raskutti et al. [48].
High probability statement. Since our proofs utilize the effective RSC which holds in a neighbor-
hood of x(cid:63) as demonstrated in Lemma 3.3  we need to bound the functional suboptimality F (xt)−F (cid:63)
in the worst case instead of in expectation. Hence inevitably the Markov inequality has to be applied
to provide the convergence statement with high probability (details can be found in the main proof).
Optimizing the choice of β. Theorem 3.4 shows that the complexity of the main loop of Rest-
Katyusha is
δ   which suggest a trade-off between the choice of β and
the total computation. With some trival computation one can derive that in theory the best choice of β
is exactly the Euler’s number (≈ 2.7). Numerically  we observe that slightly larger choice of β often
provides better performance in practice (illustrative examples can be found in supplemental material).

(cid:108)
(cid:109)
β(cid:112)32 + 12L/(nµc)

and τ ≤ rmax(Σ) 81 log d

2n(cid:107)Ax − b(cid:107)2

16

logβ2

1

4 Adaptive Rest-Katyusha

Motivated by the theory above  we further propose our practical adaptive restart heuristic of Rest-
Katyusha which is able to estimate the effective RSC on the ﬂy. Based on the convergence theory 
we observe that  with the choice of restart period S =
with a conservative
estimate µ0 ≤ µc  then we are always guaranteed to have:
Eξt\ξt−1 [F (xt+1) − F (cid:63)] ≤ 1

(cid:109)
β(cid:112)32 + 12L/(nµ0)
β2 [F (xt) − F (cid:63)] 

(15)

(cid:108)

2In [34]  a side constraint is manually added to the regularized ERM problem  hence in their setting  the
effective restricted strong-convexity is valid globally. They provide global linear convergence result of proximal
gradient descent (with additional re-projection steps) at a cost of additional side-constraints.

6

due to the fact that an underestimation of the RSC will leads to a longer restart period that we
actually need3. The intuition behind our adaptive restart heuristic is: if we overestimate µc  the above
inequality will be violated. Hence an adaptive estimation of µc can be achieved via a convergence
speed check. However the above inequality cannot be evaluated directly in practice since it is in
expectation and demands the knowledge of F (cid:63). In [29  Prop. 4]  it has been shown that with the
composite gradient map:

T (x) = arg min

L
2

q

(cid:107)x − q(cid:107)2

2 + (cid:104)(cid:79)f (x)  q − x(cid:105) + λg(q) 

(16)

the value of F (x) − F (cid:63) can be lower bounded:

and also it can be approximately upper bounded by O((cid:107)T (x) − x(cid:107)2
assumed  which reads:

F (x) − F (cid:63) ≥ O(cid:107)T (x) − x(cid:107)2
2 

(17)
2) if local quadratic growth is

∃α > 0  r > 0  F (x) − F (cid:63) ≥ α(cid:107)x − x(cid:63)(cid:107)2

(18)
Hence in our adaptive restart heuristic we check the convergence speed via evaluating the composite
gradient map at the snapshot points where full gradients have already been calculated. Because of
this  the only main additional computational overhead of this adaptive restart scheme is the proximal
operation of g(.) at the restart points.

2 ∀x s.t. (cid:107)x − x(cid:63)(cid:107)2

2 < r.

Algorithm 3 Adaptive Rest-Katyusha (x0  µ0  S0  β  T  L)

Initialize: Epoch length m = 2n; Initial restart period S =

(cid:24)

(cid:113)

32 + 12L
nµ0

β

(cid:25)

;

x1 = Katyusha (x0  m  S0  L)
Calculate the composite gradient map:
T (x1) = arg minx
2 (cid:107)x − x1(cid:107)2
for t = 1  . . .   T do

L

2 + (cid:104)(cid:79)f (x1)  x − x1(cid:105) + λg(x).

xt+1 = Katyusha (xt  m  S  L)
—–Track the convergence speed via the composite gradient maps:

—– Update the estimate of RSC and adaptively tune the restart period

T (xt+1) = arg minx
if (cid:107)T (xt+1) − xt+1(cid:107)2
then µ0 ← 2µ0  else µ0 ← µ0/2. S =

2 (cid:107)x − xt+1(cid:107)2
2 ≤ 1

2 + (cid:104)(cid:79)f (xt+1)  x − xt+1(cid:105) + λg(x).
(cid:24)
β2(cid:107)T (xt) − xt(cid:107)2

(cid:113)

(cid:25)

2

β

L

32 + 12L
nµ0

end for

The adaptive Rest-Katyusha method is presented in Algorithm 3. We highlight the heuristic estimating
procedure for RSC parameter in the orange lines  which is additional to the original Katyusha
algorithm. The algorithm start with an initial guess µ0 and correspondingly the restart period S 
meanwhile we calculate the composite gradient map T (x1) at x1 and record the value of (cid:107)T (x1) −
2 which we use as the estimation of F (x1) − F (cid:63) (and so on). Then after S outer-loops  we restart
x1(cid:107)2
2 ≥
the algorithm and meanwhile and evaluate again the composite gradient map. If (cid:107)T (x2) − x2(cid:107)2
β2(cid:107)T (x1) − x1(cid:107)2
(cid:108)
β(cid:112)32 + 12L/(nµ0))
2  then we suspect that the RSC parameter has been overestimated  and hence
1
we reduce µ0 by a half  otherwise we double the estimation. We also update the restart period by
with the modiﬁed µ0. The forthcoming iterations follow the same
S =
updating rule as described.

(cid:109)

5 Numerical Experiments

In this section we describe our numerical experiments on our proposed algorithm Rest-Katyusha
(Alg.2) and also the adaptive Rest-Katyusha (Alg.3). We focus on the Lasso regression task:

x(cid:63) ∈ arg min
x∈Rd

F (x) :=

(cid:107)Ax − b(cid:107)2

2 + λ(cid:107)x(cid:107)1

1
2n

(19)

(cid:111)

.

(cid:110)

3An inaccurate estimate of the RSC will lead to a compromised convergence rate. Detailed discussion and

analysis of Rest-Katyusha with a rough RSC estimate can be found in the Appendix.

7

To enforce sparsity on regression parameter we use the (cid:96)1 penalty with various degrees of regulariza-
tion parameters chosen from the set λ ∈ {1 × 10p  2 × 10p  5 × 10p  p ∈ Z}. For comparison  the
performance of (proximal) SVRG and the original Katyusha method is also shown in the plots. We
run all the algorithms with their theoretical step sizes for Madelon and REGED dataset  while for
RCV1 dataset we adopt minibatch scheme for all the algorithms and grid-search the step sizes which
optimize these algorithms’ performance.

Table 1: Datasets for the Experiments and Minibatch Size Choice for the Algorithms

DATA SET
(A) MADELON
(B) RCV1
(C) REGED

SIZE (n  d)
(2000  500)

(20242  47236)

(500  999)

MINIBATCH

1
80
1

REF.
[49]
[49]
[50]

Figure 1: Lasso Experiments on (A) Madelon and (B) RCV1

(A) λ = 5 × 10−5  (cid:107)x(cid:63)(cid:107)0 = 42

(A) λ = 2 × 10−5  (cid:107)x(cid:63)(cid:107)0 = 159

(B) λ = 1 × 10−4  (cid:107)x(cid:63)(cid:107)0 = 902

(B) λ = 1 × 10−5  (cid:107)x(cid:63)(cid:107)0 = 6315

In all our experiments we set β = 5 and S0 = S for convenience. We ﬁrst do a grid-search on the
estimate of µc for Rest-Katyusha which provides the best convergence performance  and denote it as
“Rest-Katyusha opt” in the plots. Meanwhile we also run Rest-Katyusha with RSC estimation which
is 20 times larger or smaller than the optimal one  where we denote as “Rest-Katyusha opt*20” and
“Rest-Katyusha opt/20” respectively. At the 5th plot in Figure 1 the curves for Rest-Kat opt  opt*20
and opt/20 are indistinguishable which shows that in these particular experiments their performance
are almost identical. For the adaptive Rest-Katyusha we ﬁx our starting estimate of µc as 10−5
throughout all the experiments.
From these experiments we observe that as our theory has predicted  the Rest-Katyusha achieves
accelerated linear convergence even when there is no explicit strong-convexity in the cost function
(RCV1 and REGED dataset)  and the convergence speed has a direct relationship with the sparsity
of solution. For the lasso experiments while the solution is sparser  the linear convergence speed of
Rest-Katyusha indeed become faster. Meanwhile when we run Rest-Katyusha with an inaccurate
RSC estimate  we still observe a compromised linear convergence  as predicted by our theory. In all
the experiments  we have observe that the adaptive Rest-Katyusha indeed achieves a good estimation

8

05001000150020002500# Gradient Evaluations / n-14-12-10-8-6-4-20Objective Gap (log)010002000300040005000# Gradient Evaluations / n-14-12-10-8-6-4-20Objective Gap (log)050100150200250300# Gradient Evaluations / n-14-12-10-8-6-4-20Objective Gap (log)020040060080010001200# Gradient Evaluations / n-14-12-10-8-6-4-20Objective Gap (log)Figure 2: Lasso Experiments on (C) REGED Dataset

(C) λ = 2 × 10−5  (cid:107)x(cid:63)(cid:107)0 = 80

(C) λ = 1 × 10−5  (cid:107)x(cid:63)(cid:107)0 = 127

(C) λ = 5 × 10−6  (cid:107)x(cid:63)(cid:107)0 = 209

(C) λ = 2 × 10−6  (cid:107)x(cid:63)(cid:107)0 = 343

of the RSC parameter and properly adapts the choice of restart period automatically on the ﬂy  hence
its performance is often comparable with the best tuned Rest-Katyusha. As the experimental results
shown in [34  35  36]  the linear convergence we observe is towards an arbitrary accuracy instead of
a threshold nearby the solution. This conservative aspect of the theory is inherently due to the artifact
of the RSC framework [34] and we include the extension for arbitrary accuracy regime as our future
work. We also include additional experimental results in the supplemental materials.

6 Conclusion

We developed a restart variant of the Katyusha algorithm for regularized empirical risk minimization
tasks  which is provably able to actively exploit the intrinsic low-dimensional structure of the solution
for the acceleration of convergence. Based on the convergence result we further constructed an
adaptive restart heuristic which aimed at estimating the RSC parameter on the ﬂy and adaptively
tune the restart period. The efﬁciency of this approach is validated through numerical experiments.
In future work  we aim to develop more reﬁned and provably-good adaptive restart schemes for
Rest-Katyusha algorithm to further exploit the solution’s structure for acceleration.

7 Acknowledgements

JT  FB  MG and MD would like to acknowledge the support from H2020-MSCA-ITN Machine
Sensing Training Network (MacSeNet)  project 642685; ERC grant SEQUOIA  project 724063;
EPSRC Compressed Quantitative MRI grant  number EP/M019802/1; and ERC Advanced grant 
project 694888  C-SENSE  respectively. MD is also supported by a Royal Society Wolfson Research
Merit Award. JT would like to thank Damien Scieur and Vincent Roulet for helpful discussions
during his research visit in SIERRA team.

9

050010001500# Gradient Evaluations / n-15-10-505Objective Gap (log)050010001500# Gradient Evaluations / n-15-10-505Objective Gap (log)05001000150020002500# Gradient Evaluations / n-15-10-505Objective Gap (log)010002000300040005000# Gradient Evaluations / n-15-10-505Objective Gap (log)References
[1] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media  2013.

[2] Martin J Wainwright. Structured regularizers for high-dimensional problems: Statistical and computational

issues. Annual Review of Statistics and Its Application  1:233–253  2014.

[3] Sahand N Negahban  Pradeep Ravikumar  Martin J Wainwright  and Bin Yu. A uniﬁed framework for
high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science  pages
538–557  2012.

[4] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society.

Series B (Methodological)  pages 267–288  1996.

[5] Robert Tibshirani  Martin Wainwright  and Trevor Hastie. Statistical learning with sparsity: the lasso and

generalizations. Chapman and Hall/CRC  2015.

[6] Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. Sparsity and smoothness
via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology)  67(1):91–
108  2005.

[7] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms.
In Proceedings of the twenty-ﬁrst international conference on Machine learning  page 116. ACM  2004.

[8] Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of COMP-

STAT’2010  pages 177–186. Springer  2010.

[9] Nicolas L. Roux  Mark Schmidt  and Francis R. Bach. A stochastic gradient method with an exponential
convergence _rate for ﬁnite training sets. In F. Pereira  C. J. C. Burges  L. Bottou  and K. Q. Weinberger 
editors  Advances in Neural Information Processing Systems 25  pages 2663–2671. Curran Associates 
Inc.  2012.

[10] M. Schmidt  N. Le Roux  and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.

Mathematical Programming  pages 1–30  2013.

[11] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss

minimization. Journal of Machine Learning Research  14(Feb):567–599  2013.

[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction.

In Advances in neural information processing systems  pages 315–323  2013.

[13] L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM

Journal on Optimization  24(4):2057–2075  2014.

[14] A. Defazio  F. Bach  and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems  pages
1646–1654  2014.

[15] Yurii Nesterov. A method of solving a convex programming problem with convergence rate o (1/k2). In

Soviet Mathematics Doklady  volume 27  pages 372–376  1983.

[16] Y. Nesterov. Gradient methods for minimizing composite objective function. Technical report  UCL  2007.

[17] Yurii Nesterov. Introductory lectures on convex optimization: A basic course  volume 87. Springer Science

& Business Media  2013.

[18] Qihang Lin  Zhaosong Lu  and Lin Xiao. An accelerated proximal coordinate gradient method. In Advances

in Neural Information Processing Systems  pages 3059–3067  2014.

[19] Zeyuan Allen-Zhu  Zheng Qu  Peter Richtárik  and Yang Yuan. Even faster accelerated coordinate descent
using non-uniform sampling. In International Conference on Machine Learning  pages 1110–1119  2016.

[20] Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regular-

ized loss minimization. In International Conference on Machine Learning  pages 64–72  2014.

[21] Guanghui Lan and Yi Zhou. An optimal randomized incremental gradient method. arXiv preprint

arXiv:1507.02000  2015.

[22] Yuchen Zhang and Xiao Lin. Stochastic primal-dual coordinate method for regularized empirical risk
minimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15) 
pages 353–361  2015.

10

[23] Z. Allen-Zhu. Katyusha: The ﬁrst direct acceleration of stochastic gradient methods. arXiv preprint

arXiv:1603.05953  2016.

[24] Aaron Defazio. A simple practical accelerated method for ﬁnite sums. In D. D. Lee  M. Sugiyama  U. V.
Luxburg  I. Guyon  and R. Garnett  editors  Advances in Neural Information Processing Systems 29  pages
676–684. Curran Associates  Inc.  2016.

[25] Tomoya Murata and Taiji Suzuki. Doubly accelerated stochastic variance reduced dual averaging method
for regularized empirical risk minimization. In Advances in Neural Information Processing Systems  pages
608–617  2017.

[26] Yossi Arjevani. Limitations on variance-reduction and acceleration schemes for ﬁnite sums optimization.

In Advances in Neural Information Processing Systems  pages 3543–3552  2017.

[27] B. O’Donoghue and E. Candes. Adaptive restart for accelerated gradient schemes. Foundations of

computational mathematics  15(3):715–732  2015.

[28] Vincent Roulet and Alexandre d’Aspremont. Sharpness  restart and acceleration. In Advances in Neural

Information Processing Systems  pages 1119–1129  2017.

[29] Olivier Fercoq and Zheng Qu. Adaptive restart of accelerated gradient methods under local quadratic

growth condition. arXiv preprint arXiv:1709.02300  2017.

[30] Olivier Fercoq and Zheng Qu. Restarting the accelerated coordinate descent method with a rough strong

convexity estimate. arXiv preprint arXiv:1803.05771  2018.

[31] Jialei Wang and Lin Xiao. Exploiting strong convexity from data with primal-dual ﬁrst-order algorithms.
In Doina Precup and Yee Whye Teh  editors  Proceedings of the 34th International Conference on Machine
Learning  volume 70 of Proceedings of Machine Learning Research  pages 3694–3702  International
Convention Centre  Sydney  Australia  06–11 Aug 2017. PMLR.

[32] Francis R Bach. Consistency of the group lasso and multiple kernel learning. Journal of Machine Learning

Research  9(Jun):1179–1225  2008.

[33] Jian-Feng Cai  Emmanuel J Candès  and Zuowei Shen. A singular value thresholding algorithm for matrix

completion. SIAM Journal on Optimization  20(4):1956–1982  2010.

[34] A. Agarwal  S. Negahban  and M. J. Wainwright. Fast global convergence rates of gradient methods for

high-dimensional statistical recovery. The Annals of Statistics  40(5):2452–2482  2012.

[35] Chao Qu and Huan Xu.

arXiv:1611.01957  2016.

Linear convergence of svrg in statistical estimation.

arXiv preprint

[36] Junqi Tang  Francis Bach  Mohammad Golbabaee  and Mike Davies. Structure-adaptive  variance-reduced 

and accelerated stochastic optimization. arXiv preprint arXiv:1712.03156  2017.

[37] Junqi Tang  Mohammad Golbabaee  Francis Bach  and Mike Davies. Structure-adaptive accelerated

coordinate descent. hal.archives hal-01889990  2018.

[38] M. Pilanci and M. J. Wainwright. Randomized sketches of convex programs with sharp guarantees.

Information Theory  IEEE Transactions on  61(9):5096–5115  2015.

[39] M. Pilanci and M. J. Wainwright. Iterative hessian sketch: Fast and accurate solution approximation for

constrained least-squares. Journal of Machine Learning Research  17(53):1–38  2016.

[40] Mert Pilanci and Martin J Wainwright. Newton sketch: A near linear-time optimization algorithm with

linear-quadratic convergence. SIAM Journal on Optimization  27(1):205–245  2017.

[41] Junqi Tang  Mohammad Golbabaee  and Mike E. Davies. Gradient projection iterative sketch for large-scale
constrained least-squares. In Proceedings of the 34th International Conference on Machine Learning 
volume 70 of Proceedings of Machine Learning Research  pages 3377–3386. PMLR  2017.

[42] Junqi Tang  Mohammad Golbabaee  and Mike Davies. Exploiting the structure via sketched gradient
algorithms. In Signal and Information Processing (GlobalSIP)  2017 IEEE Global Conference on  pages
1305–1309. IEEE  2017.

[43] Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate uniﬁcation of gradient and mirror

descent. arXiv preprint arXiv:1407.1537  2014.

11

[44] H. Lin  J. Mairal  and Z. Harchaoui. A universal catalyst for ﬁrst-order optimization. In Advances in

Neural Information Processing Systems  pages 3384–3392  2015.

[45] Olivier Fercoq and Zheng Qu. Restarting accelerated gradient methods with a rough strong convexity

estimate. arXiv preprint arXiv:1609.07358  2016.

[46] Samuel Vaiter  Mohammad Golbabaee  Jalal Fadili  and Gabriel Peyré. Model selection with low complexity

priors. Information and Inference: A Journal of the IMA  4(3):230–287  2015.

[47] Hamed Karimi  Julie Nutini  and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases  pages 795–811. Springer  2016.

[48] Garvesh Raskutti  Martin J Wainwright  and Bin Yu. Restricted eigenvalue properties for correlated

gaussian designs. Journal of Machine Learning Research  11(Aug):2241–2259  2010.

[49] M. Lichman. UCI machine learning repository  2013.

[50] Causality workbench team. A genomics dataset  09 2008.

12

,Junqi Tang
Mohammad Golbabaee
Francis Bach
Mike davies
Qiangeng Xu
Weiyue Wang
Duygu Ceylan
Radomir Mech
Ulrich Neumann