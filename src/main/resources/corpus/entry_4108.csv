2012,Optimal Neural Tuning Curves for Arbitrary Stimulus Distributions: Discrimax  Infomax and Minimum $L_p$ Loss,In this work we study how the stimulus distribution influences the optimal coding of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning curve are provided for a neuron obeying Poisson statistics under a given stimulus distribution. We consider a variety of optimality criteria  including maximizing discriminability  maximizing mutual information and minimizing estimation error under a general $L_p$ norm.  We generalize the Cramer-Rao lower bound and show how the $L_p$ loss can be written as a functional of the Fisher Information in the asymptotic limit  by proving the moment convergence of certain functions of Poisson random variables.  In this manner  we show how the optimal tuning curve depends upon the loss function  and the equivalence of maximizing mutual information with minimizing $L_p$ loss in the limit as $p$ goes to zero.,Optimal Neural Tuning Curves for Arbitrary

Stimulus Distributions: Discrimax  Infomax and

Minimum Lp Loss

Zhuo Wang

Department of Mathematics
University of Pennsylvania

Philadelphia  PA 19104

wangzhuo@sas.upenn.edu

Alan A. Stocker

Department of Psychology
University of Pennsylvania

Philadelphia  PA 19104

astocker@sas.upenn.edu

Department of Electrical and Systems Engineering

Daniel D. Lee

University of Pennsylvania

Philadelphia  PA 19104

ddlee@seas.upenn.edu

Abstract

In this work we study how the stimulus distribution inﬂuences the optimal coding
of an individual neuron. Closed-form solutions to the optimal sigmoidal tuning
curve are provided for a neuron obeying Poisson statistics under a given stimulus
distribution. We consider a variety of optimality criteria  including maximizing
discriminability  maximizing mutual information and minimizing estimation er-
ror under a general Lp norm. We generalize the Cramer-Rao lower bound and
show how the Lp loss can be written as a functional of the Fisher Information
in the asymptotic limit  by proving the moment convergence of certain functions
of Poisson random variables. In this manner  we show how the optimal tuning
curve depends upon the loss function  and the equivalence of maximizing mutual
information with minimizing Lp loss in the limit as p goes to zero.

1

Introduction

A neuron represents sensory information via its spike train. Rate coding maps an input stimulus to
a spiking rate via the neuron’s tuning. Previous work in computational neuroscience has tried to
explain this mapping via optimality criteria. An important factor determining the optimal shape of
the tuning curve is the input statistics of the stimulus. It has previously been observed that environ-
mental statistics can inﬂuence the neural tuning curves of sensory neurons [1  2  3  4  5]. However 
most theoretical analysis has usually assumed the input stimulus distribution to be uniform. Only
recently  theoretical work has been demonstrating how non-uniform prior distributions will affect
the optimal shape of the neural tuning curves [6  7  8  9  10].
An important factor in determining the optimal tuning curve is the optimality criterion [11]. Most
previous work used local Fisher Information [12  13  14]  the estimation square loss or discrim-
inability (discrimax) [15  16] or the mutual information (infomax) [9  17] to evaluate neural codes.
It has been shown that both the square loss and the mutual information are related to the Fisher Infor-
mation via lower bounds: the lower bound of estimation square loss is provided by the Cramer-Rao
lower bound [18  19] and the mutual information can be lower bounded by a functional of Fisher
Information as well [7]. It has also been proved that both lower bounds can be attained on the con-

1

dition that the encoding time is long enough and the estimator behaves well in the asymptotic limit.
However  there has been no previous study to integrate those two lower bounds into a more general
framework.
In this paper  we ask the question what tuning curve is optimally encoding a stimulus with an ar-
bitrary prior distribution such that the Lp estimation lost is minimized. We are able to provide
analytical solutions to the above question. With the asymptotic analysis of the maximum likelihood
estimator (MLE)  we can show how the Lp loss converges to a functional of Fisher Information in
the limit of long encoding time. The optimization of such functional can be conducted for arbitrary
stimulus prior and for all p ≥ 0 in general. The special case of p = 2 and the limit p → 0 cor-
responds to discrimax and infomax  respectively. The general result offers a framework to help us
understand the infomax problem in a new point of view: maximizing mutual information is equiva-
lent to minimizing the expected L0 loss.

2 Model and Methods

2.1 Encoding and Decoding Model

Throughout this paper we denote s as the scalar stimulus. The stimulus follows an arbitrary prior
distribution π(s). The encoding process involves a probabilistic mapping from stimulus to a random
number of spikes. For each s  the neuron will ﬁre at a predetermined ﬁring rate h(s)  representing the
neuron’s tuning curve. The encoded information will contain some noise due to neural variability.
According to the conventional Poisson noise model  if the available coding time is T   then the
observed spike count N has a Poisson distribution with parameter λ = h(s)T

P[N = k] =

1
k!

(h(s)T )k e−h(s)T

(1)

The tuning curve h(s) is assumed to be sigmoidal  i.e. monotonically increasing  but limited to
a certain range hmin ≤ h(s) ≤ hmax due to biological constraints. The decoding process is the
reverse process of encoding. The estimator ˆs = ˆs(N ) should be a function of observed count N.
One conventional choice is to use the MLE estimator. First the MLE estimator ˆλ for mean ﬁring rate
is ˆλ = N/T . There for the MLE estimator for stimulus s is simply ˆs = h−1(ˆλ).

2.2 Fisher Information and Reversal Formula

The Fisher Information can be used to describe how well one can distinguish a speciﬁc distribution
from its neighboring distributions within the same family of distributions. For a family of distribu-
tion with scalar parameter s  the Fisher Information is deﬁned as

(cid:90) (cid:18) ∂

(cid:19)2

I(s) =

log P(N|s)

P(N|s) dN.

∂s

For tuning function h(s) with Poisson spiking model  the Fisher Information is (see [12  7])

Ih(s) = T

h(cid:48)(s)2
h(s)

Further with the sigmoidal assumption  by solving the above ordinary differential equation  we can
derive the inverse formula in Eq.(4) and an equivalent constraint on Fisher Information in Eq.(5)

(2)

(3)

(4)

(cid:18)(cid:112)
1
(cid:112)
2√T
Ih(t) dt ≤ 2√T

hmin +

(cid:90) s
(cid:16)(cid:112)

h(s) =

(cid:90) s

(cid:112)

Ih(t) dt

(cid:112)

−∞
hmax −

(cid:19)2
(cid:17)

(cid:112)
This constraint is closely related to the Jeffrey’s prior  which claims that π∗(s) ∝
I(s) is the least
informative prior. The above inequality means that the normalization factor of the Jeffrey’s prior is
ﬁnite  as long as the range of ﬁring rate is limited hmin ≤ h(s) ≤ hmax.

hmin

−∞

(5)

2

3 Two Bounds on Loss Function via Fisher Information

3.1 Cramer-Rao Bound

The Cramer-Rao Bound [18] for unbiased estimators is

We can achieve maximum discriminability δ−1 by minimizing the mean asymptotic squared error
(MASE)  deﬁned in [15] as

δ2 = E[(ˆs − s)2] ≥

ds 

(7)

E[(ˆs − s)2|s] ≥

(6)

1
I(s)

(cid:90) π(s)

Ih(s)

Even if Eq.(7) is only a lower bound  it is attained by the MLE of s asymptotically. In order to
optimize the right side of Eq.(7) under the constraints Eq.(5)  variation method can be applied and
the optimal condition and the optimal solution can be written as

(cid:32)(cid:112)

(cid:16)(cid:112)

(cid:112)

hmin +

hmax −

hmin

(cid:17)(cid:82) s
(cid:82) ∞

−∞
−∞

(cid:33)2

π(t)1/3 dt
π(t)1/3 dt

(8)

Ih(s) ∝ π(s)2/3 

h2(s) =

3.2 Mutual Information Bound

Similar as the Cramer-Rao Bound  Brunel and Nadal [7] gave an upper bound of the mutual infor-
mation between the MLE ˆs and the environmental stimulus s

Imutual(ˆs  s) ≥ Hπ −

1
2

π(s) log

ds 

(9)

(cid:90)

(cid:18) 2πe

(cid:19)

Ih(s)

where Hπ is the entropy of the stimulus prior π(s). Although this is an lower bound on the mutual
information which we want to maximize  the equality holds asymptotically by the MLE of s as
stated in [7]. To maximize the mutual information  we can maximize the right side of Eq.(9) under
the constraint of Eq.(5) by variation method again and obtain the optimal condition and optimal
solution as

(cid:32)(cid:112)

(cid:16)(cid:112)

(cid:112)

(cid:17)(cid:82) s
(cid:82) ∞

−∞
−∞

(cid:33)2

π(t) dt

π(t) dt

Ih(s) ∝ π2(s) 

h0(s) =

hmin +

hmax −

hmin

(10)

To see the connection between solutions in Eq.(8) and Eq.(10)  we need the result of the following
section.

4 Asymptotic Behavior of Estimators

In general  minimizing the lower bound does not imply that the measures of interest  e.g. the left
side of Eq.(7) and Eq.(9)  is minimized. In order to make the lower bounds useful  we need to know
the conditions for which there exist ”good” estimators that can reach these theoretical lower bounds.
First we will introduce some deﬁnitions of estimator properties. Let T be the encoding time for a

neuron with Poisson noise  and ˆsT be the MLE at time T . If we denote Y (cid:48)T = √T (ˆsT − s) and
Z(cid:48) ∼ N (0  T /I(s))  then the notions we have mentioned above are deﬁned as below
(asymptotic consistency)
(asymptotic efﬁciency)

E[Y (cid:48)T ] → 0

(11)
(12)

Var[Y (cid:48)T ] → T /I(s)

D→ Z(cid:48)

Y (cid:48)T

(13)
(14)
Generally the above four conditions are listed from the weakest to the strongest  top to bottom.
To have the equality in Eq.(7) hold  we need the asymptotic consistent and asymptotic efﬁcient
estimators. To have the equality in (9) hold  we need the asymptotic normal estimators (see [7]). If

(asymptotic normality)
(p-th moment convergence)

E[|Y (cid:48)T|p] → E[|Z(cid:48)|p]

3

we want to generalize the problem even further  i.e. ﬁnding the tuning curve which minimizes the
Lp estimation loss  then we need the moment convergent estimator for all p-th moments.
Here we will give two theorems to prove that the MLE ˆs of the true stimulus s would satisfy all the
above four properties in Eq.(11)-(14). Let h(s) be the tuning curve of a neuron with Poisson spiking
noise. The the MLE of s is given by ˆs = h−1(ˆλ). We will show that the limiting distribution of
√T (ˆsT − s) is a Gaussian distribution with mean 0 and variance h(s)/h(cid:48)(s)2. We will also show
that any positive p-norm of √T (ˆsT − s) will converge the p-norm of the corresponding Gaussian
Theorem 1. Let Xi be i.i.d. Poisson distributed random variables with mean λ. Let Sn =(cid:80)n

distribution. The proof of Theorem 1 and 2 will be provided in Appendix A.

i=1 Xi

be the partial sum. Then

(a) Sn has Poisson distribution with mean nλ.
(b) Yn = √n(Sn/n − λ) converges to Z ∼ N (0  λ) in distribution.
(c) The p-th moment of Yn converges  and limn→∞ Eλ[|Yn|p] = E[|Z|p] for all p > 0.

One direct application of this theorem is that  if the tuning curve h(s) = s for (s > 0) and the
encoding time is T   then the estimator ˆs = N/T is asymptotically efﬁcient since as T → ∞ 
Var[ˆs] → E[|Zλ/√T|2] = s/T = 1/I(s).
Theorem 2. Let Xi  Sn be deﬁned as in Theorem 1. Let g(x) be any function with bounded deriva-
tive |g(cid:48)(x)| ≤ M. Then

(a) Y (cid:48)n = √n(g(Sn/n) − g(λ)) converges to Z(cid:48) ∼ N (0  λg(cid:48)(λ)2) in distribution.
(b) The p-th moment of Y (cid:48)n converges  and limn→∞ Eλ[|Y (cid:48)n|p] = E[|Z(cid:48)|p] for all p > 0.

Theorem 1 indicates that we can always estimate the ﬁring rate λ = h(s) efﬁciently by the estimator
ˆλ = N/T . Theorem 2 indicates  however  that we can also estimate a smooth transformation of
the ﬁring rate efﬁciently in the asymptotic limit T → ∞. Now  if we go back to the conventional
setting of the tuning curve λ = h(s)  we can estimate the stimulus by the estimator ˆs = h−1(ˆλ).
To meet the need of boundedness of g: |g(cid:48)(λ)| ≤ M  we have 1/g(cid:48)(λ) = h(cid:48)(s) ≥ 1/M hence this
theory only works for stimulus from a compact set s ∈ [−M  M ]  although the M can be chosen as
large as possible. The larger the M is  the longer encoding time T will be necessary to observe the
asymptotic normality and the convergence of moments.
The estimator ˆs = h−1(ˆλ) is biased for ﬁnite T   but it is asymptotically unbiased and efﬁcient. This
is because as T → ∞

Es[√T (ˆsT − s)] → E[Z(cid:48)] = 0
Vars[√T (ˆsT − s)] → E[|Z(cid:48)|2] = λ(h−1)(cid:48)(λ)2 =

(15)

h(s)
h(cid:48)(s)2 =

T
I(s)

(16)

From the above analysis we can see that the property of Lp(ˆs  s) = Es[|ˆsT − s|p] saturating the
lower bound fully relies upon the asymptotic normality. With asymptotic normality  we can do more
than just optimizing Imutual(N  s) and Lp(ˆs  s). In general we can ﬁnd the optimal tuning curve
which minimizes the expected Lp loss Lp(ˆs  s) since as T → ∞

E

(17)
I(s)/T   χ ∼ N (0  1). To calculate the right side of the above limit  we can use

(cid:112)

where Z(cid:48) = χ/
the fact that for any p ≥ 0 

(cid:104)(cid:12)(cid:12)(cid:12)√T (ˆsT − s)
(cid:12)(cid:12)(cid:12)p(cid:105)
→ E(cid:2)
|Z(cid:48)|p(cid:3)
(cid:17)p Γ(cid:0) p+1
(cid:16)√2
Γ(cid:0) 1
(cid:1)

2

K(p) = E [|χ|p] =

2

(cid:1)

where Γ(·) is the gamma function

Γ(z) =

e−ttz−1 dt

(cid:90) ∞

0

4

(18)

(19)

The general conclusion is that (Cramer-Rao Lower bound is a special case with p = 2)

(cid:104)(cid:12)(cid:12)(cid:12)√T (ˆsT − s)

(cid:12)(cid:12)(cid:12)p(cid:105)

→ E(cid:2)

|Z(cid:48)|p(cid:3) =

Es

K(p)

(I(s)/T )p/2

(20)

Figure 1: (A) Illustration of Lp-loss as a function of |ˆs− s| for different values of p. When p = 2 the
loss is the squared loss and when p → 0  the Lp loss converges to 0-1 loss pointwise. (B) The plot
of p-th absolute moments K(p) = E[|χ|p] of standard Gaussian random variable χ for p ∈ [0  4].

5 Optimal Tuning Curves: Infomax  Discrimax and More

With the asymptotic normality and moment convergence  we know the asymptotic expected Lp loss
will approach E[|Z(cid:48)|p] for each s. Hence

(cid:90)

E [|ˆs − s|p] →

π(s)Es

π(s)

I(s)p/2 ds.

To obtain the optimal tuning curve for the Lp loss  we need to solve a simple variation problem

(cid:90)

(cid:2)
|Z(cid:48)|p(cid:3) ds = K(p)
(cid:90)
(cid:90) (cid:112)

π(s)f (Ih(s)) ds

Ih(s) ds ≤ const

minimize

h

subject to

with f(cid:48)p(x) = −x−p/2−1. To solve this variational problem  the Euler-Lagrange equation and the
Lagrange multiplier method can be used to derive the optimal condition

(cid:16)

(cid:32)(cid:112)

(cid:17)

(cid:112)
Ih(s) ∝ π(s)1/(p+1)

Ih

(cid:112)

= π(s)f(cid:48)p (Ih(s)) −

0 =

∂
∂Ih

π(s)fp(Ih(s)) − λ

λ
2Ih(s)−1/2

(24)

hp(s) =

hmin +

(25)
Therefore the fp-optimal tuning curve  which minimizes the asymptotic Lp loss  is given by equation
below  followed from (4) and (25). For some examples of Lp optimal tuning curves  see Fig. 2.

⇒

(cid:16)(cid:112)
(cid:16)(cid:112)

(cid:112)

hmax −

(cid:112)

hmin

(cid:17)2

(cid:33)2

π(t)1/(p+1) dt
π(t)1/(p+1) dt

−∞
−∞
π(s)2/(p+1)

(cid:17)(cid:82) s
(cid:82) ∞
(cid:0)(cid:82) π(t)1/(p+1) dt(cid:1)2
(cid:17)−p(cid:18)(cid:90)

hmin

π(t)1/(p+1) dt

(cid:112)

(cid:19)p+1

Ip(s) = 4T

hmax −

hmin

Following from (21) and (27)  the optimal expected Lp loss is

E [|ˆs − s|p] = K(p) · (4T )−p/2(cid:16)(cid:112)

(21)

(22)

(23)

(26)

(27)

(28)

hmax −

5

00.511.5201234|ˆs−s|LossL(ˆs s)p=2.0p=1.0p=0.5p=0.1(A)0123401234pK(p)(B)A very interesting observation is that  by taking the limit p → 0  we will end up with the infomax
tuning curve. This shows that the infomax tuning curve simultaneously optimizes the mutual infor-
mation as well as the expected L0 norm of the error ˆs − s. The L0 norm can be considered as the
0-1 loss  i.e. L(ˆs  s) = 0 if ˆs = s and L(ˆs  s) = 1 otherwise. To put this in a different approach  we
may consider the natural log function as a limit of power function:

(cid:90)

(cid:18)
(cid:90)
1 − z−p/2
and we can conclude that minimizing(cid:82) π(s)I(s)−p/2ds in the limit of p → 0 (L0 loss) is the same
as maximizing(cid:82) π(s) log I(s)ds and the mutual information.

log z = lim
p→0
2
π(s) log I(s) ds = lim
p
p→0

(cid:19)
π(s)I(s)−p/2 ds

p/2

1 −

(29)

(30)

⇒

Figure 2: For stimulus with standard Gaussian prior distribution (inset ﬁgure) and various values of
p  (A) shows the optimal allocation of Fisher Information Ip(s) and (B) shows the fp-optimal tuning
curve hp(s). When p = 2 the f2-optimal (discrimax) tuning curve minimizes the squared loss and
when p = 0 the f0-optimal (infomax) tuning curve maximizes the mutual information.

6 Simulation Results

E [|ˆs − s|p] = K(p) · (4T )−p/2(cid:16)(cid:112)

(cid:112)

(cid:17)−p(cid:18)(cid:90)

Numerical simulations were performed in order to validate our theory. In each iteration  a random
stimulus s was chosen from the standard Gaussian distribution or Exponential distribution with mean
one. A Poisson neuron was simulated to generate spikes in response to that stimulus. The difference
between the MLE ˆs and s is recorded to analyze the Lp loss. In one simple task  we compared the
numerical value vs. the theoretical value of Lp loss for fq-optimal tuning curve

(cid:19)p(cid:18)(cid:90)

(cid:19)

hmax −

hmin

π(t)1/(q+1) dt

π(s)1− p

q+1 ds

(31)
The above theoretical prediction works well for distributions with compact support s ∈ [A  B]. It
also requires q > p− 1 for any distribution with tail decaying faster than exponential: π(s) ≤ e−Cs 
such as e.g. a Gaussian or exponential distribution. Otherwise the integral in the last term will blow
up in general.
The numerical and theoretical prediction of Lp loss are plotted  for both Gaussian N (0  1) prior
(Fig.3A) and Exp(1) prior (Fig.3B). The vertical axis shows 1/p · log E[|ˆs − s|p] so all p-norms are
displayed at the same unit.

6

−4−2024stimulusstuningcurvehp(s)(B)  p=0.0p=0.5p=1.0p=2.0−4−2024sπ(s)−4−2024stimulussﬁsherinfoIp(s)(A)Figure 3: The comparison between numerical result (solid curves) and theoretical prediction (dashed
curves). (A) For standard Gaussian prior. (B) For exponential prior with parameter 1.

7 Discussion

In this paper we have derived a closed form solution for the optimal tuning curve of a single neuron
given an arbitrary stimulus prior π(s) and for a variety of optimality. Our work offers a principled
explanation for the observed non-linearity in neuron tuning: Each neuron should adapt their tuning
curves to reallocate the limited amount of Fisher information they can carry and minimize the Lp
error. We have shown in section 2 that each sigmoidal tuned neuron with Poisson spiking noise has
an upper bound for the integral of square root of Fisher information and the fp-optimal tuning curve
has the form

(cid:32)(cid:112)

(cid:16)(cid:112)

(cid:112)

hp(s) =

hmin +

hmax −

hmin

(cid:17)(cid:82) s
(cid:82) ∞

−∞
−∞

(cid:33)2

π(t)1/(p+1) dt
π(t)1/(p+1) dt

(32)

where the fp-optimal tuning curve minimizes the estimation Lp loss E[|ˆs − s|p] of the decoding
process in the limit of long encoding time T . Two special and well known cases are maximum
mutual information (p = 0) and maximum discriminant (p = 2).
To obtain this result  we established two theorems regarding the asymptotic behavior of the MLE
ˆs = h−1(ˆλ). Asymptotically  the MLE converges to a standard Gaussian not only with regard to
its distribution  but also in terms of its p-th moment for arbitrary p > 0. By calculating the p-th
moments for the Gaussian random variable  we can predict the Lp loss of the encoding-decoding
process and optimize the tuning curve by minimizing the attainable limit. The Cramer-Rao lower
bound and the mutual information lower bound proposed by [7] are special cases with p = 2 or
p = 0 respectively.
So far  we have put our focus on a single neuron with sigmoidal tuning curve. However  the conclu-
sions in Theorem 1 and 2 still hold for the case of neuronal populations with bell-shaped neurons 
with correlated or uncorrelated noise. The optimal condition for Fisher information can be cal-
culated  regardless of the tuning curve(s) format. According to the assumption on the number of
neurons and the shape of the tuning curves  the optimized Fisher information can be inverted to
derive the optimal tuning curve via the same type of analysis as we presented in this paper.
One theoretic limitation of our method is that we only addressed the problem for long encoding
times  which is usually not the typical scenario in real sensory systems. Though the long encoding
time limit can be replaced by short encoding time with many identical tuned neurons. It is still an
interesting problem to ﬁnd out the optimal tuning curve for arbitrary prior  in the sense of Lp loss
function. Some work [16  20] has been done to maximize mutual information or L2 for uniformly
distributed stimuli. Another problem is that the asymptotic behavior is not uniformly true if the space
of stimulus is not compact. The asymptotic behavior will take longer to be observed if the slop of
the tuning function is too close to zero. In Theorem 2 we made the assumption that |g(cid:48)(s)| ≤ M
and that is the reason we cannot evaluate the estimation error for s with large absolute value hence
we do not have a perfect match for low p values in the simulation section (see Fig. 3).

7

0246−5.5−5−4.5−4p=0.1 q∗=0.1p=0.5 q∗=0.5p=1.0 q∗=1.0p=2.0 q∗=2.0q1/p·log(E[|ˆs−s|p])(A)0246−6−5.5−5−4.5−4p=0.1 q∗=0.1p=0.5 q∗=0.5p=1.0 q∗=1.1p=2.0 q∗=2.0q1/p·log(E[|ˆs−s|p])(B)A Proof of Theorems in Section 4

Proof. of Theorem 1
(a) Immediately follows from Poisson distribution. Use induction on n.
(b) Apply Central Limit Theorem. Notice that E[Xi] = Var[Xi] = λ for Poisson random variables.
(c) In general  convergence in distribution does not imply convergence in p-th moment. However in
our case  we do have the convergence property for all p-th moments. To show this  we need to show
for all p > 0  |Yn|p is uniformly integrable i.e. for any   there exist a K such that

This is obvious with Cauchy-Schwartz inequality and Markov inequality

E[|Yn|p · 1{|Yn|≥K}] ≤ 

≤ E[|Yn|2p] · P[|Yn| ≥ K] ≤ E[|Yn|2p]

E[|Yn|]

K → 0

To see the last limit  we use the fact that for all p > 0  supn E[|Yn|p] < ∞. According to [21] 

(cid:0)E[|Yn|p · 1{Yn≥K}](cid:1)2

(33)

(34)

(35)

(37)

(38)

p(cid:88)

a=0

p/2(cid:88)

a=0

E[|Sn − nλ|p] =

(nλ)aS2(p  a) 

where S2(p  a) denotes the number of partitions of a set of size n into a subsets with no singletons
(i.e. no subsets with only one element). For our purpose  notice that S2(p  a) = 0 for a > p/2 and
S2(p  a) ≤ pa. Therefore the supreme of E[|Yn|p] is bounded since

E[|Yn|p] = E[|√n(Sn/n − λ)|p] ≤ n−p/2

(nλ)apa ≤

n(λp)p/2+1

nλp − 1 ≤ C(λp)p/2

(36)

For arbitrary q  choose any even number p such that p > q  and by Jensen’s inequality  E[|Yn|q] ≤
E[|Yn|p]q/p. Thus for all p > 0  n  E[|Yn|p] < ∞.

Proof. of Theorem 2
(a) Denote ˆλn = Sn/n. Apply mean value theorem for g(x) near λ :

g(ˆλn) − g(λ) = g(cid:48)(λ∗)(ˆλn − λ)

(cid:17)
for some λ∗ between ˆλn and λ. Therefore
g(ˆλn) − g(λ)

√n

(cid:16)

= g(cid:48)(λ∗)√n(ˆλ − λ) D→ g(cid:48)(λ)Z

Note that ˆλn → λ in probability  λ∗ → λ in probability and g(cid:48)(λ∗) → g(cid:48)(λ) in probability  together
with the fact that √n(ˆλn − λ) D→ Z  apply Slutsky’s theorem and the conclusion follows.
(b) Use Taylor’s expansion and Slutsky’s theorem again 

(cid:16)

(cid:12)(cid:12)(cid:12)√n

(cid:17)(cid:12)(cid:12)(cid:12)p

(cid:12)(cid:12)(cid:12)g(cid:48)(λ∗)√n(ˆλ − λ)

(cid:12)(cid:12)(cid:12)p

g(ˆλn) − g(λ)

=

= |g(cid:48)(λ∗)|p |Yn|p → |g(cid:48)(λ)|p |Yn|p

(39)

To see |Y (cid:48)n|p is uniformly integrable  notice that |Y (cid:48)n|p ≥ K ⇒ |Yn|p ≥ K · M−p. The rest follows
in a similar manner as when proving Theorem 1(c).

8

References
[1] TM Maddess and SB Laughlin. Adaptation of the motion-sensitive neuron h1 is generated
locally and governed by contrast frequency. Proc. R. Soc. Lond. B Biol. Sci  225:251–275 
1985.

[2] J Atick. Could information theory provide an ecological theory of sensory processing? Net-

work  3:213–251  1992.

[3] RA Harris  DC O’Carroll  and SB Laughlin. Contrast gain reduction in ﬂy motion adaptation.

Neuron  28:595–606  2000.

[4] I Dean  NS Harper  and D McAlpine. Neural population coding of sound level adapts to

stimulus statistics. Nature neuroscience  8:1684–1689  2005.

[5] AA Stocker and EP Simoncelli. Noise characteristics and prior expectations in human visual

speed perception. Nature neuroscience  9:578–585  2006.

[6] J-P Nadal and N Parga. Non linear neurons in the low noise limit: A factorial code maximizes

information transfer  1994.

[7] N Brunel and J-P Nadal. Mutual information  ﬁsher information and population coding. Neural

Computation  10(7):1731–1757  1998.

[8] Tvd Twer and DIA MacLeod. Optimal nonlinear codes for the perception of natural colours.

Network: Computation in Neural Systems  12(3):395–407  2001.

[9] MD McDonnell and NG Stocks. Maximally informative stimuli and tuning curves for sig-

moidal rate-coding neurons and populations. Phys. Rev. Lett.  101:058103  2008.

[10] D Ganguli and EP Simoncelli. Implicit encoding of prior probabilities in optimal neural pop-

ulations. Adv. Neural Information Processing Systems  23:658–666  2010.

[11] HB Barlow. Possible principles underlying the transformation of sensory messages. M.I.T.

Press  1961.

[12] HS Seung and H Sompolinsky. Simple models for reading neuronal population codes. Proc.

of the National Aca. of Sci. of the U.S.A.  90:10749–10753  1993.

[13] K Zhang and TJ Sejnowski. Neuronal tuning: To sharpen or broaden? Neural Computation 

11:75–84  1999.

[14] A Pouget  S Deneve  J-C Ducom  and PE Latham. Narrow versus wide tuning curves: Whats

best for a population code? Neural Computation  11:85–90  1999.

[15] M Bethge  D Rotermund  and K Pawelzik. Optimal short-term population coding: when Fisher

information fails. Neural Computation  14:2317–2351  2002.

[16] M Bethge  D Rotermund  and K Pawelzik. Optimal neural rate coding leads to bimodal ﬁring

rate distributions. Netw. Comput. Neural Syst.  14:303–319  2003.

[17] S Yarrow  E Challis  and P Seris. Fisher and shannon information in ﬁnite neural populations.

Neural Computation  In Print  2012.

[18] TM Cover and J Thomas. Elements of Information Theory. Wiley  1991.
[19] SI Amari  H Nagaoka  and D Harada. Methods of Information Geometry. Translations of

Mathematical Monographs. American Mathematical Society  2007.

[20] AP Nikitin  NG Stocks  RP Morse  and MD McDonnell. Neural population coding is optimized

by discrete tuning curves. Phys. Rev. Lett.  103:138101  2009.

[21] N Privault. Generalized Bell polynomials and the combinatorics of Poisson central moments.

Electronic Journal of Combinatorics  18  2011.

9

,Mohammad Gheshlaghi azar
Alessandro Lazaric
Emma Brunskill
William Hoiles
Mihaela van der Schaar
Sagie Benaim
Lior Wolf