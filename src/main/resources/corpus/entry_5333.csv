2013,Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints,We investigate two new optimization problems — minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection  which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [9  23] which is in the worst case inapproximable. We show  however  that by phrasing these problems as constrained optimization  which is more natural for many applications  we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and  an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally  we empirically demonstrate the performance and good scalability properties of our algorithms.,Submodular Optimization with Submodular Cover

and Submodular Knapsack Constraints

Rishabh Iyer

Department of Electrical Engineering

University of Washington

rkiyer@u.washington.edu

Jeff Bilmes

Department of Electrical Engineering

University of Washington

bilmes@u.washington.edu

Abstract

We investigate two new optimization problems — minimizing a submodular
function subject to a submodular lower bound constraint (submodular cover)
and maximizing a submodular function subject to a submodular upper bound
constraint (submodular knapsack). We are motivated by a number of real-world
applications in machine learning including sensor placement and data subset
selection  which require maximizing a certain submodular function (like coverage
or diversity) while simultaneously minimizing another (like cooperative cost).
These problems are often posed as minimizing the difference between submodular
functions [9  25] which is in the worst case inapproximable. We show  however 
that by phrasing these problems as constrained optimization  which is more natural
for many applications  we achieve a number of bounded approximation guarantees.
We also show that both these problems are closely related and an approximation
algorithm solving one can be used to obtain an approximation guarantee for
the other. We provide hardness results for both problems thus showing that
our approximation factors are tight up to log-factors. Finally  we empirically
demonstrate the performance and good scalability properties of our algorithms.

Introduction

1
A set function f : 2V → R is said to be submodular [4] if for all subsets S  T ⊆ V   it holds that
f (S) + f (T ) ≥ f (S ∪ T ) + f (S ∩ T ). Deﬁning f (j|S) (cid:44) f (S ∪ j) − f (S) as the gain of j ∈ V
in the context of S ⊆ V   then f is submodular if and only if f (j|S) ≥ f (j|T ) for all S ⊆ T and
j /∈ T . The function f is monotone iff f (j|S) ≥ 0 ∀j /∈ S  S ⊆ V . For convenience  we assume
the ground set is V = {1  2 ···   n}. While general set function optimization is often intractable 
many forms of submodular function optimization can be solved near optimally or even optimally
in certain cases. Submodularity  moreover  is inherent in a large class of real-world applications 
particularly in machine learning  therefore making them extremely useful in practice.
In this paper  we study a new class of discrete optimization problems that have the following form:
Problem 1 (SCSC): min{f (X)| g(X) ≥ c} 
and Problem 2 (SCSK): max{g(X)| f (X) ≤ b} 
where f and g are monotone non-decreasing submodular functions that also  w.l.o.g.  are normalized
(f (∅) = g(∅) = 0)1  and where b and c refer to budget and cover parameters respectively. The
corresponding constraints are called the submodular cover [29] and submodular knapsack [1]
respectively and hence we refer to Problem 1 as Submodular Cost Submodular Cover (henceforth
SCSC) and Problem 2 as Submodular Cost Submodular Knapsack (henceforth SCSK). Our motivation
stems from an interesting class of problems that require minimizing a certain submodular function
f while simultaneously maximizing another submodular function g. We shall see that these naturally
1A monotone non-decreasing normalized (f (∅) = 0) submodular function is called a polymatroid function.

1

occur in applications like sensor placement  data subset selection  and many other machine learning
applications. A standard approach used in literature [9  25  15] has been to transform these problems
into minimizing the difference between submodular functions (also called DS optimization):

(1)

(cid:0)f (X) − g(X)(cid:1).

Problem 0: min
X⊆V

While a number of heuristics are available for solving Problem 0  in the worst-case it is NP-hard
and inapproximable [9]  even when f and g are monotone. Although an exact branch and bound
algorithm has been provided for this problem [15]  its complexity can be exponential in the worst case.
On the other hand  in many applications  one of the submodular functions naturally serves as part of a
constraint. For example  we might have a budget on a cooperative cost  in which case Problems 1 and
2 become applicable. The utility of Problems 1 and 2 become apparent when we consider how they
occur in real-world applications and how they subsume a number of important optimization problems.
Sensor Placement and Feature Selection: Often  the problem of choosing sensor locations can
be modeled [19  9] by maximizing the mutual information between the chosen variables A and the
unchosen set V \A (i.e. f (A) = I(XA; XV \A)). Alternatively  we may wish to maximize the mutual
information between a set of chosen sensors XA and a quantity of interest C (i.e.  f (A) = I(XA; C))
assuming that the set of features XA are conditionally independent given C [19  9]. Both these
functions are submodular. Since there are costs involved  we want to simultaneously minimize the
cost g(A). Often this cost is submodular [19  9]. For example  there is typically a discount when
purchasing sensors in bulk (economies of scale). This then becomes a form of either Problem 1 or 2.
Data subset selection: A data subset selection problem in speech and NLP involves ﬁnding a limited
vocabulary which simultaneously has a large coverage. This is particularly useful  for example in
speech recognition and machine translation  where the complexity of the algorithm is determined
by the vocabulary size. The motivation for this problem is to ﬁnd the subset of training examples
which will facilitate evaluation of prototype systems [23]. Often the objective functions encouraging
small vocabulary subsets and large acoustic spans are submodular [23  20] and hence this problem
can naturally be cast as an instance of Problems 1 and 2.
Privacy Preserving Communication: Given a set of random variables X1 ···   Xn  denote I as
an information source  and P as private information that should be ﬁltered out. Then one way
of formulating the problem of choosing a information containing but privacy preserving set of
random variables can be posed as instances of Problems 1 and 2  with f (A) = H(XA|I) and
g(A) = H(XA|P)  where H(·|·) is the conditional entropy.
Machine Translation: Another application in machine translation is to choose a subset of training
data that is optimized for given test data set  a problem previously addressed with modular functions
[24]. Deﬁning a submodular function with ground set over the union of training and test sample
inputs V = Vtr ∪ Vte  we can set f : 2Vtr → R+ to f (X) = f (X|Vte)  and take g(X) = |X|  and
b ≈ 0 in Problem 2 to address this problem. We call this the Submodular Span problem.
Apart from the real-world applications above  both Problems 1 and 2 generalize a number of well-
studied discrete optimization problems. For example the Submodular Set Cover problem (henceforth
SSC) [29] occurs as a special case of Problem 1  with f being modular and g is submodular. Similarly
the Submodular Cost Knapsack problem (henceforth SK) [28] is a special case of problem 2 again
when f is modular and g submodular. Both these problems subsume the Set Cover and Max k-Cover
problems [3]. When both f and g are modular  Problems 1 and 2 are called knapsack problems [16].
The following are some of our contributions. We show that Problems 1 and 2 are intimately
connected  in that any approximation algorithm for either problem can be used to provide guarantees
for the other problem as well. We then provide a framework of combinatorial algorithms based
on optimizing  sometimes iteratively  subproblems that are easy to solve. These subproblems
are obtained by computing either upper or lower bound approximations of the cost functions or
constraining functions. We also show that many combinatorial algorithms like the greedy algorithm
for SK [28] and SSC [29] also belong to this framework and provide the ﬁrst constant-factor
bi-criterion approximation algorithm for SSC [29] and hence the general set cover problem [3]. We
then show how with suitable choices of approximate functions  we can obtain a number of bounded
approximation guarantees and show the hardness for Problems 1 and 2  which in fact match some
of our approximation guarantees. Our guarantees and hardness results depend on the curvature of
the submodular functions [2]. We observe a strong asymmetry in the results that the factors change

2

polynomially based on the curvature of f but only by a constant-factor with the curvature of g  hence
making the SK and SSC much easier compared to SCSK and SCSC.

2 Background and Main Ideas

f (j|V \j)

f (j)

if and only if f is modular (or additive  i.e.  f (X) = (cid:80)

We ﬁrst introduce several key concepts used throughout the paper. This paper includes only the
main results and we defer all the proofs and additional discussions to the extended version [11].
Given a submodular function f  we deﬁne the total curvature  κf as2: κf = 1 − minj∈V
[2].
Intuitively  the curvature 0 ≤ κf ≤ 1 measures the distance of f from modularity and κf = 0
j∈X f (j)). A number of approx-
imation guarantees in the context of submodular optimization have been reﬁned via the cur-
vature of the submodular function [2  13  12].
In this paper  we shall witness the role
of curvature also in determining the approximations and the hardness of problems 1 and 2.
Algorithm 1: General algorithmic framework to
The main idea of this paper is a framework of
address both Problems 1 and 2
algorithms based on choosing appropriate sur-
1: for t = 1  2 ···   T do
rogate functions for f and g to optimize over.
This framework is represented in Algorithm 1.
2:
We would like to choose surrogate functions ˆft
and ˆgt such that using them  Problems 1 and 2
become easier. If the algorithm is just single
stage (not iterative)  we represent the surrogates
as ˆf and ˆg. The surrogate functions we consider
in this paper are in the forms of bounds (upper or lower) and approximations.
Modular lower bounds: Akin to convex functions  submodular functions have tight modular lower
bounds. These bounds are related to the subdifferential ∂f (Y ) of the submodular set function f at
a set Y ⊆ V [4]. Denote a subgradient at Y by hY ∈ ∂f (Y ). The extreme points of ∂f (Y ) may
be computed via a greedy algorithm: Let π be a permutation of V that assigns the elements in Y
to the ﬁrst |Y | positions (π(i) ∈ Y if and only if i ≤ |Y |). Each such permutation deﬁnes a chain
i = {π(1)  π(2)  . . .   π(i)} and Sπ|Y | = Y . This chain deﬁnes an extreme
with elements Sπ
point hπ
Y forms a lower
Y of ∂f (Y ) with entries hπ
bound of f  tight at Y — i.e.  hπ

Choose surrogate functions ˆft and ˆgt for f
and g respectively  tight at X t−1.
Obtain X t as the optimizer of Problem 1 or
2 with ˆft and ˆgt instead of f and g.

i ) − f (Sπ
Y (j) ≤ f (X) ∀X ⊆ V and hπ

i−1). Deﬁned as above  hπ

Y (π(i)) = f (Sπ
j∈X hπ

Y (Y ) = f (Y ).

0 = ∅  Sπ

4: end for

3:

Modular upper bounds: We can also deﬁne superdifferentials ∂f (Y ) of a submodular function
[14  10] at Y . It is possible  moreover  to provide speciﬁc supergradients [10  13] that deﬁne the
following two modular upper bounds (when referring either one  we use mf

Y (X) =(cid:80)
(cid:88)

X 1(Y ) (cid:44) f (X) −(cid:88)

mf

f (j|X\j) +

f (j|∅)  mf

j∈X\Y

j∈Y \X

X 2(Y ) (cid:44) f (X) −(cid:88)

X):
f (j|V \j) +

(cid:88)

j∈X\Y

j∈Y \X

f (j|X).

X 1(Y ) ≥ f (Y ) and mf

X 2(Y ) ≥ f (Y ) ∀Y ⊆ V and mf

X 1(X) = mf

X 2(X) = f (X).

Then mf
MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in
Algorithm 1  provide a class of Majorization-Minimization (MM) algorithms  akin to the algorithms
proposed in [13] for submodular optimization and in [25  9] for DS optimization (Problem 0 above).
An appropriate choice of the bounds ensures that the algorithm always improves the objective values
for Problems 1 and 2. In particular  choosing ˆft as a modular upper bound of f tight at X t  or ˆgt as a
modular lower bound of g tight at X t  or both  ensures that the objective value of Problems 1 and
2 always improves at every iteration as long as the corresponding surrogate problem can be solved
exactly. Unfortunately  Problems 1 and 2 are NP-hard even if f or g (or both) are modular [3]  and
therefore the surrogate problems themselves cannot be solved exactly. Fortunately  the surrogate
problems are often much easier than the original ones and can admit log or constant-factor guarantees.
In practice  moreover  these factors are almost 1. Furthermore  with a simple modiﬁcation of the
iterative procedure of Algorithm 1  we can guarantee improvement at every iteration [11]. What
is also fortunate and perhaps surprising  as we show in this paper below  is that unlike the case of
DS optimization (where the problem is inapproximable in general [9])  the constrained forms of
optimization (Problems 1 and 2) do have approximation guarantees.

2We can assume  w.l.o.g that f (j) > 0  g(j) > 0 ∀j ∈ V

3

Ellipsoidal Approximation: We also consider ellipsoidal approximations (EA) of f. The main
result of Goemans et. al [6] is to provide an algorithm based on approximating the submodular
polyhedron by an ellipsoid. They show that for any polymatroid function f  one can compute

an approximation of the form (cid:112)wf (X) for a certain modular weight vector wf ∈ RV   such
n log n)(cid:112)wf (X) ∀X ⊆ V . A simple trick then provides a
that (cid:112)wf (X) ≤ f (X) ≤ O(
follows: f κ(X) (cid:44)(cid:2)f (X) − (1 − κf )(cid:80)
j∈X f (j)(cid:3)/κf . Then  the submodular function f ea(X) =
(cid:112)wf κ (X) + (1 − κf )(cid:80)
(cid:18)

curvature-dependent approximation [12] — we deﬁne the κf -curve-normalized version of f as

j∈X f (j) satisﬁes [12]:

(cid:19)

√

√

κf

f ea(X) ≤ f (X) ≤ O

f ea(X) ∀X ⊆ V

(2)

1 + (

√

n log n

n log n − 1)(1 − κf )
√

f ea is multiplicatively bounded by f by a factor depending on
n and the curvature. We shall use
the result above in providing approximation bounds for Problems 1 and 2. In particular  the surrogate
functions ˆf or ˆg in Algorithm 1 can be the ellipsoidal approximations above  and the multiplicative
bounds transform into approximation guarantees for these problems.

3 Relation between SCSC and SCSK

In this section  we show a precise relationship between Problems 1 and 2. From the formu-
lation of Problems 1 and 2  it is clear that these problems are duals of each other.
Indeed 
in this section we show that the problems are polynomially transformable into each other.
Algorithm 3: Approx. algorithm for SCSC us-
Algorithm 2: Approx. algorithm for SCSK us-
ing an approximation algorithm for SCSK.
ing an approximation algorithm for SCSC.
1: Input: An SCSC instance with cover c  an
1: Input: An SCSK instance with budget b  an
[σ  ρ] approx. algo. for SCSC  &  ∈ [0  1).
2: Output: [(1 − )ρ  σ] approx. for SCSK.
3: c ← g(V )  ˆXc ← V .
4: while f ( ˆXc) > σb do
5:
6:
7: end while

2: Output: [(1 + )σ  ρ] approx. for SCSC.
3: b ← argminj f (j)  ˆXb ← ∅.
4: while g( ˆXb) < ρc do
5:
6:
7: end while

b ← (1 + )b
ˆXb ← [ρ  σ] approx. for SCSK using b.

c ← (1 − )c
ˆXc ← [σ  ρ] approx. for SCSC using c.

[ρ  σ] approx. algo. for SCSK  &  > 0.

We ﬁrst introduce the notion of bicriteria algorithms. An algorithm is a [σ  ρ] bi-criterion algorithm for
Problem 1 if it is guaranteed to obtain a set X such that f (X) ≤ σf (X∗) (approximate optimality)
and g(X) ≥ c(cid:48) = ρc (approximate feasibility)  where X∗ is an optimizer of Problem 1. Similarly  an
algorithm is a [ρ  σ] bi-criterion algorithm for Problem 2 if it is guaranteed to obtain a set X such that
g(X) ≥ ρg(X∗) and f (X) ≤ b(cid:48) = σb  where X∗ is the optimizer of Problem 2. In a bi-criterion algo-
rithm for Problems 1 and 2  typically σ ≥ 1 and ρ ≤ 1. A non-bicriterion algorithm for Problem 1 is
when ρ = 1 and a non-bicriterion algorithm for Problem 2 is when σ = 1. Algorithms 2 and 3 provide
the schematics for using an approximation algorithm for one of the problems for solving the other.
Theorem 3.1. Algorithm 2 is guaranteed to ﬁnd a set ˆXc which is a [(1 − )ρ  σ] approximation
of SCSK in at most log1/(1−)[g(V )/ minj g(j)] calls to the [σ  ρ] approximate algorithm for SCSC.
Similarly  Algorithm 3 is guaranteed to ﬁnd a set ˆXb which is a [(1 + )σ  ρ] approximation of SCSC
in log1+[f (V )/ minj f (j)] calls to a [ρ  σ] approximate algorithm for SCSK.

Theorem 3.1 implies that the complexity of Problems 1 and 2 are identical  and a solution to one of
them provides a solution to the other. Furthermore  as expected  the hardness of Problems 1 and 2 are
also almost identical. When f and g are polymatroid functions  moreover  we can provide bounded ap-
proximation guarantees for both problems  as shown in the next section. Alternatively we can also do a
binary search instead of a linear search to transform Problems 1 and 2. This essentially turns the factor
of O(1/) into O(log 1/). Due to lack of space  we defer this discussion to the extended version [11].

4

4 Approximation Algorithms

We consider several algorithms for Problems 1 and 2  which can all be characterized by the framework
of Algorithm 1  using the surrogate functions of the form of upper/lower bounds or approximations.

4.1 Approximation Algorithms for SCSC
We ﬁrst describe our approximation algorithms designed speciﬁcally for SCSC  leaving to §4.2 the
presentation of our algorithms slated for SCSK. We ﬁrst investigate a special case  the submodular
set cover (SSC)  and then provide two algorithms  one of them (ISSC) is very practical with a weaker
theoretical guarantee  and another one (EASSC) which is slow but has the tightest guarantee.
Submodular Set Cover (SSC): We start by considering a classical special case of SCSC (Problem
1) where f is already a modular function and g is a submodular function. This problem occurs
naturally in a number of problems related to active/online learning [7] and summarization [21  22].
This problem was ﬁrst investigated by Wolsey [29]  wherein he showed that a simple greedy algorithm
achieves bounded (in fact  log-factor) approximation guarantees. We show that this greedy algorithm
can naturally be viewed in the framework of our Algorithm 1 by choosing appropriate surrogate
functions ˆft and ˆgt. The idea is to use the modular function f as its own surrogate ˆft and choose the
function ˆgt as a modular lower bound of g. Akin to the framework of algorithms in [13]  the crucial
factor is the choice of the lower bound (or subgradient). Deﬁne the greedy subgradient as:

π(i) ∈ argmin

i−1  g(Sπ

(3)
g(j|Sπ
i−1)
i−1 ∪ j) < c can no longer be satisﬁed by any j /∈ Sπ
Once we reach an i where the constraint g(Sπ
i−1 
we choose the remaining elements for π arbitrarily. Let the corresponding subgradient be referred
to as hπ. Then we have the following lemma  which is an extension of [29]  and which is a simpler
description of the result stated formally in [11].
Lemma 4.1. The greedy algorithm for SSC [29] can be seen as an instance of Algorithm 1 by
choosing the surrogate function ˆf as f and ˆg as hπ (with π deﬁned in Eqn. (3)).
When g is integral  the guarantee of the greedy algorithm is Hg (cid:44) H(maxj g(j))  where
i [29] (henceforth we will use Hg for this quantity). This factor is tight up to lower-
order terms [3]. Furthermore  since this algorithm directly solves SSC  we call it the primal greedy.
We could also solve SSC by looking at its dual  which is SK [28]. Although SSC does not admit any
constant-factor approximation algorithms [3]  we can obtain a constant-factor bi-criterion guarantee:
Lemma 4.2. Using the greedy algorithm for SK [28] as the approximation oracle in Algorithm 3
provides a [1 +   1 − e−1] bi-criterion approximation algorithm for SSC  for any  > 0.

H(d) =(cid:80)d

i=1

1

(cid:26) f (j)

(cid:12)(cid:12)(cid:12)(cid:12) j /∈ Sπ

(cid:27)
i−1 ∪ j) < c

.

We call this the dual greedy. This result follows immediately from the guarantee of the submodular
cost knapsack problem [28] and Theorem 3.1. We remark that we can also use a simpler version
of the greedy iteration at every iteration [21  17] and we obtain a guarantee of (1 +   1/2(1 − e−1)).
In practice  however  both these factors are almost 1 and hence the simple variant of the greedy
algorithm sufﬁces.
Iterated Submodular Set Cover (ISSC): We next investigate an algorithm for the general SCSC
problem when both f and g are submodular. The idea here is to iteratively solve the submodular
set cover problem which can be done by replacing f by a modular upper bound at every iteration.
In particular  this can be seen as a variant of Algorithm 1  where we start with X 0 = ∅ and
choose ˆft(X) = mf
X t(X) at every iteration. The surrogate problem at each iteration becomes
min{mf
X t(X)|g(X) ≥ c}. Hence  each iteration is an instance of SSC and can be solved nearly
optimally using the greedy algorithm. We can continue this algorithm for T iterations or until
convergence. An analysis very similar to the ones in [9  13] will reveal polynomial time convergence.
Since each iteration is only the greedy algorithm  this approach is also highly practical and scalable.
Theorem 4.3. ISSC obtains an approximation factor of
1+(n−1)(1−κf ) Hg where
Kg = 1 + max{|X| : g(X) < c} and Hg is the approximation factor of the submodular set cover
using g.

1+(Kg−1)(1−κf ) ≤

KgHg

n

5

From the above  it is clear that Kg ≤ n. Notice also that Hg is essentially a log-factor. We also
see an interesting effect of the curvature κf of f. When f is modular (κf = 0)  we recover the
approximation guarantee of the submodular set cover problem. Similarly  when f has restricted
curvature  the guarantees can be much better. Moreover  the approximation guarantee already holds
after the ﬁrst iteration  so additional iterations can only further improve the objective.
Ellipsoidal Approximation based Submodular Set Cover (EASSC): In this setting  we use the
ellipsoidal approximation discussed in §2. We can compute the κf -curve-normalized version of f
(f κ  see §2)  and then compute its ellipsoidal approximation
wf κ. We then deﬁne the function
ˆf (X) = f ea(X) = κf
j∈X f (j) and use this as the surrogate function ˆf
for f. We choose ˆg as g itself. The surrogate problem becomes:

√

(cid:112)wf κ(X) + (1 − κf )(cid:80)
κf

(cid:113)

wf κ(X) + (1 − κf )

min

(cid:88)

j∈X

(cid:12)(cid:12)(cid:12)(cid:12) g(X) ≥ c

 .

f (j)

(4)

While function ˆf (X) = f ea(X) is not modular  it is a weighted sum of a concave over modular
function and a modular function. Fortunately  we can use the result from [26]  where they show

that any function of the form of(cid:112)w1(X) + w2(X) can be optimized over any polytope P with an

approximation factor of β(1 + ) for any  > 0  where β is the approximation factor of optimizing
a modular function over P. The complexity of this algorithm is polynomial in n and 1
 . We use
their algorithm to minimize f ea(X) over the submodular set cover constraint and hence we call this
algorithm EASSC.
Theorem 4.4. EASSC obtains a guarantee of O(
tion guarantee of the set cover problem.

√
n log n−1)(1−κf ) )  where Hg is the approxima-

n log nHg

1+(

√

If the function f has κf = 1  we can use a much simpler algorithm. In particular  we can minimize
(f ea(X))2 = wf (X) at every iteration  giving a surrogate problem of the form min{wf (X)|g(X) ≥
c}. This is directly an instance of SSC  and in contrast to EASSC  we just need to solve SSC once.
We call this algorithm EASSCc.
√
Corollary 4.5. EASSCc obtains an approximation guarantee of O(

n log n(cid:112)Hg).

4.2 Approximation Algorithms for SCSK

In this section  we describe our approximation algorithms for SCSK. We note the dual nature of
the algorithms in this current section to those given in §4.1. We ﬁrst investigate a special case  the
submodular knapsack (SK)  and then provide three algorithms  two of them (Gr and ISK) being
practical with slightly weaker theoretical guarantee  and another one (EASK) which is not scalable
but has the tightest guarantee.
Submodular Cost Knapsack (SK): We start with a special case of SCSK (Problem 2)  where f is
a modular function and g is a submodular function. In this case  SCSK turns into the SK problem for
which the greedy algorithm with partial enumeration provides a 1 − e−1 approximation [28]. The
greedy algorithm can be seen as an instance of Algorithm 1 with ˆg being the modular lower bound of
g and ˆf being f  which is already modular. In particular  deﬁne:

π(i) ∈ argmax

i−1  f (Sπ

i−1 ∪ {j}) ≤ b

 

(5)

(cid:26) g(j|Sπ

i−1)

f (j)

(cid:12)(cid:12)(cid:12)(cid:12) j /∈ Sπ

(cid:27)

where the remaining elements are chosen arbitrarily. The following is an informal description of the
result described formally in [11].
Lemma 4.6. Choosing the surrogate function ˆf as f and ˆg as hπ (with π deﬁned in eqn (5)) in
Algorithm 1 with appropriate initialization obtains a guarantee of 1 − 1/e for SK.
Greedy (Gr): A similar greedy algorithm can provide approximation guarantees for the general
SCSK problem  with submodular f and g. Unlike the knapsack case in (5)  however  at iteration
i−1 ∪ {j}) ≤ b which maximizes g(j|Si−1). In terms of
i we choose an element j /∈ Si−1 : f (Sπ
Algorithm 1  this is analogous to choosing a permutation  π such that:
π(i) ∈ argmax{g(j|Sπ
i−1)|j /∈ Sπ

i−1 ∪ {j}) ≤ b}.

i−1  f (Sπ

(6)

6

)kf ) ≥
  where Kf = max{|X| : f (X) ≤ b} and kf = min{|X| : f (X) ≤ b & ∀j ∈ X  f (X∪j) > b}.

Theorem 4.7. The greedy algorithm for SCSK obtains an approx. factor of 1
κg
1
Kf

(1 − ( Kf−κg

Kf

In the worst case  kf = 1 and Kf = n  in which case the guarantee is 1/n. The bound above
follows from a simple observation that the constraint {f (X) ≤ b} is down-monotone for a monotone
function f. However  in this variant  we do not use any speciﬁc information about f. In particular
it holds for maximizing a submodular function g over any down monotone constraint [2]. Hence
it is conceivable that an algorithm that uses both f and g to choose the next element could provide
better bounds. We do not  however  currently have the analysis for this.
Iterated Submodular Cost Knapsack (ISK): Here  we choose ˆft(X) as a modular upper bound
of f  tight at X t. Let ˆgt = g. Then at every iteration  we solve max{g(X)|mf
X t(X) ≤ b}  which is
a submodular maximization problem subject to a knapsack constraint (SK). As mentioned above 
j∈X f (j) and then
iteratively continue this process until convergence (note that this is an ascent algorithm). We have the
following theoretical guarantee:
Theorem 4.8. Algorithm ISK obtains a set X t such that g(X t) ≥ (1−e−1)g( ˜X)  where ˜X is the opti-
and where Kf = max{|X| : f (X) ≤ b}.
mal solution of max

greedy can solve this nearly optimally. We start with X 0 = ∅  choose ˆf0(X) =(cid:80)

g(X) | f (X) ≤ b(1+(Kf−1)(1−κf )

(cid:110)

(cid:111)

Kf

It is worth pointing out that the above bound holds even after the ﬁrst iteration of the algorithm. It is
interesting to note the similarity between this approach and ISSC. Notice that the guarantee above is
not a standard bi-criterion approximation. We show in the extended version [11] that with a simple
transformation  we can obtain a bicriterion guarantee.
Ellipsoidal Approximation based Submodular Cost Knapsack (EASK): Choosing the Ellip-
soidal Approximation f ea of f as a surrogate function  we obtain a simpler problem:

wf κ (X) + (1 − κf )

f (j) ≤ b

(7)

In order to solve this problem  we look at its dual problem (i.e.  Eqn. (4)) and use Algorithm 2 to
convert the guarantees. We call this procedure EASK. We then obtain guarantees very similar to
Theorem 4.4.
Lemma 4.9. EASK obtains a guarantee of

1 +   O(

√
n log n−1)(1−κf ) )

n log nHg

(cid:104)

1+(

√

directly choose the ellipsoidal approximation of f as(cid:112)wf (X) and solve the surrogate problem:

In the case when the submodular function has a curvature κf = 1  we can actually provide a simpler
algorithm without needing to use the conversion algorithm (Algorithm 2). In this case  we can
max{g(X) : wf (X) ≤ b2}. This surrogate problem is a submodular cost knapsack problem  which
we can solve using the greedy algorithm. We call this algorithm EASKc. This guarantee is tight up to
log factors if κf = 1.
√
Corollary 4.10. Algorithm EASKc obtains a bi-criterion guarantee of [1 − e−1  O(

n log n)].

4.3 Extensions beyond SCSC and SCSK

SCSC and SCSK can in fact be extended to more ﬂexible and complicated constraints which can arise
naturally in many applications [18  8]. These include multiple covering and knapsack constraints –
i.e.  min{f (X)|gi(X) ≥ ci  i = 1  2 ··· k} and max{g(X)|fi(X) ≤ bi  i = 1  2 ··· k}  and robust
optimization problems like max{mini gi(X)|f (X) ≤ b}  where the functions f  g  fi’s and gi’s are
submodular. We also consider SCSC and SCSK with non-monotone submodular functions. Due to
lack of space  we defer these discussions to the extended version of this paper [11].

4.4 Hardness

In this section  we provide the hardness for Problems 1 and 2. The lower bounds serve to show that
the approximation factors above are almost tight.

7

g(X)

(cid:113)

(cid:12)(cid:12)(cid:12)(cid:12) κf

max

(cid:88)

j∈X

 .
(cid:105)

.

Theorem 4.11. For any κ > 0  there exists submodular functions with curvature κ such that
no polynomial time algorithm for Problems 1 and 2 achieves a bi-criterion factor better than
σ
ρ =

1+(n1/2−−1)(1−κ) for any  > 0.

n1/2−

The above result shows that EASSC and EASK meet the bounds above to log factors. We see an
interesting curvature-dependent inﬂuence on the hardness. We also see this phenomenon in the
approximation guarantees of our algorithms. In particular  as soon as f becomes modular  the
problem becomes easy  even when g is submodular. This is not surprising since the submodular
set cover problem and the submodular cost knapsack problem both have constant factor guarantees.

5 Experiments

i∈V min{(cid:80)

g  we use two types of coverage: one is a facility location function g1(X) = (cid:80)
while the other is a saturated sum function g2(X) = (cid:80)
j∈X sij  α(cid:80)

In this section  we empirically compare the performance of the various algorithms discussed in this
paper. We are motivated by the speech data subset selection application [20  23] with the submodular
function f encouraging limited vocabulary while g tries to achieve acoustic variability. A natural
choice of the function f is a function of the form |Γ(X)|  where Γ(X) is the neighborhood function
on a bipartite graph constructed between the utterances and the words [23]. For the coverage function
i∈V maxj∈X sij
j∈V sij}. Both
these functions are deﬁned in terms of a similarity matrix S = {sij}i j∈V   which we deﬁne on the
TIMIT corpus [5]  using the string kernel metric [27] for similarity. Since some of our algorithms  like
the Ellipsoidal Approximations  are computationally intensive  we restrict ourselves to 50 utterances.
We compare our different algorithms on
Problems 1 and 2 with f being the bipartite
neighborhood and g being the facility location
and saturated sum respectively. Furthermore 
in our experiments  we observe that the neigh-
borhood function f has a curvature κf = 1.
Thus  it sufﬁces to use the simpler versions
of algorithm EA (i.e.  algorithm EASSCc and
EASKc). The results are shown in Figure 1. We observe that on the real-world instances  all our
algorithms perform almost comparably. This implies  moreover  that the iterative variants  viz. Gr 
ISSC and ISK  perform comparably to the more complicated EA-based ones  although EASSC and
EASK have better theoretical guarantees. We also compare against a baseline of selecting random
sets (of varying cardinality)  and we see that our algorithms all perform much better. In terms of
the running time  computing the Ellipsoidal Approximation for |Γ(X)| with |V | = 50 takes about 5
hours while all the iterative variants (i.e.  Gr  ISSC and ISK) take less than a second. This difference
is much more prominent on larger instances (for example |V | = 500).

Figure 1: Comparison of the algorithms in the text.

6 Discussions

In this paper  we propose a unifying framework for problems 1 and 2 based on suitable surrogate
functions. We provide a number of iterative algorithms which are very practical and scalable (like
Gr  ISK and ISSC)  and also algorithms like EASSC and EASK  which though more intensive 
obtain tight approximation bounds. Finally  we empirically compare our algorithms  and show that
the iterative algorithms compete empirically with the more complicated and theoretically better
approximation algorithms. For future work  we would like to empirically evaluate our algorithms on
many of the real world problems described above  particularly the limited vocabulary data subset
selection application for speech corpora  and the machine translation application.
Acknowledgments: Special thanks to Kai Wei and Stefanie Jegelka for discussions  to Bethany
Herwaldt for going through an early draft of this manuscript and to the anonymous reviewers for
useful reviews. This material is based upon work supported by the National Science Foundation
under Grant No. (IIS-1162606)  a Google and a Microsoft award  and by the Intel Science and
Technology Center for Pervasive Computing.

8

010020025001020304050f(X)g(X)Fac. Location/ Bipartite Neighbor.  ISSCEASSCcISKGrEASKcRandom204060801000100200300f(X)g(X)Saturated Sum/ Bipartite Neighbor  ISSCEASSCcISKGrEASKcRandomReferences
[1] A. Atamt¨urk and V. Narayanan. The submodular knapsack polytope. Discrete Optimization  2009.
[2] M. Conforti and G. Cornuejols. Submodular set functions  matroids and the greedy algorithm: tight worst-
case bounds and some generalizations of the Rado-Edmonds theorem. Discrete Applied Mathematics 
7(3):251–274  1984.

[3] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM (JACM)  1998.
[4] S. Fujishige. Submodular functions and optimization  volume 58. Elsevier Science  2005.
[5] J. Garofolo  F. Lamel  L.  J. W.  Fiscus  D. Pallet  and N. Dahlgren. Timit  acoustic-phonetic continuous

speech corpus. In DARPA  1993.

[6] M. Goemans  N. Harvey  S. Iwata  and V. Mirrokni. Approximating submodular functions everywhere. In

SODA  pages 535–544  2009.

[7] A. Guillory and J. Bilmes. Interactive submodular set cover. In ICML  2010.
[8] A. Guillory and J. Bilmes. Simultaneous learning and covering with adversarial noise. In ICML  2011.
[9] R. Iyer and J. Bilmes. Algorithms for approximate minimization of the difference between submodular

functions  with applications. In UAI  2012.

[10] R. Iyer and J. Bilmes. The submodular Bregman and Lov´asz-Bregman divergences with applications. In

NIPS  2012.

[11] R. Iyer and J. Bilmes. Submodular Optimization with Submodular Cover and Submodular Knapsack

Constraints: Extended arxiv version  2013.

[12] R. Iyer  S. Jegelka  and J. Bilmes. Curvature and Optimal Algorithms for Learning and Minimizing

Submodular Functions . In NIPS  2013.

[13] R. Iyer  S. Jegelka  and J. Bilmes. Fast semidifferential based submodular function optimization. In ICML 

2013.

[14] S. Jegelka and J. A. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In

CVPR  2011.

[15] Y. Kawahara and T. Washio. Prismatic algorithm for discrete dc programming problems. In NIPS  2011.
[16] H. Kellerer  U. Pferschy  and D. Pisinger. Knapsack problems. Springer Verlag  2004.
[17] A. Krause and C. Guestrin. A note on the budgeted maximization on submodular functions. Technical

Report CMU-CALD-05-103  Carnegie Mellon University  2005.

[18] A. Krause  B. McMahan  C. Guestrin  and A. Gupta. Robust submodular observation selection. Journal of

Machine Learning Research (JMLR)  9:2761–2801  2008.

[19] A. Krause  A. Singh  and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory 

efﬁcient algorithms and empirical studies. JMLR  9:235–284  2008.

[20] H. Lin and J. Bilmes. How to select a good training-data subset for transcription: Submodular active

selection for sequences. In Interspeech  2009.

[21] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submodular functions.

In NAACL  2010.

[22] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In The 49th Annual
Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL/HLT-
2011)  Portland  OR  June 2011.

[23] H. Lin and J. Bilmes. Optimal selection of limited vocabulary speech corpora. In Interspeech  2011.
[24] R. C. Moore and W. Lewis. Intelligent selection of language model training data. In Proceedings of the

ACL 2010 Conference Short Papers  pages 220–224. Association for Computational Linguistics  2010.

[25] M. Narasimhan and J. Bilmes. A submodular-supermodular procedure with applications to discriminative

structure learning. In UAI  2005.

[26] E. Nikolova. Approximation algorithms for ofﬂine risk-averse combinatorial optimization  2010.
[27] J. Rousu and J. Shawe-Taylor. Efﬁcient computation of gapped substring kernels on large alphabets.

Journal of Machine Learning Research  6(2):1323  2006.

[28] M. Sviridenko. A note on maximizing a submodular set function subject to a knapsack constraint.

Operations Research Letters  32(1):41–43  2004.

[29] L. A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica 

2(4):385–393  1982.

9

,Rishabh Iyer
Jeff Bilmes
Mohammad Saberian
Nuno Vasconcelos
Thomas Pumir
Samy Jelassi
Nicolas Boumal