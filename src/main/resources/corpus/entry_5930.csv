2016,What Makes Objects Similar: A Unified Multi-Metric Learning Approach,Linkages are essentially determined by similarity measures that may be derived from multiple perspectives. For example  spatial linkages are usually generated based on localities of heterogeneous data  whereas semantic linkages can come from various properties  such as different physical meanings behind social relations. Many existing metric learning models focus on spatial linkages  but leave the rich semantic factors unconsidered. Similarities based on these models are usually overdetermined on linkages. We propose a Unified Multi-Metric Learning (UM2L) framework to exploit multiple types of metrics. In UM2L  a type of combination operator is introduced for distance characterization from multiple perspectives  and thus can introduce flexibilities for representing and utilizing both spatial and semantic linkages. Besides  we propose a uniform solver for UM2L which is guaranteed to converge. Extensive experiments on diverse applications exhibit the superior classification performance and comprehensibility of UM2L. Visualization results also validate its ability on physical meanings discovery.,What Makes Objects Similar:

A Uniﬁed Multi-Metric Learning Approach

Han-Jia Ye

Yuan Jiang

Zhi-Hua Zhou

De-Chuan Zhan
National Key Laboratory for Novel Software Technology 

Xue-Min Si

Nanjing University  Nanjing  210023  China

{yehj zhandc sixm jiangy zhouzh}@lamda.nju.edu.cn

Abstract

Linkages are essentially determined by similarity measures that may be derived
from multiple perspectives. For example  spatial linkages are usually generated
based on localities of heterogeneous data  whereas semantic linkages can come
from various properties  such as different physical meanings behind social rela-
tions. Many existing metric learning models focus on spatial linkages  but leave
the rich semantic factors unconsidered. Similarities based on these models are
usually overdetermined on linkages. We propose a Uniﬁed Multi-Metric Learn-
ing (UM2L) framework to exploit multiple types of metrics. In UM2L  a type of
combination operator is introduced for distance characterization from multiple per-
spectives  and thus can introduce ﬂexibilities for representing and utilizing both
spatial and semantic linkages. Besides  we propose a uniform solver for UM2L
which is guaranteed to converge. Extensive experiments on diverse applications
exhibit the superior classiﬁcation performance and comprehensibility of UM2L.
Visualization results also validate its ability on physical meanings discovery.

1 Introduction

Similarities measure the closeness of connections between objects and usually are reﬂected by dis-
tances. Distance Metric Learning (DML) aims to learn appropriate metric that can ﬁgure out the
underlying linkages or connections  thus can greatly improve the performance of similarity-based
classiﬁers  such as kNN.
Objects are linked with each other for different reasons. Global DML methods consider the deter-
ministic single metric which measures similarities between all object pairs. Recently  investigations
on local DML have considered locality speciﬁc approaches  and consequently multiple metrics are
learned. These metrics are either in charge of different spatial areas [15  20] or responsible for each
speciﬁc instance [7  22]. Both global and local DML methods emphasize the linkage constraints
(including must-link and cannot-link) in localities with univocal semantic meaning  e.g.  the side
information of class. However  there can be many different reasons for two instances to be similar
in real world applications [3  9].
Linkages between objects can be with multiple latent semantics. For example  in a social network 
friendship linkages may lie on different hobbies of users. Although a user has many friends  their
common hobbies could be different and as a consequence  one can be friends with others for differ-
ent reasons. Another concrete example is  for articles on “A. Feature Learning” which are closely
related to both “B. Feature Selection” and “C. Subspace Models”  their connections are different in
semantics. The linkage between A and B emphasizes “picking up some helpful features”  while the
common semantic between A and C is about “extracting subspaces” or “ feature transformation”.
These phenomena clearly indicate ambiguities rather than a single meaning in linkage generation.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Hence  the distance/similarity measurements are overdetermined in these applications. As a conse-
quence  a new type of multi-metric learner which can describe the ambiguous linkages is desired.
In this paper  we propose a Uniﬁed Multi-Metric Learning (UM2L) approach which integrates the
consideration of linking semantic ambiguities and localities in one framework. In the training pro-
cess  more than one metric is learned to measure distances between instances and each of them
reﬂects a type of inherent spatial or semantic properties of objects. During the test  UM2L can auto-
matically pick up or integrate these measurements  since semantically/spatially similar data points
have small distances and otherwise they are pulled away from each other; such a mechanism en-
ables the adaptation to environment to some degree  which is important for the development of
learnwares [25]. Furthermore  the proposed framework can be easily adapted to different types of
ambiguous circumstances: by specifying the mechanism of metric integration  various types of link-
ages in applications can be considered; by incorporating sparse constraints  UM2L also turns out
good visualization results reﬂecting physical meanings of latent linkages between objects; besides 
by limiting the number of metrics or specifying the regularizer  the approach can be degenerated
to some popular DML methods  such as MMLMNN [20]. Beneﬁtting from alternative strategy and
stochastic techniques  the general framework can be optimized steadily and efﬁciently.
Our main contributions are: (I) A Uniﬁed Multi-Metric Learning framework considering both data
localities and ambiguous semantics linkages. (II) A ﬂexible framework adaptable for different tasks.
(III) Uniﬁed and efﬁcient optimization solutions  superior and interpretable results.
The rest of this paper starts with some notations. Then the UM2L framework is presented in detail 
which is followed by a review of related work. The last are experiments and conclusion.

2 The Uniﬁed Multi-Metric Framework

Dis2

√

∑

d
i

⊤ ∈ S +

M (xt − yt) = Tr(M At

Generally speaking  the supervision information for Distance Metric Learning (DML) is formed as
pairwise constraints or triplet sets. We restrict our discussion on the latter one  T = {xt  yt  zt}T
t=1 
since it provides more local information. In each triplet  target instance yt is more similar to xt
than imposter zt and {xt  yt  zt} ∈ Rd. Sd and S +
d are the set of symmetric and positive semi-
deﬁnite (PSD) matrix of size d × d  respectively. I is the identity matrix. Matrix Frobenius Norm
∥M∥F =
Tr(M⊤M ). Let mi and mj denote the i-th row and j-th column of matrix M respec-
∥mi∥2. Operator [·]+ = max(·  0) preserves the non-negative
tively  and ℓ2;1-norm ∥M∥2;1 =
part of the input value. DML aims at learning a metric M ∈ S +
d making similar instances have small
distances to each other and dissimilar ones far apart. The (squared) Mahalanobis distance between
pair (xt  yt) with metric M can be denoted as:
M (xt  yt) = (xt − yt)
⊤

(1)
xy = (xt − yt)(xt − yt)
d is the outer product of difference between instance xt and
At
yt. The distance in Eq.1 assumes that there is a single type of relationship between object features 
which uses univocal linkages between objects.
Multi-metric learning takes data heterogeneities into consideration. However  both single metric
learned by global DML and multiple metrics learned with local methods focus on exploiting locality
information  i.e.  constraints or metrics are closely related to the localities. In particular  local DML
approaches mainly aim at learning a set of multiple metrics one for each local area. In this paper 
a general multi-metric conﬁguration is investigated to deal with linkage ambiguities from both se-
mantic and locality perspectives. We denote the set of K multiple metrics to be learned as MK =
{M1  M2  . . .   MK} and {Mk}K
d . Similarity score between a pair of instances based on Mk 
w.l.o.g.  can be set as the negative distance  i.e.  fMk (xt  yt) = −Dis2
(xt  yt). In multi-metric
scenario  consequently  there will be a set of multiple similarity scores fMK = {fMk
}K
k=1. Each
metric/score in the set reﬂects a particular semantic or spatial view of data. The overall similarity
score f v(xt  yt) = κv(fMK (xt  yt))  v = {1  2} and κv(·) is a functional operator closely related
to concrete applications  which maps the set of similarity scores w.r.t. all metrics to a single value.
With these discussions  the Uniﬁed Multi-Metric Learning (UM2L) framework can be denoted as:

∈ S +

xy).

T∑

ℓ

t=1

minMK

1
T

(
f 1(xt  yt) − f 2(xt  zt)

)

K∑

+ λ

Ωk(Mk) .

k=1

k=1

Mk

(2)

2

The overall inter-instance similarity f 1 and f 2 are based on operators κ1 and κ2 respectively. ℓ(·) is
a convex loss function which encourages (xt  yt) to have larger overall similarity score than (xt  zt).
Note that although inter-instance similarities are deﬁned on different metrics in MK  the convex loss
function ℓ(·) acts as a bridge and makes the similarities measured by different metrics comparable as
in [20]. The fact that triplet restrictions being provided without specifying concrete measurements
makes it reasonable to use ﬂexible κs. For instance  in a social network  similar nodes only share
some common interests (features) rather than consistently possessing all interests. Tendency on
different types of hobbies can be reﬂected by various metrics. Therefore  the similarity scores may
be calculated with different measurements and operator κv is used for taking charge of “selecting”
or “integrating” the right base metric for measuring similarities. The choices of loss functions and
κs are substantial issues in this framework and will be described later. Convex regularizer Ωk(Mk)
can impose prior or structure information on base metric Mk. λ ≥ 0 is a balance parameter.

2.1 Choices for κ

UM2L takes both spatial and ambiguous semantic linkages into account based on the conﬁgurations
of κ  which integrates or selects base metrics. As an integrator  in applications where locality related
multiple metrics are needed  κ can be an RBF like function which decreases as the distance is in-
creasing. The locality determines the impact of each metric. When κ acts as a selector  UM2L should
automatically assign triplets to one of the metrics which can explain instance similarity/dissimilarity
best. Besides  from the aspect of loss function ℓ(·)  the elected fs form a comparable set of simi-
larity measurements [17  20]. In this case  we may implement the operator κ by choosing the most
remarkable base metric making the pair of instances xt and yt similar. Advantages of selection
mechanism are two folds. First  it reduces the impact of initial triplets construction in localities [19];
second  it stresses the most evident semantic and reﬂects the consideration of ambiguous semantics
in a linkage construction. Choices of κs heavily depend on concrete applications. It is actually a
combiner and can get inspiration from ensemble methods [24]. Here  we mainly consider 4 different
types of linkage based on various sets of κs as follows.
Apical Dominance Similarity (ADS): which is named after the phenomenon in auxanology of
plants  where the most important term dominates the evaluation. In this case  κ1 = κ2 = max(·) 
i.e.  maximum similarity among all similarities calculated with MK on similar pair (xt  yt) should
be larger than the maximum similarity of (xt  zt). This corresponds to similar pairs being close
to each other under at least one measurement  meanwhile dissimilar pairs are disconnected by all
different measurements. This type of linkage generation often occurs in social network applications 
e.g.  nodes are linked together for a portion of similar orientations while nodes are unlinked because
there are no common interests. By explicitly modeling each node in a social network as an instance 
each of the base metrics {Mk}K
k=1 can represent parts of semantics in linkages. Then the dissimilar
pair in a triplet  e.g.  the non-friendship relationship  should be with small similarity scores over
MK; while for the similar pair  there should be at least one base similarity score with high value 
which reﬂects their common interests [3  11].
One Vote Similarity (OVS): which indicates the existence of potential key metric in MK  i.e. 
either similar or dissimilar pair is judged by at least one key metric respectively  while remaining
metrics with other semantic meanings are ignored. In this case  κ1 = max(·) and κ2 = min(·). This
type of similarity should usually be applied as an “interpreter” in domains like image  video which
are with complicated semantics. The learned metrics reveal different latent concepts in objects. Note
that simply applying OVS in UM2L with impropriate regularizer Ω will lead to a trivial solution  i.e. 
Mk = 0  which satisﬁes all similar pair restrictions yet has no generalization ability. Therefore  we
need to set Ωk(Mk) = ∥Mk − I∥2
Rank Grouping Similarity (RGS): which groups the pairs and makes the similar pairs with higher
ranks than dissimilar ones. This is the most rigorous similarity and we also refer it as One-Vote
Veto Similarity (OV2S). In this case  κ1 = min(·) while κ2 = max(·)  which regards the pairs as
dissimilar even when there is only one metric denying the linkage. This case is usually applied to
applications where latent multiple views exist and different views are measured by different metrics
∑
in MK. In these applications  it is obviously required that all potential views obtain consistencies 
and weak conﬂict detected by one metric should also be punished by RGS (OV2S) loss.
(·).
Average Case Similarity (ACS): which treats all metrics in MK equally  i.e.  κ1 = κ2 =
This is the general case when there is no prior knowledge on applications.

F or restrict the trace of Mk to equal to 1.

3

∑
There are many derivatives of similarity where κv is conﬁgured as min(·)  max(·) and
(·). Fur-
thermore  κv in fact can be with richer forms  and we will leave the discussions of choosing different
κs later in section 3. Besides  in the framework  multiple choices of the regularizer Ωk(·) can be
made. As most DML methods [14]  Ωk(Mk) can be set as ∥Mk∥2
F . Yet it also can be incorporated
with more structural information  e.g.  we can conﬁgure Ω(Mk) = ∥Mk∥2;1  where the row/column
sparsity ﬁlters inﬂuential features for composing linkages in a network; or Ωk(Mk) = Tr(Mk) 
which guarantees the low rank property for all metrics. Due to the high applicability of the proposed
framework  we name it as UM2L (Uniﬁed Multi-Metric Learning).

2.2 General Solutions for UM2L
UM2L can be solved alternatively between metrics MK and afﬁliation portion of each instance 
when κ is a piecewise linear operator such as max(·) and min(·). For example  in the case
of ADS  the metric used to measure the similarity of pair (xt  yt) is decided by: kt
v;∗ =
arg maxk fMk (xt  yt)  which is the index of the metric Mk that has the largest similarity value
over the pair. Once the dominating key metric of each instance is found  the whole optimization
problem is convex w.r.t. each Mk  which can be easily optimized. On account of the convexity of
each sub-problem in the alternating approach  the whole objective is ensured to decrease in itera-
tions so as to converge eventually. It is notable that when dealing with a single triplet in a stochastic
approach  the convergence can be guaranteed as well in Theorem 1  which will be introduced later.
In batch case  for facilitating the discussion  we can implement ℓ(·) as the smooth hinge loss  i.e. 
2 (1 − x)2 otherwise. If trace norm Ωk(Mk) =
ℓ(x) = [ 1
Tr(Mk) is used  MK can be solved with accelerated projected gradient descent method. If the
2
whole objective in Eq. 2 is denoted as LMK   the gradient w.r.t. one metric Mk can be computed as:

− x]+ if x ≥ 1 or x ≤ 0 and equals to 1

∑

∑

1
T

∇t
Mk (at) + λI  

∂LMK
∂Mk

1
T

∂ℓ(Tr(Mkt

2;(cid:3) At

xz) − Tr(Mkt
∂Mk

1;(cid:3) At

xy))

=

t∈ ^Tk

+ λI =

(3)
where the ﬁrst part is a sum of gradients over the triplets subset ˆTk whose membership indexes
containing k  i.e.  ˆTk = {t | k = kt
(at)  with
at = Tr(Mkt
∇t

xy)  for triplet t ∈ ˆTk is:
xy − δ(k = kt

2;∗}. The separated gradient ∇t

{
xz) − Tr(Mkt

1;∗ or k = kt

2;∗)At
xz

(at) =

2;(cid:3) At

1;(cid:3)At

t∈ ^Tk

Mk

.

1;∗)At
1;∗)(1 − at)At

xy − δ(k = kt

2;∗)(1 − at)At

xz

if at ≥ 1
if at ≤ 0
otherwise

0
δ(k = kt
δ(k = kt

Mk

δ(·) is the Kronecker delta function  which contributes to the computation of the gradient when κv is
optimized by Mk. After accelerated gradient descent  a projection step is conducted to maintain the
PSD property of each solution. If structured sparsity is stressed  ℓ2;1-norm is used as a regularizer 
i.e.  Ωk(Mk) = ∥Mk∥2;1. FISTA [2] can be used to optimize the non-smooth regularizer efﬁciently:
after a gradient descent with step size γ on the smooth loss to get an intermediate solution Vk =
Mk−γ 1
(at)  the following proximal sub-problem is conducted to get a further update:

∑

∇t

T

t∈ ^Tk

Mk

M

′
k = arg min
M∈Sd

∥M − Vk∥2

F + λ∥M∥2;1 .

1
2

(4)

The PSD property of Mk can be ensured by a projection in each iteration  or can often be preserved
by last step projection [14]. Hence  in Eq. 4  only symmetric constraint of Mk is imposed. Since
ℓ2;1-norm considers only one-side (row-wise) property of a matrix  Lim et al. [12] uses iterative
symmetric projection to get a solution  which has heavy computational cost in some cases. In a
reweighted way  the proximal subproblem can be tackled by the following lemma efﬁciently.
Lemma 1 The proximal problem in Eq. 4 can be solved by updating diagonal matrixes D1 and D2
and symmetric matrix M alternatively:
}d
{D1;ii =
i=1 ; vec(M ) = (I ⊗ (I +
−1vec(Vk)  
2∥mi∥2
where vec(·) is the vector form of a matrix and ⊗ means the Kronecker product. Due to the diagonal
property of each term  the update of M can be further simpliﬁed.1

D2 ⊗ I))

2∥mi∥2

  D2;ii =

D1) + (

λ
2

λ
2

1

1

1Detailed derivation and efﬁciency comparison are in the supplementary material.

4

The update of M in Lemma 1 takes row-wise and column-wise ℓ2-norm into consideration simulta-
neously  and usually gets converged in about 5 ∼ 10 iterations.
The batch solution for UM2L can beneﬁt from the acceleration strategy [2]. The computational cost
of a full gradient  however  sometimes becomes the dominant expense owing to the huge number
of triplets. Inspired by [6]  we propose a stochastic solution  which manipulates one triplet in each
iteration. In the s-th iteration  we sample a triplet (xs  ys  zs) uniformly and update current solution
set Ms

K∑
k=1. The whole objective of s-th iteration with Ms
}K

K = {M s

K is:

k

(5)

LsMs

K

= ℓ(f 1(xs  ys) − f 2(xs  zs)) + λ

Ωk(M s

k ).

k=1

Similar to proximal gradient solution  after doing (sub-) gradient descent on the loss function
}K
in Eq. 5  proximal operator can be utilized to update base metrics {M s
k=1. The stochas-
tic strategy is guaranteed to converge theoretically. By denoting M∗
K) ∈
∗
arg min

∗
1   . . .   M
K) as the optimal solution  given totally S iterations  we have:

Ls(M s

1   . . .   M s

K = (M

∑

S
s=1

Theorem 1 Suppose in UM2L framework  the loss ℓ(·) is a convex one and selection operator κv
is in piecewise linear form. If each training instance ∥x∥2 ≤ 1  the sub-gradient set of Ωk(·) is
bounded by R  i.e.  ∥∂Ωk(Mk)∥2
≤ R2 and sub-gradient of loss ℓ(·) is bounded by C. When for
S∑
each base metric2 ∥Mk − M
∥F ≤ D  it holds that:3

∗
k

F

k

√

LsMs

K

− LsM(cid:3)

K

≤ 2GD + B

S

with G2 = max(C 2  R2) and B = ( D2

2 + 8G2). Given hinge loss  C 2 = 16.

s=1

3 Related Work and Discussions

Global DML approaches devote to ﬁnding a single metric for all instances [5  20] while local DML
approaches further take spatial data heterogeneities into consideration. Recently  different types of
local metric approaches are proposed  either assigning cluster-speciﬁc metric to instance based on
locality [20] or constructing local metrics generatively [13] or discriminatively [15  18]. Further-
more  instance speciﬁc metric learning methods [7  22] extend the locality properties of linkages to
extreme and gain improved classiﬁcation performance. However  these DML methods  either global
or local  take univocal semantic from label  namely  the side information.
Richness of semantics is noticed and exploited by machine learning researchers [3  11]. In DML
community  PSD [9] and SCA [4] are proposed. PSD works as collective classiﬁcation which is
less related to UM2L. SCA  a multi-metric learning method based on pairwise constraints  focuses
on learning metrics under one speciﬁc type of ambiguities  i.e.  linkages are with competitive se-
mantic meanings. UM2L is a more general multi-metric learning framework which considers triplet
constraints and various kinds of ambiguous linkages from both localities and semantic views.
UM2L maintains good compatibilities and can degenerate to several state-of-the-art DML methods.
For example  by considering univocal semantic (K = 1)  we can get a global metric learning model
used in [14]. If we further choose the hinge loss and set the regularizer Ω(M ) = tr(M B) with B
an intra-class similar pair covariance matrix  UM2L degrades to LMNN [20]. With trace norm on
M  [10] is recovered. For multi-metric approaches  if we set κv as the indicator of classes for the
second instance in a similar or dissimilar pair  UM2L can be transformed to MMLMNN [20].

4 Experiments on Different Types of Applications

Due to different choices of κs in UM2L  we test the framework in diverse real applications  namely
social linkages/feature pattern discovering  classiﬁcation  physical semantic meaning distinguishing
and visualization on multi-view semantic detection. To simplify the discussion  we use alternative
batch solver  smooth hinge loss and set regularizer Ωk(Mk) = ∥Mk∥2;1 if without further statement.
Triplets are constructed with 3 targets and 10 impostors with Euclidean nearest neighbors.

2This condition generally holds according to the norm regularizer in the objective function.
3Detailed proof can be found in the supplementary material.

5

4.1 Comparisons on Social Linkage/Feature Pattern Discovering

ADS conﬁguration is designed for social linkage and pattern discovering. To validate the effective-
ness of UM2LADS   we test it on social network data and synthetic data to show its grouping ability
on linkages and features  respectively.
Social linkages come from 6 real world Facebook network datasets from [11]. Given friendship
circles of an ego user and users’ binary features  the goal of ego-user linkages discovering is to utilize
the overall linkage and ﬁgure out how users are grouped. We form instances by taking absolute value
of differences between features of ego and the others. After circles with < 5 nodes are removed 
K is conﬁgured as the number of circles remained. Pairwise distance is computed by each metric
in MK  and a threshold is tuned on the training set to ﬁlter out irrelevant users. Thus  users with
different common hobbies are grouped together. MAC detects group assignments based on binary
features [8]; SCA constructs user linkages in a probabilistic way  and EGO [11] can directly output
user circles. KMeans (KM) and Spectral Clustering (SC) directly group users based on their features
without using linkages. Performance is measured by Balanced Error Rate (BER) [11]  the lower the
better. Results are listed in Table 1  which shows UM2LADS performs the best on most datasets.

Table 1: BER of the linkage discovering compar-
isons on Facebook datasets: UM2LADS vs. others

Table 2: BER of feature pattern discovery compar-
isons on synthetic datasets: UM2LADS vs. others

BER↓
Facebook_348
Facebook_414
Facebook_686
Facebook_698
Facebook_1684
Facebook_3980

KM SP MAC

.669
.721
.637
.661
.807
.708

.669
.721
.637
.661
.807
.708

.730
.699
.681
.640
.767
.541

SCA

.847
.870
.772
.729
.844
.667

EGO UM2L
.405
.420
.391
.420
.465
.402

.426
.449
.446
.392
.491
.538

BER↓
syn1
syn2
ad
ccd
my_movie
reuters

KM SP

.382
.564
.670
.244
.370
.704

.382
.564
.670
.244
.370
.704

SCA EGO UM2L
.355
.323
.381
.071
.155
.398

.392
.399
.400
.250
.249
.400

.467
.428
.583
.225
.347
.609

Similarly  we test feature pattern discovering ability of UM2LADS on 4 transformed multi-view
datasets. For each dataset  we ﬁrst extract principal components of each view  and construct sub-
linkage candidates between instances with random thresholds on each single view. Thus  these
candidates are various among different views. After that  the overall linkage is further generated
from these candidates using “or” operation. With features on each view and the overall linkage  the
goal of feature pattern discovering is to reveal responsible features for each sub-linkage. Zero-value
rows/columns of learned metrics indicate irrelevant features in the corresponding group. Syn1 and
syn2 are purely synthetic datasets with features sampled from Uniform  Beta  Binomial  Gamma and
Normal distributions using different parameters. BER results are listed in Table 2 and UM2LADS
achieves the best on all datasets. These assessments indicate UM2LADS can ﬁgure out reasonable
linkages or patterns hidden behind observations  and even better than domain speciﬁc methods.

4.2 Comparisons on Classiﬁcation Performance

To test classiﬁcation generalization performance  our framework is compared with 8 state-of-the-art
metric learning methods on 10 benchmark datasets and 8 large scale datasets (results of 8 large scale
data are in the supplementary material). In detail  global DML methods: ITML [5]  LMNN [20]
and EIG [21]; local and instance speciﬁc DML methods: PLML [18]  SCML (local version) [15];
MMLMNN [20]  ISD [22] and SCA [4].
In UM2L  distance values from different metrics are comparable. Therefore in the test phase  we ﬁrst
compute 3 nearest neighbors for testing instance ˜x using each base metric Mk. Then 3× K distance
values are collected adaptively and the smallest 3 ones (3 instances with the highest similarity scores)
form neighbor candidates. Majority voting over them is used for prediction.
Evaluations on classiﬁcation are repeated for 30 times. In each trial  70% of instances are used
for training  and the remaining part is for test. Cross-validation is employed for parameters tun-
ing. Generalization errors (mean±std.) based on 3NN are listed in Table 3 where Euclidean dis-
tance results (EUCLID) are also listed as a baseline. Considering the abilities of multi-semantic
description of ADS and the rigorous restrictions of RGS  UM2LADS=RGS are implemented in this
comparison. Number of metrics K is conﬁgured as the number of classes. Table 3 clearly shows
that UM2LADS=RGS perform well on most datasets. Especially  UM2LRGS achieves best on more
datasets according to t-tests and this can be attributed to the rigorous restrictions of RGS.

6

Table 3: Comparisons of classiﬁcation performance (test errors  mean ± std.) based on 3NN. UM2LADS and
UM2LRGS are compared. The best performance on each dataset is in bold. Last two rows list the Win/Tie/Lose
counts of UM2LADS=RGS against other methods on all datasets with t-test at signiﬁcance level 95%.

UM2LADS UM2LRGS
Autompg .201±.034 .225±.031
.070±.018 .086±.020
Clean1
.281±.019 .284±.030
German
.312±.043 .293±.047
Glass
.276±.044 .307±.068
Hayes-r
.190±.035 .194±.063
Heart-s
.051±.015 .048±.013
House-v
.363±.045 .342±.047
Liver-d
.023±.038 .029±.034
Segment
.136±.032 .132±.036
Sonar
UM2LADS vs. others
UM2LRGS vs. others

W / T / L
W / T / L

PLML

ISD

SCA

ITML

LMNN

EIG

SCML MMLMNN

EUCLID
.265±.048 .253±.026 .256±.032 .288±.033 .286±.037 .292±.032 .259±.037 .266±.031 .260±.036
.098±.027 .100±.027 .097±.022 .143±.023 .306±.072 .141±.024 .084±.021 .127±.021 .139±.023
.280±.016 .302±.021 .289±.019 .297±.017 .292±.023 .288±.021 .292±.021 .284±.014 .296±.021
.389±.050 .328±.054 .296±.047 .334±.050 .529±.053 .311±.038 .315±.049 .314±.050 .307±.042
.436±.201 .296±.053 .282±.062 .378±.093 .379±.068 .342±.080 .314±.072 .289±.067 .398±.046
.365±.127 .205±.040 .191±.037 .192±.036 .203±.039 .186±.032 .200±.026 .189±.034 .190±.030
.121±.240 .066±.019 .055±.017 .072±.024 .174±.075 .063±.023 .061±.017 .080±.024 .083±.025
.361±.055 .371±.042 .372±.045 .364±.042 .408±.011 .377±.052 .373±.045 .380±.037 .384±.040
.041±.031 .041±.008 .036±.006 .063±.009 .324±.043 .050±.012 .039±.006 .059±.016 .050±.007
.171±.048 .193±.045 .157±.038 .182±.038 .220±.040 .174±.039 .145±.032 .159±.042 .168±.036
8 / 2 / 0
6 / 4 / 0
6 / 4 / 0
8 / 2 / 0

8 / 2 / 0
8 / 2 / 0

6 / 4 / 0
8 / 2 / 0

5 / 5 / 0
8 / 2 / 0

6 / 4 / 0
7 / 3 / 0

7 / 3 / 0
8 / 2 / 0

4 / 6 / 0
5 / 5 / 0

7 / 3 / 0
9 / 1 / 0

(a) LMNN

(b) PLML 1

(c) PLML 2

(d) MMLMNN 1

(e) MMLMNN 2

(f) MMLMNN 3

(g) UM2L 1

(h) UM2L 2

(i) UM2L 3

(j) UM2L 4

(k) UM2L 5

(l) UM2L 6

Figure 1: Word clouds generated from the results of compared DML methods. The size of word depends on the
⊤
importance weight of each word (feature). The weight is calculated by decomposing each metric Mk = LkL
k  
and calculate the ℓ2-norm of each row in Lk  where each row corresponds to a speciﬁc word. Each subplot
gives a word cloud for a base metric learned from DML approaches.

4.3 Comparisons of Latent Semantic Discovering

UM2L is proposed for DML with both localities and semantic linkages considered. Hence  to inves-
tigate the ability of latent semantics discovering  two assessments in real applications are performed 
i.e.  Academic Paper Linkages Explanation (APLE) and Image Weak Label Discovering (IWLD).
In APLE  data are collected from 2012-2015 ICML papers  which can be connected with each other
by more than one topic  yet only the session ID is captured to form explicit linkages. 3 main di-
rections of sessions are picked up in this assessment  i.e.  “feature learning”  “online learning” and
“deep learning”. No sub-ﬁelds and additional labels/topics are provided. Simplest TF-IDF is used
to extract features  which forms a corpus of 220 papers and 1622 words in total. Aiming at ﬁnding
the hidden linkages together with their causes  both UM2LADS and UM2LOVS are invoked. To avoid
trivial solutions  regularizer for each metric is conﬁgured as Ωk(Mk) = ∥Mk − I∥2
F for UM2LOVS.
All feature (word) weights and correlations can be provided by learned metrics  i.e.  with decompo-
⊤
sition Mk = LkL
k   the ℓ2-norm value of each row in Lk can be regarded as the weight for each
feature (word). The importance of feature (word) weights is demonstrated in word clouds in Fig. 1 
where the size of fonts reﬂects the weights of each word. Due to the page limits  supplementary
materials represent full evaluations.
Fig. 1 shows the results of LMNN [20] (a)  PLML [18] (b  c)  MMLMNN [20] (d  e  f) and UM2LOVS
(g ∼ l) with K = 6  respectively. Global method LMNN returns one subplot. The metric learned by
LMNN perhaps has discriminative ability but the weights of words cannot distinguish subﬁelds in 3
selective domains. For multi-metric learning approaches PLML and MMLMNN  though they can pro-
vide more than one base metric and consequently have multiple word clouds  the words presented in
subplots are not with legible physical semantic meanings. Especially  PLML outputs multiple met-
rics which are similar to each other (tends to global learner’s behavior) and only focus on ﬁrst part
of the alphabet  while MMLMNN by default only learns multiple metrics with the number of base
metrics equaling to the number of classes. However  results of UM2LOVS clearly demonstrate all 3
ﬁelds. On session “online learning”  it can discover different sub-ﬁelds such as “online convex opti-

7

(b) (mountains  sea)

(a) ADS subspace 1 (b) ADS subspace 2 (c) RGS subspace

(a) (sea  mountains)

(c) (sea  sunset)
Figure 2: Results of visual semantic discovery on im-
ages. The ﬁrst annotation in the bracket is the provided
weak label. The second one is one of the latent semantic
labels discovered by UM2L.

Figure 3: Subspaces discovered by UM2LADS (a b)
and UM2LRGS (c).
Instances possess 2 semantic
properties  i.e.  color and shape. Blue dot-lines give
the decision boundary.

mization” (g and h)  and “online (multi-) armed bandit problem” (j); for session “feature learning” 
it has “feature score” (i) and “PCA projection” (l); and for “deep learning”  the word cloud returns
popular words like “network layer”  “autoencoder” and “layer”(k).
Besides APLE  the second application is about weak label discovering in images from [23]  where
the most obvious label for each image is used for triplets constraints generation. UM2LOVS can
obtain multiple metrics  each of which is with a certain visual semantic. By computing similarities
based on different metrics  latent semantics can be discovered  i.e.  if we assume images connected
with high similarities share the same label  missing labels can be completed as in Fig. 2. More weak
label results can be found in the supplementary material.

4.4

Investigations of Latent Multi-View Detection

Another direct application of UM2L is hidden multi-view detection  where data can be described by
multiple views from different channels yet feature partitions are not clearly provided [16]. Data with
multi-view goes consistent with the assumption of ADS or RGS conﬁguration. ADS emphasizes the
existence of relevant views and aims at decomposing helpful aspects or views; while RGS requires
full accordance among views. Trace norm regularizes the approach in this part to get low dimen-
sional projection. UM2L framework facilitates the understanding of data by decomposing each base
metric to low dimensional subspace  i.e.  for each base metric Mk  2 eigen-vectors Lk ∈ Rd×2
corresponding to the largest 2 eigen-values are picked as orthogonal bases.
The hidden multi-view data [1] are composed of 200 instances and each instance has two hidden
views  namely color and shape. We perform UM2LADS=RGS on this dataset with K = 2. Results of
other methods such as SCA can be found in the supplementary material. Fig. 3 (a) (b) give the 2-D
visualization results by plotting the projected instances in subspaces corresponding to metric M1
and M2 of UM2LADS. It clearly shows that M1 captures the semantic view of color  and M2 reﬂects
the meaning of shape. While for UM2LRGS  the visualization result of one of the obtained metrics
is showed in Fig. 3 (c). It can be clearly found that both UM2LADS and UM2LRGS can capture the
two different semantic views hidden in data. Moreover  since UM2LRGS requires more accordance 
it can capture these physical meanings with a single metric.

5 Conclusion

In this paper  we propose the Uniﬁed Multi-Metric Learning (UM2L) framework which can exploit
side information from multiple aspects such as locality and semantics linkage constraints. It is no-
table that both types of constraints can be absorbed in the multi-metric loss functions with a type of
ﬂexible function operator κ in UM2L. By implementing κ in different forms  UM2L can be used for
local metric learning in classiﬁcation  latent semantic linkage discovering  etc.  or degrade to state-
of-the-art DML approaches. The regularizer in UM2L is ﬂexible for different purposes. UM2L can be
solved by various optimization techniques such as proximal gradient and accelerated stochastic ap-
proaches  and theoretical guarantee on the convergence is proved. Experiments show the superiority
of UM2L in classiﬁcation performance and hidden semantics discovery. Automatic determination of
the number of base metrics is an interesting future work.
Acknowledgements This research was supported by NSFC (61273301  61333014)  Collaborative
Innovation Center of Novel Software Technology and Industrialization  and Tencent Fund.

8

References
[1] E. Amid and A. Ukkonen. Multiview triplet embedding: Learning attributes in multiple maps. In ICML 

pages 1472–1480  Lille  France  2015.

[2] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.

SIIMS  2(1):183–202  2009.

[3] D. Chakrabarti  S. Funiak  J. Chang  and S. Macskassy. Joint inference of multiple label types in large

networks. In ICML  pages 874–882  Beijing  China  2014.

[4] S. Changpinyo  K. Liu  and F. Sha. Similarity component analysis. In NIPS  pages 1511–1519. MIT Press 

Cambridge  MA.  2013.

[5] J. V. Davis  B. Kulis  P. Jain  S. Sra  and I. S. Dhillon. Information-theoretic metric learning. In ICML 

pages 209–216  Corvalis  OR.  2007.

[6] J. C. Duchi and Y. Singer. Efﬁcient online and batch learning using forward backward splitting. JMLR 

10:2899–2934  2009.

[7] E. Fetaya and S. Ullman. Learning local invariant mahalanobis distances. In ICML  pages 162–168  Pairs 

France  2015.

[8] M. Frank  A. P. Streich  D. Basin  and J. M. Buhmann. Multi-assignment clustering for boolean data.

JMLR  13:459–489  2012.

[9] J.-H. Hu  D.-C. Zhan  X. Wu  Y. Jiang  and Z.-H. Zhou. Pairwised speciﬁc distance learning from physical

linkages. TKDD  9(3):Article 20  2015.

[10] K. Huang  Y. Ying  and C. Campbell. GSML: A uniﬁed framework for sparse metric learning. In ICDM 

pages 189–198  Miami  FL.  2009.

[11] J. Leskovec and J. Mcauley. Learning to discover social circles in ego networks. In NIPS  pages 539–547.

MIT Press  Cambridge  MA.  2012.

[12] D. Lim  G. Lanckriet  and B. McFee. Robust structural metric learning. In ICML  pages 615–623  Atlanta 

GA.  2013.

[13] Y.-K. Noh  B.-T. Zhang  and D. Lee. Generative local metric learning for nearest neighbor classiﬁcation.

In NIPS  pages 1822–1830. MIT Press  Cambridge  MA.  2010.

[14] Q. Qian  R. Jin  S. Zhu  and Y. Lin. Fine-grained visual categorization via multi-stage metric learning. In

CVPR  pages 3716–3724  Boston  MA.  2015.

[15] Y. Shi  A. Bellet  and F. Sha. Sparse compositional metric learning. In AAAI  pages 2078–2084  Quebec 

Canada  2014.

[16] W. Wang and Z.-H. Zhou. A new analysis of co-training. In ICML  pages 1135–1142  Haifa  Israel  2010.

[17] B. Wang  J. Jiang  W. Wang  Z.-H. Zhou  and Z. Tu. Unsupervised metric fusion by cross diffusion. In

CVPR  pages 2997–3004  Providence  RI.  2012.

[18] J. Wang  A. Kalousis  and A. Woznica. Parametric local metric learning for nearest neighbor classiﬁcation.

In NIPS  pages 1601–1609. MIT Press  Cambridge  MA.  2012.

[19] J. Wang  A. Woznica  and A. Kalousis. Learning neighborhoods for metric learning. In ECML/PKDD 

pages 223–236  Bristol  UK  2012.

[20] K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation.

JMLR  10:207–244  2009.

[21] Y. Ying and P. Li. Distance metric learning with eigenvalue optimization. JMLR  13:1–26  2012.

[22] D.-C. Zhan  M. Li  Y.-F. Li  and Z.-H. Zhou. Learning instance speciﬁc distances using metric propagation.

In ICML  pages 1225–1232  Montreal  Canada  2009.

[23] M.-L. Zhang and Z.-H. Zhou. ML-KNN: A lazy learning approach to multi-label learning. Pattern

Recognition  40(7):2038–2048  2007.

[24] Z.-H. Zhou. Ensemble methods: foundations and algorithms. Chapman & Hall/CRC  Boca Raton  FL. 

2012.

[25] Z.-H. Zhou. Learnware: On the future of machine learning. Frontiers of Computer Science  10(4):589–

590  2016.

9

,Gustavo Malkomes
Matt Kusner
Wenlin Chen
Kilian Weinberger
Benjamin Moseley
Han-Jia Ye
De-Chuan Zhan
Xue-Min Si
Yuan Jiang
Zhi-Hua Zhou
Ehsan Elhamifar
M. Clara De Paolis Kaluza