2019,Optimal Sketching for Kronecker Product Regression and Low Rank Approximation,We study the Kronecker product regression problem  in which the design matrix is a Kronecker product of two or more matrices. Formally  given $A_i \in \R^{n_i \times d_i}$ for $i=1 2 \dots q$  where $n_i \gg d_i$ for each $i$  and $b \in \R^{n_1 n_2 \cdots n_q}$  let $\mathcal{A} = A_i \otimes A_2 \otimes \cdots \otimes A_q$. Then for $p \in [1 2]$  the goal is to find $x \in \R^{d_1 \cdots d_q}$ that approximately minimizes $\|\mathcal{A}x - b\|_p$. Recently  Diao  Song  Sun  and Woodruff (AISTATS  2018) gave an algorithm which is faster than forming the Kronecker product $\mathcal{A} \in \R^{n_1 \cdots n_q \times d_1 \cdots d_q}$. Specifically  for $p=2$ they achieve a running time of $O(\sum_{i=1}^q   \texttt{nnz}(A_i) + \texttt{nnz}(b))$  where $ \texttt{nnz}(A_i)$ is the number of non-zero entries in $A_i$. Note that $\texttt{nnz}(b)$ can be as large as $\Theta(n_1 \cdots n_q)$. For $p=1 $ $q=2$ and $n_1 = n_2$  they achieve a worse bound of $O(n_1^{3/2} \text{poly}(d_1d_2) + \texttt{nnz}(b))$. In this work  we provide significantly faster algorithms. For $p=2$  our running time is $O(\sum_{i=1}^q   \texttt{nnz}(A_i) )$  which has no dependence on $\texttt{nnz}(b)$.  For $p<2$  our running time is $O(\sum_{i=1}^q   \texttt{nnz}(A_i) + \texttt{nnz}(b))$  which matches the prior best running time for $p=2$.  We also consider the related all-pairs regression problem  where given $A \in \R^{n \times d}  b \in \R^n$   we want to solve $\min_{x \in \R^d} \|\bar{A}x - \bar{b}\|_p$  where $\bar{A} \in \R^{n^2 \times d}  \bar{b} \in \R^{n^2}$ consist of all pairwise differences of the rows of $A b$. We give an $O(\texttt{nnz}(A))$ time algorithm for $p \in[1 2]$  improving the $\Omega(n^2)$ time required to form $\bar{A}$. Finally  we initiate the study of Kronecker product low rank and and low-trank approximation. For input $\mathcal{A}$ as above  we give $O(\sum_{i=1}^q  \texttt{nnz}(A_i))$ time algorithms  which is much faster than computing $\mathcal{A}$.,Optimal Sketching for Kronecker Product Regression

and Low Rank Approximation

Huaian Diao∗ Rajesh Jayaram† Zhao Song‡ Wen Sun§ David P. Woodruff¶

Abstract

of O((cid:80)q
time is O((cid:80)q
our running time is O((cid:80)q

We study the Kronecker product regression problem  in which the design matrix
is a Kronecker product of two or more matrices. Formally  given Ai ∈ Rni×di
for i = 1  2  . . .   q where ni (cid:29) di for each i  and b ∈ Rn1n2···nq  let A =
A1 ⊗ A2 ⊗ ··· ⊗ Aq. Then for p ∈ [1  2]  the goal is to ﬁnd x ∈ Rd1···dq that
approximately minimizes (cid:107)Ax − b(cid:107)p. Recently  Diao  Song  Sun  and Woodruff
(AISTATS  2018) gave an algorithm which is faster than forming the Kronecker
product A ∈ Rn1···nq×d1···dq. Speciﬁcally  for p = 2 they achieve a running time
i=1 nnz(Ai) + nnz(b))  where nnz(Ai) is the number of non-zero entries
in Ai. Note that nnz(b) can be as large as Θ(n1 ··· nq). For p = 1  q = 2 and
n1 = n2  they achieve a worse bound of O(n3/2
In this work  we provide signiﬁcantly faster algorithms. For p = 2  our running
i=1 nnz(Ai))  which has no dependence on nnz(b). For p < 2 
i=1 nnz(Ai) + nnz(b))  which matches the prior best
running time for p = 2. We also consider the related all-pairs regression problem 
where given A ∈ Rn×d  b ∈ Rn  we want to solve minx∈Rd (cid:107) ¯Ax − ¯b(cid:107)p  where
¯A ∈ Rn2×d  ¯b ∈ Rn2 consist of all pairwise differences of the rows of A  b.
We give an O(nnz(A)) time algorithm for p ∈ [1  2]  improving the Ω(n2) time
required to form ¯A. Finally  we initiate the study of Kronecker product low rank
i=1 nnz(Ai))
time algorithms  which is much faster than computing A.

and low t-rank approximation. For input A as above  we give O((cid:80)q

1 poly(d1d2) + nnz(b)).

1

Introduction

In the q-th order Kronecker product regression problem  one is given matrices A1  A2  . . .   Aq 
where Ai ∈ Rni×di  as well as a vector b ∈ Rn1n2···nq  and the goal is to obtain a solution to
the optimization problem:

min

x∈Rd1d2···dq

(cid:107)(A1 ⊗ A2 ··· ⊗ Aq)x − b(cid:107)p 

Statistics  Northeast Normal University  China

∗hadiao@nenu.edu.cn. Key Laboratory for Applied Statistics of MOE and School of Mathematics and
†rkjayara@cs.cmu.edu. Carnegie Mellon University. Rajesh Jayaram would like to thank support from
the Ofﬁce of Naval Research (ONR) grant N00014-18-1-2562. This work was partly done while Rajesh Ja-
yaram was visiting the Simons Institute for the Theory of Computing.
‡zhaosong@uw.edu. University of Washington. This work was partly done while Zhao Song was visiting
§sun.wen@microsoft.com. Microsoft Research New York.
¶dwoodruf@cs.cmu.edu. Carnegie Mellon University. David Woodruff would like to thank support from
the Ofﬁce of Naval Research (ONR) grant N00014-18-1-2562. This work was also partly done while David
Woodruff was visiting the Simons Institute for the Theory of Computing.

the Simons Institute for the Theory of Computing.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

where p ∈ [1  2]  and for a vector x ∈ Rn the (cid:96)p norm is deﬁned by (cid:107)x(cid:107)p = ((cid:80)n

i=1 |xi|p)1/p. For
p = 2  this is known as least squares regression  and for p = 1 this is known as least absolute
deviation regression.
Kronecker product regression is a special case of ordinary regression in which the design matrix
is highly structured. Namely  the design matrix is the Kronecker product of two or more smaller
matrices. Such Kronecker product matrices naturally arise in applications such as spline regression 
signal processing  and multivariate data ﬁtting. We refer the reader to [VL92  VLP93  GVL13] for
further background and applications of Kronecker product regression. As discussed in [DSSW18] 
Kronecker product regression also arises in structured blind deconvolution problems [OY05]  and
the bivariate problem of surface ﬁtting and multidimensional density smoothing [EM06].
A recent work of Diao  Song  Sun  and Woodruff [DSSW18] utilizes sketching techniques to output
an x ∈ Rd1d2···dq with objective function at most (1 + )-times larger than optimal  for both least
squares and least absolute deviation Kronecker product regression. Importantly  their time complex-
ity is faster than the time needed to explicitly compute the product A1⊗···⊗Aq. We note that sketch-
ing itself is a powerful tool for compressing extremely high dimensional data  and has been used in
a number of tensor related problems  e.g.  [SWZ16  LHW17  DSSW18  SWZ19b  AKK+20].

For least squares regression  the algorithm of [DSSW18] achieves O((cid:80)q

i=1 nnz(Ai) + nnz(b) +
poly(d/)) time  where nnz(C) for a matrix C denotes the number of non-zero entries of C. Note
that the focus is on the over-constrained regression setting  when ni (cid:29) di for each i  and so the goal
is to have a small running time dependence on the ni’s. We remark that over-constrained regression
has been the focus of a large body of work over the past decade  which primarily attempts to design
fast regression algorithms in the big data (large sample size) regime  see  e.g.  [Mah11  Woo14] for
surveys.

Observe that explicitly forming the matrix A1 ⊗ ··· ⊗ Aq would take(cid:81)q
can be as large as(cid:81)q
Unfortunately  since b ∈ Rn1n2···nq  we can have nnz(b) =(cid:81)q
to solve this problem in time sub-linear in nnz(b)  with a dominant term of O((cid:80)q

i=1 nnz(Ai) time  which
i=1 nidi  and so the results of [DSSW18] offer a large computational advantage.
i=1 ni  and therefore nnz(b) is likely
to be the dominant term in the running time. This leaves open the question of whether it is possible

i=1 nnz(Ai)).

For least absolute deviation regression  the bounds of [DSSW18] achieved are still an improvement
over computing A1 ⊗ ··· ⊗ Aq  though worse than the bounds for least squares regression. The
authors focus on q = 2 and the special case n = n1 = n2. Here  they obtain a running time
of O(n3/2 poly(d1d2/) + nnz(b))6. This leaves open the question of whether an input-sparsity
O(nnz(A1) + nnz(A2) + nnz(b) + poly(d1d2/)) time algorithm exists.

All-Pairs Regression In this work  we also study the related all-pairs regression problem. Given
A ∈ Rn×d  b ∈ Rn  the goal is to approximately solve the (cid:96)p regression problem minx (cid:107) ¯Ax − ¯b(cid:107)p 
where ¯A ∈ Rn2×d is the matrix formed by taking all pairwise differences of the rows of A (and ¯b
is deﬁned similarly). For p = 1  this is known as the rank regression estimator  which has a long
history in statistics. It is closely related to the renowned Wilconxon rank test [WL09]  and enjoys
the desirable property of being robust with substantial efﬁciency gain with respect to heavy-tailed
random errors  while maintaining high efﬁciency for Gaussian errors [WKL09  WL09  WPB+18 
Wan19a]. In many ways  it has properties more desirable in practice than that of the Huber M-
estimator [WPB+18  Wan19b]. Recently  the all-pairs loss function was also used by [WPB+18]
as an alternative approach to overcoming the challenges of tuning parameter selection for the Lasso
algorithm. However  the rank regression estimator is computationally intensive to compute  even
for moderately sized data  since the standard procedure (for p = 1) is to solve a linear program
with O(n2) constraints. In this work  we demonstrate the ﬁrst highly efﬁcient algorithm for this
estimator.

Low-Rank Approximation Finally  in addition to regression  we extend our techniques to the
Low Rank Approximation (LRA) problem. Here  given a large data matrix A  the goal is to
6We remark that while the nnz(b) term is not written in the Theorem of [DSSW18]  their approach of
leverage score sampling from a well-conditioned basis requires one to sample from a well conditioned basis of
[A1 ⊗ A2  b] for a subspace embedding. As stated  their algorithm only sampled from [A1 ⊗ A2]. To ﬁx this
omission  their algorithm would require an additional nnz(b) time to leverage score sample from the augmented
matrix.

2

ﬁnd a low rank matrix B which well-approximates A. LRA is useful in numerous applica-
tions  such as compressing massive datasets to their primary components for storage  denois-
ing  and fast matrix-vector products. Thus  designing fast algorithms for approximate LRA has
become a large and highly active area of research; see [Woo14] for a survey. For an incom-
plete list of recent work using sketching techniques for LRA  see [CW13  MM13  NN13  BW14 
CW15b  CW15a  RSW16  BWZ16  SWZ17  MW17  CGK+17  LHW17  SWZ18  BW18  SWZ19a 
SWZ19b  SWZ19c  BBB+19  IVWW19] and the references therein.
Motivated by the importance of LRA  we initiate the study of low-rank approximation of Kronecker
product matrices. Given q matrices A1 ···   Aq where Ai ∈ Rni×di  ni (cid:29) di  A = ⊗q
i=1Ai 
the goal is to output a rank-k matrix B ∈ Rn×d such that (cid:107)B − A(cid:107)2
F ≤ (1 + ) OPTk 
where OPTk is the cost of the best rank-k approximation  n = n1 ··· nq  and d = d1 ··· dq.
Here (cid:107)A(cid:107)2
i j. The fastest general purpose algorithms for this problem run in time
O(nnz(A) + poly(dk/)) [CW13]. However  as in regression  if A = ⊗q
i=1Ai  we have nnz(A) =
i=1 nnz(Ai)  which grows very quickly. Instead  one might also hope to obtain a running time of

F = (cid:80)

i j A2

(cid:81)q
O((cid:80)q

i=1 nnz(Ai) + poly(dk/)).

1.1 Our Contributions

Our main contribution is an input sparsity time (1 + )-approximation algorithm to Kronecker
product regression for every p ∈ [1  2]  and q ≥ 2. Given Ai ∈ Rni×di  i = 1  . . .   q 
i=1 ni  together with accuracy parameter  ∈ (0  1/2) and fail-
i=1 di such that
|(A1 ⊗ ··· ⊗ Aq)x(cid:48) − b(cid:107)p ≤ (1 + ) minx (cid:107)(A1 ⊗ ··· ⊗ Aq)x − b(cid:107)p holds with probability at

and b ∈ Rn where n = (cid:81)q
ure probability δ > 0  the goal is to output a vector x(cid:48) ∈ Rd where d = (cid:81)q
least 1 − δ. For p = 2  our algorithm runs in (cid:101)O(cid:0)(cid:80)q
time is (cid:101)O (((cid:80)q

i=1 nnz(Ai)) + poly(dδ−1/)(cid:1) time.7 Notice

that this is sub-linear in the input size  since it does not depend on nnz(b). For p < 2  the running

i=1 nnz(Ai) + nnz(b) + poly(d/)) log(1/δ)).

Observe that in both cases  this running time is signiﬁcantly faster than the time to write down
A1 ⊗ ··· ⊗ Aq. For p = 2  up to logarithmic factors  the running time is the same as the time
required to simply read each of the Ai. Moreover  in the setting p < 2  q = 2 and n1 = n2
considered in [DSSW18]  our algorithm offers a substantial improvement over their running time
of O(n3/2 poly(d1d2/)). We empirically evaluate our Kronecker product regression algorithm on
exactly the same datasets as those used in [DSSW18]. For p ∈ {1  2}  the accuracy of our algorithm
is nearly the same as that of [DSSW18]  while the running time is signiﬁcantly faster.
For the all-pairs (or rank) regression problem  we ﬁrst note that for A ∈ Rn×d  one can rewrite
¯A ∈ Rn2×d as the difference of Kronecker products ¯A = A ⊗ 1n − 1n ⊗ A where 1n ∈ Rn is
the all ones vector. Since ¯A is not a Kronecker product itself  our earlier techniques for Kronecker
product regression are not directly applicable. Therefore  we utilize new ideas  in addition to careful

sketching techniques  to obtain an (cid:101)O(nnz(A) + poly(d/)) time algorithm for p ∈ [1  2]  which

p  . . .  |yn2|p/(cid:107)y(cid:107)p

p) in (cid:101)O(nd + poly(ds)) time. For the (cid:96)p

improves substantially on the O(n2d) time required to even compute ¯A  by a factor of at least n.
Our main technical contribution for both our (cid:96)p regression algorithm and the rank regression problem
is a novel and highly efﬁcient (cid:96)p sampling algorithm. Speciﬁcally  for the rank-regression problem
we demonstrate  for a given x ∈ Rd  how to independently sample s entries of a vector ¯Ax = y ∈
Rn2 from the (cid:96)p distribution (|y1|p/(cid:107)y(cid:107)p
and in time (cid:101)O((cid:80)q
regression problem  we demonstrate the same result when y = (A1 ⊗ ··· ⊗ Aq)x − b ∈ Rn1···nq 
i=1 nnz(Ai) + nnz(b) + poly(ds)). This result allows us to sample a small number
of rows of the input to use in our sketch. Our algorithm draws from a large number of disparate
sketching techniques  such as the dyadic trick for quickly ﬁnding heavy hitters [CM05  KNPW11 
O((cid:80)q
LNNT16  NS19]  and the precision sampling framework from the streaming literature [AKO11].
For the Kronecker Product Low-Rank Approximation (LRA) problem  we give an input sparsity
(cid:80)q
i=1 nnz(Ai) is substantially smaller than the nnz(A) = (cid:81)q
i=1 nnz(Ai) + poly(dk/))-time algorithm which computes a rank-k matrix B such that
(cid:107)B − ⊗q
F ≤ (1 + ) minrank −k B(cid:48) (cid:107)B(cid:48) − ⊗q
F . Note again that the dominant term
i=1 nnz(Ai) time required to write
7For a function f (n  d    δ)  (cid:101)O(f ) = O(f · poly(log n))

i=1Ai(cid:107)2

i=1Ai(cid:107)2

3

i1

i2

i3

(cid:80)

(cid:80)

(cid:107)A(cid:107)p = ((cid:80)

Using similar sketching ideas  we provide an O((cid:80)q

down the Kronecker Product A  which is also the running time of state-of-the-art general purpose
LRA algorithms [CW13  MM13  NN13]. Thus  our results demonstrate that substantially faster
algorithms for approximate LRA are possible for inputs with a Kronecker product structure.
Finally  motivated by [VL00]  we use our techniques to solve the low-trank approximation problem 
where we are given an arbitrary matrix A ∈ Rnq×nq  and the goal is to output a trank-k matrix
B ∈ Rnq×nq such that (cid:107)B−A(cid:107)F is minimized. Here  the trank of a matrix B is the smallest integer
k such that B can be written as a summation of k matrices  where each matrix is the Kronecker
product of q matrices with dimensions n×n. Compressing a matrix A to a low-trank approximation
yields many of the same beneﬁts as LRA  such as compact representation  fast matrix-vector product 
and fast matrix multiplication  and thus is applicable in many of the settings where LRA is used.
i=1 nnz(Ai) + poly(d1 ··· dq/)) time algorithm
for this problem under various loss functions. Our results for low-trank approximation can be found
in the full version of this work.
2 Preliminaries
Notation For a tensor A ∈ Rn1×n2×n3  we use (cid:107)A(cid:107)p to denote the entry-wise (cid:96)p norm of A  i.e. 
|Ai1 i2 i3|p)1/p. For n ∈ N  let [n] = {1  2  . . .   n}. For a matrix A  let
Ai ∗ denote the i-th row of A  and A∗ j the j-th column. For a  b ∈ R and  ∈ (0  1)  we write
a = (1 ± )b to denote (1 − )b ≤ a ≤ (1 + )b. We now deﬁne various sketching matrices used by
our algorithms.
Stable Transformations We will utilize the well-known p-stable distribution  Dp (see [Nol07 
Ind06] for further discussion)  which exist for p ∈ (0  2]. For p ∈ (0  2)  X ∼ Dp is deﬁned by
√−1tX)] = exp(−|t|p)  and can be efﬁciently generated to a
its characteristic function EX [exp(
ﬁxed precision [Nol07  KNW10]. For p = 2  D2 is just the standard Gaussian distribution  and for
i=1 ziai ∼ z(cid:107)a(cid:107)p where (cid:107)a(cid:107)p = ((cid:80)n
p = 1  D1 is the Cauchy distribution. The distribution Dp has the property that if z1  . . .   zn ∼ Dp
i=1 |ai|p)1/p  and z ∼ Dp. This
property will allow us to utilize sketches with entries independently drawn from Dp to preserve the
(cid:96)p norm.
Deﬁnition 2.1 (Dense p-stable Transform  [CDMI+13  SW11]). Let p ∈ [1  2]. Let S = σ · C ∈
Rm×n  where σ is a scalar  and each entry of C ∈ Rm×n is chosen independently from Dp.
We will also need a sparse version of the above.
Deﬁnition 2.2 (Sparse p-Stable Transform  [MM13  CDMI+13]). Let p ∈ [1  2]. Let Π = σ · SC ∈
Rm×n  where σ is a scalar  S ∈ Rm×n has each column chosen independently and uniformly from
the m standard basis vectors of Rm  and C ∈ Rn×n is a diagonal matrix with diagonals chosen
independently from the standard p-stable distribution. For any matrix A ∈ Rn×d  ΠA can be
computed in O(nnz(A)) time.

are i.i.d.  and a ∈ Rn  then(cid:80)n

One nice property of p-stable transformations is that they provide low-distortion (cid:96)p embeddings.
Lemma 2.3 (Theorem 1.4 of [WW19]; see also Theorem 2 and 4 of [MM13] for earlier work 8 ).
Fix A ∈ Rn×d  and let S ∈ Rk×n be a sparse or dense p-stable transform for p ∈ [1  2)  with
k = Θ(d2/δ). Then with probability 1 − δ  for all x ∈ Rd:

(cid:107)Ax(cid:107)p ≤ (cid:107)SAx(cid:107)p ≤ O(d log d)(cid:107)Ax(cid:107)p

We simply call a matrix S ∈ Rk×n a low distortion (cid:96)p embedding for A ∈ Rn×d if it satisﬁes the
above inequality for all x ∈ Rd.
Leverage Scores & Well Condition Bases. We now introduce the notions of (cid:96)2 leverage scores
and well-conditioned bases for a matrix A ∈ Rn×d.
Deﬁnition 2.4 ((cid:96)2-Leverage Scores  [Woo14  BSS12]). Given a matrix A ∈ Rn×d  let A = Q · R
denote the QR factorization of matrix A. For each i ∈ [n]  we deﬁne σi =
  where

(cid:107)(AR−1)i(cid:107)2
(cid:107)AR−1(cid:107)2

2

F

8In discussion with the authors of these works  the original O((d log d)1/p) distortion factors stated in these
papers should be replaced with O(d log d); as we do not optimize the poly(d) factors in our analysis  this does
not affect our bounds.

4

i=1 ni.

for i = 1  . . .   q do

i=1 di  n ←(cid:81)q

d ←(cid:81)q
Compute approximate leverage scores(cid:101)σi(Aj) for all j ∈ [q]  i ∈ [nj].
i=1 ni  m ← Θ(d/(δ2)).
(cid:98)x = arg minx∈Rd (cid:107)D(A1 ⊗ A2 ⊗ ··· ⊗ Aq)x − Db(cid:107)2
return(cid:98)x
d ←(cid:81)q

Algorithm 1 Our (cid:96)2 Kronecker Product Regression Algorithm
1: procedure (cid:96)2 KRONECKER REGRESSION(({Ai  ni  di}i∈[q]  b))
2:
3:
4:
5:
6:
7:
8: end procedure
Algorithm 2 Our (cid:96)p Kronecker Product Regression Algorithm  1 ≤ p < 2
1: procedure O(1)-APPROXIMATE (cid:96)p REGRESSION({Ai  ni  di}i∈[q])
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

si ← O(qd2
i )
Generate sparse p-stable transform Si ∈ Rsi×n (def 2.2)
Take the QR factorization of SiAi = QiRi to obtain Ri ∈ Rdi×di
Let Z ∈ Rd×τ be a dense p-stable transform for τ = Θ(log(n))
for j = 1  . . .   ni do

i=1 di  n ←(cid:81)q

ai j ← medianη∈[τ ]{(|(AiR−1

qi = min{1  r1q(cid:48)

i} and 0 otherwise  where r1 = Θ(d3/2).

end for
Deﬁne a distribution D = {q(cid:48)
Let Π ∈ Rn×n denote a diagonal sampling matrix  where Πi i = 1/q1/p
Let x(cid:48) ∈ Rd denote the solution of
return x(cid:48)

14:
15:
16:
17: end procedure
18: procedure (1 + )-APPROXIMATE (cid:96)p REGRESSION(x(cid:48) ∈ Rd)
Implicitly deﬁne ρ = (A1 ⊗ A2 ⊗ ··· ⊗ Aq)x(cid:48) − b ∈ Rn
19:
Compute a diagonal sampling matrix Σ ∈ Rn×n such that Σi i = 1/α1/p
20:
p}} where r2 = Θ(d3/3).

minx∈Rd (cid:107)Π(A1 ⊗ A2 ⊗ ··· ⊗ Aq)x − Πb(cid:107)p

αi = min{1  max{qi  r2|ρi|p/(cid:107)ρ(cid:107)p

n} by q(cid:48)(cid:80)q

=(cid:81)q

1  q(cid:48)

1  . . .   q(cid:48)

(cid:81)j−1

i=1 ji

l=1 nl

end for

13:

i Z)j η|/θp)p}  where θp is the median of Dp.

(cid:46) Theorem 3.2

(cid:46) Lemma 2.3
(cid:46) Fact 2.6
(cid:46) Deﬁnition 2.1

i=1 ai ji.

i with probability
(cid:46) [DDH+09]

(AR−1)i ∈ Rd is the i-th row of matrix (AR−1) ∈ Rn×d. We say that σ ∈ Rn is the (cid:96)2 leverage
score vector of A.
Deﬁnition 2.5 (((cid:96)p  α  β) Well-Conditioned Basis  [Cla05]). Given a matrix A ∈ Rn×d  we say
U ∈ Rn×d is an ((cid:96)p  α  β) well-conditioned basis for the column span of A if the columns of U span
the columns of A  and if for any x ∈ Rd  we have α(cid:107)x(cid:107)p ≤ (cid:107)U x(cid:107)p ≤ β(cid:107)x(cid:107)p  where α ≤ 1 ≤ β. If
β/α = dO(1)  then we simply say that U is an (cid:96)p well conditioned basis for A.
Fact 2.6 ([WW19  MM13]). Let A ∈ Rn×d  and let SA ∈ Rk×d be a low distortion (cid:96)p embedding
for A (see Lemma 2.3)  where k = O(d2/δ). Let SA = QR be the QR decomposition of SA. Then
AR−1 is an (cid:96)p well-conditioned basis with probability 1 − δ.

Construct diagonal leverage score sampling matrix D ∈ Rn×n  with m non-zero entries
Compute (via the psuedo-inverse)

(cid:46) Theorem 3.1

21:

Compute (cid:98)x = arg minx∈Rd (cid:107)Σ(A1 ⊗ A2 ⊗ ··· ⊗ Aq) − Σb(cid:107)p (via convex optimization
return(cid:98)x

methods  e.g.  [BCLL18  AKPS19  LSZ19])

i with probability

22:
23: end procedure

3 Kronecker Product Regression
We ﬁrst introduce our algorithm for p = 2. Our algorithm for 1 ≤ p < 2 is given in Section
3.1. Our regression algorithm for p = 2 is formally stated in Algorithm 1. Recall that our input
design matrix is A = ⊗q
i=1Ai  where Ai ∈ Rni×di  and we are also given b ∈ Rn1···nq. Let

5

n =(cid:81)q

i=1 ni and d =(cid:81)q

i=1 di. The crucial insight of the algorithm is that one can approximately
compute the leverage scores of A given only good approximations to the leverage scores of each Ai.
Applying this fact gives a efﬁcient algorithm for sampling rows of A with probability proportional
to the leverage scores. Following standard arguments  we will show that by restricting the regression
problem to the sampled rows  we can obtain our desired (1 ± )-approximate solution efﬁciently.
Our main theorem for this section is stated below.
where Ai ∈ Rni×di  and b ∈ Rn  where n = (cid:81)q
Theorem 3.1 (Kronecker product (cid:96)2 regression). Let D ∈ Rn×n be the diagonal row sampling
i=1 di. Then let (cid:98)x =
matrix generated in Algorithm 1  with m = Θ(d/(δ2)) non-zero entries  and let A = ⊗q
i=1Ai 
we have (cid:107)A(cid:98)x − b(cid:107)2 ≤ (1 + )(cid:107)Ax∗ − b(cid:107)2. Moreover  the total running time required to compute(cid:98)x
arg minx∈Rd (cid:107)DAx − Db(cid:107)2  and let x∗ = arg minx(cid:48)∈Rd (cid:107)Ax − b(cid:107)2. Then with probability 1 − δ 
is (cid:101)O((cid:80)q

i=1 ni and d = (cid:81)q

i=1 nnz(Ai) + (dq/(δ))O(1)). 9

3.1 Kronecker Product (cid:96)p Regression
We now consider (cid:96)p regression for 1 ≤ p < 2. Our algorithm is stated formally in Algorithm 2.
Our main theorem is as follows.
Theorem 3.2 (Main result  (cid:96)p (1+)-approximate regression). Fix 1 ≤ p < 2. Then for any constant
i=1 di. Let

q = O(1)  given matrices A1  A2 ···   Aq  where Ai ∈ Rni×di  let n =(cid:81)q
(cid:98)x ∈ Rd be the output of Algorithm 2. Then
(cid:101)O (((cid:80)q

(cid:107)(A1 ⊗ A2 ⊗ ··· ⊗ Aq)(cid:98)x − b(cid:107)p ≤ (1 + ) min
i=1 nnz(Ai) + nnz(b) + poly(d log(1/δ)/)) log(1/δ)) time to output(cid:98)x ∈ Rd.

(cid:107)(A1 ⊗ A2 ⊗ ··· ⊗ Aq)x − b(cid:107)p
In

i=1 ni  d =(cid:81)q

algorithm takes

1 − δ.

holds with

probability

at

least

x∈Rn

addition 

our

Our high level approach follows that of [DDH+09]. Namely  we ﬁrst obtain a vector x(cid:48) which is an
O(1)-approximate solution to the optimal solution. This is done by ﬁrst constructing (implicitly) a
matrix U ∈ Rn×d that is a well-conditioned basis for the design matrix A1 ⊗ ··· ⊗ Aq. We then
efﬁciently sample rows of U with probability proportional to their (cid:96)p norm (which must be done
without even explicitly computing most of U). We then use the results of [DDH+09] to demonstrate
that solving the regression problem constrained to these sampled rows gives a solution x(cid:48) ∈ Rd such
that (cid:107)(A1 ⊗ ··· ⊗ Aq)x(cid:48) − b(cid:107)p ≤ 8 minx∈Rd (cid:107)(A1 ⊗ ··· ⊗ Aq)x(cid:48) − b(cid:107)p.
We deﬁne the residual error ρ = (A1 ⊗ ··· ⊗ Aq)x(cid:48) − b ∈ Rn of x(cid:48). Our goal is to sample
additional rows i ∈ [n] with probability proportional to their residual error |ρi|p/(cid:107)ρ(cid:107)p
p  and solve
the regression problem restricted to the sampled rows. However  we cannot afford to compute even
a small fraction of the entries in ρ (even when b is dense  and certainly not when b is sparse). So to
carry out this sampling efﬁciently  we design an involved  multi-part sketching and sampling routine.
This sampling technique is the main technical contribution of this section  and relies on a number of
techniques  such as the Dyadic trick for quickly ﬁnding heavy hitters from the streaming literature 
and a careful pre-processing step to avoid a poly(d)-blow up in the runtime. Given these samples 

we can obtain the solution(cid:98)x after solving the regression problem on the sampled rows  and the fact

that this gives a (1 + ) approximate solution will follow from Theorem 6 of [DDH+09].

4 All-Pairs Regression
Given a matrix A ∈ Rn×d and b ∈ Rn  let ¯A ∈ Rn2×d be the matrix such that ¯Ai+(j−1)n ∗ =
Ai ∗ − Aj ∗  and let ¯b ∈ Rn2 be deﬁned by ¯bi+(j−1)n = bi − bj. Thus  ¯A consists of all pairwise
differences of rows of A  and ¯b consists of all pairwise differences of rows of b . The (cid:96)p all pairs
regression problem on the inputs A  b is to solve minx∈Rd (cid:107) ¯Ax − ¯b(cid:107)p.

9We remark that the exponent of d in the runtime can be bounded by 3. To see this  ﬁrst note that the main
computation taking place is the leverage score computation. For a q input matrices  we need to generate the
leverage scores to precision Θ(1/q)  and the complexity to achieve this is O(d3/q4) by the results of [CW13].
The remaining computation is to compute the pseudo-inverse of a d/2 × d matrix  which requires O(d3/2)
time  so the additive term in the Theorem can be replaced with O(d3/2 + d3/q4).

6

First note that this problem has a close connection to Kronecker product regression. Namely  the
matrix ¯A can be written ¯A = A ⊗ 1n − 1n ⊗ A  where 1n ∈ Rn is the all 1’s vector. Similarly 
¯b = b ⊗ 1n − 1n ⊗ b. For simplicity  we now drop the superscript and write 1 = 1n.
Our algorithm is given formally in Algorithm 3. The main technical step takes place on line 7 
where we sample rows of the matrix (F ⊗ 1 − 1 ⊗ F )R−1 with probability proportional to their (cid:96)p
norms. This is done by an involved sampling procedure described in the full version of this work.
We summarize the guarantee of our algorithm in the following theorem.
Theorem 4.1. Given A ∈ Rn×d and b ∈ Rn  for p ∈ [1  2]  let ¯A = A ⊗ 1 − 1 ⊗ A ∈ Rn2×d

and ¯b = b ⊗ 1 − 1 ⊗ b ∈ Rn2. Then there is an algorithm for that outputs (cid:98)x ∈ Rd such that
with probability 1 − δ we have (cid:107) ¯A(cid:98)x − ¯b(cid:107)p ≤ (1 + ) minx∈Rd (cid:107) ¯Ax − ¯b(cid:107)p. The running time is
(cid:101)O(nnz(A) + (d/(δ))O(1)).

Algorithm 3 Our All-Pairs Regression Algorithm
1: procedure ALL-PAIRS REGRESSION(A  b)
F = [A  b] ∈ Rn×d+1. r ← poly(d/)
2:
Generate S1  S2 ∈ Rk×n sparse p-stable transforms for k = poly(d/(δ)).
3:
Sketch (S1 ⊗ S2)(F ⊗ 1 − 1 ⊗ F ).
4:
Compute QR decomposition: (S1 ⊗ S2)(F ⊗ 1 − 1 ⊗ F ) = QR.
5:
Let M = (F ⊗ 1 − 1 ⊗ F )R−1  and σi = (cid:107)Mi ∗(cid:107)p
6:
7:

Obtain row sampling diagonal matrix Π ∈ Rn×n such that Πi i = 1/(cid:101)qi
with probability qi ≥ min{1  rσi}  where(cid:101)qi = (1 ± 2)qi.
return(cid:98)x   where(cid:98)x = arg minx∈Rd (cid:107)Π( ¯Ax − ¯b)(cid:107)p.

p/(cid:107)M(cid:107)p
p.

1/p independently

8:
9: end procedure

n = (cid:81)q

i=1 ni and d = (cid:81)q

5 Low Rank Approximation of Kronecker Product Matrices
We now consider low rank approximation of Kronecker product matrices. Given q matrices
A1  A2  . . .   Aq  where Ai ∈ Rni×di  the goal is to output a rank-k matrix B ∈ Rn×d  where
i=1 di  such that (cid:107)B − A(cid:107)F ≤ (1 + ) OPTk  where OPTk =
minrank−k A(cid:48) (cid:107)A(cid:48) − A(cid:107)F   and A = ⊗q
i=1Ai. Our approach employs the Count-Sketch distribu-
tion of matrices [CW13  Woo14]. A count-sketch matrix S is generated as follows. Each column of
S contains exactly one non-zero entry. The non-zero entry is placed in a uniformly random row  and
the value of the non-zero entry is either 1 or −1 chosen uniformly at random.
Our algorithm is as follows. We sample q independent Count-Sketch matrices S1  . . . Sq  with Si ∈
Rki×ni  where k1 = ··· = kq = Θ(qk2/2). We then compute M = (⊗q
i=1Si)A  and let U ∈ Rk×d
be the top k right singular vectors of M. Finally  we output B = AU(cid:62)U in factored form (as q + 1
separate matrices  A1  A2  . . .   Aq  U)  as the desired rank-k approximation to A. The following
theorem demosntrates the correctness of this algorithm.
i=1 nnz(Ai)+
d poly(k/)) log(1/δ)) and outputs a rank k-matrix B in factored form such that (cid:107)B − A(cid:107)F ≤
(1 + ) OPTk with probability 1 − δ. with probability 9/10.

Theorem 5.1. For any constant q ≥ 2  there is an algorithm which runs in time O(((cid:80)q

6 Numerical Simulations

In our numerical simulations  we compare our algorithms to two baselines: (1) brute force  i.e. 
directly solving regression without sketching  and (2) the methods based sketching developed in
[DSSW18]. All methods were implemented in Matlab on a Linux machine. We remark that in our
implementation  we simpliﬁed some of the steps of our theoretical algorithm  such as the residual
sampling algorithm used in Alg. 2. We found that in practice  even with these simpliﬁcations  our
algorithms already demonstrated substantial improvements over prior work.
Following the experimental setup in [DSSW18]  we generate matrices A1 ∈ R300×15  A2 ∈
R300×15  and b ∈ R3002  such that all entries of A1  A2  b are sampled i.i.d. from a normal distribu-
tion. Note that A1 ⊗ A2 ∈ R90000×225. We deﬁne Tbf to be the time of the brute force algorithm 

7

Table 1: Results for (cid:96)2 and (cid:96)1-regression with respect to different sketch sizes m.

r(cid:48)

e

(cid:96)2

(cid:96)1

m
8100
12100
16129
2000
4000
8000
12000
16000

m/n
.09
.13
.18
.02
.04
.09
.13
.18

re

rt
2.48% 1.51% 0.05
1.55% 0.98% 0.06
1.20% 0.71% 0.07
7.72% 9.10% 0.02
4.26% 4.00% 0.03
1.85% 1.6% 0.07
1.29% 0.99% 0.09
1.01% 0.70% 0.14

r(cid:48)
t
0.22
0.24
0.08
0.59
0.75
0.83
0.79
0.90

Told to be the time of the algorithms from [DSSW18]  and Tours to be the time of our algorithms.
We are interested in the time ratio with respect to the brute force algorithm and the algorithms from
[DSSW18]  deﬁned as  rt = Tours/Tbf  and r(cid:48)
t = Tours/Told. The goal is to show that our methods
are signiﬁcantly faster than both baselines  i.e.  both rt and r(cid:48)
We are also interested in the quality of the solutions computed from our algorithms  compared to the
brute force method and the method from [DSSW18]. Denote the solution from our method as xour 
the solution from the brute force method as xbf  and the solution from the method in [DSSW18] as
xold. We deﬁne the relative residual percentage reand r(cid:48)

t are signiﬁcantly less than 1.

e to be:

re = 100

|(cid:107)Axours − b(cid:107) − (cid:107)Axbf − b(cid:107)|

(cid:107)Axbf − b(cid:107)

r(cid:48)
e = 100

 

|(cid:107)Axold − b(cid:107) − (cid:107)Axbf − b(cid:107)|

(cid:107)Axbf − b(cid:107)

Where A = A1 ⊗ A2. The goal is to show that re is close zero  i.e.  our approximate solution is
comparable to the optimal solution in terms of minimizing the error (cid:107)Ax − b(cid:107).
Throughout the simulations  we use a moderate input matrix size so that we can accommodate the
brute force algorithm and to compare to the exact solution. We consider varying values of m  where
M denotes the size of the sketch (number of rows) used in either the algorithms of [DSSW18] or
the algorithms in this paper. We also include a column m/n in the table  which is the ratio between
the size of the sketch and the original matrix A1 ⊗ A2. Note in this case that n = 90000.
Simulation Results for (cid:96)2 We ﬁrst compare our algorithm  Alg. 1  to baselines under the (cid:96)2 norm.
In our implementation  minx (cid:107)Ax− b(cid:107)2 is solved by Matlab backslash A\b. Table 1 summarizes the
comparison between our approach and the two baselines. The numbers are averaged over 5 random
trials. First of all  we notice that our method in general provides slightly less accurate solutions
than the method in [DSSW18]  i.e.  re > r(cid:48)
e in this case. However  comparing to the brute force
algorithm  our method still generates relatively accurate solutions  especially when m is large  e.g. 
the optimal solution is around 1% when m ≈ 16000. On
the relative residual percentage w.r.t.
the other hand  as suggested by our theoretical improvements for (cid:96)2  our method is signiﬁcantly
faster than the method from [DSSW18]  consistently across all sketch sizes m. Note that when
m ≈ 16000  our method is around 10 times faster than the method in [DSSW18]. For small m  our
approach is around 5 times faster than the method in [DSSW18].
Simulation Results for (cid:96)1 We compare our algorithm  Alg. 2  to two baselines under the (cid:96)1-
norm. The ﬁrst is a brute-force solution  and the second is the algorithm for [DSSW18]. For
minx (cid:107)Ax − b(cid:107)1  the brute for solution is obtained via a Linear Programming solver in Gurobi
[GO16]. Table 1 summarizes the comparison of our approach to the two baselines under the (cid:96)1-
norm. The statistics are averaged over 5 random trials. Compared to the Brute Force algorithm  our
method is consistently around 10 times faster  while in general we have relative residual percentage
around 1%. Compared to the method from [DSSW18]  our approach is consistently faster (around
1.3 times faster). Note our method has slightly higher accuracy than the one from [DSSW18] when
the sketch size is small  but slightly worse accuracy when the sketch size increases.

Acknowledgments
The authors would like to thank Lan Wang and Ruosong Wang for a helpful discussion. The authors
would like to thank Lan Wang for introducing the All-Pairs Regression problem to us.

8

References
[AKK+20] Thomas D. Ahle  Michael Kapralov  Jakob B. T. Knudsen  Rasmus Pagh  Ameya Vel-
ingker  David P. Woodruff  and Amir Zandieh. Oblivious sketching of high-degree
polynomial kernels. In SODA. Merger version of https://arxiv.org/pdf/1909.
01410.pdf and https://arxiv.org/pdf/1909.01821.pdf  2020.

[AKO11] Alexandr Andoni  Robert Krauthgamer  and Krzysztof Onak. Streaming algorithms
via precision sampling. In Foundations of Computer Science (FOCS)  2011 IEEE 52nd
Annual Symposium on  pages 363–372. IEEE  https://arxiv.org/pdf/1011.
1263  2011.

[AKPS19] Deeksha Adil  Rasmus Kyng  Richard Peng  and Sushant Sachdeva. Iterative reﬁne-
ment for (cid:96)p-norm regression. In Proceedings of the Thirtieth Annual ACM-SIAM Sym-
posium on Discrete Algorithms (SODA)  pages 1405–1424. SIAM  2019.

[BBB+19] Frank Ban  Vijay Bhattiprolu  Karl Bringmann  Pavel Kolev  Euiwoong Lee  and
David P Woodruff. A ptas for (cid:96)p-low rank approximation. In Proceedings of the Thir-
tieth Annual ACM-SIAM Symposium on Discrete Algorithms  pages 747–766. SIAM 
2019.

[BCLL18] Sébastien Bubeck  Michael B Cohen  Yin Tat Lee  and Yuanzhi Li. An homotopy
method for (cid:96)p regression provably beyond self-concordance and in input-sparsity time.
In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing
(STOC)  pages 1130–1137. ACM  2018.

[BSS12] Joshua Batson  Daniel A Spielman  and Nikhil Srivastava. Twice-ramanujan spar-
In SIAM Journal on Computing  volume 41(6)  pages 1704–1721. https:

siﬁers.
//arxiv.org/pdf/0808.0163  2012.

[BW14] Christos Boutsidis and David P Woodruff. Optimal CUR matrix decompositions. In
Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC) 
pages 353–362. ACM  https://arxiv.org/pdf/1405.7910  2014.

[BW18] Ainesh Bakshi and David Woodruff. Sublinear time low-rank approximation of dis-
tance matrices. In Advances in Neural Information Processing Systems  pages 3782–
3792  2018.

[BWZ16] Christos Boutsidis  David P Woodruff  and Peilin Zhong. Optimal principal compo-
nent analysis in distributed and streaming models. In Proceedings of the 48th Annual
ACM SIGACT Symposium on Theory of Computing (STOC)  pages 236–249. ACM 
https://arxiv.org/pdf/1504.06729  2016.

[CDMI+13] Kenneth L Clarkson  Petros Drineas  Malik Magdon-Ismail  Michael W Mahoney 
Xiangrui Meng  and David P Woodruff. The fast cauchy transform and faster robust
linear regression. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA)  pages 466–477. Society for Industrial and Applied
Mathematics  https://arxiv.org/pdf/1207.4684  2013.

[CGK+17] Flavio Chierichetti  Sreenivas Gollapudi  Ravi Kumar  Silvio Lattanzi  Rina Panigrahy 
and David P Woodruff. Algorithms for (cid:96)p low rank approximation. In ICML. arXiv
preprint arXiv:1705.06730  2017.

[Cla05] Kenneth L Clarkson. Subgradient and sampling algorithms for (cid:96)1 regression.

In
Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms
(SODA)  pages 257–266  2005.

[CM05] Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the

count-min sketch and its applications. Journal of Algorithms  55(1):58–75  2005.

[CW13] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression
in input sparsity time. In Symposium on Theory of Computing Conference (STOC) 
pages 81–90. https://arxiv.org/pdf/1207.6365  2013.

9

[CW15a] Kenneth L Clarkson and David P Woodruff. Input sparsity and hardness for robust
subspace approximation. In 2015 IEEE 56th Annual Symposium on Foundations of
Computer Science (FOCS)  pages 310–329. IEEE  https://arxiv.org/pdf/1510.
06073  2015.

[CW15b] Kenneth L Clarkson and David P Woodruff. Sketching for m-estimators: A uniﬁed
approach to robust regression. In Proceedings of the Twenty-Sixth Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA)  pages 921–939. SIAM  2015.

[DDH+09] Anirban Dasgupta  Petros Drineas  Boulos Harb  Ravi Kumar  and Michael W Ma-
honey. Sampling algorithms and coresets for (cid:96)p regression. SIAM Journal on Comput-
ing  38(5):2060–2078  2009.

[DSSW18] Huaian Diao  Zhao Song  Wen Sun  and David P. Woodruff. Sketching for Kronecker

product regression and p-splines. AISTATS 2018  2018.

[EM06] Paul HC Eilers and Brian D Marx. Multidimensional density smoothing with p-splines.

In Proceedings of the 21st international workshop on statistical modelling  2006.

[GO16] Inc. Gurobi Optimization. Gurobi optimizer reference manual  2016.

[GVL13] Gene H. Golub and Charles F. Van Loan. Matrix computations. Johns Hopkins Studies
in the Mathematical Sciences. Johns Hopkins University Press  Baltimore  MD  2013.

[Ind06] Piotr Indyk. Stable distributions  pseudorandom generators  embeddings  and data

stream computation. Journal of the ACM (JACM)  53(3):307–323  2006.

[IVWW19] Piotr Indyk  Ali Vakilian  Tal Wagner  and David P. Woodruff. Sample-optimal low-

rank approximation of distance matrices. In COLT  2019.

[KNPW11] Daniel M Kane  Jelani Nelson  Ely Porat  and David P Woodruff. Fast moment esti-
mation in data streams in optimal space. In Proceedings of the forty-third annual ACM
symposium on Theory of computing (STOC)  pages 745–754. ACM  2011.

[KNW10] Daniel M Kane  Jelani Nelson  and David P Woodruff. On the exact space complexity
In Proceedings of the twenty-ﬁrst annual

of sketching and streaming small norms.
ACM-SIAM symposium on Discrete Algorithms  pages 1161–1178. SIAM  2010.

[LHW17] Xingguo Li  Jarvis Haupt  and David Woodruff. Near optimal sketching of low-
rank tensor regression. In Advances in Neural Information Processing Systems  pages
3466–3476  2017.

[LNNT16] Kasper Green Larsen  Jelani Nelson  Huy L Nguyên  and Mikkel Thorup. Heavy
In 2016 IEEE 57th Annual Symposium on

hitters via cluster-preserving clustering.
Foundations of Computer Science (FOCS)  pages 61–70. IEEE  2016.

[LSZ19] Yin Tat Lee  Zhao Song  and Qiuyi Zhang. Solving empirical risk minimization in
the current matrix multiplication time. In COLT. https://arxiv.org/pdf/1905.
04447.pdf  2019.

[Mah11] Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations and

Trends in Machine Learning  3(2):123–224  2011.

[MM13] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in
input-sparsity time and applications to robust linear regression. In Proceedings of the
forty-ﬁfth annual ACM symposium on Theory of computing (STOC)  pages 91–100.
ACM  https://arxiv.org/pdf/1210.3135  2013.

[MW17] Cameron Musco and David P Woodruff. Sublinear time low-rank approximation of
positive semideﬁnite matrices. In 2017 IEEE 58th Annual Symposium on Foundations
of Computer Science (FOCS)  pages 672–683. IEEE  2017.

10

[NN13] Jelani Nelson and Huy L Nguyên. Osnap: Faster numerical linear algebra algorithms
via sparser subspace embeddings. In 2013 IEEE 54th Annual Symposium on Foun-
dations of Computer Science (FOCS)  pages 117–126. IEEE  https://arxiv.org/
pdf/1211.1002  2013.

[Nol07] John P Nolan. Stable distributions. 2007.

[NS19] Vasileios Nakos and Zhao Song. Stronger L2/L2 compressed sensing; without iter-
ating. In Proceedings of the 51st Annual ACM Symposium on Theory of Computing
(STOC)  2019.

[OY05] S. Oh  S. Kwon and J. Yun. A method for structured linear total least norm on blind

deconvolution problem. Applied Mathematics and Computing  19:151–164  2005.

[RSW16] Ilya Razenshteyn  Zhao Song  and David P Woodruff. Weighted low rank approxima-
tions with provable guarantees. In Proceedings of the 48th Annual Symposium on the
Theory of Computing (STOC)  2016.

[SW11] Christian Sohler and David P Woodruff. Subspace embeddings for the (cid:96)1-norm with
applications. In Proceedings of the forty-third annual ACM symposium on Theory of
computing  pages 755–764. ACM  2011.

[SWZ16] Zhao Song  David P. Woodruff  and Huan Zhang. Sublinear time orthogonal tensor
decomposition. In Advances in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems (NIPS) 2016  December 5-10 
2016  Barcelona  Spain  pages 793–801  2016.

[SWZ17] Zhao Song  David P Woodruff  and Peilin Zhong. Low rank approximation with en-
trywise (cid:96)1-norm error. In Proceedings of the 49th Annual Symposium on the Theory of
Computing (STOC). ACM  https://arxiv.org/pdf/1611.00898  2017.

[SWZ18] Zhao Song  David P Woodruff  and Peilin Zhong. Towards a zero-one law for entry-

wise low rank approximation. arXiv preprint arXiv:1811.01442  2018.

[SWZ19a] Zhao Song  David P Woodruff  and Peilin Zhong. Average case column subset selec-

tion for entrywise (cid:96)1-norm loss. In NeurIPS  2019.

[SWZ19b] Zhao Song  David P Woodruff  and Peilin Zhong. Relative error tensor low rank

approximation. In SODA 2019. https://arxiv.org/pdf/1704.08246  2019.

[SWZ19c] Zhao Song  David P Woodruff  and Peilin Zhong. Towards a zero-one law for column

subset selection. In NeurIPS  2019.

[VL92] Charles F Van Loan. Computational frameworks for the fast Fourier transform  vol-
ume 10 of Frontiers in Applied Mathematics. Society for Industrial and Applied Math-
ematics (SIAM)  Philadelphia  PA  1992.

[VL00] Charles F Van Loan. The ubiquitous kronecker product. Journal of computational and

applied mathematics  123(1-2):85–100  2000.

[VLP93] Charles F Van Loan and N. Pitsianis. Approximation with Kronecker products. In Lin-
ear algebra for large scale and real-time applications (Leuven  1992)  volume 232 of
NATO Adv. Sci. Inst. Ser. E Appl. Sci.  pages 293–314. Kluwer Acad. Publ.  Dordrecht 
1993.

[Wan19a] Lan Wang. A new tuning-free approach to high-dimensional regression. .  2019.

[Wan19b] Lan Wang. Personal communication. 2019.

[WKL09] Lan Wang  Bo Kai  and Runze Li. Local rank inference for varying coefﬁcient models.

Journal of the American Statistical Association  104(488):1631–1645  2009.

[WL09] Lan Wang and Runze Li. Weighted wilcoxon-type smoothly clipped absolute deviation

method. Biometrics  65(2):564–571  2009.

11

[Woo14] David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations and

Trends in Theoretical Computer Science  10(1-2):1–157  2014.

[WPB+18] Lan Wang  Bo Peng  Jelena Bradic  Runze Li  and Yunan Wu. A tuning-free robust
and efﬁcient approach to high-dimensional regression. Technical report  School of
Statistics  University of Minnesota  2018.

[WW19] Ruosong Wang and David P Woodruff. Tight bounds for (cid:96)p oblivious subspace em-

beddings. In SODA  2019.

12

,Shikib Mehri
Leonid Sigal
Huaian Diao
Rajesh Jayaram
Zhao Song
Wen Sun
David Woodruff