2010,Fast detection of multiple change-points shared by many signals using group LARS,We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases  and provide empirical evidence to support the consistency results.,Fast detection of multiple change-points shared by

many signals using group LARS

Jean-Philippe Vert and Kevin Bleakley

Mines ParisTech CBIO  Institut Curie  INSERM U900

{firstname.lastname}@mines-paristech.fr

Abstract

We present a fast algorithm for the detection of multiple change-points when each
is frequently shared by members of a set of co-occurring one-dimensional signals.
We give conditions on consistency of the method when the number of signals
increases  and provide empirical evidence to support the consistency results.

1

Introduction

Finding the place (or time) where most or all of a set of one-dimensional signals (or proﬁles) jointly
change in some speciﬁc way is an important question in several ﬁelds. A ﬁrst common situation is
when we want to ﬁnd change-points in a multidimensional signal  for instance  we may want to auto-
matically detect changes from human speech to other sound in a movie  based on data representation
of features coming from both the audio and visual tracks [1]. Another important situation is when we
are confronted with several 1-dimensional signals which we believe share common change-points 
e.g.  genomic proﬁles of several patients. The latter application is increasingly important in biology
and medicine  in particular for the detection of copy-number variation along the genome  though it
is also useful for microarray and genetic linkage studies [2]. The common thread in all of these is
the search for data patterns shared by a set of patients at precise places on the genome; in particular 
sudden changes in measurement. As opposed to the segmentation of multi-dimensional signals such
as speech  the length of the signal (i.e.  the number of probes along the genome) is ﬁxed for a given
technology while the number of signals (i.e.  the number of patients) can increase. It is therefore of
interest to develop method to identify multiple change-points shared by several signals which can
beneﬁt from increasing the number of proﬁles.
There exists a vast literature on the change-point detection problem [3  4]. Here we focus on the
problem of approximating a multidimensional signal by a piecewise-constant one  using quadratic
error criteria. It is well-known that the optimal segmentation of a p-dimensional signal of length
n into k segments can be obtained in O(n2pk) by dynamic programming [5  6  7]. The quadratic
complexity in n2 is however prohibitive in applications such as genomics  where n can be in the or-
der of 105 to 107 with current technology. An alternative to such global procedures  which estimate
change-points as solutions of a global optimization problem  are fast local procedures such as binary
segmentation [8]  which detect breakpoints by iteratively applying a method for single change-point
detection to the segments obtained after the previous change-point is detected. While such recursive
methods can be extremely fast  in the order of O(np log(k)) when the single change-point detector
is O(np)  quality of segmentation is questionable when compared with global procedures [9].
For p = 1 (a single signal)  an interesting alternative to these global and local procedures is to
express the optimal segmentation as the solution of a convex optimization problem  using the (con-
vex) total variation instead of the (non-convex) number of jumps to penalize a piecewise-constant
function  in order to approximate the original signal [10  11]. The resulting piecewise-constant ap-
proximation of the signal  deﬁned as the global minimum of the objective function  beneﬁts from

1

theoretical guaranties in terms of correctly detecting change-points [12  13]  and can be implemented
efﬁciently in O(nk) or O(n log(n)) [14  12  15].
In this paper we propose an extension of total-variation based methods for single signals to the
multidimensional setting  in order to approximate a multidimensional signal by a piecewise con-
stant signal with multiple change-points. We deﬁne the approximation as the solution of a convex
optimization problem  which involves a quadratic approximation error penalized by the (cid:96)1 norm of
increments of the function. The problem can be reformulated as a group LASSO problem  which we
propose to solve approximately with a group LARS procedure [16]. Using the particular structure
of the design matrix  we can ﬁnd the ﬁrst k change-points in O(npk)  extending the method of [12]
to the multidimensional setting.
Unlike most previous theoretical investigations of change-point methods  we are not interested in
the case where the dimension p is ﬁxed and the length of the proﬁles n increases  but in the opposite
situation where n is ﬁxed and p increases. Indeed  this corresponds to the case in genomics where 
for example  n would be the ﬁxed number of probes used to measure a signal along the genome 
and p the number of samples or patients analyzed. We want to design a method that beneﬁts from
increasing p in order to identify shared change-points  even though the signal-to-noise ratio may be
very low within each signal. As a ﬁrst step towards this question  we give conditions under which
our method is able to consistently identify a single change-point as p increases. We also show by
simulation that our method is able to consistently identify multiple change-points  as p → +∞ 
validating its relevance in practical settings. To conclude  we present possible applications of the
method in the study of copy number variations in cancer.

(cid:113)(cid:80)u

2 Notation
For any two integers u ≤ v  let [u  v] denote the interval {u  u + 1  . . .   v}. For any u × v matrix
M we note Mi j its (i  j)-th entry. (cid:107)M(cid:107) =
i j is its Frobenius norm (or Euclidean

norm in the case of vectors). For any subsets of indices A =(cid:8)a1  . . .   a|A|(cid:9) ∈ [1  u]|A| and B =
(cid:0)b1  . . .   b|B|(cid:1) ∈ [1  v]|B|  we denote by MA B the |A| × |B| matrix with entries Mai bj for (i  j) ∈

[1 |A|] × [1 |B|]. For simplicity we will use • instead of [1  u] or [1  v]  i.e.  Ai • is the i-th row of
A and A• j is the j-th column of A. We note 1u v the u× v matrix of ones  and Ip the p× p identity
matrix.

i=1

j=1 M 2

(cid:80)v

3 Formulation
We consider p proﬁles of length n  stored in an n × p matrix Y . The i-th proﬁle Y• i =
(Y1 i  . . .   Yn i) is the i-th column of Y . We assume that each proﬁle is a piecewise-constant signal
corrupted by noise  and that change-points locations tend to be shared across proﬁles. Our goal is
to detect these shared change-points  and beneﬁt from the possibly large number p of proﬁles to
increase the statistical power of change-point detection.
When p = 1 (single proﬁle)  a popular method to ﬁnd change-points in a signal is to approximate it
by a piecewise constant function using total variation (TV) denoising [10]  i.e.  to solve

n−1(cid:88)

i=1

n−1(cid:88)

i=1

2

(cid:107) Y − U (cid:107)2 + λ

min
U∈Rn

| Ui+1 − Ui | .

(1)

For a given λ > 0  the solution U ∈ Rn of (1) is piecewise-constant and its change-points are
predicted to be those of Y . Adding penalties proportional to the (cid:96)1 ot (cid:96)2 norm of U to (1) does
not change the position of the change-points detected [11  17]  and the capacity of TV denoising to
correctly identify change-points when n increases has been investigated in [12  13].
Here we propose to generalize TV denoising to multiple proﬁles by considering the following convex
optimization problem  for Y ∈ Rn×p:

(cid:107) Y − U (cid:107)2 + λ

(cid:107) Ui+1 • − Ui • (cid:107) .

(2)

min

U∈Rn×p

The second term in (2) penalizes the sum of Euclidean norms of the increments of U  seen as
a time-dependent multidimensional vector. Intuitively  this penalty will enforce many increments
Ui+1 • − Ui • to collapse to 0  just like the total variation in (1). As a result the solution of (2)
provides an approximation of the proﬁles Y by a n × p matrix of piecewise-constant proﬁles U
which share change-points. In the following  we propose a fast algorithm to approximately solve (2)
(Section 4)  discuss theoretically whether the solution identiﬁes correctly the change-points (Section
5)  and provide an empirical evaluation of the method (Section 6).

4

Implementation

Although (2) is a convex optimization problem that can in principle be solved by general-purpose
solvers [18]  we are often working in dimensions that can reach millions  making this approach
impractical. Moreover  we would ideally like to obtain solutions for various values of λ  corre-
sponding to various numbers of change-points  in order to be able to select the optimal number of
change-points using various statistical criteria. In the single proﬁle case (p = 1)  [14] proposed a
fast coordinate descent-like method  [12] showed how to ﬁnd the ﬁrst k change-points iteratively in
O(nk)  and [15] proposed an O(n ln(n)) method to ﬁnd all change-points. However  none of these
methods is applicable directly to the p > 1 setting since they all rely on speciﬁc properties of the
p = 1 case  such as the fact that the solution is piecewise-afﬁne in λ and that the set of change-points
is monotically decreasing with λ.
In order to propose a fast method to solve (2) in the p > 1 setting  let us ﬁrst reformulate it as
a group LASSO regression problem [16]. To this end  we make the change of variables (β  γ) ∈
R(n−1)×p × R1×p given by:

γ = U1 •  

βi • = Ui+1 • − Ui •

for i = 1  . . .   n − 1 .

In other words βi j is the jump between the i-th and the (i + 1)-th positions of the j-th proﬁle. We
immediately get an expression of U as a function of β and γ:

U1 • = γ  

Ui • = γ +

i−1(cid:88)

βj •

for i = 2  . . .   n .

This can be rewritten in matrix form as

j=1

where X is the n × (n − 1) matrix with entries Xi j = 1 for i > j. Making this change of variable 
we can re-express (2) as follows:

U = 1n 1γ + Xβ  

(3)
For any β ∈ R(n−1)×p  the minimum in γ is reached for γ = 11 n(Y − Xβ)/n. Plugging this into
(3)  we get that the matrix of jumps β is solution of

β∈R(n−1)×p  γ∈R1×p

min

(cid:107) Y − Xβ − 1n 1γ (cid:107)2 + λ

(cid:107) βi • (cid:107) .

n−1(cid:88)

i=1

n−1(cid:88)

i=1

min

β∈R(n−1)×p

(cid:107) ¯Y − ¯Xβ (cid:107)2 + λ

(cid:107) βi • (cid:107)  

(4)

where ¯Y and ¯X are obtained from Y and X by centering each column.
Equation 4 is a group LASSO problem  with a particular design matrix and particular groups of
features. Since existing methods to exactly solve group LASSO regression problems remain difﬁcult
to apply here – in particular we do not want to store in memory the n × (n − 1) design matrix when
n is in the millions – we propose to approximate instead the solution of (4) with the group LARS
strategy  which was proposed by [16] as a good approximation to the regularization path of the group
LASSO. More precisely  the group LARS approximates the solution path of (4) with a piecewise-
afﬁne set of solutions  and iteratively ﬁnds change-points. While the original group LARS method
requires storing and manipulation of the design matrix [16]  which we can not afford here  we can
extend technical results of [12] to show that the particular structure of the design matrix ¯X allows
efﬁcient computation of matrix inverses and products.

3

Lemma 1. For any R ∈ Rn×p  we can compute C = ¯X(cid:62)R in O(np) time and memory.

Lemma 2. For any A =(cid:0)a1  . . .   a|A|(cid:1)  set of distinct indices with 1 ≤ a1 < . . . < a|A| ≤ n  the
matrix(cid:0) ¯X(cid:62)

(cid:1) is invertible  and for any |A| × p matrix R  the matrix

¯X• A

• A

C =(cid:0) ¯X(cid:62)

(cid:1)−1

¯X• A

• A

R

can be computed in O(|A|p) time and memory.
Proof of these results can be found in Supplementary Materials.
Algorithm 1 describes the fast group LARS method to approximately solve (4). At each subse-
quent iteration to ﬁnd the next change-point  we follow steps 3–8 which have maximum complexity
O(np)  resulting in O(npk) complexity in time and O(np) in memory to ﬁnd the ﬁrst k change-
points with the fast group LARS algorithm.

Algorithm 1 Fast group LARS algorithm
Require: centered data ¯Y   number of breakpoints k.
1: Initialize r = ¯Y   A = ∅.
2: for i = 1 to k do
3:
4:
5:

Descent direction: compute w = (cid:0) ¯X(cid:62)

Compute ˆc = ¯X(cid:62)r using Lemma 1.
If i = 1  ﬁnd the ﬁrst breakpoint : ˆa = argmin j∈[1 n] (cid:107) ˆcj • (cid:107)  A = {ˆa}.

A • ¯XA •(cid:1)−1 ˆcA • using Lemma 2  then uA = ¯XAw

with cumulative sums  then a = ¯X(cid:62)uA using Lemma 1.
Descent step: for each u ∈ [1  n]\A  ﬁnd if it exists the smallest positive solution αu of the
second-order polynomial in α:

6:

(cid:107) ˆcu • − αau • (cid:107)2 = (cid:107) ˆcv • − αav • (cid:107)2  

where v is any element of A.
Find the next breakpoint: ˆu = argmin [1 p]\A αu.
Update A = A ∪ {ˆu} and r = r − aˆuuA.

7:
8:
9: end for

5 Theoretical analysis

In this section  we study theoretically to what extent the estimator (2) recovers correct change-points.
The vast majority of existing theoretical results for ofﬂine segmentation and change-point detection
consider the setting where p is ﬁxed (usually p = 1)  and n increases. This typically corresponds to
a setting where we can sample a continuous signal with increasing density  and wish to locate more
precisely the underlying change-points as the density increases.
Here we propose a radically different analysis  motivated by applications in genomics. Here  the
length of proﬁles n is ﬁxed for a given technology  but the number of proﬁles p can increase when
more biological samples or patients are analyzed. The property we would like to study is then  for
a given change-point detection method  whether increasing p for ﬁxed n allows us to locate more
precisely the change-points. While this simply translates our intuition that increasing the number of
proﬁles should increase the statistical power of change-point detection  and while this property was
empirically observed in [2]  we are not aware of previous theoretical results in this setting.

5.1 Consistent estimation of a single change-point

As a ﬁrst step towards the analysis of this "ﬁxed n increasing p" setting  let us assume that the
observed centered proﬁles ¯Y are obtained by adding noise to a set of proﬁles with a single shared
change-point between positions u and u + 1  for some u ∈ [1  n − 1]. In other words  we assume
that
where β∗ is an (n−1)×p matrix of zeros except for the u-th row β∗
u •  and W is a noise matrix whose
entries are assumed to be independent and identically distributed with respect to a centered Gaussian

¯Y = ¯Xβ∗ + W  

4

(cid:1)

u i

i≥1

jumps(cid:0)β∗

k = 1/k(cid:80)k

  and letting ¯β2

distribution with variance σ2. In this section we study the probability that the ﬁrst breakpoint found
by our procedure is the correct one  when p increases. We therefore consider an inﬁnite sequence of
k exists and
is ﬁnite. We ﬁrst show that  as p increases  the ﬁrst selected change-point is always the given by the
same formula.
Lemma 3. Assume  without loss of generality  that u ≥ n/2. When p → +∞  the ﬁrst change-point
selected is

u i)2  we assume that ¯β2 = limk→∞ ¯β2

i=1(β∗

¯β2 i2 (n − u)2

+ σ2 i (n − i)

n2

n

.

(5)

ˆu = argmax
i∈[1 u]

with probability tending to 1.

From this we easily deduce under which condition the correct change point is selected  i.e.  when
ˆu = u:
Theorem 4. Let α = u/n and

α = n ¯β2 (1 − α)2(α − 1
2n)
2 − 1

α − 1

2n

˜σ2

.

(6)

α  the probability that the ﬁrst selected change-point is the correct one tends to 1 as

When σ2 < ˜σ2
p → +∞. When σ2 > ˜σ2
This theorem  whose proof along with that of Lemma 3 can be found in Supplementary Materials 
deserves several comments.

α  it is not the correct one with probability tending to 1.

• To detect a change-point at position u = αn  the noise level σ2 must not be larger than
the critical value σα given by (7)  hence the method is not consistent for all positions. σα
increases monotonically from α = 1/2 to 1  meaning that change-points near the boundary
are more difﬁcult to detect correctly than change-points near the center. The most difﬁcult
change point is the last one (u = n − 1) which can only be detected consistently if σ2 is
smaller than

• For a given level of noise σ2  change-point detection is asymptotically correct for any

α ∈ [  1 − ]  where  satisﬁes σ2 = ¯σ2

¯σ2
1−1/n =

2 ¯β2
n

+ o(n−1).

(cid:115)

 =

1−  i.e. 
2n ¯β2 + o(n−1/2) .

σ2

This shows in particular that increasing the proﬁle length n increases the interval where
change-points are correctly identiﬁed  and that we can get as close as possible to the bound-
ary for n large enough.

• When σ2 < σ2
α then the correct change-point is found consistently when p increases 
showing the beneﬁt of the accumulation of many proﬁles.
• It is possible to make the detection of the ﬁrst change-point consistent uniformly over the
full signal  by simply subtracting the term pσ2i(n−i)/n from (cid:107) ¯ci • (cid:107)2  which is maximized
over i to select the ﬁrst change-point. Then  a simple modiﬁcation of Lemma 3 shows that 
as p → +∞  any given change-point is a.s. found. However  this modiﬁcation  easy to do
for the ﬁrst change-point  is not obvious to extend to successive change-points detected by
group LARS. We consider it an interesting future challenge to develop variants of the group
LARS iterative segmentation method whose performance does not depend on the position
of the change points.

5.2 Consistent estimation of a single change-point with ﬂuctuating position

An interesting variant of the problem of detecting a change-point common to many proﬁles is that of
detecting a change-point with similar location in many proﬁles  allowing ﬂuctuations in the precise

5

location of the change-point. This can be modeled by assuming that the proﬁles are random  and that
the i-th proﬁle has a change-point of value βi at position Ui  where (βi  Ui)i=1 ... p are independent
and identically distributed according to a distribution P = Pβ ⊗ PU (i.e.  we assume βi independent
from Ui). We denote ¯β2 = EPβ β2 and pi = PU (U = i) for i ∈ [1  n − 1]. Assuming that the
support of PU is [a  b] with 1 ≤ a ≤ b ≤ n − 1  the following result extends Theorem 4 by showing
that  under a condition on the noise level  the ﬁrst change-point discovered is indeed in the support
of PU :
Theorem 5. Let α = U/n be the random position of the change-point on [0  1] and αm = a/n and
αM = b/n the position of the left and right boundaries of the support of PU scaled to [0  1]. Let
also

(cid:2)(1 − Eα)2 + var(α)2(cid:3) (αm − 1

(7)
If 1/2 ∈ (αm  αM )  then for any σ2 the probability that the ﬁrst selected change-point is in the
support of P tends to 1 as p → +∞. If 1/2 < αm  then the probability that the ﬁrst selected
change-point is in the support of P tends to 1 when σ2 < ˜σ2
α  it is not the correct
one with probability tending to 1.

α  . When σ2 > ˜σ2

PU = n ¯β2
˜σ2

αm − 1

2 − 1

2n

2n)

.

This theorem  whose proof is postponed to Supplementary Materials  illustrates the robustness of
the method to handle ﬂuctuations in the precise position of the change-point shared between the
proﬁles. Although this situation rarely occurs when we are considering classical multidimensional
signals such as ﬁnancial time series or video signals  it is likely to be the rule when we consider
proﬁles coming from different biological samples. Although the theorem only gives a condition
on the noise level to ensure that the selected change-point lies in the support of the distribution of
change-point locations  a precise estimate of the location of the selected change-point as a function
of PU   which generalizes Lemma 3  is given in the proof.

5.3 The case of multiple change-points

While the theoretical results presented above focus on the detection of a single change-point  the
real interest of the method is to estimate multiple change-points. The extension of Theorem 4 to
this setting is beyond the scope of this paper  and is postponed for future efforts. We nevertheless
conjecture here that we can consistently estimate multiple change-points under conditions on the
level of noise (not too large)  the distance between them (not to small)  and the correlations between
their jumps (not too large). Indeed  following the ideas in the proof of Theorem 4  we must analyze
the path of the vectors (ˆci ...)  and check that  for some λ in (2)  they reach their maximum norm
precisely at the true change-points. The situation is more complicated than in the single change-
point case since the vectors (ˆci ...) must hit a hypersphere at each correct change-point  and must
remain strictly within the hypersphere between consecutive change-points. This can be ensured if the
noise level is not too high (like in the single change-point case)  and if the positions corresponding
to successive change-points on the hypersphere are far enough from each other. In practice this
translates to conditions that two successive change-points should not be too close to each other  and
that proﬁles should have  if possible  independent jumps (direction  etc.). We provide experimental
results below that conﬁrm that  when the noise is not too large  we can indeed correctly identify
several change-points  with a probability of success increasing to 1 as p increases.

6 Experiments

In this section we give experimental evidence both for theoretical O(npk) complexity and Theorem
4. Figure 1 shows linearity in each of p  n and k respectively whilst ﬁxing the other two variables 
conﬁrming the O(npk) complexity.
To test Theorem 4  we considered signals of length 100  each with a unique change-point located
at position u. We ﬁxed α = 0.8; assuming for simplicity that each signal jumps a height of 1 at
the change-point  we get ¯β2 = 1  and it is then easy to calculate the critical value ˜σ2
α = 10.78. We
set the variance of the centered Gaussian noise added to each signal to ˜σ2
α  and ran 1000 trials for
each u. we expect that for 50 ≤ u < 80 there is convergence in accuracy to 1  and for u > 80 
convergence in accuracy to zero. This is indeed what is seen in Figure 2 (left panel)  with u = 80
the limit case between the two different modes of convergence.

6

Figure 1: Speed trials.
(a) CPU time for ﬁnding 50 change-points when there are 2000 probes and the
number of proﬁles varies from 1 to 20.
(b) CPU time when ﬁnding 50 change-points with the number of
proﬁles ﬁxed at 20 and the number of probes varying from 1000 to 10000 in intervals of 1000. (c) CPU time
for 20 proﬁles and 2000 probes when selecting from 1 to 50 change-points.

Figure 2: Single change-point accuracy. Accuracy as a function of the number of proﬁles p when the
change-point is placed in a variety of positions from: u = 50 to u = 90 (left panel)  or: u = 50 ± 2 to
u = 90 ± 2 (right panel)  for a signal of length 100.

The right-hand-side panel of Figure 2 shows results for the same trials except that change-point
locations can vary uniformly in the interval u ± 2. As predicted by Theorem 5  we see that the
accuracy of the method remains extremely robust against ﬂuctuations in the exact change-point
location.
To investigate the potential for extending the results of the article to the case of many shared
change-points  we further simulated proﬁles of length 100 with a change-point at all of positions
10  20  . . .   90. The jump at each change-point was drawn from a centered Gaussian with variance
1. We then ﬁxed various values of σ2 and looked at convergence in accuracy as the number of
signals increased. One thousand trials were performed for each σ2  and results are presented in
Figure 3. Denoting α the set of change-point locations {10  20  . . .   90}   it appears that a critical
value ˜σ2
α exists and lies close to 0.27; below 0.27 we have convergence in accuracy to 1  and above 
convergence to zero.
An interesting application of the fast group LARS method is in the joint segmentation of copy-
number proﬁles. For a set of individuals with the same disease (e.g. a type of cancer)  we expect
there to be regions of the genome which are frequently gained (potentially containing oncogenes) or
lost (potentially containing tumor suppressor genes) in many or all of the patients. These regions are
separated by change-points. Figure 4 shows Chromosome 8 of three bladder cancer copy-number
proﬁles. We see that in the region of probe 60  a copy number change occurs on all three proﬁles.
Though it is not in exactly the same place on all proﬁles  the sharing of information across proﬁles

7

010200.050.10.150.20.250.30.350.4time(s)p(a)050001000000.20.40.60.811.21.41.61.8ntime(s)(b)0204000.050.10.150.20.250.30.35ktime(s)(c)010020030040000.20.40.60.81pAccuracy  u=50u=60u=70u=80u=90010020030040000.20.40.60.81pAccuracy  u=50u=60u=70u=80u=90Figure 3: Multiple change-point accuracy. Accuracy as a function of the number of proﬁles p when
change-points are placed at the nine positions {10  20  . . .   90} and the value of σ2 is varied from 0.1 to 0.4.
The proﬁle length is 100.

allows the approximate location to be found. The bottom right panel shows the smoothed proﬁles
superimposed on the same axes. A promising use of these smoothed signals  beyond visualization
of many proﬁles simultaneously  is to detect regions of frequent gain of loss by testing the average
proﬁle values on each segment for signiﬁcant positive (gain) or negative (loss) values. Preliminary
experiments on simulated and real data suggest that our method is more accurate and two orders of
magnitude faster than the state-of-the-art H-HMM [19] method for that purpose.

Figure 4: Segmented and smoothed bladder cancer copy-number proﬁles. Probes shown are
located on Chromosome 8. A shared change-point hotspot is found in the region of probe 60.

7 Conclusion

We have proposed a framework that extends total-variation based approximation to the multidi-
mensional setting  developed a fast algorithm to approximately solve it  shown theoretically that
the method can consistently estimate change-points  and validated the results experimentally. We
have not discussed the problem of choosing the number of change-points  and suggest in practice
to use existing criteria for this purpose [6  7]. We observed both theoretically and empirically that
increasing the number of proﬁles is highly beneﬁcial to detect shared change-points
Acknowledgements We thank Zaid Harchaoui and Francis Bach for useful discussions. This work
was supported by ANR grants ANR-07-BLAN-0311-03 and ANR-09-BLAN-0051-04.

8

01000200030004000500000.20.40.60.81pAccuracy  sigma2=0.05sigma2= 0.1sigma2= 0.2sigma2=0.27sigma2= 0.4050100150−1−0.500.51Probe050100150−1−0.500.51Probe050100150−1−0.500.51Probe050100150−1−0.500.51ProbeReferences
[1] Z. Harchaoui  F. Vallet  A. Lung-Yut-Fong  and O. Cappe. A regularized kernel-based approach
to unsupervised audio segmentation. In ICASSP ’09: Proceedings of the 2009 IEEE Interna-
tional Conference on Acoustics  Speech and Signal Processing  pages 1665–1668  Washington 
DC  USA  2009. IEEE Computer Society.

[2] N. R. Zhang  D. O. Siegmund  H. Ji  and J. Li. Detecting simultaneous change-points in

multiple sequences. Biometrika  97(3):631–645  2010.

[3] M. Basseville and N. Nikiforov. Detection of abrupt changes: theory and application. Infor-

mation and System Sciences Series. Prentice Hall Information  1993.

[4] B. Brodsky and B. Darkhovsky. Nonparametric Methods in Change-Point Problems. Kluwer

Academic Publishers  1993.

[5] Y. C. Yao. Estimating the number of change-points via schwarz criterion. Stat. Probab. Lett. 

6:181–189  1988.

[6] L. Birgé and P. Massart. Gaussian model selection. J. Eur. Math. Soc.  3:203–268  2001.
[7] M. Lavielle and G. Teyssière. Detection of multiple change-points in multivariate time series.

Lithuanian Mathematical Journal  46(3):287–306  2006.

[8] L. J. Vostrikova. Detection of disorder in multidimensional stochastic processes. Soviet Math-

ematics Doklady  24:55–59  1981.

[9] M. Lavielle and Teyssière. Adaptive detection of multiple change-points in asset price volatil-
In G. Teyssière and A. Kirman  editors  Long-Memory in Economics  pages 129–156.

ity.
Springer Verlag  Berlin  2005.

[10] L. I. Rudin  S. Osher  and E. Fatemi. Nonlinear total variation based noise removal algorithms.

Physica D  60:259–268  1992.

[11] R. Tibshirani  M. Saunders  S. Rosset  J. Zhu  and K. Knight. Sparsity and smoothness via the

fused lasso. J. R. Stat. Soc. Ser. B Stat. Methodol.  67(1):91–108  2005.

[12] Z. Harchaoui and C. Levy-Leduc. Catching change-points with lasso. In J.C. Platt  D. Koller 
Y. Singer  and S. Roweis  editors  Advances in Neural Information Processing Systems 20 
pages 617–624. MIT Press  Cambridge  MA  2008.

[13] A. Rinaldo. Properties and reﬁnements of the fused lasso. Ann. Stat.  37(5B):2922–2952 

2009.

[14] J. Friedman  T. Hastie  H. Höﬂing  and R. Tibshirani. Pathwise coordinate optimization. Ann.

Appl. Statist.  1(1):302–332  2007.

[15] H. Hoeﬂing. A path algorithm for the Fused Lasso Signal Approximator. Technical Report

0910.0526v1  arXiv  Oct. 2009.

[16] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J.

R. Stat. Soc. Ser. B  68(1):49–67  2006.

[17] J. Mairal  F. Bach  J. Ponce  and G. Sapiro. Online learning for matrix factorization and sparse

coding. J. Mach. Learn. Res.  11:19–60  2010.

[18] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press  New York 

NY  USA  2004.

[19] S.P. Shah  W.L. Lam  R.T. Ng  and K.P. Murphy. Modeling recurrent DNA copy number

alterations in array CGH data. Bioinformatics  23(13):i450–i458  2007.

9

,Gunnar Kedenburg
Raphael Fonteneau
Remi Munos