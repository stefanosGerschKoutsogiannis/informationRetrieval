2012,Minimization of Continuous Bethe Approximations: A Positive Variation,We develop convergent minimization algorithms for Bethe variational approximations which explicitly constrain marginal estimates to families of valid distributions.  While existing message passing algorithms define fixed point iterations corresponding to stationary points of the Bethe free energy  their greedy dynamics do not distinguish between local minima and maxima  and can fail to converge. For continuous estimation problems  this instability is linked to the creation of invalid marginal estimates  such as Gaussians with negative variance. Conversely  our approach leverages multiplier methods with well-understood convergence properties  and uses bound projection methods to ensure that marginal approximations are valid at all iterations. We derive general algorithms for discrete and Gaussian pairwise Markov random fields  showing improvements over standard loopy belief propagation. We also apply our method to a hybrid model with both discrete and continuous variables  showing improvements over expectation propagation.,Minimization of Continuous Bethe Approximations:

A Positive Variation

Department of Computer Science  Brown University  Providence  RI

Jason L. Pacheco and Erik B. Sudderth
{pachecoj sudderth}@cs.brown.edu

Abstract

We develop convergent minimization algorithms for Bethe variational approxima-
tions which explicitly constrain marginal estimates to families of valid distribu-
tions. While existing message passing algorithms deﬁne ﬁxed point iterations cor-
responding to stationary points of the Bethe free energy  their greedy dynamics do
not distinguish between local minima and maxima  and can fail to converge. For
continuous estimation problems  this instability is linked to the creation of invalid
marginal estimates  such as Gaussians with negative variance. Conversely  our
approach leverages multiplier methods with well-understood convergence proper-
ties  and uses bound projection methods to ensure that marginal approximations
are valid at all iterations. We derive general algorithms for discrete and Gaussian
pairwise Markov random ﬁelds  showing improvements over standard loopy be-
lief propagation. We also apply our method to a hybrid model with both discrete
and continuous variables  showing improvements over expectation propagation.

1

Introduction

Variational inference algorithms pose probabilistic inference as an optimization over distributions.
Typically the optimization is formulated by minimizing an objective known as the Gibbs free en-
ergy [1]. Variational methods relax an otherwise intractable optimal inference problem by approx-
imating the entropy-based objective  and considering appropriately simpliﬁed families of approxi-
mating distributions [2]. Local message passing algorithms offer a computationally efﬁcient method
for extremizing variational free energies. Loopy belief propagation (LBP)  for example  optimizes
a relaxed objective known as the Bethe free energy [1  2]  which we review in Sec. 2. Expectation
propagation (EP) [3] is a generalization of LBP which shares the same objective  but optimizes over
a relaxed set of constraints [4] applicable to a broader family of continuous inference problems.
In general  neither LBP nor EP are guaranteed to converge. Even in simple continuous models  both
methods may improperly estimate invalid or degenerate marginal distributions  such as Gaussians
with negative variance. Such degeneracy typically occurs in classes of models for which conver-
gence properties are poor  and there is evidence that these problems are related [5  6] 
Extensive work has gone into developing algorithms which improve on LBP for models with discrete
variables  for example by bounding [7  8] or convexifying [9] the free energy objective. Gradient
optimization methods have been applied successfully to binary Ising models [10]  but when applied
to Gaussian models this approach suffers similar non-convergence and degeneracy issues as LBP.
Work on optimization of continuous variational free energies has primarily focused on addressing
convergence problems [11]. None of these approaches directly address degeneracy in the continuous
case  and computation may be prohibitively expensive for these direct minimization schemes.
By leveraging gradient projection methods from the extensive literature on constrained nonlinear
optimization  we develop an algorithm which ensures that marginal estimates remain valid and nor-
malizable at all iterations. In doing so  we account for important constraints which have been ignored

1

by previous variational derivations of the expectation propagation algorithm [12  6  11]. Moreover 
by adapting the method of multipliers [13]  we guarantee that our inference algorithm converges for
most models of practical interest.
We begin by introducing the Bethe variational problem (Sec. 2). We brieﬂy review the correspon-
dence between the Lagrangian formalism and message passing and discuss implicit normalizability
assumptions which  when violated  lead to degeneracy in message passing algorithms. We discuss
the method of multipliers  gradient projection  and convergence properties (Sec. 3). We then pro-
vide derivations (Sec. 4) for discrete MRFs  Gaussian MRFs  and hybrid models with potentials
deﬁned by discrete mixtures of Gaussian distributions. Experimental results in Sec. 5 demonstrate
substantial improvements over baseline message passing algorithms.

2 Bethe Variational Problems

For simplicity  we restrict our attention to pairwise Markov random ﬁelds (MRF) [2]  with graphs
G(V E) deﬁned by nodes V and undirected edges E. The joint distribution then factorizes as

(cid:89)

s∈V

p(x) =

1
Zp

(cid:89)

(s t)∈E

ϕs(xs)

ϕst(xs  xt)

(1)

for some non-negative potential functions ϕ(·). Often this distribution is a posterior given ﬁxed
observations y  but we suppress this dependence for notational simplicity. We are interested in
computing the log partition function log Zp  and/or the marginal distributions p(xs)  s ∈ V.
Let q(x; µ) denote an exponential family of densities with sufﬁcient statistics φ(x) ∈ Rd:

q(x; µ) ∝ exp{θT φ(x)}  µ = Eq[φ(x)].

(2)
To simplify subsequent algorithm development  we index distributions via their mean parameters µ.
We associate each node s ∈ V with an exponential family qs(xs; µs)  φs(x) ∈ Rds  and each edge
(s  t) ∈ E with a family qst(xs  xt; µst)  φst(x) ∈ Rdst. Because qs(xs; µs) is a valid probability
distribution  µs must lie in a set of realizable mean parameters  µs ∈ Ms. Similarly  µst ∈ Mst.
For example  Ms and Mst might require Gaussians to have positive semideﬁnite covariances.
We can express the log partition as the solution to an optimization problem 

− log Zp = min
µ∈M(G)

Eµ[− log p(x)] − H[µ] = min
µ∈M(G)

(3)
where H[µ] is the entropy of q(x; µ)  Eµ[·] denotes expectation with respect to q(x; µ)  and F(µ) is
known as the variational free energy. Mean parameters µ lie in the marginal polytope M(G) if and
only if there exists some valid  joint probability distribution with those moments.
Exactly characterizing M(G) may require exponentially many constraints  so we relax the optimiza-
tion to be over a set of locally consistent marginal distributions L(G)  which are properly normalized
and satisfy expectation constraints associated with each edge of the graph:

F(µ) 

Cs(µ) = 1 −

qs(xs; µs) dxs  Cts(µ) = µs − Eqst[φs(xs)].

(4)
This is a relaxation in the sense that M(G) ⊂ L(G) with strict equality if G does not contain cycles.
We approximate the entropy H[µ] with the entropy of a tree-structured distribution q(x; µ). Such an
approximation is tractable and consistent with L(G)  and yields the Bethe free energy:
F B(µ) =

Eqst [log qst(xs  xt; µst)− ψst(xs  xt)]−(cid:88)

(ns− 1) Eqs[log qs(xs; µs)− ϕs(xs)]

(cid:90)

M =(cid:83)

(5)
Here  ψst(·) = ϕst(·)ϕs(·)ϕt(·)  and the mean parameters µ are valid within the constraint set

st Mst. The resulting objective is the Bethe variational problem (BVP):

s Ms

(cid:88)
(cid:83)

(s t)∈E

s∈V

FB(µ)

minimize
subject to Cts(µ) = 0 ∀s ∈ V  t ∈ N (s)

µ

Cs(µ) = 0 ∀s ∈ V 
{µs : s ∈ V} ∪ {µst : (s  t) ∈ E} ∈ M.

(6)

Here  N (s) denotes the set of neighbors of node s ∈ V.

2

2.1 Correspondence to Message Passing

We can optimize the BVP (6) by relaxing the normalization and local consistency constraints with
Lagrange multipliers. Constrained minima are characterized by stationary points of the Lagrangian 

L(x  λ) = F B(q) +

λsCs +

λtsCts.

(7)

(cid:88)

s

(cid:88)

(cid:88)

s

t∈N (s)

Equivalence between LBP ﬁxed points and stationary points of the Lagrangian for the discrete case
have been discussed extensively [1  2]. Similar correspondence has been shown more generally for
EP ﬁxed points [2  4]. Since our focus is on the continuous case we brieﬂy review the correspon-
dence between Gaussian LBP ﬁxed points and the Gaussian Bethe free energy. For simplicity we
focus on zero-mean p(x) = N (x | 0  J−1)  where diagonal precision entries Jss = As and

(cid:27)

(cid:26)

− 1
2

x2
sAs

ϕs(xs) = exp

 

ϕst(xs  xt) = exp

Let q(xs) = N (xs | 0  Vs)  q(xs  xt) = N (( xs
The Gaussian Bethe free energy then equals

(cid:26)

− 1
2

(cid:19)(cid:18) xs

(cid:18) 0
(cid:19)(cid:27)
xt ) | 0  Σst)  Σst =(cid:0) Vts Pst
(cid:1)  and (cid:101)Bst =(cid:0) As Jst
(cid:19)
(cid:18) ns − 1

(xs xt)

Jst
0

Pts Vst

Jst At

Jst

xt

(VsAs − log Vs) .

.

(cid:1).

(8)

2

FGB(V  Σ) =

1
2

(cid:88)

(s t)∈E

(cid:16)
tr(Σst(cid:101)Bst) − log |Σst|(cid:17) −(cid:88)
(cid:88)

(cid:88)

s∈V

s

t∈N (s)

The locally consistent marginal polytope L(G) consists of the constraints Cts(V ) = Vs − Vts for
all nodes s ∈ V and edges (s  t) ∈ E. The Lagrangian is given by 

L(V  Σ  λ) = FGB(V  Σ) +

λts [Vs − Vts] .

(9)

Taking the derivative with respect to the node marginal variance ∂ L
V −1
s = As + 1

= 0 yields the stationary point
t∈N (s) λts. For a Gaussian LBP algorithm with messages parametrized as
sΛt→s
2 x2

(cid:9)  ﬁxed points of the node marginal precision are given by

∂Vs

ns−1

(cid:80)
mt→s(xs) = exp(cid:8)− 1
(cid:80)

(cid:88)

t∈N (s)

Λs = As +

Λt→s

Let λts = − 1
a∈N (s)\t Λa→s. Substituting back into the stationary point conditions yields
s ⇒ Λs. A similar construction holds for the pairwise marginals. Inverting the correspondence
V −1
between multipliers and message parameters yields the converse V −1

s ⇐ Λs (c.f. [4]).

2

2.2 Message Passing Non-Convergence and Degeneracy

While local message passing algorithms are convenient for many applications  their convergence
is not guaranteed in general.
In particular  LBP often fails to converge for networks with tight
loops [1] such as the 3×3 lattice of Figure 1(a). For non-Gaussian models with continuous variables 
convergence of the EP algorithm can be even more problematic [11].
For continuous models message updates may yield degenerate  unnormalizable marginal distribu-
tions which do not correspond to stationary points of the Lagrangian. For example  for Gaussian
MRFs the Bethe free energy F B(·) in (5) is derived from expectations with respect to variational
distributions qs(xs; µs)  qst(xs  xt; µst). If a set of hypothesized marginals are not normalizable
(positive variance)  the Gaussian Bethe free energy F GB(·) is invalid and undeﬁned.
Degenerate marginals arise because the constraint set M is not represented in the Lagrangian (7);
this issue is mentioned brieﬂy in [2] but is not dealt with computationally. Figure 1(b) demonstrates
this issue for a simple  three-node Gaussian MRF. Here LBP produces marginal variances which
oscillate between impossibly large positive  and non-sensical negative  values. Such degeneracies
are arguably more problematic for EP since its moment matching steps require expected values with
respect to an augmented distribution [3]  which may involve an unbounded integral.

3

(a)

(b)

(c)

Figure 1: (a) Bethe free energy versus iteration for 3x3 toroidal binary MRF. (b) Node marginal variance
estimates per iteration for a symmetric  single-cycle Gaussian MRF with three nodes (plot is of x1  other nodes
are similar). (c) For the model from (b)  the Gaussian Bethe free energy is unbounded on the constraint set.

2.3 Unboundedness of the Gaussian Bethe Free Energy

Conditions under which the simple LBP and EP updates are guaranteed to be accurate are of great
practical interest. For Gaussian MRFs  the class of pairwise normalizable models are sufﬁcient to
guarantee LBP stability and convergence [5]. For non-pairwise normalizable models the Gaussian
Bethe free energy is unbounded below [6] on the set of local consistency constraints L(G).
We offer a small example consisting of a non-pairwise normalizable symmetric single cycle with 3
nodes. Diagonal precision elements are Jss = 1.0  and off-diagonal elements Jst = 0.6. We embed
marginalization constraints into a symmetric parametrization Vs = V and Σst =
. Feasi-
ble solutions within the constraint set are characterized by V > 0 and −1 < ρ < 1. Substituting
this parametrization into the Gaussian free energy (8)  and performing some simple algebra  yields

(cid:16) V ρV

(cid:17)

ρV V

FGB(V  ρ) = − 3
2

log V +

3
2

V (1 + 1.2ρ) − 3
2

log(1 − ρ2).

1.2 the free energy is unbounded below at rate O(−V ). Figure 1(c) illustrates the Bethe

For ρ < − 1
free energy for this model as a function of V   and for several values of ρ.
More generally  it has been shown that Gaussian EP messages are always normalizable (positive
variance) for models with log-concave potentials [14].
It has been conjectured  but not proven 
that EP is also guaranteed to converge for such models [15]. For Gaussian MRFs  we note that
the family of log-concave models coincides with the pairwise normalizability condition. Our work
seeks to improve inference for non-log-concave models with bounded Bethe free energies.

3 Method of Multipliers

Given our complete constrained formulation of the Bethe variational problem  we avoid convergence
and degeneracy problems via direct minimization using the method of multipliers (MoM) [13]. In
general terms  given some convex feasible region M  consider the equality constrained problem

With penalty parameter c > 0  we form the augmented Lagrangian function 

minimize

x∈M f (x)

subject to h(x) = 0

(10)
Given a multiplier vector λk and penalty parameter ck we update the primal and dual variables as 

Lc(x  λ) = f (x) + λT h(x) +

c||h(x)||2

1
2

xk = arg min

λk+1 = λk + ckh(xk).

The penalty multiplier can be updated as ck+1 ≥ ck according to some ﬁxed update schedule  or
based on the results of the optimization step. An update rule that we ﬁnd useful [13] is to increase
the penalty parameter by β > 1 if the constraint violation is not improved by a factor 0 < γ < 1
over the previous iteration 

x∈MLck (x  λk) 
(cid:26) βck

4

ck+1 =

ck

if (cid:107)h(xk)(cid:107) > γ(cid:107)h(xk−1)(cid:107) 
if (cid:107)h(xk)(cid:107) ≤ γ(cid:107)h(xk−1)(cid:107).

050100150200250300350400450100101102103104105Bethe Free EnergyIteration  Belief PropagationMoM0102030405060708090100−100−50050100150200250300Iteration #Variance of Node x1  TrueLBPMoM100101102103104−1000−800−600−400−20002004006008001000Variance (V)Gaussian Bethe Free Energy  ρ∈{−0.9 −0.87 −0.85 −0.7 −0.5}3.1 Gradient Projection Methods
The augmented Lagrangian Lc(x  λ) is a partial one  where feasibility of mean parameters (x ∈ M)
is enforced explicitly by projection. A simple gradient projection method [13] deﬁnes a sequence

xk+1 = xk + αk(¯xk − xk) 

¯xk = [xk − sk∇f (xk)]+ .

The notation [·]+ denotes a projection onto the constraint set M. After taking a step sk > 0 in the
direction of the negative gradient  we project the result onto the constraint set to obtain a feasible
direction ¯xk. We then compute xk+1 by taking a step αk ∈ (0  1] in the direction of (¯xk − xk). If
xk − sk∇f (xk) is feasible  gradient projection reduces to unconstrained steepest descent.
There are multiple such projection steps in each inner-loop iteration of MoM (e.g. each xk update).
For our experiments we use a projected quasi-Newton method [16] and step-sizes αk and sk are
chosen using an Armijo rule [13  Prop. 2.3.1].

3.2 Convergence of Multiplier Methods

xx Lc(x∗  λ∗)z > 0.

Convergence and rate of convergence results have been proven [17  Proposition 2.4] for the Method
of Multipliers with a quadratic penalty and multiplier iteration λk+1 = λk + ckh(xk). The main
regularity assumptions are that the sequence {λk} is bounded  and there is a local minimum for
which a Lagrange multiplier pair (x∗  λ∗) exists satisfying second-order sufﬁciency conditions  so
xx L0(x∗  λ∗)z > 0 for all z (cid:54)= 0. It then follows that there exists
that ∇x L0(x∗  λ∗) = 0 and zT∇2
some ¯c such that for all c ≥ ¯c  the augmented Lagrangian also contains a strict local minimum
zT∇2
For convergence  the initialization of the Lagrange multiplier λ0 and penalty parameter c0 must be
such that (cid:107)λ0 − λ∗(cid:107) < δc0 for some δ > 0 and c ≥ ¯c which depend on the objective and constraints.
In practice  a poor initialization of the multiplier λ0 can often be offset by a sufﬁciently high c0. A
ﬁnal technical note is that convergence proofs assume the sequence of unconstrained optimizations
which yield xk stays in the neighborhood of x∗ after some k. This does not hold in general  but can
be encouraged by warm-starting the unconstrained optimization with the previous xk−1.
To invoke existing convergence results we must show that a local minimum x∗ exists for each of
the free energies we consider; a sufﬁcient condition is then that the Bethe free energy is bounded
from below. This property has been previously established for general discrete MRFs [18]  for pair-
wise normalizable Gaussian MRFs [6]  and for the clutter model [3]. For non-pairwise normalizable
Gaussian MRFs  the example of Section 2.3 shows that the Bethe variational objective is unbounded
below  and further may not contain any local optima. While the method of multipliers does not con-
verge in this situation  its non-convergence is due to fundamental ﬂaws in the Bethe approximation.

4 MoM Algorithms for Probabilistic Inference

We derive MoM algorithms which minimize the Bethe free energy for three different families of
graphical models. For each model we deﬁne the form of the joint distribution  Bethe free energy (5) 
local consistency constraints  augmented Lagrangian  and the gradient projection step. Gradients 
which can be notationally cumbersome  are given in the supplemental material.

4.1 Gaussian Markov Random Fields

We have already introduced the Lagrangian (9) for the Gaussian MRF. The Gaussian Bethe free
energy (8) is always unbounded below off of the constraint set in node marginal variances Vs. We
correct this by adding an additional ﬁxed penalty in the augmented Lagrangian 

Lc(V  Σ  λ) = FGB(V ) +

(cid:88)

(cid:88)
(cid:88)

s

λts [Vs − Vts]

t∈N (s)
[log Vs − log Vts]2 +

(cid:88)

+

κ
2

s

t∈N (s)

(cid:88)

(cid:88)

s

t∈N (s)

c
2

[Vs − Vts]2 .

We keep κ ≥ 1 ﬁxed so that existing convergence theory remains applicable. The set of realizeable
mean parameters M is the set of symmetric positive semideﬁnite matrices Vs  Σst. We therefore

5

must solve a series of constrained optimizations of the form  minV Σ Lck (V  Σ  λk)  subject to Vs ≥
0  Σst (cid:23) 0. The gradient projection step is easily expressed in terms of correlation coefﬁcients ρst 

(cid:20)

Σst =

√
Vst
VstVts

ρst

ρst

VstVts
Vts

√

(cid:21)

.

Then  Σst (cid:23) 0 if and only if Vst ≥ 0  Vts ≥ 0  and −1 ≤ ρst ≤ 1. The projection step is then 

Vst = max(0  Vst) 

Vts = max(0  Vts) 

ρst = max(−1  min(1  ρst)).

The full MoM algorithm then follows from gradient derivations in the supplemental material.
Recall that in Section 2.3  we showed that the Gaussian Bethe free energy is unbounded on the
constraint set for non-pairwise normalizable models. We run MoM on the symmetric three-node
cycle from this discussion and ﬁnd that MoM  correctly  identiﬁes an unbounded direction  and
Figure 1(b) shows that the node marginal variances indeed diverge to inﬁnity.

4.2 Discrete Markov Random Fields
Consider a discrete MRF where all variables xs ∈ Xs = {1  . . .   Ks}. The variational marginal
I(xs k)  and have mean parameters τ ∈ RKs. Let
τ (xs) denote element xs of vector τ. Pairwise marginals have mean parameters τst ∈ RKs×Kt
similarly indexed as τst(xs  xt). The discrete Bethe free energy is then

k=1 τ (xs)

xs

xt

(s t)∈E

FB(τ ; ϕ) =

distributions are then qs(xs; τ ) = (cid:81)Ks
(cid:88)
(cid:88)
(cid:88)
−(cid:88)
(cid:88)
Cs(τ ) = 1 −(cid:88)
τs(xs)  Cts(xs; τ ) = τs(xs) −(cid:88)
(cid:34)(cid:88)
(cid:88)
(cid:88)
(cid:34)(cid:88)
(cid:88)

The augmented Lagrangian is then 
Lc(τ  λ  ξ; ϕ) = FB(τ ; ϕ) +

λts(xs)Cts(xs; τ ) +

(cid:88)

(cid:88)

(s t)∈E

s∈V

xs

xs

xs

xt

xt

+

ξssCs(τ ) +

Cs(τ )2 +

c
2

s∈V

c
2

(s t)∈E

xs

s∈V

τst(xs  xt)[log τst(xs  xt) − log φst(xs  xt)]

(ns − 1)τs(xs)[log τs(xs) − log ϕs(xs)].

For this discrete model  our expectation constraints reduce to the following normalization and
marginalization constraints:

τst(xs  xt).

λst(xt)Cst(xt; τ )

(cid:35)
(cid:88)

xt

(11)

(cid:35)

Cts(xs; τ )2 +

Cst(xt; τ )2

.

Mean parameters must be non-negative to be valid  so M = {τs  τst : τs ≥ 0  τst ≥ 0}. This
constraint is enforced by a bound projection τs(xs) = max(0  τs(xs))  and similarly for the pair-
wise marginals. While these constraints are never active in BP ﬁxed point iterations  they must be
enforced in gradient optimization. With these pieces and the gradient computations presented in the
supplement  implementation of MoM optimization for the discrete MRF is straightforward.

4.3 Discrete Mixtures of Gaussian Potentials

We are particularly interested in tractable inference in hybrid models with discrete and conditionally
Gaussian random variables. A simple example of such a model is the clutter problem [3]  whose
joint distribution models N conditionally independent Gaussian observations {yi}N
i=1. These obser-
vations may either be centered on a target scalar x ∈ R (zi = 1) or drawn from a background clutter
distribution (zi = 0). If target observations occur with frequency β0  we then have
0)(1−zi)N (x  σ2

yi | x  zi ∼ N (0  σ2

x ∼ N (µ0  P0) 

zi ∼ Ber(β0) 

1)zi

The corresponding variational posterior distributions are 

q0(x) = N (m0  V0) 

qi(x  zi) = ((1 − βi)N (x | mi0  Vi0))(1−zi) (βiN (x | mi1  Vi1))zi .

6

Assuming normalizable marginals with V0 ≥ 0  Vi0 ≥ 0  Vi1 ≥ 0  as always ensured by our
multiplier method  we deﬁne the Bethe free energy F CGB(m  V  β) in terms of the mean parameters
in the supplemental material. Expectation constraints are given by 

Cmean

i

= E0[x] − Ei[x]  Cvar

i = Var0[x] − Vari[x] 

where Ei[·] and Var i[·] denote the mean and variance of the Gaussian mixture qi(x  zi). Com-
bining the free energy  constraints  and additional positive semideﬁnite constraints on the marginal
variances we have the BVP for the clutter model 

minimize

m V β

FCGB(m  V  β; ϕ)

subject to Cmean

= 0  Cvar

i = 0  for all i = 1  2  . . .   N

i

V0 ≥ 0  Vi0 ≥ 0  Vi1 ≥ 0

(12)

Derivation of the free energy and augmented Lagrangian is somewhat lengthy  and so is deferred to
the supplement. Projection of the variances onto the constraint set is a simple thresholding operation.

5 Experimental Results

5.1 Discrete Markov Random Fields

We consider binary Ising models  with variables arranged in NxN lattices with toroidal boundary
conditions. Potentials are parametrized as in [19]  so that

(cid:20) exp(hs)

exp(−hs)

(cid:21)

ψs =

  ψst =

(cid:20) exp(Jst)

exp(−Jst)

(cid:21)

.

exp(−Jst)
exp(Jst)

We sample 500 instances at random from a 10x10 toroidal lattice with each Jst ∼ N (0  1) and
hs ∼ N (0  0.01). LBP is run for a maximum of 1000 iterations  and MoM is initialized with a
single iteration of LBP. We report average L1 error of the approximate marginals as compared to
the true marginals computed with the junction tree algorithm [20]. Marginal errors are reported in
Figure 2(a top)  and there is a clear improvement over LBP in the majority of cases.
Direct evaluation of the Bethe free energy does not take into account constraint violations for non-
convergent LBP runs. The augmented Lagrangian penalizes constraint violation  but requires a
penalty parameter which LBP does not provide. For an objective comparison  we construct a pe-
nalized Bethe free energy by evaluating the augmented Lagrangian with ﬁxed penalty c = 1 and
multipliers λ = 0. We evaluate this objective at the ﬁnal iteration of both algorithms. As we see in
Figure 2(a bottom)  MoM ﬁnds a lower free energy for most trials.
Our implementations of LBP and MoM are in Matlab  and emphasize correctness over efﬁciency.
Nevertheless  computation time for LBP exceeds that of MoM. Wall clock time is measured in
seconds across various trials  and the percentiles for LBP are 25%: 1040.46  50%: 1042.57  and
75%: 1044.85. For MoM they are 25%: 290.25  50%: 381.62  and 75%: 454.52.

5.2 Gaussian Markov Random Fields

For the Gaussian case we again sample 500 random instances from a 10x10 lattice with toroidal
boundary conditions. We randomly sample only pairwise normalizable instances and initialization
is provided with a single iteration of Gaussian LBP. We ﬁnd that MoM is generally insensitive to
initialization in this model. True marginals are computed by explicitly inverting the model precision
matrix and average symmetric L1 error with respect to truth is reported in Figure 2(b top).
For pairwise normalizable models  Gaussian LBP is guaranteed to converge to the unique ﬁxed point
of the Bethe free energy  so it is reassuring that MoM optimization matches LBP performance. The
value of the augmented Lagrangian at the ﬁnal iteration is shown in Figure 2(b bottom) and again
shows that MoM matches Gaussian LBP on pairwise normalizable models. Computation time for
MoM is slightly faster with median wall clock time of 58.76 seconds as compared to 103.17 seconds
for LBP. The 25% and 75% percentiles are 37.81 and 92.10 seconds for MoM compared to 88.40
and 125.59 seconds for LBP.

7

(a)

(b)

Figure 2: Performance of MoM and LBP on randomly generated (a) discrete 10 × 10 toroidal Ising MRFs 
(b) 10× 10 toroidal Gaussian MRfs  and (c) clutter models with N = 30 observations. Each point corresponds
to a single model instance. Top: L1 error between estimated and true marginal distributions  averaged over all
nodes. Bottom: Penalized Bethe free energy constructed by setting λ = 0  c = 1 in the augmented Lagrangian.

(c)

5.3 Discrete Mixtures of Gaussian Potentials

To test the beneﬁts of avoiding degenerate marginals  we consider the clutter model of Sec. 4.3 with
µ0 = 0  P0 = 100 and β0 = 0.25. The variance of the clutter distribution is σ2
0 = 10  and of the
target distribution σ2
1 = 1. We sample N = 30 observations for each trial instance.
A good initialization of the multipliers is critical to performance of MoM. We generate 10 initializa-
tions by running 5 iterations of EP  each with a different random message update schedule  compute
the corresponding Lagrange multipliers for each  and use the one with the lowest value of the aug-
mented Lagrangian. Similarly  we measure EP’s performance by the best performing of 10 longer
runs. Both methods are run for a maximum of 1000 iterations  and true marginals are computed
numerically by ﬁnely discretizing the scalar target x.
We sample 500 random instances and report average L1 error with respect to true marginals in
Figure 2(c top). We see a signiﬁcant improvement in the majority of runs. Similarly  the augmented
Lagrangian comparison is shown in Figure 2(c bottom) and MoM often ﬁnds a better penalized free
energy. While MoM and EP can both suffer from local optima  MoM avoids non-convergence and
the output of invalid (negative variance) marginal distributions. Median wall clock time for EP is
0.59 seconds  and 9.80 seconds for MoM. The 25% and 75% percentiles are 0.42 and 0.84 seconds
for EP and 0.51 and 49.19 seconds for MoM.

6 Discussion

We have proposed an approach for directly minimizing the Bethe variational problem motivated by
successful methods in nonlinear programming. Our approach is unique in that we do not relax the
constraint on normalizability of the marginals  rather we explicitly enforce it at all points in the op-
timization. This method directly avoids the creation of degenerate distributions — for example with
negative variance — which frequently occur in more greedy approaches for minimizing the Bethe
free energy. In addition we obtain convergence guarantees under broadly applicable assumptions.

8

00.10.20.30.40.50.600.10.20.30.40.5Belief PropagationMethod of MultipliersAvg. Marginal Error0.40.420.440.460.480.50.40.410.420.430.440.450.460.470.480.490.5Belief PropagationMethod of MultipliersAvg. Marginal Error00.20.40.60.8100.10.20.30.40.50.60.70.80.91Expectation PropagationMethod of MultipliersAvg. Marginal Error−180−170−160−150−140−130−120−180−170−160−150−140−130−120Belief PropagationMethod of MultipliersAugmented Lagrange−20020406080100120−20020406080100120Belief PropagationMethod of MultipliersAugmented Lagrange10010210410610−1100101102103104105106107Expectation PropagationMethod of MultipliersAugmented LagrangeReferences
[1] J.S. Yedidia  W.T. Freeman  and Y. Weiss. Constructing free-energy approximations and gener-
alized Belief Propagation algorithms. Information Theory  IEEE Transactions on  51(7):2282–
2312  2005.

[2] M. J. Wainwright and M. I. Jordan. Graphical models  exponential families  and variational

inference. Technical report  UC Berkeley  Dept. of Statistics  2003.

[3] T. P. Minka. Expectation Propagation for approximate Bayesian inference. Uncertainty in

Artiﬁcial Intelligence  17:362–369  2001.

[4] Tom Heskes  Wim Wiegerinck  Ole Winther  and Onno Zoeter. Approximate inference tech-
niques with expectation constraints. Journal of Statistical Mechanics: Theory and Experiment 
page 11015  2005.

[5] Dmitry M. Malioutov  Jason K. Johnson  and Alan S. Willsky. Walk-sums and Belief Propa-
gation in Gaussian graphical models. Journal of Machine Learning Research  7:2031–2064 
2006.

[6] B. Cseke and T. Heskes. Properties of bethe free energies and message passing in Gaussian

models. Journal of Artiﬁcial Intelligence Research  41(2):1–24  2011.

[7] A. Yuille. CCCP algorithms to minimize the Bethe and Kikuchi free energies: Convergent

alternatives to Belief Propagation. Neural Computation  14:1691–1722  2002.

[8] B. Kappen T. Heskes  K. Albers. Approximate inference and constrained optimization. Un-

certainty in Artiﬁcial Intelligence  13:313–320  2003.

[9] Martin J. Wainwright  Tommi S. Jaakkola  and Alan S. Willsky. Tree-reweighted Belief Prop-
agation algorithms and approximate ML estimation by pseudo-moment matching. In In AIS-
TATS  2003.

[10] M. Welling and Y.W. Teh. Belief optimization for binary networks: A stable alternative to

Loopy Belief Propagation. In Uncertainty in Artiﬁcial Intelligence  2001.

[11] T. Heskes and O. Zoeter. Expectation Propagation for approximate inference in dynamic

Bayesian networks. Uncertainty in Artiﬁcial Intelligence  18:216–223  2002.

[12] T. Minka. The EP energy function and minimization schemes. Technical report  MIT Media

Lab  2001.

[13] D.P. Bertsekas. Nonlinear programming. Athena Scientiﬁc  1999.
[14] M. Seeger. Bayesian inference and optimal design for the sparse linear model. Journal of

Machine Learning Research  9:759–813  2008.

[15] C. Rasmussen. Gaussian Processes for Machine Learning. MIT Press  2006.
[16] M. Schmidt  E. Van Den Berg  M. Friedlander  and K. Murphy. Optimizing costly functions
with simple constraints: A limited-memory projected quasi-Newton algorithm. In AI & Statis-
tics  2009.

[17] D.P. Bertsekas. Constrained optimization and Lagrange multiplier methods. Computer Science

and Applied Mathematics  Boston: Academic Press  1982  1  1982.

[18] T. Heskes. On the uniqueness of loopy belief propagation ﬁxed points. Neural Computation 

16(11):2379–2413  2004.

[19] J.S. Yedidia  W.T. Freeman  and Y. Weiss. Generalized Belief Propagation. Advances in neural

information processing systems  pages 689–695  2001.

[20] Joris M. Mooij. libDAI: A free and open source C++ library for discrete approximate inference

in graphical models. Journal of Machine Learning Research  11:2169–2173  August 2010.

9

,Rashish Tandon
Karthikeyan Shanmugam
Pradeep Ravikumar
Alexandros Dimakis
Zhongwen Xu
Hado van Hasselt
David Silver