2018,Inference in Deep Gaussian Processes using Stochastic Gradient Hamiltonian Monte Carlo,Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Processes that combine well calibrated uncertainty estimates with the high flexibility of multilayer models. One of the biggest challenges with these models is that exact inference is intractable. The current state-of-the-art inference method  Variational Inference (VI)  employs a Gaussian approximation to the posterior distribution. This can be a potentially poor unimodal approximation of the generally multimodal posterior. In this work  we provide evidence for the non-Gaussian nature of the posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method to generate samples. To efficiently optimize the hyperparameters  we introduce the Moving Window MCEM algorithm. This results in significantly better predictions at a lower computational cost than its VI counterpart. Thus our method establishes a new state-of-the-art for inference in DGPs.,Inference in Deep Gaussian Processes using
Stochastic Gradient Hamiltonian Monte Carlo

Marton Havasi

Department of Engineering
University of Cambridge

mh740@cam.ac.uk

Jos´e Miguel Hern´andez-Lobato

Department of Engineering
University of Cambridge 

Microsoft Research 
Alan Turing Institute
jmh233@cam.ac.uk

Juan Jos´e Murillo-Fuentes

Department of Signal Theory and Communications

University of Sevilla

murillo@us.es

Abstract

Deep Gaussian Processes (DGPs) are hierarchical generalizations of Gaussian Pro-
cesses that combine well calibrated uncertainty estimates with the high ﬂexibility
of multilayer models. One of the biggest challenges with these models is that exact
inference is intractable. The current state-of-the-art inference method  Variational
Inference (VI)  employs a Gaussian approximation to the posterior distribution.
This can be a potentially poor unimodal approximation of the generally multimodal
posterior. In this work  we provide evidence for the non-Gaussian nature of the
posterior and we apply the Stochastic Gradient Hamiltonian Monte Carlo method
to generate samples. To efﬁciently optimize the hyperparameters  we introduce the
Moving Window MCEM algorithm. This results in signiﬁcantly better predictions
at a lower computational cost than its VI counterpart. Thus our method establishes
a new state-of-the-art for inference in DGPs.

1

Introduction

Deep Gaussian Processes (DGP) [Damianou and Lawrence  2013] are multilayer predictive models
that are highly ﬂexible and can accurately model uncertainty. In particular  they have been shown to
perform well on a multitude of supervised regression tasks ranging from small (∼500 datapoints) to
large datasets (∼500 000 datapoints) [Salimbeni and Deisenroth  2017  Bui et al.  2016  Cutajar et al. 
2016]. Their main beneﬁt over neural networks is that they are capable of capturing uncertainty in
their predictions. This makes them good candidates for tasks where the prediction uncertainty plays a
crucial role  for example  black-box Bayesian Optimization problems and a variety of safety-critical
applications such as autonomous vehicles and medical diagnostics.
Deep Gaussian Processes introduce a multilayer hierarchy to Gaussian Processes (GP) [Williams and
Rasmussen  1996]. A GP is a non-parametric model that assumes a jointly Gaussian distribution for
any ﬁnite set of inputs. The covariance of any pair of inputs is determined by the covariance function.
GPs can be a robust choice due to being non-parametric and analytically computable  however  one
issue is that choosing the covariance function often requires hand tuning and expert knowledge of
the dataset  which is not possible without prior knowledge of the problem at hand. In a multilayer
hierarchy  the hidden layers overcome this limitation by stretching and warping the input space 

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montr´eal  Canada.

Figure 1: (Left): Deep Gaussian Process illustration1. (Middle): Histograms of a random selection of
inducing outputs. The best-ﬁt Gaussian distribution is denoted with a dashed line. Some of them
exhibit a clear multimodal behaviour. (Right): P-values for 100 randomly selected inducing outputs
per dataset. The null hypotheses are that their distributions are Gaussian.

resulting in a Bayesian ‘self-tuning’ covariance function that ﬁts the data without any human input
[Damianou  2015].
The deep hierarchical generalization of GPs is done in a fully connected  feed-forward manner. The
outputs of the previous layer serve as an input to the next. However  a signiﬁcant difference from
neural networks is that the layer outputs are probabilistic rather than exact values so the uncertainty is
propagated through the network. The left part of Figure 1 illustrates the concept with a single hidden
layer. The input to the hidden layer is the input data x and the output of the hidden layer f1 serves as
the input data to the output layer  which itself is formed by GPs.
Exact inference is infeasible in GPs for large datasets due to the high computational cost of working
with the inverse covariance matrix. Instead  the posterior is approximated using a small set of pseudo
datapoints (∼100) also referred to as inducing points [Snelson and Ghahramani  2006  Titsias  2009 
Qui˜nonero-Candela and Rasmussen  2005]. We assume this inducing point framework throughout
the paper. Predictions are made using the inducing points to avoid computing the covariance matrix
of the whole dataset. Both in GPs and DGPs  the inducing outputs are treated as latent variables that
need to be marginalized.
The current state-of-the-art inference method in DGPs is Doubly Stochastic Variation Inference
(DSVI) [Salimbeni and Deisenroth  2017] which has been shown to outperform Expectation Prop-
agation [Minka  2001  Bui et al.  2016] and it also has better performance than Bayesian Neural
Networks with Probabilistic Backpropagation [Hern´andez-Lobato and Adams  2015] and Bayesian
Neural Networks with earlier inference methods such as Variation Inference [Graves  2011]  Stochas-
tic Gradient Langevin Dynamics [Welling and Teh  2011] and Hybrid Monte Carlo [Neal  1993].
However  a drawback of DSVI is that it approximates the posterior distribution with a Gaussian. We
show  with high conﬁdence  that the posterior distribution is non-Gaussian for every dataset that
we examine in this work. This ﬁnding motivates the use of inference methods with a more ﬂexible
posterior approximations.
In this work  we apply an inference method new to DGPs  Stochastic Gradient Hamiltonian Monte
Carlo (SGHMC)  a sampling method that accurately and efﬁciently captures the posterior distribution.
In order to apply a sampling-based inference method to DGPs  we have to tackle the problem of
optimizing the large number of hyperparameters. To address this problem  we propose Moving
Window Monte Carlo Expectation Maximization  a novel method for obtaining the Maximum
Likelihood (ML) estimate of the hyperparameters. This method is fast  efﬁcient and generally
applicable to any probabilistic model and MCMC sampler.
One might expect a sampling method such as SGHMC to be more computationally intensive than a
variational method such as DSVI. However  in DGPs  sampling from the posterior is inexpensive 
since it does not require the recomputation of the inverse covariance matrix  which only depends on

1Image source: Daniel Hern´andez-Lobato

2

f1xybostonenergyconcretewine_redkin8nmpowernavalproteinyear103010261022101810141010106102p-value128294=105the hyperparameters. Furthermore  calculating the layerwise variance has a higher cost in the VI
setting.
Lastly  we conduct experiments on a variety of supervised regression and classiﬁcation tasks. We
show empirically that our work signiﬁcantly improves predictions on medium-large datasets at a
lower computational cost.
Our contributions can be summarized in three points.

1. Demonstrating the non-Gaussianity of the posterior. We provide evidence that every regres-

sion dataset that we examine in this work has a non-Gaussian posterior.

2. We use SGHMC to directly sample from the posterior distribution of a DGP. Experiments

show that this new inference method outperforms preceding works.

3. We introduce Moving Window MCEM  a novel algorithm for efﬁciently optimizing the

hyperparameters when using a MCMC sampler for inference.

2 Background and Related Work

This section provides the background on Gaussian Processes and Deep Gaussian Processes for
regression and establishes the notation used in the paper.

2.1 Single Layer Gaussian Process
Gaussian processes deﬁne a posterior distribution over functions f : RD → R given a set of
input-output pairs x = {x1  . . .   xN} and y = {y1  . . .   yN} respectively. Under the GP model 
it is assumed that the function values f = f (x)  where f (x) denotes {f (x1)  . . .   f (xN )}  are
jointly Gaussian with a ﬁxed covariance function k : RD × RD → R. The conditional distribution
of y is obtained via the likelihood function p(y|f ). A commonly used likelihood function is
p(y|f ) = N (y|f   Iσ2) (constant Gaussian noise).
The computational cost of exact inference is O(N 3)  rendering it computationally infeasible for
large datasets. A common approach uses a set of pseudo datapoints Z = {z1  . . . zM}  u = f (Z)
[Snelson and Ghahramani  2006  Titsias  2009] and writes the joint probability density function as

p(y  f   u) = p(y|f )p(f|u)p(u) .

The distribution of f given the inducing outputs u can be expressed as p(f|u) = N (µ  Σ) with

µ = KxZK−1
Σ = Kxx − KxZK−1

ZZu

ZZK T

xZ

where the notation KAB refers to the covariance matrix between two sets of points A  B with entries
[KAB]ij = k(Ai  Bj) where Ai and Bj are the i-th and j-th elements of A and B respectively.
In order to obtain the posterior of f  u must be marginalized  yielding the equation

(cid:90)

p(f|y) =

p(f|u)p(u|y)du .

Note that f is conditionally independent of y given u.
For single layer GPs  Variational Inference (VI) can be used for marginalization. VI approximates the
joint posterior distribution p(f   u|y) with the variational posterior q(f   u) = p(f|u)q(u)  where
q(u) = N (u|m  S).

This choice of q(u) allows for exact inference of the marginal q(f|m  S) =(cid:82) p(f|u)q(u)du =

N (f|˜µ  ˜Σ)

where ˜µ = KxZK−1

ZZm  

˜Σ = Kxx − KxZK−1

ZZ(KZZ − S)K−1

ZZK T

xZ .

(1)

The variational parameters m and S need to be optimized. This is done by minimizing the Kullback-
Leibler divergence of the true and the approximate posteriors  which is equivalent to maximizing a
lower bound to the marginal likelihood (Evidence Lower Bound or ELBO):
log p(y) ≥ Eq(f  u)

(cid:2) log p(y  f   u) − log q(f   u)(cid:3) = Eq(f|m S)

(cid:2) log p(y|f )(cid:3) − KL(cid:2)q(u)||p(u)(cid:3) .

3

2.2 Deep Gaussian Process

In a DGP of depth L  each layer is a GP that models a function fl with input fl−1 and output fl for
l = 1  . . .   L (f0 = x) as illustrated in the left part of Figure 1. The inducing inputs for the layers are
denoted by Z1  . . .  ZL with associated inducing outputs u1 = f1(Z1)  . . .  uL = fL(ZL).
The joint probability density function can be written analogously to the GP model case:

p(cid:0)y {fl}L

l=1 {ul}L

l=1

(cid:1) = p(y|fL)

L(cid:89)

p(fl|ul)p(ul) .

(2)

Inference

2.3
The goal of inference is to marginalize the inducing outputs {ul}L
l=1 and
approximate the marginal likelihood p(y). This section discusses prior works regarding inference.

l=1 and layer outputs {fl}L

l=1

Doubly Stochastic Variation Inference DSVI is an extension of Variational Inference to DGPs
[Salimbeni and Deisenroth  2017] that approximates the posterior of the inducing outputs ul with
independent multivariate Gaussians q(ul) = N (ul|ml  Sl).
The layer outputs naturally follow the single layer model in Eq. 1:

(cid:90) L(cid:89)

q(fl|fl−1) = N (fl|˜µl  ˜Σl)  
q(fl|fl−1)dfl . . . dfL−1 .

q(fL) =

l=1

The ﬁrst term in the resulting ELBO  L = Eq(fL)
estimated by sampling the layer outputs through minibatches to allow scaling to large datasets.

(cid:2) log p(y|fL)(cid:3) −(cid:80)L

l=1 KL(cid:2)q(ul)||p(ul)(cid:3)  is then

Sampling-based inference for Gaussian Processes
In a related work  Hensman et al. [2015] use
Hybrid MC sampling in single layer GPs. They consider joint sampling of the GP hyperparameters
and the inducing outputs. This work cannot straightforwardly be extended to DGPs because of
the high cost of sampling the GP hyperparameters. Moreover  it uses a costly method  Bayesian
Optimization  to tune the parameters of the sampler which further limits its applicability to DGPs.

3 Analysis of the Deep Gaussian Process Posterior

Adopting a new inference method over variational inference is motivated by the restrictive form that
VI assumes about the posterior distribution. The variational assumption is that p({ul}L
l=1|y) takes
the form of a multivariate Gaussian that assumes independence between the layers. While in a single
layer model  a Gaussian approximation to the posterior is provably correct [Williams and Rasmussen 
1996]  this is not the case for DGPs.
First  we illustrate with a toy problem that the posterior distribution in DGPs can be multimodal.
Following that  we provide evidence that every regression dataset that we consider in this work results
in a non-Gaussian posterior distribution.

Multimodal toy problem The multimodality of the posterior of a two layer DGP (L = 2) is
demonstrated on a toy problem (Table 1). For the purpose of the demonstration  we made the
simplifying assumption that σ2 = 0  so the likelihood function has no noise. This toy problem has
two Maximum-A-Posteriori (MAP) solutions (Mode A and Mode B). The table shows the variational
posterior at each layer for DSVI. We can see that it ﬁts one of the modes randomly (depending on the
initialization) while completely ignoring the other. On the other hand  a sampling method such as
SGHMC (as implemented in the following section) explores both of the modes and therefore provides
a better approximation to the posterior.

Empirical evidence To further support our claim regarding the multimodality of the posterior  we
give empirical evidence that  for real-world datasets  the posterior is not Gaussian.

4

Table 1: The layer inputs and outputs of a two layer DGP. Under DSVI  we show the mean and the
standard deviation of the variational distribution. In the case of SGHMC  samples from each layer
are shown. The two MAP solutions are shown under Mode A and Mode B.

Toy Problem

DSVI

SGHMC

Mode A

Mode B

Layer 1

Layer 2

Figure 2: The toy prob-
lem with 7 datapoints.

We conduct the following analysis. Consider the null hypothesis that the posterior under a dataset is a
multivariate Gaussian distribution. This null hypothesis implies that the distribution of each inducing
output is a Gaussian. We examine the approximate posterior samples generated by SGHMC for
each inducing output  using the implementation of SGHMC for DGPs described in the next section.
In order to derive p-values  we apply the kurtosis test for Gaussianity [Cramer  1998]. This test is
commonly used to identify multimodal distributions because these often have signiﬁcantly higher
kurtosis (also called 4th moment).
For each dataset  we calculate the p-values of 100 randomly selected inducing outputs and compare
the results against the probability threshold α = 10−5. The Bonferroni correction was applied to α
to account for the high number of concurrent hypothesis tests. The results are displayed in the right
part of Figure 1. Since every single dataset had p-values under the threshold  we can state with 99%
certainty that all of these datasets have a non-Gaussian posterior.

4 Sampling-based Inference for Deep Gaussian Processes

Unlike with VI  when using sampling methods  we do not have access to an approximate posterior
distribution q(u) to generate predictions with. Instead  we have to rely on approximate samples
generated from the posterior which in turn can be used to make predictions [Dunlop et al.  2017 
Hoffman  2017].
In practice  run a sampling process which has two phases. The burn-in phase is used to determine
the hyperparameters of the model and the sampler. The hyperparameters of the sampler are selected
using a heuristic auto-tuning approach  while the hyperparameters of the DGP are optimized using
the novel Moving Window MCEM algorithm.
In the sampling phase  the sampler is run using the ﬁxed hyperparameters. Since consecutive samples
are highly correlated  we save one sample every 50 iterations and generate 200 samples for prediction.
Once the posterior samples are obtained  predictions can be made by combining the per-sample
predictions to obtain a mixture distribution. Note that it is not more expensive to make predictions
using this sampler than in DSVI since DSVI needs to sample the layer outputs to make predictions.

4.1 Stochastic Gradient Hamiltonian Monte Carlo

SGHMC [Chen et al.  2014] is a Markov Chain Monte Carlo sampling method [Neal  1993] for
producing samples from the intractable posterior distribution of the inducing outputs p(u|y) purely
from stochastic gradient estimates.
With the introduction of an auxiliary variable  r  the sampling procedure provides samples from
the joint distribution p(u  r|y). The equations that describe the MCMC process can be related to

5

1.51.00.50.00.51.01.5x0.60.40.20.00.20.40.6y3210123x2.01.51.00.50.00.51.01.52.0f13210123x2.01.51.00.50.00.51.01.52.0f13210123x2.01.51.00.50.00.51.01.52.0f13210123x2.01.51.00.50.00.51.01.52.0f13210123f11.00.50.00.51.0y3210123f11.00.50.00.51.0y3210123f11.00.50.00.51.0y3210123f11.00.50.00.51.0yAlgorithm 1: Moving Window MCEM
initialize(θ);
initialize(u);
initialize(samples [1··· m]);
for i ← 0 to maxiter do
u(cid:48) ← randomElement(samples);
stepSGD( ∂p(y u(cid:48)|x θ))
u ∼ p(u|y  x  θ));
push front(samples  u);
pop back (samples);

);

∂θ

end
Figure 3: (Left): Pseudocode for Moving Window MCEM. (Middle): Comparison of predictive
performance of Moving Window MCEM and MCEM algorithms. Vertical lines denote E-steps in
MCEM algorithm. Higher is better. (Right): Comparison of the convergence of the different inference
methods. Higher is better.

Hamiltonian dynamics [Brooks et al.  2011  Neal  1993]. The negative log-posterior U (u) acts as the
potential energy and r plays the role of the kinetic energy:

p(u  r|y) ∝ exp(cid:0) − U (u) − 1

rT M−1r(cid:1)  

U (u) = − log p(u|y) .

2

In HMC the exact description of motion requires the computation of the gradient ∇U (u) in each
update step  which is impractical for large datasets because of the high cost of integrating the layer
outputs out in Eq. 2. This integral can be approximated by a lower bound that can be evaluated by
Monte Carlo sampling [Salimbeni and Deisenroth  2017]:

(cid:90)

(cid:20) p(y  f   u)

(cid:21)

p(f|u)

(cid:20) p(y  f i  u)

(cid:21)

p(f i|u)

 

log p(u  y) = log

p(y  f   u)df ≥

log

p(f|u)df ≈ log

(cid:90)

p(f|u) =(cid:81)L

where f i is a Monte Carlo sample from the predictive distribution of the layer outputs: f i ∼

l=1 p(fl|ul  fl−1). This leads to the estimate

log p(u  y) ≈ log[p(y|f i  u)p(u)] = log p(y|f i  u) + log p(u)  

that we can use to approximate the gradient since ∇U (u) = −∇ log p(u|y) = −∇ log p(u  y).
Chen et al. [2014] show that approximate posterior sampling is still possible with stochastic gradient
estimates (obtained by subsampling the data) if the following update equations are used:

∆u = M−1r  

∆r = −∇U (u) − CM−1r + N(cid:0)0  2(C − ˆB)(cid:1)  

where C is the friction term  M is the mass matrix  ˆB is the Fisher information matrix and  is the
step-size.
One caveat of SGHMC is that it has multiple parameters (C  M  ˆB  ) that can be difﬁcult to set
without prior knowledge of the model and the data. We use the auto-tuning approach of Springenberg
et al. [2016] to set these parameters which has been shown to work well for Bayesian Neural Networks
(BNN). The similar nature of DGPs and BNNs strongly suggests that the same methodology is
applicable to DGPs.

4.2 Moving Window Markov Chain Expectation Maximization

Optimizing the hyperparameters θ (parameters of the covariance function  inducing inputs and
parameters of the likelihood function) prove difﬁcult for MCMC methods [Turner and Sahani 
2011]. The naive approach consisting in optimizing them as the sampler progresses fails because

6

0100200300Runtime (sec)0.00.20.40.60.81.01.2MLLMoving Window MCEMMCEM0500100015002000Runtime (sec)2.852.802.752.702.652.60MLL20 000 iterations SGHMC DGP 3Dec DGP 3DGP 3subsequent samples are highly correlated and as a result  the hyperparameters simply ﬁt this moving 
point-estimate of the posterior.
Monte Carlo Expectation Maximization (MCEM) [Wei and Tanner  1990] is the natural extension of
the Expectation Maximization algorithm that works with posterior samples to obtain the Maximum
Likelihood estimate of the hyperparameters. MCEM alternates between two steps. The E-step
samples from the posterior and the M-step maximizes the average log joint probability of the samples
and the data:
E-step:

M-step:

θ = arg max

Q(θ)  

θ

u1...m ∼ p(u|y  x  θ) .

(cid:80)m
i=1 log p(y  ui|x  θ).

where Q(θ) = 1
m
However  there is a signiﬁcant drawback to MCEM: If the number of samples m used in the M-step
is too low then there is a risk of the hyperparameters overﬁtting to those samples. On the other hand 
if m is too high  the M-step becomes too expensive to compute. Furthermore  in the M-step  θ is
maximized via gradient ascent  which means that the computational cost increases linearly with m.
To address this  we introduce a novel extension of MCEM called Moving Window MCEM. Our
method optimizes the hyperparameters at the same cost as the previously described naive approach
while avoiding its overﬁtting issues.
The idea behind Moving Window MCEM is to intertwine the E and M steps. Instead of generating
new samples and then maximizing Q(θ) until convergence  we maintain a set of samples and take
small steps towards the maximum of Q(θ). In the E-step  we generate one new sample and add
it to the set while discarding the oldest sample (hence Moving Window). This is followed by the
M-step  in which we take a random sample from the set and use it to take an approximate gradient
step towards the maximum of Q(θ). Algorithm 1 on the left side of Figure 3 presents the pseudocode
for Moving Window MCEM.
There are two advantages over MCEM. Firstly  the cost of each update of the hyperparameters is
constant and does not scale with m since it only requires a single sample. Secondly  Moving Window
MCEM converges faster than MCEM. The middle plot of Figure 3 demonstrates this. MCEM
iteratively ﬁts the hyperparameters for a speciﬁc set of posterior samples. Since hyperparameters and
posterior samples are highly coupled  this alternating update scheme converges slowly [Neath et al. 
2013]. To mitigate this problem  Moving Window MCEM continuously updates its population of
samples by generating a new sample after each gradient step.
To produce the plot in the center of Figure 3  we plotted the predictive log-likelihood on the test set
against the number of iterations of the algorithm to demonstrate the superior performance of Moving
Window MCEM over MCEM. For MCEM  we used a set size of m = 10 (larger m would slow down
the method) which we generated over 500 MCMC steps. For Moving Window MCEM  we used a
window size of m = 300. The model used in this experiment is a DGP with one hidden layer trained
on the kin8nm dataset.

5 Decoupled Deep Gaussian Processes

This section describes an extension to DGPs that enables using a large number of inducing points
without signiﬁcantly impacting performance. This method is only applicable in the case of DSVI  so
we considered it as a baseline model in our experiments.
Using the dual formulation of a GP as a Gaussian measure  it has been shown that it does not
necessarily have to be the case that ˜µ and ˜Σ (Eq. 1) are parameterized by the same set of inducing
points [Cheng and Boots  2017  2016]. In the case of DGPs  this means that one can use two separate
sets of inducing points. One set to compute the layerwise mean and one set to compute the layerwise
variance.
In the variational inference setting  computing the layerwise variance has a signiﬁcantly higher cost
than computing the layerwise mean. This means that a larger set of inducing points can be used to
compute the layerwise mean and a smaller set of inducing points to compute the layerwise variance
to improve the predictive performance without impacting the computational cost.

7

Figure 4: Log-likelihood and standard deviation for each method on the UCI datasets. Ranking
Summary: average rank and standard deviation. Right is better. Best viewed in colour.

Unfortunately  the parameterization advocated by Cheng and Boots [2017] has poor convergence
properties. The dependencies in the ELBO result in a highly non-convex optimization problem  which
then leads to high variance gradients. To combat this problem  we used a different parameterization
that lifts the dependencies and achieves stable convergence. Further details on these issues can be
found in the supplementary material.

6 Experiments
We conducted experiments2 on 9 UCI benchmark datasets ranging from small (∼500 datapoints) to
large (∼500 000) for a fair comparison against the baseline. In each regression task  we measured the
average test Log-Likelihood (MLL) and compared the results. Figure 4 shows the MLL values and
their standard deviation over 10 repetitions.
Following Salimbeni and Deisenroth [2017]  in all of the models  we set the learning rate to the
default 0.01  the minibatch size to 10 000 and the number of iterations to 20 000. One iteration
involves drawing a sample from the window and updating the hyperparameters by gradient descent
as illustrated in Algorithm 1 in the left side of Figure 3. The depth varied from 0 hidden layers up
to 4 with 10 nodes per layer. The covariance function was a standard squared exponential function
with separate lengthscales per dimension. We exercised a random 0.8-0.2 train-test split. In the year
dataset  we used a ﬁxed train-test split to avoid the ‘producer effect’ by making sure no song from a
given artist ended up in both the train and test sets.
Baselines: The main baselines for our experiments were the Doubly Stochastic DGPs. For a faithful
comparison  we used the same parameters as in the original paper. In terms of the number of inducing
points (the inducing inputs are always shared across the latent dimensions)  we tested two variants.
First  the original  coupled version with M = 100 inducing points per layer (DGP). Secondly  a
decoupled version (Dec DGP) with Ma = 300 points for the mean and Mb = 50 for the variance.
2Our code is based on the Tensorﬂow [Abadi et al.  2015] computing library and it is publicly available at

https://github.com/cambridge-mlg/sghmc_dgp.

8

3.02.52.0BNNSGPDec SGPDGP 2DGP 3DGP 4DGP 5Dec DGP 2Dec DGP 3Dec DGP 4Dec DGP 5SGHMC DGP 2SGHMC DGP 3SGHMC DGP 4SGHMC DGP 5bostonD=13 N=5061.251.000.75wine_redD=11 N=1 5993.503.253.002.75concreteD=8 N=1 0301.001.25kin8nmD=8 N=8 1922.752.50proteinD=9 N=45 7303.63.4BNNSGPDec SGPDGP 2DGP 3DGP 4DGP 5Dec DGP 2Dec DGP 3Dec DGP 4Dec DGP 5SGHMC DGP 2SGHMC DGP 3SGHMC DGP 4SGHMC DGP 5yearD=90 N=515 34546navalD=17 N=11 9342.82.72.6powerD=4 N=9 56810energyD=8 N=7681st5th10th15thRankingSummaryThis workDecoupled VI DGPDoubly Stochastic VI DGPSingle Layer GPBayesian Neural NetworkThese numbers were chosen so that the runtime of a single iteration is the same as the coupled
version. Further baselines were provided by coupled (SGP: M = 100) and decoupled (Dec SGP:
Ma = 300  Mb = 50) single layer GP. The ﬁnal baseline was a Robust Bayesian Neural Network
(BNN) [Springenberg et al.  2016] with three hidden layers and 50 nodes per layer.
SGHMC DGP (This work): The architecture of this model is the same as the baseline models.
M = 100 inducing inputs were used to stay consistent with the baseline. The burn-in phase consisted
of 20 000 iterations followed by the sampling phase during which 200 samples were drawn over the
course of 10 000 iterations.

MNIST classiﬁcation SGHMC is also effective on classiﬁcation problems. Using the Robust-Max
[Hern´andez-Lobato et al.  2011] likelihood function  we applied the model to the MNIST dataset. The
SGP and Dec SGP models achieved an accuracy of 96.8 % and 97.7 % respectively. Regarding the
deep models  the best performing model was Dec DGP 3 with 98.1 % followed by SGHMC DGP 3
with 98.0 % and DGP 3 with 97.8 %. [Salimbeni and Deisenroth  2017] report slightly higher values
of 98.11 % for DGP 3. This difference can be attributed to different initialization of the parameters.

Harvard Clean Energy Project This regression dataset was produced for the Harvard Clean
Energy Project [Hachmann et al.  2011]. It measures the efﬁciency of organic photovoltaic molecules.
It is a high-dimensional dataset (60 000 datapoints and 512 binary features) that is known to beneﬁt
from deep models. SGHMC DGP 5 established a new state-of-the-art predictive performance with
test MLL of −0.83. DGP 2-5 reached up-to −1.25. Other available results on this dataset are −0.99
for DGPs with Expectation Propagation and BNNs with −1.37 [Bui et al.  2016].

Runtime To support our claim that SGHMC has a lower computational cost than DSVI  we plot
the test MLL at different stages during the training process on the protein dataset (the right plot in
Figure 3). SGHMC converges faster and to a higher limit than DSVI. SGHMC reached the target
20 000 iterations 1.6 times faster.

7 Conclusions

This paper described and demonstrated an inference method new to DGPs  SGHMC  that samples
from the posterior distribution in the usual inducing point framework. We described a novel Moving
Window MCEM algorithm that was demonstrably able to optimize hyperparameters in a fast and
efﬁcient manner. This signiﬁcantly improved performance on medium-large datasets at a reduced
computational cost and thus established a new state-of-the-art for inference in DGPs.

Acknowledgements

We want to thank Adri`a Gariga-Alonso  John Bronskill  Robert Peharz and Siddharth Swaroop for
their helpful comments and thank Intel and EPSRC for their generous support.
Juan Jos´e Murillo-Fuentes acknowledges funding from the Spanish government (TEC2016- 78434-
C3-R) and the European Union (MINECO/FEDER  UE).

References
M. Abadi  A. Agarwal  P. Barham  E. Brevdo  Z. Chen  C. Citro  G. S. Corrado  A. Davis  J. Dean 
M. Devin  S. Ghemawat  I. Goodfellow  A. Harp  G. Irving  M. Isard  Y. Jia  R. Jozefowicz 
L. Kaiser  M. Kudlur  J. Levenberg  D. Man´e  R. Monga  S. Moore  D. Murray  C. Olah  M. Schuster 
J. Shlens  B. Steiner  I. Sutskever  K. Talwar  P. Tucker  V. Vanhoucke  V. Vasudevan  F. Vi´egas 
O. Vinyals  P. Warden  M. Wattenberg  M. Wicke  Y. Yu  and X. Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems  2015. URL https://www.tensorflow.org/.
Software available from tensorﬂow.org.

S. Brooks  A. Gelman  G. Jones  and X.-L. Meng. Handbook of Markov chain Monte Carlo. CRC

press  2011.

9

T. Bui  D. Hern´andez-Lobato  J. Hernandez-Lobato  Y. Li  and R. Turner. Deep Gaussian processes
for regression using approximate expectation propagation. In International Conference on Machine
Learning  pages 1472–1481  2016.

T. Chen  E. Fox  and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In International

Conference on Machine Learning  pages 1683–1691  2014.

C.-A. Cheng and B. Boots. Incremental variational sparse Gaussian process regression. In Advances

in Neural Information Processing Systems  pages 4410–4418  2016.

C.-A. Cheng and B. Boots. Variational inference for Gaussian process models with linear complexity.

In Advances in Neural Information Processing Systems  pages 5190–5200  2017.

D. Cramer. Fundamental Statistics for Social Research: Step-by-Step Calculations and Computer
Techniques Using SPSS for Windows. Routledge  New York  NY  10001  1998. ISBN 0415172039.

K. Cutajar  E. V. Bonilla  P. Michiardi  and M. Filippone. Random feature expansions for deep

Gaussian processes. arXiv preprint arXiv:1610.04386  2016.

A. Damianou. Deep Gaussian processes and variational propagation of uncertainty. PhD thesis 

University of Shefﬁeld  2015.

A. Damianou and N. Lawrence. Deep Gaussian processes. In Artiﬁcial Intelligence and Statistics 

pages 207–215  2013.

M. M. Dunlop  M. Girolami  A. M. Stuart  and A. L. Teckentrup. How deep are deep Gaussian

processes? arXiv preprint arXiv:1711.11280  2017.

A. Graves. Practical variational inference for neural networks. In Advances in Neural Information

Processing Systems  pages 2348–2356  2011.

J. Hachmann  R. Olivares-Amaya  S. Atahan-Evrenk  C. Amador-Bedolla  R. S. S´anchez-Carrera 
A. Gold-Parker  L. Vogt  A. M. Brockway  and A. Aspuru-Guzik. The Harvard clean energy
project: large-scale computational screening and design of organic photovoltaics on the world
community grid. The Journal of Physical Chemistry Letters  2(17):2241–2251  2011.

J. Hensman  A. G. Matthews  M. Filippone  and Z. Ghahramani. MCMC for variationally sparse
Gaussian processes. In Advances in Neural Information Processing Systems  pages 1648–1656 
2015.

D. Hern´andez-Lobato  J. M. Hern´andez-Lobato  and P. Dupont. Robust multi-class Gaussian process

classiﬁcation. In Advances in neural information processing systems  pages 280–288  2011.

J. M. Hern´andez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of
Bayesian neural networks. In International Conference on Machine Learning  pages 1861–1869 
2015.

M. D. Hoffman. Learning deep latent Gaussian models with Markov chain Monte Carlo.

International Conference on Machine Learning  pages 1510–1519  2017.

In

T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the
Seventeenth conference on Uncertainty in artiﬁcial intelligence  pages 362–369. Morgan Kaufmann
Publishers Inc.  2001.

R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. 1993.

R. C. Neath et al. On convergence properties of the Monte Carlo EM algorithm. In Advances in
Modern Statistical Theory and Applications: A Festschrift in Honor of Morris L. Eaton  pages
43–62. Institute of Mathematical Statistics  2013.

J. Qui˜nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process

regression. Journal of Machine Learning Research  6(Dec):1939–1959  2005.

H. Salimbeni and M. Deisenroth. Doubly stochastic variational inference for deep Gaussian processes.

In Advances in Neural Information Processing Systems  pages 4591–4602  2017.

10

E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs.

In Y. Weiss 
B. Sch¨olkopf  and J. C. Platt  editors  Advances in Neural Information Processing Systems 18 
pages 1257–1264. MIT Press  2006.

J. T. Springenberg  A. Klein  S. Falkner  and F. Hutter. Bayesian optimization with robust Bayesian
neural networks. In Advances in Neural Information Processing Systems  pages 4134–4142  2016.

M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In D. van Dyk and
M. Welling  editors  Proceedings of the Twelth International Conference on Artiﬁcial Intelligence
and Statistics  volume 5 of Proceedings of Machine Learning Research  pages 567–574  Hilton
Clearwater Beach Resort  Clearwater Beach  Florida USA  16–18 Apr 2009. PMLR.

R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series

models. Bayesian Time series models  1(3.1):3–1  2011.

G. C. Wei and M. A. Tanner. A Monte Carlo implementation of the EM algorithm and the poor man’s
data augmentation algorithms. Journal of the American statistical Association  85(411):699–704 
1990.

M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics.

In
Proceedings of the 28th International Conference on Machine Learning (ICML-11)  pages 681–
688  2011.

C. K. Williams and C. E. Rasmussen. Gaussian processes for regression. In Advances in neural

information processing systems  pages 514–520  1996.

11

,Marton Havasi
José Miguel Hernández-Lobato
Juan José Murillo-Fuentes