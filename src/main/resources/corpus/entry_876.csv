2018,But How Does It Work in Theory? Linear SVM with Random Features,We prove that  under low noise assumptions  the support vector machine with $N\ll m$ random features (RFSVM) can achieve the learning rate faster than $O(1/\sqrt{m})$ on a training set with $m$ samples when an optimized feature map is used. Our work extends the previous fast rate analysis of random features method from least square loss to 0-1 loss. We also show that the reweighted feature selection method  which approximates the optimized feature map  helps improve the performance of RFSVM in experiments on a synthetic data set.,But How Does It Work in Theory?
Linear SVM with Random Features

Yitong Sun

Department of Mathematics

University of Michigan
Ann Arbor  MI  48109
syitong@umich.edu

Anna Gilbert

Department of Mathematics

University of Michigan
annacg@umich.edu

Ambuj Tewari

Department of Statistics
University of Michigan
tewaria@umich.edu

Abstract

√

We prove that  under low noise assumptions  the support vector machine with
N (cid:28) m random features (RFSVM) can achieve the learning rate faster than
m) on a training set with m samples when an optimized feature map is
O(1/
used. Our work extends the previous fast rate analysis of random features method
from least square loss to 0-1 loss. We also show that the reweighted feature
selection method  which approximates the optimized feature map  helps improve
the performance of RFSVM in experiments on a synthetic data set.

1

Introduction

Kernel methods such as kernel support vector machines (KSVMs) have been widely and successfully
used in classiﬁcation tasks (Steinwart and Christmann [2008]). The power of kernel methods comes
from the fact that they implicitly map the data to a high dimensional  or even inﬁnite dimensional 
feature space  where points with different labels can be separated by a linear functional. It is  however 
time-consuming to compute the kernel matrix and thus KSVMs do not scale well to extremely
large datasets. To overcome this challenge  researchers have developed various ways to efﬁciently
approximate the kernel matrix or the kernel function.
The random features method  proposed by Rahimi and Recht [2008]  maps the data to a ﬁnite
dimensional feature space as a random approximation to the feature space of RBF kernels. With
explicit ﬁnite dimensional feature vectors available  the original KSVM is converted to a linear
support vector machine (LSVM)  that can be trained by faster algorithms (Shalev-Shwartz et al.
[2011]  Hsieh et al. [2008]) and tested in constant time with respect to the number of training samples.
For example  Huang et al. [2014] and Dai et al. [2014] applied RFSVM or its variant to datasets
containing millions of data points and achieved performance comparable to deep neural nets.
Despite solid practical performance  there is a lack of clear theoretical guarantees for the learning
rate of RFSVM. Rahimi and Recht [2009] obtained a risk gap of order O(1/
N ) between the
best RFSVM and KSVM classiﬁers  where N is the number of features. Although the order of
the error bound is correct for general cases  it is too pessimistic to justify or to explain the actual
computational beneﬁts of random features method in practice. And the model is formulated as a
constrained optimization problem  which is rarely used in practice.
Cortes et al. [2010] and Sutherland and Schneider [2015] considered the performance of RFSVM as a
perturbed optimization problem  using the fact that the dual form of KSVM is a constrained quadratic
optimization problem. Although the maximizer of a quadratic function depends continuously on the
quadratic form  its dependence is weak and thus  both papers failed to obtain an informative bound
for the excess risk of RFSVM in the classiﬁcation problem. In particular  such an approach requires
RFSVM and KSVM to be compared under the same hyper-parameters. This assumption is  in fact 

√

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

√

problematic because the optimal conﬁguration of hyper-parameters of RFSVM is not necessarily
the same as those for the corresponding KSVM. In this sense  RFSVM is more like an independent
learning model instead of just an approximation to KSVM.
In regression settings  the learning rate of random features method was studied by Rudi and Rosasco
[2017] under the assumption that the regression function is in the RKHS  namely the realizable
√
case. They show that the uniform feature sampling only requires O(
m log(m)) features to achieve
m) risk of squared loss. They further show that a data-dependent sampling can achieve a
O(1/
rate of O(1/mα)  where 1/2 ≤ α ≤ 1  with even fewer features  when the regression function is
sufﬁciently smooth and the spectrum of the kernel integral operator decays sufﬁciently fast. However 
the method leading to these results depends on the closed form of the least squares solution  and thus
we cannot easily extend these results to non-smooth loss functions used in RFSVM. Bach [2017]
recently shows that for any given approximation accuracy  the number of random features required is
given by the degrees of freedom of the kernel operator under such an accuracy level  when optimized
features are available. This result is crucial for sample complexity analysis of RFSVM  though not
many details are provided on this topic in Bach’s work.
In this paper  we investigate the performance of RFSVM formulated as a regularized optimization
problem on classiﬁcation tasks. In contrast to the slow learning rate in previous results by Rahimi and
Recht [2009] and Bach [2017]  we show  for the ﬁrst time  that RFSVM can achieve fast learning rate
with far fewer features than the number of samples when the optimized features (see Assumption 2)
are available  and thus we justify the potential computational beneﬁts of RFSVM on classiﬁcation
tasks. We mainly considered two learning scenarios: the realizable case  and then unrealizable
case  where the Bayes classiﬁer does not belong to the RKHS of the feature map. In particular  our
contributions are threefold:

1. We prove that under Massart’s low noise condition  with an optimized feature map  RFSVM
can achieve a learning rate of ˜O(m
2+c2 ) number of features when the
Bayes classiﬁer belongs to the RKHS of a kernel whose spectrum decays polynomially
(λi = O(i−c2)). When the decay rate of the spectrum of kernel operator is sub-exponential 
the learning rate can be improved to ˜O(1/m) with only ˜O(lnd(m)) number of features.

1+c2 ) 1  with ˜O(m

− c2

2

2. When the Bayes classiﬁer satisﬁes the separation condition; that is  when the two classes
of points are apart by a positive distance  we prove that the RFSVM using an optimized
feature map corresponding to Gaussian kernel can achieve a learning rate of ˜O(1/m) with
˜O(ln2d(m)) number of features.

3. Our theoretical analysis suggests reweighting random features before training. We conﬁrm

its beneﬁt in our experiments over synthetic data sets.

We begin in Section 2 with a brief introduction of RKHS  random features and the problem formu-
lation  and set up the notations we use throughout the rest of the paper. In Section 3  we provide
our main theoretical results (see the appendices for the proofs)  and in Section 4  we verify the
performance of RFSVM in experiments. In particular  we show the improvement brought by the
reweighted feature selection algorithm. The conclusion and some open questions are summarized
at the end. The proofs of our main theorems follow from a combination of the sample complexity
analysis scheme used by Steinwart and Christmann [2008] and the approximation error result of
Bach [2017]. The fast rate is achieved due to the fact that the Rademacher complexity of the RKHS

of N random features and with regularization parameter λ is only O((cid:112)N log(1/λ))  while N and

1/λ need not be too large to control the approximation error when optimized features are available.
Detailed proofs and more experimental results are provided in the Appendices for interested readers.

2 Preliminaries and notations
Throughout this paper  a labeled data point is a point (x  y) in X × {−1  1}  where X is a bounded
subset of Rd. X × {−1  1} is equipped with a probability distribution P.

1 ˜O(n) represents a quantity less than Cn logk(n) for some k.

2

2.1 Kernels and Random Features
A positive deﬁnite kernel function k (x  x(cid:48)) deﬁned on X × X determines the unique corresponding
reproducing kernel Hilbert space (RKHS)  denoted by Fk. A map φ from the data space X to a
Hilbert space H such that (cid:104)φ (x)   φ (x(cid:48))(cid:105)H = k (x  x(cid:48)) is called a feature map of k and H is called a
feature space. For any f ∈ F  there exists an h ∈ H such that (cid:104)h  φ(x)(cid:105)H = f (x)  and the inﬁmum
of the norms of all such hs is equal to (cid:107)f(cid:107)F . On the other hand  given any feature map φ into H 
a kernel function is deﬁned by the equation above  and we call Fk the RKHS corresponding to φ 
denoted by Fφ.
A common choice of feature space is the L2 space of a probability space (ω  Ω  ν). An important

observation is that for any probability density function q(ω) deﬁned on Ω  φ(ω; x)/(cid:112)q(ω) with

probability measure q(ω)dν(ω) deﬁnes the same kernel function with the feature map φ(ω; x) under
the distribution ν. One can sample the image of x under the feature map φ  an L2 function φ(ω; x)  at
points {ω1  . . .   ωN} according to the probability distribution ν to approximately represent x. Then
the vector in RN is called a random feature vector of x  denoted by φN (x). The corresponding kernel
function determined by φN is denoted by kN .
A well-known construction of random features is the random Fourier features proposed by Rahimi
and Recht [2008]. The feature map is deﬁned as follows 

φ : X → L2(Rd  ν) ⊕ L2(Rd  ν)

x (cid:55)→ (cos (ω · x)   sin (ω · x)) .

And the corresponding random feature vector is

φN (x) =

1√
N

(cos (ω · x)  ···   cos (ω · x)   sin (ω · x)  ···   sin (ω · x))

(cid:124)

 

where ωis are sampled according to ν. Different choices of ν deﬁne different translation invariant
kernels (see Rahimi and Recht [2008]). When ν is the normal distribution with mean 0 and variance
γ−2  the kernel function deﬁned by the feature map is Gaussian kernel with bandwidth parameter γ 

(cid:18)

(cid:19)

.

kγ(x  x(cid:48)) = exp

−(cid:107)x − x(cid:48)(cid:107)2

2γ2

Equivalently  we may consider the feature map φγ(ω; x) := φ(ω/γ; x) with ν being standard normal
distribution.
A more general and more abstract feature map can be constructed using an orthonormal set of
L2(X   PX ). Given the orthonormal set {ei} consisting of bounded functions  and a nonnegative
sequence (λi) ∈ (cid:96)1  we can deﬁne a feature map

f (cid:55)→

Σ is of trace class with trace norm(cid:82) k(x  x) dPX (x). When the integral operator is determined by a

k(x  t)f (t) dPX (t) .

feature map φ  we denote it by Σφ  and the ith eigenvalue in a descending order by λi(Σφ). Note
that the regularization paramter is also denoted by λ but without a subscript. The decay rate of the
spectrum of Σφ plays an important role in the analysis of learning rate of random features method.

X

φ(ω; x) =

λiei(x)ei(ω)  

i=1

(cid:80)∞
with feature space L2(ω X   PX ).
is given by k(x  x(cid:48)) =
i=1 λiei(x)ei(x(cid:48)). The feature map and the kernel function are well deﬁned because of the
boundedness assumption on {ei}. A similar representation can be obtained for a continuous kernel
Every positive deﬁnite kernel function k satisfying that(cid:82) k(x  x) dPX (x) < ∞ deﬁnes an integral
function on a compact set by Mercer’s Theorem (Lax [2002]).

The corresponding kernel

operator on L2(x X   PX ) by

Σ : L2(X   PX ) → L2(X   PX )

∞(cid:88)

(cid:112)

(cid:90)

3

2.2 Formulation of Support Vector Machine
i=1 generated i.i.d. by P and a function f : X → R  usually called a
Given m samples {(xi  yi)}m
hypothesis in the machine learning context  the empirical and expected risks with respect to the loss
m(cid:88)
function (cid:96) are deﬁned by

(cid:96) (yi  f (xi)) R(cid:96)P (f ) := E(x y)∼P(cid:96) (y  f (x))  

m (f ) :=

R(cid:96)

1
m

i=1

respectively.
The 0-1 loss is commonly used to measure the performance of classiﬁers:

(cid:26)1 if f (x)y ≤ 0;

0

if f (x)y > 0.

(cid:96)0−1(y  f (x)) =

The function that minimizes the expected risk under 0-1 loss is called the Bayes classiﬁer  deﬁned by

f∗P (x) := sgn (E[y | x]) .

(f ) −
The goal of the classiﬁcation task is to ﬁnd a good hypothesis f with small excess risk R0−1P
R0−1P
(f∗P ). And to ﬁnd the good hypothesis based on the samples  one minimizes the empirical risk.
However  using 0-1 loss  it is hard to ﬁnd the global minimizer of the empirical risk because the loss
function is discontinuous and non-convex. A popular surrogate loss function in practice is the hinge
loss: (cid:96)h(f ) = max(0  1 − yf (x))  which guarantees that
RhP(f ) ≥ R0−1P

RhP(f ) − inf

(f ) − R0−1P

(f∗P )  

f

where Rh means R(cid:96)h and R0−1 means R(cid:96)0−1. See Steinwart and Christmann [2008] for more details.
A regularizer can be added into the optimization objective with a scalar multiplier λ to avoid overﬁtting
the random samples. Throughout this paper  we consider the most commonly used (cid:96)2 regularization.
Therefore  the solution of the binary classiﬁcation problem is given by minimizing the following
objective

Rm λ(f ) = Rh

m(f ) +

(cid:107)f(cid:107)2F  

λ
2

over a hypothesis class F. When F is the RKHS of some kernel function  the algorithm described
above is called kernel support vector machine. Note that for technical convenience  we do not include
the bias term in the formulation of hypothesis so that all these functions are from the RKHS instead
of the product space of RKHS and R (see Chapter 1 of Steinwart and Christmann [2008] for more
explanation of such a convention). Note that Rm λ is strongly convex and thus the inﬁmum will be
attained by some function in F. We denote it by fm λ.
When random features φN and the corresponding RKHS are considered  we add N into the subscripts
of the notations deﬁned above to indicate the number of random features. For example FN for the
RKHS  fN m λ for the solution of the optimization problem.

3 Main Results

In this section we state our main results on the fast learning rates of RFSVM in different scenarios.
First  we need the following assumption on the distribution of data  which is required for all the
results in this paper.
Assumption 1. There exists V ≥ 2 such that

|E(x y)∼P[y | x]| ≥ 2/V .

This assumption is called Massart’s low noise condition in many references (see for example Koltchin-
skii et al. [2011]). When V = 2 then all the data points have deterministic labels almost surely.
Therefore it is easier to learn the true classiﬁer based on observations. In the proof  Massart’s low
noise condition guarantees the variance condition (Steinwart and Christmann [2008])

E[((cid:96)h(f (x)) − (cid:96)h(f∗P (x)))2] ≤ V (Rh(f ) − Rh(f∗P ))  

(1)

4

which is a common requirement for the fast rate results. Massart’s condition is an extreme case of a
more general low noise condition  called Tsybakov’s condition. For the simplicity of the theorem  we
only consider Massart’s condition in our work  but our main results can be generalized to Tsybakov’s
condition.
The second assumption is about the quality of random features. It was ﬁrst introduced in Bach
[2017]’s approximation results.
Assumption 2. A feature map φ : X → L2(ω  Ω  ν)) is called optimized if there exists a small
constant µ0 such that for any µ ≤ µ0 

(cid:107)(Σ + µI)−1/2φ(ω; x)(cid:107)2

L2(P) ≤ tr(Σ(Σ + µI)−1) =

sup
ω∈Ω

λi(Σ)

.

λi(Σ) + µ

i=1

∞(cid:88)

For any given µ  the quantity on the left hand side of the inequality is called leverage score with
respect to µ  which is directly related with the number of features required to approximate a function
in the RKHS of φ. The quantity on the right hand side is called degrees of freedom by Bach [2017]
and effective dimension by Rudi and Rosasco [2017]  denoted by d(µ). Note that whatever the
RKHS is  we can always construct optimized feature map for it. In the Appendix A we describe
two examples of constructing optimized feature map. When a feature map is optimized  it is easy to
control its leverage score by the decay rate of the spectrum of Σ  as described below.
Deﬁnition 1. We say that the spectrum of Σ : L2(X   P) → L2(X   P) decays at a polynomial rate if
there exist c1 > 0 and c2 > 1 such that

λi(Σ) ≤ c1i−c2 .

We say that it decays sub-exponentially if there exist c3  c4 > 0 such that

λi(Σ) ≤ c3 exp(−c4i1/d) .

√

The decay rate of the spectrum of Σ characterizes the capacity of the hypothesis space to search
for the solution  which further determines the number of random features required in the learning
process. Indeed  when the feature map is optimized  the number of features required to approximate
a function in the RKHS with accuracy O(
µ) is upper bounded by O(d(µ) ln(d(µ))). When the
spectrum decays polynomially  the degrees of freedom d(µ) is O(µ−1/c2 )  and when it decays
sub-exponentially  d(µ) is O(lnd(c3/µ)) (see Lemma 6 in Appendix C for details). Examples on the
kernels with polynomial and sub-exponential spectrum decays can be found in Bach [2017]. Our
proof of Lemma 8 also provides some useful discussion.
With these preparations  we can state our ﬁrst theorem now.
Theorem 1. Assume that P satisﬁes Assumption 1  and the feature map φ satisﬁes Assumption 2. If
f∗P ∈ Fφ with (cid:107)f∗P(cid:107)Fφ ≤ R. Then when the spectrum of Σφ decays polynomially  by choosing

− c2
2+c2

λ = m

N = 10Cc1 c2m

2

2+c2 (ln(32Cc1 c2 m

2

2+c2 ) + ln(1/δ))  

we have

R0−1P

(fN m λ) − R0−1P

(f∗P ) ≤ Cc1 c2 V Rm

− c2

2+c2 ((ln(1/δ) + ln(m)))  

with probability 1 − 4δ. When the spectrum of Σφ decays sub-exponentially  by choosing

λ = 1/m
N = 25Cd c4 lnd(m)(ln(80Cd c4 lnd(m)) + ln(1/δ))  

we have

R0−1P

(fN m λ) − R0−1P

(f∗P ) ≤ Cc3 c4 d R V

1
m

with probability 1 − 4δ when m ≥ exp((c4 ∨ 1

c4

)d2/2).

5

(cid:16)

(cid:17)

 

logd+2(m) + log(1/δ)

√

This theorem characterizes the learning rate of RFSVM in realizable cases; that is  when the Bayes
classiﬁer belongs to the RKHS of the feature map. For polynomially decaying spectrum  when
c2 > 2  we get a learning rate faster than 1/
m. Rudi and Rosasco [2017] obtained a similar fast
learning rate for kernel ridge regression with random features (RFKRR)  assuming polynomial decay
of the spectrum of Σφ and the existence of a minimizer of the risk in Fφ. Our theorem extends
their result to classiﬁcation problems and exponential decay spectrum. However  we have to use
a stronger assumption that f∗P ∈ Fφ so that the low noise condition can be applied to derive the
√
variance condition. For RFKRR  the rate faster than O(1/
m) will be achieved whenever c2 > 1 
and the number of features required is only square root of our result. We think that this is mainly
caused by the fact that their surrogate loss is squared. The result for the sub-exponentially decaying
spectrum is not investigated for RFKRR  so we cannot make a comparison. We believe that this is the
ﬁrst result showing that RFSVM can achieve ˜O(1/m) with only ˜O(lnd(m)) features. Note however
that when d is large  the sub-exponential case requires a large number of samples  even possibly
larger than the polynomial case. This is clearly an artifact of our analysis since we can always use the
polynomial case to provide an upper bound! We therefore suspect that there is considerable room
for improving our analysis of high dimensional data in the sub-exponential decay case. In particular 
removing the exponential dependence on d under reasonable assumptions is an interesting direction
for future work.
To remove the realizability assumption  we provide our second theorem  on the learning rate of
RFSVM in unrealizable case. We focus on the random features corresponding to the Gaussian
kernel as introduced in Section 2. When the Bayes classiﬁer does not belong to the RKHS  we
need an approximation theorem to estimate the gap of risks. The approximation property of RKHS
of Gaussian kernel has been studied in Steinwart and Christmann [2008]  where the margin noise
exponent is deﬁned to derive the risk gap. Here we introduce the simpler and stronger separation
condition  which leads to a strong result.
The points in X can be collected in to two sets according to their labels as follows 

X1 := {x ∈ X | E(y | x) > 0}
X−1 := {x ∈ X | E(y | x) < 0} .
The distance of a point x ∈ Xi to the set X−i is denoted by ∆(x).
Assumption 3. We say that the data distribution satisﬁes a separation condition if there exists τ > 0
such that PX (∆(x) < τ ) = 0.
Intuitively  Assumption 3 requires the two classes to be far apart from each other almost surely. This
separation assumption is an extreme case when the margin noise exponent goes to inﬁnity.
The separation condition characterizes a different aspect of data distribution from Massart’s low
noise condition. Massart’s low noise condition guarantees that the random samples represent the
distribution behind them accurately  while the separation condition guarantees the existence of a
smooth  in the sense of small derivatives  function achieving the same risk with the Bayes classiﬁer.
With both assumptions imposed on P  we can get a fast learning rate of ln2d+1 m/m with only
ln2d(m) random features  as stated in the following theorem.
Theorem 2. Assume that X is bounded by radius ρ. The data distribution has density function upper
bounded by a constant B  and satisﬁes Assumption 1 and 3. Then by choosing

λ = 1/m γ = τ /

ln m N = Cτ d ρ ln2d m(ln ln m + ln(1/δ))  

the RFSVM using an optimized feature map corresponding to the Gaussian kernel with bandwidth γ
achieves the learning rate

√

R0−1P

(fN m λ) − R0−1P

(f∗P ) ≤ Cτ V d ρ B

m

ln2d+1(m)(ln ln(m) + ln(1/δ))

 

with probability greater than 1 − 4δ for m ≥ m0  where m0 depends on τ  ρ  d.
To the best of our knowledge  this is the ﬁrst theorem on the fast learning rate of random features
method in the unrealizable case. It only assumes that the data distribution satisﬁes low noise and
separation conditions  and shows that with an optimized feature distribution  the learning rate of

6

˜O(1/m) can be achieved using only ln2d+1(m) (cid:28) m features. This justiﬁes the beneﬁt of using
RFSVM in binary classiﬁcation problems. The assumption of a bounded data set and a bounded
distribution density function can be dropped if we assume that the probability density function
is upper bounded by C exp(−γ2(cid:107)x(cid:107)2/2)  which sufﬁces to provide the sub-exponential decay of
spectrum of Σφ. But we prefer the simpler form of the results under current conditions. We speculate
that the conclusion of Theorem 2 can be generalized to all sub-Gaussian data.
The main drawback of our two theorems is the assumption of an optimized feature distribution  which
is hard to obtain in practice. Developing a data-dependent feature selection method is therefore an
important problem for future work on RFSVM. Bach [2017] proposed an algorithm to approximate
the optimized feature map from any feature map. Adapted to our setup  the reweighted feature
selection algorithm is described as follows.

1. Select M i.i.d. random vectors {ωi}M
2. Select L data points {xi}L
√
3. Generate the matrix Φ with columns φM (xi)/
4. Compute {ri}M
(cid:124)
+ µI)−1.
5. Resample N features from {ωi}M

i=1  the diagonal of ΦΦ

L.

i=1 uniformly from the training set.

i=1 according to the distribution dνγ.

(cid:124)

i=1 according to the probability distribution pi = ri/(cid:80) ri.

(ΦΦ

The theoretical guarantees of this algorithm have not been discussed in the literature. A result in this
direction will be extremely useful for guiding practioners. However  it is outside the scope of our
work. Instead  here we implement it in our experiment and empirically compare the performance of
RFSVM using this reweighted feature selection method to the performance of RFSVM without this
preprocessing step; see Section 4.
For the realizable case  if we drop the assumption of optimized feature map  only weak results can be
obtained for the learning rate and the number of features required (see Appendix E for more details).
In particular  we can only show that 1/2 random features are sufﬁcient to guarantee the learning
rate less than  when 1/3 samples are available. Though not helpful for justifying the computational
beneﬁt of random features method  this result matches the parallel result for RFKRR in Rudi and
Rosasco [2017] and the approximation result in Sriperumbudur and Szabo [2015]. We conjecture that
this upper bound is also optimal for RFSVM.
Rudi and Rosasco [2017] also compared the performance of RFKRR with Nystrom method  which
is the other popular method to scale kernel ridge regression to large data sets.We do not ﬁnd any
theoretical guarantees on the fast learning rate of SVM with Nystrom method on classiﬁcation
problems in the literature  though there are several works on its approximation quality to the accurate
model and its empirical performance (see Yang et al. [2012]  Zhang et al. [2012]). The tools used in
this paper should also work for learning rate analysis of SVM using Nystrom method. We leave this
analysis to the future.

4 Experimental Results

In this section we evaluate the performance of RFSVM with the reweighted feature selection al-
gorithm2. The sample points shown in Figure 3 are generated from either the inner circle or outer
annulus uniformly with equal probability  where the radius of the inner circle is 0.9  and the radius
of the outer annulus ranges from 1.1 to 2. The points from the inner circle are labeled by -1 with
probability 0.9  while the points from the outer annulus are labeled by 1 with probability 0.9. In such
a simple case  the unit circle describes the Bayes classiﬁer.
First  we compared the performance of RFSVM with that of KSVM on the training set with 1000
samples  over a large range of regularization parameter (−7 ≤ log λ ≤ 1). The bandwidth parameter
γ is ﬁxed to be an estimate of the average distance among the training samples. After training 
models are tested on a large testing set (> 105). For RFSVM  we considered the effect of the number
of features by setting N to be 1  3  5  10 and 20  respectively. Moreover  both feature selection
methods  simple random feature selection (labeled by ‘unif’ in the ﬁgures)  which does not apply any
preprocess on drawing features  and reweighted feature selection (labeled by ‘opt’ in the ﬁgures) are

2The source code is available at https://github.com/syitong/randfourier.

7

Figure 1: RFSVM with 1 feature.

Figure 2: RFSVM with 20 features.

“ksvm” is for KSVM with Gaussian kernel  “unif” is for RFSVM with direct feature sampling  and “opt” is for
RFSVM with reweighted feature sampling. Error bars represent standard deviation over 10 runs.

Figure 3: Distribution of Training Samples.

50 points are shown in the graph. Blue crosses rep-
resent the points labeled by -1  and red circles the
points labeled by 1. The unit circle is one of the best
classiﬁer for these data with 90% accuracy.

Figure 4: Learning Rate of RFSVMs.

The excess risks of RFSVMs with the simple random
feature selection (“unif”) and the reweighted feature
selection (“opt”) are shown for different sample sizes.
The error rate is the excess risk. The error bars repre-
sent the standard deviation over 10 runs.

inspected. For the reweighted method  we set M = 100N and L = 0.3m to compute the weight of
each feature. Every RFSVM is run 10 times  and the average accuracy and standard deviation are
presented.
The results of KSVM  RFSVMs with 1 and 20 features are shown in Figure 1 and Figure 2 respectively
(see the results of other levels of features in Appendix F in the supplementary material). The
performance of RFSVM is slightly worse than the KSVM  but improves as the number of features
increases. It also performs better when the reweighted method is applied to generate features.
To further compare the performance of simple feature selection and reweighted feature selection
methods  we plot the learning rate of RFSVM with O(ln2(m)) features and the best λs for each
sample size m. KSVM is not included here since it is too slow on training sets of size larger than 104
in our experiment compared to RFSVM. The error rate in Figure 4 is the excess risk between learned
classiﬁers and the Bayes classiﬁer. We can see that the excess risk decays as m increases  and the
RFSVM using reweighted feature selection method outperforms the simple feature selection.
According to Theorem 2  the beneﬁt brought by optimized feature map  that is  the fast learning
rate  will show up when the sample size is greater than O(exp(d)) (see Appendix D). The number
of random features required also depends on d  the dimension of data. For data of small dimension
and large sample size  as in our experiment  it is not a problem. However  in applications of image

8

−7−6−5−4−3−2−101log(λ)0.400.450.500.550.600.650.700.750.800.850.900.951.00accuracyksvmunifopt−7−6−5−4−3−2−101log(λ)0.400.450.500.550.600.650.700.750.800.850.900.951.00accuracyksvmunifopt−2−1012−2.0−1.5−1.0−0.50.00.51.01.52.02.53.03.54.04.55.05.5log(m)0.000.050.100.15error rateunifoptrecognition  the dimension of the data is usually very large and it is hard for our theorem to explain
the performance of RFSVM. On the other hand  if we do not pursue the fast learning rate  the analysis
for general feature maps  not necessarily optimized  gives a learning rate of O(m−1/3) with O(m2/3)
random features  which does not depend on the dimension of data (see Appendix E). Actually  for
high dimensional data  there is barely any improvement in the performance of RFSVM by using
reweighted feature selection method (see Appendix F). It is important to understand the role of d to
fully understand the power of random features method.

5 Conclusion

Our study proves that the fast learning rate is possible for RFSVM in both realizable and unrealizable
scenarios when the optimized feature map is available. In particular  the number of features required
is far less than the sample size  which implies considerably faster training and testing using the
random features method. Moreover  we show in the experiments that even though we can only
approximate the optimized feature distribution using the reweighted feature selection method  it 
indeed  has better performance than the simple random feature selection. Considering that such a
reweighted method does not rely on the label distribution at all  it will be useful in learning scenarios
where multiple classiﬁcation problems share the same features but differ in the class labels. We
believe that a theoretical guarantee of the performance of the reweighted feature selection method
and properly understanding the dependence on the dimensionality of data are interesting directions
for future work.

Acknowledgements

AT acknowledges the support of a Sloan Research Fellowship.
ACG acknowledges the support of a Simons Foundation Fellowship.

References
Francis Bach. On the equivalence between kernel quadrature rules and random feature expansions.

Journal of Machine Learning Research  18(21):1–38  2017.

Corinna Cortes  Mehryar Mohri  and Ameet Talwalkar. On the impact of kernel approximation on
learning accuracy. Journal of Machine Learning Research  9:113–120  2010. ISSN 1532-4435.

Felipe Cucker and Steve Smale. On the mathematical foundations of learning. Bulletin of the

American Mathematical Society  39:1–49  2002.

Bo Dai  Bo Xie  Niao He  Yingyu Liang  Anant Raj  Maria-Florina F Balcan  and Le Song. Scalable
kernel methods via doubly stochastic gradients. In Z. Ghahramani  M. Welling  C. Cortes  N. D.
Lawrence  and K. Q. Weinberger  editors  Advances in Neural Information Processing Systems 27 
pages 3041–3049. Curran Associates  Inc.  2014.

Moulines Eric  Francis R Bach  and Zaïd Harchaoui. Testing for homogeneity with kernel Fisher
discriminant analysis. In Advances in Neural Information Processing Systems  pages 609–616 
2008.

Cho-Jui Hsieh  Kai-Wei Chang  Chih-Jen Lin  S. Sathiya Keerthi  and S. Sundararajan. A dual
coordinate descent method for large-scale linear svm. In Proceedings of the 25th International
Conference on Machine Learning  ICML ’08  pages 408–415  New York  NY  USA  2008. ACM.
ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390208.

P. S. Huang  H. Avron  T. N. Sainath  V. Sindhwani  and B. Ramabhadran. Kernel methods match
deep neural networks on timit. In 2014 IEEE International Conference on Acoustics  Speech and
Signal Processing (ICASSP)  pages 205–209  May 2014. doi: 10.1109/ICASSP.2014.6853587.

Vladimir. Koltchinskii  SpringerLink (Online service)  and École d’Été de Probabilités de Saint-Flour.
Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems École d’Été
de Probabilités de Saint-Flour XXXVIII-2008. Lecture Notes in Mathematics 0075-8434 ;2033.
Springer-Verlag Berlin Heidelberg  Berlin  Heidelberg  2011.

9

P.D. Lax. Functional analysis. Pure and applied mathematics. Wiley  2002. ISBN 9780471556046.

URL https://books.google.com/books?id=-jbvAAAAMAAJ.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. C. Platt 
D. Koller  Y. Singer  and S. T. Roweis  editors  Advances in Neural Information Processing Systems
20  pages 1177–1184. Curran Associates  Inc.  2008.

Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: Replacing minimization
with randomization in learning. In D. Koller  D. Schuurmans  Y. Bengio  and L. Bottou  editors 
Advances in Neural Information Processing Systems 21  pages 1313–1320. Curran Associates  Inc. 
2009.

Alessandro Rudi and Lorenzo Rosasco. Generalization properties of learning with random features.

In Advances in Neural Information Processing Systems  pages 3218–3228  2017.

Clint Scovel  Don Hush  Ingo Steinwart  and James Theiler. Radial kernels and their reproducing

kernel hilbert spaces. Journal of Complexity  26(6):641–660  2010.

Shai Shalev-Shwartz  Yoram Singer  Nathan Srebro  and Andrew Cotter. Pegasos: primal estimated
sub-gradient solver for svm. Mathematical Programming  127(1):3–30  2011. ISSN 1436-4646.
doi: 10.1007/s10107-010-0420-4.

Bharath Sriperumbudur and Zoltan Szabo.

fea-
In C. Cortes  N. D. Lawrence  D. D. Lee  M. Sugiyama  and R. Gar-
Information Processing Systems 28  pages 1144–
URL http://papers.nips.cc/paper/

tures.
nett  editors  Advances in Neural
1152. Curran Associates 
5740-optimal-rates-for-random-fourier-features.pdf.

random fourier

Optimal

rates

for

Inc.  2015.

I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics.

Springer New York  2008. ISBN 9780387772424.

Dougal J. Sutherland and Jeff G. Schneider. On the error of random fourier features. CoRR 

abs/1506.02785  2015.

Harold Widom. Asymptotic behavior of the eigenvalues of certain integral equations. Transactions
of the American Mathematical Society  109(2):278–295  1963. ISSN 00029947. URL http:
//www.jstor.org/stable/1993907.

Tianbao Yang  Yu-feng Li  Mehrdad Mahdavi  Rong Jin  and Zhi-Hua Zhou. Nyström method vs
random fourier features: A theoretical and empirical comparison. In F. Pereira  C. J. C. Burges 
L. Bottou  and K. Q. Weinberger  editors  Advances in Neural Information Processing Systems 25 
pages 476–484. Curran Associates  Inc.  2012.

Kai Zhang  Liang Lan  Zhuang Wang  and Fabian Moerchen. Scaling up kernel svm on limited
resources: A low-rank linearization approach. In Neil D. Lawrence and Mark Girolami  editors 
Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics 
volume 22 of Proceedings of Machine Learning Research  pages 1425–1434  La Palma  Ca-
nary Islands  21–23 Apr 2012. PMLR. URL http://proceedings.mlr.press/v22/
zhang12d.html.

10

,Seungil You
David Ding
Kevin Canini
Jan Pfeifer
Maya Gupta
Yitong Sun
Anna Gilbert
Ambuj Tewari