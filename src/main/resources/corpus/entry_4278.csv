2016,Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games,Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently  a general model of combinatorial partial monitoring (CPM) games was proposed \cite{lincombinatorial2014}  where the learner's action space can be exponentially large and adversary samples its moves from a bounded  continuous space  according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves $O(T^{2/3}\log T)$ distribution independent and $O(\log T)$ distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model  our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve $O(T^{2/3}\sqrt{\log T})$ distribution independent and $O(\log^2 T)$ distribution dependent regret respectively. Crucially  our framework needs only the simpler ``argmax'' oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm  PEGE2  which combines gap estimation with a PEGE algorithm  to achieve an $O(\log T)$ regret bound  matching the GCB guarantee but removing the dependence on size of  the learner's action space. However  like GCB  PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally  we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely  online ranking with feedback at the top.,Phased Exploration with Greedy Exploitation in

Stochastic Combinatorial Partial Monitoring Games

Sougata Chaudhuri
Department of Statistics

University of Michigan Ann Arbor

sougata@umich.edu

Department of Statistics and Department of EECS

Ambuj Tewari

University of Michigan Ann Arbor

tewaria@umich.edu

Abstract

Partial monitoring games are repeated games where the learner receives feedback
that might be different from adversary’s move or even the reward gained by the
learner. Recently  a general model of combinatorial partial monitoring (CPM)
games was proposed [1]  where the learner’s action space can be exponentially
large and adversary samples its moves from a bounded  continuous space  according
to a ﬁxed distribution. The paper gave a conﬁdence bound based algorithm (GCB)
that achieves O(T 2/3 log T ) distribution independent and O(log T ) distribution
dependent regret bounds. The implementation of their algorithm depends on
two separate ofﬂine oracles and the distribution dependent regret additionally
requires existence of a unique optimal action for the learner. Adopting their CPM
model  our ﬁrst contribution is a Phased Exploration with Greedy Exploitation
(PEGE) algorithmic framework for the problem. Different algorithms within
log T ) distribution independent and O(log2 T )
distribution dependent regret respectively. Crucially  our framework needs only the
simpler “argmax” oracle from GCB and the distribution dependent regret does not
require existence of a unique optimal action. Our second contribution is another
algorithm  PEGE2  which combines gap estimation with a PEGE algorithm  to
achieve an O(log T ) regret bound  matching the GCB guarantee but removing the
dependence on size of the learner’s action space. However  like GCB  PEGE2
requires access to both ofﬂine oracles and the existence of a unique optimal action.
Finally  we discuss how our algorithm can be efﬁciently applied to a CPM problem
of practical interest: namely  online ranking with feedback at the top.

the framework achieve O(T 2/3√

1

Introduction

Partial monitoring (PM) games are repeated games played between a learner and an adversary over
discrete time points. At every time point  the learner and adversary each simultaneously select an
action  from their respective action sets  and the learner gains a reward  which is a function of the two
actions. In PM games  the learner receives limited feedback  which might neither be adversary’s move
(full information games) nor the reward gained (bandit games). In stochastic PM games  adversary
generates actions which are independent and identically distributed according to a distribution ﬁxed
before the start of the game and unknown to the learner. The learner’s objective is to develop a
learning strategy that incurs low regret over time  based on the feedback received during the course of
the game. Regret is deﬁned as the difference between cumulative reward of the learner’s strategy and
the best ﬁxed learner’s action in hindsight. The usual learning strategies in online games combine
some form of exploration (getting feedback on certain learner’s actions) and exploitation (playing the
perceived optimal action based on current estimates).

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

Starting with early work in the 2000s [2  3]  the study of ﬁnite PM games reached a culmination
point with a comprehensive and complete classiﬁcation [4]. We refer the reader to these works for
more references and also note that newer results continue to appear [5]. Finite PM games restrict
both the learner’s and adversary’s action spaces to be ﬁnite  with a very general feedback model. All
ﬁnite partial monitoring games can be classiﬁed into one of four categories  with minimax regret
Θ(T )  Θ(T 2/3)  Θ(T 1/2) and Θ(1). The classiﬁcation is governed by global and local observability
properties pertaining to a game [4]. Another line of work has extended traditional multi-armed
bandit problem (MAB) [6] to include combinatorial action spaces for learner (CMAB) [7  8]. The
combinatorial action space can be exponentially large  rendering traditional MAB algorithms designed
for small ﬁnite action spaces  impractical with regret bounds scaling with size of action space. The
CMAB algorithms exploit a ﬁnite subset of base actions  which are speciﬁc to the structure of problem
at hand  leading to practical algorithms and regret bounds that do not scale with  or scale very mildly
with  the size of the learner’s action space.
While ﬁnite PM and CMAB problems have witnessed a lot of activity  there is only one paper [1]
on combinatorial partial monitoring (CPM) games  to the best of our knowledge. In that paper  the
authors combined the combinatorial aspect of CMAB with the limited feedback aspect of ﬁnite
PM games  to develop a CPM model. The model extended PM games to include combinatorial
action spaces for learner  which might be exponentially large  and inﬁnite action spaces for the
adversary. Neither of these situations can be handled by generic algorithms for ﬁnite PM games.
Speciﬁcally  the model considered an action space X for the learner  that has a small subset of actions
deﬁning a global observable set (see Assumption 2 in Section 2). The adversary’s action space is
a continuous  bounded vector space with the adversary sampling moves from a ﬁxed distribution
over the vector space. The reward function considered is a general non-linear function of learner’s
and adversary’s actions  with some restrictions (see Assumptions 1 & 3 in Section 2). The model
incorporated a linear feedback mechanism where the feedback received is a linear transformation of
adversary’s move. Inspired by the classic conﬁdence bound algorithms for MABs  such as UCB [6] 
the authors proposed a Global Conﬁdence Bound (GCB) algorithm that enjoyed two types of regret
bound. The ﬁrst one was a distribution independent O(T 2/3 log T ) regret bound and the second
one was a distribution dependent O(log T ) regret bound. A distribution dependent regret bound
involves factors speciﬁc to the adversary’s ﬁxed distribution  while distribution independent means
the regret bound holds over all possible distributions in a broad class of distributions. Both bounds
also had a logarithmic dependence on |X|. The algorithm combined online estimation with two
ofﬂine computational oracles. The ﬁrst oracle ﬁnds the action(s) achieving maximum value of reward
function over X   for a particular adversary action (argmax oracle)  and the second oracle ﬁnds the
action(s) achieving second maximum value of reward function over X   for a particular adversary
action (arg-secondmax oracle). Moreover  the distribution dependent regret bound requires existence
of a unique optimal learner action. The inspiration for the CPM model came from various applications
like crowdsourcing and matching problems like matching products with customers.
Our Contributions. We adopt the CPM model proposed earlier [1]. However  instead of using
upper conﬁdence bound techniques  our work is motivated by another classic technique developed
for MABs  namely that of forced exploration. This technique was already used in the classic paper
of Robbins [9] and has also been called “forcing with certainty equivalence” in the control theory
literature [10]. We develop a Phased Exploration with Greedy Exploitation (PEGE) algorithmic
framework (Section 3) borrowing the PEGE terminology from work on linearly parameterized bandits
log T )
distribution independent and O(log2 T ) distribution dependent regret. Signiﬁcantly  the framework
combines online estimation with only the argmax oracle from GCB  which is a practical advantage
over requiring an additional arg-secondmax oracle. Moreover  the distribution dependent regret does
not require existence of unique optimal action. Uniqueness of optimal action can be an unreasonable
assumption  especially in the presence of a combinatorial action space. Our second contribution
is another algorithm PEGE2 (Section 4) that combines a PEGE algorithm with Gap estimation  to
achieve a distribution dependent O(log T ) regret bound  thus matching the GCB regret guarantee
in terms of T and gap. Here  gap refers to the difference between expected reward of optimal
and second optimal learner’s actions. However  like GCB  PEGE2 does require access to both the
oracles  existence of unique optimal action for O(log T ) regret and its regret is never larger than
O(T 2/3 log T ) when there is no unique optimal action. A crucial advantage of PEGE and PEGE2
over GCB is that all our regret bounds are independent of |X|  only depending on the size of the

[11]. When the framework is instantiated with different parameters  it achieves O(T 2/3√

2

small global observable set. Thus  though we have adopted the CPM model [1]  our regret bounds
are meaningful for countably inﬁnite or even continuous learner’s action space  whereas GCB regret
bound has an explicit logarithmic dependence on |X|. We provide a detailed comparison of our
work with the GCB algorithm in Section 5. Finally  we discuss how our algorithms can be efﬁciently
applied in the CPM problem of online ranking with feedback restricted to top ranked items (Section 6) 
a problem already considered [12] but analyzed in a non-stochastic setting.

2 Preliminaries and Assumptions

The online game is played between a learner and an adversary  over discrete rounds indexed by
t = 1  2  . . .. The learner’s action set is denoted as X which can be exponentially large. The
adversary’s action set is the inﬁnite set [0  1]n. The adversary ﬁxes a distribution p on [0  1]n before
start of the game (adversary’s strategy)  with p unknown to the learner. At each round of the game 
adversary samples θ(t) ∈ [0  1]n according to p  with Eθ(t)∼p[θ(t)] = θ∗
p. The learner chooses
x(t) ∈ X and gets reward r(x(t)  θ(t)). However  the learner might not get to know either θ(t) (as
in a full information game) or r(x(t)  θ(t)) (as in a bandit game). In fact  the learner receives  as
feedback  a linear transformation of θ(t).That is  every action x ∈ X has an associated transformation
matrix Mx ∈ Rmx×n. On playing action x(t)  the learner receives a feedback Mx(t) · θ(t) ∈ Rmx.
Note that the game with the deﬁned feedback mechanism subsumes full information and bandit
games. Mx = In×n  ∀x makes it a full information game since Mx · θ = θ. If r(x  θ) = x · θ  then
Mx = x ∈ Rn makes it a bandit game. The dimension n  action space X   reward function r(· ·) and
transformation matrices Mx  ∀x ∈ X are known to the learner. The goal of the learner is to minimize
the expected regret  which  for a given time horizon T   is:

R(T ) = T · max
x∈X

E[r(x  θ)] − T(cid:88)

t=1

E[r(x(t)  θ(t))]

(1)

where the expectation in the ﬁrst term is taken over θ  w.r.t. distribution p  and the second expectation
is taken over θ and possible randomness in the learner’s algorithm.
Assumption 1. (Restriction on Reward Function) The ﬁrst assumption is that Eθ∼p[r(x  θ)] =
¯r(x  θ∗
p  which is
always satisﬁed if r(x  θ) is a linear function of θ  or if distribution p happens to be any distribution
with support [0  1]n and fully parameterized by its mean θ∗
p. With this assumption  the expected regret
becomes:

p)  for some function ¯r(· ·). That is  the expected reward is a function of x and θ∗

R(T ) = T · ¯r(x∗  θ∗

E[¯r(x(t)  θ∗

p)].

(2)

p) − T(cid:88)

t=1

p). Then ∆x = ¯r(x∗  θ∗

p) − ¯r(x  θ∗

p) =
p)   ∆max = max{∆x : x ∈ X} and ∆ =

For distribution dependent regret bounds  we deﬁne gaps in expected rewards: Let x∗ ∈ S(θ∗
argmaxx∈X ¯r(x  θ∗
min{∆x : x ∈ X   ∆x > 0}.
Assumption 2. (Existence of Global Observable Set) The second assumption is on the existence
of a global observable set  which is a subset of learner’s action set and is required for estimating
an adversary’s move θ. The global observable set is deﬁned as follows: for a set of actions σ =
{x1  x2  . . .   x|σ|} ⊆ X   let their transformation matrices be stacked in a top down fashion to obtain
a R(cid:80)|σ|
i=1 mxi×n dimensional matrix Mσ. σ is said to be a global observable set if Mσ has full column
σ Mσ = In×n.
rank  i.e.  rank(Mσ) = n. Then  the Moore-Penrose pseudoinverse M +
Without the assumption on the existence of global observable set  it might be the case that even
if the learner plays all actions in X on same θ  the learner might not be able to recover θ (as
σ Mσ = I n×n will not hold without full rank assumption). In that case  learner might not be
M +
able to distinguish between θ∗
p2  corresponding to two different adversary’s strategies. Then 
with non-zero probability  the learner can suffer Ω(T ) regret and no learner strategy can guarantee a
sub-linear in T regret (the intuition forms the base of the global observability condition in [2]). Note
that the size of the global observable set is small  i.e.  |σ| ≤ n. A global observable set can be found
by including an action x in σ if it strictly increases the rank of Mσ  till the rank reaches n. There can 
of course  be more than one global observable set.

σ satisﬁes M +

p1 and θ∗

3

Assumption 3. (Lipschitz Continuity of Expected Reward Function) The third assumption is on
the Lipschitz continuity of expected reward function in its second argument. More precisely  it is
assumed that ∃ R > 0 such that ∀ x ∈ X   for any θ1 and θ2  |¯r(x  θ1) − ¯r(x  θ2)| ≤ R(cid:107)θ1 − θ2(cid:107)2.
This assumption is reasonable since otherwise  a small error in estimation of mean reward vector
θ∗
p can introduce a large change in expected reward  leading to difﬁculty in controlling regret over
time. The Lipschitz condition holds trivially for expected reward functions which are linear in second
argument. The continuity assumption  along with the fact that adversary’s moves are in [0  1]n 
implies boundedness of expected reward for any learner’s action and any adversary’s action. We
denote Rmax = maxx∈X  θ∈[0 1]n ¯r(x  θ).
The three assumptions above will be made throughout. However  the fourth assumption will only be
made in a subset of our results.
Assumption 4. (Unique Optimal Action) The optimal action x∗ = argmaxx∈X ¯r(x  θ∗
Denote a second best action (which may not be unique) by x∗
that ∆ = ¯r(x∗  θ∗

− = argmaxx∈X  x(cid:54)=x∗ ¯r(x  θ∗

p) is unique.
p). Note

p) − ¯r(x∗

−  θ∗
p).

3 Phased Exploration with Greedy Exploitation

Algorithm 1 (PEGE) uses the classic idea of doing exploration in phases that are successively further
apart from each other. In between exploration phases  we select action greedily by completely trusting
the current estimates. The constant β controls how much we explore in a given phase and the constant
α along with the function C(·) determines how much we exploit. This idea is classic in the bandit
literature [9–11] but has not been applied to the CPM framework to the best of our knowledge.

Algorithm 1 The PEGE Algorithmic Framework
1: Inputs: α  β and function C(·) (to determine amount of exploration/exploitation in each phase).

For i = 1 to |σ| (σ is global observable set)

For j = 1 to bβ

Let tj i = t and θ(tj i  b) = θ(t) where t is current time point
Play xi ∈ σ and get feedback Mxi · θ(tj i  b) ∈ Rmxi .

2: For b = 1  2  . . .  
Exploration
3:
4:
5:
6:
7:
8:
9:
10:
11:

End For
Estimation
˜θj i = M +

End For

(cid:80)b
(cid:80)iβ
σ (Mx1 · θ(tj 1  i)  . . .   Mx|σ| · θ(tj |σ|  i)) ∈ Rn.
(cid:80)b

˜θj i

i=1

∈ Rn.
ˆθ(b) =
x(b) ∈ argmaxx∈X ¯r(x  ˆθ(b)).
Exploitation

j=1
j=1 jβ

For i = 1 to exp(C(bα))

Play x(b).

End For

12:

13:
14:
15:
16:
17:
18: End For

p) = M +

It is easy to see that the estimators in Algorithm 1 have the following properties: Ep[˜θj i] =
σ (Mx1 · θ∗
p. Using the fact that
M +
σ = (M(cid:62)
M +
(cid:107)˜θj i − θ∗

p  . . .   Mx|σ| · θ∗
σ Mσ)−1M(cid:62)
σ   we also have the following bound on estimation error of θ∗
p:
p(cid:107)2 ≤ (cid:107)M +
σ (Mx1 · θ(tj 1  i)  . . .   Mx|σ| · θ(tj |σ|  i)) − M +
|σ|(cid:88)
(cid:107)(M(cid:62)
M(cid:62)
σ Mσ)−1

p(cid:107)2
σ Mσθ∗
σ Mσ)−1M(cid:62)

p and hence Ep[ˆθ] = θ∗

Mxk · (θ(tj k  i) − θ∗

p)(cid:107)2 ≤ √

σ Mσ · θ∗

= (cid:107)(M(cid:62)

p = θ∗

|σ|(cid:88)

n

xk

xk

Mxk(cid:107)2 =: βσ
(3)

k=1

k=1

4

where the constant βσ deﬁned above depends only on the structure of the linear transformation
matrices of the global observer set and not on adversary strategy p.
Our ﬁrst result is about the regret of Algorithm 1 when within phase number b  the exploration part
spends |σ| rounds (constant w.r.t. b) and the exploitation part grows polynomially with b.
Theorem 1. (Distribution Independent Regret) When Algorithm 1 is initialized with the param-
eters C(a) = log a  α = 1/2 and β = 0  and the online game is played over T rounds  we get the
following bound on expected regret:

R(T ) ≤ Rmax|σ|T 2/3 + 2RβσT 2/3(cid:112)

log 2e2 + 2 log T + Rmax

(4)

where βσ is the constant as deﬁned in Eq. 3.

Our next result is about the regret of Algorithm 1 when within phase number b  the exploration part
spends |σ| · b rounds (linearly increasing with b) and the exploitation part grows exponentially with b.
Theorem 2. (Distribution Dependent Regret) When Algorithm 1 is initialized with the parameters
C(a) = h · a  for a tuning parameter h > 0  α = 1 and β = 1  and the online game is played over T
rounds  we get the following bound on expected regret:
√

4

2πe2R∆maxβσ

+

h2(2R2β2

σ )

∆2

e

.

(5)

∆

R(T ) ≤(cid:88)

(cid:18) log T

(cid:19)2

∆x

x∈σ

h

Such an explicit bound for a PEGE algorithm that is polylogarithmic in T and explicitly states the
multiplicative and additive constants involved in not known  to the best of our knowledge  even in
the bandit literature (e.g.  earlier bounds [10] are asymptotic) whereas here we prove it in the CPM
setting. Note that the additive constant above  though ﬁnite  blows up exponentially fast as ∆ → 0
for a ﬁxed h. It is well behaved however  if the tuning parameter h is on the same scale as ∆. This
line of thought motivates us to estimate the gap to within constant factors and then feed that estimate
into a PEGE algorithm. This is what we will do in the next section.

4 Combining Gap Estimation with PEGE

Algorithm 2 tries to estimate the gap ∆ to within a constant multiplicative factor. However  if there is
no unique optimal action or when the true gap is small  gap estimation can take a very large amount
of time. To prevent that from happening  the algorithm also takes in a threshold T0 as input and
deﬁnitely stops if the threshold is reached. The result below assures us that  with high probability 
the algorithm behaves as expected. That is  if there is a unique optimal action and the gap is large
enough to be estimated with a given conﬁdence before the threshold T0 kicks in  it will output an
estimate ˆ∆ in the range [0.5∆  1.5∆]. On the other hand  if there is no unique optimal action  it does
not generate an estimate of ∆ and instead runs out of the exploration budget T0.
Theorem 3. (Gap Estimation within Constant Factors) Let T0 ≥ 1 and δ ∈ (0  1) and deﬁne
T1(δ) = 256R2β2

δ . Consider Algorithm 2 run with

  T2(δ) = 16R2β2

log 512e2R2β2

σ

log 4e2

∆2δ

∆2

σ

σ

(cid:115)

∆2

Then  the following 3 claims hold.

w(b) =

R2β2

σ log( 4e2b2

δ

b

)

.

(6)

Algorithm 2 stops in T1(δ) episodes and outputs an estimate ˆ∆ that satisﬁes 1

1. Suppose Assumption 4 holds and T1(δ) < T0. Then with probability at least 1 − δ 
2 ∆.
2. Suppose Assumption 4 holds and T0 ≤ T1(δ). Then with probability at least 1 − δ  the
algorithm either outputs “threshold exceeded” or outputs an estimate ˆ∆ that satisﬁes
2 ∆ ≤ ˆ∆ ≤ 3
2 ∆. Furthermore  if it outputs ˆ∆  it must be the case that the algorithm stopped
1
at an episode b such that T2(δ) < b < T0.

2 ∆ ≤ ˆ∆ ≤ 3

3. Suppose Assumption 4 fails. Then  with probability at least 1 − δ  Algorithm 2 stops in T0

episodes and outputs “threshold exceeded”.

5

Algorithm 2 Algorithm for Gap Estimation
1: Inputs: T0 (exploration threshold) and δ (conﬁdence parameter)

For i = 1 to |σ|

2: For b = 1  2  . . .  
Exploration
3:
4:
5:
6:
7:
8:
9:

End For
Estimation
˜θb = M +

(Denote) ti = t and θ(ti  b) = θ(t) (t is current time point).
Play xi ∈ σ and get feedback Mxi · θ(ti  b) ∈ Rmxi .
(cid:80)b
σ (Mx1 · θ(t1  b)  . . .   Mx|σ| · θ(t|σ|  b)) ∈ Rn.

˜θi

∈ Rn.

i=1
b

Stopping Rule (w(b) is deﬁned as in Eq. (6))
If argmaxx∈X ¯r(x  ˆθ(b)) is unique:

ˆx(b) = argmaxx∈X ¯r(x  ˆθ(b))
ˆx−(b) = argmaxx∈X  x(cid:54)=ˆx(b) ¯r(x  ˆθ(b)) (need not be unique)
If ¯r(ˆx(b)  ˆθ(b)) − ¯r(ˆx−(b)  ˆθ(b)) > 6w(b):
STOP and output ˆ∆ = ¯r(ˆx(b)  ˆθ(b)) − ¯r(ˆx−(b)  ˆθ(b))

10:

ˆθ(b) =

11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
End If
22: End For

End If

End If
If b > T0:

STOP and output “threshold exceeded”

Equipped with Theorem 3  we are now ready to combine Algorithm 2 with Algorithm 1 to give
Algorithm 3. Algorithm 3 ﬁrst calls Algorithm 2. If Algorithm 2 outputs an estimate ˆ∆ it is fed into
Algorithm 1. If the threshold T0 is exceeded  then the remaining time is spent in pure exploitation.
Note that by choosing T0 to be of order T 2/3 we can guarantee a worst case regret of the same order
even when unique optimality assumption fails. For PM games that are globally observable but not
locally observable  such a distribution independent O(T 2/3) bound is known to be optimal [4].
Theorem 4. (Regret Bound for PEGE2) Consider Algorithm 3 run with knowledge of the number
T of rounds. Consider the distribution independent bound

maxT )2/3(cid:112)log(4e2T 3) + Rmax 

B1(T ) = 2(2Rβσ|σ|2R2

and the distribution dependent bound
512e2R2β2

256R2β2
σ

σT

log

∆2

B2(T ) =
+ Rmax.
If Assumption 4 fails  then the expected regret of Algorithm 3 is bounded as R(T ) ≤ B1(T ). If
Assumption 4 holds  then the expected regret of Algorithm 3 is bounded as

x∈σ

∆x

∆2

∆2

36R2β2
σ log T
∆2

+

8e2R2β2
σ

Rmax|σ| +

(cid:88)

O(T 2/3 log T )

if T1(δ) < T0
if T0 ≤ T1(δ)

 

(7)

where T1(δ) is as deﬁned in Theorem 3 and δ  T0 are as deﬁned in Algorithm 3.

In the above theorem  note that T1(δ) scales as Θ( 1
cases in Eq. (7) correspond to large gap and small gap situations respectively.

∆2 log T

∆2 ) and T0 as Θ(T 2/3). Thus  the two

5 Comparison with GCB Algorithm

We provide a detailed comparison of our results with those obtained for GCB [1]. (a) While we
use the same CPM model  our solution is inspired by the forced exploration technique while GCB

6

(cid:26)B2(T )

R(T ) ≤

Algorithm 3 Algorithm Combining PEGE with Gap Estimation (PEGE2)
1: Input: T (total number of rounds)

(cid:16) 2RβσT

(cid:17)2/3

and δ = 1/T

2: Call Algorithm 2 with inputs T0 =
|σ|Rmax
3: If Algorithm 2 returns “threshold exceeded”:
4: Let ˆθ(T0) be the latest estimate of θ∗
5: Play ˆx(T0) = argmaxx∈X ¯r(x  ˆθ) for the remaining T − T0|σ| rounds
6: Else:
7: Let ˆ∆ be the gap estimate produced by Algorithm 2
8: For all remaining time steps  run Algorithm 1 with parameters C(a) = ha with

p maintained by Algorithm 2

h = ˆ∆2
9R2β2
σ

9: End If

  α = 1  β = 0

(b) One instantiation of our PEGE framework gives an O(T 2/3√

is inspired by the conﬁdence bound technique  both of which are classic in the bandit literature.
log T ) distribution independent
regret bound (Theorem 1)  which does not require call to arg-secondmax oracle. This is of substantial
practical advantage over GCB since even for linear optimization problems over polyhedra  standard
routines usually do not have option of computing action(s) that achieve second maximum value
for the objective function. (c) Another instantiation of the PEGE framework gives an O(log2 T )
distribution dependent regret bound (Theorem 2)  which neither requires call to arg-secondmax oracle
nor the assumption of existence of unique optimal action for learner. This is once again important 
since the assumption of existence of unique optimal action might be impractical  especially for
exponentially large action space. However  the caveat is that improper setting of the tuning parameter
h in Theorem 2 can lead to an exponentially large additive component in the regret. (d) A crucial
point  which we had highlighted in the beginning  is that the regret bounds achieved by PEGE and
PEGE2 do not have dependence on size of learner’s action space  i.e.  |X|. The dependence is only on
the size of global observable set σ  which is guaranteed to be not more than dimension of adversary’s
action space. Thus  though we have adopted the CPM model [1]  our algorithms achieve meaningful
regret bounds for countably inﬁnite or even continuous learner’s action space. In contrast  the GCB
regret bounds have explicit  logarithmic dependence on size of learner’s action space. Thus  their
results cannot be extended to problems with inﬁnite learner’s action space (see Section 6 for an
example)  and are restricted to large  but ﬁnite action spaces. (e) The PEGE2 algorithm is a true
analogue of the GCB algorithm  matching the regret bounds of GCB in terms of T and gap ∆ with
the advantage that it has no dependence on |X|. The disadvantage  however  is that PEGE2 requires
knowledge of time horizon T   while GCB is an anytime algorithm. It remains an open problem to
design an algorithm that combines the strengths of PEGE2 and GCB.

6 Application to Online Ranking

A recent paper studied the problem of online ranking with feedback restricted to top ranked items
[12]. The problem was studied in a non-stochastic setting  i.e.  it was assumed that an oblivious
adversary generates reward vectors. Moreover  the learner’s action space was exponentially large in
number of items to be ranked. The paper made the connection of the problem setting to PM games
(but not combinatorial PM games) and proposed an efﬁcient algorithm for the speciﬁc problem at
hand. However  a careful reading of the paper shows that their algorithmic techniques can handle the
CPM model we have discussed so far  but in the non-stochastic setting. The reward function is linear
in both learner’s and adversary’s moves  adversary’s move is restricted to a ﬁnite space of vectors and
feedback is a linear transformation of adversary’s move. In this section  we give a brief description
of the problem setting and show how our algorithms can be used to efﬁciently solve the problem of
online ranking with feedback on top ranked items in the stochastic setting. We also give an example
of how the ranking problem setting can be somewhat naturally extended to one which has continuous
action space for learner  instead of large but ﬁnite action space.
The paper considered an online ranking problem  where a learner repeatedly re-ranks a set of n  ﬁxed
items  to satisfy diverse users’ preferences  who visit the system sequentially. Each learner action x

7

p) = f (x) · θ∗

is a permutation of the n items. Each user has like/dislike preference for each item  varying between
users  with each user’s preferences encoded as an n length binary relevance vector θ. Once the ranked
list of items is presented to the user  the user scans through the items  but gives relevance feedback
only on top ranked item. However  the performance of the learner is judged based on full ranked list
and unrevealed  full relevance vector. Thus  we have a PM game  where neither adversary generated
relevance vector nor reward is revealed to learner. The paper showed how a number of practical
ranking measures  like Discounted Cumulative Gain (DCG)  can be expressed as a linear function 
i.e.  r(x  θ) = f (x)· θ. The practical motivation of the work was based on learning a ranking strategy
to satisfy diverse user preferences  but with limited feedback received due to user burden constraints
and privacy concerns.
Online Ranking with Feedback at Top as a Stochastic CPM Game. We show how our algorithms
can be applied in online ranking with feedback for top ranked items by showing how it is a speciﬁc
instance of the CPM model and how our key assumptions are satisﬁed. The learner’s action space
is the ﬁnite but exponentially large space of X = n! permutations. Adversary’s move is an n
dimensional relevance vector  and thus  is restricted to {0  1}n (ﬁnite space of size 2n) contained
in [0  1]n. In the stochastic setting  we can assume that adversary samples θ ∈ {0  1}n from a ﬁxed
distribution on the space. Since the feedback on playing a permutation is the relevance of top ranked
item  each move x has an associated transformation matrix (vector) Mx ∈ {0  1}n  with 1 in the place
of the item which is ranked at the top by x and 0 everywhere else. Thus  Mx · θ gives the relevance
of item ranked at the top by x. The global observable set σ is the set of any n actions  where each
action  in turn  puts a distinct item on top. Hence  Mσ is the n × n dimensional permutation matrix.
Assumption 1 is satisﬁed because the reward function is linear in θ and ¯r(x  θ∗
p  where
p ∈ [0  1]n. Assumption 2 is satisﬁed since there will always be a global observable set
Ep[θ] = θ∗
of size n and can be found easily. In fact  there will be multiple global observable sets  with the
freedom to choose any one of them. Assumption 3 is satisﬁed due to the expected reward function
being linear in second argument. The Lipschitz constant is maxx∈X (cid:107)f (x)(cid:107)2  which is always less
than some small polynomial factor of n  depending on speciﬁc f (·). The value of βσ can be easily
seen to be n3/2. The argmax oracle returns the permutation which simply sorts items according to
their corresponding θ values. The arg-secondmax oracle is more complicated  though feasible. It
requires ﬁrst sorting the items according to θ and then compare each pair of consecutive items to see
where least drop in reward value occurs and switch the corresponding items.
Likely Failure of Unique Optimal Action Assumption. Assumption 4 is unlikely to hold in this
problem setting (though of course theoretically possible). The mean relevance vector θ∗
p effectively
reﬂects the average preference of all users for each of the n items. It is very likely that at least a
few items will not be liked by anyone and which will ultimately be always ranked at the bottom.
Equally possible is that two items will have same user preference on average  and can be exchanged
without hurting the optimal ranking. Thus  existence of an unique optimal ranking  which indicates
that each item will have different average user preference than every other item  is unlikely. Thus 
PEGE algorithm can still be applied to get poly-logarithmic regret (Theorem 2)  but GCB will only
achieve O(T 2/3 log T ) regret.
A PM Game with Inﬁnite Learner Action Space. We give a simple modiﬁcation of the ranking
problem above to show how the learner can have continuous action space. The learner now ranks the
items by producing an n dimensional score vector x ∈ [0  1]n and sorting items according to their
scores. Thus the learner’s action space is now an uncountably inﬁnite continuous space. As before 
the user gets to see the ranked list and gives relevance feedback on top ranked item. The learner’s
performance will now be judged by a continuous loss function  instead of a discrete-valued ranking
measure  since its moves are in a continuous space. Consider the simplest loss  viz.  the squared
“loss” r(x  θ) = −(cid:107)x − θ(cid:107)2
2 (note -ve sign to keep reward interpetation). It can be easily seen that
p  if the relevance vectors θ are in {0  1}n. Thus 
¯r(x  θ∗
the Lipschitz condition is satisﬁed. The global observable set is still of size n  with the n actions
being any n score vectors  whose sorted orders place each of the n items  in turn  on top. βσ remains
Eθ∼pr(x  θ) = Eθ∼p[θ] = θ∗
same as before  with argmaxx
p. Both PEGE and PEGE2 can achieve
meaningful regret bound for this problem  while GCB cannot.

p) = Eθ∼p[r(x  θ)] = −(cid:107)x(cid:107)2

2 + 2x· θ∗

p − 1· θ∗

Acknowledgements

We acknowledge the support of NSF via grants IIS 1452099 and CCF 1422157.

8

References
[1] Tian Lin  Bruno Abrahao  Robert Kleinberg  John Lui  and Wei Chen. Combinatorial par-
tial monitoring game with linear feedback and its applications. In Proceedings of the 31th
International Conference on Machine Learning  pages 901–909. ACM  2014.

[2] Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary
feedback and loss. In Proceedings of the 14th Annual Conference on Computational Learning
Theory  pages 208–223. Springer  2001.

[3] Nicolo Cesa-Bianchi  Gábor Lugosi  and Gilles Stoltz. Regret minimization under partial

monitoring. Mathematics of Operations Research  pages 562–580  2006.

[4] Gabor Bartok et al. Partial monitoring–classiﬁcation  regret bounds  and algorithms. Mathemat-

ics of Operations Research  39(4):967–997  2014.

[5] Junpei Komiyama  Junya Honda  and Hiroshi Nakagawa. Regret lower bound and optimal
algorithm in ﬁnite stochastic partial monitoring. In Advances in Neural Information Processing
Systems  pages 1783–1791  2015.

[6] Peter Auer  Nicolo Cesa-Bianchi  and Paul Fischer. Finite-time analysis of the multiarmed

bandit problem. Machine Learning  47(2-3):235–256  2002.

[7] Wei Chen  Yajun Wang  and Yang Yuan. Combinatorial multi-armed bandit: General framework
and applications. In Proceedings of the 30th International Conference on Machine Learning 
pages 151–159  2013.

[8] Branislav Kveton  Zheng Wen  Azin Ashkan  and Csaba Szepesvari. Tight regret bounds
In Proceedings of the Eighteenth International

for stochastic combinatorial semi-bandits.
Conference on Artiﬁcial Intelligence and Statistics  pages 535–543  2015.

[9] Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins

Selected Papers  pages 169–177. Springer  1985.

[10] Rajeev Agrawal and Demosthenis Teneketzis. Certainty equivalence control with forcing:

revisited. Systems & Control Letters  13(5):405–412  1989.

[11] Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of

Operations Research  35(2):395–411  2010.

[12] Sougata Chaudhuri and Ambuj Tewari. Online ranking with top-1 feedback. In Proceedings
of the 18th International Conference on Artiﬁcial Intelligence and Statistics  pages 129–137.
ACM  2015.

[13] Thomas P Hayes. A large-deviation inequality for vector-valued martingales. Combinatorics 

Probability and Computing  2005.

9

,Matthew Johnson
James Saunderson
Alan Willsky
Zhen Cui
Hong Chang
Sougata Chaudhuri
Ambuj Tewari
Zhonghui You
Kun Yan
Jinmian Ye
Meng Ma
Ping Wang