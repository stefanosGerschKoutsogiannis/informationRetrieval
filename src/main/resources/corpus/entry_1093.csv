2012,Confusion-Based Online Learning and a Passive-Aggressive Scheme,This paper provides the first ---to the best of our knowledge--- analysis of online learning algorithms for multiclass problems when the {\em confusion} matrix is taken as a performance measure. The work builds upon recent and elegant results on noncommutative concentration inequalities  i.e. concentration inequalities that apply to matrices  and more precisely to matrix martingales.  We do establish generalization bounds for online learning algorithm and show how the theoretical study motivate the proposition of a new confusion-friendly learning procedure. This learning algorithm  called \copa (for COnfusion Passive-Aggressive) is a passive-aggressive learning algorithm; it is shown that the update equations for \copa can be computed analytically  thus allowing the user from having to recours to any optimization package to implement it.,Confusion-Based Online Learning and a

Passive-Aggressive Scheme

QARMA  Laboratoire d’Informatique Fondamentale de Marseille

Liva Ralaivola

Aix-Marseille University  France

liva.ralaivola@lif.univ-mrs.fr

Abstract

This paper provides the ﬁrst —to the best of our knowledge— analysis of online
learning algorithms for multiclass problems when the confusion matrix is taken
as a performance measure. The work builds upon recent and elegant results on
noncommutative concentration inequalities  i.e. concentration inequalities that
apply to matrices  and  more precisely  to matrix martingales. We do establish
generalization bounds for online learning algorithms and show how the theoretical
study motivates the proposition of a new confusion-friendly learning procedure.
This learning algorithm  called COPA (for COnfusion Passive-Aggressive) is a
passive-aggressive learning algorithm; it is shown that the update equations for
COPA can be computed analytically and  henceforth  there is no need to recourse
to any optimization package to implement it.

1

Introduction

This paper aims at promoting an infrequent way to tackle multiclass prediction problems: we ad-
vocate for the use of the confusion matrix —the matrix which reports the probability of predicting
class q for an instance of class p for all potential label pair (p  q)— as the objective ‘function’ to
be optimized. This way  we step aside the more widespread viewpoint of relying on the misclassi-
ﬁcation rate —the probability of misclassifying a point— as a performance measure for multiclass
predictors. There are obvious reasons for taking this perspective  among which we may name the fol-
lowing. First  the confusion information is a ﬁner-grain information than the misclassiﬁcation rate 
as it allows one to precisely identify the types of error made by a classiﬁer. Second  the confusion
matrix is independent of the class distributions  since it reports conditional probability distributions:
a consequence is that a predictor learned to achieve a ‘small’ confusion matrix will probably be in-
sensitive to class imbalance and it will also be robust to changes in class prior distributions between
train and test data. Finally  there are many application domains such as medicine  bioinformatics 
information retrieval  where the confusion matrix (or an estimate thereof) is precisely the object of
interest for an expert who wants to assess the relevance of an automatic prediction procedure.

Contributions. We essentially provide two contributions. On the one hand  we provide a statistical
analysis for the generalization ability of online learning algorithms producing predictors that aim at
optimizing the confusion. This requires us to introduce relevant statistical quantities that are taken
advantage of via a concentration inequality for matrix martingales proposed by [8]. Motivated by our
statistical analysis  we propose an online learning algorithm from the family of passive aggressive
learning algorithms [2]: this algorithm is inherently designed to optimize the confusion matrix and
numerical simulations are provided that show it reaches its goal.

Outline of the paper. Section 2 formalizes our pursued objective of targetting a small confu-
sion error. Section 3 provides our results regarding the ability of online confusion-aware learning

1

procedures to achieve a small confusion together with the update equations for COPA  a new passive-
aggressive learning procedure designed to control the confusion risk. Section 4 reports numerical
results that should be viewed as a proof of concept for the effectiveness of our algorithm.

2 Problem and Motivation

2.1 Notation
We focus on the problem of multiclass classiﬁcation: the input space is denoted by X and the target
space is Y = {1  . . .   Q}. The training sequence Z = {Zt = (Xt  Yt)}T
t=1 is made of T identically
and independently random pairs Zt = (Xt  Yt) distributed according to some unknown distribution
D over X × Y. The sequence of input data will be referred to as X = {Xt}T
t=1 and the sequence
of corresponding labels Y = {Yt}T
t=1. We may write that Z is distributed according to DT =
⊗T
t=1D. Z1:t denotes the subsequence Z1:t = {(Xτ   Yτ )}t
τ =1. We use DX|y for the conditional
distribution on X given that Y = y; therefore  for a given sequence y = (y1  . . .   yT ) ∈ Y T  
DX|y = ⊗T
t=1DX|yt is the distribution of the random sample X = {X1  . . .   XT} over X T such
that Xt is distributed according to DX|yt. E[·] and EX|y respectively denote the expectation with
respect to D and DX|y.
For a sequence y of labels  T (y) = [T1(y)··· TQ(y)] ∈ NQ is such that Tq(y) is the number of
times label q appears in y. Often  we will drop the dependence upon y for T (y). Throughout  we
make the simplifying assumption that Tq > 1 for all q —otherwise  our analysis still holds but extra
care and notation must be taken for handling classes absent from Z.
The space of hypotheses we consider is H (e.g. H ⊆ {f : f : X → R}Q)  and A designates an
online learning algorithm that produces hypothesis ht ∈ H when it encounters a new example Zt.
Finally  (cid:96) = ((cid:96)q|p)1≤p q≤Q is a family of class-dependent loss functionals (cid:96)q|p : H × X → R+. For
a point x ∈ X of class y ∈ Y  (cid:96)q|y(h  x) is the cost of h’s favoring class q over y for x.
Example 1 (Misclassiﬁcation Loss). The family of (cost-sensitive) misclassiﬁcation losses (cid:96)misclass
is deﬁned as
where Cpq ∈ R+  ∀p  q ∈ Y and χ[E] = 1 if E is true and 0 otherwise.
Example 2 (Hinge Loss). The family of multiclass hinge losses (cid:96)hinge is such that  given W =

{w1  . . .   wQ} ∈ X Q with(cid:80) wq = 0 and hypothesis hW such that hW (x) = [(cid:104)w1  x(cid:105)···(cid:104)wQ  x(cid:105)]

.
= χ[h(x)=q]Cyq 

(cid:96)misclass
q|y

(h  x)

(1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq  x(cid:105) +

(cid:12)(cid:12)(cid:12)(cid:12)+

1

Q − 1

(cid:96)hinge
q|y (hW   x)

.
=

  where | · |+ = max(0 ·).

(2)

2.2 From the Confusion Matrix to the Confusion Risk  and their Minimization

In many situations  e.g. class-imbalanced datasets  it is important not to mea-

Confusion matrix.
sure the quality of a predictor h based upon its classiﬁcation rate
= PXY (h(X) (cid:54)= Y )
.

R(h)

(3)
only; this may lead to erroneous conclusions regarding the quality of h. Indeed  if  for instance 
some class q is predominantly present in the data at hand  say P(Y = q) = 1 − ε  for some small
ε > 0  then the predictor hmaj that always outputs hmaj(x) = q regardless of x has a classiﬁcation
error lower that is at most ε  whereas it never correctly predicts the class of data from classes p (cid:54)= q.
A more informative object is the confusion matrix C(h) ∈ RQ×Q of h  which is deﬁned as:

= (P(h(X) = q)|Y = p)1≤p q≤Q .
.

(4)
The nondiagonal entries of C(h) provides the information as to the types of confusion  and their
prevalence  h makes when predicting the class of some x. Let us now abuse the notation and denote
C(h) the confusion matrix where the diagonal entries are zeroed. It is straightforward to see  that  if
π = [P(Y = 1)··· P(Y = Q)] is the vector of class prior distributions then  the misclassiﬁcation
rate R(h) (cf. (3)) can be retrieved as

C(h)

R(h) = (cid:107)πC(h)(cid:107)1 

2

where (cid:107) · (cid:107)1 denotes the 1-norm. This says that  with little additional information  the misclassiﬁca-
tion rate might be obtained from the confusion matrix  while the converse is not true.
Is is clear that having the confusion matrix C(h) be zero means that h is a perfect predictor. When
such situation is not possible (if the data are corrupted by noise  for instance)  a valuable objective
might be to look for a classiﬁer h having a confusion matrix as close to zero as possible. Here  we
choose to measure the closeness to zero of matrices through the operator norm (cid:107) · (cid:107)  deﬁned as:

(cid:107)M(cid:107) .

(cid:107)M v(cid:107)2
(cid:107)v(cid:107)2

(5)
where (cid:107) · (cid:107)2 is the standard Euclidean norm —(cid:107)M(cid:107) is merely the largest singular value of M. In
addition to being a valid norm  the operator norm has the following nice property  that will be of
help to us. Given two matrices A = (aij) and B = (bij) of the same dimensions

= max
v(cid:54)=0

 

0 ≤ aij ≤ bij  ∀i  j ⇒ (cid:107)A(cid:107) ≤ (cid:107)B(cid:107).

(6)

Given the equivalence between norms and the deﬁnition of the operator norm  it is easy to see that

R(h) ≤(cid:112)Q(cid:107)C(h)(cid:107) 

and targetting a small confusion matrix for h may have the consequence of implying a small mis-
classiﬁcation risk R(h).
The discussion conducted so far brings us to a natural goal of multiclass learning  that of minimizing
the norm of the confusion matrix  i.e. that of solving the following optimization problem

h∈H(cid:107)C(h)(cid:107).

min

However  addressing this question directly poses a number of problems  both from the statistical and
algorithmic sides: a) it is not possible to compute C(h) as it depends on the unknown distribution D
and b) relying on empirical estimates of C(h) as is folk in statistical learning  requires to deal with
the indicator function χ[] that appears in (1)  which is not optimization-friendly.

Confusion Risk.
In order to deal with the latter problem and prepare the ground for tackling the
former one from a theoretical point of view  we now introduce and discuss the confusion risk  which
is parameterized by a family of loss functions that may be surrogate for the indicator loss χ[].
Deﬁnition 1 (Confusion Risk). The confusion risk C(cid:96)(h) of h is deﬁned as

C(cid:96)(h) =(cid:0)c(cid:96)

pq

(cid:1)

1≤p q≤Q

∈ RQ×Q  with c(cid:96)

pq

.
=

0

if p (cid:54)= q 
otherwise.

(7)

(cid:26) EX|p(cid:96)q|p(h  X)

Observe that if the family (cid:96)misclass of losses from Example 1 is retained  and Cpq = 1 for all p  q then
C(cid:96)misclass(h) is precisely the confusion matrix discussed above (with the diagonal set to zero).
Similarly as before  the (cid:96)-risk R(cid:96)(h) is deﬁned as R(cid:96)(h)
The following lemma directly comes from Equation (6).
Lemma 1. Let h ∈ H. If 0 ≤ χ[h(x)(cid:54)=p] ≤ (cid:96)q|p(h  x)   ∀x ∈ X   ∀p  q ∈ Y  then (cid:107)C(h)(cid:107) ≤ (cid:107)C(cid:96)(h)(cid:107).
This says that if we recourse to surrogate (cid:96) that upper bounds the 0-1 indicator function then the
norm of the confusion risk is always larger than the norm of the confusion matrix.
Armed with the confusion risk and the last lemma  we may now turn towards the legitimate objective

= (cid:107)πC(cid:96)(h)(cid:107)1 and R(cid:96)(h) ≤ √

Q(cid:107)C(cid:96)(h)(cid:107).

.

h∈H(cid:107)C(cid:96)(h)(cid:107) 

min

a small value of (cid:107)C(cid:96)(h)(cid:107) implying a small value of (cid:107)C(h)(cid:107) (which was our primary goal).
It is still impossible to solve this problem because of the unknown distribution D according to which
expectations are computed. However  as already evoked  it is possible to derive learning strategies
based on empirical estimates of the confusion risk  that ensures (cid:107)C(cid:96)(h)(cid:107) will be small. The next
section is devoted to show how this could be done.

3

3 Bounds on the Confusion Risk via Online Learning and COPA

(Empirical) Online Confusion Risk

3.1
Assume an online learning algorithm A that outputs hypotheses from a family H: run on the traning
sequence Z ∼ DT   A outputs hypotheses h = {h0  . . .   hT}  where h0 is an arbitrary hypothesis.

Deﬁnition 2 ((cid:98)L|p(· ·) and L|p(·) matrices). Let (cid:96) = ((cid:96)q|p) be a family of losses and let p ∈ Y.
(cid:98)L|p(h  x) = ((cid:98)luv)1≤u v≤Q ∈ RQ×Q  with(cid:98)luv

For h ∈ H and x ∈ X   we deﬁne

(cid:26) (cid:96)v|u(h  x)

if u = p and v (cid:54)= u 
otherwise.

.
=

0

The matrix L|p(h) ∈ RQ×Q is naturally given by

L|p(h) = EX|p(cid:98)L|p(h  X).

Remark 1. Note that the only row that may be nonzero in (cid:98)L|p(h  x) and L|p(h) is the pth row.

Deﬁnition 3 ((Conditional) Online Confusion Risk). Let y ∈ Y T be a ﬁxed sequence of labels and
Y a random sequence of T labels. Let h = {h0  . . .   hT−1} be a sequence of T hypotheses.
The conditional and non-conditional confusion risks C(cid:96) y(h) and C(cid:96) Y (h) are deﬁned as

(8)

(9)

C(cid:96) y(h)

.
=

L|yt(ht−1) 

and  C(cid:96) Y (h)

.
=

1
Tyt

L|Yt(ht−1).

1
TYt

(10)

Remark 2. C(cid:96) Y (h) is a random variable. In order to provide our generalization bounds  it will be
more convenient for us to work with the conditional confusion C(cid:96) y(h). A simple argument will
then allow us to provide generalization bounds for C(cid:96) Y (h).
Deﬁnition 4. Let y ∈ Y T be a ﬁxed sequence of labels. Let h = {h0  . . .   hT−1} be the hypotheses
output by A when run on Zy = {(Xt  yt)}T

The (non-)conditional empirical online confusion risks (cid:98)C(cid:96) y(h  X) and (cid:98)C(cid:96) Y (h  X) are
(cid:98)C(cid:96) y(h  X)
(cid:98)L|Yt(ht−1  Xt).

t=1  such that Xt is distributed according to DX|yt.

(cid:98)L|yt(ht−1  Xt) 

(cid:98)C(cid:96) Y (h  X)

T(cid:88)

T(cid:88)

and 

(11)

.
=

.
=

1
TYt

t=1

1
Tyt

t=1

T(cid:88)

t=1

T(cid:88)

t=1

We now are almost ready to provide our results. We just need to introduce a pivotal result that is a
corollary of Theorem 7.1 from [8]  a proof of which can be found in the appendix.
Corollary 1 (Rectangular Matrix Azuma). Consider a sequence {Uk} of d1 × d2 random matrices 
and a ﬁxed sequence of scalars {Mk} such that

Then  for all t > 0 

EUk|U1...Uk−1Uk = 0  and (cid:107)Uk(cid:107) ≤ Mk almost surely.

(cid:110)(cid:13)(cid:13)(cid:13)(cid:88)

P

Uk

k

(cid:13)(cid:13)(cid:13) ≥ t
(cid:111) ≤ (d1 + d2) exp

(cid:26)

(cid:27)

− t2
2σ2

  with σ2 .
=

(cid:88)

k

M 2
k .

3.2 New Results

This subsection reports our main results.
Theorem 1. Suppose that the losses in (cid:96) are such that 0 ≤ (cid:96)q|p ≤ M for some M > 0. Fix y ∈ Y T .
For any δ ∈ (0; 1]  it holds with probability 1 − δ over the draw of X ∼ DX|y that

(cid:13)(cid:13)(cid:13)C(cid:96) y(h) − (cid:98)C(cid:96) y(h  X)

(cid:13)(cid:13)(cid:13) ≤ M

(cid:115)

(cid:88)

2Q

1
Tp

p

log

Q
δ

(12)

where h = {h0  . . .   hT−1} is the set of hypotheses output by A when provided with {(Xt  yt)}T

t=1.

4

Therefore  with probability at least 1 − δ

(cid:107)C(cid:96) y(h)(cid:107) ≤(cid:13)(cid:13)(cid:13)(cid:98)C(cid:96) y(h  X)

(cid:115)

(cid:13)(cid:13)(cid:13) + M

(cid:88)

p

2Q

1
Tp

log

Q
δ

.

(13)

Proof. The proof is straightforward using Corollary 1. Indeed  consider the random variable

.
=

Ut

1
Tyt

L|yt(ht−1) − 1
Tyt

On the one hand  we observe that:

(cid:98)L|yt(ht−1  Xt).

since EXt|X 1:t−1y1:t(cid:98)L|yt(ht−1  Xt) = L|yt(ht−1). On the other hand  introducing

EXt|X 1:t−1 yUt = EXt|X 1:t−1 y1:t

Ut = 0 

= EXt|yt(cid:96)q|yt(ht−1  Xt) − (cid:96)q|yt(ht−1  Xt) 
.

∆t q

we observe that

(cid:107)Ut(cid:107) = sup
v:(cid:107)v(cid:107)≤1

(cid:107)Utv(cid:107)2 =

1
Tyt

sup

v:(cid:107)v(cid:107)≤1

∆t qvq

q(cid:54)=yt

(cid:12)(cid:12)(cid:12)(cid:88)

(cid:12)(cid:12)(cid:12) =

(cid:115)(cid:88)

q(cid:54)=yt

1
Tyt

(cid:112)Q.

t q ≤ M
∆2
Tyt

where we used that the only row of Ut not to be zero is its ytth row (see Remark 1)  the fact that
supv:(cid:107)v(cid:107)≤1 v · u = (cid:107)u(cid:107)2 and the assumption 0 ≤ (cid:96)q|p ≤ M  which gives that |∆t q| ≤ M.
Using Corollary 1 on the matrix martingale {Ut}  where (cid:107)Ut(cid:107) ≤ M

Q/Tyt almost surely  gives

√

Setting the right hand side to δ gives that  with probability at least 1 − δ

P

(cid:40)

Ut

t

(cid:110)(cid:13)(cid:13)(cid:13)(cid:88)
(cid:13)(cid:13)(cid:13)(cid:88)

(cid:13)(cid:13)(cid:13) ≥ ε
(cid:111) ≤ 2Q exp
(cid:115)
(cid:13)(cid:13)(cid:13) ≤ M
(cid:88)

2Q

Ut

t

(cid:41)

2M 2Q(cid:80)

ε2

−

1
T 2
yt

t

1
T 2
yt

t

ln

2Q
δ

.

(cid:80)
t T −2

yt

=(cid:80)

p T −1

p

gives (12). The triangle inequality |(cid:107)A(cid:107) − (cid:107)B(cid:107)| ≤ (cid:107)A − B(cid:107) gives (13).

If one takes a little step back to fully understand Theorem 1  it may not be as rejoicing as expected.
Indeed  it provides a bound on the norm of the average confusion risks of hypotheses h0  . . .   hT−1 
which  from a practical point of view  does not say much about the norm of the confusion risk
of a speciﬁc hypothesis.
In fact  as is usual in online learning [1]  it provides a bound on the
confusion risk of the Gibbs classiﬁer  which uniformly samples over h0  . . .   hT−1 before outputting
a prediction. Just as emphasized by [1]  things may turn a little bit more enticing when the loss
functions (cid:96) are convex with respect to their ﬁrst argument  i.e.
∀h  h(cid:48) ∈ H ∀p  q ∈ Y ∀λ ∈ [0  1]  (cid:96)q|p(λh + (1− λ)h(cid:48)  x) ≤ λ(cid:96)q|p(h) + (1− λ)(cid:96)q|p(h(cid:48)  x). (14)
In that case  we may show the following theorem  that is much more compatible with the stated goal
of trying to ﬁnd a hypothesis that has small (or at least  controlled) confusion risk.
Theorem 2. In addition to the assumptions of Theorem 1 we now assume that (cid:96) is made of convex
losses (as deﬁned in (14)).
For any δ ∈ (0; 1]  it holds with probability 1 − δ over the draw of X ∼ DX|y that

(cid:107)C(cid:96) (h)(cid:107) ≤(cid:13)(cid:13)(cid:13)(cid:98)C(cid:96) y(h  X)

(cid:13)(cid:13)(cid:13) + M

(cid:115)

(cid:88)

2Q

1
Tp

p

log

Q
δ

  with h

.
=

1
T

ht−1.

(15)

T(cid:88)

t=1

Proof. It is a direct consequence of the convexity of (cid:96) combined with Equation (6).

5

(cid:80)

It is now time to give the argument allowing us to state results for the non-conditional online
If a random event E(A  B) deﬁned with respect to random variables A and
confusion risks.
B is such that PA|B=b(E(A  b)) ≥ 1 − δ for all possible values of b then PAB(E(A  B)) =
b(1 − δ)PB(B = b) = 1 − δ. The results of Theorem 1

PA|B=b(E(A  b))PB(B = b) ≥ (cid:80)

and Theorem 2 may be therefore stated in terms of Y instead of y.
In light of the generic results of this subsection  we are now ready to motivate and derive a new
online learning algorithm that aims at a small confusion risk.

b

3.3 Online Learning with COPA

and the question as to how minimize this quantity.

This subsection presents a new online algorithm  COPA (for COnfusion Passive-Aggressive learn-
ing). Before giving the full detail of our algorithm  we further discuss implications of Theorem 2.
A ﬁrst message from Theorem 2 is that  provided the functions (cid:96) considered are convex  it is relevant
to use the average hypothesis h as a predictor. We indeed know by (15) that the confusion risk of h

is bounded by (cid:107)(cid:98)C(cid:96) y(h  X)(cid:107)  plus some quantity directly related to the number of training data en-
countered. The second message from the theorem is that the focus naturally comes to (cid:107)(cid:98)C(cid:96) y(h  X)(cid:107)
According to Deﬁnition 4  (cid:98)C(cid:96) y(h  X) is the sum of
(cid:98)L|yt(ht−1  Xt)/Tyt.
(cid:98)L|yt(h  Xt)/Tyt with respect to h to get ht  with the hope that the instantaneous risk of ht on
Xt+1 will be small: one may want to minimize the norm of (cid:98)L|yt(h  Xt)/Tyt and pose a problem
(cid:13)(cid:13)(cid:13)(cid:13) .
(cid:98)L|yt(h  Xt)
However  as remarked before  (cid:98)L|yt has only one nonzero row  its ytth. Therefore  the operator norm
of (cid:98)L|yt(h  Xt)/Tyt is simply the Euclidean norm of its ytth row. Trying to minimize the square

instantaneous confusion matrices
it does make sense to try to minimize each entry of

like the following:

In light of (6) 

min

h

Euclidean norm of this row might be written as

(cid:96)2
q|yt

(h  Xt).

(16)

W = {w1  . . .   wQ} with(cid:80)

This last equation is the starting point of COPA. To see how the connection is made  we make some
instantiations. The hypothesis space H is made of linear classiﬁers  so that a sequence of vectors
q wq = 0 deﬁnes a hypothesis hW . The family (cid:96) COPA builds upon is

Tyt

(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:88)

q(cid:54)=yt

min

h

1
T 2
yt

(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq  x(cid:105) +

(cid:12)(cid:12)(cid:12)(cid:12)+
(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq  x(cid:105) +

(cid:88)

q(cid:54)=y

(cid:96)q|p(hW   x) =

1

Q − 1

  ∀p  q ∈ Y.

In other words  COPA is an instance of Example 2. We focus on this setting because it is known that 
in the batch scenario  it provides Bayes consistent classiﬁers [3  4]. Given a current set of vectors
Q}  using (16)  and implementing a passive-aggressive learning strategy [2]  the
Wt = {wt
new set of vectors Wt+1 is searched as the solution of

1  . . .   wt

W (cid:80)

min
q wq=0

1
2

(cid:13)(cid:13)wq − wt

q

(cid:13)(cid:13)2

2

Q(cid:88)

q=1

+

C
2T 2
y

(cid:12)(cid:12)(cid:12)(cid:12)2

+

1

Q − 1

.

(17)

It turns out that it is possible to ﬁnd the solution of this optimization problem without having to
recourse to any involved optimization procedure. This is akin to a result obtained by [6]  which
applies for another family of loss functions. We indeed prove the following theorem (proof in
supplementary material).
Theorem 3 (COPA update). Suppose we want to solve

W (cid:80)

min
q wq=0

1
2

(cid:13)(cid:13)wq − wt

q

(cid:13)(cid:13)2

2

Q(cid:88)

q=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)wq  x(cid:105) +

(cid:88)

q(cid:54)=y

(cid:12)(cid:12)(cid:12)(cid:12)2

+

1

Q − 1

.

(18)

+

C
2

6

Algorithm 1 COPA
Input: z = {(xt  yt)}T
Output: W = {w1  . . .   wQ}  the classiﬁcation vectors

t=1 training set (realization of Z)  R number of epochs over z  C cost

receive (xt  yt)
compute α∗ according to (20)
∀q  perform the update: wτ +1
τ ← τ + 1

q ← wτ

q −(cid:16)

(cid:17)

(cid:80)I∗
q=1 α∗

q

xt

q − 1
α∗

Q

τ = 0
w0
1 = . . . = w0
Q
for r=1 to R do

for t=1 to T do

end for
end for
∀q  wq ← 1

τ

(cid:80)τ

k=1 wk
q

Let (cid:96)t

q be deﬁned as

(19)

(cid:88)

q∈I∗

(cid:96)t
q.

(20)

  where sα(I∗)

.
=

Q

κQ − I∗(cid:107)x(cid:107)2

x  q = 1  . . .   Q

(21)

Let σ be a permutation deﬁned over {1  . . .   Q − 1} taking values in Y\{y} such that

= (cid:104)wq  x(cid:105) + 1/(Q − 1).
.

(cid:96)t
q

σ(1) ≥ . . . ≥ (cid:96)t
(cid:96)t
Let I∗ be the largest index I ∈ {1  . . .   Q − 1} such that

σ(Q−1).

(cid:96)t
σ(I) +

(cid:107)x(cid:107)2

κQ − (I − 1)(cid:107)x(cid:107)2

σ(q) > 0  with κ
(cid:96)t

If I∗ is set to I∗ .

= {σ(1)  . . .   σ(I∗)}  then we may deﬁne α∗ = [α∗

+ (cid:107)x(cid:107)2

.
=

1
C
1 ··· α∗

Q] as

I−1(cid:88)

q=1

(cid:18)

 1

κ
0

α∗

q

.
=

(cid:19)

(cid:107)x(cid:107)2
Q

(cid:96)t
q +

sα(I∗)

and the vectors

w∗

q

.
= wt

q −

if q ∈ I∗
otherwise

q − 1
α∗
Q

(cid:32)

(cid:33)

α∗

q

I∗(cid:88)

q=1

are the solution of optimization problem (18). These equations provide COPA’s update procedure.

The full COPA algorithm is depicted in Algorithm 1.

4 Numerical Simulations

We here report results of preliminary simulations of COPA on a toy dataset. We generate a set of
5000 samples according to three Gaussian distributions each of variance σ2I with σ = 0.5. One
of the Gaussian is centered at (0  1)  the other at (1  0) and the last one at (−1  0). The respective
weights of the Gaussian distributions are 0.9  0.05 and 0.05. The ﬁrst generated sample is used
to choose the parameter C of COPA with a half split of the data between train and test; 10 other
samples of size 5000 are generated and split as 2500 for learning and 2500 for testing and the results
are averaged on the 10 samples. We compare the results of COPA to that of a simple multiclass
perceptron procedure (the number of epochs for each algorithm is set to 5). As recommended by the
theory we average the classiﬁcation vector to get our predictors (both for COPA and the perceptron).
The essential ﬁnding of these simulations is that COPA and the perceptron achieve similar rate of
classiﬁcation accuracy  0.85 and 0.86  respectively but the norm of the confusion matrix of COPA is
0.10 and that of the Perceptron is 0.18. This means COPA indeed does its job in trying to minimize
the confusion risk.

7

5 Conclusion

In this paper  we have provided new bounds for online learning algorithms aiming at controlling
their confusion risk. To the best of our knowledge  these results are the ﬁrst of this kind. Motivated
by the theoretical results  we have proposed a passive-aggressive learning algorithm  COPA  that
has the seducing property that its updates can be computed easily  without having to resort to any
optimization package. Preliminary numerical simulations tend to support the effectiveness of COPA.
In addition to complementary numerical simulations  we want to pursue our work in several direc-
tions. First  we would like to know whether efﬁcient update equation can be derived if a simple
hinge  instead of a square hinge is used. Second  we would like to see if a full regret-style analysis
can be made to study the properties of COPA. Also  we are interested in comparing the merits of our
theoretical results with those recently proposed in [5] and [7]  which propose to address learning
with the confusion matrix from the algorithmic stability and the PAC-Bayesian viewpoints. Finally 
we would like to see how the proposed material can be of some use in the realm of structure predic-
tion and by extension  in the case where the confusion matrix to consider is inﬁnite-dimensional.
Acknowledgments. Work partially supported by Pascal 2 NoE ICT-216886-NOE  French ANR
Projects ASAP (ANR-09-DEFIS–001) and GRETA (ANR-12-BS02-0004).

Appendix
Theorem 4 (Matrix Azuma-Hoeffding  [8]). Consider a ﬁnite sequence {Xk} of self-adjoint ma-
trices in dimension d  and a ﬁxed sequence {Ak} of self-adjoint matrices that satisfy Ek−1Xk = 0
and

(cid:52) A2

X 2
k

k  and AkXk = XkAk  almost surely.

Then  for all t ≥ 0 

with σ2 =(cid:13)(cid:13)(cid:80)

k A2
k

P

(cid:13)(cid:13) .

(cid:110)

(cid:16)(cid:88)

λmax

Xk

k

(cid:17) ≥ t

(cid:111) ≤ d · e−t2/2σ2

 

Proof of Corollary 1. To prove the result  it sufﬁces to make use of the dilation technique and apply
Theorem 4. The self-adjoint dilation D(U ) of a matrix U ∈ Rd1×d2 is the self-adjoint matrix D(U )
of order d1 + d2 deﬁned by

(cid:21)

(cid:20) 0 U

U∗

0

D(U ) =

where U∗ is the adjoint of U (as U has only real coefﬁcient here  U∗ is the transpose of U).
As recalled in [8]  (cid:107)D(U )(cid:107) = (cid:107)U(cid:107) and  therefore  the largest eigenvalue λmax of D2(U ) is equal to
(cid:107)U(cid:107)2 and D2(U ) (cid:52) (cid:107)U(cid:107)2I.
Considering Uk  we get that  almost surely:

D2(Uk) (cid:52) M 2

I 

k

and since dilation is a linear operator  we have that

EUk|U1···Uk−1D(Uk) = 0.

The sequence of matrices {D(Uk)} is therefore a matrix martingale that veriﬁes the assumption of
Theorem 4  the application of which gives
D(Uk)

(cid:16)(cid:88)

λmax

P

(cid:17) ≥ t

(cid:110)
k . Thanks to the linearity of D 
= λmax
λmax

(cid:16)(cid:88)

D(Uk)

(cid:17)

(cid:111) ≤ (d1 + d2) exp
(cid:17)(cid:17)
(cid:16)D(cid:16)(cid:88)

=

Uk

k

with σ2 =(cid:80)

(cid:26)
(cid:13)(cid:13)(cid:13)(cid:88)

 

(cid:27)
(cid:13)(cid:13)(cid:13)  

− t2
2σ2

k M 2

Uk

k

k

k

which closes the proof.

8

References
[1] N. Cesa-Bianchi  A. Conconi  and C. Gentile. On the generalization ability of online learning

algorithms. IEEE Transactions on Information Theory  50(9):2050–2057  2004.

[2] Koby Crammer  Ofer Dekel  Joseph Keshet  Shai Shalev-Shwartz  and Yoram Singer. Online

passive-aggressive algorithms. Journal of Machine Learning Research  7:551–585  2006.

[3] Y. Lee. Multicategory support vector machines  theory  and application to the classiﬁcation of

microarray data and satellite radiance data. Technical report  University of Wisconsin  2002.

[4] Y. Lee  Y. Lin  and G. Wahba. Multicategory support vector machines. Journal of the American

Statistical Association  99:67–81  march 2004.

[5] P. Machart and L. Ralaivola. Confusion matrix stability bounds for multiclass classiﬁcation.

Technical report  Aix-Marseille University  2012.

[6] S. Matsushima  N. Shimizu  K. Yoshida  T. Ninomiya  and H. Nakagawa. Exact passive-
aggressive algorithm for multiclass classiﬁcation using support class. In SDM 10  pages 303–
314  2010.

[7] E. Morvant  S. Koc¸o  and L. Ralaivola. PAC-Bayesian Generalization Bound on Confusion
Matrix for Multi-Class Classiﬁcation. In John Langford and Joelle Pineau  editors  International
Conference on Machine Learning  pages 815–822  Edinburgh  United Kingdom  2012.

[8] J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-

tional Mathematics  pages 1–46  2011.

9

,Durk Kingma
Tim Salimans
Max Welling