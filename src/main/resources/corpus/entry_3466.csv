2019,Minimum Stein Discrepancy Estimators,When maximum likelihood estimation is infeasible  one often turns to score matching  contrastive divergence  or minimum probability flow to obtain tractable parameter estimates. We provide a unifying perspective of these techniques as minimum Stein discrepancy estimators  and use this lens to design new diffusion kernel Stein discrepancy (DKSD) and diffusion score matching (DSM) estimators with complementary strengths. We establish the consistency  asymptotic normality  and robustness of DKSD and DSM estimators  then derive stochastic Riemannian gradient descent algorithms for their efficient optimisation. The main strength of our methodology is its flexibility  which allows us to design estimators with desirable properties for specific models at hand by carefully selecting a Stein discrepancy.
 We illustrate this advantage for several challenging problems for score matching  such as non-smooth  heavy-tailed or light-tailed densities.,Minimum Stein Discrepancy Estimators

Alessandro Barp

Department of Mathematics
Imperial College London

a.barp16@imperial.ac.uk

François-Xavier Briol

Department of Statistical Science

University College London

f.briol@ucl.ac.uk

Andrew B. Duncan

Department of Mathematics
Imperial College London

a.duncan@imperial.ac.uk

Mark Girolami

Department of Engineering
University of Cambridge
mag92@eng.cam.ac.uk

Abstract

Lester Mackey

Microsoft Research
Cambridge  MA  USA

lmackey@microsoft.com

When maximum likelihood estimation is infeasible  one often turns to score match-
ing  contrastive divergence  or minimum probability ﬂow to obtain tractable param-
eter estimates. We provide a unifying perspective of these techniques as minimum
Stein discrepancy estimators  and use this lens to design new diffusion kernel
Stein discrepancy (DKSD) and diffusion score matching (DSM) estimators with
complementary strengths. We establish the consistency  asymptotic normality  and
robustness of DKSD and DSM estimators  then derive stochastic Riemannian gra-
dient descent algorithms for their efﬁcient optimisation. The main strength of our
methodology is its ﬂexibility  which allows us to design estimators with desirable
properties for speciﬁc models at hand by carefully selecting a Stein discrepancy.
We illustrate this advantage for several challenging problems for score matching 
such as non-smooth  heavy-tailed or light-tailed densities.

1

Introduction

Maximum likelihood estimation [9] is a de facto standard for estimating the unknown parameters in a
statistical model {Pθ : θ ∈ Θ}. However  the computation and optimization of a likelihood typically
requires access to the normalizing constants of the model distributions. This poses difﬁculties for
complex statistical models for which direct computation of the normalisation constant would entail
prohibitive multidimensional integration of an unnormalised density. Examples of such models
arise naturally in modelling images [27  39]  natural language [54]  Markov random ﬁelds [61]
and nonparametric density estimation [63  69]. To by-pass this issue  various approaches have
been proposed to address parametric inference for unnormalised models  including Monte Carlo
maximum likelihood [22]  contrastive divergence [28]  minimum probability ﬂow learning [62] 
noise-contrastive estimation [10  26  27] and score matching (SM) [34  35].
The SM estimator is a minimum score estimator [16] based on the Hyvärinen scoring rule that avoids
normalizing constants by depending on Pθ only through the gradient of its log density ∇x log pθ. SM
estimators have proven to be a widely applicable method for estimation for models with unnormalised
smooth positive densities  with generalisations to bounded domains [35] and compact Riemannian
manifolds [51]. Despite the ﬂexibility of this approach  SM has three important and distinct limitations.
Firstly  as the Hyvärinen score depends on the Laplacian of the log-density  SM estimation will be
expensive in high dimension and will break down for non-smooth models or for models in which
the second derivative grows very rapidly. Secondly  as we shall demonstrate  SM estimators can
behave poorly for models with heavy tailed distributions. Thirdly  the SM estimator is not robust to

33rd Conference on Neural Information Processing Systems (NeurIPS 2019)  Vancouver  Canada.

outliers in many applications of interest. Each of these situations arise naturally for energy models 
particularly product-of-experts models and ICA models [33].
In a separate strand of research  new approaches have been developed to measure discrepancy between
an unnormalised distribution and a sample. In [23  25  50  24]  it was shown that Stein’s method can
be used to construct discrepancies that control weak convergence of an empirical measure to a target.
In this paper we consider minimum Stein discrepancy (SD) estimators and show that SM  minimum
probability ﬂow and contrastive divergence estimators are all special cases. Within this class we
focus on SDs constructed from reproducing kernel Hilbert Spaces (RKHS)  establishing the consis-
tency  asymptotic normality and robustness of these estimators. We demonstrate that these SDs are
appropriate for estimation of non-smooth distributions and heavy- or light- tailed distributions. The
remainder of the paper is organized as follows. In Section 2 we introduce the class of minimum SD
estimators  then investigate asymptotic properties of SD estimators based on kernels in Section 3 
demonstrating consistency and asymptotic normality under general conditions  as well as conditions
for robustness. Section 4 presents three toy problems where SM breaks down  but our new estimators
are able to recover the truth. All proofs are in the supplementary materials.

2 Minimum Stein Discrepancy Estimators
Let PX the set of Borel probability measures on X . Given identical and independent (IID) realisations
from Q ∈ PX on an open subset X ⊂ Rd  the objective is to ﬁnd a sequence of measures Pn
that approximate Q in an appropriate sense. More precisely we will consider a family PΘ =
{Pθ : θ ∈ Θ} ⊂ PX together with a function D : PX × PX → R+ which quantiﬁes the
discrepancy between any two measures in PX   and wish to estimate an optimal parameter θ∗
satisfying θ∗ ∈ arg minθ∈Θ D(Q(cid:107)Pθ). In practice  it is often difﬁcult to compute the discrepancy
D explicitly  and it is useful to consider a random approximation ˆD({Xi}n
i=1(cid:107)Pθ) based on a IID
sample X1  . . .   Xn ∼ Q  such that ˆD({Xi}n
i=1(cid:107)Pθ) a.s.−−→ D(Q(cid:107)Pθ) as n → ∞. We then consider
the sequence of estimators

n ∈ argminθ∈Θ
ˆθD

ˆD({Xi}n

i=1(cid:107)Pθ).

The choice of discrepancy will impact the consistency  efﬁciency and robustness of the estimators.
Examples of such estimators include minimum distance estimators [4  58] where the discrepancy
will be a metric on probability measures  including minimum maximum mean discrepancy (MMD)
estimation [18  42  8] and minimum Wasserstein estimation [19  21  6].
More generally  minimum scoring rule estimators [16] arise from proper scoring rules  for ex-
ample Hyvärinen  Bregman and Tsallis scoring rules. These discrepancies are often statistical
divergences  i.e.  D(Q(cid:107)P) = 0 ⇔ P = Q for all P  Q in a subset of PX . Suppose that Pθ
and Q are absolutely continuous with respect to a common measure λ on X   with respective
positive densities pθ and q. Then a well-known statistical divergence is the Kullback-Leibler
X log pθdQ. Minimising
X log pθdQ  which can be estimated using the likelihood
i=1 log pθ(Xi). Informally  we see that minimising the KL-divergence is

(KL) divergence KL(Q(cid:107)Pθ) ≡ (cid:82)
KL(Q(cid:107)Pθ) is equivalent to maximising(cid:82)
(cid:99)KL({Xi}n

X log(dQ/dPθ)dQ = (cid:82)

X log qdQ −(cid:82)

equivalent to performing maximum likelihood estimation.
For our purposes we are interested in discrepancies that can be evaluated when Pθ is only known
up to normalisation  precluding the use of KL divergence. We instead consider a related class of
discrepancies based on integral probability pseudometric (IPM) [55] and Stein’s method [3  11  65].
Let Γ(Y) ≡ Γ(X  Y) ≡ {f : X → Y}. A map SP : G ⊂ Γ(Rd) → Γ(R) is a Stein operator
X SP[f ]dP = 0 ∀f ∈ G for any P. We can then deﬁne an associated Stein
discrepancy (SD) [23] using an IPM with entry-dependent function space F ≡ SPθ [G]

over a Stein class G if(cid:82)

i=1(cid:107)Pθ) ≡ 1

(cid:80)n

n

(cid:12)(cid:12)(cid:82)

X f dPθ −(cid:82)

X f dQ(cid:12)(cid:12) = supg∈G

(cid:12)(cid:12)(cid:82)

X SPθ [g]dQ(cid:12)(cid:12).

θ [G](Q(cid:107)Pθ) ≡ supf∈SP

SDSP

(1)
The Stein discrepancy depends on Q only through expectations  and does not require the existence of
a density  therefore permitting Q to be an empirical measure. If P has a C 1 density p on X   one can
consider the Langevin-Stein discrepancy arising from the Stein operator Tp[g] ≡ (cid:104)∇ log p  g(cid:105) + ∇ ·
g [23  25]. In this case  the Stein discrepancy will not depend on the normalising constant of p.

θ [G]

2

In this paper  for an arbitrary m ∈ Γ(Rd×d) which we call diffusion matrix  we shall consider the
more general diffusion Stein operators [25]: S m
p [A] ≡ (1/p)∇ · (pmA) 
where g ∈ Γ(Rd)  A ∈ Γ(Rd×d)  and the associated minimum Stein discrepancy estimators which
i=1 ∼ Q  we will focus on the estimators
minimise (1). As we will only have access to a sample {Xi}n

p [g] ≡ (1/p)∇ · (pmg)   S m

minimising an approximation (cid:99)SDSP

i=1(cid:107)Pθ) based on a U-statistic of the Q-integral:

θ [G]({Xi}n

n ≡ argminθ∈Θ(cid:99)SDSP

ˆθStein

θ [G]({Xi}n

i (cid:107)Pθ).

Related and complementary approaches to inference using SDs include the nonparametric estimator
of [41]  the density ratio approach of [47] and the variational inference algorithms of [49  60]. We
now highlight several instances of SDs which will be studied in detail in this paper.

2.1 Example 1: Diffusion Kernel Stein Discrepancy Estimators

: Γ(cid:0)X × X   Rd×d(cid:1) → Γ(cid:0)R(cid:1) which acts ﬁrst on the ﬁrst variable and then

A convenient choice of Stein class is the unit ball of reproducing kernel Hilbert spaces (RKHS)
[5] of a scalar kernel function k. For the Langevin Stein operator Tp  the resulting kernel Stein
discrepancy (KSD) ﬁrst appeared in [57] and has since been considered extensively in the context
of hypothesis testing  measuring sample quality and approximation of probability measures in [12–
14  17  24  44  46  43]. In this paper  we consider a more general class of discrepancies based on the
diffusion Stein operator and matrix-valued kernels.
Consider an RKHS Hd of functions f ∈ Γ(Rd) with (matrix-valued) kernel K ∈ Γ(X × X   Rd×d) 
Kx ≡ K(x ·) (see Appendix A.3 and A.4 for further details). The Stein operator S m
p [f ] induces
an operator S m 2
on the second one. We brieﬂy mention two simple examples of matrix kernels constructed from
scalar kernels. If we want the components of f to be orthogonal  we can use the diagonal kernel
(i) K = diag(λ1k1  . . .   λdkd) where λi > 0 and ki is a C 2 kernel on X   for i = 1  . . .   d; else we
can “correlate" the components by setting (ii) K = Bk where k is a (scalar) kernel on X and B is a
(constant) symmetric positive deﬁnite matrix.
We propose to study diffusion kernel Stein discrepancies indexed by K and m (see Appendix B):
Theorem 1 (Diffusion Kernel Stein Discrepancy). For any kernel K  we ﬁnd that S m
(cid:104)S m 1

p Kx  f(cid:105)Hd for any f ∈ Hd. Moreover if x (cid:55)→ (cid:107)S m 1

p S m 1

p [f ](x) =

p

DKSDK m(Q(cid:107)P)2 ≡ sup h∈Hd
(cid:107)h(cid:107)≤1

k0(x  y) ≡ S m 2

p S m 1

p K(x  y) =

X

(cid:82)

X S m

=(cid:82)

X k0(x  y)dQ(x)dQ(y)

p [h]dQ(cid:12)(cid:12)2

p Kx(cid:107)Hd ∈ L1(Q)  we have

(cid:12)(cid:12)(cid:82)
p(y)p(x)∇y · ∇x ·(cid:0)p(x)m(x)K(x  y)m(y)(cid:62)p(y)(cid:1).
(cid:80)

1

(2)

(cid:80)
n(n−1)
i=1(cid:107)Pθ)2.

1

2

p

n

i(cid:54)=j k0

1≤i<j≤n k0

i=1(cid:107)Pθ)2 =

n(n−1)
∈ argminθ∈Θ

θ(Xi  Xj) =
(cid:92)DKSDK m({Xi}n

In order to use these for minimum SD estimation  we propose the following U-statistic approximation:
(cid:92)DKSDK m({Xi}n
θ(Xi  Xj)  (3)
with associated estimators: ˆθDKSD
As the proof shows  the Stein kernel k0 is indeed a (scalar) kernel obtained from the feature map
φ : X → Hd  φ(x) ≡ S m 1
[K]|x. For K = Ik  m = Ih  DKSD is a KSD with scalar kernel
h(x)k(x  y)h(y)  and if h = 1 our objective becomes the usual Langevin-based KSD of [14  24  46 
57] (see Appendix B.4). The work of [45] discussed the potential of optimizing the KSD with gradient
descent but did not evaluate its merits. In the sections to follow  we will see the advantages conferred
by introducing more ﬂexible diffusion operators  matrix kernels  and Riemannian optimization.
Now that our DKSD estimators are deﬁned  an important remaining question is under which conditions
can DKSD discriminate distinct probability measures. To answer  we will need several deﬁnitions.
[K]dQ = 0  and that it is strictly
X×X dµ(cid:62)(x)K(x  y)dµ(y) > 0 for any ﬁnite non-zero signed
p Kx  f(cid:105)Hd we have that f ∈ Hd is in the Stein

We say a matrix kernel K is in the Stein class of Q if(cid:82)
integrally positive deﬁnite (IPD) if(cid:82)
class (i.e. (cid:82)

q [f ]dQ = 0) when K is also in the class. Setting sp ≡ m(cid:62)∇ log p ∈ Γ(Rd):

vector Borel measure µ. From S m

p [f ](x) = (cid:104)S m 1

Proposition 1 (DKSD as a Statistical Divergence). Suppose K is IPD and in the Stein class of Q 
and m(x) is invertible. If sp − sq ∈ L1(Q)  then DKSDK m(Q(cid:107)P)2 = 0 iff Q = P.

X S m 1

X S m

q

3

See Appendix B.5 for the proof. Note that this proposition generalises Proposition 3.3 from [46] to
a signiﬁcantly larger class of SD. For the matrix kernels introduced above  the proposition below
shows that K is IPD when its associated scalar kernels are; a well-studied problem [64].
Proposition 2 (IPD Matrix Kernels). (i) Let K = diag(k1  . . .   kd). Then K is IPD iff each kernel
ki is IPD. (ii) Let K = Bk for B be symmetric positive deﬁnite. Then K is IPD iff k is IPD.

2.2 Example 2: Diffusion Score Matching Estimators

A well-known family of estimators are the score matching (SM) estimators (based on the Fisher
or Hyvarinen divergence) [34  35]. As will be shown below  these can be seen as special cases of
minimum SD estimators. The SM discrepancy is computable for sufﬁciently smooth densities:

2 dQ =(cid:82)

X

SM(Q(cid:107)P) ≡(cid:82)
X (cid:107)∇ log p − ∇ log q(cid:107)2
we consider the approximation(cid:100)SM({Xi}n
argminθ∈Θ(cid:100)SM({Xi}n

where ∆ denotes the Laplacian and we have used the divergence theorem. If P = Pθ  the ﬁrst
integral above does not depend on θ  and the second one does not depend on the density of Q  so
2(cid:107)∇ log pθ(Xi)(cid:107)2
n ≡

based on an unbiased estimation for the minimiser of the SM divergence  and its estimators ˆθSM

i=1 ∆ log pθ(Xi) + 1

i=1(cid:107)Pθ) ≡ 1

n

2

2 + 2∆ log p(cid:1)dQ

2 + (cid:107)∇ log p(cid:107)2

i=1(cid:107)Pθ)  for independent random vectors Xi ∼ Q.

(cid:0)(cid:107)∇ log q(cid:107)2
(cid:80)n

The SM discrepancy can also be generalised to include higher-order derivatives of the log-likelihood
[48] and does not require a normalised model. We will now introduce a further generalisation that we
call diffusion score matching (DSM) which is a SD constructed from the diffusion Stein operator (see
Appendix B.6):
Theorem 2 (Diffusion Score Matching). Let X = Rd and consider some diffusion Stein operator
S m
p for some function m ∈ Γ(Rd×d) and the Stein class G ≡ {g = (g1  . . .   gd) ∈ C 1(X   Rd) ∩
L2(X ; Q) : (cid:107)g(cid:107)L2(X ;Q) ≤ 1}. If p  q > 0 are differentiable and sp − sq ∈ L2(Q)  then we deﬁne the
diffusion score matching divergence as the Stein discrepancy 

This satisﬁes DSMm(Q(cid:107)P) = 0 iff Q = P when m(x) is invertible. Moreover  if p is twice-
differentiable  and qmm(cid:62)∇ log p ∇ · (qmm(cid:62)∇ log p) ∈ L1(Rd)  then Stoke’s theorem gives

DSMm(Q(cid:107)P) ≡ supf∈Sp[G]

DSMm(Q(cid:107)P) =(cid:82)

X

(cid:12)(cid:12)(cid:82)
X f dQ −(cid:82)
(cid:0)(cid:107)m(cid:62)∇x log p(cid:107)2

X f dP(cid:12)(cid:12)2

=(cid:82)

X

2 + (cid:107)m(cid:62)∇ log q(cid:107)2

(cid:13)(cid:13)m(cid:62)(∇ log q − ∇ log p)(cid:13)(cid:13)2
2 + 2∇ ·(cid:0)mm(cid:62)∇ log p(cid:1)(cid:1)dQ.

2dQ.

n

Notably  DSMm recovers SM when m(x)m(x)(cid:62) = I and the (generalised) non-negative score
matching estimator of [48] with the choice m(x) ≡ diag(h1(x1)1/2  . . .   hd(xd)1/2). Like standard
SM  DSM is only deﬁned for distributions with sufﬁciently smooth densities. Since the θ-dependent
part of DSMm(Q(cid:107)Pθ) does not depend on the density of Q  and can be estimated using an empirical
mean  leading to the estimators ˆθDSM

(cid:91)DSMm({Xi}n

(cid:80)n
≡ argminθ∈Θ

(cid:0)(cid:107)m(cid:62)∇x log pθ(cid:107)2

i=1(cid:107)Pθ) for

2 + 2∇ ·(cid:0)mm(cid:62)∇ log pθ

i=1

(cid:1)(cid:1)(Xi)

(cid:91)DSMm({Xi}n

i=1(cid:107)Pθ) ≡ 1

n

where {Xi}n
i=1 is a sample from Q. Note that this is only possible if m is independent of θ  in
contrast to DKSD where m can depend on X × Θ  thus leading to a more ﬂexible class of estimators.
An interesting remark is that the DSMm discrepancy may in fact be obtained as a limit of DKSD
over a sequence of target-dependent kernels: see Appendix B.6 for the complete result which corrects
and signiﬁcantly generalises previously established connections between the SM divergence and
KSD (such as in Sec. 5 of [46]).
We conclude by commenting on the computational complexity. Evaluating the DKSD loss function
requires O(n2d2) computation  due to the U-statistic and a matrix-matrix product. However  if
K = diag(λ1k1  . . .   λdkd) or K = Bk  and if m is a diagonal matrix  then we can by-pass
expensive matrix products and the cost is O(n2d)  making it comparable to that of KSD. Although
we do not consider these in this paper  recent approximations to KSD could also be adapted to DKSD
to reduce the computational cost to O(nd) [32  36]. The DSM loss function has computational cost
O(nd2)  which is comparable to the SM loss. From a computational viewpoint  DSM will hence be
preferable to DKSD for large n  whilst DKSD will be preferable to DSM for large d.

4

2.3 Further Examples: Contrastive Divergence and Minimum Probability Flow

Before analysing DKSD and DSM estimators further  we show that the class of minimum SD
θ   n ∈ N be a
estimators also includes other well-known estimators for unnormalised models. Let X n
Markov process with unique invariant probality measure Pθ  for example a Metropolis-Hastings chain.
θ = x]. Choosing the
Let P n
θ and Stein class G = {log pθ + c : c ∈ R}  leads to the following SD:
Stein operator Sp = I − P n
θ(cid:107)Pθ) 

θ )|X 0
θ log pθ)dQ = KL(Q(cid:107)Pθ) − KL(Qn

θ be the associated transition semigroup  i.e. (P n

CD(Q(cid:107)Pθ) =(cid:82)

θ f )(x) = E[f (X n

θ |X 0

θ is the law of X n

θ (cid:28) Pθ  which is the loss
where Qn
function associated with contrastive divergence (CD) [28  45]. Suppose now that X is a ﬁnite set.
Given θ ∈ Θ let Pθ be the transition matrix for a Markov process with unique invariant distribution
Pθ. Suppose we observe data {xi}n
i=1 and let q be the corresponding empirical distribution. Choosing
the Stein operator Sp = I − Pθ and the Stein set G = {f ∈ Γ(R) : (cid:107)f(cid:107)∞ ≤ 1}. Note that 
g ∈ arg supg∈G |Q(Sp[g])| will satisfy g(i) = sgn(q(cid:62)(I − Pθ)i)  and the resulting Stein discrepancy
is the minimum probability ﬂow loss objective function [62]:

X (log pθ − P n
θ ∼ Q and assuming that Q (cid:28) Pθ and Qn

MPFL(Q(cid:107)P) =(cid:80)

(cid:12)(cid:12)((I − Pθ)(cid:62)q)y

(cid:12)(cid:12) =(cid:80)

y

(cid:12)(cid:12)(cid:12) 1

n

(cid:80)

y(cid:54)∈{xi}n

i=1

x∈{xi}n

i=1

(I − Pθ)xy

(cid:12)(cid:12)(cid:12).

2.4

Implementing Minimum SD Estimators: Stochastic Riemannian Gradient Descent

In order to implement the minimum SD estimators  we propose to use a stochastic gradient descent
(SGD) algorithm associated to the information geometry induced by the SD on the parameter space.
More precisely  consider a parametric family PΘ of probability measures on X with Θ ⊂ Rm.
Given a discrepancy D : PΘ × PΘ → R satisfying D(Pα(cid:107)Pθ) = 0 iff Pα = Pθ (called a statistical
divergence)  its associated information matrix ﬁeld on Θ is deﬁned as the map θ (cid:55)→ g(θ)  where g(θ)
is the symmetric bilinear form g(θ)ij = − 1
2 (∂2/∂αi∂θj)D(Pα(cid:107)Pθ)|α=θ [2]. When g is positive
deﬁnite  we can use it to perform (Riemannian) gradient descent on the parameter space Θ. We
provide below the information matrices of DKSD and DSM (and hence extends results of [37]):
Proposition 3 (Information Tensor DKSD). Assume the conditions of Proposition 1 hold. The
information tensor associated to DKSD is positive semi-deﬁnite and has components

θ (y)∇y∂θi log pθ(y)dPθ(x)dPθ(y).
Proposition 4 (Information Tensor DSM). Assume the conditions of Theorem 2 hold. The infor-
mation tensor deﬁned by DSM is positive semi-deﬁnite and has components

mθ(x)K(x  y)m(cid:62)

X

gDKSD(θ)ij =(cid:82)

(cid:82)
(cid:62)
X (∇x∂θj log pθ(x))
gDSM(θ)ij =(cid:82)

(cid:10)m(cid:62)∇∂θi log pθ  m(cid:62)∇∂θj log pθ

(cid:11)dPθ.

X

See Appendix C for the proofs. Given an (information) Riemannian metric  recall the gradient ﬂow
of a curve θ on the Riemannian manifold Θ is the solution to ˙θ(t) = −∇θ(t) SD(Q(cid:107)Pθ)  where ∇θ
denotes the Riemannian gradient at θ. It is the curve that follows the direction of steepest decrease
(measured with respect to the Riemannian metric) of the function SD(Q(cid:107)Pθ) (see Appendix A.5).
The well-studied natural gradient descent [1  2] corresponds to the case in which the Riemannian
manifold is Θ = Rm equipped with the Fisher metric and SD is replaced by KL. When Θ is
a linear manifold with coordinates (θi) we have ∇θ SD(Q(cid:107)Pθ) = g(θ)−1dθ SD(Q(cid:107)Pθ)  where
i}i)−1dθt(cid:99)SD({X t
dθf denotes the tuple (∂θif ). We will approximate this at step t of the descent using the biased
i=1(cid:107)Pθ)  where ˆgθt({X t
i}n
i}n
estimator ˆgθt({X t
i=1) is an unbiased estimator for the
information matrix g(θt) and {X t
i ∼ Q}i is a sample at step t. In general  we have no guarantee
that ˆgθt is invertible  and so we may need a further approximation step to obtain an invertible matrix.
i=1)−1dθt(cid:99)SD({X t
Given a sequence (γt) of step sizes we will approximate the gradient ﬂow with
i}n
i}n
i=1(cid:107)Pθ).

ˆθt+1 = ˆθt − γtˆgθt({X t

Minimum SD estimators hold additional appeal for exponential family models  since their densities
have the form pθ(x) ∝ exp((cid:104)θ  T (x)(cid:105)Rm ) exp(b(x)) for natural parameters θ ∈ Rm  sufﬁcient
statistics T ∈ Γ(Rm)  and base measure exp(b(x)). For these models  the U-statistic approximations
of DKSD and DSM are convex quadratics with closed form solutions whenever K and m are
independent of θ. Moreover  since the absolute value of an afﬁne function is convex  and the
supremum of convex functions is convex  any SD with a diffusion Stein operator is convex in θ 
provided m and the Stein class G are independent of θ.

5

3 Theoretical Properties for Minimum Stein Discrepancy Estimators

∗

a.s.−−→ θDKSD

We now show that the DKSD and DSM estimators have many desirable properties such as consistency 
asymptotic normality and bias-robustness. These results do not only provide us with reassuring
theoretical guarantees on the performance of our algorithms  but can also be a practical tool for
choosing a Stein operator and Stein class given an inference problem of interest.
almost sure convergence:
We begin by establishing strong consistency and for DKSD; i.e.
≡ argminθ∈Θ DKSDK m(Q(cid:107)Pθ)2. This will be followed by a proof of asymp-
ˆθDKSD
∈ PΘ. In the
n
totic normality. We will assume we are in the speciﬁed setting  so that Q = PθDKSD
misspeciﬁed setting  we will need to also assume the existence of a unique minimiser.
Theorem 3 (Strong Consistency of DKSD). Let X = Rd  Θ ⊂ Rm. Suppose that K is bounded
with bounded derivatives up to order 2  that k0(x  y) is continuously-differentiable on an Rm-open
neighbourhood of Θ  and that for any compact subset C ⊂ Θ there exist functions f1  f2  g1  g2 such
that for Q-a.e. x ∈ X  

1. (cid:13)(cid:13)m(cid:62)(x)∇ log pθ(x)(cid:13)(cid:13) ≤ f1(x)  where f1 ∈ L1(Q) and continuous 
(cid:0)m(x)(cid:62)∇ log pθ(x)(cid:1)(cid:13)(cid:13) ≤ g1(x)  where g1 ∈ L1(Q) is continuous 
2. (cid:13)(cid:13)∇θ

∗

3. (cid:107)m(x)(cid:107) + (cid:107)∇xm(x)(cid:107) ≤ f2(x) where f2 ∈ L1(Q) and continuous 
4. (cid:107)∇θm(x)(cid:107) + (cid:107)∇θ∇xm(x)(cid:107) ≤ g2(x) where g2 ∈ L1(Q) is continuous.
Assume further that θ (cid:55)→ Pθ is injective. Then we have a unique minimiser θDKSD
  and if either Θ is
i=1(cid:107)Pθ)2 are convex  then ˆθDKSD
compact  or θDKSD
is
strongly consistent.
Theorem 4 (Central Limit Theorem for DKSD). Let X and Θ be open subsets of Rd and Rm
respectively. Let K be a bounded kernel with bounded derivatives up to order 2 and suppose that
ˆθDKSD
such that
n
θ → (cid:92)DKSDK m({Xi}n
i=1  Pθ)2 is twice continuously differentiable for θ ∈ N and  for Q-a.e.
x ∈ X  

and that there exists a compact neighbourhood N ⊂ Θ of θDKSD

∈ int(Θ) and Θ and θ (cid:55)→ (cid:92)DKSDK m({Xi}n

p−→ θDKSD

∗

∗

∗

∗

n

(cid:0)m(x)(cid:62)∇ log pθ(x)(cid:1)(cid:107) ≤ g1(x) 

1. (cid:107)m(cid:62)(x)∇ log pθ(x)(cid:107) + (cid:107)∇θ
2. (cid:107)m(x)(cid:107) + (cid:107)∇xm(x)(cid:107) + (cid:107)∇θm(x)(cid:107) + (cid:107)∇θ∇xm(x)(cid:107) ≤ f2(x) 
3. (cid:107)∇θ∇θ
4. (cid:107)∇θ∇θm(x)(cid:107) + (cid:107)∇θ∇θ∇xm(x)(cid:107) + (cid:107)∇θ∇θ∇θm(x)(cid:107) + (cid:107)∇θ∇θ∇θ∇xm(x)(cid:107) ≤ g2(x) 
where f1  f2 ∈ L2(Q) g1  g2 ∈ L1(Q) are continuous. Suppose also that the information tensor g is
invertible at θDKSD

(cid:0)m(x)(cid:62)∇ log pθ(x)(cid:1)(cid:107) + (cid:107)∇θ∇θ∇θ
(cid:16)ˆθDKSD
(cid:17) d−→ N(cid:0)0  g−1
(cid:16)(cid:82)
where ΣDKSD =(cid:82)
− θDKSD
∗
X ∇θk0

∗
(x  z)dQ(z)

(cid:17) ⊗(cid:16)(cid:82)

∗
X ∇θk0

)ΣDKSDg−1

(x  y)dQ(y)

DKSD(θDKSD

DKSD(θDKSD

)(cid:1) 

dQ(x).

. Then

∗
√

(cid:17)

X

n

n

(cid:0)m(x)(cid:62)∇ log pθ(x)(cid:1)(cid:107) ≤ f1(x) 

θDKSD
∗

θDKSD
∗

See Appendix D for proofs. For both results  the assumptions on the kernel are satisﬁed by most
kernels common in the literature  such as Gaussian  inverse-multiquadric (IMQ) and any Matérn
kernels with smoothness greater than 2. Similarly  the assumptions on the model are very weak given
that the diffusion tensor m can be adapted to guarantee consistency and asymptotic normality.
We now prove analogous results for DSM. This time we show weak consistency  i.e. convergence in
X Fθ(x)dQ(x). This will
probability: ˆθDSM
be a sufﬁcient form of convergence for asymptotic normality.
Theorem 5 (Weak Consistency of DSM). Let X be an open subset of Rd  and Θ ⊂ Rm. Suppose
log pθ(·) ∈ C 2(X ) and m ∈ C 1(X )  and (cid:107)∇x log pθ(x)(cid:107) ≤ f1(x) for Q-a.e. x. Suppose also that
(cid:107)∇x∇x log pθ(x)| ≤ f2(x) on any compact set C ⊂ Θ for Q-a.e. x  where (cid:107)m(cid:62)(cid:107)f1 ∈ L2(Q) 
(cid:107)∇ · (mm(cid:62))(cid:107)f1 ∈ L1(Q)  (cid:107)mm(cid:62)(cid:107)∞f2 ∈ L1(Q). If either Θ is compact  or Θ and θ (cid:55)→ Fθ are
convex and θDSM∗

≡ argminθ∈Θ DSMm(Q(cid:107)Pθ) = argminθ∈Θ

is weakly consistent for θDSM∗

∈ int(Θ)  then ˆθDSM

p−→ θDSM∗

(cid:82)

n

n

.

6

Theorem 6 (Central Limit Theorem for DSM). Let X   Θ be open subsets of Rd and Rm respec-
  θ (cid:55)→ log pθ(x) is twice continuously differentiable on a closed ball
tively. Suppose ˆθDSM
¯B(  θDSM∗

) ⊂ Θ  and that for Q-a.e. x ∈ X  

p−→ θDSM∗

n

(i) (cid:107)m(x)m(cid:62)(x)(cid:107) + (cid:107)∇x · (m(x)m(cid:62)(x))(cid:107)

(cid:107)∇θ∇x log pθ(x)(cid:107) + (cid:107)∇θ∇x∇x log pθ(x)(cid:107) ≤ f2(x)  with f1f2  f1f 2

≤

f1(x)  and (cid:107)∇x log pθ(x)(cid:107) +

2 ∈ L2(Q)

(ii) for θ ∈ ¯B(  θ∗)  (cid:107)∇θ∇x log pθ(cid:107)2 +(cid:107)∇x log pθ(cid:107)(cid:107)∇θ∇θ∇x log pθ(cid:107) +(cid:107)∇θ∇θ∇x log pθ(cid:107) +

(cid:107)∇θ∇θ∇x∇x log pθ(cid:107) ≤ g1(x)  and f1g1 ∈ L1(Q).
  we have

Then  if the information tensor is invertible at θDSM∗

(cid:16)ˆθDSM

n

√

n

X ∇θFθDSM∗

where ΣDSM =(cid:82)

(cid:17) d−→ N(cid:0)0  g−1

DSM

(cid:0)θDSM∗

(cid:1)ΣDSMg−1

DSM

(cid:0)θDSM∗

(cid:1)(cid:1).

− θDSM∗
(x) ⊗ ∇θFθDSM∗

(x)dQ(x).

then IF(z  Pθ) = gDKSD(θ)−1(cid:82)

Then if x (cid:55)→ ((cid:107)sp(x)(cid:107) + (cid:107)∇θsp(x)(cid:107))(cid:82) F (x  y)Q(dy)|θDKSD

All of the proofs can be found in Appendix D.2. An important special case covered by our theory is that
of natural exponential families  which admit densities of the form log pθ(x) ∝ (cid:104)θ  T (x)(cid:105)Rm + b(x).
If K is IPD with bounded derivative up to order 2  ∇T has linearly independent rows  m is invertible 
and (cid:107)∇T m(cid:107) (cid:107)∇xb(cid:107)(cid:107)m(cid:107) (cid:107)∇xm(cid:107) + (cid:107)m(cid:107) ∈ L2(Q)  then the sequence of minimum DKSD and
DSM estimators are strongly consistent and asymptotically normal (see Appendix D.3).
Before concluding this section  we turn to a concept of importance to practical inference: robustness
when subjected to corrupted data [31]. We quantify the robustness of DKSD and DSM estimators in
terms of their inﬂuence function  which can be interpreted as measuring the impact of an inﬁnitesimal
perturbation of a distribution P by a Dirac located at a point z ∈ X on the estimator. If θQ denotes the
unique minimum SD estimator for Q  then the inﬂuence functions is given by IF(z  Q) ≡ ∂tθQt|t=0
if it exists  where Qt = (1− t)Q + tδz  for t ∈ [0  1]. An estimator is said to be bias robust if IF(z  Q)
is bounded in z.
Proposition 7 (Robustness of DKSD estimators). Suppose that the map θ → Pθ over Θ
X ∇θk0(z  y)dPθ(y). Moreover  suppose that
is injective 
y (cid:55)→ F (x  y) is Q-integrable for any x  where F (x  y) = (cid:107)K(x  y)sp(y)(cid:107)  (cid:107)K(x  y)∇θsp(y)(cid:107) 
(cid:107)∇xK(x  y)sp(y)(cid:107)  (cid:107)∇xK(x  y)∇θsp(y)(cid:107)  (cid:107)∇y∇x(K(x  y)m(y))(cid:107) (cid:107)∇y∇x(K(x  y)∇θm(y))(cid:107).
is bounded  the DKSD estimators
are bias robust: supz∈X (cid:107) IF(z  Q)(cid:107) < ∞.
The analogous results for DSM estimators can be found in Appendix E. Consider a Gaussian
location model  i.e. pθ ∝ exp(−(cid:107)x − θ(cid:107)2
2)  for θ ∈ Rd. The Gaussian kernel satisﬁes the
assumptions of Proposition 7 so that supz (cid:107) IF(z  Q)(cid:107) < ∞  even when m = I.
Indeed
(cid:107) IF(z  Pθ)(cid:107) ≤ C(θ)e−(cid:107)z−θ(cid:107)2/4(cid:107)z − θ(cid:107)  where z (cid:55)→ e−(cid:107)z−θ(cid:107)2/4(cid:107)z − θ(cid:107) is uniformly bounded over
X xdQ(x) 
which is unbounded with respect to z  and is thus not robust. This clearly demonstrates the
importance of carefully selecting a Stein class for use in minimum SD estimators. An alterna-
tive way of inducing robustness is to introduce a spatially decaying diffusion matrix in DSM.
To this end  consider the minimum DSM estimator with scalar diffusion coefﬁcient m. Then
tion yields that the associated inﬂuence function will be bounded if both m(x) and (cid:107)∇m(x)(cid:107) decay
as (cid:107)x(cid:107) → ∞. This clearly demonstrates another signiﬁcant advantage provided by the ﬂexibility of
our family of diffusion SD  where the Stein operator also plays an important role.

θ. In contrast  the SM estimator has an inﬂuence function of the form IF(z  Q) = z −(cid:82)
θDSM = ((cid:82)

X ∇m2(x)dQ(x)(cid:1). A straightforward calcula-

X m2(x)dQ(x))−1(cid:0)(cid:82)

X m2(x)xdQ(x) +(cid:82)

∗

4 Numerical Experiments

In this section  we explore several examples which demonstrate worrying breakpoints for SM  and
highlight how these can be straightforwardly handled using KSD  DKSD and DSM.

4.1 Rough densities: the symmetric Bessel distributions

A major drawback of SM is the smoothness requirement on the target density. However  this can be
remedied by choosing alternative Stein classes  as will be demonstrated in the case of the symmetric

7

Figure 1: Minimum SD Estimators for the Symmetric Bessel Distribution. We consider the case where
θ∗
1 = 0 and θ∗

2 = 1 and n = 500 for a range of smoothness parameter values s in d = 1.

Figure 2: Minimum SD Estimators for Non-standardised Student-t Distributions. We consider a
student-t problem with ν = 5  θ∗

2 = 10 and n = 300.

1 = 25  θ∗

Bessel distributions. Let Ks−d/2 denote the modiﬁed Bessel function of the second kind with
parameter s − d/2. This distribution generalises the Laplace distribution [40] and has log-density:
log pθ(x) ∝ ((cid:107)x − θ1(cid:107)2/θ2)(s−d/2)Ks−d/2((cid:107)x − θ1(cid:107)2/θ2) where θ1 ∈ Rd is a location parameter
and θ2 > 0 a scale parameter. The parameter s ≥ d/2 encodes smoothness.
We compared SM with KSD based on a Gaussian kernel and a range of lengthscale values in Fig. 1.
These results are based on n = 500 IID realisations in d = 1. The case s = 1 corresponds to a
Laplace distribution  and we notice that both SM and KSD are able to obtain a reasonable estimate
of the location. For rougher values  for example s = 0.6  we notice that KSD outperforms SM
for certain choices of lengthscales  whereas for s = 2  SM and KSD are both able to recover the
parameter. Analogous results for scale can be found in Appendix F.1  and Appendix F.2 illustrates
the trade-off between efﬁciency and robustness on this problem.

4.2 Heavy-tailed distributions: the non-standardised student-t distributions

2/θ2

A second drawback of standard SM is that it is inefﬁcient for heavy-tailed distributions. To demon-
strate this  we focus on non-standardised student-t distributions: pθ(x) ∝ (1/θ2)(1 + (1/ν)(cid:107)x −
θ1(cid:107)2
2)−(ν+1)/2 where θ1 ∈ R is a location parameter and θ2 > 0 a scale parameter. The parameter
ν determines the degrees of freedom: when ν = 1  we have a Cauchy distribution  whereas ν = ∞
gives the Gaussian distribution. For small values of ν  the student-t distribution is heavy-tailed.
We illustrate SM and KSD for ν = 5 in Fig. 2  where we take an IMQ kernel k(x  y; c  β) =
(c2 +(cid:107)x− y(cid:107)2
2)β with c = 1. and β = −0.5. This choice of ν guarantees the ﬁrst two moments exist 
but the distribution is still heavy-tailed. In the left plot  both SM and KSD struggle to recover θ∗
1 when
n = 300  and the loss functions are far from convex. However  DKSD with mθ(x) = 1+(cid:107)x−θ1(cid:107)2/θ2
2
can estimate θ1 very accurately. In the middle left plot  we instead estimate θ2 with SM  KSD and
their correponding non-negative version (NNSM & NNKSD  m(x) = x)  which are particularly
well suited for scale parameters. NNSM and NNKSD provide improvements on SM and KSD  but
DKSD with mθ(x) = ((x − θ1)/θ2)(1 + (1/ν)(cid:107)x − θ1(cid:107)2
2) provides signiﬁcant further gains.
On the right-hand side  we also consider the advantage of the Riemannian SGD algorithm over
SGD by illustrating them on the KSD loss function with n = 1000. Both algorithms use constant
stepsizes and minibatches of size 50. As demonstrated  Riemmannian SGD converges within a few
dozen iterations  whereas SGD hasn’t converged after 1000 iterations. Additional experiments on the
robustness of these estimators is also available in Appendix F.2.

2/θ2

4.3 Robust estimators for light-tailed distributions: the generalised Gamma distributions

Our ﬁnal example demonstrates a third failure mode for SM: its lack of robustness for light-tailed
distributions. We consider generalised gamma location models with likelihoods pθ(x) ∝ exp(−(x −
θ1)θ2 ) where θ1 is a location parameter and θ2 determines how fast the tails decay. The larger θ2 

8

Figure 3: Minimum SD Estimators for Generalised Gamma Distributions under Corruption. We
consider the case where θ∗

2 = 2 (left and middle) or θ∗

2 = 5 (right). Here n = 300.

1 = 0 and θ∗

the lighter the tails will be and vice-versa. We set n = 300 and corrupt 80 points by setting them
to the value x = 8. A robust estimator should obtain a good approximation of θ∗ even under this
corruption. The left plot in Fig. 3 considers a Gaussian model (i.e. θ∗
2 = 2); we see that SM is not
robust for this very simple model whereas DSM with m(x) = 1/(1 + (cid:107)x(cid:107)α)  α = 2 is robust. The
middle plot shows that DKSD with this same m is also robust  and conﬁrms the analytical results
of the previous section. Finally  the right plot considers the case θ∗
2 = 5 and we see that α can be
chosen as a function of θ2 to guarantee robustness. In general  taking α ≥ θ∗
2 − 1 will guarantee a
bounded inﬂuence function. Such a choice allows us to obtain robust estimators even for models with
very light tails.

i  (cid:80)d

applied elementwise

exp(η(θ)(cid:62)ψ(x)) where ψ(x) = ((cid:80)d

4.4 Efﬁcient estimators for a simple unnormalised model
Finally we consider a simple intractable model from [47]: pθ(x) ∝
i=3 x1xi  tanh(x))(cid:62)
and tanh is
=
(−0.5  0.2  0.6  0  0  0  θ  0).
is intractable since
we cannot easily compute its normalisation constant due to the
difﬁculty of integrating the unnormalised part of the model.
Our results based on n = 200 samples show that DKSD with
m(x) = diag(1/(1 + x)) is able to recover θ∗ = −1  whereas both
SM and KSD provide less accurate estimates of the parameter. This
illustrates yet again that a judicious choice of diffusion matrix can
signiﬁcantly improve the efﬁciency of our estimators.

to x and η(θ)

i=1 x2

This model

Figure 4: Estimators for a Sim-
ple Intractable Model

5 Conclusion

This paper introduced a general approach for constructing minimum distance estimators based on
Stein’s method  and demonstrated that many popular inference schemes can be recovered as special
cases. This class of algorithms gives us additional ﬂexibility through the choice of an operator and
function space (the Stein operator and Stein class)  which can be used to tailor the inference scheme to
trade-off efﬁciency and robustness. However  this paper only scratches the surface of what is possible
with minimum SD estimators. Looking ahead  it will be interesting to identify diffusion matrices
which increase efﬁciency for important classes of problems in machine learning. One example on
which we foresee progress are the product of student-t experts models [38  66  68]  whose heavy tails
render estimation challenging for SM. Advantages could also be found for other energy models  such
as large graphical models where the kernel could be adapted to the graph [67].

Acknowledgments

AB was supported by a Roth scholarship from the Department of Mathematics at Imperial College
London. FXB was supported by the EPSRC grants [EP/L016710/1  EP/R018413/1]. AD and MG
were supported by the Lloyds Register Foundation Programme on Data-Centric Engineering  the
UKRI Strategic Priorities Fund under the EPSRC Grant [EP/T001569/1] and the Alan Turing Institute
under the EPSRC grant [EP/N510129/1]. MG was supported by the EPSRC grants [EP/J016934/3 
EP/K034154/1  EP/P020720/1  EP/R018413/1].

9

References
[1] S.-I. Amari. Natural gradient works efﬁciently in learning. Neural Computation  10(2):251–276 

1998.

[2] S.-I. Amari. Information Geometry and Its Applications  volume 194. Springer  2016.

[3] A. Barbour and L. H. Y. Chen. An introduction to Stein’s method. Lecture Notes Series  Institute

for Mathematical Sciences  National University of Singapore  2005.

[4] A. Basu  H. Shioya  and C. Park. Statistical Inference: The Minimum Distance Approach. CRC

Press  2011.

[5] A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and

Statistics. Springer Science+Business Media  New York  2004.

[6] E. Bernton  P. E. Jacob  M. Gerber  and C. P. Robert. Approximate Bayesian computation
with the Wasserstein distance. Journal of the Royal Statistical Society Series B: Statistical
Methodology  81(2):235–269  2019.

[7] S. Bonnabel. Stochastic gradient descent on Riemannian manifolds. IEEE Transactions on

Automatic Control  58(9):2217–2229  2013.

[8] F.-X. Briol  A. Barp  A. B. Duncan  and M. Girolami. Statistical inference for generative models

with maximum mean discrepancy. arXiv:1906.05944  2019.

[9] G. Casella and R. Berger. Statistical Inference. 2001.

[10] C. Ceylan and M. U. Gutmann. Conditional noise-contrastive estimation of unnormalised

models. arXiv:1806.03664  2018.

[11] L. H. Y. Chen  L. Goldstein  and Q.-M. Shao. Normal Approximation by Stein’s Method.

Springer  2011.

[12] W. Y. Chen  L. Mackey  J. Gorham  F.-X. Briol  and C. J. Oates. Stein points. In Proceedings of

the International Conference on Machine Learning  PMLR 80:843-852  2018.

[13] W. Y. Chen  A. Barp  F.-X. Briol  J. Gorham  M. Girolami  L. Mackey  and C. J. Oates. Stein
point Markov chain Monte Carlo. In International Conference on Machine Learning  PMLR 97 
pages 1011–1021  2019.

[14] K. Chwialkowski  H. Strathmann  and A. Gretton. A kernel test of goodness of ﬁt.

International Conference on Machine Learning  pages 2606–2615  2016.

In

[15] A. P. Dawid and M. Musio. Theory and applications of proper scoring rules. Metron  72(2):

169–183  2014. ISSN 2281695X. doi: 10.1007/s40300-014-0039-y.

[16] A. P. Dawid  M. Musio  and L. Ventura. Minimum scoring rule inference. Scandinavian Journal

of Statistics  43(1):123–138  2016.

[17] G. Detommaso  T. Cui  Y. Marzouk  A. Spantini  and R. Scheichl. A stein variational newton
method. In Advances in Neural Information Processing Systems 31  pages 9169–9179. 2018.

[18] G. K. Dziugaite  D. M. Roy  and Z. Ghahramani. Training generative neural networks via

maximum mean discrepancy optimization. In Uncertainty in Artiﬁcial Intelligence  2015.

[19] C. Frogner  C. Zhang  H. Mobahi  M. Araya-Polo  and T. Poggio. Learning with a Wasserstein

loss. In Advances in Neural Information Processing Systems  pages 2053–2061  2015.

[20] D. Gabay. Minimizing a differentiable function over a differential manifold. Journal of

Optimization Theory and Applications  37(2):177–219  1982.

[21] A. Genevay  G. Peyré  and M. Cuturi. Learning generative models with Sinkhorn divergences.
In Proceedings of the Twenty-First International Conference on Artiﬁcial Intelligence and
Statistics  PMLR 84  pages 1608–1617  2018.

10

[22] C. J. Geyer. On the convergence of Monte Carlo maximum likelihood calculations. Journal of

the Royal Statistical Society: Series B (Methodological)  56(1):261–274  1994.

[23] J. Gorham and L. Mackey. Measuring sample quality with Stein’s method. In Advances in

Neural Information Processing Systems  pages 226–234  2015.

[24] J. Gorham and L. Mackey. Measuring sample quality with kernels. In Proceedings of the

International Conference on Machine Learning  pages 1292–1301  2017.

[25] J. Gorham  A. Duncan  L. Mackey  and S. Vollmer. Measuring sample quality with diffusions.

arXiv:1506.03039. To appear in Annals of Applied Probability.  2016.

[26] M. U. Gutmann and A. Hyvärinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the Thirteenth International Conference
on Artiﬁcial Intelligence and Statistics  pages 297–304  2010.

[27] M. U. Gutmann and A. Hyvarinen. Noise-contrastive estimation of unnormalized statistical
models  with applications to natural image statistics. Journal of Machine Learning Research 
13:307–361  2012.

[28] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural

Computation  14(8):1771–1800  2002.

[29] W. Hoeffding. A class of statistics with asymptotically normal distribution. The Annals of

Mathematical Statistics  pages 293–325  1948.

[30] W. Hoeffding. The strong law of large numbers for U-statistics. Technical report  North Carolina

State University Department of Statistics  1961.

[31] P. J. Huber and E. M. Ronchetti. Robust Statistics. Wiley  2009.

[32] J. Huggins and L. Mackey. Random feature stein discrepancies.

Information Processing Systems  pages 1899–1909  2018.

In Advances in Neural

[33] A. Hyvärinen. Sparse code shrinkage: Denoising of nongaussian data by maximum likelihood

estimation. Neural computation  11(7):1739–1768  1999.

[34] A. Hyvärinen. Estimation of non-normalized statistical models by score matching. Journal of

Machine Learning Research  6:695–708  2006.

[35] A. Hyvärinen. Some extensions of score matching. Computational Statistics and Data Analysis 

51(5):2499–2512  2007.

[36] W. Jitkrittum  W. Xu  Z. Szabo  K. Fukumizu  and A. Gretton. A linear-time kernel goodness-

of-ﬁt test. In Advances in Neural Information Processing Systems  pages 261–270  2017.

[37] R. Karakida  M. Okada  and S.-I. Amari. Adaptive natural gradient learning algorithms for
unnormalized statistical models. Artiﬁcial Neural Networks and Machine Learning - ICANN 
2016.

[38] D. P. Kingma and Y. LeCun. Regularized estimation of image statistics by score matching. In

Advances in Neural Information Processing Systems  pages 1126–1134  2010.

[39] U. Köster and A. Hyvärinen. A two-layer model of natural stimuli estimated with score

matching. Neural Computation  22(9):2308–2333  2010.

[40] S. Kotz  T. J. Kozubowski  and K. Podgorski. The Laplace Distribution and Generalizations.

Springer  2001.

[41] Y. Li and R. E. Turner. Gradient estimators for implicit models. In International Conference on

Learning Representations  2018.

[42] Y. Li  K. Swersky  and R. Zemel. Generative moment matching networks. In Proceedings of

the International Conference on Machine Learning  volume 37  pages 1718–1727  2015.

11

[43] C. Liu and J. Zhu. Riemannian Stein Variational Gradient Descent for Bayesian Inference. (i) 

2017. URL http://arxiv.org/abs/1711.11216.

[44] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose Bayesian inference

algorithm. In Advances in Neural Information Processing Systems  2016.

[45] Q. Liu and D. Wang. Learning deep energy models: Contrastive divergence vs. amortized mle.

arXiv preprint arXiv:1707.00797  2017.

[46] Q. Liu  J. Lee  and M. Jordan. A kernelized Stein discrepancy for goodness-of-ﬁt tests. In

Proceedings of the International Conference on Machine Learning  pages 276–284  2016.

[47] S. Liu  T. Kanamori  W. Jitkrittum  and Y. Chen. Fisher efﬁcient inference of intractable models.

arXiv:1805.07454  2018.

[48] S. Lyu. Interpretation and generalization of score matching. In Conference on Uncertainty in

Artiﬁcial Intelligence  pages 359–366  2009.

[49] C. Ma and D. Barber. Black-box Stein divergence minimization for learning latent variable

models. Advances in Approximate Bayesian Inference  NIPS 2017 Workshop  2017.

[50] L. Mackey and J. Gorham. Multivariate Stein factors for a class of strongly log-concave

distributions. Electronic Communications in Probability  21  2016.

[51] K. V. Mardia  J. T. Kent  and A. K. Laha. Score matching estimators for directional distributions.

arXiv preprint arXiv:1604.08470  2016.

[52] C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neural computation  17(1):

177–204  2005.

[53] M. Micheli and J. A. Glaunes. Matrix-valued kernels for shape deformation analysis. arXiv

preprint arXiv:1308.5739  2013.

[54] A. Mnih and Y. W. Teh. A fast and simple algorithm for training neural probabilistic language
models. In Proceedings of the International Conference on Machine Learning  pages 419–426 
2012.

[55] A. Muller. Integral probability metrics and their generating classes of functions. Advances in

Applied Probability  29(2):429–443  1997.

[56] W. K. Newey and D. McFadden. Large sample estimation and hypothesis testing. Handbook of

Econometrics  4:2111–2245  1994.

[57] C. J. Oates  M. Girolami  and N. Chopin. Control functionals for Monte Carlo integration.

Journal of the Royal Statistical Society B: Statistical Methodology  79(3):695–718  2017.

[58] L. Pardo. Statistical Inference Based on Divergence Measures  volume 170. Chapman and

Hall/CRC  2005.

[59] S. Pigola and A. G. Setti. Global divergence theorems in nonlinear PDEs and geometry. Ensaios

Matemáticos  26:1–77  2014.

[60] R. Ranganath  J. Altosaar  D. Tran  and D. M. Blei. Operator variational inference. In Advances

in Neural Information Processing Systems  pages 496–504  2016.

[61] S. Roth and M. J. Black. Fields of experts. International Journal of Computer Vision  82(2):

205  2009.

[62] J. Sohl-dickstein  P. Battaglino  and M. R. DeWeese. Minimum probability ﬂow learning. In
Proceedings of the 28th International Conference on International Conference on Machine
Learning  pages 905–912  2011.

[63] B. Sriperumbudur  K. Fukumizu  A. Gretton  A. Hyvärinen  and R. Kumar. Density estimation
in inﬁnite dimensional exponential families. Journal of Machine Learning Research  18(1):
1830–1888  2017.

12

[64] B. K. Sriperumbudur  A. Gretton  K. Fukumizu  B. Schölkopf  and G. Lanckriet. Hilbert space
embeddings and metrics on probability measures. Journal of Machine Learning Research  11:
1517–1561  2010.

[65] C. Stein. A bound for the error in the normal approximation to the distribution of a sum of
dependent random variables. In Proceedings of 6th Berkeley Symposium on Mathematical
Statistics and Probability  pages 583–602. University of California Press  1972.

[66] K. Swersky  M. A. Ranzato  D. Buchman  B. M. Marlin  and N. de Freitas. On autoencoders and
score matching for energy based models. In International Conference on Machine Learning 
pages 1201–1208  2011.

[67] S. V. N. Vishwanathan  N. Schraudolph  R. Kondor  and K. Borgwardt. Graph kernels. Journal

of Machine Learning Research  pages 1201–1242  2010.

[68] M. Welling  G. Hinton  and S. Osindero. Learning sparse topographic representations with
products of student-t distributions. In Advances in Neural Information Processing Systems 
pages 1383–1390  2003.

[69] L. Wenliang  D. Sutherland  H. Strathmann  and A. Gretton. Learning deep kernels for expo-

nential family densities. arXiv:1811.08357  2018.

[70] I.-K. Yeo and R. A. Johnson. A uniform strong law of large numbers for U-statistics with
application to transforming to near symmetry. Statistics & Probability Letters  51(1):63–69 
2001.

13

,Alessandro Barp
Francois-Xavier Briol
Andrew Duncan
Mark Girolami
Lester Mackey