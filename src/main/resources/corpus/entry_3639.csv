2018,Multiplicative Weights Updates with Constant Step-Size in Graphical Constant-Sum Games,Since Multiplicative Weights (MW) updates are the discrete analogue of the continuous Replicator Dynamics (RD)  some researchers had expected their qualitative behaviours would be similar. We show that this is false in the context of graphical constant-sum games  which include two-person zero-sum games as special cases. In such games which have a fully-mixed Nash Equilibrium (NE)  it was known that RD satisfy the permanence and Poincare recurrence properties  but we show that MW updates with any constant step-size eps > 0 converge to the boundary of the state space  and thus do not satisfy the two properties. Using this result  we show that MW updates have a regret lower bound of Omega( 1 / (eps T) )  while it was known that the regret of RD is upper bounded by O( 1 / T ).

Interestingly  the regret perspective can be useful for better understanding of the behaviours of MW updates. In a two-person zero-sum game  if it has a unique NE which is fully mixed  then we show  via regret  that for any sufficiently small eps  there exist at least two probability densities and a constant Z > 0  such that for any arbitrarily small z > 0  each of the two densities fluctuates above Z and below z infinitely often.,Multiplicative Weights Updates with Constant
Step-Size in Graphical Constant-Sum Games

Yun Kuen Cheung ∗

Singapore University of Technology and Design

Singapore

yunkuen_cheung@sutd.edu.sg

Abstract

Since Multiplicative Weights (MW) updates are the discrete analogue of the contin-
uous Replicator Dynamics (RD)  some researchers had expected their qualitative
behaviours would be similar. We show that this is false in the context of graphical
constant-sum games  which include two-person zero-sum games as special cases.
In such games which have a fully-mixed Nash Equilibrium (NE)  it was known that
RD satisfy the permanence and Poincaré recurrence properties  but we show that
MW updates with any constant step-size ε > 0 converge to the boundary of the
state space  and thus do not satisfy the two properties. Using this result  we show
that MW updates have a regret lower bound of Ω(1/(εT ))  while it was known
that the regret of RD is upper bounded by O(1/T ).
Interestingly  the regret perspective can be useful for better understanding of the
behaviours of MW updates. In a two-person zero-sum game  if it has a unique NE
which is fully mixed  then we show  via regret  that for any sufﬁciently small ε 
there exist at least two probability densities and a constant Z > 0  such that for
any arbitrarily small z > 0  each of the two densities ﬂuctuates above Z and below
z inﬁnitely often.

1

Introduction

The concept of Nash Equilibrium (NE) has been central in game theory. The existential proof of
Nash [20] is non-constructive  while the deﬁnition of NE itself is also static  both of which shed no
insight how NE can be computed or reached. In turn  lots of researchers have devoted efforts to justify
the concept of NE by providing algorithms/dynamics which might compute/reach a NE. Among
them  Multiplicative Weights (MW) updates have drawn a lot of attention  due to its simplicity and
naturalness  and perhaps more importantly  its distributive implementability2 which is essential in
games we observe in reality  where information communicated between players is often very limited.
MW updates have also made profound impacts in algorithm design; see [1] for details. However 
various PPAD-hardness and communication complexity results [8  6  13] serve as strong indicators
that no efﬁcient algorithm/dynamic  MW updates included  can efﬁciently compute/reach NE for
general games. But can MW updates do so for interesting sub-families of games?
The best we could hope for is pointwise convergence toward a NE  but it is known not to hold
even in the simplest scenario of two-person zero-sum games. A weaker notion of convergence 
called empirical convergence (i.e.  the average of the time series history converges)  has been sought.
While this notion might seem less natural  it is interesting from the perspective of statistics; this
∗Most work done while the author was at Max-Planck Institute for Informatics  Saarland Informatics Campus.
2In the context of game dynamics  distributive implementability means each player only needs information
she can observe locally (e.g.  payoffs to each of her own strategies) to run the updates  and does not need to
know other global information such as the value of the underlying game matrix and the updates of other players.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

√
T )  hence the empirical average at time T forms an O(1/

notion  coined as “ergodic convergence” in the study of dynamical systems  is central in a branch
of mathematics called ergodic theory  where its initial development was motivated by the study
of time-average behaviours of systems of interest in statistical physics. Freund and Schapire [10]
showed that MW updates converge to NE empirically in any two-person zero-sum game using the
notion of regret; their analysis also yields a simple and beautiful proof of John von Neumann’s
Minimax Theorem. Daskalakis and Papadimitriou [9] and Cai and Daskalakis [4] generalized to
graphical constant-sum games and separable zero-sum games.
It should be noted that  however  their precise result is actually the following: if the ﬁnishing time
√
T is known a priori3  then there is a step-size ε  which depends on T   such that the regret at time
T is O(1/
T )-approximate NE. When
ε is a ﬁxed constant and T (cid:37) ∞  their analyses can only show a regret bound of O(ε)  and it
is not clear whether empirical convergence toward NE occurs. Recently there are several work
showing distributed dynamics achieve oT (1) regret in two-person zero-sum games [7  24] and general
games [28]  but all of them require the step-size to be diminishing or adaptive. However  in some
applications of biological system models  e.g.  population dynamics (see [26  15]) and evolution [5] 
diminishing or adaptive step-size is unnatural. Also  from an algorithmic perspective  it is natural
to ask what happens if the step-size is constant  and if the use of diminishing or adaptive step-size
is unavoidable. Indeed  in last year  Palaiopanos et al. [21] have addressed the same question in the
context of congestion games.
In this paper  we study MW updates with constant step-size in graphical constant-sum games. It was
known that Replicator Dynamics (RD)  the continuous analogue of MW updates  in such games are
permanent [14]  i.e.  all probability densities are bounded away from zero throughout. Also  RD in
such games satisfy the Poincaré recurrence property [23  18]. In algorithm design and modelling of
biological systems  continuous-time dynamics like RD are hardly feasible  so we are interested in
the behaviours of MW updates. Due to the analog between MW updates and RD  it seems natural to
expect MW updates could retain the above good properties of RD. Unfortunately  this is not true. We
show that MW updates in such games do not satisfy permanence and Poincaré recurrence properties;
indeed  we show that MW updates converge toward the boundary of the state space (unless the
starting point is NE)  which is stronger than non-satisfaction of the above two properties.
We note a recent independent work of Bailey and Piliouras [2]  who show the same result in a more
general setting: ﬁrst  they generalize to discrete Follow-The-Regularized-Leader (FTRL) dynamics 
and second  they allow the step-sizes to be mildly decreasing. Precisely  they show that if a FTRL
dynamic guarantees every update must stay in the interior of the state space (MW is an example
of such FTRL dynamic)  then the dynamic converges toward the boundary; otherwise  they show
that the FTRL dynamic gets arbitrarily close to the boundary inﬁnitely often (this is weaker than
“convergence toward the boundary”). In our paper  we proceed further by using the result to get a
better understanding of the behaviours of MW updates and the regret  as we will discuss next.
We show a regret lower bound of Ω(1/(εT )) plus a positive term  which is proportional to the average
variance of payoffs among strategies over time. This lower bound should be compared to RD’s regret
upper bound O(1/T ) by Sorin [27  Theorem 3.1].4 Mertikopoulos et al. [18] generalized Sorin’s
upper bound to continuous FTRL dynamics.
Interestingly  the regret perspective can be useful for better understanding of the behaviours of MW
updates. In a two-person zero-sum game  if it has a unique NE which is fully mixed  then we show 
via regret  that for any sufﬁciently small ε  there exist at least two probability densities and a constant
Z > 0  such that for any arbitrarily small z > 0  each of the two densities ﬂuctuates above Z and
below z inﬁnitely often.

Continuous vs. Discrete Dynamics. As we will see  from a high-level perspective  we are exploit-
ing the interplay between continuous dynamics and their analogous discrete dynamics. Such interplay
has been exploited before in other contexts; see  for instance  Sorin [27]  Kwon and Mertikopou-
los [17] and Benaïm [3]. Brieﬂy speaking  in [27  17]  they showed that the disparity between the two
dynamics is under control up to a certain time by choosing a suitable time-dependent step-size. Thus 

3Now it is standard that the knowing-the-ﬁnishing-time assumption can be get rid of by employing a

“doubling trick”  but the step-size will be diminishing over time.

4Sorin [27] showed this upper bound in the unilateral setting  i.e.  the bound holds for any player which uses
RD  while how the environment (e.g.  game payoffs  other players behaviours) varies over time does not matter.

2

if a good property holds for the continuous dynamic (which is often easier to show in the continuous
time setting)  it might carry over to the discrete analogue  sometimes with a depreciation due to the
disparity. In the contexts we study  we show that the disparity must accumulate indeﬁnitely  and
eventually lead to very different qualitative behaviours between RD and MW updates.

Other Related Work. The long term (asymptotic) behaviours of learning/evolution dynamics
in games  biological and other systems have attracted attention from researchers across multiple
disciplines for decades; see the text of Hofbauer and Sigmund [15] for an extensive summary. A
large number of work have focused on continuous dynamics. Even in simple games like Paper-
Rock-Scissors or small three-player graphical games  learning dynamics exhibit rich and sometimes
surprising long term behaviours; see  for instance  [29  11  12  25  16  22  19]. Discrete dynamics
are more relevant from an algorithmic perspective  and certainly deserve more attention in this
algorithmic era. Our work suggests that the long term behaviours of continuous dynamics and their
discrete counterparts can be very different  and a better understanding of such behaviours might shed
insights on game-theoretic benchmarks such as regret.

2 Preliminary

Types of Games  and Nash Equilibrium.
In a general bimatrix game with two players  suppose
Players A and B have strategy sets SA and SB respectively. The game is depicted by a bimatrix
M = [(aij  bij)]i∈SA j∈SB : when Player A chooses strategy i and Player B chooses strategy j 
aij  bij are the payoffs to Players A and B respectively. Such a game is called two-person constant-
sum game if for all i ∈ SA  j ∈ SB  aij + bij = C for some real number C; such a game is called
two-person zero-sum game if it is a two-person constant-sum game with C = 0.
In a game with m players  we number the players by 1  2 ···   m  and let Si denote the strategy
set of Player i  and ni := |Si|. For any s = (s1  s2 ···   sm) ∈ ×m
i=1Si  where si is the choice
of strategy of Player i  let ui(s) denote the payoff to Player i. Such a game is called separable
zero-sum multiplayer game if for any s =∈ ×m
A game with m players is a graphical polymatrix game if the game is deﬁned as follows: on an
undirected graph G = ([m]  E)  each edge (i  j) ∈ E corresponds to a bimatrix game between
Players i and j with strategy sets Si and Sj respectively. It is worth noting that the strategy set of a
Player i in different bimatrix games is the same  and every time she plays the game  she must choose
the same strategy for all these bimatrix games. Such a game is a graphical constant-sum game if
the bimatrix game corresponded by every edge is a two-person constant-sum game (different bimatrix
games may have different constants C).
Theorem 1 ([4]). Every separable zero-sum multiplayer game can be transformed into a graphical
constant-sum game  while preserving all the payoffs.

i=1Si (cid:80)m

i=1 ui(s) = 0.

j∈Si

a vector xi ∈ Rni with(cid:80)

Due to the above theorem  we focus on developing our results on graphical constant-sum games; all
these results automatically carry over to separable zero-sum multiplayer games. Also  by suitable
scaling  we can assume that the payoff to every player always lie within the interval [−1  +1].
A mixed strategy of a Player i is a probability distribution over her strategy set  represented by
xij = 1  where xij is the probability density that strategy j is
chosen. When each Player i chooses a mixed strategy xi independently  the payoff function extends
naturally by ui(x1  x2 ···   xm) = Es∼×m
j=1xj [ui(s)]. The boundary of the mixed strategy space
is ∪i j∈Si{x | xij = 0}.
We say (x1  x2 ···   xm) is a Nash equilibrium (NE) if no player can change her mixed strategy for
achieving a higher payoff. Precisely  for any Player i  let x−i denote the mixed strategies chosen by
i  x−i) ≤ ui(xi  x−i).
all players other than Player i  then for any mixed strategy x(cid:48)
A NE is said to be fully mixed if no probability density in any of the xi is zero. For any Player i  let
ej denote a pure strategy j of her (i.e.  a mixed strategy with probability one on strategy j).

i of Player i  ui(x(cid:48)

Replicator Dynamic and Multiplicative Weights Updates.
In a game with m players where each
player employs a Replicator Dynamic (RD)  each Player i maintains a mixed strategy xi(t) which is
updated continuously with time t. Let x(t) = (x1(t)  x2(t) ···   xm(t)). The update rule is given

3

by a differential equation system  for each strategy j ∈ Si 

xij(t) = xij(t) · [ui(ej  x−i(t)) − ui(x(t))] .

d
dt

If times are discrete at non-negative integers  and if each Player i employs a Multiplicative Weights
(MW) updates with step-size εi > 0  the update rule is

xij(t + 1) =

(cid:80)

xij(t) · exp (εi · ui(ej  x−i(t)))
k∈Si

xik(t) · exp (εi · ui(ek  x−i(t)))

.

It is well-known that MW updates are discrete analogue of RD.
Throughout this paper  we always assume that for all i ∈ [m] and j ∈ Si  every starting density xij(0)
is strictly positive  i.e.  x(0) is fully mixed. Also  in all our results  we always assume εi ≤ 1/4.
The regret of Player i is

(cid:34)(cid:32)
(cid:34)(cid:32)

max
j∈Si

1
T

1
T

max
j∈Si

(cid:90) T
T−1(cid:88)

0

(cid:33)
(cid:33)

−

(cid:90) T
− T−1(cid:88)

0

(cid:35)
(cid:35)

ui(ej  x−i(t)) dt

ui(x(t)) dt

ui(ej  x−i(t))

ui(x(t))

t=0

t=0

for continuous-time model  T > 0;

for discrete-time model  T ∈ N.

3 Permanence and Poincaré Recurrence

We ﬁrst present two prior results concerning RD in general games and graphical constant-sum games.
Theorem 2 ([27]). In any multiplayer game where the payoff function to Player i is Lebesgue
integrable in the mixed strategies of all players and the game parameters  if Player i employs RD 
while the game parameters and mixed strategies of all other players are measurable functions of time 
then for any T > 0  the regret of Player i is at most 1
Theorem 3 ([23]; see also [15  18]). If a graphical constant-sum game admits a fully-mixed NE x∗
ij·
x∗

and all players employ RD  then for any fully mixed starting point x(0)  H(t) := −(cid:80)m
(cid:80)
ln(xij(t)) is a constant for all t ≥ 0. Consequently  for all t ≥ 0  xij(t) ≥ exp(cid:0)−H(0)/x∗

T · maxj∈Si ln

(cid:1) > 0 

so the system is permanent (i.e.  all xij’s are bounded away from zero throughout)  and the ω-set of
the dynamic is bounded away from the boundary. Also  the dynamic satisﬁes the Poincaré recurrence
property.

j∈Si
ij

1

xij (0) .

i=1

For MW updates  to cope with the scenarios where different players use different step-sizes  we make
a slight modiﬁcation of the Hamiltonian function H in [23]:

H(t) := − m(cid:88)

1
εi

i=1

ij · ln(xij(t)).
x∗

Lemma 4. If a graphical constant-sum game admits a fully-mixed NE x∗ and all players employ
MW updates  then H(t + 1) ≥ H(t). More speciﬁcally  let Vi(t) denote

xik(t)· (exp (εi · ui(ek  x−i(t))) − 1)2 −

xik(t) · (exp (εi · ui(ek  x−i(t))) − 1)

 

(cid:33)2

Vi(t)

εi

≥ H(t + 1) − H(t) ≥ 1

4

i=1

Vi(t)

εi

.

Before proving the lemma  we note that Vi(t) is indeed the variance of the following ran-
dom variable  and thus is always non-negative:
the random variable realizes the value
(exp (εi · ui(ek  x−i(t))) − 1) with probability xik(t)  for all k ∈ Si. Moreover  if xi(t) is fully
mixed  then Vi(t) is zero if and only if ui(ek  x−i(t)) is identical for all k ∈ Si.

4

·(cid:88)

j∈Si

(cid:32)(cid:88)
(cid:80)m

k∈Si

i=1

(cid:88)
then (cid:80)m

k∈Si

i=1

1
εi

(cid:34)

ij ·
x∗

εi · ui(ej  x−i(t)) − ln

Proof. We ﬁrst expand H(t + 1) − H(t) = −(cid:80)m
= − m(cid:88)
 − 1
= − m(cid:88)
the ﬁnal equality holds since for each i ∈ [m] (cid:80)

·(cid:88)

(cid:88)

ij · ui(ej  x−i(t))
x∗

· ln

j∈Si

j∈Si

(cid:124)

i=1

i=1

εi

1
εi

·(cid:80)
(cid:32)(cid:88)
(cid:32)(cid:88)

k∈Si

k∈Si

x∗
ij = 1.

j∈Si

ij · ln xij (t+1)
x∗

xij (t) as follows:

j∈Si

xik(t) · exp (εi · ui(ek  x−i(t)))

xik(t) · exp (εi · ui(ek  x−i(t)))

(cid:123)(cid:122)

L

(cid:33)(cid:35)
(cid:33)
(cid:125)

 ;

Noting that each (exp (εi · ui(ek  x−i(t))) − 1) is within the interval [e−εi − 1  eεi − 1]  and noting
that in this interval the function ln(1 + y) + y2/4 is concave but the function ln(1 + y) + y2 is convex 
by the Jensen’s inequality  we have

Vi(t) ≥ L − (cid:88)
m(cid:88)
≥ H(t + 1) − H(t) − m(cid:88)

k∈Si

i=1

(cid:88)

i=1

k∈Si

m(cid:88)

i=1

Vi(t)

εi

Thus 

xik(t) · εi · ui(ek  x−i(t)) ≥ m(cid:88)

Vi(t)

4

.

i=1

(xik(t) − x∗

ik) · ui(ek  x−i(t)) ≥ 1
4

(1)

Vi(t)

εi

.

m(cid:88)

i=1

By following the proof of Theorem 3 in [23]  one can show that the above double summation is zero.
We defer this part of the proof to Section 7.
Theorem 5. If a graphical constant-sum game admits a fully-mixed NE x∗ and all players employ
MW updates  while the starting point x(0) is not a NE  then

(a) for any δ > 0  there exists a time Tδ such that for all t ≥ Tδ  there exists some i ∈ [m] and

j ∈ Si with xij(t) ≤ δ. Thus  the ω-set of the dynamic is a subset of the boundary;

(b) let U be an open neighbourhood of x which is bounded away from the boundary  the MW
updates will enter U only ﬁnitely often  i.e.  there exists a time T such that for all t ≥ T  
x(t) /∈ U; in other words  Poincaré recurrence property does not hold.

Theorem 5 should be compared with Theorem 3. The key message is that although MW updates
are the discrete analogue of RD  their qualitative behaviours differ signiﬁcantly. The main technical
reason behind is that the discretization from RD to MW updates introduces some second-order terms
which accumulate in the bad way and keep pushing the MW updates toward the boundary. Such
bad accumulation exists even when ε is arbitrarily tiny  which might be surprising to people not
familiar with numerical methods  since they might have the misconception that once the step-size ε
is sufﬁciently small  the discretization would always yield a good approximation of its continuous
counterpart. Theorem 5(b) is a direct corollary of Theorem 5(a).
Proof. Suppose that H(t) is bounded by some constant q throughout. By Lemma 4  H(t) ≥ H(0)
for all t ≥ 0. Thus  for all t ≥ 0  x(t) always lies in the domain

(x1  x2 ···   xm)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

− m(cid:88)

i=1

1
εi

 ∈ [H(0)  q]

 .

ij · ln xij
x∗

Consider the function V (x1 ···   xm) :=

xik · (exp (εi · ui(ek  x−i)) − 1)2 −

xik · (exp (εi · ui(ek  x−i)) − 1)

D :=

m(cid:88)

i=1

·

1
εi

(cid:88)

k∈Si

·(cid:88)
(cid:32)(cid:88)

j∈Si

k∈Si

(cid:33)2 .

We argue that inf x∈D V (x) is positive  which follows readily by the following sequence of observa-
tions. By deﬁnition of D  it is bounded away from the boundary. Note that D is a compact set  since
it is the inverse of a continuous function for some closed interval. Also  D is bounded away from

5

any NE (this is a simple corollary of Lemma 4  since the set of NE is closed). Since for any fully
mixed point x  V (x) = 0 if and only if x is a NE  and since V is a continuous function on D  we can
conclude that {V (x) | x ∈ D} is bounded away from zero.
Let inf x∈D V (x) = r > 0. By Lemma 4  after t =
H(t) > q  a contradiction.
Thus  we can conclude that for any q > 0  there exists a time Tq such that for all t ≥ Tq  H(t) > q.
This implies that for all t ≥ Tq  there exists i ∈ [m] and j ∈ Si such that x∗
  and
ij
εi
. Theorem 5 follows by setting q = 1
hence xij(t) < exp

(cid:108) 4(q−H(0))

q(cid:80)m
(cid:1)·(cid:80)m
i=1 ni
i=1 ni.

(cid:16)− εiq(cid:80)m

+ 1 steps of the MW updates 

·(cid:0)ln 1

·ln 1

xij (t) >

(cid:17)

(cid:109)

r

δ

mini εi

i=1 ni

1

1

(cid:17)

(cid:16)

2(ni−1) · mink∈Si xik(0)

4 Regret Lower Bound
Theorem 6. In any graphical constant-sum game which admits a fully-mixed NE x∗ and all players
employ MW updates  while the starting point x(0) is not a NE  then there exists a sufﬁciently large
T such that for all t ≥ T   there exists a Player i (which can change w.r.t. t) with regret at least
εit · ln
. In particular  if xi(0) is uniform  then the regret is at least
1 +
4εi(ni−1)t .
We compare Theorem 6 with a lower bound result in Daskalakis et al. [7  Theorem 2]. Our result
focuses on MW updates  and it applies to any graphical constant-sum game with a fully-mixed NE.
The result in [7] focuses on a general lower bound of Ω(1/T ) regret for any type of distributed
protocol  which is more general than ours. To do so  they constructed a speciﬁc class of zero-sum
games and show that any distributed protocol must lead to an Ω(1/T ) regret in one of such games.

maxk∈Si xik(0)

1

Proof. First  we show the following inequality; (*) follows from the fact ln(y) is a concave function
of y  and a use of the Jensen’s inequality.

T−1(cid:88)

t=0

=

ln

ln xij(T ) − ln xij(0)

T

t=0

T−1(cid:88)
T−1(cid:88)
T−1(cid:88)

t=0

t=0

=

εi
T

(∗)≤ εi

T

=

εi
T

ui(ej  x−i(t)) − 1
T

ui(ej  x−i(t)) − 1
T

ui(ej  x−i(t)) − εi
T

xij(t + 1)

xij(t)

(cid:32)(cid:88)
(cid:88)

ln

t=0

T−1(cid:88)
T−1(cid:88)
T−1(cid:88)

t=0

t=0

k∈Si

ui(x(t)).

(cid:33)

xik(t) · exp (εi · ui(ek  x−i(t)))

k∈Si
xik(t) · εi · ui(ek  x−i(t))

Note that the ﬁnal expression  when maximizing over all j ∈ Si  is exactly εi times the regret of
Player i.
2 · mini∈[m] j∈Si xij(0)  there exists a time T such that for all t ≥ T   there
By Theorem 5  for δ = 1
exists i ∈ [m]  j ∈ Si such that xij(t) ≤ xij(0)/2. Thus  for that Player i  there exists some strategy
(cid:18)
k ∈ Si \ {j} such that xik(t) ≥ xik(0) +
ln xik(T )−ln xik(0) ≥ ln

(cid:18)
2(ni−1) · xij(0)  and hence

≥ ln

(cid:19)

1 +

1

1

1

(cid:19)

.

· mink∈Si xik(0)
maxk∈Si xik(0)

· xij(0)
xik(0)

2(ni − 1)

2(ni − 1)
εiT times the RHS of the above inequality.

1 +

1

Thus  the regret of Player i is at least

Indeed  by using (1)  the inequality (*) can be improved  and then we have

ln xij(T ) − ln xij(0)

εiT

+

1

4εiT

T−1(cid:88)

t=0

T−1(cid:88)

t=0

Vi(t) ≤ 1
T

6

ui(ej  x−i(t)) − 1
T

T−1(cid:88)

t=0

ui(x(t))

(cid:80)T−1

1

4εiT

Thus 
t=0 Vi(t) can serve as a lower bound of regret. In the last section  we show that if
the starting point is fully mixed but not NE  if MW updates were to stay away from the boundary 
then Vi(t) is bounded away from zero  and thus a regret lower bound of Ωεi(1) could follow. But
MW updates do converge to the boundary  so it is not clear how to derive a tight lower bound on this
sum. In particular  we cannot rule out the possibility that MW updates converge to a subgame NE (a
subgame is obtained from the original game by removing at least one strategy of some player)  and if
this happens  Vi(t) converges to zero.
We note that essentially the same proof yields the following more general result about general games 
which states that if the dynamic is not Poincaré recurrent  then the regret is at least Ω(1/T ) eventually.
Proposition 7. In any general game where all players employ MW updates with starting point x(0) 
if there exists an open neighbourhood B around x(0) and a time T such that for all t ≥ T   x(t) /∈ B 
then for all t ≥ T   there exists a Player i (which can change w.r.t. t) with regret at least Ω(1/T ) 
where the hidden constant in Ω(·) depends only on ε and the radius of B.
We do not have any improvement on the generic regret upper bound. In the next section  we will
use such the generic bound  which is 2εi + C(x(0))
for Player i  where C(x(0)) is a constant which
depends on the starting point. (See [10].)5 We bound this regret by 2.1 · εi for all sufﬁciently large T .

εiT

5

Inﬁnitely Often Almost Extinction  Inﬁnitely Often Resurgence

In this section  we focus on two-person zero-sum (or constant-sum) games. Theorem 5 applies  i.e. 
beyond some ﬁnite time  there must exist some tiny probability density. A natural question to ask is
will one density be tiny forever  or some densities take turn to be tiny? In this section  we prove that
the former case cannot happen when ε is sufﬁciently small. For any two-person zero-sum game G  let
val(G) denote its game value w.r.t. Player 1. In this section  we write ε = max{ε1  ε2}.
Given a two-person zero-sum game G  name the two players 1 and 2  and their strategy sets are S1
and S2 respectively. For i = 1  2  and each j ∈ Si  let Gij denote the two-person zero-sum game
which is formed from G by removing the strategy j from Player i. Now  deﬁne

(cid:8)val(G) − val(G1j)(cid:9)   min

k∈S2

(cid:8)val(G2k) − val(G)(cid:9)(cid:27)

.

(cid:26)

θ(G) := min

min
j∈S1

(cid:80)T−1
t=0 u1(x(t)) ∈ val(G) ± 2.1 · ε.

Using von Neumann’s Minimax Theorem  it is easy to prove that θ(G) ≥ 0. Intuitively this should be
also clear  since removing one strategy from Player 1 will surely not beneﬁt her  and removing one
strategy from Player 2 provides less choices available to Player 2  and hence might beneﬁt Player 1.
The following two lemmas can be easily proved using the linear program (LP) formulation of
two-person zero-sum game and the Minimax Theorem; see Section 7 for their proofs.
Lemma 8. If G is a two-person zero-sum game with a unique NE which is fully mixed  then θ(G) > 0.
Lemma 9. In a two-person zero-sum game G  if both players employ MW updates  for all sufﬁciently
large T   we have 1
T
Theorem 10. In a two-person zero-sum game with a unique NE which is fully mixed  if both players
employ MW updates with step-size εi < θ(G)/7  then there exists at least two probability densities
xij(t) which exhibit the following pattern: (a) for any δ > 0  xij(t) < δ for inﬁnitely many t; and
(b) xij(t) ≥ θ(G)/7 for inﬁnitely many t.
We give some intuition before giving the proof. By Theorem 5  there exists T such that for all t ≥ T  
there must exist a density at time t which is below δ. Thus  it is possible to ﬁnd a ﬁxed i and j ∈ Si
such that the property (a) holds. Suppose this i = 2; the case i = 1 is symmetric. Suppose that for
this xij  property (b) does not hold  i.e.  xij(t) remains below some κ after some time T (cid:48). Intuitively 
this implies that from time T (cid:48) onwards  the game essentially becomes Gij  modulo perturbation of
magnitude O(κ). By Lemma 9  the long-run average payoff to Player 1 from time T (cid:48) onward is
within the interval val(Gij) ± O(κ) ± O(ε). On the other hand  by Lemma 9 again  the long-run
average payoff to Player 1 from time 0 onward is within the interval val(G) ± O(ε). These two
average payoffs should match  but when ε  κ are both small  the two intervals do not overlap  a
contradiction.

√
T ) upper bound is indeed coming from this bound and pick εi = Θ(1/

√
T ).

5The well-known O(1/

7

Proof. To avoid cluster of algebra  we let v := val(G) and vij := val(Gij). Let κ := θ(G)/7.
Suppose that the concerned density is x1j. By the regret upper bound  for all k ∈ S1  and for all
sufﬁciently large T (cid:48)(cid:48) > T (cid:48) 

 T (cid:48)(cid:48)(cid:88)

1

t=T (cid:48)

T (cid:48)(cid:48) − T (cid:48) + 1

u1(ek  x2(t)) − T (cid:48)(cid:48)(cid:88)
(cid:80)T (cid:48)(cid:48)
(cid:80)T (cid:48)(cid:48)
t=T (cid:48) x2(t)) ≥ v1j. Thus 
T (cid:48)(cid:48)(cid:88)

1

t=T (cid:48)

1

Note that we can rewrite
the Minimax Theorem  we can guarantee that
u1(ek 

T (cid:48)(cid:48)−T (cid:48)+1

1

T (cid:48)(cid:48)−T (cid:48)+1

 ≤ 2.1 · ε.
(cid:80)T (cid:48)(cid:48)

1

u1(x(t))

t=T (cid:48) u1(ek  x2(t)) as u1(ek 

t=T (cid:48) x2(t)). By
there is some k ∈ S1 \ {j} such that

T (cid:48)(cid:48)−T (cid:48)+1

ek 
T (cid:48)(cid:48)(cid:88)

t=T (cid:48)

(2)
Next  consider Player 2. By the regret upper bound  for all k ∈ S2  and for all sufﬁciently large
T (cid:48)(cid:48) > T (cid:48) 

T (cid:48)(cid:48) − T (cid:48) + 1

t=T (cid:48)

u1(x(t)) ≥ v1j − 2.1 · ε.

ek 

u2

(cid:124)

=

1

T (cid:48)(cid:48) − T (cid:48) + 1


(cid:125)

x1(t)

T (cid:48)(cid:48)(cid:88)
(cid:123)(cid:122)
 T (cid:48)(cid:48)(cid:88)
u2(ek  x1(t)) − T (cid:48)(cid:48)(cid:88)

t=T (cid:48)

+

Wk

1

T (cid:48)(cid:48) − T (cid:48) + 1

t=T (cid:48)

u1(x(t))

T (cid:48)(cid:48)(cid:88)
 ≤ 2.1 · ε.

u2(x(t))

t=T (cid:48)

1

T (cid:48)(cid:48) − T (cid:48) + 1

t=T (cid:48)

Note that in the summation in the term Wk  x1j(t) < κ by assumption. For each t  we construct a new
1j(t) = 0  and for any k ∈ S1 \ {j} 
probability distribution on S1  denoted by x(cid:48)
1k(t) = x1k(t) + 1|S1|−1 · x1j(t). Since the payoff value is always within the interval ±1  we have
x(cid:48)

1(t)  as follows: x(cid:48)

W (cid:48)

k − 2κ := u2

1

T (cid:48)(cid:48) − T (cid:48) + 1

x(cid:48)
1(t)

 − 2κ ≤ Wk.

T (cid:48)(cid:48)(cid:88)

t=T (cid:48)

By the Minimax Theorem  we can guarantee that there is some k ∈ S2 such that W (cid:48)
Combining all the inequalities above yields

k ≥ −v1j.

1

T (cid:48)(cid:48) − T (cid:48) + 1

u1(x(t)) ≤ v1j + 2.1 · ε + 2κ.

(3)

Inequalities (2) and (3) imply that the long-run average payoff to Player 1 from time T (cid:48) onward is
within the interval v1j ± 2.1 · ε ± 2κ. But Lemma 9 states that the long-run average payoff to Player
1 from time 0 onward is within the interval v ± 2.1 · ε. Since the former average is obtained by only
ignoring ﬁnitely many terms in the beginning  these two averages are essentially the same in the long
run  i.e.  the two intervals must overlap. However  this is not possible when ε  κ ≤ θ(G)/7.
Finally  note that among the times t where xij(t) ≥ θ(G)/7  there must be another density  say
xi(cid:48)j(cid:48)  satisfying xi(cid:48)j(cid:48)(t) < δ inﬁnitely often. By reiterating the above argument for this xi(cid:48)j(cid:48)  we are
done.

6 Discussion and Some Open Problems

In this paper  we provide a better understanding of MW updates with constant step-size in graphical
constant-sum games. Yet  a number of interesting problems are still unsolved. We raise some:
• While we provide a lower bound on the regret  the best upper bound we know is still the generic
ε + oT (1) one  which applies in rather general scenarios and has not exploited any structure of
graphical constant-sum games. Will better lower/upper bound be admissible?

8

• We can only prove that the ﬂuctuating pattern described in Theorem 10 exists for two-person
constant-sum games  but not general graphical constant-sum games. The technical reason is we
need several nice properties of von Neumann’s LP formulation and Minimax Theorem6 to establish
Lemmas 8 and 9  which are not known for graphical constant-sum games. Can we generalize?
• Even for two-person zero-sum games  Theorem 10 has not yet provided the complete picture. Will
all densities exhibit such ﬂuctuating pattern  or only some of them do? If it is the latter  given the
game and the starting point  can we determine (by a mathematical proof  or by a polynomial time
algorithm) which densities exhibit such ﬂuctuating phenomenon?

7 Missing Proofs

The Double Summation in the Proof of Lemma 4.
In a graphical constant-sum game  suppose
the underlying graph is G = ([m]  E)  and each edge (i  (cid:96)) ∈ E corresponds to a constant-sum game;
we will use the matrix Ai(cid:96) to denote the payoffs to Player i in this game.
In the calculation below  we write x• ≡ x•(t)  i.e.  we ignore the parameter t.
First  we rewrite the double summation as below:

m(cid:88)

(cid:88)

i=1

k∈Si

(xik − x∗

m(cid:88)

(cid:88)

i=1

(cid:96):(i (cid:96))∈E

(xi − x∗

i )TAi(cid:96)x(cid:96).

(cid:16)(cid:80)

ik) · ui(ek  x−i) =
(cid:88)

(xi − x∗

(cid:96):(i (cid:96))∈E

(cid:96) = 0 

i )TAi(cid:96)x∗
(cid:88)

(cid:17)

must

(cid:1)(cid:105)

.

Since x∗ is fully mixed  by the deﬁnition of NE  every entry in the vector
be identical. Thus 

(cid:96):(i (cid:96))∈E Ai(cid:96)x∗

(cid:96)

and hence

m(cid:88)

(cid:88)

i=1

k∈Si

ik) · ui(ek  x−i) =

m(cid:88)
(cid:2)(xi − x∗
(cid:96) ) + (x(cid:96) − x∗
i )TAi(cid:96)(x(cid:96) − x∗
(cid:104)(cid:0)(xi)TAi(cid:96)x(cid:96) + (x(cid:96))TA(cid:96)ixi

(cid:1) + (cid:0)(x∗

(cid:96):(i (cid:96))∈E

i=1

(xik − x∗

(cid:88)
(cid:88)

(i (cid:96))∈E

(i (cid:96))∈E

=

=

− (cid:0)(x∗

(xi − x∗

i )TAi(cid:96)(x(cid:96) − x∗
(cid:96) )

(cid:96) )TA(cid:96)i(xi − x∗

i )(cid:3)
(cid:1) − (cid:0)(xi)TAi(cid:96)x∗

i )TAi(cid:96)x∗

(cid:96) + (x∗

(cid:96) )TA(cid:96)ix∗

(cid:1)
(cid:96) + (x∗

i

i )TAi(cid:96)x(cid:96) + (x(cid:96))TA(cid:96)ix∗

i

(cid:96) )TA(cid:96)ixi

Note that in the ﬁnal expression  there are four terms  while each term is the sum of payoffs to the
Players i and (cid:96) in the two-person constant-sum game corresponded by the edge (i  (cid:96)) assuming the
players are using some mixed strategies. Therefore  the four terms are equal  and thus the overall
expression is zero.

Proof of Lemma 8. We prove the case when Player 1 has strategy j removed; the case for Player 2
is symmetric.
The game value val(G) can be described to be the following: Player 1 picks a probability distribution
1  x2) is always at least v  and val(G) is
x∗
1 such that no matter what Player 2’s choice x2 is  u1(x∗
the maximum possible value of v  while x∗
1 forms the mixed strategy of the player in a NE. Due to
the assumption that the unique NE is fully mixed  there is a unique fully mixed x∗
1 that attains the
maximum possible value of v; in other words  any x1 with x1j = 0 (which is equivalent to strategy j
being removed) must attain a value of v strictly less than val(G)  i.e.  val(G1j) < val(G).

Proof of Lemma 9. The proof follows closely the logic behind the derivations of inequalities (2)
and (3).

6The root of these properties is the “absolute conﬂict” nature of two-person constant-sum games  which does

not exist in general graphical constant-sum games.

9

Acknowledgments

The author would like to acknowledge Singapore NRF 2018 Fellowship NRF-NRFF2018-07 and
MOE AcRF Tier 2 Grant 2016-T2-1-170. The author thanks the anonymous reviewers for their helpful
suggestions and comments  and for pointing out the prior work about continuous replicator/FTRL
dynamics and the interplay between them and their discrete counterparts.

References
[1] Sanjeev Arora  Elad Hazan  and Satyen Kale. The multiplicative weights update method: a meta-algorithm

and applications. Theory of Computing  8(1):121–164  2012.

[2] James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In EC  pages

321–338  2018.

[3] Michel Benaïm. Dynamics of stochastic approximation algorithms. In Jacques Azéma  Michel Émery 

Michel Ledoux  and Marc Yor  editors  Séminaire de Probabilités XXXIII  pages 1–68  1999.

[4] Yang Cai and Constantinos Daskalakis. On minmax theorems for multiplayer games. In SODA  pages

217–234  2011.

[5] Erick Chastain  Adi Livnat  Christos H. Papadimitriou  and Umesh V. Vazirani. Multiplicative updates in

coordination games and the theory of evolution. In ITCS  pages 57–58  2013.

[6] Xi Chen  Xiaotie Deng  and Shang-Hua Teng. Settling the complexity of computing two-player nash

equilibria. J. ACM  56(3):14:1–14:57  2009.

[7] Constantinos Daskalakis  Alan Deckelbaum  and Anthony Kim. Near-optimal no-regret algorithms for

zero-sum games. Games and Economic Behavior  92:327–348  2015.

[8] Constantinos Daskalakis  Paul W. Goldberg  and Christos H. Papadimitriou. The complexity of computing

a nash equilibrium. SIAM J. Comput.  39(1):195–259  2009.

[9] Constantinos Daskalakis and Christos H. Papadimitriou. On a network generalization of the minmax

theorem. In ICALP  Part II  pages 423–434  2009.

[10] Yoav Freund and Robert E. Schapire. Game theory  on-line prediction and boosting. In COLT  pages

325–332  1996.

[11] Andreas Gaunersdorfer. Time averages for heteroclinic attractors. SIAM J. Appl. Math.  52:1476–1489 

1992.

[12] Andreas Gaunersdorfer and Josef Hofbauer. Fictitious play  shapley polygons  and the replicator equation.

Games and Economic Behavior  11:279–303  1995.

[13] Sergiu Hart and Yishay Mansour. How long to equilibrium? the communication complexity of uncoupled

equilibrium procedures. Games and Economic Behavior  69(1):107–126  2010.

[14] Josef Hofbauer and Karl Sigmund. Permanence for replicator equations. In Dynamical Systems  pages

70–91. Springer Berlin Heidelberg  1987.

[15] Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge University

Press  1998.

[16] Josef Hofbauer  Sylvain Sorin  and Yannick Viossat. Time average replicator and best-reply dynamics.

Math. Oper. Res.  34(2):263–269  2009.

[17] Joon Kwon and Panayotis Mertikopoulos. A continuous-time approach to online optimization. Journal of

Dynamics and Games  4(2):125–148  2017.

[18] Panayotis Mertikopoulos  Christos Papadimitriou  and Georgios Piliouras. Cycles in adversarial regularized

learning. In SODA  pages 2703–2717  2018.

[19] Sai Ganesh Nagarajan  Sameh Mohamed  and Georgios Piliouras. Three body problems in evolutionary

game dynamics: Convergence  periodicity and limit cycles. In AAMAS  pages 685–693  2018.

[20] John Nash. Non-cooperative games. The Annals of Mathematics  54(2):286–295  1951.

10

[21] Gerasimos Palaiopanos  Ioannis Panageas  and Georgios Piliouras. Multiplicative weights update with
constant step-size in congestion games: Convergence  limit cycles and chaos. In NIPS  pages 5874–5884 
2017.

[22] Georgios Piliouras and Leonard J. Schulman. Learning dynamics and the co-evolution of competing sexual

species. In ITCS  pages 59:1–59:3  2018.

[23] Georgios Piliouras and Jeff S. Shamma. Optimization despite chaos: Convex relaxations to complex limit

sets via poincaré recurrence. In SODA  pages 861–873  2014.

[24] Alexander Rakhlin and Karthik Sridharan. Optimization  learning  and games with predictable sequences.

In NIPS  pages 3066–3074  2013.

[25] Yuzuru Sato  Eizo Akiyama  and J. Doyne Farmer. Chaos in learning a simple two-person game. PNAS 

99(7):4748–4751  2002.

[26] Peter Schuster  Karl Sigmund  Josef Hofbauer  and Robert Wolff. Selfregulation of behaviour in animal

societies. part I: Symmetric contests. Biological Cybernetics  40(1):1–8  1981.

[27] Sylvain Sorin. Exponential weight algorithm in continuous time. Math. Program.  116(1-2):513–528 

2009.

[28] Vasilis Syrgkanis  Alekh Agarwal  Haipeng Luo  and Robert E. Schapire. Fast convergence of regularized

learning in games. In NIPS  pages 2989–2997  2015.

[29] E. C. Zeeman. Population dynamics from game theory. Lecture Notes in Mathematics  819:472–497  1980.

11

,Oriol Vinyals
Charles Blundell
Timothy Lillicrap
koray kavukcuoglu
Daan Wierstra
Yun Kuen Cheung