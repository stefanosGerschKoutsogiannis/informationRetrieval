2017,A Sharp Error Analysis for the Fused Lasso  with Application to Approximate Changepoint Screening,In the 1-dimensional multiple changepoint detection problem  we derive a new fast error rate for the fused lasso estimator  under the assumption that the mean vector has a sparse number of changepoints. This rate is seen to be suboptimal (compared to the minimax rate) by only a factor of $\log\log{n}$. Our proof technique is centered around a novel construction that we call a lower interpolant. We extend our results to misspecified models and exponential family distributions. We also describe the implications of our error analysis for the approximate screening of changepoints.,A Sharp Error Analysis for the Fused Lasso  with
Application to Approximate Changepoint Screening

Kevin Lin

Carnegie Mellon University

Pittsburgh  PA 15213

kevinl1@andrew.cmu.edu

James Sharpnack

University of California  Davis

Davis  CA 95616

jsharpna@ucdavis.edu

Alessandro Rinaldo

Carnegie Mellon University

Pittsburgh  PA 15213

arinaldo@stat.cmu.edu

Ryan J. Tibshirani

Carnegie Mellon University

Pittsburgh  PA 15213

ryantibs@stat.cmu.edu

Abstract

In the 1-dimensional multiple changepoint detection problem  we derive a new fast
error rate for the fused lasso estimator  under the assumption that the mean vector
has a sparse number of changepoints. This rate is seen to be suboptimal (compared
to the minimax rate) by only a factor of log log n. Our proof technique is centered
around a novel construction that we call a lower interpolant. We extend our results
to misspeciﬁed models and exponential family distributions. We also describe the
implications of our error analysis for the approximate screening of changepoints.

1

Introduction

Consider the 1-dimensional multiple changepoint model

i = 1  . . .   n 

yi = θ0 i + i 

(1)
where i  i = 1  . . .   n are i.i.d. errors  and θ0 i  i = 1  . . .   n is a piecewise constant mean sequence 
having a set of changepoints

S0 =(cid:8)i ∈ {1  . . .   n − 1} : θ0 i (cid:54)= θ0 i+1

(2)
This is a well-studied setting  and there is a large body of literature on estimation of the piecewise
constant mean vector θ0 ∈ Rn and its changepoints S0 using various estimators; refer  e.g.  to the
surveys Brodsky and Darkhovski (1993); Chen and Gupta (2000); Eckley et al. (2011).
In this work  we consider the 1-dimensional fused lasso (also called 1d fused lasso  or simply fused
lasso) estimator  which  given a data vector y ∈ Rn from a model as in (1)  is deﬁned by

(cid:9).

(yi − θi)2 + λ

|θi − θi+1| 

i=1

i=1

θ∈Rn

(3)
where λ ≥ 0 serves as a tuning parameter. This was proposed and named by Tibshirani et al. (2005) 
but the same idea was proposed earlier in signal processing  under the name total variation denoising 
by Rudin et al. (1992). Variants of the fused lasso have been used in biology to detect regions where
two genomic samples differ due to genetic variations (Tibshirani and Wang  2008)  in ﬁnance to detect
shifts in the stock market (Chan et al.  2014)  and in neuroscience to detect changes in stationary
behaviors of the brain (Aston and Kirch  2012). Popularity of the fused lasso can be attributed in part
to its computational scalability  the optimization problem in (3) being convex and highly structured.
There has also been plenty of supporting statistical theory developed for the fused lasso  which we
review in Section 2.

n−1(cid:88)

(cid:98)θ = argmin

n(cid:88)

1
2

31st Conference on Neural Information Processing Systems (NIPS 2017)  Long Beach  CA  USA.

Notation. We will make use of the following quantities that are deﬁned in terms of the mean θ0 in
(1) and its changepoint set S0 in (2). We denote the size of the changepoint set by s0 = |S0|. We
enumerate S0 = {t1  . . .   ts0}  where 1 ≤ t1 < . . . < ts0 < n  and for convenience we set t0 = 0 
ts0+1 = n. The smallest distance between changepoints in θ0 is denoted by

Wn = min

i=0 1... s0

(ti+1 − ti) 

and the smallest distance between consecutive levels of θ0 by
|θ0 i+1 − θ0 i|.

Hn = min
i∈S0

(4)

(5)

(6)

We use D ∈ R(n−1)×n to denote the difference operator
0
1
...

1
0 −1
...

D =

. . .
. . .
...
0 . . . −1

 −1

0

 .

0
0

1

Note that s0 = (cid:107)Dθ0(cid:107)0. We write DS to extract rows of D indexed by a subset S ⊆ {1  . . .   n − 1} 
and D−S to extract the rows in Sc = {1  . . .   n − 1} \ S.
For a vector x ∈ Rn  we use (cid:107)x(cid:107)2
2/n to denote its length-scaled (cid:96)2 norm. For sequences
an  bn  we use standard asymptotic notation: an = O(bn) to denote that an/bn is bounded for large
enough n  an = Ω(bn) to denote that bn/an is bounded for large enough n  an = Θ(bn) to denote
that both an = O(bn) and an = Ω(bn)  an = o(bn) to denote that an/bn → 0  and an = ω(bn)
to denote that bn/an → 0. For random sequences An  Bn  we write An = OP(Bn) to denote that
An/Bn is bounded in probability. A random variable Z is said to have a sub-Gaussian distribution
provided that E(Z) = 0 and P(|Z| > t) ≤ 2 exp(−t2/(2σ2)) for all t ≥ 0  and a constant σ > 0.

n = (cid:107)x(cid:107)2

Summary of results. Our main focus is on deriving a sharp estimation error bound for the fused
lasso  parametrized by the number of changepoints s0 in θ0. We also study several consequences of
our error bound and its analysis. A summary of our contributions is as follows.

• New error analysis for the fused lasso. In Section 3  we develop a new error analysis for
the fused lasso  in the model (1) with sub-Gaussian errors. Our analysis leverages a novel
quantity that we call a lower interpolant to approximate the fused lasso estimate (once it has
been orthogonalized with respect to the changepoint structure of the mean θ0) with 2s0 + 2
monotonic segments  which allows for ﬁner control of the empirical process term.
When s0 = O(1)  and the changepoint locations in S0 are (asymptotically) evenly spaced 
n = O(log n(log log n)/n) for the fused lasso estimator

our main result implies E(cid:107)(cid:98)θ − θ0(cid:107)2
(cid:98)θ in (3). This is slower than the minimax rate by a log log n factor. Our result improves on

previously established results from Dalalyan et al. (2017)  and after the completion of this
paper  was itself improved upon by Guntuboyina et al. (2017) (who are able to remove the
extraneous log log n factor).
• Extension to misspeciﬁed and exponential family models. In Section 4  we extend our
error analysis to cover a mean vector θ0 that is not necessarily piecewise constant (or in
other words  has potentially many changepoints). In Section 5  we extend our analysis to
exponential family models. The latter extension  especially  is of practical importance  as
many applications  e.g.  CNV data analysis  call for changepoint detection on count data.
• Application to approximate screening and recovery. In Section 6  we establish that the
maximum distance between any true changepoint and its nearest estimated changepoint is
n) using the fused lasso  when s0 = O(1) and all changepoints are
OP(log n(log log n)/H 2
(asymptotically) evenly spaced. After applying simple post-processing step  we show that
the maximum distance between any estimated changepoint and its nearest true changepoint
is of the same order. Our proof technique relies only on the estimation error rate of the fused
lasso  and therefore immediately generalizes to any estimator of θ0  where the distance (for
approximate changepoint screening and recovery) is a function of the inherent error rate.

The supplementary document gives numerical simulations that support the theory in this paper.

2

2 Preliminary review of existing theory

We begin by describing known results on the quantity (cid:107)(cid:98)θ − θ0(cid:107)2
fused lasso estimate(cid:98)θ in (3) and the mean θ0 in (1).

n  the estimation error between the

Early results on the fused lasso are found in Mammen and van de Geer (1997) (see also Tibshirani
(2014) for a translation to a setting more consistent with that of the current paper). These authors
study what may be called the weak sparsity case  in which it is that assumed (cid:107)Dθ0(cid:107)1 ≤ Cn  with D
being the difference operator in (6). Assuming additionally that the errors in (1) are sub-Gaussian 
−1/3
Mammen and van de Geer (1997) show that for a choice of tuning parameter λ = Θ(n1/3C
) 
n

the fused lasso estimate(cid:98)θ in (3) satisﬁes
(cid:107)(cid:98)θ − θ0(cid:107)2

n = OP(n−2/3C 2/3
n ).

(7)

The weak sparsity setting is not the focus of our paper  but we still recall the above result to give
a sense of the difference between the weak and strong sparsity settings  the latter being the setting
in which we assume control over s0 = (cid:107)Dθ0(cid:107)0  as we do in the current paper. Prior to this paper 
the strongest result in the strong sparsity setting was given by Dalalyan et al. (2017)  who assume

N (0  σ2) errors in (1)  and show that for λ = σ(cid:112)2n log(n/δ)  the fused lasso estimate satisﬁes

n ≤ Cσ2 s0 log(n/δ)

(8)
with probability at least 1 − 2δ  for large enough n  and a constant C > 0  where recall Wn is the
minimum distance between changepoints in θ0  as in (4). Our main result in Theorem 1 improves
upon (8) in two ways: by reducing the ﬁrst log n term inside the brackets to log s0 + log log n  and

reducing the second n/Wn term to(cid:112)n/Wn.

log n +

n

 

After our paper was completed  Guntuboyina et al. (2017) gave an even sharper error rate for the
fused lasso (and more broadly  for trend the family of higher-order ﬁltering estimates as deﬁned in
Steidl et al. (2006); Kim et al. (2009); Tibshirani (2014)). Again assuming N (0  σ2) errors in (1) 
as well as Wn ≥ cn/(s0 + 1) for some constant c ≥ 1  these authors show that the family of fused

lasso estimates {(cid:98)θλ  λ ≥ 0} (using subscripts here to explicitly denote the dependence on the tuning

parameter λ) satisﬁes

(cid:18)

(cid:19)

n
Wn

(cid:107)(cid:98)θ − θ0(cid:107)2

(cid:107)(cid:98)θλ − θ0(cid:107)2

(cid:18) en

(cid:19)

4σ2δ

 

n

n ≤ Cσ2 s0 + 1

n

+

log

inf
λ≥0

s0 + 1

sharper than ours in Theorem 1 in that (log s0 + log log n) log n +(cid:112)n/Wn is replaced essentially

(9)
with probability at least 1 − exp(−δ)  for large enough n  and a constant C > 0. The above bound is
by log Wn. (Also  the result in (9) does not actually require Wn ≥ cn/(s0 + 1)  but only requires the
distance between changepoints where jumps alternate in sign to be larger than cn/(s0 + 1)  which is
another improvement.) Further comparisons will be made in Remark 1 following Theorem 1.
There are numerous other estimators  e.g.  based on segmentation techniques or wavelets  that admit
estimation results comparable to those above. These are described in Remark 2 following Theorem 1.
Lastly  it can be seen the minimax estimation error over the class of signals θ0 with s0 changepoints 
assuming N (0  σ2) errors in (1)  satisﬁes

E(cid:107)(cid:98)θ − θ0(cid:107)2

n ≥ Cσ2 s0
n

log

(cid:18) n

(cid:19)

s0

inf(cid:98)θ

sup

(cid:107)Dθ0(cid:107)0≤s0

 

(10)

for large enough n  and a constant C > 0. This says that one cannot hope to improve the rate in (9).
The minimax result in (10) follows from standard minimax theory for sparse normal means problems 
as in  e.g.  Johnstone (2015); for a proof  see Padilla et al. (2016).

3 Sharp error analysis for the fused lasso estimator

Here we derive a sharper error bound for the fused lasso  improving upon the previously established
result of Dalalyan et al. (2017) as stated in (8). Our proof is based on a concept that we call a lower
interpolant  which as far as we can tell  is a new idea that may be of interest in its own right.

3

distribution. Then under a choice of tuning parameter λ = (nWn)1/4  the fused lasso estimate(cid:98)θ in

Theorem 1. Assume the data model in (1)  with errors i  i = 1  . . .   n i.i.d. from a sub-Gaussian

(3) satisﬁes

(cid:107)(cid:98)θ − θ0(cid:107)2

n ≤ γ2c

s0
n

(log s0 + log log n) log n +

with probability at least 1 − exp(−Cγ)  for all γ > 1 and n ≥ N  where c  C  N > 0 are constants
that depend on only σ (the parameter appearing in the sub-Gaussian distribution of the errors).

An immediate corollary is as follows.
Corollary 1. Under the same assumptions as in Theorem 1  we have

E(cid:107)(cid:98)θ − θ0(cid:107)2

n ≤ c

s0
n

(log s0 + log log n) log n +

(cid:32)

(cid:32)

(cid:33)

 

(cid:114) n

Wn

(cid:33)

 

(cid:114) n

Wn

for some constant c > 0.

E(cid:107)(cid:98)θλ − θ0(cid:107)2

(cid:18) en

(cid:19)

 

We give some remarks comparing Theorem 1 to related results in the literature.
Remark 1 (Comparison to Dalalyan et al. (2017); Guntuboyina et al. (2017)). We can see that
the result in Theorem 1 is sharper than that in (8) from Dalalyan et al. (2017) for any s0  Wn  as

log s0 ≤ log n and(cid:112)n/Wn ≤ n/Wn. Moreover  when s0 = O(1) and Wn = Θ(n)  the rates are
in that it reduces the factor of (log s0 + log log n) log n +(cid:112)n/Wn to a single term of log Wn. In

log2 n/n and log n(log log n)/n from Theorem 1 and (8)  respectively.
Comparing the result in Theorem 1 to that in (9) from Guntuboyina et al. (2017)  the latter is sharper

the case s0 = O(1) and Wn = Θ(n)  the rates are log n(log log n)/n and log n/n from Theorem 1
and (8)  respectively  and the latter rate cannot be improved  owing to the minimax lower bound in
(10). Similar to our expectation bound in Corollary 1  Guntuboyina et al. (2017) establish

n ≤ Cσ2 s0 + 1

n

for the family of fused lasso estimates {(cid:98)θλ  λ ≥ 0}  for large enough n  and a constant C > 0. Like

s0 + 1

inf
λ≥0

(11)

log

i=1 |θi − θi+1| in (3) with the (cid:96)0 penalty(cid:80)n−1

n = O(log2 n/n) as shown by Donoho and Johnstone (1994). Pairing unbal-
anced Haar (UH) wavelets with a basis selection method  Fryzlewicz (2007) developed an estimator
n = O(log2 n/n). Though they are not written in this form  the results in

their high probability result in (9)  their expectation result in (11) is stated in terms of an inﬁmum
over λ ≥ 0  and does not provide an explicit value of λ that attains the bound. (Inspection of their
proofs suggests that it is not at all easy to make such a value of λ explicit.) Meanwhile  Theorem 1
and Corollary 1 have the advantage this choice is made explicit  as in λ = (nWn)1/4.
deﬁned by replacing the (cid:96)1 penalty(cid:80)n−1
Remark 2 (Comparison to other estimators). Various other estimators obtain comparable estima-
and denoted say by(cid:98)θPotts  satisﬁes a bound (cid:107)(cid:98)θPotts − θ0(cid:107)2
tion error rates. In what follows  all results are stated in the case s0 = O(1). The Potts estimator 
i=1 1{θi (cid:54)= θi+1} 
et al. (2009). Wavelet denoising (placing weak conditions on the wavelet basis)  denoted by(cid:98)θwav 
satisﬁes E(cid:107)(cid:98)θwav − θ0(cid:107)2
n = O(log n/n) a.s. as shown by Boysen
(cid:98)θUH with E(cid:107)(cid:98)θUH − θ0(cid:107)2
Fryzlewicz (2016) imply that his “tail-greedy” unbalanced Haar (TGUH) estimator (cid:98)θTGUH  satisﬁes
(cid:107)(cid:98)θTGUH − θ0(cid:107)2
of the fused lasso estimate(cid:98)θ in (3)):
(cid:107)(cid:98)θ − θ0(cid:107)2
To precisely control the empirical process term (cid:62)((cid:98)θ − θ0)  we consider a decomposition
where we deﬁne(cid:98)δ = P0((cid:98)θ − θ0) and(cid:98)x = P1(cid:98)θ. Here P0 is the projection matrix onto the piecewise

2 ≤ 2(cid:62)((cid:98)θ − θ0) + 2λ(cid:0)(cid:107)Dθ0(cid:107)1 − (cid:107)D(cid:98)θ(cid:107)1
(cid:62)((cid:98)θ − θ0) = (cid:62)(cid:98)δ + (cid:62)(cid:98)x 

Here is an overview of the proof of Theorem 1. The full proof is deferred until the supplement  as
with all proofs in this paper. We begin by deriving a basic inequality (stemming from the optimality

constant structure inherent in θ0  and P1 = I − P0. More precisely  writing S0 = {t1  . . .   ts0} for
the set of ordered changepoints in θ0  we deﬁne Bj = {tj + 1  . . .   tj+1}  and denote by 1Bj ∈ Rn

n = O(log2 n/n) with probability tending to 1.

(cid:1).

(12)

4

}. The parameter(cid:98)δ lies in an low-dimensional
subspace  which makes bounding the term (cid:62)(cid:98)δ relatively easy. Bounding the term (cid:62)(cid:98)x requires a
the indicator of block Bj  for j = 0  . . .   s0. In this notation  P0 is the projection onto the (s0 + 1)-
dimensional linear subspace R = span{1B0  . . .   1Bs0
Lemma 1 is a deterministic result ensuring the existence of what we call a lower interpolant(cid:98)z to(cid:98)x.
much more intricate argument  which is spelled out in the following lemmas.
This interpolant approximates(cid:98)x using 2s0 + 2 monotonic segments  and its empirical process term
(cid:62)(cid:98)z can be ﬁnely controlled  as shown in Lemma 2. The residual from the interpolant approximation 
denoted (cid:98)w =(cid:98)x −(cid:98)z  has an empirical process term (cid:62)(cid:98)w that is more crudely controlled  in Lemma 3.
Put together  as in (cid:62)(cid:98)x = (cid:62)(cid:98)z + (cid:62)(cid:98)w  gives the ﬁnal control on (cid:62)(cid:98)x.
Before stating Lemma 1  we deﬁne the class of vectors containing the lower interpolant. Given any
collection of changepoints t1 < . . . < ts0 (and t0 = 0  ts0+1 = n)  let M be the set of “piecewise
monotonic” vectors z ∈ Rn  with the following properties  for each i = 0  . . .   s0:

(i) there exists a point t(cid:48)
i  . . .   ti+1};

nonincreasing over the segment j ∈ {ti + 1  . . .   t(cid:48)
j ∈ {t(cid:48)

i such that ti + 1 ≤ t(cid:48)

i ≤ ti+1  and such that the absolute value |zj| is
i}  and nondecreasing over the segment

(ii) the signs remain constant on the monotone pieces 

sign(zti) · sign(zj) ≥ 0 
sign(zti+1) · sign(zj) ≥ 0 

j = ti + 1  . . .   t(cid:48)
i 
j = t(cid:48)

i + 1  . . .   ti+1.

Now we state our lemma that characterizes the lower interpolant.
Lemma 1. Given changepoints t0 < . . . < ts0+1  and any x ∈ Rn  there exists a vector z ∈ M (not
necessarily unique)  such that the following statements hold:

(cid:107)D−S0x(cid:107)1 = (cid:107)D−S0z(cid:107)1 + (cid:107)D−S0(x − z)(cid:107)1 
√
s0√
(cid:107)DS0 x(cid:107)1 = (cid:107)DS0 z(cid:107)1 ≤ (cid:107)D−S0z(cid:107)1 +
4
Wn
(cid:107)z(cid:107)2 ≤ (cid:107)x(cid:107)2
(cid:107)x − z(cid:107)2 ≤ (cid:107)x(cid:107)2 

and

(cid:107)z(cid:107)2 

(13)

(14)

(15)
where D ∈ R(n−1)×n is the difference matrix in (6). We call a vector z with these properties a lower
interpolant to x.

Loosely speaking  the lower interpolant(cid:98)z can be visualized by taking a string that lies initially on top
of(cid:98)x  is nailed down at the changepoints t0  . . . ts0+1  and then pulled taut while maintaining that it is
not greater (elementwise) than(cid:98)x  in magnitude. Here “pulling taut” means that (cid:107)D(cid:98)z(cid:107)1 is made small.
Figure 1 provides illustrations of the interpolant(cid:98)z to(cid:98)x for a few examples.
Note that(cid:98)z consists of 2s0 + 2 monotonic pieces. This special structure leads to a sharp concentration
(cid:32)
I (log s0 + log log n)(cid:1) 

inequality. The next lemma is the primary contributor to the fast rate given in Theorem 1.
Lemma 2. Given changepoints t1 < . . . < ts0  there exists constants cI   CI   NI > 0 such that when
 ∈ Rn has i.i.d. sub-Gaussian components 

(cid:112)(log s0 + log log n)s0 log n

≤ 2 exp(cid:0) − CI γ2c2

(cid:33)

> γcI

P

|(cid:62)z|
(cid:107)z(cid:107)2

sup
z∈M

for any γ > 1  and n ≥ NI.

Finally  the following lemma controls the residuals  (cid:98)w =(cid:98)x −(cid:98)z.
(cid:19)

(cid:18)

(cid:112)(cid:107)D−S0w(cid:107)1(cid:107)w(cid:107)2

|(cid:62)w|

P

sup
w∈R⊥

> γcR(ns0)1/4

≤ 2 exp(−CRγ2c2

R

√

s0) 

Lemma 3. Given changepoints t1 < . . . < ts0  there exists constants cR  CR > 0 such that when
 ∈ Rn has i.i.d. sub-Gaussian components 

for any γ > 1  where R⊥ is the orthogonal complement of R = span{1B0  . . .   1Bs0

}.

5

Figure 1: The lower interpolants for two examples (in the left and right columns)  each with n = 800 points. In
the top row  the data y (in gray) and underlying signal θ0 (red) are plotted across the locations 1  . . .   n. Also

shown is the fused lasso estimate(cid:98)θ (blue). In the bottom row  the error vector(cid:98)x = P1(cid:98)θ is plotted (blue) as well

as the interpolant (black)  and the dotted vertical lines (red) denote the changepoints t1  . . . ts0 of θ0.

4 Extension to misspeciﬁed models

We consider data from the model in (1) but where the mean θ0 is not necessarily piecewise constant
(i.e.  where s0 is potentially large). Let us deﬁne
(cid:107)θ0 − θ(cid:107)2

subject to (cid:107)Dθ(cid:107)0 ≤ s 

θ0(s) = argmin

(16)

θ∈Rn

which we call the best s-approximation to θ0. We now present an extension of Theorem 1.
Theorem 2. Assume the data model in (1)  with errors i  i = 1  . . .   n i.i.d. from a sub-Gaussian
distribution. For any s  consider the best s-approximation θ0(s) to θ0  as in (16)  and let Wn(s) be
the minimum distance between the s changepoints in θ0(s). Then under a choice of tuning parameter

λ = (nWn(s))1/4  the fused lasso estimate(cid:98)θ in (3) satisﬁes

n ≤ (cid:107)θ0(s) − θ0(cid:107)2

(17)
with probability at least 1 − exp(−Cγ)  for all γ > 1 and n ≥ N  where c  C  N > 0 are constants

that depend on only σ. Further  if λ is chosen large enough so that (cid:107)D(cid:98)θ(cid:107)0 ≤ s on an event E  then

(log s + log log n) log n +

n + γ2c

Wn(s)

 

(cid:107)(cid:98)θ − θ0(cid:107)2

2

(cid:32)

s
n

(cid:33)

(cid:114) n
(cid:33)

n
λ2

(cid:32)

(cid:107)(cid:98)θ − θ0(s)(cid:107)2

n ≤ γ2c

s
n

(18)
on E intersected with an event of probability at least 1 − exp(−Cγ)  for all γ > 1  n ≥ N  where
c  C  N > 0 are the same constants as above.

(log s + log log n) log n +

Wn(s)

+

 

λ2

The ﬁrst result in (17) in Theorem 2 is a standard oracle inequality. It provides a bound on the error
of the fused lasso estimator that decomposes into two parts  the ﬁrst term being the approximation
error  determined by the proximity of θ0(s) to θ0  and second term being the usual bound we would
encounter if the mean truly had s changepoints.

The second result in (18) in the theorem is a direct bound on the estimation error (cid:107)(cid:98)θ − θ0(s)(cid:107)2
take λ to be large enough for(cid:98)θ to itself have s changepoints. But the rate worsens as λ grows larger 
with s changepoints  then we may have to take λ very large to ensure that(cid:98)θ has s changepoints).

see that the estimation error can be small  apparently regardless of the size of (cid:107)θ0(s) − θ0(cid:107)2

so implicitly  the proximity of θ0(s) to θ0 does play an role (if θ0 were actually far away from a signal

n. We
n  if we

6

llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0200400600800−50510Index0200400600800−1.5−0.50.00.51.0llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0200400600800−202468Index0200400600800−3−2−101Remark 3 (Comparison to other results). Dalalyan et al. (2017); Guntuboyina et al. (2017) also
provide oracle inequalities and their results could be adapted to take forms as in Theorem 2. It is not
clear to us that previous results on other estimators  such as those from Remark 2  adapt as easily.

5 Extension to exponential family models
We consider data y = (y1  . . .   yn) ∈ Rn with independent components distributed according to

p(yi; θ0 i) = h(yi) exp(cid:0)yiθ0 i − Λ(θ0 i)(cid:1) 

(19)
Here  for each i = 1  . . .   n  the parameter θ0 i is the natural parameter in the exponential family and
Λ is the cumulant generating function. As before  in the location model  we are mainly interested in
the case in which the natural parameter vector θ0 is piecewise constant (with s0 denoting its number
of changepoints  as before). Estimation is now based on penalization of the negative log-likelihood:

i = 1  . . .   n.

(cid:98)θ = argmin

θ∈Rn

n(cid:88)

(cid:0) − yiθi + Λ(θi)(cid:1) + λ

n(cid:88)

i=1

i=1

|θi − θi+1| 

(20)

Since the cumulant generating function Λ is always convex in exponential families  the above is a
convex optimization problem. We present an estimation error bound the present setting.
Theorem 3. Assume the data model in (19)  with a strictly convex  twice continuously differentiable
cumulant generating function Λ. Assume that θ0 i ∈ [l  u]  i = 1  . . .   n for constants l  u ∈ R  and
add the constraints θi ∈ [l  u]  i = 1  . . .   n in the optimization problem in (20). Finally  assume that
the random variables yi − E(yi)  i = 1  . . .   n obey a sub-Gaussian distribution  with parameter σ.
Then under a choice of tuning parameter λ = (nWn)1/4  the exponential family fused lasso estimate

(cid:98)θ in (20) (subject to the additional boundedness constraints) satisﬁes

(cid:33)

 

(cid:114) n

Wn

(cid:32)

(cid:107)(cid:98)θ − θ0(cid:107)2

n ≤ γ2c

s0
n

(log s0 + log log n) log n +

with probability at least 1 − exp(−Cγ)  for all γ > 1 and n ≥ N  where c  C  N > 0 are constants
that depend on only l  u  σ.
Remark 4 (Roles of l  u). The restriction of θ0 i and the optimization parameters in (20) to [l  u] 
for i = 1  . . .   n  is used to ensure that the second derivative of Λ is bounded away from zero. (The
same property could be accomplished by instead adding a small squared (cid:96)2 penalty on θ in (20).) A
more reﬁned analysis could alleviate the need for this bounded domain (or extra squared (cid:96)2 penalty)
but we do not pursue this for simplicity.
Remark 5 (Sub-Gaussianity in exponential families). When are the random variables yi − E(yi) 
i = 1  . . .   n sub-Gaussian  in an exponential family model (19)? A simple sufﬁcient condition (not
speciﬁc to exponential families  in fact) is that these centered variates are bounded. This covers the
binomial model yi ∼ Bin(k  µ(θ0 i))  where µ(θ0 i) = 1/(1 + e−θ0 i)  i = 1  . . .   n  and k is a ﬁxed
constant. Hence Theorem 3 applies to binomial data.
For Poisson data yi ∼ Pois(µ(θ0 i))  where µ(θ0 i) = eθ0 i  i = 1  . . .   n  we now give two options
for the analysis. The ﬁrst is to assume a maximum achieveable count (which may be reasonable in
CNV data) and then apply Theorem 3 owing again to boundedness. The second is to invoke the fact
that Poisson random variables have sub-exponential (rather than sub-Gaussian) tails  and then use a

truncation argument  to show that for the Poisson fused lasso estimate(cid:98)θ in (20) (under the additional

boundedness constraints)  with λ = log n(nWn)1/4 

n ≤ γ2c

s0 log n

n

(log s0 + log log n) log n +

(21)
with probability at least 1 − exp(−Cγ) − 1/n  for all γ > 1 and n ≥ N  where c  C  N > 0 are
constants depending on l  u. This is slower than the rate in Theorem 3 by a factor of log n.
Remark 6 (Comparison to other results). The results in Dalalyan et al. (2017); Guntuboyina et al.
(2017) assume normal errors. It seems believable to us that the results of Dalalyan et al. (2017) could
be extended to sub-Gaussian errors and hence exponential family data  in a manner similar to what
we have done above in Theorem 3. To us  this is less clear for the results of Guntuboyina et al. (2017) 
which rely on some technical calculations involving Gaussian widths. It is even less clear to us how
results from other estimators  as in Remark 2  extend to exponential family data.

Wn

 

(cid:107)(cid:98)θ − θ0(cid:107)2

(cid:32)

(cid:33)

(cid:114) n

7

6 Approximate changepoint screening and recovery

In many applications of changepoint detection  one may be interested in estimation of the changepoint
locations in θ0  rather than the mean vector θ0 as a whole. In this section  we show that estimation of
the changepoint locations and of θ0 itself are two very closely linked problems  in the following sense:
any procedure with guarantees on its error in estimating θ0 automatically has certain approximate
changepoint detection guarantees  and not surprisingly  a faster error rate (in estimating θ0) translates
into a stronger statement about approximate changepoint detection. We use this general link to prove
new approximate changepoint screening results for the fused lasso. We also show that in general a
simple post-processing step may be used to discard spurious detected changepoints  and again apply
this to the fused lasso to yield new approximate changepoint recovery results.
It helps to introduce some additional notation. For a vector θ ∈ Rn  we write S(θ) for the set of its
changepoint indices  i.e. 

S(θ) =(cid:8)i ∈ {1  . . .   n − 1} : θi (cid:54)= θi+1

(cid:9).

Recall  we abbreviate S0 = S(θ0) for the changepoints of the underlying mean θ0. For two discrete
sets A  B  we deﬁne the metrics
min
a∈A

and dH (A  B) = max(cid:8)d(A|B)  d(B|A)}.

d(A|B) = max
b∈B

|a − b|

The ﬁrst metric above can be seen as a one-sided screening distance from B to A  measuring the
furthest distance of an element in B to its closest element in A. The second metric above is known as
the Hausdorff distance between A and B.

Approximate changepoint screening. We present our general theorem on changepoint screening.
The basic idea behind the result is quite simple: if an estimator misses a (large) changepoint in θ0 
then its estimation error must suffer  and we can use this fact to bound the screening distance.

Theorem 4. Let(cid:101)θ ∈ Rn be an estimator such that (cid:107)(cid:101)θ − θ0(cid:107)2
(cid:18) nRn

n =
o(Wn)  where  recall  Hn is the minimum gap between adjacent levels of θ0  deﬁned in (5)  and Wn
is the minimum distance between adjacent changepoints of θ0  deﬁned in (4). Then

d(cid:0)S((cid:101)θ)| S0
tantly  Theorem 4 assumes no data model whatsoever  and treats(cid:101)θ as a generic estimator of θ0. (Of
course  through the statement (cid:107)(cid:101)θ − θ0(cid:107)2
n = OP(Rn)  one can see that(cid:101)θ is random  constructed from
data that depends on θ0  but no speciﬁc data model is required  nor are any speciﬁc properties of(cid:101)θ 

Remark 7 (Generic setting: no speciﬁc data model  and no assumptions on estimator). Impor-

n = OP(Rn). Assume that nRn/H 2

(cid:1) = OP

other than its error rate.) This ﬂexibility allows for the result to be applied in any problem setting in
which one has control of the error in estimating a piecewise constant parameter θ0 (in some cases
this may be easier to obtain  compared to direct analysis of detection properties). A similar idea was
used (concurrently and independently) by Fryzlewicz (2016) in the analysis of the TGUH estimator.

(cid:19)

H 2
n

.

−1/3
n

) satisﬁes

Combining the above theorem with known error rates for the fused lasso estimator—(7) in the weak
sparsity case  and Theorem 1 in the strong sparsity case—gives the following result.
Corollary 2. Assume the data model in (1)  with errors i  i = 1  . . .   n i.i.d. from a sub-Gaussian
distribution. Let Cn = (cid:107)Dθ0(cid:107)1  and assume that Hn = ω(n1/6C 1/3
Wn). Then the fused lasso
n /

estimator(cid:98)θ in (3) with λ = Θ(n1/3C
Alternatively  assume s0 = O(1)  Wn = Θ(n)  and Hn = ω((cid:112)log n(log log n)/n). Then the fused
Remark 8 (Changepoint detection limit). The restriction Hn = ω((cid:112)log n(log log n)/n) for (23)

(cid:18) n1/3C 2/3
(cid:19)
(cid:1) = OP
d(cid:0)S((cid:98)θ)| S0
(cid:18) log n(log log n)
(cid:1) = OP
d(cid:0)S((cid:98)θ)| S0

√
lasso with λ = Θ(

in Corollary 2 is very close to the optimal detection limit of Hn = ω(1/
n): Duembgen and Walther
(2008) showed that in Gaussian changepoint model with a single elevated region  and Wn = Θ(n) 
there is no test for detecting a changepoint that has asymptotic power 1 unless Hn = ω(1/

n) satisﬁes

n
H 2
n

(cid:19)

(22)

√

n).

(23)

H 2
n

.

√

.

√

8

H 2
n

Combining Theorem 4 with (21) gives the following (a similar result holds for the binomial model).
Corollary 3. Assume yi ∼ Pois(eθ0 i)  independently  for i = 1  . . .   n  and assume (cid:107)θ0(cid:107)∞ = O(1) 

s0 = O(1)  Wn = Θ(n)  Hn = ω(log n(cid:112)log log n/n). Then for the Poisson fused lasso estimator
(cid:98)θ in (20) (subject to appropriate boundedness constraints) with λ = Θ(log n

n)  we have

√

d(cid:0)S((cid:98)θ)| S0

(cid:1) = OP

(cid:18) log2 n(log log n)

(cid:19)

.

bn

j=i+1

1
bn

(cid:101)θj 

j=i−bn+1

i(cid:88)

i+bn(cid:88)

(cid:101)θj − 1

Fi((cid:101)θ) =

for i = bn  . . .   n − bn 

Approximate changepoint recovery. We present a post-processing procedure for the estimated

and retaining only locations at which the ﬁlter value is large (in magnitude)  we can approximately
recovery the changepoints of θ0  in the Hausdorff metric.

changepoints in(cid:101)θ  to eliminate changepoints of(cid:101)θ that lie far away from changepoints of θ0. Our
procedure is based on convolving(cid:101)θ with a ﬁlter that resembles the mother Haar wavelet. Consider
for an integral bandwidth bn > 0. By evaluating the ﬁlter Fi((cid:101)θ) at all locations i = bn  . . .   n − bn 
Theorem 5. Let(cid:101)θ ∈ Rn be such that (cid:107)(cid:101)θ − θ0(cid:107)2
(cid:111) ∪ {bn  n − bn} 
IF ((cid:101)θ) =
and deﬁne a set of ﬁltered points SF ((cid:101)θ) = {i ∈ IF ((cid:101)θ) : |Fi((cid:101)θ)| ≥ τn}  for a threshold level τn. If
Note that the set of ﬁltered points |SF ((cid:101)θ)| in Theorem 5 is not necessarily of a subset of the original
set of estimated changepoints S((cid:101)θ)  but it has the property |SF ((cid:101)θ)| ≤ 3|S((cid:101)θ)| + 2.

(cid:110)
i ∈ {bn  . . .   n − bn} : i ∈ S((cid:101)θ)  or i + bn ∈ S((cid:101)θ)  or i − bn ∈ S((cid:101)θ)

n)  2bn ≤ Wn  and τn/Hn → ρ ∈ (0  1) as n → ∞  then
dH

evaluate the ﬁlter in (24) with bandwidth bn at locations in

n = OP(Rn). Consider the following procedure: we

(cid:0)SF ((cid:101)θ)  S0

bn  τn satisfy bn = ω(nRn/H 2

(cid:1) ≤ 2bn

(cid:17) → 1

as n → ∞.

P(cid:16)

(24)

We ﬁnish with corollaries for the fused lasso. For space reasons  remarks comparing them to related
approximate recovery results in the literature are deferred to the supplement.
Corollary 4. Assume the data model in (1)  with errors i  i = 1  . . .   n i.i.d. from a sub-Gaussian
distribution. Let Cn = (cid:107)Dθ0(cid:107)1. If we apply the post-processing procedure in Theorem 5 to the fused
n(cid:99) ≤ Wn/2 for a sequence
νn → ∞  and τn/Hn → ρ ∈ (0  1)  then

)  bn = (cid:98)n1/3C 2/3

−1/3
n

n/H 2

n ν2

(cid:18)

lasso estimator(cid:98)θ in (3) with λ = Θ(n1/3C
(cid:0)SF ((cid:98)θ)  S0
(cid:0)SF ((cid:98)θ)  S0

(cid:18)

dH

dH

P

(cid:1) ≤ 2n1/3C 2/3

n ν2
n

H 2
n

(cid:19)

→ 1

as n → ∞.

(25)

√
Alternatively  assuming s0 = O(1)  Wn = Θ(n)  if we apply the same post-processing procedure to
n(cid:99) ≤ Wn/2 for a sequence νn → ∞ 
(cid:19)
the fused lasso with λ = Θ(
and τn/Hn → ρ ∈ (0  1)  then

n)  bn = (cid:98)log n(log log n)ν2

(26)
Corollary 5. Assume yi ∼ Pois(eθ0 i)  independently  for i = 1  . . .   n  and assume (cid:107)θ0(cid:107)∞ = O(1) 
lasso estimator(cid:98)θ in (20) (subject to appropriate boundedness constraints) with λ = Θ(log n
√
s0 = O(1)  Wn = Θ(n). If we apply the post-processing method in Theorem 5 to the Poisson fused
n) 
n(cid:99) ≤ Wn/2 for a sequence νn → ∞  and τn/Hn → ρ ∈ (0  1)  then
bn = (cid:98)log2 n(log log n)ν2

(cid:1) ≤ 2 log n(log log n)ν2

as n → ∞.

n/H 2

→ 1

H 2
n

P

n

(cid:18)

n/H 2

(cid:0)SF ((cid:98)θ)  S0

(cid:1) ≤ 2 log2 n(log log n)ν2

n

dH

H 2
n

(cid:19)

→ 1

as n → ∞.

P

7 Summary

We gave a new error analysis for the fused lasso  with extensions to misspeciﬁed models and data
from exponential families. We showed that error bounds for general changepoint estimators lead to
approximate changepoint screening results  and after post-processing  approximate recovery results.

Acknolwedgements. JS was supported by NSF Grant DMS-1712996. RT was supported by NSF
Grant DMS-1554123.

9

References
John AD Aston and Claudia Kirch. Evaluating stationarity via change-point alternatives with

applications to fMRI data. The Annals of Applied Statistics  6(4):1906–1948  2012.

Leif Boysen  Angela Kempe  Volkmar Liebscher  Axel Munk  and Olaf Wittich. Consistencies and
rates of convergence of jump-penalized least squares estimators. The Annals of Statistics  37(1):
157–183  2009.

Boris Brodsky and Boris Darkhovski. Nonparametric Methods in Change-Point Problems. Springer 

1993.

Ngai Hang Chan  Chun Yip Yau  and Rong-Mao Zhang. Group lasso for structural break time series.

Journal of the American Statistical Association  109(506):590–599  2014.

Jie Chen and Arjun Gupta. Parametric Statistical Change Point Analysis. Birkhauser  2000.

Arnak S. Dalalyan  Mohamed Hebiri  and Johannes Lederer. On the prediction performance of the

lasso. Bernoulli  23(1):552–581  2017.

David L Donoho and Iain M Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika 

81(3):425–455  1994.

Lutz Duembgen and Guenther Walther. Multiscale inference about a density. The Annals of Statistics 

36(4):1758–1785  2008.

Idris Eckley  Paul Fearnhead  and Rebecca Killick. Analysis of changepoint models. In David
Barber  Taylan Cemgil  and Silvia Chiappa  editors  Bayesian Time Series Models  chapter 10 
pages 205–224. Cambridge University Press  Cambridge  2011.

Piotr Fryzlewicz. Unbalanced Haar technique for nonparametric function estimation. Journal of the

American Statistical Association  102(480):1318–1327  2007.

Piotr Fryzlewicz. Tail-greedy bottom-up data decompositions and fast multiple change-point detection.

2016. URL http://stats.lse.ac.uk/fryzlewicz/tguh/tguh.pdf.

Adityanand Guntuboyina  Donovan Lieu  Sabyasachi Chatterjee  and Bodhisattva Sen. Spatial

adaptation in trend ﬁltering. arXiv preprint arXiv:1702.05113  2017.

Iain M. Johnstone. Gaussian Estimation: Sequence and Wavelet Models. Cambridge University

Press  2015. Draft version.

Seung-Jean Kim  Kwangmoo Koh  Stephen Boyd  and Dimitry Gorinevsky. (cid:96)1 trend ﬁltering. SIAM

Review  51(2):339–360  2009.

Enno Mammen and Sara van de Geer. Locally adaptive regression splines. The Annals of Statistics 

25(1):387–413  1997.

Oscar Hernan Madrid Padilla  James Sharpnack  James Scott    and Ryan J. Tibshirani. The DFS
fused lasso: Linear-time denoising over general graphs. arXiv preprint arXiv:1608.03384  2016.

Leonid Rudin  Stanley Osher  and Emad Faterni. Nonlinear total variation based noise removal

algorithms. Physica D: Nonlinear Phenomena  60(1–4):259–268  1992.

Gabriel Steidl  Stephan Didas  and Julia Neumann. Splines in higher order TV regularization.

International Journal of Computer Vision  70(3):214–255  2006.

Robert Tibshirani and Pei Wang. Spatial smoothing and hot spot detection for cgh data using the

fused lasso. Biostatistics  9(1):18–29  2008.

Robert Tibshirani  Michael Saunders  Saharon Rosset  Ji Zhu  and Keith Knight. Sparsity and
smoothness via the fused Lasso. Journal of the Royal Statistical Society: Series B (Statistical
Methodology)  67(1):91–108  2005.

Ryan J. Tibshirani. Adaptive piecewise polynomial estimation via trend ﬁltering. The Annals of

Statistics  42(1):285–323  2014.

10

,Kevin Lin
James Sharpnack
Alessandro Rinaldo
Ryan Tibshirani
Stephen Mussmann
Percy Liang