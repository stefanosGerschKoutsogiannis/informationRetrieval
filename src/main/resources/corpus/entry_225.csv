2015,A Structural Smoothing Framework For Robust Graph Comparison,In this paper  we propose a general smoothing framework for graph kernels by  taking \textit{structural similarity} into account  and apply it to  derive smoothed variants of popular graph kernels. Our framework is inspired by state-of-the-art smoothing  techniques used in natural language processing (NLP). However  unlike  NLP applications which primarily deal with strings  we show how one  can apply smoothing to a richer class of inter-dependent  sub-structures that naturally arise in graphs. Moreover  we discuss  extensions of the Pitman-Yor process that can be adapted to smooth  structured objects thereby leading to novel graph kernels. Our  kernels are able to tackle the diagonal dominance problem  while  respecting the structural similarity between sub-structures    especially under the presence of edge or label noise.  Experimental evaluation shows that not only our kernels outperform  the unsmoothed variants  but also achieve statistically significant  improvements in classification accuracy over several other graph  kernels that have been recently proposed in literature. Our kernels  are competitive in terms of runtime  and offer a viable option for  practitioners.,A Structural Smoothing Framework For Robust

Graph-Comparison

Pinar Yanardag

Department of Computer Science

Purdue University

West Lafayette  IN  47906  USA

ypinar@purdue.edu

S.V.N. Vishwanathan

Department of Computer Science

University of California

Santa Cruz  CA  95064  USA

vishy@ucsc.edu

Abstract

In this paper  we propose a general smoothing framework for graph kernels by
taking structural similarity into account  and apply it to derive smoothed variants
of popular graph kernels. Our framework is inspired by state-of-the-art smoothing
techniques used in natural language processing (NLP). However  unlike NLP ap-
plications that primarily deal with strings  we show how one can apply smoothing
to a richer class of inter-dependent sub-structures that naturally arise in graphs.
Moreover  we discuss extensions of the Pitman-Yor process that can be adapted
to smooth structured objects  thereby leading to novel graph kernels. Our kernels
are able to tackle the diagonal dominance problem while respecting the structural
similarity between features. Experimental evaluation shows that not only our ker-
nels achieve statistically signiﬁcant improvements over the unsmoothed variants 
but also outperform several other graph kernels in the literature. Our kernels are
competitive in terms of runtime  and offer a viable option for practitioners.

1

Introduction

In many applications we are interested in computing similarities between structured objects such
as graphs. For instance  one might aim to classify chemical compounds by predicting whether a
compound is active in an anti-cancer screen or not. A kernel function which corresponds to a dot
product in a reproducing kernel Hilbert space offers a ﬂexible way to solve this problem [19]. R-
convolution [10] is a framework for computing kernels between discrete objects where the key idea
is to recursively decompose structured objects into sub-structures. Let (cid:104)· ·(cid:105)H denote a dot product in
a reproducing kernel Hilbert space  G represent a graph and φ (G) represent a vector of sub-structure
frequencies. The kernel between two graphs G and G(cid:48) is computed by K (G G(cid:48)) = (cid:104)φ (G)   φ (G(cid:48))(cid:105)H.
Many existing graph kernels can be viewed as instances of R-convolution kernels. For instance  the
graphlet kernel [22] decomposes a graph into graphlets  Weisfeiler-Lehman Subtree kernel (referred
as Weisfeiler-Lehman for the rest of the paper) [23] decomposes a graph into subtrees  and the
shortest-path kernel [1] decomposes a graph into shortest-paths. However  R-convolution based
graph kernels suffer from a few drawbacks. First  the size of the feature space often grows exponen-
tially. As size of the space grows  the probability that two graphs will contain similar sub-structures
becomes very small. Therefore  a graph becomes similar to itself but not to any other graph in the
training data. This is well known as the diagonal dominance problem [11] where the resulting kernel
matrix is close to the identity matrix. Second  lower order sub-structures tend to be more numerous
while a vast majority of the sub-structures occurs rarely. In other words  a few sub-structures dom-
inate the distribution. This exhibits a strong power-law behavior and results in underestimation of
the true distribution. Third  the sub-structures used to deﬁne a graph kernel are often related to each
other. However  an R-convolution kernel only respects exact matchings. This problem is particu-

1

Figure 1: Graphlets of size k ≤ 5.

larly important when noise is present in the training data and considering partial similarity between
sub-structures might alleviate the noise problem.
Our solution: In this paper  we propose to tackle the above problems by using a general framework
to smooth graph kernels that are deﬁned using a frequency vector of decomposed structures. We
use structure information by encoding relationships between lower and higher order sub-structures
in order to derive our method. The remainder of this paper is structured as follows. In Section 2 
we review three families of graph kernels for which our smoothing is applicable. In Section 3  we
review smoothing methods for multinomial distributions. In Section 4  we introduce a framework
for smoothing structured objects. In Section 5  we propose a Bayesian variant of our model that
is extended from the Hierarchical Pitman-Yor process [25]. In Section 6  we discuss related work.
In Section 7  we compare smoothed graph kernels to their unsmoothed variants as well as to other
state-of-the-art graph kernels. We report results on classiﬁcation accuracy on several benchmark
datasets as well as their noisy-variants. Section 8 concludes the paper.

2 Graph kernels

are vectors of normalized counts of graphlets  that is  the i-th component of fG

) denotes the frequency of graphlet Gi occurring as a sub-graph of G (resp. G(cid:48)).

Existing graphs kernels based on R-convolution can be categorized into three major families: graph
kernels based on limited-sized subgraphs [e.g. 22]  graph kernels based on subtree patterns [e.g.
18  21]  and graph kernels based on walks [e.g. 27] or paths [e.g. 1].
Graph kernels based on subgraphs: A graphlet G [17] is non-isomorphic sub-graph of size-k 
(see Figure 1). Given two graphs G and G(cid:48)  the kernel [22] is deﬁned as KGK(G G(cid:48)) =
where fG and fG(cid:48)
(resp. fG(cid:48)
Graph kernels based on subtree patterns: Weisfeiler-Lehman [21] is a popular instance of graph
kernels that decompose a graph into its subtree patterns. It simply iterates over each vertex in a
graph  and compresses the label of the vertex and labels of its neighbors into a multiset label. The
vertex is then relabeled with the compressed label to be used for the next iteration. Algorithm con-
cludes after running for h iterations  and the compressed labels are used for constructing a frequency
vector for each graph. Formally  given G and G(cid:48)  this kernel is deﬁned as KW L(G G(cid:48)) =
where lG contains the frequency of each compressed label occurring in h iterations.
Graph kernels based on walks or paths: Shortest-path graph kernel [1] is a popular instance of
this family. This kernel simply compares the sorted endpoints and the length of shortest-paths that
are common between two graphs. Formally  let PG represent the set of all shortest-paths in graph
G  and pi ∈ PG denote a triplet (ls  le  nk) where nk is the length of the path and ls and le are the
labels of the source and sink vertices  respectively. The kernel between graphs G and G(cid:48) is deﬁned
as KSP (G G(cid:48)) =
where i-th component of pG contains the frequency of i-th triplet
occurring in graph G (resp. pG(cid:48)
).

pG  pG(cid:48)(cid:69)
(cid:68)

(cid:68)

fG  fG(cid:48)(cid:69)

(cid:68)
lG  lG(cid:48)(cid:69)

3 Smoothing multinomial distributions

In this section  we brieﬂy review smoothing techniques for multinomial distributions. Let
e1  e2  . . .   em represent a sequence of n discrete events drawn from a ground set A = {1  2  . . .   V }.

2

denotes the number of times the event a appears in the observed sequence and m =(cid:80)

Figure 2: Topologically sorted graphlet DAG for k ≤ 5 where nodes are colored based on degree.
Suppose  we would like to estimate the probability P (ei = a) for some a ∈ A. It is well known
that the Maximum Likelihood Estimate (MLE) can be computed as PM LE (ei = a) = ca
m where ca
j cj denotes
the total number of observed events. However  MLE of the multinomial distribution is spiky since it
assigns zero probability to the events that did not occur in the observed sequence. In other words  an
event with low probability is often estimated to have zero probability mass. The general idea behind
smoothing is to adjust the MLE of the probabilities by pushing the high probabilities downwards
and pushing low or zero probabilities upwards in order to produce a more accurate distribution on
the events [30]. Interpolated smoothing methods offer a ﬂexible solution between the higher-order
maximum likelihood model and lower-order smoothed model (or so-called  fallback model). The
way the fallback model is designed is the key to deﬁne a new smoothing method1. Absolute dis-
counting [15] and Interpolated Kneser-Ney [12] are two popular instances of interpolated smoothing
methods:

max{ca − d  0}

md × d

PA (ei = a) =

(1)
Here  d > 0 is a discount factor  md := |{a : ca > d}| is the number of events whose counts
are larger than d  while P (cid:48)
A is the fallback distribution. Absolute discounting deﬁnes the fallback
distribution as the smoothed version of the lower-order MLE while Kneser-Ney uses an unusual
estimate of the fallback distribution by using number of different contexts that the event follows in
the lower order model.

m

m

+

P (cid:48)
A (ei = a) .

4 Smoothing structured objects

In this section  we ﬁrst propose a new interpolated smoothing framework that is applicable to a
richer set of objects such as graphs by using a Directed Acyclic Graph (DAG). We then discuss how
to design such DAGs for various graph kernels.

4.1 Structural smoothing

The key to designing a new smoothing method is to deﬁne a fallback distribution  which not only
incorporates domain knowledge but is also easy to estimate recursively. Suppose  we have access
to a weighted DAG where every node at the k-th level represents an event from the ground set A.
Moreover let wij denote the weight of the edge connecting event i to event j  and Pa (resp. Ca)
denote the parents (resp. children) of event a ∈ A in the DAG. We deﬁne our structural smoothing
for events at level k as follows:

(cid:88)

P k

SS (ei = a) =

max{ca − d  0}

md × d

+

m

m

j∈Pa

P k−1
SS (j)

wja(cid:80)

a(cid:48)∈Cj

.

wja(cid:48)

(2)

The way to understand the above equation is as follows: we subtract a ﬁxed discounting factor d
from every observed event which accumulates to a total mass of md × d. Each event a receives
some portion of this accumulated probability mass from its parents. The proportion of the mass that
a parent j at level k − 1 transmits to a given child a depends on the weight wja between the parent
and the child (normalized by the sum of the weights of the edges from j to all its children)  and the
probability mass P k−1
SS (j) that is assigned to node j. In other words  the portion a child event a is
able to obtain from the total discounted mass depends on how authoritative its parents are  and how
strong the relationship between the child and its parents.

1See Table 2 in [3] for summarization of various smoothing algorithms using this general framework.

3

4.2 Designing the DAG

¯G∈CG

DAG  and nG :=(cid:80)

In order to construct a DAG for smoothing structured objects  we ﬁrst construct a vocabulary V
that denotes the set of all unique sub-structures that are going to be smoothed. Each item in the
vocabulary V corresponds to a node in the DAG. V can be generated statically or dynamically
based on the type of sub-structure the graph kernel exploits. For instance  it requires a one-time
O(2k) effort to generate the vocabulary of size ≤ k graphlets for graphlet kernel. However  one
needs to build the vocabulary dynamically in Weisfeiler-Lehman and Shortest-Path kernels since
the sub-structures depend on the node labels obtained from the datasets. After constructing the
vocabulary V   the parent/child relationship between sub-structures needs to be obtained. Given a
sub-structure s of size k  we apply a transformation to ﬁnd all possible sub-structures of size k − 1
that s can be reduced into. Each sub-structure s(cid:48) that is obtained by this transformation is assigned
as a parent of s. After obtaining the parent/child relationship between sub-structures  the DAG is
constructed by drawing a directed edge from each parent to its children nodes. Since all descendants
of a given sub-structure at depth k − 1 are at depth k  this results in a topological ordering of the
vertices  and hence the resulting graph is indeed a DAG. Next  we discuss how to construct such
DAGs for different graph kernels.
Graphlet Kernel: We construct the vocabulary V for graphlet kernel by enumerating all canonical
graphlets of size up to k2. Each canonically-labeled graphlet is a node in the DAG. We then apply
a transformation to infer the parent/child relationship between graphlets as follows: we place a
directed edge from graphlet G to G(cid:48) if  and only if  G can be obtained from G(cid:48) by deleting a node.
In other words  all edges from a graphlet G of size k − 1 point to a graphlet G(cid:48) of size k. In order to
assign weights to the edges  given a graphlet pair G and G(cid:48)  we count the number of times G can be
obtained from G(cid:48) by deleting a node (call this number nGG(cid:48)). Recall that G is of size k − 1 and G(cid:48)
is of size k  and therefore nGG(cid:48) can at most be k. Let CG denote the set of children of node G in the
nG ¯G. Then we deﬁne the weight wGG(cid:48) of the edge connecting G and G(cid:48)
as nGG(cid:48)/nG. The idea here is that the weight encodes the proportion of different ways of extending
G which results in the graphlet G(cid:48). For instance  let us consider G15 and its parents G5  G6  G7 (see
Figure 2 for the DAG of graphlets with size k ≤ 5). Even if graphlet G15 is not observed in the
training data  it still gets a probability mass proportional to the edge weight from its parents in order
to overcome the sparsity problem of unseen data.
Weisfeiler-Lehman Kernel: The Weisfeiler-Lehman kernel performs an exact matching between
the compressed multiset labels. For instance  given two labels ABCDE and ABCDF  it simply as-
signs zero value for their similarity even though two labels have a partial similarity. In order to
smooth Weisfeiler-Lehman kernel  we ﬁrst run the original algorithm and obtain the multiset repre-
sentation of each graph in the dataset. We then apply a transformation to infer the parent/child re-
lationship between compressed labels as follows: in each iteration of Weisfeiler-Lehman algorithm 
and for each multiset label of size k in the vocabulary  we generate its power set by computing all
subsets of size k − 1 while keeping the root node ﬁxed. For instance  the parents of a multiset label
ABCDE are {ABCD  ABCE  ABDE  ACDE}. Then  we simply construct the DAG by drawing a
directed edge from parent labels to children. Notice that considering only the set of labels gener-
ated from the Weisfeiler-Lehman kernel is not sufﬁcient enough for constructing a valid DAG. For
instance  it might be the case that none of the possible parents of a given label exists in the vocab-
ulary simply due to the sparsity problem (e.g.out of all possible parents of ABCDE  we might only
observe ABCE in the training data). Thus  restricting ourselves to the original vocabulary leaves
such labels orphaned in the DAG. Therefore  we consider so-called pseudo parents as a part of the
vocabulary when constructing the DAG. Since the sub-structures in this kernel are data-dependent 
we use a uniform weight between a parent and its children.
Shortest-Path Kernel: Similar to other graph kernels discussed above  shortest-path graph kernel
does not take partial similarities into account. For instance  given two shortest-paths ABCDE and
ABCDF (compressed as AE5 and AF5  respectively)  it assigns zero for their similarity since their
sink labels are different. However  one can notice that shortest-path sub-structures exhibit a strong
dependency relationship. For instance  given a shortest-path pij = {ABCDE} of size k  one can
derive the shortest-paths {ABCD  ABC  AB} of size < k as a result of the optimal sub-structure
property  that is  one can show that all sub-paths of a shortest-path are also shortest-paths with

2We used Nauty [13] to obtain canonically-labeled isomorphic representations of graphlets.

4

Figure 3: An illustration of table assignment  adapted from [9]. In this example  labels at the tables
are given by (l1  . . .   l4) = (G44  G30  G32  G44). Black dots indicate the number of occurrences of
each label in 10 draws from the Pitman-Yor process.

the same source node [6]. In order to smooth shortest-path kernel  we ﬁrst build the vocabulary by
computing all shortest-paths for each graph. Let pij be a shortest-path of size k and pij(cid:48) be a shortest-
path of size k − 1 that is obtained by removing the sink node of pij. Let lij be the compressed form
of pij that represents the sorted labels of its endpoints i and j concatenated to its length (resp. lij(cid:48)).
Then  in order to build the DAG  we draw a directed edge from lij(cid:48) of depth k − 1 to lij of depth k if
and only if pij(cid:48) is a sub-path of pij. In other words  all ascendants of lij consist of the compressed
labels obtained from sub-paths of pij of size < k. Similar to Weisfeiler-Lehman kernel  we assign a
uniform weight between parents and children.

5 Pitman-Yor Smoothing

Pitman-Yor processes are known to produce power-law distributions [8]. A novel interpretation of
interpolated Kneser-Ney is proposed by [25] as approximate inference in a hierarchical Bayesian
model consisting of Pitman-Yor process [16]. By following a similar spirit  we extend our model
to adapt Pitman-Yor process as an alternate smoothing framework. A Pitman-Yor process P on a
ground set Gk+1 of size-(k + 1) graphlets is deﬁned via Pk+1 ∼ P Y (dk+1  θk+1  Pk) where dk+1
is a discount parameter  0 ≤ dk+1 < 1  θ > −dk+1 is a strength parameter  and Pk is a base
distribution. The most intuitive way to understand draws from the Pitman-Yor process is via the
Chinese restaurant process (see Figure 3). Consider a restaurant with an inﬁnite number of tables

Algorithm 1 Insert a Customer
Input: dk+1  θk+1  Pk

t ← 0 // Occupied tables
c ← () // Counts of customers
l ← () // Labels of tables
if t = 0 then

else

t ← 1
append 1 to c
draw graphlet Gi ∼ Pk // Insert customer in parent
draw Gj ∼ wij
append Gj to l
return Gj
with probability ∝ max(0  cj − d)
cj ← cj + 1
return lj
with probability proportional to θ + dt
t ← t + 1
append 1 to c
draw graphlet Gi ∼ Pk // Insert customer in parent
draw Gj ∼ wij
append Gj to l
return Gj

end if

where customers enter the restaurant one by one. The ﬁrst customer sits at the ﬁrst table  and a
graphlet is assigned to it by drawing a sample from the base distribution since this table is occupied
for the ﬁrst time. The label of the ﬁrst table is the ﬁrst graphlet drawn from the Pitman-Yor process.

5

Subsequent customers when they enter the restaurant decide to sit at an already occupied table with
probability proportional to ci − dk+1  where ci represents the number of customers already sitting at
table i. If they sit at an already occupied table  then the label of that table denotes the next graphlet
drawn from the Pitman-Yor process. On the other hand  with probability θk+1 + dk+1t  where t is
the current number of occupied tables  a new customer might decide to occupy a new table. In this
case  the base distribution is invoked to label this table with a graphlet. Intuitively the reason this
process generates power-law behavior is because popular graphlets which are served on tables with
a large number of customers have a higher probability of attracting new customers and hence being
generated again  similar to a rich gets richer phenomenon. In a hierarchical Pitman-Yor process  the
base distribution Pk is recursively deﬁned via a Pitman-Yor process Pk ∼ P Y (dk  θk  Pk−1). In
order to label a table  we need a draw from Pk  which is obtained by inserting a customer into the
corresponding restaurant. However  adopting the traditional hierarchical Pitman-Yor process is not
straightforward in our case since the size of the context differs between levels of hierarchy  that is  a
child restaurant in the hierarchy can have more than one parent restaurant to request a label from. In
other words  Pk+1 is deﬁned over Gk+1 of size nk+1 while Pk is deﬁned over Gk of size nk ≤ nk+1.
Therefore  one needs a transformation function to transform base distributions of different sizes. We
incorporate edge weights between parent and child restaurants by using the same weighting scheme
in Section 4.2. This changes the Chinese Restaurant process as follows: When we need to label a
table  we will ﬁrst draw a size-k graphlet Gi ∼ Pk by inserting a customer into the corresponding
restaurant. Given Gi  we will draw a size-(k + 1) graphlet Gj proportional to wij  where wij is
obtained from the DAG. See Algorithm 1 for pseudo code of inserting a customer. Deletion of a
customer is handled similarly (see Algorithm 2).

Algorithm 2 Delete a Customer
Input: d  θ  P0  C  L  t
with probability ∝ cl
cl ← cl − 1
Gj ← lj
if cl = 0 then
Pk ∝ 1/wij
delete cl from c
delete lj from l
t ← t − 1

end if
return G

6 Related work

A survey of most popular graph kernel methods is already given in previous sections. Several meth-
ods proposed in smoothing structured objects [4]  [20]. Our framework is similar to dependency
tree kernels [4] since both methods are using the notion of smoothing for structured objects. How-
ever  our method is interested in the problem of smoothing the count of structured objects. Thus 
while smoothing is achieved by using a DAG  we discard the DAG once the counts are smoothed.
Another related work to ours is propagation kernels [14] that deﬁne graph features as counts of sim-
ilar node-label distributions on the respective graphs by using Locality Sensitive Hashing (LSH).
Our framework not only considers node label distributions  but also explicitly incorporates struc-
tural similarity via the DAG. Another similar work to ours is recently proposed framework by [29]
which learns the co-occurrence relationship between sub-structures by using neural language mod-
els. However  their framework do not respect the structural similarity between sub-structures  which
is an important property to consider especially in the presence of noise in edges or labels.

7 Experiments

The aim of our experiments is threefold. First  we want to show that smoothing graph kernels
signiﬁcantly improves the classiﬁcation accuracy. Second  we want to show that the smoothed
kernels are comparable to or outperform state-of-the-art graph kernels in terms of classiﬁcation

6

Table 1: Comparison of classiﬁcation accuracy (± standard deviation) of shortest-path (SP) 
Weisfeiler-Lehman (WL)  graphlet (GK) kernels with their smoothed variants. Smoothed variants
with statistically signiﬁcant improvements over the base kernels are shown in bold as measured by
a t-test with a p value of ≤ 0.05. Ramon & G¨artner (Ram & G¨ar)  p-random walk and random
walk kernels are included for additional comparison where > 72H indicates the computation did not
ﬁnish in 72 hours. Runtime for constructing the DAG and smoothing (SMTH) the counts are also
reported where ” indicates seconds and ’ indicates minutes.

DATASET
SP
SMOOTHED SP
WL
SMOOTHED WL
GK
SMOOTHED GK
PYP GK
RAM & G ¨AR
P-RANDOMWALK
RANDOM WALK
DAG/SMTH (GK)
DAG/SMTH (SP)
DAG/SMTH (WL)
DAG/SMTH (PYP)

MUTAG
85.22 ±2.43
87.94 ±2.58
82.22 ±1.87
87.44 ±1.95
81.33 ±1.02
83.17 ±0.64
83.11 ±1.23
84.88 ±1.86
80.05 ±1.64
83.72 ±1.50
1”
6”
1”
3”
2”
1”
6”
5”

PTC
58.24 ±2.44
60.82 ±1.84
60.41 ±1.93
60.47 ±2.39
55.56 ±1.46
58.44 ±1.00
57.44 ±1.44
58.47 ±0.90
59.38 ±1.66
57.85 ±1.30
6”
1”
19” 1”
17”
1”
6”
12”

ENZYMES
40.10 ±1.50
42.27 ±1.07
53.88 ±0.95
55.30 ±0.65
27.32 ±0.96
30.90 ±1.51
29.63 ±1.30
16.96 ±1.46
30.01 ±1.00
24.16 ±1.64
6”
1”
45” 1”
10” 12’
6”
21”

NCI1
73.00±0.24
73.26±0.24
84.13±0.22
84.66±0.18
62.46±0.19
62.48±0.15
62.50±0.20
56.61±0.53

PROTEINS
75.07 ±0.54
75.85 ±0.28
74.49 ±0.49
75.53 ±0.50
69.69 ±0.46
69.83 ±0.46
70.00 ±0.80
70.73 ±0.35
71.16 ±0.35 > 72H
74.22 ±0.42 > 72H
6”
9’
7’
6”

1”
1”
70’
1’

6”
9’
2”
6”

3”
17”
21’
8’

NCI109
73.00±0.21
73.01±0.31
83.83±0.31
84.72±0.21
62.33±0.14
62.48±0.11
62.68±0.18
54.62±0.23
> 72H
> 72H
6”
3”
10’ 16”
21’
2”
6”
8’

accuracy  while remaining competitive in terms of computational requirements. Third  we want to
show that our methods outperform base kernels when edge or label noise is presence.
Datasets We used the following benchmark datasets used in graph kernels: MUTAG  PTC  EN-
ZYMES  PROTEINS  NCI1 and NCI109. MUTAG is a dataset of 188 mutagenic aromatic and
heteroaromatic nitro compounds [5] with 7 discrete labels. PTC [26] is a dataset of 344 chemical
compounds has 19 discrete labels. ENZYMES is a dataset of 600 protein tertiary structures obtained
from [2]  and has 3 discrete labels. PROTEINS is a dataset of 1113 graphs obtained from [2] having
3 discrete labels. NCI1 and NCI109 [28] are two balanced datasets of chemical compounds having
size 4110 and 4127 with 37 and 38 labels  respectively.
Experimental setup We compare our framework against representative instances of major families
of graph kernels in the literature. In addition to the base kernels  we also compare our smoothed
kernels with the random walk kernel [7]  the Ramon-G¨artner subtree [18]  and p-step random walk
kernel [24]. The Random Walk  p-step Random Walk and Ramon-G¨artner are written in Matlab and
obtained from [22]. All other kernels were coded in Python except Pitman-Yor smoothing which is
coded in C++3. We used a parallel implementation for smoothing the counts of Weisfeiler-Lehman
kernel for efﬁciency. All kernels are normalized to have a unit length in the feature space. Moreover 
we use 10-fold cross validation with a binary C-Support Vector Machine (SVM) where the C value
for each fold is independently tuned using training data from that fold. In order to exclude random
effects of the fold assignments  this experiment is repeated 10 times and average prediction accuracy
of 10 experiments with their standard deviations are reported4.

7.1 Results

In our ﬁrst experiment  we compare the base kernels with their smoothed variants. As can be seen
from Table 1  smoothing improves the classiﬁcation accuracy of every base kernel on every dataset
with majority of the improvements being statistically signiﬁcant with p ≤ 0.05. We observe that
even though smoothing improves the accuracy of graphlet kernels on PROTEINS and NCI1  the im-
provements are not statistically signiﬁcant. We believe this is due to the fact that these datasets are
not sensitive to structural noise as much as the other datasets  thus considering the partial similarities

3We modiﬁed the open source implementation of PYP: https://github.com/redpony/cpyp.
4Implementations of original and smoothed versions of the kernels  datasets and detailed discussion of
parameter selection procedure with the list of parameters used in our experiments can be accessed from http:
//web.ics.purdue.edu/˜ypinar/nips.

7

Figure 4: Classiﬁcation accuracy vs. noise for base graph kernels (dashed lines) and their smoothed
variants (non-dashed lines).

do not improve the results signiﬁcantly. Moreover  PYP smoothed graphlet kernels achieve statisti-
cally signiﬁcant improvements in most of the datasets  however they are outperformed by smoothed
graphlet kernels introduced in Section 3.
In our second experiment  we picked the best smoothed kernel in terms of classiﬁcation accuracy for
each dataset  and compared it against the performance of state-of-the-art graph kernels (see Table
1). Smoothed kernels outperform other methods on all datasets  and the results are statistically
signiﬁcant on every dataset except PTC.
In our third experiment  we investigated the runtime behavior of our framework with two major
costs. First  one has to compute a DAG by using the original feature vectors. Next  the constructed
DAG need to be used to compute smoothed representations of the feature vectors. Table 1 shows
the total wallclock runtime taken by all graphs for constructing the DAG  and smoothing the counts
for each dataset. As can be seen from the runtimes  our framework adds a constant factor to the
original runtime for most of the datasets. While the DAG creation in Weisfeiler-Lehman kernel also
adds a negligible overhead  the cost of smoothing becomes signiﬁcant if the vocabulary size gets
prohibitively large due to the exponential growing nature of the kernel w.r.t. to subtree parameter h.
Finally  in our fourth experiment  we test the performance of graph kernels when edge or label
noise is present. For edge noise  we randomly removed and added {10%  20%  30%} of the edges
in each graph. For label noise  we randomly ﬂipped {25%  50%  75%} of the node labels in each
graph where random labels are selected proportionally to the original label-distribution of the graph.
Figure 4 shows the performance of smoothed graph kernels under noise. As can be seen from
the ﬁgure  smoothed kernels are able to outperform their base variants when noise is present. An
interesting observation is that even though a signiﬁcant amount of edge noise is added to PROTEINS
and NCI datasets  the performance of base kernels do not change drastically. This further supports
our observation that these datasets are not sensitive to structural noise as much as the other datasets.

8 Conclusion and Future Work

We presented a novel framework for smoothing graph kernels inspired by smoothing techniques
from natural language processing and applied our method to state-of-the-art graph kernels. Our
framework is rather general  and lends itself to many extensions. For instance  by deﬁning domain-
speciﬁc parent-child relationships  one can construct different DAGs with different weighting
schemes. Another interesting extension of our smoothing framework would be to apply it to graphs
with continuous labels. Moreover  even though we restricted ourselves to graph kernels in this pa-
per  our framework is applicable to any R-convolution kernel that uses a frequency-vector based
representation  such as string kernels.

9 Acknowledgments

We thank to Hyokun Yun for his tremendous help in implementing Pitman-Yor Processes. We also
thank to anonymous NIPS reviewers for their constructive comments  and Jiasen Yang  Joon Hee
Choi  Amani Abu Jabal and Parameswaran Raman for reviewing early drafts of the paper. This work
is supported by the National Science Foundation under grant No. #1219015.

8

References
[1] K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In ICML  pages 74–81  2005.
[2] K. M. Borgwardt  C. S. Ong  S. Sch¨onauer  S. V. N. Vishwanathan  A. J. Smola  and H.-P. Kriegel. Protein

function prediction via graph kernels. In ISMB  Detroit  USA  2005.

[3] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In ACL 

pages 310–318  1996.

[4] D. Croce  A. Moschitti  and R. Basili. Structured lexical similarity via convolution kernels on dependency

trees. In Proceedings of EMNLP  pages 1034–1046. Association for Computational Linguistics  2011.

[5] A. K. Debnath  R. L. Lopez de Compadre  G. Debnath  A. J. Shusterman  and C. Hansch. Structure-
activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molec-
ular orbital energies and hydrophobicity. J. Med. Chem  34:786–797  1991.

[6] A. Feragen  N. Kasenburg  J. Petersen  M. de Bruijne  and K. Borgwardt. Scalable kernels for graphs with

continuous attributes. In NIPS  pages 216–224  2013.

[7] T. G¨artner  P. Flach  and S. Wrobel. On graph kernels: Hardness results and efﬁcient alternatives. In

COLT  pages 129–143  2003.

[8] S. Goldwater  T. Grifﬁths  and M. Johnson. Interpolating between types and tokens by estimating power-

law generators. NIPS  2006.

[9] S. Goldwater  T. L. Grifﬁths  and M. Johnson. Producing power-law distributions and damping word

frequencies with two-stage language models. JMLR  12:2335–2382  2011.

[10] D. Haussler. Convolution kernels on discrete structures. Technical Report UCS-CRL-99-10  UC Santa

Cruz  1999.

[11] J. Kandola  T. Graepel  and J. Shawe-Taylor. Reducing kernel matrix diagonal dominance using semi-
deﬁnite programming. In COLT  volume 2777 of Lecture Notes in Computer Science  pages 288–302 
Washington  DC  2003.

[12] R. Kneser and H. Ney. Improved backing-off for M-gram language modeling. In ICASSP  1995.
[13] B. D. McKay. Nauty user’s guide (version 2.4). Australian National University  2007.
[14] M. Neumann  R. Garnett  P. Moreno  N. Patricia  and K. Kersting. Propagation kernels for partially

labeled graphs. In ICML–2012 Workshop on Mining and Learning with Graphs  Edinburgh  UK  2012.

[15] H. Ney  U. Essen  and R. Kneser. On structuring probabilistic dependences in stochastic language mod-

eling. In Computer Speech and Language  pages 1–38  1994.

[16] J. Pitman and M. Yor. The two-parameter poisson-dirichlet distribution derived from a stable subordinator.

Annals of Probability  25(2):855–900  1997.

[17] N. Przulj. Biological network comparison using graphlet degree distribution. In ECCB  2006.
[18] J. Ramon and T. G¨artner. Expressivity versus efﬁciency of graph kernels. Technical report  First Interna-

tional Workshop on Mining Graphs  Trees and Sequences (held with ECML/PKDD’03)  2003.

[19] B. Sch¨olkopf and A. J. Smola. Learning with Kernels. 2002.
[20] A. Severyn and A. Moschitti. Fast support vector machines for convolution tree kernels. Data Mining

and Knowledge Discovery  25(2):325–357  2012.

[21] N. Shervashidze and K. Borgwardt. Fast subtree kernels on graphs. In NIPS  2010.
[22] N. Shervashidze  S. V. N. Vishwanathan  T. Petri  K. Mehlhorn  and K. Borgwardt. Efﬁcient graphlet

kernels for large graph comparison. In AISTATS  2009.

[23] N. Shervashidze  P. Schweitzer  E. J. Van Leeuwen  K. Mehlhorn  and K. M. Borgwardt. Weisfeiler-

lehman graph kernels. JMLR  12:2539–2561  2011.

[24] A. J. Smola and R. Kondor. Kernels and regularization on graphs. In COLT  pages 144–158  2003.
[25] Y. W. Teh. A hierarchical bayesian language model based on pitman-yor processes. In ACL  2006.
[26] H. Toivonen  A. Srinivasan  R. D. King  S. Kramer  and C. Helma. Statistical evaluation of the predictive

toxicology challenge 2000-2001. Bioinformatics  19(10):1183–1193  July 2003.

[27] S. V. N. Vishwanathan  N. N. Schraudolph  I. R. Kondor  and K. M. Borgwardt. Graph kernels. JMLR 

2010.

[28] N. Wale  I. A. Watson  and G. Karypis. Comparison of descriptor spaces for chemical compound retrieval

and classiﬁcation. Knowledge and Information Systems  14(3):347–375  2008.

[29] P. Yanardag and S. Vishwanathan. Deep graph kernels. In KDD  pages 1365–1374. ACM  2015.
[30] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information

retrieval. ACM Trans. Inf. Syst.  22(2):179–214  2004.

9

,Yifei Ma
Roman Garnett
Jeff Schneider
Abhishek Sharma
Oncel Tuzel
Ming-Yu Liu
Pinar Yanardag
S.V.N. Vishwanathan