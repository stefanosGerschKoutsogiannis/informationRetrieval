2016,Fast Algorithms for Robust PCA via Gradient Descent,We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions  this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied  and conditions on when recovery is possible (how many observations do we need  how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems  compared to the best available algorithms. In particular  in the fully observed case  with $r$ denoting rank and $d$ dimension  we reduce the complexity from $O(r^2d^2\log(1/\epsilon))$ to $O(rd^2\log(1/\epsilon))$ -- a big savings when the rank is big. For the partially observed case  we show the complexity of our algorithm is no more than $O(r^4d\log(d)\log(1/\epsilon))$. Not only is this the best-known run-time for a provable algorithm under partial observation  but in the setting where $r$ is small compared to $d$  it also allows for near-linear-in-$d$ run-time that can be exploited in the fully-observed case as well  by simply running our algorithm on a subset of the observations.,Fast Algorithms for Robust PCA via Gradient

Descent

Xinyang Yi∗ Dohyung Park∗ Yudong Chen† Constantine Caramanis∗
†Cornell University
∗The University of Texas at Austin
∗{yixy dhpark constantine}@utexas.edu
†yudong.chen@cornell.edu

Abstract

We consider the problem of Robust PCA in the fully and partially observed set-
tings. Without corruptions  this is the well-known matrix completion problem.
From a statistical standpoint this problem has been recently well-studied  and
conditions on when recovery is possible (how many observations do we need 
how many corruptions can we tolerate) via polynomial-time algorithms is by
now understood. This paper presents and analyzes a non-convex optimization
approach that greatly reduces the computational complexity of the above prob-
lems  compared to the best available algorithms. In particular  in the fully ob-
served case  with r denoting rank and d dimension  we reduce the complexity
from O(r2d2 log(1/ε)) to O(rd2 log(1/ε)) – a big savings when the rank is big.
For the partially observed case  we show the complexity of our algorithm is no
more than O(r4d log d log(1/ε)). Not only is this the best-known run-time for a
provable algorithm under partial observation  but in the setting where r is small
compared to d  it also allows for near-linear-in-d run-time that can be exploited in
the fully-observed case as well  by simply running our algorithm on a subset of the
observations.
Introduction

1
Principal component analysis (PCA) aims to ﬁnd a low rank subspace that best-approximates a data
matrix Y ∈ Rd1×d2. The simple and standard method of PCA by singular value decomposition
(SVD) fails in many modern data problems due to missing and corrupted entries  as well as sheer scale
of the problem. Indeed  SVD is highly sensitive to outliers by virtue of the squared-error criterion
it minimizes. Moreover  its running time scales as O(rd2) to recover a rank r approximation of a
d-by-d matrix.
While there have been recent results developing provably robust algorithms for PCA (e.g.  [5  26])  the
running times range from O(r2d2) to O(d3) and hence are signiﬁcantly worse than SVD. Meanwhile 
the literature developing sub-quadratic algorithms for PCA (e.g.  [15  14  3]) seems unable to
guarantee robustness to outliers or missing data.
Our contribution lies precisely in this area: provably robust algorithms for PCA with improved
run-time. Speciﬁcally  we provide an efﬁcient algorithm with running time that matches SVD while
nearly matching the best-known robustness guarantees. In the case where rank is small compared to
dimension  we develop an algorithm with running time that is nearly linear in the dimension. This
last algorithm works by subsampling the data  and therefore we also show that our algorithm solves
the Robust PCA problem with partial observations (a generalization of matrix completion and Robust
PCA).
1.1 The Model and Related Work
We consider the following setting for robust PCA. Suppose we are given a matrix Y ∈ Rd1×d2 that
has decomposition Y = M∗ + S∗  where M∗ is a rank r matrix and S∗ is a sparse corruption matrix
containing entries with arbitrary magnitude. The goal is to recover M∗ and S∗ from Y . To ease
notation  we let d1 = d2 = d in the remainder of this section.

30th Conference on Neural Information Processing Systems (NIPS 2016)  Barcelona  Spain.

min
M S

√

Provable solutions for this model are ﬁrst provided in the works of [9] and [5]. They propose to solve
this problem by convex relaxation:

|||M|||nuc + λ(cid:107)S(cid:107)1  s.t. Y = M + S 

(1)
where |||M|||nuc denotes the nuclear norm of M. Despite analyzing the same method  the corruption
models in [5] and [9] differ. In [5]  the authors consider the setting where the entries of M∗ are
corrupted at random with probability α. They show their method succeeds in exact recovery with
α as large as 0.1  which indicates they can tolerate a constant fraction of corruptions. Work in [9]
considers a deterministic corruption model  where nonzero entries of S∗ can have arbitrary position 
but the sparsity of each row and column does not exceed αd. They prove that for exact recovery  it
can allow α = O(1/(µr
d)). This was subsequently further improved to α = O(1/(µr))  which is
in fact optimal [11  18]. Here  µ represents the incoherence of M∗ (see Section 2 for details). In this
paper  we follow this latter line and focus on the deterministic corruption model.
The state-of-the-art solver [20] for (1) has time complexity O(d3/ε) to achieve error ε  and is thus
much slower than SVD  and prohibitive for even modest values of d. Work in [21] considers the
deterministic corruption model  and improves this running time without sacriﬁcing the robustness
guarantee on α. They propose an alternating projection (AltProj) method to estimate the low
rank and sparse structures iteratively and simultaneously  and show their algorithm has complexity
O(r2d2 log(1/ε))  which is faster than the convex approach but still slower than SVD.
Non-convex approaches have recently seen numerous developments for applications in low-rank
estimation  including alternating minimization (see e.g. [19  17  16]) and gradient descent (see e.g.
[4  12  23  24  29  30]). These works have fast running times  yet do not provide robustness guarantees.
One exception is [12]  where the authors analyze a row-wise (cid:96)1 projection method for recovering
S∗. Their analysis hinges on positive semideﬁnite M∗  and the algorithm requires prior knowledge
of the (cid:96)1 norm of every row of S∗ and is thus prohibitive in practice. Another exception is work
[16]  which analyzes alternating minimization plus an overall sparse projection. Their algorithm is
shown to tolerate at most a fraction of α = O(1/(µ2/3r2/3d)) corruptions. As we discuss in Section
1.2  we can allow S∗ to have much higher sparsity α = O(1/(µr1.5))  which is close to optimal.
It is worth mentioning other works that obtain provable guarantees of non-convex algorithms or
problems including phase retrieval [6  13  28]  EM algorithms [2  25  27]  tensor decompositions [1]
and second order method [22]. It might be interesting to bring robust considerations to these works.
1.2 Our Contributions
In this paper  we develop efﬁcient non-convex algorithms for robust PCA. We propose a novel
algorithm based on the projected gradient method on the factorized space. We also extend it to solve
robust PCA in the setting with partial observations  i.e.  in addition to gross corruptions  the data
matrix has a large number of missing values. Our main contributions are summarized as follows.1
1. We propose a novel sparse estimator for the setting of deterministic corruptions. For the low-rank
structure to be identiﬁable  it is natural to assume that deterministic corruptions are “spread out” (no
more than some number in each row/column). We leverage this information in a simple but critical
algorithmic idea  that is tied to the ultimate complexity advantages our algorithm delivers.
2. Based on the proposed sparse estimator  we propose a projected gradient method on the matrix
factorized space. While non-convex  the algorithm is shown to enjoy linear convergence under proper
initialization. Along with a new initialization method  we show that robust PCA can be solved
within complexity O(rd2 log(1/ε)) while ensuring robustness α = O(1/(µr1.5)). Our algorithm is
thus faster than the best previous known algorithm by a factor of r  and enjoys superior empirical
performance as well.
3. Algorithms for Robust PCA with partial observations still rely on a computationally expensive
convex approach  as apparently this problem has evaded treatment by non-convex methods. We
consider precisely this problem. In a nutshell  we show that our gradient method succeeds (it is
guaranteed to produce the subspace of M∗) even when run on no more than O(µ2r2d log d) random
entries of Y . The computational cost is O(µ3r4d log d log(1/ε)). When rank r is small compared to
the dimension d  in fact this dramatically improves on our bound above  as our cost becomes nearly
linear in d. We show  moreover  that this savings and robustness to erasures comes at no cost in the
1To ease presentation  the discussion here assumes M∗ has constant condition number  whereas our results

below show the dependence on condition number explicitly.

2

robustness guarantee for the deterministic (gross) corruptions. While this demonstrates our algorithm
is robust to both outliers and erasures  it also provides a way to reduce computational costs even in
the fully observed setting  when r is small.
4. An immediate corollary of the above result provides a guarantee for exact matrix completion  with
general rectangular matrices  using O(µ2r2d log d) observed entries and O(µ3r4d log d log(1/ε))
time  thereby improving on existing results in [12  23].

Notation. For any index set Ω ⊆ [d1] × [d2]  we let Ω(i ·) :=(cid:8)(i  j) ∈ Ω(cid:12)(cid:12) j ∈ [d2](cid:9)  Ω(· j) :=
(cid:8)(i  j) ∈ Ω(cid:12)(cid:12) i ∈ [d1](cid:9). For any matrix A ∈ Rd1×d2  we denote its projector onto support Ω by

ΠΩ (A)  i.e.  the (i  j)-th entry of ΠΩ (A) is equal to A if (i  j) ∈ Ω and zero otherwise. The i-th
row and j-th column of A are denoted by A(i ·) and A(· j). The (i  j)-th entry is denoted as A(i j).
Operator norm of A is |||A|||op. Frobenius norm of A is |||A|||F. The (cid:96)a/(cid:96)b norm of A is denoted by
|||A|||b a  i.e.  the (cid:96)a norm of the vector formed by the (cid:96)b norm of every row. For instance  (cid:107)A(cid:107)2 ∞
stands for maxi∈[d1] (cid:107)A(i ·)(cid:107)2.
2 Problem Setup
We consider the problem where we observe a matrix Y ∈ Rd1×d2 that satisﬁes Y = M∗ + S∗  where
M∗ has rank r  and S∗ is corruption matrix with sparse support. Our goal is to recover M∗ and S∗.
In the partially observed setting  in addition to sparse corruptions  we have erasures. We assume that
each entry of M∗ + S∗ is revealed independently with probability p ∈ (0  1). In particular  for any
(i  j) ∈ [d1] × [d2]  we consider the Bernoulli model where

(cid:26)(M∗ + S∗)(i j)  with probability p;

∗ 

Y(i j) =

(2)
We denote the support of Y by Φ = {(i  j) | Y(i j) (cid:54)= ∗}. Note that we assume S∗ is not adaptive to
Φ. As is well-understood thanks to work in matrix completion  this task is impossible in general –
we need to guarantee that M∗ is not both low-rank and sparse. To avoid such identiﬁability issues 
we make the following standard assumptions on M∗ and S∗: (i) M∗ is not near-sparse or “spiky.”
We impose this by requiring M∗ to be µ-incoherent – given a singular value decomposition (SVD)
M∗ = L∗Σ∗R∗(cid:62)  we assume that

otherwise.

(cid:114) µr

d1

(cid:114) µr

.

d2

(cid:40)

Tα [A] :=

(cid:107)L∗(cid:107)2 ∞ ≤

  (cid:107)R∗(cid:107)2 ∞ ≤

(ii) The entries of S∗ are “spread out” – for α ∈ [0  1)  we assume S∗ ∈ Sα  where

Sα :=(cid:8)A ∈ Rd1×d2(cid:12)(cid:12) (cid:107)A(i ·)(cid:107)0 ≤ αd2 for all i ∈ [d1] ;(cid:107)A(· j)(cid:107)0 ≤ αd1 for all j ∈ [d2](cid:9) .

(3)

In other words  S∗ contains at most α-fraction nonzero entries per row and column.
3 Algorithms
For both the full and partial observation settings  our method proceeds in two phases. In the ﬁrst
phase  we use a new sorting-based sparse estimator to produce a rough estimate Sinit for S∗ based on
the observed matrix Y   and then ﬁnd a rank r matrix factorized as U0V (cid:62)
0 that is a rough estimate
of M∗ by performing SVD on (Y − Sinit). In the second phase  given (U0  V0)  we perform an
iterative method to produce series {(Ut  Vt)}∞
t=0. In each step t  we ﬁrst apply our sparse estimator
to produce a sparse matrix St based on (Ut  Vt)  and then perform a projected gradient descent
step on the low-rank factorized space to produce (Ut+1  Vt+1). This ﬂow is the same for full and
partial observations  though a few details differ. Algorithm 1 gives the full observation algorithm 
and Algorithm 2 gives the partial observation algorithm. We now describe the key details of each
algorithm.
Sparse Estimation. A natural idea is to keep those entries of residual matrix Y − M that have large
magnitude. At the same time  we need to make use of the dispersed property of Sα that every column
and row contain at most α-fraction of nonzero entries. Motivated by these two principles  we introduce
the following sparsiﬁcation operator: For any matrix A ∈ Rd1×d2: for all (i  j) ∈ [d1] × [d2]  we let

A(i j) 
0 

if |A(i j)| ≥ |A(αd2)
otherwise

(i ·) | and |A(i j)| ≥ |A(αd1)
(· j) |

 

(4)

3

(i ·) and A(k)

where A(k)
(· j) denote the elements of A(i ·) and A(· j) that have the k-th largest magnitude
respectively. In other words  we choose to keep those elements that are simultaneously among the
largest α-fraction entries in the corresponding row and column. In the case of entries having identical
magnitude  we break ties arbitrarily. It is thus guaranteed that Tα [A] ∈ Sα.
Algorithm 1 Fast RPCA
INPUT: Observed matrix Y with rank r and corruption fraction α; parameters γ  η; number of

// see (4) for the deﬁnition of Tα [·].

iterations T .
// Phase I: Initialization.
1: Sinit ← Tα [Y ]
2: [L  Σ  R] ← SVDr[Y − Sinit] 2
3: U0 ← LΣ1/2  V0 ← RΣ1/2. Let U V be deﬁned according to (7).
// Phase II: Gradient based iterations.
4: U0 ← ΠU (U0)  V0 ← ΠV (V0)
5: for t = 0  1  . . .   T − 1 do
6:
7:
8:
9: end for
OUTPUT: (UT   VT )

Ut+1 ← ΠU(cid:0)Ut − η∇UL(Ut  Vt; St) − 1
Vt+1 ← ΠV(cid:0)Vt − η∇V L(Ut  Vt; St) − 1

(cid:2)Y − UtV (cid:62)

t Ut − V (cid:62)
t Vt − U(cid:62)

2 ηUt(U(cid:62)
2 ηVt(V (cid:62)

St ← Tγα

(cid:3)

t Vt)(cid:1)
t Ut)(cid:1)

t

In the fully observed setting  we compute Sinit based on Y as Sinit = Tα [Y ]. In
Initialization.
the partially observed setting with sampling rate p  we let Sinit = T2pα [Y ]. In both cases  we then
set U0 = LΣ1/2 and V0 = RΣ1/2  where LΣR(cid:62) is an SVD of the best rank r approximation of
Y − Sinit.
Gradient Method on Factorized Space. After initialization  we proceed by projected gradient
descent. To do this  we deﬁne loss functions explicitly in the factored space  i.e.  in terms of U  V and
S:

L(U  V ; S)

(cid:101)L(U  V ; S)

:=

:=

|||U V (cid:62) + S − Y |||2
1
F  
2
|||ΠΦ
1
2p

(cid:0)U V (cid:62) + S − Y(cid:1)|||2

F .

(fully observed)

(partially observed)

(5)

(6)

(cid:27)

.
(7)

Recall that our goal is to recover M∗ that satisﬁes the µ-incoherent condition. Given an SVD
M∗ = L∗ΣR∗(cid:62)  we expect that the solution (U  V ) is close to (L∗Σ1/2  R∗Σ1/2) up to some
rotation. In order to serve such µ-incoherent structure  it is natural to put constraints on the row
norms of U  V based on |||M∗|||op. As |||M∗|||op is unavailable  given U0  V0 computed in the ﬁrst phase 
we rely on the sets U  V deﬁned as

(cid:26)

A ∈ Rd1×r(cid:12)(cid:12) (cid:107)A(cid:107)2 ∞ ≤

U :=

(cid:114) 2µr

d1

(cid:27)

|||U0|||op

  V :=

(cid:26)

A ∈ Rd2×r(cid:12)(cid:12) (cid:107)A(cid:107)2 ∞ ≤

(cid:114) 2µr

d2

|||V0|||op

Now we consider the following optimization problems with constraints:

min

U∈U  V ∈V S∈Sα

min

U∈U  V ∈V S∈Spα

L(U  V ; S) +

(cid:101)L(U  V ; S) +

|||U(cid:62)U − V (cid:62)V |||2
F  
|||U(cid:62)U − V (cid:62)V |||2
F .

1
8
1
64

(fully observed)

(partially observed)

(8)

(9)

The regularization term in the objectives above is used to encourage that U and V have the same
scale. Given (U0  V0)  we propose the following iterative method to produce series {(Ut  Vt)}∞
and {St}∞
t=0
t=0. We give the details for the fully observed case – the partially observed case is similar.

1 SVDr[A] stands for computing a rank-r SVD of matrix A  i.e.  ﬁnding the top r singular values and vectors
of A. Note that we only need to compute rank-r SVD approximately (see the initialization error requirement in
Theorem 1) so that we can leverage fast iterative approaches such as block power method and Krylov subspace
methods.

4

For t = 0  1  . . .  we update St using the sparse estimator St = Tγα
projected gradient update on Ut and Vt:

Ut+1 = ΠU

Vt+1 = ΠV

Ut − η∇UL(Ut  Vt; St) − 1
2
Vt − η∇V L(Ut  Vt; St) − 1
2

ηUt(U(cid:62)

ηVt(V (cid:62)

(cid:18)
(cid:18)

(cid:3)  followed by a

(cid:2)Y − UtV (cid:62)
(cid:19)
(cid:19)

t Vt)

t

 

t Ut)

.

t Ut − V (cid:62)
t Vt − U(cid:62)

Here α is the model parameter that characterizes the corruption fraction  γ and η are algorithmic
tunning parameters  which we specify in our analysis. Essentially  the above algorithm corresponds
to applying projected gradient method to optimize (8)  where S is replaced by the aforementioned
sparse estimator in each step.

Algorithm 2 Fast RPCA with partial observations
INPUT: Observed matrix Y with support Φ; parameters τ  γ  η; number of iterations T .
// Phase I: Initialization.
1: Sinit ← T2pα [ΠΦ(Y )]
2: [L  Σ  R] ← SVDr[ 1
3: U0 ← LΣ1/2  V0 ← RΣ1/2. Let U V be deﬁned according to (7).
// Phase II: Gradient based iterations.
4: U0 ← ΠU (U0)  V0 ← ΠV (V0)
5: for t = 0  1  . . .   T − 1 do
6:
7:

p (Y − Sinit)]
(cid:1)(cid:3)

(cid:2)ΠΦ
(cid:0)Y − UtV (cid:62)
(cid:16)
Ut − η∇U(cid:101)L(Ut  Vt; St) − 1
(cid:16)
Vt − η∇V (cid:101)L(Ut  Vt; St) − 1

St ← Tγpα
Ut+1 ← ΠU
Vt+1 ← ΠV

t Ut − V (cid:62)
t Vt − U(cid:62)

16 ηUt(U(cid:62)
16 ηVt(V (cid:62)

(cid:17)
(cid:17)

t Vt)

t Ut)

t

8:
9: end for
OUTPUT: (UT   VT )

4 Main Results
4.1 Analysis of Algorithm 1
We begin with some deﬁnitions and notation. It is important to deﬁne a proper error metric because
the optimal solution corresponds to a manifold and there are many distinguished pairs (U  V ) that
minimize (8). Given the SVD of the true low-rank matrix M∗ = L∗Σ∗R∗(cid:62)  we let U∗ := L∗Σ∗1/2
and V ∗ := R∗Σ∗1/2. We also let σ∗
r be sorted nonzero singular values of
M∗  and denote the condition number of M∗ by κ  i.e.  κ := σ∗
r . We deﬁne estimation error
d(U  V ; U∗  V ∗) as the minimal Frobenius norm between (U  V ) and (U∗  V ∗) with respect to the
optimal rotation  namely

2 ≥ . . . ≥ σ∗

1 ≥ σ∗

1/σ∗

d(U  V ; U∗  V ∗) := min
Q∈Qr

|||U V (cid:62) − M∗|||F (cid:46)(cid:112)σ∗

for Qr the set of r-by-r orthonormal matrices. This metric controls reconstruction error  as

when d(U  V ; U∗  V ∗) ≤ (cid:112)σ∗

1d(U  V ; U∗  V ∗) 

(11)
1. We denote the local region around the optimum (U∗  V ∗) with

B2 (ω) :=(cid:8)(U  V ) ∈ Rd1×r × Rd2×r(cid:12)(cid:12) d(U  V ; U∗  V ∗) ≤ ω(cid:9) .

radius ω as

The next two theorems provide guarantees for the initialization phase and gradient iterations  respec-
tively  of Algorithm 1.
Theorem 1 (Initialization). Consider the paired (U0  V0) produced in the ﬁrst phase of Algorithm 1.
If α ≤ 1/(16κµr)  we have

(cid:113)|||U − U∗Q|||2

F + |||V − V ∗Q|||2
F  

(10)

r(cid:112)σ∗

1.

√

√

καµr

d(U0  V0; U∗  V ∗) ≤ 28

5

Theorem 2 (Convergence). Consider the second phase of Algorithm 1. Suppose we choose γ = 2
1 for any c ≤ 1/36. There exist constants c1  c2 such that when α ≤ c1/(κ2µr)  given
and η = c/σ∗
any (U0  V0) ∈ B2

  the iterates {(Ut  Vt)}∞

t=0 satisfy

c2

(cid:16)

(cid:17)

(cid:112)σ∗
d2(Ut  Vt; U∗  V ∗) ≤(cid:16)

r /κ

(cid:17)t

1 − c
8κ

d2(U0  V0; U∗  V ∗).

0 − M∗|||op ≤ 1

√
1. When α (cid:46) 1/(µ

Therefore  using proper initialization and step size  the gradient iteration converges at a linear
rate with a constant contraction factor 1 − O(1/κ). To obtain relative precision ε compared to
the initial error  it sufﬁces to perform O(κ log(1/ε)) iterations. Note that the step size is chosen
according to 1/σ∗
κr3)  Theorem 1 and the inequality (11) together imply that
|||U0V (cid:62)
0 )) using being the
top singular value σ1(U0V (cid:62)
Combining Theorems 1 and 2 implies the following result that provides an overall guarantee for
Algorithm 1.
Corollary 1. Suppose that

1. Hence we can set the step size as η = O(1/σ1(U0V (cid:62)
0 ) of the matrix U0V (cid:62)

2 σ∗

0

(cid:40)

(cid:41)

α ≤ c min

1
√
κr3  
µ

1

µκ2r

for some constant c. Then for any ε ∈ (0  1)  Algorithm 1 with T = O(κ log(1/ε)) outputs a pair
(UT   VT ) that satisﬁes

|||UT V (cid:62)

T − M∗|||F ≤ ε · σ∗
r .

(12)

Remark 1 (Time Complexity). For simplicity we assume d1 = d2 = d. Our sparse estimator (4)
can be implemented by ﬁnding the top αd elements of each row and column via partial quick sort 
which has running time O(d2 log(αd)). Performing rank-r SVD in the ﬁrst phase and computing the
gradient in each iteration both have complexity O(rd2).3 Algorithm 1 thus has total running time
O(κrd2 log(1/ε)) for achieving an  accuracy as in (12). We note that when κ = O(1)  our algorithm
is orderwise faster than the AltProj algorithm in [21]  which has running time O(r2d2 log(1/ε)).
Moreover  our algorithm only requires computing one singular value decomposition.
Remark 2 (Robustness). Assuming κ = O(1)  our algorithm can tolerate corruption at a sparsity
level up to α = O(1/(µr
r compared to the optimal statistical
guarantee 1/(µr) obtained in [11  18  21]. This looseness is a consequence of the condition for
(U0  V0) in Theorem 2. Nevertheless  when µr = O(1)  our algorithm can tolerate a constant α
fraction of corruptions.

r)). This is worse by a factor

√

√

4.2 Analysis of Algorithm 2
We now move to the guarantees of Algorithm 2. We show here that not only can we handle partial
observations  but in fact subsampling the data in the fully observed case can signiﬁcantly reduce the
time complexity from the guarantees given in the previous section without sacriﬁcing robustness. In
particular  for smaller values of r  the complexity of Algorithm 2 has near linear dependence on the
dimension d  instead of quadratic.
In the following discussion  we let d := max{d1  d2}. The next two results control the quality of the
initialization step  and then the gradient iterations.
Theorem 3 (Initialization  partial observations). Suppose the observed indices Φ follow the Bernoulli
model given in (2). Consider the pair (U0  V0) produced in the ﬁrst phase of Algorithm 2. There exist
constants {ci}3

i=1 such that for any  ∈ (0 

r/(8c1κ))  if

√

α ≤ 1

64κµr

  p ≥ c2

then we have

d(U0  V0; U∗  V ∗) ≤ 51

καµr

√

with probability at least 1 − c3d−1.

(cid:18) µr2

(cid:19) log d
1 + 7c1(cid:112)κσ∗
r(cid:112)σ∗

d1 ∧ d2

1
α

 

1 

2 +
√

(13)

3In fact  it sufﬁces to compute the best rank-r approximation with running time independent of the eigen gap.

6

Theorem 4 (Convergence  partial observations). Suppose the observed indices Φ follow the Bernoulli
model given in (2). Consider the second phase of Algorithm 2. Suppose we choose γ = 3  and
η = c/(µrσ∗

1) for a sufﬁciently small constant c. There exist constants {ci}4

i=1 such that if

 

(14)

d1 ∧ d2
then with probability at least 1 − c3d−1  the iterates {(Ut  Vt)}∞

α ≤ c1
κ2µr

and p ≥ c2

κ4µ2r2 log d

t=0 satisfy

(cid:18)

(cid:19)t

1 − c

64µrκ

d2(U0  V0; U∗  V ∗)

d2(Ut  Vt; U∗  V ∗) ≤

(cid:16)

(cid:112)σ∗

(cid:17)

for all (U0  V0) ∈ B2

c4

r /κ

.

Setting p = 1 in the above result recovers Theorem 2 up to an additional factor µr in the contraction
factor. For achieving ε relative accuracy  now we need O(µrκ log(1/ε)) iterations. Putting Theorems
3 and 4 together  we have the following overall guarantee for Algorithm 2.
Corollary 2. Suppose that

(cid:40)

(cid:41)

α ≤ c min

1
√
κr3  
µ

1

µκ2r

  p ≥ c(cid:48) κ4µ2r2 log d

d1 ∧ d2

 

for some constants c  c(cid:48). With probability at least 1 − O(d−1)  for any ε ∈ (0  1)  Algorithm 2 with
T = O(µrκ log(1/ε)) outputs a pair (UT   VT ) that satisﬁes
T − M∗|||F ≤ ε · σ∗
r .

|||UT V (cid:62)

(15)

This result shows that partial observations do not compromise robustness to sparse corruptions: as
long as the observation probability p satisﬁes the condition in Corollary 2  Algorithm 2 enjoys the
same robustness guarantees as the method using all entries. Below we provide two remarks on the
sample and time complexity. For simplicity  we assume d1 = d2 = d  κ = O(1).
Remark 3 (Sample complexity and matrix completion). Using the lower bound on p  it is sufﬁcient
to have O(µ2r2d log d) observed entries. In the special case S∗ = 0  our partial observation model
is equivalent to the model of exact matrix completion (see  e.g.  [8]). We note that our sample
complexity (i.e.  observations needed) matches that of completing a positive semideﬁnite (PSD)
matrix by gradient descent as shown in [12]  and is better than the non-convex matrix completion
algorithms in [19] and [23]. Accordingly  our result reveals the important fact that we can obtain
robustness in matrix completion without deterioration of our statistical guarantees. It is known that
that any algorithm for solving exact matrix completion must have sample size Ω(µrd log d) [8]  and a
nearly tight upper bound O(µrd log2 d) is obtained in [10] by convex relaxation. While sub-optimal
by a factor µr  our algorithm is much faster than convex relaxation as shown below.
Remark 4 (Time complexity). Our sparse estimator on the sparse matrix with support Φ can be
implemented via partial quick sort with running time O(pd2 log(αpd)). Computing the gradient
in each step involves the two terms in the objective function (9). Computing the gradient of the
phase  performing rank-r SVD on a sparse matrix with support Φ can be done in time O(r|Φ|). We
conclude that when |Φ| = O(µ2r2d log d)  Algorithm 2 achieves the error bound (15) with running
time O(µ3r4d log d log(1/ε)). Therefore  in the small rank setting with r (cid:28) d1/3  even when full
observations are given  it is better to use Algorithm 2 by subsampling the entries of Y .
5 Numerical Results
In this section  we provide numerical results and compare the proposed algorithms with existing
methods  including the inexact augmented lagrange multiplier (IALM) approach [20] for solving
the convex relaxation (1) and the alternating projection (AltProj) algorithm proposed in [21]. All
algorithms are implemented in MATLAB 4  and the codes for existing algorithms are obtained from
their authors. SVD computation in all algorithms uses the PROPACK library.5 We ran all simulations
on a machine with Intel 32-core Xeon (E5-2699) 2.3GHz with 240GB RAM.
4Our code is available at https://www.yixinyang.org/code/RPCA_GD.zip.
5http://sun.stanford.edu/~rmunk/PROPACK/

ﬁrst term (cid:101)L takes time O(r|Φ|)  whereas the second term takes time O(r2d). In the initialization

7

(a)

(b)

(c)

Figure 1: Results on synthetic data. (a) Plot of log estimation error versus number of iterations when using
gradient descent (GD) with varying sub-sampling rate p. It is conducted using d = 5000  r = 10  α = 0.1.
(b) Plot of running time of GD versus dimension d with r = 10  α = 0.1  p = 0.15r2 log d/d. The low-rank
matrix is recovered in all instances  and the line has slope approximately one. (c) Plot of log estimation error
versus running time for different algorithms in problem with d = 5000  r = 10  α = 0.1.

Figure 2: Foreground-background separation in Restaurant and ShoppingMall videos. In each line  the leftmost
image is an original frame  and the other four are the separated background obtained from our algorithms with
p = 1  p = 0.2  AltProj  and IALM. The running time required by each algorithm is shown in the title.

Synthetic Datasets. We generate a squared data matrix Y = M∗ + S∗ ∈ Rd×d as follows. The
low-rank part M∗ is given by M∗ = AB(cid:62)  where A  B ∈ Rd×r have entries drawn independently
from a zero mean Gaussian distribution with variance 1/d. For a given sparsity parameter α  each
entry of S∗ is set to be nonzero with probability α  and the values of the nonzero entries are sampled
uniformly from [−5r/d  5r/d]. The results are summarized in Figure 1. Figure 1a shows the
convergence of our algorithms for different random instances with different sub-sampling rate p.
Figure 1b shows the running time of our algorithm with partially observed data. We note that our
algorithm is memory-efﬁcient: in the large scale setting with d = 2 × 105  using approximately
0.1% entries is sufﬁcient for the successful recovery. In contrast  AltProj and IALM are designed
to manipulate the entire matrix with d2 = 4 × 1010 entries  which is prohibitive on single machine.
Figure 1c compares our algorithms with AltProj and IALM by showing reconstruction error versus
real running time. Our algorithm requires signiﬁcantly less computation to achieve the same accuracy
level  and using only a subset of the entries provides additional speed-up.
Foreground-background Separation. We apply our method to the task of foreground-background
(FB) separation in a video. We use two public benchmarks  the Restaurant and ShoppingMall
datasets.6 Each dataset contains a video with static background. By vectorizing and stacking the
frames as columns of a matrix Y   the FB separation problem can be cast as RPCA  where the static
background corresponds to a low rank matrix M∗ with identical columns  and the moving objects in
the video can be modeled as sparse corruptions S∗. Figure 2 shows the output of different algorithms
on two frames from the dataset. Our algorithms require signiﬁcantly less running time than both
AltProj and IALM. Moreover  even with 20% sub-sampling  our methods still seem to achieve
better separation quality. The details about parameter setting and more results are deferred to the
supplemental material.

6http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html

8

Iterationcount0246810d(U V;U∗ V∗)10-210-1100GDp=1GDp=0.5GDp=0.2Dimensiond103104105Time(secs)101102103Time(secs)020406080100d(U V;U∗ V∗)10-310-210-1100101GDp=1GDp=0.5GDp=0.2GDp=0.1AltProjIALMOriginalGD(49.8s)GD 20%sample(18.1s)AltProj(101.5s)IALM(434.6s)OriginalGD(87.3s)GD 20%sample(43.4s)AltProj(283.0s)IALM(801.4s)References
[1] Animashree Anandkumar  Rong Ge  Daniel Hsu  Sham M. Kakade  and Matus Telgarsky. Tensor decompositions for learning latent

variable models. The Journal of Machine Learning Research  15(1):2773–2832  2014.

[2] Sivaraman Balakrishnan  Martin J. Wainwright  and Bin Yu. Statistical guarantees for the em algorithm: From population to sample-

based analysis. arXiv preprint arXiv:1408.2156  2014.

[3] Srinadh Bhojanapalli  Prateek Jain  and Sujay Sanghavi. Tighter low-rank approximation via sampling the leveraged element.

Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms  pages 902–920. SIAM  2015.

In

[4] Srinadh Bhojanapalli  Anastasios Kyrillidis  and Sujay Sanghavi. Dropping convexity for faster semi-deﬁnite optimization. arXiv

preprint arXiv:1509.03917  2015.

[5] Emmanuel J. Candès  Xiaodong Li  Yi Ma  and John Wright. Robust principal component analysis?

58(3):11  2011.

Journal of the ACM (JACM) 

[6] Emmanuel J. Candès  Xiaodong Li  and Mahdi Soltanolkotabi. Phase retrieval via wirtinger ﬂow: Theory and algorithms.

Transactions on Information Theory  61(4):1985–2007  2015.

IEEE

[7] Emmanuel J. Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational mathemat-

ics  9(6):717–772  2009.

[8] Emmanuel J. Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.

Information Theory  56(5):2053–2080  2010.

IEEE Transactions on

[9] Venkat Chandrasekaran  Sujay Sanghavi  Pablo A. Parrilo  and Alan S. Willsky. Rank-sparsity incoherence for matrix decomposition.

SIAM Journal on Optimization  21(2):572–596  2011.

[10] Yudong Chen. Incoherence-optimal matrix completion. IEEE Transactions on Information Theory  61(5):2909–2923  2015.

[11] Yudong Chen  Ali Jalali  Sujay Sanghavi  and Constantine Caramanis. Low-rank matrix recovery from errors and erasures.

Transactions on Information Theory  59(7):4324–4337  2013.

IEEE

[12] Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic

guarantees. arXiv preprint arXiv:1509.03025  2015.

[13] Yuxin Chen and Emmanuel J. Candès. Solving random quadratic systems of equations is nearly as easy as solving linear systems. In

Advances in Neural Information Processing Systems  pages 739–747  2015.

[14] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the

forty-ﬁfth annual ACM symposium on Theory of computing  pages 81–90. ACM  2013.

[15] Alan Frieze  Ravi Kannan  and Santosh Vempala. Fast monte-carlo algorithms for ﬁnding low-rank approximations. Journal of the ACM

(JACM)  51(6):1025–1041  2004.

[16] Quanquan Gu  Zhaoran Wang  and Han Liu. Low-rank and sparse structure pursuit via alternating minimization. In Proceedings of the

19th International Conference on Artiﬁcial Intelligence and Statistics  pages 600–609  2016.

[17] Moritz Hardt. Understanding alternating minimization for matrix completion. In 2014 IEEE 55th Annual Symposium on Foundations of

Computer Science (FOCS)  pages 651–660. IEEE  2014.

[18] Daniel Hsu  Sham M. Kakade  and Tong Zhang. Robust matrix decomposition with sparse corruptions. IEEE Transactions on Information

Theory  57(11):7221–7234  2011.

[19] Prateek Jain  Praneeth Netrapalli  and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of

the forty-ﬁfth annual ACM symposium on Theory of computing  pages 665–674. ACM  2013.

[20] Zhouchen Lin  Minming Chen  and Yi Ma. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices.

Arxiv preprint arxiv:1009.5055v3  2013.

[21] Praneeth Netrapalli  UN Niranjan  Sujay Sanghavi  Animashree Anandkumar  and Prateek Jain. Non-convex robust pca. In Advances in

Neural Information Processing Systems  pages 1107–1115  2014.

[22] Ju Sun  Qing Qu  and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096  2015.

[23] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In 2015 IEEE 56th Annual Symposium on

Foundations of Computer Science (FOCS)  pages 270–289. IEEE  2015.

[24] Stephen Tu  Ross Boczar  Mahdi Soltanolkotabi  and Benjamin Recht. Low-rank solutions of linear matrix equations via procrustes ﬂow.

arXiv preprint arXiv:1507.03566  2015.

[25] Zhaoran Wang  Quanquan Gu  Yang Ning  and Han Liu. High dimensional em algorithm: Statistical optimization and asymptotic

normality. In Advances in Neural Information Processing Systems  pages 2512–2520  2015.

[26] Huan Xu  Constantine Caramanis  and Sujay Sanghavi. Robust pca via outlier pursuit.

58(5):3047–3064  May 2012.

IEEE Transactions on Information Theory 

[27] Xinyang Yi and Constantine Caramanis. Regularized em algorithms: A uniﬁed framework and statistical guarantees. In Advances in

Neural Information Processing Systems  pages 1567–1575  2015.

[28] Huishuai Zhang  Yuejie Chi  and Yingbin Liang. Provable non-convex phase retrieval with outliers: Median truncated wirtinger ﬂow.

arXiv preprint arXiv:1603.03805  2016.

[29] Tuo Zhao  Zhaoran Wang  and Han Liu. A nonconvex optimization framework for low rank matrix estimation. In Advances in Neural

Information Processing Systems  pages 559–567  2015.

[30] Qinqing Zheng and John Lafferty. A convergent gradient descent algorithm for rank minimization and semideﬁnite programming from

random linear measurements. In Advances in Neural Information Processing Systems  pages 109–117  2015.

9

,Xinyang Yi
Dohyung Park
Yudong Chen
Constantine Caramanis