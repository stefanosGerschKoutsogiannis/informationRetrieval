2018,Masking: A New Perspective of Noisy Supervision,It is important to learn various types of classifiers given training data with noisy labels. Noisy labels  in the most popular noise model hitherto  are corrupted from ground-truth labels by an unknown noise transition matrix. Thus  by estimating this matrix  classifiers can escape from overfitting those noisy labels. However  such estimation is practically difficult  due to either the indirect nature of two-step approaches  or not big enough data to afford end-to-end approaches. In this paper  we propose a human-assisted approach called ''Masking'' that conveys human cognition of invalid class transitions and naturally speculates the structure of the noise transition matrix. To this end  we derive a structure-aware probabilistic model incorporating a structure prior  and solve the challenges from structure extraction and structure alignment. Thanks to Masking  we only estimate unmasked noise transition probabilities and the burden of estimation is tremendously reduced. We conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise structures as well as the industrial-level Clothing1M with agnostic noise structure  and the results show that Masking can improve the robustness of classifiers significantly.,Masking: A New Perspective of Noisy Supervision

Bo Han∗1 2  Jiangchao Yao∗3 1  Gang Niu2  Mingyuan Zhou4 

Ivor W. Tsang1  Ya Zhang3  Masashi Sugiyama2 5

1Centre for Artiﬁcial Intelligence  University of Technology Sydney

2Center for Advanced Intelligence Project  RIKEN

3Cooperative Medianet Innovation Center  Shanghai Jiao Tong University

4McCombs School of Business  The University of Texas at Austin

5Graduate School of Frontier Sciences  University of Tokyo

Abstract

It is important to learn various types of classiﬁers given training data with noisy
labels. Noisy labels  in the most popular noise model hitherto  are corrupted from
ground-truth labels by an unknown noise transition matrix. Thus  by estimating
this matrix  classiﬁers can escape from overﬁtting those noisy labels. However 
such estimation is practically difﬁcult  due to either the indirect nature of two-step
approaches  or not big enough data to afford end-to-end approaches. In this paper 
we propose a human-assisted approach called “Masking” that conveys human cog-
nition of invalid class transitions and naturally speculates the structure of the noise
transition matrix. To this end  we derive a structure-aware probabilistic model
incorporating a structure prior  and solve the challenges from structure extraction
and structure alignment. Thanks to Masking  we only estimate unmasked noise
transition probabilities and the burden of estimation is tremendously reduced. We
conduct extensive experiments on CIFAR-10 and CIFAR-100 with three noise
structures as well as the industrial-level Clothing1M with agnostic noise struc-
ture  and the results show that Masking can improve the robustness of classiﬁers
signiﬁcantly.

1

Introduction

It is always challenging to learn from noisy labels [2  34  4  37  25]  since these labels are systemati-
cally corrupted. As a negative effect  noisy labels inevitably degenerate the accuracy of classiﬁers.
This negative effect becomes more prominent for deep learning  since these complex models can fully
memorize noisy labels  which correspondingly degenerates their generalization [48]. Unfortunately 
noisy labels are ubiquitous and unavoidable in our daily life  such as web queries [24]  social-network
tagging [6]  crowdsourcing [45]  medical images [8]  and ﬁnancial analysis [1].
To handle such noisy labels  recent approaches explore three directions mainly. One direction focuses
on training only on selected samples  which leverages the sample-selection bias [18] to overcome
the label noise issue. For example  MentorNet [19] trains on “small-loss” samples. Meanwhile 
Decoupling [26] trains on “disagreement” samples  for which the predictions of two networks
disagree. However  since the data for training are selected on the ﬂy rather than selected in the
beginning  it is hard to characterize these sample-selection biases  and then it is hard to give any
theoretical guarantee on the consistency of learning.
Another direction develops regularization techniques  including explicit and implicit regularizations.
This direction employs the regularization bias to overcome the label noise issue. Explicit regulariza-
∗The ﬁrst two authors (Bo Han and Jiangchao Yao) made equal contributions. The implementation is available

at https://github.com/bhanML/Masking.

32nd Conference on Neural Information Processing Systems (NeurIPS 2018)  Montréal  Canada.

tion is added to the objective function  such as manifold regularization [5] and virtual adversarial
training [30]. Implicit regularization is designed for training algorithms  such as temporal ensem-
bling [20] and mean teacher [41]. Nevertheless  both approaches introduce a permanent regularization
bias  and the learned classier barely reaches the optimal performance [9].
The last direction estimates the noise transition matrix without introducing sample-selection bias
and regularization bias. As an approximation of real-world corruption  noisy labels are theoretically
ﬂipped from the ground-truth labels by an unknown noise transition matrix. In this approach  the
accuracy of classiﬁers can be improved by estimating this matrix accurately. The previous methods
for estimating the noise transition matrix can be roughly summarized into two solutions.
One solution estimates the noise transition matrix in advance  and subsequently learns the classiﬁer
based on this estimated matrix. For example  Patrini et al. [32] leveraged a two-step solution to
estimate the noise transition matrix. The beneﬁt is to require limited data only for the estimation
procedure. Nonetheless  their method is too heuristic to estimate the noise transition matrix accurately.
The other solution jointly estimates the noise transition matrix and learns the classiﬁer in an end-to-
end framework. For instance  on top of the softmax layer  Sukhbaatar et al. [39] added a constrained
linear layer to model the noise transition matrix  while Goldberger et al. [10] added a nonlinear
softmax layer. The beneﬁt is the generality of their uniﬁed learning framework. However  their
brute-force learning leads to inexact estimation due to a ﬁnite dataset.
Therefore  an important question is  with a ﬁnite dataset  can we leverage a constrained end-to-end
model to overcome the above deﬁciencies? In this paper  we present a human-assisted approach
called “Masking”. Masking conveys human cognition of invalid class transitions (i.e.  cat (cid:61) car) 
and speculates the structure of the noise transition matrix. The structure information can be viewed
as a constraint to improve the estimation procedure. Namely  given the structure information  we can
focus on estimating the noise transition probability along the structure  which reduces the estimation
burden largely.
To instantiate our approach  we derive a structure-aware probabilistic model  by incorporating a
structure prior. In the realization  we encounter two practical challenges: structure extraction and
structure alignment. Speciﬁcally  to address the structure extraction challenge  we propose a tempered
sigmoid function to simulate the human cognition on the structure of the noise transition matrix. To
address the structure alignment challenge  we propose a variant of Generative Adversarial Networks
(GANs) [12] to avoid the difﬁculty of specifying the explicit distributions. We conduct extensive
experiments on two benchmark datasets (CIFAR-10 and CIFAR-100) with three noise structures  and
the industrial-level dataset (Clothing1M[46]) with agnostic noise structure. The experimental results
demonstrate that the proposed approach can improve the robustness of classiﬁers signiﬁcantly.

2 A new perspective of noisy supervision

In this section  we explore the noisy supervision from a brand new perspective  namely the structure
of the noise transition matrix. First  we discuss where noisy labels normally come from  and why
we can speculate the structure of the noise transition matrix. Then  we present the representative
structures of the noise transition matrix.
In practice  noisy labels mainly come from the interaction between humans and tasks  such as
social-network tagging and crowdsourcing. Assume that the more complex the interaction is  the
more efforts human beings spend. Due to cognitive psychology [29]  human cognition can mask
invalid class transitions  and highlight valid class transitions automatically. This denotes that  human
cognition can speculate the structure of the noise transition matrix correspondingly.
One effort-saving interaction is that  you tag a cluster of scenery images in social networks  such
as beach  prairie  and mountain. However  the foreground of some images appears a dog or a cat 
yielding noisy labels [35]. Thus  human cognition masks invalid class transitions (i.e.  beach (cid:61)
mountain)  and highlights valid class transitions (i.e.  beach ↔ dog). This noise structure should be
the diagonal matrix coupled with two column lines  namely a column-diagonal matrix (Figure 1(a)) 
where the column lines correspond to the dog and cat classes  respectively.

2

(a) Column-diagonal

(b) Tri-diagonal

(c) Block-diagonal

Figure 1: Three types of noise structure. Vertical axis denotes the class of ground-truth label  while
horizontal axis denotes the class of noisy label. White block means the valid class transitions  while
black block means the invalid class transitions.

Another effort-consuming interactions stem from the task annotation on Amazon Mechanical Turk 2.
Even with high-degree efforts  amateur workers may be potentially confused by very similar classes
to yield noisy labels  due to their limited expertise. There are two practical cases.
The ﬁne-grained case denotes that  the transition from one class to its similar class is continuous (e.g. 
Australian terrier  Norfolk terrier  Norwich terrier  Irish terrier  and Scotch terrier)  and workers make
mistakes in the adjacent positions [7]. Thus  human cognition can mask invalid class transitions (i.e. 
Australian terrier (cid:61) Norwich terrier)  and highlight valid class transitions (i.e.  Norfolk terrier ↔
Norwich terrier ↔ Irish terrier). This noise structure should be a tri-diagonal matrix (Figure 1(b)).
The hierarchical-grained case denotes that  the transition among super-classes is discrete (e.g.  aquatic
mammals and ﬂowers) and impossible  while the transition among sub-classes is continuous (e.g. 
aquatic mammals contain beaver  dolphin  otter  seal  and whale) and possible [32]. Thus  human
cognition can mask invalid class transitions (i.e.  aquatic mammals (cid:61) ﬂowers)  and highlight valid
class transitions (i.e.  beaver ↔ dolphin). This noise structure should be a block-diagonal matrix
(Figure 1(c))  where each block represents a super-class.
When we already know the noise structure from human cognition  we only focus on estimating the
noise transition probability. However  how can we instill the structure information into the estimation
procedure? We are going to answer this question in the following sections.

3 Learning with Masking

We brieﬂy show how benchmark models handle noisy labels  and reveal their deﬁciencies (Section 3.1).
Then  we show why the Masking approach can solve such issues (Section 3.2). After the Masking
procedure  we present a straightforward idea to incorporate the structure information  and show
its potential dilemma (Section 3.2.1). Fortunately  we ﬁnd a suitable approach to incorporate the
structure information into an end-to-end model (Section 3.2.2). To realize this approach  we encounter
two practical challenges  and present principled solutions (Section 3.2.3).

3.1 Deﬁciency of benchmark models

Figure 2(a) is a basic model to train a classiﬁer in the setting of noisy labels. Assuming that x
represents a “Dog” image  its latent ground-truth label y should be a “Dog” class. However  its
annotated label ˜y belongs to the “Cat” class. In essence  the noisy label ˜y is ﬂipped from the ground-
truth label y by an unknown noise transition matrix. Therefore  current techniques tend to improve
the accuracy of classiﬁer (x → y) by estimating the noise transition matrix (y → ˜y).
Here  we introduce two benchmark realizations of Figure 2(a). The ﬁrst benchmark model comes
from Patrini et al. [32]  which uses the anchor set condition [21] to independently estimate the noise

2https://www.mturk.com/

3

Noisy labelGround-truth label(a) Benchmark model.

(b) MASKING model.

Figure 2: Comparison of benchmark models and our MASKING model. Assume that  (x  ˜y) denotes
the instance with the noisy label  and y represents the latent ground-truth label. T is the noise
transition matrix  where Tij = Pr(˜y = ej|y = ei). Left Panel: a benchmark model. Right Panel:
MASKING models the matrix T by an explicit variable s. Thus  we embed a structure constraint on
the variable s  where the structure information come from human cognition h.

transition matrix (y → ˜y). Based on the estimated matrix  they learn the classiﬁer (x → y) by
the strategy of loss corrections. However  the estimation phase is not justiﬁed for agnostic noisy
data  which thus limits the performance of the classiﬁer. The other benchmark model comes from
Goldberger and Ben-Reuven [10]  which uniﬁes the two steps of the ﬁrst benchmark model into a
joint fashion. Speciﬁcally  the noise transition matrix (y → ˜y) modeled by a nonlinear softmax layer
connects the classiﬁer (x → y) with noisy labels ˜y for the end-to-end training. However  due to a
ﬁnite dataset  this brute-force estimation may suffer from many local minima.

3.2 Does structure matter?

Therefore  given a ﬁnite dataset  can we leverage a constrained end-to-end model to solve the above
deﬁciencies? The answer is afﬁrmative. The reason can be explained intuitively: when human
cognition masks the invalid class transitions (i.e.  cat (cid:61) car)  the structure information is available
as the constraint. The constrained end-to-end model only focuses on estimating the noise transition
probability. The estimation burden will be largely reduced  and thus the brute-force estimation can
ﬁnd a good local minimum more easily.
In this paper  we summarize this human-assisted approach as “Masking”. Intuitively  with high
probability  Masking means some class transitions will not happen (i.e.  cat (cid:61) car)  while some class
transitions will happen (i.e.  beaver ↔ dolphin). Before delving into our realization  we ﬁrst present
a straightforward idea to incorporate the structure information into an end-to-end model as follows.

3.2.1 Straightforward dilemma

For structure instillation  the most straightforward idea is to add a regularizer on the objective of
an end-to-end model. Namely  we can use a regularizer to represent the structure constraint. Due
to the regularization effect in the optimization [11]  the noise transition matrix learned by previous
models [32  10] will satisfy our expectation regarding the structure. For example  we can apply the
Lagrange multiplier in the benchmark model from [10] to instantiate this idea.
However  such a deterministic method may not be easily implemented in practice due to three reasons.
First  such a class of regularizers requires a suitable distance measure to compute the distance between
the learned transition matrix and the prior  and the corresponding regularization parameter. In deep
learning scenarios  it is quite hard to justify the choice of a distance measure  e.g.  why choosing L2
distance instead of other distance measures for the structure instillation. Second  if we leverage a
noisy validation set  we need to construct the unbiased risk estimator for the backward correction.
This is impossible  as the inverse of estimated noise transition matrix cannot be accurately computed.
Last but not least  even though we construct a clean validation set  it needs to repeat the training
procedure to tune the regularization parameter  which consumes remarkable computational resources.
Speciﬁcally  it requires a lot of non-trivial trials in the training-validation phase to ﬁnd an optimal
weight. For example  the Residual-52 net will take about one week to be well trained on the
WebVision dataset  consisting of 16 million images with noisy labels [21]. If we consider adding a

4

x<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>ey<latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit>y<latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit>x<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>y<latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit>s<latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit><latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit><latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit><latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit>h<latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit><latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit><latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit><latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit>ey<latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit>x<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>ey<latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit>y<latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit>x<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>y<latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit><latexit sha1_base64="l29WxoUb9DEbvmhLG7jHtZ0OU24=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KokIeix68diC/YA2lM120q7dbMLuRgihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEsG1cd1vZ219Y3Nru7RT3t3bPzisHB23dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/mdJ1Sax/LBZAn6ER1JHnJGjZWa2aBSdWvuHGSVeAWpQoHGoPLVH8YsjVAaJqjWPc9NjJ9TZTgTOC33U40JZRM6wp6lkkao/Xx+6JScW2VIwljZkobM1d8TOY20zqLAdkbUjPWyNxP/83qpCW/8nMskNSjZYlGYCmJiMvuaDLlCZkRmCWWK21sJG1NFmbHZlG0I3vLLq6R9WfPcmte8qtZvizhKcApncAEeXEMd7qEBLWCA8Ayv8OY8Oi/Ou/OxaF1zipkT+APn8wfnvYz9</latexit>s<latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit><latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit><latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit><latexit sha1_base64="N1eK0lTaQQAFA6yHzcECkl4oWJk=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipqQflilt1FyDrxMtJBXI0BuWv/jBmaYTSMEG17nluYvyMKsOZwFmpn2pMKJvQEfYslTRC7WeLQ2fkwipDEsbKljRkof6eyGik9TQKbGdEzVivenPxP6+XmvDGz7hMUoOSLReFqSAmJvOvyZArZEZMLaFMcXsrYWOqKDM2m5INwVt9eZ20r6qeW/Wa15X6bR5HEc7gHC7BgxrU4R4a0AIGCM/wCm/Oo/PivDsfy9aCk8+cwh84nz/epYz3</latexit>h<latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit><latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit><latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit><latexit sha1_base64="SsYKOOt1jTfiNkyHK7x6GFFEbxM=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lEqMeiF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dgobm1vbO8Xd0t7+weFR+fikreNUMWyxWMSqG1CNgktsGW4EdhOFNAoEdoLJ3dzvPKHSPJYPZpqgH9GR5CFn1FipOR6UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8tDp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasIbP+MySQ1KtlwUpoKYmMy/JkOukBkxtYQyxe2thI2poszYbEo2BG/15XXSvqp6btVrXlfqt3kcRTiDc7gED2pQh3toQAsYIDzDK7w5j86L8+58LFsLTj5zCn/gfP4AzfmM7A==</latexit>ey<latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit><latexit sha1_base64="QT2IxtzsZE19pW+mY+Tf5fnZv8c=">AAAB9HicbVBNS8NAEJ3Ur1q/qh69BIvgqSQi6LHoxWMF+wFtKJvNpF262cTdTSWE/g4vHhTx6o/x5r9x2+agrQ8GHu/NMDPPTzhT2nG+rdLa+sbmVnm7srO7t39QPTxqqziVFFs05rHs+kQhZwJbmmmO3UQiiXyOHX98O/M7E5SKxeJBZwl6ERkKFjJKtJG8/hMLUDMeYJ5NB9WaU3fmsFeJW5AaFGgOql/9IKZphEJTTpTquU6ivZxIzSjHaaWfKkwIHZMh9gwVJELl5fOjp/aZUQI7jKUpoe25+nsiJ5FSWeSbzojokVr2ZuJ/Xi/V4bWXM5GkGgVdLApTbuvYniVgB0wi1TwzhFDJzK02HRFJqDY5VUwI7vLLq6R9UXedunt/WWvcFHGU4QRO4RxcuIIG3EETWkDhEZ7hFd6sifVivVsfi9aSVcwcwx9Ynz9uYpKE</latexit>regularizer to instill structure information  each trial for adjusting the weight is a disaster. To sum up 
we do not consider this straightforward regularization approach.

3.2.2 When structure meets generative model

Based on the previous discussions  we conjecture that the Bayesian method should be a more suitable
tool to model the structure information  since the structure information can be explicitly represented
as the prior. Following this conjecture  we deduce an end-to-end probabilistic model to incorporate
the structure information as shown in Figure 2(b).
Compared to benchmark models in Figure 2(a)  we model the noise transition matrix with a random
variable s  and we instill the structure information by controlling the prior of its corresponding
structure variable  termed as so. Here  we assume there exists a deterministic function f (·) such
that so = f (s). The reason that we make such an assumption can be intuitively explained with the
observation  once one arbitrary matrix is given  human cognition can certainly describe its structure 
e.g.  diagonal or tri-diagonal. Thus  there must be a function that can implement the mapping from
s to its structure so. For clarity  we present an exemplar generative process for “multi-class” &
“single-label” classiﬁcation problem with noisy supervision.

• The latent ground-truth label y ∼ P (y|x)  where P (y|x) is a Categorical distribution 3.
• The noise transition matrix s ∼ P (s) and its structure so ∼ P (so)  where P (s) is an
implicit distribution modeled by neural networks without the exact form (i.e.  multi-Dirac
distribution)  P (so) = P (s) ds

  and f (·) is the mapping function from s to so.

• The noisy label ˜y ∼ P (˜y|y  s)  where P (˜y|y  s) models the transition from y to ˜y given s.
According to the above generative process  we can deduce the following evidence lower bound
(ELBO) (the details in Appendix A) to approximate the log-likelihood of the noisy data  which can
be named as MASKING. MASKING is a structure-aware probabilistic model:

dso(cid:12)(cid:12)so=f (s)

− lnQ(so)/ P (so)
(cid:124)(cid:123)(cid:122)(cid:125)

structure prior

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)so=f (s)



 

(1)

where Q(s) is the variational distribution to approximate the posterior of the noise transition matrix s 
and Q(so) = Q(s) ds
is the corresponding variational distribution of the structure so. Eq. (1)
seamlessly uniﬁes previous models and structure instillation  remarked as the following beneﬁts.

Remark 1 The ﬁrst term inside the expectation in Eq. (1) recovers the previous benchmark models 
representing the log-likelihood from x to the noisy label ˜y. The second term inside the expectation in
Eq. (1) is for structure instillation  reﬂecting the inconsistency between the distribution Q(so) learned
from the training data and the structure prior P (so) provided by human cognition.
As a whole  MASKING model beneﬁts from the human guidance (the second term) in the procedure
of learning with noisy supervisions (the ﬁrst term)  which avoids the unexpected local minima with
incorrect structures in previous works. Moreover  we avoid the difﬁculty of hyperparameter selection
by deducing a Bayesian framework  which does not require the regularization parameter to be tuned.
Note that human cognition may introduce uncertainty. In this case  we just focus on the certain
knowledge and use this knowledge as the prior; on the other hand  we dispose the uncertain knowledge
by Masking. A special case is when we do not have any transition knowledge  we can unmask the
whole matrix  i.e.  allowing all possible transitions. This naturally degenerates our MASKING model
to the unconstrained S-adapation method.

3.2.3 Towards principled realization

To realize the MASKING model  concretely  the second term in Eq. (1)  we encounter two practical
challenges  and present the principled solutions as follows.

3For the single-label classiﬁcation  a Categorical distribution is a natural choice. For the multi-label

classiﬁcation  a Multinomial distribution is used  since one example corresponds to multiple labels.

5

ln P (˜y|x) ≥ E

P (˜y|y  s)P (y|x)

previous model

(cid:123)(cid:122)

(cid:125)

Q(s)


ln(cid:88)y
(cid:124)
dso(cid:12)(cid:12)so=f (s)

f (s) =

  where α ∈ (0  1)  β (cid:28) 1 

(2)

1

1 + exp(− s−α
β )

Challenge from structure extraction: One challenge comes from how to specify the mapping
function f (·) in Eq. (1)  which extracts the structure variable so from the variable s. Without this step 
we cannot compute the structure variable so for Q(so) = Q(s) ds
  and let alone optimize
the second term in Eq. (1). However  to the best of our knowledge  there is no related work that
speciﬁes f (·) in the area of structure extraction.
Here  we explore a principled solution by simulating human cognition on the structure of the noise
transition matrix. In terms of the noise transition probability between two classes  human cognition
considers the small value (i.e.  0.5%) as a sign of the invalid class transition  but the large value
(i.e.  20%) as a noise pattern [14]. It indicates that  the larger transition probability is favored when
quantifying the noise transition matrix into a structure. Such a procedure is very similar to the
thresholding binarization operation with a tempered sigmoid function (Eq. (2)). Thus  we can use the
following tempered sigmoid function as f (·) to simulate the mapping from s to so 

dso(cid:12)(cid:12)so=f (s)

and we name its output f (s) as the masked structure so from s. By controlling the location parameter
α and the scale parameter β  Eq. (2) quantiﬁes s into the structure so for the second term in Eq. (1).
Challenge from structure alignment: The second term in Eq. (1) is to make the structure learned
from the training data (represented by Q(so))  close to the human prior (represented by P (so)). We
consider it as structure alignment. The challenge here is how to specify the distributions P (so) and
Q(so) in Eq. (1) to reasonably measure their divergence for the structure alignment. The smaller
divergence between P (so) and Q(so)  the more similar they are.
This question is difﬁcult because  for prior P (so)  we usually provide one or a few limited structure
candidates  and human cognition has the sparse empirical certainty [13] according to a speciﬁc noisy
data. That is to say  P (so) should be a distribution that concentrates on one or a few points  e.g. 
a multi-Dirac distribution or a multi-spike distribution4. These distributions are quite unstable for
optimization  since they are approximately discrete  and easy to cause the computational overﬂow
problem [36]. Regarding Q(so)  it is equal to specifying Q(s) since they are correlated by so = f (s).
If we ﬁnd Q(s) such that E
Q(s) [ln Q(so)/P (so)] is analytically computable  we can avoid this
challenge. However  such a speciﬁcation with existing distributions is usually intractable.
Fortunately  we can employ the implicit models to deal with the above dilemma [42]. This is because
in the implicit models  the distribution is directly simulated with neural networks plus a random noise 
which avoids to specify an explicit distribution. Following this methodology  Q(s)  Q(so) and P (so)
in Eq. (1) can be implemented like Generative Adversarial Networks (GANs). Speciﬁcally  Q(s)
and the divergence between Q(so) and P (so) are parameterized with two neural networks  i.e.  one
generator and one discriminator  and then play an adversarial game [12]. However  different from the
original GANs  our model has one extra discriminator (called as reconstructor)  since the ﬁrst term in
Eq. (1) involves s  which will act as the second discriminator during the game.
Concretely  the corresponding implementation of each term in Eq. (1) is illustrated in Figure 3 
consisting of three modules  generator  discriminator and reconstructor. The generator is responsible
for generating a distribution Q(s) of the noise transition matrix s  which serves for both terms in
Eq. (1). The discriminator implements the function of the second term in Eq. (1) to measure the
difference M(so  ˆso) between the extracted structure so with Eq. (2) and our prior structure ˆso. The
reconstructor is for the ﬁrst term in Eq. (1)  facilitating the classiﬁer prediction P (y|x) and the noise
transition matrix s to yield noisy labels ˜y. In this way  we instill the structure information from
human cognition into an end-to-end model.

4 Related literature

Except several works mentioned before  we survey other solutions for noisy labels here. Statistical
learning focuses on theoretical guarantees  consisting of three directions - surrogate losses  noise rate
estimation and probabilistic modeling. For example  in the surrogate losses  Natarajan et al. [31]
proposed an unbiased estimator to provide the noise corrected loss approach. Masnadi-Shirazi et al.

4https://en.wikipedia.org/wiki/Dirac_delta_function;

wiki/Spike-and-slab_variable_selection

https://en.wikipedia.org/

6

Figure 3: A GAN-like structure to model the structure instillation on learning with noisy supervision.

[27] presented a robust non-convex loss  which is the special case in a family of robust losses [16]. In
noise rate estimation  both Menon et al. [28] and Liu et al. [23] proposed a class-probability estimator
using order statistics on the range of scores. Sanderson et al. [38] presented the same estimator using
the slope of the ROC curve. In probabilistic modeling  Raykar et al. [33] proposed a two-coin model
to handle noisy labels from multiple annotators. Yan et al. [47] extended this two-coin model by
setting the dynamic ﬂipping probability associated with samples.
Deep learning acquires better performance due to its complex nonlinearity [19  44  17  49]. For
example  Li et al. proposed a uniﬁed framework to distill the knowledge from clean labels and
knowledge graph [22]  which can be exploited to learn a better model from noisy labels. Veit et al.
trained a label cleaning network by a small set of clean labels  and used this network to reduce the
noise in large-scale noisy labels [43]. Tanaka et al. presented a joint optimization framework to learn
parameters and estimate true labels simultaneously [40]. Ren et al. leveraged an additional validation
set to adaptively assign weights to training examples in every iteration [35]. Rodrigues et al. added
a crowd layer after the output layer for noisy labels from multiple annotators [37]. However  all
methods require extra resources (e.g.  knowledge graph or validation set) or more complex networks.

5 Experiments

In this section  we verify the robustness of MASKING from two folds. First  we conduct experiments
on two benchmark datasets with three types of noise structure: namely (1) column-diagonal (Fig-
ure 1(a)); (2) tri-diagonal (Figure 1(b)); and (3) block-diagonal (Figure 1(c)). Second  we conduct
experiments on one industrial-level dataset with agnostic noise structure.
Benchmark datasets. CIFAR-10 and CIFAR-100 datasets are used. Both datasets consist of 50k
samples for training and 10k samples for testing  where each sample is a 32 × 32 color image and its
label. For CIFAR-10  we randomly ﬂip the labels of the training set according to the ﬁrst two types of
noise structure  which has been illustrated in Figure 1 with predeﬁned transition probabilities. For
CIFAR-100  we implement the similar procedure to generate the noisy data  but follow the last type of
noise structure in Figure 1(c). All predeﬁned transition probabilities are found in Table 2 (4th row).

Industrial-level dataset. An industrial-level dataset called Clothing1M [46] from online shopping
websites (i.e.  Taobao.com) is used here  where the ground-truth transition matrix is not available.
Clothing1M includes mislabeled images of different clothes  such as hoodie  jacket and windbreaker.
This dataset consist of 1000k samples for training and 1k samples for testing  where each sample
is a 256 × 256 color image and its label. Although we cannot know the accurate structure prior for
Clothing1M  we can distill an approximated structure from the pre-estimated transition matrix [46].
We consider the transition probabilities greater than 0.1 as the valid transition patterns  and the
transition probabilities smaller than 0.01 as the invalid transition patterns.

Baselines and measurements. We compare MASKING with the state-of-the-art & the most related
techniques for noisy supervision: (1) forward correction [32] (F-correction) and (2) S-adaptation [10].
We also compare it with directly training deep networks on noisy data (marked as (3) NOISY) and
clean data (marked as (4) CLEAN). The performance of CLEAN can be viewed as an oracle or an
upper bound. The prediction accuracy is used to evaluate the classiﬁcation performance of each
model in the test set. Besides  we qualitatively visualize the noise transition matrix when the training
of each model converges  to analyze whether the true noise transition matrix is approached.

Implementations. All experiments are conducted on a NVIDIA TITAN GPU  and all methods are
implemented by Tensorﬂow. We adopt the same base network as the classiﬁer of all methods  and
apply the cross-entropy loss for noisy labels. The stochastic gradient descent optimizer has been
used to update the parameter of baselines and the classiﬁer in MASKING. For the generator and the

7

ReconstructorGeneratorDiscriminatorˆso<latexit sha1_base64="uK94STgC410u4qfBylHeVeP5KCI=">AAAB8HicbVDLSgNBEOyNrxhfUY9eFoPgKeyKoMegF48RzEOSJcxOJsmQeSwzvUJY8hVePCji1c/x5t84SfagiQUNRVU33V1xIrjFIPj2CmvrG5tbxe3Szu7e/kH58KhpdWooa1AttGnHxDLBFWsgR8HaiWFExoK14vHtzG89MWO5Vg84SVgkyVDxAacEnfTYHRHM7LSne+VKUA3m8FdJmJMK5Kj3yl/dvqapZAqpINZ2wiDBKCMGORVsWuqmliWEjsmQdRxVRDIbZfODp/6ZU/r+QBtXCv25+nsiI9LaiYxdpyQ4ssveTPzP66Q4uI4yrpIUmaKLRYNU+Kj92fd+nxtGUUwcIdRwd6tPR8QQii6jkgshXH55lTQvqmFQDe8vK7WbPI4inMApnEMIV1CDO6hDAyhIeIZXePOM9+K9ex+L1oKXzxzDH3ifPzFCkKY=</latexit><latexit sha1_base64="uK94STgC410u4qfBylHeVeP5KCI=">AAAB8HicbVDLSgNBEOyNrxhfUY9eFoPgKeyKoMegF48RzEOSJcxOJsmQeSwzvUJY8hVePCji1c/x5t84SfagiQUNRVU33V1xIrjFIPj2CmvrG5tbxe3Szu7e/kH58KhpdWooa1AttGnHxDLBFWsgR8HaiWFExoK14vHtzG89MWO5Vg84SVgkyVDxAacEnfTYHRHM7LSne+VKUA3m8FdJmJMK5Kj3yl/dvqapZAqpINZ2wiDBKCMGORVsWuqmliWEjsmQdRxVRDIbZfODp/6ZU/r+QBtXCv25+nsiI9LaiYxdpyQ4ssveTPzP66Q4uI4yrpIUmaKLRYNU+Kj92fd+nxtGUUwcIdRwd6tPR8QQii6jkgshXH55lTQvqmFQDe8vK7WbPI4inMApnEMIV1CDO6hDAyhIeIZXePOM9+K9ex+L1oKXzxzDH3ifPzFCkKY=</latexit><latexit sha1_base64="uK94STgC410u4qfBylHeVeP5KCI=">AAAB8HicbVDLSgNBEOyNrxhfUY9eFoPgKeyKoMegF48RzEOSJcxOJsmQeSwzvUJY8hVePCji1c/x5t84SfagiQUNRVU33V1xIrjFIPj2CmvrG5tbxe3Szu7e/kH58KhpdWooa1AttGnHxDLBFWsgR8HaiWFExoK14vHtzG89MWO5Vg84SVgkyVDxAacEnfTYHRHM7LSne+VKUA3m8FdJmJMK5Kj3yl/dvqapZAqpINZ2wiDBKCMGORVsWuqmliWEjsmQdRxVRDIbZfODp/6ZU/r+QBtXCv25+nsiI9LaiYxdpyQ4ssveTPzP66Q4uI4yrpIUmaKLRYNU+Kj92fd+nxtGUUwcIdRwd6tPR8QQii6jkgshXH55lTQvqmFQDe8vK7WbPI4inMApnEMIV1CDO6hDAyhIeIZXePOM9+K9ex+L1oKXzxzDH3ifPzFCkKY=</latexit><latexit sha1_base64="uK94STgC410u4qfBylHeVeP5KCI=">AAAB8HicbVDLSgNBEOyNrxhfUY9eFoPgKeyKoMegF48RzEOSJcxOJsmQeSwzvUJY8hVePCji1c/x5t84SfagiQUNRVU33V1xIrjFIPj2CmvrG5tbxe3Szu7e/kH58KhpdWooa1AttGnHxDLBFWsgR8HaiWFExoK14vHtzG89MWO5Vg84SVgkyVDxAacEnfTYHRHM7LSne+VKUA3m8FdJmJMK5Kj3yl/dvqapZAqpINZ2wiDBKCMGORVsWuqmliWEjsmQdRxVRDIbZfODp/6ZU/r+QBtXCv25+nsiI9LaiYxdpyQ4ssveTPzP66Q4uI4yrpIUmaKLRYNU+Kj92fd+nxtGUUwcIdRwd6tPR8QQii6jkgshXH55lTQvqmFQDe8vK7WbPI4inMApnEMIV1CDO6hDAyhIeIZXePOM9+K9ex+L1oKXzxzDH3ifPzFCkKY=</latexit>M(so ˆso)<latexit sha1_base64="LHU5za+gXeOl74Q/dOj5daVbyN4=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahgpREBF0W3bgRKtgHNCFMptN26GQmzEyEEgJu/BU3LhRx60+482+ctFlo64ELh3Pu5d57wphRpR3n2yotLa+srpXXKxubW9s79u5eW4lEYtLCggnZDZEijHLS0lQz0o0lQVHISCccX+d+54FIRQW/15OY+BEacjqgGGkjBfaBFyE9woilt1lNBeLUGyGdqiwQJ4FdderOFHCRuAWpggLNwP7y+gInEeEaM6RUz3Vi7adIaooZySpeokiM8BgNSc9QjiKi/HT6QwaPjdKHAyFNcQ2n6u+JFEVKTaLQdOYXq3kvF//zeokeXPop5XGiCcezRYOEQS1gHgjsU0mwZhNDEJbU3ArxCEmEtYmtYkJw519eJO2zuuvU3bvzauOqiKMMDsERqAEXXIAGuAFN0AIYPIJn8ArerCfrxXq3PmatJauY2Qd/YH3+AP6yl7o=</latexit><latexit sha1_base64="LHU5za+gXeOl74Q/dOj5daVbyN4=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahgpREBF0W3bgRKtgHNCFMptN26GQmzEyEEgJu/BU3LhRx60+482+ctFlo64ELh3Pu5d57wphRpR3n2yotLa+srpXXKxubW9s79u5eW4lEYtLCggnZDZEijHLS0lQz0o0lQVHISCccX+d+54FIRQW/15OY+BEacjqgGGkjBfaBFyE9woilt1lNBeLUGyGdqiwQJ4FdderOFHCRuAWpggLNwP7y+gInEeEaM6RUz3Vi7adIaooZySpeokiM8BgNSc9QjiKi/HT6QwaPjdKHAyFNcQ2n6u+JFEVKTaLQdOYXq3kvF//zeokeXPop5XGiCcezRYOEQS1gHgjsU0mwZhNDEJbU3ArxCEmEtYmtYkJw519eJO2zuuvU3bvzauOqiKMMDsERqAEXXIAGuAFN0AIYPIJn8ArerCfrxXq3PmatJauY2Qd/YH3+AP6yl7o=</latexit><latexit sha1_base64="LHU5za+gXeOl74Q/dOj5daVbyN4=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahgpREBF0W3bgRKtgHNCFMptN26GQmzEyEEgJu/BU3LhRx60+482+ctFlo64ELh3Pu5d57wphRpR3n2yotLa+srpXXKxubW9s79u5eW4lEYtLCggnZDZEijHLS0lQz0o0lQVHISCccX+d+54FIRQW/15OY+BEacjqgGGkjBfaBFyE9woilt1lNBeLUGyGdqiwQJ4FdderOFHCRuAWpggLNwP7y+gInEeEaM6RUz3Vi7adIaooZySpeokiM8BgNSc9QjiKi/HT6QwaPjdKHAyFNcQ2n6u+JFEVKTaLQdOYXq3kvF//zeokeXPop5XGiCcezRYOEQS1gHgjsU0mwZhNDEJbU3ArxCEmEtYmtYkJw519eJO2zuuvU3bvzauOqiKMMDsERqAEXXIAGuAFN0AIYPIJn8ArerCfrxXq3PmatJauY2Qd/YH3+AP6yl7o=</latexit><latexit sha1_base64="LHU5za+gXeOl74Q/dOj5daVbyN4=">AAACA3icbVDLSsNAFJ3UV62vqDvdDBahgpREBF0W3bgRKtgHNCFMptN26GQmzEyEEgJu/BU3LhRx60+482+ctFlo64ELh3Pu5d57wphRpR3n2yotLa+srpXXKxubW9s79u5eW4lEYtLCggnZDZEijHLS0lQz0o0lQVHISCccX+d+54FIRQW/15OY+BEacjqgGGkjBfaBFyE9woilt1lNBeLUGyGdqiwQJ4FdderOFHCRuAWpggLNwP7y+gInEeEaM6RUz3Vi7adIaooZySpeokiM8BgNSc9QjiKi/HT6QwaPjdKHAyFNcQ2n6u+JFEVKTaLQdOYXq3kvF//zeokeXPop5XGiCcezRYOEQS1gHgjsU0mwZhNDEJbU3ArxCEmEtYmtYkJw519eJO2zuuvU3bvzauOqiKMMDsERqAEXXIAGuAFN0AIYPIJn8ArerCfrxXq3PmatJauY2Qd/YH3+AP6yl7o=</latexit>f(s)<latexit sha1_base64="Jhpx4XzbS3hrmmDELnPapeR9/3Q=">AAAB63icbVBNSwMxEJ34WetX1aOXYBHqpeyKoMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+0dr6xubWdmmnvLu3f3BYOTpumzjVlLVoLGLdDYlhgivWstwK1k00IzIUrBNO7nK/88S04bF6tNOEBZKMFI84JTaXopq5GFSqXt2bA68SvyBVKNAcVL76w5imkilLBTGm53uJDTKiLaeCzcr91LCE0AkZsZ6jikhmgmx+6wyfO2WIo1i7UhbP1d8TGZHGTGXoOiWxY7Ps5eJ/Xi+10U2QcZWklim6WBSlAtsY54/jIdeMWjF1hFDN3a2Yjokm1Lp4yi4Ef/nlVdK+rPte3X+4qjZuizhKcApnUAMfrqEB99CEFlAYwzO8whuS6AW9o49F6xoqZk7gD9DnD2TGjcw=</latexit><latexit sha1_base64="Jhpx4XzbS3hrmmDELnPapeR9/3Q=">AAAB63icbVBNSwMxEJ34WetX1aOXYBHqpeyKoMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+0dr6xubWdmmnvLu3f3BYOTpumzjVlLVoLGLdDYlhgivWstwK1k00IzIUrBNO7nK/88S04bF6tNOEBZKMFI84JTaXopq5GFSqXt2bA68SvyBVKNAcVL76w5imkilLBTGm53uJDTKiLaeCzcr91LCE0AkZsZ6jikhmgmx+6wyfO2WIo1i7UhbP1d8TGZHGTGXoOiWxY7Ps5eJ/Xi+10U2QcZWklim6WBSlAtsY54/jIdeMWjF1hFDN3a2Yjokm1Lp4yi4Ef/nlVdK+rPte3X+4qjZuizhKcApnUAMfrqEB99CEFlAYwzO8whuS6AW9o49F6xoqZk7gD9DnD2TGjcw=</latexit><latexit sha1_base64="Jhpx4XzbS3hrmmDELnPapeR9/3Q=">AAAB63icbVBNSwMxEJ34WetX1aOXYBHqpeyKoMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+0dr6xubWdmmnvLu3f3BYOTpumzjVlLVoLGLdDYlhgivWstwK1k00IzIUrBNO7nK/88S04bF6tNOEBZKMFI84JTaXopq5GFSqXt2bA68SvyBVKNAcVL76w5imkilLBTGm53uJDTKiLaeCzcr91LCE0AkZsZ6jikhmgmx+6wyfO2WIo1i7UhbP1d8TGZHGTGXoOiWxY7Ps5eJ/Xi+10U2QcZWklim6WBSlAtsY54/jIdeMWjF1hFDN3a2Yjokm1Lp4yi4Ef/nlVdK+rPte3X+4qjZuizhKcApnUAMfrqEB99CEFlAYwzO8whuS6AW9o49F6xoqZk7gD9DnD2TGjcw=</latexit><latexit sha1_base64="Jhpx4XzbS3hrmmDELnPapeR9/3Q=">AAAB63icbVBNSwMxEJ34WetX1aOXYBHqpeyKoMeiF48V7Ae0S8mm2TY0yS5JVihL/4IXD4p49Q9589+YbfegrQ8GHu/NMDMvTAQ31vO+0dr6xubWdmmnvLu3f3BYOTpumzjVlLVoLGLdDYlhgivWstwK1k00IzIUrBNO7nK/88S04bF6tNOEBZKMFI84JTaXopq5GFSqXt2bA68SvyBVKNAcVL76w5imkilLBTGm53uJDTKiLaeCzcr91LCE0AkZsZ6jikhmgmx+6wyfO2WIo1i7UhbP1d8TGZHGTGXoOiWxY7Ps5eJ/Xi+10U2QcZWklim6WBSlAtsY54/jIdeMWjF1hFDN3a2Yjokm1Lp4yi4Ef/nlVdK+rPte3X+4qjZuizhKcApnUAMfrqEB99CEFlAYwzO8whuS6AW9o49F6xoqZk7gD9DnD2TGjcw=</latexit>Q(s)<latexit sha1_base64="9HgpQwPAVd9SIk09rSaUDp/SV+c=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjy2YD+gDWWz3bRLdzdhdyOU0L/gxYMiXv1D3vw3btIctPXBwOO9GWbmBTFn2rjut1Pa2Nza3invVvb2Dw6PqscnXR0litAOiXik+gHWlDNJO4YZTvuxolgEnPaC2X3m956o0iySj2YeU1/giWQhI9hkUruuL0fVmttwc6B14hWkBgVao+rXcByRRFBpCMdaDzw3Nn6KlWGE00VlmGgaYzLDEzqwVGJBtZ/mty7QhVXGKIyULWlQrv6eSLHQei4C2ymwmepVLxP/8waJCW/9lMk4MVSS5aIw4chEKHscjZmixPC5JZgoZm9FZIoVJsbGU7EheKsvr5PuVcNzG177uta8K+IowxmcQx08uIEmPEALOkBgCs/wCm+OcF6cd+dj2VpyiplT+APn8wdEs423</latexit><latexit sha1_base64="9HgpQwPAVd9SIk09rSaUDp/SV+c=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjy2YD+gDWWz3bRLdzdhdyOU0L/gxYMiXv1D3vw3btIctPXBwOO9GWbmBTFn2rjut1Pa2Nza3invVvb2Dw6PqscnXR0litAOiXik+gHWlDNJO4YZTvuxolgEnPaC2X3m956o0iySj2YeU1/giWQhI9hkUruuL0fVmttwc6B14hWkBgVao+rXcByRRFBpCMdaDzw3Nn6KlWGE00VlmGgaYzLDEzqwVGJBtZ/mty7QhVXGKIyULWlQrv6eSLHQei4C2ymwmepVLxP/8waJCW/9lMk4MVSS5aIw4chEKHscjZmixPC5JZgoZm9FZIoVJsbGU7EheKsvr5PuVcNzG177uta8K+IowxmcQx08uIEmPEALOkBgCs/wCm+OcF6cd+dj2VpyiplT+APn8wdEs423</latexit><latexit sha1_base64="9HgpQwPAVd9SIk09rSaUDp/SV+c=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjy2YD+gDWWz3bRLdzdhdyOU0L/gxYMiXv1D3vw3btIctPXBwOO9GWbmBTFn2rjut1Pa2Nza3invVvb2Dw6PqscnXR0litAOiXik+gHWlDNJO4YZTvuxolgEnPaC2X3m956o0iySj2YeU1/giWQhI9hkUruuL0fVmttwc6B14hWkBgVao+rXcByRRFBpCMdaDzw3Nn6KlWGE00VlmGgaYzLDEzqwVGJBtZ/mty7QhVXGKIyULWlQrv6eSLHQei4C2ymwmepVLxP/8waJCW/9lMk4MVSS5aIw4chEKHscjZmixPC5JZgoZm9FZIoVJsbGU7EheKsvr5PuVcNzG177uta8K+IowxmcQx08uIEmPEALOkBgCs/wCm+OcF6cd+dj2VpyiplT+APn8wdEs423</latexit><latexit sha1_base64="9HgpQwPAVd9SIk09rSaUDp/SV+c=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBahXkoigh6LXjy2YD+gDWWz3bRLdzdhdyOU0L/gxYMiXv1D3vw3btIctPXBwOO9GWbmBTFn2rjut1Pa2Nza3invVvb2Dw6PqscnXR0litAOiXik+gHWlDNJO4YZTvuxolgEnPaC2X3m956o0iySj2YeU1/giWQhI9hkUruuL0fVmttwc6B14hWkBgVao+rXcByRRFBpCMdaDzw3Nn6KlWGE00VlmGgaYzLDEzqwVGJBtZ/mty7QhVXGKIyULWlQrv6eSLHQei4C2ymwmepVLxP/8waJCW/9lMk4MVSS5aIw4chEKHscjZmixPC5JZgoZm9FZIoVJsbGU7EheKsvr5PuVcNzG177uta8K+IowxmcQx08uIEmPEALOkBgCs/wCm+OcF6cd+dj2VpyiplT+APn8wdEs423</latexit>P(y|x)<latexit sha1_base64="v8QIkTyxC+jelQXIrdGI2iwwVEk=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRahXsquCHosevFYwX5Au5Rsmm1js8mSZMVl7X/w4kERr/4fb/4b03YP2vpg4PHeDDPzgpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DlpaJIrRJJJeqE2BNORO0aZjhtBMriqOA03Ywvp767QeqNJPizqQx9SM8FCxkBBsrtRrV9OnxtF+uuDV3BrRMvJxUIEejX/7qDSRJIioM4VjrrufGxs+wMoxwOin1Ek1jTMZ4SLuWChxR7WezayfoxCoDFEplSxg0U39PZDjSOo0C2xlhM9KL3lT8z+smJrz0MybixFBB5ovChCMj0fR1NGCKEsNTSzBRzN6KyAgrTIwNqGRD8BZfXiats5rn1rzb80r9Ko+jCEdwDFXw4ALqcAMNaAKBe3iGV3hzpPPivDsf89aCk88cwh84nz8LYI7E</latexit><latexit sha1_base64="v8QIkTyxC+jelQXIrdGI2iwwVEk=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRahXsquCHosevFYwX5Au5Rsmm1js8mSZMVl7X/w4kERr/4fb/4b03YP2vpg4PHeDDPzgpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DlpaJIrRJJJeqE2BNORO0aZjhtBMriqOA03Ywvp767QeqNJPizqQx9SM8FCxkBBsrtRrV9OnxtF+uuDV3BrRMvJxUIEejX/7qDSRJIioM4VjrrufGxs+wMoxwOin1Ek1jTMZ4SLuWChxR7WezayfoxCoDFEplSxg0U39PZDjSOo0C2xlhM9KL3lT8z+smJrz0MybixFBB5ovChCMj0fR1NGCKEsNTSzBRzN6KyAgrTIwNqGRD8BZfXiats5rn1rzb80r9Ko+jCEdwDFXw4ALqcAMNaAKBe3iGV3hzpPPivDsf89aCk88cwh84nz8LYI7E</latexit><latexit sha1_base64="v8QIkTyxC+jelQXIrdGI2iwwVEk=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRahXsquCHosevFYwX5Au5Rsmm1js8mSZMVl7X/w4kERr/4fb/4b03YP2vpg4PHeDDPzgpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DlpaJIrRJJJeqE2BNORO0aZjhtBMriqOA03Ywvp767QeqNJPizqQx9SM8FCxkBBsrtRrV9OnxtF+uuDV3BrRMvJxUIEejX/7qDSRJIioM4VjrrufGxs+wMoxwOin1Ek1jTMZ4SLuWChxR7WezayfoxCoDFEplSxg0U39PZDjSOo0C2xlhM9KL3lT8z+smJrz0MybixFBB5ovChCMj0fR1NGCKEsNTSzBRzN6KyAgrTIwNqGRD8BZfXiats5rn1rzb80r9Ko+jCEdwDFXw4ALqcAMNaAKBe3iGV3hzpPPivDsf89aCk88cwh84nz8LYI7E</latexit><latexit sha1_base64="v8QIkTyxC+jelQXIrdGI2iwwVEk=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRahXsquCHosevFYwX5Au5Rsmm1js8mSZMVl7X/w4kERr/4fb/4b03YP2vpg4PHeDDPzgpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DlpaJIrRJJJeqE2BNORO0aZjhtBMriqOA03Ywvp767QeqNJPizqQx9SM8FCxkBBsrtRrV9OnxtF+uuDV3BrRMvJxUIEejX/7qDSRJIioM4VjrrufGxs+wMoxwOin1Ek1jTMZ4SLuWChxR7WezayfoxCoDFEplSxg0U39PZDjSOo0C2xlhM9KL3lT8z+smJrz0MybixFBB5ovChCMj0fR1NGCKEsNTSzBRzN6KyAgrTIwNqGRD8BZfXiats5rn1rzb80r9Ko+jCEdwDFXw4ALqcAMNaAKBe3iGV3hzpPPivDsf89aCk88cwh84nz8LYI7E</latexit>P(˜y|y s)<latexit sha1_base64="qiyGA1q05pxJKOz8GedXP/Ohy7k=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBahgpREBD0WvXisYD+gDWWz2bRLN5uwOymE2H/ixYMiXv0n3vw3btsctPXBwOO9GWbm+YngGhzn2yqtrW9sbpW3Kzu7e/sH9uFRW8epoqxFYxGrrk80E1yyFnAQrJsoRiJfsI4/vpv5nQlTmsfyEbKEeREZSh5ySsBIA9tu1vrARcDybPqUXejzgV116s4ceJW4BamiAs2B/dUPYppGTAIVROue6yTg5UQBp4JNK/1Us4TQMRmynqGSREx7+fzyKT4zSoDDWJmSgOfq74mcRFpnkW86IwIjvezNxP+8XgrhjZdzmaTAJF0sClOBIcazGHDAFaMgMkMIVdzciumIKELBhFUxIbjLL6+S9mXdderuw1W1cVvEUUYn6BTVkIuuUQPdoyZqIYom6Bm9ojcrt16sd+tj0Vqyiplj9AfW5w82jJNf</latexit><latexit sha1_base64="qiyGA1q05pxJKOz8GedXP/Ohy7k=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBahgpREBD0WvXisYD+gDWWz2bRLN5uwOymE2H/ixYMiXv0n3vw3btsctPXBwOO9GWbm+YngGhzn2yqtrW9sbpW3Kzu7e/sH9uFRW8epoqxFYxGrrk80E1yyFnAQrJsoRiJfsI4/vpv5nQlTmsfyEbKEeREZSh5ySsBIA9tu1vrARcDybPqUXejzgV116s4ceJW4BamiAs2B/dUPYppGTAIVROue6yTg5UQBp4JNK/1Us4TQMRmynqGSREx7+fzyKT4zSoDDWJmSgOfq74mcRFpnkW86IwIjvezNxP+8XgrhjZdzmaTAJF0sClOBIcazGHDAFaMgMkMIVdzciumIKELBhFUxIbjLL6+S9mXdderuw1W1cVvEUUYn6BTVkIuuUQPdoyZqIYom6Bm9ojcrt16sd+tj0Vqyiplj9AfW5w82jJNf</latexit><latexit sha1_base64="qiyGA1q05pxJKOz8GedXP/Ohy7k=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBahgpREBD0WvXisYD+gDWWz2bRLN5uwOymE2H/ixYMiXv0n3vw3btsctPXBwOO9GWbm+YngGhzn2yqtrW9sbpW3Kzu7e/sH9uFRW8epoqxFYxGrrk80E1yyFnAQrJsoRiJfsI4/vpv5nQlTmsfyEbKEeREZSh5ySsBIA9tu1vrARcDybPqUXejzgV116s4ceJW4BamiAs2B/dUPYppGTAIVROue6yTg5UQBp4JNK/1Us4TQMRmynqGSREx7+fzyKT4zSoDDWJmSgOfq74mcRFpnkW86IwIjvezNxP+8XgrhjZdzmaTAJF0sClOBIcazGHDAFaMgMkMIVdzciumIKELBhFUxIbjLL6+S9mXdderuw1W1cVvEUUYn6BTVkIuuUQPdoyZqIYom6Bm9ojcrt16sd+tj0Vqyiplj9AfW5w82jJNf</latexit><latexit sha1_base64="qiyGA1q05pxJKOz8GedXP/Ohy7k=">AAAB+XicbVBNS8NAEN3Ur1q/oh69LBahgpREBD0WvXisYD+gDWWz2bRLN5uwOymE2H/ixYMiXv0n3vw3btsctPXBwOO9GWbm+YngGhzn2yqtrW9sbpW3Kzu7e/sH9uFRW8epoqxFYxGrrk80E1yyFnAQrJsoRiJfsI4/vpv5nQlTmsfyEbKEeREZSh5ySsBIA9tu1vrARcDybPqUXejzgV116s4ceJW4BamiAs2B/dUPYppGTAIVROue6yTg5UQBp4JNK/1Us4TQMRmynqGSREx7+fzyKT4zSoDDWJmSgOfq74mcRFpnkW86IwIjvezNxP+8XgrhjZdzmaTAJF0sClOBIcazGHDAFaMgMkMIVdzciumIKELBhFUxIbjLL6+S9mXdderuw1W1cVvEUUYn6BTVkIuuUQPdoyZqIYom6Bm9ojcrt16sd+tj0Vqyiplj9AfW5w82jJNf</latexit>discriminator  we follow the advice in Gulrajani et al. [15] to choose the RMSProp optimizer. For
both datasets  the batch size is set to 128 for 15 000 iterations. α and β in Eq. (2) are respectively set
0.05 and 0.005. The estimation of the noise transition matrix in F-correction and the initialization of
the adaption layer in S-adaptation follow the strategy in Patrini et al. [32]. Note that we have tried the
original initialization way in Goldberger and Ben-Reuven [10]  but it is not better than the way in [32].
More details about the network architectures and the learning rates are summarized in Appendix B.
Our implementation of MASKING is available at https://github.com/bhanML/Masking.

Empirical results. We train MASKING and baselines (except CLEAN) on the noisy datasets and
validate the performance in the clean test datasets. Figure 4 depicts the test accuracy on benchmark
datasets with three types of noise transition matrix. According to the comparison  we can ﬁnd that
MASKING persistently outperforms F-correction  S-adaptation and NOISY. In terms of tri-diagonal
and block-diagonal cases  it almost achieves the performance comparable to that of CLEAN.
Besides  Table 2 presents the visualization of the noise transition matrix estimated by F-correction 
S-adaptation and MASKING  as well as the true noise transition matrix. As can be seen  with the
guidance of the prior structure  MASKING infers the noise transition matrix better than two baselines.
Speciﬁcally  we observe that for the estimation on the tri-diagonal structure  both F-correction and
S-adaptation severely fail. F-correction pre-estimates a non-ideal matrix  and S-adaptation tunes it
worse  while MASKING learns it better. To avoid the performance drop when directly training on the
noisy dataset as [19  3]  we have conﬁgured the dropout layer in deep neural networks as [3].

(a) Column-diagnoal (CIFAR-10)

(b) Tri-diagnoal (CIFAR-10)

(c) Block-diagnoal (CIFAR-100)

Figure 4: Test accuracy vs iterations on benchmark datasets with three types of noise structure.

Table 1: Test accuracy on Clothing1M with agnostic noise structure.

Models
NOISY
F-correction
S-adaptation
MASKING
CLEAN

Performance(%)

68.9
69.8
70.3
71.1
75.2

Furthermore  test accuracy of all methods on Clothing1M dataset is shown in Table 1. The comparison
denotes that  when the noise model of the training data is completely unknown to all methods 
MASKING still outperforms other methods. Compared to results in Figure 4  the robustness of
MASKING marginally outperforms that of F-correction and S-adaptation. We conjecture two reasons
exist: First  the structure prior we used here is not the ground-truth noise structure. Second  the
ground-truth noise structure of Clothing1M is too complex to be estimated easily. To solve the
estimation issue  we can use crowdsourcing to provide labels in practice  and invite an expert to ﬁnd
the accurate noise structure. This should be much cheaper than letting the expert assign labels.
6 Conclusions

This paper presents a Masking approach. This approach conveys human cognition of invalid class
transitions  and speculates the structure of the noise transition matrix. Given the structure information 

8

0.00.20.40.60.81.01.21.4Iteration×1050.400.500.600.700.800.90Accuracy0.00.20.40.60.81.01.21.4Iteration×1050.400.500.600.700.800.90Accuracy0.00.20.40.60.81.01.21.4Iteration×1050.300.350.400.450.500.550.60AccuracyNOISYF-correctionS-adaptationMASKINGCLEANTable 2: The estimation of the noise transition matrix by F-correction (1st row)  S-adaptation (2nd
row) and MASKING (3rd row)  and the truth (4th row) in the case of three types of noise transition
structure: column-diagonal (1st column)  tri-diagonal (2nd column)  block-diagonal (3rd column).

we derive a structure-aware probabilistic model (MASKING)  which incorporates a structure prior.
Empirical results demonstrate that our approach can improve the robustness of classiﬁers obviously.
In future  we will explore how MASKING self-corrects the incorrect noise structure. Namely  when
the noise structure is wrongly set at the initial stage  how does our model correct the initial structure
by learning from the ﬁnite dataset?

Acknowledgments.

MS was supported by the International Research Center for Neurointelligence (WPI-IRCN) at The
University of Tokyo Institutes for Advanced Study. IWT was supported by ARC FT130100746 
DP180100106 and LP150100671. MZ acknowledges the support of Award IIS-1812699 from the U.S.
National Science Foundation. YZ was supported by the High Technology Research and Development
Program of China (2015AA015801)  NSFC (61521062)  and STCSM (18DZ2270700). BH would
like to thank the ﬁnancial support from RIKEN-AIP. JY would like to thank the ﬁnancial support
SJTU-CMIC and UTS-CAI. We gratefully acknowledge the support of NVIDIA Corporation with
the donation of the Titan Xp GPU used for this research.

9

References
[1] Y. Aït-Sahalia  J. Fan  and D. Xiu. High-frequency covariance estimates with noisy and asynchronous

ﬁnancial data. Journal of the American Statistical Association  105(492):1504–1517  2010.

[2] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning  2(4):343–370  1988.

[3] D. Arpit  S. Jastrz˛ebski  N. Ballas  D. Krueger  E. Bengio  M. Kanwal  T. Maharaj  A. Fischer  A. Courville 

and Y. Bengio. A closer look at memorization in deep networks. In ICML  2017.

[4] S. Azadi  J. Feng  S. Jegelka  and T. Darrell. Auxiliary image regularization for deep cnns with noisy

labels. In ICLR  2016.

[5] M. Belkin  P. Niyogi  and V. Sindhwani. Manifold regularization: A geometric framework for learning
from labeled and unlabeled examples. Journal of Machine Learning Research  7(Nov):2399–2434  2006.

[6] Y. Cha and J. Cho. Social-network analysis using topic models. In SIGIR  2012.

[7] J. Deng  J. Krause  and L. Fei-Fei. Fine-grained crowdsourcing for ﬁne-grained recognition. In CVPR 

2013.

[8] Y. Dgani  H. Greenspan  and J. Goldberger. Training a neural network based on unreliable human annotation

of medical images. In ISBI  2018.

[9] P. Domingos and M. Pazzani. On the optimality of the simple bayesian classiﬁer under zero-one loss.

Machine Learning  29(2-3):103–130  1997.

[10] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR 

2017.

[11] I. Goodfellow  Y. Bengio  and A. Courville. Deep Learning. MIT Press  2016.

[12] I. Goodfellow  J. Pouget-Abadie  M. Mirza  B. Xu  D. Warde-Farley  S. Ozair  A. Courville  and Y. Bengio.

Generative adversarial nets. In NIPS  2014.

[13] N. Goodman. Sense and certainty. The Philosophical Review  pages 160–167  1952.

[14] P. Grigolini  G. Aquino  M. Bologna  M. Lukovi´c  and B. West. A theory of 1/f noise in human cognition.

Physica A: Statistical Mechanics and its Applications  388(19):4192–4204  2009.

[15] I. Gulrajani  F. Ahmed  M. Arjovsky  V. Dumoulin  and A. Courville. Improved training of wasserstein

gans. In NIPS  2017.

[16] B. Han  I. Tsang  and L. Chen. On the convergence of a family of robust losses for stochastic gradient

descent. In ECML-PKDD  2016.

[17] D. Hendrycks  M. Mazeika  D. Wilson  and K. Gimpel. Using trusted data to train deep networks on labels

corrupted by severe noise. In NIPS  2018.

[18] J. Huang  A. Gretton  K. Borgwardt  B. Schölkopf  and A. Smola. Correcting sample selection bias by

unlabeled data. In NIPS  2007.

[19] L. Jiang  Z. Zhou  T. Leung  L. Li  and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very

deep neural networks on corrupted labels. In ICML  2018.

[20] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR  2017.

[21] W. Li  L. Wang  W. Li  E. Agustsson  and L. Van Gool. Webvision database: Visual learning and

understanding from web data. arXiv:1708.02862  2017.

[22] Y. Li  J. Yang  Y. Song  L. Cao  J. Luo  and J. Li. Learning from noisy labels with distillation. In ICCV 

2017.

[23] T. Liu and D. Tao. Classiﬁcation with noisy labels by importance reweighting. IEEE Transactions on

Pattern Analysis and Machine Intelligence  38(3):447–461  2016.

[24] W. Liu  Y. Jiang  J. Luo  and S. Chang. Noise resistant graph ranking for improved web image search. In

CVPR  2011.

[25] X. Ma  Y. Wang  M. Houle  S. Zhou  S. Erfani  S. Xia  S. Wijewickrema  and J. Bailey. Dimensionality-

driven learning with noisy labels. In ICML  2018.

10

[26] E. Malach and S. Shalev-Shwartz. Decoupling" when to update" from" how to update". In NIPS  2017.

[27] H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classiﬁcation:

robustness to outliers  and savageboost. In NIPS  2009.

theory 

[28] A. Menon  B. Van Rooyen  C. Ong  and B. Williamson. Learning from corrupted binary labels via

class-probability estimation. In ICML  2015.

[29] R. Michalski  J. Carbonell  and T. Mitchell. Machine Learning: An Artiﬁcial Intelligence Approach.

Springer Science & Business Media  2013.

[30] T. Miyato  S. Maeda  M. Koyama  and S. Ishii. Virtual adversarial training: A regularization method for

supervised and semi-supervised learning. ICLR  2016.

[31] N. Natarajan  I. Dhillon  P. Ravikumar  and A. Tewari. Learning with noisy labels. In NIPS  2013.

[32] G. Patrini  A. Rozza  A. Menon  R. Nock  and L. Qu. Making deep neural networks robust to label noise:

A loss correction approach. In CVPR  2017.

[33] V. Raykar  S. Yu  L. Zhao  G. Valadez  C. Florin  L. Bogoni  and L. Moy. Learning from crowds. Journal

of Machine Learning Research  11(Apr):1297–1322  2010.

[34] S. Reed  H. Lee  D. Anguelov  C. Szegedy  D. Erhan  and A. Rabinovich. Training deep neural networks

on noisy labels with bootstrapping. In ICLR  2015.

[35] M. Ren  W. Zeng  B. Yang  and R. Urtasun. Learning to reweight examples for robust deep learning. In

ICML  2018.

[36] V. Rockova and K. McAlinn. Dynamic variable selection with Spike-and-Slab process priors.

arXiv:1708.00085  2017.

[37] F. Rodrigues and F. Pereira. Deep learning from crowds. In AAAI  2018.

[38] T. Sanderson and C. Scott. Class proportion estimation with application to multiclass anomaly rejection.

In AISTATS  2014.

[39] S. Sukhbaatar  J. Bruna  M. Paluri  L. Bourdev  and R. Fergus. Training convolutional networks with noisy

labels. In ICLR workshop  2015.

[40] D. Tanaka  D. Ikami  T. Yamasaki  and K. Aizawa. Joint optimization framework for learning with noisy

labels. In CVPR  2018.

[41] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets

improve semi-supervised deep learning results. In NIPS  2017.

[42] D. Tran  R. Ranganath  and D. Blei. Hierarchical implicit models and likelihood-free variational inference.

In NIPS  2017.

[43] A. Veit  N. Alldrin  G. Chechik  I. Krasin  A. Gupta  and S. Belongie. Learning from noisy large-scale

datasets with minimal supervision. In CVPR  2017.

[44] Y. Wang  W. Liu  X. Ma  J. Bailey  H. Zha  L. Song  and S. Xia. Iterative learning with open-set noisy

labels. In CVPR  2018.

[45] P. Welinder  S. Branson  P. Perona  and S. Belongie. The multidimensional wisdom of crowds. In NIPS 

2010.

[46] T. Xiao  T. Xia  Y. Yang  C. Huang  and X. Wang. Learning from massive noisy labeled data for image

classiﬁcation. In CVPR  2015.

[47] Y. Yan  R. Rosales  G. Fung  R. Subramanian  and J. Dy. Learning from multiple annotators with varying

expertise. Machine Learning  95(3):291–327  2014.

[48] C. Zhang  S. Bengio  M. Hardt  B. Recht  and O. Vinyals. Understanding deep learning requires rethinking

generalization. In ICLR  2017.

[49] Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy

labels. In NIPS  2018.

11

,Bo Han
Jiangchao Yao
Gang Niu
Mingyuan Zhou
Ivor Tsang
Ya Zhang
Masashi Sugiyama
Gabriele Farina
Chun Kai Ling
Fei Fang
Tuomas Sandholm