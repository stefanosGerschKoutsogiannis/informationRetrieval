2015,Practical and Optimal LSH for Angular Distance,We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal  running time exponent. Unlike earlier algorithms with this property (e.g.  Spherical LSH (Andoni-Indyk-Nguyen-Razenshteyn 2014) (Andoni-Razenshteyn 2015))  our algorithm is also practical  improving upon the well-studied hyperplane LSH (Charikar 2002) in practice. We also introduce a multiprobe version of this algorithm and conduct an experimental evaluation on real and synthetic data sets.We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.,Practical and Optimal LSH for Angular Distance

Alexandr Andoni∗
Columbia University

Piotr Indyk

MIT

Thijs Laarhoven
TU Eindhoven

Ilya Razenshteyn

MIT

Ludwig Schmidt

MIT

Abstract

We show the existence of a Locality-Sensitive Hashing (LSH) family for the angu-
lar distance that yields an approximate Near Neighbor Search algorithm with the
asymptotically optimal running time exponent. Unlike earlier algorithms with this
property (e.g.  Spherical LSH [1  2])  our algorithm is also practical  improving
upon the well-studied hyperplane LSH [3] in practice. We also introduce a mul-
tiprobe version of this algorithm and conduct an experimental evaluation on real
and synthetic data sets.
We complement the above positive results with a ﬁne-grained lower bound for the
quality of any LSH family for angular distance. Our lower bound implies that the
above LSH family exhibits a trade-off between evaluation time and quality that is
close to optimal for a natural class of LSH functions.

1

Introduction

Nearest neighbor search is a key algorithmic problem with applications in several ﬁelds including
computer vision  information retrieval  and machine learning [4]. Given a set of n points P ⊂ Rd 
the goal is to build a data structure that answers nearest neighbor queries efﬁciently: for a given query
point q ∈ Rd  ﬁnd the point p ∈ P that is closest to q under an appropriately chosen distance metric.
The main algorithmic design goals are usually a fast query time  a small memory footprint  and—in
the approximate setting—a good quality of the returned solution.
There is a wide range of algorithms for nearest neighbor search based on techniques such as space
partitioning with indexing  as well as dimension reduction or sketching [5]. A popular method for
point sets in high-dimensional spaces is Locality-Sensitive Hashing (LSH) [6  3]  an approach that
offers a provably sub-linear query time and sub-quadratic space complexity  and has been shown
to achieve good empirical performance in a variety of applications [4]. The method relies on the
notion of locality-sensitive hash functions. Intuitively  a hash function is locality-sensitive if its
probability of collision is higher for “nearby” points than for points that are “far apart”. More formally 
two points are nearby if their distance is at most r1  and they are far apart if their distance is at
least r2 = c · r1  where c > 1 quantiﬁes the gap between “near” and “far”. The quality of a
hash function is characterized by two key parameters: p1 is the collision probability for nearby
points  and p2 is the collision probability for points that are far apart. The gap between p1 and p2
determines how “sensitive” the hash function is to changes in distance  and this property is captured
by the parameter ρ = log 1/p1
  which can usually be expressed as a function of the distance gap c.
log 1/p2
The problem of designing good locality-sensitive hash functions and LSH-based efﬁcient nearest
neighbor search algorithms has attracted signiﬁcant attention over the last few years.

∗The authors are listed in alphabetical order.

1

In this paper  we focus on LSH for the Euclidean distance on the unit sphere  which is an important
special case for several reasons. First  the spherical case is relevant in practice: Euclidean distance
on a sphere corresponds to the angular distance or cosine similarity  which are commonly used in
applications such as comparing image feature vectors [7]  speaker representations [8]  and tf-idf data
sets [9]. Moreover  on the theoretical side  the paper [2] shows a reduction from Nearest Neighbor
Search in the entire Euclidean space to the spherical case. These connections lead to a natural
question: what are good LSH families for this special case?
On the theoretical side  the recent work of [1  2] gives the best known provable guarantees for LSH-
based nearest neighbor search w.r.t. the Euclidean distance on the unit sphere. Speciﬁcally  their
2c2−1 .1 E.g.  for the
algorithm has a query time of O(nρ) and space complexity of O(n1+ρ) for ρ = 1
approximation factor c = 2  the algorithm achieves a query time of n1/7+o(1). At the heart of the
algorithm is an LSH scheme called Spherical LSH  which works for unit vectors. Its key property
is that it can distinguish between distances r1 = √2/c and r2 = √2 with probabilities yielding
2c2−1 (the formula for the full range of distances is more complex and given in Section 3).
ρ = 1
Unfortunately  the scheme as described in the paper is not applicable in practice as it is based on
rather complex hash functions that are very time consuming to evaluate. E.g.  simply evaluating
a single hash function from [2] can take more time than a linear scan over 106 points. Since an LSH
data structure contains many individual hash functions  using their scheme would be slower than
a simple linear scan over all points in P unless the number of points n is extremely large.
On the practical side  the hyperplane LSH introduced in the inﬂuential work of Charikar [3] has worse
theoretical guarantees  but works well in practice. Since the hyperplane LSH can be implemented
very efﬁciently  it is the standard hash function in practical LSH-based nearest neighbor algorithms2
and the resulting implementations has been shown to improve over a linear scan on real data by
multiple orders of magnitude [14  9].
The aforementioned discrepancy between the theory and practice of LSH raises an important ques-
tion: is there a locality-sensitive hash function with optimal guarantees that also improves over the
hyperplane LSH in practice?
In this paper we show that there is a family of locality-sensitive hash functions that achieves both
objectives. Speciﬁcally  the hash functions match the theoretical guarantee of Spherical LSH from [2]
and  when combined with additional techniques  give better experimental results than the hyperplane
LSH. More speciﬁcally  our contributions are:

Theoretical guarantees for the cross-polytope LSH. We show that a hash function based on
randomly rotated cross-polytopes (i.e.  unit balls of the (cid:96)1-norm) achieves the same parameter ρ as
the Spherical LSH scheme in [2]  assuming data points are unit vectors. While the cross-polytope
LSH family has been proposed by researchers before [15  16] we give the ﬁrst theoretical analysis of
its performance.

Fine-grained lower bound for cosine similarity LSH. To highlight the difﬁculty of obtaining
optimal and practical LSH schemes  we prove the ﬁrst non-asymptotic lower bound on the trade-off
between the collision probabilities p1 and p2. So far  the optimal LSH upper bound ρ = 1
2c2−1 (from
[1  2] and cross-polytope from here) attain this bound only in the limit  as p1  p2 → 0. Very small p1
and p2 are undesirable since the hash evaluation time is often proportional to 1/p2. Our lower bound
proves this is unavoidable: if we require p2 to be large  ρ has to be suboptimal.
This result has two important implications for designing practical hash functions. First  it shows that
the trade-offs achieved by the cross-polytope LSH and the scheme of [1  2] are essentially optimal.
Second  the lower bound guides design of future LSH functions: if one is to signiﬁcantly improve
upon the cross-polytope LSH  one has to design a hash function that is computed more efﬁciently
than by explicitly enumerating its range (see Section 4 for a more detailed discussion).

Multiprobe scheme for the cross-polytope LSH. The space complexity of an LSH data structure
is sub-quadratic  but even this is often too large (i.e.  strongly super-linear in the number of points) 

1This running time is known to be essentially optimal for a large class of algorithms [10  11].
2Note that if the data points are binary  more efﬁcient LSH schemes exist [12  13]. However  in this paper

we consider algorithms for general (non-binary) vectors.

2

and several methods have been proposed to address this issue. Empirically  the most efﬁcient scheme
is multiprobe LSH [14]  which leads to a signiﬁcantly reduced memory footprint for the hyperplane
LSH. In order to make the cross-polytope LSH competitive in practice with the multiprobe hyperplane
LSH  we propose a novel multiprobe scheme for the cross-polytope LSH.
We complement these contributions with an experimental evaluation on both real and synthetic
data (SIFT vectors  tf-idf data  and a random point set). In order to make the cross-polytope LSH
practical  we combine it with fast pseudo-random rotations [17] via the Fast Hadamard Transform 
and feature hashing [18] to exploit sparsity of data. Our results show that for data sets with around 105
to 108 points  our multiprobe variant of the cross-polytope LSH is up to 10× faster than an efﬁcient
implementation of the hyperplane LSH  and up to 700× faster than a linear scan. To the best of
our knowledge  our combination of techniques provides the ﬁrst “exponent-optimal” algorithm that
empirically improves over the hyperplane LSH in terms of query time for an exact nearest neighbor
search.

1.1 Related work

The cross-polytope LSH functions were originally proposed in [15]. However  the analysis in that
paper was mostly experimental. Speciﬁcally  the probabilities p1 and p2 of the proposed LSH func-
tions were estimated empirically using the Monte Carlo method. Similar hash functions were later
proposed in [16]. The latter paper also uses DFT to speed-up the random matrix-vector matrix multi-
plication operation. Both of the aforementioned papers consider only the single-probe algorithm.
There are several works that show lower bounds on the quality of LSH hash functions [19  10  20  11].
However  those papers provide only a lower bound on the ρ parameter for asymptotic values of p1
and p2  as opposed to an actual trade-off between these two quantities. In this paper we provide such
a trade-off  with implications as outlined in the introduction.

2 Preliminaries
We use (cid:107).(cid:107) to denote the Euclidean (a.k.a. (cid:96)2) norm on Rd. We also use Sd−1 to denote the unit
sphere in Rd centered in the origin. The Gaussian distribution with mean zero and variance of one is
denoted by N (0  1). Let µ be a normalized Haar measure on Sd−1 (that is  µ(Sd−1) = 1). Note that
µ it corresponds to the uniform distribution over Sd−1. We also let u ∼ Sd−1 be a point sampled
from Sd−1 uniformly at random. For η ∈ R we denote
[X ≥ η] =

(cid:90) ∞

−t2/2 dt.

1
√2π

Φc(η) =

Pr

X∼N (0 1)

e

η

We will be interested in the Near Neighbor Search on the sphere Sd−1 with respect to the Euclidean
distance. Note that the angular distance can be expressed via the Euclidean distance between normal-
ized vectors  so our results apply to the angular distance as well.
Deﬁnition 1. Given an n-point dataset P ⊂ Sd−1 on the sphere  the goal of the (c  r)-Approximate
Near Neighbor problem (ANN) is to build a data structure that  given a query q ∈ Sd−1 with the
promise that there exists a datapoint p ∈ P with (cid:107)p − q(cid:107) ≤ r  reports a datapoint p(cid:48)
∈ P within
distance cr from q.
Deﬁnition 2. We say that a hash family H on the sphere Sd−1 is (r1  r2  p1  p2)-sensitive  if for
every p  q ∈ Sd−1 one has Pr
h∼H[h(x) = h(y)] ≤ p2 if
(cid:107)x − y(cid:107) ≥ r2 
It is known [6] that an efﬁcient (r  cr  p1  p2)-sensitive hash family implies a data structure for (c  r)-
ANN with space O(n1+ρ/p1 + dn) and query time O(d · nρ/p1)  where ρ = log(1/p1)
log(1/p2).
3 Cross-polytope LSH

h∼H[h(x) = h(y)] ≥ p1 if (cid:107)x − y(cid:107) ≤ r1  and Pr

In this section  we describe the cross-polytope LSH  analyze it  and show how to make it practical.
First  we recall the deﬁnition of the cross-polytope LSH [15]: Consider the following hash family

3

H for points on a unit sphere Sd−1 ⊂ Rd. Let A ∈ Rd×d be a random matrix with i.i.d. Gaussian
entries (“a random rotation”). To hash a point x ∈ Sd−1  we compute y = Ax/(cid:107)Ax(cid:107) ∈ Sd−1 and
then ﬁnd the point closest to y from {±ei}1≤i≤d  where ei is the i-th standard basis vector of Rd.
We use the closest neighbor as a hash of x.
The following theorem bounds the collision probability for two points under the above family H.
Theorem 1. Suppose that p  q ∈ Sd−1 are such that (cid:107)p − q(cid:107) = τ  where 0 < τ < 2. Then 

(cid:2)h(p) = h(q)(cid:3) =

1

ln

Pr
h∼H

τ 2

4 − τ 2 · ln d + Oτ (ln ln d) .

Before we show how to prove this theorem  we brieﬂy describe its implications. Theorem 1 shows
that the cross-polytope LSH achieves essentially the same bounds on the collision probabilities as the
(theoretically) optimal LSH for the sphere from [2] (see Section “Spherical LSH” there). In particular 
substituting the bounds from Theorem 1 for the cross-polytope LSH into the standard reduction from
Near Neighbor Search to LSH [6]  we obtain the following data structure with sub-quadratic space
and sublinear query time for Near Neighbor Search on a sphere.
Corollary 1. The (c  r)-ANN on a unit sphere Sd−1 can be solved in space O(n1+ρ + dn) and query
time O(d · nρ)  where ρ = 1
We now outline the proof of Theorem 1. For the full proof  see Appendix B.
Due to the spherical symmetry of Gaussians  we can assume that p = e1 and q = αe1 + βe2  where
α  β are such that α2 + β2 = 1 and (α − 1)2 + β2 = τ 2. Then  we expand the collision probability:
h∼H[h(p) = h(q)] = 2d · Pr
Pr

h∼H[h(p) = h(q) = e1]

c2 · 4−c2r2

4−r2 + o(1) .

(cid:20)

= 2d ·

Pr

u v∼N (0 1)d

= 2d · E

X1 Y1

Pr
X2 Y2

(cid:104)
[∀i |ui| ≤ u1 and |αui + βvi| ≤ αu1 + βv1]
|X2| ≤ X1 and |αX2 + βY2| ≤ αX1 + βY1

(cid:105)d−1(cid:21)

 

(1)

where X1  Y1  X2  Y2 ∼ N (0  1). Indeed  the ﬁrst step is due to the spherical symmetry of the hash
family  the second step follows from the above discussion about replacing a random orthogonal
matrix with a Gaussian one and that one can assume w.l.o.g. that p = e1 and q = αe1 + βe2; the last
step is due to the independence of the entries of u and v.
Thus  proving Theorem 1 reduces to estimating the right-hand side of (1). Note that the probability
Pr[|X2| ≤ X1 and |αX2 +βY2| ≤ αX1 +βY1] is equal to the Gaussian area of the planar set SX1 Y1
shown in Figure 1a. The latter is heuristically equal to 1 − e−∆2/2  where ∆ is the distance from
the origin to the complement of SX1 Y1  which is easy to compute (see Appendix A for the precise
statement of this argument). Using this estimate  we compute (1) by taking the outer expectation.

3.1 Making the cross-polytope LSH practical

As described above  the cross-polytope LSH is not quite practical. The main bottleneck is sampling 
storing  and applying a random rotation. In particular  to multiply a random Gaussian matrix with a
vector  we need time proportional to d2  which is infeasible for large d.

Pseudo-random rotations. To rectify this issue  we instead use pseudo-random rotations. Instead
of multiplying an input vector x by a random Gaussian matrix  we apply the following linear trans-
formation: x (cid:55)→ HD3HD2HD1x  where H is the Hadamard transform  and Di for i ∈ {1  2  3}
is a random diagonal ±1-matrix. Clearly  this is an orthogonal transformation  which one can store
in space O(d) and evaluate in time O(d log d) using the Fast Hadamard Transform. This is simi-
lar to pseudo-random rotations used in the context of LSH [21]  dimensionality reduction [17]  or
compressed sensing [22]. While we are currently not aware how to prove rigorously that such pseudo-
random rotations perform as well as the fully random ones  empirical evaluations show that three
applications of HDi are exactly equivalent to applying a true random rotation (when d tends to
inﬁnity). We note that only two applications of HDi are not sufﬁcient.

4

Figure 1

ρ
y
t
i
v
i
t
i
s
n
e
S

0.4

0.35

0.3

0.25

0.2

0.15

100

Cross-polytope LSH
Lower bound

108

104
Number of parts T

1012

1016

(a) The set appearing in the analysis of the cross-
polytope LSH: SX1Y1 = {|x| ≤ X1 and |αx +
βy| ≤ αX1 + βY1}.

√

√
(b) Trade-off between ρ and the number of parts
for distances
2 (approximation c =
2); both bounds tend to 1/7 (see discussion in Sec-
tion 4).

2/2 and

Feature hashing. While we can apply a pseudo-random rotation in time O(d log d)  even this
can be too slow. E.g.  consider an input vector x that is sparse: the number of non-zero entries of
x is s much smaller than d. In this case  we can evaluate the hyperplane LSH from [3] in time
O(s)  while computing the cross-polytope LSH (even with pseudo-random rotations) still takes time
O(d log d). To speed-up the cross-polytope LSH for sparse vectors  we apply feature hashing [18]:
before performing a pseudo-random rotation  we reduce the dimension from d to d(cid:48)
(cid:28) d by applying
a linear map x (cid:55)→ Sx  where S is a random sparse d(cid:48)
× d matrix  whose columns have one non-zero
±1 entry sampled uniformly. This way  the evaluation time becomes O(s + d(cid:48) log d(cid:48)). 3
“Partial” cross-polytope LSH.
In the above discussion  we deﬁned the cross-polytope LSH as a
hash family that returns the closest neighbor among {±ei}1≤i≤d as a hash (after a (pseudo-)random
rotation). In principle  we do not have to consider all d basis vectors when computing the closest
neighbor. By restricting the hash to d(cid:48)
≤ d basis vectors instead  Theorem 1 still holds for the
new hash family (with d replaced by d(cid:48)) since the analysis is essentially dimension-free. This slight
generalization of the cross-polytope LSH turns out to be useful for experiments (see Section 6). Note
that the case d(cid:48) = 1 corresponds to the hyperplane LSH.

4 Lower bound
Let H be a hash family on Sd−1. For 0 < r1 < r2 < 2 we would like to understand the trade-off
between p1 and p2  where p1 is the smallest probability of collision under H for points at distance
at most r1 and p2 is the largest probability of collision for points at distance at least r2. We focus
on the case r2 ≈ √2 because setting r2 to √2 − o(1) (as d tends to inﬁnity) allows us to replace p2

with the following quantity that is somewhat easier to handle:

∗
2 = Pr
p
h∼H

u v∼Sd−1

[h(u) = h(v)].

This quantity is at most p2 + o(1)  since the distance between two random points on a unit sphere
Sd−1 is tightly concentrated around √2. So for a hash family H on a unit sphere Sd−1  we would
like to understand the upper bound on p1 in terms of p∗
(cid:34)
(cid:18)
For 0 ≤ τ ≤ √2 and η ∈ R  we deﬁne
X ≥ η and

2 and 0 < r1 < √2.
(cid:114)

τ 4
4 · Y ≥ η

(cid:35)(cid:46)

[X ≥ η] .

X Y ∼N (0 1)

Pr

X∼N (0 1)

Λ(τ  η) =

Pr

· X +

τ 2 −

(cid:19)

τ 2
2

1 −

3Note that one can apply Lemma 2 from the arXiv version of [18] to claim that—after such a dimension
reduction—the distance between any two points remains sufﬁciently concentrated for the bounds from Theo-
rem 1 to still hold (with d replaced by d(cid:48)).

5

x=−X1x=X1αx+βy=αX1+βY1αx+βy=−(αX1+βY1)We are now ready to formulate the main result of this section.
Theorem 2. Let H be a hash family on Sd−1 such that every function in H partitions the sphere into
at most T parts of measure at most 1/2. Then we have p1 ≤ Λ(r1  η) + o(1)  where η ∈ R is such
that Φc(η) = p∗
2 and o(1) is a quantity that depends on T and r1 and tends to 0 as d tends to inﬁnity.

T (indeed  p∗

The idea of the proof is ﬁrst to reason about one part of the partition using the isoperimetric inequality
from [23]  and then to apply a certain averaging argument by proving concavity of a function related
to Λ using a delicate analytic argument. For the full proof  see Appendix C.
We note that the above requirement of all parts induced by H having measure at most 1/2 is only a
technicality. We conjecture that Theorem 2 holds without this restriction. In any case  as we will see
below  in the interesting range of parameters this restriction is essentially irrelevant.
One can observe that if every hash function in H partitions the sphere into at most T parts  then
p∗
2 is precisely the average sum of squares of measures of the parts). This observation 
2 ≥ 1
combined with Theorem 2  leads to the following interesting consequence. Speciﬁcally  we can
numerically estimate Λ in order to give a lower bound on ρ = log(1/p1)
log(1/p2) for any hash family H in
which every function induces at most T parts of measure at most 1/2. See Figure 1b  where we plot
this lower bound for r1 = √2/2 4 together with an upper bound that is given by the cross-polytope
LSH5 (for which we use numerical estimates for (1)). We can make several conclusions from this
plot. First  the cross-polytope LSH gives an almost optimal trade-off between ρ and T . Given that the
evaluation time for the cross-polytope LSH is O(T log T ) (if one uses pseudo-random rotations)  we
conclude that in order to improve upon the cross-polytope LSH substantially in practice  one should
design an LSH family with ρ being close to optimal and evaluation time that is sublinear in T . We
note that none of the known LSH families for a sphere has been shown to have this property. This
direction looks especially interesting since the convergence of ρ to the optimal value (as T tends to

inﬁnity) is extremely slow (for instance  according to Figure 1b  for r1 = √2/2 and r2 ≈ √2 we
need more than 105 parts to achieve ρ ≤ 0.2  whereas the optimal ρ is 1/7 ≈ 0.143).
5 Multiprobe LSH for the cross-polytope LSH

We now describe our multiprobe scheme for the cross-polytope LSH  which is a method for reducing
the number of independent hash tables in an LSH data structure. Given a query point q  a “standard”
LSH data structure considers only a single cell in each of the L hash tables (the cell is given by the
hash value hi(q) for i ∈ [L]). In multiprobe LSH  we consider candidates from multiple cells in each
table [14]. The rationale is the following: points p that are close to q but fail to collide with q under
hash function hi are still likely to hash to a value that is close to hi(q). By probing multiple hash
locations close to hi(q) in the same table  multiprobe LSH achieves a given probability of success
with a smaller number of hash tables than “standard” LSH. Multiprobe LSH has been shown to
perform well in practice [14  24].
The main ingredient in multiprobe LSH is a probing scheme for generating and ranking possible
modiﬁcations of the hash value hi(q). The probing scheme should be computationally efﬁcient and
ensure that more likely hash locations are probed ﬁrst. For a single cross-polytope hash  the order of
alternative hash values is straightforward: let x be the (pseudo-)randomly rotated version of query
point q. Recall that the “main” hash value is hi(q) = arg maxj∈[d] |xj|.6 Then it is easy to see
that the second highest probability of collision is achieved for the hash value corresponding to the
coordinate with the second largest absolute value  etc. Therefore  we consider the indices i ∈ [d]
sorted by their absolute value as our probing sequence or “ranking” for a single cross-polytope.
The remaining question is how to combine multiple cross-polytope rankings when we have more
than one hash function. As in the analysis of the cross-polytope LSH (see Section 3  we consider
two points q = e1 and p = αe1 + βe2 at distance R. Let A(i) be the i.i.d. Gaussian matrix of hash

4The situation is qualitatively similar for other values of r1.
5More speciﬁcally  for the “partial” version from Section 3.1  since T should be constant  while d grows
6In order to simplify notation  we consider a slightly modiﬁed version of the cross-polytope LSH that maps
both the standard basis vector +ej and its opposite −ej to the same hash value. It is easy to extend the multiprobe
scheme deﬁned here to the “full” cross-polytope LSH from Section 3.

6

function hi  and let x(i) = A(i)e1 be the randomly rotated version of point q. Given x(i)  we are
interested in the probability of p hashing to a certain combination of the individual cross-polytope
rankings. More formally  let r(i)
vi be the index of the vi-th largest element of |x(i)|  where v ∈ [d]k
speciﬁes the alternative probing location. Then we would like to compute

(cid:12)(cid:12) = r(i)

vi

(cid:12)(cid:12)(cid:12) A(i)e1 = x(i)(cid:105)

.

Pr

A(1) ... A(k)

=

vi for all i ∈ [k] | A(i)q = x(i)(cid:3)
(cid:2)hi(p) = r(i)
(cid:104)
k(cid:89)
(cid:12)(cid:12)(α · A(i)e1 + β · A(i)e2)j
(cid:12)(cid:12)(cid:12) Ae1 = x
(cid:104)

(cid:12)(cid:12) = v

arg max

Pr
A(i)

j∈[d]

(cid:105)

Pr

i=1

=

.

Pr
A

j∈[d]

arg max

arg max

y∼N (0 Id)

β
α · y)j

(cid:12)(cid:12)(x +
(cid:12)(cid:12)(x + β

(cid:12)(cid:12)(αx + β · Ae2)j

If we knew this probability for all v ∈ [d]k  we could sort the probing locations by their probability.
We now show how to approximate this probability efﬁciently for a single value of i (and hence drop
(cid:104)
(cid:105)
(cid:12)(cid:12) = v
the superscripts to simplify notation). WLOG  we permute the rows of A so that rv = v and get
(cid:12)(cid:12) = v}. Similar
j∈[d]
The RHS is the Gaussian measure of the set S = {y ∈ Rd | arg maxj∈[d]
to the analysis of the cross-polytope LSH  we approximate the measure of S by its distance to the
origin. Then the probability of probing location v is proportional to exp(−(cid:107)yx v(cid:107)2)  where yx v is the
shortest vector y such that arg maxj |x+y|j = v. Note that the factor β/α becomes a proportionality
constant  and hence the probing scheme does not require to know the distance R. For computational
performance and simplicity  we make a further approximation and use yx v = (maxi |xi|−|xv|)· ev 
i.e.  we only consider modifying a single coordinate to reach the set S.
Once we have estimated the probabilities for each vi ∈ [d]  we incrementally construct the probing
sequence using a binary heap  similar to the approach in [14]. For a probing sequence of length m 
the resulting algorithm has running time O(L· d log d + m log m). In our experiments  we found that
the O(L· d log d) time taken to sort the probing candidates vi dominated the running time of the hash
function evaluation. In order to circumvent this issue  we use an incremental sorting approach that
only sorts the relevant parts of each cross-polytope and gives a running time of O(L · d + m log m).
6 Experiments

α y)j

We now show that the cross-polytope LSH  combined with our multiprobe extension  leads to an
algorithm that is also efﬁcient in practice and improves over the hyperplane LSH on several data sets.
The focus of our experiments is the query time for an exact nearest neighbor search. Since hyperplane
LSH has been compared to other nearest-neighbor algorithms before [8]  we limit our attention to
the relative speed-up compared with hyperplane hashing.
We evaluate the two hashing schemes on three types of data sets. We use a synthetic data set of
randomly generated points because this allows us to vary a single problem parameter while keeping
the remaining parameters constant. We also investigate the performance of our algorithm on real
data: two tf-idf data sets [25] and a set of SIFT feature vectors [7]. We have chosen these data sets in
order to illustrate when the cross-polytope LSH gives large improvements over the hyperplane LSH 
and when the improvements are more modest. See Appendix D for a more detailed description of
the data sets and our experimental setup (implementation details  CPU  etc.).
In all experiments  we set the algorithm parameters so that the empirical probability of successfully
ﬁnding the exact nearest neighbor is at least 0.9. Moreover  we set the number of LSH tables L
so that the amount of additional memory occupied by the LSH data structure is comparable to the
amount of memory necessary for storing the data set. We believe that this is the most interesting
regime because signiﬁcant memory overheads are often impossible for large data sets. In order to
determine the parameters that are not ﬁxed by the above constraints  we perform a grid search over
the remaining parameter space and report the best combination of parameters. For the cross-polytope
hash  we consider “partial” cross-polytopes in the last of the k hash functions in order to get a smooth
trade-off between the various parameters (see Section 3.1).

Multiprobe experiments.
In order to demonstrate that the multiprobe scheme is critical for making
the cross-polytope LSH competitive with hyperplane hashing  we compare the performance of a

7

Data set Method

NYT
NYT
pubmed
pubmed
SIFT
SIFT

HP
CP
HP
CP
HP
CP

Query

time (ms)
120 ms
35 ms
857 ms
213 ms
3.7 ms
3.1 ms

Speed-up

vs HP

3.4×

4.0×

1.2×

Best k

19

2 (64)

20

2 (512)

30
6 (1)

Number of
candidates

Hashing
time (ms)

Distances
time (ms)

57 200
17 900

1 481 000
304 000
18 600
13 400

16
3.0
36
18
0.2
0.6

96
30
762
168
3.0
2.2

Table 1: Average running times for a single nearest neighbor query with the hyperplane (HP) and
cross-polytope (CP) algorithms on three real data sets. The cross-polytope LSH is faster than the
hyperplane LSH on all data sets  with signiﬁcant speed-ups for the two tf-idf data sets NYT and
pubmed. For the cross-polytope LSH  the entries for k include both the number of individual hash
functions per table and (in parenthesis) the dimension of the last of the k cross-polytopes.

“standard” cross-polytope LSH data structure with our multiprobe variant on an instance of the random
data set (n = 220  d = 128). As can be seen in Table 2 (Appendix D)  the multiprobe variant is about
13× faster in our memory-constrained setting (L = 10). Note that in all of the following experiments 
the speed-up of the multiprobe cross-polytope LSH compared to the multiprobe hyperplane LSH is
less than 11×. Hence without our multiprobe addition  the cross-polytope LSH would be slower than
the hyperplane LSH  for which a multiprobe scheme is already known [14].

Experiments on random data. Next  we show that the better time complexity of the cross-
polytope LSH already applies for moderate values of n. In particular  we compare the cross-polytope
LSH  combined with fast rotations (Section 3.1) and our multiprobe scheme  to a multi-probe hyper-
plane LSH on random data. We keep the dimension d = 128 and the distance to the nearest neighbor
R = √2/2 ﬁxed  and vary the size of the data set from 220 to 228. The number of hash tables L is set
to 10. For 220 points  the cross-polytope LSH is already 3.5× faster than the hyperplane LSH  and for
n = 228 the speedup is 10.3× (see Table 3 in Appendix D). Compared to a linear scan  the speed-up
achieved by the cross-polytope LSH ranges from 76× for n = 220 to about 700× for n = 228.
Experiments on real data. On the SIFT data set (n = 106 and d = 128)  the cross-polytope
LSH achieves a modest speed-up of 1.2× compared to the hyperplane LSH (see Table 1). On the
other hand  the speed-up is is 3 − 4× on the two tf-idf data sets  which is a signiﬁcant improvement
considering the relatively small size of the NYT data set (n ≈ 300  000). One important difference
between the data sets is that the typical distance to the nearest neighbor is smaller in the SIFT data
set  which can make the nearest neighbor problem easier (see Appendix D). Since the tf-idf data sets
are very high-dimensional but sparse (d ≈ 100  000)  we use the feature hashing approach described
in Section 3.1 in order to reduce the hashing time of the cross-polytope LSH (the standard hyperplane
LSH already runs in time proportional to the sparsity of a vector). We use 1024 and 2048 as feature
hashing dimensions for NYT and pubmed  respectively.

Acknowledgments

We thank Michael Kapralov for many valuable discussions during various stages of this work. We
also thank Stefanie Jegelka and Rasmus Pagh for helpful conversations. This work was supported
in part by the NSF and the Simons Foundation. Work done in part while the ﬁrst author was at the
Simons Institute for the Theory of Computing.

8

References
[1] Alexandr Andoni  Piotr Indyk  Huy L. Nguyen  and Ilya Razenshteyn. Beyond locality-sensitive hashing.

In SODA  2014. Full version at http://arxiv.org/abs/1306.1547.

[2] Alexandr Andoni and Ilya Razenshteyn. Optimal data-dependent hashing for approximate near neighbors.

In STOC  2015. Full version at http://arxiv.org/abs/1501.01062.

[3] Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC  2002.
[4] Gregory Shakhnarovich  Trevor Darrell  and Piotr Indyk. Nearest-Neighbor Methods in Learning and

Vision: Theory and Practice. MIT Press  Cambridge  MA  2005.

[5] Hanan Samet. Foundations of multidimensional and metric data structures. Morgan Kaufmann  2006.
[6] Sariel Har-Peled  Piotr Indyk  and Rajeev Motwani. Approximate nearest neighbor: Towards removing

the curse of dimensionality. Theory of Computing  8(14):321–350  2012.

[7] Hervé Jégou  Matthijs Douze  and Cordelia Schmid. Product quantization for nearest neighbor search.

IEEE Transactions on Pattern Analysis and Machine Intelligence  33(1):117–128  2011.

[8] Ludwig Schmidt  Matthew Shariﬁ  and Ignacio Lopez Moreno. Large-scale speaker identiﬁcation. In

ICASSP  2014.

[9] Narayanan Sundaram  Aizana Turmukhametova  Nadathur Satish  Todd Mostak  Piotr Indyk  Samuel Mad-
den  and Pradeep Dubey. Streaming similarity search over one billion tweets using parallel locality-
sensitive hashing. In VLDB  2013.

[10] Moshe Dubiner. Bucketing coding and information theory for the statistical high-dimensional nearest-

neighbor problem. IEEE Transactions on Information Theory  56(8):4166–4179  2010.

[11] Alexandr Andoni and Ilya Razenshteyn. Tight lower bounds for data-dependent locality-sensitive hashing 

2015. Available at http://arxiv.org/abs/1507.04299.

[12] Anshumali Shrivastava and Ping Li. Fast near neighbor search in high-dimensional binary data. In Machine

Learning and Knowledge Discovery in Databases  pages 474–489. Springer  2012.

[13] Anshumali Shrivastava and Ping Li. Densifying one permutation hashing via rotation for fast near neighbor

search. In ICML  2014.

[14] Qin Lv  William Josephson  Zhe Wang  Moses Charikar  and Kai Li. Multi-probe lsh: efﬁcient indexing

for high-dimensional similarity search. In VLDB  2007.

[15] Kengo Terasawa and Yuzuru Tanaka. Spherical lsh for approximate nearest neighbor search on unit

hypersphere. In Algorithms and Data Structures  pages 27–38. Springer  2007.

[16] Kave Eshghi and Shyamsundar Rajaram. Locality sensitive hash functions based on concomitant rank

order statistics. In KDD  2008.

[17] Nir Ailon and Bernard Chazelle. The fast Johnson–Lindenstrauss transform and approximate nearest

neighbors. SIAM Journal on Computing  39(1):302–322  2009.

[18] Kilian Q. Weinberger  Anirban Dasgupta  John Langford  Alexander J. Smola  and Josh Attenberg. Feature

hashing for large scale multitask learning. In ICML  2009.

[19] Rajeev Motwani  Assaf Naor  and Rina Panigrahy. Lower bounds on locality sensitive hashing. SIAM

Journal on Discrete Mathematics  21(4):930–935  2007.

[20] Ryan O’Donnell  Yi Wu  and Yuan Zhou. Optimal lower bounds for locality-sensitive hashing (except

when q is tiny). ACM Transactions on Computation Theory  6(1):5  2014.

[21] Anirban Dasgupta  Ravi Kumar  and Tamás Sarlós. Fast locality-sensitive hashing. In KDD  2011.
[22] Nir Ailon and Holger Rauhut. Fast and RIP-optimal transforms. Discrete & Computational Geometry 

52(4):780–798  2014.

[23] Uriel Feige and Gideon Schechtman. On the optimality of the random hyperplane rounding technique for

MAX CUT. Random Structures and Algorithms  20(3):403–440  2002.

[24] Malcolm Slaney  Yury Lifshits  and Junfeng He. Optimal parameters for locality-sensitive hashing. Pro-

ceedings of the IEEE  100(9):2604–2623  2012.

[25] Moshe Lichman. UCI machine learning repository  2013.
[26] Persi Diaconis and David Freedman. A dozen de Finetti-style results in search of a theory. Annales de

l’institut Henri Poincaré (B) Probabilités et Statistiques  23(S2):397–423  1987.

9

,Xianghang Liu
Justin Domke
Alexandr Andoni
Piotr Indyk
Thijs Laarhoven
Ilya Razenshteyn
Ludwig Schmidt
Yiwen Guo
Anbang Yao
Yurong Chen